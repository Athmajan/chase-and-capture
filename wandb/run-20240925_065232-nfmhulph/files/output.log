Epoch: 0, Batch Gradient Norm: 30.83825703649474
Epoch: 0, Batch Gradient Norm after: 22.360676714501565
Epoch 1/10000, Prediction Accuracy = 21.256%, Loss = 61.306365203857425
Epoch: 1, Batch Gradient Norm: 57.68454090102363
Epoch: 1, Batch Gradient Norm after: 22.36067699416988
Epoch 2/10000, Prediction Accuracy = 22.31%, Loss = 54.95706405639648
Epoch: 2, Batch Gradient Norm: 86.51288328252254
Epoch: 2, Batch Gradient Norm after: 22.360680200988416
Epoch 3/10000, Prediction Accuracy = 22.31%, Loss = 47.201578521728514
Epoch: 3, Batch Gradient Norm: 108.825584130182
Epoch: 3, Batch Gradient Norm after: 22.360679536569126
Epoch 4/10000, Prediction Accuracy = 22.31%, Loss = 39.05247421264649
Epoch: 4, Batch Gradient Norm: 121.51179282095266
Epoch: 4, Batch Gradient Norm after: 22.360679338319585
Epoch 5/10000, Prediction Accuracy = 22.31%, Loss = 31.215591430664062
Epoch: 5, Batch Gradient Norm: 122.81481746829768
Epoch: 5, Batch Gradient Norm after: 22.36068018673022
Epoch 6/10000, Prediction Accuracy = 22.31%, Loss = 24.07008934020996
Epoch: 6, Batch Gradient Norm: 111.08158371938212
Epoch: 6, Batch Gradient Norm after: 22.360678742119642
Epoch 7/10000, Prediction Accuracy = 22.31%, Loss = 18.03077220916748
Epoch: 7, Batch Gradient Norm: 85.592095339956
Epoch: 7, Batch Gradient Norm after: 22.360681707491818
Epoch 8/10000, Prediction Accuracy = 22.31%, Loss = 13.485481071472169
Epoch: 8, Batch Gradient Norm: 48.69475967786333
Epoch: 8, Batch Gradient Norm after: 22.3606772569808
Epoch 9/10000, Prediction Accuracy = 21.728%, Loss = 10.70569667816162
Epoch: 9, Batch Gradient Norm: 15.911827332194667
Epoch: 9, Batch Gradient Norm after: 15.586557375255873
Epoch 10/10000, Prediction Accuracy = 21.688%, Loss = 9.477281761169433
Epoch: 10, Batch Gradient Norm: 9.066228373587874
Epoch: 10, Batch Gradient Norm after: 9.066228373587874
Epoch 11/10000, Prediction Accuracy = 23.216%, Loss = 8.960032272338868
Epoch: 11, Batch Gradient Norm: 8.315252613821
Epoch: 11, Batch Gradient Norm after: 8.315252613821
Epoch 12/10000, Prediction Accuracy = 26.224%, Loss = 8.580217552185058
Epoch: 12, Batch Gradient Norm: 7.984473048507747
Epoch: 12, Batch Gradient Norm after: 7.984473048507747
Epoch 13/10000, Prediction Accuracy = 26.096000000000004%, Loss = 8.258563709259032
Epoch: 13, Batch Gradient Norm: 7.740690128899157
Epoch: 13, Batch Gradient Norm after: 7.740690128899157
Epoch 14/10000, Prediction Accuracy = 26.018%, Loss = 7.975510787963867
Epoch: 14, Batch Gradient Norm: 7.531746700412259
Epoch: 14, Batch Gradient Norm after: 7.531746700412259
Epoch 15/10000, Prediction Accuracy = 26.248%, Loss = 7.720184230804444
Epoch: 15, Batch Gradient Norm: 7.331420171626963
Epoch: 15, Batch Gradient Norm after: 7.331420171626963
Epoch 16/10000, Prediction Accuracy = 26.468%, Loss = 7.487103366851807
Epoch: 16, Batch Gradient Norm: 7.147407406275365
Epoch: 16, Batch Gradient Norm after: 7.147407406275365
Epoch 17/10000, Prediction Accuracy = 26.564%, Loss = 7.274161243438721
Epoch: 17, Batch Gradient Norm: 6.953520738093226
Epoch: 17, Batch Gradient Norm after: 6.953520738093226
Epoch 18/10000, Prediction Accuracy = 26.608000000000004%, Loss = 7.078750324249268
Epoch: 18, Batch Gradient Norm: 6.759115822181302
Epoch: 18, Batch Gradient Norm after: 6.759115822181302
Epoch 19/10000, Prediction Accuracy = 26.848000000000003%, Loss = 6.899297904968262
Epoch: 19, Batch Gradient Norm: 6.548286996738066
Epoch: 19, Batch Gradient Norm after: 6.548286996738066
Epoch 20/10000, Prediction Accuracy = 27.006%, Loss = 6.734857368469238
Epoch: 20, Batch Gradient Norm: 6.328977073491941
Epoch: 20, Batch Gradient Norm after: 6.328977073491941
Epoch 21/10000, Prediction Accuracy = 27.259999999999998%, Loss = 6.584880924224853
Epoch: 21, Batch Gradient Norm: 6.111073889228365
Epoch: 21, Batch Gradient Norm after: 6.111073889228365
Epoch 22/10000, Prediction Accuracy = 27.398000000000003%, Loss = 6.448303890228272
Epoch: 22, Batch Gradient Norm: 5.820701976509416
Epoch: 22, Batch Gradient Norm after: 5.820701976509416
Epoch 23/10000, Prediction Accuracy = 27.483999999999998%, Loss = 6.327087116241455
Epoch: 23, Batch Gradient Norm: 5.552469029335883
Epoch: 23, Batch Gradient Norm after: 5.552469029335883
Epoch 24/10000, Prediction Accuracy = 27.574%, Loss = 6.220308303833008
Epoch: 24, Batch Gradient Norm: 5.284163868229233
Epoch: 24, Batch Gradient Norm after: 5.284163868229233
Epoch 25/10000, Prediction Accuracy = 27.743999999999993%, Loss = 6.12627477645874
Epoch: 25, Batch Gradient Norm: 5.039696342882825
Epoch: 25, Batch Gradient Norm after: 5.039696342882825
Epoch 26/10000, Prediction Accuracy = 27.815999999999995%, Loss = 6.043085289001465
Epoch: 26, Batch Gradient Norm: 4.834608197976132
Epoch: 26, Batch Gradient Norm after: 4.834608197976132
Epoch 27/10000, Prediction Accuracy = 27.841999999999995%, Loss = 5.9693046569824215
Epoch: 27, Batch Gradient Norm: 4.56384898240736
Epoch: 27, Batch Gradient Norm after: 4.56384898240736
Epoch 28/10000, Prediction Accuracy = 27.976%, Loss = 5.905747890472412
Epoch: 28, Batch Gradient Norm: 4.356644015958054
Epoch: 28, Batch Gradient Norm after: 4.356644015958054
Epoch 29/10000, Prediction Accuracy = 28.062%, Loss = 5.851030445098877
Epoch: 29, Batch Gradient Norm: 4.165472723284105
Epoch: 29, Batch Gradient Norm after: 4.165472723284105
Epoch 30/10000, Prediction Accuracy = 28.215999999999998%, Loss = 5.8034075736999515
Epoch: 30, Batch Gradient Norm: 4.005861026598747
Epoch: 30, Batch Gradient Norm after: 4.005861026598747
Epoch 31/10000, Prediction Accuracy = 28.404000000000003%, Loss = 5.761951160430908
Epoch: 31, Batch Gradient Norm: 3.848198908943152
Epoch: 31, Batch Gradient Norm after: 3.848198908943152
Epoch 32/10000, Prediction Accuracy = 28.538%, Loss = 5.725635719299317
Epoch: 32, Batch Gradient Norm: 3.7474779687909803
Epoch: 32, Batch Gradient Norm after: 3.7474779687909803
Epoch 33/10000, Prediction Accuracy = 28.692%, Loss = 5.693530082702637
Epoch: 33, Batch Gradient Norm: 3.697138698656206
Epoch: 33, Batch Gradient Norm after: 3.697138698656206
Epoch 34/10000, Prediction Accuracy = 28.836000000000002%, Loss = 5.664832878112793
Epoch: 34, Batch Gradient Norm: 3.6223033637836317
Epoch: 34, Batch Gradient Norm after: 3.6223033637836317
Epoch 35/10000, Prediction Accuracy = 28.990000000000002%, Loss = 5.638739585876465
Epoch: 35, Batch Gradient Norm: 3.6200044837482883
Epoch: 35, Batch Gradient Norm after: 3.6200044837482883
Epoch 36/10000, Prediction Accuracy = 29.176%, Loss = 5.614857006072998
Epoch: 36, Batch Gradient Norm: 3.5320323498214248
Epoch: 36, Batch Gradient Norm after: 3.5320323498214248
Epoch 37/10000, Prediction Accuracy = 29.427999999999997%, Loss = 5.592520332336425
Epoch: 37, Batch Gradient Norm: 3.516142715547697
Epoch: 37, Batch Gradient Norm after: 3.516142715547697
Epoch 38/10000, Prediction Accuracy = 29.642000000000003%, Loss = 5.571765518188476
Epoch: 38, Batch Gradient Norm: 3.5209211121427293
Epoch: 38, Batch Gradient Norm after: 3.5209211121427293
Epoch 39/10000, Prediction Accuracy = 29.824%, Loss = 5.552039337158203
Epoch: 39, Batch Gradient Norm: 3.512338169549239
Epoch: 39, Batch Gradient Norm after: 3.512338169549239
Epoch 40/10000, Prediction Accuracy = 30.101999999999997%, Loss = 5.532955646514893
Epoch: 40, Batch Gradient Norm: 3.507840301147094
Epoch: 40, Batch Gradient Norm after: 3.507840301147094
Epoch 41/10000, Prediction Accuracy = 30.336000000000002%, Loss = 5.514401817321778
Epoch: 41, Batch Gradient Norm: 3.510733930636489
Epoch: 41, Batch Gradient Norm after: 3.510733930636489
Epoch 42/10000, Prediction Accuracy = 30.54%, Loss = 5.496171283721924
Epoch: 42, Batch Gradient Norm: 3.4889918135244518
Epoch: 42, Batch Gradient Norm after: 3.4889918135244518
Epoch 43/10000, Prediction Accuracy = 30.79%, Loss = 5.47818660736084
Epoch: 43, Batch Gradient Norm: 3.487500357930197
Epoch: 43, Batch Gradient Norm after: 3.487500357930197
Epoch 44/10000, Prediction Accuracy = 30.964%, Loss = 5.460337448120117
Epoch: 44, Batch Gradient Norm: 3.4429799845499516
Epoch: 44, Batch Gradient Norm after: 3.4429799845499516
Epoch 45/10000, Prediction Accuracy = 31.278%, Loss = 5.442366886138916
Epoch: 45, Batch Gradient Norm: 3.411809653889633
Epoch: 45, Batch Gradient Norm after: 3.411809653889633
Epoch 46/10000, Prediction Accuracy = 31.445999999999998%, Loss = 5.4245658874511715
Epoch: 46, Batch Gradient Norm: 3.392954685290518
Epoch: 46, Batch Gradient Norm after: 3.392954685290518
Epoch 47/10000, Prediction Accuracy = 31.688%, Loss = 5.406852340698242
Epoch: 47, Batch Gradient Norm: 3.275434820869874
Epoch: 47, Batch Gradient Norm after: 3.275434820869874
Epoch 48/10000, Prediction Accuracy = 31.829999999999995%, Loss = 5.38891019821167
Epoch: 48, Batch Gradient Norm: 3.246512974357559
Epoch: 48, Batch Gradient Norm after: 3.246512974357559
Epoch 49/10000, Prediction Accuracy = 32.034%, Loss = 5.371062564849853
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode20-dqn/classify_rsmProp2.py", line 143, in <module>
    for data in data_loader:
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 678, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 264, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 142, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 119, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 171, in collate_numpy_array_fn
    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 171, in <listcomp>
    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
                    ^^^^^^^^^^^^^^^^^^
KeyboardInterrupt