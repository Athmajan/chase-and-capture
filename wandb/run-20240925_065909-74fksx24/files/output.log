Epoch: 0, Batch Gradient Norm: 28.975123359731928
Epoch: 0, Batch Gradient Norm after: 22.36067770429285
Epoch 1/10000, Prediction Accuracy = 23.322%, Loss = 61.19813232421875
Epoch: 1, Batch Gradient Norm: 54.784695766535165
Epoch: 1, Batch Gradient Norm after: 22.36067748653528
Epoch 2/10000, Prediction Accuracy = 23.322%, Loss = 55.46730346679688
Epoch: 2, Batch Gradient Norm: 82.8945017830942
Epoch: 2, Batch Gradient Norm after: 22.360680759174787
Epoch 3/10000, Prediction Accuracy = 23.322%, Loss = 48.41346130371094
Epoch: 3, Batch Gradient Norm: 105.60715132692945
Epoch: 3, Batch Gradient Norm after: 22.360680542495132
Epoch 4/10000, Prediction Accuracy = 23.322%, Loss = 40.93974075317383
Epoch: 4, Batch Gradient Norm: 120.09622514668537
Epoch: 4, Batch Gradient Norm after: 22.360680500990874
Epoch 5/10000, Prediction Accuracy = 23.322%, Loss = 33.66149787902832
Epoch: 5, Batch Gradient Norm: 125.26806730565347
Epoch: 5, Batch Gradient Norm after: 22.360680014579973
Epoch 6/10000, Prediction Accuracy = 23.322%, Loss = 26.933946990966795
Epoch: 6, Batch Gradient Norm: 120.14683622659544
Epoch: 6, Batch Gradient Norm after: 22.360680038323146
Epoch 7/10000, Prediction Accuracy = 23.322%, Loss = 21.054958724975585
Epoch: 7, Batch Gradient Norm: 104.4452262475383
Epoch: 7, Batch Gradient Norm after: 22.360678159480425
Epoch 8/10000, Prediction Accuracy = 23.322%, Loss = 16.312121963500978
Epoch: 8, Batch Gradient Norm: 79.63531577021396
Epoch: 8, Batch Gradient Norm after: 22.36067806335564
Epoch 9/10000, Prediction Accuracy = 23.322%, Loss = 12.92765121459961
Epoch: 9, Batch Gradient Norm: 50.935656890051305
Epoch: 9, Batch Gradient Norm after: 22.36067706788066
Epoch 10/10000, Prediction Accuracy = 23.322%, Loss = 10.896991729736328
Epoch: 10, Batch Gradient Norm: 26.005774640445164
Epoch: 10, Batch Gradient Norm after: 21.29301917384941
Epoch 11/10000, Prediction Accuracy = 23.322%, Loss = 9.784738159179687
Epoch: 11, Batch Gradient Norm: 11.621272001034356
Epoch: 11, Batch Gradient Norm after: 11.621272001034356
Epoch 12/10000, Prediction Accuracy = 23.32%, Loss = 9.129858589172363
Epoch: 12, Batch Gradient Norm: 8.261933685052645
Epoch: 12, Batch Gradient Norm after: 8.261933685052645
Epoch 13/10000, Prediction Accuracy = 23.412%, Loss = 8.711855697631837
Epoch: 13, Batch Gradient Norm: 7.561758196858792
Epoch: 13, Batch Gradient Norm after: 7.561758196858792
Epoch 14/10000, Prediction Accuracy = 23.706%, Loss = 8.364726448059082
Epoch: 14, Batch Gradient Norm: 7.276130058160593
Epoch: 14, Batch Gradient Norm after: 7.276130058160593
Epoch 15/10000, Prediction Accuracy = 23.88%, Loss = 8.058969306945801
Epoch: 15, Batch Gradient Norm: 7.1008762127254785
Epoch: 15, Batch Gradient Norm after: 7.1008762127254785
Epoch 16/10000, Prediction Accuracy = 23.874000000000002%, Loss = 7.781204509735107
Epoch: 16, Batch Gradient Norm: 6.949751518727152
Epoch: 16, Batch Gradient Norm after: 6.949751518727152
Epoch 17/10000, Prediction Accuracy = 23.788%, Loss = 7.524703788757324
Epoch: 17, Batch Gradient Norm: 6.790131192837977
Epoch: 17, Batch Gradient Norm after: 6.790131192837977
Epoch 18/10000, Prediction Accuracy = 23.767999999999997%, Loss = 7.2872621536254885
Epoch: 18, Batch Gradient Norm: 6.638342768233587
Epoch: 18, Batch Gradient Norm after: 6.638342768233587
Epoch 19/10000, Prediction Accuracy = 23.767999999999997%, Loss = 7.067206287384034
Epoch: 19, Batch Gradient Norm: 6.487613471331897
Epoch: 19, Batch Gradient Norm after: 6.487613471331897
Epoch 20/10000, Prediction Accuracy = 23.804%, Loss = 6.862811183929443
Epoch: 20, Batch Gradient Norm: 6.319665436613279
Epoch: 20, Batch Gradient Norm after: 6.319665436613279
Epoch 21/10000, Prediction Accuracy = 23.784%, Loss = 6.672057247161865
Epoch: 21, Batch Gradient Norm: 6.124716066343544
Epoch: 21, Batch Gradient Norm after: 6.124716066343544
Epoch 22/10000, Prediction Accuracy = 23.895999999999997%, Loss = 6.493464374542237
Epoch: 22, Batch Gradient Norm: 5.886198867142748
Epoch: 22, Batch Gradient Norm after: 5.886198867142748
Epoch 23/10000, Prediction Accuracy = 23.93%, Loss = 6.331033992767334
Epoch: 23, Batch Gradient Norm: 5.620254032157757
Epoch: 23, Batch Gradient Norm after: 5.620254032157757
Epoch 24/10000, Prediction Accuracy = 23.97%, Loss = 6.185162734985352
Epoch: 24, Batch Gradient Norm: 5.3239412062517
Epoch: 24, Batch Gradient Norm after: 5.3239412062517
Epoch 25/10000, Prediction Accuracy = 24.009999999999998%, Loss = 6.0552948951721195
Epoch: 25, Batch Gradient Norm: 5.016575247734178
Epoch: 25, Batch Gradient Norm after: 5.016575247734178
Epoch 26/10000, Prediction Accuracy = 24.003999999999998%, Loss = 5.940514850616455
Epoch: 26, Batch Gradient Norm: 4.710614841862833
Epoch: 26, Batch Gradient Norm after: 4.710614841862833
Epoch 27/10000, Prediction Accuracy = 23.976000000000003%, Loss = 5.839717864990234
Epoch: 27, Batch Gradient Norm: 4.3973666900639214
Epoch: 27, Batch Gradient Norm after: 4.3973666900639214
Epoch 28/10000, Prediction Accuracy = 24.038%, Loss = 5.752348899841309
Epoch: 28, Batch Gradient Norm: 4.101730345040534
Epoch: 28, Batch Gradient Norm after: 4.101730345040534
Epoch 29/10000, Prediction Accuracy = 24.116%, Loss = 5.676517486572266
Epoch: 29, Batch Gradient Norm: 3.8179436125972397
Epoch: 29, Batch Gradient Norm after: 3.8179436125972397
Epoch 30/10000, Prediction Accuracy = 24.119999999999997%, Loss = 5.610898494720459
Epoch: 30, Batch Gradient Norm: 3.6125004479092735
Epoch: 30, Batch Gradient Norm after: 3.6125004479092735
Epoch 31/10000, Prediction Accuracy = 24.21%, Loss = 5.55396089553833
Epoch: 31, Batch Gradient Norm: 3.3907356550818464
Epoch: 31, Batch Gradient Norm after: 3.3907356550818464
Epoch 32/10000, Prediction Accuracy = 24.376%, Loss = 5.5044426918029785
Epoch: 32, Batch Gradient Norm: 3.2348183021125982
Epoch: 32, Batch Gradient Norm after: 3.2348183021125982
Epoch 33/10000, Prediction Accuracy = 24.576%, Loss = 5.460787773132324
Epoch: 33, Batch Gradient Norm: 3.0854090325407
Epoch: 33, Batch Gradient Norm after: 3.0854090325407
Epoch 34/10000, Prediction Accuracy = 24.822%, Loss = 5.421781730651856
Epoch: 34, Batch Gradient Norm: 2.9990811004384597
Epoch: 34, Batch Gradient Norm after: 2.9990811004384597
Epoch 35/10000, Prediction Accuracy = 25.080000000000002%, Loss = 5.386656761169434
Epoch: 35, Batch Gradient Norm: 2.9195270759843965
Epoch: 35, Batch Gradient Norm after: 2.9195270759843965
Epoch 36/10000, Prediction Accuracy = 25.292%, Loss = 5.35418815612793
Epoch: 36, Batch Gradient Norm: 2.883481194313759
Epoch: 36, Batch Gradient Norm after: 2.883481194313759
Epoch 37/10000, Prediction Accuracy = 25.672000000000004%, Loss = 5.323231029510498
Epoch: 37, Batch Gradient Norm: 2.8912503913950895
Epoch: 37, Batch Gradient Norm after: 2.8912503913950895
Epoch 38/10000, Prediction Accuracy = 25.984%, Loss = 5.293537235260009
Epoch: 38, Batch Gradient Norm: 2.8481373397316285
Epoch: 38, Batch Gradient Norm after: 2.8481373397316285
Epoch 39/10000, Prediction Accuracy = 26.369999999999997%, Loss = 5.2647298812866214
Epoch: 39, Batch Gradient Norm: 2.848636987989641
Epoch: 39, Batch Gradient Norm after: 2.848636987989641
Epoch 40/10000, Prediction Accuracy = 26.689999999999998%, Loss = 5.236908054351806
Epoch: 40, Batch Gradient Norm: 2.819860126744782
Epoch: 40, Batch Gradient Norm after: 2.819860126744782
Epoch 41/10000, Prediction Accuracy = 26.926%, Loss = 5.209571456909179
Epoch: 41, Batch Gradient Norm: 2.8247518780037697
Epoch: 41, Batch Gradient Norm after: 2.8247518780037697
Epoch 42/10000, Prediction Accuracy = 27.176%, Loss = 5.182520961761474
Epoch: 42, Batch Gradient Norm: 2.7999475468099764
Epoch: 42, Batch Gradient Norm after: 2.7999475468099764
Epoch 43/10000, Prediction Accuracy = 27.410000000000004%, Loss = 5.155638313293457
Epoch: 43, Batch Gradient Norm: 2.8354761819897885
Epoch: 43, Batch Gradient Norm after: 2.8354761819897885
Epoch 44/10000, Prediction Accuracy = 27.570000000000004%, Loss = 5.128850269317627
Epoch: 44, Batch Gradient Norm: 2.759763059848594
Epoch: 44, Batch Gradient Norm after: 2.759763059848594
Epoch 45/10000, Prediction Accuracy = 27.72%, Loss = 5.101869201660156
Epoch: 45, Batch Gradient Norm: 3.05412496578275
Epoch: 45, Batch Gradient Norm after: 3.05412496578275
Epoch 46/10000, Prediction Accuracy = 27.965999999999998%, Loss = 5.075159549713135
Epoch: 46, Batch Gradient Norm: 3.8852499320264307
Epoch: 46, Batch Gradient Norm after: 3.8852499320264307
Epoch 47/10000, Prediction Accuracy = 28.124000000000002%, Loss = 5.049646377563477
Epoch: 47, Batch Gradient Norm: 8.979188148860816
Epoch: 47, Batch Gradient Norm after: 8.979188148860816
Epoch 48/10000, Prediction Accuracy = 28.451999999999998%, Loss = 5.039053916931152
Epoch: 48, Batch Gradient Norm: 10.663626396478861
Epoch: 48, Batch Gradient Norm after: 10.663626396478861
Epoch 49/10000, Prediction Accuracy = 28.338%, Loss = 5.0219934463500975
Epoch: 49, Batch Gradient Norm: 7.352989381961912
Epoch: 49, Batch Gradient Norm after: 7.352989381961912
Epoch 50/10000, Prediction Accuracy = 28.958%, Loss = 4.979906749725342
Epoch: 50, Batch Gradient Norm: 4.1079623397441996
Epoch: 50, Batch Gradient Norm after: 4.1079623397441996
Epoch 51/10000, Prediction Accuracy = 29.088%, Loss = 4.944204425811767
Epoch: 51, Batch Gradient Norm: 3.9790205039524644
Epoch: 51, Batch Gradient Norm after: 3.9790205039524644
Epoch 52/10000, Prediction Accuracy = 29.274%, Loss = 4.916933536529541
Epoch: 52, Batch Gradient Norm: 3.3364009438050832
Epoch: 52, Batch Gradient Norm after: 3.3364009438050832
Epoch 53/10000, Prediction Accuracy = 29.538%, Loss = 4.888662052154541
Epoch: 53, Batch Gradient Norm: 4.02535207812269
Epoch: 53, Batch Gradient Norm after: 4.02535207812269
Epoch 54/10000, Prediction Accuracy = 29.714%, Loss = 4.862015724182129
Epoch: 54, Batch Gradient Norm: 4.047141571613423
Epoch: 54, Batch Gradient Norm after: 4.047141571613423
Epoch 55/10000, Prediction Accuracy = 29.815999999999995%, Loss = 4.834082698822021
Epoch: 55, Batch Gradient Norm: 6.964598745446382
Epoch: 55, Batch Gradient Norm after: 6.964598745446382
Epoch 56/10000, Prediction Accuracy = 30.032000000000004%, Loss = 4.813100433349609
Epoch: 56, Batch Gradient Norm: 9.903568657361264
Epoch: 56, Batch Gradient Norm after: 9.903568657361264
Epoch 57/10000, Prediction Accuracy = 29.890000000000004%, Loss = 4.7978802680969235
Epoch: 57, Batch Gradient Norm: 11.25224816410386
Epoch: 57, Batch Gradient Norm after: 11.25224816410386
Epoch 58/10000, Prediction Accuracy = 30.266%, Loss = 4.776428318023681
Epoch: 58, Batch Gradient Norm: 7.04133590169858
Epoch: 58, Batch Gradient Norm after: 7.04133590169858
Epoch 59/10000, Prediction Accuracy = 30.492%, Loss = 4.730082225799561
Epoch: 59, Batch Gradient Norm: 6.0764464902887365
Epoch: 59, Batch Gradient Norm after: 6.0764464902887365
Epoch 60/10000, Prediction Accuracy = 30.71%, Loss = 4.698318004608154
Epoch: 60, Batch Gradient Norm: 4.415782448186449
Epoch: 60, Batch Gradient Norm after: 4.415782448186449
Epoch 61/10000, Prediction Accuracy = 30.919999999999998%, Loss = 4.6658330917358395
Epoch: 61, Batch Gradient Norm: 5.445050136513653
Epoch: 61, Batch Gradient Norm after: 5.445050136513653
Epoch 62/10000, Prediction Accuracy = 31.0%, Loss = 4.638949584960938
Epoch: 62, Batch Gradient Norm: 5.252702966401479
Epoch: 62, Batch Gradient Norm after: 5.252702966401479
Epoch 63/10000, Prediction Accuracy = 31.052%, Loss = 4.609538459777832
Epoch: 63, Batch Gradient Norm: 8.319197845340671
Epoch: 63, Batch Gradient Norm after: 8.319197845340671
Epoch 64/10000, Prediction Accuracy = 31.282%, Loss = 4.589407920837402
Epoch: 64, Batch Gradient Norm: 9.908315133120595
Epoch: 64, Batch Gradient Norm after: 9.908315133120595
Epoch 65/10000, Prediction Accuracy = 31.305999999999994%, Loss = 4.567715644836426
Epoch: 65, Batch Gradient Norm: 11.498921773209489
Epoch: 65, Batch Gradient Norm after: 11.498921773209489
Epoch 66/10000, Prediction Accuracy = 31.695999999999998%, Loss = 4.545945930480957
Epoch: 66, Batch Gradient Norm: 8.067263194922141
Epoch: 66, Batch Gradient Norm after: 8.067263194922141
Epoch 67/10000, Prediction Accuracy = 31.703999999999997%, Loss = 4.501825428009033
Epoch: 67, Batch Gradient Norm: 7.541594639322089
Epoch: 67, Batch Gradient Norm after: 7.541594639322089
Epoch 68/10000, Prediction Accuracy = 31.968%, Loss = 4.4701377868652346
Epoch: 68, Batch Gradient Norm: 5.507737518261981
Epoch: 68, Batch Gradient Norm after: 5.507737518261981
Epoch 69/10000, Prediction Accuracy = 32.148%, Loss = 4.4352771759033205
Epoch: 69, Batch Gradient Norm: 6.849426860506472
Epoch: 69, Batch Gradient Norm after: 6.849426860506472
Epoch 70/10000, Prediction Accuracy = 32.38%, Loss = 4.409094905853271
Epoch: 70, Batch Gradient Norm: 6.624746259005781
Epoch: 70, Batch Gradient Norm after: 6.624746259005781
Epoch 71/10000, Prediction Accuracy = 32.438%, Loss = 4.379351329803467
Epoch: 71, Batch Gradient Norm: 9.87305487385431
Epoch: 71, Batch Gradient Norm after: 9.87305487385431
Epoch 72/10000, Prediction Accuracy = 32.682%, Loss = 4.36121883392334
Epoch: 72, Batch Gradient Norm: 10.421570170917631
Epoch: 72, Batch Gradient Norm after: 10.421570170917631
Epoch 73/10000, Prediction Accuracy = 32.709999999999994%, Loss = 4.335301780700684
Epoch: 73, Batch Gradient Norm: 11.576383346710632
Epoch: 73, Batch Gradient Norm after: 11.576383346710632
Epoch 74/10000, Prediction Accuracy = 33.006%, Loss = 4.311085796356201
Epoch: 74, Batch Gradient Norm: 8.255757714076193
Epoch: 74, Batch Gradient Norm after: 8.255757714076193
Epoch 75/10000, Prediction Accuracy = 33.041999999999994%, Loss = 4.268181324005127
Epoch: 75, Batch Gradient Norm: 8.341885374302079
Epoch: 75, Batch Gradient Norm after: 8.341885374302079
Epoch 76/10000, Prediction Accuracy = 33.354%, Loss = 4.238746070861817
Epoch: 76, Batch Gradient Norm: 6.429527543782692
Epoch: 76, Batch Gradient Norm after: 6.429527543782692
Epoch 77/10000, Prediction Accuracy = 33.44799999999999%, Loss = 4.204039192199707
Epoch: 77, Batch Gradient Norm: 8.330373941994246
Epoch: 77, Batch Gradient Norm after: 8.330373941994246
Epoch 78/10000, Prediction Accuracy = 33.635999999999996%, Loss = 4.180168914794922
Epoch: 78, Batch Gradient Norm: 8.259712934760604
Epoch: 78, Batch Gradient Norm after: 8.259712934760604
Epoch 79/10000, Prediction Accuracy = 33.788%, Loss = 4.151269626617432
Epoch: 79, Batch Gradient Norm: 11.567417396448334
Epoch: 79, Batch Gradient Norm after: 11.567417396448334
Epoch 80/10000, Prediction Accuracy = 34.152%, Loss = 4.135258293151855
Epoch: 80, Batch Gradient Norm: 11.040171738679472
Epoch: 80, Batch Gradient Norm after: 11.040171738679472
Epoch 81/10000, Prediction Accuracy = 34.38799999999999%, Loss = 4.104701900482178
Epoch: 81, Batch Gradient Norm: 11.767810147840633
Epoch: 81, Batch Gradient Norm after: 11.767810147840633
Epoch 82/10000, Prediction Accuracy = 34.42999999999999%, Loss = 4.078670310974121
Epoch: 82, Batch Gradient Norm: 8.550847458064935
Epoch: 82, Batch Gradient Norm after: 8.550847458064935
Epoch 83/10000, Prediction Accuracy = 34.698%, Loss = 4.037302875518799
Epoch: 83, Batch Gradient Norm: 9.223577150249929
Epoch: 83, Batch Gradient Norm after: 9.223577150249929
Epoch 84/10000, Prediction Accuracy = 34.746%, Loss = 4.010648155212403
Epoch: 84, Batch Gradient Norm: 7.601616916051184
Epoch: 84, Batch Gradient Norm after: 7.601616916051184
Epoch 85/10000, Prediction Accuracy = 35.010000000000005%, Loss = 3.977312755584717
Epoch: 85, Batch Gradient Norm: 10.02629138478508
Epoch: 85, Batch Gradient Norm after: 10.02629138478508
Epoch 86/10000, Prediction Accuracy = 35.11%, Loss = 3.956929636001587
Epoch: 86, Batch Gradient Norm: 9.944064637974249
Epoch: 86, Batch Gradient Norm after: 9.944064637974249
Epoch 87/10000, Prediction Accuracy = 35.296%, Loss = 3.929235506057739
Epoch: 87, Batch Gradient Norm: 12.765983029966392
Epoch: 87, Batch Gradient Norm after: 12.765983029966392
Epoch 88/10000, Prediction Accuracy = 35.506%, Loss = 3.9133253574371336
Epoch: 88, Batch Gradient Norm: 11.186761741337543
Epoch: 88, Batch Gradient Norm after: 11.186761741337543
Epoch 89/10000, Prediction Accuracy = 35.538%, Loss = 3.879105472564697
Epoch: 89, Batch Gradient Norm: 11.850829307544608
Epoch: 89, Batch Gradient Norm after: 11.850829307544608
Epoch 90/10000, Prediction Accuracy = 35.762%, Loss = 3.8539591312408445
Epoch: 90, Batch Gradient Norm: 9.120580521111025
Epoch: 90, Batch Gradient Norm after: 9.120580521111025
Epoch 91/10000, Prediction Accuracy = 35.806%, Loss = 3.8165297508239746
Epoch: 91, Batch Gradient Norm: 10.418088142305825
Epoch: 91, Batch Gradient Norm after: 10.418088142305825
Epoch 92/10000, Prediction Accuracy = 36.01%, Loss = 3.793788194656372
Epoch: 92, Batch Gradient Norm: 9.134630704093166
Epoch: 92, Batch Gradient Norm after: 9.134630704093166
Epoch 93/10000, Prediction Accuracy = 36.077999999999996%, Loss = 3.762974739074707
Epoch: 93, Batch Gradient Norm: 11.940232901528544
Epoch: 93, Batch Gradient Norm after: 11.940232901528544
Epoch 94/10000, Prediction Accuracy = 36.274%, Loss = 3.746821403503418
Epoch: 94, Batch Gradient Norm: 11.50628807112066
Epoch: 94, Batch Gradient Norm after: 11.50628807112066
Epoch 95/10000, Prediction Accuracy = 36.279999999999994%, Loss = 3.7195342540740968
Epoch: 95, Batch Gradient Norm: 13.677225960543453
Epoch: 95, Batch Gradient Norm after: 13.677225960543453
Epoch 96/10000, Prediction Accuracy = 36.556000000000004%, Loss = 3.7031336784362794
Epoch: 96, Batch Gradient Norm: 11.307727647723553
Epoch: 96, Batch Gradient Norm after: 11.307727647723553
Epoch 97/10000, Prediction Accuracy = 36.518%, Loss = 3.667850637435913
Epoch: 97, Batch Gradient Norm: 12.131718561250167
Epoch: 97, Batch Gradient Norm after: 12.131718561250167
Epoch 98/10000, Prediction Accuracy = 36.93000000000001%, Loss = 3.6457536697387694
Epoch: 98, Batch Gradient Norm: 9.851847543172873
Epoch: 98, Batch Gradient Norm after: 9.851847543172873
Epoch 99/10000, Prediction Accuracy = 36.684%, Loss = 3.612870454788208
Epoch: 99, Batch Gradient Norm: 11.728615308180675
Epoch: 99, Batch Gradient Norm after: 11.728615308180675
Epoch 100/10000, Prediction Accuracy = 37.102%, Loss = 3.5950209140777587
Epoch: 100, Batch Gradient Norm: 10.772935238506758
Epoch: 100, Batch Gradient Norm after: 10.772935238506758
Epoch 101/10000, Prediction Accuracy = 36.854%, Loss = 3.567715644836426
Epoch: 101, Batch Gradient Norm: 13.750437242642061
Epoch: 101, Batch Gradient Norm after: 13.750437242642061
Epoch 102/10000, Prediction Accuracy = 37.208000000000006%, Loss = 3.5558735847473146
Epoch: 102, Batch Gradient Norm: 12.597570539519033
Epoch: 102, Batch Gradient Norm after: 12.597570539519033
Epoch 103/10000, Prediction Accuracy = 37.108%, Loss = 3.5280638694763184
Epoch: 103, Batch Gradient Norm: 14.05666704311312
Epoch: 103, Batch Gradient Norm after: 14.05666704311312
Epoch 104/10000, Prediction Accuracy = 37.408%, Loss = 3.511257791519165
Epoch: 104, Batch Gradient Norm: 11.35618081593247
Epoch: 104, Batch Gradient Norm after: 11.35618081593247
Epoch 105/10000, Prediction Accuracy = 37.272000000000006%, Loss = 3.478035068511963
Epoch: 105, Batch Gradient Norm: 12.578021463542123
Epoch: 105, Batch Gradient Norm after: 12.578021463542123
Epoch 106/10000, Prediction Accuracy = 37.535999999999994%, Loss = 3.460298442840576
Epoch: 106, Batch Gradient Norm: 10.744968248467123
Epoch: 106, Batch Gradient Norm after: 10.744968248467123
Epoch 107/10000, Prediction Accuracy = 37.6%, Loss = 3.4320999145507813
Epoch: 107, Batch Gradient Norm: 13.265725038627325
Epoch: 107, Batch Gradient Norm after: 13.265725038627325
Epoch 108/10000, Prediction Accuracy = 37.95%, Loss = 3.4198829174041747
Epoch: 108, Batch Gradient Norm: 12.399513867418214
Epoch: 108, Batch Gradient Norm after: 12.399513867418214
Epoch 109/10000, Prediction Accuracy = 37.751999999999995%, Loss = 3.3957469940185545
Epoch: 109, Batch Gradient Norm: 14.958183451745134
Epoch: 109, Batch Gradient Norm after: 14.958183451745134
Epoch 110/10000, Prediction Accuracy = 37.934%, Loss = 3.385644960403442
Epoch: 110, Batch Gradient Norm: 13.027176507316396
Epoch: 110, Batch Gradient Norm after: 13.027176507316396
Epoch 111/10000, Prediction Accuracy = 37.894%, Loss = 3.357551336288452
Epoch: 111, Batch Gradient Norm: 14.277492010238298
Epoch: 111, Batch Gradient Norm after: 14.277492010238298
Epoch 112/10000, Prediction Accuracy = 38.112%, Loss = 3.342687702178955
Epoch: 112, Batch Gradient Norm: 11.701607054857911
Epoch: 112, Batch Gradient Norm after: 11.701607054857911
Epoch 113/10000, Prediction Accuracy = 38.059999999999995%, Loss = 3.313554573059082
Epoch: 113, Batch Gradient Norm: 13.422916060428912
Epoch: 113, Batch Gradient Norm after: 13.422916060428912
Epoch 114/10000, Prediction Accuracy = 38.217999999999996%, Loss = 3.3006968021392824
Epoch: 114, Batch Gradient Norm: 11.866650656773237
Epoch: 114, Batch Gradient Norm after: 11.866650656773237
Epoch 115/10000, Prediction Accuracy = 38.17%, Loss = 3.276491975784302
Epoch: 115, Batch Gradient Norm: 14.582728125692055
Epoch: 115, Batch Gradient Norm after: 14.582728125692055
Epoch 116/10000, Prediction Accuracy = 38.3%, Loss = 3.2683616638183595
Epoch: 116, Batch Gradient Norm: 13.549377387288251
Epoch: 116, Batch Gradient Norm after: 13.549377387288251
Epoch 117/10000, Prediction Accuracy = 38.274%, Loss = 3.2464455127716065
Epoch: 117, Batch Gradient Norm: 15.872698179405967
Epoch: 117, Batch Gradient Norm after: 15.872698179405967
Epoch 118/10000, Prediction Accuracy = 38.344%, Loss = 3.23862566947937
Epoch: 118, Batch Gradient Norm: 13.460096647410348
Epoch: 118, Batch Gradient Norm after: 13.460096647410348
Epoch 119/10000, Prediction Accuracy = 38.366%, Loss = 3.211765193939209
Epoch: 119, Batch Gradient Norm: 14.765368008312368
Epoch: 119, Batch Gradient Norm after: 14.765368008312368
Epoch 120/10000, Prediction Accuracy = 38.44%, Loss = 3.2001357078552246
Epoch: 120, Batch Gradient Norm: 12.2585590878317
Epoch: 120, Batch Gradient Norm after: 12.2585590878317
Epoch 121/10000, Prediction Accuracy = 38.501999999999995%, Loss = 3.174616241455078
Epoch: 121, Batch Gradient Norm: 14.320066655229477
Epoch: 121, Batch Gradient Norm after: 14.320066655229477
Epoch 122/10000, Prediction Accuracy = 38.786%, Loss = 3.166053533554077
Epoch: 122, Batch Gradient Norm: 12.88458340727326
Epoch: 122, Batch Gradient Norm after: 12.88458340727326
Epoch 123/10000, Prediction Accuracy = 38.854%, Loss = 3.1452507972717285
Epoch: 123, Batch Gradient Norm: 15.703187423091116
Epoch: 123, Batch Gradient Norm after: 15.703187423091116
Epoch 124/10000, Prediction Accuracy = 38.816%, Loss = 3.1408469676971436
Epoch: 124, Batch Gradient Norm: 14.220027264612996
Epoch: 124, Batch Gradient Norm after: 14.220027264612996
Epoch 125/10000, Prediction Accuracy = 38.88000000000001%, Loss = 3.120177984237671
Epoch: 125, Batch Gradient Norm: 16.244121673734806
Epoch: 125, Batch Gradient Norm after: 16.244121673734806
Epoch 126/10000, Prediction Accuracy = 38.928%, Loss = 3.11392822265625
Epoch: 126, Batch Gradient Norm: 13.687932797747434
Epoch: 126, Batch Gradient Norm after: 13.687932797747434
Epoch 127/10000, Prediction Accuracy = 38.95%, Loss = 3.089833211898804
Epoch: 127, Batch Gradient Norm: 15.214970829589427
Epoch: 127, Batch Gradient Norm after: 15.214970829589427
Epoch 128/10000, Prediction Accuracy = 38.986000000000004%, Loss = 3.0818642139434815
Epoch: 128, Batch Gradient Norm: 12.96252428524983
Epoch: 128, Batch Gradient Norm after: 12.96252428524983
Epoch 129/10000, Prediction Accuracy = 39.036%, Loss = 3.0602391719818116
Epoch: 129, Batch Gradient Norm: 15.32506579143875
Epoch: 129, Batch Gradient Norm after: 15.32506579143875
Epoch 130/10000, Prediction Accuracy = 39.028%, Loss = 3.0557483196258546
Epoch: 130, Batch Gradient Norm: 13.847941442062575
Epoch: 130, Batch Gradient Norm after: 13.847941442062575
Epoch 131/10000, Prediction Accuracy = 39.128%, Loss = 3.03751220703125
Epoch: 131, Batch Gradient Norm: 16.429868388809965
Epoch: 131, Batch Gradient Norm after: 16.429868388809965
Epoch 132/10000, Prediction Accuracy = 39.124%, Loss = 3.0350346088409426
Epoch: 132, Batch Gradient Norm: 14.612345528011458
Epoch: 132, Batch Gradient Norm after: 14.612345528011458
Epoch 133/10000, Prediction Accuracy = 39.160000000000004%, Loss = 3.0158018589019777
Epoch: 133, Batch Gradient Norm: 16.539735625997757
Epoch: 133, Batch Gradient Norm after: 16.539735625997757
Epoch 134/10000, Prediction Accuracy = 39.135999999999996%, Loss = 3.01151647567749
Epoch: 134, Batch Gradient Norm: 14.107256563560801
Epoch: 134, Batch Gradient Norm after: 14.107256563560801
Epoch 135/10000, Prediction Accuracy = 39.2%, Loss = 2.990684986114502
Epoch: 135, Batch Gradient Norm: 15.877180273739084
Epoch: 135, Batch Gradient Norm after: 15.877180273739084
Epoch 136/10000, Prediction Accuracy = 39.202%, Loss = 2.9859899044036866
Epoch: 136, Batch Gradient Norm: 13.775981658689526
Epoch: 136, Batch Gradient Norm after: 13.775981658689526
Epoch 137/10000, Prediction Accuracy = 39.218%, Loss = 2.967315673828125
Epoch: 137, Batch Gradient Norm: 16.12449840652907
Epoch: 137, Batch Gradient Norm after: 16.12449840652907
Epoch 138/10000, Prediction Accuracy = 39.269999999999996%, Loss = 2.9651514530181884
Epoch: 138, Batch Gradient Norm: 14.462600777320704
Epoch: 138, Batch Gradient Norm after: 14.462600777320704
Epoch 139/10000, Prediction Accuracy = 39.272%, Loss = 2.9484370708465577
Epoch: 139, Batch Gradient Norm: 16.867629124798107
Epoch: 139, Batch Gradient Norm after: 16.867629124798107
Epoch 140/10000, Prediction Accuracy = 39.294%, Loss = 2.9473132133483886
Epoch: 140, Batch Gradient Norm: 14.946991959383917
Epoch: 140, Batch Gradient Norm after: 14.946991959383917
Epoch 141/10000, Prediction Accuracy = 39.33%, Loss = 2.9299145698547364
Epoch: 141, Batch Gradient Norm: 16.900905329027857
Epoch: 141, Batch Gradient Norm after: 16.900905329027857
Epoch 142/10000, Prediction Accuracy = 39.348%, Loss = 2.927655076980591
Epoch: 142, Batch Gradient Norm: 14.576004523704743
Epoch: 142, Batch Gradient Norm after: 14.576004523704743
Epoch 143/10000, Prediction Accuracy = 39.432%, Loss = 2.9093673706054686
Epoch: 143, Batch Gradient Norm: 16.45534222643659
Epoch: 143, Batch Gradient Norm after: 16.45534222643659
Epoch 144/10000, Prediction Accuracy = 39.403999999999996%, Loss = 2.90701642036438
Epoch: 144, Batch Gradient Norm: 14.40036997389246
Epoch: 144, Batch Gradient Norm after: 14.40036997389246
Epoch 145/10000, Prediction Accuracy = 39.466%, Loss = 2.890360927581787
Epoch: 145, Batch Gradient Norm: 16.7230605981165
Epoch: 145, Batch Gradient Norm after: 16.7230605981165
Epoch 146/10000, Prediction Accuracy = 39.476%, Loss = 2.8899956703186036
Epoch: 146, Batch Gradient Norm: 14.964143095900118
Epoch: 146, Batch Gradient Norm after: 14.964143095900118
Epoch 147/10000, Prediction Accuracy = 39.510000000000005%, Loss = 2.8747503757476807
Epoch: 147, Batch Gradient Norm: 17.216602950235572
Epoch: 147, Batch Gradient Norm after: 17.216602950235572
Epoch 148/10000, Prediction Accuracy = 39.546%, Loss = 2.874782752990723
Epoch: 148, Batch Gradient Norm: 15.184781233625989
Epoch: 148, Batch Gradient Norm after: 15.184781233625989
Epoch 149/10000, Prediction Accuracy = 39.598%, Loss = 2.8588518142700194
Epoch: 149, Batch Gradient Norm: 17.107682062291477
Epoch: 149, Batch Gradient Norm after: 17.107682062291477
Epoch 150/10000, Prediction Accuracy = 39.604%, Loss = 2.8580062866210936
Epoch: 150, Batch Gradient Norm: 14.893534345205932
Epoch: 150, Batch Gradient Norm after: 14.893534345205932
Epoch 151/10000, Prediction Accuracy = 39.636%, Loss = 2.841919994354248
Epoch: 151, Batch Gradient Norm: 16.864700661800953
Epoch: 151, Batch Gradient Norm after: 16.864700661800953
Epoch 152/10000, Prediction Accuracy = 39.652%, Loss = 2.8414270877838135
Epoch: 152, Batch Gradient Norm: 14.864981436468444
Epoch: 152, Batch Gradient Norm after: 14.864981436468444
Epoch 153/10000, Prediction Accuracy = 39.75%, Loss = 2.826604986190796
Epoch: 153, Batch Gradient Norm: 17.080791751117395
Epoch: 153, Batch Gradient Norm after: 17.080791751117395
Epoch 154/10000, Prediction Accuracy = 39.658%, Loss = 2.8273104190826417
Epoch: 154, Batch Gradient Norm: 15.259886925611147
Epoch: 154, Batch Gradient Norm after: 15.259886925611147
Epoch 155/10000, Prediction Accuracy = 39.739999999999995%, Loss = 2.8134077072143553
Epoch: 155, Batch Gradient Norm: 17.469589826807233
Epoch: 155, Batch Gradient Norm after: 17.469589826807233
Epoch 156/10000, Prediction Accuracy = 39.702%, Loss = 2.8145193099975585
Epoch: 156, Batch Gradient Norm: 15.517952547755378
Epoch: 156, Batch Gradient Norm after: 15.517952547755378
Epoch 157/10000, Prediction Accuracy = 39.79%, Loss = 2.8003482818603516
Epoch: 157, Batch Gradient Norm: 17.51643952185432
Epoch: 157, Batch Gradient Norm after: 17.51643952185432
Epoch 158/10000, Prediction Accuracy = 39.778000000000006%, Loss = 2.800994062423706
Epoch: 158, Batch Gradient Norm: 15.363751549125451
Epoch: 158, Batch Gradient Norm after: 15.363751549125451
Epoch 159/10000, Prediction Accuracy = 39.854%, Loss = 2.786456823348999
Epoch: 159, Batch Gradient Norm: 17.279532659374084
Epoch: 159, Batch Gradient Norm after: 17.279532659374084
Epoch 160/10000, Prediction Accuracy = 39.79600000000001%, Loss = 2.786892318725586
Epoch: 160, Batch Gradient Norm: 15.268348436513607
Epoch: 160, Batch Gradient Norm after: 15.268348436513607
Epoch 161/10000, Prediction Accuracy = 39.898%, Loss = 2.7732842445373533
Epoch: 161, Batch Gradient Norm: 17.3752899775262
Epoch: 161, Batch Gradient Norm after: 17.3752899775262
Epoch 162/10000, Prediction Accuracy = 39.836%, Loss = 2.7745625019073485
Epoch: 162, Batch Gradient Norm: 15.55432970507509
Epoch: 162, Batch Gradient Norm after: 15.55432970507509
Epoch 163/10000, Prediction Accuracy = 39.942%, Loss = 2.7618600845336916
Epoch: 163, Batch Gradient Norm: 17.70192503843909
Epoch: 163, Batch Gradient Norm after: 17.70192503843909
Epoch 164/10000, Prediction Accuracy = 39.912%, Loss = 2.7636256217956543
Epoch: 164, Batch Gradient Norm: 15.84586784348524
Epoch: 164, Batch Gradient Norm after: 15.84586784348524
Epoch 165/10000, Prediction Accuracy = 40.08%, Loss = 2.7508712291717528
Epoch: 165, Batch Gradient Norm: 17.80140401375028
Epoch: 165, Batch Gradient Norm after: 17.80140401375028
Epoch 166/10000, Prediction Accuracy = 39.96%, Loss = 2.7521515846252442
Epoch: 166, Batch Gradient Norm: 15.712752587375165
Epoch: 166, Batch Gradient Norm after: 15.712752587375165
Epoch 167/10000, Prediction Accuracy = 40.152%, Loss = 2.7388503551483154
Epoch: 167, Batch Gradient Norm: 17.457992480091395
Epoch: 167, Batch Gradient Norm after: 17.457992480091395
Epoch 168/10000, Prediction Accuracy = 40.00599999999999%, Loss = 2.739556407928467
Epoch: 168, Batch Gradient Norm: 15.435227336606424
Epoch: 168, Batch Gradient Norm after: 15.435227336606424
Epoch 169/10000, Prediction Accuracy = 40.166000000000004%, Loss = 2.726872444152832
Epoch: 169, Batch Gradient Norm: 17.374322214469306
Epoch: 169, Batch Gradient Norm after: 17.374322214469306
Epoch 170/10000, Prediction Accuracy = 40.025999999999996%, Loss = 2.7283145904541017
Epoch: 170, Batch Gradient Norm: 15.670583230821963
Epoch: 170, Batch Gradient Norm after: 15.670583230821963
Epoch 171/10000, Prediction Accuracy = 40.19%, Loss = 2.7169082164764404
Epoch: 171, Batch Gradient Norm: 17.774000092596182
Epoch: 171, Batch Gradient Norm after: 17.774000092596182
Epoch 172/10000, Prediction Accuracy = 40.058%, Loss = 2.7192142009735107
Epoch: 172, Batch Gradient Norm: 16.103678855930276
Epoch: 172, Batch Gradient Norm after: 16.103678855930276
Epoch 173/10000, Prediction Accuracy = 40.26%, Loss = 2.7079883575439454
Epoch: 173, Batch Gradient Norm: 17.9796019131679
Epoch: 173, Batch Gradient Norm after: 17.9796019131679
Epoch 174/10000, Prediction Accuracy = 40.132%, Loss = 2.709773540496826
Epoch: 174, Batch Gradient Norm: 16.042403094057622
Epoch: 174, Batch Gradient Norm after: 16.042403094057622
Epoch 175/10000, Prediction Accuracy = 40.306%, Loss = 2.6977612495422365
Epoch: 175, Batch Gradient Norm: 17.76418994892295
Epoch: 175, Batch Gradient Norm after: 17.76418994892295
Epoch 176/10000, Prediction Accuracy = 40.236000000000004%, Loss = 2.699074649810791
Epoch: 176, Batch Gradient Norm: 15.848387403193843
Epoch: 176, Batch Gradient Norm after: 15.848387403193843
Epoch 177/10000, Prediction Accuracy = 40.424%, Loss = 2.687394380569458
Epoch: 177, Batch Gradient Norm: 17.649122003751643
Epoch: 177, Batch Gradient Norm after: 17.649122003751643
Epoch 178/10000, Prediction Accuracy = 40.284000000000006%, Loss = 2.688976764678955
Epoch: 178, Batch Gradient Norm: 15.885235214682952
Epoch: 178, Batch Gradient Norm after: 15.885235214682952
Epoch 179/10000, Prediction Accuracy = 40.422000000000004%, Loss = 2.678056335449219
Epoch: 179, Batch Gradient Norm: 17.811170532845857
Epoch: 179, Batch Gradient Norm after: 17.811170532845857
Epoch 180/10000, Prediction Accuracy = 40.382000000000005%, Loss = 2.680231046676636
Epoch: 180, Batch Gradient Norm: 16.131800895559486
Epoch: 180, Batch Gradient Norm after: 16.131800895559486
Epoch 181/10000, Prediction Accuracy = 40.49400000000001%, Loss = 2.6697155475616454
Epoch: 181, Batch Gradient Norm: 18.018188666899473
Epoch: 181, Batch Gradient Norm after: 18.018188666899473
Epoch 182/10000, Prediction Accuracy = 40.394%, Loss = 2.6719491958618162
Epoch: 182, Batch Gradient Norm: 16.278766682266962
Epoch: 182, Batch Gradient Norm after: 16.278766682266962
Epoch 183/10000, Prediction Accuracy = 40.542%, Loss = 2.661312770843506
Epoch: 183, Batch Gradient Norm: 18.048970392254553
Epoch: 183, Batch Gradient Norm after: 18.048970392254553
Epoch 184/10000, Prediction Accuracy = 40.408%, Loss = 2.663251543045044
Epoch: 184, Batch Gradient Norm: 16.460838186993172
Epoch: 184, Batch Gradient Norm after: 16.460838186993172
Epoch 185/10000, Prediction Accuracy = 40.586%, Loss = 2.653266763687134
Epoch: 185, Batch Gradient Norm: 18.13524551887096
Epoch: 185, Batch Gradient Norm after: 18.13524551887096
Epoch 186/10000, Prediction Accuracy = 40.5%, Loss = 2.6550239086151124
Epoch: 186, Batch Gradient Norm: 16.24759875502664
Epoch: 186, Batch Gradient Norm after: 16.24759875502664
Epoch 187/10000, Prediction Accuracy = 40.642%, Loss = 2.644138526916504
Epoch: 187, Batch Gradient Norm: 17.991296329663292
Epoch: 187, Batch Gradient Norm after: 17.991296329663292
Epoch 188/10000, Prediction Accuracy = 40.501999999999995%, Loss = 2.6459353446960447
Epoch: 188, Batch Gradient Norm: 16.310652336693508
Epoch: 188, Batch Gradient Norm after: 16.310652336693508
Epoch 189/10000, Prediction Accuracy = 40.62%, Loss = 2.6358426570892335
Epoch: 189, Batch Gradient Norm: 18.1300082245723
Epoch: 189, Batch Gradient Norm after: 18.1300082245723
Epoch 190/10000, Prediction Accuracy = 40.618%, Loss = 2.638135051727295
Epoch: 190, Batch Gradient Norm: 16.423357807316698
Epoch: 190, Batch Gradient Norm after: 16.423357807316698
Epoch 191/10000, Prediction Accuracy = 40.69%, Loss = 2.6280161857604982
Epoch: 191, Batch Gradient Norm: 18.283688669039332
Epoch: 191, Batch Gradient Norm after: 18.283688669039332
Epoch 192/10000, Prediction Accuracy = 40.604%, Loss = 2.6304805278778076
Epoch: 192, Batch Gradient Norm: 16.61004625824651
Epoch: 192, Batch Gradient Norm after: 16.61004625824651
Epoch 193/10000, Prediction Accuracy = 40.715999999999994%, Loss = 2.6205132961273194
Epoch: 193, Batch Gradient Norm: 18.264735543692623
Epoch: 193, Batch Gradient Norm after: 18.264735543692623
Epoch 194/10000, Prediction Accuracy = 40.705999999999996%, Loss = 2.622461938858032
Epoch: 194, Batch Gradient Norm: 16.446960576114908
Epoch: 194, Batch Gradient Norm after: 16.446960576114908
Epoch 195/10000, Prediction Accuracy = 40.736%, Loss = 2.6121842861175537
Epoch: 195, Batch Gradient Norm: 18.073256153032997
Epoch: 195, Batch Gradient Norm after: 18.073256153032997
Epoch 196/10000, Prediction Accuracy = 40.806%, Loss = 2.614110517501831
Epoch: 196, Batch Gradient Norm: 16.391884806199748
Epoch: 196, Batch Gradient Norm after: 16.391884806199748
Epoch 197/10000, Prediction Accuracy = 40.768%, Loss = 2.6044469356536863
Epoch: 197, Batch Gradient Norm: 18.161078518941633
Epoch: 197, Batch Gradient Norm after: 18.161078518941633
Epoch 198/10000, Prediction Accuracy = 40.844%, Loss = 2.6069749355316163
Epoch: 198, Batch Gradient Norm: 16.68688572093574
Epoch: 198, Batch Gradient Norm after: 16.68688572093574
Epoch 199/10000, Prediction Accuracy = 40.822%, Loss = 2.5980433940887453
Epoch: 199, Batch Gradient Norm: 18.469575164635746
Epoch: 199, Batch Gradient Norm after: 18.469575164635746
Epoch 200/10000, Prediction Accuracy = 40.862%, Loss = 2.600756216049194
Epoch: 200, Batch Gradient Norm: 16.802974846874537
Epoch: 200, Batch Gradient Norm after: 16.802974846874537
Epoch 201/10000, Prediction Accuracy = 40.839999999999996%, Loss = 2.591231918334961
Epoch: 201, Batch Gradient Norm: 18.37633276072661
Epoch: 201, Batch Gradient Norm after: 18.37633276072661
Epoch 202/10000, Prediction Accuracy = 40.9%, Loss = 2.593281316757202
Epoch: 202, Batch Gradient Norm: 16.62528423282647
Epoch: 202, Batch Gradient Norm after: 16.62528423282647
Epoch 203/10000, Prediction Accuracy = 40.898%, Loss = 2.583610248565674
Epoch: 203, Batch Gradient Norm: 18.237038537806527
Epoch: 203, Batch Gradient Norm after: 18.237038537806527
Epoch 204/10000, Prediction Accuracy = 40.961999999999996%, Loss = 2.5857518672943116
Epoch: 204, Batch Gradient Norm: 16.62395371967334
Epoch: 204, Batch Gradient Norm after: 16.62395371967334
Epoch 205/10000, Prediction Accuracy = 40.934000000000005%, Loss = 2.5766777992248535
Epoch: 205, Batch Gradient Norm: 18.36749503701385
Epoch: 205, Batch Gradient Norm after: 18.36749503701385
Epoch 206/10000, Prediction Accuracy = 41.0%, Loss = 2.5793628692626953
Epoch: 206, Batch Gradient Norm: 16.856856851918003
Epoch: 206, Batch Gradient Norm after: 16.856856851918003
Epoch 207/10000, Prediction Accuracy = 41.00600000000001%, Loss = 2.5706695079803468
Epoch: 207, Batch Gradient Norm: 18.599752273246075
Epoch: 207, Batch Gradient Norm after: 18.599752273246075
Epoch 208/10000, Prediction Accuracy = 41.044%, Loss = 2.5734538555145265
Epoch: 208, Batch Gradient Norm: 17.004897421923797
Epoch: 208, Batch Gradient Norm after: 17.004897421923797
Epoch 209/10000, Prediction Accuracy = 41.044%, Loss = 2.5645215034484865
Epoch: 209, Batch Gradient Norm: 18.586373442511483
Epoch: 209, Batch Gradient Norm after: 18.586373442511483
Epoch 210/10000, Prediction Accuracy = 41.056%, Loss = 2.56683087348938
Epoch: 210, Batch Gradient Norm: 16.864830976897544
Epoch: 210, Batch Gradient Norm after: 16.864830976897544
Epoch 211/10000, Prediction Accuracy = 41.098%, Loss = 2.5575700759887696
Epoch: 211, Batch Gradient Norm: 18.42971073600767
Epoch: 211, Batch Gradient Norm after: 18.42971073600767
Epoch 212/10000, Prediction Accuracy = 41.089999999999996%, Loss = 2.5597932815551756
Epoch: 212, Batch Gradient Norm: 16.826016781318746
Epoch: 212, Batch Gradient Norm after: 16.826016781318746
Epoch 213/10000, Prediction Accuracy = 41.164%, Loss = 2.5510379791259767
Epoch: 213, Batch Gradient Norm: 18.496131908737997
Epoch: 213, Batch Gradient Norm after: 18.496131908737997
Epoch 214/10000, Prediction Accuracy = 41.148%, Loss = 2.5536585807800294
Epoch: 214, Batch Gradient Norm: 17.011287757875728
Epoch: 214, Batch Gradient Norm after: 17.011287757875728
Epoch 215/10000, Prediction Accuracy = 41.184000000000005%, Loss = 2.5453312397003174
Epoch: 215, Batch Gradient Norm: 18.698252859072877
Epoch: 215, Batch Gradient Norm after: 18.698252859072877
Epoch 216/10000, Prediction Accuracy = 41.17%, Loss = 2.5481611251831056
Epoch: 216, Batch Gradient Norm: 17.14231434596084
Epoch: 216, Batch Gradient Norm after: 17.14231434596084
Epoch 217/10000, Prediction Accuracy = 41.202%, Loss = 2.5396438598632813
Epoch: 217, Batch Gradient Norm: 18.730102120019556
Epoch: 217, Batch Gradient Norm after: 18.730102120019556
Epoch 218/10000, Prediction Accuracy = 41.212%, Loss = 2.542158317565918
Epoch: 218, Batch Gradient Norm: 17.078168856140884
Epoch: 218, Batch Gradient Norm after: 17.078168856140884
Epoch 219/10000, Prediction Accuracy = 41.262%, Loss = 2.533425760269165
Epoch: 219, Batch Gradient Norm: 18.625060064763087
Epoch: 219, Batch Gradient Norm after: 18.625060064763087
Epoch 220/10000, Prediction Accuracy = 41.288%, Loss = 2.5357919692993165
Epoch: 220, Batch Gradient Norm: 17.039338015980327
Epoch: 220, Batch Gradient Norm after: 17.039338015980327
Epoch 221/10000, Prediction Accuracy = 41.34400000000001%, Loss = 2.527387237548828
Epoch: 221, Batch Gradient Norm: 18.67916720849847
Epoch: 221, Batch Gradient Norm after: 18.67916720849847
Epoch 222/10000, Prediction Accuracy = 41.336%, Loss = 2.5300940990448
Epoch: 222, Batch Gradient Norm: 17.170197827868524
Epoch: 222, Batch Gradient Norm after: 17.170197827868524
Epoch 223/10000, Prediction Accuracy = 41.388%, Loss = 2.5219927787780763
Epoch: 223, Batch Gradient Norm: 18.81383445360834
Epoch: 223, Batch Gradient Norm after: 18.81383445360834
Epoch 224/10000, Prediction Accuracy = 41.394%, Loss = 2.5247984409332274
Epoch: 224, Batch Gradient Norm: 17.326360015169524
Epoch: 224, Batch Gradient Norm after: 17.326360015169524
Epoch 225/10000, Prediction Accuracy = 41.472%, Loss = 2.5167898654937746
Epoch: 225, Batch Gradient Norm: 18.912489097216636
Epoch: 225, Batch Gradient Norm after: 18.912489097216636
Epoch 226/10000, Prediction Accuracy = 41.428%, Loss = 2.5194506645202637
Epoch: 226, Batch Gradient Norm: 17.344385414788505
Epoch: 226, Batch Gradient Norm after: 17.344385414788505
Epoch 227/10000, Prediction Accuracy = 41.489999999999995%, Loss = 2.511208200454712
Epoch: 227, Batch Gradient Norm: 18.838916294338304
Epoch: 227, Batch Gradient Norm after: 18.838916294338304
Epoch 228/10000, Prediction Accuracy = 41.494%, Loss = 2.5135698318481445
Epoch: 228, Batch Gradient Norm: 17.257461440194486
Epoch: 228, Batch Gradient Norm after: 17.257461440194486
Epoch 229/10000, Prediction Accuracy = 41.558%, Loss = 2.505365419387817
Epoch: 229, Batch Gradient Norm: 18.775315429846078
Epoch: 229, Batch Gradient Norm after: 18.775315429846078
Epoch 230/10000, Prediction Accuracy = 41.522%, Loss = 2.507843351364136
Epoch: 230, Batch Gradient Norm: 17.276931034422557
Epoch: 230, Batch Gradient Norm after: 17.276931034422557
Epoch 231/10000, Prediction Accuracy = 41.646%, Loss = 2.4999813079833983
Epoch: 231, Batch Gradient Norm: 18.85497308340139
Epoch: 231, Batch Gradient Norm after: 18.85497308340139
Epoch 232/10000, Prediction Accuracy = 41.584%, Loss = 2.5027021884918215
Epoch: 232, Batch Gradient Norm: 17.39604219443751
Epoch: 232, Batch Gradient Norm after: 17.39604219443751
Epoch 233/10000, Prediction Accuracy = 41.7%, Loss = 2.495006227493286
Epoch: 233, Batch Gradient Norm: 18.96762016409814
Epoch: 233, Batch Gradient Norm after: 18.96762016409814
Epoch 234/10000, Prediction Accuracy = 41.629999999999995%, Loss = 2.4977624893188475
Epoch: 234, Batch Gradient Norm: 17.474134945205563
Epoch: 234, Batch Gradient Norm after: 17.474134945205563
Epoch 235/10000, Prediction Accuracy = 41.745999999999995%, Loss = 2.4899830341339113
Epoch: 235, Batch Gradient Norm: 19.101994812286343
Epoch: 235, Batch Gradient Norm after: 19.101994812286343
Epoch 236/10000, Prediction Accuracy = 41.718%, Loss = 2.492951440811157
Epoch: 236, Batch Gradient Norm: 17.683182603728536
Epoch: 236, Batch Gradient Norm after: 17.683182603728536
Epoch 237/10000, Prediction Accuracy = 41.786%, Loss = 2.48543496131897
Epoch: 237, Batch Gradient Norm: 19.022927054694183
Epoch: 237, Batch Gradient Norm after: 19.022927054694183
Epoch 238/10000, Prediction Accuracy = 41.754000000000005%, Loss = 2.4874727725982666
Epoch: 238, Batch Gradient Norm: 17.35217785452072
Epoch: 238, Batch Gradient Norm after: 17.35217785452072
Epoch 239/10000, Prediction Accuracy = 41.84%, Loss = 2.479230260848999
Epoch: 239, Batch Gradient Norm: 18.81101100615964
Epoch: 239, Batch Gradient Norm after: 18.81101100615964
Epoch 240/10000, Prediction Accuracy = 41.762%, Loss = 2.481587028503418
Epoch: 240, Batch Gradient Norm: 17.41253956199121
Epoch: 240, Batch Gradient Norm after: 17.41253956199121
Epoch 241/10000, Prediction Accuracy = 41.872%, Loss = 2.4743559837341307
Epoch: 241, Batch Gradient Norm: 18.98465768770941
Epoch: 241, Batch Gradient Norm after: 18.98465768770941
Epoch 242/10000, Prediction Accuracy = 41.83%, Loss = 2.4771414756774903
Epoch: 242, Batch Gradient Norm: 17.62773541493235
Epoch: 242, Batch Gradient Norm after: 17.62773541493235
Epoch 243/10000, Prediction Accuracy = 41.896%, Loss = 2.4700762271881103
Epoch: 243, Batch Gradient Norm: 19.207547953775915
Epoch: 243, Batch Gradient Norm after: 19.207547953775915
Epoch 244/10000, Prediction Accuracy = 41.844%, Loss = 2.4729651927948
Epoch: 244, Batch Gradient Norm: 17.754722005488652
Epoch: 244, Batch Gradient Norm after: 17.754722005488652
Epoch 245/10000, Prediction Accuracy = 41.942%, Loss = 2.4655877590179442
Epoch: 245, Batch Gradient Norm: 19.196864335917322
Epoch: 245, Batch Gradient Norm after: 19.196864335917322
Epoch 246/10000, Prediction Accuracy = 41.876%, Loss = 2.4680223941802977
Epoch: 246, Batch Gradient Norm: 17.64895166752065
Epoch: 246, Batch Gradient Norm after: 17.64895166752065
Epoch 247/10000, Prediction Accuracy = 41.98799999999999%, Loss = 2.4604076385498046
Epoch: 247, Batch Gradient Norm: 19.06887519373517
Epoch: 247, Batch Gradient Norm after: 19.06887519373517
Epoch 248/10000, Prediction Accuracy = 41.93%, Loss = 2.4627376556396485
Epoch: 248, Batch Gradient Norm: 17.588252560542973
Epoch: 248, Batch Gradient Norm after: 17.588252560542973
Epoch 249/10000, Prediction Accuracy = 42.025999999999996%, Loss = 2.455430841445923
Epoch: 249, Batch Gradient Norm: 19.115158968266552
Epoch: 249, Batch Gradient Norm after: 19.115158968266552
Epoch 250/10000, Prediction Accuracy = 41.980000000000004%, Loss = 2.4581238269805907
Epoch: 250, Batch Gradient Norm: 17.723523651983147
Epoch: 250, Batch Gradient Norm after: 17.723523651983147
Epoch 251/10000, Prediction Accuracy = 42.07%, Loss = 2.4511420249938967
Epoch: 251, Batch Gradient Norm: 19.28562182860317
Epoch: 251, Batch Gradient Norm after: 19.28562182860317
Epoch 252/10000, Prediction Accuracy = 42.0%, Loss = 2.4539996147155763
Epoch: 252, Batch Gradient Norm: 17.89652503357634
Epoch: 252, Batch Gradient Norm after: 17.89652503357634
Epoch 253/10000, Prediction Accuracy = 42.122%, Loss = 2.4470160961151124
Epoch: 253, Batch Gradient Norm: 19.348437409094412
Epoch: 253, Batch Gradient Norm after: 19.348437409094412
Epoch 254/10000, Prediction Accuracy = 42.062%, Loss = 2.449576711654663
Epoch: 254, Batch Gradient Norm: 17.92128158292999
Epoch: 254, Batch Gradient Norm after: 17.92128158292999
Epoch 255/10000, Prediction Accuracy = 42.152%, Loss = 2.442493200302124
Epoch: 255, Batch Gradient Norm: 19.342259848171867
Epoch: 255, Batch Gradient Norm after: 19.342259848171867
Epoch 256/10000, Prediction Accuracy = 42.138%, Loss = 2.4449664115905763
Epoch: 256, Batch Gradient Norm: 17.833760878689194
Epoch: 256, Batch Gradient Norm after: 17.833760878689194
Epoch 257/10000, Prediction Accuracy = 42.18%, Loss = 2.437660980224609
Epoch: 257, Batch Gradient Norm: 19.21658141860079
Epoch: 257, Batch Gradient Norm after: 19.21658141860079
Epoch 258/10000, Prediction Accuracy = 42.181999999999995%, Loss = 2.439991807937622
Epoch: 258, Batch Gradient Norm: 17.765084691538462
Epoch: 258, Batch Gradient Norm after: 17.765084691538462
Epoch 259/10000, Prediction Accuracy = 42.184000000000005%, Loss = 2.4329341411590577
Epoch: 259, Batch Gradient Norm: 19.29712552066656
Epoch: 259, Batch Gradient Norm after: 19.29712552066656
Epoch 260/10000, Prediction Accuracy = 42.204%, Loss = 2.435761022567749
Epoch: 260, Batch Gradient Norm: 17.967965023589155
Epoch: 260, Batch Gradient Norm after: 17.967965023589155
Epoch 261/10000, Prediction Accuracy = 42.227999999999994%, Loss = 2.4291449069976805
Epoch: 261, Batch Gradient Norm: 19.515163795073526
Epoch: 261, Batch Gradient Norm after: 19.515163795073526
Epoch 262/10000, Prediction Accuracy = 42.239999999999995%, Loss = 2.432085657119751
Epoch: 262, Batch Gradient Norm: 18.10835656844432
Epoch: 262, Batch Gradient Norm after: 18.10835656844432
Epoch 263/10000, Prediction Accuracy = 42.268%, Loss = 2.4251944541931154
Epoch: 263, Batch Gradient Norm: 19.53493724424502
Epoch: 263, Batch Gradient Norm after: 19.53493724424502
Epoch 264/10000, Prediction Accuracy = 42.27%, Loss = 2.427765893936157
Epoch: 264, Batch Gradient Norm: 18.013529267674567
Epoch: 264, Batch Gradient Norm after: 18.013529267674567
Epoch 265/10000, Prediction Accuracy = 42.333999999999996%, Loss = 2.420556163787842
Epoch: 265, Batch Gradient Norm: 19.402154701516125
Epoch: 265, Batch Gradient Norm after: 19.402154701516125
Epoch 266/10000, Prediction Accuracy = 42.315999999999995%, Loss = 2.4229732513427735
Epoch: 266, Batch Gradient Norm: 17.915625735654633
Epoch: 266, Batch Gradient Norm after: 17.915625735654633
Epoch 267/10000, Prediction Accuracy = 42.384%, Loss = 2.4159708499908445
Epoch: 267, Batch Gradient Norm: 19.40262129161264
Epoch: 267, Batch Gradient Norm after: 19.40262129161264
Epoch 268/10000, Prediction Accuracy = 42.374%, Loss = 2.418691349029541
Epoch: 268, Batch Gradient Norm: 18.04293606963242
Epoch: 268, Batch Gradient Norm after: 18.04293606963242
Epoch 269/10000, Prediction Accuracy = 42.414%, Loss = 2.4121368885040284
Epoch: 269, Batch Gradient Norm: 19.581666702685812
Epoch: 269, Batch Gradient Norm after: 19.581666702685812
Epoch 270/10000, Prediction Accuracy = 42.416%, Loss = 2.4150914669036867
Epoch: 270, Batch Gradient Norm: 18.21420388855698
Epoch: 270, Batch Gradient Norm after: 18.21420388855698
Epoch 271/10000, Prediction Accuracy = 42.45%, Loss = 2.4084951400756838
Epoch: 271, Batch Gradient Norm: 19.684957450521722
Epoch: 271, Batch Gradient Norm after: 19.684957450521722
Epoch 272/10000, Prediction Accuracy = 42.412%, Loss = 2.4112568378448485
Epoch: 272, Batch Gradient Norm: 18.201531255227792
Epoch: 272, Batch Gradient Norm after: 18.201531255227792
Epoch 273/10000, Prediction Accuracy = 42.486000000000004%, Loss = 2.4042999744415283
Epoch: 273, Batch Gradient Norm: 19.608778015529243
Epoch: 273, Batch Gradient Norm after: 19.608778015529243
Epoch 274/10000, Prediction Accuracy = 42.480000000000004%, Loss = 2.4068422317504883
Epoch: 274, Batch Gradient Norm: 18.127377440300265
Epoch: 274, Batch Gradient Norm after: 18.127377440300265
Epoch 275/10000, Prediction Accuracy = 42.536%, Loss = 2.3999313354492187
Epoch: 275, Batch Gradient Norm: 19.59311743776776
Epoch: 275, Batch Gradient Norm after: 19.59311743776776
Epoch 276/10000, Prediction Accuracy = 42.522%, Loss = 2.4026515007019045
Epoch: 276, Batch Gradient Norm: 18.186670009018513
Epoch: 276, Batch Gradient Norm after: 18.186670009018513
Epoch 277/10000, Prediction Accuracy = 42.56%, Loss = 2.396044683456421
Epoch: 277, Batch Gradient Norm: 19.697041014687017
Epoch: 277, Batch Gradient Norm after: 19.697041014687017
Epoch 278/10000, Prediction Accuracy = 42.556%, Loss = 2.398928928375244
Epoch: 278, Batch Gradient Norm: 18.328461429089167
Epoch: 278, Batch Gradient Norm after: 18.328461429089167
Epoch 279/10000, Prediction Accuracy = 42.617999999999995%, Loss = 2.392438364028931
Epoch: 279, Batch Gradient Norm: 19.80729248204549
Epoch: 279, Batch Gradient Norm after: 19.80729248204549
Epoch 280/10000, Prediction Accuracy = 42.59%, Loss = 2.3952520847320558
Epoch: 280, Batch Gradient Norm: 18.374310057950513
Epoch: 280, Batch Gradient Norm after: 18.374310057950513
Epoch 281/10000, Prediction Accuracy = 42.626000000000005%, Loss = 2.388570022583008
Epoch: 281, Batch Gradient Norm: 19.771089893908197
Epoch: 281, Batch Gradient Norm after: 19.771089893908197
Epoch 282/10000, Prediction Accuracy = 42.616%, Loss = 2.3910982608795166
Epoch: 282, Batch Gradient Norm: 18.299137155625523
Epoch: 282, Batch Gradient Norm after: 18.299137155625523
Epoch 283/10000, Prediction Accuracy = 42.684%, Loss = 2.384336996078491
Epoch: 283, Batch Gradient Norm: 19.709138680123605
Epoch: 283, Batch Gradient Norm after: 19.709138680123605
Epoch 284/10000, Prediction Accuracy = 42.64%, Loss = 2.386897563934326
Epoch: 284, Batch Gradient Norm: 18.31995507541568
Epoch: 284, Batch Gradient Norm after: 18.31995507541568
Epoch 285/10000, Prediction Accuracy = 42.704%, Loss = 2.3804440021514894
Epoch: 285, Batch Gradient Norm: 19.80562016543308
Epoch: 285, Batch Gradient Norm after: 19.80562016543308
Epoch 286/10000, Prediction Accuracy = 42.66%, Loss = 2.383289337158203
Epoch: 286, Batch Gradient Norm: 18.461459526381958
Epoch: 286, Batch Gradient Norm after: 18.461459526381958
Epoch 287/10000, Prediction Accuracy = 42.74%, Loss = 2.376985454559326
Epoch: 287, Batch Gradient Norm: 19.92289649321603
Epoch: 287, Batch Gradient Norm after: 19.92289649321603
Epoch 288/10000, Prediction Accuracy = 42.692%, Loss = 2.3797746658325196
Epoch: 288, Batch Gradient Norm: 18.530063137699138
Epoch: 288, Batch Gradient Norm after: 18.530063137699138
Epoch 289/10000, Prediction Accuracy = 42.774%, Loss = 2.3733089447021483
Epoch: 289, Batch Gradient Norm: 19.9185236680718
Epoch: 289, Batch Gradient Norm after: 19.9185236680718
Epoch 290/10000, Prediction Accuracy = 42.717999999999996%, Loss = 2.3758647441864014
Epoch: 290, Batch Gradient Norm: 18.475507241712275
Epoch: 290, Batch Gradient Norm after: 18.475507241712275
Epoch 291/10000, Prediction Accuracy = 42.806%, Loss = 2.369285154342651
Epoch: 291, Batch Gradient Norm: 19.879350967329366
Epoch: 291, Batch Gradient Norm after: 19.879350967329366
Epoch 292/10000, Prediction Accuracy = 42.754%, Loss = 2.371866798400879
Epoch: 292, Batch Gradient Norm: 18.47585793465497
Epoch: 292, Batch Gradient Norm after: 18.47585793465497
Epoch 293/10000, Prediction Accuracy = 42.816%, Loss = 2.365460824966431
Epoch: 293, Batch Gradient Norm: 19.921079980667024
Epoch: 293, Batch Gradient Norm after: 19.921079980667024
Epoch 294/10000, Prediction Accuracy = 42.738%, Loss = 2.368180179595947
Epoch: 294, Batch Gradient Norm: 18.57555455412478
Epoch: 294, Batch Gradient Norm after: 18.57555455412478
Epoch 295/10000, Prediction Accuracy = 42.826%, Loss = 2.3619832515716555
Epoch: 295, Batch Gradient Norm: 19.943370686632456
Epoch: 295, Batch Gradient Norm after: 19.93105542604915
Epoch 296/10000, Prediction Accuracy = 42.78%, Loss = 2.3644508361816405
Epoch: 296, Batch Gradient Norm: 18.596021797513394
Epoch: 296, Batch Gradient Norm after: 18.596021797513394
Epoch 297/10000, Prediction Accuracy = 42.85%, Loss = 2.3582572460174562
Epoch: 297, Batch Gradient Norm: 19.950911303025194
Epoch: 297, Batch Gradient Norm after: 19.933019834853592
Epoch 298/10000, Prediction Accuracy = 42.812%, Loss = 2.360691690444946
Epoch: 298, Batch Gradient Norm: 18.651538560665344
Epoch: 298, Batch Gradient Norm after: 18.651538560665344
Epoch 299/10000, Prediction Accuracy = 42.866%, Loss = 2.3546874046325685
Epoch: 299, Batch Gradient Norm: 19.938339859296644
Epoch: 299, Batch Gradient Norm after: 19.906599597888164
Epoch 300/10000, Prediction Accuracy = 42.836%, Loss = 2.3569037914276123
Epoch: 300, Batch Gradient Norm: 18.6857420784422
Epoch: 300, Batch Gradient Norm after: 18.6857420784422
Epoch 301/10000, Prediction Accuracy = 42.912%, Loss = 2.3510782718658447
Epoch: 301, Batch Gradient Norm: 19.920288703107182
Epoch: 301, Batch Gradient Norm after: 19.87032117975745
Epoch 302/10000, Prediction Accuracy = 42.864%, Loss = 2.3531195163726806
Epoch: 302, Batch Gradient Norm: 18.743601781843612
Epoch: 302, Batch Gradient Norm after: 18.743601781843612
Epoch 303/10000, Prediction Accuracy = 42.94199999999999%, Loss = 2.3475555896759035
Epoch: 303, Batch Gradient Norm: 19.921384598495994
Epoch: 303, Batch Gradient Norm after: 19.85050287702278
Epoch 304/10000, Prediction Accuracy = 42.902%, Loss = 2.3494519233703612
Epoch: 304, Batch Gradient Norm: 18.80778443338919
Epoch: 304, Batch Gradient Norm after: 18.80778443338919
Epoch 305/10000, Prediction Accuracy = 43.022000000000006%, Loss = 2.344113492965698
Epoch: 305, Batch Gradient Norm: 19.912021491228714
Epoch: 305, Batch Gradient Norm after: 19.81747547080835
Epoch 306/10000, Prediction Accuracy = 42.936%, Loss = 2.3457741737365723
Epoch: 306, Batch Gradient Norm: 18.86905762943991
Epoch: 306, Batch Gradient Norm after: 18.86905762943991
Epoch 307/10000, Prediction Accuracy = 43.048%, Loss = 2.3406882762908934
Epoch: 307, Batch Gradient Norm: 19.880460231939367
Epoch: 307, Batch Gradient Norm after: 19.7558620153614
Epoch 308/10000, Prediction Accuracy = 42.95%, Loss = 2.3420547008514405
Epoch: 308, Batch Gradient Norm: 18.929156225883357
Epoch: 308, Batch Gradient Norm after: 18.929156225883357
Epoch 309/10000, Prediction Accuracy = 43.056%, Loss = 2.3372783184051515
Epoch: 309, Batch Gradient Norm: 19.859768838196235
Epoch: 309, Batch Gradient Norm after: 19.70131621382481
Epoch 310/10000, Prediction Accuracy = 42.962%, Loss = 2.3383884906768797
Epoch: 310, Batch Gradient Norm: 19.03587841514412
Epoch: 310, Batch Gradient Norm after: 19.03587841514412
Epoch 311/10000, Prediction Accuracy = 43.064%, Loss = 2.3340456962585447
Epoch: 311, Batch Gradient Norm: 19.82694838180588
Epoch: 311, Batch Gradient Norm after: 19.62237967218768
Epoch 312/10000, Prediction Accuracy = 42.983999999999995%, Loss = 2.334685182571411
Epoch: 312, Batch Gradient Norm: 19.12258659225952
Epoch: 312, Batch Gradient Norm after: 19.12258659225952
Epoch 313/10000, Prediction Accuracy = 43.077999999999996%, Loss = 2.3307418823242188
Epoch: 313, Batch Gradient Norm: 19.77927412222477
Epoch: 313, Batch Gradient Norm after: 19.52862518424749
Epoch 314/10000, Prediction Accuracy = 43.046%, Loss = 2.330986499786377
Epoch: 314, Batch Gradient Norm: 19.169420802378422
Epoch: 314, Batch Gradient Norm after: 19.169420802378422
Epoch 315/10000, Prediction Accuracy = 43.08200000000001%, Loss = 2.327363872528076
Epoch: 315, Batch Gradient Norm: 19.758498615774577
Epoch: 315, Batch Gradient Norm after: 19.468812405452955
Epoch 316/10000, Prediction Accuracy = 43.068%, Loss = 2.327393102645874
Epoch: 316, Batch Gradient Norm: 19.2982550243074
Epoch: 316, Batch Gradient Norm after: 19.25364545565972
Epoch 317/10000, Prediction Accuracy = 43.122%, Loss = 2.324262523651123
Epoch: 317, Batch Gradient Norm: 19.897564874310408
Epoch: 317, Batch Gradient Norm after: 19.643730950566002
Epoch 318/10000, Prediction Accuracy = 43.072%, Loss = 2.324349594116211
Epoch: 318, Batch Gradient Norm: 19.657755391582914
Epoch: 318, Batch Gradient Norm after: 19.525561603276426
Epoch 319/10000, Prediction Accuracy = 43.14%, Loss = 2.3219356536865234
Epoch: 319, Batch Gradient Norm: 20.072441531408675
Epoch: 319, Batch Gradient Norm after: 19.90811580705306
Epoch 320/10000, Prediction Accuracy = 43.092%, Loss = 2.32144455909729
Epoch: 320, Batch Gradient Norm: 19.898267343611405
Epoch: 320, Batch Gradient Norm after: 19.729354007093754
Epoch 321/10000, Prediction Accuracy = 43.160000000000004%, Loss = 2.319256067276001
Epoch: 321, Batch Gradient Norm: 20.08409092616282
Epoch: 321, Batch Gradient Norm after: 19.96804387151112
Epoch 322/10000, Prediction Accuracy = 43.076%, Loss = 2.3179945945739746
Epoch: 322, Batch Gradient Norm: 19.779566710697335
Epoch: 322, Batch Gradient Norm after: 19.656889437543327
Epoch 323/10000, Prediction Accuracy = 43.19%, Loss = 2.3153872013092043
Epoch: 323, Batch Gradient Norm: 19.936134249177535
Epoch: 323, Batch Gradient Norm after: 19.781666108121502
Epoch 324/10000, Prediction Accuracy = 43.088%, Loss = 2.313989591598511
Epoch: 324, Batch Gradient Norm: 19.544722771298165
Epoch: 324, Batch Gradient Norm after: 19.470945971500342
Epoch 325/10000, Prediction Accuracy = 43.184000000000005%, Loss = 2.3110535621643065
Epoch: 325, Batch Gradient Norm: 19.861612855954082
Epoch: 325, Batch Gradient Norm after: 19.65669212810487
Epoch 326/10000, Prediction Accuracy = 43.098%, Loss = 2.310176658630371
Epoch: 326, Batch Gradient Norm: 19.563561942241407
Epoch: 326, Batch Gradient Norm after: 19.47072916745704
Epoch 327/10000, Prediction Accuracy = 43.21399999999999%, Loss = 2.307529401779175
Epoch: 327, Batch Gradient Norm: 19.956225381532285
Epoch: 327, Batch Gradient Norm after: 19.75698236421117
Epoch 328/10000, Prediction Accuracy = 43.14%, Loss = 2.3069405555725098
Epoch: 328, Batch Gradient Norm: 19.876479455784644
Epoch: 328, Batch Gradient Norm after: 19.706533949312277
Epoch 329/10000, Prediction Accuracy = 43.254%, Loss = 2.305051326751709
Epoch: 329, Batch Gradient Norm: 20.0989961835748
Epoch: 329, Batch Gradient Norm after: 19.970680961591167
Epoch 330/10000, Prediction Accuracy = 43.181999999999995%, Loss = 2.3039269924163817
Epoch: 330, Batch Gradient Norm: 20.09770130031468
Epoch: 330, Batch Gradient Norm after: 19.88659464728724
Epoch 331/10000, Prediction Accuracy = 43.254000000000005%, Loss = 2.3023120880126955
Epoch: 331, Batch Gradient Norm: 20.151012899919557
Epoch: 331, Batch Gradient Norm after: 20.07367156158933
Epoch 332/10000, Prediction Accuracy = 43.164%, Loss = 2.300652837753296
Epoch: 332, Batch Gradient Norm: 20.098446421133875
Epoch: 332, Batch Gradient Norm after: 19.903180158494354
Epoch 333/10000, Prediction Accuracy = 43.29599999999999%, Loss = 2.298888921737671
Epoch: 333, Batch Gradient Norm: 20.08985389422205
Epoch: 333, Batch Gradient Norm after: 20.00706788311196
Epoch 334/10000, Prediction Accuracy = 43.164%, Loss = 2.29702467918396
Epoch: 334, Batch Gradient Norm: 19.960795957950232
Epoch: 334, Batch Gradient Norm after: 19.80171341632997
Epoch 335/10000, Prediction Accuracy = 43.338%, Loss = 2.295026397705078
Epoch: 335, Batch Gradient Norm: 20.011492225929807
Epoch: 335, Batch Gradient Norm after: 19.896031888910795
Epoch 336/10000, Prediction Accuracy = 43.214%, Loss = 2.2933637619018556
Epoch: 336, Batch Gradient Norm: 19.8828775916572
Epoch: 336, Batch Gradient Norm after: 19.735844843580857
Epoch 337/10000, Prediction Accuracy = 43.372%, Loss = 2.291380739212036
Epoch: 337, Batch Gradient Norm: 20.023306811605494
Epoch: 337, Batch Gradient Norm after: 19.89228881850863
Epoch 338/10000, Prediction Accuracy = 43.25599999999999%, Loss = 2.290030670166016
Epoch: 338, Batch Gradient Norm: 20.02571643510677
Epoch: 338, Batch Gradient Norm after: 19.835246785496842
Epoch 339/10000, Prediction Accuracy = 43.394000000000005%, Loss = 2.2884613990783693
Epoch: 339, Batch Gradient Norm: 20.129299378885058
Epoch: 339, Batch Gradient Norm after: 20.033485397703235
Epoch 340/10000, Prediction Accuracy = 43.296%, Loss = 2.2869970321655275
Epoch: 340, Batch Gradient Norm: 20.354263471714887
Epoch: 340, Batch Gradient Norm after: 20.050462956684093
Epoch 341/10000, Prediction Accuracy = 43.422%, Loss = 2.2861897468566896
Epoch: 341, Batch Gradient Norm: 20.285548632797735
Epoch: 341, Batch Gradient Norm after: 20.285548632797735
Epoch 342/10000, Prediction Accuracy = 43.33%, Loss = 2.2841552257537843
Epoch: 342, Batch Gradient Norm: 20.484845345140837
Epoch: 342, Batch Gradient Norm after: 20.20836221576118
Epoch 343/10000, Prediction Accuracy = 43.465999999999994%, Loss = 2.2833110809326174
Epoch: 343, Batch Gradient Norm: 20.24448749735607
Epoch: 343, Batch Gradient Norm after: 20.24448749735607
Epoch 344/10000, Prediction Accuracy = 43.378%, Loss = 2.2806952953338624
Epoch: 344, Batch Gradient Norm: 20.28872860549075
Epoch: 344, Batch Gradient Norm after: 20.06405042794136
Epoch 345/10000, Prediction Accuracy = 43.495999999999995%, Loss = 2.2793266773223877
Epoch: 345, Batch Gradient Norm: 20.136920548325875
Epoch: 345, Batch Gradient Norm after: 20.097488787030517
Epoch 346/10000, Prediction Accuracy = 43.43000000000001%, Loss = 2.2770450115203857
Epoch: 346, Batch Gradient Norm: 20.121748195609435
Epoch: 346, Batch Gradient Norm after: 19.92629449437909
Epoch 347/10000, Prediction Accuracy = 43.477999999999994%, Loss = 2.275478458404541
Epoch: 347, Batch Gradient Norm: 20.112525219593547
Epoch: 347, Batch Gradient Norm after: 20.03946596498702
Epoch 348/10000, Prediction Accuracy = 43.474000000000004%, Loss = 2.2736722946166994
Epoch: 348, Batch Gradient Norm: 20.21673967349481
Epoch: 348, Batch Gradient Norm after: 19.984148135517568
Epoch 349/10000, Prediction Accuracy = 43.498000000000005%, Loss = 2.2725051403045655
Epoch: 349, Batch Gradient Norm: 20.23700659133242
Epoch: 349, Batch Gradient Norm after: 20.19499673886229
Epoch 350/10000, Prediction Accuracy = 43.494%, Loss = 2.270803165435791
Epoch: 350, Batch Gradient Norm: 20.49187610495128
Epoch: 350, Batch Gradient Norm after: 20.196238314184765
Epoch 351/10000, Prediction Accuracy = 43.507999999999996%, Loss = 2.270137643814087
Epoch: 351, Batch Gradient Norm: 20.25801811664014
Epoch: 351, Batch Gradient Norm after: 20.25801811664014
Epoch 352/10000, Prediction Accuracy = 43.510000000000005%, Loss = 2.267577600479126
Epoch: 352, Batch Gradient Norm: 20.55458094228786
Epoch: 352, Batch Gradient Norm after: 20.263567813627226
Epoch 353/10000, Prediction Accuracy = 43.54%, Loss = 2.267083835601807
Epoch: 353, Batch Gradient Norm: 20.32345259246168
Epoch: 353, Batch Gradient Norm after: 20.323049169919205
Epoch 354/10000, Prediction Accuracy = 43.522000000000006%, Loss = 2.26453218460083
Epoch: 354, Batch Gradient Norm: 20.57487017842181
Epoch: 354, Batch Gradient Norm after: 20.290148200615217
Epoch 355/10000, Prediction Accuracy = 43.536%, Loss = 2.2638949394226073
Epoch: 355, Batch Gradient Norm: 20.287880527019777
Epoch: 355, Batch Gradient Norm after: 20.27915312463594
Epoch 356/10000, Prediction Accuracy = 43.534000000000006%, Loss = 2.261140966415405
Epoch: 356, Batch Gradient Norm: 20.550610677444112
Epoch: 356, Batch Gradient Norm after: 20.266804837141674
Epoch 357/10000, Prediction Accuracy = 43.550000000000004%, Loss = 2.2605292797088623
Epoch: 357, Batch Gradient Norm: 20.303761772166713
Epoch: 357, Batch Gradient Norm after: 20.298548161737063
Epoch 358/10000, Prediction Accuracy = 43.54%, Loss = 2.2579397201538085
Epoch: 358, Batch Gradient Norm: 20.609431574878315
Epoch: 358, Batch Gradient Norm after: 20.33192867752851
Epoch 359/10000, Prediction Accuracy = 43.574%, Loss = 2.2574657917022707
Epoch: 359, Batch Gradient Norm: 20.23236507341884
Epoch: 359, Batch Gradient Norm after: 20.195000056928414
Epoch 360/10000, Prediction Accuracy = 43.571999999999996%, Loss = 2.254407024383545
Epoch: 360, Batch Gradient Norm: 20.56571125400603
Epoch: 360, Batch Gradient Norm after: 20.278438030797794
Epoch 361/10000, Prediction Accuracy = 43.6%, Loss = 2.2539975166320803
Epoch: 361, Batch Gradient Norm: 20.320736018437152
Epoch: 361, Batch Gradient Norm after: 20.296648509554053
Epoch 362/10000, Prediction Accuracy = 43.586%, Loss = 2.2513736724853515
Epoch: 362, Batch Gradient Norm: 20.68837089481057
Epoch: 362, Batch Gradient Norm after: 20.402257082692174
Epoch 363/10000, Prediction Accuracy = 43.622%, Loss = 2.251111078262329
Epoch: 363, Batch Gradient Norm: 20.21142159383938
Epoch: 363, Batch Gradient Norm after: 20.135378619680722
Epoch 364/10000, Prediction Accuracy = 43.638%, Loss = 2.2477673530578612
Epoch: 364, Batch Gradient Norm: 20.645431567024715
Epoch: 364, Batch Gradient Norm after: 20.34974035196984
Epoch 365/10000, Prediction Accuracy = 43.65%, Loss = 2.2477195262908936
Epoch: 365, Batch Gradient Norm: 20.311865137393248
Epoch: 365, Batch Gradient Norm after: 20.24643598709831
Epoch 366/10000, Prediction Accuracy = 43.642%, Loss = 2.2448861598968506
Epoch: 366, Batch Gradient Norm: 20.79581415732715
Epoch: 366, Batch Gradient Norm after: 20.500932609726924
Epoch 367/10000, Prediction Accuracy = 43.684%, Loss = 2.245023775100708
Epoch: 367, Batch Gradient Norm: 20.179591732880475
Epoch: 367, Batch Gradient Norm after: 20.046964418044187
Epoch 368/10000, Prediction Accuracy = 43.666%, Loss = 2.241273212432861
Epoch: 368, Batch Gradient Norm: 20.721624134595096
Epoch: 368, Batch Gradient Norm after: 20.414721933273846
Epoch 369/10000, Prediction Accuracy = 43.706%, Loss = 2.2415931701660154
Epoch: 369, Batch Gradient Norm: 20.30418944775325
Epoch: 369, Batch Gradient Norm after: 20.18604512939315
Epoch 370/10000, Prediction Accuracy = 43.709999999999994%, Loss = 2.2384985446929933
Epoch: 370, Batch Gradient Norm: 20.906182208585026
Epoch: 370, Batch Gradient Norm after: 20.59940062335587
Epoch 371/10000, Prediction Accuracy = 43.718%, Loss = 2.2390488147735597
Epoch: 371, Batch Gradient Norm: 20.125771554417735
Epoch: 371, Batch Gradient Norm after: 19.910464286383846
Epoch 372/10000, Prediction Accuracy = 43.739999999999995%, Loss = 2.2348005771636963
Epoch: 372, Batch Gradient Norm: 20.802294569993624
Epoch: 372, Batch Gradient Norm after: 20.47129945989529
Epoch 373/10000, Prediction Accuracy = 43.732%, Loss = 2.2355541706085207
Epoch: 373, Batch Gradient Norm: 20.31414446256303
Epoch: 373, Batch Gradient Norm after: 20.13644865927437
Epoch 374/10000, Prediction Accuracy = 43.769999999999996%, Loss = 2.2322754859924316
Epoch: 374, Batch Gradient Norm: 21.08243705503218
Epoch: 374, Batch Gradient Norm after: 20.755488318650464
Epoch 375/10000, Prediction Accuracy = 43.75%, Loss = 2.2333632469177247
Epoch: 375, Batch Gradient Norm: 20.017282123902202
Epoch: 375, Batch Gradient Norm after: 19.701640796531468
Epoch 376/10000, Prediction Accuracy = 43.782%, Loss = 2.22822642326355
Epoch: 376, Batch Gradient Norm: 20.861521207675388
Epoch: 376, Batch Gradient Norm after: 20.513183256162677
Epoch 377/10000, Prediction Accuracy = 43.79%, Loss = 2.229513931274414
Epoch: 377, Batch Gradient Norm: 20.341596905469537
Epoch: 377, Batch Gradient Norm after: 20.09754640529987
Epoch 378/10000, Prediction Accuracy = 43.806%, Loss = 2.2261356830596926
Epoch: 378, Batch Gradient Norm: 21.288590021902134
Epoch: 378, Batch Gradient Norm after: 20.843032383614116
Epoch 379/10000, Prediction Accuracy = 43.818%, Loss = 2.2278573513031006
Epoch: 379, Batch Gradient Norm: 20.40762203014352
Epoch: 379, Batch Gradient Norm after: 20.180143700603008
Epoch 380/10000, Prediction Accuracy = 43.846000000000004%, Loss = 2.2232481956481935
Epoch: 380, Batch Gradient Norm: 21.49772421678849
Epoch: 380, Batch Gradient Norm after: 20.980229256640893
Epoch 381/10000, Prediction Accuracy = 43.824%, Loss = 2.225481557846069
Epoch: 381, Batch Gradient Norm: 20.4390891488392
Epoch: 381, Batch Gradient Norm after: 20.17052023670314
Epoch 382/10000, Prediction Accuracy = 43.884%, Loss = 2.2202858448028566
Epoch: 382, Batch Gradient Norm: 21.508147768414027
Epoch: 382, Batch Gradient Norm after: 21.00451741303577
Epoch 383/10000, Prediction Accuracy = 43.854%, Loss = 2.2224666118621825
Epoch: 383, Batch Gradient Norm: 20.46486810768978
Epoch: 383, Batch Gradient Norm after: 20.225553644669315
Epoch 384/10000, Prediction Accuracy = 43.932%, Loss = 2.2173004150390625
Epoch: 384, Batch Gradient Norm: 21.511437819382603
Epoch: 384, Batch Gradient Norm after: 21.009011746696526
Epoch 385/10000, Prediction Accuracy = 43.862%, Loss = 2.21942400932312
Epoch: 385, Batch Gradient Norm: 20.46732935692635
Epoch: 385, Batch Gradient Norm after: 20.232395626775578
Epoch 386/10000, Prediction Accuracy = 43.948%, Loss = 2.214283895492554
Epoch: 386, Batch Gradient Norm: 21.5211045118779
Epoch: 386, Batch Gradient Norm after: 21.018535363991127
Epoch 387/10000, Prediction Accuracy = 43.886%, Loss = 2.2164234161376952
Epoch: 387, Batch Gradient Norm: 20.461321227141376
Epoch: 387, Batch Gradient Norm after: 20.219103088018244
Epoch 388/10000, Prediction Accuracy = 43.946000000000005%, Loss = 2.2112498760223387
Epoch: 388, Batch Gradient Norm: 21.531268602341697
Epoch: 388, Batch Gradient Norm after: 21.026031713749923
Epoch 389/10000, Prediction Accuracy = 43.92%, Loss = 2.2134464263916014
Epoch: 389, Batch Gradient Norm: 20.46923836337784
Epoch: 389, Batch Gradient Norm after: 20.219642950923635
Epoch 390/10000, Prediction Accuracy = 43.96%, Loss = 2.208275556564331
Epoch: 390, Batch Gradient Norm: 21.57713933816995
Epoch: 390, Batch Gradient Norm after: 21.062602331665182
Epoch 391/10000, Prediction Accuracy = 43.946%, Loss = 2.210582447052002
Epoch: 391, Batch Gradient Norm: 20.44633640146282
Epoch: 391, Batch Gradient Norm after: 20.174074146470097
Epoch 392/10000, Prediction Accuracy = 43.952%, Loss = 2.2052063941955566
Epoch: 392, Batch Gradient Norm: 21.591208185322195
Epoch: 392, Batch Gradient Norm after: 21.07681781732919
Epoch 393/10000, Prediction Accuracy = 43.977999999999994%, Loss = 2.2076253414154055
Epoch: 393, Batch Gradient Norm: 20.46737115336348
Epoch: 393, Batch Gradient Norm after: 20.17875245231581
Epoch 394/10000, Prediction Accuracy = 43.986000000000004%, Loss = 2.202289342880249
Epoch: 394, Batch Gradient Norm: 21.661192515396575
Epoch: 394, Batch Gradient Norm after: 21.143891990388084
Epoch 395/10000, Prediction Accuracy = 44.007999999999996%, Loss = 2.2048845291137695
Epoch: 395, Batch Gradient Norm: 20.423287737046103
Epoch: 395, Batch Gradient Norm after: 20.094260901547106
Epoch 396/10000, Prediction Accuracy = 44.01199999999999%, Loss = 2.1991862773895265
Epoch: 396, Batch Gradient Norm: 21.668939832688366
Epoch: 396, Batch Gradient Norm after: 21.1421079550653
Epoch 397/10000, Prediction Accuracy = 44.02400000000001%, Loss = 2.2019416809082033
Epoch: 397, Batch Gradient Norm: 20.450531572365925
Epoch: 397, Batch Gradient Norm after: 20.10654408354481
Epoch 398/10000, Prediction Accuracy = 44.00599999999999%, Loss = 2.196329355239868
Epoch: 398, Batch Gradient Norm: 21.764909824472067
Epoch: 398, Batch Gradient Norm after: 21.215911215401302
Epoch 399/10000, Prediction Accuracy = 44.038%, Loss = 2.199316644668579
Epoch: 399, Batch Gradient Norm: 20.461966542712688
Epoch: 399, Batch Gradient Norm after: 20.102182517766487
Epoch 400/10000, Prediction Accuracy = 44.01199999999999%, Loss = 2.1934401512146
Epoch: 400, Batch Gradient Norm: 21.83706566542295
Epoch: 400, Batch Gradient Norm after: 21.249089543546805
Epoch 401/10000, Prediction Accuracy = 44.042%, Loss = 2.196619987487793
Epoch: 401, Batch Gradient Norm: 20.570030504400254
Epoch: 401, Batch Gradient Norm after: 20.237620561478984
Epoch 402/10000, Prediction Accuracy = 44.016%, Loss = 2.1908477306365968
Epoch: 402, Batch Gradient Norm: 22.00580645012441
Epoch: 402, Batch Gradient Norm after: 21.349166223762364
Epoch 403/10000, Prediction Accuracy = 44.056%, Loss = 2.194251298904419
Epoch: 403, Batch Gradient Norm: 20.758675575297815
Epoch: 403, Batch Gradient Norm after: 20.497823230755067
Epoch 404/10000, Prediction Accuracy = 44.041999999999994%, Loss = 2.1885284900665285
Epoch: 404, Batch Gradient Norm: 22.2184143177138
Epoch: 404, Batch Gradient Norm after: 21.48944434707755
Epoch 405/10000, Prediction Accuracy = 44.054%, Loss = 2.192048978805542
Epoch: 405, Batch Gradient Norm: 20.940737792141093
Epoch: 405, Batch Gradient Norm after: 20.767483087079697
Epoch 406/10000, Prediction Accuracy = 44.05800000000001%, Loss = 2.186191272735596
Epoch: 406, Batch Gradient Norm: 22.351743716904185
Epoch: 406, Batch Gradient Norm after: 21.587714439763996
Epoch 407/10000, Prediction Accuracy = 44.044%, Loss = 2.1896108627319335
Epoch: 407, Batch Gradient Norm: 20.992853823111638
Epoch: 407, Batch Gradient Norm after: 20.876643746122824
Epoch 408/10000, Prediction Accuracy = 44.06400000000001%, Loss = 2.1834681034088135
Epoch: 408, Batch Gradient Norm: 22.331019331538478
Epoch: 408, Batch Gradient Norm after: 21.591520415776145
Epoch 409/10000, Prediction Accuracy = 44.074%, Loss = 2.1866495609283447
Epoch: 409, Batch Gradient Norm: 20.900662724519975
Epoch: 409, Batch Gradient Norm after: 20.76753549584424
Epoch 410/10000, Prediction Accuracy = 44.04600000000001%, Loss = 2.180293846130371
Epoch: 410, Batch Gradient Norm: 22.173654592970088
Epoch: 410, Batch Gradient Norm after: 21.497597385235782
Epoch 411/10000, Prediction Accuracy = 44.08%, Loss = 2.1832318782806395
Epoch: 411, Batch Gradient Norm: 20.696609124019485
Epoch: 411, Batch Gradient Norm after: 20.51600764602383
Epoch 412/10000, Prediction Accuracy = 44.064%, Loss = 2.176780939102173
Epoch: 412, Batch Gradient Norm: 22.007332651766458
Epoch: 412, Batch Gradient Norm after: 21.379111349328724
Epoch 413/10000, Prediction Accuracy = 44.11%, Loss = 2.1798017978668214
Epoch: 413, Batch Gradient Norm: 20.587249195640346
Epoch: 413, Batch Gradient Norm after: 20.336680668682458
Epoch 414/10000, Prediction Accuracy = 44.082%, Loss = 2.1735772609710695
Epoch: 414, Batch Gradient Norm: 21.973982984433633
Epoch: 414, Batch Gradient Norm after: 21.336428953119622
Epoch 415/10000, Prediction Accuracy = 44.122%, Loss = 2.1768136501312254
Epoch: 415, Batch Gradient Norm: 20.66061668994345
Epoch: 415, Batch Gradient Norm after: 20.39754848243829
Epoch 416/10000, Prediction Accuracy = 44.089999999999996%, Loss = 2.170955514907837
Epoch: 416, Batch Gradient Norm: 22.154956995612295
Epoch: 416, Batch Gradient Norm after: 21.43299469900319
Epoch 417/10000, Prediction Accuracy = 44.12800000000001%, Loss = 2.1745466232299804
Epoch: 417, Batch Gradient Norm: 20.917654153050357
Epoch: 417, Batch Gradient Norm after: 20.72457096142373
Epoch 418/10000, Prediction Accuracy = 44.108%, Loss = 2.1689172267913817
Epoch: 418, Batch Gradient Norm: 22.492656850424975
Epoch: 418, Batch Gradient Norm after: 21.6433511262701
Epoch 419/10000, Prediction Accuracy = 44.118%, Loss = 2.1728372097015383
Epoch: 419, Batch Gradient Norm: 21.244385793359292
Epoch: 419, Batch Gradient Norm after: 21.1804906341887
Epoch 420/10000, Prediction Accuracy = 44.134%, Loss = 2.1671093463897706
Epoch: 420, Batch Gradient Norm: 22.707428686992035
Epoch: 420, Batch Gradient Norm after: 21.807729545780266
Epoch 421/10000, Prediction Accuracy = 44.114%, Loss = 2.1707321643829345
Epoch: 421, Batch Gradient Norm: 21.21502780345854
Epoch: 421, Batch Gradient Norm after: 21.187937975403344
Epoch 422/10000, Prediction Accuracy = 44.136%, Loss = 2.164210557937622
Epoch: 422, Batch Gradient Norm: 22.65987658155868
Epoch: 422, Batch Gradient Norm after: 21.78337292189991
Epoch 423/10000, Prediction Accuracy = 44.134%, Loss = 2.1677458763122557
Epoch: 423, Batch Gradient Norm: 21.158092135700535
Epoch: 423, Batch Gradient Norm after: 21.158092135700535
Epoch 424/10000, Prediction Accuracy = 44.15%, Loss = 2.161207580566406
Epoch: 424, Batch Gradient Norm: 22.562643324434575
Epoch: 424, Batch Gradient Norm after: 21.733035936659743
Epoch 425/10000, Prediction Accuracy = 44.160000000000004%, Loss = 2.164602041244507
Epoch: 425, Batch Gradient Norm: 21.116946995176274
Epoch: 425, Batch Gradient Norm after: 21.09854483973903
Epoch 426/10000, Prediction Accuracy = 44.148%, Loss = 2.1582844257354736
Epoch: 426, Batch Gradient Norm: 22.47610764924068
Epoch: 426, Batch Gradient Norm after: 21.681232894642545
Epoch 427/10000, Prediction Accuracy = 44.182%, Loss = 2.161497211456299
Epoch: 427, Batch Gradient Norm: 21.03212498589054
Epoch: 427, Batch Gradient Norm after: 20.983608322927907
Epoch 428/10000, Prediction Accuracy = 44.18000000000001%, Loss = 2.1552462577819824
Epoch: 428, Batch Gradient Norm: 22.40453842574944
Epoch: 428, Batch Gradient Norm after: 21.62967556554195
Epoch 429/10000, Prediction Accuracy = 44.21%, Loss = 2.1584607124328614
Epoch: 429, Batch Gradient Norm: 21.052670078899972
Epoch: 429, Batch Gradient Norm after: 20.97851380049478
Epoch 430/10000, Prediction Accuracy = 44.188%, Loss = 2.15252685546875
Epoch: 430, Batch Gradient Norm: 22.491737003739033
Epoch: 430, Batch Gradient Norm after: 21.672979833965876
Epoch 431/10000, Prediction Accuracy = 44.224000000000004%, Loss = 2.1559436321258545
Epoch: 431, Batch Gradient Norm: 21.14005619310924
Epoch: 431, Batch Gradient Norm after: 21.09634074814434
Epoch 432/10000, Prediction Accuracy = 44.242000000000004%, Loss = 2.15000319480896
Epoch: 432, Batch Gradient Norm: 22.62858452053101
Epoch: 432, Batch Gradient Norm after: 21.754327867614684
Epoch 433/10000, Prediction Accuracy = 44.23800000000001%, Loss = 2.1536062240600584
Epoch: 433, Batch Gradient Norm: 21.25121220097365
Epoch: 433, Batch Gradient Norm after: 21.25121220097365
Epoch 434/10000, Prediction Accuracy = 44.272%, Loss = 2.14758358001709
Epoch: 434, Batch Gradient Norm: 22.691891321460158
Epoch: 434, Batch Gradient Norm after: 21.804420628141937
Epoch 435/10000, Prediction Accuracy = 44.258%, Loss = 2.151046085357666
Epoch: 435, Batch Gradient Norm: 21.225086341925657
Epoch: 435, Batch Gradient Norm after: 21.21611740834303
Epoch 436/10000, Prediction Accuracy = 44.286%, Loss = 2.1447341442108154
Epoch: 436, Batch Gradient Norm: 22.704842216173176
Epoch: 436, Batch Gradient Norm after: 21.800584075731653
Epoch 437/10000, Prediction Accuracy = 44.266%, Loss = 2.1483348846435546
Epoch: 437, Batch Gradient Norm: 21.19963777270711
Epoch: 437, Batch Gradient Norm after: 21.19963777270711
Epoch 438/10000, Prediction Accuracy = 44.296%, Loss = 2.141899824142456
Epoch: 438, Batch Gradient Norm: 22.709739530575906
Epoch: 438, Batch Gradient Norm after: 21.78165755738619
Epoch 439/10000, Prediction Accuracy = 44.284000000000006%, Loss = 2.1455976486206056
Epoch: 439, Batch Gradient Norm: 21.294273918349962
Epoch: 439, Batch Gradient Norm after: 21.262331745459196
Epoch 440/10000, Prediction Accuracy = 44.32%, Loss = 2.139461803436279
Epoch: 440, Batch Gradient Norm: 22.78816227700721
Epoch: 440, Batch Gradient Norm after: 21.84959442762815
Epoch 441/10000, Prediction Accuracy = 44.327999999999996%, Loss = 2.143124723434448
Epoch: 441, Batch Gradient Norm: 21.29595150160904
Epoch: 441, Batch Gradient Norm after: 21.25054967877101
Epoch 442/10000, Prediction Accuracy = 44.31999999999999%, Loss = 2.136731481552124
Epoch: 442, Batch Gradient Norm: 22.77010708105696
Epoch: 442, Batch Gradient Norm after: 21.844842636269938
Epoch 443/10000, Prediction Accuracy = 44.344%, Loss = 2.1403125762939452
Epoch: 443, Batch Gradient Norm: 21.203845249323656
Epoch: 443, Batch Gradient Norm after: 21.12899768754266
Epoch 444/10000, Prediction Accuracy = 44.346%, Loss = 2.133717107772827
Epoch: 444, Batch Gradient Norm: 22.760839884535944
Epoch: 444, Batch Gradient Norm after: 21.822703532238627
Epoch 445/10000, Prediction Accuracy = 44.356%, Loss = 2.1375407218933105
Epoch: 445, Batch Gradient Norm: 21.320304525092762
Epoch: 445, Batch Gradient Norm after: 21.265134107880684
Epoch 446/10000, Prediction Accuracy = 44.354%, Loss = 2.131336736679077
Epoch: 446, Batch Gradient Norm: 22.86078004109352
Epoch: 446, Batch Gradient Norm after: 21.893670638371034
Epoch 447/10000, Prediction Accuracy = 44.376%, Loss = 2.1351430416107178
Epoch: 447, Batch Gradient Norm: 21.31810565344697
Epoch: 447, Batch Gradient Norm after: 21.25014026923844
Epoch 448/10000, Prediction Accuracy = 44.372%, Loss = 2.128611612319946
Epoch: 448, Batch Gradient Norm: 22.898392932367003
Epoch: 448, Batch Gradient Norm after: 21.909992145058844
Epoch 449/10000, Prediction Accuracy = 44.396%, Loss = 2.1325448036193846
Epoch: 449, Batch Gradient Norm: 21.411133844875373
Epoch: 449, Batch Gradient Norm after: 21.354321146037236
Epoch 450/10000, Prediction Accuracy = 44.382%, Loss = 2.126171875
Epoch: 450, Batch Gradient Norm: 23.02559163687295
Epoch: 450, Batch Gradient Norm after: 21.99134250423732
Epoch 451/10000, Prediction Accuracy = 44.408%, Loss = 2.130263090133667
Epoch: 451, Batch Gradient Norm: 21.575948182933633
Epoch: 451, Batch Gradient Norm after: 21.56630545561682
Epoch 452/10000, Prediction Accuracy = 44.39%, Loss = 2.123987627029419
Epoch: 452, Batch Gradient Norm: 23.12555383986154
Epoch: 452, Batch Gradient Norm after: 22.077255573669284
Epoch 453/10000, Prediction Accuracy = 44.431999999999995%, Loss = 2.127919912338257
Epoch: 453, Batch Gradient Norm: 21.574278030838034
Epoch: 453, Batch Gradient Norm after: 21.5711461358883
Epoch 454/10000, Prediction Accuracy = 44.402%, Loss = 2.1212879180908204
Epoch: 454, Batch Gradient Norm: 23.12080365101959
Epoch: 454, Batch Gradient Norm after: 22.073408168571856
Epoch 455/10000, Prediction Accuracy = 44.454%, Loss = 2.1252015113830565
Epoch: 455, Batch Gradient Norm: 21.563610063082994
Epoch: 455, Batch Gradient Norm after: 21.563610063082994
Epoch 456/10000, Prediction Accuracy = 44.422000000000004%, Loss = 2.1185799121856688
Epoch: 456, Batch Gradient Norm: 23.124846735835593
Epoch: 456, Batch Gradient Norm after: 22.072725046235902
Epoch 457/10000, Prediction Accuracy = 44.476%, Loss = 2.1225231170654295
Epoch: 457, Batch Gradient Norm: 21.633077475603084
Epoch: 457, Batch Gradient Norm after: 21.633077475603084
Epoch 458/10000, Prediction Accuracy = 44.432%, Loss = 2.1161217212677004
Epoch: 458, Batch Gradient Norm: 23.151127915250676
Epoch: 458, Batch Gradient Norm after: 22.106446797466692
Epoch 459/10000, Prediction Accuracy = 44.502%, Loss = 2.1199492454528808
Epoch: 459, Batch Gradient Norm: 21.48917566460533
Epoch: 459, Batch Gradient Norm after: 21.4563053876298
Epoch 460/10000, Prediction Accuracy = 44.456%, Loss = 2.1130127906799316
Epoch: 460, Batch Gradient Norm: 23.129607223496812
Epoch: 460, Batch Gradient Norm after: 22.060140236294792
Epoch 461/10000, Prediction Accuracy = 44.52%, Loss = 2.117192602157593
Epoch: 461, Batch Gradient Norm: 21.602974916849487
Epoch: 461, Batch Gradient Norm after: 21.602974916849487
Epoch 462/10000, Prediction Accuracy = 44.461999999999996%, Loss = 2.110714817047119
Epoch: 462, Batch Gradient Norm: 23.17404991218227
Epoch: 462, Batch Gradient Norm after: 22.11508127202864
Epoch 463/10000, Prediction Accuracy = 44.538000000000004%, Loss = 2.1147053718566893
Epoch: 463, Batch Gradient Norm: 21.525985489858765
Epoch: 463, Batch Gradient Norm after: 21.47053641895506
Epoch 464/10000, Prediction Accuracy = 44.480000000000004%, Loss = 2.107836198806763
Epoch: 464, Batch Gradient Norm: 23.17212031133894
Epoch: 464, Batch Gradient Norm after: 22.09196312167854
Epoch 465/10000, Prediction Accuracy = 44.552%, Loss = 2.1120513916015624
Epoch: 465, Batch Gradient Norm: 21.66858285714744
Epoch: 465, Batch Gradient Norm after: 21.630472391279625
Epoch 466/10000, Prediction Accuracy = 44.50599999999999%, Loss = 2.105639600753784
Epoch: 466, Batch Gradient Norm: 23.226649976023047
Epoch: 466, Batch Gradient Norm after: 22.149630661687453
Epoch 467/10000, Prediction Accuracy = 44.571999999999996%, Loss = 2.1096119403839113
Epoch: 467, Batch Gradient Norm: 21.50630437573827
Epoch: 467, Batch Gradient Norm after: 21.401451675100606
Epoch 468/10000, Prediction Accuracy = 44.536%, Loss = 2.1025086402893067
Epoch: 468, Batch Gradient Norm: 23.215966648796854
Epoch: 468, Batch Gradient Norm after: 22.10728442183719
Epoch 469/10000, Prediction Accuracy = 44.586%, Loss = 2.1069217681884767
Epoch: 469, Batch Gradient Norm: 21.716012271004825
Epoch: 469, Batch Gradient Norm after: 21.647846353466583
Epoch 470/10000, Prediction Accuracy = 44.556%, Loss = 2.1005406856536863
Epoch: 470, Batch Gradient Norm: 23.291435758055687
Epoch: 470, Batch Gradient Norm after: 22.190862775056
Epoch 471/10000, Prediction Accuracy = 44.586%, Loss = 2.1045849323272705
Epoch: 471, Batch Gradient Norm: 21.452063355343856
Epoch: 471, Batch Gradient Norm after: 21.27693303224736
Epoch 472/10000, Prediction Accuracy = 44.576%, Loss = 2.0971155166625977
Epoch: 472, Batch Gradient Norm: 23.260158629733997
Epoch: 472, Batch Gradient Norm after: 22.116346036080703
Epoch 473/10000, Prediction Accuracy = 44.598%, Loss = 2.101840686798096
Epoch: 473, Batch Gradient Norm: 21.804576195388947
Epoch: 473, Batch Gradient Norm after: 21.702686418958205
Epoch 474/10000, Prediction Accuracy = 44.586%, Loss = 2.0956066131591795
Epoch: 474, Batch Gradient Norm: 23.388972690181713
Epoch: 474, Batch Gradient Norm after: 22.25784929030415
Epoch 475/10000, Prediction Accuracy = 44.634%, Loss = 2.0997116565704346
Epoch: 475, Batch Gradient Norm: 21.357006604705823
Epoch: 475, Batch Gradient Norm after: 21.08509539691576
Epoch 476/10000, Prediction Accuracy = 44.604%, Loss = 2.091648006439209
Epoch: 476, Batch Gradient Norm: 23.311681568703598
Epoch: 476, Batch Gradient Norm after: 22.114317491559902
Epoch 477/10000, Prediction Accuracy = 44.64%, Loss = 2.0968091011047365
Epoch: 477, Batch Gradient Norm: 22.004985765127916
Epoch: 477, Batch Gradient Norm after: 21.89203773125706
Epoch 478/10000, Prediction Accuracy = 44.608%, Loss = 2.0910441398620607
Epoch: 478, Batch Gradient Norm: 23.541792188564095
Epoch: 478, Batch Gradient Norm after: 22.3606784714447
Epoch 479/10000, Prediction Accuracy = 44.641999999999996%, Loss = 2.09504075050354
Epoch: 479, Batch Gradient Norm: 21.209595116714414
Epoch: 479, Batch Gradient Norm after: 20.812045092591564
Epoch 480/10000, Prediction Accuracy = 44.634%, Loss = 2.0860352516174316
Epoch: 480, Batch Gradient Norm: 23.35452100890684
Epoch: 480, Batch Gradient Norm after: 22.09318579813828
Epoch 481/10000, Prediction Accuracy = 44.67%, Loss = 2.091754913330078
Epoch: 481, Batch Gradient Norm: 22.319231683697744
Epoch: 481, Batch Gradient Norm after: 22.171061678950487
Epoch 482/10000, Prediction Accuracy = 44.654%, Loss = 2.0868552684783936
Epoch: 482, Batch Gradient Norm: 23.64011088479333
Epoch: 482, Batch Gradient Norm after: 22.360678286028396
Epoch 483/10000, Prediction Accuracy = 44.674%, Loss = 2.090190315246582
Epoch: 483, Batch Gradient Norm: 21.53304641754527
Epoch: 483, Batch Gradient Norm after: 21.188466789333848
Epoch 484/10000, Prediction Accuracy = 44.668%, Loss = 2.0818787097930906
Epoch: 484, Batch Gradient Norm: 23.504459039751808
Epoch: 484, Batch Gradient Norm after: 22.24356782721523
Epoch 485/10000, Prediction Accuracy = 44.682%, Loss = 2.087145709991455
Epoch: 485, Batch Gradient Norm: 21.83954512719401
Epoch: 485, Batch Gradient Norm after: 21.546443305359578
Epoch 486/10000, Prediction Accuracy = 44.67399999999999%, Loss = 2.080265665054321
Epoch: 486, Batch Gradient Norm: 23.660078861946044
Epoch: 486, Batch Gradient Norm after: 22.360678447650997
Epoch 487/10000, Prediction Accuracy = 44.702000000000005%, Loss = 2.0851243019104
Epoch: 487, Batch Gradient Norm: 21.572899155642553
Epoch: 487, Batch Gradient Norm after: 21.15522744505874
Epoch 488/10000, Prediction Accuracy = 44.69800000000001%, Loss = 2.076925277709961
Epoch: 488, Batch Gradient Norm: 23.615002480330567
Epoch: 488, Batch Gradient Norm after: 22.302092303085594
Epoch 489/10000, Prediction Accuracy = 44.72%, Loss = 2.082419490814209
Epoch: 489, Batch Gradient Norm: 21.812581341043266
Epoch: 489, Batch Gradient Norm after: 21.41998100971512
Epoch 490/10000, Prediction Accuracy = 44.708%, Loss = 2.075123691558838
Epoch: 490, Batch Gradient Norm: 23.74808813915054
Epoch: 490, Batch Gradient Norm after: 22.36067627882549
Epoch 491/10000, Prediction Accuracy = 44.718%, Loss = 2.0803354263305662
Epoch: 491, Batch Gradient Norm: 21.871297112957222
Epoch: 491, Batch Gradient Norm after: 21.460574890163173
Epoch 492/10000, Prediction Accuracy = 44.728%, Loss = 2.072778558731079
Epoch: 492, Batch Gradient Norm: 23.80721112629076
Epoch: 492, Batch Gradient Norm after: 22.36067617865785
Epoch 493/10000, Prediction Accuracy = 44.738%, Loss = 2.078001546859741
Epoch: 493, Batch Gradient Norm: 22.08024626912485
Epoch: 493, Batch Gradient Norm after: 21.7051770693306
Epoch 494/10000, Prediction Accuracy = 44.718%, Loss = 2.070906162261963
Epoch: 494, Batch Gradient Norm: 23.911631670505503
Epoch: 494, Batch Gradient Norm after: 22.360676369124445
Epoch 495/10000, Prediction Accuracy = 44.742000000000004%, Loss = 2.0758127689361574
Epoch: 495, Batch Gradient Norm: 22.414170863016626
Epoch: 495, Batch Gradient Norm after: 22.008335488729365
Epoch 496/10000, Prediction Accuracy = 44.724000000000004%, Loss = 2.0694390773773192
Epoch: 496, Batch Gradient Norm: 23.798375742799426
Epoch: 496, Batch Gradient Norm after: 22.36067947242902
Epoch 497/10000, Prediction Accuracy = 44.748000000000005%, Loss = 2.072956895828247
Epoch: 497, Batch Gradient Norm: 22.036063947982143
Epoch: 497, Batch Gradient Norm after: 21.626945637504623
Epoch 498/10000, Prediction Accuracy = 44.726%, Loss = 2.065791702270508
Epoch: 498, Batch Gradient Norm: 23.939110679388897
Epoch: 498, Batch Gradient Norm after: 22.360677977248542
Epoch 499/10000, Prediction Accuracy = 44.762%, Loss = 2.0709070682525637
Epoch: 499, Batch Gradient Norm: 22.504061240003303
Epoch: 499, Batch Gradient Norm after: 22.059004373059913
Epoch 500/10000, Prediction Accuracy = 44.742000000000004%, Loss = 2.0647785663604736
Epoch: 500, Batch Gradient Norm: 23.80441601976743
Epoch: 500, Batch Gradient Norm after: 22.360677287648432
Epoch 501/10000, Prediction Accuracy = 44.788%, Loss = 2.068002700805664
Epoch: 501, Batch Gradient Norm: 22.04007069349303
Epoch: 501, Batch Gradient Norm after: 21.595459137069437
Epoch 502/10000, Prediction Accuracy = 44.739999999999995%, Loss = 2.060865545272827
Epoch: 502, Batch Gradient Norm: 23.997862429232285
Epoch: 502, Batch Gradient Norm after: 22.360678352708344
Epoch 503/10000, Prediction Accuracy = 44.796%, Loss = 2.0661035060882567
Epoch: 503, Batch Gradient Norm: 22.689577912252712
Epoch: 503, Batch Gradient Norm after: 22.181429905408
Epoch 504/10000, Prediction Accuracy = 44.763999999999996%, Loss = 2.0604094505310058
Epoch: 504, Batch Gradient Norm: 23.79378567747955
Epoch: 504, Batch Gradient Norm after: 22.35327075981263
Epoch 505/10000, Prediction Accuracy = 44.808%, Loss = 2.0629794597625732
Epoch: 505, Batch Gradient Norm: 22.04871652667688
Epoch: 505, Batch Gradient Norm after: 21.549237994760716
Epoch 506/10000, Prediction Accuracy = 44.774%, Loss = 2.0559717655181884
Epoch: 506, Batch Gradient Norm: 24.055483626446204
Epoch: 506, Batch Gradient Norm after: 22.36067667760115
Epoch 507/10000, Prediction Accuracy = 44.82000000000001%, Loss = 2.061326837539673
Epoch: 507, Batch Gradient Norm: 22.79391702541945
Epoch: 507, Batch Gradient Norm after: 22.26193244421138
Epoch 508/10000, Prediction Accuracy = 44.78%, Loss = 2.05582013130188
Epoch: 508, Batch Gradient Norm: 23.859610616809093
Epoch: 508, Batch Gradient Norm after: 22.360679245181842
Epoch 509/10000, Prediction Accuracy = 44.83399999999999%, Loss = 2.058268642425537
Epoch: 509, Batch Gradient Norm: 22.22295018202446
Epoch: 509, Batch Gradient Norm after: 21.65026892162689
Epoch 510/10000, Prediction Accuracy = 44.792%, Loss = 2.0516139030456544
Epoch: 510, Batch Gradient Norm: 24.029834624598173
Epoch: 510, Batch Gradient Norm after: 22.36068007213096
Epoch 511/10000, Prediction Accuracy = 44.844%, Loss = 2.056352424621582
Epoch: 511, Batch Gradient Norm: 22.791737411159943
Epoch: 511, Batch Gradient Norm after: 22.156273606631295
Epoch 512/10000, Prediction Accuracy = 44.816%, Loss = 2.0509355068206787
Epoch: 512, Batch Gradient Norm: 23.871059379718368
Epoch: 512, Batch Gradient Norm after: 22.355390975264143
Epoch 513/10000, Prediction Accuracy = 44.85600000000001%, Loss = 2.0534371376037597
Epoch: 513, Batch Gradient Norm: 22.28542881953651
Epoch: 513, Batch Gradient Norm after: 21.631571776189382
Epoch 514/10000, Prediction Accuracy = 44.826%, Loss = 2.046953248977661
Epoch: 514, Batch Gradient Norm: 24.064224615875105
Epoch: 514, Batch Gradient Norm after: 22.360678283509852
Epoch 515/10000, Prediction Accuracy = 44.856%, Loss = 2.0516003131866456
Epoch: 515, Batch Gradient Norm: 22.846667936814274
Epoch: 515, Batch Gradient Norm after: 22.155574707177298
Epoch 516/10000, Prediction Accuracy = 44.83800000000001%, Loss = 2.04625244140625
Epoch: 516, Batch Gradient Norm: 23.945672021509647
Epoch: 516, Batch Gradient Norm after: 22.36067927335543
Epoch 517/10000, Prediction Accuracy = 44.854%, Loss = 2.048820686340332
Epoch: 517, Batch Gradient Norm: 22.500719148280204
Epoch: 517, Batch Gradient Norm after: 21.750925678962716
Epoch 518/10000, Prediction Accuracy = 44.84%, Loss = 2.042782211303711
Epoch: 518, Batch Gradient Norm: 24.045948659033687
Epoch: 518, Batch Gradient Norm after: 22.36067684968563
Epoch 519/10000, Prediction Accuracy = 44.862%, Loss = 2.04671893119812
Epoch: 519, Batch Gradient Norm: 22.84752085829792
Epoch: 519, Batch Gradient Norm after: 22.03547305936641
Epoch 520/10000, Prediction Accuracy = 44.852%, Loss = 2.041466474533081
Epoch: 520, Batch Gradient Norm: 23.967193089058693
Epoch: 520, Batch Gradient Norm after: 22.347261456387212
Epoch 521/10000, Prediction Accuracy = 44.866%, Loss = 2.0440699577331545
Epoch: 521, Batch Gradient Norm: 22.629739832876133
Epoch: 521, Batch Gradient Norm after: 21.783090580247105
Epoch 522/10000, Prediction Accuracy = 44.86999999999999%, Loss = 2.0383934497833254
Epoch: 522, Batch Gradient Norm: 24.074085256292726
Epoch: 522, Batch Gradient Norm after: 22.36067914650855
Epoch 523/10000, Prediction Accuracy = 44.88000000000001%, Loss = 2.041995048522949
Epoch: 523, Batch Gradient Norm: 22.88673360119849
Epoch: 523, Batch Gradient Norm after: 21.981468574865634
Epoch 524/10000, Prediction Accuracy = 44.884%, Loss = 2.0367830753326417
Epoch: 524, Batch Gradient Norm: 24.03387264929698
Epoch: 524, Batch Gradient Norm after: 22.355309228196628
Epoch 525/10000, Prediction Accuracy = 44.896%, Loss = 2.039486360549927
Epoch: 525, Batch Gradient Norm: 22.74194140455059
Epoch: 525, Batch Gradient Norm after: 21.787822724070164
Epoch 526/10000, Prediction Accuracy = 44.888%, Loss = 2.033939790725708
Epoch: 526, Batch Gradient Norm: 24.111668679563422
Epoch: 526, Batch Gradient Norm after: 22.360675260943005
Epoch 527/10000, Prediction Accuracy = 44.896%, Loss = 2.037326431274414
Epoch: 527, Batch Gradient Norm: 22.93427239244219
Epoch: 527, Batch Gradient Norm after: 21.93097405913863
Epoch 528/10000, Prediction Accuracy = 44.897999999999996%, Loss = 2.0321552753448486
Epoch: 528, Batch Gradient Norm: 24.112290341561582
Epoch: 528, Batch Gradient Norm after: 22.36067443266343
Epoch 529/10000, Prediction Accuracy = 44.93%, Loss = 2.034949541091919
Epoch: 529, Batch Gradient Norm: 22.947652266582885
Epoch: 529, Batch Gradient Norm after: 21.881430890579374
Epoch 530/10000, Prediction Accuracy = 44.906%, Loss = 2.029827356338501
Epoch: 530, Batch Gradient Norm: 24.135414394099726
Epoch: 530, Batch Gradient Norm after: 22.35934057411021
Epoch 531/10000, Prediction Accuracy = 44.958000000000006%, Loss = 2.0326544761657717
Epoch: 531, Batch Gradient Norm: 22.981917158834523
Epoch: 531, Batch Gradient Norm after: 21.876620459631553
Epoch 532/10000, Prediction Accuracy = 44.934%, Loss = 2.0275650024414062
Epoch: 532, Batch Gradient Norm: 24.178534939516858
Epoch: 532, Batch Gradient Norm after: 22.36067638289994
Epoch 533/10000, Prediction Accuracy = 44.982%, Loss = 2.030422592163086
Epoch: 533, Batch Gradient Norm: 23.032700844449806
Epoch: 533, Batch Gradient Norm after: 21.916623145842635
Epoch 534/10000, Prediction Accuracy = 44.925999999999995%, Loss = 2.0253449440002442
Epoch: 534, Batch Gradient Norm: 24.24272227863333
Epoch: 534, Batch Gradient Norm after: 22.36067661717752
Epoch 535/10000, Prediction Accuracy = 45.001999999999995%, Loss = 2.0282524108886717
Epoch: 535, Batch Gradient Norm: 23.1117345436276
Epoch: 535, Batch Gradient Norm after: 22.018468236647735
Epoch 536/10000, Prediction Accuracy = 44.932%, Loss = 2.023215579986572
Epoch: 536, Batch Gradient Norm: 24.328084247717367
Epoch: 536, Batch Gradient Norm after: 22.36067904313588
Epoch 537/10000, Prediction Accuracy = 45.008%, Loss = 2.026156711578369
Epoch: 537, Batch Gradient Norm: 23.203597624396103
Epoch: 537, Batch Gradient Norm after: 22.164292633353988
Epoch 538/10000, Prediction Accuracy = 44.959999999999994%, Loss = 2.0211424827575684
Epoch: 538, Batch Gradient Norm: 24.438958484555545
Epoch: 538, Batch Gradient Norm after: 22.360678967737446
Epoch 539/10000, Prediction Accuracy = 45.028000000000006%, Loss = 2.0241527557373047
Epoch: 539, Batch Gradient Norm: 23.327962744143857
Epoch: 539, Batch Gradient Norm after: 22.264777569224766
Epoch 540/10000, Prediction Accuracy = 44.964000000000006%, Loss = 2.019190788269043
Epoch: 540, Batch Gradient Norm: 24.36336667358561
Epoch: 540, Batch Gradient Norm after: 22.360678123675484
Epoch 541/10000, Prediction Accuracy = 45.024%, Loss = 2.0215846061706544
Epoch: 541, Batch Gradient Norm: 23.261410982589105
Epoch: 541, Batch Gradient Norm after: 22.164720826503245
Epoch 542/10000, Prediction Accuracy = 44.97%, Loss = 2.016635370254517
Epoch: 542, Batch Gradient Norm: 24.482122526844385
Epoch: 542, Batch Gradient Norm after: 22.360678001440572
Epoch 543/10000, Prediction Accuracy = 45.040000000000006%, Loss = 2.019595432281494
Epoch: 543, Batch Gradient Norm: 23.39381891300741
Epoch: 543, Batch Gradient Norm after: 22.264476589162477
Epoch 544/10000, Prediction Accuracy = 44.974000000000004%, Loss = 2.0147040367126463
Epoch: 544, Batch Gradient Norm: 24.405547689922855
Epoch: 544, Batch Gradient Norm after: 22.360679385846623
Epoch 545/10000, Prediction Accuracy = 45.042%, Loss = 2.0170366764068604
Epoch: 545, Batch Gradient Norm: 23.318580591697504
Epoch: 545, Batch Gradient Norm after: 22.159118352792923
Epoch 546/10000, Prediction Accuracy = 45.007999999999996%, Loss = 2.012150263786316
Epoch: 546, Batch Gradient Norm: 24.541304355118044
Epoch: 546, Batch Gradient Norm after: 22.360678279840045
Epoch 547/10000, Prediction Accuracy = 45.062%, Loss = 2.015124797821045
Epoch: 547, Batch Gradient Norm: 23.47344699281368
Epoch: 547, Batch Gradient Norm after: 22.280776801915582
Epoch 548/10000, Prediction Accuracy = 45.024%, Loss = 2.0103276491165163
Epoch: 548, Batch Gradient Norm: 24.448258809665095
Epoch: 548, Batch Gradient Norm after: 22.36067686808808
Epoch 549/10000, Prediction Accuracy = 45.074%, Loss = 2.012533187866211
Epoch: 549, Batch Gradient Norm: 23.384693681458693
Epoch: 549, Batch Gradient Norm after: 22.154442367138106
Epoch 550/10000, Prediction Accuracy = 45.03%, Loss = 2.0077246189117433
Epoch: 550, Batch Gradient Norm: 24.590989861171185
Epoch: 550, Batch Gradient Norm after: 22.360677211526458
Epoch 551/10000, Prediction Accuracy = 45.08%, Loss = 2.0106616735458376
Epoch: 551, Batch Gradient Norm: 23.546746610182844
Epoch: 551, Batch Gradient Norm after: 22.279795798908278
Epoch 552/10000, Prediction Accuracy = 45.056%, Loss = 2.005911350250244
Epoch: 552, Batch Gradient Norm: 24.499760504064156
Epoch: 552, Batch Gradient Norm after: 22.360676473140227
Epoch 553/10000, Prediction Accuracy = 45.1%, Loss = 2.008067774772644
Epoch: 553, Batch Gradient Norm: 23.448748891122964
Epoch: 553, Batch Gradient Norm after: 22.143723040785773
Epoch 554/10000, Prediction Accuracy = 45.065999999999995%, Loss = 2.0032885789871218
Epoch: 554, Batch Gradient Norm: 24.662773921343724
Epoch: 554, Batch Gradient Norm after: 22.36067762301758
Epoch 555/10000, Prediction Accuracy = 45.116%, Loss = 2.006252908706665
Epoch: 555, Batch Gradient Norm: 23.639893147876837
Epoch: 555, Batch Gradient Norm after: 22.29312176465968
Epoch 556/10000, Prediction Accuracy = 45.07600000000001%, Loss = 2.0015925645828245
Epoch: 556, Batch Gradient Norm: 24.55089283979039
Epoch: 556, Batch Gradient Norm after: 22.360678444758523
Epoch 557/10000, Prediction Accuracy = 45.124%, Loss = 2.0036102056503298
Epoch: 557, Batch Gradient Norm: 23.527875751512454
Epoch: 557, Batch Gradient Norm after: 22.13967845317284
Epoch 558/10000, Prediction Accuracy = 45.08%, Loss = 1.9989428997039795
Epoch: 558, Batch Gradient Norm: 24.727593628562428
Epoch: 558, Batch Gradient Norm after: 22.360676569626733
Epoch 559/10000, Prediction Accuracy = 45.134%, Loss = 2.001853275299072
Epoch: 559, Batch Gradient Norm: 23.740977506148766
Epoch: 559, Batch Gradient Norm after: 22.306693509434734
Epoch 560/10000, Prediction Accuracy = 45.08800000000001%, Loss = 1.9973304271697998
Epoch: 560, Batch Gradient Norm: 24.601389943749258
Epoch: 560, Batch Gradient Norm after: 22.36067909512734
Epoch 561/10000, Prediction Accuracy = 45.158%, Loss = 1.999178981781006
Epoch: 561, Batch Gradient Norm: 23.601694274099998
Epoch: 561, Batch Gradient Norm after: 22.13036323782737
Epoch 562/10000, Prediction Accuracy = 45.11999999999999%, Loss = 1.994612193107605
Epoch: 562, Batch Gradient Norm: 24.79668622711542
Epoch: 562, Batch Gradient Norm after: 22.36067764685713
Epoch 563/10000, Prediction Accuracy = 45.158%, Loss = 1.997510266304016
Epoch: 563, Batch Gradient Norm: 23.84249649075898
Epoch: 563, Batch Gradient Norm after: 22.320057578085436
Epoch 564/10000, Prediction Accuracy = 45.13000000000001%, Loss = 1.993100905418396
Epoch: 564, Batch Gradient Norm: 24.64723737079399
Epoch: 564, Batch Gradient Norm after: 22.36067723541727
Epoch 565/10000, Prediction Accuracy = 45.178%, Loss = 1.994794487953186
Epoch: 565, Batch Gradient Norm: 23.67711994072014
Epoch: 565, Batch Gradient Norm after: 22.11566799379315
Epoch 566/10000, Prediction Accuracy = 45.144%, Loss = 1.9903032779693604
Epoch: 566, Batch Gradient Norm: 24.870466718433118
Epoch: 566, Batch Gradient Norm after: 22.360679248290428
Epoch 567/10000, Prediction Accuracy = 45.187999999999995%, Loss = 1.9931967496871947
Epoch: 567, Batch Gradient Norm: 23.954717179844017
Epoch: 567, Batch Gradient Norm after: 22.33897206979132
Epoch 568/10000, Prediction Accuracy = 45.150000000000006%, Loss = 1.9889214277267455
Epoch: 568, Batch Gradient Norm: 24.695884123255887
Epoch: 568, Batch Gradient Norm after: 22.360678811396006
Epoch 569/10000, Prediction Accuracy = 45.194%, Loss = 1.9904106616973878
Epoch: 569, Batch Gradient Norm: 23.74607664774871
Epoch: 569, Batch Gradient Norm after: 22.102853928394282
Epoch 570/10000, Prediction Accuracy = 45.178%, Loss = 1.9860136032104492
Epoch: 570, Batch Gradient Norm: 24.9546675713282
Epoch: 570, Batch Gradient Norm after: 22.360677048975777
Epoch 571/10000, Prediction Accuracy = 45.202000000000005%, Loss = 1.9889342069625855
Epoch: 571, Batch Gradient Norm: 24.060112178273197
Epoch: 571, Batch Gradient Norm after: 22.360678752959654
Epoch 572/10000, Prediction Accuracy = 45.188%, Loss = 1.9847366571426392
Epoch: 572, Batch Gradient Norm: 24.75703056395504
Epoch: 572, Batch Gradient Norm after: 22.36067879987676
Epoch 573/10000, Prediction Accuracy = 45.22%, Loss = 1.986089015007019
Epoch: 573, Batch Gradient Norm: 23.840035772446928
Epoch: 573, Batch Gradient Norm after: 22.10603479962123
Epoch 574/10000, Prediction Accuracy = 45.206%, Loss = 1.9817861080169679
Epoch: 574, Batch Gradient Norm: 25.026693309887587
Epoch: 574, Batch Gradient Norm after: 22.360678707820288
Epoch 575/10000, Prediction Accuracy = 45.222%, Loss = 1.984653854370117
Epoch: 575, Batch Gradient Norm: 24.13120871911592
Epoch: 575, Batch Gradient Norm after: 22.36067717725825
Epoch 576/10000, Prediction Accuracy = 45.226000000000006%, Loss = 1.9804415702819824
Epoch: 576, Batch Gradient Norm: 24.83902127195301
Epoch: 576, Batch Gradient Norm after: 22.360678210002014
Epoch 577/10000, Prediction Accuracy = 45.230000000000004%, Loss = 1.9818614721298218
Epoch: 577, Batch Gradient Norm: 23.95700268724708
Epoch: 577, Batch Gradient Norm after: 22.13534928045073
Epoch 578/10000, Prediction Accuracy = 45.221999999999994%, Loss = 1.9776506662368774
Epoch: 578, Batch Gradient Norm: 25.06614754866099
Epoch: 578, Batch Gradient Norm after: 22.360678336058644
Epoch 579/10000, Prediction Accuracy = 45.221999999999994%, Loss = 1.9802999019622802
Epoch: 579, Batch Gradient Norm: 24.24099182568808
Epoch: 579, Batch Gradient Norm after: 22.360676756479446
Epoch 580/10000, Prediction Accuracy = 45.236%, Loss = 1.9763089418411255
Epoch: 580, Batch Gradient Norm: 24.906761032975275
Epoch: 580, Batch Gradient Norm after: 22.36067755601785
Epoch 581/10000, Prediction Accuracy = 45.228%, Loss = 1.97758891582489
Epoch: 581, Batch Gradient Norm: 24.049094483232448
Epoch: 581, Batch Gradient Norm after: 22.138040348737587
Epoch 582/10000, Prediction Accuracy = 45.251999999999995%, Loss = 1.9734832525253296
Epoch: 582, Batch Gradient Norm: 25.142980739880013
Epoch: 582, Batch Gradient Norm after: 22.360679030445866
Epoch 583/10000, Prediction Accuracy = 45.25%, Loss = 1.9760815143585204
Epoch: 583, Batch Gradient Norm: 24.311041111770937
Epoch: 583, Batch Gradient Norm after: 22.360676550194867
Epoch 584/10000, Prediction Accuracy = 45.257999999999996%, Loss = 1.972079372406006
Epoch: 584, Batch Gradient Norm: 24.987737097377728
Epoch: 584, Batch Gradient Norm after: 22.36067838811929
Epoch 585/10000, Prediction Accuracy = 45.25%, Loss = 1.973424243927002
Epoch: 585, Batch Gradient Norm: 24.151435191747918
Epoch: 585, Batch Gradient Norm after: 22.15757915646953
Epoch 586/10000, Prediction Accuracy = 45.257999999999996%, Loss = 1.9693854808807374
Epoch: 586, Batch Gradient Norm: 25.196615491840205
Epoch: 586, Batch Gradient Norm after: 22.360677843992136
Epoch 587/10000, Prediction Accuracy = 45.266%, Loss = 1.9718374490737915
Epoch: 587, Batch Gradient Norm: 24.415065035185116
Epoch: 587, Batch Gradient Norm after: 22.360678127274667
Epoch 588/10000, Prediction Accuracy = 45.276%, Loss = 1.9680038213729858
Epoch: 588, Batch Gradient Norm: 25.053204962836737
Epoch: 588, Batch Gradient Norm after: 22.360677776791643
Epoch 589/10000, Prediction Accuracy = 45.248000000000005%, Loss = 1.9692130088806152
Epoch: 589, Batch Gradient Norm: 24.237729619550695
Epoch: 589, Batch Gradient Norm after: 22.15333499243725
Epoch 590/10000, Prediction Accuracy = 45.282%, Loss = 1.9652405261993409
Epoch: 590, Batch Gradient Norm: 25.271023920134493
Epoch: 590, Batch Gradient Norm after: 22.36067866632713
Epoch 591/10000, Prediction Accuracy = 45.233999999999995%, Loss = 1.9676717042922973
Epoch: 591, Batch Gradient Norm: 24.489801419336
Epoch: 591, Batch Gradient Norm after: 22.36067552277513
Epoch 592/10000, Prediction Accuracy = 45.275999999999996%, Loss = 1.9638504028320312
Epoch: 592, Batch Gradient Norm: 25.13697352623655
Epoch: 592, Batch Gradient Norm after: 22.360675717092523
Epoch 593/10000, Prediction Accuracy = 45.236%, Loss = 1.9650779485702514
Epoch: 593, Batch Gradient Norm: 24.354030499818048
Epoch: 593, Batch Gradient Norm after: 22.176015085923577
Epoch 594/10000, Prediction Accuracy = 45.292%, Loss = 1.9612392902374267
Epoch: 594, Batch Gradient Norm: 25.319053436397393
Epoch: 594, Batch Gradient Norm after: 22.36067922784967
Epoch 595/10000, Prediction Accuracy = 45.236000000000004%, Loss = 1.9634451627731324
Epoch: 595, Batch Gradient Norm: 24.608036334381676
Epoch: 595, Batch Gradient Norm after: 22.360677453099946
Epoch 596/10000, Prediction Accuracy = 45.30200000000001%, Loss = 1.9598643779754639
Epoch: 596, Batch Gradient Norm: 25.19376199951548
Epoch: 596, Batch Gradient Norm after: 22.360679633947267
Epoch 597/10000, Prediction Accuracy = 45.239999999999995%, Loss = 1.9609248638153076
Epoch: 597, Batch Gradient Norm: 24.436982266433567
Epoch: 597, Batch Gradient Norm after: 22.166611836909333
Epoch 598/10000, Prediction Accuracy = 45.308%, Loss = 1.9571688413619994
Epoch: 598, Batch Gradient Norm: 25.401167770308724
Epoch: 598, Batch Gradient Norm after: 22.36067857711973
Epoch 599/10000, Prediction Accuracy = 45.266%, Loss = 1.9593677282333375
Epoch: 599, Batch Gradient Norm: 24.678904027475397
Epoch: 599, Batch Gradient Norm after: 22.360679205400057
Epoch 600/10000, Prediction Accuracy = 45.31400000000001%, Loss = 1.955758309364319
Epoch: 600, Batch Gradient Norm: 25.28162808073828
Epoch: 600, Batch Gradient Norm after: 22.360677400473016
Epoch 601/10000, Prediction Accuracy = 45.26800000000001%, Loss = 1.956849718093872
Epoch: 601, Batch Gradient Norm: 24.550893629103328
Epoch: 601, Batch Gradient Norm after: 22.185092109319903
Epoch 602/10000, Prediction Accuracy = 45.324000000000005%, Loss = 1.9532088041305542
Epoch: 602, Batch Gradient Norm: 25.45115878467464
Epoch: 602, Batch Gradient Norm after: 22.36067911051391
Epoch 603/10000, Prediction Accuracy = 45.279999999999994%, Loss = 1.9552025318145752
Epoch: 603, Batch Gradient Norm: 24.803938149873034
Epoch: 603, Batch Gradient Norm after: 22.360678166210374
Epoch 604/10000, Prediction Accuracy = 45.35%, Loss = 1.951859998703003
Epoch: 604, Batch Gradient Norm: 25.3375289437528
Epoch: 604, Batch Gradient Norm after: 22.36067747614783
Epoch 605/10000, Prediction Accuracy = 45.304%, Loss = 1.9527241945266725
Epoch: 605, Batch Gradient Norm: 24.634860613784994
Epoch: 605, Batch Gradient Norm after: 22.166463789389926
Epoch 606/10000, Prediction Accuracy = 45.364%, Loss = 1.949160099029541
Epoch: 606, Batch Gradient Norm: 25.541666759361302
Epoch: 606, Batch Gradient Norm after: 22.36067950370626
Epoch 607/10000, Prediction Accuracy = 45.332%, Loss = 1.9511731624603272
Epoch: 607, Batch Gradient Norm: 24.873999377212243
Epoch: 607, Batch Gradient Norm after: 22.36067529978108
Epoch 608/10000, Prediction Accuracy = 45.374%, Loss = 1.9477770805358887
Epoch: 608, Batch Gradient Norm: 25.428227667100582
Epoch: 608, Batch Gradient Norm after: 22.36067509118057
Epoch 609/10000, Prediction Accuracy = 45.34%, Loss = 1.9487020254135132
Epoch: 609, Batch Gradient Norm: 24.77122493778496
Epoch: 609, Batch Gradient Norm after: 22.201086371853464
Epoch 610/10000, Prediction Accuracy = 45.382000000000005%, Loss = 1.9453171014785766
Epoch: 610, Batch Gradient Norm: 25.577714803383316
Epoch: 610, Batch Gradient Norm after: 22.360678768497277
Epoch 611/10000, Prediction Accuracy = 45.348%, Loss = 1.9470214366912841
Epoch: 611, Batch Gradient Norm: 25.010437778610566
Epoch: 611, Batch Gradient Norm after: 22.357184148370493
Epoch 612/10000, Prediction Accuracy = 45.378%, Loss = 1.9439588785171509
Epoch: 612, Batch Gradient Norm: 25.480481451766018
Epoch: 612, Batch Gradient Norm after: 22.360678621299243
Epoch 613/10000, Prediction Accuracy = 45.366%, Loss = 1.944623851776123
Epoch: 613, Batch Gradient Norm: 24.83907669803188
Epoch: 613, Batch Gradient Norm after: 22.17553376170421
Epoch 614/10000, Prediction Accuracy = 45.39%, Loss = 1.9412916898727417
Epoch: 614, Batch Gradient Norm: 25.672813163582575
Epoch: 614, Batch Gradient Norm after: 22.360677726800255
Epoch 615/10000, Prediction Accuracy = 45.388%, Loss = 1.9430695295333862
Epoch: 615, Batch Gradient Norm: 25.07735870141733
Epoch: 615, Batch Gradient Norm after: 22.36067725371745
Epoch 616/10000, Prediction Accuracy = 45.396%, Loss = 1.9399181365966798
Epoch: 616, Batch Gradient Norm: 25.56838869383994
Epoch: 616, Batch Gradient Norm after: 22.36067889070271
Epoch 617/10000, Prediction Accuracy = 45.388000000000005%, Loss = 1.940653419494629
Epoch: 617, Batch Gradient Norm: 24.972041276724298
Epoch: 617, Batch Gradient Norm after: 22.202830359806974
Epoch 618/10000, Prediction Accuracy = 45.42%, Loss = 1.9374847412109375
Epoch: 618, Batch Gradient Norm: 25.717684386012394
Epoch: 618, Batch Gradient Norm after: 22.36067897774148
Epoch 619/10000, Prediction Accuracy = 45.403999999999996%, Loss = 1.938987421989441
Epoch: 619, Batch Gradient Norm: 25.216319302615442
Epoch: 619, Batch Gradient Norm after: 22.36067659661425
Epoch 620/10000, Prediction Accuracy = 45.416%, Loss = 1.9361589670181274
Epoch: 620, Batch Gradient Norm: 25.62334521062477
Epoch: 620, Batch Gradient Norm after: 22.360678789619218
Epoch 621/10000, Prediction Accuracy = 45.408%, Loss = 1.9366277694702148
Epoch: 621, Batch Gradient Norm: 25.051109906783104
Epoch: 621, Batch Gradient Norm after: 22.182811398694554
Epoch 622/10000, Prediction Accuracy = 45.418%, Loss = 1.9335347175598145
Epoch: 622, Batch Gradient Norm: 25.807432382449015
Epoch: 622, Batch Gradient Norm after: 22.360678991609888
Epoch 623/10000, Prediction Accuracy = 45.42400000000001%, Loss = 1.9350592136383056
Epoch: 623, Batch Gradient Norm: 25.28081429501463
Epoch: 623, Batch Gradient Norm after: 22.36067850573031
Epoch 624/10000, Prediction Accuracy = 45.42%, Loss = 1.9321441888809203
Epoch: 624, Batch Gradient Norm: 25.718638990590136
Epoch: 624, Batch Gradient Norm after: 22.360677239504938
Epoch 625/10000, Prediction Accuracy = 45.43599999999999%, Loss = 1.9327098846435546
Epoch: 625, Batch Gradient Norm: 25.19906945615993
Epoch: 625, Batch Gradient Norm after: 22.21807142837454
Epoch 626/10000, Prediction Accuracy = 45.432%, Loss = 1.9297928094863892
Epoch: 626, Batch Gradient Norm: 25.848522751760157
Epoch: 626, Batch Gradient Norm after: 22.360676973757382
Epoch 627/10000, Prediction Accuracy = 45.444%, Loss = 1.9310014963150024
Epoch: 627, Batch Gradient Norm: 25.43649422408996
Epoch: 627, Batch Gradient Norm after: 22.358345012661225
Epoch 628/10000, Prediction Accuracy = 45.438%, Loss = 1.9284397840499878
Epoch: 628, Batch Gradient Norm: 25.7743215865666
Epoch: 628, Batch Gradient Norm after: 22.360679506620947
Epoch 629/10000, Prediction Accuracy = 45.454%, Loss = 1.9287018060684205
Epoch: 629, Batch Gradient Norm: 25.281497078268277
Epoch: 629, Batch Gradient Norm after: 22.192156666888778
Epoch 630/10000, Prediction Accuracy = 45.45399999999999%, Loss = 1.9258489608764648
Epoch: 630, Batch Gradient Norm: 25.943518070327787
Epoch: 630, Batch Gradient Norm after: 22.360677654495248
Epoch 631/10000, Prediction Accuracy = 45.446000000000005%, Loss = 1.927111601829529
Epoch: 631, Batch Gradient Norm: 25.510685608673054
Epoch: 631, Batch Gradient Norm after: 22.36067713997002
Epoch 632/10000, Prediction Accuracy = 45.456%, Loss = 1.9244738340377807
Epoch: 632, Batch Gradient Norm: 25.86556050757687
Epoch: 632, Batch Gradient Norm after: 22.360679479130138
Epoch 633/10000, Prediction Accuracy = 45.483999999999995%, Loss = 1.9248114824295044
Epoch: 633, Batch Gradient Norm: 25.422721725133897
Epoch: 633, Batch Gradient Norm after: 22.219314404346537
Epoch 634/10000, Prediction Accuracy = 45.472%, Loss = 1.9221148490905762
Epoch: 634, Batch Gradient Norm: 25.99288378326403
Epoch: 634, Batch Gradient Norm after: 22.360676007499606
Epoch 635/10000, Prediction Accuracy = 45.495999999999995%, Loss = 1.9231074333190918
Epoch: 635, Batch Gradient Norm: 25.65900362363893
Epoch: 635, Batch Gradient Norm after: 22.360676198892026
Epoch 636/10000, Prediction Accuracy = 45.470000000000006%, Loss = 1.920779013633728
Epoch: 636, Batch Gradient Norm: 25.928231708594883
Epoch: 636, Batch Gradient Norm after: 22.360676619895525
Epoch 637/10000, Prediction Accuracy = 45.506%, Loss = 1.9208583593368531
Epoch: 637, Batch Gradient Norm: 25.495237367341392
Epoch: 637, Batch Gradient Norm after: 22.191796137909133
Epoch 638/10000, Prediction Accuracy = 45.48800000000001%, Loss = 1.9181920528411864
Epoch: 638, Batch Gradient Norm: 26.095926457870664
Epoch: 638, Batch Gradient Norm after: 22.36067757313708
Epoch 639/10000, Prediction Accuracy = 45.502%, Loss = 1.9192636728286743
Epoch: 639, Batch Gradient Norm: 25.72595457377248
Epoch: 639, Batch Gradient Norm after: 22.360678339261252
Epoch 640/10000, Prediction Accuracy = 45.498000000000005%, Loss = 1.9168521165847778
Epoch: 640, Batch Gradient Norm: 26.02120993706074
Epoch: 640, Batch Gradient Norm after: 22.360677428627383
Epoch 641/10000, Prediction Accuracy = 45.518%, Loss = 1.917005681991577
Epoch: 641, Batch Gradient Norm: 25.637266413646607
Epoch: 641, Batch Gradient Norm after: 22.21986587268098
Epoch 642/10000, Prediction Accuracy = 45.504000000000005%, Loss = 1.914520788192749
Epoch: 642, Batch Gradient Norm: 26.14750386317042
Epoch: 642, Batch Gradient Norm after: 22.360678798770806
Epoch 643/10000, Prediction Accuracy = 45.51800000000001%, Loss = 1.9153281211853028
Epoch: 643, Batch Gradient Norm: 25.876982334741317
Epoch: 643, Batch Gradient Norm after: 22.360678062902412
Epoch 644/10000, Prediction Accuracy = 45.501999999999995%, Loss = 1.9132385969161987
Epoch: 644, Batch Gradient Norm: 26.082247874081265
Epoch: 644, Batch Gradient Norm after: 22.36067798409567
Epoch 645/10000, Prediction Accuracy = 45.534000000000006%, Loss = 1.9131155014038086
Epoch: 645, Batch Gradient Norm: 25.712743304776417
Epoch: 645, Batch Gradient Norm after: 22.191373833071623
Epoch 646/10000, Prediction Accuracy = 45.51%, Loss = 1.9106719255447389
Epoch: 646, Batch Gradient Norm: 26.24349357051417
Epoch: 646, Batch Gradient Norm after: 22.360679723059434
Epoch 647/10000, Prediction Accuracy = 45.534%, Loss = 1.9115462064743043
Epoch: 647, Batch Gradient Norm: 25.945227717465354
Epoch: 647, Batch Gradient Norm after: 22.360676226174537
Epoch 648/10000, Prediction Accuracy = 45.516000000000005%, Loss = 1.909350371360779
Epoch: 648, Batch Gradient Norm: 26.171854282987766
Epoch: 648, Batch Gradient Norm after: 22.360676520443526
Epoch 649/10000, Prediction Accuracy = 45.536%, Loss = 1.9093233585357665
Epoch: 649, Batch Gradient Norm: 25.863150696405004
Epoch: 649, Batch Gradient Norm after: 22.219594253373707
Epoch 650/10000, Prediction Accuracy = 45.54%, Loss = 1.9070645332336427
Epoch: 650, Batch Gradient Norm: 26.29006723067275
Epoch: 650, Batch Gradient Norm after: 22.360677449669605
Epoch 651/10000, Prediction Accuracy = 45.532%, Loss = 1.907631230354309
Epoch: 651, Batch Gradient Norm: 26.108352050711094
Epoch: 651, Batch Gradient Norm after: 22.35906452386347
Epoch 652/10000, Prediction Accuracy = 45.544%, Loss = 1.905816674232483
Epoch: 652, Batch Gradient Norm: 26.23353464429629
Epoch: 652, Batch Gradient Norm after: 22.360676677866973
Epoch 653/10000, Prediction Accuracy = 45.55200000000001%, Loss = 1.9054539442062377
Epoch: 653, Batch Gradient Norm: 25.92742862311998
Epoch: 653, Batch Gradient Norm after: 22.189067927847177
Epoch 654/10000, Prediction Accuracy = 45.57000000000001%, Loss = 1.9032161474227904
Epoch: 654, Batch Gradient Norm: 26.394942603519098
Epoch: 654, Batch Gradient Norm after: 22.36067590885197
Epoch 655/10000, Prediction Accuracy = 45.541999999999994%, Loss = 1.9038942337036133
Epoch: 655, Batch Gradient Norm: 26.154983247190746
Epoch: 655, Batch Gradient Norm after: 22.360677161801007
Epoch 656/10000, Prediction Accuracy = 45.58%, Loss = 1.901911997795105
Epoch: 656, Batch Gradient Norm: 26.331687137280525
Epoch: 656, Batch Gradient Norm after: 22.360678977956123
Epoch 657/10000, Prediction Accuracy = 45.556%, Loss = 1.9017120361328126
Epoch: 657, Batch Gradient Norm: 26.08930458522296
Epoch: 657, Batch Gradient Norm after: 22.23011307823522
Epoch 658/10000, Prediction Accuracy = 45.6%, Loss = 1.8996872425079345
Epoch: 658, Batch Gradient Norm: 26.44043771078536
Epoch: 658, Batch Gradient Norm after: 22.36067518167046
Epoch 659/10000, Prediction Accuracy = 45.568%, Loss = 1.900010848045349
Epoch: 659, Batch Gradient Norm: 26.314632528136688
Epoch: 659, Batch Gradient Norm after: 22.352364852919923
Epoch 660/10000, Prediction Accuracy = 45.604%, Loss = 1.898386263847351
Epoch: 660, Batch Gradient Norm: 26.39843499982328
Epoch: 660, Batch Gradient Norm after: 22.360678177483187
Epoch 661/10000, Prediction Accuracy = 45.598%, Loss = 1.8978964805603027
Epoch: 661, Batch Gradient Norm: 26.159014004456175
Epoch: 661, Batch Gradient Norm after: 22.20173687310455
Epoch 662/10000, Prediction Accuracy = 45.616%, Loss = 1.8958744049072265
Epoch: 662, Batch Gradient Norm: 26.54446129734435
Epoch: 662, Batch Gradient Norm after: 22.36067791568893
Epoch 663/10000, Prediction Accuracy = 45.599999999999994%, Loss = 1.896307611465454
Epoch: 663, Batch Gradient Norm: 26.378056743410863
Epoch: 663, Batch Gradient Norm after: 22.360677122571296
Epoch 664/10000, Prediction Accuracy = 45.612%, Loss = 1.8945663928985597
Epoch: 664, Batch Gradient Norm: 26.489643423349175
Epoch: 664, Batch Gradient Norm after: 22.360676583092427
Epoch 665/10000, Prediction Accuracy = 45.62%, Loss = 1.8941681623458861
Epoch: 665, Batch Gradient Norm: 26.312475220491
Epoch: 665, Batch Gradient Norm after: 22.23478324635137
Epoch 666/10000, Prediction Accuracy = 45.62%, Loss = 1.8923645496368409
Epoch: 666, Batch Gradient Norm: 26.595712315557197
Epoch: 666, Batch Gradient Norm after: 22.360677500740618
Epoch 667/10000, Prediction Accuracy = 45.644%, Loss = 1.8924635171890258
Epoch: 667, Batch Gradient Norm: 26.543033069347132
Epoch: 667, Batch Gradient Norm after: 22.357829228505505
Epoch 668/10000, Prediction Accuracy = 45.616%, Loss = 1.891104030609131
Epoch: 668, Batch Gradient Norm: 26.560345794514998
Epoch: 668, Batch Gradient Norm after: 22.360677166307113
Epoch 669/10000, Prediction Accuracy = 45.660000000000004%, Loss = 1.8903643846511842
Epoch: 669, Batch Gradient Norm: 26.382033600416722
Epoch: 669, Batch Gradient Norm after: 22.205341029407005
Epoch 670/10000, Prediction Accuracy = 45.636%, Loss = 1.8885843276977539
Epoch: 670, Batch Gradient Norm: 26.69879744768419
Epoch: 670, Batch Gradient Norm after: 22.36067808977054
Epoch 671/10000, Prediction Accuracy = 45.66799999999999%, Loss = 1.8887791156768798
Epoch: 671, Batch Gradient Norm: 26.60949405881926
Epoch: 671, Batch Gradient Norm after: 22.3606787519138
Epoch 672/10000, Prediction Accuracy = 45.642%, Loss = 1.8873206853866578
Epoch: 672, Batch Gradient Norm: 26.649639479495537
Epoch: 672, Batch Gradient Norm after: 22.360674733583178
Epoch 673/10000, Prediction Accuracy = 45.678000000000004%, Loss = 1.8866769313812255
Epoch: 673, Batch Gradient Norm: 26.54134732888891
Epoch: 673, Batch Gradient Norm after: 22.236776754013583
Epoch 674/10000, Prediction Accuracy = 45.64200000000001%, Loss = 1.8851134777069092
Epoch: 674, Batch Gradient Norm: 26.75030902650179
Epoch: 674, Batch Gradient Norm after: 22.36067551119779
Epoch 675/10000, Prediction Accuracy = 45.676%, Loss = 1.8849888086318969
Epoch: 675, Batch Gradient Norm: 26.77402321371154
Epoch: 675, Batch Gradient Norm after: 22.360677285678815
Epoch 676/10000, Prediction Accuracy = 45.65%, Loss = 1.8838715076446533
Epoch: 676, Batch Gradient Norm: 26.715892827141484
Epoch: 676, Batch Gradient Norm after: 22.36067713236114
Epoch 677/10000, Prediction Accuracy = 45.704%, Loss = 1.8829297304153443
Epoch: 677, Batch Gradient Norm: 26.618478994554703
Epoch: 677, Batch Gradient Norm after: 22.209553148112295
Epoch 678/10000, Prediction Accuracy = 45.666000000000004%, Loss = 1.8813769340515136
Epoch: 678, Batch Gradient Norm: 26.849588339583757
Epoch: 678, Batch Gradient Norm after: 22.360678013419367
Epoch 679/10000, Prediction Accuracy = 45.722%, Loss = 1.8813193082809447
Epoch: 679, Batch Gradient Norm: 26.835985121910248
Epoch: 679, Batch Gradient Norm after: 22.36067816028303
Epoch 680/10000, Prediction Accuracy = 45.67400000000001%, Loss = 1.880092740058899
Epoch: 680, Batch Gradient Norm: 26.806716943698255
Epoch: 680, Batch Gradient Norm after: 22.360677072472573
Epoch 681/10000, Prediction Accuracy = 45.74%, Loss = 1.8792456150054933
Epoch: 681, Batch Gradient Norm: 26.76554370822901
Epoch: 681, Batch Gradient Norm after: 22.237223703088333
Epoch 682/10000, Prediction Accuracy = 45.682%, Loss = 1.8779067754745484
Epoch: 682, Batch Gradient Norm: 26.906932270350982
Epoch: 682, Batch Gradient Norm after: 22.360677278002576
Epoch 683/10000, Prediction Accuracy = 45.74400000000001%, Loss = 1.8775597095489502
Epoch: 683, Batch Gradient Norm: 26.996309846136516
Epoch: 683, Batch Gradient Norm after: 22.360677811800244
Epoch 684/10000, Prediction Accuracy = 45.69799999999999%, Loss = 1.8766831398010253
Epoch: 684, Batch Gradient Norm: 26.87622367210509
Epoch: 684, Batch Gradient Norm after: 22.360677374355095
Epoch 685/10000, Prediction Accuracy = 45.762%, Loss = 1.8755236864089966
Epoch: 685, Batch Gradient Norm: 26.841752155437696
Epoch: 685, Batch Gradient Norm after: 22.211562993376752
Epoch 686/10000, Prediction Accuracy = 45.706%, Loss = 1.8742046356201172
Epoch: 686, Batch Gradient Norm: 27.00523192106471
Epoch: 686, Batch Gradient Norm after: 22.36067841276798
Epoch 687/10000, Prediction Accuracy = 45.764%, Loss = 1.8739239931106568
Epoch: 687, Batch Gradient Norm: 27.053728143400704
Epoch: 687, Batch Gradient Norm after: 22.360677129307035
Epoch 688/10000, Prediction Accuracy = 45.714%, Loss = 1.8729228496551513
Epoch: 688, Batch Gradient Norm: 26.969838860408398
Epoch: 688, Batch Gradient Norm after: 22.360679523301563
Epoch 689/10000, Prediction Accuracy = 45.784000000000006%, Loss = 1.871890950202942
Epoch: 689, Batch Gradient Norm: 27.007313221524434
Epoch: 689, Batch Gradient Norm after: 22.24824233849213
Epoch 690/10000, Prediction Accuracy = 45.75599999999999%, Loss = 1.870812964439392
Epoch: 690, Batch Gradient Norm: 27.05928383332604
Epoch: 690, Batch Gradient Norm after: 22.36067847297667
Epoch 691/10000, Prediction Accuracy = 45.785999999999994%, Loss = 1.8701877117156982
Epoch: 691, Batch Gradient Norm: 27.224422602082242
Epoch: 691, Batch Gradient Norm after: 22.35562780342232
Epoch 692/10000, Prediction Accuracy = 45.774%, Loss = 1.8695467472076417
Epoch: 692, Batch Gradient Norm: 27.041032342433642
Epoch: 692, Batch Gradient Norm after: 22.360677923796043
Epoch 693/10000, Prediction Accuracy = 45.818%, Loss = 1.8681950092315673
Epoch: 693, Batch Gradient Norm: 27.080001952927493
Epoch: 693, Batch Gradient Norm after: 22.22168086384542
Epoch 694/10000, Prediction Accuracy = 45.790000000000006%, Loss = 1.8671283006668091
Epoch: 694, Batch Gradient Norm: 27.158349468586437
Epoch: 694, Batch Gradient Norm after: 22.3606771614542
Epoch 695/10000, Prediction Accuracy = 45.836%, Loss = 1.8665766716003418
Epoch: 695, Batch Gradient Norm: 27.2879369196911
Epoch: 695, Batch Gradient Norm after: 22.360676871095727
Epoch 696/10000, Prediction Accuracy = 45.803999999999995%, Loss = 1.865836763381958
Epoch: 696, Batch Gradient Norm: 27.133126450492057
Epoch: 696, Batch Gradient Norm after: 22.360675887117456
Epoch 697/10000, Prediction Accuracy = 45.844%, Loss = 1.8645720958709717
Epoch: 697, Batch Gradient Norm: 27.239743140069656
Epoch: 697, Batch Gradient Norm after: 22.250693141594525
Epoch 698/10000, Prediction Accuracy = 45.815999999999995%, Loss = 1.863735294342041
Epoch: 698, Batch Gradient Norm: 27.219190544110024
Epoch: 698, Batch Gradient Norm after: 22.360675493915764
Epoch 699/10000, Prediction Accuracy = 45.838%, Loss = 1.8628578424453734
Epoch: 699, Batch Gradient Norm: 27.457188249581755
Epoch: 699, Batch Gradient Norm after: 22.35699785534781
Epoch 700/10000, Prediction Accuracy = 45.824%, Loss = 1.862483286857605
Epoch: 700, Batch Gradient Norm: 27.204731034218952
Epoch: 700, Batch Gradient Norm after: 22.360677291640933
Epoch 701/10000, Prediction Accuracy = 45.852%, Loss = 1.8608997106552123
Epoch: 701, Batch Gradient Norm: 27.31601133643921
Epoch: 701, Batch Gradient Norm after: 22.224356192142906
Epoch 702/10000, Prediction Accuracy = 45.82600000000001%, Loss = 1.860077714920044
Epoch: 702, Batch Gradient Norm: 27.316183717027574
Epoch: 702, Batch Gradient Norm after: 22.360679674555286
Epoch 703/10000, Prediction Accuracy = 45.872%, Loss = 1.8592684745788575
Epoch: 703, Batch Gradient Norm: 27.52030036321965
Epoch: 703, Batch Gradient Norm after: 22.36067693693051
Epoch 704/10000, Prediction Accuracy = 45.846%, Loss = 1.8587920665740967
Epoch: 704, Batch Gradient Norm: 27.295189009164417
Epoch: 704, Batch Gradient Norm after: 22.360678042704304
Epoch 705/10000, Prediction Accuracy = 45.884%, Loss = 1.857305932044983
Epoch: 705, Batch Gradient Norm: 27.486612415096793
Epoch: 705, Batch Gradient Norm after: 22.261390885179157
Epoch 706/10000, Prediction Accuracy = 45.856%, Loss = 1.8567520141601563
Epoch: 706, Batch Gradient Norm: 27.372405111135446
Epoch: 706, Batch Gradient Norm after: 22.360677165653037
Epoch 707/10000, Prediction Accuracy = 45.89%, Loss = 1.8555772304534912
Epoch: 707, Batch Gradient Norm: 27.68743076953088
Epoch: 707, Batch Gradient Norm after: 22.353408635181832
Epoch 708/10000, Prediction Accuracy = 45.866%, Loss = 1.8554694652557373
Epoch: 708, Batch Gradient Norm: 27.370462213152315
Epoch: 708, Batch Gradient Norm after: 22.360676785114126
Epoch 709/10000, Prediction Accuracy = 45.88799999999999%, Loss = 1.853659462928772
Epoch: 709, Batch Gradient Norm: 27.54123754362638
Epoch: 709, Batch Gradient Norm after: 22.22547054593853
Epoch 710/10000, Prediction Accuracy = 45.876%, Loss = 1.8530670166015626
Epoch: 710, Batch Gradient Norm: 27.47624890512727
Epoch: 710, Batch Gradient Norm after: 22.36067633185109
Epoch 711/10000, Prediction Accuracy = 45.89%, Loss = 1.852027416229248
Epoch: 711, Batch Gradient Norm: 27.750994562801605
Epoch: 711, Batch Gradient Norm after: 22.360678408886987
Epoch 712/10000, Prediction Accuracy = 45.891999999999996%, Loss = 1.8518170833587646
Epoch: 712, Batch Gradient Norm: 27.459424779251346
Epoch: 712, Batch Gradient Norm after: 22.3606788095692
Epoch 713/10000, Prediction Accuracy = 45.908%, Loss = 1.850086236000061
Epoch: 713, Batch Gradient Norm: 27.696361488732634
Epoch: 713, Batch Gradient Norm after: 22.25384746257768
Epoch 714/10000, Prediction Accuracy = 45.882%, Loss = 1.8497330665588378
Epoch: 714, Batch Gradient Norm: 27.540113439048266
Epoch: 714, Batch Gradient Norm after: 22.360679125908995
Epoch 715/10000, Prediction Accuracy = 45.912000000000006%, Loss = 1.8484063386917113
Epoch: 715, Batch Gradient Norm: 27.916438267525173
Epoch: 715, Batch Gradient Norm after: 22.360679239879683
Epoch 716/10000, Prediction Accuracy = 45.896%, Loss = 1.8485135078430175
Epoch: 716, Batch Gradient Norm: 27.530854002417662
Epoch: 716, Batch Gradient Norm after: 22.36067987667056
Epoch 717/10000, Prediction Accuracy = 45.922%, Loss = 1.8465001106262207
Epoch: 717, Batch Gradient Norm: 27.76463870670697
Epoch: 717, Batch Gradient Norm after: 22.2259269906705
Epoch 718/10000, Prediction Accuracy = 45.9%, Loss = 1.846120548248291
Epoch: 718, Batch Gradient Norm: 27.635549910065684
Epoch: 718, Batch Gradient Norm after: 22.360678711040137
Epoch 719/10000, Prediction Accuracy = 45.932%, Loss = 1.8448827028274537
Epoch: 719, Batch Gradient Norm: 27.970619683142335
Epoch: 719, Batch Gradient Norm after: 22.360678603940872
Epoch 720/10000, Prediction Accuracy = 45.92%, Loss = 1.8448771715164185
Epoch: 720, Batch Gradient Norm: 27.621737274995752
Epoch: 720, Batch Gradient Norm after: 22.3606764456563
Epoch 721/10000, Prediction Accuracy = 45.95%, Loss = 1.8429739713668822
Epoch: 721, Batch Gradient Norm: 27.933088407360476
Epoch: 721, Batch Gradient Norm after: 22.259505601639873
Epoch 722/10000, Prediction Accuracy = 45.92%, Loss = 1.8428471326828002
Epoch: 722, Batch Gradient Norm: 27.693581886822273
Epoch: 722, Batch Gradient Norm after: 22.360677286871347
Epoch 723/10000, Prediction Accuracy = 45.948%, Loss = 1.8412842512130738
Epoch: 723, Batch Gradient Norm: 28.15714685996546
Epoch: 723, Batch Gradient Norm after: 22.3606788747678
Epoch 724/10000, Prediction Accuracy = 45.924%, Loss = 1.8416651487350464
Epoch: 724, Batch Gradient Norm: 27.69190348750753
Epoch: 724, Batch Gradient Norm after: 22.360678129740442
Epoch 725/10000, Prediction Accuracy = 45.944%, Loss = 1.8394064426422119
Epoch: 725, Batch Gradient Norm: 27.99492782289963
Epoch: 725, Batch Gradient Norm after: 22.22395944823758
Epoch 726/10000, Prediction Accuracy = 45.926%, Loss = 1.8392293453216553
Epoch: 726, Batch Gradient Norm: 27.795632221556737
Epoch: 726, Batch Gradient Norm after: 22.360678524699704
Epoch 727/10000, Prediction Accuracy = 45.95%, Loss = 1.837795114517212
Epoch: 727, Batch Gradient Norm: 28.179664459643817
Epoch: 727, Batch Gradient Norm after: 22.36067764208706
Epoch 728/10000, Prediction Accuracy = 45.92999999999999%, Loss = 1.8379210472106933
Epoch: 728, Batch Gradient Norm: 27.789231477134212
Epoch: 728, Batch Gradient Norm after: 22.360678030911412
Epoch 729/10000, Prediction Accuracy = 45.95399999999999%, Loss = 1.8359230518341065
Epoch: 729, Batch Gradient Norm: 28.20116177985089
Epoch: 729, Batch Gradient Norm after: 22.282368856101797
Epoch 730/10000, Prediction Accuracy = 45.946000000000005%, Loss = 1.8361225605010987
Epoch: 730, Batch Gradient Norm: 27.8476135258291
Epoch: 730, Batch Gradient Norm after: 22.360679097432648
Epoch 731/10000, Prediction Accuracy = 45.958000000000006%, Loss = 1.8342026710510253
Epoch: 731, Batch Gradient Norm: 28.35608818785949
Epoch: 731, Batch Gradient Norm after: 22.340507991633082
Epoch 732/10000, Prediction Accuracy = 45.958000000000006%, Loss = 1.8347370624542236
Epoch: 732, Batch Gradient Norm: 27.868152027546238
Epoch: 732, Batch Gradient Norm after: 22.360679077112373
Epoch 733/10000, Prediction Accuracy = 45.96%, Loss = 1.8323897123336792
Epoch: 733, Batch Gradient Norm: 28.283981128779878
Epoch: 733, Batch Gradient Norm after: 22.268455965711958
Epoch 734/10000, Prediction Accuracy = 45.964%, Loss = 1.8326307535171509
Epoch: 734, Batch Gradient Norm: 27.9387518345022
Epoch: 734, Batch Gradient Norm after: 22.36067807772584
Epoch 735/10000, Prediction Accuracy = 45.962%, Loss = 1.8307126760482788
Epoch: 735, Batch Gradient Norm: 28.466255698040413
Epoch: 735, Batch Gradient Norm after: 22.36067796760317
Epoch 736/10000, Prediction Accuracy = 45.974%, Loss = 1.8313451766967774
Epoch: 736, Batch Gradient Norm: 27.948112357084597
Epoch: 736, Batch Gradient Norm after: 22.360678584701326
Epoch 737/10000, Prediction Accuracy = 45.974000000000004%, Loss = 1.828887391090393
Epoch: 737, Batch Gradient Norm: 28.392393718379974
Epoch: 737, Batch Gradient Norm after: 22.2660460750978
Epoch 738/10000, Prediction Accuracy = 45.97%, Loss = 1.829232358932495
Epoch: 738, Batch Gradient Norm: 28.021768606720965
Epoch: 738, Batch Gradient Norm after: 22.360678944083702
Epoch 739/10000, Prediction Accuracy = 46.0%, Loss = 1.8272249221801757
Epoch: 739, Batch Gradient Norm: 28.578158089530106
Epoch: 739, Batch Gradient Norm after: 22.36067917535964
Epoch 740/10000, Prediction Accuracy = 45.96%, Loss = 1.8279606342315673
Epoch: 740, Batch Gradient Norm: 28.031567771299773
Epoch: 740, Batch Gradient Norm after: 22.36067907545537
Epoch 741/10000, Prediction Accuracy = 46.0%, Loss = 1.825406837463379
Epoch: 741, Batch Gradient Norm: 28.513812140700022
Epoch: 741, Batch Gradient Norm after: 22.271023208602845
Epoch 742/10000, Prediction Accuracy = 45.971999999999994%, Loss = 1.8258912086486816
Epoch: 742, Batch Gradient Norm: 28.10027853897359
Epoch: 742, Batch Gradient Norm after: 22.360678736586795
Epoch 743/10000, Prediction Accuracy = 46.018%, Loss = 1.823746347427368
Epoch: 743, Batch Gradient Norm: 28.694506569943254
Epoch: 743, Batch Gradient Norm after: 22.36067925614011
Epoch 744/10000, Prediction Accuracy = 45.984%, Loss = 1.824614906311035
Epoch: 744, Batch Gradient Norm: 28.11174235923004
Epoch: 744, Batch Gradient Norm after: 22.360679051962713
Epoch 745/10000, Prediction Accuracy = 46.016000000000005%, Loss = 1.8219502687454223
Epoch: 745, Batch Gradient Norm: 28.612220401950683
Epoch: 745, Batch Gradient Norm after: 22.263609198658184
Epoch 746/10000, Prediction Accuracy = 45.998000000000005%, Loss = 1.8224886417388917
Epoch: 746, Batch Gradient Norm: 28.18833057135498
Epoch: 746, Batch Gradient Norm after: 22.360677394495582
Epoch 747/10000, Prediction Accuracy = 46.024%, Loss = 1.820311999320984
Epoch: 747, Batch Gradient Norm: 28.79067931294322
Epoch: 747, Batch Gradient Norm after: 22.360678788106014
Epoch 748/10000, Prediction Accuracy = 45.998000000000005%, Loss = 1.821203923225403
Epoch: 748, Batch Gradient Norm: 28.197562824837288
Epoch: 748, Batch Gradient Norm after: 22.360678776463423
Epoch 749/10000, Prediction Accuracy = 46.029999999999994%, Loss = 1.8185213088989258
Epoch: 749, Batch Gradient Norm: 28.749735328564924
Epoch: 749, Batch Gradient Norm after: 22.27725815423678
Epoch 750/10000, Prediction Accuracy = 45.994%, Loss = 1.8192431926727295
Epoch: 750, Batch Gradient Norm: 28.259699311957206
Epoch: 750, Batch Gradient Norm after: 22.36067792251445
Epoch 751/10000, Prediction Accuracy = 46.034000000000006%, Loss = 1.8168636083602905
Epoch: 751, Batch Gradient Norm: 28.939357389971732
Epoch: 751, Batch Gradient Norm after: 22.36067811381105
Epoch 752/10000, Prediction Accuracy = 46.002%, Loss = 1.8180247306823731
Epoch: 752, Batch Gradient Norm: 28.270912093705405
Epoch: 752, Batch Gradient Norm after: 22.36067641769123
Epoch 753/10000, Prediction Accuracy = 46.034%, Loss = 1.8150941133499146
Epoch: 753, Batch Gradient Norm: 28.825855547583277
Epoch: 753, Batch Gradient Norm after: 22.258085877038244
Epoch 754/10000, Prediction Accuracy = 46.010000000000005%, Loss = 1.815842628479004
Epoch: 754, Batch Gradient Norm: 28.347028211382664
Epoch: 754, Batch Gradient Norm after: 22.36067906832552
Epoch 755/10000, Prediction Accuracy = 46.034%, Loss = 1.8134679794311523
Epoch: 755, Batch Gradient Norm: 28.99134220473798
Epoch: 755, Batch Gradient Norm after: 22.36067781237195
Epoch 756/10000, Prediction Accuracy = 46.012%, Loss = 1.8145434379577636
Epoch: 756, Batch Gradient Norm: 28.357214600852895
Epoch: 756, Batch Gradient Norm after: 22.360677473598656
Epoch 757/10000, Prediction Accuracy = 46.054%, Loss = 1.8117117166519165
Epoch: 757, Batch Gradient Norm: 29.004369289176804
Epoch: 757, Batch Gradient Norm after: 22.297508988557873
Epoch 758/10000, Prediction Accuracy = 46.010000000000005%, Loss = 1.8127673864364624
Epoch: 758, Batch Gradient Norm: 28.4078853786793
Epoch: 758, Batch Gradient Norm after: 22.360677875017558
Epoch 759/10000, Prediction Accuracy = 46.065999999999995%, Loss = 1.8100517749786378
Epoch: 759, Batch Gradient Norm: 29.15426501062814
Epoch: 759, Batch Gradient Norm after: 22.34981331116862
Epoch 760/10000, Prediction Accuracy = 46.019999999999996%, Loss = 1.8114261627197266
Epoch: 760, Batch Gradient Norm: 28.43138469924483
Epoch: 760, Batch Gradient Norm after: 22.360676894426195
Epoch 761/10000, Prediction Accuracy = 46.071999999999996%, Loss = 1.808328628540039
Epoch: 761, Batch Gradient Norm: 29.07782550544089
Epoch: 761, Batch Gradient Norm after: 22.274368140252957
Epoch 762/10000, Prediction Accuracy = 46.028%, Loss = 1.8093566179275513
Epoch: 762, Batch Gradient Norm: 28.495276212775657
Epoch: 762, Batch Gradient Norm after: 22.36067801058347
Epoch 763/10000, Prediction Accuracy = 46.084%, Loss = 1.8066871881484985
Epoch: 763, Batch Gradient Norm: 29.261110125171292
Epoch: 763, Batch Gradient Norm after: 22.360679948420884
Epoch 764/10000, Prediction Accuracy = 46.029999999999994%, Loss = 1.8081318378448485
Epoch: 764, Batch Gradient Norm: 28.50930376096784
Epoch: 764, Batch Gradient Norm after: 22.360677885244485
Epoch 765/10000, Prediction Accuracy = 46.092%, Loss = 1.804940128326416
Epoch: 765, Batch Gradient Norm: 29.180081950276357
Epoch: 765, Batch Gradient Norm after: 22.265252006283415
Epoch 766/10000, Prediction Accuracy = 46.052%, Loss = 1.806072473526001
Epoch: 766, Batch Gradient Norm: 28.573473652732282
Epoch: 766, Batch Gradient Norm after: 22.36067642403669
Epoch 767/10000, Prediction Accuracy = 46.102%, Loss = 1.8033145904541015
Epoch: 767, Batch Gradient Norm: 29.374653070055764
Epoch: 767, Batch Gradient Norm after: 22.360677550396066
Epoch 768/10000, Prediction Accuracy = 46.064%, Loss = 1.804885458946228
Epoch: 768, Batch Gradient Norm: 28.588001664913364
Epoch: 768, Batch Gradient Norm after: 22.36068095348535
Epoch 769/10000, Prediction Accuracy = 46.102%, Loss = 1.801580047607422
Epoch: 769, Batch Gradient Norm: 29.287188625128486
Epoch: 769, Batch Gradient Norm after: 22.261205884575965
Epoch 770/10000, Prediction Accuracy = 46.065999999999995%, Loss = 1.8028019189834594
Epoch: 770, Batch Gradient Norm: 28.653583744840844
Epoch: 770, Batch Gradient Norm after: 22.36067797614054
Epoch 771/10000, Prediction Accuracy = 46.10600000000001%, Loss = 1.7999443769454957
Epoch: 771, Batch Gradient Norm: 29.483865067840338
Epoch: 771, Batch Gradient Norm after: 22.360677619136503
Epoch 772/10000, Prediction Accuracy = 46.077999999999996%, Loss = 1.801628589630127
Epoch: 772, Batch Gradient Norm: 28.67044096363274
Epoch: 772, Batch Gradient Norm after: 22.360677648982566
Epoch 773/10000, Prediction Accuracy = 46.108000000000004%, Loss = 1.7982245683670044
Epoch: 773, Batch Gradient Norm: 29.40847361829542
Epoch: 773, Batch Gradient Norm after: 22.263234692606595
Epoch 774/10000, Prediction Accuracy = 46.086%, Loss = 1.7995856523513794
Epoch: 774, Batch Gradient Norm: 28.728598803302635
Epoch: 774, Batch Gradient Norm after: 22.360677457192097
Epoch 775/10000, Prediction Accuracy = 46.120000000000005%, Loss = 1.7965951204299926
Epoch: 775, Batch Gradient Norm: 29.609775741544194
Epoch: 775, Batch Gradient Norm after: 22.360676583851976
Epoch 776/10000, Prediction Accuracy = 46.092%, Loss = 1.7984225511550904
Epoch: 776, Batch Gradient Norm: 28.746933643427045
Epoch: 776, Batch Gradient Norm after: 22.360679146028982
Epoch 777/10000, Prediction Accuracy = 46.138%, Loss = 1.7948833227157592
Epoch: 777, Batch Gradient Norm: 29.521960414435995
Epoch: 777, Batch Gradient Norm after: 22.259230763892774
Epoch 778/10000, Prediction Accuracy = 46.10799999999999%, Loss = 1.796336817741394
Epoch: 778, Batch Gradient Norm: 28.808952909852735
Epoch: 778, Batch Gradient Norm after: 22.360675281004983
Epoch 779/10000, Prediction Accuracy = 46.15%, Loss = 1.7932456493377686
Epoch: 779, Batch Gradient Norm: 29.720090446942468
Epoch: 779, Batch Gradient Norm after: 22.36067877673971
Epoch 780/10000, Prediction Accuracy = 46.116%, Loss = 1.7951876163482665
Epoch: 780, Batch Gradient Norm: 28.82744892867807
Epoch: 780, Batch Gradient Norm after: 22.36067711288082
Epoch 781/10000, Prediction Accuracy = 46.153999999999996%, Loss = 1.7915447473526
Epoch: 781, Batch Gradient Norm: 29.651770395502723
Epoch: 781, Batch Gradient Norm after: 22.266576149108186
Epoch 782/10000, Prediction Accuracy = 46.124%, Loss = 1.7931827068328858
Epoch: 782, Batch Gradient Norm: 28.887523378534635
Epoch: 782, Batch Gradient Norm after: 22.360676475362013
Epoch 783/10000, Prediction Accuracy = 46.158%, Loss = 1.7899080038070678
Epoch: 783, Batch Gradient Norm: 29.847190733778508
Epoch: 783, Batch Gradient Norm after: 22.360678498385447
Epoch 784/10000, Prediction Accuracy = 46.129999999999995%, Loss = 1.7920145273208619
Epoch: 784, Batch Gradient Norm: 28.90844500164295
Epoch: 784, Batch Gradient Norm after: 22.360676544424127
Epoch 785/10000, Prediction Accuracy = 46.164%, Loss = 1.7882282972335815
Epoch: 785, Batch Gradient Norm: 29.770957921824298
Epoch: 785, Batch Gradient Norm after: 22.267598059830398
Epoch 786/10000, Prediction Accuracy = 46.144%, Loss = 1.789983582496643
Epoch: 786, Batch Gradient Norm: 28.967897350291462
Epoch: 786, Batch Gradient Norm after: 22.36067504526396
Epoch 787/10000, Prediction Accuracy = 46.162%, Loss = 1.7866105794906617
Epoch: 787, Batch Gradient Norm: 29.962312902937704
Epoch: 787, Batch Gradient Norm after: 22.360676949971904
Epoch 788/10000, Prediction Accuracy = 46.144%, Loss = 1.7888245344161988
Epoch: 788, Batch Gradient Norm: 28.98946237189417
Epoch: 788, Batch Gradient Norm after: 22.360676579856463
Epoch 789/10000, Prediction Accuracy = 46.164%, Loss = 1.7849513292312622
Epoch: 789, Batch Gradient Norm: 29.883028734389406
Epoch: 789, Batch Gradient Norm after: 22.267117970164186
Epoch 790/10000, Prediction Accuracy = 46.153999999999996%, Loss = 1.7868139505386353
Epoch: 790, Batch Gradient Norm: 29.047554354265202
Epoch: 790, Batch Gradient Norm after: 22.360675546529276
Epoch 791/10000, Prediction Accuracy = 46.174%, Loss = 1.7833254098892213
Epoch: 791, Batch Gradient Norm: 30.066076734629352
Epoch: 791, Batch Gradient Norm after: 22.36067851259767
Epoch 792/10000, Prediction Accuracy = 46.178000000000004%, Loss = 1.7856399059295653
Epoch: 792, Batch Gradient Norm: 29.071849702910068
Epoch: 792, Batch Gradient Norm after: 22.36067452370615
Epoch 793/10000, Prediction Accuracy = 46.192%, Loss = 1.7816478490829468
Epoch: 793, Batch Gradient Norm: 30.01978363869801
Epoch: 793, Batch Gradient Norm after: 22.276034273785292
Epoch 794/10000, Prediction Accuracy = 46.19%, Loss = 1.7837046146392823
Epoch: 794, Batch Gradient Norm: 29.12387385178969
Epoch: 794, Batch Gradient Norm after: 22.360676761397798
Epoch 795/10000, Prediction Accuracy = 46.192%, Loss = 1.7800323009490966
Epoch: 795, Batch Gradient Norm: 30.219742082092324
Epoch: 795, Batch Gradient Norm after: 22.360680922917034
Epoch 796/10000, Prediction Accuracy = 46.196%, Loss = 1.7825962066650392
Epoch: 796, Batch Gradient Norm: 29.154922572860645
Epoch: 796, Batch Gradient Norm after: 22.360679311345955
Epoch 797/10000, Prediction Accuracy = 46.217999999999996%, Loss = 1.778382706642151
Epoch: 797, Batch Gradient Norm: 30.08475128085415
Epoch: 797, Batch Gradient Norm after: 22.251464228398856
Epoch 798/10000, Prediction Accuracy = 46.212%, Loss = 1.780390739440918
Epoch: 798, Batch Gradient Norm: 29.211859402724965
Epoch: 798, Batch Gradient Norm after: 22.360674453917536
Epoch 799/10000, Prediction Accuracy = 46.22599999999999%, Loss = 1.7767567873001098
Epoch: 799, Batch Gradient Norm: 30.26485151513605
Epoch: 799, Batch Gradient Norm after: 22.36067817174442
Epoch 800/10000, Prediction Accuracy = 46.208000000000006%, Loss = 1.7792087078094483
Epoch: 800, Batch Gradient Norm: 29.236323044147518
Epoch: 800, Batch Gradient Norm after: 22.360675127718206
Epoch 801/10000, Prediction Accuracy = 46.236000000000004%, Loss = 1.775119924545288
Epoch: 801, Batch Gradient Norm: 30.269973700778536
Epoch: 801, Batch Gradient Norm after: 22.287390671336855
Epoch 802/10000, Prediction Accuracy = 46.234%, Loss = 1.7774590015411378
Epoch: 802, Batch Gradient Norm: 29.285228296071082
Epoch: 802, Batch Gradient Norm after: 22.360676602679987
Epoch 803/10000, Prediction Accuracy = 46.246%, Loss = 1.7735060453414917
Epoch: 803, Batch Gradient Norm: 30.462640605490655
Epoch: 803, Batch Gradient Norm after: 22.35608707443515
Epoch 804/10000, Prediction Accuracy = 46.246%, Loss = 1.7763312339782715
Epoch: 804, Batch Gradient Norm: 29.319913123195377
Epoch: 804, Batch Gradient Norm after: 22.360674739494765
Epoch 805/10000, Prediction Accuracy = 46.266%, Loss = 1.7718918323516846
Epoch: 805, Batch Gradient Norm: 30.316958259633306
Epoch: 805, Batch Gradient Norm after: 22.249127518897243
Epoch 806/10000, Prediction Accuracy = 46.252%, Loss = 1.7740931272506715
Epoch: 806, Batch Gradient Norm: 29.37352652184717
Epoch: 806, Batch Gradient Norm after: 22.36067860738516
Epoch 807/10000, Prediction Accuracy = 46.268%, Loss = 1.7702773094177247
Epoch: 807, Batch Gradient Norm: 30.476480555299567
Epoch: 807, Batch Gradient Norm after: 22.36067989764383
Epoch 808/10000, Prediction Accuracy = 46.274%, Loss = 1.772852349281311
Epoch: 808, Batch Gradient Norm: 29.402566574000282
Epoch: 808, Batch Gradient Norm after: 22.360676485184133
Epoch 809/10000, Prediction Accuracy = 46.272000000000006%, Loss = 1.768645715713501
Epoch: 809, Batch Gradient Norm: 30.56032380789078
Epoch: 809, Batch Gradient Norm after: 22.31846846737567
Epoch 810/10000, Prediction Accuracy = 46.275999999999996%, Loss = 1.7713949918746947
Epoch: 810, Batch Gradient Norm: 29.44394258120171
Epoch: 810, Batch Gradient Norm after: 22.360679490744236
Epoch 811/10000, Prediction Accuracy = 46.282%, Loss = 1.7670377731323241
Epoch: 811, Batch Gradient Norm: 30.629757754646267
Epoch: 811, Batch Gradient Norm after: 22.32584836709129
Epoch 812/10000, Prediction Accuracy = 46.282%, Loss = 1.7698786497116088
Epoch: 812, Batch Gradient Norm: 29.48317237729384
Epoch: 812, Batch Gradient Norm after: 22.36067712013663
Epoch 813/10000, Prediction Accuracy = 46.286%, Loss = 1.7654299020767212
Epoch: 813, Batch Gradient Norm: 30.667085744278452
Epoch: 813, Batch Gradient Norm after: 22.315844733453723
Epoch 814/10000, Prediction Accuracy = 46.286%, Loss = 1.7682494878768922
Epoch: 814, Batch Gradient Norm: 29.52487752990844
Epoch: 814, Batch Gradient Norm after: 22.360676468053498
Epoch 815/10000, Prediction Accuracy = 46.284000000000006%, Loss = 1.763825535774231
Epoch: 815, Batch Gradient Norm: 30.766933981934375
Epoch: 815, Batch Gradient Norm after: 22.337208058189816
Epoch 816/10000, Prediction Accuracy = 46.288%, Loss = 1.7668327331542968
Epoch: 816, Batch Gradient Norm: 29.56372921872804
Epoch: 816, Batch Gradient Norm after: 22.360676758794174
Epoch 817/10000, Prediction Accuracy = 46.30800000000001%, Loss = 1.7622066974639892
Epoch: 817, Batch Gradient Norm: 30.7691339401122
Epoch: 817, Batch Gradient Norm after: 22.309065646764914
Epoch 818/10000, Prediction Accuracy = 46.291999999999994%, Loss = 1.7650964260101318
Epoch: 818, Batch Gradient Norm: 29.608028724755123
Epoch: 818, Batch Gradient Norm after: 22.36067520559462
Epoch 819/10000, Prediction Accuracy = 46.314%, Loss = 1.7605903625488282
Epoch: 819, Batch Gradient Norm: 30.91488602107888
Epoch: 819, Batch Gradient Norm after: 22.351320163395656
Epoch 820/10000, Prediction Accuracy = 46.314%, Loss = 1.7638308525085449
Epoch: 820, Batch Gradient Norm: 29.644869313699406
Epoch: 820, Batch Gradient Norm after: 22.360679429351322
Epoch 821/10000, Prediction Accuracy = 46.331999999999994%, Loss = 1.758991551399231
Epoch: 821, Batch Gradient Norm: 30.836898696616995
Epoch: 821, Batch Gradient Norm after: 22.287145018485496
Epoch 822/10000, Prediction Accuracy = 46.312%, Loss = 1.761862325668335
Epoch: 822, Batch Gradient Norm: 29.69207830063868
Epoch: 822, Batch Gradient Norm after: 22.360677642303767
Epoch 823/10000, Prediction Accuracy = 46.328%, Loss = 1.7573918104171753
Epoch: 823, Batch Gradient Norm: 30.98471841014779
Epoch: 823, Batch Gradient Norm after: 22.36067873039374
Epoch 824/10000, Prediction Accuracy = 46.324%, Loss = 1.7606151580810547
Epoch: 824, Batch Gradient Norm: 29.727418192724716
Epoch: 824, Batch Gradient Norm after: 22.360677783950695
Epoch 825/10000, Prediction Accuracy = 46.34%, Loss = 1.7558199644088746
Epoch: 825, Batch Gradient Norm: 31.006439173457967
Epoch: 825, Batch Gradient Norm after: 22.31199283245398
Epoch 826/10000, Prediction Accuracy = 46.318%, Loss = 1.7589639663696288
Epoch: 826, Batch Gradient Norm: 29.769358451417727
Epoch: 826, Batch Gradient Norm after: 22.360676020754973
Epoch 827/10000, Prediction Accuracy = 46.346000000000004%, Loss = 1.754225993156433
Epoch: 827, Batch Gradient Norm: 31.160700397991057
Epoch: 827, Batch Gradient Norm after: 22.35829483233594
Epoch 828/10000, Prediction Accuracy = 46.346000000000004%, Loss = 1.7577423810958863
Epoch: 828, Batch Gradient Norm: 29.81123430419431
Epoch: 828, Batch Gradient Norm after: 22.360678167491173
Epoch 829/10000, Prediction Accuracy = 46.348%, Loss = 1.75265691280365
Epoch: 829, Batch Gradient Norm: 31.07863468733854
Epoch: 829, Batch Gradient Norm after: 22.286583295298055
Epoch 830/10000, Prediction Accuracy = 46.33999999999999%, Loss = 1.7557590007781982
Epoch: 830, Batch Gradient Norm: 29.853836481858885
Epoch: 830, Batch Gradient Norm after: 22.360675083686306
Epoch 831/10000, Prediction Accuracy = 46.352%, Loss = 1.751051092147827
Epoch: 831, Batch Gradient Norm: 31.21703777683927
Epoch: 831, Batch Gradient Norm after: 22.36067792341547
Epoch 832/10000, Prediction Accuracy = 46.343999999999994%, Loss = 1.7544989347457887
Epoch: 832, Batch Gradient Norm: 29.891563844875122
Epoch: 832, Batch Gradient Norm after: 22.36067730872721
Epoch 833/10000, Prediction Accuracy = 46.366%, Loss = 1.7494948625564575
Epoch: 833, Batch Gradient Norm: 31.249444249007485
Epoch: 833, Batch Gradient Norm after: 22.31265395946126
Epoch 834/10000, Prediction Accuracy = 46.348%, Loss = 1.7528941869735717
Epoch: 834, Batch Gradient Norm: 29.93372865545251
Epoch: 834, Batch Gradient Norm after: 22.360675327343852
Epoch 835/10000, Prediction Accuracy = 46.388%, Loss = 1.7479166030883788
Epoch: 835, Batch Gradient Norm: 31.388926743647602
Epoch: 835, Batch Gradient Norm after: 22.351029748520023
Epoch 836/10000, Prediction Accuracy = 46.352%, Loss = 1.751639175415039
Epoch: 836, Batch Gradient Norm: 29.975812128473166
Epoch: 836, Batch Gradient Norm after: 22.36067731066188
Epoch 837/10000, Prediction Accuracy = 46.4%, Loss = 1.7463626861572266
Epoch: 837, Batch Gradient Norm: 31.308889920445
Epoch: 837, Batch Gradient Norm after: 22.287349987824552
Epoch 838/10000, Prediction Accuracy = 46.355999999999995%, Loss = 1.7496880531311034
Epoch: 838, Batch Gradient Norm: 30.01633051567074
Epoch: 838, Batch Gradient Norm after: 22.36067712676921
Epoch 839/10000, Prediction Accuracy = 46.410000000000004%, Loss = 1.744774341583252
Epoch: 839, Batch Gradient Norm: 31.46553141354835
Epoch: 839, Batch Gradient Norm after: 22.360676193419742
Epoch 840/10000, Prediction Accuracy = 46.364%, Loss = 1.7485033988952636
Epoch: 840, Batch Gradient Norm: 30.056729866144938
Epoch: 840, Batch Gradient Norm after: 22.360677018000025
Epoch 841/10000, Prediction Accuracy = 46.398%, Loss = 1.7432331800460816
Epoch: 841, Batch Gradient Norm: 31.463880543785084
Epoch: 841, Batch Gradient Norm after: 22.30708831191266
Epoch 842/10000, Prediction Accuracy = 46.354%, Loss = 1.7467987060546875
Epoch: 842, Batch Gradient Norm: 30.098239839019346
Epoch: 842, Batch Gradient Norm after: 22.360677881874206
Epoch 843/10000, Prediction Accuracy = 46.422%, Loss = 1.7416441202163697
Epoch: 843, Batch Gradient Norm: 31.628316560818945
Epoch: 843, Batch Gradient Norm after: 22.36067725718401
Epoch 844/10000, Prediction Accuracy = 46.372%, Loss = 1.745647668838501
Epoch: 844, Batch Gradient Norm: 30.14624536153683
Epoch: 844, Batch Gradient Norm after: 22.360676122367646
Epoch 845/10000, Prediction Accuracy = 46.436%, Loss = 1.7401108264923095
Epoch: 845, Batch Gradient Norm: 31.534984787069032
Epoch: 845, Batch Gradient Norm after: 22.285586266843808
Epoch 846/10000, Prediction Accuracy = 46.376%, Loss = 1.7436419248580932
Epoch: 846, Batch Gradient Norm: 30.1841314266587
Epoch: 846, Batch Gradient Norm after: 22.360673030865367
Epoch 847/10000, Prediction Accuracy = 46.449999999999996%, Loss = 1.73851158618927
Epoch: 847, Batch Gradient Norm: 31.666534758475986
Epoch: 847, Batch Gradient Norm after: 22.36067724377059
Epoch 848/10000, Prediction Accuracy = 46.368%, Loss = 1.7423714876174927
Epoch: 848, Batch Gradient Norm: 30.22671484296435
Epoch: 848, Batch Gradient Norm after: 22.360676506018272
Epoch 849/10000, Prediction Accuracy = 46.448%, Loss = 1.7369905471801759
Epoch: 849, Batch Gradient Norm: 31.734563305961082
Epoch: 849, Batch Gradient Norm after: 22.325539202759085
Epoch 850/10000, Prediction Accuracy = 46.374%, Loss = 1.7409128665924072
Epoch: 850, Batch Gradient Norm: 30.265063814875745
Epoch: 850, Batch Gradient Norm after: 22.36067689348745
Epoch 851/10000, Prediction Accuracy = 46.456%, Loss = 1.7354336738586427
Epoch: 851, Batch Gradient Norm: 31.827209968253385
Epoch: 851, Batch Gradient Norm after: 22.342248189265753
Epoch 852/10000, Prediction Accuracy = 46.394%, Loss = 1.739548683166504
Epoch: 852, Batch Gradient Norm: 30.310070900994145
Epoch: 852, Batch Gradient Norm after: 22.360677404028497
Epoch 853/10000, Prediction Accuracy = 46.456%, Loss = 1.7338960647583008
Epoch: 853, Batch Gradient Norm: 31.836253855171197
Epoch: 853, Batch Gradient Norm after: 22.31842547923807
Epoch 854/10000, Prediction Accuracy = 46.397999999999996%, Loss = 1.7378807067871094
Epoch: 854, Batch Gradient Norm: 30.35103327433002
Epoch: 854, Batch Gradient Norm after: 22.360674538595482
Epoch 855/10000, Prediction Accuracy = 46.462%, Loss = 1.732335591316223
Epoch: 855, Batch Gradient Norm: 31.983140000593057
Epoch: 855, Batch Gradient Norm after: 22.35899127127087
Epoch 856/10000, Prediction Accuracy = 46.406%, Loss = 1.736686611175537
Epoch: 856, Batch Gradient Norm: 30.3988286492418
Epoch: 856, Batch Gradient Norm after: 22.36067617405832
Epoch 857/10000, Prediction Accuracy = 46.472%, Loss = 1.7308308362960816
Epoch: 857, Batch Gradient Norm: 31.891083592115894
Epoch: 857, Batch Gradient Norm after: 22.28939637406728
Epoch 858/10000, Prediction Accuracy = 46.42%, Loss = 1.734720277786255
Epoch: 858, Batch Gradient Norm: 30.432812055441627
Epoch: 858, Batch Gradient Norm after: 22.36067781995484
Epoch 859/10000, Prediction Accuracy = 46.480000000000004%, Loss = 1.7292407512664796
Epoch: 859, Batch Gradient Norm: 32.029698231389474
Epoch: 859, Batch Gradient Norm after: 22.360679179522556
Epoch 860/10000, Prediction Accuracy = 46.426%, Loss = 1.7335054874420166
Epoch: 860, Batch Gradient Norm: 30.47631552869435
Epoch: 860, Batch Gradient Norm after: 22.360676467100387
Epoch 861/10000, Prediction Accuracy = 46.472%, Loss = 1.7277385234832763
Epoch: 861, Batch Gradient Norm: 32.077385385063884
Epoch: 861, Batch Gradient Norm after: 22.32360550926834
Epoch 862/10000, Prediction Accuracy = 46.426%, Loss = 1.732010507583618
Epoch: 862, Batch Gradient Norm: 30.51554744702479
Epoch: 862, Batch Gradient Norm after: 22.360677118217072
Epoch 863/10000, Prediction Accuracy = 46.483999999999995%, Loss = 1.7261966943740845
Epoch: 863, Batch Gradient Norm: 32.19343479400401
Epoch: 863, Batch Gradient Norm after: 22.35169971718057
Epoch 864/10000, Prediction Accuracy = 46.428000000000004%, Loss = 1.730731987953186
Epoch: 864, Batch Gradient Norm: 30.562997867161805
Epoch: 864, Batch Gradient Norm after: 22.360679561562023
Epoch 865/10000, Prediction Accuracy = 46.48199999999999%, Loss = 1.724680209159851
Epoch: 865, Batch Gradient Norm: 32.17225223843819
Epoch: 865, Batch Gradient Norm after: 22.31453862911574
Epoch 866/10000, Prediction Accuracy = 46.446%, Loss = 1.728992223739624
Epoch: 866, Batch Gradient Norm: 30.600271180995126
Epoch: 866, Batch Gradient Norm after: 22.36067624790556
Epoch 867/10000, Prediction Accuracy = 46.488%, Loss = 1.723123598098755
Epoch: 867, Batch Gradient Norm: 32.317007003398466
Epoch: 867, Batch Gradient Norm after: 22.360676247419637
Epoch 868/10000, Prediction Accuracy = 46.456%, Loss = 1.7277907848358154
Epoch: 868, Batch Gradient Norm: 30.646602333131348
Epoch: 868, Batch Gradient Norm after: 22.36067801369626
Epoch 869/10000, Prediction Accuracy = 46.501999999999995%, Loss = 1.721622371673584
Epoch: 869, Batch Gradient Norm: 32.26793125018637
Epoch: 869, Batch Gradient Norm after: 22.30462163813973
Epoch 870/10000, Prediction Accuracy = 46.452%, Loss = 1.7259567260742188
Epoch: 870, Batch Gradient Norm: 30.680138799991507
Epoch: 870, Batch Gradient Norm after: 22.360677376676666
Epoch 871/10000, Prediction Accuracy = 46.519999999999996%, Loss = 1.7200621604919433
Epoch: 871, Batch Gradient Norm: 32.39372863426731
Epoch: 871, Batch Gradient Norm after: 22.3606756050191
Epoch 872/10000, Prediction Accuracy = 46.464%, Loss = 1.724724817276001
Epoch: 872, Batch Gradient Norm: 30.725197262626864
Epoch: 872, Batch Gradient Norm after: 22.360677686067458
Epoch 873/10000, Prediction Accuracy = 46.536%, Loss = 1.7185718059539794
Epoch: 873, Batch Gradient Norm: 32.4159882810097
Epoch: 873, Batch Gradient Norm after: 22.322957655786666
Epoch 874/10000, Prediction Accuracy = 46.471999999999994%, Loss = 1.7231364965438842
Epoch: 874, Batch Gradient Norm: 30.762964862093334
Epoch: 874, Batch Gradient Norm after: 22.360675613663282
Epoch 875/10000, Prediction Accuracy = 46.534000000000006%, Loss = 1.7170262813568116
Epoch: 875, Batch Gradient Norm: 32.56164416000796
Epoch: 875, Batch Gradient Norm after: 22.360677119494465
Epoch 876/10000, Prediction Accuracy = 46.483999999999995%, Loss = 1.721943211555481
Epoch: 876, Batch Gradient Norm: 30.81315692805917
Epoch: 876, Batch Gradient Norm after: 22.360679558150466
Epoch 877/10000, Prediction Accuracy = 46.538%, Loss = 1.7155354976654054
Epoch: 877, Batch Gradient Norm: 32.491890617073786
Epoch: 877, Batch Gradient Norm after: 22.30272441636892
Epoch 878/10000, Prediction Accuracy = 46.49%, Loss = 1.720050859451294
Epoch: 878, Batch Gradient Norm: 30.838937206317034
Epoch: 878, Batch Gradient Norm after: 22.36067754691062
Epoch 879/10000, Prediction Accuracy = 46.544%, Loss = 1.7139579772949218
Epoch: 879, Batch Gradient Norm: 32.604881228032596
Epoch: 879, Batch Gradient Norm after: 22.360675697103524
Epoch 880/10000, Prediction Accuracy = 46.5%, Loss = 1.7187541484832765
Epoch: 880, Batch Gradient Norm: 30.88497367349764
Epoch: 880, Batch Gradient Norm after: 22.360676204362782
Epoch 881/10000, Prediction Accuracy = 46.548%, Loss = 1.7124755859375
Epoch: 881, Batch Gradient Norm: 32.70933094579825
Epoch: 881, Batch Gradient Norm after: 22.34670148963873
Epoch 882/10000, Prediction Accuracy = 46.504%, Loss = 1.7174502611160278
Epoch: 882, Batch Gradient Norm: 30.927707024847976
Epoch: 882, Batch Gradient Norm after: 22.360679137278595
Epoch 883/10000, Prediction Accuracy = 46.554%, Loss = 1.7109729766845703
Epoch: 883, Batch Gradient Norm: 32.736388630324505
Epoch: 883, Batch Gradient Norm after: 22.333213312308757
Epoch 884/10000, Prediction Accuracy = 46.510000000000005%, Loss = 1.7158923387527465
Epoch: 884, Batch Gradient Norm: 30.964236295442277
Epoch: 884, Batch Gradient Norm after: 22.360677727066037
Epoch 885/10000, Prediction Accuracy = 46.556%, Loss = 1.7094435214996337
Epoch: 885, Batch Gradient Norm: 32.85068898287581
Epoch: 885, Batch Gradient Norm after: 22.360677069407668
Epoch 886/10000, Prediction Accuracy = 46.524%, Loss = 1.714628005027771
Epoch: 886, Batch Gradient Norm: 31.012343314118926
Epoch: 886, Batch Gradient Norm after: 22.360678120720433
Epoch 887/10000, Prediction Accuracy = 46.56%, Loss = 1.707965040206909
Epoch: 887, Batch Gradient Norm: 32.82300218469699
Epoch: 887, Batch Gradient Norm after: 22.321235835660225
Epoch 888/10000, Prediction Accuracy = 46.532000000000004%, Loss = 1.7128973722457885
Epoch: 888, Batch Gradient Norm: 31.044088535797567
Epoch: 888, Batch Gradient Norm after: 22.36067915221803
Epoch 889/10000, Prediction Accuracy = 46.58%, Loss = 1.7064213752746582
Epoch: 889, Batch Gradient Norm: 32.9280755853808
Epoch: 889, Batch Gradient Norm after: 22.360675262570222
Epoch 890/10000, Prediction Accuracy = 46.548%, Loss = 1.7115985870361328
Epoch: 890, Batch Gradient Norm: 31.091857268387848
Epoch: 890, Batch Gradient Norm after: 22.36067727825102
Epoch 891/10000, Prediction Accuracy = 46.592000000000006%, Loss = 1.7049570322036742
Epoch: 891, Batch Gradient Norm: 32.97658919698109
Epoch: 891, Batch Gradient Norm after: 22.338395677045213
Epoch 892/10000, Prediction Accuracy = 46.556000000000004%, Loss = 1.7101414918899536
Epoch: 892, Batch Gradient Norm: 31.129520217633996
Epoch: 892, Batch Gradient Norm after: 22.360679710515928
Epoch 893/10000, Prediction Accuracy = 46.602%, Loss = 1.7034428119659424
Epoch: 893, Batch Gradient Norm: 33.084715876076004
Epoch: 893, Batch Gradient Norm after: 22.36028265792909
Epoch 894/10000, Prediction Accuracy = 46.566%, Loss = 1.70886287689209
Epoch: 894, Batch Gradient Norm: 31.177424338497858
Epoch: 894, Batch Gradient Norm after: 22.360677884266266
Epoch 895/10000, Prediction Accuracy = 46.617999999999995%, Loss = 1.7019734859466553
Epoch: 895, Batch Gradient Norm: 33.074366817679
Epoch: 895, Batch Gradient Norm after: 22.32974293998522
Epoch 896/10000, Prediction Accuracy = 46.576%, Loss = 1.707187294960022
Epoch: 896, Batch Gradient Norm: 31.209861898016868
Epoch: 896, Batch Gradient Norm after: 22.360674464001097
Epoch 897/10000, Prediction Accuracy = 46.622%, Loss = 1.7004433155059815
Epoch: 897, Batch Gradient Norm: 33.162175196035946
Epoch: 897, Batch Gradient Norm after: 22.360677928062394
Epoch 898/10000, Prediction Accuracy = 46.588%, Loss = 1.7058506727218627
Epoch: 898, Batch Gradient Norm: 31.25712423684807
Epoch: 898, Batch Gradient Norm after: 22.36067660146538
Epoch 899/10000, Prediction Accuracy = 46.634%, Loss = 1.698976731300354
Epoch: 899, Batch Gradient Norm: 33.232548217040275
Epoch: 899, Batch Gradient Norm after: 22.348258865729385
Epoch 900/10000, Prediction Accuracy = 46.588%, Loss = 1.7044711351394652
Epoch: 900, Batch Gradient Norm: 31.297967503749444
Epoch: 900, Batch Gradient Norm after: 22.36067736573323
Epoch 901/10000, Prediction Accuracy = 46.636%, Loss = 1.6974890232086182
Epoch: 901, Batch Gradient Norm: 33.28401109237545
Epoch: 901, Batch Gradient Norm after: 22.346520508848535
Epoch 902/10000, Prediction Accuracy = 46.606%, Loss = 1.7030217409133912
Epoch: 902, Batch Gradient Norm: 31.33829026515301
Epoch: 902, Batch Gradient Norm after: 22.360676496755218
Epoch 903/10000, Prediction Accuracy = 46.648%, Loss = 1.6960094451904297
Epoch: 903, Batch Gradient Norm: 33.35971450490339
Epoch: 903, Batch Gradient Norm after: 22.354691079867727
Epoch 904/10000, Prediction Accuracy = 46.61%, Loss = 1.7016542911529542
Epoch: 904, Batch Gradient Norm: 31.37966348227242
Epoch: 904, Batch Gradient Norm after: 22.360677081897496
Epoch 905/10000, Prediction Accuracy = 46.662%, Loss = 1.694538450241089
Epoch: 905, Batch Gradient Norm: 33.38823276112189
Epoch: 905, Batch Gradient Norm after: 22.343933801255222
Epoch 906/10000, Prediction Accuracy = 46.617999999999995%, Loss = 1.7001481771469116
Epoch: 906, Batch Gradient Norm: 31.415259011043023
Epoch: 906, Batch Gradient Norm after: 22.36067670955163
Epoch 907/10000, Prediction Accuracy = 46.672%, Loss = 1.6930567502975464
Epoch: 907, Batch Gradient Norm: 33.46731553168963
Epoch: 907, Batch Gradient Norm after: 22.36067872102183
Epoch 908/10000, Prediction Accuracy = 46.623999999999995%, Loss = 1.6988176107406616
Epoch: 908, Batch Gradient Norm: 31.455466696672676
Epoch: 908, Batch Gradient Norm after: 22.360678288891595
Epoch 909/10000, Prediction Accuracy = 46.666%, Loss = 1.6916179180145263
Epoch: 909, Batch Gradient Norm: 33.50796388994668
Epoch: 909, Batch Gradient Norm after: 22.349513863060256
Epoch 910/10000, Prediction Accuracy = 46.628%, Loss = 1.6973464965820313
Epoch: 910, Batch Gradient Norm: 31.492609864267845
Epoch: 910, Batch Gradient Norm after: 22.36067691777469
Epoch 911/10000, Prediction Accuracy = 46.678000000000004%, Loss = 1.6901400327682494
Epoch: 911, Batch Gradient Norm: 33.5719487072934
Epoch: 911, Batch Gradient Norm after: 22.36067590772882
Epoch 912/10000, Prediction Accuracy = 46.64%, Loss = 1.6959666967391969
Epoch: 912, Batch Gradient Norm: 31.531783720575483
Epoch: 912, Batch Gradient Norm after: 22.36067666330192
Epoch 913/10000, Prediction Accuracy = 46.672000000000004%, Loss = 1.6886883020401
Epoch: 913, Batch Gradient Norm: 33.642118799638624
Epoch: 913, Batch Gradient Norm after: 22.360677472436258
Epoch 914/10000, Prediction Accuracy = 46.65%, Loss = 1.6946018934249878
Epoch: 914, Batch Gradient Norm: 31.575181076679346
Epoch: 914, Batch Gradient Norm after: 22.360676912468552
Epoch 915/10000, Prediction Accuracy = 46.668%, Loss = 1.6872418642044067
Epoch: 915, Batch Gradient Norm: 33.67720775056523
Epoch: 915, Batch Gradient Norm after: 22.352204440369757
Epoch 916/10000, Prediction Accuracy = 46.664%, Loss = 1.6931320667266845
Epoch: 916, Batch Gradient Norm: 31.613408111120698
Epoch: 916, Batch Gradient Norm after: 22.36067548821883
Epoch 917/10000, Prediction Accuracy = 46.672000000000004%, Loss = 1.6857834577560424
Epoch: 917, Batch Gradient Norm: 33.74213157180845
Epoch: 917, Batch Gradient Norm after: 22.36067896799495
Epoch 918/10000, Prediction Accuracy = 46.672000000000004%, Loss = 1.6917605876922608
Epoch: 918, Batch Gradient Norm: 31.654704747808886
Epoch: 918, Batch Gradient Norm after: 22.360678297221366
Epoch 919/10000, Prediction Accuracy = 46.694%, Loss = 1.6843422651290894
Epoch: 919, Batch Gradient Norm: 33.80525596967357
Epoch: 919, Batch Gradient Norm after: 22.360676329007347
Epoch 920/10000, Prediction Accuracy = 46.672000000000004%, Loss = 1.6903835535049438
Epoch: 920, Batch Gradient Norm: 31.69577702331087
Epoch: 920, Batch Gradient Norm after: 22.360676997603147
Epoch 921/10000, Prediction Accuracy = 46.69799999999999%, Loss = 1.682890486717224
Epoch: 921, Batch Gradient Norm: 33.85181456724616
Epoch: 921, Batch Gradient Norm after: 22.360674859724625
Epoch 922/10000, Prediction Accuracy = 46.68%, Loss = 1.6889597177505493
Epoch: 922, Batch Gradient Norm: 31.733143315955406
Epoch: 922, Batch Gradient Norm after: 22.360676952638382
Epoch 923/10000, Prediction Accuracy = 46.712%, Loss = 1.6814363956451417
Epoch: 923, Batch Gradient Norm: 33.893435100646286
Epoch: 923, Batch Gradient Norm after: 22.360677499583883
Epoch 924/10000, Prediction Accuracy = 46.69%, Loss = 1.6875197649002076
Epoch: 924, Batch Gradient Norm: 31.772362415545782
Epoch: 924, Batch Gradient Norm after: 22.36067591343318
Epoch 925/10000, Prediction Accuracy = 46.74%, Loss = 1.6799877166748047
Epoch: 925, Batch Gradient Norm: 33.93137598984504
Epoch: 925, Batch Gradient Norm after: 22.36067811431647
Epoch 926/10000, Prediction Accuracy = 46.692%, Loss = 1.686072039604187
Epoch: 926, Batch Gradient Norm: 31.809647160452222
Epoch: 926, Batch Gradient Norm after: 22.360675350232402
Epoch 927/10000, Prediction Accuracy = 46.756%, Loss = 1.678542423248291
Epoch: 927, Batch Gradient Norm: 33.97504424390103
Epoch: 927, Batch Gradient Norm after: 22.360676939151116
Epoch 928/10000, Prediction Accuracy = 46.714%, Loss = 1.6846391916275025
Epoch: 928, Batch Gradient Norm: 31.84872884924238
Epoch: 928, Batch Gradient Norm after: 22.360674967103655
Epoch 929/10000, Prediction Accuracy = 46.772%, Loss = 1.6771032810211182
Epoch: 929, Batch Gradient Norm: 34.01160762837859
Epoch: 929, Batch Gradient Norm after: 22.360677207511422
Epoch 930/10000, Prediction Accuracy = 46.726%, Loss = 1.6832052230834962
Epoch: 930, Batch Gradient Norm: 31.887178846460714
Epoch: 930, Batch Gradient Norm after: 22.36067809135919
Epoch 931/10000, Prediction Accuracy = 46.774%, Loss = 1.6756659269332885
Epoch: 931, Batch Gradient Norm: 34.064480852070105
Epoch: 931, Batch Gradient Norm after: 22.360676437427195
Epoch 932/10000, Prediction Accuracy = 46.742%, Loss = 1.6818177461624146
Epoch: 932, Batch Gradient Norm: 31.92682139288524
Epoch: 932, Batch Gradient Norm after: 22.360676693817904
Epoch 933/10000, Prediction Accuracy = 46.778%, Loss = 1.6742431640625
Epoch: 933, Batch Gradient Norm: 34.11794007109971
Epoch: 933, Batch Gradient Norm after: 22.36067714276652
Epoch 934/10000, Prediction Accuracy = 46.74999999999999%, Loss = 1.6804378032684326
Epoch: 934, Batch Gradient Norm: 31.9628557974978
Epoch: 934, Batch Gradient Norm after: 22.360676069115506
Epoch 935/10000, Prediction Accuracy = 46.782%, Loss = 1.6728173732757567
Epoch: 935, Batch Gradient Norm: 34.16852937970031
Epoch: 935, Batch Gradient Norm after: 22.360677543156278
Epoch 936/10000, Prediction Accuracy = 46.751999999999995%, Loss = 1.6790538549423217
Epoch: 936, Batch Gradient Norm: 32.000956115807455
Epoch: 936, Batch Gradient Norm after: 22.36067659721438
Epoch 937/10000, Prediction Accuracy = 46.790000000000006%, Loss = 1.6713833570480348
Epoch: 937, Batch Gradient Norm: 34.21232922651037
Epoch: 937, Batch Gradient Norm after: 22.360677956828024
Epoch 938/10000, Prediction Accuracy = 46.762%, Loss = 1.6776493072509766
Epoch: 938, Batch Gradient Norm: 32.03435170546441
Epoch: 938, Batch Gradient Norm after: 22.360677887279053
Epoch 939/10000, Prediction Accuracy = 46.800000000000004%, Loss = 1.6699410438537599
Epoch: 939, Batch Gradient Norm: 34.242668027110845
Epoch: 939, Batch Gradient Norm after: 22.36067733120757
Epoch 940/10000, Prediction Accuracy = 46.76%, Loss = 1.6762027025222779
Epoch: 940, Batch Gradient Norm: 32.07155178063946
Epoch: 940, Batch Gradient Norm after: 22.360672956598634
Epoch 941/10000, Prediction Accuracy = 46.796%, Loss = 1.668511724472046
Epoch: 941, Batch Gradient Norm: 34.27081117054335
Epoch: 941, Batch Gradient Norm after: 22.36067682355068
Epoch 942/10000, Prediction Accuracy = 46.772%, Loss = 1.6747623443603517
Epoch: 942, Batch Gradient Norm: 32.10506941915367
Epoch: 942, Batch Gradient Norm after: 22.36067736408326
Epoch 943/10000, Prediction Accuracy = 46.79200000000001%, Loss = 1.6670864820480347
Epoch: 943, Batch Gradient Norm: 34.289780169078625
Epoch: 943, Batch Gradient Norm after: 22.36067862217853
Epoch 944/10000, Prediction Accuracy = 46.786%, Loss = 1.673275852203369
Epoch: 944, Batch Gradient Norm: 32.139941971268605
Epoch: 944, Batch Gradient Norm after: 22.360678663663705
Epoch 945/10000, Prediction Accuracy = 46.814%, Loss = 1.6656665325164794
Epoch: 945, Batch Gradient Norm: 34.32218587383045
Epoch: 945, Batch Gradient Norm after: 22.36067705004644
Epoch 946/10000, Prediction Accuracy = 46.775999999999996%, Loss = 1.671836519241333
Epoch: 946, Batch Gradient Norm: 32.179594416585374
Epoch: 946, Batch Gradient Norm after: 22.360676776208376
Epoch 947/10000, Prediction Accuracy = 46.834%, Loss = 1.6642752885818481
Epoch: 947, Batch Gradient Norm: 34.355413799856365
Epoch: 947, Batch Gradient Norm after: 22.360677805778856
Epoch 948/10000, Prediction Accuracy = 46.796%, Loss = 1.6704389095306396
Epoch: 948, Batch Gradient Norm: 32.20872413193939
Epoch: 948, Batch Gradient Norm after: 22.360676178469074
Epoch 949/10000, Prediction Accuracy = 46.842%, Loss = 1.6628573656082153
Epoch: 949, Batch Gradient Norm: 34.3866319749086
Epoch: 949, Batch Gradient Norm after: 22.36067555413509
Epoch 950/10000, Prediction Accuracy = 46.798%, Loss = 1.669019389152527
Epoch: 950, Batch Gradient Norm: 32.244480916976705
Epoch: 950, Batch Gradient Norm after: 22.360677619212616
Epoch 951/10000, Prediction Accuracy = 46.848%, Loss = 1.661452054977417
Epoch: 951, Batch Gradient Norm: 34.41918078989247
Epoch: 951, Batch Gradient Norm after: 22.36067790807572
Epoch 952/10000, Prediction Accuracy = 46.8%, Loss = 1.667610502243042
Epoch: 952, Batch Gradient Norm: 32.277909478397056
Epoch: 952, Batch Gradient Norm after: 22.360677535352206
Epoch 953/10000, Prediction Accuracy = 46.842%, Loss = 1.6600479125976562
Epoch: 953, Batch Gradient Norm: 34.45651430262312
Epoch: 953, Batch Gradient Norm after: 22.360679010089115
Epoch 954/10000, Prediction Accuracy = 46.812%, Loss = 1.6662184238433837
Epoch: 954, Batch Gradient Norm: 32.31576752740881
Epoch: 954, Batch Gradient Norm after: 22.36067519128296
Epoch 955/10000, Prediction Accuracy = 46.836%, Loss = 1.6586557388305665
Epoch: 955, Batch Gradient Norm: 34.49855483694568
Epoch: 955, Batch Gradient Norm after: 22.360678745721014
Epoch 956/10000, Prediction Accuracy = 46.82600000000001%, Loss = 1.6648420333862304
Epoch: 956, Batch Gradient Norm: 32.34873028146475
Epoch: 956, Batch Gradient Norm after: 22.36067745445068
Epoch 957/10000, Prediction Accuracy = 46.85%, Loss = 1.657259464263916
Epoch: 957, Batch Gradient Norm: 34.53722650589344
Epoch: 957, Batch Gradient Norm after: 22.360677208435163
Epoch 958/10000, Prediction Accuracy = 46.82600000000001%, Loss = 1.6634607076644898
Epoch: 958, Batch Gradient Norm: 32.386647634281374
Epoch: 958, Batch Gradient Norm after: 22.360677561801726
Epoch 959/10000, Prediction Accuracy = 46.85%, Loss = 1.65587260723114
Epoch: 959, Batch Gradient Norm: 34.58040156590145
Epoch: 959, Batch Gradient Norm after: 22.360677506235657
Epoch 960/10000, Prediction Accuracy = 46.833999999999996%, Loss = 1.6621070384979248
Epoch: 960, Batch Gradient Norm: 32.42041770558968
Epoch: 960, Batch Gradient Norm after: 22.36067680436741
Epoch 961/10000, Prediction Accuracy = 46.839999999999996%, Loss = 1.654483437538147
Epoch: 961, Batch Gradient Norm: 34.62537341497206
Epoch: 961, Batch Gradient Norm after: 22.360676096870304
Epoch 962/10000, Prediction Accuracy = 46.84%, Loss = 1.6607549905776977
Epoch: 962, Batch Gradient Norm: 32.45783203182944
Epoch: 962, Batch Gradient Norm after: 22.36067739245389
Epoch 963/10000, Prediction Accuracy = 46.827999999999996%, Loss = 1.6531053066253663
Epoch: 963, Batch Gradient Norm: 34.66821310343519
Epoch: 963, Batch Gradient Norm after: 22.36067436113021
Epoch 964/10000, Prediction Accuracy = 46.84%, Loss = 1.6593907833099366
Epoch: 964, Batch Gradient Norm: 32.491474729088075
Epoch: 964, Batch Gradient Norm after: 22.360678147412518
Epoch 965/10000, Prediction Accuracy = 46.84%, Loss = 1.6517213582992554
Epoch: 965, Batch Gradient Norm: 34.705207728876
Epoch: 965, Batch Gradient Norm after: 22.360676556303268
Epoch 966/10000, Prediction Accuracy = 46.839999999999996%, Loss = 1.6580165386199952
Epoch: 966, Batch Gradient Norm: 32.5253819959068
Epoch: 966, Batch Gradient Norm after: 22.360678357100124
Epoch 967/10000, Prediction Accuracy = 46.843999999999994%, Loss = 1.6503486633300781
Epoch: 967, Batch Gradient Norm: 34.7443898346179
Epoch: 967, Batch Gradient Norm after: 22.36067644736344
Epoch 968/10000, Prediction Accuracy = 46.836%, Loss = 1.6566603183746338
Epoch: 968, Batch Gradient Norm: 32.558127359470156
Epoch: 968, Batch Gradient Norm after: 22.36067791021775
Epoch 969/10000, Prediction Accuracy = 46.848%, Loss = 1.6489846467971803
Epoch: 969, Batch Gradient Norm: 34.78050152684988
Epoch: 969, Batch Gradient Norm after: 22.360677266180065
Epoch 970/10000, Prediction Accuracy = 46.84%, Loss = 1.6552941083908081
Epoch: 970, Batch Gradient Norm: 32.59158099148534
Epoch: 970, Batch Gradient Norm after: 22.360679111079083
Epoch 971/10000, Prediction Accuracy = 46.852%, Loss = 1.647623109817505
Epoch: 971, Batch Gradient Norm: 34.82084499199754
Epoch: 971, Batch Gradient Norm after: 22.360676242823583
Epoch 972/10000, Prediction Accuracy = 46.856%, Loss = 1.6539433479309082
Epoch: 972, Batch Gradient Norm: 32.62586858173288
Epoch: 972, Batch Gradient Norm after: 22.36068044641636
Epoch 973/10000, Prediction Accuracy = 46.852%, Loss = 1.6462631940841674
Epoch: 973, Batch Gradient Norm: 34.876605747224886
Epoch: 973, Batch Gradient Norm after: 22.360677845787574
Epoch 974/10000, Prediction Accuracy = 46.86%, Loss = 1.6526320457458497
Epoch: 974, Batch Gradient Norm: 32.66134963046812
Epoch: 974, Batch Gradient Norm after: 22.36067918949064
Epoch 975/10000, Prediction Accuracy = 46.85%, Loss = 1.6449064016342163
Epoch: 975, Batch Gradient Norm: 34.92503413057774
Epoch: 975, Batch Gradient Norm after: 22.36067781527824
Epoch 976/10000, Prediction Accuracy = 46.872%, Loss = 1.6512993812561034
Epoch: 976, Batch Gradient Norm: 32.696852586388516
Epoch: 976, Batch Gradient Norm after: 22.360678448799767
Epoch 977/10000, Prediction Accuracy = 46.866%, Loss = 1.643550419807434
Epoch: 977, Batch Gradient Norm: 34.95827032802745
Epoch: 977, Batch Gradient Norm after: 22.36067809546961
Epoch 978/10000, Prediction Accuracy = 46.872%, Loss = 1.6499311208724976
Epoch: 978, Batch Gradient Norm: 32.72935285280408
Epoch: 978, Batch Gradient Norm after: 22.360678467099934
Epoch 979/10000, Prediction Accuracy = 46.86999999999999%, Loss = 1.6421828031539918
Epoch: 979, Batch Gradient Norm: 34.98635512238724
Epoch: 979, Batch Gradient Norm after: 22.360677833741164
Epoch 980/10000, Prediction Accuracy = 46.88%, Loss = 1.6485495805740356
Epoch: 980, Batch Gradient Norm: 32.76279949211057
Epoch: 980, Batch Gradient Norm after: 22.360678353161735
Epoch 981/10000, Prediction Accuracy = 46.882%, Loss = 1.640824794769287
Epoch: 981, Batch Gradient Norm: 35.005621317543586
Epoch: 981, Batch Gradient Norm after: 22.360676455088143
Epoch 982/10000, Prediction Accuracy = 46.885999999999996%, Loss = 1.6471675157546997
Epoch: 982, Batch Gradient Norm: 32.79530844545699
Epoch: 982, Batch Gradient Norm after: 22.360677523285943
Epoch 983/10000, Prediction Accuracy = 46.894%, Loss = 1.639463973045349
Epoch: 983, Batch Gradient Norm: 35.03274476905442
Epoch: 983, Batch Gradient Norm after: 22.360679134836936
Epoch 984/10000, Prediction Accuracy = 46.884%, Loss = 1.6457921028137208
Epoch: 984, Batch Gradient Norm: 32.828863407046335
Epoch: 984, Batch Gradient Norm after: 22.360676884388262
Epoch 985/10000, Prediction Accuracy = 46.888%, Loss = 1.6381163358688355
Epoch: 985, Batch Gradient Norm: 35.06105222607458
Epoch: 985, Batch Gradient Norm after: 22.36067817408442
Epoch 986/10000, Prediction Accuracy = 46.884%, Loss = 1.644424057006836
Epoch: 986, Batch Gradient Norm: 32.8636622477171
Epoch: 986, Batch Gradient Norm after: 22.360675521484513
Epoch 987/10000, Prediction Accuracy = 46.906%, Loss = 1.63676016330719
Epoch: 987, Batch Gradient Norm: 35.09391742221115
Epoch: 987, Batch Gradient Norm after: 22.360673413510185
Epoch 988/10000, Prediction Accuracy = 46.882%, Loss = 1.6430714845657348
Epoch: 988, Batch Gradient Norm: 32.896033110200975
Epoch: 988, Batch Gradient Norm after: 22.360677074937033
Epoch 989/10000, Prediction Accuracy = 46.92%, Loss = 1.6354177474975586
Epoch: 989, Batch Gradient Norm: 35.12045545448426
Epoch: 989, Batch Gradient Norm after: 22.360677579484992
Epoch 990/10000, Prediction Accuracy = 46.88%, Loss = 1.641717553138733
Epoch: 990, Batch Gradient Norm: 32.92669934239543
Epoch: 990, Batch Gradient Norm after: 22.360674925344174
Epoch 991/10000, Prediction Accuracy = 46.922000000000004%, Loss = 1.6340779542922974
Epoch: 991, Batch Gradient Norm: 35.1503416256412
Epoch: 991, Batch Gradient Norm after: 22.360675988406157
Epoch 992/10000, Prediction Accuracy = 46.892%, Loss = 1.6403605461120605
Epoch: 992, Batch Gradient Norm: 32.95811823260048
Epoch: 992, Batch Gradient Norm after: 22.360678364159792
Epoch 993/10000, Prediction Accuracy = 46.92%, Loss = 1.6327193260192872
Epoch: 993, Batch Gradient Norm: 35.17251889667566
Epoch: 993, Batch Gradient Norm after: 22.360675981495532
Epoch 994/10000, Prediction Accuracy = 46.894000000000005%, Loss = 1.6389967918395996
Epoch: 994, Batch Gradient Norm: 32.99266979646449
Epoch: 994, Batch Gradient Norm after: 22.360678400894862
Epoch 995/10000, Prediction Accuracy = 46.928%, Loss = 1.6313870429992676
Epoch: 995, Batch Gradient Norm: 35.176853581982364
Epoch: 995, Batch Gradient Norm after: 22.36067720878524
Epoch 996/10000, Prediction Accuracy = 46.894000000000005%, Loss = 1.6375672101974488
Epoch: 996, Batch Gradient Norm: 33.02453953208101
Epoch: 996, Batch Gradient Norm after: 22.360675706958627
Epoch 997/10000, Prediction Accuracy = 46.924%, Loss = 1.630045509338379
Epoch: 997, Batch Gradient Norm: 35.18880776795062
Epoch: 997, Batch Gradient Norm after: 22.360678304397975
Epoch 998/10000, Prediction Accuracy = 46.9%, Loss = 1.6361650705337525
Epoch: 998, Batch Gradient Norm: 33.05920639882554
Epoch: 998, Batch Gradient Norm after: 22.36067640768794
Epoch 999/10000, Prediction Accuracy = 46.91799999999999%, Loss = 1.6287171363830566
Epoch: 999, Batch Gradient Norm: 35.204973415288876
Epoch: 999, Batch Gradient Norm after: 22.36067949948433
Epoch 1000/10000, Prediction Accuracy = 46.910000000000004%, Loss = 1.6347728967666626
Epoch: 1000, Batch Gradient Norm: 33.09263991777777
Epoch: 1000, Batch Gradient Norm after: 22.360677057472163
Epoch 1001/10000, Prediction Accuracy = 46.936%, Loss = 1.6273879289627076
Epoch: 1001, Batch Gradient Norm: 35.22730040074016
Epoch: 1001, Batch Gradient Norm after: 22.36067751196596
Epoch 1002/10000, Prediction Accuracy = 46.896%, Loss = 1.633410596847534
Epoch: 1002, Batch Gradient Norm: 33.123890109421936
Epoch: 1002, Batch Gradient Norm after: 22.360676495827207
Epoch 1003/10000, Prediction Accuracy = 46.95399999999999%, Loss = 1.6260746002197266
Epoch: 1003, Batch Gradient Norm: 35.250419990272576
Epoch: 1003, Batch Gradient Norm after: 22.360677479273352
Epoch 1004/10000, Prediction Accuracy = 46.906000000000006%, Loss = 1.63205943107605
Epoch: 1004, Batch Gradient Norm: 33.15807502377627
Epoch: 1004, Batch Gradient Norm after: 22.360677197735907
Epoch 1005/10000, Prediction Accuracy = 46.966%, Loss = 1.6247552156448364
Epoch: 1005, Batch Gradient Norm: 35.26537592499545
Epoch: 1005, Batch Gradient Norm after: 22.36067846850174
Epoch 1006/10000, Prediction Accuracy = 46.912%, Loss = 1.6307021617889403
Epoch: 1006, Batch Gradient Norm: 33.18776195985114
Epoch: 1006, Batch Gradient Norm after: 22.36067804807616
Epoch 1007/10000, Prediction Accuracy = 46.96399999999999%, Loss = 1.6234357357025146
Epoch: 1007, Batch Gradient Norm: 35.28663041609487
Epoch: 1007, Batch Gradient Norm after: 22.360675913703112
Epoch 1008/10000, Prediction Accuracy = 46.916000000000004%, Loss = 1.629355216026306
Epoch: 1008, Batch Gradient Norm: 33.21825887921861
Epoch: 1008, Batch Gradient Norm after: 22.360677006273896
Epoch 1009/10000, Prediction Accuracy = 46.982%, Loss = 1.6221184015274048
Epoch: 1009, Batch Gradient Norm: 35.29983932971651
Epoch: 1009, Batch Gradient Norm after: 22.360677257627096
Epoch 1010/10000, Prediction Accuracy = 46.926%, Loss = 1.6279884338378907
Epoch: 1010, Batch Gradient Norm: 33.24857721053629
Epoch: 1010, Batch Gradient Norm after: 22.360675728786152
Epoch 1011/10000, Prediction Accuracy = 46.988%, Loss = 1.6208168745040894
Epoch: 1011, Batch Gradient Norm: 35.313746539532055
Epoch: 1011, Batch Gradient Norm after: 22.360678528023872
Epoch 1012/10000, Prediction Accuracy = 46.928%, Loss = 1.6266228914260865
Epoch: 1012, Batch Gradient Norm: 33.282375516768695
Epoch: 1012, Batch Gradient Norm after: 22.360677546906402
Epoch 1013/10000, Prediction Accuracy = 46.994%, Loss = 1.6195144176483154
Epoch: 1013, Batch Gradient Norm: 35.3219029136471
Epoch: 1013, Batch Gradient Norm after: 22.36067676645042
Epoch 1014/10000, Prediction Accuracy = 46.94%, Loss = 1.6252522945404053
Epoch: 1014, Batch Gradient Norm: 33.31433463344072
Epoch: 1014, Batch Gradient Norm after: 22.3606760295325
Epoch 1015/10000, Prediction Accuracy = 47.012%, Loss = 1.6182212352752685
Epoch: 1015, Batch Gradient Norm: 35.33499093857573
Epoch: 1015, Batch Gradient Norm after: 22.36067637366448
Epoch 1016/10000, Prediction Accuracy = 46.948%, Loss = 1.623898410797119
Epoch: 1016, Batch Gradient Norm: 33.34571289422192
Epoch: 1016, Batch Gradient Norm after: 22.360678307262134
Epoch 1017/10000, Prediction Accuracy = 47.025999999999996%, Loss = 1.6169373989105225
Epoch: 1017, Batch Gradient Norm: 35.3489022524923
Epoch: 1017, Batch Gradient Norm after: 22.36067669918871
Epoch 1018/10000, Prediction Accuracy = 46.964%, Loss = 1.622563123703003
Epoch: 1018, Batch Gradient Norm: 33.37594749800141
Epoch: 1018, Batch Gradient Norm after: 22.360676155019398
Epoch 1019/10000, Prediction Accuracy = 47.034000000000006%, Loss = 1.6156605005264282
Epoch: 1019, Batch Gradient Norm: 35.370742512222364
Epoch: 1019, Batch Gradient Norm after: 22.360675669749444
Epoch 1020/10000, Prediction Accuracy = 46.98%, Loss = 1.6212533712387085
Epoch: 1020, Batch Gradient Norm: 33.40837696281081
Epoch: 1020, Batch Gradient Norm after: 22.3606772473268
Epoch 1021/10000, Prediction Accuracy = 47.035999999999994%, Loss = 1.6143776178359985
Epoch: 1021, Batch Gradient Norm: 35.38651332969719
Epoch: 1021, Batch Gradient Norm after: 22.360676529592123
Epoch 1022/10000, Prediction Accuracy = 46.986000000000004%, Loss = 1.6199288368225098
Epoch: 1022, Batch Gradient Norm: 33.4377007097388
Epoch: 1022, Batch Gradient Norm after: 22.360679533302886
Epoch 1023/10000, Prediction Accuracy = 47.041999999999994%, Loss = 1.6131027221679688
Epoch: 1023, Batch Gradient Norm: 35.40268977528862
Epoch: 1023, Batch Gradient Norm after: 22.3606749379538
Epoch 1024/10000, Prediction Accuracy = 46.995999999999995%, Loss = 1.6185982942581176
Epoch: 1024, Batch Gradient Norm: 33.47370927395542
Epoch: 1024, Batch Gradient Norm after: 22.360677209767594
Epoch 1025/10000, Prediction Accuracy = 47.05800000000001%, Loss = 1.611835813522339
Epoch: 1025, Batch Gradient Norm: 35.43322873448834
Epoch: 1025, Batch Gradient Norm after: 22.36067606651491
Epoch 1026/10000, Prediction Accuracy = 47.004%, Loss = 1.6173216342926025
Epoch: 1026, Batch Gradient Norm: 33.5066663864346
Epoch: 1026, Batch Gradient Norm after: 22.36067733376991
Epoch 1027/10000, Prediction Accuracy = 47.07%, Loss = 1.610559868812561
Epoch: 1027, Batch Gradient Norm: 35.47101238637093
Epoch: 1027, Batch Gradient Norm after: 22.36067735816729
Epoch 1028/10000, Prediction Accuracy = 47.014%, Loss = 1.6160870790481567
Epoch: 1028, Batch Gradient Norm: 33.539628339760476
Epoch: 1028, Batch Gradient Norm after: 22.360677757220103
Epoch 1029/10000, Prediction Accuracy = 47.086%, Loss = 1.609308910369873
Epoch: 1029, Batch Gradient Norm: 35.5142939603383
Epoch: 1029, Batch Gradient Norm after: 22.36067559430197
Epoch 1030/10000, Prediction Accuracy = 47.025999999999996%, Loss = 1.6148644924163817
Epoch: 1030, Batch Gradient Norm: 33.57187375483493
Epoch: 1030, Batch Gradient Norm after: 22.360676132671152
Epoch 1031/10000, Prediction Accuracy = 47.08399999999999%, Loss = 1.60805823802948
Epoch: 1031, Batch Gradient Norm: 35.55522553146072
Epoch: 1031, Batch Gradient Norm after: 22.360675876768134
Epoch 1032/10000, Prediction Accuracy = 47.028%, Loss = 1.6136319398880006
Epoch: 1032, Batch Gradient Norm: 33.606136525225835
Epoch: 1032, Batch Gradient Norm after: 22.360676804124115
Epoch 1033/10000, Prediction Accuracy = 47.10600000000001%, Loss = 1.6068055868148803
Epoch: 1033, Batch Gradient Norm: 35.59667889688155
Epoch: 1033, Batch Gradient Norm after: 22.360675857746745
Epoch 1034/10000, Prediction Accuracy = 47.044%, Loss = 1.6124037981033326
Epoch: 1034, Batch Gradient Norm: 33.63875684931741
Epoch: 1034, Batch Gradient Norm after: 22.360679959679697
Epoch 1035/10000, Prediction Accuracy = 47.106%, Loss = 1.6055534601211547
Epoch: 1035, Batch Gradient Norm: 35.62943096402198
Epoch: 1035, Batch Gradient Norm after: 22.36067687814853
Epoch 1036/10000, Prediction Accuracy = 47.042%, Loss = 1.6111618995666503
Epoch: 1036, Batch Gradient Norm: 33.66772646054797
Epoch: 1036, Batch Gradient Norm after: 22.3606777121406
Epoch 1037/10000, Prediction Accuracy = 47.116%, Loss = 1.6043028593063355
Epoch: 1037, Batch Gradient Norm: 35.659463769890245
Epoch: 1037, Batch Gradient Norm after: 22.360677964969504
Epoch 1038/10000, Prediction Accuracy = 47.05200000000001%, Loss = 1.6099110126495362
Epoch: 1038, Batch Gradient Norm: 33.69630848475724
Epoch: 1038, Batch Gradient Norm after: 22.36067695734594
Epoch 1039/10000, Prediction Accuracy = 47.126%, Loss = 1.6030583143234254
Epoch: 1039, Batch Gradient Norm: 35.6828390786281
Epoch: 1039, Batch Gradient Norm after: 22.360677345520397
Epoch 1040/10000, Prediction Accuracy = 47.05%, Loss = 1.6086274623870849
Epoch: 1040, Batch Gradient Norm: 33.725147239291125
Epoch: 1040, Batch Gradient Norm after: 22.360677017978205
Epoch 1041/10000, Prediction Accuracy = 47.135999999999996%, Loss = 1.601815676689148
Epoch: 1041, Batch Gradient Norm: 35.704818644557186
Epoch: 1041, Batch Gradient Norm after: 22.36067500171241
Epoch 1042/10000, Prediction Accuracy = 47.048%, Loss = 1.607344102859497
Epoch: 1042, Batch Gradient Norm: 33.754660349558
Epoch: 1042, Batch Gradient Norm after: 22.3606764076151
Epoch 1043/10000, Prediction Accuracy = 47.141999999999996%, Loss = 1.6005667924880982
Epoch: 1043, Batch Gradient Norm: 35.72484071303639
Epoch: 1043, Batch Gradient Norm after: 22.36067418439377
Epoch 1044/10000, Prediction Accuracy = 47.058%, Loss = 1.6060675859451294
Epoch: 1044, Batch Gradient Norm: 33.78734730855747
Epoch: 1044, Batch Gradient Norm after: 22.36067930059263
Epoch 1045/10000, Prediction Accuracy = 47.15%, Loss = 1.5993306159973144
Epoch: 1045, Batch Gradient Norm: 35.74319193662637
Epoch: 1045, Batch Gradient Norm after: 22.36067467455828
Epoch 1046/10000, Prediction Accuracy = 47.074%, Loss = 1.6047985076904296
Epoch: 1046, Batch Gradient Norm: 33.8168337573298
Epoch: 1046, Batch Gradient Norm after: 22.36067670264859
Epoch 1047/10000, Prediction Accuracy = 47.15%, Loss = 1.5981016397476195
Epoch: 1047, Batch Gradient Norm: 35.76332606496759
Epoch: 1047, Batch Gradient Norm after: 22.36067600533873
Epoch 1048/10000, Prediction Accuracy = 47.081999999999994%, Loss = 1.6035155296325683
Epoch: 1048, Batch Gradient Norm: 33.84820264942912
Epoch: 1048, Batch Gradient Norm after: 22.360677218794454
Epoch 1049/10000, Prediction Accuracy = 47.153999999999996%, Loss = 1.596868634223938
Epoch: 1049, Batch Gradient Norm: 35.78342258227907
Epoch: 1049, Batch Gradient Norm after: 22.360675496369236
Epoch 1050/10000, Prediction Accuracy = 47.08399999999999%, Loss = 1.6022435188293458
Epoch: 1050, Batch Gradient Norm: 33.88078961018351
Epoch: 1050, Batch Gradient Norm after: 22.3606780243898
Epoch 1051/10000, Prediction Accuracy = 47.172000000000004%, Loss = 1.5956351041793824
Epoch: 1051, Batch Gradient Norm: 35.79541620116207
Epoch: 1051, Batch Gradient Norm after: 22.360674762390683
Epoch 1052/10000, Prediction Accuracy = 47.104%, Loss = 1.6009560108184815
Epoch: 1052, Batch Gradient Norm: 33.913517739413486
Epoch: 1052, Batch Gradient Norm after: 22.36067656330468
Epoch 1053/10000, Prediction Accuracy = 47.18399999999999%, Loss = 1.5944086551666259
Epoch: 1053, Batch Gradient Norm: 35.811062696058045
Epoch: 1053, Batch Gradient Norm after: 22.360675633646082
Epoch 1054/10000, Prediction Accuracy = 47.114%, Loss = 1.5996800422668458
Epoch: 1054, Batch Gradient Norm: 33.94385465362072
Epoch: 1054, Batch Gradient Norm after: 22.360676943275124
Epoch 1055/10000, Prediction Accuracy = 47.178%, Loss = 1.5931824684143066
Epoch: 1055, Batch Gradient Norm: 35.83234002380887
Epoch: 1055, Batch Gradient Norm after: 22.360675498652913
Epoch 1056/10000, Prediction Accuracy = 47.124%, Loss = 1.598426914215088
Epoch: 1056, Batch Gradient Norm: 33.97475080847026
Epoch: 1056, Batch Gradient Norm after: 22.36067868577319
Epoch 1057/10000, Prediction Accuracy = 47.18%, Loss = 1.5919601678848267
Epoch: 1057, Batch Gradient Norm: 35.85358699690759
Epoch: 1057, Batch Gradient Norm after: 22.360678722484284
Epoch 1058/10000, Prediction Accuracy = 47.13199999999999%, Loss = 1.5971786499023437
Epoch: 1058, Batch Gradient Norm: 34.00348170559337
Epoch: 1058, Batch Gradient Norm after: 22.36067731451906
Epoch 1059/10000, Prediction Accuracy = 47.19%, Loss = 1.5907495498657227
Epoch: 1059, Batch Gradient Norm: 35.87762752200613
Epoch: 1059, Batch Gradient Norm after: 22.36067615831908
Epoch 1060/10000, Prediction Accuracy = 47.14%, Loss = 1.595942234992981
Epoch: 1060, Batch Gradient Norm: 34.03151101741091
Epoch: 1060, Batch Gradient Norm after: 22.36067807282298
Epoch 1061/10000, Prediction Accuracy = 47.199999999999996%, Loss = 1.589532494544983
Epoch: 1061, Batch Gradient Norm: 35.892700630163105
Epoch: 1061, Batch Gradient Norm after: 22.360678306709215
Epoch 1062/10000, Prediction Accuracy = 47.166000000000004%, Loss = 1.5946925401687622
Epoch: 1062, Batch Gradient Norm: 34.06136134045885
Epoch: 1062, Batch Gradient Norm after: 22.360677947095372
Epoch 1063/10000, Prediction Accuracy = 47.208%, Loss = 1.5883425235748292
Epoch: 1063, Batch Gradient Norm: 35.930257773243085
Epoch: 1063, Batch Gradient Norm after: 22.360676780866903
Epoch 1064/10000, Prediction Accuracy = 47.17%, Loss = 1.5935003757476807
Epoch: 1064, Batch Gradient Norm: 34.08984292428027
Epoch: 1064, Batch Gradient Norm after: 22.36067840953069
Epoch 1065/10000, Prediction Accuracy = 47.21999999999999%, Loss = 1.5871492862701415
Epoch: 1065, Batch Gradient Norm: 35.97357417837347
Epoch: 1065, Batch Gradient Norm after: 22.360675120505736
Epoch 1066/10000, Prediction Accuracy = 47.166%, Loss = 1.5923356294631958
Epoch: 1066, Batch Gradient Norm: 34.11899115314757
Epoch: 1066, Batch Gradient Norm after: 22.360677512588445
Epoch 1067/10000, Prediction Accuracy = 47.214%, Loss = 1.5859458208084107
Epoch: 1067, Batch Gradient Norm: 36.00763174086247
Epoch: 1067, Batch Gradient Norm after: 22.360676868498178
Epoch 1068/10000, Prediction Accuracy = 47.186%, Loss = 1.5911598920822143
Epoch: 1068, Batch Gradient Norm: 34.14763481583474
Epoch: 1068, Batch Gradient Norm after: 22.36067911170302
Epoch 1069/10000, Prediction Accuracy = 47.220000000000006%, Loss = 1.5847445011138916
Epoch: 1069, Batch Gradient Norm: 36.04145833922149
Epoch: 1069, Batch Gradient Norm after: 22.360674939268904
Epoch 1070/10000, Prediction Accuracy = 47.19%, Loss = 1.5899651765823364
Epoch: 1070, Batch Gradient Norm: 34.175953914022635
Epoch: 1070, Batch Gradient Norm after: 22.36067653823783
Epoch 1071/10000, Prediction Accuracy = 47.230000000000004%, Loss = 1.5835500955581665
Epoch: 1071, Batch Gradient Norm: 36.06467312751229
Epoch: 1071, Batch Gradient Norm after: 22.360675226669564
Epoch 1072/10000, Prediction Accuracy = 47.206%, Loss = 1.5887463808059692
Epoch: 1072, Batch Gradient Norm: 34.20631966623637
Epoch: 1072, Batch Gradient Norm after: 22.3606760833638
Epoch 1073/10000, Prediction Accuracy = 47.224000000000004%, Loss = 1.582362413406372
Epoch: 1073, Batch Gradient Norm: 36.09354015436629
Epoch: 1073, Batch Gradient Norm after: 22.360677458081117
Epoch 1074/10000, Prediction Accuracy = 47.209999999999994%, Loss = 1.5875485420227051
Epoch: 1074, Batch Gradient Norm: 34.236231076899394
Epoch: 1074, Batch Gradient Norm after: 22.36067711690537
Epoch 1075/10000, Prediction Accuracy = 47.226%, Loss = 1.5811667680740356
Epoch: 1075, Batch Gradient Norm: 36.12464587650154
Epoch: 1075, Batch Gradient Norm after: 22.360676571915246
Epoch 1076/10000, Prediction Accuracy = 47.228%, Loss = 1.5863576889038087
Epoch: 1076, Batch Gradient Norm: 34.26346145322696
Epoch: 1076, Batch Gradient Norm after: 22.36067858146195
Epoch 1077/10000, Prediction Accuracy = 47.230000000000004%, Loss = 1.579975438117981
Epoch: 1077, Batch Gradient Norm: 36.14915455291739
Epoch: 1077, Batch Gradient Norm after: 22.360676597858838
Epoch 1078/10000, Prediction Accuracy = 47.233999999999995%, Loss = 1.585154128074646
Epoch: 1078, Batch Gradient Norm: 34.29443882877879
Epoch: 1078, Batch Gradient Norm after: 22.360676715434597
Epoch 1079/10000, Prediction Accuracy = 47.234%, Loss = 1.5787887811660766
Epoch: 1079, Batch Gradient Norm: 36.17936544697782
Epoch: 1079, Batch Gradient Norm after: 22.360677405190906
Epoch 1080/10000, Prediction Accuracy = 47.23%, Loss = 1.5839639186859131
Epoch: 1080, Batch Gradient Norm: 34.32406881221755
Epoch: 1080, Batch Gradient Norm after: 22.36067743835633
Epoch 1081/10000, Prediction Accuracy = 47.242000000000004%, Loss = 1.577611780166626
Epoch: 1081, Batch Gradient Norm: 36.218913903895356
Epoch: 1081, Batch Gradient Norm after: 22.36067881917756
Epoch 1082/10000, Prediction Accuracy = 47.242000000000004%, Loss = 1.5828065395355224
Epoch: 1082, Batch Gradient Norm: 34.35390084481405
Epoch: 1082, Batch Gradient Norm after: 22.360677130302417
Epoch 1083/10000, Prediction Accuracy = 47.246%, Loss = 1.5764248847961426
Epoch: 1083, Batch Gradient Norm: 36.256656579365085
Epoch: 1083, Batch Gradient Norm after: 22.36067652532077
Epoch 1084/10000, Prediction Accuracy = 47.244%, Loss = 1.581658673286438
Epoch: 1084, Batch Gradient Norm: 34.382114686484144
Epoch: 1084, Batch Gradient Norm after: 22.360675442245753
Epoch 1085/10000, Prediction Accuracy = 47.266000000000005%, Loss = 1.5752455949783326
Epoch: 1085, Batch Gradient Norm: 36.29434985006846
Epoch: 1085, Batch Gradient Norm after: 22.36067750266421
Epoch 1086/10000, Prediction Accuracy = 47.244%, Loss = 1.5805075407028197
Epoch: 1086, Batch Gradient Norm: 34.411255467314206
Epoch: 1086, Batch Gradient Norm after: 22.360677303288533
Epoch 1087/10000, Prediction Accuracy = 47.282000000000004%, Loss = 1.574062466621399
Epoch: 1087, Batch Gradient Norm: 36.32550768796105
Epoch: 1087, Batch Gradient Norm after: 22.360677708043198
Epoch 1088/10000, Prediction Accuracy = 47.254000000000005%, Loss = 1.5793561458587646
Epoch: 1088, Batch Gradient Norm: 34.44124775664863
Epoch: 1088, Batch Gradient Norm after: 22.36067575466815
Epoch 1089/10000, Prediction Accuracy = 47.3%, Loss = 1.5728916883468629
Epoch: 1089, Batch Gradient Norm: 36.36561112550574
Epoch: 1089, Batch Gradient Norm after: 22.360678350412325
Epoch 1090/10000, Prediction Accuracy = 47.25%, Loss = 1.5782231330871581
Epoch: 1090, Batch Gradient Norm: 34.469925278590395
Epoch: 1090, Batch Gradient Norm after: 22.360677793123617
Epoch 1091/10000, Prediction Accuracy = 47.3%, Loss = 1.571722149848938
Epoch: 1091, Batch Gradient Norm: 36.397447176332356
Epoch: 1091, Batch Gradient Norm after: 22.360676761319052
Epoch 1092/10000, Prediction Accuracy = 47.264%, Loss = 1.5770764589309691
Epoch: 1092, Batch Gradient Norm: 34.50029896442665
Epoch: 1092, Batch Gradient Norm after: 22.360676680873045
Epoch 1093/10000, Prediction Accuracy = 47.306%, Loss = 1.5705654621124268
Epoch: 1093, Batch Gradient Norm: 36.44319405224378
Epoch: 1093, Batch Gradient Norm after: 22.36067821201225
Epoch 1094/10000, Prediction Accuracy = 47.26800000000001%, Loss = 1.5759601831436156
Epoch: 1094, Batch Gradient Norm: 34.52774632364893
Epoch: 1094, Batch Gradient Norm after: 22.360677202858852
Epoch 1095/10000, Prediction Accuracy = 47.306%, Loss = 1.5694054126739503
Epoch: 1095, Batch Gradient Norm: 36.50414188308973
Epoch: 1095, Batch Gradient Norm after: 22.36067836478123
Epoch 1096/10000, Prediction Accuracy = 47.267999999999994%, Loss = 1.5749035120010375
Epoch: 1096, Batch Gradient Norm: 34.55699962692045
Epoch: 1096, Batch Gradient Norm after: 22.360675205924053
Epoch 1097/10000, Prediction Accuracy = 47.306000000000004%, Loss = 1.5682348489761353
Epoch: 1097, Batch Gradient Norm: 36.55451155197024
Epoch: 1097, Batch Gradient Norm after: 22.360676809093103
Epoch 1098/10000, Prediction Accuracy = 47.284%, Loss = 1.5738182067871094
Epoch: 1098, Batch Gradient Norm: 34.58730581848828
Epoch: 1098, Batch Gradient Norm after: 22.3606761031487
Epoch 1099/10000, Prediction Accuracy = 47.308%, Loss = 1.5670717477798461
Epoch: 1099, Batch Gradient Norm: 36.60336753376631
Epoch: 1099, Batch Gradient Norm after: 22.360674188005095
Epoch 1100/10000, Prediction Accuracy = 47.29%, Loss = 1.5727054834365846
Epoch: 1100, Batch Gradient Norm: 34.61562194955074
Epoch: 1100, Batch Gradient Norm after: 22.360676510877788
Epoch 1101/10000, Prediction Accuracy = 47.3%, Loss = 1.565918755531311
Epoch: 1101, Batch Gradient Norm: 36.65255001897279
Epoch: 1101, Batch Gradient Norm after: 22.360675915233216
Epoch 1102/10000, Prediction Accuracy = 47.303999999999995%, Loss = 1.5715980052947998
Epoch: 1102, Batch Gradient Norm: 34.64559780085059
Epoch: 1102, Batch Gradient Norm after: 22.360677903974402
Epoch 1103/10000, Prediction Accuracy = 47.30799999999999%, Loss = 1.5647594213485718
Epoch: 1103, Batch Gradient Norm: 36.70455498377216
Epoch: 1103, Batch Gradient Norm after: 22.360679591406168
Epoch 1104/10000, Prediction Accuracy = 47.312%, Loss = 1.5705070734024047
Epoch: 1104, Batch Gradient Norm: 34.67413159962009
Epoch: 1104, Batch Gradient Norm after: 22.36067656313299
Epoch 1105/10000, Prediction Accuracy = 47.332%, Loss = 1.5636043071746826
Epoch: 1105, Batch Gradient Norm: 36.75000504686496
Epoch: 1105, Batch Gradient Norm after: 22.36067732616459
Epoch 1106/10000, Prediction Accuracy = 47.32%, Loss = 1.5694128274917603
Epoch: 1106, Batch Gradient Norm: 34.70406425980338
Epoch: 1106, Batch Gradient Norm after: 22.360677618397336
Epoch 1107/10000, Prediction Accuracy = 47.336%, Loss = 1.562446618080139
Epoch: 1107, Batch Gradient Norm: 36.790774886624945
Epoch: 1107, Batch Gradient Norm after: 22.360679313826346
Epoch 1108/10000, Prediction Accuracy = 47.318%, Loss = 1.5682964324951172
Epoch: 1108, Batch Gradient Norm: 34.734420773605514
Epoch: 1108, Batch Gradient Norm after: 22.360676702745025
Epoch 1109/10000, Prediction Accuracy = 47.352%, Loss = 1.561293888092041
Epoch: 1109, Batch Gradient Norm: 36.827399597397616
Epoch: 1109, Batch Gradient Norm after: 22.360677961047106
Epoch 1110/10000, Prediction Accuracy = 47.327999999999996%, Loss = 1.5671677112579345
Epoch: 1110, Batch Gradient Norm: 34.765113080117445
Epoch: 1110, Batch Gradient Norm after: 22.360678209473807
Epoch 1111/10000, Prediction Accuracy = 47.367999999999995%, Loss = 1.5601446151733398
Epoch: 1111, Batch Gradient Norm: 36.87499882535445
Epoch: 1111, Batch Gradient Norm after: 22.36067428712637
Epoch 1112/10000, Prediction Accuracy = 47.336%, Loss = 1.5660772562026977
Epoch: 1112, Batch Gradient Norm: 34.794647191222445
Epoch: 1112, Batch Gradient Norm after: 22.360678314183943
Epoch 1113/10000, Prediction Accuracy = 47.364%, Loss = 1.5590128660202027
Epoch: 1113, Batch Gradient Norm: 36.922920580396685
Epoch: 1113, Batch Gradient Norm after: 22.360677377175353
Epoch 1114/10000, Prediction Accuracy = 47.34400000000001%, Loss = 1.5649980306625366
Epoch: 1114, Batch Gradient Norm: 34.8227370211904
Epoch: 1114, Batch Gradient Norm after: 22.36067588393912
Epoch 1115/10000, Prediction Accuracy = 47.374%, Loss = 1.557873296737671
Epoch: 1115, Batch Gradient Norm: 36.960947104394265
Epoch: 1115, Batch Gradient Norm after: 22.36067738188422
Epoch 1116/10000, Prediction Accuracy = 47.348%, Loss = 1.5638936519622804
Epoch: 1116, Batch Gradient Norm: 34.84971209874714
Epoch: 1116, Batch Gradient Norm after: 22.360676932781594
Epoch 1117/10000, Prediction Accuracy = 47.384%, Loss = 1.5567435264587401
Epoch: 1117, Batch Gradient Norm: 36.99790518198236
Epoch: 1117, Batch Gradient Norm after: 22.36067907418844
Epoch 1118/10000, Prediction Accuracy = 47.349999999999994%, Loss = 1.562784481048584
Epoch: 1118, Batch Gradient Norm: 34.87852460867751
Epoch: 1118, Batch Gradient Norm after: 22.36067863845896
Epoch 1119/10000, Prediction Accuracy = 47.391999999999996%, Loss = 1.555607533454895
Epoch: 1119, Batch Gradient Norm: 37.035186287875625
Epoch: 1119, Batch Gradient Norm after: 22.360677343813215
Epoch 1120/10000, Prediction Accuracy = 47.367999999999995%, Loss = 1.5616705894470215
Epoch: 1120, Batch Gradient Norm: 34.90918350561446
Epoch: 1120, Batch Gradient Norm after: 22.360678799353526
Epoch 1121/10000, Prediction Accuracy = 47.396%, Loss = 1.5544779777526856
Epoch: 1121, Batch Gradient Norm: 37.078907522993546
Epoch: 1121, Batch Gradient Norm after: 22.360677261055443
Epoch 1122/10000, Prediction Accuracy = 47.374%, Loss = 1.5605839014053344
Epoch: 1122, Batch Gradient Norm: 34.93864800617168
Epoch: 1122, Batch Gradient Norm after: 22.36067596244507
Epoch 1123/10000, Prediction Accuracy = 47.41199999999999%, Loss = 1.553349757194519
Epoch: 1123, Batch Gradient Norm: 37.130463531791726
Epoch: 1123, Batch Gradient Norm after: 22.360677857853847
Epoch 1124/10000, Prediction Accuracy = 47.382%, Loss = 1.5595319747924805
Epoch: 1124, Batch Gradient Norm: 34.968507299106406
Epoch: 1124, Batch Gradient Norm after: 22.360677647462104
Epoch 1125/10000, Prediction Accuracy = 47.418%, Loss = 1.552214550971985
Epoch: 1125, Batch Gradient Norm: 37.17612575703037
Epoch: 1125, Batch Gradient Norm after: 22.360676762779537
Epoch 1126/10000, Prediction Accuracy = 47.384%, Loss = 1.5584569931030274
Epoch: 1126, Batch Gradient Norm: 34.995814234858756
Epoch: 1126, Batch Gradient Norm after: 22.360678540006553
Epoch 1127/10000, Prediction Accuracy = 47.42999999999999%, Loss = 1.5510810375213624
Epoch: 1127, Batch Gradient Norm: 37.22433960321012
Epoch: 1127, Batch Gradient Norm after: 22.36067597599768
Epoch 1128/10000, Prediction Accuracy = 47.396%, Loss = 1.5574025154113769
Epoch: 1128, Batch Gradient Norm: 35.02394512107199
Epoch: 1128, Batch Gradient Norm after: 22.360678278549326
Epoch 1129/10000, Prediction Accuracy = 47.434000000000005%, Loss = 1.5499548196792603
Epoch: 1129, Batch Gradient Norm: 37.27557327084243
Epoch: 1129, Batch Gradient Norm after: 22.360678080178445
Epoch 1130/10000, Prediction Accuracy = 47.391999999999996%, Loss = 1.5563480854034424
Epoch: 1130, Batch Gradient Norm: 35.05451898981319
Epoch: 1130, Batch Gradient Norm after: 22.360678156379294
Epoch 1131/10000, Prediction Accuracy = 47.452%, Loss = 1.5488271236419677
Epoch: 1131, Batch Gradient Norm: 37.31716605616635
Epoch: 1131, Batch Gradient Norm after: 22.360677431110044
Epoch 1132/10000, Prediction Accuracy = 47.398%, Loss = 1.5552674531936646
Epoch: 1132, Batch Gradient Norm: 35.083849861710384
Epoch: 1132, Batch Gradient Norm after: 22.36067646029512
Epoch 1133/10000, Prediction Accuracy = 47.458000000000006%, Loss = 1.5477108001708983
Epoch: 1133, Batch Gradient Norm: 37.359189334028564
Epoch: 1133, Batch Gradient Norm after: 22.360678190774195
Epoch 1134/10000, Prediction Accuracy = 47.4%, Loss = 1.554186725616455
Epoch: 1134, Batch Gradient Norm: 35.113754941126054
Epoch: 1134, Batch Gradient Norm after: 22.360676345346725
Epoch 1135/10000, Prediction Accuracy = 47.46%, Loss = 1.546589183807373
Epoch: 1135, Batch Gradient Norm: 37.396021306271486
Epoch: 1135, Batch Gradient Norm after: 22.360678167218037
Epoch 1136/10000, Prediction Accuracy = 47.422%, Loss = 1.5530860900878907
Epoch: 1136, Batch Gradient Norm: 35.141403387963045
Epoch: 1136, Batch Gradient Norm after: 22.360679017467188
Epoch 1137/10000, Prediction Accuracy = 47.474000000000004%, Loss = 1.5454745054244996
Epoch: 1137, Batch Gradient Norm: 37.42140955065256
Epoch: 1137, Batch Gradient Norm after: 22.360679002662
Epoch 1138/10000, Prediction Accuracy = 47.41799999999999%, Loss = 1.5519685983657836
Epoch: 1138, Batch Gradient Norm: 35.17140629894579
Epoch: 1138, Batch Gradient Norm after: 22.360678734987783
Epoch 1139/10000, Prediction Accuracy = 47.488%, Loss = 1.5443660020828247
Epoch: 1139, Batch Gradient Norm: 37.43681250395014
Epoch: 1139, Batch Gradient Norm after: 22.360677961448182
Epoch 1140/10000, Prediction Accuracy = 47.424%, Loss = 1.5508145093917847
Epoch: 1140, Batch Gradient Norm: 35.19798290049452
Epoch: 1140, Batch Gradient Norm after: 22.36067759255225
Epoch 1141/10000, Prediction Accuracy = 47.5%, Loss = 1.543250036239624
Epoch: 1141, Batch Gradient Norm: 37.45526478375561
Epoch: 1141, Batch Gradient Norm after: 22.3606795565696
Epoch 1142/10000, Prediction Accuracy = 47.42%, Loss = 1.5496756315231324
Epoch: 1142, Batch Gradient Norm: 35.22495228597851
Epoch: 1142, Batch Gradient Norm after: 22.36067659101638
Epoch 1143/10000, Prediction Accuracy = 47.50600000000001%, Loss = 1.5421382188796997
Epoch: 1143, Batch Gradient Norm: 37.468253050098554
Epoch: 1143, Batch Gradient Norm after: 22.36067870096507
Epoch 1144/10000, Prediction Accuracy = 47.416000000000004%, Loss = 1.5485295295715331
Epoch: 1144, Batch Gradient Norm: 35.25089141543755
Epoch: 1144, Batch Gradient Norm after: 22.360675159343774
Epoch 1145/10000, Prediction Accuracy = 47.516000000000005%, Loss = 1.5410362243652345
Epoch: 1145, Batch Gradient Norm: 37.48346135746345
Epoch: 1145, Batch Gradient Norm after: 22.36067948530543
Epoch 1146/10000, Prediction Accuracy = 47.419999999999995%, Loss = 1.5473824262619018
Epoch: 1146, Batch Gradient Norm: 35.28041683294851
Epoch: 1146, Batch Gradient Norm after: 22.36067837658477
Epoch 1147/10000, Prediction Accuracy = 47.528000000000006%, Loss = 1.5399414539337157
Epoch: 1147, Batch Gradient Norm: 37.50833685077831
Epoch: 1147, Batch Gradient Norm after: 22.360678849754542
Epoch 1148/10000, Prediction Accuracy = 47.424%, Loss = 1.5462748765945435
Epoch: 1148, Batch Gradient Norm: 35.30765716440146
Epoch: 1148, Batch Gradient Norm after: 22.360676164118075
Epoch 1149/10000, Prediction Accuracy = 47.536%, Loss = 1.5388482570648194
Epoch: 1149, Batch Gradient Norm: 37.53946583722393
Epoch: 1149, Batch Gradient Norm after: 22.360679955461958
Epoch 1150/10000, Prediction Accuracy = 47.438%, Loss = 1.5451929807662963
Epoch: 1150, Batch Gradient Norm: 35.33460996834019
Epoch: 1150, Batch Gradient Norm after: 22.3606769545523
Epoch 1151/10000, Prediction Accuracy = 47.534000000000006%, Loss = 1.5377509832382201
Epoch: 1151, Batch Gradient Norm: 37.57230882593382
Epoch: 1151, Batch Gradient Norm after: 22.36067951912775
Epoch 1152/10000, Prediction Accuracy = 47.45399999999999%, Loss = 1.5441200494766236
Epoch: 1152, Batch Gradient Norm: 35.361208937836594
Epoch: 1152, Batch Gradient Norm after: 22.360675283320997
Epoch 1153/10000, Prediction Accuracy = 47.546%, Loss = 1.536652970314026
Epoch: 1153, Batch Gradient Norm: 37.59946501862545
Epoch: 1153, Batch Gradient Norm after: 22.36067970189356
Epoch 1154/10000, Prediction Accuracy = 47.46%, Loss = 1.5430256843566894
Epoch: 1154, Batch Gradient Norm: 35.39019472794184
Epoch: 1154, Batch Gradient Norm after: 22.360678213357506
Epoch 1155/10000, Prediction Accuracy = 47.54%, Loss = 1.5355604410171508
Epoch: 1155, Batch Gradient Norm: 37.635556377941725
Epoch: 1155, Batch Gradient Norm after: 22.36068146858118
Epoch 1156/10000, Prediction Accuracy = 47.47800000000001%, Loss = 1.5419654369354248
Epoch: 1156, Batch Gradient Norm: 35.419585339756786
Epoch: 1156, Batch Gradient Norm after: 22.360676467294876
Epoch 1157/10000, Prediction Accuracy = 47.538%, Loss = 1.5344866514205933
Epoch: 1157, Batch Gradient Norm: 37.67486634483282
Epoch: 1157, Batch Gradient Norm after: 22.360677340090433
Epoch 1158/10000, Prediction Accuracy = 47.486000000000004%, Loss = 1.5409205675125122
Epoch: 1158, Batch Gradient Norm: 35.44742465228659
Epoch: 1158, Batch Gradient Norm after: 22.360677721643132
Epoch 1159/10000, Prediction Accuracy = 47.535999999999994%, Loss = 1.53340585231781
Epoch: 1159, Batch Gradient Norm: 37.711337414309405
Epoch: 1159, Batch Gradient Norm after: 22.36067818698221
Epoch 1160/10000, Prediction Accuracy = 47.504000000000005%, Loss = 1.5398716449737548
Epoch: 1160, Batch Gradient Norm: 35.47350617154845
Epoch: 1160, Batch Gradient Norm after: 22.360677970092155
Epoch 1161/10000, Prediction Accuracy = 47.550000000000004%, Loss = 1.5323302507400514
Epoch: 1161, Batch Gradient Norm: 37.7638582168579
Epoch: 1161, Batch Gradient Norm after: 22.360677916863118
Epoch 1162/10000, Prediction Accuracy = 47.498%, Loss = 1.5388651371002198
Epoch: 1162, Batch Gradient Norm: 35.50066785429125
Epoch: 1162, Batch Gradient Norm after: 22.36067700177942
Epoch 1163/10000, Prediction Accuracy = 47.556%, Loss = 1.531251883506775
Epoch: 1163, Batch Gradient Norm: 37.820092047511544
Epoch: 1163, Batch Gradient Norm after: 22.360677262488377
Epoch 1164/10000, Prediction Accuracy = 47.502%, Loss = 1.537882685661316
Epoch: 1164, Batch Gradient Norm: 35.52663279337113
Epoch: 1164, Batch Gradient Norm after: 22.360675894193786
Epoch 1165/10000, Prediction Accuracy = 47.565999999999995%, Loss = 1.5301746606826783
Epoch: 1165, Batch Gradient Norm: 37.875492562319636
Epoch: 1165, Batch Gradient Norm after: 22.36067594748548
Epoch 1166/10000, Prediction Accuracy = 47.508%, Loss = 1.5368945837020873
Epoch: 1166, Batch Gradient Norm: 35.55455222769838
Epoch: 1166, Batch Gradient Norm after: 22.360677027605224
Epoch 1167/10000, Prediction Accuracy = 47.568%, Loss = 1.5290996551513671
Epoch: 1167, Batch Gradient Norm: 37.92024325012173
Epoch: 1167, Batch Gradient Norm after: 22.360678311677496
Epoch 1168/10000, Prediction Accuracy = 47.510000000000005%, Loss = 1.5358763456344604
Epoch: 1168, Batch Gradient Norm: 35.58207116660566
Epoch: 1168, Batch Gradient Norm after: 22.360678112524
Epoch 1169/10000, Prediction Accuracy = 47.577999999999996%, Loss = 1.5280303955078125
Epoch: 1169, Batch Gradient Norm: 37.969574915674286
Epoch: 1169, Batch Gradient Norm after: 22.360678106412696
Epoch 1170/10000, Prediction Accuracy = 47.512%, Loss = 1.534878420829773
Epoch: 1170, Batch Gradient Norm: 35.60919275390716
Epoch: 1170, Batch Gradient Norm after: 22.36067708002185
Epoch 1171/10000, Prediction Accuracy = 47.586%, Loss = 1.5269708395004273
Epoch: 1171, Batch Gradient Norm: 38.01853219000397
Epoch: 1171, Batch Gradient Norm after: 22.360677347548613
Epoch 1172/10000, Prediction Accuracy = 47.516%, Loss = 1.5338732719421386
Epoch: 1172, Batch Gradient Norm: 35.638820720266274
Epoch: 1172, Batch Gradient Norm after: 22.360678471872873
Epoch 1173/10000, Prediction Accuracy = 47.576%, Loss = 1.5258944511413575
Epoch: 1173, Batch Gradient Norm: 38.0584667644975
Epoch: 1173, Batch Gradient Norm after: 22.36067618659246
Epoch 1174/10000, Prediction Accuracy = 47.512%, Loss = 1.5328496456146241
Epoch: 1174, Batch Gradient Norm: 35.66370106374278
Epoch: 1174, Batch Gradient Norm after: 22.36067446614169
Epoch 1175/10000, Prediction Accuracy = 47.583999999999996%, Loss = 1.5248240947723388
Epoch: 1175, Batch Gradient Norm: 38.09241832995706
Epoch: 1175, Batch Gradient Norm after: 22.360679252689593
Epoch 1176/10000, Prediction Accuracy = 47.524%, Loss = 1.531804370880127
Epoch: 1176, Batch Gradient Norm: 35.690151255923205
Epoch: 1176, Batch Gradient Norm after: 22.360677995321993
Epoch 1177/10000, Prediction Accuracy = 47.596000000000004%, Loss = 1.5237616777420044
Epoch: 1177, Batch Gradient Norm: 38.13693787560905
Epoch: 1177, Batch Gradient Norm after: 22.3606770002248
Epoch 1178/10000, Prediction Accuracy = 47.538%, Loss = 1.5307953119277955
Epoch: 1178, Batch Gradient Norm: 35.718578432435905
Epoch: 1178, Batch Gradient Norm after: 22.360677864088082
Epoch 1179/10000, Prediction Accuracy = 47.61%, Loss = 1.5227055311203004
Epoch: 1179, Batch Gradient Norm: 38.181891004707836
Epoch: 1179, Batch Gradient Norm after: 22.360677027090237
Epoch 1180/10000, Prediction Accuracy = 47.544%, Loss = 1.5297897815704347
Epoch: 1180, Batch Gradient Norm: 35.74432122179675
Epoch: 1180, Batch Gradient Norm after: 22.360677205006734
Epoch 1181/10000, Prediction Accuracy = 47.61600000000001%, Loss = 1.5216489791870118
Epoch: 1181, Batch Gradient Norm: 38.21738301681713
Epoch: 1181, Batch Gradient Norm after: 22.36067993732986
Epoch 1182/10000, Prediction Accuracy = 47.552%, Loss = 1.5287668466567994
Epoch: 1182, Batch Gradient Norm: 35.77075527403272
Epoch: 1182, Batch Gradient Norm after: 22.36067751087541
Epoch 1183/10000, Prediction Accuracy = 47.624%, Loss = 1.5205992698669433
Epoch: 1183, Batch Gradient Norm: 38.25732227206349
Epoch: 1183, Batch Gradient Norm after: 22.360678946153435
Epoch 1184/10000, Prediction Accuracy = 47.56%, Loss = 1.5277534246444702
Epoch: 1184, Batch Gradient Norm: 35.79697515892117
Epoch: 1184, Batch Gradient Norm after: 22.360677561509497
Epoch 1185/10000, Prediction Accuracy = 47.632000000000005%, Loss = 1.519550633430481
Epoch: 1185, Batch Gradient Norm: 38.29422656756306
Epoch: 1185, Batch Gradient Norm after: 22.360678530524442
Epoch 1186/10000, Prediction Accuracy = 47.564%, Loss = 1.5267345905303955
Epoch: 1186, Batch Gradient Norm: 35.82177718105656
Epoch: 1186, Batch Gradient Norm after: 22.36067642351864
Epoch 1187/10000, Prediction Accuracy = 47.634%, Loss = 1.5185044288635254
Epoch: 1187, Batch Gradient Norm: 38.33374233930104
Epoch: 1187, Batch Gradient Norm after: 22.36067789925635
Epoch 1188/10000, Prediction Accuracy = 47.568%, Loss = 1.5257250785827636
Epoch: 1188, Batch Gradient Norm: 35.846082957892456
Epoch: 1188, Batch Gradient Norm after: 22.360676828649158
Epoch 1189/10000, Prediction Accuracy = 47.634%, Loss = 1.517458724975586
Epoch: 1189, Batch Gradient Norm: 38.37653064892575
Epoch: 1189, Batch Gradient Norm after: 22.360676553766993
Epoch 1190/10000, Prediction Accuracy = 47.574%, Loss = 1.5247329235076905
Epoch: 1190, Batch Gradient Norm: 35.869929675256735
Epoch: 1190, Batch Gradient Norm after: 22.36067733632872
Epoch 1191/10000, Prediction Accuracy = 47.644000000000005%, Loss = 1.5164169788360595
Epoch: 1191, Batch Gradient Norm: 38.42501327554062
Epoch: 1191, Batch Gradient Norm after: 22.36067747583161
Epoch 1192/10000, Prediction Accuracy = 47.592%, Loss = 1.523757553100586
Epoch: 1192, Batch Gradient Norm: 35.89739508680982
Epoch: 1192, Batch Gradient Norm after: 22.36067694387148
Epoch 1193/10000, Prediction Accuracy = 47.650000000000006%, Loss = 1.5153737545013428
Epoch: 1193, Batch Gradient Norm: 38.46567061650455
Epoch: 1193, Batch Gradient Norm after: 22.360678913094848
Epoch 1194/10000, Prediction Accuracy = 47.598%, Loss = 1.5227646827697754
Epoch: 1194, Batch Gradient Norm: 35.92653746527782
Epoch: 1194, Batch Gradient Norm after: 22.360676157422365
Epoch 1195/10000, Prediction Accuracy = 47.644%, Loss = 1.514337182044983
Epoch: 1195, Batch Gradient Norm: 38.497932615404046
Epoch: 1195, Batch Gradient Norm after: 22.360680024518462
Epoch 1196/10000, Prediction Accuracy = 47.604%, Loss = 1.5217451810836793
Epoch: 1196, Batch Gradient Norm: 35.952911329589455
Epoch: 1196, Batch Gradient Norm after: 22.360679318058285
Epoch 1197/10000, Prediction Accuracy = 47.634%, Loss = 1.5133026599884034
Epoch: 1197, Batch Gradient Norm: 38.52864414024056
Epoch: 1197, Batch Gradient Norm after: 22.360678403194452
Epoch 1198/10000, Prediction Accuracy = 47.62%, Loss = 1.5207320690155028
Epoch: 1198, Batch Gradient Norm: 35.97862282302401
Epoch: 1198, Batch Gradient Norm after: 22.360678449541712
Epoch 1199/10000, Prediction Accuracy = 47.644%, Loss = 1.5122724533081056
Epoch: 1199, Batch Gradient Norm: 38.565771789028496
Epoch: 1199, Batch Gradient Norm after: 22.360678612663264
Epoch 1200/10000, Prediction Accuracy = 47.64%, Loss = 1.5197344064712524
Epoch: 1200, Batch Gradient Norm: 36.00580554062649
Epoch: 1200, Batch Gradient Norm after: 22.360678280519558
Epoch 1201/10000, Prediction Accuracy = 47.64399999999999%, Loss = 1.5112384796142577
Epoch: 1201, Batch Gradient Norm: 38.597920899009715
Epoch: 1201, Batch Gradient Norm after: 22.360678920747873
Epoch 1202/10000, Prediction Accuracy = 47.645999999999994%, Loss = 1.518728756904602
Epoch: 1202, Batch Gradient Norm: 36.03308546918708
Epoch: 1202, Batch Gradient Norm after: 22.36067607969732
Epoch 1203/10000, Prediction Accuracy = 47.646%, Loss = 1.5102149963378906
Epoch: 1203, Batch Gradient Norm: 38.63328972681133
Epoch: 1203, Batch Gradient Norm after: 22.360678351098112
Epoch 1204/10000, Prediction Accuracy = 47.650000000000006%, Loss = 1.51773202419281
Epoch: 1204, Batch Gradient Norm: 36.056781883624005
Epoch: 1204, Batch Gradient Norm after: 22.36067890199662
Epoch 1205/10000, Prediction Accuracy = 47.64999999999999%, Loss = 1.5091952800750732
Epoch: 1205, Batch Gradient Norm: 38.66444043903145
Epoch: 1205, Batch Gradient Norm after: 22.360677675398524
Epoch 1206/10000, Prediction Accuracy = 47.664%, Loss = 1.51672682762146
Epoch: 1206, Batch Gradient Norm: 36.08063833819605
Epoch: 1206, Batch Gradient Norm after: 22.36067685002266
Epoch 1207/10000, Prediction Accuracy = 47.652%, Loss = 1.5081697463989259
Epoch: 1207, Batch Gradient Norm: 38.69564297256593
Epoch: 1207, Batch Gradient Norm after: 22.36067871198754
Epoch 1208/10000, Prediction Accuracy = 47.668000000000006%, Loss = 1.5157175064086914
Epoch: 1208, Batch Gradient Norm: 36.10590157426812
Epoch: 1208, Batch Gradient Norm after: 22.36067846839787
Epoch 1209/10000, Prediction Accuracy = 47.65%, Loss = 1.5071475505828857
Epoch: 1209, Batch Gradient Norm: 38.72816627470927
Epoch: 1209, Batch Gradient Norm after: 22.360680769982313
Epoch 1210/10000, Prediction Accuracy = 47.676%, Loss = 1.5147248744964599
Epoch: 1210, Batch Gradient Norm: 36.13171055439446
Epoch: 1210, Batch Gradient Norm after: 22.36067691392073
Epoch 1211/10000, Prediction Accuracy = 47.658%, Loss = 1.506141471862793
Epoch: 1211, Batch Gradient Norm: 38.77367037681581
Epoch: 1211, Batch Gradient Norm after: 22.360675571210802
Epoch 1212/10000, Prediction Accuracy = 47.68%, Loss = 1.5137898921966553
Epoch: 1212, Batch Gradient Norm: 36.156803860375405
Epoch: 1212, Batch Gradient Norm after: 22.360676203626017
Epoch 1213/10000, Prediction Accuracy = 47.660000000000004%, Loss = 1.505126142501831
Epoch: 1213, Batch Gradient Norm: 38.819294856793995
Epoch: 1213, Batch Gradient Norm after: 22.360676557014205
Epoch 1214/10000, Prediction Accuracy = 47.686%, Loss = 1.5128487348556519
Epoch: 1214, Batch Gradient Norm: 36.18166633162751
Epoch: 1214, Batch Gradient Norm after: 22.36067793267005
Epoch 1215/10000, Prediction Accuracy = 47.67%, Loss = 1.504122543334961
Epoch: 1215, Batch Gradient Norm: 38.86828971263875
Epoch: 1215, Batch Gradient Norm after: 22.360677384802617
Epoch 1216/10000, Prediction Accuracy = 47.69%, Loss = 1.5119182586669921
Epoch: 1216, Batch Gradient Norm: 36.20355575438998
Epoch: 1216, Batch Gradient Norm after: 22.360678506193555
Epoch 1217/10000, Prediction Accuracy = 47.676%, Loss = 1.5031111717224122
Epoch: 1217, Batch Gradient Norm: 38.91781408947991
Epoch: 1217, Batch Gradient Norm after: 22.360677383938537
Epoch 1218/10000, Prediction Accuracy = 47.692%, Loss = 1.510977029800415
Epoch: 1218, Batch Gradient Norm: 36.22771735820326
Epoch: 1218, Batch Gradient Norm after: 22.360676295964122
Epoch 1219/10000, Prediction Accuracy = 47.692%, Loss = 1.5021093130111693
Epoch: 1219, Batch Gradient Norm: 38.97121214668646
Epoch: 1219, Batch Gradient Norm after: 22.3606772055003
Epoch 1220/10000, Prediction Accuracy = 47.693999999999996%, Loss = 1.5100642442703247
Epoch: 1220, Batch Gradient Norm: 36.254511225913625
Epoch: 1220, Batch Gradient Norm after: 22.360676400103987
Epoch 1221/10000, Prediction Accuracy = 47.705999999999996%, Loss = 1.5011013984680175
Epoch: 1221, Batch Gradient Norm: 39.0142997739362
Epoch: 1221, Batch Gradient Norm after: 22.360676841995303
Epoch 1222/10000, Prediction Accuracy = 47.70399999999999%, Loss = 1.5091211080551148
Epoch: 1222, Batch Gradient Norm: 36.282242702681046
Epoch: 1222, Batch Gradient Norm after: 22.360675531066946
Epoch 1223/10000, Prediction Accuracy = 47.70400000000001%, Loss = 1.500094485282898
Epoch: 1223, Batch Gradient Norm: 39.05732132339318
Epoch: 1223, Batch Gradient Norm after: 22.36067714355653
Epoch 1224/10000, Prediction Accuracy = 47.714%, Loss = 1.5081685781478882
Epoch: 1224, Batch Gradient Norm: 36.30879159216047
Epoch: 1224, Batch Gradient Norm after: 22.360676783246042
Epoch 1225/10000, Prediction Accuracy = 47.706%, Loss = 1.4990864515304565
Epoch: 1225, Batch Gradient Norm: 39.104520994759895
Epoch: 1225, Batch Gradient Norm after: 22.360677738202835
Epoch 1226/10000, Prediction Accuracy = 47.722%, Loss = 1.5072398424148559
Epoch: 1226, Batch Gradient Norm: 36.332519952927946
Epoch: 1226, Batch Gradient Norm after: 22.360676004181403
Epoch 1227/10000, Prediction Accuracy = 47.70799999999999%, Loss = 1.4980753660202026
Epoch: 1227, Batch Gradient Norm: 39.15454907629114
Epoch: 1227, Batch Gradient Norm after: 22.360679389928478
Epoch 1228/10000, Prediction Accuracy = 47.736%, Loss = 1.5063275814056396
Epoch: 1228, Batch Gradient Norm: 36.35567303345682
Epoch: 1228, Batch Gradient Norm after: 22.360678339343156
Epoch 1229/10000, Prediction Accuracy = 47.72%, Loss = 1.4970720529556274
Epoch: 1229, Batch Gradient Norm: 39.20589968998361
Epoch: 1229, Batch Gradient Norm after: 22.36067717731114
Epoch 1230/10000, Prediction Accuracy = 47.74400000000001%, Loss = 1.5054085731506348
Epoch: 1230, Batch Gradient Norm: 36.3801734685523
Epoch: 1230, Batch Gradient Norm after: 22.360677933346047
Epoch 1231/10000, Prediction Accuracy = 47.718%, Loss = 1.4960806608200072
Epoch: 1231, Batch Gradient Norm: 39.244595052330084
Epoch: 1231, Batch Gradient Norm after: 22.360679977348852
Epoch 1232/10000, Prediction Accuracy = 47.76%, Loss = 1.5044552564620972
Epoch: 1232, Batch Gradient Norm: 36.40404593045891
Epoch: 1232, Batch Gradient Norm after: 22.360677794992586
Epoch 1233/10000, Prediction Accuracy = 47.721999999999994%, Loss = 1.4950763702392578
Epoch: 1233, Batch Gradient Norm: 39.28220797018362
Epoch: 1233, Batch Gradient Norm after: 22.360676447855244
Epoch 1234/10000, Prediction Accuracy = 47.775999999999996%, Loss = 1.5035037279129029
Epoch: 1234, Batch Gradient Norm: 36.426914219348
Epoch: 1234, Batch Gradient Norm after: 22.360676071661793
Epoch 1235/10000, Prediction Accuracy = 47.734%, Loss = 1.4940830945968628
Epoch: 1235, Batch Gradient Norm: 39.326421367683174
Epoch: 1235, Batch Gradient Norm after: 22.36067669264333
Epoch 1236/10000, Prediction Accuracy = 47.772%, Loss = 1.5025782346725465
Epoch: 1236, Batch Gradient Norm: 36.451158448243504
Epoch: 1236, Batch Gradient Norm after: 22.36067859103123
Epoch 1237/10000, Prediction Accuracy = 47.75%, Loss = 1.4931030035018922
Epoch: 1237, Batch Gradient Norm: 39.37754503861173
Epoch: 1237, Batch Gradient Norm after: 22.36067514290332
Epoch 1238/10000, Prediction Accuracy = 47.786%, Loss = 1.5016646146774293
Epoch: 1238, Batch Gradient Norm: 36.476324344316545
Epoch: 1238, Batch Gradient Norm after: 22.360676528517544
Epoch 1239/10000, Prediction Accuracy = 47.754%, Loss = 1.49211847782135
Epoch: 1239, Batch Gradient Norm: 39.420252419454364
Epoch: 1239, Batch Gradient Norm after: 22.36067668570015
Epoch 1240/10000, Prediction Accuracy = 47.782000000000004%, Loss = 1.5007307529449463
Epoch: 1240, Batch Gradient Norm: 36.504834912491916
Epoch: 1240, Batch Gradient Norm after: 22.360676873004664
Epoch 1241/10000, Prediction Accuracy = 47.772%, Loss = 1.4911252975463867
Epoch: 1241, Batch Gradient Norm: 39.46031898994474
Epoch: 1241, Batch Gradient Norm after: 22.360678242766028
Epoch 1242/10000, Prediction Accuracy = 47.785999999999994%, Loss = 1.4997991085052491
Epoch: 1242, Batch Gradient Norm: 36.53153292565199
Epoch: 1242, Batch Gradient Norm after: 22.360676109725215
Epoch 1243/10000, Prediction Accuracy = 47.774%, Loss = 1.4901521682739258
Epoch: 1243, Batch Gradient Norm: 39.495576883632545
Epoch: 1243, Batch Gradient Norm after: 22.36067864437828
Epoch 1244/10000, Prediction Accuracy = 47.794%, Loss = 1.4988537788391114
Epoch: 1244, Batch Gradient Norm: 36.55625637353396
Epoch: 1244, Batch Gradient Norm after: 22.36067583399422
Epoch 1245/10000, Prediction Accuracy = 47.782000000000004%, Loss = 1.4891733407974244
Epoch: 1245, Batch Gradient Norm: 39.521880169377546
Epoch: 1245, Batch Gradient Norm after: 22.36067948449284
Epoch 1246/10000, Prediction Accuracy = 47.796%, Loss = 1.497866702079773
Epoch: 1246, Batch Gradient Norm: 36.58126726984953
Epoch: 1246, Batch Gradient Norm after: 22.360673034462124
Epoch 1247/10000, Prediction Accuracy = 47.802%, Loss = 1.4881972312927245
Epoch: 1247, Batch Gradient Norm: 39.55404360136087
Epoch: 1247, Batch Gradient Norm after: 22.36067630128478
Epoch 1248/10000, Prediction Accuracy = 47.80200000000001%, Loss = 1.4969114780426025
Epoch: 1248, Batch Gradient Norm: 36.60203533257769
Epoch: 1248, Batch Gradient Norm after: 22.3606770163842
Epoch 1249/10000, Prediction Accuracy = 47.802%, Loss = 1.4872171878814697
Epoch: 1249, Batch Gradient Norm: 39.58492609186115
Epoch: 1249, Batch Gradient Norm after: 22.36067901340959
Epoch 1250/10000, Prediction Accuracy = 47.812%, Loss = 1.4959545850753784
Epoch: 1250, Batch Gradient Norm: 36.62324779507386
Epoch: 1250, Batch Gradient Norm after: 22.360677798822124
Epoch 1251/10000, Prediction Accuracy = 47.79600000000001%, Loss = 1.4862476110458374
Epoch: 1251, Batch Gradient Norm: 39.6164020750152
Epoch: 1251, Batch Gradient Norm after: 22.360680168935634
Epoch 1252/10000, Prediction Accuracy = 47.815999999999995%, Loss = 1.494994854927063
Epoch: 1252, Batch Gradient Norm: 36.645821186639544
Epoch: 1252, Batch Gradient Norm after: 22.360675516805607
Epoch 1253/10000, Prediction Accuracy = 47.798%, Loss = 1.4852651119232179
Epoch: 1253, Batch Gradient Norm: 39.64305673318558
Epoch: 1253, Batch Gradient Norm after: 22.360677495123408
Epoch 1254/10000, Prediction Accuracy = 47.821999999999996%, Loss = 1.4940358638763427
Epoch: 1254, Batch Gradient Norm: 36.666647895530666
Epoch: 1254, Batch Gradient Norm after: 22.36067634992323
Epoch 1255/10000, Prediction Accuracy = 47.796%, Loss = 1.4842988967895507
Epoch: 1255, Batch Gradient Norm: 39.67738160909187
Epoch: 1255, Batch Gradient Norm after: 22.36067514195234
Epoch 1256/10000, Prediction Accuracy = 47.82%, Loss = 1.4930920839309691
Epoch: 1256, Batch Gradient Norm: 36.69119088851286
Epoch: 1256, Batch Gradient Norm after: 22.36067640798444
Epoch 1257/10000, Prediction Accuracy = 47.802%, Loss = 1.4833292722702027
Epoch: 1257, Batch Gradient Norm: 39.6989341733927
Epoch: 1257, Batch Gradient Norm after: 22.36067773615591
Epoch 1258/10000, Prediction Accuracy = 47.83%, Loss = 1.4921241283416748
Epoch: 1258, Batch Gradient Norm: 36.711513640200685
Epoch: 1258, Batch Gradient Norm after: 22.36067545701852
Epoch 1259/10000, Prediction Accuracy = 47.804%, Loss = 1.4823635339736938
Epoch: 1259, Batch Gradient Norm: 39.72433961496116
Epoch: 1259, Batch Gradient Norm after: 22.360677777635342
Epoch 1260/10000, Prediction Accuracy = 47.83%, Loss = 1.491161012649536
Epoch: 1260, Batch Gradient Norm: 36.73499405189118
Epoch: 1260, Batch Gradient Norm after: 22.36067506706601
Epoch 1261/10000, Prediction Accuracy = 47.812%, Loss = 1.4813969135284424
Epoch: 1261, Batch Gradient Norm: 39.74469042841394
Epoch: 1261, Batch Gradient Norm after: 22.36067873329004
Epoch 1262/10000, Prediction Accuracy = 47.848%, Loss = 1.4901972532272338
Epoch: 1262, Batch Gradient Norm: 36.75752191835652
Epoch: 1262, Batch Gradient Norm after: 22.36067533479937
Epoch 1263/10000, Prediction Accuracy = 47.815999999999995%, Loss = 1.480433177947998
Epoch: 1263, Batch Gradient Norm: 39.75397914552862
Epoch: 1263, Batch Gradient Norm after: 22.360675591125283
Epoch 1264/10000, Prediction Accuracy = 47.85600000000001%, Loss = 1.4891966819763183
Epoch: 1264, Batch Gradient Norm: 36.776083689915026
Epoch: 1264, Batch Gradient Norm after: 22.36067593532808
Epoch 1265/10000, Prediction Accuracy = 47.827999999999996%, Loss = 1.4794663190841675
Epoch: 1265, Batch Gradient Norm: 39.768323502247746
Epoch: 1265, Batch Gradient Norm after: 22.36067637425432
Epoch 1266/10000, Prediction Accuracy = 47.858000000000004%, Loss = 1.4882000207901
Epoch: 1266, Batch Gradient Norm: 36.79784691285806
Epoch: 1266, Batch Gradient Norm after: 22.360676056200422
Epoch 1267/10000, Prediction Accuracy = 47.827999999999996%, Loss = 1.4785052061080932
Epoch: 1267, Batch Gradient Norm: 39.785438913149385
Epoch: 1267, Batch Gradient Norm after: 22.360676488019163
Epoch 1268/10000, Prediction Accuracy = 47.878%, Loss = 1.4872238636016846
Epoch: 1268, Batch Gradient Norm: 36.819349655259366
Epoch: 1268, Batch Gradient Norm after: 22.360677511539976
Epoch 1269/10000, Prediction Accuracy = 47.839999999999996%, Loss = 1.477559494972229
Epoch: 1269, Batch Gradient Norm: 39.80869092497419
Epoch: 1269, Batch Gradient Norm after: 22.36067542424794
Epoch 1270/10000, Prediction Accuracy = 47.88%, Loss = 1.4862634420394898
Epoch: 1270, Batch Gradient Norm: 36.84303131850605
Epoch: 1270, Batch Gradient Norm after: 22.36067659178013
Epoch 1271/10000, Prediction Accuracy = 47.858%, Loss = 1.4766048431396483
Epoch: 1271, Batch Gradient Norm: 39.83566493998532
Epoch: 1271, Batch Gradient Norm after: 22.360677022983182
Epoch 1272/10000, Prediction Accuracy = 47.884%, Loss = 1.4853281021118163
Epoch: 1272, Batch Gradient Norm: 36.867363400099414
Epoch: 1272, Batch Gradient Norm after: 22.360677459200122
Epoch 1273/10000, Prediction Accuracy = 47.87800000000001%, Loss = 1.4756580591201782
Epoch: 1273, Batch Gradient Norm: 39.87091582566298
Epoch: 1273, Batch Gradient Norm after: 22.3606756334613
Epoch 1274/10000, Prediction Accuracy = 47.896%, Loss = 1.484411859512329
Epoch: 1274, Batch Gradient Norm: 36.890379198160716
Epoch: 1274, Batch Gradient Norm after: 22.360677613593484
Epoch 1275/10000, Prediction Accuracy = 47.882%, Loss = 1.47472186088562
Epoch: 1275, Batch Gradient Norm: 39.90842186801369
Epoch: 1275, Batch Gradient Norm after: 22.360675852976623
Epoch 1276/10000, Prediction Accuracy = 47.897999999999996%, Loss = 1.48350670337677
Epoch: 1276, Batch Gradient Norm: 36.91410569761583
Epoch: 1276, Batch Gradient Norm after: 22.360674920427332
Epoch 1277/10000, Prediction Accuracy = 47.896%, Loss = 1.4737768173217773
Epoch: 1277, Batch Gradient Norm: 39.94993699583266
Epoch: 1277, Batch Gradient Norm after: 22.36067572080317
Epoch 1278/10000, Prediction Accuracy = 47.907999999999994%, Loss = 1.4826436519622803
Epoch: 1278, Batch Gradient Norm: 36.937969112930325
Epoch: 1278, Batch Gradient Norm after: 22.360676836410498
Epoch 1279/10000, Prediction Accuracy = 47.916000000000004%, Loss = 1.4728374242782594
Epoch: 1279, Batch Gradient Norm: 40.00581984048298
Epoch: 1279, Batch Gradient Norm after: 22.360676250542525
Epoch 1280/10000, Prediction Accuracy = 47.912%, Loss = 1.4818039417266846
Epoch: 1280, Batch Gradient Norm: 36.966561231759535
Epoch: 1280, Batch Gradient Norm after: 22.3606748481547
Epoch 1281/10000, Prediction Accuracy = 47.922%, Loss = 1.4719042778015137
Epoch: 1281, Batch Gradient Norm: 40.059201835557246
Epoch: 1281, Batch Gradient Norm after: 22.36067590502636
Epoch 1282/10000, Prediction Accuracy = 47.91799999999999%, Loss = 1.480969285964966
Epoch: 1282, Batch Gradient Norm: 36.99140756080858
Epoch: 1282, Batch Gradient Norm after: 22.360676468613637
Epoch 1283/10000, Prediction Accuracy = 47.934000000000005%, Loss = 1.4709704399108887
Epoch: 1283, Batch Gradient Norm: 40.111860535832356
Epoch: 1283, Batch Gradient Norm after: 22.36067931171927
Epoch 1284/10000, Prediction Accuracy = 47.916%, Loss = 1.4801238298416137
Epoch: 1284, Batch Gradient Norm: 37.015332937450275
Epoch: 1284, Batch Gradient Norm after: 22.360676794532967
Epoch 1285/10000, Prediction Accuracy = 47.946%, Loss = 1.4700410604476928
Epoch: 1285, Batch Gradient Norm: 40.15467367453211
Epoch: 1285, Batch Gradient Norm after: 22.360678087390784
Epoch 1286/10000, Prediction Accuracy = 47.922000000000004%, Loss = 1.479246234893799
Epoch: 1286, Batch Gradient Norm: 37.041522535735545
Epoch: 1286, Batch Gradient Norm after: 22.36067972910487
Epoch 1287/10000, Prediction Accuracy = 47.946000000000005%, Loss = 1.4691092014312743
Epoch: 1287, Batch Gradient Norm: 40.19732031985645
Epoch: 1287, Batch Gradient Norm after: 22.36067838054017
Epoch 1288/10000, Prediction Accuracy = 47.928%, Loss = 1.4783735513687133
Epoch: 1288, Batch Gradient Norm: 37.06761238310072
Epoch: 1288, Batch Gradient Norm after: 22.36067906872842
Epoch 1289/10000, Prediction Accuracy = 47.952%, Loss = 1.4681731462478638
Epoch: 1289, Batch Gradient Norm: 40.23284557945326
Epoch: 1289, Batch Gradient Norm after: 22.360676298446748
Epoch 1290/10000, Prediction Accuracy = 47.936%, Loss = 1.477467679977417
Epoch: 1290, Batch Gradient Norm: 37.09076059098377
Epoch: 1290, Batch Gradient Norm after: 22.36067719250305
Epoch 1291/10000, Prediction Accuracy = 47.974000000000004%, Loss = 1.4672433376312255
Epoch: 1291, Batch Gradient Norm: 40.26832596380181
Epoch: 1291, Batch Gradient Norm after: 22.36067672231442
Epoch 1292/10000, Prediction Accuracy = 47.940000000000005%, Loss = 1.4765740633010864
Epoch: 1292, Batch Gradient Norm: 37.114603764913625
Epoch: 1292, Batch Gradient Norm after: 22.360675096879163
Epoch 1293/10000, Prediction Accuracy = 47.986000000000004%, Loss = 1.466323184967041
Epoch: 1293, Batch Gradient Norm: 40.29854695582662
Epoch: 1293, Batch Gradient Norm after: 22.36067604537286
Epoch 1294/10000, Prediction Accuracy = 47.962%, Loss = 1.4756740570068358
Epoch: 1294, Batch Gradient Norm: 37.13721173299465
Epoch: 1294, Batch Gradient Norm after: 22.360677071677955
Epoch 1295/10000, Prediction Accuracy = 48.002%, Loss = 1.4654030084609986
Epoch: 1295, Batch Gradient Norm: 40.33426414161127
Epoch: 1295, Batch Gradient Norm after: 22.36067734799743
Epoch 1296/10000, Prediction Accuracy = 47.977999999999994%, Loss = 1.4747950315475464
Epoch: 1296, Batch Gradient Norm: 37.15844170314533
Epoch: 1296, Batch Gradient Norm after: 22.360677118723647
Epoch 1297/10000, Prediction Accuracy = 48.008%, Loss = 1.4644906282424928
Epoch: 1297, Batch Gradient Norm: 40.37280256398869
Epoch: 1297, Batch Gradient Norm after: 22.360675873259254
Epoch 1298/10000, Prediction Accuracy = 47.98%, Loss = 1.4739143133163453
Epoch: 1298, Batch Gradient Norm: 37.180763238731885
Epoch: 1298, Batch Gradient Norm after: 22.360678777851618
Epoch 1299/10000, Prediction Accuracy = 48.019999999999996%, Loss = 1.4635797977447509
Epoch: 1299, Batch Gradient Norm: 40.40498705469572
Epoch: 1299, Batch Gradient Norm after: 22.36067440905601
Epoch 1300/10000, Prediction Accuracy = 47.992%, Loss = 1.47303147315979
Epoch: 1300, Batch Gradient Norm: 37.204694213937955
Epoch: 1300, Batch Gradient Norm after: 22.360675936863547
Epoch 1301/10000, Prediction Accuracy = 48.025999999999996%, Loss = 1.4626665830612182
Epoch: 1301, Batch Gradient Norm: 40.44692529862227
Epoch: 1301, Batch Gradient Norm after: 22.36067750373078
Epoch 1302/10000, Prediction Accuracy = 48.0%, Loss = 1.4721848487854003
Epoch: 1302, Batch Gradient Norm: 37.23228198385384
Epoch: 1302, Batch Gradient Norm after: 22.360676545538293
Epoch 1303/10000, Prediction Accuracy = 48.028000000000006%, Loss = 1.461757779121399
Epoch: 1303, Batch Gradient Norm: 40.48467929569162
Epoch: 1303, Batch Gradient Norm after: 22.360675666609524
Epoch 1304/10000, Prediction Accuracy = 48.006%, Loss = 1.4713262557983398
Epoch: 1304, Batch Gradient Norm: 37.25636853838452
Epoch: 1304, Batch Gradient Norm after: 22.36067637398848
Epoch 1305/10000, Prediction Accuracy = 48.028000000000006%, Loss = 1.4608511924743652
Epoch: 1305, Batch Gradient Norm: 40.51948934614297
Epoch: 1305, Batch Gradient Norm after: 22.360677585223677
Epoch 1306/10000, Prediction Accuracy = 48.022%, Loss = 1.4704556941986084
Epoch: 1306, Batch Gradient Norm: 37.279859412504386
Epoch: 1306, Batch Gradient Norm after: 22.360677085066552
Epoch 1307/10000, Prediction Accuracy = 48.041999999999994%, Loss = 1.4599453210830688
Epoch: 1307, Batch Gradient Norm: 40.55646808223811
Epoch: 1307, Batch Gradient Norm after: 22.36067684141532
Epoch 1308/10000, Prediction Accuracy = 48.041999999999994%, Loss = 1.4695898056030274
Epoch: 1308, Batch Gradient Norm: 37.300947869311024
Epoch: 1308, Batch Gradient Norm after: 22.360676754307253
Epoch 1309/10000, Prediction Accuracy = 48.04%, Loss = 1.4590378284454346
Epoch: 1309, Batch Gradient Norm: 40.58723389242648
Epoch: 1309, Batch Gradient Norm after: 22.360677986676123
Epoch 1310/10000, Prediction Accuracy = 48.046%, Loss = 1.4687020778656006
Epoch: 1310, Batch Gradient Norm: 37.326073213298415
Epoch: 1310, Batch Gradient Norm after: 22.360675914164563
Epoch 1311/10000, Prediction Accuracy = 48.052%, Loss = 1.4581364393234253
Epoch: 1311, Batch Gradient Norm: 40.61058670760134
Epoch: 1311, Batch Gradient Norm after: 22.360676033757763
Epoch 1312/10000, Prediction Accuracy = 48.05400000000001%, Loss = 1.467798614501953
Epoch: 1312, Batch Gradient Norm: 37.34616577626916
Epoch: 1312, Batch Gradient Norm after: 22.360676904484038
Epoch 1313/10000, Prediction Accuracy = 48.056%, Loss = 1.457228994369507
Epoch: 1313, Batch Gradient Norm: 40.63121246303336
Epoch: 1313, Batch Gradient Norm after: 22.360676025137405
Epoch 1314/10000, Prediction Accuracy = 48.05800000000001%, Loss = 1.4668943166732789
Epoch: 1314, Batch Gradient Norm: 37.36870236944804
Epoch: 1314, Batch Gradient Norm after: 22.360677553384107
Epoch 1315/10000, Prediction Accuracy = 48.065999999999995%, Loss = 1.4563183546066285
Epoch: 1315, Batch Gradient Norm: 40.64375923041821
Epoch: 1315, Batch Gradient Norm after: 22.360676244033737
Epoch 1316/10000, Prediction Accuracy = 48.066%, Loss = 1.4659587860107421
Epoch: 1316, Batch Gradient Norm: 37.38694267014715
Epoch: 1316, Batch Gradient Norm after: 22.360675884850707
Epoch 1317/10000, Prediction Accuracy = 48.074%, Loss = 1.4554121971130372
Epoch: 1317, Batch Gradient Norm: 40.647652226717184
Epoch: 1317, Batch Gradient Norm after: 22.360677187486235
Epoch 1318/10000, Prediction Accuracy = 48.068000000000005%, Loss = 1.465008759498596
Epoch: 1318, Batch Gradient Norm: 37.40622488059929
Epoch: 1318, Batch Gradient Norm after: 22.360676689223585
Epoch 1319/10000, Prediction Accuracy = 48.08%, Loss = 1.4545095443725586
Epoch: 1319, Batch Gradient Norm: 40.657699768435435
Epoch: 1319, Batch Gradient Norm after: 22.360677334592157
Epoch 1320/10000, Prediction Accuracy = 48.088%, Loss = 1.464068841934204
Epoch: 1320, Batch Gradient Norm: 37.42460123046155
Epoch: 1320, Batch Gradient Norm after: 22.360677993263653
Epoch 1321/10000, Prediction Accuracy = 48.088%, Loss = 1.4536183595657348
Epoch: 1321, Batch Gradient Norm: 40.68164875423239
Epoch: 1321, Batch Gradient Norm after: 22.36067607754505
Epoch 1322/10000, Prediction Accuracy = 48.1%, Loss = 1.4631816387176513
Epoch: 1322, Batch Gradient Norm: 37.44422024778774
Epoch: 1322, Batch Gradient Norm after: 22.36067645292821
Epoch 1323/10000, Prediction Accuracy = 48.084%, Loss = 1.4527197122573852
Epoch: 1323, Batch Gradient Norm: 40.69959534350374
Epoch: 1323, Batch Gradient Norm after: 22.360678300655774
Epoch 1324/10000, Prediction Accuracy = 48.098%, Loss = 1.462278461456299
Epoch: 1324, Batch Gradient Norm: 37.46347178759928
Epoch: 1324, Batch Gradient Norm after: 22.360677088380285
Epoch 1325/10000, Prediction Accuracy = 48.096%, Loss = 1.4518272161483765
Epoch: 1325, Batch Gradient Norm: 40.7218608579551
Epoch: 1325, Batch Gradient Norm after: 22.36067683134783
Epoch 1326/10000, Prediction Accuracy = 48.1%, Loss = 1.4613983869552611
Epoch: 1326, Batch Gradient Norm: 37.48414712464941
Epoch: 1326, Batch Gradient Norm after: 22.360677730461042
Epoch 1327/10000, Prediction Accuracy = 48.106%, Loss = 1.450940442085266
Epoch: 1327, Batch Gradient Norm: 40.741877269891916
Epoch: 1327, Batch Gradient Norm after: 22.36067720376657
Epoch 1328/10000, Prediction Accuracy = 48.096%, Loss = 1.4605075597763062
Epoch: 1328, Batch Gradient Norm: 37.504458057789904
Epoch: 1328, Batch Gradient Norm after: 22.36067551276256
Epoch 1329/10000, Prediction Accuracy = 48.096%, Loss = 1.4500564336776733
Epoch: 1329, Batch Gradient Norm: 40.7549654035614
Epoch: 1329, Batch Gradient Norm after: 22.360678058176443
Epoch 1330/10000, Prediction Accuracy = 48.098%, Loss = 1.4596057891845704
Epoch: 1330, Batch Gradient Norm: 37.52121219809294
Epoch: 1330, Batch Gradient Norm after: 22.360675985555012
Epoch 1331/10000, Prediction Accuracy = 48.099999999999994%, Loss = 1.449166178703308
Epoch: 1331, Batch Gradient Norm: 40.75964296768001
Epoch: 1331, Batch Gradient Norm after: 22.36067807314242
Epoch 1332/10000, Prediction Accuracy = 48.102000000000004%, Loss = 1.4586734533309937
Epoch: 1332, Batch Gradient Norm: 37.54317210744467
Epoch: 1332, Batch Gradient Norm after: 22.360678493875767
Epoch 1333/10000, Prediction Accuracy = 48.126%, Loss = 1.448291039466858
Epoch: 1333, Batch Gradient Norm: 40.77532949490798
Epoch: 1333, Batch Gradient Norm after: 22.360676996670936
Epoch 1334/10000, Prediction Accuracy = 48.112%, Loss = 1.4577731370925904
Epoch: 1334, Batch Gradient Norm: 37.56136760704785
Epoch: 1334, Batch Gradient Norm after: 22.360676148251656
Epoch 1335/10000, Prediction Accuracy = 48.124%, Loss = 1.4474087476730346
Epoch: 1335, Batch Gradient Norm: 40.77524900927609
Epoch: 1335, Batch Gradient Norm after: 22.360677778663383
Epoch 1336/10000, Prediction Accuracy = 48.116%, Loss = 1.456838059425354
Epoch: 1336, Batch Gradient Norm: 37.5756761124883
Epoch: 1336, Batch Gradient Norm after: 22.36067701927566
Epoch 1337/10000, Prediction Accuracy = 48.126%, Loss = 1.446525502204895
Epoch: 1337, Batch Gradient Norm: 40.77212275752492
Epoch: 1337, Batch Gradient Norm after: 22.360677105905722
Epoch 1338/10000, Prediction Accuracy = 48.13%, Loss = 1.4559005498886108
Epoch: 1338, Batch Gradient Norm: 37.595431277376775
Epoch: 1338, Batch Gradient Norm after: 22.36067374555424
Epoch 1339/10000, Prediction Accuracy = 48.128%, Loss = 1.445642352104187
Epoch: 1339, Batch Gradient Norm: 40.7830671104364
Epoch: 1339, Batch Gradient Norm after: 22.36067853858732
Epoch 1340/10000, Prediction Accuracy = 48.13199999999999%, Loss = 1.4549827098846435
Epoch: 1340, Batch Gradient Norm: 37.61198938647459
Epoch: 1340, Batch Gradient Norm after: 22.36067577306939
Epoch 1341/10000, Prediction Accuracy = 48.13199999999999%, Loss = 1.4447668313980102
Epoch: 1341, Batch Gradient Norm: 40.79654048459788
Epoch: 1341, Batch Gradient Norm after: 22.360678533926773
Epoch 1342/10000, Prediction Accuracy = 48.132%, Loss = 1.4540897607803345
Epoch: 1342, Batch Gradient Norm: 37.63110915860269
Epoch: 1342, Batch Gradient Norm after: 22.360677681840045
Epoch 1343/10000, Prediction Accuracy = 48.128%, Loss = 1.4438966989517212
Epoch: 1343, Batch Gradient Norm: 40.81331043225896
Epoch: 1343, Batch Gradient Norm after: 22.36067884436875
Epoch 1344/10000, Prediction Accuracy = 48.138%, Loss = 1.4532066583633423
Epoch: 1344, Batch Gradient Norm: 37.64792609262739
Epoch: 1344, Batch Gradient Norm after: 22.36067445757984
Epoch 1345/10000, Prediction Accuracy = 48.126%, Loss = 1.4430221319198608
Epoch: 1345, Batch Gradient Norm: 40.82903630424903
Epoch: 1345, Batch Gradient Norm after: 22.36067997699095
Epoch 1346/10000, Prediction Accuracy = 48.14%, Loss = 1.4523281097412108
Epoch: 1346, Batch Gradient Norm: 37.66640524171449
Epoch: 1346, Batch Gradient Norm after: 22.36067575671851
Epoch 1347/10000, Prediction Accuracy = 48.136%, Loss = 1.4421587228775024
Epoch: 1347, Batch Gradient Norm: 40.84623545795734
Epoch: 1347, Batch Gradient Norm after: 22.360677954291013
Epoch 1348/10000, Prediction Accuracy = 48.14%, Loss = 1.4514637470245362
Epoch: 1348, Batch Gradient Norm: 37.68340579935477
Epoch: 1348, Batch Gradient Norm after: 22.360675998269286
Epoch 1349/10000, Prediction Accuracy = 48.129999999999995%, Loss = 1.4413015842437744
Epoch: 1349, Batch Gradient Norm: 40.881071575650715
Epoch: 1349, Batch Gradient Norm after: 22.36067894343042
Epoch 1350/10000, Prediction Accuracy = 48.146%, Loss = 1.450650143623352
Epoch: 1350, Batch Gradient Norm: 37.702306309238274
Epoch: 1350, Batch Gradient Norm after: 22.360676208285284
Epoch 1351/10000, Prediction Accuracy = 48.152%, Loss = 1.4404420137405396
Epoch: 1351, Batch Gradient Norm: 40.90601845489214
Epoch: 1351, Batch Gradient Norm after: 22.360680015061142
Epoch 1352/10000, Prediction Accuracy = 48.160000000000004%, Loss = 1.4498133659362793
Epoch: 1352, Batch Gradient Norm: 37.716056620811344
Epoch: 1352, Batch Gradient Norm after: 22.360676494585146
Epoch 1353/10000, Prediction Accuracy = 48.168%, Loss = 1.4395875692367555
Epoch: 1353, Batch Gradient Norm: 40.92800536959927
Epoch: 1353, Batch Gradient Norm after: 22.360679124730513
Epoch 1354/10000, Prediction Accuracy = 48.16799999999999%, Loss = 1.4489683866500855
Epoch: 1354, Batch Gradient Norm: 37.731689872414464
Epoch: 1354, Batch Gradient Norm after: 22.36067506535738
Epoch 1355/10000, Prediction Accuracy = 48.176%, Loss = 1.4387326955795288
Epoch: 1355, Batch Gradient Norm: 40.94625289932612
Epoch: 1355, Batch Gradient Norm after: 22.360676795496275
Epoch 1356/10000, Prediction Accuracy = 48.17%, Loss = 1.4481101274490356
Epoch: 1356, Batch Gradient Norm: 37.746473269572725
Epoch: 1356, Batch Gradient Norm after: 22.360676672691632
Epoch 1357/10000, Prediction Accuracy = 48.18599999999999%, Loss = 1.437880778312683
Epoch: 1357, Batch Gradient Norm: 40.96437649616745
Epoch: 1357, Batch Gradient Norm after: 22.360677492701093
Epoch 1358/10000, Prediction Accuracy = 48.178000000000004%, Loss = 1.4472341537475586
Epoch: 1358, Batch Gradient Norm: 37.76228339490453
Epoch: 1358, Batch Gradient Norm after: 22.360676888971437
Epoch 1359/10000, Prediction Accuracy = 48.182%, Loss = 1.4370201110839844
Epoch: 1359, Batch Gradient Norm: 40.97537740958004
Epoch: 1359, Batch Gradient Norm after: 22.360677720343286
Epoch 1360/10000, Prediction Accuracy = 48.184%, Loss = 1.4463570594787598
Epoch: 1360, Batch Gradient Norm: 37.780309259990304
Epoch: 1360, Batch Gradient Norm after: 22.360676681887313
Epoch 1361/10000, Prediction Accuracy = 48.186%, Loss = 1.4361708402633666
Epoch: 1361, Batch Gradient Norm: 40.99081628624554
Epoch: 1361, Batch Gradient Norm after: 22.36067603559747
Epoch 1362/10000, Prediction Accuracy = 48.182%, Loss = 1.4454971313476563
Epoch: 1362, Batch Gradient Norm: 37.798824392125965
Epoch: 1362, Batch Gradient Norm after: 22.360675456396116
Epoch 1363/10000, Prediction Accuracy = 48.192%, Loss = 1.4353208065032959
Epoch: 1363, Batch Gradient Norm: 41.010714931607886
Epoch: 1363, Batch Gradient Norm after: 22.360676352776032
Epoch 1364/10000, Prediction Accuracy = 48.187999999999995%, Loss = 1.444656991958618
Epoch: 1364, Batch Gradient Norm: 37.81538504663498
Epoch: 1364, Batch Gradient Norm after: 22.36067625124162
Epoch 1365/10000, Prediction Accuracy = 48.205999999999996%, Loss = 1.4344712018966674
Epoch: 1365, Batch Gradient Norm: 41.02961625105683
Epoch: 1365, Batch Gradient Norm after: 22.36067723632154
Epoch 1366/10000, Prediction Accuracy = 48.194%, Loss = 1.4438128232955934
Epoch: 1366, Batch Gradient Norm: 37.83410203962696
Epoch: 1366, Batch Gradient Norm after: 22.36067593448382
Epoch 1367/10000, Prediction Accuracy = 48.22800000000001%, Loss = 1.433621883392334
Epoch: 1367, Batch Gradient Norm: 41.05149364898625
Epoch: 1367, Batch Gradient Norm after: 22.36067445384524
Epoch 1368/10000, Prediction Accuracy = 48.212%, Loss = 1.4429817914962768
Epoch: 1368, Batch Gradient Norm: 37.85150040695217
Epoch: 1368, Batch Gradient Norm after: 22.360675354638886
Epoch 1369/10000, Prediction Accuracy = 48.230000000000004%, Loss = 1.4327808618545532
Epoch: 1369, Batch Gradient Norm: 41.0546458201858
Epoch: 1369, Batch Gradient Norm after: 22.360675480996235
Epoch 1370/10000, Prediction Accuracy = 48.212%, Loss = 1.4421032905578612
Epoch: 1370, Batch Gradient Norm: 37.86785061532671
Epoch: 1370, Batch Gradient Norm after: 22.360678134438928
Epoch 1371/10000, Prediction Accuracy = 48.244%, Loss = 1.431934905052185
Epoch: 1371, Batch Gradient Norm: 41.06227064411824
Epoch: 1371, Batch Gradient Norm after: 22.360677341305355
Epoch 1372/10000, Prediction Accuracy = 48.233999999999995%, Loss = 1.4412317514419555
Epoch: 1372, Batch Gradient Norm: 37.88223557118759
Epoch: 1372, Batch Gradient Norm after: 22.36067703579165
Epoch 1373/10000, Prediction Accuracy = 48.25%, Loss = 1.4310920476913451
Epoch: 1373, Batch Gradient Norm: 41.07201369016692
Epoch: 1373, Batch Gradient Norm after: 22.36067704688586
Epoch 1374/10000, Prediction Accuracy = 48.244%, Loss = 1.440366506576538
Epoch: 1374, Batch Gradient Norm: 37.89780310055647
Epoch: 1374, Batch Gradient Norm after: 22.360676918702698
Epoch 1375/10000, Prediction Accuracy = 48.248000000000005%, Loss = 1.430246925354004
Epoch: 1375, Batch Gradient Norm: 41.07416576428859
Epoch: 1375, Batch Gradient Norm after: 22.360679052999778
Epoch 1376/10000, Prediction Accuracy = 48.25000000000001%, Loss = 1.4394885301589966
Epoch: 1376, Batch Gradient Norm: 37.912150196709604
Epoch: 1376, Batch Gradient Norm after: 22.360677789594035
Epoch 1377/10000, Prediction Accuracy = 48.264%, Loss = 1.4294139385223388
Epoch: 1377, Batch Gradient Norm: 41.07703006589772
Epoch: 1377, Batch Gradient Norm after: 22.360679130231436
Epoch 1378/10000, Prediction Accuracy = 48.254%, Loss = 1.4386043071746826
Epoch: 1378, Batch Gradient Norm: 37.926022127542595
Epoch: 1378, Batch Gradient Norm after: 22.360676067229637
Epoch 1379/10000, Prediction Accuracy = 48.263999999999996%, Loss = 1.4285768270492554
Epoch: 1379, Batch Gradient Norm: 41.08238131560904
Epoch: 1379, Batch Gradient Norm after: 22.360678507101948
Epoch 1380/10000, Prediction Accuracy = 48.257999999999996%, Loss = 1.4377352714538574
Epoch: 1380, Batch Gradient Norm: 37.941520721908894
Epoch: 1380, Batch Gradient Norm after: 22.360677427860356
Epoch 1381/10000, Prediction Accuracy = 48.272000000000006%, Loss = 1.4277493953704834
Epoch: 1381, Batch Gradient Norm: 41.087028311576155
Epoch: 1381, Batch Gradient Norm after: 22.360675984094378
Epoch 1382/10000, Prediction Accuracy = 48.258%, Loss = 1.4368724346160888
Epoch: 1382, Batch Gradient Norm: 37.957718258300844
Epoch: 1382, Batch Gradient Norm after: 22.36067763882625
Epoch 1383/10000, Prediction Accuracy = 48.288%, Loss = 1.4269161939620971
Epoch: 1383, Batch Gradient Norm: 41.099775835857635
Epoch: 1383, Batch Gradient Norm after: 22.360678477364267
Epoch 1384/10000, Prediction Accuracy = 48.274%, Loss = 1.4360310077667235
Epoch: 1384, Batch Gradient Norm: 37.973158506833784
Epoch: 1384, Batch Gradient Norm after: 22.36067685963872
Epoch 1385/10000, Prediction Accuracy = 48.297999999999995%, Loss = 1.4260976314544678
Epoch: 1385, Batch Gradient Norm: 41.123052484818636
Epoch: 1385, Batch Gradient Norm after: 22.360677701213614
Epoch 1386/10000, Prediction Accuracy = 48.269999999999996%, Loss = 1.4352276802062989
Epoch: 1386, Batch Gradient Norm: 37.98650598193853
Epoch: 1386, Batch Gradient Norm after: 22.360676983266483
Epoch 1387/10000, Prediction Accuracy = 48.3%, Loss = 1.425269365310669
Epoch: 1387, Batch Gradient Norm: 41.14999827209031
Epoch: 1387, Batch Gradient Norm after: 22.360677715346483
Epoch 1388/10000, Prediction Accuracy = 48.284%, Loss = 1.4344513893127442
Epoch: 1388, Batch Gradient Norm: 37.99975133816349
Epoch: 1388, Batch Gradient Norm after: 22.360676422220887
Epoch 1389/10000, Prediction Accuracy = 48.308%, Loss = 1.424453568458557
Epoch: 1389, Batch Gradient Norm: 41.179060031982154
Epoch: 1389, Batch Gradient Norm after: 22.360679961870858
Epoch 1390/10000, Prediction Accuracy = 48.29%, Loss = 1.43367440700531
Epoch: 1390, Batch Gradient Norm: 38.01690844650927
Epoch: 1390, Batch Gradient Norm after: 22.360677070971473
Epoch 1391/10000, Prediction Accuracy = 48.30800000000001%, Loss = 1.4236394882202148
Epoch: 1391, Batch Gradient Norm: 41.20715172385499
Epoch: 1391, Batch Gradient Norm after: 22.36067527964428
Epoch 1392/10000, Prediction Accuracy = 48.303999999999995%, Loss = 1.4328956842422484
Epoch: 1392, Batch Gradient Norm: 38.032146466163894
Epoch: 1392, Batch Gradient Norm after: 22.36067977979086
Epoch 1393/10000, Prediction Accuracy = 48.314%, Loss = 1.4228280305862426
Epoch: 1393, Batch Gradient Norm: 41.23445628961447
Epoch: 1393, Batch Gradient Norm after: 22.360677579404392
Epoch 1394/10000, Prediction Accuracy = 48.31999999999999%, Loss = 1.4321247816085816
Epoch: 1394, Batch Gradient Norm: 38.04908372166022
Epoch: 1394, Batch Gradient Norm after: 22.360677261971748
Epoch 1395/10000, Prediction Accuracy = 48.32000000000001%, Loss = 1.422016716003418
Epoch: 1395, Batch Gradient Norm: 41.26536073293027
Epoch: 1395, Batch Gradient Norm after: 22.360678720067927
Epoch 1396/10000, Prediction Accuracy = 48.326%, Loss = 1.431358814239502
Epoch: 1396, Batch Gradient Norm: 38.06519464947514
Epoch: 1396, Batch Gradient Norm after: 22.36067749059299
Epoch 1397/10000, Prediction Accuracy = 48.328%, Loss = 1.421212387084961
Epoch: 1397, Batch Gradient Norm: 41.29703020037925
Epoch: 1397, Batch Gradient Norm after: 22.360679180122663
Epoch 1398/10000, Prediction Accuracy = 48.314%, Loss = 1.4305970430374146
Epoch: 1398, Batch Gradient Norm: 38.080314183004134
Epoch: 1398, Batch Gradient Norm after: 22.36067708736987
Epoch 1399/10000, Prediction Accuracy = 48.344%, Loss = 1.420401358604431
Epoch: 1399, Batch Gradient Norm: 41.3197122777811
Epoch: 1399, Batch Gradient Norm after: 22.360678759750368
Epoch 1400/10000, Prediction Accuracy = 48.324%, Loss = 1.429811143875122
Epoch: 1400, Batch Gradient Norm: 38.09635171153759
Epoch: 1400, Batch Gradient Norm after: 22.360676425893942
Epoch 1401/10000, Prediction Accuracy = 48.35%, Loss = 1.419601273536682
Epoch: 1401, Batch Gradient Norm: 41.345465269545976
Epoch: 1401, Batch Gradient Norm after: 22.360677006032503
Epoch 1402/10000, Prediction Accuracy = 48.326%, Loss = 1.429042673110962
Epoch: 1402, Batch Gradient Norm: 38.1115382247425
Epoch: 1402, Batch Gradient Norm after: 22.360677065586163
Epoch 1403/10000, Prediction Accuracy = 48.352%, Loss = 1.4187940120697022
Epoch: 1403, Batch Gradient Norm: 41.37350714757575
Epoch: 1403, Batch Gradient Norm after: 22.360679863784735
Epoch 1404/10000, Prediction Accuracy = 48.315999999999995%, Loss = 1.4282841205596923
Epoch: 1404, Batch Gradient Norm: 38.124871534343285
Epoch: 1404, Batch Gradient Norm after: 22.360678036928086
Epoch 1405/10000, Prediction Accuracy = 48.36%, Loss = 1.4179897069931031
Epoch: 1405, Batch Gradient Norm: 41.40607594469476
Epoch: 1405, Batch Gradient Norm after: 22.360677663056887
Epoch 1406/10000, Prediction Accuracy = 48.306%, Loss = 1.4275349378585815
Epoch: 1406, Batch Gradient Norm: 38.13998744797394
Epoch: 1406, Batch Gradient Norm after: 22.36067708573667
Epoch 1407/10000, Prediction Accuracy = 48.366%, Loss = 1.417188835144043
Epoch: 1407, Batch Gradient Norm: 41.446969302179674
Epoch: 1407, Batch Gradient Norm after: 22.360677961911477
Epoch 1408/10000, Prediction Accuracy = 48.30799999999999%, Loss = 1.426802396774292
Epoch: 1408, Batch Gradient Norm: 38.15416107905054
Epoch: 1408, Batch Gradient Norm after: 22.36067583959667
Epoch 1409/10000, Prediction Accuracy = 48.368%, Loss = 1.416391372680664
Epoch: 1409, Batch Gradient Norm: 41.489246827686024
Epoch: 1409, Batch Gradient Norm after: 22.360677963591886
Epoch 1410/10000, Prediction Accuracy = 48.322%, Loss = 1.4260815143585206
Epoch: 1410, Batch Gradient Norm: 38.17021829004636
Epoch: 1410, Batch Gradient Norm after: 22.360676784515547
Epoch 1411/10000, Prediction Accuracy = 48.373999999999995%, Loss = 1.4156014442443847
Epoch: 1411, Batch Gradient Norm: 41.5236726075803
Epoch: 1411, Batch Gradient Norm after: 22.360677232928122
Epoch 1412/10000, Prediction Accuracy = 48.339999999999996%, Loss = 1.4253398418426513
Epoch: 1412, Batch Gradient Norm: 38.18450725049322
Epoch: 1412, Batch Gradient Norm after: 22.36067509249687
Epoch 1413/10000, Prediction Accuracy = 48.378%, Loss = 1.4148032903671264
Epoch: 1413, Batch Gradient Norm: 41.54422595150714
Epoch: 1413, Batch Gradient Norm after: 22.36067515563387
Epoch 1414/10000, Prediction Accuracy = 48.338%, Loss = 1.4245589256286622
Epoch: 1414, Batch Gradient Norm: 38.19801080571653
Epoch: 1414, Batch Gradient Norm after: 22.360676877624016
Epoch 1415/10000, Prediction Accuracy = 48.384%, Loss = 1.4140000343322754
Epoch: 1415, Batch Gradient Norm: 41.5728352656102
Epoch: 1415, Batch Gradient Norm after: 22.36067737359845
Epoch 1416/10000, Prediction Accuracy = 48.339999999999996%, Loss = 1.4237934589385985
Epoch: 1416, Batch Gradient Norm: 38.21102942382521
Epoch: 1416, Batch Gradient Norm after: 22.360674822713193
Epoch 1417/10000, Prediction Accuracy = 48.388%, Loss = 1.413206124305725
Epoch: 1417, Batch Gradient Norm: 41.60485476428391
Epoch: 1417, Batch Gradient Norm after: 22.36067797926636
Epoch 1418/10000, Prediction Accuracy = 48.338%, Loss = 1.4230483770370483
Epoch: 1418, Batch Gradient Norm: 38.2264517980581
Epoch: 1418, Batch Gradient Norm after: 22.360675840282486
Epoch 1419/10000, Prediction Accuracy = 48.379999999999995%, Loss = 1.4124211311340331
Epoch: 1419, Batch Gradient Norm: 41.64400486161623
Epoch: 1419, Batch Gradient Norm after: 22.36067599682817
Epoch 1420/10000, Prediction Accuracy = 48.334%, Loss = 1.4223159313201905
Epoch: 1420, Batch Gradient Norm: 38.241755270770575
Epoch: 1420, Batch Gradient Norm after: 22.360674316047124
Epoch 1421/10000, Prediction Accuracy = 48.384%, Loss = 1.4116345405578614
Epoch: 1421, Batch Gradient Norm: 41.68103765544463
Epoch: 1421, Batch Gradient Norm after: 22.360677351294665
Epoch 1422/10000, Prediction Accuracy = 48.336%, Loss = 1.4215828895568847
Epoch: 1422, Batch Gradient Norm: 38.25812956812595
Epoch: 1422, Batch Gradient Norm after: 22.360677962577974
Epoch 1423/10000, Prediction Accuracy = 48.391999999999996%, Loss = 1.4108452081680298
Epoch: 1423, Batch Gradient Norm: 41.70934801050672
Epoch: 1423, Batch Gradient Norm after: 22.36067752075064
Epoch 1424/10000, Prediction Accuracy = 48.337999999999994%, Loss = 1.4208338499069213
Epoch: 1424, Batch Gradient Norm: 38.2753933055019
Epoch: 1424, Batch Gradient Norm after: 22.36067594046455
Epoch 1425/10000, Prediction Accuracy = 48.391999999999996%, Loss = 1.4100590944290161
Epoch: 1425, Batch Gradient Norm: 41.737588062642615
Epoch: 1425, Batch Gradient Norm after: 22.360678379244543
Epoch 1426/10000, Prediction Accuracy = 48.342%, Loss = 1.4201006174087525
Epoch: 1426, Batch Gradient Norm: 38.291622464445354
Epoch: 1426, Batch Gradient Norm after: 22.360678700496575
Epoch 1427/10000, Prediction Accuracy = 48.388%, Loss = 1.4092779159545898
Epoch: 1427, Batch Gradient Norm: 41.76967051350883
Epoch: 1427, Batch Gradient Norm after: 22.36067442912555
Epoch 1428/10000, Prediction Accuracy = 48.348%, Loss = 1.4193663835525512
Epoch: 1428, Batch Gradient Norm: 38.30717226191523
Epoch: 1428, Batch Gradient Norm after: 22.360677449183736
Epoch 1429/10000, Prediction Accuracy = 48.386%, Loss = 1.4084973573684691
Epoch: 1429, Batch Gradient Norm: 41.80473214997845
Epoch: 1429, Batch Gradient Norm after: 22.36067657098962
Epoch 1430/10000, Prediction Accuracy = 48.354%, Loss = 1.418654727935791
Epoch: 1430, Batch Gradient Norm: 38.319978223187704
Epoch: 1430, Batch Gradient Norm after: 22.36067764688346
Epoch 1431/10000, Prediction Accuracy = 48.382000000000005%, Loss = 1.407715654373169
Epoch: 1431, Batch Gradient Norm: 41.84410776114668
Epoch: 1431, Batch Gradient Norm after: 22.36067696358892
Epoch 1432/10000, Prediction Accuracy = 48.364%, Loss = 1.4179503917694092
Epoch: 1432, Batch Gradient Norm: 38.33870651993186
Epoch: 1432, Batch Gradient Norm after: 22.36067377175539
Epoch 1433/10000, Prediction Accuracy = 48.382%, Loss = 1.4069315910339355
Epoch: 1433, Batch Gradient Norm: 41.879820027353055
Epoch: 1433, Batch Gradient Norm after: 22.36067759283827
Epoch 1434/10000, Prediction Accuracy = 48.376%, Loss = 1.4172297954559325
Epoch: 1434, Batch Gradient Norm: 38.35483535168194
Epoch: 1434, Batch Gradient Norm after: 22.36067669565837
Epoch 1435/10000, Prediction Accuracy = 48.384%, Loss = 1.4061481714248658
Epoch: 1435, Batch Gradient Norm: 41.91170799873471
Epoch: 1435, Batch Gradient Norm after: 22.360676291402314
Epoch 1436/10000, Prediction Accuracy = 48.39%, Loss = 1.4165026903152467
Epoch: 1436, Batch Gradient Norm: 38.367495685473116
Epoch: 1436, Batch Gradient Norm after: 22.36067627334168
Epoch 1437/10000, Prediction Accuracy = 48.388%, Loss = 1.4053679704666138
Epoch: 1437, Batch Gradient Norm: 41.945094279471334
Epoch: 1437, Batch Gradient Norm after: 22.360677459846585
Epoch 1438/10000, Prediction Accuracy = 48.404%, Loss = 1.4157680034637452
Epoch: 1438, Batch Gradient Norm: 38.385770440774145
Epoch: 1438, Batch Gradient Norm after: 22.360675713239367
Epoch 1439/10000, Prediction Accuracy = 48.394%, Loss = 1.404592227935791
Epoch: 1439, Batch Gradient Norm: 41.97257913245476
Epoch: 1439, Batch Gradient Norm after: 22.36067502409748
Epoch 1440/10000, Prediction Accuracy = 48.4%, Loss = 1.4150282621383667
Epoch: 1440, Batch Gradient Norm: 38.401023039451125
Epoch: 1440, Batch Gradient Norm after: 22.360677438172036
Epoch 1441/10000, Prediction Accuracy = 48.406%, Loss = 1.40381760597229
Epoch: 1441, Batch Gradient Norm: 42.01236340539675
Epoch: 1441, Batch Gradient Norm after: 22.3606755507566
Epoch 1442/10000, Prediction Accuracy = 48.402%, Loss = 1.4143283128738404
Epoch: 1442, Batch Gradient Norm: 38.41598426301766
Epoch: 1442, Batch Gradient Norm after: 22.360678750653175
Epoch 1443/10000, Prediction Accuracy = 48.410000000000004%, Loss = 1.403044080734253
Epoch: 1443, Batch Gradient Norm: 42.03685882109869
Epoch: 1443, Batch Gradient Norm after: 22.36067627206887
Epoch 1444/10000, Prediction Accuracy = 48.416000000000004%, Loss = 1.4135904788970948
Epoch: 1444, Batch Gradient Norm: 38.429091100237315
Epoch: 1444, Batch Gradient Norm after: 22.360676694281995
Epoch 1445/10000, Prediction Accuracy = 48.41799999999999%, Loss = 1.402268123626709
Epoch: 1445, Batch Gradient Norm: 42.0667477849311
Epoch: 1445, Batch Gradient Norm after: 22.36067675923743
Epoch 1446/10000, Prediction Accuracy = 48.414%, Loss = 1.4128558874130248
Epoch: 1446, Batch Gradient Norm: 38.44274629719378
Epoch: 1446, Batch Gradient Norm after: 22.36067837579798
Epoch 1447/10000, Prediction Accuracy = 48.422000000000004%, Loss = 1.401491141319275
Epoch: 1447, Batch Gradient Norm: 42.08721199115152
Epoch: 1447, Batch Gradient Norm after: 22.360677746734183
Epoch 1448/10000, Prediction Accuracy = 48.41199999999999%, Loss = 1.4121062755584717
Epoch: 1448, Batch Gradient Norm: 38.45497937140229
Epoch: 1448, Batch Gradient Norm after: 22.360677573448314
Epoch 1449/10000, Prediction Accuracy = 48.432%, Loss = 1.4007062196731568
Epoch: 1449, Batch Gradient Norm: 42.08990551593325
Epoch: 1449, Batch Gradient Norm after: 22.36067835119971
Epoch 1450/10000, Prediction Accuracy = 48.416%, Loss = 1.4112855911254882
Epoch: 1450, Batch Gradient Norm: 38.46606208535483
Epoch: 1450, Batch Gradient Norm after: 22.360677509157043
Epoch 1451/10000, Prediction Accuracy = 48.44%, Loss = 1.3999169588088989
Epoch: 1451, Batch Gradient Norm: 42.08739098704525
Epoch: 1451, Batch Gradient Norm after: 22.360674657556046
Epoch 1452/10000, Prediction Accuracy = 48.422%, Loss = 1.4104463815689088
Epoch: 1452, Batch Gradient Norm: 38.47596413415379
Epoch: 1452, Batch Gradient Norm after: 22.360675976410064
Epoch 1453/10000, Prediction Accuracy = 48.45%, Loss = 1.39913489818573
Epoch: 1453, Batch Gradient Norm: 42.09234371571369
Epoch: 1453, Batch Gradient Norm after: 22.360677667120257
Epoch 1454/10000, Prediction Accuracy = 48.434000000000005%, Loss = 1.4096326589584351
Epoch: 1454, Batch Gradient Norm: 38.4873802964745
Epoch: 1454, Batch Gradient Norm after: 22.36067573316646
Epoch 1455/10000, Prediction Accuracy = 48.46%, Loss = 1.3983506917953492
Epoch: 1455, Batch Gradient Norm: 42.092435548260376
Epoch: 1455, Batch Gradient Norm after: 22.360675656099673
Epoch 1456/10000, Prediction Accuracy = 48.444%, Loss = 1.4088037729263305
Epoch: 1456, Batch Gradient Norm: 38.50154651695884
Epoch: 1456, Batch Gradient Norm after: 22.360677387507135
Epoch 1457/10000, Prediction Accuracy = 48.468%, Loss = 1.3975685596466065
Epoch: 1457, Batch Gradient Norm: 42.09099390512948
Epoch: 1457, Batch Gradient Norm after: 22.360677462680137
Epoch 1458/10000, Prediction Accuracy = 48.44199999999999%, Loss = 1.4079722166061401
Epoch: 1458, Batch Gradient Norm: 38.51129630426584
Epoch: 1458, Batch Gradient Norm after: 22.360677218771453
Epoch 1459/10000, Prediction Accuracy = 48.48800000000001%, Loss = 1.3967992782592773
Epoch: 1459, Batch Gradient Norm: 42.098317202945694
Epoch: 1459, Batch Gradient Norm after: 22.360679109062044
Epoch 1460/10000, Prediction Accuracy = 48.446%, Loss = 1.4071835041046143
Epoch: 1460, Batch Gradient Norm: 38.52284390165606
Epoch: 1460, Batch Gradient Norm after: 22.360676666358998
Epoch 1461/10000, Prediction Accuracy = 48.494%, Loss = 1.3960317611694335
Epoch: 1461, Batch Gradient Norm: 42.11116424590385
Epoch: 1461, Batch Gradient Norm after: 22.36067814655276
Epoch 1462/10000, Prediction Accuracy = 48.458%, Loss = 1.406415629386902
Epoch: 1462, Batch Gradient Norm: 38.535765093920936
Epoch: 1462, Batch Gradient Norm after: 22.36067739582245
Epoch 1463/10000, Prediction Accuracy = 48.494%, Loss = 1.3952682495117188
Epoch: 1463, Batch Gradient Norm: 42.12559418174454
Epoch: 1463, Batch Gradient Norm after: 22.36068019945632
Epoch 1464/10000, Prediction Accuracy = 48.464%, Loss = 1.4056668281555176
Epoch: 1464, Batch Gradient Norm: 38.54782323483286
Epoch: 1464, Batch Gradient Norm after: 22.360679370992685
Epoch 1465/10000, Prediction Accuracy = 48.50600000000001%, Loss = 1.3945013523101806
Epoch: 1465, Batch Gradient Norm: 42.14010335077275
Epoch: 1465, Batch Gradient Norm after: 22.360680105393662
Epoch 1466/10000, Prediction Accuracy = 48.468%, Loss = 1.4049229145050048
Epoch: 1466, Batch Gradient Norm: 38.56142989081507
Epoch: 1466, Batch Gradient Norm after: 22.36067652156638
Epoch 1467/10000, Prediction Accuracy = 48.51199999999999%, Loss = 1.3937387943267823
Epoch: 1467, Batch Gradient Norm: 42.15312234433704
Epoch: 1467, Batch Gradient Norm after: 22.360677777753214
Epoch 1468/10000, Prediction Accuracy = 48.48%, Loss = 1.4041584014892579
Epoch: 1468, Batch Gradient Norm: 38.57318011819787
Epoch: 1468, Batch Gradient Norm after: 22.360678199041
Epoch 1469/10000, Prediction Accuracy = 48.516%, Loss = 1.3929799556732179
Epoch: 1469, Batch Gradient Norm: 42.1703354043089
Epoch: 1469, Batch Gradient Norm after: 22.36067809174903
Epoch 1470/10000, Prediction Accuracy = 48.48800000000001%, Loss = 1.4034151792526246
Epoch: 1470, Batch Gradient Norm: 38.585388108415174
Epoch: 1470, Batch Gradient Norm after: 22.360679005212166
Epoch 1471/10000, Prediction Accuracy = 48.534%, Loss = 1.392225170135498
Epoch: 1471, Batch Gradient Norm: 42.18771900652307
Epoch: 1471, Batch Gradient Norm after: 22.360677247516318
Epoch 1472/10000, Prediction Accuracy = 48.482000000000006%, Loss = 1.4026653528213502
Epoch: 1472, Batch Gradient Norm: 38.59718189676059
Epoch: 1472, Batch Gradient Norm after: 22.36067803711817
Epoch 1473/10000, Prediction Accuracy = 48.536%, Loss = 1.391480565071106
Epoch: 1473, Batch Gradient Norm: 42.199194381658224
Epoch: 1473, Batch Gradient Norm after: 22.36067963133335
Epoch 1474/10000, Prediction Accuracy = 48.48199999999999%, Loss = 1.401906633377075
Epoch: 1474, Batch Gradient Norm: 38.60999079531748
Epoch: 1474, Batch Gradient Norm after: 22.360677875437492
Epoch 1475/10000, Prediction Accuracy = 48.54%, Loss = 1.3907232999801635
Epoch: 1475, Batch Gradient Norm: 42.21155929914724
Epoch: 1475, Batch Gradient Norm after: 22.360678570506284
Epoch 1476/10000, Prediction Accuracy = 48.477999999999994%, Loss = 1.4011409044265748
Epoch: 1476, Batch Gradient Norm: 38.62344556584362
Epoch: 1476, Batch Gradient Norm after: 22.360679448425557
Epoch 1477/10000, Prediction Accuracy = 48.552%, Loss = 1.3899751424789428
Epoch: 1477, Batch Gradient Norm: 42.231297844656936
Epoch: 1477, Batch Gradient Norm after: 22.360675842928284
Epoch 1478/10000, Prediction Accuracy = 48.488%, Loss = 1.4004072427749634
Epoch: 1478, Batch Gradient Norm: 38.63381865796725
Epoch: 1478, Batch Gradient Norm after: 22.36068004064365
Epoch 1479/10000, Prediction Accuracy = 48.55400000000001%, Loss = 1.3892333269119264
Epoch: 1479, Batch Gradient Norm: 42.249985578426305
Epoch: 1479, Batch Gradient Norm after: 22.360675897954522
Epoch 1480/10000, Prediction Accuracy = 48.484%, Loss = 1.399690532684326
Epoch: 1480, Batch Gradient Norm: 38.644906366581914
Epoch: 1480, Batch Gradient Norm after: 22.360679107708584
Epoch 1481/10000, Prediction Accuracy = 48.56%, Loss = 1.3884891510009765
Epoch: 1481, Batch Gradient Norm: 42.26956861391592
Epoch: 1481, Batch Gradient Norm after: 22.360676264881562
Epoch 1482/10000, Prediction Accuracy = 48.504%, Loss = 1.3989484786987305
Epoch: 1482, Batch Gradient Norm: 38.65746540026016
Epoch: 1482, Batch Gradient Norm after: 22.36067725381087
Epoch 1483/10000, Prediction Accuracy = 48.562%, Loss = 1.3877322912216186
Epoch: 1483, Batch Gradient Norm: 42.30089719367839
Epoch: 1483, Batch Gradient Norm after: 22.360678332000067
Epoch 1484/10000, Prediction Accuracy = 48.514%, Loss = 1.3982687711715698
Epoch: 1484, Batch Gradient Norm: 38.6689942725172
Epoch: 1484, Batch Gradient Norm after: 22.360676363330477
Epoch 1485/10000, Prediction Accuracy = 48.565999999999995%, Loss = 1.387003445625305
Epoch: 1485, Batch Gradient Norm: 42.33182656790098
Epoch: 1485, Batch Gradient Norm after: 22.36067779729005
Epoch 1486/10000, Prediction Accuracy = 48.518%, Loss = 1.3975733280181886
Epoch: 1486, Batch Gradient Norm: 38.68257677499492
Epoch: 1486, Batch Gradient Norm after: 22.360679220677408
Epoch 1487/10000, Prediction Accuracy = 48.58200000000001%, Loss = 1.3862619638442992
Epoch: 1487, Batch Gradient Norm: 42.35785135375216
Epoch: 1487, Batch Gradient Norm after: 22.360676484653112
Epoch 1488/10000, Prediction Accuracy = 48.53%, Loss = 1.3968690395355225
Epoch: 1488, Batch Gradient Norm: 38.691910576276825
Epoch: 1488, Batch Gradient Norm after: 22.36067878749875
Epoch 1489/10000, Prediction Accuracy = 48.61%, Loss = 1.3855103969573974
Epoch: 1489, Batch Gradient Norm: 42.37604858980189
Epoch: 1489, Batch Gradient Norm after: 22.360675373965794
Epoch 1490/10000, Prediction Accuracy = 48.534000000000006%, Loss = 1.3961436986923217
Epoch: 1490, Batch Gradient Norm: 38.70278183774903
Epoch: 1490, Batch Gradient Norm after: 22.36067752172061
Epoch 1491/10000, Prediction Accuracy = 48.616%, Loss = 1.3847766160964965
Epoch: 1491, Batch Gradient Norm: 42.40265998612082
Epoch: 1491, Batch Gradient Norm after: 22.36067732149135
Epoch 1492/10000, Prediction Accuracy = 48.546%, Loss = 1.3954426527023316
Epoch: 1492, Batch Gradient Norm: 38.71594420716027
Epoch: 1492, Batch Gradient Norm after: 22.360676779505695
Epoch 1493/10000, Prediction Accuracy = 48.628%, Loss = 1.3840441703796387
Epoch: 1493, Batch Gradient Norm: 42.4230914150589
Epoch: 1493, Batch Gradient Norm after: 22.360676018215194
Epoch 1494/10000, Prediction Accuracy = 48.564%, Loss = 1.3947478532791138
Epoch: 1494, Batch Gradient Norm: 38.72827830406553
Epoch: 1494, Batch Gradient Norm after: 22.360679409141937
Epoch 1495/10000, Prediction Accuracy = 48.636%, Loss = 1.383297324180603
Epoch: 1495, Batch Gradient Norm: 42.43838361708338
Epoch: 1495, Batch Gradient Norm after: 22.360677668834498
Epoch 1496/10000, Prediction Accuracy = 48.577999999999996%, Loss = 1.39401113986969
Epoch: 1496, Batch Gradient Norm: 38.73816191652823
Epoch: 1496, Batch Gradient Norm after: 22.360676775900952
Epoch 1497/10000, Prediction Accuracy = 48.652%, Loss = 1.382557463645935
Epoch: 1497, Batch Gradient Norm: 42.457608331218744
Epoch: 1497, Batch Gradient Norm after: 22.36067547089221
Epoch 1498/10000, Prediction Accuracy = 48.592%, Loss = 1.3933006048202514
Epoch: 1498, Batch Gradient Norm: 38.74956602689627
Epoch: 1498, Batch Gradient Norm after: 22.360676965730356
Epoch 1499/10000, Prediction Accuracy = 48.666000000000004%, Loss = 1.3818254470825195
Epoch: 1499, Batch Gradient Norm: 42.47276250319201
Epoch: 1499, Batch Gradient Norm after: 22.360675478992867
Epoch 1500/10000, Prediction Accuracy = 48.606%, Loss = 1.3925786018371582
Epoch: 1500, Batch Gradient Norm: 38.7591600247791
Epoch: 1500, Batch Gradient Norm after: 22.36067538777652
Epoch 1501/10000, Prediction Accuracy = 48.682%, Loss = 1.3810911893844604
Epoch: 1501, Batch Gradient Norm: 42.48820624547202
Epoch: 1501, Batch Gradient Norm after: 22.36067614528495
Epoch 1502/10000, Prediction Accuracy = 48.61%, Loss = 1.3918614149093629
Epoch: 1502, Batch Gradient Norm: 38.768929899240064
Epoch: 1502, Batch Gradient Norm after: 22.36067654716472
Epoch 1503/10000, Prediction Accuracy = 48.688%, Loss = 1.3803609132766723
Epoch: 1503, Batch Gradient Norm: 42.499809861506904
Epoch: 1503, Batch Gradient Norm after: 22.36067442784208
Epoch 1504/10000, Prediction Accuracy = 48.622%, Loss = 1.391120934486389
Epoch: 1504, Batch Gradient Norm: 38.77846957087424
Epoch: 1504, Batch Gradient Norm after: 22.360679101791447
Epoch 1505/10000, Prediction Accuracy = 48.699999999999996%, Loss = 1.379632544517517
Epoch: 1505, Batch Gradient Norm: 42.512163512306216
Epoch: 1505, Batch Gradient Norm after: 22.360676393764773
Epoch 1506/10000, Prediction Accuracy = 48.632%, Loss = 1.3903866291046143
Epoch: 1506, Batch Gradient Norm: 38.78772524567756
Epoch: 1506, Batch Gradient Norm after: 22.36067834990243
Epoch 1507/10000, Prediction Accuracy = 48.7%, Loss = 1.3789056301116944
Epoch: 1507, Batch Gradient Norm: 42.52248854423808
Epoch: 1507, Batch Gradient Norm after: 22.36067585015421
Epoch 1508/10000, Prediction Accuracy = 48.641999999999996%, Loss = 1.3896623134613038
Epoch: 1508, Batch Gradient Norm: 38.798986934292145
Epoch: 1508, Batch Gradient Norm after: 22.360677928756136
Epoch 1509/10000, Prediction Accuracy = 48.712%, Loss = 1.3781787872314453
Epoch: 1509, Batch Gradient Norm: 42.53541865561954
Epoch: 1509, Batch Gradient Norm after: 22.36067858294014
Epoch 1510/10000, Prediction Accuracy = 48.662%, Loss = 1.3889397859573365
Epoch: 1510, Batch Gradient Norm: 38.80898458487273
Epoch: 1510, Batch Gradient Norm after: 22.36067595894177
Epoch 1511/10000, Prediction Accuracy = 48.71%, Loss = 1.3774449825286865
Epoch: 1511, Batch Gradient Norm: 42.5417934684676
Epoch: 1511, Batch Gradient Norm after: 22.360676220860455
Epoch 1512/10000, Prediction Accuracy = 48.66600000000001%, Loss = 1.388203239440918
Epoch: 1512, Batch Gradient Norm: 38.81967082307137
Epoch: 1512, Batch Gradient Norm after: 22.36067690807252
Epoch 1513/10000, Prediction Accuracy = 48.72%, Loss = 1.3767225742340088
Epoch: 1513, Batch Gradient Norm: 42.55666672538466
Epoch: 1513, Batch Gradient Norm after: 22.36067635775071
Epoch 1514/10000, Prediction Accuracy = 48.672000000000004%, Loss = 1.3874918460845946
Epoch: 1514, Batch Gradient Norm: 38.829429236011826
Epoch: 1514, Batch Gradient Norm after: 22.360677051905828
Epoch 1515/10000, Prediction Accuracy = 48.732000000000006%, Loss = 1.3760019302368165
Epoch: 1515, Batch Gradient Norm: 42.578108750966386
Epoch: 1515, Batch Gradient Norm after: 22.36067945568363
Epoch 1516/10000, Prediction Accuracy = 48.67999999999999%, Loss = 1.3868069171905517
Epoch: 1516, Batch Gradient Norm: 38.84122807793938
Epoch: 1516, Batch Gradient Norm after: 22.36067659747828
Epoch 1517/10000, Prediction Accuracy = 48.74%, Loss = 1.3752805233001708
Epoch: 1517, Batch Gradient Norm: 42.58986262463553
Epoch: 1517, Batch Gradient Norm after: 22.36067454900498
Epoch 1518/10000, Prediction Accuracy = 48.696000000000005%, Loss = 1.3860920906066894
Epoch: 1518, Batch Gradient Norm: 38.85166419228867
Epoch: 1518, Batch Gradient Norm after: 22.360677181389025
Epoch 1519/10000, Prediction Accuracy = 48.757999999999996%, Loss = 1.3745590686798095
Epoch: 1519, Batch Gradient Norm: 42.6002243003792
Epoch: 1519, Batch Gradient Norm after: 22.360677295963924
Epoch 1520/10000, Prediction Accuracy = 48.702%, Loss = 1.3853750944137573
Epoch: 1520, Batch Gradient Norm: 38.860704655752805
Epoch: 1520, Batch Gradient Norm after: 22.360676959899795
Epoch 1521/10000, Prediction Accuracy = 48.775999999999996%, Loss = 1.373835849761963
Epoch: 1521, Batch Gradient Norm: 42.616032915658494
Epoch: 1521, Batch Gradient Norm after: 22.36067593973546
Epoch 1522/10000, Prediction Accuracy = 48.714%, Loss = 1.3846801042556762
Epoch: 1522, Batch Gradient Norm: 38.87215409042917
Epoch: 1522, Batch Gradient Norm after: 22.36067625540273
Epoch 1523/10000, Prediction Accuracy = 48.79600000000001%, Loss = 1.3731266736984253
Epoch: 1523, Batch Gradient Norm: 42.628812439847394
Epoch: 1523, Batch Gradient Norm after: 22.36067573563851
Epoch 1524/10000, Prediction Accuracy = 48.734%, Loss = 1.3839798450469971
Epoch: 1524, Batch Gradient Norm: 38.88205961584038
Epoch: 1524, Batch Gradient Norm after: 22.360675083124278
Epoch 1525/10000, Prediction Accuracy = 48.798%, Loss = 1.372413444519043
Epoch: 1525, Batch Gradient Norm: 42.64719264016134
Epoch: 1525, Batch Gradient Norm after: 22.360680748878135
Epoch 1526/10000, Prediction Accuracy = 48.745999999999995%, Loss = 1.3832899332046509
Epoch: 1526, Batch Gradient Norm: 38.89224388793653
Epoch: 1526, Batch Gradient Norm after: 22.360677848466338
Epoch 1527/10000, Prediction Accuracy = 48.81%, Loss = 1.3716954469680787
Epoch: 1527, Batch Gradient Norm: 42.668727430749975
Epoch: 1527, Batch Gradient Norm after: 22.360677749227797
Epoch 1528/10000, Prediction Accuracy = 48.762%, Loss = 1.3826119899749756
Epoch: 1528, Batch Gradient Norm: 38.90111513131699
Epoch: 1528, Batch Gradient Norm after: 22.360675726883084
Epoch 1529/10000, Prediction Accuracy = 48.812%, Loss = 1.3709922790527345
Epoch: 1529, Batch Gradient Norm: 42.69127419631693
Epoch: 1529, Batch Gradient Norm after: 22.360680081825137
Epoch 1530/10000, Prediction Accuracy = 48.768%, Loss = 1.3819417238235474
Epoch: 1530, Batch Gradient Norm: 38.910538873558544
Epoch: 1530, Batch Gradient Norm after: 22.360676523293968
Epoch 1531/10000, Prediction Accuracy = 48.81%, Loss = 1.3702874422073363
Epoch: 1531, Batch Gradient Norm: 42.71362936116988
Epoch: 1531, Batch Gradient Norm after: 22.36067963612247
Epoch 1532/10000, Prediction Accuracy = 48.784%, Loss = 1.381276750564575
Epoch: 1532, Batch Gradient Norm: 38.92024120892918
Epoch: 1532, Batch Gradient Norm after: 22.360675572387283
Epoch 1533/10000, Prediction Accuracy = 48.815999999999995%, Loss = 1.3695826292037965
Epoch: 1533, Batch Gradient Norm: 42.739911612948376
Epoch: 1533, Batch Gradient Norm after: 22.360679299758363
Epoch 1534/10000, Prediction Accuracy = 48.79%, Loss = 1.3806121826171875
Epoch: 1534, Batch Gradient Norm: 38.93516386442262
Epoch: 1534, Batch Gradient Norm after: 22.36067735032563
Epoch 1535/10000, Prediction Accuracy = 48.815999999999995%, Loss = 1.3688902616500855
Epoch: 1535, Batch Gradient Norm: 42.76467991059234
Epoch: 1535, Batch Gradient Norm after: 22.360677798212365
Epoch 1536/10000, Prediction Accuracy = 48.792%, Loss = 1.3799556255340577
Epoch: 1536, Batch Gradient Norm: 38.9483163079929
Epoch: 1536, Batch Gradient Norm after: 22.360676185659745
Epoch 1537/10000, Prediction Accuracy = 48.818%, Loss = 1.3681889772415161
Epoch: 1537, Batch Gradient Norm: 42.793882791445355
Epoch: 1537, Batch Gradient Norm after: 22.36067866472437
Epoch 1538/10000, Prediction Accuracy = 48.798%, Loss = 1.3793205499649048
Epoch: 1538, Batch Gradient Norm: 38.96129737685244
Epoch: 1538, Batch Gradient Norm after: 22.36067577848237
Epoch 1539/10000, Prediction Accuracy = 48.830000000000005%, Loss = 1.3675051927566528
Epoch: 1539, Batch Gradient Norm: 42.83686907020503
Epoch: 1539, Batch Gradient Norm after: 22.360678700397848
Epoch 1540/10000, Prediction Accuracy = 48.79600000000001%, Loss = 1.3787136554718018
Epoch: 1540, Batch Gradient Norm: 38.974993494137166
Epoch: 1540, Batch Gradient Norm after: 22.360676345736987
Epoch 1541/10000, Prediction Accuracy = 48.832%, Loss = 1.3668171167373657
Epoch: 1541, Batch Gradient Norm: 42.87369741559636
Epoch: 1541, Batch Gradient Norm after: 22.360677126295442
Epoch 1542/10000, Prediction Accuracy = 48.796%, Loss = 1.3780988931655884
Epoch: 1542, Batch Gradient Norm: 38.98482755134458
Epoch: 1542, Batch Gradient Norm after: 22.360675155733553
Epoch 1543/10000, Prediction Accuracy = 48.827999999999996%, Loss = 1.3661166906356812
Epoch: 1543, Batch Gradient Norm: 42.90230229669787
Epoch: 1543, Batch Gradient Norm after: 22.360677241915504
Epoch 1544/10000, Prediction Accuracy = 48.809999999999995%, Loss = 1.3774547100067138
Epoch: 1544, Batch Gradient Norm: 38.995639083727575
Epoch: 1544, Batch Gradient Norm after: 22.360677843954736
Epoch 1545/10000, Prediction Accuracy = 48.834%, Loss = 1.3654148578643799
Epoch: 1545, Batch Gradient Norm: 42.9201050563822
Epoch: 1545, Batch Gradient Norm after: 22.36067868720721
Epoch 1546/10000, Prediction Accuracy = 48.812%, Loss = 1.376781415939331
Epoch: 1546, Batch Gradient Norm: 39.00426216360851
Epoch: 1546, Batch Gradient Norm after: 22.360675816515478
Epoch 1547/10000, Prediction Accuracy = 48.836%, Loss = 1.364720058441162
Epoch: 1547, Batch Gradient Norm: 42.93598396873815
Epoch: 1547, Batch Gradient Norm after: 22.36067753980375
Epoch 1548/10000, Prediction Accuracy = 48.802%, Loss = 1.376115083694458
Epoch: 1548, Batch Gradient Norm: 39.01217370460297
Epoch: 1548, Batch Gradient Norm after: 22.360678197820587
Epoch 1549/10000, Prediction Accuracy = 48.866%, Loss = 1.3640334129333496
Epoch: 1549, Batch Gradient Norm: 42.946882793437325
Epoch: 1549, Batch Gradient Norm after: 22.360677456265247
Epoch 1550/10000, Prediction Accuracy = 48.816%, Loss = 1.3754445791244507
Epoch: 1550, Batch Gradient Norm: 39.02152965598692
Epoch: 1550, Batch Gradient Norm after: 22.360678321343432
Epoch 1551/10000, Prediction Accuracy = 48.862%, Loss = 1.3633427143096923
Epoch: 1551, Batch Gradient Norm: 42.971697507801025
Epoch: 1551, Batch Gradient Norm after: 22.360676545212478
Epoch 1552/10000, Prediction Accuracy = 48.824%, Loss = 1.3747947216033936
Epoch: 1552, Batch Gradient Norm: 39.03359068603183
Epoch: 1552, Batch Gradient Norm after: 22.360676989613232
Epoch 1553/10000, Prediction Accuracy = 48.866%, Loss = 1.3626527309417724
Epoch: 1553, Batch Gradient Norm: 42.997175364729884
Epoch: 1553, Batch Gradient Norm after: 22.360678254909796
Epoch 1554/10000, Prediction Accuracy = 48.836%, Loss = 1.3741519451141357
Epoch: 1554, Batch Gradient Norm: 39.04146631472791
Epoch: 1554, Batch Gradient Norm after: 22.36067914656279
Epoch 1555/10000, Prediction Accuracy = 48.868%, Loss = 1.3619611263275146
Epoch: 1555, Batch Gradient Norm: 43.02249737542387
Epoch: 1555, Batch Gradient Norm after: 22.36067658620773
Epoch 1556/10000, Prediction Accuracy = 48.854%, Loss = 1.3735056877136231
Epoch: 1556, Batch Gradient Norm: 39.051745669955714
Epoch: 1556, Batch Gradient Norm after: 22.360676820984775
Epoch 1557/10000, Prediction Accuracy = 48.872%, Loss = 1.3612744808197021
Epoch: 1557, Batch Gradient Norm: 43.055623252763986
Epoch: 1557, Batch Gradient Norm after: 22.360675382426592
Epoch 1558/10000, Prediction Accuracy = 48.856%, Loss = 1.3728907346725463
Epoch: 1558, Batch Gradient Norm: 39.06242112014078
Epoch: 1558, Batch Gradient Norm after: 22.360678933607964
Epoch 1559/10000, Prediction Accuracy = 48.88%, Loss = 1.3605904817581176
Epoch: 1559, Batch Gradient Norm: 43.08567596188815
Epoch: 1559, Batch Gradient Norm after: 22.360678601745544
Epoch 1560/10000, Prediction Accuracy = 48.86%, Loss = 1.3722726106643677
Epoch: 1560, Batch Gradient Norm: 39.07316684661995
Epoch: 1560, Batch Gradient Norm after: 22.360677513549263
Epoch 1561/10000, Prediction Accuracy = 48.882000000000005%, Loss = 1.3599054574966432
Epoch: 1561, Batch Gradient Norm: 43.12530139658637
Epoch: 1561, Batch Gradient Norm after: 22.36067800423651
Epoch 1562/10000, Prediction Accuracy = 48.866%, Loss = 1.3716729640960694
Epoch: 1562, Batch Gradient Norm: 39.08618990367283
Epoch: 1562, Batch Gradient Norm after: 22.36067744875489
Epoch 1563/10000, Prediction Accuracy = 48.906%, Loss = 1.3592288494110107
Epoch: 1563, Batch Gradient Norm: 43.175030395808
Epoch: 1563, Batch Gradient Norm after: 22.360675685769078
Epoch 1564/10000, Prediction Accuracy = 48.866%, Loss = 1.3711142539978027
Epoch: 1564, Batch Gradient Norm: 39.101740635187134
Epoch: 1564, Batch Gradient Norm after: 22.360676400531187
Epoch 1565/10000, Prediction Accuracy = 48.912%, Loss = 1.358553409576416
Epoch: 1565, Batch Gradient Norm: 43.21956553998778
Epoch: 1565, Batch Gradient Norm after: 22.360676725772755
Epoch 1566/10000, Prediction Accuracy = 48.870000000000005%, Loss = 1.3705336570739746
Epoch: 1566, Batch Gradient Norm: 39.11781434394321
Epoch: 1566, Batch Gradient Norm after: 22.3606774579873
Epoch 1567/10000, Prediction Accuracy = 48.916%, Loss = 1.3578757524490357
Epoch: 1567, Batch Gradient Norm: 43.26709160072014
Epoch: 1567, Batch Gradient Norm after: 22.360675688046445
Epoch 1568/10000, Prediction Accuracy = 48.872%, Loss = 1.369960403442383
Epoch: 1568, Batch Gradient Norm: 39.13143268489711
Epoch: 1568, Batch Gradient Norm after: 22.360676751586986
Epoch 1569/10000, Prediction Accuracy = 48.93000000000001%, Loss = 1.3572078943252563
Epoch: 1569, Batch Gradient Norm: 43.31496575684944
Epoch: 1569, Batch Gradient Norm after: 22.360677965576834
Epoch 1570/10000, Prediction Accuracy = 48.89200000000001%, Loss = 1.3693849802017213
Epoch: 1570, Batch Gradient Norm: 39.14704216173201
Epoch: 1570, Batch Gradient Norm after: 22.360676288773462
Epoch 1571/10000, Prediction Accuracy = 48.944%, Loss = 1.3565392732620238
Epoch: 1571, Batch Gradient Norm: 43.355791385618225
Epoch: 1571, Batch Gradient Norm after: 22.360675571021797
Epoch 1572/10000, Prediction Accuracy = 48.89399999999999%, Loss = 1.3687859535217286
Epoch: 1572, Batch Gradient Norm: 39.15865482784957
Epoch: 1572, Batch Gradient Norm after: 22.360676811221033
Epoch 1573/10000, Prediction Accuracy = 48.95%, Loss = 1.3558428287506104
Epoch: 1573, Batch Gradient Norm: 43.38580102001395
Epoch: 1573, Batch Gradient Norm after: 22.360677325423534
Epoch 1574/10000, Prediction Accuracy = 48.903999999999996%, Loss = 1.3681626558303832
Epoch: 1574, Batch Gradient Norm: 39.16978129991194
Epoch: 1574, Batch Gradient Norm after: 22.360676879800213
Epoch 1575/10000, Prediction Accuracy = 48.95%, Loss = 1.3551512241363526
Epoch: 1575, Batch Gradient Norm: 43.409029462558344
Epoch: 1575, Batch Gradient Norm after: 22.360675372564597
Epoch 1576/10000, Prediction Accuracy = 48.914%, Loss = 1.3675064086914062
Epoch: 1576, Batch Gradient Norm: 39.18138390939154
Epoch: 1576, Batch Gradient Norm after: 22.360678052487735
Epoch 1577/10000, Prediction Accuracy = 48.96%, Loss = 1.3544699907302857
Epoch: 1577, Batch Gradient Norm: 43.42779146847543
Epoch: 1577, Batch Gradient Norm after: 22.36067830420799
Epoch 1578/10000, Prediction Accuracy = 48.928%, Loss = 1.366849684715271
Epoch: 1578, Batch Gradient Norm: 39.188214889111386
Epoch: 1578, Batch Gradient Norm after: 22.36067770242631
Epoch 1579/10000, Prediction Accuracy = 48.977999999999994%, Loss = 1.3537784576416017
Epoch: 1579, Batch Gradient Norm: 43.439796633726466
Epoch: 1579, Batch Gradient Norm after: 22.36068010241949
Epoch 1580/10000, Prediction Accuracy = 48.938%, Loss = 1.3661704063415527
Epoch: 1580, Batch Gradient Norm: 39.19743471194906
Epoch: 1580, Batch Gradient Norm after: 22.360676946167
Epoch 1581/10000, Prediction Accuracy = 48.988%, Loss = 1.3530959129333495
Epoch: 1581, Batch Gradient Norm: 43.464131182386595
Epoch: 1581, Batch Gradient Norm after: 22.360677696124956
Epoch 1582/10000, Prediction Accuracy = 48.946000000000005%, Loss = 1.3655216932296752
Epoch: 1582, Batch Gradient Norm: 39.20598219516342
Epoch: 1582, Batch Gradient Norm after: 22.360676974222052
Epoch 1583/10000, Prediction Accuracy = 48.998000000000005%, Loss = 1.3524060726165772
Epoch: 1583, Batch Gradient Norm: 43.482457721673505
Epoch: 1583, Batch Gradient Norm after: 22.36067826975315
Epoch 1584/10000, Prediction Accuracy = 48.962%, Loss = 1.3648558855056763
Epoch: 1584, Batch Gradient Norm: 39.21918012136853
Epoch: 1584, Batch Gradient Norm after: 22.360676411282913
Epoch 1585/10000, Prediction Accuracy = 49.006%, Loss = 1.3517202138900757
Epoch: 1585, Batch Gradient Norm: 43.49563398413026
Epoch: 1585, Batch Gradient Norm after: 22.360675940500176
Epoch 1586/10000, Prediction Accuracy = 48.971999999999994%, Loss = 1.364190649986267
Epoch: 1586, Batch Gradient Norm: 39.22616682199019
Epoch: 1586, Batch Gradient Norm after: 22.360676316619035
Epoch 1587/10000, Prediction Accuracy = 49.012%, Loss = 1.351037073135376
Epoch: 1587, Batch Gradient Norm: 43.5040219744379
Epoch: 1587, Batch Gradient Norm after: 22.360676675965742
Epoch 1588/10000, Prediction Accuracy = 48.99%, Loss = 1.363524079322815
Epoch: 1588, Batch Gradient Norm: 39.234018275038586
Epoch: 1588, Batch Gradient Norm after: 22.360675957850482
Epoch 1589/10000, Prediction Accuracy = 49.028000000000006%, Loss = 1.350359559059143
Epoch: 1589, Batch Gradient Norm: 43.52521001738751
Epoch: 1589, Batch Gradient Norm after: 22.360677205654454
Epoch 1590/10000, Prediction Accuracy = 48.998000000000005%, Loss = 1.3628873586654664
Epoch: 1590, Batch Gradient Norm: 39.24297554494064
Epoch: 1590, Batch Gradient Norm after: 22.360674106637987
Epoch 1591/10000, Prediction Accuracy = 49.034000000000006%, Loss = 1.3496851921081543
Epoch: 1591, Batch Gradient Norm: 43.54492299446879
Epoch: 1591, Batch Gradient Norm after: 22.360677350362746
Epoch 1592/10000, Prediction Accuracy = 49.012%, Loss = 1.3622371435165406
Epoch: 1592, Batch Gradient Norm: 39.252474289377055
Epoch: 1592, Batch Gradient Norm after: 22.360676840068844
Epoch 1593/10000, Prediction Accuracy = 49.04600000000001%, Loss = 1.3490120887756347
Epoch: 1593, Batch Gradient Norm: 43.567188516690024
Epoch: 1593, Batch Gradient Norm after: 22.36067905868041
Epoch 1594/10000, Prediction Accuracy = 49.028000000000006%, Loss = 1.3616040229797364
Epoch: 1594, Batch Gradient Norm: 39.259578780449715
Epoch: 1594, Batch Gradient Norm after: 22.360674669407327
Epoch 1595/10000, Prediction Accuracy = 49.048%, Loss = 1.3483315229415893
Epoch: 1595, Batch Gradient Norm: 43.59057136153085
Epoch: 1595, Batch Gradient Norm after: 22.360676362079616
Epoch 1596/10000, Prediction Accuracy = 49.038000000000004%, Loss = 1.3609694004058839
Epoch: 1596, Batch Gradient Norm: 39.269682690568416
Epoch: 1596, Batch Gradient Norm after: 22.36067687927989
Epoch 1597/10000, Prediction Accuracy = 49.056000000000004%, Loss = 1.347661590576172
Epoch: 1597, Batch Gradient Norm: 43.60101297986754
Epoch: 1597, Batch Gradient Norm after: 22.360674787616823
Epoch 1598/10000, Prediction Accuracy = 49.04600000000001%, Loss = 1.3603034496307373
Epoch: 1598, Batch Gradient Norm: 39.28105949273912
Epoch: 1598, Batch Gradient Norm after: 22.360675516652655
Epoch 1599/10000, Prediction Accuracy = 49.056000000000004%, Loss = 1.3469846963882446
Epoch: 1599, Batch Gradient Norm: 43.61455760997895
Epoch: 1599, Batch Gradient Norm after: 22.360675605233016
Epoch 1600/10000, Prediction Accuracy = 49.052%, Loss = 1.3596507072448731
Epoch: 1600, Batch Gradient Norm: 39.29100430680889
Epoch: 1600, Batch Gradient Norm after: 22.360675482468803
Epoch 1601/10000, Prediction Accuracy = 49.059999999999995%, Loss = 1.3463192939758302
Epoch: 1601, Batch Gradient Norm: 43.63796598233146
Epoch: 1601, Batch Gradient Norm after: 22.360675760186123
Epoch 1602/10000, Prediction Accuracy = 49.05%, Loss = 1.3590148448944093
Epoch: 1602, Batch Gradient Norm: 39.301452506916455
Epoch: 1602, Batch Gradient Norm after: 22.360675278507568
Epoch 1603/10000, Prediction Accuracy = 49.06999999999999%, Loss = 1.3456603050231934
Epoch: 1603, Batch Gradient Norm: 43.665560675112275
Epoch: 1603, Batch Gradient Norm after: 22.360678359435337
Epoch 1604/10000, Prediction Accuracy = 49.059999999999995%, Loss = 1.3584145069122315
Epoch: 1604, Batch Gradient Norm: 39.31180245912713
Epoch: 1604, Batch Gradient Norm after: 22.360678004148173
Epoch 1605/10000, Prediction Accuracy = 49.084%, Loss = 1.345000648498535
Epoch: 1605, Batch Gradient Norm: 43.70129663335936
Epoch: 1605, Batch Gradient Norm after: 22.36067790383979
Epoch 1606/10000, Prediction Accuracy = 49.062000000000005%, Loss = 1.3578334331512452
Epoch: 1606, Batch Gradient Norm: 39.3244480888891
Epoch: 1606, Batch Gradient Norm after: 22.360677317068415
Epoch 1607/10000, Prediction Accuracy = 49.088%, Loss = 1.3443466663360595
Epoch: 1607, Batch Gradient Norm: 43.73960497141921
Epoch: 1607, Batch Gradient Norm after: 22.360677156880577
Epoch 1608/10000, Prediction Accuracy = 49.068%, Loss = 1.3572555780410767
Epoch: 1608, Batch Gradient Norm: 39.339019570887636
Epoch: 1608, Batch Gradient Norm after: 22.360676448913573
Epoch 1609/10000, Prediction Accuracy = 49.094%, Loss = 1.3436921834945679
Epoch: 1609, Batch Gradient Norm: 43.77400537250164
Epoch: 1609, Batch Gradient Norm after: 22.36067696647557
Epoch 1610/10000, Prediction Accuracy = 49.072%, Loss = 1.3566667795181275
Epoch: 1610, Batch Gradient Norm: 39.3523303291071
Epoch: 1610, Batch Gradient Norm after: 22.360676437185866
Epoch 1611/10000, Prediction Accuracy = 49.106%, Loss = 1.3430417776107788
Epoch: 1611, Batch Gradient Norm: 43.809005393173095
Epoch: 1611, Batch Gradient Norm after: 22.36067591064267
Epoch 1612/10000, Prediction Accuracy = 49.09599999999999%, Loss = 1.356087327003479
Epoch: 1612, Batch Gradient Norm: 39.36574450380736
Epoch: 1612, Batch Gradient Norm after: 22.360675339973834
Epoch 1613/10000, Prediction Accuracy = 49.112%, Loss = 1.342395257949829
Epoch: 1613, Batch Gradient Norm: 43.8475814391511
Epoch: 1613, Batch Gradient Norm after: 22.360677210998617
Epoch 1614/10000, Prediction Accuracy = 49.114%, Loss = 1.355507493019104
Epoch: 1614, Batch Gradient Norm: 39.378621142672685
Epoch: 1614, Batch Gradient Norm after: 22.36067721002959
Epoch 1615/10000, Prediction Accuracy = 49.114%, Loss = 1.341747808456421
Epoch: 1615, Batch Gradient Norm: 43.88087723283296
Epoch: 1615, Batch Gradient Norm after: 22.36067834540143
Epoch 1616/10000, Prediction Accuracy = 49.11%, Loss = 1.3549193143844604
Epoch: 1616, Batch Gradient Norm: 39.389861894257045
Epoch: 1616, Batch Gradient Norm after: 22.360673871352645
Epoch 1617/10000, Prediction Accuracy = 49.132%, Loss = 1.3410913228988648
Epoch: 1617, Batch Gradient Norm: 43.911743268496025
Epoch: 1617, Batch Gradient Norm after: 22.360678697861065
Epoch 1618/10000, Prediction Accuracy = 49.120000000000005%, Loss = 1.354327392578125
Epoch: 1618, Batch Gradient Norm: 39.402261749277
Epoch: 1618, Batch Gradient Norm after: 22.36067334620308
Epoch 1619/10000, Prediction Accuracy = 49.136%, Loss = 1.3404454231262206
Epoch: 1619, Batch Gradient Norm: 43.944298164664275
Epoch: 1619, Batch Gradient Norm after: 22.360677747418233
Epoch 1620/10000, Prediction Accuracy = 49.132000000000005%, Loss = 1.353742790222168
Epoch: 1620, Batch Gradient Norm: 39.411514468044956
Epoch: 1620, Batch Gradient Norm after: 22.360676448736303
Epoch 1621/10000, Prediction Accuracy = 49.144000000000005%, Loss = 1.3398023128509522
Epoch: 1621, Batch Gradient Norm: 43.989642875678605
Epoch: 1621, Batch Gradient Norm after: 22.36067723681923
Epoch 1622/10000, Prediction Accuracy = 49.128%, Loss = 1.3531959533691407
Epoch: 1622, Batch Gradient Norm: 39.42377663068351
Epoch: 1622, Batch Gradient Norm after: 22.36067627314236
Epoch 1623/10000, Prediction Accuracy = 49.144%, Loss = 1.33916277885437
Epoch: 1623, Batch Gradient Norm: 44.02813231938141
Epoch: 1623, Batch Gradient Norm after: 22.36067556385622
Epoch 1624/10000, Prediction Accuracy = 49.136%, Loss = 1.3526370763778686
Epoch: 1624, Batch Gradient Norm: 39.43543304056057
Epoch: 1624, Batch Gradient Norm after: 22.360674654955524
Epoch 1625/10000, Prediction Accuracy = 49.166%, Loss = 1.3385148286819457
Epoch: 1625, Batch Gradient Norm: 44.05497203524353
Epoch: 1625, Batch Gradient Norm after: 22.360676872814047
Epoch 1626/10000, Prediction Accuracy = 49.15%, Loss = 1.3520412683486938
Epoch: 1626, Batch Gradient Norm: 39.44777665486285
Epoch: 1626, Batch Gradient Norm after: 22.360675922896263
Epoch 1627/10000, Prediction Accuracy = 49.172000000000004%, Loss = 1.3378705501556396
Epoch: 1627, Batch Gradient Norm: 44.090357606903645
Epoch: 1627, Batch Gradient Norm after: 22.360676423383108
Epoch 1628/10000, Prediction Accuracy = 49.158%, Loss = 1.3514799118041991
Epoch: 1628, Batch Gradient Norm: 39.459647009633095
Epoch: 1628, Batch Gradient Norm after: 22.360675564168385
Epoch 1629/10000, Prediction Accuracy = 49.176%, Loss = 1.337229585647583
Epoch: 1629, Batch Gradient Norm: 44.123759068823404
Epoch: 1629, Batch Gradient Norm after: 22.360676840637705
Epoch 1630/10000, Prediction Accuracy = 49.164%, Loss = 1.3509071350097657
Epoch: 1630, Batch Gradient Norm: 39.47124263574206
Epoch: 1630, Batch Gradient Norm after: 22.360678295902787
Epoch 1631/10000, Prediction Accuracy = 49.186%, Loss = 1.3365870237350463
Epoch: 1631, Batch Gradient Norm: 44.160294981795744
Epoch: 1631, Batch Gradient Norm after: 22.360676811880385
Epoch 1632/10000, Prediction Accuracy = 49.178000000000004%, Loss = 1.350344467163086
Epoch: 1632, Batch Gradient Norm: 39.48114840385869
Epoch: 1632, Batch Gradient Norm after: 22.360677672326506
Epoch 1633/10000, Prediction Accuracy = 49.194%, Loss = 1.3359435319900512
Epoch: 1633, Batch Gradient Norm: 44.19282649492604
Epoch: 1633, Batch Gradient Norm after: 22.360676948513287
Epoch 1634/10000, Prediction Accuracy = 49.176%, Loss = 1.3497709274291991
Epoch: 1634, Batch Gradient Norm: 39.48962968861726
Epoch: 1634, Batch Gradient Norm after: 22.36067687271633
Epoch 1635/10000, Prediction Accuracy = 49.209999999999994%, Loss = 1.33529953956604
Epoch: 1635, Batch Gradient Norm: 44.21581943999158
Epoch: 1635, Batch Gradient Norm after: 22.360676059612803
Epoch 1636/10000, Prediction Accuracy = 49.19000000000001%, Loss = 1.3491641283035278
Epoch: 1636, Batch Gradient Norm: 39.49629363790323
Epoch: 1636, Batch Gradient Norm after: 22.360675483013537
Epoch 1637/10000, Prediction Accuracy = 49.216%, Loss = 1.3346497297286988
Epoch: 1637, Batch Gradient Norm: 44.23235134661922
Epoch: 1637, Batch Gradient Norm after: 22.360674912341818
Epoch 1638/10000, Prediction Accuracy = 49.21%, Loss = 1.3485365867614747
Epoch: 1638, Batch Gradient Norm: 39.50311818126316
Epoch: 1638, Batch Gradient Norm after: 22.360678714435625
Epoch 1639/10000, Prediction Accuracy = 49.224000000000004%, Loss = 1.333994460105896
Epoch: 1639, Batch Gradient Norm: 44.24519481979594
Epoch: 1639, Batch Gradient Norm after: 22.36067785835099
Epoch 1640/10000, Prediction Accuracy = 49.209999999999994%, Loss = 1.3479010581970214
Epoch: 1640, Batch Gradient Norm: 39.51084334935024
Epoch: 1640, Batch Gradient Norm after: 22.360676419105104
Epoch 1641/10000, Prediction Accuracy = 49.232%, Loss = 1.3333431243896485
Epoch: 1641, Batch Gradient Norm: 44.25747848721965
Epoch: 1641, Batch Gradient Norm after: 22.360678051006577
Epoch 1642/10000, Prediction Accuracy = 49.214000000000006%, Loss = 1.3472577810287476
Epoch: 1642, Batch Gradient Norm: 39.51719320660954
Epoch: 1642, Batch Gradient Norm after: 22.360675795049627
Epoch 1643/10000, Prediction Accuracy = 49.23199999999999%, Loss = 1.3326924324035645
Epoch: 1643, Batch Gradient Norm: 44.26134965092139
Epoch: 1643, Batch Gradient Norm after: 22.360678056471023
Epoch 1644/10000, Prediction Accuracy = 49.227999999999994%, Loss = 1.3465989828109741
Epoch: 1644, Batch Gradient Norm: 39.51963148563144
Epoch: 1644, Batch Gradient Norm after: 22.36067785807723
Epoch 1645/10000, Prediction Accuracy = 49.244%, Loss = 1.3320290565490722
Epoch: 1645, Batch Gradient Norm: 44.26207346239938
Epoch: 1645, Batch Gradient Norm after: 22.360678592413596
Epoch 1646/10000, Prediction Accuracy = 49.230000000000004%, Loss = 1.3459232568740844
Epoch: 1646, Batch Gradient Norm: 39.522119998307694
Epoch: 1646, Batch Gradient Norm after: 22.360673858216927
Epoch 1647/10000, Prediction Accuracy = 49.260000000000005%, Loss = 1.331372904777527
Epoch: 1647, Batch Gradient Norm: 44.2629991789981
Epoch: 1647, Batch Gradient Norm after: 22.360676129961192
Epoch 1648/10000, Prediction Accuracy = 49.233999999999995%, Loss = 1.3452488660812378
Epoch: 1648, Batch Gradient Norm: 39.52943626934701
Epoch: 1648, Batch Gradient Norm after: 22.36067642518746
Epoch 1649/10000, Prediction Accuracy = 49.272%, Loss = 1.3307327032089233
Epoch: 1649, Batch Gradient Norm: 44.26794323923397
Epoch: 1649, Batch Gradient Norm after: 22.36067614902403
Epoch 1650/10000, Prediction Accuracy = 49.236000000000004%, Loss = 1.3446089506149292
Epoch: 1650, Batch Gradient Norm: 39.53819900629388
Epoch: 1650, Batch Gradient Norm after: 22.360676098399516
Epoch 1651/10000, Prediction Accuracy = 49.274%, Loss = 1.330101776123047
Epoch: 1651, Batch Gradient Norm: 44.284397583245166
Epoch: 1651, Batch Gradient Norm after: 22.36067824359951
Epoch 1652/10000, Prediction Accuracy = 49.248000000000005%, Loss = 1.343995213508606
Epoch: 1652, Batch Gradient Norm: 39.54946838889235
Epoch: 1652, Batch Gradient Norm after: 22.360677310879748
Epoch 1653/10000, Prediction Accuracy = 49.272%, Loss = 1.3294682025909423
Epoch: 1653, Batch Gradient Norm: 44.30676094593749
Epoch: 1653, Batch Gradient Norm after: 22.360675351091853
Epoch 1654/10000, Prediction Accuracy = 49.267999999999994%, Loss = 1.3434021711349486
Epoch: 1654, Batch Gradient Norm: 39.55975967427795
Epoch: 1654, Batch Gradient Norm after: 22.360678011748934
Epoch 1655/10000, Prediction Accuracy = 49.269999999999996%, Loss = 1.3288406610488892
Epoch: 1655, Batch Gradient Norm: 44.341609908468556
Epoch: 1655, Batch Gradient Norm after: 22.360677856297478
Epoch 1656/10000, Prediction Accuracy = 49.279999999999994%, Loss = 1.3428449869155883
Epoch: 1656, Batch Gradient Norm: 39.571099436020695
Epoch: 1656, Batch Gradient Norm after: 22.36067481562403
Epoch 1657/10000, Prediction Accuracy = 49.273999999999994%, Loss = 1.3282126426696776
Epoch: 1657, Batch Gradient Norm: 44.35850070044407
Epoch: 1657, Batch Gradient Norm after: 22.360677340014973
Epoch 1658/10000, Prediction Accuracy = 49.29600000000001%, Loss = 1.342249059677124
Epoch: 1658, Batch Gradient Norm: 39.57776013435192
Epoch: 1658, Batch Gradient Norm after: 22.360677555629664
Epoch 1659/10000, Prediction Accuracy = 49.278%, Loss = 1.3275776863098145
Epoch: 1659, Batch Gradient Norm: 44.374146556054036
Epoch: 1659, Batch Gradient Norm after: 22.36067632611195
Epoch 1660/10000, Prediction Accuracy = 49.306000000000004%, Loss = 1.341640305519104
Epoch: 1660, Batch Gradient Norm: 39.58273611296854
Epoch: 1660, Batch Gradient Norm after: 22.36067405539358
Epoch 1661/10000, Prediction Accuracy = 49.28999999999999%, Loss = 1.3269446849823
Epoch: 1661, Batch Gradient Norm: 44.39085806184929
Epoch: 1661, Batch Gradient Norm after: 22.36067760277852
Epoch 1662/10000, Prediction Accuracy = 49.326%, Loss = 1.341026735305786
Epoch: 1662, Batch Gradient Norm: 39.58855476023607
Epoch: 1662, Batch Gradient Norm after: 22.36067539126854
Epoch 1663/10000, Prediction Accuracy = 49.306%, Loss = 1.3263081073760987
Epoch: 1663, Batch Gradient Norm: 44.3967272421455
Epoch: 1663, Batch Gradient Norm after: 22.36067554204339
Epoch 1664/10000, Prediction Accuracy = 49.332%, Loss = 1.340388035774231
Epoch: 1664, Batch Gradient Norm: 39.59393731413569
Epoch: 1664, Batch Gradient Norm after: 22.360674596395512
Epoch 1665/10000, Prediction Accuracy = 49.314%, Loss = 1.3256718873977662
Epoch: 1665, Batch Gradient Norm: 44.41334735392047
Epoch: 1665, Batch Gradient Norm after: 22.360677882170343
Epoch 1666/10000, Prediction Accuracy = 49.336%, Loss = 1.33977952003479
Epoch: 1666, Batch Gradient Norm: 39.602000732732286
Epoch: 1666, Batch Gradient Norm after: 22.360677494769032
Epoch 1667/10000, Prediction Accuracy = 49.32%, Loss = 1.3250394821166993
Epoch: 1667, Batch Gradient Norm: 44.43374799232969
Epoch: 1667, Batch Gradient Norm after: 22.360679151318266
Epoch 1668/10000, Prediction Accuracy = 49.34%, Loss = 1.3391910791397095
Epoch: 1668, Batch Gradient Norm: 39.61177732356985
Epoch: 1668, Batch Gradient Norm after: 22.36067825410231
Epoch 1669/10000, Prediction Accuracy = 49.336%, Loss = 1.3244117975234986
Epoch: 1669, Batch Gradient Norm: 44.46107877117462
Epoch: 1669, Batch Gradient Norm after: 22.360678268349364
Epoch 1670/10000, Prediction Accuracy = 49.346000000000004%, Loss = 1.3386415719985962
Epoch: 1670, Batch Gradient Norm: 39.62011280062337
Epoch: 1670, Batch Gradient Norm after: 22.360675879581844
Epoch 1671/10000, Prediction Accuracy = 49.338%, Loss = 1.3237853050231934
Epoch: 1671, Batch Gradient Norm: 44.49386274548287
Epoch: 1671, Batch Gradient Norm after: 22.36067588274905
Epoch 1672/10000, Prediction Accuracy = 49.358000000000004%, Loss = 1.3380891323089599
Epoch: 1672, Batch Gradient Norm: 39.62934518104505
Epoch: 1672, Batch Gradient Norm after: 22.36067638656067
Epoch 1673/10000, Prediction Accuracy = 49.344%, Loss = 1.323164963722229
Epoch: 1673, Batch Gradient Norm: 44.53065070519921
Epoch: 1673, Batch Gradient Norm after: 22.36067679678404
Epoch 1674/10000, Prediction Accuracy = 49.352%, Loss = 1.3375590085983275
Epoch: 1674, Batch Gradient Norm: 39.644067063780874
Epoch: 1674, Batch Gradient Norm after: 22.360675947617654
Epoch 1675/10000, Prediction Accuracy = 49.34400000000001%, Loss = 1.322550320625305
Epoch: 1675, Batch Gradient Norm: 44.570328302808875
Epoch: 1675, Batch Gradient Norm after: 22.360676867764845
Epoch 1676/10000, Prediction Accuracy = 49.36%, Loss = 1.3370396614074707
Epoch: 1676, Batch Gradient Norm: 39.65907511043256
Epoch: 1676, Batch Gradient Norm after: 22.36067351643027
Epoch 1677/10000, Prediction Accuracy = 49.35%, Loss = 1.321947979927063
Epoch: 1677, Batch Gradient Norm: 44.61160397341009
Epoch: 1677, Batch Gradient Norm after: 22.360678774599513
Epoch 1678/10000, Prediction Accuracy = 49.366%, Loss = 1.336521363258362
Epoch: 1678, Batch Gradient Norm: 39.67634704211105
Epoch: 1678, Batch Gradient Norm after: 22.36067817060274
Epoch 1679/10000, Prediction Accuracy = 49.352%, Loss = 1.3213517189025878
Epoch: 1679, Batch Gradient Norm: 44.65769353045297
Epoch: 1679, Batch Gradient Norm after: 22.36067603615469
Epoch 1680/10000, Prediction Accuracy = 49.372%, Loss = 1.3360047578811645
Epoch: 1680, Batch Gradient Norm: 39.692543927126906
Epoch: 1680, Batch Gradient Norm after: 22.360677894849356
Epoch 1681/10000, Prediction Accuracy = 49.35600000000001%, Loss = 1.3207600593566895
Epoch: 1681, Batch Gradient Norm: 44.714928396210894
Epoch: 1681, Batch Gradient Norm after: 22.36067495496484
Epoch 1682/10000, Prediction Accuracy = 49.376%, Loss = 1.335534644126892
Epoch: 1682, Batch Gradient Norm: 39.71063946737829
Epoch: 1682, Batch Gradient Norm after: 22.360672815210794
Epoch 1683/10000, Prediction Accuracy = 49.364%, Loss = 1.320165205001831
Epoch: 1683, Batch Gradient Norm: 44.78168667396379
Epoch: 1683, Batch Gradient Norm after: 22.36067719952964
Epoch 1684/10000, Prediction Accuracy = 49.382000000000005%, Loss = 1.335087823867798
Epoch: 1684, Batch Gradient Norm: 39.72808625655209
Epoch: 1684, Batch Gradient Norm after: 22.36067516752982
Epoch 1685/10000, Prediction Accuracy = 49.376%, Loss = 1.3195736169815064
Epoch: 1685, Batch Gradient Norm: 44.83850274155799
Epoch: 1685, Batch Gradient Norm after: 22.360675187475135
Epoch 1686/10000, Prediction Accuracy = 49.391999999999996%, Loss = 1.334609031677246
Epoch: 1686, Batch Gradient Norm: 39.743102432177395
Epoch: 1686, Batch Gradient Norm after: 22.360674778186084
Epoch 1687/10000, Prediction Accuracy = 49.38%, Loss = 1.3189796209335327
Epoch: 1687, Batch Gradient Norm: 44.88898362545321
Epoch: 1687, Batch Gradient Norm after: 22.36067667260475
Epoch 1688/10000, Prediction Accuracy = 49.394000000000005%, Loss = 1.334126091003418
Epoch: 1688, Batch Gradient Norm: 39.763354673581034
Epoch: 1688, Batch Gradient Norm after: 22.360676542634824
Epoch 1689/10000, Prediction Accuracy = 49.402%, Loss = 1.3183809757232665
Epoch: 1689, Batch Gradient Norm: 44.93573227284546
Epoch: 1689, Batch Gradient Norm after: 22.360676819694472
Epoch 1690/10000, Prediction Accuracy = 49.394000000000005%, Loss = 1.3336274385452271
Epoch: 1690, Batch Gradient Norm: 39.77779247955527
Epoch: 1690, Batch Gradient Norm after: 22.36067647182113
Epoch 1691/10000, Prediction Accuracy = 49.410000000000004%, Loss = 1.3177740812301635
Epoch: 1691, Batch Gradient Norm: 44.97506987642509
Epoch: 1691, Batch Gradient Norm after: 22.36067802771109
Epoch 1692/10000, Prediction Accuracy = 49.404%, Loss = 1.3330998182296754
Epoch: 1692, Batch Gradient Norm: 39.79335352870809
Epoch: 1692, Batch Gradient Norm after: 22.36067642613554
Epoch 1693/10000, Prediction Accuracy = 49.426%, Loss = 1.31715886592865
Epoch: 1693, Batch Gradient Norm: 45.002369849318676
Epoch: 1693, Batch Gradient Norm after: 22.36067932351331
Epoch 1694/10000, Prediction Accuracy = 49.418%, Loss = 1.3325380086898804
Epoch: 1694, Batch Gradient Norm: 39.807281802640766
Epoch: 1694, Batch Gradient Norm after: 22.360675098737506
Epoch 1695/10000, Prediction Accuracy = 49.44199999999999%, Loss = 1.3165579080581664
Epoch: 1695, Batch Gradient Norm: 45.0272456853506
Epoch: 1695, Batch Gradient Norm after: 22.360680258636407
Epoch 1696/10000, Prediction Accuracy = 49.432%, Loss = 1.3319488763809204
Epoch: 1696, Batch Gradient Norm: 39.81658018781595
Epoch: 1696, Batch Gradient Norm after: 22.360675484162783
Epoch 1697/10000, Prediction Accuracy = 49.454%, Loss = 1.3159310817718506
Epoch: 1697, Batch Gradient Norm: 45.047885491757015
Epoch: 1697, Batch Gradient Norm after: 22.36067642163954
Epoch 1698/10000, Prediction Accuracy = 49.440000000000005%, Loss = 1.3313533067703247
Epoch: 1698, Batch Gradient Norm: 39.824599755684325
Epoch: 1698, Batch Gradient Norm after: 22.360676766867446
Epoch 1699/10000, Prediction Accuracy = 49.458000000000006%, Loss = 1.3152982234954833
Epoch: 1699, Batch Gradient Norm: 45.05635629336858
Epoch: 1699, Batch Gradient Norm after: 22.36067594053627
Epoch 1700/10000, Prediction Accuracy = 49.444%, Loss = 1.3307272434234618
Epoch: 1700, Batch Gradient Norm: 39.831734125042054
Epoch: 1700, Batch Gradient Norm after: 22.360673785340744
Epoch 1701/10000, Prediction Accuracy = 49.458000000000006%, Loss = 1.3146742343902589
Epoch: 1701, Batch Gradient Norm: 45.07469415567133
Epoch: 1701, Batch Gradient Norm after: 22.360675861328282
Epoch 1702/10000, Prediction Accuracy = 49.454%, Loss = 1.330131459236145
Epoch: 1702, Batch Gradient Norm: 39.83865607880557
Epoch: 1702, Batch Gradient Norm after: 22.36067554764486
Epoch 1703/10000, Prediction Accuracy = 49.44799999999999%, Loss = 1.3140578985214233
Epoch: 1703, Batch Gradient Norm: 45.09289183788913
Epoch: 1703, Batch Gradient Norm after: 22.36067733558016
Epoch 1704/10000, Prediction Accuracy = 49.458%, Loss = 1.329542326927185
Epoch: 1704, Batch Gradient Norm: 39.849542222718945
Epoch: 1704, Batch Gradient Norm after: 22.360677868854065
Epoch 1705/10000, Prediction Accuracy = 49.45399999999999%, Loss = 1.313452458381653
Epoch: 1705, Batch Gradient Norm: 45.118186485407854
Epoch: 1705, Batch Gradient Norm after: 22.360676321202973
Epoch 1706/10000, Prediction Accuracy = 49.462%, Loss = 1.328974199295044
Epoch: 1706, Batch Gradient Norm: 39.85998877275518
Epoch: 1706, Batch Gradient Norm after: 22.360677498244016
Epoch 1707/10000, Prediction Accuracy = 49.458000000000006%, Loss = 1.3128518342971802
Epoch: 1707, Batch Gradient Norm: 45.14299105853585
Epoch: 1707, Batch Gradient Norm after: 22.360675917258472
Epoch 1708/10000, Prediction Accuracy = 49.474000000000004%, Loss = 1.328416395187378
Epoch: 1708, Batch Gradient Norm: 39.87064962063395
Epoch: 1708, Batch Gradient Norm after: 22.360676462247085
Epoch 1709/10000, Prediction Accuracy = 49.47%, Loss = 1.3122351884841919
Epoch: 1709, Batch Gradient Norm: 45.161199429487425
Epoch: 1709, Batch Gradient Norm after: 22.36067677257559
Epoch 1710/10000, Prediction Accuracy = 49.488%, Loss = 1.3278218507766724
Epoch: 1710, Batch Gradient Norm: 39.88274063321304
Epoch: 1710, Batch Gradient Norm after: 22.36067744223643
Epoch 1711/10000, Prediction Accuracy = 49.488%, Loss = 1.3116308927536011
Epoch: 1711, Batch Gradient Norm: 45.1847281272116
Epoch: 1711, Batch Gradient Norm after: 22.360678082368956
Epoch 1712/10000, Prediction Accuracy = 49.517999999999994%, Loss = 1.3272525310516357
Epoch: 1712, Batch Gradient Norm: 39.89453608496765
Epoch: 1712, Batch Gradient Norm after: 22.360676799720345
Epoch 1713/10000, Prediction Accuracy = 49.498000000000005%, Loss = 1.3110165357589723
Epoch: 1713, Batch Gradient Norm: 45.196539675911865
Epoch: 1713, Batch Gradient Norm after: 22.360676108200654
Epoch 1714/10000, Prediction Accuracy = 49.525999999999996%, Loss = 1.326642394065857
Epoch: 1714, Batch Gradient Norm: 39.89931131230996
Epoch: 1714, Batch Gradient Norm after: 22.360676482504118
Epoch 1715/10000, Prediction Accuracy = 49.50599999999999%, Loss = 1.3103946208953858
Epoch: 1715, Batch Gradient Norm: 45.20004840680051
Epoch: 1715, Batch Gradient Norm after: 22.36067867313922
Epoch 1716/10000, Prediction Accuracy = 49.528%, Loss = 1.326019287109375
Epoch: 1716, Batch Gradient Norm: 39.90418200572712
Epoch: 1716, Batch Gradient Norm after: 22.36067409730992
Epoch 1717/10000, Prediction Accuracy = 49.512%, Loss = 1.3097697734832763
Epoch: 1717, Batch Gradient Norm: 45.200760359454335
Epoch: 1717, Batch Gradient Norm after: 22.36067552289589
Epoch 1718/10000, Prediction Accuracy = 49.523999999999994%, Loss = 1.3253936767578125
Epoch: 1718, Batch Gradient Norm: 39.90565282175555
Epoch: 1718, Batch Gradient Norm after: 22.36067605854511
Epoch 1719/10000, Prediction Accuracy = 49.532%, Loss = 1.309151530265808
Epoch: 1719, Batch Gradient Norm: 45.19473469936654
Epoch: 1719, Batch Gradient Norm after: 22.360675505596163
Epoch 1720/10000, Prediction Accuracy = 49.538%, Loss = 1.3247287273406982
Epoch: 1720, Batch Gradient Norm: 39.9076966778661
Epoch: 1720, Batch Gradient Norm after: 22.360676311253506
Epoch 1721/10000, Prediction Accuracy = 49.540000000000006%, Loss = 1.308526873588562
Epoch: 1721, Batch Gradient Norm: 45.19406537307894
Epoch: 1721, Batch Gradient Norm after: 22.360675190579936
Epoch 1722/10000, Prediction Accuracy = 49.55%, Loss = 1.3240975141525269
Epoch: 1722, Batch Gradient Norm: 39.9081016946159
Epoch: 1722, Batch Gradient Norm after: 22.360675834400393
Epoch 1723/10000, Prediction Accuracy = 49.546%, Loss = 1.3078962087631225
Epoch: 1723, Batch Gradient Norm: 45.17754687402472
Epoch: 1723, Batch Gradient Norm after: 22.360675412689663
Epoch 1724/10000, Prediction Accuracy = 49.55200000000001%, Loss = 1.3234233856201172
Epoch: 1724, Batch Gradient Norm: 39.908471163898426
Epoch: 1724, Batch Gradient Norm after: 22.360677304893557
Epoch 1725/10000, Prediction Accuracy = 49.554%, Loss = 1.3072696924209595
Epoch: 1725, Batch Gradient Norm: 45.164775298710616
Epoch: 1725, Batch Gradient Norm after: 22.36067707394226
Epoch 1726/10000, Prediction Accuracy = 49.566%, Loss = 1.3227566719055175
Epoch: 1726, Batch Gradient Norm: 39.91031720333373
Epoch: 1726, Batch Gradient Norm after: 22.36067668866892
Epoch 1727/10000, Prediction Accuracy = 49.572%, Loss = 1.3066501140594482
Epoch: 1727, Batch Gradient Norm: 45.16262610219942
Epoch: 1727, Batch Gradient Norm after: 22.360676495733248
Epoch 1728/10000, Prediction Accuracy = 49.58%, Loss = 1.3221116065979004
Epoch: 1728, Batch Gradient Norm: 39.91459749518295
Epoch: 1728, Batch Gradient Norm after: 22.36067826851474
Epoch 1729/10000, Prediction Accuracy = 49.578%, Loss = 1.3060369729995727
Epoch: 1729, Batch Gradient Norm: 45.162842659066044
Epoch: 1729, Batch Gradient Norm after: 22.360678147044563
Epoch 1730/10000, Prediction Accuracy = 49.588%, Loss = 1.3214916229248046
Epoch: 1730, Batch Gradient Norm: 39.91862172957259
Epoch: 1730, Batch Gradient Norm after: 22.36067741186702
Epoch 1731/10000, Prediction Accuracy = 49.586%, Loss = 1.3054226636886597
Epoch: 1731, Batch Gradient Norm: 45.175315668244615
Epoch: 1731, Batch Gradient Norm after: 22.360676052115156
Epoch 1732/10000, Prediction Accuracy = 49.598%, Loss = 1.3209064722061157
Epoch: 1732, Batch Gradient Norm: 39.92737222248949
Epoch: 1732, Batch Gradient Norm after: 22.360676261876154
Epoch 1733/10000, Prediction Accuracy = 49.596000000000004%, Loss = 1.3048267602920531
Epoch: 1733, Batch Gradient Norm: 45.19274456818858
Epoch: 1733, Batch Gradient Norm after: 22.36067788326205
Epoch 1734/10000, Prediction Accuracy = 49.602%, Loss = 1.3203343152999878
Epoch: 1734, Batch Gradient Norm: 39.93624855779229
Epoch: 1734, Batch Gradient Norm after: 22.36067430495702
Epoch 1735/10000, Prediction Accuracy = 49.6%, Loss = 1.304234004020691
Epoch: 1735, Batch Gradient Norm: 45.2118270636705
Epoch: 1735, Batch Gradient Norm after: 22.360675917102835
Epoch 1736/10000, Prediction Accuracy = 49.602%, Loss = 1.3197670221328734
Epoch: 1736, Batch Gradient Norm: 39.94394057152764
Epoch: 1736, Batch Gradient Norm after: 22.360674576778088
Epoch 1737/10000, Prediction Accuracy = 49.61%, Loss = 1.3036378383636475
Epoch: 1737, Batch Gradient Norm: 45.23256852882914
Epoch: 1737, Batch Gradient Norm after: 22.360678900238007
Epoch 1738/10000, Prediction Accuracy = 49.620000000000005%, Loss = 1.3192224264144898
Epoch: 1738, Batch Gradient Norm: 39.95288947663802
Epoch: 1738, Batch Gradient Norm after: 22.360675146419332
Epoch 1739/10000, Prediction Accuracy = 49.628%, Loss = 1.30305073261261
Epoch: 1739, Batch Gradient Norm: 45.25725911185042
Epoch: 1739, Batch Gradient Norm after: 22.36067533696724
Epoch 1740/10000, Prediction Accuracy = 49.632%, Loss = 1.3186748027801514
Epoch: 1740, Batch Gradient Norm: 39.960140942645616
Epoch: 1740, Batch Gradient Norm after: 22.360676226814615
Epoch 1741/10000, Prediction Accuracy = 49.628%, Loss = 1.302460503578186
Epoch: 1741, Batch Gradient Norm: 45.281243787286385
Epoch: 1741, Batch Gradient Norm after: 22.360676695356442
Epoch 1742/10000, Prediction Accuracy = 49.64%, Loss = 1.3181296586990356
Epoch: 1742, Batch Gradient Norm: 39.9669039009411
Epoch: 1742, Batch Gradient Norm after: 22.3606787820956
Epoch 1743/10000, Prediction Accuracy = 49.634%, Loss = 1.3018683671951294
Epoch: 1743, Batch Gradient Norm: 45.30451150155067
Epoch: 1743, Batch Gradient Norm after: 22.36067610959426
Epoch 1744/10000, Prediction Accuracy = 49.644%, Loss = 1.3175754785537719
Epoch: 1744, Batch Gradient Norm: 39.977695385651444
Epoch: 1744, Batch Gradient Norm after: 22.36067492263791
Epoch 1745/10000, Prediction Accuracy = 49.64999999999999%, Loss = 1.3012834548950196
Epoch: 1745, Batch Gradient Norm: 45.33454990779494
Epoch: 1745, Batch Gradient Norm after: 22.36067553308523
Epoch 1746/10000, Prediction Accuracy = 49.662%, Loss = 1.3170575857162476
Epoch: 1746, Batch Gradient Norm: 39.986523201174975
Epoch: 1746, Batch Gradient Norm after: 22.360676963413344
Epoch 1747/10000, Prediction Accuracy = 49.656%, Loss = 1.3007091522216796
Epoch: 1747, Batch Gradient Norm: 45.36802958146986
Epoch: 1747, Batch Gradient Norm after: 22.360677539346444
Epoch 1748/10000, Prediction Accuracy = 49.660000000000004%, Loss = 1.3165529489517211
Epoch: 1748, Batch Gradient Norm: 39.99536158715006
Epoch: 1748, Batch Gradient Norm after: 22.36067796725182
Epoch 1749/10000, Prediction Accuracy = 49.666%, Loss = 1.300126552581787
Epoch: 1749, Batch Gradient Norm: 45.396242885435846
Epoch: 1749, Batch Gradient Norm after: 22.360675878460643
Epoch 1750/10000, Prediction Accuracy = 49.68%, Loss = 1.3160269737243653
Epoch: 1750, Batch Gradient Norm: 40.005756969741824
Epoch: 1750, Batch Gradient Norm after: 22.360676272440397
Epoch 1751/10000, Prediction Accuracy = 49.672%, Loss = 1.2995460271835326
Epoch: 1751, Batch Gradient Norm: 45.42789987252289
Epoch: 1751, Batch Gradient Norm after: 22.360677092155544
Epoch 1752/10000, Prediction Accuracy = 49.69199999999999%, Loss = 1.315517544746399
Epoch: 1752, Batch Gradient Norm: 40.01772330731862
Epoch: 1752, Batch Gradient Norm after: 22.36067613176041
Epoch 1753/10000, Prediction Accuracy = 49.672%, Loss = 1.298972225189209
Epoch: 1753, Batch Gradient Norm: 45.46572428702605
Epoch: 1753, Batch Gradient Norm after: 22.360676248331742
Epoch 1754/10000, Prediction Accuracy = 49.696%, Loss = 1.3150235891342164
Epoch: 1754, Batch Gradient Norm: 40.02577489932543
Epoch: 1754, Batch Gradient Norm after: 22.36067443168792
Epoch 1755/10000, Prediction Accuracy = 49.682%, Loss = 1.2983869314193726
Epoch: 1755, Batch Gradient Norm: 45.494673901771584
Epoch: 1755, Batch Gradient Norm after: 22.360675789570124
Epoch 1756/10000, Prediction Accuracy = 49.70399999999999%, Loss = 1.3145092010498047
Epoch: 1756, Batch Gradient Norm: 40.0382629625992
Epoch: 1756, Batch Gradient Norm after: 22.360677011274497
Epoch 1757/10000, Prediction Accuracy = 49.682%, Loss = 1.2978091716766358
Epoch: 1757, Batch Gradient Norm: 45.52657329992592
Epoch: 1757, Batch Gradient Norm after: 22.360678798568422
Epoch 1758/10000, Prediction Accuracy = 49.71%, Loss = 1.3139944314956664
Epoch: 1758, Batch Gradient Norm: 40.04783440721436
Epoch: 1758, Batch Gradient Norm after: 22.360677297913377
Epoch 1759/10000, Prediction Accuracy = 49.688%, Loss = 1.297224473953247
Epoch: 1759, Batch Gradient Norm: 45.56360839062207
Epoch: 1759, Batch Gradient Norm after: 22.360678581107617
Epoch 1760/10000, Prediction Accuracy = 49.708%, Loss = 1.313491129875183
Epoch: 1760, Batch Gradient Norm: 40.058600338821634
Epoch: 1760, Batch Gradient Norm after: 22.360677850108694
Epoch 1761/10000, Prediction Accuracy = 49.702%, Loss = 1.2966496467590332
Epoch: 1761, Batch Gradient Norm: 45.59951397435825
Epoch: 1761, Batch Gradient Norm after: 22.3606779751973
Epoch 1762/10000, Prediction Accuracy = 49.709999999999994%, Loss = 1.3129825830459594
Epoch: 1762, Batch Gradient Norm: 40.070073906008545
Epoch: 1762, Batch Gradient Norm after: 22.36067749247402
Epoch 1763/10000, Prediction Accuracy = 49.718%, Loss = 1.296065092086792
Epoch: 1763, Batch Gradient Norm: 45.63464186375289
Epoch: 1763, Batch Gradient Norm after: 22.36067633364376
Epoch 1764/10000, Prediction Accuracy = 49.730000000000004%, Loss = 1.3124958515167235
Epoch: 1764, Batch Gradient Norm: 40.08378047512926
Epoch: 1764, Batch Gradient Norm after: 22.360677582087344
Epoch 1765/10000, Prediction Accuracy = 49.718%, Loss = 1.295483613014221
Epoch: 1765, Batch Gradient Norm: 45.65629214148458
Epoch: 1765, Batch Gradient Norm after: 22.360676945844048
Epoch 1766/10000, Prediction Accuracy = 49.74%, Loss = 1.3119448184967042
Epoch: 1766, Batch Gradient Norm: 40.09399960464662
Epoch: 1766, Batch Gradient Norm after: 22.360678134490737
Epoch 1767/10000, Prediction Accuracy = 49.726000000000006%, Loss = 1.2949018716812133
Epoch: 1767, Batch Gradient Norm: 45.68813854559284
Epoch: 1767, Batch Gradient Norm after: 22.360677291330756
Epoch 1768/10000, Prediction Accuracy = 49.75%, Loss = 1.3114202499389649
Epoch: 1768, Batch Gradient Norm: 40.105142014413886
Epoch: 1768, Batch Gradient Norm after: 22.360677703659757
Epoch 1769/10000, Prediction Accuracy = 49.730000000000004%, Loss = 1.2943269729614257
Epoch: 1769, Batch Gradient Norm: 45.72356631681208
Epoch: 1769, Batch Gradient Norm after: 22.36067879471092
Epoch 1770/10000, Prediction Accuracy = 49.757999999999996%, Loss = 1.310915231704712
Epoch: 1770, Batch Gradient Norm: 40.11702814826571
Epoch: 1770, Batch Gradient Norm after: 22.360678469349835
Epoch 1771/10000, Prediction Accuracy = 49.732%, Loss = 1.293760633468628
Epoch: 1771, Batch Gradient Norm: 45.75089639905178
Epoch: 1771, Batch Gradient Norm after: 22.36067778078055
Epoch 1772/10000, Prediction Accuracy = 49.766000000000005%, Loss = 1.3103884935379029
Epoch: 1772, Batch Gradient Norm: 40.127996556921886
Epoch: 1772, Batch Gradient Norm after: 22.360676694314904
Epoch 1773/10000, Prediction Accuracy = 49.738%, Loss = 1.2931831359863282
Epoch: 1773, Batch Gradient Norm: 45.79267772605892
Epoch: 1773, Batch Gradient Norm after: 22.36067688211863
Epoch 1774/10000, Prediction Accuracy = 49.775999999999996%, Loss = 1.3098986625671387
Epoch: 1774, Batch Gradient Norm: 40.14335114966166
Epoch: 1774, Batch Gradient Norm after: 22.360675474635112
Epoch 1775/10000, Prediction Accuracy = 49.751999999999995%, Loss = 1.292629075050354
Epoch: 1775, Batch Gradient Norm: 45.831013537123496
Epoch: 1775, Batch Gradient Norm after: 22.360674814710933
Epoch 1776/10000, Prediction Accuracy = 49.782%, Loss = 1.3093933582305908
Epoch: 1776, Batch Gradient Norm: 40.15825064136813
Epoch: 1776, Batch Gradient Norm after: 22.360676551632146
Epoch 1777/10000, Prediction Accuracy = 49.76199999999999%, Loss = 1.2920649528503418
Epoch: 1777, Batch Gradient Norm: 45.86815432041619
Epoch: 1777, Batch Gradient Norm after: 22.360678799298967
Epoch 1778/10000, Prediction Accuracy = 49.794%, Loss = 1.3089051961898803
Epoch: 1778, Batch Gradient Norm: 40.174773110058375
Epoch: 1778, Batch Gradient Norm after: 22.36067963376129
Epoch 1779/10000, Prediction Accuracy = 49.769999999999996%, Loss = 1.2915028572082519
Epoch: 1779, Batch Gradient Norm: 45.91061037090481
Epoch: 1779, Batch Gradient Norm after: 22.360677956490854
Epoch 1780/10000, Prediction Accuracy = 49.798%, Loss = 1.3084301233291626
Epoch: 1780, Batch Gradient Norm: 40.19034170669159
Epoch: 1780, Batch Gradient Norm after: 22.36067553730282
Epoch 1781/10000, Prediction Accuracy = 49.773999999999994%, Loss = 1.2909343719482422
Epoch: 1781, Batch Gradient Norm: 45.944062555912126
Epoch: 1781, Batch Gradient Norm after: 22.360678800050568
Epoch 1782/10000, Prediction Accuracy = 49.802%, Loss = 1.3079188585281372
Epoch: 1782, Batch Gradient Norm: 40.200134133359896
Epoch: 1782, Batch Gradient Norm after: 22.36067661048974
Epoch 1783/10000, Prediction Accuracy = 49.772%, Loss = 1.2903591394424438
Epoch: 1783, Batch Gradient Norm: 45.96657596014853
Epoch: 1783, Batch Gradient Norm after: 22.36067783176084
Epoch 1784/10000, Prediction Accuracy = 49.802%, Loss = 1.307371163368225
Epoch: 1784, Batch Gradient Norm: 40.20690067468738
Epoch: 1784, Batch Gradient Norm after: 22.360676529301188
Epoch 1785/10000, Prediction Accuracy = 49.79%, Loss = 1.2897687435150147
Epoch: 1785, Batch Gradient Norm: 45.979334882931006
Epoch: 1785, Batch Gradient Norm after: 22.36067596273571
Epoch 1786/10000, Prediction Accuracy = 49.814%, Loss = 1.3068044662475586
Epoch: 1786, Batch Gradient Norm: 40.2116523223585
Epoch: 1786, Batch Gradient Norm after: 22.3606764948324
Epoch 1787/10000, Prediction Accuracy = 49.802%, Loss = 1.2891738891601563
Epoch: 1787, Batch Gradient Norm: 45.992121277999196
Epoch: 1787, Batch Gradient Norm after: 22.36067737558621
Epoch 1788/10000, Prediction Accuracy = 49.818%, Loss = 1.306233549118042
Epoch: 1788, Batch Gradient Norm: 40.21941106132429
Epoch: 1788, Batch Gradient Norm after: 22.360676868761196
Epoch 1789/10000, Prediction Accuracy = 49.803999999999995%, Loss = 1.2885836124420167
Epoch: 1789, Batch Gradient Norm: 46.00247475913281
Epoch: 1789, Batch Gradient Norm after: 22.36067750447933
Epoch 1790/10000, Prediction Accuracy = 49.815999999999995%, Loss = 1.30566725730896
Epoch: 1790, Batch Gradient Norm: 40.22191254075595
Epoch: 1790, Batch Gradient Norm after: 22.36067720155876
Epoch 1791/10000, Prediction Accuracy = 49.815999999999995%, Loss = 1.2880006074905395
Epoch: 1791, Batch Gradient Norm: 46.01359246533575
Epoch: 1791, Batch Gradient Norm after: 22.360678548871746
Epoch 1792/10000, Prediction Accuracy = 49.824%, Loss = 1.3050948858261109
Epoch: 1792, Batch Gradient Norm: 40.22805396836656
Epoch: 1792, Batch Gradient Norm after: 22.36067889856251
Epoch 1793/10000, Prediction Accuracy = 49.821999999999996%, Loss = 1.2874107122421266
Epoch: 1793, Batch Gradient Norm: 46.028595053884594
Epoch: 1793, Batch Gradient Norm after: 22.36067857472695
Epoch 1794/10000, Prediction Accuracy = 49.836%, Loss = 1.304534125328064
Epoch: 1794, Batch Gradient Norm: 40.23372977570172
Epoch: 1794, Batch Gradient Norm after: 22.36067895537949
Epoch 1795/10000, Prediction Accuracy = 49.836%, Loss = 1.2868282556533814
Epoch: 1795, Batch Gradient Norm: 46.041713189393874
Epoch: 1795, Batch Gradient Norm after: 22.360677191366598
Epoch 1796/10000, Prediction Accuracy = 49.836%, Loss = 1.3039681911468506
Epoch: 1796, Batch Gradient Norm: 40.24078063914737
Epoch: 1796, Batch Gradient Norm after: 22.3606777349874
Epoch 1797/10000, Prediction Accuracy = 49.848%, Loss = 1.286227321624756
Epoch: 1797, Batch Gradient Norm: 46.04969202996549
Epoch: 1797, Batch Gradient Norm after: 22.360678102085515
Epoch 1798/10000, Prediction Accuracy = 49.838%, Loss = 1.3033859491348267
Epoch: 1798, Batch Gradient Norm: 40.24287187639568
Epoch: 1798, Batch Gradient Norm after: 22.36067863129995
Epoch 1799/10000, Prediction Accuracy = 49.852000000000004%, Loss = 1.2856468200683593
Epoch: 1799, Batch Gradient Norm: 46.04690113560559
Epoch: 1799, Batch Gradient Norm after: 22.360677077669443
Epoch 1800/10000, Prediction Accuracy = 49.852%, Loss = 1.3027761220932006
Epoch: 1800, Batch Gradient Norm: 40.242646388874554
Epoch: 1800, Batch Gradient Norm after: 22.36067571191042
Epoch 1801/10000, Prediction Accuracy = 49.855999999999995%, Loss = 1.2850504159927367
Epoch: 1801, Batch Gradient Norm: 46.04203160250075
Epoch: 1801, Batch Gradient Norm after: 22.360678756731154
Epoch 1802/10000, Prediction Accuracy = 49.85600000000001%, Loss = 1.3021659135818482
Epoch: 1802, Batch Gradient Norm: 40.242517474235704
Epoch: 1802, Batch Gradient Norm after: 22.3606793321851
Epoch 1803/10000, Prediction Accuracy = 49.874%, Loss = 1.284455919265747
Epoch: 1803, Batch Gradient Norm: 46.035099966077105
Epoch: 1803, Batch Gradient Norm after: 22.360677174675352
Epoch 1804/10000, Prediction Accuracy = 49.866%, Loss = 1.3015378952026366
Epoch: 1804, Batch Gradient Norm: 40.24418092537001
Epoch: 1804, Batch Gradient Norm after: 22.36067662404071
Epoch 1805/10000, Prediction Accuracy = 49.886%, Loss = 1.2838620662689209
Epoch: 1805, Batch Gradient Norm: 46.02688441331693
Epoch: 1805, Batch Gradient Norm after: 22.360676959936313
Epoch 1806/10000, Prediction Accuracy = 49.872%, Loss = 1.300919246673584
Epoch: 1806, Batch Gradient Norm: 40.244568641962466
Epoch: 1806, Batch Gradient Norm after: 22.36067870406621
Epoch 1807/10000, Prediction Accuracy = 49.894000000000005%, Loss = 1.2832644701004028
Epoch: 1807, Batch Gradient Norm: 46.0141806506748
Epoch: 1807, Batch Gradient Norm after: 22.360677671160527
Epoch 1808/10000, Prediction Accuracy = 49.87%, Loss = 1.30029456615448
Epoch: 1808, Batch Gradient Norm: 40.24501724859058
Epoch: 1808, Batch Gradient Norm after: 22.36067755064184
Epoch 1809/10000, Prediction Accuracy = 49.902%, Loss = 1.2826699018478394
Epoch: 1809, Batch Gradient Norm: 46.00852122027368
Epoch: 1809, Batch Gradient Norm after: 22.360678125369535
Epoch 1810/10000, Prediction Accuracy = 49.884%, Loss = 1.299690318107605
Epoch: 1810, Batch Gradient Norm: 40.245647863877714
Epoch: 1810, Batch Gradient Norm after: 22.36067856568081
Epoch 1811/10000, Prediction Accuracy = 49.918000000000006%, Loss = 1.2820779800415039
Epoch: 1811, Batch Gradient Norm: 46.01007688595238
Epoch: 1811, Batch Gradient Norm after: 22.360678005311893
Epoch 1812/10000, Prediction Accuracy = 49.902%, Loss = 1.2990989923477172
Epoch: 1812, Batch Gradient Norm: 40.247924206658745
Epoch: 1812, Batch Gradient Norm after: 22.36067714702725
Epoch 1813/10000, Prediction Accuracy = 49.92%, Loss = 1.2814927577972413
Epoch: 1813, Batch Gradient Norm: 46.005097794946366
Epoch: 1813, Batch Gradient Norm after: 22.36067815535689
Epoch 1814/10000, Prediction Accuracy = 49.900000000000006%, Loss = 1.298493218421936
Epoch: 1814, Batch Gradient Norm: 40.251401240467885
Epoch: 1814, Batch Gradient Norm after: 22.360676965044103
Epoch 1815/10000, Prediction Accuracy = 49.922000000000004%, Loss = 1.2809144735336304
Epoch: 1815, Batch Gradient Norm: 46.000397453297225
Epoch: 1815, Batch Gradient Norm after: 22.360676612809286
Epoch 1816/10000, Prediction Accuracy = 49.914%, Loss = 1.297881817817688
Epoch: 1816, Batch Gradient Norm: 40.250308146455836
Epoch: 1816, Batch Gradient Norm after: 22.36067594606173
Epoch 1817/10000, Prediction Accuracy = 49.946%, Loss = 1.2803163051605224
Epoch: 1817, Batch Gradient Norm: 45.984353454466905
Epoch: 1817, Batch Gradient Norm after: 22.360677765011946
Epoch 1818/10000, Prediction Accuracy = 49.91799999999999%, Loss = 1.2972498416900635
Epoch: 1818, Batch Gradient Norm: 40.247458787101806
Epoch: 1818, Batch Gradient Norm after: 22.360678012344977
Epoch 1819/10000, Prediction Accuracy = 49.956%, Loss = 1.279726004600525
Epoch: 1819, Batch Gradient Norm: 45.974522348035755
Epoch: 1819, Batch Gradient Norm after: 22.360677130524103
Epoch 1820/10000, Prediction Accuracy = 49.926%, Loss = 1.2966342210769652
Epoch: 1820, Batch Gradient Norm: 40.24397694142768
Epoch: 1820, Batch Gradient Norm after: 22.360676033611497
Epoch 1821/10000, Prediction Accuracy = 49.98%, Loss = 1.2791385650634766
Epoch: 1821, Batch Gradient Norm: 45.95688877967768
Epoch: 1821, Batch Gradient Norm after: 22.360676787828115
Epoch 1822/10000, Prediction Accuracy = 49.932%, Loss = 1.2960066318511962
Epoch: 1822, Batch Gradient Norm: 40.24182217124057
Epoch: 1822, Batch Gradient Norm after: 22.360677510680468
Epoch 1823/10000, Prediction Accuracy = 49.983999999999995%, Loss = 1.278558349609375
Epoch: 1823, Batch Gradient Norm: 45.94359666864378
Epoch: 1823, Batch Gradient Norm after: 22.360679100489914
Epoch 1824/10000, Prediction Accuracy = 49.93400000000001%, Loss = 1.2953711032867432
Epoch: 1824, Batch Gradient Norm: 40.244324409284886
Epoch: 1824, Batch Gradient Norm after: 22.36067668151508
Epoch 1825/10000, Prediction Accuracy = 50.007999999999996%, Loss = 1.2779884815216065
Epoch: 1825, Batch Gradient Norm: 45.93422912744224
Epoch: 1825, Batch Gradient Norm after: 22.36067638591671
Epoch 1826/10000, Prediction Accuracy = 49.955999999999996%, Loss = 1.294761347770691
Epoch: 1826, Batch Gradient Norm: 40.23564617143841
Epoch: 1826, Batch Gradient Norm after: 22.36067663945201
Epoch 1827/10000, Prediction Accuracy = 50.024%, Loss = 1.2774051666259765
Epoch: 1827, Batch Gradient Norm: 45.905507375792986
Epoch: 1827, Batch Gradient Norm after: 22.3606753003913
Epoch 1828/10000, Prediction Accuracy = 49.965999999999994%, Loss = 1.2941057443618775
Epoch: 1828, Batch Gradient Norm: 40.22696265390369
Epoch: 1828, Batch Gradient Norm after: 22.360677699258392
Epoch 1829/10000, Prediction Accuracy = 50.028%, Loss = 1.2768083333969116
Epoch: 1829, Batch Gradient Norm: 45.89091196589784
Epoch: 1829, Batch Gradient Norm after: 22.360676906338497
Epoch 1830/10000, Prediction Accuracy = 49.98199999999999%, Loss = 1.2934791088104247
Epoch: 1830, Batch Gradient Norm: 40.21878639818888
Epoch: 1830, Batch Gradient Norm after: 22.360676903562613
Epoch 1831/10000, Prediction Accuracy = 50.034%, Loss = 1.2762131452560426
Epoch: 1831, Batch Gradient Norm: 45.87003409095596
Epoch: 1831, Batch Gradient Norm after: 22.36067742561454
Epoch 1832/10000, Prediction Accuracy = 49.986000000000004%, Loss = 1.2928374528884887
Epoch: 1832, Batch Gradient Norm: 40.21031589031451
Epoch: 1832, Batch Gradient Norm after: 22.360675320110783
Epoch 1833/10000, Prediction Accuracy = 50.040000000000006%, Loss = 1.2756243228912354
Epoch: 1833, Batch Gradient Norm: 45.84845362685778
Epoch: 1833, Batch Gradient Norm after: 22.360678099928744
Epoch 1834/10000, Prediction Accuracy = 49.986000000000004%, Loss = 1.2921993494033814
Epoch: 1834, Batch Gradient Norm: 40.20689444190019
Epoch: 1834, Batch Gradient Norm after: 22.360677474192432
Epoch 1835/10000, Prediction Accuracy = 50.05%, Loss = 1.2750487089157105
Epoch: 1835, Batch Gradient Norm: 45.83205690295606
Epoch: 1835, Batch Gradient Norm after: 22.360677274081436
Epoch 1836/10000, Prediction Accuracy = 49.98599999999999%, Loss = 1.2915806293487548
Epoch: 1836, Batch Gradient Norm: 40.204736526391926
Epoch: 1836, Batch Gradient Norm after: 22.36067777577283
Epoch 1837/10000, Prediction Accuracy = 50.059999999999995%, Loss = 1.2744770288467406
Epoch: 1837, Batch Gradient Norm: 45.82508100616342
Epoch: 1837, Batch Gradient Norm after: 22.360677943254746
Epoch 1838/10000, Prediction Accuracy = 50.0%, Loss = 1.2909760236740113
Epoch: 1838, Batch Gradient Norm: 40.20416456433529
Epoch: 1838, Batch Gradient Norm after: 22.360677588146835
Epoch 1839/10000, Prediction Accuracy = 50.07000000000001%, Loss = 1.2739164113998414
Epoch: 1839, Batch Gradient Norm: 45.81512412119876
Epoch: 1839, Batch Gradient Norm after: 22.360678414774682
Epoch 1840/10000, Prediction Accuracy = 50.004%, Loss = 1.2903817653656007
Epoch: 1840, Batch Gradient Norm: 40.2008497143513
Epoch: 1840, Batch Gradient Norm after: 22.36067642422215
Epoch 1841/10000, Prediction Accuracy = 50.06999999999999%, Loss = 1.273339533805847
Epoch: 1841, Batch Gradient Norm: 45.797769176543795
Epoch: 1841, Batch Gradient Norm after: 22.3606792173392
Epoch 1842/10000, Prediction Accuracy = 50.023999999999994%, Loss = 1.2897576093673706
Epoch: 1842, Batch Gradient Norm: 40.19467122079332
Epoch: 1842, Batch Gradient Norm after: 22.360675580704676
Epoch 1843/10000, Prediction Accuracy = 50.072%, Loss = 1.2727580785751342
Epoch: 1843, Batch Gradient Norm: 45.77552992744945
Epoch: 1843, Batch Gradient Norm after: 22.360678811792802
Epoch 1844/10000, Prediction Accuracy = 50.016%, Loss = 1.2891250371932983
Epoch: 1844, Batch Gradient Norm: 40.188852601229485
Epoch: 1844, Batch Gradient Norm after: 22.360677852939478
Epoch 1845/10000, Prediction Accuracy = 50.08%, Loss = 1.272184133529663
Epoch: 1845, Batch Gradient Norm: 45.758116517950675
Epoch: 1845, Batch Gradient Norm after: 22.360678753593337
Epoch 1846/10000, Prediction Accuracy = 50.022000000000006%, Loss = 1.2885159015655518
Epoch: 1846, Batch Gradient Norm: 40.18870635251866
Epoch: 1846, Batch Gradient Norm after: 22.360676531172853
Epoch 1847/10000, Prediction Accuracy = 50.086%, Loss = 1.2716230869293212
Epoch: 1847, Batch Gradient Norm: 45.758846693760155
Epoch: 1847, Batch Gradient Norm after: 22.360677615080384
Epoch 1848/10000, Prediction Accuracy = 50.034000000000006%, Loss = 1.2879513025283813
Epoch: 1848, Batch Gradient Norm: 40.186220165703034
Epoch: 1848, Batch Gradient Norm after: 22.360677639054334
Epoch 1849/10000, Prediction Accuracy = 50.088%, Loss = 1.2710755586624145
Epoch: 1849, Batch Gradient Norm: 45.762420851874616
Epoch: 1849, Batch Gradient Norm after: 22.360678443294255
Epoch 1850/10000, Prediction Accuracy = 50.059999999999995%, Loss = 1.287394404411316
Epoch: 1850, Batch Gradient Norm: 40.188632265088934
Epoch: 1850, Batch Gradient Norm after: 22.360676913336537
Epoch 1851/10000, Prediction Accuracy = 50.096000000000004%, Loss = 1.2705219030380248
Epoch: 1851, Batch Gradient Norm: 45.7702300537299
Epoch: 1851, Batch Gradient Norm after: 22.360677780013706
Epoch 1852/10000, Prediction Accuracy = 50.062%, Loss = 1.2868516206741334
Epoch: 1852, Batch Gradient Norm: 40.19166512240313
Epoch: 1852, Batch Gradient Norm after: 22.360677354215508
Epoch 1853/10000, Prediction Accuracy = 50.096000000000004%, Loss = 1.269958209991455
Epoch: 1853, Batch Gradient Norm: 45.768313689731364
Epoch: 1853, Batch Gradient Norm after: 22.36067625007771
Epoch 1854/10000, Prediction Accuracy = 50.064%, Loss = 1.2862849950790405
Epoch: 1854, Batch Gradient Norm: 40.18981106286251
Epoch: 1854, Batch Gradient Norm after: 22.360678316039106
Epoch 1855/10000, Prediction Accuracy = 50.12%, Loss = 1.2694010257720947
Epoch: 1855, Batch Gradient Norm: 45.758706712725015
Epoch: 1855, Batch Gradient Norm after: 22.3606762277821
Epoch 1856/10000, Prediction Accuracy = 50.06%, Loss = 1.2856995582580566
Epoch: 1856, Batch Gradient Norm: 40.18699520776612
Epoch: 1856, Batch Gradient Norm after: 22.360676365556422
Epoch 1857/10000, Prediction Accuracy = 50.134%, Loss = 1.2688412427902223
Epoch: 1857, Batch Gradient Norm: 45.74396986381134
Epoch: 1857, Batch Gradient Norm after: 22.360676597435756
Epoch 1858/10000, Prediction Accuracy = 50.077999999999996%, Loss = 1.285094928741455
Epoch: 1858, Batch Gradient Norm: 40.183792297102535
Epoch: 1858, Batch Gradient Norm after: 22.36067749812017
Epoch 1859/10000, Prediction Accuracy = 50.146%, Loss = 1.2682717561721801
Epoch: 1859, Batch Gradient Norm: 45.72771782220038
Epoch: 1859, Batch Gradient Norm after: 22.3606777027558
Epoch 1860/10000, Prediction Accuracy = 50.086%, Loss = 1.2844922065734863
Epoch: 1860, Batch Gradient Norm: 40.182519007557794
Epoch: 1860, Batch Gradient Norm after: 22.360676760927934
Epoch 1861/10000, Prediction Accuracy = 50.164%, Loss = 1.2677064180374145
Epoch: 1861, Batch Gradient Norm: 45.70186669311281
Epoch: 1861, Batch Gradient Norm after: 22.36067676053893
Epoch 1862/10000, Prediction Accuracy = 50.11%, Loss = 1.2838523149490357
Epoch: 1862, Batch Gradient Norm: 40.17360434239164
Epoch: 1862, Batch Gradient Norm after: 22.360675712537276
Epoch 1863/10000, Prediction Accuracy = 50.176%, Loss = 1.2671354293823243
Epoch: 1863, Batch Gradient Norm: 45.67297354323325
Epoch: 1863, Batch Gradient Norm after: 22.360675322893346
Epoch 1864/10000, Prediction Accuracy = 50.11000000000001%, Loss = 1.2831938982009887
Epoch: 1864, Batch Gradient Norm: 40.16477723320464
Epoch: 1864, Batch Gradient Norm after: 22.360675149627426
Epoch 1865/10000, Prediction Accuracy = 50.188%, Loss = 1.2665589094161986
Epoch: 1865, Batch Gradient Norm: 45.631276599975
Epoch: 1865, Batch Gradient Norm after: 22.36067523994248
Epoch 1866/10000, Prediction Accuracy = 50.12%, Loss = 1.2825124263763428
Epoch: 1866, Batch Gradient Norm: 40.156983045092865
Epoch: 1866, Batch Gradient Norm after: 22.36067844920317
Epoch 1867/10000, Prediction Accuracy = 50.2%, Loss = 1.2659897089004517
Epoch: 1867, Batch Gradient Norm: 45.603202038338225
Epoch: 1867, Batch Gradient Norm after: 22.360676793146425
Epoch 1868/10000, Prediction Accuracy = 50.124%, Loss = 1.2818727016448974
Epoch: 1868, Batch Gradient Norm: 40.152838095078025
Epoch: 1868, Batch Gradient Norm after: 22.360676258744157
Epoch 1869/10000, Prediction Accuracy = 50.212%, Loss = 1.265430784225464
Epoch: 1869, Batch Gradient Norm: 45.581384158324695
Epoch: 1869, Batch Gradient Norm after: 22.360677783071843
Epoch 1870/10000, Prediction Accuracy = 50.132%, Loss = 1.2812628984451293
Epoch: 1870, Batch Gradient Norm: 40.148357339792966
Epoch: 1870, Batch Gradient Norm after: 22.36067444144437
Epoch 1871/10000, Prediction Accuracy = 50.22%, Loss = 1.2648715019226073
Epoch: 1871, Batch Gradient Norm: 45.56192848532982
Epoch: 1871, Batch Gradient Norm after: 22.36067692073137
Epoch 1872/10000, Prediction Accuracy = 50.144%, Loss = 1.2806645393371583
Epoch: 1872, Batch Gradient Norm: 40.142346110902196
Epoch: 1872, Batch Gradient Norm after: 22.360674270969895
Epoch 1873/10000, Prediction Accuracy = 50.222%, Loss = 1.2643104314804077
Epoch: 1873, Batch Gradient Norm: 45.546471346957325
Epoch: 1873, Batch Gradient Norm after: 22.36067970900711
Epoch 1874/10000, Prediction Accuracy = 50.15%, Loss = 1.280079960823059
Epoch: 1874, Batch Gradient Norm: 40.13881084511358
Epoch: 1874, Batch Gradient Norm after: 22.360677195062188
Epoch 1875/10000, Prediction Accuracy = 50.226%, Loss = 1.2637640953063964
Epoch: 1875, Batch Gradient Norm: 45.530331994832494
Epoch: 1875, Batch Gradient Norm after: 22.360678488704064
Epoch 1876/10000, Prediction Accuracy = 50.17%, Loss = 1.279492402076721
Epoch: 1876, Batch Gradient Norm: 40.13559247216348
Epoch: 1876, Batch Gradient Norm after: 22.36067578501891
Epoch 1877/10000, Prediction Accuracy = 50.232%, Loss = 1.2632174730300902
Epoch: 1877, Batch Gradient Norm: 45.51696188664111
Epoch: 1877, Batch Gradient Norm after: 22.36067878251661
Epoch 1878/10000, Prediction Accuracy = 50.181999999999995%, Loss = 1.2789117336273192
Epoch: 1878, Batch Gradient Norm: 40.13436238672642
Epoch: 1878, Batch Gradient Norm after: 22.36067660042355
Epoch 1879/10000, Prediction Accuracy = 50.244%, Loss = 1.2626861810684205
Epoch: 1879, Batch Gradient Norm: 45.51267348708562
Epoch: 1879, Batch Gradient Norm after: 22.36067795375221
Epoch 1880/10000, Prediction Accuracy = 50.192%, Loss = 1.2783746004104615
Epoch: 1880, Batch Gradient Norm: 40.130663949907465
Epoch: 1880, Batch Gradient Norm after: 22.36067534745137
Epoch 1881/10000, Prediction Accuracy = 50.242%, Loss = 1.2621524810791016
Epoch: 1881, Batch Gradient Norm: 45.51003819084016
Epoch: 1881, Batch Gradient Norm after: 22.360676169602357
Epoch 1882/10000, Prediction Accuracy = 50.19799999999999%, Loss = 1.2778405666351318
Epoch: 1882, Batch Gradient Norm: 40.13054284173178
Epoch: 1882, Batch Gradient Norm after: 22.360677172944353
Epoch 1883/10000, Prediction Accuracy = 50.25%, Loss = 1.2616187810897828
Epoch: 1883, Batch Gradient Norm: 45.50464670797545
Epoch: 1883, Batch Gradient Norm after: 22.360676639264035
Epoch 1884/10000, Prediction Accuracy = 50.209999999999994%, Loss = 1.2772917985916137
Epoch: 1884, Batch Gradient Norm: 40.130101623173665
Epoch: 1884, Batch Gradient Norm after: 22.360676734648898
Epoch 1885/10000, Prediction Accuracy = 50.27%, Loss = 1.2610890865325928
Epoch: 1885, Batch Gradient Norm: 45.50017494083943
Epoch: 1885, Batch Gradient Norm after: 22.360676110106
Epoch 1886/10000, Prediction Accuracy = 50.212%, Loss = 1.2767490863800048
Epoch: 1886, Batch Gradient Norm: 40.12761208908451
Epoch: 1886, Batch Gradient Norm after: 22.36067570002623
Epoch 1887/10000, Prediction Accuracy = 50.290000000000006%, Loss = 1.2605558395385743
Epoch: 1887, Batch Gradient Norm: 45.49365703019501
Epoch: 1887, Batch Gradient Norm after: 22.360675754670453
Epoch 1888/10000, Prediction Accuracy = 50.222%, Loss = 1.2761889219284057
Epoch: 1888, Batch Gradient Norm: 40.127900699669
Epoch: 1888, Batch Gradient Norm after: 22.360676685023694
Epoch 1889/10000, Prediction Accuracy = 50.306%, Loss = 1.2600273370742798
Epoch: 1889, Batch Gradient Norm: 45.490117964029174
Epoch: 1889, Batch Gradient Norm after: 22.360676232849112
Epoch 1890/10000, Prediction Accuracy = 50.23%, Loss = 1.2756509065628052
Epoch: 1890, Batch Gradient Norm: 40.12551933614387
Epoch: 1890, Batch Gradient Norm after: 22.36067616216002
Epoch 1891/10000, Prediction Accuracy = 50.312000000000005%, Loss = 1.2594995260238648
Epoch: 1891, Batch Gradient Norm: 45.483379411643334
Epoch: 1891, Batch Gradient Norm after: 22.36067549951672
Epoch 1892/10000, Prediction Accuracy = 50.238%, Loss = 1.2750929355621339
Epoch: 1892, Batch Gradient Norm: 40.12459831122933
Epoch: 1892, Batch Gradient Norm after: 22.360675318007623
Epoch 1893/10000, Prediction Accuracy = 50.336%, Loss = 1.2589677572250366
Epoch: 1893, Batch Gradient Norm: 45.47292191487373
Epoch: 1893, Batch Gradient Norm after: 22.36067602329073
Epoch 1894/10000, Prediction Accuracy = 50.244%, Loss = 1.274534320831299
Epoch: 1894, Batch Gradient Norm: 40.12309201403331
Epoch: 1894, Batch Gradient Norm after: 22.360676808599884
Epoch 1895/10000, Prediction Accuracy = 50.339999999999996%, Loss = 1.2584367275238038
Epoch: 1895, Batch Gradient Norm: 45.465965435346995
Epoch: 1895, Batch Gradient Norm after: 22.360677271822986
Epoch 1896/10000, Prediction Accuracy = 50.262%, Loss = 1.2739837884902954
Epoch: 1896, Batch Gradient Norm: 40.12314941280158
Epoch: 1896, Batch Gradient Norm after: 22.360676253123568
Epoch 1897/10000, Prediction Accuracy = 50.342%, Loss = 1.25790593624115
Epoch: 1897, Batch Gradient Norm: 45.460872796717126
Epoch: 1897, Batch Gradient Norm after: 22.360675464402153
Epoch 1898/10000, Prediction Accuracy = 50.263999999999996%, Loss = 1.2734418869018556
Epoch: 1898, Batch Gradient Norm: 40.12223901696176
Epoch: 1898, Batch Gradient Norm after: 22.360674143328435
Epoch 1899/10000, Prediction Accuracy = 50.358000000000004%, Loss = 1.2573881149291992
Epoch: 1899, Batch Gradient Norm: 45.45708513159129
Epoch: 1899, Batch Gradient Norm after: 22.360676737784786
Epoch 1900/10000, Prediction Accuracy = 50.274%, Loss = 1.272910976409912
Epoch: 1900, Batch Gradient Norm: 40.12289986386016
Epoch: 1900, Batch Gradient Norm after: 22.360675558939725
Epoch 1901/10000, Prediction Accuracy = 50.364%, Loss = 1.2568637371063232
Epoch: 1901, Batch Gradient Norm: 45.446790543119555
Epoch: 1901, Batch Gradient Norm after: 22.36067733053763
Epoch 1902/10000, Prediction Accuracy = 50.28%, Loss = 1.27235267162323
Epoch: 1902, Batch Gradient Norm: 40.117603538233034
Epoch: 1902, Batch Gradient Norm after: 22.36067688364228
Epoch 1903/10000, Prediction Accuracy = 50.378%, Loss = 1.2563276767730713
Epoch: 1903, Batch Gradient Norm: 45.43147867908696
Epoch: 1903, Batch Gradient Norm after: 22.36067480041378
Epoch 1904/10000, Prediction Accuracy = 50.286%, Loss = 1.2717856645584107
Epoch: 1904, Batch Gradient Norm: 40.11716061844087
Epoch: 1904, Batch Gradient Norm after: 22.360677122747894
Epoch 1905/10000, Prediction Accuracy = 50.386%, Loss = 1.2557995080947877
Epoch: 1905, Batch Gradient Norm: 45.42578760777105
Epoch: 1905, Batch Gradient Norm after: 22.360678107839338
Epoch 1906/10000, Prediction Accuracy = 50.29600000000001%, Loss = 1.2712514400482178
Epoch: 1906, Batch Gradient Norm: 40.117510223697835
Epoch: 1906, Batch Gradient Norm after: 22.36067441169867
Epoch 1907/10000, Prediction Accuracy = 50.39200000000001%, Loss = 1.255283498764038
Epoch: 1907, Batch Gradient Norm: 45.42062810356943
Epoch: 1907, Batch Gradient Norm after: 22.360678200622214
Epoch 1908/10000, Prediction Accuracy = 50.29200000000001%, Loss = 1.2707163095474243
Epoch: 1908, Batch Gradient Norm: 40.11429055842113
Epoch: 1908, Batch Gradient Norm after: 22.36067624034687
Epoch 1909/10000, Prediction Accuracy = 50.39200000000001%, Loss = 1.2547699451446532
Epoch: 1909, Batch Gradient Norm: 45.41527074507583
Epoch: 1909, Batch Gradient Norm after: 22.360677821381167
Epoch 1910/10000, Prediction Accuracy = 50.308%, Loss = 1.2701828241348267
Epoch: 1910, Batch Gradient Norm: 40.1121645044754
Epoch: 1910, Batch Gradient Norm after: 22.36067688753788
Epoch 1911/10000, Prediction Accuracy = 50.403999999999996%, Loss = 1.2542482376098634
Epoch: 1911, Batch Gradient Norm: 45.40192080333737
Epoch: 1911, Batch Gradient Norm after: 22.36067688970811
Epoch 1912/10000, Prediction Accuracy = 50.312%, Loss = 1.2696253299713134
Epoch: 1912, Batch Gradient Norm: 40.108153482836904
Epoch: 1912, Batch Gradient Norm after: 22.36067789815763
Epoch 1913/10000, Prediction Accuracy = 50.40599999999999%, Loss = 1.2537273168563843
Epoch: 1913, Batch Gradient Norm: 45.38374237956421
Epoch: 1913, Batch Gradient Norm after: 22.360677435723307
Epoch 1914/10000, Prediction Accuracy = 50.328%, Loss = 1.2690629959106445
Epoch: 1914, Batch Gradient Norm: 40.10554615681665
Epoch: 1914, Batch Gradient Norm after: 22.360677881335796
Epoch 1915/10000, Prediction Accuracy = 50.4%, Loss = 1.2532064199447632
Epoch: 1915, Batch Gradient Norm: 45.37023994066412
Epoch: 1915, Batch Gradient Norm after: 22.360676733023375
Epoch 1916/10000, Prediction Accuracy = 50.35%, Loss = 1.2684991359710693
Epoch: 1916, Batch Gradient Norm: 40.10465718525593
Epoch: 1916, Batch Gradient Norm after: 22.360675803689194
Epoch 1917/10000, Prediction Accuracy = 50.412%, Loss = 1.2526973247528077
Epoch: 1917, Batch Gradient Norm: 45.36195993466402
Epoch: 1917, Batch Gradient Norm after: 22.360677120224295
Epoch 1918/10000, Prediction Accuracy = 50.366%, Loss = 1.2679666757583619
Epoch: 1918, Batch Gradient Norm: 40.101572963005786
Epoch: 1918, Batch Gradient Norm after: 22.360677735886007
Epoch 1919/10000, Prediction Accuracy = 50.422000000000004%, Loss = 1.2521840572357177
Epoch: 1919, Batch Gradient Norm: 45.34664282712567
Epoch: 1919, Batch Gradient Norm after: 22.360675359757426
Epoch 1920/10000, Prediction Accuracy = 50.384%, Loss = 1.2674101829528808
Epoch: 1920, Batch Gradient Norm: 40.095640683703316
Epoch: 1920, Batch Gradient Norm after: 22.360676651510047
Epoch 1921/10000, Prediction Accuracy = 50.438%, Loss = 1.2516518115997315
Epoch: 1921, Batch Gradient Norm: 45.330515781123395
Epoch: 1921, Batch Gradient Norm after: 22.360675874549077
Epoch 1922/10000, Prediction Accuracy = 50.388%, Loss = 1.2668304443359375
Epoch: 1922, Batch Gradient Norm: 40.09298248401047
Epoch: 1922, Batch Gradient Norm after: 22.36067655732806
Epoch 1923/10000, Prediction Accuracy = 50.440000000000005%, Loss = 1.2511399745941163
Epoch: 1923, Batch Gradient Norm: 45.31653942551242
Epoch: 1923, Batch Gradient Norm after: 22.360676287974805
Epoch 1924/10000, Prediction Accuracy = 50.391999999999996%, Loss = 1.2662699222564697
Epoch: 1924, Batch Gradient Norm: 40.085917853608436
Epoch: 1924, Batch Gradient Norm after: 22.360677913103114
Epoch 1925/10000, Prediction Accuracy = 50.446%, Loss = 1.2506108045578004
Epoch: 1925, Batch Gradient Norm: 45.28672013815956
Epoch: 1925, Batch Gradient Norm after: 22.36067693307704
Epoch 1926/10000, Prediction Accuracy = 50.410000000000004%, Loss = 1.2656776428222656
Epoch: 1926, Batch Gradient Norm: 40.083635224500156
Epoch: 1926, Batch Gradient Norm after: 22.36067778407726
Epoch 1927/10000, Prediction Accuracy = 50.45399999999999%, Loss = 1.2501010417938232
Epoch: 1927, Batch Gradient Norm: 45.27031683997239
Epoch: 1927, Batch Gradient Norm after: 22.360676262173616
Epoch 1928/10000, Prediction Accuracy = 50.422000000000004%, Loss = 1.2651144981384277
Epoch: 1928, Batch Gradient Norm: 40.079976887531444
Epoch: 1928, Batch Gradient Norm after: 22.36068031872128
Epoch 1929/10000, Prediction Accuracy = 50.461999999999996%, Loss = 1.2495932102203369
Epoch: 1929, Batch Gradient Norm: 45.25448643519025
Epoch: 1929, Batch Gradient Norm after: 22.3606754526237
Epoch 1930/10000, Prediction Accuracy = 50.434%, Loss = 1.264553928375244
Epoch: 1930, Batch Gradient Norm: 40.076868292082466
Epoch: 1930, Batch Gradient Norm after: 22.360675836161967
Epoch 1931/10000, Prediction Accuracy = 50.468%, Loss = 1.2490867614746093
Epoch: 1931, Batch Gradient Norm: 45.23227667582944
Epoch: 1931, Batch Gradient Norm after: 22.360676738648767
Epoch 1932/10000, Prediction Accuracy = 50.440000000000005%, Loss = 1.2639808893203734
Epoch: 1932, Batch Gradient Norm: 40.074276153597964
Epoch: 1932, Batch Gradient Norm after: 22.3606769244789
Epoch 1933/10000, Prediction Accuracy = 50.482%, Loss = 1.2485769271850586
Epoch: 1933, Batch Gradient Norm: 45.21337435475528
Epoch: 1933, Batch Gradient Norm after: 22.360675629461696
Epoch 1934/10000, Prediction Accuracy = 50.446%, Loss = 1.2634228229522706
Epoch: 1934, Batch Gradient Norm: 40.07049681859742
Epoch: 1934, Batch Gradient Norm after: 22.360676041823837
Epoch 1935/10000, Prediction Accuracy = 50.498000000000005%, Loss = 1.2480623483657838
Epoch: 1935, Batch Gradient Norm: 45.19761893347757
Epoch: 1935, Batch Gradient Norm after: 22.36067632894883
Epoch 1936/10000, Prediction Accuracy = 50.45%, Loss = 1.2628685712814331
Epoch: 1936, Batch Gradient Norm: 40.065850020684074
Epoch: 1936, Batch Gradient Norm after: 22.3606769789035
Epoch 1937/10000, Prediction Accuracy = 50.513999999999996%, Loss = 1.2475549697875976
Epoch: 1937, Batch Gradient Norm: 45.183140631600956
Epoch: 1937, Batch Gradient Norm after: 22.360676750876543
Epoch 1938/10000, Prediction Accuracy = 50.465999999999994%, Loss = 1.262317156791687
Epoch: 1938, Batch Gradient Norm: 40.06410611515083
Epoch: 1938, Batch Gradient Norm after: 22.36067665114148
Epoch 1939/10000, Prediction Accuracy = 50.525999999999996%, Loss = 1.2470520257949829
Epoch: 1939, Batch Gradient Norm: 45.16524332164511
Epoch: 1939, Batch Gradient Norm after: 22.360677562789945
Epoch 1940/10000, Prediction Accuracy = 50.484%, Loss = 1.2617536067962647
Epoch: 1940, Batch Gradient Norm: 40.06110929180528
Epoch: 1940, Batch Gradient Norm after: 22.360676888203233
Epoch 1941/10000, Prediction Accuracy = 50.544000000000004%, Loss = 1.2465366840362548
Epoch: 1941, Batch Gradient Norm: 45.14790855324173
Epoch: 1941, Batch Gradient Norm after: 22.360677457095427
Epoch 1942/10000, Prediction Accuracy = 50.489999999999995%, Loss = 1.2612069845199585
Epoch: 1942, Batch Gradient Norm: 40.05702995586324
Epoch: 1942, Batch Gradient Norm after: 22.36067848672045
Epoch 1943/10000, Prediction Accuracy = 50.56%, Loss = 1.246029305458069
Epoch: 1943, Batch Gradient Norm: 45.14022563001387
Epoch: 1943, Batch Gradient Norm after: 22.36067359612146
Epoch 1944/10000, Prediction Accuracy = 50.516%, Loss = 1.2606873750686645
Epoch: 1944, Batch Gradient Norm: 40.05357962909713
Epoch: 1944, Batch Gradient Norm after: 22.360676221629532
Epoch 1945/10000, Prediction Accuracy = 50.57600000000001%, Loss = 1.2455279350280761
Epoch: 1945, Batch Gradient Norm: 45.13186248528993
Epoch: 1945, Batch Gradient Norm after: 22.360675178182046
Epoch 1946/10000, Prediction Accuracy = 50.519999999999996%, Loss = 1.2601651668548584
Epoch: 1946, Batch Gradient Norm: 40.05204222773422
Epoch: 1946, Batch Gradient Norm after: 22.360675616322983
Epoch 1947/10000, Prediction Accuracy = 50.586%, Loss = 1.245037317276001
Epoch: 1947, Batch Gradient Norm: 45.12578168323489
Epoch: 1947, Batch Gradient Norm after: 22.360675323142917
Epoch 1948/10000, Prediction Accuracy = 50.528000000000006%, Loss = 1.2596436977386474
Epoch: 1948, Batch Gradient Norm: 40.053978227324244
Epoch: 1948, Batch Gradient Norm after: 22.360677520317104
Epoch 1949/10000, Prediction Accuracy = 50.602%, Loss = 1.244547700881958
Epoch: 1949, Batch Gradient Norm: 45.12992166615135
Epoch: 1949, Batch Gradient Norm after: 22.360675549555182
Epoch 1950/10000, Prediction Accuracy = 50.535999999999994%, Loss = 1.2591565132141114
Epoch: 1950, Batch Gradient Norm: 40.0546438894161
Epoch: 1950, Batch Gradient Norm after: 22.360677194025147
Epoch 1951/10000, Prediction Accuracy = 50.61%, Loss = 1.2440538167953492
Epoch: 1951, Batch Gradient Norm: 45.143439913523544
Epoch: 1951, Batch Gradient Norm after: 22.36067763815856
Epoch 1952/10000, Prediction Accuracy = 50.547999999999995%, Loss = 1.2587013244628906
Epoch: 1952, Batch Gradient Norm: 40.060202565217956
Epoch: 1952, Batch Gradient Norm after: 22.36067757660466
Epoch 1953/10000, Prediction Accuracy = 50.628%, Loss = 1.2435750246047974
Epoch: 1953, Batch Gradient Norm: 45.159277601232
Epoch: 1953, Batch Gradient Norm after: 22.36067811160487
Epoch 1954/10000, Prediction Accuracy = 50.552%, Loss = 1.258262848854065
Epoch: 1954, Batch Gradient Norm: 40.06393509674214
Epoch: 1954, Batch Gradient Norm after: 22.3606745486292
Epoch 1955/10000, Prediction Accuracy = 50.634%, Loss = 1.2430963039398193
Epoch: 1955, Batch Gradient Norm: 45.17430258150061
Epoch: 1955, Batch Gradient Norm after: 22.360678255124665
Epoch 1956/10000, Prediction Accuracy = 50.558%, Loss = 1.2578135967254638
Epoch: 1956, Batch Gradient Norm: 40.068343679571186
Epoch: 1956, Batch Gradient Norm after: 22.360677514250355
Epoch 1957/10000, Prediction Accuracy = 50.634%, Loss = 1.242618727684021
Epoch: 1957, Batch Gradient Norm: 45.19196122336841
Epoch: 1957, Batch Gradient Norm after: 22.360674527706774
Epoch 1958/10000, Prediction Accuracy = 50.574%, Loss = 1.2573750972747804
Epoch: 1958, Batch Gradient Norm: 40.07537180764543
Epoch: 1958, Batch Gradient Norm after: 22.36067655068983
Epoch 1959/10000, Prediction Accuracy = 50.642%, Loss = 1.2421451568603517
Epoch: 1959, Batch Gradient Norm: 45.2121150663958
Epoch: 1959, Batch Gradient Norm after: 22.36067478678413
Epoch 1960/10000, Prediction Accuracy = 50.589999999999996%, Loss = 1.2569573879241944
Epoch: 1960, Batch Gradient Norm: 40.079681110311554
Epoch: 1960, Batch Gradient Norm after: 22.360679666803577
Epoch 1961/10000, Prediction Accuracy = 50.658%, Loss = 1.2416648864746094
Epoch: 1961, Batch Gradient Norm: 45.22770767505445
Epoch: 1961, Batch Gradient Norm after: 22.360677366506835
Epoch 1962/10000, Prediction Accuracy = 50.6%, Loss = 1.2565152883529662
Epoch: 1962, Batch Gradient Norm: 40.079357854019605
Epoch: 1962, Batch Gradient Norm after: 22.360679030380705
Epoch 1963/10000, Prediction Accuracy = 50.662%, Loss = 1.2411795616149903
Epoch: 1963, Batch Gradient Norm: 45.22827187352576
Epoch: 1963, Batch Gradient Norm after: 22.36067678754576
Epoch 1964/10000, Prediction Accuracy = 50.614%, Loss = 1.256027102470398
Epoch: 1964, Batch Gradient Norm: 40.07819005834879
Epoch: 1964, Batch Gradient Norm after: 22.360676603759135
Epoch 1965/10000, Prediction Accuracy = 50.678000000000004%, Loss = 1.2406990766525268
Epoch: 1965, Batch Gradient Norm: 45.23507789674364
Epoch: 1965, Batch Gradient Norm after: 22.360676206651792
Epoch 1966/10000, Prediction Accuracy = 50.617999999999995%, Loss = 1.2555434226989746
Epoch: 1966, Batch Gradient Norm: 40.07838895024556
Epoch: 1966, Batch Gradient Norm after: 22.360674397006154
Epoch 1967/10000, Prediction Accuracy = 50.702%, Loss = 1.2402206182479858
Epoch: 1967, Batch Gradient Norm: 45.23699014653275
Epoch: 1967, Batch Gradient Norm after: 22.360674853331677
Epoch 1968/10000, Prediction Accuracy = 50.636%, Loss = 1.2550584077835083
Epoch: 1968, Batch Gradient Norm: 40.08031235640917
Epoch: 1968, Batch Gradient Norm after: 22.36067644316506
Epoch 1969/10000, Prediction Accuracy = 50.716%, Loss = 1.2397363901138305
Epoch: 1969, Batch Gradient Norm: 45.2420331588791
Epoch: 1969, Batch Gradient Norm after: 22.36067453479098
Epoch 1970/10000, Prediction Accuracy = 50.636%, Loss = 1.2545871496200562
Epoch: 1970, Batch Gradient Norm: 40.080398127553785
Epoch: 1970, Batch Gradient Norm after: 22.360678133608136
Epoch 1971/10000, Prediction Accuracy = 50.718%, Loss = 1.239262056350708
Epoch: 1971, Batch Gradient Norm: 45.254779602126874
Epoch: 1971, Batch Gradient Norm after: 22.360676374133817
Epoch 1972/10000, Prediction Accuracy = 50.656000000000006%, Loss = 1.254140281677246
Epoch: 1972, Batch Gradient Norm: 40.085315498176406
Epoch: 1972, Batch Gradient Norm after: 22.360676228812068
Epoch 1973/10000, Prediction Accuracy = 50.726%, Loss = 1.23878812789917
Epoch: 1973, Batch Gradient Norm: 45.26842186290773
Epoch: 1973, Batch Gradient Norm after: 22.360675637084633
Epoch 1974/10000, Prediction Accuracy = 50.67%, Loss = 1.2536996841430663
Epoch: 1974, Batch Gradient Norm: 40.090360872838446
Epoch: 1974, Batch Gradient Norm after: 22.36067748258168
Epoch 1975/10000, Prediction Accuracy = 50.739999999999995%, Loss = 1.238312554359436
Epoch: 1975, Batch Gradient Norm: 45.276460309175256
Epoch: 1975, Batch Gradient Norm after: 22.36067857852524
Epoch 1976/10000, Prediction Accuracy = 50.67999999999999%, Loss = 1.253235149383545
Epoch: 1976, Batch Gradient Norm: 40.08999185942495
Epoch: 1976, Batch Gradient Norm after: 22.36067762671471
Epoch 1977/10000, Prediction Accuracy = 50.745999999999995%, Loss = 1.2378282070159912
Epoch: 1977, Batch Gradient Norm: 45.28218929675739
Epoch: 1977, Batch Gradient Norm after: 22.360677539269144
Epoch 1978/10000, Prediction Accuracy = 50.684%, Loss = 1.2527696371078492
Epoch: 1978, Batch Gradient Norm: 40.09152687740866
Epoch: 1978, Batch Gradient Norm after: 22.36067821756039
Epoch 1979/10000, Prediction Accuracy = 50.751999999999995%, Loss = 1.2373492956161498
Epoch: 1979, Batch Gradient Norm: 45.2942823897103
Epoch: 1979, Batch Gradient Norm after: 22.36067750720346
Epoch 1980/10000, Prediction Accuracy = 50.7%, Loss = 1.2523227691650392
Epoch: 1980, Batch Gradient Norm: 40.092191983756464
Epoch: 1980, Batch Gradient Norm after: 22.36067747664398
Epoch 1981/10000, Prediction Accuracy = 50.762%, Loss = 1.2368756532669067
Epoch: 1981, Batch Gradient Norm: 45.31786897471434
Epoch: 1981, Batch Gradient Norm after: 22.360675962195312
Epoch 1982/10000, Prediction Accuracy = 50.715999999999994%, Loss = 1.2518999814987182
Epoch: 1982, Batch Gradient Norm: 40.09763501949987
Epoch: 1982, Batch Gradient Norm after: 22.360679810258755
Epoch 1983/10000, Prediction Accuracy = 50.778%, Loss = 1.2364072799682617
Epoch: 1983, Batch Gradient Norm: 45.33781932252629
Epoch: 1983, Batch Gradient Norm after: 22.36067571719752
Epoch 1984/10000, Prediction Accuracy = 50.732%, Loss = 1.2514809131622315
Epoch: 1984, Batch Gradient Norm: 40.09884735709605
Epoch: 1984, Batch Gradient Norm after: 22.360676958312116
Epoch 1985/10000, Prediction Accuracy = 50.784000000000006%, Loss = 1.2359296083450317
Epoch: 1985, Batch Gradient Norm: 45.35550343161025
Epoch: 1985, Batch Gradient Norm after: 22.36067915629954
Epoch 1986/10000, Prediction Accuracy = 50.736000000000004%, Loss = 1.2510506868362428
Epoch: 1986, Batch Gradient Norm: 40.10392719320252
Epoch: 1986, Batch Gradient Norm after: 22.360678821704862
Epoch 1987/10000, Prediction Accuracy = 50.788%, Loss = 1.2354597091674804
Epoch: 1987, Batch Gradient Norm: 45.36805361128628
Epoch: 1987, Batch Gradient Norm after: 22.360673809324684
Epoch 1988/10000, Prediction Accuracy = 50.74400000000001%, Loss = 1.2506213426589965
Epoch: 1988, Batch Gradient Norm: 40.10448899178932
Epoch: 1988, Batch Gradient Norm after: 22.36067686250011
Epoch 1989/10000, Prediction Accuracy = 50.80200000000001%, Loss = 1.2349854946136474
Epoch: 1989, Batch Gradient Norm: 45.37946164798691
Epoch: 1989, Batch Gradient Norm after: 22.360678547937336
Epoch 1990/10000, Prediction Accuracy = 50.74400000000001%, Loss = 1.2501794815063476
Epoch: 1990, Batch Gradient Norm: 40.10557915326692
Epoch: 1990, Batch Gradient Norm after: 22.360675039843912
Epoch 1991/10000, Prediction Accuracy = 50.82%, Loss = 1.234506106376648
Epoch: 1991, Batch Gradient Norm: 45.391381072123515
Epoch: 1991, Batch Gradient Norm after: 22.360679405735212
Epoch 1992/10000, Prediction Accuracy = 50.754000000000005%, Loss = 1.2497338771820068
Epoch: 1992, Batch Gradient Norm: 40.106451733505374
Epoch: 1992, Batch Gradient Norm after: 22.360676519939354
Epoch 1993/10000, Prediction Accuracy = 50.83200000000001%, Loss = 1.234034252166748
Epoch: 1993, Batch Gradient Norm: 45.39580919674378
Epoch: 1993, Batch Gradient Norm after: 22.36068058803911
Epoch 1994/10000, Prediction Accuracy = 50.766%, Loss = 1.2492639303207398
Epoch: 1994, Batch Gradient Norm: 40.106128039451775
Epoch: 1994, Batch Gradient Norm after: 22.360677085232556
Epoch 1995/10000, Prediction Accuracy = 50.852000000000004%, Loss = 1.2335514783859254
Epoch: 1995, Batch Gradient Norm: 45.39808147915464
Epoch: 1995, Batch Gradient Norm after: 22.360678933160905
Epoch 1996/10000, Prediction Accuracy = 50.790000000000006%, Loss = 1.248790192604065
Epoch: 1996, Batch Gradient Norm: 40.105335284794265
Epoch: 1996, Batch Gradient Norm after: 22.36067735285631
Epoch 1997/10000, Prediction Accuracy = 50.856%, Loss = 1.2330779314041138
Epoch: 1997, Batch Gradient Norm: 45.402888604141076
Epoch: 1997, Batch Gradient Norm after: 22.360675599065463
Epoch 1998/10000, Prediction Accuracy = 50.808%, Loss = 1.2483266830444335
Epoch: 1998, Batch Gradient Norm: 40.105087620974984
Epoch: 1998, Batch Gradient Norm after: 22.360677119382423
Epoch 1999/10000, Prediction Accuracy = 50.873999999999995%, Loss = 1.2326042175292968
Epoch: 1999, Batch Gradient Norm: 45.412927950278245
Epoch: 1999, Batch Gradient Norm after: 22.360678250187515
Epoch 2000/10000, Prediction Accuracy = 50.81%, Loss = 1.2478821754455567
Epoch: 2000, Batch Gradient Norm: 40.103412380449726
Epoch: 2000, Batch Gradient Norm after: 22.360675053558502
Epoch 2001/10000, Prediction Accuracy = 50.876%, Loss = 1.2321273326873778
Epoch: 2001, Batch Gradient Norm: 45.42497543766754
Epoch: 2001, Batch Gradient Norm after: 22.360676923558742
Epoch 2002/10000, Prediction Accuracy = 50.82%, Loss = 1.2474432229995727
Epoch: 2002, Batch Gradient Norm: 40.1078154274569
Epoch: 2002, Batch Gradient Norm after: 22.36067598314553
Epoch 2003/10000, Prediction Accuracy = 50.886%, Loss = 1.2316678047180176
Epoch: 2003, Batch Gradient Norm: 45.4517198635835
Epoch: 2003, Batch Gradient Norm after: 22.36067714082535
Epoch 2004/10000, Prediction Accuracy = 50.838%, Loss = 1.2470541715621948
Epoch: 2004, Batch Gradient Norm: 40.11573774357778
Epoch: 2004, Batch Gradient Norm after: 22.360677494433162
Epoch 2005/10000, Prediction Accuracy = 50.891999999999996%, Loss = 1.2312048196792602
Epoch: 2005, Batch Gradient Norm: 45.47752066240905
Epoch: 2005, Batch Gradient Norm after: 22.360678618794573
Epoch 2006/10000, Prediction Accuracy = 50.848%, Loss = 1.2466493606567384
Epoch: 2006, Batch Gradient Norm: 40.120281631393674
Epoch: 2006, Batch Gradient Norm after: 22.360677414714257
Epoch 2007/10000, Prediction Accuracy = 50.90599999999999%, Loss = 1.230745553970337
Epoch: 2007, Batch Gradient Norm: 45.50471392623859
Epoch: 2007, Batch Gradient Norm after: 22.360678975767705
Epoch 2008/10000, Prediction Accuracy = 50.85%, Loss = 1.2462474584579468
Epoch: 2008, Batch Gradient Norm: 40.126928328987354
Epoch: 2008, Batch Gradient Norm after: 22.360678471015884
Epoch 2009/10000, Prediction Accuracy = 50.91799999999999%, Loss = 1.2302863836288451
Epoch: 2009, Batch Gradient Norm: 45.539768757284655
Epoch: 2009, Batch Gradient Norm after: 22.360677544153827
Epoch 2010/10000, Prediction Accuracy = 50.862%, Loss = 1.2458669662475585
Epoch: 2010, Batch Gradient Norm: 40.134276864403795
Epoch: 2010, Batch Gradient Norm after: 22.360680057713203
Epoch 2011/10000, Prediction Accuracy = 50.92%, Loss = 1.229828953742981
Epoch: 2011, Batch Gradient Norm: 45.56579295563722
Epoch: 2011, Batch Gradient Norm after: 22.360678838473472
Epoch 2012/10000, Prediction Accuracy = 50.878%, Loss = 1.245477056503296
Epoch: 2012, Batch Gradient Norm: 40.139102933521215
Epoch: 2012, Batch Gradient Norm after: 22.360675947427932
Epoch 2013/10000, Prediction Accuracy = 50.932%, Loss = 1.2293705701828004
Epoch: 2013, Batch Gradient Norm: 45.59165860565318
Epoch: 2013, Batch Gradient Norm after: 22.36067757467059
Epoch 2014/10000, Prediction Accuracy = 50.9%, Loss = 1.245064115524292
Epoch: 2014, Batch Gradient Norm: 40.14514922122704
Epoch: 2014, Batch Gradient Norm after: 22.360677492063513
Epoch 2015/10000, Prediction Accuracy = 50.934000000000005%, Loss = 1.2289178371429443
Epoch: 2015, Batch Gradient Norm: 45.620771883513484
Epoch: 2015, Batch Gradient Norm after: 22.36067847318954
Epoch 2016/10000, Prediction Accuracy = 50.912%, Loss = 1.2446767091751099
Epoch: 2016, Batch Gradient Norm: 40.15185589059909
Epoch: 2016, Batch Gradient Norm after: 22.360676930143832
Epoch 2017/10000, Prediction Accuracy = 50.946000000000005%, Loss = 1.2284593820571899
Epoch: 2017, Batch Gradient Norm: 45.648037217298366
Epoch: 2017, Batch Gradient Norm after: 22.360679001765188
Epoch 2018/10000, Prediction Accuracy = 50.918%, Loss = 1.2442842960357665
Epoch: 2018, Batch Gradient Norm: 40.156860673600455
Epoch: 2018, Batch Gradient Norm after: 22.3606779042568
Epoch 2019/10000, Prediction Accuracy = 50.966%, Loss = 1.2279914140701294
Epoch: 2019, Batch Gradient Norm: 45.66518638179441
Epoch: 2019, Batch Gradient Norm after: 22.36067778422405
Epoch 2020/10000, Prediction Accuracy = 50.937999999999995%, Loss = 1.2438597202301025
Epoch: 2020, Batch Gradient Norm: 40.163428214064
Epoch: 2020, Batch Gradient Norm after: 22.360676922341607
Epoch 2021/10000, Prediction Accuracy = 50.965999999999994%, Loss = 1.2275367021560668
Epoch: 2021, Batch Gradient Norm: 45.69027949236795
Epoch: 2021, Batch Gradient Norm after: 22.36067730162632
Epoch 2022/10000, Prediction Accuracy = 50.952%, Loss = 1.2434583902359009
Epoch: 2022, Batch Gradient Norm: 40.170857258613204
Epoch: 2022, Batch Gradient Norm after: 22.36067586052659
Epoch 2023/10000, Prediction Accuracy = 50.97599999999999%, Loss = 1.2270785570144653
Epoch: 2023, Batch Gradient Norm: 45.71871574831364
Epoch: 2023, Batch Gradient Norm after: 22.360679824896994
Epoch 2024/10000, Prediction Accuracy = 50.96%, Loss = 1.2430723428726196
Epoch: 2024, Batch Gradient Norm: 40.177906920197074
Epoch: 2024, Batch Gradient Norm after: 22.36067528193284
Epoch 2025/10000, Prediction Accuracy = 50.98%, Loss = 1.2266199588775635
Epoch: 2025, Batch Gradient Norm: 45.750917954322674
Epoch: 2025, Batch Gradient Norm after: 22.360677994393757
Epoch 2026/10000, Prediction Accuracy = 50.972%, Loss = 1.2426947832107544
Epoch: 2026, Batch Gradient Norm: 40.18640693327868
Epoch: 2026, Batch Gradient Norm after: 22.360677698535824
Epoch 2027/10000, Prediction Accuracy = 50.982%, Loss = 1.2261606454849243
Epoch: 2027, Batch Gradient Norm: 45.784315469125175
Epoch: 2027, Batch Gradient Norm after: 22.360679512460305
Epoch 2028/10000, Prediction Accuracy = 50.986000000000004%, Loss = 1.2423234462738038
Epoch: 2028, Batch Gradient Norm: 40.20011865261007
Epoch: 2028, Batch Gradient Norm after: 22.360676269894018
Epoch 2029/10000, Prediction Accuracy = 51.0%, Loss = 1.2257261514663695
Epoch: 2029, Batch Gradient Norm: 45.82354822215989
Epoch: 2029, Batch Gradient Norm after: 22.360679300822532
Epoch 2030/10000, Prediction Accuracy = 50.998000000000005%, Loss = 1.2419660806655883
Epoch: 2030, Batch Gradient Norm: 40.20651418967061
Epoch: 2030, Batch Gradient Norm after: 22.360675562909556
Epoch 2031/10000, Prediction Accuracy = 51.001999999999995%, Loss = 1.2252740144729615
Epoch: 2031, Batch Gradient Norm: 45.857635099081136
Epoch: 2031, Batch Gradient Norm after: 22.36067716888561
Epoch 2032/10000, Prediction Accuracy = 51.001999999999995%, Loss = 1.2416006088256837
Epoch: 2032, Batch Gradient Norm: 40.21801632746933
Epoch: 2032, Batch Gradient Norm after: 22.360676876430517
Epoch 2033/10000, Prediction Accuracy = 51.002%, Loss = 1.224830722808838
Epoch: 2033, Batch Gradient Norm: 45.89183402579973
Epoch: 2033, Batch Gradient Norm after: 22.36067838534921
Epoch 2034/10000, Prediction Accuracy = 51.01200000000001%, Loss = 1.2412333250045777
Epoch: 2034, Batch Gradient Norm: 40.222353749575205
Epoch: 2034, Batch Gradient Norm after: 22.360675104771698
Epoch 2035/10000, Prediction Accuracy = 51.024%, Loss = 1.224368405342102
Epoch: 2035, Batch Gradient Norm: 45.9181543988273
Epoch: 2035, Batch Gradient Norm after: 22.360678476763557
Epoch 2036/10000, Prediction Accuracy = 51.032%, Loss = 1.2408313512802125
Epoch: 2036, Batch Gradient Norm: 40.23155867797553
Epoch: 2036, Batch Gradient Norm after: 22.360679171394555
Epoch 2037/10000, Prediction Accuracy = 51.042%, Loss = 1.223922848701477
Epoch: 2037, Batch Gradient Norm: 45.95566616764217
Epoch: 2037, Batch Gradient Norm after: 22.360679445586545
Epoch 2038/10000, Prediction Accuracy = 51.05%, Loss = 1.2404672622680664
Epoch: 2038, Batch Gradient Norm: 40.24120413298327
Epoch: 2038, Batch Gradient Norm after: 22.360677799776283
Epoch 2039/10000, Prediction Accuracy = 51.048%, Loss = 1.223465871810913
Epoch: 2039, Batch Gradient Norm: 45.98627140788917
Epoch: 2039, Batch Gradient Norm after: 22.36067942318827
Epoch 2040/10000, Prediction Accuracy = 51.06400000000001%, Loss = 1.2400813102722168
Epoch: 2040, Batch Gradient Norm: 40.24559272591488
Epoch: 2040, Batch Gradient Norm after: 22.360676895073592
Epoch 2041/10000, Prediction Accuracy = 51.05%, Loss = 1.2229915618896485
Epoch: 2041, Batch Gradient Norm: 46.004383354876694
Epoch: 2041, Batch Gradient Norm after: 22.36067920548628
Epoch 2042/10000, Prediction Accuracy = 51.071999999999996%, Loss = 1.2396690845489502
Epoch: 2042, Batch Gradient Norm: 40.25350907709752
Epoch: 2042, Batch Gradient Norm after: 22.360676021150237
Epoch 2043/10000, Prediction Accuracy = 51.07%, Loss = 1.2225411891937257
Epoch: 2043, Batch Gradient Norm: 46.02402179411538
Epoch: 2043, Batch Gradient Norm after: 22.360678642837428
Epoch 2044/10000, Prediction Accuracy = 51.084%, Loss = 1.2392402172088623
Epoch: 2044, Batch Gradient Norm: 40.25930946697467
Epoch: 2044, Batch Gradient Norm after: 22.36067871010924
Epoch 2045/10000, Prediction Accuracy = 51.07%, Loss = 1.222081184387207
Epoch: 2045, Batch Gradient Norm: 46.035259853107696
Epoch: 2045, Batch Gradient Norm after: 22.36067722250682
Epoch 2046/10000, Prediction Accuracy = 51.06999999999999%, Loss = 1.238798689842224
Epoch: 2046, Batch Gradient Norm: 40.26199737222085
Epoch: 2046, Batch Gradient Norm after: 22.36067785418567
Epoch 2047/10000, Prediction Accuracy = 51.066%, Loss = 1.2216171264648437
Epoch: 2047, Batch Gradient Norm: 46.05246180981002
Epoch: 2047, Batch Gradient Norm after: 22.36067776143259
Epoch 2048/10000, Prediction Accuracy = 51.089999999999996%, Loss = 1.238368034362793
Epoch: 2048, Batch Gradient Norm: 40.265334156777676
Epoch: 2048, Batch Gradient Norm after: 22.360677576920036
Epoch 2049/10000, Prediction Accuracy = 51.086%, Loss = 1.22115318775177
Epoch: 2049, Batch Gradient Norm: 46.06803731689561
Epoch: 2049, Batch Gradient Norm after: 22.360679224596417
Epoch 2050/10000, Prediction Accuracy = 51.102%, Loss = 1.2379353284835815
Epoch: 2050, Batch Gradient Norm: 40.26566806204447
Epoch: 2050, Batch Gradient Norm after: 22.36067720618007
Epoch 2051/10000, Prediction Accuracy = 51.092%, Loss = 1.2206795930862426
Epoch: 2051, Batch Gradient Norm: 46.062415697919214
Epoch: 2051, Batch Gradient Norm after: 22.360679699591145
Epoch 2052/10000, Prediction Accuracy = 51.11%, Loss = 1.237446403503418
Epoch: 2052, Batch Gradient Norm: 40.264313696241494
Epoch: 2052, Batch Gradient Norm after: 22.360675858137107
Epoch 2053/10000, Prediction Accuracy = 51.12%, Loss = 1.2201979398727416
Epoch: 2053, Batch Gradient Norm: 46.04603614195167
Epoch: 2053, Batch Gradient Norm after: 22.360679028904944
Epoch 2054/10000, Prediction Accuracy = 51.129999999999995%, Loss = 1.2369150400161744
Epoch: 2054, Batch Gradient Norm: 40.26027627973772
Epoch: 2054, Batch Gradient Norm after: 22.360676838981888
Epoch 2055/10000, Prediction Accuracy = 51.13199999999999%, Loss = 1.2197112083435058
Epoch: 2055, Batch Gradient Norm: 46.029233574436454
Epoch: 2055, Batch Gradient Norm after: 22.36067867512444
Epoch 2056/10000, Prediction Accuracy = 51.144%, Loss = 1.2363864660263062
Epoch: 2056, Batch Gradient Norm: 40.25673687371742
Epoch: 2056, Batch Gradient Norm after: 22.360677226012953
Epoch 2057/10000, Prediction Accuracy = 51.15%, Loss = 1.2192406892776488
Epoch: 2057, Batch Gradient Norm: 46.03062029210219
Epoch: 2057, Batch Gradient Norm after: 22.360678522612517
Epoch 2058/10000, Prediction Accuracy = 51.154%, Loss = 1.235920524597168
Epoch: 2058, Batch Gradient Norm: 40.25860562761343
Epoch: 2058, Batch Gradient Norm after: 22.360677349298744
Epoch 2059/10000, Prediction Accuracy = 51.154%, Loss = 1.2187703847885132
Epoch: 2059, Batch Gradient Norm: 46.02246372041763
Epoch: 2059, Batch Gradient Norm after: 22.36067595740762
Epoch 2060/10000, Prediction Accuracy = 51.154%, Loss = 1.2354199886322021
Epoch: 2060, Batch Gradient Norm: 40.255139314916775
Epoch: 2060, Batch Gradient Norm after: 22.360676617168902
Epoch 2061/10000, Prediction Accuracy = 51.157999999999994%, Loss = 1.218299961090088
Epoch: 2061, Batch Gradient Norm: 46.01555016124076
Epoch: 2061, Batch Gradient Norm after: 22.360677373045913
Epoch 2062/10000, Prediction Accuracy = 51.19199999999999%, Loss = 1.2349211931228639
Epoch: 2062, Batch Gradient Norm: 40.25537593726641
Epoch: 2062, Batch Gradient Norm after: 22.36067736887874
Epoch 2063/10000, Prediction Accuracy = 51.166%, Loss = 1.2178304195404053
Epoch: 2063, Batch Gradient Norm: 46.00792333859376
Epoch: 2063, Batch Gradient Norm after: 22.36067674430312
Epoch 2064/10000, Prediction Accuracy = 51.198%, Loss = 1.234427571296692
Epoch: 2064, Batch Gradient Norm: 40.25440329688579
Epoch: 2064, Batch Gradient Norm after: 22.36067765800907
Epoch 2065/10000, Prediction Accuracy = 51.184000000000005%, Loss = 1.2173571586608887
Epoch: 2065, Batch Gradient Norm: 45.99992866106634
Epoch: 2065, Batch Gradient Norm after: 22.360675417107267
Epoch 2066/10000, Prediction Accuracy = 51.214%, Loss = 1.2339344501495362
Epoch: 2066, Batch Gradient Norm: 40.25263996507884
Epoch: 2066, Batch Gradient Norm after: 22.36067770400806
Epoch 2067/10000, Prediction Accuracy = 51.188%, Loss = 1.2168818235397338
Epoch: 2067, Batch Gradient Norm: 45.986981015395024
Epoch: 2067, Batch Gradient Norm after: 22.360678969402166
Epoch 2068/10000, Prediction Accuracy = 51.202%, Loss = 1.2334359884262085
Epoch: 2068, Batch Gradient Norm: 40.246702341994705
Epoch: 2068, Batch Gradient Norm after: 22.36067604571384
Epoch 2069/10000, Prediction Accuracy = 51.205999999999996%, Loss = 1.2163939952850342
Epoch: 2069, Batch Gradient Norm: 45.97450066273967
Epoch: 2069, Batch Gradient Norm after: 22.360676954893563
Epoch 2070/10000, Prediction Accuracy = 51.22399999999999%, Loss = 1.232919406890869
Epoch: 2070, Batch Gradient Norm: 40.240973246125364
Epoch: 2070, Batch Gradient Norm after: 22.360676037169597
Epoch 2071/10000, Prediction Accuracy = 51.21200000000001%, Loss = 1.2159181833267212
Epoch: 2071, Batch Gradient Norm: 45.95500390772877
Epoch: 2071, Batch Gradient Norm after: 22.360676196462254
Epoch 2072/10000, Prediction Accuracy = 51.23599999999999%, Loss = 1.232393479347229
Epoch: 2072, Batch Gradient Norm: 40.23353444359972
Epoch: 2072, Batch Gradient Norm after: 22.36067796357566
Epoch 2073/10000, Prediction Accuracy = 51.222%, Loss = 1.2154343843460083
Epoch: 2073, Batch Gradient Norm: 45.94026580687899
Epoch: 2073, Batch Gradient Norm after: 22.36067940422203
Epoch 2074/10000, Prediction Accuracy = 51.254%, Loss = 1.2318838357925415
Epoch: 2074, Batch Gradient Norm: 40.22946426566413
Epoch: 2074, Batch Gradient Norm after: 22.360679722416
Epoch 2075/10000, Prediction Accuracy = 51.227999999999994%, Loss = 1.2149558067321777
Epoch: 2075, Batch Gradient Norm: 45.926775851539915
Epoch: 2075, Batch Gradient Norm after: 22.360679210469513
Epoch 2076/10000, Prediction Accuracy = 51.265999999999984%, Loss = 1.2313719749450684
Epoch: 2076, Batch Gradient Norm: 40.22662882475453
Epoch: 2076, Batch Gradient Norm after: 22.360677801543062
Epoch 2077/10000, Prediction Accuracy = 51.227999999999994%, Loss = 1.2144854545593262
Epoch: 2077, Batch Gradient Norm: 45.909074351603266
Epoch: 2077, Batch Gradient Norm after: 22.360678989565503
Epoch 2078/10000, Prediction Accuracy = 51.274%, Loss = 1.230851650238037
Epoch: 2078, Batch Gradient Norm: 40.22115816676997
Epoch: 2078, Batch Gradient Norm after: 22.360677824231715
Epoch 2079/10000, Prediction Accuracy = 51.24400000000001%, Loss = 1.2140079259872436
Epoch: 2079, Batch Gradient Norm: 45.88694294723086
Epoch: 2079, Batch Gradient Norm after: 22.360678459028478
Epoch 2080/10000, Prediction Accuracy = 51.278%, Loss = 1.2303243398666381
Epoch: 2080, Batch Gradient Norm: 40.215110215838635
Epoch: 2080, Batch Gradient Norm after: 22.360679231266356
Epoch 2081/10000, Prediction Accuracy = 51.25599999999999%, Loss = 1.2135363340377807
Epoch: 2081, Batch Gradient Norm: 45.8710423438344
Epoch: 2081, Batch Gradient Norm after: 22.36067757232975
Epoch 2082/10000, Prediction Accuracy = 51.298%, Loss = 1.2298154354095459
Epoch: 2082, Batch Gradient Norm: 40.21187587399566
Epoch: 2082, Batch Gradient Norm after: 22.36067734298252
Epoch 2083/10000, Prediction Accuracy = 51.260000000000005%, Loss = 1.2130730628967286
Epoch: 2083, Batch Gradient Norm: 45.86738553917257
Epoch: 2083, Batch Gradient Norm after: 22.36067776089473
Epoch 2084/10000, Prediction Accuracy = 51.291999999999994%, Loss = 1.2293404579162597
Epoch: 2084, Batch Gradient Norm: 40.21327625122386
Epoch: 2084, Batch Gradient Norm after: 22.36067654912365
Epoch 2085/10000, Prediction Accuracy = 51.279999999999994%, Loss = 1.2126145839691163
Epoch: 2085, Batch Gradient Norm: 45.86506268876261
Epoch: 2085, Batch Gradient Norm after: 22.360674900023803
Epoch 2086/10000, Prediction Accuracy = 51.284000000000006%, Loss = 1.2288738489151
Epoch: 2086, Batch Gradient Norm: 40.211404660159864
Epoch: 2086, Batch Gradient Norm after: 22.360679732741666
Epoch 2087/10000, Prediction Accuracy = 51.3%, Loss = 1.212155556678772
Epoch: 2087, Batch Gradient Norm: 45.86284607478644
Epoch: 2087, Batch Gradient Norm after: 22.360677221493976
Epoch 2088/10000, Prediction Accuracy = 51.306%, Loss = 1.2284115076065063
Epoch: 2088, Batch Gradient Norm: 40.20640163064153
Epoch: 2088, Batch Gradient Norm after: 22.36067611855337
Epoch 2089/10000, Prediction Accuracy = 51.314%, Loss = 1.2116946220397948
Epoch: 2089, Batch Gradient Norm: 45.854109685188256
Epoch: 2089, Batch Gradient Norm after: 22.360675508368722
Epoch 2090/10000, Prediction Accuracy = 51.32000000000001%, Loss = 1.2279403924942016
Epoch: 2090, Batch Gradient Norm: 40.20066675571471
Epoch: 2090, Batch Gradient Norm after: 22.360676917603644
Epoch 2091/10000, Prediction Accuracy = 51.322%, Loss = 1.211220645904541
Epoch: 2091, Batch Gradient Norm: 45.84020273680104
Epoch: 2091, Batch Gradient Norm after: 22.360675152136192
Epoch 2092/10000, Prediction Accuracy = 51.33399999999999%, Loss = 1.227444314956665
Epoch: 2092, Batch Gradient Norm: 40.195691136853746
Epoch: 2092, Batch Gradient Norm after: 22.36067776596754
Epoch 2093/10000, Prediction Accuracy = 51.324%, Loss = 1.2107528686523437
Epoch: 2093, Batch Gradient Norm: 45.833199850236205
Epoch: 2093, Batch Gradient Norm after: 22.36067680731588
Epoch 2094/10000, Prediction Accuracy = 51.334%, Loss = 1.2269619226455688
Epoch: 2094, Batch Gradient Norm: 40.19605137566204
Epoch: 2094, Batch Gradient Norm after: 22.36067622640316
Epoch 2095/10000, Prediction Accuracy = 51.338%, Loss = 1.2103099346160888
Epoch: 2095, Batch Gradient Norm: 45.842372653111184
Epoch: 2095, Batch Gradient Norm after: 22.360677897703802
Epoch 2096/10000, Prediction Accuracy = 51.352%, Loss = 1.2265405416488648
Epoch: 2096, Batch Gradient Norm: 40.194193927712696
Epoch: 2096, Batch Gradient Norm after: 22.360676696579617
Epoch 2097/10000, Prediction Accuracy = 51.354%, Loss = 1.2098581552505494
Epoch: 2097, Batch Gradient Norm: 45.849747391482
Epoch: 2097, Batch Gradient Norm after: 22.360677148202633
Epoch 2098/10000, Prediction Accuracy = 51.354%, Loss = 1.226105761528015
Epoch: 2098, Batch Gradient Norm: 40.193637679488816
Epoch: 2098, Batch Gradient Norm after: 22.360676732496916
Epoch 2099/10000, Prediction Accuracy = 51.35600000000001%, Loss = 1.2094127655029296
Epoch: 2099, Batch Gradient Norm: 45.85153186493698
Epoch: 2099, Batch Gradient Norm after: 22.360679076968395
Epoch 2100/10000, Prediction Accuracy = 51.352%, Loss = 1.2256558895111085
Epoch: 2100, Batch Gradient Norm: 40.19271381255187
Epoch: 2100, Batch Gradient Norm after: 22.360677637940867
Epoch 2101/10000, Prediction Accuracy = 51.372%, Loss = 1.208956527709961
Epoch: 2101, Batch Gradient Norm: 45.84899199671958
Epoch: 2101, Batch Gradient Norm after: 22.36067618701321
Epoch 2102/10000, Prediction Accuracy = 51.355999999999995%, Loss = 1.2251906394958496
Epoch: 2102, Batch Gradient Norm: 40.19108114910901
Epoch: 2102, Batch Gradient Norm after: 22.360677736358923
Epoch 2103/10000, Prediction Accuracy = 51.388%, Loss = 1.2084951162338258
Epoch: 2103, Batch Gradient Norm: 45.846618730799825
Epoch: 2103, Batch Gradient Norm after: 22.36067874512847
Epoch 2104/10000, Prediction Accuracy = 51.372%, Loss = 1.2247281789779663
Epoch: 2104, Batch Gradient Norm: 40.192389500265755
Epoch: 2104, Batch Gradient Norm after: 22.360678826424063
Epoch 2105/10000, Prediction Accuracy = 51.40599999999999%, Loss = 1.2080551147460938
Epoch: 2105, Batch Gradient Norm: 45.848907742043814
Epoch: 2105, Batch Gradient Norm after: 22.360677724745642
Epoch 2106/10000, Prediction Accuracy = 51.38199999999999%, Loss = 1.2242817163467408
Epoch: 2106, Batch Gradient Norm: 40.18967012921547
Epoch: 2106, Batch Gradient Norm after: 22.360675951989297
Epoch 2107/10000, Prediction Accuracy = 51.40599999999999%, Loss = 1.2076010704040527
Epoch: 2107, Batch Gradient Norm: 45.8468892886679
Epoch: 2107, Batch Gradient Norm after: 22.360675895141696
Epoch 2108/10000, Prediction Accuracy = 51.394000000000005%, Loss = 1.2238265752792359
Epoch: 2108, Batch Gradient Norm: 40.189573238698706
Epoch: 2108, Batch Gradient Norm after: 22.36067691289147
Epoch 2109/10000, Prediction Accuracy = 51.424%, Loss = 1.2071519851684571
Epoch: 2109, Batch Gradient Norm: 45.84700032980565
Epoch: 2109, Batch Gradient Norm after: 22.360676659814715
Epoch 2110/10000, Prediction Accuracy = 51.4%, Loss = 1.2233808279037475
Epoch: 2110, Batch Gradient Norm: 40.18646987093822
Epoch: 2110, Batch Gradient Norm after: 22.360676430236417
Epoch 2111/10000, Prediction Accuracy = 51.42999999999999%, Loss = 1.206706166267395
Epoch: 2111, Batch Gradient Norm: 45.84656839798319
Epoch: 2111, Batch Gradient Norm after: 22.360677210615066
Epoch 2112/10000, Prediction Accuracy = 51.394000000000005%, Loss = 1.2229291915893554
Epoch: 2112, Batch Gradient Norm: 40.18574062235608
Epoch: 2112, Batch Gradient Norm after: 22.36067588192312
Epoch 2113/10000, Prediction Accuracy = 51.44200000000001%, Loss = 1.2062545537948608
Epoch: 2113, Batch Gradient Norm: 45.838462760388396
Epoch: 2113, Batch Gradient Norm after: 22.36067494843531
Epoch 2114/10000, Prediction Accuracy = 51.40400000000001%, Loss = 1.2224667072296143
Epoch: 2114, Batch Gradient Norm: 40.182497604303066
Epoch: 2114, Batch Gradient Norm after: 22.36067697485805
Epoch 2115/10000, Prediction Accuracy = 51.444%, Loss = 1.2058047771453857
Epoch: 2115, Batch Gradient Norm: 45.82882884765773
Epoch: 2115, Batch Gradient Norm after: 22.36067831718465
Epoch 2116/10000, Prediction Accuracy = 51.412%, Loss = 1.2219964027404786
Epoch: 2116, Batch Gradient Norm: 40.17445403319751
Epoch: 2116, Batch Gradient Norm after: 22.360678006309737
Epoch 2117/10000, Prediction Accuracy = 51.446000000000005%, Loss = 1.2053480863571167
Epoch: 2117, Batch Gradient Norm: 45.814837200946805
Epoch: 2117, Batch Gradient Norm after: 22.360676545561933
Epoch 2118/10000, Prediction Accuracy = 51.41799999999999%, Loss = 1.2215081930160523
Epoch: 2118, Batch Gradient Norm: 40.16999297439904
Epoch: 2118, Batch Gradient Norm after: 22.360676567439683
Epoch 2119/10000, Prediction Accuracy = 51.455999999999996%, Loss = 1.2048943519592286
Epoch: 2119, Batch Gradient Norm: 45.80819123549257
Epoch: 2119, Batch Gradient Norm after: 22.36067564932267
Epoch 2120/10000, Prediction Accuracy = 51.428%, Loss = 1.2210484743118286
Epoch: 2120, Batch Gradient Norm: 40.16456437273752
Epoch: 2120, Batch Gradient Norm after: 22.360677638095783
Epoch 2121/10000, Prediction Accuracy = 51.472%, Loss = 1.204433512687683
Epoch: 2121, Batch Gradient Norm: 45.793108218846854
Epoch: 2121, Batch Gradient Norm after: 22.36067761697685
Epoch 2122/10000, Prediction Accuracy = 51.44199999999999%, Loss = 1.2205496788024903
Epoch: 2122, Batch Gradient Norm: 40.16156595256333
Epoch: 2122, Batch Gradient Norm after: 22.360678615920506
Epoch 2123/10000, Prediction Accuracy = 51.477999999999994%, Loss = 1.2039769887924194
Epoch: 2123, Batch Gradient Norm: 45.78993118620013
Epoch: 2123, Batch Gradient Norm after: 22.3606782583212
Epoch 2124/10000, Prediction Accuracy = 51.446000000000005%, Loss = 1.2200937747955323
Epoch: 2124, Batch Gradient Norm: 40.162371992480615
Epoch: 2124, Batch Gradient Norm after: 22.36067603198368
Epoch 2125/10000, Prediction Accuracy = 51.48199999999999%, Loss = 1.2035353183746338
Epoch: 2125, Batch Gradient Norm: 45.79146842625727
Epoch: 2125, Batch Gradient Norm after: 22.36067654492185
Epoch 2126/10000, Prediction Accuracy = 51.458000000000006%, Loss = 1.2196513891220093
Epoch: 2126, Batch Gradient Norm: 40.15922155471978
Epoch: 2126, Batch Gradient Norm after: 22.3606750397175
Epoch 2127/10000, Prediction Accuracy = 51.504%, Loss = 1.2030957698822022
Epoch: 2127, Batch Gradient Norm: 45.78863263259506
Epoch: 2127, Batch Gradient Norm after: 22.36067789989319
Epoch 2128/10000, Prediction Accuracy = 51.46%, Loss = 1.2192023992538452
Epoch: 2128, Batch Gradient Norm: 40.15777590776192
Epoch: 2128, Batch Gradient Norm after: 22.360676431399167
Epoch 2129/10000, Prediction Accuracy = 51.512%, Loss = 1.2026522397994994
Epoch: 2129, Batch Gradient Norm: 45.783291758348874
Epoch: 2129, Batch Gradient Norm after: 22.360678395326072
Epoch 2130/10000, Prediction Accuracy = 51.472%, Loss = 1.2187384605407714
Epoch: 2130, Batch Gradient Norm: 40.15412622642182
Epoch: 2130, Batch Gradient Norm after: 22.36067692790985
Epoch 2131/10000, Prediction Accuracy = 51.524%, Loss = 1.2022042512893676
Epoch: 2131, Batch Gradient Norm: 45.77308268361597
Epoch: 2131, Batch Gradient Norm after: 22.36067750966127
Epoch 2132/10000, Prediction Accuracy = 51.484%, Loss = 1.2182695627212525
Epoch: 2132, Batch Gradient Norm: 40.1477006683333
Epoch: 2132, Batch Gradient Norm after: 22.360674589620476
Epoch 2133/10000, Prediction Accuracy = 51.538%, Loss = 1.2017600774765014
Epoch: 2133, Batch Gradient Norm: 45.76310869711422
Epoch: 2133, Batch Gradient Norm after: 22.36067808140576
Epoch 2134/10000, Prediction Accuracy = 51.492000000000004%, Loss = 1.2178030967712403
Epoch: 2134, Batch Gradient Norm: 40.14314731665271
Epoch: 2134, Batch Gradient Norm after: 22.36067782277928
Epoch 2135/10000, Prediction Accuracy = 51.536%, Loss = 1.2013142347335815
Epoch: 2135, Batch Gradient Norm: 45.763474547965416
Epoch: 2135, Batch Gradient Norm after: 22.36067845951158
Epoch 2136/10000, Prediction Accuracy = 51.510000000000005%, Loss = 1.2173734903335571
Epoch: 2136, Batch Gradient Norm: 40.13866380244869
Epoch: 2136, Batch Gradient Norm after: 22.360678740537455
Epoch 2137/10000, Prediction Accuracy = 51.544%, Loss = 1.2008731603622436
Epoch: 2137, Batch Gradient Norm: 45.75552606570212
Epoch: 2137, Batch Gradient Norm after: 22.360676515289217
Epoch 2138/10000, Prediction Accuracy = 51.512%, Loss = 1.2169076919555664
Epoch: 2138, Batch Gradient Norm: 40.13160765222921
Epoch: 2138, Batch Gradient Norm after: 22.36067496955388
Epoch 2139/10000, Prediction Accuracy = 51.548%, Loss = 1.2004146337509156
Epoch: 2139, Batch Gradient Norm: 45.75245368035375
Epoch: 2139, Batch Gradient Norm after: 22.36067638744613
Epoch 2140/10000, Prediction Accuracy = 51.52%, Loss = 1.2164681673049926
Epoch: 2140, Batch Gradient Norm: 40.128146301784234
Epoch: 2140, Batch Gradient Norm after: 22.360676356942093
Epoch 2141/10000, Prediction Accuracy = 51.556000000000004%, Loss = 1.1999709129333496
Epoch: 2141, Batch Gradient Norm: 45.75442010963906
Epoch: 2141, Batch Gradient Norm after: 22.360677129211908
Epoch 2142/10000, Prediction Accuracy = 51.541999999999994%, Loss = 1.2160335302352905
Epoch: 2142, Batch Gradient Norm: 40.12587977599021
Epoch: 2142, Batch Gradient Norm after: 22.360674461194932
Epoch 2143/10000, Prediction Accuracy = 51.564%, Loss = 1.1995317697525025
Epoch: 2143, Batch Gradient Norm: 45.75889629380452
Epoch: 2143, Batch Gradient Norm after: 22.360677854145983
Epoch 2144/10000, Prediction Accuracy = 51.565999999999995%, Loss = 1.2156168460845946
Epoch: 2144, Batch Gradient Norm: 40.124605522292526
Epoch: 2144, Batch Gradient Norm after: 22.36067652249103
Epoch 2145/10000, Prediction Accuracy = 51.572%, Loss = 1.1990893363952637
Epoch: 2145, Batch Gradient Norm: 45.7556721441738
Epoch: 2145, Batch Gradient Norm after: 22.360677274983516
Epoch 2146/10000, Prediction Accuracy = 51.58%, Loss = 1.2151807785034179
Epoch: 2146, Batch Gradient Norm: 40.119412491261734
Epoch: 2146, Batch Gradient Norm after: 22.360677096182187
Epoch 2147/10000, Prediction Accuracy = 51.58200000000001%, Loss = 1.1986342430114747
Epoch: 2147, Batch Gradient Norm: 45.73892296755586
Epoch: 2147, Batch Gradient Norm after: 22.360677418752235
Epoch 2148/10000, Prediction Accuracy = 51.592%, Loss = 1.2147007226943969
Epoch: 2148, Batch Gradient Norm: 40.11067272647126
Epoch: 2148, Batch Gradient Norm after: 22.360675857088626
Epoch 2149/10000, Prediction Accuracy = 51.596000000000004%, Loss = 1.1981745958328247
Epoch: 2149, Batch Gradient Norm: 45.720622703776485
Epoch: 2149, Batch Gradient Norm after: 22.36067838267378
Epoch 2150/10000, Prediction Accuracy = 51.60600000000001%, Loss = 1.214204525947571
Epoch: 2150, Batch Gradient Norm: 40.102078826684306
Epoch: 2150, Batch Gradient Norm after: 22.36067616188966
Epoch 2151/10000, Prediction Accuracy = 51.592000000000006%, Loss = 1.1977218389511108
Epoch: 2151, Batch Gradient Norm: 45.69283569179933
Epoch: 2151, Batch Gradient Norm after: 22.36067612992909
Epoch 2152/10000, Prediction Accuracy = 51.61%, Loss = 1.2136810302734375
Epoch: 2152, Batch Gradient Norm: 40.09324133130782
Epoch: 2152, Batch Gradient Norm after: 22.36067597573198
Epoch 2153/10000, Prediction Accuracy = 51.592000000000006%, Loss = 1.1972668170928955
Epoch: 2153, Batch Gradient Norm: 45.669304418194585
Epoch: 2153, Batch Gradient Norm after: 22.360677607043332
Epoch 2154/10000, Prediction Accuracy = 51.614%, Loss = 1.2131811141967774
Epoch: 2154, Batch Gradient Norm: 40.08456874578155
Epoch: 2154, Batch Gradient Norm after: 22.360674888120048
Epoch 2155/10000, Prediction Accuracy = 51.61%, Loss = 1.196809434890747
Epoch: 2155, Batch Gradient Norm: 45.646186469635445
Epoch: 2155, Batch Gradient Norm after: 22.360674434588976
Epoch 2156/10000, Prediction Accuracy = 51.60999999999999%, Loss = 1.2126753330230713
Epoch: 2156, Batch Gradient Norm: 40.07613256868304
Epoch: 2156, Batch Gradient Norm after: 22.360675992208837
Epoch 2157/10000, Prediction Accuracy = 51.622%, Loss = 1.1963597059249877
Epoch: 2157, Batch Gradient Norm: 45.6250883269903
Epoch: 2157, Batch Gradient Norm after: 22.36067763896426
Epoch 2158/10000, Prediction Accuracy = 51.626%, Loss = 1.2121739387512207
Epoch: 2158, Batch Gradient Norm: 40.066928244988205
Epoch: 2158, Batch Gradient Norm after: 22.360675110221162
Epoch 2159/10000, Prediction Accuracy = 51.638%, Loss = 1.1959031581878663
Epoch: 2159, Batch Gradient Norm: 45.60959903619172
Epoch: 2159, Batch Gradient Norm after: 22.360678553629036
Epoch 2160/10000, Prediction Accuracy = 51.628%, Loss = 1.2116974592208862
Epoch: 2160, Batch Gradient Norm: 40.06089778550764
Epoch: 2160, Batch Gradient Norm after: 22.360675132781
Epoch 2161/10000, Prediction Accuracy = 51.65%, Loss = 1.1954538583755494
Epoch: 2161, Batch Gradient Norm: 45.598274217431026
Epoch: 2161, Batch Gradient Norm after: 22.360679432311166
Epoch 2162/10000, Prediction Accuracy = 51.63599999999999%, Loss = 1.211233639717102
Epoch: 2162, Batch Gradient Norm: 40.058128122204906
Epoch: 2162, Batch Gradient Norm after: 22.36067618603887
Epoch 2163/10000, Prediction Accuracy = 51.65999999999999%, Loss = 1.1950116395950316
Epoch: 2163, Batch Gradient Norm: 45.590493145317296
Epoch: 2163, Batch Gradient Norm after: 22.360678670970948
Epoch 2164/10000, Prediction Accuracy = 51.65599999999999%, Loss = 1.2107808589935303
Epoch: 2164, Batch Gradient Norm: 40.05633630434128
Epoch: 2164, Batch Gradient Norm after: 22.360677317126765
Epoch 2165/10000, Prediction Accuracy = 51.672000000000004%, Loss = 1.194577431678772
Epoch: 2165, Batch Gradient Norm: 45.585077629575146
Epoch: 2165, Batch Gradient Norm after: 22.36067863433805
Epoch 2166/10000, Prediction Accuracy = 51.682%, Loss = 1.2103312969207765
Epoch: 2166, Batch Gradient Norm: 40.05430609585017
Epoch: 2166, Batch Gradient Norm after: 22.36067655219678
Epoch 2167/10000, Prediction Accuracy = 51.67%, Loss = 1.1941482782363892
Epoch: 2167, Batch Gradient Norm: 45.58412230555356
Epoch: 2167, Batch Gradient Norm after: 22.36067849803579
Epoch 2168/10000, Prediction Accuracy = 51.706%, Loss = 1.2098979234695435
Epoch: 2168, Batch Gradient Norm: 40.05447011867812
Epoch: 2168, Batch Gradient Norm after: 22.360675186547063
Epoch 2169/10000, Prediction Accuracy = 51.67999999999999%, Loss = 1.1937224864959717
Epoch: 2169, Batch Gradient Norm: 45.59651876516067
Epoch: 2169, Batch Gradient Norm after: 22.360679215929693
Epoch 2170/10000, Prediction Accuracy = 51.714%, Loss = 1.209514594078064
Epoch: 2170, Batch Gradient Norm: 40.054156007085325
Epoch: 2170, Batch Gradient Norm after: 22.36067452441339
Epoch 2171/10000, Prediction Accuracy = 51.681999999999995%, Loss = 1.1933061122894286
Epoch: 2171, Batch Gradient Norm: 45.61178674969003
Epoch: 2171, Batch Gradient Norm after: 22.360675565946703
Epoch 2172/10000, Prediction Accuracy = 51.738%, Loss = 1.2091415882110597
Epoch: 2172, Batch Gradient Norm: 40.05633424682515
Epoch: 2172, Batch Gradient Norm after: 22.360678225421125
Epoch 2173/10000, Prediction Accuracy = 51.688%, Loss = 1.1928974390029907
Epoch: 2173, Batch Gradient Norm: 45.64266578588088
Epoch: 2173, Batch Gradient Norm after: 22.36067782631009
Epoch 2174/10000, Prediction Accuracy = 51.75%, Loss = 1.2088109254837036
Epoch: 2174, Batch Gradient Norm: 40.06263387729888
Epoch: 2174, Batch Gradient Norm after: 22.360675966474556
Epoch 2175/10000, Prediction Accuracy = 51.698%, Loss = 1.1924903631210326
Epoch: 2175, Batch Gradient Norm: 45.67371796872073
Epoch: 2175, Batch Gradient Norm after: 22.36067631332406
Epoch 2176/10000, Prediction Accuracy = 51.760000000000005%, Loss = 1.2084827661514281
Epoch: 2176, Batch Gradient Norm: 40.065568540000406
Epoch: 2176, Batch Gradient Norm after: 22.360676838378993
Epoch 2177/10000, Prediction Accuracy = 51.702%, Loss = 1.1920759439468385
Epoch: 2177, Batch Gradient Norm: 45.70149555791815
Epoch: 2177, Batch Gradient Norm after: 22.36067738317801
Epoch 2178/10000, Prediction Accuracy = 51.772000000000006%, Loss = 1.2081406831741333
Epoch: 2178, Batch Gradient Norm: 40.06688757590583
Epoch: 2178, Batch Gradient Norm after: 22.360674194165885
Epoch 2179/10000, Prediction Accuracy = 51.71200000000001%, Loss = 1.191653037071228
Epoch: 2179, Batch Gradient Norm: 45.71581094801139
Epoch: 2179, Batch Gradient Norm after: 22.360677481547555
Epoch 2180/10000, Prediction Accuracy = 51.772000000000006%, Loss = 1.2077596187591553
Epoch: 2180, Batch Gradient Norm: 40.06546968735515
Epoch: 2180, Batch Gradient Norm after: 22.36067610979982
Epoch 2181/10000, Prediction Accuracy = 51.714%, Loss = 1.1912319898605346
Epoch: 2181, Batch Gradient Norm: 45.72657275613114
Epoch: 2181, Batch Gradient Norm after: 22.3606760331348
Epoch 2182/10000, Prediction Accuracy = 51.782%, Loss = 1.20736722946167
Epoch: 2182, Batch Gradient Norm: 40.06740151553812
Epoch: 2182, Batch Gradient Norm after: 22.360675101109795
Epoch 2183/10000, Prediction Accuracy = 51.722%, Loss = 1.1908093690872192
Epoch: 2183, Batch Gradient Norm: 45.74319311872586
Epoch: 2183, Batch Gradient Norm after: 22.36067833817583
Epoch 2184/10000, Prediction Accuracy = 51.794000000000004%, Loss = 1.2069854497909547
Epoch: 2184, Batch Gradient Norm: 40.069511784315495
Epoch: 2184, Batch Gradient Norm after: 22.360678386549605
Epoch 2185/10000, Prediction Accuracy = 51.727999999999994%, Loss = 1.1903898477554322
Epoch: 2185, Batch Gradient Norm: 45.75379736052295
Epoch: 2185, Batch Gradient Norm after: 22.360677303567847
Epoch 2186/10000, Prediction Accuracy = 51.818%, Loss = 1.2065979480743407
Epoch: 2186, Batch Gradient Norm: 40.0718880861346
Epoch: 2186, Batch Gradient Norm after: 22.36067536741524
Epoch 2187/10000, Prediction Accuracy = 51.734%, Loss = 1.1899706840515136
Epoch: 2187, Batch Gradient Norm: 45.762342415488554
Epoch: 2187, Batch Gradient Norm after: 22.360677450036967
Epoch 2188/10000, Prediction Accuracy = 51.82000000000001%, Loss = 1.2061954259872436
Epoch: 2188, Batch Gradient Norm: 40.07450228686142
Epoch: 2188, Batch Gradient Norm after: 22.36067780504725
Epoch 2189/10000, Prediction Accuracy = 51.751999999999995%, Loss = 1.1895482540130615
Epoch: 2189, Batch Gradient Norm: 45.773197683399445
Epoch: 2189, Batch Gradient Norm after: 22.36067822701566
Epoch 2190/10000, Prediction Accuracy = 51.826%, Loss = 1.2058012247085572
Epoch: 2190, Batch Gradient Norm: 40.07521908634562
Epoch: 2190, Batch Gradient Norm after: 22.3606784538515
Epoch 2191/10000, Prediction Accuracy = 51.760000000000005%, Loss = 1.1891314744949342
Epoch: 2191, Batch Gradient Norm: 45.78324634872314
Epoch: 2191, Batch Gradient Norm after: 22.360678167422527
Epoch 2192/10000, Prediction Accuracy = 51.838%, Loss = 1.205408263206482
Epoch: 2192, Batch Gradient Norm: 40.07522162011958
Epoch: 2192, Batch Gradient Norm after: 22.36067716263501
Epoch 2193/10000, Prediction Accuracy = 51.766%, Loss = 1.1887017726898192
Epoch: 2193, Batch Gradient Norm: 45.79431116290921
Epoch: 2193, Batch Gradient Norm after: 22.360677235727007
Epoch 2194/10000, Prediction Accuracy = 51.84400000000001%, Loss = 1.2050163745880127
Epoch: 2194, Batch Gradient Norm: 40.07444318087733
Epoch: 2194, Batch Gradient Norm after: 22.360678854588077
Epoch 2195/10000, Prediction Accuracy = 51.778%, Loss = 1.1882771492004394
Epoch: 2195, Batch Gradient Norm: 45.80948323799764
Epoch: 2195, Batch Gradient Norm after: 22.360675685398622
Epoch 2196/10000, Prediction Accuracy = 51.858000000000004%, Loss = 1.204644799232483
Epoch: 2196, Batch Gradient Norm: 40.07659402958684
Epoch: 2196, Batch Gradient Norm after: 22.36067640061392
Epoch 2197/10000, Prediction Accuracy = 51.782%, Loss = 1.1878600597381592
Epoch: 2197, Batch Gradient Norm: 45.83187671298382
Epoch: 2197, Batch Gradient Norm after: 22.360676883953936
Epoch 2198/10000, Prediction Accuracy = 51.864%, Loss = 1.2042980194091797
Epoch: 2198, Batch Gradient Norm: 40.082572987117665
Epoch: 2198, Batch Gradient Norm after: 22.360677148118686
Epoch 2199/10000, Prediction Accuracy = 51.782%, Loss = 1.187457513809204
Epoch: 2199, Batch Gradient Norm: 45.86370321644733
Epoch: 2199, Batch Gradient Norm after: 22.360675859886328
Epoch 2200/10000, Prediction Accuracy = 51.879999999999995%, Loss = 1.2039778709411622
Epoch: 2200, Batch Gradient Norm: 40.09011386920136
Epoch: 2200, Batch Gradient Norm after: 22.3606761595213
Epoch 2201/10000, Prediction Accuracy = 51.815999999999995%, Loss = 1.1870537042617797
Epoch: 2201, Batch Gradient Norm: 45.900697272708065
Epoch: 2201, Batch Gradient Norm after: 22.3606746763865
Epoch 2202/10000, Prediction Accuracy = 51.89%, Loss = 1.2036737203598022
Epoch: 2202, Batch Gradient Norm: 40.09851957317049
Epoch: 2202, Batch Gradient Norm after: 22.3606783507297
Epoch 2203/10000, Prediction Accuracy = 51.842%, Loss = 1.1866578817367555
Epoch: 2203, Batch Gradient Norm: 45.94207230863458
Epoch: 2203, Batch Gradient Norm after: 22.36067868978641
Epoch 2204/10000, Prediction Accuracy = 51.898%, Loss = 1.2033832788467407
Epoch: 2204, Batch Gradient Norm: 40.105871813727404
Epoch: 2204, Batch Gradient Norm after: 22.360676278960256
Epoch 2205/10000, Prediction Accuracy = 51.85600000000001%, Loss = 1.1862586259841919
Epoch: 2205, Batch Gradient Norm: 45.98302687011483
Epoch: 2205, Batch Gradient Norm after: 22.360676511296376
Epoch 2206/10000, Prediction Accuracy = 51.9%, Loss = 1.2030852556228637
Epoch: 2206, Batch Gradient Norm: 40.114296617258006
Epoch: 2206, Batch Gradient Norm after: 22.360675408324873
Epoch 2207/10000, Prediction Accuracy = 51.864%, Loss = 1.1858612060546876
Epoch: 2207, Batch Gradient Norm: 46.018676433334676
Epoch: 2207, Batch Gradient Norm after: 22.36067866380406
Epoch 2208/10000, Prediction Accuracy = 51.910000000000004%, Loss = 1.202772855758667
Epoch: 2208, Batch Gradient Norm: 40.121267521729955
Epoch: 2208, Batch Gradient Norm after: 22.36067588022562
Epoch 2209/10000, Prediction Accuracy = 51.882000000000005%, Loss = 1.1854583978652955
Epoch: 2209, Batch Gradient Norm: 46.05471549790801
Epoch: 2209, Batch Gradient Norm after: 22.360674909468923
Epoch 2210/10000, Prediction Accuracy = 51.934000000000005%, Loss = 1.2024645328521728
Epoch: 2210, Batch Gradient Norm: 40.12776199545446
Epoch: 2210, Batch Gradient Norm after: 22.360675910281703
Epoch 2211/10000, Prediction Accuracy = 51.89%, Loss = 1.1850513458251952
Epoch: 2211, Batch Gradient Norm: 46.081126277714034
Epoch: 2211, Batch Gradient Norm after: 22.360677581949723
Epoch 2212/10000, Prediction Accuracy = 51.94799999999999%, Loss = 1.2021273851394654
Epoch: 2212, Batch Gradient Norm: 40.13065799457552
Epoch: 2212, Batch Gradient Norm after: 22.360676624540794
Epoch 2213/10000, Prediction Accuracy = 51.90999999999999%, Loss = 1.1846326112747192
Epoch: 2213, Batch Gradient Norm: 46.10142511950318
Epoch: 2213, Batch Gradient Norm after: 22.36067678629117
Epoch 2214/10000, Prediction Accuracy = 51.952%, Loss = 1.2017611265182495
Epoch: 2214, Batch Gradient Norm: 40.130065805696944
Epoch: 2214, Batch Gradient Norm after: 22.36067674406688
Epoch 2215/10000, Prediction Accuracy = 51.918000000000006%, Loss = 1.184209156036377
Epoch: 2215, Batch Gradient Norm: 46.10576701253108
Epoch: 2215, Batch Gradient Norm after: 22.360678103297392
Epoch 2216/10000, Prediction Accuracy = 51.972%, Loss = 1.2013421058654785
Epoch: 2216, Batch Gradient Norm: 40.13103088108409
Epoch: 2216, Batch Gradient Norm after: 22.360675568204474
Epoch 2217/10000, Prediction Accuracy = 51.922000000000004%, Loss = 1.1837862014770508
Epoch: 2217, Batch Gradient Norm: 46.10874821490358
Epoch: 2217, Batch Gradient Norm after: 22.360676503484918
Epoch 2218/10000, Prediction Accuracy = 51.980000000000004%, Loss = 1.200921106338501
Epoch: 2218, Batch Gradient Norm: 40.12882190017548
Epoch: 2218, Batch Gradient Norm after: 22.360675430441223
Epoch 2219/10000, Prediction Accuracy = 51.948%, Loss = 1.1833536624908447
Epoch: 2219, Batch Gradient Norm: 46.108944706457294
Epoch: 2219, Batch Gradient Norm after: 22.360679428188384
Epoch 2220/10000, Prediction Accuracy = 51.99400000000001%, Loss = 1.2004971742630004
Epoch: 2220, Batch Gradient Norm: 40.12642445460731
Epoch: 2220, Batch Gradient Norm after: 22.36067610986512
Epoch 2221/10000, Prediction Accuracy = 51.958000000000006%, Loss = 1.182927703857422
Epoch: 2221, Batch Gradient Norm: 46.109140370058896
Epoch: 2221, Batch Gradient Norm after: 22.360678951909893
Epoch 2222/10000, Prediction Accuracy = 51.99400000000001%, Loss = 1.2000757694244384
Epoch: 2222, Batch Gradient Norm: 40.125003180972456
Epoch: 2222, Batch Gradient Norm after: 22.360676283523294
Epoch 2223/10000, Prediction Accuracy = 51.964%, Loss = 1.1824976444244384
Epoch: 2223, Batch Gradient Norm: 46.105156790238055
Epoch: 2223, Batch Gradient Norm after: 22.360676567426733
Epoch 2224/10000, Prediction Accuracy = 51.998000000000005%, Loss = 1.1996405839920044
Epoch: 2224, Batch Gradient Norm: 40.12367535352278
Epoch: 2224, Batch Gradient Norm after: 22.360676430671226
Epoch 2225/10000, Prediction Accuracy = 51.989999999999995%, Loss = 1.1820685386657714
Epoch: 2225, Batch Gradient Norm: 46.10656338361814
Epoch: 2225, Batch Gradient Norm after: 22.36067522550091
Epoch 2226/10000, Prediction Accuracy = 52.012%, Loss = 1.1992170810699463
Epoch: 2226, Batch Gradient Norm: 40.1214509296555
Epoch: 2226, Batch Gradient Norm after: 22.36067508499106
Epoch 2227/10000, Prediction Accuracy = 51.989999999999995%, Loss = 1.1816507816314696
Epoch: 2227, Batch Gradient Norm: 46.103074125727694
Epoch: 2227, Batch Gradient Norm after: 22.36067672979175
Epoch 2228/10000, Prediction Accuracy = 52.016%, Loss = 1.198782992362976
Epoch: 2228, Batch Gradient Norm: 40.116169691403854
Epoch: 2228, Batch Gradient Norm after: 22.360675362113582
Epoch 2229/10000, Prediction Accuracy = 51.99400000000001%, Loss = 1.1812148332595824
Epoch: 2229, Batch Gradient Norm: 46.084057589452534
Epoch: 2229, Batch Gradient Norm after: 22.360677656141306
Epoch 2230/10000, Prediction Accuracy = 52.03000000000001%, Loss = 1.1983068943023683
Epoch: 2230, Batch Gradient Norm: 40.10748517312275
Epoch: 2230, Batch Gradient Norm after: 22.360675358586533
Epoch 2231/10000, Prediction Accuracy = 52.008%, Loss = 1.180777096748352
Epoch: 2231, Batch Gradient Norm: 46.06839740379877
Epoch: 2231, Batch Gradient Norm after: 22.36067649969023
Epoch 2232/10000, Prediction Accuracy = 52.024%, Loss = 1.1978450775146485
Epoch: 2232, Batch Gradient Norm: 40.10113827311683
Epoch: 2232, Batch Gradient Norm after: 22.36067866379512
Epoch 2233/10000, Prediction Accuracy = 52.024%, Loss = 1.1803511381149292
Epoch: 2233, Batch Gradient Norm: 46.05564567845252
Epoch: 2233, Batch Gradient Norm after: 22.36068081400615
Epoch 2234/10000, Prediction Accuracy = 52.036%, Loss = 1.1973873853683472
Epoch: 2234, Batch Gradient Norm: 40.09547672833609
Epoch: 2234, Batch Gradient Norm after: 22.360677839223065
Epoch 2235/10000, Prediction Accuracy = 52.025999999999996%, Loss = 1.1799241065979005
Epoch: 2235, Batch Gradient Norm: 46.04704824818954
Epoch: 2235, Batch Gradient Norm after: 22.36067539214415
Epoch 2236/10000, Prediction Accuracy = 52.034000000000006%, Loss = 1.196938943862915
Epoch: 2236, Batch Gradient Norm: 40.08859446226738
Epoch: 2236, Batch Gradient Norm after: 22.360677023143413
Epoch 2237/10000, Prediction Accuracy = 52.044%, Loss = 1.1794903039932252
Epoch: 2237, Batch Gradient Norm: 46.035249384053756
Epoch: 2237, Batch Gradient Norm after: 22.36067833389142
Epoch 2238/10000, Prediction Accuracy = 52.03399999999999%, Loss = 1.1964864253997802
Epoch: 2238, Batch Gradient Norm: 40.086523477593246
Epoch: 2238, Batch Gradient Norm after: 22.36067530723362
Epoch 2239/10000, Prediction Accuracy = 52.05799999999999%, Loss = 1.1790677547454833
Epoch: 2239, Batch Gradient Norm: 46.029591827238335
Epoch: 2239, Batch Gradient Norm after: 22.36067530363699
Epoch 2240/10000, Prediction Accuracy = 52.029999999999994%, Loss = 1.1960566282272338
Epoch: 2240, Batch Gradient Norm: 40.08304881602942
Epoch: 2240, Batch Gradient Norm after: 22.36067622789224
Epoch 2241/10000, Prediction Accuracy = 52.072%, Loss = 1.1786483526229858
Epoch: 2241, Batch Gradient Norm: 46.02442325771019
Epoch: 2241, Batch Gradient Norm after: 22.360677183561393
Epoch 2242/10000, Prediction Accuracy = 52.028%, Loss = 1.195634913444519
Epoch: 2242, Batch Gradient Norm: 40.07985619400018
Epoch: 2242, Batch Gradient Norm after: 22.360678244038162
Epoch 2243/10000, Prediction Accuracy = 52.08%, Loss = 1.1782200813293457
Epoch: 2243, Batch Gradient Norm: 46.01210391383792
Epoch: 2243, Batch Gradient Norm after: 22.36067859276479
Epoch 2244/10000, Prediction Accuracy = 52.038%, Loss = 1.1951788902282714
Epoch: 2244, Batch Gradient Norm: 40.07680207696036
Epoch: 2244, Batch Gradient Norm after: 22.360676016716617
Epoch 2245/10000, Prediction Accuracy = 52.08399999999999%, Loss = 1.17779381275177
Epoch: 2245, Batch Gradient Norm: 46.00238994663227
Epoch: 2245, Batch Gradient Norm after: 22.3606757972183
Epoch 2246/10000, Prediction Accuracy = 52.04%, Loss = 1.1947382926940917
Epoch: 2246, Batch Gradient Norm: 40.07150649725812
Epoch: 2246, Batch Gradient Norm after: 22.360676444591324
Epoch 2247/10000, Prediction Accuracy = 52.098%, Loss = 1.1773788690567017
Epoch: 2247, Batch Gradient Norm: 46.00400660059177
Epoch: 2247, Batch Gradient Norm after: 22.360679210388394
Epoch 2248/10000, Prediction Accuracy = 52.056000000000004%, Loss = 1.1943224906921386
Epoch: 2248, Batch Gradient Norm: 40.070260712620595
Epoch: 2248, Batch Gradient Norm after: 22.360672830995846
Epoch 2249/10000, Prediction Accuracy = 52.11%, Loss = 1.1769608974456787
Epoch: 2249, Batch Gradient Norm: 46.010849999922534
Epoch: 2249, Batch Gradient Norm after: 22.360677257686678
Epoch 2250/10000, Prediction Accuracy = 52.076%, Loss = 1.1939326047897338
Epoch: 2250, Batch Gradient Norm: 40.06980666257359
Epoch: 2250, Batch Gradient Norm after: 22.360676469869198
Epoch 2251/10000, Prediction Accuracy = 52.126%, Loss = 1.1765432119369508
Epoch: 2251, Batch Gradient Norm: 46.014636739194046
Epoch: 2251, Batch Gradient Norm after: 22.360677726991153
Epoch 2252/10000, Prediction Accuracy = 52.086%, Loss = 1.1935220241546631
Epoch: 2252, Batch Gradient Norm: 40.067238686557424
Epoch: 2252, Batch Gradient Norm after: 22.36067539221682
Epoch 2253/10000, Prediction Accuracy = 52.138%, Loss = 1.1761312246322633
Epoch: 2253, Batch Gradient Norm: 46.01805353591189
Epoch: 2253, Batch Gradient Norm after: 22.360677597524816
Epoch 2254/10000, Prediction Accuracy = 52.098%, Loss = 1.1931280374526978
Epoch: 2254, Batch Gradient Norm: 40.065888503749434
Epoch: 2254, Batch Gradient Norm after: 22.360676006968095
Epoch 2255/10000, Prediction Accuracy = 52.144000000000005%, Loss = 1.175715160369873
Epoch: 2255, Batch Gradient Norm: 46.02494091758375
Epoch: 2255, Batch Gradient Norm after: 22.36067752736882
Epoch 2256/10000, Prediction Accuracy = 52.104%, Loss = 1.1927414655685424
Epoch: 2256, Batch Gradient Norm: 40.0643157466596
Epoch: 2256, Batch Gradient Norm after: 22.3606756237854
Epoch 2257/10000, Prediction Accuracy = 52.15%, Loss = 1.1752995491027831
Epoch: 2257, Batch Gradient Norm: 46.0323189662774
Epoch: 2257, Batch Gradient Norm after: 22.360678028282237
Epoch 2258/10000, Prediction Accuracy = 52.11%, Loss = 1.192354106903076
Epoch: 2258, Batch Gradient Norm: 40.06469617078539
Epoch: 2258, Batch Gradient Norm after: 22.3606755420864
Epoch 2259/10000, Prediction Accuracy = 52.15%, Loss = 1.1748881578445434
Epoch: 2259, Batch Gradient Norm: 46.028223554734986
Epoch: 2259, Batch Gradient Norm after: 22.360678528672384
Epoch 2260/10000, Prediction Accuracy = 52.126%, Loss = 1.1919391870498657
Epoch: 2260, Batch Gradient Norm: 40.06141426151562
Epoch: 2260, Batch Gradient Norm after: 22.36067727144947
Epoch 2261/10000, Prediction Accuracy = 52.172000000000004%, Loss = 1.1744705200195313
Epoch: 2261, Batch Gradient Norm: 46.02072646883849
Epoch: 2261, Batch Gradient Norm after: 22.360677001188133
Epoch 2262/10000, Prediction Accuracy = 52.136%, Loss = 1.1915046453475953
Epoch: 2262, Batch Gradient Norm: 40.05571231521512
Epoch: 2262, Batch Gradient Norm after: 22.36067366211736
Epoch 2263/10000, Prediction Accuracy = 52.17999999999999%, Loss = 1.1740461826324462
Epoch: 2263, Batch Gradient Norm: 46.02287740038711
Epoch: 2263, Batch Gradient Norm after: 22.360677569356255
Epoch 2264/10000, Prediction Accuracy = 52.146%, Loss = 1.1910892724990845
Epoch: 2264, Batch Gradient Norm: 40.052575787461095
Epoch: 2264, Batch Gradient Norm after: 22.36067770031523
Epoch 2265/10000, Prediction Accuracy = 52.181999999999995%, Loss = 1.173629379272461
Epoch: 2265, Batch Gradient Norm: 46.02896766672321
Epoch: 2265, Batch Gradient Norm after: 22.360676023900957
Epoch 2266/10000, Prediction Accuracy = 52.166%, Loss = 1.1906866788864137
Epoch: 2266, Batch Gradient Norm: 40.053158508583735
Epoch: 2266, Batch Gradient Norm after: 22.360676762792544
Epoch 2267/10000, Prediction Accuracy = 52.19000000000001%, Loss = 1.1732156991958618
Epoch: 2267, Batch Gradient Norm: 46.03081369347863
Epoch: 2267, Batch Gradient Norm after: 22.36067791237832
Epoch 2268/10000, Prediction Accuracy = 52.174%, Loss = 1.1902788877487183
Epoch: 2268, Batch Gradient Norm: 40.048209184247895
Epoch: 2268, Batch Gradient Norm after: 22.3606758452779
Epoch 2269/10000, Prediction Accuracy = 52.2%, Loss = 1.1728003740310669
Epoch: 2269, Batch Gradient Norm: 46.02310423174153
Epoch: 2269, Batch Gradient Norm after: 22.360679297573885
Epoch 2270/10000, Prediction Accuracy = 52.18399999999999%, Loss = 1.1898556709289552
Epoch: 2270, Batch Gradient Norm: 40.04379679390543
Epoch: 2270, Batch Gradient Norm after: 22.360675954333125
Epoch 2271/10000, Prediction Accuracy = 52.212%, Loss = 1.1723761320114137
Epoch: 2271, Batch Gradient Norm: 46.01914677504786
Epoch: 2271, Batch Gradient Norm after: 22.36067751512948
Epoch 2272/10000, Prediction Accuracy = 52.196000000000005%, Loss = 1.189437747001648
Epoch: 2272, Batch Gradient Norm: 40.041059239407566
Epoch: 2272, Batch Gradient Norm after: 22.360677934090653
Epoch 2273/10000, Prediction Accuracy = 52.217999999999996%, Loss = 1.1719692468643188
Epoch: 2273, Batch Gradient Norm: 46.023402641663495
Epoch: 2273, Batch Gradient Norm after: 22.360677887891246
Epoch 2274/10000, Prediction Accuracy = 52.214%, Loss = 1.1890386104583741
Epoch: 2274, Batch Gradient Norm: 40.038955151526494
Epoch: 2274, Batch Gradient Norm after: 22.360678174936698
Epoch 2275/10000, Prediction Accuracy = 52.232000000000006%, Loss = 1.1715646266937256
Epoch: 2275, Batch Gradient Norm: 46.029940842108765
Epoch: 2275, Batch Gradient Norm after: 22.36067589870545
Epoch 2276/10000, Prediction Accuracy = 52.227999999999994%, Loss = 1.1886504650115968
Epoch: 2276, Batch Gradient Norm: 40.03384154532811
Epoch: 2276, Batch Gradient Norm after: 22.36067654751875
Epoch 2277/10000, Prediction Accuracy = 52.25599999999999%, Loss = 1.1711501121520995
Epoch: 2277, Batch Gradient Norm: 46.027445717679626
Epoch: 2277, Batch Gradient Norm after: 22.360677846417904
Epoch 2278/10000, Prediction Accuracy = 52.25%, Loss = 1.1882374048233033
Epoch: 2278, Batch Gradient Norm: 40.02642507516908
Epoch: 2278, Batch Gradient Norm after: 22.36067712474196
Epoch 2279/10000, Prediction Accuracy = 52.254000000000005%, Loss = 1.1707266330718995
Epoch: 2279, Batch Gradient Norm: 46.01905675519694
Epoch: 2279, Batch Gradient Norm after: 22.36067434734705
Epoch 2280/10000, Prediction Accuracy = 52.263999999999996%, Loss = 1.1878108263015748
Epoch: 2280, Batch Gradient Norm: 40.022372144025155
Epoch: 2280, Batch Gradient Norm after: 22.36067549887246
Epoch 2281/10000, Prediction Accuracy = 52.272000000000006%, Loss = 1.1703050136566162
Epoch: 2281, Batch Gradient Norm: 46.00507875827287
Epoch: 2281, Batch Gradient Norm after: 22.360677468945447
Epoch 2282/10000, Prediction Accuracy = 52.263999999999996%, Loss = 1.1873611450195312
Epoch: 2282, Batch Gradient Norm: 40.015093988721695
Epoch: 2282, Batch Gradient Norm after: 22.360676368379355
Epoch 2283/10000, Prediction Accuracy = 52.288%, Loss = 1.1698822736740113
Epoch: 2283, Batch Gradient Norm: 45.986029843535924
Epoch: 2283, Batch Gradient Norm after: 22.360676407354664
Epoch 2284/10000, Prediction Accuracy = 52.286%, Loss = 1.1869025707244873
Epoch: 2284, Batch Gradient Norm: 40.00602052255013
Epoch: 2284, Batch Gradient Norm after: 22.360676045790928
Epoch 2285/10000, Prediction Accuracy = 52.286%, Loss = 1.169458556175232
Epoch: 2285, Batch Gradient Norm: 45.96757981859415
Epoch: 2285, Batch Gradient Norm after: 22.360676131253012
Epoch 2286/10000, Prediction Accuracy = 52.29%, Loss = 1.1864449501037597
Epoch: 2286, Batch Gradient Norm: 39.99933284385427
Epoch: 2286, Batch Gradient Norm after: 22.360677952612043
Epoch 2287/10000, Prediction Accuracy = 52.314%, Loss = 1.1690336465835571
Epoch: 2287, Batch Gradient Norm: 45.95082845380992
Epoch: 2287, Batch Gradient Norm after: 22.360676781233792
Epoch 2288/10000, Prediction Accuracy = 52.3%, Loss = 1.1859893560409547
Epoch: 2288, Batch Gradient Norm: 39.99175508825106
Epoch: 2288, Batch Gradient Norm after: 22.360675447017652
Epoch 2289/10000, Prediction Accuracy = 52.326%, Loss = 1.1686209917068482
Epoch: 2289, Batch Gradient Norm: 45.94379404627654
Epoch: 2289, Batch Gradient Norm after: 22.3606784871113
Epoch 2290/10000, Prediction Accuracy = 52.306000000000004%, Loss = 1.1855584859848023
Epoch: 2290, Batch Gradient Norm: 39.98366979013791
Epoch: 2290, Batch Gradient Norm after: 22.360676999360468
Epoch 2291/10000, Prediction Accuracy = 52.339999999999996%, Loss = 1.1682080507278443
Epoch: 2291, Batch Gradient Norm: 45.94122339516857
Epoch: 2291, Batch Gradient Norm after: 22.360676402518298
Epoch 2292/10000, Prediction Accuracy = 52.312%, Loss = 1.1851418495178223
Epoch: 2292, Batch Gradient Norm: 39.98120635414142
Epoch: 2292, Batch Gradient Norm after: 22.360677842120133
Epoch 2293/10000, Prediction Accuracy = 52.355999999999995%, Loss = 1.1677952766418458
Epoch: 2293, Batch Gradient Norm: 45.93329357833597
Epoch: 2293, Batch Gradient Norm after: 22.360677528640686
Epoch 2294/10000, Prediction Accuracy = 52.327999999999996%, Loss = 1.1847172260284424
Epoch: 2294, Batch Gradient Norm: 39.9741318100556
Epoch: 2294, Batch Gradient Norm after: 22.360677578624365
Epoch 2295/10000, Prediction Accuracy = 52.376%, Loss = 1.1673796892166137
Epoch: 2295, Batch Gradient Norm: 45.92175849552942
Epoch: 2295, Batch Gradient Norm after: 22.360678418403744
Epoch 2296/10000, Prediction Accuracy = 52.318%, Loss = 1.1842710971832275
Epoch: 2296, Batch Gradient Norm: 39.97006132568255
Epoch: 2296, Batch Gradient Norm after: 22.360675627593434
Epoch 2297/10000, Prediction Accuracy = 52.39%, Loss = 1.1669596910476685
Epoch: 2297, Batch Gradient Norm: 45.91539025880806
Epoch: 2297, Batch Gradient Norm after: 22.360678039614186
Epoch 2298/10000, Prediction Accuracy = 52.324%, Loss = 1.183847689628601
Epoch: 2298, Batch Gradient Norm: 39.96551878562091
Epoch: 2298, Batch Gradient Norm after: 22.360677448467342
Epoch 2299/10000, Prediction Accuracy = 52.396%, Loss = 1.166550636291504
Epoch: 2299, Batch Gradient Norm: 45.905671865970206
Epoch: 2299, Batch Gradient Norm after: 22.360679716988457
Epoch 2300/10000, Prediction Accuracy = 52.331999999999994%, Loss = 1.1834232091903687
Epoch: 2300, Batch Gradient Norm: 39.961444295153335
Epoch: 2300, Batch Gradient Norm after: 22.360676539539458
Epoch 2301/10000, Prediction Accuracy = 52.40000000000001%, Loss = 1.166143274307251
Epoch: 2301, Batch Gradient Norm: 45.90171831652906
Epoch: 2301, Batch Gradient Norm after: 22.360679210507584
Epoch 2302/10000, Prediction Accuracy = 52.343999999999994%, Loss = 1.183015489578247
Epoch: 2302, Batch Gradient Norm: 39.956545451912845
Epoch: 2302, Batch Gradient Norm after: 22.36067650219694
Epoch 2303/10000, Prediction Accuracy = 52.402%, Loss = 1.1657337188720702
Epoch: 2303, Batch Gradient Norm: 45.898514053655006
Epoch: 2303, Batch Gradient Norm after: 22.360675317659638
Epoch 2304/10000, Prediction Accuracy = 52.348%, Loss = 1.182610559463501
Epoch: 2304, Batch Gradient Norm: 39.95446055934371
Epoch: 2304, Batch Gradient Norm after: 22.36067769604583
Epoch 2305/10000, Prediction Accuracy = 52.410000000000004%, Loss = 1.1653281927108765
Epoch: 2305, Batch Gradient Norm: 45.897443372045316
Epoch: 2305, Batch Gradient Norm after: 22.360677581990323
Epoch 2306/10000, Prediction Accuracy = 52.362%, Loss = 1.182204270362854
Epoch: 2306, Batch Gradient Norm: 39.95154044146699
Epoch: 2306, Batch Gradient Norm after: 22.36067668296737
Epoch 2307/10000, Prediction Accuracy = 52.424%, Loss = 1.164916205406189
Epoch: 2307, Batch Gradient Norm: 45.88522777641257
Epoch: 2307, Batch Gradient Norm after: 22.360676144611546
Epoch 2308/10000, Prediction Accuracy = 52.37399999999999%, Loss = 1.181774616241455
Epoch: 2308, Batch Gradient Norm: 39.94238653407167
Epoch: 2308, Batch Gradient Norm after: 22.36067910780559
Epoch 2309/10000, Prediction Accuracy = 52.44200000000001%, Loss = 1.1644958019256593
Epoch: 2309, Batch Gradient Norm: 45.86680942636544
Epoch: 2309, Batch Gradient Norm after: 22.360674691115754
Epoch 2310/10000, Prediction Accuracy = 52.386%, Loss = 1.1813128471374512
Epoch: 2310, Batch Gradient Norm: 39.93162373950609
Epoch: 2310, Batch Gradient Norm after: 22.36067716431531
Epoch 2311/10000, Prediction Accuracy = 52.455999999999996%, Loss = 1.164081048965454
Epoch: 2311, Batch Gradient Norm: 45.84971720930599
Epoch: 2311, Batch Gradient Norm after: 22.360675614816767
Epoch 2312/10000, Prediction Accuracy = 52.402%, Loss = 1.1808615446090698
Epoch: 2312, Batch Gradient Norm: 39.92028928298209
Epoch: 2312, Batch Gradient Norm after: 22.360677616546013
Epoch 2313/10000, Prediction Accuracy = 52.465999999999994%, Loss = 1.16366286277771
Epoch: 2313, Batch Gradient Norm: 45.83319099780236
Epoch: 2313, Batch Gradient Norm after: 22.360676900316392
Epoch 2314/10000, Prediction Accuracy = 52.403999999999996%, Loss = 1.180409049987793
Epoch: 2314, Batch Gradient Norm: 39.9128944490958
Epoch: 2314, Batch Gradient Norm after: 22.360678977712897
Epoch 2315/10000, Prediction Accuracy = 52.476%, Loss = 1.163246989250183
Epoch: 2315, Batch Gradient Norm: 45.80816313449934
Epoch: 2315, Batch Gradient Norm after: 22.360677185851817
Epoch 2316/10000, Prediction Accuracy = 52.410000000000004%, Loss = 1.179946732521057
Epoch: 2316, Batch Gradient Norm: 39.90017386635512
Epoch: 2316, Batch Gradient Norm after: 22.360676505439134
Epoch 2317/10000, Prediction Accuracy = 52.486000000000004%, Loss = 1.162825608253479
Epoch: 2317, Batch Gradient Norm: 45.78565041333366
Epoch: 2317, Batch Gradient Norm after: 22.36067743414152
Epoch 2318/10000, Prediction Accuracy = 52.428%, Loss = 1.179482674598694
Epoch: 2318, Batch Gradient Norm: 39.89152092736111
Epoch: 2318, Batch Gradient Norm after: 22.360677734279104
Epoch 2319/10000, Prediction Accuracy = 52.496%, Loss = 1.1624114036560058
Epoch: 2319, Batch Gradient Norm: 45.76052553334243
Epoch: 2319, Batch Gradient Norm after: 22.36067676757976
Epoch 2320/10000, Prediction Accuracy = 52.44599999999999%, Loss = 1.179010844230652
Epoch: 2320, Batch Gradient Norm: 39.88079592130345
Epoch: 2320, Batch Gradient Norm after: 22.360674521440533
Epoch 2321/10000, Prediction Accuracy = 52.513999999999996%, Loss = 1.1619886398315429
Epoch: 2321, Batch Gradient Norm: 45.72899570177974
Epoch: 2321, Batch Gradient Norm after: 22.360676872478518
Epoch 2322/10000, Prediction Accuracy = 52.45399999999999%, Loss = 1.1785132169723511
Epoch: 2322, Batch Gradient Norm: 39.87003327904332
Epoch: 2322, Batch Gradient Norm after: 22.360677593826534
Epoch 2323/10000, Prediction Accuracy = 52.525999999999996%, Loss = 1.161567783355713
Epoch: 2323, Batch Gradient Norm: 45.69800888014699
Epoch: 2323, Batch Gradient Norm after: 22.360676136881562
Epoch 2324/10000, Prediction Accuracy = 52.462%, Loss = 1.178024983406067
Epoch: 2324, Batch Gradient Norm: 39.86007765768594
Epoch: 2324, Batch Gradient Norm after: 22.360675877533605
Epoch 2325/10000, Prediction Accuracy = 52.544%, Loss = 1.1611587524414062
Epoch: 2325, Batch Gradient Norm: 45.684007650752875
Epoch: 2325, Batch Gradient Norm after: 22.360675655819385
Epoch 2326/10000, Prediction Accuracy = 52.472%, Loss = 1.1775892496109008
Epoch: 2326, Batch Gradient Norm: 39.85301413826834
Epoch: 2326, Batch Gradient Norm after: 22.360675480146195
Epoch 2327/10000, Prediction Accuracy = 52.548%, Loss = 1.1607514142990112
Epoch: 2327, Batch Gradient Norm: 45.66486202993619
Epoch: 2327, Batch Gradient Norm after: 22.360676920738133
Epoch 2328/10000, Prediction Accuracy = 52.492000000000004%, Loss = 1.1771423816680908
Epoch: 2328, Batch Gradient Norm: 39.84364250356543
Epoch: 2328, Batch Gradient Norm after: 22.360675965280468
Epoch 2329/10000, Prediction Accuracy = 52.540000000000006%, Loss = 1.160338282585144
Epoch: 2329, Batch Gradient Norm: 45.64462841308531
Epoch: 2329, Batch Gradient Norm after: 22.360674001449915
Epoch 2330/10000, Prediction Accuracy = 52.50600000000001%, Loss = 1.1766905784606934
Epoch: 2330, Batch Gradient Norm: 39.836621984443006
Epoch: 2330, Batch Gradient Norm after: 22.360675907954352
Epoch 2331/10000, Prediction Accuracy = 52.55800000000001%, Loss = 1.1599332094192505
Epoch: 2331, Batch Gradient Norm: 45.63178609934343
Epoch: 2331, Batch Gradient Norm after: 22.36067644518005
Epoch 2332/10000, Prediction Accuracy = 52.516%, Loss = 1.1762646675109862
Epoch: 2332, Batch Gradient Norm: 39.829770689186304
Epoch: 2332, Batch Gradient Norm after: 22.360675307117504
Epoch 2333/10000, Prediction Accuracy = 52.565999999999995%, Loss = 1.1595318078994752
Epoch: 2333, Batch Gradient Norm: 45.61553475254123
Epoch: 2333, Batch Gradient Norm after: 22.36067629629765
Epoch 2334/10000, Prediction Accuracy = 52.532%, Loss = 1.1758208274841309
Epoch: 2334, Batch Gradient Norm: 39.82175729537459
Epoch: 2334, Batch Gradient Norm after: 22.360674860692406
Epoch 2335/10000, Prediction Accuracy = 52.572%, Loss = 1.159127902984619
Epoch: 2335, Batch Gradient Norm: 45.601863623048565
Epoch: 2335, Batch Gradient Norm after: 22.360677266944965
Epoch 2336/10000, Prediction Accuracy = 52.540000000000006%, Loss = 1.175395917892456
Epoch: 2336, Batch Gradient Norm: 39.81253150346969
Epoch: 2336, Batch Gradient Norm after: 22.360676988485547
Epoch 2337/10000, Prediction Accuracy = 52.588%, Loss = 1.1587279796600343
Epoch: 2337, Batch Gradient Norm: 45.58942818987919
Epoch: 2337, Batch Gradient Norm after: 22.36067778636473
Epoch 2338/10000, Prediction Accuracy = 52.54600000000001%, Loss = 1.1749749183654785
Epoch: 2338, Batch Gradient Norm: 39.802605589147674
Epoch: 2338, Batch Gradient Norm after: 22.360677159943336
Epoch 2339/10000, Prediction Accuracy = 52.588%, Loss = 1.158327007293701
Epoch: 2339, Batch Gradient Norm: 45.58082598220166
Epoch: 2339, Batch Gradient Norm after: 22.36067822108274
Epoch 2340/10000, Prediction Accuracy = 52.556%, Loss = 1.1745732784271241
Epoch: 2340, Batch Gradient Norm: 39.7979949671581
Epoch: 2340, Batch Gradient Norm after: 22.360677592689918
Epoch 2341/10000, Prediction Accuracy = 52.59400000000001%, Loss = 1.1579408645629883
Epoch: 2341, Batch Gradient Norm: 45.58283659174207
Epoch: 2341, Batch Gradient Norm after: 22.360677141969447
Epoch 2342/10000, Prediction Accuracy = 52.572%, Loss = 1.1742103338241576
Epoch: 2342, Batch Gradient Norm: 39.7962554643465
Epoch: 2342, Batch Gradient Norm after: 22.360677094945252
Epoch 2343/10000, Prediction Accuracy = 52.604%, Loss = 1.1575458288192748
Epoch: 2343, Batch Gradient Norm: 45.58595727104809
Epoch: 2343, Batch Gradient Norm after: 22.360679035180535
Epoch 2344/10000, Prediction Accuracy = 52.584%, Loss = 1.17384192943573
Epoch: 2344, Batch Gradient Norm: 39.79370907970688
Epoch: 2344, Batch Gradient Norm after: 22.360677337618405
Epoch 2345/10000, Prediction Accuracy = 52.63000000000001%, Loss = 1.1571544647216796
Epoch: 2345, Batch Gradient Norm: 45.58575295177632
Epoch: 2345, Batch Gradient Norm after: 22.36067938590753
Epoch 2346/10000, Prediction Accuracy = 52.602%, Loss = 1.1734590768814086
Epoch: 2346, Batch Gradient Norm: 39.78768021736427
Epoch: 2346, Batch Gradient Norm after: 22.360676630096076
Epoch 2347/10000, Prediction Accuracy = 52.638%, Loss = 1.1567623138427734
Epoch: 2347, Batch Gradient Norm: 45.57493854353604
Epoch: 2347, Batch Gradient Norm after: 22.360677522337596
Epoch 2348/10000, Prediction Accuracy = 52.636%, Loss = 1.1730530977249145
Epoch: 2348, Batch Gradient Norm: 39.77979712193018
Epoch: 2348, Batch Gradient Norm after: 22.36067532838978
Epoch 2349/10000, Prediction Accuracy = 52.652%, Loss = 1.1563637256622314
Epoch: 2349, Batch Gradient Norm: 45.55555202234793
Epoch: 2349, Batch Gradient Norm after: 22.360677760312242
Epoch 2350/10000, Prediction Accuracy = 52.64399999999999%, Loss = 1.1726114273071289
Epoch: 2350, Batch Gradient Norm: 39.77100601004638
Epoch: 2350, Batch Gradient Norm after: 22.360674613428664
Epoch 2351/10000, Prediction Accuracy = 52.666%, Loss = 1.1559582710266114
Epoch: 2351, Batch Gradient Norm: 45.531945753791724
Epoch: 2351, Batch Gradient Norm after: 22.36067809236312
Epoch 2352/10000, Prediction Accuracy = 52.652%, Loss = 1.1721555233001708
Epoch: 2352, Batch Gradient Norm: 39.75851083682486
Epoch: 2352, Batch Gradient Norm after: 22.360676137629458
Epoch 2353/10000, Prediction Accuracy = 52.676%, Loss = 1.1555438756942749
Epoch: 2353, Batch Gradient Norm: 45.501344557130395
Epoch: 2353, Batch Gradient Norm after: 22.36067817689288
Epoch 2354/10000, Prediction Accuracy = 52.669999999999995%, Loss = 1.1716921091079713
Epoch: 2354, Batch Gradient Norm: 39.74738942312697
Epoch: 2354, Batch Gradient Norm after: 22.36067814503351
Epoch 2355/10000, Prediction Accuracy = 52.681999999999995%, Loss = 1.1551371574401856
Epoch: 2355, Batch Gradient Norm: 45.47412974770178
Epoch: 2355, Batch Gradient Norm after: 22.360677422852735
Epoch 2356/10000, Prediction Accuracy = 52.68399999999999%, Loss = 1.1712348937988282
Epoch: 2356, Batch Gradient Norm: 39.7354269460616
Epoch: 2356, Batch Gradient Norm after: 22.36067776311722
Epoch 2357/10000, Prediction Accuracy = 52.693999999999996%, Loss = 1.154730772972107
Epoch: 2357, Batch Gradient Norm: 45.45837031768796
Epoch: 2357, Batch Gradient Norm after: 22.36067679975062
Epoch 2358/10000, Prediction Accuracy = 52.684000000000005%, Loss = 1.1708168029785155
Epoch: 2358, Batch Gradient Norm: 39.72976397464092
Epoch: 2358, Batch Gradient Norm after: 22.36067574332597
Epoch 2359/10000, Prediction Accuracy = 52.7%, Loss = 1.154344129562378
Epoch: 2359, Batch Gradient Norm: 45.436698926029514
Epoch: 2359, Batch Gradient Norm after: 22.360680161728325
Epoch 2360/10000, Prediction Accuracy = 52.69199999999999%, Loss = 1.1703728199005128
Epoch: 2360, Batch Gradient Norm: 39.718661864044165
Epoch: 2360, Batch Gradient Norm after: 22.360677092961055
Epoch 2361/10000, Prediction Accuracy = 52.70799999999999%, Loss = 1.1539453268051147
Epoch: 2361, Batch Gradient Norm: 45.43089309804506
Epoch: 2361, Batch Gradient Norm after: 22.360679761367283
Epoch 2362/10000, Prediction Accuracy = 52.708000000000006%, Loss = 1.16998348236084
Epoch: 2362, Batch Gradient Norm: 39.71467957182535
Epoch: 2362, Batch Gradient Norm after: 22.360676712508358
Epoch 2363/10000, Prediction Accuracy = 52.720000000000006%, Loss = 1.1535547256469727
Epoch: 2363, Batch Gradient Norm: 45.417407922836624
Epoch: 2363, Batch Gradient Norm after: 22.360677978051392
Epoch 2364/10000, Prediction Accuracy = 52.717999999999996%, Loss = 1.1695685148239137
Epoch: 2364, Batch Gradient Norm: 39.70422998473943
Epoch: 2364, Batch Gradient Norm after: 22.360677212868765
Epoch 2365/10000, Prediction Accuracy = 52.727999999999994%, Loss = 1.1531733751296998
Epoch: 2365, Batch Gradient Norm: 45.418841497496054
Epoch: 2365, Batch Gradient Norm after: 22.360676372411348
Epoch 2366/10000, Prediction Accuracy = 52.736000000000004%, Loss = 1.1692049980163575
Epoch: 2366, Batch Gradient Norm: 39.70082628887069
Epoch: 2366, Batch Gradient Norm after: 22.360676693761395
Epoch 2367/10000, Prediction Accuracy = 52.733999999999995%, Loss = 1.1527924537658691
Epoch: 2367, Batch Gradient Norm: 45.414864239093504
Epoch: 2367, Batch Gradient Norm after: 22.36067800391301
Epoch 2368/10000, Prediction Accuracy = 52.75%, Loss = 1.168832039833069
Epoch: 2368, Batch Gradient Norm: 39.68980444208384
Epoch: 2368, Batch Gradient Norm after: 22.360675268671777
Epoch 2369/10000, Prediction Accuracy = 52.736000000000004%, Loss = 1.1523978471755982
Epoch: 2369, Batch Gradient Norm: 45.41550063783635
Epoch: 2369, Batch Gradient Norm after: 22.36067788396758
Epoch 2370/10000, Prediction Accuracy = 52.763999999999996%, Loss = 1.1684602499008179
Epoch: 2370, Batch Gradient Norm: 39.68308547803714
Epoch: 2370, Batch Gradient Norm after: 22.360678348907097
Epoch 2371/10000, Prediction Accuracy = 52.738%, Loss = 1.1520184516906737
Epoch: 2371, Batch Gradient Norm: 45.41142212720572
Epoch: 2371, Batch Gradient Norm after: 22.360678637688448
Epoch 2372/10000, Prediction Accuracy = 52.772000000000006%, Loss = 1.168075156211853
Epoch: 2372, Batch Gradient Norm: 39.67562031990645
Epoch: 2372, Batch Gradient Norm after: 22.3606772870746
Epoch 2373/10000, Prediction Accuracy = 52.742%, Loss = 1.1516342401504516
Epoch: 2373, Batch Gradient Norm: 45.42192295708361
Epoch: 2373, Batch Gradient Norm after: 22.360677813051623
Epoch 2374/10000, Prediction Accuracy = 52.775999999999996%, Loss = 1.1677396774291993
Epoch: 2374, Batch Gradient Norm: 39.67444707141147
Epoch: 2374, Batch Gradient Norm after: 22.360675248123265
Epoch 2375/10000, Prediction Accuracy = 52.75599999999999%, Loss = 1.1512645959854126
Epoch: 2375, Batch Gradient Norm: 45.43349376469103
Epoch: 2375, Batch Gradient Norm after: 22.360675817195673
Epoch 2376/10000, Prediction Accuracy = 52.782000000000004%, Loss = 1.1674055337905884
Epoch: 2376, Batch Gradient Norm: 39.67114696951208
Epoch: 2376, Batch Gradient Norm after: 22.36067675841476
Epoch 2377/10000, Prediction Accuracy = 52.760000000000005%, Loss = 1.1508926391601562
Epoch: 2377, Batch Gradient Norm: 45.44749884565099
Epoch: 2377, Batch Gradient Norm after: 22.360679467670774
Epoch 2378/10000, Prediction Accuracy = 52.791999999999994%, Loss = 1.1670870304107666
Epoch: 2378, Batch Gradient Norm: 39.668402589262044
Epoch: 2378, Batch Gradient Norm after: 22.36067629475698
Epoch 2379/10000, Prediction Accuracy = 52.774%, Loss = 1.1505158185958861
Epoch: 2379, Batch Gradient Norm: 45.448224808943294
Epoch: 2379, Batch Gradient Norm after: 22.360678754147266
Epoch 2380/10000, Prediction Accuracy = 52.80800000000001%, Loss = 1.1667141675949098
Epoch: 2380, Batch Gradient Norm: 39.660689265624
Epoch: 2380, Batch Gradient Norm after: 22.36067650714856
Epoch 2381/10000, Prediction Accuracy = 52.784000000000006%, Loss = 1.1501226425170898
Epoch: 2381, Batch Gradient Norm: 45.444124477057144
Epoch: 2381, Batch Gradient Norm after: 22.36067796233796
Epoch 2382/10000, Prediction Accuracy = 52.815999999999995%, Loss = 1.1663344144821166
Epoch: 2382, Batch Gradient Norm: 39.65220972677997
Epoch: 2382, Batch Gradient Norm after: 22.36067627543841
Epoch 2383/10000, Prediction Accuracy = 52.791999999999994%, Loss = 1.1497324228286743
Epoch: 2383, Batch Gradient Norm: 45.42945666460647
Epoch: 2383, Batch Gradient Norm after: 22.360677678885686
Epoch 2384/10000, Prediction Accuracy = 52.826%, Loss = 1.1659244060516358
Epoch: 2384, Batch Gradient Norm: 39.64537201169099
Epoch: 2384, Batch Gradient Norm after: 22.360676136664683
Epoch 2385/10000, Prediction Accuracy = 52.802%, Loss = 1.1493398666381835
Epoch: 2385, Batch Gradient Norm: 45.39908170888318
Epoch: 2385, Batch Gradient Norm after: 22.36067714931915
Epoch 2386/10000, Prediction Accuracy = 52.83200000000001%, Loss = 1.1654632568359375
Epoch: 2386, Batch Gradient Norm: 39.63365658086221
Epoch: 2386, Batch Gradient Norm after: 22.360675803399133
Epoch 2387/10000, Prediction Accuracy = 52.81600000000001%, Loss = 1.1489365339279174
Epoch: 2387, Batch Gradient Norm: 45.386787580970875
Epoch: 2387, Batch Gradient Norm after: 22.360678735015817
Epoch 2388/10000, Prediction Accuracy = 52.85%, Loss = 1.1650573015213013
Epoch: 2388, Batch Gradient Norm: 39.62559238758509
Epoch: 2388, Batch Gradient Norm after: 22.360677794568787
Epoch 2389/10000, Prediction Accuracy = 52.830000000000005%, Loss = 1.1485519647598266
Epoch: 2389, Batch Gradient Norm: 45.376712988300014
Epoch: 2389, Batch Gradient Norm after: 22.36067751621443
Epoch 2390/10000, Prediction Accuracy = 52.86%, Loss = 1.1646665573120116
Epoch: 2390, Batch Gradient Norm: 39.61868751814112
Epoch: 2390, Batch Gradient Norm after: 22.36067852205629
Epoch 2391/10000, Prediction Accuracy = 52.839999999999996%, Loss = 1.1481759786605834
Epoch: 2391, Batch Gradient Norm: 45.36764225403486
Epoch: 2391, Batch Gradient Norm after: 22.36068034557105
Epoch 2392/10000, Prediction Accuracy = 52.864%, Loss = 1.1642699480056762
Epoch: 2392, Batch Gradient Norm: 39.608167363085016
Epoch: 2392, Batch Gradient Norm after: 22.360678824489874
Epoch 2393/10000, Prediction Accuracy = 52.867999999999995%, Loss = 1.1477895259857178
Epoch: 2393, Batch Gradient Norm: 45.36561613522806
Epoch: 2393, Batch Gradient Norm after: 22.36067858711397
Epoch 2394/10000, Prediction Accuracy = 52.872%, Loss = 1.163902711868286
Epoch: 2394, Batch Gradient Norm: 39.60284880470086
Epoch: 2394, Batch Gradient Norm after: 22.36067454053472
Epoch 2395/10000, Prediction Accuracy = 52.874%, Loss = 1.1474033355712892
Epoch: 2395, Batch Gradient Norm: 45.35530777991652
Epoch: 2395, Batch Gradient Norm after: 22.360679492716383
Epoch 2396/10000, Prediction Accuracy = 52.886%, Loss = 1.1635005712509154
Epoch: 2396, Batch Gradient Norm: 39.594542803607894
Epoch: 2396, Batch Gradient Norm after: 22.360676994216835
Epoch 2397/10000, Prediction Accuracy = 52.872%, Loss = 1.1470117807388305
Epoch: 2397, Batch Gradient Norm: 45.36039444668139
Epoch: 2397, Batch Gradient Norm after: 22.36067794912218
Epoch 2398/10000, Prediction Accuracy = 52.89399999999999%, Loss = 1.163139319419861
Epoch: 2398, Batch Gradient Norm: 39.59148685573928
Epoch: 2398, Batch Gradient Norm after: 22.360674989560383
Epoch 2399/10000, Prediction Accuracy = 52.86800000000001%, Loss = 1.1466322660446167
Epoch: 2399, Batch Gradient Norm: 45.358242706898686
Epoch: 2399, Batch Gradient Norm after: 22.360678850558532
Epoch 2400/10000, Prediction Accuracy = 52.898%, Loss = 1.1627633571624756
Epoch: 2400, Batch Gradient Norm: 39.586731987801706
Epoch: 2400, Batch Gradient Norm after: 22.360675873501474
Epoch 2401/10000, Prediction Accuracy = 52.876%, Loss = 1.1462535619735719
Epoch: 2401, Batch Gradient Norm: 45.34585217520833
Epoch: 2401, Batch Gradient Norm after: 22.36067721283259
Epoch 2402/10000, Prediction Accuracy = 52.906000000000006%, Loss = 1.1623557329177856
Epoch: 2402, Batch Gradient Norm: 39.57715872362617
Epoch: 2402, Batch Gradient Norm after: 22.36067555938244
Epoch 2403/10000, Prediction Accuracy = 52.88000000000001%, Loss = 1.145861840248108
Epoch: 2403, Batch Gradient Norm: 45.34218588327609
Epoch: 2403, Batch Gradient Norm after: 22.36067768183219
Epoch 2404/10000, Prediction Accuracy = 52.919999999999995%, Loss = 1.1619768619537354
Epoch: 2404, Batch Gradient Norm: 39.57143542119148
Epoch: 2404, Batch Gradient Norm after: 22.360674490442317
Epoch 2405/10000, Prediction Accuracy = 52.895999999999994%, Loss = 1.1454834461212158
Epoch: 2405, Batch Gradient Norm: 45.32714506130463
Epoch: 2405, Batch Gradient Norm after: 22.360677962883376
Epoch 2406/10000, Prediction Accuracy = 52.926%, Loss = 1.1615710020065309
Epoch: 2406, Batch Gradient Norm: 39.562063577412665
Epoch: 2406, Batch Gradient Norm after: 22.36067819482296
Epoch 2407/10000, Prediction Accuracy = 52.89200000000001%, Loss = 1.1451042890548706
Epoch: 2407, Batch Gradient Norm: 45.32009835005251
Epoch: 2407, Batch Gradient Norm after: 22.360677702017078
Epoch 2408/10000, Prediction Accuracy = 52.943999999999996%, Loss = 1.1611791610717774
Epoch: 2408, Batch Gradient Norm: 39.55707604726623
Epoch: 2408, Batch Gradient Norm after: 22.360677482208413
Epoch 2409/10000, Prediction Accuracy = 52.9%, Loss = 1.14471595287323
Epoch: 2409, Batch Gradient Norm: 45.29227922135251
Epoch: 2409, Batch Gradient Norm after: 22.360678065344626
Epoch 2410/10000, Prediction Accuracy = 52.944%, Loss = 1.160735297203064
Epoch: 2410, Batch Gradient Norm: 39.545275108502544
Epoch: 2410, Batch Gradient Norm after: 22.36067705696822
Epoch 2411/10000, Prediction Accuracy = 52.907999999999994%, Loss = 1.1443292140960692
Epoch: 2411, Batch Gradient Norm: 45.28060486756998
Epoch: 2411, Batch Gradient Norm after: 22.36067574829417
Epoch 2412/10000, Prediction Accuracy = 52.952%, Loss = 1.1603325843811034
Epoch: 2412, Batch Gradient Norm: 39.53465745194291
Epoch: 2412, Batch Gradient Norm after: 22.360674080300367
Epoch 2413/10000, Prediction Accuracy = 52.92%, Loss = 1.143942904472351
Epoch: 2413, Batch Gradient Norm: 45.26388795323212
Epoch: 2413, Batch Gradient Norm after: 22.36068038059074
Epoch 2414/10000, Prediction Accuracy = 52.965999999999994%, Loss = 1.159921407699585
Epoch: 2414, Batch Gradient Norm: 39.52583720860412
Epoch: 2414, Batch Gradient Norm after: 22.36067536815704
Epoch 2415/10000, Prediction Accuracy = 52.92%, Loss = 1.1435560226440429
Epoch: 2415, Batch Gradient Norm: 45.23219416324482
Epoch: 2415, Batch Gradient Norm after: 22.360678132483166
Epoch 2416/10000, Prediction Accuracy = 52.974000000000004%, Loss = 1.1594674587249756
Epoch: 2416, Batch Gradient Norm: 39.51317799632783
Epoch: 2416, Batch Gradient Norm after: 22.360674751741296
Epoch 2417/10000, Prediction Accuracy = 52.926%, Loss = 1.143165898323059
Epoch: 2417, Batch Gradient Norm: 45.221636768377365
Epoch: 2417, Batch Gradient Norm after: 22.360678401779907
Epoch 2418/10000, Prediction Accuracy = 52.988%, Loss = 1.1590650796890258
Epoch: 2418, Batch Gradient Norm: 39.50605752064775
Epoch: 2418, Batch Gradient Norm after: 22.360677385114805
Epoch 2419/10000, Prediction Accuracy = 52.94%, Loss = 1.1427803993225099
Epoch: 2419, Batch Gradient Norm: 45.20113975545889
Epoch: 2419, Batch Gradient Norm after: 22.360677614381267
Epoch 2420/10000, Prediction Accuracy = 53.0%, Loss = 1.1586511611938477
Epoch: 2420, Batch Gradient Norm: 39.496123678061544
Epoch: 2420, Batch Gradient Norm after: 22.36067688720522
Epoch 2421/10000, Prediction Accuracy = 52.94200000000001%, Loss = 1.1423953294754028
Epoch: 2421, Batch Gradient Norm: 45.19025317639333
Epoch: 2421, Batch Gradient Norm after: 22.36067753870056
Epoch 2422/10000, Prediction Accuracy = 53.013999999999996%, Loss = 1.158254361152649
Epoch: 2422, Batch Gradient Norm: 39.48915396283874
Epoch: 2422, Batch Gradient Norm after: 22.36067679226691
Epoch 2423/10000, Prediction Accuracy = 52.936%, Loss = 1.1420217275619506
Epoch: 2423, Batch Gradient Norm: 45.17224315197302
Epoch: 2423, Batch Gradient Norm after: 22.360675794862786
Epoch 2424/10000, Prediction Accuracy = 53.028%, Loss = 1.1578455686569213
Epoch: 2424, Batch Gradient Norm: 39.478943589855604
Epoch: 2424, Batch Gradient Norm after: 22.360675752673526
Epoch 2425/10000, Prediction Accuracy = 52.938%, Loss = 1.1416407585144044
Epoch: 2425, Batch Gradient Norm: 45.164592702295
Epoch: 2425, Batch Gradient Norm after: 22.36067504182152
Epoch 2426/10000, Prediction Accuracy = 53.038%, Loss = 1.1574657440185547
Epoch: 2426, Batch Gradient Norm: 39.475427522628785
Epoch: 2426, Batch Gradient Norm after: 22.360677658738147
Epoch 2427/10000, Prediction Accuracy = 52.948%, Loss = 1.1412676334381104
Epoch: 2427, Batch Gradient Norm: 45.13927987293612
Epoch: 2427, Batch Gradient Norm after: 22.360677447319965
Epoch 2428/10000, Prediction Accuracy = 53.05800000000001%, Loss = 1.1570374250411988
Epoch: 2428, Batch Gradient Norm: 39.4618867341323
Epoch: 2428, Batch Gradient Norm after: 22.360679834057258
Epoch 2429/10000, Prediction Accuracy = 52.955999999999996%, Loss = 1.1408833026885987
Epoch: 2429, Batch Gradient Norm: 45.11422754096929
Epoch: 2429, Batch Gradient Norm after: 22.36067668582816
Epoch 2430/10000, Prediction Accuracy = 53.076%, Loss = 1.1566158056259155
Epoch: 2430, Batch Gradient Norm: 39.45320720203681
Epoch: 2430, Batch Gradient Norm after: 22.360677557406706
Epoch 2431/10000, Prediction Accuracy = 52.967999999999996%, Loss = 1.1404921293258667
Epoch: 2431, Batch Gradient Norm: 45.078118038719936
Epoch: 2431, Batch Gradient Norm after: 22.360679136786448
Epoch 2432/10000, Prediction Accuracy = 53.092000000000006%, Loss = 1.1561597347259522
Epoch: 2432, Batch Gradient Norm: 39.441024698067636
Epoch: 2432, Batch Gradient Norm after: 22.360678466257944
Epoch 2433/10000, Prediction Accuracy = 52.972%, Loss = 1.1401007890701294
Epoch: 2433, Batch Gradient Norm: 45.04431826233845
Epoch: 2433, Batch Gradient Norm after: 22.36067670673821
Epoch 2434/10000, Prediction Accuracy = 53.105999999999995%, Loss = 1.15570011138916
Epoch: 2434, Batch Gradient Norm: 39.42452307254158
Epoch: 2434, Batch Gradient Norm after: 22.360678233557987
Epoch 2435/10000, Prediction Accuracy = 52.99399999999999%, Loss = 1.1397105216979981
Epoch: 2435, Batch Gradient Norm: 45.00432316254885
Epoch: 2435, Batch Gradient Norm after: 22.360677341644084
Epoch 2436/10000, Prediction Accuracy = 53.122%, Loss = 1.1552224636077881
Epoch: 2436, Batch Gradient Norm: 39.40937486529395
Epoch: 2436, Batch Gradient Norm after: 22.360677322443475
Epoch 2437/10000, Prediction Accuracy = 53.004000000000005%, Loss = 1.139307975769043
Epoch: 2437, Batch Gradient Norm: 44.96390044001171
Epoch: 2437, Batch Gradient Norm after: 22.36067601263112
Epoch 2438/10000, Prediction Accuracy = 53.136%, Loss = 1.1547471523284911
Epoch: 2438, Batch Gradient Norm: 39.39902751998633
Epoch: 2438, Batch Gradient Norm after: 22.36067783369758
Epoch 2439/10000, Prediction Accuracy = 53.013999999999996%, Loss = 1.1389122486114502
Epoch: 2439, Batch Gradient Norm: 44.916249035942535
Epoch: 2439, Batch Gradient Norm after: 22.360677386692487
Epoch 2440/10000, Prediction Accuracy = 53.136%, Loss = 1.154237985610962
Epoch: 2440, Batch Gradient Norm: 39.38268170997362
Epoch: 2440, Batch Gradient Norm after: 22.360677731842376
Epoch 2441/10000, Prediction Accuracy = 53.032000000000004%, Loss = 1.1385223388671875
Epoch: 2441, Batch Gradient Norm: 44.87284545132985
Epoch: 2441, Batch Gradient Norm after: 22.36067933099122
Epoch 2442/10000, Prediction Accuracy = 53.153999999999996%, Loss = 1.153761863708496
Epoch: 2442, Batch Gradient Norm: 39.370237047108944
Epoch: 2442, Batch Gradient Norm after: 22.360677336568934
Epoch 2443/10000, Prediction Accuracy = 53.04%, Loss = 1.1381388902664185
Epoch: 2443, Batch Gradient Norm: 44.82124003920707
Epoch: 2443, Batch Gradient Norm after: 22.360678032417407
Epoch 2444/10000, Prediction Accuracy = 53.15%, Loss = 1.1532548666000366
Epoch: 2444, Batch Gradient Norm: 39.35662137501586
Epoch: 2444, Batch Gradient Norm after: 22.360678088912596
Epoch 2445/10000, Prediction Accuracy = 53.048%, Loss = 1.137753462791443
Epoch: 2445, Batch Gradient Norm: 44.77262491086736
Epoch: 2445, Batch Gradient Norm after: 22.360678292745924
Epoch 2446/10000, Prediction Accuracy = 53.148%, Loss = 1.1527519941329956
Epoch: 2446, Batch Gradient Norm: 39.33941777780763
Epoch: 2446, Batch Gradient Norm after: 22.360678601665494
Epoch 2447/10000, Prediction Accuracy = 53.052%, Loss = 1.1373695850372314
Epoch: 2447, Batch Gradient Norm: 44.73084243394152
Epoch: 2447, Batch Gradient Norm after: 22.360679441710694
Epoch 2448/10000, Prediction Accuracy = 53.136%, Loss = 1.1522846221923828
Epoch: 2448, Batch Gradient Norm: 39.32868899318895
Epoch: 2448, Batch Gradient Norm after: 22.360675730422294
Epoch 2449/10000, Prediction Accuracy = 53.076%, Loss = 1.1369904279708862
Epoch: 2449, Batch Gradient Norm: 44.683254712852644
Epoch: 2449, Batch Gradient Norm after: 22.360678708689402
Epoch 2450/10000, Prediction Accuracy = 53.14399999999999%, Loss = 1.1517936944961549
Epoch: 2450, Batch Gradient Norm: 39.31542427519832
Epoch: 2450, Batch Gradient Norm after: 22.360677605523634
Epoch 2451/10000, Prediction Accuracy = 53.092%, Loss = 1.1365936517715454
Epoch: 2451, Batch Gradient Norm: 44.627815316913534
Epoch: 2451, Batch Gradient Norm after: 22.3606783754891
Epoch 2452/10000, Prediction Accuracy = 53.164%, Loss = 1.1512818574905395
Epoch: 2452, Batch Gradient Norm: 39.30192999176069
Epoch: 2452, Batch Gradient Norm after: 22.360676939559117
Epoch 2453/10000, Prediction Accuracy = 53.11%, Loss = 1.1362054824829102
Epoch: 2453, Batch Gradient Norm: 44.57836733500542
Epoch: 2453, Batch Gradient Norm after: 22.360678199218434
Epoch 2454/10000, Prediction Accuracy = 53.181999999999995%, Loss = 1.1507805585861206
Epoch: 2454, Batch Gradient Norm: 39.28986165087573
Epoch: 2454, Batch Gradient Norm after: 22.36067801384737
Epoch 2455/10000, Prediction Accuracy = 53.124%, Loss = 1.135828399658203
Epoch: 2455, Batch Gradient Norm: 44.538289121143556
Epoch: 2455, Batch Gradient Norm after: 22.360679871612966
Epoch 2456/10000, Prediction Accuracy = 53.186%, Loss = 1.150311779975891
Epoch: 2456, Batch Gradient Norm: 39.28047504149424
Epoch: 2456, Batch Gradient Norm after: 22.360675672679843
Epoch 2457/10000, Prediction Accuracy = 53.124%, Loss = 1.13544704914093
Epoch: 2457, Batch Gradient Norm: 44.48627966467755
Epoch: 2457, Batch Gradient Norm after: 22.36067554322239
Epoch 2458/10000, Prediction Accuracy = 53.194%, Loss = 1.149814200401306
Epoch: 2458, Batch Gradient Norm: 39.26622800522888
Epoch: 2458, Batch Gradient Norm after: 22.360674304852882
Epoch 2459/10000, Prediction Accuracy = 53.129999999999995%, Loss = 1.135070013999939
Epoch: 2459, Batch Gradient Norm: 44.43574971591925
Epoch: 2459, Batch Gradient Norm after: 22.36067739874486
Epoch 2460/10000, Prediction Accuracy = 53.20399999999999%, Loss = 1.1493166446685792
Epoch: 2460, Batch Gradient Norm: 39.25218264426989
Epoch: 2460, Batch Gradient Norm after: 22.36067625365981
Epoch 2461/10000, Prediction Accuracy = 53.146%, Loss = 1.1346885919570924
Epoch: 2461, Batch Gradient Norm: 44.39461122413588
Epoch: 2461, Batch Gradient Norm after: 22.360676888871097
Epoch 2462/10000, Prediction Accuracy = 53.224000000000004%, Loss = 1.148850154876709
Epoch: 2462, Batch Gradient Norm: 39.24187391060282
Epoch: 2462, Batch Gradient Norm after: 22.36067672593771
Epoch 2463/10000, Prediction Accuracy = 53.160000000000004%, Loss = 1.1343159675598145
Epoch: 2463, Batch Gradient Norm: 44.352418179009476
Epoch: 2463, Batch Gradient Norm after: 22.360677839351833
Epoch 2464/10000, Prediction Accuracy = 53.23%, Loss = 1.148383617401123
Epoch: 2464, Batch Gradient Norm: 39.22920741606857
Epoch: 2464, Batch Gradient Norm after: 22.360678599333596
Epoch 2465/10000, Prediction Accuracy = 53.166%, Loss = 1.1339443683624268
Epoch: 2465, Batch Gradient Norm: 44.312690474097536
Epoch: 2465, Batch Gradient Norm after: 22.360674971342167
Epoch 2466/10000, Prediction Accuracy = 53.233999999999995%, Loss = 1.1479256629943848
Epoch: 2466, Batch Gradient Norm: 39.21905827051412
Epoch: 2466, Batch Gradient Norm after: 22.360678700922204
Epoch 2467/10000, Prediction Accuracy = 53.17199999999999%, Loss = 1.1335691213607788
Epoch: 2467, Batch Gradient Norm: 44.270607638063865
Epoch: 2467, Batch Gradient Norm after: 22.36067723461286
Epoch 2468/10000, Prediction Accuracy = 53.24400000000001%, Loss = 1.1474654912948608
Epoch: 2468, Batch Gradient Norm: 39.20889055108994
Epoch: 2468, Batch Gradient Norm after: 22.360678239588058
Epoch 2469/10000, Prediction Accuracy = 53.188%, Loss = 1.1332014560699464
Epoch: 2469, Batch Gradient Norm: 44.22937919928336
Epoch: 2469, Batch Gradient Norm after: 22.36067891124366
Epoch 2470/10000, Prediction Accuracy = 53.25%, Loss = 1.1470093250274658
Epoch: 2470, Batch Gradient Norm: 39.19647662105322
Epoch: 2470, Batch Gradient Norm after: 22.3606771722907
Epoch 2471/10000, Prediction Accuracy = 53.184000000000005%, Loss = 1.1328372955322266
Epoch: 2471, Batch Gradient Norm: 44.193810936662686
Epoch: 2471, Batch Gradient Norm after: 22.360678436445976
Epoch 2472/10000, Prediction Accuracy = 53.25600000000001%, Loss = 1.1465610980987548
Epoch: 2472, Batch Gradient Norm: 39.18738703884463
Epoch: 2472, Batch Gradient Norm after: 22.360676578111757
Epoch 2473/10000, Prediction Accuracy = 53.194%, Loss = 1.13247811794281
Epoch: 2473, Batch Gradient Norm: 44.16424165046906
Epoch: 2473, Batch Gradient Norm after: 22.360679014237647
Epoch 2474/10000, Prediction Accuracy = 53.26400000000001%, Loss = 1.1461344957351685
Epoch: 2474, Batch Gradient Norm: 39.177946733016675
Epoch: 2474, Batch Gradient Norm after: 22.36067915490835
Epoch 2475/10000, Prediction Accuracy = 53.21%, Loss = 1.1321168184280395
Epoch: 2475, Batch Gradient Norm: 44.136118880896966
Epoch: 2475, Batch Gradient Norm after: 22.360677984474034
Epoch 2476/10000, Prediction Accuracy = 53.27%, Loss = 1.145720362663269
Epoch: 2476, Batch Gradient Norm: 39.170249429133655
Epoch: 2476, Batch Gradient Norm after: 22.36067796053972
Epoch 2477/10000, Prediction Accuracy = 53.224000000000004%, Loss = 1.1317539453506469
Epoch: 2477, Batch Gradient Norm: 44.10581879164192
Epoch: 2477, Batch Gradient Norm after: 22.360677918965713
Epoch 2478/10000, Prediction Accuracy = 53.274%, Loss = 1.1452932357788086
Epoch: 2478, Batch Gradient Norm: 39.161607310605916
Epoch: 2478, Batch Gradient Norm after: 22.360677199934965
Epoch 2479/10000, Prediction Accuracy = 53.24400000000001%, Loss = 1.1313916444778442
Epoch: 2479, Batch Gradient Norm: 44.075957205917994
Epoch: 2479, Batch Gradient Norm after: 22.36067877958808
Epoch 2480/10000, Prediction Accuracy = 53.29%, Loss = 1.144873023033142
Epoch: 2480, Batch Gradient Norm: 39.1533050281454
Epoch: 2480, Batch Gradient Norm after: 22.36067718969113
Epoch 2481/10000, Prediction Accuracy = 53.25600000000001%, Loss = 1.1310333251953124
Epoch: 2481, Batch Gradient Norm: 44.05379078715683
Epoch: 2481, Batch Gradient Norm after: 22.36067736545187
Epoch 2482/10000, Prediction Accuracy = 53.303999999999995%, Loss = 1.1444750070571899
Epoch: 2482, Batch Gradient Norm: 39.14508019696655
Epoch: 2482, Batch Gradient Norm after: 22.360676556019243
Epoch 2483/10000, Prediction Accuracy = 53.257999999999996%, Loss = 1.1306755542755127
Epoch: 2483, Batch Gradient Norm: 44.033850584047414
Epoch: 2483, Batch Gradient Norm after: 22.36067672140299
Epoch 2484/10000, Prediction Accuracy = 53.318%, Loss = 1.1440930604934691
Epoch: 2484, Batch Gradient Norm: 39.13640278550052
Epoch: 2484, Batch Gradient Norm after: 22.360676421539125
Epoch 2485/10000, Prediction Accuracy = 53.278%, Loss = 1.130320143699646
Epoch: 2485, Batch Gradient Norm: 44.020003165293936
Epoch: 2485, Batch Gradient Norm after: 22.360679540715918
Epoch 2486/10000, Prediction Accuracy = 53.330000000000005%, Loss = 1.1437221050262452
Epoch: 2486, Batch Gradient Norm: 39.12829874385723
Epoch: 2486, Batch Gradient Norm after: 22.36067828797749
Epoch 2487/10000, Prediction Accuracy = 53.282%, Loss = 1.129966402053833
Epoch: 2487, Batch Gradient Norm: 44.01217037888402
Epoch: 2487, Batch Gradient Norm after: 22.36067741588907
Epoch 2488/10000, Prediction Accuracy = 53.348%, Loss = 1.1433760404586792
Epoch: 2488, Batch Gradient Norm: 39.12000889000255
Epoch: 2488, Batch Gradient Norm after: 22.360677425193714
Epoch 2489/10000, Prediction Accuracy = 53.302%, Loss = 1.129604125022888
Epoch: 2489, Batch Gradient Norm: 44.00138983441164
Epoch: 2489, Batch Gradient Norm after: 22.360678259621412
Epoch 2490/10000, Prediction Accuracy = 53.36600000000001%, Loss = 1.1430140733718872
Epoch: 2490, Batch Gradient Norm: 39.11237948793709
Epoch: 2490, Batch Gradient Norm after: 22.360677863236415
Epoch 2491/10000, Prediction Accuracy = 53.314%, Loss = 1.1292521715164185
Epoch: 2491, Batch Gradient Norm: 43.99114175070155
Epoch: 2491, Batch Gradient Norm after: 22.360677821570523
Epoch 2492/10000, Prediction Accuracy = 53.39200000000001%, Loss = 1.1426476001739503
Epoch: 2492, Batch Gradient Norm: 39.1044494843052
Epoch: 2492, Batch Gradient Norm after: 22.36067591331675
Epoch 2493/10000, Prediction Accuracy = 53.324%, Loss = 1.1288958072662354
Epoch: 2493, Batch Gradient Norm: 43.98535903272101
Epoch: 2493, Batch Gradient Norm after: 22.360677859519818
Epoch 2494/10000, Prediction Accuracy = 53.40400000000001%, Loss = 1.142298698425293
Epoch: 2494, Batch Gradient Norm: 39.09790821908643
Epoch: 2494, Batch Gradient Norm after: 22.360678830083483
Epoch 2495/10000, Prediction Accuracy = 53.336%, Loss = 1.128546380996704
Epoch: 2495, Batch Gradient Norm: 43.98278891526476
Epoch: 2495, Batch Gradient Norm after: 22.3606781185907
Epoch 2496/10000, Prediction Accuracy = 53.412%, Loss = 1.1419633865356444
Epoch: 2496, Batch Gradient Norm: 39.08816621912326
Epoch: 2496, Batch Gradient Norm after: 22.360676690017485
Epoch 2497/10000, Prediction Accuracy = 53.348%, Loss = 1.1281920433044434
Epoch: 2497, Batch Gradient Norm: 43.97712445232307
Epoch: 2497, Batch Gradient Norm after: 22.360676760045536
Epoch 2498/10000, Prediction Accuracy = 53.406000000000006%, Loss = 1.1416136980056764
Epoch: 2498, Batch Gradient Norm: 39.07933832862272
Epoch: 2498, Batch Gradient Norm after: 22.36067627185945
Epoch 2499/10000, Prediction Accuracy = 53.352%, Loss = 1.127840518951416
Epoch: 2499, Batch Gradient Norm: 43.97131590421285
Epoch: 2499, Batch Gradient Norm after: 22.360678810210324
Epoch 2500/10000, Prediction Accuracy = 53.4%, Loss = 1.1412672758102418
Epoch: 2500, Batch Gradient Norm: 39.070753464194475
Epoch: 2500, Batch Gradient Norm after: 22.360678407401693
Epoch 2501/10000, Prediction Accuracy = 53.376%, Loss = 1.127491021156311
Epoch: 2501, Batch Gradient Norm: 43.9654750735958
Epoch: 2501, Batch Gradient Norm after: 22.36067851034062
Epoch 2502/10000, Prediction Accuracy = 53.403999999999996%, Loss = 1.1409186124801636
Epoch: 2502, Batch Gradient Norm: 39.06465862990789
Epoch: 2502, Batch Gradient Norm after: 22.36067789621837
Epoch 2503/10000, Prediction Accuracy = 53.391999999999996%, Loss = 1.127138352394104
Epoch: 2503, Batch Gradient Norm: 43.963064646311366
Epoch: 2503, Batch Gradient Norm after: 22.360679877946136
Epoch 2504/10000, Prediction Accuracy = 53.42199999999999%, Loss = 1.1405808687210084
Epoch: 2504, Batch Gradient Norm: 39.058730276626996
Epoch: 2504, Batch Gradient Norm after: 22.360674570083823
Epoch 2505/10000, Prediction Accuracy = 53.406000000000006%, Loss = 1.1267855167388916
Epoch: 2505, Batch Gradient Norm: 43.94952484569891
Epoch: 2505, Batch Gradient Norm after: 22.36067838968773
Epoch 2506/10000, Prediction Accuracy = 53.428%, Loss = 1.14020516872406
Epoch: 2506, Batch Gradient Norm: 39.05175433639203
Epoch: 2506, Batch Gradient Norm after: 22.36067599056888
Epoch 2507/10000, Prediction Accuracy = 53.41600000000001%, Loss = 1.1264360427856446
Epoch: 2507, Batch Gradient Norm: 43.93641948481155
Epoch: 2507, Batch Gradient Norm after: 22.36067854906649
Epoch 2508/10000, Prediction Accuracy = 53.448%, Loss = 1.139828586578369
Epoch: 2508, Batch Gradient Norm: 39.0455147474343
Epoch: 2508, Batch Gradient Norm after: 22.360676664313203
Epoch 2509/10000, Prediction Accuracy = 53.432%, Loss = 1.1260814666748047
Epoch: 2509, Batch Gradient Norm: 43.92920555011473
Epoch: 2509, Batch Gradient Norm after: 22.360677673173477
Epoch 2510/10000, Prediction Accuracy = 53.458000000000006%, Loss = 1.1394704341888429
Epoch: 2510, Batch Gradient Norm: 39.04019471950873
Epoch: 2510, Batch Gradient Norm after: 22.360675415982307
Epoch 2511/10000, Prediction Accuracy = 53.44%, Loss = 1.1257300853729248
Epoch: 2511, Batch Gradient Norm: 43.91120141334699
Epoch: 2511, Batch Gradient Norm after: 22.360679283779227
Epoch 2512/10000, Prediction Accuracy = 53.472%, Loss = 1.1390852212905884
Epoch: 2512, Batch Gradient Norm: 39.0315907024941
Epoch: 2512, Batch Gradient Norm after: 22.360677080526987
Epoch 2513/10000, Prediction Accuracy = 53.458000000000006%, Loss = 1.1253752708435059
Epoch: 2513, Batch Gradient Norm: 43.88946985202312
Epoch: 2513, Batch Gradient Norm after: 22.360677517675033
Epoch 2514/10000, Prediction Accuracy = 53.492%, Loss = 1.1386893749237061
Epoch: 2514, Batch Gradient Norm: 39.02103886808436
Epoch: 2514, Batch Gradient Norm after: 22.36067765789908
Epoch 2515/10000, Prediction Accuracy = 53.468%, Loss = 1.1250201225280763
Epoch: 2515, Batch Gradient Norm: 43.87492762142027
Epoch: 2515, Batch Gradient Norm after: 22.360678673698185
Epoch 2516/10000, Prediction Accuracy = 53.50599999999999%, Loss = 1.1383171558380127
Epoch: 2516, Batch Gradient Norm: 39.01568506981517
Epoch: 2516, Batch Gradient Norm after: 22.360677181570402
Epoch 2517/10000, Prediction Accuracy = 53.474000000000004%, Loss = 1.124671506881714
Epoch: 2517, Batch Gradient Norm: 43.8475853462333
Epoch: 2517, Batch Gradient Norm after: 22.360678834420924
Epoch 2518/10000, Prediction Accuracy = 53.512%, Loss = 1.137906289100647
Epoch: 2518, Batch Gradient Norm: 39.00637222036146
Epoch: 2518, Batch Gradient Norm after: 22.36068106441069
Epoch 2519/10000, Prediction Accuracy = 53.480000000000004%, Loss = 1.1243157625198363
Epoch: 2519, Batch Gradient Norm: 43.8178213162469
Epoch: 2519, Batch Gradient Norm after: 22.360677036687928
Epoch 2520/10000, Prediction Accuracy = 53.516%, Loss = 1.137495470046997
Epoch: 2520, Batch Gradient Norm: 38.99662754312012
Epoch: 2520, Batch Gradient Norm after: 22.360677611036795
Epoch 2521/10000, Prediction Accuracy = 53.480000000000004%, Loss = 1.1239691019058227
Epoch: 2521, Batch Gradient Norm: 43.79833033736805
Epoch: 2521, Batch Gradient Norm after: 22.360677793193776
Epoch 2522/10000, Prediction Accuracy = 53.524%, Loss = 1.1371102809906006
Epoch: 2522, Batch Gradient Norm: 38.989686676293424
Epoch: 2522, Batch Gradient Norm after: 22.360678075354727
Epoch 2523/10000, Prediction Accuracy = 53.482000000000006%, Loss = 1.1236187934875488
Epoch: 2523, Batch Gradient Norm: 43.77230341498202
Epoch: 2523, Batch Gradient Norm after: 22.360676560082595
Epoch 2524/10000, Prediction Accuracy = 53.538%, Loss = 1.1366975784301758
Epoch: 2524, Batch Gradient Norm: 38.9811624533092
Epoch: 2524, Batch Gradient Norm after: 22.36067663148277
Epoch 2525/10000, Prediction Accuracy = 53.496%, Loss = 1.1232649564743042
Epoch: 2525, Batch Gradient Norm: 43.753338448809664
Epoch: 2525, Batch Gradient Norm after: 22.36068072648205
Epoch 2526/10000, Prediction Accuracy = 53.55%, Loss = 1.136306929588318
Epoch: 2526, Batch Gradient Norm: 38.974219464098056
Epoch: 2526, Batch Gradient Norm after: 22.36067628699208
Epoch 2527/10000, Prediction Accuracy = 53.504%, Loss = 1.1229135274887085
Epoch: 2527, Batch Gradient Norm: 43.72942877226055
Epoch: 2527, Batch Gradient Norm after: 22.360678929293435
Epoch 2528/10000, Prediction Accuracy = 53.553999999999995%, Loss = 1.1359068155288696
Epoch: 2528, Batch Gradient Norm: 38.966610926726105
Epoch: 2528, Batch Gradient Norm after: 22.36067563099183
Epoch 2529/10000, Prediction Accuracy = 53.510000000000005%, Loss = 1.1225712537765502
Epoch: 2529, Batch Gradient Norm: 43.701957732077645
Epoch: 2529, Batch Gradient Norm after: 22.360677531421402
Epoch 2530/10000, Prediction Accuracy = 53.553999999999995%, Loss = 1.1355049848556518
Epoch: 2530, Batch Gradient Norm: 38.95830506262494
Epoch: 2530, Batch Gradient Norm after: 22.36067773587825
Epoch 2531/10000, Prediction Accuracy = 53.53399999999999%, Loss = 1.1222182273864747
Epoch: 2531, Batch Gradient Norm: 43.68123550977633
Epoch: 2531, Batch Gradient Norm after: 22.360678930007783
Epoch 2532/10000, Prediction Accuracy = 53.56999999999999%, Loss = 1.1351165771484375
Epoch: 2532, Batch Gradient Norm: 38.94982612607076
Epoch: 2532, Batch Gradient Norm after: 22.360677597014682
Epoch 2533/10000, Prediction Accuracy = 53.548%, Loss = 1.1218698978424073
Epoch: 2533, Batch Gradient Norm: 43.65617640117038
Epoch: 2533, Batch Gradient Norm after: 22.360678883462977
Epoch 2534/10000, Prediction Accuracy = 53.57199999999999%, Loss = 1.1347148656845092
Epoch: 2534, Batch Gradient Norm: 38.94451579120924
Epoch: 2534, Batch Gradient Norm after: 22.360678112527587
Epoch 2535/10000, Prediction Accuracy = 53.562%, Loss = 1.1215265989303589
Epoch: 2535, Batch Gradient Norm: 43.64283701374487
Epoch: 2535, Batch Gradient Norm after: 22.36067778627254
Epoch 2536/10000, Prediction Accuracy = 53.574%, Loss = 1.1343479394912719
Epoch: 2536, Batch Gradient Norm: 38.93731310263509
Epoch: 2536, Batch Gradient Norm after: 22.360678663544583
Epoch 2537/10000, Prediction Accuracy = 53.572%, Loss = 1.1211769819259643
Epoch: 2537, Batch Gradient Norm: 43.627430509314465
Epoch: 2537, Batch Gradient Norm after: 22.36067912894553
Epoch 2538/10000, Prediction Accuracy = 53.581999999999994%, Loss = 1.1339730978012086
Epoch: 2538, Batch Gradient Norm: 38.92861332574761
Epoch: 2538, Batch Gradient Norm after: 22.3606772340395
Epoch 2539/10000, Prediction Accuracy = 53.576%, Loss = 1.1208290815353394
Epoch: 2539, Batch Gradient Norm: 43.619731993953714
Epoch: 2539, Batch Gradient Norm after: 22.360676838071857
Epoch 2540/10000, Prediction Accuracy = 53.602%, Loss = 1.1336275100708009
Epoch: 2540, Batch Gradient Norm: 38.92082142872146
Epoch: 2540, Batch Gradient Norm after: 22.360678750627216
Epoch 2541/10000, Prediction Accuracy = 53.56600000000001%, Loss = 1.1204713821411132
Epoch: 2541, Batch Gradient Norm: 43.595346371089526
Epoch: 2541, Batch Gradient Norm after: 22.36067660852539
Epoch 2542/10000, Prediction Accuracy = 53.614%, Loss = 1.1332490205764771
Epoch: 2542, Batch Gradient Norm: 38.911459082966715
Epoch: 2542, Batch Gradient Norm after: 22.36067765602341
Epoch 2543/10000, Prediction Accuracy = 53.584%, Loss = 1.1201281309127809
Epoch: 2543, Batch Gradient Norm: 43.577052837196256
Epoch: 2543, Batch Gradient Norm after: 22.360676358602547
Epoch 2544/10000, Prediction Accuracy = 53.624%, Loss = 1.13286292552948
Epoch: 2544, Batch Gradient Norm: 38.90807528851245
Epoch: 2544, Batch Gradient Norm after: 22.360675957039046
Epoch 2545/10000, Prediction Accuracy = 53.576%, Loss = 1.1197834491729737
Epoch: 2545, Batch Gradient Norm: 43.54490895430365
Epoch: 2545, Batch Gradient Norm after: 22.360678341988148
Epoch 2546/10000, Prediction Accuracy = 53.622%, Loss = 1.1324499368667602
Epoch: 2546, Batch Gradient Norm: 38.89900956694722
Epoch: 2546, Batch Gradient Norm after: 22.360679356842123
Epoch 2547/10000, Prediction Accuracy = 53.589999999999996%, Loss = 1.119439172744751
Epoch: 2547, Batch Gradient Norm: 43.52590747865546
Epoch: 2547, Batch Gradient Norm after: 22.360679408214757
Epoch 2548/10000, Prediction Accuracy = 53.638%, Loss = 1.1320661544799804
Epoch: 2548, Batch Gradient Norm: 38.894681095774075
Epoch: 2548, Batch Gradient Norm after: 22.36067718971514
Epoch 2549/10000, Prediction Accuracy = 53.617999999999995%, Loss = 1.1190893650054932
Epoch: 2549, Batch Gradient Norm: 43.50915340809405
Epoch: 2549, Batch Gradient Norm after: 22.36067609537369
Epoch 2550/10000, Prediction Accuracy = 53.66199999999999%, Loss = 1.1316859006881714
Epoch: 2550, Batch Gradient Norm: 38.88697112805574
Epoch: 2550, Batch Gradient Norm after: 22.360676179439807
Epoch 2551/10000, Prediction Accuracy = 53.629999999999995%, Loss = 1.118746829032898
Epoch: 2551, Batch Gradient Norm: 43.49197669272077
Epoch: 2551, Batch Gradient Norm after: 22.360677880653082
Epoch 2552/10000, Prediction Accuracy = 53.674%, Loss = 1.1313195705413819
Epoch: 2552, Batch Gradient Norm: 38.87865927739613
Epoch: 2552, Batch Gradient Norm after: 22.36067735087154
Epoch 2553/10000, Prediction Accuracy = 53.63800000000001%, Loss = 1.1184073686599731
Epoch: 2553, Batch Gradient Norm: 43.48143344351603
Epoch: 2553, Batch Gradient Norm after: 22.360677240742657
Epoch 2554/10000, Prediction Accuracy = 53.67%, Loss = 1.130960512161255
Epoch: 2554, Batch Gradient Norm: 38.873645318883376
Epoch: 2554, Batch Gradient Norm after: 22.360678151837273
Epoch 2555/10000, Prediction Accuracy = 53.646%, Loss = 1.1180718421936036
Epoch: 2555, Batch Gradient Norm: 43.45859107783206
Epoch: 2555, Batch Gradient Norm after: 22.36067653964787
Epoch 2556/10000, Prediction Accuracy = 53.67999999999999%, Loss = 1.1305738687515259
Epoch: 2556, Batch Gradient Norm: 38.864770205036095
Epoch: 2556, Batch Gradient Norm after: 22.360676470781474
Epoch 2557/10000, Prediction Accuracy = 53.668000000000006%, Loss = 1.1177340030670166
Epoch: 2557, Batch Gradient Norm: 43.4410042305839
Epoch: 2557, Batch Gradient Norm after: 22.36067727094579
Epoch 2558/10000, Prediction Accuracy = 53.68599999999999%, Loss = 1.1302091121673583
Epoch: 2558, Batch Gradient Norm: 38.85660312349815
Epoch: 2558, Batch Gradient Norm after: 22.36067636172225
Epoch 2559/10000, Prediction Accuracy = 53.67999999999999%, Loss = 1.117399549484253
Epoch: 2559, Batch Gradient Norm: 43.42095631103974
Epoch: 2559, Batch Gradient Norm after: 22.360679005871447
Epoch 2560/10000, Prediction Accuracy = 53.698%, Loss = 1.1298287153244018
Epoch: 2560, Batch Gradient Norm: 38.84918253118986
Epoch: 2560, Batch Gradient Norm after: 22.360677207758716
Epoch 2561/10000, Prediction Accuracy = 53.698%, Loss = 1.1170579433441161
Epoch: 2561, Batch Gradient Norm: 43.400827312857025
Epoch: 2561, Batch Gradient Norm after: 22.360677523736577
Epoch 2562/10000, Prediction Accuracy = 53.71%, Loss = 1.1294469594955445
Epoch: 2562, Batch Gradient Norm: 38.83919388122564
Epoch: 2562, Batch Gradient Norm after: 22.360675168666027
Epoch 2563/10000, Prediction Accuracy = 53.712%, Loss = 1.1167158365249634
Epoch: 2563, Batch Gradient Norm: 43.38735937290632
Epoch: 2563, Batch Gradient Norm after: 22.360678467767475
Epoch 2564/10000, Prediction Accuracy = 53.722%, Loss = 1.1290907859802246
Epoch: 2564, Batch Gradient Norm: 38.833292494385326
Epoch: 2564, Batch Gradient Norm after: 22.360674356288385
Epoch 2565/10000, Prediction Accuracy = 53.727999999999994%, Loss = 1.1163777351379394
Epoch: 2565, Batch Gradient Norm: 43.371716177401964
Epoch: 2565, Batch Gradient Norm after: 22.360675419713626
Epoch 2566/10000, Prediction Accuracy = 53.736000000000004%, Loss = 1.1287266731262207
Epoch: 2566, Batch Gradient Norm: 38.82659078116831
Epoch: 2566, Batch Gradient Norm after: 22.36067539248842
Epoch 2567/10000, Prediction Accuracy = 53.736000000000004%, Loss = 1.1160376787185669
Epoch: 2567, Batch Gradient Norm: 43.361029729446244
Epoch: 2567, Batch Gradient Norm after: 22.36067749115667
Epoch 2568/10000, Prediction Accuracy = 53.74400000000001%, Loss = 1.1283751010894776
Epoch: 2568, Batch Gradient Norm: 38.81921321660381
Epoch: 2568, Batch Gradient Norm after: 22.36067708262897
Epoch 2569/10000, Prediction Accuracy = 53.746%, Loss = 1.1156961917877197
Epoch: 2569, Batch Gradient Norm: 43.34134584276408
Epoch: 2569, Batch Gradient Norm after: 22.36067639242586
Epoch 2570/10000, Prediction Accuracy = 53.762%, Loss = 1.1280003309249877
Epoch: 2570, Batch Gradient Norm: 38.81061040818032
Epoch: 2570, Batch Gradient Norm after: 22.36067625557993
Epoch 2571/10000, Prediction Accuracy = 53.754%, Loss = 1.1153531789779663
Epoch: 2571, Batch Gradient Norm: 43.33396185019087
Epoch: 2571, Batch Gradient Norm after: 22.360675072709743
Epoch 2572/10000, Prediction Accuracy = 53.76800000000001%, Loss = 1.1276539325714112
Epoch: 2572, Batch Gradient Norm: 38.802433770526875
Epoch: 2572, Batch Gradient Norm after: 22.36067440950772
Epoch 2573/10000, Prediction Accuracy = 53.762%, Loss = 1.1150106191635132
Epoch: 2573, Batch Gradient Norm: 43.32223734390828
Epoch: 2573, Batch Gradient Norm after: 22.36067827864867
Epoch 2574/10000, Prediction Accuracy = 53.766%, Loss = 1.1272987604141236
Epoch: 2574, Batch Gradient Norm: 38.79782226336812
Epoch: 2574, Batch Gradient Norm after: 22.36067595910613
Epoch 2575/10000, Prediction Accuracy = 53.762%, Loss = 1.1146694660186767
Epoch: 2575, Batch Gradient Norm: 43.302891608140825
Epoch: 2575, Batch Gradient Norm after: 22.36067887723451
Epoch 2576/10000, Prediction Accuracy = 53.772000000000006%, Loss = 1.1269315242767335
Epoch: 2576, Batch Gradient Norm: 38.78905633387749
Epoch: 2576, Batch Gradient Norm after: 22.36067623089243
Epoch 2577/10000, Prediction Accuracy = 53.76800000000001%, Loss = 1.1143360376358031
Epoch: 2577, Batch Gradient Norm: 43.292153507989845
Epoch: 2577, Batch Gradient Norm after: 22.36067836655125
Epoch 2578/10000, Prediction Accuracy = 53.778%, Loss = 1.1265812397003174
Epoch: 2578, Batch Gradient Norm: 38.78222500456832
Epoch: 2578, Batch Gradient Norm after: 22.360675735886026
Epoch 2579/10000, Prediction Accuracy = 53.784000000000006%, Loss = 1.1139979839324952
Epoch: 2579, Batch Gradient Norm: 43.28214442663848
Epoch: 2579, Batch Gradient Norm after: 22.360677518138104
Epoch 2580/10000, Prediction Accuracy = 53.786%, Loss = 1.1262322902679442
Epoch: 2580, Batch Gradient Norm: 38.77585234294861
Epoch: 2580, Batch Gradient Norm after: 22.36067627581129
Epoch 2581/10000, Prediction Accuracy = 53.794000000000004%, Loss = 1.1136642456054688
Epoch: 2581, Batch Gradient Norm: 43.26272132830035
Epoch: 2581, Batch Gradient Norm after: 22.36067658729462
Epoch 2582/10000, Prediction Accuracy = 53.79799999999999%, Loss = 1.1258538246154786
Epoch: 2582, Batch Gradient Norm: 38.766077635185844
Epoch: 2582, Batch Gradient Norm after: 22.36067531340117
Epoch 2583/10000, Prediction Accuracy = 53.79600000000001%, Loss = 1.1133315324783326
Epoch: 2583, Batch Gradient Norm: 43.24871190907326
Epoch: 2583, Batch Gradient Norm after: 22.36067818213055
Epoch 2584/10000, Prediction Accuracy = 53.802%, Loss = 1.1254957437515258
Epoch: 2584, Batch Gradient Norm: 38.758294894845214
Epoch: 2584, Batch Gradient Norm after: 22.360677134785774
Epoch 2585/10000, Prediction Accuracy = 53.798%, Loss = 1.1129893064498901
Epoch: 2585, Batch Gradient Norm: 43.23237377114966
Epoch: 2585, Batch Gradient Norm after: 22.360676048743013
Epoch 2586/10000, Prediction Accuracy = 53.814%, Loss = 1.1251357555389405
Epoch: 2586, Batch Gradient Norm: 38.75095891016876
Epoch: 2586, Batch Gradient Norm after: 22.36067741380063
Epoch 2587/10000, Prediction Accuracy = 53.818%, Loss = 1.1126479625701904
Epoch: 2587, Batch Gradient Norm: 43.21065402137164
Epoch: 2587, Batch Gradient Norm after: 22.36067451172603
Epoch 2588/10000, Prediction Accuracy = 53.826%, Loss = 1.1247647523880004
Epoch: 2588, Batch Gradient Norm: 38.743516264094445
Epoch: 2588, Batch Gradient Norm after: 22.360675392217843
Epoch 2589/10000, Prediction Accuracy = 53.85%, Loss = 1.1123152732849122
Epoch: 2589, Batch Gradient Norm: 43.199467011636884
Epoch: 2589, Batch Gradient Norm after: 22.360679076617227
Epoch 2590/10000, Prediction Accuracy = 53.83%, Loss = 1.1244106531143188
Epoch: 2590, Batch Gradient Norm: 38.737933722854166
Epoch: 2590, Batch Gradient Norm after: 22.360676136739272
Epoch 2591/10000, Prediction Accuracy = 53.876%, Loss = 1.1119762659072876
Epoch: 2591, Batch Gradient Norm: 43.18500255345599
Epoch: 2591, Batch Gradient Norm after: 22.360676978450122
Epoch 2592/10000, Prediction Accuracy = 53.846000000000004%, Loss = 1.124054455757141
Epoch: 2592, Batch Gradient Norm: 38.731885594033635
Epoch: 2592, Batch Gradient Norm after: 22.36067828389375
Epoch 2593/10000, Prediction Accuracy = 53.888%, Loss = 1.1116398096084594
Epoch: 2593, Batch Gradient Norm: 43.178962127797725
Epoch: 2593, Batch Gradient Norm after: 22.360677280251895
Epoch 2594/10000, Prediction Accuracy = 53.85600000000001%, Loss = 1.1237146377563476
Epoch: 2594, Batch Gradient Norm: 38.725111921276664
Epoch: 2594, Batch Gradient Norm after: 22.360677000047158
Epoch 2595/10000, Prediction Accuracy = 53.896%, Loss = 1.1113044977188111
Epoch: 2595, Batch Gradient Norm: 43.174499647003636
Epoch: 2595, Batch Gradient Norm after: 22.360679286164423
Epoch 2596/10000, Prediction Accuracy = 53.86%, Loss = 1.1233906745910645
Epoch: 2596, Batch Gradient Norm: 38.717626534663516
Epoch: 2596, Batch Gradient Norm after: 22.36067532671588
Epoch 2597/10000, Prediction Accuracy = 53.902%, Loss = 1.1109654664993287
Epoch: 2597, Batch Gradient Norm: 43.163734437034115
Epoch: 2597, Batch Gradient Norm after: 22.360676823933673
Epoch 2598/10000, Prediction Accuracy = 53.876%, Loss = 1.12304744720459
Epoch: 2598, Batch Gradient Norm: 38.70986448973119
Epoch: 2598, Batch Gradient Norm after: 22.360675727003226
Epoch 2599/10000, Prediction Accuracy = 53.91199999999999%, Loss = 1.1106313705444335
Epoch: 2599, Batch Gradient Norm: 43.16397103890343
Epoch: 2599, Batch Gradient Norm after: 22.360675948834487
Epoch 2600/10000, Prediction Accuracy = 53.88399999999999%, Loss = 1.1227262973785401
Epoch: 2600, Batch Gradient Norm: 38.70175818267093
Epoch: 2600, Batch Gradient Norm after: 22.360677750570463
Epoch 2601/10000, Prediction Accuracy = 53.92%, Loss = 1.1102979898452758
Epoch: 2601, Batch Gradient Norm: 43.16817750083174
Epoch: 2601, Batch Gradient Norm after: 22.360676064530825
Epoch 2602/10000, Prediction Accuracy = 53.88599999999999%, Loss = 1.122418260574341
Epoch: 2602, Batch Gradient Norm: 38.69384094343106
Epoch: 2602, Batch Gradient Norm after: 22.360674849268428
Epoch 2603/10000, Prediction Accuracy = 53.92999999999999%, Loss = 1.1099640369415282
Epoch: 2603, Batch Gradient Norm: 43.180306756924836
Epoch: 2603, Batch Gradient Norm after: 22.36068026893112
Epoch 2604/10000, Prediction Accuracy = 53.891999999999996%, Loss = 1.12214195728302
Epoch: 2604, Batch Gradient Norm: 38.68730423263874
Epoch: 2604, Batch Gradient Norm after: 22.360676714819927
Epoch 2605/10000, Prediction Accuracy = 53.926%, Loss = 1.1096277475357055
Epoch: 2605, Batch Gradient Norm: 43.179885241978425
Epoch: 2605, Batch Gradient Norm after: 22.36067793143077
Epoch 2606/10000, Prediction Accuracy = 53.903999999999996%, Loss = 1.121835708618164
Epoch: 2606, Batch Gradient Norm: 38.67832011404752
Epoch: 2606, Batch Gradient Norm after: 22.360676690156097
Epoch 2607/10000, Prediction Accuracy = 53.928%, Loss = 1.1092918634414672
Epoch: 2607, Batch Gradient Norm: 43.18645981763583
Epoch: 2607, Batch Gradient Norm after: 22.360677963441038
Epoch 2608/10000, Prediction Accuracy = 53.90400000000001%, Loss = 1.1215398073196412
Epoch: 2608, Batch Gradient Norm: 38.67112295658699
Epoch: 2608, Batch Gradient Norm after: 22.3606767950179
Epoch 2609/10000, Prediction Accuracy = 53.934000000000005%, Loss = 1.1089608430862428
Epoch: 2609, Batch Gradient Norm: 43.1910045987342
Epoch: 2609, Batch Gradient Norm after: 22.36067785696362
Epoch 2610/10000, Prediction Accuracy = 53.89799999999999%, Loss = 1.1212430715560913
Epoch: 2610, Batch Gradient Norm: 38.66342447988797
Epoch: 2610, Batch Gradient Norm after: 22.360677641054743
Epoch 2611/10000, Prediction Accuracy = 53.93800000000001%, Loss = 1.1086268424987793
Epoch: 2611, Batch Gradient Norm: 43.19814998728472
Epoch: 2611, Batch Gradient Norm after: 22.360678643768722
Epoch 2612/10000, Prediction Accuracy = 53.89200000000001%, Loss = 1.1209435939788819
Epoch: 2612, Batch Gradient Norm: 38.6577313923361
Epoch: 2612, Batch Gradient Norm after: 22.360677707208982
Epoch 2613/10000, Prediction Accuracy = 53.944%, Loss = 1.1082864284515381
Epoch: 2613, Batch Gradient Norm: 43.19757291896383
Epoch: 2613, Batch Gradient Norm after: 22.36067921829156
Epoch 2614/10000, Prediction Accuracy = 53.902%, Loss = 1.1206168413162232
Epoch: 2614, Batch Gradient Norm: 38.65099405621321
Epoch: 2614, Batch Gradient Norm after: 22.360677548372507
Epoch 2615/10000, Prediction Accuracy = 53.95399999999999%, Loss = 1.1079505681991577
Epoch: 2615, Batch Gradient Norm: 43.19050783633771
Epoch: 2615, Batch Gradient Norm after: 22.360678591229647
Epoch 2616/10000, Prediction Accuracy = 53.90999999999999%, Loss = 1.1202902793884277
Epoch: 2616, Batch Gradient Norm: 38.64507674163975
Epoch: 2616, Batch Gradient Norm after: 22.360677345913356
Epoch 2617/10000, Prediction Accuracy = 53.970000000000006%, Loss = 1.10762300491333
Epoch: 2617, Batch Gradient Norm: 43.19350400261832
Epoch: 2617, Batch Gradient Norm after: 22.360677128235736
Epoch 2618/10000, Prediction Accuracy = 53.91400000000001%, Loss = 1.119982695579529
Epoch: 2618, Batch Gradient Norm: 38.63586992662565
Epoch: 2618, Batch Gradient Norm after: 22.36067750490495
Epoch 2619/10000, Prediction Accuracy = 53.972%, Loss = 1.1072821855545043
Epoch: 2619, Batch Gradient Norm: 43.190037877119806
Epoch: 2619, Batch Gradient Norm after: 22.360678667234986
Epoch 2620/10000, Prediction Accuracy = 53.90599999999999%, Loss = 1.1196585655212403
Epoch: 2620, Batch Gradient Norm: 38.62826503405562
Epoch: 2620, Batch Gradient Norm after: 22.360677672729878
Epoch 2621/10000, Prediction Accuracy = 53.984%, Loss = 1.1069421768188477
Epoch: 2621, Batch Gradient Norm: 43.186566254090415
Epoch: 2621, Batch Gradient Norm after: 22.360679634729767
Epoch 2622/10000, Prediction Accuracy = 53.922000000000004%, Loss = 1.1193292379379272
Epoch: 2622, Batch Gradient Norm: 38.62053352967306
Epoch: 2622, Batch Gradient Norm after: 22.360678544423166
Epoch 2623/10000, Prediction Accuracy = 53.988%, Loss = 1.1066084384918213
Epoch: 2623, Batch Gradient Norm: 43.18742432520239
Epoch: 2623, Batch Gradient Norm after: 22.360678512806572
Epoch 2624/10000, Prediction Accuracy = 53.92999999999999%, Loss = 1.1190091133117677
Epoch: 2624, Batch Gradient Norm: 38.613261772446
Epoch: 2624, Batch Gradient Norm after: 22.360679038552195
Epoch 2625/10000, Prediction Accuracy = 53.992%, Loss = 1.1062739849090577
Epoch: 2625, Batch Gradient Norm: 43.17764998066735
Epoch: 2625, Batch Gradient Norm after: 22.360678762407442
Epoch 2626/10000, Prediction Accuracy = 53.948%, Loss = 1.1186642408370973
Epoch: 2626, Batch Gradient Norm: 38.6047987631622
Epoch: 2626, Batch Gradient Norm after: 22.360680316919122
Epoch 2627/10000, Prediction Accuracy = 53.998000000000005%, Loss = 1.1059418201446534
Epoch: 2627, Batch Gradient Norm: 43.17111364439861
Epoch: 2627, Batch Gradient Norm after: 22.360679031462592
Epoch 2628/10000, Prediction Accuracy = 53.956%, Loss = 1.1183180570602418
Epoch: 2628, Batch Gradient Norm: 38.5978678319081
Epoch: 2628, Batch Gradient Norm after: 22.360679137684293
Epoch 2629/10000, Prediction Accuracy = 54.012%, Loss = 1.1056067943572998
Epoch: 2629, Batch Gradient Norm: 43.16460330223158
Epoch: 2629, Batch Gradient Norm after: 22.360678789360385
Epoch 2630/10000, Prediction Accuracy = 53.958000000000006%, Loss = 1.1179826259613037
Epoch: 2630, Batch Gradient Norm: 38.5911641394403
Epoch: 2630, Batch Gradient Norm after: 22.360680003302427
Epoch 2631/10000, Prediction Accuracy = 54.028%, Loss = 1.1052736282348632
Epoch: 2631, Batch Gradient Norm: 43.157766625825126
Epoch: 2631, Batch Gradient Norm after: 22.360679912796147
Epoch 2632/10000, Prediction Accuracy = 53.964%, Loss = 1.1176445007324218
Epoch: 2632, Batch Gradient Norm: 38.58437435691352
Epoch: 2632, Batch Gradient Norm after: 22.360679676076256
Epoch 2633/10000, Prediction Accuracy = 54.022000000000006%, Loss = 1.104938793182373
Epoch: 2633, Batch Gradient Norm: 43.15403817893772
Epoch: 2633, Batch Gradient Norm after: 22.36067768389012
Epoch 2634/10000, Prediction Accuracy = 53.96999999999999%, Loss = 1.117313027381897
Epoch: 2634, Batch Gradient Norm: 38.578023104333866
Epoch: 2634, Batch Gradient Norm after: 22.360679419283798
Epoch 2635/10000, Prediction Accuracy = 54.012%, Loss = 1.1046093940734862
Epoch: 2635, Batch Gradient Norm: 43.152680948399684
Epoch: 2635, Batch Gradient Norm after: 22.360679313026484
Epoch 2636/10000, Prediction Accuracy = 53.974000000000004%, Loss = 1.1169950246810914
Epoch: 2636, Batch Gradient Norm: 38.56834775969639
Epoch: 2636, Batch Gradient Norm after: 22.360679747818526
Epoch 2637/10000, Prediction Accuracy = 54.02%, Loss = 1.1042752504348754
Epoch: 2637, Batch Gradient Norm: 43.13957189743303
Epoch: 2637, Batch Gradient Norm after: 22.360678961218134
Epoch 2638/10000, Prediction Accuracy = 53.977999999999994%, Loss = 1.11665678024292
Epoch: 2638, Batch Gradient Norm: 38.56153429982497
Epoch: 2638, Batch Gradient Norm after: 22.360680560619585
Epoch 2639/10000, Prediction Accuracy = 54.04%, Loss = 1.1039360284805297
Epoch: 2639, Batch Gradient Norm: 43.12805204021784
Epoch: 2639, Batch Gradient Norm after: 22.36067860875366
Epoch 2640/10000, Prediction Accuracy = 54.0%, Loss = 1.116307544708252
Epoch: 2640, Batch Gradient Norm: 38.55561141893522
Epoch: 2640, Batch Gradient Norm after: 22.360679239650395
Epoch 2641/10000, Prediction Accuracy = 54.04599999999999%, Loss = 1.1035968780517578
Epoch: 2641, Batch Gradient Norm: 43.113334133348395
Epoch: 2641, Batch Gradient Norm after: 22.360676356360955
Epoch 2642/10000, Prediction Accuracy = 54.001999999999995%, Loss = 1.1159458875656127
Epoch: 2642, Batch Gradient Norm: 38.54902413211395
Epoch: 2642, Batch Gradient Norm after: 22.360679762960526
Epoch 2643/10000, Prediction Accuracy = 54.056%, Loss = 1.1032662868499756
Epoch: 2643, Batch Gradient Norm: 43.10003034390241
Epoch: 2643, Batch Gradient Norm after: 22.3606767063916
Epoch 2644/10000, Prediction Accuracy = 54.010000000000005%, Loss = 1.115596556663513
Epoch: 2644, Batch Gradient Norm: 38.54432484479319
Epoch: 2644, Batch Gradient Norm after: 22.360676330675357
Epoch 2645/10000, Prediction Accuracy = 54.065999999999995%, Loss = 1.1029314756393434
Epoch: 2645, Batch Gradient Norm: 43.0891870619001
Epoch: 2645, Batch Gradient Norm after: 22.360678596190002
Epoch 2646/10000, Prediction Accuracy = 54.019999999999996%, Loss = 1.1152496576309203
Epoch: 2646, Batch Gradient Norm: 38.53708484358686
Epoch: 2646, Batch Gradient Norm after: 22.360678834112658
Epoch 2647/10000, Prediction Accuracy = 54.062%, Loss = 1.1025999069213868
Epoch: 2647, Batch Gradient Norm: 43.0815029809756
Epoch: 2647, Batch Gradient Norm after: 22.360679939782663
Epoch 2648/10000, Prediction Accuracy = 54.034000000000006%, Loss = 1.1149153232574462
Epoch: 2648, Batch Gradient Norm: 38.53315997536574
Epoch: 2648, Batch Gradient Norm after: 22.36067923789385
Epoch 2649/10000, Prediction Accuracy = 54.065999999999995%, Loss = 1.1022656679153442
Epoch: 2649, Batch Gradient Norm: 43.075871670551294
Epoch: 2649, Batch Gradient Norm after: 22.36067806120296
Epoch 2650/10000, Prediction Accuracy = 54.044000000000004%, Loss = 1.1145938396453858
Epoch: 2650, Batch Gradient Norm: 38.52713804068816
Epoch: 2650, Batch Gradient Norm after: 22.360680901514247
Epoch 2651/10000, Prediction Accuracy = 54.081999999999994%, Loss = 1.101927399635315
Epoch: 2651, Batch Gradient Norm: 43.07420641933702
Epoch: 2651, Batch Gradient Norm after: 22.360676472295633
Epoch 2652/10000, Prediction Accuracy = 54.06%, Loss = 1.1142654895782471
Epoch: 2652, Batch Gradient Norm: 38.52113997029324
Epoch: 2652, Batch Gradient Norm after: 22.360678968237977
Epoch 2653/10000, Prediction Accuracy = 54.108000000000004%, Loss = 1.1015928745269776
Epoch: 2653, Batch Gradient Norm: 43.07269136583585
Epoch: 2653, Batch Gradient Norm after: 22.360677844111727
Epoch 2654/10000, Prediction Accuracy = 54.074%, Loss = 1.1139391899108886
Epoch: 2654, Batch Gradient Norm: 38.51607432179697
Epoch: 2654, Batch Gradient Norm after: 22.360679043185858
Epoch 2655/10000, Prediction Accuracy = 54.10600000000001%, Loss = 1.1012643814086913
Epoch: 2655, Batch Gradient Norm: 43.06839903035304
Epoch: 2655, Batch Gradient Norm after: 22.360676819949404
Epoch 2656/10000, Prediction Accuracy = 54.084%, Loss = 1.113611888885498
Epoch: 2656, Batch Gradient Norm: 38.509596408976485
Epoch: 2656, Batch Gradient Norm after: 22.360679328878984
Epoch 2657/10000, Prediction Accuracy = 54.11999999999999%, Loss = 1.1009309768676758
Epoch: 2657, Batch Gradient Norm: 43.06329783285871
Epoch: 2657, Batch Gradient Norm after: 22.360678766183437
Epoch 2658/10000, Prediction Accuracy = 54.086%, Loss = 1.1132920742034913
Epoch: 2658, Batch Gradient Norm: 38.502361919534856
Epoch: 2658, Batch Gradient Norm after: 22.360679028500204
Epoch 2659/10000, Prediction Accuracy = 54.126%, Loss = 1.100601029396057
Epoch: 2659, Batch Gradient Norm: 43.067306554382256
Epoch: 2659, Batch Gradient Norm after: 22.360677906346126
Epoch 2660/10000, Prediction Accuracy = 54.112%, Loss = 1.1129883766174316
Epoch: 2660, Batch Gradient Norm: 38.49862141623709
Epoch: 2660, Batch Gradient Norm after: 22.360677356349214
Epoch 2661/10000, Prediction Accuracy = 54.134%, Loss = 1.1002729177474975
Epoch: 2661, Batch Gradient Norm: 43.07426301022807
Epoch: 2661, Batch Gradient Norm after: 22.36067834251696
Epoch 2662/10000, Prediction Accuracy = 54.116%, Loss = 1.1126860141754151
Epoch: 2662, Batch Gradient Norm: 38.490824566653245
Epoch: 2662, Batch Gradient Norm after: 22.360676466683277
Epoch 2663/10000, Prediction Accuracy = 54.136%, Loss = 1.0999422550201416
Epoch: 2663, Batch Gradient Norm: 43.0850078730115
Epoch: 2663, Batch Gradient Norm after: 22.360677713434285
Epoch 2664/10000, Prediction Accuracy = 54.126%, Loss = 1.1124033212661744
Epoch: 2664, Batch Gradient Norm: 38.485901558145024
Epoch: 2664, Batch Gradient Norm after: 22.360677773395956
Epoch 2665/10000, Prediction Accuracy = 54.141999999999996%, Loss = 1.0996111392974854
Epoch: 2665, Batch Gradient Norm: 43.08533018889807
Epoch: 2665, Batch Gradient Norm after: 22.36067860532746
Epoch 2666/10000, Prediction Accuracy = 54.128%, Loss = 1.1120954751968384
Epoch: 2666, Batch Gradient Norm: 38.480305388584746
Epoch: 2666, Batch Gradient Norm after: 22.360680830296914
Epoch 2667/10000, Prediction Accuracy = 54.152%, Loss = 1.0992797136306762
Epoch: 2667, Batch Gradient Norm: 43.08298695849667
Epoch: 2667, Batch Gradient Norm after: 22.360676044158584
Epoch 2668/10000, Prediction Accuracy = 54.136%, Loss = 1.1117765426635742
Epoch: 2668, Batch Gradient Norm: 38.47493820708715
Epoch: 2668, Batch Gradient Norm after: 22.360677907671636
Epoch 2669/10000, Prediction Accuracy = 54.152%, Loss = 1.0989459991455077
Epoch: 2669, Batch Gradient Norm: 43.08293032284602
Epoch: 2669, Batch Gradient Norm after: 22.36067950206008
Epoch 2670/10000, Prediction Accuracy = 54.14000000000001%, Loss = 1.1114616870880127
Epoch: 2670, Batch Gradient Norm: 38.46754683528257
Epoch: 2670, Batch Gradient Norm after: 22.360678113272417
Epoch 2671/10000, Prediction Accuracy = 54.16600000000001%, Loss = 1.0986140251159668
Epoch: 2671, Batch Gradient Norm: 43.08928563246263
Epoch: 2671, Batch Gradient Norm after: 22.360680139193562
Epoch 2672/10000, Prediction Accuracy = 54.148%, Loss = 1.1111656188964845
Epoch: 2672, Batch Gradient Norm: 38.46159579967164
Epoch: 2672, Batch Gradient Norm after: 22.36067821674232
Epoch 2673/10000, Prediction Accuracy = 54.174%, Loss = 1.0982823133468629
Epoch: 2673, Batch Gradient Norm: 43.09250153267864
Epoch: 2673, Batch Gradient Norm after: 22.360676157570026
Epoch 2674/10000, Prediction Accuracy = 54.144000000000005%, Loss = 1.1108535766601562
Epoch: 2674, Batch Gradient Norm: 38.454387830560364
Epoch: 2674, Batch Gradient Norm after: 22.360680598923466
Epoch 2675/10000, Prediction Accuracy = 54.181999999999995%, Loss = 1.097953701019287
Epoch: 2675, Batch Gradient Norm: 43.098301343086014
Epoch: 2675, Batch Gradient Norm after: 22.360678673416622
Epoch 2676/10000, Prediction Accuracy = 54.153999999999996%, Loss = 1.1105627775192262
Epoch: 2676, Batch Gradient Norm: 38.448071512498345
Epoch: 2676, Batch Gradient Norm after: 22.36067994769898
Epoch 2677/10000, Prediction Accuracy = 54.19199999999999%, Loss = 1.0976240634918213
Epoch: 2677, Batch Gradient Norm: 43.10287635373798
Epoch: 2677, Batch Gradient Norm after: 22.360677067864845
Epoch 2678/10000, Prediction Accuracy = 54.158%, Loss = 1.1102633714675902
Epoch: 2678, Batch Gradient Norm: 38.4394039764824
Epoch: 2678, Batch Gradient Norm after: 22.36067774503311
Epoch 2679/10000, Prediction Accuracy = 54.208000000000006%, Loss = 1.097296667098999
Epoch: 2679, Batch Gradient Norm: 43.10689491027081
Epoch: 2679, Batch Gradient Norm after: 22.360677384605783
Epoch 2680/10000, Prediction Accuracy = 54.166%, Loss = 1.1099595785140992
Epoch: 2680, Batch Gradient Norm: 38.42988927231388
Epoch: 2680, Batch Gradient Norm after: 22.360678885748822
Epoch 2681/10000, Prediction Accuracy = 54.20799999999999%, Loss = 1.0969634532928467
Epoch: 2681, Batch Gradient Norm: 43.10444768954391
Epoch: 2681, Batch Gradient Norm after: 22.360677854739457
Epoch 2682/10000, Prediction Accuracy = 54.174%, Loss = 1.1096402406692505
Epoch: 2682, Batch Gradient Norm: 38.42641744922776
Epoch: 2682, Batch Gradient Norm after: 22.360678619024448
Epoch 2683/10000, Prediction Accuracy = 54.220000000000006%, Loss = 1.0966298818588256
Epoch: 2683, Batch Gradient Norm: 43.09972519380039
Epoch: 2683, Batch Gradient Norm after: 22.360678412526838
Epoch 2684/10000, Prediction Accuracy = 54.186%, Loss = 1.1093108177185058
Epoch: 2684, Batch Gradient Norm: 38.41892588017196
Epoch: 2684, Batch Gradient Norm after: 22.360678726262172
Epoch 2685/10000, Prediction Accuracy = 54.224000000000004%, Loss = 1.0963016986846923
Epoch: 2685, Batch Gradient Norm: 43.095987536215965
Epoch: 2685, Batch Gradient Norm after: 22.360676626766082
Epoch 2686/10000, Prediction Accuracy = 54.198%, Loss = 1.108984351158142
Epoch: 2686, Batch Gradient Norm: 38.413339267435454
Epoch: 2686, Batch Gradient Norm after: 22.36067729040816
Epoch 2687/10000, Prediction Accuracy = 54.246%, Loss = 1.095970869064331
Epoch: 2687, Batch Gradient Norm: 43.0885303167168
Epoch: 2687, Batch Gradient Norm after: 22.360676458059785
Epoch 2688/10000, Prediction Accuracy = 54.21%, Loss = 1.1086585760116576
Epoch: 2688, Batch Gradient Norm: 38.404577421717775
Epoch: 2688, Batch Gradient Norm after: 22.360676519701325
Epoch 2689/10000, Prediction Accuracy = 54.25599999999999%, Loss = 1.0956444263458252
Epoch: 2689, Batch Gradient Norm: 43.0794988973839
Epoch: 2689, Batch Gradient Norm after: 22.36067754447981
Epoch 2690/10000, Prediction Accuracy = 54.220000000000006%, Loss = 1.1083358764648437
Epoch: 2690, Batch Gradient Norm: 38.396642288951526
Epoch: 2690, Batch Gradient Norm after: 22.36067687959192
Epoch 2691/10000, Prediction Accuracy = 54.26800000000001%, Loss = 1.0953100442886352
Epoch: 2691, Batch Gradient Norm: 43.06825927342518
Epoch: 2691, Batch Gradient Norm after: 22.360678097273208
Epoch 2692/10000, Prediction Accuracy = 54.217999999999996%, Loss = 1.1079962968826294
Epoch: 2692, Batch Gradient Norm: 38.392147885865704
Epoch: 2692, Batch Gradient Norm after: 22.360676840497643
Epoch 2693/10000, Prediction Accuracy = 54.279999999999994%, Loss = 1.0949783325195312
Epoch: 2693, Batch Gradient Norm: 43.060833341402045
Epoch: 2693, Batch Gradient Norm after: 22.360678448800307
Epoch 2694/10000, Prediction Accuracy = 54.224000000000004%, Loss = 1.1076606035232544
Epoch: 2694, Batch Gradient Norm: 38.384789906258646
Epoch: 2694, Batch Gradient Norm after: 22.360675236956432
Epoch 2695/10000, Prediction Accuracy = 54.294000000000004%, Loss = 1.094645357131958
Epoch: 2695, Batch Gradient Norm: 43.05446435158291
Epoch: 2695, Batch Gradient Norm after: 22.360676224041526
Epoch 2696/10000, Prediction Accuracy = 54.23%, Loss = 1.1073249578475952
Epoch: 2696, Batch Gradient Norm: 38.37868591355219
Epoch: 2696, Batch Gradient Norm after: 22.360676371582528
Epoch 2697/10000, Prediction Accuracy = 54.294000000000004%, Loss = 1.0943196296691895
Epoch: 2697, Batch Gradient Norm: 43.042858132060665
Epoch: 2697, Batch Gradient Norm after: 22.360677997281677
Epoch 2698/10000, Prediction Accuracy = 54.246%, Loss = 1.106982374191284
Epoch: 2698, Batch Gradient Norm: 38.37362932834473
Epoch: 2698, Batch Gradient Norm after: 22.360677496702674
Epoch 2699/10000, Prediction Accuracy = 54.279999999999994%, Loss = 1.0939908981323243
Epoch: 2699, Batch Gradient Norm: 43.03623603176868
Epoch: 2699, Batch Gradient Norm after: 22.360676494003133
Epoch 2700/10000, Prediction Accuracy = 54.25%, Loss = 1.1066619873046875
Epoch: 2700, Batch Gradient Norm: 38.36891476219891
Epoch: 2700, Batch Gradient Norm after: 22.3606771263184
Epoch 2701/10000, Prediction Accuracy = 54.272000000000006%, Loss = 1.0936617851257324
Epoch: 2701, Batch Gradient Norm: 43.023786995236264
Epoch: 2701, Batch Gradient Norm after: 22.36067786181807
Epoch 2702/10000, Prediction Accuracy = 54.242000000000004%, Loss = 1.1063177347183228
Epoch: 2702, Batch Gradient Norm: 38.363159919899665
Epoch: 2702, Batch Gradient Norm after: 22.36067707781161
Epoch 2703/10000, Prediction Accuracy = 54.272000000000006%, Loss = 1.0933344602584838
Epoch: 2703, Batch Gradient Norm: 43.02193542980788
Epoch: 2703, Batch Gradient Norm after: 22.36067743058965
Epoch 2704/10000, Prediction Accuracy = 54.238%, Loss = 1.1059970378875732
Epoch: 2704, Batch Gradient Norm: 38.35605263238523
Epoch: 2704, Batch Gradient Norm after: 22.360678896011418
Epoch 2705/10000, Prediction Accuracy = 54.275999999999996%, Loss = 1.0930096864700318
Epoch: 2705, Batch Gradient Norm: 43.02772975775039
Epoch: 2705, Batch Gradient Norm after: 22.36067803399541
Epoch 2706/10000, Prediction Accuracy = 54.254%, Loss = 1.1056975364685058
Epoch: 2706, Batch Gradient Norm: 38.34758915311019
Epoch: 2706, Batch Gradient Norm after: 22.36067743457593
Epoch 2707/10000, Prediction Accuracy = 54.29600000000001%, Loss = 1.0926835775375365
Epoch: 2707, Batch Gradient Norm: 43.02973156651135
Epoch: 2707, Batch Gradient Norm after: 22.360677231246715
Epoch 2708/10000, Prediction Accuracy = 54.262%, Loss = 1.1053966522216796
Epoch: 2708, Batch Gradient Norm: 38.33804555444184
Epoch: 2708, Batch Gradient Norm after: 22.360676625694055
Epoch 2709/10000, Prediction Accuracy = 54.314%, Loss = 1.092358446121216
Epoch: 2709, Batch Gradient Norm: 43.02919783407243
Epoch: 2709, Batch Gradient Norm after: 22.36067878576935
Epoch 2710/10000, Prediction Accuracy = 54.266%, Loss = 1.1050856113433838
Epoch: 2710, Batch Gradient Norm: 38.33067592317047
Epoch: 2710, Batch Gradient Norm after: 22.360678617243092
Epoch 2711/10000, Prediction Accuracy = 54.33%, Loss = 1.0920275449752808
Epoch: 2711, Batch Gradient Norm: 43.02713186167745
Epoch: 2711, Batch Gradient Norm after: 22.36067767868852
Epoch 2712/10000, Prediction Accuracy = 54.266%, Loss = 1.1047734260559081
Epoch: 2712, Batch Gradient Norm: 38.32575886323431
Epoch: 2712, Batch Gradient Norm after: 22.360677338929715
Epoch 2713/10000, Prediction Accuracy = 54.334%, Loss = 1.0917069435119628
Epoch: 2713, Batch Gradient Norm: 43.027578917277154
Epoch: 2713, Batch Gradient Norm after: 22.360677400590447
Epoch 2714/10000, Prediction Accuracy = 54.266%, Loss = 1.10446355342865
Epoch: 2714, Batch Gradient Norm: 38.31680083764464
Epoch: 2714, Batch Gradient Norm after: 22.360677884844623
Epoch 2715/10000, Prediction Accuracy = 54.354%, Loss = 1.09137864112854
Epoch: 2715, Batch Gradient Norm: 43.02720534605856
Epoch: 2715, Batch Gradient Norm after: 22.360677699705903
Epoch 2716/10000, Prediction Accuracy = 54.286%, Loss = 1.1041566133499146
Epoch: 2716, Batch Gradient Norm: 38.30828018629634
Epoch: 2716, Batch Gradient Norm after: 22.360678161309817
Epoch 2717/10000, Prediction Accuracy = 54.36%, Loss = 1.0910558462142945
Epoch: 2717, Batch Gradient Norm: 43.02391428040043
Epoch: 2717, Batch Gradient Norm after: 22.36067797380423
Epoch 2718/10000, Prediction Accuracy = 54.294%, Loss = 1.103848123550415
Epoch: 2718, Batch Gradient Norm: 38.301186853323514
Epoch: 2718, Batch Gradient Norm after: 22.360676379623953
Epoch 2719/10000, Prediction Accuracy = 54.366%, Loss = 1.0907302141189574
Epoch: 2719, Batch Gradient Norm: 43.01680185366366
Epoch: 2719, Batch Gradient Norm after: 22.36067745152312
Epoch 2720/10000, Prediction Accuracy = 54.29%, Loss = 1.1035182237625123
Epoch: 2720, Batch Gradient Norm: 38.293673619441975
Epoch: 2720, Batch Gradient Norm after: 22.360680836000725
Epoch 2721/10000, Prediction Accuracy = 54.35799999999999%, Loss = 1.090401005744934
Epoch: 2721, Batch Gradient Norm: 43.0076658844124
Epoch: 2721, Batch Gradient Norm after: 22.36067576630352
Epoch 2722/10000, Prediction Accuracy = 54.306%, Loss = 1.1031829833984375
Epoch: 2722, Batch Gradient Norm: 38.28656980731469
Epoch: 2722, Batch Gradient Norm after: 22.360678071171296
Epoch 2723/10000, Prediction Accuracy = 54.36600000000001%, Loss = 1.090070867538452
Epoch: 2723, Batch Gradient Norm: 42.996450957287216
Epoch: 2723, Batch Gradient Norm after: 22.36067861017319
Epoch 2724/10000, Prediction Accuracy = 54.31999999999999%, Loss = 1.1028418779373168
Epoch: 2724, Batch Gradient Norm: 38.27782331808686
Epoch: 2724, Batch Gradient Norm after: 22.360679210376823
Epoch 2725/10000, Prediction Accuracy = 54.374%, Loss = 1.0897446393966674
Epoch: 2725, Batch Gradient Norm: 42.98836848751524
Epoch: 2725, Batch Gradient Norm after: 22.36067718228087
Epoch 2726/10000, Prediction Accuracy = 54.31999999999999%, Loss = 1.10251841545105
Epoch: 2726, Batch Gradient Norm: 38.271475517054824
Epoch: 2726, Batch Gradient Norm after: 22.360677445331614
Epoch 2727/10000, Prediction Accuracy = 54.378%, Loss = 1.0894182682037354
Epoch: 2727, Batch Gradient Norm: 42.977450371816076
Epoch: 2727, Batch Gradient Norm after: 22.360678155316254
Epoch 2728/10000, Prediction Accuracy = 54.33200000000001%, Loss = 1.1021782636642456
Epoch: 2728, Batch Gradient Norm: 38.26561438027126
Epoch: 2728, Batch Gradient Norm after: 22.36067749562559
Epoch 2729/10000, Prediction Accuracy = 54.39%, Loss = 1.0890917539596559
Epoch: 2729, Batch Gradient Norm: 42.96570488351914
Epoch: 2729, Batch Gradient Norm after: 22.3606780130985
Epoch 2730/10000, Prediction Accuracy = 54.338%, Loss = 1.1018481969833374
Epoch: 2730, Batch Gradient Norm: 38.25897146797587
Epoch: 2730, Batch Gradient Norm after: 22.360678973601267
Epoch 2731/10000, Prediction Accuracy = 54.39200000000001%, Loss = 1.0887712478637694
Epoch: 2731, Batch Gradient Norm: 42.957437292411015
Epoch: 2731, Batch Gradient Norm after: 22.36067848729264
Epoch 2732/10000, Prediction Accuracy = 54.354%, Loss = 1.1015111923217773
Epoch: 2732, Batch Gradient Norm: 38.25361257879078
Epoch: 2732, Batch Gradient Norm after: 22.360676897284655
Epoch 2733/10000, Prediction Accuracy = 54.391999999999996%, Loss = 1.088446068763733
Epoch: 2733, Batch Gradient Norm: 42.947771335425195
Epoch: 2733, Batch Gradient Norm after: 22.36067726761764
Epoch 2734/10000, Prediction Accuracy = 54.364%, Loss = 1.1011732339859008
Epoch: 2734, Batch Gradient Norm: 38.24529941350721
Epoch: 2734, Batch Gradient Norm after: 22.36068118770848
Epoch 2735/10000, Prediction Accuracy = 54.398%, Loss = 1.088121485710144
Epoch: 2735, Batch Gradient Norm: 42.929260632767225
Epoch: 2735, Batch Gradient Norm after: 22.36067640501719
Epoch 2736/10000, Prediction Accuracy = 54.36800000000001%, Loss = 1.1008194923400878
Epoch: 2736, Batch Gradient Norm: 38.23817178318667
Epoch: 2736, Batch Gradient Norm after: 22.36067759186791
Epoch 2737/10000, Prediction Accuracy = 54.412%, Loss = 1.0877970457077026
Epoch: 2737, Batch Gradient Norm: 42.91011581057672
Epoch: 2737, Batch Gradient Norm after: 22.360677034236492
Epoch 2738/10000, Prediction Accuracy = 54.379999999999995%, Loss = 1.1004606008529663
Epoch: 2738, Batch Gradient Norm: 38.23189761366687
Epoch: 2738, Batch Gradient Norm after: 22.360678644984134
Epoch 2739/10000, Prediction Accuracy = 54.414%, Loss = 1.0874752283096314
Epoch: 2739, Batch Gradient Norm: 42.890702442374014
Epoch: 2739, Batch Gradient Norm after: 22.360677624484733
Epoch 2740/10000, Prediction Accuracy = 54.384%, Loss = 1.1000998735427856
Epoch: 2740, Batch Gradient Norm: 38.22365857247012
Epoch: 2740, Batch Gradient Norm after: 22.36067844618917
Epoch 2741/10000, Prediction Accuracy = 54.418000000000006%, Loss = 1.0871471166610718
Epoch: 2741, Batch Gradient Norm: 42.874111516930206
Epoch: 2741, Batch Gradient Norm after: 22.36067702810727
Epoch 2742/10000, Prediction Accuracy = 54.384%, Loss = 1.0997445106506347
Epoch: 2742, Batch Gradient Norm: 38.21680681652195
Epoch: 2742, Batch Gradient Norm after: 22.360678754388143
Epoch 2743/10000, Prediction Accuracy = 54.428%, Loss = 1.086821961402893
Epoch: 2743, Batch Gradient Norm: 42.86002839197081
Epoch: 2743, Batch Gradient Norm after: 22.360679288433676
Epoch 2744/10000, Prediction Accuracy = 54.386%, Loss = 1.0993957996368409
Epoch: 2744, Batch Gradient Norm: 38.21134741007591
Epoch: 2744, Batch Gradient Norm after: 22.360678517956707
Epoch 2745/10000, Prediction Accuracy = 54.43000000000001%, Loss = 1.0865005016326905
Epoch: 2745, Batch Gradient Norm: 42.84639876418107
Epoch: 2745, Batch Gradient Norm after: 22.360678253761442
Epoch 2746/10000, Prediction Accuracy = 54.398%, Loss = 1.0990546703338624
Epoch: 2746, Batch Gradient Norm: 38.20424848383213
Epoch: 2746, Batch Gradient Norm after: 22.36067869846631
Epoch 2747/10000, Prediction Accuracy = 54.436%, Loss = 1.0861777544021607
Epoch: 2747, Batch Gradient Norm: 42.832353777903236
Epoch: 2747, Batch Gradient Norm after: 22.36067822035263
Epoch 2748/10000, Prediction Accuracy = 54.386%, Loss = 1.0987078428268433
Epoch: 2748, Batch Gradient Norm: 38.196817369763814
Epoch: 2748, Batch Gradient Norm after: 22.360678435628312
Epoch 2749/10000, Prediction Accuracy = 54.43000000000001%, Loss = 1.0858566999435424
Epoch: 2749, Batch Gradient Norm: 42.815101578160515
Epoch: 2749, Batch Gradient Norm after: 22.36067777597088
Epoch 2750/10000, Prediction Accuracy = 54.40599999999999%, Loss = 1.0983622074127197
Epoch: 2750, Batch Gradient Norm: 38.189548499207156
Epoch: 2750, Batch Gradient Norm after: 22.36067844048143
Epoch 2751/10000, Prediction Accuracy = 54.448%, Loss = 1.0855349063873292
Epoch: 2751, Batch Gradient Norm: 42.796330130442534
Epoch: 2751, Batch Gradient Norm after: 22.36067762124093
Epoch 2752/10000, Prediction Accuracy = 54.422000000000004%, Loss = 1.0980134963989259
Epoch: 2752, Batch Gradient Norm: 38.18284915287549
Epoch: 2752, Batch Gradient Norm after: 22.360676088365015
Epoch 2753/10000, Prediction Accuracy = 54.458000000000006%, Loss = 1.0852117538452148
Epoch: 2753, Batch Gradient Norm: 42.775181761242266
Epoch: 2753, Batch Gradient Norm after: 22.36067712258257
Epoch 2754/10000, Prediction Accuracy = 54.44200000000001%, Loss = 1.0976490736007691
Epoch: 2754, Batch Gradient Norm: 38.177785247345604
Epoch: 2754, Batch Gradient Norm after: 22.360679303060326
Epoch 2755/10000, Prediction Accuracy = 54.462%, Loss = 1.0848888397216796
Epoch: 2755, Batch Gradient Norm: 42.752308757996275
Epoch: 2755, Batch Gradient Norm after: 22.360678151744914
Epoch 2756/10000, Prediction Accuracy = 54.444%, Loss = 1.0972824335098266
Epoch: 2756, Batch Gradient Norm: 38.17094444234302
Epoch: 2756, Batch Gradient Norm after: 22.36067685933537
Epoch 2757/10000, Prediction Accuracy = 54.46600000000001%, Loss = 1.0845624923706054
Epoch: 2757, Batch Gradient Norm: 42.72335787129758
Epoch: 2757, Batch Gradient Norm after: 22.36067729211382
Epoch 2758/10000, Prediction Accuracy = 54.448%, Loss = 1.0968908786773681
Epoch: 2758, Batch Gradient Norm: 38.162902644318315
Epoch: 2758, Batch Gradient Norm after: 22.360676174077422
Epoch 2759/10000, Prediction Accuracy = 54.482000000000006%, Loss = 1.0842454195022584
Epoch: 2759, Batch Gradient Norm: 42.6914858220465
Epoch: 2759, Batch Gradient Norm after: 22.360677961848086
Epoch 2760/10000, Prediction Accuracy = 54.45200000000001%, Loss = 1.0964962005615235
Epoch: 2760, Batch Gradient Norm: 38.15605956366189
Epoch: 2760, Batch Gradient Norm after: 22.36067454842915
Epoch 2761/10000, Prediction Accuracy = 54.489999999999995%, Loss = 1.0839174509048461
Epoch: 2761, Batch Gradient Norm: 42.65639833572201
Epoch: 2761, Batch Gradient Norm after: 22.36067822556868
Epoch 2762/10000, Prediction Accuracy = 54.45799999999999%, Loss = 1.096090817451477
Epoch: 2762, Batch Gradient Norm: 38.14931760778575
Epoch: 2762, Batch Gradient Norm after: 22.360675918811825
Epoch 2763/10000, Prediction Accuracy = 54.492000000000004%, Loss = 1.0835970878601073
Epoch: 2763, Batch Gradient Norm: 42.622568966800145
Epoch: 2763, Batch Gradient Norm after: 22.36067630869542
Epoch 2764/10000, Prediction Accuracy = 54.477999999999994%, Loss = 1.0956897258758544
Epoch: 2764, Batch Gradient Norm: 38.14245447133208
Epoch: 2764, Batch Gradient Norm after: 22.36067882605144
Epoch 2765/10000, Prediction Accuracy = 54.50999999999999%, Loss = 1.0832766532897948
Epoch: 2765, Batch Gradient Norm: 42.59241237560119
Epoch: 2765, Batch Gradient Norm after: 22.360677564802653
Epoch 2766/10000, Prediction Accuracy = 54.486000000000004%, Loss = 1.0953090190887451
Epoch: 2766, Batch Gradient Norm: 38.13696298902419
Epoch: 2766, Batch Gradient Norm after: 22.360678395008772
Epoch 2767/10000, Prediction Accuracy = 54.52199999999999%, Loss = 1.0829604148864747
Epoch: 2767, Batch Gradient Norm: 42.564707912851254
Epoch: 2767, Batch Gradient Norm after: 22.36067979517582
Epoch 2768/10000, Prediction Accuracy = 54.501999999999995%, Loss = 1.0949263334274293
Epoch: 2768, Batch Gradient Norm: 38.13136101590882
Epoch: 2768, Batch Gradient Norm after: 22.360677432415343
Epoch 2769/10000, Prediction Accuracy = 54.536%, Loss = 1.0826457738876343
Epoch: 2769, Batch Gradient Norm: 42.539429889908355
Epoch: 2769, Batch Gradient Norm after: 22.36067803393858
Epoch 2770/10000, Prediction Accuracy = 54.507999999999996%, Loss = 1.0945568323135375
Epoch: 2770, Batch Gradient Norm: 38.1263963419047
Epoch: 2770, Batch Gradient Norm after: 22.360678380505583
Epoch 2771/10000, Prediction Accuracy = 54.538%, Loss = 1.082330322265625
Epoch: 2771, Batch Gradient Norm: 42.50800761319584
Epoch: 2771, Batch Gradient Norm after: 22.36067844670538
Epoch 2772/10000, Prediction Accuracy = 54.516000000000005%, Loss = 1.0941632270812989
Epoch: 2772, Batch Gradient Norm: 38.120442362103034
Epoch: 2772, Batch Gradient Norm after: 22.360677053508006
Epoch 2773/10000, Prediction Accuracy = 54.552%, Loss = 1.0820135116577148
Epoch: 2773, Batch Gradient Norm: 42.474113449398985
Epoch: 2773, Batch Gradient Norm after: 22.360677308042188
Epoch 2774/10000, Prediction Accuracy = 54.528%, Loss = 1.0937659740447998
Epoch: 2774, Batch Gradient Norm: 38.11342930231545
Epoch: 2774, Batch Gradient Norm after: 22.36067503639765
Epoch 2775/10000, Prediction Accuracy = 54.56%, Loss = 1.0817033052444458
Epoch: 2775, Batch Gradient Norm: 42.441520881959796
Epoch: 2775, Batch Gradient Norm after: 22.360675843258274
Epoch 2776/10000, Prediction Accuracy = 54.54%, Loss = 1.093375277519226
Epoch: 2776, Batch Gradient Norm: 38.10827619687635
Epoch: 2776, Batch Gradient Norm after: 22.360676883649152
Epoch 2777/10000, Prediction Accuracy = 54.57000000000001%, Loss = 1.081393551826477
Epoch: 2777, Batch Gradient Norm: 42.418218901368284
Epoch: 2777, Batch Gradient Norm after: 22.360677927322627
Epoch 2778/10000, Prediction Accuracy = 54.54%, Loss = 1.0930095672607423
Epoch: 2778, Batch Gradient Norm: 38.09986414607014
Epoch: 2778, Batch Gradient Norm after: 22.360676697597782
Epoch 2779/10000, Prediction Accuracy = 54.58%, Loss = 1.0810791969299316
Epoch: 2779, Batch Gradient Norm: 42.3896385629442
Epoch: 2779, Batch Gradient Norm after: 22.360676895485412
Epoch 2780/10000, Prediction Accuracy = 54.562%, Loss = 1.0926354646682739
Epoch: 2780, Batch Gradient Norm: 38.093810455689265
Epoch: 2780, Batch Gradient Norm after: 22.360675905968158
Epoch 2781/10000, Prediction Accuracy = 54.604000000000006%, Loss = 1.0807621717453002
Epoch: 2781, Batch Gradient Norm: 42.362589232565526
Epoch: 2781, Batch Gradient Norm after: 22.36067707542975
Epoch 2782/10000, Prediction Accuracy = 54.568%, Loss = 1.0922524690628053
Epoch: 2782, Batch Gradient Norm: 38.08953037803301
Epoch: 2782, Batch Gradient Norm after: 22.360677842511066
Epoch 2783/10000, Prediction Accuracy = 54.605999999999995%, Loss = 1.080444836616516
Epoch: 2783, Batch Gradient Norm: 42.329319951984935
Epoch: 2783, Batch Gradient Norm after: 22.36067748702534
Epoch 2784/10000, Prediction Accuracy = 54.572%, Loss = 1.0918655157089234
Epoch: 2784, Batch Gradient Norm: 38.0862068640129
Epoch: 2784, Batch Gradient Norm after: 22.36067696797898
Epoch 2785/10000, Prediction Accuracy = 54.61800000000001%, Loss = 1.080136775970459
Epoch: 2785, Batch Gradient Norm: 42.303986895465336
Epoch: 2785, Batch Gradient Norm after: 22.360678372484387
Epoch 2786/10000, Prediction Accuracy = 54.580000000000005%, Loss = 1.0914949417114257
Epoch: 2786, Batch Gradient Norm: 38.08295812357677
Epoch: 2786, Batch Gradient Norm after: 22.3606793492545
Epoch 2787/10000, Prediction Accuracy = 54.63199999999999%, Loss = 1.0798280000686646
Epoch: 2787, Batch Gradient Norm: 42.2752159447243
Epoch: 2787, Batch Gradient Norm after: 22.36067711029895
Epoch 2788/10000, Prediction Accuracy = 54.596000000000004%, Loss = 1.0911179780960083
Epoch: 2788, Batch Gradient Norm: 38.07946138461668
Epoch: 2788, Batch Gradient Norm after: 22.360678810453734
Epoch 2789/10000, Prediction Accuracy = 54.64200000000001%, Loss = 1.0795197010040283
Epoch: 2789, Batch Gradient Norm: 42.25828277778975
Epoch: 2789, Batch Gradient Norm after: 22.360677555922727
Epoch 2790/10000, Prediction Accuracy = 54.59400000000001%, Loss = 1.090770959854126
Epoch: 2790, Batch Gradient Norm: 38.0733402014881
Epoch: 2790, Batch Gradient Norm after: 22.36067796397013
Epoch 2791/10000, Prediction Accuracy = 54.64200000000001%, Loss = 1.0792139053344727
Epoch: 2791, Batch Gradient Norm: 42.241810692231546
Epoch: 2791, Batch Gradient Norm after: 22.360677952397356
Epoch 2792/10000, Prediction Accuracy = 54.589999999999996%, Loss = 1.0904412984848022
Epoch: 2792, Batch Gradient Norm: 38.06588235243134
Epoch: 2792, Batch Gradient Norm after: 22.360677499472747
Epoch 2793/10000, Prediction Accuracy = 54.65400000000001%, Loss = 1.078899049758911
Epoch: 2793, Batch Gradient Norm: 42.232027676865364
Epoch: 2793, Batch Gradient Norm after: 22.36067871822632
Epoch 2794/10000, Prediction Accuracy = 54.6%, Loss = 1.0901134729385376
Epoch: 2794, Batch Gradient Norm: 38.05890753270632
Epoch: 2794, Batch Gradient Norm after: 22.3606787367074
Epoch 2795/10000, Prediction Accuracy = 54.653999999999996%, Loss = 1.07858726978302
Epoch: 2795, Batch Gradient Norm: 42.21547452617178
Epoch: 2795, Batch Gradient Norm after: 22.360677948314983
Epoch 2796/10000, Prediction Accuracy = 54.6%, Loss = 1.0897817850112914
Epoch: 2796, Batch Gradient Norm: 38.05111026973734
Epoch: 2796, Batch Gradient Norm after: 22.360677070982874
Epoch 2797/10000, Prediction Accuracy = 54.66799999999999%, Loss = 1.0782767057418823
Epoch: 2797, Batch Gradient Norm: 42.20263218310731
Epoch: 2797, Batch Gradient Norm after: 22.360678947539068
Epoch 2798/10000, Prediction Accuracy = 54.612%, Loss = 1.0894493579864502
Epoch: 2798, Batch Gradient Norm: 38.04254538054196
Epoch: 2798, Batch Gradient Norm after: 22.36067746149139
Epoch 2799/10000, Prediction Accuracy = 54.69%, Loss = 1.0779659509658814
Epoch: 2799, Batch Gradient Norm: 42.18939237564704
Epoch: 2799, Batch Gradient Norm after: 22.360678815561254
Epoch 2800/10000, Prediction Accuracy = 54.612%, Loss = 1.0891212940216064
Epoch: 2800, Batch Gradient Norm: 38.03476908109152
Epoch: 2800, Batch Gradient Norm after: 22.360679381238945
Epoch 2801/10000, Prediction Accuracy = 54.702%, Loss = 1.0776529550552367
Epoch: 2801, Batch Gradient Norm: 42.179406365630264
Epoch: 2801, Batch Gradient Norm after: 22.360676658877537
Epoch 2802/10000, Prediction Accuracy = 54.60799999999999%, Loss = 1.088799524307251
Epoch: 2802, Batch Gradient Norm: 38.02952963873381
Epoch: 2802, Batch Gradient Norm after: 22.360676316976285
Epoch 2803/10000, Prediction Accuracy = 54.70399999999999%, Loss = 1.077342939376831
Epoch: 2803, Batch Gradient Norm: 42.16552563422471
Epoch: 2803, Batch Gradient Norm after: 22.360677193941115
Epoch 2804/10000, Prediction Accuracy = 54.61%, Loss = 1.088469910621643
Epoch: 2804, Batch Gradient Norm: 38.02282563847893
Epoch: 2804, Batch Gradient Norm after: 22.360677090145682
Epoch 2805/10000, Prediction Accuracy = 54.712%, Loss = 1.0770344495773316
Epoch: 2805, Batch Gradient Norm: 42.15267111686855
Epoch: 2805, Batch Gradient Norm after: 22.360678405712893
Epoch 2806/10000, Prediction Accuracy = 54.612%, Loss = 1.0881407976150512
Epoch: 2806, Batch Gradient Norm: 38.01653728374576
Epoch: 2806, Batch Gradient Norm after: 22.360678410961267
Epoch 2807/10000, Prediction Accuracy = 54.714%, Loss = 1.076725220680237
Epoch: 2807, Batch Gradient Norm: 42.14584670323807
Epoch: 2807, Batch Gradient Norm after: 22.360678649632362
Epoch 2808/10000, Prediction Accuracy = 54.632000000000005%, Loss = 1.0878219127655029
Epoch: 2808, Batch Gradient Norm: 38.009885317559586
Epoch: 2808, Batch Gradient Norm after: 22.360676474219538
Epoch 2809/10000, Prediction Accuracy = 54.726%, Loss = 1.0764137506484985
Epoch: 2809, Batch Gradient Norm: 42.13722340704978
Epoch: 2809, Batch Gradient Norm after: 22.360678090616673
Epoch 2810/10000, Prediction Accuracy = 54.641999999999996%, Loss = 1.087493109703064
Epoch: 2810, Batch Gradient Norm: 38.00513356820864
Epoch: 2810, Batch Gradient Norm after: 22.360677940727417
Epoch 2811/10000, Prediction Accuracy = 54.73199999999999%, Loss = 1.076102900505066
Epoch: 2811, Batch Gradient Norm: 42.12320511141643
Epoch: 2811, Batch Gradient Norm after: 22.360676661169798
Epoch 2812/10000, Prediction Accuracy = 54.657999999999994%, Loss = 1.0871578931808472
Epoch: 2812, Batch Gradient Norm: 38.00054878445441
Epoch: 2812, Batch Gradient Norm after: 22.36067587269004
Epoch 2813/10000, Prediction Accuracy = 54.745999999999995%, Loss = 1.0758028984069825
Epoch: 2813, Batch Gradient Norm: 42.11305342233232
Epoch: 2813, Batch Gradient Norm after: 22.36067671151815
Epoch 2814/10000, Prediction Accuracy = 54.674%, Loss = 1.0868369340896606
Epoch: 2814, Batch Gradient Norm: 37.99301293543613
Epoch: 2814, Batch Gradient Norm after: 22.360677832783615
Epoch 2815/10000, Prediction Accuracy = 54.75599999999999%, Loss = 1.0754892587661744
Epoch: 2815, Batch Gradient Norm: 42.10857182618311
Epoch: 2815, Batch Gradient Norm after: 22.360676667470166
Epoch 2816/10000, Prediction Accuracy = 54.688%, Loss = 1.086526107788086
Epoch: 2816, Batch Gradient Norm: 37.984876464404906
Epoch: 2816, Batch Gradient Norm after: 22.36067800706749
Epoch 2817/10000, Prediction Accuracy = 54.766%, Loss = 1.0751758813858032
Epoch: 2817, Batch Gradient Norm: 42.10738124252935
Epoch: 2817, Batch Gradient Norm after: 22.360677888674196
Epoch 2818/10000, Prediction Accuracy = 54.694%, Loss = 1.0862316846847535
Epoch: 2818, Batch Gradient Norm: 37.97789198421514
Epoch: 2818, Batch Gradient Norm after: 22.36067801937137
Epoch 2819/10000, Prediction Accuracy = 54.772000000000006%, Loss = 1.0748627185821533
Epoch: 2819, Batch Gradient Norm: 42.10493728354472
Epoch: 2819, Batch Gradient Norm after: 22.36067659875677
Epoch 2820/10000, Prediction Accuracy = 54.7%, Loss = 1.0859356641769409
Epoch: 2820, Batch Gradient Norm: 37.97198727934562
Epoch: 2820, Batch Gradient Norm after: 22.36067656205902
Epoch 2821/10000, Prediction Accuracy = 54.765999999999984%, Loss = 1.0745538473129272
Epoch: 2821, Batch Gradient Norm: 42.09652835863963
Epoch: 2821, Batch Gradient Norm after: 22.360677072624426
Epoch 2822/10000, Prediction Accuracy = 54.714%, Loss = 1.085619330406189
Epoch: 2822, Batch Gradient Norm: 37.96746109197851
Epoch: 2822, Batch Gradient Norm after: 22.360676696607015
Epoch 2823/10000, Prediction Accuracy = 54.77%, Loss = 1.0742455005645752
Epoch: 2823, Batch Gradient Norm: 42.0876045114653
Epoch: 2823, Batch Gradient Norm after: 22.36067852879651
Epoch 2824/10000, Prediction Accuracy = 54.72800000000001%, Loss = 1.0853052616119385
Epoch: 2824, Batch Gradient Norm: 37.961332919597545
Epoch: 2824, Batch Gradient Norm after: 22.360676931962427
Epoch 2825/10000, Prediction Accuracy = 54.779999999999994%, Loss = 1.0739300727844239
Epoch: 2825, Batch Gradient Norm: 42.08838872129525
Epoch: 2825, Batch Gradient Norm after: 22.360678379863046
Epoch 2826/10000, Prediction Accuracy = 54.739999999999995%, Loss = 1.0850167274475098
Epoch: 2826, Batch Gradient Norm: 37.95432409290243
Epoch: 2826, Batch Gradient Norm after: 22.36067820110766
Epoch 2827/10000, Prediction Accuracy = 54.78399999999999%, Loss = 1.073619508743286
Epoch: 2827, Batch Gradient Norm: 42.086308405015316
Epoch: 2827, Batch Gradient Norm after: 22.36067953994789
Epoch 2828/10000, Prediction Accuracy = 54.734%, Loss = 1.0847175121307373
Epoch: 2828, Batch Gradient Norm: 37.94714087443911
Epoch: 2828, Batch Gradient Norm after: 22.360678718598724
Epoch 2829/10000, Prediction Accuracy = 54.788%, Loss = 1.073309850692749
Epoch: 2829, Batch Gradient Norm: 42.07939327588617
Epoch: 2829, Batch Gradient Norm after: 22.360678029348698
Epoch 2830/10000, Prediction Accuracy = 54.748000000000005%, Loss = 1.084412431716919
Epoch: 2830, Batch Gradient Norm: 37.94155695336667
Epoch: 2830, Batch Gradient Norm after: 22.360678797435316
Epoch 2831/10000, Prediction Accuracy = 54.806000000000004%, Loss = 1.0730048656463622
Epoch: 2831, Batch Gradient Norm: 42.07557769793211
Epoch: 2831, Batch Gradient Norm after: 22.360678716890718
Epoch 2832/10000, Prediction Accuracy = 54.75200000000001%, Loss = 1.0841102123260498
Epoch: 2832, Batch Gradient Norm: 37.93613162867105
Epoch: 2832, Batch Gradient Norm after: 22.36067930927012
Epoch 2833/10000, Prediction Accuracy = 54.81%, Loss = 1.0727015495300294
Epoch: 2833, Batch Gradient Norm: 42.06719477002664
Epoch: 2833, Batch Gradient Norm after: 22.360679798248622
Epoch 2834/10000, Prediction Accuracy = 54.751999999999995%, Loss = 1.0838004112243653
Epoch: 2834, Batch Gradient Norm: 37.92979584771277
Epoch: 2834, Batch Gradient Norm after: 22.360677522956156
Epoch 2835/10000, Prediction Accuracy = 54.824%, Loss = 1.0723884344100951
Epoch: 2835, Batch Gradient Norm: 42.05667589759136
Epoch: 2835, Batch Gradient Norm after: 22.360678809740968
Epoch 2836/10000, Prediction Accuracy = 54.763999999999996%, Loss = 1.0834797620773315
Epoch: 2836, Batch Gradient Norm: 37.92296577450687
Epoch: 2836, Batch Gradient Norm after: 22.36067815915419
Epoch 2837/10000, Prediction Accuracy = 54.82199999999999%, Loss = 1.0720858097076416
Epoch: 2837, Batch Gradient Norm: 42.04464094644973
Epoch: 2837, Batch Gradient Norm after: 22.360677836890265
Epoch 2838/10000, Prediction Accuracy = 54.775999999999996%, Loss = 1.0831607103347778
Epoch: 2838, Batch Gradient Norm: 37.917599939415084
Epoch: 2838, Batch Gradient Norm after: 22.360678632775397
Epoch 2839/10000, Prediction Accuracy = 54.824%, Loss = 1.0717780828475951
Epoch: 2839, Batch Gradient Norm: 42.02811735085967
Epoch: 2839, Batch Gradient Norm after: 22.360677924606435
Epoch 2840/10000, Prediction Accuracy = 54.8%, Loss = 1.0828166484832764
Epoch: 2840, Batch Gradient Norm: 37.91302903720463
Epoch: 2840, Batch Gradient Norm after: 22.36067955634394
Epoch 2841/10000, Prediction Accuracy = 54.842000000000006%, Loss = 1.0714714527130127
Epoch: 2841, Batch Gradient Norm: 42.01352641212092
Epoch: 2841, Batch Gradient Norm after: 22.36067833290678
Epoch 2842/10000, Prediction Accuracy = 54.812%, Loss = 1.082480502128601
Epoch: 2842, Batch Gradient Norm: 37.90994918299449
Epoch: 2842, Batch Gradient Norm after: 22.360679169704593
Epoch 2843/10000, Prediction Accuracy = 54.838%, Loss = 1.071164894104004
Epoch: 2843, Batch Gradient Norm: 41.99861874573325
Epoch: 2843, Batch Gradient Norm after: 22.36067803753191
Epoch 2844/10000, Prediction Accuracy = 54.824%, Loss = 1.0821480751037598
Epoch: 2844, Batch Gradient Norm: 37.905602472827056
Epoch: 2844, Batch Gradient Norm after: 22.360678033952702
Epoch 2845/10000, Prediction Accuracy = 54.842000000000006%, Loss = 1.070858883857727
Epoch: 2845, Batch Gradient Norm: 41.980423308054455
Epoch: 2845, Batch Gradient Norm after: 22.360676906528703
Epoch 2846/10000, Prediction Accuracy = 54.827999999999996%, Loss = 1.081811547279358
Epoch: 2846, Batch Gradient Norm: 37.901756992601236
Epoch: 2846, Batch Gradient Norm after: 22.3606766813402
Epoch 2847/10000, Prediction Accuracy = 54.85%, Loss = 1.0705584764480591
Epoch: 2847, Batch Gradient Norm: 41.96153683353863
Epoch: 2847, Batch Gradient Norm after: 22.36067945393803
Epoch 2848/10000, Prediction Accuracy = 54.824%, Loss = 1.0814740657806396
Epoch: 2848, Batch Gradient Norm: 37.89686668822474
Epoch: 2848, Batch Gradient Norm after: 22.36067948692691
Epoch 2849/10000, Prediction Accuracy = 54.858000000000004%, Loss = 1.0702618598937987
Epoch: 2849, Batch Gradient Norm: 41.94486926313918
Epoch: 2849, Batch Gradient Norm after: 22.360679887847997
Epoch 2850/10000, Prediction Accuracy = 54.822%, Loss = 1.0811405658721924
Epoch: 2850, Batch Gradient Norm: 37.88971514986569
Epoch: 2850, Batch Gradient Norm after: 22.360679047723096
Epoch 2851/10000, Prediction Accuracy = 54.862%, Loss = 1.0699609994888306
Epoch: 2851, Batch Gradient Norm: 41.93147177038808
Epoch: 2851, Batch Gradient Norm after: 22.3606766926778
Epoch 2852/10000, Prediction Accuracy = 54.831999999999994%, Loss = 1.080813431739807
Epoch: 2852, Batch Gradient Norm: 37.88450301692073
Epoch: 2852, Batch Gradient Norm after: 22.360679023233285
Epoch 2853/10000, Prediction Accuracy = 54.878%, Loss = 1.069655990600586
Epoch: 2853, Batch Gradient Norm: 41.91754563929857
Epoch: 2853, Batch Gradient Norm after: 22.36068006193729
Epoch 2854/10000, Prediction Accuracy = 54.84400000000001%, Loss = 1.0804888963699342
Epoch: 2854, Batch Gradient Norm: 37.880340979974605
Epoch: 2854, Batch Gradient Norm after: 22.360676325610793
Epoch 2855/10000, Prediction Accuracy = 54.891999999999996%, Loss = 1.0693525314331054
Epoch: 2855, Batch Gradient Norm: 41.90787752469096
Epoch: 2855, Batch Gradient Norm after: 22.36068020776726
Epoch 2856/10000, Prediction Accuracy = 54.84599999999999%, Loss = 1.080165410041809
Epoch: 2856, Batch Gradient Norm: 37.87526854079713
Epoch: 2856, Batch Gradient Norm after: 22.360676732111887
Epoch 2857/10000, Prediction Accuracy = 54.891999999999996%, Loss = 1.0690449953079224
Epoch: 2857, Batch Gradient Norm: 41.895403486720596
Epoch: 2857, Batch Gradient Norm after: 22.360678287090572
Epoch 2858/10000, Prediction Accuracy = 54.846000000000004%, Loss = 1.079843807220459
Epoch: 2858, Batch Gradient Norm: 37.87063377342963
Epoch: 2858, Batch Gradient Norm after: 22.36067571910607
Epoch 2859/10000, Prediction Accuracy = 54.894000000000005%, Loss = 1.0687389850616456
Epoch: 2859, Batch Gradient Norm: 41.888186682648765
Epoch: 2859, Batch Gradient Norm after: 22.360678741749375
Epoch 2860/10000, Prediction Accuracy = 54.852%, Loss = 1.0795380115509032
Epoch: 2860, Batch Gradient Norm: 37.86197841499253
Epoch: 2860, Batch Gradient Norm after: 22.36067602215582
Epoch 2861/10000, Prediction Accuracy = 54.903999999999996%, Loss = 1.0684345245361329
Epoch: 2861, Batch Gradient Norm: 41.888154125722316
Epoch: 2861, Batch Gradient Norm after: 22.36067815896079
Epoch 2862/10000, Prediction Accuracy = 54.85799999999999%, Loss = 1.0792493104934693
Epoch: 2862, Batch Gradient Norm: 37.853964022639836
Epoch: 2862, Batch Gradient Norm after: 22.360677892969402
Epoch 2863/10000, Prediction Accuracy = 54.898%, Loss = 1.0681293725967407
Epoch: 2863, Batch Gradient Norm: 41.88536978326879
Epoch: 2863, Batch Gradient Norm after: 22.360678473475055
Epoch 2864/10000, Prediction Accuracy = 54.86800000000001%, Loss = 1.0789564847946167
Epoch: 2864, Batch Gradient Norm: 37.84769705870312
Epoch: 2864, Batch Gradient Norm after: 22.36067908812604
Epoch 2865/10000, Prediction Accuracy = 54.903999999999996%, Loss = 1.0678171634674072
Epoch: 2865, Batch Gradient Norm: 41.87915045617558
Epoch: 2865, Batch Gradient Norm after: 22.360678172055184
Epoch 2866/10000, Prediction Accuracy = 54.884%, Loss = 1.078657054901123
Epoch: 2866, Batch Gradient Norm: 37.841553941081784
Epoch: 2866, Batch Gradient Norm after: 22.360677640943553
Epoch 2867/10000, Prediction Accuracy = 54.916%, Loss = 1.0675112724304199
Epoch: 2867, Batch Gradient Norm: 41.87073499814704
Epoch: 2867, Batch Gradient Norm after: 22.360678999392828
Epoch 2868/10000, Prediction Accuracy = 54.884%, Loss = 1.0783443927764893
Epoch: 2868, Batch Gradient Norm: 37.83622993619746
Epoch: 2868, Batch Gradient Norm after: 22.36067676865415
Epoch 2869/10000, Prediction Accuracy = 54.92%, Loss = 1.0672025680541992
Epoch: 2869, Batch Gradient Norm: 41.86433856232655
Epoch: 2869, Batch Gradient Norm after: 22.360678601347
Epoch 2870/10000, Prediction Accuracy = 54.88599999999999%, Loss = 1.0780471801757812
Epoch: 2870, Batch Gradient Norm: 37.829668713625196
Epoch: 2870, Batch Gradient Norm after: 22.36067798456342
Epoch 2871/10000, Prediction Accuracy = 54.94%, Loss = 1.0668987035751343
Epoch: 2871, Batch Gradient Norm: 41.856023861587765
Epoch: 2871, Batch Gradient Norm after: 22.360675851933184
Epoch 2872/10000, Prediction Accuracy = 54.90599999999999%, Loss = 1.0777353286743163
Epoch: 2872, Batch Gradient Norm: 37.82347727261573
Epoch: 2872, Batch Gradient Norm after: 22.36067616876404
Epoch 2873/10000, Prediction Accuracy = 54.94000000000001%, Loss = 1.0665976047515868
Epoch: 2873, Batch Gradient Norm: 41.85064159368656
Epoch: 2873, Batch Gradient Norm after: 22.36067692959538
Epoch 2874/10000, Prediction Accuracy = 54.928%, Loss = 1.077437400817871
Epoch: 2874, Batch Gradient Norm: 37.817332917859154
Epoch: 2874, Batch Gradient Norm after: 22.360675034092075
Epoch 2875/10000, Prediction Accuracy = 54.96600000000001%, Loss = 1.066300868988037
Epoch: 2875, Batch Gradient Norm: 41.85001154559528
Epoch: 2875, Batch Gradient Norm after: 22.360677372907617
Epoch 2876/10000, Prediction Accuracy = 54.938%, Loss = 1.0771483421325683
Epoch: 2876, Batch Gradient Norm: 37.80869937476725
Epoch: 2876, Batch Gradient Norm after: 22.360676444574963
Epoch 2877/10000, Prediction Accuracy = 54.977999999999994%, Loss = 1.0659950971603394
Epoch: 2877, Batch Gradient Norm: 41.856507746584455
Epoch: 2877, Batch Gradient Norm after: 22.360678047536815
Epoch 2878/10000, Prediction Accuracy = 54.952%, Loss = 1.0768762350082397
Epoch: 2878, Batch Gradient Norm: 37.798540749774986
Epoch: 2878, Batch Gradient Norm after: 22.36067521373894
Epoch 2879/10000, Prediction Accuracy = 54.98%, Loss = 1.0656865596771241
Epoch: 2879, Batch Gradient Norm: 41.86467686930604
Epoch: 2879, Batch Gradient Norm after: 22.360678983234113
Epoch 2880/10000, Prediction Accuracy = 54.95799999999999%, Loss = 1.0766162157058716
Epoch: 2880, Batch Gradient Norm: 37.78954724973606
Epoch: 2880, Batch Gradient Norm after: 22.3606764259277
Epoch 2881/10000, Prediction Accuracy = 54.986000000000004%, Loss = 1.0653793573379517
Epoch: 2881, Batch Gradient Norm: 41.87092412394387
Epoch: 2881, Batch Gradient Norm after: 22.360679470172716
Epoch 2882/10000, Prediction Accuracy = 54.95399999999999%, Loss = 1.0763535976409913
Epoch: 2882, Batch Gradient Norm: 37.779544869261834
Epoch: 2882, Batch Gradient Norm after: 22.360678092455608
Epoch 2883/10000, Prediction Accuracy = 54.992%, Loss = 1.0650707244873048
Epoch: 2883, Batch Gradient Norm: 41.88022634893127
Epoch: 2883, Batch Gradient Norm after: 22.36067852851343
Epoch 2884/10000, Prediction Accuracy = 54.962%, Loss = 1.0760963916778565
Epoch: 2884, Batch Gradient Norm: 37.76973250110719
Epoch: 2884, Batch Gradient Norm after: 22.36067659117415
Epoch 2885/10000, Prediction Accuracy = 55.00599999999999%, Loss = 1.0647599220275878
Epoch: 2885, Batch Gradient Norm: 41.89078435270348
Epoch: 2885, Batch Gradient Norm after: 22.360679464968037
Epoch 2886/10000, Prediction Accuracy = 54.967999999999996%, Loss = 1.0758410692214966
Epoch: 2886, Batch Gradient Norm: 37.76097712383533
Epoch: 2886, Batch Gradient Norm after: 22.360674687733756
Epoch 2887/10000, Prediction Accuracy = 55.013999999999996%, Loss = 1.064455223083496
Epoch: 2887, Batch Gradient Norm: 41.89869823573849
Epoch: 2887, Batch Gradient Norm after: 22.360677327782398
Epoch 2888/10000, Prediction Accuracy = 54.980000000000004%, Loss = 1.0755828380584718
Epoch: 2888, Batch Gradient Norm: 37.75199804888284
Epoch: 2888, Batch Gradient Norm after: 22.360678190754147
Epoch 2889/10000, Prediction Accuracy = 55.024%, Loss = 1.0641539812088012
Epoch: 2889, Batch Gradient Norm: 41.90382099850086
Epoch: 2889, Batch Gradient Norm after: 22.360678577166944
Epoch 2890/10000, Prediction Accuracy = 54.989999999999995%, Loss = 1.0753255367279053
Epoch: 2890, Batch Gradient Norm: 37.74260823771572
Epoch: 2890, Batch Gradient Norm after: 22.36067929191336
Epoch 2891/10000, Prediction Accuracy = 55.044000000000004%, Loss = 1.0638489723205566
Epoch: 2891, Batch Gradient Norm: 41.9117393153806
Epoch: 2891, Batch Gradient Norm after: 22.36067818346957
Epoch 2892/10000, Prediction Accuracy = 55.008%, Loss = 1.0750702619552612
Epoch: 2892, Batch Gradient Norm: 37.7319608140038
Epoch: 2892, Batch Gradient Norm after: 22.360677778675278
Epoch 2893/10000, Prediction Accuracy = 55.053999999999995%, Loss = 1.0635448217391967
Epoch: 2893, Batch Gradient Norm: 41.92170846735584
Epoch: 2893, Batch Gradient Norm after: 22.360678551695038
Epoch 2894/10000, Prediction Accuracy = 55.025999999999996%, Loss = 1.074818992614746
Epoch: 2894, Batch Gradient Norm: 37.72437094033852
Epoch: 2894, Batch Gradient Norm after: 22.360677939666612
Epoch 2895/10000, Prediction Accuracy = 55.07000000000001%, Loss = 1.0632370948791503
Epoch: 2895, Batch Gradient Norm: 41.92746766234841
Epoch: 2895, Batch Gradient Norm after: 22.36068134654611
Epoch 2896/10000, Prediction Accuracy = 55.04599999999999%, Loss = 1.0745577812194824
Epoch: 2896, Batch Gradient Norm: 37.71550022430607
Epoch: 2896, Batch Gradient Norm after: 22.360676873963463
Epoch 2897/10000, Prediction Accuracy = 55.076%, Loss = 1.062932062149048
Epoch: 2897, Batch Gradient Norm: 41.93265096544854
Epoch: 2897, Batch Gradient Norm after: 22.36067983146137
Epoch 2898/10000, Prediction Accuracy = 55.06%, Loss = 1.0742969274520875
Epoch: 2898, Batch Gradient Norm: 37.706912866071974
Epoch: 2898, Batch Gradient Norm after: 22.360678118945643
Epoch 2899/10000, Prediction Accuracy = 55.10600000000001%, Loss = 1.0626280546188354
Epoch: 2899, Batch Gradient Norm: 41.94039501403261
Epoch: 2899, Batch Gradient Norm after: 22.36068022471728
Epoch 2900/10000, Prediction Accuracy = 55.053999999999995%, Loss = 1.0740392446517943
Epoch: 2900, Batch Gradient Norm: 37.69692041957655
Epoch: 2900, Batch Gradient Norm after: 22.36067703693758
Epoch 2901/10000, Prediction Accuracy = 55.120000000000005%, Loss = 1.062321376800537
Epoch: 2901, Batch Gradient Norm: 41.94356569447906
Epoch: 2901, Batch Gradient Norm after: 22.360676518583777
Epoch 2902/10000, Prediction Accuracy = 55.053999999999995%, Loss = 1.0737663507461548
Epoch: 2902, Batch Gradient Norm: 37.68741716983622
Epoch: 2902, Batch Gradient Norm after: 22.360680256026953
Epoch 2903/10000, Prediction Accuracy = 55.132000000000005%, Loss = 1.0620203733444213
Epoch: 2903, Batch Gradient Norm: 41.947336460580175
Epoch: 2903, Batch Gradient Norm after: 22.360676853627282
Epoch 2904/10000, Prediction Accuracy = 55.068000000000005%, Loss = 1.0734970808029174
Epoch: 2904, Batch Gradient Norm: 37.6793075430415
Epoch: 2904, Batch Gradient Norm after: 22.360679071928534
Epoch 2905/10000, Prediction Accuracy = 55.141999999999996%, Loss = 1.0617167234420777
Epoch: 2905, Batch Gradient Norm: 41.95000345591998
Epoch: 2905, Batch Gradient Norm after: 22.36067740385228
Epoch 2906/10000, Prediction Accuracy = 55.072%, Loss = 1.0732237577438355
Epoch: 2906, Batch Gradient Norm: 37.66991357672885
Epoch: 2906, Batch Gradient Norm after: 22.360677996353605
Epoch 2907/10000, Prediction Accuracy = 55.148%, Loss = 1.0614151239395142
Epoch: 2907, Batch Gradient Norm: 41.95644076744413
Epoch: 2907, Batch Gradient Norm after: 22.360678791366524
Epoch 2908/10000, Prediction Accuracy = 55.08200000000001%, Loss = 1.072965431213379
Epoch: 2908, Batch Gradient Norm: 37.66217692879388
Epoch: 2908, Batch Gradient Norm after: 22.36067655852082
Epoch 2909/10000, Prediction Accuracy = 55.162%, Loss = 1.0611089229583741
Epoch: 2909, Batch Gradient Norm: 41.960043746394575
Epoch: 2909, Batch Gradient Norm after: 22.360679388897868
Epoch 2910/10000, Prediction Accuracy = 55.088%, Loss = 1.0726914405822754
Epoch: 2910, Batch Gradient Norm: 37.65172544036022
Epoch: 2910, Batch Gradient Norm after: 22.360675687414314
Epoch 2911/10000, Prediction Accuracy = 55.181999999999995%, Loss = 1.0608041763305665
Epoch: 2911, Batch Gradient Norm: 41.962983168912004
Epoch: 2911, Batch Gradient Norm after: 22.360677702455238
Epoch 2912/10000, Prediction Accuracy = 55.093999999999994%, Loss = 1.0724200010299683
Epoch: 2912, Batch Gradient Norm: 37.643418833154136
Epoch: 2912, Batch Gradient Norm after: 22.36067776953125
Epoch 2913/10000, Prediction Accuracy = 55.194%, Loss = 1.0605027198791503
Epoch: 2913, Batch Gradient Norm: 41.96332869551413
Epoch: 2913, Batch Gradient Norm after: 22.36067791336233
Epoch 2914/10000, Prediction Accuracy = 55.105999999999995%, Loss = 1.072138738632202
Epoch: 2914, Batch Gradient Norm: 37.6351686850098
Epoch: 2914, Batch Gradient Norm after: 22.36067596428105
Epoch 2915/10000, Prediction Accuracy = 55.193999999999996%, Loss = 1.0601983785629272
Epoch: 2915, Batch Gradient Norm: 41.964144178850994
Epoch: 2915, Batch Gradient Norm after: 22.36067897325872
Epoch 2916/10000, Prediction Accuracy = 55.122%, Loss = 1.0718653678894043
Epoch: 2916, Batch Gradient Norm: 37.62612231221265
Epoch: 2916, Batch Gradient Norm after: 22.360676555399074
Epoch 2917/10000, Prediction Accuracy = 55.196000000000005%, Loss = 1.059897208213806
Epoch: 2917, Batch Gradient Norm: 41.963706832228425
Epoch: 2917, Batch Gradient Norm after: 22.360678294513598
Epoch 2918/10000, Prediction Accuracy = 55.134%, Loss = 1.071576189994812
Epoch: 2918, Batch Gradient Norm: 37.618851913389285
Epoch: 2918, Batch Gradient Norm after: 22.360676591076984
Epoch 2919/10000, Prediction Accuracy = 55.188%, Loss = 1.0595945119857788
Epoch: 2919, Batch Gradient Norm: 41.96299883415082
Epoch: 2919, Batch Gradient Norm after: 22.360679339813295
Epoch 2920/10000, Prediction Accuracy = 55.14200000000001%, Loss = 1.0712867975234985
Epoch: 2920, Batch Gradient Norm: 37.6115842822542
Epoch: 2920, Batch Gradient Norm after: 22.360678045255266
Epoch 2921/10000, Prediction Accuracy = 55.198%, Loss = 1.059291958808899
Epoch: 2921, Batch Gradient Norm: 41.96637402095497
Epoch: 2921, Batch Gradient Norm after: 22.360678053652002
Epoch 2922/10000, Prediction Accuracy = 55.153999999999996%, Loss = 1.0710112810134889
Epoch: 2922, Batch Gradient Norm: 37.60389432706162
Epoch: 2922, Batch Gradient Norm after: 22.360676427070004
Epoch 2923/10000, Prediction Accuracy = 55.20400000000001%, Loss = 1.058992123603821
Epoch: 2923, Batch Gradient Norm: 41.965129676854914
Epoch: 2923, Batch Gradient Norm after: 22.360678960345997
Epoch 2924/10000, Prediction Accuracy = 55.15999999999999%, Loss = 1.0707288980484009
Epoch: 2924, Batch Gradient Norm: 37.597434481175185
Epoch: 2924, Batch Gradient Norm after: 22.360676910512645
Epoch 2925/10000, Prediction Accuracy = 55.218%, Loss = 1.0586930513381958
Epoch: 2925, Batch Gradient Norm: 41.961155548495135
Epoch: 2925, Batch Gradient Norm after: 22.360676609014245
Epoch 2926/10000, Prediction Accuracy = 55.164%, Loss = 1.0704341888427735
Epoch: 2926, Batch Gradient Norm: 37.58939652613275
Epoch: 2926, Batch Gradient Norm after: 22.360678476755112
Epoch 2927/10000, Prediction Accuracy = 55.226%, Loss = 1.0583957433700562
Epoch: 2927, Batch Gradient Norm: 41.952778383762684
Epoch: 2927, Batch Gradient Norm after: 22.360677509499546
Epoch 2928/10000, Prediction Accuracy = 55.17%, Loss = 1.0701303243637086
Epoch: 2928, Batch Gradient Norm: 37.58280118961032
Epoch: 2928, Batch Gradient Norm after: 22.36067743958445
Epoch 2929/10000, Prediction Accuracy = 55.238%, Loss = 1.0580992221832275
Epoch: 2929, Batch Gradient Norm: 41.94932119026847
Epoch: 2929, Batch Gradient Norm after: 22.360677884182817
Epoch 2930/10000, Prediction Accuracy = 55.188%, Loss = 1.0698412656784058
Epoch: 2930, Batch Gradient Norm: 37.57451692480439
Epoch: 2930, Batch Gradient Norm after: 22.36067666811633
Epoch 2931/10000, Prediction Accuracy = 55.246%, Loss = 1.0578013896942138
Epoch: 2931, Batch Gradient Norm: 41.94697475839144
Epoch: 2931, Batch Gradient Norm after: 22.360678498636535
Epoch 2932/10000, Prediction Accuracy = 55.198%, Loss = 1.0695567846298217
Epoch: 2932, Batch Gradient Norm: 37.56790419783076
Epoch: 2932, Batch Gradient Norm after: 22.360677981622015
Epoch 2933/10000, Prediction Accuracy = 55.254%, Loss = 1.0575010538101197
Epoch: 2933, Batch Gradient Norm: 41.94061836731773
Epoch: 2933, Batch Gradient Norm after: 22.3606788692774
Epoch 2934/10000, Prediction Accuracy = 55.205999999999996%, Loss = 1.0692588329315185
Epoch: 2934, Batch Gradient Norm: 37.56036873959125
Epoch: 2934, Batch Gradient Norm after: 22.360677429213588
Epoch 2935/10000, Prediction Accuracy = 55.248000000000005%, Loss = 1.0572025060653687
Epoch: 2935, Batch Gradient Norm: 41.93058204959146
Epoch: 2935, Batch Gradient Norm after: 22.360676736564216
Epoch 2936/10000, Prediction Accuracy = 55.218%, Loss = 1.068956232070923
Epoch: 2936, Batch Gradient Norm: 37.55481015144556
Epoch: 2936, Batch Gradient Norm after: 22.360679009453193
Epoch 2937/10000, Prediction Accuracy = 55.25%, Loss = 1.0569021701812744
Epoch: 2937, Batch Gradient Norm: 41.926775987356194
Epoch: 2937, Batch Gradient Norm after: 22.360678964750264
Epoch 2938/10000, Prediction Accuracy = 55.227999999999994%, Loss = 1.0686528921127318
Epoch: 2938, Batch Gradient Norm: 37.546985423670606
Epoch: 2938, Batch Gradient Norm after: 22.36067775810755
Epoch 2939/10000, Prediction Accuracy = 55.25999999999999%, Loss = 1.0566091537475586
Epoch: 2939, Batch Gradient Norm: 41.921817107257006
Epoch: 2939, Batch Gradient Norm after: 22.360676458799954
Epoch 2940/10000, Prediction Accuracy = 55.238%, Loss = 1.0683631658554078
Epoch: 2940, Batch Gradient Norm: 37.538560603708234
Epoch: 2940, Batch Gradient Norm after: 22.36067748406535
Epoch 2941/10000, Prediction Accuracy = 55.257999999999996%, Loss = 1.0563124895095826
Epoch: 2941, Batch Gradient Norm: 41.919117116087264
Epoch: 2941, Batch Gradient Norm after: 22.360677365964904
Epoch 2942/10000, Prediction Accuracy = 55.238%, Loss = 1.068069624900818
Epoch: 2942, Batch Gradient Norm: 37.533569401086126
Epoch: 2942, Batch Gradient Norm after: 22.360678921066288
Epoch 2943/10000, Prediction Accuracy = 55.274%, Loss = 1.0560168743133544
Epoch: 2943, Batch Gradient Norm: 41.916656381220896
Epoch: 2943, Batch Gradient Norm after: 22.360675894594475
Epoch 2944/10000, Prediction Accuracy = 55.23%, Loss = 1.067788028717041
Epoch: 2944, Batch Gradient Norm: 37.527083912672225
Epoch: 2944, Batch Gradient Norm after: 22.360679029148585
Epoch 2945/10000, Prediction Accuracy = 55.28000000000001%, Loss = 1.0557228088378907
Epoch: 2945, Batch Gradient Norm: 41.91847818327236
Epoch: 2945, Batch Gradient Norm after: 22.360676078104753
Epoch 2946/10000, Prediction Accuracy = 55.24000000000001%, Loss = 1.0675082206726074
Epoch: 2946, Batch Gradient Norm: 37.518732573353915
Epoch: 2946, Batch Gradient Norm after: 22.360675642514348
Epoch 2947/10000, Prediction Accuracy = 55.294000000000004%, Loss = 1.0554226875305175
Epoch: 2947, Batch Gradient Norm: 41.91395231103441
Epoch: 2947, Batch Gradient Norm after: 22.360678138383932
Epoch 2948/10000, Prediction Accuracy = 55.238000000000014%, Loss = 1.0672126770019532
Epoch: 2948, Batch Gradient Norm: 37.51350213820564
Epoch: 2948, Batch Gradient Norm after: 22.36067590392672
Epoch 2949/10000, Prediction Accuracy = 55.3%, Loss = 1.0551252126693726
Epoch: 2949, Batch Gradient Norm: 41.90624169079931
Epoch: 2949, Batch Gradient Norm after: 22.360677141719787
Epoch 2950/10000, Prediction Accuracy = 55.25600000000001%, Loss = 1.06691517829895
Epoch: 2950, Batch Gradient Norm: 37.50650695952111
Epoch: 2950, Batch Gradient Norm after: 22.36067675826193
Epoch 2951/10000, Prediction Accuracy = 55.306%, Loss = 1.054824447631836
Epoch: 2951, Batch Gradient Norm: 41.894444841502406
Epoch: 2951, Batch Gradient Norm after: 22.360675893162043
Epoch 2952/10000, Prediction Accuracy = 55.26400000000001%, Loss = 1.0666035175323487
Epoch: 2952, Batch Gradient Norm: 37.50057707802536
Epoch: 2952, Batch Gradient Norm after: 22.36067601034147
Epoch 2953/10000, Prediction Accuracy = 55.318000000000005%, Loss = 1.054528498649597
Epoch: 2953, Batch Gradient Norm: 41.89183254026744
Epoch: 2953, Batch Gradient Norm after: 22.360677308562376
Epoch 2954/10000, Prediction Accuracy = 55.274%, Loss = 1.0663131952285767
Epoch: 2954, Batch Gradient Norm: 37.4934802795424
Epoch: 2954, Batch Gradient Norm after: 22.360678045096375
Epoch 2955/10000, Prediction Accuracy = 55.324%, Loss = 1.054234766960144
Epoch: 2955, Batch Gradient Norm: 41.88558656555539
Epoch: 2955, Batch Gradient Norm after: 22.36067784515586
Epoch 2956/10000, Prediction Accuracy = 55.279999999999994%, Loss = 1.0660169839859008
Epoch: 2956, Batch Gradient Norm: 37.48811946499917
Epoch: 2956, Batch Gradient Norm after: 22.360679333454126
Epoch 2957/10000, Prediction Accuracy = 55.33399999999999%, Loss = 1.0539408922195435
Epoch: 2957, Batch Gradient Norm: 41.8720872580294
Epoch: 2957, Batch Gradient Norm after: 22.360677454245067
Epoch 2958/10000, Prediction Accuracy = 55.290000000000006%, Loss = 1.0656991243362426
Epoch: 2958, Batch Gradient Norm: 37.481864035058464
Epoch: 2958, Batch Gradient Norm after: 22.360677836995084
Epoch 2959/10000, Prediction Accuracy = 55.338%, Loss = 1.0536468505859375
Epoch: 2959, Batch Gradient Norm: 41.857963356656725
Epoch: 2959, Batch Gradient Norm after: 22.360676193577703
Epoch 2960/10000, Prediction Accuracy = 55.294000000000004%, Loss = 1.0653765201568604
Epoch: 2960, Batch Gradient Norm: 37.47662650808957
Epoch: 2960, Batch Gradient Norm after: 22.36067743399211
Epoch 2961/10000, Prediction Accuracy = 55.338%, Loss = 1.0533554315567017
Epoch: 2961, Batch Gradient Norm: 41.84138980501167
Epoch: 2961, Batch Gradient Norm after: 22.360677631866903
Epoch 2962/10000, Prediction Accuracy = 55.294000000000004%, Loss = 1.0650451898574829
Epoch: 2962, Batch Gradient Norm: 37.47118515861105
Epoch: 2962, Batch Gradient Norm after: 22.36067946259631
Epoch 2963/10000, Prediction Accuracy = 55.362%, Loss = 1.053060483932495
Epoch: 2963, Batch Gradient Norm: 41.825418657476966
Epoch: 2963, Batch Gradient Norm after: 22.36067628430963
Epoch 2964/10000, Prediction Accuracy = 55.303999999999995%, Loss = 1.0647241115570067
Epoch: 2964, Batch Gradient Norm: 37.463795813777615
Epoch: 2964, Batch Gradient Norm after: 22.360680054920987
Epoch 2965/10000, Prediction Accuracy = 55.379999999999995%, Loss = 1.0527690649032593
Epoch: 2965, Batch Gradient Norm: 41.80335368058735
Epoch: 2965, Batch Gradient Norm after: 22.360676925520796
Epoch 2966/10000, Prediction Accuracy = 55.303999999999995%, Loss = 1.064384150505066
Epoch: 2966, Batch Gradient Norm: 37.45844866519415
Epoch: 2966, Batch Gradient Norm after: 22.36067775226044
Epoch 2967/10000, Prediction Accuracy = 55.376%, Loss = 1.0524813652038574
Epoch: 2967, Batch Gradient Norm: 41.78559495670681
Epoch: 2967, Batch Gradient Norm after: 22.3606767515382
Epoch 2968/10000, Prediction Accuracy = 55.294000000000004%, Loss = 1.0640489339828492
Epoch: 2968, Batch Gradient Norm: 37.454596563762664
Epoch: 2968, Batch Gradient Norm after: 22.360676578782638
Epoch 2969/10000, Prediction Accuracy = 55.372%, Loss = 1.0521948337554932
Epoch: 2969, Batch Gradient Norm: 41.77086136491543
Epoch: 2969, Batch Gradient Norm after: 22.360677094706073
Epoch 2970/10000, Prediction Accuracy = 55.302%, Loss = 1.0637396097183227
Epoch: 2970, Batch Gradient Norm: 37.448415957633955
Epoch: 2970, Batch Gradient Norm after: 22.360678649366108
Epoch 2971/10000, Prediction Accuracy = 55.384%, Loss = 1.0519069671630858
Epoch: 2971, Batch Gradient Norm: 41.764447447827656
Epoch: 2971, Batch Gradient Norm after: 22.360676301014095
Epoch 2972/10000, Prediction Accuracy = 55.30800000000001%, Loss = 1.0634500980377197
Epoch: 2972, Batch Gradient Norm: 37.44230103297186
Epoch: 2972, Batch Gradient Norm after: 22.360676171824593
Epoch 2973/10000, Prediction Accuracy = 55.396%, Loss = 1.0516151189804077
Epoch: 2973, Batch Gradient Norm: 41.75478529900767
Epoch: 2973, Batch Gradient Norm after: 22.360677601117118
Epoch 2974/10000, Prediction Accuracy = 55.312%, Loss = 1.0631491899490357
Epoch: 2974, Batch Gradient Norm: 37.43609828401859
Epoch: 2974, Batch Gradient Norm after: 22.36067875263951
Epoch 2975/10000, Prediction Accuracy = 55.408%, Loss = 1.0513229846954346
Epoch: 2975, Batch Gradient Norm: 41.74815250544842
Epoch: 2975, Batch Gradient Norm after: 22.360677665948323
Epoch 2976/10000, Prediction Accuracy = 55.322%, Loss = 1.0628474473953247
Epoch: 2976, Batch Gradient Norm: 37.429499110704924
Epoch: 2976, Batch Gradient Norm after: 22.360676893532343
Epoch 2977/10000, Prediction Accuracy = 55.403999999999996%, Loss = 1.0510349512100219
Epoch: 2977, Batch Gradient Norm: 41.74454551616479
Epoch: 2977, Batch Gradient Norm after: 22.360677797332446
Epoch 2978/10000, Prediction Accuracy = 55.318%, Loss = 1.0625689506530762
Epoch: 2978, Batch Gradient Norm: 37.42438856038791
Epoch: 2978, Batch Gradient Norm after: 22.360676466454414
Epoch 2979/10000, Prediction Accuracy = 55.408%, Loss = 1.0507421731948852
Epoch: 2979, Batch Gradient Norm: 41.74078716734854
Epoch: 2979, Batch Gradient Norm after: 22.360677594567154
Epoch 2980/10000, Prediction Accuracy = 55.31999999999999%, Loss = 1.062281894683838
Epoch: 2980, Batch Gradient Norm: 37.418936077618696
Epoch: 2980, Batch Gradient Norm after: 22.360677736627686
Epoch 2981/10000, Prediction Accuracy = 55.414%, Loss = 1.0504502534866333
Epoch: 2981, Batch Gradient Norm: 41.73691192800892
Epoch: 2981, Batch Gradient Norm after: 22.360678547273306
Epoch 2982/10000, Prediction Accuracy = 55.326%, Loss = 1.061986255645752
Epoch: 2982, Batch Gradient Norm: 37.4135418010884
Epoch: 2982, Batch Gradient Norm after: 22.360679472367085
Epoch 2983/10000, Prediction Accuracy = 55.416%, Loss = 1.0501534938812256
Epoch: 2983, Batch Gradient Norm: 41.725679907743825
Epoch: 2983, Batch Gradient Norm after: 22.360677465118375
Epoch 2984/10000, Prediction Accuracy = 55.33%, Loss = 1.0616857290267945
Epoch: 2984, Batch Gradient Norm: 37.40930684613196
Epoch: 2984, Batch Gradient Norm after: 22.360676673725784
Epoch 2985/10000, Prediction Accuracy = 55.422000000000004%, Loss = 1.0498641967773437
Epoch: 2985, Batch Gradient Norm: 41.71698207636487
Epoch: 2985, Batch Gradient Norm after: 22.360677422905784
Epoch 2986/10000, Prediction Accuracy = 55.327999999999996%, Loss = 1.0613842248916625
Epoch: 2986, Batch Gradient Norm: 37.40333502997796
Epoch: 2986, Batch Gradient Norm after: 22.360679322976818
Epoch 2987/10000, Prediction Accuracy = 55.43599999999999%, Loss = 1.0495779037475585
Epoch: 2987, Batch Gradient Norm: 41.704372260106815
Epoch: 2987, Batch Gradient Norm after: 22.360678203742353
Epoch 2988/10000, Prediction Accuracy = 55.336%, Loss = 1.0610743761062622
Epoch: 2988, Batch Gradient Norm: 37.397338954504825
Epoch: 2988, Batch Gradient Norm after: 22.360678908892357
Epoch 2989/10000, Prediction Accuracy = 55.443999999999996%, Loss = 1.0492820262908935
Epoch: 2989, Batch Gradient Norm: 41.694359662505455
Epoch: 2989, Batch Gradient Norm after: 22.36067678985207
Epoch 2990/10000, Prediction Accuracy = 55.336%, Loss = 1.060774278640747
Epoch: 2990, Batch Gradient Norm: 37.39056924756256
Epoch: 2990, Batch Gradient Norm after: 22.36067874936056
Epoch 2991/10000, Prediction Accuracy = 55.45%, Loss = 1.0489970922470093
Epoch: 2991, Batch Gradient Norm: 41.680756745632735
Epoch: 2991, Batch Gradient Norm after: 22.36067819459096
Epoch 2992/10000, Prediction Accuracy = 55.346000000000004%, Loss = 1.060460352897644
Epoch: 2992, Batch Gradient Norm: 37.38665788436289
Epoch: 2992, Batch Gradient Norm after: 22.36067899481256
Epoch 2993/10000, Prediction Accuracy = 55.456%, Loss = 1.0487064599990845
Epoch: 2993, Batch Gradient Norm: 41.67441188335945
Epoch: 2993, Batch Gradient Norm after: 22.360678204629632
Epoch 2994/10000, Prediction Accuracy = 55.354%, Loss = 1.0601643085479737
Epoch: 2994, Batch Gradient Norm: 37.379822129850155
Epoch: 2994, Batch Gradient Norm after: 22.360678929047047
Epoch 2995/10000, Prediction Accuracy = 55.462%, Loss = 1.048419189453125
Epoch: 2995, Batch Gradient Norm: 41.669570994824554
Epoch: 2995, Batch Gradient Norm after: 22.360676892453345
Epoch 2996/10000, Prediction Accuracy = 55.366%, Loss = 1.0598754405975341
Epoch: 2996, Batch Gradient Norm: 37.37310791635138
Epoch: 2996, Batch Gradient Norm after: 22.36067712288003
Epoch 2997/10000, Prediction Accuracy = 55.474000000000004%, Loss = 1.048128032684326
Epoch: 2997, Batch Gradient Norm: 41.659701548967476
Epoch: 2997, Batch Gradient Norm after: 22.36067831491894
Epoch 2998/10000, Prediction Accuracy = 55.362%, Loss = 1.0595769882202148
Epoch: 2998, Batch Gradient Norm: 37.36809932141162
Epoch: 2998, Batch Gradient Norm after: 22.360677878600534
Epoch 2999/10000, Prediction Accuracy = 55.484%, Loss = 1.0478405714035035
Epoch: 2999, Batch Gradient Norm: 41.646759877827435
Epoch: 2999, Batch Gradient Norm after: 22.360677389016914
Epoch 3000/10000, Prediction Accuracy = 55.372%, Loss = 1.0592734575271607
Epoch: 3000, Batch Gradient Norm: 37.36046149277104
Epoch: 3000, Batch Gradient Norm after: 22.360675858702482
Epoch 3001/10000, Prediction Accuracy = 55.495999999999995%, Loss = 1.0475584983825683
Epoch: 3001, Batch Gradient Norm: 41.6337921052377
Epoch: 3001, Batch Gradient Norm after: 22.360677590707244
Epoch 3002/10000, Prediction Accuracy = 55.378%, Loss = 1.058965539932251
Epoch: 3002, Batch Gradient Norm: 37.35647351201016
Epoch: 3002, Batch Gradient Norm after: 22.36067674649048
Epoch 3003/10000, Prediction Accuracy = 55.498000000000005%, Loss = 1.0472698211669922
Epoch: 3003, Batch Gradient Norm: 41.61598686365184
Epoch: 3003, Batch Gradient Norm after: 22.36067702887329
Epoch 3004/10000, Prediction Accuracy = 55.37800000000001%, Loss = 1.0586541414260864
Epoch: 3004, Batch Gradient Norm: 37.3513197538819
Epoch: 3004, Batch Gradient Norm after: 22.36067530720682
Epoch 3005/10000, Prediction Accuracy = 55.498000000000005%, Loss = 1.0469859838485718
Epoch: 3005, Batch Gradient Norm: 41.6017201930408
Epoch: 3005, Batch Gradient Norm after: 22.360679900038043
Epoch 3006/10000, Prediction Accuracy = 55.386%, Loss = 1.0583391427993774
Epoch: 3006, Batch Gradient Norm: 37.34749711438643
Epoch: 3006, Batch Gradient Norm after: 22.360677662983953
Epoch 3007/10000, Prediction Accuracy = 55.50599999999999%, Loss = 1.0467005252838135
Epoch: 3007, Batch Gradient Norm: 41.585707522635644
Epoch: 3007, Batch Gradient Norm after: 22.360679218347517
Epoch 3008/10000, Prediction Accuracy = 55.402%, Loss = 1.0580161571502686
Epoch: 3008, Batch Gradient Norm: 37.34210752353781
Epoch: 3008, Batch Gradient Norm after: 22.360676890509158
Epoch 3009/10000, Prediction Accuracy = 55.512%, Loss = 1.0464144468307495
Epoch: 3009, Batch Gradient Norm: 41.5706959204794
Epoch: 3009, Batch Gradient Norm after: 22.360678447553155
Epoch 3010/10000, Prediction Accuracy = 55.416%, Loss = 1.0577079772949218
Epoch: 3010, Batch Gradient Norm: 37.33733518265333
Epoch: 3010, Batch Gradient Norm after: 22.36067908992235
Epoch 3011/10000, Prediction Accuracy = 55.51800000000001%, Loss = 1.046130633354187
Epoch: 3011, Batch Gradient Norm: 41.55839423613487
Epoch: 3011, Batch Gradient Norm after: 22.360676616778836
Epoch 3012/10000, Prediction Accuracy = 55.424%, Loss = 1.0574052572250365
Epoch: 3012, Batch Gradient Norm: 37.332130670285935
Epoch: 3012, Batch Gradient Norm after: 22.3606782401569
Epoch 3013/10000, Prediction Accuracy = 55.517999999999994%, Loss = 1.0458472967147827
Epoch: 3013, Batch Gradient Norm: 41.541329383797425
Epoch: 3013, Batch Gradient Norm after: 22.360679376413227
Epoch 3014/10000, Prediction Accuracy = 55.44200000000001%, Loss = 1.0570874452590941
Epoch: 3014, Batch Gradient Norm: 37.326575793745285
Epoch: 3014, Batch Gradient Norm after: 22.360679790590332
Epoch 3015/10000, Prediction Accuracy = 55.524%, Loss = 1.045565676689148
Epoch: 3015, Batch Gradient Norm: 41.521547380357994
Epoch: 3015, Batch Gradient Norm after: 22.36067708607811
Epoch 3016/10000, Prediction Accuracy = 55.455999999999996%, Loss = 1.0567618608474731
Epoch: 3016, Batch Gradient Norm: 37.32035292517768
Epoch: 3016, Batch Gradient Norm after: 22.360679208016553
Epoch 3017/10000, Prediction Accuracy = 55.53000000000001%, Loss = 1.0452919244766234
Epoch: 3017, Batch Gradient Norm: 41.50620664724605
Epoch: 3017, Batch Gradient Norm after: 22.36067824030548
Epoch 3018/10000, Prediction Accuracy = 55.468%, Loss = 1.0564473152160645
Epoch: 3018, Batch Gradient Norm: 37.31458169472286
Epoch: 3018, Batch Gradient Norm after: 22.36067755289579
Epoch 3019/10000, Prediction Accuracy = 55.532000000000004%, Loss = 1.045010542869568
Epoch: 3019, Batch Gradient Norm: 41.489897582450844
Epoch: 3019, Batch Gradient Norm after: 22.36067810710962
Epoch 3020/10000, Prediction Accuracy = 55.455999999999996%, Loss = 1.0561369419097901
Epoch: 3020, Batch Gradient Norm: 37.3100529150333
Epoch: 3020, Batch Gradient Norm after: 22.360676745517168
Epoch 3021/10000, Prediction Accuracy = 55.54%, Loss = 1.0447263240814209
Epoch: 3021, Batch Gradient Norm: 41.47440730828226
Epoch: 3021, Batch Gradient Norm after: 22.36067769130452
Epoch 3022/10000, Prediction Accuracy = 55.477999999999994%, Loss = 1.0558337688446044
Epoch: 3022, Batch Gradient Norm: 37.30471352589337
Epoch: 3022, Batch Gradient Norm after: 22.36067481759481
Epoch 3023/10000, Prediction Accuracy = 55.54600000000001%, Loss = 1.0444443702697754
Epoch: 3023, Batch Gradient Norm: 41.46215535254058
Epoch: 3023, Batch Gradient Norm after: 22.360678386800227
Epoch 3024/10000, Prediction Accuracy = 55.477999999999994%, Loss = 1.0555299520492554
Epoch: 3024, Batch Gradient Norm: 37.29955279636838
Epoch: 3024, Batch Gradient Norm after: 22.360676575221547
Epoch 3025/10000, Prediction Accuracy = 55.548%, Loss = 1.044159436225891
Epoch: 3025, Batch Gradient Norm: 41.44918615759839
Epoch: 3025, Batch Gradient Norm after: 22.360678201700747
Epoch 3026/10000, Prediction Accuracy = 55.482000000000006%, Loss = 1.0552203893661498
Epoch: 3026, Batch Gradient Norm: 37.29478503044914
Epoch: 3026, Batch Gradient Norm after: 22.36067997696024
Epoch 3027/10000, Prediction Accuracy = 55.55800000000001%, Loss = 1.0438793897628784
Epoch: 3027, Batch Gradient Norm: 41.433611731555295
Epoch: 3027, Batch Gradient Norm after: 22.360678189588622
Epoch 3028/10000, Prediction Accuracy = 55.488%, Loss = 1.0549116849899292
Epoch: 3028, Batch Gradient Norm: 37.291114025408554
Epoch: 3028, Batch Gradient Norm after: 22.360677496144515
Epoch 3029/10000, Prediction Accuracy = 55.576%, Loss = 1.04360191822052
Epoch: 3029, Batch Gradient Norm: 41.4212964787147
Epoch: 3029, Batch Gradient Norm after: 22.360677893941165
Epoch 3030/10000, Prediction Accuracy = 55.50600000000001%, Loss = 1.0546106100082397
Epoch: 3030, Batch Gradient Norm: 37.283762701569124
Epoch: 3030, Batch Gradient Norm after: 22.36067762510501
Epoch 3031/10000, Prediction Accuracy = 55.588%, Loss = 1.0433217525482177
Epoch: 3031, Batch Gradient Norm: 41.41324817800827
Epoch: 3031, Batch Gradient Norm after: 22.360676889257668
Epoch 3032/10000, Prediction Accuracy = 55.50600000000001%, Loss = 1.0543291330337525
Epoch: 3032, Batch Gradient Norm: 37.27844349043133
Epoch: 3032, Batch Gradient Norm after: 22.36067717760838
Epoch 3033/10000, Prediction Accuracy = 55.59400000000001%, Loss = 1.0430364608764648
Epoch: 3033, Batch Gradient Norm: 41.40374134880243
Epoch: 3033, Batch Gradient Norm after: 22.360677479213315
Epoch 3034/10000, Prediction Accuracy = 55.516000000000005%, Loss = 1.0540402412414551
Epoch: 3034, Batch Gradient Norm: 37.27328201919686
Epoch: 3034, Batch Gradient Norm after: 22.360677429840138
Epoch 3035/10000, Prediction Accuracy = 55.596000000000004%, Loss = 1.0427531242370605
Epoch: 3035, Batch Gradient Norm: 41.40082519605688
Epoch: 3035, Batch Gradient Norm after: 22.36068078953804
Epoch 3036/10000, Prediction Accuracy = 55.529999999999994%, Loss = 1.053761887550354
Epoch: 3036, Batch Gradient Norm: 37.26793253803792
Epoch: 3036, Batch Gradient Norm after: 22.36067676065707
Epoch 3037/10000, Prediction Accuracy = 55.604%, Loss = 1.0424680233001709
Epoch: 3037, Batch Gradient Norm: 41.39069364046859
Epoch: 3037, Batch Gradient Norm after: 22.360677220568796
Epoch 3038/10000, Prediction Accuracy = 55.536%, Loss = 1.0534806966781616
Epoch: 3038, Batch Gradient Norm: 37.26175217720516
Epoch: 3038, Batch Gradient Norm after: 22.360676417495117
Epoch 3039/10000, Prediction Accuracy = 55.60799999999999%, Loss = 1.0421851396560669
Epoch: 3039, Batch Gradient Norm: 41.379791134547055
Epoch: 3039, Batch Gradient Norm after: 22.360677647292583
Epoch 3040/10000, Prediction Accuracy = 55.54200000000001%, Loss = 1.053184151649475
Epoch: 3040, Batch Gradient Norm: 37.25762377249651
Epoch: 3040, Batch Gradient Norm after: 22.360676727579
Epoch 3041/10000, Prediction Accuracy = 55.612%, Loss = 1.0419036865234375
Epoch: 3041, Batch Gradient Norm: 41.36710924020061
Epoch: 3041, Batch Gradient Norm after: 22.36067618729088
Epoch 3042/10000, Prediction Accuracy = 55.568%, Loss = 1.052880859375
Epoch: 3042, Batch Gradient Norm: 37.252474727887645
Epoch: 3042, Batch Gradient Norm after: 22.36067826311268
Epoch 3043/10000, Prediction Accuracy = 55.61%, Loss = 1.0416248798370362
Epoch: 3043, Batch Gradient Norm: 41.35591444020538
Epoch: 3043, Batch Gradient Norm after: 22.360676741862747
Epoch 3044/10000, Prediction Accuracy = 55.568%, Loss = 1.0525852918624878
Epoch: 3044, Batch Gradient Norm: 37.24705508018244
Epoch: 3044, Batch Gradient Norm after: 22.360676210672633
Epoch 3045/10000, Prediction Accuracy = 55.60799999999999%, Loss = 1.0413454294204711
Epoch: 3045, Batch Gradient Norm: 41.35071243168814
Epoch: 3045, Batch Gradient Norm after: 22.360676301333836
Epoch 3046/10000, Prediction Accuracy = 55.574%, Loss = 1.0523000955581665
Epoch: 3046, Batch Gradient Norm: 37.24038179972023
Epoch: 3046, Batch Gradient Norm after: 22.36067868628105
Epoch 3047/10000, Prediction Accuracy = 55.604%, Loss = 1.0410631895065308
Epoch: 3047, Batch Gradient Norm: 41.34666627176213
Epoch: 3047, Batch Gradient Norm after: 22.36067592635161
Epoch 3048/10000, Prediction Accuracy = 55.576%, Loss = 1.0520298957824707
Epoch: 3048, Batch Gradient Norm: 37.23360935604183
Epoch: 3048, Batch Gradient Norm after: 22.36067615217463
Epoch 3049/10000, Prediction Accuracy = 55.605999999999995%, Loss = 1.0407779216766357
Epoch: 3049, Batch Gradient Norm: 41.34098401641424
Epoch: 3049, Batch Gradient Norm after: 22.36067858772053
Epoch 3050/10000, Prediction Accuracy = 55.577999999999996%, Loss = 1.0517602920532227
Epoch: 3050, Batch Gradient Norm: 37.224403877918476
Epoch: 3050, Batch Gradient Norm after: 22.36067700667825
Epoch 3051/10000, Prediction Accuracy = 55.616%, Loss = 1.0404946565628053
Epoch: 3051, Batch Gradient Norm: 41.33421713628239
Epoch: 3051, Batch Gradient Norm after: 22.360677112213413
Epoch 3052/10000, Prediction Accuracy = 55.58399999999999%, Loss = 1.051478886604309
Epoch: 3052, Batch Gradient Norm: 37.216486396935416
Epoch: 3052, Batch Gradient Norm after: 22.360675004827947
Epoch 3053/10000, Prediction Accuracy = 55.61800000000001%, Loss = 1.0402133464813232
Epoch: 3053, Batch Gradient Norm: 41.32455295678633
Epoch: 3053, Batch Gradient Norm after: 22.36067814982137
Epoch 3054/10000, Prediction Accuracy = 55.592%, Loss = 1.051190233230591
Epoch: 3054, Batch Gradient Norm: 37.210197795505614
Epoch: 3054, Batch Gradient Norm after: 22.36067625465606
Epoch 3055/10000, Prediction Accuracy = 55.622%, Loss = 1.0399327993392944
Epoch: 3055, Batch Gradient Norm: 41.31510921185042
Epoch: 3055, Batch Gradient Norm after: 22.36067502325957
Epoch 3056/10000, Prediction Accuracy = 55.614%, Loss = 1.050897216796875
Epoch: 3056, Batch Gradient Norm: 37.205786788579935
Epoch: 3056, Batch Gradient Norm after: 22.360677342304545
Epoch 3057/10000, Prediction Accuracy = 55.63599999999999%, Loss = 1.0396503686904908
Epoch: 3057, Batch Gradient Norm: 41.30459101180771
Epoch: 3057, Batch Gradient Norm after: 22.36067557658049
Epoch 3058/10000, Prediction Accuracy = 55.634%, Loss = 1.0505996704101563
Epoch: 3058, Batch Gradient Norm: 37.19910781802936
Epoch: 3058, Batch Gradient Norm after: 22.36067806225393
Epoch 3059/10000, Prediction Accuracy = 55.64%, Loss = 1.0393657445907594
Epoch: 3059, Batch Gradient Norm: 41.30054452644534
Epoch: 3059, Batch Gradient Norm after: 22.360678159151952
Epoch 3060/10000, Prediction Accuracy = 55.622%, Loss = 1.0503101110458375
Epoch: 3060, Batch Gradient Norm: 37.19300679640003
Epoch: 3060, Batch Gradient Norm after: 22.360677499828565
Epoch 3061/10000, Prediction Accuracy = 55.64399999999999%, Loss = 1.0390896558761598
Epoch: 3061, Batch Gradient Norm: 41.29770251076441
Epoch: 3061, Batch Gradient Norm after: 22.360677577280324
Epoch 3062/10000, Prediction Accuracy = 55.626%, Loss = 1.0500382423400878
Epoch: 3062, Batch Gradient Norm: 37.18509478230697
Epoch: 3062, Batch Gradient Norm after: 22.36067720423361
Epoch 3063/10000, Prediction Accuracy = 55.656000000000006%, Loss = 1.0388075113296509
Epoch: 3063, Batch Gradient Norm: 41.291356829047515
Epoch: 3063, Batch Gradient Norm after: 22.360677834569554
Epoch 3064/10000, Prediction Accuracy = 55.636%, Loss = 1.0497671365737915
Epoch: 3064, Batch Gradient Norm: 37.179350009059725
Epoch: 3064, Batch Gradient Norm after: 22.360676240244032
Epoch 3065/10000, Prediction Accuracy = 55.669999999999995%, Loss = 1.0385238885879517
Epoch: 3065, Batch Gradient Norm: 41.28510967032844
Epoch: 3065, Batch Gradient Norm after: 22.360677342746136
Epoch 3066/10000, Prediction Accuracy = 55.641999999999996%, Loss = 1.0494823932647706
Epoch: 3066, Batch Gradient Norm: 37.17497013934271
Epoch: 3066, Batch Gradient Norm after: 22.360676003398158
Epoch 3067/10000, Prediction Accuracy = 55.666%, Loss = 1.0382444381713867
Epoch: 3067, Batch Gradient Norm: 41.27671871267372
Epoch: 3067, Batch Gradient Norm after: 22.360676112581373
Epoch 3068/10000, Prediction Accuracy = 55.658%, Loss = 1.0491975784301757
Epoch: 3068, Batch Gradient Norm: 37.16720433659261
Epoch: 3068, Batch Gradient Norm after: 22.36067663027633
Epoch 3069/10000, Prediction Accuracy = 55.67199999999999%, Loss = 1.0379660844802856
Epoch: 3069, Batch Gradient Norm: 41.27232815843472
Epoch: 3069, Batch Gradient Norm after: 22.360679865008695
Epoch 3070/10000, Prediction Accuracy = 55.66600000000001%, Loss = 1.0489222288131714
Epoch: 3070, Batch Gradient Norm: 37.16117277745527
Epoch: 3070, Batch Gradient Norm after: 22.360678981704066
Epoch 3071/10000, Prediction Accuracy = 55.69200000000001%, Loss = 1.03768789768219
Epoch: 3071, Batch Gradient Norm: 41.264360594967044
Epoch: 3071, Batch Gradient Norm after: 22.360678639675566
Epoch 3072/10000, Prediction Accuracy = 55.682%, Loss = 1.0486383199691773
Epoch: 3072, Batch Gradient Norm: 37.1555507222589
Epoch: 3072, Batch Gradient Norm after: 22.360679491677228
Epoch 3073/10000, Prediction Accuracy = 55.7%, Loss = 1.0374059915542602
Epoch: 3073, Batch Gradient Norm: 41.25950873251009
Epoch: 3073, Batch Gradient Norm after: 22.360678284887925
Epoch 3074/10000, Prediction Accuracy = 55.684000000000005%, Loss = 1.0483664989471435
Epoch: 3074, Batch Gradient Norm: 37.14719908191574
Epoch: 3074, Batch Gradient Norm after: 22.360676543579185
Epoch 3075/10000, Prediction Accuracy = 55.698%, Loss = 1.0371290922164917
Epoch: 3075, Batch Gradient Norm: 41.255706923591
Epoch: 3075, Batch Gradient Norm after: 22.36067759049858
Epoch 3076/10000, Prediction Accuracy = 55.682%, Loss = 1.04809513092041
Epoch: 3076, Batch Gradient Norm: 37.13997138777724
Epoch: 3076, Batch Gradient Norm after: 22.36067726463169
Epoch 3077/10000, Prediction Accuracy = 55.696000000000005%, Loss = 1.0368497371673584
Epoch: 3077, Batch Gradient Norm: 41.248204888378325
Epoch: 3077, Batch Gradient Norm after: 22.360680901549085
Epoch 3078/10000, Prediction Accuracy = 55.69%, Loss = 1.047809648513794
Epoch: 3078, Batch Gradient Norm: 37.13409442163636
Epoch: 3078, Batch Gradient Norm after: 22.36067744231471
Epoch 3079/10000, Prediction Accuracy = 55.708000000000006%, Loss = 1.0365705490112305
Epoch: 3079, Batch Gradient Norm: 41.240914971572494
Epoch: 3079, Batch Gradient Norm after: 22.360677971302554
Epoch 3080/10000, Prediction Accuracy = 55.702%, Loss = 1.04752516746521
Epoch: 3080, Batch Gradient Norm: 37.12484072603394
Epoch: 3080, Batch Gradient Norm after: 22.36067883880357
Epoch 3081/10000, Prediction Accuracy = 55.724000000000004%, Loss = 1.0362941026687622
Epoch: 3081, Batch Gradient Norm: 41.234312531822425
Epoch: 3081, Batch Gradient Norm after: 22.360678127208892
Epoch 3082/10000, Prediction Accuracy = 55.71600000000001%, Loss = 1.0472533464431764
Epoch: 3082, Batch Gradient Norm: 37.11652265074808
Epoch: 3082, Batch Gradient Norm after: 22.360677803491495
Epoch 3083/10000, Prediction Accuracy = 55.732000000000006%, Loss = 1.0360210180282592
Epoch: 3083, Batch Gradient Norm: 41.23522169496904
Epoch: 3083, Batch Gradient Norm after: 22.36068035846547
Epoch 3084/10000, Prediction Accuracy = 55.730000000000004%, Loss = 1.046993660926819
Epoch: 3084, Batch Gradient Norm: 37.10791077817253
Epoch: 3084, Batch Gradient Norm after: 22.360674882638143
Epoch 3085/10000, Prediction Accuracy = 55.734%, Loss = 1.0357404232025147
Epoch: 3085, Batch Gradient Norm: 41.233876835767354
Epoch: 3085, Batch Gradient Norm after: 22.360677532013895
Epoch 3086/10000, Prediction Accuracy = 55.742%, Loss = 1.0467320442199708
Epoch: 3086, Batch Gradient Norm: 37.09989750749005
Epoch: 3086, Batch Gradient Norm after: 22.360674562013884
Epoch 3087/10000, Prediction Accuracy = 55.74399999999999%, Loss = 1.0354652881622315
Epoch: 3087, Batch Gradient Norm: 41.231386787574145
Epoch: 3087, Batch Gradient Norm after: 22.360676428951287
Epoch 3088/10000, Prediction Accuracy = 55.75%, Loss = 1.0464710474014283
Epoch: 3088, Batch Gradient Norm: 37.091248326435334
Epoch: 3088, Batch Gradient Norm after: 22.360676804053444
Epoch 3089/10000, Prediction Accuracy = 55.75%, Loss = 1.03518807888031
Epoch: 3089, Batch Gradient Norm: 41.227418737176336
Epoch: 3089, Batch Gradient Norm after: 22.36067993841776
Epoch 3090/10000, Prediction Accuracy = 55.75600000000001%, Loss = 1.0462055683135987
Epoch: 3090, Batch Gradient Norm: 37.08245774119492
Epoch: 3090, Batch Gradient Norm after: 22.36067572072076
Epoch 3091/10000, Prediction Accuracy = 55.760000000000005%, Loss = 1.0349110603332519
Epoch: 3091, Batch Gradient Norm: 41.223854897751
Epoch: 3091, Batch Gradient Norm after: 22.36067689287698
Epoch 3092/10000, Prediction Accuracy = 55.754%, Loss = 1.0459376335144044
Epoch: 3092, Batch Gradient Norm: 37.07428710911203
Epoch: 3092, Batch Gradient Norm after: 22.36067829077432
Epoch 3093/10000, Prediction Accuracy = 55.763999999999996%, Loss = 1.034633731842041
Epoch: 3093, Batch Gradient Norm: 41.22121175251654
Epoch: 3093, Batch Gradient Norm after: 22.36067690327803
Epoch 3094/10000, Prediction Accuracy = 55.754%, Loss = 1.0456707000732421
Epoch: 3094, Batch Gradient Norm: 37.06517038951397
Epoch: 3094, Batch Gradient Norm after: 22.360677074407423
Epoch 3095/10000, Prediction Accuracy = 55.762%, Loss = 1.0343562126159669
Epoch: 3095, Batch Gradient Norm: 41.219806720486254
Epoch: 3095, Batch Gradient Norm after: 22.360675383730296
Epoch 3096/10000, Prediction Accuracy = 55.77%, Loss = 1.045403742790222
Epoch: 3096, Batch Gradient Norm: 37.05751218696815
Epoch: 3096, Batch Gradient Norm after: 22.360679035350113
Epoch 3097/10000, Prediction Accuracy = 55.762%, Loss = 1.0340770721435546
Epoch: 3097, Batch Gradient Norm: 41.215809788781094
Epoch: 3097, Batch Gradient Norm after: 22.36067900804932
Epoch 3098/10000, Prediction Accuracy = 55.779999999999994%, Loss = 1.0451318264007567
Epoch: 3098, Batch Gradient Norm: 37.05097950796076
Epoch: 3098, Batch Gradient Norm after: 22.360678687767518
Epoch 3099/10000, Prediction Accuracy = 55.774%, Loss = 1.0337995290756226
Epoch: 3099, Batch Gradient Norm: 41.2153876799776
Epoch: 3099, Batch Gradient Norm after: 22.36067787909016
Epoch 3100/10000, Prediction Accuracy = 55.786%, Loss = 1.0448683023452758
Epoch: 3100, Batch Gradient Norm: 37.04386724578771
Epoch: 3100, Batch Gradient Norm after: 22.36067641936312
Epoch 3101/10000, Prediction Accuracy = 55.784000000000006%, Loss = 1.0335161685943604
Epoch: 3101, Batch Gradient Norm: 41.215815367736525
Epoch: 3101, Batch Gradient Norm after: 22.36067970811342
Epoch 3102/10000, Prediction Accuracy = 55.794%, Loss = 1.0446164846420287
Epoch: 3102, Batch Gradient Norm: 37.034154004604346
Epoch: 3102, Batch Gradient Norm after: 22.36067819948151
Epoch 3103/10000, Prediction Accuracy = 55.78399999999999%, Loss = 1.033237051963806
Epoch: 3103, Batch Gradient Norm: 41.21446890149391
Epoch: 3103, Batch Gradient Norm after: 22.360678221984355
Epoch 3104/10000, Prediction Accuracy = 55.791999999999994%, Loss = 1.0443570375442506
Epoch: 3104, Batch Gradient Norm: 37.02586064420411
Epoch: 3104, Batch Gradient Norm after: 22.360676309322304
Epoch 3105/10000, Prediction Accuracy = 55.78599999999999%, Loss = 1.0329619884490966
Epoch: 3105, Batch Gradient Norm: 41.212161272517505
Epoch: 3105, Batch Gradient Norm after: 22.360678121686174
Epoch 3106/10000, Prediction Accuracy = 55.791999999999994%, Loss = 1.0440963983535767
Epoch: 3106, Batch Gradient Norm: 37.01749906738902
Epoch: 3106, Batch Gradient Norm after: 22.36067851638751
Epoch 3107/10000, Prediction Accuracy = 55.786%, Loss = 1.03268461227417
Epoch: 3107, Batch Gradient Norm: 41.21065182455194
Epoch: 3107, Batch Gradient Norm after: 22.360677423964813
Epoch 3108/10000, Prediction Accuracy = 55.798%, Loss = 1.0438228845596313
Epoch: 3108, Batch Gradient Norm: 37.011155976375136
Epoch: 3108, Batch Gradient Norm after: 22.36067719783297
Epoch 3109/10000, Prediction Accuracy = 55.775999999999996%, Loss = 1.0324081182479858
Epoch: 3109, Batch Gradient Norm: 41.20445714152435
Epoch: 3109, Batch Gradient Norm after: 22.360677739676376
Epoch 3110/10000, Prediction Accuracy = 55.798%, Loss = 1.0435483694076537
Epoch: 3110, Batch Gradient Norm: 37.00509622263155
Epoch: 3110, Batch Gradient Norm after: 22.36067565881269
Epoch 3111/10000, Prediction Accuracy = 55.778%, Loss = 1.0321370601654052
Epoch: 3111, Batch Gradient Norm: 41.19368735600954
Epoch: 3111, Batch Gradient Norm after: 22.360679116377913
Epoch 3112/10000, Prediction Accuracy = 55.812%, Loss = 1.0432611465454102
Epoch: 3112, Batch Gradient Norm: 36.99896080188651
Epoch: 3112, Batch Gradient Norm after: 22.36067709606187
Epoch 3113/10000, Prediction Accuracy = 55.782000000000004%, Loss = 1.031865906715393
Epoch: 3113, Batch Gradient Norm: 41.18621234813707
Epoch: 3113, Batch Gradient Norm after: 22.360675669415333
Epoch 3114/10000, Prediction Accuracy = 55.818000000000005%, Loss = 1.0429773807525635
Epoch: 3114, Batch Gradient Norm: 36.99406746528296
Epoch: 3114, Batch Gradient Norm after: 22.360677679876282
Epoch 3115/10000, Prediction Accuracy = 55.798%, Loss = 1.0315908432006835
Epoch: 3115, Batch Gradient Norm: 41.17754705983674
Epoch: 3115, Batch Gradient Norm after: 22.360679202302453
Epoch 3116/10000, Prediction Accuracy = 55.818%, Loss = 1.0426923751831054
Epoch: 3116, Batch Gradient Norm: 36.988149244932416
Epoch: 3116, Batch Gradient Norm after: 22.360679641569604
Epoch 3117/10000, Prediction Accuracy = 55.79600000000001%, Loss = 1.031319761276245
Epoch: 3117, Batch Gradient Norm: 41.16653973700305
Epoch: 3117, Batch Gradient Norm after: 22.360680078188466
Epoch 3118/10000, Prediction Accuracy = 55.818%, Loss = 1.04240562915802
Epoch: 3118, Batch Gradient Norm: 36.982734238791735
Epoch: 3118, Batch Gradient Norm after: 22.360678769034365
Epoch 3119/10000, Prediction Accuracy = 55.802%, Loss = 1.0310462951660155
Epoch: 3119, Batch Gradient Norm: 41.149244388604394
Epoch: 3119, Batch Gradient Norm after: 22.360678971530735
Epoch 3120/10000, Prediction Accuracy = 55.818%, Loss = 1.042096161842346
Epoch: 3120, Batch Gradient Norm: 36.97881743127629
Epoch: 3120, Batch Gradient Norm after: 22.360676350772103
Epoch 3121/10000, Prediction Accuracy = 55.81%, Loss = 1.0307774305343629
Epoch: 3121, Batch Gradient Norm: 41.13002132308022
Epoch: 3121, Batch Gradient Norm after: 22.36067664925691
Epoch 3122/10000, Prediction Accuracy = 55.827999999999996%, Loss = 1.041782832145691
Epoch: 3122, Batch Gradient Norm: 36.97525220913287
Epoch: 3122, Batch Gradient Norm after: 22.36067970366418
Epoch 3123/10000, Prediction Accuracy = 55.822%, Loss = 1.0305082321166992
Epoch: 3123, Batch Gradient Norm: 41.1099490391349
Epoch: 3123, Batch Gradient Norm after: 22.360676538762917
Epoch 3124/10000, Prediction Accuracy = 55.824%, Loss = 1.0414721488952636
Epoch: 3124, Batch Gradient Norm: 36.97190120505012
Epoch: 3124, Batch Gradient Norm after: 22.36067739573606
Epoch 3125/10000, Prediction Accuracy = 55.824%, Loss = 1.0302408933639526
Epoch: 3125, Batch Gradient Norm: 41.090419050482126
Epoch: 3125, Batch Gradient Norm after: 22.360678338055873
Epoch 3126/10000, Prediction Accuracy = 55.834%, Loss = 1.0411564350128173
Epoch: 3126, Batch Gradient Norm: 36.96845139581278
Epoch: 3126, Batch Gradient Norm after: 22.360678228721223
Epoch 3127/10000, Prediction Accuracy = 55.831999999999994%, Loss = 1.029971146583557
Epoch: 3127, Batch Gradient Norm: 41.06893497268109
Epoch: 3127, Batch Gradient Norm after: 22.360678731699668
Epoch 3128/10000, Prediction Accuracy = 55.848%, Loss = 1.040837574005127
Epoch: 3128, Batch Gradient Norm: 36.965787033042744
Epoch: 3128, Batch Gradient Norm after: 22.36067927085296
Epoch 3129/10000, Prediction Accuracy = 55.85%, Loss = 1.0297019958496094
Epoch: 3129, Batch Gradient Norm: 41.04759008417001
Epoch: 3129, Batch Gradient Norm after: 22.360676082582604
Epoch 3130/10000, Prediction Accuracy = 55.85%, Loss = 1.0405253171920776
Epoch: 3130, Batch Gradient Norm: 36.96122046658776
Epoch: 3130, Batch Gradient Norm after: 22.36067798142308
Epoch 3131/10000, Prediction Accuracy = 55.87199999999999%, Loss = 1.0294349670410157
Epoch: 3131, Batch Gradient Norm: 41.02489061805864
Epoch: 3131, Batch Gradient Norm after: 22.360679717908777
Epoch 3132/10000, Prediction Accuracy = 55.852%, Loss = 1.0402108192443849
Epoch: 3132, Batch Gradient Norm: 36.95837410533175
Epoch: 3132, Batch Gradient Norm after: 22.36067825585581
Epoch 3133/10000, Prediction Accuracy = 55.888%, Loss = 1.0291675567626952
Epoch: 3133, Batch Gradient Norm: 41.00191573337003
Epoch: 3133, Batch Gradient Norm after: 22.360677092621717
Epoch 3134/10000, Prediction Accuracy = 55.86400000000001%, Loss = 1.03988618850708
Epoch: 3134, Batch Gradient Norm: 36.95695407407948
Epoch: 3134, Batch Gradient Norm after: 22.36067750214483
Epoch 3135/10000, Prediction Accuracy = 55.884%, Loss = 1.0289044618606566
Epoch: 3135, Batch Gradient Norm: 40.9783566776219
Epoch: 3135, Batch Gradient Norm after: 22.360679309589838
Epoch 3136/10000, Prediction Accuracy = 55.866%, Loss = 1.0395599842071532
Epoch: 3136, Batch Gradient Norm: 36.95398698580955
Epoch: 3136, Batch Gradient Norm after: 22.360677986961985
Epoch 3137/10000, Prediction Accuracy = 55.886%, Loss = 1.0286428928375244
Epoch: 3137, Batch Gradient Norm: 40.94837110316081
Epoch: 3137, Batch Gradient Norm after: 22.360680798297263
Epoch 3138/10000, Prediction Accuracy = 55.876%, Loss = 1.0392338752746582
Epoch: 3138, Batch Gradient Norm: 36.952562434857974
Epoch: 3138, Batch Gradient Norm after: 22.360680102178847
Epoch 3139/10000, Prediction Accuracy = 55.896%, Loss = 1.0283827304840087
Epoch: 3139, Batch Gradient Norm: 40.92056591813088
Epoch: 3139, Batch Gradient Norm after: 22.36067651279465
Epoch 3140/10000, Prediction Accuracy = 55.876%, Loss = 1.038903284072876
Epoch: 3140, Batch Gradient Norm: 36.95078071162094
Epoch: 3140, Batch Gradient Norm after: 22.360678353517077
Epoch 3141/10000, Prediction Accuracy = 55.9%, Loss = 1.0281192302703857
Epoch: 3141, Batch Gradient Norm: 40.900740641405314
Epoch: 3141, Batch Gradient Norm after: 22.360679368844444
Epoch 3142/10000, Prediction Accuracy = 55.896%, Loss = 1.0385875225067138
Epoch: 3142, Batch Gradient Norm: 36.945949985417855
Epoch: 3142, Batch Gradient Norm after: 22.360677753920605
Epoch 3143/10000, Prediction Accuracy = 55.894000000000005%, Loss = 1.0278562545776366
Epoch: 3143, Batch Gradient Norm: 40.880195580633966
Epoch: 3143, Batch Gradient Norm after: 22.36067659248008
Epoch 3144/10000, Prediction Accuracy = 55.9%, Loss = 1.038280725479126
Epoch: 3144, Batch Gradient Norm: 36.942392942192555
Epoch: 3144, Batch Gradient Norm after: 22.360679046434196
Epoch 3145/10000, Prediction Accuracy = 55.894000000000005%, Loss = 1.0275924682617188
Epoch: 3145, Batch Gradient Norm: 40.86752457565393
Epoch: 3145, Batch Gradient Norm after: 22.360677842418294
Epoch 3146/10000, Prediction Accuracy = 55.903999999999996%, Loss = 1.0379874467849732
Epoch: 3146, Batch Gradient Norm: 36.93812027283309
Epoch: 3146, Batch Gradient Norm after: 22.360677070535022
Epoch 3147/10000, Prediction Accuracy = 55.9%, Loss = 1.0273269891738892
Epoch: 3147, Batch Gradient Norm: 40.852955809487355
Epoch: 3147, Batch Gradient Norm after: 22.36067857640477
Epoch 3148/10000, Prediction Accuracy = 55.912%, Loss = 1.037696933746338
Epoch: 3148, Batch Gradient Norm: 36.93138626707886
Epoch: 3148, Batch Gradient Norm after: 22.360679842171226
Epoch 3149/10000, Prediction Accuracy = 55.90599999999999%, Loss = 1.0270634889602661
Epoch: 3149, Batch Gradient Norm: 40.84215253016838
Epoch: 3149, Batch Gradient Norm after: 22.360676380464014
Epoch 3150/10000, Prediction Accuracy = 55.914%, Loss = 1.0374163866043091
Epoch: 3150, Batch Gradient Norm: 36.92593100691124
Epoch: 3150, Batch Gradient Norm after: 22.360680158438207
Epoch 3151/10000, Prediction Accuracy = 55.912%, Loss = 1.026796078681946
Epoch: 3151, Batch Gradient Norm: 40.83505469417862
Epoch: 3151, Batch Gradient Norm after: 22.360679256071478
Epoch 3152/10000, Prediction Accuracy = 55.924%, Loss = 1.03714120388031
Epoch: 3152, Batch Gradient Norm: 36.91876535176015
Epoch: 3152, Batch Gradient Norm after: 22.360679452311658
Epoch 3153/10000, Prediction Accuracy = 55.924%, Loss = 1.0265297412872314
Epoch: 3153, Batch Gradient Norm: 40.827422912918905
Epoch: 3153, Batch Gradient Norm after: 22.3606757832584
Epoch 3154/10000, Prediction Accuracy = 55.92%, Loss = 1.0368698120117188
Epoch: 3154, Batch Gradient Norm: 36.910863839925895
Epoch: 3154, Batch Gradient Norm after: 22.36067741958757
Epoch 3155/10000, Prediction Accuracy = 55.92199999999999%, Loss = 1.0262627840042113
Epoch: 3155, Batch Gradient Norm: 40.81597193776282
Epoch: 3155, Batch Gradient Norm after: 22.360676943490965
Epoch 3156/10000, Prediction Accuracy = 55.916%, Loss = 1.0365959882736206
Epoch: 3156, Batch Gradient Norm: 36.90670726759271
Epoch: 3156, Batch Gradient Norm after: 22.360679101589355
Epoch 3157/10000, Prediction Accuracy = 55.928%, Loss = 1.0259948253631592
Epoch: 3157, Batch Gradient Norm: 40.80973866217996
Epoch: 3157, Batch Gradient Norm after: 22.360678490595205
Epoch 3158/10000, Prediction Accuracy = 55.92%, Loss = 1.0363215923309326
Epoch: 3158, Batch Gradient Norm: 36.90116321779413
Epoch: 3158, Batch Gradient Norm after: 22.360678406252656
Epoch 3159/10000, Prediction Accuracy = 55.934000000000005%, Loss = 1.0257290601730347
Epoch: 3159, Batch Gradient Norm: 40.79674577019177
Epoch: 3159, Batch Gradient Norm after: 22.360677245053008
Epoch 3160/10000, Prediction Accuracy = 55.926%, Loss = 1.0360411405563354
Epoch: 3160, Batch Gradient Norm: 36.89543640338923
Epoch: 3160, Batch Gradient Norm after: 22.36067826941593
Epoch 3161/10000, Prediction Accuracy = 55.936%, Loss = 1.0254661560058593
Epoch: 3161, Batch Gradient Norm: 40.78604981647239
Epoch: 3161, Batch Gradient Norm after: 22.360676540236874
Epoch 3162/10000, Prediction Accuracy = 55.938%, Loss = 1.0357526779174804
Epoch: 3162, Batch Gradient Norm: 36.88985687360102
Epoch: 3162, Batch Gradient Norm after: 22.360676250047906
Epoch 3163/10000, Prediction Accuracy = 55.92999999999999%, Loss = 1.0252000570297242
Epoch: 3163, Batch Gradient Norm: 40.77338140026352
Epoch: 3163, Batch Gradient Norm after: 22.360678839125093
Epoch 3164/10000, Prediction Accuracy = 55.938%, Loss = 1.0354679584503175
Epoch: 3164, Batch Gradient Norm: 36.88621569299067
Epoch: 3164, Batch Gradient Norm after: 22.36067925591617
Epoch 3165/10000, Prediction Accuracy = 55.936%, Loss = 1.0249349117279052
Epoch: 3165, Batch Gradient Norm: 40.75223838821735
Epoch: 3165, Batch Gradient Norm after: 22.360678589833892
Epoch 3166/10000, Prediction Accuracy = 55.938%, Loss = 1.035161542892456
Epoch: 3166, Batch Gradient Norm: 36.88483244033347
Epoch: 3166, Batch Gradient Norm after: 22.360677787264787
Epoch 3167/10000, Prediction Accuracy = 55.938%, Loss = 1.0246721029281616
Epoch: 3167, Batch Gradient Norm: 40.730973965793886
Epoch: 3167, Batch Gradient Norm after: 22.36067650223906
Epoch 3168/10000, Prediction Accuracy = 55.95%, Loss = 1.0348382711410522
Epoch: 3168, Batch Gradient Norm: 36.88386450925691
Epoch: 3168, Batch Gradient Norm after: 22.360678104222583
Epoch 3169/10000, Prediction Accuracy = 55.94200000000001%, Loss = 1.0244131565093995
Epoch: 3169, Batch Gradient Norm: 40.71019419406344
Epoch: 3169, Batch Gradient Norm after: 22.3606773228489
Epoch 3170/10000, Prediction Accuracy = 55.94199999999999%, Loss = 1.034528946876526
Epoch: 3170, Batch Gradient Norm: 36.88070331724591
Epoch: 3170, Batch Gradient Norm after: 22.36067770242196
Epoch 3171/10000, Prediction Accuracy = 55.94199999999999%, Loss = 1.024156093597412
Epoch: 3171, Batch Gradient Norm: 40.691995300366884
Epoch: 3171, Batch Gradient Norm after: 22.36067852560821
Epoch 3172/10000, Prediction Accuracy = 55.95%, Loss = 1.0342308759689331
Epoch: 3172, Batch Gradient Norm: 36.879459641447355
Epoch: 3172, Batch Gradient Norm after: 22.360677956229637
Epoch 3173/10000, Prediction Accuracy = 55.943999999999996%, Loss = 1.0238943815231323
Epoch: 3173, Batch Gradient Norm: 40.67610610927471
Epoch: 3173, Batch Gradient Norm after: 22.360677743188866
Epoch 3174/10000, Prediction Accuracy = 55.95799999999999%, Loss = 1.0339409112930298
Epoch: 3174, Batch Gradient Norm: 36.875407082309806
Epoch: 3174, Batch Gradient Norm after: 22.360677273794515
Epoch 3175/10000, Prediction Accuracy = 55.946000000000005%, Loss = 1.0236345529556274
Epoch: 3175, Batch Gradient Norm: 40.65786627671387
Epoch: 3175, Batch Gradient Norm after: 22.360677500373455
Epoch 3176/10000, Prediction Accuracy = 55.95799999999999%, Loss = 1.0336374521255494
Epoch: 3176, Batch Gradient Norm: 36.8733556770351
Epoch: 3176, Batch Gradient Norm after: 22.36067762459684
Epoch 3177/10000, Prediction Accuracy = 55.958000000000006%, Loss = 1.0233755826950073
Epoch: 3177, Batch Gradient Norm: 40.63893996191485
Epoch: 3177, Batch Gradient Norm after: 22.360676288074004
Epoch 3178/10000, Prediction Accuracy = 55.971999999999994%, Loss = 1.0333331823349
Epoch: 3178, Batch Gradient Norm: 36.870726356933254
Epoch: 3178, Batch Gradient Norm after: 22.360678804427668
Epoch 3179/10000, Prediction Accuracy = 55.964%, Loss = 1.0231214046478272
Epoch: 3179, Batch Gradient Norm: 40.61494769948495
Epoch: 3179, Batch Gradient Norm after: 22.360678315803657
Epoch 3180/10000, Prediction Accuracy = 55.976%, Loss = 1.0330219984054565
Epoch: 3180, Batch Gradient Norm: 36.86880082175987
Epoch: 3180, Batch Gradient Norm after: 22.36067588631954
Epoch 3181/10000, Prediction Accuracy = 55.976%, Loss = 1.0228644609451294
Epoch: 3181, Batch Gradient Norm: 40.594733445570924
Epoch: 3181, Batch Gradient Norm after: 22.360679474914956
Epoch 3182/10000, Prediction Accuracy = 55.977999999999994%, Loss = 1.0327195167541503
Epoch: 3182, Batch Gradient Norm: 36.866070602758995
Epoch: 3182, Batch Gradient Norm after: 22.360677361045905
Epoch 3183/10000, Prediction Accuracy = 55.977999999999994%, Loss = 1.022607398033142
Epoch: 3183, Batch Gradient Norm: 40.57356798877668
Epoch: 3183, Batch Gradient Norm after: 22.36067979290299
Epoch 3184/10000, Prediction Accuracy = 55.983999999999995%, Loss = 1.0324121475219727
Epoch: 3184, Batch Gradient Norm: 36.862870932871516
Epoch: 3184, Batch Gradient Norm after: 22.360677341481576
Epoch 3185/10000, Prediction Accuracy = 55.989999999999995%, Loss = 1.0223544597625733
Epoch: 3185, Batch Gradient Norm: 40.55918648734041
Epoch: 3185, Batch Gradient Norm after: 22.360676850073084
Epoch 3186/10000, Prediction Accuracy = 55.99400000000001%, Loss = 1.0321190118789674
Epoch: 3186, Batch Gradient Norm: 36.85997510790599
Epoch: 3186, Batch Gradient Norm after: 22.360678230421165
Epoch 3187/10000, Prediction Accuracy = 55.983999999999995%, Loss = 1.022092080116272
Epoch: 3187, Batch Gradient Norm: 40.543135654457366
Epoch: 3187, Batch Gradient Norm after: 22.36067833219776
Epoch 3188/10000, Prediction Accuracy = 56.00599999999999%, Loss = 1.0318263530731202
Epoch: 3188, Batch Gradient Norm: 36.855637185685545
Epoch: 3188, Batch Gradient Norm after: 22.360679097933662
Epoch 3189/10000, Prediction Accuracy = 55.99400000000001%, Loss = 1.0218356609344483
Epoch: 3189, Batch Gradient Norm: 40.52662067910862
Epoch: 3189, Batch Gradient Norm after: 22.360679375048452
Epoch 3190/10000, Prediction Accuracy = 55.99400000000001%, Loss = 1.0315335273742676
Epoch: 3190, Batch Gradient Norm: 36.85144253452347
Epoch: 3190, Batch Gradient Norm after: 22.360678580963807
Epoch 3191/10000, Prediction Accuracy = 56.004000000000005%, Loss = 1.0215839385986327
Epoch: 3191, Batch Gradient Norm: 40.51315723135098
Epoch: 3191, Batch Gradient Norm after: 22.360678700497033
Epoch 3192/10000, Prediction Accuracy = 56.0%, Loss = 1.031252098083496
Epoch: 3192, Batch Gradient Norm: 36.84474129061241
Epoch: 3192, Batch Gradient Norm after: 22.36067558116547
Epoch 3193/10000, Prediction Accuracy = 56.017999999999994%, Loss = 1.021323561668396
Epoch: 3193, Batch Gradient Norm: 40.49797012653394
Epoch: 3193, Batch Gradient Norm after: 22.360679251929145
Epoch 3194/10000, Prediction Accuracy = 56.010000000000005%, Loss = 1.030963397026062
Epoch: 3194, Batch Gradient Norm: 36.83741676293473
Epoch: 3194, Batch Gradient Norm after: 22.360675391610012
Epoch 3195/10000, Prediction Accuracy = 56.028%, Loss = 1.021068811416626
Epoch: 3195, Batch Gradient Norm: 40.48805641208561
Epoch: 3195, Batch Gradient Norm after: 22.360680919536506
Epoch 3196/10000, Prediction Accuracy = 56.028%, Loss = 1.0306888341903686
Epoch: 3196, Batch Gradient Norm: 36.830700895405734
Epoch: 3196, Batch Gradient Norm after: 22.36067333748001
Epoch 3197/10000, Prediction Accuracy = 56.036%, Loss = 1.0208065509796143
Epoch: 3197, Batch Gradient Norm: 40.48386856875766
Epoch: 3197, Batch Gradient Norm after: 22.36067917233499
Epoch 3198/10000, Prediction Accuracy = 56.034000000000006%, Loss = 1.030425000190735
Epoch: 3198, Batch Gradient Norm: 36.82329984082115
Epoch: 3198, Batch Gradient Norm after: 22.360676731515326
Epoch 3199/10000, Prediction Accuracy = 56.03799999999999%, Loss = 1.020538830757141
Epoch: 3199, Batch Gradient Norm: 40.478895735706736
Epoch: 3199, Batch Gradient Norm after: 22.360678441452457
Epoch 3200/10000, Prediction Accuracy = 56.04%, Loss = 1.0301708221435546
Epoch: 3200, Batch Gradient Norm: 36.812782144890676
Epoch: 3200, Batch Gradient Norm after: 22.360676986950992
Epoch 3201/10000, Prediction Accuracy = 56.056%, Loss = 1.0202769041061401
Epoch: 3201, Batch Gradient Norm: 40.4776592028486
Epoch: 3201, Batch Gradient Norm after: 22.36067760271646
Epoch 3202/10000, Prediction Accuracy = 56.05%, Loss = 1.0299286365509033
Epoch: 3202, Batch Gradient Norm: 36.80489511068138
Epoch: 3202, Batch Gradient Norm after: 22.360675554606154
Epoch 3203/10000, Prediction Accuracy = 56.06%, Loss = 1.0200100660324096
Epoch: 3203, Batch Gradient Norm: 40.47187936471574
Epoch: 3203, Batch Gradient Norm after: 22.3606763047598
Epoch 3204/10000, Prediction Accuracy = 56.06%, Loss = 1.0296703815460204
Epoch: 3204, Batch Gradient Norm: 36.795088823908515
Epoch: 3204, Batch Gradient Norm after: 22.360677612530157
Epoch 3205/10000, Prediction Accuracy = 56.06400000000001%, Loss = 1.0197548627853394
Epoch: 3205, Batch Gradient Norm: 40.46854448566274
Epoch: 3205, Batch Gradient Norm after: 22.3606756337218
Epoch 3206/10000, Prediction Accuracy = 56.065999999999995%, Loss = 1.029421591758728
Epoch: 3206, Batch Gradient Norm: 36.78513060813964
Epoch: 3206, Batch Gradient Norm after: 22.360677271192905
Epoch 3207/10000, Prediction Accuracy = 56.065999999999995%, Loss = 1.0194856166839599
Epoch: 3207, Batch Gradient Norm: 40.46789045498116
Epoch: 3207, Batch Gradient Norm after: 22.360678532163774
Epoch 3208/10000, Prediction Accuracy = 56.062%, Loss = 1.0291842937469482
Epoch: 3208, Batch Gradient Norm: 36.77392389390969
Epoch: 3208, Batch Gradient Norm after: 22.36067629552012
Epoch 3209/10000, Prediction Accuracy = 56.074%, Loss = 1.0192226409912108
Epoch: 3209, Batch Gradient Norm: 40.4761588243264
Epoch: 3209, Batch Gradient Norm after: 22.360678936516567
Epoch 3210/10000, Prediction Accuracy = 56.05800000000001%, Loss = 1.028956985473633
Epoch: 3210, Batch Gradient Norm: 36.76272700710174
Epoch: 3210, Batch Gradient Norm after: 22.360677640041384
Epoch 3211/10000, Prediction Accuracy = 56.084%, Loss = 1.0189502000808717
Epoch: 3211, Batch Gradient Norm: 40.48054924042464
Epoch: 3211, Batch Gradient Norm after: 22.36068035058873
Epoch 3212/10000, Prediction Accuracy = 56.053999999999995%, Loss = 1.0287284135818482
Epoch: 3212, Batch Gradient Norm: 36.752691743921545
Epoch: 3212, Batch Gradient Norm after: 22.360678776014762
Epoch 3213/10000, Prediction Accuracy = 56.081999999999994%, Loss = 1.018682050704956
Epoch: 3213, Batch Gradient Norm: 40.48053462904854
Epoch: 3213, Batch Gradient Norm after: 22.360678189058618
Epoch 3214/10000, Prediction Accuracy = 56.06600000000001%, Loss = 1.0284878492355347
Epoch: 3214, Batch Gradient Norm: 36.74266588412975
Epoch: 3214, Batch Gradient Norm after: 22.36067578031115
Epoch 3215/10000, Prediction Accuracy = 56.088%, Loss = 1.0184114217758178
Epoch: 3215, Batch Gradient Norm: 40.478136117175815
Epoch: 3215, Batch Gradient Norm after: 22.36067832238372
Epoch 3216/10000, Prediction Accuracy = 56.074%, Loss = 1.0282466173171998
Epoch: 3216, Batch Gradient Norm: 36.73125568011426
Epoch: 3216, Batch Gradient Norm after: 22.36067880023458
Epoch 3217/10000, Prediction Accuracy = 56.096000000000004%, Loss = 1.0181445240974427
Epoch: 3217, Batch Gradient Norm: 40.48605330023177
Epoch: 3217, Batch Gradient Norm after: 22.360679988277607
Epoch 3218/10000, Prediction Accuracy = 56.09400000000001%, Loss = 1.0280244112014771
Epoch: 3218, Batch Gradient Norm: 36.719945945620644
Epoch: 3218, Batch Gradient Norm after: 22.36067702112083
Epoch 3219/10000, Prediction Accuracy = 56.104%, Loss = 1.0178726315498352
Epoch: 3219, Batch Gradient Norm: 40.48816364378816
Epoch: 3219, Batch Gradient Norm after: 22.36067748644099
Epoch 3220/10000, Prediction Accuracy = 56.092000000000006%, Loss = 1.0277907609939576
Epoch: 3220, Batch Gradient Norm: 36.710612054376085
Epoch: 3220, Batch Gradient Norm after: 22.36067764515054
Epoch 3221/10000, Prediction Accuracy = 56.116%, Loss = 1.017598307132721
Epoch: 3221, Batch Gradient Norm: 40.48818221151029
Epoch: 3221, Batch Gradient Norm after: 22.360678069272154
Epoch 3222/10000, Prediction Accuracy = 56.098%, Loss = 1.0275452375411986
Epoch: 3222, Batch Gradient Norm: 36.70195517766112
Epoch: 3222, Batch Gradient Norm after: 22.360678115425003
Epoch 3223/10000, Prediction Accuracy = 56.120000000000005%, Loss = 1.0173325538635254
Epoch: 3223, Batch Gradient Norm: 40.49067463655235
Epoch: 3223, Batch Gradient Norm after: 22.360677839405923
Epoch 3224/10000, Prediction Accuracy = 56.102%, Loss = 1.027313470840454
Epoch: 3224, Batch Gradient Norm: 36.69202173019817
Epoch: 3224, Batch Gradient Norm after: 22.36067526260467
Epoch 3225/10000, Prediction Accuracy = 56.129999999999995%, Loss = 1.0170655488967895
Epoch: 3225, Batch Gradient Norm: 40.49031060488851
Epoch: 3225, Batch Gradient Norm after: 22.360675737905723
Epoch 3226/10000, Prediction Accuracy = 56.092000000000006%, Loss = 1.0270771026611327
Epoch: 3226, Batch Gradient Norm: 36.682601122962815
Epoch: 3226, Batch Gradient Norm after: 22.360674932483526
Epoch 3227/10000, Prediction Accuracy = 56.12599999999999%, Loss = 1.0167969942092896
Epoch: 3227, Batch Gradient Norm: 40.48976192180393
Epoch: 3227, Batch Gradient Norm after: 22.360676961175297
Epoch 3228/10000, Prediction Accuracy = 56.098%, Loss = 1.0268264532089233
Epoch: 3228, Batch Gradient Norm: 36.67397062196533
Epoch: 3228, Batch Gradient Norm after: 22.36067609988086
Epoch 3229/10000, Prediction Accuracy = 56.14%, Loss = 1.0165308117866516
Epoch: 3229, Batch Gradient Norm: 40.48557848874014
Epoch: 3229, Batch Gradient Norm after: 22.360676535090985
Epoch 3230/10000, Prediction Accuracy = 56.108000000000004%, Loss = 1.02657310962677
Epoch: 3230, Batch Gradient Norm: 36.666081634913375
Epoch: 3230, Batch Gradient Norm after: 22.36067822219003
Epoch 3231/10000, Prediction Accuracy = 56.15%, Loss = 1.0162670254707336
Epoch: 3231, Batch Gradient Norm: 40.48498037986019
Epoch: 3231, Batch Gradient Norm after: 22.360677818156905
Epoch 3232/10000, Prediction Accuracy = 56.124%, Loss = 1.026326310634613
Epoch: 3232, Batch Gradient Norm: 36.65602363587954
Epoch: 3232, Batch Gradient Norm after: 22.360676138429653
Epoch 3233/10000, Prediction Accuracy = 56.164%, Loss = 1.0160025000572204
Epoch: 3233, Batch Gradient Norm: 40.4808488875785
Epoch: 3233, Batch Gradient Norm after: 22.360678033737145
Epoch 3234/10000, Prediction Accuracy = 56.114%, Loss = 1.0260766386985778
Epoch: 3234, Batch Gradient Norm: 36.648914144555945
Epoch: 3234, Batch Gradient Norm after: 22.360676662420676
Epoch 3235/10000, Prediction Accuracy = 56.166%, Loss = 1.0157394051551818
Epoch: 3235, Batch Gradient Norm: 40.47601788177799
Epoch: 3235, Batch Gradient Norm after: 22.360678454392833
Epoch 3236/10000, Prediction Accuracy = 56.116%, Loss = 1.0258277535438538
Epoch: 3236, Batch Gradient Norm: 36.64074663474998
Epoch: 3236, Batch Gradient Norm after: 22.360677269797087
Epoch 3237/10000, Prediction Accuracy = 56.188%, Loss = 1.0154729723930358
Epoch: 3237, Batch Gradient Norm: 40.47446460738114
Epoch: 3237, Batch Gradient Norm after: 22.3606760562016
Epoch 3238/10000, Prediction Accuracy = 56.122%, Loss = 1.0255838871002196
Epoch: 3238, Batch Gradient Norm: 36.63167388072345
Epoch: 3238, Batch Gradient Norm after: 22.36067569019074
Epoch 3239/10000, Prediction Accuracy = 56.19000000000001%, Loss = 1.0152091979980469
Epoch: 3239, Batch Gradient Norm: 40.475615302684
Epoch: 3239, Batch Gradient Norm after: 22.360676507031616
Epoch 3240/10000, Prediction Accuracy = 56.141999999999996%, Loss = 1.0253471970558166
Epoch: 3240, Batch Gradient Norm: 36.62216109638549
Epoch: 3240, Batch Gradient Norm after: 22.360677796313738
Epoch 3241/10000, Prediction Accuracy = 56.19%, Loss = 1.0149401426315308
Epoch: 3241, Batch Gradient Norm: 40.47343888081632
Epoch: 3241, Batch Gradient Norm after: 22.360679092630583
Epoch 3242/10000, Prediction Accuracy = 56.14999999999999%, Loss = 1.0251057982444762
Epoch: 3242, Batch Gradient Norm: 36.61541227043167
Epoch: 3242, Batch Gradient Norm after: 22.360676991783322
Epoch 3243/10000, Prediction Accuracy = 56.202%, Loss = 1.0146755814552306
Epoch: 3243, Batch Gradient Norm: 40.464803767174764
Epoch: 3243, Batch Gradient Norm after: 22.360678901476696
Epoch 3244/10000, Prediction Accuracy = 56.164%, Loss = 1.024847388267517
Epoch: 3244, Batch Gradient Norm: 36.61028727936239
Epoch: 3244, Batch Gradient Norm after: 22.360676094234577
Epoch 3245/10000, Prediction Accuracy = 56.208000000000006%, Loss = 1.0144155979156495
Epoch: 3245, Batch Gradient Norm: 40.45706835214691
Epoch: 3245, Batch Gradient Norm after: 22.360678946851852
Epoch 3246/10000, Prediction Accuracy = 56.162%, Loss = 1.0245806455612183
Epoch: 3246, Batch Gradient Norm: 36.60449329304543
Epoch: 3246, Batch Gradient Norm after: 22.360677830514554
Epoch 3247/10000, Prediction Accuracy = 56.21%, Loss = 1.0141551613807678
Epoch: 3247, Batch Gradient Norm: 40.4517106685195
Epoch: 3247, Batch Gradient Norm after: 22.36067865138634
Epoch 3248/10000, Prediction Accuracy = 56.160000000000004%, Loss = 1.0243220567703246
Epoch: 3248, Batch Gradient Norm: 36.59478713698042
Epoch: 3248, Batch Gradient Norm after: 22.360677077140416
Epoch 3249/10000, Prediction Accuracy = 56.212%, Loss = 1.0138977408409118
Epoch: 3249, Batch Gradient Norm: 40.44926080240909
Epoch: 3249, Batch Gradient Norm after: 22.36067931897945
Epoch 3250/10000, Prediction Accuracy = 56.174%, Loss = 1.0240763187408448
Epoch: 3250, Batch Gradient Norm: 36.58782972719709
Epoch: 3250, Batch Gradient Norm after: 22.36067524470821
Epoch 3251/10000, Prediction Accuracy = 56.222%, Loss = 1.0136322498321533
Epoch: 3251, Batch Gradient Norm: 40.43806152969338
Epoch: 3251, Batch Gradient Norm after: 22.360678610196672
Epoch 3252/10000, Prediction Accuracy = 56.193999999999996%, Loss = 1.0238180756568909
Epoch: 3252, Batch Gradient Norm: 36.58088765046671
Epoch: 3252, Batch Gradient Norm after: 22.36067642643243
Epoch 3253/10000, Prediction Accuracy = 56.23199999999999%, Loss = 1.013372528553009
Epoch: 3253, Batch Gradient Norm: 40.43429622577977
Epoch: 3253, Batch Gradient Norm after: 22.36067784913319
Epoch 3254/10000, Prediction Accuracy = 56.205999999999996%, Loss = 1.023565137386322
Epoch: 3254, Batch Gradient Norm: 36.57087824180787
Epoch: 3254, Batch Gradient Norm after: 22.360677347760927
Epoch 3255/10000, Prediction Accuracy = 56.248000000000005%, Loss = 1.0131115317344666
Epoch: 3255, Batch Gradient Norm: 40.432404824702914
Epoch: 3255, Batch Gradient Norm after: 22.36067824354465
Epoch 3256/10000, Prediction Accuracy = 56.21%, Loss = 1.0233150601387024
Epoch: 3256, Batch Gradient Norm: 36.56149762112851
Epoch: 3256, Batch Gradient Norm after: 22.360677987910982
Epoch 3257/10000, Prediction Accuracy = 56.260000000000005%, Loss = 1.0128467321395873
Epoch: 3257, Batch Gradient Norm: 40.432400267505194
Epoch: 3257, Batch Gradient Norm after: 22.36067735448547
Epoch 3258/10000, Prediction Accuracy = 56.222%, Loss = 1.0230668783187866
Epoch: 3258, Batch Gradient Norm: 36.553838427334874
Epoch: 3258, Batch Gradient Norm after: 22.360677574317137
Epoch 3259/10000, Prediction Accuracy = 56.275999999999996%, Loss = 1.0125796675682068
Epoch: 3259, Batch Gradient Norm: 40.43261986976528
Epoch: 3259, Batch Gradient Norm after: 22.360679753281687
Epoch 3260/10000, Prediction Accuracy = 56.226%, Loss = 1.0228269219398498
Epoch: 3260, Batch Gradient Norm: 36.545763842401286
Epoch: 3260, Batch Gradient Norm after: 22.360676817139765
Epoch 3261/10000, Prediction Accuracy = 56.28000000000001%, Loss = 1.0123156309127808
Epoch: 3261, Batch Gradient Norm: 40.431563490406674
Epoch: 3261, Batch Gradient Norm after: 22.36067664072943
Epoch 3262/10000, Prediction Accuracy = 56.224000000000004%, Loss = 1.0225887298583984
Epoch: 3262, Batch Gradient Norm: 36.538102355399445
Epoch: 3262, Batch Gradient Norm after: 22.36067662627603
Epoch 3263/10000, Prediction Accuracy = 56.294000000000004%, Loss = 1.0120555877685546
Epoch: 3263, Batch Gradient Norm: 40.43443417970734
Epoch: 3263, Batch Gradient Norm after: 22.360677131457678
Epoch 3264/10000, Prediction Accuracy = 56.234%, Loss = 1.022354507446289
Epoch: 3264, Batch Gradient Norm: 36.527123290592954
Epoch: 3264, Batch Gradient Norm after: 22.36067553066194
Epoch 3265/10000, Prediction Accuracy = 56.31%, Loss = 1.0117904186248778
Epoch: 3265, Batch Gradient Norm: 40.44025330429046
Epoch: 3265, Batch Gradient Norm after: 22.36067915666285
Epoch 3266/10000, Prediction Accuracy = 56.232000000000006%, Loss = 1.0221346020698547
Epoch: 3266, Batch Gradient Norm: 36.516667465600925
Epoch: 3266, Batch Gradient Norm after: 22.360676699877875
Epoch 3267/10000, Prediction Accuracy = 56.315999999999995%, Loss = 1.0115275382995605
Epoch: 3267, Batch Gradient Norm: 40.44432636483031
Epoch: 3267, Batch Gradient Norm after: 22.360677838563674
Epoch 3268/10000, Prediction Accuracy = 56.226%, Loss = 1.0219048261642456
Epoch: 3268, Batch Gradient Norm: 36.50750857514857
Epoch: 3268, Batch Gradient Norm after: 22.360675638595502
Epoch 3269/10000, Prediction Accuracy = 56.32000000000001%, Loss = 1.0112657427787781
Epoch: 3269, Batch Gradient Norm: 40.44404738026058
Epoch: 3269, Batch Gradient Norm after: 22.360676454528296
Epoch 3270/10000, Prediction Accuracy = 56.232000000000006%, Loss = 1.0216652393341064
Epoch: 3270, Batch Gradient Norm: 36.497989348204165
Epoch: 3270, Batch Gradient Norm after: 22.360677583506693
Epoch 3271/10000, Prediction Accuracy = 56.331999999999994%, Loss = 1.0110041975975037
Epoch: 3271, Batch Gradient Norm: 40.442469550727736
Epoch: 3271, Batch Gradient Norm after: 22.36067548944991
Epoch 3272/10000, Prediction Accuracy = 56.236000000000004%, Loss = 1.0214211225509644
Epoch: 3272, Batch Gradient Norm: 36.49065671037737
Epoch: 3272, Batch Gradient Norm after: 22.36067600806699
Epoch 3273/10000, Prediction Accuracy = 56.33399999999999%, Loss = 1.0107465982437134
Epoch: 3273, Batch Gradient Norm: 40.43857288455382
Epoch: 3273, Batch Gradient Norm after: 22.360678405334525
Epoch 3274/10000, Prediction Accuracy = 56.242%, Loss = 1.0211711525917053
Epoch: 3274, Batch Gradient Norm: 36.48159123666211
Epoch: 3274, Batch Gradient Norm after: 22.360679143983297
Epoch 3275/10000, Prediction Accuracy = 56.342%, Loss = 1.0104861855506897
Epoch: 3275, Batch Gradient Norm: 40.43724231362887
Epoch: 3275, Batch Gradient Norm after: 22.36067927058501
Epoch 3276/10000, Prediction Accuracy = 56.24400000000001%, Loss = 1.0209242343902587
Epoch: 3276, Batch Gradient Norm: 36.47369016829233
Epoch: 3276, Batch Gradient Norm after: 22.36067845496889
Epoch 3277/10000, Prediction Accuracy = 56.354%, Loss = 1.0102235913276671
Epoch: 3277, Batch Gradient Norm: 40.437752466937226
Epoch: 3277, Batch Gradient Norm after: 22.360678124434397
Epoch 3278/10000, Prediction Accuracy = 56.246%, Loss = 1.0206837058067322
Epoch: 3278, Batch Gradient Norm: 36.46548343654305
Epoch: 3278, Batch Gradient Norm after: 22.36067813045767
Epoch 3279/10000, Prediction Accuracy = 56.367999999999995%, Loss = 1.0099664807319642
Epoch: 3279, Batch Gradient Norm: 40.43407683431694
Epoch: 3279, Batch Gradient Norm after: 22.360676999358724
Epoch 3280/10000, Prediction Accuracy = 56.254000000000005%, Loss = 1.0204366087913512
Epoch: 3280, Batch Gradient Norm: 36.4557670288594
Epoch: 3280, Batch Gradient Norm after: 22.360677544946572
Epoch 3281/10000, Prediction Accuracy = 56.372%, Loss = 1.00970059633255
Epoch: 3281, Batch Gradient Norm: 40.43046078506288
Epoch: 3281, Batch Gradient Norm after: 22.360676772554505
Epoch 3282/10000, Prediction Accuracy = 56.25600000000001%, Loss = 1.0201866149902343
Epoch: 3282, Batch Gradient Norm: 36.446593490122815
Epoch: 3282, Batch Gradient Norm after: 22.3606774659235
Epoch 3283/10000, Prediction Accuracy = 56.376%, Loss = 1.0094396352767945
Epoch: 3283, Batch Gradient Norm: 40.425026787530136
Epoch: 3283, Batch Gradient Norm after: 22.360676951371268
Epoch 3284/10000, Prediction Accuracy = 56.27%, Loss = 1.019930398464203
Epoch: 3284, Batch Gradient Norm: 36.43958181959378
Epoch: 3284, Batch Gradient Norm after: 22.360676838243897
Epoch 3285/10000, Prediction Accuracy = 56.370000000000005%, Loss = 1.0091804623603822
Epoch: 3285, Batch Gradient Norm: 40.4203397014509
Epoch: 3285, Batch Gradient Norm after: 22.360678721277566
Epoch 3286/10000, Prediction Accuracy = 56.272000000000006%, Loss = 1.0196778535842896
Epoch: 3286, Batch Gradient Norm: 36.43325919064148
Epoch: 3286, Batch Gradient Norm after: 22.36067715097518
Epoch 3287/10000, Prediction Accuracy = 56.36800000000001%, Loss = 1.008922564983368
Epoch: 3287, Batch Gradient Norm: 40.413114764142186
Epoch: 3287, Batch Gradient Norm after: 22.36067749662274
Epoch 3288/10000, Prediction Accuracy = 56.282%, Loss = 1.0194243311882019
Epoch: 3288, Batch Gradient Norm: 36.42446621809347
Epoch: 3288, Batch Gradient Norm after: 22.360677704174474
Epoch 3289/10000, Prediction Accuracy = 56.366%, Loss = 1.0086608171463012
Epoch: 3289, Batch Gradient Norm: 40.4092826914311
Epoch: 3289, Batch Gradient Norm after: 22.360676703030713
Epoch 3290/10000, Prediction Accuracy = 56.29%, Loss = 1.0191691994667054
Epoch: 3290, Batch Gradient Norm: 36.419177245121354
Epoch: 3290, Batch Gradient Norm after: 22.36067789715459
Epoch 3291/10000, Prediction Accuracy = 56.386%, Loss = 1.0084000468254088
Epoch: 3291, Batch Gradient Norm: 40.40169037095343
Epoch: 3291, Batch Gradient Norm after: 22.36067678530172
Epoch 3292/10000, Prediction Accuracy = 56.3%, Loss = 1.0189152598381042
Epoch: 3292, Batch Gradient Norm: 36.41224071489185
Epoch: 3292, Batch Gradient Norm after: 22.360677289061496
Epoch 3293/10000, Prediction Accuracy = 56.394000000000005%, Loss = 1.0081451296806336
Epoch: 3293, Batch Gradient Norm: 40.39490785057078
Epoch: 3293, Batch Gradient Norm after: 22.360678248313828
Epoch 3294/10000, Prediction Accuracy = 56.30800000000001%, Loss = 1.018657398223877
Epoch: 3294, Batch Gradient Norm: 36.404255620862735
Epoch: 3294, Batch Gradient Norm after: 22.360675977543575
Epoch 3295/10000, Prediction Accuracy = 56.398%, Loss = 1.0078894376754761
Epoch: 3295, Batch Gradient Norm: 40.39359171198468
Epoch: 3295, Batch Gradient Norm after: 22.360678725798103
Epoch 3296/10000, Prediction Accuracy = 56.312%, Loss = 1.0184111714363098
Epoch: 3296, Batch Gradient Norm: 36.396924532289056
Epoch: 3296, Batch Gradient Norm after: 22.360676819821897
Epoch 3297/10000, Prediction Accuracy = 56.406000000000006%, Loss = 1.0076287150382996
Epoch: 3297, Batch Gradient Norm: 40.38989724920647
Epoch: 3297, Batch Gradient Norm after: 22.36067689041625
Epoch 3298/10000, Prediction Accuracy = 56.31600000000001%, Loss = 1.0181626200675964
Epoch: 3298, Batch Gradient Norm: 36.39036766233783
Epoch: 3298, Batch Gradient Norm after: 22.360675215180674
Epoch 3299/10000, Prediction Accuracy = 56.40599999999999%, Loss = 1.007371437549591
Epoch: 3299, Batch Gradient Norm: 40.38952834546722
Epoch: 3299, Batch Gradient Norm after: 22.360677077522922
Epoch 3300/10000, Prediction Accuracy = 56.330000000000005%, Loss = 1.0179163575172425
Epoch: 3300, Batch Gradient Norm: 36.382728794351564
Epoch: 3300, Batch Gradient Norm after: 22.36067785374143
Epoch 3301/10000, Prediction Accuracy = 56.40599999999999%, Loss = 1.0071141839027404
Epoch: 3301, Batch Gradient Norm: 40.391233555467295
Epoch: 3301, Batch Gradient Norm after: 22.360678246586716
Epoch 3302/10000, Prediction Accuracy = 56.33%, Loss = 1.0176823496818543
Epoch: 3302, Batch Gradient Norm: 36.37306258272226
Epoch: 3302, Batch Gradient Norm after: 22.360675506439193
Epoch 3303/10000, Prediction Accuracy = 56.403999999999996%, Loss = 1.0068506360054017
Epoch: 3303, Batch Gradient Norm: 40.39362680618897
Epoch: 3303, Batch Gradient Norm after: 22.36067849841174
Epoch 3304/10000, Prediction Accuracy = 56.334%, Loss = 1.0174528002738952
Epoch: 3304, Batch Gradient Norm: 36.36411751111137
Epoch: 3304, Batch Gradient Norm after: 22.360676894129256
Epoch 3305/10000, Prediction Accuracy = 56.40400000000001%, Loss = 1.0065884828567504
Epoch: 3305, Batch Gradient Norm: 40.39272060426154
Epoch: 3305, Batch Gradient Norm after: 22.360676574248146
Epoch 3306/10000, Prediction Accuracy = 56.354000000000006%, Loss = 1.0172147393226623
Epoch: 3306, Batch Gradient Norm: 36.356702671162786
Epoch: 3306, Batch Gradient Norm after: 22.360678451478414
Epoch 3307/10000, Prediction Accuracy = 56.402%, Loss = 1.0063278436660767
Epoch: 3307, Batch Gradient Norm: 40.392391321973136
Epoch: 3307, Batch Gradient Norm after: 22.360677694818097
Epoch 3308/10000, Prediction Accuracy = 56.364%, Loss = 1.0169695496559144
Epoch: 3308, Batch Gradient Norm: 36.34929888882506
Epoch: 3308, Batch Gradient Norm after: 22.36068026059436
Epoch 3309/10000, Prediction Accuracy = 56.42%, Loss = 1.006068503856659
Epoch: 3309, Batch Gradient Norm: 40.391251469576666
Epoch: 3309, Batch Gradient Norm after: 22.360676717486566
Epoch 3310/10000, Prediction Accuracy = 56.367999999999995%, Loss = 1.0167237758636474
Epoch: 3310, Batch Gradient Norm: 36.34016673258241
Epoch: 3310, Batch Gradient Norm after: 22.360677971770297
Epoch 3311/10000, Prediction Accuracy = 56.428%, Loss = 1.0058133006095886
Epoch: 3311, Batch Gradient Norm: 40.3928270654892
Epoch: 3311, Batch Gradient Norm after: 22.360677631192864
Epoch 3312/10000, Prediction Accuracy = 56.39%, Loss = 1.0164850354194641
Epoch: 3312, Batch Gradient Norm: 36.333471362106266
Epoch: 3312, Batch Gradient Norm after: 22.360678133029094
Epoch 3313/10000, Prediction Accuracy = 56.45%, Loss = 1.0055531978607177
Epoch: 3313, Batch Gradient Norm: 40.39391332861582
Epoch: 3313, Batch Gradient Norm after: 22.360676250633354
Epoch 3314/10000, Prediction Accuracy = 56.398%, Loss = 1.016241717338562
Epoch: 3314, Batch Gradient Norm: 36.32698886575746
Epoch: 3314, Batch Gradient Norm after: 22.36067906168224
Epoch 3315/10000, Prediction Accuracy = 56.464%, Loss = 1.005290961265564
Epoch: 3315, Batch Gradient Norm: 40.397875413009274
Epoch: 3315, Batch Gradient Norm after: 22.360677661036533
Epoch 3316/10000, Prediction Accuracy = 56.41600000000001%, Loss = 1.0160113453865052
Epoch: 3316, Batch Gradient Norm: 36.319336013206375
Epoch: 3316, Batch Gradient Norm after: 22.36067987736074
Epoch 3317/10000, Prediction Accuracy = 56.476%, Loss = 1.00503249168396
Epoch: 3317, Batch Gradient Norm: 40.40349848368022
Epoch: 3317, Batch Gradient Norm after: 22.360677354156515
Epoch 3318/10000, Prediction Accuracy = 56.412%, Loss = 1.0157890319824219
Epoch: 3318, Batch Gradient Norm: 36.31254279461571
Epoch: 3318, Batch Gradient Norm after: 22.36067808514976
Epoch 3319/10000, Prediction Accuracy = 56.48%, Loss = 1.0047736406326293
Epoch: 3319, Batch Gradient Norm: 40.40727770696365
Epoch: 3319, Batch Gradient Norm after: 22.360678060462146
Epoch 3320/10000, Prediction Accuracy = 56.407999999999994%, Loss = 1.0155663967132569
Epoch: 3320, Batch Gradient Norm: 36.30455452543881
Epoch: 3320, Batch Gradient Norm after: 22.36067532965709
Epoch 3321/10000, Prediction Accuracy = 56.492%, Loss = 1.0045170783996582
Epoch: 3321, Batch Gradient Norm: 40.411712943233795
Epoch: 3321, Batch Gradient Norm after: 22.36067873571061
Epoch 3322/10000, Prediction Accuracy = 56.403999999999996%, Loss = 1.0153395414352417
Epoch: 3322, Batch Gradient Norm: 36.296376534388216
Epoch: 3322, Batch Gradient Norm after: 22.36067628237371
Epoch 3323/10000, Prediction Accuracy = 56.484%, Loss = 1.0042578816413879
Epoch: 3323, Batch Gradient Norm: 40.415837476434454
Epoch: 3323, Batch Gradient Norm after: 22.360678929296245
Epoch 3324/10000, Prediction Accuracy = 56.416%, Loss = 1.015115213394165
Epoch: 3324, Batch Gradient Norm: 36.286170183113086
Epoch: 3324, Batch Gradient Norm after: 22.36067818503708
Epoch 3325/10000, Prediction Accuracy = 56.504%, Loss = 1.0040047407150268
Epoch: 3325, Batch Gradient Norm: 40.42763588960585
Epoch: 3325, Batch Gradient Norm after: 22.360677467533975
Epoch 3326/10000, Prediction Accuracy = 56.438%, Loss = 1.0149187803268434
Epoch: 3326, Batch Gradient Norm: 36.2758331103268
Epoch: 3326, Batch Gradient Norm after: 22.36067709392912
Epoch 3327/10000, Prediction Accuracy = 56.498000000000005%, Loss = 1.0037464380264283
Epoch: 3327, Batch Gradient Norm: 40.43877610198816
Epoch: 3327, Batch Gradient Norm after: 22.360678715755842
Epoch 3328/10000, Prediction Accuracy = 56.438%, Loss = 1.0147135138511658
Epoch: 3328, Batch Gradient Norm: 36.26559159309514
Epoch: 3328, Batch Gradient Norm after: 22.360675209718444
Epoch 3329/10000, Prediction Accuracy = 56.496%, Loss = 1.0034841299057007
Epoch: 3329, Batch Gradient Norm: 40.4492935691062
Epoch: 3329, Batch Gradient Norm after: 22.360679559691373
Epoch 3330/10000, Prediction Accuracy = 56.462%, Loss = 1.0145075559616088
Epoch: 3330, Batch Gradient Norm: 36.25513391771116
Epoch: 3330, Batch Gradient Norm after: 22.360674904788446
Epoch 3331/10000, Prediction Accuracy = 56.5%, Loss = 1.0032262086868287
Epoch: 3331, Batch Gradient Norm: 40.458395316618855
Epoch: 3331, Batch Gradient Norm after: 22.360677153435816
Epoch 3332/10000, Prediction Accuracy = 56.464%, Loss = 1.0142969846725465
Epoch: 3332, Batch Gradient Norm: 36.244658927434514
Epoch: 3332, Batch Gradient Norm after: 22.36067717685825
Epoch 3333/10000, Prediction Accuracy = 56.496%, Loss = 1.0029677510261537
Epoch: 3333, Batch Gradient Norm: 40.46650957239538
Epoch: 3333, Batch Gradient Norm after: 22.36067513895383
Epoch 3334/10000, Prediction Accuracy = 56.484%, Loss = 1.0140753269195557
Epoch: 3334, Batch Gradient Norm: 36.23627143721869
Epoch: 3334, Batch Gradient Norm after: 22.36067679898714
Epoch 3335/10000, Prediction Accuracy = 56.50999999999999%, Loss = 1.002713680267334
Epoch: 3335, Batch Gradient Norm: 40.470070910774275
Epoch: 3335, Batch Gradient Norm after: 22.360677541310725
Epoch 3336/10000, Prediction Accuracy = 56.496%, Loss = 1.013853168487549
Epoch: 3336, Batch Gradient Norm: 36.22613468667285
Epoch: 3336, Batch Gradient Norm after: 22.360677306620158
Epoch 3337/10000, Prediction Accuracy = 56.516%, Loss = 1.0024608969688416
Epoch: 3337, Batch Gradient Norm: 40.47465837770657
Epoch: 3337, Batch Gradient Norm after: 22.36067952729813
Epoch 3338/10000, Prediction Accuracy = 56.5%, Loss = 1.013633918762207
Epoch: 3338, Batch Gradient Norm: 36.21859734343437
Epoch: 3338, Batch Gradient Norm after: 22.360677665775192
Epoch 3339/10000, Prediction Accuracy = 56.529999999999994%, Loss = 1.002203631401062
Epoch: 3339, Batch Gradient Norm: 40.47950684702645
Epoch: 3339, Batch Gradient Norm after: 22.36067992838422
Epoch 3340/10000, Prediction Accuracy = 56.501999999999995%, Loss = 1.0134130477905274
Epoch: 3340, Batch Gradient Norm: 36.20979527970717
Epoch: 3340, Batch Gradient Norm after: 22.360676708437826
Epoch 3341/10000, Prediction Accuracy = 56.525999999999996%, Loss = 1.001948058605194
Epoch: 3341, Batch Gradient Norm: 40.48897393736721
Epoch: 3341, Batch Gradient Norm after: 22.360679668976683
Epoch 3342/10000, Prediction Accuracy = 56.513999999999996%, Loss = 1.013205063343048
Epoch: 3342, Batch Gradient Norm: 36.20236387839896
Epoch: 3342, Batch Gradient Norm after: 22.360676747198315
Epoch 3343/10000, Prediction Accuracy = 56.529999999999994%, Loss = 1.0016908407211305
Epoch: 3343, Batch Gradient Norm: 40.4941886662398
Epoch: 3343, Batch Gradient Norm after: 22.360678391443106
Epoch 3344/10000, Prediction Accuracy = 56.513999999999996%, Loss = 1.0129830479621886
Epoch: 3344, Batch Gradient Norm: 36.19359540697288
Epoch: 3344, Batch Gradient Norm after: 22.36067651954131
Epoch 3345/10000, Prediction Accuracy = 56.529999999999994%, Loss = 1.0014381408691406
Epoch: 3345, Batch Gradient Norm: 40.492071039490845
Epoch: 3345, Batch Gradient Norm after: 22.360678383192283
Epoch 3346/10000, Prediction Accuracy = 56.516%, Loss = 1.0127384662628174
Epoch: 3346, Batch Gradient Norm: 36.18429989179644
Epoch: 3346, Batch Gradient Norm after: 22.36067766234211
Epoch 3347/10000, Prediction Accuracy = 56.524%, Loss = 1.0011873364448547
Epoch: 3347, Batch Gradient Norm: 40.493842102202294
Epoch: 3347, Batch Gradient Norm after: 22.36067904172405
Epoch 3348/10000, Prediction Accuracy = 56.524%, Loss = 1.0125109553337097
Epoch: 3348, Batch Gradient Norm: 36.17534411352197
Epoch: 3348, Batch Gradient Norm after: 22.360677220364288
Epoch 3349/10000, Prediction Accuracy = 56.536%, Loss = 1.0009288787841797
Epoch: 3349, Batch Gradient Norm: 40.495148995041234
Epoch: 3349, Batch Gradient Norm after: 22.360679369067515
Epoch 3350/10000, Prediction Accuracy = 56.519999999999996%, Loss = 1.012286078929901
Epoch: 3350, Batch Gradient Norm: 36.167293954752616
Epoch: 3350, Batch Gradient Norm after: 22.360677413795205
Epoch 3351/10000, Prediction Accuracy = 56.536%, Loss = 1.0006726264953614
Epoch: 3351, Batch Gradient Norm: 40.49729950877212
Epoch: 3351, Batch Gradient Norm after: 22.360677518142065
Epoch 3352/10000, Prediction Accuracy = 56.528%, Loss = 1.0120544195175172
Epoch: 3352, Batch Gradient Norm: 36.16081197627136
Epoch: 3352, Batch Gradient Norm after: 22.360680033754857
Epoch 3353/10000, Prediction Accuracy = 56.548%, Loss = 1.0004161953926087
Epoch: 3353, Batch Gradient Norm: 40.49509551906412
Epoch: 3353, Batch Gradient Norm after: 22.360679877985458
Epoch 3354/10000, Prediction Accuracy = 56.519999999999996%, Loss = 1.0118146300315858
Epoch: 3354, Batch Gradient Norm: 36.154092482309586
Epoch: 3354, Batch Gradient Norm after: 22.36067780464933
Epoch 3355/10000, Prediction Accuracy = 56.55800000000001%, Loss = 1.0001629114151
Epoch: 3355, Batch Gradient Norm: 40.48839433287363
Epoch: 3355, Batch Gradient Norm after: 22.360676592117553
Epoch 3356/10000, Prediction Accuracy = 56.529999999999994%, Loss = 1.011566436290741
Epoch: 3356, Batch Gradient Norm: 36.147945885015716
Epoch: 3356, Batch Gradient Norm after: 22.360677467603793
Epoch 3357/10000, Prediction Accuracy = 56.552%, Loss = 0.9999118924140931
Epoch: 3357, Batch Gradient Norm: 40.48274515730586
Epoch: 3357, Batch Gradient Norm after: 22.360679786113543
Epoch 3358/10000, Prediction Accuracy = 56.55%, Loss = 1.0113234400749207
Epoch: 3358, Batch Gradient Norm: 36.14285008851877
Epoch: 3358, Batch Gradient Norm after: 22.360677583058756
Epoch 3359/10000, Prediction Accuracy = 56.552%, Loss = 0.9996594667434693
Epoch: 3359, Batch Gradient Norm: 40.47911534845496
Epoch: 3359, Batch Gradient Norm after: 22.360679532947678
Epoch 3360/10000, Prediction Accuracy = 56.556000000000004%, Loss = 1.011076533794403
Epoch: 3360, Batch Gradient Norm: 36.13423530472782
Epoch: 3360, Batch Gradient Norm after: 22.360679505101608
Epoch 3361/10000, Prediction Accuracy = 56.55799999999999%, Loss = 0.9994036436080933
Epoch: 3361, Batch Gradient Norm: 40.475358152739474
Epoch: 3361, Batch Gradient Norm after: 22.36068062196861
Epoch 3362/10000, Prediction Accuracy = 56.553999999999995%, Loss = 1.0108358502388
Epoch: 3362, Batch Gradient Norm: 36.127705915599904
Epoch: 3362, Batch Gradient Norm after: 22.36067863714747
Epoch 3363/10000, Prediction Accuracy = 56.55800000000001%, Loss = 0.9991495728492736
Epoch: 3363, Batch Gradient Norm: 40.47220479787222
Epoch: 3363, Batch Gradient Norm after: 22.36067959249064
Epoch 3364/10000, Prediction Accuracy = 56.57000000000001%, Loss = 1.0105967760086059
Epoch: 3364, Batch Gradient Norm: 36.12036598785916
Epoch: 3364, Batch Gradient Norm after: 22.36067923728979
Epoch 3365/10000, Prediction Accuracy = 56.564%, Loss = 0.9988963603973389
Epoch: 3365, Batch Gradient Norm: 40.47279846153188
Epoch: 3365, Batch Gradient Norm after: 22.360679120171937
Epoch 3366/10000, Prediction Accuracy = 56.57000000000001%, Loss = 1.010357415676117
Epoch: 3366, Batch Gradient Norm: 36.11206659723956
Epoch: 3366, Batch Gradient Norm after: 22.360678054997997
Epoch 3367/10000, Prediction Accuracy = 56.562%, Loss = 0.9986465573310852
Epoch: 3367, Batch Gradient Norm: 40.46796475256595
Epoch: 3367, Batch Gradient Norm after: 22.36067921852971
Epoch 3368/10000, Prediction Accuracy = 56.568%, Loss = 1.0101125597953797
Epoch: 3368, Batch Gradient Norm: 36.10830823865963
Epoch: 3368, Batch Gradient Norm after: 22.36067932098474
Epoch 3369/10000, Prediction Accuracy = 56.565999999999995%, Loss = 0.9983947277069092
Epoch: 3369, Batch Gradient Norm: 40.45746188255928
Epoch: 3369, Batch Gradient Norm after: 22.360678106947084
Epoch 3370/10000, Prediction Accuracy = 56.568000000000005%, Loss = 1.0098501324653626
Epoch: 3370, Batch Gradient Norm: 36.10057795801007
Epoch: 3370, Batch Gradient Norm after: 22.360678456428033
Epoch 3371/10000, Prediction Accuracy = 56.574%, Loss = 0.9981454372406006
Epoch: 3371, Batch Gradient Norm: 40.44769606507576
Epoch: 3371, Batch Gradient Norm after: 22.360678257879396
Epoch 3372/10000, Prediction Accuracy = 56.57000000000001%, Loss = 1.0095919251441956
Epoch: 3372, Batch Gradient Norm: 36.09534913818299
Epoch: 3372, Batch Gradient Norm after: 22.360676266643683
Epoch 3373/10000, Prediction Accuracy = 56.589999999999996%, Loss = 0.997894537448883
Epoch: 3373, Batch Gradient Norm: 40.43907891490018
Epoch: 3373, Batch Gradient Norm after: 22.360679057155735
Epoch 3374/10000, Prediction Accuracy = 56.576%, Loss = 1.0093357801437377
Epoch: 3374, Batch Gradient Norm: 36.08973914572794
Epoch: 3374, Batch Gradient Norm after: 22.36067772147252
Epoch 3375/10000, Prediction Accuracy = 56.589999999999996%, Loss = 0.9976448059082031
Epoch: 3375, Batch Gradient Norm: 40.42629890504355
Epoch: 3375, Batch Gradient Norm after: 22.360677751741807
Epoch 3376/10000, Prediction Accuracy = 56.57000000000001%, Loss = 1.0090709686279298
Epoch: 3376, Batch Gradient Norm: 36.083596907599386
Epoch: 3376, Batch Gradient Norm after: 22.360677790584766
Epoch 3377/10000, Prediction Accuracy = 56.59400000000001%, Loss = 0.9973979711532592
Epoch: 3377, Batch Gradient Norm: 40.41632285604005
Epoch: 3377, Batch Gradient Norm after: 22.360678948892733
Epoch 3378/10000, Prediction Accuracy = 56.577999999999996%, Loss = 1.0088129043579102
Epoch: 3378, Batch Gradient Norm: 36.07634710344877
Epoch: 3378, Batch Gradient Norm after: 22.360678291371478
Epoch 3379/10000, Prediction Accuracy = 56.602%, Loss = 0.997148311138153
Epoch: 3379, Batch Gradient Norm: 40.40422737777846
Epoch: 3379, Batch Gradient Norm after: 22.36067950505718
Epoch 3380/10000, Prediction Accuracy = 56.584%, Loss = 1.0085516214370727
Epoch: 3380, Batch Gradient Norm: 36.0693136470406
Epoch: 3380, Batch Gradient Norm after: 22.360679851814705
Epoch 3381/10000, Prediction Accuracy = 56.6%, Loss = 0.9969043493270874
Epoch: 3381, Batch Gradient Norm: 40.38941882098066
Epoch: 3381, Batch Gradient Norm after: 22.360676976648126
Epoch 3382/10000, Prediction Accuracy = 56.577999999999996%, Loss = 1.0082834362983704
Epoch: 3382, Batch Gradient Norm: 36.06248713108509
Epoch: 3382, Batch Gradient Norm after: 22.36067611966262
Epoch 3383/10000, Prediction Accuracy = 56.593999999999994%, Loss = 0.9966566443443299
Epoch: 3383, Batch Gradient Norm: 40.37103632198156
Epoch: 3383, Batch Gradient Norm after: 22.360675374003634
Epoch 3384/10000, Prediction Accuracy = 56.576%, Loss = 1.0080007076263429
Epoch: 3384, Batch Gradient Norm: 36.05872533111428
Epoch: 3384, Batch Gradient Norm after: 22.360677623956754
Epoch 3385/10000, Prediction Accuracy = 56.598%, Loss = 0.9964120507240295
Epoch: 3385, Batch Gradient Norm: 40.34991596861372
Epoch: 3385, Batch Gradient Norm after: 22.360676883949665
Epoch 3386/10000, Prediction Accuracy = 56.58%, Loss = 1.0077096939086914
Epoch: 3386, Batch Gradient Norm: 36.05347956205074
Epoch: 3386, Batch Gradient Norm after: 22.360675423706667
Epoch 3387/10000, Prediction Accuracy = 56.598%, Loss = 0.9961682677268981
Epoch: 3387, Batch Gradient Norm: 40.335493673176046
Epoch: 3387, Batch Gradient Norm after: 22.360676919129443
Epoch 3388/10000, Prediction Accuracy = 56.584%, Loss = 1.0074408292770385
Epoch: 3388, Batch Gradient Norm: 36.048099879457126
Epoch: 3388, Batch Gradient Norm after: 22.360678549243275
Epoch 3389/10000, Prediction Accuracy = 56.602%, Loss = 0.9959236979484558
Epoch: 3389, Batch Gradient Norm: 40.322906951012406
Epoch: 3389, Batch Gradient Norm after: 22.360675355067094
Epoch 3390/10000, Prediction Accuracy = 56.58%, Loss = 1.0071772456169128
Epoch: 3390, Batch Gradient Norm: 36.04374785316905
Epoch: 3390, Batch Gradient Norm after: 22.360676366216143
Epoch 3391/10000, Prediction Accuracy = 56.61200000000001%, Loss = 0.9956734776496887
Epoch: 3391, Batch Gradient Norm: 40.30840650415399
Epoch: 3391, Batch Gradient Norm after: 22.360678246979436
Epoch 3392/10000, Prediction Accuracy = 56.58%, Loss = 1.0069094896316528
Epoch: 3392, Batch Gradient Norm: 36.03834597429047
Epoch: 3392, Batch Gradient Norm after: 22.360676059544403
Epoch 3393/10000, Prediction Accuracy = 56.61%, Loss = 0.9954277992248535
Epoch: 3393, Batch Gradient Norm: 40.29115327369207
Epoch: 3393, Batch Gradient Norm after: 22.36067656020015
Epoch 3394/10000, Prediction Accuracy = 56.574%, Loss = 1.0066275358200074
Epoch: 3394, Batch Gradient Norm: 36.03318868809476
Epoch: 3394, Batch Gradient Norm after: 22.36067729366488
Epoch 3395/10000, Prediction Accuracy = 56.622%, Loss = 0.9951842427253723
Epoch: 3395, Batch Gradient Norm: 40.275645692785524
Epoch: 3395, Batch Gradient Norm after: 22.3606770856471
Epoch 3396/10000, Prediction Accuracy = 56.574%, Loss = 1.006350302696228
Epoch: 3396, Batch Gradient Norm: 36.028204318308234
Epoch: 3396, Batch Gradient Norm after: 22.36067524242631
Epoch 3397/10000, Prediction Accuracy = 56.63199999999999%, Loss = 0.9949398756027221
Epoch: 3397, Batch Gradient Norm: 40.258486740542445
Epoch: 3397, Batch Gradient Norm after: 22.360677217381046
Epoch 3398/10000, Prediction Accuracy = 56.581999999999994%, Loss = 1.00607008934021
Epoch: 3398, Batch Gradient Norm: 36.02557931460945
Epoch: 3398, Batch Gradient Norm after: 22.3606786549812
Epoch 3399/10000, Prediction Accuracy = 56.65%, Loss = 0.9946966171264648
Epoch: 3399, Batch Gradient Norm: 40.24283655074424
Epoch: 3399, Batch Gradient Norm after: 22.360677965144212
Epoch 3400/10000, Prediction Accuracy = 56.59400000000001%, Loss = 1.0057876706123352
Epoch: 3400, Batch Gradient Norm: 36.01946104959117
Epoch: 3400, Batch Gradient Norm after: 22.36067846593511
Epoch 3401/10000, Prediction Accuracy = 56.652%, Loss = 0.9944548964500427
Epoch: 3401, Batch Gradient Norm: 40.22818296720389
Epoch: 3401, Batch Gradient Norm after: 22.360678832357696
Epoch 3402/10000, Prediction Accuracy = 56.598%, Loss = 1.0055127024650574
Epoch: 3402, Batch Gradient Norm: 36.017843533379015
Epoch: 3402, Batch Gradient Norm after: 22.360679492109583
Epoch 3403/10000, Prediction Accuracy = 56.65%, Loss = 0.9942071676254273
Epoch: 3403, Batch Gradient Norm: 40.2123270274319
Epoch: 3403, Batch Gradient Norm after: 22.360679070099728
Epoch 3404/10000, Prediction Accuracy = 56.605999999999995%, Loss = 1.0052355766296386
Epoch: 3404, Batch Gradient Norm: 36.01311186154873
Epoch: 3404, Batch Gradient Norm after: 22.360677586534102
Epoch 3405/10000, Prediction Accuracy = 56.64399999999999%, Loss = 0.9939621329307556
Epoch: 3405, Batch Gradient Norm: 40.1965797938472
Epoch: 3405, Batch Gradient Norm after: 22.360676082915543
Epoch 3406/10000, Prediction Accuracy = 56.61199999999999%, Loss = 1.0049569368362428
Epoch: 3406, Batch Gradient Norm: 36.00810138727961
Epoch: 3406, Batch Gradient Norm after: 22.360678823143274
Epoch 3407/10000, Prediction Accuracy = 56.653999999999996%, Loss = 0.9937216639518738
Epoch: 3407, Batch Gradient Norm: 40.181114328468425
Epoch: 3407, Batch Gradient Norm after: 22.36067705817297
Epoch 3408/10000, Prediction Accuracy = 56.620000000000005%, Loss = 1.004676353931427
Epoch: 3408, Batch Gradient Norm: 36.003780175722376
Epoch: 3408, Batch Gradient Norm after: 22.360678020127427
Epoch 3409/10000, Prediction Accuracy = 56.668000000000006%, Loss = 0.9934812426567078
Epoch: 3409, Batch Gradient Norm: 40.16536674466434
Epoch: 3409, Batch Gradient Norm after: 22.360677114868533
Epoch 3410/10000, Prediction Accuracy = 56.628%, Loss = 1.004405391216278
Epoch: 3410, Batch Gradient Norm: 35.99914206631333
Epoch: 3410, Batch Gradient Norm after: 22.360677847652582
Epoch 3411/10000, Prediction Accuracy = 56.666%, Loss = 0.9932399868965149
Epoch: 3411, Batch Gradient Norm: 40.150119668339876
Epoch: 3411, Batch Gradient Norm after: 22.360676976650822
Epoch 3412/10000, Prediction Accuracy = 56.636%, Loss = 1.0041380524635315
Epoch: 3412, Batch Gradient Norm: 35.994440020184726
Epoch: 3412, Batch Gradient Norm after: 22.3606786967273
Epoch 3413/10000, Prediction Accuracy = 56.672000000000004%, Loss = 0.9929993510246277
Epoch: 3413, Batch Gradient Norm: 40.13522320362813
Epoch: 3413, Batch Gradient Norm after: 22.36067743348162
Epoch 3414/10000, Prediction Accuracy = 56.636%, Loss = 1.0038721203804015
Epoch: 3414, Batch Gradient Norm: 35.98885615210814
Epoch: 3414, Batch Gradient Norm after: 22.36067868355928
Epoch 3415/10000, Prediction Accuracy = 56.66799999999999%, Loss = 0.9927597165107727
Epoch: 3415, Batch Gradient Norm: 40.12243631222964
Epoch: 3415, Batch Gradient Norm after: 22.360679228572494
Epoch 3416/10000, Prediction Accuracy = 56.64%, Loss = 1.003604567050934
Epoch: 3416, Batch Gradient Norm: 35.98391175185086
Epoch: 3416, Batch Gradient Norm after: 22.360680482560984
Epoch 3417/10000, Prediction Accuracy = 56.681999999999995%, Loss = 0.9925191164016723
Epoch: 3417, Batch Gradient Norm: 40.10941462093159
Epoch: 3417, Batch Gradient Norm after: 22.360676977966058
Epoch 3418/10000, Prediction Accuracy = 56.652%, Loss = 1.003332793712616
Epoch: 3418, Batch Gradient Norm: 35.97832200661487
Epoch: 3418, Batch Gradient Norm after: 22.360678513314813
Epoch 3419/10000, Prediction Accuracy = 56.694%, Loss = 0.9922790169715882
Epoch: 3419, Batch Gradient Norm: 40.09256356780768
Epoch: 3419, Batch Gradient Norm after: 22.360677933638215
Epoch 3420/10000, Prediction Accuracy = 56.662%, Loss = 1.0030589938163756
Epoch: 3420, Batch Gradient Norm: 35.974164884311534
Epoch: 3420, Batch Gradient Norm after: 22.36067879906704
Epoch 3421/10000, Prediction Accuracy = 56.7%, Loss = 0.9920374393463135
Epoch: 3421, Batch Gradient Norm: 40.0798403922407
Epoch: 3421, Batch Gradient Norm after: 22.360676761075545
Epoch 3422/10000, Prediction Accuracy = 56.676%, Loss = 1.0027960538864136
Epoch: 3422, Batch Gradient Norm: 35.97012500780918
Epoch: 3422, Batch Gradient Norm after: 22.36067882868108
Epoch 3423/10000, Prediction Accuracy = 56.7%, Loss = 0.9917984366416931
Epoch: 3423, Batch Gradient Norm: 40.07353658680939
Epoch: 3423, Batch Gradient Norm after: 22.36067670159793
Epoch 3424/10000, Prediction Accuracy = 56.68399999999999%, Loss = 1.00255047082901
Epoch: 3424, Batch Gradient Norm: 35.96416001709068
Epoch: 3424, Batch Gradient Norm after: 22.360677059644047
Epoch 3425/10000, Prediction Accuracy = 56.709999999999994%, Loss = 0.9915572762489319
Epoch: 3425, Batch Gradient Norm: 40.06527535579938
Epoch: 3425, Batch Gradient Norm after: 22.36067529692351
Epoch 3426/10000, Prediction Accuracy = 56.69%, Loss = 1.002293634414673
Epoch: 3426, Batch Gradient Norm: 35.95768622935138
Epoch: 3426, Batch Gradient Norm after: 22.36067631790886
Epoch 3427/10000, Prediction Accuracy = 56.705999999999996%, Loss = 0.9913169860839843
Epoch: 3427, Batch Gradient Norm: 40.0595760265666
Epoch: 3427, Batch Gradient Norm after: 22.36067708762755
Epoch 3428/10000, Prediction Accuracy = 56.698%, Loss = 1.0020519137382506
Epoch: 3428, Batch Gradient Norm: 35.95246430915702
Epoch: 3428, Batch Gradient Norm after: 22.360677483428248
Epoch 3429/10000, Prediction Accuracy = 56.70399999999999%, Loss = 0.9910707831382751
Epoch: 3429, Batch Gradient Norm: 40.05973955921141
Epoch: 3429, Batch Gradient Norm after: 22.360675308116136
Epoch 3430/10000, Prediction Accuracy = 56.705999999999996%, Loss = 1.0018263339996338
Epoch: 3430, Batch Gradient Norm: 35.944174963256785
Epoch: 3430, Batch Gradient Norm after: 22.3606775168998
Epoch 3431/10000, Prediction Accuracy = 56.705999999999996%, Loss = 0.9908226490020752
Epoch: 3431, Batch Gradient Norm: 40.061014238318585
Epoch: 3431, Batch Gradient Norm after: 22.36067713866933
Epoch 3432/10000, Prediction Accuracy = 56.708000000000006%, Loss = 1.0016118288040161
Epoch: 3432, Batch Gradient Norm: 35.93632356821608
Epoch: 3432, Batch Gradient Norm after: 22.360679157451727
Epoch 3433/10000, Prediction Accuracy = 56.71200000000001%, Loss = 0.990581500530243
Epoch: 3433, Batch Gradient Norm: 40.06509227513943
Epoch: 3433, Batch Gradient Norm after: 22.360678555908404
Epoch 3434/10000, Prediction Accuracy = 56.71400000000001%, Loss = 1.001399028301239
Epoch: 3434, Batch Gradient Norm: 35.92662787791163
Epoch: 3434, Batch Gradient Norm after: 22.360678549136935
Epoch 3435/10000, Prediction Accuracy = 56.73%, Loss = 0.9903355002403259
Epoch: 3435, Batch Gradient Norm: 40.06805163606639
Epoch: 3435, Batch Gradient Norm after: 22.360675382136005
Epoch 3436/10000, Prediction Accuracy = 56.715999999999994%, Loss = 1.0011887788772582
Epoch: 3436, Batch Gradient Norm: 35.914856687918245
Epoch: 3436, Batch Gradient Norm after: 22.360675715892214
Epoch 3437/10000, Prediction Accuracy = 56.74399999999999%, Loss = 0.9900852918624878
Epoch: 3437, Batch Gradient Norm: 40.077467188031804
Epoch: 3437, Batch Gradient Norm after: 22.36067699694327
Epoch 3438/10000, Prediction Accuracy = 56.73%, Loss = 1.000984799861908
Epoch: 3438, Batch Gradient Norm: 35.90610939891142
Epoch: 3438, Batch Gradient Norm after: 22.36067893329377
Epoch 3439/10000, Prediction Accuracy = 56.738%, Loss = 0.9898411989212036
Epoch: 3439, Batch Gradient Norm: 40.08431283309467
Epoch: 3439, Batch Gradient Norm after: 22.36067884916831
Epoch 3440/10000, Prediction Accuracy = 56.738%, Loss = 1.0007814168930054
Epoch: 3440, Batch Gradient Norm: 35.897426122916585
Epoch: 3440, Batch Gradient Norm after: 22.36068038325574
Epoch 3441/10000, Prediction Accuracy = 56.746%, Loss = 0.9895951509475708
Epoch: 3441, Batch Gradient Norm: 40.08792201033199
Epoch: 3441, Batch Gradient Norm after: 22.36067634366617
Epoch 3442/10000, Prediction Accuracy = 56.739999999999995%, Loss = 1.0005601167678833
Epoch: 3442, Batch Gradient Norm: 35.88887674173357
Epoch: 3442, Batch Gradient Norm after: 22.360680077726926
Epoch 3443/10000, Prediction Accuracy = 56.760000000000005%, Loss = 0.9893529176712036
Epoch: 3443, Batch Gradient Norm: 40.0866852881504
Epoch: 3443, Batch Gradient Norm after: 22.360678250972136
Epoch 3444/10000, Prediction Accuracy = 56.748000000000005%, Loss = 1.000338113307953
Epoch: 3444, Batch Gradient Norm: 35.881939613983725
Epoch: 3444, Batch Gradient Norm after: 22.360680644235607
Epoch 3445/10000, Prediction Accuracy = 56.760000000000005%, Loss = 0.9891085028648376
Epoch: 3445, Batch Gradient Norm: 40.08534665133053
Epoch: 3445, Batch Gradient Norm after: 22.360677687707813
Epoch 3446/10000, Prediction Accuracy = 56.751999999999995%, Loss = 1.000106406211853
Epoch: 3446, Batch Gradient Norm: 35.872742297205264
Epoch: 3446, Batch Gradient Norm after: 22.360678627822306
Epoch 3447/10000, Prediction Accuracy = 56.766%, Loss = 0.9888659119606018
Epoch: 3447, Batch Gradient Norm: 40.08921910619409
Epoch: 3447, Batch Gradient Norm after: 22.36067791360656
Epoch 3448/10000, Prediction Accuracy = 56.75599999999999%, Loss = 0.9998955607414246
Epoch: 3448, Batch Gradient Norm: 35.864459414180274
Epoch: 3448, Batch Gradient Norm after: 22.360678276917746
Epoch 3449/10000, Prediction Accuracy = 56.760000000000005%, Loss = 0.9886190056800842
Epoch: 3449, Batch Gradient Norm: 40.095981839147015
Epoch: 3449, Batch Gradient Norm after: 22.360678562525305
Epoch 3450/10000, Prediction Accuracy = 56.763999999999996%, Loss = 0.9996806025505066
Epoch: 3450, Batch Gradient Norm: 35.8572880038013
Epoch: 3450, Batch Gradient Norm after: 22.360679670215827
Epoch 3451/10000, Prediction Accuracy = 56.751999999999995%, Loss = 0.9883759021759033
Epoch: 3451, Batch Gradient Norm: 40.096315260712494
Epoch: 3451, Batch Gradient Norm after: 22.360677490433762
Epoch 3452/10000, Prediction Accuracy = 56.76800000000001%, Loss = 0.9994645357131958
Epoch: 3452, Batch Gradient Norm: 35.850442557771935
Epoch: 3452, Batch Gradient Norm after: 22.360677017592817
Epoch 3453/10000, Prediction Accuracy = 56.75599999999999%, Loss = 0.9881330132484436
Epoch: 3453, Batch Gradient Norm: 40.09721229756706
Epoch: 3453, Batch Gradient Norm after: 22.3606794057635
Epoch 3454/10000, Prediction Accuracy = 56.77%, Loss = 0.9992356777191163
Epoch: 3454, Batch Gradient Norm: 35.84160989143018
Epoch: 3454, Batch Gradient Norm after: 22.36067978589642
Epoch 3455/10000, Prediction Accuracy = 56.767999999999994%, Loss = 0.9878958702087403
Epoch: 3455, Batch Gradient Norm: 40.09708611817242
Epoch: 3455, Batch Gradient Norm after: 22.360678615535843
Epoch 3456/10000, Prediction Accuracy = 56.76800000000001%, Loss = 0.9990147233009339
Epoch: 3456, Batch Gradient Norm: 35.835703441814054
Epoch: 3456, Batch Gradient Norm after: 22.360677631656767
Epoch 3457/10000, Prediction Accuracy = 56.76800000000001%, Loss = 0.9876513242721557
Epoch: 3457, Batch Gradient Norm: 40.09495014310354
Epoch: 3457, Batch Gradient Norm after: 22.360678489799298
Epoch 3458/10000, Prediction Accuracy = 56.78000000000001%, Loss = 0.9987845063209534
Epoch: 3458, Batch Gradient Norm: 35.8304399569858
Epoch: 3458, Batch Gradient Norm after: 22.36067823990293
Epoch 3459/10000, Prediction Accuracy = 56.774%, Loss = 0.9874101519584656
Epoch: 3459, Batch Gradient Norm: 40.09190112711612
Epoch: 3459, Batch Gradient Norm after: 22.360678516244352
Epoch 3460/10000, Prediction Accuracy = 56.79%, Loss = 0.9985512018203735
Epoch: 3460, Batch Gradient Norm: 35.824815902122836
Epoch: 3460, Batch Gradient Norm after: 22.360677051175966
Epoch 3461/10000, Prediction Accuracy = 56.786%, Loss = 0.9871677279472351
Epoch: 3461, Batch Gradient Norm: 40.0901206366669
Epoch: 3461, Batch Gradient Norm after: 22.360677839312412
Epoch 3462/10000, Prediction Accuracy = 56.794000000000004%, Loss = 0.9983232855796814
Epoch: 3462, Batch Gradient Norm: 35.81972563087918
Epoch: 3462, Batch Gradient Norm after: 22.360676481426044
Epoch 3463/10000, Prediction Accuracy = 56.80200000000001%, Loss = 0.9869245648384094
Epoch: 3463, Batch Gradient Norm: 40.0865546619068
Epoch: 3463, Batch Gradient Norm after: 22.360680551061865
Epoch 3464/10000, Prediction Accuracy = 56.788%, Loss = 0.9980895280838012
Epoch: 3464, Batch Gradient Norm: 35.81184871744035
Epoch: 3464, Batch Gradient Norm after: 22.360675670046952
Epoch 3465/10000, Prediction Accuracy = 56.80800000000001%, Loss = 0.9866857528686523
Epoch: 3465, Batch Gradient Norm: 40.09017009481672
Epoch: 3465, Batch Gradient Norm after: 22.360678574078438
Epoch 3466/10000, Prediction Accuracy = 56.802%, Loss = 0.9978756308555603
Epoch: 3466, Batch Gradient Norm: 35.80336832380077
Epoch: 3466, Batch Gradient Norm after: 22.360679163828227
Epoch 3467/10000, Prediction Accuracy = 56.802%, Loss = 0.9864412426948548
Epoch: 3467, Batch Gradient Norm: 40.0977082533695
Epoch: 3467, Batch Gradient Norm after: 22.360678658447082
Epoch 3468/10000, Prediction Accuracy = 56.8%, Loss = 0.9976714372634887
Epoch: 3468, Batch Gradient Norm: 35.79473224094174
Epoch: 3468, Batch Gradient Norm after: 22.360677251065912
Epoch 3469/10000, Prediction Accuracy = 56.806%, Loss = 0.9861948728561402
Epoch: 3469, Batch Gradient Norm: 40.105854865046766
Epoch: 3469, Batch Gradient Norm after: 22.360677424473796
Epoch 3470/10000, Prediction Accuracy = 56.79600000000001%, Loss = 0.9974729776382446
Epoch: 3470, Batch Gradient Norm: 35.784503461261565
Epoch: 3470, Batch Gradient Norm after: 22.360677793654652
Epoch 3471/10000, Prediction Accuracy = 56.81%, Loss = 0.9859543204307556
Epoch: 3471, Batch Gradient Norm: 40.11763743430585
Epoch: 3471, Batch Gradient Norm after: 22.360679412283968
Epoch 3472/10000, Prediction Accuracy = 56.79600000000001%, Loss = 0.9972777843475342
Epoch: 3472, Batch Gradient Norm: 35.77751458126299
Epoch: 3472, Batch Gradient Norm after: 22.360677116971342
Epoch 3473/10000, Prediction Accuracy = 56.814%, Loss = 0.9857085227966309
Epoch: 3473, Batch Gradient Norm: 40.12506827788783
Epoch: 3473, Batch Gradient Norm after: 22.360679283660186
Epoch 3474/10000, Prediction Accuracy = 56.8%, Loss = 0.9970793843269348
Epoch: 3474, Batch Gradient Norm: 35.76772938776229
Epoch: 3474, Batch Gradient Norm after: 22.360675682009443
Epoch 3475/10000, Prediction Accuracy = 56.824%, Loss = 0.9854604363441467
Epoch: 3475, Batch Gradient Norm: 40.13501072016364
Epoch: 3475, Batch Gradient Norm after: 22.360680423355817
Epoch 3476/10000, Prediction Accuracy = 56.803999999999995%, Loss = 0.9968801975250244
Epoch: 3476, Batch Gradient Norm: 35.75951496919555
Epoch: 3476, Batch Gradient Norm after: 22.360677118041902
Epoch 3477/10000, Prediction Accuracy = 56.815999999999995%, Loss = 0.9852156519889832
Epoch: 3477, Batch Gradient Norm: 40.14370343726721
Epoch: 3477, Batch Gradient Norm after: 22.360678350983193
Epoch 3478/10000, Prediction Accuracy = 56.818000000000005%, Loss = 0.9966952323913574
Epoch: 3478, Batch Gradient Norm: 35.75187939666185
Epoch: 3478, Batch Gradient Norm after: 22.36067860857531
Epoch 3479/10000, Prediction Accuracy = 56.815999999999995%, Loss = 0.9849716901779175
Epoch: 3479, Batch Gradient Norm: 40.152452753479
Epoch: 3479, Batch Gradient Norm after: 22.3606780107923
Epoch 3480/10000, Prediction Accuracy = 56.815999999999995%, Loss = 0.9964940786361695
Epoch: 3480, Batch Gradient Norm: 35.74474566576032
Epoch: 3480, Batch Gradient Norm after: 22.36067689972511
Epoch 3481/10000, Prediction Accuracy = 56.822%, Loss = 0.9847288012504578
Epoch: 3481, Batch Gradient Norm: 40.16105557054749
Epoch: 3481, Batch Gradient Norm after: 22.360677980936163
Epoch 3482/10000, Prediction Accuracy = 56.818000000000005%, Loss = 0.9963053464889526
Epoch: 3482, Batch Gradient Norm: 35.73824480037868
Epoch: 3482, Batch Gradient Norm after: 22.360677735757317
Epoch 3483/10000, Prediction Accuracy = 56.824%, Loss = 0.9844837069511414
Epoch: 3483, Batch Gradient Norm: 40.17167849964266
Epoch: 3483, Batch Gradient Norm after: 22.36068084252488
Epoch 3484/10000, Prediction Accuracy = 56.84000000000001%, Loss = 0.9961056709289551
Epoch: 3484, Batch Gradient Norm: 35.72908049211809
Epoch: 3484, Batch Gradient Norm after: 22.360679004274107
Epoch 3485/10000, Prediction Accuracy = 56.83200000000001%, Loss = 0.9842443227767944
Epoch: 3485, Batch Gradient Norm: 40.1837359966328
Epoch: 3485, Batch Gradient Norm after: 22.360678946565482
Epoch 3486/10000, Prediction Accuracy = 56.846000000000004%, Loss = 0.9959120035171509
Epoch: 3486, Batch Gradient Norm: 35.72039260328843
Epoch: 3486, Batch Gradient Norm after: 22.360678500648227
Epoch 3487/10000, Prediction Accuracy = 56.83200000000001%, Loss = 0.9839980959892273
Epoch: 3487, Batch Gradient Norm: 40.19966723867368
Epoch: 3487, Batch Gradient Norm after: 22.36067836784089
Epoch 3488/10000, Prediction Accuracy = 56.85%, Loss = 0.995741891860962
Epoch: 3488, Batch Gradient Norm: 35.71353509260785
Epoch: 3488, Batch Gradient Norm after: 22.360679824051008
Epoch 3489/10000, Prediction Accuracy = 56.854%, Loss = 0.9837529540061951
Epoch: 3489, Batch Gradient Norm: 40.20618453149571
Epoch: 3489, Batch Gradient Norm after: 22.360678921372287
Epoch 3490/10000, Prediction Accuracy = 56.862%, Loss = 0.9955421209335327
Epoch: 3490, Batch Gradient Norm: 35.70591689109719
Epoch: 3490, Batch Gradient Norm after: 22.36067835587497
Epoch 3491/10000, Prediction Accuracy = 56.854%, Loss = 0.983509635925293
Epoch: 3491, Batch Gradient Norm: 40.21525782216164
Epoch: 3491, Batch Gradient Norm after: 22.360679046612844
Epoch 3492/10000, Prediction Accuracy = 56.86%, Loss = 0.9953482866287231
Epoch: 3492, Batch Gradient Norm: 35.699430726739706
Epoch: 3492, Batch Gradient Norm after: 22.360679800635967
Epoch 3493/10000, Prediction Accuracy = 56.848%, Loss = 0.9832720875740051
Epoch: 3493, Batch Gradient Norm: 40.22650367576598
Epoch: 3493, Batch Gradient Norm after: 22.360678233660604
Epoch 3494/10000, Prediction Accuracy = 56.862%, Loss = 0.9951484441757202
Epoch: 3494, Batch Gradient Norm: 35.690716483482134
Epoch: 3494, Batch Gradient Norm after: 22.360677311172473
Epoch 3495/10000, Prediction Accuracy = 56.86%, Loss = 0.9830336809158325
Epoch: 3495, Batch Gradient Norm: 40.24046973632806
Epoch: 3495, Batch Gradient Norm after: 22.36067926246287
Epoch 3496/10000, Prediction Accuracy = 56.862%, Loss = 0.994963526725769
Epoch: 3496, Batch Gradient Norm: 35.68444639693026
Epoch: 3496, Batch Gradient Norm after: 22.360677675894284
Epoch 3497/10000, Prediction Accuracy = 56.864%, Loss = 0.9827909827232361
Epoch: 3497, Batch Gradient Norm: 40.250842742871136
Epoch: 3497, Batch Gradient Norm after: 22.36067794156852
Epoch 3498/10000, Prediction Accuracy = 56.862%, Loss = 0.9947721123695373
Epoch: 3498, Batch Gradient Norm: 35.67667116498469
Epoch: 3498, Batch Gradient Norm after: 22.360676897691697
Epoch 3499/10000, Prediction Accuracy = 56.86400000000001%, Loss = 0.9825477242469788
Epoch: 3499, Batch Gradient Norm: 40.25682257793703
Epoch: 3499, Batch Gradient Norm after: 22.36067810520862
Epoch 3500/10000, Prediction Accuracy = 56.86999999999999%, Loss = 0.9945644617080689
Epoch: 3500, Batch Gradient Norm: 35.66979775190702
Epoch: 3500, Batch Gradient Norm after: 22.360678675401463
Epoch 3501/10000, Prediction Accuracy = 56.86600000000001%, Loss = 0.9823111057281494
Epoch: 3501, Batch Gradient Norm: 40.2678767078399
Epoch: 3501, Batch Gradient Norm after: 22.36067875542625
Epoch 3502/10000, Prediction Accuracy = 56.86600000000001%, Loss = 0.9943689465522766
Epoch: 3502, Batch Gradient Norm: 35.66188230004206
Epoch: 3502, Batch Gradient Norm after: 22.360680158513993
Epoch 3503/10000, Prediction Accuracy = 56.86800000000001%, Loss = 0.9820716977119446
Epoch: 3503, Batch Gradient Norm: 40.277841872902926
Epoch: 3503, Batch Gradient Norm after: 22.360677382290106
Epoch 3504/10000, Prediction Accuracy = 56.870000000000005%, Loss = 0.9941763639450073
Epoch: 3504, Batch Gradient Norm: 35.654763834452176
Epoch: 3504, Batch Gradient Norm after: 22.360677295872854
Epoch 3505/10000, Prediction Accuracy = 56.860000000000014%, Loss = 0.9818305611610413
Epoch: 3505, Batch Gradient Norm: 40.28059925145943
Epoch: 3505, Batch Gradient Norm after: 22.360679191188723
Epoch 3506/10000, Prediction Accuracy = 56.867999999999995%, Loss = 0.9939667344093323
Epoch: 3506, Batch Gradient Norm: 35.64699628436798
Epoch: 3506, Batch Gradient Norm after: 22.360679917923516
Epoch 3507/10000, Prediction Accuracy = 56.86%, Loss = 0.9815927743911743
Epoch: 3507, Batch Gradient Norm: 40.28268417733997
Epoch: 3507, Batch Gradient Norm after: 22.360678250442188
Epoch 3508/10000, Prediction Accuracy = 56.88399999999999%, Loss = 0.9937506675720215
Epoch: 3508, Batch Gradient Norm: 35.64221183336616
Epoch: 3508, Batch Gradient Norm after: 22.360678331598535
Epoch 3509/10000, Prediction Accuracy = 56.878%, Loss = 0.9813523530960083
Epoch: 3509, Batch Gradient Norm: 40.28719108299413
Epoch: 3509, Batch Gradient Norm after: 22.360677237939143
Epoch 3510/10000, Prediction Accuracy = 56.9%, Loss = 0.9935320615768433
Epoch: 3510, Batch Gradient Norm: 35.63491981483254
Epoch: 3510, Batch Gradient Norm after: 22.36067688872498
Epoch 3511/10000, Prediction Accuracy = 56.874%, Loss = 0.9811128973960876
Epoch: 3511, Batch Gradient Norm: 40.28905279923677
Epoch: 3511, Batch Gradient Norm after: 22.360678003438252
Epoch 3512/10000, Prediction Accuracy = 56.903999999999996%, Loss = 0.9933211803436279
Epoch: 3512, Batch Gradient Norm: 35.62804318625786
Epoch: 3512, Batch Gradient Norm after: 22.360678252726068
Epoch 3513/10000, Prediction Accuracy = 56.888%, Loss = 0.9808774709701538
Epoch: 3513, Batch Gradient Norm: 40.29153532034017
Epoch: 3513, Batch Gradient Norm after: 22.360677208214547
Epoch 3514/10000, Prediction Accuracy = 56.918000000000006%, Loss = 0.9931019067764282
Epoch: 3514, Batch Gradient Norm: 35.622478571695794
Epoch: 3514, Batch Gradient Norm after: 22.360676375193833
Epoch 3515/10000, Prediction Accuracy = 56.896%, Loss = 0.9806406021118164
Epoch: 3515, Batch Gradient Norm: 40.2891330185088
Epoch: 3515, Batch Gradient Norm after: 22.360677931591376
Epoch 3516/10000, Prediction Accuracy = 56.92%, Loss = 0.9928815722465515
Epoch: 3516, Batch Gradient Norm: 35.61695683465451
Epoch: 3516, Batch Gradient Norm after: 22.36067693690566
Epoch 3517/10000, Prediction Accuracy = 56.9%, Loss = 0.9803994536399842
Epoch: 3517, Batch Gradient Norm: 40.28227717178807
Epoch: 3517, Batch Gradient Norm after: 22.36067736561624
Epoch 3518/10000, Prediction Accuracy = 56.922000000000004%, Loss = 0.9926394581794739
Epoch: 3518, Batch Gradient Norm: 35.611804125693
Epoch: 3518, Batch Gradient Norm after: 22.360679667904932
Epoch 3519/10000, Prediction Accuracy = 56.908%, Loss = 0.9801669359207154
Epoch: 3519, Batch Gradient Norm: 40.27690891464273
Epoch: 3519, Batch Gradient Norm after: 22.36067841794795
Epoch 3520/10000, Prediction Accuracy = 56.928%, Loss = 0.9924006581306457
Epoch: 3520, Batch Gradient Norm: 35.60599696790578
Epoch: 3520, Batch Gradient Norm after: 22.360678362324318
Epoch 3521/10000, Prediction Accuracy = 56.916%, Loss = 0.9799289703369141
Epoch: 3521, Batch Gradient Norm: 40.271831572932406
Epoch: 3521, Batch Gradient Norm after: 22.360677737140033
Epoch 3522/10000, Prediction Accuracy = 56.94000000000001%, Loss = 0.9921568989753723
Epoch: 3522, Batch Gradient Norm: 35.598765586479765
Epoch: 3522, Batch Gradient Norm after: 22.360677640835345
Epoch 3523/10000, Prediction Accuracy = 56.919999999999995%, Loss = 0.9796925067901612
Epoch: 3523, Batch Gradient Norm: 40.264567840245626
Epoch: 3523, Batch Gradient Norm after: 22.360678582777435
Epoch 3524/10000, Prediction Accuracy = 56.964%, Loss = 0.9919201254844665
Epoch: 3524, Batch Gradient Norm: 35.59175764880168
Epoch: 3524, Batch Gradient Norm after: 22.360677448134687
Epoch 3525/10000, Prediction Accuracy = 56.928%, Loss = 0.9794539451599121
Epoch: 3525, Batch Gradient Norm: 40.260597089968314
Epoch: 3525, Batch Gradient Norm after: 22.36067928098895
Epoch 3526/10000, Prediction Accuracy = 56.976%, Loss = 0.9916871070861817
Epoch: 3526, Batch Gradient Norm: 35.58370976082602
Epoch: 3526, Batch Gradient Norm after: 22.360677285663705
Epoch 3527/10000, Prediction Accuracy = 56.926%, Loss = 0.9792173385620118
Epoch: 3527, Batch Gradient Norm: 40.26088165670443
Epoch: 3527, Batch Gradient Norm after: 22.360677585026327
Epoch 3528/10000, Prediction Accuracy = 56.984%, Loss = 0.9914682269096374
Epoch: 3528, Batch Gradient Norm: 35.57496920682836
Epoch: 3528, Batch Gradient Norm after: 22.360676989237234
Epoch 3529/10000, Prediction Accuracy = 56.93399999999999%, Loss = 0.9789799451828003
Epoch: 3529, Batch Gradient Norm: 40.25498857676687
Epoch: 3529, Batch Gradient Norm after: 22.360675268913397
Epoch 3530/10000, Prediction Accuracy = 56.99399999999999%, Loss = 0.991234278678894
Epoch: 3530, Batch Gradient Norm: 35.56813814335414
Epoch: 3530, Batch Gradient Norm after: 22.360676516314914
Epoch 3531/10000, Prediction Accuracy = 56.94000000000001%, Loss = 0.9787453651428223
Epoch: 3531, Batch Gradient Norm: 40.2484104133198
Epoch: 3531, Batch Gradient Norm after: 22.36067703053824
Epoch 3532/10000, Prediction Accuracy = 57.001999999999995%, Loss = 0.9909958600997925
Epoch: 3532, Batch Gradient Norm: 35.56193217151813
Epoch: 3532, Batch Gradient Norm after: 22.360676240833744
Epoch 3533/10000, Prediction Accuracy = 56.95400000000001%, Loss = 0.9785081386566162
Epoch: 3533, Batch Gradient Norm: 40.23834238788787
Epoch: 3533, Batch Gradient Norm after: 22.36067813786919
Epoch 3534/10000, Prediction Accuracy = 57.00599999999999%, Loss = 0.9907471656799316
Epoch: 3534, Batch Gradient Norm: 35.556140430683854
Epoch: 3534, Batch Gradient Norm after: 22.360678321176348
Epoch 3535/10000, Prediction Accuracy = 56.970000000000006%, Loss = 0.9782738089561462
Epoch: 3535, Batch Gradient Norm: 40.22602167113753
Epoch: 3535, Batch Gradient Norm after: 22.360678783936514
Epoch 3536/10000, Prediction Accuracy = 56.998000000000005%, Loss = 0.9904970049858093
Epoch: 3536, Batch Gradient Norm: 35.55031509633159
Epoch: 3536, Batch Gradient Norm after: 22.36067816922241
Epoch 3537/10000, Prediction Accuracy = 56.968%, Loss = 0.9780367136001586
Epoch: 3537, Batch Gradient Norm: 40.20692356959497
Epoch: 3537, Batch Gradient Norm after: 22.360677278752235
Epoch 3538/10000, Prediction Accuracy = 57.010000000000005%, Loss = 0.9902277827262879
Epoch: 3538, Batch Gradient Norm: 35.543913319976774
Epoch: 3538, Batch Gradient Norm after: 22.36067743056926
Epoch 3539/10000, Prediction Accuracy = 56.972%, Loss = 0.9778037309646607
Epoch: 3539, Batch Gradient Norm: 40.19302773462623
Epoch: 3539, Batch Gradient Norm after: 22.360678813947185
Epoch 3540/10000, Prediction Accuracy = 57.016%, Loss = 0.9899695158004761
Epoch: 3540, Batch Gradient Norm: 35.540880787565236
Epoch: 3540, Batch Gradient Norm after: 22.36067714695022
Epoch 3541/10000, Prediction Accuracy = 56.962%, Loss = 0.9775688529014588
Epoch: 3541, Batch Gradient Norm: 40.17639848782829
Epoch: 3541, Batch Gradient Norm after: 22.36068013115453
Epoch 3542/10000, Prediction Accuracy = 57.02%, Loss = 0.9896999835968018
Epoch: 3542, Batch Gradient Norm: 35.53463211065204
Epoch: 3542, Batch Gradient Norm after: 22.360676160019153
Epoch 3543/10000, Prediction Accuracy = 56.967999999999996%, Loss = 0.977326500415802
Epoch: 3543, Batch Gradient Norm: 40.16158282964395
Epoch: 3543, Batch Gradient Norm after: 22.36067710306752
Epoch 3544/10000, Prediction Accuracy = 57.028%, Loss = 0.9894358515739441
Epoch: 3544, Batch Gradient Norm: 35.5281489708806
Epoch: 3544, Batch Gradient Norm after: 22.36067563846322
Epoch 3545/10000, Prediction Accuracy = 56.983999999999995%, Loss = 0.977092695236206
Epoch: 3545, Batch Gradient Norm: 40.14926356761303
Epoch: 3545, Batch Gradient Norm after: 22.36067975381748
Epoch 3546/10000, Prediction Accuracy = 57.036%, Loss = 0.9891811490058899
Epoch: 3546, Batch Gradient Norm: 35.523957749223264
Epoch: 3546, Batch Gradient Norm after: 22.360675289620165
Epoch 3547/10000, Prediction Accuracy = 56.998000000000005%, Loss = 0.9768561959266663
Epoch: 3547, Batch Gradient Norm: 40.13428837876257
Epoch: 3547, Batch Gradient Norm after: 22.360677827408168
Epoch 3548/10000, Prediction Accuracy = 57.036%, Loss = 0.9889225721359253
Epoch: 3548, Batch Gradient Norm: 35.5185777813783
Epoch: 3548, Batch Gradient Norm after: 22.360676081892365
Epoch 3549/10000, Prediction Accuracy = 56.992%, Loss = 0.9766257882118226
Epoch: 3549, Batch Gradient Norm: 40.121034992353714
Epoch: 3549, Batch Gradient Norm after: 22.36067854083722
Epoch 3550/10000, Prediction Accuracy = 57.038%, Loss = 0.9886642336845398
Epoch: 3550, Batch Gradient Norm: 35.51047979088206
Epoch: 3550, Batch Gradient Norm after: 22.36067694102363
Epoch 3551/10000, Prediction Accuracy = 56.996%, Loss = 0.9763929009437561
Epoch: 3551, Batch Gradient Norm: 40.1092692952559
Epoch: 3551, Batch Gradient Norm after: 22.360679102481374
Epoch 3552/10000, Prediction Accuracy = 57.056%, Loss = 0.9884084939956665
Epoch: 3552, Batch Gradient Norm: 35.506366020605974
Epoch: 3552, Batch Gradient Norm after: 22.360677839930315
Epoch 3553/10000, Prediction Accuracy = 57.0%, Loss = 0.9761590123176574
Epoch: 3553, Batch Gradient Norm: 40.09110446324037
Epoch: 3553, Batch Gradient Norm after: 22.360679176474456
Epoch 3554/10000, Prediction Accuracy = 57.06199999999999%, Loss = 0.9881460428237915
Epoch: 3554, Batch Gradient Norm: 35.498981846593836
Epoch: 3554, Batch Gradient Norm after: 22.360676066953474
Epoch 3555/10000, Prediction Accuracy = 57.00599999999999%, Loss = 0.9759285926818848
Epoch: 3555, Batch Gradient Norm: 40.07447069841383
Epoch: 3555, Batch Gradient Norm after: 22.36067732250906
Epoch 3556/10000, Prediction Accuracy = 57.07000000000001%, Loss = 0.9878857135772705
Epoch: 3556, Batch Gradient Norm: 35.49553042409634
Epoch: 3556, Batch Gradient Norm after: 22.360678146190967
Epoch 3557/10000, Prediction Accuracy = 57.007999999999996%, Loss = 0.9756981015205384
Epoch: 3557, Batch Gradient Norm: 40.053340781301536
Epoch: 3557, Batch Gradient Norm after: 22.360678244083843
Epoch 3558/10000, Prediction Accuracy = 57.068%, Loss = 0.9876081943511963
Epoch: 3558, Batch Gradient Norm: 35.48934873528217
Epoch: 3558, Batch Gradient Norm after: 22.36067941391525
Epoch 3559/10000, Prediction Accuracy = 57.012%, Loss = 0.9754699468612671
Epoch: 3559, Batch Gradient Norm: 40.02956187846458
Epoch: 3559, Batch Gradient Norm after: 22.360678067860267
Epoch 3560/10000, Prediction Accuracy = 57.074%, Loss = 0.9873299717903137
Epoch: 3560, Batch Gradient Norm: 35.486233148070774
Epoch: 3560, Batch Gradient Norm after: 22.36067809433436
Epoch 3561/10000, Prediction Accuracy = 57.022000000000006%, Loss = 0.9752439618110657
Epoch: 3561, Batch Gradient Norm: 40.00176111863512
Epoch: 3561, Batch Gradient Norm after: 22.360678843618736
Epoch 3562/10000, Prediction Accuracy = 57.08399999999999%, Loss = 0.9870357871055603
Epoch: 3562, Batch Gradient Norm: 35.482823080680596
Epoch: 3562, Batch Gradient Norm after: 22.360679035356345
Epoch 3563/10000, Prediction Accuracy = 57.024%, Loss = 0.9750135540962219
Epoch: 3563, Batch Gradient Norm: 39.97653441664061
Epoch: 3563, Batch Gradient Norm after: 22.360677275283305
Epoch 3564/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.9867451667785645
Epoch: 3564, Batch Gradient Norm: 35.478090195330886
Epoch: 3564, Batch Gradient Norm after: 22.360677225609113
Epoch 3565/10000, Prediction Accuracy = 57.028%, Loss = 0.9747891187667846
Epoch: 3565, Batch Gradient Norm: 39.955205168401605
Epoch: 3565, Batch Gradient Norm after: 22.360677013162913
Epoch 3566/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.9864689826965332
Epoch: 3566, Batch Gradient Norm: 35.47220171688512
Epoch: 3566, Batch Gradient Norm after: 22.36067865030168
Epoch 3567/10000, Prediction Accuracy = 57.036%, Loss = 0.9745617151260376
Epoch: 3567, Batch Gradient Norm: 39.93884966455655
Epoch: 3567, Batch Gradient Norm after: 22.360679094829795
Epoch 3568/10000, Prediction Accuracy = 57.098%, Loss = 0.9862051248550415
Epoch: 3568, Batch Gradient Norm: 35.466620646870595
Epoch: 3568, Batch Gradient Norm after: 22.360677977699215
Epoch 3569/10000, Prediction Accuracy = 57.048%, Loss = 0.9743284463882447
Epoch: 3569, Batch Gradient Norm: 39.91843987422988
Epoch: 3569, Batch Gradient Norm after: 22.36067959774697
Epoch 3570/10000, Prediction Accuracy = 57.10799999999999%, Loss = 0.9859324097633362
Epoch: 3570, Batch Gradient Norm: 35.463012818514336
Epoch: 3570, Batch Gradient Norm after: 22.360680146995726
Epoch 3571/10000, Prediction Accuracy = 57.064%, Loss = 0.9740986466407776
Epoch: 3571, Batch Gradient Norm: 39.8999355157527
Epoch: 3571, Batch Gradient Norm after: 22.36067947375141
Epoch 3572/10000, Prediction Accuracy = 57.112%, Loss = 0.9856647968292236
Epoch: 3572, Batch Gradient Norm: 35.4585966993341
Epoch: 3572, Batch Gradient Norm after: 22.360678630447897
Epoch 3573/10000, Prediction Accuracy = 57.074%, Loss = 0.9738729238510132
Epoch: 3573, Batch Gradient Norm: 39.88276811966777
Epoch: 3573, Batch Gradient Norm after: 22.36067790662909
Epoch 3574/10000, Prediction Accuracy = 57.116%, Loss = 0.985400938987732
Epoch: 3574, Batch Gradient Norm: 35.45306229002214
Epoch: 3574, Batch Gradient Norm after: 22.360677530078856
Epoch 3575/10000, Prediction Accuracy = 57.074%, Loss = 0.9736487746238709
Epoch: 3575, Batch Gradient Norm: 39.8665263790541
Epoch: 3575, Batch Gradient Norm after: 22.36067691862783
Epoch 3576/10000, Prediction Accuracy = 57.112%, Loss = 0.9851441383361816
Epoch: 3576, Batch Gradient Norm: 35.4486449501226
Epoch: 3576, Batch Gradient Norm after: 22.360678986901558
Epoch 3577/10000, Prediction Accuracy = 57.08200000000001%, Loss = 0.973421049118042
Epoch: 3577, Batch Gradient Norm: 39.84957055970071
Epoch: 3577, Batch Gradient Norm after: 22.360676980623442
Epoch 3578/10000, Prediction Accuracy = 57.104%, Loss = 0.9848770499229431
Epoch: 3578, Batch Gradient Norm: 35.443199043016655
Epoch: 3578, Batch Gradient Norm after: 22.36067576435948
Epoch 3579/10000, Prediction Accuracy = 57.086%, Loss = 0.9731936693191529
Epoch: 3579, Batch Gradient Norm: 39.8319402720339
Epoch: 3579, Batch Gradient Norm after: 22.360680118205043
Epoch 3580/10000, Prediction Accuracy = 57.117999999999995%, Loss = 0.9846129894256592
Epoch: 3580, Batch Gradient Norm: 35.438930236463925
Epoch: 3580, Batch Gradient Norm after: 22.360678348362413
Epoch 3581/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.9729672551155091
Epoch: 3581, Batch Gradient Norm: 39.810874033977285
Epoch: 3581, Batch Gradient Norm after: 22.360677534068962
Epoch 3582/10000, Prediction Accuracy = 57.129999999999995%, Loss = 0.9843365788459778
Epoch: 3582, Batch Gradient Norm: 35.43367763083903
Epoch: 3582, Batch Gradient Norm after: 22.36067708479003
Epoch 3583/10000, Prediction Accuracy = 57.093999999999994%, Loss = 0.9727436184883118
Epoch: 3583, Batch Gradient Norm: 39.79208937252559
Epoch: 3583, Batch Gradient Norm after: 22.360678091412314
Epoch 3584/10000, Prediction Accuracy = 57.144000000000005%, Loss = 0.9840694427490234
Epoch: 3584, Batch Gradient Norm: 35.42945985353542
Epoch: 3584, Batch Gradient Norm after: 22.360677397411298
Epoch 3585/10000, Prediction Accuracy = 57.102%, Loss = 0.9725194692611694
Epoch: 3585, Batch Gradient Norm: 39.77214310379448
Epoch: 3585, Batch Gradient Norm after: 22.360677837351542
Epoch 3586/10000, Prediction Accuracy = 57.144000000000005%, Loss = 0.9838099956512452
Epoch: 3586, Batch Gradient Norm: 35.42360500040032
Epoch: 3586, Batch Gradient Norm after: 22.360678007613053
Epoch 3587/10000, Prediction Accuracy = 57.112%, Loss = 0.9722952604293823
Epoch: 3587, Batch Gradient Norm: 39.75633573481802
Epoch: 3587, Batch Gradient Norm after: 22.360677913134158
Epoch 3588/10000, Prediction Accuracy = 57.146%, Loss = 0.9835572838783264
Epoch: 3588, Batch Gradient Norm: 35.41867325136286
Epoch: 3588, Batch Gradient Norm after: 22.36067897495374
Epoch 3589/10000, Prediction Accuracy = 57.112%, Loss = 0.9720691323280335
Epoch: 3589, Batch Gradient Norm: 39.738636987485755
Epoch: 3589, Batch Gradient Norm after: 22.36067912423154
Epoch 3590/10000, Prediction Accuracy = 57.148%, Loss = 0.9832957863807679
Epoch: 3590, Batch Gradient Norm: 35.412462880070954
Epoch: 3590, Batch Gradient Norm after: 22.360677471329677
Epoch 3591/10000, Prediction Accuracy = 57.11%, Loss = 0.9718415021896363
Epoch: 3591, Batch Gradient Norm: 39.72313611879473
Epoch: 3591, Batch Gradient Norm after: 22.36067827044214
Epoch 3592/10000, Prediction Accuracy = 57.148%, Loss = 0.9830387949943542
Epoch: 3592, Batch Gradient Norm: 35.40852347606487
Epoch: 3592, Batch Gradient Norm after: 22.3606776078138
Epoch 3593/10000, Prediction Accuracy = 57.11%, Loss = 0.9716138720512391
Epoch: 3593, Batch Gradient Norm: 39.70622792261459
Epoch: 3593, Batch Gradient Norm after: 22.360677694783035
Epoch 3594/10000, Prediction Accuracy = 57.15%, Loss = 0.9827794432640076
Epoch: 3594, Batch Gradient Norm: 35.40438259215389
Epoch: 3594, Batch Gradient Norm after: 22.360676857521103
Epoch 3595/10000, Prediction Accuracy = 57.11%, Loss = 0.9713919878005981
Epoch: 3595, Batch Gradient Norm: 39.68586600173705
Epoch: 3595, Batch Gradient Norm after: 22.360677013972982
Epoch 3596/10000, Prediction Accuracy = 57.15%, Loss = 0.9825108647346497
Epoch: 3596, Batch Gradient Norm: 35.400926393907575
Epoch: 3596, Batch Gradient Norm after: 22.360680023478118
Epoch 3597/10000, Prediction Accuracy = 57.108000000000004%, Loss = 0.9711708188056946
Epoch: 3597, Batch Gradient Norm: 39.664205483691205
Epoch: 3597, Batch Gradient Norm after: 22.360678752827344
Epoch 3598/10000, Prediction Accuracy = 57.153999999999996%, Loss = 0.9822409868240356
Epoch: 3598, Batch Gradient Norm: 35.397359419027076
Epoch: 3598, Batch Gradient Norm after: 22.36067781802674
Epoch 3599/10000, Prediction Accuracy = 57.120000000000005%, Loss = 0.9709473848342896
Epoch: 3599, Batch Gradient Norm: 39.64129219104947
Epoch: 3599, Batch Gradient Norm after: 22.360676515774497
Epoch 3600/10000, Prediction Accuracy = 57.14399999999999%, Loss = 0.9819724678993225
Epoch: 3600, Batch Gradient Norm: 35.39556188014485
Epoch: 3600, Batch Gradient Norm after: 22.36067822198513
Epoch 3601/10000, Prediction Accuracy = 57.138%, Loss = 0.9707233905792236
Epoch: 3601, Batch Gradient Norm: 39.6213834203629
Epoch: 3601, Batch Gradient Norm after: 22.36067768028329
Epoch 3602/10000, Prediction Accuracy = 57.144000000000005%, Loss = 0.9817056655883789
Epoch: 3602, Batch Gradient Norm: 35.393309300361786
Epoch: 3602, Batch Gradient Norm after: 22.360678398196196
Epoch 3603/10000, Prediction Accuracy = 57.134%, Loss = 0.9705023527145386
Epoch: 3603, Batch Gradient Norm: 39.59895650185113
Epoch: 3603, Batch Gradient Norm after: 22.36067990578728
Epoch 3604/10000, Prediction Accuracy = 57.153999999999996%, Loss = 0.981438672542572
Epoch: 3604, Batch Gradient Norm: 35.38905122496192
Epoch: 3604, Batch Gradient Norm after: 22.36067731180982
Epoch 3605/10000, Prediction Accuracy = 57.11800000000001%, Loss = 0.9702823519706726
Epoch: 3605, Batch Gradient Norm: 39.58080206908428
Epoch: 3605, Batch Gradient Norm after: 22.360678907094126
Epoch 3606/10000, Prediction Accuracy = 57.16600000000001%, Loss = 0.9811809659004211
Epoch: 3606, Batch Gradient Norm: 35.38590037742116
Epoch: 3606, Batch Gradient Norm after: 22.360677145129948
Epoch 3607/10000, Prediction Accuracy = 57.105999999999995%, Loss = 0.9700610041618347
Epoch: 3607, Batch Gradient Norm: 39.56327011456554
Epoch: 3607, Batch Gradient Norm after: 22.360678190234214
Epoch 3608/10000, Prediction Accuracy = 57.15%, Loss = 0.9809244394302368
Epoch: 3608, Batch Gradient Norm: 35.38270391647086
Epoch: 3608, Batch Gradient Norm after: 22.360679633693852
Epoch 3609/10000, Prediction Accuracy = 57.1%, Loss = 0.9698382377624511
Epoch: 3609, Batch Gradient Norm: 39.5484977344317
Epoch: 3609, Batch Gradient Norm after: 22.360676558649864
Epoch 3610/10000, Prediction Accuracy = 57.15200000000001%, Loss = 0.9806729078292846
Epoch: 3610, Batch Gradient Norm: 35.37863987033616
Epoch: 3610, Batch Gradient Norm after: 22.360678635618076
Epoch 3611/10000, Prediction Accuracy = 57.104%, Loss = 0.9696152925491333
Epoch: 3611, Batch Gradient Norm: 39.53557981092804
Epoch: 3611, Batch Gradient Norm after: 22.360676923175006
Epoch 3612/10000, Prediction Accuracy = 57.158%, Loss = 0.9804215550422668
Epoch: 3612, Batch Gradient Norm: 35.372817437681334
Epoch: 3612, Batch Gradient Norm after: 22.360679959445505
Epoch 3613/10000, Prediction Accuracy = 57.120000000000005%, Loss = 0.9693905472755432
Epoch: 3613, Batch Gradient Norm: 39.52545187540988
Epoch: 3613, Batch Gradient Norm after: 22.360675613384835
Epoch 3614/10000, Prediction Accuracy = 57.166%, Loss = 0.9801867604255676
Epoch: 3614, Batch Gradient Norm: 35.366394665125526
Epoch: 3614, Batch Gradient Norm after: 22.36067743893762
Epoch 3615/10000, Prediction Accuracy = 57.122%, Loss = 0.9691670417785645
Epoch: 3615, Batch Gradient Norm: 39.519578268835794
Epoch: 3615, Batch Gradient Norm after: 22.36067769447709
Epoch 3616/10000, Prediction Accuracy = 57.164%, Loss = 0.9799578547477722
Epoch: 3616, Batch Gradient Norm: 35.36013581931976
Epoch: 3616, Batch Gradient Norm after: 22.36067933176963
Epoch 3617/10000, Prediction Accuracy = 57.128%, Loss = 0.9689417123794556
Epoch: 3617, Batch Gradient Norm: 39.50851831984459
Epoch: 3617, Batch Gradient Norm after: 22.36067807737187
Epoch 3618/10000, Prediction Accuracy = 57.164%, Loss = 0.9797257065773011
Epoch: 3618, Batch Gradient Norm: 35.35337863531751
Epoch: 3618, Batch Gradient Norm after: 22.360675995787563
Epoch 3619/10000, Prediction Accuracy = 57.13199999999999%, Loss = 0.9687231659889222
Epoch: 3619, Batch Gradient Norm: 39.50078765592704
Epoch: 3619, Batch Gradient Norm after: 22.36067759604582
Epoch 3620/10000, Prediction Accuracy = 57.168000000000006%, Loss = 0.9794971346855164
Epoch: 3620, Batch Gradient Norm: 35.34553137427117
Epoch: 3620, Batch Gradient Norm after: 22.360678527367533
Epoch 3621/10000, Prediction Accuracy = 57.132000000000005%, Loss = 0.9684963345527648
Epoch: 3621, Batch Gradient Norm: 39.49794537529004
Epoch: 3621, Batch Gradient Norm after: 22.360676173765548
Epoch 3622/10000, Prediction Accuracy = 57.174%, Loss = 0.9792851448059082
Epoch: 3622, Batch Gradient Norm: 35.33739398187936
Epoch: 3622, Batch Gradient Norm after: 22.36067849545053
Epoch 3623/10000, Prediction Accuracy = 57.124%, Loss = 0.9682728767395019
Epoch: 3623, Batch Gradient Norm: 39.49631005916428
Epoch: 3623, Batch Gradient Norm after: 22.360675738465186
Epoch 3624/10000, Prediction Accuracy = 57.186%, Loss = 0.979077672958374
Epoch: 3624, Batch Gradient Norm: 35.32930442475436
Epoch: 3624, Batch Gradient Norm after: 22.360678018422448
Epoch 3625/10000, Prediction Accuracy = 57.13199999999999%, Loss = 0.9680474877357483
Epoch: 3625, Batch Gradient Norm: 39.50156279887369
Epoch: 3625, Batch Gradient Norm after: 22.360678971984427
Epoch 3626/10000, Prediction Accuracy = 57.18000000000001%, Loss = 0.978878653049469
Epoch: 3626, Batch Gradient Norm: 35.319875157907866
Epoch: 3626, Batch Gradient Norm after: 22.360675608814855
Epoch 3627/10000, Prediction Accuracy = 57.13000000000001%, Loss = 0.9678173422813415
Epoch: 3627, Batch Gradient Norm: 39.50494389535223
Epoch: 3627, Batch Gradient Norm after: 22.360677457699584
Epoch 3628/10000, Prediction Accuracy = 57.184000000000005%, Loss = 0.9786785960197448
Epoch: 3628, Batch Gradient Norm: 35.312825621002986
Epoch: 3628, Batch Gradient Norm after: 22.36067799393641
Epoch 3629/10000, Prediction Accuracy = 57.128%, Loss = 0.9675923943519592
Epoch: 3629, Batch Gradient Norm: 39.505966359458675
Epoch: 3629, Batch Gradient Norm after: 22.360677112931313
Epoch 3630/10000, Prediction Accuracy = 57.184000000000005%, Loss = 0.9784688949584961
Epoch: 3630, Batch Gradient Norm: 35.30568758151595
Epoch: 3630, Batch Gradient Norm after: 22.36068052198326
Epoch 3631/10000, Prediction Accuracy = 57.128%, Loss = 0.9673681020736694
Epoch: 3631, Batch Gradient Norm: 39.503895630537414
Epoch: 3631, Batch Gradient Norm after: 22.360679015100605
Epoch 3632/10000, Prediction Accuracy = 57.176%, Loss = 0.9782610177993775
Epoch: 3632, Batch Gradient Norm: 35.29886588696807
Epoch: 3632, Batch Gradient Norm after: 22.360679155732072
Epoch 3633/10000, Prediction Accuracy = 57.126%, Loss = 0.9671429634094239
Epoch: 3633, Batch Gradient Norm: 39.500798222260315
Epoch: 3633, Batch Gradient Norm after: 22.3606778019528
Epoch 3634/10000, Prediction Accuracy = 57.184000000000005%, Loss = 0.9780564188957215
Epoch: 3634, Batch Gradient Norm: 35.29094894828692
Epoch: 3634, Batch Gradient Norm after: 22.36067755645681
Epoch 3635/10000, Prediction Accuracy = 57.134%, Loss = 0.9669139146804809
Epoch: 3635, Batch Gradient Norm: 39.49952411820197
Epoch: 3635, Batch Gradient Norm after: 22.360677816007456
Epoch 3636/10000, Prediction Accuracy = 57.194%, Loss = 0.9778515696525574
Epoch: 3636, Batch Gradient Norm: 35.28365961833199
Epoch: 3636, Batch Gradient Norm after: 22.360676344028388
Epoch 3637/10000, Prediction Accuracy = 57.124%, Loss = 0.9666889429092407
Epoch: 3637, Batch Gradient Norm: 39.493282149556585
Epoch: 3637, Batch Gradient Norm after: 22.36067787826105
Epoch 3638/10000, Prediction Accuracy = 57.19199999999999%, Loss = 0.977627956867218
Epoch: 3638, Batch Gradient Norm: 35.27868046960504
Epoch: 3638, Batch Gradient Norm after: 22.36067597079104
Epoch 3639/10000, Prediction Accuracy = 57.134%, Loss = 0.9664611339569091
Epoch: 3639, Batch Gradient Norm: 39.481766597367944
Epoch: 3639, Batch Gradient Norm after: 22.36067765514494
Epoch 3640/10000, Prediction Accuracy = 57.205999999999996%, Loss = 0.9773890256881714
Epoch: 3640, Batch Gradient Norm: 35.275030855504646
Epoch: 3640, Batch Gradient Norm after: 22.360677922956075
Epoch 3641/10000, Prediction Accuracy = 57.138%, Loss = 0.9662416577339172
Epoch: 3641, Batch Gradient Norm: 39.466355245231874
Epoch: 3641, Batch Gradient Norm after: 22.360676437646685
Epoch 3642/10000, Prediction Accuracy = 57.214%, Loss = 0.9771436095237732
Epoch: 3642, Batch Gradient Norm: 35.27180759533764
Epoch: 3642, Batch Gradient Norm after: 22.360677990953466
Epoch 3643/10000, Prediction Accuracy = 57.13000000000001%, Loss = 0.966020929813385
Epoch: 3643, Batch Gradient Norm: 39.45270747896203
Epoch: 3643, Batch Gradient Norm after: 22.360679207428696
Epoch 3644/10000, Prediction Accuracy = 57.227999999999994%, Loss = 0.9768988966941834
Epoch: 3644, Batch Gradient Norm: 35.26744223538231
Epoch: 3644, Batch Gradient Norm after: 22.36067888281952
Epoch 3645/10000, Prediction Accuracy = 57.124%, Loss = 0.9658054709434509
Epoch: 3645, Batch Gradient Norm: 39.43841136515959
Epoch: 3645, Batch Gradient Norm after: 22.360677926090034
Epoch 3646/10000, Prediction Accuracy = 57.23%, Loss = 0.9766525745391845
Epoch: 3646, Batch Gradient Norm: 35.26336984947291
Epoch: 3646, Batch Gradient Norm after: 22.360676664243424
Epoch 3647/10000, Prediction Accuracy = 57.138%, Loss = 0.9655887246131897
Epoch: 3647, Batch Gradient Norm: 39.424072891234104
Epoch: 3647, Batch Gradient Norm after: 22.360677180668116
Epoch 3648/10000, Prediction Accuracy = 57.24399999999999%, Loss = 0.9763980627059936
Epoch: 3648, Batch Gradient Norm: 35.258801890469144
Epoch: 3648, Batch Gradient Norm after: 22.36067963629165
Epoch 3649/10000, Prediction Accuracy = 57.15200000000001%, Loss = 0.9653727650642395
Epoch: 3649, Batch Gradient Norm: 39.40825471880579
Epoch: 3649, Batch Gradient Norm after: 22.360678272705414
Epoch 3650/10000, Prediction Accuracy = 57.25999999999999%, Loss = 0.976146137714386
Epoch: 3650, Batch Gradient Norm: 35.25402726825433
Epoch: 3650, Batch Gradient Norm after: 22.360677002630677
Epoch 3651/10000, Prediction Accuracy = 57.168000000000006%, Loss = 0.9651617050170899
Epoch: 3651, Batch Gradient Norm: 39.39195820204386
Epoch: 3651, Batch Gradient Norm after: 22.360677350377582
Epoch 3652/10000, Prediction Accuracy = 57.262%, Loss = 0.9759005546569824
Epoch: 3652, Batch Gradient Norm: 35.2495654389892
Epoch: 3652, Batch Gradient Norm after: 22.360676909878325
Epoch 3653/10000, Prediction Accuracy = 57.174%, Loss = 0.9649451851844788
Epoch: 3653, Batch Gradient Norm: 39.379307926268964
Epoch: 3653, Batch Gradient Norm after: 22.360675673531397
Epoch 3654/10000, Prediction Accuracy = 57.262%, Loss = 0.9756579875946045
Epoch: 3654, Batch Gradient Norm: 35.24537801620252
Epoch: 3654, Batch Gradient Norm after: 22.360674937293656
Epoch 3655/10000, Prediction Accuracy = 57.17%, Loss = 0.9647341132164001
Epoch: 3655, Batch Gradient Norm: 39.364317199817016
Epoch: 3655, Batch Gradient Norm after: 22.360678309496794
Epoch 3656/10000, Prediction Accuracy = 57.27%, Loss = 0.9754136085510254
Epoch: 3656, Batch Gradient Norm: 35.24046643734055
Epoch: 3656, Batch Gradient Norm after: 22.360677459905897
Epoch 3657/10000, Prediction Accuracy = 57.16799999999999%, Loss = 0.9645201444625855
Epoch: 3657, Batch Gradient Norm: 39.349857323534245
Epoch: 3657, Batch Gradient Norm after: 22.360678514603524
Epoch 3658/10000, Prediction Accuracy = 57.279999999999994%, Loss = 0.9751715302467346
Epoch: 3658, Batch Gradient Norm: 35.23668415906336
Epoch: 3658, Batch Gradient Norm after: 22.3606767350289
Epoch 3659/10000, Prediction Accuracy = 57.178%, Loss = 0.9643048167228698
Epoch: 3659, Batch Gradient Norm: 39.332920624834415
Epoch: 3659, Batch Gradient Norm after: 22.360677746637126
Epoch 3660/10000, Prediction Accuracy = 57.288%, Loss = 0.9749259948730469
Epoch: 3660, Batch Gradient Norm: 35.23067775564683
Epoch: 3660, Batch Gradient Norm after: 22.360677829814065
Epoch 3661/10000, Prediction Accuracy = 57.196000000000005%, Loss = 0.9640894174575806
Epoch: 3661, Batch Gradient Norm: 39.322923330129704
Epoch: 3661, Batch Gradient Norm after: 22.360679247246882
Epoch 3662/10000, Prediction Accuracy = 57.291999999999994%, Loss = 0.9746968388557434
Epoch: 3662, Batch Gradient Norm: 35.22423431133493
Epoch: 3662, Batch Gradient Norm after: 22.360675676541106
Epoch 3663/10000, Prediction Accuracy = 57.202%, Loss = 0.9638745903968811
Epoch: 3663, Batch Gradient Norm: 39.309574152241176
Epoch: 3663, Batch Gradient Norm after: 22.360677552292188
Epoch 3664/10000, Prediction Accuracy = 57.294000000000004%, Loss = 0.9744650602340699
Epoch: 3664, Batch Gradient Norm: 35.21827237639965
Epoch: 3664, Batch Gradient Norm after: 22.360676734638353
Epoch 3665/10000, Prediction Accuracy = 57.21%, Loss = 0.9636587142944336
Epoch: 3665, Batch Gradient Norm: 39.299010461690166
Epoch: 3665, Batch Gradient Norm after: 22.360678826369394
Epoch 3666/10000, Prediction Accuracy = 57.3%, Loss = 0.9742399454116821
Epoch: 3666, Batch Gradient Norm: 35.211562225416486
Epoch: 3666, Batch Gradient Norm after: 22.36067814812593
Epoch 3667/10000, Prediction Accuracy = 57.214%, Loss = 0.9634395360946655
Epoch: 3667, Batch Gradient Norm: 39.28825484338447
Epoch: 3667, Batch Gradient Norm after: 22.3606785441058
Epoch 3668/10000, Prediction Accuracy = 57.314%, Loss = 0.9740145087242127
Epoch: 3668, Batch Gradient Norm: 35.206223146077015
Epoch: 3668, Batch Gradient Norm after: 22.360677421163167
Epoch 3669/10000, Prediction Accuracy = 57.21600000000001%, Loss = 0.9632177829742432
Epoch: 3669, Batch Gradient Norm: 39.278189666041214
Epoch: 3669, Batch Gradient Norm after: 22.360677536623005
Epoch 3670/10000, Prediction Accuracy = 57.322%, Loss = 0.9737828373908997
Epoch: 3670, Batch Gradient Norm: 35.20039227641359
Epoch: 3670, Batch Gradient Norm after: 22.36067881883895
Epoch 3671/10000, Prediction Accuracy = 57.227999999999994%, Loss = 0.9630013227462768
Epoch: 3671, Batch Gradient Norm: 39.26376691559403
Epoch: 3671, Batch Gradient Norm after: 22.360677504934014
Epoch 3672/10000, Prediction Accuracy = 57.326%, Loss = 0.9735435366630554
Epoch: 3672, Batch Gradient Norm: 35.195346262080314
Epoch: 3672, Batch Gradient Norm after: 22.36067824671
Epoch 3673/10000, Prediction Accuracy = 57.23%, Loss = 0.9627842307090759
Epoch: 3673, Batch Gradient Norm: 39.25444812052477
Epoch: 3673, Batch Gradient Norm after: 22.360678653746433
Epoch 3674/10000, Prediction Accuracy = 57.326%, Loss = 0.9733041405677796
Epoch: 3674, Batch Gradient Norm: 35.18873625030968
Epoch: 3674, Batch Gradient Norm after: 22.360677471966866
Epoch 3675/10000, Prediction Accuracy = 57.236000000000004%, Loss = 0.9625706911087036
Epoch: 3675, Batch Gradient Norm: 39.24224909327279
Epoch: 3675, Batch Gradient Norm after: 22.36067633766567
Epoch 3676/10000, Prediction Accuracy = 57.327999999999996%, Loss = 0.9730727195739746
Epoch: 3676, Batch Gradient Norm: 35.18222943916314
Epoch: 3676, Batch Gradient Norm after: 22.360675666634425
Epoch 3677/10000, Prediction Accuracy = 57.246%, Loss = 0.9623573899269104
Epoch: 3677, Batch Gradient Norm: 39.22931009987224
Epoch: 3677, Batch Gradient Norm after: 22.360678847894174
Epoch 3678/10000, Prediction Accuracy = 57.334%, Loss = 0.972834312915802
Epoch: 3678, Batch Gradient Norm: 35.17601932255386
Epoch: 3678, Batch Gradient Norm after: 22.360677071634406
Epoch 3679/10000, Prediction Accuracy = 57.248000000000005%, Loss = 0.9621488571166992
Epoch: 3679, Batch Gradient Norm: 39.21752811626239
Epoch: 3679, Batch Gradient Norm after: 22.360677101452808
Epoch 3680/10000, Prediction Accuracy = 57.336%, Loss = 0.9726047992706299
Epoch: 3680, Batch Gradient Norm: 35.16914778191217
Epoch: 3680, Batch Gradient Norm after: 22.360677026790615
Epoch 3681/10000, Prediction Accuracy = 57.25%, Loss = 0.9619334101676941
Epoch: 3681, Batch Gradient Norm: 39.20569669131747
Epoch: 3681, Batch Gradient Norm after: 22.36067778735093
Epoch 3682/10000, Prediction Accuracy = 57.336%, Loss = 0.9723715901374816
Epoch: 3682, Batch Gradient Norm: 35.16052633381826
Epoch: 3682, Batch Gradient Norm after: 22.360679354358197
Epoch 3683/10000, Prediction Accuracy = 57.248000000000005%, Loss = 0.9617222785949707
Epoch: 3683, Batch Gradient Norm: 39.19556794808043
Epoch: 3683, Batch Gradient Norm after: 22.36067752594011
Epoch 3684/10000, Prediction Accuracy = 57.34999999999999%, Loss = 0.9721410155296326
Epoch: 3684, Batch Gradient Norm: 35.15513354694203
Epoch: 3684, Batch Gradient Norm after: 22.360679000194217
Epoch 3685/10000, Prediction Accuracy = 57.254%, Loss = 0.9615084409713746
Epoch: 3685, Batch Gradient Norm: 39.18242150575159
Epoch: 3685, Batch Gradient Norm after: 22.36067895442333
Epoch 3686/10000, Prediction Accuracy = 57.34599999999999%, Loss = 0.9719037532806396
Epoch: 3686, Batch Gradient Norm: 35.14868012956269
Epoch: 3686, Batch Gradient Norm after: 22.360678676357367
Epoch 3687/10000, Prediction Accuracy = 57.27%, Loss = 0.9612986207008362
Epoch: 3687, Batch Gradient Norm: 39.16993140590611
Epoch: 3687, Batch Gradient Norm after: 22.36067868689389
Epoch 3688/10000, Prediction Accuracy = 57.35%, Loss = 0.9716659426689148
Epoch: 3688, Batch Gradient Norm: 35.14356262150931
Epoch: 3688, Batch Gradient Norm after: 22.360677464302924
Epoch 3689/10000, Prediction Accuracy = 57.275999999999996%, Loss = 0.961084246635437
Epoch: 3689, Batch Gradient Norm: 39.16137575522925
Epoch: 3689, Batch Gradient Norm after: 22.360677495166524
Epoch 3690/10000, Prediction Accuracy = 57.346000000000004%, Loss = 0.9714383602142334
Epoch: 3690, Batch Gradient Norm: 35.13859801304708
Epoch: 3690, Batch Gradient Norm after: 22.360676852853967
Epoch 3691/10000, Prediction Accuracy = 57.282000000000004%, Loss = 0.9608710527420044
Epoch: 3691, Batch Gradient Norm: 39.15079492781074
Epoch: 3691, Batch Gradient Norm after: 22.360677543947162
Epoch 3692/10000, Prediction Accuracy = 57.36%, Loss = 0.9712129592895508
Epoch: 3692, Batch Gradient Norm: 35.13305009598358
Epoch: 3692, Batch Gradient Norm after: 22.36067592273845
Epoch 3693/10000, Prediction Accuracy = 57.288%, Loss = 0.9606544375419617
Epoch: 3693, Batch Gradient Norm: 39.14169788405497
Epoch: 3693, Batch Gradient Norm after: 22.36067919574318
Epoch 3694/10000, Prediction Accuracy = 57.352%, Loss = 0.9709938764572144
Epoch: 3694, Batch Gradient Norm: 35.127442244395645
Epoch: 3694, Batch Gradient Norm after: 22.360676148220794
Epoch 3695/10000, Prediction Accuracy = 57.294000000000004%, Loss = 0.9604390144348145
Epoch: 3695, Batch Gradient Norm: 39.134020534086766
Epoch: 3695, Batch Gradient Norm after: 22.360677777178
Epoch 3696/10000, Prediction Accuracy = 57.352%, Loss = 0.9707685112953186
Epoch: 3696, Batch Gradient Norm: 35.121362063107526
Epoch: 3696, Batch Gradient Norm after: 22.36067783914253
Epoch 3697/10000, Prediction Accuracy = 57.29600000000001%, Loss = 0.9602262377738953
Epoch: 3697, Batch Gradient Norm: 39.12655742709862
Epoch: 3697, Batch Gradient Norm after: 22.36067711832415
Epoch 3698/10000, Prediction Accuracy = 57.362%, Loss = 0.9705520868301392
Epoch: 3698, Batch Gradient Norm: 35.11453240562956
Epoch: 3698, Batch Gradient Norm after: 22.360679059414238
Epoch 3699/10000, Prediction Accuracy = 57.30800000000001%, Loss = 0.9600114941596984
Epoch: 3699, Batch Gradient Norm: 39.11811310046869
Epoch: 3699, Batch Gradient Norm after: 22.3606794769928
Epoch 3700/10000, Prediction Accuracy = 57.358000000000004%, Loss = 0.9703348398208618
Epoch: 3700, Batch Gradient Norm: 35.10720683543825
Epoch: 3700, Batch Gradient Norm after: 22.360678379437076
Epoch 3701/10000, Prediction Accuracy = 57.322%, Loss = 0.9597944378852844
Epoch: 3701, Batch Gradient Norm: 39.108640318233014
Epoch: 3701, Batch Gradient Norm after: 22.360676728395955
Epoch 3702/10000, Prediction Accuracy = 57.36%, Loss = 0.9701130032539368
Epoch: 3702, Batch Gradient Norm: 35.100217020502306
Epoch: 3702, Batch Gradient Norm after: 22.36067850024554
Epoch 3703/10000, Prediction Accuracy = 57.322%, Loss = 0.9595789313316345
Epoch: 3703, Batch Gradient Norm: 39.09914890627053
Epoch: 3703, Batch Gradient Norm after: 22.3606799114313
Epoch 3704/10000, Prediction Accuracy = 57.36800000000001%, Loss = 0.9698912262916565
Epoch: 3704, Batch Gradient Norm: 35.09499467567447
Epoch: 3704, Batch Gradient Norm after: 22.360678305818585
Epoch 3705/10000, Prediction Accuracy = 57.324%, Loss = 0.9593652844429016
Epoch: 3705, Batch Gradient Norm: 39.089779739945506
Epoch: 3705, Batch Gradient Norm after: 22.36067780693026
Epoch 3706/10000, Prediction Accuracy = 57.36800000000001%, Loss = 0.9696728110313415
Epoch: 3706, Batch Gradient Norm: 35.08871390326859
Epoch: 3706, Batch Gradient Norm after: 22.360677885258298
Epoch 3707/10000, Prediction Accuracy = 57.334%, Loss = 0.959152626991272
Epoch: 3707, Batch Gradient Norm: 39.07909659040968
Epoch: 3707, Batch Gradient Norm after: 22.36067966574483
Epoch 3708/10000, Prediction Accuracy = 57.367999999999995%, Loss = 0.9694473624229432
Epoch: 3708, Batch Gradient Norm: 35.082082656987865
Epoch: 3708, Batch Gradient Norm after: 22.360678530275237
Epoch 3709/10000, Prediction Accuracy = 57.339999999999996%, Loss = 0.9589456439018249
Epoch: 3709, Batch Gradient Norm: 39.0682439733293
Epoch: 3709, Batch Gradient Norm after: 22.36067830227955
Epoch 3710/10000, Prediction Accuracy = 57.36999999999999%, Loss = 0.9692256927490235
Epoch: 3710, Batch Gradient Norm: 35.07650070698656
Epoch: 3710, Batch Gradient Norm after: 22.360677635052827
Epoch 3711/10000, Prediction Accuracy = 57.327999999999996%, Loss = 0.9587342619895936
Epoch: 3711, Batch Gradient Norm: 39.05650611535417
Epoch: 3711, Batch Gradient Norm after: 22.3606783481014
Epoch 3712/10000, Prediction Accuracy = 57.376%, Loss = 0.968999445438385
Epoch: 3712, Batch Gradient Norm: 35.07081752723332
Epoch: 3712, Batch Gradient Norm after: 22.360675783672317
Epoch 3713/10000, Prediction Accuracy = 57.338%, Loss = 0.958518123626709
Epoch: 3713, Batch Gradient Norm: 39.048329482201815
Epoch: 3713, Batch Gradient Norm after: 22.360677826797446
Epoch 3714/10000, Prediction Accuracy = 57.379999999999995%, Loss = 0.9687779545783997
Epoch: 3714, Batch Gradient Norm: 35.06448987485569
Epoch: 3714, Batch Gradient Norm after: 22.360675818547865
Epoch 3715/10000, Prediction Accuracy = 57.342%, Loss = 0.9583060383796692
Epoch: 3715, Batch Gradient Norm: 39.03752151685237
Epoch: 3715, Batch Gradient Norm after: 22.360677619165926
Epoch 3716/10000, Prediction Accuracy = 57.386%, Loss = 0.9685604572296143
Epoch: 3716, Batch Gradient Norm: 35.058087018452014
Epoch: 3716, Batch Gradient Norm after: 22.360677555149472
Epoch 3717/10000, Prediction Accuracy = 57.34400000000001%, Loss = 0.958095121383667
Epoch: 3717, Batch Gradient Norm: 39.02882710576801
Epoch: 3717, Batch Gradient Norm after: 22.360678168846654
Epoch 3718/10000, Prediction Accuracy = 57.39399999999999%, Loss = 0.968344759941101
Epoch: 3718, Batch Gradient Norm: 35.053069629458086
Epoch: 3718, Batch Gradient Norm after: 22.360675095181943
Epoch 3719/10000, Prediction Accuracy = 57.346000000000004%, Loss = 0.9578837871551513
Epoch: 3719, Batch Gradient Norm: 39.02201448747399
Epoch: 3719, Batch Gradient Norm after: 22.360677823709874
Epoch 3720/10000, Prediction Accuracy = 57.406000000000006%, Loss = 0.9681246638298034
Epoch: 3720, Batch Gradient Norm: 35.04790557219004
Epoch: 3720, Batch Gradient Norm after: 22.36067853257163
Epoch 3721/10000, Prediction Accuracy = 57.354%, Loss = 0.9576725006103516
Epoch: 3721, Batch Gradient Norm: 39.01584387533879
Epoch: 3721, Batch Gradient Norm after: 22.36067952594852
Epoch 3722/10000, Prediction Accuracy = 57.406000000000006%, Loss = 0.9679123878479003
Epoch: 3722, Batch Gradient Norm: 35.040659549411544
Epoch: 3722, Batch Gradient Norm after: 22.360674891702725
Epoch 3723/10000, Prediction Accuracy = 57.37199999999999%, Loss = 0.9574612379074097
Epoch: 3723, Batch Gradient Norm: 39.011835313973634
Epoch: 3723, Batch Gradient Norm after: 22.360676396271934
Epoch 3724/10000, Prediction Accuracy = 57.414%, Loss = 0.9677130937576294
Epoch: 3724, Batch Gradient Norm: 35.02986067091149
Epoch: 3724, Batch Gradient Norm after: 22.360676037485657
Epoch 3725/10000, Prediction Accuracy = 57.370000000000005%, Loss = 0.9572474479675293
Epoch: 3725, Batch Gradient Norm: 39.01336690786877
Epoch: 3725, Batch Gradient Norm after: 22.360676170672036
Epoch 3726/10000, Prediction Accuracy = 57.41799999999999%, Loss = 0.967523205280304
Epoch: 3726, Batch Gradient Norm: 35.02208398142691
Epoch: 3726, Batch Gradient Norm after: 22.360677492765927
Epoch 3727/10000, Prediction Accuracy = 57.374%, Loss = 0.9570293188095093
Epoch: 3727, Batch Gradient Norm: 39.01185004004363
Epoch: 3727, Batch Gradient Norm after: 22.360678382077054
Epoch 3728/10000, Prediction Accuracy = 57.41799999999999%, Loss = 0.9673221468925476
Epoch: 3728, Batch Gradient Norm: 35.01415005060431
Epoch: 3728, Batch Gradient Norm after: 22.360678083685663
Epoch 3729/10000, Prediction Accuracy = 57.378%, Loss = 0.9568155527114868
Epoch: 3729, Batch Gradient Norm: 39.00580180746393
Epoch: 3729, Batch Gradient Norm after: 22.360676864695552
Epoch 3730/10000, Prediction Accuracy = 57.424%, Loss = 0.9671168804168702
Epoch: 3730, Batch Gradient Norm: 35.00666381076115
Epoch: 3730, Batch Gradient Norm after: 22.360676669192888
Epoch 3731/10000, Prediction Accuracy = 57.382000000000005%, Loss = 0.9566020250320435
Epoch: 3731, Batch Gradient Norm: 39.0008275937275
Epoch: 3731, Batch Gradient Norm after: 22.360678019265347
Epoch 3732/10000, Prediction Accuracy = 57.428%, Loss = 0.9669040083885193
Epoch: 3732, Batch Gradient Norm: 34.99945326043568
Epoch: 3732, Batch Gradient Norm after: 22.360676579098573
Epoch 3733/10000, Prediction Accuracy = 57.384%, Loss = 0.9563902258872986
Epoch: 3733, Batch Gradient Norm: 39.00020080439085
Epoch: 3733, Batch Gradient Norm after: 22.3606762861735
Epoch 3734/10000, Prediction Accuracy = 57.42%, Loss = 0.9667062401771546
Epoch: 3734, Batch Gradient Norm: 34.99141578992633
Epoch: 3734, Batch Gradient Norm after: 22.36067867916835
Epoch 3735/10000, Prediction Accuracy = 57.39000000000001%, Loss = 0.9561770558357239
Epoch: 3735, Batch Gradient Norm: 38.99848865695189
Epoch: 3735, Batch Gradient Norm after: 22.360676870206724
Epoch 3736/10000, Prediction Accuracy = 57.422000000000004%, Loss = 0.9665094137191772
Epoch: 3736, Batch Gradient Norm: 34.98498962965895
Epoch: 3736, Batch Gradient Norm after: 22.360677070416443
Epoch 3737/10000, Prediction Accuracy = 57.398%, Loss = 0.955959963798523
Epoch: 3737, Batch Gradient Norm: 38.997770331487004
Epoch: 3737, Batch Gradient Norm after: 22.360676476119586
Epoch 3738/10000, Prediction Accuracy = 57.424%, Loss = 0.9663145542144775
Epoch: 3738, Batch Gradient Norm: 34.977376596794514
Epoch: 3738, Batch Gradient Norm after: 22.36067822444527
Epoch 3739/10000, Prediction Accuracy = 57.402%, Loss = 0.9557472586631774
Epoch: 3739, Batch Gradient Norm: 38.99912924109652
Epoch: 3739, Batch Gradient Norm after: 22.360676566424253
Epoch 3740/10000, Prediction Accuracy = 57.42999999999999%, Loss = 0.9661229968070983
Epoch: 3740, Batch Gradient Norm: 34.96865299482333
Epoch: 3740, Batch Gradient Norm after: 22.360679075894883
Epoch 3741/10000, Prediction Accuracy = 57.414%, Loss = 0.9555313110351562
Epoch: 3741, Batch Gradient Norm: 38.99665833171466
Epoch: 3741, Batch Gradient Norm after: 22.360676107444572
Epoch 3742/10000, Prediction Accuracy = 57.44000000000001%, Loss = 0.965925145149231
Epoch: 3742, Batch Gradient Norm: 34.961273125608244
Epoch: 3742, Batch Gradient Norm after: 22.360678661442144
Epoch 3743/10000, Prediction Accuracy = 57.422000000000004%, Loss = 0.9553186535835266
Epoch: 3743, Batch Gradient Norm: 38.99826734943496
Epoch: 3743, Batch Gradient Norm after: 22.36067548930911
Epoch 3744/10000, Prediction Accuracy = 57.436000000000014%, Loss = 0.9657313823699951
Epoch: 3744, Batch Gradient Norm: 34.95157622545409
Epoch: 3744, Batch Gradient Norm after: 22.360675977910788
Epoch 3745/10000, Prediction Accuracy = 57.416%, Loss = 0.9551027417182922
Epoch: 3745, Batch Gradient Norm: 38.998691402828555
Epoch: 3745, Batch Gradient Norm after: 22.36067505014491
Epoch 3746/10000, Prediction Accuracy = 57.434000000000005%, Loss = 0.9655440330505372
Epoch: 3746, Batch Gradient Norm: 34.94755838138358
Epoch: 3746, Batch Gradient Norm after: 22.3606752171577
Epoch 3747/10000, Prediction Accuracy = 57.424%, Loss = 0.9548898577690125
Epoch: 3747, Batch Gradient Norm: 38.998231996072356
Epoch: 3747, Batch Gradient Norm after: 22.360675236654703
Epoch 3748/10000, Prediction Accuracy = 57.436%, Loss = 0.9653530120849609
Epoch: 3748, Batch Gradient Norm: 34.938361774718544
Epoch: 3748, Batch Gradient Norm after: 22.36067709845135
Epoch 3749/10000, Prediction Accuracy = 57.424%, Loss = 0.9546739101409912
Epoch: 3749, Batch Gradient Norm: 39.001827546700255
Epoch: 3749, Batch Gradient Norm after: 22.360678113560986
Epoch 3750/10000, Prediction Accuracy = 57.443999999999996%, Loss = 0.9651654958724976
Epoch: 3750, Batch Gradient Norm: 34.92941629093957
Epoch: 3750, Batch Gradient Norm after: 22.360676831902033
Epoch 3751/10000, Prediction Accuracy = 57.432%, Loss = 0.9544585585594177
Epoch: 3751, Batch Gradient Norm: 38.99966874123176
Epoch: 3751, Batch Gradient Norm after: 22.360678627173158
Epoch 3752/10000, Prediction Accuracy = 57.44200000000001%, Loss = 0.9649604201316834
Epoch: 3752, Batch Gradient Norm: 34.92248177954662
Epoch: 3752, Batch Gradient Norm after: 22.36067647930825
Epoch 3753/10000, Prediction Accuracy = 57.44000000000001%, Loss = 0.9542461991310119
Epoch: 3753, Batch Gradient Norm: 38.99325208168917
Epoch: 3753, Batch Gradient Norm after: 22.360676488926387
Epoch 3754/10000, Prediction Accuracy = 57.42999999999999%, Loss = 0.9647530317306519
Epoch: 3754, Batch Gradient Norm: 34.9160396883925
Epoch: 3754, Batch Gradient Norm after: 22.360676566103972
Epoch 3755/10000, Prediction Accuracy = 57.443999999999996%, Loss = 0.9540320873260498
Epoch: 3755, Batch Gradient Norm: 38.98653027646347
Epoch: 3755, Batch Gradient Norm after: 22.360676865530664
Epoch 3756/10000, Prediction Accuracy = 57.436000000000014%, Loss = 0.9645416975021363
Epoch: 3756, Batch Gradient Norm: 34.91077976077654
Epoch: 3756, Batch Gradient Norm after: 22.360678124483155
Epoch 3757/10000, Prediction Accuracy = 57.448%, Loss = 0.9538240551948547
Epoch: 3757, Batch Gradient Norm: 38.98181895277254
Epoch: 3757, Batch Gradient Norm after: 22.3606759027192
Epoch 3758/10000, Prediction Accuracy = 57.438%, Loss = 0.9643367767333985
Epoch: 3758, Batch Gradient Norm: 34.90383305505104
Epoch: 3758, Batch Gradient Norm after: 22.360676174532504
Epoch 3759/10000, Prediction Accuracy = 57.44%, Loss = 0.9536078691482544
Epoch: 3759, Batch Gradient Norm: 38.9743770697511
Epoch: 3759, Batch Gradient Norm after: 22.360677884673102
Epoch 3760/10000, Prediction Accuracy = 57.438%, Loss = 0.9641286134719849
Epoch: 3760, Batch Gradient Norm: 34.89753054954342
Epoch: 3760, Batch Gradient Norm after: 22.36067662746506
Epoch 3761/10000, Prediction Accuracy = 57.436%, Loss = 0.9533986568450927
Epoch: 3761, Batch Gradient Norm: 38.971073944980645
Epoch: 3761, Batch Gradient Norm after: 22.360678512195474
Epoch 3762/10000, Prediction Accuracy = 57.44%, Loss = 0.9639278411865234
Epoch: 3762, Batch Gradient Norm: 34.890196376608365
Epoch: 3762, Batch Gradient Norm after: 22.36067669124959
Epoch 3763/10000, Prediction Accuracy = 57.42999999999999%, Loss = 0.9531900405883789
Epoch: 3763, Batch Gradient Norm: 38.964941298837594
Epoch: 3763, Batch Gradient Norm after: 22.360678093581505
Epoch 3764/10000, Prediction Accuracy = 57.45%, Loss = 0.9637208104133606
Epoch: 3764, Batch Gradient Norm: 34.88382175866576
Epoch: 3764, Batch Gradient Norm after: 22.360677249544505
Epoch 3765/10000, Prediction Accuracy = 57.426%, Loss = 0.9529764175415039
Epoch: 3765, Batch Gradient Norm: 38.962001089975416
Epoch: 3765, Batch Gradient Norm after: 22.360678565311087
Epoch 3766/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.9635238528251648
Epoch: 3766, Batch Gradient Norm: 34.87479215751491
Epoch: 3766, Batch Gradient Norm after: 22.360674732712745
Epoch 3767/10000, Prediction Accuracy = 57.431999999999995%, Loss = 0.9527624130249024
Epoch: 3767, Batch Gradient Norm: 38.961827407976166
Epoch: 3767, Batch Gradient Norm after: 22.360677954905388
Epoch 3768/10000, Prediction Accuracy = 57.474000000000004%, Loss = 0.9633334040641784
Epoch: 3768, Batch Gradient Norm: 34.864479242875696
Epoch: 3768, Batch Gradient Norm after: 22.360675688327074
Epoch 3769/10000, Prediction Accuracy = 57.443999999999996%, Loss = 0.9525508165359498
Epoch: 3769, Batch Gradient Norm: 38.96212473736859
Epoch: 3769, Batch Gradient Norm after: 22.360677563621532
Epoch 3770/10000, Prediction Accuracy = 57.462%, Loss = 0.9631446838378906
Epoch: 3770, Batch Gradient Norm: 34.85740711991072
Epoch: 3770, Batch Gradient Norm after: 22.360677112282943
Epoch 3771/10000, Prediction Accuracy = 57.45%, Loss = 0.9523367166519165
Epoch: 3771, Batch Gradient Norm: 38.963100144722596
Epoch: 3771, Batch Gradient Norm after: 22.360679640130574
Epoch 3772/10000, Prediction Accuracy = 57.472%, Loss = 0.9629642009735108
Epoch: 3772, Batch Gradient Norm: 34.84904022466868
Epoch: 3772, Batch Gradient Norm after: 22.360677414166677
Epoch 3773/10000, Prediction Accuracy = 57.446000000000005%, Loss = 0.9521161437034606
Epoch: 3773, Batch Gradient Norm: 38.9631573999912
Epoch: 3773, Batch Gradient Norm after: 22.360679492543795
Epoch 3774/10000, Prediction Accuracy = 57.468%, Loss = 0.9627684116363525
Epoch: 3774, Batch Gradient Norm: 34.84172406803221
Epoch: 3774, Batch Gradient Norm after: 22.360678833116808
Epoch 3775/10000, Prediction Accuracy = 57.45200000000001%, Loss = 0.951904046535492
Epoch: 3775, Batch Gradient Norm: 38.960185350192305
Epoch: 3775, Batch Gradient Norm after: 22.36068007513682
Epoch 3776/10000, Prediction Accuracy = 57.464%, Loss = 0.9625669836997985
Epoch: 3776, Batch Gradient Norm: 34.832464904766375
Epoch: 3776, Batch Gradient Norm after: 22.36067778597439
Epoch 3777/10000, Prediction Accuracy = 57.455999999999996%, Loss = 0.9516960978507996
Epoch: 3777, Batch Gradient Norm: 38.96181449212046
Epoch: 3777, Batch Gradient Norm after: 22.360678742677077
Epoch 3778/10000, Prediction Accuracy = 57.46%, Loss = 0.9623782634735107
Epoch: 3778, Batch Gradient Norm: 34.82346934063536
Epoch: 3778, Batch Gradient Norm after: 22.36067926326837
Epoch 3779/10000, Prediction Accuracy = 57.458000000000006%, Loss = 0.9514821171760559
Epoch: 3779, Batch Gradient Norm: 38.96545658953868
Epoch: 3779, Batch Gradient Norm after: 22.36067841511679
Epoch 3780/10000, Prediction Accuracy = 57.46600000000001%, Loss = 0.9621941804885864
Epoch: 3780, Batch Gradient Norm: 34.81603061943405
Epoch: 3780, Batch Gradient Norm after: 22.36067560725131
Epoch 3781/10000, Prediction Accuracy = 57.455999999999996%, Loss = 0.9512667894363404
Epoch: 3781, Batch Gradient Norm: 38.96937691106239
Epoch: 3781, Batch Gradient Norm after: 22.360677940841285
Epoch 3782/10000, Prediction Accuracy = 57.474000000000004%, Loss = 0.9620182514190674
Epoch: 3782, Batch Gradient Norm: 34.8063506867456
Epoch: 3782, Batch Gradient Norm after: 22.36067560370039
Epoch 3783/10000, Prediction Accuracy = 57.464%, Loss = 0.95105140209198
Epoch: 3783, Batch Gradient Norm: 38.96632135487464
Epoch: 3783, Batch Gradient Norm after: 22.360678031066566
Epoch 3784/10000, Prediction Accuracy = 57.467999999999996%, Loss = 0.9618224382400513
Epoch: 3784, Batch Gradient Norm: 34.79858399945773
Epoch: 3784, Batch Gradient Norm after: 22.360675491690348
Epoch 3785/10000, Prediction Accuracy = 57.467999999999996%, Loss = 0.9508429408073426
Epoch: 3785, Batch Gradient Norm: 38.960836798684916
Epoch: 3785, Batch Gradient Norm after: 22.360676800708777
Epoch 3786/10000, Prediction Accuracy = 57.464%, Loss = 0.9616158127784729
Epoch: 3786, Batch Gradient Norm: 34.79222172940915
Epoch: 3786, Batch Gradient Norm after: 22.36067870522129
Epoch 3787/10000, Prediction Accuracy = 57.474000000000004%, Loss = 0.9506313920021057
Epoch: 3787, Batch Gradient Norm: 38.95458697100777
Epoch: 3787, Batch Gradient Norm after: 22.360680226884515
Epoch 3788/10000, Prediction Accuracy = 57.470000000000006%, Loss = 0.9614092826843261
Epoch: 3788, Batch Gradient Norm: 34.784860780240386
Epoch: 3788, Batch Gradient Norm after: 22.36067817863651
Epoch 3789/10000, Prediction Accuracy = 57.486000000000004%, Loss = 0.950420868396759
Epoch: 3789, Batch Gradient Norm: 38.95022987350929
Epoch: 3789, Batch Gradient Norm after: 22.36067850313702
Epoch 3790/10000, Prediction Accuracy = 57.48%, Loss = 0.9612013816833496
Epoch: 3790, Batch Gradient Norm: 34.780786761511074
Epoch: 3790, Batch Gradient Norm after: 22.360677358349882
Epoch 3791/10000, Prediction Accuracy = 57.496%, Loss = 0.9502081274986267
Epoch: 3791, Batch Gradient Norm: 38.9470082482909
Epoch: 3791, Batch Gradient Norm after: 22.360681047937785
Epoch 3792/10000, Prediction Accuracy = 57.483999999999995%, Loss = 0.9609942674636841
Epoch: 3792, Batch Gradient Norm: 34.77448661914902
Epoch: 3792, Batch Gradient Norm after: 22.360677724016185
Epoch 3793/10000, Prediction Accuracy = 57.508%, Loss = 0.9500033497810364
Epoch: 3793, Batch Gradient Norm: 38.93827464685614
Epoch: 3793, Batch Gradient Norm after: 22.36067786272516
Epoch 3794/10000, Prediction Accuracy = 57.492000000000004%, Loss = 0.9607831120491028
Epoch: 3794, Batch Gradient Norm: 34.77120683957271
Epoch: 3794, Batch Gradient Norm after: 22.360674740350028
Epoch 3795/10000, Prediction Accuracy = 57.50600000000001%, Loss = 0.9497981548309327
Epoch: 3795, Batch Gradient Norm: 38.92944762839413
Epoch: 3795, Batch Gradient Norm after: 22.36067732542451
Epoch 3796/10000, Prediction Accuracy = 57.5%, Loss = 0.9605656623840332
Epoch: 3796, Batch Gradient Norm: 34.765951046260625
Epoch: 3796, Batch Gradient Norm after: 22.360677425850138
Epoch 3797/10000, Prediction Accuracy = 57.513999999999996%, Loss = 0.9495928049087524
Epoch: 3797, Batch Gradient Norm: 38.917147703877404
Epoch: 3797, Batch Gradient Norm after: 22.360679317333187
Epoch 3798/10000, Prediction Accuracy = 57.504%, Loss = 0.960342288017273
Epoch: 3798, Batch Gradient Norm: 34.75951284629519
Epoch: 3798, Batch Gradient Norm after: 22.360678618866793
Epoch 3799/10000, Prediction Accuracy = 57.516000000000005%, Loss = 0.9493896842002869
Epoch: 3799, Batch Gradient Norm: 38.90613070288421
Epoch: 3799, Batch Gradient Norm after: 22.360679865589397
Epoch 3800/10000, Prediction Accuracy = 57.504%, Loss = 0.9601163387298584
Epoch: 3800, Batch Gradient Norm: 34.75408644232301
Epoch: 3800, Batch Gradient Norm after: 22.360679191281616
Epoch 3801/10000, Prediction Accuracy = 57.516%, Loss = 0.9491863965988159
Epoch: 3801, Batch Gradient Norm: 38.89741197455201
Epoch: 3801, Batch Gradient Norm after: 22.360677250998787
Epoch 3802/10000, Prediction Accuracy = 57.504000000000005%, Loss = 0.9599053025245666
Epoch: 3802, Batch Gradient Norm: 34.74661094620355
Epoch: 3802, Batch Gradient Norm after: 22.360677001575286
Epoch 3803/10000, Prediction Accuracy = 57.516%, Loss = 0.948983347415924
Epoch: 3803, Batch Gradient Norm: 38.8929660473793
Epoch: 3803, Batch Gradient Norm after: 22.360677508291808
Epoch 3804/10000, Prediction Accuracy = 57.504%, Loss = 0.959700071811676
Epoch: 3804, Batch Gradient Norm: 34.73822533298925
Epoch: 3804, Batch Gradient Norm after: 22.36067511167486
Epoch 3805/10000, Prediction Accuracy = 57.512%, Loss = 0.948778235912323
Epoch: 3805, Batch Gradient Norm: 38.89158048628023
Epoch: 3805, Batch Gradient Norm after: 22.36067911697175
Epoch 3806/10000, Prediction Accuracy = 57.504%, Loss = 0.959510886669159
Epoch: 3806, Batch Gradient Norm: 34.73211919262633
Epoch: 3806, Batch Gradient Norm after: 22.36067736419561
Epoch 3807/10000, Prediction Accuracy = 57.510000000000005%, Loss = 0.9485649704933167
Epoch: 3807, Batch Gradient Norm: 38.885233103834565
Epoch: 3807, Batch Gradient Norm after: 22.36067730804458
Epoch 3808/10000, Prediction Accuracy = 57.512%, Loss = 0.9593076944351197
Epoch: 3808, Batch Gradient Norm: 34.7258566232789
Epoch: 3808, Batch Gradient Norm after: 22.360676223103635
Epoch 3809/10000, Prediction Accuracy = 57.504%, Loss = 0.9483574628829956
Epoch: 3809, Batch Gradient Norm: 38.88174194229157
Epoch: 3809, Batch Gradient Norm after: 22.36067831948921
Epoch 3810/10000, Prediction Accuracy = 57.51800000000001%, Loss = 0.9591040015220642
Epoch: 3810, Batch Gradient Norm: 34.72019967544307
Epoch: 3810, Batch Gradient Norm after: 22.36067716626023
Epoch 3811/10000, Prediction Accuracy = 57.501999999999995%, Loss = 0.9481496810913086
Epoch: 3811, Batch Gradient Norm: 38.87250815457751
Epoch: 3811, Batch Gradient Norm after: 22.36067839721231
Epoch 3812/10000, Prediction Accuracy = 57.528%, Loss = 0.958895492553711
Epoch: 3812, Batch Gradient Norm: 34.71471226476624
Epoch: 3812, Batch Gradient Norm after: 22.36067965849561
Epoch 3813/10000, Prediction Accuracy = 57.508%, Loss = 0.9479452133178711
Epoch: 3813, Batch Gradient Norm: 38.86028198161067
Epoch: 3813, Batch Gradient Norm after: 22.360676341605252
Epoch 3814/10000, Prediction Accuracy = 57.54%, Loss = 0.9586694955825805
Epoch: 3814, Batch Gradient Norm: 34.70999216421153
Epoch: 3814, Batch Gradient Norm after: 22.360679705986772
Epoch 3815/10000, Prediction Accuracy = 57.513999999999996%, Loss = 0.9477455735206604
Epoch: 3815, Batch Gradient Norm: 38.84400300299983
Epoch: 3815, Batch Gradient Norm after: 22.36067870482544
Epoch 3816/10000, Prediction Accuracy = 57.55%, Loss = 0.9584328532218933
Epoch: 3816, Batch Gradient Norm: 34.705868424990996
Epoch: 3816, Batch Gradient Norm after: 22.36067938925641
Epoch 3817/10000, Prediction Accuracy = 57.525999999999996%, Loss = 0.9475443482398986
Epoch: 3817, Batch Gradient Norm: 38.82592103571618
Epoch: 3817, Batch Gradient Norm after: 22.36067784801524
Epoch 3818/10000, Prediction Accuracy = 57.552%, Loss = 0.9581931114196778
Epoch: 3818, Batch Gradient Norm: 34.703089782797576
Epoch: 3818, Batch Gradient Norm after: 22.36067905742155
Epoch 3819/10000, Prediction Accuracy = 57.532%, Loss = 0.9473458409309388
Epoch: 3819, Batch Gradient Norm: 38.80647461490681
Epoch: 3819, Batch Gradient Norm after: 22.36067675197021
Epoch 3820/10000, Prediction Accuracy = 57.55800000000001%, Loss = 0.9579533934593201
Epoch: 3820, Batch Gradient Norm: 34.69882705722843
Epoch: 3820, Batch Gradient Norm after: 22.360676558565363
Epoch 3821/10000, Prediction Accuracy = 57.54%, Loss = 0.9471472024917602
Epoch: 3821, Batch Gradient Norm: 38.791232225900096
Epoch: 3821, Batch Gradient Norm after: 22.360678281721224
Epoch 3822/10000, Prediction Accuracy = 57.556%, Loss = 0.9577209115028381
Epoch: 3822, Batch Gradient Norm: 34.69693300476859
Epoch: 3822, Batch Gradient Norm after: 22.360677566089294
Epoch 3823/10000, Prediction Accuracy = 57.538%, Loss = 0.9469450354576111
Epoch: 3823, Batch Gradient Norm: 38.77250694046014
Epoch: 3823, Batch Gradient Norm after: 22.36067708006076
Epoch 3824/10000, Prediction Accuracy = 57.556000000000004%, Loss = 0.9574819087982178
Epoch: 3824, Batch Gradient Norm: 34.69313912908735
Epoch: 3824, Batch Gradient Norm after: 22.360678263206648
Epoch 3825/10000, Prediction Accuracy = 57.544000000000004%, Loss = 0.9467459321022034
Epoch: 3825, Batch Gradient Norm: 38.75672566565431
Epoch: 3825, Batch Gradient Norm after: 22.36067694690757
Epoch 3826/10000, Prediction Accuracy = 57.56999999999999%, Loss = 0.9572523355484008
Epoch: 3826, Batch Gradient Norm: 34.68827775124662
Epoch: 3826, Batch Gradient Norm after: 22.360681021206698
Epoch 3827/10000, Prediction Accuracy = 57.54600000000001%, Loss = 0.9465450644493103
Epoch: 3827, Batch Gradient Norm: 38.742667137317156
Epoch: 3827, Batch Gradient Norm after: 22.36067494191676
Epoch 3828/10000, Prediction Accuracy = 57.58200000000001%, Loss = 0.957030200958252
Epoch: 3828, Batch Gradient Norm: 34.682826603419684
Epoch: 3828, Batch Gradient Norm after: 22.360678964417236
Epoch 3829/10000, Prediction Accuracy = 57.552%, Loss = 0.9463459610939026
Epoch: 3829, Batch Gradient Norm: 38.73186084133108
Epoch: 3829, Batch Gradient Norm after: 22.360674370853918
Epoch 3830/10000, Prediction Accuracy = 57.580000000000005%, Loss = 0.9568201899528503
Epoch: 3830, Batch Gradient Norm: 34.67490004532441
Epoch: 3830, Batch Gradient Norm after: 22.360677719927747
Epoch 3831/10000, Prediction Accuracy = 57.562%, Loss = 0.946148693561554
Epoch: 3831, Batch Gradient Norm: 38.72758708083062
Epoch: 3831, Batch Gradient Norm after: 22.360675089532524
Epoch 3832/10000, Prediction Accuracy = 57.576%, Loss = 0.9566123843193054
Epoch: 3832, Batch Gradient Norm: 34.66916664244718
Epoch: 3832, Batch Gradient Norm after: 22.360677246857406
Epoch 3833/10000, Prediction Accuracy = 57.564%, Loss = 0.945943820476532
Epoch: 3833, Batch Gradient Norm: 38.72226387091994
Epoch: 3833, Batch Gradient Norm after: 22.360674982479594
Epoch 3834/10000, Prediction Accuracy = 57.577999999999996%, Loss = 0.95641587972641
Epoch: 3834, Batch Gradient Norm: 34.66286815223306
Epoch: 3834, Batch Gradient Norm after: 22.36067611529155
Epoch 3835/10000, Prediction Accuracy = 57.568000000000005%, Loss = 0.9457408428192139
Epoch: 3835, Batch Gradient Norm: 38.716426376498895
Epoch: 3835, Batch Gradient Norm after: 22.36067806553115
Epoch 3836/10000, Prediction Accuracy = 57.576%, Loss = 0.9562168598175049
Epoch: 3836, Batch Gradient Norm: 34.65433607883216
Epoch: 3836, Batch Gradient Norm after: 22.36067758506902
Epoch 3837/10000, Prediction Accuracy = 57.574%, Loss = 0.9455398440361023
Epoch: 3837, Batch Gradient Norm: 38.70985351024129
Epoch: 3837, Batch Gradient Norm after: 22.360678661079287
Epoch 3838/10000, Prediction Accuracy = 57.572%, Loss = 0.9560133457183838
Epoch: 3838, Batch Gradient Norm: 34.64896903775484
Epoch: 3838, Batch Gradient Norm after: 22.360678998937495
Epoch 3839/10000, Prediction Accuracy = 57.581999999999994%, Loss = 0.9453324794769287
Epoch: 3839, Batch Gradient Norm: 38.700583054001505
Epoch: 3839, Batch Gradient Norm after: 22.36067874264484
Epoch 3840/10000, Prediction Accuracy = 57.58%, Loss = 0.9558048129081727
Epoch: 3840, Batch Gradient Norm: 34.64169534246528
Epoch: 3840, Batch Gradient Norm after: 22.36067839285887
Epoch 3841/10000, Prediction Accuracy = 57.584%, Loss = 0.9451310753822326
Epoch: 3841, Batch Gradient Norm: 38.696108735330846
Epoch: 3841, Batch Gradient Norm after: 22.360674695929408
Epoch 3842/10000, Prediction Accuracy = 57.588%, Loss = 0.955601966381073
Epoch: 3842, Batch Gradient Norm: 34.63505963529709
Epoch: 3842, Batch Gradient Norm after: 22.36067769984078
Epoch 3843/10000, Prediction Accuracy = 57.596000000000004%, Loss = 0.9449276924133301
Epoch: 3843, Batch Gradient Norm: 38.68864277266236
Epoch: 3843, Batch Gradient Norm after: 22.360678583931445
Epoch 3844/10000, Prediction Accuracy = 57.584%, Loss = 0.9553983449935913
Epoch: 3844, Batch Gradient Norm: 34.63041882879554
Epoch: 3844, Batch Gradient Norm after: 22.3606765033595
Epoch 3845/10000, Prediction Accuracy = 57.602%, Loss = 0.9447222471237182
Epoch: 3845, Batch Gradient Norm: 38.678221100188864
Epoch: 3845, Batch Gradient Norm after: 22.36068011621554
Epoch 3846/10000, Prediction Accuracy = 57.586%, Loss = 0.9551795840263366
Epoch: 3846, Batch Gradient Norm: 34.625599324918454
Epoch: 3846, Batch Gradient Norm after: 22.360675817190238
Epoch 3847/10000, Prediction Accuracy = 57.61%, Loss = 0.9445266842842102
Epoch: 3847, Batch Gradient Norm: 38.667249871182726
Epoch: 3847, Batch Gradient Norm after: 22.36067858664029
Epoch 3848/10000, Prediction Accuracy = 57.586%, Loss = 0.9549623370170593
Epoch: 3848, Batch Gradient Norm: 34.618642536463575
Epoch: 3848, Batch Gradient Norm after: 22.360677252598176
Epoch 3849/10000, Prediction Accuracy = 57.61600000000001%, Loss = 0.944328510761261
Epoch: 3849, Batch Gradient Norm: 38.660925767394474
Epoch: 3849, Batch Gradient Norm after: 22.360678950713133
Epoch 3850/10000, Prediction Accuracy = 57.596000000000004%, Loss = 0.9547567367553711
Epoch: 3850, Batch Gradient Norm: 34.61281484403495
Epoch: 3850, Batch Gradient Norm after: 22.360679243092793
Epoch 3851/10000, Prediction Accuracy = 57.622%, Loss = 0.9441277384757996
Epoch: 3851, Batch Gradient Norm: 38.650209738720534
Epoch: 3851, Batch Gradient Norm after: 22.360677048081698
Epoch 3852/10000, Prediction Accuracy = 57.61%, Loss = 0.9545427083969116
Epoch: 3852, Batch Gradient Norm: 34.60808131648878
Epoch: 3852, Batch Gradient Norm after: 22.360679297774986
Epoch 3853/10000, Prediction Accuracy = 57.636%, Loss = 0.943926465511322
Epoch: 3853, Batch Gradient Norm: 38.641219706529625
Epoch: 3853, Batch Gradient Norm after: 22.36067713585639
Epoch 3854/10000, Prediction Accuracy = 57.604%, Loss = 0.9543304085731507
Epoch: 3854, Batch Gradient Norm: 34.60264504633053
Epoch: 3854, Batch Gradient Norm after: 22.36067637998625
Epoch 3855/10000, Prediction Accuracy = 57.64%, Loss = 0.9437309384346009
Epoch: 3855, Batch Gradient Norm: 38.62732206445949
Epoch: 3855, Batch Gradient Norm after: 22.360677158079856
Epoch 3856/10000, Prediction Accuracy = 57.614%, Loss = 0.9541179656982421
Epoch: 3856, Batch Gradient Norm: 34.596619140948654
Epoch: 3856, Batch Gradient Norm after: 22.360676046193525
Epoch 3857/10000, Prediction Accuracy = 57.64%, Loss = 0.9435317158699036
Epoch: 3857, Batch Gradient Norm: 38.6168341695284
Epoch: 3857, Batch Gradient Norm after: 22.36067895252578
Epoch 3858/10000, Prediction Accuracy = 57.614%, Loss = 0.9539059400558472
Epoch: 3858, Batch Gradient Norm: 34.5915376951564
Epoch: 3858, Batch Gradient Norm after: 22.360679264696095
Epoch 3859/10000, Prediction Accuracy = 57.641999999999996%, Loss = 0.9433339476585388
Epoch: 3859, Batch Gradient Norm: 38.604327524689
Epoch: 3859, Batch Gradient Norm after: 22.36067870453941
Epoch 3860/10000, Prediction Accuracy = 57.628%, Loss = 0.9536871314048767
Epoch: 3860, Batch Gradient Norm: 34.58772544224469
Epoch: 3860, Batch Gradient Norm after: 22.360677868362494
Epoch 3861/10000, Prediction Accuracy = 57.641999999999996%, Loss = 0.9431376218795776
Epoch: 3861, Batch Gradient Norm: 38.58904735526261
Epoch: 3861, Batch Gradient Norm after: 22.360678192214056
Epoch 3862/10000, Prediction Accuracy = 57.624%, Loss = 0.953458595275879
Epoch: 3862, Batch Gradient Norm: 34.58403465088112
Epoch: 3862, Batch Gradient Norm after: 22.36067557260094
Epoch 3863/10000, Prediction Accuracy = 57.653999999999996%, Loss = 0.942943012714386
Epoch: 3863, Batch Gradient Norm: 38.572968497879096
Epoch: 3863, Batch Gradient Norm after: 22.36067780500507
Epoch 3864/10000, Prediction Accuracy = 57.629999999999995%, Loss = 0.953227984905243
Epoch: 3864, Batch Gradient Norm: 34.5818708395836
Epoch: 3864, Batch Gradient Norm after: 22.360677728185028
Epoch 3865/10000, Prediction Accuracy = 57.65599999999999%, Loss = 0.9427473664283752
Epoch: 3865, Batch Gradient Norm: 38.55452876743284
Epoch: 3865, Batch Gradient Norm after: 22.36067824320621
Epoch 3866/10000, Prediction Accuracy = 57.632000000000005%, Loss = 0.9529930233955384
Epoch: 3866, Batch Gradient Norm: 34.58039574720996
Epoch: 3866, Batch Gradient Norm after: 22.360677096298648
Epoch 3867/10000, Prediction Accuracy = 57.67199999999999%, Loss = 0.942553174495697
Epoch: 3867, Batch Gradient Norm: 38.53725956243146
Epoch: 3867, Batch Gradient Norm after: 22.360677008260453
Epoch 3868/10000, Prediction Accuracy = 57.629999999999995%, Loss = 0.9527591705322266
Epoch: 3868, Batch Gradient Norm: 34.577370674507726
Epoch: 3868, Batch Gradient Norm after: 22.360677382509028
Epoch 3869/10000, Prediction Accuracy = 57.674%, Loss = 0.9423624277114868
Epoch: 3869, Batch Gradient Norm: 38.51817718408754
Epoch: 3869, Batch Gradient Norm after: 22.3606781289917
Epoch 3870/10000, Prediction Accuracy = 57.634%, Loss = 0.9525262236595153
Epoch: 3870, Batch Gradient Norm: 34.57301220536237
Epoch: 3870, Batch Gradient Norm after: 22.360677605844568
Epoch 3871/10000, Prediction Accuracy = 57.678%, Loss = 0.9421703934669494
Epoch: 3871, Batch Gradient Norm: 38.503424103903335
Epoch: 3871, Batch Gradient Norm after: 22.360678834710118
Epoch 3872/10000, Prediction Accuracy = 57.629999999999995%, Loss = 0.9523008346557618
Epoch: 3872, Batch Gradient Norm: 34.571112108576436
Epoch: 3872, Batch Gradient Norm after: 22.36067758703524
Epoch 3873/10000, Prediction Accuracy = 57.686%, Loss = 0.9419765114784241
Epoch: 3873, Batch Gradient Norm: 38.484501414780254
Epoch: 3873, Batch Gradient Norm after: 22.36068009502405
Epoch 3874/10000, Prediction Accuracy = 57.636%, Loss = 0.952067518234253
Epoch: 3874, Batch Gradient Norm: 34.5683502681507
Epoch: 3874, Batch Gradient Norm after: 22.36067710436366
Epoch 3875/10000, Prediction Accuracy = 57.68399999999999%, Loss = 0.9417841553688049
Epoch: 3875, Batch Gradient Norm: 38.46701573818979
Epoch: 3875, Batch Gradient Norm after: 22.36067846991285
Epoch 3876/10000, Prediction Accuracy = 57.644000000000005%, Loss = 0.9518315434455872
Epoch: 3876, Batch Gradient Norm: 34.56356219263908
Epoch: 3876, Batch Gradient Norm after: 22.360676896959614
Epoch 3877/10000, Prediction Accuracy = 57.686%, Loss = 0.9415939927101136
Epoch: 3877, Batch Gradient Norm: 38.453462676349034
Epoch: 3877, Batch Gradient Norm after: 22.360676678372236
Epoch 3878/10000, Prediction Accuracy = 57.65%, Loss = 0.9516021490097046
Epoch: 3878, Batch Gradient Norm: 34.55903886048293
Epoch: 3878, Batch Gradient Norm after: 22.3606768033221
Epoch 3879/10000, Prediction Accuracy = 57.69%, Loss = 0.9414024710655212
Epoch: 3879, Batch Gradient Norm: 38.43841654870129
Epoch: 3879, Batch Gradient Norm after: 22.360680169930266
Epoch 3880/10000, Prediction Accuracy = 57.644000000000005%, Loss = 0.9513830304145813
Epoch: 3880, Batch Gradient Norm: 34.554555986656425
Epoch: 3880, Batch Gradient Norm after: 22.360677344135087
Epoch 3881/10000, Prediction Accuracy = 57.684000000000005%, Loss = 0.9412073373794556
Epoch: 3881, Batch Gradient Norm: 38.42311319601419
Epoch: 3881, Batch Gradient Norm after: 22.360678796768564
Epoch 3882/10000, Prediction Accuracy = 57.64%, Loss = 0.9511634707450867
Epoch: 3882, Batch Gradient Norm: 34.55005574763161
Epoch: 3882, Batch Gradient Norm after: 22.360677999143167
Epoch 3883/10000, Prediction Accuracy = 57.688%, Loss = 0.9410144925117493
Epoch: 3883, Batch Gradient Norm: 38.411332212928976
Epoch: 3883, Batch Gradient Norm after: 22.360675960485604
Epoch 3884/10000, Prediction Accuracy = 57.65%, Loss = 0.9509490251541137
Epoch: 3884, Batch Gradient Norm: 34.54463113330934
Epoch: 3884, Batch Gradient Norm after: 22.360675905851295
Epoch 3885/10000, Prediction Accuracy = 57.69199999999999%, Loss = 0.9408182024955749
Epoch: 3885, Batch Gradient Norm: 38.402869398576115
Epoch: 3885, Batch Gradient Norm after: 22.360677884458557
Epoch 3886/10000, Prediction Accuracy = 57.652%, Loss = 0.9507363915443421
Epoch: 3886, Batch Gradient Norm: 34.53950992653244
Epoch: 3886, Batch Gradient Norm after: 22.360675799846177
Epoch 3887/10000, Prediction Accuracy = 57.702%, Loss = 0.9406215071678161
Epoch: 3887, Batch Gradient Norm: 38.392301024569065
Epoch: 3887, Batch Gradient Norm after: 22.360677230502045
Epoch 3888/10000, Prediction Accuracy = 57.656000000000006%, Loss = 0.9505304217338562
Epoch: 3888, Batch Gradient Norm: 34.53368325542568
Epoch: 3888, Batch Gradient Norm after: 22.36067580634621
Epoch 3889/10000, Prediction Accuracy = 57.708000000000006%, Loss = 0.9404282927513122
Epoch: 3889, Batch Gradient Norm: 38.38403678700766
Epoch: 3889, Batch Gradient Norm after: 22.360676107360533
Epoch 3890/10000, Prediction Accuracy = 57.660000000000004%, Loss = 0.950326657295227
Epoch: 3890, Batch Gradient Norm: 34.52779204977584
Epoch: 3890, Batch Gradient Norm after: 22.36067630037901
Epoch 3891/10000, Prediction Accuracy = 57.70399999999999%, Loss = 0.940229332447052
Epoch: 3891, Batch Gradient Norm: 38.37938794826475
Epoch: 3891, Batch Gradient Norm after: 22.360674419223933
Epoch 3892/10000, Prediction Accuracy = 57.674%, Loss = 0.9501298785209655
Epoch: 3892, Batch Gradient Norm: 34.523215327932874
Epoch: 3892, Batch Gradient Norm after: 22.360676121533388
Epoch 3893/10000, Prediction Accuracy = 57.708000000000006%, Loss = 0.9400273203849793
Epoch: 3893, Batch Gradient Norm: 38.37402028680414
Epoch: 3893, Batch Gradient Norm after: 22.360675665428108
Epoch 3894/10000, Prediction Accuracy = 57.672000000000004%, Loss = 0.949935507774353
Epoch: 3894, Batch Gradient Norm: 34.51679979016792
Epoch: 3894, Batch Gradient Norm after: 22.3606765307189
Epoch 3895/10000, Prediction Accuracy = 57.715999999999994%, Loss = 0.9398288249969482
Epoch: 3895, Batch Gradient Norm: 38.366610629491205
Epoch: 3895, Batch Gradient Norm after: 22.360676796673324
Epoch 3896/10000, Prediction Accuracy = 57.672000000000004%, Loss = 0.9497332572937012
Epoch: 3896, Batch Gradient Norm: 34.51293510636183
Epoch: 3896, Batch Gradient Norm after: 22.360677001708403
Epoch 3897/10000, Prediction Accuracy = 57.722%, Loss = 0.9396332859992981
Epoch: 3897, Batch Gradient Norm: 38.35738157432283
Epoch: 3897, Batch Gradient Norm after: 22.360674974203985
Epoch 3898/10000, Prediction Accuracy = 57.68000000000001%, Loss = 0.9495209097862244
Epoch: 3898, Batch Gradient Norm: 34.50693332898376
Epoch: 3898, Batch Gradient Norm after: 22.36067525464417
Epoch 3899/10000, Prediction Accuracy = 57.724000000000004%, Loss = 0.9394392371177673
Epoch: 3899, Batch Gradient Norm: 38.34928224048739
Epoch: 3899, Batch Gradient Norm after: 22.360674138044676
Epoch 3900/10000, Prediction Accuracy = 57.674%, Loss = 0.9493174433708191
Epoch: 3900, Batch Gradient Norm: 34.50170051408608
Epoch: 3900, Batch Gradient Norm after: 22.36067565299911
Epoch 3901/10000, Prediction Accuracy = 57.727999999999994%, Loss = 0.9392423510551453
Epoch: 3901, Batch Gradient Norm: 38.346616795724664
Epoch: 3901, Batch Gradient Norm after: 22.36067507558394
Epoch 3902/10000, Prediction Accuracy = 57.68399999999999%, Loss = 0.9491300821304322
Epoch: 3902, Batch Gradient Norm: 34.49543539305507
Epoch: 3902, Batch Gradient Norm after: 22.360676168202815
Epoch 3903/10000, Prediction Accuracy = 57.726%, Loss = 0.9390497446060181
Epoch: 3903, Batch Gradient Norm: 38.34275370292323
Epoch: 3903, Batch Gradient Norm after: 22.360676900093527
Epoch 3904/10000, Prediction Accuracy = 57.69200000000001%, Loss = 0.9489358782768249
Epoch: 3904, Batch Gradient Norm: 34.489176512644995
Epoch: 3904, Batch Gradient Norm after: 22.36067749003171
Epoch 3905/10000, Prediction Accuracy = 57.726%, Loss = 0.9388507246971131
Epoch: 3905, Batch Gradient Norm: 38.34172156377223
Epoch: 3905, Batch Gradient Norm after: 22.360675211585146
Epoch 3906/10000, Prediction Accuracy = 57.698%, Loss = 0.948759663105011
Epoch: 3906, Batch Gradient Norm: 34.48060666256648
Epoch: 3906, Batch Gradient Norm after: 22.360677954465213
Epoch 3907/10000, Prediction Accuracy = 57.73599999999999%, Loss = 0.938645887374878
Epoch: 3907, Batch Gradient Norm: 38.3421103743377
Epoch: 3907, Batch Gradient Norm after: 22.360677431806444
Epoch 3908/10000, Prediction Accuracy = 57.708000000000006%, Loss = 0.9485798954963685
Epoch: 3908, Batch Gradient Norm: 34.47383316502681
Epoch: 3908, Batch Gradient Norm after: 22.36067763077991
Epoch 3909/10000, Prediction Accuracy = 57.74400000000001%, Loss = 0.9384495735168457
Epoch: 3909, Batch Gradient Norm: 38.33872927134134
Epoch: 3909, Batch Gradient Norm after: 22.36067705272915
Epoch 3910/10000, Prediction Accuracy = 57.708000000000006%, Loss = 0.9483880758285522
Epoch: 3910, Batch Gradient Norm: 34.46689593066836
Epoch: 3910, Batch Gradient Norm after: 22.360678246851396
Epoch 3911/10000, Prediction Accuracy = 57.751999999999995%, Loss = 0.9382494926452637
Epoch: 3911, Batch Gradient Norm: 38.33676860602425
Epoch: 3911, Batch Gradient Norm after: 22.360675065566177
Epoch 3912/10000, Prediction Accuracy = 57.714%, Loss = 0.9482061266899109
Epoch: 3912, Batch Gradient Norm: 34.45906933235234
Epoch: 3912, Batch Gradient Norm after: 22.360678496523086
Epoch 3913/10000, Prediction Accuracy = 57.757999999999996%, Loss = 0.9380523204803467
Epoch: 3913, Batch Gradient Norm: 38.32763526324857
Epoch: 3913, Batch Gradient Norm after: 22.36067550913358
Epoch 3914/10000, Prediction Accuracy = 57.724000000000004%, Loss = 0.9480031847953796
Epoch: 3914, Batch Gradient Norm: 34.45442213349069
Epoch: 3914, Batch Gradient Norm after: 22.360677849632104
Epoch 3915/10000, Prediction Accuracy = 57.757999999999996%, Loss = 0.9378582119941712
Epoch: 3915, Batch Gradient Norm: 38.319968791240925
Epoch: 3915, Batch Gradient Norm after: 22.36067765926792
Epoch 3916/10000, Prediction Accuracy = 57.739999999999995%, Loss = 0.9478028535842895
Epoch: 3916, Batch Gradient Norm: 34.449253672748306
Epoch: 3916, Batch Gradient Norm after: 22.360681042136182
Epoch 3917/10000, Prediction Accuracy = 57.77%, Loss = 0.9376615047454834
Epoch: 3917, Batch Gradient Norm: 38.31306699857948
Epoch: 3917, Batch Gradient Norm after: 22.360677095116976
Epoch 3918/10000, Prediction Accuracy = 57.746%, Loss = 0.9476017951965332
Epoch: 3918, Batch Gradient Norm: 34.44158834575866
Epoch: 3918, Batch Gradient Norm after: 22.360678836238776
Epoch 3919/10000, Prediction Accuracy = 57.77%, Loss = 0.9374716639518738
Epoch: 3919, Batch Gradient Norm: 38.30508483586591
Epoch: 3919, Batch Gradient Norm after: 22.360677159856614
Epoch 3920/10000, Prediction Accuracy = 57.742%, Loss = 0.9473970532417297
Epoch: 3920, Batch Gradient Norm: 34.43492910034793
Epoch: 3920, Batch Gradient Norm after: 22.36067901048957
Epoch 3921/10000, Prediction Accuracy = 57.774%, Loss = 0.9372750282287597
Epoch: 3921, Batch Gradient Norm: 38.29709573857598
Epoch: 3921, Batch Gradient Norm after: 22.360677310781497
Epoch 3922/10000, Prediction Accuracy = 57.75%, Loss = 0.9472087621688843
Epoch: 3922, Batch Gradient Norm: 34.42885271721536
Epoch: 3922, Batch Gradient Norm after: 22.36067927898921
Epoch 3923/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.9370771884918213
Epoch: 3923, Batch Gradient Norm: 38.29259951583261
Epoch: 3923, Batch Gradient Norm after: 22.3606759266101
Epoch 3924/10000, Prediction Accuracy = 57.760000000000005%, Loss = 0.9470123887062073
Epoch: 3924, Batch Gradient Norm: 34.42297841513147
Epoch: 3924, Batch Gradient Norm after: 22.36067726067959
Epoch 3925/10000, Prediction Accuracy = 57.778%, Loss = 0.9368827104568481
Epoch: 3925, Batch Gradient Norm: 38.282403621506546
Epoch: 3925, Batch Gradient Norm after: 22.360675336010495
Epoch 3926/10000, Prediction Accuracy = 57.767999999999994%, Loss = 0.9468099355697632
Epoch: 3926, Batch Gradient Norm: 34.41736755430691
Epoch: 3926, Batch Gradient Norm after: 22.36067636354509
Epoch 3927/10000, Prediction Accuracy = 57.778%, Loss = 0.9366922497749328
Epoch: 3927, Batch Gradient Norm: 38.268322591577544
Epoch: 3927, Batch Gradient Norm after: 22.360674888390015
Epoch 3928/10000, Prediction Accuracy = 57.757999999999996%, Loss = 0.9465995907783509
Epoch: 3928, Batch Gradient Norm: 34.41392778940412
Epoch: 3928, Batch Gradient Norm after: 22.360677604216125
Epoch 3929/10000, Prediction Accuracy = 57.790000000000006%, Loss = 0.9365050196647644
Epoch: 3929, Batch Gradient Norm: 38.25758133123467
Epoch: 3929, Batch Gradient Norm after: 22.360675598634867
Epoch 3930/10000, Prediction Accuracy = 57.77%, Loss = 0.9463871121406555
Epoch: 3930, Batch Gradient Norm: 34.40855708077028
Epoch: 3930, Batch Gradient Norm after: 22.360678787514896
Epoch 3931/10000, Prediction Accuracy = 57.794000000000004%, Loss = 0.9363176465034485
Epoch: 3931, Batch Gradient Norm: 38.24085187857454
Epoch: 3931, Batch Gradient Norm after: 22.360675774318693
Epoch 3932/10000, Prediction Accuracy = 57.767999999999994%, Loss = 0.9461652874946594
Epoch: 3932, Batch Gradient Norm: 34.40656894299424
Epoch: 3932, Batch Gradient Norm after: 22.360678235915458
Epoch 3933/10000, Prediction Accuracy = 57.798%, Loss = 0.9361308574676513
Epoch: 3933, Batch Gradient Norm: 38.22567692450156
Epoch: 3933, Batch Gradient Norm after: 22.360675440690386
Epoch 3934/10000, Prediction Accuracy = 57.779999999999994%, Loss = 0.9459397673606873
Epoch: 3934, Batch Gradient Norm: 34.40295915871848
Epoch: 3934, Batch Gradient Norm after: 22.36067764593164
Epoch 3935/10000, Prediction Accuracy = 57.80800000000001%, Loss = 0.9359435677528382
Epoch: 3935, Batch Gradient Norm: 38.21078420295631
Epoch: 3935, Batch Gradient Norm after: 22.360675949789805
Epoch 3936/10000, Prediction Accuracy = 57.8%, Loss = 0.9457274198532104
Epoch: 3936, Batch Gradient Norm: 34.400475118949515
Epoch: 3936, Batch Gradient Norm after: 22.360677850470232
Epoch 3937/10000, Prediction Accuracy = 57.818000000000005%, Loss = 0.9357553839683532
Epoch: 3937, Batch Gradient Norm: 38.19833844704681
Epoch: 3937, Batch Gradient Norm after: 22.36067446377739
Epoch 3938/10000, Prediction Accuracy = 57.806000000000004%, Loss = 0.9455178737640381
Epoch: 3938, Batch Gradient Norm: 34.39686675843187
Epoch: 3938, Batch Gradient Norm after: 22.360678455102963
Epoch 3939/10000, Prediction Accuracy = 57.81999999999999%, Loss = 0.9355629444122314
Epoch: 3939, Batch Gradient Norm: 38.18640208670396
Epoch: 3939, Batch Gradient Norm after: 22.360676878855294
Epoch 3940/10000, Prediction Accuracy = 57.818%, Loss = 0.9453030586242676
Epoch: 3940, Batch Gradient Norm: 34.39356490186598
Epoch: 3940, Batch Gradient Norm after: 22.360679615557885
Epoch 3941/10000, Prediction Accuracy = 57.827999999999996%, Loss = 0.9353749871253967
Epoch: 3941, Batch Gradient Norm: 38.173222607634685
Epoch: 3941, Batch Gradient Norm after: 22.36067695395083
Epoch 3942/10000, Prediction Accuracy = 57.83%, Loss = 0.9450827836990356
Epoch: 3942, Batch Gradient Norm: 34.38985435182328
Epoch: 3942, Batch Gradient Norm after: 22.360680271216342
Epoch 3943/10000, Prediction Accuracy = 57.83399999999999%, Loss = 0.9351880669593811
Epoch: 3943, Batch Gradient Norm: 38.15712832100056
Epoch: 3943, Batch Gradient Norm after: 22.360677098098375
Epoch 3944/10000, Prediction Accuracy = 57.830000000000005%, Loss = 0.9448641538619995
Epoch: 3944, Batch Gradient Norm: 34.38609626553974
Epoch: 3944, Batch Gradient Norm after: 22.360678688037492
Epoch 3945/10000, Prediction Accuracy = 57.826%, Loss = 0.9350031852722168
Epoch: 3945, Batch Gradient Norm: 38.14800527969386
Epoch: 3945, Batch Gradient Norm after: 22.360676216946533
Epoch 3946/10000, Prediction Accuracy = 57.826%, Loss = 0.9446486234664917
Epoch: 3946, Batch Gradient Norm: 34.38199070092533
Epoch: 3946, Batch Gradient Norm after: 22.36067880231186
Epoch 3947/10000, Prediction Accuracy = 57.826%, Loss = 0.9348103046417237
Epoch: 3947, Batch Gradient Norm: 38.13785003017936
Epoch: 3947, Batch Gradient Norm after: 22.360678363608468
Epoch 3948/10000, Prediction Accuracy = 57.830000000000005%, Loss = 0.9444548487663269
Epoch: 3948, Batch Gradient Norm: 34.377869712926206
Epoch: 3948, Batch Gradient Norm after: 22.360676512739744
Epoch 3949/10000, Prediction Accuracy = 57.822%, Loss = 0.9346191644668579
Epoch: 3949, Batch Gradient Norm: 38.12985987626558
Epoch: 3949, Batch Gradient Norm after: 22.36067793557716
Epoch 3950/10000, Prediction Accuracy = 57.83200000000001%, Loss = 0.9442558050155639
Epoch: 3950, Batch Gradient Norm: 34.37145363166525
Epoch: 3950, Batch Gradient Norm after: 22.360678156045143
Epoch 3951/10000, Prediction Accuracy = 57.836%, Loss = 0.9344255089759826
Epoch: 3951, Batch Gradient Norm: 38.122793221489296
Epoch: 3951, Batch Gradient Norm after: 22.360678723411983
Epoch 3952/10000, Prediction Accuracy = 57.839999999999996%, Loss = 0.9440648913383484
Epoch: 3952, Batch Gradient Norm: 34.36534560297894
Epoch: 3952, Batch Gradient Norm after: 22.360676741283513
Epoch 3953/10000, Prediction Accuracy = 57.83%, Loss = 0.9342318534851074
Epoch: 3953, Batch Gradient Norm: 38.11833550556488
Epoch: 3953, Batch Gradient Norm after: 22.360677741259636
Epoch 3954/10000, Prediction Accuracy = 57.843999999999994%, Loss = 0.9438709616661072
Epoch: 3954, Batch Gradient Norm: 34.35805636078565
Epoch: 3954, Batch Gradient Norm after: 22.360677516480255
Epoch 3955/10000, Prediction Accuracy = 57.836%, Loss = 0.9340374827384949
Epoch: 3955, Batch Gradient Norm: 38.11492374082056
Epoch: 3955, Batch Gradient Norm after: 22.360679269606855
Epoch 3956/10000, Prediction Accuracy = 57.84400000000001%, Loss = 0.9436892747879029
Epoch: 3956, Batch Gradient Norm: 34.351408449778134
Epoch: 3956, Batch Gradient Norm after: 22.36067861445705
Epoch 3957/10000, Prediction Accuracy = 57.852%, Loss = 0.9338414549827576
Epoch: 3957, Batch Gradient Norm: 38.115447357868206
Epoch: 3957, Batch Gradient Norm after: 22.360677177312343
Epoch 3958/10000, Prediction Accuracy = 57.842%, Loss = 0.94351145029068
Epoch: 3958, Batch Gradient Norm: 34.34246009689712
Epoch: 3958, Batch Gradient Norm after: 22.360678170720206
Epoch 3959/10000, Prediction Accuracy = 57.862%, Loss = 0.9336450219154357
Epoch: 3959, Batch Gradient Norm: 38.11647797772461
Epoch: 3959, Batch Gradient Norm after: 22.360676075430003
Epoch 3960/10000, Prediction Accuracy = 57.846000000000004%, Loss = 0.9433378219604492
Epoch: 3960, Batch Gradient Norm: 34.33439283851263
Epoch: 3960, Batch Gradient Norm after: 22.360679620457276
Epoch 3961/10000, Prediction Accuracy = 57.85799999999999%, Loss = 0.9334441781044006
Epoch: 3961, Batch Gradient Norm: 38.117275339162795
Epoch: 3961, Batch Gradient Norm after: 22.360677693617703
Epoch 3962/10000, Prediction Accuracy = 57.848%, Loss = 0.9431677103042603
Epoch: 3962, Batch Gradient Norm: 34.32428915185827
Epoch: 3962, Batch Gradient Norm after: 22.36067697958112
Epoch 3963/10000, Prediction Accuracy = 57.864%, Loss = 0.9332494854927063
Epoch: 3963, Batch Gradient Norm: 38.119030438191615
Epoch: 3963, Batch Gradient Norm after: 22.360677949637527
Epoch 3964/10000, Prediction Accuracy = 57.85%, Loss = 0.9430051326751709
Epoch: 3964, Batch Gradient Norm: 34.31507891982265
Epoch: 3964, Batch Gradient Norm after: 22.36067742850125
Epoch 3965/10000, Prediction Accuracy = 57.86800000000001%, Loss = 0.9330489993095398
Epoch: 3965, Batch Gradient Norm: 38.11757883199648
Epoch: 3965, Batch Gradient Norm after: 22.360676475913174
Epoch 3966/10000, Prediction Accuracy = 57.852%, Loss = 0.9428258776664734
Epoch: 3966, Batch Gradient Norm: 34.308332039903036
Epoch: 3966, Batch Gradient Norm after: 22.360678695895672
Epoch 3967/10000, Prediction Accuracy = 57.864%, Loss = 0.932859230041504
Epoch: 3967, Batch Gradient Norm: 38.12028561429952
Epoch: 3967, Batch Gradient Norm after: 22.360678569728034
Epoch 3968/10000, Prediction Accuracy = 57.85%, Loss = 0.9426439046859741
Epoch: 3968, Batch Gradient Norm: 34.30015409716662
Epoch: 3968, Batch Gradient Norm after: 22.360677205214618
Epoch 3969/10000, Prediction Accuracy = 57.870000000000005%, Loss = 0.9326622128486634
Epoch: 3969, Batch Gradient Norm: 38.12199232234438
Epoch: 3969, Batch Gradient Norm after: 22.360676182488525
Epoch 3970/10000, Prediction Accuracy = 57.85%, Loss = 0.9424776434898376
Epoch: 3970, Batch Gradient Norm: 34.29122719429875
Epoch: 3970, Batch Gradient Norm after: 22.360677426135386
Epoch 3971/10000, Prediction Accuracy = 57.878%, Loss = 0.9324678778648376
Epoch: 3971, Batch Gradient Norm: 38.12609735431138
Epoch: 3971, Batch Gradient Norm after: 22.360675842783436
Epoch 3972/10000, Prediction Accuracy = 57.855999999999995%, Loss = 0.9423148155212402
Epoch: 3972, Batch Gradient Norm: 34.282665127964734
Epoch: 3972, Batch Gradient Norm after: 22.360677905023753
Epoch 3973/10000, Prediction Accuracy = 57.878%, Loss = 0.9322715640068054
Epoch: 3973, Batch Gradient Norm: 38.12848311324669
Epoch: 3973, Batch Gradient Norm after: 22.360676293904238
Epoch 3974/10000, Prediction Accuracy = 57.86200000000001%, Loss = 0.9421372771263122
Epoch: 3974, Batch Gradient Norm: 34.27427415850763
Epoch: 3974, Batch Gradient Norm after: 22.360680051455766
Epoch 3975/10000, Prediction Accuracy = 57.888%, Loss = 0.9320798993110657
Epoch: 3975, Batch Gradient Norm: 38.13196786823494
Epoch: 3975, Batch Gradient Norm after: 22.360678378361847
Epoch 3976/10000, Prediction Accuracy = 57.867999999999995%, Loss = 0.9419743061065674
Epoch: 3976, Batch Gradient Norm: 34.26540840434923
Epoch: 3976, Batch Gradient Norm after: 22.360680511582597
Epoch 3977/10000, Prediction Accuracy = 57.894000000000005%, Loss = 0.9318834185600281
Epoch: 3977, Batch Gradient Norm: 38.13656441017115
Epoch: 3977, Batch Gradient Norm after: 22.360678321468303
Epoch 3978/10000, Prediction Accuracy = 57.872%, Loss = 0.941817045211792
Epoch: 3978, Batch Gradient Norm: 34.25547450383959
Epoch: 3978, Batch Gradient Norm after: 22.36067682849559
Epoch 3979/10000, Prediction Accuracy = 57.898%, Loss = 0.9316861629486084
Epoch: 3979, Batch Gradient Norm: 38.145781438206534
Epoch: 3979, Batch Gradient Norm after: 22.360677560379052
Epoch 3980/10000, Prediction Accuracy = 57.872%, Loss = 0.9416653990745545
Epoch: 3980, Batch Gradient Norm: 34.24572443498151
Epoch: 3980, Batch Gradient Norm after: 22.3606771823718
Epoch 3981/10000, Prediction Accuracy = 57.9%, Loss = 0.9314891934394837
Epoch: 3981, Batch Gradient Norm: 38.150674427334
Epoch: 3981, Batch Gradient Norm after: 22.36067989561278
Epoch 3982/10000, Prediction Accuracy = 57.874%, Loss = 0.9415073275566102
Epoch: 3982, Batch Gradient Norm: 34.236603752397
Epoch: 3982, Batch Gradient Norm after: 22.36067693791211
Epoch 3983/10000, Prediction Accuracy = 57.898%, Loss = 0.9312958478927612
Epoch: 3983, Batch Gradient Norm: 38.157947683144364
Epoch: 3983, Batch Gradient Norm after: 22.360678903420496
Epoch 3984/10000, Prediction Accuracy = 57.872%, Loss = 0.9413480758666992
Epoch: 3984, Batch Gradient Norm: 34.228598319437715
Epoch: 3984, Batch Gradient Norm after: 22.360678442803806
Epoch 3985/10000, Prediction Accuracy = 57.903999999999996%, Loss = 0.9310977697372437
Epoch: 3985, Batch Gradient Norm: 38.159860144539174
Epoch: 3985, Batch Gradient Norm after: 22.360677643660214
Epoch 3986/10000, Prediction Accuracy = 57.878%, Loss = 0.9411768198013306
Epoch: 3986, Batch Gradient Norm: 34.222520170183714
Epoch: 3986, Batch Gradient Norm after: 22.360678726565894
Epoch 3987/10000, Prediction Accuracy = 57.907999999999994%, Loss = 0.9309068202972413
Epoch: 3987, Batch Gradient Norm: 38.159174831849114
Epoch: 3987, Batch Gradient Norm after: 22.360680037090802
Epoch 3988/10000, Prediction Accuracy = 57.88199999999999%, Loss = 0.9409984946250916
Epoch: 3988, Batch Gradient Norm: 34.21616625279543
Epoch: 3988, Batch Gradient Norm after: 22.360676740286056
Epoch 3989/10000, Prediction Accuracy = 57.914%, Loss = 0.9307139873504638
Epoch: 3989, Batch Gradient Norm: 38.15828749570964
Epoch: 3989, Batch Gradient Norm after: 22.36067899578693
Epoch 3990/10000, Prediction Accuracy = 57.89399999999999%, Loss = 0.940820038318634
Epoch: 3990, Batch Gradient Norm: 34.20841391569576
Epoch: 3990, Batch Gradient Norm after: 22.360677520633153
Epoch 3991/10000, Prediction Accuracy = 57.924%, Loss = 0.9305224418640137
Epoch: 3991, Batch Gradient Norm: 38.16034060103115
Epoch: 3991, Batch Gradient Norm after: 22.360679594711087
Epoch 3992/10000, Prediction Accuracy = 57.884%, Loss = 0.9406463623046875
Epoch: 3992, Batch Gradient Norm: 34.20212047881058
Epoch: 3992, Batch Gradient Norm after: 22.36067602295968
Epoch 3993/10000, Prediction Accuracy = 57.93000000000001%, Loss = 0.9303275346755981
Epoch: 3993, Batch Gradient Norm: 38.162798069031325
Epoch: 3993, Batch Gradient Norm after: 22.36067871046946
Epoch 3994/10000, Prediction Accuracy = 57.888%, Loss = 0.9404762744903564
Epoch: 3994, Batch Gradient Norm: 34.19391445332135
Epoch: 3994, Batch Gradient Norm after: 22.360676945241508
Epoch 3995/10000, Prediction Accuracy = 57.931999999999995%, Loss = 0.9301344633102417
Epoch: 3995, Batch Gradient Norm: 38.161490755548314
Epoch: 3995, Batch Gradient Norm after: 22.360678857734847
Epoch 3996/10000, Prediction Accuracy = 57.894000000000005%, Loss = 0.9402984976768494
Epoch: 3996, Batch Gradient Norm: 34.18761508335149
Epoch: 3996, Batch Gradient Norm after: 22.36067544731021
Epoch 3997/10000, Prediction Accuracy = 57.938%, Loss = 0.9299428462982178
Epoch: 3997, Batch Gradient Norm: 38.16139253003512
Epoch: 3997, Batch Gradient Norm after: 22.36067820516502
Epoch 3998/10000, Prediction Accuracy = 57.916%, Loss = 0.9401274442672729
Epoch: 3998, Batch Gradient Norm: 34.18229154064667
Epoch: 3998, Batch Gradient Norm after: 22.36067446429907
Epoch 3999/10000, Prediction Accuracy = 57.94199999999999%, Loss = 0.929752767086029
Epoch: 3999, Batch Gradient Norm: 38.1626026964304
Epoch: 3999, Batch Gradient Norm after: 22.36067746677852
Epoch 4000/10000, Prediction Accuracy = 57.922000000000004%, Loss = 0.9399558544158936
Epoch: 4000, Batch Gradient Norm: 34.17337031274412
Epoch: 4000, Batch Gradient Norm after: 22.36067641862927
Epoch 4001/10000, Prediction Accuracy = 57.946000000000005%, Loss = 0.9295592904090881
Epoch: 4001, Batch Gradient Norm: 38.16696948166745
Epoch: 4001, Batch Gradient Norm after: 22.360678862914657
Epoch 4002/10000, Prediction Accuracy = 57.932%, Loss = 0.9397913813591003
Epoch: 4002, Batch Gradient Norm: 34.16509267225942
Epoch: 4002, Batch Gradient Norm after: 22.36067909734086
Epoch 4003/10000, Prediction Accuracy = 57.944%, Loss = 0.9293662428855896
Epoch: 4003, Batch Gradient Norm: 38.16897240536684
Epoch: 4003, Batch Gradient Norm after: 22.360680186721574
Epoch 4004/10000, Prediction Accuracy = 57.934000000000005%, Loss = 0.9396318554878235
Epoch: 4004, Batch Gradient Norm: 34.15788370147279
Epoch: 4004, Batch Gradient Norm after: 22.36067588334322
Epoch 4005/10000, Prediction Accuracy = 57.944%, Loss = 0.9291714668273926
Epoch: 4005, Batch Gradient Norm: 38.16932322073242
Epoch: 4005, Batch Gradient Norm after: 22.360676630217434
Epoch 4006/10000, Prediction Accuracy = 57.94%, Loss = 0.9394604444503785
Epoch: 4006, Batch Gradient Norm: 34.151698502917405
Epoch: 4006, Batch Gradient Norm after: 22.360677547065027
Epoch 4007/10000, Prediction Accuracy = 57.94%, Loss = 0.9289801478385925
Epoch: 4007, Batch Gradient Norm: 38.1684196728239
Epoch: 4007, Batch Gradient Norm after: 22.360679553166058
Epoch 4008/10000, Prediction Accuracy = 57.94200000000001%, Loss = 0.9392808198928833
Epoch: 4008, Batch Gradient Norm: 34.14470188401587
Epoch: 4008, Batch Gradient Norm after: 22.360676707673203
Epoch 4009/10000, Prediction Accuracy = 57.952%, Loss = 0.9287948131561279
Epoch: 4009, Batch Gradient Norm: 38.16665546368239
Epoch: 4009, Batch Gradient Norm after: 22.360677124046454
Epoch 4010/10000, Prediction Accuracy = 57.952%, Loss = 0.9390992641448974
Epoch: 4010, Batch Gradient Norm: 34.13941746387677
Epoch: 4010, Batch Gradient Norm after: 22.360677862848174
Epoch 4011/10000, Prediction Accuracy = 57.962%, Loss = 0.928607702255249
Epoch: 4011, Batch Gradient Norm: 38.16231798996085
Epoch: 4011, Batch Gradient Norm after: 22.36067929840484
Epoch 4012/10000, Prediction Accuracy = 57.952%, Loss = 0.9389150738716125
Epoch: 4012, Batch Gradient Norm: 34.133093171183184
Epoch: 4012, Batch Gradient Norm after: 22.360677869070102
Epoch 4013/10000, Prediction Accuracy = 57.962%, Loss = 0.9284209370613098
Epoch: 4013, Batch Gradient Norm: 38.15716077012791
Epoch: 4013, Batch Gradient Norm after: 22.360677257321136
Epoch 4014/10000, Prediction Accuracy = 57.94199999999999%, Loss = 0.9387277007102967
Epoch: 4014, Batch Gradient Norm: 34.129324896010274
Epoch: 4014, Batch Gradient Norm after: 22.360677834537046
Epoch 4015/10000, Prediction Accuracy = 57.962%, Loss = 0.9282339453697205
Epoch: 4015, Batch Gradient Norm: 38.15270485605726
Epoch: 4015, Batch Gradient Norm after: 22.36067494312135
Epoch 4016/10000, Prediction Accuracy = 57.948%, Loss = 0.938541316986084
Epoch: 4016, Batch Gradient Norm: 34.121345581876156
Epoch: 4016, Batch Gradient Norm after: 22.360677139300247
Epoch 4017/10000, Prediction Accuracy = 57.965999999999994%, Loss = 0.9280471801757812
Epoch: 4017, Batch Gradient Norm: 38.147928096466714
Epoch: 4017, Batch Gradient Norm after: 22.360676254335953
Epoch 4018/10000, Prediction Accuracy = 57.96400000000001%, Loss = 0.9383587598800659
Epoch: 4018, Batch Gradient Norm: 34.116980273525066
Epoch: 4018, Batch Gradient Norm after: 22.36067767215846
Epoch 4019/10000, Prediction Accuracy = 57.972%, Loss = 0.9278607368469238
Epoch: 4019, Batch Gradient Norm: 38.14481196850344
Epoch: 4019, Batch Gradient Norm after: 22.360677629690006
Epoch 4020/10000, Prediction Accuracy = 57.96%, Loss = 0.9381812930107116
Epoch: 4020, Batch Gradient Norm: 34.110443106592264
Epoch: 4020, Batch Gradient Norm after: 22.360677641527325
Epoch 4021/10000, Prediction Accuracy = 57.977999999999994%, Loss = 0.9276717901229858
Epoch: 4021, Batch Gradient Norm: 38.139070157909146
Epoch: 4021, Batch Gradient Norm after: 22.360677860531297
Epoch 4022/10000, Prediction Accuracy = 57.970000000000006%, Loss = 0.9379951477050781
Epoch: 4022, Batch Gradient Norm: 34.10599600342455
Epoch: 4022, Batch Gradient Norm after: 22.360677020438587
Epoch 4023/10000, Prediction Accuracy = 57.98%, Loss = 0.9274843335151672
Epoch: 4023, Batch Gradient Norm: 38.13421368989304
Epoch: 4023, Batch Gradient Norm after: 22.360678089963372
Epoch 4024/10000, Prediction Accuracy = 57.976%, Loss = 0.9378128528594971
Epoch: 4024, Batch Gradient Norm: 34.10142923413934
Epoch: 4024, Batch Gradient Norm after: 22.360679640245284
Epoch 4025/10000, Prediction Accuracy = 57.977999999999994%, Loss = 0.9272979378700257
Epoch: 4025, Batch Gradient Norm: 38.13000107641792
Epoch: 4025, Batch Gradient Norm after: 22.360675823950707
Epoch 4026/10000, Prediction Accuracy = 57.986000000000004%, Loss = 0.9376291990280151
Epoch: 4026, Batch Gradient Norm: 34.0945563547663
Epoch: 4026, Batch Gradient Norm after: 22.36067615620141
Epoch 4027/10000, Prediction Accuracy = 57.988%, Loss = 0.9271106719970703
Epoch: 4027, Batch Gradient Norm: 38.126560610769694
Epoch: 4027, Batch Gradient Norm after: 22.360675404439206
Epoch 4028/10000, Prediction Accuracy = 57.98199999999999%, Loss = 0.9374536156654358
Epoch: 4028, Batch Gradient Norm: 34.08717409478179
Epoch: 4028, Batch Gradient Norm after: 22.36067747017106
Epoch 4029/10000, Prediction Accuracy = 58.0%, Loss = 0.9269243359565735
Epoch: 4029, Batch Gradient Norm: 38.1215130758695
Epoch: 4029, Batch Gradient Norm after: 22.36067653986392
Epoch 4030/10000, Prediction Accuracy = 57.986000000000004%, Loss = 0.9372647643089295
Epoch: 4030, Batch Gradient Norm: 34.08181853695098
Epoch: 4030, Batch Gradient Norm after: 22.36067679963166
Epoch 4031/10000, Prediction Accuracy = 58.0%, Loss = 0.9267376780509948
Epoch: 4031, Batch Gradient Norm: 38.11544918920465
Epoch: 4031, Batch Gradient Norm after: 22.360674546778277
Epoch 4032/10000, Prediction Accuracy = 57.989999999999995%, Loss = 0.9370754837989808
Epoch: 4032, Batch Gradient Norm: 34.07651168224804
Epoch: 4032, Batch Gradient Norm after: 22.36067587542903
Epoch 4033/10000, Prediction Accuracy = 58.0%, Loss = 0.9265546441078186
Epoch: 4033, Batch Gradient Norm: 38.10941256148532
Epoch: 4033, Batch Gradient Norm after: 22.36067618765897
Epoch 4034/10000, Prediction Accuracy = 57.996%, Loss = 0.9368822455406189
Epoch: 4034, Batch Gradient Norm: 34.07190368072514
Epoch: 4034, Batch Gradient Norm after: 22.36067896102147
Epoch 4035/10000, Prediction Accuracy = 58.012%, Loss = 0.9263713955879211
Epoch: 4035, Batch Gradient Norm: 38.10113593904969
Epoch: 4035, Batch Gradient Norm after: 22.36067771485941
Epoch 4036/10000, Prediction Accuracy = 58.0%, Loss = 0.9366912841796875
Epoch: 4036, Batch Gradient Norm: 34.067687497936035
Epoch: 4036, Batch Gradient Norm after: 22.360679045055406
Epoch 4037/10000, Prediction Accuracy = 58.004000000000005%, Loss = 0.9261842727661133
Epoch: 4037, Batch Gradient Norm: 38.08926779616039
Epoch: 4037, Batch Gradient Norm after: 22.360677158303055
Epoch 4038/10000, Prediction Accuracy = 58.012%, Loss = 0.9364963650703431
Epoch: 4038, Batch Gradient Norm: 34.06321228216062
Epoch: 4038, Batch Gradient Norm after: 22.360677501014344
Epoch 4039/10000, Prediction Accuracy = 58.004000000000005%, Loss = 0.9260048270225525
Epoch: 4039, Batch Gradient Norm: 38.0766068850368
Epoch: 4039, Batch Gradient Norm after: 22.36067522549058
Epoch 4040/10000, Prediction Accuracy = 58.004%, Loss = 0.9362936854362488
Epoch: 4040, Batch Gradient Norm: 34.05816366063572
Epoch: 4040, Batch Gradient Norm after: 22.360678188831738
Epoch 4041/10000, Prediction Accuracy = 58.001999999999995%, Loss = 0.925823175907135
Epoch: 4041, Batch Gradient Norm: 38.06474954245221
Epoch: 4041, Batch Gradient Norm after: 22.36067746141481
Epoch 4042/10000, Prediction Accuracy = 58.004%, Loss = 0.9360952734947204
Epoch: 4042, Batch Gradient Norm: 34.05364482921586
Epoch: 4042, Batch Gradient Norm after: 22.36067642828284
Epoch 4043/10000, Prediction Accuracy = 58.004000000000005%, Loss = 0.9256406903266907
Epoch: 4043, Batch Gradient Norm: 38.05451056188941
Epoch: 4043, Batch Gradient Norm after: 22.36067716905285
Epoch 4044/10000, Prediction Accuracy = 58.008%, Loss = 0.9358946681022644
Epoch: 4044, Batch Gradient Norm: 34.050344656277254
Epoch: 4044, Batch Gradient Norm after: 22.36067725723776
Epoch 4045/10000, Prediction Accuracy = 58.012%, Loss = 0.9254580020904541
Epoch: 4045, Batch Gradient Norm: 38.04528499630978
Epoch: 4045, Batch Gradient Norm after: 22.36067641153213
Epoch 4046/10000, Prediction Accuracy = 58.012%, Loss = 0.9356940269470215
Epoch: 4046, Batch Gradient Norm: 34.0452168480337
Epoch: 4046, Batch Gradient Norm after: 22.360677956335966
Epoch 4047/10000, Prediction Accuracy = 58.025999999999996%, Loss = 0.9252823233604431
Epoch: 4047, Batch Gradient Norm: 38.038978901547026
Epoch: 4047, Batch Gradient Norm after: 22.360675562638633
Epoch 4048/10000, Prediction Accuracy = 58.007999999999996%, Loss = 0.9355135321617126
Epoch: 4048, Batch Gradient Norm: 34.03936263900375
Epoch: 4048, Batch Gradient Norm after: 22.36067597094678
Epoch 4049/10000, Prediction Accuracy = 58.02%, Loss = 0.9250967621803283
Epoch: 4049, Batch Gradient Norm: 38.04133661533474
Epoch: 4049, Batch Gradient Norm after: 22.360677992501664
Epoch 4050/10000, Prediction Accuracy = 58.007999999999996%, Loss = 0.9353466153144836
Epoch: 4050, Batch Gradient Norm: 34.034767638075195
Epoch: 4050, Batch Gradient Norm after: 22.360678566167728
Epoch 4051/10000, Prediction Accuracy = 58.016%, Loss = 0.9249055624008179
Epoch: 4051, Batch Gradient Norm: 38.037416273989415
Epoch: 4051, Batch Gradient Norm after: 22.36067682616949
Epoch 4052/10000, Prediction Accuracy = 58.010000000000005%, Loss = 0.9351690769195556
Epoch: 4052, Batch Gradient Norm: 34.02831486563974
Epoch: 4052, Batch Gradient Norm after: 22.360678933265905
Epoch 4053/10000, Prediction Accuracy = 58.016%, Loss = 0.9247241735458374
Epoch: 4053, Batch Gradient Norm: 38.03466768752328
Epoch: 4053, Batch Gradient Norm after: 22.360677765472804
Epoch 4054/10000, Prediction Accuracy = 58.008%, Loss = 0.9349902391433715
Epoch: 4054, Batch Gradient Norm: 34.023782483372955
Epoch: 4054, Batch Gradient Norm after: 22.360680313606522
Epoch 4055/10000, Prediction Accuracy = 58.025999999999996%, Loss = 0.9245392918586731
Epoch: 4055, Batch Gradient Norm: 38.03637264945356
Epoch: 4055, Batch Gradient Norm after: 22.360676039748835
Epoch 4056/10000, Prediction Accuracy = 58.007999999999996%, Loss = 0.9348190188407898
Epoch: 4056, Batch Gradient Norm: 34.01821566768872
Epoch: 4056, Batch Gradient Norm after: 22.36067994899779
Epoch 4057/10000, Prediction Accuracy = 58.019999999999996%, Loss = 0.9243518352508545
Epoch: 4057, Batch Gradient Norm: 38.03502389233846
Epoch: 4057, Batch Gradient Norm after: 22.360675799213077
Epoch 4058/10000, Prediction Accuracy = 58.012%, Loss = 0.934636652469635
Epoch: 4058, Batch Gradient Norm: 34.01279279175819
Epoch: 4058, Batch Gradient Norm after: 22.36067919449987
Epoch 4059/10000, Prediction Accuracy = 58.017999999999994%, Loss = 0.9241697907447814
Epoch: 4059, Batch Gradient Norm: 38.03409957670286
Epoch: 4059, Batch Gradient Norm after: 22.360677873269623
Epoch 4060/10000, Prediction Accuracy = 58.012%, Loss = 0.9344631314277649
Epoch: 4060, Batch Gradient Norm: 34.0070834602211
Epoch: 4060, Batch Gradient Norm after: 22.360680037251228
Epoch 4061/10000, Prediction Accuracy = 58.028%, Loss = 0.9239805102348327
Epoch: 4061, Batch Gradient Norm: 38.033350748246534
Epoch: 4061, Batch Gradient Norm after: 22.36067882327323
Epoch 4062/10000, Prediction Accuracy = 58.022000000000006%, Loss = 0.934291410446167
Epoch: 4062, Batch Gradient Norm: 34.001069360821376
Epoch: 4062, Batch Gradient Norm after: 22.3606797014133
Epoch 4063/10000, Prediction Accuracy = 58.028%, Loss = 0.923797607421875
Epoch: 4063, Batch Gradient Norm: 38.032542149175306
Epoch: 4063, Batch Gradient Norm after: 22.36067863182169
Epoch 4064/10000, Prediction Accuracy = 58.019999999999996%, Loss = 0.9341241836547851
Epoch: 4064, Batch Gradient Norm: 33.99400836184405
Epoch: 4064, Batch Gradient Norm after: 22.360679478294266
Epoch 4065/10000, Prediction Accuracy = 58.03800000000001%, Loss = 0.9236099004745484
Epoch: 4065, Batch Gradient Norm: 38.03007801360486
Epoch: 4065, Batch Gradient Norm after: 22.36067780934501
Epoch 4066/10000, Prediction Accuracy = 58.02199999999999%, Loss = 0.9339464783668519
Epoch: 4066, Batch Gradient Norm: 33.98703204824001
Epoch: 4066, Batch Gradient Norm after: 22.36067901666786
Epoch 4067/10000, Prediction Accuracy = 58.03599999999999%, Loss = 0.9234283328056335
Epoch: 4067, Batch Gradient Norm: 38.029646436896435
Epoch: 4067, Batch Gradient Norm after: 22.360677502256273
Epoch 4068/10000, Prediction Accuracy = 58.019999999999996%, Loss = 0.9337811946868897
Epoch: 4068, Batch Gradient Norm: 33.98286819148901
Epoch: 4068, Batch Gradient Norm after: 22.360677928669862
Epoch 4069/10000, Prediction Accuracy = 58.029999999999994%, Loss = 0.9232394456863403
Epoch: 4069, Batch Gradient Norm: 38.02354780048109
Epoch: 4069, Batch Gradient Norm after: 22.36067744118358
Epoch 4070/10000, Prediction Accuracy = 58.019999999999996%, Loss = 0.9335951685905457
Epoch: 4070, Batch Gradient Norm: 33.97755223621571
Epoch: 4070, Batch Gradient Norm after: 22.360677266218087
Epoch 4071/10000, Prediction Accuracy = 58.02%, Loss = 0.9230616450309753
Epoch: 4071, Batch Gradient Norm: 38.016184638922184
Epoch: 4071, Batch Gradient Norm after: 22.36067662058705
Epoch 4072/10000, Prediction Accuracy = 58.028%, Loss = 0.9334065318107605
Epoch: 4072, Batch Gradient Norm: 33.97188120824322
Epoch: 4072, Batch Gradient Norm after: 22.360677026844456
Epoch 4073/10000, Prediction Accuracy = 58.024%, Loss = 0.9228774785995484
Epoch: 4073, Batch Gradient Norm: 38.00439858526141
Epoch: 4073, Batch Gradient Norm after: 22.36067704206179
Epoch 4074/10000, Prediction Accuracy = 58.032000000000004%, Loss = 0.9332126617431641
Epoch: 4074, Batch Gradient Norm: 33.96629267753245
Epoch: 4074, Batch Gradient Norm after: 22.360676692392353
Epoch 4075/10000, Prediction Accuracy = 58.028%, Loss = 0.92269926071167
Epoch: 4075, Batch Gradient Norm: 37.996816856858715
Epoch: 4075, Batch Gradient Norm after: 22.360679643365692
Epoch 4076/10000, Prediction Accuracy = 58.038%, Loss = 0.9330266237258911
Epoch: 4076, Batch Gradient Norm: 33.96043517938086
Epoch: 4076, Batch Gradient Norm after: 22.360678963994825
Epoch 4077/10000, Prediction Accuracy = 58.04%, Loss = 0.9225149750709534
Epoch: 4077, Batch Gradient Norm: 37.990212134097675
Epoch: 4077, Batch Gradient Norm after: 22.360678484049505
Epoch 4078/10000, Prediction Accuracy = 58.041999999999994%, Loss = 0.9328314065933228
Epoch: 4078, Batch Gradient Norm: 33.95635530813595
Epoch: 4078, Batch Gradient Norm after: 22.36067930440885
Epoch 4079/10000, Prediction Accuracy = 58.038%, Loss = 0.9223328590393066
Epoch: 4079, Batch Gradient Norm: 37.98415864255809
Epoch: 4079, Batch Gradient Norm after: 22.360677253180455
Epoch 4080/10000, Prediction Accuracy = 58.038%, Loss = 0.9326496839523315
Epoch: 4080, Batch Gradient Norm: 33.952106650153155
Epoch: 4080, Batch Gradient Norm after: 22.36067965619584
Epoch 4081/10000, Prediction Accuracy = 58.038%, Loss = 0.9221519827842712
Epoch: 4081, Batch Gradient Norm: 37.97773658470438
Epoch: 4081, Batch Gradient Norm after: 22.360677226715897
Epoch 4082/10000, Prediction Accuracy = 58.038%, Loss = 0.932463800907135
Epoch: 4082, Batch Gradient Norm: 33.94667901956287
Epoch: 4082, Batch Gradient Norm after: 22.360677911336406
Epoch 4083/10000, Prediction Accuracy = 58.041999999999994%, Loss = 0.9219711065292359
Epoch: 4083, Batch Gradient Norm: 37.97146927522748
Epoch: 4083, Batch Gradient Norm after: 22.360676246270607
Epoch 4084/10000, Prediction Accuracy = 58.03799999999999%, Loss = 0.9322818040847778
Epoch: 4084, Batch Gradient Norm: 33.940809289096265
Epoch: 4084, Batch Gradient Norm after: 22.36067745193297
Epoch 4085/10000, Prediction Accuracy = 58.036%, Loss = 0.9217902660369873
Epoch: 4085, Batch Gradient Norm: 37.96564046697999
Epoch: 4085, Batch Gradient Norm after: 22.360677614100258
Epoch 4086/10000, Prediction Accuracy = 58.03399999999999%, Loss = 0.9321026921272277
Epoch: 4086, Batch Gradient Norm: 33.93391423997675
Epoch: 4086, Batch Gradient Norm after: 22.36067565120486
Epoch 4087/10000, Prediction Accuracy = 58.038%, Loss = 0.9216067552566528
Epoch: 4087, Batch Gradient Norm: 37.95862396043673
Epoch: 4087, Batch Gradient Norm after: 22.36067673885181
Epoch 4088/10000, Prediction Accuracy = 58.036%, Loss = 0.9319170594215394
Epoch: 4088, Batch Gradient Norm: 33.930758456710905
Epoch: 4088, Batch Gradient Norm after: 22.36067651357844
Epoch 4089/10000, Prediction Accuracy = 58.04600000000001%, Loss = 0.9214259386062622
Epoch: 4089, Batch Gradient Norm: 37.95061602478945
Epoch: 4089, Batch Gradient Norm after: 22.360679465052804
Epoch 4090/10000, Prediction Accuracy = 58.034000000000006%, Loss = 0.9317289352416992
Epoch: 4090, Batch Gradient Norm: 33.92569262408201
Epoch: 4090, Batch Gradient Norm after: 22.36067485537276
Epoch 4091/10000, Prediction Accuracy = 58.038%, Loss = 0.9212439298629761
Epoch: 4091, Batch Gradient Norm: 37.93979203011041
Epoch: 4091, Batch Gradient Norm after: 22.360679025842277
Epoch 4092/10000, Prediction Accuracy = 58.04600000000001%, Loss = 0.9315402865409851
Epoch: 4092, Batch Gradient Norm: 33.92192079540758
Epoch: 4092, Batch Gradient Norm after: 22.36067713944848
Epoch 4093/10000, Prediction Accuracy = 58.048%, Loss = 0.9210642814636231
Epoch: 4093, Batch Gradient Norm: 37.93189771696711
Epoch: 4093, Batch Gradient Norm after: 22.360677836202434
Epoch 4094/10000, Prediction Accuracy = 58.036%, Loss = 0.9313479542732239
Epoch: 4094, Batch Gradient Norm: 33.915512514425245
Epoch: 4094, Batch Gradient Norm after: 22.360677571747296
Epoch 4095/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.9208852648735046
Epoch: 4095, Batch Gradient Norm: 37.919526333003084
Epoch: 4095, Batch Gradient Norm after: 22.360676479839448
Epoch 4096/10000, Prediction Accuracy = 58.036%, Loss = 0.9311409235000611
Epoch: 4096, Batch Gradient Norm: 33.91245890844293
Epoch: 4096, Batch Gradient Norm after: 22.36067605869854
Epoch 4097/10000, Prediction Accuracy = 58.056000000000004%, Loss = 0.9207061409950257
Epoch: 4097, Batch Gradient Norm: 37.908499815680365
Epoch: 4097, Batch Gradient Norm after: 22.36067839036835
Epoch 4098/10000, Prediction Accuracy = 58.036%, Loss = 0.9309349298477173
Epoch: 4098, Batch Gradient Norm: 33.90961812205529
Epoch: 4098, Batch Gradient Norm after: 22.360678487336926
Epoch 4099/10000, Prediction Accuracy = 58.048%, Loss = 0.9205288290977478
Epoch: 4099, Batch Gradient Norm: 37.89099831049952
Epoch: 4099, Batch Gradient Norm after: 22.360677219105636
Epoch 4100/10000, Prediction Accuracy = 58.048%, Loss = 0.9307249307632446
Epoch: 4100, Batch Gradient Norm: 33.90598864034287
Epoch: 4100, Batch Gradient Norm after: 22.360677038631486
Epoch 4101/10000, Prediction Accuracy = 58.04599999999999%, Loss = 0.9203530669212341
Epoch: 4101, Batch Gradient Norm: 37.87455103944012
Epoch: 4101, Batch Gradient Norm after: 22.360677766590165
Epoch 4102/10000, Prediction Accuracy = 58.056%, Loss = 0.9305085301399231
Epoch: 4102, Batch Gradient Norm: 33.902901918501485
Epoch: 4102, Batch Gradient Norm after: 22.36067758940565
Epoch 4103/10000, Prediction Accuracy = 58.05%, Loss = 0.9201808929443359
Epoch: 4103, Batch Gradient Norm: 37.855973781994244
Epoch: 4103, Batch Gradient Norm after: 22.360678402803057
Epoch 4104/10000, Prediction Accuracy = 58.065999999999995%, Loss = 0.9302938103675842
Epoch: 4104, Batch Gradient Norm: 33.90222721662086
Epoch: 4104, Batch Gradient Norm after: 22.360676815863666
Epoch 4105/10000, Prediction Accuracy = 58.053999999999995%, Loss = 0.920006000995636
Epoch: 4105, Batch Gradient Norm: 37.84097635879551
Epoch: 4105, Batch Gradient Norm after: 22.360680157701715
Epoch 4106/10000, Prediction Accuracy = 58.068%, Loss = 0.9300867199897767
Epoch: 4106, Batch Gradient Norm: 33.89949552775889
Epoch: 4106, Batch Gradient Norm after: 22.360677388995835
Epoch 4107/10000, Prediction Accuracy = 58.06%, Loss = 0.9198303461074829
Epoch: 4107, Batch Gradient Norm: 37.827086391212745
Epoch: 4107, Batch Gradient Norm after: 22.360676646798165
Epoch 4108/10000, Prediction Accuracy = 58.06%, Loss = 0.9298821568489075
Epoch: 4108, Batch Gradient Norm: 33.89652713801783
Epoch: 4108, Batch Gradient Norm after: 22.36067630580465
Epoch 4109/10000, Prediction Accuracy = 58.064%, Loss = 0.9196549892425537
Epoch: 4109, Batch Gradient Norm: 37.81553586111735
Epoch: 4109, Batch Gradient Norm after: 22.360678782781324
Epoch 4110/10000, Prediction Accuracy = 58.07199999999999%, Loss = 0.9296851992607117
Epoch: 4110, Batch Gradient Norm: 33.891149484560195
Epoch: 4110, Batch Gradient Norm after: 22.360677653980787
Epoch 4111/10000, Prediction Accuracy = 58.065999999999995%, Loss = 0.9194757103919983
Epoch: 4111, Batch Gradient Norm: 37.805335585067866
Epoch: 4111, Batch Gradient Norm after: 22.36067771935744
Epoch 4112/10000, Prediction Accuracy = 58.06999999999999%, Loss = 0.9294891476631164
Epoch: 4112, Batch Gradient Norm: 33.887194088582554
Epoch: 4112, Batch Gradient Norm after: 22.36067833352186
Epoch 4113/10000, Prediction Accuracy = 58.05999999999999%, Loss = 0.9193003535270691
Epoch: 4113, Batch Gradient Norm: 37.79294577634525
Epoch: 4113, Batch Gradient Norm after: 22.3606785489171
Epoch 4114/10000, Prediction Accuracy = 58.080000000000005%, Loss = 0.9292879223823547
Epoch: 4114, Batch Gradient Norm: 33.882517534990775
Epoch: 4114, Batch Gradient Norm after: 22.360676958443353
Epoch 4115/10000, Prediction Accuracy = 58.056%, Loss = 0.9191219925880432
Epoch: 4115, Batch Gradient Norm: 37.77913501547335
Epoch: 4115, Batch Gradient Norm after: 22.36067839501489
Epoch 4116/10000, Prediction Accuracy = 58.086%, Loss = 0.9290865659713745
Epoch: 4116, Batch Gradient Norm: 33.880094179799464
Epoch: 4116, Batch Gradient Norm after: 22.36067723676946
Epoch 4117/10000, Prediction Accuracy = 58.05400000000001%, Loss = 0.918949294090271
Epoch: 4117, Batch Gradient Norm: 37.76443560525846
Epoch: 4117, Batch Gradient Norm after: 22.36067936862815
Epoch 4118/10000, Prediction Accuracy = 58.09000000000001%, Loss = 0.928882646560669
Epoch: 4118, Batch Gradient Norm: 33.876479870987914
Epoch: 4118, Batch Gradient Norm after: 22.360676018839364
Epoch 4119/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.91877521276474
Epoch: 4119, Batch Gradient Norm: 37.74756241047444
Epoch: 4119, Batch Gradient Norm after: 22.360678810174548
Epoch 4120/10000, Prediction Accuracy = 58.09400000000001%, Loss = 0.9286706805229187
Epoch: 4120, Batch Gradient Norm: 33.87343524079917
Epoch: 4120, Batch Gradient Norm after: 22.360680138042003
Epoch 4121/10000, Prediction Accuracy = 58.068%, Loss = 0.9186046719551086
Epoch: 4121, Batch Gradient Norm: 37.734296016454145
Epoch: 4121, Batch Gradient Norm after: 22.360679023053194
Epoch 4122/10000, Prediction Accuracy = 58.104%, Loss = 0.928463876247406
Epoch: 4122, Batch Gradient Norm: 33.86812780028586
Epoch: 4122, Batch Gradient Norm after: 22.360677460552843
Epoch 4123/10000, Prediction Accuracy = 58.074%, Loss = 0.918430483341217
Epoch: 4123, Batch Gradient Norm: 37.7224138825414
Epoch: 4123, Batch Gradient Norm after: 22.360679981510124
Epoch 4124/10000, Prediction Accuracy = 58.104%, Loss = 0.9282658576965332
Epoch: 4124, Batch Gradient Norm: 33.86473223363245
Epoch: 4124, Batch Gradient Norm after: 22.36067551709733
Epoch 4125/10000, Prediction Accuracy = 58.077999999999996%, Loss = 0.9182558059692383
Epoch: 4125, Batch Gradient Norm: 37.708797689771195
Epoch: 4125, Batch Gradient Norm after: 22.360677954624336
Epoch 4126/10000, Prediction Accuracy = 58.1%, Loss = 0.928076195716858
Epoch: 4126, Batch Gradient Norm: 33.85807097024399
Epoch: 4126, Batch Gradient Norm after: 22.360675215026312
Epoch 4127/10000, Prediction Accuracy = 58.081999999999994%, Loss = 0.9180814862251282
Epoch: 4127, Batch Gradient Norm: 37.70332542569844
Epoch: 4127, Batch Gradient Norm after: 22.360678210568167
Epoch 4128/10000, Prediction Accuracy = 58.098%, Loss = 0.9278923749923706
Epoch: 4128, Batch Gradient Norm: 33.85339112230435
Epoch: 4128, Batch Gradient Norm after: 22.360677718923636
Epoch 4129/10000, Prediction Accuracy = 58.089999999999996%, Loss = 0.9179034352302551
Epoch: 4129, Batch Gradient Norm: 37.70043914955326
Epoch: 4129, Batch Gradient Norm after: 22.36067861226281
Epoch 4130/10000, Prediction Accuracy = 58.102%, Loss = 0.9277163147926331
Epoch: 4130, Batch Gradient Norm: 33.847612701945785
Epoch: 4130, Batch Gradient Norm after: 22.360678628348982
Epoch 4131/10000, Prediction Accuracy = 58.086%, Loss = 0.9177245378494263
Epoch: 4131, Batch Gradient Norm: 37.691164292086576
Epoch: 4131, Batch Gradient Norm after: 22.36067648074665
Epoch 4132/10000, Prediction Accuracy = 58.104%, Loss = 0.9275312304496766
Epoch: 4132, Batch Gradient Norm: 33.84212544269604
Epoch: 4132, Batch Gradient Norm after: 22.36067701719935
Epoch 4133/10000, Prediction Accuracy = 58.086%, Loss = 0.9175473332405091
Epoch: 4133, Batch Gradient Norm: 37.6885444679698
Epoch: 4133, Batch Gradient Norm after: 22.36067729274685
Epoch 4134/10000, Prediction Accuracy = 58.11600000000001%, Loss = 0.9273554921150208
Epoch: 4134, Batch Gradient Norm: 33.83533238307722
Epoch: 4134, Batch Gradient Norm after: 22.36067921005527
Epoch 4135/10000, Prediction Accuracy = 58.084%, Loss = 0.9173709869384765
Epoch: 4135, Batch Gradient Norm: 37.683517199264955
Epoch: 4135, Batch Gradient Norm after: 22.36067764991764
Epoch 4136/10000, Prediction Accuracy = 58.126%, Loss = 0.9271814465522766
Epoch: 4136, Batch Gradient Norm: 33.82788651572057
Epoch: 4136, Batch Gradient Norm after: 22.360678409629504
Epoch 4137/10000, Prediction Accuracy = 58.089999999999996%, Loss = 0.9171930432319642
Epoch: 4137, Batch Gradient Norm: 37.67906268380439
Epoch: 4137, Batch Gradient Norm after: 22.36067565028836
Epoch 4138/10000, Prediction Accuracy = 58.126%, Loss = 0.9270098209381104
Epoch: 4138, Batch Gradient Norm: 33.822559905480674
Epoch: 4138, Batch Gradient Norm after: 22.360678966006436
Epoch 4139/10000, Prediction Accuracy = 58.098%, Loss = 0.9170148253440857
Epoch: 4139, Batch Gradient Norm: 37.67276257980894
Epoch: 4139, Batch Gradient Norm after: 22.360675935627853
Epoch 4140/10000, Prediction Accuracy = 58.138%, Loss = 0.9268372178077697
Epoch: 4140, Batch Gradient Norm: 33.81614264381432
Epoch: 4140, Batch Gradient Norm after: 22.360675611201444
Epoch 4141/10000, Prediction Accuracy = 58.112%, Loss = 0.9168392539024353
Epoch: 4141, Batch Gradient Norm: 37.66535442469152
Epoch: 4141, Batch Gradient Norm after: 22.360676398999374
Epoch 4142/10000, Prediction Accuracy = 58.141999999999996%, Loss = 0.9266472935676575
Epoch: 4142, Batch Gradient Norm: 33.811071414711954
Epoch: 4142, Batch Gradient Norm after: 22.360679569329232
Epoch 4143/10000, Prediction Accuracy = 58.120000000000005%, Loss = 0.9166632533073426
Epoch: 4143, Batch Gradient Norm: 37.656598827712074
Epoch: 4143, Batch Gradient Norm after: 22.360676413352174
Epoch 4144/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9264588356018066
Epoch: 4144, Batch Gradient Norm: 33.80697583496633
Epoch: 4144, Batch Gradient Norm after: 22.36067758674953
Epoch 4145/10000, Prediction Accuracy = 58.126%, Loss = 0.9164865136146545
Epoch: 4145, Batch Gradient Norm: 37.64633979374831
Epoch: 4145, Batch Gradient Norm after: 22.360677542189368
Epoch 4146/10000, Prediction Accuracy = 58.156000000000006%, Loss = 0.9262678027153015
Epoch: 4146, Batch Gradient Norm: 33.80317600080239
Epoch: 4146, Batch Gradient Norm after: 22.360679162513144
Epoch 4147/10000, Prediction Accuracy = 58.116%, Loss = 0.9163128495216369
Epoch: 4147, Batch Gradient Norm: 37.63818149873173
Epoch: 4147, Batch Gradient Norm after: 22.360678009957397
Epoch 4148/10000, Prediction Accuracy = 58.15599999999999%, Loss = 0.9260839343070983
Epoch: 4148, Batch Gradient Norm: 33.796381446711926
Epoch: 4148, Batch Gradient Norm after: 22.36067777690217
Epoch 4149/10000, Prediction Accuracy = 58.112%, Loss = 0.9161316752433777
Epoch: 4149, Batch Gradient Norm: 37.6353537726119
Epoch: 4149, Batch Gradient Norm after: 22.36067624839273
Epoch 4150/10000, Prediction Accuracy = 58.148%, Loss = 0.9259124636650086
Epoch: 4150, Batch Gradient Norm: 33.79142425296472
Epoch: 4150, Batch Gradient Norm after: 22.36067833134136
Epoch 4151/10000, Prediction Accuracy = 58.105999999999995%, Loss = 0.915953254699707
Epoch: 4151, Batch Gradient Norm: 37.63197515764702
Epoch: 4151, Batch Gradient Norm after: 22.360677291309383
Epoch 4152/10000, Prediction Accuracy = 58.152%, Loss = 0.9257386803627015
Epoch: 4152, Batch Gradient Norm: 33.784806895046145
Epoch: 4152, Batch Gradient Norm after: 22.360676131351905
Epoch 4153/10000, Prediction Accuracy = 58.105999999999995%, Loss = 0.9157734394073487
Epoch: 4153, Batch Gradient Norm: 37.63010215937978
Epoch: 4153, Batch Gradient Norm after: 22.36067862232268
Epoch 4154/10000, Prediction Accuracy = 58.148%, Loss = 0.9255710482597351
Epoch: 4154, Batch Gradient Norm: 33.778575979301536
Epoch: 4154, Batch Gradient Norm after: 22.360677240130315
Epoch 4155/10000, Prediction Accuracy = 58.102%, Loss = 0.9155948638916016
Epoch: 4155, Batch Gradient Norm: 37.62929537375103
Epoch: 4155, Batch Gradient Norm after: 22.360676493799918
Epoch 4156/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9254082083702088
Epoch: 4156, Batch Gradient Norm: 33.77106520801007
Epoch: 4156, Batch Gradient Norm after: 22.360676928901736
Epoch 4157/10000, Prediction Accuracy = 58.10600000000001%, Loss = 0.9154128432273865
Epoch: 4157, Batch Gradient Norm: 37.62958471179636
Epoch: 4157, Batch Gradient Norm after: 22.36067358958755
Epoch 4158/10000, Prediction Accuracy = 58.144000000000005%, Loss = 0.9252479553222657
Epoch: 4158, Batch Gradient Norm: 33.76174516847122
Epoch: 4158, Batch Gradient Norm after: 22.360678136197194
Epoch 4159/10000, Prediction Accuracy = 58.117999999999995%, Loss = 0.9152372717857361
Epoch: 4159, Batch Gradient Norm: 37.63042924821871
Epoch: 4159, Batch Gradient Norm after: 22.36067891727984
Epoch 4160/10000, Prediction Accuracy = 58.14%, Loss = 0.9250874996185303
Epoch: 4160, Batch Gradient Norm: 33.75476021552775
Epoch: 4160, Batch Gradient Norm after: 22.36067817050309
Epoch 4161/10000, Prediction Accuracy = 58.117999999999995%, Loss = 0.9150583028793335
Epoch: 4161, Batch Gradient Norm: 37.627214617769894
Epoch: 4161, Batch Gradient Norm after: 22.360680017108304
Epoch 4162/10000, Prediction Accuracy = 58.15%, Loss = 0.9249227404594421
Epoch: 4162, Batch Gradient Norm: 33.748277788992965
Epoch: 4162, Batch Gradient Norm after: 22.36067884919374
Epoch 4163/10000, Prediction Accuracy = 58.116%, Loss = 0.9148805499076843
Epoch: 4163, Batch Gradient Norm: 37.624822120557084
Epoch: 4163, Batch Gradient Norm after: 22.360678374316254
Epoch 4164/10000, Prediction Accuracy = 58.152%, Loss = 0.9247587203979493
Epoch: 4164, Batch Gradient Norm: 33.74151043735929
Epoch: 4164, Batch Gradient Norm after: 22.360677660170893
Epoch 4165/10000, Prediction Accuracy = 58.11800000000001%, Loss = 0.9147020936012268
Epoch: 4165, Batch Gradient Norm: 37.624035801732646
Epoch: 4165, Batch Gradient Norm after: 22.36067690794596
Epoch 4166/10000, Prediction Accuracy = 58.160000000000004%, Loss = 0.9245863914489746
Epoch: 4166, Batch Gradient Norm: 33.73518281434038
Epoch: 4166, Batch Gradient Norm after: 22.36067684628284
Epoch 4167/10000, Prediction Accuracy = 58.11400000000001%, Loss = 0.9145242094993591
Epoch: 4167, Batch Gradient Norm: 37.61907572347792
Epoch: 4167, Batch Gradient Norm after: 22.360677377477067
Epoch 4168/10000, Prediction Accuracy = 58.15599999999999%, Loss = 0.9244219422340393
Epoch: 4168, Batch Gradient Norm: 33.73006832555134
Epoch: 4168, Batch Gradient Norm after: 22.36067637668593
Epoch 4169/10000, Prediction Accuracy = 58.116%, Loss = 0.9143483757972717
Epoch: 4169, Batch Gradient Norm: 37.61283273357215
Epoch: 4169, Batch Gradient Norm after: 22.36067671096689
Epoch 4170/10000, Prediction Accuracy = 58.164%, Loss = 0.9242456793785095
Epoch: 4170, Batch Gradient Norm: 33.72530847322307
Epoch: 4170, Batch Gradient Norm after: 22.360677729382136
Epoch 4171/10000, Prediction Accuracy = 58.129999999999995%, Loss = 0.9141739130020141
Epoch: 4171, Batch Gradient Norm: 37.60698032617898
Epoch: 4171, Batch Gradient Norm after: 22.36067788716526
Epoch 4172/10000, Prediction Accuracy = 58.16600000000001%, Loss = 0.9240684390068055
Epoch: 4172, Batch Gradient Norm: 33.719333700917254
Epoch: 4172, Batch Gradient Norm after: 22.360677197274132
Epoch 4173/10000, Prediction Accuracy = 58.13199999999999%, Loss = 0.9139970183372498
Epoch: 4173, Batch Gradient Norm: 37.602458035512214
Epoch: 4173, Batch Gradient Norm after: 22.360679110968597
Epoch 4174/10000, Prediction Accuracy = 58.172000000000004%, Loss = 0.9238937616348266
Epoch: 4174, Batch Gradient Norm: 33.71588431865
Epoch: 4174, Batch Gradient Norm after: 22.36067612935034
Epoch 4175/10000, Prediction Accuracy = 58.13399999999999%, Loss = 0.9138217091560363
Epoch: 4175, Batch Gradient Norm: 37.59377807327737
Epoch: 4175, Batch Gradient Norm after: 22.36067688329135
Epoch 4176/10000, Prediction Accuracy = 58.17%, Loss = 0.923708152770996
Epoch: 4176, Batch Gradient Norm: 33.710579353597566
Epoch: 4176, Batch Gradient Norm after: 22.360676148272415
Epoch 4177/10000, Prediction Accuracy = 58.13399999999999%, Loss = 0.9136526226997376
Epoch: 4177, Batch Gradient Norm: 37.58923704604799
Epoch: 4177, Batch Gradient Norm after: 22.360677055453625
Epoch 4178/10000, Prediction Accuracy = 58.178%, Loss = 0.9235335111618042
Epoch: 4178, Batch Gradient Norm: 33.7075312320708
Epoch: 4178, Batch Gradient Norm after: 22.36067736321138
Epoch 4179/10000, Prediction Accuracy = 58.146%, Loss = 0.9134795427322387
Epoch: 4179, Batch Gradient Norm: 37.58111601737462
Epoch: 4179, Batch Gradient Norm after: 22.360677927898585
Epoch 4180/10000, Prediction Accuracy = 58.178%, Loss = 0.9233478784561158
Epoch: 4180, Batch Gradient Norm: 33.703368074505164
Epoch: 4180, Batch Gradient Norm after: 22.360674384602913
Epoch 4181/10000, Prediction Accuracy = 58.152%, Loss = 0.9133084297180176
Epoch: 4181, Batch Gradient Norm: 37.57252696674751
Epoch: 4181, Batch Gradient Norm after: 22.36067810048996
Epoch 4182/10000, Prediction Accuracy = 58.178%, Loss = 0.9231630444526673
Epoch: 4182, Batch Gradient Norm: 33.697360877931736
Epoch: 4182, Batch Gradient Norm after: 22.36067384241008
Epoch 4183/10000, Prediction Accuracy = 58.152%, Loss = 0.9131356477737427
Epoch: 4183, Batch Gradient Norm: 37.56397074829433
Epoch: 4183, Batch Gradient Norm after: 22.360676951872346
Epoch 4184/10000, Prediction Accuracy = 58.178%, Loss = 0.922980010509491
Epoch: 4184, Batch Gradient Norm: 33.69449050871043
Epoch: 4184, Batch Gradient Norm after: 22.360676471922666
Epoch 4185/10000, Prediction Accuracy = 58.148%, Loss = 0.912966275215149
Epoch: 4185, Batch Gradient Norm: 37.55071667379842
Epoch: 4185, Batch Gradient Norm after: 22.360677121004578
Epoch 4186/10000, Prediction Accuracy = 58.181999999999995%, Loss = 0.9227828025817871
Epoch: 4186, Batch Gradient Norm: 33.69064675984576
Epoch: 4186, Batch Gradient Norm after: 22.360677050425267
Epoch 4187/10000, Prediction Accuracy = 58.14%, Loss = 0.9127987861633301
Epoch: 4187, Batch Gradient Norm: 37.5409026866745
Epoch: 4187, Batch Gradient Norm after: 22.36067682370343
Epoch 4188/10000, Prediction Accuracy = 58.17999999999999%, Loss = 0.9225959777832031
Epoch: 4188, Batch Gradient Norm: 33.68596745814738
Epoch: 4188, Batch Gradient Norm after: 22.36067494112496
Epoch 4189/10000, Prediction Accuracy = 58.141999999999996%, Loss = 0.9126265287399292
Epoch: 4189, Batch Gradient Norm: 37.53197290802479
Epoch: 4189, Batch Gradient Norm after: 22.360677639913106
Epoch 4190/10000, Prediction Accuracy = 58.176%, Loss = 0.9224147796630859
Epoch: 4190, Batch Gradient Norm: 33.68164924284267
Epoch: 4190, Batch Gradient Norm after: 22.360675866835578
Epoch 4191/10000, Prediction Accuracy = 58.14399999999999%, Loss = 0.912452507019043
Epoch: 4191, Batch Gradient Norm: 37.527144361454994
Epoch: 4191, Batch Gradient Norm after: 22.36067766351644
Epoch 4192/10000, Prediction Accuracy = 58.178%, Loss = 0.922239089012146
Epoch: 4192, Batch Gradient Norm: 33.676946571888514
Epoch: 4192, Batch Gradient Norm after: 22.360676987255616
Epoch 4193/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9122766852378845
Epoch: 4193, Batch Gradient Norm: 37.52431908864299
Epoch: 4193, Batch Gradient Norm after: 22.360676944248173
Epoch 4194/10000, Prediction Accuracy = 58.18800000000001%, Loss = 0.9220642924308777
Epoch: 4194, Batch Gradient Norm: 33.671235096791854
Epoch: 4194, Batch Gradient Norm after: 22.36067794948094
Epoch 4195/10000, Prediction Accuracy = 58.146%, Loss = 0.912105131149292
Epoch: 4195, Batch Gradient Norm: 37.521212298578014
Epoch: 4195, Batch Gradient Norm after: 22.360678542330337
Epoch 4196/10000, Prediction Accuracy = 58.194%, Loss = 0.9218941688537597
Epoch: 4196, Batch Gradient Norm: 33.664987958356136
Epoch: 4196, Batch Gradient Norm after: 22.36067650034144
Epoch 4197/10000, Prediction Accuracy = 58.14399999999999%, Loss = 0.9119311809539795
Epoch: 4197, Batch Gradient Norm: 37.51890573456632
Epoch: 4197, Batch Gradient Norm after: 22.360675627663223
Epoch 4198/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9217251300811767
Epoch: 4198, Batch Gradient Norm: 33.659162773699805
Epoch: 4198, Batch Gradient Norm after: 22.360678275902377
Epoch 4199/10000, Prediction Accuracy = 58.138%, Loss = 0.911755633354187
Epoch: 4199, Batch Gradient Norm: 37.515848392378004
Epoch: 4199, Batch Gradient Norm after: 22.360677978613964
Epoch 4200/10000, Prediction Accuracy = 58.2%, Loss = 0.9215591549873352
Epoch: 4200, Batch Gradient Norm: 33.65402315457824
Epoch: 4200, Batch Gradient Norm after: 22.36067837877104
Epoch 4201/10000, Prediction Accuracy = 58.14000000000001%, Loss = 0.9115795254707336
Epoch: 4201, Batch Gradient Norm: 37.511219149018714
Epoch: 4201, Batch Gradient Norm after: 22.360677274033474
Epoch 4202/10000, Prediction Accuracy = 58.193999999999996%, Loss = 0.9213838815689087
Epoch: 4202, Batch Gradient Norm: 33.648092584543804
Epoch: 4202, Batch Gradient Norm after: 22.3606780191272
Epoch 4203/10000, Prediction Accuracy = 58.138%, Loss = 0.9114066243171692
Epoch: 4203, Batch Gradient Norm: 37.511537711215425
Epoch: 4203, Batch Gradient Norm after: 22.36067846711373
Epoch 4204/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9212190628051757
Epoch: 4204, Batch Gradient Norm: 33.64090484328342
Epoch: 4204, Batch Gradient Norm after: 22.360676964242188
Epoch 4205/10000, Prediction Accuracy = 58.141999999999996%, Loss = 0.9112334012985229
Epoch: 4205, Batch Gradient Norm: 37.5082438525907
Epoch: 4205, Batch Gradient Norm after: 22.360678153301087
Epoch 4206/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9210516452789307
Epoch: 4206, Batch Gradient Norm: 33.6356360961085
Epoch: 4206, Batch Gradient Norm after: 22.360678767875676
Epoch 4207/10000, Prediction Accuracy = 58.141999999999996%, Loss = 0.9110574126243591
Epoch: 4207, Batch Gradient Norm: 37.50172335232337
Epoch: 4207, Batch Gradient Norm after: 22.360677316405802
Epoch 4208/10000, Prediction Accuracy = 58.202%, Loss = 0.92087641954422
Epoch: 4208, Batch Gradient Norm: 33.63021863607309
Epoch: 4208, Batch Gradient Norm after: 22.360674877240577
Epoch 4209/10000, Prediction Accuracy = 58.14399999999999%, Loss = 0.9108842730522155
Epoch: 4209, Batch Gradient Norm: 37.496500196603385
Epoch: 4209, Batch Gradient Norm after: 22.36067831549095
Epoch 4210/10000, Prediction Accuracy = 58.194%, Loss = 0.9206999778747559
Epoch: 4210, Batch Gradient Norm: 33.626644641199995
Epoch: 4210, Batch Gradient Norm after: 22.360678198314215
Epoch 4211/10000, Prediction Accuracy = 58.138%, Loss = 0.9107113838195801
Epoch: 4211, Batch Gradient Norm: 37.48871370540956
Epoch: 4211, Batch Gradient Norm after: 22.360678027354968
Epoch 4212/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9205182433128357
Epoch: 4212, Batch Gradient Norm: 33.62109574916516
Epoch: 4212, Batch Gradient Norm after: 22.360677723422096
Epoch 4213/10000, Prediction Accuracy = 58.13199999999999%, Loss = 0.9105408430099488
Epoch: 4213, Batch Gradient Norm: 37.4804139755484
Epoch: 4213, Batch Gradient Norm after: 22.36068032231971
Epoch 4214/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9203323125839233
Epoch: 4214, Batch Gradient Norm: 33.618800287155075
Epoch: 4214, Batch Gradient Norm after: 22.3606765297996
Epoch 4215/10000, Prediction Accuracy = 58.14%, Loss = 0.9103746294975281
Epoch: 4215, Batch Gradient Norm: 37.470601511364116
Epoch: 4215, Batch Gradient Norm after: 22.360676528227444
Epoch 4216/10000, Prediction Accuracy = 58.202%, Loss = 0.9201478242874146
Epoch: 4216, Batch Gradient Norm: 33.615296448858494
Epoch: 4216, Batch Gradient Norm after: 22.360678767300815
Epoch 4217/10000, Prediction Accuracy = 58.14%, Loss = 0.9102049350738526
Epoch: 4217, Batch Gradient Norm: 37.459124420257545
Epoch: 4217, Batch Gradient Norm after: 22.36067596747378
Epoch 4218/10000, Prediction Accuracy = 58.206%, Loss = 0.9199582099914551
Epoch: 4218, Batch Gradient Norm: 33.611122481648046
Epoch: 4218, Batch Gradient Norm after: 22.360676348419233
Epoch 4219/10000, Prediction Accuracy = 58.138%, Loss = 0.9100344061851502
Epoch: 4219, Batch Gradient Norm: 37.45024508175135
Epoch: 4219, Batch Gradient Norm after: 22.360675737645032
Epoch 4220/10000, Prediction Accuracy = 58.2%, Loss = 0.9197725653648376
Epoch: 4220, Batch Gradient Norm: 33.60565011612816
Epoch: 4220, Batch Gradient Norm after: 22.360676466866305
Epoch 4221/10000, Prediction Accuracy = 58.144000000000005%, Loss = 0.9098617672920227
Epoch: 4221, Batch Gradient Norm: 37.44553795056469
Epoch: 4221, Batch Gradient Norm after: 22.360676648652873
Epoch 4222/10000, Prediction Accuracy = 58.20400000000001%, Loss = 0.9195956110954284
Epoch: 4222, Batch Gradient Norm: 33.60111564444803
Epoch: 4222, Batch Gradient Norm after: 22.36067797490273
Epoch 4223/10000, Prediction Accuracy = 58.152%, Loss = 0.9096919298171997
Epoch: 4223, Batch Gradient Norm: 37.433164618495915
Epoch: 4223, Batch Gradient Norm after: 22.36067704803105
Epoch 4224/10000, Prediction Accuracy = 58.21%, Loss = 0.9194036483764648
Epoch: 4224, Batch Gradient Norm: 33.59763715524123
Epoch: 4224, Batch Gradient Norm after: 22.360676887358224
Epoch 4225/10000, Prediction Accuracy = 58.164%, Loss = 0.9095256686210632
Epoch: 4225, Batch Gradient Norm: 37.42659702600003
Epoch: 4225, Batch Gradient Norm after: 22.360677423353515
Epoch 4226/10000, Prediction Accuracy = 58.220000000000006%, Loss = 0.9192268013954162
Epoch: 4226, Batch Gradient Norm: 33.59374530035146
Epoch: 4226, Batch Gradient Norm after: 22.360675999699666
Epoch 4227/10000, Prediction Accuracy = 58.172000000000004%, Loss = 0.9093557953834533
Epoch: 4227, Batch Gradient Norm: 37.41607146606508
Epoch: 4227, Batch Gradient Norm after: 22.360677976820746
Epoch 4228/10000, Prediction Accuracy = 58.222%, Loss = 0.9190483093261719
Epoch: 4228, Batch Gradient Norm: 33.5891845218995
Epoch: 4228, Batch Gradient Norm after: 22.36067585979702
Epoch 4229/10000, Prediction Accuracy = 58.17999999999999%, Loss = 0.9091861724853516
Epoch: 4229, Batch Gradient Norm: 37.40779661828917
Epoch: 4229, Batch Gradient Norm after: 22.360678630737883
Epoch 4230/10000, Prediction Accuracy = 58.23%, Loss = 0.918869137763977
Epoch: 4230, Batch Gradient Norm: 33.58451853231096
Epoch: 4230, Batch Gradient Norm after: 22.360675481520243
Epoch 4231/10000, Prediction Accuracy = 58.17999999999999%, Loss = 0.9090156435966492
Epoch: 4231, Batch Gradient Norm: 37.401451659297166
Epoch: 4231, Batch Gradient Norm after: 22.360678000621224
Epoch 4232/10000, Prediction Accuracy = 58.236000000000004%, Loss = 0.9186883807182312
Epoch: 4232, Batch Gradient Norm: 33.58052526149187
Epoch: 4232, Batch Gradient Norm after: 22.36067384944975
Epoch 4233/10000, Prediction Accuracy = 58.186%, Loss = 0.9088449954986573
Epoch: 4233, Batch Gradient Norm: 37.39417519702278
Epoch: 4233, Batch Gradient Norm after: 22.36067724796776
Epoch 4234/10000, Prediction Accuracy = 58.234%, Loss = 0.9185096144676208
Epoch: 4234, Batch Gradient Norm: 33.577814049069296
Epoch: 4234, Batch Gradient Norm after: 22.360677268946457
Epoch 4235/10000, Prediction Accuracy = 58.188%, Loss = 0.9086780786514282
Epoch: 4235, Batch Gradient Norm: 37.38388539762462
Epoch: 4235, Batch Gradient Norm after: 22.360675277129506
Epoch 4236/10000, Prediction Accuracy = 58.232000000000006%, Loss = 0.9183196544647216
Epoch: 4236, Batch Gradient Norm: 33.57498933303232
Epoch: 4236, Batch Gradient Norm after: 22.36067695729218
Epoch 4237/10000, Prediction Accuracy = 58.19%, Loss = 0.9085142731666564
Epoch: 4237, Batch Gradient Norm: 37.36887012954473
Epoch: 4237, Batch Gradient Norm after: 22.360676750162874
Epoch 4238/10000, Prediction Accuracy = 58.230000000000004%, Loss = 0.9181268453598023
Epoch: 4238, Batch Gradient Norm: 33.573020130317964
Epoch: 4238, Batch Gradient Norm after: 22.360675603707186
Epoch 4239/10000, Prediction Accuracy = 58.193999999999996%, Loss = 0.9083468556404114
Epoch: 4239, Batch Gradient Norm: 37.35391652233638
Epoch: 4239, Batch Gradient Norm after: 22.36067537169281
Epoch 4240/10000, Prediction Accuracy = 58.236000000000004%, Loss = 0.9179259657859802
Epoch: 4240, Batch Gradient Norm: 33.571482392248974
Epoch: 4240, Batch Gradient Norm after: 22.360678483755688
Epoch 4241/10000, Prediction Accuracy = 58.214%, Loss = 0.9081815719604492
Epoch: 4241, Batch Gradient Norm: 37.34002476637113
Epoch: 4241, Batch Gradient Norm after: 22.360676376490602
Epoch 4242/10000, Prediction Accuracy = 58.238%, Loss = 0.9177257061004639
Epoch: 4242, Batch Gradient Norm: 33.569953194624716
Epoch: 4242, Batch Gradient Norm after: 22.360676106969645
Epoch 4243/10000, Prediction Accuracy = 58.212%, Loss = 0.9080190062522888
Epoch: 4243, Batch Gradient Norm: 37.32455476852013
Epoch: 4243, Batch Gradient Norm after: 22.360676983843646
Epoch 4244/10000, Prediction Accuracy = 58.234%, Loss = 0.9175302028656006
Epoch: 4244, Batch Gradient Norm: 33.56793649021147
Epoch: 4244, Batch Gradient Norm after: 22.360678569836555
Epoch 4245/10000, Prediction Accuracy = 58.21600000000001%, Loss = 0.9078551530838013
Epoch: 4245, Batch Gradient Norm: 37.310069346171694
Epoch: 4245, Batch Gradient Norm after: 22.360675293338634
Epoch 4246/10000, Prediction Accuracy = 58.24400000000001%, Loss = 0.9173370838165283
Epoch: 4246, Batch Gradient Norm: 33.568178238990726
Epoch: 4246, Batch Gradient Norm after: 22.360678271101953
Epoch 4247/10000, Prediction Accuracy = 58.21%, Loss = 0.9076866745948792
Epoch: 4247, Batch Gradient Norm: 37.293287629679625
Epoch: 4247, Batch Gradient Norm after: 22.360675762829974
Epoch 4248/10000, Prediction Accuracy = 58.24000000000001%, Loss = 0.9171333312988281
Epoch: 4248, Batch Gradient Norm: 33.56731870044885
Epoch: 4248, Batch Gradient Norm after: 22.360678683646576
Epoch 4249/10000, Prediction Accuracy = 58.21999999999999%, Loss = 0.907522976398468
Epoch: 4249, Batch Gradient Norm: 37.278291935239295
Epoch: 4249, Batch Gradient Norm after: 22.36067605875171
Epoch 4250/10000, Prediction Accuracy = 58.239999999999995%, Loss = 0.9169363379478455
Epoch: 4250, Batch Gradient Norm: 33.56478358539157
Epoch: 4250, Batch Gradient Norm after: 22.36067846408715
Epoch 4251/10000, Prediction Accuracy = 58.217999999999996%, Loss = 0.9073645353317261
Epoch: 4251, Batch Gradient Norm: 37.26356115144028
Epoch: 4251, Batch Gradient Norm after: 22.360675854931674
Epoch 4252/10000, Prediction Accuracy = 58.25599999999999%, Loss = 0.9167408108711242
Epoch: 4252, Batch Gradient Norm: 33.563372905241245
Epoch: 4252, Batch Gradient Norm after: 22.360677319511538
Epoch 4253/10000, Prediction Accuracy = 58.218%, Loss = 0.9072021126747132
Epoch: 4253, Batch Gradient Norm: 37.250963139629754
Epoch: 4253, Batch Gradient Norm after: 22.360674741598185
Epoch 4254/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.9165566682815551
Epoch: 4254, Batch Gradient Norm: 33.56079795796758
Epoch: 4254, Batch Gradient Norm after: 22.360676991935687
Epoch 4255/10000, Prediction Accuracy = 58.22399999999999%, Loss = 0.9070380687713623
Epoch: 4255, Batch Gradient Norm: 37.237438289664404
Epoch: 4255, Batch Gradient Norm after: 22.36067703509172
Epoch 4256/10000, Prediction Accuracy = 58.263999999999996%, Loss = 0.9163655757904052
Epoch: 4256, Batch Gradient Norm: 33.55985643234263
Epoch: 4256, Batch Gradient Norm after: 22.360676258465386
Epoch 4257/10000, Prediction Accuracy = 58.232000000000006%, Loss = 0.9068784356117249
Epoch: 4257, Batch Gradient Norm: 37.222619391766116
Epoch: 4257, Batch Gradient Norm after: 22.360675709992705
Epoch 4258/10000, Prediction Accuracy = 58.258%, Loss = 0.9161708116531372
Epoch: 4258, Batch Gradient Norm: 33.55741463781042
Epoch: 4258, Batch Gradient Norm after: 22.360678254642682
Epoch 4259/10000, Prediction Accuracy = 58.224000000000004%, Loss = 0.9067198753356933
Epoch: 4259, Batch Gradient Norm: 37.20879165423008
Epoch: 4259, Batch Gradient Norm after: 22.360677370800115
Epoch 4260/10000, Prediction Accuracy = 58.26800000000001%, Loss = 0.9159729719161988
Epoch: 4260, Batch Gradient Norm: 33.555866383539545
Epoch: 4260, Batch Gradient Norm after: 22.36067821799814
Epoch 4261/10000, Prediction Accuracy = 58.232000000000006%, Loss = 0.9065568685531616
Epoch: 4261, Batch Gradient Norm: 37.194253105522485
Epoch: 4261, Batch Gradient Norm after: 22.360676040679355
Epoch 4262/10000, Prediction Accuracy = 58.274%, Loss = 0.915778648853302
Epoch: 4262, Batch Gradient Norm: 33.55559193997042
Epoch: 4262, Batch Gradient Norm after: 22.36067677179441
Epoch 4263/10000, Prediction Accuracy = 58.23%, Loss = 0.9063964009284973
Epoch: 4263, Batch Gradient Norm: 37.17876916832256
Epoch: 4263, Batch Gradient Norm after: 22.360676588758032
Epoch 4264/10000, Prediction Accuracy = 58.275999999999996%, Loss = 0.9155840516090393
Epoch: 4264, Batch Gradient Norm: 33.55206914166073
Epoch: 4264, Batch Gradient Norm after: 22.360678242357757
Epoch 4265/10000, Prediction Accuracy = 58.239999999999995%, Loss = 0.9062361240386962
Epoch: 4265, Batch Gradient Norm: 37.16977890796441
Epoch: 4265, Batch Gradient Norm after: 22.360675769945225
Epoch 4266/10000, Prediction Accuracy = 58.286%, Loss = 0.9154086709022522
Epoch: 4266, Batch Gradient Norm: 33.548567706994724
Epoch: 4266, Batch Gradient Norm after: 22.360677271368047
Epoch 4267/10000, Prediction Accuracy = 58.25%, Loss = 0.9060713648796082
Epoch: 4267, Batch Gradient Norm: 37.160866364534954
Epoch: 4267, Batch Gradient Norm after: 22.36067712418444
Epoch 4268/10000, Prediction Accuracy = 58.279999999999994%, Loss = 0.9152244210243226
Epoch: 4268, Batch Gradient Norm: 33.54686427748238
Epoch: 4268, Batch Gradient Norm after: 22.360676546941793
Epoch 4269/10000, Prediction Accuracy = 58.25599999999999%, Loss = 0.9059064507484436
Epoch: 4269, Batch Gradient Norm: 37.15050094450769
Epoch: 4269, Batch Gradient Norm after: 22.36067607574223
Epoch 4270/10000, Prediction Accuracy = 58.28399999999999%, Loss = 0.9150447010993957
Epoch: 4270, Batch Gradient Norm: 33.540830157402375
Epoch: 4270, Batch Gradient Norm after: 22.3606771794306
Epoch 4271/10000, Prediction Accuracy = 58.266%, Loss = 0.9057438492774963
Epoch: 4271, Batch Gradient Norm: 37.14612851183205
Epoch: 4271, Batch Gradient Norm after: 22.360676145143444
Epoch 4272/10000, Prediction Accuracy = 58.286%, Loss = 0.9148754954338074
Epoch: 4272, Batch Gradient Norm: 33.536639449772004
Epoch: 4272, Batch Gradient Norm after: 22.36067643745684
Epoch 4273/10000, Prediction Accuracy = 58.269999999999996%, Loss = 0.9055718541145324
Epoch: 4273, Batch Gradient Norm: 37.14493744912175
Epoch: 4273, Batch Gradient Norm after: 22.360674939839853
Epoch 4274/10000, Prediction Accuracy = 58.290000000000006%, Loss = 0.9147260308265686
Epoch: 4274, Batch Gradient Norm: 33.52994641905918
Epoch: 4274, Batch Gradient Norm after: 22.360677480079886
Epoch 4275/10000, Prediction Accuracy = 58.27199999999999%, Loss = 0.9054009079933166
Epoch: 4275, Batch Gradient Norm: 37.14147067550922
Epoch: 4275, Batch Gradient Norm after: 22.36067563806981
Epoch 4276/10000, Prediction Accuracy = 58.286%, Loss = 0.914563000202179
Epoch: 4276, Batch Gradient Norm: 33.522319161503894
Epoch: 4276, Batch Gradient Norm after: 22.36067592822757
Epoch 4277/10000, Prediction Accuracy = 58.278%, Loss = 0.9052322983741761
Epoch: 4277, Batch Gradient Norm: 37.13794066278991
Epoch: 4277, Batch Gradient Norm after: 22.360676058472784
Epoch 4278/10000, Prediction Accuracy = 58.284000000000006%, Loss = 0.914402186870575
Epoch: 4278, Batch Gradient Norm: 33.51513013878778
Epoch: 4278, Batch Gradient Norm after: 22.36067850152967
Epoch 4279/10000, Prediction Accuracy = 58.282000000000004%, Loss = 0.905063271522522
Epoch: 4279, Batch Gradient Norm: 37.132363181504104
Epoch: 4279, Batch Gradient Norm after: 22.3606765754896
Epoch 4280/10000, Prediction Accuracy = 58.286%, Loss = 0.9142404913902282
Epoch: 4280, Batch Gradient Norm: 33.5108223086133
Epoch: 4280, Batch Gradient Norm after: 22.3606785659946
Epoch 4281/10000, Prediction Accuracy = 58.282000000000004%, Loss = 0.9048952460289001
Epoch: 4281, Batch Gradient Norm: 37.13125046142786
Epoch: 4281, Batch Gradient Norm after: 22.36067679504433
Epoch 4282/10000, Prediction Accuracy = 58.279999999999994%, Loss = 0.9140856981277465
Epoch: 4282, Batch Gradient Norm: 33.50173929860588
Epoch: 4282, Batch Gradient Norm after: 22.36067781488286
Epoch 4283/10000, Prediction Accuracy = 58.288%, Loss = 0.9047239422798157
Epoch: 4283, Batch Gradient Norm: 37.13236957539971
Epoch: 4283, Batch Gradient Norm after: 22.360675864662234
Epoch 4284/10000, Prediction Accuracy = 58.275999999999996%, Loss = 0.9139310956001282
Epoch: 4284, Batch Gradient Norm: 33.49320055390278
Epoch: 4284, Batch Gradient Norm after: 22.3606804139156
Epoch 4285/10000, Prediction Accuracy = 58.294000000000004%, Loss = 0.9045499801635742
Epoch: 4285, Batch Gradient Norm: 37.13322702797246
Epoch: 4285, Batch Gradient Norm after: 22.360677592269944
Epoch 4286/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9137813687324524
Epoch: 4286, Batch Gradient Norm: 33.485016644766404
Epoch: 4286, Batch Gradient Norm after: 22.360676929411085
Epoch 4287/10000, Prediction Accuracy = 58.30799999999999%, Loss = 0.904378068447113
Epoch: 4287, Batch Gradient Norm: 37.13186527334632
Epoch: 4287, Batch Gradient Norm after: 22.36067815425514
Epoch 4288/10000, Prediction Accuracy = 58.278%, Loss = 0.9136311054229737
Epoch: 4288, Batch Gradient Norm: 33.47738942362653
Epoch: 4288, Batch Gradient Norm after: 22.360678524570712
Epoch 4289/10000, Prediction Accuracy = 58.312%, Loss = 0.9042084097862244
Epoch: 4289, Batch Gradient Norm: 37.130746488983355
Epoch: 4289, Batch Gradient Norm after: 22.360675159276862
Epoch 4290/10000, Prediction Accuracy = 58.278%, Loss = 0.913471007347107
Epoch: 4290, Batch Gradient Norm: 33.471797749028404
Epoch: 4290, Batch Gradient Norm after: 22.360676591422454
Epoch 4291/10000, Prediction Accuracy = 58.306%, Loss = 0.9040400862693787
Epoch: 4291, Batch Gradient Norm: 37.12410068000782
Epoch: 4291, Batch Gradient Norm after: 22.36067654469083
Epoch 4292/10000, Prediction Accuracy = 58.286%, Loss = 0.9133033633232117
Epoch: 4292, Batch Gradient Norm: 33.46933484061966
Epoch: 4292, Batch Gradient Norm after: 22.360674513679264
Epoch 4293/10000, Prediction Accuracy = 58.314%, Loss = 0.9038718461990356
Epoch: 4293, Batch Gradient Norm: 37.119780944272485
Epoch: 4293, Batch Gradient Norm after: 22.36067779867687
Epoch 4294/10000, Prediction Accuracy = 58.282%, Loss = 0.9131338834762573
Epoch: 4294, Batch Gradient Norm: 33.46477233737648
Epoch: 4294, Batch Gradient Norm after: 22.36067665924677
Epoch 4295/10000, Prediction Accuracy = 58.314%, Loss = 0.903704845905304
Epoch: 4295, Batch Gradient Norm: 37.11085589832972
Epoch: 4295, Batch Gradient Norm after: 22.360678324007672
Epoch 4296/10000, Prediction Accuracy = 58.282%, Loss = 0.9129624128341675
Epoch: 4296, Batch Gradient Norm: 33.45999906473657
Epoch: 4296, Batch Gradient Norm after: 22.36067796080019
Epoch 4297/10000, Prediction Accuracy = 58.31600000000001%, Loss = 0.9035445213317871
Epoch: 4297, Batch Gradient Norm: 37.10011086972368
Epoch: 4297, Batch Gradient Norm after: 22.360677790094268
Epoch 4298/10000, Prediction Accuracy = 58.279999999999994%, Loss = 0.9127806186676025
Epoch: 4298, Batch Gradient Norm: 33.45547268456038
Epoch: 4298, Batch Gradient Norm after: 22.360676142840436
Epoch 4299/10000, Prediction Accuracy = 58.326%, Loss = 0.9033837080001831
Epoch: 4299, Batch Gradient Norm: 37.08846881372312
Epoch: 4299, Batch Gradient Norm after: 22.36067878473116
Epoch 4300/10000, Prediction Accuracy = 58.294000000000004%, Loss = 0.9125967979431152
Epoch: 4300, Batch Gradient Norm: 33.452009843832364
Epoch: 4300, Batch Gradient Norm after: 22.360676620857316
Epoch 4301/10000, Prediction Accuracy = 58.324%, Loss = 0.9032199740409851
Epoch: 4301, Batch Gradient Norm: 37.07811714096289
Epoch: 4301, Batch Gradient Norm after: 22.36067650633991
Epoch 4302/10000, Prediction Accuracy = 58.294000000000004%, Loss = 0.9124183058738708
Epoch: 4302, Batch Gradient Norm: 33.448485945197596
Epoch: 4302, Batch Gradient Norm after: 22.360677047449382
Epoch 4303/10000, Prediction Accuracy = 58.326%, Loss = 0.9030546069145202
Epoch: 4303, Batch Gradient Norm: 37.069173018028394
Epoch: 4303, Batch Gradient Norm after: 22.36067817407774
Epoch 4304/10000, Prediction Accuracy = 58.3%, Loss = 0.9122377753257751
Epoch: 4304, Batch Gradient Norm: 33.44360144177393
Epoch: 4304, Batch Gradient Norm after: 22.360675955939776
Epoch 4305/10000, Prediction Accuracy = 58.324%, Loss = 0.9028908014297485
Epoch: 4305, Batch Gradient Norm: 37.05594276048248
Epoch: 4305, Batch Gradient Norm after: 22.36067644083188
Epoch 4306/10000, Prediction Accuracy = 58.302%, Loss = 0.9120555639266967
Epoch: 4306, Batch Gradient Norm: 33.44329541015871
Epoch: 4306, Batch Gradient Norm after: 22.36067662823761
Epoch 4307/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.9027284979820251
Epoch: 4307, Batch Gradient Norm: 37.04515145337521
Epoch: 4307, Batch Gradient Norm after: 22.360676093992947
Epoch 4308/10000, Prediction Accuracy = 58.31600000000001%, Loss = 0.9118699789047241
Epoch: 4308, Batch Gradient Norm: 33.438844007508
Epoch: 4308, Batch Gradient Norm after: 22.36067635824021
Epoch 4309/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.9025655746459961
Epoch: 4309, Batch Gradient Norm: 37.039221485610106
Epoch: 4309, Batch Gradient Norm after: 22.360678423277495
Epoch 4310/10000, Prediction Accuracy = 58.30800000000001%, Loss = 0.9117068767547607
Epoch: 4310, Batch Gradient Norm: 33.43236887194208
Epoch: 4310, Batch Gradient Norm after: 22.36067730177644
Epoch 4311/10000, Prediction Accuracy = 58.34400000000001%, Loss = 0.9023992419242859
Epoch: 4311, Batch Gradient Norm: 37.033125314687304
Epoch: 4311, Batch Gradient Norm after: 22.3606759078826
Epoch 4312/10000, Prediction Accuracy = 58.314%, Loss = 0.9115414142608642
Epoch: 4312, Batch Gradient Norm: 33.426779271008456
Epoch: 4312, Batch Gradient Norm after: 22.36067626073611
Epoch 4313/10000, Prediction Accuracy = 58.346000000000004%, Loss = 0.9022332787513733
Epoch: 4313, Batch Gradient Norm: 37.023363676744204
Epoch: 4313, Batch Gradient Norm after: 22.360675298540624
Epoch 4314/10000, Prediction Accuracy = 58.31%, Loss = 0.9113659262657166
Epoch: 4314, Batch Gradient Norm: 33.42180191193822
Epoch: 4314, Batch Gradient Norm after: 22.36067450312108
Epoch 4315/10000, Prediction Accuracy = 58.352%, Loss = 0.9020700454711914
Epoch: 4315, Batch Gradient Norm: 37.01622415697306
Epoch: 4315, Batch Gradient Norm after: 22.360674505297162
Epoch 4316/10000, Prediction Accuracy = 58.306%, Loss = 0.9111923575401306
Epoch: 4316, Batch Gradient Norm: 33.41870294182968
Epoch: 4316, Batch Gradient Norm after: 22.360676854858742
Epoch 4317/10000, Prediction Accuracy = 58.343999999999994%, Loss = 0.901909327507019
Epoch: 4317, Batch Gradient Norm: 37.005939226239335
Epoch: 4317, Batch Gradient Norm after: 22.36067852863248
Epoch 4318/10000, Prediction Accuracy = 58.30400000000001%, Loss = 0.9110182046890258
Epoch: 4318, Batch Gradient Norm: 33.41250803764027
Epoch: 4318, Batch Gradient Norm after: 22.36067763992361
Epoch 4319/10000, Prediction Accuracy = 58.346000000000004%, Loss = 0.9017462849617004
Epoch: 4319, Batch Gradient Norm: 37.00043850489915
Epoch: 4319, Batch Gradient Norm after: 22.360675091371476
Epoch 4320/10000, Prediction Accuracy = 58.306000000000004%, Loss = 0.9108471274375916
Epoch: 4320, Batch Gradient Norm: 33.40599498877948
Epoch: 4320, Batch Gradient Norm after: 22.360678204534118
Epoch 4321/10000, Prediction Accuracy = 58.352%, Loss = 0.9015761613845825
Epoch: 4321, Batch Gradient Norm: 36.996106478800144
Epoch: 4321, Batch Gradient Norm after: 22.3606748956329
Epoch 4322/10000, Prediction Accuracy = 58.322%, Loss = 0.9106817126274109
Epoch: 4322, Batch Gradient Norm: 33.398559603097524
Epoch: 4322, Batch Gradient Norm after: 22.36067756674471
Epoch 4323/10000, Prediction Accuracy = 58.358000000000004%, Loss = 0.901410436630249
Epoch: 4323, Batch Gradient Norm: 36.990661230682704
Epoch: 4323, Batch Gradient Norm after: 22.36067931331591
Epoch 4324/10000, Prediction Accuracy = 58.322%, Loss = 0.9105174660682678
Epoch: 4324, Batch Gradient Norm: 33.39498829683248
Epoch: 4324, Batch Gradient Norm after: 22.360676628170644
Epoch 4325/10000, Prediction Accuracy = 58.354%, Loss = 0.9012491941452027
Epoch: 4325, Batch Gradient Norm: 36.98403681719087
Epoch: 4325, Batch Gradient Norm after: 22.36067843517415
Epoch 4326/10000, Prediction Accuracy = 58.33%, Loss = 0.9103582501411438
Epoch: 4326, Batch Gradient Norm: 33.38939411112032
Epoch: 4326, Batch Gradient Norm after: 22.360676579384123
Epoch 4327/10000, Prediction Accuracy = 58.35600000000001%, Loss = 0.9010828018188477
Epoch: 4327, Batch Gradient Norm: 36.98221096209609
Epoch: 4327, Batch Gradient Norm after: 22.36067668259656
Epoch 4328/10000, Prediction Accuracy = 58.33800000000001%, Loss = 0.9102011084556579
Epoch: 4328, Batch Gradient Norm: 33.381926348365916
Epoch: 4328, Batch Gradient Norm after: 22.36067787366448
Epoch 4329/10000, Prediction Accuracy = 58.36%, Loss = 0.9009172439575195
Epoch: 4329, Batch Gradient Norm: 36.97925940591431
Epoch: 4329, Batch Gradient Norm after: 22.3606765691078
Epoch 4330/10000, Prediction Accuracy = 58.33200000000001%, Loss = 0.9100404858589173
Epoch: 4330, Batch Gradient Norm: 33.37826929612521
Epoch: 4330, Batch Gradient Norm after: 22.360677793855785
Epoch 4331/10000, Prediction Accuracy = 58.352%, Loss = 0.9007491350173951
Epoch: 4331, Batch Gradient Norm: 36.977820919688966
Epoch: 4331, Batch Gradient Norm after: 22.360675090479063
Epoch 4332/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.909886646270752
Epoch: 4332, Batch Gradient Norm: 33.372349369969925
Epoch: 4332, Batch Gradient Norm after: 22.360679447878553
Epoch 4333/10000, Prediction Accuracy = 58.355999999999995%, Loss = 0.9005829095840454
Epoch: 4333, Batch Gradient Norm: 36.9728482046379
Epoch: 4333, Batch Gradient Norm after: 22.36067725095567
Epoch 4334/10000, Prediction Accuracy = 58.342%, Loss = 0.909726905822754
Epoch: 4334, Batch Gradient Norm: 33.36780769119946
Epoch: 4334, Batch Gradient Norm after: 22.3606788124505
Epoch 4335/10000, Prediction Accuracy = 58.366%, Loss = 0.9004173755645752
Epoch: 4335, Batch Gradient Norm: 36.9692550257344
Epoch: 4335, Batch Gradient Norm after: 22.360677767573307
Epoch 4336/10000, Prediction Accuracy = 58.338%, Loss = 0.909564471244812
Epoch: 4336, Batch Gradient Norm: 33.36169234614913
Epoch: 4336, Batch Gradient Norm after: 22.360678961810468
Epoch 4337/10000, Prediction Accuracy = 58.36999999999999%, Loss = 0.9002517938613892
Epoch: 4337, Batch Gradient Norm: 36.963266056298075
Epoch: 4337, Batch Gradient Norm after: 22.360677948078912
Epoch 4338/10000, Prediction Accuracy = 58.33599999999999%, Loss = 0.9094009160995483
Epoch: 4338, Batch Gradient Norm: 33.3550552436886
Epoch: 4338, Batch Gradient Norm after: 22.36067600093261
Epoch 4339/10000, Prediction Accuracy = 58.38199999999999%, Loss = 0.9000901103019714
Epoch: 4339, Batch Gradient Norm: 36.96011425359314
Epoch: 4339, Batch Gradient Norm after: 22.360680167449434
Epoch 4340/10000, Prediction Accuracy = 58.338%, Loss = 0.9092426419258117
Epoch: 4340, Batch Gradient Norm: 33.34892587738549
Epoch: 4340, Batch Gradient Norm after: 22.36067752726814
Epoch 4341/10000, Prediction Accuracy = 58.376%, Loss = 0.8999237656593323
Epoch: 4341, Batch Gradient Norm: 36.95634375077951
Epoch: 4341, Batch Gradient Norm after: 22.360675592633186
Epoch 4342/10000, Prediction Accuracy = 58.338%, Loss = 0.9090802311897278
Epoch: 4342, Batch Gradient Norm: 33.34118809275799
Epoch: 4342, Batch Gradient Norm after: 22.360677437936335
Epoch 4343/10000, Prediction Accuracy = 58.382000000000005%, Loss = 0.8997596979141236
Epoch: 4343, Batch Gradient Norm: 36.95400425019302
Epoch: 4343, Batch Gradient Norm after: 22.360677679112744
Epoch 4344/10000, Prediction Accuracy = 58.33399999999999%, Loss = 0.9089256048202514
Epoch: 4344, Batch Gradient Norm: 33.33820098195771
Epoch: 4344, Batch Gradient Norm after: 22.36067664902426
Epoch 4345/10000, Prediction Accuracy = 58.39000000000001%, Loss = 0.8995942711830139
Epoch: 4345, Batch Gradient Norm: 36.95281653827291
Epoch: 4345, Batch Gradient Norm after: 22.3606773759723
Epoch 4346/10000, Prediction Accuracy = 58.327999999999996%, Loss = 0.9087656259536743
Epoch: 4346, Batch Gradient Norm: 33.33109377815825
Epoch: 4346, Batch Gradient Norm after: 22.360678496461713
Epoch 4347/10000, Prediction Accuracy = 58.39%, Loss = 0.8994292616844177
Epoch: 4347, Batch Gradient Norm: 36.954286169060865
Epoch: 4347, Batch Gradient Norm after: 22.36067853935994
Epoch 4348/10000, Prediction Accuracy = 58.338%, Loss = 0.9086175680160522
Epoch: 4348, Batch Gradient Norm: 33.321677322196905
Epoch: 4348, Batch Gradient Norm after: 22.360678823007454
Epoch 4349/10000, Prediction Accuracy = 58.412%, Loss = 0.8992650985717774
Epoch: 4349, Batch Gradient Norm: 36.952154457097635
Epoch: 4349, Batch Gradient Norm after: 22.360678317787745
Epoch 4350/10000, Prediction Accuracy = 58.33599999999999%, Loss = 0.9084646821022033
Epoch: 4350, Batch Gradient Norm: 33.31584109838865
Epoch: 4350, Batch Gradient Norm after: 22.360678485066344
Epoch 4351/10000, Prediction Accuracy = 58.408%, Loss = 0.8990994691848755
Epoch: 4351, Batch Gradient Norm: 36.950381165349
Epoch: 4351, Batch Gradient Norm after: 22.36067764148725
Epoch 4352/10000, Prediction Accuracy = 58.34400000000001%, Loss = 0.908313536643982
Epoch: 4352, Batch Gradient Norm: 33.31086692544758
Epoch: 4352, Batch Gradient Norm after: 22.360676763648115
Epoch 4353/10000, Prediction Accuracy = 58.41600000000001%, Loss = 0.8989299535751343
Epoch: 4353, Batch Gradient Norm: 36.947101412496856
Epoch: 4353, Batch Gradient Norm after: 22.36067844646258
Epoch 4354/10000, Prediction Accuracy = 58.354%, Loss = 0.9081523418426514
Epoch: 4354, Batch Gradient Norm: 33.30705281301119
Epoch: 4354, Batch Gradient Norm after: 22.36067760892918
Epoch 4355/10000, Prediction Accuracy = 58.42%, Loss = 0.8987694501876831
Epoch: 4355, Batch Gradient Norm: 36.94631878453169
Epoch: 4355, Batch Gradient Norm after: 22.36067744489707
Epoch 4356/10000, Prediction Accuracy = 58.358000000000004%, Loss = 0.9079908847808837
Epoch: 4356, Batch Gradient Norm: 33.30187480566169
Epoch: 4356, Batch Gradient Norm after: 22.360675850905395
Epoch 4357/10000, Prediction Accuracy = 58.42%, Loss = 0.8986029028892517
Epoch: 4357, Batch Gradient Norm: 36.93896291708218
Epoch: 4357, Batch Gradient Norm after: 22.360675427208875
Epoch 4358/10000, Prediction Accuracy = 58.36%, Loss = 0.9078338742256165
Epoch: 4358, Batch Gradient Norm: 33.29950595308196
Epoch: 4358, Batch Gradient Norm after: 22.360676892176482
Epoch 4359/10000, Prediction Accuracy = 58.426%, Loss = 0.8984375119209289
Epoch: 4359, Batch Gradient Norm: 36.93273739477146
Epoch: 4359, Batch Gradient Norm after: 22.360677717529917
Epoch 4360/10000, Prediction Accuracy = 58.362%, Loss = 0.9076669216156006
Epoch: 4360, Batch Gradient Norm: 33.29418135764802
Epoch: 4360, Batch Gradient Norm after: 22.36067659522431
Epoch 4361/10000, Prediction Accuracy = 58.424%, Loss = 0.8982784390449524
Epoch: 4361, Batch Gradient Norm: 36.929542391457325
Epoch: 4361, Batch Gradient Norm after: 22.360678650085227
Epoch 4362/10000, Prediction Accuracy = 58.352%, Loss = 0.9075083255767822
Epoch: 4362, Batch Gradient Norm: 33.28980138367024
Epoch: 4362, Batch Gradient Norm after: 22.360677165546285
Epoch 4363/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8981162071228027
Epoch: 4363, Batch Gradient Norm: 36.926996834693504
Epoch: 4363, Batch Gradient Norm after: 22.360677775722422
Epoch 4364/10000, Prediction Accuracy = 58.354%, Loss = 0.9073498368263244
Epoch: 4364, Batch Gradient Norm: 33.284697172719646
Epoch: 4364, Batch Gradient Norm after: 22.360676711257252
Epoch 4365/10000, Prediction Accuracy = 58.42800000000001%, Loss = 0.89795423746109
Epoch: 4365, Batch Gradient Norm: 36.91857058089126
Epoch: 4365, Batch Gradient Norm after: 22.360678655786774
Epoch 4366/10000, Prediction Accuracy = 58.358000000000004%, Loss = 0.9071792244911194
Epoch: 4366, Batch Gradient Norm: 33.284716981310815
Epoch: 4366, Batch Gradient Norm after: 22.36067635509302
Epoch 4367/10000, Prediction Accuracy = 58.428%, Loss = 0.8977954030036926
Epoch: 4367, Batch Gradient Norm: 36.91045373784012
Epoch: 4367, Batch Gradient Norm after: 22.360677146699135
Epoch 4368/10000, Prediction Accuracy = 58.36%, Loss = 0.9070099115371704
Epoch: 4368, Batch Gradient Norm: 33.27882758720923
Epoch: 4368, Batch Gradient Norm after: 22.360676173324663
Epoch 4369/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.8976375579833984
Epoch: 4369, Batch Gradient Norm: 36.89985702568789
Epoch: 4369, Batch Gradient Norm after: 22.3606762824938
Epoch 4370/10000, Prediction Accuracy = 58.354%, Loss = 0.9068382501602172
Epoch: 4370, Batch Gradient Norm: 33.27474223402499
Epoch: 4370, Batch Gradient Norm after: 22.360676133398492
Epoch 4371/10000, Prediction Accuracy = 58.44%, Loss = 0.8974794626235962
Epoch: 4371, Batch Gradient Norm: 36.89751253520273
Epoch: 4371, Batch Gradient Norm after: 22.360678345999382
Epoch 4372/10000, Prediction Accuracy = 58.366%, Loss = 0.9066850543022156
Epoch: 4372, Batch Gradient Norm: 33.267935248479155
Epoch: 4372, Batch Gradient Norm after: 22.360675801378953
Epoch 4373/10000, Prediction Accuracy = 58.436%, Loss = 0.8973182559013366
Epoch: 4373, Batch Gradient Norm: 36.89390974292903
Epoch: 4373, Batch Gradient Norm after: 22.3606770397819
Epoch 4374/10000, Prediction Accuracy = 58.367999999999995%, Loss = 0.9065275430679322
Epoch: 4374, Batch Gradient Norm: 33.26154033578957
Epoch: 4374, Batch Gradient Norm after: 22.360676094497723
Epoch 4375/10000, Prediction Accuracy = 58.428%, Loss = 0.8971551775932312
Epoch: 4375, Batch Gradient Norm: 36.88698692803118
Epoch: 4375, Batch Gradient Norm after: 22.36067747241629
Epoch 4376/10000, Prediction Accuracy = 58.372%, Loss = 0.9063586354255676
Epoch: 4376, Batch Gradient Norm: 33.258947998064556
Epoch: 4376, Batch Gradient Norm after: 22.360674540087576
Epoch 4377/10000, Prediction Accuracy = 58.424%, Loss = 0.8969935655593873
Epoch: 4377, Batch Gradient Norm: 36.88243172900048
Epoch: 4377, Batch Gradient Norm after: 22.36067852813734
Epoch 4378/10000, Prediction Accuracy = 58.386%, Loss = 0.906201434135437
Epoch: 4378, Batch Gradient Norm: 33.251440800922595
Epoch: 4378, Batch Gradient Norm after: 22.360675071081037
Epoch 4379/10000, Prediction Accuracy = 58.42199999999999%, Loss = 0.8968316435813903
Epoch: 4379, Batch Gradient Norm: 36.875624102679716
Epoch: 4379, Batch Gradient Norm after: 22.360677953520398
Epoch 4380/10000, Prediction Accuracy = 58.39200000000001%, Loss = 0.9060345649719238
Epoch: 4380, Batch Gradient Norm: 33.24779227887873
Epoch: 4380, Batch Gradient Norm after: 22.360677365334713
Epoch 4381/10000, Prediction Accuracy = 58.432%, Loss = 0.8966705441474915
Epoch: 4381, Batch Gradient Norm: 36.86462168970412
Epoch: 4381, Batch Gradient Norm after: 22.360677769684184
Epoch 4382/10000, Prediction Accuracy = 58.388%, Loss = 0.9058610200881958
Epoch: 4382, Batch Gradient Norm: 33.24300283329637
Epoch: 4382, Batch Gradient Norm after: 22.360676501395186
Epoch 4383/10000, Prediction Accuracy = 58.438%, Loss = 0.8965119123458862
Epoch: 4383, Batch Gradient Norm: 36.85026934963142
Epoch: 4383, Batch Gradient Norm after: 22.360679572302352
Epoch 4384/10000, Prediction Accuracy = 58.396%, Loss = 0.9056762814521789
Epoch: 4384, Batch Gradient Norm: 33.240985517989785
Epoch: 4384, Batch Gradient Norm after: 22.360676199832625
Epoch 4385/10000, Prediction Accuracy = 58.436%, Loss = 0.8963578820228577
Epoch: 4385, Batch Gradient Norm: 36.83619189044693
Epoch: 4385, Batch Gradient Norm after: 22.36067956260614
Epoch 4386/10000, Prediction Accuracy = 58.39%, Loss = 0.9054935455322266
Epoch: 4386, Batch Gradient Norm: 33.24018148170587
Epoch: 4386, Batch Gradient Norm after: 22.36067677465136
Epoch 4387/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.8962050318717957
Epoch: 4387, Batch Gradient Norm: 36.8225855910351
Epoch: 4387, Batch Gradient Norm after: 22.360676841422816
Epoch 4388/10000, Prediction Accuracy = 58.398%, Loss = 0.9053059816360474
Epoch: 4388, Batch Gradient Norm: 33.23580345777958
Epoch: 4388, Batch Gradient Norm after: 22.36067961880672
Epoch 4389/10000, Prediction Accuracy = 58.426%, Loss = 0.8960528016090393
Epoch: 4389, Batch Gradient Norm: 36.80823308731238
Epoch: 4389, Batch Gradient Norm after: 22.360678719848877
Epoch 4390/10000, Prediction Accuracy = 58.40599999999999%, Loss = 0.905117928981781
Epoch: 4390, Batch Gradient Norm: 33.23720153069177
Epoch: 4390, Batch Gradient Norm after: 22.36067669177058
Epoch 4391/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8958984017372131
Epoch: 4391, Batch Gradient Norm: 36.794065906057114
Epoch: 4391, Batch Gradient Norm after: 22.360676954149408
Epoch 4392/10000, Prediction Accuracy = 58.414%, Loss = 0.9049377202987671
Epoch: 4392, Batch Gradient Norm: 33.23440743825021
Epoch: 4392, Batch Gradient Norm after: 22.360676251594196
Epoch 4393/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8957479357719421
Epoch: 4393, Batch Gradient Norm: 36.77758693832292
Epoch: 4393, Batch Gradient Norm after: 22.36067711438475
Epoch 4394/10000, Prediction Accuracy = 58.41799999999999%, Loss = 0.9047373414039612
Epoch: 4394, Batch Gradient Norm: 33.23734975176393
Epoch: 4394, Batch Gradient Norm after: 22.36067746785591
Epoch 4395/10000, Prediction Accuracy = 58.428%, Loss = 0.8955979824066163
Epoch: 4395, Batch Gradient Norm: 36.759842705009376
Epoch: 4395, Batch Gradient Norm after: 22.360676105998078
Epoch 4396/10000, Prediction Accuracy = 58.424%, Loss = 0.9045475125312805
Epoch: 4396, Batch Gradient Norm: 33.23675720597011
Epoch: 4396, Batch Gradient Norm after: 22.360675658829763
Epoch 4397/10000, Prediction Accuracy = 58.43399999999999%, Loss = 0.8954458236694336
Epoch: 4397, Batch Gradient Norm: 36.746036598001055
Epoch: 4397, Batch Gradient Norm after: 22.360678062967978
Epoch 4398/10000, Prediction Accuracy = 58.424%, Loss = 0.9043583869934082
Epoch: 4398, Batch Gradient Norm: 33.23344025586278
Epoch: 4398, Batch Gradient Norm after: 22.360677772986755
Epoch 4399/10000, Prediction Accuracy = 58.428%, Loss = 0.8952948927879334
Epoch: 4399, Batch Gradient Norm: 36.73162482516273
Epoch: 4399, Batch Gradient Norm after: 22.360677880074327
Epoch 4400/10000, Prediction Accuracy = 58.428%, Loss = 0.9041739940643311
Epoch: 4400, Batch Gradient Norm: 33.23349307269697
Epoch: 4400, Batch Gradient Norm after: 22.3606771406001
Epoch 4401/10000, Prediction Accuracy = 58.42800000000001%, Loss = 0.8951477766036987
Epoch: 4401, Batch Gradient Norm: 36.71663604059773
Epoch: 4401, Batch Gradient Norm after: 22.360677423143333
Epoch 4402/10000, Prediction Accuracy = 58.431999999999995%, Loss = 0.9039843559265137
Epoch: 4402, Batch Gradient Norm: 33.23007215742272
Epoch: 4402, Batch Gradient Norm after: 22.360676950659382
Epoch 4403/10000, Prediction Accuracy = 58.428%, Loss = 0.8949989914894104
Epoch: 4403, Batch Gradient Norm: 36.70229489490386
Epoch: 4403, Batch Gradient Norm after: 22.360676264235703
Epoch 4404/10000, Prediction Accuracy = 58.43599999999999%, Loss = 0.9037982106208802
Epoch: 4404, Batch Gradient Norm: 33.231049876952035
Epoch: 4404, Batch Gradient Norm after: 22.36067751269957
Epoch 4405/10000, Prediction Accuracy = 58.436%, Loss = 0.8948479533195496
Epoch: 4405, Batch Gradient Norm: 36.68878533356816
Epoch: 4405, Batch Gradient Norm after: 22.360675550650868
Epoch 4406/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.9036136150360108
Epoch: 4406, Batch Gradient Norm: 33.22822183599424
Epoch: 4406, Batch Gradient Norm after: 22.360677128142644
Epoch 4407/10000, Prediction Accuracy = 58.436%, Loss = 0.8947004914283753
Epoch: 4407, Batch Gradient Norm: 36.67445756605302
Epoch: 4407, Batch Gradient Norm after: 22.360675870057094
Epoch 4408/10000, Prediction Accuracy = 58.444%, Loss = 0.9034235000610351
Epoch: 4408, Batch Gradient Norm: 33.226398673917515
Epoch: 4408, Batch Gradient Norm after: 22.360677005158486
Epoch 4409/10000, Prediction Accuracy = 58.438%, Loss = 0.8945488810539246
Epoch: 4409, Batch Gradient Norm: 36.66549166472409
Epoch: 4409, Batch Gradient Norm after: 22.360675223083657
Epoch 4410/10000, Prediction Accuracy = 58.448%, Loss = 0.903256607055664
Epoch: 4410, Batch Gradient Norm: 33.22666857245112
Epoch: 4410, Batch Gradient Norm after: 22.360678441435667
Epoch 4411/10000, Prediction Accuracy = 58.444%, Loss = 0.8943922758102417
Epoch: 4411, Batch Gradient Norm: 36.653542198075
Epoch: 4411, Batch Gradient Norm after: 22.36067646384813
Epoch 4412/10000, Prediction Accuracy = 58.448%, Loss = 0.9030772805213928
Epoch: 4412, Batch Gradient Norm: 33.223796837483754
Epoch: 4412, Batch Gradient Norm after: 22.360676757639222
Epoch 4413/10000, Prediction Accuracy = 58.438%, Loss = 0.894240427017212
Epoch: 4413, Batch Gradient Norm: 36.645587533417775
Epoch: 4413, Batch Gradient Norm after: 22.360676553202712
Epoch 4414/10000, Prediction Accuracy = 58.462%, Loss = 0.9029029488563538
Epoch: 4414, Batch Gradient Norm: 33.222019454089235
Epoch: 4414, Batch Gradient Norm after: 22.360676008943507
Epoch 4415/10000, Prediction Accuracy = 58.432%, Loss = 0.8940850496292114
Epoch: 4415, Batch Gradient Norm: 36.63832301616218
Epoch: 4415, Batch Gradient Norm after: 22.360676605096288
Epoch 4416/10000, Prediction Accuracy = 58.467999999999996%, Loss = 0.902730405330658
Epoch: 4416, Batch Gradient Norm: 33.21784507069294
Epoch: 4416, Batch Gradient Norm after: 22.360676634144703
Epoch 4417/10000, Prediction Accuracy = 58.438%, Loss = 0.8939325213432312
Epoch: 4417, Batch Gradient Norm: 36.62689185109748
Epoch: 4417, Batch Gradient Norm after: 22.360675986422955
Epoch 4418/10000, Prediction Accuracy = 58.48199999999999%, Loss = 0.9025556445121765
Epoch: 4418, Batch Gradient Norm: 33.21422353768035
Epoch: 4418, Batch Gradient Norm after: 22.36067694272501
Epoch 4419/10000, Prediction Accuracy = 58.44000000000001%, Loss = 0.8937781691551209
Epoch: 4419, Batch Gradient Norm: 36.61857689030064
Epoch: 4419, Batch Gradient Norm after: 22.36067673450911
Epoch 4420/10000, Prediction Accuracy = 58.483999999999995%, Loss = 0.9023846864700318
Epoch: 4420, Batch Gradient Norm: 33.21359705656715
Epoch: 4420, Batch Gradient Norm after: 22.360676345040805
Epoch 4421/10000, Prediction Accuracy = 58.438%, Loss = 0.8936225891113281
Epoch: 4421, Batch Gradient Norm: 36.609551238857925
Epoch: 4421, Batch Gradient Norm after: 22.360676662678785
Epoch 4422/10000, Prediction Accuracy = 58.486000000000004%, Loss = 0.9022155404090881
Epoch: 4422, Batch Gradient Norm: 33.208836736468314
Epoch: 4422, Batch Gradient Norm after: 22.360676543243418
Epoch 4423/10000, Prediction Accuracy = 58.44000000000001%, Loss = 0.8934679746627807
Epoch: 4423, Batch Gradient Norm: 36.60060268630155
Epoch: 4423, Batch Gradient Norm after: 22.360676151100805
Epoch 4424/10000, Prediction Accuracy = 58.484%, Loss = 0.9020478248596191
Epoch: 4424, Batch Gradient Norm: 33.205307972085926
Epoch: 4424, Batch Gradient Norm after: 22.360675566349844
Epoch 4425/10000, Prediction Accuracy = 58.434000000000005%, Loss = 0.8933135509490967
Epoch: 4425, Batch Gradient Norm: 36.59122974348605
Epoch: 4425, Batch Gradient Norm after: 22.360676962451898
Epoch 4426/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.9018814563751221
Epoch: 4426, Batch Gradient Norm: 33.20197066461729
Epoch: 4426, Batch Gradient Norm after: 22.36067724044224
Epoch 4427/10000, Prediction Accuracy = 58.44%, Loss = 0.8931577682495118
Epoch: 4427, Batch Gradient Norm: 36.58241095766671
Epoch: 4427, Batch Gradient Norm after: 22.360678224095068
Epoch 4428/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.9017100930213928
Epoch: 4428, Batch Gradient Norm: 33.20130288931933
Epoch: 4428, Batch Gradient Norm after: 22.360677763647896
Epoch 4429/10000, Prediction Accuracy = 58.448%, Loss = 0.8930033922195435
Epoch: 4429, Batch Gradient Norm: 36.57504395176606
Epoch: 4429, Batch Gradient Norm after: 22.360676361607347
Epoch 4430/10000, Prediction Accuracy = 58.492000000000004%, Loss = 0.9015453457832336
Epoch: 4430, Batch Gradient Norm: 33.19605853358825
Epoch: 4430, Batch Gradient Norm after: 22.360679484762194
Epoch 4431/10000, Prediction Accuracy = 58.45%, Loss = 0.8928498387336731
Epoch: 4431, Batch Gradient Norm: 36.5646154593992
Epoch: 4431, Batch Gradient Norm after: 22.360678552988585
Epoch 4432/10000, Prediction Accuracy = 58.49400000000001%, Loss = 0.9013733863830566
Epoch: 4432, Batch Gradient Norm: 33.19305703543052
Epoch: 4432, Batch Gradient Norm after: 22.36067738105326
Epoch 4433/10000, Prediction Accuracy = 58.45%, Loss = 0.8926955580711364
Epoch: 4433, Batch Gradient Norm: 36.55245588794039
Epoch: 4433, Batch Gradient Norm after: 22.360678293556695
Epoch 4434/10000, Prediction Accuracy = 58.496%, Loss = 0.901191794872284
Epoch: 4434, Batch Gradient Norm: 33.19241028231335
Epoch: 4434, Batch Gradient Norm after: 22.360678542465436
Epoch 4435/10000, Prediction Accuracy = 58.45%, Loss = 0.8925454139709472
Epoch: 4435, Batch Gradient Norm: 36.542681801501274
Epoch: 4435, Batch Gradient Norm after: 22.360679081614226
Epoch 4436/10000, Prediction Accuracy = 58.5%, Loss = 0.9010223507881164
Epoch: 4436, Batch Gradient Norm: 33.18757426540458
Epoch: 4436, Batch Gradient Norm after: 22.36067876245524
Epoch 4437/10000, Prediction Accuracy = 58.452%, Loss = 0.892391312122345
Epoch: 4437, Batch Gradient Norm: 36.533192257485396
Epoch: 4437, Batch Gradient Norm after: 22.360677488299633
Epoch 4438/10000, Prediction Accuracy = 58.498000000000005%, Loss = 0.9008496165275574
Epoch: 4438, Batch Gradient Norm: 33.18458607656079
Epoch: 4438, Batch Gradient Norm after: 22.360679188472254
Epoch 4439/10000, Prediction Accuracy = 58.45%, Loss = 0.8922401785850524
Epoch: 4439, Batch Gradient Norm: 36.518826968219194
Epoch: 4439, Batch Gradient Norm after: 22.360679087918367
Epoch 4440/10000, Prediction Accuracy = 58.504%, Loss = 0.9006692886352539
Epoch: 4440, Batch Gradient Norm: 33.18460181775375
Epoch: 4440, Batch Gradient Norm after: 22.36067621231388
Epoch 4441/10000, Prediction Accuracy = 58.45399999999999%, Loss = 0.8920942783355713
Epoch: 4441, Batch Gradient Norm: 36.50767735481409
Epoch: 4441, Batch Gradient Norm after: 22.36067830683531
Epoch 4442/10000, Prediction Accuracy = 58.504%, Loss = 0.9004945993423462
Epoch: 4442, Batch Gradient Norm: 33.18240475651618
Epoch: 4442, Batch Gradient Norm after: 22.360677746399915
Epoch 4443/10000, Prediction Accuracy = 58.458000000000006%, Loss = 0.8919420599937439
Epoch: 4443, Batch Gradient Norm: 36.49335954844368
Epoch: 4443, Batch Gradient Norm after: 22.360677577423317
Epoch 4444/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.9003090143203736
Epoch: 4444, Batch Gradient Norm: 33.18086859378784
Epoch: 4444, Batch Gradient Norm after: 22.360678509887382
Epoch 4445/10000, Prediction Accuracy = 58.46400000000001%, Loss = 0.8917936444282532
Epoch: 4445, Batch Gradient Norm: 36.47858167055653
Epoch: 4445, Batch Gradient Norm after: 22.360679573271685
Epoch 4446/10000, Prediction Accuracy = 58.496%, Loss = 0.9001261234283447
Epoch: 4446, Batch Gradient Norm: 33.18004277705878
Epoch: 4446, Batch Gradient Norm after: 22.36067691386205
Epoch 4447/10000, Prediction Accuracy = 58.46400000000001%, Loss = 0.8916459918022156
Epoch: 4447, Batch Gradient Norm: 36.46731984046403
Epoch: 4447, Batch Gradient Norm after: 22.360679435802552
Epoch 4448/10000, Prediction Accuracy = 58.49400000000001%, Loss = 0.8999505639076233
Epoch: 4448, Batch Gradient Norm: 33.178922914873716
Epoch: 4448, Batch Gradient Norm after: 22.360677519128917
Epoch 4449/10000, Prediction Accuracy = 58.470000000000006%, Loss = 0.8914931535720825
Epoch: 4449, Batch Gradient Norm: 36.46178059292468
Epoch: 4449, Batch Gradient Norm after: 22.360679055953682
Epoch 4450/10000, Prediction Accuracy = 58.498000000000005%, Loss = 0.8997852325439453
Epoch: 4450, Batch Gradient Norm: 33.1737436422318
Epoch: 4450, Batch Gradient Norm after: 22.36067646289185
Epoch 4451/10000, Prediction Accuracy = 58.468%, Loss = 0.8913371324539184
Epoch: 4451, Batch Gradient Norm: 36.45331077413147
Epoch: 4451, Batch Gradient Norm after: 22.360677811382065
Epoch 4452/10000, Prediction Accuracy = 58.5%, Loss = 0.8996177077293396
Epoch: 4452, Batch Gradient Norm: 33.170065921686586
Epoch: 4452, Batch Gradient Norm after: 22.36067951373029
Epoch 4453/10000, Prediction Accuracy = 58.465999999999994%, Loss = 0.8911835193634033
Epoch: 4453, Batch Gradient Norm: 36.44808523685107
Epoch: 4453, Batch Gradient Norm after: 22.360678340312283
Epoch 4454/10000, Prediction Accuracy = 58.5%, Loss = 0.8994590520858765
Epoch: 4454, Batch Gradient Norm: 33.16464591935793
Epoch: 4454, Batch Gradient Norm after: 22.36067575066765
Epoch 4455/10000, Prediction Accuracy = 58.464%, Loss = 0.8910305857658386
Epoch: 4455, Batch Gradient Norm: 36.4406411509739
Epoch: 4455, Batch Gradient Norm after: 22.360678766406792
Epoch 4456/10000, Prediction Accuracy = 58.498000000000005%, Loss = 0.8992989301681519
Epoch: 4456, Batch Gradient Norm: 33.160008406967535
Epoch: 4456, Batch Gradient Norm after: 22.36067797001668
Epoch 4457/10000, Prediction Accuracy = 58.465999999999994%, Loss = 0.8908750891685486
Epoch: 4457, Batch Gradient Norm: 36.43606319893428
Epoch: 4457, Batch Gradient Norm after: 22.360679734504536
Epoch 4458/10000, Prediction Accuracy = 58.50599999999999%, Loss = 0.8991442918777466
Epoch: 4458, Batch Gradient Norm: 33.15415569900732
Epoch: 4458, Batch Gradient Norm after: 22.360677413030228
Epoch 4459/10000, Prediction Accuracy = 58.468%, Loss = 0.8907190918922424
Epoch: 4459, Batch Gradient Norm: 36.4321249412394
Epoch: 4459, Batch Gradient Norm after: 22.36067980219883
Epoch 4460/10000, Prediction Accuracy = 58.512%, Loss = 0.8989933729171753
Epoch: 4460, Batch Gradient Norm: 33.148701632358765
Epoch: 4460, Batch Gradient Norm after: 22.360676910175187
Epoch 4461/10000, Prediction Accuracy = 58.470000000000006%, Loss = 0.8905578017234802
Epoch: 4461, Batch Gradient Norm: 36.434719357544594
Epoch: 4461, Batch Gradient Norm after: 22.360679349011612
Epoch 4462/10000, Prediction Accuracy = 58.512%, Loss = 0.8988535404205322
Epoch: 4462, Batch Gradient Norm: 33.13982378896447
Epoch: 4462, Batch Gradient Norm after: 22.360675665465806
Epoch 4463/10000, Prediction Accuracy = 58.476%, Loss = 0.890397047996521
Epoch: 4463, Batch Gradient Norm: 36.433807787175716
Epoch: 4463, Batch Gradient Norm after: 22.360680179085186
Epoch 4464/10000, Prediction Accuracy = 58.512%, Loss = 0.8987102150917053
Epoch: 4464, Batch Gradient Norm: 33.13306591081328
Epoch: 4464, Batch Gradient Norm after: 22.36067774117232
Epoch 4465/10000, Prediction Accuracy = 58.48%, Loss = 0.8902378439903259
Epoch: 4465, Batch Gradient Norm: 36.435282848468425
Epoch: 4465, Batch Gradient Norm after: 22.360679863604236
Epoch 4466/10000, Prediction Accuracy = 58.510000000000005%, Loss = 0.8985704779624939
Epoch: 4466, Batch Gradient Norm: 33.122816158873334
Epoch: 4466, Batch Gradient Norm after: 22.360676643113884
Epoch 4467/10000, Prediction Accuracy = 58.476%, Loss = 0.8900793671607972
Epoch: 4467, Batch Gradient Norm: 36.43270365960212
Epoch: 4467, Batch Gradient Norm after: 22.360678254974456
Epoch 4468/10000, Prediction Accuracy = 58.50599999999999%, Loss = 0.8984241604804992
Epoch: 4468, Batch Gradient Norm: 33.11727209748564
Epoch: 4468, Batch Gradient Norm after: 22.36067845103312
Epoch 4469/10000, Prediction Accuracy = 58.47600000000001%, Loss = 0.8899232983589173
Epoch: 4469, Batch Gradient Norm: 36.42708734627294
Epoch: 4469, Batch Gradient Norm after: 22.36067880447948
Epoch 4470/10000, Prediction Accuracy = 58.510000000000005%, Loss = 0.898270583152771
Epoch: 4470, Batch Gradient Norm: 33.11103412599761
Epoch: 4470, Batch Gradient Norm after: 22.3606802414153
Epoch 4471/10000, Prediction Accuracy = 58.480000000000004%, Loss = 0.8897708415985107
Epoch: 4471, Batch Gradient Norm: 36.42200891983338
Epoch: 4471, Batch Gradient Norm after: 22.36067843869254
Epoch 4472/10000, Prediction Accuracy = 58.512%, Loss = 0.8981161117553711
Epoch: 4472, Batch Gradient Norm: 33.10609757935266
Epoch: 4472, Batch Gradient Norm after: 22.3606795599113
Epoch 4473/10000, Prediction Accuracy = 58.492000000000004%, Loss = 0.8896166205406189
Epoch: 4473, Batch Gradient Norm: 36.4169266764563
Epoch: 4473, Batch Gradient Norm after: 22.360680851926737
Epoch 4474/10000, Prediction Accuracy = 58.519999999999996%, Loss = 0.8979570269584656
Epoch: 4474, Batch Gradient Norm: 33.09998551999723
Epoch: 4474, Batch Gradient Norm after: 22.36067947579281
Epoch 4475/10000, Prediction Accuracy = 58.49400000000001%, Loss = 0.8894640803337097
Epoch: 4475, Batch Gradient Norm: 36.411728183236065
Epoch: 4475, Batch Gradient Norm after: 22.36067980189338
Epoch 4476/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.8978030562400818
Epoch: 4476, Batch Gradient Norm: 33.09401186851527
Epoch: 4476, Batch Gradient Norm after: 22.360679182426242
Epoch 4477/10000, Prediction Accuracy = 58.492000000000004%, Loss = 0.8893065929412842
Epoch: 4477, Batch Gradient Norm: 36.40643958911133
Epoch: 4477, Batch Gradient Norm after: 22.36067843659848
Epoch 4478/10000, Prediction Accuracy = 58.516%, Loss = 0.8976508975028992
Epoch: 4478, Batch Gradient Norm: 33.08941421876381
Epoch: 4478, Batch Gradient Norm after: 22.36067826457018
Epoch 4479/10000, Prediction Accuracy = 58.504%, Loss = 0.889152991771698
Epoch: 4479, Batch Gradient Norm: 36.397539877979895
Epoch: 4479, Batch Gradient Norm after: 22.36067646545605
Epoch 4480/10000, Prediction Accuracy = 58.51800000000001%, Loss = 0.897490119934082
Epoch: 4480, Batch Gradient Norm: 33.083802150658435
Epoch: 4480, Batch Gradient Norm after: 22.360676902010855
Epoch 4481/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.8890007376670838
Epoch: 4481, Batch Gradient Norm: 36.39086678305518
Epoch: 4481, Batch Gradient Norm after: 22.360676441437217
Epoch 4482/10000, Prediction Accuracy = 58.516000000000005%, Loss = 0.8973351240158081
Epoch: 4482, Batch Gradient Norm: 33.079173592271054
Epoch: 4482, Batch Gradient Norm after: 22.360678728221068
Epoch 4483/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8888495564460754
Epoch: 4483, Batch Gradient Norm: 36.38106696912655
Epoch: 4483, Batch Gradient Norm after: 22.360678653865826
Epoch 4484/10000, Prediction Accuracy = 58.517999999999994%, Loss = 0.8971708178520202
Epoch: 4484, Batch Gradient Norm: 33.07473784160444
Epoch: 4484, Batch Gradient Norm after: 22.36067793391301
Epoch 4485/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.8886936068534851
Epoch: 4485, Batch Gradient Norm: 36.37806960266465
Epoch: 4485, Batch Gradient Norm after: 22.360679465524715
Epoch 4486/10000, Prediction Accuracy = 58.524%, Loss = 0.8970192193984985
Epoch: 4486, Batch Gradient Norm: 33.0695174067797
Epoch: 4486, Batch Gradient Norm after: 22.360677205577364
Epoch 4487/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.8885376930236817
Epoch: 4487, Batch Gradient Norm: 36.37290137507778
Epoch: 4487, Batch Gradient Norm after: 22.36067648336596
Epoch 4488/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.8968661785125732
Epoch: 4488, Batch Gradient Norm: 33.064199511041764
Epoch: 4488, Batch Gradient Norm after: 22.360676903178295
Epoch 4489/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8883831381797791
Epoch: 4489, Batch Gradient Norm: 36.3661874576374
Epoch: 4489, Batch Gradient Norm after: 22.36067920136458
Epoch 4490/10000, Prediction Accuracy = 58.536%, Loss = 0.896711778640747
Epoch: 4490, Batch Gradient Norm: 33.05781259094217
Epoch: 4490, Batch Gradient Norm after: 22.36067740227019
Epoch 4491/10000, Prediction Accuracy = 58.524%, Loss = 0.8882284522056579
Epoch: 4491, Batch Gradient Norm: 36.36560041759596
Epoch: 4491, Batch Gradient Norm after: 22.360677942159096
Epoch 4492/10000, Prediction Accuracy = 58.536%, Loss = 0.8965735077857971
Epoch: 4492, Batch Gradient Norm: 33.0500705786879
Epoch: 4492, Batch Gradient Norm after: 22.36067722710636
Epoch 4493/10000, Prediction Accuracy = 58.52199999999999%, Loss = 0.8880673527717591
Epoch: 4493, Batch Gradient Norm: 36.36584798947776
Epoch: 4493, Batch Gradient Norm after: 22.36067834669731
Epoch 4494/10000, Prediction Accuracy = 58.532%, Loss = 0.8964316725730896
Epoch: 4494, Batch Gradient Norm: 33.042826683350405
Epoch: 4494, Batch Gradient Norm after: 22.360676447874994
Epoch 4495/10000, Prediction Accuracy = 58.519999999999996%, Loss = 0.8879070162773133
Epoch: 4495, Batch Gradient Norm: 36.367414198642095
Epoch: 4495, Batch Gradient Norm after: 22.36067615009052
Epoch 4496/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.8962943077087402
Epoch: 4496, Batch Gradient Norm: 33.03650306818758
Epoch: 4496, Batch Gradient Norm after: 22.360675284869888
Epoch 4497/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.8877449750900268
Epoch: 4497, Batch Gradient Norm: 36.3658433511946
Epoch: 4497, Batch Gradient Norm after: 22.360676752598152
Epoch 4498/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.8961556553840637
Epoch: 4498, Batch Gradient Norm: 33.02903674301566
Epoch: 4498, Batch Gradient Norm after: 22.3606780898588
Epoch 4499/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.8875860333442688
Epoch: 4499, Batch Gradient Norm: 36.36483460202476
Epoch: 4499, Batch Gradient Norm after: 22.36067675403109
Epoch 4500/10000, Prediction Accuracy = 58.529999999999994%, Loss = 0.8960128903388977
Epoch: 4500, Batch Gradient Norm: 33.020631116452826
Epoch: 4500, Batch Gradient Norm after: 22.360676212578404
Epoch 4501/10000, Prediction Accuracy = 58.544%, Loss = 0.8874277353286744
Epoch: 4501, Batch Gradient Norm: 36.36637306107247
Epoch: 4501, Batch Gradient Norm after: 22.360674759165352
Epoch 4502/10000, Prediction Accuracy = 58.528%, Loss = 0.8958794474601746
Epoch: 4502, Batch Gradient Norm: 33.01322603776757
Epoch: 4502, Batch Gradient Norm after: 22.36067775038463
Epoch 4503/10000, Prediction Accuracy = 58.55%, Loss = 0.8872693300247192
Epoch: 4503, Batch Gradient Norm: 36.366974229779814
Epoch: 4503, Batch Gradient Norm after: 22.3606770719385
Epoch 4504/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8957409262657166
Epoch: 4504, Batch Gradient Norm: 33.006108225584406
Epoch: 4504, Batch Gradient Norm after: 22.360679870314886
Epoch 4505/10000, Prediction Accuracy = 58.55%, Loss = 0.8871082782745361
Epoch: 4505, Batch Gradient Norm: 36.36564592462699
Epoch: 4505, Batch Gradient Norm after: 22.36067670891035
Epoch 4506/10000, Prediction Accuracy = 58.536%, Loss = 0.8956020474433899
Epoch: 4506, Batch Gradient Norm: 32.999988057041286
Epoch: 4506, Batch Gradient Norm after: 22.360676945057566
Epoch 4507/10000, Prediction Accuracy = 58.55400000000001%, Loss = 0.8869513392448425
Epoch: 4507, Batch Gradient Norm: 36.365881068900876
Epoch: 4507, Batch Gradient Norm after: 22.36067866672854
Epoch 4508/10000, Prediction Accuracy = 58.538%, Loss = 0.8954607248306274
Epoch: 4508, Batch Gradient Norm: 32.99463527500906
Epoch: 4508, Batch Gradient Norm after: 22.360677248291516
Epoch 4509/10000, Prediction Accuracy = 58.553999999999995%, Loss = 0.8867962002754212
Epoch: 4509, Batch Gradient Norm: 36.370721374376565
Epoch: 4509, Batch Gradient Norm after: 22.360678608489764
Epoch 4510/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8953287839889527
Epoch: 4510, Batch Gradient Norm: 32.98489672912012
Epoch: 4510, Batch Gradient Norm after: 22.36067866281613
Epoch 4511/10000, Prediction Accuracy = 58.56400000000001%, Loss = 0.8866375207901
Epoch: 4511, Batch Gradient Norm: 36.36594317139513
Epoch: 4511, Batch Gradient Norm after: 22.360678645640828
Epoch 4512/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.8951799273490906
Epoch: 4512, Batch Gradient Norm: 32.98127330293598
Epoch: 4512, Batch Gradient Norm after: 22.360678224694507
Epoch 4513/10000, Prediction Accuracy = 58.55799999999999%, Loss = 0.886482322216034
Epoch: 4513, Batch Gradient Norm: 36.36448494638495
Epoch: 4513, Batch Gradient Norm after: 22.360679376105516
Epoch 4514/10000, Prediction Accuracy = 58.538%, Loss = 0.89503093957901
Epoch: 4514, Batch Gradient Norm: 32.97357883742201
Epoch: 4514, Batch Gradient Norm after: 22.360678805261863
Epoch 4515/10000, Prediction Accuracy = 58.568000000000005%, Loss = 0.8863274216651916
Epoch: 4515, Batch Gradient Norm: 36.36483644999757
Epoch: 4515, Batch Gradient Norm after: 22.36067811504919
Epoch 4516/10000, Prediction Accuracy = 58.54%, Loss = 0.8948951005935669
Epoch: 4516, Batch Gradient Norm: 32.967416721870194
Epoch: 4516, Batch Gradient Norm after: 22.360678323174596
Epoch 4517/10000, Prediction Accuracy = 58.565999999999995%, Loss = 0.886168098449707
Epoch: 4517, Batch Gradient Norm: 36.36568902805607
Epoch: 4517, Batch Gradient Norm after: 22.36067917798888
Epoch 4518/10000, Prediction Accuracy = 58.54200000000001%, Loss = 0.8947572588920594
Epoch: 4518, Batch Gradient Norm: 32.96094338357953
Epoch: 4518, Batch Gradient Norm after: 22.360679796503618
Epoch 4519/10000, Prediction Accuracy = 58.568000000000005%, Loss = 0.8860138177871704
Epoch: 4519, Batch Gradient Norm: 36.364981954043884
Epoch: 4519, Batch Gradient Norm after: 22.360678111990484
Epoch 4520/10000, Prediction Accuracy = 58.53800000000001%, Loss = 0.8946199417114258
Epoch: 4520, Batch Gradient Norm: 32.954852511680045
Epoch: 4520, Batch Gradient Norm after: 22.360680526775585
Epoch 4521/10000, Prediction Accuracy = 58.56600000000001%, Loss = 0.8858598232269287
Epoch: 4521, Batch Gradient Norm: 36.36229041152378
Epoch: 4521, Batch Gradient Norm after: 22.360676120232434
Epoch 4522/10000, Prediction Accuracy = 58.54599999999999%, Loss = 0.8944747447967529
Epoch: 4522, Batch Gradient Norm: 32.94863338615641
Epoch: 4522, Batch Gradient Norm after: 22.36067927741257
Epoch 4523/10000, Prediction Accuracy = 58.576%, Loss = 0.8857053399085999
Epoch: 4523, Batch Gradient Norm: 36.3593639313495
Epoch: 4523, Batch Gradient Norm after: 22.36067742740984
Epoch 4524/10000, Prediction Accuracy = 58.55000000000001%, Loss = 0.8943289518356323
Epoch: 4524, Batch Gradient Norm: 32.94441551066642
Epoch: 4524, Batch Gradient Norm after: 22.360678764978314
Epoch 4525/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.8855520844459533
Epoch: 4525, Batch Gradient Norm: 36.35588585223848
Epoch: 4525, Batch Gradient Norm after: 22.36067860397514
Epoch 4526/10000, Prediction Accuracy = 58.55799999999999%, Loss = 0.8941808938980103
Epoch: 4526, Batch Gradient Norm: 32.93769168008825
Epoch: 4526, Batch Gradient Norm after: 22.360678947733465
Epoch 4527/10000, Prediction Accuracy = 58.586%, Loss = 0.8853995323181152
Epoch: 4527, Batch Gradient Norm: 36.35838481227653
Epoch: 4527, Batch Gradient Norm after: 22.3606792552396
Epoch 4528/10000, Prediction Accuracy = 58.553999999999995%, Loss = 0.8940357565879822
Epoch: 4528, Batch Gradient Norm: 32.93147140375654
Epoch: 4528, Batch Gradient Norm after: 22.360680134420697
Epoch 4529/10000, Prediction Accuracy = 58.584%, Loss = 0.8852439641952514
Epoch: 4529, Batch Gradient Norm: 36.353434860284395
Epoch: 4529, Batch Gradient Norm after: 22.360678486797752
Epoch 4530/10000, Prediction Accuracy = 58.552%, Loss = 0.8938806056976318
Epoch: 4530, Batch Gradient Norm: 32.927144529040476
Epoch: 4530, Batch Gradient Norm after: 22.36067907450808
Epoch 4531/10000, Prediction Accuracy = 58.576%, Loss = 0.8850938320159912
Epoch: 4531, Batch Gradient Norm: 36.34778251508384
Epoch: 4531, Batch Gradient Norm after: 22.360679663688458
Epoch 4532/10000, Prediction Accuracy = 58.552%, Loss = 0.8937196016311646
Epoch: 4532, Batch Gradient Norm: 32.92427883285436
Epoch: 4532, Batch Gradient Norm after: 22.36067921729256
Epoch 4533/10000, Prediction Accuracy = 58.58%, Loss = 0.8849434018135071
Epoch: 4533, Batch Gradient Norm: 36.340794839451874
Epoch: 4533, Batch Gradient Norm after: 22.36067886975579
Epoch 4534/10000, Prediction Accuracy = 58.562%, Loss = 0.8935640215873718
Epoch: 4534, Batch Gradient Norm: 32.91769844634234
Epoch: 4534, Batch Gradient Norm after: 22.360678184028597
Epoch 4535/10000, Prediction Accuracy = 58.581999999999994%, Loss = 0.8847944378852844
Epoch: 4535, Batch Gradient Norm: 36.33672487624193
Epoch: 4535, Batch Gradient Norm after: 22.360677644774732
Epoch 4536/10000, Prediction Accuracy = 58.565999999999995%, Loss = 0.8934136629104614
Epoch: 4536, Batch Gradient Norm: 32.91325284185887
Epoch: 4536, Batch Gradient Norm after: 22.360677038986253
Epoch 4537/10000, Prediction Accuracy = 58.584%, Loss = 0.8846471786499024
Epoch: 4537, Batch Gradient Norm: 36.326795280197544
Epoch: 4537, Batch Gradient Norm after: 22.36067907668303
Epoch 4538/10000, Prediction Accuracy = 58.568000000000005%, Loss = 0.8932489395141602
Epoch: 4538, Batch Gradient Norm: 32.91333294488246
Epoch: 4538, Batch Gradient Norm after: 22.360676897534
Epoch 4539/10000, Prediction Accuracy = 58.584%, Loss = 0.8844992399215699
Epoch: 4539, Batch Gradient Norm: 36.31657791927861
Epoch: 4539, Batch Gradient Norm after: 22.3606784563697
Epoch 4540/10000, Prediction Accuracy = 58.57600000000001%, Loss = 0.893080985546112
Epoch: 4540, Batch Gradient Norm: 32.908506864441016
Epoch: 4540, Batch Gradient Norm after: 22.360675458412885
Epoch 4541/10000, Prediction Accuracy = 58.596000000000004%, Loss = 0.8843547701835632
Epoch: 4541, Batch Gradient Norm: 36.304402453491925
Epoch: 4541, Batch Gradient Norm after: 22.360677669887224
Epoch 4542/10000, Prediction Accuracy = 58.580000000000005%, Loss = 0.8929098725318909
Epoch: 4542, Batch Gradient Norm: 32.907731992470296
Epoch: 4542, Batch Gradient Norm after: 22.36067780904754
Epoch 4543/10000, Prediction Accuracy = 58.598%, Loss = 0.8842124581336975
Epoch: 4543, Batch Gradient Norm: 36.28973861442443
Epoch: 4543, Batch Gradient Norm after: 22.360678315702973
Epoch 4544/10000, Prediction Accuracy = 58.588%, Loss = 0.8927259325981141
Epoch: 4544, Batch Gradient Norm: 32.908003943606275
Epoch: 4544, Batch Gradient Norm after: 22.360679068129244
Epoch 4545/10000, Prediction Accuracy = 58.604%, Loss = 0.8840692281723023
Epoch: 4545, Batch Gradient Norm: 36.277030515050164
Epoch: 4545, Batch Gradient Norm after: 22.360677365623022
Epoch 4546/10000, Prediction Accuracy = 58.588%, Loss = 0.8925495624542237
Epoch: 4546, Batch Gradient Norm: 32.907024409218224
Epoch: 4546, Batch Gradient Norm after: 22.360679427788618
Epoch 4547/10000, Prediction Accuracy = 58.61%, Loss = 0.8839272618293762
Epoch: 4547, Batch Gradient Norm: 36.26552819070733
Epoch: 4547, Batch Gradient Norm after: 22.36067672349465
Epoch 4548/10000, Prediction Accuracy = 58.598%, Loss = 0.8923739314079284
Epoch: 4548, Batch Gradient Norm: 32.907742585558715
Epoch: 4548, Batch Gradient Norm after: 22.360677693345874
Epoch 4549/10000, Prediction Accuracy = 58.616%, Loss = 0.8837881326675415
Epoch: 4549, Batch Gradient Norm: 36.24887157739283
Epoch: 4549, Batch Gradient Norm after: 22.360677935706818
Epoch 4550/10000, Prediction Accuracy = 58.596000000000004%, Loss = 0.8921889305114746
Epoch: 4550, Batch Gradient Norm: 32.90920081422494
Epoch: 4550, Batch Gradient Norm after: 22.360678905281123
Epoch 4551/10000, Prediction Accuracy = 58.612%, Loss = 0.8836485743522644
Epoch: 4551, Batch Gradient Norm: 36.23634871260966
Epoch: 4551, Batch Gradient Norm after: 22.36067572343118
Epoch 4552/10000, Prediction Accuracy = 58.596000000000004%, Loss = 0.8920129656791687
Epoch: 4552, Batch Gradient Norm: 32.90774760098154
Epoch: 4552, Batch Gradient Norm after: 22.36067984769645
Epoch 4553/10000, Prediction Accuracy = 58.608000000000004%, Loss = 0.8835046529769898
Epoch: 4553, Batch Gradient Norm: 36.22553102849467
Epoch: 4553, Batch Gradient Norm after: 22.36067520851749
Epoch 4554/10000, Prediction Accuracy = 58.598%, Loss = 0.8918472051620483
Epoch: 4554, Batch Gradient Norm: 32.90775480302745
Epoch: 4554, Batch Gradient Norm after: 22.360680076765068
Epoch 4555/10000, Prediction Accuracy = 58.612%, Loss = 0.8833642125129699
Epoch: 4555, Batch Gradient Norm: 36.21382604888269
Epoch: 4555, Batch Gradient Norm after: 22.360676338924943
Epoch 4556/10000, Prediction Accuracy = 58.6%, Loss = 0.8916834831237793
Epoch: 4556, Batch Gradient Norm: 32.90357290036547
Epoch: 4556, Batch Gradient Norm after: 22.360679468806136
Epoch 4557/10000, Prediction Accuracy = 58.60999999999999%, Loss = 0.8832218766212463
Epoch: 4557, Batch Gradient Norm: 36.19773191073531
Epoch: 4557, Batch Gradient Norm after: 22.360677447044196
Epoch 4558/10000, Prediction Accuracy = 58.60600000000001%, Loss = 0.8915018916130066
Epoch: 4558, Batch Gradient Norm: 32.90432894856379
Epoch: 4558, Batch Gradient Norm after: 22.360677622116018
Epoch 4559/10000, Prediction Accuracy = 58.61%, Loss = 0.8830832123756409
Epoch: 4559, Batch Gradient Norm: 36.182003587612286
Epoch: 4559, Batch Gradient Norm after: 22.36067708015284
Epoch 4560/10000, Prediction Accuracy = 58.614%, Loss = 0.8913210868835449
Epoch: 4560, Batch Gradient Norm: 32.902214520864206
Epoch: 4560, Batch Gradient Norm after: 22.36067994425548
Epoch 4561/10000, Prediction Accuracy = 58.61%, Loss = 0.8829464673995971
Epoch: 4561, Batch Gradient Norm: 36.16616782103943
Epoch: 4561, Batch Gradient Norm after: 22.36067746286203
Epoch 4562/10000, Prediction Accuracy = 58.616%, Loss = 0.8911407828330994
Epoch: 4562, Batch Gradient Norm: 32.903000711672185
Epoch: 4562, Batch Gradient Norm after: 22.360677584240484
Epoch 4563/10000, Prediction Accuracy = 58.614%, Loss = 0.8828050017356872
Epoch: 4563, Batch Gradient Norm: 36.154334929181545
Epoch: 4563, Batch Gradient Norm after: 22.360676917690174
Epoch 4564/10000, Prediction Accuracy = 58.61400000000001%, Loss = 0.8909684896469117
Epoch: 4564, Batch Gradient Norm: 32.90087205104504
Epoch: 4564, Batch Gradient Norm after: 22.36067475913362
Epoch 4565/10000, Prediction Accuracy = 58.617999999999995%, Loss = 0.8826612591743469
Epoch: 4565, Batch Gradient Norm: 36.141775121332756
Epoch: 4565, Batch Gradient Norm after: 22.360676967328562
Epoch 4566/10000, Prediction Accuracy = 58.61%, Loss = 0.8907979011535645
Epoch: 4566, Batch Gradient Norm: 32.89977173123906
Epoch: 4566, Batch Gradient Norm after: 22.360676353228083
Epoch 4567/10000, Prediction Accuracy = 58.626%, Loss = 0.8825210571289063
Epoch: 4567, Batch Gradient Norm: 36.13027005850709
Epoch: 4567, Batch Gradient Norm after: 22.360675928524053
Epoch 4568/10000, Prediction Accuracy = 58.61999999999999%, Loss = 0.8906284093856811
Epoch: 4568, Batch Gradient Norm: 32.89962755117184
Epoch: 4568, Batch Gradient Norm after: 22.36067973407766
Epoch 4569/10000, Prediction Accuracy = 58.620000000000005%, Loss = 0.882376754283905
Epoch: 4569, Batch Gradient Norm: 36.12119302183092
Epoch: 4569, Batch Gradient Norm after: 22.360677052798838
Epoch 4570/10000, Prediction Accuracy = 58.614%, Loss = 0.8904682397842407
Epoch: 4570, Batch Gradient Norm: 32.89404283694268
Epoch: 4570, Batch Gradient Norm after: 22.360678753488376
Epoch 4571/10000, Prediction Accuracy = 58.624%, Loss = 0.8822346806526185
Epoch: 4571, Batch Gradient Norm: 36.11224958760156
Epoch: 4571, Batch Gradient Norm after: 22.36067699687498
Epoch 4572/10000, Prediction Accuracy = 58.612%, Loss = 0.890306806564331
Epoch: 4572, Batch Gradient Norm: 32.89136162912242
Epoch: 4572, Batch Gradient Norm after: 22.360679205599254
Epoch 4573/10000, Prediction Accuracy = 58.622%, Loss = 0.8820885300636292
Epoch: 4573, Batch Gradient Norm: 36.10270921853216
Epoch: 4573, Batch Gradient Norm after: 22.360678486710214
Epoch 4574/10000, Prediction Accuracy = 58.616%, Loss = 0.8901436567306519
Epoch: 4574, Batch Gradient Norm: 32.88904507183653
Epoch: 4574, Batch Gradient Norm after: 22.36068242547363
Epoch 4575/10000, Prediction Accuracy = 58.63199999999999%, Loss = 0.8819431662559509
Epoch: 4575, Batch Gradient Norm: 36.09506584375327
Epoch: 4575, Batch Gradient Norm after: 22.3606768460174
Epoch 4576/10000, Prediction Accuracy = 58.61%, Loss = 0.8899878978729248
Epoch: 4576, Batch Gradient Norm: 32.88335410880098
Epoch: 4576, Batch Gradient Norm after: 22.360678627844358
Epoch 4577/10000, Prediction Accuracy = 58.64200000000001%, Loss = 0.8817962884902955
Epoch: 4577, Batch Gradient Norm: 36.08718280150858
Epoch: 4577, Batch Gradient Norm after: 22.36067802121096
Epoch 4578/10000, Prediction Accuracy = 58.614%, Loss = 0.8898340344429017
Epoch: 4578, Batch Gradient Norm: 32.88036960382415
Epoch: 4578, Batch Gradient Norm after: 22.360679454173635
Epoch 4579/10000, Prediction Accuracy = 58.638%, Loss = 0.8816532611846923
Epoch: 4579, Batch Gradient Norm: 36.077021180586215
Epoch: 4579, Batch Gradient Norm after: 22.36067919523684
Epoch 4580/10000, Prediction Accuracy = 58.614%, Loss = 0.889677906036377
Epoch: 4580, Batch Gradient Norm: 32.87643564394761
Epoch: 4580, Batch Gradient Norm after: 22.36068011733882
Epoch 4581/10000, Prediction Accuracy = 58.638%, Loss = 0.8815054059028625
Epoch: 4581, Batch Gradient Norm: 36.06982400006184
Epoch: 4581, Batch Gradient Norm after: 22.360677002471473
Epoch 4582/10000, Prediction Accuracy = 58.622%, Loss = 0.8895224452018737
Epoch: 4582, Batch Gradient Norm: 32.8717170966067
Epoch: 4582, Batch Gradient Norm after: 22.360679594200118
Epoch 4583/10000, Prediction Accuracy = 58.64000000000001%, Loss = 0.881362783908844
Epoch: 4583, Batch Gradient Norm: 36.06345945432364
Epoch: 4583, Batch Gradient Norm after: 22.36067745950788
Epoch 4584/10000, Prediction Accuracy = 58.63000000000001%, Loss = 0.8893702864646912
Epoch: 4584, Batch Gradient Norm: 32.86833721663711
Epoch: 4584, Batch Gradient Norm after: 22.360679890205112
Epoch 4585/10000, Prediction Accuracy = 58.638%, Loss = 0.8812143087387085
Epoch: 4585, Batch Gradient Norm: 36.056120356269716
Epoch: 4585, Batch Gradient Norm after: 22.360677817722063
Epoch 4586/10000, Prediction Accuracy = 58.63000000000001%, Loss = 0.8892099976539611
Epoch: 4586, Batch Gradient Norm: 32.86480889929493
Epoch: 4586, Batch Gradient Norm after: 22.360680520651922
Epoch 4587/10000, Prediction Accuracy = 58.63399999999999%, Loss = 0.8810710549354553
Epoch: 4587, Batch Gradient Norm: 36.04640585260146
Epoch: 4587, Batch Gradient Norm after: 22.36067802925988
Epoch 4588/10000, Prediction Accuracy = 58.632000000000005%, Loss = 0.8890451669692994
Epoch: 4588, Batch Gradient Norm: 32.86171619788478
Epoch: 4588, Batch Gradient Norm after: 22.36067992569488
Epoch 4589/10000, Prediction Accuracy = 58.638%, Loss = 0.8809256672859191
Epoch: 4589, Batch Gradient Norm: 36.03593357611886
Epoch: 4589, Batch Gradient Norm after: 22.360678157081093
Epoch 4590/10000, Prediction Accuracy = 58.628%, Loss = 0.888883399963379
Epoch: 4590, Batch Gradient Norm: 32.859685649271555
Epoch: 4590, Batch Gradient Norm after: 22.360678581088138
Epoch 4591/10000, Prediction Accuracy = 58.64%, Loss = 0.8807836174964905
Epoch: 4591, Batch Gradient Norm: 36.02704547880782
Epoch: 4591, Batch Gradient Norm after: 22.36067582578392
Epoch 4592/10000, Prediction Accuracy = 58.638%, Loss = 0.8887192487716675
Epoch: 4592, Batch Gradient Norm: 32.85617847957639
Epoch: 4592, Batch Gradient Norm after: 22.36067951200041
Epoch 4593/10000, Prediction Accuracy = 58.648%, Loss = 0.8806425571441651
Epoch: 4593, Batch Gradient Norm: 36.01861990489994
Epoch: 4593, Batch Gradient Norm after: 22.360678298106254
Epoch 4594/10000, Prediction Accuracy = 58.652%, Loss = 0.8885606050491333
Epoch: 4594, Batch Gradient Norm: 32.852783195660805
Epoch: 4594, Batch Gradient Norm after: 22.360680724514857
Epoch 4595/10000, Prediction Accuracy = 58.657999999999994%, Loss = 0.8804966092109681
Epoch: 4595, Batch Gradient Norm: 36.0123360877034
Epoch: 4595, Batch Gradient Norm after: 22.360678721563964
Epoch 4596/10000, Prediction Accuracy = 58.65%, Loss = 0.8884078979492187
Epoch: 4596, Batch Gradient Norm: 32.850547309109
Epoch: 4596, Batch Gradient Norm after: 22.360680384799206
Epoch 4597/10000, Prediction Accuracy = 58.66799999999999%, Loss = 0.8803545117378235
Epoch: 4597, Batch Gradient Norm: 36.00415572657526
Epoch: 4597, Batch Gradient Norm after: 22.36067659403484
Epoch 4598/10000, Prediction Accuracy = 58.641999999999996%, Loss = 0.8882465124130249
Epoch: 4598, Batch Gradient Norm: 32.847045127033056
Epoch: 4598, Batch Gradient Norm after: 22.360678824926854
Epoch 4599/10000, Prediction Accuracy = 58.67%, Loss = 0.8802101612091064
Epoch: 4599, Batch Gradient Norm: 35.99730760814093
Epoch: 4599, Batch Gradient Norm after: 22.360678356918417
Epoch 4600/10000, Prediction Accuracy = 58.64%, Loss = 0.8880922675132752
Epoch: 4600, Batch Gradient Norm: 32.84277342604838
Epoch: 4600, Batch Gradient Norm after: 22.360680905293137
Epoch 4601/10000, Prediction Accuracy = 58.669999999999995%, Loss = 0.8800641059875488
Epoch: 4601, Batch Gradient Norm: 35.98801663674928
Epoch: 4601, Batch Gradient Norm after: 22.360677209973115
Epoch 4602/10000, Prediction Accuracy = 58.644000000000005%, Loss = 0.8879377961158752
Epoch: 4602, Batch Gradient Norm: 32.83992738731424
Epoch: 4602, Batch Gradient Norm after: 22.36067937984107
Epoch 4603/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.8799205780029297
Epoch: 4603, Batch Gradient Norm: 35.97880026216375
Epoch: 4603, Batch Gradient Norm after: 22.36067837293377
Epoch 4604/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.8877756357192993
Epoch: 4604, Batch Gradient Norm: 32.837906323710136
Epoch: 4604, Batch Gradient Norm after: 22.360677749116835
Epoch 4605/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8797815918922425
Epoch: 4605, Batch Gradient Norm: 35.96643370519644
Epoch: 4605, Batch Gradient Norm after: 22.360678998222394
Epoch 4606/10000, Prediction Accuracy = 58.65599999999999%, Loss = 0.8876129031181336
Epoch: 4606, Batch Gradient Norm: 32.834267126566345
Epoch: 4606, Batch Gradient Norm after: 22.360678585483203
Epoch 4607/10000, Prediction Accuracy = 58.67%, Loss = 0.8796417832374572
Epoch: 4607, Batch Gradient Norm: 35.95380948048477
Epoch: 4607, Batch Gradient Norm after: 22.360678173518806
Epoch 4608/10000, Prediction Accuracy = 58.657999999999994%, Loss = 0.887449312210083
Epoch: 4608, Batch Gradient Norm: 32.83368518825609
Epoch: 4608, Batch Gradient Norm after: 22.36067783825162
Epoch 4609/10000, Prediction Accuracy = 58.676%, Loss = 0.8794981241226196
Epoch: 4609, Batch Gradient Norm: 35.941888848100696
Epoch: 4609, Batch Gradient Norm after: 22.360678856492903
Epoch 4610/10000, Prediction Accuracy = 58.664%, Loss = 0.8872851133346558
Epoch: 4610, Batch Gradient Norm: 32.83225898440586
Epoch: 4610, Batch Gradient Norm after: 22.360677492114693
Epoch 4611/10000, Prediction Accuracy = 58.67%, Loss = 0.8793623685836792
Epoch: 4611, Batch Gradient Norm: 35.93087617517805
Epoch: 4611, Batch Gradient Norm after: 22.36068039894211
Epoch 4612/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.8871160507202148
Epoch: 4612, Batch Gradient Norm: 32.831709686842785
Epoch: 4612, Batch Gradient Norm after: 22.36067819335463
Epoch 4613/10000, Prediction Accuracy = 58.67%, Loss = 0.8792204856872559
Epoch: 4613, Batch Gradient Norm: 35.92106264713816
Epoch: 4613, Batch Gradient Norm after: 22.360678563310778
Epoch 4614/10000, Prediction Accuracy = 58.666%, Loss = 0.8869568467140198
Epoch: 4614, Batch Gradient Norm: 32.82646386044785
Epoch: 4614, Batch Gradient Norm after: 22.360678425946723
Epoch 4615/10000, Prediction Accuracy = 58.666%, Loss = 0.87907874584198
Epoch: 4615, Batch Gradient Norm: 35.908852348025604
Epoch: 4615, Batch Gradient Norm after: 22.360678093060816
Epoch 4616/10000, Prediction Accuracy = 58.67%, Loss = 0.8867931604385376
Epoch: 4616, Batch Gradient Norm: 32.82574985673678
Epoch: 4616, Batch Gradient Norm after: 22.360679482945393
Epoch 4617/10000, Prediction Accuracy = 58.664%, Loss = 0.8789384722709656
Epoch: 4617, Batch Gradient Norm: 35.897112686354454
Epoch: 4617, Batch Gradient Norm after: 22.360680599127193
Epoch 4618/10000, Prediction Accuracy = 58.674%, Loss = 0.8866277694702148
Epoch: 4618, Batch Gradient Norm: 32.82544484279906
Epoch: 4618, Batch Gradient Norm after: 22.360677518575244
Epoch 4619/10000, Prediction Accuracy = 58.67%, Loss = 0.8788001894950866
Epoch: 4619, Batch Gradient Norm: 35.88599600586349
Epoch: 4619, Batch Gradient Norm after: 22.360680395178107
Epoch 4620/10000, Prediction Accuracy = 58.676%, Loss = 0.8864605307579041
Epoch: 4620, Batch Gradient Norm: 32.82478622999222
Epoch: 4620, Batch Gradient Norm after: 22.360676874932373
Epoch 4621/10000, Prediction Accuracy = 58.67199999999999%, Loss = 0.8786612629890442
Epoch: 4621, Batch Gradient Norm: 35.8765913282407
Epoch: 4621, Batch Gradient Norm after: 22.36068082100636
Epoch 4622/10000, Prediction Accuracy = 58.67399999999999%, Loss = 0.8863002538681031
Epoch: 4622, Batch Gradient Norm: 32.81967422267367
Epoch: 4622, Batch Gradient Norm after: 22.360679210681504
Epoch 4623/10000, Prediction Accuracy = 58.664%, Loss = 0.8785237073898315
Epoch: 4623, Batch Gradient Norm: 35.86802353925746
Epoch: 4623, Batch Gradient Norm after: 22.36067999400418
Epoch 4624/10000, Prediction Accuracy = 58.67399999999999%, Loss = 0.8861425518989563
Epoch: 4624, Batch Gradient Norm: 32.8144428999516
Epoch: 4624, Batch Gradient Norm after: 22.360677306633022
Epoch 4625/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.878380823135376
Epoch: 4625, Batch Gradient Norm: 35.86011045653603
Epoch: 4625, Batch Gradient Norm after: 22.36067812296643
Epoch 4626/10000, Prediction Accuracy = 58.676%, Loss = 0.8859898567199707
Epoch: 4626, Batch Gradient Norm: 32.811904238994856
Epoch: 4626, Batch Gradient Norm after: 22.360678626326838
Epoch 4627/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8782358527183532
Epoch: 4627, Batch Gradient Norm: 35.855622776825484
Epoch: 4627, Batch Gradient Norm after: 22.360678353383122
Epoch 4628/10000, Prediction Accuracy = 58.674%, Loss = 0.8858448147773743
Epoch: 4628, Batch Gradient Norm: 32.80717922220125
Epoch: 4628, Batch Gradient Norm after: 22.360677875854584
Epoch 4629/10000, Prediction Accuracy = 58.674%, Loss = 0.8780876517295837
Epoch: 4629, Batch Gradient Norm: 35.84997727610549
Epoch: 4629, Batch Gradient Norm after: 22.360677361374673
Epoch 4630/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.8856952667236329
Epoch: 4630, Batch Gradient Norm: 32.80365920028223
Epoch: 4630, Batch Gradient Norm after: 22.360678260770538
Epoch 4631/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8779422879219055
Epoch: 4631, Batch Gradient Norm: 35.84079401579198
Epoch: 4631, Batch Gradient Norm after: 22.36067700859521
Epoch 4632/10000, Prediction Accuracy = 58.674%, Loss = 0.8855367541313172
Epoch: 4632, Batch Gradient Norm: 32.802976360653865
Epoch: 4632, Batch Gradient Norm after: 22.36067854571178
Epoch 4633/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.8777975082397461
Epoch: 4633, Batch Gradient Norm: 35.83524240666071
Epoch: 4633, Batch Gradient Norm after: 22.360677970860838
Epoch 4634/10000, Prediction Accuracy = 58.664%, Loss = 0.8853869438171387
Epoch: 4634, Batch Gradient Norm: 32.79587784928212
Epoch: 4634, Batch Gradient Norm after: 22.36067961239787
Epoch 4635/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.8776568174362183
Epoch: 4635, Batch Gradient Norm: 35.82472981799849
Epoch: 4635, Batch Gradient Norm after: 22.36067936169347
Epoch 4636/10000, Prediction Accuracy = 58.666%, Loss = 0.8852256774902344
Epoch: 4636, Batch Gradient Norm: 32.79725506741911
Epoch: 4636, Batch Gradient Norm after: 22.360678793986295
Epoch 4637/10000, Prediction Accuracy = 58.674%, Loss = 0.8775177717208862
Epoch: 4637, Batch Gradient Norm: 35.81238430176406
Epoch: 4637, Batch Gradient Norm after: 22.36067939975312
Epoch 4638/10000, Prediction Accuracy = 58.66799999999999%, Loss = 0.8850580215454101
Epoch: 4638, Batch Gradient Norm: 32.79704453579596
Epoch: 4638, Batch Gradient Norm after: 22.360678976731624
Epoch 4639/10000, Prediction Accuracy = 58.68000000000001%, Loss = 0.8773882389068604
Epoch: 4639, Batch Gradient Norm: 35.79793053166515
Epoch: 4639, Batch Gradient Norm after: 22.3606767434773
Epoch 4640/10000, Prediction Accuracy = 58.662%, Loss = 0.8848892569541931
Epoch: 4640, Batch Gradient Norm: 32.797085770940825
Epoch: 4640, Batch Gradient Norm after: 22.360679307161465
Epoch 4641/10000, Prediction Accuracy = 58.676%, Loss = 0.8772541761398316
Epoch: 4641, Batch Gradient Norm: 35.77996489403339
Epoch: 4641, Batch Gradient Norm after: 22.36067815102201
Epoch 4642/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8847082376480102
Epoch: 4642, Batch Gradient Norm: 32.798625250586866
Epoch: 4642, Batch Gradient Norm after: 22.360678946035055
Epoch 4643/10000, Prediction Accuracy = 58.678%, Loss = 0.8771288514137268
Epoch: 4643, Batch Gradient Norm: 35.762792116131514
Epoch: 4643, Batch Gradient Norm after: 22.360679912418
Epoch 4644/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8845278501510621
Epoch: 4644, Batch Gradient Norm: 32.8044922780066
Epoch: 4644, Batch Gradient Norm after: 22.36067915265285
Epoch 4645/10000, Prediction Accuracy = 58.678%, Loss = 0.8769983053207397
Epoch: 4645, Batch Gradient Norm: 35.74506674287729
Epoch: 4645, Batch Gradient Norm after: 22.360680058961393
Epoch 4646/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8843454599380494
Epoch: 4646, Batch Gradient Norm: 32.80533646128359
Epoch: 4646, Batch Gradient Norm after: 22.360679476887128
Epoch 4647/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.8768701314926147
Epoch: 4647, Batch Gradient Norm: 35.727018892687035
Epoch: 4647, Batch Gradient Norm after: 22.360677559756923
Epoch 4648/10000, Prediction Accuracy = 58.669999999999995%, Loss = 0.8841677784919739
Epoch: 4648, Batch Gradient Norm: 32.80887599678729
Epoch: 4648, Batch Gradient Norm after: 22.3606785882195
Epoch 4649/10000, Prediction Accuracy = 58.669999999999995%, Loss = 0.8767417669296265
Epoch: 4649, Batch Gradient Norm: 35.71088865077651
Epoch: 4649, Batch Gradient Norm after: 22.360676813751287
Epoch 4650/10000, Prediction Accuracy = 58.664%, Loss = 0.8839873433113098
Epoch: 4650, Batch Gradient Norm: 32.81129557583763
Epoch: 4650, Batch Gradient Norm after: 22.360679737106768
Epoch 4651/10000, Prediction Accuracy = 58.664%, Loss = 0.8766129851341248
Epoch: 4651, Batch Gradient Norm: 35.69308094509634
Epoch: 4651, Batch Gradient Norm after: 22.360676797884537
Epoch 4652/10000, Prediction Accuracy = 58.664%, Loss = 0.883808946609497
Epoch: 4652, Batch Gradient Norm: 32.812888185853645
Epoch: 4652, Batch Gradient Norm after: 22.360679793923733
Epoch 4653/10000, Prediction Accuracy = 58.666%, Loss = 0.8764837741851806
Epoch: 4653, Batch Gradient Norm: 35.68168087903557
Epoch: 4653, Batch Gradient Norm after: 22.360677938439263
Epoch 4654/10000, Prediction Accuracy = 58.660000000000004%, Loss = 0.8836412549018859
Epoch: 4654, Batch Gradient Norm: 32.81276120428059
Epoch: 4654, Batch Gradient Norm after: 22.36068002033024
Epoch 4655/10000, Prediction Accuracy = 58.660000000000004%, Loss = 0.8763516902923584
Epoch: 4655, Batch Gradient Norm: 35.66754894691504
Epoch: 4655, Batch Gradient Norm after: 22.360678732244512
Epoch 4656/10000, Prediction Accuracy = 58.658%, Loss = 0.8834672212600708
Epoch: 4656, Batch Gradient Norm: 32.814890326818556
Epoch: 4656, Batch Gradient Norm after: 22.360678608807905
Epoch 4657/10000, Prediction Accuracy = 58.664%, Loss = 0.8762176752090454
Epoch: 4657, Batch Gradient Norm: 35.656657292412575
Epoch: 4657, Batch Gradient Norm after: 22.360680055708134
Epoch 4658/10000, Prediction Accuracy = 58.662%, Loss = 0.8833071231842041
Epoch: 4658, Batch Gradient Norm: 32.81385528223056
Epoch: 4658, Batch Gradient Norm after: 22.3606785751932
Epoch 4659/10000, Prediction Accuracy = 58.660000000000004%, Loss = 0.8760832667350769
Epoch: 4659, Batch Gradient Norm: 35.64586248575527
Epoch: 4659, Batch Gradient Norm after: 22.36068086276169
Epoch 4660/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8831462144851685
Epoch: 4660, Batch Gradient Norm: 32.811067294459285
Epoch: 4660, Batch Gradient Norm after: 22.360677900515533
Epoch 4661/10000, Prediction Accuracy = 58.664%, Loss = 0.8759459733963013
Epoch: 4661, Batch Gradient Norm: 35.63599085631587
Epoch: 4661, Batch Gradient Norm after: 22.360677520401744
Epoch 4662/10000, Prediction Accuracy = 58.664%, Loss = 0.8829869031906128
Epoch: 4662, Batch Gradient Norm: 32.811005517883935
Epoch: 4662, Batch Gradient Norm after: 22.36067849942459
Epoch 4663/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.8758125066757202
Epoch: 4663, Batch Gradient Norm: 35.63000709588683
Epoch: 4663, Batch Gradient Norm after: 22.36067716541648
Epoch 4664/10000, Prediction Accuracy = 58.664%, Loss = 0.8828344702720642
Epoch: 4664, Batch Gradient Norm: 32.80750703588185
Epoch: 4664, Batch Gradient Norm after: 22.36067793168219
Epoch 4665/10000, Prediction Accuracy = 58.669999999999995%, Loss = 0.8756702780723572
Epoch: 4665, Batch Gradient Norm: 35.62081410809407
Epoch: 4665, Batch Gradient Norm after: 22.360677345649826
Epoch 4666/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.8826856255531311
Epoch: 4666, Batch Gradient Norm: 32.80324957152645
Epoch: 4666, Batch Gradient Norm after: 22.360677238525007
Epoch 4667/10000, Prediction Accuracy = 58.676%, Loss = 0.8755306124687194
Epoch: 4667, Batch Gradient Norm: 35.613814458179455
Epoch: 4667, Batch Gradient Norm after: 22.360677456794942
Epoch 4668/10000, Prediction Accuracy = 58.674%, Loss = 0.8825337409973144
Epoch: 4668, Batch Gradient Norm: 32.80159147327854
Epoch: 4668, Batch Gradient Norm after: 22.360676958623348
Epoch 4669/10000, Prediction Accuracy = 58.67%, Loss = 0.8753922581672668
Epoch: 4669, Batch Gradient Norm: 35.608243534256054
Epoch: 4669, Batch Gradient Norm after: 22.360679392674676
Epoch 4670/10000, Prediction Accuracy = 58.674%, Loss = 0.882383668422699
Epoch: 4670, Batch Gradient Norm: 32.79548774809268
Epoch: 4670, Batch Gradient Norm after: 22.360678299892538
Epoch 4671/10000, Prediction Accuracy = 58.666%, Loss = 0.875247061252594
Epoch: 4671, Batch Gradient Norm: 35.60295609634596
Epoch: 4671, Batch Gradient Norm after: 22.36067848191814
Epoch 4672/10000, Prediction Accuracy = 58.681999999999995%, Loss = 0.8822418808937073
Epoch: 4672, Batch Gradient Norm: 32.79360678886761
Epoch: 4672, Batch Gradient Norm after: 22.360678018244915
Epoch 4673/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.8751076579093933
Epoch: 4673, Batch Gradient Norm: 35.59786983078637
Epoch: 4673, Batch Gradient Norm after: 22.360677540297143
Epoch 4674/10000, Prediction Accuracy = 58.694%, Loss = 0.8820908188819885
Epoch: 4674, Batch Gradient Norm: 32.78947840723484
Epoch: 4674, Batch Gradient Norm after: 22.360677832988575
Epoch 4675/10000, Prediction Accuracy = 58.674%, Loss = 0.8749677062034606
Epoch: 4675, Batch Gradient Norm: 35.59187869086027
Epoch: 4675, Batch Gradient Norm after: 22.3606783683719
Epoch 4676/10000, Prediction Accuracy = 58.688%, Loss = 0.8819420814514161
Epoch: 4676, Batch Gradient Norm: 32.78506287909325
Epoch: 4676, Batch Gradient Norm after: 22.36067864957466
Epoch 4677/10000, Prediction Accuracy = 58.674%, Loss = 0.874831223487854
Epoch: 4677, Batch Gradient Norm: 35.58196358165454
Epoch: 4677, Batch Gradient Norm after: 22.36067941936493
Epoch 4678/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8817888259887695
Epoch: 4678, Batch Gradient Norm: 32.783609752218226
Epoch: 4678, Batch Gradient Norm after: 22.360679073551225
Epoch 4679/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8746939659118652
Epoch: 4679, Batch Gradient Norm: 35.5774857713199
Epoch: 4679, Batch Gradient Norm after: 22.360677219833352
Epoch 4680/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8816438436508178
Epoch: 4680, Batch Gradient Norm: 32.77668133650856
Epoch: 4680, Batch Gradient Norm after: 22.36067783804706
Epoch 4681/10000, Prediction Accuracy = 58.676%, Loss = 0.8745558023452759
Epoch: 4681, Batch Gradient Norm: 35.57022029220515
Epoch: 4681, Batch Gradient Norm after: 22.36067954070355
Epoch 4682/10000, Prediction Accuracy = 58.688%, Loss = 0.8814967989921569
Epoch: 4682, Batch Gradient Norm: 32.77114038749187
Epoch: 4682, Batch Gradient Norm after: 22.360678856499632
Epoch 4683/10000, Prediction Accuracy = 58.676%, Loss = 0.8744112610816955
Epoch: 4683, Batch Gradient Norm: 35.56458988487404
Epoch: 4683, Batch Gradient Norm after: 22.36067960568466
Epoch 4684/10000, Prediction Accuracy = 58.694%, Loss = 0.8813526272773743
Epoch: 4684, Batch Gradient Norm: 32.7656838474636
Epoch: 4684, Batch Gradient Norm after: 22.360678513019238
Epoch 4685/10000, Prediction Accuracy = 58.66799999999999%, Loss = 0.8742725133895874
Epoch: 4685, Batch Gradient Norm: 35.55814811947828
Epoch: 4685, Batch Gradient Norm after: 22.36067929465905
Epoch 4686/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8812014937400818
Epoch: 4686, Batch Gradient Norm: 32.764456422337155
Epoch: 4686, Batch Gradient Norm after: 22.360679053415865
Epoch 4687/10000, Prediction Accuracy = 58.681999999999995%, Loss = 0.8741325497627258
Epoch: 4687, Batch Gradient Norm: 35.55128768797145
Epoch: 4687, Batch Gradient Norm after: 22.360677803069116
Epoch 4688/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8810557961463928
Epoch: 4688, Batch Gradient Norm: 32.7624268661517
Epoch: 4688, Batch Gradient Norm after: 22.360679721869886
Epoch 4689/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.8739885807037353
Epoch: 4689, Batch Gradient Norm: 35.543220448933226
Epoch: 4689, Batch Gradient Norm after: 22.360678599924384
Epoch 4690/10000, Prediction Accuracy = 58.70399999999999%, Loss = 0.8809063911437989
Epoch: 4690, Batch Gradient Norm: 32.75670414415516
Epoch: 4690, Batch Gradient Norm after: 22.360677279464145
Epoch 4691/10000, Prediction Accuracy = 58.682%, Loss = 0.8738539457321167
Epoch: 4691, Batch Gradient Norm: 35.53828544946514
Epoch: 4691, Batch Gradient Norm after: 22.36067763921602
Epoch 4692/10000, Prediction Accuracy = 58.712%, Loss = 0.8807604670524597
Epoch: 4692, Batch Gradient Norm: 32.751732885304136
Epoch: 4692, Batch Gradient Norm after: 22.360679377943583
Epoch 4693/10000, Prediction Accuracy = 58.69%, Loss = 0.8737147092819214
Epoch: 4693, Batch Gradient Norm: 35.53638400253575
Epoch: 4693, Batch Gradient Norm after: 22.360676557659982
Epoch 4694/10000, Prediction Accuracy = 58.708000000000006%, Loss = 0.8806167840957642
Epoch: 4694, Batch Gradient Norm: 32.74474702538679
Epoch: 4694, Batch Gradient Norm after: 22.360680068151108
Epoch 4695/10000, Prediction Accuracy = 58.693999999999996%, Loss = 0.8735689997673035
Epoch: 4695, Batch Gradient Norm: 35.53041305714621
Epoch: 4695, Batch Gradient Norm after: 22.360677520352727
Epoch 4696/10000, Prediction Accuracy = 58.708000000000006%, Loss = 0.8804751873016358
Epoch: 4696, Batch Gradient Norm: 32.74007019040063
Epoch: 4696, Batch Gradient Norm after: 22.36067795915037
Epoch 4697/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8734299182891846
Epoch: 4697, Batch Gradient Norm: 35.52846790207691
Epoch: 4697, Batch Gradient Norm after: 22.36067616733565
Epoch 4698/10000, Prediction Accuracy = 58.708000000000006%, Loss = 0.8803407311439514
Epoch: 4698, Batch Gradient Norm: 32.732314461232875
Epoch: 4698, Batch Gradient Norm after: 22.360679461425075
Epoch 4699/10000, Prediction Accuracy = 58.71%, Loss = 0.8732769846916199
Epoch: 4699, Batch Gradient Norm: 35.525733732555885
Epoch: 4699, Batch Gradient Norm after: 22.36067735846636
Epoch 4700/10000, Prediction Accuracy = 58.71%, Loss = 0.8802084684371948
Epoch: 4700, Batch Gradient Norm: 32.73114167276068
Epoch: 4700, Batch Gradient Norm after: 22.360678012797013
Epoch 4701/10000, Prediction Accuracy = 58.71%, Loss = 0.8731342673301696
Epoch: 4701, Batch Gradient Norm: 35.52040769217834
Epoch: 4701, Batch Gradient Norm after: 22.36067919676005
Epoch 4702/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8800649404525757
Epoch: 4702, Batch Gradient Norm: 32.723814285883726
Epoch: 4702, Batch Gradient Norm after: 22.360677627564108
Epoch 4703/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8729931235313415
Epoch: 4703, Batch Gradient Norm: 35.51818291119689
Epoch: 4703, Batch Gradient Norm after: 22.360677207339577
Epoch 4704/10000, Prediction Accuracy = 58.712%, Loss = 0.8799256920814514
Epoch: 4704, Batch Gradient Norm: 32.71985418527776
Epoch: 4704, Batch Gradient Norm after: 22.36067920093776
Epoch 4705/10000, Prediction Accuracy = 58.714%, Loss = 0.8728487968444825
Epoch: 4705, Batch Gradient Norm: 35.515049960310385
Epoch: 4705, Batch Gradient Norm after: 22.360678096315475
Epoch 4706/10000, Prediction Accuracy = 58.70399999999999%, Loss = 0.8797913908958435
Epoch: 4706, Batch Gradient Norm: 32.713176634636284
Epoch: 4706, Batch Gradient Norm after: 22.360678514547498
Epoch 4707/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8727078914642334
Epoch: 4707, Batch Gradient Norm: 35.50600470543203
Epoch: 4707, Batch Gradient Norm after: 22.360678328290824
Epoch 4708/10000, Prediction Accuracy = 58.702%, Loss = 0.8796448230743408
Epoch: 4708, Batch Gradient Norm: 32.71097043227545
Epoch: 4708, Batch Gradient Norm after: 22.360678052883344
Epoch 4709/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.8725659608840942
Epoch: 4709, Batch Gradient Norm: 35.50112019021109
Epoch: 4709, Batch Gradient Norm after: 22.360678599627978
Epoch 4710/10000, Prediction Accuracy = 58.702%, Loss = 0.8795003771781922
Epoch: 4710, Batch Gradient Norm: 32.7036380027244
Epoch: 4710, Batch Gradient Norm after: 22.360679026505093
Epoch 4711/10000, Prediction Accuracy = 58.73%, Loss = 0.8724273443222046
Epoch: 4711, Batch Gradient Norm: 35.49526963455539
Epoch: 4711, Batch Gradient Norm after: 22.360676632272895
Epoch 4712/10000, Prediction Accuracy = 58.712%, Loss = 0.8793589353561402
Epoch: 4712, Batch Gradient Norm: 32.69823497715963
Epoch: 4712, Batch Gradient Norm after: 22.360678799874066
Epoch 4713/10000, Prediction Accuracy = 58.734%, Loss = 0.8722826600074768
Epoch: 4713, Batch Gradient Norm: 35.49451084882952
Epoch: 4713, Batch Gradient Norm after: 22.36067720783412
Epoch 4714/10000, Prediction Accuracy = 58.712%, Loss = 0.8792273759841919
Epoch: 4714, Batch Gradient Norm: 32.687053666897995
Epoch: 4714, Batch Gradient Norm after: 22.360677351138907
Epoch 4715/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8721363425254822
Epoch: 4715, Batch Gradient Norm: 35.4919379181728
Epoch: 4715, Batch Gradient Norm after: 22.360679425296492
Epoch 4716/10000, Prediction Accuracy = 58.71%, Loss = 0.879094660282135
Epoch: 4716, Batch Gradient Norm: 32.67983843096556
Epoch: 4716, Batch Gradient Norm after: 22.36067882121954
Epoch 4717/10000, Prediction Accuracy = 58.742000000000004%, Loss = 0.8719890594482422
Epoch: 4717, Batch Gradient Norm: 35.49300837947683
Epoch: 4717, Batch Gradient Norm after: 22.360678560276348
Epoch 4718/10000, Prediction Accuracy = 58.70799999999999%, Loss = 0.8789711952209472
Epoch: 4718, Batch Gradient Norm: 32.67154902281019
Epoch: 4718, Batch Gradient Norm after: 22.36067894215695
Epoch 4719/10000, Prediction Accuracy = 58.738%, Loss = 0.8718405127525329
Epoch: 4719, Batch Gradient Norm: 35.490325390706786
Epoch: 4719, Batch Gradient Norm after: 22.360675958920552
Epoch 4720/10000, Prediction Accuracy = 58.717999999999996%, Loss = 0.8788339495658875
Epoch: 4720, Batch Gradient Norm: 32.667818038608985
Epoch: 4720, Batch Gradient Norm after: 22.360679551273517
Epoch 4721/10000, Prediction Accuracy = 58.73199999999999%, Loss = 0.8716986060142518
Epoch: 4721, Batch Gradient Norm: 35.48583695776233
Epoch: 4721, Batch Gradient Norm after: 22.36067724956356
Epoch 4722/10000, Prediction Accuracy = 58.714%, Loss = 0.8786978602409363
Epoch: 4722, Batch Gradient Norm: 32.66049428788472
Epoch: 4722, Batch Gradient Norm after: 22.36068083735124
Epoch 4723/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8715586066246033
Epoch: 4723, Batch Gradient Norm: 35.4805570941485
Epoch: 4723, Batch Gradient Norm after: 22.36067856280209
Epoch 4724/10000, Prediction Accuracy = 58.722%, Loss = 0.8785577893257142
Epoch: 4724, Batch Gradient Norm: 32.6556601006191
Epoch: 4724, Batch Gradient Norm after: 22.36067932824745
Epoch 4725/10000, Prediction Accuracy = 58.730000000000004%, Loss = 0.8714129805564881
Epoch: 4725, Batch Gradient Norm: 35.477821251004336
Epoch: 4725, Batch Gradient Norm after: 22.36067715874589
Epoch 4726/10000, Prediction Accuracy = 58.71999999999999%, Loss = 0.8784174442291259
Epoch: 4726, Batch Gradient Norm: 32.649380890068244
Epoch: 4726, Batch Gradient Norm after: 22.36068032578686
Epoch 4727/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8712736964225769
Epoch: 4727, Batch Gradient Norm: 35.47168341653695
Epoch: 4727, Batch Gradient Norm after: 22.360678467600355
Epoch 4728/10000, Prediction Accuracy = 58.718%, Loss = 0.8782748699188232
Epoch: 4728, Batch Gradient Norm: 32.64653174256302
Epoch: 4728, Batch Gradient Norm after: 22.36068018095111
Epoch 4729/10000, Prediction Accuracy = 58.73%, Loss = 0.8711301445960998
Epoch: 4729, Batch Gradient Norm: 35.47012410759388
Epoch: 4729, Batch Gradient Norm after: 22.36067726042219
Epoch 4730/10000, Prediction Accuracy = 58.720000000000006%, Loss = 0.8781408667564392
Epoch: 4730, Batch Gradient Norm: 32.63823941083473
Epoch: 4730, Batch Gradient Norm after: 22.360681565737146
Epoch 4731/10000, Prediction Accuracy = 58.734%, Loss = 0.870989179611206
Epoch: 4731, Batch Gradient Norm: 35.46418041149347
Epoch: 4731, Batch Gradient Norm after: 22.3606772615144
Epoch 4732/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.878005039691925
Epoch: 4732, Batch Gradient Norm: 32.63390631880842
Epoch: 4732, Batch Gradient Norm after: 22.36067967664358
Epoch 4733/10000, Prediction Accuracy = 58.742%, Loss = 0.8708490610122681
Epoch: 4733, Batch Gradient Norm: 35.4605860322567
Epoch: 4733, Batch Gradient Norm after: 22.360676587822965
Epoch 4734/10000, Prediction Accuracy = 58.726%, Loss = 0.8778696537017823
Epoch: 4734, Batch Gradient Norm: 32.628601086544315
Epoch: 4734, Batch Gradient Norm after: 22.36068106170241
Epoch 4735/10000, Prediction Accuracy = 58.738%, Loss = 0.8707009077072143
Epoch: 4735, Batch Gradient Norm: 35.460824539512465
Epoch: 4735, Batch Gradient Norm after: 22.360679916577045
Epoch 4736/10000, Prediction Accuracy = 58.732000000000006%, Loss = 0.8777422070503235
Epoch: 4736, Batch Gradient Norm: 32.61981846881644
Epoch: 4736, Batch Gradient Norm after: 22.36068079976001
Epoch 4737/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.8705612659454346
Epoch: 4737, Batch Gradient Norm: 35.45829889372788
Epoch: 4737, Batch Gradient Norm after: 22.36067921190821
Epoch 4738/10000, Prediction Accuracy = 58.734%, Loss = 0.8776040434837341
Epoch: 4738, Batch Gradient Norm: 32.61368786124662
Epoch: 4738, Batch Gradient Norm after: 22.360680128145884
Epoch 4739/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8704188704490662
Epoch: 4739, Batch Gradient Norm: 35.457724353563975
Epoch: 4739, Batch Gradient Norm after: 22.360679911948527
Epoch 4740/10000, Prediction Accuracy = 58.734%, Loss = 0.8774812817573547
Epoch: 4740, Batch Gradient Norm: 32.6037053835443
Epoch: 4740, Batch Gradient Norm after: 22.360680464310853
Epoch 4741/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.8702691555023193
Epoch: 4741, Batch Gradient Norm: 35.457213170074084
Epoch: 4741, Batch Gradient Norm after: 22.36067754664701
Epoch 4742/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8773496627807618
Epoch: 4742, Batch Gradient Norm: 32.59704318764581
Epoch: 4742, Batch Gradient Norm after: 22.36068021531219
Epoch 4743/10000, Prediction Accuracy = 58.72399999999999%, Loss = 0.8701295971870422
Epoch: 4743, Batch Gradient Norm: 35.45520955920376
Epoch: 4743, Batch Gradient Norm after: 22.36067781604356
Epoch 4744/10000, Prediction Accuracy = 58.738%, Loss = 0.8772189259529114
Epoch: 4744, Batch Gradient Norm: 32.59014187035383
Epoch: 4744, Batch Gradient Norm after: 22.36067999249345
Epoch 4745/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8699871659278869
Epoch: 4745, Batch Gradient Norm: 35.44916850551197
Epoch: 4745, Batch Gradient Norm after: 22.360679017716418
Epoch 4746/10000, Prediction Accuracy = 58.746%, Loss = 0.8770836234092713
Epoch: 4746, Batch Gradient Norm: 32.5834586926453
Epoch: 4746, Batch Gradient Norm after: 22.360681793883394
Epoch 4747/10000, Prediction Accuracy = 58.718%, Loss = 0.8698510766029358
Epoch: 4747, Batch Gradient Norm: 35.44490307094364
Epoch: 4747, Batch Gradient Norm after: 22.360676506342053
Epoch 4748/10000, Prediction Accuracy = 58.746%, Loss = 0.8769457697868347
Epoch: 4748, Batch Gradient Norm: 32.579424386233626
Epoch: 4748, Batch Gradient Norm after: 22.36067905750126
Epoch 4749/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8697067141532898
Epoch: 4749, Batch Gradient Norm: 35.43967592072782
Epoch: 4749, Batch Gradient Norm after: 22.360676644462664
Epoch 4750/10000, Prediction Accuracy = 58.74000000000001%, Loss = 0.8768039584159851
Epoch: 4750, Batch Gradient Norm: 32.573447780857826
Epoch: 4750, Batch Gradient Norm after: 22.360680356330604
Epoch 4751/10000, Prediction Accuracy = 58.71%, Loss = 0.8695735573768616
Epoch: 4751, Batch Gradient Norm: 35.42851006549587
Epoch: 4751, Batch Gradient Norm after: 22.360677309604153
Epoch 4752/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8766575932502747
Epoch: 4752, Batch Gradient Norm: 32.57077022750974
Epoch: 4752, Batch Gradient Norm after: 22.36067913558995
Epoch 4753/10000, Prediction Accuracy = 58.714%, Loss = 0.8694364786148071
Epoch: 4753, Batch Gradient Norm: 35.422832323186675
Epoch: 4753, Batch Gradient Norm after: 22.360679053088322
Epoch 4754/10000, Prediction Accuracy = 58.738%, Loss = 0.8765118479728699
Epoch: 4754, Batch Gradient Norm: 32.56709052796004
Epoch: 4754, Batch Gradient Norm after: 22.360678904555204
Epoch 4755/10000, Prediction Accuracy = 58.70399999999999%, Loss = 0.8692954897880554
Epoch: 4755, Batch Gradient Norm: 35.41909693790845
Epoch: 4755, Batch Gradient Norm after: 22.360677392012573
Epoch 4756/10000, Prediction Accuracy = 58.738000000000014%, Loss = 0.8763707995414733
Epoch: 4756, Batch Gradient Norm: 32.56077183124141
Epoch: 4756, Batch Gradient Norm after: 22.360678783209885
Epoch 4757/10000, Prediction Accuracy = 58.712%, Loss = 0.8691537261009217
Epoch: 4757, Batch Gradient Norm: 35.41228295289804
Epoch: 4757, Batch Gradient Norm after: 22.360678205814764
Epoch 4758/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8762252449989318
Epoch: 4758, Batch Gradient Norm: 32.558706766448694
Epoch: 4758, Batch Gradient Norm after: 22.360680122579012
Epoch 4759/10000, Prediction Accuracy = 58.705999999999996%, Loss = 0.8690158605575562
Epoch: 4759, Batch Gradient Norm: 35.40879521154811
Epoch: 4759, Batch Gradient Norm after: 22.360676388211708
Epoch 4760/10000, Prediction Accuracy = 58.74400000000001%, Loss = 0.8760874390602111
Epoch: 4760, Batch Gradient Norm: 32.551188452409036
Epoch: 4760, Batch Gradient Norm after: 22.36067933235632
Epoch 4761/10000, Prediction Accuracy = 58.702%, Loss = 0.8688759684562684
Epoch: 4761, Batch Gradient Norm: 35.406130579003026
Epoch: 4761, Batch Gradient Norm after: 22.360676412297487
Epoch 4762/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.875948166847229
Epoch: 4762, Batch Gradient Norm: 32.54780886211251
Epoch: 4762, Batch Gradient Norm after: 22.360677683258878
Epoch 4763/10000, Prediction Accuracy = 58.706%, Loss = 0.8687319040298462
Epoch: 4763, Batch Gradient Norm: 35.403712865601165
Epoch: 4763, Batch Gradient Norm after: 22.36067867151671
Epoch 4764/10000, Prediction Accuracy = 58.746%, Loss = 0.8758182525634766
Epoch: 4764, Batch Gradient Norm: 32.54145140208285
Epoch: 4764, Batch Gradient Norm after: 22.360677491820535
Epoch 4765/10000, Prediction Accuracy = 58.705999999999996%, Loss = 0.8685936570167542
Epoch: 4765, Batch Gradient Norm: 35.39573140056924
Epoch: 4765, Batch Gradient Norm after: 22.360678485261456
Epoch 4766/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8756702780723572
Epoch: 4766, Batch Gradient Norm: 32.53980123930114
Epoch: 4766, Batch Gradient Norm after: 22.36067800881905
Epoch 4767/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8684575319290161
Epoch: 4767, Batch Gradient Norm: 35.388439040424
Epoch: 4767, Batch Gradient Norm after: 22.36067779285205
Epoch 4768/10000, Prediction Accuracy = 58.75%, Loss = 0.8755232095718384
Epoch: 4768, Batch Gradient Norm: 32.53550768107338
Epoch: 4768, Batch Gradient Norm after: 22.36068145928555
Epoch 4769/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8683221101760864
Epoch: 4769, Batch Gradient Norm: 35.38270542120828
Epoch: 4769, Batch Gradient Norm after: 22.360678949306415
Epoch 4770/10000, Prediction Accuracy = 58.760000000000005%, Loss = 0.8753790020942688
Epoch: 4770, Batch Gradient Norm: 32.53368854739619
Epoch: 4770, Batch Gradient Norm after: 22.36067909191645
Epoch 4771/10000, Prediction Accuracy = 58.734%, Loss = 0.8681849002838135
Epoch: 4771, Batch Gradient Norm: 35.37705738102897
Epoch: 4771, Batch Gradient Norm after: 22.360679263286034
Epoch 4772/10000, Prediction Accuracy = 58.762%, Loss = 0.8752414584159851
Epoch: 4772, Batch Gradient Norm: 32.52950411619202
Epoch: 4772, Batch Gradient Norm after: 22.36067823772723
Epoch 4773/10000, Prediction Accuracy = 58.726%, Loss = 0.8680544495582581
Epoch: 4773, Batch Gradient Norm: 35.36797345513618
Epoch: 4773, Batch Gradient Norm after: 22.36067653481669
Epoch 4774/10000, Prediction Accuracy = 58.767999999999994%, Loss = 0.8750868201255798
Epoch: 4774, Batch Gradient Norm: 32.52713876496762
Epoch: 4774, Batch Gradient Norm after: 22.360677035729136
Epoch 4775/10000, Prediction Accuracy = 58.734%, Loss = 0.8679200649261475
Epoch: 4775, Batch Gradient Norm: 35.35948663887128
Epoch: 4775, Batch Gradient Norm after: 22.360677241008894
Epoch 4776/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8749430656433106
Epoch: 4776, Batch Gradient Norm: 32.523374177870046
Epoch: 4776, Batch Gradient Norm after: 22.360679272510097
Epoch 4777/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8677895903587342
Epoch: 4777, Batch Gradient Norm: 35.353874805305864
Epoch: 4777, Batch Gradient Norm after: 22.360679615438166
Epoch 4778/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8747921943664551
Epoch: 4778, Batch Gradient Norm: 32.519075757657355
Epoch: 4778, Batch Gradient Norm after: 22.36067860680953
Epoch 4779/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8676545023918152
Epoch: 4779, Batch Gradient Norm: 35.34735273611482
Epoch: 4779, Batch Gradient Norm after: 22.36067684378882
Epoch 4780/10000, Prediction Accuracy = 58.77%, Loss = 0.8746474504470825
Epoch: 4780, Batch Gradient Norm: 32.51479752323587
Epoch: 4780, Batch Gradient Norm after: 22.360676737096156
Epoch 4781/10000, Prediction Accuracy = 58.73599999999999%, Loss = 0.8675209283828735
Epoch: 4781, Batch Gradient Norm: 35.339164083155566
Epoch: 4781, Batch Gradient Norm after: 22.360680243653334
Epoch 4782/10000, Prediction Accuracy = 58.76800000000001%, Loss = 0.8745000481605529
Epoch: 4782, Batch Gradient Norm: 32.50998274406639
Epoch: 4782, Batch Gradient Norm after: 22.360680110605045
Epoch 4783/10000, Prediction Accuracy = 58.73199999999999%, Loss = 0.8673893332481384
Epoch: 4783, Batch Gradient Norm: 35.333383522700146
Epoch: 4783, Batch Gradient Norm after: 22.36067780143869
Epoch 4784/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8743591427803039
Epoch: 4784, Batch Gradient Norm: 32.50688416387917
Epoch: 4784, Batch Gradient Norm after: 22.360679713428176
Epoch 4785/10000, Prediction Accuracy = 58.726%, Loss = 0.86725252866745
Epoch: 4785, Batch Gradient Norm: 35.32678986913224
Epoch: 4785, Batch Gradient Norm after: 22.360676408240934
Epoch 4786/10000, Prediction Accuracy = 58.782%, Loss = 0.8742171287536621
Epoch: 4786, Batch Gradient Norm: 32.501772391861266
Epoch: 4786, Batch Gradient Norm after: 22.360679552756846
Epoch 4787/10000, Prediction Accuracy = 58.733999999999995%, Loss = 0.8671180725097656
Epoch: 4787, Batch Gradient Norm: 35.32013926337384
Epoch: 4787, Batch Gradient Norm after: 22.36067631139183
Epoch 4788/10000, Prediction Accuracy = 58.78599999999999%, Loss = 0.8740760326385498
Epoch: 4788, Batch Gradient Norm: 32.49828988896933
Epoch: 4788, Batch Gradient Norm after: 22.360679669238408
Epoch 4789/10000, Prediction Accuracy = 58.742%, Loss = 0.866985285282135
Epoch: 4789, Batch Gradient Norm: 35.312119938639164
Epoch: 4789, Batch Gradient Norm after: 22.360678343378193
Epoch 4790/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8739291191101074
Epoch: 4790, Batch Gradient Norm: 32.49722010198993
Epoch: 4790, Batch Gradient Norm after: 22.360679526104565
Epoch 4791/10000, Prediction Accuracy = 58.746%, Loss = 0.8668489217758178
Epoch: 4791, Batch Gradient Norm: 35.30630722094697
Epoch: 4791, Batch Gradient Norm after: 22.36067946109303
Epoch 4792/10000, Prediction Accuracy = 58.786%, Loss = 0.8737886905670166
Epoch: 4792, Batch Gradient Norm: 32.49324340097425
Epoch: 4792, Batch Gradient Norm after: 22.36067905923101
Epoch 4793/10000, Prediction Accuracy = 58.754%, Loss = 0.8667218804359436
Epoch: 4793, Batch Gradient Norm: 35.29351883656335
Epoch: 4793, Batch Gradient Norm after: 22.36067826319407
Epoch 4794/10000, Prediction Accuracy = 58.778%, Loss = 0.8736274361610412
Epoch: 4794, Batch Gradient Norm: 32.493511204362235
Epoch: 4794, Batch Gradient Norm after: 22.360681972454447
Epoch 4795/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8665955662727356
Epoch: 4795, Batch Gradient Norm: 35.28362180734584
Epoch: 4795, Batch Gradient Norm after: 22.36067770734299
Epoch 4796/10000, Prediction Accuracy = 58.786%, Loss = 0.873474383354187
Epoch: 4796, Batch Gradient Norm: 32.49170024499057
Epoch: 4796, Batch Gradient Norm after: 22.360679221244254
Epoch 4797/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.8664657473564148
Epoch: 4797, Batch Gradient Norm: 35.27487807641482
Epoch: 4797, Batch Gradient Norm after: 22.360678877172703
Epoch 4798/10000, Prediction Accuracy = 58.782000000000004%, Loss = 0.8733267426490784
Epoch: 4798, Batch Gradient Norm: 32.49192470831138
Epoch: 4798, Batch Gradient Norm after: 22.360679473925913
Epoch 4799/10000, Prediction Accuracy = 58.75%, Loss = 0.866335391998291
Epoch: 4799, Batch Gradient Norm: 35.267583548877504
Epoch: 4799, Batch Gradient Norm after: 22.360679151989363
Epoch 4800/10000, Prediction Accuracy = 58.791999999999994%, Loss = 0.873179042339325
Epoch: 4800, Batch Gradient Norm: 32.48868438616584
Epoch: 4800, Batch Gradient Norm after: 22.360678886018306
Epoch 4801/10000, Prediction Accuracy = 58.74400000000001%, Loss = 0.8662060618400573
Epoch: 4801, Batch Gradient Norm: 35.25790973070666
Epoch: 4801, Batch Gradient Norm after: 22.360678228287963
Epoch 4802/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.873023509979248
Epoch: 4802, Batch Gradient Norm: 32.48878451578449
Epoch: 4802, Batch Gradient Norm after: 22.360679069909892
Epoch 4803/10000, Prediction Accuracy = 58.74400000000001%, Loss = 0.8660766363143921
Epoch: 4803, Batch Gradient Norm: 35.24966688319039
Epoch: 4803, Batch Gradient Norm after: 22.360678512716873
Epoch 4804/10000, Prediction Accuracy = 58.79200000000001%, Loss = 0.8728779792785645
Epoch: 4804, Batch Gradient Norm: 32.48606543532907
Epoch: 4804, Batch Gradient Norm after: 22.360677241116917
Epoch 4805/10000, Prediction Accuracy = 58.746%, Loss = 0.865947687625885
Epoch: 4805, Batch Gradient Norm: 35.24304506315011
Epoch: 4805, Batch Gradient Norm after: 22.36067851576614
Epoch 4806/10000, Prediction Accuracy = 58.798%, Loss = 0.8727372765541077
Epoch: 4806, Batch Gradient Norm: 32.48283934559053
Epoch: 4806, Batch Gradient Norm after: 22.360681161233604
Epoch 4807/10000, Prediction Accuracy = 58.754000000000005%, Loss = 0.865814208984375
Epoch: 4807, Batch Gradient Norm: 35.2382319726108
Epoch: 4807, Batch Gradient Norm after: 22.360678393508827
Epoch 4808/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.8725966691970826
Epoch: 4808, Batch Gradient Norm: 32.479589970043044
Epoch: 4808, Batch Gradient Norm after: 22.36068037584302
Epoch 4809/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8656799077987671
Epoch: 4809, Batch Gradient Norm: 35.231159282959766
Epoch: 4809, Batch Gradient Norm after: 22.360678601293344
Epoch 4810/10000, Prediction Accuracy = 58.802%, Loss = 0.8724541306495667
Epoch: 4810, Batch Gradient Norm: 32.474243595126325
Epoch: 4810, Batch Gradient Norm after: 22.360679918514833
Epoch 4811/10000, Prediction Accuracy = 58.760000000000005%, Loss = 0.8655472993850708
Epoch: 4811, Batch Gradient Norm: 35.224847372955175
Epoch: 4811, Batch Gradient Norm after: 22.36067811695301
Epoch 4812/10000, Prediction Accuracy = 58.802%, Loss = 0.8723137140274048
Epoch: 4812, Batch Gradient Norm: 32.47165251137147
Epoch: 4812, Batch Gradient Norm after: 22.360681422851776
Epoch 4813/10000, Prediction Accuracy = 58.766%, Loss = 0.8654106378555297
Epoch: 4813, Batch Gradient Norm: 35.21991963936335
Epoch: 4813, Batch Gradient Norm after: 22.36067857584853
Epoch 4814/10000, Prediction Accuracy = 58.80799999999999%, Loss = 0.8721750497817993
Epoch: 4814, Batch Gradient Norm: 32.46847745400676
Epoch: 4814, Batch Gradient Norm after: 22.360681596072816
Epoch 4815/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.8652721881866455
Epoch: 4815, Batch Gradient Norm: 35.215681667676215
Epoch: 4815, Batch Gradient Norm after: 22.360677417684432
Epoch 4816/10000, Prediction Accuracy = 58.8%, Loss = 0.8720407843589782
Epoch: 4816, Batch Gradient Norm: 32.46031309055152
Epoch: 4816, Batch Gradient Norm after: 22.360681992468777
Epoch 4817/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8651344537734985
Epoch: 4817, Batch Gradient Norm: 35.21441092766454
Epoch: 4817, Batch Gradient Norm after: 22.36067866668007
Epoch 4818/10000, Prediction Accuracy = 58.79%, Loss = 0.8719153761863708
Epoch: 4818, Batch Gradient Norm: 32.45458946482638
Epoch: 4818, Batch Gradient Norm after: 22.360680116377633
Epoch 4819/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8649947166442871
Epoch: 4819, Batch Gradient Norm: 35.21104221473574
Epoch: 4819, Batch Gradient Norm after: 22.360678965875277
Epoch 4820/10000, Prediction Accuracy = 58.788%, Loss = 0.8717805981636048
Epoch: 4820, Batch Gradient Norm: 32.44936688351587
Epoch: 4820, Batch Gradient Norm after: 22.36068115496679
Epoch 4821/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8648516058921814
Epoch: 4821, Batch Gradient Norm: 35.20823138970841
Epoch: 4821, Batch Gradient Norm after: 22.360679648285227
Epoch 4822/10000, Prediction Accuracy = 58.798%, Loss = 0.8716547131538391
Epoch: 4822, Batch Gradient Norm: 32.4432023032296
Epoch: 4822, Batch Gradient Norm after: 22.360679671974296
Epoch 4823/10000, Prediction Accuracy = 58.782%, Loss = 0.8647123813629151
Epoch: 4823, Batch Gradient Norm: 35.2059662722164
Epoch: 4823, Batch Gradient Norm after: 22.360678886181347
Epoch 4824/10000, Prediction Accuracy = 58.791999999999994%, Loss = 0.8715228080749512
Epoch: 4824, Batch Gradient Norm: 32.43799496997672
Epoch: 4824, Batch Gradient Norm after: 22.360678170010036
Epoch 4825/10000, Prediction Accuracy = 58.79200000000001%, Loss = 0.864573085308075
Epoch: 4825, Batch Gradient Norm: 35.20331764356056
Epoch: 4825, Batch Gradient Norm after: 22.360680081604595
Epoch 4826/10000, Prediction Accuracy = 58.802%, Loss = 0.871383786201477
Epoch: 4826, Batch Gradient Norm: 32.431811849264335
Epoch: 4826, Batch Gradient Norm after: 22.360680111036636
Epoch 4827/10000, Prediction Accuracy = 58.786%, Loss = 0.8644325375556946
Epoch: 4827, Batch Gradient Norm: 35.203542575825715
Epoch: 4827, Batch Gradient Norm after: 22.360676738670065
Epoch 4828/10000, Prediction Accuracy = 58.79599999999999%, Loss = 0.8712589859962463
Epoch: 4828, Batch Gradient Norm: 32.42362287756807
Epoch: 4828, Batch Gradient Norm after: 22.360679787356343
Epoch 4829/10000, Prediction Accuracy = 58.788%, Loss = 0.8642946124076843
Epoch: 4829, Batch Gradient Norm: 35.200012104676226
Epoch: 4829, Batch Gradient Norm after: 22.3606784485658
Epoch 4830/10000, Prediction Accuracy = 58.798%, Loss = 0.8711266994476319
Epoch: 4830, Batch Gradient Norm: 32.418640414909554
Epoch: 4830, Batch Gradient Norm after: 22.36067855695919
Epoch 4831/10000, Prediction Accuracy = 58.79%, Loss = 0.8641584873199463
Epoch: 4831, Batch Gradient Norm: 35.19583079265102
Epoch: 4831, Batch Gradient Norm after: 22.36067787117126
Epoch 4832/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.8709920287132263
Epoch: 4832, Batch Gradient Norm: 32.41280437851539
Epoch: 4832, Batch Gradient Norm after: 22.36068047935131
Epoch 4833/10000, Prediction Accuracy = 58.78000000000001%, Loss = 0.8640191078186035
Epoch: 4833, Batch Gradient Norm: 35.19360339301423
Epoch: 4833, Batch Gradient Norm after: 22.360677067317965
Epoch 4834/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.8708608150482178
Epoch: 4834, Batch Gradient Norm: 32.40724568739574
Epoch: 4834, Batch Gradient Norm after: 22.360678648181896
Epoch 4835/10000, Prediction Accuracy = 58.788%, Loss = 0.8638797998428345
Epoch: 4835, Batch Gradient Norm: 35.189020220248594
Epoch: 4835, Batch Gradient Norm after: 22.36067690678692
Epoch 4836/10000, Prediction Accuracy = 58.798%, Loss = 0.8707271695137024
Epoch: 4836, Batch Gradient Norm: 32.40283085241213
Epoch: 4836, Batch Gradient Norm after: 22.36067712836414
Epoch 4837/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.8637508153915405
Epoch: 4837, Batch Gradient Norm: 35.18263747286948
Epoch: 4837, Batch Gradient Norm after: 22.36067765800728
Epoch 4838/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8705867528915405
Epoch: 4838, Batch Gradient Norm: 32.39930439186217
Epoch: 4838, Batch Gradient Norm after: 22.360679243663785
Epoch 4839/10000, Prediction Accuracy = 58.79200000000001%, Loss = 0.8636141657829285
Epoch: 4839, Batch Gradient Norm: 35.17781658312322
Epoch: 4839, Batch Gradient Norm after: 22.360678084298506
Epoch 4840/10000, Prediction Accuracy = 58.798%, Loss = 0.870454978942871
Epoch: 4840, Batch Gradient Norm: 32.39304564893729
Epoch: 4840, Batch Gradient Norm after: 22.360679224826473
Epoch 4841/10000, Prediction Accuracy = 58.79200000000001%, Loss = 0.863481855392456
Epoch: 4841, Batch Gradient Norm: 35.176516727864566
Epoch: 4841, Batch Gradient Norm after: 22.360676544543086
Epoch 4842/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8703241229057312
Epoch: 4842, Batch Gradient Norm: 32.38522690026128
Epoch: 4842, Batch Gradient Norm after: 22.360679167825708
Epoch 4843/10000, Prediction Accuracy = 58.8%, Loss = 0.8633463025093079
Epoch: 4843, Batch Gradient Norm: 35.17102226114018
Epoch: 4843, Batch Gradient Norm after: 22.360677518298367
Epoch 4844/10000, Prediction Accuracy = 58.8%, Loss = 0.8701893806457519
Epoch: 4844, Batch Gradient Norm: 32.381360488749515
Epoch: 4844, Batch Gradient Norm after: 22.360679056259038
Epoch 4845/10000, Prediction Accuracy = 58.8%, Loss = 0.8632114648818969
Epoch: 4845, Batch Gradient Norm: 35.16871456482985
Epoch: 4845, Batch Gradient Norm after: 22.360678938589135
Epoch 4846/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.8700531363487244
Epoch: 4846, Batch Gradient Norm: 32.37580206253294
Epoch: 4846, Batch Gradient Norm after: 22.360679438417126
Epoch 4847/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.863076138496399
Epoch: 4847, Batch Gradient Norm: 35.16219339888975
Epoch: 4847, Batch Gradient Norm after: 22.360676947957778
Epoch 4848/10000, Prediction Accuracy = 58.8%, Loss = 0.8699214935302735
Epoch: 4848, Batch Gradient Norm: 32.3680710087042
Epoch: 4848, Batch Gradient Norm after: 22.360678558232394
Epoch 4849/10000, Prediction Accuracy = 58.806000000000004%, Loss = 0.862943172454834
Epoch: 4849, Batch Gradient Norm: 35.15640440984918
Epoch: 4849, Batch Gradient Norm after: 22.360677437120437
Epoch 4850/10000, Prediction Accuracy = 58.798%, Loss = 0.8697810292243957
Epoch: 4850, Batch Gradient Norm: 32.36641033831335
Epoch: 4850, Batch Gradient Norm after: 22.3606792316517
Epoch 4851/10000, Prediction Accuracy = 58.81%, Loss = 0.8628106117248535
Epoch: 4851, Batch Gradient Norm: 35.149039075791265
Epoch: 4851, Batch Gradient Norm after: 22.360676339851217
Epoch 4852/10000, Prediction Accuracy = 58.806000000000004%, Loss = 0.8696324110031128
Epoch: 4852, Batch Gradient Norm: 32.36200945258261
Epoch: 4852, Batch Gradient Norm after: 22.360678144184394
Epoch 4853/10000, Prediction Accuracy = 58.812%, Loss = 0.8626760363578796
Epoch: 4853, Batch Gradient Norm: 35.14663917031539
Epoch: 4853, Batch Gradient Norm after: 22.360677673634775
Epoch 4854/10000, Prediction Accuracy = 58.79999999999999%, Loss = 0.8694997549057006
Epoch: 4854, Batch Gradient Norm: 32.35658916846292
Epoch: 4854, Batch Gradient Norm after: 22.36067843947649
Epoch 4855/10000, Prediction Accuracy = 58.812%, Loss = 0.8625473380088806
Epoch: 4855, Batch Gradient Norm: 35.13789724618553
Epoch: 4855, Batch Gradient Norm after: 22.360678755926788
Epoch 4856/10000, Prediction Accuracy = 58.806%, Loss = 0.8693589448928833
Epoch: 4856, Batch Gradient Norm: 32.35235042504939
Epoch: 4856, Batch Gradient Norm after: 22.360680892826657
Epoch 4857/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8624146103858947
Epoch: 4857, Batch Gradient Norm: 35.13382963286211
Epoch: 4857, Batch Gradient Norm after: 22.360675320889257
Epoch 4858/10000, Prediction Accuracy = 58.8%, Loss = 0.8692243337631226
Epoch: 4858, Batch Gradient Norm: 32.34928219928586
Epoch: 4858, Batch Gradient Norm after: 22.360679822249008
Epoch 4859/10000, Prediction Accuracy = 58.806%, Loss = 0.8622799754142761
Epoch: 4859, Batch Gradient Norm: 35.126852250712226
Epoch: 4859, Batch Gradient Norm after: 22.360679078724377
Epoch 4860/10000, Prediction Accuracy = 58.80800000000001%, Loss = 0.8690867185592651
Epoch: 4860, Batch Gradient Norm: 32.344035914961246
Epoch: 4860, Batch Gradient Norm after: 22.36067747155542
Epoch 4861/10000, Prediction Accuracy = 58.802%, Loss = 0.8621473550796509
Epoch: 4861, Batch Gradient Norm: 35.121854134065494
Epoch: 4861, Batch Gradient Norm after: 22.3606782890553
Epoch 4862/10000, Prediction Accuracy = 58.806%, Loss = 0.8689464926719666
Epoch: 4862, Batch Gradient Norm: 32.34037917597836
Epoch: 4862, Batch Gradient Norm after: 22.360678397491252
Epoch 4863/10000, Prediction Accuracy = 58.802%, Loss = 0.8620106935501098
Epoch: 4863, Batch Gradient Norm: 35.116173035683
Epoch: 4863, Batch Gradient Norm after: 22.360678131159357
Epoch 4864/10000, Prediction Accuracy = 58.80999999999999%, Loss = 0.8688096046447754
Epoch: 4864, Batch Gradient Norm: 32.33671303250921
Epoch: 4864, Batch Gradient Norm after: 22.360677391867256
Epoch 4865/10000, Prediction Accuracy = 58.812%, Loss = 0.8618823766708374
Epoch: 4865, Batch Gradient Norm: 35.11016824518945
Epoch: 4865, Batch Gradient Norm after: 22.36067752211437
Epoch 4866/10000, Prediction Accuracy = 58.80799999999999%, Loss = 0.8686707019805908
Epoch: 4866, Batch Gradient Norm: 32.33305704990862
Epoch: 4866, Batch Gradient Norm after: 22.360677653286945
Epoch 4867/10000, Prediction Accuracy = 58.814%, Loss = 0.8617425441741944
Epoch: 4867, Batch Gradient Norm: 35.10581960603502
Epoch: 4867, Batch Gradient Norm after: 22.3606783795443
Epoch 4868/10000, Prediction Accuracy = 58.814%, Loss = 0.8685396194458008
Epoch: 4868, Batch Gradient Norm: 32.32706527657123
Epoch: 4868, Batch Gradient Norm after: 22.360677091333823
Epoch 4869/10000, Prediction Accuracy = 58.812%, Loss = 0.8616038680076599
Epoch: 4869, Batch Gradient Norm: 35.10218862950305
Epoch: 4869, Batch Gradient Norm after: 22.36067830099094
Epoch 4870/10000, Prediction Accuracy = 58.830000000000005%, Loss = 0.8684067606925965
Epoch: 4870, Batch Gradient Norm: 32.322929403354784
Epoch: 4870, Batch Gradient Norm after: 22.36067790326802
Epoch 4871/10000, Prediction Accuracy = 58.82000000000001%, Loss = 0.8614695906639099
Epoch: 4871, Batch Gradient Norm: 35.1007904087032
Epoch: 4871, Batch Gradient Norm after: 22.36067772343054
Epoch 4872/10000, Prediction Accuracy = 58.826%, Loss = 0.868276059627533
Epoch: 4872, Batch Gradient Norm: 32.31456924809037
Epoch: 4872, Batch Gradient Norm after: 22.360677140237026
Epoch 4873/10000, Prediction Accuracy = 58.815999999999995%, Loss = 0.8613328695297241
Epoch: 4873, Batch Gradient Norm: 35.09780393507914
Epoch: 4873, Batch Gradient Norm after: 22.360677708715258
Epoch 4874/10000, Prediction Accuracy = 58.83%, Loss = 0.8681488394737243
Epoch: 4874, Batch Gradient Norm: 32.30951469022893
Epoch: 4874, Batch Gradient Norm after: 22.36067825869355
Epoch 4875/10000, Prediction Accuracy = 58.80999999999999%, Loss = 0.8611961245536804
Epoch: 4875, Batch Gradient Norm: 35.09633788727304
Epoch: 4875, Batch Gradient Norm after: 22.360677606620676
Epoch 4876/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8680220484733582
Epoch: 4876, Batch Gradient Norm: 32.30003476404399
Epoch: 4876, Batch Gradient Norm after: 22.360677740308756
Epoch 4877/10000, Prediction Accuracy = 58.814%, Loss = 0.861055588722229
Epoch: 4877, Batch Gradient Norm: 35.09734396038018
Epoch: 4877, Batch Gradient Norm after: 22.36067947481247
Epoch 4878/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8678974747657776
Epoch: 4878, Batch Gradient Norm: 32.29560331782089
Epoch: 4878, Batch Gradient Norm after: 22.36067844349791
Epoch 4879/10000, Prediction Accuracy = 58.826%, Loss = 0.8609131336212158
Epoch: 4879, Batch Gradient Norm: 35.094671387864764
Epoch: 4879, Batch Gradient Norm after: 22.360678591299173
Epoch 4880/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.867772126197815
Epoch: 4880, Batch Gradient Norm: 32.28632973270176
Epoch: 4880, Batch Gradient Norm after: 22.36067872867251
Epoch 4881/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.8607691884040832
Epoch: 4881, Batch Gradient Norm: 35.095126425083635
Epoch: 4881, Batch Gradient Norm after: 22.360676694722375
Epoch 4882/10000, Prediction Accuracy = 58.836%, Loss = 0.867653489112854
Epoch: 4882, Batch Gradient Norm: 32.27930036870038
Epoch: 4882, Batch Gradient Norm after: 22.360678123494434
Epoch 4883/10000, Prediction Accuracy = 58.812%, Loss = 0.8606309294700623
Epoch: 4883, Batch Gradient Norm: 35.09423454871714
Epoch: 4883, Batch Gradient Norm after: 22.360677460358882
Epoch 4884/10000, Prediction Accuracy = 58.85%, Loss = 0.8675281763076782
Epoch: 4884, Batch Gradient Norm: 32.27398926451729
Epoch: 4884, Batch Gradient Norm after: 22.36067961906933
Epoch 4885/10000, Prediction Accuracy = 58.812%, Loss = 0.8604922771453858
Epoch: 4885, Batch Gradient Norm: 35.092960296780305
Epoch: 4885, Batch Gradient Norm after: 22.360679042439283
Epoch 4886/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.86739661693573
Epoch: 4886, Batch Gradient Norm: 32.26627718415322
Epoch: 4886, Batch Gradient Norm after: 22.36067661458483
Epoch 4887/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.8603548526763916
Epoch: 4887, Batch Gradient Norm: 35.09138525647208
Epoch: 4887, Batch Gradient Norm after: 22.360678697449224
Epoch 4888/10000, Prediction Accuracy = 58.84400000000001%, Loss = 0.8672706484794617
Epoch: 4888, Batch Gradient Norm: 32.25833452570421
Epoch: 4888, Batch Gradient Norm after: 22.360679149407307
Epoch 4889/10000, Prediction Accuracy = 58.830000000000005%, Loss = 0.8602168321609497
Epoch: 4889, Batch Gradient Norm: 35.08855493166518
Epoch: 4889, Batch Gradient Norm after: 22.36067723544767
Epoch 4890/10000, Prediction Accuracy = 58.848%, Loss = 0.8671443939208985
Epoch: 4890, Batch Gradient Norm: 32.25423615728848
Epoch: 4890, Batch Gradient Norm after: 22.36067792835504
Epoch 4891/10000, Prediction Accuracy = 58.838%, Loss = 0.8600795030593872
Epoch: 4891, Batch Gradient Norm: 35.086362759787036
Epoch: 4891, Batch Gradient Norm after: 22.360678525375693
Epoch 4892/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.8670182943344116
Epoch: 4892, Batch Gradient Norm: 32.24845522612007
Epoch: 4892, Batch Gradient Norm after: 22.360678219623207
Epoch 4893/10000, Prediction Accuracy = 58.834%, Loss = 0.8599456906318664
Epoch: 4893, Batch Gradient Norm: 35.08231628139951
Epoch: 4893, Batch Gradient Norm after: 22.36067681012643
Epoch 4894/10000, Prediction Accuracy = 58.85%, Loss = 0.8668903946876526
Epoch: 4894, Batch Gradient Norm: 32.242443285663335
Epoch: 4894, Batch Gradient Norm after: 22.360677578854233
Epoch 4895/10000, Prediction Accuracy = 58.843999999999994%, Loss = 0.859807550907135
Epoch: 4895, Batch Gradient Norm: 35.08219025953168
Epoch: 4895, Batch Gradient Norm after: 22.360679123021544
Epoch 4896/10000, Prediction Accuracy = 58.848%, Loss = 0.8667679309844971
Epoch: 4896, Batch Gradient Norm: 32.23276703317255
Epoch: 4896, Batch Gradient Norm after: 22.360678232251633
Epoch 4897/10000, Prediction Accuracy = 58.843999999999994%, Loss = 0.8596680283546447
Epoch: 4897, Batch Gradient Norm: 35.08229260032317
Epoch: 4897, Batch Gradient Norm after: 22.360678367021627
Epoch 4898/10000, Prediction Accuracy = 58.854000000000006%, Loss = 0.8666488409042359
Epoch: 4898, Batch Gradient Norm: 32.22325500195657
Epoch: 4898, Batch Gradient Norm after: 22.36067989856629
Epoch 4899/10000, Prediction Accuracy = 58.843999999999994%, Loss = 0.8595245361328125
Epoch: 4899, Batch Gradient Norm: 35.08081436393174
Epoch: 4899, Batch Gradient Norm after: 22.360680291484446
Epoch 4900/10000, Prediction Accuracy = 58.852%, Loss = 0.8665288329124451
Epoch: 4900, Batch Gradient Norm: 32.21752317141589
Epoch: 4900, Batch Gradient Norm after: 22.36067910452627
Epoch 4901/10000, Prediction Accuracy = 58.852%, Loss = 0.8593901038169861
Epoch: 4901, Batch Gradient Norm: 35.0753175179153
Epoch: 4901, Batch Gradient Norm after: 22.360678500449797
Epoch 4902/10000, Prediction Accuracy = 58.85200000000001%, Loss = 0.8663919329643249
Epoch: 4902, Batch Gradient Norm: 32.21219640112517
Epoch: 4902, Batch Gradient Norm after: 22.36067718494544
Epoch 4903/10000, Prediction Accuracy = 58.85799999999999%, Loss = 0.8592557668685913
Epoch: 4903, Batch Gradient Norm: 35.073985275559174
Epoch: 4903, Batch Gradient Norm after: 22.360681038338186
Epoch 4904/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8662715911865234
Epoch: 4904, Batch Gradient Norm: 32.20303516015964
Epoch: 4904, Batch Gradient Norm after: 22.360678033113487
Epoch 4905/10000, Prediction Accuracy = 58.864%, Loss = 0.8591162204742432
Epoch: 4905, Batch Gradient Norm: 35.073946821896726
Epoch: 4905, Batch Gradient Norm after: 22.360678497041214
Epoch 4906/10000, Prediction Accuracy = 58.843999999999994%, Loss = 0.8661564469337464
Epoch: 4906, Batch Gradient Norm: 32.195724958192905
Epoch: 4906, Batch Gradient Norm after: 22.360676174693666
Epoch 4907/10000, Prediction Accuracy = 58.864%, Loss = 0.8589709281921387
Epoch: 4907, Batch Gradient Norm: 35.075387492470966
Epoch: 4907, Batch Gradient Norm after: 22.360678040737124
Epoch 4908/10000, Prediction Accuracy = 58.836%, Loss = 0.8660411477088928
Epoch: 4908, Batch Gradient Norm: 32.18557460421569
Epoch: 4908, Batch Gradient Norm after: 22.36067778970536
Epoch 4909/10000, Prediction Accuracy = 58.874%, Loss = 0.8588214874267578
Epoch: 4909, Batch Gradient Norm: 35.079727993833075
Epoch: 4909, Batch Gradient Norm after: 22.360679303959415
Epoch 4910/10000, Prediction Accuracy = 58.846000000000004%, Loss = 0.8659258008003234
Epoch: 4910, Batch Gradient Norm: 32.17635020320537
Epoch: 4910, Batch Gradient Norm after: 22.360678830159006
Epoch 4911/10000, Prediction Accuracy = 58.886%, Loss = 0.8586748480796814
Epoch: 4911, Batch Gradient Norm: 35.08204570039468
Epoch: 4911, Batch Gradient Norm after: 22.360680571754482
Epoch 4912/10000, Prediction Accuracy = 58.854000000000006%, Loss = 0.8658200144767761
Epoch: 4912, Batch Gradient Norm: 32.1664541792317
Epoch: 4912, Batch Gradient Norm after: 22.360678989151104
Epoch 4913/10000, Prediction Accuracy = 58.88399999999999%, Loss = 0.8585359215736389
Epoch: 4913, Batch Gradient Norm: 35.08404469177235
Epoch: 4913, Batch Gradient Norm after: 22.360679052292568
Epoch 4914/10000, Prediction Accuracy = 58.862%, Loss = 0.8657008647918701
Epoch: 4914, Batch Gradient Norm: 32.16075828258434
Epoch: 4914, Batch Gradient Norm after: 22.360679503113357
Epoch 4915/10000, Prediction Accuracy = 58.888%, Loss = 0.8584004640579224
Epoch: 4915, Batch Gradient Norm: 35.081324006133464
Epoch: 4915, Batch Gradient Norm after: 22.36068041414231
Epoch 4916/10000, Prediction Accuracy = 58.86%, Loss = 0.8655648469924927
Epoch: 4916, Batch Gradient Norm: 32.15777062742471
Epoch: 4916, Batch Gradient Norm after: 22.360677839430686
Epoch 4917/10000, Prediction Accuracy = 58.88399999999999%, Loss = 0.8582615613937378
Epoch: 4917, Batch Gradient Norm: 35.07709540138312
Epoch: 4917, Batch Gradient Norm after: 22.360679549872597
Epoch 4918/10000, Prediction Accuracy = 58.862%, Loss = 0.8654360055923462
Epoch: 4918, Batch Gradient Norm: 32.15230930611182
Epoch: 4918, Batch Gradient Norm after: 22.360678873626906
Epoch 4919/10000, Prediction Accuracy = 58.888%, Loss = 0.8581303715705871
Epoch: 4919, Batch Gradient Norm: 35.07501080186428
Epoch: 4919, Batch Gradient Norm after: 22.360679819234633
Epoch 4920/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.8653071403503418
Epoch: 4920, Batch Gradient Norm: 32.1466185222962
Epoch: 4920, Batch Gradient Norm after: 22.360679462355137
Epoch 4921/10000, Prediction Accuracy = 58.896%, Loss = 0.8580017805099487
Epoch: 4921, Batch Gradient Norm: 35.06847104713471
Epoch: 4921, Batch Gradient Norm after: 22.36067820085063
Epoch 4922/10000, Prediction Accuracy = 58.864%, Loss = 0.8651723384857177
Epoch: 4922, Batch Gradient Norm: 32.14196436724554
Epoch: 4922, Batch Gradient Norm after: 22.36067680384619
Epoch 4923/10000, Prediction Accuracy = 58.90400000000001%, Loss = 0.8578704833984375
Epoch: 4923, Batch Gradient Norm: 35.06910447265544
Epoch: 4923, Batch Gradient Norm after: 22.360678015982256
Epoch 4924/10000, Prediction Accuracy = 58.86%, Loss = 0.8650512576103211
Epoch: 4924, Batch Gradient Norm: 32.13253387816816
Epoch: 4924, Batch Gradient Norm after: 22.360678571718733
Epoch 4925/10000, Prediction Accuracy = 58.903999999999996%, Loss = 0.8577330231666564
Epoch: 4925, Batch Gradient Norm: 35.06707773849625
Epoch: 4925, Batch Gradient Norm after: 22.360679160727628
Epoch 4926/10000, Prediction Accuracy = 58.864%, Loss = 0.864928913116455
Epoch: 4926, Batch Gradient Norm: 32.125873582801624
Epoch: 4926, Batch Gradient Norm after: 22.360679979897817
Epoch 4927/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.857594895362854
Epoch: 4927, Batch Gradient Norm: 35.064668617194975
Epoch: 4927, Batch Gradient Norm after: 22.3606775680756
Epoch 4928/10000, Prediction Accuracy = 58.864%, Loss = 0.864800488948822
Epoch: 4928, Batch Gradient Norm: 32.121360214003005
Epoch: 4928, Batch Gradient Norm after: 22.360678356605415
Epoch 4929/10000, Prediction Accuracy = 58.90999999999999%, Loss = 0.8574623346328736
Epoch: 4929, Batch Gradient Norm: 35.06515301074817
Epoch: 4929, Batch Gradient Norm after: 22.36067690655307
Epoch 4930/10000, Prediction Accuracy = 58.870000000000005%, Loss = 0.8646906971931457
Epoch: 4930, Batch Gradient Norm: 32.112793914934166
Epoch: 4930, Batch Gradient Norm after: 22.360678060098486
Epoch 4931/10000, Prediction Accuracy = 58.924%, Loss = 0.8573236465454102
Epoch: 4931, Batch Gradient Norm: 35.06370840135262
Epoch: 4931, Batch Gradient Norm after: 22.360676787898093
Epoch 4932/10000, Prediction Accuracy = 58.874%, Loss = 0.8645695328712464
Epoch: 4932, Batch Gradient Norm: 32.10828639960146
Epoch: 4932, Batch Gradient Norm after: 22.360677487895323
Epoch 4933/10000, Prediction Accuracy = 58.92%, Loss = 0.8571860074996949
Epoch: 4933, Batch Gradient Norm: 35.06154416801526
Epoch: 4933, Batch Gradient Norm after: 22.360677215178544
Epoch 4934/10000, Prediction Accuracy = 58.867999999999995%, Loss = 0.8644441485404968
Epoch: 4934, Batch Gradient Norm: 32.098997516333526
Epoch: 4934, Batch Gradient Norm after: 22.36067871111533
Epoch 4935/10000, Prediction Accuracy = 58.910000000000004%, Loss = 0.8570508837699891
Epoch: 4935, Batch Gradient Norm: 35.059790646352475
Epoch: 4935, Batch Gradient Norm after: 22.360677730006806
Epoch 4936/10000, Prediction Accuracy = 58.870000000000005%, Loss = 0.8643133282661438
Epoch: 4936, Batch Gradient Norm: 32.09572026972124
Epoch: 4936, Batch Gradient Norm after: 22.36067714967127
Epoch 4937/10000, Prediction Accuracy = 58.910000000000004%, Loss = 0.8569102048873901
Epoch: 4937, Batch Gradient Norm: 35.058810928092626
Epoch: 4937, Batch Gradient Norm after: 22.360678160662072
Epoch 4938/10000, Prediction Accuracy = 58.874%, Loss = 0.8641908049583436
Epoch: 4938, Batch Gradient Norm: 32.08933849551911
Epoch: 4938, Batch Gradient Norm after: 22.36067772167956
Epoch 4939/10000, Prediction Accuracy = 58.926%, Loss = 0.8567808628082275
Epoch: 4939, Batch Gradient Norm: 35.053077970899814
Epoch: 4939, Batch Gradient Norm after: 22.360679208098798
Epoch 4940/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.8640629768371582
Epoch: 4940, Batch Gradient Norm: 32.085627249382526
Epoch: 4940, Batch Gradient Norm after: 22.36067745003427
Epoch 4941/10000, Prediction Accuracy = 58.92%, Loss = 0.8566488981246948
Epoch: 4941, Batch Gradient Norm: 35.04863998600168
Epoch: 4941, Batch Gradient Norm after: 22.360679746809378
Epoch 4942/10000, Prediction Accuracy = 58.878%, Loss = 0.8639310240745545
Epoch: 4942, Batch Gradient Norm: 32.07952371415591
Epoch: 4942, Batch Gradient Norm after: 22.360680024463225
Epoch 4943/10000, Prediction Accuracy = 58.92999999999999%, Loss = 0.8565179467201233
Epoch: 4943, Batch Gradient Norm: 35.042575650974044
Epoch: 4943, Batch Gradient Norm after: 22.36067816086532
Epoch 4944/10000, Prediction Accuracy = 58.876%, Loss = 0.8637918472290039
Epoch: 4944, Batch Gradient Norm: 32.07796071048059
Epoch: 4944, Batch Gradient Norm after: 22.360678169520735
Epoch 4945/10000, Prediction Accuracy = 58.932%, Loss = 0.8563891053199768
Epoch: 4945, Batch Gradient Norm: 35.03945941074465
Epoch: 4945, Batch Gradient Norm after: 22.3606775470863
Epoch 4946/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.8636647701263428
Epoch: 4946, Batch Gradient Norm: 32.07213826725653
Epoch: 4946, Batch Gradient Norm after: 22.36067748303875
Epoch 4947/10000, Prediction Accuracy = 58.92999999999999%, Loss = 0.8562572956085205
Epoch: 4947, Batch Gradient Norm: 35.03674610103672
Epoch: 4947, Batch Gradient Norm after: 22.360678530381065
Epoch 4948/10000, Prediction Accuracy = 58.878%, Loss = 0.863539183139801
Epoch: 4948, Batch Gradient Norm: 32.067150219244795
Epoch: 4948, Batch Gradient Norm after: 22.360679857316523
Epoch 4949/10000, Prediction Accuracy = 58.93399999999999%, Loss = 0.8561218738555908
Epoch: 4949, Batch Gradient Norm: 35.03671977642243
Epoch: 4949, Batch Gradient Norm after: 22.36067926376935
Epoch 4950/10000, Prediction Accuracy = 58.884%, Loss = 0.8634158492088317
Epoch: 4950, Batch Gradient Norm: 32.05869764971445
Epoch: 4950, Batch Gradient Norm after: 22.360678479369394
Epoch 4951/10000, Prediction Accuracy = 58.924%, Loss = 0.855992341041565
Epoch: 4951, Batch Gradient Norm: 35.03024843755763
Epoch: 4951, Batch Gradient Norm after: 22.360678321022732
Epoch 4952/10000, Prediction Accuracy = 58.886%, Loss = 0.8632804036140442
Epoch: 4952, Batch Gradient Norm: 32.05622721809871
Epoch: 4952, Batch Gradient Norm after: 22.360678312072135
Epoch 4953/10000, Prediction Accuracy = 58.918000000000006%, Loss = 0.8558597803115845
Epoch: 4953, Batch Gradient Norm: 35.02961655475509
Epoch: 4953, Batch Gradient Norm after: 22.360679303119564
Epoch 4954/10000, Prediction Accuracy = 58.882000000000005%, Loss = 0.8631556510925293
Epoch: 4954, Batch Gradient Norm: 32.04824325754184
Epoch: 4954, Batch Gradient Norm after: 22.36067791150874
Epoch 4955/10000, Prediction Accuracy = 58.90999999999999%, Loss = 0.855727744102478
Epoch: 4955, Batch Gradient Norm: 35.02582134941224
Epoch: 4955, Batch Gradient Norm after: 22.36067651745952
Epoch 4956/10000, Prediction Accuracy = 58.886%, Loss = 0.8630309700965881
Epoch: 4956, Batch Gradient Norm: 32.043063797218885
Epoch: 4956, Batch Gradient Norm after: 22.360678978384396
Epoch 4957/10000, Prediction Accuracy = 58.91799999999999%, Loss = 0.8555951952934265
Epoch: 4957, Batch Gradient Norm: 35.0191588563072
Epoch: 4957, Batch Gradient Norm after: 22.360678116680127
Epoch 4958/10000, Prediction Accuracy = 58.89%, Loss = 0.8628991484642029
Epoch: 4958, Batch Gradient Norm: 32.04293682814632
Epoch: 4958, Batch Gradient Norm after: 22.36067728911315
Epoch 4959/10000, Prediction Accuracy = 58.92%, Loss = 0.8554654121398926
Epoch: 4959, Batch Gradient Norm: 35.012451250650656
Epoch: 4959, Batch Gradient Norm after: 22.360678981024712
Epoch 4960/10000, Prediction Accuracy = 58.888%, Loss = 0.8627569079399109
Epoch: 4960, Batch Gradient Norm: 32.040430428332606
Epoch: 4960, Batch Gradient Norm after: 22.360679196493948
Epoch 4961/10000, Prediction Accuracy = 58.92%, Loss = 0.8553371429443359
Epoch: 4961, Batch Gradient Norm: 35.00345086773845
Epoch: 4961, Batch Gradient Norm after: 22.360677816571897
Epoch 4962/10000, Prediction Accuracy = 58.89399999999999%, Loss = 0.862612509727478
Epoch: 4962, Batch Gradient Norm: 32.040634255424486
Epoch: 4962, Batch Gradient Norm after: 22.360676373171785
Epoch 4963/10000, Prediction Accuracy = 58.91600000000001%, Loss = 0.8552099466323853
Epoch: 4963, Batch Gradient Norm: 34.99740983648656
Epoch: 4963, Batch Gradient Norm after: 22.360678608626547
Epoch 4964/10000, Prediction Accuracy = 58.896%, Loss = 0.862483561038971
Epoch: 4964, Batch Gradient Norm: 32.03225528895627
Epoch: 4964, Batch Gradient Norm after: 22.36067763694775
Epoch 4965/10000, Prediction Accuracy = 58.91600000000001%, Loss = 0.8550836563110351
Epoch: 4965, Batch Gradient Norm: 34.993321206482534
Epoch: 4965, Batch Gradient Norm after: 22.360680407506667
Epoch 4966/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.8623568773269653
Epoch: 4966, Batch Gradient Norm: 32.02718783064887
Epoch: 4966, Batch Gradient Norm after: 22.360678047695693
Epoch 4967/10000, Prediction Accuracy = 58.924%, Loss = 0.8549479603767395
Epoch: 4967, Batch Gradient Norm: 34.99332503812675
Epoch: 4967, Batch Gradient Norm after: 22.36068123830007
Epoch 4968/10000, Prediction Accuracy = 58.912%, Loss = 0.8622372031211853
Epoch: 4968, Batch Gradient Norm: 32.016387703262076
Epoch: 4968, Batch Gradient Norm after: 22.360676193333212
Epoch 4969/10000, Prediction Accuracy = 58.936%, Loss = 0.8548127174377441
Epoch: 4969, Batch Gradient Norm: 34.99429204777212
Epoch: 4969, Batch Gradient Norm after: 22.36068116227157
Epoch 4970/10000, Prediction Accuracy = 58.91799999999999%, Loss = 0.8621182799339294
Epoch: 4970, Batch Gradient Norm: 32.00926676557002
Epoch: 4970, Batch Gradient Norm after: 22.360678525363713
Epoch 4971/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8546733736991883
Epoch: 4971, Batch Gradient Norm: 34.995552828510235
Epoch: 4971, Batch Gradient Norm after: 22.360678444549777
Epoch 4972/10000, Prediction Accuracy = 58.926%, Loss = 0.8620033144950867
Epoch: 4972, Batch Gradient Norm: 32.001251031852775
Epoch: 4972, Batch Gradient Norm after: 22.360676367354962
Epoch 4973/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8545352339744567
Epoch: 4973, Batch Gradient Norm: 34.99830736142721
Epoch: 4973, Batch Gradient Norm after: 22.360679665228616
Epoch 4974/10000, Prediction Accuracy = 58.91799999999999%, Loss = 0.861894178390503
Epoch: 4974, Batch Gradient Norm: 31.9923872526642
Epoch: 4974, Batch Gradient Norm after: 22.360677373437788
Epoch 4975/10000, Prediction Accuracy = 58.932%, Loss = 0.8544016361236573
Epoch: 4975, Batch Gradient Norm: 34.99692067429362
Epoch: 4975, Batch Gradient Norm after: 22.360679572463567
Epoch 4976/10000, Prediction Accuracy = 58.92999999999999%, Loss = 0.8617724061012269
Epoch: 4976, Batch Gradient Norm: 31.98811425306111
Epoch: 4976, Batch Gradient Norm after: 22.360679683567948
Epoch 4977/10000, Prediction Accuracy = 58.934000000000005%, Loss = 0.8542664766311645
Epoch: 4977, Batch Gradient Norm: 34.99702116888757
Epoch: 4977, Batch Gradient Norm after: 22.360679267455353
Epoch 4978/10000, Prediction Accuracy = 58.93399999999999%, Loss = 0.8616525530815125
Epoch: 4978, Batch Gradient Norm: 31.980143918814637
Epoch: 4978, Batch Gradient Norm after: 22.360678188890276
Epoch 4979/10000, Prediction Accuracy = 58.938%, Loss = 0.8541322112083435
Epoch: 4979, Batch Gradient Norm: 34.991674411040236
Epoch: 4979, Batch Gradient Norm after: 22.360680541522356
Epoch 4980/10000, Prediction Accuracy = 58.928%, Loss = 0.8615228414535523
Epoch: 4980, Batch Gradient Norm: 31.977568733741457
Epoch: 4980, Batch Gradient Norm after: 22.360677695362263
Epoch 4981/10000, Prediction Accuracy = 58.94%, Loss = 0.8540016174316406
Epoch: 4981, Batch Gradient Norm: 34.98849639696058
Epoch: 4981, Batch Gradient Norm after: 22.360678995415952
Epoch 4982/10000, Prediction Accuracy = 58.938%, Loss = 0.8613937735557556
Epoch: 4982, Batch Gradient Norm: 31.97291761198917
Epoch: 4982, Batch Gradient Norm after: 22.360676962970548
Epoch 4983/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.8538731575012207
Epoch: 4983, Batch Gradient Norm: 34.98135065952029
Epoch: 4983, Batch Gradient Norm after: 22.36067824674379
Epoch 4984/10000, Prediction Accuracy = 58.94000000000001%, Loss = 0.8612581968307496
Epoch: 4984, Batch Gradient Norm: 31.968484486027652
Epoch: 4984, Batch Gradient Norm after: 22.36067869079237
Epoch 4985/10000, Prediction Accuracy = 58.952%, Loss = 0.8537453293800354
Epoch: 4985, Batch Gradient Norm: 34.97409095725775
Epoch: 4985, Batch Gradient Norm after: 22.36067979714461
Epoch 4986/10000, Prediction Accuracy = 58.938%, Loss = 0.8611205101013184
Epoch: 4986, Batch Gradient Norm: 31.96643098489898
Epoch: 4986, Batch Gradient Norm after: 22.360677248525118
Epoch 4987/10000, Prediction Accuracy = 58.95799999999999%, Loss = 0.853620445728302
Epoch: 4987, Batch Gradient Norm: 34.96688493544213
Epoch: 4987, Batch Gradient Norm after: 22.36067931008417
Epoch 4988/10000, Prediction Accuracy = 58.934000000000005%, Loss = 0.8609834432601928
Epoch: 4988, Batch Gradient Norm: 31.960773515838348
Epoch: 4988, Batch Gradient Norm after: 22.360676454541558
Epoch 4989/10000, Prediction Accuracy = 58.95799999999999%, Loss = 0.8534919261932373
Epoch: 4989, Batch Gradient Norm: 34.96175242451657
Epoch: 4989, Batch Gradient Norm after: 22.360680928572105
Epoch 4990/10000, Prediction Accuracy = 58.936%, Loss = 0.8608538150787354
Epoch: 4990, Batch Gradient Norm: 31.957836057855452
Epoch: 4990, Batch Gradient Norm after: 22.360677062224763
Epoch 4991/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.8533619165420532
Epoch: 4991, Batch Gradient Norm: 34.9612781811679
Epoch: 4991, Batch Gradient Norm after: 22.36067937080147
Epoch 4992/10000, Prediction Accuracy = 58.94200000000001%, Loss = 0.8607319831848145
Epoch: 4992, Batch Gradient Norm: 31.950867335872946
Epoch: 4992, Batch Gradient Norm after: 22.360678799212884
Epoch 4993/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8532342553138733
Epoch: 4993, Batch Gradient Norm: 34.955964465076406
Epoch: 4993, Batch Gradient Norm after: 22.3606820302319
Epoch 4994/10000, Prediction Accuracy = 58.938%, Loss = 0.8606027483940124
Epoch: 4994, Batch Gradient Norm: 31.947893016006013
Epoch: 4994, Batch Gradient Norm after: 22.360677680697737
Epoch 4995/10000, Prediction Accuracy = 58.95%, Loss = 0.8530975341796875
Epoch: 4995, Batch Gradient Norm: 34.95302746435914
Epoch: 4995, Batch Gradient Norm after: 22.36068037374471
Epoch 4996/10000, Prediction Accuracy = 58.943999999999996%, Loss = 0.8604745864868164
Epoch: 4996, Batch Gradient Norm: 31.939347095628253
Epoch: 4996, Batch Gradient Norm after: 22.360678565237897
Epoch 4997/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.8529634833335876
Epoch: 4997, Batch Gradient Norm: 34.950702151854955
Epoch: 4997, Batch Gradient Norm after: 22.36067795708822
Epoch 4998/10000, Prediction Accuracy = 58.943999999999996%, Loss = 0.860353410243988
Epoch: 4998, Batch Gradient Norm: 31.93405129877823
Epoch: 4998, Batch Gradient Norm after: 22.360678918710946
Epoch 4999/10000, Prediction Accuracy = 58.948%, Loss = 0.8528294682502746
Epoch: 4999, Batch Gradient Norm: 34.9476169976308
Epoch: 4999, Batch Gradient Norm after: 22.36068090188829
Epoch 5000/10000, Prediction Accuracy = 58.934000000000005%, Loss = 0.8602317571640015
Epoch: 5000, Batch Gradient Norm: 31.9291805142786
Epoch: 5000, Batch Gradient Norm after: 22.360677335765605
Epoch 5001/10000, Prediction Accuracy = 58.94%, Loss = 0.8527018547058105
Epoch: 5001, Batch Gradient Norm: 34.944015983577515
Epoch: 5001, Batch Gradient Norm after: 22.360680422934532
Epoch 5002/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8601097822189331
Epoch: 5002, Batch Gradient Norm: 31.92487628777409
Epoch: 5002, Batch Gradient Norm after: 22.360678055675677
Epoch 5003/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8525754809379578
Epoch: 5003, Batch Gradient Norm: 34.93627068397989
Epoch: 5003, Batch Gradient Norm after: 22.36068001013194
Epoch 5004/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8599629521369934
Epoch: 5004, Batch Gradient Norm: 31.92155672598328
Epoch: 5004, Batch Gradient Norm after: 22.360677397168168
Epoch 5005/10000, Prediction Accuracy = 58.948%, Loss = 0.8524511337280274
Epoch: 5005, Batch Gradient Norm: 34.92616133351716
Epoch: 5005, Batch Gradient Norm after: 22.360679358335386
Epoch 5006/10000, Prediction Accuracy = 58.952%, Loss = 0.8598168373107911
Epoch: 5006, Batch Gradient Norm: 31.919243310665998
Epoch: 5006, Batch Gradient Norm after: 22.360677177657973
Epoch 5007/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.8523300290107727
Epoch: 5007, Batch Gradient Norm: 34.91724375483668
Epoch: 5007, Batch Gradient Norm after: 22.360680337791177
Epoch 5008/10000, Prediction Accuracy = 58.943999999999996%, Loss = 0.8596777439117431
Epoch: 5008, Batch Gradient Norm: 31.917703164389696
Epoch: 5008, Batch Gradient Norm after: 22.360678650720903
Epoch 5009/10000, Prediction Accuracy = 58.95%, Loss = 0.8522101402282715
Epoch: 5009, Batch Gradient Norm: 34.91082139552565
Epoch: 5009, Batch Gradient Norm after: 22.36067859499158
Epoch 5010/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8595372676849365
Epoch: 5010, Batch Gradient Norm: 31.914899639220863
Epoch: 5010, Batch Gradient Norm after: 22.360675917185347
Epoch 5011/10000, Prediction Accuracy = 58.958000000000006%, Loss = 0.8520894289016724
Epoch: 5011, Batch Gradient Norm: 34.90379007933642
Epoch: 5011, Batch Gradient Norm after: 22.36067918428252
Epoch 5012/10000, Prediction Accuracy = 58.94%, Loss = 0.8594045042991638
Epoch: 5012, Batch Gradient Norm: 31.9092620331924
Epoch: 5012, Batch Gradient Norm after: 22.360676765475727
Epoch 5013/10000, Prediction Accuracy = 58.95799999999999%, Loss = 0.8519612789154053
Epoch: 5013, Batch Gradient Norm: 34.89991933448649
Epoch: 5013, Batch Gradient Norm after: 22.360679689864043
Epoch 5014/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8592790365219116
Epoch: 5014, Batch Gradient Norm: 31.90523423933993
Epoch: 5014, Batch Gradient Norm after: 22.3606771468148
Epoch 5015/10000, Prediction Accuracy = 58.959999999999994%, Loss = 0.851834750175476
Epoch: 5015, Batch Gradient Norm: 34.89559231703802
Epoch: 5015, Batch Gradient Norm after: 22.360678975258924
Epoch 5016/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.8591578125953674
Epoch: 5016, Batch Gradient Norm: 31.898369346713157
Epoch: 5016, Batch Gradient Norm after: 22.360674429009187
Epoch 5017/10000, Prediction Accuracy = 58.955999999999996%, Loss = 0.8517050504684448
Epoch: 5017, Batch Gradient Norm: 34.89364191868572
Epoch: 5017, Batch Gradient Norm after: 22.360679441906626
Epoch 5018/10000, Prediction Accuracy = 58.95%, Loss = 0.8590311169624328
Epoch: 5018, Batch Gradient Norm: 31.892431813879504
Epoch: 5018, Batch Gradient Norm after: 22.360676334177885
Epoch 5019/10000, Prediction Accuracy = 58.96600000000001%, Loss = 0.8515756845474243
Epoch: 5019, Batch Gradient Norm: 34.8899264799546
Epoch: 5019, Batch Gradient Norm after: 22.36067866060135
Epoch 5020/10000, Prediction Accuracy = 58.95%, Loss = 0.8588996291160583
Epoch: 5020, Batch Gradient Norm: 31.888402681154925
Epoch: 5020, Batch Gradient Norm after: 22.360677781173145
Epoch 5021/10000, Prediction Accuracy = 58.976%, Loss = 0.8514469385147094
Epoch: 5021, Batch Gradient Norm: 34.88096351414447
Epoch: 5021, Batch Gradient Norm after: 22.36067675000095
Epoch 5022/10000, Prediction Accuracy = 58.955999999999996%, Loss = 0.8587709188461303
Epoch: 5022, Batch Gradient Norm: 31.882183307344025
Epoch: 5022, Batch Gradient Norm after: 22.36068085775466
Epoch 5023/10000, Prediction Accuracy = 58.977999999999994%, Loss = 0.8513188481330871
Epoch: 5023, Batch Gradient Norm: 34.877651890348716
Epoch: 5023, Batch Gradient Norm after: 22.360680063058368
Epoch 5024/10000, Prediction Accuracy = 58.959999999999994%, Loss = 0.8586431622505188
Epoch: 5024, Batch Gradient Norm: 31.881540271628026
Epoch: 5024, Batch Gradient Norm after: 22.360678026642002
Epoch 5025/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8511933803558349
Epoch: 5025, Batch Gradient Norm: 34.87147844929116
Epoch: 5025, Batch Gradient Norm after: 22.360677758495484
Epoch 5026/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8585166811943055
Epoch: 5026, Batch Gradient Norm: 31.878181101178075
Epoch: 5026, Batch Gradient Norm after: 22.360680974495597
Epoch 5027/10000, Prediction Accuracy = 58.983999999999995%, Loss = 0.8510687947273254
Epoch: 5027, Batch Gradient Norm: 34.864968518749876
Epoch: 5027, Batch Gradient Norm after: 22.36067429393403
Epoch 5028/10000, Prediction Accuracy = 58.955999999999996%, Loss = 0.8583783864974975
Epoch: 5028, Batch Gradient Norm: 31.875034153018603
Epoch: 5028, Batch Gradient Norm after: 22.36067861629904
Epoch 5029/10000, Prediction Accuracy = 58.988%, Loss = 0.8509441256523133
Epoch: 5029, Batch Gradient Norm: 34.85478021806984
Epoch: 5029, Batch Gradient Norm after: 22.360678542746896
Epoch 5030/10000, Prediction Accuracy = 58.962%, Loss = 0.8582295298576355
Epoch: 5030, Batch Gradient Norm: 31.874919996825266
Epoch: 5030, Batch Gradient Norm after: 22.360679458575728
Epoch 5031/10000, Prediction Accuracy = 58.988%, Loss = 0.8508200168609619
Epoch: 5031, Batch Gradient Norm: 34.842336801332465
Epoch: 5031, Batch Gradient Norm after: 22.36067856943509
Epoch 5032/10000, Prediction Accuracy = 58.974000000000004%, Loss = 0.8580832004547119
Epoch: 5032, Batch Gradient Norm: 31.875742609387455
Epoch: 5032, Batch Gradient Norm after: 22.360680529881588
Epoch 5033/10000, Prediction Accuracy = 58.99399999999999%, Loss = 0.8507057785987854
Epoch: 5033, Batch Gradient Norm: 34.82727244040246
Epoch: 5033, Batch Gradient Norm after: 22.360679592717894
Epoch 5034/10000, Prediction Accuracy = 58.974000000000004%, Loss = 0.8579395294189454
Epoch: 5034, Batch Gradient Norm: 31.877536953391314
Epoch: 5034, Batch Gradient Norm after: 22.360676745820328
Epoch 5035/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.850596296787262
Epoch: 5035, Batch Gradient Norm: 34.81725239164888
Epoch: 5035, Batch Gradient Norm after: 22.360681575984376
Epoch 5036/10000, Prediction Accuracy = 58.983999999999995%, Loss = 0.8577943682670593
Epoch: 5036, Batch Gradient Norm: 31.876167582016315
Epoch: 5036, Batch Gradient Norm after: 22.360681104874068
Epoch 5037/10000, Prediction Accuracy = 59.004%, Loss = 0.8504734635353088
Epoch: 5037, Batch Gradient Norm: 34.80763558010896
Epoch: 5037, Batch Gradient Norm after: 22.360679783185304
Epoch 5038/10000, Prediction Accuracy = 58.974000000000004%, Loss = 0.8576433420181274
Epoch: 5038, Batch Gradient Norm: 31.875515283674513
Epoch: 5038, Batch Gradient Norm after: 22.360680554905738
Epoch 5039/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.8503480672836303
Epoch: 5039, Batch Gradient Norm: 34.79857732728387
Epoch: 5039, Batch Gradient Norm after: 22.360679051826015
Epoch 5040/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8575040459632873
Epoch: 5040, Batch Gradient Norm: 31.871967978391343
Epoch: 5040, Batch Gradient Norm after: 22.3606796428975
Epoch 5041/10000, Prediction Accuracy = 59.001999999999995%, Loss = 0.8502304077148437
Epoch: 5041, Batch Gradient Norm: 34.789667056947096
Epoch: 5041, Batch Gradient Norm after: 22.360679688597696
Epoch 5042/10000, Prediction Accuracy = 58.988%, Loss = 0.8573739290237427
Epoch: 5042, Batch Gradient Norm: 31.872809330876052
Epoch: 5042, Batch Gradient Norm after: 22.360678414148072
Epoch 5043/10000, Prediction Accuracy = 59.016%, Loss = 0.8501110911369324
Epoch: 5043, Batch Gradient Norm: 34.781023391906714
Epoch: 5043, Batch Gradient Norm after: 22.36068037244918
Epoch 5044/10000, Prediction Accuracy = 58.988%, Loss = 0.8572386264801025
Epoch: 5044, Batch Gradient Norm: 31.868986093518785
Epoch: 5044, Batch Gradient Norm after: 22.360679992344107
Epoch 5045/10000, Prediction Accuracy = 59.024%, Loss = 0.8499920487403869
Epoch: 5045, Batch Gradient Norm: 34.77283088156595
Epoch: 5045, Batch Gradient Norm after: 22.360677094664798
Epoch 5046/10000, Prediction Accuracy = 58.992%, Loss = 0.8570937633514404
Epoch: 5046, Batch Gradient Norm: 31.8634665597347
Epoch: 5046, Batch Gradient Norm after: 22.360677495029844
Epoch 5047/10000, Prediction Accuracy = 59.010000000000005%, Loss = 0.8498553395271301
Epoch: 5047, Batch Gradient Norm: 34.769265443006724
Epoch: 5047, Batch Gradient Norm after: 22.36068012706675
Epoch 5048/10000, Prediction Accuracy = 58.996%, Loss = 0.8569684743881225
Epoch: 5048, Batch Gradient Norm: 31.859055390948775
Epoch: 5048, Batch Gradient Norm after: 22.360679300188345
Epoch 5049/10000, Prediction Accuracy = 59.016000000000005%, Loss = 0.849725866317749
Epoch: 5049, Batch Gradient Norm: 34.76549234473324
Epoch: 5049, Batch Gradient Norm after: 22.36068028418982
Epoch 5050/10000, Prediction Accuracy = 58.989999999999995%, Loss = 0.856843626499176
Epoch: 5050, Batch Gradient Norm: 31.85351561781296
Epoch: 5050, Batch Gradient Norm after: 22.360678109306335
Epoch 5051/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.8496040463447571
Epoch: 5051, Batch Gradient Norm: 34.75902821819956
Epoch: 5051, Batch Gradient Norm after: 22.360680805138504
Epoch 5052/10000, Prediction Accuracy = 58.989999999999995%, Loss = 0.8567267179489135
Epoch: 5052, Batch Gradient Norm: 31.849091867023347
Epoch: 5052, Batch Gradient Norm after: 22.360679700855265
Epoch 5053/10000, Prediction Accuracy = 59.036%, Loss = 0.8494839668273926
Epoch: 5053, Batch Gradient Norm: 34.75521939622352
Epoch: 5053, Batch Gradient Norm after: 22.36067839332132
Epoch 5054/10000, Prediction Accuracy = 58.99400000000001%, Loss = 0.8565977215766907
Epoch: 5054, Batch Gradient Norm: 31.84327937927422
Epoch: 5054, Batch Gradient Norm after: 22.36067918166027
Epoch 5055/10000, Prediction Accuracy = 59.034000000000006%, Loss = 0.8493574738502503
Epoch: 5055, Batch Gradient Norm: 34.74973684371353
Epoch: 5055, Batch Gradient Norm after: 22.36067978376717
Epoch 5056/10000, Prediction Accuracy = 58.99400000000001%, Loss = 0.856461477279663
Epoch: 5056, Batch Gradient Norm: 31.840051953075285
Epoch: 5056, Batch Gradient Norm after: 22.36067824528161
Epoch 5057/10000, Prediction Accuracy = 59.034000000000006%, Loss = 0.84923175573349
Epoch: 5057, Batch Gradient Norm: 34.74246518589787
Epoch: 5057, Batch Gradient Norm after: 22.36067916146516
Epoch 5058/10000, Prediction Accuracy = 58.996%, Loss = 0.8563296556472778
Epoch: 5058, Batch Gradient Norm: 31.837317039633668
Epoch: 5058, Batch Gradient Norm after: 22.360678427380613
Epoch 5059/10000, Prediction Accuracy = 59.038%, Loss = 0.8491073846817017
Epoch: 5059, Batch Gradient Norm: 34.73918953770128
Epoch: 5059, Batch Gradient Norm after: 22.360680138923215
Epoch 5060/10000, Prediction Accuracy = 59.00599999999999%, Loss = 0.8562129139900208
Epoch: 5060, Batch Gradient Norm: 31.83198499813381
Epoch: 5060, Batch Gradient Norm after: 22.360678649704777
Epoch 5061/10000, Prediction Accuracy = 59.036%, Loss = 0.8489875793457031
Epoch: 5061, Batch Gradient Norm: 34.73292861042093
Epoch: 5061, Batch Gradient Norm after: 22.36068025230238
Epoch 5062/10000, Prediction Accuracy = 59.0%, Loss = 0.8560779452323913
Epoch: 5062, Batch Gradient Norm: 31.82913113225844
Epoch: 5062, Batch Gradient Norm after: 22.360679122893075
Epoch 5063/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.848863136768341
Epoch: 5063, Batch Gradient Norm: 34.72699846791537
Epoch: 5063, Batch Gradient Norm after: 22.36067915965997
Epoch 5064/10000, Prediction Accuracy = 59.00600000000001%, Loss = 0.8559385538101196
Epoch: 5064, Batch Gradient Norm: 31.827757149806107
Epoch: 5064, Batch Gradient Norm after: 22.360677670343904
Epoch 5065/10000, Prediction Accuracy = 59.034000000000006%, Loss = 0.8487357497215271
Epoch: 5065, Batch Gradient Norm: 34.72116640869923
Epoch: 5065, Batch Gradient Norm after: 22.360679239900808
Epoch 5066/10000, Prediction Accuracy = 59.004%, Loss = 0.8558122277259826
Epoch: 5066, Batch Gradient Norm: 31.822005950309446
Epoch: 5066, Batch Gradient Norm after: 22.360677503111948
Epoch 5067/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.8486150026321411
Epoch: 5067, Batch Gradient Norm: 34.715543119529116
Epoch: 5067, Batch Gradient Norm after: 22.36067784544836
Epoch 5068/10000, Prediction Accuracy = 59.012%, Loss = 0.8556839108467102
Epoch: 5068, Batch Gradient Norm: 31.81709371704869
Epoch: 5068, Batch Gradient Norm after: 22.36067654573609
Epoch 5069/10000, Prediction Accuracy = 59.032000000000004%, Loss = 0.8484923958778381
Epoch: 5069, Batch Gradient Norm: 34.71334736590524
Epoch: 5069, Batch Gradient Norm after: 22.360677188190767
Epoch 5070/10000, Prediction Accuracy = 59.012000000000015%, Loss = 0.8555609583854675
Epoch: 5070, Batch Gradient Norm: 31.81279765946676
Epoch: 5070, Batch Gradient Norm after: 22.360678471989985
Epoch 5071/10000, Prediction Accuracy = 59.036%, Loss = 0.8483608484268188
Epoch: 5071, Batch Gradient Norm: 34.710905053541005
Epoch: 5071, Batch Gradient Norm after: 22.360676759326694
Epoch 5072/10000, Prediction Accuracy = 59.012%, Loss = 0.8554408311843872
Epoch: 5072, Batch Gradient Norm: 31.806412762971807
Epoch: 5072, Batch Gradient Norm after: 22.360678355307442
Epoch 5073/10000, Prediction Accuracy = 59.038%, Loss = 0.8482382774353028
Epoch: 5073, Batch Gradient Norm: 34.71125548128158
Epoch: 5073, Batch Gradient Norm after: 22.360678078281484
Epoch 5074/10000, Prediction Accuracy = 59.02%, Loss = 0.8553232431411744
Epoch: 5074, Batch Gradient Norm: 31.79714810949519
Epoch: 5074, Batch Gradient Norm after: 22.360678676884316
Epoch 5075/10000, Prediction Accuracy = 59.038%, Loss = 0.8481024026870727
Epoch: 5075, Batch Gradient Norm: 34.710546030291056
Epoch: 5075, Batch Gradient Norm after: 22.36067780194321
Epoch 5076/10000, Prediction Accuracy = 59.028%, Loss = 0.8552108407020569
Epoch: 5076, Batch Gradient Norm: 31.79113951826291
Epoch: 5076, Batch Gradient Norm after: 22.360677897181798
Epoch 5077/10000, Prediction Accuracy = 59.04%, Loss = 0.8479692101478576
Epoch: 5077, Batch Gradient Norm: 34.70999587196946
Epoch: 5077, Batch Gradient Norm after: 22.360678914574372
Epoch 5078/10000, Prediction Accuracy = 59.024%, Loss = 0.8550997018814087
Epoch: 5078, Batch Gradient Norm: 31.7848744534093
Epoch: 5078, Batch Gradient Norm after: 22.360679187870513
Epoch 5079/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8478392958641052
Epoch: 5079, Batch Gradient Norm: 34.705048290119116
Epoch: 5079, Batch Gradient Norm after: 22.360678906099906
Epoch 5080/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.8549745917320252
Epoch: 5080, Batch Gradient Norm: 31.7784265498176
Epoch: 5080, Batch Gradient Norm after: 22.36068121519999
Epoch 5081/10000, Prediction Accuracy = 59.044%, Loss = 0.8477138161659241
Epoch: 5081, Batch Gradient Norm: 34.70739861117864
Epoch: 5081, Batch Gradient Norm after: 22.360678405406492
Epoch 5082/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.854860270023346
Epoch: 5082, Batch Gradient Norm: 31.77104728458133
Epoch: 5082, Batch Gradient Norm after: 22.360679274830492
Epoch 5083/10000, Prediction Accuracy = 59.048%, Loss = 0.8475804686546325
Epoch: 5083, Batch Gradient Norm: 34.70372140173746
Epoch: 5083, Batch Gradient Norm after: 22.36067882896463
Epoch 5084/10000, Prediction Accuracy = 59.036%, Loss = 0.8547469496726989
Epoch: 5084, Batch Gradient Norm: 31.767339398873062
Epoch: 5084, Batch Gradient Norm after: 22.36067840122041
Epoch 5085/10000, Prediction Accuracy = 59.048%, Loss = 0.8474546432495117
Epoch: 5085, Batch Gradient Norm: 34.69276123107058
Epoch: 5085, Batch Gradient Norm after: 22.360677411073706
Epoch 5086/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.8546015024185181
Epoch: 5086, Batch Gradient Norm: 31.76928505084323
Epoch: 5086, Batch Gradient Norm after: 22.360678139706636
Epoch 5087/10000, Prediction Accuracy = 59.052%, Loss = 0.8473368287086487
Epoch: 5087, Batch Gradient Norm: 34.68224452661835
Epoch: 5087, Batch Gradient Norm after: 22.360677211150723
Epoch 5088/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.8544722557067871
Epoch: 5088, Batch Gradient Norm: 31.763855885846514
Epoch: 5088, Batch Gradient Norm after: 22.36067967820515
Epoch 5089/10000, Prediction Accuracy = 59.04%, Loss = 0.8472196102142334
Epoch: 5089, Batch Gradient Norm: 34.67424858956637
Epoch: 5089, Batch Gradient Norm after: 22.360678243222065
Epoch 5090/10000, Prediction Accuracy = 59.038%, Loss = 0.8543410301208496
Epoch: 5090, Batch Gradient Norm: 31.761727777508064
Epoch: 5090, Batch Gradient Norm after: 22.36068024031032
Epoch 5091/10000, Prediction Accuracy = 59.04%, Loss = 0.8471003651618958
Epoch: 5091, Batch Gradient Norm: 34.66479912064
Epoch: 5091, Batch Gradient Norm after: 22.360677139040327
Epoch 5092/10000, Prediction Accuracy = 59.032%, Loss = 0.854208254814148
Epoch: 5092, Batch Gradient Norm: 31.758802230992888
Epoch: 5092, Batch Gradient Norm after: 22.360679504788713
Epoch 5093/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8469804525375366
Epoch: 5093, Batch Gradient Norm: 34.65646381540698
Epoch: 5093, Batch Gradient Norm after: 22.36067669501354
Epoch 5094/10000, Prediction Accuracy = 59.036%, Loss = 0.8540748596191406
Epoch: 5094, Batch Gradient Norm: 31.75606272109024
Epoch: 5094, Batch Gradient Norm after: 22.360679496159598
Epoch 5095/10000, Prediction Accuracy = 59.05%, Loss = 0.8468570590019227
Epoch: 5095, Batch Gradient Norm: 34.64943015594431
Epoch: 5095, Batch Gradient Norm after: 22.360675954753315
Epoch 5096/10000, Prediction Accuracy = 59.03399999999999%, Loss = 0.8539493799209594
Epoch: 5096, Batch Gradient Norm: 31.751017085233027
Epoch: 5096, Batch Gradient Norm after: 22.36067890165622
Epoch 5097/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8467351078987122
Epoch: 5097, Batch Gradient Norm: 34.64392192229867
Epoch: 5097, Batch Gradient Norm after: 22.36067878850752
Epoch 5098/10000, Prediction Accuracy = 59.036%, Loss = 0.8538171410560608
Epoch: 5098, Batch Gradient Norm: 31.747022364536974
Epoch: 5098, Batch Gradient Norm after: 22.360678946461903
Epoch 5099/10000, Prediction Accuracy = 59.052%, Loss = 0.8466104388236999
Epoch: 5099, Batch Gradient Norm: 34.63818497366428
Epoch: 5099, Batch Gradient Norm after: 22.36067709173039
Epoch 5100/10000, Prediction Accuracy = 59.038%, Loss = 0.8536863088607788
Epoch: 5100, Batch Gradient Norm: 31.742133817557587
Epoch: 5100, Batch Gradient Norm after: 22.360680624701523
Epoch 5101/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.8464816570281982
Epoch: 5101, Batch Gradient Norm: 34.63640403533358
Epoch: 5101, Batch Gradient Norm after: 22.360676752085265
Epoch 5102/10000, Prediction Accuracy = 59.040000000000006%, Loss = 0.8535691022872924
Epoch: 5102, Batch Gradient Norm: 31.736800037803476
Epoch: 5102, Batch Gradient Norm after: 22.360679317206667
Epoch 5103/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8463525295257568
Epoch: 5103, Batch Gradient Norm: 34.63178264867664
Epoch: 5103, Batch Gradient Norm after: 22.360675794105482
Epoch 5104/10000, Prediction Accuracy = 59.038%, Loss = 0.8534393191337586
Epoch: 5104, Batch Gradient Norm: 31.732319020177016
Epoch: 5104, Batch Gradient Norm after: 22.360679918669728
Epoch 5105/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.8462269067764282
Epoch: 5105, Batch Gradient Norm: 34.6266950928058
Epoch: 5105, Batch Gradient Norm after: 22.36067861303858
Epoch 5106/10000, Prediction Accuracy = 59.038%, Loss = 0.8533179879188537
Epoch: 5106, Batch Gradient Norm: 31.727662353395274
Epoch: 5106, Batch Gradient Norm after: 22.360678182018273
Epoch 5107/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8461027860641479
Epoch: 5107, Batch Gradient Norm: 34.62084494912878
Epoch: 5107, Batch Gradient Norm after: 22.36067789945807
Epoch 5108/10000, Prediction Accuracy = 59.04200000000001%, Loss = 0.8531941175460815
Epoch: 5108, Batch Gradient Norm: 31.72690558365253
Epoch: 5108, Batch Gradient Norm after: 22.36068026801022
Epoch 5109/10000, Prediction Accuracy = 59.044000000000004%, Loss = 0.8459811329841613
Epoch: 5109, Batch Gradient Norm: 34.613742688254206
Epoch: 5109, Batch Gradient Norm after: 22.36067773898236
Epoch 5110/10000, Prediction Accuracy = 59.044000000000004%, Loss = 0.8530663251876831
Epoch: 5110, Batch Gradient Norm: 31.725320592877537
Epoch: 5110, Batch Gradient Norm after: 22.36068017260233
Epoch 5111/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.845856761932373
Epoch: 5111, Batch Gradient Norm: 34.6097275466854
Epoch: 5111, Batch Gradient Norm after: 22.36067666558311
Epoch 5112/10000, Prediction Accuracy = 59.048%, Loss = 0.8529363155364991
Epoch: 5112, Batch Gradient Norm: 31.721111187829607
Epoch: 5112, Batch Gradient Norm after: 22.360677415709695
Epoch 5113/10000, Prediction Accuracy = 59.05%, Loss = 0.8457291007041932
Epoch: 5113, Batch Gradient Norm: 34.60159472339082
Epoch: 5113, Batch Gradient Norm after: 22.36067786773524
Epoch 5114/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.852804696559906
Epoch: 5114, Batch Gradient Norm: 31.721609614622412
Epoch: 5114, Batch Gradient Norm after: 22.360680530397932
Epoch 5115/10000, Prediction Accuracy = 59.056%, Loss = 0.8456157684326172
Epoch: 5115, Batch Gradient Norm: 34.59487573030261
Epoch: 5115, Batch Gradient Norm after: 22.360676638968272
Epoch 5116/10000, Prediction Accuracy = 59.05%, Loss = 0.852675986289978
Epoch: 5116, Batch Gradient Norm: 31.719217141312
Epoch: 5116, Batch Gradient Norm after: 22.360680305297052
Epoch 5117/10000, Prediction Accuracy = 59.062%, Loss = 0.8455018162727356
Epoch: 5117, Batch Gradient Norm: 34.58189765225129
Epoch: 5117, Batch Gradient Norm after: 22.360677701137348
Epoch 5118/10000, Prediction Accuracy = 59.05800000000001%, Loss = 0.8525365829467774
Epoch: 5118, Batch Gradient Norm: 31.72022761579738
Epoch: 5118, Batch Gradient Norm after: 22.360677939489833
Epoch 5119/10000, Prediction Accuracy = 59.05799999999999%, Loss = 0.8453879952430725
Epoch: 5119, Batch Gradient Norm: 34.573446124628774
Epoch: 5119, Batch Gradient Norm after: 22.360678145247423
Epoch 5120/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.8523913383483886
Epoch: 5120, Batch Gradient Norm: 31.720252873729564
Epoch: 5120, Batch Gradient Norm after: 22.360678932423884
Epoch 5121/10000, Prediction Accuracy = 59.048%, Loss = 0.8452674984931946
Epoch: 5121, Batch Gradient Norm: 34.56542572317817
Epoch: 5121, Batch Gradient Norm after: 22.36067823817776
Epoch 5122/10000, Prediction Accuracy = 59.040000000000006%, Loss = 0.8522547245025635
Epoch: 5122, Batch Gradient Norm: 31.717159051768093
Epoch: 5122, Batch Gradient Norm after: 22.36068104949127
Epoch 5123/10000, Prediction Accuracy = 59.048%, Loss = 0.8451422214508056
Epoch: 5123, Batch Gradient Norm: 34.55842508024135
Epoch: 5123, Batch Gradient Norm after: 22.360678822086673
Epoch 5124/10000, Prediction Accuracy = 59.03399999999999%, Loss = 0.8521253943443299
Epoch: 5124, Batch Gradient Norm: 31.716917314922952
Epoch: 5124, Batch Gradient Norm after: 22.36068143357417
Epoch 5125/10000, Prediction Accuracy = 59.05%, Loss = 0.8450307011604309
Epoch: 5125, Batch Gradient Norm: 34.54914599423471
Epoch: 5125, Batch Gradient Norm after: 22.360680473326962
Epoch 5126/10000, Prediction Accuracy = 59.044%, Loss = 0.8519898056983948
Epoch: 5126, Batch Gradient Norm: 31.718509981776176
Epoch: 5126, Batch Gradient Norm after: 22.360679357487168
Epoch 5127/10000, Prediction Accuracy = 59.052%, Loss = 0.8449166893959046
Epoch: 5127, Batch Gradient Norm: 34.5390048236483
Epoch: 5127, Batch Gradient Norm after: 22.36067864910793
Epoch 5128/10000, Prediction Accuracy = 59.038%, Loss = 0.8518517851829529
Epoch: 5128, Batch Gradient Norm: 31.713692151278927
Epoch: 5128, Batch Gradient Norm after: 22.360680377981385
Epoch 5129/10000, Prediction Accuracy = 59.05%, Loss = 0.8447987794876098
Epoch: 5129, Batch Gradient Norm: 34.531560693562454
Epoch: 5129, Batch Gradient Norm after: 22.360679791647634
Epoch 5130/10000, Prediction Accuracy = 59.038%, Loss = 0.8517193078994751
Epoch: 5130, Batch Gradient Norm: 31.70985583307979
Epoch: 5130, Batch Gradient Norm after: 22.360679983399475
Epoch 5131/10000, Prediction Accuracy = 59.056%, Loss = 0.8446731567382812
Epoch: 5131, Batch Gradient Norm: 34.52719615155148
Epoch: 5131, Batch Gradient Norm after: 22.360678106383222
Epoch 5132/10000, Prediction Accuracy = 59.05%, Loss = 0.8515938878059387
Epoch: 5132, Batch Gradient Norm: 31.703625852056202
Epoch: 5132, Batch Gradient Norm after: 22.360679314775584
Epoch 5133/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.8445510149002076
Epoch: 5133, Batch Gradient Norm: 34.525820224424606
Epoch: 5133, Batch Gradient Norm after: 22.360677883349712
Epoch 5134/10000, Prediction Accuracy = 59.05%, Loss = 0.8514781951904297
Epoch: 5134, Batch Gradient Norm: 31.69714810241983
Epoch: 5134, Batch Gradient Norm after: 22.36067807352818
Epoch 5135/10000, Prediction Accuracy = 59.065999999999995%, Loss = 0.8444263339042664
Epoch: 5135, Batch Gradient Norm: 34.52735951544016
Epoch: 5135, Batch Gradient Norm after: 22.360679450751082
Epoch 5136/10000, Prediction Accuracy = 59.048%, Loss = 0.8513689160346984
Epoch: 5136, Batch Gradient Norm: 31.687170374479013
Epoch: 5136, Batch Gradient Norm after: 22.360679665332924
Epoch 5137/10000, Prediction Accuracy = 59.068%, Loss = 0.8442975044250488
Epoch: 5137, Batch Gradient Norm: 34.52771493955894
Epoch: 5137, Batch Gradient Norm after: 22.36067899724345
Epoch 5138/10000, Prediction Accuracy = 59.05%, Loss = 0.8512621760368347
Epoch: 5138, Batch Gradient Norm: 31.67960797759133
Epoch: 5138, Batch Gradient Norm after: 22.360679601317635
Epoch 5139/10000, Prediction Accuracy = 59.068000000000005%, Loss = 0.8441630244255066
Epoch: 5139, Batch Gradient Norm: 34.5299294810424
Epoch: 5139, Batch Gradient Norm after: 22.360680017946915
Epoch 5140/10000, Prediction Accuracy = 59.048%, Loss = 0.8511559844017029
Epoch: 5140, Batch Gradient Norm: 31.671380122460636
Epoch: 5140, Batch Gradient Norm after: 22.360681458302064
Epoch 5141/10000, Prediction Accuracy = 59.064%, Loss = 0.8440315842628479
Epoch: 5141, Batch Gradient Norm: 34.53053103832438
Epoch: 5141, Batch Gradient Norm after: 22.360677604021227
Epoch 5142/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8510428905487061
Epoch: 5142, Batch Gradient Norm: 31.664136629346604
Epoch: 5142, Batch Gradient Norm after: 22.360679742076353
Epoch 5143/10000, Prediction Accuracy = 59.05999999999999%, Loss = 0.8438994765281678
Epoch: 5143, Batch Gradient Norm: 34.52926007740612
Epoch: 5143, Batch Gradient Norm after: 22.360680148381743
Epoch 5144/10000, Prediction Accuracy = 59.048%, Loss = 0.8509286284446717
Epoch: 5144, Batch Gradient Norm: 31.65539283518049
Epoch: 5144, Batch Gradient Norm after: 22.36067966758962
Epoch 5145/10000, Prediction Accuracy = 59.05799999999999%, Loss = 0.8437661290168762
Epoch: 5145, Batch Gradient Norm: 34.53116204685746
Epoch: 5145, Batch Gradient Norm after: 22.360678880163814
Epoch 5146/10000, Prediction Accuracy = 59.044000000000004%, Loss = 0.8508242487907409
Epoch: 5146, Batch Gradient Norm: 31.648162228185548
Epoch: 5146, Batch Gradient Norm after: 22.36067737027343
Epoch 5147/10000, Prediction Accuracy = 59.064%, Loss = 0.8436354041099549
Epoch: 5147, Batch Gradient Norm: 34.53400571857597
Epoch: 5147, Batch Gradient Norm after: 22.36067957898008
Epoch 5148/10000, Prediction Accuracy = 59.044000000000004%, Loss = 0.8507224798202515
Epoch: 5148, Batch Gradient Norm: 31.639284246557892
Epoch: 5148, Batch Gradient Norm after: 22.3606792150818
Epoch 5149/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.843496572971344
Epoch: 5149, Batch Gradient Norm: 34.53736690253121
Epoch: 5149, Batch Gradient Norm after: 22.360678597198543
Epoch 5150/10000, Prediction Accuracy = 59.044%, Loss = 0.8506209850311279
Epoch: 5150, Batch Gradient Norm: 31.632327967169015
Epoch: 5150, Batch Gradient Norm after: 22.360679119512834
Epoch 5151/10000, Prediction Accuracy = 59.062%, Loss = 0.8433579564094543
Epoch: 5151, Batch Gradient Norm: 34.541706771209206
Epoch: 5151, Batch Gradient Norm after: 22.36068006489721
Epoch 5152/10000, Prediction Accuracy = 59.052%, Loss = 0.8505165576934814
Epoch: 5152, Batch Gradient Norm: 31.621569826137986
Epoch: 5152, Batch Gradient Norm after: 22.36067657154522
Epoch 5153/10000, Prediction Accuracy = 59.06%, Loss = 0.8432241082191467
Epoch: 5153, Batch Gradient Norm: 34.542665379035135
Epoch: 5153, Batch Gradient Norm after: 22.36067981204085
Epoch 5154/10000, Prediction Accuracy = 59.04%, Loss = 0.8504125475883484
Epoch: 5154, Batch Gradient Norm: 31.61574612067033
Epoch: 5154, Batch Gradient Norm after: 22.360677324982458
Epoch 5155/10000, Prediction Accuracy = 59.068%, Loss = 0.8430932044982911
Epoch: 5155, Batch Gradient Norm: 34.54296750177737
Epoch: 5155, Batch Gradient Norm after: 22.360679123335224
Epoch 5156/10000, Prediction Accuracy = 59.04%, Loss = 0.8503138184547424
Epoch: 5156, Batch Gradient Norm: 31.60795900531048
Epoch: 5156, Batch Gradient Norm after: 22.36067906766456
Epoch 5157/10000, Prediction Accuracy = 59.072%, Loss = 0.8429694652557373
Epoch: 5157, Batch Gradient Norm: 34.542597543694285
Epoch: 5157, Batch Gradient Norm after: 22.360676793957147
Epoch 5158/10000, Prediction Accuracy = 59.048%, Loss = 0.8502049803733825
Epoch: 5158, Batch Gradient Norm: 31.60046710772271
Epoch: 5158, Batch Gradient Norm after: 22.360679421159794
Epoch 5159/10000, Prediction Accuracy = 59.074%, Loss = 0.8428478121757508
Epoch: 5159, Batch Gradient Norm: 34.53686970097565
Epoch: 5159, Batch Gradient Norm after: 22.360677720273006
Epoch 5160/10000, Prediction Accuracy = 59.05%, Loss = 0.8500663995742798
Epoch: 5160, Batch Gradient Norm: 31.600324832908253
Epoch: 5160, Batch Gradient Norm after: 22.360679454487073
Epoch 5161/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8427231192588807
Epoch: 5161, Batch Gradient Norm: 34.531236901359286
Epoch: 5161, Batch Gradient Norm after: 22.360678060473056
Epoch 5162/10000, Prediction Accuracy = 59.065999999999995%, Loss = 0.8499363183975219
Epoch: 5162, Batch Gradient Norm: 31.596960135532616
Epoch: 5162, Batch Gradient Norm after: 22.360678492240574
Epoch 5163/10000, Prediction Accuracy = 59.074%, Loss = 0.8426069974899292
Epoch: 5163, Batch Gradient Norm: 34.52117458037904
Epoch: 5163, Batch Gradient Norm after: 22.360679085506668
Epoch 5164/10000, Prediction Accuracy = 59.074%, Loss = 0.849798858165741
Epoch: 5164, Batch Gradient Norm: 31.5965598663199
Epoch: 5164, Batch Gradient Norm after: 22.360679108600582
Epoch 5165/10000, Prediction Accuracy = 59.086%, Loss = 0.8424923777580261
Epoch: 5165, Batch Gradient Norm: 34.515322528597196
Epoch: 5165, Batch Gradient Norm after: 22.360678104561345
Epoch 5166/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.8496749758720398
Epoch: 5166, Batch Gradient Norm: 31.592741736141996
Epoch: 5166, Batch Gradient Norm after: 22.360677414572834
Epoch 5167/10000, Prediction Accuracy = 59.092%, Loss = 0.8423771500587464
Epoch: 5167, Batch Gradient Norm: 34.508718184900275
Epoch: 5167, Batch Gradient Norm after: 22.360678502639566
Epoch 5168/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.8495476007461548
Epoch: 5168, Batch Gradient Norm: 31.590112574859212
Epoch: 5168, Batch Gradient Norm after: 22.360676286406502
Epoch 5169/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8422530651092529
Epoch: 5169, Batch Gradient Norm: 34.506012099261795
Epoch: 5169, Batch Gradient Norm after: 22.36067885444098
Epoch 5170/10000, Prediction Accuracy = 59.064%, Loss = 0.849416995048523
Epoch: 5170, Batch Gradient Norm: 31.5851138426964
Epoch: 5170, Batch Gradient Norm after: 22.360677595026466
Epoch 5171/10000, Prediction Accuracy = 59.088%, Loss = 0.8421278238296509
Epoch: 5171, Batch Gradient Norm: 34.49835807975297
Epoch: 5171, Batch Gradient Norm after: 22.360678079884615
Epoch 5172/10000, Prediction Accuracy = 59.064%, Loss = 0.8492876768112183
Epoch: 5172, Batch Gradient Norm: 31.584526849718884
Epoch: 5172, Batch Gradient Norm after: 22.36067859243448
Epoch 5173/10000, Prediction Accuracy = 59.08%, Loss = 0.8420099496841431
Epoch: 5173, Batch Gradient Norm: 34.48995854407635
Epoch: 5173, Batch Gradient Norm after: 22.36067766644799
Epoch 5174/10000, Prediction Accuracy = 59.076%, Loss = 0.8491529941558837
Epoch: 5174, Batch Gradient Norm: 31.58309756636354
Epoch: 5174, Batch Gradient Norm after: 22.36067911487446
Epoch 5175/10000, Prediction Accuracy = 59.093999999999994%, Loss = 0.841896367073059
Epoch: 5175, Batch Gradient Norm: 34.48197684463313
Epoch: 5175, Batch Gradient Norm after: 22.360679577977958
Epoch 5176/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8490227818489074
Epoch: 5176, Batch Gradient Norm: 31.580524822210002
Epoch: 5176, Batch Gradient Norm after: 22.360677755665687
Epoch 5177/10000, Prediction Accuracy = 59.086%, Loss = 0.8417780995368958
Epoch: 5177, Batch Gradient Norm: 34.47408199808294
Epoch: 5177, Batch Gradient Norm after: 22.36068076033504
Epoch 5178/10000, Prediction Accuracy = 59.074%, Loss = 0.8488902688026428
Epoch: 5178, Batch Gradient Norm: 31.58049219215832
Epoch: 5178, Batch Gradient Norm after: 22.360677876235155
Epoch 5179/10000, Prediction Accuracy = 59.088%, Loss = 0.8416620492935181
Epoch: 5179, Batch Gradient Norm: 34.46455695178941
Epoch: 5179, Batch Gradient Norm after: 22.360681138814638
Epoch 5180/10000, Prediction Accuracy = 59.072%, Loss = 0.8487492561340332
Epoch: 5180, Batch Gradient Norm: 31.578668611591606
Epoch: 5180, Batch Gradient Norm after: 22.360679280054995
Epoch 5181/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8415415525436402
Epoch: 5181, Batch Gradient Norm: 34.459268653427834
Epoch: 5181, Batch Gradient Norm after: 22.360680040491385
Epoch 5182/10000, Prediction Accuracy = 59.076%, Loss = 0.8486185789108276
Epoch: 5182, Batch Gradient Norm: 31.574548868891945
Epoch: 5182, Batch Gradient Norm after: 22.360679463292108
Epoch 5183/10000, Prediction Accuracy = 59.098%, Loss = 0.8414232850074768
Epoch: 5183, Batch Gradient Norm: 34.45615547097493
Epoch: 5183, Batch Gradient Norm after: 22.360679866195785
Epoch 5184/10000, Prediction Accuracy = 59.076%, Loss = 0.8484972715377808
Epoch: 5184, Batch Gradient Norm: 31.571150695023892
Epoch: 5184, Batch Gradient Norm after: 22.360679439496614
Epoch 5185/10000, Prediction Accuracy = 59.098%, Loss = 0.8413064479827881
Epoch: 5185, Batch Gradient Norm: 34.448905602042956
Epoch: 5185, Batch Gradient Norm after: 22.360677241628537
Epoch 5186/10000, Prediction Accuracy = 59.074%, Loss = 0.8483707904815674
Epoch: 5186, Batch Gradient Norm: 31.568137739527618
Epoch: 5186, Batch Gradient Norm after: 22.360678262095938
Epoch 5187/10000, Prediction Accuracy = 59.1%, Loss = 0.8411862015724182
Epoch: 5187, Batch Gradient Norm: 34.446667543750216
Epoch: 5187, Batch Gradient Norm after: 22.36067781423702
Epoch 5188/10000, Prediction Accuracy = 59.077999999999996%, Loss = 0.8482474684715271
Epoch: 5188, Batch Gradient Norm: 31.563478289349593
Epoch: 5188, Batch Gradient Norm after: 22.360678700427208
Epoch 5189/10000, Prediction Accuracy = 59.110000000000014%, Loss = 0.8410595536231995
Epoch: 5189, Batch Gradient Norm: 34.441677970913574
Epoch: 5189, Batch Gradient Norm after: 22.360679429418845
Epoch 5190/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8481235980987549
Epoch: 5190, Batch Gradient Norm: 31.5616889994097
Epoch: 5190, Batch Gradient Norm after: 22.36067814923835
Epoch 5191/10000, Prediction Accuracy = 59.10600000000001%, Loss = 0.8409374356269836
Epoch: 5191, Batch Gradient Norm: 34.43347433864295
Epoch: 5191, Batch Gradient Norm after: 22.360678873624263
Epoch 5192/10000, Prediction Accuracy = 59.08%, Loss = 0.8480039119720459
Epoch: 5192, Batch Gradient Norm: 31.5593565645863
Epoch: 5192, Batch Gradient Norm after: 22.36067943916589
Epoch 5193/10000, Prediction Accuracy = 59.10200000000001%, Loss = 0.8408260583877564
Epoch: 5193, Batch Gradient Norm: 34.42498318022239
Epoch: 5193, Batch Gradient Norm after: 22.360677173069117
Epoch 5194/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.8478709697723389
Epoch: 5194, Batch Gradient Norm: 31.559865688830374
Epoch: 5194, Batch Gradient Norm after: 22.36068014796578
Epoch 5195/10000, Prediction Accuracy = 59.10999999999999%, Loss = 0.8407126069068909
Epoch: 5195, Batch Gradient Norm: 34.416384050937374
Epoch: 5195, Batch Gradient Norm after: 22.360679908980806
Epoch 5196/10000, Prediction Accuracy = 59.088%, Loss = 0.8477296471595764
Epoch: 5196, Batch Gradient Norm: 31.55690756577123
Epoch: 5196, Batch Gradient Norm after: 22.360679314834062
Epoch 5197/10000, Prediction Accuracy = 59.11600000000001%, Loss = 0.8405950665473938
Epoch: 5197, Batch Gradient Norm: 34.407528621222454
Epoch: 5197, Batch Gradient Norm after: 22.3606803083287
Epoch 5198/10000, Prediction Accuracy = 59.06400000000001%, Loss = 0.8475947141647339
Epoch: 5198, Batch Gradient Norm: 31.55701524586248
Epoch: 5198, Batch Gradient Norm after: 22.360679789431586
Epoch 5199/10000, Prediction Accuracy = 59.11600000000001%, Loss = 0.8404806733131409
Epoch: 5199, Batch Gradient Norm: 34.402264100939945
Epoch: 5199, Batch Gradient Norm after: 22.360680118855914
Epoch 5200/10000, Prediction Accuracy = 59.072%, Loss = 0.8474633097648621
Epoch: 5200, Batch Gradient Norm: 31.553851232941064
Epoch: 5200, Batch Gradient Norm after: 22.360678742723827
Epoch 5201/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8403643131256103
Epoch: 5201, Batch Gradient Norm: 34.39798299050773
Epoch: 5201, Batch Gradient Norm after: 22.360679521697165
Epoch 5202/10000, Prediction Accuracy = 59.084%, Loss = 0.847333574295044
Epoch: 5202, Batch Gradient Norm: 31.549766493506567
Epoch: 5202, Batch Gradient Norm after: 22.360679703112304
Epoch 5203/10000, Prediction Accuracy = 59.11999999999999%, Loss = 0.8402452588081359
Epoch: 5203, Batch Gradient Norm: 34.39219062513154
Epoch: 5203, Batch Gradient Norm after: 22.360677975075266
Epoch 5204/10000, Prediction Accuracy = 59.084%, Loss = 0.8472057223320008
Epoch: 5204, Batch Gradient Norm: 31.547678969123748
Epoch: 5204, Batch Gradient Norm after: 22.36067950893617
Epoch 5205/10000, Prediction Accuracy = 59.117999999999995%, Loss = 0.8401275992393493
Epoch: 5205, Batch Gradient Norm: 34.384851206288616
Epoch: 5205, Batch Gradient Norm after: 22.360678953579992
Epoch 5206/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.847081434726715
Epoch: 5206, Batch Gradient Norm: 31.542458973984047
Epoch: 5206, Batch Gradient Norm after: 22.36068056358667
Epoch 5207/10000, Prediction Accuracy = 59.124%, Loss = 0.8400084495544433
Epoch: 5207, Batch Gradient Norm: 34.37969574418528
Epoch: 5207, Batch Gradient Norm after: 22.360679759723148
Epoch 5208/10000, Prediction Accuracy = 59.092%, Loss = 0.8469536781311036
Epoch: 5208, Batch Gradient Norm: 31.541735452812322
Epoch: 5208, Batch Gradient Norm after: 22.360678712822846
Epoch 5209/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8398935914039611
Epoch: 5209, Batch Gradient Norm: 34.37314052967261
Epoch: 5209, Batch Gradient Norm after: 22.360677479017436
Epoch 5210/10000, Prediction Accuracy = 59.08399999999999%, Loss = 0.8468281865119934
Epoch: 5210, Batch Gradient Norm: 31.538819844888348
Epoch: 5210, Batch Gradient Norm after: 22.36067891847514
Epoch 5211/10000, Prediction Accuracy = 59.11800000000001%, Loss = 0.8397814273834229
Epoch: 5211, Batch Gradient Norm: 34.36481196757306
Epoch: 5211, Batch Gradient Norm after: 22.3606765061777
Epoch 5212/10000, Prediction Accuracy = 59.08200000000001%, Loss = 0.8467013478279114
Epoch: 5212, Batch Gradient Norm: 31.536309077878567
Epoch: 5212, Batch Gradient Norm after: 22.360678296310205
Epoch 5213/10000, Prediction Accuracy = 59.12800000000001%, Loss = 0.8396639943122863
Epoch: 5213, Batch Gradient Norm: 34.36182099096371
Epoch: 5213, Batch Gradient Norm after: 22.36067617052761
Epoch 5214/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.846574854850769
Epoch: 5214, Batch Gradient Norm: 31.532006683203083
Epoch: 5214, Batch Gradient Norm after: 22.36067727824692
Epoch 5215/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8395421981811524
Epoch: 5215, Batch Gradient Norm: 34.35751490719774
Epoch: 5215, Batch Gradient Norm after: 22.36067771532034
Epoch 5216/10000, Prediction Accuracy = 59.088%, Loss = 0.8464477896690369
Epoch: 5216, Batch Gradient Norm: 31.528723564276458
Epoch: 5216, Batch Gradient Norm after: 22.36067962864014
Epoch 5217/10000, Prediction Accuracy = 59.132000000000005%, Loss = 0.8394170284271241
Epoch: 5217, Batch Gradient Norm: 34.353531326243115
Epoch: 5217, Batch Gradient Norm after: 22.36067892589357
Epoch 5218/10000, Prediction Accuracy = 59.098%, Loss = 0.8463262319564819
Epoch: 5218, Batch Gradient Norm: 31.525378234453676
Epoch: 5218, Batch Gradient Norm after: 22.36067859366368
Epoch 5219/10000, Prediction Accuracy = 59.12199999999999%, Loss = 0.8392998218536377
Epoch: 5219, Batch Gradient Norm: 34.34879490319052
Epoch: 5219, Batch Gradient Norm after: 22.360679545737803
Epoch 5220/10000, Prediction Accuracy = 59.092%, Loss = 0.8462058186531067
Epoch: 5220, Batch Gradient Norm: 31.52398942644378
Epoch: 5220, Batch Gradient Norm after: 22.360677261337756
Epoch 5221/10000, Prediction Accuracy = 59.126%, Loss = 0.8391901016235351
Epoch: 5221, Batch Gradient Norm: 34.34474438344625
Epoch: 5221, Batch Gradient Norm after: 22.36067637487511
Epoch 5222/10000, Prediction Accuracy = 59.1%, Loss = 0.8460769295692444
Epoch: 5222, Batch Gradient Norm: 31.52181309151179
Epoch: 5222, Batch Gradient Norm after: 22.360679061116016
Epoch 5223/10000, Prediction Accuracy = 59.124%, Loss = 0.8390721559524537
Epoch: 5223, Batch Gradient Norm: 34.33522619369147
Epoch: 5223, Batch Gradient Norm after: 22.360676810541165
Epoch 5224/10000, Prediction Accuracy = 59.102%, Loss = 0.8459354639053345
Epoch: 5224, Batch Gradient Norm: 31.52053328399174
Epoch: 5224, Batch Gradient Norm after: 22.360678752444045
Epoch 5225/10000, Prediction Accuracy = 59.126%, Loss = 0.8389547944068909
Epoch: 5225, Batch Gradient Norm: 34.32547960208684
Epoch: 5225, Batch Gradient Norm after: 22.36067862607006
Epoch 5226/10000, Prediction Accuracy = 59.104%, Loss = 0.8458052039146423
Epoch: 5226, Batch Gradient Norm: 31.52085460463957
Epoch: 5226, Batch Gradient Norm after: 22.36067982706452
Epoch 5227/10000, Prediction Accuracy = 59.13599999999999%, Loss = 0.8388414859771729
Epoch: 5227, Batch Gradient Norm: 34.318172701151546
Epoch: 5227, Batch Gradient Norm after: 22.360679347485323
Epoch 5228/10000, Prediction Accuracy = 59.09599999999999%, Loss = 0.8456764221191406
Epoch: 5228, Batch Gradient Norm: 31.520498471435566
Epoch: 5228, Batch Gradient Norm after: 22.360679647270423
Epoch 5229/10000, Prediction Accuracy = 59.14399999999999%, Loss = 0.8387332081794738
Epoch: 5229, Batch Gradient Norm: 34.309414773356025
Epoch: 5229, Batch Gradient Norm after: 22.360679889467264
Epoch 5230/10000, Prediction Accuracy = 59.104%, Loss = 0.8455484747886658
Epoch: 5230, Batch Gradient Norm: 31.52108489564409
Epoch: 5230, Batch Gradient Norm after: 22.360677825096165
Epoch 5231/10000, Prediction Accuracy = 59.156000000000006%, Loss = 0.8386226654052734
Epoch: 5231, Batch Gradient Norm: 34.301771815431024
Epoch: 5231, Batch Gradient Norm after: 22.360678744879685
Epoch 5232/10000, Prediction Accuracy = 59.1%, Loss = 0.8454147577285767
Epoch: 5232, Batch Gradient Norm: 31.51793301350019
Epoch: 5232, Batch Gradient Norm after: 22.360680874511356
Epoch 5233/10000, Prediction Accuracy = 59.15%, Loss = 0.83850017786026
Epoch: 5233, Batch Gradient Norm: 34.29912599666814
Epoch: 5233, Batch Gradient Norm after: 22.36067751275582
Epoch 5234/10000, Prediction Accuracy = 59.10600000000001%, Loss = 0.8452945351600647
Epoch: 5234, Batch Gradient Norm: 31.51426686775102
Epoch: 5234, Batch Gradient Norm after: 22.36067768940705
Epoch 5235/10000, Prediction Accuracy = 59.15%, Loss = 0.838378369808197
Epoch: 5235, Batch Gradient Norm: 34.295768144171085
Epoch: 5235, Batch Gradient Norm after: 22.360677243458777
Epoch 5236/10000, Prediction Accuracy = 59.114%, Loss = 0.8451744198799134
Epoch: 5236, Batch Gradient Norm: 31.508868881531846
Epoch: 5236, Batch Gradient Norm after: 22.360679875553277
Epoch 5237/10000, Prediction Accuracy = 59.152%, Loss = 0.8382619738578796
Epoch: 5237, Batch Gradient Norm: 34.29166921590695
Epoch: 5237, Batch Gradient Norm after: 22.360678410181915
Epoch 5238/10000, Prediction Accuracy = 59.116%, Loss = 0.8450593948364258
Epoch: 5238, Batch Gradient Norm: 31.505941721579745
Epoch: 5238, Batch Gradient Norm after: 22.36067846919445
Epoch 5239/10000, Prediction Accuracy = 59.166%, Loss = 0.8381438732147217
Epoch: 5239, Batch Gradient Norm: 34.28888689266727
Epoch: 5239, Batch Gradient Norm after: 22.360679828246393
Epoch 5240/10000, Prediction Accuracy = 59.117999999999995%, Loss = 0.8449425578117371
Epoch: 5240, Batch Gradient Norm: 31.50187256603775
Epoch: 5240, Batch Gradient Norm after: 22.360679236046778
Epoch 5241/10000, Prediction Accuracy = 59.174%, Loss = 0.8380226254463196
Epoch: 5241, Batch Gradient Norm: 34.2866156147416
Epoch: 5241, Batch Gradient Norm after: 22.360680668363578
Epoch 5242/10000, Prediction Accuracy = 59.122%, Loss = 0.8448288559913635
Epoch: 5242, Batch Gradient Norm: 31.49540998342543
Epoch: 5242, Batch Gradient Norm after: 22.360680184096285
Epoch 5243/10000, Prediction Accuracy = 59.176%, Loss = 0.837900173664093
Epoch: 5243, Batch Gradient Norm: 34.28448116790163
Epoch: 5243, Batch Gradient Norm after: 22.360678023835025
Epoch 5244/10000, Prediction Accuracy = 59.122%, Loss = 0.844711446762085
Epoch: 5244, Batch Gradient Norm: 31.491414188094446
Epoch: 5244, Batch Gradient Norm after: 22.3606795305506
Epoch 5245/10000, Prediction Accuracy = 59.178%, Loss = 0.8377789378166198
Epoch: 5245, Batch Gradient Norm: 34.281295087070745
Epoch: 5245, Batch Gradient Norm after: 22.360680218489755
Epoch 5246/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8445888638496399
Epoch: 5246, Batch Gradient Norm: 31.486443704717292
Epoch: 5246, Batch Gradient Norm after: 22.360680840608758
Epoch 5247/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8376606106758118
Epoch: 5247, Batch Gradient Norm: 34.27686281867091
Epoch: 5247, Batch Gradient Norm after: 22.360677632200762
Epoch 5248/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8444711208343506
Epoch: 5248, Batch Gradient Norm: 31.482718998730913
Epoch: 5248, Batch Gradient Norm after: 22.360678010821054
Epoch 5249/10000, Prediction Accuracy = 59.18000000000001%, Loss = 0.8375410318374634
Epoch: 5249, Batch Gradient Norm: 34.272888744697376
Epoch: 5249, Batch Gradient Norm after: 22.360678645332136
Epoch 5250/10000, Prediction Accuracy = 59.11600000000001%, Loss = 0.8443549275398254
Epoch: 5250, Batch Gradient Norm: 31.47958795122911
Epoch: 5250, Batch Gradient Norm after: 22.36067686710675
Epoch 5251/10000, Prediction Accuracy = 59.186%, Loss = 0.8374205470085144
Epoch: 5251, Batch Gradient Norm: 34.26625250811119
Epoch: 5251, Batch Gradient Norm after: 22.360679466199816
Epoch 5252/10000, Prediction Accuracy = 59.126%, Loss = 0.844232702255249
Epoch: 5252, Batch Gradient Norm: 31.477556209205492
Epoch: 5252, Batch Gradient Norm after: 22.360679366552976
Epoch 5253/10000, Prediction Accuracy = 59.194%, Loss = 0.837308657169342
Epoch: 5253, Batch Gradient Norm: 34.26074689153225
Epoch: 5253, Batch Gradient Norm after: 22.3606802553458
Epoch 5254/10000, Prediction Accuracy = 59.122%, Loss = 0.844105064868927
Epoch: 5254, Batch Gradient Norm: 31.473126891920653
Epoch: 5254, Batch Gradient Norm after: 22.360678014951795
Epoch 5255/10000, Prediction Accuracy = 59.188%, Loss = 0.8371879935264588
Epoch: 5255, Batch Gradient Norm: 34.25240810238797
Epoch: 5255, Batch Gradient Norm after: 22.360676442172785
Epoch 5256/10000, Prediction Accuracy = 59.132000000000005%, Loss = 0.8439700961112976
Epoch: 5256, Batch Gradient Norm: 31.471467202069267
Epoch: 5256, Batch Gradient Norm after: 22.360678645716238
Epoch 5257/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8370663404464722
Epoch: 5257, Batch Gradient Norm: 34.246347989888235
Epoch: 5257, Batch Gradient Norm after: 22.3606776314609
Epoch 5258/10000, Prediction Accuracy = 59.128%, Loss = 0.843845808506012
Epoch: 5258, Batch Gradient Norm: 31.46943330428671
Epoch: 5258, Batch Gradient Norm after: 22.36068061101744
Epoch 5259/10000, Prediction Accuracy = 59.184000000000005%, Loss = 0.836947500705719
Epoch: 5259, Batch Gradient Norm: 34.23895072073487
Epoch: 5259, Batch Gradient Norm after: 22.3606772476812
Epoch 5260/10000, Prediction Accuracy = 59.13000000000001%, Loss = 0.8437263250350953
Epoch: 5260, Batch Gradient Norm: 31.465392144080532
Epoch: 5260, Batch Gradient Norm after: 22.360678659296227
Epoch 5261/10000, Prediction Accuracy = 59.184000000000005%, Loss = 0.836836314201355
Epoch: 5261, Batch Gradient Norm: 34.23469581879203
Epoch: 5261, Batch Gradient Norm after: 22.36067598910499
Epoch 5262/10000, Prediction Accuracy = 59.129999999999995%, Loss = 0.843611192703247
Epoch: 5262, Batch Gradient Norm: 31.46126186870421
Epoch: 5262, Batch Gradient Norm after: 22.360679982511574
Epoch 5263/10000, Prediction Accuracy = 59.18599999999999%, Loss = 0.8367209434509277
Epoch: 5263, Batch Gradient Norm: 34.229220333445525
Epoch: 5263, Batch Gradient Norm after: 22.36067784504528
Epoch 5264/10000, Prediction Accuracy = 59.136%, Loss = 0.843492078781128
Epoch: 5264, Batch Gradient Norm: 31.457593365989315
Epoch: 5264, Batch Gradient Norm after: 22.360679342246772
Epoch 5265/10000, Prediction Accuracy = 59.186%, Loss = 0.8366027474403381
Epoch: 5265, Batch Gradient Norm: 34.224391606147265
Epoch: 5265, Batch Gradient Norm after: 22.360679718809752
Epoch 5266/10000, Prediction Accuracy = 59.134%, Loss = 0.8433693408966064
Epoch: 5266, Batch Gradient Norm: 31.452485527356597
Epoch: 5266, Batch Gradient Norm after: 22.360678974572632
Epoch 5267/10000, Prediction Accuracy = 59.178%, Loss = 0.8364773869514466
Epoch: 5267, Batch Gradient Norm: 34.220806045691084
Epoch: 5267, Batch Gradient Norm after: 22.360677869740076
Epoch 5268/10000, Prediction Accuracy = 59.136%, Loss = 0.8432446599006653
Epoch: 5268, Batch Gradient Norm: 31.448352661777804
Epoch: 5268, Batch Gradient Norm after: 22.360680063172598
Epoch 5269/10000, Prediction Accuracy = 59.178%, Loss = 0.836355984210968
Epoch: 5269, Batch Gradient Norm: 34.217655051640214
Epoch: 5269, Batch Gradient Norm after: 22.36067750235327
Epoch 5270/10000, Prediction Accuracy = 59.146%, Loss = 0.8431384325027466
Epoch: 5270, Batch Gradient Norm: 31.442515863312323
Epoch: 5270, Batch Gradient Norm after: 22.360679315038016
Epoch 5271/10000, Prediction Accuracy = 59.19200000000001%, Loss = 0.8362441897392273
Epoch: 5271, Batch Gradient Norm: 34.21508820961222
Epoch: 5271, Batch Gradient Norm after: 22.360679482367107
Epoch 5272/10000, Prediction Accuracy = 59.146%, Loss = 0.8430316925048829
Epoch: 5272, Batch Gradient Norm: 31.43736717548746
Epoch: 5272, Batch Gradient Norm after: 22.360679444871337
Epoch 5273/10000, Prediction Accuracy = 59.182%, Loss = 0.8361325860023499
Epoch: 5273, Batch Gradient Norm: 34.213497136403895
Epoch: 5273, Batch Gradient Norm after: 22.36067790337116
Epoch 5274/10000, Prediction Accuracy = 59.146%, Loss = 0.8429015159606934
Epoch: 5274, Batch Gradient Norm: 31.433061965984773
Epoch: 5274, Batch Gradient Norm after: 22.36067844207494
Epoch 5275/10000, Prediction Accuracy = 59.188%, Loss = 0.8360077738761902
Epoch: 5275, Batch Gradient Norm: 34.20959996004877
Epoch: 5275, Batch Gradient Norm after: 22.360681271725838
Epoch 5276/10000, Prediction Accuracy = 59.144000000000005%, Loss = 0.8427785515785218
Epoch: 5276, Batch Gradient Norm: 31.428280788434684
Epoch: 5276, Batch Gradient Norm after: 22.360681548788875
Epoch 5277/10000, Prediction Accuracy = 59.19%, Loss = 0.83588125705719
Epoch: 5277, Batch Gradient Norm: 34.20433967475959
Epoch: 5277, Batch Gradient Norm after: 22.360679226435888
Epoch 5278/10000, Prediction Accuracy = 59.152%, Loss = 0.8426603317260742
Epoch: 5278, Batch Gradient Norm: 31.423450385675633
Epoch: 5278, Batch Gradient Norm after: 22.360680047265646
Epoch 5279/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8357656955718994
Epoch: 5279, Batch Gradient Norm: 34.20136160674019
Epoch: 5279, Batch Gradient Norm after: 22.36067984852092
Epoch 5280/10000, Prediction Accuracy = 59.169999999999995%, Loss = 0.842548668384552
Epoch: 5280, Batch Gradient Norm: 31.41988133947261
Epoch: 5280, Batch Gradient Norm after: 22.360680443094413
Epoch 5281/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8356554985046387
Epoch: 5281, Batch Gradient Norm: 34.19810333658479
Epoch: 5281, Batch Gradient Norm after: 22.360679388620774
Epoch 5282/10000, Prediction Accuracy = 59.172000000000004%, Loss = 0.8424256324768067
Epoch: 5282, Batch Gradient Norm: 31.41551084263337
Epoch: 5282, Batch Gradient Norm after: 22.360682232812984
Epoch 5283/10000, Prediction Accuracy = 59.198%, Loss = 0.835538911819458
Epoch: 5283, Batch Gradient Norm: 34.19250906943189
Epoch: 5283, Batch Gradient Norm after: 22.36067856596563
Epoch 5284/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8423026323318481
Epoch: 5284, Batch Gradient Norm: 31.41100317937809
Epoch: 5284, Batch Gradient Norm after: 22.360677677244865
Epoch 5285/10000, Prediction Accuracy = 59.20200000000001%, Loss = 0.835413646697998
Epoch: 5285, Batch Gradient Norm: 34.18932756570084
Epoch: 5285, Batch Gradient Norm after: 22.360680517164692
Epoch 5286/10000, Prediction Accuracy = 59.152%, Loss = 0.8421754240989685
Epoch: 5286, Batch Gradient Norm: 31.406941027016103
Epoch: 5286, Batch Gradient Norm after: 22.360677095917993
Epoch 5287/10000, Prediction Accuracy = 59.20399999999999%, Loss = 0.8352968215942382
Epoch: 5287, Batch Gradient Norm: 34.1808810350004
Epoch: 5287, Batch Gradient Norm after: 22.360680567561722
Epoch 5288/10000, Prediction Accuracy = 59.164%, Loss = 0.8420534014701844
Epoch: 5288, Batch Gradient Norm: 31.405455248619738
Epoch: 5288, Batch Gradient Norm after: 22.360679274299724
Epoch 5289/10000, Prediction Accuracy = 59.21600000000001%, Loss = 0.835183608531952
Epoch: 5289, Batch Gradient Norm: 34.17335400544408
Epoch: 5289, Batch Gradient Norm after: 22.360680565160028
Epoch 5290/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8419370412826538
Epoch: 5290, Batch Gradient Norm: 31.404115335920242
Epoch: 5290, Batch Gradient Norm after: 22.360680009160248
Epoch 5291/10000, Prediction Accuracy = 59.212%, Loss = 0.8350815176963806
Epoch: 5291, Batch Gradient Norm: 34.16724910567381
Epoch: 5291, Batch Gradient Norm after: 22.360679128900767
Epoch 5292/10000, Prediction Accuracy = 59.172000000000004%, Loss = 0.841818344593048
Epoch: 5292, Batch Gradient Norm: 31.401308626273703
Epoch: 5292, Batch Gradient Norm after: 22.360678684111267
Epoch 5293/10000, Prediction Accuracy = 59.212%, Loss = 0.8349681854248047
Epoch: 5293, Batch Gradient Norm: 34.158102342471935
Epoch: 5293, Batch Gradient Norm after: 22.360679273900814
Epoch 5294/10000, Prediction Accuracy = 59.174%, Loss = 0.8416837334632874
Epoch: 5294, Batch Gradient Norm: 31.40229414296124
Epoch: 5294, Batch Gradient Norm after: 22.360679579835935
Epoch 5295/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8348469257354736
Epoch: 5295, Batch Gradient Norm: 34.15030010777402
Epoch: 5295, Batch Gradient Norm after: 22.360678007789957
Epoch 5296/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.841538381576538
Epoch: 5296, Batch Gradient Norm: 31.401691800337222
Epoch: 5296, Batch Gradient Norm after: 22.36067891523307
Epoch 5297/10000, Prediction Accuracy = 59.20400000000001%, Loss = 0.8347290277481079
Epoch: 5297, Batch Gradient Norm: 34.14008239931142
Epoch: 5297, Batch Gradient Norm after: 22.36067745383997
Epoch 5298/10000, Prediction Accuracy = 59.176%, Loss = 0.8414073824882508
Epoch: 5298, Batch Gradient Norm: 31.40222922669586
Epoch: 5298, Batch Gradient Norm after: 22.360678968978135
Epoch 5299/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8346235990524292
Epoch: 5299, Batch Gradient Norm: 34.13094941123392
Epoch: 5299, Batch Gradient Norm after: 22.36067843443201
Epoch 5300/10000, Prediction Accuracy = 59.178%, Loss = 0.8412876486778259
Epoch: 5300, Batch Gradient Norm: 31.400453195879813
Epoch: 5300, Batch Gradient Norm after: 22.360680511952648
Epoch 5301/10000, Prediction Accuracy = 59.212%, Loss = 0.834525191783905
Epoch: 5301, Batch Gradient Norm: 34.12432028965755
Epoch: 5301, Batch Gradient Norm after: 22.360679707140807
Epoch 5302/10000, Prediction Accuracy = 59.158%, Loss = 0.8411722540855407
Epoch: 5302, Batch Gradient Norm: 31.39949613169868
Epoch: 5302, Batch Gradient Norm after: 22.360678980208895
Epoch 5303/10000, Prediction Accuracy = 59.206%, Loss = 0.8344170570373535
Epoch: 5303, Batch Gradient Norm: 34.11242272803786
Epoch: 5303, Batch Gradient Norm after: 22.36067781966657
Epoch 5304/10000, Prediction Accuracy = 59.176%, Loss = 0.8410257458686828
Epoch: 5304, Batch Gradient Norm: 31.400028746951538
Epoch: 5304, Batch Gradient Norm after: 22.360676852021697
Epoch 5305/10000, Prediction Accuracy = 59.214%, Loss = 0.8342987656593323
Epoch: 5305, Batch Gradient Norm: 34.10504173666225
Epoch: 5305, Batch Gradient Norm after: 22.360678281151063
Epoch 5306/10000, Prediction Accuracy = 59.17600000000001%, Loss = 0.8408870816230773
Epoch: 5306, Batch Gradient Norm: 31.39878326631155
Epoch: 5306, Batch Gradient Norm after: 22.3606778283713
Epoch 5307/10000, Prediction Accuracy = 59.20799999999999%, Loss = 0.8341837525367737
Epoch: 5307, Batch Gradient Norm: 34.0956930295628
Epoch: 5307, Batch Gradient Norm after: 22.360679266463563
Epoch 5308/10000, Prediction Accuracy = 59.182%, Loss = 0.8407660126686096
Epoch: 5308, Batch Gradient Norm: 31.399515021632574
Epoch: 5308, Batch Gradient Norm after: 22.36067863382633
Epoch 5309/10000, Prediction Accuracy = 59.222%, Loss = 0.8340729594230651
Epoch: 5309, Batch Gradient Norm: 34.08596991500638
Epoch: 5309, Batch Gradient Norm after: 22.360678405731242
Epoch 5310/10000, Prediction Accuracy = 59.18000000000001%, Loss = 0.8406473755836487
Epoch: 5310, Batch Gradient Norm: 31.40127231569523
Epoch: 5310, Batch Gradient Norm after: 22.36067806877397
Epoch 5311/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.8339787244796752
Epoch: 5311, Batch Gradient Norm: 34.07602856566886
Epoch: 5311, Batch Gradient Norm after: 22.360680136541834
Epoch 5312/10000, Prediction Accuracy = 59.17%, Loss = 0.840513813495636
Epoch: 5312, Batch Gradient Norm: 31.401719297701863
Epoch: 5312, Batch Gradient Norm after: 22.360679629634664
Epoch 5313/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.8338705420494079
Epoch: 5313, Batch Gradient Norm: 34.06467992389442
Epoch: 5313, Batch Gradient Norm after: 22.3606804989468
Epoch 5314/10000, Prediction Accuracy = 59.174%, Loss = 0.8403735518455505
Epoch: 5314, Batch Gradient Norm: 31.40195496784166
Epoch: 5314, Batch Gradient Norm after: 22.360679587826954
Epoch 5315/10000, Prediction Accuracy = 59.217999999999996%, Loss = 0.8337623357772828
Epoch: 5315, Batch Gradient Norm: 34.052768581078844
Epoch: 5315, Batch Gradient Norm after: 22.360679035309136
Epoch 5316/10000, Prediction Accuracy = 59.184000000000005%, Loss = 0.8402296543121338
Epoch: 5316, Batch Gradient Norm: 31.401375451473008
Epoch: 5316, Batch Gradient Norm after: 22.360678931412757
Epoch 5317/10000, Prediction Accuracy = 59.214%, Loss = 0.8336454153060913
Epoch: 5317, Batch Gradient Norm: 34.04619635938338
Epoch: 5317, Batch Gradient Norm after: 22.36067874539433
Epoch 5318/10000, Prediction Accuracy = 59.188%, Loss = 0.840097713470459
Epoch: 5318, Batch Gradient Norm: 31.40121501042166
Epoch: 5318, Batch Gradient Norm after: 22.36067983538085
Epoch 5319/10000, Prediction Accuracy = 59.227999999999994%, Loss = 0.8335337042808533
Epoch: 5319, Batch Gradient Norm: 34.039793467523495
Epoch: 5319, Batch Gradient Norm after: 22.360677837045614
Epoch 5320/10000, Prediction Accuracy = 59.19599999999999%, Loss = 0.8399844288825988
Epoch: 5320, Batch Gradient Norm: 31.398901260397675
Epoch: 5320, Batch Gradient Norm after: 22.360679583138793
Epoch 5321/10000, Prediction Accuracy = 59.22800000000001%, Loss = 0.833432650566101
Epoch: 5321, Batch Gradient Norm: 34.034593495954404
Epoch: 5321, Batch Gradient Norm after: 22.360679304264924
Epoch 5322/10000, Prediction Accuracy = 59.182%, Loss = 0.8398728728294372
Epoch: 5322, Batch Gradient Norm: 31.398044895329015
Epoch: 5322, Batch Gradient Norm after: 22.360681017051633
Epoch 5323/10000, Prediction Accuracy = 59.218%, Loss = 0.8333248257637024
Epoch: 5323, Batch Gradient Norm: 34.028738712380445
Epoch: 5323, Batch Gradient Norm after: 22.360678799771826
Epoch 5324/10000, Prediction Accuracy = 59.194%, Loss = 0.8397479176521301
Epoch: 5324, Batch Gradient Norm: 31.395253913808624
Epoch: 5324, Batch Gradient Norm after: 22.360679430535985
Epoch 5325/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.833201003074646
Epoch: 5325, Batch Gradient Norm: 34.024267986277756
Epoch: 5325, Batch Gradient Norm after: 22.360678064604183
Epoch 5326/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8396138548851013
Epoch: 5326, Batch Gradient Norm: 31.392471537458416
Epoch: 5326, Batch Gradient Norm after: 22.36067964903937
Epoch 5327/10000, Prediction Accuracy = 59.214%, Loss = 0.8330789804458618
Epoch: 5327, Batch Gradient Norm: 34.02167855334246
Epoch: 5327, Batch Gradient Norm after: 22.360681239381737
Epoch 5328/10000, Prediction Accuracy = 59.20400000000001%, Loss = 0.8394943475723267
Epoch: 5328, Batch Gradient Norm: 31.390300346676746
Epoch: 5328, Batch Gradient Norm after: 22.36067996923778
Epoch 5329/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.8329637765884399
Epoch: 5329, Batch Gradient Norm: 34.01451450708349
Epoch: 5329, Batch Gradient Norm after: 22.360679935189538
Epoch 5330/10000, Prediction Accuracy = 59.212%, Loss = 0.8393783211708069
Epoch: 5330, Batch Gradient Norm: 31.38909933861052
Epoch: 5330, Batch Gradient Norm after: 22.360681327186356
Epoch 5331/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.8328592419624329
Epoch: 5331, Batch Gradient Norm: 34.008549500326524
Epoch: 5331, Batch Gradient Norm after: 22.360679507097938
Epoch 5332/10000, Prediction Accuracy = 59.202%, Loss = 0.8392633438110352
Epoch: 5332, Batch Gradient Norm: 31.388945412633912
Epoch: 5332, Batch Gradient Norm after: 22.36068082906825
Epoch 5333/10000, Prediction Accuracy = 59.222%, Loss = 0.8327569961547852
Epoch: 5333, Batch Gradient Norm: 34.000684347844924
Epoch: 5333, Batch Gradient Norm after: 22.360680017378925
Epoch 5334/10000, Prediction Accuracy = 59.214%, Loss = 0.839134681224823
Epoch: 5334, Batch Gradient Norm: 31.388121491753836
Epoch: 5334, Batch Gradient Norm after: 22.36067851954184
Epoch 5335/10000, Prediction Accuracy = 59.218%, Loss = 0.8326409220695495
Epoch: 5335, Batch Gradient Norm: 33.99341010950213
Epoch: 5335, Batch Gradient Norm after: 22.360681451891253
Epoch 5336/10000, Prediction Accuracy = 59.214%, Loss = 0.8389965057373047
Epoch: 5336, Batch Gradient Norm: 31.38378412857442
Epoch: 5336, Batch Gradient Norm after: 22.360679501267143
Epoch 5337/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.8325150847434998
Epoch: 5337, Batch Gradient Norm: 33.989953230834196
Epoch: 5337, Batch Gradient Norm after: 22.360679234264385
Epoch 5338/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8388913989067077
Epoch: 5338, Batch Gradient Norm: 31.380515435882753
Epoch: 5338, Batch Gradient Norm after: 22.360678185079454
Epoch 5339/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.832407534122467
Epoch: 5339, Batch Gradient Norm: 33.98668208449943
Epoch: 5339, Batch Gradient Norm after: 22.36068133651224
Epoch 5340/10000, Prediction Accuracy = 59.21600000000001%, Loss = 0.8387824296951294
Epoch: 5340, Batch Gradient Norm: 31.377212706901567
Epoch: 5340, Batch Gradient Norm after: 22.360678483903136
Epoch 5341/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.8322973847389221
Epoch: 5341, Batch Gradient Norm: 33.98376735644994
Epoch: 5341, Batch Gradient Norm after: 22.36067999114024
Epoch 5342/10000, Prediction Accuracy = 59.222%, Loss = 0.8386743545532227
Epoch: 5342, Batch Gradient Norm: 31.37099191843978
Epoch: 5342, Batch Gradient Norm after: 22.360679942125504
Epoch 5343/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.832181704044342
Epoch: 5343, Batch Gradient Norm: 33.98209795325397
Epoch: 5343, Batch Gradient Norm after: 22.360678683407333
Epoch 5344/10000, Prediction Accuracy = 59.21999999999999%, Loss = 0.8385559439659118
Epoch: 5344, Batch Gradient Norm: 31.36626285878815
Epoch: 5344, Batch Gradient Norm after: 22.360678953887728
Epoch 5345/10000, Prediction Accuracy = 59.227999999999994%, Loss = 0.8320538640022278
Epoch: 5345, Batch Gradient Norm: 33.97879187177415
Epoch: 5345, Batch Gradient Norm after: 22.36067951290232
Epoch 5346/10000, Prediction Accuracy = 59.226%, Loss = 0.8384278297424317
Epoch: 5346, Batch Gradient Norm: 31.36205046975702
Epoch: 5346, Batch Gradient Norm after: 22.360678966488443
Epoch 5347/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8319307088851928
Epoch: 5347, Batch Gradient Norm: 33.977479213319725
Epoch: 5347, Batch Gradient Norm after: 22.360681922808407
Epoch 5348/10000, Prediction Accuracy = 59.23%, Loss = 0.8383175253868103
Epoch: 5348, Batch Gradient Norm: 31.357471165071754
Epoch: 5348, Batch Gradient Norm after: 22.360678739498272
Epoch 5349/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.831816303730011
Epoch: 5349, Batch Gradient Norm: 33.97415439822773
Epoch: 5349, Batch Gradient Norm after: 22.36068029879047
Epoch 5350/10000, Prediction Accuracy = 59.23199999999999%, Loss = 0.8382107853889466
Epoch: 5350, Batch Gradient Norm: 31.354198097599877
Epoch: 5350, Batch Gradient Norm after: 22.360678134419746
Epoch 5351/10000, Prediction Accuracy = 59.23199999999999%, Loss = 0.8317116737365723
Epoch: 5351, Batch Gradient Norm: 33.9696840221819
Epoch: 5351, Batch Gradient Norm after: 22.360678082107942
Epoch 5352/10000, Prediction Accuracy = 59.218%, Loss = 0.8381059169769287
Epoch: 5352, Batch Gradient Norm: 31.349594330545695
Epoch: 5352, Batch Gradient Norm after: 22.360676134453133
Epoch 5353/10000, Prediction Accuracy = 59.226%, Loss = 0.8315963387489319
Epoch: 5353, Batch Gradient Norm: 33.96502749095487
Epoch: 5353, Batch Gradient Norm after: 22.360680247250073
Epoch 5354/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.8379854202270508
Epoch: 5354, Batch Gradient Norm: 31.346738278101135
Epoch: 5354, Batch Gradient Norm after: 22.3606789418626
Epoch 5355/10000, Prediction Accuracy = 59.23%, Loss = 0.8314807534217834
Epoch: 5355, Batch Gradient Norm: 33.962049539892206
Epoch: 5355, Batch Gradient Norm after: 22.360675943293696
Epoch 5356/10000, Prediction Accuracy = 59.242%, Loss = 0.8378472924232483
Epoch: 5356, Batch Gradient Norm: 31.342776052097527
Epoch: 5356, Batch Gradient Norm after: 22.36067817999568
Epoch 5357/10000, Prediction Accuracy = 59.233999999999995%, Loss = 0.8313538789749145
Epoch: 5357, Batch Gradient Norm: 33.958654656186326
Epoch: 5357, Batch Gradient Norm after: 22.360679125748153
Epoch 5358/10000, Prediction Accuracy = 59.226%, Loss = 0.8377292156219482
Epoch: 5358, Batch Gradient Norm: 31.339432635143634
Epoch: 5358, Batch Gradient Norm after: 22.36067703170574
Epoch 5359/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8312358260154724
Epoch: 5359, Batch Gradient Norm: 33.955772728854285
Epoch: 5359, Batch Gradient Norm after: 22.360678565758292
Epoch 5360/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.8376267075538635
Epoch: 5360, Batch Gradient Norm: 31.33493388848297
Epoch: 5360, Batch Gradient Norm after: 22.360677899463866
Epoch 5361/10000, Prediction Accuracy = 59.23%, Loss = 0.8311328530311585
Epoch: 5361, Batch Gradient Norm: 33.95095761975513
Epoch: 5361, Batch Gradient Norm after: 22.36067856608121
Epoch 5362/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.837526285648346
Epoch: 5362, Batch Gradient Norm: 31.331606193947092
Epoch: 5362, Batch Gradient Norm after: 22.36067876087531
Epoch 5363/10000, Prediction Accuracy = 59.23%, Loss = 0.8310296773910523
Epoch: 5363, Batch Gradient Norm: 33.947388338620875
Epoch: 5363, Batch Gradient Norm after: 22.360677989640845
Epoch 5364/10000, Prediction Accuracy = 59.234%, Loss = 0.837399959564209
Epoch: 5364, Batch Gradient Norm: 31.32754868217571
Epoch: 5364, Batch Gradient Norm after: 22.36067861018267
Epoch 5365/10000, Prediction Accuracy = 59.238%, Loss = 0.8309054851531983
Epoch: 5365, Batch Gradient Norm: 33.945847138672235
Epoch: 5365, Batch Gradient Norm after: 22.360679479560094
Epoch 5366/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.8372841358184815
Epoch: 5366, Batch Gradient Norm: 31.320699133261655
Epoch: 5366, Batch Gradient Norm after: 22.360677378422107
Epoch 5367/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8307826519012451
Epoch: 5367, Batch Gradient Norm: 33.94319056414243
Epoch: 5367, Batch Gradient Norm after: 22.36067789619514
Epoch 5368/10000, Prediction Accuracy = 59.254%, Loss = 0.8371650218963623
Epoch: 5368, Batch Gradient Norm: 31.31822505519589
Epoch: 5368, Batch Gradient Norm after: 22.360678428372427
Epoch 5369/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8306684017181396
Epoch: 5369, Batch Gradient Norm: 33.94056536563787
Epoch: 5369, Batch Gradient Norm after: 22.36067729753953
Epoch 5370/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.8370562314987182
Epoch: 5370, Batch Gradient Norm: 31.31281311235663
Epoch: 5370, Batch Gradient Norm after: 22.36067846846183
Epoch 5371/10000, Prediction Accuracy = 59.246%, Loss = 0.8305538535118103
Epoch: 5371, Batch Gradient Norm: 33.93725344216629
Epoch: 5371, Batch Gradient Norm after: 22.36067960064316
Epoch 5372/10000, Prediction Accuracy = 59.25%, Loss = 0.8369507670402527
Epoch: 5372, Batch Gradient Norm: 31.31033979785926
Epoch: 5372, Batch Gradient Norm after: 22.360677029531193
Epoch 5373/10000, Prediction Accuracy = 59.226%, Loss = 0.8304400205612182
Epoch: 5373, Batch Gradient Norm: 33.93660666817269
Epoch: 5373, Batch Gradient Norm after: 22.36068124177325
Epoch 5374/10000, Prediction Accuracy = 59.25%, Loss = 0.8368363380432129
Epoch: 5374, Batch Gradient Norm: 31.30431300305434
Epoch: 5374, Batch Gradient Norm after: 22.360680438180886
Epoch 5375/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.8303183317184448
Epoch: 5375, Batch Gradient Norm: 33.93469362917954
Epoch: 5375, Batch Gradient Norm after: 22.36067865795993
Epoch 5376/10000, Prediction Accuracy = 59.266%, Loss = 0.8367165327072144
Epoch: 5376, Batch Gradient Norm: 31.301326603289198
Epoch: 5376, Batch Gradient Norm after: 22.360679159831793
Epoch 5377/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.8302000641822815
Epoch: 5377, Batch Gradient Norm: 33.93118924860355
Epoch: 5377, Batch Gradient Norm after: 22.3606798812693
Epoch 5378/10000, Prediction Accuracy = 59.274%, Loss = 0.8366005301475525
Epoch: 5378, Batch Gradient Norm: 31.297492019170463
Epoch: 5378, Batch Gradient Norm after: 22.360680165875575
Epoch 5379/10000, Prediction Accuracy = 59.254%, Loss = 0.8300871253013611
Epoch: 5379, Batch Gradient Norm: 33.930185214230235
Epoch: 5379, Batch Gradient Norm after: 22.360679207885457
Epoch 5380/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.8364880084991455
Epoch: 5380, Batch Gradient Norm: 31.29382415633943
Epoch: 5380, Batch Gradient Norm after: 22.360677632545315
Epoch 5381/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.8299793243408203
Epoch: 5381, Batch Gradient Norm: 33.92624278482003
Epoch: 5381, Batch Gradient Norm after: 22.360680744341735
Epoch 5382/10000, Prediction Accuracy = 59.274%, Loss = 0.836383318901062
Epoch: 5382, Batch Gradient Norm: 31.28962913697755
Epoch: 5382, Batch Gradient Norm after: 22.360677448028277
Epoch 5383/10000, Prediction Accuracy = 59.246%, Loss = 0.8298670172691345
Epoch: 5383, Batch Gradient Norm: 33.92376582600654
Epoch: 5383, Batch Gradient Norm after: 22.360680182234027
Epoch 5384/10000, Prediction Accuracy = 59.272000000000006%, Loss = 0.8362637042999268
Epoch: 5384, Batch Gradient Norm: 31.28462588102243
Epoch: 5384, Batch Gradient Norm after: 22.360679565486322
Epoch 5385/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.8297517657279968
Epoch: 5385, Batch Gradient Norm: 33.92053841798455
Epoch: 5385, Batch Gradient Norm after: 22.36067933967206
Epoch 5386/10000, Prediction Accuracy = 59.278%, Loss = 0.8361499428749084
Epoch: 5386, Batch Gradient Norm: 31.279740517540287
Epoch: 5386, Batch Gradient Norm after: 22.360676504330172
Epoch 5387/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.8296258449554443
Epoch: 5387, Batch Gradient Norm: 33.921076964675606
Epoch: 5387, Batch Gradient Norm after: 22.360678955699672
Epoch 5388/10000, Prediction Accuracy = 59.266%, Loss = 0.836041522026062
Epoch: 5388, Batch Gradient Norm: 31.27428853042591
Epoch: 5388, Batch Gradient Norm after: 22.36067858605009
Epoch 5389/10000, Prediction Accuracy = 59.242%, Loss = 0.8295068144798279
Epoch: 5389, Batch Gradient Norm: 33.92080902699949
Epoch: 5389, Batch Gradient Norm after: 22.360681303663156
Epoch 5390/10000, Prediction Accuracy = 59.284000000000006%, Loss = 0.8359352111816406
Epoch: 5390, Batch Gradient Norm: 31.269652160098403
Epoch: 5390, Batch Gradient Norm after: 22.36067808898389
Epoch 5391/10000, Prediction Accuracy = 59.246%, Loss = 0.8293967843055725
Epoch: 5391, Batch Gradient Norm: 33.91788882307484
Epoch: 5391, Batch Gradient Norm after: 22.360678422783437
Epoch 5392/10000, Prediction Accuracy = 59.274%, Loss = 0.8358341455459595
Epoch: 5392, Batch Gradient Norm: 31.26484498158296
Epoch: 5392, Batch Gradient Norm after: 22.360677142483432
Epoch 5393/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.8292845487594604
Epoch: 5393, Batch Gradient Norm: 33.91616933351649
Epoch: 5393, Batch Gradient Norm after: 22.36067811232056
Epoch 5394/10000, Prediction Accuracy = 59.274%, Loss = 0.8357190608978271
Epoch: 5394, Batch Gradient Norm: 31.259113351558284
Epoch: 5394, Batch Gradient Norm after: 22.360678023611182
Epoch 5395/10000, Prediction Accuracy = 59.242%, Loss = 0.8291699290275574
Epoch: 5395, Batch Gradient Norm: 33.91256609181265
Epoch: 5395, Batch Gradient Norm after: 22.360678625104953
Epoch 5396/10000, Prediction Accuracy = 59.278%, Loss = 0.8356000065803528
Epoch: 5396, Batch Gradient Norm: 31.257958985610454
Epoch: 5396, Batch Gradient Norm after: 22.36067786202101
Epoch 5397/10000, Prediction Accuracy = 59.25%, Loss = 0.8290504097938538
Epoch: 5397, Batch Gradient Norm: 33.90598550615142
Epoch: 5397, Batch Gradient Norm after: 22.360680387512083
Epoch 5398/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.835478150844574
Epoch: 5398, Batch Gradient Norm: 31.255036045218233
Epoch: 5398, Batch Gradient Norm after: 22.360678411569914
Epoch 5399/10000, Prediction Accuracy = 59.258%, Loss = 0.8289383888244629
Epoch: 5399, Batch Gradient Norm: 33.901306856633326
Epoch: 5399, Batch Gradient Norm after: 22.360679948769683
Epoch 5400/10000, Prediction Accuracy = 59.29%, Loss = 0.8353655934333801
Epoch: 5400, Batch Gradient Norm: 31.254016734633087
Epoch: 5400, Batch Gradient Norm after: 22.36067881871035
Epoch 5401/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.8288298845291138
Epoch: 5401, Batch Gradient Norm: 33.89866474914557
Epoch: 5401, Batch Gradient Norm after: 22.360679723538023
Epoch 5402/10000, Prediction Accuracy = 59.28399999999999%, Loss = 0.8352543592453003
Epoch: 5402, Batch Gradient Norm: 31.25118298027599
Epoch: 5402, Batch Gradient Norm after: 22.360678498797416
Epoch 5403/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.8287227272987365
Epoch: 5403, Batch Gradient Norm: 33.89286174495729
Epoch: 5403, Batch Gradient Norm after: 22.36067963159444
Epoch 5404/10000, Prediction Accuracy = 59.284000000000006%, Loss = 0.8351369857788086
Epoch: 5404, Batch Gradient Norm: 31.252070669901332
Epoch: 5404, Batch Gradient Norm after: 22.36067913135574
Epoch 5405/10000, Prediction Accuracy = 59.242%, Loss = 0.8286038517951966
Epoch: 5405, Batch Gradient Norm: 33.882580255538414
Epoch: 5405, Batch Gradient Norm after: 22.36068087657373
Epoch 5406/10000, Prediction Accuracy = 59.27%, Loss = 0.8350071907043457
Epoch: 5406, Batch Gradient Norm: 31.252163494412155
Epoch: 5406, Batch Gradient Norm after: 22.360678865205166
Epoch 5407/10000, Prediction Accuracy = 59.25%, Loss = 0.8284986019134521
Epoch: 5407, Batch Gradient Norm: 33.87151946900171
Epoch: 5407, Batch Gradient Norm after: 22.360680308306527
Epoch 5408/10000, Prediction Accuracy = 59.282000000000004%, Loss = 0.83486967086792
Epoch: 5408, Batch Gradient Norm: 31.25609182609779
Epoch: 5408, Batch Gradient Norm after: 22.36067723862117
Epoch 5409/10000, Prediction Accuracy = 59.246%, Loss = 0.8284016013145447
Epoch: 5409, Batch Gradient Norm: 33.86141871279294
Epoch: 5409, Batch Gradient Norm after: 22.36067718300316
Epoch 5410/10000, Prediction Accuracy = 59.282%, Loss = 0.8347380876541137
Epoch: 5410, Batch Gradient Norm: 31.25845286929916
Epoch: 5410, Batch Gradient Norm after: 22.36067756289466
Epoch 5411/10000, Prediction Accuracy = 59.254%, Loss = 0.8282968997955322
Epoch: 5411, Batch Gradient Norm: 33.84954912716253
Epoch: 5411, Batch Gradient Norm after: 22.360680979470157
Epoch 5412/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.8346102476119995
Epoch: 5412, Batch Gradient Norm: 31.258552697073036
Epoch: 5412, Batch Gradient Norm after: 22.360678224065957
Epoch 5413/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8282079577445984
Epoch: 5413, Batch Gradient Norm: 33.84088951581207
Epoch: 5413, Batch Gradient Norm after: 22.360679210490456
Epoch 5414/10000, Prediction Accuracy = 59.29200000000001%, Loss = 0.8344850540161133
Epoch: 5414, Batch Gradient Norm: 31.258302050630782
Epoch: 5414, Batch Gradient Norm after: 22.36067955688157
Epoch 5415/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8281060934066773
Epoch: 5415, Batch Gradient Norm: 33.828412885254934
Epoch: 5415, Batch Gradient Norm after: 22.360676111053817
Epoch 5416/10000, Prediction Accuracy = 59.278%, Loss = 0.834344494342804
Epoch: 5416, Batch Gradient Norm: 31.262427852279295
Epoch: 5416, Batch Gradient Norm after: 22.36067822330953
Epoch 5417/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.8279906272888183
Epoch: 5417, Batch Gradient Norm: 33.81929665446394
Epoch: 5417, Batch Gradient Norm after: 22.36067879009372
Epoch 5418/10000, Prediction Accuracy = 59.306%, Loss = 0.8342049837112426
Epoch: 5418, Batch Gradient Norm: 31.261187757782576
Epoch: 5418, Batch Gradient Norm after: 22.360678967593245
Epoch 5419/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.8278865337371826
Epoch: 5419, Batch Gradient Norm: 33.81279875078637
Epoch: 5419, Batch Gradient Norm after: 22.36068045705911
Epoch 5420/10000, Prediction Accuracy = 59.28800000000001%, Loss = 0.8340878844261169
Epoch: 5420, Batch Gradient Norm: 31.26432149030904
Epoch: 5420, Batch Gradient Norm after: 22.36067918397506
Epoch 5421/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.8277796864509582
Epoch: 5421, Batch Gradient Norm: 33.803269229618124
Epoch: 5421, Batch Gradient Norm after: 22.36067967893692
Epoch 5422/10000, Prediction Accuracy = 59.298%, Loss = 0.833970582485199
Epoch: 5422, Batch Gradient Norm: 31.262508927152243
Epoch: 5422, Batch Gradient Norm after: 22.36067810925137
Epoch 5423/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.8276883244514466
Epoch: 5423, Batch Gradient Norm: 33.798730320965284
Epoch: 5423, Batch Gradient Norm after: 22.36067812246329
Epoch 5424/10000, Prediction Accuracy = 59.31%, Loss = 0.8338511347770691
Epoch: 5424, Batch Gradient Norm: 31.2626510189503
Epoch: 5424, Batch Gradient Norm after: 22.360678255330853
Epoch 5425/10000, Prediction Accuracy = 59.24000000000001%, Loss = 0.8275721907615662
Epoch: 5425, Batch Gradient Norm: 33.79059608615384
Epoch: 5425, Batch Gradient Norm after: 22.36067638355383
Epoch 5426/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.8337186098098754
Epoch: 5426, Batch Gradient Norm: 31.260728300091174
Epoch: 5426, Batch Gradient Norm after: 22.360677646544683
Epoch 5427/10000, Prediction Accuracy = 59.254%, Loss = 0.8274689078330993
Epoch: 5427, Batch Gradient Norm: 33.783181199175566
Epoch: 5427, Batch Gradient Norm after: 22.360678796723228
Epoch 5428/10000, Prediction Accuracy = 59.294%, Loss = 0.8335888385772705
Epoch: 5428, Batch Gradient Norm: 31.26219084827239
Epoch: 5428, Batch Gradient Norm after: 22.36067622922921
Epoch 5429/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.8273591041564942
Epoch: 5429, Batch Gradient Norm: 33.778698994205286
Epoch: 5429, Batch Gradient Norm after: 22.36067754765296
Epoch 5430/10000, Prediction Accuracy = 59.298%, Loss = 0.8334686994552613
Epoch: 5430, Batch Gradient Norm: 31.259116414162442
Epoch: 5430, Batch Gradient Norm after: 22.360679812749773
Epoch 5431/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.827251386642456
Epoch: 5431, Batch Gradient Norm: 33.77341808790706
Epoch: 5431, Batch Gradient Norm after: 22.36067843364673
Epoch 5432/10000, Prediction Accuracy = 59.31%, Loss = 0.8333614468574524
Epoch: 5432, Batch Gradient Norm: 31.25983574116052
Epoch: 5432, Batch Gradient Norm after: 22.360676577898715
Epoch 5433/10000, Prediction Accuracy = 59.242%, Loss = 0.8271458983421326
Epoch: 5433, Batch Gradient Norm: 33.766509513045825
Epoch: 5433, Batch Gradient Norm after: 22.360679014398162
Epoch 5434/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8332377672195435
Epoch: 5434, Batch Gradient Norm: 31.26206774972362
Epoch: 5434, Batch Gradient Norm after: 22.36067872110902
Epoch 5435/10000, Prediction Accuracy = 59.25%, Loss = 0.82704598903656
Epoch: 5435, Batch Gradient Norm: 33.75736567350451
Epoch: 5435, Batch Gradient Norm after: 22.36067746966076
Epoch 5436/10000, Prediction Accuracy = 59.306%, Loss = 0.8331015467643738
Epoch: 5436, Batch Gradient Norm: 31.26239855471297
Epoch: 5436, Batch Gradient Norm after: 22.360676418640157
Epoch 5437/10000, Prediction Accuracy = 59.272000000000006%, Loss = 0.8269270181655883
Epoch: 5437, Batch Gradient Norm: 33.75221413121931
Epoch: 5437, Batch Gradient Norm after: 22.360679717244846
Epoch 5438/10000, Prediction Accuracy = 59.302%, Loss = 0.8329841613769531
Epoch: 5438, Batch Gradient Norm: 31.261206127047547
Epoch: 5438, Batch Gradient Norm after: 22.360680042412277
Epoch 5439/10000, Prediction Accuracy = 59.258%, Loss = 0.8268192291259766
Epoch: 5439, Batch Gradient Norm: 33.74800729327342
Epoch: 5439, Batch Gradient Norm after: 22.36067835541315
Epoch 5440/10000, Prediction Accuracy = 59.31600000000001%, Loss = 0.8328663468360901
Epoch: 5440, Batch Gradient Norm: 31.26147287091454
Epoch: 5440, Batch Gradient Norm after: 22.360679194219003
Epoch 5441/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8267211198806763
Epoch: 5441, Batch Gradient Norm: 33.74057791789751
Epoch: 5441, Batch Gradient Norm after: 22.360676399220758
Epoch 5442/10000, Prediction Accuracy = 59.30799999999999%, Loss = 0.8327505469322205
Epoch: 5442, Batch Gradient Norm: 31.25873437827181
Epoch: 5442, Batch Gradient Norm after: 22.360677601579116
Epoch 5443/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.8266183137893677
Epoch: 5443, Batch Gradient Norm: 33.73614808788659
Epoch: 5443, Batch Gradient Norm after: 22.360678401770958
Epoch 5444/10000, Prediction Accuracy = 59.312%, Loss = 0.8326382279396057
Epoch: 5444, Batch Gradient Norm: 31.25918161194213
Epoch: 5444, Batch Gradient Norm after: 22.360679393818565
Epoch 5445/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.8265110611915588
Epoch: 5445, Batch Gradient Norm: 33.729686064637775
Epoch: 5445, Batch Gradient Norm after: 22.360680219262836
Epoch 5446/10000, Prediction Accuracy = 59.31%, Loss = 0.8325087308883667
Epoch: 5446, Batch Gradient Norm: 31.258428785132665
Epoch: 5446, Batch Gradient Norm after: 22.360677913367592
Epoch 5447/10000, Prediction Accuracy = 59.266000000000005%, Loss = 0.8264052629470825
Epoch: 5447, Batch Gradient Norm: 33.72107517843342
Epoch: 5447, Batch Gradient Norm after: 22.36068094870893
Epoch 5448/10000, Prediction Accuracy = 59.298%, Loss = 0.8323864817619324
Epoch: 5448, Batch Gradient Norm: 31.259666983753537
Epoch: 5448, Batch Gradient Norm after: 22.36067846861639
Epoch 5449/10000, Prediction Accuracy = 59.269999999999996%, Loss = 0.8262924671173095
Epoch: 5449, Batch Gradient Norm: 33.714746517161586
Epoch: 5449, Batch Gradient Norm after: 22.360678767593257
Epoch 5450/10000, Prediction Accuracy = 59.294%, Loss = 0.8322579383850097
Epoch: 5450, Batch Gradient Norm: 31.261270417436165
Epoch: 5450, Batch Gradient Norm after: 22.360675797921786
Epoch 5451/10000, Prediction Accuracy = 59.278%, Loss = 0.8261929392814636
Epoch: 5451, Batch Gradient Norm: 33.705652704306004
Epoch: 5451, Batch Gradient Norm after: 22.36067881868017
Epoch 5452/10000, Prediction Accuracy = 59.306%, Loss = 0.8321384191513062
Epoch: 5452, Batch Gradient Norm: 31.26324506151976
Epoch: 5452, Batch Gradient Norm after: 22.36067868132282
Epoch 5453/10000, Prediction Accuracy = 59.25600000000001%, Loss = 0.826108455657959
Epoch: 5453, Batch Gradient Norm: 33.69791666893382
Epoch: 5453, Batch Gradient Norm after: 22.36067803395022
Epoch 5454/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.8320132970809937
Epoch: 5454, Batch Gradient Norm: 31.26331615739797
Epoch: 5454, Batch Gradient Norm after: 22.360678748962385
Epoch 5455/10000, Prediction Accuracy = 59.266%, Loss = 0.8260051250457764
Epoch: 5455, Batch Gradient Norm: 33.69224245342319
Epoch: 5455, Batch Gradient Norm after: 22.36067903477085
Epoch 5456/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.8318861842155456
Epoch: 5456, Batch Gradient Norm: 31.26583103225344
Epoch: 5456, Batch Gradient Norm after: 22.360679019225184
Epoch 5457/10000, Prediction Accuracy = 59.27199999999999%, Loss = 0.8258993864059448
Epoch: 5457, Batch Gradient Norm: 33.686602854664066
Epoch: 5457, Batch Gradient Norm after: 22.36067932593074
Epoch 5458/10000, Prediction Accuracy = 59.29599999999999%, Loss = 0.831754183769226
Epoch: 5458, Batch Gradient Norm: 31.269053296922483
Epoch: 5458, Batch Gradient Norm after: 22.36067917170252
Epoch 5459/10000, Prediction Accuracy = 59.278%, Loss = 0.8257903814315796
Epoch: 5459, Batch Gradient Norm: 33.678871503183515
Epoch: 5459, Batch Gradient Norm after: 22.360678481110455
Epoch 5460/10000, Prediction Accuracy = 59.294%, Loss = 0.8316335558891297
Epoch: 5460, Batch Gradient Norm: 31.26659616153457
Epoch: 5460, Batch Gradient Norm after: 22.360677281836086
Epoch 5461/10000, Prediction Accuracy = 59.288%, Loss = 0.8256965398788452
Epoch: 5461, Batch Gradient Norm: 33.675408496072585
Epoch: 5461, Batch Gradient Norm after: 22.360678480639262
Epoch 5462/10000, Prediction Accuracy = 59.322%, Loss = 0.831520962715149
Epoch: 5462, Batch Gradient Norm: 31.268083933818232
Epoch: 5462, Batch Gradient Norm after: 22.360676610184502
Epoch 5463/10000, Prediction Accuracy = 59.267999999999994%, Loss = 0.8255919575691223
Epoch: 5463, Batch Gradient Norm: 33.66680193626255
Epoch: 5463, Batch Gradient Norm after: 22.3606773650742
Epoch 5464/10000, Prediction Accuracy = 59.314%, Loss = 0.8313926935195923
Epoch: 5464, Batch Gradient Norm: 31.270817052503972
Epoch: 5464, Batch Gradient Norm after: 22.36067685529272
Epoch 5465/10000, Prediction Accuracy = 59.26800000000001%, Loss = 0.8254998564720154
Epoch: 5465, Batch Gradient Norm: 33.659704670692484
Epoch: 5465, Batch Gradient Norm after: 22.36067874145781
Epoch 5466/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.831270444393158
Epoch: 5466, Batch Gradient Norm: 31.26974566152165
Epoch: 5466, Batch Gradient Norm after: 22.360678951304546
Epoch 5467/10000, Prediction Accuracy = 59.272000000000006%, Loss = 0.8253890872001648
Epoch: 5467, Batch Gradient Norm: 33.655930185286394
Epoch: 5467, Batch Gradient Norm after: 22.360677610238543
Epoch 5468/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.8311474084854126
Epoch: 5468, Batch Gradient Norm: 31.27199851463807
Epoch: 5468, Batch Gradient Norm after: 22.360677342919807
Epoch 5469/10000, Prediction Accuracy = 59.27%, Loss = 0.8252756953239441
Epoch: 5469, Batch Gradient Norm: 33.64732727088412
Epoch: 5469, Batch Gradient Norm after: 22.360680444193388
Epoch 5470/10000, Prediction Accuracy = 59.298%, Loss = 0.8310294866561889
Epoch: 5470, Batch Gradient Norm: 31.269672384128828
Epoch: 5470, Batch Gradient Norm after: 22.360678137888975
Epoch 5471/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.8251807689666748
Epoch: 5471, Batch Gradient Norm: 33.64003459488205
Epoch: 5471, Batch Gradient Norm after: 22.360680266593096
Epoch 5472/10000, Prediction Accuracy = 59.326%, Loss = 0.8309191107749939
Epoch: 5472, Batch Gradient Norm: 31.270141516953696
Epoch: 5472, Batch Gradient Norm after: 22.360678295749807
Epoch 5473/10000, Prediction Accuracy = 59.288%, Loss = 0.8250805497169494
Epoch: 5473, Batch Gradient Norm: 33.633089037061914
Epoch: 5473, Batch Gradient Norm after: 22.360678741477113
Epoch 5474/10000, Prediction Accuracy = 59.318%, Loss = 0.83079594373703
Epoch: 5474, Batch Gradient Norm: 31.269121032044314
Epoch: 5474, Batch Gradient Norm after: 22.360679410482472
Epoch 5475/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.8249757647514343
Epoch: 5475, Batch Gradient Norm: 33.629704662739435
Epoch: 5475, Batch Gradient Norm after: 22.360681855703163
Epoch 5476/10000, Prediction Accuracy = 59.314%, Loss = 0.8306789755821228
Epoch: 5476, Batch Gradient Norm: 31.265936119952475
Epoch: 5476, Batch Gradient Norm after: 22.360678329603008
Epoch 5477/10000, Prediction Accuracy = 59.29%, Loss = 0.8248596429824829
Epoch: 5477, Batch Gradient Norm: 33.62417869392168
Epoch: 5477, Batch Gradient Norm after: 22.360678127868134
Epoch 5478/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.8305618047714234
Epoch: 5478, Batch Gradient Norm: 31.26670234901822
Epoch: 5478, Batch Gradient Norm after: 22.36068003228673
Epoch 5479/10000, Prediction Accuracy = 59.298%, Loss = 0.8247512698173523
Epoch: 5479, Batch Gradient Norm: 33.619199550869645
Epoch: 5479, Batch Gradient Norm after: 22.36068052480698
Epoch 5480/10000, Prediction Accuracy = 59.306%, Loss = 0.8304481983184815
Epoch: 5480, Batch Gradient Norm: 31.26266045982398
Epoch: 5480, Batch Gradient Norm after: 22.360679476260646
Epoch 5481/10000, Prediction Accuracy = 59.303999999999995%, Loss = 0.8246503591537475
Epoch: 5481, Batch Gradient Norm: 33.61389365126087
Epoch: 5481, Batch Gradient Norm after: 22.36067954024106
Epoch 5482/10000, Prediction Accuracy = 59.324%, Loss = 0.8303395628929138
Epoch: 5482, Batch Gradient Norm: 31.26018709658236
Epoch: 5482, Batch Gradient Norm after: 22.360679686681934
Epoch 5483/10000, Prediction Accuracy = 59.303999999999995%, Loss = 0.8245498895645141
Epoch: 5483, Batch Gradient Norm: 33.61012358788643
Epoch: 5483, Batch Gradient Norm after: 22.36067921296299
Epoch 5484/10000, Prediction Accuracy = 59.334%, Loss = 0.8302316546440125
Epoch: 5484, Batch Gradient Norm: 31.259507515364394
Epoch: 5484, Batch Gradient Norm after: 22.360678244700768
Epoch 5485/10000, Prediction Accuracy = 59.31%, Loss = 0.8244411826133728
Epoch: 5485, Batch Gradient Norm: 33.604389862580426
Epoch: 5485, Batch Gradient Norm after: 22.36068074121829
Epoch 5486/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.8301124334335327
Epoch: 5486, Batch Gradient Norm: 31.26119376548314
Epoch: 5486, Batch Gradient Norm after: 22.36068056757262
Epoch 5487/10000, Prediction Accuracy = 59.312%, Loss = 0.8243337035179138
Epoch: 5487, Batch Gradient Norm: 33.59771426417797
Epoch: 5487, Batch Gradient Norm after: 22.36067922427723
Epoch 5488/10000, Prediction Accuracy = 59.31%, Loss = 0.8299845814704895
Epoch: 5488, Batch Gradient Norm: 31.25953896500459
Epoch: 5488, Batch Gradient Norm after: 22.360679020426268
Epoch 5489/10000, Prediction Accuracy = 59.303999999999995%, Loss = 0.8242224216461181
Epoch: 5489, Batch Gradient Norm: 33.59230085503401
Epoch: 5489, Batch Gradient Norm after: 22.360677646280752
Epoch 5490/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8298662781715394
Epoch: 5490, Batch Gradient Norm: 31.26010549130047
Epoch: 5490, Batch Gradient Norm after: 22.3606821330231
Epoch 5491/10000, Prediction Accuracy = 59.302%, Loss = 0.8241081476211548
Epoch: 5491, Batch Gradient Norm: 33.58740121598835
Epoch: 5491, Batch Gradient Norm after: 22.36067881306681
Epoch 5492/10000, Prediction Accuracy = 59.312%, Loss = 0.8297570586204529
Epoch: 5492, Batch Gradient Norm: 31.259253094616938
Epoch: 5492, Batch Gradient Norm after: 22.360677689731137
Epoch 5493/10000, Prediction Accuracy = 59.322%, Loss = 0.8240187406539917
Epoch: 5493, Batch Gradient Norm: 33.57929170092466
Epoch: 5493, Batch Gradient Norm after: 22.36067843540666
Epoch 5494/10000, Prediction Accuracy = 59.324%, Loss = 0.8296470642089844
Epoch: 5494, Batch Gradient Norm: 31.2570662510667
Epoch: 5494, Batch Gradient Norm after: 22.360678777982994
Epoch 5495/10000, Prediction Accuracy = 59.324%, Loss = 0.8239222764968872
Epoch: 5495, Batch Gradient Norm: 33.57312285152027
Epoch: 5495, Batch Gradient Norm after: 22.360679644980962
Epoch 5496/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8295239806175232
Epoch: 5496, Batch Gradient Norm: 31.257140536621264
Epoch: 5496, Batch Gradient Norm after: 22.360679344243945
Epoch 5497/10000, Prediction Accuracy = 59.330000000000005%, Loss = 0.8238096952438354
Epoch: 5497, Batch Gradient Norm: 33.56542290347577
Epoch: 5497, Batch Gradient Norm after: 22.360678701165792
Epoch 5498/10000, Prediction Accuracy = 59.312%, Loss = 0.8293956398963929
Epoch: 5498, Batch Gradient Norm: 31.26035057679887
Epoch: 5498, Batch Gradient Norm after: 22.360679028126768
Epoch 5499/10000, Prediction Accuracy = 59.3%, Loss = 0.8236964344978333
Epoch: 5499, Batch Gradient Norm: 33.55804992474642
Epoch: 5499, Batch Gradient Norm after: 22.360679988915564
Epoch 5500/10000, Prediction Accuracy = 59.302%, Loss = 0.8292704463005066
Epoch: 5500, Batch Gradient Norm: 31.26172284009964
Epoch: 5500, Batch Gradient Norm after: 22.360680952256214
Epoch 5501/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8236035227775573
Epoch: 5501, Batch Gradient Norm: 33.54792187020451
Epoch: 5501, Batch Gradient Norm after: 22.36068097791678
Epoch 5502/10000, Prediction Accuracy = 59.318000000000005%, Loss = 0.8291533708572387
Epoch: 5502, Batch Gradient Norm: 31.262453418326146
Epoch: 5502, Batch Gradient Norm after: 22.36067859677939
Epoch 5503/10000, Prediction Accuracy = 59.33399999999999%, Loss = 0.8235073089599609
Epoch: 5503, Batch Gradient Norm: 33.54232993002233
Epoch: 5503, Batch Gradient Norm after: 22.360681255749792
Epoch 5504/10000, Prediction Accuracy = 59.324%, Loss = 0.8290428280830383
Epoch: 5504, Batch Gradient Norm: 31.261553690878635
Epoch: 5504, Batch Gradient Norm after: 22.360679233278198
Epoch 5505/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.8234177470207215
Epoch: 5505, Batch Gradient Norm: 33.53332286432848
Epoch: 5505, Batch Gradient Norm after: 22.360677490761965
Epoch 5506/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.8289158940315247
Epoch: 5506, Batch Gradient Norm: 31.26467409210687
Epoch: 5506, Batch Gradient Norm after: 22.360678001823374
Epoch 5507/10000, Prediction Accuracy = 59.338%, Loss = 0.8233189702033996
Epoch: 5507, Batch Gradient Norm: 33.52546088169511
Epoch: 5507, Batch Gradient Norm after: 22.360677894710882
Epoch 5508/10000, Prediction Accuracy = 59.322%, Loss = 0.8287822961807251
Epoch: 5508, Batch Gradient Norm: 31.27074494302307
Epoch: 5508, Batch Gradient Norm after: 22.360680740172675
Epoch 5509/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.823208999633789
Epoch: 5509, Batch Gradient Norm: 33.515057681002205
Epoch: 5509, Batch Gradient Norm after: 22.360679274162194
Epoch 5510/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.828648817539215
Epoch: 5510, Batch Gradient Norm: 31.27189823676131
Epoch: 5510, Batch Gradient Norm after: 22.36068120107908
Epoch 5511/10000, Prediction Accuracy = 59.314%, Loss = 0.8231128096580506
Epoch: 5511, Batch Gradient Norm: 33.51083371425413
Epoch: 5511, Batch Gradient Norm after: 22.36067985749943
Epoch 5512/10000, Prediction Accuracy = 59.314%, Loss = 0.8285362958908081
Epoch: 5512, Batch Gradient Norm: 31.27049597336613
Epoch: 5512, Batch Gradient Norm after: 22.36068032392789
Epoch 5513/10000, Prediction Accuracy = 59.342000000000006%, Loss = 0.8230021715164184
Epoch: 5513, Batch Gradient Norm: 33.50737920813156
Epoch: 5513, Batch Gradient Norm after: 22.36067902831843
Epoch 5514/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8284359216690064
Epoch: 5514, Batch Gradient Norm: 31.2663587293803
Epoch: 5514, Batch Gradient Norm after: 22.36068105003906
Epoch 5515/10000, Prediction Accuracy = 59.35%, Loss = 0.8229063391685486
Epoch: 5515, Batch Gradient Norm: 33.501744159238704
Epoch: 5515, Batch Gradient Norm after: 22.360680329689995
Epoch 5516/10000, Prediction Accuracy = 59.342%, Loss = 0.8283255457878113
Epoch: 5516, Batch Gradient Norm: 31.2622500987972
Epoch: 5516, Batch Gradient Norm after: 22.360677921758562
Epoch 5517/10000, Prediction Accuracy = 59.35200000000001%, Loss = 0.8228026747703552
Epoch: 5517, Batch Gradient Norm: 33.49932211930324
Epoch: 5517, Batch Gradient Norm after: 22.360679331712397
Epoch 5518/10000, Prediction Accuracy = 59.336%, Loss = 0.8282090663909912
Epoch: 5518, Batch Gradient Norm: 31.26122609922901
Epoch: 5518, Batch Gradient Norm after: 22.36067980401564
Epoch 5519/10000, Prediction Accuracy = 59.33200000000001%, Loss = 0.8226773262023925
Epoch: 5519, Batch Gradient Norm: 33.492192879930705
Epoch: 5519, Batch Gradient Norm after: 22.36067943694731
Epoch 5520/10000, Prediction Accuracy = 59.312%, Loss = 0.8280861616134644
Epoch: 5520, Batch Gradient Norm: 31.26022386559628
Epoch: 5520, Batch Gradient Norm after: 22.36067996081544
Epoch 5521/10000, Prediction Accuracy = 59.302%, Loss = 0.8225731730461121
Epoch: 5521, Batch Gradient Norm: 33.485356578610954
Epoch: 5521, Batch Gradient Norm after: 22.360678270168357
Epoch 5522/10000, Prediction Accuracy = 59.312%, Loss = 0.8279669880867004
Epoch: 5522, Batch Gradient Norm: 31.258793507610868
Epoch: 5522, Batch Gradient Norm after: 22.3606799093441
Epoch 5523/10000, Prediction Accuracy = 59.33%, Loss = 0.8224871993064881
Epoch: 5523, Batch Gradient Norm: 33.47700614267758
Epoch: 5523, Batch Gradient Norm after: 22.36067886492049
Epoch 5524/10000, Prediction Accuracy = 59.318%, Loss = 0.8278528332710267
Epoch: 5524, Batch Gradient Norm: 31.25888745324509
Epoch: 5524, Batch Gradient Norm after: 22.36067991473576
Epoch 5525/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8223919987678527
Epoch: 5525, Batch Gradient Norm: 33.4711752617849
Epoch: 5525, Batch Gradient Norm after: 22.360679498179447
Epoch 5526/10000, Prediction Accuracy = 59.342000000000006%, Loss = 0.8277460932731628
Epoch: 5526, Batch Gradient Norm: 31.26047873318477
Epoch: 5526, Batch Gradient Norm after: 22.36067975191251
Epoch 5527/10000, Prediction Accuracy = 59.35999999999999%, Loss = 0.8222814083099366
Epoch: 5527, Batch Gradient Norm: 33.46702007494336
Epoch: 5527, Batch Gradient Norm after: 22.3606758915113
Epoch 5528/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8276262283325195
Epoch: 5528, Batch Gradient Norm: 31.25880846889178
Epoch: 5528, Batch Gradient Norm after: 22.360680061298552
Epoch 5529/10000, Prediction Accuracy = 59.342%, Loss = 0.8221791386604309
Epoch: 5529, Batch Gradient Norm: 33.4601840730294
Epoch: 5529, Batch Gradient Norm after: 22.3606805062085
Epoch 5530/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8274943351745605
Epoch: 5530, Batch Gradient Norm: 31.2568512993389
Epoch: 5530, Batch Gradient Norm after: 22.3606772542554
Epoch 5531/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8220742821693421
Epoch: 5531, Batch Gradient Norm: 33.4565756687223
Epoch: 5531, Batch Gradient Norm after: 22.360679508660947
Epoch 5532/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8273803472518921
Epoch: 5532, Batch Gradient Norm: 31.249074911301765
Epoch: 5532, Batch Gradient Norm after: 22.360678742613246
Epoch 5533/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.821958327293396
Epoch: 5533, Batch Gradient Norm: 33.45704994333203
Epoch: 5533, Batch Gradient Norm after: 22.360679139204706
Epoch 5534/10000, Prediction Accuracy = 59.334%, Loss = 0.8272963166236877
Epoch: 5534, Batch Gradient Norm: 31.246800593965105
Epoch: 5534, Batch Gradient Norm after: 22.360679659503088
Epoch 5535/10000, Prediction Accuracy = 59.364%, Loss = 0.8218545317649841
Epoch: 5535, Batch Gradient Norm: 33.45270145786396
Epoch: 5535, Batch Gradient Norm after: 22.360678139164182
Epoch 5536/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8271999955177307
Epoch: 5536, Batch Gradient Norm: 31.240406288313103
Epoch: 5536, Batch Gradient Norm after: 22.36068095982084
Epoch 5537/10000, Prediction Accuracy = 59.36399999999999%, Loss = 0.8217575073242187
Epoch: 5537, Batch Gradient Norm: 33.44894635225264
Epoch: 5537, Batch Gradient Norm after: 22.360678972839732
Epoch 5538/10000, Prediction Accuracy = 59.33%, Loss = 0.8270860314369202
Epoch: 5538, Batch Gradient Norm: 31.24092958648781
Epoch: 5538, Batch Gradient Norm after: 22.360678874959028
Epoch 5539/10000, Prediction Accuracy = 59.36800000000001%, Loss = 0.8216357469558716
Epoch: 5539, Batch Gradient Norm: 33.44490839136958
Epoch: 5539, Batch Gradient Norm after: 22.360680004769847
Epoch 5540/10000, Prediction Accuracy = 59.33399999999999%, Loss = 0.8269591331481934
Epoch: 5540, Batch Gradient Norm: 31.23815543654574
Epoch: 5540, Batch Gradient Norm after: 22.36068065731323
Epoch 5541/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8215194582939148
Epoch: 5541, Batch Gradient Norm: 33.441085665956955
Epoch: 5541, Batch Gradient Norm after: 22.360679533046227
Epoch 5542/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.8268449306488037
Epoch: 5542, Batch Gradient Norm: 31.233641454976368
Epoch: 5542, Batch Gradient Norm after: 22.36068105915547
Epoch 5543/10000, Prediction Accuracy = 59.334%, Loss = 0.8214115977287293
Epoch: 5543, Batch Gradient Norm: 33.437491453681545
Epoch: 5543, Batch Gradient Norm after: 22.360677116314477
Epoch 5544/10000, Prediction Accuracy = 59.31600000000001%, Loss = 0.8267431259155273
Epoch: 5544, Batch Gradient Norm: 31.233535407531036
Epoch: 5544, Batch Gradient Norm after: 22.360679748601232
Epoch 5545/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8213090777397156
Epoch: 5545, Batch Gradient Norm: 33.430426802944794
Epoch: 5545, Batch Gradient Norm after: 22.36067659693706
Epoch 5546/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8266401529312134
Epoch: 5546, Batch Gradient Norm: 31.232016766319983
Epoch: 5546, Batch Gradient Norm after: 22.360679333129237
Epoch 5547/10000, Prediction Accuracy = 59.372%, Loss = 0.8212194442749023
Epoch: 5547, Batch Gradient Norm: 33.42495262826838
Epoch: 5547, Batch Gradient Norm after: 22.360679468595915
Epoch 5548/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.8265319705009461
Epoch: 5548, Batch Gradient Norm: 31.227722985757424
Epoch: 5548, Batch Gradient Norm after: 22.36067816695903
Epoch 5549/10000, Prediction Accuracy = 59.376%, Loss = 0.8211203813552856
Epoch: 5549, Batch Gradient Norm: 33.419517002845225
Epoch: 5549, Batch Gradient Norm after: 22.360679556173682
Epoch 5550/10000, Prediction Accuracy = 59.336%, Loss = 0.8264024734497071
Epoch: 5550, Batch Gradient Norm: 31.2300994141415
Epoch: 5550, Batch Gradient Norm after: 22.36067925636523
Epoch 5551/10000, Prediction Accuracy = 59.342%, Loss = 0.8209980010986329
Epoch: 5551, Batch Gradient Norm: 33.41489313625802
Epoch: 5551, Batch Gradient Norm after: 22.360678274536664
Epoch 5552/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.8262787938117981
Epoch: 5552, Batch Gradient Norm: 31.228160045821753
Epoch: 5552, Batch Gradient Norm after: 22.360678681370132
Epoch 5553/10000, Prediction Accuracy = 59.348%, Loss = 0.8208943247795105
Epoch: 5553, Batch Gradient Norm: 33.40883038593265
Epoch: 5553, Batch Gradient Norm after: 22.360676493923
Epoch 5554/10000, Prediction Accuracy = 59.324%, Loss = 0.826163935661316
Epoch: 5554, Batch Gradient Norm: 31.226023659191227
Epoch: 5554, Batch Gradient Norm after: 22.360679605612805
Epoch 5555/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.820798933506012
Epoch: 5555, Batch Gradient Norm: 33.403093731845686
Epoch: 5555, Batch Gradient Norm after: 22.36067901625823
Epoch 5556/10000, Prediction Accuracy = 59.326%, Loss = 0.8260701656341553
Epoch: 5556, Batch Gradient Norm: 31.225052645000446
Epoch: 5556, Batch Gradient Norm after: 22.36067845721565
Epoch 5557/10000, Prediction Accuracy = 59.376%, Loss = 0.8207049131393432
Epoch: 5557, Batch Gradient Norm: 33.3973198996873
Epoch: 5557, Batch Gradient Norm after: 22.360676938169423
Epoch 5558/10000, Prediction Accuracy = 59.322%, Loss = 0.8259611248970031
Epoch: 5558, Batch Gradient Norm: 31.224723619243107
Epoch: 5558, Batch Gradient Norm after: 22.360679592827484
Epoch 5559/10000, Prediction Accuracy = 59.384%, Loss = 0.8206049799919128
Epoch: 5559, Batch Gradient Norm: 33.39192151294596
Epoch: 5559, Batch Gradient Norm after: 22.36067929396209
Epoch 5560/10000, Prediction Accuracy = 59.334%, Loss = 0.8258341312408447
Epoch: 5560, Batch Gradient Norm: 31.224735148953165
Epoch: 5560, Batch Gradient Norm after: 22.360680106681023
Epoch 5561/10000, Prediction Accuracy = 59.354%, Loss = 0.8204903841018677
Epoch: 5561, Batch Gradient Norm: 33.38695478225077
Epoch: 5561, Batch Gradient Norm after: 22.360681127996827
Epoch 5562/10000, Prediction Accuracy = 59.32800000000001%, Loss = 0.8257053136825562
Epoch: 5562, Batch Gradient Norm: 31.220547876928816
Epoch: 5562, Batch Gradient Norm after: 22.360680655178104
Epoch 5563/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.8203811526298523
Epoch: 5563, Batch Gradient Norm: 33.383515752920744
Epoch: 5563, Batch Gradient Norm after: 22.36067766633354
Epoch 5564/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.8255961775779724
Epoch: 5564, Batch Gradient Norm: 31.222223040056846
Epoch: 5564, Batch Gradient Norm after: 22.36067992507311
Epoch 5565/10000, Prediction Accuracy = 59.366%, Loss = 0.8202648758888245
Epoch: 5565, Batch Gradient Norm: 33.377774366690765
Epoch: 5565, Batch Gradient Norm after: 22.360679607229784
Epoch 5566/10000, Prediction Accuracy = 59.326%, Loss = 0.8254973888397217
Epoch: 5566, Batch Gradient Norm: 31.218848724858443
Epoch: 5566, Batch Gradient Norm after: 22.360677646847545
Epoch 5567/10000, Prediction Accuracy = 59.388%, Loss = 0.8201785683631897
Epoch: 5567, Batch Gradient Norm: 33.37187039283229
Epoch: 5567, Batch Gradient Norm after: 22.360680206610414
Epoch 5568/10000, Prediction Accuracy = 59.336%, Loss = 0.8254110097885132
Epoch: 5568, Batch Gradient Norm: 31.212997358248536
Epoch: 5568, Batch Gradient Norm after: 22.360677565675193
Epoch 5569/10000, Prediction Accuracy = 59.42%, Loss = 0.8200905680656433
Epoch: 5569, Batch Gradient Norm: 33.369140329986614
Epoch: 5569, Batch Gradient Norm after: 22.36068022858014
Epoch 5570/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8252950310707092
Epoch: 5570, Batch Gradient Norm: 31.21245393293709
Epoch: 5570, Batch Gradient Norm after: 22.3606773052978
Epoch 5571/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8199700474739074
Epoch: 5571, Batch Gradient Norm: 33.364944445813535
Epoch: 5571, Batch Gradient Norm after: 22.36067779329757
Epoch 5572/10000, Prediction Accuracy = 59.326%, Loss = 0.8251571893692017
Epoch: 5572, Batch Gradient Norm: 31.209997463455753
Epoch: 5572, Batch Gradient Norm after: 22.360678550776978
Epoch 5573/10000, Prediction Accuracy = 59.36%, Loss = 0.8198489546775818
Epoch: 5573, Batch Gradient Norm: 33.36146679846741
Epoch: 5573, Batch Gradient Norm after: 22.360678993377096
Epoch 5574/10000, Prediction Accuracy = 59.324%, Loss = 0.825043797492981
Epoch: 5574, Batch Gradient Norm: 31.21074857166285
Epoch: 5574, Batch Gradient Norm after: 22.360678837828203
Epoch 5575/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8197322130203247
Epoch: 5575, Batch Gradient Norm: 33.35442974709775
Epoch: 5575, Batch Gradient Norm after: 22.360680994726753
Epoch 5576/10000, Prediction Accuracy = 59.338%, Loss = 0.8249337434768677
Epoch: 5576, Batch Gradient Norm: 31.209377468693727
Epoch: 5576, Batch Gradient Norm after: 22.360677679945145
Epoch 5577/10000, Prediction Accuracy = 59.382000000000005%, Loss = 0.8196456432342529
Epoch: 5577, Batch Gradient Norm: 33.347324424355094
Epoch: 5577, Batch Gradient Norm after: 22.36067859599196
Epoch 5578/10000, Prediction Accuracy = 59.334%, Loss = 0.8248545408248902
Epoch: 5578, Batch Gradient Norm: 31.20393088245201
Epoch: 5578, Batch Gradient Norm after: 22.36067685448043
Epoch 5579/10000, Prediction Accuracy = 59.42199999999999%, Loss = 0.8195738911628723
Epoch: 5579, Batch Gradient Norm: 33.34109643608407
Epoch: 5579, Batch Gradient Norm after: 22.360679205543256
Epoch 5580/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.824774968624115
Epoch: 5580, Batch Gradient Norm: 31.203991019376225
Epoch: 5580, Batch Gradient Norm after: 22.360677764253925
Epoch 5581/10000, Prediction Accuracy = 59.414%, Loss = 0.8194629788398743
Epoch: 5581, Batch Gradient Norm: 33.33630801228159
Epoch: 5581, Batch Gradient Norm after: 22.360681025469397
Epoch 5582/10000, Prediction Accuracy = 59.334%, Loss = 0.8246208190917969
Epoch: 5582, Batch Gradient Norm: 31.197756536982787
Epoch: 5582, Batch Gradient Norm after: 22.360677794319376
Epoch 5583/10000, Prediction Accuracy = 59.36%, Loss = 0.8193310618400573
Epoch: 5583, Batch Gradient Norm: 33.33376039046708
Epoch: 5583, Batch Gradient Norm after: 22.360679718498265
Epoch 5584/10000, Prediction Accuracy = 59.31999999999999%, Loss = 0.8244890093803405
Epoch: 5584, Batch Gradient Norm: 31.200534837256118
Epoch: 5584, Batch Gradient Norm after: 22.360677653865963
Epoch 5585/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.819205367565155
Epoch: 5585, Batch Gradient Norm: 33.326515220498244
Epoch: 5585, Batch Gradient Norm after: 22.360680409227825
Epoch 5586/10000, Prediction Accuracy = 59.31%, Loss = 0.8243751525878906
Epoch: 5586, Batch Gradient Norm: 31.194197150668913
Epoch: 5586, Batch Gradient Norm after: 22.360679610748917
Epoch 5587/10000, Prediction Accuracy = 59.366%, Loss = 0.8191039443016053
Epoch: 5587, Batch Gradient Norm: 33.326756388442995
Epoch: 5587, Batch Gradient Norm after: 22.36068059774959
Epoch 5588/10000, Prediction Accuracy = 59.334%, Loss = 0.8242979407310486
Epoch: 5588, Batch Gradient Norm: 31.19008350574283
Epoch: 5588, Batch Gradient Norm after: 22.36067860101003
Epoch 5589/10000, Prediction Accuracy = 59.416%, Loss = 0.819023859500885
Epoch: 5589, Batch Gradient Norm: 33.32391961563789
Epoch: 5589, Batch Gradient Norm after: 22.360680991768138
Epoch 5590/10000, Prediction Accuracy = 59.352%, Loss = 0.8242403984069824
Epoch: 5590, Batch Gradient Norm: 31.185784250854912
Epoch: 5590, Batch Gradient Norm after: 22.36067872711638
Epoch 5591/10000, Prediction Accuracy = 59.424%, Loss = 0.818937087059021
Epoch: 5591, Batch Gradient Norm: 33.31904287214092
Epoch: 5591, Batch Gradient Norm after: 22.360681008243787
Epoch 5592/10000, Prediction Accuracy = 59.336%, Loss = 0.8241123914718628
Epoch: 5592, Batch Gradient Norm: 31.18130848877951
Epoch: 5592, Batch Gradient Norm after: 22.360680380553333
Epoch 5593/10000, Prediction Accuracy = 59.39799999999999%, Loss = 0.818812894821167
Epoch: 5593, Batch Gradient Norm: 33.31438593628046
Epoch: 5593, Batch Gradient Norm after: 22.36067989708062
Epoch 5594/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8239582896232605
Epoch: 5594, Batch Gradient Norm: 31.179034639803096
Epoch: 5594, Batch Gradient Norm after: 22.36067895346769
Epoch 5595/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8186713218688965
Epoch: 5595, Batch Gradient Norm: 33.31092934794822
Epoch: 5595, Batch Gradient Norm after: 22.360679218255488
Epoch 5596/10000, Prediction Accuracy = 59.331999999999994%, Loss = 0.8238438248634339
Epoch: 5596, Batch Gradient Norm: 31.177393816215197
Epoch: 5596, Batch Gradient Norm after: 22.36067946707425
Epoch 5597/10000, Prediction Accuracy = 59.348%, Loss = 0.8185657978057861
Epoch: 5597, Batch Gradient Norm: 33.30924376926885
Epoch: 5597, Batch Gradient Norm after: 22.360679453076564
Epoch 5598/10000, Prediction Accuracy = 59.35%, Loss = 0.8237411141395569
Epoch: 5598, Batch Gradient Norm: 31.173738774183334
Epoch: 5598, Batch Gradient Norm after: 22.360677769538974
Epoch 5599/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.8184768676757812
Epoch: 5599, Batch Gradient Norm: 33.30591457201445
Epoch: 5599, Batch Gradient Norm after: 22.36068083830187
Epoch 5600/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.8236696243286132
Epoch: 5600, Batch Gradient Norm: 31.17020288268029
Epoch: 5600, Batch Gradient Norm after: 22.360676683081
Epoch 5601/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8183999180793762
Epoch: 5601, Batch Gradient Norm: 33.30138812509628
Epoch: 5601, Batch Gradient Norm after: 22.360681243977762
Epoch 5602/10000, Prediction Accuracy = 59.348%, Loss = 0.8235749721527099
Epoch: 5602, Batch Gradient Norm: 31.168195004301406
Epoch: 5602, Batch Gradient Norm after: 22.36068144005743
Epoch 5603/10000, Prediction Accuracy = 59.414%, Loss = 0.818294632434845
Epoch: 5603, Batch Gradient Norm: 33.295401210809175
Epoch: 5603, Batch Gradient Norm after: 22.360678597462694
Epoch 5604/10000, Prediction Accuracy = 59.352%, Loss = 0.8234300971031189
Epoch: 5604, Batch Gradient Norm: 31.163897266704804
Epoch: 5604, Batch Gradient Norm after: 22.36067844283191
Epoch 5605/10000, Prediction Accuracy = 59.376%, Loss = 0.818170142173767
Epoch: 5605, Batch Gradient Norm: 33.29494895124317
Epoch: 5605, Batch Gradient Norm after: 22.360680314838007
Epoch 5606/10000, Prediction Accuracy = 59.352%, Loss = 0.8232966899871826
Epoch: 5606, Batch Gradient Norm: 31.160829528726037
Epoch: 5606, Batch Gradient Norm after: 22.360677765869596
Epoch 5607/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8180415868759155
Epoch: 5607, Batch Gradient Norm: 33.29045117123313
Epoch: 5607, Batch Gradient Norm after: 22.36067980944167
Epoch 5608/10000, Prediction Accuracy = 59.34999999999999%, Loss = 0.8231967210769653
Epoch: 5608, Batch Gradient Norm: 31.158655657486158
Epoch: 5608, Batch Gradient Norm after: 22.36067699154319
Epoch 5609/10000, Prediction Accuracy = 59.386%, Loss = 0.8179419994354248
Epoch: 5609, Batch Gradient Norm: 33.28604008074128
Epoch: 5609, Batch Gradient Norm after: 22.360680226036685
Epoch 5610/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.8231064081192017
Epoch: 5610, Batch Gradient Norm: 31.15719708850828
Epoch: 5610, Batch Gradient Norm after: 22.360676484945948
Epoch 5611/10000, Prediction Accuracy = 59.428%, Loss = 0.817864465713501
Epoch: 5611, Batch Gradient Norm: 33.28189249098714
Epoch: 5611, Batch Gradient Norm after: 22.360678864744454
Epoch 5612/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8230270385742188
Epoch: 5612, Batch Gradient Norm: 31.153632460268728
Epoch: 5612, Batch Gradient Norm after: 22.360679027644363
Epoch 5613/10000, Prediction Accuracy = 59.436%, Loss = 0.8177731275558472
Epoch: 5613, Batch Gradient Norm: 33.276684047096914
Epoch: 5613, Batch Gradient Norm after: 22.36068247160271
Epoch 5614/10000, Prediction Accuracy = 59.35%, Loss = 0.8229160666465759
Epoch: 5614, Batch Gradient Norm: 31.149221182285487
Epoch: 5614, Batch Gradient Norm after: 22.360677865153274
Epoch 5615/10000, Prediction Accuracy = 59.41799999999999%, Loss = 0.8176596522331238
Epoch: 5615, Batch Gradient Norm: 33.274513747097046
Epoch: 5615, Batch Gradient Norm after: 22.360681322757284
Epoch 5616/10000, Prediction Accuracy = 59.354%, Loss = 0.8227778792381286
Epoch: 5616, Batch Gradient Norm: 31.14606330403795
Epoch: 5616, Batch Gradient Norm after: 22.36067886567987
Epoch 5617/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.8175197839736938
Epoch: 5617, Batch Gradient Norm: 33.27165503779558
Epoch: 5617, Batch Gradient Norm after: 22.36067891064091
Epoch 5618/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.8226598739624024
Epoch: 5618, Batch Gradient Norm: 31.141772081599353
Epoch: 5618, Batch Gradient Norm after: 22.360678952356228
Epoch 5619/10000, Prediction Accuracy = 59.386%, Loss = 0.8174004554748535
Epoch: 5619, Batch Gradient Norm: 33.26678898309539
Epoch: 5619, Batch Gradient Norm after: 22.360682378240142
Epoch 5620/10000, Prediction Accuracy = 59.362%, Loss = 0.8225653052330018
Epoch: 5620, Batch Gradient Norm: 31.139112918946914
Epoch: 5620, Batch Gradient Norm after: 22.360677624048364
Epoch 5621/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.8173060894012452
Epoch: 5621, Batch Gradient Norm: 33.2615628198299
Epoch: 5621, Batch Gradient Norm after: 22.36067998836182
Epoch 5622/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.8224897623062134
Epoch: 5622, Batch Gradient Norm: 31.136907632683712
Epoch: 5622, Batch Gradient Norm after: 22.360677673141925
Epoch 5623/10000, Prediction Accuracy = 59.456%, Loss = 0.8172380447387695
Epoch: 5623, Batch Gradient Norm: 33.2585247418069
Epoch: 5623, Batch Gradient Norm after: 22.360682460231196
Epoch 5624/10000, Prediction Accuracy = 59.354%, Loss = 0.8224025011062622
Epoch: 5624, Batch Gradient Norm: 31.13004778838065
Epoch: 5624, Batch Gradient Norm after: 22.360677577393
Epoch 5625/10000, Prediction Accuracy = 59.44%, Loss = 0.8171330332756043
Epoch: 5625, Batch Gradient Norm: 33.25533527950759
Epoch: 5625, Batch Gradient Norm after: 22.360681849360567
Epoch 5626/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.8222695469856263
Epoch: 5626, Batch Gradient Norm: 31.129666712097197
Epoch: 5626, Batch Gradient Norm after: 22.36067923523257
Epoch 5627/10000, Prediction Accuracy = 59.396%, Loss = 0.8170086145401001
Epoch: 5627, Batch Gradient Norm: 33.24735881090113
Epoch: 5627, Batch Gradient Norm after: 22.36067957742579
Epoch 5628/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8221267223358154
Epoch: 5628, Batch Gradient Norm: 31.12993853319438
Epoch: 5628, Batch Gradient Norm after: 22.36067991494338
Epoch 5629/10000, Prediction Accuracy = 59.391999999999996%, Loss = 0.816887640953064
Epoch: 5629, Batch Gradient Norm: 33.24254480301517
Epoch: 5629, Batch Gradient Norm after: 22.360680935436157
Epoch 5630/10000, Prediction Accuracy = 59.352%, Loss = 0.822014856338501
Epoch: 5630, Batch Gradient Norm: 31.127008212067317
Epoch: 5630, Batch Gradient Norm after: 22.36068066114413
Epoch 5631/10000, Prediction Accuracy = 59.416%, Loss = 0.8167890191078186
Epoch: 5631, Batch Gradient Norm: 33.2383812381348
Epoch: 5631, Batch Gradient Norm after: 22.360680120877458
Epoch 5632/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8219346165657043
Epoch: 5632, Batch Gradient Norm: 31.122579979251725
Epoch: 5632, Batch Gradient Norm after: 22.360677536313293
Epoch 5633/10000, Prediction Accuracy = 59.426%, Loss = 0.8167011618614197
Epoch: 5633, Batch Gradient Norm: 33.23460633520071
Epoch: 5633, Batch Gradient Norm after: 22.360680846777125
Epoch 5634/10000, Prediction Accuracy = 59.342%, Loss = 0.821862256526947
Epoch: 5634, Batch Gradient Norm: 31.119799073840277
Epoch: 5634, Batch Gradient Norm after: 22.36068016075322
Epoch 5635/10000, Prediction Accuracy = 59.45%, Loss = 0.8166134715080261
Epoch: 5635, Batch Gradient Norm: 33.2321784309954
Epoch: 5635, Batch Gradient Norm after: 22.360682531603565
Epoch 5636/10000, Prediction Accuracy = 59.338%, Loss = 0.8217460751533509
Epoch: 5636, Batch Gradient Norm: 31.11244545503961
Epoch: 5636, Batch Gradient Norm after: 22.360678738264586
Epoch 5637/10000, Prediction Accuracy = 59.424%, Loss = 0.8164896726608276
Epoch: 5637, Batch Gradient Norm: 33.23231422748093
Epoch: 5637, Batch Gradient Norm after: 22.36068137278426
Epoch 5638/10000, Prediction Accuracy = 59.374%, Loss = 0.8216169714927674
Epoch: 5638, Batch Gradient Norm: 31.105800905878667
Epoch: 5638, Batch Gradient Norm after: 22.360677666845895
Epoch 5639/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.8163520574569703
Epoch: 5639, Batch Gradient Norm: 33.231856387013075
Epoch: 5639, Batch Gradient Norm after: 22.360681361844993
Epoch 5640/10000, Prediction Accuracy = 59.376%, Loss = 0.821506118774414
Epoch: 5640, Batch Gradient Norm: 31.09796478306093
Epoch: 5640, Batch Gradient Norm after: 22.36067758575908
Epoch 5641/10000, Prediction Accuracy = 59.42%, Loss = 0.8162348628044128
Epoch: 5641, Batch Gradient Norm: 33.23269467111691
Epoch: 5641, Batch Gradient Norm after: 22.36068067470641
Epoch 5642/10000, Prediction Accuracy = 59.384%, Loss = 0.8214220643043518
Epoch: 5642, Batch Gradient Norm: 31.09371026733672
Epoch: 5642, Batch Gradient Norm after: 22.360677791067342
Epoch 5643/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.8161309719085693
Epoch: 5643, Batch Gradient Norm: 33.23223648699413
Epoch: 5643, Batch Gradient Norm after: 22.360679167664745
Epoch 5644/10000, Prediction Accuracy = 59.36199999999999%, Loss = 0.8213406562805176
Epoch: 5644, Batch Gradient Norm: 31.086896676105447
Epoch: 5644, Batch Gradient Norm after: 22.360679560069595
Epoch 5645/10000, Prediction Accuracy = 59.428%, Loss = 0.8160439491271972
Epoch: 5645, Batch Gradient Norm: 33.230169373742605
Epoch: 5645, Batch Gradient Norm after: 22.360682742820554
Epoch 5646/10000, Prediction Accuracy = 59.366%, Loss = 0.8212605476379394
Epoch: 5646, Batch Gradient Norm: 31.080600117007336
Epoch: 5646, Batch Gradient Norm after: 22.36067745742121
Epoch 5647/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.8159294247627258
Epoch: 5647, Batch Gradient Norm: 33.230211052148
Epoch: 5647, Batch Gradient Norm after: 22.360682300816265
Epoch 5648/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.8211468815803528
Epoch: 5648, Batch Gradient Norm: 31.074049089407744
Epoch: 5648, Batch Gradient Norm after: 22.360679119735497
Epoch 5649/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.8157983422279358
Epoch: 5649, Batch Gradient Norm: 33.23038096963432
Epoch: 5649, Batch Gradient Norm after: 22.360674797134244
Epoch 5650/10000, Prediction Accuracy = 59.396%, Loss = 0.8210220336914062
Epoch: 5650, Batch Gradient Norm: 31.064675319931105
Epoch: 5650, Batch Gradient Norm after: 22.360677749038427
Epoch 5651/10000, Prediction Accuracy = 59.42%, Loss = 0.8156713128089905
Epoch: 5651, Batch Gradient Norm: 33.22956848190611
Epoch: 5651, Batch Gradient Norm after: 22.360676952962717
Epoch 5652/10000, Prediction Accuracy = 59.38199999999999%, Loss = 0.8209275722503662
Epoch: 5652, Batch Gradient Norm: 31.055189886388334
Epoch: 5652, Batch Gradient Norm after: 22.36067690988918
Epoch 5653/10000, Prediction Accuracy = 59.426%, Loss = 0.815555477142334
Epoch: 5653, Batch Gradient Norm: 33.23049322497908
Epoch: 5653, Batch Gradient Norm after: 22.360678482342585
Epoch 5654/10000, Prediction Accuracy = 59.37199999999999%, Loss = 0.8208520174026489
Epoch: 5654, Batch Gradient Norm: 31.04579726955905
Epoch: 5654, Batch Gradient Norm after: 22.360676275059845
Epoch 5655/10000, Prediction Accuracy = 59.44%, Loss = 0.8154648542404175
Epoch: 5655, Batch Gradient Norm: 33.2321044339199
Epoch: 5655, Batch Gradient Norm after: 22.360680491264407
Epoch 5656/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.8207895994186402
Epoch: 5656, Batch Gradient Norm: 31.03626731452833
Epoch: 5656, Batch Gradient Norm after: 22.360678002070063
Epoch 5657/10000, Prediction Accuracy = 59.448%, Loss = 0.8153703212738037
Epoch: 5657, Batch Gradient Norm: 33.23253925110588
Epoch: 5657, Batch Gradient Norm after: 22.360681763184537
Epoch 5658/10000, Prediction Accuracy = 59.362%, Loss = 0.8206949472427368
Epoch: 5658, Batch Gradient Norm: 31.028750782720834
Epoch: 5658, Batch Gradient Norm after: 22.36067678262182
Epoch 5659/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.8152436494827271
Epoch: 5659, Batch Gradient Norm: 33.23107998470293
Epoch: 5659, Batch Gradient Norm after: 22.3606801931646
Epoch 5660/10000, Prediction Accuracy = 59.36800000000001%, Loss = 0.8205592632293701
Epoch: 5660, Batch Gradient Norm: 31.02322318425602
Epoch: 5660, Batch Gradient Norm after: 22.360678902067537
Epoch 5661/10000, Prediction Accuracy = 59.426%, Loss = 0.8151027917861938
Epoch: 5661, Batch Gradient Norm: 33.23059538523069
Epoch: 5661, Batch Gradient Norm after: 22.36068223547116
Epoch 5662/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.8204415917396546
Epoch: 5662, Batch Gradient Norm: 31.015983627086605
Epoch: 5662, Batch Gradient Norm after: 22.360677068575143
Epoch 5663/10000, Prediction Accuracy = 59.428%, Loss = 0.8149828553199768
Epoch: 5663, Batch Gradient Norm: 33.23009584323283
Epoch: 5663, Batch Gradient Norm after: 22.360679525316034
Epoch 5664/10000, Prediction Accuracy = 59.402%, Loss = 0.8203566312789917
Epoch: 5664, Batch Gradient Norm: 31.012817067524548
Epoch: 5664, Batch Gradient Norm after: 22.36068035690431
Epoch 5665/10000, Prediction Accuracy = 59.432%, Loss = 0.8148797154426575
Epoch: 5665, Batch Gradient Norm: 33.22659888938712
Epoch: 5665, Batch Gradient Norm after: 22.360679117924484
Epoch 5666/10000, Prediction Accuracy = 59.376%, Loss = 0.8202755570411682
Epoch: 5666, Batch Gradient Norm: 31.006723886533162
Epoch: 5666, Batch Gradient Norm after: 22.360675565139015
Epoch 5667/10000, Prediction Accuracy = 59.436%, Loss = 0.8148067235946655
Epoch: 5667, Batch Gradient Norm: 33.22407325480214
Epoch: 5667, Batch Gradient Norm after: 22.36068099173781
Epoch 5668/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.8202053427696228
Epoch: 5668, Batch Gradient Norm: 31.004067740532705
Epoch: 5668, Batch Gradient Norm after: 22.36067858722174
Epoch 5669/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.8147112727165222
Epoch: 5669, Batch Gradient Norm: 33.21739588398004
Epoch: 5669, Batch Gradient Norm after: 22.360680708415227
Epoch 5670/10000, Prediction Accuracy = 59.384%, Loss = 0.8200700521469116
Epoch: 5670, Batch Gradient Norm: 31.000395963254487
Epoch: 5670, Batch Gradient Norm after: 22.360676805478285
Epoch 5671/10000, Prediction Accuracy = 59.438%, Loss = 0.8145877957344055
Epoch: 5671, Batch Gradient Norm: 33.21458405712357
Epoch: 5671, Batch Gradient Norm after: 22.360680595564972
Epoch 5672/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.8199310779571534
Epoch: 5672, Batch Gradient Norm: 30.99985272826296
Epoch: 5672, Batch Gradient Norm after: 22.36067926019213
Epoch 5673/10000, Prediction Accuracy = 59.428%, Loss = 0.8144590973854064
Epoch: 5673, Batch Gradient Norm: 33.21017431823623
Epoch: 5673, Batch Gradient Norm after: 22.36067932043385
Epoch 5674/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8198137998580932
Epoch: 5674, Batch Gradient Norm: 30.996804846670376
Epoch: 5674, Batch Gradient Norm after: 22.36067858258774
Epoch 5675/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.8143561959266663
Epoch: 5675, Batch Gradient Norm: 33.20733127218854
Epoch: 5675, Batch Gradient Norm after: 22.36068187942411
Epoch 5676/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8197273850440979
Epoch: 5676, Batch Gradient Norm: 30.997072668602
Epoch: 5676, Batch Gradient Norm after: 22.360677108091704
Epoch 5677/10000, Prediction Accuracy = 59.45%, Loss = 0.8142664909362793
Epoch: 5677, Batch Gradient Norm: 33.20093710575347
Epoch: 5677, Batch Gradient Norm after: 22.36068008872697
Epoch 5678/10000, Prediction Accuracy = 59.388%, Loss = 0.8196473360061646
Epoch: 5678, Batch Gradient Norm: 30.991928846315247
Epoch: 5678, Batch Gradient Norm after: 22.360679506093188
Epoch 5679/10000, Prediction Accuracy = 59.452%, Loss = 0.8141972661018372
Epoch: 5679, Batch Gradient Norm: 33.19643707450159
Epoch: 5679, Batch Gradient Norm after: 22.36068077092008
Epoch 5680/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.819548487663269
Epoch: 5680, Batch Gradient Norm: 30.991116421580923
Epoch: 5680, Batch Gradient Norm after: 22.360675029602923
Epoch 5681/10000, Prediction Accuracy = 59.456%, Loss = 0.8140832781791687
Epoch: 5681, Batch Gradient Norm: 33.191792446232625
Epoch: 5681, Batch Gradient Norm after: 22.36068068096556
Epoch 5682/10000, Prediction Accuracy = 59.402%, Loss = 0.819397258758545
Epoch: 5682, Batch Gradient Norm: 30.990148611031024
Epoch: 5682, Batch Gradient Norm after: 22.36067829981223
Epoch 5683/10000, Prediction Accuracy = 59.436%, Loss = 0.8139622449874878
Epoch: 5683, Batch Gradient Norm: 33.190021731538565
Epoch: 5683, Batch Gradient Norm after: 22.3606790937316
Epoch 5684/10000, Prediction Accuracy = 59.424%, Loss = 0.8192757487297058
Epoch: 5684, Batch Gradient Norm: 30.987103632462837
Epoch: 5684, Batch Gradient Norm after: 22.36067829925743
Epoch 5685/10000, Prediction Accuracy = 59.448%, Loss = 0.8138468980789184
Epoch: 5685, Batch Gradient Norm: 33.186270971801605
Epoch: 5685, Batch Gradient Norm after: 22.36067910092331
Epoch 5686/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.8191755414009094
Epoch: 5686, Batch Gradient Norm: 30.98741708308929
Epoch: 5686, Batch Gradient Norm after: 22.360678830644908
Epoch 5687/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.8137467503547668
Epoch: 5687, Batch Gradient Norm: 33.183757881935065
Epoch: 5687, Batch Gradient Norm after: 22.360680926346397
Epoch 5688/10000, Prediction Accuracy = 59.391999999999996%, Loss = 0.8190921068191528
Epoch: 5688, Batch Gradient Norm: 30.98223648835786
Epoch: 5688, Batch Gradient Norm after: 22.360679327311672
Epoch 5689/10000, Prediction Accuracy = 59.448%, Loss = 0.8136728048324585
Epoch: 5689, Batch Gradient Norm: 33.18014034393998
Epoch: 5689, Batch Gradient Norm after: 22.360681328112793
Epoch 5690/10000, Prediction Accuracy = 59.407999999999994%, Loss = 0.8190122008323669
Epoch: 5690, Batch Gradient Norm: 30.979832732480798
Epoch: 5690, Batch Gradient Norm after: 22.360678769344634
Epoch 5691/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.8135755658149719
Epoch: 5691, Batch Gradient Norm: 33.17417694041612
Epoch: 5691, Batch Gradient Norm after: 22.360679834312513
Epoch 5692/10000, Prediction Accuracy = 59.407999999999994%, Loss = 0.8188851356506348
Epoch: 5692, Batch Gradient Norm: 30.976627317578238
Epoch: 5692, Batch Gradient Norm after: 22.360677480120312
Epoch 5693/10000, Prediction Accuracy = 59.44%, Loss = 0.8134561419487
Epoch: 5693, Batch Gradient Norm: 33.1741052320788
Epoch: 5693, Batch Gradient Norm after: 22.360679287044604
Epoch 5694/10000, Prediction Accuracy = 59.41400000000001%, Loss = 0.8187602758407593
Epoch: 5694, Batch Gradient Norm: 30.973046326936743
Epoch: 5694, Batch Gradient Norm after: 22.36067841678365
Epoch 5695/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.8133351087570191
Epoch: 5695, Batch Gradient Norm: 33.16982604345234
Epoch: 5695, Batch Gradient Norm after: 22.360680057476984
Epoch 5696/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8186498999595642
Epoch: 5696, Batch Gradient Norm: 30.969816808751542
Epoch: 5696, Batch Gradient Norm after: 22.360677953702226
Epoch 5697/10000, Prediction Accuracy = 59.43399999999999%, Loss = 0.8132273912429809
Epoch: 5697, Batch Gradient Norm: 33.16540811750402
Epoch: 5697, Batch Gradient Norm after: 22.360680114901342
Epoch 5698/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.8185545802116394
Epoch: 5698, Batch Gradient Norm: 30.968581543015453
Epoch: 5698, Batch Gradient Norm after: 22.360677021735576
Epoch 5699/10000, Prediction Accuracy = 59.45400000000001%, Loss = 0.8131468415260314
Epoch: 5699, Batch Gradient Norm: 33.162134771796964
Epoch: 5699, Batch Gradient Norm after: 22.36067790941638
Epoch 5700/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.8184808135032654
Epoch: 5700, Batch Gradient Norm: 30.966995747090156
Epoch: 5700, Batch Gradient Norm after: 22.360676021190088
Epoch 5701/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8130776166915894
Epoch: 5701, Batch Gradient Norm: 33.15453563562871
Epoch: 5701, Batch Gradient Norm after: 22.360680322510216
Epoch 5702/10000, Prediction Accuracy = 59.408%, Loss = 0.8183705806732178
Epoch: 5702, Batch Gradient Norm: 30.965834123486015
Epoch: 5702, Batch Gradient Norm after: 22.360678686698243
Epoch 5703/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8129706382751465
Epoch: 5703, Batch Gradient Norm: 33.15016577690317
Epoch: 5703, Batch Gradient Norm after: 22.360679449388478
Epoch 5704/10000, Prediction Accuracy = 59.418000000000006%, Loss = 0.8182264924049377
Epoch: 5704, Batch Gradient Norm: 30.96419479568584
Epoch: 5704, Batch Gradient Norm after: 22.360677212463205
Epoch 5705/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.8128397345542908
Epoch: 5705, Batch Gradient Norm: 33.1443306336531
Epoch: 5705, Batch Gradient Norm after: 22.360678339902563
Epoch 5706/10000, Prediction Accuracy = 59.408%, Loss = 0.818099570274353
Epoch: 5706, Batch Gradient Norm: 30.9626702394001
Epoch: 5706, Batch Gradient Norm after: 22.360676245191016
Epoch 5707/10000, Prediction Accuracy = 59.43000000000001%, Loss = 0.8127301335334778
Epoch: 5707, Batch Gradient Norm: 33.13766400408313
Epoch: 5707, Batch Gradient Norm after: 22.360678721756475
Epoch 5708/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8179950594902039
Epoch: 5708, Batch Gradient Norm: 30.966212165312022
Epoch: 5708, Batch Gradient Norm after: 22.360676886567703
Epoch 5709/10000, Prediction Accuracy = 59.432%, Loss = 0.8126412391662597
Epoch: 5709, Batch Gradient Norm: 33.13281053306701
Epoch: 5709, Batch Gradient Norm after: 22.36068035914302
Epoch 5710/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8179123163223266
Epoch: 5710, Batch Gradient Norm: 30.965352448559067
Epoch: 5710, Batch Gradient Norm after: 22.36067792653499
Epoch 5711/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8125967383384705
Epoch: 5711, Batch Gradient Norm: 33.12228614246477
Epoch: 5711, Batch Gradient Norm after: 22.360679159221796
Epoch 5712/10000, Prediction Accuracy = 59.402%, Loss = 0.8178206562995911
Epoch: 5712, Batch Gradient Norm: 30.968145278220703
Epoch: 5712, Batch Gradient Norm after: 22.360679289247045
Epoch 5713/10000, Prediction Accuracy = 59.46%, Loss = 0.8125155687332153
Epoch: 5713, Batch Gradient Norm: 33.11485990803835
Epoch: 5713, Batch Gradient Norm after: 22.360678724299618
Epoch 5714/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8176725387573243
Epoch: 5714, Batch Gradient Norm: 30.972839708271213
Epoch: 5714, Batch Gradient Norm after: 22.360678812374175
Epoch 5715/10000, Prediction Accuracy = 59.44799999999999%, Loss = 0.8123894453048706
Epoch: 5715, Batch Gradient Norm: 33.10678688932037
Epoch: 5715, Batch Gradient Norm after: 22.360677940594716
Epoch 5716/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8175180435180665
Epoch: 5716, Batch Gradient Norm: 30.972994931254174
Epoch: 5716, Batch Gradient Norm after: 22.360678594131294
Epoch 5717/10000, Prediction Accuracy = 59.45%, Loss = 0.8122777700424194
Epoch: 5717, Batch Gradient Norm: 33.10282277896349
Epoch: 5717, Batch Gradient Norm after: 22.360677438496253
Epoch 5718/10000, Prediction Accuracy = 59.432%, Loss = 0.8174053788185119
Epoch: 5718, Batch Gradient Norm: 30.97282114354448
Epoch: 5718, Batch Gradient Norm after: 22.360675983305637
Epoch 5719/10000, Prediction Accuracy = 59.432%, Loss = 0.8121777892112731
Epoch: 5719, Batch Gradient Norm: 33.09597868281593
Epoch: 5719, Batch Gradient Norm after: 22.36067637652955
Epoch 5720/10000, Prediction Accuracy = 59.424%, Loss = 0.8173121690750123
Epoch: 5720, Batch Gradient Norm: 30.971835619413493
Epoch: 5720, Batch Gradient Norm after: 22.36067894206415
Epoch 5721/10000, Prediction Accuracy = 59.46%, Loss = 0.8121162056922913
Epoch: 5721, Batch Gradient Norm: 33.091886432232215
Epoch: 5721, Batch Gradient Norm after: 22.360681612631023
Epoch 5722/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.8172437906265259
Epoch: 5722, Batch Gradient Norm: 30.970999930570684
Epoch: 5722, Batch Gradient Norm after: 22.36068024147921
Epoch 5723/10000, Prediction Accuracy = 59.468%, Loss = 0.8120528340339661
Epoch: 5723, Batch Gradient Norm: 33.08485473053294
Epoch: 5723, Batch Gradient Norm after: 22.360677780245545
Epoch 5724/10000, Prediction Accuracy = 59.412%, Loss = 0.8171319127082824
Epoch: 5724, Batch Gradient Norm: 30.969873595389593
Epoch: 5724, Batch Gradient Norm after: 22.360678873056255
Epoch 5725/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.8119315385818482
Epoch: 5725, Batch Gradient Norm: 33.07491132833432
Epoch: 5725, Batch Gradient Norm after: 22.36067890460309
Epoch 5726/10000, Prediction Accuracy = 59.412%, Loss = 0.8169798135757447
Epoch: 5726, Batch Gradient Norm: 30.972172788900576
Epoch: 5726, Batch Gradient Norm after: 22.36067619332236
Epoch 5727/10000, Prediction Accuracy = 59.438%, Loss = 0.8118176579475402
Epoch: 5727, Batch Gradient Norm: 33.06681247910322
Epoch: 5727, Batch Gradient Norm after: 22.360677227583793
Epoch 5728/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.8168384671211243
Epoch: 5728, Batch Gradient Norm: 30.97429905702591
Epoch: 5728, Batch Gradient Norm after: 22.360674327964556
Epoch 5729/10000, Prediction Accuracy = 59.448%, Loss = 0.8117118716239929
Epoch: 5729, Batch Gradient Norm: 33.05941569871098
Epoch: 5729, Batch Gradient Norm after: 22.36067765849413
Epoch 5730/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.8167336106300354
Epoch: 5730, Batch Gradient Norm: 30.97651271770775
Epoch: 5730, Batch Gradient Norm after: 22.360677062950586
Epoch 5731/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8116282224655151
Epoch: 5731, Batch Gradient Norm: 33.05410019514226
Epoch: 5731, Batch Gradient Norm after: 22.360677187584006
Epoch 5732/10000, Prediction Accuracy = 59.424%, Loss = 0.8166526198387146
Epoch: 5732, Batch Gradient Norm: 30.976753211382658
Epoch: 5732, Batch Gradient Norm after: 22.360676824145457
Epoch 5733/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8115732431411743
Epoch: 5733, Batch Gradient Norm: 33.04657315588945
Epoch: 5733, Batch Gradient Norm after: 22.360680696987643
Epoch 5734/10000, Prediction Accuracy = 59.402%, Loss = 0.8165568113327026
Epoch: 5734, Batch Gradient Norm: 30.978370175221304
Epoch: 5734, Batch Gradient Norm after: 22.360678087654748
Epoch 5735/10000, Prediction Accuracy = 59.459999999999994%, Loss = 0.8114885807037353
Epoch: 5735, Batch Gradient Norm: 33.04050258340035
Epoch: 5735, Batch Gradient Norm after: 22.36067758025855
Epoch 5736/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8164225816726685
Epoch: 5736, Batch Gradient Norm: 30.984259085613264
Epoch: 5736, Batch Gradient Norm after: 22.360677200652553
Epoch 5737/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.8113729357719421
Epoch: 5737, Batch Gradient Norm: 33.033062397419236
Epoch: 5737, Batch Gradient Norm after: 22.360678608638263
Epoch 5738/10000, Prediction Accuracy = 59.44%, Loss = 0.8162669897079468
Epoch: 5738, Batch Gradient Norm: 30.98358640489718
Epoch: 5738, Batch Gradient Norm after: 22.360675470750376
Epoch 5739/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8112560629844665
Epoch: 5739, Batch Gradient Norm: 33.02746223552906
Epoch: 5739, Batch Gradient Norm after: 22.360679826050468
Epoch 5740/10000, Prediction Accuracy = 59.472%, Loss = 0.8161520600318909
Epoch: 5740, Batch Gradient Norm: 30.983268124671792
Epoch: 5740, Batch Gradient Norm after: 22.360676990094543
Epoch 5741/10000, Prediction Accuracy = 59.45%, Loss = 0.8111576080322266
Epoch: 5741, Batch Gradient Norm: 33.02431993747824
Epoch: 5741, Batch Gradient Norm after: 22.36067596470241
Epoch 5742/10000, Prediction Accuracy = 59.428%, Loss = 0.8160614013671875
Epoch: 5742, Batch Gradient Norm: 30.983414089446594
Epoch: 5742, Batch Gradient Norm after: 22.3606774374062
Epoch 5743/10000, Prediction Accuracy = 59.476%, Loss = 0.8110798954963684
Epoch: 5743, Batch Gradient Norm: 33.01944282890699
Epoch: 5743, Batch Gradient Norm after: 22.360680367293796
Epoch 5744/10000, Prediction Accuracy = 59.412%, Loss = 0.8159877300262451
Epoch: 5744, Batch Gradient Norm: 30.98248141483194
Epoch: 5744, Batch Gradient Norm after: 22.36067631074902
Epoch 5745/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8110211849212646
Epoch: 5745, Batch Gradient Norm: 33.012636728318434
Epoch: 5745, Batch Gradient Norm after: 22.360680472260245
Epoch 5746/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8158888578414917
Epoch: 5746, Batch Gradient Norm: 30.984605117320736
Epoch: 5746, Batch Gradient Norm after: 22.36067741744606
Epoch 5747/10000, Prediction Accuracy = 59.46%, Loss = 0.8109153151512146
Epoch: 5747, Batch Gradient Norm: 33.006534997714105
Epoch: 5747, Batch Gradient Norm after: 22.360680416486545
Epoch 5748/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.8157388091087341
Epoch: 5748, Batch Gradient Norm: 30.984049227336737
Epoch: 5748, Batch Gradient Norm after: 22.360677471101535
Epoch 5749/10000, Prediction Accuracy = 59.464%, Loss = 0.8107946515083313
Epoch: 5749, Batch Gradient Norm: 33.00314881704676
Epoch: 5749, Batch Gradient Norm after: 22.36068054786336
Epoch 5750/10000, Prediction Accuracy = 59.45200000000001%, Loss = 0.815608274936676
Epoch: 5750, Batch Gradient Norm: 30.979211591524766
Epoch: 5750, Batch Gradient Norm after: 22.360676259204844
Epoch 5751/10000, Prediction Accuracy = 59.448%, Loss = 0.810670804977417
Epoch: 5751, Batch Gradient Norm: 33.00125870801182
Epoch: 5751, Batch Gradient Norm after: 22.360678167316596
Epoch 5752/10000, Prediction Accuracy = 59.462%, Loss = 0.8155157208442688
Epoch: 5752, Batch Gradient Norm: 30.97341271682506
Epoch: 5752, Batch Gradient Norm after: 22.360678524948078
Epoch 5753/10000, Prediction Accuracy = 59.46%, Loss = 0.8105728149414062
Epoch: 5753, Batch Gradient Norm: 33.00242090279515
Epoch: 5753, Batch Gradient Norm after: 22.36067886834829
Epoch 5754/10000, Prediction Accuracy = 59.452%, Loss = 0.8154415965080262
Epoch: 5754, Batch Gradient Norm: 30.970500018895486
Epoch: 5754, Batch Gradient Norm after: 22.36067741557744
Epoch 5755/10000, Prediction Accuracy = 59.46400000000001%, Loss = 0.8104981303215026
Epoch: 5755, Batch Gradient Norm: 32.99792100655048
Epoch: 5755, Batch Gradient Norm after: 22.360678069209943
Epoch 5756/10000, Prediction Accuracy = 59.428%, Loss = 0.8153694272041321
Epoch: 5756, Batch Gradient Norm: 30.965166185114313
Epoch: 5756, Batch Gradient Norm after: 22.360677235449046
Epoch 5757/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8104296445846557
Epoch: 5757, Batch Gradient Norm: 32.99519377584159
Epoch: 5757, Batch Gradient Norm after: 22.36067838354613
Epoch 5758/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.8152536153793335
Epoch: 5758, Batch Gradient Norm: 30.961147141376305
Epoch: 5758, Batch Gradient Norm after: 22.360677369848414
Epoch 5759/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.8102944254875183
Epoch: 5759, Batch Gradient Norm: 32.99264920710257
Epoch: 5759, Batch Gradient Norm after: 22.36067827945439
Epoch 5760/10000, Prediction Accuracy = 59.443999999999996%, Loss = 0.8151183485984802
Epoch: 5760, Batch Gradient Norm: 30.958161340542198
Epoch: 5760, Batch Gradient Norm after: 22.360676669891834
Epoch 5761/10000, Prediction Accuracy = 59.462%, Loss = 0.8101650476455688
Epoch: 5761, Batch Gradient Norm: 32.98736385580097
Epoch: 5761, Batch Gradient Norm after: 22.360676842870657
Epoch 5762/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.8150086641311646
Epoch: 5762, Batch Gradient Norm: 30.958389601601834
Epoch: 5762, Batch Gradient Norm after: 22.360678912570428
Epoch 5763/10000, Prediction Accuracy = 59.452%, Loss = 0.8100531935691834
Epoch: 5763, Batch Gradient Norm: 32.984151671729535
Epoch: 5763, Batch Gradient Norm after: 22.36067806391756
Epoch 5764/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8149139523506165
Epoch: 5764, Batch Gradient Norm: 30.953297274966395
Epoch: 5764, Batch Gradient Norm after: 22.360678298285045
Epoch 5765/10000, Prediction Accuracy = 59.48%, Loss = 0.809968638420105
Epoch: 5765, Batch Gradient Norm: 32.98265388645782
Epoch: 5765, Batch Gradient Norm after: 22.360678701569338
Epoch 5766/10000, Prediction Accuracy = 59.436%, Loss = 0.8148468375205994
Epoch: 5766, Batch Gradient Norm: 30.951483461580903
Epoch: 5766, Batch Gradient Norm after: 22.360677968318022
Epoch 5767/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.809915280342102
Epoch: 5767, Batch Gradient Norm: 32.976239547817734
Epoch: 5767, Batch Gradient Norm after: 22.36067893390185
Epoch 5768/10000, Prediction Accuracy = 59.444%, Loss = 0.8147574663162231
Epoch: 5768, Batch Gradient Norm: 30.948996990231446
Epoch: 5768, Batch Gradient Norm after: 22.360676114888236
Epoch 5769/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8098099589347839
Epoch: 5769, Batch Gradient Norm: 32.9709342447086
Epoch: 5769, Batch Gradient Norm after: 22.360680655999996
Epoch 5770/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.8146190881729126
Epoch: 5770, Batch Gradient Norm: 30.94950309600989
Epoch: 5770, Batch Gradient Norm after: 22.3606776363113
Epoch 5771/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8096852540969849
Epoch: 5771, Batch Gradient Norm: 32.96664884588861
Epoch: 5771, Batch Gradient Norm after: 22.36067888634446
Epoch 5772/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8144871711730957
Epoch: 5772, Batch Gradient Norm: 30.94799826407739
Epoch: 5772, Batch Gradient Norm after: 22.36067799117091
Epoch 5773/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.809568452835083
Epoch: 5773, Batch Gradient Norm: 32.96285314657824
Epoch: 5773, Batch Gradient Norm after: 22.36067825381535
Epoch 5774/10000, Prediction Accuracy = 59.45%, Loss = 0.814386522769928
Epoch: 5774, Batch Gradient Norm: 30.94753277431818
Epoch: 5774, Batch Gradient Norm after: 22.36067873177687
Epoch 5775/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8094716191291809
Epoch: 5775, Batch Gradient Norm: 32.95778646302453
Epoch: 5775, Batch Gradient Norm after: 22.360678704831138
Epoch 5776/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8143023133277894
Epoch: 5776, Batch Gradient Norm: 30.947322767773446
Epoch: 5776, Batch Gradient Norm after: 22.360678622760105
Epoch 5777/10000, Prediction Accuracy = 59.482000000000006%, Loss = 0.8094125986099243
Epoch: 5777, Batch Gradient Norm: 32.95135081717066
Epoch: 5777, Batch Gradient Norm after: 22.36067965146123
Epoch 5778/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8142310619354248
Epoch: 5778, Batch Gradient Norm: 30.94882370810144
Epoch: 5778, Batch Gradient Norm after: 22.360679225119295
Epoch 5779/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8093515992164612
Epoch: 5779, Batch Gradient Norm: 32.94120378703156
Epoch: 5779, Batch Gradient Norm after: 22.360681346817103
Epoch 5780/10000, Prediction Accuracy = 59.468%, Loss = 0.814097273349762
Epoch: 5780, Batch Gradient Norm: 30.954068957419516
Epoch: 5780, Batch Gradient Norm after: 22.360678785643437
Epoch 5781/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8092410922050476
Epoch: 5781, Batch Gradient Norm: 32.93133763011437
Epoch: 5781, Batch Gradient Norm after: 22.360678833431997
Epoch 5782/10000, Prediction Accuracy = 59.476%, Loss = 0.8139403104782105
Epoch: 5782, Batch Gradient Norm: 30.957462054287667
Epoch: 5782, Batch Gradient Norm after: 22.360678635555725
Epoch 5783/10000, Prediction Accuracy = 59.448%, Loss = 0.8091249823570251
Epoch: 5783, Batch Gradient Norm: 32.92727571201606
Epoch: 5783, Batch Gradient Norm after: 22.360678329068765
Epoch 5784/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8138180613517761
Epoch: 5784, Batch Gradient Norm: 30.96059867321158
Epoch: 5784, Batch Gradient Norm after: 22.3606774070285
Epoch 5785/10000, Prediction Accuracy = 59.476%, Loss = 0.8090180158615112
Epoch: 5785, Batch Gradient Norm: 32.91977616383437
Epoch: 5785, Batch Gradient Norm after: 22.36067913733028
Epoch 5786/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8137207269668579
Epoch: 5786, Batch Gradient Norm: 30.960561748138378
Epoch: 5786, Batch Gradient Norm after: 22.360675732289746
Epoch 5787/10000, Prediction Accuracy = 59.464%, Loss = 0.8089516162872314
Epoch: 5787, Batch Gradient Norm: 32.91289984701257
Epoch: 5787, Batch Gradient Norm after: 22.360681040313267
Epoch 5788/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.813655436038971
Epoch: 5788, Batch Gradient Norm: 30.960977826149993
Epoch: 5788, Batch Gradient Norm after: 22.360675560224056
Epoch 5789/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8089080572128295
Epoch: 5789, Batch Gradient Norm: 32.90786890508175
Epoch: 5789, Batch Gradient Norm after: 22.360682047298532
Epoch 5790/10000, Prediction Accuracy = 59.46999999999999%, Loss = 0.8135766863822937
Epoch: 5790, Batch Gradient Norm: 30.95815478882622
Epoch: 5790, Batch Gradient Norm after: 22.360676163547073
Epoch 5791/10000, Prediction Accuracy = 59.476%, Loss = 0.8088130712509155
Epoch: 5791, Batch Gradient Norm: 32.903091334884294
Epoch: 5791, Batch Gradient Norm after: 22.36068151449156
Epoch 5792/10000, Prediction Accuracy = 59.472%, Loss = 0.8134208679199219
Epoch: 5792, Batch Gradient Norm: 30.960181372661115
Epoch: 5792, Batch Gradient Norm after: 22.360676023959467
Epoch 5793/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8086764335632324
Epoch: 5793, Batch Gradient Norm: 32.8961911417273
Epoch: 5793, Batch Gradient Norm after: 22.36067859499631
Epoch 5794/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8132781505584716
Epoch: 5794, Batch Gradient Norm: 30.958148704292405
Epoch: 5794, Batch Gradient Norm after: 22.360676051013264
Epoch 5795/10000, Prediction Accuracy = 59.476%, Loss = 0.808558177947998
Epoch: 5795, Batch Gradient Norm: 32.8920349117768
Epoch: 5795, Batch Gradient Norm after: 22.36067816743848
Epoch 5796/10000, Prediction Accuracy = 59.468%, Loss = 0.8131781578063965
Epoch: 5796, Batch Gradient Norm: 30.95737315450938
Epoch: 5796, Batch Gradient Norm after: 22.360675701361263
Epoch 5797/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8084667086601257
Epoch: 5797, Batch Gradient Norm: 32.88800860561427
Epoch: 5797, Batch Gradient Norm after: 22.360679290628603
Epoch 5798/10000, Prediction Accuracy = 59.472%, Loss = 0.8130983471870422
Epoch: 5798, Batch Gradient Norm: 30.95731682623054
Epoch: 5798, Batch Gradient Norm after: 22.36067620884725
Epoch 5799/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8084061622619629
Epoch: 5799, Batch Gradient Norm: 32.88100165095815
Epoch: 5799, Batch Gradient Norm after: 22.360679986201422
Epoch 5800/10000, Prediction Accuracy = 59.476%, Loss = 0.8130407094955444
Epoch: 5800, Batch Gradient Norm: 30.952468498173733
Epoch: 5800, Batch Gradient Norm after: 22.360678527631514
Epoch 5801/10000, Prediction Accuracy = 59.456%, Loss = 0.8083452224731446
Epoch: 5801, Batch Gradient Norm: 32.876500798883164
Epoch: 5801, Batch Gradient Norm after: 22.360678619704693
Epoch 5802/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8129243493080139
Epoch: 5802, Batch Gradient Norm: 30.950030206687185
Epoch: 5802, Batch Gradient Norm after: 22.360678278091697
Epoch 5803/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8082207202911377
Epoch: 5803, Batch Gradient Norm: 32.8692589161372
Epoch: 5803, Batch Gradient Norm after: 22.360677057925702
Epoch 5804/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.8127705335617066
Epoch: 5804, Batch Gradient Norm: 30.948997612958923
Epoch: 5804, Batch Gradient Norm after: 22.360679174945776
Epoch 5805/10000, Prediction Accuracy = 59.476%, Loss = 0.8080840349197388
Epoch: 5805, Batch Gradient Norm: 32.86819656699983
Epoch: 5805, Batch Gradient Norm after: 22.360677091925886
Epoch 5806/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8126624584197998
Epoch: 5806, Batch Gradient Norm: 30.94208763165847
Epoch: 5806, Batch Gradient Norm after: 22.360677378250404
Epoch 5807/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.8079720854759216
Epoch: 5807, Batch Gradient Norm: 32.86805851350736
Epoch: 5807, Batch Gradient Norm after: 22.36067772540627
Epoch 5808/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.812583327293396
Epoch: 5808, Batch Gradient Norm: 30.936157845012467
Epoch: 5808, Batch Gradient Norm after: 22.360678242058622
Epoch 5809/10000, Prediction Accuracy = 59.462%, Loss = 0.8078823208808898
Epoch: 5809, Batch Gradient Norm: 32.86617693465307
Epoch: 5809, Batch Gradient Norm after: 22.36068180583381
Epoch 5810/10000, Prediction Accuracy = 59.484%, Loss = 0.8125202059745789
Epoch: 5810, Batch Gradient Norm: 30.928863129857305
Epoch: 5810, Batch Gradient Norm after: 22.360680409683457
Epoch 5811/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8078222393989563
Epoch: 5811, Batch Gradient Norm: 32.86423292686906
Epoch: 5811, Batch Gradient Norm after: 22.360679184893947
Epoch 5812/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8124491930007934
Epoch: 5812, Batch Gradient Norm: 30.922037763691108
Epoch: 5812, Batch Gradient Norm after: 22.36067651147488
Epoch 5813/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8077249884605407
Epoch: 5813, Batch Gradient Norm: 32.85870557430842
Epoch: 5813, Batch Gradient Norm after: 22.36068118024708
Epoch 5814/10000, Prediction Accuracy = 59.476%, Loss = 0.812301504611969
Epoch: 5814, Batch Gradient Norm: 30.924598868011632
Epoch: 5814, Batch Gradient Norm after: 22.36067875940941
Epoch 5815/10000, Prediction Accuracy = 59.476%, Loss = 0.8075845122337342
Epoch: 5815, Batch Gradient Norm: 32.8525323008717
Epoch: 5815, Batch Gradient Norm after: 22.36068018249786
Epoch 5816/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8121615290641785
Epoch: 5816, Batch Gradient Norm: 30.925530991351266
Epoch: 5816, Batch Gradient Norm after: 22.36067713058889
Epoch 5817/10000, Prediction Accuracy = 59.504%, Loss = 0.8074724793434143
Epoch: 5817, Batch Gradient Norm: 32.84788485758596
Epoch: 5817, Batch Gradient Norm after: 22.360678628407523
Epoch 5818/10000, Prediction Accuracy = 59.462%, Loss = 0.8120615601539611
Epoch: 5818, Batch Gradient Norm: 30.92448391011363
Epoch: 5818, Batch Gradient Norm after: 22.3606786970904
Epoch 5819/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.8073769330978393
Epoch: 5819, Batch Gradient Norm: 32.84655252655722
Epoch: 5819, Batch Gradient Norm after: 22.360680150316185
Epoch 5820/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.8119905352592468
Epoch: 5820, Batch Gradient Norm: 30.91612685917049
Epoch: 5820, Batch Gradient Norm after: 22.36067709217348
Epoch 5821/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8073172926902771
Epoch: 5821, Batch Gradient Norm: 32.843451765209245
Epoch: 5821, Batch Gradient Norm after: 22.360680475839946
Epoch 5822/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8119450092315674
Epoch: 5822, Batch Gradient Norm: 30.90723089750818
Epoch: 5822, Batch Gradient Norm after: 22.36067840351799
Epoch 5823/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.8072436332702637
Epoch: 5823, Batch Gradient Norm: 32.83949754536169
Epoch: 5823, Batch Gradient Norm after: 22.36067944624552
Epoch 5824/10000, Prediction Accuracy = 59.472%, Loss = 0.8118322849273681
Epoch: 5824, Batch Gradient Norm: 30.906597326555485
Epoch: 5824, Batch Gradient Norm after: 22.3606804254904
Epoch 5825/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8071079134941102
Epoch: 5825, Batch Gradient Norm: 32.83537953937635
Epoch: 5825, Batch Gradient Norm after: 22.360678567618383
Epoch 5826/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8116659164428711
Epoch: 5826, Batch Gradient Norm: 30.905972222981205
Epoch: 5826, Batch Gradient Norm after: 22.360676523561338
Epoch 5827/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.8069699883460999
Epoch: 5827, Batch Gradient Norm: 32.83183998156409
Epoch: 5827, Batch Gradient Norm after: 22.36067895345195
Epoch 5828/10000, Prediction Accuracy = 59.462%, Loss = 0.8115623235702515
Epoch: 5828, Batch Gradient Norm: 30.90003228876505
Epoch: 5828, Batch Gradient Norm after: 22.360677027331203
Epoch 5829/10000, Prediction Accuracy = 59.504%, Loss = 0.8068662881851196
Epoch: 5829, Batch Gradient Norm: 32.8280130252299
Epoch: 5829, Batch Gradient Norm after: 22.360676870542832
Epoch 5830/10000, Prediction Accuracy = 59.46199999999999%, Loss = 0.8114806056022644
Epoch: 5830, Batch Gradient Norm: 30.89780594643157
Epoch: 5830, Batch Gradient Norm after: 22.36067534204869
Epoch 5831/10000, Prediction Accuracy = 59.46999999999999%, Loss = 0.8067991375923157
Epoch: 5831, Batch Gradient Norm: 32.8244316325422
Epoch: 5831, Batch Gradient Norm after: 22.360680218120155
Epoch 5832/10000, Prediction Accuracy = 59.496%, Loss = 0.8114172339439392
Epoch: 5832, Batch Gradient Norm: 30.896452824453604
Epoch: 5832, Batch Gradient Norm after: 22.36067909571862
Epoch 5833/10000, Prediction Accuracy = 59.438%, Loss = 0.8067543148994446
Epoch: 5833, Batch Gradient Norm: 32.81666515846308
Epoch: 5833, Batch Gradient Norm after: 22.360680264197345
Epoch 5834/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8113297343254089
Epoch: 5834, Batch Gradient Norm: 30.896955072179015
Epoch: 5834, Batch Gradient Norm after: 22.360678116383465
Epoch 5835/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8066514372825623
Epoch: 5835, Batch Gradient Norm: 32.80717545130458
Epoch: 5835, Batch Gradient Norm after: 22.36067965820959
Epoch 5836/10000, Prediction Accuracy = 59.492%, Loss = 0.8111696124076844
Epoch: 5836, Batch Gradient Norm: 30.896658983337915
Epoch: 5836, Batch Gradient Norm after: 22.360676072530527
Epoch 5837/10000, Prediction Accuracy = 59.482000000000006%, Loss = 0.8065109491348267
Epoch: 5837, Batch Gradient Norm: 32.80419563769791
Epoch: 5837, Batch Gradient Norm after: 22.360678428311388
Epoch 5838/10000, Prediction Accuracy = 59.472%, Loss = 0.8110410690307617
Epoch: 5838, Batch Gradient Norm: 30.89239444090167
Epoch: 5838, Batch Gradient Norm after: 22.360677620469193
Epoch 5839/10000, Prediction Accuracy = 59.512%, Loss = 0.8063959240913391
Epoch: 5839, Batch Gradient Norm: 32.80055583348315
Epoch: 5839, Batch Gradient Norm after: 22.360680336894394
Epoch 5840/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.810946810245514
Epoch: 5840, Batch Gradient Norm: 30.89342368904785
Epoch: 5840, Batch Gradient Norm after: 22.36067647158201
Epoch 5841/10000, Prediction Accuracy = 59.488%, Loss = 0.8063065648078919
Epoch: 5841, Batch Gradient Norm: 32.79597769678782
Epoch: 5841, Batch Gradient Norm after: 22.360679445447694
Epoch 5842/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8108677744865418
Epoch: 5842, Batch Gradient Norm: 30.888919247387815
Epoch: 5842, Batch Gradient Norm after: 22.360675828529754
Epoch 5843/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8062568664550781
Epoch: 5843, Batch Gradient Norm: 32.793286381014354
Epoch: 5843, Batch Gradient Norm after: 22.360678489108857
Epoch 5844/10000, Prediction Accuracy = 59.488%, Loss = 0.8108094811439515
Epoch: 5844, Batch Gradient Norm: 30.885291439843083
Epoch: 5844, Batch Gradient Norm after: 22.360678295074045
Epoch 5845/10000, Prediction Accuracy = 59.444%, Loss = 0.806184720993042
Epoch: 5845, Batch Gradient Norm: 32.786874087455196
Epoch: 5845, Batch Gradient Norm after: 22.360678017499527
Epoch 5846/10000, Prediction Accuracy = 59.472%, Loss = 0.8106961488723755
Epoch: 5846, Batch Gradient Norm: 30.884550925414366
Epoch: 5846, Batch Gradient Norm after: 22.36067613253713
Epoch 5847/10000, Prediction Accuracy = 59.472%, Loss = 0.8060517072677612
Epoch: 5847, Batch Gradient Norm: 32.778316726635225
Epoch: 5847, Batch Gradient Norm after: 22.360678602357876
Epoch 5848/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8105384945869446
Epoch: 5848, Batch Gradient Norm: 30.882436877654907
Epoch: 5848, Batch Gradient Norm after: 22.36067760737253
Epoch 5849/10000, Prediction Accuracy = 59.516%, Loss = 0.8059240818023682
Epoch: 5849, Batch Gradient Norm: 32.77417529417946
Epoch: 5849, Batch Gradient Norm after: 22.360679912378288
Epoch 5850/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8104272365570069
Epoch: 5850, Batch Gradient Norm: 30.879926567981354
Epoch: 5850, Batch Gradient Norm after: 22.360676196817597
Epoch 5851/10000, Prediction Accuracy = 59.504000000000005%, Loss = 0.8058198690414429
Epoch: 5851, Batch Gradient Norm: 32.76802234370762
Epoch: 5851, Batch Gradient Norm after: 22.36067740450583
Epoch 5852/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.8103411316871643
Epoch: 5852, Batch Gradient Norm: 30.878904473146726
Epoch: 5852, Batch Gradient Norm after: 22.360678327247378
Epoch 5853/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8057525992393494
Epoch: 5853, Batch Gradient Norm: 32.7667334361738
Epoch: 5853, Batch Gradient Norm after: 22.360679137891527
Epoch 5854/10000, Prediction Accuracy = 59.484%, Loss = 0.8102811813354492
Epoch: 5854, Batch Gradient Norm: 30.87557512847268
Epoch: 5854, Batch Gradient Norm after: 22.360677940832996
Epoch 5855/10000, Prediction Accuracy = 59.45200000000001%, Loss = 0.805705189704895
Epoch: 5855, Batch Gradient Norm: 32.76264237989919
Epoch: 5855, Batch Gradient Norm after: 22.360679607880037
Epoch 5856/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8102023482322693
Epoch: 5856, Batch Gradient Norm: 30.87073141283029
Epoch: 5856, Batch Gradient Norm after: 22.36067992033771
Epoch 5857/10000, Prediction Accuracy = 59.44%, Loss = 0.8055987477302551
Epoch: 5857, Batch Gradient Norm: 32.75643737296034
Epoch: 5857, Batch Gradient Norm after: 22.360681463901837
Epoch 5858/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8100581765174866
Epoch: 5858, Batch Gradient Norm: 30.86758381960982
Epoch: 5858, Batch Gradient Norm after: 22.360676317503508
Epoch 5859/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.8054492592811584
Epoch: 5859, Batch Gradient Norm: 32.75526022015491
Epoch: 5859, Batch Gradient Norm after: 22.36067900817827
Epoch 5860/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8099238634109497
Epoch: 5860, Batch Gradient Norm: 30.862332714815704
Epoch: 5860, Batch Gradient Norm after: 22.360676684840584
Epoch 5861/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.8053299188613892
Epoch: 5861, Batch Gradient Norm: 32.752036644647866
Epoch: 5861, Batch Gradient Norm after: 22.360679336435457
Epoch 5862/10000, Prediction Accuracy = 59.46400000000001%, Loss = 0.8098312258720398
Epoch: 5862, Batch Gradient Norm: 30.858884855741067
Epoch: 5862, Batch Gradient Norm after: 22.360676010189938
Epoch 5863/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.8052318692207336
Epoch: 5863, Batch Gradient Norm: 32.74926174935193
Epoch: 5863, Batch Gradient Norm after: 22.36068094101823
Epoch 5864/10000, Prediction Accuracy = 59.462%, Loss = 0.8097608208656311
Epoch: 5864, Batch Gradient Norm: 30.854029783885235
Epoch: 5864, Batch Gradient Norm after: 22.360678061954435
Epoch 5865/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.805172598361969
Epoch: 5865, Batch Gradient Norm: 32.74704740777945
Epoch: 5865, Batch Gradient Norm after: 22.360680377054045
Epoch 5866/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8097100734710694
Epoch: 5866, Batch Gradient Norm: 30.847180247733768
Epoch: 5866, Batch Gradient Norm after: 22.36067749419572
Epoch 5867/10000, Prediction Accuracy = 59.456%, Loss = 0.8051055908203125
Epoch: 5867, Batch Gradient Norm: 32.74344224182486
Epoch: 5867, Batch Gradient Norm after: 22.36068064320724
Epoch 5868/10000, Prediction Accuracy = 59.48%, Loss = 0.8096070528030396
Epoch: 5868, Batch Gradient Norm: 30.8423473508442
Epoch: 5868, Batch Gradient Norm after: 22.360678780479088
Epoch 5869/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8049696445465088
Epoch: 5869, Batch Gradient Norm: 32.74228799633254
Epoch: 5869, Batch Gradient Norm after: 22.36067938682233
Epoch 5870/10000, Prediction Accuracy = 59.464%, Loss = 0.8094531059265136
Epoch: 5870, Batch Gradient Norm: 30.839393973472415
Epoch: 5870, Batch Gradient Norm after: 22.360678170604213
Epoch 5871/10000, Prediction Accuracy = 59.504000000000005%, Loss = 0.8048240184783936
Epoch: 5871, Batch Gradient Norm: 32.74207006880464
Epoch: 5871, Batch Gradient Norm after: 22.360676977054496
Epoch 5872/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8093452334403992
Epoch: 5872, Batch Gradient Norm: 30.83084968410447
Epoch: 5872, Batch Gradient Norm after: 22.36067669509841
Epoch 5873/10000, Prediction Accuracy = 59.496%, Loss = 0.8047020316123963
Epoch: 5873, Batch Gradient Norm: 32.74217234706203
Epoch: 5873, Batch Gradient Norm after: 22.36067742896128
Epoch 5874/10000, Prediction Accuracy = 59.462%, Loss = 0.8092642664909363
Epoch: 5874, Batch Gradient Norm: 30.824849141036907
Epoch: 5874, Batch Gradient Norm after: 22.360680941585375
Epoch 5875/10000, Prediction Accuracy = 59.492%, Loss = 0.8046220541000366
Epoch: 5875, Batch Gradient Norm: 32.74083739019724
Epoch: 5875, Batch Gradient Norm after: 22.36067884600415
Epoch 5876/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8092086911201477
Epoch: 5876, Batch Gradient Norm: 30.819929762555297
Epoch: 5876, Batch Gradient Norm after: 22.360679186185997
Epoch 5877/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8045604586601257
Epoch: 5877, Batch Gradient Norm: 32.73935010281496
Epoch: 5877, Batch Gradient Norm after: 22.3606775200454
Epoch 5878/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.809144926071167
Epoch: 5878, Batch Gradient Norm: 30.81141403834189
Epoch: 5878, Batch Gradient Norm after: 22.360678038046363
Epoch 5879/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.8044620037078858
Epoch: 5879, Batch Gradient Norm: 32.735416155163556
Epoch: 5879, Batch Gradient Norm after: 22.360680936346128
Epoch 5880/10000, Prediction Accuracy = 59.476%, Loss = 0.8090140700340271
Epoch: 5880, Batch Gradient Norm: 30.807803220412616
Epoch: 5880, Batch Gradient Norm after: 22.360678290500747
Epoch 5881/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.8043161034584045
Epoch: 5881, Batch Gradient Norm: 32.73364577058565
Epoch: 5881, Batch Gradient Norm after: 22.360680251563725
Epoch 5882/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8088825106620788
Epoch: 5882, Batch Gradient Norm: 30.80326928598109
Epoch: 5882, Batch Gradient Norm after: 22.360675697261556
Epoch 5883/10000, Prediction Accuracy = 59.5%, Loss = 0.8041863560676574
Epoch: 5883, Batch Gradient Norm: 32.73010205494957
Epoch: 5883, Batch Gradient Norm after: 22.360679240237275
Epoch 5884/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8087884187698364
Epoch: 5884, Batch Gradient Norm: 30.799062744010087
Epoch: 5884, Batch Gradient Norm after: 22.36067674260382
Epoch 5885/10000, Prediction Accuracy = 59.496%, Loss = 0.804087781906128
Epoch: 5885, Batch Gradient Norm: 32.73028481753672
Epoch: 5885, Batch Gradient Norm after: 22.36068164247352
Epoch 5886/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8087157487869263
Epoch: 5886, Batch Gradient Norm: 30.79711914717775
Epoch: 5886, Batch Gradient Norm after: 22.360677390178882
Epoch 5887/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8040257215499877
Epoch: 5887, Batch Gradient Norm: 32.72471025398539
Epoch: 5887, Batch Gradient Norm after: 22.360678578224018
Epoch 5888/10000, Prediction Accuracy = 59.472%, Loss = 0.8086580872535706
Epoch: 5888, Batch Gradient Norm: 30.794151885185045
Epoch: 5888, Batch Gradient Norm after: 22.360677241014738
Epoch 5889/10000, Prediction Accuracy = 59.45%, Loss = 0.8039683818817138
Epoch: 5889, Batch Gradient Norm: 32.72009149052356
Epoch: 5889, Batch Gradient Norm after: 22.360678566121027
Epoch 5890/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8085575461387634
Epoch: 5890, Batch Gradient Norm: 30.792311201787076
Epoch: 5890, Batch Gradient Norm after: 22.36067611334746
Epoch 5891/10000, Prediction Accuracy = 59.46%, Loss = 0.8038446426391601
Epoch: 5891, Batch Gradient Norm: 32.71477305596007
Epoch: 5891, Batch Gradient Norm after: 22.36067781588072
Epoch 5892/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.8084028601646424
Epoch: 5892, Batch Gradient Norm: 30.791767428969074
Epoch: 5892, Batch Gradient Norm after: 22.36067876750569
Epoch 5893/10000, Prediction Accuracy = 59.516%, Loss = 0.8037113547325134
Epoch: 5893, Batch Gradient Norm: 32.709423850079304
Epoch: 5893, Batch Gradient Norm after: 22.36067761143201
Epoch 5894/10000, Prediction Accuracy = 59.452%, Loss = 0.8082822561264038
Epoch: 5894, Batch Gradient Norm: 30.788258642024868
Epoch: 5894, Batch Gradient Norm after: 22.36067765969833
Epoch 5895/10000, Prediction Accuracy = 59.5%, Loss = 0.8035982370376586
Epoch: 5895, Batch Gradient Norm: 32.706158691825436
Epoch: 5895, Batch Gradient Norm after: 22.360677867897813
Epoch 5896/10000, Prediction Accuracy = 59.472%, Loss = 0.8081981420516968
Epoch: 5896, Batch Gradient Norm: 30.78539501638373
Epoch: 5896, Batch Gradient Norm after: 22.360677888548548
Epoch 5897/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.8035125732421875
Epoch: 5897, Batch Gradient Norm: 32.70590998217712
Epoch: 5897, Batch Gradient Norm after: 22.360677962325724
Epoch 5898/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8081366300582886
Epoch: 5898, Batch Gradient Norm: 30.776480345637932
Epoch: 5898, Batch Gradient Norm after: 22.36067865936749
Epoch 5899/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8034621596336364
Epoch: 5899, Batch Gradient Norm: 32.70437520434397
Epoch: 5899, Batch Gradient Norm after: 22.36067774338638
Epoch 5900/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8080825805664062
Epoch: 5900, Batch Gradient Norm: 30.768769444842796
Epoch: 5900, Batch Gradient Norm after: 22.360678350252165
Epoch 5901/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8033664345741272
Epoch: 5901, Batch Gradient Norm: 32.69928036079066
Epoch: 5901, Batch Gradient Norm after: 22.36067708303081
Epoch 5902/10000, Prediction Accuracy = 59.482000000000006%, Loss = 0.8079554677009583
Epoch: 5902, Batch Gradient Norm: 30.765432703189745
Epoch: 5902, Batch Gradient Norm after: 22.360677252555618
Epoch 5903/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.8032202124595642
Epoch: 5903, Batch Gradient Norm: 32.69796324039756
Epoch: 5903, Batch Gradient Norm after: 22.360678452643192
Epoch 5904/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.8078097462654114
Epoch: 5904, Batch Gradient Norm: 30.763181815025373
Epoch: 5904, Batch Gradient Norm after: 22.360677639269138
Epoch 5905/10000, Prediction Accuracy = 59.53000000000001%, Loss = 0.8030818462371826
Epoch: 5905, Batch Gradient Norm: 32.69596995370102
Epoch: 5905, Batch Gradient Norm after: 22.36067902194257
Epoch 5906/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.8077155113220215
Epoch: 5906, Batch Gradient Norm: 30.75576306128925
Epoch: 5906, Batch Gradient Norm after: 22.360677294082926
Epoch 5907/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.8029787540435791
Epoch: 5907, Batch Gradient Norm: 32.69541356782637
Epoch: 5907, Batch Gradient Norm after: 22.360679406601037
Epoch 5908/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.8076462507247925
Epoch: 5908, Batch Gradient Norm: 30.748796006296818
Epoch: 5908, Batch Gradient Norm after: 22.360680636860337
Epoch 5909/10000, Prediction Accuracy = 59.476%, Loss = 0.8029185891151428
Epoch: 5909, Batch Gradient Norm: 32.69329559697023
Epoch: 5909, Batch Gradient Norm after: 22.36067772342553
Epoch 5910/10000, Prediction Accuracy = 59.472%, Loss = 0.8076032280921936
Epoch: 5910, Batch Gradient Norm: 30.74140074347
Epoch: 5910, Batch Gradient Norm after: 22.36067772220238
Epoch 5911/10000, Prediction Accuracy = 59.464%, Loss = 0.8028595328330994
Epoch: 5911, Batch Gradient Norm: 32.690614887970725
Epoch: 5911, Batch Gradient Norm after: 22.360676858089615
Epoch 5912/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8075053691864014
Epoch: 5912, Batch Gradient Norm: 30.734250309535764
Epoch: 5912, Batch Gradient Norm after: 22.360678025387827
Epoch 5913/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8027291059494018
Epoch: 5913, Batch Gradient Norm: 32.68843911294371
Epoch: 5913, Batch Gradient Norm after: 22.36067946213526
Epoch 5914/10000, Prediction Accuracy = 59.492%, Loss = 0.8073568701744079
Epoch: 5914, Batch Gradient Norm: 30.73048805238484
Epoch: 5914, Batch Gradient Norm after: 22.360676836963638
Epoch 5915/10000, Prediction Accuracy = 59.50600000000001%, Loss = 0.802576732635498
Epoch: 5915, Batch Gradient Norm: 32.68558561515307
Epoch: 5915, Batch Gradient Norm after: 22.360675811171106
Epoch 5916/10000, Prediction Accuracy = 59.46%, Loss = 0.8072447896003723
Epoch: 5916, Batch Gradient Norm: 30.726965425846718
Epoch: 5916, Batch Gradient Norm after: 22.360678530381755
Epoch 5917/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.802460503578186
Epoch: 5917, Batch Gradient Norm: 32.683424519401775
Epoch: 5917, Batch Gradient Norm after: 22.36067822806799
Epoch 5918/10000, Prediction Accuracy = 59.46%, Loss = 0.8071625351905822
Epoch: 5918, Batch Gradient Norm: 30.71844459972626
Epoch: 5918, Batch Gradient Norm after: 22.36067813903872
Epoch 5919/10000, Prediction Accuracy = 59.48%, Loss = 0.8023702025413513
Epoch: 5919, Batch Gradient Norm: 32.68460126060213
Epoch: 5919, Batch Gradient Norm after: 22.360676987541424
Epoch 5920/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8071099162101746
Epoch: 5920, Batch Gradient Norm: 30.71163489716511
Epoch: 5920, Batch Gradient Norm after: 22.36067650578461
Epoch 5921/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8023202180862427
Epoch: 5921, Batch Gradient Norm: 32.68276105008189
Epoch: 5921, Batch Gradient Norm after: 22.36067737472907
Epoch 5922/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8070442318916321
Epoch: 5922, Batch Gradient Norm: 30.705188863846452
Epoch: 5922, Batch Gradient Norm after: 22.36067762081573
Epoch 5923/10000, Prediction Accuracy = 59.456%, Loss = 0.8022204875946045
Epoch: 5923, Batch Gradient Norm: 32.67956537423486
Epoch: 5923, Batch Gradient Norm after: 22.36067860245466
Epoch 5924/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8069165945053101
Epoch: 5924, Batch Gradient Norm: 30.699415252955273
Epoch: 5924, Batch Gradient Norm after: 22.360676614528977
Epoch 5925/10000, Prediction Accuracy = 59.482000000000006%, Loss = 0.8020825862884522
Epoch: 5925, Batch Gradient Norm: 32.675169072551206
Epoch: 5925, Batch Gradient Norm after: 22.360676895139157
Epoch 5926/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8067868113517761
Epoch: 5926, Batch Gradient Norm: 30.700659813453093
Epoch: 5926, Batch Gradient Norm after: 22.360675689879454
Epoch 5927/10000, Prediction Accuracy = 59.504%, Loss = 0.8019631266593933
Epoch: 5927, Batch Gradient Norm: 32.67265203195132
Epoch: 5927, Batch Gradient Norm after: 22.360678691828074
Epoch 5928/10000, Prediction Accuracy = 59.459999999999994%, Loss = 0.8066832780838012
Epoch: 5928, Batch Gradient Norm: 30.697753043619397
Epoch: 5928, Batch Gradient Norm after: 22.360676683951354
Epoch 5929/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.8018623352050781
Epoch: 5929, Batch Gradient Norm: 32.66903578104634
Epoch: 5929, Batch Gradient Norm after: 22.360677804852017
Epoch 5930/10000, Prediction Accuracy = 59.45%, Loss = 0.806608247756958
Epoch: 5930, Batch Gradient Norm: 30.689921205459953
Epoch: 5930, Batch Gradient Norm after: 22.360677692173656
Epoch 5931/10000, Prediction Accuracy = 59.46%, Loss = 0.8018007516860962
Epoch: 5931, Batch Gradient Norm: 32.667927125244695
Epoch: 5931, Batch Gradient Norm after: 22.360677605286888
Epoch 5932/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.8065632462501526
Epoch: 5932, Batch Gradient Norm: 30.679756066326537
Epoch: 5932, Batch Gradient Norm after: 22.36067387991517
Epoch 5933/10000, Prediction Accuracy = 59.468%, Loss = 0.8017309308052063
Epoch: 5933, Batch Gradient Norm: 32.667108831420144
Epoch: 5933, Batch Gradient Norm after: 22.360678638844963
Epoch 5934/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8064829587936402
Epoch: 5934, Batch Gradient Norm: 30.669402944539677
Epoch: 5934, Batch Gradient Norm after: 22.360675896742563
Epoch 5935/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8016018509864807
Epoch: 5935, Batch Gradient Norm: 32.6648625694324
Epoch: 5935, Batch Gradient Norm after: 22.3606798071875
Epoch 5936/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8063401818275452
Epoch: 5936, Batch Gradient Norm: 30.663379934456596
Epoch: 5936, Batch Gradient Norm after: 22.360675204266006
Epoch 5937/10000, Prediction Accuracy = 59.492%, Loss = 0.8014503359794617
Epoch: 5937, Batch Gradient Norm: 32.66659348726029
Epoch: 5937, Batch Gradient Norm after: 22.36067607243054
Epoch 5938/10000, Prediction Accuracy = 59.468%, Loss = 0.8062300205230712
Epoch: 5938, Batch Gradient Norm: 30.65550794583715
Epoch: 5938, Batch Gradient Norm after: 22.360678134113094
Epoch 5939/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.8013231992721558
Epoch: 5939, Batch Gradient Norm: 32.66563480324141
Epoch: 5939, Batch Gradient Norm after: 22.36067853233349
Epoch 5940/10000, Prediction Accuracy = 59.452%, Loss = 0.8061499834060669
Epoch: 5940, Batch Gradient Norm: 30.648372339523075
Epoch: 5940, Batch Gradient Norm after: 22.360675729200054
Epoch 5941/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8012250781059265
Epoch: 5941, Batch Gradient Norm: 32.66451171322684
Epoch: 5941, Batch Gradient Norm after: 22.360678432949125
Epoch 5942/10000, Prediction Accuracy = 59.448%, Loss = 0.8060955882072449
Epoch: 5942, Batch Gradient Norm: 30.64093425484087
Epoch: 5942, Batch Gradient Norm after: 22.360679611550836
Epoch 5943/10000, Prediction Accuracy = 59.459999999999994%, Loss = 0.8011861205101013
Epoch: 5943, Batch Gradient Norm: 32.662878773598344
Epoch: 5943, Batch Gradient Norm after: 22.360676317476766
Epoch 5944/10000, Prediction Accuracy = 59.484%, Loss = 0.8060593485832215
Epoch: 5944, Batch Gradient Norm: 30.628231984432013
Epoch: 5944, Batch Gradient Norm after: 22.360677664745577
Epoch 5945/10000, Prediction Accuracy = 59.462%, Loss = 0.8011126756668091
Epoch: 5945, Batch Gradient Norm: 32.6592921076728
Epoch: 5945, Batch Gradient Norm after: 22.360678337374797
Epoch 5946/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8059505224227905
Epoch: 5946, Batch Gradient Norm: 30.623457864662416
Epoch: 5946, Batch Gradient Norm after: 22.360678867324843
Epoch 5947/10000, Prediction Accuracy = 59.468%, Loss = 0.8009511232376099
Epoch: 5947, Batch Gradient Norm: 32.659119951185914
Epoch: 5947, Batch Gradient Norm after: 22.3606782068639
Epoch 5948/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.8057846665382385
Epoch: 5948, Batch Gradient Norm: 30.62115269524176
Epoch: 5948, Batch Gradient Norm after: 22.360678498203487
Epoch 5949/10000, Prediction Accuracy = 59.524%, Loss = 0.8008041024208069
Epoch: 5949, Batch Gradient Norm: 32.656118477973195
Epoch: 5949, Batch Gradient Norm after: 22.360679513524676
Epoch 5950/10000, Prediction Accuracy = 59.462%, Loss = 0.8056805253028869
Epoch: 5950, Batch Gradient Norm: 30.614051095408225
Epoch: 5950, Batch Gradient Norm after: 22.360679068300843
Epoch 5951/10000, Prediction Accuracy = 59.524%, Loss = 0.8006883382797241
Epoch: 5951, Batch Gradient Norm: 32.65612270938203
Epoch: 5951, Batch Gradient Norm after: 22.360681069616675
Epoch 5952/10000, Prediction Accuracy = 59.443999999999996%, Loss = 0.8056107997894287
Epoch: 5952, Batch Gradient Norm: 30.6037990136921
Epoch: 5952, Batch Gradient Norm after: 22.360677992613088
Epoch 5953/10000, Prediction Accuracy = 59.462%, Loss = 0.8006070256233215
Epoch: 5953, Batch Gradient Norm: 32.657661167484555
Epoch: 5953, Batch Gradient Norm after: 22.360678346326438
Epoch 5954/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.8055706858634949
Epoch: 5954, Batch Gradient Norm: 30.59557253172475
Epoch: 5954, Batch Gradient Norm after: 22.3606762275091
Epoch 5955/10000, Prediction Accuracy = 59.476%, Loss = 0.8005658507347106
Epoch: 5955, Batch Gradient Norm: 32.65575501863445
Epoch: 5955, Batch Gradient Norm after: 22.360679292993073
Epoch 5956/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8055279016494751
Epoch: 5956, Batch Gradient Norm: 30.58458250399643
Epoch: 5956, Batch Gradient Norm after: 22.360677173473483
Epoch 5957/10000, Prediction Accuracy = 59.45%, Loss = 0.800469183921814
Epoch: 5957, Batch Gradient Norm: 32.65170132022711
Epoch: 5957, Batch Gradient Norm after: 22.360677969472583
Epoch 5958/10000, Prediction Accuracy = 59.46%, Loss = 0.8053778648376465
Epoch: 5958, Batch Gradient Norm: 30.584049811833907
Epoch: 5958, Batch Gradient Norm after: 22.360678743508558
Epoch 5959/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8003097772598267
Epoch: 5959, Batch Gradient Norm: 32.650365056065766
Epoch: 5959, Batch Gradient Norm after: 22.36067934277865
Epoch 5960/10000, Prediction Accuracy = 59.443999999999996%, Loss = 0.8052346587181092
Epoch: 5960, Batch Gradient Norm: 30.58089523335564
Epoch: 5960, Batch Gradient Norm after: 22.360678792654852
Epoch 5961/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.8001735210418701
Epoch: 5961, Batch Gradient Norm: 32.64900622016476
Epoch: 5961, Batch Gradient Norm after: 22.36067966353076
Epoch 5962/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.8051460981369019
Epoch: 5962, Batch Gradient Norm: 30.57091207994068
Epoch: 5962, Batch Gradient Norm after: 22.360679005175534
Epoch 5963/10000, Prediction Accuracy = 59.528%, Loss = 0.8000662684440613
Epoch: 5963, Batch Gradient Norm: 32.64950670527853
Epoch: 5963, Batch Gradient Norm after: 22.360676936241752
Epoch 5964/10000, Prediction Accuracy = 59.424%, Loss = 0.8050833821296692
Epoch: 5964, Batch Gradient Norm: 30.56214000241213
Epoch: 5964, Batch Gradient Norm after: 22.360678543023806
Epoch 5965/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.8000011205673218
Epoch: 5965, Batch Gradient Norm: 32.65058137334051
Epoch: 5965, Batch Gradient Norm after: 22.360679005335058
Epoch 5966/10000, Prediction Accuracy = 59.472%, Loss = 0.8050474286079407
Epoch: 5966, Batch Gradient Norm: 30.551802544989467
Epoch: 5966, Batch Gradient Norm after: 22.36067784282267
Epoch 5967/10000, Prediction Accuracy = 59.468%, Loss = 0.7999379515647889
Epoch: 5967, Batch Gradient Norm: 32.652836544952756
Epoch: 5967, Batch Gradient Norm after: 22.360678036543092
Epoch 5968/10000, Prediction Accuracy = 59.484%, Loss = 0.804965615272522
Epoch: 5968, Batch Gradient Norm: 30.538750946045624
Epoch: 5968, Batch Gradient Norm after: 22.360677415669805
Epoch 5969/10000, Prediction Accuracy = 59.462%, Loss = 0.7997907757759094
Epoch: 5969, Batch Gradient Norm: 32.654766662069434
Epoch: 5969, Batch Gradient Norm after: 22.360678529463165
Epoch 5970/10000, Prediction Accuracy = 59.45%, Loss = 0.8048313617706299
Epoch: 5970, Batch Gradient Norm: 30.526796426390632
Epoch: 5970, Batch Gradient Norm after: 22.36067832710938
Epoch 5971/10000, Prediction Accuracy = 59.484%, Loss = 0.7996272206306457
Epoch: 5971, Batch Gradient Norm: 32.658680143508946
Epoch: 5971, Batch Gradient Norm after: 22.360678843294615
Epoch 5972/10000, Prediction Accuracy = 59.456%, Loss = 0.804730761051178
Epoch: 5972, Batch Gradient Norm: 30.51577894788803
Epoch: 5972, Batch Gradient Norm after: 22.360679231404543
Epoch 5973/10000, Prediction Accuracy = 59.55%, Loss = 0.7994914293289185
Epoch: 5973, Batch Gradient Norm: 32.66308019094155
Epoch: 5973, Batch Gradient Norm after: 22.360678297993616
Epoch 5974/10000, Prediction Accuracy = 59.444%, Loss = 0.8046658515930176
Epoch: 5974, Batch Gradient Norm: 30.501542402311262
Epoch: 5974, Batch Gradient Norm after: 22.360678625593604
Epoch 5975/10000, Prediction Accuracy = 59.483999999999995%, Loss = 0.7993890285491944
Epoch: 5975, Batch Gradient Norm: 32.66804439984566
Epoch: 5975, Batch Gradient Norm after: 22.360679365116294
Epoch 5976/10000, Prediction Accuracy = 59.476%, Loss = 0.8046281218528748
Epoch: 5976, Batch Gradient Norm: 30.49054589141076
Epoch: 5976, Batch Gradient Norm after: 22.360679546111466
Epoch 5977/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7993312358856202
Epoch: 5977, Batch Gradient Norm: 32.670837374840914
Epoch: 5977, Batch Gradient Norm after: 22.3606799530727
Epoch 5978/10000, Prediction Accuracy = 59.48%, Loss = 0.804599118232727
Epoch: 5978, Batch Gradient Norm: 30.47790339017998
Epoch: 5978, Batch Gradient Norm after: 22.360680498084147
Epoch 5979/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.799248456954956
Epoch: 5979, Batch Gradient Norm: 32.6708613524842
Epoch: 5979, Batch Gradient Norm after: 22.360679384586117
Epoch 5980/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8044942498207093
Epoch: 5980, Batch Gradient Norm: 30.4686976069628
Epoch: 5980, Batch Gradient Norm after: 22.360679626236553
Epoch 5981/10000, Prediction Accuracy = 59.452%, Loss = 0.7990883111953735
Epoch: 5981, Batch Gradient Norm: 32.67215413633938
Epoch: 5981, Batch Gradient Norm after: 22.360680538093323
Epoch 5982/10000, Prediction Accuracy = 59.45400000000001%, Loss = 0.8043434262275696
Epoch: 5982, Batch Gradient Norm: 30.463561783900413
Epoch: 5982, Batch Gradient Norm after: 22.360679133550097
Epoch 5983/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7989328503608704
Epoch: 5983, Batch Gradient Norm: 32.67382657639188
Epoch: 5983, Batch Gradient Norm after: 22.360678163642003
Epoch 5984/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.8042511224746705
Epoch: 5984, Batch Gradient Norm: 30.45624194942494
Epoch: 5984, Batch Gradient Norm after: 22.360678161246156
Epoch 5985/10000, Prediction Accuracy = 59.52%, Loss = 0.7988224029541016
Epoch: 5985, Batch Gradient Norm: 32.67129785680127
Epoch: 5985, Batch Gradient Norm after: 22.36067851835805
Epoch 5986/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.8041743397712707
Epoch: 5986, Batch Gradient Norm: 30.45197768681646
Epoch: 5986, Batch Gradient Norm after: 22.360679188864015
Epoch 5987/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7987495064735413
Epoch: 5987, Batch Gradient Norm: 32.66945016932437
Epoch: 5987, Batch Gradient Norm after: 22.360677601034997
Epoch 5988/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8041361212730408
Epoch: 5988, Batch Gradient Norm: 30.445639847253897
Epoch: 5988, Batch Gradient Norm after: 22.36067773531986
Epoch 5989/10000, Prediction Accuracy = 59.48%, Loss = 0.7987300276756286
Epoch: 5989, Batch Gradient Norm: 32.66307567618084
Epoch: 5989, Batch Gradient Norm after: 22.360679810347523
Epoch 5990/10000, Prediction Accuracy = 59.496%, Loss = 0.8040881633758545
Epoch: 5990, Batch Gradient Norm: 30.44185403444182
Epoch: 5990, Batch Gradient Norm after: 22.36067590201295
Epoch 5991/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7986434817314148
Epoch: 5991, Batch Gradient Norm: 32.656850616322814
Epoch: 5991, Batch Gradient Norm after: 22.360678782017686
Epoch 5992/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.8039090514183045
Epoch: 5992, Batch Gradient Norm: 30.443725360576625
Epoch: 5992, Batch Gradient Norm after: 22.36067929760284
Epoch 5993/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.7984751224517822
Epoch: 5993, Batch Gradient Norm: 32.65230486105372
Epoch: 5993, Batch Gradient Norm after: 22.360677233063452
Epoch 5994/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8037608027458191
Epoch: 5994, Batch Gradient Norm: 30.445504727678625
Epoch: 5994, Batch Gradient Norm after: 22.3606772858969
Epoch 5995/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7983588576316833
Epoch: 5995, Batch Gradient Norm: 32.64572823559982
Epoch: 5995, Batch Gradient Norm after: 22.360677030403302
Epoch 5996/10000, Prediction Accuracy = 59.484%, Loss = 0.8036604762077332
Epoch: 5996, Batch Gradient Norm: 30.442408946400636
Epoch: 5996, Batch Gradient Norm after: 22.36067984249522
Epoch 5997/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.7982635498046875
Epoch: 5997, Batch Gradient Norm: 32.6395520935246
Epoch: 5997, Batch Gradient Norm after: 22.360677109867265
Epoch 5998/10000, Prediction Accuracy = 59.428%, Loss = 0.8035871267318726
Epoch: 5998, Batch Gradient Norm: 30.441640357449778
Epoch: 5998, Batch Gradient Norm after: 22.36067740806974
Epoch 5999/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7982317924499511
Epoch: 5999, Batch Gradient Norm: 32.63511606783703
Epoch: 5999, Batch Gradient Norm after: 22.36067707123258
Epoch 6000/10000, Prediction Accuracy = 59.50599999999999%, Loss = 0.8035456418991089
Epoch: 6000, Batch Gradient Norm: 30.43988237576548
Epoch: 6000, Batch Gradient Norm after: 22.36067760915107
Epoch 6001/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7981895685195923
Epoch: 6001, Batch Gradient Norm: 32.625977751292034
Epoch: 6001, Batch Gradient Norm after: 22.360678716221862
Epoch 6002/10000, Prediction Accuracy = 59.48599999999999%, Loss = 0.8034376978874207
Epoch: 6002, Batch Gradient Norm: 30.43931859097759
Epoch: 6002, Batch Gradient Norm after: 22.36067840280913
Epoch 6003/10000, Prediction Accuracy = 59.46%, Loss = 0.7980613827705383
Epoch: 6003, Batch Gradient Norm: 32.620911092631644
Epoch: 6003, Batch Gradient Norm after: 22.36067640227913
Epoch 6004/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.803262448310852
Epoch: 6004, Batch Gradient Norm: 30.440735481792803
Epoch: 6004, Batch Gradient Norm after: 22.360680637740288
Epoch 6005/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7979238152503967
Epoch: 6005, Batch Gradient Norm: 32.61395750996496
Epoch: 6005, Batch Gradient Norm after: 22.36067968434996
Epoch 6006/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8031370282173157
Epoch: 6006, Batch Gradient Norm: 30.44019710468911
Epoch: 6006, Batch Gradient Norm after: 22.36067899263193
Epoch 6007/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.797817873954773
Epoch: 6007, Batch Gradient Norm: 32.60948192148307
Epoch: 6007, Batch Gradient Norm after: 22.36067590029655
Epoch 6008/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8030457854270935
Epoch: 6008, Batch Gradient Norm: 30.440129311381412
Epoch: 6008, Batch Gradient Norm after: 22.36067847130399
Epoch 6009/10000, Prediction Accuracy = 59.438%, Loss = 0.7977458477020264
Epoch: 6009, Batch Gradient Norm: 32.60364570409865
Epoch: 6009, Batch Gradient Norm after: 22.360678388267875
Epoch 6010/10000, Prediction Accuracy = 59.452%, Loss = 0.8029819607734681
Epoch: 6010, Batch Gradient Norm: 30.438717622834464
Epoch: 6010, Batch Gradient Norm after: 22.360678518282402
Epoch 6011/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7977248191833496
Epoch: 6011, Batch Gradient Norm: 32.59792049829682
Epoch: 6011, Batch Gradient Norm after: 22.360679302186863
Epoch 6012/10000, Prediction Accuracy = 59.488%, Loss = 0.8029374003410339
Epoch: 6012, Batch Gradient Norm: 30.434353867307344
Epoch: 6012, Batch Gradient Norm after: 22.36067798333342
Epoch 6013/10000, Prediction Accuracy = 59.484%, Loss = 0.7976592063903809
Epoch: 6013, Batch Gradient Norm: 32.58995324458779
Epoch: 6013, Batch Gradient Norm after: 22.360678437056574
Epoch 6014/10000, Prediction Accuracy = 59.49399999999999%, Loss = 0.802793824672699
Epoch: 6014, Batch Gradient Norm: 30.43789017842701
Epoch: 6014, Batch Gradient Norm after: 22.360679789219745
Epoch 6015/10000, Prediction Accuracy = 59.436%, Loss = 0.7975152611732483
Epoch: 6015, Batch Gradient Norm: 32.58310132734945
Epoch: 6015, Batch Gradient Norm after: 22.360676746886952
Epoch 6016/10000, Prediction Accuracy = 59.459999999999994%, Loss = 0.802620244026184
Epoch: 6016, Batch Gradient Norm: 30.4369550317607
Epoch: 6016, Batch Gradient Norm after: 22.360679264185876
Epoch 6017/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.7973872184753418
Epoch: 6017, Batch Gradient Norm: 32.57572249051505
Epoch: 6017, Batch Gradient Norm after: 22.36067740167601
Epoch 6018/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.8025179982185364
Epoch: 6018, Batch Gradient Norm: 30.43880467573383
Epoch: 6018, Batch Gradient Norm after: 22.36067806102488
Epoch 6019/10000, Prediction Accuracy = 59.507999999999996%, Loss = 0.7972951531410217
Epoch: 6019, Batch Gradient Norm: 32.57050920622208
Epoch: 6019, Batch Gradient Norm after: 22.360677962630813
Epoch 6020/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.8024316668510437
Epoch: 6020, Batch Gradient Norm: 30.43590620315541
Epoch: 6020, Batch Gradient Norm after: 22.360677584368112
Epoch 6021/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7972460389137268
Epoch: 6021, Batch Gradient Norm: 32.56850828407163
Epoch: 6021, Batch Gradient Norm after: 22.36067755053742
Epoch 6022/10000, Prediction Accuracy = 59.472%, Loss = 0.8023946523666382
Epoch: 6022, Batch Gradient Norm: 30.429853879719513
Epoch: 6022, Batch Gradient Norm after: 22.36067875604037
Epoch 6023/10000, Prediction Accuracy = 59.50600000000001%, Loss = 0.7972180128097535
Epoch: 6023, Batch Gradient Norm: 32.562004117906824
Epoch: 6023, Batch Gradient Norm after: 22.360680898795174
Epoch 6024/10000, Prediction Accuracy = 59.476%, Loss = 0.8023317098617554
Epoch: 6024, Batch Gradient Norm: 30.4284423393445
Epoch: 6024, Batch Gradient Norm after: 22.360678280820583
Epoch 6025/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7971122145652771
Epoch: 6025, Batch Gradient Norm: 32.554527407182256
Epoch: 6025, Batch Gradient Norm after: 22.36067776217587
Epoch 6026/10000, Prediction Accuracy = 59.462%, Loss = 0.8021496891975403
Epoch: 6026, Batch Gradient Norm: 30.427773954811872
Epoch: 6026, Batch Gradient Norm after: 22.360679200025317
Epoch 6027/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7969497323036194
Epoch: 6027, Batch Gradient Norm: 32.55105587525159
Epoch: 6027, Batch Gradient Norm after: 22.360680071536503
Epoch 6028/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.8020116806030273
Epoch: 6028, Batch Gradient Norm: 30.428321422141806
Epoch: 6028, Batch Gradient Norm after: 22.360678079141202
Epoch 6029/10000, Prediction Accuracy = 59.55%, Loss = 0.7968336462974548
Epoch: 6029, Batch Gradient Norm: 32.54413892287039
Epoch: 6029, Batch Gradient Norm after: 22.36068017452279
Epoch 6030/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.8019183039665222
Epoch: 6030, Batch Gradient Norm: 30.42776354660876
Epoch: 6030, Batch Gradient Norm after: 22.360677721221865
Epoch 6031/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7967550039291382
Epoch: 6031, Batch Gradient Norm: 32.53986017718905
Epoch: 6031, Batch Gradient Norm after: 22.360679803922864
Epoch 6032/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.8018478989601135
Epoch: 6032, Batch Gradient Norm: 30.428716979071886
Epoch: 6032, Batch Gradient Norm after: 22.360678576953017
Epoch 6033/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7967369556427002
Epoch: 6033, Batch Gradient Norm: 32.53509364211381
Epoch: 6033, Batch Gradient Norm after: 22.360677325954285
Epoch 6034/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.801805567741394
Epoch: 6034, Batch Gradient Norm: 30.42326590756274
Epoch: 6034, Batch Gradient Norm after: 22.36068099896759
Epoch 6035/10000, Prediction Accuracy = 59.508%, Loss = 0.7966866374015809
Epoch: 6035, Batch Gradient Norm: 32.527538667862224
Epoch: 6035, Batch Gradient Norm after: 22.360678447072605
Epoch 6036/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8016823291778564
Epoch: 6036, Batch Gradient Norm: 30.42332454428486
Epoch: 6036, Batch Gradient Norm after: 22.3606790957838
Epoch 6037/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7965412259101867
Epoch: 6037, Batch Gradient Norm: 32.52099465524077
Epoch: 6037, Batch Gradient Norm after: 22.360679073519872
Epoch 6038/10000, Prediction Accuracy = 59.45%, Loss = 0.8015112042427063
Epoch: 6038, Batch Gradient Norm: 30.425108595085387
Epoch: 6038, Batch Gradient Norm after: 22.360678099476
Epoch 6039/10000, Prediction Accuracy = 59.519999999999996%, Loss = 0.7964119195938111
Epoch: 6039, Batch Gradient Norm: 32.514099011360884
Epoch: 6039, Batch Gradient Norm after: 22.36067794570042
Epoch 6040/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.8013920307159423
Epoch: 6040, Batch Gradient Norm: 30.423346612581884
Epoch: 6040, Batch Gradient Norm after: 22.360678002351865
Epoch 6041/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7963001370429993
Epoch: 6041, Batch Gradient Norm: 32.51343761570464
Epoch: 6041, Batch Gradient Norm after: 22.360679404598876
Epoch 6042/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.8013148069381714
Epoch: 6042, Batch Gradient Norm: 30.416354702675005
Epoch: 6042, Batch Gradient Norm after: 22.36067842695222
Epoch 6043/10000, Prediction Accuracy = 59.462%, Loss = 0.7962339758872986
Epoch: 6043, Batch Gradient Norm: 32.51523861836211
Epoch: 6043, Batch Gradient Norm after: 22.360679062043253
Epoch 6044/10000, Prediction Accuracy = 59.444%, Loss = 0.8012738585472107
Epoch: 6044, Batch Gradient Norm: 30.40822171758682
Epoch: 6044, Batch Gradient Norm after: 22.360676922718874
Epoch 6045/10000, Prediction Accuracy = 59.504000000000005%, Loss = 0.7961948275566101
Epoch: 6045, Batch Gradient Norm: 32.51335525815051
Epoch: 6045, Batch Gradient Norm after: 22.360678697616592
Epoch 6046/10000, Prediction Accuracy = 59.496%, Loss = 0.8012484908103943
Epoch: 6046, Batch Gradient Norm: 30.39969186257183
Epoch: 6046, Batch Gradient Norm after: 22.360677976683963
Epoch 6047/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7961020231246948
Epoch: 6047, Batch Gradient Norm: 32.5104906360887
Epoch: 6047, Batch Gradient Norm after: 22.360679882483613
Epoch 6048/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.8010891914367676
Epoch: 6048, Batch Gradient Norm: 30.391828307051462
Epoch: 6048, Batch Gradient Norm after: 22.36067774282354
Epoch 6049/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.7959282755851745
Epoch: 6049, Batch Gradient Norm: 32.50982774237954
Epoch: 6049, Batch Gradient Norm after: 22.360678022723274
Epoch 6050/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.8009576559066772
Epoch: 6050, Batch Gradient Norm: 30.386790967134022
Epoch: 6050, Batch Gradient Norm after: 22.36067700106252
Epoch 6051/10000, Prediction Accuracy = 59.54%, Loss = 0.7957880735397339
Epoch: 6051, Batch Gradient Norm: 32.51110525564666
Epoch: 6051, Batch Gradient Norm after: 22.360678937988226
Epoch 6052/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.8008826017379761
Epoch: 6052, Batch Gradient Norm: 30.375443025982275
Epoch: 6052, Batch Gradient Norm after: 22.360678051971643
Epoch 6053/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7956768870353699
Epoch: 6053, Batch Gradient Norm: 32.512373242549906
Epoch: 6053, Batch Gradient Norm after: 22.360677176508435
Epoch 6054/10000, Prediction Accuracy = 59.444%, Loss = 0.8008293747901917
Epoch: 6054, Batch Gradient Norm: 30.366307413649242
Epoch: 6054, Batch Gradient Norm after: 22.360678378411805
Epoch 6055/10000, Prediction Accuracy = 59.48%, Loss = 0.795633065700531
Epoch: 6055, Batch Gradient Norm: 32.512499276302044
Epoch: 6055, Batch Gradient Norm after: 22.360680217269103
Epoch 6056/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.8008033514022828
Epoch: 6056, Batch Gradient Norm: 30.35916190137618
Epoch: 6056, Batch Gradient Norm after: 22.360679574817866
Epoch 6057/10000, Prediction Accuracy = 59.52199999999999%, Loss = 0.7955900311470032
Epoch: 6057, Batch Gradient Norm: 32.50707054527475
Epoch: 6057, Batch Gradient Norm after: 22.36068031796947
Epoch 6058/10000, Prediction Accuracy = 59.50599999999999%, Loss = 0.8007139801979065
Epoch: 6058, Batch Gradient Norm: 30.35449413868505
Epoch: 6058, Batch Gradient Norm after: 22.360679220564105
Epoch 6059/10000, Prediction Accuracy = 59.48%, Loss = 0.795458459854126
Epoch: 6059, Batch Gradient Norm: 32.50276827396375
Epoch: 6059, Batch Gradient Norm after: 22.360678950505665
Epoch 6060/10000, Prediction Accuracy = 59.448%, Loss = 0.8005443453788758
Epoch: 6060, Batch Gradient Norm: 30.35127616110064
Epoch: 6060, Batch Gradient Norm after: 22.36068045950193
Epoch 6061/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.795310628414154
Epoch: 6061, Batch Gradient Norm: 32.499058650118656
Epoch: 6061, Batch Gradient Norm after: 22.36067738289411
Epoch 6062/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.8004287719726563
Epoch: 6062, Batch Gradient Norm: 30.35004334139406
Epoch: 6062, Batch Gradient Norm after: 22.360677597833302
Epoch 6063/10000, Prediction Accuracy = 59.540000000000006%, Loss = 0.7951971411705017
Epoch: 6063, Batch Gradient Norm: 32.49539439595167
Epoch: 6063, Batch Gradient Norm after: 22.360676972023185
Epoch 6064/10000, Prediction Accuracy = 59.492%, Loss = 0.8003423810005188
Epoch: 6064, Batch Gradient Norm: 30.34558714946175
Epoch: 6064, Batch Gradient Norm after: 22.360680503423367
Epoch 6065/10000, Prediction Accuracy = 59.462%, Loss = 0.7951197147369384
Epoch: 6065, Batch Gradient Norm: 32.493407623526714
Epoch: 6065, Batch Gradient Norm after: 22.36067735018225
Epoch 6066/10000, Prediction Accuracy = 59.452%, Loss = 0.8002929925918579
Epoch: 6066, Batch Gradient Norm: 30.341534872598704
Epoch: 6066, Batch Gradient Norm after: 22.36067857739938
Epoch 6067/10000, Prediction Accuracy = 59.50599999999999%, Loss = 0.7950850367546082
Epoch: 6067, Batch Gradient Norm: 32.48745182523582
Epoch: 6067, Batch Gradient Norm after: 22.360678812600725
Epoch 6068/10000, Prediction Accuracy = 59.504%, Loss = 0.8002477407455444
Epoch: 6068, Batch Gradient Norm: 30.338040716509607
Epoch: 6068, Batch Gradient Norm after: 22.360678584487555
Epoch 6069/10000, Prediction Accuracy = 59.5%, Loss = 0.7950145363807678
Epoch: 6069, Batch Gradient Norm: 32.482279939946174
Epoch: 6069, Batch Gradient Norm after: 22.360678760035476
Epoch 6070/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.8001165866851807
Epoch: 6070, Batch Gradient Norm: 30.331994142066375
Epoch: 6070, Batch Gradient Norm after: 22.36067869880841
Epoch 6071/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.7948691725730896
Epoch: 6071, Batch Gradient Norm: 32.48081983539603
Epoch: 6071, Batch Gradient Norm after: 22.360679047935747
Epoch 6072/10000, Prediction Accuracy = 59.46%, Loss = 0.7999697804450989
Epoch: 6072, Batch Gradient Norm: 30.329067012204973
Epoch: 6072, Batch Gradient Norm after: 22.360678948455025
Epoch 6073/10000, Prediction Accuracy = 59.524%, Loss = 0.7947305679321289
Epoch: 6073, Batch Gradient Norm: 32.47848189288981
Epoch: 6073, Batch Gradient Norm after: 22.360678139274626
Epoch 6074/10000, Prediction Accuracy = 59.5%, Loss = 0.799868905544281
Epoch: 6074, Batch Gradient Norm: 30.323879401192453
Epoch: 6074, Batch Gradient Norm after: 22.360680552201753
Epoch 6075/10000, Prediction Accuracy = 59.54%, Loss = 0.7946205973625183
Epoch: 6075, Batch Gradient Norm: 32.47627829356571
Epoch: 6075, Batch Gradient Norm after: 22.36067835816501
Epoch 6076/10000, Prediction Accuracy = 59.472%, Loss = 0.7998035788536072
Epoch: 6076, Batch Gradient Norm: 30.314698704424575
Epoch: 6076, Batch Gradient Norm after: 22.360677667790558
Epoch 6077/10000, Prediction Accuracy = 59.46%, Loss = 0.7945566058158875
Epoch: 6077, Batch Gradient Norm: 32.47811748909704
Epoch: 6077, Batch Gradient Norm after: 22.360680518731037
Epoch 6078/10000, Prediction Accuracy = 59.45400000000001%, Loss = 0.7997729301452636
Epoch: 6078, Batch Gradient Norm: 30.307339867415866
Epoch: 6078, Batch Gradient Norm after: 22.360678998118754
Epoch 6079/10000, Prediction Accuracy = 59.5%, Loss = 0.7945220708847046
Epoch: 6079, Batch Gradient Norm: 32.47449464681738
Epoch: 6079, Batch Gradient Norm after: 22.360680659454076
Epoch 6080/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7997373461723327
Epoch: 6080, Batch Gradient Norm: 30.299904325180393
Epoch: 6080, Batch Gradient Norm after: 22.36067608274091
Epoch 6081/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.7944167017936706
Epoch: 6081, Batch Gradient Norm: 32.469504252400334
Epoch: 6081, Batch Gradient Norm after: 22.360680044499706
Epoch 6082/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7995709180831909
Epoch: 6082, Batch Gradient Norm: 30.298840548046083
Epoch: 6082, Batch Gradient Norm after: 22.36067934506426
Epoch 6083/10000, Prediction Accuracy = 59.492%, Loss = 0.7942575335502624
Epoch: 6083, Batch Gradient Norm: 32.46507591317666
Epoch: 6083, Batch Gradient Norm after: 22.360677565824183
Epoch 6084/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7994190573692321
Epoch: 6084, Batch Gradient Norm: 30.297376675574622
Epoch: 6084, Batch Gradient Norm after: 22.360678734221963
Epoch 6085/10000, Prediction Accuracy = 59.524%, Loss = 0.7941285014152527
Epoch: 6085, Batch Gradient Norm: 32.46056162623656
Epoch: 6085, Batch Gradient Norm after: 22.360677899073398
Epoch 6086/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7993348360061645
Epoch: 6086, Batch Gradient Norm: 30.29531143325919
Epoch: 6086, Batch Gradient Norm after: 22.36067688847694
Epoch 6087/10000, Prediction Accuracy = 59.5%, Loss = 0.7940392971038819
Epoch: 6087, Batch Gradient Norm: 32.459146075672486
Epoch: 6087, Batch Gradient Norm after: 22.36067879761351
Epoch 6088/10000, Prediction Accuracy = 59.46%, Loss = 0.7992673277854919
Epoch: 6088, Batch Gradient Norm: 30.293304612546333
Epoch: 6088, Batch Gradient Norm after: 22.360678235835294
Epoch 6089/10000, Prediction Accuracy = 59.52%, Loss = 0.7939963936805725
Epoch: 6089, Batch Gradient Norm: 32.45633474474499
Epoch: 6089, Batch Gradient Norm after: 22.360678014693235
Epoch 6090/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7992313504219055
Epoch: 6090, Batch Gradient Norm: 30.29015796383416
Epoch: 6090, Batch Gradient Norm after: 22.360679266077547
Epoch 6091/10000, Prediction Accuracy = 59.516%, Loss = 0.7939639091491699
Epoch: 6091, Batch Gradient Norm: 32.451627504259164
Epoch: 6091, Batch Gradient Norm after: 22.360676917365506
Epoch 6092/10000, Prediction Accuracy = 59.496%, Loss = 0.7991443991661071
Epoch: 6092, Batch Gradient Norm: 30.285673554784204
Epoch: 6092, Batch Gradient Norm after: 22.360676766831567
Epoch 6093/10000, Prediction Accuracy = 59.528%, Loss = 0.7938380718231202
Epoch: 6093, Batch Gradient Norm: 32.44620544147516
Epoch: 6093, Batch Gradient Norm after: 22.360680320152124
Epoch 6094/10000, Prediction Accuracy = 59.46%, Loss = 0.7989755272865295
Epoch: 6094, Batch Gradient Norm: 30.2833391002511
Epoch: 6094, Batch Gradient Norm after: 22.36067737081759
Epoch 6095/10000, Prediction Accuracy = 59.52%, Loss = 0.7936866998672485
Epoch: 6095, Batch Gradient Norm: 32.44405024409727
Epoch: 6095, Batch Gradient Norm after: 22.360680091503333
Epoch 6096/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.798848831653595
Epoch: 6096, Batch Gradient Norm: 30.27962132320405
Epoch: 6096, Batch Gradient Norm after: 22.360674376447424
Epoch 6097/10000, Prediction Accuracy = 59.52%, Loss = 0.7935696601867676
Epoch: 6097, Batch Gradient Norm: 32.4412888993693
Epoch: 6097, Batch Gradient Norm after: 22.36067817573594
Epoch 6098/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.7987731456756592
Epoch: 6098, Batch Gradient Norm: 30.273101713602475
Epoch: 6098, Batch Gradient Norm after: 22.36067592987075
Epoch 6099/10000, Prediction Accuracy = 59.488%, Loss = 0.7934838652610778
Epoch: 6099, Batch Gradient Norm: 32.442991539205856
Epoch: 6099, Batch Gradient Norm after: 22.360677745662535
Epoch 6100/10000, Prediction Accuracy = 59.462%, Loss = 0.7987333297729492
Epoch: 6100, Batch Gradient Norm: 30.265321022121697
Epoch: 6100, Batch Gradient Norm after: 22.360679205082366
Epoch 6101/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7934504866600036
Epoch: 6101, Batch Gradient Norm: 32.44269682916422
Epoch: 6101, Batch Gradient Norm after: 22.360680945966966
Epoch 6102/10000, Prediction Accuracy = 59.504000000000005%, Loss = 0.7987108349800109
Epoch: 6102, Batch Gradient Norm: 30.25721937570354
Epoch: 6102, Batch Gradient Norm after: 22.36067959685327
Epoch 6103/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.793383264541626
Epoch: 6103, Batch Gradient Norm: 32.440488271096775
Epoch: 6103, Batch Gradient Norm after: 22.360680657307462
Epoch 6104/10000, Prediction Accuracy = 59.492%, Loss = 0.7985793948173523
Epoch: 6104, Batch Gradient Norm: 30.252828261901605
Epoch: 6104, Batch Gradient Norm after: 22.3606771422725
Epoch 6105/10000, Prediction Accuracy = 59.52%, Loss = 0.7932220816612243
Epoch: 6105, Batch Gradient Norm: 32.437412169137474
Epoch: 6105, Batch Gradient Norm after: 22.360679475859346
Epoch 6106/10000, Prediction Accuracy = 59.476%, Loss = 0.7984208106994629
Epoch: 6106, Batch Gradient Norm: 30.24987052052624
Epoch: 6106, Batch Gradient Norm after: 22.36067887514518
Epoch 6107/10000, Prediction Accuracy = 59.52%, Loss = 0.7930791735649109
Epoch: 6107, Batch Gradient Norm: 32.43577509859075
Epoch: 6107, Batch Gradient Norm after: 22.360678645552085
Epoch 6108/10000, Prediction Accuracy = 59.524%, Loss = 0.7983249068260193
Epoch: 6108, Batch Gradient Norm: 30.243368169289944
Epoch: 6108, Batch Gradient Norm after: 22.360678631241804
Epoch 6109/10000, Prediction Accuracy = 59.504000000000005%, Loss = 0.7929720997810363
Epoch: 6109, Batch Gradient Norm: 32.437217769471694
Epoch: 6109, Batch Gradient Norm after: 22.360677921905147
Epoch 6110/10000, Prediction Accuracy = 59.484%, Loss = 0.7982585906982422
Epoch: 6110, Batch Gradient Norm: 30.23691001000366
Epoch: 6110, Batch Gradient Norm after: 22.360676860618256
Epoch 6111/10000, Prediction Accuracy = 59.508%, Loss = 0.7929120540618897
Epoch: 6111, Batch Gradient Norm: 32.436822226371795
Epoch: 6111, Batch Gradient Norm after: 22.360680176242518
Epoch 6112/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.7982312917709351
Epoch: 6112, Batch Gradient Norm: 30.23005194722105
Epoch: 6112, Batch Gradient Norm after: 22.360677938112683
Epoch 6113/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7928972482681275
Epoch: 6113, Batch Gradient Norm: 32.43119002850028
Epoch: 6113, Batch Gradient Norm after: 22.360678059252997
Epoch 6114/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.7981882333755493
Epoch: 6114, Batch Gradient Norm: 30.223725878979927
Epoch: 6114, Batch Gradient Norm after: 22.36067875793938
Epoch 6115/10000, Prediction Accuracy = 59.512%, Loss = 0.7927968025207519
Epoch: 6115, Batch Gradient Norm: 32.426011028171835
Epoch: 6115, Batch Gradient Norm after: 22.360678101074132
Epoch 6116/10000, Prediction Accuracy = 59.484%, Loss = 0.7980144381523132
Epoch: 6116, Batch Gradient Norm: 30.22483660091766
Epoch: 6116, Batch Gradient Norm after: 22.36067857510333
Epoch 6117/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7926334023475647
Epoch: 6117, Batch Gradient Norm: 32.42134529115589
Epoch: 6117, Batch Gradient Norm after: 22.36067971334513
Epoch 6118/10000, Prediction Accuracy = 59.5%, Loss = 0.7978576302528382
Epoch: 6118, Batch Gradient Norm: 30.22423524116976
Epoch: 6118, Batch Gradient Norm after: 22.360675918571147
Epoch 6119/10000, Prediction Accuracy = 59.516%, Loss = 0.7925133466720581
Epoch: 6119, Batch Gradient Norm: 32.416568794845716
Epoch: 6119, Batch Gradient Norm after: 22.360678182373178
Epoch 6120/10000, Prediction Accuracy = 59.52%, Loss = 0.7977729082107544
Epoch: 6120, Batch Gradient Norm: 30.220295352324015
Epoch: 6120, Batch Gradient Norm after: 22.36067657486863
Epoch 6121/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7924194097518921
Epoch: 6121, Batch Gradient Norm: 32.413736538478226
Epoch: 6121, Batch Gradient Norm after: 22.360677319843873
Epoch 6122/10000, Prediction Accuracy = 59.5%, Loss = 0.7977145910263062
Epoch: 6122, Batch Gradient Norm: 30.215797854511685
Epoch: 6122, Batch Gradient Norm after: 22.36067887294394
Epoch 6123/10000, Prediction Accuracy = 59.52%, Loss = 0.7923847675323487
Epoch: 6123, Batch Gradient Norm: 32.411798909972894
Epoch: 6123, Batch Gradient Norm after: 22.360679976849283
Epoch 6124/10000, Prediction Accuracy = 59.488%, Loss = 0.7976891756057739
Epoch: 6124, Batch Gradient Norm: 30.211490070585736
Epoch: 6124, Batch Gradient Norm after: 22.360678620973516
Epoch 6125/10000, Prediction Accuracy = 59.536%, Loss = 0.7923502564430237
Epoch: 6125, Batch Gradient Norm: 32.40721937417803
Epoch: 6125, Batch Gradient Norm after: 22.360677974590637
Epoch 6126/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7975961446762085
Epoch: 6126, Batch Gradient Norm: 30.20974806607359
Epoch: 6126, Batch Gradient Norm after: 22.360678657543122
Epoch 6127/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.7922215938568116
Epoch: 6127, Batch Gradient Norm: 32.39859278435312
Epoch: 6127, Batch Gradient Norm after: 22.36067997899173
Epoch 6128/10000, Prediction Accuracy = 59.48%, Loss = 0.7974037647247314
Epoch: 6128, Batch Gradient Norm: 30.21568255463588
Epoch: 6128, Batch Gradient Norm after: 22.360678228014432
Epoch 6129/10000, Prediction Accuracy = 59.532%, Loss = 0.7920847654342651
Epoch: 6129, Batch Gradient Norm: 32.39283749728615
Epoch: 6129, Batch Gradient Norm after: 22.360680488191054
Epoch 6130/10000, Prediction Accuracy = 59.516%, Loss = 0.7972728729248046
Epoch: 6130, Batch Gradient Norm: 30.217791339675554
Epoch: 6130, Batch Gradient Norm after: 22.36067704390464
Epoch 6131/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7919838070869446
Epoch: 6131, Batch Gradient Norm: 32.38579326110766
Epoch: 6131, Batch Gradient Norm after: 22.360676969192255
Epoch 6132/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7971869826316833
Epoch: 6132, Batch Gradient Norm: 30.214794163608783
Epoch: 6132, Batch Gradient Norm after: 22.360677021278818
Epoch 6133/10000, Prediction Accuracy = 59.54%, Loss = 0.7919142246246338
Epoch: 6133, Batch Gradient Norm: 32.38115459307283
Epoch: 6133, Batch Gradient Norm after: 22.36067887986589
Epoch 6134/10000, Prediction Accuracy = 59.488%, Loss = 0.797135841846466
Epoch: 6134, Batch Gradient Norm: 30.21437324503583
Epoch: 6134, Batch Gradient Norm after: 22.36067750332503
Epoch 6135/10000, Prediction Accuracy = 59.516%, Loss = 0.7919004201889038
Epoch: 6135, Batch Gradient Norm: 32.37349822120005
Epoch: 6135, Batch Gradient Norm after: 22.360679003898603
Epoch 6136/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7970992207527161
Epoch: 6136, Batch Gradient Norm: 30.209985287079515
Epoch: 6136, Batch Gradient Norm after: 22.360677651640806
Epoch 6137/10000, Prediction Accuracy = 59.53399999999999%, Loss = 0.7918363213539124
Epoch: 6137, Batch Gradient Norm: 32.367383109833696
Epoch: 6137, Batch Gradient Norm after: 22.36067811362125
Epoch 6138/10000, Prediction Accuracy = 59.488%, Loss = 0.7969604372978211
Epoch: 6138, Batch Gradient Norm: 30.20901246802287
Epoch: 6138, Batch Gradient Norm after: 22.36067833127259
Epoch 6139/10000, Prediction Accuracy = 59.544%, Loss = 0.7916818022727966
Epoch: 6139, Batch Gradient Norm: 32.363730933297894
Epoch: 6139, Batch Gradient Norm after: 22.360679586703466
Epoch 6140/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7967997908592224
Epoch: 6140, Batch Gradient Norm: 30.208215883539
Epoch: 6140, Batch Gradient Norm after: 22.36067860253586
Epoch 6141/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7915547370910645
Epoch: 6141, Batch Gradient Norm: 32.36015791503815
Epoch: 6141, Batch Gradient Norm after: 22.360678178635652
Epoch 6142/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7966975569725037
Epoch: 6142, Batch Gradient Norm: 30.20709220275279
Epoch: 6142, Batch Gradient Norm after: 22.36067855253756
Epoch 6143/10000, Prediction Accuracy = 59.528%, Loss = 0.7914569973945618
Epoch: 6143, Batch Gradient Norm: 32.35358008586237
Epoch: 6143, Batch Gradient Norm after: 22.360680118152814
Epoch 6144/10000, Prediction Accuracy = 59.511999999999986%, Loss = 0.7966165781021118
Epoch: 6144, Batch Gradient Norm: 30.204119059212466
Epoch: 6144, Batch Gradient Norm after: 22.36068079328207
Epoch 6145/10000, Prediction Accuracy = 59.536000000000016%, Loss = 0.791400671005249
Epoch: 6145, Batch Gradient Norm: 32.35393016191831
Epoch: 6145, Batch Gradient Norm after: 22.360678936658744
Epoch 6146/10000, Prediction Accuracy = 59.476%, Loss = 0.7965817809104919
Epoch: 6146, Batch Gradient Norm: 30.20096462713721
Epoch: 6146, Batch Gradient Norm after: 22.360678165582133
Epoch 6147/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7913788676261901
Epoch: 6147, Batch Gradient Norm: 32.34880154790483
Epoch: 6147, Batch Gradient Norm after: 22.360679451253937
Epoch 6148/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7965253710746765
Epoch: 6148, Batch Gradient Norm: 30.196325383512157
Epoch: 6148, Batch Gradient Norm after: 22.360678298815532
Epoch 6149/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7912824034690857
Epoch: 6149, Batch Gradient Norm: 32.344265095106714
Epoch: 6149, Batch Gradient Norm after: 22.360679268310683
Epoch 6150/10000, Prediction Accuracy = 59.476%, Loss = 0.7963652849197388
Epoch: 6150, Batch Gradient Norm: 30.19740553728474
Epoch: 6150, Batch Gradient Norm after: 22.360676963206014
Epoch 6151/10000, Prediction Accuracy = 59.536%, Loss = 0.7911337971687317
Epoch: 6151, Batch Gradient Norm: 32.33652855772813
Epoch: 6151, Batch Gradient Norm after: 22.360679368015873
Epoch 6152/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7962089419364929
Epoch: 6152, Batch Gradient Norm: 30.19964074330012
Epoch: 6152, Batch Gradient Norm after: 22.360679471359635
Epoch 6153/10000, Prediction Accuracy = 59.540000000000006%, Loss = 0.7910200238227845
Epoch: 6153, Batch Gradient Norm: 32.330781481714226
Epoch: 6153, Batch Gradient Norm after: 22.360679490020704
Epoch 6154/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.7961167812347412
Epoch: 6154, Batch Gradient Norm: 30.1996085662717
Epoch: 6154, Batch Gradient Norm after: 22.36067898325114
Epoch 6155/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7909371137619019
Epoch: 6155, Batch Gradient Norm: 32.32558201008261
Epoch: 6155, Batch Gradient Norm after: 22.36067947905893
Epoch 6156/10000, Prediction Accuracy = 59.52%, Loss = 0.7960523962974548
Epoch: 6156, Batch Gradient Norm: 30.19746682839075
Epoch: 6156, Batch Gradient Norm after: 22.3606792921069
Epoch 6157/10000, Prediction Accuracy = 59.54%, Loss = 0.7909017086029053
Epoch: 6157, Batch Gradient Norm: 32.324384223689435
Epoch: 6157, Batch Gradient Norm after: 22.360678693574116
Epoch 6158/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7960217833518982
Epoch: 6158, Batch Gradient Norm: 30.19306554150077
Epoch: 6158, Batch Gradient Norm after: 22.36067851316258
Epoch 6159/10000, Prediction Accuracy = 59.556%, Loss = 0.7908671736717224
Epoch: 6159, Batch Gradient Norm: 32.318315070835936
Epoch: 6159, Batch Gradient Norm after: 22.360679190404092
Epoch 6160/10000, Prediction Accuracy = 59.5%, Loss = 0.7959196090698242
Epoch: 6160, Batch Gradient Norm: 30.186407185666912
Epoch: 6160, Batch Gradient Norm after: 22.360679985056716
Epoch 6161/10000, Prediction Accuracy = 59.55%, Loss = 0.7907341241836547
Epoch: 6161, Batch Gradient Norm: 32.31827660720341
Epoch: 6161, Batch Gradient Norm after: 22.36067913087797
Epoch 6162/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7957618236541748
Epoch: 6162, Batch Gradient Norm: 30.182871125903226
Epoch: 6162, Batch Gradient Norm after: 22.360680391447676
Epoch 6163/10000, Prediction Accuracy = 59.548%, Loss = 0.7905868530273438
Epoch: 6163, Batch Gradient Norm: 32.314574815904045
Epoch: 6163, Batch Gradient Norm after: 22.36068010689675
Epoch 6164/10000, Prediction Accuracy = 59.54%, Loss = 0.7956424713134765
Epoch: 6164, Batch Gradient Norm: 30.179617517969085
Epoch: 6164, Batch Gradient Norm after: 22.360677767433913
Epoch 6165/10000, Prediction Accuracy = 59.56%, Loss = 0.7904755115509033
Epoch: 6165, Batch Gradient Norm: 32.313110486854626
Epoch: 6165, Batch Gradient Norm after: 22.360679185214252
Epoch 6166/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7955610394477844
Epoch: 6166, Batch Gradient Norm: 30.177110573061317
Epoch: 6166, Batch Gradient Norm after: 22.360678822008648
Epoch 6167/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7904011845588684
Epoch: 6167, Batch Gradient Norm: 32.31041757752236
Epoch: 6167, Batch Gradient Norm after: 22.360678519420546
Epoch 6168/10000, Prediction Accuracy = 59.5%, Loss = 0.7955200552940369
Epoch: 6168, Batch Gradient Norm: 30.173536134943742
Epoch: 6168, Batch Gradient Norm after: 22.360675905768378
Epoch 6169/10000, Prediction Accuracy = 59.548%, Loss = 0.7903883814811706
Epoch: 6169, Batch Gradient Norm: 32.30704589121498
Epoch: 6169, Batch Gradient Norm after: 22.36067885438155
Epoch 6170/10000, Prediction Accuracy = 59.504%, Loss = 0.7954965353012085
Epoch: 6170, Batch Gradient Norm: 30.165743044861323
Epoch: 6170, Batch Gradient Norm after: 22.360679388874285
Epoch 6171/10000, Prediction Accuracy = 59.572%, Loss = 0.7903211355209351
Epoch: 6171, Batch Gradient Norm: 32.29986223878076
Epoch: 6171, Batch Gradient Norm after: 22.360678255350205
Epoch 6172/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7953543663024902
Epoch: 6172, Batch Gradient Norm: 30.16584386281238
Epoch: 6172, Batch Gradient Norm after: 22.360680742205968
Epoch 6173/10000, Prediction Accuracy = 59.540000000000006%, Loss = 0.7901677131652832
Epoch: 6173, Batch Gradient Norm: 32.29349431180778
Epoch: 6173, Batch Gradient Norm after: 22.360679293205465
Epoch 6174/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7951850533485413
Epoch: 6174, Batch Gradient Norm: 30.164410271573935
Epoch: 6174, Batch Gradient Norm after: 22.36068028574927
Epoch 6175/10000, Prediction Accuracy = 59.556000000000004%, Loss = 0.7900395274162293
Epoch: 6175, Batch Gradient Norm: 32.28849334557192
Epoch: 6175, Batch Gradient Norm after: 22.360679257532468
Epoch 6176/10000, Prediction Accuracy = 59.548%, Loss = 0.7950748801231384
Epoch: 6176, Batch Gradient Norm: 30.16399770823206
Epoch: 6176, Batch Gradient Norm after: 22.36067886915695
Epoch 6177/10000, Prediction Accuracy = 59.58200000000001%, Loss = 0.7899469614028931
Epoch: 6177, Batch Gradient Norm: 32.28223777750004
Epoch: 6177, Batch Gradient Norm after: 22.360681540014415
Epoch 6178/10000, Prediction Accuracy = 59.532%, Loss = 0.7949971079826355
Epoch: 6178, Batch Gradient Norm: 30.162992329551695
Epoch: 6178, Batch Gradient Norm after: 22.360677811110204
Epoch 6179/10000, Prediction Accuracy = 59.532%, Loss = 0.7898857593536377
Epoch: 6179, Batch Gradient Norm: 32.28017450599992
Epoch: 6179, Batch Gradient Norm after: 22.360678672215837
Epoch 6180/10000, Prediction Accuracy = 59.487999999999985%, Loss = 0.7949584603309632
Epoch: 6180, Batch Gradient Norm: 30.16083437388341
Epoch: 6180, Batch Gradient Norm after: 22.360678867453665
Epoch 6181/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7898774743080139
Epoch: 6181, Batch Gradient Norm: 32.27424324762539
Epoch: 6181, Batch Gradient Norm after: 22.360679797256854
Epoch 6182/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7949193596839905
Epoch: 6182, Batch Gradient Norm: 30.154978054501445
Epoch: 6182, Batch Gradient Norm after: 22.36067508558831
Epoch 6183/10000, Prediction Accuracy = 59.584%, Loss = 0.7897931575775147
Epoch: 6183, Batch Gradient Norm: 32.2666609404075
Epoch: 6183, Batch Gradient Norm after: 22.360678176939356
Epoch 6184/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.7947561264038085
Epoch: 6184, Batch Gradient Norm: 30.154864691096623
Epoch: 6184, Batch Gradient Norm after: 22.360678591834134
Epoch 6185/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.7896375536918641
Epoch: 6185, Batch Gradient Norm: 32.26235526216676
Epoch: 6185, Batch Gradient Norm after: 22.360678230064856
Epoch 6186/10000, Prediction Accuracy = 59.536%, Loss = 0.7945938587188721
Epoch: 6186, Batch Gradient Norm: 30.15174989436029
Epoch: 6186, Batch Gradient Norm after: 22.360679978451984
Epoch 6187/10000, Prediction Accuracy = 59.586%, Loss = 0.789514172077179
Epoch: 6187, Batch Gradient Norm: 32.257685345207356
Epoch: 6187, Batch Gradient Norm after: 22.360677559272887
Epoch 6188/10000, Prediction Accuracy = 59.556000000000004%, Loss = 0.7945021390914917
Epoch: 6188, Batch Gradient Norm: 30.15028271614817
Epoch: 6188, Batch Gradient Norm after: 22.360678376794738
Epoch 6189/10000, Prediction Accuracy = 59.562%, Loss = 0.7894179344177246
Epoch: 6189, Batch Gradient Norm: 32.25299327100413
Epoch: 6189, Batch Gradient Norm after: 22.36067920422833
Epoch 6190/10000, Prediction Accuracy = 59.536%, Loss = 0.7944391131401062
Epoch: 6190, Batch Gradient Norm: 30.148401177840576
Epoch: 6190, Batch Gradient Norm after: 22.360678813489724
Epoch 6191/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.7893778085708618
Epoch: 6191, Batch Gradient Norm: 32.24793293942822
Epoch: 6191, Batch Gradient Norm after: 22.360678123762398
Epoch 6192/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7944191336631775
Epoch: 6192, Batch Gradient Norm: 30.14258339057144
Epoch: 6192, Batch Gradient Norm after: 22.360678819296524
Epoch 6193/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7893671154975891
Epoch: 6193, Batch Gradient Norm: 32.24269994879438
Epoch: 6193, Batch Gradient Norm after: 22.360679469388906
Epoch 6194/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7943456768989563
Epoch: 6194, Batch Gradient Norm: 30.136649827721016
Epoch: 6194, Batch Gradient Norm after: 22.360680137232936
Epoch 6195/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.789242947101593
Epoch: 6195, Batch Gradient Norm: 32.23674368178285
Epoch: 6195, Batch Gradient Norm after: 22.360678255426404
Epoch 6196/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.794168221950531
Epoch: 6196, Batch Gradient Norm: 30.137724352721495
Epoch: 6196, Batch Gradient Norm after: 22.36067804316444
Epoch 6197/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7890870571136475
Epoch: 6197, Batch Gradient Norm: 32.232429097686015
Epoch: 6197, Batch Gradient Norm after: 22.3606776317509
Epoch 6198/10000, Prediction Accuracy = 59.55799999999999%, Loss = 0.7940315485000611
Epoch: 6198, Batch Gradient Norm: 30.136701342221436
Epoch: 6198, Batch Gradient Norm after: 22.360678199126756
Epoch 6199/10000, Prediction Accuracy = 59.574%, Loss = 0.7889772176742553
Epoch: 6199, Batch Gradient Norm: 32.2238148905813
Epoch: 6199, Batch Gradient Norm after: 22.360676532974022
Epoch 6200/10000, Prediction Accuracy = 59.544%, Loss = 0.7939443945884704
Epoch: 6200, Batch Gradient Norm: 30.135882590216568
Epoch: 6200, Batch Gradient Norm after: 22.360680173129943
Epoch 6201/10000, Prediction Accuracy = 59.556%, Loss = 0.7888975024223328
Epoch: 6201, Batch Gradient Norm: 32.22165268361557
Epoch: 6201, Batch Gradient Norm after: 22.360680132556002
Epoch 6202/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.7938858866691589
Epoch: 6202, Batch Gradient Norm: 30.133687519526017
Epoch: 6202, Batch Gradient Norm after: 22.36067823486035
Epoch 6203/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7888757705688476
Epoch: 6203, Batch Gradient Norm: 32.21528200007003
Epoch: 6203, Batch Gradient Norm after: 22.360678626140906
Epoch 6204/10000, Prediction Accuracy = 59.528%, Loss = 0.7938603639602662
Epoch: 6204, Batch Gradient Norm: 30.128701079536786
Epoch: 6204, Batch Gradient Norm after: 22.360677126163203
Epoch 6205/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7888363361358642
Epoch: 6205, Batch Gradient Norm: 32.21126280464519
Epoch: 6205, Batch Gradient Norm after: 22.360680849183584
Epoch 6206/10000, Prediction Accuracy = 59.516%, Loss = 0.7937550067901611
Epoch: 6206, Batch Gradient Norm: 30.122770885488386
Epoch: 6206, Batch Gradient Norm after: 22.360677515391213
Epoch 6207/10000, Prediction Accuracy = 59.572%, Loss = 0.7886896252632141
Epoch: 6207, Batch Gradient Norm: 32.205393979230074
Epoch: 6207, Batch Gradient Norm after: 22.360676960720255
Epoch 6208/10000, Prediction Accuracy = 59.519999999999996%, Loss = 0.7935795187950134
Epoch: 6208, Batch Gradient Norm: 30.12339536757162
Epoch: 6208, Batch Gradient Norm after: 22.36067900785961
Epoch 6209/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7885495901107789
Epoch: 6209, Batch Gradient Norm: 32.20053092769319
Epoch: 6209, Batch Gradient Norm after: 22.360676771931747
Epoch 6210/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.793463921546936
Epoch: 6210, Batch Gradient Norm: 30.118841942241513
Epoch: 6210, Batch Gradient Norm after: 22.360679213886357
Epoch 6211/10000, Prediction Accuracy = 59.588%, Loss = 0.7884361386299134
Epoch: 6211, Batch Gradient Norm: 32.19771656157841
Epoch: 6211, Batch Gradient Norm after: 22.360680771130802
Epoch 6212/10000, Prediction Accuracy = 59.536%, Loss = 0.793388831615448
Epoch: 6212, Batch Gradient Norm: 30.11611743686445
Epoch: 6212, Batch Gradient Norm after: 22.36067746107214
Epoch 6213/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7883688092231751
Epoch: 6213, Batch Gradient Norm: 32.19551892883705
Epoch: 6213, Batch Gradient Norm after: 22.360679308517355
Epoch 6214/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.7933528423309326
Epoch: 6214, Batch Gradient Norm: 30.1115417758875
Epoch: 6214, Batch Gradient Norm after: 22.36067910762205
Epoch 6215/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7883575677871704
Epoch: 6215, Batch Gradient Norm: 32.19184000653338
Epoch: 6215, Batch Gradient Norm after: 22.360679878990265
Epoch 6216/10000, Prediction Accuracy = 59.516%, Loss = 0.7933337926864624
Epoch: 6216, Batch Gradient Norm: 30.107136235607896
Epoch: 6216, Batch Gradient Norm after: 22.36067873741508
Epoch 6217/10000, Prediction Accuracy = 59.584%, Loss = 0.788293468952179
Epoch: 6217, Batch Gradient Norm: 32.18504219092749
Epoch: 6217, Batch Gradient Norm after: 22.360679962927207
Epoch 6218/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.793168044090271
Epoch: 6218, Batch Gradient Norm: 30.10613151444253
Epoch: 6218, Batch Gradient Norm after: 22.360679971850747
Epoch 6219/10000, Prediction Accuracy = 59.574%, Loss = 0.7881267189979553
Epoch: 6219, Batch Gradient Norm: 32.17910875495857
Epoch: 6219, Batch Gradient Norm after: 22.360678471252157
Epoch 6220/10000, Prediction Accuracy = 59.56600000000001%, Loss = 0.7929949998855591
Epoch: 6220, Batch Gradient Norm: 30.108042911493023
Epoch: 6220, Batch Gradient Norm after: 22.360678132528697
Epoch 6221/10000, Prediction Accuracy = 59.6%, Loss = 0.7880015254020691
Epoch: 6221, Batch Gradient Norm: 32.174835754831676
Epoch: 6221, Batch Gradient Norm after: 22.3606795541127
Epoch 6222/10000, Prediction Accuracy = 59.576%, Loss = 0.7929005980491638
Epoch: 6222, Batch Gradient Norm: 30.10361138710314
Epoch: 6222, Batch Gradient Norm after: 22.36067783901694
Epoch 6223/10000, Prediction Accuracy = 59.58200000000001%, Loss = 0.7878990173339844
Epoch: 6223, Batch Gradient Norm: 32.17088438201698
Epoch: 6223, Batch Gradient Norm after: 22.36067923568391
Epoch 6224/10000, Prediction Accuracy = 59.568000000000005%, Loss = 0.7928370356559753
Epoch: 6224, Batch Gradient Norm: 30.098416615756822
Epoch: 6224, Batch Gradient Norm after: 22.360677565554678
Epoch 6225/10000, Prediction Accuracy = 59.61%, Loss = 0.7878537058830262
Epoch: 6225, Batch Gradient Norm: 32.16963577037028
Epoch: 6225, Batch Gradient Norm after: 22.360680101478337
Epoch 6226/10000, Prediction Accuracy = 59.524%, Loss = 0.7928168654441834
Epoch: 6226, Batch Gradient Norm: 30.089931222608985
Epoch: 6226, Batch Gradient Norm after: 22.360676887825147
Epoch 6227/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.787840747833252
Epoch: 6227, Batch Gradient Norm: 32.164921881624274
Epoch: 6227, Batch Gradient Norm after: 22.36067994597011
Epoch 6228/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.7927717685699462
Epoch: 6228, Batch Gradient Norm: 30.080857516331957
Epoch: 6228, Batch Gradient Norm after: 22.360678024099833
Epoch 6229/10000, Prediction Accuracy = 59.592%, Loss = 0.787733280658722
Epoch: 6229, Batch Gradient Norm: 32.16130963036525
Epoch: 6229, Batch Gradient Norm after: 22.360679756533816
Epoch 6230/10000, Prediction Accuracy = 59.512%, Loss = 0.7925971031188965
Epoch: 6230, Batch Gradient Norm: 30.076438205225063
Epoch: 6230, Batch Gradient Norm after: 22.360680168864818
Epoch 6231/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7875629663467407
Epoch: 6231, Batch Gradient Norm: 32.16225314880991
Epoch: 6231, Batch Gradient Norm after: 22.36067735220413
Epoch 6232/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.7924598813056946
Epoch: 6232, Batch Gradient Norm: 30.07102805668528
Epoch: 6232, Batch Gradient Norm after: 22.36067845361544
Epoch 6233/10000, Prediction Accuracy = 59.596000000000004%, Loss = 0.7874366998672485
Epoch: 6233, Batch Gradient Norm: 32.15889089492269
Epoch: 6233, Batch Gradient Norm after: 22.36068013147087
Epoch 6234/10000, Prediction Accuracy = 59.59000000000001%, Loss = 0.792383325099945
Epoch: 6234, Batch Gradient Norm: 30.065841980210013
Epoch: 6234, Batch Gradient Norm after: 22.360679063002003
Epoch 6235/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7873464226722717
Epoch: 6235, Batch Gradient Norm: 32.15552073679257
Epoch: 6235, Batch Gradient Norm after: 22.360679502366363
Epoch 6236/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.7923313617706299
Epoch: 6236, Batch Gradient Norm: 30.0617523583571
Epoch: 6236, Batch Gradient Norm after: 22.36067867692299
Epoch 6237/10000, Prediction Accuracy = 59.616%, Loss = 0.7873151421546936
Epoch: 6237, Batch Gradient Norm: 32.15563801737815
Epoch: 6237, Batch Gradient Norm after: 22.360681079501795
Epoch 6238/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.7923137784004212
Epoch: 6238, Batch Gradient Norm: 30.052153866778987
Epoch: 6238, Batch Gradient Norm after: 22.360679466634007
Epoch 6239/10000, Prediction Accuracy = 59.6%, Loss = 0.7872717142105102
Epoch: 6239, Batch Gradient Norm: 32.15140275911596
Epoch: 6239, Batch Gradient Norm after: 22.360680103137614
Epoch 6240/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7922250747680664
Epoch: 6240, Batch Gradient Norm: 30.04668153394155
Epoch: 6240, Batch Gradient Norm after: 22.360678819032888
Epoch 6241/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.7871367812156678
Epoch: 6241, Batch Gradient Norm: 32.15061790349799
Epoch: 6241, Batch Gradient Norm after: 22.360679248528964
Epoch 6242/10000, Prediction Accuracy = 59.564%, Loss = 0.7920532584190368
Epoch: 6242, Batch Gradient Norm: 30.045177610477918
Epoch: 6242, Batch Gradient Norm after: 22.360676427199394
Epoch 6243/10000, Prediction Accuracy = 59.624%, Loss = 0.786984121799469
Epoch: 6243, Batch Gradient Norm: 32.14750516336108
Epoch: 6243, Batch Gradient Norm after: 22.360678503744857
Epoch 6244/10000, Prediction Accuracy = 59.59000000000001%, Loss = 0.7919424295425415
Epoch: 6244, Batch Gradient Norm: 30.03879204427689
Epoch: 6244, Batch Gradient Norm after: 22.360675750952364
Epoch 6245/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7868748426437377
Epoch: 6245, Batch Gradient Norm: 32.144623490510504
Epoch: 6245, Batch Gradient Norm after: 22.360680082766557
Epoch 6246/10000, Prediction Accuracy = 59.586%, Loss = 0.7918710350990296
Epoch: 6246, Batch Gradient Norm: 30.029891497055193
Epoch: 6246, Batch Gradient Norm after: 22.36067949540445
Epoch 6247/10000, Prediction Accuracy = 59.60799999999999%, Loss = 0.7867834687232971
Epoch: 6247, Batch Gradient Norm: 32.14749454493273
Epoch: 6247, Batch Gradient Norm after: 22.360678493755152
Epoch 6248/10000, Prediction Accuracy = 59.538%, Loss = 0.7918529272079468
Epoch: 6248, Batch Gradient Norm: 30.018434578658425
Epoch: 6248, Batch Gradient Norm after: 22.360678333669586
Epoch 6249/10000, Prediction Accuracy = 59.602%, Loss = 0.7867633104324341
Epoch: 6249, Batch Gradient Norm: 32.14722731681136
Epoch: 6249, Batch Gradient Norm after: 22.360678310194306
Epoch 6250/10000, Prediction Accuracy = 59.562%, Loss = 0.7918482542037963
Epoch: 6250, Batch Gradient Norm: 30.009423332713354
Epoch: 6250, Batch Gradient Norm after: 22.36067921304683
Epoch 6251/10000, Prediction Accuracy = 59.61%, Loss = 0.786691415309906
Epoch: 6251, Batch Gradient Norm: 32.14265207891249
Epoch: 6251, Batch Gradient Norm after: 22.360680186265828
Epoch 6252/10000, Prediction Accuracy = 59.536%, Loss = 0.7917046666145324
Epoch: 6252, Batch Gradient Norm: 30.00380163479821
Epoch: 6252, Batch Gradient Norm after: 22.36067759444907
Epoch 6253/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.7865172147750854
Epoch: 6253, Batch Gradient Norm: 32.14388181742355
Epoch: 6253, Batch Gradient Norm after: 22.360678416155633
Epoch 6254/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7915425062179565
Epoch: 6254, Batch Gradient Norm: 30.000647374180964
Epoch: 6254, Batch Gradient Norm after: 22.360675191629195
Epoch 6255/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.7863780498504639
Epoch: 6255, Batch Gradient Norm: 32.14063834860616
Epoch: 6255, Batch Gradient Norm after: 22.3606805339771
Epoch 6256/10000, Prediction Accuracy = 59.588%, Loss = 0.7914561629295349
Epoch: 6256, Batch Gradient Norm: 29.995573685576105
Epoch: 6256, Batch Gradient Norm after: 22.360678019093285
Epoch 6257/10000, Prediction Accuracy = 59.61%, Loss = 0.7862775206565857
Epoch: 6257, Batch Gradient Norm: 32.139132221529195
Epoch: 6257, Batch Gradient Norm after: 22.36068019667479
Epoch 6258/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7913947939872742
Epoch: 6258, Batch Gradient Norm: 29.989687420407837
Epoch: 6258, Batch Gradient Norm after: 22.36067772216229
Epoch 6259/10000, Prediction Accuracy = 59.626%, Loss = 0.7862272024154663
Epoch: 6259, Batch Gradient Norm: 32.142042036001506
Epoch: 6259, Batch Gradient Norm after: 22.36067773432945
Epoch 6260/10000, Prediction Accuracy = 59.562%, Loss = 0.791391909122467
Epoch: 6260, Batch Gradient Norm: 29.978135942605775
Epoch: 6260, Batch Gradient Norm after: 22.360677982990335
Epoch 6261/10000, Prediction Accuracy = 59.612%, Loss = 0.7862178921699524
Epoch: 6261, Batch Gradient Norm: 32.139274455706364
Epoch: 6261, Batch Gradient Norm after: 22.360679112008164
Epoch 6262/10000, Prediction Accuracy = 59.586%, Loss = 0.791359829902649
Epoch: 6262, Batch Gradient Norm: 29.969558979642514
Epoch: 6262, Batch Gradient Norm after: 22.360680149057018
Epoch 6263/10000, Prediction Accuracy = 59.6%, Loss = 0.7860976576805114
Epoch: 6263, Batch Gradient Norm: 32.13662254132084
Epoch: 6263, Batch Gradient Norm after: 22.36067993990209
Epoch 6264/10000, Prediction Accuracy = 59.564%, Loss = 0.7911697268486023
Epoch: 6264, Batch Gradient Norm: 29.967650146090868
Epoch: 6264, Batch Gradient Norm after: 22.360677184406608
Epoch 6265/10000, Prediction Accuracy = 59.616%, Loss = 0.7859249114990234
Epoch: 6265, Batch Gradient Norm: 32.13419202649087
Epoch: 6265, Batch Gradient Norm after: 22.360679615803573
Epoch 6266/10000, Prediction Accuracy = 59.592%, Loss = 0.7910322189331055
Epoch: 6266, Batch Gradient Norm: 29.965504123902843
Epoch: 6266, Batch Gradient Norm after: 22.360676409148105
Epoch 6267/10000, Prediction Accuracy = 59.616%, Loss = 0.7858121275901795
Epoch: 6267, Batch Gradient Norm: 32.13189941546362
Epoch: 6267, Batch Gradient Norm after: 22.360678178375377
Epoch 6268/10000, Prediction Accuracy = 59.598%, Loss = 0.7909502863883973
Epoch: 6268, Batch Gradient Norm: 29.96012700850743
Epoch: 6268, Batch Gradient Norm after: 22.36067892196935
Epoch 6269/10000, Prediction Accuracy = 59.61199999999999%, Loss = 0.7857134222984314
Epoch: 6269, Batch Gradient Norm: 32.12973104348912
Epoch: 6269, Batch Gradient Norm after: 22.360679592246125
Epoch 6270/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.7909014821052551
Epoch: 6270, Batch Gradient Norm: 29.95318289168108
Epoch: 6270, Batch Gradient Norm after: 22.36067630557349
Epoch 6271/10000, Prediction Accuracy = 59.622%, Loss = 0.7856841325759888
Epoch: 6271, Batch Gradient Norm: 32.12841377653879
Epoch: 6271, Batch Gradient Norm after: 22.36067884661198
Epoch 6272/10000, Prediction Accuracy = 59.568000000000005%, Loss = 0.790894603729248
Epoch: 6272, Batch Gradient Norm: 29.94447939013902
Epoch: 6272, Batch Gradient Norm after: 22.360677930735747
Epoch 6273/10000, Prediction Accuracy = 59.624%, Loss = 0.7856494903564453
Epoch: 6273, Batch Gradient Norm: 32.12425959023064
Epoch: 6273, Batch Gradient Norm after: 22.360681441645706
Epoch 6274/10000, Prediction Accuracy = 59.564%, Loss = 0.7908144593238831
Epoch: 6274, Batch Gradient Norm: 29.93688014728581
Epoch: 6274, Batch Gradient Norm after: 22.360678602354312
Epoch 6275/10000, Prediction Accuracy = 59.624%, Loss = 0.7855148434638977
Epoch: 6275, Batch Gradient Norm: 32.12479573906827
Epoch: 6275, Batch Gradient Norm after: 22.360680579114074
Epoch 6276/10000, Prediction Accuracy = 59.598%, Loss = 0.7906414031982422
Epoch: 6276, Batch Gradient Norm: 29.934697189145393
Epoch: 6276, Batch Gradient Norm after: 22.360678495526724
Epoch 6277/10000, Prediction Accuracy = 59.60999999999999%, Loss = 0.785360860824585
Epoch: 6277, Batch Gradient Norm: 32.121389175081255
Epoch: 6277, Batch Gradient Norm after: 22.360680635023787
Epoch 6278/10000, Prediction Accuracy = 59.58%, Loss = 0.7905215382575989
Epoch: 6278, Batch Gradient Norm: 29.93041329411802
Epoch: 6278, Batch Gradient Norm after: 22.36067938593592
Epoch 6279/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7852524876594543
Epoch: 6279, Batch Gradient Norm: 32.11894659263671
Epoch: 6279, Batch Gradient Norm after: 22.360680090914208
Epoch 6280/10000, Prediction Accuracy = 59.6%, Loss = 0.7904458045959473
Epoch: 6280, Batch Gradient Norm: 29.924701291301975
Epoch: 6280, Batch Gradient Norm after: 22.360678877909034
Epoch 6281/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.7851614117622375
Epoch: 6281, Batch Gradient Norm: 32.11910185874552
Epoch: 6281, Batch Gradient Norm after: 22.360678876189397
Epoch 6282/10000, Prediction Accuracy = 59.602%, Loss = 0.7904077291488647
Epoch: 6282, Batch Gradient Norm: 29.915675515992383
Epoch: 6282, Batch Gradient Norm after: 22.360679921004223
Epoch 6283/10000, Prediction Accuracy = 59.614%, Loss = 0.7851435303688049
Epoch: 6283, Batch Gradient Norm: 32.11432151197937
Epoch: 6283, Batch Gradient Norm after: 22.360681497584807
Epoch 6284/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7904049754142761
Epoch: 6284, Batch Gradient Norm: 29.909766341672395
Epoch: 6284, Batch Gradient Norm after: 22.360678494257318
Epoch 6285/10000, Prediction Accuracy = 59.628%, Loss = 0.7851016640663147
Epoch: 6285, Batch Gradient Norm: 32.10914813499633
Epoch: 6285, Batch Gradient Norm after: 22.36068076029207
Epoch 6286/10000, Prediction Accuracy = 59.56600000000001%, Loss = 0.7902837276458741
Epoch: 6286, Batch Gradient Norm: 29.90584778826394
Epoch: 6286, Batch Gradient Norm after: 22.360679717113616
Epoch 6287/10000, Prediction Accuracy = 59.628%, Loss = 0.7849419116973877
Epoch: 6287, Batch Gradient Norm: 32.109311818038236
Epoch: 6287, Batch Gradient Norm after: 22.36067959891094
Epoch 6288/10000, Prediction Accuracy = 59.60600000000001%, Loss = 0.7901074409484863
Epoch: 6288, Batch Gradient Norm: 29.90596691214849
Epoch: 6288, Batch Gradient Norm after: 22.360679194154518
Epoch 6289/10000, Prediction Accuracy = 59.63000000000001%, Loss = 0.784798514842987
Epoch: 6289, Batch Gradient Norm: 32.104902867646594
Epoch: 6289, Batch Gradient Norm after: 22.360678873574024
Epoch 6290/10000, Prediction Accuracy = 59.592%, Loss = 0.7900025963783264
Epoch: 6290, Batch Gradient Norm: 29.901920319758272
Epoch: 6290, Batch Gradient Norm after: 22.36067897317051
Epoch 6291/10000, Prediction Accuracy = 59.638%, Loss = 0.7846941113471985
Epoch: 6291, Batch Gradient Norm: 32.10012755459957
Epoch: 6291, Batch Gradient Norm after: 22.36067920597353
Epoch 6292/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.7899283647537232
Epoch: 6292, Batch Gradient Norm: 29.90068971844006
Epoch: 6292, Batch Gradient Norm after: 22.36068023273151
Epoch 6293/10000, Prediction Accuracy = 59.652%, Loss = 0.7846287846565246
Epoch: 6293, Batch Gradient Norm: 32.09689404621183
Epoch: 6293, Batch Gradient Norm after: 22.36067846494964
Epoch 6294/10000, Prediction Accuracy = 59.586%, Loss = 0.7899012804031372
Epoch: 6294, Batch Gradient Norm: 29.89739380205838
Epoch: 6294, Batch Gradient Norm after: 22.360679403249943
Epoch 6295/10000, Prediction Accuracy = 59.617999999999995%, Loss = 0.7846364974975586
Epoch: 6295, Batch Gradient Norm: 32.091769796435585
Epoch: 6295, Batch Gradient Norm after: 22.36067841492656
Epoch 6296/10000, Prediction Accuracy = 59.626%, Loss = 0.7898719787597657
Epoch: 6296, Batch Gradient Norm: 29.89344284599344
Epoch: 6296, Batch Gradient Norm after: 22.360678537809527
Epoch 6297/10000, Prediction Accuracy = 59.638%, Loss = 0.7845578193664551
Epoch: 6297, Batch Gradient Norm: 32.087357753710556
Epoch: 6297, Batch Gradient Norm after: 22.3606801346732
Epoch 6298/10000, Prediction Accuracy = 59.588%, Loss = 0.7897059917449951
Epoch: 6298, Batch Gradient Norm: 29.894067975769225
Epoch: 6298, Batch Gradient Norm after: 22.360679241945522
Epoch 6299/10000, Prediction Accuracy = 59.648%, Loss = 0.7843960046768188
Epoch: 6299, Batch Gradient Norm: 32.08050825839841
Epoch: 6299, Batch Gradient Norm after: 22.360678751984146
Epoch 6300/10000, Prediction Accuracy = 59.624%, Loss = 0.78954017162323
Epoch: 6300, Batch Gradient Norm: 29.89341560760299
Epoch: 6300, Batch Gradient Norm after: 22.36067713252606
Epoch 6301/10000, Prediction Accuracy = 59.65999999999999%, Loss = 0.784269392490387
Epoch: 6301, Batch Gradient Norm: 32.07562865392303
Epoch: 6301, Batch Gradient Norm after: 22.36068094028212
Epoch 6302/10000, Prediction Accuracy = 59.602%, Loss = 0.7894533038139343
Epoch: 6302, Batch Gradient Norm: 29.888729689671496
Epoch: 6302, Batch Gradient Norm after: 22.360677738879897
Epoch 6303/10000, Prediction Accuracy = 59.67%, Loss = 0.7841734886169434
Epoch: 6303, Batch Gradient Norm: 32.0741831291261
Epoch: 6303, Batch Gradient Norm after: 22.360679044019303
Epoch 6304/10000, Prediction Accuracy = 59.628%, Loss = 0.7893924593925477
Epoch: 6304, Batch Gradient Norm: 29.885343023418528
Epoch: 6304, Batch Gradient Norm after: 22.36067717490351
Epoch 6305/10000, Prediction Accuracy = 59.66399999999999%, Loss = 0.7841428995132447
Epoch: 6305, Batch Gradient Norm: 32.070009494595595
Epoch: 6305, Batch Gradient Norm after: 22.360677831244235
Epoch 6306/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7893773674964905
Epoch: 6306, Batch Gradient Norm: 29.881495569145024
Epoch: 6306, Batch Gradient Norm after: 22.360678923633596
Epoch 6307/10000, Prediction Accuracy = 59.63399999999999%, Loss = 0.7841356515884399
Epoch: 6307, Batch Gradient Norm: 32.06379843859074
Epoch: 6307, Batch Gradient Norm after: 22.360679777917582
Epoch 6308/10000, Prediction Accuracy = 59.626%, Loss = 0.7893139004707337
Epoch: 6308, Batch Gradient Norm: 29.87647262443443
Epoch: 6308, Batch Gradient Norm after: 22.360678659962655
Epoch 6309/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7840160369873047
Epoch: 6309, Batch Gradient Norm: 32.056628152141705
Epoch: 6309, Batch Gradient Norm after: 22.360679326672468
Epoch 6310/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.7891195178031921
Epoch: 6310, Batch Gradient Norm: 29.879694553448665
Epoch: 6310, Batch Gradient Norm after: 22.360678647271314
Epoch 6311/10000, Prediction Accuracy = 59.668000000000006%, Loss = 0.783858597278595
Epoch: 6311, Batch Gradient Norm: 32.052202236202675
Epoch: 6311, Batch Gradient Norm after: 22.36067956668225
Epoch 6312/10000, Prediction Accuracy = 59.622%, Loss = 0.7889835476875305
Epoch: 6312, Batch Gradient Norm: 29.87908875707141
Epoch: 6312, Batch Gradient Norm after: 22.3606794408438
Epoch 6313/10000, Prediction Accuracy = 59.674%, Loss = 0.7837520122528077
Epoch: 6313, Batch Gradient Norm: 32.047974192073816
Epoch: 6313, Batch Gradient Norm after: 22.360680568637125
Epoch 6314/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7889052271842957
Epoch: 6314, Batch Gradient Norm: 29.87451782658282
Epoch: 6314, Batch Gradient Norm after: 22.360679082237755
Epoch 6315/10000, Prediction Accuracy = 59.664%, Loss = 0.7836732983589172
Epoch: 6315, Batch Gradient Norm: 32.04479447068557
Epoch: 6315, Batch Gradient Norm after: 22.360677322787545
Epoch 6316/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7888595581054687
Epoch: 6316, Batch Gradient Norm: 29.86999294224906
Epoch: 6316, Batch Gradient Norm after: 22.36067826064831
Epoch 6317/10000, Prediction Accuracy = 59.628%, Loss = 0.7836663961410523
Epoch: 6317, Batch Gradient Norm: 32.043259868401435
Epoch: 6317, Batch Gradient Norm after: 22.360679539858364
Epoch 6318/10000, Prediction Accuracy = 59.628%, Loss = 0.7888667106628418
Epoch: 6318, Batch Gradient Norm: 29.859705206008798
Epoch: 6318, Batch Gradient Norm after: 22.360680698063913
Epoch 6319/10000, Prediction Accuracy = 59.61999999999999%, Loss = 0.7836245536804199
Epoch: 6319, Batch Gradient Norm: 32.03897450861512
Epoch: 6319, Batch Gradient Norm after: 22.36068005600284
Epoch 6320/10000, Prediction Accuracy = 59.628%, Loss = 0.788737416267395
Epoch: 6320, Batch Gradient Norm: 29.859231700756336
Epoch: 6320, Batch Gradient Norm after: 22.36068073134235
Epoch 6321/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.7834630250930786
Epoch: 6321, Batch Gradient Norm: 32.036933068909825
Epoch: 6321, Batch Gradient Norm after: 22.360676812667123
Epoch 6322/10000, Prediction Accuracy = 59.64%, Loss = 0.7885537981987
Epoch: 6322, Batch Gradient Norm: 29.86136126744412
Epoch: 6322, Batch Gradient Norm after: 22.360677674396655
Epoch 6323/10000, Prediction Accuracy = 59.668000000000006%, Loss = 0.7833196997642518
Epoch: 6323, Batch Gradient Norm: 32.03032291652109
Epoch: 6323, Batch Gradient Norm after: 22.360679345563444
Epoch 6324/10000, Prediction Accuracy = 59.628%, Loss = 0.7884469985961914
Epoch: 6324, Batch Gradient Norm: 29.859335784936093
Epoch: 6324, Batch Gradient Norm after: 22.36067866951143
Epoch 6325/10000, Prediction Accuracy = 59.676%, Loss = 0.7832180500030518
Epoch: 6325, Batch Gradient Norm: 32.02626055907354
Epoch: 6325, Batch Gradient Norm after: 22.36068058899584
Epoch 6326/10000, Prediction Accuracy = 59.622%, Loss = 0.7883748650550843
Epoch: 6326, Batch Gradient Norm: 29.854784839147865
Epoch: 6326, Batch Gradient Norm after: 22.36067914221445
Epoch 6327/10000, Prediction Accuracy = 59.686%, Loss = 0.7831574559211731
Epoch: 6327, Batch Gradient Norm: 32.02524203346163
Epoch: 6327, Batch Gradient Norm after: 22.36067814944764
Epoch 6328/10000, Prediction Accuracy = 59.628%, Loss = 0.7883533358573913
Epoch: 6328, Batch Gradient Norm: 29.85180198580376
Epoch: 6328, Batch Gradient Norm after: 22.360679711543966
Epoch 6329/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7831751108169556
Epoch: 6329, Batch Gradient Norm: 32.0173598633493
Epoch: 6329, Batch Gradient Norm after: 22.360679432171864
Epoch 6330/10000, Prediction Accuracy = 59.64%, Loss = 0.788315498828888
Epoch: 6330, Batch Gradient Norm: 29.84919819077672
Epoch: 6330, Batch Gradient Norm after: 22.36067847967385
Epoch 6331/10000, Prediction Accuracy = 59.646%, Loss = 0.783090102672577
Epoch: 6331, Batch Gradient Norm: 32.010079518690056
Epoch: 6331, Batch Gradient Norm after: 22.360679402343127
Epoch 6332/10000, Prediction Accuracy = 59.646%, Loss = 0.7881392955780029
Epoch: 6332, Batch Gradient Norm: 29.85447785834508
Epoch: 6332, Batch Gradient Norm after: 22.36067990250672
Epoch 6333/10000, Prediction Accuracy = 59.666%, Loss = 0.7829313755035401
Epoch: 6333, Batch Gradient Norm: 32.00394300689545
Epoch: 6333, Batch Gradient Norm after: 22.3606793160446
Epoch 6334/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.787975013256073
Epoch: 6334, Batch Gradient Norm: 29.85758636234296
Epoch: 6334, Batch Gradient Norm after: 22.360675942154835
Epoch 6335/10000, Prediction Accuracy = 59.674%, Loss = 0.7828232169151306
Epoch: 6335, Batch Gradient Norm: 31.9923049335
Epoch: 6335, Batch Gradient Norm after: 22.360680096893482
Epoch 6336/10000, Prediction Accuracy = 59.624%, Loss = 0.787876307964325
Epoch: 6336, Batch Gradient Norm: 29.860281264215793
Epoch: 6336, Batch Gradient Norm after: 22.360676638050926
Epoch 6337/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.782738208770752
Epoch: 6337, Batch Gradient Norm: 31.98671018318346
Epoch: 6337, Batch Gradient Norm after: 22.360678967612028
Epoch 6338/10000, Prediction Accuracy = 59.652%, Loss = 0.7878085017204285
Epoch: 6338, Batch Gradient Norm: 29.86048594572605
Epoch: 6338, Batch Gradient Norm after: 22.360679626791974
Epoch 6339/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7827164769172669
Epoch: 6339, Batch Gradient Norm: 31.979619671411573
Epoch: 6339, Batch Gradient Norm after: 22.360679036430643
Epoch 6340/10000, Prediction Accuracy = 59.634%, Loss = 0.7877874851226807
Epoch: 6340, Batch Gradient Norm: 29.86339569150225
Epoch: 6340, Batch Gradient Norm after: 22.360677847964922
Epoch 6341/10000, Prediction Accuracy = 59.64000000000001%, Loss = 0.7827311754226685
Epoch: 6341, Batch Gradient Norm: 31.967877514322943
Epoch: 6341, Batch Gradient Norm after: 22.360676635180855
Epoch 6342/10000, Prediction Accuracy = 59.65%, Loss = 0.7876967072486878
Epoch: 6342, Batch Gradient Norm: 29.86288478722413
Epoch: 6342, Batch Gradient Norm after: 22.360680248035465
Epoch 6343/10000, Prediction Accuracy = 59.658%, Loss = 0.7826188206672668
Epoch: 6343, Batch Gradient Norm: 31.961136758061592
Epoch: 6343, Batch Gradient Norm after: 22.36067793201077
Epoch 6344/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.7875171422958374
Epoch: 6344, Batch Gradient Norm: 29.870319481529357
Epoch: 6344, Batch Gradient Norm after: 22.360681104909244
Epoch 6345/10000, Prediction Accuracy = 59.69%, Loss = 0.7824737071990967
Epoch: 6345, Batch Gradient Norm: 31.954537030211085
Epoch: 6345, Batch Gradient Norm after: 22.360678141225435
Epoch 6346/10000, Prediction Accuracy = 59.658%, Loss = 0.7873629689216614
Epoch: 6346, Batch Gradient Norm: 29.872543708543603
Epoch: 6346, Batch Gradient Norm after: 22.360677818580015
Epoch 6347/10000, Prediction Accuracy = 59.684000000000005%, Loss = 0.7823716163635254
Epoch: 6347, Batch Gradient Norm: 31.947995412982543
Epoch: 6347, Batch Gradient Norm after: 22.36067881453731
Epoch 6348/10000, Prediction Accuracy = 59.628%, Loss = 0.7872747182846069
Epoch: 6348, Batch Gradient Norm: 29.87177627943582
Epoch: 6348, Batch Gradient Norm after: 22.360677828628607
Epoch 6349/10000, Prediction Accuracy = 59.694%, Loss = 0.7822918057441711
Epoch: 6349, Batch Gradient Norm: 31.941170546120166
Epoch: 6349, Batch Gradient Norm after: 22.360676347023126
Epoch 6350/10000, Prediction Accuracy = 59.662%, Loss = 0.7872285604476928
Epoch: 6350, Batch Gradient Norm: 29.874849611718275
Epoch: 6350, Batch Gradient Norm after: 22.360677408544355
Epoch 6351/10000, Prediction Accuracy = 59.664%, Loss = 0.7823046684265137
Epoch: 6351, Batch Gradient Norm: 31.934287033876483
Epoch: 6351, Batch Gradient Norm after: 22.360677269435143
Epoch 6352/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.7872256994247436
Epoch: 6352, Batch Gradient Norm: 29.871460937790157
Epoch: 6352, Batch Gradient Norm after: 22.36067646721489
Epoch 6353/10000, Prediction Accuracy = 59.626%, Loss = 0.7822962760925293
Epoch: 6353, Batch Gradient Norm: 31.923114295723888
Epoch: 6353, Batch Gradient Norm after: 22.360677802067787
Epoch 6354/10000, Prediction Accuracy = 59.65999999999999%, Loss = 0.7870872855186463
Epoch: 6354, Batch Gradient Norm: 29.873223704590597
Epoch: 6354, Batch Gradient Norm after: 22.36067982887075
Epoch 6355/10000, Prediction Accuracy = 59.656000000000006%, Loss = 0.7821355938911438
Epoch: 6355, Batch Gradient Norm: 31.917209044959055
Epoch: 6355, Batch Gradient Norm after: 22.36067892357008
Epoch 6356/10000, Prediction Accuracy = 59.676%, Loss = 0.7868911385536194
Epoch: 6356, Batch Gradient Norm: 29.878881416265802
Epoch: 6356, Batch Gradient Norm after: 22.360678790895143
Epoch 6357/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7820101499557495
Epoch: 6357, Batch Gradient Norm: 31.909417686417022
Epoch: 6357, Batch Gradient Norm after: 22.36067840021895
Epoch 6358/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7867722988128663
Epoch: 6358, Batch Gradient Norm: 29.883422750076516
Epoch: 6358, Batch Gradient Norm after: 22.36067859839106
Epoch 6359/10000, Prediction Accuracy = 59.69%, Loss = 0.7819220542907714
Epoch: 6359, Batch Gradient Norm: 31.902110272176934
Epoch: 6359, Batch Gradient Norm after: 22.36068012716981
Epoch 6360/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7866849541664124
Epoch: 6360, Batch Gradient Norm: 29.8858717265931
Epoch: 6360, Batch Gradient Norm after: 22.360678008368556
Epoch 6361/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.7818681478500367
Epoch: 6361, Batch Gradient Norm: 31.895103758618937
Epoch: 6361, Batch Gradient Norm after: 22.360678847932714
Epoch 6362/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.7866494536399842
Epoch: 6362, Batch Gradient Norm: 29.883529781339536
Epoch: 6362, Batch Gradient Norm after: 22.36067954021663
Epoch 6363/10000, Prediction Accuracy = 59.658%, Loss = 0.7818753123283386
Epoch: 6363, Batch Gradient Norm: 31.889145528740706
Epoch: 6363, Batch Gradient Norm after: 22.360675724308024
Epoch 6364/10000, Prediction Accuracy = 59.686%, Loss = 0.7866363286972046
Epoch: 6364, Batch Gradient Norm: 29.878542766538782
Epoch: 6364, Batch Gradient Norm after: 22.360678714442194
Epoch 6365/10000, Prediction Accuracy = 59.634%, Loss = 0.7818179368972779
Epoch: 6365, Batch Gradient Norm: 31.882601412430247
Epoch: 6365, Batch Gradient Norm after: 22.360680056171635
Epoch 6366/10000, Prediction Accuracy = 59.666%, Loss = 0.7864840269088745
Epoch: 6366, Batch Gradient Norm: 29.879133091345153
Epoch: 6366, Batch Gradient Norm after: 22.36067951303304
Epoch 6367/10000, Prediction Accuracy = 59.674%, Loss = 0.7816611409187317
Epoch: 6367, Batch Gradient Norm: 31.880634740671844
Epoch: 6367, Batch Gradient Norm after: 22.3606790870968
Epoch 6368/10000, Prediction Accuracy = 59.676%, Loss = 0.7863120675086975
Epoch: 6368, Batch Gradient Norm: 29.88007567615707
Epoch: 6368, Batch Gradient Norm after: 22.360678168870162
Epoch 6369/10000, Prediction Accuracy = 59.7%, Loss = 0.781533682346344
Epoch: 6369, Batch Gradient Norm: 31.87497747444717
Epoch: 6369, Batch Gradient Norm after: 22.36068061819533
Epoch 6370/10000, Prediction Accuracy = 59.628%, Loss = 0.7862136960029602
Epoch: 6370, Batch Gradient Norm: 29.877421945171974
Epoch: 6370, Batch Gradient Norm after: 22.360677753417153
Epoch 6371/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7814388871192932
Epoch: 6371, Batch Gradient Norm: 31.869774568336553
Epoch: 6371, Batch Gradient Norm after: 22.36067972113894
Epoch 6372/10000, Prediction Accuracy = 59.658%, Loss = 0.7861390709877014
Epoch: 6372, Batch Gradient Norm: 29.877535604831596
Epoch: 6372, Batch Gradient Norm after: 22.36067935951435
Epoch 6373/10000, Prediction Accuracy = 59.698%, Loss = 0.781395149230957
Epoch: 6373, Batch Gradient Norm: 31.86715348131815
Epoch: 6373, Batch Gradient Norm after: 22.360677574739817
Epoch 6374/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7861314415931702
Epoch: 6374, Batch Gradient Norm: 29.871752671323268
Epoch: 6374, Batch Gradient Norm after: 22.360679183230474
Epoch 6375/10000, Prediction Accuracy = 59.64200000000001%, Loss = 0.7814125537872314
Epoch: 6375, Batch Gradient Norm: 31.861818652549804
Epoch: 6375, Batch Gradient Norm after: 22.36067740889561
Epoch 6376/10000, Prediction Accuracy = 59.669999999999995%, Loss = 0.786098039150238
Epoch: 6376, Batch Gradient Norm: 29.865114781755057
Epoch: 6376, Batch Gradient Norm after: 22.360678588243577
Epoch 6377/10000, Prediction Accuracy = 59.646%, Loss = 0.7813055276870727
Epoch: 6377, Batch Gradient Norm: 31.858105387252756
Epoch: 6377, Batch Gradient Norm after: 22.360680008699003
Epoch 6378/10000, Prediction Accuracy = 59.664%, Loss = 0.7859100937843323
Epoch: 6378, Batch Gradient Norm: 29.866165654847816
Epoch: 6378, Batch Gradient Norm after: 22.360679228530156
Epoch 6379/10000, Prediction Accuracy = 59.702%, Loss = 0.7811348557472229
Epoch: 6379, Batch Gradient Norm: 31.855810302875895
Epoch: 6379, Batch Gradient Norm after: 22.360679458396547
Epoch 6380/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.7857668995857239
Epoch: 6380, Batch Gradient Norm: 29.86019564759338
Epoch: 6380, Batch Gradient Norm after: 22.36067822744958
Epoch 6381/10000, Prediction Accuracy = 59.686%, Loss = 0.7810138702392578
Epoch: 6381, Batch Gradient Norm: 31.85307804997527
Epoch: 6381, Batch Gradient Norm after: 22.360679794563065
Epoch 6382/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7856935024261474
Epoch: 6382, Batch Gradient Norm: 29.855167388705432
Epoch: 6382, Batch Gradient Norm after: 22.36067841266801
Epoch 6383/10000, Prediction Accuracy = 59.698%, Loss = 0.7809242963790893
Epoch: 6383, Batch Gradient Norm: 31.85180695499555
Epoch: 6383, Batch Gradient Norm after: 22.360682263981058
Epoch 6384/10000, Prediction Accuracy = 59.674%, Loss = 0.7856467962265015
Epoch: 6384, Batch Gradient Norm: 29.853561021878864
Epoch: 6384, Batch Gradient Norm after: 22.36068015392315
Epoch 6385/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.7809128403663635
Epoch: 6385, Batch Gradient Norm: 31.848588820931802
Epoch: 6385, Batch Gradient Norm after: 22.360679107797736
Epoch 6386/10000, Prediction Accuracy = 59.698%, Loss = 0.7856508016586303
Epoch: 6386, Batch Gradient Norm: 29.84655911312554
Epoch: 6386, Batch Gradient Norm after: 22.360677044510332
Epoch 6387/10000, Prediction Accuracy = 59.63000000000001%, Loss = 0.7808932900428772
Epoch: 6387, Batch Gradient Norm: 31.841784806758348
Epoch: 6387, Batch Gradient Norm after: 22.360679046767174
Epoch 6388/10000, Prediction Accuracy = 59.7%, Loss = 0.7855467438697815
Epoch: 6388, Batch Gradient Norm: 29.842875776801712
Epoch: 6388, Batch Gradient Norm after: 22.360677923142607
Epoch 6389/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.780747652053833
Epoch: 6389, Batch Gradient Norm: 31.838494400508846
Epoch: 6389, Batch Gradient Norm after: 22.36068061371082
Epoch 6390/10000, Prediction Accuracy = 59.67199999999999%, Loss = 0.7853653073310852
Epoch: 6390, Batch Gradient Norm: 29.842351943570076
Epoch: 6390, Batch Gradient Norm after: 22.36068063186547
Epoch 6391/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7806034684181213
Epoch: 6391, Batch Gradient Norm: 31.833867199784372
Epoch: 6391, Batch Gradient Norm after: 22.360678975937965
Epoch 6392/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.7852458477020263
Epoch: 6392, Batch Gradient Norm: 29.83968882193869
Epoch: 6392, Batch Gradient Norm after: 22.360678838112413
Epoch 6393/10000, Prediction Accuracy = 59.7%, Loss = 0.7804949760437012
Epoch: 6393, Batch Gradient Norm: 31.83058859783054
Epoch: 6393, Batch Gradient Norm after: 22.360679881021834
Epoch 6394/10000, Prediction Accuracy = 59.658%, Loss = 0.7851762056350708
Epoch: 6394, Batch Gradient Norm: 29.839082417563013
Epoch: 6394, Batch Gradient Norm after: 22.360678624338952
Epoch 6395/10000, Prediction Accuracy = 59.722%, Loss = 0.7804242849349976
Epoch: 6395, Batch Gradient Norm: 31.828060011498213
Epoch: 6395, Batch Gradient Norm after: 22.36068165271458
Epoch 6396/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7851526498794555
Epoch: 6396, Batch Gradient Norm: 29.832501473283084
Epoch: 6396, Batch Gradient Norm after: 22.360680042735538
Epoch 6397/10000, Prediction Accuracy = 59.664%, Loss = 0.780429458618164
Epoch: 6397, Batch Gradient Norm: 31.824714027199242
Epoch: 6397, Batch Gradient Norm after: 22.36067944065363
Epoch 6398/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.7851443648338318
Epoch: 6398, Batch Gradient Norm: 29.824829938727216
Epoch: 6398, Batch Gradient Norm after: 22.360678593363986
Epoch 6399/10000, Prediction Accuracy = 59.648%, Loss = 0.780367398262024
Epoch: 6399, Batch Gradient Norm: 31.817043561840656
Epoch: 6399, Batch Gradient Norm after: 22.360678351282996
Epoch 6400/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.7849923610687256
Epoch: 6400, Batch Gradient Norm: 29.826779322028596
Epoch: 6400, Batch Gradient Norm after: 22.360677530386457
Epoch 6401/10000, Prediction Accuracy = 59.694%, Loss = 0.7802002191543579
Epoch: 6401, Batch Gradient Norm: 31.812791255259203
Epoch: 6401, Batch Gradient Norm after: 22.360679297547772
Epoch 6402/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7848326325416565
Epoch: 6402, Batch Gradient Norm: 29.823465836263455
Epoch: 6402, Batch Gradient Norm after: 22.36067852119744
Epoch 6403/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7800753474235534
Epoch: 6403, Batch Gradient Norm: 31.80572518794425
Epoch: 6403, Batch Gradient Norm after: 22.360678410039053
Epoch 6404/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.7847368836402893
Epoch: 6404, Batch Gradient Norm: 29.822371094669244
Epoch: 6404, Batch Gradient Norm after: 22.360677399253813
Epoch 6405/10000, Prediction Accuracy = 59.71%, Loss = 0.7799820899963379
Epoch: 6405, Batch Gradient Norm: 31.802406667667515
Epoch: 6405, Batch Gradient Norm after: 22.360681439121432
Epoch 6406/10000, Prediction Accuracy = 59.66200000000001%, Loss = 0.7846601843833924
Epoch: 6406, Batch Gradient Norm: 29.820260961137535
Epoch: 6406, Batch Gradient Norm after: 22.360677146216137
Epoch 6407/10000, Prediction Accuracy = 59.71600000000001%, Loss = 0.7799374103546143
Epoch: 6407, Batch Gradient Norm: 31.799457543603015
Epoch: 6407, Batch Gradient Norm after: 22.360677357041975
Epoch 6408/10000, Prediction Accuracy = 59.71%, Loss = 0.7846473097801209
Epoch: 6408, Batch Gradient Norm: 29.81533622974085
Epoch: 6408, Batch Gradient Norm after: 22.360678178822596
Epoch 6409/10000, Prediction Accuracy = 59.628%, Loss = 0.7799496412277221
Epoch: 6409, Batch Gradient Norm: 31.796879385653224
Epoch: 6409, Batch Gradient Norm after: 22.360679113901213
Epoch 6410/10000, Prediction Accuracy = 59.7%, Loss = 0.7846190690994262
Epoch: 6410, Batch Gradient Norm: 29.80905031711037
Epoch: 6410, Batch Gradient Norm after: 22.360678613171256
Epoch 6411/10000, Prediction Accuracy = 59.65%, Loss = 0.7798461079597473
Epoch: 6411, Batch Gradient Norm: 31.791308514489437
Epoch: 6411, Batch Gradient Norm after: 22.360681489396416
Epoch 6412/10000, Prediction Accuracy = 59.678%, Loss = 0.7844398617744446
Epoch: 6412, Batch Gradient Norm: 29.807320918615954
Epoch: 6412, Batch Gradient Norm after: 22.36068214661879
Epoch 6413/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7796802878379822
Epoch: 6413, Batch Gradient Norm: 31.78548985624714
Epoch: 6413, Batch Gradient Norm after: 22.36067937232927
Epoch 6414/10000, Prediction Accuracy = 59.688%, Loss = 0.7842915773391723
Epoch: 6414, Batch Gradient Norm: 29.808007015054734
Epoch: 6414, Batch Gradient Norm after: 22.360678442740735
Epoch 6415/10000, Prediction Accuracy = 59.702%, Loss = 0.7795666933059693
Epoch: 6415, Batch Gradient Norm: 31.781147955317564
Epoch: 6415, Batch Gradient Norm after: 22.360679018813745
Epoch 6416/10000, Prediction Accuracy = 59.662%, Loss = 0.7842078566551208
Epoch: 6416, Batch Gradient Norm: 29.805338196704025
Epoch: 6416, Batch Gradient Norm after: 22.36068096603119
Epoch 6417/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7794713377952576
Epoch: 6417, Batch Gradient Norm: 31.779886497808306
Epoch: 6417, Batch Gradient Norm after: 22.36068004674646
Epoch 6418/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.784145176410675
Epoch: 6418, Batch Gradient Norm: 29.800217917727863
Epoch: 6418, Batch Gradient Norm after: 22.360678519286832
Epoch 6419/10000, Prediction Accuracy = 59.694%, Loss = 0.7794453024864196
Epoch: 6419, Batch Gradient Norm: 31.779571752535304
Epoch: 6419, Batch Gradient Norm after: 22.360677786260613
Epoch 6420/10000, Prediction Accuracy = 59.722%, Loss = 0.7841494917869568
Epoch: 6420, Batch Gradient Norm: 29.79641553285938
Epoch: 6420, Batch Gradient Norm after: 22.360679787834805
Epoch 6421/10000, Prediction Accuracy = 59.656000000000006%, Loss = 0.7794517993927002
Epoch: 6421, Batch Gradient Norm: 31.77227863369422
Epoch: 6421, Batch Gradient Norm after: 22.360679333502627
Epoch 6422/10000, Prediction Accuracy = 59.71199999999999%, Loss = 0.784082543849945
Epoch: 6422, Batch Gradient Norm: 29.78810302823642
Epoch: 6422, Batch Gradient Norm after: 22.36067987667806
Epoch 6423/10000, Prediction Accuracy = 59.644000000000005%, Loss = 0.7793249249458313
Epoch: 6423, Batch Gradient Norm: 31.769657509500373
Epoch: 6423, Batch Gradient Norm after: 22.360677893850113
Epoch 6424/10000, Prediction Accuracy = 59.706%, Loss = 0.7838961601257324
Epoch: 6424, Batch Gradient Norm: 29.786651204013758
Epoch: 6424, Batch Gradient Norm after: 22.360679343188565
Epoch 6425/10000, Prediction Accuracy = 59.73%, Loss = 0.7791534304618836
Epoch: 6425, Batch Gradient Norm: 31.765539261138617
Epoch: 6425, Batch Gradient Norm after: 22.360679143235963
Epoch 6426/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.7837645888328553
Epoch: 6426, Batch Gradient Norm: 29.782806203652413
Epoch: 6426, Batch Gradient Norm after: 22.360677813258857
Epoch 6427/10000, Prediction Accuracy = 59.734%, Loss = 0.7790437340736389
Epoch: 6427, Batch Gradient Norm: 31.76195207475338
Epoch: 6427, Batch Gradient Norm after: 22.360677565981547
Epoch 6428/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7836925029754639
Epoch: 6428, Batch Gradient Norm: 29.77675732192982
Epoch: 6428, Batch Gradient Norm after: 22.360678732760004
Epoch 6429/10000, Prediction Accuracy = 59.71999999999999%, Loss = 0.7789532780647278
Epoch: 6429, Batch Gradient Norm: 31.764023304793007
Epoch: 6429, Batch Gradient Norm after: 22.36067815148528
Epoch 6430/10000, Prediction Accuracy = 59.702%, Loss = 0.7836509346961975
Epoch: 6430, Batch Gradient Norm: 29.768912373111714
Epoch: 6430, Batch Gradient Norm after: 22.360677961493156
Epoch 6431/10000, Prediction Accuracy = 59.688%, Loss = 0.7789438962936401
Epoch: 6431, Batch Gradient Norm: 31.763879746670526
Epoch: 6431, Batch Gradient Norm after: 22.36067779775109
Epoch 6432/10000, Prediction Accuracy = 59.722%, Loss = 0.7836674690246582
Epoch: 6432, Batch Gradient Norm: 29.76191380791522
Epoch: 6432, Batch Gradient Norm after: 22.360677440313893
Epoch 6433/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.778913927078247
Epoch: 6433, Batch Gradient Norm: 31.75767581794285
Epoch: 6433, Batch Gradient Norm after: 22.360680109859747
Epoch 6434/10000, Prediction Accuracy = 59.71400000000001%, Loss = 0.7835567116737365
Epoch: 6434, Batch Gradient Norm: 29.75580841040042
Epoch: 6434, Batch Gradient Norm after: 22.360677468232836
Epoch 6435/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.7787610054016113
Epoch: 6435, Batch Gradient Norm: 31.75288121941761
Epoch: 6435, Batch Gradient Norm after: 22.360682063539993
Epoch 6436/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.783372950553894
Epoch: 6436, Batch Gradient Norm: 29.75218642751522
Epoch: 6436, Batch Gradient Norm after: 22.36068026749734
Epoch 6437/10000, Prediction Accuracy = 59.738%, Loss = 0.7786064982414246
Epoch: 6437, Batch Gradient Norm: 31.75240045818664
Epoch: 6437, Batch Gradient Norm after: 22.360680831565293
Epoch 6438/10000, Prediction Accuracy = 59.688%, Loss = 0.7832740902900696
Epoch: 6438, Batch Gradient Norm: 29.749775493702113
Epoch: 6438, Batch Gradient Norm after: 22.36067939869233
Epoch 6439/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7784961700439453
Epoch: 6439, Batch Gradient Norm: 31.747099160955987
Epoch: 6439, Batch Gradient Norm after: 22.360678948363866
Epoch 6440/10000, Prediction Accuracy = 59.676%, Loss = 0.783201026916504
Epoch: 6440, Batch Gradient Norm: 29.743686802499138
Epoch: 6440, Batch Gradient Norm after: 22.36067804258243
Epoch 6441/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7784264087677002
Epoch: 6441, Batch Gradient Norm: 31.74785045737325
Epoch: 6441, Batch Gradient Norm after: 22.360678468191423
Epoch 6442/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7831881642341614
Epoch: 6442, Batch Gradient Norm: 29.737344995610968
Epoch: 6442, Batch Gradient Norm after: 22.360677882341953
Epoch 6443/10000, Prediction Accuracy = 59.66200000000001%, Loss = 0.7784345149993896
Epoch: 6443, Batch Gradient Norm: 31.74449444541908
Epoch: 6443, Batch Gradient Norm after: 22.36067917923973
Epoch 6444/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7831897258758544
Epoch: 6444, Batch Gradient Norm: 29.726861084405815
Epoch: 6444, Batch Gradient Norm after: 22.360678192508242
Epoch 6445/10000, Prediction Accuracy = 59.666%, Loss = 0.7783687829971313
Epoch: 6445, Batch Gradient Norm: 31.74213483541718
Epoch: 6445, Batch Gradient Norm after: 22.360677022190117
Epoch 6446/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7830357551574707
Epoch: 6446, Batch Gradient Norm: 29.721305501253955
Epoch: 6446, Batch Gradient Norm after: 22.36067741394841
Epoch 6447/10000, Prediction Accuracy = 59.714%, Loss = 0.7781808733940124
Epoch: 6447, Batch Gradient Norm: 31.74189840420706
Epoch: 6447, Batch Gradient Norm after: 22.36067777083903
Epoch 6448/10000, Prediction Accuracy = 59.714%, Loss = 0.7828769683837891
Epoch: 6448, Batch Gradient Norm: 29.718358869773432
Epoch: 6448, Batch Gradient Norm after: 22.360678705665972
Epoch 6449/10000, Prediction Accuracy = 59.73%, Loss = 0.7780457854270935
Epoch: 6449, Batch Gradient Norm: 31.739309351066826
Epoch: 6449, Batch Gradient Norm after: 22.360678688882228
Epoch 6450/10000, Prediction Accuracy = 59.664%, Loss = 0.7827974796295166
Epoch: 6450, Batch Gradient Norm: 29.711028319264727
Epoch: 6450, Batch Gradient Norm after: 22.360678571584106
Epoch 6451/10000, Prediction Accuracy = 59.73199999999999%, Loss = 0.7779389023780823
Epoch: 6451, Batch Gradient Norm: 31.73965951586726
Epoch: 6451, Batch Gradient Norm after: 22.36067695783426
Epoch 6452/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7827386617660522
Epoch: 6452, Batch Gradient Norm: 29.707671495877936
Epoch: 6452, Batch Gradient Norm after: 22.360679005568237
Epoch 6453/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.7778932571411132
Epoch: 6453, Batch Gradient Norm: 31.7390301235485
Epoch: 6453, Batch Gradient Norm after: 22.36067999799264
Epoch 6454/10000, Prediction Accuracy = 59.742%, Loss = 0.7827444076538086
Epoch: 6454, Batch Gradient Norm: 29.696616003535983
Epoch: 6454, Batch Gradient Norm after: 22.360677051972722
Epoch 6455/10000, Prediction Accuracy = 59.7%, Loss = 0.7779083371162414
Epoch: 6455, Batch Gradient Norm: 31.736691155122312
Epoch: 6455, Batch Gradient Norm after: 22.360679069970224
Epoch 6456/10000, Prediction Accuracy = 59.722%, Loss = 0.7827159762382507
Epoch: 6456, Batch Gradient Norm: 29.686666648482916
Epoch: 6456, Batch Gradient Norm after: 22.360676650441818
Epoch 6457/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.7777840852737427
Epoch: 6457, Batch Gradient Norm: 31.73438821438165
Epoch: 6457, Batch Gradient Norm after: 22.36067957886831
Epoch 6458/10000, Prediction Accuracy = 59.718%, Loss = 0.7825337648391724
Epoch: 6458, Batch Gradient Norm: 29.6856826142553
Epoch: 6458, Batch Gradient Norm after: 22.36067750246703
Epoch 6459/10000, Prediction Accuracy = 59.767999999999994%, Loss = 0.7776034832000732
Epoch: 6459, Batch Gradient Norm: 31.73156190704481
Epoch: 6459, Batch Gradient Norm after: 22.360677786105494
Epoch 6460/10000, Prediction Accuracy = 59.7%, Loss = 0.7823890089988709
Epoch: 6460, Batch Gradient Norm: 29.68286175087014
Epoch: 6460, Batch Gradient Norm after: 22.36067597912487
Epoch 6461/10000, Prediction Accuracy = 59.722%, Loss = 0.7774935841560364
Epoch: 6461, Batch Gradient Norm: 31.72920682544305
Epoch: 6461, Batch Gradient Norm after: 22.360677393399506
Epoch 6462/10000, Prediction Accuracy = 59.67999999999999%, Loss = 0.7823206543922424
Epoch: 6462, Batch Gradient Norm: 29.678091379396637
Epoch: 6462, Batch Gradient Norm after: 22.360678197397064
Epoch 6463/10000, Prediction Accuracy = 59.766%, Loss = 0.7774059772491455
Epoch: 6463, Batch Gradient Norm: 31.728733097132377
Epoch: 6463, Batch Gradient Norm after: 22.36067770932989
Epoch 6464/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7822690963745117
Epoch: 6464, Batch Gradient Norm: 29.672155092768737
Epoch: 6464, Batch Gradient Norm after: 22.360675544411396
Epoch 6465/10000, Prediction Accuracy = 59.694%, Loss = 0.7773945569992066
Epoch: 6465, Batch Gradient Norm: 31.7276399104817
Epoch: 6465, Batch Gradient Norm after: 22.36067967230516
Epoch 6466/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7822953820228576
Epoch: 6466, Batch Gradient Norm: 29.667454906584638
Epoch: 6466, Batch Gradient Norm after: 22.360677471075977
Epoch 6467/10000, Prediction Accuracy = 59.702%, Loss = 0.7773782014846802
Epoch: 6467, Batch Gradient Norm: 31.722808097179335
Epoch: 6467, Batch Gradient Norm after: 22.360680231873406
Epoch 6468/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7821988344192505
Epoch: 6468, Batch Gradient Norm: 29.658353713402423
Epoch: 6468, Batch Gradient Norm after: 22.36067695646119
Epoch 6469/10000, Prediction Accuracy = 59.678%, Loss = 0.7772250056266785
Epoch: 6469, Batch Gradient Norm: 31.721165955427004
Epoch: 6469, Batch Gradient Norm after: 22.36067820478747
Epoch 6470/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7820144534111023
Epoch: 6470, Batch Gradient Norm: 29.655768436228698
Epoch: 6470, Batch Gradient Norm after: 22.36067616026106
Epoch 6471/10000, Prediction Accuracy = 59.75999999999999%, Loss = 0.7770676612854004
Epoch: 6471, Batch Gradient Norm: 31.718926469890068
Epoch: 6471, Batch Gradient Norm after: 22.360678549984943
Epoch 6472/10000, Prediction Accuracy = 59.698%, Loss = 0.7819060444831848
Epoch: 6472, Batch Gradient Norm: 29.652056212696056
Epoch: 6472, Batch Gradient Norm after: 22.360679964474834
Epoch 6473/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7769553780555725
Epoch: 6473, Batch Gradient Norm: 31.71641081056897
Epoch: 6473, Batch Gradient Norm after: 22.36067828179706
Epoch 6474/10000, Prediction Accuracy = 59.709999999999994%, Loss = 0.7818394064903259
Epoch: 6474, Batch Gradient Norm: 29.645001725119137
Epoch: 6474, Batch Gradient Norm after: 22.360675989667257
Epoch 6475/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7768755316734314
Epoch: 6475, Batch Gradient Norm: 31.721100892941934
Epoch: 6475, Batch Gradient Norm after: 22.360678706177623
Epoch 6476/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7818207740783691
Epoch: 6476, Batch Gradient Norm: 29.63695819368415
Epoch: 6476, Batch Gradient Norm after: 22.360676484962827
Epoch 6477/10000, Prediction Accuracy = 59.709999999999994%, Loss = 0.7768827438354492
Epoch: 6477, Batch Gradient Norm: 31.718221940278926
Epoch: 6477, Batch Gradient Norm after: 22.360677174004167
Epoch 6478/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7818424701690674
Epoch: 6478, Batch Gradient Norm: 29.62956033273813
Epoch: 6478, Batch Gradient Norm after: 22.360678322612436
Epoch 6479/10000, Prediction Accuracy = 59.714%, Loss = 0.7768297314643859
Epoch: 6479, Batch Gradient Norm: 31.71152560830027
Epoch: 6479, Batch Gradient Norm after: 22.360677730203104
Epoch 6480/10000, Prediction Accuracy = 59.74399999999999%, Loss = 0.7816958665847779
Epoch: 6480, Batch Gradient Norm: 29.628973457143505
Epoch: 6480, Batch Gradient Norm after: 22.36067885668016
Epoch 6481/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.7766612648963929
Epoch: 6481, Batch Gradient Norm: 31.70476261515814
Epoch: 6481, Batch Gradient Norm after: 22.36067757180514
Epoch 6482/10000, Prediction Accuracy = 59.70799999999999%, Loss = 0.781505036354065
Epoch: 6482, Batch Gradient Norm: 29.63362237665796
Epoch: 6482, Batch Gradient Norm after: 22.360677209963747
Epoch 6483/10000, Prediction Accuracy = 59.74799999999999%, Loss = 0.7765358090400696
Epoch: 6483, Batch Gradient Norm: 31.697441578999836
Epoch: 6483, Batch Gradient Norm after: 22.36067880834083
Epoch 6484/10000, Prediction Accuracy = 59.70799999999999%, Loss = 0.7814079880714416
Epoch: 6484, Batch Gradient Norm: 29.63248059901597
Epoch: 6484, Batch Gradient Norm after: 22.360678130508624
Epoch 6485/10000, Prediction Accuracy = 59.73%, Loss = 0.776444387435913
Epoch: 6485, Batch Gradient Norm: 31.69508974065399
Epoch: 6485, Batch Gradient Norm after: 22.36067842016154
Epoch 6486/10000, Prediction Accuracy = 59.714%, Loss = 0.7813355922698975
Epoch: 6486, Batch Gradient Norm: 29.629990616847998
Epoch: 6486, Batch Gradient Norm after: 22.36067857284601
Epoch 6487/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7763980746269226
Epoch: 6487, Batch Gradient Norm: 31.69417050692838
Epoch: 6487, Batch Gradient Norm after: 22.36067882609996
Epoch 6488/10000, Prediction Accuracy = 59.74399999999999%, Loss = 0.7813468098640441
Epoch: 6488, Batch Gradient Norm: 29.624342780720966
Epoch: 6488, Batch Gradient Norm after: 22.360677466423894
Epoch 6489/10000, Prediction Accuracy = 59.73%, Loss = 0.7764245629310608
Epoch: 6489, Batch Gradient Norm: 31.688767619815096
Epoch: 6489, Batch Gradient Norm after: 22.36067991402036
Epoch 6490/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7813161134719848
Epoch: 6490, Batch Gradient Norm: 29.619585169000977
Epoch: 6490, Batch Gradient Norm after: 22.36067858645454
Epoch 6491/10000, Prediction Accuracy = 59.686%, Loss = 0.776316225528717
Epoch: 6491, Batch Gradient Norm: 31.681850219200363
Epoch: 6491, Batch Gradient Norm after: 22.360678421719122
Epoch 6492/10000, Prediction Accuracy = 59.72800000000001%, Loss = 0.7811222672462463
Epoch: 6492, Batch Gradient Norm: 29.620100210605766
Epoch: 6492, Batch Gradient Norm after: 22.36067913832923
Epoch 6493/10000, Prediction Accuracy = 59.774%, Loss = 0.7761464953422547
Epoch: 6493, Batch Gradient Norm: 31.676995285965802
Epoch: 6493, Batch Gradient Norm after: 22.360677736403186
Epoch 6494/10000, Prediction Accuracy = 59.7%, Loss = 0.7809679865837097
Epoch: 6494, Batch Gradient Norm: 29.620551258048547
Epoch: 6494, Batch Gradient Norm after: 22.36067741397405
Epoch 6495/10000, Prediction Accuracy = 59.73599999999999%, Loss = 0.7760408401489258
Epoch: 6495, Batch Gradient Norm: 31.6681683356936
Epoch: 6495, Batch Gradient Norm after: 22.360679631285358
Epoch 6496/10000, Prediction Accuracy = 59.702%, Loss = 0.7808824181556702
Epoch: 6496, Batch Gradient Norm: 29.61931778641306
Epoch: 6496, Batch Gradient Norm after: 22.360677629537108
Epoch 6497/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7759573101997376
Epoch: 6497, Batch Gradient Norm: 31.66584845710335
Epoch: 6497, Batch Gradient Norm after: 22.36067971455223
Epoch 6498/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7808232188224793
Epoch: 6498, Batch Gradient Norm: 29.619254517893207
Epoch: 6498, Batch Gradient Norm after: 22.360679165359326
Epoch 6499/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7759464144706726
Epoch: 6499, Batch Gradient Norm: 31.661813405683937
Epoch: 6499, Batch Gradient Norm after: 22.36067834798371
Epoch 6500/10000, Prediction Accuracy = 59.742%, Loss = 0.7808400988578796
Epoch: 6500, Batch Gradient Norm: 29.620141536912364
Epoch: 6500, Batch Gradient Norm after: 22.360677621146326
Epoch 6501/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7759638547897338
Epoch: 6501, Batch Gradient Norm: 31.650963063483236
Epoch: 6501, Batch Gradient Norm after: 22.360679508697928
Epoch 6502/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7807411432266236
Epoch: 6502, Batch Gradient Norm: 29.614608435185595
Epoch: 6502, Batch Gradient Norm after: 22.36067992275284
Epoch 6503/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7758258938789367
Epoch: 6503, Batch Gradient Norm: 31.647439625219114
Epoch: 6503, Batch Gradient Norm after: 22.36068023009142
Epoch 6504/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7805451154708862
Epoch: 6504, Batch Gradient Norm: 29.617059999492167
Epoch: 6504, Batch Gradient Norm after: 22.360677969688915
Epoch 6505/10000, Prediction Accuracy = 59.774%, Loss = 0.775665819644928
Epoch: 6505, Batch Gradient Norm: 31.64242423342113
Epoch: 6505, Batch Gradient Norm after: 22.36068078418362
Epoch 6506/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.780415415763855
Epoch: 6506, Batch Gradient Norm: 29.617068144193457
Epoch: 6506, Batch Gradient Norm after: 22.360677192467882
Epoch 6507/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7755677580833436
Epoch: 6507, Batch Gradient Norm: 31.634401047683077
Epoch: 6507, Batch Gradient Norm after: 22.360680579569664
Epoch 6508/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7803365826606751
Epoch: 6508, Batch Gradient Norm: 29.617948868814977
Epoch: 6508, Batch Gradient Norm after: 22.3606770938761
Epoch 6509/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7754971504211425
Epoch: 6509, Batch Gradient Norm: 31.6327986399828
Epoch: 6509, Batch Gradient Norm after: 22.360679127549535
Epoch 6510/10000, Prediction Accuracy = 59.754%, Loss = 0.7802964448928833
Epoch: 6510, Batch Gradient Norm: 29.614086533873905
Epoch: 6510, Batch Gradient Norm after: 22.36067897571732
Epoch 6511/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7755155086517334
Epoch: 6511, Batch Gradient Norm: 31.62930478055263
Epoch: 6511, Batch Gradient Norm after: 22.3606794376341
Epoch 6512/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7803323864936829
Epoch: 6512, Batch Gradient Norm: 29.60452347722806
Epoch: 6512, Batch Gradient Norm after: 22.360677129069433
Epoch 6513/10000, Prediction Accuracy = 59.734%, Loss = 0.7755054235458374
Epoch: 6513, Batch Gradient Norm: 31.6224322770331
Epoch: 6513, Batch Gradient Norm after: 22.360683020745736
Epoch 6514/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7801895260810852
Epoch: 6514, Batch Gradient Norm: 29.604073946457298
Epoch: 6514, Batch Gradient Norm after: 22.360678836528873
Epoch 6515/10000, Prediction Accuracy = 59.714%, Loss = 0.7753222584724426
Epoch: 6515, Batch Gradient Norm: 31.617092428193697
Epoch: 6515, Batch Gradient Norm after: 22.360676434413044
Epoch 6516/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7799861788749695
Epoch: 6516, Batch Gradient Norm: 29.6018552108525
Epoch: 6516, Batch Gradient Norm after: 22.36067864565651
Epoch 6517/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7751831889152527
Epoch: 6517, Batch Gradient Norm: 31.613484904100428
Epoch: 6517, Batch Gradient Norm after: 22.36068148909023
Epoch 6518/10000, Prediction Accuracy = 59.726%, Loss = 0.7798928260803223
Epoch: 6518, Batch Gradient Norm: 29.597792302773758
Epoch: 6518, Batch Gradient Norm after: 22.360680651241363
Epoch 6519/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7750783801078797
Epoch: 6519, Batch Gradient Norm: 31.61110944341172
Epoch: 6519, Batch Gradient Norm after: 22.360679419310006
Epoch 6520/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7798293948173523
Epoch: 6520, Batch Gradient Norm: 29.592406565170098
Epoch: 6520, Batch Gradient Norm after: 22.360678862583455
Epoch 6521/10000, Prediction Accuracy = 59.708000000000006%, Loss = 0.7750262498855591
Epoch: 6521, Batch Gradient Norm: 31.613429167605236
Epoch: 6521, Batch Gradient Norm after: 22.360679191702616
Epoch 6522/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7798388123512268
Epoch: 6522, Batch Gradient Norm: 29.58324576926677
Epoch: 6522, Batch Gradient Norm after: 22.36067506929006
Epoch 6523/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7750532984733581
Epoch: 6523, Batch Gradient Norm: 31.613169360266685
Epoch: 6523, Batch Gradient Norm after: 22.360678230673653
Epoch 6524/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7798288106918335
Epoch: 6524, Batch Gradient Norm: 29.568419770417336
Epoch: 6524, Batch Gradient Norm after: 22.36067718937832
Epoch 6525/10000, Prediction Accuracy = 59.730000000000004%, Loss = 0.7749358773231506
Epoch: 6525, Batch Gradient Norm: 31.60991480049283
Epoch: 6525, Batch Gradient Norm after: 22.36067847660947
Epoch 6526/10000, Prediction Accuracy = 59.714%, Loss = 0.7796533346176148
Epoch: 6526, Batch Gradient Norm: 29.566157907361085
Epoch: 6526, Batch Gradient Norm after: 22.36067888070279
Epoch 6527/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7747502088546753
Epoch: 6527, Batch Gradient Norm: 31.609379323386282
Epoch: 6527, Batch Gradient Norm after: 22.360679221159756
Epoch 6528/10000, Prediction Accuracy = 59.722%, Loss = 0.7795086979866028
Epoch: 6528, Batch Gradient Norm: 29.559230232695736
Epoch: 6528, Batch Gradient Norm after: 22.36067992633303
Epoch 6529/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.7746248364448547
Epoch: 6529, Batch Gradient Norm: 31.605994150998026
Epoch: 6529, Batch Gradient Norm after: 22.3606789743916
Epoch 6530/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7794411420822144
Epoch: 6530, Batch Gradient Norm: 29.55024247583897
Epoch: 6530, Batch Gradient Norm after: 22.360675420862684
Epoch 6531/10000, Prediction Accuracy = 59.766%, Loss = 0.7745257139205932
Epoch: 6531, Batch Gradient Norm: 31.606090216474076
Epoch: 6531, Batch Gradient Norm after: 22.360677063538585
Epoch 6532/10000, Prediction Accuracy = 59.693999999999996%, Loss = 0.7793917417526245
Epoch: 6532, Batch Gradient Norm: 29.548443381295478
Epoch: 6532, Batch Gradient Norm after: 22.360677492157354
Epoch 6533/10000, Prediction Accuracy = 59.75%, Loss = 0.7745103955268859
Epoch: 6533, Batch Gradient Norm: 31.60591463600136
Epoch: 6533, Batch Gradient Norm after: 22.36067956543211
Epoch 6534/10000, Prediction Accuracy = 59.754%, Loss = 0.779422378540039
Epoch: 6534, Batch Gradient Norm: 29.539950647723973
Epoch: 6534, Batch Gradient Norm after: 22.360676530760227
Epoch 6535/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7745233178138733
Epoch: 6535, Batch Gradient Norm: 31.59923400875586
Epoch: 6535, Batch Gradient Norm after: 22.360679259098067
Epoch 6536/10000, Prediction Accuracy = 59.766%, Loss = 0.7793469309806824
Epoch: 6536, Batch Gradient Norm: 29.5359428257155
Epoch: 6536, Batch Gradient Norm after: 22.3606784606631
Epoch 6537/10000, Prediction Accuracy = 59.73%, Loss = 0.7743723392486572
Epoch: 6537, Batch Gradient Norm: 31.592465057311315
Epoch: 6537, Batch Gradient Norm after: 22.36067867601415
Epoch 6538/10000, Prediction Accuracy = 59.726%, Loss = 0.7791373252868652
Epoch: 6538, Batch Gradient Norm: 29.536820995005158
Epoch: 6538, Batch Gradient Norm after: 22.36067563455372
Epoch 6539/10000, Prediction Accuracy = 59.782%, Loss = 0.7742113590240478
Epoch: 6539, Batch Gradient Norm: 31.586502168076212
Epoch: 6539, Batch Gradient Norm after: 22.360679989344824
Epoch 6540/10000, Prediction Accuracy = 59.726%, Loss = 0.7790103793144226
Epoch: 6540, Batch Gradient Norm: 29.537846289068614
Epoch: 6540, Batch Gradient Norm after: 22.360678854136317
Epoch 6541/10000, Prediction Accuracy = 59.788%, Loss = 0.7741147518157959
Epoch: 6541, Batch Gradient Norm: 31.578347771948266
Epoch: 6541, Batch Gradient Norm after: 22.360679932983718
Epoch 6542/10000, Prediction Accuracy = 59.754%, Loss = 0.7789303541183472
Epoch: 6542, Batch Gradient Norm: 29.53255643666129
Epoch: 6542, Batch Gradient Norm after: 22.360675344178386
Epoch 6543/10000, Prediction Accuracy = 59.786%, Loss = 0.774039113521576
Epoch: 6543, Batch Gradient Norm: 31.58036796172081
Epoch: 6543, Batch Gradient Norm after: 22.360679021117043
Epoch 6544/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7789008259773255
Epoch: 6544, Batch Gradient Norm: 29.529523248261473
Epoch: 6544, Batch Gradient Norm after: 22.360679166335174
Epoch 6545/10000, Prediction Accuracy = 59.75%, Loss = 0.7740604519844055
Epoch: 6545, Batch Gradient Norm: 31.574751492792167
Epoch: 6545, Batch Gradient Norm after: 22.36068033484326
Epoch 6546/10000, Prediction Accuracy = 59.766000000000005%, Loss = 0.7789318680763244
Epoch: 6546, Batch Gradient Norm: 29.52141716062058
Epoch: 6546, Batch Gradient Norm after: 22.360678642302755
Epoch 6547/10000, Prediction Accuracy = 59.75%, Loss = 0.7740367174148559
Epoch: 6547, Batch Gradient Norm: 31.570167059224804
Epoch: 6547, Batch Gradient Norm after: 22.3606799678152
Epoch 6548/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7787980556488037
Epoch: 6548, Batch Gradient Norm: 29.521102913213408
Epoch: 6548, Batch Gradient Norm after: 22.36067710274079
Epoch 6549/10000, Prediction Accuracy = 59.73599999999999%, Loss = 0.7738578915596008
Epoch: 6549, Batch Gradient Norm: 31.564762418438267
Epoch: 6549, Batch Gradient Norm after: 22.36067911814253
Epoch 6550/10000, Prediction Accuracy = 59.722%, Loss = 0.7785974025726319
Epoch: 6550, Batch Gradient Norm: 29.523238023493377
Epoch: 6550, Batch Gradient Norm after: 22.360676797236156
Epoch 6551/10000, Prediction Accuracy = 59.802%, Loss = 0.7737152695655822
Epoch: 6551, Batch Gradient Norm: 31.55741713650287
Epoch: 6551, Batch Gradient Norm after: 22.360678072966653
Epoch 6552/10000, Prediction Accuracy = 59.714%, Loss = 0.7784893035888671
Epoch: 6552, Batch Gradient Norm: 29.52327589441894
Epoch: 6552, Batch Gradient Norm after: 22.36067628918024
Epoch 6553/10000, Prediction Accuracy = 59.786%, Loss = 0.7736271142959594
Epoch: 6553, Batch Gradient Norm: 31.550135459755943
Epoch: 6553, Batch Gradient Norm after: 22.360681484374908
Epoch 6554/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7784151554107666
Epoch: 6554, Batch Gradient Norm: 29.523564788715056
Epoch: 6554, Batch Gradient Norm after: 22.36067874339878
Epoch 6555/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7735792875289917
Epoch: 6555, Batch Gradient Norm: 31.550892210207547
Epoch: 6555, Batch Gradient Norm after: 22.360679187824203
Epoch 6556/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7784177899360657
Epoch: 6556, Batch Gradient Norm: 29.517930085747555
Epoch: 6556, Batch Gradient Norm after: 22.360676695373762
Epoch 6557/10000, Prediction Accuracy = 59.766%, Loss = 0.7736274838447571
Epoch: 6557, Batch Gradient Norm: 31.545586599482714
Epoch: 6557, Batch Gradient Norm after: 22.360681023781886
Epoch 6558/10000, Prediction Accuracy = 59.80999999999999%, Loss = 0.7784090876579285
Epoch: 6558, Batch Gradient Norm: 29.514003881897448
Epoch: 6558, Batch Gradient Norm after: 22.36067527221879
Epoch 6559/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7735293865203857
Epoch: 6559, Batch Gradient Norm: 31.537527559529696
Epoch: 6559, Batch Gradient Norm after: 22.360678656789958
Epoch 6560/10000, Prediction Accuracy = 59.734%, Loss = 0.7782236218452454
Epoch: 6560, Batch Gradient Norm: 29.510674017466737
Epoch: 6560, Batch Gradient Norm after: 22.360676448448796
Epoch 6561/10000, Prediction Accuracy = 59.774%, Loss = 0.7733486771583558
Epoch: 6561, Batch Gradient Norm: 31.536849360657364
Epoch: 6561, Batch Gradient Norm after: 22.3606784523996
Epoch 6562/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.778062891960144
Epoch: 6562, Batch Gradient Norm: 29.5064817783581
Epoch: 6562, Batch Gradient Norm after: 22.36067836424333
Epoch 6563/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7732298970222473
Epoch: 6563, Batch Gradient Norm: 31.53308434698044
Epoch: 6563, Batch Gradient Norm after: 22.36067726870627
Epoch 6564/10000, Prediction Accuracy = 59.730000000000004%, Loss = 0.7779855966567993
Epoch: 6564, Batch Gradient Norm: 29.50501123848963
Epoch: 6564, Batch Gradient Norm after: 22.360676245702496
Epoch 6565/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.7731353163719177
Epoch: 6565, Batch Gradient Norm: 31.530989690660576
Epoch: 6565, Batch Gradient Norm after: 22.360679180427635
Epoch 6566/10000, Prediction Accuracy = 59.73%, Loss = 0.7779250860214233
Epoch: 6566, Batch Gradient Norm: 29.499850711993723
Epoch: 6566, Batch Gradient Norm after: 22.360679506708834
Epoch 6567/10000, Prediction Accuracy = 59.766%, Loss = 0.7731072306632996
Epoch: 6567, Batch Gradient Norm: 31.528490766495775
Epoch: 6567, Batch Gradient Norm after: 22.36067909516213
Epoch 6568/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7779445648193359
Epoch: 6568, Batch Gradient Norm: 29.495243399785597
Epoch: 6568, Batch Gradient Norm after: 22.3606785501315
Epoch 6569/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7731430411338807
Epoch: 6569, Batch Gradient Norm: 31.522928516556913
Epoch: 6569, Batch Gradient Norm after: 22.360678179792586
Epoch 6570/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7779052257537842
Epoch: 6570, Batch Gradient Norm: 29.487316342670013
Epoch: 6570, Batch Gradient Norm after: 22.360675613412297
Epoch 6571/10000, Prediction Accuracy = 59.769999999999996%, Loss = 0.7730145573616027
Epoch: 6571, Batch Gradient Norm: 31.515593872077172
Epoch: 6571, Batch Gradient Norm after: 22.360678846896413
Epoch 6572/10000, Prediction Accuracy = 59.738%, Loss = 0.777700412273407
Epoch: 6572, Batch Gradient Norm: 29.487087546360716
Epoch: 6572, Batch Gradient Norm after: 22.360677022534613
Epoch 6573/10000, Prediction Accuracy = 59.788%, Loss = 0.7728260636329651
Epoch: 6573, Batch Gradient Norm: 31.515804078876062
Epoch: 6573, Batch Gradient Norm after: 22.360678139683372
Epoch 6574/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7775678634643555
Epoch: 6574, Batch Gradient Norm: 29.48292081733878
Epoch: 6574, Batch Gradient Norm after: 22.360680252991262
Epoch 6575/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.7727113723754883
Epoch: 6575, Batch Gradient Norm: 31.514254505363656
Epoch: 6575, Batch Gradient Norm after: 22.360678574584124
Epoch 6576/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7774986982345581
Epoch: 6576, Batch Gradient Norm: 29.480561747415265
Epoch: 6576, Batch Gradient Norm after: 22.360674774751963
Epoch 6577/10000, Prediction Accuracy = 59.81199999999999%, Loss = 0.7726231575012207
Epoch: 6577, Batch Gradient Norm: 31.512273400981922
Epoch: 6577, Batch Gradient Norm after: 22.360677950346453
Epoch 6578/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7774537205696106
Epoch: 6578, Batch Gradient Norm: 29.476337607322673
Epoch: 6578, Batch Gradient Norm after: 22.360675064333257
Epoch 6579/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7726259708404541
Epoch: 6579, Batch Gradient Norm: 31.510824623597077
Epoch: 6579, Batch Gradient Norm after: 22.3606765846959
Epoch 6580/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7774749398231506
Epoch: 6580, Batch Gradient Norm: 29.468575386339346
Epoch: 6580, Batch Gradient Norm after: 22.36067771540537
Epoch 6581/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7726335525512695
Epoch: 6581, Batch Gradient Norm: 31.502783907713763
Epoch: 6581, Batch Gradient Norm after: 22.360678632759384
Epoch 6582/10000, Prediction Accuracy = 59.8%, Loss = 0.7773882627487183
Epoch: 6582, Batch Gradient Norm: 29.466260986318936
Epoch: 6582, Batch Gradient Norm after: 22.360678060261243
Epoch 6583/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7724854230880738
Epoch: 6583, Batch Gradient Norm: 31.49891610681495
Epoch: 6583, Batch Gradient Norm after: 22.360678696321298
Epoch 6584/10000, Prediction Accuracy = 59.75600000000001%, Loss = 0.77718106508255
Epoch: 6584, Batch Gradient Norm: 29.46651541540011
Epoch: 6584, Batch Gradient Norm after: 22.36067775236908
Epoch 6585/10000, Prediction Accuracy = 59.85%, Loss = 0.772316324710846
Epoch: 6585, Batch Gradient Norm: 31.497273091183967
Epoch: 6585, Batch Gradient Norm after: 22.360678924103304
Epoch 6586/10000, Prediction Accuracy = 59.708000000000006%, Loss = 0.7770674467086792
Epoch: 6586, Batch Gradient Norm: 29.46625636609628
Epoch: 6586, Batch Gradient Norm after: 22.360681006129045
Epoch 6587/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7722112655639648
Epoch: 6587, Batch Gradient Norm: 31.490076496037695
Epoch: 6587, Batch Gradient Norm after: 22.36067859570308
Epoch 6588/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7769916653633118
Epoch: 6588, Batch Gradient Norm: 29.46572167369168
Epoch: 6588, Batch Gradient Norm after: 22.36067738600202
Epoch 6589/10000, Prediction Accuracy = 59.802%, Loss = 0.7721423983573914
Epoch: 6589, Batch Gradient Norm: 31.489228847358845
Epoch: 6589, Batch Gradient Norm after: 22.36068026025666
Epoch 6590/10000, Prediction Accuracy = 59.754%, Loss = 0.7769650220870972
Epoch: 6590, Batch Gradient Norm: 29.462014296166995
Epoch: 6590, Batch Gradient Norm after: 22.360677745713453
Epoch 6591/10000, Prediction Accuracy = 59.77199999999999%, Loss = 0.7721860766410827
Epoch: 6591, Batch Gradient Norm: 31.484878001754296
Epoch: 6591, Batch Gradient Norm after: 22.360678074625945
Epoch 6592/10000, Prediction Accuracy = 59.8%, Loss = 0.7770074963569641
Epoch: 6592, Batch Gradient Norm: 29.453980315566042
Epoch: 6592, Batch Gradient Norm after: 22.360676194208807
Epoch 6593/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7721639513969422
Epoch: 6593, Batch Gradient Norm: 31.47499958856241
Epoch: 6593, Batch Gradient Norm after: 22.360678250308574
Epoch 6594/10000, Prediction Accuracy = 59.79%, Loss = 0.7768563270568848
Epoch: 6594, Batch Gradient Norm: 29.451897815435977
Epoch: 6594, Batch Gradient Norm after: 22.36067932838247
Epoch 6595/10000, Prediction Accuracy = 59.762%, Loss = 0.7719695329666137
Epoch: 6595, Batch Gradient Norm: 31.47322126482547
Epoch: 6595, Batch Gradient Norm after: 22.360678135229
Epoch 6596/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.776654851436615
Epoch: 6596, Batch Gradient Norm: 29.456482392875884
Epoch: 6596, Batch Gradient Norm after: 22.36067814721896
Epoch 6597/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7718338012695313
Epoch: 6597, Batch Gradient Norm: 31.468668663790645
Epoch: 6597, Batch Gradient Norm after: 22.36067986808262
Epoch 6598/10000, Prediction Accuracy = 59.717999999999996%, Loss = 0.7765548348426818
Epoch: 6598, Batch Gradient Norm: 29.454454215123036
Epoch: 6598, Batch Gradient Norm after: 22.360677356215273
Epoch 6599/10000, Prediction Accuracy = 59.798%, Loss = 0.7717431068420411
Epoch: 6599, Batch Gradient Norm: 31.463090086110622
Epoch: 6599, Batch Gradient Norm after: 22.360678825809966
Epoch 6600/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7764837622642518
Epoch: 6600, Batch Gradient Norm: 29.45448560647688
Epoch: 6600, Batch Gradient Norm after: 22.3606779402994
Epoch 6601/10000, Prediction Accuracy = 59.778%, Loss = 0.7717036485671998
Epoch: 6601, Batch Gradient Norm: 31.46025420795421
Epoch: 6601, Batch Gradient Norm after: 22.36067759090727
Epoch 6602/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7764920830726624
Epoch: 6602, Batch Gradient Norm: 29.45149749368902
Epoch: 6602, Batch Gradient Norm after: 22.360677751167707
Epoch 6603/10000, Prediction Accuracy = 59.788%, Loss = 0.771750009059906
Epoch: 6603, Batch Gradient Norm: 31.453682815938066
Epoch: 6603, Batch Gradient Norm after: 22.3606779984177
Epoch 6604/10000, Prediction Accuracy = 59.8%, Loss = 0.7764750003814698
Epoch: 6604, Batch Gradient Norm: 29.44666197291043
Epoch: 6604, Batch Gradient Norm after: 22.36067702628363
Epoch 6605/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7716613173484802
Epoch: 6605, Batch Gradient Norm: 31.445393868384905
Epoch: 6605, Batch Gradient Norm after: 22.3606799517992
Epoch 6606/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.776282548904419
Epoch: 6606, Batch Gradient Norm: 29.447542127850284
Epoch: 6606, Batch Gradient Norm after: 22.36067794866214
Epoch 6607/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7714797496795655
Epoch: 6607, Batch Gradient Norm: 31.441157775469247
Epoch: 6607, Batch Gradient Norm after: 22.360679193901763
Epoch 6608/10000, Prediction Accuracy = 59.746%, Loss = 0.7761195302009583
Epoch: 6608, Batch Gradient Norm: 29.450964769714446
Epoch: 6608, Batch Gradient Norm after: 22.360676735061087
Epoch 6609/10000, Prediction Accuracy = 59.812%, Loss = 0.7713716268539429
Epoch: 6609, Batch Gradient Norm: 31.436287965567903
Epoch: 6609, Batch Gradient Norm after: 22.360677443717226
Epoch 6610/10000, Prediction Accuracy = 59.738%, Loss = 0.7760364890098572
Epoch: 6610, Batch Gradient Norm: 29.451570087060507
Epoch: 6610, Batch Gradient Norm after: 22.360677411800488
Epoch 6611/10000, Prediction Accuracy = 59.822%, Loss = 0.7712834596633911
Epoch: 6611, Batch Gradient Norm: 31.432393885507462
Epoch: 6611, Batch Gradient Norm after: 22.36067938596016
Epoch 6612/10000, Prediction Accuracy = 59.75%, Loss = 0.7759739398956299
Epoch: 6612, Batch Gradient Norm: 29.449711355157497
Epoch: 6612, Batch Gradient Norm after: 22.360674792258887
Epoch 6613/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.771271800994873
Epoch: 6613, Batch Gradient Norm: 31.428676390674315
Epoch: 6613, Batch Gradient Norm after: 22.360678959225233
Epoch 6614/10000, Prediction Accuracy = 59.83200000000001%, Loss = 0.7759925246238708
Epoch: 6614, Batch Gradient Norm: 29.445187138809338
Epoch: 6614, Batch Gradient Norm after: 22.360677440197843
Epoch 6615/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7713186860084533
Epoch: 6615, Batch Gradient Norm: 31.420453973267676
Epoch: 6615, Batch Gradient Norm after: 22.360679081225605
Epoch 6616/10000, Prediction Accuracy = 59.812%, Loss = 0.7759492039680481
Epoch: 6616, Batch Gradient Norm: 29.43927157262762
Epoch: 6616, Batch Gradient Norm after: 22.36067848696413
Epoch 6617/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.771180534362793
Epoch: 6617, Batch Gradient Norm: 31.417472439008883
Epoch: 6617, Batch Gradient Norm after: 22.360678655543573
Epoch 6618/10000, Prediction Accuracy = 59.767999999999994%, Loss = 0.7757395148277283
Epoch: 6618, Batch Gradient Norm: 29.43737574397627
Epoch: 6618, Batch Gradient Norm after: 22.360680112078516
Epoch 6619/10000, Prediction Accuracy = 59.822%, Loss = 0.7710046768188477
Epoch: 6619, Batch Gradient Norm: 31.414276491005452
Epoch: 6619, Batch Gradient Norm after: 22.360679351833777
Epoch 6620/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7756003975868225
Epoch: 6620, Batch Gradient Norm: 29.440967855791165
Epoch: 6620, Batch Gradient Norm after: 22.360677580015285
Epoch 6621/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.7709014773368835
Epoch: 6621, Batch Gradient Norm: 31.407215082173625
Epoch: 6621, Batch Gradient Norm after: 22.36067978224019
Epoch 6622/10000, Prediction Accuracy = 59.742%, Loss = 0.7755221247673034
Epoch: 6622, Batch Gradient Norm: 29.441683050866036
Epoch: 6622, Batch Gradient Norm after: 22.360678433954398
Epoch 6623/10000, Prediction Accuracy = 59.814%, Loss = 0.7708245873451233
Epoch: 6623, Batch Gradient Norm: 31.402496981504125
Epoch: 6623, Batch Gradient Norm after: 22.360679129537818
Epoch 6624/10000, Prediction Accuracy = 59.762%, Loss = 0.7754791736602783
Epoch: 6624, Batch Gradient Norm: 29.441037489754066
Epoch: 6624, Batch Gradient Norm after: 22.36067919357703
Epoch 6625/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7708413362503052
Epoch: 6625, Batch Gradient Norm: 31.398981539504295
Epoch: 6625, Batch Gradient Norm after: 22.36068015133983
Epoch 6626/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7755083560943603
Epoch: 6626, Batch Gradient Norm: 29.43648764579146
Epoch: 6626, Batch Gradient Norm after: 22.36067677013275
Epoch 6627/10000, Prediction Accuracy = 59.786%, Loss = 0.7708553433418274
Epoch: 6627, Batch Gradient Norm: 31.390687524129795
Epoch: 6627, Batch Gradient Norm after: 22.36068006127326
Epoch 6628/10000, Prediction Accuracy = 59.822%, Loss = 0.7754028558731079
Epoch: 6628, Batch Gradient Norm: 29.433421711153795
Epoch: 6628, Batch Gradient Norm after: 22.36067881414882
Epoch 6629/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.770689845085144
Epoch: 6629, Batch Gradient Norm: 31.38587933061021
Epoch: 6629, Batch Gradient Norm after: 22.36067887222111
Epoch 6630/10000, Prediction Accuracy = 59.758%, Loss = 0.7751936912536621
Epoch: 6630, Batch Gradient Norm: 29.435136701721813
Epoch: 6630, Batch Gradient Norm after: 22.360679229200088
Epoch 6631/10000, Prediction Accuracy = 59.836%, Loss = 0.7705348014831543
Epoch: 6631, Batch Gradient Norm: 31.382298454286644
Epoch: 6631, Batch Gradient Norm after: 22.36067879852986
Epoch 6632/10000, Prediction Accuracy = 59.722%, Loss = 0.7750809073448182
Epoch: 6632, Batch Gradient Norm: 29.435320651844616
Epoch: 6632, Batch Gradient Norm after: 22.360677394159858
Epoch 6633/10000, Prediction Accuracy = 59.802%, Loss = 0.7704455375671386
Epoch: 6633, Batch Gradient Norm: 31.37623026162781
Epoch: 6633, Batch Gradient Norm after: 22.36067897891577
Epoch 6634/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7750076532363892
Epoch: 6634, Batch Gradient Norm: 29.436563987250498
Epoch: 6634, Batch Gradient Norm after: 22.360677296358553
Epoch 6635/10000, Prediction Accuracy = 59.798%, Loss = 0.7703822016716003
Epoch: 6635, Batch Gradient Norm: 31.37231373674789
Epoch: 6635, Batch Gradient Norm after: 22.360677696640643
Epoch 6636/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.7749941229820252
Epoch: 6636, Batch Gradient Norm: 29.43489077505954
Epoch: 6636, Batch Gradient Norm after: 22.360678077151135
Epoch 6637/10000, Prediction Accuracy = 59.806%, Loss = 0.7704389452934265
Epoch: 6637, Batch Gradient Norm: 31.36630684072725
Epoch: 6637, Batch Gradient Norm after: 22.36067772472995
Epoch 6638/10000, Prediction Accuracy = 59.831999999999994%, Loss = 0.7750280261039734
Epoch: 6638, Batch Gradient Norm: 29.42625139956235
Epoch: 6638, Batch Gradient Norm after: 22.360677677235643
Epoch 6639/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7703824400901794
Epoch: 6639, Batch Gradient Norm: 31.359935105195753
Epoch: 6639, Batch Gradient Norm after: 22.3606788137356
Epoch 6640/10000, Prediction Accuracy = 59.788%, Loss = 0.7748507380485534
Epoch: 6640, Batch Gradient Norm: 29.42240036637462
Epoch: 6640, Batch Gradient Norm after: 22.36067840887671
Epoch 6641/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.770183265209198
Epoch: 6641, Batch Gradient Norm: 31.35950008897413
Epoch: 6641, Batch Gradient Norm after: 22.360678977099834
Epoch 6642/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7746618628501892
Epoch: 6642, Batch Gradient Norm: 29.426289449813176
Epoch: 6642, Batch Gradient Norm after: 22.3606779688611
Epoch 6643/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7700633883476258
Epoch: 6643, Batch Gradient Norm: 31.353423378653417
Epoch: 6643, Batch Gradient Norm after: 22.360679592131966
Epoch 6644/10000, Prediction Accuracy = 59.746%, Loss = 0.7745733737945557
Epoch: 6644, Batch Gradient Norm: 29.430589921665288
Epoch: 6644, Batch Gradient Norm after: 22.360677724789962
Epoch 6645/10000, Prediction Accuracy = 59.81600000000001%, Loss = 0.7699827075004577
Epoch: 6645, Batch Gradient Norm: 31.346505113354745
Epoch: 6645, Batch Gradient Norm after: 22.36067641682747
Epoch 6646/10000, Prediction Accuracy = 59.746%, Loss = 0.7744986653327942
Epoch: 6646, Batch Gradient Norm: 29.43048752141494
Epoch: 6646, Batch Gradient Norm after: 22.360679875259752
Epoch 6647/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7699549078941346
Epoch: 6647, Batch Gradient Norm: 31.342716572229207
Epoch: 6647, Batch Gradient Norm after: 22.360675750274346
Epoch 6648/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.7745184421539306
Epoch: 6648, Batch Gradient Norm: 29.426873788943727
Epoch: 6648, Batch Gradient Norm after: 22.36067879443568
Epoch 6649/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.7700083494186402
Epoch: 6649, Batch Gradient Norm: 31.3357330539902
Epoch: 6649, Batch Gradient Norm after: 22.360680189604224
Epoch 6650/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.7744945764541626
Epoch: 6650, Batch Gradient Norm: 29.4210095276218
Epoch: 6650, Batch Gradient Norm after: 22.36067706622384
Epoch 6651/10000, Prediction Accuracy = 59.82199999999999%, Loss = 0.7698893785476685
Epoch: 6651, Batch Gradient Norm: 31.331286872797925
Epoch: 6651, Batch Gradient Norm after: 22.360676685504288
Epoch 6652/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.774277925491333
Epoch: 6652, Batch Gradient Norm: 29.422924316559648
Epoch: 6652, Batch Gradient Norm after: 22.360677503964173
Epoch 6653/10000, Prediction Accuracy = 59.81999999999999%, Loss = 0.7697168111801147
Epoch: 6653, Batch Gradient Norm: 31.328447934161684
Epoch: 6653, Batch Gradient Norm after: 22.360678580736167
Epoch 6654/10000, Prediction Accuracy = 59.75%, Loss = 0.7741348624229432
Epoch: 6654, Batch Gradient Norm: 29.42314844142401
Epoch: 6654, Batch Gradient Norm after: 22.360677039095993
Epoch 6655/10000, Prediction Accuracy = 59.812%, Loss = 0.7696076393127441
Epoch: 6655, Batch Gradient Norm: 31.32269722925763
Epoch: 6655, Batch Gradient Norm after: 22.36067818857133
Epoch 6656/10000, Prediction Accuracy = 59.762%, Loss = 0.774058198928833
Epoch: 6656, Batch Gradient Norm: 29.424167201050167
Epoch: 6656, Batch Gradient Norm after: 22.360676869526955
Epoch 6657/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7695302367210388
Epoch: 6657, Batch Gradient Norm: 31.31893479968002
Epoch: 6657, Batch Gradient Norm after: 22.360680196423615
Epoch 6658/10000, Prediction Accuracy = 59.766%, Loss = 0.7740105867385865
Epoch: 6658, Batch Gradient Norm: 29.421055097609003
Epoch: 6658, Batch Gradient Norm after: 22.360679068988848
Epoch 6659/10000, Prediction Accuracy = 59.822%, Loss = 0.7695317983627319
Epoch: 6659, Batch Gradient Norm: 31.316983738143765
Epoch: 6659, Batch Gradient Norm after: 22.360679558634455
Epoch 6660/10000, Prediction Accuracy = 59.806%, Loss = 0.7740437984466553
Epoch: 6660, Batch Gradient Norm: 29.4138568600593
Epoch: 6660, Batch Gradient Norm after: 22.36067758005949
Epoch 6661/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.7695384979248047
Epoch: 6661, Batch Gradient Norm: 31.312330896988282
Epoch: 6661, Batch Gradient Norm after: 22.360678192814337
Epoch 6662/10000, Prediction Accuracy = 59.814%, Loss = 0.7739675283432007
Epoch: 6662, Batch Gradient Norm: 29.40781645706642
Epoch: 6662, Batch Gradient Norm after: 22.36067675335033
Epoch 6663/10000, Prediction Accuracy = 59.826%, Loss = 0.7693835496902466
Epoch: 6663, Batch Gradient Norm: 31.309671603785144
Epoch: 6663, Batch Gradient Norm after: 22.360676306837426
Epoch 6664/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7737633228302002
Epoch: 6664, Batch Gradient Norm: 29.404039982635783
Epoch: 6664, Batch Gradient Norm after: 22.360677540795436
Epoch 6665/10000, Prediction Accuracy = 59.826%, Loss = 0.7692241191864013
Epoch: 6665, Batch Gradient Norm: 31.306365080163097
Epoch: 6665, Batch Gradient Norm after: 22.360678714916848
Epoch 6666/10000, Prediction Accuracy = 59.758%, Loss = 0.7736479282379151
Epoch: 6666, Batch Gradient Norm: 29.40306403512179
Epoch: 6666, Batch Gradient Norm after: 22.36067723170118
Epoch 6667/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7691190481185913
Epoch: 6667, Batch Gradient Norm: 31.300588264831653
Epoch: 6667, Batch Gradient Norm after: 22.360677401119304
Epoch 6668/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7735756516456604
Epoch: 6668, Batch Gradient Norm: 29.40332939239468
Epoch: 6668, Batch Gradient Norm after: 22.360678019676
Epoch 6669/10000, Prediction Accuracy = 59.788%, Loss = 0.7690582275390625
Epoch: 6669, Batch Gradient Norm: 31.29549139099885
Epoch: 6669, Batch Gradient Norm after: 22.360678855015607
Epoch 6670/10000, Prediction Accuracy = 59.79%, Loss = 0.7735514283180237
Epoch: 6670, Batch Gradient Norm: 29.403006519646052
Epoch: 6670, Batch Gradient Norm after: 22.36067595015286
Epoch 6671/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7691091775894165
Epoch: 6671, Batch Gradient Norm: 31.293608096183235
Epoch: 6671, Batch Gradient Norm after: 22.36067963730083
Epoch 6672/10000, Prediction Accuracy = 59.83599999999999%, Loss = 0.7735857486724853
Epoch: 6672, Batch Gradient Norm: 29.399287998515376
Epoch: 6672, Batch Gradient Norm after: 22.3606766249786
Epoch 6673/10000, Prediction Accuracy = 59.812%, Loss = 0.7690842747688293
Epoch: 6673, Batch Gradient Norm: 31.283213681739223
Epoch: 6673, Batch Gradient Norm after: 22.360676220110303
Epoch 6674/10000, Prediction Accuracy = 59.79599999999999%, Loss = 0.7734163641929627
Epoch: 6674, Batch Gradient Norm: 29.400225785368303
Epoch: 6674, Batch Gradient Norm after: 22.36067892597503
Epoch 6675/10000, Prediction Accuracy = 59.824%, Loss = 0.7688961267471314
Epoch: 6675, Batch Gradient Norm: 31.276924344817235
Epoch: 6675, Batch Gradient Norm after: 22.360680928192437
Epoch 6676/10000, Prediction Accuracy = 59.786%, Loss = 0.7732171416282654
Epoch: 6676, Batch Gradient Norm: 29.405969380233604
Epoch: 6676, Batch Gradient Norm after: 22.360678566330794
Epoch 6677/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7687739491462707
Epoch: 6677, Batch Gradient Norm: 31.26799684641703
Epoch: 6677, Batch Gradient Norm after: 22.360680244263012
Epoch 6678/10000, Prediction Accuracy = 59.754%, Loss = 0.7731234788894653
Epoch: 6678, Batch Gradient Norm: 29.407019777849232
Epoch: 6678, Batch Gradient Norm after: 22.360675375523595
Epoch 6679/10000, Prediction Accuracy = 59.814%, Loss = 0.7686923503875732
Epoch: 6679, Batch Gradient Norm: 31.261964325683657
Epoch: 6679, Batch Gradient Norm after: 22.360678402439532
Epoch 6680/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7730505704879761
Epoch: 6680, Batch Gradient Norm: 29.407959733653957
Epoch: 6680, Batch Gradient Norm after: 22.36067696583471
Epoch 6681/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7686628937721253
Epoch: 6681, Batch Gradient Norm: 31.259618649458904
Epoch: 6681, Batch Gradient Norm after: 22.360679253818013
Epoch 6682/10000, Prediction Accuracy = 59.798%, Loss = 0.7730612993240357
Epoch: 6682, Batch Gradient Norm: 29.40367188143749
Epoch: 6682, Batch Gradient Norm after: 22.360678499533364
Epoch 6683/10000, Prediction Accuracy = 59.83200000000001%, Loss = 0.7687224864959716
Epoch: 6683, Batch Gradient Norm: 31.253265009820623
Epoch: 6683, Batch Gradient Norm after: 22.360678697371398
Epoch 6684/10000, Prediction Accuracy = 59.85%, Loss = 0.7730584263801574
Epoch: 6684, Batch Gradient Norm: 29.398467469198412
Epoch: 6684, Batch Gradient Norm after: 22.360676207706657
Epoch 6685/10000, Prediction Accuracy = 59.842%, Loss = 0.7686217188835144
Epoch: 6685, Batch Gradient Norm: 31.24533405141437
Epoch: 6685, Batch Gradient Norm after: 22.36067978704625
Epoch 6686/10000, Prediction Accuracy = 59.786%, Loss = 0.7728432774543762
Epoch: 6686, Batch Gradient Norm: 29.399081868731653
Epoch: 6686, Batch Gradient Norm after: 22.360676782947657
Epoch 6687/10000, Prediction Accuracy = 59.80400000000001%, Loss = 0.7684370279312134
Epoch: 6687, Batch Gradient Norm: 31.242759233521067
Epoch: 6687, Batch Gradient Norm after: 22.36067874790753
Epoch 6688/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7726894378662109
Epoch: 6688, Batch Gradient Norm: 29.399621582583645
Epoch: 6688, Batch Gradient Norm after: 22.3606777428235
Epoch 6689/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7683257579803466
Epoch: 6689, Batch Gradient Norm: 31.23753240840507
Epoch: 6689, Batch Gradient Norm after: 22.360677345422665
Epoch 6690/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7726186156272888
Epoch: 6690, Batch Gradient Norm: 29.39794518201287
Epoch: 6690, Batch Gradient Norm after: 22.36067585466889
Epoch 6691/10000, Prediction Accuracy = 59.812%, Loss = 0.7682425498962402
Epoch: 6691, Batch Gradient Norm: 31.23427649222271
Epoch: 6691, Batch Gradient Norm after: 22.360678670610863
Epoch 6692/10000, Prediction Accuracy = 59.798%, Loss = 0.7725705623626709
Epoch: 6692, Batch Gradient Norm: 29.3962886108388
Epoch: 6692, Batch Gradient Norm after: 22.360676649407864
Epoch 6693/10000, Prediction Accuracy = 59.83%, Loss = 0.7682553410530091
Epoch: 6693, Batch Gradient Norm: 31.233812720252022
Epoch: 6693, Batch Gradient Norm after: 22.360678711106743
Epoch 6694/10000, Prediction Accuracy = 59.806%, Loss = 0.7726002097129822
Epoch: 6694, Batch Gradient Norm: 29.387503020804097
Epoch: 6694, Batch Gradient Norm after: 22.360676712330775
Epoch 6695/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7682657599449157
Epoch: 6695, Batch Gradient Norm: 31.227882084384277
Epoch: 6695, Batch Gradient Norm after: 22.36067836761182
Epoch 6696/10000, Prediction Accuracy = 59.842%, Loss = 0.772532868385315
Epoch: 6696, Batch Gradient Norm: 29.379409025902316
Epoch: 6696, Batch Gradient Norm after: 22.360676477020327
Epoch 6697/10000, Prediction Accuracy = 59.83%, Loss = 0.768103814125061
Epoch: 6697, Batch Gradient Norm: 31.22618050126427
Epoch: 6697, Batch Gradient Norm after: 22.36067847954051
Epoch 6698/10000, Prediction Accuracy = 59.798%, Loss = 0.7723228454589843
Epoch: 6698, Batch Gradient Norm: 29.379581617958802
Epoch: 6698, Batch Gradient Norm after: 22.360678017313074
Epoch 6699/10000, Prediction Accuracy = 59.822%, Loss = 0.7679372906684876
Epoch: 6699, Batch Gradient Norm: 31.223852019849954
Epoch: 6699, Batch Gradient Norm after: 22.360677735522337
Epoch 6700/10000, Prediction Accuracy = 59.786%, Loss = 0.7722109794616699
Epoch: 6700, Batch Gradient Norm: 29.373730457999873
Epoch: 6700, Batch Gradient Norm after: 22.36067590486443
Epoch 6701/10000, Prediction Accuracy = 59.834%, Loss = 0.7678330898284912
Epoch: 6701, Batch Gradient Norm: 31.219372012518416
Epoch: 6701, Batch Gradient Norm after: 22.360678463202813
Epoch 6702/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7721481204032898
Epoch: 6702, Batch Gradient Norm: 29.36871362954263
Epoch: 6702, Batch Gradient Norm after: 22.360677928800094
Epoch 6703/10000, Prediction Accuracy = 59.836%, Loss = 0.7677556991577148
Epoch: 6703, Batch Gradient Norm: 31.22083371209306
Epoch: 6703, Batch Gradient Norm after: 22.36067884187184
Epoch 6704/10000, Prediction Accuracy = 59.786%, Loss = 0.7721327185630799
Epoch: 6704, Batch Gradient Norm: 29.363112882159392
Epoch: 6704, Batch Gradient Norm after: 22.360675962755302
Epoch 6705/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.76780207157135
Epoch: 6705, Batch Gradient Norm: 31.21988831383204
Epoch: 6705, Batch Gradient Norm after: 22.360678119205033
Epoch 6706/10000, Prediction Accuracy = 59.866%, Loss = 0.7721832633018494
Epoch: 6706, Batch Gradient Norm: 29.346392383251608
Epoch: 6706, Batch Gradient Norm after: 22.36067626217421
Epoch 6707/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.7677606821060181
Epoch: 6707, Batch Gradient Norm: 31.216599598685356
Epoch: 6707, Batch Gradient Norm after: 22.3606769578042
Epoch 6708/10000, Prediction Accuracy = 59.814%, Loss = 0.7720491170883179
Epoch: 6708, Batch Gradient Norm: 29.34048261142885
Epoch: 6708, Batch Gradient Norm after: 22.360675539937713
Epoch 6709/10000, Prediction Accuracy = 59.834%, Loss = 0.7675583720207214
Epoch: 6709, Batch Gradient Norm: 31.21546785991383
Epoch: 6709, Batch Gradient Norm after: 22.36067743417764
Epoch 6710/10000, Prediction Accuracy = 59.802%, Loss = 0.7718457818031311
Epoch: 6710, Batch Gradient Norm: 29.3369838350675
Epoch: 6710, Batch Gradient Norm after: 22.360676071692936
Epoch 6711/10000, Prediction Accuracy = 59.85%, Loss = 0.7674066066741944
Epoch: 6711, Batch Gradient Norm: 31.211716916313613
Epoch: 6711, Batch Gradient Norm after: 22.36067712358367
Epoch 6712/10000, Prediction Accuracy = 59.814%, Loss = 0.771772038936615
Epoch: 6712, Batch Gradient Norm: 29.33444954697992
Epoch: 6712, Batch Gradient Norm after: 22.360675519193077
Epoch 6713/10000, Prediction Accuracy = 59.836%, Loss = 0.7673090696334839
Epoch: 6713, Batch Gradient Norm: 31.210123943727186
Epoch: 6713, Batch Gradient Norm after: 22.36067692619923
Epoch 6714/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7717054963111878
Epoch: 6714, Batch Gradient Norm: 29.327720561396116
Epoch: 6714, Batch Gradient Norm after: 22.360678149153536
Epoch 6715/10000, Prediction Accuracy = 59.824%, Loss = 0.7672558069229126
Epoch: 6715, Batch Gradient Norm: 31.214231296383975
Epoch: 6715, Batch Gradient Norm after: 22.36067774775085
Epoch 6716/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7717276334762573
Epoch: 6716, Batch Gradient Norm: 29.320672030761884
Epoch: 6716, Batch Gradient Norm after: 22.36067672780333
Epoch 6717/10000, Prediction Accuracy = 59.864%, Loss = 0.7673138856887818
Epoch: 6717, Batch Gradient Norm: 31.207498969983902
Epoch: 6717, Batch Gradient Norm after: 22.360677281278306
Epoch 6718/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.7717469573020935
Epoch: 6718, Batch Gradient Norm: 29.30998166511457
Epoch: 6718, Batch Gradient Norm after: 22.360677747613476
Epoch 6719/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.7672173738479614
Epoch: 6719, Batch Gradient Norm: 31.202966359014763
Epoch: 6719, Batch Gradient Norm after: 22.36067994376422
Epoch 6720/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7715476870536804
Epoch: 6720, Batch Gradient Norm: 29.30802110179308
Epoch: 6720, Batch Gradient Norm after: 22.36067758565885
Epoch 6721/10000, Prediction Accuracy = 59.822%, Loss = 0.7670237302780152
Epoch: 6721, Batch Gradient Norm: 31.203645691383
Epoch: 6721, Batch Gradient Norm after: 22.360676840692626
Epoch 6722/10000, Prediction Accuracy = 59.802%, Loss = 0.7713806390762329
Epoch: 6722, Batch Gradient Norm: 29.306401866816184
Epoch: 6722, Batch Gradient Norm after: 22.360677134336882
Epoch 6723/10000, Prediction Accuracy = 59.838%, Loss = 0.7669007778167725
Epoch: 6723, Batch Gradient Norm: 31.20006378317711
Epoch: 6723, Batch Gradient Norm after: 22.3606788165285
Epoch 6724/10000, Prediction Accuracy = 59.814%, Loss = 0.7713184356689453
Epoch: 6724, Batch Gradient Norm: 29.3003300357353
Epoch: 6724, Batch Gradient Norm after: 22.36067664954623
Epoch 6725/10000, Prediction Accuracy = 59.862%, Loss = 0.7668099403381348
Epoch: 6725, Batch Gradient Norm: 31.19834174925013
Epoch: 6725, Batch Gradient Norm after: 22.36068033034408
Epoch 6726/10000, Prediction Accuracy = 59.802%, Loss = 0.7712618947029114
Epoch: 6726, Batch Gradient Norm: 29.296428050482326
Epoch: 6726, Batch Gradient Norm after: 22.36067797677336
Epoch 6727/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7667942762374877
Epoch: 6727, Batch Gradient Norm: 31.197942511280132
Epoch: 6727, Batch Gradient Norm after: 22.360678004158768
Epoch 6728/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7712946653366088
Epoch: 6728, Batch Gradient Norm: 29.28908553169807
Epoch: 6728, Batch Gradient Norm after: 22.360678767809453
Epoch 6729/10000, Prediction Accuracy = 59.864%, Loss = 0.7668241739273072
Epoch: 6729, Batch Gradient Norm: 31.190107094636108
Epoch: 6729, Batch Gradient Norm after: 22.360680046838414
Epoch 6730/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7712628602981567
Epoch: 6730, Batch Gradient Norm: 29.281544084175376
Epoch: 6730, Batch Gradient Norm after: 22.360675744673483
Epoch 6731/10000, Prediction Accuracy = 59.852%, Loss = 0.7666958212852478
Epoch: 6731, Batch Gradient Norm: 31.18724569544891
Epoch: 6731, Batch Gradient Norm after: 22.360679903803437
Epoch 6732/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7710434675216675
Epoch: 6732, Batch Gradient Norm: 29.284233936898527
Epoch: 6732, Batch Gradient Norm after: 22.36067861391258
Epoch 6733/10000, Prediction Accuracy = 59.822%, Loss = 0.7665228128433228
Epoch: 6733, Batch Gradient Norm: 31.184361905476383
Epoch: 6733, Batch Gradient Norm after: 22.360676658288607
Epoch 6734/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7709120869636535
Epoch: 6734, Batch Gradient Norm: 29.28393989052845
Epoch: 6734, Batch Gradient Norm after: 22.360676506375032
Epoch 6735/10000, Prediction Accuracy = 59.836%, Loss = 0.7664201617240906
Epoch: 6735, Batch Gradient Norm: 31.177585937880664
Epoch: 6735, Batch Gradient Norm after: 22.36067644257051
Epoch 6736/10000, Prediction Accuracy = 59.83200000000001%, Loss = 0.7708459019660949
Epoch: 6736, Batch Gradient Norm: 29.282944088619026
Epoch: 6736, Batch Gradient Norm after: 22.360678076833477
Epoch 6737/10000, Prediction Accuracy = 59.86%, Loss = 0.7663383245468139
Epoch: 6737, Batch Gradient Norm: 31.175631444291863
Epoch: 6737, Batch Gradient Norm after: 22.360679682977768
Epoch 6738/10000, Prediction Accuracy = 59.788%, Loss = 0.7708054423332215
Epoch: 6738, Batch Gradient Norm: 29.27664505973674
Epoch: 6738, Batch Gradient Norm after: 22.360675675622655
Epoch 6739/10000, Prediction Accuracy = 59.83800000000001%, Loss = 0.7663538694381714
Epoch: 6739, Batch Gradient Norm: 31.17678461679917
Epoch: 6739, Batch Gradient Norm after: 22.36067961716942
Epoch 6740/10000, Prediction Accuracy = 59.855999999999995%, Loss = 0.7708767175674438
Epoch: 6740, Batch Gradient Norm: 29.26898870490211
Epoch: 6740, Batch Gradient Norm after: 22.360678720757
Epoch 6741/10000, Prediction Accuracy = 59.864%, Loss = 0.7663679122924805
Epoch: 6741, Batch Gradient Norm: 31.1666150387904
Epoch: 6741, Batch Gradient Norm after: 22.360679697610845
Epoch 6742/10000, Prediction Accuracy = 59.86199999999999%, Loss = 0.7707704544067383
Epoch: 6742, Batch Gradient Norm: 29.26365540601985
Epoch: 6742, Batch Gradient Norm after: 22.360676216572294
Epoch 6743/10000, Prediction Accuracy = 59.876%, Loss = 0.7661845564842225
Epoch: 6743, Batch Gradient Norm: 31.164055056597114
Epoch: 6743, Batch Gradient Norm after: 22.360680015188024
Epoch 6744/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7705417275428772
Epoch: 6744, Batch Gradient Norm: 29.269853886114937
Epoch: 6744, Batch Gradient Norm after: 22.360676316450462
Epoch 6745/10000, Prediction Accuracy = 59.852%, Loss = 0.7660417199134827
Epoch: 6745, Batch Gradient Norm: 31.158009135134296
Epoch: 6745, Batch Gradient Norm after: 22.360678359026544
Epoch 6746/10000, Prediction Accuracy = 59.80400000000001%, Loss = 0.7704352378845215
Epoch: 6746, Batch Gradient Norm: 29.269796605856946
Epoch: 6746, Batch Gradient Norm after: 22.360678977944183
Epoch 6747/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.7659511923789978
Epoch: 6747, Batch Gradient Norm: 31.14992009419364
Epoch: 6747, Batch Gradient Norm after: 22.360679240055067
Epoch 6748/10000, Prediction Accuracy = 59.822%, Loss = 0.7703652381896973
Epoch: 6748, Batch Gradient Norm: 29.269237142388693
Epoch: 6748, Batch Gradient Norm after: 22.360679003031084
Epoch 6749/10000, Prediction Accuracy = 59.836%, Loss = 0.7658876180648804
Epoch: 6749, Batch Gradient Norm: 31.15123836285298
Epoch: 6749, Batch Gradient Norm after: 22.36067933149367
Epoch 6750/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.7703512072563171
Epoch: 6750, Batch Gradient Norm: 29.268121403171605
Epoch: 6750, Batch Gradient Norm after: 22.360677843981826
Epoch 6751/10000, Prediction Accuracy = 59.884%, Loss = 0.7659453392028809
Epoch: 6751, Batch Gradient Norm: 31.1448196552409
Epoch: 6751, Batch Gradient Norm after: 22.360678209758348
Epoch 6752/10000, Prediction Accuracy = 59.878%, Loss = 0.7703780412673951
Epoch: 6752, Batch Gradient Norm: 29.263203233682095
Epoch: 6752, Batch Gradient Norm after: 22.360677478061117
Epoch 6753/10000, Prediction Accuracy = 59.862%, Loss = 0.765900707244873
Epoch: 6753, Batch Gradient Norm: 31.138807816368523
Epoch: 6753, Batch Gradient Norm after: 22.36067835163611
Epoch 6754/10000, Prediction Accuracy = 59.812%, Loss = 0.7702125787734986
Epoch: 6754, Batch Gradient Norm: 29.261744026447058
Epoch: 6754, Batch Gradient Norm after: 22.360677436575674
Epoch 6755/10000, Prediction Accuracy = 59.843999999999994%, Loss = 0.7657197117805481
Epoch: 6755, Batch Gradient Norm: 31.133556683603533
Epoch: 6755, Batch Gradient Norm after: 22.360677267005723
Epoch 6756/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.7700178384780884
Epoch: 6756, Batch Gradient Norm: 29.2659021257617
Epoch: 6756, Batch Gradient Norm after: 22.360678981710493
Epoch 6757/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.7655976414680481
Epoch: 6757, Batch Gradient Norm: 31.124764743566146
Epoch: 6757, Batch Gradient Norm after: 22.360674855970327
Epoch 6758/10000, Prediction Accuracy = 59.806%, Loss = 0.769929826259613
Epoch: 6758, Batch Gradient Norm: 29.26560256767572
Epoch: 6758, Batch Gradient Norm after: 22.360677667501935
Epoch 6759/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.7655181169509888
Epoch: 6759, Batch Gradient Norm: 31.12103318102316
Epoch: 6759, Batch Gradient Norm after: 22.36068120987632
Epoch 6760/10000, Prediction Accuracy = 59.842000000000006%, Loss = 0.7698612689971924
Epoch: 6760, Batch Gradient Norm: 29.267105916359565
Epoch: 6760, Batch Gradient Norm after: 22.360679056633245
Epoch 6761/10000, Prediction Accuracy = 59.836%, Loss = 0.7654917597770691
Epoch: 6761, Batch Gradient Norm: 31.121137313299094
Epoch: 6761, Batch Gradient Norm after: 22.36067854246816
Epoch 6762/10000, Prediction Accuracy = 59.834%, Loss = 0.7698750972747803
Epoch: 6762, Batch Gradient Norm: 29.26237497480152
Epoch: 6762, Batch Gradient Norm after: 22.360677765989312
Epoch 6763/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.7655480980873108
Epoch: 6763, Batch Gradient Norm: 31.114193865556757
Epoch: 6763, Batch Gradient Norm after: 22.360676662508364
Epoch 6764/10000, Prediction Accuracy = 59.872%, Loss = 0.7698745250701904
Epoch: 6764, Batch Gradient Norm: 29.254828716025123
Epoch: 6764, Batch Gradient Norm after: 22.360677892933346
Epoch 6765/10000, Prediction Accuracy = 59.88399999999999%, Loss = 0.7654423236846923
Epoch: 6765, Batch Gradient Norm: 31.107725286696855
Epoch: 6765, Batch Gradient Norm after: 22.36067855801457
Epoch 6766/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.7696615576744079
Epoch: 6766, Batch Gradient Norm: 29.25723732154964
Epoch: 6766, Batch Gradient Norm after: 22.36067714466914
Epoch 6767/10000, Prediction Accuracy = 59.83200000000001%, Loss = 0.7652651906013489
Epoch: 6767, Batch Gradient Norm: 31.10174450787377
Epoch: 6767, Batch Gradient Norm after: 22.360678574187236
Epoch 6768/10000, Prediction Accuracy = 59.802%, Loss = 0.7695021510124207
Epoch: 6768, Batch Gradient Norm: 29.26468720431126
Epoch: 6768, Batch Gradient Norm after: 22.360678371501873
Epoch 6769/10000, Prediction Accuracy = 59.876%, Loss = 0.7651657462120056
Epoch: 6769, Batch Gradient Norm: 31.092085374949377
Epoch: 6769, Batch Gradient Norm after: 22.36067864235591
Epoch 6770/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7694278359413147
Epoch: 6770, Batch Gradient Norm: 29.26376449411972
Epoch: 6770, Batch Gradient Norm after: 22.360676588684633
Epoch 6771/10000, Prediction Accuracy = 59.884%, Loss = 0.7650826096534729
Epoch: 6771, Batch Gradient Norm: 31.090731328650463
Epoch: 6771, Batch Gradient Norm after: 22.36067937051688
Epoch 6772/10000, Prediction Accuracy = 59.818%, Loss = 0.7693652153015137
Epoch: 6772, Batch Gradient Norm: 29.263691941472473
Epoch: 6772, Batch Gradient Norm after: 22.360677784028166
Epoch 6773/10000, Prediction Accuracy = 59.86999999999999%, Loss = 0.7651033520698547
Epoch: 6773, Batch Gradient Norm: 31.089626725718546
Epoch: 6773, Batch Gradient Norm after: 22.36067657255862
Epoch 6774/10000, Prediction Accuracy = 59.85600000000001%, Loss = 0.769422972202301
Epoch: 6774, Batch Gradient Norm: 29.25568234844966
Epoch: 6774, Batch Gradient Norm after: 22.360678941256563
Epoch 6775/10000, Prediction Accuracy = 59.864%, Loss = 0.765145230293274
Epoch: 6775, Batch Gradient Norm: 31.083464770104296
Epoch: 6775, Batch Gradient Norm after: 22.36067773347115
Epoch 6776/10000, Prediction Accuracy = 59.876%, Loss = 0.76936194896698
Epoch: 6776, Batch Gradient Norm: 29.249765169644103
Epoch: 6776, Batch Gradient Norm after: 22.3606759247312
Epoch 6777/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.7649762868881226
Epoch: 6777, Batch Gradient Norm: 31.07665151121629
Epoch: 6777, Batch Gradient Norm after: 22.360678064510434
Epoch 6778/10000, Prediction Accuracy = 59.818%, Loss = 0.7691230773925781
Epoch: 6778, Batch Gradient Norm: 29.25117748978641
Epoch: 6778, Batch Gradient Norm after: 22.360678417837104
Epoch 6779/10000, Prediction Accuracy = 59.870000000000005%, Loss = 0.7648167014122009
Epoch: 6779, Batch Gradient Norm: 31.069409040237005
Epoch: 6779, Batch Gradient Norm after: 22.360678387991296
Epoch 6780/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7690072178840637
Epoch: 6780, Batch Gradient Norm: 29.256864812488665
Epoch: 6780, Batch Gradient Norm after: 22.36067819450621
Epoch 6781/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.7647289514541626
Epoch: 6781, Batch Gradient Norm: 31.06030205416952
Epoch: 6781, Batch Gradient Norm after: 22.360677803869645
Epoch 6782/10000, Prediction Accuracy = 59.818%, Loss = 0.7689316391944885
Epoch: 6782, Batch Gradient Norm: 29.25717831692304
Epoch: 6782, Batch Gradient Norm after: 22.360678364113927
Epoch 6783/10000, Prediction Accuracy = 59.872%, Loss = 0.7646628022193909
Epoch: 6783, Batch Gradient Norm: 31.06139912151185
Epoch: 6783, Batch Gradient Norm after: 22.36067759484882
Epoch 6784/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7688947319984436
Epoch: 6784, Batch Gradient Norm: 29.258294520292782
Epoch: 6784, Batch Gradient Norm after: 22.360678943865363
Epoch 6785/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7647032260894775
Epoch: 6785, Batch Gradient Norm: 31.055080797665184
Epoch: 6785, Batch Gradient Norm after: 22.360679743438567
Epoch 6786/10000, Prediction Accuracy = 59.864%, Loss = 0.7689311981201172
Epoch: 6786, Batch Gradient Norm: 29.255034605164774
Epoch: 6786, Batch Gradient Norm after: 22.360677948710936
Epoch 6787/10000, Prediction Accuracy = 59.886%, Loss = 0.7647088527679443
Epoch: 6787, Batch Gradient Norm: 31.044944035398395
Epoch: 6787, Batch Gradient Norm after: 22.36067919089034
Epoch 6788/10000, Prediction Accuracy = 59.854%, Loss = 0.7688069105148315
Epoch: 6788, Batch Gradient Norm: 29.254771158655558
Epoch: 6788, Batch Gradient Norm after: 22.360678172321048
Epoch 6789/10000, Prediction Accuracy = 59.884%, Loss = 0.7645398378372192
Epoch: 6789, Batch Gradient Norm: 31.03786695791733
Epoch: 6789, Batch Gradient Norm after: 22.360677865657678
Epoch 6790/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.7685786485671997
Epoch: 6790, Batch Gradient Norm: 29.25941770350258
Epoch: 6790, Batch Gradient Norm after: 22.360677822185348
Epoch 6791/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7644041419029236
Epoch: 6791, Batch Gradient Norm: 31.030588675410172
Epoch: 6791, Batch Gradient Norm after: 22.36067843628443
Epoch 6792/10000, Prediction Accuracy = 59.814%, Loss = 0.7684864163398742
Epoch: 6792, Batch Gradient Norm: 29.260301573804448
Epoch: 6792, Batch Gradient Norm after: 22.360678110824917
Epoch 6793/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.7643247365951538
Epoch: 6793, Batch Gradient Norm: 31.024429842822993
Epoch: 6793, Batch Gradient Norm after: 22.360678154752033
Epoch 6794/10000, Prediction Accuracy = 59.836%, Loss = 0.768407118320465
Epoch: 6794, Batch Gradient Norm: 29.261465042528336
Epoch: 6794, Batch Gradient Norm after: 22.360677664664323
Epoch 6795/10000, Prediction Accuracy = 59.896%, Loss = 0.7642720103263855
Epoch: 6795, Batch Gradient Norm: 31.026314043640728
Epoch: 6795, Batch Gradient Norm after: 22.36067632111101
Epoch 6796/10000, Prediction Accuracy = 59.83%, Loss = 0.7683992743492126
Epoch: 6796, Batch Gradient Norm: 29.25743916208438
Epoch: 6796, Batch Gradient Norm after: 22.36067522258115
Epoch 6797/10000, Prediction Accuracy = 59.882000000000005%, Loss = 0.7643393039703369
Epoch: 6797, Batch Gradient Norm: 31.02245290599705
Epoch: 6797, Batch Gradient Norm after: 22.360675497032883
Epoch 6798/10000, Prediction Accuracy = 59.872%, Loss = 0.7684449791908264
Epoch: 6798, Batch Gradient Norm: 29.246853218739158
Epoch: 6798, Batch Gradient Norm after: 22.360677179877644
Epoch 6799/10000, Prediction Accuracy = 59.874%, Loss = 0.7642818927764893
Epoch: 6799, Batch Gradient Norm: 31.015587351224593
Epoch: 6799, Batch Gradient Norm after: 22.360678081516742
Epoch 6800/10000, Prediction Accuracy = 59.842%, Loss = 0.7682666540145874
Epoch: 6800, Batch Gradient Norm: 29.246116246765297
Epoch: 6800, Batch Gradient Norm after: 22.36067869457959
Epoch 6801/10000, Prediction Accuracy = 59.842%, Loss = 0.7640796422958374
Epoch: 6801, Batch Gradient Norm: 31.011920100494997
Epoch: 6801, Batch Gradient Norm after: 22.360679840596998
Epoch 6802/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.7680762529373169
Epoch: 6802, Batch Gradient Norm: 29.24710304048467
Epoch: 6802, Batch Gradient Norm after: 22.36067886716635
Epoch 6803/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.7639472246170044
Epoch: 6803, Batch Gradient Norm: 31.006713064641946
Epoch: 6803, Batch Gradient Norm after: 22.360678218670483
Epoch 6804/10000, Prediction Accuracy = 59.836%, Loss = 0.7680078983306885
Epoch: 6804, Batch Gradient Norm: 29.242156756285965
Epoch: 6804, Batch Gradient Norm after: 22.36067789304154
Epoch 6805/10000, Prediction Accuracy = 59.9%, Loss = 0.7638553023338318
Epoch: 6805, Batch Gradient Norm: 31.006355167445363
Epoch: 6805, Batch Gradient Norm after: 22.360680365266035
Epoch 6806/10000, Prediction Accuracy = 59.838%, Loss = 0.7679423570632935
Epoch: 6806, Batch Gradient Norm: 29.2351239558318
Epoch: 6806, Batch Gradient Norm after: 22.36067849269961
Epoch 6807/10000, Prediction Accuracy = 59.854%, Loss = 0.7638291597366333
Epoch: 6807, Batch Gradient Norm: 31.009184262545055
Epoch: 6807, Batch Gradient Norm after: 22.360675966019226
Epoch 6808/10000, Prediction Accuracy = 59.85%, Loss = 0.7679728865623474
Epoch: 6808, Batch Gradient Norm: 29.22812082321051
Epoch: 6808, Batch Gradient Norm after: 22.36067561721037
Epoch 6809/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7638752102851868
Epoch: 6809, Batch Gradient Norm: 31.001959275204026
Epoch: 6809, Batch Gradient Norm after: 22.360676038995578
Epoch 6810/10000, Prediction Accuracy = 59.884%, Loss = 0.7679678082466126
Epoch: 6810, Batch Gradient Norm: 29.221125289354354
Epoch: 6810, Batch Gradient Norm after: 22.360676367250626
Epoch 6811/10000, Prediction Accuracy = 59.9%, Loss = 0.7637619972229004
Epoch: 6811, Batch Gradient Norm: 30.9992007448427
Epoch: 6811, Batch Gradient Norm after: 22.360678589046486
Epoch 6812/10000, Prediction Accuracy = 59.858000000000004%, Loss = 0.7677538156509399
Epoch: 6812, Batch Gradient Norm: 29.21680353683125
Epoch: 6812, Batch Gradient Norm after: 22.36067586085414
Epoch 6813/10000, Prediction Accuracy = 59.862%, Loss = 0.7635664343833923
Epoch: 6813, Batch Gradient Norm: 30.999307317409937
Epoch: 6813, Batch Gradient Norm after: 22.360679266768877
Epoch 6814/10000, Prediction Accuracy = 59.85600000000001%, Loss = 0.7676242470741272
Epoch: 6814, Batch Gradient Norm: 29.21326128954079
Epoch: 6814, Batch Gradient Norm after: 22.360677418080087
Epoch 6815/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7634531736373902
Epoch: 6815, Batch Gradient Norm: 30.99577659988902
Epoch: 6815, Batch Gradient Norm after: 22.360678421794677
Epoch 6816/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7675607800483704
Epoch: 6816, Batch Gradient Norm: 29.209011869385765
Epoch: 6816, Batch Gradient Norm after: 22.36067696539234
Epoch 6817/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.763366448879242
Epoch: 6817, Batch Gradient Norm: 30.994135717834038
Epoch: 6817, Batch Gradient Norm after: 22.360678339097046
Epoch 6818/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7675130605697632
Epoch: 6818, Batch Gradient Norm: 29.205605266835367
Epoch: 6818, Batch Gradient Norm after: 22.36067752535332
Epoch 6819/10000, Prediction Accuracy = 59.870000000000005%, Loss = 0.763373327255249
Epoch: 6819, Batch Gradient Norm: 30.992644105278913
Epoch: 6819, Batch Gradient Norm after: 22.360678649309794
Epoch 6820/10000, Prediction Accuracy = 59.878%, Loss = 0.7675511837005615
Epoch: 6820, Batch Gradient Norm: 29.198253789813396
Epoch: 6820, Batch Gradient Norm after: 22.360676921204693
Epoch 6821/10000, Prediction Accuracy = 59.878%, Loss = 0.7634092569351196
Epoch: 6821, Batch Gradient Norm: 30.983319159441756
Epoch: 6821, Batch Gradient Norm after: 22.3606787219178
Epoch 6822/10000, Prediction Accuracy = 59.884%, Loss = 0.7674943089485169
Epoch: 6822, Batch Gradient Norm: 29.194636105337246
Epoch: 6822, Batch Gradient Norm after: 22.360676341533324
Epoch 6823/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7632593750953675
Epoch: 6823, Batch Gradient Norm: 30.97828939212462
Epoch: 6823, Batch Gradient Norm after: 22.36067975363667
Epoch 6824/10000, Prediction Accuracy = 59.85%, Loss = 0.7672664761543274
Epoch: 6824, Batch Gradient Norm: 29.197486174124165
Epoch: 6824, Batch Gradient Norm after: 22.360680782518727
Epoch 6825/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7630884170532226
Epoch: 6825, Batch Gradient Norm: 30.975910128366653
Epoch: 6825, Batch Gradient Norm after: 22.36067916991792
Epoch 6826/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.7671548128128052
Epoch: 6826, Batch Gradient Norm: 29.19829867997068
Epoch: 6826, Batch Gradient Norm after: 22.36067642929316
Epoch 6827/10000, Prediction Accuracy = 59.9%, Loss = 0.7630001902580261
Epoch: 6827, Batch Gradient Norm: 30.969970321223503
Epoch: 6827, Batch Gradient Norm after: 22.36067766234205
Epoch 6828/10000, Prediction Accuracy = 59.855999999999995%, Loss = 0.7670854926109314
Epoch: 6828, Batch Gradient Norm: 29.194229853826826
Epoch: 6828, Batch Gradient Norm after: 22.360678106473063
Epoch 6829/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7629235029220581
Epoch: 6829, Batch Gradient Norm: 30.96972840228999
Epoch: 6829, Batch Gradient Norm after: 22.36067882916996
Epoch 6830/10000, Prediction Accuracy = 59.836%, Loss = 0.7670542597770691
Epoch: 6830, Batch Gradient Norm: 29.191618810511596
Epoch: 6830, Batch Gradient Norm after: 22.360677937251467
Epoch 6831/10000, Prediction Accuracy = 59.906000000000006%, Loss = 0.7629601001739502
Epoch: 6831, Batch Gradient Norm: 30.96635239112187
Epoch: 6831, Batch Gradient Norm after: 22.36067793939539
Epoch 6832/10000, Prediction Accuracy = 59.888%, Loss = 0.7671232104301453
Epoch: 6832, Batch Gradient Norm: 29.185393734408745
Epoch: 6832, Batch Gradient Norm after: 22.36067836681543
Epoch 6833/10000, Prediction Accuracy = 59.898%, Loss = 0.762963080406189
Epoch: 6833, Batch Gradient Norm: 30.95661530059219
Epoch: 6833, Batch Gradient Norm after: 22.36067690669316
Epoch 6834/10000, Prediction Accuracy = 59.888%, Loss = 0.7669795036315918
Epoch: 6834, Batch Gradient Norm: 29.187016236744693
Epoch: 6834, Batch Gradient Norm after: 22.36067615160099
Epoch 6835/10000, Prediction Accuracy = 59.884%, Loss = 0.7627844214439392
Epoch: 6835, Batch Gradient Norm: 30.95499367170132
Epoch: 6835, Batch Gradient Norm after: 22.3606773449037
Epoch 6836/10000, Prediction Accuracy = 59.854%, Loss = 0.7667539477348327
Epoch: 6836, Batch Gradient Norm: 29.193212398870326
Epoch: 6836, Batch Gradient Norm after: 22.360675953156594
Epoch 6837/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.762641429901123
Epoch: 6837, Batch Gradient Norm: 30.950232725524813
Epoch: 6837, Batch Gradient Norm after: 22.360677949063586
Epoch 6838/10000, Prediction Accuracy = 59.862%, Loss = 0.7666637778282166
Epoch: 6838, Batch Gradient Norm: 29.1931403746697
Epoch: 6838, Batch Gradient Norm after: 22.360675512493643
Epoch 6839/10000, Prediction Accuracy = 59.902%, Loss = 0.7625606775283813
Epoch: 6839, Batch Gradient Norm: 30.94366830427313
Epoch: 6839, Batch Gradient Norm after: 22.360678672934952
Epoch 6840/10000, Prediction Accuracy = 59.874%, Loss = 0.7665986657142639
Epoch: 6840, Batch Gradient Norm: 29.192807298187816
Epoch: 6840, Batch Gradient Norm after: 22.360678800844866
Epoch 6841/10000, Prediction Accuracy = 59.888%, Loss = 0.7625064849853516
Epoch: 6841, Batch Gradient Norm: 30.943009636977816
Epoch: 6841, Batch Gradient Norm after: 22.360678168396724
Epoch 6842/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.7665978193283081
Epoch: 6842, Batch Gradient Norm: 29.187357375346256
Epoch: 6842, Batch Gradient Norm after: 22.360675580591028
Epoch 6843/10000, Prediction Accuracy = 59.908%, Loss = 0.7625643491744996
Epoch: 6843, Batch Gradient Norm: 30.939205279146627
Epoch: 6843, Batch Gradient Norm after: 22.360678394642612
Epoch 6844/10000, Prediction Accuracy = 59.902%, Loss = 0.7666401863098145
Epoch: 6844, Batch Gradient Norm: 29.178990269273434
Epoch: 6844, Batch Gradient Norm after: 22.36067483294179
Epoch 6845/10000, Prediction Accuracy = 59.902%, Loss = 0.7624997615814209
Epoch: 6845, Batch Gradient Norm: 30.934017149816647
Epoch: 6845, Batch Gradient Norm after: 22.36068133888281
Epoch 6846/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.7664550304412842
Epoch: 6846, Batch Gradient Norm: 29.17601372189054
Epoch: 6846, Batch Gradient Norm after: 22.360676122022404
Epoch 6847/10000, Prediction Accuracy = 59.862%, Loss = 0.7623024225234986
Epoch: 6847, Batch Gradient Norm: 30.932937976001753
Epoch: 6847, Batch Gradient Norm after: 22.36067929668922
Epoch 6848/10000, Prediction Accuracy = 59.86%, Loss = 0.7662779927253723
Epoch: 6848, Batch Gradient Norm: 29.176051176205846
Epoch: 6848, Batch Gradient Norm after: 22.360677765622118
Epoch 6849/10000, Prediction Accuracy = 59.928%, Loss = 0.7621816396713257
Epoch: 6849, Batch Gradient Norm: 30.930445026511176
Epoch: 6849, Batch Gradient Norm after: 22.36067784452338
Epoch 6850/10000, Prediction Accuracy = 59.872%, Loss = 0.7662093281745911
Epoch: 6850, Batch Gradient Norm: 29.17484633977479
Epoch: 6850, Batch Gradient Norm after: 22.360676958410373
Epoch 6851/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7620971202850342
Epoch: 6851, Batch Gradient Norm: 30.925243645260988
Epoch: 6851, Batch Gradient Norm after: 22.360679562594438
Epoch 6852/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.766148054599762
Epoch: 6852, Batch Gradient Norm: 29.17461535864301
Epoch: 6852, Batch Gradient Norm after: 22.36067648027521
Epoch 6853/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.7620703458786011
Epoch: 6853, Batch Gradient Norm: 30.9231131440592
Epoch: 6853, Batch Gradient Norm after: 22.36067805618803
Epoch 6854/10000, Prediction Accuracy = 59.888%, Loss = 0.7661680579185486
Epoch: 6854, Batch Gradient Norm: 29.169571295922616
Epoch: 6854, Batch Gradient Norm after: 22.360677473247975
Epoch 6855/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.762132728099823
Epoch: 6855, Batch Gradient Norm: 30.916275721857005
Epoch: 6855, Batch Gradient Norm after: 22.36067725671201
Epoch 6856/10000, Prediction Accuracy = 59.895999999999994%, Loss = 0.7661730170249939
Epoch: 6856, Batch Gradient Norm: 29.1606274884554
Epoch: 6856, Batch Gradient Norm after: 22.36067672791313
Epoch 6857/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.7620266199111938
Epoch: 6857, Batch Gradient Norm: 30.91060589300876
Epoch: 6857, Batch Gradient Norm after: 22.36067664683243
Epoch 6858/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7659533500671387
Epoch: 6858, Batch Gradient Norm: 29.163521993117197
Epoch: 6858, Batch Gradient Norm after: 22.360676251113247
Epoch 6859/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7618414521217346
Epoch: 6859, Batch Gradient Norm: 30.906385636493354
Epoch: 6859, Batch Gradient Norm after: 22.360678654238068
Epoch 6860/10000, Prediction Accuracy = 59.886%, Loss = 0.7658020257949829
Epoch: 6860, Batch Gradient Norm: 29.164145400392908
Epoch: 6860, Batch Gradient Norm after: 22.360678939743945
Epoch 6861/10000, Prediction Accuracy = 59.912%, Loss = 0.7617433071136475
Epoch: 6861, Batch Gradient Norm: 30.900486848129017
Epoch: 6861, Batch Gradient Norm after: 22.360679119641155
Epoch 6862/10000, Prediction Accuracy = 59.878%, Loss = 0.7657307147979736
Epoch: 6862, Batch Gradient Norm: 29.162806139323102
Epoch: 6862, Batch Gradient Norm after: 22.360678728182638
Epoch 6863/10000, Prediction Accuracy = 59.928%, Loss = 0.7616583228111267
Epoch: 6863, Batch Gradient Norm: 30.89856770336542
Epoch: 6863, Batch Gradient Norm after: 22.36067818700196
Epoch 6864/10000, Prediction Accuracy = 59.872%, Loss = 0.7656798958778381
Epoch: 6864, Batch Gradient Norm: 29.15999639105973
Epoch: 6864, Batch Gradient Norm after: 22.36067727962105
Epoch 6865/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7616535902023316
Epoch: 6865, Batch Gradient Norm: 30.898345210514762
Epoch: 6865, Batch Gradient Norm after: 22.36067637588313
Epoch 6866/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.7657224535942078
Epoch: 6866, Batch Gradient Norm: 29.154312882878163
Epoch: 6866, Batch Gradient Norm after: 22.36067777114148
Epoch 6867/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7617156744003296
Epoch: 6867, Batch Gradient Norm: 30.891514948195628
Epoch: 6867, Batch Gradient Norm after: 22.36067716714922
Epoch 6868/10000, Prediction Accuracy = 59.895999999999994%, Loss = 0.7656960248947143
Epoch: 6868, Batch Gradient Norm: 29.14532281082789
Epoch: 6868, Batch Gradient Norm after: 22.360676348797412
Epoch 6869/10000, Prediction Accuracy = 59.896%, Loss = 0.7615652322769165
Epoch: 6869, Batch Gradient Norm: 30.8897486038363
Epoch: 6869, Batch Gradient Norm after: 22.360678805989735
Epoch 6870/10000, Prediction Accuracy = 59.88399999999999%, Loss = 0.7654543519020081
Epoch: 6870, Batch Gradient Norm: 29.146214999072104
Epoch: 6870, Batch Gradient Norm after: 22.360676120217153
Epoch 6871/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7613819003105163
Epoch: 6871, Batch Gradient Norm: 30.885984978072074
Epoch: 6871, Batch Gradient Norm after: 22.36067930036906
Epoch 6872/10000, Prediction Accuracy = 59.882000000000005%, Loss = 0.7653299689292907
Epoch: 6872, Batch Gradient Norm: 29.150276145706567
Epoch: 6872, Batch Gradient Norm after: 22.360678292969094
Epoch 6873/10000, Prediction Accuracy = 59.926%, Loss = 0.7612970113754273
Epoch: 6873, Batch Gradient Norm: 30.87742828622722
Epoch: 6873, Batch Gradient Norm after: 22.36067842806296
Epoch 6874/10000, Prediction Accuracy = 59.876%, Loss = 0.7652642488479614
Epoch: 6874, Batch Gradient Norm: 29.147753340783808
Epoch: 6874, Batch Gradient Norm after: 22.36067774985209
Epoch 6875/10000, Prediction Accuracy = 59.928%, Loss = 0.7612169981002808
Epoch: 6875, Batch Gradient Norm: 30.87862490330247
Epoch: 6875, Batch Gradient Norm after: 22.36067984632218
Epoch 6876/10000, Prediction Accuracy = 59.866%, Loss = 0.7652383685112
Epoch: 6876, Batch Gradient Norm: 29.14420864733219
Epoch: 6876, Batch Gradient Norm after: 22.36067681343242
Epoch 6877/10000, Prediction Accuracy = 59.888%, Loss = 0.7612419843673706
Epoch: 6877, Batch Gradient Norm: 30.879469810823394
Epoch: 6877, Batch Gradient Norm after: 22.360678790284005
Epoch 6878/10000, Prediction Accuracy = 59.924%, Loss = 0.7652920007705688
Epoch: 6878, Batch Gradient Norm: 29.13570543482817
Epoch: 6878, Batch Gradient Norm after: 22.360678882292163
Epoch 6879/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.761253547668457
Epoch: 6879, Batch Gradient Norm: 30.87197917929813
Epoch: 6879, Batch Gradient Norm after: 22.360674612828245
Epoch 6880/10000, Prediction Accuracy = 59.924%, Loss = 0.7651965022087097
Epoch: 6880, Batch Gradient Norm: 29.13053108930084
Epoch: 6880, Batch Gradient Norm after: 22.360677988296395
Epoch 6881/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7610725164413452
Epoch: 6881, Batch Gradient Norm: 30.87010570250804
Epoch: 6881, Batch Gradient Norm after: 22.36067807111519
Epoch 6882/10000, Prediction Accuracy = 59.864%, Loss = 0.7649656057357788
Epoch: 6882, Batch Gradient Norm: 29.132885344879394
Epoch: 6882, Batch Gradient Norm after: 22.36067968529151
Epoch 6883/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7609252572059632
Epoch: 6883, Batch Gradient Norm: 30.864103384751186
Epoch: 6883, Batch Gradient Norm after: 22.360677533102137
Epoch 6884/10000, Prediction Accuracy = 59.888%, Loss = 0.7648761153221131
Epoch: 6884, Batch Gradient Norm: 29.133345941882553
Epoch: 6884, Batch Gradient Norm after: 22.36067774936392
Epoch 6885/10000, Prediction Accuracy = 59.932%, Loss = 0.7608395338058471
Epoch: 6885, Batch Gradient Norm: 30.856659700388995
Epoch: 6885, Batch Gradient Norm after: 22.36067852301173
Epoch 6886/10000, Prediction Accuracy = 59.884%, Loss = 0.7648118376731873
Epoch: 6886, Batch Gradient Norm: 29.131027287629372
Epoch: 6886, Batch Gradient Norm after: 22.360675597868045
Epoch 6887/10000, Prediction Accuracy = 59.928%, Loss = 0.760773777961731
Epoch: 6887, Batch Gradient Norm: 30.859123924200947
Epoch: 6887, Batch Gradient Norm after: 22.360677462290944
Epoch 6888/10000, Prediction Accuracy = 59.858000000000004%, Loss = 0.7648106575012207
Epoch: 6888, Batch Gradient Norm: 29.12961463546702
Epoch: 6888, Batch Gradient Norm after: 22.360676298576404
Epoch 6889/10000, Prediction Accuracy = 59.914%, Loss = 0.7608304977416992
Epoch: 6889, Batch Gradient Norm: 30.856396668598798
Epoch: 6889, Batch Gradient Norm after: 22.360678652473304
Epoch 6890/10000, Prediction Accuracy = 59.928%, Loss = 0.7648517608642578
Epoch: 6890, Batch Gradient Norm: 29.12002697938247
Epoch: 6890, Batch Gradient Norm after: 22.360678649874014
Epoch 6891/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.7608003854751587
Epoch: 6891, Batch Gradient Norm: 30.850237818864883
Epoch: 6891, Batch Gradient Norm after: 22.360680860432435
Epoch 6892/10000, Prediction Accuracy = 59.93399999999999%, Loss = 0.7646991610527039
Epoch: 6892, Batch Gradient Norm: 29.117632886852356
Epoch: 6892, Batch Gradient Norm after: 22.36067705813171
Epoch 6893/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7605939745903015
Epoch: 6893, Batch Gradient Norm: 30.84799577337136
Epoch: 6893, Batch Gradient Norm after: 22.360678368813623
Epoch 6894/10000, Prediction Accuracy = 59.89%, Loss = 0.7644925832748413
Epoch: 6894, Batch Gradient Norm: 29.119543779055224
Epoch: 6894, Batch Gradient Norm after: 22.360677409671272
Epoch 6895/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7604715943336486
Epoch: 6895, Batch Gradient Norm: 30.84126786195294
Epoch: 6895, Batch Gradient Norm after: 22.36067669384236
Epoch 6896/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.764421546459198
Epoch: 6896, Batch Gradient Norm: 29.119078356817347
Epoch: 6896, Batch Gradient Norm after: 22.360678273901126
Epoch 6897/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7603803992271423
Epoch: 6897, Batch Gradient Norm: 30.8374766670442
Epoch: 6897, Batch Gradient Norm after: 22.36067997685345
Epoch 6898/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.7643582344055175
Epoch: 6898, Batch Gradient Norm: 29.11681590288627
Epoch: 6898, Batch Gradient Norm after: 22.360678159324905
Epoch 6899/10000, Prediction Accuracy = 59.938%, Loss = 0.7603479743003845
Epoch: 6899, Batch Gradient Norm: 30.83550869234233
Epoch: 6899, Batch Gradient Norm after: 22.360676881615742
Epoch 6900/10000, Prediction Accuracy = 59.924%, Loss = 0.764389944076538
Epoch: 6900, Batch Gradient Norm: 29.113657737743047
Epoch: 6900, Batch Gradient Norm after: 22.360675575766088
Epoch 6901/10000, Prediction Accuracy = 59.914%, Loss = 0.7604318261146545
Epoch: 6901, Batch Gradient Norm: 30.827463817351134
Epoch: 6901, Batch Gradient Norm after: 22.360679033578315
Epoch 6902/10000, Prediction Accuracy = 59.902%, Loss = 0.7644095182418823
Epoch: 6902, Batch Gradient Norm: 29.107077281225095
Epoch: 6902, Batch Gradient Norm after: 22.360675343978404
Epoch 6903/10000, Prediction Accuracy = 59.896%, Loss = 0.7603286981582642
Epoch: 6903, Batch Gradient Norm: 30.82160087842149
Epoch: 6903, Batch Gradient Norm after: 22.3606770374538
Epoch 6904/10000, Prediction Accuracy = 59.898%, Loss = 0.7641829252243042
Epoch: 6904, Batch Gradient Norm: 29.106305708011845
Epoch: 6904, Batch Gradient Norm after: 22.360676492811074
Epoch 6905/10000, Prediction Accuracy = 59.886%, Loss = 0.7601265072822571
Epoch: 6905, Batch Gradient Norm: 30.820100565323663
Epoch: 6905, Batch Gradient Norm after: 22.360677994081662
Epoch 6906/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7640289068222046
Epoch: 6906, Batch Gradient Norm: 29.10299839318758
Epoch: 6906, Batch Gradient Norm after: 22.36067774749615
Epoch 6907/10000, Prediction Accuracy = 59.895999999999994%, Loss = 0.7600173115730285
Epoch: 6907, Batch Gradient Norm: 30.816259483439985
Epoch: 6907, Batch Gradient Norm after: 22.360678230210418
Epoch 6908/10000, Prediction Accuracy = 59.902%, Loss = 0.7639749765396118
Epoch: 6908, Batch Gradient Norm: 29.09587003086949
Epoch: 6908, Batch Gradient Norm after: 22.360678013471194
Epoch 6909/10000, Prediction Accuracy = 59.914%, Loss = 0.7599241495132446
Epoch: 6909, Batch Gradient Norm: 30.818996698055816
Epoch: 6909, Batch Gradient Norm after: 22.360677831615444
Epoch 6910/10000, Prediction Accuracy = 59.896%, Loss = 0.7639316082000732
Epoch: 6910, Batch Gradient Norm: 29.088777617945514
Epoch: 6910, Batch Gradient Norm after: 22.360676173710935
Epoch 6911/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7599002599716187
Epoch: 6911, Batch Gradient Norm: 30.82074327650485
Epoch: 6911, Batch Gradient Norm after: 22.36068040553181
Epoch 6912/10000, Prediction Accuracy = 59.928%, Loss = 0.7639958024024963
Epoch: 6912, Batch Gradient Norm: 29.079641175588172
Epoch: 6912, Batch Gradient Norm after: 22.360676784336526
Epoch 6913/10000, Prediction Accuracy = 59.898%, Loss = 0.7599521279335022
Epoch: 6913, Batch Gradient Norm: 30.813904839823113
Epoch: 6913, Batch Gradient Norm after: 22.36067589795846
Epoch 6914/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7639375329017639
Epoch: 6914, Batch Gradient Norm: 29.076533104904673
Epoch: 6914, Batch Gradient Norm after: 22.360676113417064
Epoch 6915/10000, Prediction Accuracy = 59.914%, Loss = 0.7598031282424926
Epoch: 6915, Batch Gradient Norm: 30.811148546279725
Epoch: 6915, Batch Gradient Norm after: 22.360679296447977
Epoch 6916/10000, Prediction Accuracy = 59.902%, Loss = 0.763713276386261
Epoch: 6916, Batch Gradient Norm: 29.07239178374155
Epoch: 6916, Batch Gradient Norm after: 22.360677734932633
Epoch 6917/10000, Prediction Accuracy = 59.92%, Loss = 0.7596228957176209
Epoch: 6917, Batch Gradient Norm: 30.81097540398638
Epoch: 6917, Batch Gradient Norm after: 22.360678833143858
Epoch 6918/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7636012434959412
Epoch: 6918, Batch Gradient Norm: 29.074175639805155
Epoch: 6918, Batch Gradient Norm after: 22.360681296646565
Epoch 6919/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7595397353172302
Epoch: 6919, Batch Gradient Norm: 30.803087474610052
Epoch: 6919, Batch Gradient Norm after: 22.360677847229695
Epoch 6920/10000, Prediction Accuracy = 59.902%, Loss = 0.7635338187217713
Epoch: 6920, Batch Gradient Norm: 29.071249826243395
Epoch: 6920, Batch Gradient Norm after: 22.360677979537535
Epoch 6921/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.759462833404541
Epoch: 6921, Batch Gradient Norm: 30.8019163344772
Epoch: 6921, Batch Gradient Norm after: 22.3606772969108
Epoch 6922/10000, Prediction Accuracy = 59.902%, Loss = 0.7635083198547363
Epoch: 6922, Batch Gradient Norm: 29.066042218538087
Epoch: 6922, Batch Gradient Norm after: 22.360676314294185
Epoch 6923/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7595008134841919
Epoch: 6923, Batch Gradient Norm: 30.80010895375211
Epoch: 6923, Batch Gradient Norm after: 22.36067921095833
Epoch 6924/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7635841250419617
Epoch: 6924, Batch Gradient Norm: 29.05868159585154
Epoch: 6924, Batch Gradient Norm after: 22.360677007108034
Epoch 6925/10000, Prediction Accuracy = 59.924%, Loss = 0.7595246076583863
Epoch: 6925, Batch Gradient Norm: 30.792872106431602
Epoch: 6925, Batch Gradient Norm after: 22.36067773057026
Epoch 6926/10000, Prediction Accuracy = 59.93399999999999%, Loss = 0.7634623169898986
Epoch: 6926, Batch Gradient Norm: 29.053208920725602
Epoch: 6926, Batch Gradient Norm after: 22.360676939149844
Epoch 6927/10000, Prediction Accuracy = 59.926%, Loss = 0.7593205451965332
Epoch: 6927, Batch Gradient Norm: 30.792194208288294
Epoch: 6927, Batch Gradient Norm after: 22.36067590175076
Epoch 6928/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7632355093955994
Epoch: 6928, Batch Gradient Norm: 29.05095691808414
Epoch: 6928, Batch Gradient Norm after: 22.36067969511544
Epoch 6929/10000, Prediction Accuracy = 59.93399999999999%, Loss = 0.7591709613800048
Epoch: 6929, Batch Gradient Norm: 30.789608426791496
Epoch: 6929, Batch Gradient Norm after: 22.360677574915307
Epoch 6930/10000, Prediction Accuracy = 59.9%, Loss = 0.7631534576416016
Epoch: 6930, Batch Gradient Norm: 29.049774527998352
Epoch: 6930, Batch Gradient Norm after: 22.360678961566407
Epoch 6931/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7590820789337158
Epoch: 6931, Batch Gradient Norm: 30.78497664475735
Epoch: 6931, Batch Gradient Norm after: 22.360678688128207
Epoch 6932/10000, Prediction Accuracy = 59.902%, Loss = 0.763085412979126
Epoch: 6932, Batch Gradient Norm: 29.046026762078167
Epoch: 6932, Batch Gradient Norm after: 22.36067865459583
Epoch 6933/10000, Prediction Accuracy = 59.94599999999999%, Loss = 0.7590242266654968
Epoch: 6933, Batch Gradient Norm: 30.786738844243587
Epoch: 6933, Batch Gradient Norm after: 22.36067796655936
Epoch 6934/10000, Prediction Accuracy = 59.898%, Loss = 0.7630977272987366
Epoch: 6934, Batch Gradient Norm: 29.04194440235602
Epoch: 6934, Batch Gradient Norm after: 22.360677052451287
Epoch 6935/10000, Prediction Accuracy = 59.92800000000001%, Loss = 0.7590771198272706
Epoch: 6935, Batch Gradient Norm: 30.783546013220974
Epoch: 6935, Batch Gradient Norm after: 22.360678144016344
Epoch 6936/10000, Prediction Accuracy = 59.926%, Loss = 0.7631380438804627
Epoch: 6936, Batch Gradient Norm: 29.034663847137953
Epoch: 6936, Batch Gradient Norm after: 22.360677100476877
Epoch 6937/10000, Prediction Accuracy = 59.926%, Loss = 0.7590362787246704
Epoch: 6937, Batch Gradient Norm: 30.775680788597132
Epoch: 6937, Batch Gradient Norm after: 22.360680843760417
Epoch 6938/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7629613637924194
Epoch: 6938, Batch Gradient Norm: 29.03060076379094
Epoch: 6938, Batch Gradient Norm after: 22.360678522804267
Epoch 6939/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7588303685188293
Epoch: 6939, Batch Gradient Norm: 30.776627279961836
Epoch: 6939, Batch Gradient Norm after: 22.360677724975112
Epoch 6940/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7627778887748718
Epoch: 6940, Batch Gradient Norm: 29.031471188145055
Epoch: 6940, Batch Gradient Norm after: 22.36067948573125
Epoch 6941/10000, Prediction Accuracy = 59.914%, Loss = 0.7587140202522278
Epoch: 6941, Batch Gradient Norm: 30.77031654062771
Epoch: 6941, Batch Gradient Norm after: 22.360677231142265
Epoch 6942/10000, Prediction Accuracy = 59.89%, Loss = 0.7627049446105957
Epoch: 6942, Batch Gradient Norm: 29.030168126922042
Epoch: 6942, Batch Gradient Norm after: 22.360678995343136
Epoch 6943/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.7586241722106933
Epoch: 6943, Batch Gradient Norm: 30.768481589194568
Epoch: 6943, Batch Gradient Norm after: 22.360677256653762
Epoch 6944/10000, Prediction Accuracy = 59.908%, Loss = 0.762645173072815
Epoch: 6944, Batch Gradient Norm: 29.025230415609112
Epoch: 6944, Batch Gradient Norm after: 22.36067739103672
Epoch 6945/10000, Prediction Accuracy = 59.94%, Loss = 0.758587121963501
Epoch: 6945, Batch Gradient Norm: 30.767491646906223
Epoch: 6945, Batch Gradient Norm after: 22.360679006913166
Epoch 6946/10000, Prediction Accuracy = 59.914%, Loss = 0.7626844882965088
Epoch: 6946, Batch Gradient Norm: 29.022831708681
Epoch: 6946, Batch Gradient Norm after: 22.360676935196384
Epoch 6947/10000, Prediction Accuracy = 59.944%, Loss = 0.758668851852417
Epoch: 6947, Batch Gradient Norm: 30.7623125666195
Epoch: 6947, Batch Gradient Norm after: 22.360680432988747
Epoch 6948/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.7626991748809815
Epoch: 6948, Batch Gradient Norm: 29.01517393506649
Epoch: 6948, Batch Gradient Norm after: 22.36067723887433
Epoch 6949/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7585639357566833
Epoch: 6949, Batch Gradient Norm: 30.755814473616997
Epoch: 6949, Batch Gradient Norm after: 22.360678169664318
Epoch 6950/10000, Prediction Accuracy = 59.912%, Loss = 0.7624795198440552
Epoch: 6950, Batch Gradient Norm: 29.019032746590508
Epoch: 6950, Batch Gradient Norm after: 22.360678276927484
Epoch 6951/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7583716988563538
Epoch: 6951, Batch Gradient Norm: 30.753123377718516
Epoch: 6951, Batch Gradient Norm after: 22.36067701051163
Epoch 6952/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7623086333274841
Epoch: 6952, Batch Gradient Norm: 29.02738499653309
Epoch: 6952, Batch Gradient Norm after: 22.360677907130203
Epoch 6953/10000, Prediction Accuracy = 59.916%, Loss = 0.7582825064659119
Epoch: 6953, Batch Gradient Norm: 30.743858321590185
Epoch: 6953, Batch Gradient Norm after: 22.36067775911415
Epoch 6954/10000, Prediction Accuracy = 59.9%, Loss = 0.7622388124465942
Epoch: 6954, Batch Gradient Norm: 29.026738070529387
Epoch: 6954, Batch Gradient Norm after: 22.36067931045968
Epoch 6955/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.758207643032074
Epoch: 6955, Batch Gradient Norm: 30.740200433336607
Epoch: 6955, Batch Gradient Norm after: 22.360677921095732
Epoch 6956/10000, Prediction Accuracy = 59.92%, Loss = 0.7621743559837342
Epoch: 6956, Batch Gradient Norm: 29.030140962139388
Epoch: 6956, Batch Gradient Norm after: 22.360678701480534
Epoch 6957/10000, Prediction Accuracy = 59.948%, Loss = 0.758203136920929
Epoch: 6957, Batch Gradient Norm: 30.735885615699637
Epoch: 6957, Batch Gradient Norm after: 22.360678823060745
Epoch 6958/10000, Prediction Accuracy = 59.924%, Loss = 0.7622314691543579
Epoch: 6958, Batch Gradient Norm: 29.02888862129031
Epoch: 6958, Batch Gradient Norm after: 22.360677807075955
Epoch 6959/10000, Prediction Accuracy = 59.928%, Loss = 0.7582850694656372
Epoch: 6959, Batch Gradient Norm: 30.726371130797272
Epoch: 6959, Batch Gradient Norm after: 22.360677524787256
Epoch 6960/10000, Prediction Accuracy = 59.926%, Loss = 0.7621854066848754
Epoch: 6960, Batch Gradient Norm: 29.02472539593387
Epoch: 6960, Batch Gradient Norm after: 22.360675658635948
Epoch 6961/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7581405758857727
Epoch: 6961, Batch Gradient Norm: 30.721160218959533
Epoch: 6961, Batch Gradient Norm after: 22.360676716794668
Epoch 6962/10000, Prediction Accuracy = 59.92199999999999%, Loss = 0.7619584321975708
Epoch: 6962, Batch Gradient Norm: 29.0239856528934
Epoch: 6962, Batch Gradient Norm after: 22.3606753784431
Epoch 6963/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7579686284065247
Epoch: 6963, Batch Gradient Norm: 30.72039039409693
Epoch: 6963, Batch Gradient Norm after: 22.360679282009592
Epoch 6964/10000, Prediction Accuracy = 59.902%, Loss = 0.7618201017379761
Epoch: 6964, Batch Gradient Norm: 29.026786099279903
Epoch: 6964, Batch Gradient Norm after: 22.36067856060089
Epoch 6965/10000, Prediction Accuracy = 59.938%, Loss = 0.7578845143318176
Epoch: 6965, Batch Gradient Norm: 30.714345475085587
Epoch: 6965, Batch Gradient Norm after: 22.360678226541697
Epoch 6966/10000, Prediction Accuracy = 59.916%, Loss = 0.7617567420005799
Epoch: 6966, Batch Gradient Norm: 29.024433253884432
Epoch: 6966, Batch Gradient Norm after: 22.36067780803022
Epoch 6967/10000, Prediction Accuracy = 59.916%, Loss = 0.7578042507171631
Epoch: 6967, Batch Gradient Norm: 30.712843932016902
Epoch: 6967, Batch Gradient Norm after: 22.360678317059694
Epoch 6968/10000, Prediction Accuracy = 59.916%, Loss = 0.7617117881774902
Epoch: 6968, Batch Gradient Norm: 29.02474274434404
Epoch: 6968, Batch Gradient Norm after: 22.36067715803416
Epoch 6969/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7578165888786316
Epoch: 6969, Batch Gradient Norm: 30.710632891027522
Epoch: 6969, Batch Gradient Norm after: 22.360677632927032
Epoch 6970/10000, Prediction Accuracy = 59.936%, Loss = 0.7617796301841736
Epoch: 6970, Batch Gradient Norm: 29.016102788835315
Epoch: 6970, Batch Gradient Norm after: 22.360677496865275
Epoch 6971/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7578703999519348
Epoch: 6971, Batch Gradient Norm: 30.70371850522636
Epoch: 6971, Batch Gradient Norm after: 22.360679257144778
Epoch 6972/10000, Prediction Accuracy = 59.928%, Loss = 0.7617043852806091
Epoch: 6972, Batch Gradient Norm: 29.010782096457522
Epoch: 6972, Batch Gradient Norm after: 22.36067633942598
Epoch 6973/10000, Prediction Accuracy = 59.948%, Loss = 0.7576901316642761
Epoch: 6973, Batch Gradient Norm: 30.70102552201238
Epoch: 6973, Batch Gradient Norm after: 22.360678085254627
Epoch 6974/10000, Prediction Accuracy = 59.952%, Loss = 0.7614803194999695
Epoch: 6974, Batch Gradient Norm: 29.009840561667144
Epoch: 6974, Batch Gradient Norm after: 22.360677534890385
Epoch 6975/10000, Prediction Accuracy = 59.948%, Loss = 0.7575244784355164
Epoch: 6975, Batch Gradient Norm: 30.69913344833854
Epoch: 6975, Batch Gradient Norm after: 22.360676923791914
Epoch 6976/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7613755226135254
Epoch: 6976, Batch Gradient Norm: 29.0095736648457
Epoch: 6976, Batch Gradient Norm after: 22.360678742312444
Epoch 6977/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7574387907981872
Epoch: 6977, Batch Gradient Norm: 30.693843762678895
Epoch: 6977, Batch Gradient Norm after: 22.360678207562675
Epoch 6978/10000, Prediction Accuracy = 59.912%, Loss = 0.7613138556480408
Epoch: 6978, Batch Gradient Norm: 29.007399521401762
Epoch: 6978, Batch Gradient Norm after: 22.360678349179064
Epoch 6979/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7573664665222168
Epoch: 6979, Batch Gradient Norm: 30.694103353752237
Epoch: 6979, Batch Gradient Norm after: 22.36067803625261
Epoch 6980/10000, Prediction Accuracy = 59.94%, Loss = 0.7612974643707275
Epoch: 6980, Batch Gradient Norm: 29.00673483352435
Epoch: 6980, Batch Gradient Norm after: 22.360678631149895
Epoch 6981/10000, Prediction Accuracy = 59.932%, Loss = 0.7574195146560669
Epoch: 6981, Batch Gradient Norm: 30.6890268488874
Epoch: 6981, Batch Gradient Norm after: 22.360680084012028
Epoch 6982/10000, Prediction Accuracy = 59.936%, Loss = 0.7613689303398132
Epoch: 6982, Batch Gradient Norm: 28.99867362804346
Epoch: 6982, Batch Gradient Norm after: 22.36067690335461
Epoch 6983/10000, Prediction Accuracy = 59.944%, Loss = 0.7574169754981994
Epoch: 6983, Batch Gradient Norm: 30.68222597553971
Epoch: 6983, Batch Gradient Norm after: 22.360676984247974
Epoch 6984/10000, Prediction Accuracy = 59.938%, Loss = 0.7612131834030151
Epoch: 6984, Batch Gradient Norm: 28.99596147256138
Epoch: 6984, Batch Gradient Norm after: 22.36067838515832
Epoch 6985/10000, Prediction Accuracy = 59.948%, Loss = 0.757210087776184
Epoch: 6985, Batch Gradient Norm: 30.684088438846032
Epoch: 6985, Batch Gradient Norm after: 22.360677699338662
Epoch 6986/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7610068202018738
Epoch: 6986, Batch Gradient Norm: 28.995269282735542
Epoch: 6986, Batch Gradient Norm after: 22.36067917270807
Epoch 6987/10000, Prediction Accuracy = 59.944%, Loss = 0.7570764183998108
Epoch: 6987, Batch Gradient Norm: 30.678485113254315
Epoch: 6987, Batch Gradient Norm after: 22.36067963996152
Epoch 6988/10000, Prediction Accuracy = 59.902%, Loss = 0.7609296441078186
Epoch: 6988, Batch Gradient Norm: 28.992110515968648
Epoch: 6988, Batch Gradient Norm after: 22.36067904811334
Epoch 6989/10000, Prediction Accuracy = 59.95%, Loss = 0.7569873094558716
Epoch: 6989, Batch Gradient Norm: 30.677123600888365
Epoch: 6989, Batch Gradient Norm after: 22.36068055996889
Epoch 6990/10000, Prediction Accuracy = 59.93399999999999%, Loss = 0.7608710646629333
Epoch: 6990, Batch Gradient Norm: 28.988928937485976
Epoch: 6990, Batch Gradient Norm after: 22.360679012368674
Epoch 6991/10000, Prediction Accuracy = 59.962%, Loss = 0.7569470405578613
Epoch: 6991, Batch Gradient Norm: 30.675134917140923
Epoch: 6991, Batch Gradient Norm after: 22.360678919850383
Epoch 6992/10000, Prediction Accuracy = 59.936%, Loss = 0.7609008908271789
Epoch: 6992, Batch Gradient Norm: 28.9845909289292
Epoch: 6992, Batch Gradient Norm after: 22.36067791451829
Epoch 6993/10000, Prediction Accuracy = 59.936%, Loss = 0.7570168018341065
Epoch: 6993, Batch Gradient Norm: 30.669489804817626
Epoch: 6993, Batch Gradient Norm after: 22.36067748806161
Epoch 6994/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7609175562858581
Epoch: 6994, Batch Gradient Norm: 28.977449754532998
Epoch: 6994, Batch Gradient Norm after: 22.360677150640207
Epoch 6995/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.7569244503974915
Epoch: 6995, Batch Gradient Norm: 30.66536028294965
Epoch: 6995, Batch Gradient Norm after: 22.36067853986393
Epoch 6996/10000, Prediction Accuracy = 59.938%, Loss = 0.7607231259346008
Epoch: 6996, Batch Gradient Norm: 28.976677916051923
Epoch: 6996, Batch Gradient Norm after: 22.360679047179712
Epoch 6997/10000, Prediction Accuracy = 59.948%, Loss = 0.7567353010177612
Epoch: 6997, Batch Gradient Norm: 30.664694365126365
Epoch: 6997, Batch Gradient Norm after: 22.360678132407166
Epoch 6998/10000, Prediction Accuracy = 59.92%, Loss = 0.7605506420135498
Epoch: 6998, Batch Gradient Norm: 28.978631769894083
Epoch: 6998, Batch Gradient Norm after: 22.360677674992044
Epoch 6999/10000, Prediction Accuracy = 59.952%, Loss = 0.7566390752792358
Epoch: 6999, Batch Gradient Norm: 30.65674717456053
Epoch: 6999, Batch Gradient Norm after: 22.36067790071971
Epoch 7000/10000, Prediction Accuracy = 59.926%, Loss = 0.7604826331138611
Epoch: 7000, Batch Gradient Norm: 28.976560589858114
Epoch: 7000, Batch Gradient Norm after: 22.360678445992548
Epoch 7001/10000, Prediction Accuracy = 59.962%, Loss = 0.7565579414367676
Epoch: 7001, Batch Gradient Norm: 30.65416883520299
Epoch: 7001, Batch Gradient Norm after: 22.36067778918044
Epoch 7002/10000, Prediction Accuracy = 59.898%, Loss = 0.7604241609573364
Epoch: 7002, Batch Gradient Norm: 28.97571662565873
Epoch: 7002, Batch Gradient Norm after: 22.36067790391466
Epoch 7003/10000, Prediction Accuracy = 59.962%, Loss = 0.7565470814704895
Epoch: 7003, Batch Gradient Norm: 30.65341840831825
Epoch: 7003, Batch Gradient Norm after: 22.360679729187158
Epoch 7004/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.7604905843734742
Epoch: 7004, Batch Gradient Norm: 28.967803002453326
Epoch: 7004, Batch Gradient Norm after: 22.360677945357672
Epoch 7005/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7566162824630738
Epoch: 7005, Batch Gradient Norm: 30.647897180583804
Epoch: 7005, Batch Gradient Norm after: 22.36067835479827
Epoch 7006/10000, Prediction Accuracy = 59.948%, Loss = 0.7604636430740357
Epoch: 7006, Batch Gradient Norm: 28.959333614088504
Epoch: 7006, Batch Gradient Norm after: 22.360675615976447
Epoch 7007/10000, Prediction Accuracy = 59.948%, Loss = 0.756464958190918
Epoch: 7007, Batch Gradient Norm: 30.646246736302665
Epoch: 7007, Batch Gradient Norm after: 22.36067818060783
Epoch 7008/10000, Prediction Accuracy = 59.962%, Loss = 0.7602329730987549
Epoch: 7008, Batch Gradient Norm: 28.955382406619474
Epoch: 7008, Batch Gradient Norm after: 22.360675631390723
Epoch 7009/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7562795758247376
Epoch: 7009, Batch Gradient Norm: 30.646498053585418
Epoch: 7009, Batch Gradient Norm after: 22.360676185777724
Epoch 7010/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.760104489326477
Epoch: 7010, Batch Gradient Norm: 28.954067612592688
Epoch: 7010, Batch Gradient Norm after: 22.360679053316357
Epoch 7011/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.756194531917572
Epoch: 7011, Batch Gradient Norm: 30.640104431292425
Epoch: 7011, Batch Gradient Norm after: 22.360678206814192
Epoch 7012/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7600362420082092
Epoch: 7012, Batch Gradient Norm: 28.95255031153594
Epoch: 7012, Batch Gradient Norm after: 22.3606763057146
Epoch 7013/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.7561131358146668
Epoch: 7013, Batch Gradient Norm: 30.63750822722875
Epoch: 7013, Batch Gradient Norm after: 22.360678133237695
Epoch 7014/10000, Prediction Accuracy = 59.94%, Loss = 0.7599941372871399
Epoch: 7014, Batch Gradient Norm: 28.955724044203443
Epoch: 7014, Batch Gradient Norm after: 22.36067952934561
Epoch 7015/10000, Prediction Accuracy = 59.944%, Loss = 0.7561381459236145
Epoch: 7015, Batch Gradient Norm: 30.636088077681926
Epoch: 7015, Batch Gradient Norm after: 22.360676042152267
Epoch 7016/10000, Prediction Accuracy = 59.932%, Loss = 0.760083532333374
Epoch: 7016, Batch Gradient Norm: 28.94726726428951
Epoch: 7016, Batch Gradient Norm after: 22.36068014015576
Epoch 7017/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7561891913414002
Epoch: 7017, Batch Gradient Norm: 30.63017230954654
Epoch: 7017, Batch Gradient Norm after: 22.36067757620788
Epoch 7018/10000, Prediction Accuracy = 59.92%, Loss = 0.7599895119667053
Epoch: 7018, Batch Gradient Norm: 28.940214557233443
Epoch: 7018, Batch Gradient Norm after: 22.36067822851764
Epoch 7019/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7559943079948426
Epoch: 7019, Batch Gradient Norm: 30.626657958705636
Epoch: 7019, Batch Gradient Norm after: 22.360679838891567
Epoch 7020/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7597508788108825
Epoch: 7020, Batch Gradient Norm: 28.942664125214986
Epoch: 7020, Batch Gradient Norm after: 22.360677190296173
Epoch 7021/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7558462262153626
Epoch: 7021, Batch Gradient Norm: 30.627008030738637
Epoch: 7021, Batch Gradient Norm after: 22.360676713684647
Epoch 7022/10000, Prediction Accuracy = 59.91200000000001%, Loss = 0.7596484661102295
Epoch: 7022, Batch Gradient Norm: 28.938928296538933
Epoch: 7022, Batch Gradient Norm after: 22.36068125276612
Epoch 7023/10000, Prediction Accuracy = 59.952%, Loss = 0.7557651400566101
Epoch: 7023, Batch Gradient Norm: 30.620725364947052
Epoch: 7023, Batch Gradient Norm after: 22.360677307337255
Epoch 7024/10000, Prediction Accuracy = 59.938%, Loss = 0.759585177898407
Epoch: 7024, Batch Gradient Norm: 28.938390902845583
Epoch: 7024, Batch Gradient Norm after: 22.360677519611954
Epoch 7025/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7556971073150635
Epoch: 7025, Batch Gradient Norm: 30.62121061460612
Epoch: 7025, Batch Gradient Norm after: 22.360676863774927
Epoch 7026/10000, Prediction Accuracy = 59.952%, Loss = 0.7595723509788513
Epoch: 7026, Batch Gradient Norm: 28.937840555660586
Epoch: 7026, Batch Gradient Norm after: 22.360679180045192
Epoch 7027/10000, Prediction Accuracy = 59.938%, Loss = 0.7557515263557434
Epoch: 7027, Batch Gradient Norm: 30.61614936489344
Epoch: 7027, Batch Gradient Norm after: 22.360677109648776
Epoch 7028/10000, Prediction Accuracy = 59.928%, Loss = 0.7596512556076049
Epoch: 7028, Batch Gradient Norm: 28.9281300060681
Epoch: 7028, Batch Gradient Norm after: 22.360679323042607
Epoch 7029/10000, Prediction Accuracy = 59.91400000000001%, Loss = 0.7557395100593567
Epoch: 7029, Batch Gradient Norm: 30.610572130017864
Epoch: 7029, Batch Gradient Norm after: 22.360678552521748
Epoch 7030/10000, Prediction Accuracy = 59.936%, Loss = 0.7594918131828308
Epoch: 7030, Batch Gradient Norm: 28.92438349191654
Epoch: 7030, Batch Gradient Norm after: 22.360679528942548
Epoch 7031/10000, Prediction Accuracy = 59.96%, Loss = 0.7555428504943847
Epoch: 7031, Batch Gradient Norm: 30.606160558790133
Epoch: 7031, Batch Gradient Norm after: 22.360678640805578
Epoch 7032/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.7592801332473755
Epoch: 7032, Batch Gradient Norm: 28.926613991321013
Epoch: 7032, Batch Gradient Norm after: 22.360678260974485
Epoch 7033/10000, Prediction Accuracy = 59.976%, Loss = 0.7554222702980041
Epoch: 7033, Batch Gradient Norm: 30.600782192052122
Epoch: 7033, Batch Gradient Norm after: 22.360678978237033
Epoch 7034/10000, Prediction Accuracy = 59.95%, Loss = 0.7592022538185119
Epoch: 7034, Batch Gradient Norm: 28.924826631853712
Epoch: 7034, Batch Gradient Norm after: 22.360678733437705
Epoch 7035/10000, Prediction Accuracy = 59.98%, Loss = 0.7553407311439514
Epoch: 7035, Batch Gradient Norm: 30.59631615054983
Epoch: 7035, Batch Gradient Norm after: 22.3606776456661
Epoch 7036/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7591366291046142
Epoch: 7036, Batch Gradient Norm: 28.92649542444101
Epoch: 7036, Batch Gradient Norm after: 22.360680131386257
Epoch 7037/10000, Prediction Accuracy = 59.962%, Loss = 0.755292010307312
Epoch: 7037, Batch Gradient Norm: 30.594654613088405
Epoch: 7037, Batch Gradient Norm after: 22.360679615854448
Epoch 7038/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7591488242149353
Epoch: 7038, Batch Gradient Norm: 28.923016063256128
Epoch: 7038, Batch Gradient Norm after: 22.36067758130026
Epoch 7039/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.7553550720214843
Epoch: 7039, Batch Gradient Norm: 30.590902961783815
Epoch: 7039, Batch Gradient Norm after: 22.36067848327447
Epoch 7040/10000, Prediction Accuracy = 59.924%, Loss = 0.7592048287391663
Epoch: 7040, Batch Gradient Norm: 28.91286754229103
Epoch: 7040, Batch Gradient Norm after: 22.36067788639128
Epoch 7041/10000, Prediction Accuracy = 59.926%, Loss = 0.7553058505058289
Epoch: 7041, Batch Gradient Norm: 30.58470583290813
Epoch: 7041, Batch Gradient Norm after: 22.36067739421411
Epoch 7042/10000, Prediction Accuracy = 59.95%, Loss = 0.7590210676193238
Epoch: 7042, Batch Gradient Norm: 28.910890111700255
Epoch: 7042, Batch Gradient Norm after: 22.360678606683337
Epoch 7043/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7551036596298217
Epoch: 7043, Batch Gradient Norm: 30.58420570002874
Epoch: 7043, Batch Gradient Norm after: 22.360678789657513
Epoch 7044/10000, Prediction Accuracy = 59.95%, Loss = 0.75882488489151
Epoch: 7044, Batch Gradient Norm: 28.91291281096542
Epoch: 7044, Batch Gradient Norm after: 22.360678380749363
Epoch 7045/10000, Prediction Accuracy = 59.98%, Loss = 0.7549970626831055
Epoch: 7045, Batch Gradient Norm: 30.57882735832196
Epoch: 7045, Batch Gradient Norm after: 22.360680510552903
Epoch 7046/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7587570905685425
Epoch: 7046, Batch Gradient Norm: 28.90858524501885
Epoch: 7046, Batch Gradient Norm after: 22.36068148499365
Epoch 7047/10000, Prediction Accuracy = 59.996%, Loss = 0.7549065947532654
Epoch: 7047, Batch Gradient Norm: 30.576734569739045
Epoch: 7047, Batch Gradient Norm after: 22.360678920290784
Epoch 7048/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.7586982011795044
Epoch: 7048, Batch Gradient Norm: 28.906017098069498
Epoch: 7048, Batch Gradient Norm after: 22.360679607021805
Epoch 7049/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.754870080947876
Epoch: 7049, Batch Gradient Norm: 30.576070609087154
Epoch: 7049, Batch Gradient Norm after: 22.3606756925045
Epoch 7050/10000, Prediction Accuracy = 59.962%, Loss = 0.7587421894073486
Epoch: 7050, Batch Gradient Norm: 28.900978921081975
Epoch: 7050, Batch Gradient Norm after: 22.36067755321475
Epoch 7051/10000, Prediction Accuracy = 59.94599999999999%, Loss = 0.7549481987953186
Epoch: 7051, Batch Gradient Norm: 30.573865227344875
Epoch: 7051, Batch Gradient Norm after: 22.3606768509408
Epoch 7052/10000, Prediction Accuracy = 59.938%, Loss = 0.7587654829025269
Epoch: 7052, Batch Gradient Norm: 28.892543279066388
Epoch: 7052, Batch Gradient Norm after: 22.36067638218132
Epoch 7053/10000, Prediction Accuracy = 59.96%, Loss = 0.7548235893249512
Epoch: 7053, Batch Gradient Norm: 30.56846693768122
Epoch: 7053, Batch Gradient Norm after: 22.360677941816757
Epoch 7054/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7585449576377868
Epoch: 7054, Batch Gradient Norm: 28.891347525207692
Epoch: 7054, Batch Gradient Norm after: 22.36067847914734
Epoch 7055/10000, Prediction Accuracy = 59.972%, Loss = 0.7546342730522155
Epoch: 7055, Batch Gradient Norm: 30.56965736229341
Epoch: 7055, Batch Gradient Norm after: 22.360678408367324
Epoch 7056/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7583879590034485
Epoch: 7056, Batch Gradient Norm: 28.89035458870896
Epoch: 7056, Batch Gradient Norm after: 22.360677324417082
Epoch 7057/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7545441746711731
Epoch: 7057, Batch Gradient Norm: 30.562443081947826
Epoch: 7057, Batch Gradient Norm after: 22.360678691971486
Epoch 7058/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.7583303928375245
Epoch: 7058, Batch Gradient Norm: 28.887360191054025
Epoch: 7058, Batch Gradient Norm after: 22.360679617901624
Epoch 7059/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7544495224952698
Epoch: 7059, Batch Gradient Norm: 30.561347300806112
Epoch: 7059, Batch Gradient Norm after: 22.36067906094388
Epoch 7060/10000, Prediction Accuracy = 59.95%, Loss = 0.758285117149353
Epoch: 7060, Batch Gradient Norm: 28.884370497956517
Epoch: 7060, Batch Gradient Norm after: 22.360677800581975
Epoch 7061/10000, Prediction Accuracy = 59.944%, Loss = 0.7544527888298035
Epoch: 7061, Batch Gradient Norm: 30.560840505582384
Epoch: 7061, Batch Gradient Norm after: 22.36067760232618
Epoch 7062/10000, Prediction Accuracy = 59.94799999999999%, Loss = 0.7583647012710572
Epoch: 7062, Batch Gradient Norm: 28.876451767264562
Epoch: 7062, Batch Gradient Norm after: 22.36067678948337
Epoch 7063/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7545083999633789
Epoch: 7063, Batch Gradient Norm: 30.553080193799712
Epoch: 7063, Batch Gradient Norm after: 22.360677094293173
Epoch 7064/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7583047866821289
Epoch: 7064, Batch Gradient Norm: 28.872460014404478
Epoch: 7064, Batch Gradient Norm after: 22.360676309645864
Epoch 7065/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.7543459296226501
Epoch: 7065, Batch Gradient Norm: 30.55012007348518
Epoch: 7065, Batch Gradient Norm after: 22.36067949587663
Epoch 7066/10000, Prediction Accuracy = 59.95%, Loss = 0.7580681800842285
Epoch: 7066, Batch Gradient Norm: 28.875632154851566
Epoch: 7066, Batch Gradient Norm after: 22.360678719462936
Epoch 7067/10000, Prediction Accuracy = 59.962%, Loss = 0.7541837930679322
Epoch: 7067, Batch Gradient Norm: 30.54640663650782
Epoch: 7067, Batch Gradient Norm after: 22.36067704776499
Epoch 7068/10000, Prediction Accuracy = 59.926%, Loss = 0.757949149608612
Epoch: 7068, Batch Gradient Norm: 28.879079192305866
Epoch: 7068, Batch Gradient Norm after: 22.36067726954349
Epoch 7069/10000, Prediction Accuracy = 59.996%, Loss = 0.754111909866333
Epoch: 7069, Batch Gradient Norm: 30.53639621568595
Epoch: 7069, Batch Gradient Norm after: 22.360676467977523
Epoch 7070/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7578865647315979
Epoch: 7070, Batch Gradient Norm: 28.878601853250082
Epoch: 7070, Batch Gradient Norm after: 22.360678393763788
Epoch 7071/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.7540345072746277
Epoch: 7071, Batch Gradient Norm: 30.536944655723623
Epoch: 7071, Batch Gradient Norm after: 22.360680611487325
Epoch 7072/10000, Prediction Accuracy = 59.962%, Loss = 0.7578638434410095
Epoch: 7072, Batch Gradient Norm: 28.87821392339675
Epoch: 7072, Batch Gradient Norm after: 22.360676574119807
Epoch 7073/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7540746569633484
Epoch: 7073, Batch Gradient Norm: 30.532547800495575
Epoch: 7073, Batch Gradient Norm after: 22.360679875841768
Epoch 7074/10000, Prediction Accuracy = 59.938%, Loss = 0.7579478263854981
Epoch: 7074, Batch Gradient Norm: 28.869599487289324
Epoch: 7074, Batch Gradient Norm after: 22.36067438505516
Epoch 7075/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.754093873500824
Epoch: 7075, Batch Gradient Norm: 30.52655913011036
Epoch: 7075, Batch Gradient Norm after: 22.360678385279616
Epoch 7076/10000, Prediction Accuracy = 59.932%, Loss = 0.7578168392181397
Epoch: 7076, Batch Gradient Norm: 28.86520293752722
Epoch: 7076, Batch Gradient Norm after: 22.360675086588707
Epoch 7077/10000, Prediction Accuracy = 59.972%, Loss = 0.7538961172103882
Epoch: 7077, Batch Gradient Norm: 30.526117818648146
Epoch: 7077, Batch Gradient Norm after: 22.360677632251964
Epoch 7078/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7575943350791932
Epoch: 7078, Batch Gradient Norm: 28.867060231677492
Epoch: 7078, Batch Gradient Norm after: 22.360678089863672
Epoch 7079/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7537629842758179
Epoch: 7079, Batch Gradient Norm: 30.520430582164675
Epoch: 7079, Batch Gradient Norm after: 22.3606780094158
Epoch 7080/10000, Prediction Accuracy = 59.964%, Loss = 0.7575117707252502
Epoch: 7080, Batch Gradient Norm: 28.867752930290937
Epoch: 7080, Batch Gradient Norm after: 22.360679155731916
Epoch 7081/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7536856532096863
Epoch: 7081, Batch Gradient Norm: 30.51402893514359
Epoch: 7081, Batch Gradient Norm after: 22.36067908064398
Epoch 7082/10000, Prediction Accuracy = 59.968%, Loss = 0.7574418783187866
Epoch: 7082, Batch Gradient Norm: 28.866329240663536
Epoch: 7082, Batch Gradient Norm after: 22.36067731781953
Epoch 7083/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7536257266998291
Epoch: 7083, Batch Gradient Norm: 30.516145464545165
Epoch: 7083, Batch Gradient Norm after: 22.360679183568205
Epoch 7084/10000, Prediction Accuracy = 59.94%, Loss = 0.7574442386627197
Epoch: 7084, Batch Gradient Norm: 28.86378969364232
Epoch: 7084, Batch Gradient Norm after: 22.360676203788802
Epoch 7085/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7536935091018677
Epoch: 7085, Batch Gradient Norm: 30.510051691119397
Epoch: 7085, Batch Gradient Norm after: 22.36067978742556
Epoch 7086/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.757528281211853
Epoch: 7086, Batch Gradient Norm: 28.856649973654225
Epoch: 7086, Batch Gradient Norm after: 22.36067968550702
Epoch 7087/10000, Prediction Accuracy = 59.936%, Loss = 0.7536760330200195
Epoch: 7087, Batch Gradient Norm: 30.503523075977427
Epoch: 7087, Batch Gradient Norm after: 22.36067885048087
Epoch 7088/10000, Prediction Accuracy = 59.964%, Loss = 0.7573371767997742
Epoch: 7088, Batch Gradient Norm: 28.85767836313998
Epoch: 7088, Batch Gradient Norm after: 22.360675354208883
Epoch 7089/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7534617424011231
Epoch: 7089, Batch Gradient Norm: 30.501528428771124
Epoch: 7089, Batch Gradient Norm after: 22.360680928333156
Epoch 7090/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7571305274963379
Epoch: 7090, Batch Gradient Norm: 28.862555279529357
Epoch: 7090, Batch Gradient Norm after: 22.360676993547106
Epoch 7091/10000, Prediction Accuracy = 60.004%, Loss = 0.7533622026443482
Epoch: 7091, Batch Gradient Norm: 30.49145299451835
Epoch: 7091, Batch Gradient Norm after: 22.36068107137561
Epoch 7092/10000, Prediction Accuracy = 59.982000000000006%, Loss = 0.7570589542388916
Epoch: 7092, Batch Gradient Norm: 28.86147818254915
Epoch: 7092, Batch Gradient Norm after: 22.360679214481973
Epoch 7093/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7532838225364685
Epoch: 7093, Batch Gradient Norm: 30.487321887360164
Epoch: 7093, Batch Gradient Norm after: 22.360677124628523
Epoch 7094/10000, Prediction Accuracy = 59.962%, Loss = 0.7569938898086548
Epoch: 7094, Batch Gradient Norm: 28.8631755309568
Epoch: 7094, Batch Gradient Norm after: 22.360677964472004
Epoch 7095/10000, Prediction Accuracy = 59.95%, Loss = 0.7532558679580689
Epoch: 7095, Batch Gradient Norm: 30.48810360149183
Epoch: 7095, Batch Gradient Norm after: 22.36068089152868
Epoch 7096/10000, Prediction Accuracy = 59.95%, Loss = 0.7570292353630066
Epoch: 7096, Batch Gradient Norm: 28.858812170146532
Epoch: 7096, Batch Gradient Norm after: 22.36067668867346
Epoch 7097/10000, Prediction Accuracy = 59.948%, Loss = 0.7533308863639832
Epoch: 7097, Batch Gradient Norm: 30.480551855564848
Epoch: 7097, Batch Gradient Norm after: 22.360677760999526
Epoch 7098/10000, Prediction Accuracy = 59.95%, Loss = 0.7570432305335999
Epoch: 7098, Batch Gradient Norm: 28.851673641394783
Epoch: 7098, Batch Gradient Norm after: 22.360677530478302
Epoch 7099/10000, Prediction Accuracy = 59.964%, Loss = 0.7532374858856201
Epoch: 7099, Batch Gradient Norm: 30.47611199669684
Epoch: 7099, Batch Gradient Norm after: 22.360679278254775
Epoch 7100/10000, Prediction Accuracy = 59.962%, Loss = 0.7568371176719666
Epoch: 7100, Batch Gradient Norm: 28.85258115656576
Epoch: 7100, Batch Gradient Norm after: 22.36067653667808
Epoch 7101/10000, Prediction Accuracy = 59.964%, Loss = 0.753049349784851
Epoch: 7101, Batch Gradient Norm: 30.474245538772713
Epoch: 7101, Batch Gradient Norm after: 22.360679342763046
Epoch 7102/10000, Prediction Accuracy = 59.98%, Loss = 0.7566675662994384
Epoch: 7102, Batch Gradient Norm: 28.855167820875323
Epoch: 7102, Batch Gradient Norm after: 22.360678386761514
Epoch 7103/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7529605150222778
Epoch: 7103, Batch Gradient Norm: 30.466704045576314
Epoch: 7103, Batch Gradient Norm after: 22.36067941689708
Epoch 7104/10000, Prediction Accuracy = 59.980000000000004%, Loss = 0.7566060304641724
Epoch: 7104, Batch Gradient Norm: 28.85570366044666
Epoch: 7104, Batch Gradient Norm after: 22.36067847240791
Epoch 7105/10000, Prediction Accuracy = 60.016%, Loss = 0.7528818368911743
Epoch: 7105, Batch Gradient Norm: 30.462923416847094
Epoch: 7105, Batch Gradient Norm after: 22.360678535431855
Epoch 7106/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.7565472245216369
Epoch: 7106, Batch Gradient Norm: 28.85649166489828
Epoch: 7106, Batch Gradient Norm after: 22.360678210516795
Epoch 7107/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7528804659843444
Epoch: 7107, Batch Gradient Norm: 30.459318844090927
Epoch: 7107, Batch Gradient Norm after: 22.36067793506813
Epoch 7108/10000, Prediction Accuracy = 59.948%, Loss = 0.7566144108772278
Epoch: 7108, Batch Gradient Norm: 28.853337779644036
Epoch: 7108, Batch Gradient Norm after: 22.36067670145528
Epoch 7109/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7529516816139221
Epoch: 7109, Batch Gradient Norm: 30.45318659231576
Epoch: 7109, Batch Gradient Norm after: 22.36067856549531
Epoch 7110/10000, Prediction Accuracy = 59.95%, Loss = 0.7565768957138062
Epoch: 7110, Batch Gradient Norm: 28.8477092362113
Epoch: 7110, Batch Gradient Norm after: 22.360678524401088
Epoch 7111/10000, Prediction Accuracy = 59.962%, Loss = 0.7528099179267883
Epoch: 7111, Batch Gradient Norm: 30.44803674135248
Epoch: 7111, Batch Gradient Norm after: 22.360677721802624
Epoch 7112/10000, Prediction Accuracy = 59.976%, Loss = 0.7563270807266236
Epoch: 7112, Batch Gradient Norm: 28.8504829807872
Epoch: 7112, Batch Gradient Norm after: 22.360676332646097
Epoch 7113/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7526345729827881
Epoch: 7113, Batch Gradient Norm: 30.446763384054027
Epoch: 7113, Batch Gradient Norm after: 22.360678134855124
Epoch 7114/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7562112092971802
Epoch: 7114, Batch Gradient Norm: 28.853180887624518
Epoch: 7114, Batch Gradient Norm after: 22.360679485050156
Epoch 7115/10000, Prediction Accuracy = 60.016%, Loss = 0.7525597810745239
Epoch: 7115, Batch Gradient Norm: 30.437987531261484
Epoch: 7115, Batch Gradient Norm after: 22.360676820864118
Epoch 7116/10000, Prediction Accuracy = 59.968%, Loss = 0.7561423182487488
Epoch: 7116, Batch Gradient Norm: 28.852550727672845
Epoch: 7116, Batch Gradient Norm after: 22.36067948360861
Epoch 7117/10000, Prediction Accuracy = 59.996%, Loss = 0.7524822235107422
Epoch: 7117, Batch Gradient Norm: 30.438370373581048
Epoch: 7117, Batch Gradient Norm after: 22.360679693253147
Epoch 7118/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7561185836791993
Epoch: 7118, Batch Gradient Norm: 28.852783820125865
Epoch: 7118, Batch Gradient Norm after: 22.36067753923057
Epoch 7119/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7525148749351501
Epoch: 7119, Batch Gradient Norm: 30.43353258147818
Epoch: 7119, Batch Gradient Norm after: 22.360679508883646
Epoch 7120/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7561737060546875
Epoch: 7120, Batch Gradient Norm: 28.847895679267598
Epoch: 7120, Batch Gradient Norm after: 22.36067751929095
Epoch 7121/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7525449156761169
Epoch: 7121, Batch Gradient Norm: 30.42751631438444
Epoch: 7121, Batch Gradient Norm after: 22.36067648134256
Epoch 7122/10000, Prediction Accuracy = 59.972%, Loss = 0.7560911297798156
Epoch: 7122, Batch Gradient Norm: 28.843392801719208
Epoch: 7122, Batch Gradient Norm after: 22.36067754408636
Epoch 7123/10000, Prediction Accuracy = 59.96%, Loss = 0.7523729562759399
Epoch: 7123, Batch Gradient Norm: 30.4239015598096
Epoch: 7123, Batch Gradient Norm after: 22.360678473700865
Epoch 7124/10000, Prediction Accuracy = 59.97600000000001%, Loss = 0.7558520674705506
Epoch: 7124, Batch Gradient Norm: 28.850462151343763
Epoch: 7124, Batch Gradient Norm after: 22.360678016226714
Epoch 7125/10000, Prediction Accuracy = 59.980000000000004%, Loss = 0.7522282719612121
Epoch: 7125, Batch Gradient Norm: 30.419299901990716
Epoch: 7125, Batch Gradient Norm after: 22.360677559759264
Epoch 7126/10000, Prediction Accuracy = 59.952%, Loss = 0.7557636141777039
Epoch: 7126, Batch Gradient Norm: 28.85070539767228
Epoch: 7126, Batch Gradient Norm after: 22.360677931575044
Epoch 7127/10000, Prediction Accuracy = 60.012%, Loss = 0.7521519064903259
Epoch: 7127, Batch Gradient Norm: 30.412017745588287
Epoch: 7127, Batch Gradient Norm after: 22.360679383071222
Epoch 7128/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7556939363479614
Epoch: 7128, Batch Gradient Norm: 28.84937446707453
Epoch: 7128, Batch Gradient Norm after: 22.360677179147036
Epoch 7129/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7520880460739136
Epoch: 7129, Batch Gradient Norm: 30.416137801947606
Epoch: 7129, Batch Gradient Norm after: 22.360675435481824
Epoch 7130/10000, Prediction Accuracy = 59.982000000000006%, Loss = 0.7556920647621155
Epoch: 7130, Batch Gradient Norm: 28.844677558703342
Epoch: 7130, Batch Gradient Norm after: 22.36067772881037
Epoch 7131/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7521304726600647
Epoch: 7131, Batch Gradient Norm: 30.41220475231146
Epoch: 7131, Batch Gradient Norm after: 22.360677535494315
Epoch 7132/10000, Prediction Accuracy = 59.976%, Loss = 0.7557600617408753
Epoch: 7132, Batch Gradient Norm: 28.83471811688496
Epoch: 7132, Batch Gradient Norm after: 22.36067576726802
Epoch 7133/10000, Prediction Accuracy = 59.972%, Loss = 0.7521276831626892
Epoch: 7133, Batch Gradient Norm: 30.40526774143812
Epoch: 7133, Batch Gradient Norm after: 22.36067693223943
Epoch 7134/10000, Prediction Accuracy = 59.976%, Loss = 0.755637812614441
Epoch: 7134, Batch Gradient Norm: 28.830321080838
Epoch: 7134, Batch Gradient Norm after: 22.36067837367853
Epoch 7135/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.7519271492958068
Epoch: 7135, Batch Gradient Norm: 30.40467645650392
Epoch: 7135, Batch Gradient Norm after: 22.360678427364306
Epoch 7136/10000, Prediction Accuracy = 59.98%, Loss = 0.7554031848907471
Epoch: 7136, Batch Gradient Norm: 28.83373723136585
Epoch: 7136, Batch Gradient Norm after: 22.36067815410724
Epoch 7137/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.7518045544624329
Epoch: 7137, Batch Gradient Norm: 30.40069163181214
Epoch: 7137, Batch Gradient Norm after: 22.360678892743742
Epoch 7138/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.7553393006324768
Epoch: 7138, Batch Gradient Norm: 28.830522121704004
Epoch: 7138, Batch Gradient Norm after: 22.360677330579637
Epoch 7139/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7517187237739563
Epoch: 7139, Batch Gradient Norm: 30.396159631912617
Epoch: 7139, Batch Gradient Norm after: 22.360678145477852
Epoch 7140/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7552762508392334
Epoch: 7140, Batch Gradient Norm: 28.825864737751655
Epoch: 7140, Batch Gradient Norm after: 22.360677443326246
Epoch 7141/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7516640782356262
Epoch: 7141, Batch Gradient Norm: 30.40161522764955
Epoch: 7141, Batch Gradient Norm after: 22.360678699646485
Epoch 7142/10000, Prediction Accuracy = 59.976%, Loss = 0.7553007245063782
Epoch: 7142, Batch Gradient Norm: 28.822194386397587
Epoch: 7142, Batch Gradient Norm after: 22.360678597944112
Epoch 7143/10000, Prediction Accuracy = 59.948%, Loss = 0.7517252683639526
Epoch: 7143, Batch Gradient Norm: 30.39640367078986
Epoch: 7143, Batch Gradient Norm after: 22.360678689366186
Epoch 7144/10000, Prediction Accuracy = 59.976%, Loss = 0.7553592920303345
Epoch: 7144, Batch Gradient Norm: 28.809675823770533
Epoch: 7144, Batch Gradient Norm after: 22.360675487294504
Epoch 7145/10000, Prediction Accuracy = 59.982000000000006%, Loss = 0.7516738414764405
Epoch: 7145, Batch Gradient Norm: 30.39266025613629
Epoch: 7145, Batch Gradient Norm after: 22.360676735245526
Epoch 7146/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7551613211631775
Epoch: 7146, Batch Gradient Norm: 28.80525123971125
Epoch: 7146, Batch Gradient Norm after: 22.360677349661437
Epoch 7147/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7514631509780884
Epoch: 7147, Batch Gradient Norm: 30.39098901657066
Epoch: 7147, Batch Gradient Norm after: 22.36067499090294
Epoch 7148/10000, Prediction Accuracy = 59.996%, Loss = 0.7549714088439942
Epoch: 7148, Batch Gradient Norm: 28.80789437373166
Epoch: 7148, Batch Gradient Norm after: 22.360679828201633
Epoch 7149/10000, Prediction Accuracy = 60.028%, Loss = 0.7513609290122986
Epoch: 7149, Batch Gradient Norm: 30.383678647811927
Epoch: 7149, Batch Gradient Norm after: 22.36067798318687
Epoch 7150/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7549152851104737
Epoch: 7150, Batch Gradient Norm: 28.806380261538443
Epoch: 7150, Batch Gradient Norm after: 22.360676280661195
Epoch 7151/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7512672901153564
Epoch: 7151, Batch Gradient Norm: 30.386541102042276
Epoch: 7151, Batch Gradient Norm after: 22.360676248486342
Epoch 7152/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7548637747764587
Epoch: 7152, Batch Gradient Norm: 28.802987339805327
Epoch: 7152, Batch Gradient Norm after: 22.36067699715954
Epoch 7153/10000, Prediction Accuracy = 59.96%, Loss = 0.7512394428253174
Epoch: 7153, Batch Gradient Norm: 30.383951224203976
Epoch: 7153, Batch Gradient Norm after: 22.360676890649156
Epoch 7154/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.7548964738845825
Epoch: 7154, Batch Gradient Norm: 28.798850463667453
Epoch: 7154, Batch Gradient Norm after: 22.360677107540635
Epoch 7155/10000, Prediction Accuracy = 59.962%, Loss = 0.7512964606285095
Epoch: 7155, Batch Gradient Norm: 30.376155538206746
Epoch: 7155, Batch Gradient Norm after: 22.360678645011244
Epoch 7156/10000, Prediction Accuracy = 59.995999999999995%, Loss = 0.7549249291419983
Epoch: 7156, Batch Gradient Norm: 28.790130856153457
Epoch: 7156, Batch Gradient Norm after: 22.360677279241706
Epoch 7157/10000, Prediction Accuracy = 59.968%, Loss = 0.7512224555015564
Epoch: 7157, Batch Gradient Norm: 30.372858071598138
Epoch: 7157, Batch Gradient Norm after: 22.36067718383302
Epoch 7158/10000, Prediction Accuracy = 59.996%, Loss = 0.7547062397003174
Epoch: 7158, Batch Gradient Norm: 28.78820830589802
Epoch: 7158, Batch Gradient Norm after: 22.360676742429003
Epoch 7159/10000, Prediction Accuracy = 60.016%, Loss = 0.7510142683982849
Epoch: 7159, Batch Gradient Norm: 30.3706554449664
Epoch: 7159, Batch Gradient Norm after: 22.360678409726095
Epoch 7160/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7545457482337952
Epoch: 7160, Batch Gradient Norm: 28.791548392909863
Epoch: 7160, Batch Gradient Norm after: 22.360678300628752
Epoch 7161/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7509262561798096
Epoch: 7161, Batch Gradient Norm: 30.364003868124865
Epoch: 7161, Batch Gradient Norm after: 22.360679864232104
Epoch 7162/10000, Prediction Accuracy = 59.972%, Loss = 0.7544908881187439
Epoch: 7162, Batch Gradient Norm: 28.78989255928635
Epoch: 7162, Batch Gradient Norm after: 22.360677069592036
Epoch 7163/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7508359551429749
Epoch: 7163, Batch Gradient Norm: 30.36356865420216
Epoch: 7163, Batch Gradient Norm after: 22.3606784133913
Epoch 7164/10000, Prediction Accuracy = 59.972%, Loss = 0.754438865184784
Epoch: 7164, Batch Gradient Norm: 28.79002998783357
Epoch: 7164, Batch Gradient Norm after: 22.360676084610102
Epoch 7165/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7508346915245057
Epoch: 7165, Batch Gradient Norm: 30.36605675810396
Epoch: 7165, Batch Gradient Norm after: 22.360676747057997
Epoch 7166/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7545015335083007
Epoch: 7166, Batch Gradient Norm: 28.7796470566977
Epoch: 7166, Batch Gradient Norm after: 22.360680681732802
Epoch 7167/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7508846044540405
Epoch: 7167, Batch Gradient Norm: 30.362368330906797
Epoch: 7167, Batch Gradient Norm after: 22.36067734068855
Epoch 7168/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7544954895973206
Epoch: 7168, Batch Gradient Norm: 28.767757994716334
Epoch: 7168, Batch Gradient Norm after: 22.360678667049804
Epoch 7169/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7507371664047241
Epoch: 7169, Batch Gradient Norm: 30.361718539607487
Epoch: 7169, Batch Gradient Norm after: 22.360676516981286
Epoch 7170/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7542505621910095
Epoch: 7170, Batch Gradient Norm: 28.762869327761038
Epoch: 7170, Batch Gradient Norm after: 22.360679341751645
Epoch 7171/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.750551724433899
Epoch: 7171, Batch Gradient Norm: 30.3614660548155
Epoch: 7171, Batch Gradient Norm after: 22.360680866149895
Epoch 7172/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7541326999664306
Epoch: 7172, Batch Gradient Norm: 28.76187900793808
Epoch: 7172, Batch Gradient Norm after: 22.36068096517243
Epoch 7173/10000, Prediction Accuracy = 60.032%, Loss = 0.7504631757736206
Epoch: 7173, Batch Gradient Norm: 30.356335547125425
Epoch: 7173, Batch Gradient Norm after: 22.36068007029905
Epoch 7174/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.754078722000122
Epoch: 7174, Batch Gradient Norm: 28.759106910103103
Epoch: 7174, Batch Gradient Norm after: 22.36067971217726
Epoch 7175/10000, Prediction Accuracy = 59.996%, Loss = 0.7503744721412658
Epoch: 7175, Batch Gradient Norm: 30.357780831549007
Epoch: 7175, Batch Gradient Norm after: 22.360678504356002
Epoch 7176/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7540568947792053
Epoch: 7176, Batch Gradient Norm: 28.757123263971963
Epoch: 7176, Batch Gradient Norm after: 22.36067987470834
Epoch 7177/10000, Prediction Accuracy = 59.976%, Loss = 0.7504068732261657
Epoch: 7177, Batch Gradient Norm: 30.356828725004252
Epoch: 7177, Batch Gradient Norm after: 22.360678022090557
Epoch 7178/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7541346311569214
Epoch: 7178, Batch Gradient Norm: 28.749982615479016
Epoch: 7178, Batch Gradient Norm after: 22.36067716510039
Epoch 7179/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.7504370927810669
Epoch: 7179, Batch Gradient Norm: 30.35207978994069
Epoch: 7179, Batch Gradient Norm after: 22.360680270267483
Epoch 7180/10000, Prediction Accuracy = 59.988%, Loss = 0.7540335297584534
Epoch: 7180, Batch Gradient Norm: 28.74427527895023
Epoch: 7180, Batch Gradient Norm after: 22.360680177355757
Epoch 7181/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7502501130104064
Epoch: 7181, Batch Gradient Norm: 30.346636284488167
Epoch: 7181, Batch Gradient Norm after: 22.360676912315277
Epoch 7182/10000, Prediction Accuracy = 60.004%, Loss = 0.7538007616996765
Epoch: 7182, Batch Gradient Norm: 28.748458236302003
Epoch: 7182, Batch Gradient Norm after: 22.360679087051352
Epoch 7183/10000, Prediction Accuracy = 60.004%, Loss = 0.7501135110855103
Epoch: 7183, Batch Gradient Norm: 30.343380432754884
Epoch: 7183, Batch Gradient Norm after: 22.360678484531334
Epoch 7184/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7537065625190735
Epoch: 7184, Batch Gradient Norm: 28.753194887638973
Epoch: 7184, Batch Gradient Norm after: 22.360676833686323
Epoch 7185/10000, Prediction Accuracy = 60.038%, Loss = 0.7500443816184997
Epoch: 7185, Batch Gradient Norm: 30.335700660514927
Epoch: 7185, Batch Gradient Norm after: 22.360679330760256
Epoch 7186/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7536444664001465
Epoch: 7186, Batch Gradient Norm: 28.75749026238419
Epoch: 7186, Batch Gradient Norm after: 22.360680650149746
Epoch 7187/10000, Prediction Accuracy = 60.004%, Loss = 0.7499990820884704
Epoch: 7187, Batch Gradient Norm: 30.332197836865266
Epoch: 7187, Batch Gradient Norm after: 22.360679533990755
Epoch 7188/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7536290645599365
Epoch: 7188, Batch Gradient Norm: 28.756034764414935
Epoch: 7188, Batch Gradient Norm after: 22.360677223126284
Epoch 7189/10000, Prediction Accuracy = 59.98%, Loss = 0.7500434637069702
Epoch: 7189, Batch Gradient Norm: 30.328782701128052
Epoch: 7189, Batch Gradient Norm after: 22.36067835569643
Epoch 7190/10000, Prediction Accuracy = 60.004%, Loss = 0.7536905527114868
Epoch: 7190, Batch Gradient Norm: 28.747426802040888
Epoch: 7190, Batch Gradient Norm after: 22.360677617555044
Epoch 7191/10000, Prediction Accuracy = 59.96%, Loss = 0.7500545859336853
Epoch: 7191, Batch Gradient Norm: 30.321233561604263
Epoch: 7191, Batch Gradient Norm after: 22.360676858693843
Epoch 7192/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7535424113273621
Epoch: 7192, Batch Gradient Norm: 28.748809657914624
Epoch: 7192, Batch Gradient Norm after: 22.360677325221907
Epoch 7193/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7498528480529785
Epoch: 7193, Batch Gradient Norm: 30.320866472166035
Epoch: 7193, Batch Gradient Norm after: 22.360676210648812
Epoch 7194/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7533248543739319
Epoch: 7194, Batch Gradient Norm: 28.753312015252234
Epoch: 7194, Batch Gradient Norm after: 22.360678059548583
Epoch 7195/10000, Prediction Accuracy = 60.02%, Loss = 0.7497460961341857
Epoch: 7195, Batch Gradient Norm: 30.31034888899236
Epoch: 7195, Batch Gradient Norm after: 22.360678633011673
Epoch 7196/10000, Prediction Accuracy = 59.996%, Loss = 0.7532595515251159
Epoch: 7196, Batch Gradient Norm: 28.755340080646548
Epoch: 7196, Batch Gradient Norm after: 22.360679914263788
Epoch 7197/10000, Prediction Accuracy = 60.024%, Loss = 0.7496654510498046
Epoch: 7197, Batch Gradient Norm: 30.306570583391398
Epoch: 7197, Batch Gradient Norm after: 22.36067936958315
Epoch 7198/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7531909823417664
Epoch: 7198, Batch Gradient Norm: 28.7587194483167
Epoch: 7198, Batch Gradient Norm after: 22.360677601750965
Epoch 7199/10000, Prediction Accuracy = 59.996%, Loss = 0.7496385455131531
Epoch: 7199, Batch Gradient Norm: 30.30545500023095
Epoch: 7199, Batch Gradient Norm after: 22.36068019299379
Epoch 7200/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.7532144665718079
Epoch: 7200, Batch Gradient Norm: 28.756212695143297
Epoch: 7200, Batch Gradient Norm after: 22.36067670835417
Epoch 7201/10000, Prediction Accuracy = 59.938%, Loss = 0.7497155070304871
Epoch: 7201, Batch Gradient Norm: 30.29740609514273
Epoch: 7201, Batch Gradient Norm after: 22.360679002382632
Epoch 7202/10000, Prediction Accuracy = 60.004%, Loss = 0.7532514572143555
Epoch: 7202, Batch Gradient Norm: 28.750540760820414
Epoch: 7202, Batch Gradient Norm after: 22.360677363947186
Epoch 7203/10000, Prediction Accuracy = 59.976%, Loss = 0.7496543526649475
Epoch: 7203, Batch Gradient Norm: 30.29053208580844
Epoch: 7203, Batch Gradient Norm after: 22.360680249028935
Epoch 7204/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7530346393585206
Epoch: 7204, Batch Gradient Norm: 28.755363039177922
Epoch: 7204, Batch Gradient Norm after: 22.360678484319592
Epoch 7205/10000, Prediction Accuracy = 60.02%, Loss = 0.7494582414627076
Epoch: 7205, Batch Gradient Norm: 30.289450738441797
Epoch: 7205, Batch Gradient Norm after: 22.360679280266577
Epoch 7206/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7528579831123352
Epoch: 7206, Batch Gradient Norm: 28.75820535519983
Epoch: 7206, Batch Gradient Norm after: 22.360679482021165
Epoch 7207/10000, Prediction Accuracy = 60.028%, Loss = 0.74936945438385
Epoch: 7207, Batch Gradient Norm: 30.28189773170375
Epoch: 7207, Batch Gradient Norm after: 22.360677693793686
Epoch 7208/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7528006196022033
Epoch: 7208, Batch Gradient Norm: 28.755812531710404
Epoch: 7208, Batch Gradient Norm after: 22.36067842390006
Epoch 7209/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.749286413192749
Epoch: 7209, Batch Gradient Norm: 30.28108227142494
Epoch: 7209, Batch Gradient Norm after: 22.360677446872938
Epoch 7210/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7527501702308654
Epoch: 7210, Batch Gradient Norm: 28.754299016809952
Epoch: 7210, Batch Gradient Norm after: 22.36067740367967
Epoch 7211/10000, Prediction Accuracy = 59.98%, Loss = 0.7492741584777832
Epoch: 7211, Batch Gradient Norm: 30.282990108398113
Epoch: 7211, Batch Gradient Norm after: 22.360678915174972
Epoch 7212/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.7528006315231324
Epoch: 7212, Batch Gradient Norm: 28.746358285347014
Epoch: 7212, Batch Gradient Norm after: 22.360676890039173
Epoch 7213/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.7493239045143127
Epoch: 7213, Batch Gradient Norm: 30.275586524038474
Epoch: 7213, Batch Gradient Norm after: 22.360677933517753
Epoch 7214/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.752787446975708
Epoch: 7214, Batch Gradient Norm: 28.738696982937764
Epoch: 7214, Batch Gradient Norm after: 22.360678603494478
Epoch 7215/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.749207603931427
Epoch: 7215, Batch Gradient Norm: 30.27270623772732
Epoch: 7215, Batch Gradient Norm after: 22.360680372264696
Epoch 7216/10000, Prediction Accuracy = 60.01800000000001%, Loss = 0.7525673508644104
Epoch: 7216, Batch Gradient Norm: 28.737725048939517
Epoch: 7216, Batch Gradient Norm after: 22.360678378625963
Epoch 7217/10000, Prediction Accuracy = 60.028%, Loss = 0.7490211963653565
Epoch: 7217, Batch Gradient Norm: 30.271508002281823
Epoch: 7217, Batch Gradient Norm after: 22.360677989393103
Epoch 7218/10000, Prediction Accuracy = 60.008%, Loss = 0.7524383306503296
Epoch: 7218, Batch Gradient Norm: 28.73597666486617
Epoch: 7218, Batch Gradient Norm after: 22.360679792156176
Epoch 7219/10000, Prediction Accuracy = 60.036%, Loss = 0.7489399313926697
Epoch: 7219, Batch Gradient Norm: 30.265332035179004
Epoch: 7219, Batch Gradient Norm after: 22.36067772901076
Epoch 7220/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.7523808002471923
Epoch: 7220, Batch Gradient Norm: 28.733486776835896
Epoch: 7220, Batch Gradient Norm after: 22.360678034771738
Epoch 7221/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7488483905792236
Epoch: 7221, Batch Gradient Norm: 30.265101003922886
Epoch: 7221, Batch Gradient Norm after: 22.360679221189223
Epoch 7222/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7523436784744263
Epoch: 7222, Batch Gradient Norm: 28.725808507458748
Epoch: 7222, Batch Gradient Norm after: 22.360677836021786
Epoch 7223/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7488538384437561
Epoch: 7223, Batch Gradient Norm: 30.26814398487292
Epoch: 7223, Batch Gradient Norm after: 22.360677583891665
Epoch 7224/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.7524330019950867
Epoch: 7224, Batch Gradient Norm: 28.71202041238299
Epoch: 7224, Batch Gradient Norm after: 22.360680766292155
Epoch 7225/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7489065408706665
Epoch: 7225, Batch Gradient Norm: 30.261953522199807
Epoch: 7225, Batch Gradient Norm after: 22.36068018286538
Epoch 7226/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7524028301239014
Epoch: 7226, Batch Gradient Norm: 28.69683909318873
Epoch: 7226, Batch Gradient Norm after: 22.36067636423527
Epoch 7227/10000, Prediction Accuracy = 60.004%, Loss = 0.7487176299095154
Epoch: 7227, Batch Gradient Norm: 30.263189349656685
Epoch: 7227, Batch Gradient Norm after: 22.36067842516554
Epoch 7228/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7521572589874268
Epoch: 7228, Batch Gradient Norm: 28.6921584002609
Epoch: 7228, Batch Gradient Norm after: 22.360677623247206
Epoch 7229/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7485264301300049
Epoch: 7229, Batch Gradient Norm: 30.267615407982287
Epoch: 7229, Batch Gradient Norm after: 22.360677658184727
Epoch 7230/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7520588278770447
Epoch: 7230, Batch Gradient Norm: 28.68050924187067
Epoch: 7230, Batch Gradient Norm after: 22.360679270712417
Epoch 7231/10000, Prediction Accuracy = 60.06%, Loss = 0.7484377741813659
Epoch: 7231, Batch Gradient Norm: 30.2627198901251
Epoch: 7231, Batch Gradient Norm after: 22.360677542001515
Epoch 7232/10000, Prediction Accuracy = 59.988%, Loss = 0.7520106792449951
Epoch: 7232, Batch Gradient Norm: 28.67340897090322
Epoch: 7232, Batch Gradient Norm after: 22.360680132680628
Epoch 7233/10000, Prediction Accuracy = 60.0%, Loss = 0.7483413100242615
Epoch: 7233, Batch Gradient Norm: 30.26736645945246
Epoch: 7233, Batch Gradient Norm after: 22.360679989766176
Epoch 7234/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.751994502544403
Epoch: 7234, Batch Gradient Norm: 28.66723503235356
Epoch: 7234, Batch Gradient Norm after: 22.360679060531254
Epoch 7235/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7483654260635376
Epoch: 7235, Batch Gradient Norm: 30.264804432176778
Epoch: 7235, Batch Gradient Norm after: 22.360678427596426
Epoch 7236/10000, Prediction Accuracy = 60.024%, Loss = 0.7520795464515686
Epoch: 7236, Batch Gradient Norm: 28.650002045307385
Epoch: 7236, Batch Gradient Norm after: 22.360677511158535
Epoch 7237/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.7483695149421692
Epoch: 7237, Batch Gradient Norm: 30.261833560117008
Epoch: 7237, Batch Gradient Norm after: 22.360678287984044
Epoch 7238/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7519924879074097
Epoch: 7238, Batch Gradient Norm: 28.638359105116212
Epoch: 7238, Batch Gradient Norm after: 22.360677636051005
Epoch 7239/10000, Prediction Accuracy = 60.016%, Loss = 0.7481766104698181
Epoch: 7239, Batch Gradient Norm: 30.26430332875895
Epoch: 7239, Batch Gradient Norm after: 22.360677750736198
Epoch 7240/10000, Prediction Accuracy = 60.03399999999999%, Loss = 0.7517673134803772
Epoch: 7240, Batch Gradient Norm: 28.635578927566275
Epoch: 7240, Batch Gradient Norm after: 22.36067837255553
Epoch 7241/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7480192899703979
Epoch: 7241, Batch Gradient Norm: 30.263830666941143
Epoch: 7241, Batch Gradient Norm after: 22.360677988429675
Epoch 7242/10000, Prediction Accuracy = 60.0%, Loss = 0.751690948009491
Epoch: 7242, Batch Gradient Norm: 28.62646309246215
Epoch: 7242, Batch Gradient Norm after: 22.360679463089507
Epoch 7243/10000, Prediction Accuracy = 60.048%, Loss = 0.7479329466819763
Epoch: 7243, Batch Gradient Norm: 30.2591961963259
Epoch: 7243, Batch Gradient Norm after: 22.360678247875462
Epoch 7244/10000, Prediction Accuracy = 59.98%, Loss = 0.7516419291496277
Epoch: 7244, Batch Gradient Norm: 28.618483294964616
Epoch: 7244, Batch Gradient Norm after: 22.3606806040699
Epoch 7245/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.747848629951477
Epoch: 7245, Batch Gradient Norm: 30.263823132924227
Epoch: 7245, Batch Gradient Norm after: 22.360679511336176
Epoch 7246/10000, Prediction Accuracy = 60.004%, Loss = 0.7516507267951965
Epoch: 7246, Batch Gradient Norm: 28.613707564286614
Epoch: 7246, Batch Gradient Norm after: 22.36067681902384
Epoch 7247/10000, Prediction Accuracy = 60.0%, Loss = 0.7479020714759826
Epoch: 7247, Batch Gradient Norm: 30.258174179041315
Epoch: 7247, Batch Gradient Norm after: 22.360678032620537
Epoch 7248/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7517295837402344
Epoch: 7248, Batch Gradient Norm: 28.604953077766034
Epoch: 7248, Batch Gradient Norm after: 22.360679122807326
Epoch 7249/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7478975057601929
Epoch: 7249, Batch Gradient Norm: 30.251006894914365
Epoch: 7249, Batch Gradient Norm after: 22.36067896725496
Epoch 7250/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.751563823223114
Epoch: 7250, Batch Gradient Norm: 28.597977217230767
Epoch: 7250, Batch Gradient Norm after: 22.360677354762533
Epoch 7251/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.747678542137146
Epoch: 7251, Batch Gradient Norm: 30.257591976668564
Epoch: 7251, Batch Gradient Norm after: 22.36067915369351
Epoch 7252/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.7513587355613709
Epoch: 7252, Batch Gradient Norm: 28.596912110372266
Epoch: 7252, Batch Gradient Norm after: 22.36067852189742
Epoch 7253/10000, Prediction Accuracy = 60.056%, Loss = 0.7475532174110413
Epoch: 7253, Batch Gradient Norm: 30.251464167116122
Epoch: 7253, Batch Gradient Norm after: 22.360677575326406
Epoch 7254/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7513008952140808
Epoch: 7254, Batch Gradient Norm: 28.59415652650741
Epoch: 7254, Batch Gradient Norm after: 22.36067891719972
Epoch 7255/10000, Prediction Accuracy = 60.062%, Loss = 0.7474712491035461
Epoch: 7255, Batch Gradient Norm: 30.245997679520713
Epoch: 7255, Batch Gradient Norm after: 22.36067859253299
Epoch 7256/10000, Prediction Accuracy = 60.012000000000015%, Loss = 0.7512383460998535
Epoch: 7256, Batch Gradient Norm: 28.593463679314187
Epoch: 7256, Batch Gradient Norm after: 22.3606771637388
Epoch 7257/10000, Prediction Accuracy = 60.038%, Loss = 0.7474357128143311
Epoch: 7257, Batch Gradient Norm: 30.244494892527513
Epoch: 7257, Batch Gradient Norm after: 22.360679705621042
Epoch 7258/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7512766242027282
Epoch: 7258, Batch Gradient Norm: 28.5897533162998
Epoch: 7258, Batch Gradient Norm after: 22.36067829694335
Epoch 7259/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7475079655647278
Epoch: 7259, Batch Gradient Norm: 30.237934128861937
Epoch: 7259, Batch Gradient Norm after: 22.360678775784724
Epoch 7260/10000, Prediction Accuracy = 60.024%, Loss = 0.7513142943382263
Epoch: 7260, Batch Gradient Norm: 28.583058682223157
Epoch: 7260, Batch Gradient Norm after: 22.36067824939557
Epoch 7261/10000, Prediction Accuracy = 59.96%, Loss = 0.7474363446235657
Epoch: 7261, Batch Gradient Norm: 30.23410288664047
Epoch: 7261, Batch Gradient Norm after: 22.360681466252693
Epoch 7262/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.7510924100875854
Epoch: 7262, Batch Gradient Norm: 28.580169503798967
Epoch: 7262, Batch Gradient Norm after: 22.36067720079554
Epoch 7263/10000, Prediction Accuracy = 60.02%, Loss = 0.7472276449203491
Epoch: 7263, Batch Gradient Norm: 30.2354257964816
Epoch: 7263, Batch Gradient Norm after: 22.360677900914386
Epoch 7264/10000, Prediction Accuracy = 60.012%, Loss = 0.7509331345558167
Epoch: 7264, Batch Gradient Norm: 28.57894471803557
Epoch: 7264, Batch Gradient Norm after: 22.360679805843258
Epoch 7265/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.747129476070404
Epoch: 7265, Batch Gradient Norm: 30.229222752623762
Epoch: 7265, Batch Gradient Norm after: 22.360678217659498
Epoch 7266/10000, Prediction Accuracy = 59.984%, Loss = 0.7508798599243164
Epoch: 7266, Batch Gradient Norm: 28.57633487122178
Epoch: 7266, Batch Gradient Norm after: 22.36067880443557
Epoch 7267/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7470408797264099
Epoch: 7267, Batch Gradient Norm: 30.22550799686763
Epoch: 7267, Batch Gradient Norm after: 22.360679512161305
Epoch 7268/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7508330821990967
Epoch: 7268, Batch Gradient Norm: 28.57399882944756
Epoch: 7268, Batch Gradient Norm after: 22.36067859241132
Epoch 7269/10000, Prediction Accuracy = 60.008%, Loss = 0.7470351099967957
Epoch: 7269, Batch Gradient Norm: 30.227916785329043
Epoch: 7269, Batch Gradient Norm after: 22.360678402752782
Epoch 7270/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.750895619392395
Epoch: 7270, Batch Gradient Norm: 28.56803669443557
Epoch: 7270, Batch Gradient Norm after: 22.360678160188336
Epoch 7271/10000, Prediction Accuracy = 59.972%, Loss = 0.7470898032188416
Epoch: 7271, Batch Gradient Norm: 30.21834346741142
Epoch: 7271, Batch Gradient Norm after: 22.360679446158546
Epoch 7272/10000, Prediction Accuracy = 60.03399999999999%, Loss = 0.750875985622406
Epoch: 7272, Batch Gradient Norm: 28.560493836311657
Epoch: 7272, Batch Gradient Norm after: 22.36067854955577
Epoch 7273/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7469622969627381
Epoch: 7273, Batch Gradient Norm: 30.219730250613715
Epoch: 7273, Batch Gradient Norm after: 22.360678778725173
Epoch 7274/10000, Prediction Accuracy = 60.024%, Loss = 0.7506508231163025
Epoch: 7274, Batch Gradient Norm: 28.55975015128973
Epoch: 7274, Batch Gradient Norm after: 22.36067823434358
Epoch 7275/10000, Prediction Accuracy = 60.038%, Loss = 0.7467797636985779
Epoch: 7275, Batch Gradient Norm: 30.216831883676484
Epoch: 7275, Batch Gradient Norm after: 22.360678330919068
Epoch 7276/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.7505213141441345
Epoch: 7276, Batch Gradient Norm: 28.559770675581255
Epoch: 7276, Batch Gradient Norm after: 22.360679652966272
Epoch 7277/10000, Prediction Accuracy = 60.062%, Loss = 0.746699845790863
Epoch: 7277, Batch Gradient Norm: 30.209447072521524
Epoch: 7277, Batch Gradient Norm after: 22.360678400909435
Epoch 7278/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.7504691481590271
Epoch: 7278, Batch Gradient Norm: 28.55659062165043
Epoch: 7278, Batch Gradient Norm after: 22.360679020468066
Epoch 7279/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7466062664985657
Epoch: 7279, Batch Gradient Norm: 30.2101996452248
Epoch: 7279, Batch Gradient Norm after: 22.360680489768008
Epoch 7280/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7504342675209046
Epoch: 7280, Batch Gradient Norm: 28.55591847390418
Epoch: 7280, Batch Gradient Norm after: 22.360678161827472
Epoch 7281/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7466415166854858
Epoch: 7281, Batch Gradient Norm: 30.20859256640495
Epoch: 7281, Batch Gradient Norm after: 22.36067942466196
Epoch 7282/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.7505283713340759
Epoch: 7282, Batch Gradient Norm: 28.54648776549236
Epoch: 7282, Batch Gradient Norm after: 22.360680715939278
Epoch 7283/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7466921925544738
Epoch: 7283, Batch Gradient Norm: 30.2036073472409
Epoch: 7283, Batch Gradient Norm after: 22.360679620767087
Epoch 7284/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.750442385673523
Epoch: 7284, Batch Gradient Norm: 28.543084184401234
Epoch: 7284, Batch Gradient Norm after: 22.36067864540632
Epoch 7285/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.746500289440155
Epoch: 7285, Batch Gradient Norm: 30.20121803155205
Epoch: 7285, Batch Gradient Norm after: 22.360677901139134
Epoch 7286/10000, Prediction Accuracy = 60.036%, Loss = 0.7502024292945861
Epoch: 7286, Batch Gradient Norm: 28.545342415622837
Epoch: 7286, Batch Gradient Norm after: 22.360676461869275
Epoch 7287/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7463494896888733
Epoch: 7287, Batch Gradient Norm: 30.198407228851785
Epoch: 7287, Batch Gradient Norm after: 22.360678284282567
Epoch 7288/10000, Prediction Accuracy = 59.992%, Loss = 0.7501080155372619
Epoch: 7288, Batch Gradient Norm: 28.54373038716794
Epoch: 7288, Batch Gradient Norm after: 22.360677054291052
Epoch 7289/10000, Prediction Accuracy = 60.056%, Loss = 0.7462762594223022
Epoch: 7289, Batch Gradient Norm: 30.191385444289526
Epoch: 7289, Batch Gradient Norm after: 22.36067927403402
Epoch 7290/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7500506162643432
Epoch: 7290, Batch Gradient Norm: 28.5382829257192
Epoch: 7290, Batch Gradient Norm after: 22.360674842929818
Epoch 7291/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7462061405181885
Epoch: 7291, Batch Gradient Norm: 30.191929010987177
Epoch: 7291, Batch Gradient Norm after: 22.36068111031371
Epoch 7292/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7500488996505738
Epoch: 7292, Batch Gradient Norm: 28.538343150708375
Epoch: 7292, Batch Gradient Norm after: 22.360679381039024
Epoch 7293/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7462632060050964
Epoch: 7293, Batch Gradient Norm: 30.187766461732203
Epoch: 7293, Batch Gradient Norm after: 22.360679132208666
Epoch 7294/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.7501252055168152
Epoch: 7294, Batch Gradient Norm: 28.529566898306214
Epoch: 7294, Batch Gradient Norm after: 22.360677351609244
Epoch 7295/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7462606072425843
Epoch: 7295, Batch Gradient Norm: 30.182359460164612
Epoch: 7295, Batch Gradient Norm after: 22.360679716968736
Epoch 7296/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.749963641166687
Epoch: 7296, Batch Gradient Norm: 28.528377390734054
Epoch: 7296, Batch Gradient Norm after: 22.360677851273607
Epoch 7297/10000, Prediction Accuracy = 60.04%, Loss = 0.7460599422454834
Epoch: 7297, Batch Gradient Norm: 30.180849199125834
Epoch: 7297, Batch Gradient Norm after: 22.360679090034363
Epoch 7298/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7497581720352173
Epoch: 7298, Batch Gradient Norm: 28.534873239953527
Epoch: 7298, Batch Gradient Norm after: 22.360676884578467
Epoch 7299/10000, Prediction Accuracy = 60.062%, Loss = 0.7459584832191467
Epoch: 7299, Batch Gradient Norm: 30.17100322380166
Epoch: 7299, Batch Gradient Norm after: 22.36068094701327
Epoch 7300/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7496720314025879
Epoch: 7300, Batch Gradient Norm: 28.535329925307256
Epoch: 7300, Batch Gradient Norm after: 22.360676898921426
Epoch 7301/10000, Prediction Accuracy = 60.04599999999999%, Loss = 0.7458848118782043
Epoch: 7301, Batch Gradient Norm: 30.166807399716035
Epoch: 7301, Batch Gradient Norm after: 22.36067822878886
Epoch 7302/10000, Prediction Accuracy = 59.996%, Loss = 0.7496135473251343
Epoch: 7302, Batch Gradient Norm: 28.53131528430389
Epoch: 7302, Batch Gradient Norm after: 22.36067882017134
Epoch 7303/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7458292245864868
Epoch: 7303, Batch Gradient Norm: 30.16961726417913
Epoch: 7303, Batch Gradient Norm after: 22.360679065744165
Epoch 7304/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.7496363401412964
Epoch: 7304, Batch Gradient Norm: 28.52745169646886
Epoch: 7304, Batch Gradient Norm after: 22.36068018508615
Epoch 7305/10000, Prediction Accuracy = 59.988%, Loss = 0.7459025263786316
Epoch: 7305, Batch Gradient Norm: 30.16189943874309
Epoch: 7305, Batch Gradient Norm after: 22.36067851521756
Epoch 7306/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7497012972831726
Epoch: 7306, Batch Gradient Norm: 28.519866502987917
Epoch: 7306, Batch Gradient Norm after: 22.360677735676283
Epoch 7307/10000, Prediction Accuracy = 60.0%, Loss = 0.7458575129508972
Epoch: 7307, Batch Gradient Norm: 30.158048117804935
Epoch: 7307, Batch Gradient Norm after: 22.36067937371886
Epoch 7308/10000, Prediction Accuracy = 60.06%, Loss = 0.7494947075843811
Epoch: 7308, Batch Gradient Norm: 28.520298303472064
Epoch: 7308, Batch Gradient Norm after: 22.36067733822789
Epoch 7309/10000, Prediction Accuracy = 60.038%, Loss = 0.7456531763076782
Epoch: 7309, Batch Gradient Norm: 30.15813355672084
Epoch: 7309, Batch Gradient Norm after: 22.36068022672549
Epoch 7310/10000, Prediction Accuracy = 60.004%, Loss = 0.7493120551109314
Epoch: 7310, Batch Gradient Norm: 28.525506983071082
Epoch: 7310, Batch Gradient Norm after: 22.360676992231053
Epoch 7311/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7455725193023681
Epoch: 7311, Batch Gradient Norm: 30.145916791907425
Epoch: 7311, Batch Gradient Norm after: 22.360676544029978
Epoch 7312/10000, Prediction Accuracy = 60.012%, Loss = 0.7492441654205322
Epoch: 7312, Batch Gradient Norm: 28.527675055777593
Epoch: 7312, Batch Gradient Norm after: 22.360677173133716
Epoch 7313/10000, Prediction Accuracy = 60.064%, Loss = 0.745490562915802
Epoch: 7313, Batch Gradient Norm: 30.14113453371793
Epoch: 7313, Batch Gradient Norm after: 22.360678303762313
Epoch 7314/10000, Prediction Accuracy = 60.028%, Loss = 0.7491893291473388
Epoch: 7314, Batch Gradient Norm: 28.523099994624403
Epoch: 7314, Batch Gradient Norm after: 22.360677916670355
Epoch 7315/10000, Prediction Accuracy = 60.024%, Loss = 0.7454639554023743
Epoch: 7315, Batch Gradient Norm: 30.142599418151732
Epoch: 7315, Batch Gradient Norm after: 22.36067892222652
Epoch 7316/10000, Prediction Accuracy = 60.048%, Loss = 0.7492387056350708
Epoch: 7316, Batch Gradient Norm: 28.518652207178114
Epoch: 7316, Batch Gradient Norm after: 22.36067773672706
Epoch 7317/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7455377697944641
Epoch: 7317, Batch Gradient Norm: 30.134690907200085
Epoch: 7317, Batch Gradient Norm after: 22.36067807157713
Epoch 7318/10000, Prediction Accuracy = 60.038%, Loss = 0.7492485642433167
Epoch: 7318, Batch Gradient Norm: 28.5141986257817
Epoch: 7318, Batch Gradient Norm after: 22.36067674918872
Epoch 7319/10000, Prediction Accuracy = 59.992%, Loss = 0.7454378366470337
Epoch: 7319, Batch Gradient Norm: 30.13115660365908
Epoch: 7319, Batch Gradient Norm after: 22.360678076874876
Epoch 7320/10000, Prediction Accuracy = 60.072%, Loss = 0.7490265250205994
Epoch: 7320, Batch Gradient Norm: 28.515762815784242
Epoch: 7320, Batch Gradient Norm after: 22.360677027976543
Epoch 7321/10000, Prediction Accuracy = 60.038%, Loss = 0.7452587604522705
Epoch: 7321, Batch Gradient Norm: 30.131156112990116
Epoch: 7321, Batch Gradient Norm after: 22.360678222197002
Epoch 7322/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7488673686981201
Epoch: 7322, Batch Gradient Norm: 28.519189196170604
Epoch: 7322, Batch Gradient Norm after: 22.360677458997845
Epoch 7323/10000, Prediction Accuracy = 60.09000000000001%, Loss = 0.7451822757720947
Epoch: 7323, Batch Gradient Norm: 30.12198566954997
Epoch: 7323, Batch Gradient Norm after: 22.360678144952633
Epoch 7324/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7488090038299561
Epoch: 7324, Batch Gradient Norm: 28.514333926702495
Epoch: 7324, Batch Gradient Norm after: 22.360677037640755
Epoch 7325/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7450945138931274
Epoch: 7325, Batch Gradient Norm: 30.119550575330273
Epoch: 7325, Batch Gradient Norm after: 22.360676587471445
Epoch 7326/10000, Prediction Accuracy = 60.00999999999999%, Loss = 0.7487754106521607
Epoch: 7326, Batch Gradient Norm: 28.51181302389899
Epoch: 7326, Batch Gradient Norm after: 22.36067629631687
Epoch 7327/10000, Prediction Accuracy = 60.004%, Loss = 0.7450939774513244
Epoch: 7327, Batch Gradient Norm: 30.119704907683566
Epoch: 7327, Batch Gradient Norm after: 22.360676552493434
Epoch 7328/10000, Prediction Accuracy = 60.072%, Loss = 0.7488596916198731
Epoch: 7328, Batch Gradient Norm: 28.502429193263197
Epoch: 7328, Batch Gradient Norm after: 22.360678016016912
Epoch 7329/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.745160448551178
Epoch: 7329, Batch Gradient Norm: 30.11209235095227
Epoch: 7329, Batch Gradient Norm after: 22.36067847238708
Epoch 7330/10000, Prediction Accuracy = 60.048%, Loss = 0.7488152623176575
Epoch: 7330, Batch Gradient Norm: 28.500277353636854
Epoch: 7330, Batch Gradient Norm after: 22.360678281182587
Epoch 7331/10000, Prediction Accuracy = 60.04%, Loss = 0.7449958562850952
Epoch: 7331, Batch Gradient Norm: 30.112911031716333
Epoch: 7331, Batch Gradient Norm after: 22.360675932678205
Epoch 7332/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7485638499259949
Epoch: 7332, Batch Gradient Norm: 28.50015350469177
Epoch: 7332, Batch Gradient Norm after: 22.360677501547606
Epoch 7333/10000, Prediction Accuracy = 60.04%, Loss = 0.7448318481445313
Epoch: 7333, Batch Gradient Norm: 30.109681382478584
Epoch: 7333, Batch Gradient Norm after: 22.36067808679928
Epoch 7334/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7484585762023925
Epoch: 7334, Batch Gradient Norm: 28.49975581937525
Epoch: 7334, Batch Gradient Norm after: 22.360675898170015
Epoch 7335/10000, Prediction Accuracy = 60.092%, Loss = 0.7447673797607421
Epoch: 7335, Batch Gradient Norm: 30.104057515748902
Epoch: 7335, Batch Gradient Norm after: 22.3606767110258
Epoch 7336/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.748398756980896
Epoch: 7336, Batch Gradient Norm: 28.496995748139636
Epoch: 7336, Batch Gradient Norm after: 22.360677531007145
Epoch 7337/10000, Prediction Accuracy = 60.052%, Loss = 0.7446808695793152
Epoch: 7337, Batch Gradient Norm: 30.103459544436365
Epoch: 7337, Batch Gradient Norm after: 22.360677081782917
Epoch 7338/10000, Prediction Accuracy = 60.038%, Loss = 0.7483860611915588
Epoch: 7338, Batch Gradient Norm: 28.49485793827256
Epoch: 7338, Batch Gradient Norm after: 22.360676882332548
Epoch 7339/10000, Prediction Accuracy = 60.008%, Loss = 0.7447206377983093
Epoch: 7339, Batch Gradient Norm: 30.099535868820322
Epoch: 7339, Batch Gradient Norm after: 22.360679597375075
Epoch 7340/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7484710097312928
Epoch: 7340, Batch Gradient Norm: 28.489692849047845
Epoch: 7340, Batch Gradient Norm after: 22.36067683137629
Epoch 7341/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.744756531715393
Epoch: 7341, Batch Gradient Norm: 30.09080252562178
Epoch: 7341, Batch Gradient Norm after: 22.360678950290325
Epoch 7342/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7483451724052429
Epoch: 7342, Batch Gradient Norm: 28.490946852814066
Epoch: 7342, Batch Gradient Norm after: 22.360678456530888
Epoch 7343/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7445783138275146
Epoch: 7343, Batch Gradient Norm: 30.089591664512962
Epoch: 7343, Batch Gradient Norm after: 22.36067690107801
Epoch 7344/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7481069445610047
Epoch: 7344, Batch Gradient Norm: 28.499282443279444
Epoch: 7344, Batch Gradient Norm after: 22.36067793203638
Epoch 7345/10000, Prediction Accuracy = 60.072%, Loss = 0.7444579243659973
Epoch: 7345, Batch Gradient Norm: 30.083275838596766
Epoch: 7345, Batch Gradient Norm after: 22.360678231239728
Epoch 7346/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7480217695236206
Epoch: 7346, Batch Gradient Norm: 28.498702604684528
Epoch: 7346, Batch Gradient Norm after: 22.360680038967214
Epoch 7347/10000, Prediction Accuracy = 60.092%, Loss = 0.7443892478942871
Epoch: 7347, Batch Gradient Norm: 30.074764902089136
Epoch: 7347, Batch Gradient Norm after: 22.36067777972784
Epoch 7348/10000, Prediction Accuracy = 60.024%, Loss = 0.7479594945907593
Epoch: 7348, Batch Gradient Norm: 28.501155226601163
Epoch: 7348, Batch Gradient Norm after: 22.36067713616278
Epoch 7349/10000, Prediction Accuracy = 60.064%, Loss = 0.7443336963653564
Epoch: 7349, Batch Gradient Norm: 30.072885197980984
Epoch: 7349, Batch Gradient Norm after: 22.360677620637098
Epoch 7350/10000, Prediction Accuracy = 60.052%, Loss = 0.7479670643806458
Epoch: 7350, Batch Gradient Norm: 28.499904577968262
Epoch: 7350, Batch Gradient Norm after: 22.36067798945209
Epoch 7351/10000, Prediction Accuracy = 60.016%, Loss = 0.7443986535072327
Epoch: 7351, Batch Gradient Norm: 30.06946197031988
Epoch: 7351, Batch Gradient Norm after: 22.36067796576735
Epoch 7352/10000, Prediction Accuracy = 60.056%, Loss = 0.7480294108390808
Epoch: 7352, Batch Gradient Norm: 28.489474579610093
Epoch: 7352, Batch Gradient Norm after: 22.36067730818577
Epoch 7353/10000, Prediction Accuracy = 60.004%, Loss = 0.7443630814552307
Epoch: 7353, Batch Gradient Norm: 30.064233276568864
Epoch: 7353, Batch Gradient Norm after: 22.360679090265233
Epoch 7354/10000, Prediction Accuracy = 60.068%, Loss = 0.7478550553321839
Epoch: 7354, Batch Gradient Norm: 28.488906054424785
Epoch: 7354, Batch Gradient Norm after: 22.360678011721863
Epoch 7355/10000, Prediction Accuracy = 60.04%, Loss = 0.7441724061965942
Epoch: 7355, Batch Gradient Norm: 30.064671616846244
Epoch: 7355, Batch Gradient Norm after: 22.360676824286628
Epoch 7356/10000, Prediction Accuracy = 60.012%, Loss = 0.7476638436317444
Epoch: 7356, Batch Gradient Norm: 28.49138650389113
Epoch: 7356, Batch Gradient Norm after: 22.360678872205323
Epoch 7357/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7440664410591126
Epoch: 7357, Batch Gradient Norm: 30.059075808212487
Epoch: 7357, Batch Gradient Norm after: 22.360678260125248
Epoch 7358/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7476071357727051
Epoch: 7358, Batch Gradient Norm: 28.48689396105279
Epoch: 7358, Batch Gradient Norm after: 22.360679901357127
Epoch 7359/10000, Prediction Accuracy = 60.084%, Loss = 0.7439920663833618
Epoch: 7359, Batch Gradient Norm: 30.052567054252034
Epoch: 7359, Batch Gradient Norm after: 22.360679433499826
Epoch 7360/10000, Prediction Accuracy = 60.028%, Loss = 0.7475482583045959
Epoch: 7360, Batch Gradient Norm: 28.485602065225407
Epoch: 7360, Batch Gradient Norm after: 22.360677408274473
Epoch 7361/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7439544916152954
Epoch: 7361, Batch Gradient Norm: 30.05214104816447
Epoch: 7361, Batch Gradient Norm after: 22.360678974132785
Epoch 7362/10000, Prediction Accuracy = 60.06%, Loss = 0.7475787281990052
Epoch: 7362, Batch Gradient Norm: 28.481924211468044
Epoch: 7362, Batch Gradient Norm after: 22.36067680773162
Epoch 7363/10000, Prediction Accuracy = 59.988%, Loss = 0.7440340399742127
Epoch: 7363, Batch Gradient Norm: 30.046214096228184
Epoch: 7363, Batch Gradient Norm after: 22.36068049455284
Epoch 7364/10000, Prediction Accuracy = 60.040000000000006%, Loss = 0.7476170897483826
Epoch: 7364, Batch Gradient Norm: 28.473879272975807
Epoch: 7364, Batch Gradient Norm after: 22.360676948310854
Epoch 7365/10000, Prediction Accuracy = 59.996%, Loss = 0.743949830532074
Epoch: 7365, Batch Gradient Norm: 30.040466412262997
Epoch: 7365, Batch Gradient Norm after: 22.36067599225803
Epoch 7366/10000, Prediction Accuracy = 60.092%, Loss = 0.7474063515663147
Epoch: 7366, Batch Gradient Norm: 28.477637629881986
Epoch: 7366, Batch Gradient Norm after: 22.360677258801424
Epoch 7367/10000, Prediction Accuracy = 60.048%, Loss = 0.7437750458717346
Epoch: 7367, Batch Gradient Norm: 30.039334875748953
Epoch: 7367, Batch Gradient Norm after: 22.360678457625987
Epoch 7368/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7472379088401795
Epoch: 7368, Batch Gradient Norm: 28.480042733190466
Epoch: 7368, Batch Gradient Norm after: 22.36067837567877
Epoch 7369/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7436867713928222
Epoch: 7369, Batch Gradient Norm: 30.03158437545141
Epoch: 7369, Batch Gradient Norm after: 22.36067922842308
Epoch 7370/10000, Prediction Accuracy = 60.02%, Loss = 0.7471819281578064
Epoch: 7370, Batch Gradient Norm: 28.473440287780814
Epoch: 7370, Batch Gradient Norm after: 22.36067838553079
Epoch 7371/10000, Prediction Accuracy = 60.086%, Loss = 0.7435968279838562
Epoch: 7371, Batch Gradient Norm: 30.029149263829304
Epoch: 7371, Batch Gradient Norm after: 22.360679414735575
Epoch 7372/10000, Prediction Accuracy = 60.048%, Loss = 0.7471340298652649
Epoch: 7372, Batch Gradient Norm: 28.468979653524613
Epoch: 7372, Batch Gradient Norm after: 22.36067809682077
Epoch 7373/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7435622692108155
Epoch: 7373, Batch Gradient Norm: 30.02974862266146
Epoch: 7373, Batch Gradient Norm after: 22.360677931143577
Epoch 7374/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.747206175327301
Epoch: 7374, Batch Gradient Norm: 28.461804564488844
Epoch: 7374, Batch Gradient Norm after: 22.36067619766304
Epoch 7375/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7436564445495606
Epoch: 7375, Batch Gradient Norm: 30.02442016258294
Epoch: 7375, Batch Gradient Norm after: 22.360678404158918
Epoch 7376/10000, Prediction Accuracy = 60.05%, Loss = 0.7472035050392151
Epoch: 7376, Batch Gradient Norm: 28.45195051061852
Epoch: 7376, Batch Gradient Norm after: 22.360676056122678
Epoch 7377/10000, Prediction Accuracy = 59.995999999999995%, Loss = 0.7435110211372375
Epoch: 7377, Batch Gradient Norm: 30.02247986193822
Epoch: 7377, Batch Gradient Norm after: 22.360679872892838
Epoch 7378/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7469609618186951
Epoch: 7378, Batch Gradient Norm: 28.454457274508975
Epoch: 7378, Batch Gradient Norm after: 22.360677115603675
Epoch 7379/10000, Prediction Accuracy = 60.056000000000004%, Loss = 0.7433371067047119
Epoch: 7379, Batch Gradient Norm: 30.023654444920375
Epoch: 7379, Batch Gradient Norm after: 22.360678660882733
Epoch 7380/10000, Prediction Accuracy = 60.044%, Loss = 0.7468246221542358
Epoch: 7380, Batch Gradient Norm: 28.45474930598252
Epoch: 7380, Batch Gradient Norm after: 22.360679090394907
Epoch 7381/10000, Prediction Accuracy = 60.098%, Loss = 0.7432708740234375
Epoch: 7381, Batch Gradient Norm: 30.016560304542597
Epoch: 7381, Batch Gradient Norm after: 22.36067890825542
Epoch 7382/10000, Prediction Accuracy = 60.00999999999999%, Loss = 0.7467724204063415
Epoch: 7382, Batch Gradient Norm: 28.452628667452426
Epoch: 7382, Batch Gradient Norm after: 22.360678021008948
Epoch 7383/10000, Prediction Accuracy = 60.062%, Loss = 0.74317946434021
Epoch: 7383, Batch Gradient Norm: 30.013669405682233
Epoch: 7383, Batch Gradient Norm after: 22.360678130379004
Epoch 7384/10000, Prediction Accuracy = 60.02%, Loss = 0.7467458248138428
Epoch: 7384, Batch Gradient Norm: 28.450772252391914
Epoch: 7384, Batch Gradient Norm after: 22.360678106272857
Epoch 7385/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.7431997537612915
Epoch: 7385, Batch Gradient Norm: 30.01198979958016
Epoch: 7385, Batch Gradient Norm after: 22.360677644275306
Epoch 7386/10000, Prediction Accuracy = 60.062%, Loss = 0.7468394279479981
Epoch: 7386, Batch Gradient Norm: 28.44429992365766
Epoch: 7386, Batch Gradient Norm after: 22.36067832790719
Epoch 7387/10000, Prediction Accuracy = 60.0%, Loss = 0.7432497382164002
Epoch: 7387, Batch Gradient Norm: 30.003219698353025
Epoch: 7387, Batch Gradient Norm after: 22.360677155339733
Epoch 7388/10000, Prediction Accuracy = 60.06%, Loss = 0.7467319130897522
Epoch: 7388, Batch Gradient Norm: 28.44084874965903
Epoch: 7388, Batch Gradient Norm after: 22.360676933768993
Epoch 7389/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7430679202079773
Epoch: 7389, Batch Gradient Norm: 30.003294120384027
Epoch: 7389, Batch Gradient Norm after: 22.360679190197764
Epoch 7390/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7465022921562194
Epoch: 7390, Batch Gradient Norm: 28.445663252579802
Epoch: 7390, Batch Gradient Norm after: 22.360675020470406
Epoch 7391/10000, Prediction Accuracy = 60.098%, Loss = 0.7429335832595825
Epoch: 7391, Batch Gradient Norm: 29.999173107738212
Epoch: 7391, Batch Gradient Norm after: 22.360680801461
Epoch 7392/10000, Prediction Accuracy = 60.062%, Loss = 0.7464119553565979
Epoch: 7392, Batch Gradient Norm: 28.443022688484078
Epoch: 7392, Batch Gradient Norm after: 22.36067853633386
Epoch 7393/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7428649187088012
Epoch: 7393, Batch Gradient Norm: 29.99387495647918
Epoch: 7393, Batch Gradient Norm after: 22.360677446342024
Epoch 7394/10000, Prediction Accuracy = 60.038%, Loss = 0.7463539123535157
Epoch: 7394, Batch Gradient Norm: 28.44003900344722
Epoch: 7394, Batch Gradient Norm after: 22.360677227520814
Epoch 7395/10000, Prediction Accuracy = 60.062%, Loss = 0.7427924633026123
Epoch: 7395, Batch Gradient Norm: 29.992111471757898
Epoch: 7395, Batch Gradient Norm after: 22.360679496918006
Epoch 7396/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7463529348373413
Epoch: 7396, Batch Gradient Norm: 28.441612747416627
Epoch: 7396, Batch Gradient Norm after: 22.3606774302583
Epoch 7397/10000, Prediction Accuracy = 60.008%, Loss = 0.7428599953651428
Epoch: 7397, Batch Gradient Norm: 29.988620492050362
Epoch: 7397, Batch Gradient Norm after: 22.360678058018166
Epoch 7398/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.7464389562606811
Epoch: 7398, Batch Gradient Norm: 28.435055491492076
Epoch: 7398, Batch Gradient Norm after: 22.36067806697694
Epoch 7399/10000, Prediction Accuracy = 59.992%, Loss = 0.7428556084632874
Epoch: 7399, Batch Gradient Norm: 29.97935949144954
Epoch: 7399, Batch Gradient Norm after: 22.360679565436648
Epoch 7400/10000, Prediction Accuracy = 60.072%, Loss = 0.7462631344795227
Epoch: 7400, Batch Gradient Norm: 28.43354000827628
Epoch: 7400, Batch Gradient Norm after: 22.36067808105277
Epoch 7401/10000, Prediction Accuracy = 60.052%, Loss = 0.7426612734794616
Epoch: 7401, Batch Gradient Norm: 29.978963836584605
Epoch: 7401, Batch Gradient Norm after: 22.360677249520492
Epoch 7402/10000, Prediction Accuracy = 60.044%, Loss = 0.7460635662078857
Epoch: 7402, Batch Gradient Norm: 28.43778719091594
Epoch: 7402, Batch Gradient Norm after: 22.360677592222594
Epoch 7403/10000, Prediction Accuracy = 60.1%, Loss = 0.7425505995750428
Epoch: 7403, Batch Gradient Norm: 29.9752828279189
Epoch: 7403, Batch Gradient Norm after: 22.36067869615891
Epoch 7404/10000, Prediction Accuracy = 60.056%, Loss = 0.7459905624389649
Epoch: 7404, Batch Gradient Norm: 28.431694215549584
Epoch: 7404, Batch Gradient Norm after: 22.36067952459893
Epoch 7405/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7424662590026856
Epoch: 7405, Batch Gradient Norm: 29.972666336748453
Epoch: 7405, Batch Gradient Norm after: 22.360679836225284
Epoch 7406/10000, Prediction Accuracy = 60.052%, Loss = 0.7459436178207397
Epoch: 7406, Batch Gradient Norm: 28.425514402837123
Epoch: 7406, Batch Gradient Norm after: 22.360677967286772
Epoch 7407/10000, Prediction Accuracy = 60.048%, Loss = 0.7424010276794434
Epoch: 7407, Batch Gradient Norm: 29.975093625215408
Epoch: 7407, Batch Gradient Norm after: 22.36067861669301
Epoch 7408/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.745979368686676
Epoch: 7408, Batch Gradient Norm: 28.42143313539002
Epoch: 7408, Batch Gradient Norm after: 22.36068049039091
Epoch 7409/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7424826860427857
Epoch: 7409, Batch Gradient Norm: 29.9695471171976
Epoch: 7409, Batch Gradient Norm after: 22.360677817349675
Epoch 7410/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.746039879322052
Epoch: 7410, Batch Gradient Norm: 28.409857914814843
Epoch: 7410, Batch Gradient Norm after: 22.360677387191426
Epoch 7411/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7424015998840332
Epoch: 7411, Batch Gradient Norm: 29.96544472186704
Epoch: 7411, Batch Gradient Norm after: 22.36067735356732
Epoch 7412/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7458401918411255
Epoch: 7412, Batch Gradient Norm: 28.408878313950858
Epoch: 7412, Batch Gradient Norm after: 22.36067832674405
Epoch 7413/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7422099471092224
Epoch: 7413, Batch Gradient Norm: 29.964691574458183
Epoch: 7413, Batch Gradient Norm after: 22.360680525090043
Epoch 7414/10000, Prediction Accuracy = 60.03000000000001%, Loss = 0.7456566452980041
Epoch: 7414, Batch Gradient Norm: 28.413253449002024
Epoch: 7414, Batch Gradient Norm after: 22.360679761630124
Epoch 7415/10000, Prediction Accuracy = 60.104%, Loss = 0.7421267032623291
Epoch: 7415, Batch Gradient Norm: 29.96003410031022
Epoch: 7415, Batch Gradient Norm after: 22.360678547594855
Epoch 7416/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7456014037132264
Epoch: 7416, Batch Gradient Norm: 28.409045653010295
Epoch: 7416, Batch Gradient Norm after: 22.36067854032138
Epoch 7417/10000, Prediction Accuracy = 60.088%, Loss = 0.7420425295829773
Epoch: 7417, Batch Gradient Norm: 29.95483024583002
Epoch: 7417, Batch Gradient Norm after: 22.360676619870798
Epoch 7418/10000, Prediction Accuracy = 60.062%, Loss = 0.7455498695373535
Epoch: 7418, Batch Gradient Norm: 28.406357364453992
Epoch: 7418, Batch Gradient Norm after: 22.360678811430752
Epoch 7419/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7420048356056214
Epoch: 7419, Batch Gradient Norm: 29.955259559934714
Epoch: 7419, Batch Gradient Norm after: 22.360678187313635
Epoch 7420/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7456200957298279
Epoch: 7420, Batch Gradient Norm: 28.39809699523514
Epoch: 7420, Batch Gradient Norm after: 22.360679675775938
Epoch 7421/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7420846819877625
Epoch: 7421, Batch Gradient Norm: 29.950682519637876
Epoch: 7421, Batch Gradient Norm after: 22.360676042417218
Epoch 7422/10000, Prediction Accuracy = 60.07800000000001%, Loss = 0.7456152081489563
Epoch: 7422, Batch Gradient Norm: 28.392184381729116
Epoch: 7422, Batch Gradient Norm after: 22.360677398258055
Epoch 7423/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7419582009315491
Epoch: 7423, Batch Gradient Norm: 29.944719151461964
Epoch: 7423, Batch Gradient Norm after: 22.36067553325223
Epoch 7424/10000, Prediction Accuracy = 60.098%, Loss = 0.7453955173492431
Epoch: 7424, Batch Gradient Norm: 28.394862481221825
Epoch: 7424, Batch Gradient Norm after: 22.36067892928525
Epoch 7425/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.74179767370224
Epoch: 7425, Batch Gradient Norm: 29.943491050743194
Epoch: 7425, Batch Gradient Norm after: 22.360677507899894
Epoch 7426/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7452485799789429
Epoch: 7426, Batch Gradient Norm: 28.398218008704152
Epoch: 7426, Batch Gradient Norm after: 22.360680267034123
Epoch 7427/10000, Prediction Accuracy = 60.104%, Loss = 0.7417264819145203
Epoch: 7427, Batch Gradient Norm: 29.936856508365924
Epoch: 7427, Batch Gradient Norm after: 22.360677171319598
Epoch 7428/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7451870083808899
Epoch: 7428, Batch Gradient Norm: 28.397400599028916
Epoch: 7428, Batch Gradient Norm after: 22.360680912805936
Epoch 7429/10000, Prediction Accuracy = 60.098%, Loss = 0.7416466474533081
Epoch: 7429, Batch Gradient Norm: 29.931798831507486
Epoch: 7429, Batch Gradient Norm after: 22.360681100663786
Epoch 7430/10000, Prediction Accuracy = 60.016%, Loss = 0.7451391577720642
Epoch: 7430, Batch Gradient Norm: 28.39936111430096
Epoch: 7430, Batch Gradient Norm after: 22.360679067437474
Epoch 7431/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7416502118110657
Epoch: 7431, Batch Gradient Norm: 29.929410128056873
Epoch: 7431, Batch Gradient Norm after: 22.360678164693066
Epoch 7432/10000, Prediction Accuracy = 60.072%, Loss = 0.7452248573303223
Epoch: 7432, Batch Gradient Norm: 28.393977838287668
Epoch: 7432, Batch Gradient Norm after: 22.360679737707702
Epoch 7433/10000, Prediction Accuracy = 59.996%, Loss = 0.7417312145233155
Epoch: 7433, Batch Gradient Norm: 29.919646345745708
Epoch: 7433, Batch Gradient Norm after: 22.360679405237434
Epoch 7434/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7451724767684936
Epoch: 7434, Batch Gradient Norm: 28.392197318838665
Epoch: 7434, Batch Gradient Norm after: 22.36068003486351
Epoch 7435/10000, Prediction Accuracy = 60.004%, Loss = 0.7415790438652039
Epoch: 7435, Batch Gradient Norm: 29.91466863656936
Epoch: 7435, Batch Gradient Norm after: 22.360677443490257
Epoch 7436/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7449297308921814
Epoch: 7436, Batch Gradient Norm: 28.397817281137154
Epoch: 7436, Batch Gradient Norm after: 22.360677912464233
Epoch 7437/10000, Prediction Accuracy = 60.086%, Loss = 0.7414260625839233
Epoch: 7437, Batch Gradient Norm: 29.9126760558001
Epoch: 7437, Batch Gradient Norm after: 22.360678035041172
Epoch 7438/10000, Prediction Accuracy = 60.072%, Loss = 0.7448142647743226
Epoch: 7438, Batch Gradient Norm: 28.396948162319006
Epoch: 7438, Batch Gradient Norm after: 22.360679612420594
Epoch 7439/10000, Prediction Accuracy = 60.122%, Loss = 0.7413689255714416
Epoch: 7439, Batch Gradient Norm: 29.905852364094045
Epoch: 7439, Batch Gradient Norm after: 22.36067837098753
Epoch 7440/10000, Prediction Accuracy = 60.072%, Loss = 0.7447575569152832
Epoch: 7440, Batch Gradient Norm: 28.39901210300526
Epoch: 7440, Batch Gradient Norm after: 22.360678930711764
Epoch 7441/10000, Prediction Accuracy = 60.072%, Loss = 0.7412729978561401
Epoch: 7441, Batch Gradient Norm: 29.905776541396353
Epoch: 7441, Batch Gradient Norm after: 22.360681006366235
Epoch 7442/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7447335004806519
Epoch: 7442, Batch Gradient Norm: 28.395947903380026
Epoch: 7442, Batch Gradient Norm after: 22.3606780010111
Epoch 7443/10000, Prediction Accuracy = 60.024%, Loss = 0.7413132190704346
Epoch: 7443, Batch Gradient Norm: 29.90364758344979
Epoch: 7443, Batch Gradient Norm after: 22.36067777964523
Epoch 7444/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7448478937149048
Epoch: 7444, Batch Gradient Norm: 28.387659261788293
Epoch: 7444, Batch Gradient Norm after: 22.360679578097745
Epoch 7445/10000, Prediction Accuracy = 60.008%, Loss = 0.7413559556007385
Epoch: 7445, Batch Gradient Norm: 29.89580875918066
Epoch: 7445, Batch Gradient Norm after: 22.360675171100237
Epoch 7446/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.744730544090271
Epoch: 7446, Batch Gradient Norm: 28.381772304592747
Epoch: 7446, Batch Gradient Norm after: 22.36067843746306
Epoch 7447/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.741149652004242
Epoch: 7447, Batch Gradient Norm: 29.89475493231401
Epoch: 7447, Batch Gradient Norm after: 22.360678305249998
Epoch 7448/10000, Prediction Accuracy = 60.076%, Loss = 0.7444939374923706
Epoch: 7448, Batch Gradient Norm: 28.387484957207228
Epoch: 7448, Batch Gradient Norm after: 22.360677783595964
Epoch 7449/10000, Prediction Accuracy = 60.098%, Loss = 0.7410341978073121
Epoch: 7449, Batch Gradient Norm: 29.89112108063471
Epoch: 7449, Batch Gradient Norm after: 22.360678156131886
Epoch 7450/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7444056987762451
Epoch: 7450, Batch Gradient Norm: 28.38547752000774
Epoch: 7450, Batch Gradient Norm after: 22.360681505475434
Epoch 7451/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7409668803215027
Epoch: 7451, Batch Gradient Norm: 29.884875125456983
Epoch: 7451, Batch Gradient Norm after: 22.360679455873697
Epoch 7452/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7443491339683532
Epoch: 7452, Batch Gradient Norm: 28.386000536722097
Epoch: 7452, Batch Gradient Norm after: 22.36067848026725
Epoch 7453/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7408935546875
Epoch: 7453, Batch Gradient Norm: 29.885521395131764
Epoch: 7453, Batch Gradient Norm after: 22.360678236173836
Epoch 7454/10000, Prediction Accuracy = 60.102%, Loss = 0.7443655729293823
Epoch: 7454, Batch Gradient Norm: 28.379557630899537
Epoch: 7454, Batch Gradient Norm after: 22.360678249746673
Epoch 7455/10000, Prediction Accuracy = 60.038%, Loss = 0.7409509301185608
Epoch: 7455, Batch Gradient Norm: 29.88119416757006
Epoch: 7455, Batch Gradient Norm after: 22.360675718631846
Epoch 7456/10000, Prediction Accuracy = 60.084%, Loss = 0.7444322109222412
Epoch: 7456, Batch Gradient Norm: 28.37048480546787
Epoch: 7456, Batch Gradient Norm after: 22.360676138233718
Epoch 7457/10000, Prediction Accuracy = 60.036%, Loss = 0.7409125089645385
Epoch: 7457, Batch Gradient Norm: 29.873054610269953
Epoch: 7457, Batch Gradient Norm after: 22.360678370449484
Epoch 7458/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7442591786384583
Epoch: 7458, Batch Gradient Norm: 28.3689728662303
Epoch: 7458, Batch Gradient Norm after: 22.3606772878812
Epoch 7459/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.7407348036766053
Epoch: 7459, Batch Gradient Norm: 29.875916037331585
Epoch: 7459, Batch Gradient Norm after: 22.36067857104568
Epoch 7460/10000, Prediction Accuracy = 60.062%, Loss = 0.74406977891922
Epoch: 7460, Batch Gradient Norm: 28.37498399417581
Epoch: 7460, Batch Gradient Norm after: 22.360678670777713
Epoch 7461/10000, Prediction Accuracy = 60.092%, Loss = 0.7406501173973083
Epoch: 7461, Batch Gradient Norm: 29.870495133979343
Epoch: 7461, Batch Gradient Norm after: 22.360679233365698
Epoch 7462/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.7440019726753235
Epoch: 7462, Batch Gradient Norm: 28.372326904334017
Epoch: 7462, Batch Gradient Norm after: 22.36067834455732
Epoch 7463/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7405788421630859
Epoch: 7463, Batch Gradient Norm: 29.864069554357968
Epoch: 7463, Batch Gradient Norm after: 22.360679722616318
Epoch 7464/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7439427375793457
Epoch: 7464, Batch Gradient Norm: 28.37418568394317
Epoch: 7464, Batch Gradient Norm after: 22.360676366877993
Epoch 7465/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7405124545097351
Epoch: 7465, Batch Gradient Norm: 29.865171680442852
Epoch: 7465, Batch Gradient Norm after: 22.360678467586677
Epoch 7466/10000, Prediction Accuracy = 60.084%, Loss = 0.7440090417861939
Epoch: 7466, Batch Gradient Norm: 28.36621059356553
Epoch: 7466, Batch Gradient Norm after: 22.360677886207625
Epoch 7467/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7406207084655761
Epoch: 7467, Batch Gradient Norm: 29.859643420333967
Epoch: 7467, Batch Gradient Norm after: 22.360678263407337
Epoch 7468/10000, Prediction Accuracy = 60.08%, Loss = 0.7440359830856323
Epoch: 7468, Batch Gradient Norm: 28.358801533309776
Epoch: 7468, Batch Gradient Norm after: 22.360676369724366
Epoch 7469/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7405113339424133
Epoch: 7469, Batch Gradient Norm: 29.85274241307311
Epoch: 7469, Batch Gradient Norm after: 22.360679473617072
Epoch 7470/10000, Prediction Accuracy = 60.116%, Loss = 0.743805730342865
Epoch: 7470, Batch Gradient Norm: 28.362643459994413
Epoch: 7470, Batch Gradient Norm after: 22.360677593156392
Epoch 7471/10000, Prediction Accuracy = 60.064%, Loss = 0.7403323650360107
Epoch: 7471, Batch Gradient Norm: 29.854193812709347
Epoch: 7471, Batch Gradient Norm after: 22.360677675436577
Epoch 7472/10000, Prediction Accuracy = 60.06%, Loss = 0.7436434149742126
Epoch: 7472, Batch Gradient Norm: 28.370347374964282
Epoch: 7472, Batch Gradient Norm after: 22.360678832544576
Epoch 7473/10000, Prediction Accuracy = 60.114%, Loss = 0.7402745008468627
Epoch: 7473, Batch Gradient Norm: 29.847016878041128
Epoch: 7473, Batch Gradient Norm after: 22.360676934243212
Epoch 7474/10000, Prediction Accuracy = 60.084%, Loss = 0.7435798883438111
Epoch: 7474, Batch Gradient Norm: 28.371011179057536
Epoch: 7474, Batch Gradient Norm after: 22.36068018166384
Epoch 7475/10000, Prediction Accuracy = 60.116%, Loss = 0.7402017474174499
Epoch: 7475, Batch Gradient Norm: 29.842314728901144
Epoch: 7475, Batch Gradient Norm after: 22.360677744535078
Epoch 7476/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7435306787490845
Epoch: 7476, Batch Gradient Norm: 28.368378656146763
Epoch: 7476, Batch Gradient Norm after: 22.36067830949572
Epoch 7477/10000, Prediction Accuracy = 60.038%, Loss = 0.7401664853096008
Epoch: 7477, Batch Gradient Norm: 29.84048306829546
Epoch: 7477, Batch Gradient Norm after: 22.36067864995993
Epoch 7478/10000, Prediction Accuracy = 60.088%, Loss = 0.7436163544654846
Epoch: 7478, Batch Gradient Norm: 28.360090048769923
Epoch: 7478, Batch Gradient Norm after: 22.360677020074288
Epoch 7479/10000, Prediction Accuracy = 60.056%, Loss = 0.7402422428131104
Epoch: 7479, Batch Gradient Norm: 29.835233427533858
Epoch: 7479, Batch Gradient Norm after: 22.360677921502862
Epoch 7480/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.7436075687408448
Epoch: 7480, Batch Gradient Norm: 28.35760621251652
Epoch: 7480, Batch Gradient Norm after: 22.360677171796095
Epoch 7481/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.7401137709617615
Epoch: 7481, Batch Gradient Norm: 29.829023699020826
Epoch: 7481, Batch Gradient Norm after: 22.360679328399552
Epoch 7482/10000, Prediction Accuracy = 60.092%, Loss = 0.7433675289154053
Epoch: 7482, Batch Gradient Norm: 28.36519729412134
Epoch: 7482, Batch Gradient Norm after: 22.360679848833275
Epoch 7483/10000, Prediction Accuracy = 60.086%, Loss = 0.7399670600891113
Epoch: 7483, Batch Gradient Norm: 29.826617750908717
Epoch: 7483, Batch Gradient Norm after: 22.3606780904574
Epoch 7484/10000, Prediction Accuracy = 60.072%, Loss = 0.7432256460189819
Epoch: 7484, Batch Gradient Norm: 28.371489767148898
Epoch: 7484, Batch Gradient Norm after: 22.36067995859822
Epoch 7485/10000, Prediction Accuracy = 60.084%, Loss = 0.7399116396903992
Epoch: 7485, Batch Gradient Norm: 29.818924370888876
Epoch: 7485, Batch Gradient Norm after: 22.3606765380159
Epoch 7486/10000, Prediction Accuracy = 60.08%, Loss = 0.743167519569397
Epoch: 7486, Batch Gradient Norm: 28.3739310934182
Epoch: 7486, Batch Gradient Norm after: 22.36067819016987
Epoch 7487/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.739835786819458
Epoch: 7487, Batch Gradient Norm: 29.81534899010222
Epoch: 7487, Batch Gradient Norm after: 22.360679361158176
Epoch 7488/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.7431284308433532
Epoch: 7488, Batch Gradient Norm: 28.37331098129448
Epoch: 7488, Batch Gradient Norm after: 22.360678773051713
Epoch 7489/10000, Prediction Accuracy = 60.024%, Loss = 0.7398413419723511
Epoch: 7489, Batch Gradient Norm: 29.815196451427
Epoch: 7489, Batch Gradient Norm after: 22.36067739982325
Epoch 7490/10000, Prediction Accuracy = 60.102%, Loss = 0.7432232618331909
Epoch: 7490, Batch Gradient Norm: 28.364908659106725
Epoch: 7490, Batch Gradient Norm after: 22.36067903508398
Epoch 7491/10000, Prediction Accuracy = 60.064%, Loss = 0.7399117827415467
Epoch: 7491, Batch Gradient Norm: 29.80561119717087
Epoch: 7491, Batch Gradient Norm after: 22.360675560819622
Epoch 7492/10000, Prediction Accuracy = 60.088%, Loss = 0.7431611657142639
Epoch: 7492, Batch Gradient Norm: 28.362586602634586
Epoch: 7492, Batch Gradient Norm after: 22.360679178935943
Epoch 7493/10000, Prediction Accuracy = 60.086%, Loss = 0.7397503972053527
Epoch: 7493, Batch Gradient Norm: 29.802087412830364
Epoch: 7493, Batch Gradient Norm after: 22.360679651121917
Epoch 7494/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7429185509681702
Epoch: 7494, Batch Gradient Norm: 28.371496646427865
Epoch: 7494, Batch Gradient Norm after: 22.360678798998066
Epoch 7495/10000, Prediction Accuracy = 60.088%, Loss = 0.7396247029304505
Epoch: 7495, Batch Gradient Norm: 29.799392264682716
Epoch: 7495, Batch Gradient Norm after: 22.36067767647306
Epoch 7496/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7428020119667054
Epoch: 7496, Batch Gradient Norm: 28.37529347547728
Epoch: 7496, Batch Gradient Norm after: 22.360678784857914
Epoch 7497/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7395684957504273
Epoch: 7497, Batch Gradient Norm: 29.79018033387564
Epoch: 7497, Batch Gradient Norm after: 22.360678155187703
Epoch 7498/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7427455425262451
Epoch: 7498, Batch Gradient Norm: 28.375365272991814
Epoch: 7498, Batch Gradient Norm after: 22.360679146978242
Epoch 7499/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7394840955734253
Epoch: 7499, Batch Gradient Norm: 29.788983042059648
Epoch: 7499, Batch Gradient Norm after: 22.360676638301307
Epoch 7500/10000, Prediction Accuracy = 60.088%, Loss = 0.7427254557609558
Epoch: 7500, Batch Gradient Norm: 28.375454406522046
Epoch: 7500, Batch Gradient Norm after: 22.360680150203265
Epoch 7501/10000, Prediction Accuracy = 60.044%, Loss = 0.7395283460617066
Epoch: 7501, Batch Gradient Norm: 29.784466502736656
Epoch: 7501, Batch Gradient Norm after: 22.36067558447692
Epoch 7502/10000, Prediction Accuracy = 60.092%, Loss = 0.7428096652030944
Epoch: 7502, Batch Gradient Norm: 28.370491747713686
Epoch: 7502, Batch Gradient Norm after: 22.360676024028965
Epoch 7503/10000, Prediction Accuracy = 60.06%, Loss = 0.7395683765411377
Epoch: 7503, Batch Gradient Norm: 29.776619653598324
Epoch: 7503, Batch Gradient Norm after: 22.360678979716226
Epoch 7504/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7426997423171997
Epoch: 7504, Batch Gradient Norm: 28.370905647663143
Epoch: 7504, Batch Gradient Norm after: 22.360677662515712
Epoch 7505/10000, Prediction Accuracy = 60.1%, Loss = 0.7393906235694885
Epoch: 7505, Batch Gradient Norm: 29.77479379911063
Epoch: 7505, Batch Gradient Norm after: 22.360679518008055
Epoch 7506/10000, Prediction Accuracy = 60.098%, Loss = 0.7424686670303344
Epoch: 7506, Batch Gradient Norm: 28.379835048996757
Epoch: 7506, Batch Gradient Norm after: 22.360678085961155
Epoch 7507/10000, Prediction Accuracy = 60.102%, Loss = 0.7392794013023376
Epoch: 7507, Batch Gradient Norm: 29.771340862744374
Epoch: 7507, Batch Gradient Norm after: 22.360678718395963
Epoch 7508/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7423702001571655
Epoch: 7508, Batch Gradient Norm: 28.38593082038291
Epoch: 7508, Batch Gradient Norm after: 22.360678915947727
Epoch 7509/10000, Prediction Accuracy = 60.086%, Loss = 0.7392319321632386
Epoch: 7509, Batch Gradient Norm: 29.760659342566488
Epoch: 7509, Batch Gradient Norm after: 22.360678864052478
Epoch 7510/10000, Prediction Accuracy = 60.102%, Loss = 0.7423087358474731
Epoch: 7510, Batch Gradient Norm: 28.384473199627685
Epoch: 7510, Batch Gradient Norm after: 22.36067717818705
Epoch 7511/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7391528010368347
Epoch: 7511, Batch Gradient Norm: 29.761824284911118
Epoch: 7511, Batch Gradient Norm after: 22.36067790034736
Epoch 7512/10000, Prediction Accuracy = 60.104%, Loss = 0.7423152089118957
Epoch: 7512, Batch Gradient Norm: 28.383538925093703
Epoch: 7512, Batch Gradient Norm after: 22.36067870329076
Epoch 7513/10000, Prediction Accuracy = 60.072%, Loss = 0.7392195224761963
Epoch: 7513, Batch Gradient Norm: 29.757700571600793
Epoch: 7513, Batch Gradient Norm after: 22.36067771817488
Epoch 7514/10000, Prediction Accuracy = 60.102%, Loss = 0.7423855066299438
Epoch: 7514, Batch Gradient Norm: 28.37790552551579
Epoch: 7514, Batch Gradient Norm after: 22.360678480698358
Epoch 7515/10000, Prediction Accuracy = 60.048%, Loss = 0.7392175197601318
Epoch: 7515, Batch Gradient Norm: 29.749802005613738
Epoch: 7515, Batch Gradient Norm after: 22.3606777111049
Epoch 7516/10000, Prediction Accuracy = 60.134%, Loss = 0.7422249555587769
Epoch: 7516, Batch Gradient Norm: 28.383018876960694
Epoch: 7516, Batch Gradient Norm after: 22.3606783207871
Epoch 7517/10000, Prediction Accuracy = 60.068%, Loss = 0.739037299156189
Epoch: 7517, Batch Gradient Norm: 29.74901878074964
Epoch: 7517, Batch Gradient Norm after: 22.360680117541737
Epoch 7518/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7420209288597107
Epoch: 7518, Batch Gradient Norm: 28.39076144615173
Epoch: 7518, Batch Gradient Norm after: 22.360677341897063
Epoch 7519/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.738946008682251
Epoch: 7519, Batch Gradient Norm: 29.74370796085978
Epoch: 7519, Batch Gradient Norm after: 22.360678028264104
Epoch 7520/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7419415950775147
Epoch: 7520, Batch Gradient Norm: 28.391738906340404
Epoch: 7520, Batch Gradient Norm after: 22.360678409860895
Epoch 7521/10000, Prediction Accuracy = 60.088%, Loss = 0.738884699344635
Epoch: 7521, Batch Gradient Norm: 29.73716669307291
Epoch: 7521, Batch Gradient Norm after: 22.360677335162734
Epoch 7522/10000, Prediction Accuracy = 60.11%, Loss = 0.7418837785720825
Epoch: 7522, Batch Gradient Norm: 28.392169586331605
Epoch: 7522, Batch Gradient Norm after: 22.360679259375253
Epoch 7523/10000, Prediction Accuracy = 60.084%, Loss = 0.7388266563415528
Epoch: 7523, Batch Gradient Norm: 29.738331394660488
Epoch: 7523, Batch Gradient Norm after: 22.36067934938873
Epoch 7524/10000, Prediction Accuracy = 60.11%, Loss = 0.7419305443763733
Epoch: 7524, Batch Gradient Norm: 28.389299882323012
Epoch: 7524, Batch Gradient Norm after: 22.360676762245166
Epoch 7525/10000, Prediction Accuracy = 60.072%, Loss = 0.7389299511909485
Epoch: 7525, Batch Gradient Norm: 29.733199182286167
Epoch: 7525, Batch Gradient Norm after: 22.360678751919934
Epoch 7526/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7419841885566711
Epoch: 7526, Batch Gradient Norm: 28.38225462830213
Epoch: 7526, Batch Gradient Norm after: 22.360677568501465
Epoch 7527/10000, Prediction Accuracy = 60.05%, Loss = 0.7388485312461853
Epoch: 7527, Batch Gradient Norm: 29.72783004155591
Epoch: 7527, Batch Gradient Norm after: 22.3606768145582
Epoch 7528/10000, Prediction Accuracy = 60.138%, Loss = 0.7417581081390381
Epoch: 7528, Batch Gradient Norm: 28.387364481604966
Epoch: 7528, Batch Gradient Norm after: 22.360679378578325
Epoch 7529/10000, Prediction Accuracy = 60.068%, Loss = 0.7386663675308227
Epoch: 7529, Batch Gradient Norm: 29.72511244091106
Epoch: 7529, Batch Gradient Norm after: 22.360678778555407
Epoch 7530/10000, Prediction Accuracy = 60.102%, Loss = 0.741586709022522
Epoch: 7530, Batch Gradient Norm: 28.395960271360025
Epoch: 7530, Batch Gradient Norm after: 22.360678762106424
Epoch 7531/10000, Prediction Accuracy = 60.06%, Loss = 0.7386083960533142
Epoch: 7531, Batch Gradient Norm: 29.71698227533065
Epoch: 7531, Batch Gradient Norm after: 22.36067824222529
Epoch 7532/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7415318012237548
Epoch: 7532, Batch Gradient Norm: 28.39706506438294
Epoch: 7532, Batch Gradient Norm after: 22.360679516442445
Epoch 7533/10000, Prediction Accuracy = 60.09000000000001%, Loss = 0.7385316848754883
Epoch: 7533, Batch Gradient Norm: 29.713554255593365
Epoch: 7533, Batch Gradient Norm after: 22.360678628973737
Epoch 7534/10000, Prediction Accuracy = 60.102%, Loss = 0.7414719223976135
Epoch: 7534, Batch Gradient Norm: 28.399031951336294
Epoch: 7534, Batch Gradient Norm after: 22.3606775406715
Epoch 7535/10000, Prediction Accuracy = 60.068%, Loss = 0.7385126948356628
Epoch: 7535, Batch Gradient Norm: 29.7140226329447
Epoch: 7535, Batch Gradient Norm after: 22.360676062132786
Epoch 7536/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7415542244911194
Epoch: 7536, Batch Gradient Norm: 28.396124424834994
Epoch: 7536, Batch Gradient Norm after: 22.360676273324493
Epoch 7537/10000, Prediction Accuracy = 60.06%, Loss = 0.7386057019233704
Epoch: 7537, Batch Gradient Norm: 29.706112227361025
Epoch: 7537, Batch Gradient Norm after: 22.36067861888231
Epoch 7538/10000, Prediction Accuracy = 60.072%, Loss = 0.74153972864151
Epoch: 7538, Batch Gradient Norm: 28.387891984520294
Epoch: 7538, Batch Gradient Norm after: 22.36067737551102
Epoch 7539/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7384694933891296
Epoch: 7539, Batch Gradient Norm: 29.70317289274673
Epoch: 7539, Batch Gradient Norm after: 22.360680434954897
Epoch 7540/10000, Prediction Accuracy = 60.138%, Loss = 0.7413134574890137
Epoch: 7540, Batch Gradient Norm: 28.394695592393777
Epoch: 7540, Batch Gradient Norm after: 22.36068111894531
Epoch 7541/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.7383136510848999
Epoch: 7541, Batch Gradient Norm: 29.70302459116922
Epoch: 7541, Batch Gradient Norm after: 22.36067968795878
Epoch 7542/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7411699533462525
Epoch: 7542, Batch Gradient Norm: 28.39949142596078
Epoch: 7542, Batch Gradient Norm after: 22.360680090839622
Epoch 7543/10000, Prediction Accuracy = 60.076%, Loss = 0.738262128829956
Epoch: 7543, Batch Gradient Norm: 29.694966254714995
Epoch: 7543, Batch Gradient Norm after: 22.360679038624788
Epoch 7544/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.7411126494407654
Epoch: 7544, Batch Gradient Norm: 28.39748767077228
Epoch: 7544, Batch Gradient Norm after: 22.360677509593824
Epoch 7545/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7381731152534485
Epoch: 7545, Batch Gradient Norm: 29.69349975249417
Epoch: 7545, Batch Gradient Norm after: 22.360679681366438
Epoch 7546/10000, Prediction Accuracy = 60.114%, Loss = 0.7410737991333007
Epoch: 7546, Batch Gradient Norm: 28.397428792189398
Epoch: 7546, Batch Gradient Norm after: 22.360680928334784
Epoch 7547/10000, Prediction Accuracy = 60.06%, Loss = 0.7381757616996765
Epoch: 7547, Batch Gradient Norm: 29.69095344968514
Epoch: 7547, Batch Gradient Norm after: 22.360678354572617
Epoch 7548/10000, Prediction Accuracy = 60.092%, Loss = 0.7411751389503479
Epoch: 7548, Batch Gradient Norm: 28.392089258734966
Epoch: 7548, Batch Gradient Norm after: 22.36067955264143
Epoch 7549/10000, Prediction Accuracy = 60.072%, Loss = 0.738254451751709
Epoch: 7549, Batch Gradient Norm: 29.685939087894997
Epoch: 7549, Batch Gradient Norm after: 22.360677981848486
Epoch 7550/10000, Prediction Accuracy = 60.07199999999999%, Loss = 0.7411208510398865
Epoch: 7550, Batch Gradient Norm: 28.380848040956593
Epoch: 7550, Batch Gradient Norm after: 22.36067842557516
Epoch 7551/10000, Prediction Accuracy = 60.098%, Loss = 0.7380820631980896
Epoch: 7551, Batch Gradient Norm: 29.684851753258833
Epoch: 7551, Batch Gradient Norm after: 22.360682784381005
Epoch 7552/10000, Prediction Accuracy = 60.138%, Loss = 0.7408894181251526
Epoch: 7552, Batch Gradient Norm: 28.387533764190472
Epoch: 7552, Batch Gradient Norm after: 22.360678572766037
Epoch 7553/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7379336595535279
Epoch: 7553, Batch Gradient Norm: 29.685734195297677
Epoch: 7553, Batch Gradient Norm after: 22.36067987180698
Epoch 7554/10000, Prediction Accuracy = 60.08%, Loss = 0.7407782912254334
Epoch: 7554, Batch Gradient Norm: 28.386898666280622
Epoch: 7554, Batch Gradient Norm after: 22.360677763424857
Epoch 7555/10000, Prediction Accuracy = 60.086%, Loss = 0.7378785729408264
Epoch: 7555, Batch Gradient Norm: 29.6784652202388
Epoch: 7555, Batch Gradient Norm after: 22.36068076330196
Epoch 7556/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7407215595245361
Epoch: 7556, Batch Gradient Norm: 28.385166074904333
Epoch: 7556, Batch Gradient Norm after: 22.360678791279554
Epoch 7557/10000, Prediction Accuracy = 60.084%, Loss = 0.7377863168716431
Epoch: 7557, Batch Gradient Norm: 29.6777578310329
Epoch: 7557, Batch Gradient Norm after: 22.36068134053957
Epoch 7558/10000, Prediction Accuracy = 60.116%, Loss = 0.7407078146934509
Epoch: 7558, Batch Gradient Norm: 28.384390894812764
Epoch: 7558, Batch Gradient Norm after: 22.36067863712202
Epoch 7559/10000, Prediction Accuracy = 60.05%, Loss = 0.7378113389015197
Epoch: 7559, Batch Gradient Norm: 29.67564444161526
Epoch: 7559, Batch Gradient Norm after: 22.36067914972834
Epoch 7560/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7407997488975525
Epoch: 7560, Batch Gradient Norm: 28.379087813356822
Epoch: 7560, Batch Gradient Norm after: 22.36067841808859
Epoch 7561/10000, Prediction Accuracy = 60.076%, Loss = 0.7378821015357971
Epoch: 7561, Batch Gradient Norm: 29.66784166633549
Epoch: 7561, Batch Gradient Norm after: 22.360680502884104
Epoch 7562/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7407044053077698
Epoch: 7562, Batch Gradient Norm: 28.378383036110694
Epoch: 7562, Batch Gradient Norm after: 22.36067694233976
Epoch 7563/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7376984715461731
Epoch: 7563, Batch Gradient Norm: 29.66860466993451
Epoch: 7563, Batch Gradient Norm after: 22.36068004883739
Epoch 7564/10000, Prediction Accuracy = 60.152%, Loss = 0.7404724955558777
Epoch: 7564, Batch Gradient Norm: 28.37877617057936
Epoch: 7564, Batch Gradient Norm after: 22.36067897388213
Epoch 7565/10000, Prediction Accuracy = 60.116%, Loss = 0.7375712275505066
Epoch: 7565, Batch Gradient Norm: 29.661596325819744
Epoch: 7565, Batch Gradient Norm after: 22.36067819507656
Epoch 7566/10000, Prediction Accuracy = 60.1%, Loss = 0.7403804063796997
Epoch: 7566, Batch Gradient Norm: 28.380827915643728
Epoch: 7566, Batch Gradient Norm after: 22.360679542275044
Epoch 7567/10000, Prediction Accuracy = 60.076%, Loss = 0.7375109910964965
Epoch: 7567, Batch Gradient Norm: 29.655572836301683
Epoch: 7567, Batch Gradient Norm after: 22.360680937684112
Epoch 7568/10000, Prediction Accuracy = 60.134%, Loss = 0.740327262878418
Epoch: 7568, Batch Gradient Norm: 28.376712964416356
Epoch: 7568, Batch Gradient Norm after: 22.36067829758962
Epoch 7569/10000, Prediction Accuracy = 60.098%, Loss = 0.7374161005020141
Epoch: 7569, Batch Gradient Norm: 29.659490070950547
Epoch: 7569, Batch Gradient Norm after: 22.360678769741863
Epoch 7570/10000, Prediction Accuracy = 60.124%, Loss = 0.7403291583061218
Epoch: 7570, Batch Gradient Norm: 28.374543965578937
Epoch: 7570, Batch Gradient Norm after: 22.360678456052717
Epoch 7571/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.737471079826355
Epoch: 7571, Batch Gradient Norm: 29.655999057029153
Epoch: 7571, Batch Gradient Norm after: 22.360678120347572
Epoch 7572/10000, Prediction Accuracy = 60.084%, Loss = 0.7404103517532349
Epoch: 7572, Batch Gradient Norm: 28.366314730411997
Epoch: 7572, Batch Gradient Norm after: 22.36067805716896
Epoch 7573/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7374779939651489
Epoch: 7573, Batch Gradient Norm: 29.651443143902217
Epoch: 7573, Batch Gradient Norm after: 22.360679549092065
Epoch 7574/10000, Prediction Accuracy = 60.126%, Loss = 0.7402806520462036
Epoch: 7574, Batch Gradient Norm: 28.361495996870172
Epoch: 7574, Batch Gradient Norm after: 22.360677579056617
Epoch 7575/10000, Prediction Accuracy = 60.1%, Loss = 0.7372792601585388
Epoch: 7575, Batch Gradient Norm: 29.653814793145827
Epoch: 7575, Batch Gradient Norm after: 22.360679234568305
Epoch 7576/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7400769829750061
Epoch: 7576, Batch Gradient Norm: 28.35840145219944
Epoch: 7576, Batch Gradient Norm after: 22.360678034588375
Epoch 7577/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.737171220779419
Epoch: 7577, Batch Gradient Norm: 29.649612156909452
Epoch: 7577, Batch Gradient Norm after: 22.36067875092321
Epoch 7578/10000, Prediction Accuracy = 60.098%, Loss = 0.7400089502334595
Epoch: 7578, Batch Gradient Norm: 28.352836832125917
Epoch: 7578, Batch Gradient Norm after: 22.360678826291203
Epoch 7579/10000, Prediction Accuracy = 60.076%, Loss = 0.7370874166488648
Epoch: 7579, Batch Gradient Norm: 29.64489125903886
Epoch: 7579, Batch Gradient Norm after: 22.360676972493525
Epoch 7580/10000, Prediction Accuracy = 60.116%, Loss = 0.7399564266204834
Epoch: 7580, Batch Gradient Norm: 28.34659729679077
Epoch: 7580, Batch Gradient Norm after: 22.360680739407254
Epoch 7581/10000, Prediction Accuracy = 60.102%, Loss = 0.7370039343833923
Epoch: 7581, Batch Gradient Norm: 29.649863099232675
Epoch: 7581, Batch Gradient Norm after: 22.36068145933647
Epoch 7582/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.739990496635437
Epoch: 7582, Batch Gradient Norm: 28.338102362595908
Epoch: 7582, Batch Gradient Norm after: 22.360677197088332
Epoch 7583/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.7370684981346131
Epoch: 7583, Batch Gradient Norm: 29.64423231598551
Epoch: 7583, Batch Gradient Norm after: 22.360679814746362
Epoch 7584/10000, Prediction Accuracy = 60.088%, Loss = 0.7400720953941345
Epoch: 7584, Batch Gradient Norm: 28.327562676018324
Epoch: 7584, Batch Gradient Norm after: 22.360677721066896
Epoch 7585/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7370284199714661
Epoch: 7585, Batch Gradient Norm: 29.640281344009317
Epoch: 7585, Batch Gradient Norm after: 22.360678570039973
Epoch 7586/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7398846030235291
Epoch: 7586, Batch Gradient Norm: 28.324839403759114
Epoch: 7586, Batch Gradient Norm after: 22.36068003186336
Epoch 7587/10000, Prediction Accuracy = 60.124%, Loss = 0.7368269443511963
Epoch: 7587, Batch Gradient Norm: 29.642338653015297
Epoch: 7587, Batch Gradient Norm after: 22.360680727138693
Epoch 7588/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7396978616714478
Epoch: 7588, Batch Gradient Norm: 28.322924942357858
Epoch: 7588, Batch Gradient Norm after: 22.36067943335756
Epoch 7589/10000, Prediction Accuracy = 60.074%, Loss = 0.7367320537567139
Epoch: 7589, Batch Gradient Norm: 29.638531745988587
Epoch: 7589, Batch Gradient Norm after: 22.360679601551578
Epoch 7590/10000, Prediction Accuracy = 60.114%, Loss = 0.7396494269371032
Epoch: 7590, Batch Gradient Norm: 28.31925159765262
Epoch: 7590, Batch Gradient Norm after: 22.360677048975642
Epoch 7591/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.736648416519165
Epoch: 7591, Batch Gradient Norm: 29.63325334446482
Epoch: 7591, Batch Gradient Norm after: 22.360679973179074
Epoch 7592/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7395954012870789
Epoch: 7592, Batch Gradient Norm: 28.31726244989313
Epoch: 7592, Batch Gradient Norm after: 22.36067857964887
Epoch 7593/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.7366043448448181
Epoch: 7593, Batch Gradient Norm: 29.63569295041474
Epoch: 7593, Batch Gradient Norm after: 22.36067702217283
Epoch 7594/10000, Prediction Accuracy = 60.102%, Loss = 0.7396585583686829
Epoch: 7594, Batch Gradient Norm: 28.306983806580735
Epoch: 7594, Batch Gradient Norm after: 22.360678335521456
Epoch 7595/10000, Prediction Accuracy = 60.102%, Loss = 0.7366849303245544
Epoch: 7595, Batch Gradient Norm: 29.631160136743464
Epoch: 7595, Batch Gradient Norm after: 22.360678071867625
Epoch 7596/10000, Prediction Accuracy = 60.11200000000001%, Loss = 0.739696991443634
Epoch: 7596, Batch Gradient Norm: 28.29893526888874
Epoch: 7596, Batch Gradient Norm after: 22.36067809408311
Epoch 7597/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7365816950798034
Epoch: 7597, Batch Gradient Norm: 29.628351604129307
Epoch: 7597, Batch Gradient Norm after: 22.360677373039998
Epoch 7598/10000, Prediction Accuracy = 60.136%, Loss = 0.7394626498222351
Epoch: 7598, Batch Gradient Norm: 28.29964655525304
Epoch: 7598, Batch Gradient Norm after: 22.36067855012296
Epoch 7599/10000, Prediction Accuracy = 60.086%, Loss = 0.7363979935646057
Epoch: 7599, Batch Gradient Norm: 29.62900668921728
Epoch: 7599, Batch Gradient Norm after: 22.360678467942254
Epoch 7600/10000, Prediction Accuracy = 60.104%, Loss = 0.7393172860145569
Epoch: 7600, Batch Gradient Norm: 28.29880366412781
Epoch: 7600, Batch Gradient Norm after: 22.360677449855654
Epoch 7601/10000, Prediction Accuracy = 60.05799999999999%, Loss = 0.7363347768783569
Epoch: 7601, Batch Gradient Norm: 29.620126174028496
Epoch: 7601, Batch Gradient Norm after: 22.360679309864125
Epoch 7602/10000, Prediction Accuracy = 60.114%, Loss = 0.73927001953125
Epoch: 7602, Batch Gradient Norm: 28.294391842843325
Epoch: 7602, Batch Gradient Norm after: 22.36067878679938
Epoch 7603/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7362341165542603
Epoch: 7603, Batch Gradient Norm: 29.6184660072985
Epoch: 7603, Batch Gradient Norm after: 22.360680255286173
Epoch 7604/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7392316699028015
Epoch: 7604, Batch Gradient Norm: 28.292906793254883
Epoch: 7604, Batch Gradient Norm after: 22.360677853174327
Epoch 7605/10000, Prediction Accuracy = 60.1%, Loss = 0.7362311959266663
Epoch: 7605, Batch Gradient Norm: 29.62179117495073
Epoch: 7605, Batch Gradient Norm after: 22.36067881216257
Epoch 7606/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7393242716789246
Epoch: 7606, Batch Gradient Norm: 28.277567286218034
Epoch: 7606, Batch Gradient Norm after: 22.360678742465083
Epoch 7607/10000, Prediction Accuracy = 60.128%, Loss = 0.7362826347351075
Epoch: 7607, Batch Gradient Norm: 29.618353743373767
Epoch: 7607, Batch Gradient Norm after: 22.360677975324055
Epoch 7608/10000, Prediction Accuracy = 60.112%, Loss = 0.7392877101898193
Epoch: 7608, Batch Gradient Norm: 28.268382540296052
Epoch: 7608, Batch Gradient Norm after: 22.36067912141709
Epoch 7609/10000, Prediction Accuracy = 60.128%, Loss = 0.7361287593841552
Epoch: 7609, Batch Gradient Norm: 29.615014704151402
Epoch: 7609, Batch Gradient Norm after: 22.36068034915913
Epoch 7610/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7390635251998902
Epoch: 7610, Batch Gradient Norm: 28.270914422413725
Epoch: 7610, Batch Gradient Norm after: 22.360678179903655
Epoch 7611/10000, Prediction Accuracy = 60.124%, Loss = 0.7359800100326538
Epoch: 7611, Batch Gradient Norm: 29.615300362379262
Epoch: 7611, Batch Gradient Norm after: 22.36067787804771
Epoch 7612/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7389508366584778
Epoch: 7612, Batch Gradient Norm: 28.268459382376637
Epoch: 7612, Batch Gradient Norm after: 22.360678860400288
Epoch 7613/10000, Prediction Accuracy = 60.06%, Loss = 0.7359247088432312
Epoch: 7613, Batch Gradient Norm: 29.60757714036428
Epoch: 7613, Batch Gradient Norm after: 22.36067857943526
Epoch 7614/10000, Prediction Accuracy = 60.116%, Loss = 0.7389053106307983
Epoch: 7614, Batch Gradient Norm: 28.26000098002651
Epoch: 7614, Batch Gradient Norm after: 22.36067757959759
Epoch 7615/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.73581303358078
Epoch: 7615, Batch Gradient Norm: 29.611017457013823
Epoch: 7615, Batch Gradient Norm after: 22.360677742528967
Epoch 7616/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7388882637023926
Epoch: 7616, Batch Gradient Norm: 28.25377698364702
Epoch: 7616, Batch Gradient Norm after: 22.360678546531837
Epoch 7617/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.735835075378418
Epoch: 7617, Batch Gradient Norm: 29.61397069642127
Epoch: 7617, Batch Gradient Norm after: 22.360676474600275
Epoch 7618/10000, Prediction Accuracy = 60.126%, Loss = 0.7389852166175842
Epoch: 7618, Batch Gradient Norm: 28.24052596509486
Epoch: 7618, Batch Gradient Norm after: 22.36067883537171
Epoch 7619/10000, Prediction Accuracy = 60.128%, Loss = 0.7358597159385681
Epoch: 7619, Batch Gradient Norm: 29.608653946230245
Epoch: 7619, Batch Gradient Norm after: 22.360678855435715
Epoch 7620/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7388923048973084
Epoch: 7620, Batch Gradient Norm: 28.232572150330544
Epoch: 7620, Batch Gradient Norm after: 22.36067672659602
Epoch 7621/10000, Prediction Accuracy = 60.128%, Loss = 0.7356718182563782
Epoch: 7621, Batch Gradient Norm: 29.610547399961167
Epoch: 7621, Batch Gradient Norm after: 22.360679224776288
Epoch 7622/10000, Prediction Accuracy = 60.152%, Loss = 0.7386745810508728
Epoch: 7622, Batch Gradient Norm: 28.23231875539394
Epoch: 7622, Batch Gradient Norm after: 22.36067812016987
Epoch 7623/10000, Prediction Accuracy = 60.136%, Loss = 0.7355491876602173
Epoch: 7623, Batch Gradient Norm: 29.607337800270646
Epoch: 7623, Batch Gradient Norm after: 22.360677162478858
Epoch 7624/10000, Prediction Accuracy = 60.128%, Loss = 0.7385936260223389
Epoch: 7624, Batch Gradient Norm: 28.22689055082917
Epoch: 7624, Batch Gradient Norm after: 22.36067899813211
Epoch 7625/10000, Prediction Accuracy = 60.064%, Loss = 0.735480272769928
Epoch: 7625, Batch Gradient Norm: 29.603242812475724
Epoch: 7625, Batch Gradient Norm after: 22.360681594046678
Epoch 7626/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7385427355766296
Epoch: 7626, Batch Gradient Norm: 28.218550760891073
Epoch: 7626, Batch Gradient Norm after: 22.36067695161705
Epoch 7627/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7353806972503663
Epoch: 7627, Batch Gradient Norm: 29.605805064077362
Epoch: 7627, Batch Gradient Norm after: 22.360677591359526
Epoch 7628/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7385502338409424
Epoch: 7628, Batch Gradient Norm: 28.209306507428927
Epoch: 7628, Batch Gradient Norm after: 22.36067727423437
Epoch 7629/10000, Prediction Accuracy = 60.122%, Loss = 0.7354103088378906
Epoch: 7629, Batch Gradient Norm: 29.607297999175923
Epoch: 7629, Batch Gradient Norm after: 22.360680069014926
Epoch 7630/10000, Prediction Accuracy = 60.11%, Loss = 0.7386642932891846
Epoch: 7630, Batch Gradient Norm: 28.192977831577974
Epoch: 7630, Batch Gradient Norm after: 22.3606776956481
Epoch 7631/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7354107856750488
Epoch: 7631, Batch Gradient Norm: 29.60244429581672
Epoch: 7631, Batch Gradient Norm after: 22.36067801052258
Epoch 7632/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7385369181632996
Epoch: 7632, Batch Gradient Norm: 28.185655838931577
Epoch: 7632, Batch Gradient Norm after: 22.36067588394828
Epoch 7633/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7351993083953857
Epoch: 7633, Batch Gradient Norm: 29.609588338742316
Epoch: 7633, Batch Gradient Norm after: 22.360678848751434
Epoch 7634/10000, Prediction Accuracy = 60.122%, Loss = 0.7383175849914551
Epoch: 7634, Batch Gradient Norm: 28.180111250118404
Epoch: 7634, Batch Gradient Norm after: 22.360678466289272
Epoch 7635/10000, Prediction Accuracy = 60.088%, Loss = 0.7350916385650634
Epoch: 7635, Batch Gradient Norm: 29.608368616004174
Epoch: 7635, Batch Gradient Norm after: 22.360679662826637
Epoch 7636/10000, Prediction Accuracy = 60.11199999999999%, Loss = 0.7382658123970032
Epoch: 7636, Batch Gradient Norm: 28.177158403666905
Epoch: 7636, Batch Gradient Norm after: 22.360676708190933
Epoch 7637/10000, Prediction Accuracy = 60.072%, Loss = 0.735029673576355
Epoch: 7637, Batch Gradient Norm: 29.601276477868915
Epoch: 7637, Batch Gradient Norm after: 22.360678315366368
Epoch 7638/10000, Prediction Accuracy = 60.11%, Loss = 0.738201630115509
Epoch: 7638, Batch Gradient Norm: 28.177008553060713
Epoch: 7638, Batch Gradient Norm after: 22.360677788755464
Epoch 7639/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7349487543106079
Epoch: 7639, Batch Gradient Norm: 29.60096253219903
Epoch: 7639, Batch Gradient Norm after: 22.36067648731231
Epoch 7640/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7382441282272338
Epoch: 7640, Batch Gradient Norm: 28.170204699557672
Epoch: 7640, Batch Gradient Norm after: 22.360677965927255
Epoch 7641/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7350277662277221
Epoch: 7641, Batch Gradient Norm: 29.597912521470104
Epoch: 7641, Batch Gradient Norm after: 22.360677686890305
Epoch 7642/10000, Prediction Accuracy = 60.102%, Loss = 0.7383208632469177
Epoch: 7642, Batch Gradient Norm: 28.155950140413577
Epoch: 7642, Batch Gradient Norm after: 22.36067487929455
Epoch 7643/10000, Prediction Accuracy = 60.148%, Loss = 0.7349489092826843
Epoch: 7643, Batch Gradient Norm: 29.594285444295963
Epoch: 7643, Batch Gradient Norm after: 22.360677263812363
Epoch 7644/10000, Prediction Accuracy = 60.164%, Loss = 0.7381126284599304
Epoch: 7644, Batch Gradient Norm: 28.157846355228116
Epoch: 7644, Batch Gradient Norm after: 22.36067731177992
Epoch 7645/10000, Prediction Accuracy = 60.124%, Loss = 0.7347682237625122
Epoch: 7645, Batch Gradient Norm: 29.595368470610435
Epoch: 7645, Batch Gradient Norm after: 22.36067877433789
Epoch 7646/10000, Prediction Accuracy = 60.104%, Loss = 0.7379378914833069
Epoch: 7646, Batch Gradient Norm: 28.15267379041507
Epoch: 7646, Batch Gradient Norm after: 22.3606773808884
Epoch 7647/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7346847653388977
Epoch: 7647, Batch Gradient Norm: 29.591686851662107
Epoch: 7647, Batch Gradient Norm after: 22.360678257162714
Epoch 7648/10000, Prediction Accuracy = 60.134%, Loss = 0.7378929257392883
Epoch: 7648, Batch Gradient Norm: 28.146845428303607
Epoch: 7648, Batch Gradient Norm after: 22.3606771704255
Epoch 7649/10000, Prediction Accuracy = 60.11%, Loss = 0.7345925211906433
Epoch: 7649, Batch Gradient Norm: 29.588934401412367
Epoch: 7649, Batch Gradient Norm after: 22.360677159845874
Epoch 7650/10000, Prediction Accuracy = 60.124%, Loss = 0.737848424911499
Epoch: 7650, Batch Gradient Norm: 28.143962697746893
Epoch: 7650, Batch Gradient Norm after: 22.36067720005891
Epoch 7651/10000, Prediction Accuracy = 60.116%, Loss = 0.7345454335212708
Epoch: 7651, Batch Gradient Norm: 29.590049240312272
Epoch: 7651, Batch Gradient Norm after: 22.360676355867163
Epoch 7652/10000, Prediction Accuracy = 60.166%, Loss = 0.7379200696945191
Epoch: 7652, Batch Gradient Norm: 28.131213083097343
Epoch: 7652, Batch Gradient Norm after: 22.360676342548384
Epoch 7653/10000, Prediction Accuracy = 60.166%, Loss = 0.7346218347549438
Epoch: 7653, Batch Gradient Norm: 29.5858612476015
Epoch: 7653, Batch Gradient Norm after: 22.360677453858063
Epoch 7654/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7379534363746643
Epoch: 7654, Batch Gradient Norm: 28.121509476616996
Epoch: 7654, Batch Gradient Norm after: 22.360676580716518
Epoch 7655/10000, Prediction Accuracy = 60.17%, Loss = 0.7345084428787232
Epoch: 7655, Batch Gradient Norm: 29.585321543754148
Epoch: 7655, Batch Gradient Norm after: 22.360679427250673
Epoch 7656/10000, Prediction Accuracy = 60.148%, Loss = 0.7377306461334229
Epoch: 7656, Batch Gradient Norm: 28.121022231407057
Epoch: 7656, Batch Gradient Norm after: 22.36067672860775
Epoch 7657/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7343358516693115
Epoch: 7657, Batch Gradient Norm: 29.586743965433502
Epoch: 7657, Batch Gradient Norm after: 22.360677577873922
Epoch 7658/10000, Prediction Accuracy = 60.11%, Loss = 0.7375783920288086
Epoch: 7658, Batch Gradient Norm: 28.119256507955512
Epoch: 7658, Batch Gradient Norm after: 22.3606788618534
Epoch 7659/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.7342689156532287
Epoch: 7659, Batch Gradient Norm: 29.584308259293703
Epoch: 7659, Batch Gradient Norm after: 22.36067861711436
Epoch 7660/10000, Prediction Accuracy = 60.114%, Loss = 0.7375414609909058
Epoch: 7660, Batch Gradient Norm: 28.110813999118424
Epoch: 7660, Batch Gradient Norm after: 22.360678177962114
Epoch 7661/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7341631650924683
Epoch: 7661, Batch Gradient Norm: 29.580378478396977
Epoch: 7661, Batch Gradient Norm after: 22.36067894161298
Epoch 7662/10000, Prediction Accuracy = 60.116%, Loss = 0.7375006198883056
Epoch: 7662, Batch Gradient Norm: 28.107386953595604
Epoch: 7662, Batch Gradient Norm after: 22.360676486677146
Epoch 7663/10000, Prediction Accuracy = 60.136%, Loss = 0.7341477990150451
Epoch: 7663, Batch Gradient Norm: 29.58237220679104
Epoch: 7663, Batch Gradient Norm after: 22.360679264196108
Epoch 7664/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7375991940498352
Epoch: 7664, Batch Gradient Norm: 28.095166338863237
Epoch: 7664, Batch Gradient Norm after: 22.360676159913236
Epoch 7665/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7342102408409119
Epoch: 7665, Batch Gradient Norm: 29.577599129946993
Epoch: 7665, Batch Gradient Norm after: 22.360680420942977
Epoch 7666/10000, Prediction Accuracy = 60.136%, Loss = 0.7375753283500671
Epoch: 7666, Batch Gradient Norm: 28.084243438584117
Epoch: 7666, Batch Gradient Norm after: 22.360677348754887
Epoch 7667/10000, Prediction Accuracy = 60.152%, Loss = 0.7340532898902893
Epoch: 7667, Batch Gradient Norm: 29.5765903302047
Epoch: 7667, Batch Gradient Norm after: 22.360678728684633
Epoch 7668/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7373432159423828
Epoch: 7668, Batch Gradient Norm: 28.085960063288006
Epoch: 7668, Batch Gradient Norm after: 22.360680810956993
Epoch 7669/10000, Prediction Accuracy = 60.138%, Loss = 0.7339066147804261
Epoch: 7669, Batch Gradient Norm: 29.575535663538762
Epoch: 7669, Batch Gradient Norm after: 22.360678941731187
Epoch 7670/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7372271776199341
Epoch: 7670, Batch Gradient Norm: 28.085076243960366
Epoch: 7670, Batch Gradient Norm after: 22.360680298342444
Epoch 7671/10000, Prediction Accuracy = 60.086%, Loss = 0.7338491916656494
Epoch: 7671, Batch Gradient Norm: 29.570010901186002
Epoch: 7671, Batch Gradient Norm after: 22.36067971752687
Epoch 7672/10000, Prediction Accuracy = 60.114%, Loss = 0.7371798396110535
Epoch: 7672, Batch Gradient Norm: 28.07955711592882
Epoch: 7672, Batch Gradient Norm after: 22.360677844348025
Epoch 7673/10000, Prediction Accuracy = 60.114%, Loss = 0.7337486624717713
Epoch: 7673, Batch Gradient Norm: 29.56961789503763
Epoch: 7673, Batch Gradient Norm after: 22.360678255334545
Epoch 7674/10000, Prediction Accuracy = 60.122%, Loss = 0.7371550917625427
Epoch: 7674, Batch Gradient Norm: 28.075641102113963
Epoch: 7674, Batch Gradient Norm after: 22.360676770253008
Epoch 7675/10000, Prediction Accuracy = 60.134%, Loss = 0.7337645292282104
Epoch: 7675, Batch Gradient Norm: 29.569654148711255
Epoch: 7675, Batch Gradient Norm after: 22.360678718637317
Epoch 7676/10000, Prediction Accuracy = 60.128%, Loss = 0.7372696280479432
Epoch: 7676, Batch Gradient Norm: 28.06524358639464
Epoch: 7676, Batch Gradient Norm after: 22.360677702357176
Epoch 7677/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.7338142633438111
Epoch: 7677, Batch Gradient Norm: 29.563283139750983
Epoch: 7677, Batch Gradient Norm after: 22.360677246755035
Epoch 7678/10000, Prediction Accuracy = 60.14%, Loss = 0.7371723771095275
Epoch: 7678, Batch Gradient Norm: 28.058030404826948
Epoch: 7678, Batch Gradient Norm after: 22.360674255898036
Epoch 7679/10000, Prediction Accuracy = 60.146%, Loss = 0.7336181759834289
Epoch: 7679, Batch Gradient Norm: 29.562687321341826
Epoch: 7679, Batch Gradient Norm after: 22.360676905479792
Epoch 7680/10000, Prediction Accuracy = 60.17%, Loss = 0.7369461297988892
Epoch: 7680, Batch Gradient Norm: 28.059332338637397
Epoch: 7680, Batch Gradient Norm after: 22.360678154929378
Epoch 7681/10000, Prediction Accuracy = 60.146%, Loss = 0.7335034132003784
Epoch: 7681, Batch Gradient Norm: 29.563931401985833
Epoch: 7681, Batch Gradient Norm after: 22.360678285131087
Epoch 7682/10000, Prediction Accuracy = 60.128%, Loss = 0.7368614673614502
Epoch: 7682, Batch Gradient Norm: 28.056396383520767
Epoch: 7682, Batch Gradient Norm after: 22.360677879004403
Epoch 7683/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.733445656299591
Epoch: 7683, Batch Gradient Norm: 29.557092649283906
Epoch: 7683, Batch Gradient Norm after: 22.360679579051745
Epoch 7684/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7368175983428955
Epoch: 7684, Batch Gradient Norm: 28.049553286726717
Epoch: 7684, Batch Gradient Norm after: 22.36067665119863
Epoch 7685/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7333467125892639
Epoch: 7685, Batch Gradient Norm: 29.557266668967948
Epoch: 7685, Batch Gradient Norm after: 22.360678397823058
Epoch 7686/10000, Prediction Accuracy = 60.17%, Loss = 0.7368165135383606
Epoch: 7686, Batch Gradient Norm: 28.042467299684358
Epoch: 7686, Batch Gradient Norm after: 22.36067703479406
Epoch 7687/10000, Prediction Accuracy = 60.162%, Loss = 0.7333882331848145
Epoch: 7687, Batch Gradient Norm: 29.557131792011933
Epoch: 7687, Batch Gradient Norm after: 22.360679051346185
Epoch 7688/10000, Prediction Accuracy = 60.12600000000001%, Loss = 0.7369297862052917
Epoch: 7688, Batch Gradient Norm: 28.025882712506377
Epoch: 7688, Batch Gradient Norm after: 22.36067521576995
Epoch 7689/10000, Prediction Accuracy = 60.188%, Loss = 0.7333908438682556
Epoch: 7689, Batch Gradient Norm: 29.551946144928756
Epoch: 7689, Batch Gradient Norm after: 22.360679026828198
Epoch 7690/10000, Prediction Accuracy = 60.14%, Loss = 0.7367900729179382
Epoch: 7690, Batch Gradient Norm: 28.01893224282306
Epoch: 7690, Batch Gradient Norm after: 22.360677044435192
Epoch 7691/10000, Prediction Accuracy = 60.148%, Loss = 0.7331830024719238
Epoch: 7691, Batch Gradient Norm: 29.55727528169231
Epoch: 7691, Batch Gradient Norm after: 22.360679247488285
Epoch 7692/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7365791916847229
Epoch: 7692, Batch Gradient Norm: 28.02058114887031
Epoch: 7692, Batch Gradient Norm after: 22.360678864042455
Epoch 7693/10000, Prediction Accuracy = 60.126%, Loss = 0.7330791831016541
Epoch: 7693, Batch Gradient Norm: 29.554961905972657
Epoch: 7693, Batch Gradient Norm after: 22.360678947275705
Epoch 7694/10000, Prediction Accuracy = 60.124%, Loss = 0.7365197896957397
Epoch: 7694, Batch Gradient Norm: 28.017787392968366
Epoch: 7694, Batch Gradient Norm after: 22.360676188066826
Epoch 7695/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.7330081939697266
Epoch: 7695, Batch Gradient Norm: 29.546930030115984
Epoch: 7695, Batch Gradient Norm after: 22.360679488134327
Epoch 7696/10000, Prediction Accuracy = 60.122%, Loss = 0.7364616394042969
Epoch: 7696, Batch Gradient Norm: 28.015476108419637
Epoch: 7696, Batch Gradient Norm after: 22.360678378286842
Epoch 7697/10000, Prediction Accuracy = 60.134%, Loss = 0.7329383373260498
Epoch: 7697, Batch Gradient Norm: 29.54545132468546
Epoch: 7697, Batch Gradient Norm after: 22.36067920444959
Epoch 7698/10000, Prediction Accuracy = 60.188%, Loss = 0.7364889502525329
Epoch: 7698, Batch Gradient Norm: 28.01138791416533
Epoch: 7698, Batch Gradient Norm after: 22.3606767140812
Epoch 7699/10000, Prediction Accuracy = 60.188%, Loss = 0.7330212593078613
Epoch: 7699, Batch Gradient Norm: 29.542049575525148
Epoch: 7699, Batch Gradient Norm after: 22.36068046985585
Epoch 7700/10000, Prediction Accuracy = 60.116%, Loss = 0.736566698551178
Epoch: 7700, Batch Gradient Norm: 27.998780257109917
Epoch: 7700, Batch Gradient Norm after: 22.360677952704656
Epoch 7701/10000, Prediction Accuracy = 60.17%, Loss = 0.732964015007019
Epoch: 7701, Batch Gradient Norm: 29.53862018899784
Epoch: 7701, Batch Gradient Norm after: 22.360677355290612
Epoch 7702/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7363697409629821
Epoch: 7702, Batch Gradient Norm: 27.99502382712892
Epoch: 7702, Batch Gradient Norm after: 22.36067800093247
Epoch 7703/10000, Prediction Accuracy = 60.146%, Loss = 0.7327686667442321
Epoch: 7703, Batch Gradient Norm: 29.541466114448323
Epoch: 7703, Batch Gradient Norm after: 22.360679802556685
Epoch 7704/10000, Prediction Accuracy = 60.138%, Loss = 0.7361962795257568
Epoch: 7704, Batch Gradient Norm: 27.99394668970963
Epoch: 7704, Batch Gradient Norm after: 22.36067904771265
Epoch 7705/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7326881527900696
Epoch: 7705, Batch Gradient Norm: 29.538573893901745
Epoch: 7705, Batch Gradient Norm after: 22.36067765143241
Epoch 7706/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.7361586213111877
Epoch: 7706, Batch Gradient Norm: 27.99147255807027
Epoch: 7706, Batch Gradient Norm after: 22.3606777354917
Epoch 7707/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7325992584228516
Epoch: 7707, Batch Gradient Norm: 29.53209125282689
Epoch: 7707, Batch Gradient Norm after: 22.360681117300064
Epoch 7708/10000, Prediction Accuracy = 60.124%, Loss = 0.7360961198806762
Epoch: 7708, Batch Gradient Norm: 27.989837173225368
Epoch: 7708, Batch Gradient Norm after: 22.360678075838635
Epoch 7709/10000, Prediction Accuracy = 60.15%, Loss = 0.7325592279434204
Epoch: 7709, Batch Gradient Norm: 29.53508248155202
Epoch: 7709, Batch Gradient Norm after: 22.36067890989132
Epoch 7710/10000, Prediction Accuracy = 60.19%, Loss = 0.7361753344535827
Epoch: 7710, Batch Gradient Norm: 27.98157738113981
Epoch: 7710, Batch Gradient Norm after: 22.360675687670625
Epoch 7711/10000, Prediction Accuracy = 60.19%, Loss = 0.7326528310775757
Epoch: 7711, Batch Gradient Norm: 29.529514843156615
Epoch: 7711, Batch Gradient Norm after: 22.36067877956956
Epoch 7712/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7361920237541199
Epoch: 7712, Batch Gradient Norm: 27.97532473420636
Epoch: 7712, Batch Gradient Norm after: 22.36067841745712
Epoch 7713/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7325241088867187
Epoch: 7713, Batch Gradient Norm: 29.523669089593156
Epoch: 7713, Batch Gradient Norm after: 22.360678121463092
Epoch 7714/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.735957396030426
Epoch: 7714, Batch Gradient Norm: 27.976923558099333
Epoch: 7714, Batch Gradient Norm after: 22.360675729353037
Epoch 7715/10000, Prediction Accuracy = 60.148%, Loss = 0.7323720812797546
Epoch: 7715, Batch Gradient Norm: 29.52573190028364
Epoch: 7715, Batch Gradient Norm after: 22.360677976902146
Epoch 7716/10000, Prediction Accuracy = 60.134%, Loss = 0.7358152031898498
Epoch: 7716, Batch Gradient Norm: 27.98545071026027
Epoch: 7716, Batch Gradient Norm after: 22.360678801510034
Epoch 7717/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7323103427886963
Epoch: 7717, Batch Gradient Norm: 29.515518221148962
Epoch: 7717, Batch Gradient Norm after: 22.3606783922353
Epoch 7718/10000, Prediction Accuracy = 60.148%, Loss = 0.7357592582702637
Epoch: 7718, Batch Gradient Norm: 27.990541978959634
Epoch: 7718, Batch Gradient Norm after: 22.360678340482135
Epoch 7719/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7322417616844177
Epoch: 7719, Batch Gradient Norm: 29.50884155298172
Epoch: 7719, Batch Gradient Norm after: 22.360680974996022
Epoch 7720/10000, Prediction Accuracy = 60.128%, Loss = 0.7357059001922608
Epoch: 7720, Batch Gradient Norm: 27.991537993363067
Epoch: 7720, Batch Gradient Norm after: 22.360675752304477
Epoch 7721/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7322420239448547
Epoch: 7721, Batch Gradient Norm: 29.50890700968991
Epoch: 7721, Batch Gradient Norm after: 22.360676988175587
Epoch 7722/10000, Prediction Accuracy = 60.162%, Loss = 0.7357954382896423
Epoch: 7722, Batch Gradient Norm: 27.986384691824874
Epoch: 7722, Batch Gradient Norm after: 22.360676394620818
Epoch 7723/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7323272824287415
Epoch: 7723, Batch Gradient Norm: 29.497696845388873
Epoch: 7723, Batch Gradient Norm after: 22.360680174233792
Epoch 7724/10000, Prediction Accuracy = 60.166%, Loss = 0.7357576131820679
Epoch: 7724, Batch Gradient Norm: 27.98215771833571
Epoch: 7724, Batch Gradient Norm after: 22.360677056940773
Epoch 7725/10000, Prediction Accuracy = 60.17%, Loss = 0.7321792244911194
Epoch: 7725, Batch Gradient Norm: 29.49510738503068
Epoch: 7725, Batch Gradient Norm after: 22.36067910617284
Epoch 7726/10000, Prediction Accuracy = 60.17%, Loss = 0.7355198621749878
Epoch: 7726, Batch Gradient Norm: 27.988327680121113
Epoch: 7726, Batch Gradient Norm after: 22.360677123355117
Epoch 7727/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7320435166358947
Epoch: 7727, Batch Gradient Norm: 29.495315746773244
Epoch: 7727, Batch Gradient Norm after: 22.3606774341547
Epoch 7728/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7354010462760925
Epoch: 7728, Batch Gradient Norm: 27.995560313372774
Epoch: 7728, Batch Gradient Norm after: 22.36067802704076
Epoch 7729/10000, Prediction Accuracy = 60.116%, Loss = 0.7319980382919311
Epoch: 7729, Batch Gradient Norm: 29.48419934976713
Epoch: 7729, Batch Gradient Norm after: 22.360677729549067
Epoch 7730/10000, Prediction Accuracy = 60.126%, Loss = 0.7353500843048095
Epoch: 7730, Batch Gradient Norm: 27.997510463620706
Epoch: 7730, Batch Gradient Norm after: 22.360678495049648
Epoch 7731/10000, Prediction Accuracy = 60.138%, Loss = 0.7319168210029602
Epoch: 7731, Batch Gradient Norm: 29.478421673633697
Epoch: 7731, Batch Gradient Norm after: 22.360677815184193
Epoch 7732/10000, Prediction Accuracy = 60.148%, Loss = 0.7353023409843444
Epoch: 7732, Batch Gradient Norm: 28.000922780928217
Epoch: 7732, Batch Gradient Norm after: 22.360676818157867
Epoch 7733/10000, Prediction Accuracy = 60.174%, Loss = 0.7319428086280823
Epoch: 7733, Batch Gradient Norm: 29.477620619026386
Epoch: 7733, Batch Gradient Norm after: 22.360681145447195
Epoch 7734/10000, Prediction Accuracy = 60.158%, Loss = 0.7354103565216065
Epoch: 7734, Batch Gradient Norm: 27.989362147030157
Epoch: 7734, Batch Gradient Norm after: 22.360676251245852
Epoch 7735/10000, Prediction Accuracy = 60.174%, Loss = 0.7320140600204468
Epoch: 7735, Batch Gradient Norm: 29.470295741651526
Epoch: 7735, Batch Gradient Norm after: 22.360678833834953
Epoch 7736/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7353366732597351
Epoch: 7736, Batch Gradient Norm: 27.984782165969698
Epoch: 7736, Batch Gradient Norm after: 22.36067783749916
Epoch 7737/10000, Prediction Accuracy = 60.182%, Loss = 0.7318377137184143
Epoch: 7737, Batch Gradient Norm: 29.467151347727008
Epoch: 7737, Batch Gradient Norm after: 22.36067895667995
Epoch 7738/10000, Prediction Accuracy = 60.164%, Loss = 0.7350968480110168
Epoch: 7738, Batch Gradient Norm: 27.997644127643845
Epoch: 7738, Batch Gradient Norm after: 22.360679707271963
Epoch 7739/10000, Prediction Accuracy = 60.162%, Loss = 0.7317102551460266
Epoch: 7739, Batch Gradient Norm: 29.46405086153033
Epoch: 7739, Batch Gradient Norm after: 22.360676854606748
Epoch 7740/10000, Prediction Accuracy = 60.15%, Loss = 0.7349934220314026
Epoch: 7740, Batch Gradient Norm: 27.998107602026
Epoch: 7740, Batch Gradient Norm after: 22.360677800035756
Epoch 7741/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7316721081733704
Epoch: 7741, Batch Gradient Norm: 29.455835569953646
Epoch: 7741, Batch Gradient Norm after: 22.36067812299735
Epoch 7742/10000, Prediction Accuracy = 60.15%, Loss = 0.7349399924278259
Epoch: 7742, Batch Gradient Norm: 28.001583514195136
Epoch: 7742, Batch Gradient Norm after: 22.360679980173753
Epoch 7743/10000, Prediction Accuracy = 60.15%, Loss = 0.7315893292427063
Epoch: 7743, Batch Gradient Norm: 29.44796139820198
Epoch: 7743, Batch Gradient Norm after: 22.36067948799777
Epoch 7744/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7349127054214477
Epoch: 7744, Batch Gradient Norm: 28.000677760765186
Epoch: 7744, Batch Gradient Norm after: 22.3606802917155
Epoch 7745/10000, Prediction Accuracy = 60.176%, Loss = 0.7316346883773803
Epoch: 7745, Batch Gradient Norm: 29.448547130327217
Epoch: 7745, Batch Gradient Norm after: 22.360677073246205
Epoch 7746/10000, Prediction Accuracy = 60.15%, Loss = 0.7350191712379456
Epoch: 7746, Batch Gradient Norm: 27.991827643038587
Epoch: 7746, Batch Gradient Norm after: 22.360675810044196
Epoch 7747/10000, Prediction Accuracy = 60.186%, Loss = 0.7316819667816162
Epoch: 7747, Batch Gradient Norm: 29.438852347347876
Epoch: 7747, Batch Gradient Norm after: 22.360676911359857
Epoch 7748/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7348928928375245
Epoch: 7748, Batch Gradient Norm: 27.99329885099541
Epoch: 7748, Batch Gradient Norm after: 22.36067733377189
Epoch 7749/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7314998149871826
Epoch: 7749, Batch Gradient Norm: 29.435331453677502
Epoch: 7749, Batch Gradient Norm after: 22.360680018206825
Epoch 7750/10000, Prediction Accuracy = 60.17%, Loss = 0.7346694111824036
Epoch: 7750, Batch Gradient Norm: 28.0028089087854
Epoch: 7750, Batch Gradient Norm after: 22.36067861079503
Epoch 7751/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7313893675804138
Epoch: 7751, Batch Gradient Norm: 29.43029198553536
Epoch: 7751, Batch Gradient Norm after: 22.36067764540032
Epoch 7752/10000, Prediction Accuracy = 60.17%, Loss = 0.7345887064933777
Epoch: 7752, Batch Gradient Norm: 28.005599101134752
Epoch: 7752, Batch Gradient Norm after: 22.360679546822208
Epoch 7753/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.731351101398468
Epoch: 7753, Batch Gradient Norm: 29.419815908374705
Epoch: 7753, Batch Gradient Norm after: 22.360678673066946
Epoch 7754/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7345261096954345
Epoch: 7754, Batch Gradient Norm: 28.00566316843046
Epoch: 7754, Batch Gradient Norm after: 22.360678771521368
Epoch 7755/10000, Prediction Accuracy = 60.134%, Loss = 0.7312708973884583
Epoch: 7755, Batch Gradient Norm: 29.41848220053823
Epoch: 7755, Batch Gradient Norm after: 22.360679703120734
Epoch 7756/10000, Prediction Accuracy = 60.21%, Loss = 0.7345304608345031
Epoch: 7756, Batch Gradient Norm: 28.008414153858894
Epoch: 7756, Batch Gradient Norm after: 22.36067792392401
Epoch 7757/10000, Prediction Accuracy = 60.198%, Loss = 0.7313535928726196
Epoch: 7757, Batch Gradient Norm: 29.412988827000348
Epoch: 7757, Batch Gradient Norm after: 22.36067856515106
Epoch 7758/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7346293687820434
Epoch: 7758, Batch Gradient Norm: 27.994298751109792
Epoch: 7758, Batch Gradient Norm after: 22.36067781870579
Epoch 7759/10000, Prediction Accuracy = 60.186%, Loss = 0.7313492178916932
Epoch: 7759, Batch Gradient Norm: 29.408215626852705
Epoch: 7759, Batch Gradient Norm after: 22.36067871816614
Epoch 7760/10000, Prediction Accuracy = 60.166%, Loss = 0.7344544053077697
Epoch: 7760, Batch Gradient Norm: 27.99678726988163
Epoch: 7760, Batch Gradient Norm after: 22.36067962163488
Epoch 7761/10000, Prediction Accuracy = 60.174%, Loss = 0.7311493754386902
Epoch: 7761, Batch Gradient Norm: 29.404678389127934
Epoch: 7761, Batch Gradient Norm after: 22.360678431332843
Epoch 7762/10000, Prediction Accuracy = 60.162%, Loss = 0.7342547416687012
Epoch: 7762, Batch Gradient Norm: 28.003467186964347
Epoch: 7762, Batch Gradient Norm after: 22.360677640195444
Epoch 7763/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7310664415359497
Epoch: 7763, Batch Gradient Norm: 29.39816205184398
Epoch: 7763, Batch Gradient Norm after: 22.360677867768715
Epoch 7764/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.7341882348060608
Epoch: 7764, Batch Gradient Norm: 28.007624318213757
Epoch: 7764, Batch Gradient Norm after: 22.360678963361895
Epoch 7765/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7310177206993103
Epoch: 7765, Batch Gradient Norm: 29.38679637061305
Epoch: 7765, Batch Gradient Norm after: 22.36068118490752
Epoch 7766/10000, Prediction Accuracy = 60.14%, Loss = 0.734122097492218
Epoch: 7766, Batch Gradient Norm: 28.006705686502144
Epoch: 7766, Batch Gradient Norm after: 22.360677226243716
Epoch 7767/10000, Prediction Accuracy = 60.178%, Loss = 0.7309603452682495
Epoch: 7767, Batch Gradient Norm: 29.390819220065747
Epoch: 7767, Batch Gradient Norm after: 22.3606766818957
Epoch 7768/10000, Prediction Accuracy = 60.20400000000001%, Loss = 0.7341519117355346
Epoch: 7768, Batch Gradient Norm: 28.007279485507876
Epoch: 7768, Batch Gradient Norm after: 22.360678057430945
Epoch 7769/10000, Prediction Accuracy = 60.222%, Loss = 0.7310537815093994
Epoch: 7769, Batch Gradient Norm: 29.382216171989903
Epoch: 7769, Batch Gradient Norm after: 22.36067711408693
Epoch 7770/10000, Prediction Accuracy = 60.146%, Loss = 0.7342272400856018
Epoch: 7770, Batch Gradient Norm: 27.996882371269038
Epoch: 7770, Batch Gradient Norm after: 22.360676459041528
Epoch 7771/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7309995889663696
Epoch: 7771, Batch Gradient Norm: 29.37748099337876
Epoch: 7771, Batch Gradient Norm after: 22.360678892375365
Epoch 7772/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7340103387832642
Epoch: 7772, Batch Gradient Norm: 28.00166021650726
Epoch: 7772, Batch Gradient Norm after: 22.360678992020382
Epoch 7773/10000, Prediction Accuracy = 60.186%, Loss = 0.7308119177818299
Epoch: 7773, Batch Gradient Norm: 29.375552122550626
Epoch: 7773, Batch Gradient Norm after: 22.360678078521786
Epoch 7774/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7338350296020508
Epoch: 7774, Batch Gradient Norm: 28.007479400768716
Epoch: 7774, Batch Gradient Norm after: 22.360676387763306
Epoch 7775/10000, Prediction Accuracy = 60.104%, Loss = 0.7307558417320251
Epoch: 7775, Batch Gradient Norm: 29.365731222167913
Epoch: 7775, Batch Gradient Norm after: 22.36067783670207
Epoch 7776/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7337827444076538
Epoch: 7776, Batch Gradient Norm: 28.007520159540764
Epoch: 7776, Batch Gradient Norm after: 22.360679112291425
Epoch 7777/10000, Prediction Accuracy = 60.146%, Loss = 0.7306830644607544
Epoch: 7777, Batch Gradient Norm: 29.359952345543892
Epoch: 7777, Batch Gradient Norm after: 22.36068137763713
Epoch 7778/10000, Prediction Accuracy = 60.148%, Loss = 0.733721125125885
Epoch: 7778, Batch Gradient Norm: 28.006318065609882
Epoch: 7778, Batch Gradient Norm after: 22.360677345063454
Epoch 7779/10000, Prediction Accuracy = 60.176%, Loss = 0.7306499600410461
Epoch: 7779, Batch Gradient Norm: 29.36060499946256
Epoch: 7779, Batch Gradient Norm after: 22.360678123962984
Epoch 7780/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7337867975234985
Epoch: 7780, Batch Gradient Norm: 27.99827926591652
Epoch: 7780, Batch Gradient Norm after: 22.360679365727666
Epoch 7781/10000, Prediction Accuracy = 60.238%, Loss = 0.7307317614555359
Epoch: 7781, Batch Gradient Norm: 29.354973558240157
Epoch: 7781, Batch Gradient Norm after: 22.36067673583106
Epoch 7782/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7338109493255616
Epoch: 7782, Batch Gradient Norm: 27.990390212430942
Epoch: 7782, Batch Gradient Norm after: 22.360675832564635
Epoch 7783/10000, Prediction Accuracy = 60.218%, Loss = 0.73062082529068
Epoch: 7783, Batch Gradient Norm: 29.35410633197112
Epoch: 7783, Batch Gradient Norm after: 22.360678308362463
Epoch 7784/10000, Prediction Accuracy = 60.212%, Loss = 0.733589768409729
Epoch: 7784, Batch Gradient Norm: 27.98863255029659
Epoch: 7784, Batch Gradient Norm after: 22.360676948010948
Epoch 7785/10000, Prediction Accuracy = 60.198%, Loss = 0.7304481267929077
Epoch: 7785, Batch Gradient Norm: 29.353174577585992
Epoch: 7785, Batch Gradient Norm after: 22.360678041061234
Epoch 7786/10000, Prediction Accuracy = 60.152%, Loss = 0.7334436058998108
Epoch: 7786, Batch Gradient Norm: 27.994603330198164
Epoch: 7786, Batch Gradient Norm after: 22.360678877277547
Epoch 7787/10000, Prediction Accuracy = 60.136%, Loss = 0.730395495891571
Epoch: 7787, Batch Gradient Norm: 29.343789059886333
Epoch: 7787, Batch Gradient Norm after: 22.360679439030626
Epoch 7788/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7333990931510925
Epoch: 7788, Batch Gradient Norm: 27.99199166392234
Epoch: 7788, Batch Gradient Norm after: 22.36067722590457
Epoch 7789/10000, Prediction Accuracy = 60.164%, Loss = 0.7303182005882263
Epoch: 7789, Batch Gradient Norm: 29.339144849683233
Epoch: 7789, Batch Gradient Norm after: 22.360682129466905
Epoch 7790/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.7333447933197021
Epoch: 7790, Batch Gradient Norm: 27.994999119573666
Epoch: 7790, Batch Gradient Norm after: 22.360676652716474
Epoch 7791/10000, Prediction Accuracy = 60.178%, Loss = 0.7303011298179627
Epoch: 7791, Batch Gradient Norm: 29.340229414208842
Epoch: 7791, Batch Gradient Norm after: 22.360681645929056
Epoch 7792/10000, Prediction Accuracy = 60.176%, Loss = 0.7334354162216187
Epoch: 7792, Batch Gradient Norm: 27.990002762491272
Epoch: 7792, Batch Gradient Norm after: 22.36067619285043
Epoch 7793/10000, Prediction Accuracy = 60.234%, Loss = 0.7303916335105896
Epoch: 7793, Batch Gradient Norm: 29.331289035666906
Epoch: 7793, Batch Gradient Norm after: 22.360679822985492
Epoch 7794/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7334118723869324
Epoch: 7794, Batch Gradient Norm: 27.978979891175758
Epoch: 7794, Batch Gradient Norm after: 22.360676816790047
Epoch 7795/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7302488923072815
Epoch: 7795, Batch Gradient Norm: 29.330546036220408
Epoch: 7795, Batch Gradient Norm after: 22.36067961149326
Epoch 7796/10000, Prediction Accuracy = 60.178%, Loss = 0.7331780076026917
Epoch: 7796, Batch Gradient Norm: 27.98913440152364
Epoch: 7796, Batch Gradient Norm after: 22.360679715640096
Epoch 7797/10000, Prediction Accuracy = 60.18599999999999%, Loss = 0.7300957322120667
Epoch: 7797, Batch Gradient Norm: 29.326856731957
Epoch: 7797, Batch Gradient Norm after: 22.360677368039095
Epoch 7798/10000, Prediction Accuracy = 60.166%, Loss = 0.7330477595329284
Epoch: 7798, Batch Gradient Norm: 27.988563472564742
Epoch: 7798, Batch Gradient Norm after: 22.360679741975616
Epoch 7799/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7300556421279907
Epoch: 7799, Batch Gradient Norm: 29.318002267331835
Epoch: 7799, Batch Gradient Norm after: 22.360680129941045
Epoch 7800/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7330092310905456
Epoch: 7800, Batch Gradient Norm: 27.987889911014005
Epoch: 7800, Batch Gradient Norm after: 22.360679996499144
Epoch 7801/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7299615025520325
Epoch: 7801, Batch Gradient Norm: 29.317696220646294
Epoch: 7801, Batch Gradient Norm after: 22.360680387948147
Epoch 7802/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7329618573188782
Epoch: 7802, Batch Gradient Norm: 27.98810575822072
Epoch: 7802, Batch Gradient Norm after: 22.360677068753496
Epoch 7803/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7299703240394593
Epoch: 7803, Batch Gradient Norm: 29.31573934314817
Epoch: 7803, Batch Gradient Norm after: 22.360678060905048
Epoch 7804/10000, Prediction Accuracy = 60.194%, Loss = 0.7330581188201905
Epoch: 7804, Batch Gradient Norm: 27.976393505573178
Epoch: 7804, Batch Gradient Norm after: 22.36067551020417
Epoch 7805/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7300332546234131
Epoch: 7805, Batch Gradient Norm: 29.311190770064677
Epoch: 7805, Batch Gradient Norm after: 22.36067873940798
Epoch 7806/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7330138921737671
Epoch: 7806, Batch Gradient Norm: 27.966300401577673
Epoch: 7806, Batch Gradient Norm after: 22.360677876064255
Epoch 7807/10000, Prediction Accuracy = 60.202%, Loss = 0.7298686265945434
Epoch: 7807, Batch Gradient Norm: 29.3092033330873
Epoch: 7807, Batch Gradient Norm after: 22.360679341241866
Epoch 7808/10000, Prediction Accuracy = 60.174%, Loss = 0.7327758073806763
Epoch: 7808, Batch Gradient Norm: 27.975306469351448
Epoch: 7808, Batch Gradient Norm after: 22.360678393061857
Epoch 7809/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7297315120697021
Epoch: 7809, Batch Gradient Norm: 29.304938089725525
Epoch: 7809, Batch Gradient Norm after: 22.36067945833745
Epoch 7810/10000, Prediction Accuracy = 60.186%, Loss = 0.7326815128326416
Epoch: 7810, Batch Gradient Norm: 27.975627077342754
Epoch: 7810, Batch Gradient Norm after: 22.360679155045464
Epoch 7811/10000, Prediction Accuracy = 60.114%, Loss = 0.7296857595443725
Epoch: 7811, Batch Gradient Norm: 29.29906326989431
Epoch: 7811, Batch Gradient Norm after: 22.360678408236137
Epoch 7812/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.732633638381958
Epoch: 7812, Batch Gradient Norm: 27.972346359374193
Epoch: 7812, Batch Gradient Norm after: 22.360678487624643
Epoch 7813/10000, Prediction Accuracy = 60.176%, Loss = 0.7295831203460693
Epoch: 7813, Batch Gradient Norm: 29.29521481847611
Epoch: 7813, Batch Gradient Norm after: 22.3606785573186
Epoch 7814/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7326073765754699
Epoch: 7814, Batch Gradient Norm: 27.969729383761546
Epoch: 7814, Batch Gradient Norm after: 22.36067827665056
Epoch 7815/10000, Prediction Accuracy = 60.188%, Loss = 0.7296029925346375
Epoch: 7815, Batch Gradient Norm: 29.295488119674605
Epoch: 7815, Batch Gradient Norm after: 22.36067762857467
Epoch 7816/10000, Prediction Accuracy = 60.164%, Loss = 0.7327316403388977
Epoch: 7816, Batch Gradient Norm: 27.952294080314022
Epoch: 7816, Batch Gradient Norm after: 22.360679692566514
Epoch 7817/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7296705484390259
Epoch: 7817, Batch Gradient Norm: 29.291386550557693
Epoch: 7817, Batch Gradient Norm after: 22.36067817652482
Epoch 7818/10000, Prediction Accuracy = 60.148%, Loss = 0.7326413512229919
Epoch: 7818, Batch Gradient Norm: 27.94973910546664
Epoch: 7818, Batch Gradient Norm after: 22.360677429301436
Epoch 7819/10000, Prediction Accuracy = 60.21400000000001%, Loss = 0.7294600963592529
Epoch: 7819, Batch Gradient Norm: 29.291520343672765
Epoch: 7819, Batch Gradient Norm after: 22.360679337865864
Epoch 7820/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.7323968291282654
Epoch: 7820, Batch Gradient Norm: 27.954980459516772
Epoch: 7820, Batch Gradient Norm after: 22.360678585223216
Epoch 7821/10000, Prediction Accuracy = 60.128%, Loss = 0.7293466448783874
Epoch: 7821, Batch Gradient Norm: 29.29039669673704
Epoch: 7821, Batch Gradient Norm after: 22.360677611301853
Epoch 7822/10000, Prediction Accuracy = 60.174%, Loss = 0.7323236107826233
Epoch: 7822, Batch Gradient Norm: 27.951742538976976
Epoch: 7822, Batch Gradient Norm after: 22.36068055722592
Epoch 7823/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7292903304100037
Epoch: 7823, Batch Gradient Norm: 29.28290900791893
Epoch: 7823, Batch Gradient Norm after: 22.36067792402795
Epoch 7824/10000, Prediction Accuracy = 60.178%, Loss = 0.7322680830955506
Epoch: 7824, Batch Gradient Norm: 27.943666143700984
Epoch: 7824, Batch Gradient Norm after: 22.360679486309678
Epoch 7825/10000, Prediction Accuracy = 60.15%, Loss = 0.7291924357414246
Epoch: 7825, Batch Gradient Norm: 29.284978420622632
Epoch: 7825, Batch Gradient Norm after: 22.360678021314374
Epoch 7826/10000, Prediction Accuracy = 60.218%, Loss = 0.732270348072052
Epoch: 7826, Batch Gradient Norm: 27.937453183695172
Epoch: 7826, Batch Gradient Norm after: 22.36067799597143
Epoch 7827/10000, Prediction Accuracy = 60.198%, Loss = 0.7292356491088867
Epoch: 7827, Batch Gradient Norm: 29.283003447956794
Epoch: 7827, Batch Gradient Norm after: 22.360676692492486
Epoch 7828/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7323710680007934
Epoch: 7828, Batch Gradient Norm: 27.92432432223568
Epoch: 7828, Batch Gradient Norm after: 22.36067712168339
Epoch 7829/10000, Prediction Accuracy = 60.214%, Loss = 0.7292366266250611
Epoch: 7829, Batch Gradient Norm: 29.27914640739156
Epoch: 7829, Batch Gradient Norm after: 22.36067882727164
Epoch 7830/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7322340607643127
Epoch: 7830, Batch Gradient Norm: 27.92240382212859
Epoch: 7830, Batch Gradient Norm after: 22.360679027452594
Epoch 7831/10000, Prediction Accuracy = 60.202%, Loss = 0.7290465235710144
Epoch: 7831, Batch Gradient Norm: 29.279946385206674
Epoch: 7831, Batch Gradient Norm after: 22.36068061906939
Epoch 7832/10000, Prediction Accuracy = 60.188%, Loss = 0.7320234894752502
Epoch: 7832, Batch Gradient Norm: 27.92830710215737
Epoch: 7832, Batch Gradient Norm after: 22.36067673798369
Epoch 7833/10000, Prediction Accuracy = 60.128%, Loss = 0.7289612054824829
Epoch: 7833, Batch Gradient Norm: 29.27793335098909
Epoch: 7833, Batch Gradient Norm after: 22.360680518059585
Epoch 7834/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.73196439743042
Epoch: 7834, Batch Gradient Norm: 27.92616056189396
Epoch: 7834, Batch Gradient Norm after: 22.36067763728558
Epoch 7835/10000, Prediction Accuracy = 60.138%, Loss = 0.7288942098617553
Epoch: 7835, Batch Gradient Norm: 29.269530772886142
Epoch: 7835, Batch Gradient Norm after: 22.36067786698115
Epoch 7836/10000, Prediction Accuracy = 60.174%, Loss = 0.7319062113761902
Epoch: 7836, Batch Gradient Norm: 27.922459298437406
Epoch: 7836, Batch Gradient Norm after: 22.360677098566544
Epoch 7837/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.7288165926933289
Epoch: 7837, Batch Gradient Norm: 29.272215219228542
Epoch: 7837, Batch Gradient Norm after: 22.360679749267604
Epoch 7838/10000, Prediction Accuracy = 60.21200000000001%, Loss = 0.7319354891777039
Epoch: 7838, Batch Gradient Norm: 27.919637710841815
Epoch: 7838, Batch Gradient Norm after: 22.36067820476853
Epoch 7839/10000, Prediction Accuracy = 60.238%, Loss = 0.7289027810096741
Epoch: 7839, Batch Gradient Norm: 29.26677010271344
Epoch: 7839, Batch Gradient Norm after: 22.360675956578643
Epoch 7840/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7320326089859008
Epoch: 7840, Batch Gradient Norm: 27.906751898931553
Epoch: 7840, Batch Gradient Norm after: 22.360677643134967
Epoch 7841/10000, Prediction Accuracy = 60.21%, Loss = 0.728874146938324
Epoch: 7841, Batch Gradient Norm: 29.261157463792276
Epoch: 7841, Batch Gradient Norm after: 22.360677636514428
Epoch 7842/10000, Prediction Accuracy = 60.174%, Loss = 0.7318265438079834
Epoch: 7842, Batch Gradient Norm: 27.909490045047356
Epoch: 7842, Batch Gradient Norm after: 22.360678624874687
Epoch 7843/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7286686897277832
Epoch: 7843, Batch Gradient Norm: 29.262550355069415
Epoch: 7843, Batch Gradient Norm after: 22.360678737638906
Epoch 7844/10000, Prediction Accuracy = 60.178%, Loss = 0.7316411733627319
Epoch: 7844, Batch Gradient Norm: 27.912103877037232
Epoch: 7844, Batch Gradient Norm after: 22.360678331710265
Epoch 7845/10000, Prediction Accuracy = 60.138%, Loss = 0.7286120414733886
Epoch: 7845, Batch Gradient Norm: 29.25608763306227
Epoch: 7845, Batch Gradient Norm after: 22.360677062103186
Epoch 7846/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7315987706184387
Epoch: 7846, Batch Gradient Norm: 27.91184033176057
Epoch: 7846, Batch Gradient Norm after: 22.360678107372735
Epoch 7847/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7285282850265503
Epoch: 7847, Batch Gradient Norm: 29.25133217806118
Epoch: 7847, Batch Gradient Norm after: 22.360678220596952
Epoch 7848/10000, Prediction Accuracy = 60.158%, Loss = 0.7315318346023559
Epoch: 7848, Batch Gradient Norm: 27.907334139128952
Epoch: 7848, Batch Gradient Norm after: 22.360677323226128
Epoch 7849/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7284755706787109
Epoch: 7849, Batch Gradient Norm: 29.253361372813412
Epoch: 7849, Batch Gradient Norm after: 22.360677612280497
Epoch 7850/10000, Prediction Accuracy = 60.162%, Loss = 0.7316010475158692
Epoch: 7850, Batch Gradient Norm: 27.90524572513265
Epoch: 7850, Batch Gradient Norm after: 22.360680723738735
Epoch 7851/10000, Prediction Accuracy = 60.249999999999986%, Loss = 0.7285704851150513
Epoch: 7851, Batch Gradient Norm: 29.245773588114684
Epoch: 7851, Batch Gradient Norm after: 22.36067466500955
Epoch 7852/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7316363692283631
Epoch: 7852, Batch Gradient Norm: 27.890661371075556
Epoch: 7852, Batch Gradient Norm after: 22.360678396959855
Epoch 7853/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.728468656539917
Epoch: 7853, Batch Gradient Norm: 29.24636424450041
Epoch: 7853, Batch Gradient Norm after: 22.36067684177862
Epoch 7854/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7314153790473938
Epoch: 7854, Batch Gradient Norm: 27.89726422730614
Epoch: 7854, Batch Gradient Norm after: 22.360679608313752
Epoch 7855/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7282968640327454
Epoch: 7855, Batch Gradient Norm: 29.24295945851544
Epoch: 7855, Batch Gradient Norm after: 22.360677742918924
Epoch 7856/10000, Prediction Accuracy = 60.2%, Loss = 0.7312639236450196
Epoch: 7856, Batch Gradient Norm: 27.90008521515617
Epoch: 7856, Batch Gradient Norm after: 22.360678499942605
Epoch 7857/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7282487154006958
Epoch: 7857, Batch Gradient Norm: 29.23609518947034
Epoch: 7857, Batch Gradient Norm after: 22.360679556097473
Epoch 7858/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7312211036682129
Epoch: 7858, Batch Gradient Norm: 27.90274487550015
Epoch: 7858, Batch Gradient Norm after: 22.36067935243741
Epoch 7859/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7281671047210694
Epoch: 7859, Batch Gradient Norm: 29.230931752830884
Epoch: 7859, Batch Gradient Norm after: 22.360679171271467
Epoch 7860/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7311617612838746
Epoch: 7860, Batch Gradient Norm: 27.899737951909543
Epoch: 7860, Batch Gradient Norm after: 22.360678781847124
Epoch 7861/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.7281453371047973
Epoch: 7861, Batch Gradient Norm: 29.23026788595949
Epoch: 7861, Batch Gradient Norm after: 22.36067652361368
Epoch 7862/10000, Prediction Accuracy = 60.148%, Loss = 0.7312584161758423
Epoch: 7862, Batch Gradient Norm: 27.893481398307976
Epoch: 7862, Batch Gradient Norm after: 22.36067996227486
Epoch 7863/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7282320022583008
Epoch: 7863, Batch Gradient Norm: 29.222645369407413
Epoch: 7863, Batch Gradient Norm after: 22.36067603875632
Epoch 7864/10000, Prediction Accuracy = 60.164%, Loss = 0.7312455534934997
Epoch: 7864, Batch Gradient Norm: 27.88125324251072
Epoch: 7864, Batch Gradient Norm after: 22.36067804471993
Epoch 7865/10000, Prediction Accuracy = 60.234%, Loss = 0.7280953049659729
Epoch: 7865, Batch Gradient Norm: 29.22427093865653
Epoch: 7865, Batch Gradient Norm after: 22.3606817488965
Epoch 7866/10000, Prediction Accuracy = 60.188%, Loss = 0.7310061573982238
Epoch: 7866, Batch Gradient Norm: 27.889164085536862
Epoch: 7866, Batch Gradient Norm after: 22.360678303521876
Epoch 7867/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7279340028762817
Epoch: 7867, Batch Gradient Norm: 29.222745535707947
Epoch: 7867, Batch Gradient Norm after: 22.36067869006019
Epoch 7868/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7308859467506409
Epoch: 7868, Batch Gradient Norm: 27.88844831675488
Epoch: 7868, Batch Gradient Norm after: 22.360678664342043
Epoch 7869/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.727893078327179
Epoch: 7869, Batch Gradient Norm: 29.214628012449445
Epoch: 7869, Batch Gradient Norm after: 22.360677784042263
Epoch 7870/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.7308444380760193
Epoch: 7870, Batch Gradient Norm: 27.890376562476288
Epoch: 7870, Batch Gradient Norm after: 22.36067752666716
Epoch 7871/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.727802062034607
Epoch: 7871, Batch Gradient Norm: 29.20910668070289
Epoch: 7871, Batch Gradient Norm after: 22.360679603558438
Epoch 7872/10000, Prediction Accuracy = 60.178%, Loss = 0.7307982087135315
Epoch: 7872, Batch Gradient Norm: 27.8886854085376
Epoch: 7872, Batch Gradient Norm after: 22.360678823496993
Epoch 7873/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7278151869773865
Epoch: 7873, Batch Gradient Norm: 29.210345336413344
Epoch: 7873, Batch Gradient Norm after: 22.360676291306604
Epoch 7874/10000, Prediction Accuracy = 60.158%, Loss = 0.7309178709983826
Epoch: 7874, Batch Gradient Norm: 27.876643344892106
Epoch: 7874, Batch Gradient Norm after: 22.360676698713895
Epoch 7875/10000, Prediction Accuracy = 60.234%, Loss = 0.7278846979141236
Epoch: 7875, Batch Gradient Norm: 29.20236212009974
Epoch: 7875, Batch Gradient Norm after: 22.360677621807024
Epoch 7876/10000, Prediction Accuracy = 60.162%, Loss = 0.73085618019104
Epoch: 7876, Batch Gradient Norm: 27.87402677765429
Epoch: 7876, Batch Gradient Norm after: 22.360676592894954
Epoch 7877/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.7277026295661926
Epoch: 7877, Batch Gradient Norm: 29.201623237024428
Epoch: 7877, Batch Gradient Norm after: 22.36067929795635
Epoch 7878/10000, Prediction Accuracy = 60.2%, Loss = 0.7306125164031982
Epoch: 7878, Batch Gradient Norm: 27.87301038189593
Epoch: 7878, Batch Gradient Norm after: 22.360677033405757
Epoch 7879/10000, Prediction Accuracy = 60.128%, Loss = 0.7275738596916199
Epoch: 7879, Batch Gradient Norm: 29.200608101097092
Epoch: 7879, Batch Gradient Norm after: 22.36067907269609
Epoch 7880/10000, Prediction Accuracy = 60.198%, Loss = 0.7305277585983276
Epoch: 7880, Batch Gradient Norm: 27.876785988125626
Epoch: 7880, Batch Gradient Norm after: 22.36068065919412
Epoch 7881/10000, Prediction Accuracy = 60.158%, Loss = 0.7275181412696838
Epoch: 7881, Batch Gradient Norm: 29.19211957092774
Epoch: 7881, Batch Gradient Norm after: 22.36067811996888
Epoch 7882/10000, Prediction Accuracy = 60.214%, Loss = 0.7304689288139343
Epoch: 7882, Batch Gradient Norm: 27.877751173278146
Epoch: 7882, Batch Gradient Norm after: 22.36067807419073
Epoch 7883/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7274415254592895
Epoch: 7883, Batch Gradient Norm: 29.189250903431038
Epoch: 7883, Batch Gradient Norm after: 22.360676563987937
Epoch 7884/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7304590582847595
Epoch: 7884, Batch Gradient Norm: 27.873170910944012
Epoch: 7884, Batch Gradient Norm after: 22.360677944152034
Epoch 7885/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7274924635887146
Epoch: 7885, Batch Gradient Norm: 29.18624979129999
Epoch: 7885, Batch Gradient Norm after: 22.36067833311842
Epoch 7886/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7305719375610351
Epoch: 7886, Batch Gradient Norm: 27.86572440312718
Epoch: 7886, Batch Gradient Norm after: 22.36067647424116
Epoch 7887/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7275209188461303
Epoch: 7887, Batch Gradient Norm: 29.17839412115369
Epoch: 7887, Batch Gradient Norm after: 22.360678420183543
Epoch 7888/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7304330348968506
Epoch: 7888, Batch Gradient Norm: 27.86646395766232
Epoch: 7888, Batch Gradient Norm after: 22.36067922130617
Epoch 7889/10000, Prediction Accuracy = 60.20400000000001%, Loss = 0.7273323059082031
Epoch: 7889, Batch Gradient Norm: 29.177456754109354
Epoch: 7889, Batch Gradient Norm after: 22.360678861026713
Epoch 7890/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.7302108526229858
Epoch: 7890, Batch Gradient Norm: 27.877178924249815
Epoch: 7890, Batch Gradient Norm after: 22.36067884442141
Epoch 7891/10000, Prediction Accuracy = 60.136%, Loss = 0.7272521018981933
Epoch: 7891, Batch Gradient Norm: 29.175355049603304
Epoch: 7891, Batch Gradient Norm after: 22.36067940961976
Epoch 7892/10000, Prediction Accuracy = 60.19799999999999%, Loss = 0.7301400661468506
Epoch: 7892, Batch Gradient Norm: 27.877795225631825
Epoch: 7892, Batch Gradient Norm after: 22.3606820808628
Epoch 7893/10000, Prediction Accuracy = 60.136%, Loss = 0.7271986603736877
Epoch: 7893, Batch Gradient Norm: 29.165200924688623
Epoch: 7893, Batch Gradient Norm after: 22.360678404266608
Epoch 7894/10000, Prediction Accuracy = 60.198%, Loss = 0.7300832033157348
Epoch: 7894, Batch Gradient Norm: 27.88024956491722
Epoch: 7894, Batch Gradient Norm after: 22.360678159352638
Epoch 7895/10000, Prediction Accuracy = 60.158%, Loss = 0.7271201491355896
Epoch: 7895, Batch Gradient Norm: 29.163908088420527
Epoch: 7895, Batch Gradient Norm after: 22.360677479502765
Epoch 7896/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.7300921201705932
Epoch: 7896, Batch Gradient Norm: 27.879227238877053
Epoch: 7896, Batch Gradient Norm after: 22.36067724541831
Epoch 7897/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.727204418182373
Epoch: 7897, Batch Gradient Norm: 29.16028699305589
Epoch: 7897, Batch Gradient Norm after: 22.36067704019293
Epoch 7898/10000, Prediction Accuracy = 60.164%, Loss = 0.7302067399024963
Epoch: 7898, Batch Gradient Norm: 27.862943212129373
Epoch: 7898, Batch Gradient Norm after: 22.36067710742703
Epoch 7899/10000, Prediction Accuracy = 60.222%, Loss = 0.7271936774253845
Epoch: 7899, Batch Gradient Norm: 29.153968505329683
Epoch: 7899, Batch Gradient Norm after: 22.360676715802725
Epoch 7900/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7300179123878479
Epoch: 7900, Batch Gradient Norm: 27.86630000175679
Epoch: 7900, Batch Gradient Norm after: 22.360677078915494
Epoch 7901/10000, Prediction Accuracy = 60.202%, Loss = 0.7269886136054993
Epoch: 7901, Batch Gradient Norm: 29.156138833369035
Epoch: 7901, Batch Gradient Norm after: 22.360679843965727
Epoch 7902/10000, Prediction Accuracy = 60.188%, Loss = 0.7298121452331543
Epoch: 7902, Batch Gradient Norm: 27.87446612240385
Epoch: 7902, Batch Gradient Norm after: 22.360680414395166
Epoch 7903/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7269191861152648
Epoch: 7903, Batch Gradient Norm: 29.14884853003158
Epoch: 7903, Batch Gradient Norm after: 22.360681059023158
Epoch 7904/10000, Prediction Accuracy = 60.174%, Loss = 0.7297593951225281
Epoch: 7904, Batch Gradient Norm: 27.870277757908397
Epoch: 7904, Batch Gradient Norm after: 22.360675890811557
Epoch 7905/10000, Prediction Accuracy = 60.122%, Loss = 0.7268596410751342
Epoch: 7905, Batch Gradient Norm: 29.142970678732045
Epoch: 7905, Batch Gradient Norm after: 22.36067771435492
Epoch 7906/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7297015428543091
Epoch: 7906, Batch Gradient Norm: 27.876280749635033
Epoch: 7906, Batch Gradient Norm after: 22.360677630902593
Epoch 7907/10000, Prediction Accuracy = 60.176%, Loss = 0.7268015503883362
Epoch: 7907, Batch Gradient Norm: 29.13905998935495
Epoch: 7907, Batch Gradient Norm after: 22.360678452554442
Epoch 7908/10000, Prediction Accuracy = 60.188%, Loss = 0.729728353023529
Epoch: 7908, Batch Gradient Norm: 27.87148997557855
Epoch: 7908, Batch Gradient Norm after: 22.360677844899467
Epoch 7909/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.726900291442871
Epoch: 7909, Batch Gradient Norm: 29.133087661008076
Epoch: 7909, Batch Gradient Norm after: 22.36067679276573
Epoch 7910/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7298018455505371
Epoch: 7910, Batch Gradient Norm: 27.867826484066338
Epoch: 7910, Batch Gradient Norm after: 22.360675286217806
Epoch 7911/10000, Prediction Accuracy = 60.21%, Loss = 0.7268479347229004
Epoch: 7911, Batch Gradient Norm: 29.126451236682993
Epoch: 7911, Batch Gradient Norm after: 22.360675376151736
Epoch 7912/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.7295837044715882
Epoch: 7912, Batch Gradient Norm: 27.87329365749355
Epoch: 7912, Batch Gradient Norm after: 22.360676234818463
Epoch 7913/10000, Prediction Accuracy = 60.188%, Loss = 0.7266642689704895
Epoch: 7913, Batch Gradient Norm: 29.12759020825817
Epoch: 7913, Batch Gradient Norm after: 22.36067652871232
Epoch 7914/10000, Prediction Accuracy = 60.212%, Loss = 0.7294139981269836
Epoch: 7914, Batch Gradient Norm: 27.880003962026187
Epoch: 7914, Batch Gradient Norm after: 22.36067872907379
Epoch 7915/10000, Prediction Accuracy = 60.146%, Loss = 0.7266180038452148
Epoch: 7915, Batch Gradient Norm: 29.11932443571101
Epoch: 7915, Batch Gradient Norm after: 22.36067683469729
Epoch 7916/10000, Prediction Accuracy = 60.17%, Loss = 0.7293743014335632
Epoch: 7916, Batch Gradient Norm: 27.87403425359606
Epoch: 7916, Batch Gradient Norm after: 22.360678914626707
Epoch 7917/10000, Prediction Accuracy = 60.146%, Loss = 0.7265307784080506
Epoch: 7917, Batch Gradient Norm: 29.115868339952325
Epoch: 7917, Batch Gradient Norm after: 22.360677687683605
Epoch 7918/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7293151617050171
Epoch: 7918, Batch Gradient Norm: 27.878730456787604
Epoch: 7918, Batch Gradient Norm after: 22.360678989958785
Epoch 7919/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.726505446434021
Epoch: 7919, Batch Gradient Norm: 29.11528069251721
Epoch: 7919, Batch Gradient Norm after: 22.360678148099034
Epoch 7920/10000, Prediction Accuracy = 60.152%, Loss = 0.7293856382369995
Epoch: 7920, Batch Gradient Norm: 27.867632919743443
Epoch: 7920, Batch Gradient Norm after: 22.360677259994247
Epoch 7921/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7265936851501464
Epoch: 7921, Batch Gradient Norm: 29.110747289957565
Epoch: 7921, Batch Gradient Norm after: 22.360678070459127
Epoch 7922/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.7294251322746277
Epoch: 7922, Batch Gradient Norm: 27.860431468851115
Epoch: 7922, Batch Gradient Norm after: 22.360675762265757
Epoch 7923/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7264807343482971
Epoch: 7923, Batch Gradient Norm: 29.107437259606154
Epoch: 7923, Batch Gradient Norm after: 22.36067611867863
Epoch 7924/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.729169762134552
Epoch: 7924, Batch Gradient Norm: 27.863043061722937
Epoch: 7924, Batch Gradient Norm after: 22.360678834562954
Epoch 7925/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.7263060212135315
Epoch: 7925, Batch Gradient Norm: 29.109685209974252
Epoch: 7925, Batch Gradient Norm after: 22.36067669934777
Epoch 7926/10000, Prediction Accuracy = 60.202%, Loss = 0.7290352702140808
Epoch: 7926, Batch Gradient Norm: 27.866303241064553
Epoch: 7926, Batch Gradient Norm after: 22.360678245714286
Epoch 7927/10000, Prediction Accuracy = 60.124%, Loss = 0.7262618541717529
Epoch: 7927, Batch Gradient Norm: 29.10012459535445
Epoch: 7927, Batch Gradient Norm after: 22.36067688686623
Epoch 7928/10000, Prediction Accuracy = 60.174%, Loss = 0.7289992809295655
Epoch: 7928, Batch Gradient Norm: 27.860964908597815
Epoch: 7928, Batch Gradient Norm after: 22.36067911213723
Epoch 7929/10000, Prediction Accuracy = 60.17600000000001%, Loss = 0.7261671781539917
Epoch: 7929, Batch Gradient Norm: 29.09845400503682
Epoch: 7929, Batch Gradient Norm after: 22.36067589683882
Epoch 7930/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7289535522460937
Epoch: 7930, Batch Gradient Norm: 27.863813135317102
Epoch: 7930, Batch Gradient Norm after: 22.360678446471013
Epoch 7931/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7261762857437134
Epoch: 7931, Batch Gradient Norm: 29.100066017518312
Epoch: 7931, Batch Gradient Norm after: 22.36067798810157
Epoch 7932/10000, Prediction Accuracy = 60.164%, Loss = 0.7290530443191529
Epoch: 7932, Batch Gradient Norm: 27.853457936984935
Epoch: 7932, Batch Gradient Norm after: 22.360677760889157
Epoch 7933/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7262399792671204
Epoch: 7933, Batch Gradient Norm: 29.092229417637057
Epoch: 7933, Batch Gradient Norm after: 22.360678687934758
Epoch 7934/10000, Prediction Accuracy = 60.174%, Loss = 0.7290117740631104
Epoch: 7934, Batch Gradient Norm: 27.84730638475343
Epoch: 7934, Batch Gradient Norm after: 22.360674438811316
Epoch 7935/10000, Prediction Accuracy = 60.234%, Loss = 0.7260753154754639
Epoch: 7935, Batch Gradient Norm: 29.092229006381313
Epoch: 7935, Batch Gradient Norm after: 22.360678449825865
Epoch 7936/10000, Prediction Accuracy = 60.17%, Loss = 0.7287656545639039
Epoch: 7936, Batch Gradient Norm: 27.845063551783838
Epoch: 7936, Batch Gradient Norm after: 22.360677125488834
Epoch 7937/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7259381055831909
Epoch: 7937, Batch Gradient Norm: 29.09343100958422
Epoch: 7937, Batch Gradient Norm after: 22.360678692412645
Epoch 7938/10000, Prediction Accuracy = 60.188%, Loss = 0.728682017326355
Epoch: 7938, Batch Gradient Norm: 27.84517258243442
Epoch: 7938, Batch Gradient Norm after: 22.36067846701578
Epoch 7939/10000, Prediction Accuracy = 60.146%, Loss = 0.7258950352668763
Epoch: 7939, Batch Gradient Norm: 29.084053846835104
Epoch: 7939, Batch Gradient Norm after: 22.36067704073461
Epoch 7940/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7286352276802063
Epoch: 7940, Batch Gradient Norm: 27.843173605610186
Epoch: 7940, Batch Gradient Norm after: 22.360678954204733
Epoch 7941/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7257938861846924
Epoch: 7941, Batch Gradient Norm: 29.082733954908093
Epoch: 7941, Batch Gradient Norm after: 22.36067693332039
Epoch 7942/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7286096215248108
Epoch: 7942, Batch Gradient Norm: 27.84560214941133
Epoch: 7942, Batch Gradient Norm after: 22.360680407053525
Epoch 7943/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7258269309997558
Epoch: 7943, Batch Gradient Norm: 29.081209612896096
Epoch: 7943, Batch Gradient Norm after: 22.360676170046297
Epoch 7944/10000, Prediction Accuracy = 60.152%, Loss = 0.7287263154983521
Epoch: 7944, Batch Gradient Norm: 27.840578682253252
Epoch: 7944, Batch Gradient Norm after: 22.360676947399366
Epoch 7945/10000, Prediction Accuracy = 60.242%, Loss = 0.7259045600891113
Epoch: 7945, Batch Gradient Norm: 29.0716113406375
Epoch: 7945, Batch Gradient Norm after: 22.3606782946561
Epoch 7946/10000, Prediction Accuracy = 60.148%, Loss = 0.7286337733268737
Epoch: 7946, Batch Gradient Norm: 27.832703590488382
Epoch: 7946, Batch Gradient Norm after: 22.360677730722138
Epoch 7947/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7257170915603638
Epoch: 7947, Batch Gradient Norm: 29.074552299588746
Epoch: 7947, Batch Gradient Norm after: 22.360677620696396
Epoch 7948/10000, Prediction Accuracy = 60.2%, Loss = 0.7283840179443359
Epoch: 7948, Batch Gradient Norm: 27.840405174325927
Epoch: 7948, Batch Gradient Norm after: 22.36067809135783
Epoch 7949/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7255938410758972
Epoch: 7949, Batch Gradient Norm: 29.07117146134893
Epoch: 7949, Batch Gradient Norm after: 22.36067614869159
Epoch 7950/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7283034205436707
Epoch: 7950, Batch Gradient Norm: 27.841849963978394
Epoch: 7950, Batch Gradient Norm after: 22.360679927118316
Epoch 7951/10000, Prediction Accuracy = 60.162%, Loss = 0.7255598664283752
Epoch: 7951, Batch Gradient Norm: 29.062674304659254
Epoch: 7951, Batch Gradient Norm after: 22.360677197070228
Epoch 7952/10000, Prediction Accuracy = 60.186%, Loss = 0.7282514095306396
Epoch: 7952, Batch Gradient Norm: 27.8458047429262
Epoch: 7952, Batch Gradient Norm after: 22.360678812263924
Epoch 7953/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7254727363586426
Epoch: 7953, Batch Gradient Norm: 29.0603692203915
Epoch: 7953, Batch Gradient Norm after: 22.360677755388885
Epoch 7954/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.728235948085785
Epoch: 7954, Batch Gradient Norm: 27.846491758457613
Epoch: 7954, Batch Gradient Norm after: 22.360680012027746
Epoch 7955/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7255431652069092
Epoch: 7955, Batch Gradient Norm: 29.059031145299514
Epoch: 7955, Batch Gradient Norm after: 22.360678907556654
Epoch 7956/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7283486485481262
Epoch: 7956, Batch Gradient Norm: 27.84103270555838
Epoch: 7956, Batch Gradient Norm after: 22.360676641364478
Epoch 7957/10000, Prediction Accuracy = 60.218%, Loss = 0.7255699634552002
Epoch: 7957, Batch Gradient Norm: 29.049460105103396
Epoch: 7957, Batch Gradient Norm after: 22.360679091779524
Epoch 7958/10000, Prediction Accuracy = 60.138%, Loss = 0.7282037138938904
Epoch: 7958, Batch Gradient Norm: 27.839755971461855
Epoch: 7958, Batch Gradient Norm after: 22.360677960359762
Epoch 7959/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7253819227218627
Epoch: 7959, Batch Gradient Norm: 29.050513894947848
Epoch: 7959, Batch Gradient Norm after: 22.36067679219038
Epoch 7960/10000, Prediction Accuracy = 60.178%, Loss = 0.7279776930809021
Epoch: 7960, Batch Gradient Norm: 27.84435238567192
Epoch: 7960, Batch Gradient Norm after: 22.360679288214794
Epoch 7961/10000, Prediction Accuracy = 60.162%, Loss = 0.7252948760986329
Epoch: 7961, Batch Gradient Norm: 29.043454870356726
Epoch: 7961, Batch Gradient Norm after: 22.360677756552843
Epoch 7962/10000, Prediction Accuracy = 60.176%, Loss = 0.7279260396957398
Epoch: 7962, Batch Gradient Norm: 27.842650735120973
Epoch: 7962, Batch Gradient Norm after: 22.360678253010988
Epoch 7963/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7252484202384949
Epoch: 7963, Batch Gradient Norm: 29.03528514269224
Epoch: 7963, Batch Gradient Norm after: 22.360678843380036
Epoch 7964/10000, Prediction Accuracy = 60.202%, Loss = 0.7278578400611877
Epoch: 7964, Batch Gradient Norm: 27.84763700281505
Epoch: 7964, Batch Gradient Norm after: 22.360678747271702
Epoch 7965/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.7251685976982116
Epoch: 7965, Batch Gradient Norm: 29.03520112077447
Epoch: 7965, Batch Gradient Norm after: 22.360675966720624
Epoch 7966/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7278754711151123
Epoch: 7966, Batch Gradient Norm: 27.845666473919362
Epoch: 7966, Batch Gradient Norm after: 22.360679684776805
Epoch 7967/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7252561807632446
Epoch: 7967, Batch Gradient Norm: 29.03094211027962
Epoch: 7967, Batch Gradient Norm after: 22.360677146596945
Epoch 7968/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.727995765209198
Epoch: 7968, Batch Gradient Norm: 27.830978472342306
Epoch: 7968, Batch Gradient Norm after: 22.360675484635493
Epoch 7969/10000, Prediction Accuracy = 60.20400000000001%, Loss = 0.7252370595932007
Epoch: 7969, Batch Gradient Norm: 29.02564508190294
Epoch: 7969, Batch Gradient Norm after: 22.360677735653248
Epoch 7970/10000, Prediction Accuracy = 60.146%, Loss = 0.7277823567390442
Epoch: 7970, Batch Gradient Norm: 27.834103049562444
Epoch: 7970, Batch Gradient Norm after: 22.360677366232338
Epoch 7971/10000, Prediction Accuracy = 60.198%, Loss = 0.7250264286994934
Epoch: 7971, Batch Gradient Norm: 29.026353756889872
Epoch: 7971, Batch Gradient Norm after: 22.360678163598024
Epoch 7972/10000, Prediction Accuracy = 60.194%, Loss = 0.7275888204574585
Epoch: 7972, Batch Gradient Norm: 27.840177708604767
Epoch: 7972, Batch Gradient Norm after: 22.360677759290347
Epoch 7973/10000, Prediction Accuracy = 60.15%, Loss = 0.7249770283699035
Epoch: 7973, Batch Gradient Norm: 29.016789471707746
Epoch: 7973, Batch Gradient Norm after: 22.36067947890491
Epoch 7974/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7275555968284607
Epoch: 7974, Batch Gradient Norm: 27.83853998522547
Epoch: 7974, Batch Gradient Norm after: 22.360677300188787
Epoch 7975/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7248980164527893
Epoch: 7975, Batch Gradient Norm: 29.012117200232126
Epoch: 7975, Batch Gradient Norm after: 22.360676939201547
Epoch 7976/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7274900555610657
Epoch: 7976, Batch Gradient Norm: 27.83885731677767
Epoch: 7976, Batch Gradient Norm after: 22.36067769492521
Epoch 7977/10000, Prediction Accuracy = 60.188%, Loss = 0.7248523473739624
Epoch: 7977, Batch Gradient Norm: 29.01535368489775
Epoch: 7977, Batch Gradient Norm after: 22.360677868027814
Epoch 7978/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7275488138198852
Epoch: 7978, Batch Gradient Norm: 27.831576610442344
Epoch: 7978, Batch Gradient Norm after: 22.360678030117825
Epoch 7979/10000, Prediction Accuracy = 60.23%, Loss = 0.7249394536018372
Epoch: 7979, Batch Gradient Norm: 29.008921721208598
Epoch: 7979, Batch Gradient Norm after: 22.360675289443563
Epoch 7980/10000, Prediction Accuracy = 60.178%, Loss = 0.7276081442832947
Epoch: 7980, Batch Gradient Norm: 27.82386336035076
Epoch: 7980, Batch Gradient Norm after: 22.360677427411545
Epoch 7981/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7248498678207398
Epoch: 7981, Batch Gradient Norm: 29.005381250519655
Epoch: 7981, Batch Gradient Norm after: 22.36067744175699
Epoch 7982/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.7273530483245849
Epoch: 7982, Batch Gradient Norm: 27.82418225740291
Epoch: 7982, Batch Gradient Norm after: 22.360679069817714
Epoch 7983/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7246694684028625
Epoch: 7983, Batch Gradient Norm: 29.006307949003116
Epoch: 7983, Batch Gradient Norm after: 22.360678728427185
Epoch 7984/10000, Prediction Accuracy = 60.186%, Loss = 0.7272273182868958
Epoch: 7984, Batch Gradient Norm: 27.82956134166252
Epoch: 7984, Batch Gradient Norm after: 22.36067936865524
Epoch 7985/10000, Prediction Accuracy = 60.166%, Loss = 0.7246293783187866
Epoch: 7985, Batch Gradient Norm: 28.995975235101195
Epoch: 7985, Batch Gradient Norm after: 22.360679434634488
Epoch 7986/10000, Prediction Accuracy = 60.164%, Loss = 0.7271805763244629
Epoch: 7986, Batch Gradient Norm: 27.829817414562115
Epoch: 7986, Batch Gradient Norm after: 22.36067948693368
Epoch 7987/10000, Prediction Accuracy = 60.17%, Loss = 0.7245419740676879
Epoch: 7987, Batch Gradient Norm: 28.993647107928442
Epoch: 7987, Batch Gradient Norm after: 22.36067830583719
Epoch 7988/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7271319270133972
Epoch: 7988, Batch Gradient Norm: 27.831020750456325
Epoch: 7988, Batch Gradient Norm after: 22.360676442552233
Epoch 7989/10000, Prediction Accuracy = 60.19%, Loss = 0.7245266556739807
Epoch: 7989, Batch Gradient Norm: 28.994171671954376
Epoch: 7989, Batch Gradient Norm after: 22.360677767360162
Epoch 7990/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7272262692451477
Epoch: 7990, Batch Gradient Norm: 27.817812054746337
Epoch: 7990, Batch Gradient Norm after: 22.360677451926243
Epoch 7991/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7246002435684205
Epoch: 7991, Batch Gradient Norm: 28.987519449895434
Epoch: 7991, Batch Gradient Norm after: 22.360677457448375
Epoch 7992/10000, Prediction Accuracy = 60.186%, Loss = 0.7272268652915954
Epoch: 7992, Batch Gradient Norm: 27.810963460023977
Epoch: 7992, Batch Gradient Norm after: 22.360675790962624
Epoch 7993/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7244655728340149
Epoch: 7993, Batch Gradient Norm: 28.98868255803577
Epoch: 7993, Batch Gradient Norm after: 22.36067701673538
Epoch 7994/10000, Prediction Accuracy = 60.182%, Loss = 0.7269703626632691
Epoch: 7994, Batch Gradient Norm: 27.810455291721418
Epoch: 7994, Batch Gradient Norm after: 22.360679261397458
Epoch 7995/10000, Prediction Accuracy = 60.174%, Loss = 0.7242974162101745
Epoch: 7995, Batch Gradient Norm: 28.98994161523193
Epoch: 7995, Batch Gradient Norm after: 22.360678071769673
Epoch 7996/10000, Prediction Accuracy = 60.176%, Loss = 0.7268701910972595
Epoch: 7996, Batch Gradient Norm: 27.808039521411466
Epoch: 7996, Batch Gradient Norm after: 22.36067907325323
Epoch 7997/10000, Prediction Accuracy = 60.166%, Loss = 0.7242619633674622
Epoch: 7997, Batch Gradient Norm: 28.98088959961267
Epoch: 7997, Batch Gradient Norm after: 22.360676522736068
Epoch 7998/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7268370270729065
Epoch: 7998, Batch Gradient Norm: 27.803486176708805
Epoch: 7998, Batch Gradient Norm after: 22.360677968136446
Epoch 7999/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7241443157196045
Epoch: 7999, Batch Gradient Norm: 28.983746561857732
Epoch: 7999, Batch Gradient Norm after: 22.360677144507637
Epoch 8000/10000, Prediction Accuracy = 60.206%, Loss = 0.7267918705940246
Epoch: 8000, Batch Gradient Norm: 27.795357954348823
Epoch: 8000, Batch Gradient Norm after: 22.360680001070282
Epoch 8001/10000, Prediction Accuracy = 60.206%, Loss = 0.7241515398025513
Epoch: 8001, Batch Gradient Norm: 28.985523372128682
Epoch: 8001, Batch Gradient Norm after: 22.360676556706803
Epoch 8002/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7269227623939514
Epoch: 8002, Batch Gradient Norm: 27.781646710663967
Epoch: 8002, Batch Gradient Norm after: 22.360678941459927
Epoch 8003/10000, Prediction Accuracy = 60.234%, Loss = 0.7241989612579346
Epoch: 8003, Batch Gradient Norm: 28.980129701898225
Epoch: 8003, Batch Gradient Norm after: 22.360679150910602
Epoch 8004/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7268665075302124
Epoch: 8004, Batch Gradient Norm: 27.76865165265155
Epoch: 8004, Batch Gradient Norm after: 22.36067688593588
Epoch 8005/10000, Prediction Accuracy = 60.21400000000001%, Loss = 0.7240149497985839
Epoch: 8005, Batch Gradient Norm: 28.982183064775622
Epoch: 8005, Batch Gradient Norm after: 22.360677829147132
Epoch 8006/10000, Prediction Accuracy = 60.178%, Loss = 0.726620888710022
Epoch: 8006, Batch Gradient Norm: 27.767011997577207
Epoch: 8006, Batch Gradient Norm after: 22.360676933073727
Epoch 8007/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.7238749027252197
Epoch: 8007, Batch Gradient Norm: 28.982626877460408
Epoch: 8007, Batch Gradient Norm after: 22.360674240804713
Epoch 8008/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7265434265136719
Epoch: 8008, Batch Gradient Norm: 27.770501051075367
Epoch: 8008, Batch Gradient Norm after: 22.360677518682724
Epoch 8009/10000, Prediction Accuracy = 60.158%, Loss = 0.7238390922546387
Epoch: 8009, Batch Gradient Norm: 28.974477821071076
Epoch: 8009, Batch Gradient Norm after: 22.36067701342794
Epoch 8010/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7264995813369751
Epoch: 8010, Batch Gradient Norm: 27.763485049098865
Epoch: 8010, Batch Gradient Norm after: 22.360679421995748
Epoch 8011/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7237311482429505
Epoch: 8011, Batch Gradient Norm: 28.974955883539877
Epoch: 8011, Batch Gradient Norm after: 22.360677035275934
Epoch 8012/10000, Prediction Accuracy = 60.21%, Loss = 0.7264700531959534
Epoch: 8012, Batch Gradient Norm: 27.761518903515174
Epoch: 8012, Batch Gradient Norm after: 22.360678006709247
Epoch 8013/10000, Prediction Accuracy = 60.222%, Loss = 0.7237553119659423
Epoch: 8013, Batch Gradient Norm: 28.976264321079274
Epoch: 8013, Batch Gradient Norm after: 22.360678722463703
Epoch 8014/10000, Prediction Accuracy = 60.19%, Loss = 0.7266006946563721
Epoch: 8014, Batch Gradient Norm: 27.748354188814822
Epoch: 8014, Batch Gradient Norm after: 22.36067894858636
Epoch 8015/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7237853884696961
Epoch: 8015, Batch Gradient Norm: 28.970716086411972
Epoch: 8015, Batch Gradient Norm after: 22.36067778004568
Epoch 8016/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7265039443969726
Epoch: 8016, Batch Gradient Norm: 27.738645281451912
Epoch: 8016, Batch Gradient Norm after: 22.36067872012745
Epoch 8017/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7235930919647217
Epoch: 8017, Batch Gradient Norm: 28.97428535324763
Epoch: 8017, Batch Gradient Norm after: 22.3606803209136
Epoch 8018/10000, Prediction Accuracy = 60.198%, Loss = 0.7262692451477051
Epoch: 8018, Batch Gradient Norm: 27.741029361534153
Epoch: 8018, Batch Gradient Norm after: 22.36067887137971
Epoch 8019/10000, Prediction Accuracy = 60.152%, Loss = 0.7234822511672974
Epoch: 8019, Batch Gradient Norm: 28.96998155324214
Epoch: 8019, Batch Gradient Norm after: 22.360677614731603
Epoch 8020/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7262024044990539
Epoch: 8020, Batch Gradient Norm: 27.741754349140578
Epoch: 8020, Batch Gradient Norm after: 22.360679291309367
Epoch 8021/10000, Prediction Accuracy = 60.148%, Loss = 0.7234483003616333
Epoch: 8021, Batch Gradient Norm: 28.96263473979312
Epoch: 8021, Batch Gradient Norm after: 22.360678761873213
Epoch 8022/10000, Prediction Accuracy = 60.17199999999999%, Loss = 0.726155698299408
Epoch: 8022, Batch Gradient Norm: 27.743157383914053
Epoch: 8022, Batch Gradient Norm after: 22.360677696765386
Epoch 8023/10000, Prediction Accuracy = 60.186%, Loss = 0.7233423113822937
Epoch: 8023, Batch Gradient Norm: 28.96207843914978
Epoch: 8023, Batch Gradient Norm after: 22.360676970772236
Epoch 8024/10000, Prediction Accuracy = 60.176%, Loss = 0.7261402249336243
Epoch: 8024, Batch Gradient Norm: 27.74247437411522
Epoch: 8024, Batch Gradient Norm after: 22.360678341028265
Epoch 8025/10000, Prediction Accuracy = 60.234%, Loss = 0.7234138250350952
Epoch: 8025, Batch Gradient Norm: 28.959029351836683
Epoch: 8025, Batch Gradient Norm after: 22.36067981123072
Epoch 8026/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7262777209281921
Epoch: 8026, Batch Gradient Norm: 27.72488985668978
Epoch: 8026, Batch Gradient Norm after: 22.360677053711246
Epoch 8027/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7234154343605042
Epoch: 8027, Batch Gradient Norm: 28.95888764876579
Epoch: 8027, Batch Gradient Norm after: 22.36067916387667
Epoch 8028/10000, Prediction Accuracy = 60.18599999999999%, Loss = 0.7261123895645142
Epoch: 8028, Batch Gradient Norm: 27.720045190651746
Epoch: 8028, Batch Gradient Norm after: 22.360679324225007
Epoch 8029/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7231955766677857
Epoch: 8029, Batch Gradient Norm: 28.960988439788
Epoch: 8029, Batch Gradient Norm after: 22.360678338921115
Epoch 8030/10000, Prediction Accuracy = 60.18800000000001%, Loss = 0.7259005546569824
Epoch: 8030, Batch Gradient Norm: 27.726078870710637
Epoch: 8030, Batch Gradient Norm after: 22.360675381743267
Epoch 8031/10000, Prediction Accuracy = 60.112%, Loss = 0.723127031326294
Epoch: 8031, Batch Gradient Norm: 28.954000091480324
Epoch: 8031, Batch Gradient Norm after: 22.36067745292888
Epoch 8032/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7258574366569519
Epoch: 8032, Batch Gradient Norm: 27.728201201143627
Epoch: 8032, Batch Gradient Norm after: 22.360675709014586
Epoch 8033/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7230772495269775
Epoch: 8033, Batch Gradient Norm: 28.947419145657673
Epoch: 8033, Batch Gradient Norm after: 22.360676482178793
Epoch 8034/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7257920980453492
Epoch: 8034, Batch Gradient Norm: 27.732242985998163
Epoch: 8034, Batch Gradient Norm after: 22.360677731534356
Epoch 8035/10000, Prediction Accuracy = 60.152%, Loss = 0.7229942321777344
Epoch: 8035, Batch Gradient Norm: 28.947001505550094
Epoch: 8035, Batch Gradient Norm after: 22.360678656145044
Epoch 8036/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7258268117904663
Epoch: 8036, Batch Gradient Norm: 27.72405103679013
Epoch: 8036, Batch Gradient Norm after: 22.360676580167347
Epoch 8037/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7230957269668579
Epoch: 8037, Batch Gradient Norm: 28.945877686261166
Epoch: 8037, Batch Gradient Norm after: 22.360677003727616
Epoch 8038/10000, Prediction Accuracy = 60.176%, Loss = 0.7259369850158691
Epoch: 8038, Batch Gradient Norm: 27.711682550382214
Epoch: 8038, Batch Gradient Norm after: 22.36067926616348
Epoch 8039/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7230230808258057
Epoch: 8039, Batch Gradient Norm: 28.941313873907106
Epoch: 8039, Batch Gradient Norm after: 22.360679819459005
Epoch 8040/10000, Prediction Accuracy = 60.162%, Loss = 0.7256982445716857
Epoch: 8040, Batch Gradient Norm: 27.711021105707815
Epoch: 8040, Batch Gradient Norm after: 22.360676330253426
Epoch 8041/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7228287935256958
Epoch: 8041, Batch Gradient Norm: 28.941651035982645
Epoch: 8041, Batch Gradient Norm after: 22.36067757819493
Epoch 8042/10000, Prediction Accuracy = 60.214%, Loss = 0.7255373954772949
Epoch: 8042, Batch Gradient Norm: 27.71683387858887
Epoch: 8042, Batch Gradient Norm after: 22.360678216609887
Epoch 8043/10000, Prediction Accuracy = 60.124%, Loss = 0.7227794289588928
Epoch: 8043, Batch Gradient Norm: 28.93444915052838
Epoch: 8043, Batch Gradient Norm after: 22.360677388693556
Epoch 8044/10000, Prediction Accuracy = 60.15%, Loss = 0.7254948854446411
Epoch: 8044, Batch Gradient Norm: 27.713760941799254
Epoch: 8044, Batch Gradient Norm after: 22.360677568717474
Epoch 8045/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7227126479148864
Epoch: 8045, Batch Gradient Norm: 28.92890872645738
Epoch: 8045, Batch Gradient Norm after: 22.360678116144662
Epoch 8046/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7254412770271301
Epoch: 8046, Batch Gradient Norm: 27.71867664764675
Epoch: 8046, Batch Gradient Norm after: 22.360676780748257
Epoch 8047/10000, Prediction Accuracy = 60.164%, Loss = 0.7226629137992859
Epoch: 8047, Batch Gradient Norm: 28.933335648827615
Epoch: 8047, Batch Gradient Norm after: 22.360675443053903
Epoch 8048/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7254991888999939
Epoch: 8048, Batch Gradient Norm: 27.708182075263885
Epoch: 8048, Batch Gradient Norm after: 22.360676970020858
Epoch 8049/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7227540016174316
Epoch: 8049, Batch Gradient Norm: 28.92849906566175
Epoch: 8049, Batch Gradient Norm after: 22.360678164066005
Epoch 8050/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.725545072555542
Epoch: 8050, Batch Gradient Norm: 27.696012574626643
Epoch: 8050, Batch Gradient Norm after: 22.360679172884677
Epoch 8051/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7226431965827942
Epoch: 8051, Batch Gradient Norm: 28.92721502176957
Epoch: 8051, Batch Gradient Norm after: 22.360678188305265
Epoch 8052/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7253129124641419
Epoch: 8052, Batch Gradient Norm: 27.69633088106758
Epoch: 8052, Batch Gradient Norm after: 22.360677454500827
Epoch 8053/10000, Prediction Accuracy = 60.21%, Loss = 0.722468888759613
Epoch: 8053, Batch Gradient Norm: 28.927531060841112
Epoch: 8053, Batch Gradient Norm after: 22.360678670731517
Epoch 8054/10000, Prediction Accuracy = 60.198%, Loss = 0.7251810193061828
Epoch: 8054, Batch Gradient Norm: 27.698784895947902
Epoch: 8054, Batch Gradient Norm after: 22.360676754518483
Epoch 8055/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7224315524101257
Epoch: 8055, Batch Gradient Norm: 28.919936445743414
Epoch: 8055, Batch Gradient Norm after: 22.360679413628596
Epoch 8056/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7251553535461426
Epoch: 8056, Batch Gradient Norm: 27.69666643367611
Epoch: 8056, Batch Gradient Norm after: 22.360677959245955
Epoch 8057/10000, Prediction Accuracy = 60.166%, Loss = 0.7223376750946044
Epoch: 8057, Batch Gradient Norm: 28.916482272346038
Epoch: 8057, Batch Gradient Norm after: 22.360674326962286
Epoch 8058/10000, Prediction Accuracy = 60.186%, Loss = 0.7250975847244263
Epoch: 8058, Batch Gradient Norm: 27.693562649005944
Epoch: 8058, Batch Gradient Norm after: 22.36067492054432
Epoch 8059/10000, Prediction Accuracy = 60.178%, Loss = 0.7223139524459838
Epoch: 8059, Batch Gradient Norm: 28.921345329805092
Epoch: 8059, Batch Gradient Norm after: 22.36067814155515
Epoch 8060/10000, Prediction Accuracy = 60.188%, Loss = 0.7252038598060608
Epoch: 8060, Batch Gradient Norm: 27.678830544817277
Epoch: 8060, Batch Gradient Norm after: 22.360677007023863
Epoch 8061/10000, Prediction Accuracy = 60.24799999999999%, Loss = 0.7223822712898255
Epoch: 8061, Batch Gradient Norm: 28.917453078041422
Epoch: 8061, Batch Gradient Norm after: 22.360678394906188
Epoch 8062/10000, Prediction Accuracy = 60.17%, Loss = 0.72521812915802
Epoch: 8062, Batch Gradient Norm: 27.667818941170655
Epoch: 8062, Batch Gradient Norm after: 22.36067554059679
Epoch 8063/10000, Prediction Accuracy = 60.188%, Loss = 0.722234845161438
Epoch: 8063, Batch Gradient Norm: 28.915194202786378
Epoch: 8063, Batch Gradient Norm after: 22.360679116019906
Epoch 8064/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.7249478459358215
Epoch: 8064, Batch Gradient Norm: 27.672627529608796
Epoch: 8064, Batch Gradient Norm after: 22.360680155702134
Epoch 8065/10000, Prediction Accuracy = 60.17%, Loss = 0.7220765829086304
Epoch: 8065, Batch Gradient Norm: 28.915160047720615
Epoch: 8065, Batch Gradient Norm after: 22.36067784731151
Epoch 8066/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7248434066772461
Epoch: 8066, Batch Gradient Norm: 27.671653794420042
Epoch: 8066, Batch Gradient Norm after: 22.360676179112613
Epoch 8067/10000, Prediction Accuracy = 60.146%, Loss = 0.7220451593399048
Epoch: 8067, Batch Gradient Norm: 28.909669006002673
Epoch: 8067, Batch Gradient Norm after: 22.36068024982903
Epoch 8068/10000, Prediction Accuracy = 60.14%, Loss = 0.7248108625411988
Epoch: 8068, Batch Gradient Norm: 27.669679911885858
Epoch: 8068, Batch Gradient Norm after: 22.360679868769356
Epoch 8069/10000, Prediction Accuracy = 60.164%, Loss = 0.7219395160675048
Epoch: 8069, Batch Gradient Norm: 28.907494400929874
Epoch: 8069, Batch Gradient Norm after: 22.360677124161146
Epoch 8070/10000, Prediction Accuracy = 60.212%, Loss = 0.7247689247131348
Epoch: 8070, Batch Gradient Norm: 27.669202453552064
Epoch: 8070, Batch Gradient Norm after: 22.360677974505254
Epoch 8071/10000, Prediction Accuracy = 60.2%, Loss = 0.7219460010528564
Epoch: 8071, Batch Gradient Norm: 28.910020949054033
Epoch: 8071, Batch Gradient Norm after: 22.360677325913603
Epoch 8072/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7248720407485962
Epoch: 8072, Batch Gradient Norm: 27.657964874552185
Epoch: 8072, Batch Gradient Norm after: 22.360676955272666
Epoch 8073/10000, Prediction Accuracy = 60.234%, Loss = 0.7220064640045166
Epoch: 8073, Batch Gradient Norm: 28.90558243699953
Epoch: 8073, Batch Gradient Norm after: 22.360676343867254
Epoch 8074/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7248288750648498
Epoch: 8074, Batch Gradient Norm: 27.649077140935617
Epoch: 8074, Batch Gradient Norm after: 22.36067596070782
Epoch 8075/10000, Prediction Accuracy = 60.202%, Loss = 0.7218435287475586
Epoch: 8075, Batch Gradient Norm: 28.903924705793614
Epoch: 8075, Batch Gradient Norm after: 22.360679226821702
Epoch 8076/10000, Prediction Accuracy = 60.19%, Loss = 0.7245874643325806
Epoch: 8076, Batch Gradient Norm: 27.646455566944663
Epoch: 8076, Batch Gradient Norm after: 22.36067660883827
Epoch 8077/10000, Prediction Accuracy = 60.174000000000014%, Loss = 0.7217042446136475
Epoch: 8077, Batch Gradient Norm: 28.908661407121883
Epoch: 8077, Batch Gradient Norm after: 22.360677473770863
Epoch 8078/10000, Prediction Accuracy = 60.178%, Loss = 0.7245054841041565
Epoch: 8078, Batch Gradient Norm: 27.649672234677716
Epoch: 8078, Batch Gradient Norm after: 22.36067959178515
Epoch 8079/10000, Prediction Accuracy = 60.166%, Loss = 0.7216807007789612
Epoch: 8079, Batch Gradient Norm: 28.89647963649758
Epoch: 8079, Batch Gradient Norm after: 22.36068068470879
Epoch 8080/10000, Prediction Accuracy = 60.148%, Loss = 0.7244601964950561
Epoch: 8080, Batch Gradient Norm: 27.651669683239763
Epoch: 8080, Batch Gradient Norm after: 22.36067920782155
Epoch 8081/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7215789914131164
Epoch: 8081, Batch Gradient Norm: 28.89494268315706
Epoch: 8081, Batch Gradient Norm after: 22.360679125786568
Epoch 8082/10000, Prediction Accuracy = 60.218%, Loss = 0.724424934387207
Epoch: 8082, Batch Gradient Norm: 27.646298001210734
Epoch: 8082, Batch Gradient Norm after: 22.360678373201676
Epoch 8083/10000, Prediction Accuracy = 60.226%, Loss = 0.7216057896614074
Epoch: 8083, Batch Gradient Norm: 28.89725643181286
Epoch: 8083, Batch Gradient Norm after: 22.360677227698023
Epoch 8084/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7245787858963013
Epoch: 8084, Batch Gradient Norm: 27.63167228757897
Epoch: 8084, Batch Gradient Norm after: 22.36067955040162
Epoch 8085/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7216452240943909
Epoch: 8085, Batch Gradient Norm: 28.89191232656459
Epoch: 8085, Batch Gradient Norm after: 22.360679582555534
Epoch 8086/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.724461019039154
Epoch: 8086, Batch Gradient Norm: 27.62364206755242
Epoch: 8086, Batch Gradient Norm after: 22.360676870394244
Epoch 8087/10000, Prediction Accuracy = 60.198%, Loss = 0.7214486837387085
Epoch: 8087, Batch Gradient Norm: 28.893888126244192
Epoch: 8087, Batch Gradient Norm after: 22.360676922014775
Epoch 8088/10000, Prediction Accuracy = 60.202%, Loss = 0.724224865436554
Epoch: 8088, Batch Gradient Norm: 27.625419929934775
Epoch: 8088, Batch Gradient Norm after: 22.360678516406118
Epoch 8089/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7213414192199707
Epoch: 8089, Batch Gradient Norm: 28.891933910136437
Epoch: 8089, Batch Gradient Norm after: 22.36067635938005
Epoch 8090/10000, Prediction Accuracy = 60.164%, Loss = 0.7241617441177368
Epoch: 8090, Batch Gradient Norm: 27.623179200887666
Epoch: 8090, Batch Gradient Norm after: 22.360677353281037
Epoch 8091/10000, Prediction Accuracy = 60.148%, Loss = 0.7213070750236511
Epoch: 8091, Batch Gradient Norm: 28.885307988282
Epoch: 8091, Batch Gradient Norm after: 22.36067678147993
Epoch 8092/10000, Prediction Accuracy = 60.164%, Loss = 0.7241111636161804
Epoch: 8092, Batch Gradient Norm: 27.625428763564503
Epoch: 8092, Batch Gradient Norm after: 22.360677850334742
Epoch 8093/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7212022304534912
Epoch: 8093, Batch Gradient Norm: 28.882062220866185
Epoch: 8093, Batch Gradient Norm after: 22.360677425462228
Epoch 8094/10000, Prediction Accuracy = 60.178%, Loss = 0.7241020679473877
Epoch: 8094, Batch Gradient Norm: 27.625966553908334
Epoch: 8094, Batch Gradient Norm after: 22.360678150286105
Epoch 8095/10000, Prediction Accuracy = 60.21999999999999%, Loss = 0.7212767004966736
Epoch: 8095, Batch Gradient Norm: 28.87896668109324
Epoch: 8095, Batch Gradient Norm after: 22.360678268509265
Epoch 8096/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7242292523384094
Epoch: 8096, Batch Gradient Norm: 27.61652688989106
Epoch: 8096, Batch Gradient Norm after: 22.36067917266853
Epoch 8097/10000, Prediction Accuracy = 60.242%, Loss = 0.7212858438491822
Epoch: 8097, Batch Gradient Norm: 28.872042530792616
Epoch: 8097, Batch Gradient Norm after: 22.360677559879253
Epoch 8098/10000, Prediction Accuracy = 60.188%, Loss = 0.7240540504455566
Epoch: 8098, Batch Gradient Norm: 27.614051317260635
Epoch: 8098, Batch Gradient Norm after: 22.36067808947941
Epoch 8099/10000, Prediction Accuracy = 60.2%, Loss = 0.7210944414138794
Epoch: 8099, Batch Gradient Norm: 28.872347870570717
Epoch: 8099, Batch Gradient Norm after: 22.360679328423338
Epoch 8100/10000, Prediction Accuracy = 60.202%, Loss = 0.7238469600677491
Epoch: 8100, Batch Gradient Norm: 27.619860573737732
Epoch: 8100, Batch Gradient Norm after: 22.360678332884344
Epoch 8101/10000, Prediction Accuracy = 60.124%, Loss = 0.7210243940353394
Epoch: 8101, Batch Gradient Norm: 28.868431818412912
Epoch: 8101, Batch Gradient Norm after: 22.36067879457413
Epoch 8102/10000, Prediction Accuracy = 60.152%, Loss = 0.7237952828407288
Epoch: 8102, Batch Gradient Norm: 27.618272751222303
Epoch: 8102, Batch Gradient Norm after: 22.360675815781534
Epoch 8103/10000, Prediction Accuracy = 60.148%, Loss = 0.720981764793396
Epoch: 8103, Batch Gradient Norm: 28.860406698627877
Epoch: 8103, Batch Gradient Norm after: 22.36067910986841
Epoch 8104/10000, Prediction Accuracy = 60.174%, Loss = 0.7237362384796142
Epoch: 8104, Batch Gradient Norm: 27.62424244441352
Epoch: 8104, Batch Gradient Norm after: 22.360677368579523
Epoch 8105/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7208909869194031
Epoch: 8105, Batch Gradient Norm: 28.86000100588626
Epoch: 8105, Batch Gradient Norm after: 22.360675232799824
Epoch 8106/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7237666606903076
Epoch: 8106, Batch Gradient Norm: 27.622435348342893
Epoch: 8106, Batch Gradient Norm after: 22.36067714465276
Epoch 8107/10000, Prediction Accuracy = 60.226%, Loss = 0.7210046410560608
Epoch: 8107, Batch Gradient Norm: 28.85233501263383
Epoch: 8107, Batch Gradient Norm after: 22.360679095871344
Epoch 8108/10000, Prediction Accuracy = 60.212%, Loss = 0.7238802433013916
Epoch: 8108, Batch Gradient Norm: 27.6112383118794
Epoch: 8108, Batch Gradient Norm after: 22.360678305566406
Epoch 8109/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.7209598302841187
Epoch: 8109, Batch Gradient Norm: 28.847961353570653
Epoch: 8109, Batch Gradient Norm after: 22.360676331058674
Epoch 8110/10000, Prediction Accuracy = 60.176%, Loss = 0.7236469984054565
Epoch: 8110, Batch Gradient Norm: 27.612996181621234
Epoch: 8110, Batch Gradient Norm after: 22.360676947392065
Epoch 8111/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7207631587982177
Epoch: 8111, Batch Gradient Norm: 28.848567604331993
Epoch: 8111, Batch Gradient Norm after: 22.360677173489965
Epoch 8112/10000, Prediction Accuracy = 60.212%, Loss = 0.7234640002250672
Epoch: 8112, Batch Gradient Norm: 27.619463103021292
Epoch: 8112, Batch Gradient Norm after: 22.36067786565675
Epoch 8113/10000, Prediction Accuracy = 60.15%, Loss = 0.7207261204719544
Epoch: 8113, Batch Gradient Norm: 28.8423395196486
Epoch: 8113, Batch Gradient Norm after: 22.36067556445628
Epoch 8114/10000, Prediction Accuracy = 60.136%, Loss = 0.7234302401542664
Epoch: 8114, Batch Gradient Norm: 27.620663573082957
Epoch: 8114, Batch Gradient Norm after: 22.36067791338672
Epoch 8115/10000, Prediction Accuracy = 60.164%, Loss = 0.7206474184989929
Epoch: 8115, Batch Gradient Norm: 28.834880479028833
Epoch: 8115, Batch Gradient Norm after: 22.360676765815253
Epoch 8116/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7233652353286744
Epoch: 8116, Batch Gradient Norm: 27.620739236201274
Epoch: 8116, Batch Gradient Norm after: 22.360679460152756
Epoch 8117/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7205926299095153
Epoch: 8117, Batch Gradient Norm: 28.83690958120783
Epoch: 8117, Batch Gradient Norm after: 22.36067845685042
Epoch 8118/10000, Prediction Accuracy = 60.188%, Loss = 0.7234342217445373
Epoch: 8118, Batch Gradient Norm: 27.60869499132572
Epoch: 8118, Batch Gradient Norm after: 22.360678510895667
Epoch 8119/10000, Prediction Accuracy = 60.234%, Loss = 0.720681095123291
Epoch: 8119, Batch Gradient Norm: 28.83311451151875
Epoch: 8119, Batch Gradient Norm after: 22.360678572332155
Epoch 8120/10000, Prediction Accuracy = 60.212%, Loss = 0.7234799861907959
Epoch: 8120, Batch Gradient Norm: 27.596309224417062
Epoch: 8120, Batch Gradient Norm after: 22.360675380991328
Epoch 8121/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7205682396888733
Epoch: 8121, Batch Gradient Norm: 28.834394980833228
Epoch: 8121, Batch Gradient Norm after: 22.36067720964499
Epoch 8122/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7232532858848572
Epoch: 8122, Batch Gradient Norm: 27.591818265889362
Epoch: 8122, Batch Gradient Norm after: 22.36067883709985
Epoch 8123/10000, Prediction Accuracy = 60.2%, Loss = 0.7203927159309387
Epoch: 8123, Batch Gradient Norm: 28.836589881948086
Epoch: 8123, Batch Gradient Norm after: 22.360676653842052
Epoch 8124/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7231185793876648
Epoch: 8124, Batch Gradient Norm: 27.598177808563868
Epoch: 8124, Batch Gradient Norm after: 22.36067944547538
Epoch 8125/10000, Prediction Accuracy = 60.162%, Loss = 0.7203592300415039
Epoch: 8125, Batch Gradient Norm: 28.828173368373847
Epoch: 8125, Batch Gradient Norm after: 22.360679205179
Epoch 8126/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7230836391448975
Epoch: 8126, Batch Gradient Norm: 27.59498376511997
Epoch: 8126, Batch Gradient Norm after: 22.36067686073071
Epoch 8127/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7202698469161988
Epoch: 8127, Batch Gradient Norm: 28.82481960327598
Epoch: 8127, Batch Gradient Norm after: 22.360677539980184
Epoch 8128/10000, Prediction Accuracy = 60.202%, Loss = 0.7230271697044373
Epoch: 8128, Batch Gradient Norm: 27.58925484937475
Epoch: 8128, Batch Gradient Norm after: 22.360677850951312
Epoch 8129/10000, Prediction Accuracy = 60.176%, Loss = 0.7202362418174744
Epoch: 8129, Batch Gradient Norm: 28.82930463623848
Epoch: 8129, Batch Gradient Norm after: 22.360677864961303
Epoch 8130/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7231375455856324
Epoch: 8130, Batch Gradient Norm: 27.581863403363773
Epoch: 8130, Batch Gradient Norm after: 22.360677569386194
Epoch 8131/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7203283548355103
Epoch: 8131, Batch Gradient Norm: 28.82185593683688
Epoch: 8131, Batch Gradient Norm after: 22.360676357199328
Epoch 8132/10000, Prediction Accuracy = 60.23%, Loss = 0.7231452465057373
Epoch: 8132, Batch Gradient Norm: 27.57077741509122
Epoch: 8132, Batch Gradient Norm after: 22.360679718538876
Epoch 8133/10000, Prediction Accuracy = 60.2%, Loss = 0.7201868414878845
Epoch: 8133, Batch Gradient Norm: 28.82280608738532
Epoch: 8133, Batch Gradient Norm after: 22.360679594884864
Epoch 8134/10000, Prediction Accuracy = 60.19%, Loss = 0.7228890419006347
Epoch: 8134, Batch Gradient Norm: 27.57281723232093
Epoch: 8134, Batch Gradient Norm after: 22.360679141800414
Epoch 8135/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7200236797332764
Epoch: 8135, Batch Gradient Norm: 28.823336659313334
Epoch: 8135, Batch Gradient Norm after: 22.360676765427804
Epoch 8136/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.7227753400802612
Epoch: 8136, Batch Gradient Norm: 27.57373697601426
Epoch: 8136, Batch Gradient Norm after: 22.360678455473916
Epoch 8137/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7200045585632324
Epoch: 8137, Batch Gradient Norm: 28.814681458313093
Epoch: 8137, Batch Gradient Norm after: 22.360678180887763
Epoch 8138/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7227441549301148
Epoch: 8138, Batch Gradient Norm: 27.568464348379575
Epoch: 8138, Batch Gradient Norm after: 22.36067949849193
Epoch 8139/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7199028730392456
Epoch: 8139, Batch Gradient Norm: 28.81239200777713
Epoch: 8139, Batch Gradient Norm after: 22.360676292966918
Epoch 8140/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7226912617683411
Epoch: 8140, Batch Gradient Norm: 27.569050871110882
Epoch: 8140, Batch Gradient Norm after: 22.36067632463011
Epoch 8141/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7198795557022095
Epoch: 8141, Batch Gradient Norm: 28.81477762398321
Epoch: 8141, Batch Gradient Norm after: 22.360677279893967
Epoch 8142/10000, Prediction Accuracy = 60.214%, Loss = 0.7228214025497437
Epoch: 8142, Batch Gradient Norm: 27.555886940993084
Epoch: 8142, Batch Gradient Norm after: 22.360678574679504
Epoch 8143/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.7199568748474121
Epoch: 8143, Batch Gradient Norm: 28.812048753195025
Epoch: 8143, Batch Gradient Norm after: 22.36067759473098
Epoch 8144/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7227849125862121
Epoch: 8144, Batch Gradient Norm: 27.539937360504066
Epoch: 8144, Batch Gradient Norm after: 22.360677977191376
Epoch 8145/10000, Prediction Accuracy = 60.2%, Loss = 0.7197839021682739
Epoch: 8145, Batch Gradient Norm: 28.814421282650095
Epoch: 8145, Batch Gradient Norm after: 22.360675694733526
Epoch 8146/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7225571870803833
Epoch: 8146, Batch Gradient Norm: 27.540434391357284
Epoch: 8146, Batch Gradient Norm after: 22.360676508085703
Epoch 8147/10000, Prediction Accuracy = 60.174%, Loss = 0.719638192653656
Epoch: 8147, Batch Gradient Norm: 28.813966753206074
Epoch: 8147, Batch Gradient Norm after: 22.360676615287925
Epoch 8148/10000, Prediction Accuracy = 60.202%, Loss = 0.7224486231803894
Epoch: 8148, Batch Gradient Norm: 27.540243953448083
Epoch: 8148, Batch Gradient Norm after: 22.36067918684505
Epoch 8149/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7196117043495178
Epoch: 8149, Batch Gradient Norm: 28.806153802601546
Epoch: 8149, Batch Gradient Norm after: 22.360677420103734
Epoch 8150/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.7224117994308472
Epoch: 8150, Batch Gradient Norm: 27.533895221783983
Epoch: 8150, Batch Gradient Norm after: 22.360678061059357
Epoch 8151/10000, Prediction Accuracy = 60.176%, Loss = 0.7195040822029114
Epoch: 8151, Batch Gradient Norm: 28.802483460441163
Epoch: 8151, Batch Gradient Norm after: 22.360678532680918
Epoch 8152/10000, Prediction Accuracy = 60.202%, Loss = 0.7223753571510315
Epoch: 8152, Batch Gradient Norm: 27.536906142512247
Epoch: 8152, Batch Gradient Norm after: 22.360677963773437
Epoch 8153/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7195143938064575
Epoch: 8153, Batch Gradient Norm: 28.80275481998036
Epoch: 8153, Batch Gradient Norm after: 22.36067796572866
Epoch 8154/10000, Prediction Accuracy = 60.242%, Loss = 0.7225106477737426
Epoch: 8154, Batch Gradient Norm: 27.52541560421346
Epoch: 8154, Batch Gradient Norm after: 22.360678593662808
Epoch 8155/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7195836424827575
Epoch: 8155, Batch Gradient Norm: 28.79744627104254
Epoch: 8155, Batch Gradient Norm after: 22.360677714495083
Epoch 8156/10000, Prediction Accuracy = 60.214%, Loss = 0.7224382042884827
Epoch: 8156, Batch Gradient Norm: 27.516111281838445
Epoch: 8156, Batch Gradient Norm after: 22.360678985603972
Epoch 8157/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.7193946838378906
Epoch: 8157, Batch Gradient Norm: 28.7986790957038
Epoch: 8157, Batch Gradient Norm after: 22.360676913682383
Epoch 8158/10000, Prediction Accuracy = 60.21200000000001%, Loss = 0.7221964240074158
Epoch: 8158, Batch Gradient Norm: 27.515519849688854
Epoch: 8158, Batch Gradient Norm after: 22.360677805912452
Epoch 8159/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7192781448364258
Epoch: 8159, Batch Gradient Norm: 28.79876865297361
Epoch: 8159, Batch Gradient Norm after: 22.36067715919562
Epoch 8160/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7221213579177856
Epoch: 8160, Batch Gradient Norm: 27.517481322397106
Epoch: 8160, Batch Gradient Norm after: 22.36067874777985
Epoch 8161/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7192499876022339
Epoch: 8161, Batch Gradient Norm: 28.7890527075297
Epoch: 8161, Batch Gradient Norm after: 22.360678241256814
Epoch 8162/10000, Prediction Accuracy = 60.17199999999999%, Loss = 0.7220731258392334
Epoch: 8162, Batch Gradient Norm: 27.51127571267249
Epoch: 8162, Batch Gradient Norm after: 22.360676512744252
Epoch 8163/10000, Prediction Accuracy = 60.174%, Loss = 0.7191233515739441
Epoch: 8163, Batch Gradient Norm: 28.79031982344519
Epoch: 8163, Batch Gradient Norm after: 22.360677288464895
Epoch 8164/10000, Prediction Accuracy = 60.178%, Loss = 0.7220607399940491
Epoch: 8164, Batch Gradient Norm: 27.502451153063717
Epoch: 8164, Batch Gradient Norm after: 22.360678216200153
Epoch 8165/10000, Prediction Accuracy = 60.186%, Loss = 0.7191632390022278
Epoch: 8165, Batch Gradient Norm: 28.791231131715083
Epoch: 8165, Batch Gradient Norm after: 22.3606781626738
Epoch 8166/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.722219729423523
Epoch: 8166, Batch Gradient Norm: 27.48478788094818
Epoch: 8166, Batch Gradient Norm after: 22.36067678328301
Epoch 8167/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7191769003868103
Epoch: 8167, Batch Gradient Norm: 28.788792223748594
Epoch: 8167, Batch Gradient Norm after: 22.360677824371656
Epoch 8168/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7220692753791809
Epoch: 8168, Batch Gradient Norm: 27.479155405284228
Epoch: 8168, Batch Gradient Norm after: 22.36068045176617
Epoch 8169/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7189655542373657
Epoch: 8169, Batch Gradient Norm: 28.7937214704799
Epoch: 8169, Batch Gradient Norm after: 22.360679426761017
Epoch 8170/10000, Prediction Accuracy = 60.19%, Loss = 0.721853232383728
Epoch: 8170, Batch Gradient Norm: 27.48343882595786
Epoch: 8170, Batch Gradient Norm after: 22.3606770140055
Epoch 8171/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.718886387348175
Epoch: 8171, Batch Gradient Norm: 28.788512495817336
Epoch: 8171, Batch Gradient Norm after: 22.36067828697907
Epoch 8172/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.721789813041687
Epoch: 8172, Batch Gradient Norm: 27.48334770187842
Epoch: 8172, Batch Gradient Norm after: 22.36067982258137
Epoch 8173/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7188418984413147
Epoch: 8173, Batch Gradient Norm: 28.77920441839463
Epoch: 8173, Batch Gradient Norm after: 22.360678210662293
Epoch 8174/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7217368245124817
Epoch: 8174, Batch Gradient Norm: 27.47950948475527
Epoch: 8174, Batch Gradient Norm after: 22.3606809114496
Epoch 8175/10000, Prediction Accuracy = 60.174%, Loss = 0.7187489986419677
Epoch: 8175, Batch Gradient Norm: 28.782443630329123
Epoch: 8175, Batch Gradient Norm after: 22.360678461439868
Epoch 8176/10000, Prediction Accuracy = 60.19599999999999%, Loss = 0.7217555284500122
Epoch: 8176, Batch Gradient Norm: 27.475169207745658
Epoch: 8176, Batch Gradient Norm after: 22.360676760817597
Epoch 8177/10000, Prediction Accuracy = 60.202%, Loss = 0.7188015222549439
Epoch: 8177, Batch Gradient Norm: 28.779093907107864
Epoch: 8177, Batch Gradient Norm after: 22.360680750857206
Epoch 8178/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7218785285949707
Epoch: 8178, Batch Gradient Norm: 27.460901038392763
Epoch: 8178, Batch Gradient Norm after: 22.360676954299723
Epoch 8179/10000, Prediction Accuracy = 60.21400000000001%, Loss = 0.718790602684021
Epoch: 8179, Batch Gradient Norm: 28.774905384796593
Epoch: 8179, Batch Gradient Norm after: 22.360678349165212
Epoch 8180/10000, Prediction Accuracy = 60.186%, Loss = 0.7216952443122864
Epoch: 8180, Batch Gradient Norm: 27.459951751402624
Epoch: 8180, Batch Gradient Norm after: 22.360678675945277
Epoch 8181/10000, Prediction Accuracy = 60.188%, Loss = 0.7185970067977905
Epoch: 8181, Batch Gradient Norm: 28.781675471538637
Epoch: 8181, Batch Gradient Norm after: 22.36067498820174
Epoch 8182/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7214983463287353
Epoch: 8182, Batch Gradient Norm: 27.46432431876528
Epoch: 8182, Batch Gradient Norm after: 22.36067729402155
Epoch 8183/10000, Prediction Accuracy = 60.174%, Loss = 0.7185395359992981
Epoch: 8183, Batch Gradient Norm: 28.773293676876197
Epoch: 8183, Batch Gradient Norm after: 22.3606776320369
Epoch 8184/10000, Prediction Accuracy = 60.136%, Loss = 0.721458637714386
Epoch: 8184, Batch Gradient Norm: 27.465332581789937
Epoch: 8184, Batch Gradient Norm after: 22.36067914521754
Epoch 8185/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.718489670753479
Epoch: 8185, Batch Gradient Norm: 28.765754548570445
Epoch: 8185, Batch Gradient Norm after: 22.360679252578922
Epoch 8186/10000, Prediction Accuracy = 60.202%, Loss = 0.7213948369026184
Epoch: 8186, Batch Gradient Norm: 27.462861411911874
Epoch: 8186, Batch Gradient Norm after: 22.360680717873045
Epoch 8187/10000, Prediction Accuracy = 60.176%, Loss = 0.7184096693992614
Epoch: 8187, Batch Gradient Norm: 28.766775721350108
Epoch: 8187, Batch Gradient Norm after: 22.360675709724646
Epoch 8188/10000, Prediction Accuracy = 60.18800000000001%, Loss = 0.7214462518692016
Epoch: 8188, Batch Gradient Norm: 27.455631374059287
Epoch: 8188, Batch Gradient Norm after: 22.360676054543685
Epoch 8189/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7184944868087768
Epoch: 8189, Batch Gradient Norm: 28.76435579939048
Epoch: 8189, Batch Gradient Norm after: 22.360676538696218
Epoch 8190/10000, Prediction Accuracy = 60.238000000000014%, Loss = 0.7215519070625305
Epoch: 8190, Batch Gradient Norm: 27.43857448824859
Epoch: 8190, Batch Gradient Norm after: 22.36067653708215
Epoch 8191/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7184121489524842
Epoch: 8191, Batch Gradient Norm: 28.763875366163322
Epoch: 8191, Batch Gradient Norm after: 22.3606786062163
Epoch 8192/10000, Prediction Accuracy = 60.174%, Loss = 0.7213159084320069
Epoch: 8192, Batch Gradient Norm: 27.43625093060007
Epoch: 8192, Batch Gradient Norm after: 22.36067617908798
Epoch 8193/10000, Prediction Accuracy = 60.202%, Loss = 0.7182147741317749
Epoch: 8193, Batch Gradient Norm: 28.77004195516548
Epoch: 8193, Batch Gradient Norm after: 22.360680315594802
Epoch 8194/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7211539149284363
Epoch: 8194, Batch Gradient Norm: 27.439497750412954
Epoch: 8194, Batch Gradient Norm after: 22.360677475091073
Epoch 8195/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7181779980659485
Epoch: 8195, Batch Gradient Norm: 28.761351899820475
Epoch: 8195, Batch Gradient Norm after: 22.360674964667428
Epoch 8196/10000, Prediction Accuracy = 60.148%, Loss = 0.7211236119270324
Epoch: 8196, Batch Gradient Norm: 27.435340985582638
Epoch: 8196, Batch Gradient Norm after: 22.360678254877378
Epoch 8197/10000, Prediction Accuracy = 60.198%, Loss = 0.7180942416191101
Epoch: 8197, Batch Gradient Norm: 28.756491080892143
Epoch: 8197, Batch Gradient Norm after: 22.360678176262965
Epoch 8198/10000, Prediction Accuracy = 60.212%, Loss = 0.7210633993148804
Epoch: 8198, Batch Gradient Norm: 27.43493814226949
Epoch: 8198, Batch Gradient Norm after: 22.360679212330776
Epoch 8199/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.7180370807647705
Epoch: 8199, Batch Gradient Norm: 28.75565619367653
Epoch: 8199, Batch Gradient Norm after: 22.3606767783541
Epoch 8200/10000, Prediction Accuracy = 60.2%, Loss = 0.7211367368698121
Epoch: 8200, Batch Gradient Norm: 27.426021404421455
Epoch: 8200, Batch Gradient Norm after: 22.36067819287921
Epoch 8201/10000, Prediction Accuracy = 60.234%, Loss = 0.7181190371513366
Epoch: 8201, Batch Gradient Norm: 28.747991883473407
Epoch: 8201, Batch Gradient Norm after: 22.360681065560257
Epoch 8202/10000, Prediction Accuracy = 60.238%, Loss = 0.7212119221687316
Epoch: 8202, Batch Gradient Norm: 27.41354915145813
Epoch: 8202, Batch Gradient Norm after: 22.360677702138172
Epoch 8203/10000, Prediction Accuracy = 60.222%, Loss = 0.7180306315422058
Epoch: 8203, Batch Gradient Norm: 28.75389083507362
Epoch: 8203, Batch Gradient Norm after: 22.360677073623712
Epoch 8204/10000, Prediction Accuracy = 60.178%, Loss = 0.720962917804718
Epoch: 8204, Batch Gradient Norm: 27.415982430679534
Epoch: 8204, Batch Gradient Norm after: 22.360677997473196
Epoch 8205/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.717845106124878
Epoch: 8205, Batch Gradient Norm: 28.75303509073239
Epoch: 8205, Batch Gradient Norm after: 22.360679080315
Epoch 8206/10000, Prediction Accuracy = 60.19%, Loss = 0.7208177089691162
Epoch: 8206, Batch Gradient Norm: 27.417339512547116
Epoch: 8206, Batch Gradient Norm after: 22.36067715566997
Epoch 8207/10000, Prediction Accuracy = 60.188%, Loss = 0.7178267478942871
Epoch: 8207, Batch Gradient Norm: 28.745693591513888
Epoch: 8207, Batch Gradient Norm after: 22.360677361735842
Epoch 8208/10000, Prediction Accuracy = 60.15%, Loss = 0.72078857421875
Epoch: 8208, Batch Gradient Norm: 27.419654498696765
Epoch: 8208, Batch Gradient Norm after: 22.360678640620627
Epoch 8209/10000, Prediction Accuracy = 60.2%, Loss = 0.7177409648895263
Epoch: 8209, Batch Gradient Norm: 28.739219918609272
Epoch: 8209, Batch Gradient Norm after: 22.360679603976262
Epoch 8210/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7207202792167664
Epoch: 8210, Batch Gradient Norm: 27.421165625441354
Epoch: 8210, Batch Gradient Norm after: 22.36067842374831
Epoch 8211/10000, Prediction Accuracy = 60.15%, Loss = 0.717701530456543
Epoch: 8211, Batch Gradient Norm: 28.73914835376694
Epoch: 8211, Batch Gradient Norm after: 22.36067647903135
Epoch 8212/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.720810079574585
Epoch: 8212, Batch Gradient Norm: 27.41232594155886
Epoch: 8212, Batch Gradient Norm after: 22.360677748800715
Epoch 8213/10000, Prediction Accuracy = 60.226%, Loss = 0.7177953243255615
Epoch: 8213, Batch Gradient Norm: 28.733849877839884
Epoch: 8213, Batch Gradient Norm after: 22.360679019225234
Epoch 8214/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.720842719078064
Epoch: 8214, Batch Gradient Norm: 27.400378399380823
Epoch: 8214, Batch Gradient Norm after: 22.360677509447964
Epoch 8215/10000, Prediction Accuracy = 60.226%, Loss = 0.7176735162734985
Epoch: 8215, Batch Gradient Norm: 28.73523130855915
Epoch: 8215, Batch Gradient Norm after: 22.3606777622872
Epoch 8216/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7205894112586975
Epoch: 8216, Batch Gradient Norm: 27.405557690600848
Epoch: 8216, Batch Gradient Norm after: 22.360677933837522
Epoch 8217/10000, Prediction Accuracy = 60.202%, Loss = 0.7175129771232605
Epoch: 8217, Batch Gradient Norm: 28.7365078444614
Epoch: 8217, Batch Gradient Norm after: 22.36067900184414
Epoch 8218/10000, Prediction Accuracy = 60.19%, Loss = 0.720459258556366
Epoch: 8218, Batch Gradient Norm: 27.408830780570895
Epoch: 8218, Batch Gradient Norm after: 22.360678335288533
Epoch 8219/10000, Prediction Accuracy = 60.2%, Loss = 0.7174977779388427
Epoch: 8219, Batch Gradient Norm: 28.726625415995006
Epoch: 8219, Batch Gradient Norm after: 22.360676146711814
Epoch 8220/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7204272270202636
Epoch: 8220, Batch Gradient Norm: 27.406471164873256
Epoch: 8220, Batch Gradient Norm after: 22.36067677566392
Epoch 8221/10000, Prediction Accuracy = 60.202%, Loss = 0.7174051403999329
Epoch: 8221, Batch Gradient Norm: 28.719801273548775
Epoch: 8221, Batch Gradient Norm after: 22.36067606825494
Epoch 8222/10000, Prediction Accuracy = 60.222%, Loss = 0.7203741669654846
Epoch: 8222, Batch Gradient Norm: 27.41059503914327
Epoch: 8222, Batch Gradient Norm after: 22.360677184465874
Epoch 8223/10000, Prediction Accuracy = 60.148%, Loss = 0.7173904299736023
Epoch: 8223, Batch Gradient Norm: 28.71954820044051
Epoch: 8223, Batch Gradient Norm after: 22.36067638826438
Epoch 8224/10000, Prediction Accuracy = 60.234%, Loss = 0.7204840898513794
Epoch: 8224, Batch Gradient Norm: 27.400813746156285
Epoch: 8224, Batch Gradient Norm after: 22.360677096776158
Epoch 8225/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.7174806237220764
Epoch: 8225, Batch Gradient Norm: 28.712627087025663
Epoch: 8225, Batch Gradient Norm after: 22.36067827165794
Epoch 8226/10000, Prediction Accuracy = 60.238%, Loss = 0.7204668760299683
Epoch: 8226, Batch Gradient Norm: 27.396363018295336
Epoch: 8226, Batch Gradient Norm after: 22.36067784566375
Epoch 8227/10000, Prediction Accuracy = 60.206%, Loss = 0.717336118221283
Epoch: 8227, Batch Gradient Norm: 28.714969229677884
Epoch: 8227, Batch Gradient Norm after: 22.360678497490667
Epoch 8228/10000, Prediction Accuracy = 60.188%, Loss = 0.720204210281372
Epoch: 8228, Batch Gradient Norm: 27.39691374921718
Epoch: 8228, Batch Gradient Norm after: 22.360678456770565
Epoch 8229/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7172021269798279
Epoch: 8229, Batch Gradient Norm: 28.71740960101399
Epoch: 8229, Batch Gradient Norm after: 22.360676348778966
Epoch 8230/10000, Prediction Accuracy = 60.174%, Loss = 0.7201117038726806
Epoch: 8230, Batch Gradient Norm: 27.401490423782814
Epoch: 8230, Batch Gradient Norm after: 22.360676947865823
Epoch 8231/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7171794295310974
Epoch: 8231, Batch Gradient Norm: 28.70586137988862
Epoch: 8231, Batch Gradient Norm after: 22.360674113196612
Epoch 8232/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7200652718544006
Epoch: 8232, Batch Gradient Norm: 27.401302056798627
Epoch: 8232, Batch Gradient Norm after: 22.360678954197102
Epoch 8233/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7170765161514282
Epoch: 8233, Batch Gradient Norm: 28.702591307179826
Epoch: 8233, Batch Gradient Norm after: 22.36067649376928
Epoch 8234/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7200299024581909
Epoch: 8234, Batch Gradient Norm: 27.400180770285846
Epoch: 8234, Batch Gradient Norm after: 22.360678040494392
Epoch 8235/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7170946955680847
Epoch: 8235, Batch Gradient Norm: 28.703722437022215
Epoch: 8235, Batch Gradient Norm after: 22.360679182408923
Epoch 8236/10000, Prediction Accuracy = 60.24799999999999%, Loss = 0.7201594591140748
Epoch: 8236, Batch Gradient Norm: 27.38785741561679
Epoch: 8236, Batch Gradient Norm after: 22.360676381752643
Epoch 8237/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7171565294265747
Epoch: 8237, Batch Gradient Norm: 28.696568193097615
Epoch: 8237, Batch Gradient Norm after: 22.360681210791554
Epoch 8238/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7200894236564637
Epoch: 8238, Batch Gradient Norm: 27.378300016942955
Epoch: 8238, Batch Gradient Norm after: 22.36067983076059
Epoch 8239/10000, Prediction Accuracy = 60.202%, Loss = 0.7169753551483155
Epoch: 8239, Batch Gradient Norm: 28.700194029291755
Epoch: 8239, Batch Gradient Norm after: 22.360675979774207
Epoch 8240/10000, Prediction Accuracy = 60.194%, Loss = 0.7198463916778565
Epoch: 8240, Batch Gradient Norm: 27.375379107250176
Epoch: 8240, Batch Gradient Norm after: 22.36067946045221
Epoch 8241/10000, Prediction Accuracy = 60.188%, Loss = 0.7168529510498047
Epoch: 8241, Batch Gradient Norm: 28.701771290662315
Epoch: 8241, Batch Gradient Norm after: 22.360677952906933
Epoch 8242/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7197830200195312
Epoch: 8242, Batch Gradient Norm: 27.377109751963854
Epoch: 8242, Batch Gradient Norm after: 22.360678232588455
Epoch 8243/10000, Prediction Accuracy = 60.174%, Loss = 0.7168226480484009
Epoch: 8243, Batch Gradient Norm: 28.68866346908473
Epoch: 8243, Batch Gradient Norm after: 22.360678618306487
Epoch 8244/10000, Prediction Accuracy = 60.178%, Loss = 0.719734501838684
Epoch: 8244, Batch Gradient Norm: 27.367711018327046
Epoch: 8244, Batch Gradient Norm after: 22.360677705463917
Epoch 8245/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7167027473449707
Epoch: 8245, Batch Gradient Norm: 28.69315972522656
Epoch: 8245, Batch Gradient Norm after: 22.360677934305553
Epoch 8246/10000, Prediction Accuracy = 60.202%, Loss = 0.7197400450706481
Epoch: 8246, Batch Gradient Norm: 27.363660406342447
Epoch: 8246, Batch Gradient Norm after: 22.360678225310686
Epoch 8247/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7167486548423767
Epoch: 8247, Batch Gradient Norm: 28.693358102586277
Epoch: 8247, Batch Gradient Norm after: 22.360677306412406
Epoch 8248/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.7198679566383361
Epoch: 8248, Batch Gradient Norm: 27.34930269945757
Epoch: 8248, Batch Gradient Norm after: 22.360678048425743
Epoch 8249/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7167585849761963
Epoch: 8249, Batch Gradient Norm: 28.687630552672935
Epoch: 8249, Batch Gradient Norm after: 22.36067986833073
Epoch 8250/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7197265625
Epoch: 8250, Batch Gradient Norm: 27.342704329954756
Epoch: 8250, Batch Gradient Norm after: 22.36067985634791
Epoch 8251/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7165695667266846
Epoch: 8251, Batch Gradient Norm: 28.69063828847214
Epoch: 8251, Batch Gradient Norm after: 22.36067752329267
Epoch 8252/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.719515073299408
Epoch: 8252, Batch Gradient Norm: 27.345375284110524
Epoch: 8252, Batch Gradient Norm after: 22.36067813977941
Epoch 8253/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.71648268699646
Epoch: 8253, Batch Gradient Norm: 28.68782621908341
Epoch: 8253, Batch Gradient Norm after: 22.360678905176876
Epoch 8254/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7194685697555542
Epoch: 8254, Batch Gradient Norm: 27.341584652780035
Epoch: 8254, Batch Gradient Norm after: 22.360677945210178
Epoch 8255/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7164352178573609
Epoch: 8255, Batch Gradient Norm: 28.680746463884134
Epoch: 8255, Batch Gradient Norm after: 22.360678167169393
Epoch 8256/10000, Prediction Accuracy = 60.2%, Loss = 0.719414222240448
Epoch: 8256, Batch Gradient Norm: 27.339580352969243
Epoch: 8256, Batch Gradient Norm after: 22.360679313942615
Epoch 8257/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7163325786590576
Epoch: 8257, Batch Gradient Norm: 28.684291408502173
Epoch: 8257, Batch Gradient Norm after: 22.360677638807484
Epoch 8258/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7194358587265015
Epoch: 8258, Batch Gradient Norm: 27.33338186643526
Epoch: 8258, Batch Gradient Norm after: 22.360676807924964
Epoch 8259/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7164036631584167
Epoch: 8259, Batch Gradient Norm: 28.680834226216362
Epoch: 8259, Batch Gradient Norm after: 22.36067648329744
Epoch 8260/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7195692420005798
Epoch: 8260, Batch Gradient Norm: 27.321521876906477
Epoch: 8260, Batch Gradient Norm after: 22.36067646837841
Epoch 8261/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7163846492767334
Epoch: 8261, Batch Gradient Norm: 28.67764537814862
Epoch: 8261, Batch Gradient Norm after: 22.360677384874016
Epoch 8262/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.7193709254264832
Epoch: 8262, Batch Gradient Norm: 27.319381992802498
Epoch: 8262, Batch Gradient Norm after: 22.360678523551957
Epoch 8263/10000, Prediction Accuracy = 60.198%, Loss = 0.7161849021911622
Epoch: 8263, Batch Gradient Norm: 28.680379523714546
Epoch: 8263, Batch Gradient Norm after: 22.36067885319526
Epoch 8264/10000, Prediction Accuracy = 60.212%, Loss = 0.7191665887832641
Epoch: 8264, Batch Gradient Norm: 27.32807842958111
Epoch: 8264, Batch Gradient Norm after: 22.36067709065382
Epoch 8265/10000, Prediction Accuracy = 60.2%, Loss = 0.716138482093811
Epoch: 8265, Batch Gradient Norm: 28.672476734184187
Epoch: 8265, Batch Gradient Norm after: 22.360674774576825
Epoch 8266/10000, Prediction Accuracy = 60.148%, Loss = 0.7191343426704406
Epoch: 8266, Batch Gradient Norm: 27.323991224804097
Epoch: 8266, Batch Gradient Norm after: 22.36067649984632
Epoch 8267/10000, Prediction Accuracy = 60.206%, Loss = 0.7160750746726989
Epoch: 8267, Batch Gradient Norm: 28.66674005399718
Epoch: 8267, Batch Gradient Norm after: 22.360677587540284
Epoch 8268/10000, Prediction Accuracy = 60.2%, Loss = 0.7190686583518981
Epoch: 8268, Batch Gradient Norm: 27.32600828373494
Epoch: 8268, Batch Gradient Norm after: 22.36067995967726
Epoch 8269/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7159929394721984
Epoch: 8269, Batch Gradient Norm: 28.666674332254686
Epoch: 8269, Batch Gradient Norm after: 22.360676745900324
Epoch 8270/10000, Prediction Accuracy = 60.19799999999999%, Loss = 0.7191171169281005
Epoch: 8270, Batch Gradient Norm: 27.322921876236975
Epoch: 8270, Batch Gradient Norm after: 22.360680034171022
Epoch 8271/10000, Prediction Accuracy = 60.198%, Loss = 0.7160915851593017
Epoch: 8271, Batch Gradient Norm: 28.660905694667395
Epoch: 8271, Batch Gradient Norm after: 22.360677430392826
Epoch 8272/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.719224727153778
Epoch: 8272, Batch Gradient Norm: 27.308186955119584
Epoch: 8272, Batch Gradient Norm after: 22.36067878745062
Epoch 8273/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7160332918167114
Epoch: 8273, Batch Gradient Norm: 28.65800540035308
Epoch: 8273, Batch Gradient Norm after: 22.36067873376562
Epoch 8274/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7189828753471375
Epoch: 8274, Batch Gradient Norm: 27.31113590228253
Epoch: 8274, Batch Gradient Norm after: 22.360677637164148
Epoch 8275/10000, Prediction Accuracy = 60.21%, Loss = 0.7158472180366516
Epoch: 8275, Batch Gradient Norm: 28.66056558427735
Epoch: 8275, Batch Gradient Norm after: 22.360677373320538
Epoch 8276/10000, Prediction Accuracy = 60.2%, Loss = 0.7188152074813843
Epoch: 8276, Batch Gradient Norm: 27.32106913978489
Epoch: 8276, Batch Gradient Norm after: 22.360676081087725
Epoch 8277/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.7158252000808716
Epoch: 8277, Batch Gradient Norm: 28.647784696148825
Epoch: 8277, Batch Gradient Norm after: 22.360677058500006
Epoch 8278/10000, Prediction Accuracy = 60.152%, Loss = 0.7187784552574158
Epoch: 8278, Batch Gradient Norm: 27.32112932674592
Epoch: 8278, Batch Gradient Norm after: 22.36067901771294
Epoch 8279/10000, Prediction Accuracy = 60.194%, Loss = 0.7157502174377441
Epoch: 8279, Batch Gradient Norm: 28.641075603106266
Epoch: 8279, Batch Gradient Norm after: 22.360678556017422
Epoch 8280/10000, Prediction Accuracy = 60.222%, Loss = 0.718710744380951
Epoch: 8280, Batch Gradient Norm: 27.32341791576374
Epoch: 8280, Batch Gradient Norm after: 22.36067667713351
Epoch 8281/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.715712308883667
Epoch: 8281, Batch Gradient Norm: 28.64354020431591
Epoch: 8281, Batch Gradient Norm after: 22.360678527552555
Epoch 8282/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7188090682029724
Epoch: 8282, Batch Gradient Norm: 27.31170051680542
Epoch: 8282, Batch Gradient Norm after: 22.360677702157332
Epoch 8283/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7158148169517518
Epoch: 8283, Batch Gradient Norm: 28.637311868923454
Epoch: 8283, Batch Gradient Norm after: 22.360678218422624
Epoch 8284/10000, Prediction Accuracy = 60.246%, Loss = 0.7188574552536011
Epoch: 8284, Batch Gradient Norm: 27.297831458876356
Epoch: 8284, Batch Gradient Norm after: 22.360677333681313
Epoch 8285/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7156888246536255
Epoch: 8285, Batch Gradient Norm: 28.639562916360287
Epoch: 8285, Batch Gradient Norm after: 22.360679584990987
Epoch 8286/10000, Prediction Accuracy = 60.178%, Loss = 0.7186001896858215
Epoch: 8286, Batch Gradient Norm: 27.300643092506924
Epoch: 8286, Batch Gradient Norm after: 22.36068033650259
Epoch 8287/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7155175328254699
Epoch: 8287, Batch Gradient Norm: 28.63979323260519
Epoch: 8287, Batch Gradient Norm after: 22.360676296335434
Epoch 8288/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7184722065925598
Epoch: 8288, Batch Gradient Norm: 27.304753470489995
Epoch: 8288, Batch Gradient Norm after: 22.360678567753876
Epoch 8289/10000, Prediction Accuracy = 60.178%, Loss = 0.7155095934867859
Epoch: 8289, Batch Gradient Norm: 28.627691333039763
Epoch: 8289, Batch Gradient Norm after: 22.36067672597746
Epoch 8290/10000, Prediction Accuracy = 60.152%, Loss = 0.7184432029724122
Epoch: 8290, Batch Gradient Norm: 27.30041072912809
Epoch: 8290, Batch Gradient Norm after: 22.36067986111283
Epoch 8291/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7153948426246644
Epoch: 8291, Batch Gradient Norm: 28.62911215329384
Epoch: 8291, Batch Gradient Norm after: 22.3606754056995
Epoch 8292/10000, Prediction Accuracy = 60.222%, Loss = 0.7183966755867004
Epoch: 8292, Batch Gradient Norm: 27.296115537407847
Epoch: 8292, Batch Gradient Norm after: 22.3606755269071
Epoch 8293/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7153740525245667
Epoch: 8293, Batch Gradient Norm: 28.631001487675192
Epoch: 8293, Batch Gradient Norm after: 22.360676075216006
Epoch 8294/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.7185104727745056
Epoch: 8294, Batch Gradient Norm: 27.2856916911636
Epoch: 8294, Batch Gradient Norm after: 22.36067535911278
Epoch 8295/10000, Prediction Accuracy = 60.218%, Loss = 0.7154563784599304
Epoch: 8295, Batch Gradient Norm: 28.625535383874524
Epoch: 8295, Batch Gradient Norm after: 22.360677957573085
Epoch 8296/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.7185116052627564
Epoch: 8296, Batch Gradient Norm: 27.271741185939845
Epoch: 8296, Batch Gradient Norm after: 22.360679142113614
Epoch 8297/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7153049826622009
Epoch: 8297, Batch Gradient Norm: 28.62839274685435
Epoch: 8297, Batch Gradient Norm after: 22.360677475433686
Epoch 8298/10000, Prediction Accuracy = 60.178%, Loss = 0.7182599902153015
Epoch: 8298, Batch Gradient Norm: 27.274500019339797
Epoch: 8298, Batch Gradient Norm after: 22.360678040568306
Epoch 8299/10000, Prediction Accuracy = 60.24000000000001%, Loss = 0.7151482105255127
Epoch: 8299, Batch Gradient Norm: 28.627811400441317
Epoch: 8299, Batch Gradient Norm after: 22.360678004727568
Epoch 8300/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7181580066680908
Epoch: 8300, Batch Gradient Norm: 27.274522028966786
Epoch: 8300, Batch Gradient Norm after: 22.360679782784626
Epoch 8301/10000, Prediction Accuracy = 60.174%, Loss = 0.7151347398757935
Epoch: 8301, Batch Gradient Norm: 28.617748746464187
Epoch: 8301, Batch Gradient Norm after: 22.360677308694147
Epoch 8302/10000, Prediction Accuracy = 60.164%, Loss = 0.7181210517883301
Epoch: 8302, Batch Gradient Norm: 27.271019153212126
Epoch: 8302, Batch Gradient Norm after: 22.360677986920994
Epoch 8303/10000, Prediction Accuracy = 60.222%, Loss = 0.7150158643722534
Epoch: 8303, Batch Gradient Norm: 28.618428418545285
Epoch: 8303, Batch Gradient Norm after: 22.36067926551316
Epoch 8304/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7180813670158386
Epoch: 8304, Batch Gradient Norm: 27.269635306807405
Epoch: 8304, Batch Gradient Norm after: 22.360679106283655
Epoch 8305/10000, Prediction Accuracy = 60.186%, Loss = 0.7150188684463501
Epoch: 8305, Batch Gradient Norm: 28.621676610045373
Epoch: 8305, Batch Gradient Norm after: 22.360677687819468
Epoch 8306/10000, Prediction Accuracy = 60.25%, Loss = 0.7182104229927063
Epoch: 8306, Batch Gradient Norm: 27.255851393603294
Epoch: 8306, Batch Gradient Norm after: 22.360676338421406
Epoch 8307/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7150979280471802
Epoch: 8307, Batch Gradient Norm: 28.613072047426343
Epoch: 8307, Batch Gradient Norm after: 22.36067817632189
Epoch 8308/10000, Prediction Accuracy = 60.254%, Loss = 0.7181741833686829
Epoch: 8308, Batch Gradient Norm: 27.245313485748607
Epoch: 8308, Batch Gradient Norm after: 22.36067832786749
Epoch 8309/10000, Prediction Accuracy = 60.202%, Loss = 0.7149282574653626
Epoch: 8309, Batch Gradient Norm: 28.617184782097706
Epoch: 8309, Batch Gradient Norm after: 22.36067793885286
Epoch 8310/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.7179234623908997
Epoch: 8310, Batch Gradient Norm: 27.248236951533766
Epoch: 8310, Batch Gradient Norm after: 22.360675666014206
Epoch 8311/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7147917985916138
Epoch: 8311, Batch Gradient Norm: 28.615198496218095
Epoch: 8311, Batch Gradient Norm after: 22.360678087657
Epoch 8312/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.7178385019302368
Epoch: 8312, Batch Gradient Norm: 27.246396454816754
Epoch: 8312, Batch Gradient Norm after: 22.36067900997273
Epoch 8313/10000, Prediction Accuracy = 60.19%, Loss = 0.7147806286811829
Epoch: 8313, Batch Gradient Norm: 28.60570421765294
Epoch: 8313, Batch Gradient Norm after: 22.36067758460059
Epoch 8314/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7177988409996032
Epoch: 8314, Batch Gradient Norm: 27.245150145475147
Epoch: 8314, Batch Gradient Norm after: 22.360680511164517
Epoch 8315/10000, Prediction Accuracy = 60.226%, Loss = 0.7146515369415283
Epoch: 8315, Batch Gradient Norm: 28.604175722649522
Epoch: 8315, Batch Gradient Norm after: 22.36067987364691
Epoch 8316/10000, Prediction Accuracy = 60.222%, Loss = 0.717773973941803
Epoch: 8316, Batch Gradient Norm: 27.241331447091625
Epoch: 8316, Batch Gradient Norm after: 22.360677726240585
Epoch 8317/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7146863460540771
Epoch: 8317, Batch Gradient Norm: 28.60918302604411
Epoch: 8317, Batch Gradient Norm after: 22.360678772895408
Epoch 8318/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7179239153861999
Epoch: 8318, Batch Gradient Norm: 27.226185446369694
Epoch: 8318, Batch Gradient Norm after: 22.360677736878102
Epoch 8319/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7147342443466187
Epoch: 8319, Batch Gradient Norm: 28.602711766020175
Epoch: 8319, Batch Gradient Norm after: 22.36067960366553
Epoch 8320/10000, Prediction Accuracy = 60.25%, Loss = 0.7178245186805725
Epoch: 8320, Batch Gradient Norm: 27.21827926516889
Epoch: 8320, Batch Gradient Norm after: 22.360679419874444
Epoch 8321/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7145330429077148
Epoch: 8321, Batch Gradient Norm: 28.607023548319916
Epoch: 8321, Batch Gradient Norm after: 22.36067735368297
Epoch 8322/10000, Prediction Accuracy = 60.218%, Loss = 0.7175796866416931
Epoch: 8322, Batch Gradient Norm: 27.220690273405182
Epoch: 8322, Batch Gradient Norm after: 22.360675661978643
Epoch 8323/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7144366264343261
Epoch: 8323, Batch Gradient Norm: 28.60177904662069
Epoch: 8323, Batch Gradient Norm after: 22.360677165737954
Epoch 8324/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.717521071434021
Epoch: 8324, Batch Gradient Norm: 27.222108197617505
Epoch: 8324, Batch Gradient Norm after: 22.36067857626412
Epoch 8325/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7143976569175721
Epoch: 8325, Batch Gradient Norm: 28.59349678665545
Epoch: 8325, Batch Gradient Norm after: 22.360677669758545
Epoch 8326/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.717468535900116
Epoch: 8326, Batch Gradient Norm: 27.214575454663567
Epoch: 8326, Batch Gradient Norm after: 22.36067951299349
Epoch 8327/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7142949104309082
Epoch: 8327, Batch Gradient Norm: 28.59674615663316
Epoch: 8327, Batch Gradient Norm after: 22.360678426030805
Epoch 8328/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7174837350845337
Epoch: 8328, Batch Gradient Norm: 27.212954688683453
Epoch: 8328, Batch Gradient Norm after: 22.360677939023088
Epoch 8329/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7143513679504394
Epoch: 8329, Batch Gradient Norm: 28.597614972117658
Epoch: 8329, Batch Gradient Norm after: 22.360677983553593
Epoch 8330/10000, Prediction Accuracy = 60.262%, Loss = 0.7176233172416687
Epoch: 8330, Batch Gradient Norm: 27.195564049995202
Epoch: 8330, Batch Gradient Norm after: 22.360678090689518
Epoch 8331/10000, Prediction Accuracy = 60.212%, Loss = 0.714346170425415
Epoch: 8331, Batch Gradient Norm: 28.593419697736948
Epoch: 8331, Batch Gradient Norm after: 22.360675068331044
Epoch 8332/10000, Prediction Accuracy = 60.218%, Loss = 0.7174585819244385
Epoch: 8332, Batch Gradient Norm: 27.188032964834413
Epoch: 8332, Batch Gradient Norm after: 22.36067682191708
Epoch 8333/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7141501426696777
Epoch: 8333, Batch Gradient Norm: 28.597495466900032
Epoch: 8333, Batch Gradient Norm after: 22.360676137650977
Epoch 8334/10000, Prediction Accuracy = 60.212%, Loss = 0.7172502517700196
Epoch: 8334, Batch Gradient Norm: 27.19543239880763
Epoch: 8334, Batch Gradient Norm after: 22.36067840246645
Epoch 8335/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7140881299972535
Epoch: 8335, Batch Gradient Norm: 28.588388195371724
Epoch: 8335, Batch Gradient Norm after: 22.360676684950207
Epoch 8336/10000, Prediction Accuracy = 60.176%, Loss = 0.7172054052352905
Epoch: 8336, Batch Gradient Norm: 27.189916338059273
Epoch: 8336, Batch Gradient Norm after: 22.360676836588944
Epoch 8337/10000, Prediction Accuracy = 60.17%, Loss = 0.7140313863754273
Epoch: 8337, Batch Gradient Norm: 28.5830153465752
Epoch: 8337, Batch Gradient Norm after: 22.360676669247404
Epoch 8338/10000, Prediction Accuracy = 60.202%, Loss = 0.7171495914459228
Epoch: 8338, Batch Gradient Norm: 27.188266470380853
Epoch: 8338, Batch Gradient Norm after: 22.360679545039485
Epoch 8339/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.7139357686042785
Epoch: 8339, Batch Gradient Norm: 28.587382577442735
Epoch: 8339, Batch Gradient Norm after: 22.360677983414465
Epoch 8340/10000, Prediction Accuracy = 60.212%, Loss = 0.7172024726867676
Epoch: 8340, Batch Gradient Norm: 27.17682260506795
Epoch: 8340, Batch Gradient Norm after: 22.36067864206805
Epoch 8341/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.714012610912323
Epoch: 8341, Batch Gradient Norm: 28.5840679133235
Epoch: 8341, Batch Gradient Norm after: 22.360677729541667
Epoch 8342/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7173291921615601
Epoch: 8342, Batch Gradient Norm: 27.159229201516442
Epoch: 8342, Batch Gradient Norm after: 22.36067726404108
Epoch 8343/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7139552235603333
Epoch: 8343, Batch Gradient Norm: 28.583687496604522
Epoch: 8343, Batch Gradient Norm after: 22.36067787253112
Epoch 8344/10000, Prediction Accuracy = 60.238%, Loss = 0.717110550403595
Epoch: 8344, Batch Gradient Norm: 27.153995741561303
Epoch: 8344, Batch Gradient Norm after: 22.360679679884967
Epoch 8345/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.713763165473938
Epoch: 8345, Batch Gradient Norm: 28.586692215881904
Epoch: 8345, Batch Gradient Norm after: 22.360676194301664
Epoch 8346/10000, Prediction Accuracy = 60.20200000000001%, Loss = 0.7169342041015625
Epoch: 8346, Batch Gradient Norm: 27.161392325809675
Epoch: 8346, Batch Gradient Norm after: 22.3606768359534
Epoch 8347/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7137323617935181
Epoch: 8347, Batch Gradient Norm: 28.57458588946737
Epoch: 8347, Batch Gradient Norm after: 22.360675864716015
Epoch 8348/10000, Prediction Accuracy = 60.174%, Loss = 0.7168980240821838
Epoch: 8348, Batch Gradient Norm: 27.162969186520446
Epoch: 8348, Batch Gradient Norm after: 22.36067816655895
Epoch 8349/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7136612892150879
Epoch: 8349, Batch Gradient Norm: 28.56976266492831
Epoch: 8349, Batch Gradient Norm after: 22.360676810216237
Epoch 8350/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7168319940567016
Epoch: 8350, Batch Gradient Norm: 27.15829500916832
Epoch: 8350, Batch Gradient Norm after: 22.36067886399374
Epoch 8351/10000, Prediction Accuracy = 60.206%, Loss = 0.7135944247245789
Epoch: 8351, Batch Gradient Norm: 28.57294430060593
Epoch: 8351, Batch Gradient Norm after: 22.36067691991392
Epoch 8352/10000, Prediction Accuracy = 60.222%, Loss = 0.716911518573761
Epoch: 8352, Batch Gradient Norm: 27.151485675179117
Epoch: 8352, Batch Gradient Norm after: 22.3606792303633
Epoch 8353/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7136705756187439
Epoch: 8353, Batch Gradient Norm: 28.566319724865124
Epoch: 8353, Batch Gradient Norm after: 22.36067842729385
Epoch 8354/10000, Prediction Accuracy = 60.266%, Loss = 0.7169798493385315
Epoch: 8354, Batch Gradient Norm: 27.14171467926983
Epoch: 8354, Batch Gradient Norm after: 22.36067626232574
Epoch 8355/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7135959267616272
Epoch: 8355, Batch Gradient Norm: 28.565012286178355
Epoch: 8355, Batch Gradient Norm after: 22.36067637889352
Epoch 8356/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7167409181594848
Epoch: 8356, Batch Gradient Norm: 27.14166496639501
Epoch: 8356, Batch Gradient Norm after: 22.360681166272055
Epoch 8357/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7134268760681153
Epoch: 8357, Batch Gradient Norm: 28.567428861222673
Epoch: 8357, Batch Gradient Norm after: 22.360676040020316
Epoch 8358/10000, Prediction Accuracy = 60.222%, Loss = 0.7165930032730102
Epoch: 8358, Batch Gradient Norm: 27.149296565632373
Epoch: 8358, Batch Gradient Norm after: 22.36067841229743
Epoch 8359/10000, Prediction Accuracy = 60.198%, Loss = 0.7134042024612427
Epoch: 8359, Batch Gradient Norm: 28.55582456760848
Epoch: 8359, Batch Gradient Norm after: 22.360675074873946
Epoch 8360/10000, Prediction Accuracy = 60.176%, Loss = 0.7165483713150025
Epoch: 8360, Batch Gradient Norm: 27.149760635227135
Epoch: 8360, Batch Gradient Norm after: 22.36067846101343
Epoch 8361/10000, Prediction Accuracy = 60.2%, Loss = 0.7133266568183899
Epoch: 8361, Batch Gradient Norm: 28.550197001342408
Epoch: 8361, Batch Gradient Norm after: 22.360677440825075
Epoch 8362/10000, Prediction Accuracy = 60.222%, Loss = 0.7164857029914856
Epoch: 8362, Batch Gradient Norm: 27.151707185368473
Epoch: 8362, Batch Gradient Norm after: 22.360678099433347
Epoch 8363/10000, Prediction Accuracy = 60.214%, Loss = 0.7132849574089051
Epoch: 8363, Batch Gradient Norm: 28.5494109192748
Epoch: 8363, Batch Gradient Norm after: 22.36067490839882
Epoch 8364/10000, Prediction Accuracy = 60.258%, Loss = 0.7165687561035157
Epoch: 8364, Batch Gradient Norm: 27.143101953644173
Epoch: 8364, Batch Gradient Norm after: 22.360678520617245
Epoch 8365/10000, Prediction Accuracy = 60.214%, Loss = 0.7133782982826233
Epoch: 8365, Batch Gradient Norm: 28.545338625935177
Epoch: 8365, Batch Gradient Norm after: 22.36067752997798
Epoch 8366/10000, Prediction Accuracy = 60.282%, Loss = 0.7166391491889954
Epoch: 8366, Batch Gradient Norm: 27.131301939133543
Epoch: 8366, Batch Gradient Norm after: 22.360680232481844
Epoch 8367/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7132734060287476
Epoch: 8367, Batch Gradient Norm: 28.544815783678953
Epoch: 8367, Batch Gradient Norm after: 22.360678189186284
Epoch 8368/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7163885116577149
Epoch: 8368, Batch Gradient Norm: 27.13313234903363
Epoch: 8368, Batch Gradient Norm after: 22.360678637911843
Epoch 8369/10000, Prediction Accuracy = 60.20400000000001%, Loss = 0.7131023287773133
Epoch: 8369, Batch Gradient Norm: 28.548758918543864
Epoch: 8369, Batch Gradient Norm after: 22.36067506585047
Epoch 8370/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7162459850311279
Epoch: 8370, Batch Gradient Norm: 27.135899665884658
Epoch: 8370, Batch Gradient Norm after: 22.360676408461952
Epoch 8371/10000, Prediction Accuracy = 60.188%, Loss = 0.7130872011184692
Epoch: 8371, Batch Gradient Norm: 28.534731205841233
Epoch: 8371, Batch Gradient Norm after: 22.360676788692682
Epoch 8372/10000, Prediction Accuracy = 60.164%, Loss = 0.7162128329277039
Epoch: 8372, Batch Gradient Norm: 27.132914834947933
Epoch: 8372, Batch Gradient Norm after: 22.360678089492396
Epoch 8373/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7129939198493958
Epoch: 8373, Batch Gradient Norm: 28.532866089308893
Epoch: 8373, Batch Gradient Norm after: 22.360677304959907
Epoch 8374/10000, Prediction Accuracy = 60.212%, Loss = 0.7161573886871337
Epoch: 8374, Batch Gradient Norm: 27.132660559637454
Epoch: 8374, Batch Gradient Norm after: 22.36067870254645
Epoch 8375/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.7129488468170166
Epoch: 8375, Batch Gradient Norm: 28.534596197037178
Epoch: 8375, Batch Gradient Norm after: 22.360677271577888
Epoch 8376/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.7162612080574036
Epoch: 8376, Batch Gradient Norm: 27.125449836537406
Epoch: 8376, Batch Gradient Norm after: 22.360679180867532
Epoch 8377/10000, Prediction Accuracy = 60.214%, Loss = 0.7130424499511718
Epoch: 8377, Batch Gradient Norm: 28.527637704605
Epoch: 8377, Batch Gradient Norm after: 22.3606772150103
Epoch 8378/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.7162922382354736
Epoch: 8378, Batch Gradient Norm: 27.113629096781693
Epoch: 8378, Batch Gradient Norm after: 22.360677721616938
Epoch 8379/10000, Prediction Accuracy = 60.238%, Loss = 0.71292564868927
Epoch: 8379, Batch Gradient Norm: 28.526136792900775
Epoch: 8379, Batch Gradient Norm after: 22.36067635207442
Epoch 8380/10000, Prediction Accuracy = 60.21%, Loss = 0.7160409688949585
Epoch: 8380, Batch Gradient Norm: 27.119251497720416
Epoch: 8380, Batch Gradient Norm after: 22.360678439088836
Epoch 8381/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7127670645713806
Epoch: 8381, Batch Gradient Norm: 28.526639389580964
Epoch: 8381, Batch Gradient Norm after: 22.360675664681416
Epoch 8382/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7159054398536682
Epoch: 8382, Batch Gradient Norm: 27.12102436758241
Epoch: 8382, Batch Gradient Norm after: 22.360677019902496
Epoch 8383/10000, Prediction Accuracy = 60.188%, Loss = 0.7127580642700195
Epoch: 8383, Batch Gradient Norm: 28.514401605687762
Epoch: 8383, Batch Gradient Norm after: 22.36067354779891
Epoch 8384/10000, Prediction Accuracy = 60.19%, Loss = 0.7158753752708436
Epoch: 8384, Batch Gradient Norm: 27.119730135849746
Epoch: 8384, Batch Gradient Norm after: 22.36067645738391
Epoch 8385/10000, Prediction Accuracy = 60.218%, Loss = 0.7126518130302429
Epoch: 8385, Batch Gradient Norm: 28.514912657428006
Epoch: 8385, Batch Gradient Norm after: 22.36067787321537
Epoch 8386/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7158222556114197
Epoch: 8386, Batch Gradient Norm: 27.120022029273297
Epoch: 8386, Batch Gradient Norm after: 22.360676999877374
Epoch 8387/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.7126305341720581
Epoch: 8387, Batch Gradient Norm: 28.51546513156413
Epoch: 8387, Batch Gradient Norm after: 22.360678089798444
Epoch 8388/10000, Prediction Accuracy = 60.278%, Loss = 0.715935218334198
Epoch: 8388, Batch Gradient Norm: 27.103970412200322
Epoch: 8388, Batch Gradient Norm after: 22.3606787489848
Epoch 8389/10000, Prediction Accuracy = 60.202%, Loss = 0.7127121806144714
Epoch: 8389, Batch Gradient Norm: 28.51078208042785
Epoch: 8389, Batch Gradient Norm after: 22.360679323348652
Epoch 8390/10000, Prediction Accuracy = 60.266%, Loss = 0.7159583687782287
Epoch: 8390, Batch Gradient Norm: 27.093235900934722
Epoch: 8390, Batch Gradient Norm after: 22.360679261495033
Epoch 8391/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7125702619552612
Epoch: 8391, Batch Gradient Norm: 28.51200168768858
Epoch: 8391, Batch Gradient Norm after: 22.360676993072
Epoch 8392/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7157021045684815
Epoch: 8392, Batch Gradient Norm: 27.09971123482794
Epoch: 8392, Batch Gradient Norm after: 22.360677661886882
Epoch 8393/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7124284386634827
Epoch: 8393, Batch Gradient Norm: 28.511338622545303
Epoch: 8393, Batch Gradient Norm after: 22.360676506139065
Epoch 8394/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7155762672424316
Epoch: 8394, Batch Gradient Norm: 27.101338571181447
Epoch: 8394, Batch Gradient Norm after: 22.360678845998475
Epoch 8395/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7124229073524475
Epoch: 8395, Batch Gradient Norm: 28.49926907062707
Epoch: 8395, Batch Gradient Norm after: 22.36067558735457
Epoch 8396/10000, Prediction Accuracy = 60.19%, Loss = 0.7155461072921753
Epoch: 8396, Batch Gradient Norm: 27.10312593281474
Epoch: 8396, Batch Gradient Norm after: 22.360678471626493
Epoch 8397/10000, Prediction Accuracy = 60.214%, Loss = 0.7123269200325012
Epoch: 8397, Batch Gradient Norm: 28.495162722451102
Epoch: 8397, Batch Gradient Norm after: 22.360679410612036
Epoch 8398/10000, Prediction Accuracy = 60.238%, Loss = 0.7154829382896424
Epoch: 8398, Batch Gradient Norm: 27.102871193719075
Epoch: 8398, Batch Gradient Norm after: 22.360676389330873
Epoch 8399/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7123199582099915
Epoch: 8399, Batch Gradient Norm: 28.496160011914213
Epoch: 8399, Batch Gradient Norm after: 22.36067798642603
Epoch 8400/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.7156084299087524
Epoch: 8400, Batch Gradient Norm: 27.096098277843737
Epoch: 8400, Batch Gradient Norm after: 22.360677174757097
Epoch 8401/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7124115824699402
Epoch: 8401, Batch Gradient Norm: 28.489926667350737
Epoch: 8401, Batch Gradient Norm after: 22.360677355313
Epoch 8402/10000, Prediction Accuracy = 60.262%, Loss = 0.7156067371368409
Epoch: 8402, Batch Gradient Norm: 27.089250305807646
Epoch: 8402, Batch Gradient Norm after: 22.360678330387824
Epoch 8403/10000, Prediction Accuracy = 60.234%, Loss = 0.7122740626335144
Epoch: 8403, Batch Gradient Norm: 28.48667253531224
Epoch: 8403, Batch Gradient Norm after: 22.360676924341934
Epoch 8404/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7153377771377564
Epoch: 8404, Batch Gradient Norm: 27.10118910876135
Epoch: 8404, Batch Gradient Norm after: 22.3606793658234
Epoch 8405/10000, Prediction Accuracy = 60.238%, Loss = 0.7121473670005798
Epoch: 8405, Batch Gradient Norm: 28.48164932235608
Epoch: 8405, Batch Gradient Norm after: 22.360676622069487
Epoch 8406/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7152191400527954
Epoch: 8406, Batch Gradient Norm: 27.107081816166364
Epoch: 8406, Batch Gradient Norm after: 22.360678234498394
Epoch 8407/10000, Prediction Accuracy = 60.186%, Loss = 0.7121482014656066
Epoch: 8407, Batch Gradient Norm: 28.4671476048903
Epoch: 8407, Batch Gradient Norm after: 22.36067654121143
Epoch 8408/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7151774048805237
Epoch: 8408, Batch Gradient Norm: 27.10762945905575
Epoch: 8408, Batch Gradient Norm after: 22.360678864209067
Epoch 8409/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.7120354890823364
Epoch: 8409, Batch Gradient Norm: 28.467355994550378
Epoch: 8409, Batch Gradient Norm after: 22.36067712786272
Epoch 8410/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7151377081871033
Epoch: 8410, Batch Gradient Norm: 27.107701266916784
Epoch: 8410, Batch Gradient Norm after: 22.360678030630474
Epoch 8411/10000, Prediction Accuracy = 60.212%, Loss = 0.7120501756668091
Epoch: 8411, Batch Gradient Norm: 28.469517465076994
Epoch: 8411, Batch Gradient Norm after: 22.360678468322565
Epoch 8412/10000, Prediction Accuracy = 60.274%, Loss = 0.7152517318725586
Epoch: 8412, Batch Gradient Norm: 27.098953793216236
Epoch: 8412, Batch Gradient Norm after: 22.360677980521967
Epoch 8413/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7121233820915223
Epoch: 8413, Batch Gradient Norm: 28.462442334033575
Epoch: 8413, Batch Gradient Norm after: 22.36067834685341
Epoch 8414/10000, Prediction Accuracy = 60.274%, Loss = 0.7152284622192383
Epoch: 8414, Batch Gradient Norm: 27.0949090507856
Epoch: 8414, Batch Gradient Norm after: 22.360676741063074
Epoch 8415/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.711981451511383
Epoch: 8415, Batch Gradient Norm: 28.459778398754775
Epoch: 8415, Batch Gradient Norm after: 22.36067683034418
Epoch 8416/10000, Prediction Accuracy = 60.226%, Loss = 0.7149662375450134
Epoch: 8416, Batch Gradient Norm: 27.10264279794955
Epoch: 8416, Batch Gradient Norm after: 22.360679454031132
Epoch 8417/10000, Prediction Accuracy = 60.22600000000001%, Loss = 0.7118654727935791
Epoch: 8417, Batch Gradient Norm: 28.452102131982976
Epoch: 8417, Batch Gradient Norm after: 22.360677913486757
Epoch 8418/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7148656725883484
Epoch: 8418, Batch Gradient Norm: 27.111507922571242
Epoch: 8418, Batch Gradient Norm after: 22.36067720099921
Epoch 8419/10000, Prediction Accuracy = 60.21%, Loss = 0.7118735551834107
Epoch: 8419, Batch Gradient Norm: 28.438488368038417
Epoch: 8419, Batch Gradient Norm after: 22.360676364566483
Epoch 8420/10000, Prediction Accuracy = 60.174%, Loss = 0.7148205518722535
Epoch: 8420, Batch Gradient Norm: 27.114019967891547
Epoch: 8420, Batch Gradient Norm after: 22.36067750189637
Epoch 8421/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7117546081542969
Epoch: 8421, Batch Gradient Norm: 28.436726606571682
Epoch: 8421, Batch Gradient Norm after: 22.36067843964394
Epoch 8422/10000, Prediction Accuracy = 60.214%, Loss = 0.7147922039031982
Epoch: 8422, Batch Gradient Norm: 27.11698153035421
Epoch: 8422, Batch Gradient Norm after: 22.36067554299209
Epoch 8423/10000, Prediction Accuracy = 60.218%, Loss = 0.7117978692054748
Epoch: 8423, Batch Gradient Norm: 28.439517272500275
Epoch: 8423, Batch Gradient Norm after: 22.36067916305367
Epoch 8424/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7149270892143249
Epoch: 8424, Batch Gradient Norm: 27.103340619355528
Epoch: 8424, Batch Gradient Norm after: 22.36067879328086
Epoch 8425/10000, Prediction Accuracy = 60.21%, Loss = 0.7118445038795471
Epoch: 8425, Batch Gradient Norm: 28.43032919426339
Epoch: 8425, Batch Gradient Norm after: 22.36067836122309
Epoch 8426/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7148495674133301
Epoch: 8426, Batch Gradient Norm: 27.09517563477201
Epoch: 8426, Batch Gradient Norm after: 22.360677124238848
Epoch 8427/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7116669535636901
Epoch: 8427, Batch Gradient Norm: 28.431634917126903
Epoch: 8427, Batch Gradient Norm after: 22.360678547217475
Epoch 8428/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7145909190177917
Epoch: 8428, Batch Gradient Norm: 27.104370774747366
Epoch: 8428, Batch Gradient Norm after: 22.360679720421366
Epoch 8429/10000, Prediction Accuracy = 60.176%, Loss = 0.711575448513031
Epoch: 8429, Batch Gradient Norm: 28.424050699300718
Epoch: 8429, Batch Gradient Norm after: 22.36067647168324
Epoch 8430/10000, Prediction Accuracy = 60.164%, Loss = 0.7145228505134582
Epoch: 8430, Batch Gradient Norm: 27.107196895383133
Epoch: 8430, Batch Gradient Norm after: 22.360678059822913
Epoch 8431/10000, Prediction Accuracy = 60.2%, Loss = 0.711539888381958
Epoch: 8431, Batch Gradient Norm: 28.413873546276335
Epoch: 8431, Batch Gradient Norm after: 22.36067698243361
Epoch 8432/10000, Prediction Accuracy = 60.194%, Loss = 0.7144715905189514
Epoch: 8432, Batch Gradient Norm: 27.1063761532331
Epoch: 8432, Batch Gradient Norm after: 22.36068027594867
Epoch 8433/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7114386677742004
Epoch: 8433, Batch Gradient Norm: 28.41818466559483
Epoch: 8433, Batch Gradient Norm after: 22.36067481170161
Epoch 8434/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7144755840301513
Epoch: 8434, Batch Gradient Norm: 27.098312491402314
Epoch: 8434, Batch Gradient Norm after: 22.36067548105324
Epoch 8435/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7114789366722107
Epoch: 8435, Batch Gradient Norm: 28.420035021112227
Epoch: 8435, Batch Gradient Norm after: 22.360679401409914
Epoch 8436/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7146076083183288
Epoch: 8436, Batch Gradient Norm: 27.084795853923033
Epoch: 8436, Batch Gradient Norm after: 22.360675416433445
Epoch 8437/10000, Prediction Accuracy = 60.222%, Loss = 0.7114932298660278
Epoch: 8437, Batch Gradient Norm: 28.415062248765857
Epoch: 8437, Batch Gradient Norm after: 22.360676110890626
Epoch 8438/10000, Prediction Accuracy = 60.27%, Loss = 0.7144941449165344
Epoch: 8438, Batch Gradient Norm: 27.07596018844597
Epoch: 8438, Batch Gradient Norm after: 22.360677160902984
Epoch 8439/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.71130450963974
Epoch: 8439, Batch Gradient Norm: 28.415933614657074
Epoch: 8439, Batch Gradient Norm after: 22.360677536797706
Epoch 8440/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7142544507980346
Epoch: 8440, Batch Gradient Norm: 27.080753144447623
Epoch: 8440, Batch Gradient Norm after: 22.360678305254062
Epoch 8441/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7112292170524597
Epoch: 8441, Batch Gradient Norm: 28.412450512802994
Epoch: 8441, Batch Gradient Norm after: 22.36067635886616
Epoch 8442/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7142042517662048
Epoch: 8442, Batch Gradient Norm: 27.082553457273498
Epoch: 8442, Batch Gradient Norm after: 22.36067919148657
Epoch 8443/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.711188280582428
Epoch: 8443, Batch Gradient Norm: 28.40319228747637
Epoch: 8443, Batch Gradient Norm after: 22.36067654254483
Epoch 8444/10000, Prediction Accuracy = 60.202%, Loss = 0.7141426205635071
Epoch: 8444, Batch Gradient Norm: 27.078737706250863
Epoch: 8444, Batch Gradient Norm after: 22.360681070844517
Epoch 8445/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7110856771469116
Epoch: 8445, Batch Gradient Norm: 28.40556541674486
Epoch: 8445, Batch Gradient Norm after: 22.360676979085866
Epoch 8446/10000, Prediction Accuracy = 60.226%, Loss = 0.714171028137207
Epoch: 8446, Batch Gradient Norm: 27.07210362305318
Epoch: 8446, Batch Gradient Norm after: 22.360675924211204
Epoch 8447/10000, Prediction Accuracy = 60.234%, Loss = 0.7111481785774231
Epoch: 8447, Batch Gradient Norm: 28.405976067673596
Epoch: 8447, Batch Gradient Norm after: 22.360678797272133
Epoch 8448/10000, Prediction Accuracy = 60.278%, Loss = 0.7143154740333557
Epoch: 8448, Batch Gradient Norm: 27.05682155484955
Epoch: 8448, Batch Gradient Norm after: 22.3606796560203
Epoch 8449/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7111456036567688
Epoch: 8449, Batch Gradient Norm: 28.401750695650914
Epoch: 8449, Batch Gradient Norm after: 22.360675760534985
Epoch 8450/10000, Prediction Accuracy = 60.262%, Loss = 0.7141504049301147
Epoch: 8450, Batch Gradient Norm: 27.053249165388273
Epoch: 8450, Batch Gradient Norm after: 22.360676111238437
Epoch 8451/10000, Prediction Accuracy = 60.234%, Loss = 0.7109453439712524
Epoch: 8451, Batch Gradient Norm: 28.404063995269205
Epoch: 8451, Batch Gradient Norm after: 22.360680180158088
Epoch 8452/10000, Prediction Accuracy = 60.21%, Loss = 0.7139186382293701
Epoch: 8452, Batch Gradient Norm: 27.056189079768544
Epoch: 8452, Batch Gradient Norm after: 22.360676880687727
Epoch 8453/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.7108844518661499
Epoch: 8453, Batch Gradient Norm: 28.39694690412062
Epoch: 8453, Batch Gradient Norm after: 22.36067882885301
Epoch 8454/10000, Prediction Accuracy = 60.148%, Loss = 0.7138909459114074
Epoch: 8454, Batch Gradient Norm: 27.054939106204223
Epoch: 8454, Batch Gradient Norm after: 22.36067739346667
Epoch 8455/10000, Prediction Accuracy = 60.17%, Loss = 0.7108237385749817
Epoch: 8455, Batch Gradient Norm: 28.392085813239785
Epoch: 8455, Batch Gradient Norm after: 22.360676967622247
Epoch 8456/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7138275980949402
Epoch: 8456, Batch Gradient Norm: 27.052694009382634
Epoch: 8456, Batch Gradient Norm after: 22.360677221674855
Epoch 8457/10000, Prediction Accuracy = 60.186%, Loss = 0.7107266545295715
Epoch: 8457, Batch Gradient Norm: 28.39504606530613
Epoch: 8457, Batch Gradient Norm after: 22.360677666052332
Epoch 8458/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7138664484024048
Epoch: 8458, Batch Gradient Norm: 27.043637575384583
Epoch: 8458, Batch Gradient Norm after: 22.360675128043802
Epoch 8459/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7107932329177856
Epoch: 8459, Batch Gradient Norm: 28.395986338795165
Epoch: 8459, Batch Gradient Norm after: 22.36067715735457
Epoch 8460/10000, Prediction Accuracy = 60.282%, Loss = 0.7140008807182312
Epoch: 8460, Batch Gradient Norm: 27.02687068619454
Epoch: 8460, Batch Gradient Norm after: 22.360676797827733
Epoch 8461/10000, Prediction Accuracy = 60.246%, Loss = 0.7107749104499816
Epoch: 8461, Batch Gradient Norm: 28.391889616347104
Epoch: 8461, Batch Gradient Norm after: 22.360678141427808
Epoch 8462/10000, Prediction Accuracy = 60.262%, Loss = 0.7138154864311218
Epoch: 8462, Batch Gradient Norm: 27.021185091894445
Epoch: 8462, Batch Gradient Norm after: 22.36067655001236
Epoch 8463/10000, Prediction Accuracy = 60.242%, Loss = 0.7105738162994385
Epoch: 8463, Batch Gradient Norm: 28.396666331992982
Epoch: 8463, Batch Gradient Norm after: 22.360676846890023
Epoch 8464/10000, Prediction Accuracy = 60.212%, Loss = 0.713610827922821
Epoch: 8464, Batch Gradient Norm: 27.020874336150907
Epoch: 8464, Batch Gradient Norm after: 22.360677521848913
Epoch 8465/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7105144381523132
Epoch: 8465, Batch Gradient Norm: 28.388570532268762
Epoch: 8465, Batch Gradient Norm after: 22.36067602148148
Epoch 8466/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.713590431213379
Epoch: 8466, Batch Gradient Norm: 27.016766255675453
Epoch: 8466, Batch Gradient Norm after: 22.360677265545934
Epoch 8467/10000, Prediction Accuracy = 60.176%, Loss = 0.7104467153549194
Epoch: 8467, Batch Gradient Norm: 28.384220521984453
Epoch: 8467, Batch Gradient Norm after: 22.36067764907437
Epoch 8468/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7135217070579529
Epoch: 8468, Batch Gradient Norm: 27.013155301778077
Epoch: 8468, Batch Gradient Norm after: 22.36067803953274
Epoch 8469/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7103640079498291
Epoch: 8469, Batch Gradient Norm: 28.387237620232522
Epoch: 8469, Batch Gradient Norm after: 22.36067859747855
Epoch 8470/10000, Prediction Accuracy = 60.27%, Loss = 0.7135886907577514
Epoch: 8470, Batch Gradient Norm: 27.005109069861927
Epoch: 8470, Batch Gradient Norm after: 22.36067806898012
Epoch 8471/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.7104407906532287
Epoch: 8471, Batch Gradient Norm: 28.384636796009108
Epoch: 8471, Batch Gradient Norm after: 22.360678490360193
Epoch 8472/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.7137061715126037
Epoch: 8472, Batch Gradient Norm: 26.989858156279507
Epoch: 8472, Batch Gradient Norm after: 22.360675690459377
Epoch 8473/10000, Prediction Accuracy = 60.254%, Loss = 0.7103843212127685
Epoch: 8473, Batch Gradient Norm: 28.381359551592194
Epoch: 8473, Batch Gradient Norm after: 22.360677993175482
Epoch 8474/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.7134809613227844
Epoch: 8474, Batch Gradient Norm: 26.99006837882308
Epoch: 8474, Batch Gradient Norm after: 22.360676910496913
Epoch 8475/10000, Prediction Accuracy = 60.234%, Loss = 0.7101921319961548
Epoch: 8475, Batch Gradient Norm: 28.38382923391962
Epoch: 8475, Batch Gradient Norm after: 22.360677708894904
Epoch 8476/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.713302493095398
Epoch: 8476, Batch Gradient Norm: 26.99015386900703
Epoch: 8476, Batch Gradient Norm after: 22.360676162142916
Epoch 8477/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7101588726043702
Epoch: 8477, Batch Gradient Norm: 28.376786954346855
Epoch: 8477, Batch Gradient Norm after: 22.360678771834806
Epoch 8478/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7132723331451416
Epoch: 8478, Batch Gradient Norm: 26.983590810783767
Epoch: 8478, Batch Gradient Norm after: 22.36067626785835
Epoch 8479/10000, Prediction Accuracy = 60.166%, Loss = 0.7100809454917908
Epoch: 8479, Batch Gradient Norm: 28.372607657162717
Epoch: 8479, Batch Gradient Norm after: 22.360676923892015
Epoch 8480/10000, Prediction Accuracy = 60.2%, Loss = 0.7132159829139709
Epoch: 8480, Batch Gradient Norm: 26.985639405474384
Epoch: 8480, Batch Gradient Norm after: 22.360676855187545
Epoch 8481/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.7100046396255493
Epoch: 8481, Batch Gradient Norm: 28.373644805429524
Epoch: 8481, Batch Gradient Norm after: 22.3606771420893
Epoch 8482/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.7132828235626221
Epoch: 8482, Batch Gradient Norm: 26.9740460054067
Epoch: 8482, Batch Gradient Norm after: 22.360676659517605
Epoch 8483/10000, Prediction Accuracy = 60.246%, Loss = 0.7100917220115661
Epoch: 8483, Batch Gradient Norm: 28.36986353969041
Epoch: 8483, Batch Gradient Norm after: 22.360678914600538
Epoch 8484/10000, Prediction Accuracy = 60.286%, Loss = 0.7134033918380738
Epoch: 8484, Batch Gradient Norm: 26.960015498163937
Epoch: 8484, Batch Gradient Norm after: 22.36067741480177
Epoch 8485/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7100327134132385
Epoch: 8485, Batch Gradient Norm: 28.36840418116228
Epoch: 8485, Batch Gradient Norm after: 22.360677957797286
Epoch 8486/10000, Prediction Accuracy = 60.274%, Loss = 0.7131734848022461
Epoch: 8486, Batch Gradient Norm: 26.95599646529349
Epoch: 8486, Batch Gradient Norm after: 22.360675927683765
Epoch 8487/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7098432540893554
Epoch: 8487, Batch Gradient Norm: 28.374801456492122
Epoch: 8487, Batch Gradient Norm after: 22.360677102702354
Epoch 8488/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.7129912376403809
Epoch: 8488, Batch Gradient Norm: 26.961795780376516
Epoch: 8488, Batch Gradient Norm after: 22.360676382589244
Epoch 8489/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7098137855529785
Epoch: 8489, Batch Gradient Norm: 28.36389207872438
Epoch: 8489, Batch Gradient Norm after: 22.360676245975604
Epoch 8490/10000, Prediction Accuracy = 60.166%, Loss = 0.7129677772521973
Epoch: 8490, Batch Gradient Norm: 26.95931414184599
Epoch: 8490, Batch Gradient Norm after: 22.36067752975737
Epoch 8491/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7097272157669068
Epoch: 8491, Batch Gradient Norm: 28.360640055352256
Epoch: 8491, Batch Gradient Norm after: 22.360675733007334
Epoch 8492/10000, Prediction Accuracy = 60.212%, Loss = 0.7129004955291748
Epoch: 8492, Batch Gradient Norm: 26.95403738287527
Epoch: 8492, Batch Gradient Norm after: 22.360675722778915
Epoch 8493/10000, Prediction Accuracy = 60.21%, Loss = 0.7096568703651428
Epoch: 8493, Batch Gradient Norm: 28.36325040345569
Epoch: 8493, Batch Gradient Norm after: 22.36067530094217
Epoch 8494/10000, Prediction Accuracy = 60.28399999999999%, Loss = 0.7129847407341003
Epoch: 8494, Batch Gradient Norm: 26.948352362265595
Epoch: 8494, Batch Gradient Norm after: 22.36067700722143
Epoch 8495/10000, Prediction Accuracy = 60.254%, Loss = 0.7097453236579895
Epoch: 8495, Batch Gradient Norm: 28.358008912329367
Epoch: 8495, Batch Gradient Norm after: 22.360680288898823
Epoch 8496/10000, Prediction Accuracy = 60.294%, Loss = 0.7130728125572204
Epoch: 8496, Batch Gradient Norm: 26.93620244216834
Epoch: 8496, Batch Gradient Norm after: 22.360674603282632
Epoch 8497/10000, Prediction Accuracy = 60.254%, Loss = 0.7096694111824036
Epoch: 8497, Batch Gradient Norm: 28.35600678564177
Epoch: 8497, Batch Gradient Norm after: 22.36067521481998
Epoch 8498/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.7128348827362061
Epoch: 8498, Batch Gradient Norm: 26.937828707600918
Epoch: 8498, Batch Gradient Norm after: 22.360678055156242
Epoch 8499/10000, Prediction Accuracy = 60.246%, Loss = 0.7095009207725524
Epoch: 8499, Batch Gradient Norm: 28.360893707875817
Epoch: 8499, Batch Gradient Norm after: 22.360676141949735
Epoch 8500/10000, Prediction Accuracy = 60.198%, Loss = 0.7126700401306152
Epoch: 8500, Batch Gradient Norm: 26.94077140668429
Epoch: 8500, Batch Gradient Norm after: 22.36067811425668
Epoch 8501/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7094717025756836
Epoch: 8501, Batch Gradient Norm: 28.3497650779669
Epoch: 8501, Batch Gradient Norm after: 22.360676098322955
Epoch 8502/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7126417636871338
Epoch: 8502, Batch Gradient Norm: 26.937157048233132
Epoch: 8502, Batch Gradient Norm after: 22.36067804892886
Epoch 8503/10000, Prediction Accuracy = 60.162%, Loss = 0.7093876719474792
Epoch: 8503, Batch Gradient Norm: 28.345839714110195
Epoch: 8503, Batch Gradient Norm after: 22.360676730369605
Epoch 8504/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.712583863735199
Epoch: 8504, Batch Gradient Norm: 26.93981660541663
Epoch: 8504, Batch Gradient Norm after: 22.36067726237213
Epoch 8505/10000, Prediction Accuracy = 60.212%, Loss = 0.7093270421028137
Epoch: 8505, Batch Gradient Norm: 28.346575669822162
Epoch: 8505, Batch Gradient Norm after: 22.360676643937794
Epoch 8506/10000, Prediction Accuracy = 60.266%, Loss = 0.7126735925674439
Epoch: 8506, Batch Gradient Norm: 26.931514525279443
Epoch: 8506, Batch Gradient Norm after: 22.360677268420062
Epoch 8507/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7094366431236268
Epoch: 8507, Batch Gradient Norm: 28.34207695291428
Epoch: 8507, Batch Gradient Norm after: 22.360677856813755
Epoch 8508/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7127632379531861
Epoch: 8508, Batch Gradient Norm: 26.91470481000806
Epoch: 8508, Batch Gradient Norm after: 22.360675238495258
Epoch 8509/10000, Prediction Accuracy = 60.238%, Loss = 0.7093287706375122
Epoch: 8509, Batch Gradient Norm: 28.342753003241725
Epoch: 8509, Batch Gradient Norm after: 22.36067705509163
Epoch 8510/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.712503182888031
Epoch: 8510, Batch Gradient Norm: 26.912371235685363
Epoch: 8510, Batch Gradient Norm after: 22.360677183927198
Epoch 8511/10000, Prediction Accuracy = 60.214%, Loss = 0.7091489434242249
Epoch: 8511, Batch Gradient Norm: 28.34874932783703
Epoch: 8511, Batch Gradient Norm after: 22.36067819860244
Epoch 8512/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7123605728149414
Epoch: 8512, Batch Gradient Norm: 26.916084236352642
Epoch: 8512, Batch Gradient Norm after: 22.36067868568594
Epoch 8513/10000, Prediction Accuracy = 60.174%, Loss = 0.709132719039917
Epoch: 8513, Batch Gradient Norm: 28.33437371401334
Epoch: 8513, Batch Gradient Norm after: 22.36067642845892
Epoch 8514/10000, Prediction Accuracy = 60.152%, Loss = 0.7123295664787292
Epoch: 8514, Batch Gradient Norm: 26.916053388978366
Epoch: 8514, Batch Gradient Norm after: 22.360677557876343
Epoch 8515/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7090338468551636
Epoch: 8515, Batch Gradient Norm: 28.331769516577932
Epoch: 8515, Batch Gradient Norm after: 22.36067686191772
Epoch 8516/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7122689366340638
Epoch: 8516, Batch Gradient Norm: 26.915027034297367
Epoch: 8516, Batch Gradient Norm after: 22.360676368046864
Epoch 8517/10000, Prediction Accuracy = 60.202%, Loss = 0.7090003728866577
Epoch: 8517, Batch Gradient Norm: 28.336994179831798
Epoch: 8517, Batch Gradient Norm after: 22.360677127057933
Epoch 8518/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7123904228210449
Epoch: 8518, Batch Gradient Norm: 26.903602383323154
Epoch: 8518, Batch Gradient Norm after: 22.36067732597671
Epoch 8519/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.7090879440307617
Epoch: 8519, Batch Gradient Norm: 28.331205670029128
Epoch: 8519, Batch Gradient Norm after: 22.360679915465973
Epoch 8520/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.7124325037002563
Epoch: 8520, Batch Gradient Norm: 26.89111704841921
Epoch: 8520, Batch Gradient Norm after: 22.36067982936352
Epoch 8521/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.7089534401893616
Epoch: 8521, Batch Gradient Norm: 28.33451463972701
Epoch: 8521, Batch Gradient Norm after: 22.36067724245202
Epoch 8522/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7121700525283814
Epoch: 8522, Batch Gradient Norm: 26.890998241459155
Epoch: 8522, Batch Gradient Norm after: 22.360677309458985
Epoch 8523/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.708799421787262
Epoch: 8523, Batch Gradient Norm: 28.334459210792794
Epoch: 8523, Batch Gradient Norm after: 22.36067592624434
Epoch 8524/10000, Prediction Accuracy = 60.202%, Loss = 0.7120420217514039
Epoch: 8524, Batch Gradient Norm: 26.88989229839592
Epoch: 8524, Batch Gradient Norm after: 22.360678257073374
Epoch 8525/10000, Prediction Accuracy = 60.176%, Loss = 0.708792507648468
Epoch: 8525, Batch Gradient Norm: 28.324989309930743
Epoch: 8525, Batch Gradient Norm after: 22.36067557920436
Epoch 8526/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.7120200157165527
Epoch: 8526, Batch Gradient Norm: 26.883781726661084
Epoch: 8526, Batch Gradient Norm after: 22.36067747525635
Epoch 8527/10000, Prediction Accuracy = 60.176%, Loss = 0.7086684823036193
Epoch: 8527, Batch Gradient Norm: 28.324868881354355
Epoch: 8527, Batch Gradient Norm after: 22.360677388945696
Epoch 8528/10000, Prediction Accuracy = 60.214%, Loss = 0.7119740843772888
Epoch: 8528, Batch Gradient Norm: 26.886374804867586
Epoch: 8528, Batch Gradient Norm after: 22.360675053000264
Epoch 8529/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7086494207382202
Epoch: 8529, Batch Gradient Norm: 28.32806071104483
Epoch: 8529, Batch Gradient Norm after: 22.36067646989074
Epoch 8530/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.7121161341667175
Epoch: 8530, Batch Gradient Norm: 26.875181932697835
Epoch: 8530, Batch Gradient Norm after: 22.36067649969581
Epoch 8531/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7087530374526978
Epoch: 8531, Batch Gradient Norm: 28.320634554304572
Epoch: 8531, Batch Gradient Norm after: 22.360676073909648
Epoch 8532/10000, Prediction Accuracy = 60.282%, Loss = 0.712109899520874
Epoch: 8532, Batch Gradient Norm: 26.863883926734506
Epoch: 8532, Batch Gradient Norm after: 22.360677222405304
Epoch 8533/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7085860371589661
Epoch: 8533, Batch Gradient Norm: 28.319546054999503
Epoch: 8533, Batch Gradient Norm after: 22.360678033845186
Epoch 8534/10000, Prediction Accuracy = 60.238%, Loss = 0.7118317604064941
Epoch: 8534, Batch Gradient Norm: 26.866084110919342
Epoch: 8534, Batch Gradient Norm after: 22.360679951286045
Epoch 8535/10000, Prediction Accuracy = 60.194%, Loss = 0.7084511637687683
Epoch: 8535, Batch Gradient Norm: 28.324150303901266
Epoch: 8535, Batch Gradient Norm after: 22.36067458972552
Epoch 8536/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.7117353200912475
Epoch: 8536, Batch Gradient Norm: 26.86780569376839
Epoch: 8536, Batch Gradient Norm after: 22.360678452001427
Epoch 8537/10000, Prediction Accuracy = 60.186%, Loss = 0.7084402084350586
Epoch: 8537, Batch Gradient Norm: 28.31099478386754
Epoch: 8537, Batch Gradient Norm after: 22.360676263433476
Epoch 8538/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7116951704025268
Epoch: 8538, Batch Gradient Norm: 26.86594881835345
Epoch: 8538, Batch Gradient Norm after: 22.360677538817704
Epoch 8539/10000, Prediction Accuracy = 60.198%, Loss = 0.7083165645599365
Epoch: 8539, Batch Gradient Norm: 28.312758665239013
Epoch: 8539, Batch Gradient Norm after: 22.360676983823772
Epoch 8540/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7116687417030334
Epoch: 8540, Batch Gradient Norm: 26.866244052603466
Epoch: 8540, Batch Gradient Norm after: 22.360675152046216
Epoch 8541/10000, Prediction Accuracy = 60.258%, Loss = 0.7083460450172424
Epoch: 8541, Batch Gradient Norm: 28.312757336479756
Epoch: 8541, Batch Gradient Norm after: 22.360675268266398
Epoch 8542/10000, Prediction Accuracy = 60.27%, Loss = 0.7118128180503845
Epoch: 8542, Batch Gradient Norm: 26.853855629561803
Epoch: 8542, Batch Gradient Norm after: 22.360676935462983
Epoch 8543/10000, Prediction Accuracy = 60.246%, Loss = 0.7084081292152404
Epoch: 8543, Batch Gradient Norm: 28.308133372985893
Epoch: 8543, Batch Gradient Norm after: 22.36067476051619
Epoch 8544/10000, Prediction Accuracy = 60.286%, Loss = 0.711756682395935
Epoch: 8544, Batch Gradient Norm: 26.84321999228488
Epoch: 8544, Batch Gradient Norm after: 22.360676399706286
Epoch 8545/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7082269668579102
Epoch: 8545, Batch Gradient Norm: 28.309813345352463
Epoch: 8545, Batch Gradient Norm after: 22.36067741149524
Epoch 8546/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.711496901512146
Epoch: 8546, Batch Gradient Norm: 26.84801409899452
Epoch: 8546, Batch Gradient Norm after: 22.360677926702614
Epoch 8547/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.7081202149391175
Epoch: 8547, Batch Gradient Norm: 28.309685439348122
Epoch: 8547, Batch Gradient Norm after: 22.360676296882446
Epoch 8548/10000, Prediction Accuracy = 60.17%, Loss = 0.7114271283149719
Epoch: 8548, Batch Gradient Norm: 26.84647158331022
Epoch: 8548, Batch Gradient Norm after: 22.360676182549742
Epoch 8549/10000, Prediction Accuracy = 60.194%, Loss = 0.7081116199493408
Epoch: 8549, Batch Gradient Norm: 28.299065479908325
Epoch: 8549, Batch Gradient Norm after: 22.360679416193328
Epoch 8550/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7113835215568542
Epoch: 8550, Batch Gradient Norm: 26.84631033105746
Epoch: 8550, Batch Gradient Norm after: 22.360676938002786
Epoch 8551/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.707982063293457
Epoch: 8551, Batch Gradient Norm: 28.300363572803043
Epoch: 8551, Batch Gradient Norm after: 22.360676925938083
Epoch 8552/10000, Prediction Accuracy = 60.242%, Loss = 0.7113745450973511
Epoch: 8552, Batch Gradient Norm: 26.847621993481173
Epoch: 8552, Batch Gradient Norm after: 22.360676464682964
Epoch 8553/10000, Prediction Accuracy = 60.274%, Loss = 0.7080333232879639
Epoch: 8553, Batch Gradient Norm: 28.299330808918693
Epoch: 8553, Batch Gradient Norm after: 22.360677075582064
Epoch 8554/10000, Prediction Accuracy = 60.294%, Loss = 0.711529552936554
Epoch: 8554, Batch Gradient Norm: 26.83668046041377
Epoch: 8554, Batch Gradient Norm after: 22.360676237130377
Epoch 8555/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7080747723579407
Epoch: 8555, Batch Gradient Norm: 28.294758392299077
Epoch: 8555, Batch Gradient Norm after: 22.360676425198246
Epoch 8556/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.7114039778709411
Epoch: 8556, Batch Gradient Norm: 26.833072981462845
Epoch: 8556, Batch Gradient Norm after: 22.36067449307345
Epoch 8557/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.7078855991363525
Epoch: 8557, Batch Gradient Norm: 28.296586939847554
Epoch: 8557, Batch Gradient Norm after: 22.360676871469465
Epoch 8558/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7111597776412963
Epoch: 8558, Batch Gradient Norm: 26.835510216811354
Epoch: 8558, Batch Gradient Norm after: 22.3606788046522
Epoch 8559/10000, Prediction Accuracy = 60.176%, Loss = 0.7078062653541565
Epoch: 8559, Batch Gradient Norm: 28.293266372893687
Epoch: 8559, Batch Gradient Norm after: 22.360675688057274
Epoch 8560/10000, Prediction Accuracy = 60.174%, Loss = 0.7111022233963012
Epoch: 8560, Batch Gradient Norm: 26.83747645979669
Epoch: 8560, Batch Gradient Norm after: 22.36067986368517
Epoch 8561/10000, Prediction Accuracy = 60.188%, Loss = 0.707783305644989
Epoch: 8561, Batch Gradient Norm: 28.28031160370599
Epoch: 8561, Batch Gradient Norm after: 22.360675625422687
Epoch 8562/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.7110443353652954
Epoch: 8562, Batch Gradient Norm: 26.838783814468513
Epoch: 8562, Batch Gradient Norm after: 22.3606798924099
Epoch 8563/10000, Prediction Accuracy = 60.222%, Loss = 0.7076713800430298
Epoch: 8563, Batch Gradient Norm: 28.284868363124755
Epoch: 8563, Batch Gradient Norm after: 22.360676473020312
Epoch 8564/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.7110560417175293
Epoch: 8564, Batch Gradient Norm: 26.83891733802564
Epoch: 8564, Batch Gradient Norm after: 22.360674191915503
Epoch 8565/10000, Prediction Accuracy = 60.262%, Loss = 0.707736361026764
Epoch: 8565, Batch Gradient Norm: 28.283428823745147
Epoch: 8565, Batch Gradient Norm after: 22.360678429522945
Epoch 8566/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.711200749874115
Epoch: 8566, Batch Gradient Norm: 26.82574866935853
Epoch: 8566, Batch Gradient Norm after: 22.360677082390918
Epoch 8567/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7077605724334717
Epoch: 8567, Batch Gradient Norm: 28.276949660672404
Epoch: 8567, Batch Gradient Norm after: 22.36067768941955
Epoch 8568/10000, Prediction Accuracy = 60.306%, Loss = 0.711053192615509
Epoch: 8568, Batch Gradient Norm: 26.822954609384567
Epoch: 8568, Batch Gradient Norm after: 22.36067518435452
Epoch 8569/10000, Prediction Accuracy = 60.254%, Loss = 0.7075726389884949
Epoch: 8569, Batch Gradient Norm: 28.279884095644412
Epoch: 8569, Batch Gradient Norm after: 22.36067412579778
Epoch 8570/10000, Prediction Accuracy = 60.242%, Loss = 0.710815942287445
Epoch: 8570, Batch Gradient Norm: 26.827581780559083
Epoch: 8570, Batch Gradient Norm after: 22.36067729462669
Epoch 8571/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7075119614601135
Epoch: 8571, Batch Gradient Norm: 28.27374353056595
Epoch: 8571, Batch Gradient Norm after: 22.360674828091714
Epoch 8572/10000, Prediction Accuracy = 60.158%, Loss = 0.7107743501663208
Epoch: 8572, Batch Gradient Norm: 26.827332266386815
Epoch: 8572, Batch Gradient Norm after: 22.36067751645486
Epoch 8573/10000, Prediction Accuracy = 60.19%, Loss = 0.707480251789093
Epoch: 8573, Batch Gradient Norm: 28.263735970746723
Epoch: 8573, Batch Gradient Norm after: 22.360672664363186
Epoch 8574/10000, Prediction Accuracy = 60.186%, Loss = 0.7107145309448242
Epoch: 8574, Batch Gradient Norm: 26.824870395917795
Epoch: 8574, Batch Gradient Norm after: 22.36067808841756
Epoch 8575/10000, Prediction Accuracy = 60.212%, Loss = 0.7073555231094361
Epoch: 8575, Batch Gradient Norm: 28.26851196196874
Epoch: 8575, Batch Gradient Norm after: 22.360677469289623
Epoch 8576/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.7107580900192261
Epoch: 8576, Batch Gradient Norm: 26.821353262491485
Epoch: 8576, Batch Gradient Norm after: 22.360677281310984
Epoch 8577/10000, Prediction Accuracy = 60.274%, Loss = 0.7074477434158325
Epoch: 8577, Batch Gradient Norm: 28.269062629556714
Epoch: 8577, Batch Gradient Norm after: 22.360677625066817
Epoch 8578/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7109061479568481
Epoch: 8578, Batch Gradient Norm: 26.806914252017098
Epoch: 8578, Batch Gradient Norm after: 22.360677394639527
Epoch 8579/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.7074264645576477
Epoch: 8579, Batch Gradient Norm: 28.26371580017014
Epoch: 8579, Batch Gradient Norm after: 22.360678012136006
Epoch 8580/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.7107082724571228
Epoch: 8580, Batch Gradient Norm: 26.802311393222915
Epoch: 8580, Batch Gradient Norm after: 22.36067709865973
Epoch 8581/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.707222068309784
Epoch: 8581, Batch Gradient Norm: 28.26991294643691
Epoch: 8581, Batch Gradient Norm after: 22.360677286339776
Epoch 8582/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7104966640472412
Epoch: 8582, Batch Gradient Norm: 26.80995096734743
Epoch: 8582, Batch Gradient Norm after: 22.360677853505
Epoch 8583/10000, Prediction Accuracy = 60.174%, Loss = 0.7071897506713867
Epoch: 8583, Batch Gradient Norm: 28.25876712269729
Epoch: 8583, Batch Gradient Norm after: 22.36067517463227
Epoch 8584/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7104603409767151
Epoch: 8584, Batch Gradient Norm: 26.806795202979103
Epoch: 8584, Batch Gradient Norm after: 22.36067719861303
Epoch 8585/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7071340918540955
Epoch: 8585, Batch Gradient Norm: 28.250692085034764
Epoch: 8585, Batch Gradient Norm after: 22.36067561266834
Epoch 8586/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.7103939652442932
Epoch: 8586, Batch Gradient Norm: 26.812770540470602
Epoch: 8586, Batch Gradient Norm after: 22.360677327038463
Epoch 8587/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7070460915565491
Epoch: 8587, Batch Gradient Norm: 28.254593111576572
Epoch: 8587, Batch Gradient Norm after: 22.36067561617848
Epoch 8588/10000, Prediction Accuracy = 60.302%, Loss = 0.7104601383209228
Epoch: 8588, Batch Gradient Norm: 26.803252466433097
Epoch: 8588, Batch Gradient Norm after: 22.360677010707665
Epoch 8589/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7071430087089539
Epoch: 8589, Batch Gradient Norm: 28.25294955561543
Epoch: 8589, Batch Gradient Norm after: 22.360678516851966
Epoch 8590/10000, Prediction Accuracy = 60.286%, Loss = 0.7105869650840759
Epoch: 8590, Batch Gradient Norm: 26.791769661379618
Epoch: 8590, Batch Gradient Norm after: 22.360676714669246
Epoch 8591/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.7070914268493652
Epoch: 8591, Batch Gradient Norm: 28.246537163535343
Epoch: 8591, Batch Gradient Norm after: 22.360678844804614
Epoch 8592/10000, Prediction Accuracy = 60.286%, Loss = 0.7103591561317444
Epoch: 8592, Batch Gradient Norm: 26.79541718908191
Epoch: 8592, Batch Gradient Norm after: 22.360677359459114
Epoch 8593/10000, Prediction Accuracy = 60.254%, Loss = 0.70690838098526
Epoch: 8593, Batch Gradient Norm: 28.24512115833269
Epoch: 8593, Batch Gradient Norm after: 22.360675301555755
Epoch 8594/10000, Prediction Accuracy = 60.222%, Loss = 0.7101572513580322
Epoch: 8594, Batch Gradient Norm: 26.80460183252047
Epoch: 8594, Batch Gradient Norm after: 22.360676297702224
Epoch 8595/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7069003582000732
Epoch: 8595, Batch Gradient Norm: 28.236457767683127
Epoch: 8595, Batch Gradient Norm after: 22.360673947245537
Epoch 8596/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7101263046264649
Epoch: 8596, Batch Gradient Norm: 26.803532885939074
Epoch: 8596, Batch Gradient Norm after: 22.36067809230017
Epoch 8597/10000, Prediction Accuracy = 60.19%, Loss = 0.7068374037742615
Epoch: 8597, Batch Gradient Norm: 28.22731437917446
Epoch: 8597, Batch Gradient Norm after: 22.36067439422417
Epoch 8598/10000, Prediction Accuracy = 60.174%, Loss = 0.7100538372993469
Epoch: 8598, Batch Gradient Norm: 26.80431018503447
Epoch: 8598, Batch Gradient Norm after: 22.360679796118767
Epoch 8599/10000, Prediction Accuracy = 60.234%, Loss = 0.7067582726478576
Epoch: 8599, Batch Gradient Norm: 28.236119264471785
Epoch: 8599, Batch Gradient Norm after: 22.360676261669543
Epoch 8600/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.710151195526123
Epoch: 8600, Batch Gradient Norm: 26.80085700997496
Epoch: 8600, Batch Gradient Norm after: 22.360677473160433
Epoch 8601/10000, Prediction Accuracy = 60.226%, Loss = 0.7068705081939697
Epoch: 8601, Batch Gradient Norm: 28.22894626704184
Epoch: 8601, Batch Gradient Norm after: 22.360677152032235
Epoch 8602/10000, Prediction Accuracy = 60.29%, Loss = 0.7102388501167297
Epoch: 8602, Batch Gradient Norm: 26.79233885507752
Epoch: 8602, Batch Gradient Norm after: 22.36067652443136
Epoch 8603/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.7067972540855407
Epoch: 8603, Batch Gradient Norm: 28.22307278415702
Epoch: 8603, Batch Gradient Norm after: 22.36067830105259
Epoch 8604/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.7099903583526611
Epoch: 8604, Batch Gradient Norm: 26.798222211737215
Epoch: 8604, Batch Gradient Norm after: 22.360678078148926
Epoch 8605/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7066254377365112
Epoch: 8605, Batch Gradient Norm: 28.224037088623245
Epoch: 8605, Batch Gradient Norm after: 22.36067446040759
Epoch 8606/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.7098108768463135
Epoch: 8606, Batch Gradient Norm: 26.804944029173875
Epoch: 8606, Batch Gradient Norm after: 22.360677224546272
Epoch 8607/10000, Prediction Accuracy = 60.158%, Loss = 0.7066175818443299
Epoch: 8607, Batch Gradient Norm: 28.21230852013164
Epoch: 8607, Batch Gradient Norm after: 22.360674397825107
Epoch 8608/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7097828149795532
Epoch: 8608, Batch Gradient Norm: 26.802460620695232
Epoch: 8608, Batch Gradient Norm after: 22.360678238911113
Epoch 8609/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7065450310707092
Epoch: 8609, Batch Gradient Norm: 28.205191915686733
Epoch: 8609, Batch Gradient Norm after: 22.36067624037813
Epoch 8610/10000, Prediction Accuracy = 60.186%, Loss = 0.7097183465957642
Epoch: 8610, Batch Gradient Norm: 26.80599364205811
Epoch: 8610, Batch Gradient Norm after: 22.360676994327978
Epoch 8611/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.7064856767654419
Epoch: 8611, Batch Gradient Norm: 28.212313689999128
Epoch: 8611, Batch Gradient Norm after: 22.360674979816036
Epoch 8612/10000, Prediction Accuracy = 60.31%, Loss = 0.7098160028457642
Epoch: 8612, Batch Gradient Norm: 26.801417951981986
Epoch: 8612, Batch Gradient Norm after: 22.36067883808679
Epoch 8613/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7065981149673461
Epoch: 8613, Batch Gradient Norm: 28.20641175561638
Epoch: 8613, Batch Gradient Norm after: 22.360678548454356
Epoch 8614/10000, Prediction Accuracy = 60.286%, Loss = 0.7098921418190003
Epoch: 8614, Batch Gradient Norm: 26.788477628852046
Epoch: 8614, Batch Gradient Norm after: 22.36067735628014
Epoch 8615/10000, Prediction Accuracy = 60.266%, Loss = 0.7064935326576233
Epoch: 8615, Batch Gradient Norm: 28.204862010983092
Epoch: 8615, Batch Gradient Norm after: 22.360676830825696
Epoch 8616/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.7096493244171143
Epoch: 8616, Batch Gradient Norm: 26.787029346719645
Epoch: 8616, Batch Gradient Norm after: 22.3606781616217
Epoch 8617/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.7063262939453125
Epoch: 8617, Batch Gradient Norm: 28.204449709765075
Epoch: 8617, Batch Gradient Norm after: 22.360676632072515
Epoch 8618/10000, Prediction Accuracy = 60.218%, Loss = 0.7094855189323426
Epoch: 8618, Batch Gradient Norm: 26.79233482461292
Epoch: 8618, Batch Gradient Norm after: 22.36067738102125
Epoch 8619/10000, Prediction Accuracy = 60.164%, Loss = 0.7063177466392517
Epoch: 8619, Batch Gradient Norm: 28.194382903422706
Epoch: 8619, Batch Gradient Norm after: 22.36067724924289
Epoch 8620/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.7094651699066162
Epoch: 8620, Batch Gradient Norm: 26.78941154594976
Epoch: 8620, Batch Gradient Norm after: 22.360677894427297
Epoch 8621/10000, Prediction Accuracy = 60.19%, Loss = 0.7062362313270569
Epoch: 8621, Batch Gradient Norm: 28.189668715780712
Epoch: 8621, Batch Gradient Norm after: 22.360676463162044
Epoch 8622/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7093980789184571
Epoch: 8622, Batch Gradient Norm: 26.78699009187966
Epoch: 8622, Batch Gradient Norm after: 22.360677981303926
Epoch 8623/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.7061607360839843
Epoch: 8623, Batch Gradient Norm: 28.19775213057655
Epoch: 8623, Batch Gradient Norm after: 22.360675257975487
Epoch 8624/10000, Prediction Accuracy = 60.306%, Loss = 0.7095157861709595
Epoch: 8624, Batch Gradient Norm: 26.78152360311636
Epoch: 8624, Batch Gradient Norm after: 22.360677628928627
Epoch 8625/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.706279706954956
Epoch: 8625, Batch Gradient Norm: 28.193391197836558
Epoch: 8625, Batch Gradient Norm after: 22.36067715167955
Epoch 8626/10000, Prediction Accuracy = 60.286%, Loss = 0.709583580493927
Epoch: 8626, Batch Gradient Norm: 26.766960919861543
Epoch: 8626, Batch Gradient Norm after: 22.36067692378847
Epoch 8627/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7061573386192321
Epoch: 8627, Batch Gradient Norm: 28.19183719726035
Epoch: 8627, Batch Gradient Norm after: 22.36067553569838
Epoch 8628/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7093263745307923
Epoch: 8628, Batch Gradient Norm: 26.76762392245343
Epoch: 8628, Batch Gradient Norm after: 22.360677977100444
Epoch 8629/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7059908032417297
Epoch: 8629, Batch Gradient Norm: 28.191034524280298
Epoch: 8629, Batch Gradient Norm after: 22.360675542216285
Epoch 8630/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7091737627983093
Epoch: 8630, Batch Gradient Norm: 26.77235610184561
Epoch: 8630, Batch Gradient Norm after: 22.36067517382843
Epoch 8631/10000, Prediction Accuracy = 60.194%, Loss = 0.7059857487678528
Epoch: 8631, Batch Gradient Norm: 28.178257953301543
Epoch: 8631, Batch Gradient Norm after: 22.360673323156956
Epoch 8632/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7091470003128052
Epoch: 8632, Batch Gradient Norm: 26.770059068994097
Epoch: 8632, Batch Gradient Norm after: 22.360678432030934
Epoch 8633/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7059000730514526
Epoch: 8633, Batch Gradient Norm: 28.176085237104317
Epoch: 8633, Batch Gradient Norm after: 22.360677378326763
Epoch 8634/10000, Prediction Accuracy = 60.198%, Loss = 0.7090826153755188
Epoch: 8634, Batch Gradient Norm: 26.770984167500853
Epoch: 8634, Batch Gradient Norm after: 22.360676370784763
Epoch 8635/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.7058521986007691
Epoch: 8635, Batch Gradient Norm: 28.18340870498439
Epoch: 8635, Batch Gradient Norm after: 22.36067625142313
Epoch 8636/10000, Prediction Accuracy = 60.31%, Loss = 0.7091998219490051
Epoch: 8636, Batch Gradient Norm: 26.76171835092226
Epoch: 8636, Batch Gradient Norm after: 22.360677127236535
Epoch 8637/10000, Prediction Accuracy = 60.25%, Loss = 0.7059619665145874
Epoch: 8637, Batch Gradient Norm: 28.177663208068378
Epoch: 8637, Batch Gradient Norm after: 22.360676797201148
Epoch 8638/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.7092512369155883
Epoch: 8638, Batch Gradient Norm: 26.750989430650584
Epoch: 8638, Batch Gradient Norm after: 22.360677234825264
Epoch 8639/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.7058339834213256
Epoch: 8639, Batch Gradient Norm: 28.17680180927256
Epoch: 8639, Batch Gradient Norm after: 22.360676688594886
Epoch 8640/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.7089900732040405
Epoch: 8640, Batch Gradient Norm: 26.750823792620597
Epoch: 8640, Batch Gradient Norm after: 22.360676523560127
Epoch 8641/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.7056702613830567
Epoch: 8641, Batch Gradient Norm: 28.177592938655458
Epoch: 8641, Batch Gradient Norm after: 22.360675426967173
Epoch 8642/10000, Prediction Accuracy = 60.214%, Loss = 0.7088545083999633
Epoch: 8642, Batch Gradient Norm: 26.752473097007076
Epoch: 8642, Batch Gradient Norm after: 22.360676758463963
Epoch 8643/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7056690692901612
Epoch: 8643, Batch Gradient Norm: 28.16422729549847
Epoch: 8643, Batch Gradient Norm after: 22.360674723891425
Epoch 8644/10000, Prediction Accuracy = 60.202%, Loss = 0.7088276863098144
Epoch: 8644, Batch Gradient Norm: 26.75051132767623
Epoch: 8644, Batch Gradient Norm after: 22.36067921205551
Epoch 8645/10000, Prediction Accuracy = 60.19%, Loss = 0.7055697441101074
Epoch: 8645, Batch Gradient Norm: 28.163369426887147
Epoch: 8645, Batch Gradient Norm after: 22.36067831199397
Epoch 8646/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7087735772132874
Epoch: 8646, Batch Gradient Norm: 26.75113022066754
Epoch: 8646, Batch Gradient Norm after: 22.360677509926724
Epoch 8647/10000, Prediction Accuracy = 60.258%, Loss = 0.7055373787879944
Epoch: 8647, Batch Gradient Norm: 28.166318449235895
Epoch: 8647, Batch Gradient Norm after: 22.36067653694003
Epoch 8648/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.7089004158973694
Epoch: 8648, Batch Gradient Norm: 26.744225517058066
Epoch: 8648, Batch Gradient Norm after: 22.360677877034497
Epoch 8649/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.7056578040122986
Epoch: 8649, Batch Gradient Norm: 28.159535437373457
Epoch: 8649, Batch Gradient Norm after: 22.360676778034104
Epoch 8650/10000, Prediction Accuracy = 60.286%, Loss = 0.708925461769104
Epoch: 8650, Batch Gradient Norm: 26.732418467027223
Epoch: 8650, Batch Gradient Norm after: 22.3606779146147
Epoch 8651/10000, Prediction Accuracy = 60.27%, Loss = 0.7055088996887207
Epoch: 8651, Batch Gradient Norm: 28.159464973427625
Epoch: 8651, Batch Gradient Norm after: 22.360676450964643
Epoch 8652/10000, Prediction Accuracy = 60.254%, Loss = 0.7086601614952087
Epoch: 8652, Batch Gradient Norm: 26.738273609670347
Epoch: 8652, Batch Gradient Norm after: 22.36067668685708
Epoch 8653/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.7053690075874328
Epoch: 8653, Batch Gradient Norm: 28.156489515665612
Epoch: 8653, Batch Gradient Norm after: 22.36067542317208
Epoch 8654/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.7085246682167053
Epoch: 8654, Batch Gradient Norm: 26.742341165272855
Epoch: 8654, Batch Gradient Norm after: 22.360675899358235
Epoch 8655/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7053697347640991
Epoch: 8655, Batch Gradient Norm: 28.1413143142454
Epoch: 8655, Batch Gradient Norm after: 22.360671639540165
Epoch 8656/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7084984302520752
Epoch: 8656, Batch Gradient Norm: 26.739479648669963
Epoch: 8656, Batch Gradient Norm after: 22.36067874259733
Epoch 8657/10000, Prediction Accuracy = 60.21%, Loss = 0.7052612900733948
Epoch: 8657, Batch Gradient Norm: 28.14328818046366
Epoch: 8657, Batch Gradient Norm after: 22.36067776469873
Epoch 8658/10000, Prediction Accuracy = 60.212%, Loss = 0.708450198173523
Epoch: 8658, Batch Gradient Norm: 26.740436762672687
Epoch: 8658, Batch Gradient Norm after: 22.360677119708956
Epoch 8659/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.705241596698761
Epoch: 8659, Batch Gradient Norm: 28.145544604224526
Epoch: 8659, Batch Gradient Norm after: 22.360675030335283
Epoch 8660/10000, Prediction Accuracy = 60.31%, Loss = 0.7085799098014831
Epoch: 8660, Batch Gradient Norm: 26.730965661453233
Epoch: 8660, Batch Gradient Norm after: 22.360678144656344
Epoch 8661/10000, Prediction Accuracy = 60.274%, Loss = 0.7053475022315979
Epoch: 8661, Batch Gradient Norm: 28.140431542054927
Epoch: 8661, Batch Gradient Norm after: 22.360677497836516
Epoch 8662/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.7085776567459107
Epoch: 8662, Batch Gradient Norm: 26.722786510052114
Epoch: 8662, Batch Gradient Norm after: 22.36067682669747
Epoch 8663/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.7051916360855103
Epoch: 8663, Batch Gradient Norm: 28.138704426745942
Epoch: 8663, Batch Gradient Norm after: 22.36067668481757
Epoch 8664/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.7083212256431579
Epoch: 8664, Batch Gradient Norm: 26.72651190449067
Epoch: 8664, Batch Gradient Norm after: 22.360678814743473
Epoch 8665/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7050601840019226
Epoch: 8665, Batch Gradient Norm: 28.137457790481847
Epoch: 8665, Batch Gradient Norm after: 22.360676923248608
Epoch 8666/10000, Prediction Accuracy = 60.212%, Loss = 0.708201277256012
Epoch: 8666, Batch Gradient Norm: 26.72929570664497
Epoch: 8666, Batch Gradient Norm after: 22.360679430522474
Epoch 8667/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7050602316856385
Epoch: 8667, Batch Gradient Norm: 28.125809436698447
Epoch: 8667, Batch Gradient Norm after: 22.360675170784702
Epoch 8668/10000, Prediction Accuracy = 60.19599999999999%, Loss = 0.7081744551658631
Epoch: 8668, Batch Gradient Norm: 26.726960158498905
Epoch: 8668, Batch Gradient Norm after: 22.360675359522258
Epoch 8669/10000, Prediction Accuracy = 60.202%, Loss = 0.7049555659294129
Epoch: 8669, Batch Gradient Norm: 28.12714579028257
Epoch: 8669, Batch Gradient Norm after: 22.360676019361797
Epoch 8670/10000, Prediction Accuracy = 60.214%, Loss = 0.7081226229667663
Epoch: 8670, Batch Gradient Norm: 26.729956873857386
Epoch: 8670, Batch Gradient Norm after: 22.360679129190775
Epoch 8671/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.7049512386322021
Epoch: 8671, Batch Gradient Norm: 28.131621465042993
Epoch: 8671, Batch Gradient Norm after: 22.360675290950024
Epoch 8672/10000, Prediction Accuracy = 60.282%, Loss = 0.7082727670669555
Epoch: 8672, Batch Gradient Norm: 26.715595105453186
Epoch: 8672, Batch Gradient Norm after: 22.36068009555309
Epoch 8673/10000, Prediction Accuracy = 60.274%, Loss = 0.7050375461578369
Epoch: 8673, Batch Gradient Norm: 28.126149699007378
Epoch: 8673, Batch Gradient Norm after: 22.360675351087572
Epoch 8674/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.708238160610199
Epoch: 8674, Batch Gradient Norm: 26.70551476662921
Epoch: 8674, Batch Gradient Norm after: 22.360676989616035
Epoch 8675/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7048598051071167
Epoch: 8675, Batch Gradient Norm: 28.12519808120555
Epoch: 8675, Batch Gradient Norm after: 22.36067340850126
Epoch 8676/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7079862236976624
Epoch: 8676, Batch Gradient Norm: 26.709538410054503
Epoch: 8676, Batch Gradient Norm after: 22.360676597739047
Epoch 8677/10000, Prediction Accuracy = 60.242%, Loss = 0.7047344088554383
Epoch: 8677, Batch Gradient Norm: 28.125284216462
Epoch: 8677, Batch Gradient Norm after: 22.360675405586008
Epoch 8678/10000, Prediction Accuracy = 60.218%, Loss = 0.7078896641731263
Epoch: 8678, Batch Gradient Norm: 26.70594552008462
Epoch: 8678, Batch Gradient Norm after: 22.360675978534225
Epoch 8679/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.7047353029251099
Epoch: 8679, Batch Gradient Norm: 28.113557821882306
Epoch: 8679, Batch Gradient Norm after: 22.360676340866007
Epoch 8680/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.7078652620315552
Epoch: 8680, Batch Gradient Norm: 26.70431290158642
Epoch: 8680, Batch Gradient Norm after: 22.36067817239515
Epoch 8681/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.7046044588088989
Epoch: 8681, Batch Gradient Norm: 28.115994370547753
Epoch: 8681, Batch Gradient Norm after: 22.360675272891193
Epoch 8682/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.7078305363655091
Epoch: 8682, Batch Gradient Norm: 26.7041936272998
Epoch: 8682, Batch Gradient Norm after: 22.360677594698885
Epoch 8683/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.7046364784240723
Epoch: 8683, Batch Gradient Norm: 28.121879924623304
Epoch: 8683, Batch Gradient Norm after: 22.360672817793514
Epoch 8684/10000, Prediction Accuracy = 60.29%, Loss = 0.7080039024353028
Epoch: 8684, Batch Gradient Norm: 26.687973353471286
Epoch: 8684, Batch Gradient Norm after: 22.360678391905537
Epoch 8685/10000, Prediction Accuracy = 60.278%, Loss = 0.7047026753425598
Epoch: 8685, Batch Gradient Norm: 28.115913911227672
Epoch: 8685, Batch Gradient Norm after: 22.360674768477022
Epoch 8686/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.7079171895980835
Epoch: 8686, Batch Gradient Norm: 26.680633063974426
Epoch: 8686, Batch Gradient Norm after: 22.360679828283676
Epoch 8687/10000, Prediction Accuracy = 60.266%, Loss = 0.7045053839683533
Epoch: 8687, Batch Gradient Norm: 28.117281669543566
Epoch: 8687, Batch Gradient Norm after: 22.36067450185444
Epoch 8688/10000, Prediction Accuracy = 60.214%, Loss = 0.7076616287231445
Epoch: 8688, Batch Gradient Norm: 26.68102475141043
Epoch: 8688, Batch Gradient Norm after: 22.360678564239063
Epoch 8689/10000, Prediction Accuracy = 60.238%, Loss = 0.7043981313705444
Epoch: 8689, Batch Gradient Norm: 28.112697218055427
Epoch: 8689, Batch Gradient Norm after: 22.360673572350613
Epoch 8690/10000, Prediction Accuracy = 60.19%, Loss = 0.707589328289032
Epoch: 8690, Batch Gradient Norm: 26.683513049333126
Epoch: 8690, Batch Gradient Norm after: 22.360680117790533
Epoch 8691/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.7044019341468811
Epoch: 8691, Batch Gradient Norm: 28.100595652907337
Epoch: 8691, Batch Gradient Norm after: 22.36067608224902
Epoch 8692/10000, Prediction Accuracy = 60.212%, Loss = 0.7075503945350647
Epoch: 8692, Batch Gradient Norm: 26.681049078817296
Epoch: 8692, Batch Gradient Norm after: 22.36067929064435
Epoch 8693/10000, Prediction Accuracy = 60.23%, Loss = 0.7042781233787536
Epoch: 8693, Batch Gradient Norm: 28.10491443938364
Epoch: 8693, Batch Gradient Norm after: 22.360678590298416
Epoch 8694/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.7075382828712463
Epoch: 8694, Batch Gradient Norm: 26.677588248483694
Epoch: 8694, Batch Gradient Norm after: 22.360676879969482
Epoch 8695/10000, Prediction Accuracy = 60.238%, Loss = 0.704317855834961
Epoch: 8695, Batch Gradient Norm: 28.107269185060737
Epoch: 8695, Batch Gradient Norm after: 22.36067714837921
Epoch 8696/10000, Prediction Accuracy = 60.306%, Loss = 0.7077167630195618
Epoch: 8696, Batch Gradient Norm: 26.66330360923917
Epoch: 8696, Batch Gradient Norm after: 22.36067802997332
Epoch 8697/10000, Prediction Accuracy = 60.3%, Loss = 0.7043733596801758
Epoch: 8697, Batch Gradient Norm: 28.10044416638512
Epoch: 8697, Batch Gradient Norm after: 22.36067451741388
Epoch 8698/10000, Prediction Accuracy = 60.324%, Loss = 0.7075926899909973
Epoch: 8698, Batch Gradient Norm: 26.654792545898776
Epoch: 8698, Batch Gradient Norm after: 22.360676373604527
Epoch 8699/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.7041614770889282
Epoch: 8699, Batch Gradient Norm: 28.105865429655392
Epoch: 8699, Batch Gradient Norm after: 22.360675543501575
Epoch 8700/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.7073453426361084
Epoch: 8700, Batch Gradient Norm: 26.656431753154266
Epoch: 8700, Batch Gradient Norm after: 22.360678073434293
Epoch 8701/10000, Prediction Accuracy = 60.226%, Loss = 0.7040756225585938
Epoch: 8701, Batch Gradient Norm: 28.098385917671862
Epoch: 8701, Batch Gradient Norm after: 22.360674936447484
Epoch 8702/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.7072944879531861
Epoch: 8702, Batch Gradient Norm: 26.655886870726405
Epoch: 8702, Batch Gradient Norm after: 22.360677230693543
Epoch 8703/10000, Prediction Accuracy = 60.198%, Loss = 0.7040518999099732
Epoch: 8703, Batch Gradient Norm: 28.090318007701544
Epoch: 8703, Batch Gradient Norm after: 22.36067620161382
Epoch 8704/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.707244336605072
Epoch: 8704, Batch Gradient Norm: 26.6537876962396
Epoch: 8704, Batch Gradient Norm after: 22.36067892720597
Epoch 8705/10000, Prediction Accuracy = 60.222%, Loss = 0.7039270997047424
Epoch: 8705, Batch Gradient Norm: 28.094817974371804
Epoch: 8705, Batch Gradient Norm after: 22.360675986105562
Epoch 8706/10000, Prediction Accuracy = 60.266000000000005%, Loss = 0.7072519779205322
Epoch: 8706, Batch Gradient Norm: 26.646773367179918
Epoch: 8706, Batch Gradient Norm after: 22.3606761627756
Epoch 8707/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.7039957046508789
Epoch: 8707, Batch Gradient Norm: 28.095472746680947
Epoch: 8707, Batch Gradient Norm after: 22.360675989452062
Epoch 8708/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.7074232459068298
Epoch: 8708, Batch Gradient Norm: 26.637093647459405
Epoch: 8708, Batch Gradient Norm after: 22.3606786711234
Epoch 8709/10000, Prediction Accuracy = 60.288%, Loss = 0.7040160059928894
Epoch: 8709, Batch Gradient Norm: 28.092033376548336
Epoch: 8709, Batch Gradient Norm after: 22.360674851265987
Epoch 8710/10000, Prediction Accuracy = 60.306%, Loss = 0.7072643280029297
Epoch: 8710, Batch Gradient Norm: 26.6290708233644
Epoch: 8710, Batch Gradient Norm after: 22.36067857733242
Epoch 8711/10000, Prediction Accuracy = 60.266%, Loss = 0.7038139820098877
Epoch: 8711, Batch Gradient Norm: 28.094382455139417
Epoch: 8711, Batch Gradient Norm after: 22.360675528079412
Epoch 8712/10000, Prediction Accuracy = 60.23%, Loss = 0.7070308804512024
Epoch: 8712, Batch Gradient Norm: 26.631277078893667
Epoch: 8712, Batch Gradient Norm after: 22.360678099314075
Epoch 8713/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.7037506103515625
Epoch: 8713, Batch Gradient Norm: 28.086993959850098
Epoch: 8713, Batch Gradient Norm after: 22.360675452067245
Epoch 8714/10000, Prediction Accuracy = 60.186%, Loss = 0.7069990754127502
Epoch: 8714, Batch Gradient Norm: 26.631824527427234
Epoch: 8714, Batch Gradient Norm after: 22.360679305120403
Epoch 8715/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7037145018577575
Epoch: 8715, Batch Gradient Norm: 28.07840848787738
Epoch: 8715, Batch Gradient Norm after: 22.36067497857346
Epoch 8716/10000, Prediction Accuracy = 60.21%, Loss = 0.7069436073303222
Epoch: 8716, Batch Gradient Norm: 26.62939283118641
Epoch: 8716, Batch Gradient Norm after: 22.36067896074773
Epoch 8717/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7036001205444335
Epoch: 8717, Batch Gradient Norm: 28.080759740323373
Epoch: 8717, Batch Gradient Norm after: 22.36067701976685
Epoch 8718/10000, Prediction Accuracy = 60.29%, Loss = 0.7069713711738587
Epoch: 8718, Batch Gradient Norm: 26.62742696080206
Epoch: 8718, Batch Gradient Norm after: 22.360676409571912
Epoch 8719/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7036947250366211
Epoch: 8719, Batch Gradient Norm: 28.07874929524109
Epoch: 8719, Batch Gradient Norm after: 22.360675158997616
Epoch 8720/10000, Prediction Accuracy = 60.314%, Loss = 0.7071305274963379
Epoch: 8720, Batch Gradient Norm: 26.61720609701261
Epoch: 8720, Batch Gradient Norm after: 22.360677500492557
Epoch 8721/10000, Prediction Accuracy = 60.278%, Loss = 0.7036780714988708
Epoch: 8721, Batch Gradient Norm: 28.074122210783003
Epoch: 8721, Batch Gradient Norm after: 22.360676429456657
Epoch 8722/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.7069283723831177
Epoch: 8722, Batch Gradient Norm: 26.614087241115342
Epoch: 8722, Batch Gradient Norm after: 22.36067764006808
Epoch 8723/10000, Prediction Accuracy = 60.238%, Loss = 0.7034861087799072
Epoch: 8723, Batch Gradient Norm: 28.078507459854293
Epoch: 8723, Batch Gradient Norm after: 22.360677414328926
Epoch 8724/10000, Prediction Accuracy = 60.246%, Loss = 0.7067109107971191
Epoch: 8724, Batch Gradient Norm: 26.615053435340624
Epoch: 8724, Batch Gradient Norm after: 22.360677290636627
Epoch 8725/10000, Prediction Accuracy = 60.21%, Loss = 0.7034371376037598
Epoch: 8725, Batch Gradient Norm: 28.06971189491208
Epoch: 8725, Batch Gradient Norm after: 22.360674756138206
Epoch 8726/10000, Prediction Accuracy = 60.198%, Loss = 0.7066855311393738
Epoch: 8726, Batch Gradient Norm: 26.612055411135323
Epoch: 8726, Batch Gradient Norm after: 22.360679056902935
Epoch 8727/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.703387439250946
Epoch: 8727, Batch Gradient Norm: 28.060769611589475
Epoch: 8727, Batch Gradient Norm after: 22.360675363485825
Epoch 8728/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.7066294312477112
Epoch: 8728, Batch Gradient Norm: 26.612072048752644
Epoch: 8728, Batch Gradient Norm after: 22.360678293047105
Epoch 8729/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7032844424247742
Epoch: 8729, Batch Gradient Norm: 28.06747291640056
Epoch: 8729, Batch Gradient Norm after: 22.3606738476576
Epoch 8730/10000, Prediction Accuracy = 60.302%, Loss = 0.7066808223724366
Epoch: 8730, Batch Gradient Norm: 26.609538972678028
Epoch: 8730, Batch Gradient Norm after: 22.36067776818602
Epoch 8731/10000, Prediction Accuracy = 60.282%, Loss = 0.7033892035484314
Epoch: 8731, Batch Gradient Norm: 28.06371859239238
Epoch: 8731, Batch Gradient Norm after: 22.36067559708077
Epoch 8732/10000, Prediction Accuracy = 60.306%, Loss = 0.7068053364753724
Epoch: 8732, Batch Gradient Norm: 26.598936904590953
Epoch: 8732, Batch Gradient Norm after: 22.36067768308962
Epoch 8733/10000, Prediction Accuracy = 60.27%, Loss = 0.7033427596092224
Epoch: 8733, Batch Gradient Norm: 28.060108171817806
Epoch: 8733, Batch Gradient Norm after: 22.36067833007641
Epoch 8734/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.7065903425216675
Epoch: 8734, Batch Gradient Norm: 26.59407996950243
Epoch: 8734, Batch Gradient Norm after: 22.360677347212643
Epoch 8735/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7031619906425476
Epoch: 8735, Batch Gradient Norm: 28.05995045510463
Epoch: 8735, Batch Gradient Norm after: 22.36067573083053
Epoch 8736/10000, Prediction Accuracy = 60.25200000000001%, Loss = 0.7063997507095336
Epoch: 8736, Batch Gradient Norm: 26.599332677524668
Epoch: 8736, Batch Gradient Norm after: 22.360678010808943
Epoch 8737/10000, Prediction Accuracy = 60.226%, Loss = 0.7031323075294494
Epoch: 8737, Batch Gradient Norm: 28.05090822352006
Epoch: 8737, Batch Gradient Norm after: 22.360677178924135
Epoch 8738/10000, Prediction Accuracy = 60.194%, Loss = 0.7063695907592773
Epoch: 8738, Batch Gradient Norm: 26.596324570416602
Epoch: 8738, Batch Gradient Norm after: 22.360679404496263
Epoch 8739/10000, Prediction Accuracy = 60.23%, Loss = 0.7030739426612854
Epoch: 8739, Batch Gradient Norm: 28.0423369920629
Epoch: 8739, Batch Gradient Norm after: 22.36067529756905
Epoch 8740/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.7063136458396911
Epoch: 8740, Batch Gradient Norm: 26.597036212374658
Epoch: 8740, Batch Gradient Norm after: 22.360678779348188
Epoch 8741/10000, Prediction Accuracy = 60.25%, Loss = 0.7029839754104614
Epoch: 8741, Batch Gradient Norm: 28.04887830731569
Epoch: 8741, Batch Gradient Norm after: 22.36067658466119
Epoch 8742/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.7063929438591003
Epoch: 8742, Batch Gradient Norm: 26.596930457751274
Epoch: 8742, Batch Gradient Norm after: 22.360676143534956
Epoch 8743/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.7031021833419799
Epoch: 8743, Batch Gradient Norm: 28.041756651723368
Epoch: 8743, Batch Gradient Norm after: 22.360677862806
Epoch 8744/10000, Prediction Accuracy = 60.314%, Loss = 0.7064824819564819
Epoch: 8744, Batch Gradient Norm: 26.590491052790586
Epoch: 8744, Batch Gradient Norm after: 22.360675913618394
Epoch 8745/10000, Prediction Accuracy = 60.266%, Loss = 0.7030317783355713
Epoch: 8745, Batch Gradient Norm: 28.038214826152256
Epoch: 8745, Batch Gradient Norm after: 22.360675879065564
Epoch 8746/10000, Prediction Accuracy = 60.29%, Loss = 0.7062345981597901
Epoch: 8746, Batch Gradient Norm: 26.59191843275984
Epoch: 8746, Batch Gradient Norm after: 22.360676592565497
Epoch 8747/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7028629183769226
Epoch: 8747, Batch Gradient Norm: 28.035619816497274
Epoch: 8747, Batch Gradient Norm after: 22.360677732377162
Epoch 8748/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7060642004013061
Epoch: 8748, Batch Gradient Norm: 26.59958351409954
Epoch: 8748, Batch Gradient Norm after: 22.36067745217974
Epoch 8749/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7028517007827759
Epoch: 8749, Batch Gradient Norm: 28.024465407594555
Epoch: 8749, Batch Gradient Norm after: 22.36067520630507
Epoch 8750/10000, Prediction Accuracy = 60.21%, Loss = 0.7060325264930725
Epoch: 8750, Batch Gradient Norm: 26.594705997442286
Epoch: 8750, Batch Gradient Norm after: 22.360678001360565
Epoch 8751/10000, Prediction Accuracy = 60.242%, Loss = 0.7027838826179504
Epoch: 8751, Batch Gradient Norm: 28.019393051655058
Epoch: 8751, Batch Gradient Norm after: 22.360677367631137
Epoch 8752/10000, Prediction Accuracy = 60.188%, Loss = 0.7059762954711915
Epoch: 8752, Batch Gradient Norm: 26.598036183033813
Epoch: 8752, Batch Gradient Norm after: 22.36067863292186
Epoch 8753/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7027111172676086
Epoch: 8753, Batch Gradient Norm: 28.02335195077734
Epoch: 8753, Batch Gradient Norm after: 22.360676022788972
Epoch 8754/10000, Prediction Accuracy = 60.302%, Loss = 0.7060841917991638
Epoch: 8754, Batch Gradient Norm: 26.59588344092764
Epoch: 8754, Batch Gradient Norm after: 22.360678079435583
Epoch 8755/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7028298854827881
Epoch: 8755, Batch Gradient Norm: 28.019347193117312
Epoch: 8755, Batch Gradient Norm after: 22.36067797259081
Epoch 8756/10000, Prediction Accuracy = 60.322%, Loss = 0.7061498403549195
Epoch: 8756, Batch Gradient Norm: 26.582947067478496
Epoch: 8756, Batch Gradient Norm after: 22.360675830584768
Epoch 8757/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.7027117967605591
Epoch: 8757, Batch Gradient Norm: 28.022203120511755
Epoch: 8757, Batch Gradient Norm after: 22.36067550755044
Epoch 8758/10000, Prediction Accuracy = 60.302%, Loss = 0.7058932542800903
Epoch: 8758, Batch Gradient Norm: 26.577560636340763
Epoch: 8758, Batch Gradient Norm after: 22.360677233833588
Epoch 8759/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7025506615638732
Epoch: 8759, Batch Gradient Norm: 28.022630865616257
Epoch: 8759, Batch Gradient Norm after: 22.360676555150214
Epoch 8760/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.7057520747184753
Epoch: 8760, Batch Gradient Norm: 26.579311992730652
Epoch: 8760, Batch Gradient Norm after: 22.36067772936782
Epoch 8761/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.7025305986404419
Epoch: 8761, Batch Gradient Norm: 28.008543923583282
Epoch: 8761, Batch Gradient Norm after: 22.36067522829319
Epoch 8762/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.7057372808456421
Epoch: 8762, Batch Gradient Norm: 26.575260458271426
Epoch: 8762, Batch Gradient Norm after: 22.360678411808074
Epoch 8763/10000, Prediction Accuracy = 60.25%, Loss = 0.7024395704269409
Epoch: 8763, Batch Gradient Norm: 28.009160461096872
Epoch: 8763, Batch Gradient Norm after: 22.3606787081485
Epoch 8764/10000, Prediction Accuracy = 60.19%, Loss = 0.7056725025177002
Epoch: 8764, Batch Gradient Norm: 26.57072734005024
Epoch: 8764, Batch Gradient Norm after: 22.36067734773411
Epoch 8765/10000, Prediction Accuracy = 60.254%, Loss = 0.7023881554603577
Epoch: 8765, Batch Gradient Norm: 28.0157239185001
Epoch: 8765, Batch Gradient Norm after: 22.360676590727344
Epoch 8766/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.7058088898658752
Epoch: 8766, Batch Gradient Norm: 26.565215870318973
Epoch: 8766, Batch Gradient Norm after: 22.360676276824584
Epoch 8767/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.7024964928627014
Epoch: 8767, Batch Gradient Norm: 28.0064178464247
Epoch: 8767, Batch Gradient Norm after: 22.36067971992273
Epoch 8768/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.7058553576469422
Epoch: 8768, Batch Gradient Norm: 26.55122265840249
Epoch: 8768, Batch Gradient Norm after: 22.360676415150767
Epoch 8769/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.7023664474487304
Epoch: 8769, Batch Gradient Norm: 28.010310699697524
Epoch: 8769, Batch Gradient Norm after: 22.360673538584255
Epoch 8770/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.7055894255638122
Epoch: 8770, Batch Gradient Norm: 26.557776283300033
Epoch: 8770, Batch Gradient Norm after: 22.360677506062473
Epoch 8771/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.7022163152694703
Epoch: 8771, Batch Gradient Norm: 28.00585121560139
Epoch: 8771, Batch Gradient Norm after: 22.36067582300807
Epoch 8772/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7054526329040527
Epoch: 8772, Batch Gradient Norm: 26.557152543057615
Epoch: 8772, Batch Gradient Norm after: 22.360678686928665
Epoch 8773/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7022136330604554
Epoch: 8773, Batch Gradient Norm: 27.99343084357286
Epoch: 8773, Batch Gradient Norm after: 22.36067799262325
Epoch 8774/10000, Prediction Accuracy = 60.226%, Loss = 0.7054257869720459
Epoch: 8774, Batch Gradient Norm: 26.557591721911017
Epoch: 8774, Batch Gradient Norm after: 22.36067680891486
Epoch 8775/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7021099090576172
Epoch: 8775, Batch Gradient Norm: 27.99259002492546
Epoch: 8775, Batch Gradient Norm after: 22.360675918142594
Epoch 8776/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.7053687572479248
Epoch: 8776, Batch Gradient Norm: 26.557296361397164
Epoch: 8776, Batch Gradient Norm after: 22.360677794207945
Epoch 8777/10000, Prediction Accuracy = 60.254%, Loss = 0.7020860433578491
Epoch: 8777, Batch Gradient Norm: 27.99715046854013
Epoch: 8777, Batch Gradient Norm after: 22.360677601358155
Epoch 8778/10000, Prediction Accuracy = 60.288%, Loss = 0.7055010557174682
Epoch: 8778, Batch Gradient Norm: 26.550404363053804
Epoch: 8778, Batch Gradient Norm after: 22.360677182889933
Epoch 8779/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7021889090538025
Epoch: 8779, Batch Gradient Norm: 27.990225040323963
Epoch: 8779, Batch Gradient Norm after: 22.360676906972685
Epoch 8780/10000, Prediction Accuracy = 60.302%, Loss = 0.7055207014083862
Epoch: 8780, Batch Gradient Norm: 26.53963803710026
Epoch: 8780, Batch Gradient Norm after: 22.360675666026562
Epoch 8781/10000, Prediction Accuracy = 60.258%, Loss = 0.7020420551300048
Epoch: 8781, Batch Gradient Norm: 27.989089703927323
Epoch: 8781, Batch Gradient Norm after: 22.360674913601514
Epoch 8782/10000, Prediction Accuracy = 60.288%, Loss = 0.7052611112594604
Epoch: 8782, Batch Gradient Norm: 26.546496199499696
Epoch: 8782, Batch Gradient Norm after: 22.36067816196302
Epoch 8783/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7019175529479981
Epoch: 8783, Batch Gradient Norm: 27.98378439971041
Epoch: 8783, Batch Gradient Norm after: 22.360676801816005
Epoch 8784/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7051261305809021
Epoch: 8784, Batch Gradient Norm: 26.548761507570237
Epoch: 8784, Batch Gradient Norm after: 22.36067719571096
Epoch 8785/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7019234538078308
Epoch: 8785, Batch Gradient Norm: 27.972573590736836
Epoch: 8785, Batch Gradient Norm after: 22.360674493492677
Epoch 8786/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7051000118255615
Epoch: 8786, Batch Gradient Norm: 26.54948009039626
Epoch: 8786, Batch Gradient Norm after: 22.36067457316618
Epoch 8787/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7018285632133484
Epoch: 8787, Batch Gradient Norm: 27.971369130081037
Epoch: 8787, Batch Gradient Norm after: 22.360673842594267
Epoch 8788/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7050419807434082
Epoch: 8788, Batch Gradient Norm: 26.550016747712224
Epoch: 8788, Batch Gradient Norm after: 22.360674933649967
Epoch 8789/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.7018002271652222
Epoch: 8789, Batch Gradient Norm: 27.97433403243587
Epoch: 8789, Batch Gradient Norm after: 22.360673950918134
Epoch 8790/10000, Prediction Accuracy = 60.29%, Loss = 0.7051778435707092
Epoch: 8790, Batch Gradient Norm: 26.541509190492917
Epoch: 8790, Batch Gradient Norm after: 22.360679577329453
Epoch 8791/10000, Prediction Accuracy = 60.294%, Loss = 0.7019210815429687
Epoch: 8791, Batch Gradient Norm: 27.969564037665506
Epoch: 8791, Batch Gradient Norm after: 22.36067925990232
Epoch 8792/10000, Prediction Accuracy = 60.306%, Loss = 0.7051956415176391
Epoch: 8792, Batch Gradient Norm: 26.534672950594604
Epoch: 8792, Batch Gradient Norm after: 22.36067604138578
Epoch 8793/10000, Prediction Accuracy = 60.258%, Loss = 0.7017572164535523
Epoch: 8793, Batch Gradient Norm: 27.96756516113782
Epoch: 8793, Batch Gradient Norm after: 22.36067568753528
Epoch 8794/10000, Prediction Accuracy = 60.274%, Loss = 0.7049139618873597
Epoch: 8794, Batch Gradient Norm: 26.53804182756848
Epoch: 8794, Batch Gradient Norm after: 22.360675730506856
Epoch 8795/10000, Prediction Accuracy = 60.23%, Loss = 0.7016291737556457
Epoch: 8795, Batch Gradient Norm: 27.962480892003125
Epoch: 8795, Batch Gradient Norm after: 22.36067721676066
Epoch 8796/10000, Prediction Accuracy = 60.246%, Loss = 0.7048032522201538
Epoch: 8796, Batch Gradient Norm: 26.54370751257607
Epoch: 8796, Batch Gradient Norm after: 22.36067795377987
Epoch 8797/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.7016435503959656
Epoch: 8797, Batch Gradient Norm: 27.948726031362533
Epoch: 8797, Batch Gradient Norm after: 22.360676649883207
Epoch 8798/10000, Prediction Accuracy = 60.246%, Loss = 0.7047774434089661
Epoch: 8798, Batch Gradient Norm: 26.54668993954032
Epoch: 8798, Batch Gradient Norm after: 22.360676504706312
Epoch 8799/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.7015385508537293
Epoch: 8799, Batch Gradient Norm: 27.944907423314255
Epoch: 8799, Batch Gradient Norm after: 22.36067555778971
Epoch 8800/10000, Prediction Accuracy = 60.246%, Loss = 0.7047025561332703
Epoch: 8800, Batch Gradient Norm: 26.55288946864362
Epoch: 8800, Batch Gradient Norm after: 22.360677873476185
Epoch 8801/10000, Prediction Accuracy = 60.254%, Loss = 0.7015433669090271
Epoch: 8801, Batch Gradient Norm: 27.948016053769436
Epoch: 8801, Batch Gradient Norm after: 22.360677041494895
Epoch 8802/10000, Prediction Accuracy = 60.29%, Loss = 0.7048458456993103
Epoch: 8802, Batch Gradient Norm: 26.547180841703323
Epoch: 8802, Batch Gradient Norm after: 22.36068037238519
Epoch 8803/10000, Prediction Accuracy = 60.288%, Loss = 0.7016555666923523
Epoch: 8803, Batch Gradient Norm: 27.94098029990355
Epoch: 8803, Batch Gradient Norm after: 22.360676749948894
Epoch 8804/10000, Prediction Accuracy = 60.294%, Loss = 0.7048348188400269
Epoch: 8804, Batch Gradient Norm: 26.538496480257756
Epoch: 8804, Batch Gradient Norm after: 22.360674869465793
Epoch 8805/10000, Prediction Accuracy = 60.274%, Loss = 0.7015019059181213
Epoch: 8805, Batch Gradient Norm: 27.94106810183911
Epoch: 8805, Batch Gradient Norm after: 22.36067673503233
Epoch 8806/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7045692086219788
Epoch: 8806, Batch Gradient Norm: 26.548087304982957
Epoch: 8806, Batch Gradient Norm after: 22.360677500641156
Epoch 8807/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.7013681054115295
Epoch: 8807, Batch Gradient Norm: 27.935573325581593
Epoch 8808/10000, Prediction Accuracy = 60.254%, Loss = 0.7044521450996399
Epoch: 8808, Batch Gradient Norm: 26.552519890452068
Epoch: 8808, Batch Gradient Norm after: 22.360677331510235
Epoch 8809/10000, Prediction Accuracy = 60.234%, Loss = 0.7013795375823975
Epoch: 8809, Batch Gradient Norm: 27.92486219559295
Epoch: 8809, Batch Gradient Norm after: 22.360676784110733
Epoch 8810/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.7044269919395447
Epoch: 8810, Batch Gradient Norm: 26.55019504394532
Epoch: 8810, Batch Gradient Norm after: 22.36067698348861
Epoch 8811/10000, Prediction Accuracy = 60.234%, Loss = 0.7012787818908691
Epoch: 8811, Batch Gradient Norm: 27.923134342863968
Epoch: 8811, Batch Gradient Norm after: 22.360675316258998
Epoch 8812/10000, Prediction Accuracy = 60.242%, Loss = 0.7043755769729614
Epoch: 8812, Batch Gradient Norm: 26.550313536470547
Epoch: 8812, Batch Gradient Norm after: 22.36067612388555
Epoch 8813/10000, Prediction Accuracy = 60.262%, Loss = 0.7012534022331238
Epoch: 8813, Batch Gradient Norm: 27.927315744037184
Epoch: 8813, Batch Gradient Norm after: 22.36067548277144
Epoch 8814/10000, Prediction Accuracy = 60.298%, Loss = 0.7045229315757752
Epoch: 8814, Batch Gradient Norm: 26.539360650826037
Epoch: 8814, Batch Gradient Norm after: 22.360678411287836
Epoch 8815/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.7013718843460083
Epoch: 8815, Batch Gradient Norm: 27.920962264822816
Epoch: 8815, Batch Gradient Norm after: 22.360677252016384
Epoch 8816/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.704519784450531
Epoch: 8816, Batch Gradient Norm: 26.531780956819237
Epoch: 8816, Batch Gradient Norm after: 22.36067730250471
Epoch 8817/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.7012053728103638
Epoch: 8817, Batch Gradient Norm: 27.91888937161285
Epoch: 8817, Batch Gradient Norm after: 22.360674863669324
Epoch 8818/10000, Prediction Accuracy = 60.266%, Loss = 0.704246723651886
Epoch: 8818, Batch Gradient Norm: 26.534577018434938
Epoch: 8818, Batch Gradient Norm after: 22.36067655016679
Epoch 8819/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.7010692596435547
Epoch: 8819, Batch Gradient Norm: 27.918963235966974
Epoch: 8819, Batch Gradient Norm after: 22.3606726097887
Epoch 8820/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7041389584541321
Epoch: 8820, Batch Gradient Norm: 26.538520128572443
Epoch: 8820, Batch Gradient Norm after: 22.360677670045558
Epoch 8821/10000, Prediction Accuracy = 60.254%, Loss = 0.7010691642761231
Epoch: 8821, Batch Gradient Norm: 27.905311640175128
Epoch: 8821, Batch Gradient Norm after: 22.360675155263127
Epoch 8822/10000, Prediction Accuracy = 60.254%, Loss = 0.7041156768798829
Epoch: 8822, Batch Gradient Norm: 26.534502003933152
Epoch: 8822, Batch Gradient Norm after: 22.360674740167894
Epoch 8823/10000, Prediction Accuracy = 60.238%, Loss = 0.7009689569473266
Epoch: 8823, Batch Gradient Norm: 27.904847970591497
Epoch: 8823, Batch Gradient Norm after: 22.360676728443337
Epoch 8824/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.7040600299835205
Epoch: 8824, Batch Gradient Norm: 26.538934373547736
Epoch: 8824, Batch Gradient Norm after: 22.36067554562046
Epoch 8825/10000, Prediction Accuracy = 60.27%, Loss = 0.7009564757347106
Epoch: 8825, Batch Gradient Norm: 27.909855806519847
Epoch: 8825, Batch Gradient Norm after: 22.360677432024897
Epoch 8826/10000, Prediction Accuracy = 60.306%, Loss = 0.7042117476463318
Epoch: 8826, Batch Gradient Norm: 26.530526807513876
Epoch: 8826, Batch Gradient Norm after: 22.36067743731888
Epoch 8827/10000, Prediction Accuracy = 60.29%, Loss = 0.701067852973938
Epoch: 8827, Batch Gradient Norm: 27.90236403108698
Epoch: 8827, Batch Gradient Norm after: 22.360674373297616
Epoch 8828/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.7041841387748718
Epoch: 8828, Batch Gradient Norm: 26.523332639702605
Epoch: 8828, Batch Gradient Norm after: 22.360676749687006
Epoch 8829/10000, Prediction Accuracy = 60.286%, Loss = 0.7008998394012451
Epoch: 8829, Batch Gradient Norm: 27.89862325271584
Epoch: 8829, Batch Gradient Norm after: 22.360673676486147
Epoch 8830/10000, Prediction Accuracy = 60.278%, Loss = 0.7039191126823425
Epoch: 8830, Batch Gradient Norm: 26.524660245452516
Epoch: 8830, Batch Gradient Norm after: 22.360678353122722
Epoch 8831/10000, Prediction Accuracy = 60.246%, Loss = 0.7007753729820252
Epoch: 8831, Batch Gradient Norm: 27.899079534436925
Epoch: 8831, Batch Gradient Norm after: 22.360674629187457
Epoch 8832/10000, Prediction Accuracy = 60.262%, Loss = 0.7038153409957886
Epoch: 8832, Batch Gradient Norm: 26.529226864212863
Epoch: 8832, Batch Gradient Norm after: 22.360675980457877
Epoch 8833/10000, Prediction Accuracy = 60.25%, Loss = 0.7007752776145935
Epoch: 8833, Batch Gradient Norm: 27.88699755097185
Epoch: 8833, Batch Gradient Norm after: 22.360674354460116
Epoch 8834/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7037889122962951
Epoch: 8834, Batch Gradient Norm: 26.529521474134057
Epoch: 8834, Batch Gradient Norm after: 22.360676503190064
Epoch 8835/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7006759881973267
Epoch: 8835, Batch Gradient Norm: 27.883188275874943
Epoch: 8835, Batch Gradient Norm after: 22.360673559002432
Epoch 8836/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7037366151809692
Epoch: 8836, Batch Gradient Norm: 26.529106380365032
Epoch: 8836, Batch Gradient Norm after: 22.36067489580608
Epoch 8837/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7006684899330139
Epoch: 8837, Batch Gradient Norm: 27.890425321713884
Epoch: 8837, Batch Gradient Norm after: 22.36067693339462
Epoch 8838/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.7038957834243774
Epoch: 8838, Batch Gradient Norm: 26.522739275469426
Epoch: 8838, Batch Gradient Norm after: 22.360676838737085
Epoch 8839/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.7007870435714721
Epoch: 8839, Batch Gradient Norm: 27.88069449321036
Epoch: 8839, Batch Gradient Norm after: 22.360676957040262
Epoch 8840/10000, Prediction Accuracy = 60.326%, Loss = 0.7038605690002442
Epoch: 8840, Batch Gradient Norm: 26.514936501850997
Epoch: 8840, Batch Gradient Norm after: 22.36067773408105
Epoch 8841/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7006043195724487
Epoch: 8841, Batch Gradient Norm: 27.879081781405887
Epoch: 8841, Batch Gradient Norm after: 22.360675321641875
Epoch 8842/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.7035922884941102
Epoch: 8842, Batch Gradient Norm: 26.519302083848725
Epoch: 8842, Batch Gradient Norm after: 22.36067496137829
Epoch 8843/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.7004818320274353
Epoch: 8843, Batch Gradient Norm: 27.877212165196052
Epoch: 8843, Batch Gradient Norm after: 22.36067583306557
Epoch 8844/10000, Prediction Accuracy = 60.262%, Loss = 0.7034979104995728
Epoch: 8844, Batch Gradient Norm: 26.519186207363497
Epoch: 8844, Batch Gradient Norm after: 22.360677045199672
Epoch 8845/10000, Prediction Accuracy = 60.25%, Loss = 0.7004892110824585
Epoch: 8845, Batch Gradient Norm: 27.86674603690547
Epoch: 8845, Batch Gradient Norm after: 22.36067584445285
Epoch 8846/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.7034658551216125
Epoch: 8846, Batch Gradient Norm: 26.518046780936885
Epoch: 8846, Batch Gradient Norm after: 22.36067309555618
Epoch 8847/10000, Prediction Accuracy = 60.21400000000001%, Loss = 0.7003790855407714
Epoch: 8847, Batch Gradient Norm: 27.866549179879144
Epoch: 8847, Batch Gradient Norm after: 22.360675853704606
Epoch 8848/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.7034286022186279
Epoch: 8848, Batch Gradient Norm: 26.52369523779384
Epoch: 8848, Batch Gradient Norm after: 22.36067684398861
Epoch 8849/10000, Prediction Accuracy = 60.27%, Loss = 0.7003914594650269
Epoch: 8849, Batch Gradient Norm: 27.86915992854644
Epoch: 8849, Batch Gradient Norm after: 22.360677329263723
Epoch 8850/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.7035886287689209
Epoch: 8850, Batch Gradient Norm: 26.51043164123487
Epoch: 8850, Batch Gradient Norm after: 22.360678306724864
Epoch 8851/10000, Prediction Accuracy = 60.312%, Loss = 0.7004887580871582
Epoch: 8851, Batch Gradient Norm: 27.861705781929437
Epoch: 8851, Batch Gradient Norm after: 22.360675668998258
Epoch 8852/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.7035300493240356
Epoch: 8852, Batch Gradient Norm: 26.50806181806844
Epoch: 8852, Batch Gradient Norm after: 22.36067646988973
Epoch 8853/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.7002985954284668
Epoch: 8853, Batch Gradient Norm: 27.859995012674688
Epoch: 8853, Batch Gradient Norm after: 22.36067578696495
Epoch 8854/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.7032594323158264
Epoch: 8854, Batch Gradient Norm: 26.51211904270815
Epoch: 8854, Batch Gradient Norm after: 22.360678401051874
Epoch 8855/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.7001933455467224
Epoch: 8855, Batch Gradient Norm: 27.8559522411946
Epoch: 8855, Batch Gradient Norm after: 22.36067539591322
Epoch 8856/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.7031802415847779
Epoch: 8856, Batch Gradient Norm: 26.51340794329826
Epoch: 8856, Batch Gradient Norm after: 22.3606784866837
Epoch 8857/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.7001996397972107
Epoch: 8857, Batch Gradient Norm: 27.845812311918124
Epoch: 8857, Batch Gradient Norm after: 22.360677021204168
Epoch 8858/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.7031372427940369
Epoch: 8858, Batch Gradient Norm: 26.515593636040276
Epoch: 8858, Batch Gradient Norm after: 22.36067517935394
Epoch 8859/10000, Prediction Accuracy = 60.222%, Loss = 0.7000905871391296
Epoch: 8859, Batch Gradient Norm: 27.844275904215795
Epoch: 8859, Batch Gradient Norm after: 22.360676213021694
Epoch 8860/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.7031079292297363
Epoch: 8860, Batch Gradient Norm: 26.518935973815168
Epoch: 8860, Batch Gradient Norm after: 22.360675402308406
Epoch 8861/10000, Prediction Accuracy = 60.286%, Loss = 0.700112235546112
Epoch: 8861, Batch Gradient Norm: 27.844927708311047
Epoch: 8861, Batch Gradient Norm after: 22.360677724019176
Epoch 8862/10000, Prediction Accuracy = 60.322%, Loss = 0.7032711982727051
Epoch: 8862, Batch Gradient Norm: 26.502632966024787
Epoch: 8862, Batch Gradient Norm after: 22.36067695013816
Epoch 8863/10000, Prediction Accuracy = 60.29%, Loss = 0.7001964807510376
Epoch: 8863, Batch Gradient Norm: 27.84231162947782
Epoch: 8863, Batch Gradient Norm after: 22.36067608960919
Epoch 8864/10000, Prediction Accuracy = 60.314%, Loss = 0.7031962633132934
Epoch: 8864, Batch Gradient Norm: 26.498707664786437
Epoch: 8864, Batch Gradient Norm after: 22.360677725768706
Epoch 8865/10000, Prediction Accuracy = 60.302%, Loss = 0.6999999284744263
Epoch: 8865, Batch Gradient Norm: 27.84144043683619
Epoch: 8865, Batch Gradient Norm after: 22.360675664483566
Epoch 8866/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.7029289841651917
Epoch: 8866, Batch Gradient Norm: 26.50133156517376
Epoch: 8866, Batch Gradient Norm after: 22.360678206454836
Epoch 8867/10000, Prediction Accuracy = 60.25%, Loss = 0.6999056577682495
Epoch: 8867, Batch Gradient Norm: 27.8413737389455
Epoch: 8867, Batch Gradient Norm after: 22.36067611962407
Epoch 8868/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.7028733491897583
Epoch: 8868, Batch Gradient Norm: 26.499595816696857
Epoch: 8868, Batch Gradient Norm after: 22.36067913459111
Epoch 8869/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.699899685382843
Epoch: 8869, Batch Gradient Norm: 27.828857915200395
Epoch: 8869, Batch Gradient Norm after: 22.360675455919242
Epoch 8870/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.702833890914917
Epoch: 8870, Batch Gradient Norm: 26.495934336419385
Epoch: 8870, Batch Gradient Norm after: 22.360675691602484
Epoch 8871/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6997673034667968
Epoch: 8871, Batch Gradient Norm: 27.833679700310512
Epoch: 8871, Batch Gradient Norm after: 22.360678571052453
Epoch 8872/10000, Prediction Accuracy = 60.278%, Loss = 0.7028228878974915
Epoch: 8872, Batch Gradient Norm: 26.49135608633923
Epoch: 8872, Batch Gradient Norm after: 22.36067691033585
Epoch 8873/10000, Prediction Accuracy = 60.27%, Loss = 0.6998033165931702
Epoch: 8873, Batch Gradient Norm: 27.834494333935464
Epoch: 8873, Batch Gradient Norm after: 22.360679244656385
Epoch 8874/10000, Prediction Accuracy = 60.33%, Loss = 0.7029994487762451
Epoch: 8874, Batch Gradient Norm: 26.476674198224668
Epoch: 8874, Batch Gradient Norm after: 22.360678442498934
Epoch 8875/10000, Prediction Accuracy = 60.288%, Loss = 0.6998618245124817
Epoch: 8875, Batch Gradient Norm: 27.826943053211608
Epoch: 8875, Batch Gradient Norm after: 22.360676976885678
Epoch 8876/10000, Prediction Accuracy = 60.302%, Loss = 0.7028867721557617
Epoch: 8876, Batch Gradient Norm: 26.465822471925105
Epoch: 8876, Batch Gradient Norm after: 22.360677216128522
Epoch 8877/10000, Prediction Accuracy = 60.288%, Loss = 0.6996390223503113
Epoch: 8877, Batch Gradient Norm: 27.83373469256045
Epoch: 8877, Batch Gradient Norm after: 22.36067699406341
Epoch 8878/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.7026392102241517
Epoch: 8878, Batch Gradient Norm: 26.471388171512526
Epoch: 8878, Batch Gradient Norm after: 22.360677292266583
Epoch 8879/10000, Prediction Accuracy = 60.238%, Loss = 0.6995646357536316
Epoch: 8879, Batch Gradient Norm: 27.828339243602063
Epoch: 8879, Batch Gradient Norm after: 22.360674914803905
Epoch 8880/10000, Prediction Accuracy = 60.3%, Loss = 0.7025935411453247
Epoch: 8880, Batch Gradient Norm: 26.466123410487615
Epoch: 8880, Batch Gradient Norm after: 22.360675446964624
Epoch 8881/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.699543559551239
Epoch: 8881, Batch Gradient Norm: 27.81905837984463
Epoch: 8881, Batch Gradient Norm after: 22.360676145376207
Epoch 8882/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.702544116973877
Epoch: 8882, Batch Gradient Norm: 26.464500480139012
Epoch: 8882, Batch Gradient Norm after: 22.3606774673578
Epoch 8883/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.69942227602005
Epoch: 8883, Batch Gradient Norm: 27.81879504756703
Epoch: 8883, Batch Gradient Norm after: 22.36067841302185
Epoch 8884/10000, Prediction Accuracy = 60.27%, Loss = 0.7025490880012513
Epoch: 8884, Batch Gradient Norm: 26.462874780843247
Epoch: 8884, Batch Gradient Norm after: 22.360676435247658
Epoch 8885/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.6994904041290283
Epoch: 8885, Batch Gradient Norm: 27.82029429001134
Epoch: 8885, Batch Gradient Norm after: 22.3606780269428
Epoch 8886/10000, Prediction Accuracy = 60.336%, Loss = 0.7027149558067322
Epoch: 8886, Batch Gradient Norm: 26.45577842327983
Epoch: 8886, Batch Gradient Norm after: 22.360676436043303
Epoch 8887/10000, Prediction Accuracy = 60.274%, Loss = 0.699523413181305
Epoch: 8887, Batch Gradient Norm: 27.811357899696294
Epoch: 8887, Batch Gradient Norm after: 22.36067708253405
Epoch 8888/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.7025575637817383
Epoch: 8888, Batch Gradient Norm: 26.449334697161362
Epoch: 8888, Batch Gradient Norm after: 22.360677958403755
Epoch 8889/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.6993191957473754
Epoch: 8889, Batch Gradient Norm: 27.81528135546786
Epoch: 8889, Batch Gradient Norm after: 22.360677482921865
Epoch 8890/10000, Prediction Accuracy = 60.294%, Loss = 0.7023359417915345
Epoch: 8890, Batch Gradient Norm: 26.4542970626387
Epoch: 8890, Batch Gradient Norm after: 22.360678050585076
Epoch 8891/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6992590665817261
Epoch: 8891, Batch Gradient Norm: 27.811021729965617
Epoch: 8891, Batch Gradient Norm after: 22.36067698521002
Epoch 8892/10000, Prediction Accuracy = 60.29599999999999%, Loss = 0.702289879322052
Epoch: 8892, Batch Gradient Norm: 26.453515785613654
Epoch: 8892, Batch Gradient Norm after: 22.360676672444363
Epoch 8893/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6992533564567566
Epoch: 8893, Batch Gradient Norm: 27.797600547165818
Epoch: 8893, Batch Gradient Norm after: 22.360676919117697
Epoch 8894/10000, Prediction Accuracy = 60.238%, Loss = 0.7022329926490783
Epoch: 8894, Batch Gradient Norm: 26.45361673336168
Epoch: 8894, Batch Gradient Norm after: 22.360677662713822
Epoch 8895/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6991356015205383
Epoch: 8895, Batch Gradient Norm: 27.797816450896317
Epoch: 8895, Batch Gradient Norm after: 22.360679904337953
Epoch 8896/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.7022440671920777
Epoch: 8896, Batch Gradient Norm: 26.457142132377104
Epoch: 8896, Batch Gradient Norm after: 22.360676950746893
Epoch 8897/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6992237091064453
Epoch: 8897, Batch Gradient Norm: 27.798715904917987
Epoch: 8897, Batch Gradient Norm after: 22.36067437032309
Epoch 8898/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.7024277448654175
Epoch: 8898, Batch Gradient Norm: 26.443590630404316
Epoch: 8898, Batch Gradient Norm after: 22.36067782147682
Epoch 8899/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6992416977882385
Epoch: 8899, Batch Gradient Norm: 27.792563246119816
Epoch: 8899, Batch Gradient Norm after: 22.36067768624946
Epoch 8900/10000, Prediction Accuracy = 60.31%, Loss = 0.7022361755371094
Epoch: 8900, Batch Gradient Norm: 26.439386873158288
Epoch: 8900, Batch Gradient Norm after: 22.360678229369675
Epoch 8901/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6990221738815308
Epoch: 8901, Batch Gradient Norm: 27.79687864217122
Epoch: 8901, Batch Gradient Norm after: 22.360675880942555
Epoch 8902/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.7020136594772339
Epoch: 8902, Batch Gradient Norm: 26.4417290550924
Epoch: 8902, Batch Gradient Norm after: 22.360675891890438
Epoch 8903/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6989663124084473
Epoch: 8903, Batch Gradient Norm: 27.790882661555425
Epoch: 8903, Batch Gradient Norm after: 22.360677418480098
Epoch 8904/10000, Prediction Accuracy = 60.29%, Loss = 0.701972246170044
Epoch: 8904, Batch Gradient Norm: 26.438410119877755
Epoch: 8904, Batch Gradient Norm after: 22.360677365232046
Epoch 8905/10000, Prediction Accuracy = 60.266000000000005%, Loss = 0.6989435672760009
Epoch: 8905, Batch Gradient Norm: 27.783365153675014
Epoch: 8905, Batch Gradient Norm after: 22.36067672058378
Epoch 8906/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.70192049741745
Epoch: 8906, Batch Gradient Norm: 26.434525237362994
Epoch: 8906, Batch Gradient Norm after: 22.360676221183084
Epoch 8907/10000, Prediction Accuracy = 60.278%, Loss = 0.6988224387168884
Epoch: 8907, Batch Gradient Norm: 27.787504857486272
Epoch: 8907, Batch Gradient Norm after: 22.36067651149807
Epoch 8908/10000, Prediction Accuracy = 60.278000000000006%, Loss = 0.7019564151763916
Epoch: 8908, Batch Gradient Norm: 26.425881714436777
Epoch: 8908, Batch Gradient Norm after: 22.360674698710312
Epoch 8909/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.698893141746521
Epoch: 8909, Batch Gradient Norm: 27.792611605715127
Epoch: 8909, Batch Gradient Norm after: 22.36067841102079
Epoch 8910/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.7021060109138488
Epoch: 8910, Batch Gradient Norm: 26.407335295390077
Epoch: 8910, Batch Gradient Norm after: 22.360677584765416
Epoch 8911/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6988757491111756
Epoch: 8911, Batch Gradient Norm: 27.788024330888206
Epoch: 8911, Batch Gradient Norm after: 22.36067812600272
Epoch 8912/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.7019471526145935
Epoch: 8912, Batch Gradient Norm: 26.406197509373897
Epoch: 8912, Batch Gradient Norm after: 22.360677184364203
Epoch 8913/10000, Prediction Accuracy = 60.282%, Loss = 0.6986788868904114
Epoch: 8913, Batch Gradient Norm: 27.79044444356699
Epoch: 8913, Batch Gradient Norm after: 22.36067913034985
Epoch 8914/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.7017265796661377
Epoch: 8914, Batch Gradient Norm: 26.40876316953065
Epoch: 8914, Batch Gradient Norm after: 22.360677331199184
Epoch 8915/10000, Prediction Accuracy = 60.29%, Loss = 0.6986382126808166
Epoch: 8915, Batch Gradient Norm: 27.784090400770342
Epoch: 8915, Batch Gradient Norm after: 22.36067760410353
Epoch 8916/10000, Prediction Accuracy = 60.282%, Loss = 0.7016910314559937
Epoch: 8916, Batch Gradient Norm: 26.409673946835134
Epoch: 8916, Batch Gradient Norm after: 22.360676748085908
Epoch 8917/10000, Prediction Accuracy = 60.246%, Loss = 0.6986131310462952
Epoch: 8917, Batch Gradient Norm: 27.77366781229531
Epoch: 8917, Batch Gradient Norm after: 22.360675941884736
Epoch 8918/10000, Prediction Accuracy = 60.266%, Loss = 0.7016258835792542
Epoch: 8918, Batch Gradient Norm: 26.409751921886862
Epoch: 8918, Batch Gradient Norm after: 22.360676606886663
Epoch 8919/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6984979152679444
Epoch: 8919, Batch Gradient Norm: 27.777810142626116
Epoch: 8919, Batch Gradient Norm after: 22.360676633112938
Epoch 8920/10000, Prediction Accuracy = 60.266%, Loss = 0.7016719698905944
Epoch: 8920, Batch Gradient Norm: 26.404853966422923
Epoch: 8920, Batch Gradient Norm after: 22.360678448553784
Epoch 8921/10000, Prediction Accuracy = 60.278%, Loss = 0.6985958576202392
Epoch: 8921, Batch Gradient Norm: 27.777528373318333
Epoch: 8921, Batch Gradient Norm after: 22.360675699884453
Epoch 8922/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.7018379092216491
Epoch: 8922, Batch Gradient Norm: 26.389959627850747
Epoch: 8922, Batch Gradient Norm after: 22.360676555611473
Epoch 8923/10000, Prediction Accuracy = 60.282%, Loss = 0.6985874772071838
Epoch: 8923, Batch Gradient Norm: 27.7713591489704
Epoch: 8923, Batch Gradient Norm after: 22.360677720001934
Epoch 8924/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.7016229391098022
Epoch: 8924, Batch Gradient Norm: 26.39142138152373
Epoch: 8924, Batch Gradient Norm after: 22.360677679615907
Epoch 8925/10000, Prediction Accuracy = 60.286%, Loss = 0.6983714818954467
Epoch: 8925, Batch Gradient Norm: 27.7739045797931
Epoch: 8925, Batch Gradient Norm after: 22.360677364904298
Epoch 8926/10000, Prediction Accuracy = 60.30400000000001%, Loss = 0.7014145374298095
Epoch: 8926, Batch Gradient Norm: 26.396596534146816
Epoch: 8926, Batch Gradient Norm after: 22.36067486365116
Epoch 8927/10000, Prediction Accuracy = 60.27%, Loss = 0.6983402132987976
Epoch: 8927, Batch Gradient Norm: 27.765154403009067
Epoch: 8927, Batch Gradient Norm after: 22.36067704534148
Epoch 8928/10000, Prediction Accuracy = 60.282%, Loss = 0.7013819694519043
Epoch: 8928, Batch Gradient Norm: 26.39746945571194
Epoch: 8928, Batch Gradient Norm after: 22.360677406322875
Epoch 8929/10000, Prediction Accuracy = 60.25%, Loss = 0.6983106851577758
Epoch: 8929, Batch Gradient Norm: 27.75315261705015
Epoch: 8929, Batch Gradient Norm after: 22.3606790267174
Epoch 8930/10000, Prediction Accuracy = 60.274%, Loss = 0.7013161420822144
Epoch: 8930, Batch Gradient Norm: 26.40107716480501
Epoch: 8930, Batch Gradient Norm after: 22.360680806469723
Epoch 8931/10000, Prediction Accuracy = 60.294%, Loss = 0.6982126593589782
Epoch: 8931, Batch Gradient Norm: 27.754413182366353
Epoch: 8931, Batch Gradient Norm after: 22.360676937776564
Epoch 8932/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.7013705253601075
Epoch: 8932, Batch Gradient Norm: 26.398706211403447
Epoch: 8932, Batch Gradient Norm after: 22.360680123282485
Epoch 8933/10000, Prediction Accuracy = 60.298%, Loss = 0.698312270641327
Epoch: 8933, Batch Gradient Norm: 27.750116741747263
Epoch: 8933, Batch Gradient Norm after: 22.36067948075091
Epoch 8934/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.7014966607093811
Epoch: 8934, Batch Gradient Norm: 26.38896874087123
Epoch: 8934, Batch Gradient Norm after: 22.36067855887285
Epoch 8935/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.6982954740524292
Epoch: 8935, Batch Gradient Norm: 27.743410103642084
Epoch: 8935, Batch Gradient Norm after: 22.360678553164828
Epoch 8936/10000, Prediction Accuracy = 60.302%, Loss = 0.7012845158576966
Epoch: 8936, Batch Gradient Norm: 26.391226908704507
Epoch: 8936, Batch Gradient Norm after: 22.36067908812193
Epoch 8937/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6980953097343445
Epoch: 8937, Batch Gradient Norm: 27.747816889211364
Epoch: 8937, Batch Gradient Norm after: 22.36067676764284
Epoch 8938/10000, Prediction Accuracy = 60.29%, Loss = 0.7010757446289062
Epoch: 8938, Batch Gradient Norm: 26.402869216602866
Epoch: 8938, Batch Gradient Norm after: 22.360677661081205
Epoch 8939/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6980877995491028
Epoch: 8939, Batch Gradient Norm: 27.736314742657857
Epoch: 8939, Batch Gradient Norm after: 22.360675665487346
Epoch 8940/10000, Prediction Accuracy = 60.306%, Loss = 0.7010538101196289
Epoch: 8940, Batch Gradient Norm: 26.400894286824418
Epoch: 8940, Batch Gradient Norm after: 22.360677852418835
Epoch 8941/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6980368494987488
Epoch: 8941, Batch Gradient Norm: 27.728070741460876
Epoch: 8941, Batch Gradient Norm after: 22.36067640312451
Epoch 8942/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.700980269908905
Epoch: 8942, Batch Gradient Norm: 26.403756586195232
Epoch: 8942, Batch Gradient Norm after: 22.3606769248318
Epoch 8943/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6979505896568299
Epoch: 8943, Batch Gradient Norm: 27.732221298551597
Epoch: 8943, Batch Gradient Norm after: 22.360676658744907
Epoch 8944/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.7010498642921448
Epoch: 8944, Batch Gradient Norm: 26.39995843609201
Epoch: 8944, Batch Gradient Norm after: 22.36067772665772
Epoch 8945/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.698067307472229
Epoch: 8945, Batch Gradient Norm: 27.725528284911533
Epoch: 8945, Batch Gradient Norm after: 22.360676325953158
Epoch 8946/10000, Prediction Accuracy = 60.336%, Loss = 0.7011679530143737
Epoch: 8946, Batch Gradient Norm: 26.389456848086294
Epoch: 8946, Batch Gradient Norm after: 22.36067764764375
Epoch 8947/10000, Prediction Accuracy = 60.3%, Loss = 0.6980067729949951
Epoch: 8947, Batch Gradient Norm: 27.723574848418604
Epoch: 8947, Batch Gradient Norm after: 22.36067707154199
Epoch 8948/10000, Prediction Accuracy = 60.31%, Loss = 0.7009312152862549
Epoch: 8948, Batch Gradient Norm: 26.39052489190895
Epoch: 8948, Batch Gradient Norm after: 22.360677740473765
Epoch 8949/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6978130459785461
Epoch: 8949, Batch Gradient Norm: 27.72314800795886
Epoch: 8949, Batch Gradient Norm after: 22.360676729416316
Epoch 8950/10000, Prediction Accuracy = 60.312%, Loss = 0.7007486581802368
Epoch: 8950, Batch Gradient Norm: 26.396414113564553
Epoch: 8950, Batch Gradient Norm after: 22.360678438247614
Epoch 8951/10000, Prediction Accuracy = 60.25%, Loss = 0.6978190898895263
Epoch: 8951, Batch Gradient Norm: 27.71367604395927
Epoch: 8951, Batch Gradient Norm after: 22.36067467739008
Epoch 8952/10000, Prediction Accuracy = 60.3%, Loss = 0.70073401927948
Epoch: 8952, Batch Gradient Norm: 26.397187783997627
Epoch: 8952, Batch Gradient Norm after: 22.360677929345652
Epoch 8953/10000, Prediction Accuracy = 60.242%, Loss = 0.6977432847023011
Epoch: 8953, Batch Gradient Norm: 27.70947692882636
Epoch: 8953, Batch Gradient Norm after: 22.360680174121153
Epoch 8954/10000, Prediction Accuracy = 60.312%, Loss = 0.700660514831543
Epoch: 8954, Batch Gradient Norm: 26.397089858364783
Epoch: 8954, Batch Gradient Norm after: 22.36067743369957
Epoch 8955/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6976743459701538
Epoch: 8955, Batch Gradient Norm: 27.713791519054173
Epoch: 8955, Batch Gradient Norm after: 22.36067802640291
Epoch 8956/10000, Prediction Accuracy = 60.31%, Loss = 0.7007646918296814
Epoch: 8956, Batch Gradient Norm: 26.38880245201456
Epoch: 8956, Batch Gradient Norm after: 22.360676212894372
Epoch 8957/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6977940678596497
Epoch: 8957, Batch Gradient Norm: 27.707096592952755
Epoch: 8957, Batch Gradient Norm after: 22.360676288102002
Epoch 8958/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.7008454084396363
Epoch: 8958, Batch Gradient Norm: 26.37661117099838
Epoch: 8958, Batch Gradient Norm after: 22.360675991573018
Epoch 8959/10000, Prediction Accuracy = 60.314%, Loss = 0.6976877689361572
Epoch: 8959, Batch Gradient Norm: 27.705144379279233
Epoch: 8959, Batch Gradient Norm after: 22.36067614691516
Epoch 8960/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.7005945801734924
Epoch: 8960, Batch Gradient Norm: 26.380918904243394
Epoch: 8960, Batch Gradient Norm after: 22.36068009105157
Epoch 8961/10000, Prediction Accuracy = 60.238%, Loss = 0.6975172996520996
Epoch: 8961, Batch Gradient Norm: 27.70763953337162
Epoch: 8961, Batch Gradient Norm after: 22.36067676280143
Epoch 8962/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.7004340410232544
Epoch: 8962, Batch Gradient Norm: 26.382811652727693
Epoch: 8962, Batch Gradient Norm after: 22.360678307519535
Epoch 8963/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6975190758705139
Epoch: 8963, Batch Gradient Norm: 27.69702496114088
Epoch: 8963, Batch Gradient Norm after: 22.360674653073183
Epoch 8964/10000, Prediction Accuracy = 60.298%, Loss = 0.7004272103309631
Epoch: 8964, Batch Gradient Norm: 26.378380195527107
Epoch: 8964, Batch Gradient Norm after: 22.36067873323882
Epoch 8965/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6974333167076111
Epoch: 8965, Batch Gradient Norm: 27.69516737012816
Epoch: 8965, Batch Gradient Norm after: 22.360677766320045
Epoch 8966/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.7003599762916565
Epoch: 8966, Batch Gradient Norm: 26.378164451500993
Epoch: 8966, Batch Gradient Norm after: 22.360675763176076
Epoch 8967/10000, Prediction Accuracy = 60.27%, Loss = 0.6973708510398865
Epoch: 8967, Batch Gradient Norm: 27.70178114749099
Epoch: 8967, Batch Gradient Norm after: 22.36067542018427
Epoch 8968/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.7004889488220215
Epoch: 8968, Batch Gradient Norm: 26.367628258275875
Epoch: 8968, Batch Gradient Norm after: 22.360677146299746
Epoch 8969/10000, Prediction Accuracy = 60.302%, Loss = 0.6974972605705261
Epoch: 8969, Batch Gradient Norm: 27.693917740080565
Epoch: 8969, Batch Gradient Norm after: 22.36067644472061
Epoch 8970/10000, Prediction Accuracy = 60.362%, Loss = 0.7005321741104126
Epoch: 8970, Batch Gradient Norm: 26.359552927292718
Epoch: 8970, Batch Gradient Norm after: 22.36067788476415
Epoch 8971/10000, Prediction Accuracy = 60.3%, Loss = 0.6973584651947021
Epoch: 8971, Batch Gradient Norm: 27.692534745687997
Epoch: 8971, Batch Gradient Norm after: 22.360674955756355
Epoch 8972/10000, Prediction Accuracy = 60.302%, Loss = 0.7002694487571717
Epoch: 8972, Batch Gradient Norm: 26.36196419857064
Epoch: 8972, Batch Gradient Norm after: 22.360679124786266
Epoch 8973/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6972050309181214
Epoch: 8973, Batch Gradient Norm: 27.69133857282216
Epoch: 8973, Batch Gradient Norm after: 22.360675273318666
Epoch 8974/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.7001331567764282
Epoch: 8974, Batch Gradient Norm: 26.36453969292052
Epoch: 8974, Batch Gradient Norm after: 22.360677759832132
Epoch 8975/10000, Prediction Accuracy = 60.262%, Loss = 0.6972155809402466
Epoch: 8975, Batch Gradient Norm: 27.680955214038466
Epoch: 8975, Batch Gradient Norm after: 22.360675784531107
Epoch 8976/10000, Prediction Accuracy = 60.298%, Loss = 0.7001137018203736
Epoch: 8976, Batch Gradient Norm: 26.36837296435248
Epoch: 8976, Batch Gradient Norm after: 22.360677488016645
Epoch 8977/10000, Prediction Accuracy = 60.25%, Loss = 0.6971238613128662
Epoch: 8977, Batch Gradient Norm: 27.67533762191717
Epoch: 8977, Batch Gradient Norm after: 22.360676021916987
Epoch 8978/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.7000485897064209
Epoch: 8978, Batch Gradient Norm: 26.374448120350227
Epoch: 8978, Batch Gradient Norm after: 22.36067797328395
Epoch 8979/10000, Prediction Accuracy = 60.266000000000005%, Loss = 0.6971018791198731
Epoch: 8979, Batch Gradient Norm: 27.67690456259622
Epoch: 8979, Batch Gradient Norm after: 22.36067553490624
Epoch 8980/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.7001769185066223
Epoch: 8980, Batch Gradient Norm: 26.368393636694154
Epoch: 8980, Batch Gradient Norm after: 22.360677200823357
Epoch 8981/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6972338438034058
Epoch: 8981, Batch Gradient Norm: 27.668850273580947
Epoch: 8981, Batch Gradient Norm after: 22.36067706525324
Epoch 8982/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.7001980543136597
Epoch: 8982, Batch Gradient Norm: 26.363416941483358
Epoch: 8982, Batch Gradient Norm after: 22.360677712829187
Epoch 8983/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.6970847725868226
Epoch: 8983, Batch Gradient Norm: 27.66542781043844
Epoch: 8983, Batch Gradient Norm after: 22.360675406117096
Epoch 8984/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6999209880828857
Epoch: 8984, Batch Gradient Norm: 26.366926405652496
Epoch: 8984, Batch Gradient Norm after: 22.360675562182532
Epoch 8985/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.69694744348526
Epoch: 8985, Batch Gradient Norm: 27.665501053845215
Epoch: 8985, Batch Gradient Norm after: 22.360676426987435
Epoch 8986/10000, Prediction Accuracy = 60.31%, Loss = 0.6998016119003296
Epoch: 8986, Batch Gradient Norm: 26.37540845745688
Epoch: 8986, Batch Gradient Norm after: 22.360675961979677
Epoch 8987/10000, Prediction Accuracy = 60.258%, Loss = 0.6969691038131713
Epoch: 8987, Batch Gradient Norm: 27.64930170651435
Epoch: 8987, Batch Gradient Norm after: 22.360677749175167
Epoch 8988/10000, Prediction Accuracy = 60.306%, Loss = 0.6997809767723083
Epoch: 8988, Batch Gradient Norm: 26.371521869916606
Epoch: 8988, Batch Gradient Norm after: 22.360677129434816
Epoch 8989/10000, Prediction Accuracy = 60.254%, Loss = 0.6968573093414306
Epoch: 8989, Batch Gradient Norm: 27.648717564614675
Epoch: 8989, Batch Gradient Norm after: 22.360675598038526
Epoch 8990/10000, Prediction Accuracy = 60.334%, Loss = 0.6997296333312988
Epoch: 8990, Batch Gradient Norm: 26.372479858360172
Epoch: 8990, Batch Gradient Norm after: 22.360675835631344
Epoch 8991/10000, Prediction Accuracy = 60.29%, Loss = 0.6968427777290345
Epoch: 8991, Batch Gradient Norm: 27.65137666339293
Epoch: 8991, Batch Gradient Norm after: 22.3606752905886
Epoch 8992/10000, Prediction Accuracy = 60.35%, Loss = 0.6998939275741577
Epoch: 8992, Batch Gradient Norm: 26.36128173735518
Epoch: 8992, Batch Gradient Norm after: 22.360677883916786
Epoch 8993/10000, Prediction Accuracy = 60.322%, Loss = 0.6969629526138306
Epoch: 8993, Batch Gradient Norm: 27.642922125889424
Epoch: 8993, Batch Gradient Norm after: 22.36067779588055
Epoch 8994/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6998862028121948
Epoch: 8994, Batch Gradient Norm: 26.347390881816516
Epoch: 8994, Batch Gradient Norm after: 22.360678723245353
Epoch 8995/10000, Prediction Accuracy = 60.302%, Loss = 0.6967689514160156
Epoch: 8995, Batch Gradient Norm: 27.646330088599242
Epoch: 8995, Batch Gradient Norm after: 22.360675976577415
Epoch 8996/10000, Prediction Accuracy = 60.3%, Loss = 0.6996077179908753
Epoch: 8996, Batch Gradient Norm: 26.34957377798942
Epoch: 8996, Batch Gradient Norm after: 22.360679760188937
Epoch 8997/10000, Prediction Accuracy = 60.24000000000001%, Loss = 0.696637499332428
Epoch: 8997, Batch Gradient Norm: 27.645984846664568
Epoch: 8997, Batch Gradient Norm after: 22.360676449497273
Epoch 8998/10000, Prediction Accuracy = 60.312%, Loss = 0.6995143413543701
Epoch: 8998, Batch Gradient Norm: 26.346367768343516
Epoch: 8998, Batch Gradient Norm after: 22.360677318404136
Epoch 8999/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6966320037841797
Epoch: 8999, Batch Gradient Norm: 27.638047770738805
Epoch: 8999, Batch Gradient Norm after: 22.3606751085795
Epoch 9000/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6994856238365174
Epoch: 9000, Batch Gradient Norm: 26.34244443330786
Epoch: 9000, Batch Gradient Norm after: 22.36067747401663
Epoch 9001/10000, Prediction Accuracy = 60.254%, Loss = 0.696506917476654
Epoch: 9001, Batch Gradient Norm: 27.637764787126237
Epoch: 9001, Batch Gradient Norm after: 22.36067852265213
Epoch 9002/10000, Prediction Accuracy = 60.33%, Loss = 0.6994518995285034
Epoch: 9002, Batch Gradient Norm: 26.34528554598901
Epoch: 9002, Batch Gradient Norm after: 22.36067889490488
Epoch 9003/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6965092778205871
Epoch: 9003, Batch Gradient Norm: 27.638354504318972
Epoch: 9003, Batch Gradient Norm after: 22.360674471485854
Epoch 9004/10000, Prediction Accuracy = 60.342%, Loss = 0.699609637260437
Epoch: 9004, Batch Gradient Norm: 26.330424441699478
Epoch: 9004, Batch Gradient Norm after: 22.360677649257227
Epoch 9005/10000, Prediction Accuracy = 60.306%, Loss = 0.696622347831726
Epoch: 9005, Batch Gradient Norm: 27.63372318378406
Epoch: 9005, Batch Gradient Norm after: 22.36067670163151
Epoch 9006/10000, Prediction Accuracy = 60.336%, Loss = 0.6995824337005615
Epoch: 9006, Batch Gradient Norm: 26.320811347306503
Epoch: 9006, Batch Gradient Norm after: 22.360678495070633
Epoch 9007/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6964231491088867
Epoch: 9007, Batch Gradient Norm: 27.63728900356695
Epoch: 9007, Batch Gradient Norm after: 22.360677121987518
Epoch 9008/10000, Prediction Accuracy = 60.302%, Loss = 0.6993014812469482
Epoch: 9008, Batch Gradient Norm: 26.330972379207797
Epoch: 9008, Batch Gradient Norm after: 22.360679998901315
Epoch 9009/10000, Prediction Accuracy = 60.254%, Loss = 0.6963175058364868
Epoch: 9009, Batch Gradient Norm: 27.63322649612102
Epoch: 9009, Batch Gradient Norm after: 22.360675149799306
Epoch 9010/10000, Prediction Accuracy = 60.322%, Loss = 0.6992239594459534
Epoch: 9010, Batch Gradient Norm: 26.329505123123834
Epoch: 9010, Batch Gradient Norm after: 22.360676383436918
Epoch 9011/10000, Prediction Accuracy = 60.238%, Loss = 0.6963265538215637
Epoch: 9011, Batch Gradient Norm: 27.622023727826015
Epoch: 9011, Batch Gradient Norm after: 22.36067663130985
Epoch 9012/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6991869449615479
Epoch: 9012, Batch Gradient Norm: 26.32654048872625
Epoch: 9012, Batch Gradient Norm after: 22.360676884240867
Epoch 9013/10000, Prediction Accuracy = 60.262%, Loss = 0.6961902618408203
Epoch: 9013, Batch Gradient Norm: 27.625287527716967
Epoch: 9013, Batch Gradient Norm after: 22.360678295305963
Epoch 9014/10000, Prediction Accuracy = 60.314%, Loss = 0.6991563439369202
Epoch: 9014, Batch Gradient Norm: 26.327123804139184
Epoch: 9014, Batch Gradient Norm after: 22.360677109311084
Epoch 9015/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6962199211120605
Epoch: 9015, Batch Gradient Norm: 27.622749229637485
Epoch: 9015, Batch Gradient Norm after: 22.36067940507344
Epoch 9016/10000, Prediction Accuracy = 60.33%, Loss = 0.6993348479270936
Epoch: 9016, Batch Gradient Norm: 26.319831339661935
Epoch: 9016, Batch Gradient Norm after: 22.360678805765215
Epoch 9017/10000, Prediction Accuracy = 60.302%, Loss = 0.6963232278823852
Epoch: 9017, Batch Gradient Norm: 27.612492114454195
Epoch: 9017, Batch Gradient Norm after: 22.36067693198419
Epoch 9018/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6992514252662658
Epoch: 9018, Batch Gradient Norm: 26.317498713828712
Epoch: 9018, Batch Gradient Norm after: 22.360676632863655
Epoch 9019/10000, Prediction Accuracy = 60.32199999999999%, Loss = 0.6961204648017884
Epoch: 9019, Batch Gradient Norm: 27.61308919372468
Epoch: 9019, Batch Gradient Norm after: 22.360676233767038
Epoch 9020/10000, Prediction Accuracy = 60.31%, Loss = 0.6989736199378968
Epoch: 9020, Batch Gradient Norm: 26.321414373851347
Epoch: 9020, Batch Gradient Norm after: 22.360679267138025
Epoch 9021/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6960324883460999
Epoch: 9021, Batch Gradient Norm: 27.610252425338718
Epoch: 9021, Batch Gradient Norm after: 22.36067639471217
Epoch 9022/10000, Prediction Accuracy = 60.318%, Loss = 0.6989138126373291
Epoch: 9022, Batch Gradient Norm: 26.323137847582867
Epoch: 9022, Batch Gradient Norm after: 22.360678991834277
Epoch 9023/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6960305333137512
Epoch: 9023, Batch Gradient Norm: 27.60092356990467
Epoch: 9023, Batch Gradient Norm after: 22.360675770616
Epoch 9024/10000, Prediction Accuracy = 60.302%, Loss = 0.6988738059997559
Epoch: 9024, Batch Gradient Norm: 26.317983720036867
Epoch: 9024, Batch Gradient Norm after: 22.360677261222097
Epoch 9025/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6959017038345336
Epoch: 9025, Batch Gradient Norm: 27.60297953139627
Epoch: 9025, Batch Gradient Norm after: 22.360678116839306
Epoch 9026/10000, Prediction Accuracy = 60.32800000000001%, Loss = 0.6988476872444153
Epoch: 9026, Batch Gradient Norm: 26.318666025593252
Epoch: 9026, Batch Gradient Norm after: 22.360677365991787
Epoch 9027/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.695932126045227
Epoch: 9027, Batch Gradient Norm: 27.60341426219582
Epoch: 9027, Batch Gradient Norm after: 22.360677533339896
Epoch 9028/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6990289330482483
Epoch: 9028, Batch Gradient Norm: 26.30730306289392
Epoch: 9028, Batch Gradient Norm after: 22.360679421995396
Epoch 9029/10000, Prediction Accuracy = 60.302%, Loss = 0.6960312724113464
Epoch: 9029, Batch Gradient Norm: 27.59391153776604
Epoch: 9029, Batch Gradient Norm after: 22.360674598001665
Epoch 9030/10000, Prediction Accuracy = 60.35%, Loss = 0.6989541411399841
Epoch: 9030, Batch Gradient Norm: 26.30240349514247
Epoch: 9030, Batch Gradient Norm after: 22.360677762144967
Epoch 9031/10000, Prediction Accuracy = 60.318%, Loss = 0.6958261132240295
Epoch: 9031, Batch Gradient Norm: 27.59424425405753
Epoch: 9031, Batch Gradient Norm after: 22.360673930363582
Epoch 9032/10000, Prediction Accuracy = 60.318%, Loss = 0.6986732125282288
Epoch: 9032, Batch Gradient Norm: 26.307298141817625
Epoch: 9032, Batch Gradient Norm after: 22.3606769158502
Epoch 9033/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.6957354426383973
Epoch: 9033, Batch Gradient Norm: 27.593205716768082
Epoch: 9033, Batch Gradient Norm after: 22.36067605365043
Epoch 9034/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6986129760742188
Epoch: 9034, Batch Gradient Norm: 26.30817909136574
Epoch: 9034, Batch Gradient Norm after: 22.360678370603996
Epoch 9035/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6957486271858215
Epoch: 9035, Batch Gradient Norm: 27.582228429433123
Epoch: 9035, Batch Gradient Norm after: 22.36067591409867
Epoch 9036/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6985664248466492
Epoch: 9036, Batch Gradient Norm: 26.305910381682605
Epoch: 9036, Batch Gradient Norm after: 22.360676746876866
Epoch 9037/10000, Prediction Accuracy = 60.288%, Loss = 0.6956130862236023
Epoch: 9037, Batch Gradient Norm: 27.583610591368593
Epoch: 9037, Batch Gradient Norm after: 22.360677795370464
Epoch 9038/10000, Prediction Accuracy = 60.324%, Loss = 0.6985395550727844
Epoch: 9038, Batch Gradient Norm: 26.309372796851633
Epoch: 9038, Batch Gradient Norm after: 22.36067778982028
Epoch 9039/10000, Prediction Accuracy = 60.338%, Loss = 0.6956578373908997
Epoch: 9039, Batch Gradient Norm: 27.583045732380885
Epoch: 9039, Batch Gradient Norm after: 22.360678458281416
Epoch 9040/10000, Prediction Accuracy = 60.326%, Loss = 0.6987218856811523
Epoch: 9040, Batch Gradient Norm: 26.294285551280808
Epoch: 9040, Batch Gradient Norm after: 22.36067793496703
Epoch 9041/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6957389831542968
Epoch: 9041, Batch Gradient Norm: 27.577500138652287
Epoch: 9041, Batch Gradient Norm after: 22.360677601416437
Epoch 9042/10000, Prediction Accuracy = 60.33%, Loss = 0.6986333131790161
Epoch: 9042, Batch Gradient Norm: 26.288492871682042
Epoch: 9042, Batch Gradient Norm after: 22.36067742203163
Epoch 9043/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6955263137817382
Epoch: 9043, Batch Gradient Norm: 27.578266115038396
Epoch: 9043, Batch Gradient Norm after: 22.36067400502568
Epoch 9044/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.6983607411384583
Epoch: 9044, Batch Gradient Norm: 26.292301369698848
Epoch: 9044, Batch Gradient Norm after: 22.360676592714384
Epoch 9045/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6954419016838074
Epoch: 9045, Batch Gradient Norm: 27.576438871004974
Epoch: 9045, Batch Gradient Norm after: 22.360676439128753
Epoch 9046/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6983028173446655
Epoch: 9046, Batch Gradient Norm: 26.294472936913728
Epoch: 9046, Batch Gradient Norm after: 22.360677254736448
Epoch 9047/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6954407453536987
Epoch: 9047, Batch Gradient Norm: 27.566156671477284
Epoch: 9047, Batch Gradient Norm after: 22.36067479249325
Epoch 9048/10000, Prediction Accuracy = 60.3%, Loss = 0.6982568860054016
Epoch: 9048, Batch Gradient Norm: 26.290486545345118
Epoch: 9048, Batch Gradient Norm after: 22.360679290362835
Epoch 9049/10000, Prediction Accuracy = 60.278%, Loss = 0.6953168034553527
Epoch: 9049, Batch Gradient Norm: 27.56746372050292
Epoch: 9049, Batch Gradient Norm after: 22.360677103638526
Epoch 9050/10000, Prediction Accuracy = 60.298%, Loss = 0.6982442378997803
Epoch: 9050, Batch Gradient Norm: 26.28904222271738
Epoch: 9050, Batch Gradient Norm after: 22.36067942367877
Epoch 9051/10000, Prediction Accuracy = 60.338%, Loss = 0.6953604221343994
Epoch: 9051, Batch Gradient Norm: 27.56902608091991
Epoch: 9051, Batch Gradient Norm after: 22.360677350328544
Epoch 9052/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6984167337417603
Epoch: 9052, Batch Gradient Norm: 26.274879247762836
Epoch: 9052, Batch Gradient Norm after: 22.36067694468059
Epoch 9053/10000, Prediction Accuracy = 60.306%, Loss = 0.6954264163970947
Epoch: 9053, Batch Gradient Norm: 27.561236484740157
Epoch: 9053, Batch Gradient Norm after: 22.360678482230792
Epoch 9054/10000, Prediction Accuracy = 60.324%, Loss = 0.6983128547668457
Epoch: 9054, Batch Gradient Norm: 26.27172897979565
Epoch: 9054, Batch Gradient Norm after: 22.36067779932376
Epoch 9055/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6952251076698304
Epoch: 9055, Batch Gradient Norm: 27.562054215756937
Epoch: 9055, Batch Gradient Norm after: 22.360677861102406
Epoch 9056/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6980431079864502
Epoch: 9056, Batch Gradient Norm: 26.279633852658606
Epoch: 9056, Batch Gradient Norm after: 22.3606740547765
Epoch 9057/10000, Prediction Accuracy = 60.254%, Loss = 0.6951604366302491
Epoch: 9057, Batch Gradient Norm: 27.557536528948177
Epoch: 9057, Batch Gradient Norm after: 22.36067874639923
Epoch 9058/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.698005199432373
Epoch: 9058, Batch Gradient Norm: 26.279656272853906
Epoch: 9058, Batch Gradient Norm after: 22.360676519320347
Epoch 9059/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6951541304588318
Epoch: 9059, Batch Gradient Norm: 27.544096981460612
Epoch: 9059, Batch Gradient Norm after: 22.360674394463327
Epoch 9060/10000, Prediction Accuracy = 60.31%, Loss = 0.6979427218437195
Epoch: 9060, Batch Gradient Norm: 26.281407890255704
Epoch: 9060, Batch Gradient Norm after: 22.360678762445726
Epoch 9061/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.695019519329071
Epoch: 9061, Batch Gradient Norm: 27.54924282995797
Epoch: 9061, Batch Gradient Norm after: 22.360678025384875
Epoch 9062/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6979576468467712
Epoch: 9062, Batch Gradient Norm: 26.277191314273423
Epoch: 9062, Batch Gradient Norm after: 22.360678145958662
Epoch 9063/10000, Prediction Accuracy = 60.342%, Loss = 0.6950937986373902
Epoch: 9063, Batch Gradient Norm: 27.547981839467802
Epoch: 9063, Batch Gradient Norm after: 22.36067665386456
Epoch 9064/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6981255769729614
Epoch: 9064, Batch Gradient Norm: 26.26036620973722
Epoch: 9064, Batch Gradient Norm after: 22.360675192105194
Epoch 9065/10000, Prediction Accuracy = 60.306%, Loss = 0.6951185584068298
Epoch: 9065, Batch Gradient Norm: 27.54424484688702
Epoch: 9065, Batch Gradient Norm after: 22.360677476957516
Epoch 9066/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.697982132434845
Epoch: 9066, Batch Gradient Norm: 26.256699266417943
Epoch: 9066, Batch Gradient Norm after: 22.360677431911608
Epoch 9067/10000, Prediction Accuracy = 60.28800000000001%, Loss = 0.6949100375175477
Epoch: 9067, Batch Gradient Norm: 27.544024122967652
Epoch: 9067, Batch Gradient Norm after: 22.36067579556013
Epoch 9068/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6977344989776612
Epoch: 9068, Batch Gradient Norm: 26.258925128875703
Epoch: 9068, Batch Gradient Norm after: 22.36067597331522
Epoch 9069/10000, Prediction Accuracy = 60.27%, Loss = 0.694843852519989
Epoch: 9069, Batch Gradient Norm: 27.541206183693063
Epoch: 9069, Batch Gradient Norm after: 22.36067467061202
Epoch 9070/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6977142453193664
Epoch: 9070, Batch Gradient Norm: 26.256668253534983
Epoch: 9070, Batch Gradient Norm after: 22.360676016277246
Epoch 9071/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.6948222160339356
Epoch: 9071, Batch Gradient Norm: 27.533051409005257
Epoch: 9071, Batch Gradient Norm after: 22.360673957897102
Epoch 9072/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6976494073867798
Epoch: 9072, Batch Gradient Norm: 26.24965587460374
Epoch: 9072, Batch Gradient Norm after: 22.360677306627874
Epoch 9073/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6946852922439575
Epoch: 9073, Batch Gradient Norm: 27.539918507983437
Epoch: 9073, Batch Gradient Norm after: 22.36067719454946
Epoch 9074/10000, Prediction Accuracy = 60.314%, Loss = 0.6976860642433167
Epoch: 9074, Batch Gradient Norm: 26.24086329764436
Epoch: 9074, Batch Gradient Norm after: 22.36067742507213
Epoch 9075/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6947631478309632
Epoch: 9075, Batch Gradient Norm: 27.53921204843022
Epoch: 9075, Batch Gradient Norm after: 22.360676412550525
Epoch 9076/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6978617548942566
Epoch: 9076, Batch Gradient Norm: 26.226127091912904
Epoch: 9076, Batch Gradient Norm after: 22.360677789962846
Epoch 9077/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6947629451751709
Epoch: 9077, Batch Gradient Norm: 27.534021320210744
Epoch: 9077, Batch Gradient Norm after: 22.360677666956526
Epoch 9078/10000, Prediction Accuracy = 60.33%, Loss = 0.6976783037185669
Epoch: 9078, Batch Gradient Norm: 26.21925091492106
Epoch: 9078, Batch Gradient Norm after: 22.36067646942695
Epoch 9079/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6945470094680786
Epoch: 9079, Batch Gradient Norm: 27.538679518839086
Epoch: 9079, Batch Gradient Norm after: 22.36067463181833
Epoch 9080/10000, Prediction Accuracy = 60.324%, Loss = 0.6974523186683654
Epoch: 9080, Batch Gradient Norm: 26.22438307238206
Epoch: 9080, Batch Gradient Norm after: 22.360677186560697
Epoch 9081/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6945147037506103
Epoch: 9081, Batch Gradient Norm: 27.53140425751149
Epoch: 9081, Batch Gradient Norm after: 22.360676175243384
Epoch 9082/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6974428653717041
Epoch: 9082, Batch Gradient Norm: 26.22137175782392
Epoch: 9082, Batch Gradient Norm after: 22.360676513890947
Epoch 9083/10000, Prediction Accuracy = 60.258%, Loss = 0.694467294216156
Epoch: 9083, Batch Gradient Norm: 27.526608364443263
Epoch: 9083, Batch Gradient Norm after: 22.360676508015768
Epoch 9084/10000, Prediction Accuracy = 60.334%, Loss = 0.6973654627799988
Epoch: 9084, Batch Gradient Norm: 26.217875565799464
Epoch: 9084, Batch Gradient Norm after: 22.360678786581126
Epoch 9085/10000, Prediction Accuracy = 60.27%, Loss = 0.6943596482276917
Epoch: 9085, Batch Gradient Norm: 27.5310641706881
Epoch: 9085, Batch Gradient Norm after: 22.360676645579282
Epoch 9086/10000, Prediction Accuracy = 60.312%, Loss = 0.6974162459373474
Epoch: 9086, Batch Gradient Norm: 26.21148009320198
Epoch: 9086, Batch Gradient Norm after: 22.360675619535414
Epoch 9087/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6944509625434876
Epoch: 9087, Batch Gradient Norm: 27.525766910463997
Epoch: 9087, Batch Gradient Norm after: 22.360677345850146
Epoch 9088/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6975582957267761
Epoch: 9088, Batch Gradient Norm: 26.202292611635592
Epoch: 9088, Batch Gradient Norm after: 22.360675388207223
Epoch 9089/10000, Prediction Accuracy = 60.322%, Loss = 0.6944187641143799
Epoch: 9089, Batch Gradient Norm: 27.521726667133798
Epoch: 9089, Batch Gradient Norm after: 22.360678694041578
Epoch 9090/10000, Prediction Accuracy = 60.326%, Loss = 0.6973599553108215
Epoch: 9090, Batch Gradient Norm: 26.199351469181398
Epoch: 9090, Batch Gradient Norm after: 22.36067688405559
Epoch 9091/10000, Prediction Accuracy = 60.278%, Loss = 0.6942286610603332
Epoch: 9091, Batch Gradient Norm: 27.523573787492527
Epoch: 9091, Batch Gradient Norm after: 22.360676446905725
Epoch 9092/10000, Prediction Accuracy = 60.324%, Loss = 0.6971499443054199
Epoch: 9092, Batch Gradient Norm: 26.208733214648856
Epoch: 9092, Batch Gradient Norm after: 22.360676481084706
Epoch 9093/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.694215989112854
Epoch: 9093, Batch Gradient Norm: 27.513948200225204
Epoch: 9093, Batch Gradient Norm after: 22.360676276662733
Epoch 9094/10000, Prediction Accuracy = 60.342%, Loss = 0.6971413135528565
Epoch: 9094, Batch Gradient Norm: 26.20536942779211
Epoch: 9094, Batch Gradient Norm after: 22.360677380013705
Epoch 9095/10000, Prediction Accuracy = 60.262%, Loss = 0.6941759705543518
Epoch: 9095, Batch Gradient Norm: 27.50641073921619
Epoch: 9095, Batch Gradient Norm after: 22.360676020206153
Epoch 9096/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6970572113990784
Epoch: 9096, Batch Gradient Norm: 26.20296475971485
Epoch: 9096, Batch Gradient Norm after: 22.36067673569649
Epoch 9097/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.694055712223053
Epoch: 9097, Batch Gradient Norm: 27.512763385034784
Epoch: 9097, Batch Gradient Norm after: 22.360673299466523
Epoch 9098/10000, Prediction Accuracy = 60.31%, Loss = 0.6971262335777283
Epoch: 9098, Batch Gradient Norm: 26.201433352087918
Epoch: 9098, Batch Gradient Norm after: 22.360675575234254
Epoch 9099/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6941743493080139
Epoch: 9099, Batch Gradient Norm: 27.507372467775497
Epoch: 9099, Batch Gradient Norm after: 22.360676958031657
Epoch 9100/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6972805857658386
Epoch: 9100, Batch Gradient Norm: 26.188281110662235
Epoch: 9100, Batch Gradient Norm after: 22.360677405178834
Epoch 9101/10000, Prediction Accuracy = 60.32800000000001%, Loss = 0.6941397905349731
Epoch: 9101, Batch Gradient Norm: 27.503639744558637
Epoch: 9101, Batch Gradient Norm after: 22.3606782685506
Epoch 9102/10000, Prediction Accuracy = 60.334%, Loss = 0.697055721282959
Epoch: 9102, Batch Gradient Norm: 26.185687190691446
Epoch: 9102, Batch Gradient Norm after: 22.360679047430967
Epoch 9103/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6939221143722534
Epoch: 9103, Batch Gradient Norm: 27.507239062518998
Epoch: 9103, Batch Gradient Norm after: 22.360678562651632
Epoch 9104/10000, Prediction Accuracy = 60.334%, Loss = 0.6968490004539489
Epoch: 9104, Batch Gradient Norm: 26.19217025957385
Epoch: 9104, Batch Gradient Norm after: 22.360679028217312
Epoch 9105/10000, Prediction Accuracy = 60.238%, Loss = 0.6939237236976623
Epoch: 9105, Batch Gradient Norm: 27.498144718510613
Epoch: 9105, Batch Gradient Norm after: 22.36067518911394
Epoch 9106/10000, Prediction Accuracy = 60.324%, Loss = 0.6968532800674438
Epoch: 9106, Batch Gradient Norm: 26.183984563171702
Epoch: 9106, Batch Gradient Norm after: 22.360677607469025
Epoch 9107/10000, Prediction Accuracy = 60.274%, Loss = 0.6938411355018616
Epoch: 9107, Batch Gradient Norm: 27.49304665677101
Epoch: 9107, Batch Gradient Norm after: 22.36067634058862
Epoch 9108/10000, Prediction Accuracy = 60.354%, Loss = 0.6967756509780884
Epoch: 9108, Batch Gradient Norm: 26.17929452106615
Epoch: 9108, Batch Gradient Norm after: 22.360675795906076
Epoch 9109/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6937339901924133
Epoch: 9109, Batch Gradient Norm: 27.5018201238613
Epoch: 9109, Batch Gradient Norm after: 22.36067663315579
Epoch 9110/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6968597292900085
Epoch: 9110, Batch Gradient Norm: 26.172262359524066
Epoch: 9110, Batch Gradient Norm after: 22.360677065260106
Epoch 9111/10000, Prediction Accuracy = 60.324%, Loss = 0.6938438057899475
Epoch: 9111, Batch Gradient Norm: 27.497800842303345
Epoch: 9111, Batch Gradient Norm after: 22.360676435880727
Epoch 9112/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6969783306121826
Epoch: 9112, Batch Gradient Norm: 26.15289571459555
Epoch: 9112, Batch Gradient Norm after: 22.36067442539221
Epoch 9113/10000, Prediction Accuracy = 60.312%, Loss = 0.6937693238258362
Epoch: 9113, Batch Gradient Norm: 27.49590282595514
Epoch: 9113, Batch Gradient Norm after: 22.360677560753043
Epoch 9114/10000, Prediction Accuracy = 60.342%, Loss = 0.6967543125152588
Epoch: 9114, Batch Gradient Norm: 26.15150683202451
Epoch: 9114, Batch Gradient Norm after: 22.360680523797985
Epoch 9115/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6935774207115173
Epoch: 9115, Batch Gradient Norm: 27.500326755442455
Epoch: 9115, Batch Gradient Norm after: 22.360676578741913
Epoch 9116/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6965728282928467
Epoch: 9116, Batch Gradient Norm: 26.156051370330513
Epoch: 9116, Batch Gradient Norm after: 22.360676086568912
Epoch 9117/10000, Prediction Accuracy = 60.25%, Loss = 0.6935743927955628
Epoch: 9117, Batch Gradient Norm: 27.49269364719297
Epoch: 9117, Batch Gradient Norm after: 22.360674859962728
Epoch 9118/10000, Prediction Accuracy = 60.324%, Loss = 0.6965820908546447
Epoch: 9118, Batch Gradient Norm: 26.146862446696932
Epoch: 9118, Batch Gradient Norm after: 22.36067882326572
Epoch 9119/10000, Prediction Accuracy = 60.262%, Loss = 0.693496310710907
Epoch: 9119, Batch Gradient Norm: 27.486710732179542
Epoch: 9119, Batch Gradient Norm after: 22.360676930761663
Epoch 9120/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6965029239654541
Epoch: 9120, Batch Gradient Norm: 26.147684306302146
Epoch: 9120, Batch Gradient Norm after: 22.360676945389045
Epoch 9121/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6933965921401978
Epoch: 9121, Batch Gradient Norm: 27.495194916816615
Epoch: 9121, Batch Gradient Norm after: 22.360676260377282
Epoch 9122/10000, Prediction Accuracy = 60.31%, Loss = 0.6965837240219116
Epoch: 9122, Batch Gradient Norm: 26.141124276423305
Epoch: 9122, Batch Gradient Norm after: 22.360673758304564
Epoch 9123/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6935198903083801
Epoch: 9123, Batch Gradient Norm: 27.487688394225334
Epoch: 9123, Batch Gradient Norm after: 22.360676825623198
Epoch 9124/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6967194437980652
Epoch: 9124, Batch Gradient Norm: 26.12257238992829
Epoch: 9124, Batch Gradient Norm after: 22.360676594028046
Epoch 9125/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.693438458442688
Epoch: 9125, Batch Gradient Norm: 27.486761770819097
Epoch: 9125, Batch Gradient Norm after: 22.36067900092762
Epoch 9126/10000, Prediction Accuracy = 60.342%, Loss = 0.6964799761772156
Epoch: 9126, Batch Gradient Norm: 26.120488738435082
Epoch: 9126, Batch Gradient Norm after: 22.360678154996876
Epoch 9127/10000, Prediction Accuracy = 60.278%, Loss = 0.6932371020317077
Epoch: 9127, Batch Gradient Norm: 27.491110769289744
Epoch: 9127, Batch Gradient Norm after: 22.360677311856218
Epoch 9128/10000, Prediction Accuracy = 60.352%, Loss = 0.6962962746620178
Epoch: 9128, Batch Gradient Norm: 26.12434844077041
Epoch: 9128, Batch Gradient Norm after: 22.36067613959231
Epoch 9129/10000, Prediction Accuracy = 60.24000000000001%, Loss = 0.6932542562484741
Epoch: 9129, Batch Gradient Norm: 27.481353411568683
Epoch: 9129, Batch Gradient Norm after: 22.360676650613645
Epoch 9130/10000, Prediction Accuracy = 60.322%, Loss = 0.696300995349884
Epoch: 9130, Batch Gradient Norm: 26.11773322204518
Epoch: 9130, Batch Gradient Norm after: 22.360678824556697
Epoch 9131/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6931562662124634
Epoch: 9131, Batch Gradient Norm: 27.474559285925203
Epoch: 9131, Batch Gradient Norm after: 22.360678187921813
Epoch 9132/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6962180733680725
Epoch: 9132, Batch Gradient Norm: 26.118491863190528
Epoch: 9132, Batch Gradient Norm after: 22.360679401204592
Epoch 9133/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6930670857429504
Epoch: 9133, Batch Gradient Norm: 27.481990233206442
Epoch: 9133, Batch Gradient Norm after: 22.360675659018277
Epoch 9134/10000, Prediction Accuracy = 60.298%, Loss = 0.6963106632232666
Epoch: 9134, Batch Gradient Norm: 26.11403015332301
Epoch: 9134, Batch Gradient Norm after: 22.360676134309042
Epoch 9135/10000, Prediction Accuracy = 60.326%, Loss = 0.6931833863258362
Epoch: 9135, Batch Gradient Norm: 27.474205343992534
Epoch: 9135, Batch Gradient Norm after: 22.360675753597036
Epoch 9136/10000, Prediction Accuracy = 60.324%, Loss = 0.6964154720306397
Epoch: 9136, Batch Gradient Norm: 26.099792129819274
Epoch: 9136, Batch Gradient Norm after: 22.360677186425235
Epoch 9137/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6931016445159912
Epoch: 9137, Batch Gradient Norm: 27.472702145140936
Epoch: 9137, Batch Gradient Norm after: 22.360678049079702
Epoch 9138/10000, Prediction Accuracy = 60.35%, Loss = 0.6961823105812073
Epoch: 9138, Batch Gradient Norm: 26.100095168629654
Epoch: 9138, Batch Gradient Norm after: 22.36067792169516
Epoch 9139/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6929246425628662
Epoch: 9139, Batch Gradient Norm: 27.475047100341623
Epoch: 9139, Batch Gradient Norm after: 22.3606790682254
Epoch 9140/10000, Prediction Accuracy = 60.348%, Loss = 0.6959982633590698
Epoch: 9140, Batch Gradient Norm: 26.10823389653814
Epoch: 9140, Batch Gradient Norm after: 22.360676421034363
Epoch 9141/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6929433584213257
Epoch: 9141, Batch Gradient Norm: 27.464000086260107
Epoch: 9141, Batch Gradient Norm after: 22.360676372841645
Epoch 9142/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6959999918937683
Epoch: 9142, Batch Gradient Norm: 26.104451199477626
Epoch: 9142, Batch Gradient Norm after: 22.360677841776173
Epoch 9143/10000, Prediction Accuracy = 60.258%, Loss = 0.6928648352622986
Epoch: 9143, Batch Gradient Norm: 27.454864810451795
Epoch: 9143, Batch Gradient Norm after: 22.36067794917378
Epoch 9144/10000, Prediction Accuracy = 60.348%, Loss = 0.6959098219871521
Epoch: 9144, Batch Gradient Norm: 26.111179064866622
Epoch: 9144, Batch Gradient Norm after: 22.36067788938664
Epoch 9145/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6927809715270996
Epoch: 9145, Batch Gradient Norm: 27.463059644795074
Epoch: 9145, Batch Gradient Norm after: 22.3606758299798
Epoch 9146/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6959964036941528
Epoch: 9146, Batch Gradient Norm: 26.10565531251236
Epoch: 9146, Batch Gradient Norm after: 22.36067665034734
Epoch 9147/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.6929139971733094
Epoch: 9147, Batch Gradient Norm: 27.45637527569808
Epoch: 9147, Batch Gradient Norm after: 22.360676503943928
Epoch 9148/10000, Prediction Accuracy = 60.322%, Loss = 0.6961227655410767
Epoch: 9148, Batch Gradient Norm: 26.0929452490913
Epoch: 9148, Batch Gradient Norm after: 22.360675232448784
Epoch 9149/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.692835533618927
Epoch: 9149, Batch Gradient Norm: 27.452280049943816
Epoch: 9149, Batch Gradient Norm after: 22.3606767922374
Epoch 9150/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6958634257316589
Epoch: 9150, Batch Gradient Norm: 26.097052547575153
Epoch: 9150, Batch Gradient Norm after: 22.360677814921093
Epoch 9151/10000, Prediction Accuracy = 60.294%, Loss = 0.6926500916481018
Epoch: 9151, Batch Gradient Norm: 27.454398543874277
Epoch: 9151, Batch Gradient Norm after: 22.360677119272935
Epoch 9152/10000, Prediction Accuracy = 60.366%, Loss = 0.6956886053085327
Epoch: 9152, Batch Gradient Norm: 26.10501467972623
Epoch: 9152, Batch Gradient Norm after: 22.360675381733163
Epoch 9153/10000, Prediction Accuracy = 60.245999999999995%, Loss = 0.6926665782928467
Epoch: 9153, Batch Gradient Norm: 27.442172665126837
Epoch: 9153, Batch Gradient Norm after: 22.360676448769468
Epoch 9154/10000, Prediction Accuracy = 60.334%, Loss = 0.6956894516944885
Epoch: 9154, Batch Gradient Norm: 26.099939212803676
Epoch: 9154, Batch Gradient Norm after: 22.360677867901657
Epoch 9155/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6925853490829468
Epoch: 9155, Batch Gradient Norm: 27.43868253989173
Epoch: 9155, Batch Gradient Norm after: 22.360676664320657
Epoch 9156/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.695600152015686
Epoch: 9156, Batch Gradient Norm: 26.101811267405004
Epoch: 9156, Batch Gradient Norm after: 22.360678718233753
Epoch 9157/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6925066947937012
Epoch: 9157, Batch Gradient Norm: 27.44171379783848
Epoch: 9157, Batch Gradient Norm after: 22.360676661704296
Epoch 9158/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.6956963539123535
Epoch: 9158, Batch Gradient Norm: 26.09895477501233
Epoch: 9158, Batch Gradient Norm after: 22.360676782640365
Epoch 9159/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.692632794380188
Epoch: 9159, Batch Gradient Norm: 27.43493908137234
Epoch: 9159, Batch Gradient Norm after: 22.360675259821814
Epoch 9160/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6958041787147522
Epoch: 9160, Batch Gradient Norm: 26.083932063715263
Epoch: 9160, Batch Gradient Norm after: 22.360676415526626
Epoch 9161/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6925447702407836
Epoch: 9161, Batch Gradient Norm: 27.430600564663987
Epoch: 9161, Batch Gradient Norm after: 22.360674749946032
Epoch 9162/10000, Prediction Accuracy = 60.354%, Loss = 0.6955520868301391
Epoch: 9162, Batch Gradient Norm: 26.08550319154279
Epoch: 9162, Batch Gradient Norm after: 22.36068165312032
Epoch 9163/10000, Prediction Accuracy = 60.294%, Loss = 0.6923561930656433
Epoch: 9163, Batch Gradient Norm: 27.43709109887428
Epoch: 9163, Batch Gradient Norm after: 22.36067821137044
Epoch 9164/10000, Prediction Accuracy = 60.354%, Loss = 0.6953782916069031
Epoch: 9164, Batch Gradient Norm: 26.09242134199994
Epoch: 9164, Batch Gradient Norm after: 22.36067653454138
Epoch 9165/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6923831701278687
Epoch: 9165, Batch Gradient Norm: 27.424862636036483
Epoch: 9165, Batch Gradient Norm after: 22.360676014175272
Epoch 9166/10000, Prediction Accuracy = 60.336%, Loss = 0.6953859686851501
Epoch: 9166, Batch Gradient Norm: 26.08519675573365
Epoch: 9166, Batch Gradient Norm after: 22.360676548816357
Epoch 9167/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.6923011898994446
Epoch: 9167, Batch Gradient Norm: 27.420087166241352
Epoch: 9167, Batch Gradient Norm after: 22.360674777826755
Epoch 9168/10000, Prediction Accuracy = 60.334%, Loss = 0.6952946305274963
Epoch: 9168, Batch Gradient Norm: 26.08758294573752
Epoch: 9168, Batch Gradient Norm after: 22.360677837679557
Epoch 9169/10000, Prediction Accuracy = 60.288%, Loss = 0.6922108888626098
Epoch: 9169, Batch Gradient Norm: 27.4234842935613
Epoch: 9169, Batch Gradient Norm after: 22.36067535514405
Epoch 9170/10000, Prediction Accuracy = 60.3%, Loss = 0.695393693447113
Epoch: 9170, Batch Gradient Norm: 26.084199544959066
Epoch: 9170, Batch Gradient Norm after: 22.360675098409914
Epoch 9171/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6923421144485473
Epoch: 9171, Batch Gradient Norm: 27.417500725542382
Epoch: 9171, Batch Gradient Norm after: 22.360676532298868
Epoch 9172/10000, Prediction Accuracy = 60.324%, Loss = 0.6955142259597779
Epoch: 9172, Batch Gradient Norm: 26.070357176012795
Epoch: 9172, Batch Gradient Norm after: 22.36067749586369
Epoch 9173/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6922621369361878
Epoch: 9173, Batch Gradient Norm: 27.412650347743664
Epoch: 9173, Batch Gradient Norm after: 22.36067750767729
Epoch 9174/10000, Prediction Accuracy = 60.348%, Loss = 0.6952548861503601
Epoch: 9174, Batch Gradient Norm: 26.07562814649153
Epoch: 9174, Batch Gradient Norm after: 22.360677338111756
Epoch 9175/10000, Prediction Accuracy = 60.29%, Loss = 0.6920830607414246
Epoch: 9175, Batch Gradient Norm: 27.417343458812585
Epoch: 9175, Batch Gradient Norm after: 22.360675822032032
Epoch 9176/10000, Prediction Accuracy = 60.35%, Loss = 0.6950667858123779
Epoch: 9176, Batch Gradient Norm: 26.082199424564926
Epoch: 9176, Batch Gradient Norm after: 22.360677913315484
Epoch 9177/10000, Prediction Accuracy = 60.254%, Loss = 0.6920986652374268
Epoch: 9177, Batch Gradient Norm: 27.406751120076883
Epoch: 9177, Batch Gradient Norm after: 22.36067462897418
Epoch 9178/10000, Prediction Accuracy = 60.338%, Loss = 0.6950745105743408
Epoch: 9178, Batch Gradient Norm: 26.077967780247466
Epoch: 9178, Batch Gradient Norm after: 22.36067882430948
Epoch 9179/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6920198202133179
Epoch: 9179, Batch Gradient Norm: 27.398622865450154
Epoch: 9179, Batch Gradient Norm after: 22.360677889316417
Epoch 9180/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.694985294342041
Epoch: 9180, Batch Gradient Norm: 26.076944226584057
Epoch: 9180, Batch Gradient Norm after: 22.36067792796773
Epoch 9181/10000, Prediction Accuracy = 60.302%, Loss = 0.6919310212135314
Epoch: 9181, Batch Gradient Norm: 27.40395541211213
Epoch: 9181, Batch Gradient Norm after: 22.36067704040604
Epoch 9182/10000, Prediction Accuracy = 60.29%, Loss = 0.6950779557228088
Epoch: 9182, Batch Gradient Norm: 26.07518785064634
Epoch: 9182, Batch Gradient Norm after: 22.36067706335918
Epoch 9183/10000, Prediction Accuracy = 60.338%, Loss = 0.6920571208000184
Epoch: 9183, Batch Gradient Norm: 27.398974987990744
Epoch: 9183, Batch Gradient Norm after: 22.360675573628274
Epoch 9184/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6952053904533386
Epoch: 9184, Batch Gradient Norm: 26.06223914532788
Epoch: 9184, Batch Gradient Norm after: 22.360673706099796
Epoch 9185/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6919825315475464
Epoch: 9185, Batch Gradient Norm: 27.395290104685756
Epoch: 9185, Batch Gradient Norm after: 22.360678168401016
Epoch 9186/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6949476003646851
Epoch: 9186, Batch Gradient Norm: 26.065683648322587
Epoch: 9186, Batch Gradient Norm after: 22.360681136196828
Epoch 9187/10000, Prediction Accuracy = 60.290000000000006%, Loss = 0.6917991280555725
Epoch: 9187, Batch Gradient Norm: 27.396920805606808
Epoch: 9187, Batch Gradient Norm after: 22.36067659819259
Epoch 9188/10000, Prediction Accuracy = 60.342%, Loss = 0.6947652220726013
Epoch: 9188, Batch Gradient Norm: 26.070500556792247
Epoch: 9188, Batch Gradient Norm after: 22.36067777976191
Epoch 9189/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6918177962303161
Epoch: 9189, Batch Gradient Norm: 27.388914728275207
Epoch: 9189, Batch Gradient Norm after: 22.3606787522105
Epoch 9190/10000, Prediction Accuracy = 60.342%, Loss = 0.694763445854187
Epoch: 9190, Batch Gradient Norm: 26.070763236748746
Epoch: 9190, Batch Gradient Norm after: 22.360677748001585
Epoch 9191/10000, Prediction Accuracy = 60.258%, Loss = 0.6917532324790955
Epoch: 9191, Batch Gradient Norm: 27.37937277798721
Epoch: 9191, Batch Gradient Norm after: 22.360676401628023
Epoch 9192/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6946786999702453
Epoch: 9192, Batch Gradient Norm: 26.07406707160664
Epoch: 9192, Batch Gradient Norm after: 22.36067726898156
Epoch 9193/10000, Prediction Accuracy = 60.298%, Loss = 0.6916545629501343
Epoch: 9193, Batch Gradient Norm: 27.382962940000205
Epoch: 9193, Batch Gradient Norm after: 22.36067528104664
Epoch 9194/10000, Prediction Accuracy = 60.29%, Loss = 0.6947676658630371
Epoch: 9194, Batch Gradient Norm: 26.06410510369927
Epoch: 9194, Batch Gradient Norm after: 22.360676073353368
Epoch 9195/10000, Prediction Accuracy = 60.338%, Loss = 0.6917892336845398
Epoch: 9195, Batch Gradient Norm: 27.378191215174656
Epoch: 9195, Batch Gradient Norm after: 22.36067763879374
Epoch 9196/10000, Prediction Accuracy = 60.326%, Loss = 0.6949139595031738
Epoch: 9196, Batch Gradient Norm: 26.053011027551545
Epoch: 9196, Batch Gradient Norm after: 22.360677772599466
Epoch 9197/10000, Prediction Accuracy = 60.336%, Loss = 0.6917184114456176
Epoch: 9197, Batch Gradient Norm: 27.373320289648117
Epoch: 9197, Batch Gradient Norm after: 22.360678043744613
Epoch 9198/10000, Prediction Accuracy = 60.352%, Loss = 0.6946484327316285
Epoch: 9198, Batch Gradient Norm: 26.055880282457093
Epoch: 9198, Batch Gradient Norm after: 22.36067820278448
Epoch 9199/10000, Prediction Accuracy = 60.278%, Loss = 0.6915192842483521
Epoch: 9199, Batch Gradient Norm: 27.379234831032445
Epoch: 9199, Batch Gradient Norm after: 22.36067822518502
Epoch 9200/10000, Prediction Accuracy = 60.334%, Loss = 0.6944562554359436
Epoch: 9200, Batch Gradient Norm: 26.059723253853754
Epoch: 9200, Batch Gradient Norm after: 22.360676944240545
Epoch 9201/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.6915385365486145
Epoch: 9201, Batch Gradient Norm: 27.368183635360865
Epoch: 9201, Batch Gradient Norm after: 22.360679310311554
Epoch 9202/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6944665431976318
Epoch: 9202, Batch Gradient Norm: 26.057060893658324
Epoch: 9202, Batch Gradient Norm after: 22.36067634189716
Epoch 9203/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6914570450782775
Epoch: 9203, Batch Gradient Norm: 27.362566735124325
Epoch: 9203, Batch Gradient Norm after: 22.360676094542455
Epoch 9204/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6943678379058837
Epoch: 9204, Batch Gradient Norm: 26.05502329746936
Epoch: 9204, Batch Gradient Norm after: 22.36067909039883
Epoch 9205/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.6913645267486572
Epoch: 9205, Batch Gradient Norm: 27.369063172532776
Epoch: 9205, Batch Gradient Norm after: 22.360677275425306
Epoch 9206/10000, Prediction Accuracy = 60.298%, Loss = 0.6944696187973023
Epoch: 9206, Batch Gradient Norm: 26.0457859106027
Epoch: 9206, Batch Gradient Norm after: 22.360675705266434
Epoch 9207/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6914715766906738
Epoch: 9207, Batch Gradient Norm: 27.363247494704467
Epoch: 9207, Batch Gradient Norm after: 22.360676577262936
Epoch 9208/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6946015000343323
Epoch: 9208, Batch Gradient Norm: 26.03560513450449
Epoch: 9208, Batch Gradient Norm after: 22.36067741630553
Epoch 9209/10000, Prediction Accuracy = 60.35%, Loss = 0.6914044618606567
Epoch: 9209, Batch Gradient Norm: 27.35975049687235
Epoch: 9209, Batch Gradient Norm after: 22.360678598527112
Epoch 9210/10000, Prediction Accuracy = 60.35%, Loss = 0.6943472146987915
Epoch: 9210, Batch Gradient Norm: 26.03227105980178
Epoch: 9210, Batch Gradient Norm after: 22.360677948547426
Epoch 9211/10000, Prediction Accuracy = 60.278%, Loss = 0.6912135124206543
Epoch: 9211, Batch Gradient Norm: 27.36556547643786
Epoch: 9211, Batch Gradient Norm after: 22.360676343857325
Epoch 9212/10000, Prediction Accuracy = 60.326%, Loss = 0.6941687941551209
Epoch: 9212, Batch Gradient Norm: 26.039503156449314
Epoch: 9212, Batch Gradient Norm after: 22.360678320913415
Epoch 9213/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6912224411964416
Epoch: 9213, Batch Gradient Norm: 27.35524451750598
Epoch: 9213, Batch Gradient Norm after: 22.360677709058717
Epoch 9214/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6941786527633667
Epoch: 9214, Batch Gradient Norm: 26.03142228337989
Epoch: 9214, Batch Gradient Norm after: 22.36067683151096
Epoch 9215/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.6911349892616272
Epoch: 9215, Batch Gradient Norm: 27.351347295633204
Epoch: 9215, Batch Gradient Norm after: 22.360676405712997
Epoch 9216/10000, Prediction Accuracy = 60.324%, Loss = 0.6940950632095337
Epoch: 9216, Batch Gradient Norm: 26.029393567727904
Epoch: 9216, Batch Gradient Norm after: 22.360678744866235
Epoch 9217/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6910367012023926
Epoch: 9217, Batch Gradient Norm: 27.35723161681139
Epoch: 9217, Batch Gradient Norm after: 22.360678360134333
Epoch 9218/10000, Prediction Accuracy = 60.290000000000006%, Loss = 0.6941741585731507
Epoch: 9218, Batch Gradient Norm: 26.026181241119023
Epoch: 9218, Batch Gradient Norm after: 22.36067759339319
Epoch 9219/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6911658525466919
Epoch: 9219, Batch Gradient Norm: 27.349258235567845
Epoch: 9219, Batch Gradient Norm after: 22.360677746012932
Epoch 9220/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6943357706069946
Epoch: 9220, Batch Gradient Norm: 26.01162210440341
Epoch: 9220, Batch Gradient Norm after: 22.360677220779298
Epoch 9221/10000, Prediction Accuracy = 60.338%, Loss = 0.6911156654357911
Epoch: 9221, Batch Gradient Norm: 27.346890632544604
Epoch: 9221, Batch Gradient Norm after: 22.360677194701378
Epoch 9222/10000, Prediction Accuracy = 60.348%, Loss = 0.6940678477287292
Epoch: 9222, Batch Gradient Norm: 26.017684999501483
Epoch: 9222, Batch Gradient Norm after: 22.360678156288383
Epoch 9223/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6909098029136658
Epoch: 9223, Batch Gradient Norm: 27.3486972273099
Epoch: 9223, Batch Gradient Norm after: 22.360676590583367
Epoch 9224/10000, Prediction Accuracy = 60.318%, Loss = 0.693866765499115
Epoch: 9224, Batch Gradient Norm: 26.025891000059755
Epoch: 9224, Batch Gradient Norm after: 22.360676134211868
Epoch 9225/10000, Prediction Accuracy = 60.246%, Loss = 0.6909306287765503
Epoch: 9225, Batch Gradient Norm: 27.33497187636218
Epoch: 9225, Batch Gradient Norm after: 22.360675616954985
Epoch 9226/10000, Prediction Accuracy = 60.35%, Loss = 0.693874990940094
Epoch: 9226, Batch Gradient Norm: 26.030006769869082
Epoch: 9226, Batch Gradient Norm after: 22.36067698108627
Epoch 9227/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6908665657043457
Epoch: 9227, Batch Gradient Norm: 27.325694555449598
Epoch: 9227, Batch Gradient Norm after: 22.36067625315739
Epoch 9228/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6937702775001526
Epoch: 9228, Batch Gradient Norm: 26.03687400061594
Epoch: 9228, Batch Gradient Norm after: 22.36067699126866
Epoch 9229/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6907890677452088
Epoch: 9229, Batch Gradient Norm: 27.332003362232374
Epoch: 9229, Batch Gradient Norm after: 22.36067566700245
Epoch 9230/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6938432097434998
Epoch: 9230, Batch Gradient Norm: 26.037327012976984
Epoch: 9230, Batch Gradient Norm after: 22.360676893751826
Epoch 9231/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.690927517414093
Epoch: 9231, Batch Gradient Norm: 27.32047774180989
Epoch: 9231, Batch Gradient Norm after: 22.360676238462215
Epoch 9232/10000, Prediction Accuracy = 60.338%, Loss = 0.6939756870269775
Epoch: 9232, Batch Gradient Norm: 26.02879230230178
Epoch: 9232, Batch Gradient Norm after: 22.360678135786603
Epoch 9233/10000, Prediction Accuracy = 60.354%, Loss = 0.6908748745918274
Epoch: 9233, Batch Gradient Norm: 27.315165303263917
Epoch: 9233, Batch Gradient Norm after: 22.36067680471614
Epoch 9234/10000, Prediction Accuracy = 60.348%, Loss = 0.693721342086792
Epoch: 9234, Batch Gradient Norm: 26.035161116395876
Epoch: 9234, Batch Gradient Norm after: 22.360677974343893
Epoch 9235/10000, Prediction Accuracy = 60.29%, Loss = 0.6906953096389771
Epoch: 9235, Batch Gradient Norm: 27.316328645361033
Epoch: 9235, Batch Gradient Norm after: 22.36067521827752
Epoch 9236/10000, Prediction Accuracy = 60.33%, Loss = 0.6935254454612731
Epoch: 9236, Batch Gradient Norm: 26.04224114945171
Epoch: 9236, Batch Gradient Norm after: 22.360676909799977
Epoch 9237/10000, Prediction Accuracy = 60.246%, Loss = 0.6907120466232299
Epoch: 9237, Batch Gradient Norm: 27.30672201407354
Epoch: 9237, Batch Gradient Norm after: 22.360680444720042
Epoch 9238/10000, Prediction Accuracy = 60.362%, Loss = 0.6935344219207764
Epoch: 9238, Batch Gradient Norm: 26.042685687715082
Epoch: 9238, Batch Gradient Norm after: 22.360677327617204
Epoch 9239/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6906435370445252
Epoch: 9239, Batch Gradient Norm: 27.29637187608627
Epoch: 9239, Batch Gradient Norm after: 22.36067685943358
Epoch 9240/10000, Prediction Accuracy = 60.324%, Loss = 0.6934364795684814
Epoch: 9240, Batch Gradient Norm: 26.046730430729113
Epoch: 9240, Batch Gradient Norm after: 22.360676803159315
Epoch 9241/10000, Prediction Accuracy = 60.294%, Loss = 0.6905619382858277
Epoch: 9241, Batch Gradient Norm: 27.300945713624415
Epoch: 9241, Batch Gradient Norm after: 22.360675173707467
Epoch 9242/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6935208439826965
Epoch: 9242, Batch Gradient Norm: 26.042648276105968
Epoch: 9242, Batch Gradient Norm after: 22.360676602359874
Epoch 9243/10000, Prediction Accuracy = 60.338%, Loss = 0.6907034993171692
Epoch: 9243, Batch Gradient Norm: 27.292930933351315
Epoch: 9243, Batch Gradient Norm after: 22.360676831149785
Epoch 9244/10000, Prediction Accuracy = 60.336%, Loss = 0.693667197227478
Epoch: 9244, Batch Gradient Norm: 26.036063342726234
Epoch: 9244, Batch Gradient Norm after: 22.360677969377914
Epoch 9245/10000, Prediction Accuracy = 60.354%, Loss = 0.6906435370445252
Epoch: 9245, Batch Gradient Norm: 27.286916740365452
Epoch: 9245, Batch Gradient Norm after: 22.360676261749866
Epoch 9246/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6933898329734802
Epoch: 9246, Batch Gradient Norm: 26.039521063011144
Epoch: 9246, Batch Gradient Norm after: 22.36067768921259
Epoch 9247/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.6904463529586792
Epoch: 9247, Batch Gradient Norm: 27.29014988795136
Epoch: 9247, Batch Gradient Norm after: 22.36067578981141
Epoch 9248/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6932072281837464
Epoch: 9248, Batch Gradient Norm: 26.043519590014142
Epoch: 9248, Batch Gradient Norm after: 22.360676751858698
Epoch 9249/10000, Prediction Accuracy = 60.25%, Loss = 0.6904739260673523
Epoch: 9249, Batch Gradient Norm: 27.2810271043403
Epoch: 9249, Batch Gradient Norm after: 22.360679384352938
Epoch 9250/10000, Prediction Accuracy = 60.362%, Loss = 0.693216872215271
Epoch: 9250, Batch Gradient Norm: 26.033669579026146
Epoch: 9250, Batch Gradient Norm after: 22.360678268752224
Epoch 9251/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.6903772473335266
Epoch: 9251, Batch Gradient Norm: 27.27903988182171
Epoch: 9251, Batch Gradient Norm after: 22.360675447549642
Epoch 9252/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6931318998336792
Epoch: 9252, Batch Gradient Norm: 26.03357613804957
Epoch: 9252, Batch Gradient Norm after: 22.36067753611991
Epoch 9253/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6902857780456543
Epoch: 9253, Batch Gradient Norm: 27.28369811975776
Epoch: 9253, Batch Gradient Norm after: 22.360675773022425
Epoch 9254/10000, Prediction Accuracy = 60.278%, Loss = 0.6932326078414917
Epoch: 9254, Batch Gradient Norm: 26.026157589049333
Epoch: 9254, Batch Gradient Norm after: 22.36067549462279
Epoch 9255/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6904054284095764
Epoch: 9255, Batch Gradient Norm: 27.275625328330406
Epoch: 9255, Batch Gradient Norm after: 22.36067748093989
Epoch 9256/10000, Prediction Accuracy = 60.334%, Loss = 0.6933539271354675
Epoch: 9256, Batch Gradient Norm: 26.01081779871765
Epoch: 9256, Batch Gradient Norm after: 22.36067826469459
Epoch 9257/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6903146266937256
Epoch: 9257, Batch Gradient Norm: 27.273481831385865
Epoch: 9257, Batch Gradient Norm after: 22.360676476229514
Epoch 9258/10000, Prediction Accuracy = 60.326%, Loss = 0.693086040019989
Epoch: 9258, Batch Gradient Norm: 26.01207026623658
Epoch: 9258, Batch Gradient Norm after: 22.36067840029329
Epoch 9259/10000, Prediction Accuracy = 60.286%, Loss = 0.6901341199874877
Epoch: 9259, Batch Gradient Norm: 27.279490513058768
Epoch: 9259, Batch Gradient Norm after: 22.360677084530355
Epoch 9260/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6929231524467468
Epoch: 9260, Batch Gradient Norm: 26.01496933780324
Epoch: 9260, Batch Gradient Norm after: 22.36067830278914
Epoch 9261/10000, Prediction Accuracy = 60.25%, Loss = 0.6901629447937012
Epoch: 9261, Batch Gradient Norm: 27.26740503254945
Epoch: 9261, Batch Gradient Norm after: 22.36067868693228
Epoch 9262/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6929404973983765
Epoch: 9262, Batch Gradient Norm: 26.00970291577341
Epoch: 9262, Batch Gradient Norm after: 22.36067587055041
Epoch 9263/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6900457382202149
Epoch: 9263, Batch Gradient Norm: 27.264301371926116
Epoch: 9263, Batch Gradient Norm after: 22.360677678991696
Epoch 9264/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.692849588394165
Epoch: 9264, Batch Gradient Norm: 26.01054196751859
Epoch: 9264, Batch Gradient Norm after: 22.360680490597613
Epoch 9265/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6899709582328797
Epoch: 9265, Batch Gradient Norm: 27.268693168055528
Epoch: 9265, Batch Gradient Norm after: 22.360677864215912
Epoch 9266/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6929794073104858
Epoch: 9266, Batch Gradient Norm: 25.998850111588997
Epoch: 9266, Batch Gradient Norm after: 22.36067491216968
Epoch 9267/10000, Prediction Accuracy = 60.326%, Loss = 0.690105676651001
Epoch: 9267, Batch Gradient Norm: 27.26164770452721
Epoch: 9267, Batch Gradient Norm after: 22.360678665522865
Epoch 9268/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6930589795112609
Epoch: 9268, Batch Gradient Norm: 25.985083774899415
Epoch: 9268, Batch Gradient Norm after: 22.360676220484866
Epoch 9269/10000, Prediction Accuracy = 60.366%, Loss = 0.6899678230285644
Epoch: 9269, Batch Gradient Norm: 27.262964292125286
Epoch: 9269, Batch Gradient Norm after: 22.36067862877944
Epoch 9270/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.69278404712677
Epoch: 9270, Batch Gradient Norm: 25.98636942071198
Epoch: 9270, Batch Gradient Norm after: 22.36067786393223
Epoch 9271/10000, Prediction Accuracy = 60.30400000000001%, Loss = 0.6897953987121582
Epoch: 9271, Batch Gradient Norm: 27.264768754414142
Epoch: 9271, Batch Gradient Norm after: 22.360677406706586
Epoch 9272/10000, Prediction Accuracy = 60.33%, Loss = 0.6926490187644958
Epoch: 9272, Batch Gradient Norm: 25.984574766171132
Epoch: 9272, Batch Gradient Norm after: 22.360678553977486
Epoch 9273/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6898357987403869
Epoch: 9273, Batch Gradient Norm: 27.25369168782655
Epoch: 9273, Batch Gradient Norm after: 22.360677462844226
Epoch 9274/10000, Prediction Accuracy = 60.342%, Loss = 0.6926584720611573
Epoch: 9274, Batch Gradient Norm: 25.977623635436654
Epoch: 9274, Batch Gradient Norm after: 22.36067509913304
Epoch 9275/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6897092461585999
Epoch: 9275, Batch Gradient Norm: 27.254021723147705
Epoch: 9275, Batch Gradient Norm after: 22.360676783768277
Epoch 9276/10000, Prediction Accuracy = 60.326%, Loss = 0.6925788521766663
Epoch: 9276, Batch Gradient Norm: 25.97800641598608
Epoch: 9276, Batch Gradient Norm after: 22.360678238032776
Epoch 9277/10000, Prediction Accuracy = 60.352%, Loss = 0.6896467447280884
Epoch: 9277, Batch Gradient Norm: 27.25644111519389
Epoch: 9277, Batch Gradient Norm after: 22.360675981494097
Epoch 9278/10000, Prediction Accuracy = 60.306%, Loss = 0.6927202820777894
Epoch: 9278, Batch Gradient Norm: 25.965893823154957
Epoch: 9278, Batch Gradient Norm after: 22.360676661535777
Epoch 9279/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6897857427597046
Epoch: 9279, Batch Gradient Norm: 27.251552237797195
Epoch: 9279, Batch Gradient Norm after: 22.36067812087824
Epoch 9280/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6928059339523316
Epoch: 9280, Batch Gradient Norm: 25.957207878522535
Epoch: 9280, Batch Gradient Norm after: 22.360677614048345
Epoch 9281/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6896491765975952
Epoch: 9281, Batch Gradient Norm: 27.247717872216096
Epoch: 9281, Batch Gradient Norm after: 22.360677664781672
Epoch 9282/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.692492401599884
Epoch: 9282, Batch Gradient Norm: 25.95688834026187
Epoch: 9282, Batch Gradient Norm after: 22.36067807128916
Epoch 9283/10000, Prediction Accuracy = 60.306%, Loss = 0.689474081993103
Epoch: 9283, Batch Gradient Norm: 27.252157381912188
Epoch: 9283, Batch Gradient Norm after: 22.3606744586138
Epoch 9284/10000, Prediction Accuracy = 60.324%, Loss = 0.692370331287384
Epoch: 9284, Batch Gradient Norm: 25.960980348741643
Epoch: 9284, Batch Gradient Norm after: 22.3606775845439
Epoch 9285/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.6895194888114929
Epoch: 9285, Batch Gradient Norm: 27.241897803670565
Epoch: 9285, Batch Gradient Norm after: 22.360677844537072
Epoch 9286/10000, Prediction Accuracy = 60.35%, Loss = 0.6923739671707153
Epoch: 9286, Batch Gradient Norm: 25.954538026566297
Epoch: 9286, Batch Gradient Norm after: 22.36067619585753
Epoch 9287/10000, Prediction Accuracy = 60.25%, Loss = 0.6893956780433654
Epoch: 9287, Batch Gradient Norm: 27.240057879912758
Epoch: 9287, Batch Gradient Norm after: 22.36067844582419
Epoch 9288/10000, Prediction Accuracy = 60.338%, Loss = 0.6922873616218567
Epoch: 9288, Batch Gradient Norm: 25.95324828252755
Epoch: 9288, Batch Gradient Norm after: 22.36067625406388
Epoch 9289/10000, Prediction Accuracy = 60.362%, Loss = 0.6893356800079345
Epoch: 9289, Batch Gradient Norm: 27.24328278383008
Epoch: 9289, Batch Gradient Norm after: 22.36067637587226
Epoch 9290/10000, Prediction Accuracy = 60.33%, Loss = 0.6924261569976806
Epoch: 9290, Batch Gradient Norm: 25.942483170253453
Epoch: 9290, Batch Gradient Norm after: 22.360676026564224
Epoch 9291/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6894554257392883
Epoch: 9291, Batch Gradient Norm: 27.236256414573116
Epoch: 9291, Batch Gradient Norm after: 22.36068027542895
Epoch 9292/10000, Prediction Accuracy = 60.35%, Loss = 0.6925092339515686
Epoch: 9292, Batch Gradient Norm: 25.9308403301008
Epoch: 9292, Batch Gradient Norm after: 22.360675649386817
Epoch 9293/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6893198966979981
Epoch: 9293, Batch Gradient Norm: 27.236614509632492
Epoch: 9293, Batch Gradient Norm after: 22.360677005200714
Epoch 9294/10000, Prediction Accuracy = 60.334%, Loss = 0.6922178864479065
Epoch: 9294, Batch Gradient Norm: 25.930782715446174
Epoch: 9294, Batch Gradient Norm after: 22.36067670257035
Epoch 9295/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6891607046127319
Epoch: 9295, Batch Gradient Norm: 27.241017224643812
Epoch: 9295, Batch Gradient Norm after: 22.36067591369359
Epoch 9296/10000, Prediction Accuracy = 60.324%, Loss = 0.6920815944671631
Epoch: 9296, Batch Gradient Norm: 25.9354382730198
Epoch: 9296, Batch Gradient Norm after: 22.360676572079075
Epoch 9297/10000, Prediction Accuracy = 60.254%, Loss = 0.6892014861106872
Epoch: 9297, Batch Gradient Norm: 27.230753532825045
Epoch: 9297, Batch Gradient Norm after: 22.360678818106383
Epoch 9298/10000, Prediction Accuracy = 60.35%, Loss = 0.6920966863632202
Epoch: 9298, Batch Gradient Norm: 25.926536007366707
Epoch: 9298, Batch Gradient Norm after: 22.360677736383405
Epoch 9299/10000, Prediction Accuracy = 60.238%, Loss = 0.6890773773193359
Epoch: 9299, Batch Gradient Norm: 27.22599225244026
Epoch: 9299, Batch Gradient Norm after: 22.36067727974344
Epoch 9300/10000, Prediction Accuracy = 60.334%, Loss = 0.6920085072517395
Epoch: 9300, Batch Gradient Norm: 25.9328919216908
Epoch: 9300, Batch Gradient Norm after: 22.360678809790265
Epoch 9301/10000, Prediction Accuracy = 60.366%, Loss = 0.6890192031860352
Epoch: 9301, Batch Gradient Norm: 27.229112870007654
Epoch: 9301, Batch Gradient Norm after: 22.360677592533502
Epoch 9302/10000, Prediction Accuracy = 60.336%, Loss = 0.6921484231948852
Epoch: 9302, Batch Gradient Norm: 25.92141636655446
Epoch: 9302, Batch Gradient Norm after: 22.36067731345734
Epoch 9303/10000, Prediction Accuracy = 60.336%, Loss = 0.6891774773597718
Epoch: 9303, Batch Gradient Norm: 27.221586459761593
Epoch: 9303, Batch Gradient Norm after: 22.360678747204375
Epoch 9304/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6922293663024902
Epoch: 9304, Batch Gradient Norm: 25.912284699479674
Epoch: 9304, Batch Gradient Norm after: 22.360676667069924
Epoch 9305/10000, Prediction Accuracy = 60.35%, Loss = 0.6890363216400146
Epoch: 9305, Batch Gradient Norm: 27.220005468377675
Epoch: 9305, Batch Gradient Norm after: 22.360676180394567
Epoch 9306/10000, Prediction Accuracy = 60.354%, Loss = 0.6919244885444641
Epoch: 9306, Batch Gradient Norm: 25.91246050059536
Epoch: 9306, Batch Gradient Norm after: 22.360679350194857
Epoch 9307/10000, Prediction Accuracy = 60.3%, Loss = 0.6888581395149231
Epoch: 9307, Batch Gradient Norm: 27.226338382858525
Epoch: 9307, Batch Gradient Norm after: 22.360675981010253
Epoch 9308/10000, Prediction Accuracy = 60.318%, Loss = 0.6917922019958496
Epoch: 9308, Batch Gradient Norm: 25.917313461863014
Epoch: 9308, Batch Gradient Norm after: 22.360678522203692
Epoch 9309/10000, Prediction Accuracy = 60.254%, Loss = 0.6888994574546814
Epoch: 9309, Batch Gradient Norm: 27.215650214781
Epoch: 9309, Batch Gradient Norm after: 22.360677927299484
Epoch 9310/10000, Prediction Accuracy = 60.35%, Loss = 0.6917996048927307
Epoch: 9310, Batch Gradient Norm: 25.904559681857375
Epoch: 9310, Batch Gradient Norm after: 22.360675957977257
Epoch 9311/10000, Prediction Accuracy = 60.246%, Loss = 0.6887644767761231
Epoch: 9311, Batch Gradient Norm: 27.215424932797
Epoch: 9311, Batch Gradient Norm after: 22.36067723118595
Epoch 9312/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6917206406593323
Epoch: 9312, Batch Gradient Norm: 25.899821642012995
Epoch: 9312, Batch Gradient Norm after: 22.360676564267475
Epoch 9313/10000, Prediction Accuracy = 60.354%, Loss = 0.6886992573738098
Epoch: 9313, Batch Gradient Norm: 27.218581204410956
Epoch: 9313, Batch Gradient Norm after: 22.360677437834717
Epoch 9314/10000, Prediction Accuracy = 60.326%, Loss = 0.6918621778488159
Epoch: 9314, Batch Gradient Norm: 25.888756876125527
Epoch: 9314, Batch Gradient Norm after: 22.360676279069278
Epoch 9315/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6888264656066895
Epoch: 9315, Batch Gradient Norm: 27.212432387388994
Epoch: 9315, Batch Gradient Norm after: 22.360678205492253
Epoch 9316/10000, Prediction Accuracy = 60.338%, Loss = 0.6919433236122131
Epoch: 9316, Batch Gradient Norm: 25.878837981450737
Epoch: 9316, Batch Gradient Norm after: 22.360675350240854
Epoch 9317/10000, Prediction Accuracy = 60.35%, Loss = 0.6886975646018982
Epoch: 9317, Batch Gradient Norm: 27.210441324172983
Epoch: 9317, Batch Gradient Norm after: 22.360675165711733
Epoch 9318/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.6916544675827027
Epoch: 9318, Batch Gradient Norm: 25.878737193568984
Epoch: 9318, Batch Gradient Norm after: 22.360679828179585
Epoch 9319/10000, Prediction Accuracy = 60.31%, Loss = 0.6885216951370239
Epoch: 9319, Batch Gradient Norm: 27.21947775224141
Epoch: 9319, Batch Gradient Norm after: 22.36067620182016
Epoch 9320/10000, Prediction Accuracy = 60.324%, Loss = 0.6915245652198792
Epoch: 9320, Batch Gradient Norm: 25.88416016208858
Epoch: 9320, Batch Gradient Norm after: 22.36067694659925
Epoch 9321/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.688557231426239
Epoch: 9321, Batch Gradient Norm: 27.20715735250732
Epoch: 9321, Batch Gradient Norm after: 22.3606794857271
Epoch 9322/10000, Prediction Accuracy = 60.342%, Loss = 0.6915399670600891
Epoch: 9322, Batch Gradient Norm: 25.87667068709998
Epoch: 9322, Batch Gradient Norm after: 22.360679459910298
Epoch 9323/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6884477138519287
Epoch: 9323, Batch Gradient Norm: 27.20346952998267
Epoch: 9323, Batch Gradient Norm after: 22.360677555030396
Epoch 9324/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6914471745491028
Epoch: 9324, Batch Gradient Norm: 25.876494967290512
Epoch: 9324, Batch Gradient Norm after: 22.360676612761278
Epoch 9325/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.688379955291748
Epoch: 9325, Batch Gradient Norm: 27.205407139739634
Epoch: 9325, Batch Gradient Norm after: 22.360677052410693
Epoch 9326/10000, Prediction Accuracy = 60.322%, Loss = 0.6915835618972779
Epoch: 9326, Batch Gradient Norm: 25.869726694803408
Epoch: 9326, Batch Gradient Norm after: 22.36067548530394
Epoch 9327/10000, Prediction Accuracy = 60.35%, Loss = 0.6885270595550537
Epoch: 9327, Batch Gradient Norm: 27.19745826272685
Epoch: 9327, Batch Gradient Norm after: 22.360677343474773
Epoch 9328/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6916885137557983
Epoch: 9328, Batch Gradient Norm: 25.85420538840882
Epoch: 9328, Batch Gradient Norm after: 22.36067665625363
Epoch 9329/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6884036898612976
Epoch: 9329, Batch Gradient Norm: 27.19731060149997
Epoch: 9329, Batch Gradient Norm after: 22.360679173713354
Epoch 9330/10000, Prediction Accuracy = 60.36%, Loss = 0.6913753747940063
Epoch: 9330, Batch Gradient Norm: 25.85760104084654
Epoch: 9330, Batch Gradient Norm after: 22.360679392655953
Epoch 9331/10000, Prediction Accuracy = 60.298%, Loss = 0.6882166624069214
Epoch: 9331, Batch Gradient Norm: 27.202905001411775
Epoch: 9331, Batch Gradient Norm after: 22.36067640046437
Epoch 9332/10000, Prediction Accuracy = 60.33%, Loss = 0.6912410140037537
Epoch: 9332, Batch Gradient Norm: 25.86298541791077
Epoch: 9332, Batch Gradient Norm after: 22.360678170997687
Epoch 9333/10000, Prediction Accuracy = 60.274%, Loss = 0.6882658481597901
Epoch: 9333, Batch Gradient Norm: 27.190284973620145
Epoch: 9333, Batch Gradient Norm after: 22.36067578874698
Epoch 9334/10000, Prediction Accuracy = 60.35%, Loss = 0.6912418723106384
Epoch: 9334, Batch Gradient Norm: 25.855691910368332
Epoch: 9334, Batch Gradient Norm after: 22.36067573745281
Epoch 9335/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6881517887115478
Epoch: 9335, Batch Gradient Norm: 27.187670305201603
Epoch: 9335, Batch Gradient Norm after: 22.360676988890596
Epoch 9336/10000, Prediction Accuracy = 60.352%, Loss = 0.6911567807197571
Epoch: 9336, Batch Gradient Norm: 25.855999750464452
Epoch: 9336, Batch Gradient Norm after: 22.36067805203017
Epoch 9337/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6880778074264526
Epoch: 9337, Batch Gradient Norm: 27.190099619721725
Epoch: 9337, Batch Gradient Norm after: 22.36067443760563
Epoch 9338/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6912858963012696
Epoch: 9338, Batch Gradient Norm: 25.849705710953653
Epoch: 9338, Batch Gradient Norm after: 22.360676679406186
Epoch 9339/10000, Prediction Accuracy = 60.36%, Loss = 0.6882096052169799
Epoch: 9339, Batch Gradient Norm: 27.18413361695168
Epoch: 9339, Batch Gradient Norm after: 22.36067718340876
Epoch 9340/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6913753032684327
Epoch: 9340, Batch Gradient Norm: 25.83254788750759
Epoch: 9340, Batch Gradient Norm after: 22.360677005356045
Epoch 9341/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6880899667739868
Epoch: 9341, Batch Gradient Norm: 27.18371326351692
Epoch: 9341, Batch Gradient Norm after: 22.360675110137073
Epoch 9342/10000, Prediction Accuracy = 60.362%, Loss = 0.6910993337631226
Epoch: 9342, Batch Gradient Norm: 25.835031915681853
Epoch: 9342, Batch Gradient Norm after: 22.360678263446896
Epoch 9343/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6879127383232116
Epoch: 9343, Batch Gradient Norm: 27.190040944185483
Epoch: 9343, Batch Gradient Norm after: 22.360675256624244
Epoch 9344/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6909555673599244
Epoch: 9344, Batch Gradient Norm: 25.83861151863823
Epoch: 9344, Batch Gradient Norm after: 22.360677826081588
Epoch 9345/10000, Prediction Accuracy = 60.288%, Loss = 0.6879533529281616
Epoch: 9345, Batch Gradient Norm: 27.18056967947228
Epoch: 9345, Batch Gradient Norm after: 22.36067764312578
Epoch 9346/10000, Prediction Accuracy = 60.354%, Loss = 0.6909673571586609
Epoch: 9346, Batch Gradient Norm: 25.830811496749764
Epoch: 9346, Batch Gradient Norm after: 22.360679590439716
Epoch 9347/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6878274559974671
Epoch: 9347, Batch Gradient Norm: 27.176272957215648
Epoch: 9347, Batch Gradient Norm after: 22.36067636752923
Epoch 9348/10000, Prediction Accuracy = 60.348%, Loss = 0.6908817052841186
Epoch: 9348, Batch Gradient Norm: 25.831640561948767
Epoch: 9348, Batch Gradient Norm after: 22.360677155151354
Epoch 9349/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6877652049064636
Epoch: 9349, Batch Gradient Norm: 27.179449813216273
Epoch: 9349, Batch Gradient Norm after: 22.36067645409538
Epoch 9350/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6910218358039856
Epoch: 9350, Batch Gradient Norm: 25.82193461272974
Epoch: 9350, Batch Gradient Norm after: 22.360677323874956
Epoch 9351/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6879284739494324
Epoch: 9351, Batch Gradient Norm: 27.171430053870242
Epoch: 9351, Batch Gradient Norm after: 22.3606799571481
Epoch 9352/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.6911219596862793
Epoch: 9352, Batch Gradient Norm: 25.813691154842804
Epoch: 9352, Batch Gradient Norm after: 22.360675649455928
Epoch 9353/10000, Prediction Accuracy = 60.364%, Loss = 0.6877867221832276
Epoch: 9353, Batch Gradient Norm: 27.168946799162693
Epoch: 9353, Batch Gradient Norm after: 22.360675205651784
Epoch 9354/10000, Prediction Accuracy = 60.372%, Loss = 0.6908004283905029
Epoch: 9354, Batch Gradient Norm: 25.817982494509632
Epoch: 9354, Batch Gradient Norm after: 22.360677177897767
Epoch 9355/10000, Prediction Accuracy = 60.314%, Loss = 0.6876158237457275
Epoch: 9355, Batch Gradient Norm: 27.172250379093345
Epoch: 9355, Batch Gradient Norm after: 22.360678782849302
Epoch 9356/10000, Prediction Accuracy = 60.348%, Loss = 0.6906649112701416
Epoch: 9356, Batch Gradient Norm: 25.823619703841377
Epoch: 9356, Batch Gradient Norm after: 22.360676887642715
Epoch 9357/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.6876487493515014
Epoch: 9357, Batch Gradient Norm: 27.162953406667288
Epoch: 9357, Batch Gradient Norm after: 22.36067825187635
Epoch 9358/10000, Prediction Accuracy = 60.366%, Loss = 0.6906654238700867
Epoch: 9358, Batch Gradient Norm: 25.820072279088915
Epoch: 9358, Batch Gradient Norm after: 22.360676553541843
Epoch 9359/10000, Prediction Accuracy = 60.27%, Loss = 0.68753662109375
Epoch: 9359, Batch Gradient Norm: 27.158685615910883
Epoch: 9359, Batch Gradient Norm after: 22.360675893306578
Epoch 9360/10000, Prediction Accuracy = 60.362%, Loss = 0.6905867576599121
Epoch: 9360, Batch Gradient Norm: 25.822735769315837
Epoch: 9360, Batch Gradient Norm after: 22.36067754081358
Epoch 9361/10000, Prediction Accuracy = 60.364%, Loss = 0.6874936342239379
Epoch: 9361, Batch Gradient Norm: 27.15998151971592
Epoch: 9361, Batch Gradient Norm after: 22.360676922639012
Epoch 9362/10000, Prediction Accuracy = 60.372%, Loss = 0.6907073259353638
Epoch: 9362, Batch Gradient Norm: 25.815483459738658
Epoch: 9362, Batch Gradient Norm after: 22.360674827361937
Epoch 9363/10000, Prediction Accuracy = 60.336%, Loss = 0.6876379489898682
Epoch: 9363, Batch Gradient Norm: 27.152423889404485
Epoch: 9363, Batch Gradient Norm after: 22.360678720377585
Epoch 9364/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6907835364341736
Epoch: 9364, Batch Gradient Norm: 25.80344137824532
Epoch: 9364, Batch Gradient Norm after: 22.360676827729296
Epoch 9365/10000, Prediction Accuracy = 60.354%, Loss = 0.6875036239624024
Epoch: 9365, Batch Gradient Norm: 27.153610913226416
Epoch: 9365, Batch Gradient Norm after: 22.360676141399665
Epoch 9366/10000, Prediction Accuracy = 60.388%, Loss = 0.690497100353241
Epoch: 9366, Batch Gradient Norm: 25.811043920103767
Epoch: 9366, Batch Gradient Norm after: 22.360679318770952
Epoch 9367/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6873403191566467
Epoch: 9367, Batch Gradient Norm: 27.15613518425209
Epoch: 9367, Batch Gradient Norm after: 22.360676940252226
Epoch 9368/10000, Prediction Accuracy = 60.352%, Loss = 0.6903592705726623
Epoch: 9368, Batch Gradient Norm: 25.816371049595354
Epoch: 9368, Batch Gradient Norm after: 22.360678910002008
Epoch 9369/10000, Prediction Accuracy = 60.28399999999999%, Loss = 0.6873788237571716
Epoch: 9369, Batch Gradient Norm: 27.14706406784724
Epoch: 9369, Batch Gradient Norm after: 22.360680792197293
Epoch 9370/10000, Prediction Accuracy = 60.364%, Loss = 0.6903736352920532
Epoch: 9370, Batch Gradient Norm: 25.809452967659254
Epoch: 9370, Batch Gradient Norm after: 22.36067995827076
Epoch 9371/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6872668862342834
Epoch: 9371, Batch Gradient Norm: 27.139743014790668
Epoch: 9371, Batch Gradient Norm after: 22.360676301925484
Epoch 9372/10000, Prediction Accuracy = 60.376%, Loss = 0.690282154083252
Epoch: 9372, Batch Gradient Norm: 25.816801226941397
Epoch: 9372, Batch Gradient Norm after: 22.360679368015862
Epoch 9373/10000, Prediction Accuracy = 60.376%, Loss = 0.6872165560722351
Epoch: 9373, Batch Gradient Norm: 27.143040137756984
Epoch: 9373, Batch Gradient Norm after: 22.360677807976888
Epoch 9374/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6904029607772827
Epoch: 9374, Batch Gradient Norm: 25.810684721919575
Epoch: 9374, Batch Gradient Norm after: 22.360675887786154
Epoch 9375/10000, Prediction Accuracy = 60.33799999999999%, Loss = 0.6873929381370545
Epoch: 9375, Batch Gradient Norm: 27.13191581164135
Epoch: 9375, Batch Gradient Norm after: 22.360676392161825
Epoch 9376/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6905073523521423
Epoch: 9376, Batch Gradient Norm: 25.799785126572516
Epoch: 9376, Batch Gradient Norm after: 22.360674858304932
Epoch 9377/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6872560381889343
Epoch: 9377, Batch Gradient Norm: 27.13203358268823
Epoch: 9377, Batch Gradient Norm after: 22.36067470575083
Epoch 9378/10000, Prediction Accuracy = 60.376%, Loss = 0.6901934742927551
Epoch: 9378, Batch Gradient Norm: 25.806207511050975
Epoch: 9378, Batch Gradient Norm after: 22.36067681348167
Epoch 9379/10000, Prediction Accuracy = 60.31%, Loss = 0.6870689511299133
Epoch: 9379, Batch Gradient Norm: 27.135072350898287
Epoch: 9379, Batch Gradient Norm after: 22.360678587616754
Epoch 9380/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.690049946308136
Epoch: 9380, Batch Gradient Norm: 25.808909578652518
Epoch: 9380, Batch Gradient Norm after: 22.360678291918557
Epoch 9381/10000, Prediction Accuracy = 60.286%, Loss = 0.6871170043945313
Epoch: 9381, Batch Gradient Norm: 27.12410316659587
Epoch: 9381, Batch Gradient Norm after: 22.360677069697363
Epoch 9382/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.690064799785614
Epoch: 9382, Batch Gradient Norm: 25.80746428149078
Epoch: 9382, Batch Gradient Norm after: 22.360680895678495
Epoch 9383/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6870081186294555
Epoch: 9383, Batch Gradient Norm: 27.119093906511132
Epoch: 9383, Batch Gradient Norm after: 22.3606782783526
Epoch 9384/10000, Prediction Accuracy = 60.398%, Loss = 0.6899702310562134
Epoch: 9384, Batch Gradient Norm: 25.805563333046173
Epoch: 9384, Batch Gradient Norm after: 22.360677957804327
Epoch 9385/10000, Prediction Accuracy = 60.378%, Loss = 0.6869484066963196
Epoch: 9385, Batch Gradient Norm: 27.126825424192276
Epoch: 9385, Batch Gradient Norm after: 22.360675647186156
Epoch 9386/10000, Prediction Accuracy = 60.36%, Loss = 0.6900998950004578
Epoch: 9386, Batch Gradient Norm: 25.79681255235864
Epoch: 9386, Batch Gradient Norm after: 22.360676081578188
Epoch 9387/10000, Prediction Accuracy = 60.338%, Loss = 0.6870856881141663
Epoch: 9387, Batch Gradient Norm: 27.11695105977437
Epoch: 9387, Batch Gradient Norm after: 22.360677982360713
Epoch 9388/10000, Prediction Accuracy = 60.334%, Loss = 0.6902047276496888
Epoch: 9388, Batch Gradient Norm: 25.78156488006571
Epoch: 9388, Batch Gradient Norm after: 22.36067796049739
Epoch 9389/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6869623303413391
Epoch: 9389, Batch Gradient Norm: 27.116206193305054
Epoch: 9389, Batch Gradient Norm after: 22.36067643995119
Epoch 9390/10000, Prediction Accuracy = 60.39%, Loss = 0.6899092197418213
Epoch: 9390, Batch Gradient Norm: 25.788534114559976
Epoch: 9390, Batch Gradient Norm after: 22.360679877161147
Epoch 9391/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.6867789030075073
Epoch: 9391, Batch Gradient Norm: 27.11798076719219
Epoch: 9391, Batch Gradient Norm after: 22.360677399317186
Epoch 9392/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6897544026374817
Epoch: 9392, Batch Gradient Norm: 25.800087240198742
Epoch: 9392, Batch Gradient Norm after: 22.36067643515102
Epoch 9393/10000, Prediction Accuracy = 60.298%, Loss = 0.6868371367454529
Epoch: 9393, Batch Gradient Norm: 27.104781990016193
Epoch: 9393, Batch Gradient Norm after: 22.360676476760023
Epoch 9394/10000, Prediction Accuracy = 60.366%, Loss = 0.6897670269012451
Epoch: 9394, Batch Gradient Norm: 25.793829841445497
Epoch: 9394, Batch Gradient Norm after: 22.360678439828607
Epoch 9395/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.6867400884628296
Epoch: 9395, Batch Gradient Norm: 27.101912363513527
Epoch: 9395, Batch Gradient Norm after: 22.360675844633693
Epoch 9396/10000, Prediction Accuracy = 60.378%, Loss = 0.689669919013977
Epoch: 9396, Batch Gradient Norm: 25.79243961600221
Epoch: 9396, Batch Gradient Norm after: 22.360677828950752
Epoch 9397/10000, Prediction Accuracy = 60.386%, Loss = 0.6866617798805237
Epoch: 9397, Batch Gradient Norm: 27.107775144966677
Epoch: 9397, Batch Gradient Norm after: 22.360677311306855
Epoch 9398/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6898041844367981
Epoch: 9398, Batch Gradient Norm: 25.7836823133118
Epoch: 9398, Batch Gradient Norm after: 22.36067431127385
Epoch 9399/10000, Prediction Accuracy = 60.34000000000001%, Loss = 0.6868181228637695
Epoch: 9399, Batch Gradient Norm: 27.099195720390682
Epoch: 9399, Batch Gradient Norm after: 22.360679004293758
Epoch 9400/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6899427771568298
Epoch: 9400, Batch Gradient Norm: 25.765877650654446
Epoch: 9400, Batch Gradient Norm after: 22.360675857106497
Epoch 9401/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6866866111755371
Epoch: 9401, Batch Gradient Norm: 27.098967135921374
Epoch: 9401, Batch Gradient Norm after: 22.360675596289145
Epoch 9402/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6896230816841126
Epoch: 9402, Batch Gradient Norm: 25.775588845846606
Epoch: 9402, Batch Gradient Norm after: 22.360677513958436
Epoch 9403/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.6864977598190307
Epoch: 9403, Batch Gradient Norm: 27.099429629971677
Epoch: 9403, Batch Gradient Norm after: 22.360675596026248
Epoch 9404/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6894715070724488
Epoch: 9404, Batch Gradient Norm: 25.782085671098006
Epoch: 9404, Batch Gradient Norm after: 22.36067704772675
Epoch 9405/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6865440011024475
Epoch: 9405, Batch Gradient Norm: 27.087466086870034
Epoch: 9405, Batch Gradient Norm after: 22.36067607257128
Epoch 9406/10000, Prediction Accuracy = 60.364%, Loss = 0.689480459690094
Epoch: 9406, Batch Gradient Norm: 25.777371522878582
Epoch: 9406, Batch Gradient Norm after: 22.360679376478007
Epoch 9407/10000, Prediction Accuracy = 60.3%, Loss = 0.6864269137382507
Epoch: 9407, Batch Gradient Norm: 27.085592561904566
Epoch: 9407, Batch Gradient Norm after: 22.360677812857865
Epoch 9408/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6893885970115662
Epoch: 9408, Batch Gradient Norm: 25.778393702576967
Epoch: 9408, Batch Gradient Norm after: 22.360681005990195
Epoch 9409/10000, Prediction Accuracy = 60.388%, Loss = 0.6863778471946717
Epoch: 9409, Batch Gradient Norm: 27.091149616509426
Epoch: 9409, Batch Gradient Norm after: 22.360676023247187
Epoch 9410/10000, Prediction Accuracy = 60.362%, Loss = 0.689508318901062
Epoch: 9410, Batch Gradient Norm: 25.7677350200048
Epoch: 9410, Batch Gradient Norm after: 22.360676777372205
Epoch 9411/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6865243434906005
Epoch: 9411, Batch Gradient Norm: 27.08283345233619
Epoch: 9411, Batch Gradient Norm after: 22.36067941282007
Epoch 9412/10000, Prediction Accuracy = 60.33%, Loss = 0.6896318793296814
Epoch: 9412, Batch Gradient Norm: 25.756762766976998
Epoch: 9412, Batch Gradient Norm after: 22.36067755041161
Epoch 9413/10000, Prediction Accuracy = 60.364%, Loss = 0.6864000916481018
Epoch: 9413, Batch Gradient Norm: 27.08012716069083
Epoch: 9413, Batch Gradient Norm after: 22.36067482489599
Epoch 9414/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6893147945404052
Epoch: 9414, Batch Gradient Norm: 25.764569183229032
Epoch: 9414, Batch Gradient Norm after: 22.360677205900448
Epoch 9415/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.686224615573883
Epoch: 9415, Batch Gradient Norm: 27.08263316685523
Epoch: 9415, Batch Gradient Norm after: 22.360677970693327
Epoch 9416/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6891800403594971
Epoch: 9416, Batch Gradient Norm: 25.767239853384428
Epoch: 9416, Batch Gradient Norm after: 22.360678708867276
Epoch 9417/10000, Prediction Accuracy = 60.324%, Loss = 0.6862805366516114
Epoch: 9417, Batch Gradient Norm: 27.07357685904842
Epoch: 9417, Batch Gradient Norm after: 22.3606764112116
Epoch 9418/10000, Prediction Accuracy = 60.378%, Loss = 0.6891928434371948
Epoch: 9418, Batch Gradient Norm: 25.762128474859317
Epoch: 9418, Batch Gradient Norm after: 22.36067917589302
Epoch 9419/10000, Prediction Accuracy = 60.288%, Loss = 0.6861429929733276
Epoch: 9419, Batch Gradient Norm: 27.06899649027574
Epoch: 9419, Batch Gradient Norm after: 22.360679360805168
Epoch 9420/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6890992164611817
Epoch: 9420, Batch Gradient Norm: 25.761274568798747
Epoch: 9420, Batch Gradient Norm after: 22.36067717289516
Epoch 9421/10000, Prediction Accuracy = 60.372%, Loss = 0.686095929145813
Epoch: 9421, Batch Gradient Norm: 27.07635293080757
Epoch: 9421, Batch Gradient Norm after: 22.36067807874001
Epoch 9422/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.689244294166565
Epoch: 9422, Batch Gradient Norm: 25.75013003601122
Epoch: 9422, Batch Gradient Norm after: 22.36067707829049
Epoch 9423/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6862451553344726
Epoch: 9423, Batch Gradient Norm: 27.067708010228767
Epoch: 9423, Batch Gradient Norm after: 22.360678215917364
Epoch 9424/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6893358945846557
Epoch: 9424, Batch Gradient Norm: 25.733729295514358
Epoch: 9424, Batch Gradient Norm after: 22.3606767489398
Epoch 9425/10000, Prediction Accuracy = 60.372%, Loss = 0.6860826015472412
Epoch: 9425, Batch Gradient Norm: 27.072279544667893
Epoch: 9425, Batch Gradient Norm after: 22.36067746270135
Epoch 9426/10000, Prediction Accuracy = 60.396%, Loss = 0.6890351414680481
Epoch: 9426, Batch Gradient Norm: 25.734636922529752
Epoch: 9426, Batch Gradient Norm after: 22.360677594342977
Epoch 9427/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6859030485153198
Epoch: 9427, Batch Gradient Norm: 27.07403873944624
Epoch: 9427, Batch Gradient Norm after: 22.360677410840015
Epoch 9428/10000, Prediction Accuracy = 60.374%, Loss = 0.6889105677604676
Epoch: 9428, Batch Gradient Norm: 25.733587312451633
Epoch: 9428, Batch Gradient Norm after: 22.360678827904373
Epoch 9429/10000, Prediction Accuracy = 60.302%, Loss = 0.6859536409378052
Epoch: 9429, Batch Gradient Norm: 27.061884546940544
Epoch: 9429, Batch Gradient Norm after: 22.36067813577051
Epoch 9430/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6889262199401855
Epoch: 9430, Batch Gradient Norm: 25.727770427203062
Epoch: 9430, Batch Gradient Norm after: 22.360677359281496
Epoch 9431/10000, Prediction Accuracy = 60.3%, Loss = 0.6858172655105591
Epoch: 9431, Batch Gradient Norm: 27.062292609799492
Epoch: 9431, Batch Gradient Norm after: 22.360679213848208
Epoch 9432/10000, Prediction Accuracy = 60.412%, Loss = 0.6888312697410583
Epoch: 9432, Batch Gradient Norm: 25.729904523550953
Epoch: 9432, Batch Gradient Norm after: 22.360676870533183
Epoch 9433/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6857815027236939
Epoch: 9433, Batch Gradient Norm: 27.064657190504043
Epoch: 9433, Batch Gradient Norm after: 22.360677387317505
Epoch 9434/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6889870643615723
Epoch: 9434, Batch Gradient Norm: 25.71908419177626
Epoch: 9434, Batch Gradient Norm after: 22.360677424394993
Epoch 9435/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6859175443649292
Epoch: 9435, Batch Gradient Norm: 27.059229540814062
Epoch: 9435, Batch Gradient Norm after: 22.360680491784176
Epoch 9436/10000, Prediction Accuracy = 60.324%, Loss = 0.6890617132186889
Epoch: 9436, Batch Gradient Norm: 25.707448169778246
Epoch: 9436, Batch Gradient Norm after: 22.360676611958045
Epoch 9437/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6857633829116822
Epoch: 9437, Batch Gradient Norm: 27.059748704147268
Epoch: 9437, Batch Gradient Norm after: 22.36067689654838
Epoch 9438/10000, Prediction Accuracy = 60.412%, Loss = 0.6887532711029053
Epoch: 9438, Batch Gradient Norm: 25.71034463994493
Epoch: 9438, Batch Gradient Norm after: 22.360676932825548
Epoch 9439/10000, Prediction Accuracy = 60.312%, Loss = 0.6855985641479492
Epoch: 9439, Batch Gradient Norm: 27.05936764875614
Epoch: 9439, Batch Gradient Norm after: 22.360677663084815
Epoch 9440/10000, Prediction Accuracy = 60.376%, Loss = 0.6886330842971802
Epoch: 9440, Batch Gradient Norm: 25.71835621594831
Epoch: 9440, Batch Gradient Norm after: 22.36067893627644
Epoch 9441/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6856683731079102
Epoch: 9441, Batch Gradient Norm: 27.04426934760699
Epoch: 9441, Batch Gradient Norm after: 22.360679435667258
Epoch 9442/10000, Prediction Accuracy = 60.376%, Loss = 0.6886394500732422
Epoch: 9442, Batch Gradient Norm: 25.709983079096673
Epoch: 9442, Batch Gradient Norm after: 22.360677102758483
Epoch 9443/10000, Prediction Accuracy = 60.306%, Loss = 0.685528826713562
Epoch: 9443, Batch Gradient Norm: 27.046256546338284
Epoch: 9443, Batch Gradient Norm after: 22.360678048663626
Epoch 9444/10000, Prediction Accuracy = 60.408%, Loss = 0.688543176651001
Epoch: 9444, Batch Gradient Norm: 25.712685587356333
Epoch: 9444, Batch Gradient Norm after: 22.360676982623612
Epoch 9445/10000, Prediction Accuracy = 60.348%, Loss = 0.685497272014618
Epoch: 9445, Batch Gradient Norm: 27.048500981270664
Epoch: 9445, Batch Gradient Norm after: 22.36067864260298
Epoch 9446/10000, Prediction Accuracy = 60.366%, Loss = 0.6887105584144593
Epoch: 9446, Batch Gradient Norm: 25.701167044891953
Epoch: 9446, Batch Gradient Norm after: 22.360676721590526
Epoch 9447/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6856423854827881
Epoch: 9447, Batch Gradient Norm: 27.03962202299414
Epoch: 9447, Batch Gradient Norm after: 22.360678049961013
Epoch 9448/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6887700080871582
Epoch: 9448, Batch Gradient Norm: 25.690800307354195
Epoch: 9448, Batch Gradient Norm after: 22.36067667919458
Epoch 9449/10000, Prediction Accuracy = 60.36200000000001%, Loss = 0.6854686617851258
Epoch: 9449, Batch Gradient Norm: 27.04228436497417
Epoch: 9449, Batch Gradient Norm after: 22.360676484119637
Epoch 9450/10000, Prediction Accuracy = 60.40400000000001%, Loss = 0.6884527087211609
Epoch: 9450, Batch Gradient Norm: 25.693521473142397
Epoch: 9450, Batch Gradient Norm after: 22.36067774548261
Epoch 9451/10000, Prediction Accuracy = 60.322%, Loss = 0.6853148937225342
Epoch: 9451, Batch Gradient Norm: 27.04457577375168
Epoch: 9451, Batch Gradient Norm after: 22.360677045677857
Epoch 9452/10000, Prediction Accuracy = 60.372%, Loss = 0.6883487343788147
Epoch: 9452, Batch Gradient Norm: 25.69990807078712
Epoch: 9452, Batch Gradient Norm after: 22.360677811522038
Epoch 9453/10000, Prediction Accuracy = 60.294%, Loss = 0.6853775024414063
Epoch: 9453, Batch Gradient Norm: 27.030825323502707
Epoch: 9453, Batch Gradient Norm after: 22.36067955913497
Epoch 9454/10000, Prediction Accuracy = 60.39%, Loss = 0.6883472323417663
Epoch: 9454, Batch Gradient Norm: 25.691427955872026
Epoch: 9454, Batch Gradient Norm after: 22.36067654138778
Epoch 9455/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6852224588394165
Epoch: 9455, Batch Gradient Norm: 27.03293246738963
Epoch: 9455, Batch Gradient Norm after: 22.360677365712608
Epoch 9456/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6882710576057434
Epoch: 9456, Batch Gradient Norm: 25.69184773857258
Epoch: 9456, Batch Gradient Norm after: 22.360679381063644
Epoch 9457/10000, Prediction Accuracy = 60.334%, Loss = 0.6852076053619385
Epoch: 9457, Batch Gradient Norm: 27.033598943729853
Epoch: 9457, Batch Gradient Norm after: 22.360677092823106
Epoch 9458/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6884425640106201
Epoch: 9458, Batch Gradient Norm: 25.680523054004663
Epoch: 9458, Batch Gradient Norm after: 22.36067843261299
Epoch 9459/10000, Prediction Accuracy = 60.362%, Loss = 0.6853446722030639
Epoch: 9459, Batch Gradient Norm: 27.025777419218738
Epoch: 9459, Batch Gradient Norm after: 22.360678153418426
Epoch 9460/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6884637594223022
Epoch: 9460, Batch Gradient Norm: 25.66684519018597
Epoch: 9460, Batch Gradient Norm after: 22.360678090415927
Epoch 9461/10000, Prediction Accuracy = 60.366%, Loss = 0.6851540565490722
Epoch: 9461, Batch Gradient Norm: 27.030766763728014
Epoch: 9461, Batch Gradient Norm after: 22.36067505230003
Epoch 9462/10000, Prediction Accuracy = 60.402%, Loss = 0.6881664514541626
Epoch: 9462, Batch Gradient Norm: 25.67451223546966
Epoch: 9462, Batch Gradient Norm after: 22.36067950849509
Epoch 9463/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6850170016288757
Epoch: 9463, Batch Gradient Norm: 27.031581771903394
Epoch: 9463, Batch Gradient Norm after: 22.360675101078723
Epoch 9464/10000, Prediction Accuracy = 60.384%, Loss = 0.688066017627716
Epoch: 9464, Batch Gradient Norm: 25.675678827821304
Epoch: 9464, Batch Gradient Norm after: 22.36067753616579
Epoch 9465/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.6850779533386231
Epoch: 9465, Batch Gradient Norm: 27.018915382302406
Epoch: 9465, Batch Gradient Norm after: 22.36067940343617
Epoch 9466/10000, Prediction Accuracy = 60.376%, Loss = 0.6880764245986939
Epoch: 9466, Batch Gradient Norm: 25.67128192639997
Epoch: 9466, Batch Gradient Norm after: 22.360677266734005
Epoch 9467/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6849271297454834
Epoch: 9467, Batch Gradient Norm: 27.018596141526793
Epoch: 9467, Batch Gradient Norm after: 22.360677399958718
Epoch 9468/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6879922986030579
Epoch: 9468, Batch Gradient Norm: 25.670371202993728
Epoch: 9468, Batch Gradient Norm after: 22.360678769676284
Epoch 9469/10000, Prediction Accuracy = 60.338%, Loss = 0.6849147558212281
Epoch: 9469, Batch Gradient Norm: 27.02076729349479
Epoch: 9469, Batch Gradient Norm after: 22.36067642323431
Epoch 9470/10000, Prediction Accuracy = 60.366%, Loss = 0.6881795763969422
Epoch: 9470, Batch Gradient Norm: 25.658483948924285
Epoch: 9470, Batch Gradient Norm after: 22.36067752824486
Epoch 9471/10000, Prediction Accuracy = 60.386%, Loss = 0.6850445389747619
Epoch: 9471, Batch Gradient Norm: 27.015899947964638
Epoch: 9471, Batch Gradient Norm after: 22.360678556513065
Epoch 9472/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6881807446479797
Epoch: 9472, Batch Gradient Norm: 25.64335159486263
Epoch: 9472, Batch Gradient Norm after: 22.360675711723143
Epoch 9473/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6848327159881592
Epoch: 9473, Batch Gradient Norm: 27.021234194881277
Epoch: 9473, Batch Gradient Norm after: 22.36067569146784
Epoch 9474/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6878867626190186
Epoch: 9474, Batch Gradient Norm: 25.649940220601177
Epoch: 9474, Batch Gradient Norm after: 22.360680285125024
Epoch 9475/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6847115397453308
Epoch: 9475, Batch Gradient Norm: 27.019682537868707
Epoch: 9475, Batch Gradient Norm after: 22.36067831686057
Epoch 9476/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.6877991437911988
Epoch: 9476, Batch Gradient Norm: 25.6540640438313
Epoch: 9476, Batch Gradient Norm after: 22.360677381210607
Epoch 9477/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6847790360450745
Epoch: 9477, Batch Gradient Norm: 27.004944958969947
Epoch: 9477, Batch Gradient Norm after: 22.360678162728174
Epoch 9478/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6877845644950866
Epoch: 9478, Batch Gradient Norm: 25.652440763834733
Epoch: 9478, Batch Gradient Norm after: 22.360677500615367
Epoch 9479/10000, Prediction Accuracy = 60.34000000000001%, Loss = 0.6846208572387695
Epoch: 9479, Batch Gradient Norm: 27.004925012834345
Epoch: 9479, Batch Gradient Norm after: 22.360679341465662
Epoch 9480/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6877203345298767
Epoch: 9480, Batch Gradient Norm: 25.653830910512863
Epoch: 9480, Batch Gradient Norm after: 22.36067829374937
Epoch 9481/10000, Prediction Accuracy = 60.35%, Loss = 0.6846348643302917
Epoch: 9481, Batch Gradient Norm: 27.006748479591288
Epoch: 9481, Batch Gradient Norm after: 22.36067678084035
Epoch 9482/10000, Prediction Accuracy = 60.348%, Loss = 0.6879044651985169
Epoch: 9482, Batch Gradient Norm: 25.64024438155004
Epoch: 9482, Batch Gradient Norm after: 22.360678788242428
Epoch 9483/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6847524166107177
Epoch: 9483, Batch Gradient Norm: 27.00145250365944
Epoch: 9483, Batch Gradient Norm after: 22.36067493796302
Epoch 9484/10000, Prediction Accuracy = 60.318%, Loss = 0.6878860354423523
Epoch: 9484, Batch Gradient Norm: 25.630779581385564
Epoch: 9484, Batch Gradient Norm after: 22.360678329370852
Epoch 9485/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6845351099967957
Epoch: 9485, Batch Gradient Norm: 27.003145779585743
Epoch: 9485, Batch Gradient Norm after: 22.36067596893159
Epoch 9486/10000, Prediction Accuracy = 60.412%, Loss = 0.6875752568244934
Epoch: 9486, Batch Gradient Norm: 25.641937190993733
Epoch: 9486, Batch Gradient Norm after: 22.360677500362527
Epoch 9487/10000, Prediction Accuracy = 60.362%, Loss = 0.6844397783279419
Epoch: 9487, Batch Gradient Norm: 27.001924736928554
Epoch: 9487, Batch Gradient Norm after: 22.360678605742535
Epoch 9488/10000, Prediction Accuracy = 60.36199999999999%, Loss = 0.6875163793563843
Epoch: 9488, Batch Gradient Norm: 25.641430268195695
Epoch: 9488, Batch Gradient Norm after: 22.3606766425975
Epoch 9489/10000, Prediction Accuracy = 60.29%, Loss = 0.6845046401023864
Epoch: 9489, Batch Gradient Norm: 26.98919749818336
Epoch: 9489, Batch Gradient Norm after: 22.360677331433262
Epoch 9490/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6874914526939392
Epoch: 9490, Batch Gradient Norm: 25.638867430863232
Epoch: 9490, Batch Gradient Norm after: 22.360678967356915
Epoch 9491/10000, Prediction Accuracy = 60.352%, Loss = 0.6843250632286072
Epoch: 9491, Batch Gradient Norm: 26.991006979695744
Epoch: 9491, Batch Gradient Norm after: 22.36067656057364
Epoch 9492/10000, Prediction Accuracy = 60.396%, Loss = 0.6874462127685547
Epoch: 9492, Batch Gradient Norm: 25.6355603031831
Epoch: 9492, Batch Gradient Norm after: 22.36067942543221
Epoch 9493/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6843711853027343
Epoch: 9493, Batch Gradient Norm: 26.992766019347428
Epoch: 9493, Batch Gradient Norm after: 22.36067869594045
Epoch 9494/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.6876656770706177
Epoch: 9494, Batch Gradient Norm: 25.619327783924536
Epoch: 9494, Batch Gradient Norm after: 22.36067872332668
Epoch 9495/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6844613313674927
Epoch: 9495, Batch Gradient Norm: 26.989083610793948
Epoch: 9495, Batch Gradient Norm after: 22.36067506664548
Epoch 9496/10000, Prediction Accuracy = 60.354%, Loss = 0.6875716090202332
Epoch: 9496, Batch Gradient Norm: 25.608638291336305
Epoch: 9496, Batch Gradient Norm after: 22.36067918874099
Epoch 9497/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6842097520828248
Epoch: 9497, Batch Gradient Norm: 26.994051440525407
Epoch: 9497, Batch Gradient Norm after: 22.360677945200102
Epoch 9498/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6872900128364563
Epoch: 9498, Batch Gradient Norm: 25.612896232781452
Epoch: 9498, Batch Gradient Norm after: 22.36067700903204
Epoch 9499/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6841400384902954
Epoch: 9499, Batch Gradient Norm: 26.994504792820834
Epoch: 9499, Batch Gradient Norm after: 22.360677683371176
Epoch 9500/10000, Prediction Accuracy = 60.36199999999999%, Loss = 0.687255346775055
Epoch: 9500, Batch Gradient Norm: 25.610113637387247
Epoch: 9500, Batch Gradient Norm after: 22.360675676639573
Epoch 9501/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6841704607009887
Epoch: 9501, Batch Gradient Norm: 26.980413503963316
Epoch: 9501, Batch Gradient Norm after: 22.3606784852722
Epoch 9502/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.687218713760376
Epoch: 9502, Batch Gradient Norm: 25.606929724467644
Epoch: 9502, Batch Gradient Norm after: 22.360678748100693
Epoch 9503/10000, Prediction Accuracy = 60.378%, Loss = 0.6839956998825073
Epoch: 9503, Batch Gradient Norm: 26.98662999413561
Epoch: 9503, Batch Gradient Norm after: 22.36067871674374
Epoch 9504/10000, Prediction Accuracy = 60.364%, Loss = 0.6872003197669982
Epoch: 9504, Batch Gradient Norm: 25.606651251450188
Epoch: 9504, Batch Gradient Norm after: 22.360675122722323
Epoch 9505/10000, Prediction Accuracy = 60.336%, Loss = 0.6840712308883667
Epoch: 9505, Batch Gradient Norm: 26.987542915334355
Epoch: 9505, Batch Gradient Norm after: 22.360677752920964
Epoch 9506/10000, Prediction Accuracy = 60.314%, Loss = 0.6874374508857727
Epoch: 9506, Batch Gradient Norm: 25.584363590518073
Epoch: 9506, Batch Gradient Norm after: 22.360679068021856
Epoch 9507/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6841367363929749
Epoch: 9507, Batch Gradient Norm: 26.9828788872642
Epoch: 9507, Batch Gradient Norm after: 22.36067952505614
Epoch 9508/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6873003959655761
Epoch: 9508, Batch Gradient Norm: 25.577956930058544
Epoch: 9508, Batch Gradient Norm after: 22.36067776544212
Epoch 9509/10000, Prediction Accuracy = 60.324%, Loss = 0.6838757991790771
Epoch: 9509, Batch Gradient Norm: 26.986722311961863
Epoch: 9509, Batch Gradient Norm after: 22.36067854879336
Epoch 9510/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6870130896568298
Epoch: 9510, Batch Gradient Norm: 25.587129427940393
Epoch: 9510, Batch Gradient Norm after: 22.360675975921804
Epoch 9511/10000, Prediction Accuracy = 60.366%, Loss = 0.6838451504707337
Epoch: 9511, Batch Gradient Norm: 26.98503691404629
Epoch: 9511, Batch Gradient Norm after: 22.36067730872401
Epoch 9512/10000, Prediction Accuracy = 60.36%, Loss = 0.6870030045509339
Epoch: 9512, Batch Gradient Norm: 25.58068935589366
Epoch: 9512, Batch Gradient Norm after: 22.36067712338923
Epoch 9513/10000, Prediction Accuracy = 60.312%, Loss = 0.6838453769683838
Epoch: 9513, Batch Gradient Norm: 26.97297147200576
Epoch: 9513, Batch Gradient Norm after: 22.36067705058847
Epoch 9514/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6869496464729309
Epoch: 9514, Batch Gradient Norm: 25.579518189497556
Epoch: 9514, Batch Gradient Norm after: 22.36067950471911
Epoch 9515/10000, Prediction Accuracy = 60.38599999999999%, Loss = 0.6836794495582581
Epoch: 9515, Batch Gradient Norm: 26.97981823913733
Epoch: 9515, Batch Gradient Norm after: 22.360677358940155
Epoch 9516/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.686959171295166
Epoch: 9516, Batch Gradient Norm: 25.574073987917707
Epoch: 9516, Batch Gradient Norm after: 22.360678808197303
Epoch 9517/10000, Prediction Accuracy = 60.35200000000001%, Loss = 0.6837725520133973
Epoch: 9517, Batch Gradient Norm: 26.976610142782324
Epoch: 9517, Batch Gradient Norm after: 22.36067782370663
Epoch 9518/10000, Prediction Accuracy = 60.318%, Loss = 0.6871909856796264
Epoch: 9518, Batch Gradient Norm: 25.554623534211995
Epoch: 9518, Batch Gradient Norm after: 22.36067833290705
Epoch 9519/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6837998032569885
Epoch: 9519, Batch Gradient Norm: 26.972261732774356
Epoch: 9519, Batch Gradient Norm after: 22.36067482721309
Epoch 9520/10000, Prediction Accuracy = 60.35%, Loss = 0.6870074510574341
Epoch: 9520, Batch Gradient Norm: 25.550544825995363
Epoch: 9520, Batch Gradient Norm after: 22.36067686828526
Epoch 9521/10000, Prediction Accuracy = 60.33%, Loss = 0.6835513591766358
Epoch: 9521, Batch Gradient Norm: 26.97957361923534
Epoch: 9521, Batch Gradient Norm after: 22.36067681083388
Epoch 9522/10000, Prediction Accuracy = 60.416%, Loss = 0.6867583990097046
Epoch: 9522, Batch Gradient Norm: 25.553969788628933
Epoch: 9522, Batch Gradient Norm after: 22.36067910788568
Epoch 9523/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6835233092308044
Epoch: 9523, Batch Gradient Norm: 26.973836849192985
Epoch: 9523, Batch Gradient Norm after: 22.36067857021719
Epoch 9524/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6867533683776855
Epoch: 9524, Batch Gradient Norm: 25.553625797116705
Epoch: 9524, Batch Gradient Norm after: 22.360675699396413
Epoch 9525/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6835212230682373
Epoch: 9525, Batch Gradient Norm: 26.958146623495725
Epoch: 9525, Batch Gradient Norm after: 22.360677451997695
Epoch 9526/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6866767764091491
Epoch: 9526, Batch Gradient Norm: 25.556778380774208
Epoch: 9526, Batch Gradient Norm after: 22.3606787647016
Epoch 9527/10000, Prediction Accuracy = 60.362%, Loss = 0.6833722233772278
Epoch: 9527, Batch Gradient Norm: 26.9654133325248
Epoch: 9527, Batch Gradient Norm after: 22.360676079294493
Epoch 9528/10000, Prediction Accuracy = 60.4%, Loss = 0.6867040753364563
Epoch: 9528, Batch Gradient Norm: 25.550871783832545
Epoch: 9528, Batch Gradient Norm after: 22.36067670197604
Epoch 9529/10000, Prediction Accuracy = 60.364%, Loss = 0.6834957599639893
Epoch: 9529, Batch Gradient Norm: 26.96254943265406
Epoch: 9529, Batch Gradient Norm after: 22.360679646641362
Epoch 9530/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6869442701339722
Epoch: 9530, Batch Gradient Norm: 25.533536121953134
Epoch: 9530, Batch Gradient Norm after: 22.360675874666477
Epoch 9531/10000, Prediction Accuracy = 60.39%, Loss = 0.6834906339645386
Epoch: 9531, Batch Gradient Norm: 26.956358841692193
Epoch: 9531, Batch Gradient Norm after: 22.360677204304213
Epoch 9532/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6867011547088623
Epoch: 9532, Batch Gradient Norm: 25.536662483622788
Epoch: 9532, Batch Gradient Norm after: 22.360680332760175
Epoch 9533/10000, Prediction Accuracy = 60.334%, Loss = 0.6832578182220459
Epoch: 9533, Batch Gradient Norm: 26.960140992962874
Epoch: 9533, Batch Gradient Norm after: 22.360676135018213
Epoch 9534/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6864631295204162
Epoch: 9534, Batch Gradient Norm: 25.542093743215666
Epoch: 9534, Batch Gradient Norm after: 22.360676885664788
Epoch 9535/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6832602143287658
Epoch: 9535, Batch Gradient Norm: 26.951608261839986
Epoch: 9535, Batch Gradient Norm after: 22.360677166874996
Epoch 9536/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6864718794822693
Epoch: 9536, Batch Gradient Norm: 25.539930773667898
Epoch: 9536, Batch Gradient Norm after: 22.36067795067933
Epoch 9537/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6832493424415589
Epoch: 9537, Batch Gradient Norm: 26.939503763996136
Epoch: 9537, Batch Gradient Norm after: 22.360677629858746
Epoch 9538/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6863837718963623
Epoch: 9538, Batch Gradient Norm: 25.546093460725324
Epoch: 9538, Batch Gradient Norm after: 22.36067825755093
Epoch 9539/10000, Prediction Accuracy = 60.352%, Loss = 0.6831027388572692
Epoch: 9539, Batch Gradient Norm: 26.94474667731768
Epoch: 9539, Batch Gradient Norm after: 22.36067766236786
Epoch 9540/10000, Prediction Accuracy = 60.38800000000001%, Loss = 0.6864368081092834
Epoch: 9540, Batch Gradient Norm: 25.540627339654318
Epoch: 9540, Batch Gradient Norm after: 22.36067705499769
Epoch 9541/10000, Prediction Accuracy = 60.36999999999999%, Loss = 0.6832423686981202
Epoch: 9541, Batch Gradient Norm: 26.940796150105403
Epoch: 9541, Batch Gradient Norm after: 22.360677310806466
Epoch 9542/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6866533994674683
Epoch: 9542, Batch Gradient Norm: 25.524576309174325
Epoch: 9542, Batch Gradient Norm after: 22.360676108360426
Epoch 9543/10000, Prediction Accuracy = 60.402%, Loss = 0.6832104563713074
Epoch: 9543, Batch Gradient Norm: 26.935764047449386
Epoch: 9543, Batch Gradient Norm after: 22.360675422496993
Epoch 9544/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6863798141479492
Epoch: 9544, Batch Gradient Norm: 25.52556017249371
Epoch: 9544, Batch Gradient Norm after: 22.360677573169312
Epoch 9545/10000, Prediction Accuracy = 60.33%, Loss = 0.6829787611961364
Epoch: 9545, Batch Gradient Norm: 26.941694759307634
Epoch: 9545, Batch Gradient Norm after: 22.360677187418826
Epoch 9546/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.686175525188446
Epoch: 9546, Batch Gradient Norm: 25.533796006560593
Epoch: 9546, Batch Gradient Norm after: 22.36067849276228
Epoch 9547/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6829843282699585
Epoch: 9547, Batch Gradient Norm: 26.932256001034137
Epoch: 9547, Batch Gradient Norm after: 22.360678127788137
Epoch 9548/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6861749172210694
Epoch: 9548, Batch Gradient Norm: 25.531602477077083
Epoch: 9548, Batch Gradient Norm after: 22.360676906658465
Epoch 9549/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6829715847969056
Epoch: 9549, Batch Gradient Norm: 26.917856003221136
Epoch: 9549, Batch Gradient Norm after: 22.360677625030174
Epoch 9550/10000, Prediction Accuracy = 60.388%, Loss = 0.6860862851142884
Epoch: 9550, Batch Gradient Norm: 25.53608337926095
Epoch: 9550, Batch Gradient Norm after: 22.360679240176623
Epoch 9551/10000, Prediction Accuracy = 60.34799999999999%, Loss = 0.6828336119651794
Epoch: 9551, Batch Gradient Norm: 26.924645055461134
Epoch: 9551, Batch Gradient Norm after: 22.360674818322376
Epoch 9552/10000, Prediction Accuracy = 60.4%, Loss = 0.686150336265564
Epoch: 9552, Batch Gradient Norm: 25.53255886641876
Epoch: 9552, Batch Gradient Norm after: 22.360678103907496
Epoch 9553/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6829892158508301
Epoch: 9553, Batch Gradient Norm: 26.91926864652454
Epoch: 9553, Batch Gradient Norm after: 22.360675471156753
Epoch 9554/10000, Prediction Accuracy = 60.334%, Loss = 0.6863654255867004
Epoch: 9554, Batch Gradient Norm: 25.517777546603682
Epoch: 9554, Batch Gradient Norm after: 22.360677216104083
Epoch 9555/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6829549670219421
Epoch: 9555, Batch Gradient Norm: 26.914980951586635
Epoch: 9555, Batch Gradient Norm after: 22.360674828434437
Epoch 9556/10000, Prediction Accuracy = 60.37399999999999%, Loss = 0.6860866189002991
Epoch: 9556, Batch Gradient Norm: 25.522077197821158
Epoch: 9556, Batch Gradient Norm after: 22.360677206747127
Epoch 9557/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6827243089675903
Epoch: 9557, Batch Gradient Norm: 26.91132636236633
Epoch: 9557, Batch Gradient Norm after: 22.36067788657461
Epoch 9558/10000, Prediction Accuracy = 60.412%, Loss = 0.6858704328536988
Epoch: 9558, Batch Gradient Norm: 25.532828488295507
Epoch: 9558, Batch Gradient Norm after: 22.360676479726017
Epoch 9559/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6827575325965881
Epoch: 9559, Batch Gradient Norm: 26.900777264392886
Epoch: 9559, Batch Gradient Norm after: 22.36067791634966
Epoch 9560/10000, Prediction Accuracy = 60.398%, Loss = 0.6858787417411805
Epoch: 9560, Batch Gradient Norm: 25.532413720832434
Epoch: 9560, Batch Gradient Norm after: 22.360674903891052
Epoch 9561/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6827219247817993
Epoch: 9561, Batch Gradient Norm: 26.88919150456307
Epoch: 9561, Batch Gradient Norm after: 22.360677449918896
Epoch 9562/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6857834458351135
Epoch: 9562, Batch Gradient Norm: 25.53642052041573
Epoch: 9562, Batch Gradient Norm after: 22.360680170152367
Epoch 9563/10000, Prediction Accuracy = 60.352%, Loss = 0.6825992226600647
Epoch: 9563, Batch Gradient Norm: 26.897720360632384
Epoch: 9563, Batch Gradient Norm after: 22.360676934943434
Epoch 9564/10000, Prediction Accuracy = 60.4%, Loss = 0.6858561515808106
Epoch: 9564, Batch Gradient Norm: 25.532577918884662
Epoch: 9564, Batch Gradient Norm after: 22.360677884955454
Epoch 9565/10000, Prediction Accuracy = 60.378%, Loss = 0.6827552676200866
Epoch: 9565, Batch Gradient Norm: 26.891813715134884
Epoch: 9565, Batch Gradient Norm after: 22.360675549177632
Epoch 9566/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6860414624214173
Epoch: 9566, Batch Gradient Norm: 25.520290038586186
Epoch: 9566, Batch Gradient Norm after: 22.36067996334778
Epoch 9567/10000, Prediction Accuracy = 60.398%, Loss = 0.6826973557472229
Epoch: 9567, Batch Gradient Norm: 26.886384241693758
Epoch: 9567, Batch Gradient Norm after: 22.360677806193372
Epoch 9568/10000, Prediction Accuracy = 60.362%, Loss = 0.6857594132423401
Epoch: 9568, Batch Gradient Norm: 25.525327253689913
Epoch: 9568, Batch Gradient Norm after: 22.360677666119493
Epoch 9569/10000, Prediction Accuracy = 60.312%, Loss = 0.6824898242950439
Epoch: 9569, Batch Gradient Norm: 26.88662026352848
Epoch: 9569, Batch Gradient Norm after: 22.360678650911694
Epoch 9570/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6855610489845276
Epoch: 9570, Batch Gradient Norm: 25.539076804027786
Epoch: 9570, Batch Gradient Norm after: 22.36067574506867
Epoch 9571/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6825231432914733
Epoch: 9571, Batch Gradient Norm: 26.87325275304158
Epoch: 9571, Batch Gradient Norm after: 22.360678849520202
Epoch 9572/10000, Prediction Accuracy = 60.398%, Loss = 0.6855641007423401
Epoch: 9572, Batch Gradient Norm: 25.536714511823476
Epoch: 9572, Batch Gradient Norm after: 22.360676910545585
Epoch 9573/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6824761152267456
Epoch: 9573, Batch Gradient Norm: 26.863207771112357
Epoch: 9573, Batch Gradient Norm after: 22.360675940089035
Epoch 9574/10000, Prediction Accuracy = 60.388%, Loss = 0.685470175743103
Epoch: 9574, Batch Gradient Norm: 25.5407817566206
Epoch: 9574, Batch Gradient Norm after: 22.36067846063978
Epoch 9575/10000, Prediction Accuracy = 60.35%, Loss = 0.6823663711547852
Epoch: 9575, Batch Gradient Norm: 26.872761471860166
Epoch: 9575, Batch Gradient Norm after: 22.360678134931206
Epoch 9576/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6855587482452392
Epoch: 9576, Batch Gradient Norm: 25.53771931399
Epoch: 9576, Batch Gradient Norm after: 22.360678765905835
Epoch 9577/10000, Prediction Accuracy = 60.376%, Loss = 0.682532274723053
Epoch: 9577, Batch Gradient Norm: 26.864911183123095
Epoch: 9577, Batch Gradient Norm after: 22.36067998245519
Epoch 9578/10000, Prediction Accuracy = 60.334%, Loss = 0.68574538230896
Epoch: 9578, Batch Gradient Norm: 25.52341609119712
Epoch: 9578, Batch Gradient Norm after: 22.360675023192492
Epoch 9579/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.6824615955352783
Epoch: 9579, Batch Gradient Norm: 26.860709560174765
Epoch: 9579, Batch Gradient Norm after: 22.360677520779653
Epoch 9580/10000, Prediction Accuracy = 60.388%, Loss = 0.6854451775550843
Epoch: 9580, Batch Gradient Norm: 25.52746573692218
Epoch: 9580, Batch Gradient Norm after: 22.3606791776905
Epoch 9581/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6822462916374207
Epoch: 9581, Batch Gradient Norm: 26.860403888979878
Epoch: 9581, Batch Gradient Norm after: 22.360677770747916
Epoch 9582/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6852503180503845
Epoch: 9582, Batch Gradient Norm: 25.540200662462105
Epoch: 9582, Batch Gradient Norm after: 22.360676526580246
Epoch 9583/10000, Prediction Accuracy = 60.326%, Loss = 0.6822900652885437
Epoch: 9583, Batch Gradient Norm: 26.847593200701453
Epoch: 9583, Batch Gradient Norm after: 22.36067768067492
Epoch 9584/10000, Prediction Accuracy = 60.39%, Loss = 0.6852651238441467
Epoch: 9584, Batch Gradient Norm: 25.542069729040602
Epoch: 9584, Batch Gradient Norm after: 22.360677157015953
Epoch 9585/10000, Prediction Accuracy = 60.35%, Loss = 0.6822377562522888
Epoch: 9585, Batch Gradient Norm: 26.836349565557285
Epoch: 9585, Batch Gradient Norm after: 22.360677808490387
Epoch 9586/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6851576685905456
Epoch: 9586, Batch Gradient Norm: 25.5525532083973
Epoch: 9586, Batch Gradient Norm after: 22.360676891644758
Epoch 9587/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6821488380432129
Epoch: 9587, Batch Gradient Norm: 26.844120703581325
Epoch: 9587, Batch Gradient Norm after: 22.360678674843786
Epoch 9588/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6852503895759583
Epoch: 9588, Batch Gradient Norm: 25.551234492439786
Epoch: 9588, Batch Gradient Norm after: 22.360677889685974
Epoch 9589/10000, Prediction Accuracy = 60.39%, Loss = 0.68232342004776
Epoch: 9589, Batch Gradient Norm: 26.83599915508429
Epoch: 9589, Batch Gradient Norm after: 22.360678797709514
Epoch 9590/10000, Prediction Accuracy = 60.33%, Loss = 0.6854067444801331
Epoch: 9590, Batch Gradient Norm: 25.53933537398834
Epoch: 9590, Batch Gradient Norm after: 22.360679190047257
Epoch 9591/10000, Prediction Accuracy = 60.412%, Loss = 0.6822478413581848
Epoch: 9591, Batch Gradient Norm: 26.830265323533055
Epoch: 9591, Batch Gradient Norm after: 22.360679450090835
Epoch 9592/10000, Prediction Accuracy = 60.388%, Loss = 0.6851115107536316
Epoch: 9592, Batch Gradient Norm: 25.546408534730976
Epoch: 9592, Batch Gradient Norm after: 22.36068015290124
Epoch 9593/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6820501208305358
Epoch: 9593, Batch Gradient Norm: 26.827577152374985
Epoch: 9593, Batch Gradient Norm after: 22.36067788756256
Epoch 9594/10000, Prediction Accuracy = 60.44%, Loss = 0.6849209427833557
Epoch: 9594, Batch Gradient Norm: 25.559278933408432
Epoch: 9594, Batch Gradient Norm after: 22.360675767812655
Epoch 9595/10000, Prediction Accuracy = 60.318%, Loss = 0.6821034669876098
Epoch: 9595, Batch Gradient Norm: 26.812298210553653
Epoch: 9595, Batch Gradient Norm after: 22.360678558752618
Epoch 9596/10000, Prediction Accuracy = 60.388%, Loss = 0.6849275469779968
Epoch: 9596, Batch Gradient Norm: 25.56138351798009
Epoch: 9596, Batch Gradient Norm after: 22.360678958809654
Epoch 9597/10000, Prediction Accuracy = 60.354%, Loss = 0.6820337772369385
Epoch: 9597, Batch Gradient Norm: 26.80601447795621
Epoch: 9597, Batch Gradient Norm after: 22.360675690900365
Epoch 9598/10000, Prediction Accuracy = 60.396%, Loss = 0.6848265886306762
Epoch: 9598, Batch Gradient Norm: 25.568174599968465
Epoch: 9598, Batch Gradient Norm after: 22.360679143565967
Epoch 9599/10000, Prediction Accuracy = 60.336%, Loss = 0.6819516897201539
Epoch: 9599, Batch Gradient Norm: 26.81264101601852
Epoch: 9599, Batch Gradient Norm after: 22.360676972145754
Epoch 9600/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6849321246147155
Epoch: 9600, Batch Gradient Norm: 25.56618218021162
Epoch: 9600, Batch Gradient Norm after: 22.360677461160012
Epoch 9601/10000, Prediction Accuracy = 60.39%, Loss = 0.6821205735206604
Epoch: 9601, Batch Gradient Norm: 26.80713949272365
Epoch: 9601, Batch Gradient Norm after: 22.360678940759872
Epoch 9602/10000, Prediction Accuracy = 60.338%, Loss = 0.6850958704948426
Epoch: 9602, Batch Gradient Norm: 25.54810692227903
Epoch: 9602, Batch Gradient Norm after: 22.360678212132182
Epoch 9603/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.6820302486419678
Epoch: 9603, Batch Gradient Norm: 26.801791514332944
Epoch: 9603, Batch Gradient Norm after: 22.360677769172074
Epoch 9604/10000, Prediction Accuracy = 60.39%, Loss = 0.6847948431968689
Epoch: 9604, Batch Gradient Norm: 25.557237681967916
Epoch: 9604, Batch Gradient Norm after: 22.36068073391898
Epoch 9605/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6818288922309875
Epoch: 9605, Batch Gradient Norm: 26.801819863894792
Epoch: 9605, Batch Gradient Norm after: 22.36067735111722
Epoch 9606/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6846112012863159
Epoch: 9606, Batch Gradient Norm: 25.567899798002365
Epoch: 9606, Batch Gradient Norm after: 22.36067756679962
Epoch 9607/10000, Prediction Accuracy = 60.33%, Loss = 0.6818902969360352
Epoch: 9607, Batch Gradient Norm: 26.786931509891993
Epoch: 9607, Batch Gradient Norm after: 22.36067932288656
Epoch 9608/10000, Prediction Accuracy = 60.4%, Loss = 0.6846311330795288
Epoch: 9608, Batch Gradient Norm: 25.567824378907755
Epoch: 9608, Batch Gradient Norm after: 22.36067738819028
Epoch 9609/10000, Prediction Accuracy = 60.352%, Loss = 0.6818057656288147
Epoch: 9609, Batch Gradient Norm: 26.782163281248156
Epoch: 9609, Batch Gradient Norm after: 22.360676698234165
Epoch 9610/10000, Prediction Accuracy = 60.398%, Loss = 0.6845250606536866
Epoch: 9610, Batch Gradient Norm: 25.575894549765618
Epoch: 9610, Batch Gradient Norm after: 22.360678693492474
Epoch 9611/10000, Prediction Accuracy = 60.352%, Loss = 0.6817257404327393
Epoch: 9611, Batch Gradient Norm: 26.786060833480075
Epoch: 9611, Batch Gradient Norm after: 22.360676145781145
Epoch 9612/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6846364259719848
Epoch: 9612, Batch Gradient Norm: 25.574629629040242
Epoch: 9612, Batch Gradient Norm after: 22.36067772692457
Epoch 9613/10000, Prediction Accuracy = 60.386%, Loss = 0.681890344619751
Epoch: 9613, Batch Gradient Norm: 26.778898558947052
Epoch: 9613, Batch Gradient Norm after: 22.36067928370815
Epoch 9614/10000, Prediction Accuracy = 60.348%, Loss = 0.6847675800323486
Epoch: 9614, Batch Gradient Norm: 25.564589219640023
Epoch: 9614, Batch Gradient Norm after: 22.360677513934757
Epoch 9615/10000, Prediction Accuracy = 60.414%, Loss = 0.6818071126937866
Epoch: 9615, Batch Gradient Norm: 26.773032557743203
Epoch: 9615, Batch Gradient Norm after: 22.360676870148623
Epoch 9616/10000, Prediction Accuracy = 60.4%, Loss = 0.6844668388366699
Epoch: 9616, Batch Gradient Norm: 25.57189801923429
Epoch: 9616, Batch Gradient Norm after: 22.36067768611153
Epoch 9617/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6816140413284302
Epoch: 9617, Batch Gradient Norm: 26.771969221908435
Epoch: 9617, Batch Gradient Norm after: 22.360677435599104
Epoch 9618/10000, Prediction Accuracy = 60.426%, Loss = 0.684296715259552
Epoch: 9618, Batch Gradient Norm: 25.5842320591827
Epoch: 9618, Batch Gradient Norm after: 22.360677384371733
Epoch 9619/10000, Prediction Accuracy = 60.326%, Loss = 0.6816823363304139
Epoch: 9619, Batch Gradient Norm: 26.759307635091627
Epoch: 9619, Batch Gradient Norm after: 22.360679780642954
Epoch 9620/10000, Prediction Accuracy = 60.4%, Loss = 0.6843100309371948
Epoch: 9620, Batch Gradient Norm: 25.58578291920764
Epoch: 9620, Batch Gradient Norm after: 22.36067841824163
Epoch 9621/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6816003084182739
Epoch: 9621, Batch Gradient Norm: 26.75351560869801
Epoch: 9621, Batch Gradient Norm after: 22.360678795753625
Epoch 9622/10000, Prediction Accuracy = 60.4%, Loss = 0.6842108368873596
Epoch: 9622, Batch Gradient Norm: 25.588371793970264
Epoch: 9622, Batch Gradient Norm after: 22.360677585338454
Epoch 9623/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6815151810646057
Epoch: 9623, Batch Gradient Norm: 26.75702264483573
Epoch: 9623, Batch Gradient Norm after: 22.36067846565464
Epoch 9624/10000, Prediction Accuracy = 60.412%, Loss = 0.684318232536316
Epoch: 9624, Batch Gradient Norm: 25.589573268103276
Epoch: 9624, Batch Gradient Norm after: 22.36067665564034
Epoch 9625/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6816821575164795
Epoch: 9625, Batch Gradient Norm: 26.748394636357606
Epoch: 9625, Batch Gradient Norm after: 22.36067956050023
Epoch 9626/10000, Prediction Accuracy = 60.342%, Loss = 0.6844714522361756
Epoch: 9626, Batch Gradient Norm: 25.577176014662985
Epoch: 9626, Batch Gradient Norm after: 22.36068051977761
Epoch 9627/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6816044569015502
Epoch: 9627, Batch Gradient Norm: 26.742719071262538
Epoch: 9627, Batch Gradient Norm after: 22.36067741691254
Epoch 9628/10000, Prediction Accuracy = 60.398%, Loss = 0.6841599345207214
Epoch: 9628, Batch Gradient Norm: 25.580988309569626
Epoch: 9628, Batch Gradient Norm after: 22.360676080662518
Epoch 9629/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6814012169837952
Epoch: 9629, Batch Gradient Norm: 26.746600472909055
Epoch: 9629, Batch Gradient Norm after: 22.360677862778672
Epoch 9630/10000, Prediction Accuracy = 60.436%, Loss = 0.6839888215065002
Epoch: 9630, Batch Gradient Norm: 25.592835946937853
Epoch: 9630, Batch Gradient Norm after: 22.360678530749453
Epoch 9631/10000, Prediction Accuracy = 60.336%, Loss = 0.6814589500427246
Epoch: 9631, Batch Gradient Norm: 26.733380744697044
Epoch: 9631, Batch Gradient Norm after: 22.360677544311876
Epoch 9632/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6840048670768738
Epoch: 9632, Batch Gradient Norm: 25.587998787643965
Epoch: 9632, Batch Gradient Norm after: 22.360679193159076
Epoch 9633/10000, Prediction Accuracy = 60.348%, Loss = 0.6813809752464295
Epoch: 9633, Batch Gradient Norm: 26.728381317027388
Epoch: 9633, Batch Gradient Norm after: 22.36067751950186
Epoch 9634/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6839079260826111
Epoch: 9634, Batch Gradient Norm: 25.592808051516677
Epoch: 9634, Batch Gradient Norm after: 22.36067963394586
Epoch 9635/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6812752246856689
Epoch: 9635, Batch Gradient Norm: 26.73448000446475
Epoch: 9635, Batch Gradient Norm after: 22.360679396017805
Epoch 9636/10000, Prediction Accuracy = 60.416%, Loss = 0.6840151906013489
Epoch: 9636, Batch Gradient Norm: 25.58633947106169
Epoch: 9636, Batch Gradient Norm after: 22.360678870980284
Epoch 9637/10000, Prediction Accuracy = 60.384%, Loss = 0.6814289450645447
Epoch: 9637, Batch Gradient Norm: 26.727703536449955
Epoch: 9637, Batch Gradient Norm after: 22.36067645338245
Epoch 9638/10000, Prediction Accuracy = 60.348%, Loss = 0.6841727495193481
Epoch: 9638, Batch Gradient Norm: 25.57126641829166
Epoch: 9638, Batch Gradient Norm after: 22.360679262112615
Epoch 9639/10000, Prediction Accuracy = 60.39%, Loss = 0.6813606142997741
Epoch: 9639, Batch Gradient Norm: 26.726341866467184
Epoch: 9639, Batch Gradient Norm after: 22.360678307664006
Epoch 9640/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6838810563087463
Epoch: 9640, Batch Gradient Norm: 25.568128487128106
Epoch: 9640, Batch Gradient Norm after: 22.3606766173008
Epoch 9641/10000, Prediction Accuracy = 60.334%, Loss = 0.6811246752738953
Epoch: 9641, Batch Gradient Norm: 26.732767429506552
Epoch: 9641, Batch Gradient Norm after: 22.360680615220723
Epoch 9642/10000, Prediction Accuracy = 60.438%, Loss = 0.6837145447731018
Epoch: 9642, Batch Gradient Norm: 25.57501063523951
Epoch: 9642, Batch Gradient Norm after: 22.360676979090517
Epoch 9643/10000, Prediction Accuracy = 60.334%, Loss = 0.6811671853065491
Epoch: 9643, Batch Gradient Norm: 26.72012461814731
Epoch: 9643, Batch Gradient Norm after: 22.360678022029827
Epoch 9644/10000, Prediction Accuracy = 60.39%, Loss = 0.683732271194458
Epoch: 9644, Batch Gradient Norm: 25.569641789975645
Epoch: 9644, Batch Gradient Norm after: 22.36067666279457
Epoch 9645/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.681097435951233
Epoch: 9645, Batch Gradient Norm: 26.71493087177807
Epoch: 9645, Batch Gradient Norm after: 22.360677941152076
Epoch 9646/10000, Prediction Accuracy = 60.39799999999999%, Loss = 0.6836358904838562
Epoch: 9646, Batch Gradient Norm: 25.571651286007217
Epoch: 9646, Batch Gradient Norm after: 22.360676532399076
Epoch 9647/10000, Prediction Accuracy = 60.36%, Loss = 0.6809815049171448
Epoch: 9647, Batch Gradient Norm: 26.722481757734528
Epoch: 9647, Batch Gradient Norm after: 22.360677860784932
Epoch 9648/10000, Prediction Accuracy = 60.436%, Loss = 0.683740770816803
Epoch: 9648, Batch Gradient Norm: 25.564144220456598
Epoch: 9648, Batch Gradient Norm after: 22.360676799544734
Epoch 9649/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6811384320259094
Epoch: 9649, Batch Gradient Norm: 26.71678404785481
Epoch: 9649, Batch Gradient Norm after: 22.36067818710969
Epoch 9650/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6839133143424988
Epoch: 9650, Batch Gradient Norm: 25.542418295768805
Epoch: 9650, Batch Gradient Norm after: 22.360678027976142
Epoch 9651/10000, Prediction Accuracy = 60.398%, Loss = 0.6810596823692322
Epoch: 9651, Batch Gradient Norm: 26.718186216648856
Epoch: 9651, Batch Gradient Norm after: 22.36067790923353
Epoch 9652/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.683632206916809
Epoch: 9652, Batch Gradient Norm: 25.538527665824844
Epoch: 9652, Batch Gradient Norm after: 22.36067859366585
Epoch 9653/10000, Prediction Accuracy = 60.338%, Loss = 0.6808152556419372
Epoch: 9653, Batch Gradient Norm: 26.724898368079675
Epoch: 9653, Batch Gradient Norm after: 22.36067917895238
Epoch 9654/10000, Prediction Accuracy = 60.448%, Loss = 0.6834577083587646
Epoch: 9654, Batch Gradient Norm: 25.542370465617132
Epoch: 9654, Batch Gradient Norm after: 22.360677008392532
Epoch 9655/10000, Prediction Accuracy = 60.338%, Loss = 0.6808465242385864
Epoch: 9655, Batch Gradient Norm: 26.716736124756725
Epoch: 9655, Batch Gradient Norm after: 22.36067767366756
Epoch 9656/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6834804892539978
Epoch: 9656, Batch Gradient Norm: 25.53905315937966
Epoch: 9656, Batch Gradient Norm after: 22.360676699626275
Epoch 9657/10000, Prediction Accuracy = 60.336%, Loss = 0.6807816863059998
Epoch: 9657, Batch Gradient Norm: 26.708200363192358
Epoch: 9657, Batch Gradient Norm after: 22.360676371892506
Epoch 9658/10000, Prediction Accuracy = 60.386%, Loss = 0.6833867192268371
Epoch: 9658, Batch Gradient Norm: 25.536276922078482
Epoch: 9658, Batch Gradient Norm after: 22.360678516146308
Epoch 9659/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6806528687477111
Epoch: 9659, Batch Gradient Norm: 26.717714826470633
Epoch: 9659, Batch Gradient Norm after: 22.360677573529372
Epoch 9660/10000, Prediction Accuracy = 60.45799999999999%, Loss = 0.6834861040115356
Epoch: 9660, Batch Gradient Norm: 25.528236585366436
Epoch: 9660, Batch Gradient Norm after: 22.36067864517591
Epoch 9661/10000, Prediction Accuracy = 60.386%, Loss = 0.6808131098747253
Epoch: 9661, Batch Gradient Norm: 26.7128081435824
Epoch: 9661, Batch Gradient Norm after: 22.36067517705802
Epoch 9662/10000, Prediction Accuracy = 60.36%, Loss = 0.683676278591156
Epoch: 9662, Batch Gradient Norm: 25.510349763180788
Epoch: 9662, Batch Gradient Norm after: 22.360676397014036
Epoch 9663/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6807382702827454
Epoch: 9663, Batch Gradient Norm: 26.710180719042302
Epoch: 9663, Batch Gradient Norm after: 22.360679784328955
Epoch 9664/10000, Prediction Accuracy = 60.38800000000001%, Loss = 0.6833775758743286
Epoch: 9664, Batch Gradient Norm: 25.50819977879702
Epoch: 9664, Batch Gradient Norm after: 22.36067796258009
Epoch 9665/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.680497658252716
Epoch: 9665, Batch Gradient Norm: 26.717316320106896
Epoch: 9665, Batch Gradient Norm after: 22.360677850608784
Epoch 9666/10000, Prediction Accuracy = 60.444%, Loss = 0.6832037210464478
Epoch: 9666, Batch Gradient Norm: 25.51509466822087
Epoch: 9666, Batch Gradient Norm after: 22.360676438261628
Epoch 9667/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6805302619934082
Epoch: 9667, Batch Gradient Norm: 26.707506890300692
Epoch: 9667, Batch Gradient Norm after: 22.360678784949044
Epoch 9668/10000, Prediction Accuracy = 60.388%, Loss = 0.6832186222076416
Epoch: 9668, Batch Gradient Norm: 25.511427576985657
Epoch: 9668, Batch Gradient Norm after: 22.360679640224934
Epoch 9669/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.680469024181366
Epoch: 9669, Batch Gradient Norm: 26.7008958989526
Epoch: 9669, Batch Gradient Norm after: 22.36067787749206
Epoch 9670/10000, Prediction Accuracy = 60.396%, Loss = 0.6831250786781311
Epoch: 9670, Batch Gradient Norm: 25.510742325602347
Epoch: 9670, Batch Gradient Norm after: 22.360678640626528
Epoch 9671/10000, Prediction Accuracy = 60.378%, Loss = 0.6803463816642761
Epoch: 9671, Batch Gradient Norm: 26.707764471707055
Epoch: 9671, Batch Gradient Norm after: 22.36067897507832
Epoch 9672/10000, Prediction Accuracy = 60.443999999999996%, Loss = 0.683217716217041
Epoch: 9672, Batch Gradient Norm: 25.504811500618516
Epoch: 9672, Batch Gradient Norm after: 22.36067785919663
Epoch 9673/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6804967045783996
Epoch: 9673, Batch Gradient Norm: 26.70418538057197
Epoch: 9673, Batch Gradient Norm after: 22.36067837432537
Epoch 9674/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.683419406414032
Epoch: 9674, Batch Gradient Norm: 25.482133052794886
Epoch: 9674, Batch Gradient Norm after: 22.360677271251426
Epoch 9675/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6804336547851563
Epoch: 9675, Batch Gradient Norm: 26.703710400237348
Epoch: 9675, Batch Gradient Norm after: 22.360679774324055
Epoch 9676/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6831351399421692
Epoch: 9676, Batch Gradient Norm: 25.481552988685436
Epoch: 9676, Batch Gradient Norm after: 22.360680037059872
Epoch 9677/10000, Prediction Accuracy = 60.322%, Loss = 0.6801890134811401
Epoch: 9677, Batch Gradient Norm: 26.709812898171574
Epoch: 9677, Batch Gradient Norm after: 22.36067954601754
Epoch 9678/10000, Prediction Accuracy = 60.44%, Loss = 0.6829437851905823
Epoch: 9678, Batch Gradient Norm: 25.489393550462196
Epoch: 9678, Batch Gradient Norm after: 22.36067675371017
Epoch 9679/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6802304506301879
Epoch: 9679, Batch Gradient Norm: 26.698841061606128
Epoch: 9679, Batch Gradient Norm after: 22.36067806011272
Epoch 9680/10000, Prediction Accuracy = 60.398%, Loss = 0.6829721093177795
Epoch: 9680, Batch Gradient Norm: 25.4791941341148
Epoch: 9680, Batch Gradient Norm after: 22.360676706611624
Epoch 9681/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6801519632339478
Epoch: 9681, Batch Gradient Norm: 26.691225061068227
Epoch: 9681, Batch Gradient Norm after: 22.360676874141088
Epoch 9682/10000, Prediction Accuracy = 60.384%, Loss = 0.6828817486763
Epoch: 9682, Batch Gradient Norm: 25.483436957035025
Epoch: 9682, Batch Gradient Norm after: 22.36067895078465
Epoch 9683/10000, Prediction Accuracy = 60.378%, Loss = 0.6800349593162537
Epoch: 9683, Batch Gradient Norm: 26.699348081534897
Epoch: 9683, Batch Gradient Norm after: 22.360680013037147
Epoch 9684/10000, Prediction Accuracy = 60.436%, Loss = 0.6829671263694763
Epoch: 9684, Batch Gradient Norm: 25.48194481406047
Epoch: 9684, Batch Gradient Norm after: 22.360677761727732
Epoch 9685/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.680197286605835
Epoch: 9685, Batch Gradient Norm: 26.69150088753836
Epoch: 9685, Batch Gradient Norm after: 22.360677192499587
Epoch 9686/10000, Prediction Accuracy = 60.366%, Loss = 0.6831578373908996
Epoch: 9686, Batch Gradient Norm: 25.46594112656937
Epoch: 9686, Batch Gradient Norm after: 22.36067685745933
Epoch 9687/10000, Prediction Accuracy = 60.416%, Loss = 0.680144464969635
Epoch: 9687, Batch Gradient Norm: 26.689317282015423
Epoch: 9687, Batch Gradient Norm after: 22.36067678278069
Epoch 9688/10000, Prediction Accuracy = 60.388%, Loss = 0.6828671455383301
Epoch: 9688, Batch Gradient Norm: 25.46628658386281
Epoch: 9688, Batch Gradient Norm after: 22.360678225996704
Epoch 9689/10000, Prediction Accuracy = 60.32800000000001%, Loss = 0.6799052715301513
Epoch: 9689, Batch Gradient Norm: 26.693079523819527
Epoch: 9689, Batch Gradient Norm after: 22.360677592665724
Epoch 9690/10000, Prediction Accuracy = 60.44599999999999%, Loss = 0.682674503326416
Epoch: 9690, Batch Gradient Norm: 25.473030989449246
Epoch: 9690, Batch Gradient Norm after: 22.360679218179282
Epoch 9691/10000, Prediction Accuracy = 60.35%, Loss = 0.6799545645713806
Epoch: 9691, Batch Gradient Norm: 26.68374528444378
Epoch: 9691, Batch Gradient Norm after: 22.36067990033873
Epoch 9692/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6827008128166199
Epoch: 9692, Batch Gradient Norm: 25.47020967074986
Epoch: 9692, Batch Gradient Norm after: 22.360676537610882
Epoch 9693/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6798734307289124
Epoch: 9693, Batch Gradient Norm: 26.67373826242197
Epoch: 9693, Batch Gradient Norm after: 22.360677542363266
Epoch 9694/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.682596218585968
Epoch: 9694, Batch Gradient Norm: 25.47223046359323
Epoch: 9694, Batch Gradient Norm after: 22.360678632693705
Epoch 9695/10000, Prediction Accuracy = 60.384%, Loss = 0.6797727346420288
Epoch: 9695, Batch Gradient Norm: 26.6807670194401
Epoch: 9695, Batch Gradient Norm after: 22.36067754821299
Epoch 9696/10000, Prediction Accuracy = 60.424%, Loss = 0.6826998353004455
Epoch: 9696, Batch Gradient Norm: 25.46885227467951
Epoch: 9696, Batch Gradient Norm after: 22.36067544400202
Epoch 9697/10000, Prediction Accuracy = 60.414%, Loss = 0.6799356937408447
Epoch: 9697, Batch Gradient Norm: 26.67427363555862
Epoch: 9697, Batch Gradient Norm after: 22.360677746626397
Epoch 9698/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6828729152679444
Epoch: 9698, Batch Gradient Norm: 25.451404139484595
Epoch: 9698, Batch Gradient Norm after: 22.360676689197348
Epoch 9699/10000, Prediction Accuracy = 60.42%, Loss = 0.6798558712005616
Epoch: 9699, Batch Gradient Norm: 26.673024074822436
Epoch: 9699, Batch Gradient Norm after: 22.36067806645101
Epoch 9700/10000, Prediction Accuracy = 60.39%, Loss = 0.6825804948806763
Epoch: 9700, Batch Gradient Norm: 25.45075228215578
Epoch: 9700, Batch Gradient Norm after: 22.360679836538324
Epoch 9701/10000, Prediction Accuracy = 60.33%, Loss = 0.6796309351921082
Epoch: 9701, Batch Gradient Norm: 26.677681017976287
Epoch: 9701, Batch Gradient Norm after: 22.36067841699857
Epoch 9702/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6823973536491394
Epoch: 9702, Batch Gradient Norm: 25.462512862076977
Epoch: 9702, Batch Gradient Norm after: 22.360676482262797
Epoch 9703/10000, Prediction Accuracy = 60.366%, Loss = 0.679706609249115
Epoch: 9703, Batch Gradient Norm: 26.6632944265076
Epoch: 9703, Batch Gradient Norm after: 22.360678115857205
Epoch 9704/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6824379086494445
Epoch: 9704, Batch Gradient Norm: 25.453621598738394
Epoch: 9704, Batch Gradient Norm after: 22.36067805593017
Epoch 9705/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.679613983631134
Epoch: 9705, Batch Gradient Norm: 26.656607018128366
Epoch: 9705, Batch Gradient Norm after: 22.36067746253232
Epoch 9706/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6823210477828979
Epoch: 9706, Batch Gradient Norm: 25.45739303860736
Epoch: 9706, Batch Gradient Norm after: 22.360679947321383
Epoch 9707/10000, Prediction Accuracy = 60.372%, Loss = 0.6795091152191162
Epoch: 9707, Batch Gradient Norm: 26.66468343392715
Epoch: 9707, Batch Gradient Norm after: 22.360676311890728
Epoch 9708/10000, Prediction Accuracy = 60.396%, Loss = 0.682453453540802
Epoch: 9708, Batch Gradient Norm: 25.450570027318413
Epoch: 9708, Batch Gradient Norm after: 22.3606782866421
Epoch 9709/10000, Prediction Accuracy = 60.4%, Loss = 0.6796837449073792
Epoch: 9709, Batch Gradient Norm: 26.657749978674754
Epoch: 9709, Batch Gradient Norm after: 22.360678377698733
Epoch 9710/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6825909733772277
Epoch: 9710, Batch Gradient Norm: 25.436887348622452
Epoch: 9710, Batch Gradient Norm after: 22.36067770153359
Epoch 9711/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6795586347579956
Epoch: 9711, Batch Gradient Norm: 26.656203573643634
Epoch: 9711, Batch Gradient Norm after: 22.360676020423277
Epoch 9712/10000, Prediction Accuracy = 60.4%, Loss = 0.682269835472107
Epoch: 9712, Batch Gradient Norm: 25.43936936842511
Epoch: 9712, Batch Gradient Norm after: 22.36067800983085
Epoch 9713/10000, Prediction Accuracy = 60.352%, Loss = 0.679362952709198
Epoch: 9713, Batch Gradient Norm: 26.65788925388543
Epoch: 9713, Batch Gradient Norm after: 22.3606748545081
Epoch 9714/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.6821255087852478
Epoch: 9714, Batch Gradient Norm: 25.446399938794748
Epoch: 9714, Batch Gradient Norm after: 22.36067790989417
Epoch 9715/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.679429006576538
Epoch: 9715, Batch Gradient Norm: 26.644713866916845
Epoch: 9715, Batch Gradient Norm after: 22.36067703872275
Epoch 9716/10000, Prediction Accuracy = 60.398%, Loss = 0.6821466326713562
Epoch: 9716, Batch Gradient Norm: 25.440436484673885
Epoch: 9716, Batch Gradient Norm after: 22.360679763311527
Epoch 9717/10000, Prediction Accuracy = 60.366%, Loss = 0.6793224215507507
Epoch: 9717, Batch Gradient Norm: 26.638924947771212
Epoch: 9717, Batch Gradient Norm after: 22.36067667453037
Epoch 9718/10000, Prediction Accuracy = 60.402%, Loss = 0.682051146030426
Epoch: 9718, Batch Gradient Norm: 25.44770885711056
Epoch: 9718, Batch Gradient Norm after: 22.360679114488125
Epoch 9719/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6792481660842895
Epoch: 9719, Batch Gradient Norm: 26.64651610401087
Epoch: 9719, Batch Gradient Norm after: 22.360677551207324
Epoch 9720/10000, Prediction Accuracy = 60.4%, Loss = 0.6821853280067444
Epoch: 9720, Batch Gradient Norm: 25.439576998284764
Epoch: 9720, Batch Gradient Norm after: 22.36067686125033
Epoch 9721/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6794331431388855
Epoch: 9721, Batch Gradient Norm: 26.638338555658823
Epoch: 9721, Batch Gradient Norm after: 22.360675364542285
Epoch 9722/10000, Prediction Accuracy = 60.378%, Loss = 0.6823288798332214
Epoch: 9722, Batch Gradient Norm: 25.424256715004063
Epoch: 9722, Batch Gradient Norm after: 22.360677856427206
Epoch 9723/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6792931318283081
Epoch: 9723, Batch Gradient Norm: 26.635861962350017
Epoch: 9723, Batch Gradient Norm after: 22.36067761880347
Epoch 9724/10000, Prediction Accuracy = 60.398%, Loss = 0.6819921612739563
Epoch: 9724, Batch Gradient Norm: 25.427978453771082
Epoch: 9724, Batch Gradient Norm after: 22.360677450513144
Epoch 9725/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.679090940952301
Epoch: 9725, Batch Gradient Norm: 26.642569051156087
Epoch: 9725, Batch Gradient Norm after: 22.360676422990267
Epoch 9726/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6818574428558349
Epoch: 9726, Batch Gradient Norm: 25.42923096289043
Epoch: 9726, Batch Gradient Norm after: 22.360677800147467
Epoch 9727/10000, Prediction Accuracy = 60.354%, Loss = 0.679190194606781
Epoch: 9727, Batch Gradient Norm: 26.62662426232868
Epoch: 9727, Batch Gradient Norm after: 22.360675133426554
Epoch 9728/10000, Prediction Accuracy = 60.378%, Loss = 0.681899631023407
Epoch: 9728, Batch Gradient Norm: 25.423512417212827
Epoch: 9728, Batch Gradient Norm after: 22.36067939692708
Epoch 9729/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6790295243263245
Epoch: 9729, Batch Gradient Norm: 26.624943918383302
Epoch: 9729, Batch Gradient Norm after: 22.36067719275599
Epoch 9730/10000, Prediction Accuracy = 60.44%, Loss = 0.6817803263664246
Epoch: 9730, Batch Gradient Norm: 25.425677021534813
Epoch: 9730, Batch Gradient Norm after: 22.36067809580318
Epoch 9731/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6789672136306762
Epoch: 9731, Batch Gradient Norm: 26.63385397287985
Epoch: 9731, Batch Gradient Norm after: 22.360677833033947
Epoch 9732/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6819388031959533
Epoch: 9732, Batch Gradient Norm: 25.41391885430305
Epoch: 9732, Batch Gradient Norm after: 22.360678086319446
Epoch 9733/10000, Prediction Accuracy = 60.402%, Loss = 0.6791308283805847
Epoch: 9733, Batch Gradient Norm: 26.628273101271308
Epoch: 9733, Batch Gradient Norm after: 22.360676841641588
Epoch 9734/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6820253968238831
Epoch: 9734, Batch Gradient Norm: 25.406175867162943
Epoch: 9734, Batch Gradient Norm after: 22.360676787154233
Epoch 9735/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.678972327709198
Epoch: 9735, Batch Gradient Norm: 26.62232588352132
Epoch: 9735, Batch Gradient Norm after: 22.360676542580208
Epoch 9736/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.681702709197998
Epoch: 9736, Batch Gradient Norm: 25.40878396049439
Epoch: 9736, Batch Gradient Norm after: 22.360678894444426
Epoch 9737/10000, Prediction Accuracy = 60.36400000000001%, Loss = 0.6788124680519104
Epoch: 9737, Batch Gradient Norm: 26.625918450963475
Epoch: 9737, Batch Gradient Norm after: 22.36067710979855
Epoch 9738/10000, Prediction Accuracy = 60.412%, Loss = 0.6815905094146728
Epoch: 9738, Batch Gradient Norm: 25.41420026277121
Epoch: 9738, Batch Gradient Norm after: 22.36067782665598
Epoch 9739/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6788916707038879
Epoch: 9739, Batch Gradient Norm: 26.611209477190055
Epoch: 9739, Batch Gradient Norm after: 22.36067941526327
Epoch 9740/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6816033720970154
Epoch: 9740, Batch Gradient Norm: 25.412094606705903
Epoch: 9740, Batch Gradient Norm after: 22.36067655805021
Epoch 9741/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6787571907043457
Epoch: 9741, Batch Gradient Norm: 26.60852966515978
Epoch: 9741, Batch Gradient Norm after: 22.360676868387934
Epoch 9742/10000, Prediction Accuracy = 60.444%, Loss = 0.6815041303634644
Epoch: 9742, Batch Gradient Norm: 25.411977213745228
Epoch: 9742, Batch Gradient Norm after: 22.360673927854815
Epoch 9743/10000, Prediction Accuracy = 60.372%, Loss = 0.678709614276886
Epoch: 9743, Batch Gradient Norm: 26.615892547135175
Epoch: 9743, Batch Gradient Norm after: 22.360676792225583
Epoch 9744/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6816740989685058
Epoch: 9744, Batch Gradient Norm: 25.406952392516445
Epoch: 9744, Batch Gradient Norm after: 22.36067909023724
Epoch 9745/10000, Prediction Accuracy = 60.424%, Loss = 0.6788989067077636
Epoch: 9745, Batch Gradient Norm: 26.606865067786025
Epoch: 9745, Batch Gradient Norm after: 22.36067771373145
Epoch 9746/10000, Prediction Accuracy = 60.386%, Loss = 0.6817782878875732
Epoch: 9746, Batch Gradient Norm: 25.39096610311785
Epoch: 9746, Batch Gradient Norm after: 22.36067857993379
Epoch 9747/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.6787196159362793
Epoch: 9747, Batch Gradient Norm: 26.60817436438999
Epoch: 9747, Batch Gradient Norm after: 22.360678060387144
Epoch 9748/10000, Prediction Accuracy = 60.40999999999999%, Loss = 0.6814269304275513
Epoch: 9748, Batch Gradient Norm: 25.392968087628876
Epoch: 9748, Batch Gradient Norm after: 22.360677190297604
Epoch 9749/10000, Prediction Accuracy = 60.372%, Loss = 0.6785400629043579
Epoch: 9749, Batch Gradient Norm: 26.612757700573294
Epoch: 9749, Batch Gradient Norm after: 22.36067805875945
Epoch 9750/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6813255190849304
Epoch: 9750, Batch Gradient Norm: 25.395424929259075
Epoch: 9750, Batch Gradient Norm after: 22.36067705267171
Epoch 9751/10000, Prediction Accuracy = 60.348%, Loss = 0.678640079498291
Epoch: 9751, Batch Gradient Norm: 26.597919103768614
Epoch: 9751, Batch Gradient Norm after: 22.360679363558177
Epoch 9752/10000, Prediction Accuracy = 60.402%, Loss = 0.6813375234603882
Epoch: 9752, Batch Gradient Norm: 25.389750372598375
Epoch: 9752, Batch Gradient Norm after: 22.36067909639622
Epoch 9753/10000, Prediction Accuracy = 60.366%, Loss = 0.6784601092338562
Epoch: 9753, Batch Gradient Norm: 26.59726685186273
Epoch: 9753, Batch Gradient Norm after: 22.36067733282976
Epoch 9754/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.6812404513359069
Epoch: 9754, Batch Gradient Norm: 25.38782638065948
Epoch: 9754, Batch Gradient Norm after: 22.360674776975664
Epoch 9755/10000, Prediction Accuracy = 60.384%, Loss = 0.6784214377403259
Epoch: 9755, Batch Gradient Norm: 26.606407386754977
Epoch: 9755, Batch Gradient Norm after: 22.36067769088195
Epoch 9756/10000, Prediction Accuracy = 60.36999999999999%, Loss = 0.6814164161682129
Epoch: 9756, Batch Gradient Norm: 25.374632991629607
Epoch: 9756, Batch Gradient Norm after: 22.360677531109197
Epoch 9757/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.678558099269867
Epoch: 9757, Batch Gradient Norm: 26.60040461499208
Epoch: 9757, Batch Gradient Norm after: 22.360677141773103
Epoch 9758/10000, Prediction Accuracy = 60.388%, Loss = 0.6814695596694946
Epoch: 9758, Batch Gradient Norm: 25.360466407378727
Epoch: 9758, Batch Gradient Norm after: 22.360675392156814
Epoch 9759/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6783881545066833
Epoch: 9759, Batch Gradient Norm: 26.602427461147464
Epoch: 9759, Batch Gradient Norm after: 22.36067736085694
Epoch 9760/10000, Prediction Accuracy = 60.428%, Loss = 0.6811752200126648
Epoch: 9760, Batch Gradient Norm: 25.362533995411077
Epoch: 9760, Batch Gradient Norm after: 22.36067540826641
Epoch 9761/10000, Prediction Accuracy = 60.362%, Loss = 0.6782278418540955
Epoch: 9761, Batch Gradient Norm: 26.606991359280727
Epoch: 9761, Batch Gradient Norm after: 22.360676447564497
Epoch 9762/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6810787081718445
Epoch: 9762, Batch Gradient Norm: 25.360126294821697
Epoch: 9762, Batch Gradient Norm after: 22.36067532504514
Epoch 9763/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6783064126968383
Epoch: 9763, Batch Gradient Norm: 26.592358153378104
Epoch: 9763, Batch Gradient Norm after: 22.360679826538306
Epoch 9764/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6810914397239685
Epoch: 9764, Batch Gradient Norm: 25.354118595425287
Epoch: 9764, Batch Gradient Norm after: 22.360677086801598
Epoch 9765/10000, Prediction Accuracy = 60.372%, Loss = 0.6781368613243103
Epoch: 9765, Batch Gradient Norm: 26.59503717412934
Epoch: 9765, Batch Gradient Norm after: 22.360677670183666
Epoch 9766/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6810002326965332
Epoch: 9766, Batch Gradient Norm: 25.352080621667536
Epoch: 9766, Batch Gradient Norm after: 22.36067554091527
Epoch 9767/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6781025052070617
Epoch: 9767, Batch Gradient Norm: 26.603265859245337
Epoch: 9767, Batch Gradient Norm after: 22.360677770246905
Epoch 9768/10000, Prediction Accuracy = 60.376%, Loss = 0.6812057971954346
Epoch: 9768, Batch Gradient Norm: 25.339720521859206
Epoch: 9768, Batch Gradient Norm after: 22.360677159876186
Epoch 9769/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6782842516899109
Epoch: 9769, Batch Gradient Norm: 26.59601829789732
Epoch: 9769, Batch Gradient Norm after: 22.360675337689365
Epoch 9770/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6812848806381225
Epoch: 9770, Batch Gradient Norm: 25.325630034397587
Epoch: 9770, Batch Gradient Norm after: 22.36067625149005
Epoch 9771/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6780754804611206
Epoch: 9771, Batch Gradient Norm: 26.59600161250267
Epoch: 9771, Batch Gradient Norm after: 22.360677391462918
Epoch 9772/10000, Prediction Accuracy = 60.424%, Loss = 0.6809236168861389
Epoch: 9772, Batch Gradient Norm: 25.330930357621277
Epoch: 9772, Batch Gradient Norm after: 22.360677230852623
Epoch 9773/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6779235363006592
Epoch: 9773, Batch Gradient Norm: 26.60061842775738
Epoch: 9773, Batch Gradient Norm after: 22.360679663707632
Epoch 9774/10000, Prediction Accuracy = 60.4%, Loss = 0.6808391571044922
Epoch: 9774, Batch Gradient Norm: 25.32763071441865
Epoch: 9774, Batch Gradient Norm after: 22.360677449201834
Epoch 9775/10000, Prediction Accuracy = 60.342%, Loss = 0.6780088663101196
Epoch: 9775, Batch Gradient Norm: 26.58450601218705
Epoch: 9775, Batch Gradient Norm after: 22.360676883398376
Epoch 9776/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6808407425880432
Epoch: 9776, Batch Gradient Norm: 25.324814503279764
Epoch: 9776, Batch Gradient Norm after: 22.360677555226275
Epoch 9777/10000, Prediction Accuracy = 60.366%, Loss = 0.6778178572654724
Epoch: 9777, Batch Gradient Norm: 26.588010910744313
Epoch: 9777, Batch Gradient Norm after: 22.360676108913424
Epoch 9778/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6807525992393494
Epoch: 9778, Batch Gradient Norm: 25.32505724180032
Epoch: 9778, Batch Gradient Norm after: 22.36067610804166
Epoch 9779/10000, Prediction Accuracy = 60.374%, Loss = 0.6778040647506713
Epoch: 9779, Batch Gradient Norm: 26.59385253398754
Epoch: 9779, Batch Gradient Norm after: 22.360676060427036
Epoch 9780/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6809348106384278
Epoch: 9780, Batch Gradient Norm: 25.312469908642555
Epoch: 9780, Batch Gradient Norm after: 22.360676316692825
Epoch 9781/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6779347896575928
Epoch: 9781, Batch Gradient Norm: 26.58745221867241
Epoch: 9781, Batch Gradient Norm after: 22.360676260936604
Epoch 9782/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6809622049331665
Epoch: 9782, Batch Gradient Norm: 25.300648874731717
Epoch: 9782, Batch Gradient Norm after: 22.36067767019822
Epoch 9783/10000, Prediction Accuracy = 60.40999999999999%, Loss = 0.6777510166168212
Epoch: 9783, Batch Gradient Norm: 26.590566602384605
Epoch: 9783, Batch Gradient Norm after: 22.360676109051273
Epoch 9784/10000, Prediction Accuracy = 60.45%, Loss = 0.6806641101837159
Epoch: 9784, Batch Gradient Norm: 25.302790593755343
Epoch: 9784, Batch Gradient Norm after: 22.360677670201866
Epoch 9785/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6776185035705566
Epoch: 9785, Batch Gradient Norm: 26.59197352599781
Epoch: 9785, Batch Gradient Norm after: 22.36067474850396
Epoch 9786/10000, Prediction Accuracy = 60.432%, Loss = 0.6805821776390075
Epoch: 9786, Batch Gradient Norm: 25.307033708993643
Epoch: 9786, Batch Gradient Norm after: 22.360674575597162
Epoch 9787/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.677712082862854
Epoch: 9787, Batch Gradient Norm: 26.57499492676675
Epoch: 9787, Batch Gradient Norm after: 22.36068072915445
Epoch 9788/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6805856227874756
Epoch: 9788, Batch Gradient Norm: 25.30422309296497
Epoch: 9788, Batch Gradient Norm after: 22.360678996508987
Epoch 9789/10000, Prediction Accuracy = 60.378%, Loss = 0.6775312423706055
Epoch: 9789, Batch Gradient Norm: 26.57610213028524
Epoch: 9789, Batch Gradient Norm after: 22.360678766317694
Epoch 9790/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6804925203323364
Epoch: 9790, Batch Gradient Norm: 25.30688035686077
Epoch: 9790, Batch Gradient Norm after: 22.360677127901603
Epoch 9791/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6775287866592408
Epoch: 9791, Batch Gradient Norm: 26.581455130795263
Epoch: 9791, Batch Gradient Norm after: 22.360678129201684
Epoch 9792/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6807086825370788
Epoch: 9792, Batch Gradient Norm: 25.294327364441752
Epoch: 9792, Batch Gradient Norm after: 22.360678448340618
Epoch 9793/10000, Prediction Accuracy = 60.398%, Loss = 0.6777007579803467
Epoch: 9793, Batch Gradient Norm: 26.570511980170462
Epoch: 9793, Batch Gradient Norm after: 22.36067766221748
Epoch 9794/10000, Prediction Accuracy = 60.408%, Loss = 0.6807281970977783
Epoch: 9794, Batch Gradient Norm: 25.28656227833759
Epoch: 9794, Batch Gradient Norm after: 22.360676883164185
Epoch 9795/10000, Prediction Accuracy = 60.416%, Loss = 0.6774855852127075
Epoch: 9795, Batch Gradient Norm: 26.571874973991406
Epoch: 9795, Batch Gradient Norm after: 22.360674718185024
Epoch 9796/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6803747534751892
Epoch: 9796, Batch Gradient Norm: 25.290839590488286
Epoch: 9796, Batch Gradient Norm after: 22.36067566300182
Epoch 9797/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6773592948913574
Epoch: 9797, Batch Gradient Norm: 26.57497103177968
Epoch: 9797, Batch Gradient Norm after: 22.360677837747733
Epoch 9798/10000, Prediction Accuracy = 60.42600000000001%, Loss = 0.6803170204162597
Epoch: 9798, Batch Gradient Norm: 25.295563168882705
Epoch: 9798, Batch Gradient Norm after: 22.36067850135444
Epoch 9799/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6774480819702149
Epoch: 9799, Batch Gradient Norm: 26.555821329747904
Epoch: 9799, Batch Gradient Norm after: 22.360676412618787
Epoch 9800/10000, Prediction Accuracy = 60.398%, Loss = 0.680299437046051
Epoch: 9800, Batch Gradient Norm: 25.295717140101566
Epoch: 9800, Batch Gradient Norm after: 22.360680697680294
Epoch 9801/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6772635698318481
Epoch: 9801, Batch Gradient Norm: 26.559453252143033
Epoch: 9801, Batch Gradient Norm after: 22.360674777697525
Epoch 9802/10000, Prediction Accuracy = 60.456%, Loss = 0.6802104830741882
Epoch: 9802, Batch Gradient Norm: 25.300510255875846
Epoch: 9802, Batch Gradient Norm after: 22.360676964122554
Epoch 9803/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6772824645042419
Epoch: 9803, Batch Gradient Norm: 26.563378753925946
Epoch: 9803, Batch Gradient Norm after: 22.36067632349977
Epoch 9804/10000, Prediction Accuracy = 60.376%, Loss = 0.6804109454154968
Epoch: 9804, Batch Gradient Norm: 25.29164924166964
Epoch: 9804, Batch Gradient Norm after: 22.360677530714298
Epoch 9805/10000, Prediction Accuracy = 60.416%, Loss = 0.677430522441864
Epoch: 9805, Batch Gradient Norm: 26.552725594700302
Epoch: 9805, Batch Gradient Norm after: 22.360678250545835
Epoch 9806/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.680401086807251
Epoch: 9806, Batch Gradient Norm: 25.28410773889918
Epoch: 9806, Batch Gradient Norm after: 22.36067811681731
Epoch 9807/10000, Prediction Accuracy = 60.402%, Loss = 0.677225387096405
Epoch: 9807, Batch Gradient Norm: 26.554802042766774
Epoch: 9807, Batch Gradient Norm after: 22.360677028935434
Epoch 9808/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.680078649520874
Epoch: 9808, Batch Gradient Norm: 25.29402733644161
Epoch: 9808, Batch Gradient Norm after: 22.36067682794987
Epoch 9809/10000, Prediction Accuracy = 60.35%, Loss = 0.6771302461624146
Epoch: 9809, Batch Gradient Norm: 26.553685244422624
Epoch: 9809, Batch Gradient Norm after: 22.360678966234786
Epoch 9810/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6800217628479004
Epoch: 9810, Batch Gradient Norm: 25.299231145224976
Epoch: 9810, Batch Gradient Norm after: 22.360677700903803
Epoch 9811/10000, Prediction Accuracy = 60.354%, Loss = 0.677216398715973
Epoch: 9811, Batch Gradient Norm: 26.536312073199323
Epoch: 9811, Batch Gradient Norm after: 22.360676854985517
Epoch 9812/10000, Prediction Accuracy = 60.396%, Loss = 0.6800018429756165
Epoch: 9812, Batch Gradient Norm: 25.299806000685944
Epoch: 9812, Batch Gradient Norm after: 22.36067860858262
Epoch 9813/10000, Prediction Accuracy = 60.366%, Loss = 0.6770331501960755
Epoch: 9813, Batch Gradient Norm: 26.537918113641602
Epoch: 9813, Batch Gradient Norm after: 22.360676172323853
Epoch 9814/10000, Prediction Accuracy = 60.46%, Loss = 0.6799212217330932
Epoch: 9814, Batch Gradient Norm: 25.297959958446874
Epoch: 9814, Batch Gradient Norm after: 22.36067663931121
Epoch 9815/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.677056348323822
Epoch: 9815, Batch Gradient Norm: 26.544527587657825
Epoch: 9815, Batch Gradient Norm after: 22.360676403149192
Epoch 9816/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6801624774932862
Epoch: 9816, Batch Gradient Norm: 25.28479123318493
Epoch: 9816, Batch Gradient Norm after: 22.360679314420896
Epoch 9817/10000, Prediction Accuracy = 60.40999999999999%, Loss = 0.6772104024887085
Epoch: 9817, Batch Gradient Norm: 26.532805764042507
Epoch: 9817, Batch Gradient Norm after: 22.360676561337684
Epoch 9818/10000, Prediction Accuracy = 60.378%, Loss = 0.6801308989524841
Epoch: 9818, Batch Gradient Norm: 25.282034545885733
Epoch: 9818, Batch Gradient Norm after: 22.360675919427884
Epoch 9819/10000, Prediction Accuracy = 60.4%, Loss = 0.6769723653793335
Epoch: 9819, Batch Gradient Norm: 26.53572501490977
Epoch: 9819, Batch Gradient Norm after: 22.36067571643913
Epoch 9820/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6797832131385804
Epoch: 9820, Batch Gradient Norm: 25.285664802607393
Epoch: 9820, Batch Gradient Norm after: 22.36067984621269
Epoch 9821/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6768844604492188
Epoch: 9821, Batch Gradient Norm: 26.534778769111558
Epoch: 9821, Batch Gradient Norm after: 22.360677180765222
Epoch 9822/10000, Prediction Accuracy = 60.426%, Loss = 0.6797530651092529
Epoch: 9822, Batch Gradient Norm: 25.285634540904038
Epoch: 9822, Batch Gradient Norm after: 22.360678509000525
Epoch 9823/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6769508004188538
Epoch: 9823, Batch Gradient Norm: 26.51953121955285
Epoch: 9823, Batch Gradient Norm after: 22.36067799215637
Epoch 9824/10000, Prediction Accuracy = 60.416%, Loss = 0.679724109172821
Epoch: 9824, Batch Gradient Norm: 25.28645761174285
Epoch: 9824, Batch Gradient Norm after: 22.36067866341904
Epoch 9825/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6767618656158447
Epoch: 9825, Batch Gradient Norm: 26.522233724705252
Epoch: 9825, Batch Gradient Norm after: 22.360677305123563
Epoch 9826/10000, Prediction Accuracy = 60.46%, Loss = 0.679658031463623
Epoch: 9826, Batch Gradient Norm: 25.28932751430389
Epoch: 9826, Batch Gradient Norm after: 22.360675365311344
Epoch 9827/10000, Prediction Accuracy = 60.402%, Loss = 0.6768043279647827
Epoch: 9827, Batch Gradient Norm: 26.527049110999286
Epoch: 9827, Batch Gradient Norm after: 22.36067636547034
Epoch 9828/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6798705101013184
Epoch: 9828, Batch Gradient Norm: 25.2732633336852
Epoch: 9828, Batch Gradient Norm after: 22.360677022787293
Epoch 9829/10000, Prediction Accuracy = 60.428%, Loss = 0.6769261240959168
Epoch: 9829, Batch Gradient Norm: 26.5181241336911
Epoch: 9829, Batch Gradient Norm after: 22.360676844769475
Epoch 9830/10000, Prediction Accuracy = 60.364%, Loss = 0.6798273801803589
Epoch: 9830, Batch Gradient Norm: 25.26609444510038
Epoch: 9830, Batch Gradient Norm after: 22.360676078318438
Epoch 9831/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6766993999481201
Epoch: 9831, Batch Gradient Norm: 26.521716316011798
Epoch: 9831, Batch Gradient Norm after: 22.360678072832226
Epoch 9832/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.6795078396797181
Epoch: 9832, Batch Gradient Norm: 25.277036140667246
Epoch: 9832, Batch Gradient Norm after: 22.360677794373625
Epoch 9833/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6766280770301819
Epoch: 9833, Batch Gradient Norm: 26.52073020871519
Epoch: 9833, Batch Gradient Norm after: 22.360680245025378
Epoch 9834/10000, Prediction Accuracy = 60.424%, Loss = 0.6794801354408264
Epoch: 9834, Batch Gradient Norm: 25.274872903291477
Epoch: 9834, Batch Gradient Norm after: 22.360677332392445
Epoch 9835/10000, Prediction Accuracy = 60.35%, Loss = 0.6766883492469787
Epoch: 9835, Batch Gradient Norm: 26.505178444870253
Epoch: 9835, Batch Gradient Norm after: 22.3606764090913
Epoch 9836/10000, Prediction Accuracy = 60.42%, Loss = 0.6794525265693665
Epoch: 9836, Batch Gradient Norm: 25.274515754542357
Epoch: 9836, Batch Gradient Norm after: 22.360679733404044
Epoch 9837/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6765018820762634
Epoch: 9837, Batch Gradient Norm: 26.508057609746313
Epoch: 9837, Batch Gradient Norm after: 22.36067895060253
Epoch 9838/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6793928980827332
Epoch: 9838, Batch Gradient Norm: 25.27512382010359
Epoch: 9838, Batch Gradient Norm after: 22.36067553613218
Epoch 9839/10000, Prediction Accuracy = 60.412%, Loss = 0.6765583157539368
Epoch: 9839, Batch Gradient Norm: 26.512375109810364
Epoch: 9839, Batch Gradient Norm after: 22.360676769977378
Epoch 9840/10000, Prediction Accuracy = 60.348%, Loss = 0.6796328783035278
Epoch: 9840, Batch Gradient Norm: 25.263912721010335
Epoch: 9840, Batch Gradient Norm after: 22.360678629774306
Epoch 9841/10000, Prediction Accuracy = 60.444%, Loss = 0.6766849279403686
Epoch: 9841, Batch Gradient Norm: 26.501264788903224
Epoch: 9841, Batch Gradient Norm after: 22.36067783131602
Epoch 9842/10000, Prediction Accuracy = 60.35%, Loss = 0.6795485496520997
Epoch: 9842, Batch Gradient Norm: 25.25547222334102
Epoch: 9842, Batch Gradient Norm after: 22.360679393231628
Epoch 9843/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6764276027679443
Epoch: 9843, Batch Gradient Norm: 26.506046368531248
Epoch: 9843, Batch Gradient Norm after: 22.36067568362659
Epoch 9844/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6792276263236999
Epoch: 9844, Batch Gradient Norm: 25.26412922144466
Epoch: 9844, Batch Gradient Norm after: 22.36067712531471
Epoch 9845/10000, Prediction Accuracy = 60.338%, Loss = 0.6763717889785766
Epoch: 9845, Batch Gradient Norm: 26.504013786392136
Epoch: 9845, Batch Gradient Norm after: 22.36067763772931
Epoch 9846/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.67920902967453
Epoch: 9846, Batch Gradient Norm: 25.26504986781551
Epoch: 9846, Batch Gradient Norm after: 22.360676636060408
Epoch 9847/10000, Prediction Accuracy = 60.35%, Loss = 0.6764266967773438
Epoch: 9847, Batch Gradient Norm: 26.489382266185697
Epoch: 9847, Batch Gradient Norm after: 22.360679722535426
Epoch 9848/10000, Prediction Accuracy = 60.398%, Loss = 0.6791681528091431
Epoch: 9848, Batch Gradient Norm: 25.26583310495048
Epoch: 9848, Batch Gradient Norm after: 22.36067869634605
Epoch 9849/10000, Prediction Accuracy = 60.4%, Loss = 0.676241135597229
Epoch: 9849, Batch Gradient Norm: 26.492822914059822
Epoch: 9849, Batch Gradient Norm after: 22.36067525516562
Epoch 9850/10000, Prediction Accuracy = 60.459999999999994%, Loss = 0.679122245311737
Epoch: 9850, Batch Gradient Norm: 25.273625404859818
Epoch: 9850, Batch Gradient Norm after: 22.360678681159264
Epoch 9851/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6763233542442322
Epoch: 9851, Batch Gradient Norm: 26.48983039012209
Epoch: 9851, Batch Gradient Norm after: 22.360676260735396
Epoch 9852/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6793453454971313
Epoch: 9852, Batch Gradient Norm: 25.265249762433506
Epoch: 9852, Batch Gradient Norm after: 22.36068023196136
Epoch 9853/10000, Prediction Accuracy = 60.436%, Loss = 0.6764389038085937
Epoch: 9853, Batch Gradient Norm: 26.47966684336153
Epoch: 9853, Batch Gradient Norm after: 22.36067622715246
Epoch 9854/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6792269825935364
Epoch: 9854, Batch Gradient Norm: 25.260455196015943
Epoch: 9854, Batch Gradient Norm after: 22.360677329234008
Epoch 9855/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6761979222297668
Epoch: 9855, Batch Gradient Norm: 26.48638235986018
Epoch: 9855, Batch Gradient Norm after: 22.360675298895778
Epoch 9856/10000, Prediction Accuracy = 60.484%, Loss = 0.6789222240447998
Epoch: 9856, Batch Gradient Norm: 25.269675927207985
Epoch: 9856, Batch Gradient Norm after: 22.360679078941576
Epoch 9857/10000, Prediction Accuracy = 60.36%, Loss = 0.6761610627174377
Epoch: 9857, Batch Gradient Norm: 26.481546235997705
Epoch: 9857, Batch Gradient Norm after: 22.360676951853694
Epoch 9858/10000, Prediction Accuracy = 60.414%, Loss = 0.6789233207702636
Epoch: 9858, Batch Gradient Norm: 25.268453382738112
Epoch: 9858, Batch Gradient Norm after: 22.360676996767353
Epoch 9859/10000, Prediction Accuracy = 60.352%, Loss = 0.6761992692947387
Epoch: 9859, Batch Gradient Norm: 26.468383254685182
Epoch: 9859, Batch Gradient Norm after: 22.360675732384504
Epoch 9860/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6788632988929748
Epoch: 9860, Batch Gradient Norm: 25.271029939427653
Epoch: 9860, Batch Gradient Norm after: 22.360676687306494
Epoch 9861/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6760170221328735
Epoch: 9861, Batch Gradient Norm: 26.475009369685978
Epoch: 9861, Batch Gradient Norm after: 22.36067552422213
Epoch 9862/10000, Prediction Accuracy = 60.464%, Loss = 0.6788401961326599
Epoch: 9862, Batch Gradient Norm: 25.275277829420254
Epoch: 9862, Batch Gradient Norm after: 22.360675134001443
Epoch 9863/10000, Prediction Accuracy = 60.42%, Loss = 0.6761081695556641
Epoch: 9863, Batch Gradient Norm: 26.471717072242594
Epoch: 9863, Batch Gradient Norm after: 22.36067738660225
Epoch 9864/10000, Prediction Accuracy = 60.34000000000001%, Loss = 0.6790759563446045
Epoch: 9864, Batch Gradient Norm: 25.26182000664622
Epoch: 9864, Batch Gradient Norm after: 22.360678054000545
Epoch 9865/10000, Prediction Accuracy = 60.428%, Loss = 0.6762024998664856
Epoch: 9865, Batch Gradient Norm: 26.464262730334333
Epoch: 9865, Batch Gradient Norm after: 22.360677084745692
Epoch 9866/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6789241552352905
Epoch: 9866, Batch Gradient Norm: 25.25754184060004
Epoch: 9866, Batch Gradient Norm after: 22.36067571837936
Epoch 9867/10000, Prediction Accuracy = 60.416%, Loss = 0.6759437799453736
Epoch: 9867, Batch Gradient Norm: 26.468582747823753
Epoch: 9867, Batch Gradient Norm after: 22.36067671193855
Epoch 9868/10000, Prediction Accuracy = 60.465999999999994%, Loss = 0.678631067276001
Epoch: 9868, Batch Gradient Norm: 25.271677070371368
Epoch: 9868, Batch Gradient Norm after: 22.36067736899712
Epoch 9869/10000, Prediction Accuracy = 60.36%, Loss = 0.6759329199790954
Epoch: 9869, Batch Gradient Norm: 26.463240871703185
Epoch: 9869, Batch Gradient Norm after: 22.360677718505325
Epoch 9870/10000, Prediction Accuracy = 60.412%, Loss = 0.6786337256431579
Epoch: 9870, Batch Gradient Norm: 25.27239428725194
Epoch: 9870, Batch Gradient Norm after: 22.360675995668952
Epoch 9871/10000, Prediction Accuracy = 60.352%, Loss = 0.675972843170166
Epoch: 9871, Batch Gradient Norm: 26.446855962854723
Epoch: 9871, Batch Gradient Norm after: 22.36067601317969
Epoch 9872/10000, Prediction Accuracy = 60.402%, Loss = 0.6785668969154358
Epoch: 9872, Batch Gradient Norm: 25.27609806276501
Epoch: 9872, Batch Gradient Norm after: 22.360678771496158
Epoch 9873/10000, Prediction Accuracy = 60.42800000000001%, Loss = 0.6757930278778076
Epoch: 9873, Batch Gradient Norm: 26.455558049822148
Epoch: 9873, Batch Gradient Norm after: 22.36067780365807
Epoch 9874/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6785450935363769
Epoch: 9874, Batch Gradient Norm: 25.27886077704521
Epoch: 9874, Batch Gradient Norm after: 22.36067594936486
Epoch 9875/10000, Prediction Accuracy = 60.414%, Loss = 0.6758935213088989
Epoch: 9875, Batch Gradient Norm: 26.452069528906776
Epoch: 9875, Batch Gradient Norm after: 22.360677155078374
Epoch 9876/10000, Prediction Accuracy = 60.336%, Loss = 0.6787913084030152
Epoch: 9876, Batch Gradient Norm: 25.262048755412142
Epoch: 9876, Batch Gradient Norm after: 22.36067785872121
Epoch 9877/10000, Prediction Accuracy = 60.426%, Loss = 0.675969922542572
Epoch: 9877, Batch Gradient Norm: 26.445718463214604
Epoch: 9877, Batch Gradient Norm after: 22.360676676672078
Epoch 9878/10000, Prediction Accuracy = 60.336%, Loss = 0.6786283493041992
Epoch: 9878, Batch Gradient Norm: 25.257854815347276
Epoch: 9878, Batch Gradient Norm after: 22.360675781744526
Epoch 9879/10000, Prediction Accuracy = 60.424%, Loss = 0.675705862045288
Epoch: 9879, Batch Gradient Norm: 26.449865160779648
Epoch: 9879, Batch Gradient Norm after: 22.360678567317727
Epoch 9880/10000, Prediction Accuracy = 60.468%, Loss = 0.6783447623252868
Epoch: 9880, Batch Gradient Norm: 25.267446934013538
Epoch: 9880, Batch Gradient Norm after: 22.360678343178463
Epoch 9881/10000, Prediction Accuracy = 60.364%, Loss = 0.6756944060325623
Epoch: 9881, Batch Gradient Norm: 26.444986812243357
Epoch: 9881, Batch Gradient Norm after: 22.360676409415184
Epoch 9882/10000, Prediction Accuracy = 60.419999999999995%, Loss = 0.6783546447753906
Epoch: 9882, Batch Gradient Norm: 25.267677881676608
Epoch: 9882, Batch Gradient Norm after: 22.36067659154323
Epoch 9883/10000, Prediction Accuracy = 60.348%, Loss = 0.6757182121276856
Epoch: 9883, Batch Gradient Norm: 26.431611765835587
Epoch: 9883, Batch Gradient Norm after: 22.360675757132217
Epoch 9884/10000, Prediction Accuracy = 60.396%, Loss = 0.678279185295105
Epoch: 9884, Batch Gradient Norm: 25.27117913122935
Epoch: 9884, Batch Gradient Norm after: 22.36067749072351
Epoch 9885/10000, Prediction Accuracy = 60.42600000000001%, Loss = 0.6755463480949402
Epoch: 9885, Batch Gradient Norm: 26.43960017784654
Epoch: 9885, Batch Gradient Norm after: 22.360678220958988
Epoch 9886/10000, Prediction Accuracy = 60.48%, Loss = 0.6782667756080627
Epoch: 9886, Batch Gradient Norm: 25.274549859732623
Epoch: 9886, Batch Gradient Norm after: 22.360678543546058
Epoch 9887/10000, Prediction Accuracy = 60.42%, Loss = 0.67564457654953
Epoch: 9887, Batch Gradient Norm: 26.434028571491318
Epoch: 9887, Batch Gradient Norm after: 22.36067566995485
Epoch 9888/10000, Prediction Accuracy = 60.324%, Loss = 0.6785037398338318
Epoch: 9888, Batch Gradient Norm: 25.260537225710824
Epoch: 9888, Batch Gradient Norm after: 22.360676446086174
Epoch 9889/10000, Prediction Accuracy = 60.424%, Loss = 0.6757259726524353
Epoch: 9889, Batch Gradient Norm: 26.42702709190211
Epoch: 9889, Batch Gradient Norm after: 22.360678191245704
Epoch 9890/10000, Prediction Accuracy = 60.334%, Loss = 0.6783528208732605
Epoch: 9890, Batch Gradient Norm: 25.25461237506983
Epoch: 9890, Batch Gradient Norm after: 22.36067608510701
Epoch 9891/10000, Prediction Accuracy = 60.42%, Loss = 0.6754679203033447
Epoch: 9891, Batch Gradient Norm: 26.43306607475788
Epoch: 9891, Batch Gradient Norm after: 22.3606778727409
Epoch 9892/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6780674815177917
Epoch: 9892, Batch Gradient Norm: 25.26595476524382
Epoch: 9892, Batch Gradient Norm after: 22.360678731649475
Epoch 9893/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6754517674446106
Epoch: 9893, Batch Gradient Norm: 26.426028473574206
Epoch: 9893, Batch Gradient Norm after: 22.36067926783904
Epoch 9894/10000, Prediction Accuracy = 60.412%, Loss = 0.6780769467353821
Epoch: 9894, Batch Gradient Norm: 25.261538554156264
Epoch: 9894, Batch Gradient Norm after: 22.360676544691948
Epoch 9895/10000, Prediction Accuracy = 60.348%, Loss = 0.6754896879196167
Epoch: 9895, Batch Gradient Norm: 26.411898279780587
Epoch: 9895, Batch Gradient Norm after: 22.36067651196896
Epoch 9896/10000, Prediction Accuracy = 60.4%, Loss = 0.6780089616775513
Epoch: 9896, Batch Gradient Norm: 25.266182175444246
Epoch: 9896, Batch Gradient Norm after: 22.360679978026432
Epoch 9897/10000, Prediction Accuracy = 60.44%, Loss = 0.6752964973449707
Epoch: 9897, Batch Gradient Norm: 26.42157699656415
Epoch: 9897, Batch Gradient Norm after: 22.360677784099234
Epoch 9898/10000, Prediction Accuracy = 60.472%, Loss = 0.6779965996742249
Epoch: 9898, Batch Gradient Norm: 25.267182320086032
Epoch: 9898, Batch Gradient Norm after: 22.36067784214721
Epoch 9899/10000, Prediction Accuracy = 60.426%, Loss = 0.6753960251808167
Epoch: 9899, Batch Gradient Norm: 26.41992207507458
Epoch: 9899, Batch Gradient Norm after: 22.36067548695222
Epoch 9900/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6782378673553466
Epoch: 9900, Batch Gradient Norm: 25.24971723192993
Epoch: 9900, Batch Gradient Norm after: 22.3606766002423
Epoch 9901/10000, Prediction Accuracy = 60.436%, Loss = 0.6754668474197387
Epoch: 9901, Batch Gradient Norm: 26.41097819945757
Epoch: 9901, Batch Gradient Norm after: 22.360676201338034
Epoch 9902/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6780724883079529
Epoch: 9902, Batch Gradient Norm: 25.241156006119525
Epoch: 9902, Batch Gradient Norm after: 22.360677299352773
Epoch 9903/10000, Prediction Accuracy = 60.428%, Loss = 0.6751941442489624
Epoch: 9903, Batch Gradient Norm: 26.41768052533642
Epoch: 9903, Batch Gradient Norm after: 22.36067696921096
Epoch 9904/10000, Prediction Accuracy = 60.480000000000004%, Loss = 0.6777979969978333
Epoch: 9904, Batch Gradient Norm: 25.252007599655553
Epoch: 9904, Batch Gradient Norm after: 22.360677777689077
Epoch 9905/10000, Prediction Accuracy = 60.354%, Loss = 0.6751828551292419
Epoch: 9905, Batch Gradient Norm: 26.412209024985856
Epoch: 9905, Batch Gradient Norm after: 22.360678218418773
Epoch 9906/10000, Prediction Accuracy = 60.426%, Loss = 0.6778016328811646
Epoch: 9906, Batch Gradient Norm: 25.24991424232422
Epoch: 9906, Batch Gradient Norm after: 22.360677702497316
Epoch 9907/10000, Prediction Accuracy = 60.372%, Loss = 0.6752104043960572
Epoch: 9907, Batch Gradient Norm: 26.39943720601816
Epoch: 9907, Batch Gradient Norm after: 22.360678065723235
Epoch 9908/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6777332901954651
Epoch: 9908, Batch Gradient Norm: 25.252567524350464
Epoch: 9908, Batch Gradient Norm after: 22.36067937499091
Epoch 9909/10000, Prediction Accuracy = 60.424%, Loss = 0.6750295758247375
Epoch: 9909, Batch Gradient Norm: 26.409324695276133
Epoch: 9909, Batch Gradient Norm after: 22.360676145962465
Epoch 9910/10000, Prediction Accuracy = 60.46199999999999%, Loss = 0.6777309298515319
Epoch: 9910, Batch Gradient Norm: 25.244939766279987
Epoch: 9910, Batch Gradient Norm after: 22.36067538582411
Epoch 9911/10000, Prediction Accuracy = 60.416%, Loss = 0.675112521648407
Epoch: 9911, Batch Gradient Norm: 26.40822414471496
Epoch: 9911, Batch Gradient Norm after: 22.36067624475189
Epoch 9912/10000, Prediction Accuracy = 60.322%, Loss = 0.6779851913452148
Epoch: 9912, Batch Gradient Norm: 25.22589036812965
Epoch: 9912, Batch Gradient Norm after: 22.36067511600996
Epoch 9913/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6751835107803345
Epoch: 9913, Batch Gradient Norm: 26.401301053959187
Epoch: 9913, Batch Gradient Norm after: 22.360676112070998
Epoch 9914/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6778204917907715
Epoch: 9914, Batch Gradient Norm: 25.2165817550556
Epoch: 9914, Batch Gradient Norm after: 22.360677449579335
Epoch 9915/10000, Prediction Accuracy = 60.426%, Loss = 0.6749120116233825
Epoch: 9915, Batch Gradient Norm: 26.409075390259876
Epoch: 9915, Batch Gradient Norm after: 22.36067952583255
Epoch 9916/10000, Prediction Accuracy = 60.492%, Loss = 0.6775412440299988
Epoch: 9916, Batch Gradient Norm: 25.227565927340308
Epoch: 9916, Batch Gradient Norm after: 22.360678350143605
Epoch 9917/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6749031066894531
Epoch: 9917, Batch Gradient Norm: 26.402103008674512
Epoch: 9917, Batch Gradient Norm after: 22.360679165244505
Epoch 9918/10000, Prediction Accuracy = 60.424%, Loss = 0.6775612592697143
Epoch: 9918, Batch Gradient Norm: 25.22337690469881
Epoch: 9918, Batch Gradient Norm after: 22.36067698939448
Epoch 9919/10000, Prediction Accuracy = 60.374%, Loss = 0.674927830696106
Epoch: 9919, Batch Gradient Norm: 26.388198693060772
Epoch: 9919, Batch Gradient Norm after: 22.360675880469177
Epoch 9920/10000, Prediction Accuracy = 60.414%, Loss = 0.677486264705658
Epoch: 9920, Batch Gradient Norm: 25.225747184757665
Epoch: 9920, Batch Gradient Norm after: 22.36067801558527
Epoch 9921/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6747375011444092
Epoch: 9921, Batch Gradient Norm: 26.396653031872614
Epoch: 9921, Batch Gradient Norm after: 22.36067633605742
Epoch 9922/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6774787664413452
Epoch: 9922, Batch Gradient Norm: 25.22554563840175
Epoch: 9922, Batch Gradient Norm after: 22.360677812862512
Epoch 9923/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6748487949371338
Epoch: 9923, Batch Gradient Norm: 26.39283250761007
Epoch: 9923, Batch Gradient Norm after: 22.360675933610505
Epoch 9924/10000, Prediction Accuracy = 60.324%, Loss = 0.6777396321296691
Epoch: 9924, Batch Gradient Norm: 25.20505245967111
Epoch: 9924, Batch Gradient Norm after: 22.3606782928749
Epoch 9925/10000, Prediction Accuracy = 60.44%, Loss = 0.6749148488044738
Epoch: 9925, Batch Gradient Norm: 26.38857387213821
Epoch: 9925, Batch Gradient Norm after: 22.36067836299741
Epoch 9926/10000, Prediction Accuracy = 60.338%, Loss = 0.6775469303131103
Epoch: 9926, Batch Gradient Norm: 25.202398356651177
Epoch: 9926, Batch Gradient Norm after: 22.360676983688734
Epoch 9927/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6746386528015137
Epoch: 9927, Batch Gradient Norm: 26.393083470230202
Epoch: 9927, Batch Gradient Norm after: 22.36068016026362
Epoch 9928/10000, Prediction Accuracy = 60.484%, Loss = 0.6772681713104248
Epoch: 9928, Batch Gradient Norm: 25.212542875184244
Epoch: 9928, Batch Gradient Norm after: 22.360678176827406
Epoch 9929/10000, Prediction Accuracy = 60.36%, Loss = 0.6746253371238708
Epoch: 9929, Batch Gradient Norm: 26.389396194681037
Epoch: 9929, Batch Gradient Norm after: 22.36067670702696
Epoch 9930/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6772786498069763
Epoch: 9930, Batch Gradient Norm: 25.209626919194296
Epoch: 9930, Batch Gradient Norm after: 22.360677295876492
Epoch 9931/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6746501445770263
Epoch: 9931, Batch Gradient Norm: 26.37709973160396
Epoch: 9931, Batch Gradient Norm after: 22.36067772732457
Epoch 9932/10000, Prediction Accuracy = 60.419999999999995%, Loss = 0.677211058139801
Epoch: 9932, Batch Gradient Norm: 25.21040319431332
Epoch: 9932, Batch Gradient Norm after: 22.360678215340865
Epoch 9933/10000, Prediction Accuracy = 60.42%, Loss = 0.6744693636894226
Epoch: 9933, Batch Gradient Norm: 26.38291277330771
Epoch: 9933, Batch Gradient Norm after: 22.360676590506326
Epoch 9934/10000, Prediction Accuracy = 60.46%, Loss = 0.6772103786468506
Epoch: 9934, Batch Gradient Norm: 25.208981298063613
Epoch: 9934, Batch Gradient Norm after: 22.360675612417445
Epoch 9935/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6745646357536316
Epoch: 9935, Batch Gradient Norm: 26.381637464237695
Epoch: 9935, Batch Gradient Norm after: 22.36067696917834
Epoch 9936/10000, Prediction Accuracy = 60.336%, Loss = 0.6774627089500427
Epoch: 9936, Batch Gradient Norm: 25.19036017048177
Epoch: 9936, Batch Gradient Norm after: 22.36067654328467
Epoch 9937/10000, Prediction Accuracy = 60.438%, Loss = 0.6746241211891174
Epoch: 9937, Batch Gradient Norm: 26.375579799317745
Epoch: 9937, Batch Gradient Norm after: 22.360676316352382
Epoch 9938/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.67728670835495
Epoch: 9938, Batch Gradient Norm: 25.184121866307915
Epoch: 9938, Batch Gradient Norm after: 22.360675399618895
Epoch 9939/10000, Prediction Accuracy = 60.436%, Loss = 0.6743645668029785
Epoch: 9939, Batch Gradient Norm: 26.38174781402417
Epoch: 9939, Batch Gradient Norm after: 22.360679378641684
Epoch 9940/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.6770115852355957
Epoch: 9940, Batch Gradient Norm: 25.192602999036154
Epoch: 9940, Batch Gradient Norm after: 22.360678343011326
Epoch 9941/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6743557095527649
Epoch: 9941, Batch Gradient Norm: 26.37709180981947
Epoch: 9941, Batch Gradient Norm after: 22.360678482960974
Epoch 9942/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6770345091819763
Epoch: 9942, Batch Gradient Norm: 25.18714840833042
Epoch: 9942, Batch Gradient Norm after: 22.360677170337304
Epoch 9943/10000, Prediction Accuracy = 60.374%, Loss = 0.6743878722190857
Epoch: 9943, Batch Gradient Norm: 26.363062287042037
Epoch: 9943, Batch Gradient Norm after: 22.360676738378846
Epoch 9944/10000, Prediction Accuracy = 60.43399999999999%, Loss = 0.6769661426544189
Epoch: 9944, Batch Gradient Norm: 25.188240124743224
Epoch: 9944, Batch Gradient Norm after: 22.360676607306186
Epoch 9945/10000, Prediction Accuracy = 60.42%, Loss = 0.6741819143295288
Epoch: 9945, Batch Gradient Norm: 26.37139694720225
Epoch: 9945, Batch Gradient Norm after: 22.360677093685894
Epoch 9946/10000, Prediction Accuracy = 60.468%, Loss = 0.6769579291343689
Epoch: 9946, Batch Gradient Norm: 25.18555539569784
Epoch: 9946, Batch Gradient Norm after: 22.36067797744044
Epoch 9947/10000, Prediction Accuracy = 60.412%, Loss = 0.6742732763290405
Epoch: 9947, Batch Gradient Norm: 26.37301824205482
Epoch: 9947, Batch Gradient Norm after: 22.360676985397177
Epoch 9948/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.6772345662117004
Epoch: 9948, Batch Gradient Norm: 25.16361761773897
Epoch: 9948, Batch Gradient Norm after: 22.360675954514505
Epoch 9949/10000, Prediction Accuracy = 60.448%, Loss = 0.6743470191955566
Epoch: 9949, Batch Gradient Norm: 26.36778091332213
Epoch: 9949, Batch Gradient Norm after: 22.360678278531058
Epoch 9950/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.677044153213501
Epoch: 9950, Batch Gradient Norm: 25.156997241216796
Epoch: 9950, Batch Gradient Norm after: 22.360678394375526
Epoch 9951/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6740647912025451
Epoch: 9951, Batch Gradient Norm: 26.37385584134915
Epoch: 9951, Batch Gradient Norm after: 22.36067895614768
Epoch 9952/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.6767677545547486
Epoch: 9952, Batch Gradient Norm: 25.161662113919466
Epoch: 9952, Batch Gradient Norm after: 22.360675657001416
Epoch 9953/10000, Prediction Accuracy = 60.366%, Loss = 0.6740408897399902
Epoch: 9953, Batch Gradient Norm: 26.36893133413427
Epoch: 9953, Batch Gradient Norm after: 22.360678401627233
Epoch 9954/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6767754077911377
Epoch: 9954, Batch Gradient Norm: 25.160135628786794
Epoch: 9954, Batch Gradient Norm after: 22.36067625980812
Epoch 9955/10000, Prediction Accuracy = 60.378%, Loss = 0.6740814924240113
Epoch: 9955, Batch Gradient Norm: 26.356543966814773
Epoch: 9955, Batch Gradient Norm after: 22.36067590932467
Epoch 9956/10000, Prediction Accuracy = 60.438%, Loss = 0.676713228225708
Epoch: 9956, Batch Gradient Norm: 25.158514221977534
Epoch: 9956, Batch Gradient Norm after: 22.36067871646565
Epoch 9957/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6738874793052674
Epoch: 9957, Batch Gradient Norm: 26.36542751856531
Epoch: 9957, Batch Gradient Norm after: 22.360676211713667
Epoch 9958/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6767022609710693
Epoch: 9958, Batch Gradient Norm: 25.158374249099012
Epoch: 9958, Batch Gradient Norm after: 22.360675047545154
Epoch 9959/10000, Prediction Accuracy = 60.424%, Loss = 0.67396399974823
Epoch: 9959, Batch Gradient Norm: 26.364535398570627
Epoch: 9959, Batch Gradient Norm after: 22.360677396138705
Epoch 9960/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6769531726837158
Epoch: 9960, Batch Gradient Norm: 25.143126123753248
Epoch: 9960, Batch Gradient Norm after: 22.36067618769437
Epoch 9961/10000, Prediction Accuracy = 60.45%, Loss = 0.6740652203559876
Epoch: 9961, Batch Gradient Norm: 26.356354633401878
Epoch: 9961, Batch Gradient Norm after: 22.360677868016136
Epoch 9962/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6768066048622131
Epoch: 9962, Batch Gradient Norm: 25.13207416955075
Epoch: 9962, Batch Gradient Norm after: 22.360678274456983
Epoch 9963/10000, Prediction Accuracy = 60.44199999999999%, Loss = 0.6737910151481629
Epoch: 9963, Batch Gradient Norm: 26.364386736535646
Epoch: 9963, Batch Gradient Norm after: 22.360678381720657
Epoch 9964/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.6765180706977845
Epoch: 9964, Batch Gradient Norm: 25.13934003144882
Epoch: 9964, Batch Gradient Norm after: 22.36067868499993
Epoch 9965/10000, Prediction Accuracy = 60.372%, Loss = 0.6737651348114013
Epoch: 9965, Batch Gradient Norm: 26.358787704699154
Epoch: 9965, Batch Gradient Norm after: 22.360678352936503
Epoch 9966/10000, Prediction Accuracy = 60.432%, Loss = 0.6765136361122132
Epoch: 9966, Batch Gradient Norm: 25.136643528216517
Epoch: 9966, Batch Gradient Norm after: 22.36067624850167
Epoch 9967/10000, Prediction Accuracy = 60.362%, Loss = 0.6738235592842102
Epoch: 9967, Batch Gradient Norm: 26.34471787378937
Epoch: 9967, Batch Gradient Norm after: 22.36067901509386
Epoch 9968/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6764804244041442
Epoch: 9968, Batch Gradient Norm: 25.13498305987401
Epoch: 9968, Batch Gradient Norm after: 22.360677955356785
Epoch 9969/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6736102819442749
Epoch: 9969, Batch Gradient Norm: 26.352245917995933
Epoch: 9969, Batch Gradient Norm after: 22.360677292938103
Epoch 9970/10000, Prediction Accuracy = 60.48600000000001%, Loss = 0.6764309048652649
Epoch: 9970, Batch Gradient Norm: 25.139216541053226
Epoch: 9970, Batch Gradient Norm after: 22.360676223102804
Epoch 9971/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6736762881278991
Epoch: 9971, Batch Gradient Norm: 26.351152202792317
Epoch: 9971, Batch Gradient Norm after: 22.36067709800109
Epoch 9972/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.676695954799652
Epoch: 9972, Batch Gradient Norm: 25.121866862090705
Epoch: 9972, Batch Gradient Norm after: 22.36067606460177
Epoch 9973/10000, Prediction Accuracy = 60.456%, Loss = 0.6738012790679931
Epoch: 9973, Batch Gradient Norm: 26.344342875469174
Epoch: 9973, Batch Gradient Norm after: 22.3606784619768
Epoch 9974/10000, Prediction Accuracy = 60.36%, Loss = 0.6765787363052368
Epoch: 9974, Batch Gradient Norm: 25.112267310049557
Epoch: 9974, Batch Gradient Norm after: 22.360678731974502
Epoch 9975/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6735265135765076
Epoch: 9975, Batch Gradient Norm: 26.3486887779563
Epoch: 9975, Batch Gradient Norm after: 22.36067549646785
Epoch 9976/10000, Prediction Accuracy = 60.492000000000004%, Loss = 0.6762567043304444
Epoch: 9976, Batch Gradient Norm: 25.121507526461276
Epoch: 9976, Batch Gradient Norm after: 22.360675731401592
Epoch 9977/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6734701991081238
Epoch: 9977, Batch Gradient Norm: 26.347001616868994
Epoch: 9977, Batch Gradient Norm after: 22.36067634251437
Epoch 9978/10000, Prediction Accuracy = 60.465999999999994%, Loss = 0.6762412667274476
Epoch: 9978, Batch Gradient Norm: 25.12000727497516
Epoch: 9978, Batch Gradient Norm after: 22.360677984513316
Epoch 9979/10000, Prediction Accuracy = 60.372%, Loss = 0.6735406398773194
Epoch: 9979, Batch Gradient Norm: 26.330820954746343
Epoch: 9979, Batch Gradient Norm after: 22.360678089945356
Epoch 9980/10000, Prediction Accuracy = 60.458000000000006%, Loss = 0.676207959651947
Epoch: 9980, Batch Gradient Norm: 25.120366678378705
Epoch: 9980, Batch Gradient Norm after: 22.360680420312526
Epoch 9981/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6733418703079224
Epoch: 9981, Batch Gradient Norm: 26.336509303473537
Epoch: 9981, Batch Gradient Norm after: 22.360677437204583
Epoch 9982/10000, Prediction Accuracy = 60.504%, Loss = 0.6761473178863525
Epoch: 9982, Batch Gradient Norm: 25.124218721792165
Epoch: 9982, Batch Gradient Norm after: 22.360673869144204
Epoch 9983/10000, Prediction Accuracy = 60.424%, Loss = 0.6733996868133545
Epoch: 9983, Batch Gradient Norm: 26.336603597628965
Epoch: 9983, Batch Gradient Norm after: 22.36067638004477
Epoch 9984/10000, Prediction Accuracy = 60.326%, Loss = 0.6764027595520019
Epoch: 9984, Batch Gradient Norm: 25.111744935648964
Epoch: 9984, Batch Gradient Norm after: 22.360675622664708
Epoch 9985/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6735373616218567
Epoch: 9985, Batch Gradient Norm: 26.328516366382512
Epoch: 9985, Batch Gradient Norm after: 22.36067510371635
Epoch 9986/10000, Prediction Accuracy = 60.364%, Loss = 0.6763314843177796
Epoch: 9986, Batch Gradient Norm: 25.10124395569273
Epoch: 9986, Batch Gradient Norm after: 22.360676830818896
Epoch 9987/10000, Prediction Accuracy = 60.43000000000001%, Loss = 0.6732842922210693
Epoch: 9987, Batch Gradient Norm: 26.331941729768104
Epoch: 9987, Batch Gradient Norm after: 22.360673700475434
Epoch 9988/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.6759897589683532
Epoch: 9988, Batch Gradient Norm: 25.111437246220913
Epoch: 9988, Batch Gradient Norm after: 22.36067839479468
Epoch 9989/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6732155919075012
Epoch: 9989, Batch Gradient Norm: 26.330475365228395
Epoch: 9989, Batch Gradient Norm after: 22.360676323595857
Epoch 9990/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6759751677513123
Epoch: 9990, Batch Gradient Norm: 25.110771147929242
Epoch: 9990, Batch Gradient Norm after: 22.360677269209148
Epoch 9991/10000, Prediction Accuracy = 60.36%, Loss = 0.6733096241950989
Epoch: 9991, Batch Gradient Norm: 26.311831402741674
Epoch: 9991, Batch Gradient Norm after: 22.36067693203219
Epoch 9992/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6759475708007813
Epoch: 9992, Batch Gradient Norm: 25.11189914483089
Epoch: 9992, Batch Gradient Norm after: 22.360678466193026
Epoch 9993/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6730957150459289
Epoch: 9993, Batch Gradient Norm: 26.31881972337653
Epoch: 9993, Batch Gradient Norm after: 22.36067966457988
Epoch 9994/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.6758679509162903
Epoch: 9994, Batch Gradient Norm: 25.122320868676585
Epoch: 9994, Batch Gradient Norm after: 22.360677558422136
Epoch 9995/10000, Prediction Accuracy = 60.414%, Loss = 0.6731579661369324
Epoch: 9995, Batch Gradient Norm: 26.317272316251785
Epoch: 9995, Batch Gradient Norm after: 22.360676491953118
Epoch 9996/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.676120376586914
Epoch: 9996, Batch Gradient Norm: 25.108811739300293
Epoch: 9996, Batch Gradient Norm after: 22.36067634828828
Epoch 9997/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6733060836791992
Epoch: 9997, Batch Gradient Norm: 26.30856929136744
Epoch: 9997, Batch Gradient Norm after: 22.36067672452247
Epoch 9998/10000, Prediction Accuracy = 60.362%, Loss = 0.6760468721389771
Epoch: 9998, Batch Gradient Norm: 25.099469896487157
Epoch: 9998, Batch Gradient Norm after: 22.36067871689588
Epoch 9999/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6730483889579773
Epoch: 9999, Batch Gradient Norm: 26.311517400273516
Epoch: 9999, Batch Gradient Norm after: 22.360680060898066
Epoch 10000/10000, Prediction Accuracy = 60.462%, Loss = 0.6757164359092712
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode20-dqn/classify_rsmProp2.py", line 215, in <module>
    'Norm after clipping' : total_norm_after,
    ^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/pyplot.py", line 607, in show
    return _get_backend_mod().show(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 3567, in show
    cls.mainloop()
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backends/backend_macosx.py", line 178, in start_main_loop
    with _allow_interrupt_macos():
  File "/Users/athmajanvivekananthan/miniconda3/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 1686, in _allow_interrupt
    old_sigint_handler(*handler_args)
KeyboardInterrupt