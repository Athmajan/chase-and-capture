Epoch: 0, Batch Gradient Norm: 6.081678698254004
Epoch: 0, Batch Gradient Norm after: 6.081678698254004
Epoch 1/10000, Prediction Accuracy = 21.923076923076923%, Loss = 0.17502202953283602
Epoch: 1, Batch Gradient Norm: 4.22604531510797
Epoch: 1, Batch Gradient Norm after: 4.22604531510797
Epoch 2/10000, Prediction Accuracy = 26.815384615384616%, Loss = 0.09270340490799683
Epoch: 2, Batch Gradient Norm: 2.88556218514442
Epoch: 2, Batch Gradient Norm after: 2.88556218514442
Epoch 3/10000, Prediction Accuracy = 27.323076923076922%, Loss = 0.057691541142188586
Epoch: 3, Batch Gradient Norm: 1.83981499221602
Epoch: 3, Batch Gradient Norm after: 1.83981499221602
Epoch 4/10000, Prediction Accuracy = 28.43846153846154%, Loss = 0.03950218999615082
Epoch: 4, Batch Gradient Norm: 1.0975981484568833
Epoch: 4, Batch Gradient Norm after: 1.0975981484568833
Epoch 5/10000, Prediction Accuracy = 30.107692307692314%, Loss = 0.03100628749682353
Epoch: 5, Batch Gradient Norm: 0.6268700312209396
Epoch: 5, Batch Gradient Norm after: 0.6268700312209396
Epoch 6/10000, Prediction Accuracy = 31.21538461538461%, Loss = 0.02722458727657795
Epoch: 6, Batch Gradient Norm: 0.3829904179090934
Epoch: 6, Batch Gradient Norm after: 0.3829904179090934
Epoch 7/10000, Prediction Accuracy = 32.449999999999996%, Loss = 0.025464318979244966
Epoch: 7, Batch Gradient Norm: 0.2740555938887357
Epoch: 7, Batch Gradient Norm after: 0.2740555938887357
Epoch 8/10000, Prediction Accuracy = 33.40384615384615%, Loss = 0.024403012572572783
Epoch: 8, Batch Gradient Norm: 0.25480970396399927
Epoch: 8, Batch Gradient Norm after: 0.25480970396399927
Epoch 9/10000, Prediction Accuracy = 34.400000000000006%, Loss = 0.023757716497549645
Epoch: 9, Batch Gradient Norm: 0.26208390086859884
Epoch: 9, Batch Gradient Norm after: 0.26208390086859884
Epoch 10/10000, Prediction Accuracy = 35.153846153846146%, Loss = 0.023265481568299808
Epoch: 10, Batch Gradient Norm: 0.24580001385151193
Epoch: 10, Batch Gradient Norm after: 0.24580001385151193
Epoch 11/10000, Prediction Accuracy = 35.488461538461536%, Loss = 0.02277922658966138
Epoch: 11, Batch Gradient Norm: 0.24298254923365986
Epoch: 11, Batch Gradient Norm after: 0.24298254923365986
Epoch 12/10000, Prediction Accuracy = 36.24615384615385%, Loss = 0.022381214424967766
Epoch: 12, Batch Gradient Norm: 0.23740879782157864
Epoch: 12, Batch Gradient Norm after: 0.23740879782157864
Epoch 13/10000, Prediction Accuracy = 36.81153846153846%, Loss = 0.02202825190929266
Epoch: 13, Batch Gradient Norm: 0.23680587423116536
Epoch: 13, Batch Gradient Norm after: 0.23680587423116536
Epoch 14/10000, Prediction Accuracy = 37.161538461538456%, Loss = 0.021729495376348495
Epoch: 14, Batch Gradient Norm: 0.23476896155661522
Epoch: 14, Batch Gradient Norm after: 0.23476896155661522
Epoch 15/10000, Prediction Accuracy = 37.46153846153846%, Loss = 0.021505358843849257
Epoch: 15, Batch Gradient Norm: 0.23719480105134452
Epoch: 15, Batch Gradient Norm after: 0.23719480105134452
Epoch 16/10000, Prediction Accuracy = 38.07692307692308%, Loss = 0.021274836733937263
Epoch: 16, Batch Gradient Norm: 0.21768580499883666
Epoch: 16, Batch Gradient Norm after: 0.21768580499883666
Epoch 17/10000, Prediction Accuracy = 38.14230769230768%, Loss = 0.021030997427610252
Epoch: 17, Batch Gradient Norm: 0.21208208362957204
Epoch: 17, Batch Gradient Norm after: 0.21208208362957204
Epoch 18/10000, Prediction Accuracy = 38.5923076923077%, Loss = 0.020796104979056578
Epoch: 18, Batch Gradient Norm: 0.22563741945408716
Epoch: 18, Batch Gradient Norm after: 0.22563741945408716
Epoch 19/10000, Prediction Accuracy = 38.846153846153854%, Loss = 0.020669818211060304
Epoch: 19, Batch Gradient Norm: 0.21891303668663145
Epoch: 19, Batch Gradient Norm after: 0.21891303668663145
Epoch 20/10000, Prediction Accuracy = 39.21153846153846%, Loss = 0.02048111492051528
Epoch: 20, Batch Gradient Norm: 0.2259913380616109
Epoch: 20, Batch Gradient Norm after: 0.2259913380616109
Epoch 21/10000, Prediction Accuracy = 39.30384615384615%, Loss = 0.020349924237682268
Epoch: 21, Batch Gradient Norm: 0.2365572030565562
Epoch: 21, Batch Gradient Norm after: 0.2365572030565562
Epoch 22/10000, Prediction Accuracy = 39.81153846153846%, Loss = 0.020204832777380943
Epoch: 22, Batch Gradient Norm: 0.24164987598914542
Epoch: 22, Batch Gradient Norm after: 0.24164987598914542
Epoch 23/10000, Prediction Accuracy = 39.87692307692308%, Loss = 0.020090082517037026
Epoch: 23, Batch Gradient Norm: 0.2314808984205619
Epoch: 23, Batch Gradient Norm after: 0.2314808984205619
Epoch 24/10000, Prediction Accuracy = 40.23076923076923%, Loss = 0.019931629156837098
Epoch: 24, Batch Gradient Norm: 0.23862870905544045
Epoch: 24, Batch Gradient Norm after: 0.23862870905544045
Epoch 25/10000, Prediction Accuracy = 40.33461538461539%, Loss = 0.019824128024853192
Epoch: 25, Batch Gradient Norm: 0.2315405742168113
Epoch: 25, Batch Gradient Norm after: 0.2315405742168113
Epoch 26/10000, Prediction Accuracy = 40.43846153846153%, Loss = 0.019745241564053755
Epoch: 26, Batch Gradient Norm: 0.2618458937008836
Epoch: 26, Batch Gradient Norm after: 0.2618458937008836
Epoch 27/10000, Prediction Accuracy = 40.62307692307692%, Loss = 0.019649759937937442
Epoch: 27, Batch Gradient Norm: 0.24546635470054787
Epoch: 27, Batch Gradient Norm after: 0.24546635470054787
Epoch 28/10000, Prediction Accuracy = 41.04999999999999%, Loss = 0.01952400588645385
Epoch: 28, Batch Gradient Norm: 0.24489104695259722
Epoch: 28, Batch Gradient Norm after: 0.24489104695259722
Epoch 29/10000, Prediction Accuracy = 41.0076923076923%, Loss = 0.01943335897074296
Epoch: 29, Batch Gradient Norm: 0.23647205380568237
Epoch: 29, Batch Gradient Norm after: 0.23647205380568237
Epoch 30/10000, Prediction Accuracy = 41.35000000000001%, Loss = 0.019313876302196428
Epoch: 30, Batch Gradient Norm: 0.22117380747835516
Epoch: 30, Batch Gradient Norm after: 0.22117380747835516
Epoch 31/10000, Prediction Accuracy = 41.50384615384616%, Loss = 0.019201732742098663
Epoch: 31, Batch Gradient Norm: 0.21168459288156138
Epoch: 31, Batch Gradient Norm after: 0.21168459288156138
Epoch 32/10000, Prediction Accuracy = 41.842307692307685%, Loss = 0.019096102852087755
Epoch: 32, Batch Gradient Norm: 0.22638607243894404
Epoch: 32, Batch Gradient Norm after: 0.22638607243894404
Epoch 33/10000, Prediction Accuracy = 41.48846153846154%, Loss = 0.019031070029506318
Epoch: 33, Batch Gradient Norm: 0.2324574790234319
Epoch: 33, Batch Gradient Norm after: 0.2324574790234319
Epoch 34/10000, Prediction Accuracy = 42.01538461538462%, Loss = 0.01893442038160104
Epoch: 34, Batch Gradient Norm: 0.2289787787749631
Epoch: 34, Batch Gradient Norm after: 0.2289787787749631
Epoch 35/10000, Prediction Accuracy = 42.18076923076922%, Loss = 0.018890097737312317
Epoch: 35, Batch Gradient Norm: 0.2365196715855449
Epoch: 35, Batch Gradient Norm after: 0.2365196715855449
Epoch 36/10000, Prediction Accuracy = 42.17307692307692%, Loss = 0.018805382056878164
Epoch: 36, Batch Gradient Norm: 0.230964827922324
Epoch: 36, Batch Gradient Norm after: 0.230964827922324
Epoch 37/10000, Prediction Accuracy = 42.41923076923077%, Loss = 0.018686365909301318
Epoch: 37, Batch Gradient Norm: 0.2338132668975074
Epoch: 37, Batch Gradient Norm after: 0.2338132668975074
Epoch 38/10000, Prediction Accuracy = 42.53461538461538%, Loss = 0.018680848754369296
Epoch: 38, Batch Gradient Norm: 0.2280258848625862
Epoch: 38, Batch Gradient Norm after: 0.2280258848625862
Epoch 39/10000, Prediction Accuracy = 42.83846153846154%, Loss = 0.018572839979942028
Epoch: 39, Batch Gradient Norm: 0.2381528931930855
Epoch: 39, Batch Gradient Norm after: 0.2381528931930855
Epoch 40/10000, Prediction Accuracy = 42.75%, Loss = 0.018570623288934048
Epoch: 40, Batch Gradient Norm: 0.22545735132406858
Epoch: 40, Batch Gradient Norm after: 0.22545735132406858
Epoch 41/10000, Prediction Accuracy = 42.97692307692307%, Loss = 0.018397478816600945
Epoch: 41, Batch Gradient Norm: 0.24895515879189864
Epoch: 41, Batch Gradient Norm after: 0.24895515879189864
Epoch 42/10000, Prediction Accuracy = 43.080769230769235%, Loss = 0.01838061657662575
Epoch: 42, Batch Gradient Norm: 0.24074654382612987
Epoch: 42, Batch Gradient Norm after: 0.24074654382612987
Epoch 43/10000, Prediction Accuracy = 43.034615384615385%, Loss = 0.018360529238214858
Epoch: 43, Batch Gradient Norm: 0.22912809405579815
Epoch: 43, Batch Gradient Norm after: 0.22912809405579815
Epoch 44/10000, Prediction Accuracy = 43.38461538461539%, Loss = 0.01826530809585865
Epoch: 44, Batch Gradient Norm: 0.2488695489469122
Epoch: 44, Batch Gradient Norm after: 0.2488695489469122
Epoch 45/10000, Prediction Accuracy = 43.45384615384615%, Loss = 0.018224301389776744
Epoch: 45, Batch Gradient Norm: 0.2396086661596429
Epoch: 45, Batch Gradient Norm after: 0.2396086661596429
Epoch 46/10000, Prediction Accuracy = 43.592307692307706%, Loss = 0.01814832495382199
Epoch: 46, Batch Gradient Norm: 0.23911065136254195
Epoch: 46, Batch Gradient Norm after: 0.23911065136254195
Epoch 47/10000, Prediction Accuracy = 43.584615384615375%, Loss = 0.018072435489067666
Epoch: 47, Batch Gradient Norm: 0.26180805264479906
Epoch: 47, Batch Gradient Norm after: 0.26180805264479906
Epoch 48/10000, Prediction Accuracy = 43.53846153846154%, Loss = 0.018092714250087738
Epoch: 48, Batch Gradient Norm: 0.27155798555786576
Epoch: 48, Batch Gradient Norm after: 0.27155798555786576
Epoch 49/10000, Prediction Accuracy = 43.57692307692308%, Loss = 0.01804558588908269
Epoch: 49, Batch Gradient Norm: 0.2566372405087021
Epoch: 49, Batch Gradient Norm after: 0.2566372405087021
Epoch 50/10000, Prediction Accuracy = 43.934615384615384%, Loss = 0.017906275792763784
Epoch: 50, Batch Gradient Norm: 0.2455864398292497
Epoch: 50, Batch Gradient Norm after: 0.2455864398292497
Epoch 51/10000, Prediction Accuracy = 43.86153846153846%, Loss = 0.017870527477218554
Epoch: 51, Batch Gradient Norm: 0.25718477641243487
Epoch: 51, Batch Gradient Norm after: 0.25718477641243487
Epoch 52/10000, Prediction Accuracy = 44.142307692307696%, Loss = 0.017863691712801274
Epoch: 52, Batch Gradient Norm: 0.26006689732523075
Epoch: 52, Batch Gradient Norm after: 0.26006689732523075
Epoch 53/10000, Prediction Accuracy = 44.21153846153846%, Loss = 0.017819443287757728
Epoch: 53, Batch Gradient Norm: 0.24874116436757082
Epoch: 53, Batch Gradient Norm after: 0.24874116436757082
Epoch 54/10000, Prediction Accuracy = 44.28846153846155%, Loss = 0.017759162502793167
Epoch: 54, Batch Gradient Norm: 0.2405351943423421
Epoch: 54, Batch Gradient Norm after: 0.2405351943423421
Epoch 55/10000, Prediction Accuracy = 44.47692307692307%, Loss = 0.01767095923423767
Epoch: 55, Batch Gradient Norm: 0.25180627620981816
Epoch: 55, Batch Gradient Norm after: 0.25180627620981816
Epoch 56/10000, Prediction Accuracy = 44.611538461538466%, Loss = 0.017630362023527805
Epoch: 56, Batch Gradient Norm: 0.25280838639761805
Epoch: 56, Batch Gradient Norm after: 0.25280838639761805
Epoch 57/10000, Prediction Accuracy = 44.67307692307691%, Loss = 0.017589895197978385
Epoch: 57, Batch Gradient Norm: 0.2814179763525884
Epoch: 57, Batch Gradient Norm after: 0.2814179763525884
Epoch 58/10000, Prediction Accuracy = 44.40384615384615%, Loss = 0.017602203986965693
Epoch: 58, Batch Gradient Norm: 0.26224456505780586
Epoch: 58, Batch Gradient Norm after: 0.26224456505780586
Epoch 59/10000, Prediction Accuracy = 44.72692307692308%, Loss = 0.017502035659093123
Epoch: 59, Batch Gradient Norm: 0.26218877921385614
Epoch: 59, Batch Gradient Norm after: 0.26218877921385614
Epoch 60/10000, Prediction Accuracy = 44.87307692307692%, Loss = 0.017477986474449817
Epoch: 60, Batch Gradient Norm: 0.2730590334321159
Epoch: 60, Batch Gradient Norm after: 0.2730590334321159
Epoch 61/10000, Prediction Accuracy = 44.542307692307695%, Loss = 0.017461933195590973
Epoch: 61, Batch Gradient Norm: 0.2600291869134599
Epoch: 61, Batch Gradient Norm after: 0.2600291869134599
Epoch 62/10000, Prediction Accuracy = 44.93846153846153%, Loss = 0.017402762690415748
Epoch: 62, Batch Gradient Norm: 0.26925501611072966
Epoch: 62, Batch Gradient Norm after: 0.26925501611072966
Epoch 63/10000, Prediction Accuracy = 44.82307692307692%, Loss = 0.017297309035292037
Epoch: 63, Batch Gradient Norm: 0.2669987232651626
Epoch: 63, Batch Gradient Norm after: 0.2669987232651626
Epoch 64/10000, Prediction Accuracy = 45.034615384615385%, Loss = 0.017271313529748183
Epoch: 64, Batch Gradient Norm: 0.26177428408322817
Epoch: 64, Batch Gradient Norm after: 0.26177428408322817
Epoch 65/10000, Prediction Accuracy = 45.28076923076923%, Loss = 0.01719441293523862
Epoch: 65, Batch Gradient Norm: 0.2523054745753085
Epoch: 65, Batch Gradient Norm after: 0.2523054745753085
Epoch 66/10000, Prediction Accuracy = 45.48846153846154%, Loss = 0.01715799979865551
Epoch: 66, Batch Gradient Norm: 0.2622164658623329
Epoch: 66, Batch Gradient Norm after: 0.2622164658623329
Epoch 67/10000, Prediction Accuracy = 45.673076923076934%, Loss = 0.017129885176053412
Epoch: 67, Batch Gradient Norm: 0.27621397414360055
Epoch: 67, Batch Gradient Norm after: 0.27621397414360055
Epoch 68/10000, Prediction Accuracy = 45.55769230769231%, Loss = 0.017120948777748987
Epoch: 68, Batch Gradient Norm: 0.2751083321401745
Epoch: 68, Batch Gradient Norm after: 0.2751083321401745
Epoch 69/10000, Prediction Accuracy = 45.57307692307693%, Loss = 0.017057775877989255
Epoch: 69, Batch Gradient Norm: 0.2763670236898133
Epoch: 69, Batch Gradient Norm after: 0.2763670236898133
Epoch 70/10000, Prediction Accuracy = 45.6076923076923%, Loss = 0.017040723504928443
Epoch: 70, Batch Gradient Norm: 0.2712317990308939
Epoch: 70, Batch Gradient Norm after: 0.2712317990308939
Epoch 71/10000, Prediction Accuracy = 45.723076923076924%, Loss = 0.01697381456884054
Epoch: 71, Batch Gradient Norm: 0.301974361318301
Epoch: 71, Batch Gradient Norm after: 0.301974361318301
Epoch 72/10000, Prediction Accuracy = 45.48846153846154%, Loss = 0.016984816210774276
Epoch: 72, Batch Gradient Norm: 0.29149109088460096
Epoch: 72, Batch Gradient Norm after: 0.29149109088460096
Epoch 73/10000, Prediction Accuracy = 45.684615384615384%, Loss = 0.016950581652613785
Epoch: 73, Batch Gradient Norm: 0.29264627240012636
Epoch: 73, Batch Gradient Norm after: 0.29264627240012636
Epoch 74/10000, Prediction Accuracy = 45.68846153846153%, Loss = 0.016887069321595706
Epoch: 74, Batch Gradient Norm: 0.3077100062349231
Epoch: 74, Batch Gradient Norm after: 0.3077100062349231
Epoch 75/10000, Prediction Accuracy = 45.853846153846156%, Loss = 0.01690141913982538
Epoch: 75, Batch Gradient Norm: 0.29641519639975294
Epoch: 75, Batch Gradient Norm after: 0.29641519639975294
Epoch 76/10000, Prediction Accuracy = 46.04615384615385%, Loss = 0.016832811901202567
Epoch: 76, Batch Gradient Norm: 0.30256681510610917
Epoch: 76, Batch Gradient Norm after: 0.30256681510610917
Epoch 77/10000, Prediction Accuracy = 46.21153846153846%, Loss = 0.016794696163672667
Epoch: 77, Batch Gradient Norm: 0.2965305189861049
Epoch: 77, Batch Gradient Norm after: 0.2965305189861049
Epoch 78/10000, Prediction Accuracy = 46.165384615384625%, Loss = 0.016717040768036477
Epoch: 78, Batch Gradient Norm: 0.2894810198532066
Epoch: 78, Batch Gradient Norm after: 0.2894810198532066
Epoch 79/10000, Prediction Accuracy = 46.24615384615385%, Loss = 0.016678283707453653
Epoch: 79, Batch Gradient Norm: 0.28664411597099726
Epoch: 79, Batch Gradient Norm after: 0.28664411597099726
Epoch 80/10000, Prediction Accuracy = 46.22692307692307%, Loss = 0.01662781934898633
Epoch: 80, Batch Gradient Norm: 0.30191298750828205
Epoch: 80, Batch Gradient Norm after: 0.30191298750828205
Epoch 81/10000, Prediction Accuracy = 46.23846153846154%, Loss = 0.01661488628731324
Epoch: 81, Batch Gradient Norm: 0.3048444580005621
Epoch: 81, Batch Gradient Norm after: 0.3048444580005621
Epoch 82/10000, Prediction Accuracy = 46.49615384615386%, Loss = 0.016556619594876584
Epoch: 82, Batch Gradient Norm: 0.3217065994518961
Epoch: 82, Batch Gradient Norm after: 0.3217065994518961
Epoch 83/10000, Prediction Accuracy = 46.35769230769232%, Loss = 0.01656089465205486
Epoch: 83, Batch Gradient Norm: 0.31263038251097724
Epoch: 83, Batch Gradient Norm after: 0.31263038251097724
Epoch 84/10000, Prediction Accuracy = 46.59615384615385%, Loss = 0.016523576986331206
Epoch: 84, Batch Gradient Norm: 0.3286129601135809
Epoch: 84, Batch Gradient Norm after: 0.3286129601135809
Epoch 85/10000, Prediction Accuracy = 46.54615384615385%, Loss = 0.016491012791028388
Epoch: 85, Batch Gradient Norm: 0.3239048531470167
Epoch: 85, Batch Gradient Norm after: 0.3239048531470167
Epoch 86/10000, Prediction Accuracy = 46.823076923076925%, Loss = 0.016488973910991963
Epoch: 86, Batch Gradient Norm: 0.3318742167832082
Epoch: 86, Batch Gradient Norm after: 0.3318742167832082
Epoch 87/10000, Prediction Accuracy = 46.63461538461537%, Loss = 0.016463217826989982
Epoch: 87, Batch Gradient Norm: 0.32482400918425247
Epoch: 87, Batch Gradient Norm after: 0.32482400918425247
Epoch 88/10000, Prediction Accuracy = 47.11538461538463%, Loss = 0.016408871572751265
Epoch: 88, Batch Gradient Norm: 0.3330106165283788
Epoch: 88, Batch Gradient Norm after: 0.3330106165283788
Epoch 89/10000, Prediction Accuracy = 46.63461538461539%, Loss = 0.016356125760536928
Epoch: 89, Batch Gradient Norm: 0.3347977027655794
Epoch: 89, Batch Gradient Norm after: 0.3347977027655794
Epoch 90/10000, Prediction Accuracy = 47.080769230769235%, Loss = 0.016343427248872243
Epoch: 90, Batch Gradient Norm: 0.3483944586630117
Epoch: 90, Batch Gradient Norm after: 0.3483944586630117
Epoch 91/10000, Prediction Accuracy = 46.71923076923076%, Loss = 0.016297140516913854
Epoch: 91, Batch Gradient Norm: 0.34193596683837585
Epoch: 91, Batch Gradient Norm after: 0.34193596683837585
Epoch 92/10000, Prediction Accuracy = 47.434615384615384%, Loss = 0.01624198444187641
Epoch: 92, Batch Gradient Norm: 0.33670437034053147
Epoch: 92, Batch Gradient Norm after: 0.33670437034053147
Epoch 93/10000, Prediction Accuracy = 47.115384615384606%, Loss = 0.01624103721517783
Epoch: 93, Batch Gradient Norm: 0.3343284627244365
Epoch: 93, Batch Gradient Norm after: 0.3343284627244365
Epoch 94/10000, Prediction Accuracy = 47.19230769230769%, Loss = 0.016203147144271776
Epoch: 94, Batch Gradient Norm: 0.3476907165593267
Epoch: 94, Batch Gradient Norm after: 0.3476907165593267
Epoch 95/10000, Prediction Accuracy = 47.05384615384615%, Loss = 0.01615351653442933
Epoch: 95, Batch Gradient Norm: 0.3488628920600584
Epoch: 95, Batch Gradient Norm after: 0.3488628920600584
Epoch 96/10000, Prediction Accuracy = 47.36538461538461%, Loss = 0.016094799225146953
Epoch: 96, Batch Gradient Norm: 0.36138538442541984
Epoch: 96, Batch Gradient Norm after: 0.36138538442541984
Epoch 97/10000, Prediction Accuracy = 47.53846153846154%, Loss = 0.016072140505107548
Epoch: 97, Batch Gradient Norm: 0.3508238641700288
Epoch: 97, Batch Gradient Norm after: 0.3508238641700288
Epoch 98/10000, Prediction Accuracy = 47.75384615384616%, Loss = 0.016037041178116433
Epoch: 98, Batch Gradient Norm: 0.3406089807358892
Epoch: 98, Batch Gradient Norm after: 0.3406089807358892
Epoch 99/10000, Prediction Accuracy = 47.58461538461539%, Loss = 0.015982778671269234
Epoch: 99, Batch Gradient Norm: 0.3471187720942466
Epoch: 99, Batch Gradient Norm after: 0.3471187720942466
Epoch 100/10000, Prediction Accuracy = 47.98076923076923%, Loss = 0.015915231134455938
Epoch: 100, Batch Gradient Norm: 0.35917461670343237
Epoch: 100, Batch Gradient Norm after: 0.35917461670343237
Epoch 101/10000, Prediction Accuracy = 47.54615384615384%, Loss = 0.0159345858085614
Epoch: 101, Batch Gradient Norm: 0.3521714185998867
Epoch: 101, Batch Gradient Norm after: 0.3521714185998867
Epoch 102/10000, Prediction Accuracy = 47.77307692307692%, Loss = 0.015834857494785234
Epoch: 102, Batch Gradient Norm: 0.39240636591874484
Epoch: 102, Batch Gradient Norm after: 0.39240636591874484
Epoch 103/10000, Prediction Accuracy = 47.98461538461538%, Loss = 0.015935809064943057
Epoch: 103, Batch Gradient Norm: 0.3561880960655211
Epoch: 103, Batch Gradient Norm after: 0.3561880960655211
Epoch 104/10000, Prediction Accuracy = 48.24615384615385%, Loss = 0.01582655468239234
Epoch: 104, Batch Gradient Norm: 0.38693252897584696
Epoch: 104, Batch Gradient Norm after: 0.38693252897584696
Epoch 105/10000, Prediction Accuracy = 47.91923076923078%, Loss = 0.015824863615517434
Epoch: 105, Batch Gradient Norm: 0.3877108611880455
Epoch: 105, Batch Gradient Norm after: 0.3877108611880455
Epoch 106/10000, Prediction Accuracy = 47.776923076923076%, Loss = 0.015847165065889176
Epoch: 106, Batch Gradient Norm: 0.3670923816680983
Epoch: 106, Batch Gradient Norm after: 0.3670923816680983
Epoch 107/10000, Prediction Accuracy = 47.91153846153846%, Loss = 0.015747101977467537
Epoch: 107, Batch Gradient Norm: 0.38045777049513285
Epoch: 107, Batch Gradient Norm after: 0.38045777049513285
Epoch 108/10000, Prediction Accuracy = 48.284615384615385%, Loss = 0.015713931849369638
Epoch: 108, Batch Gradient Norm: 0.3613739773299265
Epoch: 108, Batch Gradient Norm after: 0.3613739773299265
Epoch 109/10000, Prediction Accuracy = 48.14615384615385%, Loss = 0.01568941206026536
Epoch: 109, Batch Gradient Norm: 0.40514061655528594
Epoch: 109, Batch Gradient Norm after: 0.40514061655528594
Epoch 110/10000, Prediction Accuracy = 48.161538461538456%, Loss = 0.01570436229499487
Epoch: 110, Batch Gradient Norm: 0.39955297606419915
Epoch: 110, Batch Gradient Norm after: 0.39955297606419915
Epoch 111/10000, Prediction Accuracy = 47.88461538461539%, Loss = 0.015685718649855025
Epoch: 111, Batch Gradient Norm: 0.43034859874037745
Epoch: 111, Batch Gradient Norm after: 0.43034859874037745
Epoch 112/10000, Prediction Accuracy = 48.41923076923077%, Loss = 0.015701518179132387
Epoch: 112, Batch Gradient Norm: 0.4253435140469709
Epoch: 112, Batch Gradient Norm after: 0.4253435140469709
Epoch 113/10000, Prediction Accuracy = 48.05384615384615%, Loss = 0.015665745147718832
Epoch: 113, Batch Gradient Norm: 0.40484882337504186
Epoch: 113, Batch Gradient Norm after: 0.40484882337504186
Epoch 114/10000, Prediction Accuracy = 48.4076923076923%, Loss = 0.015589285283707656
Epoch: 114, Batch Gradient Norm: 0.40663517343928485
Epoch: 114, Batch Gradient Norm after: 0.40663517343928485
Epoch 115/10000, Prediction Accuracy = 48.6576923076923%, Loss = 0.015527084827996217
Epoch: 115, Batch Gradient Norm: 0.4068338683503028
Epoch: 115, Batch Gradient Norm after: 0.4068338683503028
Epoch 116/10000, Prediction Accuracy = 48.47692307692308%, Loss = 0.015500840587684741
Epoch: 116, Batch Gradient Norm: 0.42488267267194674
Epoch: 116, Batch Gradient Norm after: 0.42488267267194674
Epoch 117/10000, Prediction Accuracy = 48.419230769230765%, Loss = 0.015500739575005494
Epoch: 117, Batch Gradient Norm: 0.42600136358510315
Epoch: 117, Batch Gradient Norm after: 0.42600136358510315
Epoch 118/10000, Prediction Accuracy = 48.73461538461538%, Loss = 0.01544340535138662
Epoch: 118, Batch Gradient Norm: 0.414259633931363
Epoch: 118, Batch Gradient Norm after: 0.414259633931363
Epoch 119/10000, Prediction Accuracy = 48.87307692307692%, Loss = 0.015391534051069846
Epoch: 119, Batch Gradient Norm: 0.4216711226573688
Epoch: 119, Batch Gradient Norm after: 0.4216711226573688
Epoch 120/10000, Prediction Accuracy = 48.8%, Loss = 0.015368928488057394
Epoch: 120, Batch Gradient Norm: 0.4451949606140847
Epoch: 120, Batch Gradient Norm after: 0.4451949606140847
Epoch 121/10000, Prediction Accuracy = 48.61923076923078%, Loss = 0.015395494894339489
Epoch: 121, Batch Gradient Norm: 0.438027872519146
Epoch: 121, Batch Gradient Norm after: 0.438027872519146
Epoch 122/10000, Prediction Accuracy = 48.82307692307692%, Loss = 0.015372154876016654
Epoch: 122, Batch Gradient Norm: 0.4699988382119399
Epoch: 122, Batch Gradient Norm after: 0.4699988382119399
Epoch 123/10000, Prediction Accuracy = 48.93461538461539%, Loss = 0.015347056735593539
Epoch: 123, Batch Gradient Norm: 0.43456787221310694
Epoch: 123, Batch Gradient Norm after: 0.43456787221310694
Epoch 124/10000, Prediction Accuracy = 48.97692307692308%, Loss = 0.01528330512631398
Epoch: 124, Batch Gradient Norm: 0.45516332506228785
Epoch: 124, Batch Gradient Norm after: 0.45516332506228785
Epoch 125/10000, Prediction Accuracy = 49.23461538461538%, Loss = 0.015256626101640554
Epoch: 125, Batch Gradient Norm: 0.49626746564434177
Epoch: 125, Batch Gradient Norm after: 0.49626746564434177
Epoch 126/10000, Prediction Accuracy = 49.26923076923077%, Loss = 0.015314007822710734
Epoch: 126, Batch Gradient Norm: 0.471036305785067
Epoch: 126, Batch Gradient Norm after: 0.471036305785067
Epoch 127/10000, Prediction Accuracy = 49.042307692307695%, Loss = 0.01524814311414957
Epoch: 127, Batch Gradient Norm: 0.5069338305941878
Epoch: 127, Batch Gradient Norm after: 0.5069338305941878
Epoch 128/10000, Prediction Accuracy = 49.19230769230769%, Loss = 0.015278295040703736
Epoch: 128, Batch Gradient Norm: 0.49146653307898275
Epoch: 128, Batch Gradient Norm after: 0.49146653307898275
Epoch 129/10000, Prediction Accuracy = 49.05384615384615%, Loss = 0.015227886203389902
Epoch: 129, Batch Gradient Norm: 0.4475538286927831
Epoch: 129, Batch Gradient Norm after: 0.4475538286927831
Epoch 130/10000, Prediction Accuracy = 49.2423076923077%, Loss = 0.015134670986579014
Epoch: 130, Batch Gradient Norm: 0.46999669836447816
Epoch: 130, Batch Gradient Norm after: 0.46999669836447816
Epoch 131/10000, Prediction Accuracy = 49.05%, Loss = 0.015064796003011556
Epoch: 131, Batch Gradient Norm: 0.4564329134423681
Epoch: 131, Batch Gradient Norm after: 0.4564329134423681
Epoch 132/10000, Prediction Accuracy = 49.71923076923077%, Loss = 0.01502751695135465
Epoch: 132, Batch Gradient Norm: 0.44118147639662697
Epoch: 132, Batch Gradient Norm after: 0.44118147639662697
Epoch 133/10000, Prediction Accuracy = 49.55769230769231%, Loss = 0.014987496874080254
Epoch: 133, Batch Gradient Norm: 0.488995069250938
Epoch: 133, Batch Gradient Norm after: 0.488995069250938
Epoch 134/10000, Prediction Accuracy = 49.41153846153846%, Loss = 0.015047804524119083
Epoch: 134, Batch Gradient Norm: 0.48237075992624734
Epoch: 134, Batch Gradient Norm after: 0.48237075992624734
Epoch 135/10000, Prediction Accuracy = 49.70384615384615%, Loss = 0.014968272800055834
Epoch: 135, Batch Gradient Norm: 0.48361848583479383
Epoch: 135, Batch Gradient Norm after: 0.48361848583479383
Epoch 136/10000, Prediction Accuracy = 49.76153846153845%, Loss = 0.014932561880693985
Epoch: 136, Batch Gradient Norm: 0.49019166518494667
Epoch: 136, Batch Gradient Norm after: 0.49019166518494667
Epoch 137/10000, Prediction Accuracy = 49.78846153846153%, Loss = 0.014990056220155496
Epoch: 137, Batch Gradient Norm: 0.4901787943408772
Epoch: 137, Batch Gradient Norm after: 0.4901787943408772
Epoch 138/10000, Prediction Accuracy = 49.657692307692315%, Loss = 0.014920627483381676
Epoch: 138, Batch Gradient Norm: 0.49551887598341915
Epoch: 138, Batch Gradient Norm after: 0.49551887598341915
Epoch 139/10000, Prediction Accuracy = 49.82692307692308%, Loss = 0.014908040945346538
Epoch: 139, Batch Gradient Norm: 0.4995423999072954
Epoch: 139, Batch Gradient Norm after: 0.4995423999072954
Epoch 140/10000, Prediction Accuracy = 50.04230769230768%, Loss = 0.014843542773563128
Epoch: 140, Batch Gradient Norm: 0.5128043780282889
Epoch: 140, Batch Gradient Norm after: 0.5128043780282889
Epoch 141/10000, Prediction Accuracy = 49.934615384615384%, Loss = 0.014870866058537593
Epoch: 141, Batch Gradient Norm: 0.50968294632312
Epoch: 141, Batch Gradient Norm after: 0.50968294632312
Epoch 142/10000, Prediction Accuracy = 49.88076923076923%, Loss = 0.014820567833689543
Epoch: 142, Batch Gradient Norm: 0.5037675794590066
Epoch: 142, Batch Gradient Norm after: 0.5037675794590066
Epoch 143/10000, Prediction Accuracy = 50.00384615384615%, Loss = 0.014750653304732762
Epoch: 143, Batch Gradient Norm: 0.5619876568378933
Epoch: 143, Batch Gradient Norm after: 0.5619876568378933
Epoch 144/10000, Prediction Accuracy = 49.79230769230769%, Loss = 0.014799201431182714
Epoch: 144, Batch Gradient Norm: 0.5430640351327441
Epoch: 144, Batch Gradient Norm after: 0.5430640351327441
Epoch 145/10000, Prediction Accuracy = 50.080769230769235%, Loss = 0.01479174390148658
Epoch: 145, Batch Gradient Norm: 0.5259617709051194
Epoch: 145, Batch Gradient Norm after: 0.5259617709051194
Epoch 146/10000, Prediction Accuracy = 50.45%, Loss = 0.014789457146364909
Epoch: 146, Batch Gradient Norm: 0.526092228099984
Epoch: 146, Batch Gradient Norm after: 0.526092228099984
Epoch 147/10000, Prediction Accuracy = 50.03461538461538%, Loss = 0.014648460997984959
Epoch: 147, Batch Gradient Norm: 0.5021356523112773
Epoch: 147, Batch Gradient Norm after: 0.5021356523112773
Epoch 148/10000, Prediction Accuracy = 50.21538461538462%, Loss = 0.014585808134422852
Epoch: 148, Batch Gradient Norm: 0.5380209971622193
Epoch: 148, Batch Gradient Norm after: 0.5380209971622193
Epoch 149/10000, Prediction Accuracy = 50.31923076923077%, Loss = 0.014564888121990057
Epoch: 149, Batch Gradient Norm: 0.5706351845276588
Epoch: 149, Batch Gradient Norm after: 0.5706351845276588
Epoch 150/10000, Prediction Accuracy = 50.088461538461544%, Loss = 0.014623945364012169
Epoch: 150, Batch Gradient Norm: 0.5403602450820538
Epoch: 150, Batch Gradient Norm after: 0.5403602450820538
Epoch 151/10000, Prediction Accuracy = 50.40384615384615%, Loss = 0.014583832512681302
Epoch: 151, Batch Gradient Norm: 0.5353981271734084
Epoch: 151, Batch Gradient Norm after: 0.5353981271734084
Epoch 152/10000, Prediction Accuracy = 50.46923076923077%, Loss = 0.01457062249000256
Epoch: 152, Batch Gradient Norm: 0.6263284185764659
Epoch: 152, Batch Gradient Norm after: 0.6263284185764659
Epoch 153/10000, Prediction Accuracy = 50.15384615384615%, Loss = 0.014673815896877876
Epoch: 153, Batch Gradient Norm: 0.6200119082185214
Epoch: 153, Batch Gradient Norm after: 0.6200119082185214
Epoch 154/10000, Prediction Accuracy = 50.11923076923077%, Loss = 0.0146068949968769
Epoch: 154, Batch Gradient Norm: 0.5683981149872258
Epoch: 154, Batch Gradient Norm after: 0.5683981149872258
Epoch 155/10000, Prediction Accuracy = 50.361538461538466%, Loss = 0.014528305484698368
Epoch: 155, Batch Gradient Norm: 0.5567650797556007
Epoch: 155, Batch Gradient Norm after: 0.5567650797556007
Epoch 156/10000, Prediction Accuracy = 50.63076923076923%, Loss = 0.014437097626236768
Epoch: 156, Batch Gradient Norm: 0.5529896186153161
Epoch: 156, Batch Gradient Norm after: 0.5529896186153161
Epoch 157/10000, Prediction Accuracy = 50.26153846153846%, Loss = 0.014433738632270923
Epoch: 157, Batch Gradient Norm: 0.5542829789591421
Epoch: 157, Batch Gradient Norm after: 0.5542829789591421
Epoch 158/10000, Prediction Accuracy = 50.95769230769232%, Loss = 0.01440519832361203
Epoch: 158, Batch Gradient Norm: 0.6031053870271595
Epoch: 158, Batch Gradient Norm after: 0.6031053870271595
Epoch 159/10000, Prediction Accuracy = 50.49230769230769%, Loss = 0.014464598149061203
Epoch: 159, Batch Gradient Norm: 0.6131255529761034
Epoch: 159, Batch Gradient Norm after: 0.6131255529761034
Epoch 160/10000, Prediction Accuracy = 50.32307692307692%, Loss = 0.014482482193181148
Epoch: 160, Batch Gradient Norm: 0.5897756686970081
Epoch: 160, Batch Gradient Norm after: 0.5897756686970081
Epoch 161/10000, Prediction Accuracy = 50.51153846153846%, Loss = 0.01445720294633737
Epoch: 161, Batch Gradient Norm: 0.5982213917009157
Epoch: 161, Batch Gradient Norm after: 0.5982213917009157
Epoch 162/10000, Prediction Accuracy = 50.36923076923077%, Loss = 0.014401596899215992
Epoch: 162, Batch Gradient Norm: 0.5688361329999609
Epoch: 162, Batch Gradient Norm after: 0.5688361329999609
Epoch 163/10000, Prediction Accuracy = 50.79615384615384%, Loss = 0.01426002519348493
Epoch: 163, Batch Gradient Norm: 0.5515201699475667
Epoch: 163, Batch Gradient Norm after: 0.5515201699475667
Epoch 164/10000, Prediction Accuracy = 51.11538461538461%, Loss = 0.014194804028822826
Epoch: 164, Batch Gradient Norm: 0.5697597460906916
Epoch: 164, Batch Gradient Norm after: 0.5697597460906916
Epoch 165/10000, Prediction Accuracy = 51.06153846153846%, Loss = 0.014194094002819978
Epoch: 165, Batch Gradient Norm: 0.6162181886849157
Epoch: 165, Batch Gradient Norm after: 0.6162181886849157
Epoch 166/10000, Prediction Accuracy = 50.815384615384616%, Loss = 0.014298251973321805
Epoch: 166, Batch Gradient Norm: 0.5905353790321455
Epoch: 166, Batch Gradient Norm after: 0.5905353790321455
Epoch 167/10000, Prediction Accuracy = 51.534615384615385%, Loss = 0.014118005879796468
Epoch: 167, Batch Gradient Norm: 0.6098642858288109
Epoch: 167, Batch Gradient Norm after: 0.6098642858288109
Epoch 168/10000, Prediction Accuracy = 51.26923076923077%, Loss = 0.014176680634801205
Epoch: 168, Batch Gradient Norm: 0.5787332710247887
Epoch: 168, Batch Gradient Norm after: 0.5787332710247887
Epoch 169/10000, Prediction Accuracy = 51.026923076923076%, Loss = 0.014116009195836691
Epoch: 169, Batch Gradient Norm: 0.5909354160480812
Epoch: 169, Batch Gradient Norm after: 0.5909354160480812
Epoch 170/10000, Prediction Accuracy = 51.276923076923076%, Loss = 0.014130917950891532
Epoch: 170, Batch Gradient Norm: 0.5741654580642456
Epoch: 170, Batch Gradient Norm after: 0.5741654580642456
Epoch 171/10000, Prediction Accuracy = 51.184615384615384%, Loss = 0.014094161442839183
Epoch: 171, Batch Gradient Norm: 0.6102199241101809
Epoch: 171, Batch Gradient Norm after: 0.6102199241101809
Epoch 172/10000, Prediction Accuracy = 50.9076923076923%, Loss = 0.014111482824843664
Epoch: 172, Batch Gradient Norm: 0.6535012826041252
Epoch: 172, Batch Gradient Norm after: 0.6535012826041252
Epoch 173/10000, Prediction Accuracy = 50.926923076923075%, Loss = 0.014150239885426484
Epoch: 173, Batch Gradient Norm: 0.677144384180857
Epoch: 173, Batch Gradient Norm after: 0.677144384180857
Epoch 174/10000, Prediction Accuracy = 51.16153846153846%, Loss = 0.014131693670955988
Epoch: 174, Batch Gradient Norm: 0.6040226344268065
Epoch: 174, Batch Gradient Norm after: 0.6040226344268065
Epoch 175/10000, Prediction Accuracy = 51.323076923076925%, Loss = 0.013976077262598734
Epoch: 175, Batch Gradient Norm: 0.5833417016398423
Epoch: 175, Batch Gradient Norm after: 0.5833417016398423
Epoch 176/10000, Prediction Accuracy = 51.3923076923077%, Loss = 0.013961235491129069
Epoch: 176, Batch Gradient Norm: 0.6411987958700976
Epoch: 176, Batch Gradient Norm after: 0.6411987958700976
Epoch 177/10000, Prediction Accuracy = 51.807692307692314%, Loss = 0.014013738443072025
Epoch: 177, Batch Gradient Norm: 0.6126316756483865
Epoch: 177, Batch Gradient Norm after: 0.6126316756483865
Epoch 178/10000, Prediction Accuracy = 51.50384615384615%, Loss = 0.01391563886919847
Epoch: 178, Batch Gradient Norm: 0.6540981631665175
Epoch: 178, Batch Gradient Norm after: 0.6540981631665175
Epoch 179/10000, Prediction Accuracy = 51.50769230769231%, Loss = 0.013964742350463685
Epoch: 179, Batch Gradient Norm: 0.6340312416315278
Epoch: 179, Batch Gradient Norm after: 0.6340312416315278
Epoch 180/10000, Prediction Accuracy = 51.642307692307696%, Loss = 0.013894704528726064
Epoch: 180, Batch Gradient Norm: 0.7080763626621043
Epoch: 180, Batch Gradient Norm after: 0.7080763626621043
Epoch 181/10000, Prediction Accuracy = 51.23846153846154%, Loss = 0.014072364554382287
Epoch: 181, Batch Gradient Norm: 0.672459445186774
Epoch: 181, Batch Gradient Norm after: 0.672459445186774
Epoch 182/10000, Prediction Accuracy = 51.2%, Loss = 0.013928780188927284
Epoch: 182, Batch Gradient Norm: 0.7107089885228912
Epoch: 182, Batch Gradient Norm after: 0.7107089885228912
Epoch 183/10000, Prediction Accuracy = 51.469230769230776%, Loss = 0.013946912537973661
Epoch: 183, Batch Gradient Norm: 0.7187344784410985
Epoch: 183, Batch Gradient Norm after: 0.7187344784410985
Epoch 184/10000, Prediction Accuracy = 51.669230769230765%, Loss = 0.013912237894076567
Epoch: 184, Batch Gradient Norm: 0.6708873441965936
Epoch: 184, Batch Gradient Norm after: 0.6708873441965936
Epoch 185/10000, Prediction Accuracy = 51.638461538461534%, Loss = 0.013836378661485819
Epoch: 185, Batch Gradient Norm: 0.6861507994969099
Epoch: 185, Batch Gradient Norm after: 0.6861507994969099
Epoch 186/10000, Prediction Accuracy = 51.54230769230769%, Loss = 0.01389307020088801
Epoch: 186, Batch Gradient Norm: 0.7296420946680974
Epoch: 186, Batch Gradient Norm after: 0.7296420946680974
Epoch 187/10000, Prediction Accuracy = 51.411538461538456%, Loss = 0.013918791611034136
Epoch: 187, Batch Gradient Norm: 0.6529123955213434
Epoch: 187, Batch Gradient Norm after: 0.6529123955213434
Epoch 188/10000, Prediction Accuracy = 51.861538461538466%, Loss = 0.013749008496793417
Epoch: 188, Batch Gradient Norm: 0.6813027894842509
Epoch: 188, Batch Gradient Norm after: 0.6813027894842509
Epoch 189/10000, Prediction Accuracy = 51.71153846153846%, Loss = 0.01376040716870473
Epoch: 189, Batch Gradient Norm: 0.6914949819174427
Epoch: 189, Batch Gradient Norm after: 0.6914949819174427
Epoch 190/10000, Prediction Accuracy = 51.74615384615384%, Loss = 0.013763047754764557
Epoch: 190, Batch Gradient Norm: 0.6939726961258273
Epoch: 190, Batch Gradient Norm after: 0.6939726961258273
Epoch 191/10000, Prediction Accuracy = 51.74615384615385%, Loss = 0.013721514994708391
Epoch: 191, Batch Gradient Norm: 0.6555567877148072
Epoch: 191, Batch Gradient Norm after: 0.6555567877148072
Epoch 192/10000, Prediction Accuracy = 52.13846153846154%, Loss = 0.013612039387226105
Epoch: 192, Batch Gradient Norm: 0.7112561727609771
Epoch: 192, Batch Gradient Norm after: 0.7112561727609771
Epoch 193/10000, Prediction Accuracy = 51.876923076923084%, Loss = 0.013695015357090877
Epoch: 193, Batch Gradient Norm: 0.6749395267308357
Epoch: 193, Batch Gradient Norm after: 0.6749395267308357
Epoch 194/10000, Prediction Accuracy = 51.96923076923077%, Loss = 0.013567367353691505
Epoch: 194, Batch Gradient Norm: 0.6537231941005233
Epoch: 194, Batch Gradient Norm after: 0.6537231941005233
Epoch 195/10000, Prediction Accuracy = 52.41538461538461%, Loss = 0.01351193288484445
Epoch: 195, Batch Gradient Norm: 0.6430133683458185
Epoch: 195, Batch Gradient Norm after: 0.6430133683458185
Epoch 196/10000, Prediction Accuracy = 52.23461538461538%, Loss = 0.013512604941542331
Epoch: 196, Batch Gradient Norm: 0.7306674405647784
Epoch: 196, Batch Gradient Norm after: 0.7306674405647784
Epoch 197/10000, Prediction Accuracy = 52.073076923076925%, Loss = 0.013602837203786923
Epoch: 197, Batch Gradient Norm: 0.6698226176005845
Epoch: 197, Batch Gradient Norm after: 0.6698226176005845
Epoch 198/10000, Prediction Accuracy = 52.64615384615385%, Loss = 0.01345814592563189
Epoch: 198, Batch Gradient Norm: 0.6915512078956356
Epoch: 198, Batch Gradient Norm after: 0.6915512078956356
Epoch 199/10000, Prediction Accuracy = 52.153846153846146%, Loss = 0.013497429326749764
Epoch: 199, Batch Gradient Norm: 0.7082887791316006
Epoch: 199, Batch Gradient Norm after: 0.7082887791316006
Epoch 200/10000, Prediction Accuracy = 52.21153846153846%, Loss = 0.013530076409761723
Epoch: 200, Batch Gradient Norm: 0.7636778484909228
Epoch: 200, Batch Gradient Norm after: 0.7636778484909228
Epoch 201/10000, Prediction Accuracy = 52.2576923076923%, Loss = 0.013507310587626237
Epoch: 201, Batch Gradient Norm: 0.7426181539088802
Epoch: 201, Batch Gradient Norm after: 0.7426181539088802
Epoch 202/10000, Prediction Accuracy = 52.12692307692306%, Loss = 0.01346841175109148
Epoch: 202, Batch Gradient Norm: 0.7258902555598398
Epoch: 202, Batch Gradient Norm after: 0.7258902555598398
Epoch 203/10000, Prediction Accuracy = 52.573076923076925%, Loss = 0.01348613489132661
Epoch: 203, Batch Gradient Norm: 0.7461183356705144
Epoch: 203, Batch Gradient Norm after: 0.7461183356705144
Epoch 204/10000, Prediction Accuracy = 52.62307692307692%, Loss = 0.013478377762322243
Epoch: 204, Batch Gradient Norm: 0.7225720435526205
Epoch: 204, Batch Gradient Norm after: 0.7225720435526205
Epoch 205/10000, Prediction Accuracy = 52.73461538461538%, Loss = 0.013450523766760644
Epoch: 205, Batch Gradient Norm: 0.7252865369049202
Epoch: 205, Batch Gradient Norm after: 0.7252865369049202
Epoch 206/10000, Prediction Accuracy = 52.76153846153846%, Loss = 0.013396327885297628
Epoch: 206, Batch Gradient Norm: 0.7138394786479422
Epoch: 206, Batch Gradient Norm after: 0.7138394786479422
Epoch 207/10000, Prediction Accuracy = 52.96153846153846%, Loss = 0.013272693309073266
Epoch: 207, Batch Gradient Norm: 0.7311394451169531
Epoch: 207, Batch Gradient Norm after: 0.7311394451169531
Epoch 208/10000, Prediction Accuracy = 52.73461538461539%, Loss = 0.01328082626255659
Epoch: 208, Batch Gradient Norm: 0.8134667943956261
Epoch: 208, Batch Gradient Norm after: 0.8134667943956261
Epoch 209/10000, Prediction Accuracy = 52.23846153846154%, Loss = 0.01344616524875164
Epoch: 209, Batch Gradient Norm: 0.7712249908516084
Epoch: 209, Batch Gradient Norm after: 0.7712249908516084
Epoch 210/10000, Prediction Accuracy = 52.69615384615384%, Loss = 0.013352506005993256
Epoch: 210, Batch Gradient Norm: 0.7392796022490459
Epoch: 210, Batch Gradient Norm after: 0.7392796022490459
Epoch 211/10000, Prediction Accuracy = 52.53076923076923%, Loss = 0.013273944146931171
Epoch: 211, Batch Gradient Norm: 0.6673141941277855
Epoch: 211, Batch Gradient Norm after: 0.6673141941277855
Epoch 212/10000, Prediction Accuracy = 53.11538461538463%, Loss = 0.013127908253898988
Epoch: 212, Batch Gradient Norm: 0.8440197551627734
Epoch: 212, Batch Gradient Norm after: 0.8440197551627734
Epoch 213/10000, Prediction Accuracy = 52.44230769230769%, Loss = 0.01337433928767076
Epoch: 213, Batch Gradient Norm: 0.8131060584313534
Epoch: 213, Batch Gradient Norm after: 0.8131060584313534
Epoch 214/10000, Prediction Accuracy = 52.81538461538462%, Loss = 0.013336627457577448
Epoch: 214, Batch Gradient Norm: 0.7817008068349383
Epoch: 214, Batch Gradient Norm after: 0.7817008068349383
Epoch 215/10000, Prediction Accuracy = 52.86538461538461%, Loss = 0.013235926054991208
Epoch: 215, Batch Gradient Norm: 0.7919162839242788
Epoch: 215, Batch Gradient Norm after: 0.7919162839242788
Epoch 216/10000, Prediction Accuracy = 53.21153846153846%, Loss = 0.013156754824404534
Epoch: 216, Batch Gradient Norm: 0.7889869945743573
Epoch: 216, Batch Gradient Norm after: 0.7889869945743573
Epoch 217/10000, Prediction Accuracy = 53.05%, Loss = 0.01316073164343834
Epoch: 217, Batch Gradient Norm: 0.8036701665923788
Epoch: 217, Batch Gradient Norm after: 0.8036701665923788
Epoch 218/10000, Prediction Accuracy = 53.17307692307692%, Loss = 0.013128149824646803
Epoch: 218, Batch Gradient Norm: 0.9304035358794702
Epoch: 218, Batch Gradient Norm after: 0.9304035358794702
Epoch 219/10000, Prediction Accuracy = 52.607692307692304%, Loss = 0.01327879404505858
Epoch: 219, Batch Gradient Norm: 0.9175916125598508
Epoch: 219, Batch Gradient Norm after: 0.9175916125598508
Epoch 220/10000, Prediction Accuracy = 52.76923076923076%, Loss = 0.013318644048502812
Epoch: 220, Batch Gradient Norm: 0.7606054401510114
Epoch: 220, Batch Gradient Norm after: 0.7606054401510114
Epoch 221/10000, Prediction Accuracy = 53.27307692307693%, Loss = 0.013070247709178008
Epoch: 221, Batch Gradient Norm: 0.8003600451702664
Epoch: 221, Batch Gradient Norm after: 0.8003600451702664
Epoch 222/10000, Prediction Accuracy = 53.073076923076925%, Loss = 0.013050165099020187
Epoch: 222, Batch Gradient Norm: 0.7998423628305702
Epoch: 222, Batch Gradient Norm after: 0.7998423628305702
Epoch 223/10000, Prediction Accuracy = 53.25769230769231%, Loss = 0.013067288825718256
Epoch: 223, Batch Gradient Norm: 0.8473688919274801
Epoch: 223, Batch Gradient Norm after: 0.8473688919274801
Epoch 224/10000, Prediction Accuracy = 52.75384615384616%, Loss = 0.013155550266114565
Epoch: 224, Batch Gradient Norm: 0.7987906601730491
Epoch: 224, Batch Gradient Norm after: 0.7987906601730491
Epoch 225/10000, Prediction Accuracy = 53.29230769230769%, Loss = 0.013020049207485639
Epoch: 225, Batch Gradient Norm: 0.762728631615213
Epoch: 225, Batch Gradient Norm after: 0.762728631615213
Epoch 226/10000, Prediction Accuracy = 53.33846153846154%, Loss = 0.012952172054121127
Epoch: 226, Batch Gradient Norm: 0.7595214405751769
Epoch: 226, Batch Gradient Norm after: 0.7595214405751769
Epoch 227/10000, Prediction Accuracy = 53.4076923076923%, Loss = 0.012909407154298745
Epoch: 227, Batch Gradient Norm: 0.8073887459785136
Epoch: 227, Batch Gradient Norm after: 0.8073887459785136
Epoch 228/10000, Prediction Accuracy = 53.25769230769232%, Loss = 0.012981565955739755
Epoch: 228, Batch Gradient Norm: 0.825301517338742
Epoch: 228, Batch Gradient Norm after: 0.825301517338742
Epoch 229/10000, Prediction Accuracy = 53.77307692307692%, Loss = 0.01294399776424353
Epoch: 229, Batch Gradient Norm: 0.8199091440172938
Epoch: 229, Batch Gradient Norm after: 0.8199091440172938
Epoch 230/10000, Prediction Accuracy = 53.39999999999999%, Loss = 0.012954346692332855
Epoch: 230, Batch Gradient Norm: 0.8690222898979174
Epoch: 230, Batch Gradient Norm after: 0.8690222898979174
Epoch 231/10000, Prediction Accuracy = 53.62307692307692%, Loss = 0.013001419818745209
Epoch: 231, Batch Gradient Norm: 0.7962432965754138
Epoch: 231, Batch Gradient Norm after: 0.7962432965754138
Epoch 232/10000, Prediction Accuracy = 53.576923076923066%, Loss = 0.012838427049036209
Epoch: 232, Batch Gradient Norm: 0.8126700651724971
Epoch: 232, Batch Gradient Norm after: 0.8126700651724971
Epoch 233/10000, Prediction Accuracy = 53.62692307692308%, Loss = 0.012802267375473794
Epoch: 233, Batch Gradient Norm: 0.8645883136287609
Epoch: 233, Batch Gradient Norm after: 0.8645883136287609
Epoch 234/10000, Prediction Accuracy = 52.84615384615384%, Loss = 0.012988934723230509
Epoch: 234, Batch Gradient Norm: 0.8691208071748212
Epoch: 234, Batch Gradient Norm after: 0.8691208071748212
Epoch 235/10000, Prediction Accuracy = 53.550000000000004%, Loss = 0.012931750299265752
Epoch: 235, Batch Gradient Norm: 0.86533670558549
Epoch: 235, Batch Gradient Norm after: 0.86533670558549
Epoch 236/10000, Prediction Accuracy = 53.53846153846153%, Loss = 0.012847918658875503
Epoch: 236, Batch Gradient Norm: 0.8790647248399268
Epoch: 236, Batch Gradient Norm after: 0.8790647248399268
Epoch 237/10000, Prediction Accuracy = 53.473076923076924%, Loss = 0.012838128309410352
Epoch: 237, Batch Gradient Norm: 0.9704777495592406
Epoch: 237, Batch Gradient Norm after: 0.9704777495592406
Epoch 238/10000, Prediction Accuracy = 53.16153846153846%, Loss = 0.012936311702315625
Epoch: 238, Batch Gradient Norm: 0.8786178967216494
Epoch: 238, Batch Gradient Norm after: 0.8786178967216494
Epoch 239/10000, Prediction Accuracy = 53.98076923076923%, Loss = 0.012749328492925717
Epoch: 239, Batch Gradient Norm: 0.9306525962287999
Epoch: 239, Batch Gradient Norm after: 0.9306525962287999
Epoch 240/10000, Prediction Accuracy = 53.44615384615384%, Loss = 0.012852297451060552
Epoch: 240, Batch Gradient Norm: 0.858967689230158
Epoch: 240, Batch Gradient Norm after: 0.858967689230158
Epoch 241/10000, Prediction Accuracy = 53.92307692307692%, Loss = 0.012711877003312111
Epoch: 241, Batch Gradient Norm: 0.8187695315001131
Epoch: 241, Batch Gradient Norm after: 0.8187695315001131
Epoch 242/10000, Prediction Accuracy = 53.853846153846156%, Loss = 0.012622577568086294
Epoch: 242, Batch Gradient Norm: 0.8147880350690242
Epoch: 242, Batch Gradient Norm after: 0.8147880350690242
Epoch 243/10000, Prediction Accuracy = 53.86153846153847%, Loss = 0.01265205736630238
Epoch: 243, Batch Gradient Norm: 0.8615914367825732
Epoch: 243, Batch Gradient Norm after: 0.8615914367825732
Epoch 244/10000, Prediction Accuracy = 53.90384615384615%, Loss = 0.012724246256626569
Epoch: 244, Batch Gradient Norm: 0.9594963836709732
Epoch: 244, Batch Gradient Norm after: 0.9594963836709732
Epoch 245/10000, Prediction Accuracy = 53.42307692307692%, Loss = 0.012783086787049588
Epoch: 245, Batch Gradient Norm: 0.9435689144059103
Epoch: 245, Batch Gradient Norm after: 0.9435689144059103
Epoch 246/10000, Prediction Accuracy = 53.638461538461556%, Loss = 0.012737541244580196
Epoch: 246, Batch Gradient Norm: 0.8751609404475379
Epoch: 246, Batch Gradient Norm after: 0.8751609404475379
Epoch 247/10000, Prediction Accuracy = 54.06923076923077%, Loss = 0.012614079106312532
Epoch: 247, Batch Gradient Norm: 0.8635637611869806
Epoch: 247, Batch Gradient Norm after: 0.8635637611869806
Epoch 248/10000, Prediction Accuracy = 54.06153846153847%, Loss = 0.012559337756381584
Epoch: 248, Batch Gradient Norm: 0.8589855004431979
Epoch: 248, Batch Gradient Norm after: 0.8589855004431979
Epoch 249/10000, Prediction Accuracy = 54.28076923076924%, Loss = 0.012611554147532353
Epoch: 249, Batch Gradient Norm: 0.8774077285741138
Epoch: 249, Batch Gradient Norm after: 0.8774077285741138
Epoch 250/10000, Prediction Accuracy = 54.09615384615385%, Loss = 0.012567033418096028
Epoch: 250, Batch Gradient Norm: 0.882591577801874
Epoch: 250, Batch Gradient Norm after: 0.882591577801874
Epoch 251/10000, Prediction Accuracy = 54.28076923076923%, Loss = 0.012557492949641667
Epoch: 251, Batch Gradient Norm: 0.9078149238732477
Epoch: 251, Batch Gradient Norm after: 0.9078149238732477
Epoch 252/10000, Prediction Accuracy = 53.984615384615374%, Loss = 0.012628341093659401
Epoch: 252, Batch Gradient Norm: 0.9325057505733251
Epoch: 252, Batch Gradient Norm after: 0.9325057505733251
Epoch 253/10000, Prediction Accuracy = 54.11153846153847%, Loss = 0.012613561004400253
Epoch: 253, Batch Gradient Norm: 0.9299713286603705
Epoch: 253, Batch Gradient Norm after: 0.9299713286603705
Epoch 254/10000, Prediction Accuracy = 53.96923076923077%, Loss = 0.012542329298762174
Epoch: 254, Batch Gradient Norm: 0.9559629165586062
Epoch: 254, Batch Gradient Norm after: 0.9559629165586062
Epoch 255/10000, Prediction Accuracy = 54.2%, Loss = 0.012554290489508556
Epoch: 255, Batch Gradient Norm: 0.9433821565988264
Epoch: 255, Batch Gradient Norm after: 0.9433821565988264
Epoch 256/10000, Prediction Accuracy = 54.43846153846153%, Loss = 0.012502990806332001
Epoch: 256, Batch Gradient Norm: 0.9104618790217507
Epoch: 256, Batch Gradient Norm after: 0.9104618790217507
Epoch 257/10000, Prediction Accuracy = 54.2846153846154%, Loss = 0.01248844526708126
Epoch: 257, Batch Gradient Norm: 0.9968376900605381
Epoch: 257, Batch Gradient Norm after: 0.9968376900605381
Epoch 258/10000, Prediction Accuracy = 54.00000000000001%, Loss = 0.012602238557659663
Epoch: 258, Batch Gradient Norm: 0.9289308811219558
Epoch: 258, Batch Gradient Norm after: 0.9289308811219558
Epoch 259/10000, Prediction Accuracy = 54.37307692307692%, Loss = 0.01247281008041822
Epoch: 259, Batch Gradient Norm: 0.9367035646851565
Epoch: 259, Batch Gradient Norm after: 0.9367035646851565
Epoch 260/10000, Prediction Accuracy = 54.7%, Loss = 0.012405270925508095
Epoch: 260, Batch Gradient Norm: 1.027528842346739
Epoch: 260, Batch Gradient Norm after: 1.027528842346739
Epoch 261/10000, Prediction Accuracy = 54.00384615384615%, Loss = 0.012546070421544405
Epoch: 261, Batch Gradient Norm: 0.913990637264048
Epoch: 261, Batch Gradient Norm after: 0.913990637264048
Epoch 262/10000, Prediction Accuracy = 54.615384615384606%, Loss = 0.012424695807007642
Epoch: 262, Batch Gradient Norm: 1.013689231001858
Epoch: 262, Batch Gradient Norm after: 1.013689231001858
Epoch 263/10000, Prediction Accuracy = 54.75384615384615%, Loss = 0.012468761549546169
Epoch: 263, Batch Gradient Norm: 0.8933520114765388
Epoch: 263, Batch Gradient Norm after: 0.8933520114765388
Epoch 264/10000, Prediction Accuracy = 54.33846153846154%, Loss = 0.01231140005760468
Epoch: 264, Batch Gradient Norm: 0.9295316972068758
Epoch: 264, Batch Gradient Norm after: 0.9295316972068758
Epoch 265/10000, Prediction Accuracy = 54.45384615384615%, Loss = 0.012344624775533494
Epoch: 265, Batch Gradient Norm: 1.0060449112744267
Epoch: 265, Batch Gradient Norm after: 1.0060449112744267
Epoch 266/10000, Prediction Accuracy = 54.534615384615385%, Loss = 0.012384243739339022
Epoch: 266, Batch Gradient Norm: 1.0592755080922036
Epoch: 266, Batch Gradient Norm after: 1.0592755080922036
Epoch 267/10000, Prediction Accuracy = 54.18076923076924%, Loss = 0.012498579059655849
Epoch: 267, Batch Gradient Norm: 0.9135094941554048
Epoch: 267, Batch Gradient Norm after: 0.9135094941554048
Epoch 268/10000, Prediction Accuracy = 54.91923076923077%, Loss = 0.012244029758641353
Epoch: 268, Batch Gradient Norm: 1.0271440302913963
Epoch: 268, Batch Gradient Norm after: 1.0271440302913963
Epoch 269/10000, Prediction Accuracy = 54.5923076923077%, Loss = 0.012405720825951833
Epoch: 269, Batch Gradient Norm: 0.967902377502271
Epoch: 269, Batch Gradient Norm after: 0.967902377502271
Epoch 270/10000, Prediction Accuracy = 54.815384615384616%, Loss = 0.01231878580382237
Epoch: 270, Batch Gradient Norm: 0.9865141191058893
Epoch: 270, Batch Gradient Norm after: 0.9865141191058893
Epoch 271/10000, Prediction Accuracy = 54.55769230769231%, Loss = 0.012296343723741861
Epoch: 271, Batch Gradient Norm: 0.9902592425384388
Epoch: 271, Batch Gradient Norm after: 0.9902592425384388
Epoch 272/10000, Prediction Accuracy = 54.79615384615385%, Loss = 0.012331261514471127
Epoch: 272, Batch Gradient Norm: 0.9645327199086923
Epoch: 272, Batch Gradient Norm after: 0.9645327199086923
Epoch 273/10000, Prediction Accuracy = 55.06153846153845%, Loss = 0.012222821824252605
Epoch: 273, Batch Gradient Norm: 0.9975415797828906
Epoch: 273, Batch Gradient Norm after: 0.9975415797828906
Epoch 274/10000, Prediction Accuracy = 54.83461538461539%, Loss = 0.012227017647371842
Epoch: 274, Batch Gradient Norm: 1.0581011296665916
Epoch: 274, Batch Gradient Norm after: 1.0581011296665916
Epoch 275/10000, Prediction Accuracy = 54.68461538461539%, Loss = 0.012222357309208466
Epoch: 275, Batch Gradient Norm: 1.0680465090860485
Epoch: 275, Batch Gradient Norm after: 1.0680465090860485
Epoch 276/10000, Prediction Accuracy = 54.65384615384615%, Loss = 0.012276318640663074
Epoch: 276, Batch Gradient Norm: 1.0016799589698626
Epoch: 276, Batch Gradient Norm after: 1.0016799589698626
Epoch 277/10000, Prediction Accuracy = 54.91538461538461%, Loss = 0.01223165957400432
Epoch: 277, Batch Gradient Norm: 0.9987119325468738
Epoch: 277, Batch Gradient Norm after: 0.9987119325468738
Epoch 278/10000, Prediction Accuracy = 54.76538461538462%, Loss = 0.012177426654558916
Epoch: 278, Batch Gradient Norm: 1.0034240130320453
Epoch: 278, Batch Gradient Norm after: 1.0034240130320453
Epoch 279/10000, Prediction Accuracy = 55.24999999999999%, Loss = 0.012181009380863262
Epoch: 279, Batch Gradient Norm: 0.9587954935977056
Epoch: 279, Batch Gradient Norm after: 0.9587954935977056
Epoch 280/10000, Prediction Accuracy = 55.08846153846154%, Loss = 0.012066851584957195
Epoch: 280, Batch Gradient Norm: 0.9660273744397652
Epoch: 280, Batch Gradient Norm after: 0.9660273744397652
Epoch 281/10000, Prediction Accuracy = 55.09615384615385%, Loss = 0.012046770837444525
Epoch: 281, Batch Gradient Norm: 1.0228747877879047
Epoch: 281, Batch Gradient Norm after: 1.0228747877879047
Epoch 282/10000, Prediction Accuracy = 54.96153846153846%, Loss = 0.0121406835432236
Epoch: 282, Batch Gradient Norm: 1.09969987661582
Epoch: 282, Batch Gradient Norm after: 1.09969987661582
Epoch 283/10000, Prediction Accuracy = 54.719230769230755%, Loss = 0.012241053108412486
Epoch: 283, Batch Gradient Norm: 1.1195503746841111
Epoch: 283, Batch Gradient Norm after: 1.1195503746841111
Epoch 284/10000, Prediction Accuracy = 54.73461538461539%, Loss = 0.012249149310474213
Epoch: 284, Batch Gradient Norm: 1.0562373093129382
Epoch: 284, Batch Gradient Norm after: 1.0562373093129382
Epoch 285/10000, Prediction Accuracy = 55.18076923076923%, Loss = 0.012182598432096152
Epoch: 285, Batch Gradient Norm: 1.1327499820167515
Epoch: 285, Batch Gradient Norm after: 1.1327499820167515
Epoch 286/10000, Prediction Accuracy = 55.065384615384616%, Loss = 0.012149954931094097
Epoch: 286, Batch Gradient Norm: 1.1089992759849907
Epoch: 286, Batch Gradient Norm after: 1.1089992759849907
Epoch 287/10000, Prediction Accuracy = 54.62307692307692%, Loss = 0.012155444217989078
Epoch: 287, Batch Gradient Norm: 1.0024704025581892
Epoch: 287, Batch Gradient Norm after: 1.0024704025581892
Epoch 288/10000, Prediction Accuracy = 55.50769230769231%, Loss = 0.012010668332760151
Epoch: 288, Batch Gradient Norm: 1.0426877685243843
Epoch: 288, Batch Gradient Norm after: 1.0426877685243843
Epoch 289/10000, Prediction Accuracy = 55.392307692307696%, Loss = 0.012066516022269543
Epoch: 289, Batch Gradient Norm: 1.0156928263702114
Epoch: 289, Batch Gradient Norm after: 1.0156928263702114
Epoch 290/10000, Prediction Accuracy = 55.31153846153847%, Loss = 0.011974243877025751
Epoch: 290, Batch Gradient Norm: 0.9497303319584501
Epoch: 290, Batch Gradient Norm after: 0.9497303319584501
Epoch 291/10000, Prediction Accuracy = 55.53846153846154%, Loss = 0.011949005536735058
Epoch: 291, Batch Gradient Norm: 0.9902493163034588
Epoch: 291, Batch Gradient Norm after: 0.9902493163034588
Epoch 292/10000, Prediction Accuracy = 55.646153846153844%, Loss = 0.011909394524991512
Epoch: 292, Batch Gradient Norm: 1.0193986401097153
Epoch: 292, Batch Gradient Norm after: 1.0193986401097153
Epoch 293/10000, Prediction Accuracy = 54.90384615384615%, Loss = 0.011952695221855091
Epoch: 293, Batch Gradient Norm: 1.0303473169377368
Epoch: 293, Batch Gradient Norm after: 1.0303473169377368
Epoch 294/10000, Prediction Accuracy = 55.892307692307696%, Loss = 0.011886910105553957
Epoch: 294, Batch Gradient Norm: 1.0942805385450982
Epoch: 294, Batch Gradient Norm after: 1.0942805385450982
Epoch 295/10000, Prediction Accuracy = 55.54615384615385%, Loss = 0.01194011176434847
Epoch: 295, Batch Gradient Norm: 1.079041935891705
Epoch: 295, Batch Gradient Norm after: 1.079041935891705
Epoch 296/10000, Prediction Accuracy = 55.396153846153844%, Loss = 0.011980862786563544
Epoch: 296, Batch Gradient Norm: 1.0319866789120797
Epoch: 296, Batch Gradient Norm after: 1.0319866789120797
Epoch 297/10000, Prediction Accuracy = 55.6923076923077%, Loss = 0.011889179881948691
Epoch: 297, Batch Gradient Norm: 1.0051218358297689
Epoch: 297, Batch Gradient Norm after: 1.0051218358297689
Epoch 298/10000, Prediction Accuracy = 55.50384615384616%, Loss = 0.011846766950419316
Epoch: 298, Batch Gradient Norm: 1.058377940668604
Epoch: 298, Batch Gradient Norm after: 1.058377940668604
Epoch 299/10000, Prediction Accuracy = 55.51923076923076%, Loss = 0.011844379755739983
Epoch: 299, Batch Gradient Norm: 1.0833258933391237
Epoch: 299, Batch Gradient Norm after: 1.0833258933391237
Epoch 300/10000, Prediction Accuracy = 55.57692307692308%, Loss = 0.011881363506500538
Epoch: 300, Batch Gradient Norm: 1.048960592233847
Epoch: 300, Batch Gradient Norm after: 1.048960592233847
Epoch 301/10000, Prediction Accuracy = 55.611538461538466%, Loss = 0.011841102574880306
Epoch: 301, Batch Gradient Norm: 1.1144093566461977
Epoch: 301, Batch Gradient Norm after: 1.1144093566461977
Epoch 302/10000, Prediction Accuracy = 55.784615384615385%, Loss = 0.01192944862235051
Epoch: 302, Batch Gradient Norm: 1.1065024715932952
Epoch: 302, Batch Gradient Norm after: 1.1065024715932952
Epoch 303/10000, Prediction Accuracy = 55.61538461538461%, Loss = 0.011852491432084488
Epoch: 303, Batch Gradient Norm: 1.1024058598801416
Epoch: 303, Batch Gradient Norm after: 1.1024058598801416
Epoch 304/10000, Prediction Accuracy = 55.323076923076925%, Loss = 0.011871248053816648
Epoch: 304, Batch Gradient Norm: 1.0972775471811156
Epoch: 304, Batch Gradient Norm after: 1.0972775471811156
Epoch 305/10000, Prediction Accuracy = 55.457692307692305%, Loss = 0.011865730325763043
Epoch: 305, Batch Gradient Norm: 1.063237148686925
Epoch: 305, Batch Gradient Norm after: 1.063237148686925
Epoch 306/10000, Prediction Accuracy = 55.29230769230769%, Loss = 0.011811781101501904
Epoch: 306, Batch Gradient Norm: 1.0716217061589137
Epoch: 306, Batch Gradient Norm after: 1.0716217061589137
Epoch 307/10000, Prediction Accuracy = 55.73076923076923%, Loss = 0.01178894772265966
Epoch: 307, Batch Gradient Norm: 0.9841878477550352
Epoch: 307, Batch Gradient Norm after: 0.9841878477550352
Epoch 308/10000, Prediction Accuracy = 56.02307692307693%, Loss = 0.01161743664684204
Epoch: 308, Batch Gradient Norm: 1.0169385876050465
Epoch: 308, Batch Gradient Norm after: 1.0169385876050465
Epoch 309/10000, Prediction Accuracy = 56.088461538461544%, Loss = 0.011689357531185333
Epoch: 309, Batch Gradient Norm: 0.9934771679072735
Epoch: 309, Batch Gradient Norm after: 0.9934771679072735
Epoch 310/10000, Prediction Accuracy = 56.30384615384615%, Loss = 0.011634957761718677
Epoch: 310, Batch Gradient Norm: 1.08916223401333
Epoch: 310, Batch Gradient Norm after: 1.08916223401333
Epoch 311/10000, Prediction Accuracy = 55.650000000000006%, Loss = 0.011677932137480149
Epoch: 311, Batch Gradient Norm: 1.081657397883197
Epoch: 311, Batch Gradient Norm after: 1.081657397883197
Epoch 312/10000, Prediction Accuracy = 55.61923076923077%, Loss = 0.01173831013819346
Epoch: 312, Batch Gradient Norm: 1.083491590740019
Epoch: 312, Batch Gradient Norm after: 1.083491590740019
Epoch 313/10000, Prediction Accuracy = 55.91153846153846%, Loss = 0.011661309462327223
Epoch: 313, Batch Gradient Norm: 1.1439484781927691
Epoch: 313, Batch Gradient Norm after: 1.1439484781927691
Epoch 314/10000, Prediction Accuracy = 56.08076923076922%, Loss = 0.011692452745941969
Epoch: 314, Batch Gradient Norm: 1.2977200834040747
Epoch: 314, Batch Gradient Norm after: 1.2977200834040747
Epoch 315/10000, Prediction Accuracy = 55.592307692307685%, Loss = 0.011885723600593897
Epoch: 315, Batch Gradient Norm: 1.1388221419530231
Epoch: 315, Batch Gradient Norm after: 1.1388221419530231
Epoch 316/10000, Prediction Accuracy = 55.923076923076934%, Loss = 0.011675802847513786
Epoch: 316, Batch Gradient Norm: 1.1121393803280744
Epoch: 316, Batch Gradient Norm after: 1.1121393803280744
Epoch 317/10000, Prediction Accuracy = 55.965384615384615%, Loss = 0.011620651429089216
Epoch: 317, Batch Gradient Norm: 1.1436474722881107
Epoch: 317, Batch Gradient Norm after: 1.1436474722881107
Epoch 318/10000, Prediction Accuracy = 55.93846153846153%, Loss = 0.011684908531606197
Epoch: 318, Batch Gradient Norm: 1.191547111382731
Epoch: 318, Batch Gradient Norm after: 1.191547111382731
Epoch 319/10000, Prediction Accuracy = 55.776923076923076%, Loss = 0.011715978670578737
Epoch 00319: reducing learning rate of group 0 to 1.0000e-05.
Epoch: 319, Batch Gradient Norm: 0.8637098030889845
Epoch: 319, Batch Gradient Norm after: 0.8637098030889845
Epoch 320/10000, Prediction Accuracy = 57.373076923076916%, Loss = 0.010899349617270323
Epoch: 320, Batch Gradient Norm: 0.6873512346506839
Epoch: 320, Batch Gradient Norm after: 0.6873512346506839
Epoch 321/10000, Prediction Accuracy = 58.23461538461538%, Loss = 0.010600788733707024
Epoch: 321, Batch Gradient Norm: 0.6635694812834018
Epoch: 321, Batch Gradient Norm after: 0.6635694812834018
Epoch 322/10000, Prediction Accuracy = 58.7%, Loss = 0.01052069878922059
Epoch: 322, Batch Gradient Norm: 0.6349214087217494
Epoch: 322, Batch Gradient Norm after: 0.6349214087217494
Epoch 323/10000, Prediction Accuracy = 58.73461538461538%, Loss = 0.010452844560719453
Epoch: 323, Batch Gradient Norm: 0.6394786680480762
Epoch: 323, Batch Gradient Norm after: 0.6394786680480762
Epoch 324/10000, Prediction Accuracy = 58.90384615384615%, Loss = 0.010428664847635306
Epoch: 324, Batch Gradient Norm: 0.6655844494488167
Epoch: 324, Batch Gradient Norm after: 0.6655844494488167
Epoch 325/10000, Prediction Accuracy = 58.66153846153846%, Loss = 0.010439594491170002
Epoch: 325, Batch Gradient Norm: 0.6551145847446304
Epoch: 325, Batch Gradient Norm after: 0.6551145847446304
Epoch 326/10000, Prediction Accuracy = 58.8%, Loss = 0.010417061499678172
Epoch: 326, Batch Gradient Norm: 0.6640846553166775
Epoch: 326, Batch Gradient Norm after: 0.6640846553166775
Epoch 327/10000, Prediction Accuracy = 58.73461538461539%, Loss = 0.010471222205803944
Epoch: 327, Batch Gradient Norm: 0.6503603089196974
Epoch: 327, Batch Gradient Norm after: 0.6503603089196974
Epoch 328/10000, Prediction Accuracy = 58.82307692307693%, Loss = 0.010394915508536192
Epoch: 328, Batch Gradient Norm: 0.6535492918183344
Epoch: 328, Batch Gradient Norm after: 0.6535492918183344
Epoch 329/10000, Prediction Accuracy = 58.63846153846154%, Loss = 0.010377409151540352
Epoch: 329, Batch Gradient Norm: 0.6595191562082835
Epoch: 329, Batch Gradient Norm after: 0.6595191562082835
Epoch 330/10000, Prediction Accuracy = 58.91538461538461%, Loss = 0.01037806193702496
Epoch: 330, Batch Gradient Norm: 0.6643988474143353
Epoch: 330, Batch Gradient Norm after: 0.6643988474143353
Epoch 331/10000, Prediction Accuracy = 58.84999999999999%, Loss = 0.010373348083633643
Epoch: 331, Batch Gradient Norm: 0.6573665952275012
Epoch: 331, Batch Gradient Norm after: 0.6573665952275012
Epoch 332/10000, Prediction Accuracy = 58.75%, Loss = 0.010426437912079005
Epoch: 332, Batch Gradient Norm: 0.6563628730998087
Epoch: 332, Batch Gradient Norm after: 0.6563628730998087
Epoch 333/10000, Prediction Accuracy = 58.86923076923078%, Loss = 0.010420716009460963
Epoch: 333, Batch Gradient Norm: 0.6689359844455166
Epoch: 333, Batch Gradient Norm after: 0.6689359844455166
Epoch 334/10000, Prediction Accuracy = 58.92692307692307%, Loss = 0.01037587232601184
Epoch: 334, Batch Gradient Norm: 0.6660212685923317
Epoch: 334, Batch Gradient Norm after: 0.6660212685923317
Epoch 335/10000, Prediction Accuracy = 59.119230769230775%, Loss = 0.010362357975771794
Epoch: 335, Batch Gradient Norm: 0.6654258453182939
Epoch: 335, Batch Gradient Norm after: 0.6654258453182939
Epoch 336/10000, Prediction Accuracy = 59.00384615384616%, Loss = 0.010328929942960922
Epoch: 336, Batch Gradient Norm: 0.6862413368027737
Epoch: 336, Batch Gradient Norm after: 0.6862413368027737
Epoch 337/10000, Prediction Accuracy = 59.01923076923077%, Loss = 0.010341244534804271
Epoch: 337, Batch Gradient Norm: 0.6824545237248233
Epoch: 337, Batch Gradient Norm after: 0.6824545237248233
Epoch 338/10000, Prediction Accuracy = 58.87692307692308%, Loss = 0.010381032927678181
Epoch: 338, Batch Gradient Norm: 0.6704238936627595
Epoch: 338, Batch Gradient Norm after: 0.6704238936627595
Epoch 339/10000, Prediction Accuracy = 58.99615384615384%, Loss = 0.010341750816083871
Epoch: 339, Batch Gradient Norm: 0.6644653766133041
Epoch: 339, Batch Gradient Norm after: 0.6644653766133041
Epoch 340/10000, Prediction Accuracy = 58.830769230769235%, Loss = 0.010354322452957813
Epoch: 340, Batch Gradient Norm: 0.6971955597452715
Epoch: 340, Batch Gradient Norm after: 0.6971955597452715
Epoch 341/10000, Prediction Accuracy = 58.888461538461534%, Loss = 0.010331716746664964
Epoch: 341, Batch Gradient Norm: 0.6810989260309741
Epoch: 341, Batch Gradient Norm after: 0.6810989260309741
Epoch 342/10000, Prediction Accuracy = 58.75384615384616%, Loss = 0.010329796574436702
Epoch: 342, Batch Gradient Norm: 0.6885940261614214
Epoch: 342, Batch Gradient Norm after: 0.6885940261614214
Epoch 343/10000, Prediction Accuracy = 58.99615384615385%, Loss = 0.010339405674200792
Epoch: 343, Batch Gradient Norm: 0.677033487173298
Epoch: 343, Batch Gradient Norm after: 0.677033487173298
Epoch 344/10000, Prediction Accuracy = 58.86538461538461%, Loss = 0.01028899705180755
Epoch: 344, Batch Gradient Norm: 0.6790820844733799
Epoch: 344, Batch Gradient Norm after: 0.6790820844733799
Epoch 345/10000, Prediction Accuracy = 59.130769230769225%, Loss = 0.010274184294618093
Epoch: 345, Batch Gradient Norm: 0.6978458906551894
Epoch: 345, Batch Gradient Norm after: 0.6978458906551894
Epoch 346/10000, Prediction Accuracy = 59.01923076923077%, Loss = 0.010339475953235077
Epoch: 346, Batch Gradient Norm: 0.7054568449134502
Epoch: 346, Batch Gradient Norm after: 0.7054568449134502
Epoch 347/10000, Prediction Accuracy = 58.97692307692307%, Loss = 0.01032613912740579
Epoch: 347, Batch Gradient Norm: 0.6992800882502685
Epoch: 347, Batch Gradient Norm after: 0.6992800882502685
Epoch 348/10000, Prediction Accuracy = 59.00384615384616%, Loss = 0.010321730532898353
Epoch: 348, Batch Gradient Norm: 0.7007535073934521
Epoch: 348, Batch Gradient Norm after: 0.7007535073934521
Epoch 349/10000, Prediction Accuracy = 58.93076923076923%, Loss = 0.010329772145129167
Epoch: 349, Batch Gradient Norm: 0.671694581331996
Epoch: 349, Batch Gradient Norm after: 0.671694581331996
Epoch 350/10000, Prediction Accuracy = 58.957692307692305%, Loss = 0.010310934498333015
Epoch: 350, Batch Gradient Norm: 0.708359693392786
Epoch: 350, Batch Gradient Norm after: 0.708359693392786
Epoch 351/10000, Prediction Accuracy = 58.76923076923077%, Loss = 0.01033657524161614
Epoch: 351, Batch Gradient Norm: 0.6951019320830341
Epoch: 351, Batch Gradient Norm after: 0.6951019320830341
Epoch 352/10000, Prediction Accuracy = 59.02307692307692%, Loss = 0.010307072231975885
Epoch: 352, Batch Gradient Norm: 0.692336875095053
Epoch: 352, Batch Gradient Norm after: 0.692336875095053
Epoch 353/10000, Prediction Accuracy = 59.02307692307693%, Loss = 0.010277005628897594
Epoch: 353, Batch Gradient Norm: 0.715457446083343
Epoch: 353, Batch Gradient Norm after: 0.715457446083343
Epoch 354/10000, Prediction Accuracy = 58.81538461538461%, Loss = 0.010288411106627721
Epoch: 354, Batch Gradient Norm: 0.6999414648325516
Epoch: 354, Batch Gradient Norm after: 0.6999414648325516
Epoch 355/10000, Prediction Accuracy = 59.12692307692306%, Loss = 0.01025402839653767
Epoch: 355, Batch Gradient Norm: 0.7299012899408677
Epoch: 355, Batch Gradient Norm after: 0.7299012899408677
Epoch 356/10000, Prediction Accuracy = 58.96923076923076%, Loss = 0.01033686839330655
Epoch: 356, Batch Gradient Norm: 0.7078863049787033
Epoch: 356, Batch Gradient Norm after: 0.7078863049787033
Epoch 357/10000, Prediction Accuracy = 58.79230769230769%, Loss = 0.010284918217131725
Epoch: 357, Batch Gradient Norm: 0.6872980205714587
Epoch: 357, Batch Gradient Norm after: 0.6872980205714587
Epoch 358/10000, Prediction Accuracy = 59.17307692307692%, Loss = 0.010216378535215672
Epoch: 358, Batch Gradient Norm: 0.701717543803519
Epoch: 358, Batch Gradient Norm after: 0.701717543803519
Epoch 359/10000, Prediction Accuracy = 59.11923076923077%, Loss = 0.01026057108090474
Epoch: 359, Batch Gradient Norm: 0.7137062314545333
Epoch: 359, Batch Gradient Norm after: 0.7137062314545333
Epoch 360/10000, Prediction Accuracy = 59.20384615384616%, Loss = 0.01024314073415903
Epoch: 360, Batch Gradient Norm: 0.704342538919405
Epoch: 360, Batch Gradient Norm after: 0.704342538919405
Epoch 361/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010248160706116604
Epoch: 361, Batch Gradient Norm: 0.7367819335486009
Epoch: 361, Batch Gradient Norm after: 0.7367819335486009
Epoch 362/10000, Prediction Accuracy = 58.96153846153846%, Loss = 0.01028949602578695
Epoch: 362, Batch Gradient Norm: 0.7026616065974934
Epoch: 362, Batch Gradient Norm after: 0.7026616065974934
Epoch 363/10000, Prediction Accuracy = 59.015384615384626%, Loss = 0.01025549995784576
Epoch: 363, Batch Gradient Norm: 0.7325824656370163
Epoch: 363, Batch Gradient Norm after: 0.7325824656370163
Epoch 364/10000, Prediction Accuracy = 58.96153846153847%, Loss = 0.010296454868064476
Epoch: 364, Batch Gradient Norm: 0.7311046516406461
Epoch: 364, Batch Gradient Norm after: 0.7311046516406461
Epoch 365/10000, Prediction Accuracy = 59.06153846153846%, Loss = 0.01028645009948657
Epoch: 365, Batch Gradient Norm: 0.732586553620467
Epoch: 365, Batch Gradient Norm after: 0.732586553620467
Epoch 366/10000, Prediction Accuracy = 59.076923076923066%, Loss = 0.01026527569271051
Epoch: 366, Batch Gradient Norm: 0.7244439588090354
Epoch: 366, Batch Gradient Norm after: 0.7244439588090354
Epoch 367/10000, Prediction Accuracy = 58.965384615384615%, Loss = 0.010264312060406575
Epoch: 367, Batch Gradient Norm: 0.7402737517083788
Epoch: 367, Batch Gradient Norm after: 0.7402737517083788
Epoch 368/10000, Prediction Accuracy = 58.97307692307693%, Loss = 0.010271676816046238
Epoch: 368, Batch Gradient Norm: 0.7304087454276705
Epoch: 368, Batch Gradient Norm after: 0.7304087454276705
Epoch 369/10000, Prediction Accuracy = 59.04615384615385%, Loss = 0.010236138621201882
Epoch 00369: reducing learning rate of group 0 to 1.0000e-06.
Epoch: 369, Batch Gradient Norm: 0.6935170204932036
Epoch: 369, Batch Gradient Norm after: 0.6935170204932036
Epoch 370/10000, Prediction Accuracy = 59.16153846153846%, Loss = 0.010138691044770755
Epoch: 370, Batch Gradient Norm: 0.6405979510431213
Epoch: 370, Batch Gradient Norm after: 0.6405979510431213
Epoch 371/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.010100639926699491
Epoch: 371, Batch Gradient Norm: 0.6429312898815737
Epoch: 371, Batch Gradient Norm after: 0.6429312898815737
Epoch 372/10000, Prediction Accuracy = 59.330769230769235%, Loss = 0.010119578944375882
Epoch: 372, Batch Gradient Norm: 0.647564472190284
Epoch: 372, Batch Gradient Norm after: 0.647564472190284
Epoch 373/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.01014394943530743
Epoch: 373, Batch Gradient Norm: 0.6511931889774781
Epoch: 373, Batch Gradient Norm after: 0.6511931889774781
Epoch 374/10000, Prediction Accuracy = 59.39230769230768%, Loss = 0.010147381358994888
Epoch: 374, Batch Gradient Norm: 0.643102880164842
Epoch: 374, Batch Gradient Norm after: 0.643102880164842
Epoch 375/10000, Prediction Accuracy = 59.19615384615385%, Loss = 0.010128511259189019
Epoch: 375, Batch Gradient Norm: 0.6338012429693758
Epoch: 375, Batch Gradient Norm after: 0.6338012429693758
Epoch 376/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010124739402761826
Epoch: 376, Batch Gradient Norm: 0.6507546032747424
Epoch: 376, Batch Gradient Norm after: 0.6507546032747424
Epoch 377/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010113691624540549
Epoch: 377, Batch Gradient Norm: 0.6487533085336783
Epoch: 377, Batch Gradient Norm after: 0.6487533085336783
Epoch 378/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.010081598176979102
Epoch: 378, Batch Gradient Norm: 0.6579611260093268
Epoch: 378, Batch Gradient Norm after: 0.6579611260093268
Epoch 379/10000, Prediction Accuracy = 59.23076923076923%, Loss = 0.010145874120868169
Epoch: 379, Batch Gradient Norm: 0.6350901789230396
Epoch: 379, Batch Gradient Norm after: 0.6350901789230396
Epoch 380/10000, Prediction Accuracy = 59.13846153846154%, Loss = 0.010089143824118834
Epoch: 380, Batch Gradient Norm: 0.6424856018758542
Epoch: 380, Batch Gradient Norm after: 0.6424856018758542
Epoch 381/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010119945670549687
Epoch: 381, Batch Gradient Norm: 0.6421808767817541
Epoch: 381, Batch Gradient Norm after: 0.6421808767817541
Epoch 382/10000, Prediction Accuracy = 59.60000000000001%, Loss = 0.01009517191694333
Epoch: 382, Batch Gradient Norm: 0.6443801425233834
Epoch: 382, Batch Gradient Norm after: 0.6443801425233834
Epoch 383/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010101811029016972
Epoch: 383, Batch Gradient Norm: 0.647715587605659
Epoch: 383, Batch Gradient Norm after: 0.647715587605659
Epoch 384/10000, Prediction Accuracy = 59.35384615384616%, Loss = 0.010115859456933461
Epoch: 384, Batch Gradient Norm: 0.6659364728885853
Epoch: 384, Batch Gradient Norm after: 0.6659364728885853
Epoch 385/10000, Prediction Accuracy = 59.219230769230776%, Loss = 0.010197831532702996
Epoch: 385, Batch Gradient Norm: 0.6560597954440189
Epoch: 385, Batch Gradient Norm after: 0.6560597954440189
Epoch 386/10000, Prediction Accuracy = 59.57692307692308%, Loss = 0.010095027132103076
Epoch: 386, Batch Gradient Norm: 0.6472445815232564
Epoch: 386, Batch Gradient Norm after: 0.6472445815232564
Epoch 387/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010133027815474914
Epoch: 387, Batch Gradient Norm: 0.6460963162173735
Epoch: 387, Batch Gradient Norm after: 0.6460963162173735
Epoch 388/10000, Prediction Accuracy = 59.369230769230775%, Loss = 0.010144245380965563
Epoch: 388, Batch Gradient Norm: 0.64957482181567
Epoch: 388, Batch Gradient Norm after: 0.64957482181567
Epoch 389/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010117971696532689
Epoch 00389: reducing learning rate of group 0 to 1.0000e-07.
Epoch: 389, Batch Gradient Norm: 0.6241720285701063
Epoch: 389, Batch Gradient Norm after: 0.6241720285701063
Epoch 390/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010088565472799998
Epoch: 390, Batch Gradient Norm: 0.6373008840959348
Epoch: 390, Batch Gradient Norm after: 0.6373008840959348
Epoch 391/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.010042475536465645
Epoch: 391, Batch Gradient Norm: 0.6296416761403643
Epoch: 391, Batch Gradient Norm after: 0.6296416761403643
Epoch 392/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.010094724022425137
Epoch: 392, Batch Gradient Norm: 0.6416524206961997
Epoch: 392, Batch Gradient Norm after: 0.6416524206961997
Epoch 393/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.01011561186840901
Epoch: 393, Batch Gradient Norm: 0.6399750869646479
Epoch: 393, Batch Gradient Norm after: 0.6399750869646479
Epoch 394/10000, Prediction Accuracy = 59.526923076923076%, Loss = 0.010113579650911001
Epoch: 394, Batch Gradient Norm: 0.6375383113527338
Epoch: 394, Batch Gradient Norm after: 0.6375383113527338
Epoch 395/10000, Prediction Accuracy = 59.42692307692308%, Loss = 0.010086960589083342
Epoch: 395, Batch Gradient Norm: 0.637332197950454
Epoch: 395, Batch Gradient Norm after: 0.637332197950454
Epoch 396/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010102996316093665
Epoch: 396, Batch Gradient Norm: 0.6388273662261628
Epoch: 396, Batch Gradient Norm after: 0.6388273662261628
Epoch 397/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010098194990020532
Epoch: 397, Batch Gradient Norm: 0.6369173830224046
Epoch: 397, Batch Gradient Norm after: 0.6369173830224046
Epoch 398/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.0100715015656673
Epoch: 398, Batch Gradient Norm: 0.6273562647530293
Epoch: 398, Batch Gradient Norm after: 0.6273562647530293
Epoch 399/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010070104008683791
Epoch: 399, Batch Gradient Norm: 0.6313194086357545
Epoch: 399, Batch Gradient Norm after: 0.6313194086357545
Epoch 400/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010089546943513246
Epoch: 400, Batch Gradient Norm: 0.6489933757385564
Epoch: 400, Batch Gradient Norm after: 0.6489933757385564
Epoch 401/10000, Prediction Accuracy = 59.19615384615384%, Loss = 0.010125444485591007
Epoch: 401, Batch Gradient Norm: 0.6351480193142152
Epoch: 401, Batch Gradient Norm after: 0.6351480193142152
Epoch 402/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010090125509752678
Epoch 00402: reducing learning rate of group 0 to 1.0000e-08.
Epoch: 402, Batch Gradient Norm: 0.6379895764717964
Epoch: 402, Batch Gradient Norm after: 0.6379895764717964
Epoch 403/10000, Prediction Accuracy = 59.2153846153846%, Loss = 0.01011159156377499
Epoch: 403, Batch Gradient Norm: 0.6422664355778381
Epoch: 403, Batch Gradient Norm after: 0.6422664355778381
Epoch 404/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010119734045404654
Epoch: 404, Batch Gradient Norm: 0.6342495678014792
Epoch: 404, Batch Gradient Norm after: 0.6342495678014792
Epoch 405/10000, Prediction Accuracy = 59.36538461538463%, Loss = 0.010078892541619448
Epoch: 405, Batch Gradient Norm: 0.6297099255606666
Epoch: 405, Batch Gradient Norm after: 0.6297099255606666
Epoch 406/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010098297722064532
Epoch: 406, Batch Gradient Norm: 0.6446655318534671
Epoch: 406, Batch Gradient Norm after: 0.6446655318534671
Epoch 407/10000, Prediction Accuracy = 59.476923076923086%, Loss = 0.01009128987789154
Epoch: 407, Batch Gradient Norm: 0.6283911710424446
Epoch: 407, Batch Gradient Norm after: 0.6283911710424446
Epoch 408/10000, Prediction Accuracy = 59.52692307692307%, Loss = 0.01012149123618236
Epoch: 408, Batch Gradient Norm: 0.6594559619739
Epoch: 408, Batch Gradient Norm after: 0.6594559619739
Epoch 409/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.010078878929981818
Epoch: 409, Batch Gradient Norm: 0.6304950269281755
Epoch: 409, Batch Gradient Norm after: 0.6304950269281755
Epoch 410/10000, Prediction Accuracy = 59.43076923076924%, Loss = 0.010087134244923409
Epoch: 410, Batch Gradient Norm: 0.6604034999171234
Epoch: 410, Batch Gradient Norm after: 0.6604034999171234
Epoch 411/10000, Prediction Accuracy = 59.21923076923076%, Loss = 0.01014851384724562
Epoch: 411, Batch Gradient Norm: 0.6393837191714452
Epoch: 411, Batch Gradient Norm after: 0.6393837191714452
Epoch 412/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010107674564306553
Epoch: 412, Batch Gradient Norm: 0.6259534135399365
Epoch: 412, Batch Gradient Norm after: 0.6259534135399365
Epoch 413/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010096949453537282
Epoch: 413, Batch Gradient Norm: 0.6368987150473032
Epoch: 413, Batch Gradient Norm after: 0.6368987150473032
Epoch 414/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010083089940823041
Epoch: 414, Batch Gradient Norm: 0.6439331882511002
Epoch: 414, Batch Gradient Norm after: 0.6439331882511002
Epoch 415/10000, Prediction Accuracy = 59.08076923076923%, Loss = 0.010195856269162435
Epoch: 415, Batch Gradient Norm: 0.6415099778760378
Epoch: 415, Batch Gradient Norm after: 0.6415099778760378
Epoch 416/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.01013582070859579
Epoch: 416, Batch Gradient Norm: 0.6457587996456258
Epoch: 416, Batch Gradient Norm after: 0.6457587996456258
Epoch 417/10000, Prediction Accuracy = 59.40000000000001%, Loss = 0.010154951148881363
Epoch: 417, Batch Gradient Norm: 0.6423513938147606
Epoch: 417, Batch Gradient Norm after: 0.6423513938147606
Epoch 418/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010154095120154895
Epoch: 418, Batch Gradient Norm: 0.642443309740086
Epoch: 418, Batch Gradient Norm after: 0.642443309740086
Epoch 419/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010092000047174783
Epoch: 419, Batch Gradient Norm: 0.6464781337774759
Epoch: 419, Batch Gradient Norm after: 0.6464781337774759
Epoch 420/10000, Prediction Accuracy = 59.42307692307691%, Loss = 0.010078179650008678
Epoch: 420, Batch Gradient Norm: 0.6402677881445622
Epoch: 420, Batch Gradient Norm after: 0.6402677881445622
Epoch 421/10000, Prediction Accuracy = 59.51923076923077%, Loss = 0.010065870001338996
Epoch: 421, Batch Gradient Norm: 0.6376917848053373
Epoch: 421, Batch Gradient Norm after: 0.6376917848053373
Epoch 422/10000, Prediction Accuracy = 59.31538461538462%, Loss = 0.010067940331422366
Epoch: 422, Batch Gradient Norm: 0.6406133030643434
Epoch: 422, Batch Gradient Norm after: 0.6406133030643434
Epoch 423/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010109151641909894
Epoch: 423, Batch Gradient Norm: 0.634344080281156
Epoch: 423, Batch Gradient Norm after: 0.634344080281156
Epoch 424/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010144089062053423
Epoch: 424, Batch Gradient Norm: 0.6536831425999131
Epoch: 424, Batch Gradient Norm after: 0.6536831425999131
Epoch 425/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010149462291827569
Epoch: 425, Batch Gradient Norm: 0.6560510294167894
Epoch: 425, Batch Gradient Norm after: 0.6560510294167894
Epoch 426/10000, Prediction Accuracy = 59.350000000000016%, Loss = 0.010123093254291095
Epoch: 426, Batch Gradient Norm: 0.6426160193522471
Epoch: 426, Batch Gradient Norm after: 0.6426160193522471
Epoch 427/10000, Prediction Accuracy = 59.43461538461539%, Loss = 0.010057265368791727
Epoch: 427, Batch Gradient Norm: 0.6474511788274336
Epoch: 427, Batch Gradient Norm after: 0.6474511788274336
Epoch 428/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010076772135037642
Epoch: 428, Batch Gradient Norm: 0.6510148637492535
Epoch: 428, Batch Gradient Norm after: 0.6510148637492535
Epoch 429/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010132974658447962
Epoch: 429, Batch Gradient Norm: 0.6299408572111902
Epoch: 429, Batch Gradient Norm after: 0.6299408572111902
Epoch 430/10000, Prediction Accuracy = 59.22692307692308%, Loss = 0.01010948856576131
Epoch: 430, Batch Gradient Norm: 0.6313275300478844
Epoch: 430, Batch Gradient Norm after: 0.6313275300478844
Epoch 431/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010117912091887914
Epoch: 431, Batch Gradient Norm: 0.6520660721851014
Epoch: 431, Batch Gradient Norm after: 0.6520660721851014
Epoch 432/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010146296153274866
Epoch: 432, Batch Gradient Norm: 0.6413816770108811
Epoch: 432, Batch Gradient Norm after: 0.6413816770108811
Epoch 433/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.01013165139234983
Epoch: 433, Batch Gradient Norm: 0.6304263100419341
Epoch: 433, Batch Gradient Norm after: 0.6304263100419341
Epoch 434/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010062347954282394
Epoch: 434, Batch Gradient Norm: 0.6400953786621248
Epoch: 434, Batch Gradient Norm after: 0.6400953786621248
Epoch 435/10000, Prediction Accuracy = 59.542307692307695%, Loss = 0.010117105351617703
Epoch: 435, Batch Gradient Norm: 0.6393144480416467
Epoch: 435, Batch Gradient Norm after: 0.6393144480416467
Epoch 436/10000, Prediction Accuracy = 59.423076923076934%, Loss = 0.010097534037553348
Epoch: 436, Batch Gradient Norm: 0.6453809325277812
Epoch: 436, Batch Gradient Norm after: 0.6453809325277812
Epoch 437/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.01010552879709464
Epoch: 437, Batch Gradient Norm: 0.6448065952723265
Epoch: 437, Batch Gradient Norm after: 0.6448065952723265
Epoch 438/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.010091798594937874
Epoch: 438, Batch Gradient Norm: 0.6507576352658605
Epoch: 438, Batch Gradient Norm after: 0.6507576352658605
Epoch 439/10000, Prediction Accuracy = 59.45384615384614%, Loss = 0.010108873463020874
Epoch: 439, Batch Gradient Norm: 0.635521140180919
Epoch: 439, Batch Gradient Norm after: 0.635521140180919
Epoch 440/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.01014474886827744
Epoch: 440, Batch Gradient Norm: 0.6289212029039526
Epoch: 440, Batch Gradient Norm after: 0.6289212029039526
Epoch 441/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.010074343245763045
Epoch: 441, Batch Gradient Norm: 0.6447449074893488
Epoch: 441, Batch Gradient Norm after: 0.6447449074893488
Epoch 442/10000, Prediction Accuracy = 59.73461538461539%, Loss = 0.01006943266838789
Epoch: 442, Batch Gradient Norm: 0.6295035847250784
Epoch: 442, Batch Gradient Norm after: 0.6295035847250784
Epoch 443/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010088073232999215
Epoch: 443, Batch Gradient Norm: 0.6409904963125458
Epoch: 443, Batch Gradient Norm after: 0.6409904963125458
Epoch 444/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010100475799005765
Epoch: 444, Batch Gradient Norm: 0.6459078051469288
Epoch: 444, Batch Gradient Norm after: 0.6459078051469288
Epoch 445/10000, Prediction Accuracy = 59.419230769230765%, Loss = 0.010147183560408078
Epoch: 445, Batch Gradient Norm: 0.6356231551741129
Epoch: 445, Batch Gradient Norm after: 0.6356231551741129
Epoch 446/10000, Prediction Accuracy = 59.20384615384616%, Loss = 0.010083364967543345
Epoch: 446, Batch Gradient Norm: 0.6382391889126237
Epoch: 446, Batch Gradient Norm after: 0.6382391889126237
Epoch 447/10000, Prediction Accuracy = 59.361538461538466%, Loss = 0.010101596394983621
Epoch: 447, Batch Gradient Norm: 0.6270677303293404
Epoch: 447, Batch Gradient Norm after: 0.6270677303293404
Epoch 448/10000, Prediction Accuracy = 59.22307692307693%, Loss = 0.010059938551141666
Epoch: 448, Batch Gradient Norm: 0.6371535614802866
Epoch: 448, Batch Gradient Norm after: 0.6371535614802866
Epoch 449/10000, Prediction Accuracy = 59.62692307692308%, Loss = 0.010076306043909146
Epoch: 449, Batch Gradient Norm: 0.6403588894089088
Epoch: 449, Batch Gradient Norm after: 0.6403588894089088
Epoch 450/10000, Prediction Accuracy = 59.45%, Loss = 0.010117619441678891
Epoch: 450, Batch Gradient Norm: 0.6370794469614198
Epoch: 450, Batch Gradient Norm after: 0.6370794469614198
Epoch 451/10000, Prediction Accuracy = 59.43076923076922%, Loss = 0.010069951916543337
Epoch: 451, Batch Gradient Norm: 0.6472413286797908
Epoch: 451, Batch Gradient Norm after: 0.6472413286797908
Epoch 452/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.010133568627329973
Epoch: 452, Batch Gradient Norm: 0.6308817675728137
Epoch: 452, Batch Gradient Norm after: 0.6308817675728137
Epoch 453/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010102865429451833
Epoch: 453, Batch Gradient Norm: 0.6307306018366732
Epoch: 453, Batch Gradient Norm after: 0.6307306018366732
Epoch 454/10000, Prediction Accuracy = 59.50000000000001%, Loss = 0.010108613624022557
Epoch: 454, Batch Gradient Norm: 0.6371667856066554
Epoch: 454, Batch Gradient Norm after: 0.6371667856066554
Epoch 455/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010142561406470262
Epoch: 455, Batch Gradient Norm: 0.6178767351438083
Epoch: 455, Batch Gradient Norm after: 0.6178767351438083
Epoch 456/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010059528267727448
Epoch: 456, Batch Gradient Norm: 0.633198863594479
Epoch: 456, Batch Gradient Norm after: 0.633198863594479
Epoch 457/10000, Prediction Accuracy = 59.25384615384615%, Loss = 0.010122925831148257
Epoch: 457, Batch Gradient Norm: 0.6401486523686749
Epoch: 457, Batch Gradient Norm after: 0.6401486523686749
Epoch 458/10000, Prediction Accuracy = 59.25000000000001%, Loss = 0.010112075206752006
Epoch: 458, Batch Gradient Norm: 0.6391351406532101
Epoch: 458, Batch Gradient Norm after: 0.6391351406532101
Epoch 459/10000, Prediction Accuracy = 59.361538461538466%, Loss = 0.010108900543015737
Epoch: 459, Batch Gradient Norm: 0.6372524654517324
Epoch: 459, Batch Gradient Norm after: 0.6372524654517324
Epoch 460/10000, Prediction Accuracy = 59.384615384615394%, Loss = 0.010127473049438916
Epoch: 460, Batch Gradient Norm: 0.6474025790658301
Epoch: 460, Batch Gradient Norm after: 0.6474025790658301
Epoch 461/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.010124440161654582
Epoch: 461, Batch Gradient Norm: 0.6402254440933403
Epoch: 461, Batch Gradient Norm after: 0.6402254440933403
Epoch 462/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010115716248177566
Epoch: 462, Batch Gradient Norm: 0.6484636712599364
Epoch: 462, Batch Gradient Norm after: 0.6484636712599364
Epoch 463/10000, Prediction Accuracy = 59.39615384615384%, Loss = 0.010103130569824805
Epoch: 463, Batch Gradient Norm: 0.6332546895893584
Epoch: 463, Batch Gradient Norm after: 0.6332546895893584
Epoch 464/10000, Prediction Accuracy = 59.36923076923078%, Loss = 0.010063166515185283
Epoch: 464, Batch Gradient Norm: 0.666535929675196
Epoch: 464, Batch Gradient Norm after: 0.666535929675196
Epoch 465/10000, Prediction Accuracy = 59.14615384615385%, Loss = 0.010177290496917872
Epoch: 465, Batch Gradient Norm: 0.6337687372203681
Epoch: 465, Batch Gradient Norm after: 0.6337687372203681
Epoch 466/10000, Prediction Accuracy = 59.692307692307686%, Loss = 0.010056956814458737
Epoch: 466, Batch Gradient Norm: 0.6336887721200564
Epoch: 466, Batch Gradient Norm after: 0.6336887721200564
Epoch 467/10000, Prediction Accuracy = 59.20769230769231%, Loss = 0.010076646764691059
Epoch: 467, Batch Gradient Norm: 0.6505032950505546
Epoch: 467, Batch Gradient Norm after: 0.6505032950505546
Epoch 468/10000, Prediction Accuracy = 59.26538461538462%, Loss = 0.010119451854664546
Epoch: 468, Batch Gradient Norm: 0.6292273112177981
Epoch: 468, Batch Gradient Norm after: 0.6292273112177981
Epoch 469/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010098908311472489
Epoch: 469, Batch Gradient Norm: 0.6510652386162388
Epoch: 469, Batch Gradient Norm after: 0.6510652386162388
Epoch 470/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010122096882416652
Epoch: 470, Batch Gradient Norm: 0.6274523998023326
Epoch: 470, Batch Gradient Norm after: 0.6274523998023326
Epoch 471/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010082058536891755
Epoch: 471, Batch Gradient Norm: 0.6433197479751819
Epoch: 471, Batch Gradient Norm after: 0.6433197479751819
Epoch 472/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010082176385017542
Epoch: 472, Batch Gradient Norm: 0.6488832133311622
Epoch: 472, Batch Gradient Norm after: 0.6488832133311622
Epoch 473/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010098667815327644
Epoch: 473, Batch Gradient Norm: 0.6319891794197473
Epoch: 473, Batch Gradient Norm after: 0.6319891794197473
Epoch 474/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010125829623295711
Epoch: 474, Batch Gradient Norm: 0.6465788717919974
Epoch: 474, Batch Gradient Norm after: 0.6465788717919974
Epoch 475/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010103053341691311
Epoch: 475, Batch Gradient Norm: 0.6515982909542104
Epoch: 475, Batch Gradient Norm after: 0.6515982909542104
Epoch 476/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010111263236747338
Epoch: 476, Batch Gradient Norm: 0.6366159671588635
Epoch: 476, Batch Gradient Norm after: 0.6366159671588635
Epoch 477/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.010064420576852102
Epoch: 477, Batch Gradient Norm: 0.632636794042293
Epoch: 477, Batch Gradient Norm after: 0.632636794042293
Epoch 478/10000, Prediction Accuracy = 59.423076923076934%, Loss = 0.010095022690410797
Epoch: 478, Batch Gradient Norm: 0.657661428449287
Epoch: 478, Batch Gradient Norm after: 0.657661428449287
Epoch 479/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010108330430319676
Epoch: 479, Batch Gradient Norm: 0.6414488680351073
Epoch: 479, Batch Gradient Norm after: 0.6414488680351073
Epoch 480/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.010083802259312226
Epoch: 480, Batch Gradient Norm: 0.64485636225151
Epoch: 480, Batch Gradient Norm after: 0.64485636225151
Epoch 481/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.010099651793447824
Epoch: 481, Batch Gradient Norm: 0.631984355372876
Epoch: 481, Batch Gradient Norm after: 0.631984355372876
Epoch 482/10000, Prediction Accuracy = 59.56538461538462%, Loss = 0.010094040718216162
Epoch: 482, Batch Gradient Norm: 0.6412212453833426
Epoch: 482, Batch Gradient Norm after: 0.6412212453833426
Epoch 483/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010075250282310523
Epoch: 483, Batch Gradient Norm: 0.6504698292034586
Epoch: 483, Batch Gradient Norm after: 0.6504698292034586
Epoch 484/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010089224562622033
Epoch: 484, Batch Gradient Norm: 0.6393512568061843
Epoch: 484, Batch Gradient Norm after: 0.6393512568061843
Epoch 485/10000, Prediction Accuracy = 59.56153846153846%, Loss = 0.010062431701673912
Epoch: 485, Batch Gradient Norm: 0.6428136682160582
Epoch: 485, Batch Gradient Norm after: 0.6428136682160582
Epoch 486/10000, Prediction Accuracy = 59.22692307692307%, Loss = 0.01011774423890389
Epoch: 486, Batch Gradient Norm: 0.6252027790193866
Epoch: 486, Batch Gradient Norm after: 0.6252027790193866
Epoch 487/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.010112742606837016
Epoch: 487, Batch Gradient Norm: 0.6479232709266086
Epoch: 487, Batch Gradient Norm after: 0.6479232709266086
Epoch 488/10000, Prediction Accuracy = 59.33846153846154%, Loss = 0.010107750502916483
Epoch: 488, Batch Gradient Norm: 0.6397173565961193
Epoch: 488, Batch Gradient Norm after: 0.6397173565961193
Epoch 489/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010094085063498754
Epoch: 489, Batch Gradient Norm: 0.6524950206942682
Epoch: 489, Batch Gradient Norm after: 0.6524950206942682
Epoch 490/10000, Prediction Accuracy = 59.13461538461539%, Loss = 0.010130949819890352
Epoch: 490, Batch Gradient Norm: 0.6226845630457795
Epoch: 490, Batch Gradient Norm after: 0.6226845630457795
Epoch 491/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.010050937246817809
Epoch: 491, Batch Gradient Norm: 0.6565744996502677
Epoch: 491, Batch Gradient Norm after: 0.6565744996502677
Epoch 492/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010109638580336021
Epoch: 492, Batch Gradient Norm: 0.6384592877992898
Epoch: 492, Batch Gradient Norm after: 0.6384592877992898
Epoch 493/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.0101156232592005
Epoch: 493, Batch Gradient Norm: 0.6316926177908412
Epoch: 493, Batch Gradient Norm after: 0.6316926177908412
Epoch 494/10000, Prediction Accuracy = 59.284615384615385%, Loss = 0.010094408232432146
Epoch: 494, Batch Gradient Norm: 0.6432595021009365
Epoch: 494, Batch Gradient Norm after: 0.6432595021009365
Epoch 495/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.01010098229520596
Epoch: 495, Batch Gradient Norm: 0.6514353683202796
Epoch: 495, Batch Gradient Norm after: 0.6514353683202796
Epoch 496/10000, Prediction Accuracy = 59.580769230769235%, Loss = 0.010106887596731003
Epoch: 496, Batch Gradient Norm: 0.6463968831376308
Epoch: 496, Batch Gradient Norm after: 0.6463968831376308
Epoch 497/10000, Prediction Accuracy = 59.58461538461539%, Loss = 0.010104079086046953
Epoch: 497, Batch Gradient Norm: 0.6377181825428871
Epoch: 497, Batch Gradient Norm after: 0.6377181825428871
Epoch 498/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010067923854176816
Epoch: 498, Batch Gradient Norm: 0.6397596514444804
Epoch: 498, Batch Gradient Norm after: 0.6397596514444804
Epoch 499/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010097596221245252
Epoch: 499, Batch Gradient Norm: 0.6244474238466073
Epoch: 499, Batch Gradient Norm after: 0.6244474238466073
Epoch 500/10000, Prediction Accuracy = 59.56153846153846%, Loss = 0.010036341989269624
Epoch: 500, Batch Gradient Norm: 0.6289724188701072
Epoch: 500, Batch Gradient Norm after: 0.6289724188701072
Epoch 501/10000, Prediction Accuracy = 59.596153846153854%, Loss = 0.010118236836905662
Epoch: 501, Batch Gradient Norm: 0.6381829726828103
Epoch: 501, Batch Gradient Norm after: 0.6381829726828103
Epoch 502/10000, Prediction Accuracy = 59.61153846153846%, Loss = 0.010051158543389577
Epoch: 502, Batch Gradient Norm: 0.630032989297149
Epoch: 502, Batch Gradient Norm after: 0.630032989297149
Epoch 503/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010071277976609193
Epoch: 503, Batch Gradient Norm: 0.646919399701741
Epoch: 503, Batch Gradient Norm after: 0.646919399701741
Epoch 504/10000, Prediction Accuracy = 59.56153846153847%, Loss = 0.0100689553297483
Epoch: 504, Batch Gradient Norm: 0.6481497482602372
Epoch: 504, Batch Gradient Norm after: 0.6481497482602372
Epoch 505/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010085642122878479
Epoch: 505, Batch Gradient Norm: 0.6324040306396834
Epoch: 505, Batch Gradient Norm after: 0.6324040306396834
Epoch 506/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.010082155251159118
Epoch: 506, Batch Gradient Norm: 0.6308531013839941
Epoch: 506, Batch Gradient Norm after: 0.6308531013839941
Epoch 507/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010136685262505826
Epoch: 507, Batch Gradient Norm: 0.6459696240272691
Epoch: 507, Batch Gradient Norm after: 0.6459696240272691
Epoch 508/10000, Prediction Accuracy = 59.30769230769231%, Loss = 0.0100768984367068
Epoch: 508, Batch Gradient Norm: 0.6396743704578381
Epoch: 508, Batch Gradient Norm after: 0.6396743704578381
Epoch 509/10000, Prediction Accuracy = 59.41153846153847%, Loss = 0.010117778769479347
Epoch: 509, Batch Gradient Norm: 0.6524659040820899
Epoch: 509, Batch Gradient Norm after: 0.6524659040820899
Epoch 510/10000, Prediction Accuracy = 59.19615384615384%, Loss = 0.010107099436796628
Epoch: 510, Batch Gradient Norm: 0.6488547668981376
Epoch: 510, Batch Gradient Norm after: 0.6488547668981376
Epoch 511/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010102309000033598
Epoch: 511, Batch Gradient Norm: 0.6454043789393388
Epoch: 511, Batch Gradient Norm after: 0.6454043789393388
Epoch 512/10000, Prediction Accuracy = 59.45%, Loss = 0.010077138789571248
Epoch: 512, Batch Gradient Norm: 0.6370348390517375
Epoch: 512, Batch Gradient Norm after: 0.6370348390517375
Epoch 513/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010086503667900195
Epoch: 513, Batch Gradient Norm: 0.6312070925078032
Epoch: 513, Batch Gradient Norm after: 0.6312070925078032
Epoch 514/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010113174597231241
Epoch: 514, Batch Gradient Norm: 0.6320196906865017
Epoch: 514, Batch Gradient Norm after: 0.6320196906865017
Epoch 515/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010111014645260114
Epoch: 515, Batch Gradient Norm: 0.6359057283756998
Epoch: 515, Batch Gradient Norm after: 0.6359057283756998
Epoch 516/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010068539171837844
Epoch: 516, Batch Gradient Norm: 0.6469539555020478
Epoch: 516, Batch Gradient Norm after: 0.6469539555020478
Epoch 517/10000, Prediction Accuracy = 59.63076923076923%, Loss = 0.010109528182790829
Epoch: 517, Batch Gradient Norm: 0.6357224139102415
Epoch: 517, Batch Gradient Norm after: 0.6357224139102415
Epoch 518/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010067129221100073
Epoch: 518, Batch Gradient Norm: 0.6469708247024785
Epoch: 518, Batch Gradient Norm after: 0.6469708247024785
Epoch 519/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.010144985567491788
Epoch: 519, Batch Gradient Norm: 0.6515003181085065
Epoch: 519, Batch Gradient Norm after: 0.6515003181085065
Epoch 520/10000, Prediction Accuracy = 59.24999999999999%, Loss = 0.010111633186730055
Epoch: 520, Batch Gradient Norm: 0.6503974176139963
Epoch: 520, Batch Gradient Norm after: 0.6503974176139963
Epoch 521/10000, Prediction Accuracy = 59.31923076923076%, Loss = 0.010125933644863276
Epoch: 521, Batch Gradient Norm: 0.6325519183370455
Epoch: 521, Batch Gradient Norm after: 0.6325519183370455
Epoch 522/10000, Prediction Accuracy = 59.48846153846155%, Loss = 0.010082736468085876
Epoch: 522, Batch Gradient Norm: 0.6276348085126887
Epoch: 522, Batch Gradient Norm after: 0.6276348085126887
Epoch 523/10000, Prediction Accuracy = 59.26153846153846%, Loss = 0.010073896354207626
Epoch: 523, Batch Gradient Norm: 0.6328376208227531
Epoch: 523, Batch Gradient Norm after: 0.6328376208227531
Epoch 524/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.01010720173899944
Epoch: 524, Batch Gradient Norm: 0.6341777492189136
Epoch: 524, Batch Gradient Norm after: 0.6341777492189136
Epoch 525/10000, Prediction Accuracy = 59.638461538461534%, Loss = 0.010084491581297837
Epoch: 525, Batch Gradient Norm: 0.645126579085741
Epoch: 525, Batch Gradient Norm after: 0.645126579085741
Epoch 526/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010140377311752392
Epoch: 526, Batch Gradient Norm: 0.6444559192400885
Epoch: 526, Batch Gradient Norm after: 0.6444559192400885
Epoch 527/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010108120667819794
Epoch: 527, Batch Gradient Norm: 0.6537325394784187
Epoch: 527, Batch Gradient Norm after: 0.6537325394784187
Epoch 528/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010138871649710031
Epoch: 528, Batch Gradient Norm: 0.6452892013854176
Epoch: 528, Batch Gradient Norm after: 0.6452892013854176
Epoch 529/10000, Prediction Accuracy = 59.26153846153847%, Loss = 0.010116753311684499
Epoch: 529, Batch Gradient Norm: 0.6596684516154974
Epoch: 529, Batch Gradient Norm after: 0.6596684516154974
Epoch 530/10000, Prediction Accuracy = 59.37692307692309%, Loss = 0.010100180713030009
Epoch: 530, Batch Gradient Norm: 0.6388021260928237
Epoch: 530, Batch Gradient Norm after: 0.6388021260928237
Epoch 531/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.01014494072072781
Epoch: 531, Batch Gradient Norm: 0.6458753182689726
Epoch: 531, Batch Gradient Norm after: 0.6458753182689726
Epoch 532/10000, Prediction Accuracy = 59.12307692307692%, Loss = 0.010127088843056789
Epoch: 532, Batch Gradient Norm: 0.6237206952547424
Epoch: 532, Batch Gradient Norm after: 0.6237206952547424
Epoch 533/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.010079375539834682
Epoch: 533, Batch Gradient Norm: 0.6453574997392724
Epoch: 533, Batch Gradient Norm after: 0.6453574997392724
Epoch 534/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010117963887751102
Epoch: 534, Batch Gradient Norm: 0.636575489690851
Epoch: 534, Batch Gradient Norm after: 0.636575489690851
Epoch 535/10000, Prediction Accuracy = 59.48461538461539%, Loss = 0.010097440976936083
Epoch: 535, Batch Gradient Norm: 0.6380128285581752
Epoch: 535, Batch Gradient Norm after: 0.6380128285581752
Epoch 536/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.01014251304933658
Epoch: 536, Batch Gradient Norm: 0.6513088738288811
Epoch: 536, Batch Gradient Norm after: 0.6513088738288811
Epoch 537/10000, Prediction Accuracy = 59.173076923076934%, Loss = 0.010116120585455345
Epoch: 537, Batch Gradient Norm: 0.6442676359270494
Epoch: 537, Batch Gradient Norm after: 0.6442676359270494
Epoch 538/10000, Prediction Accuracy = 59.273076923076935%, Loss = 0.010114344051824166
Epoch: 538, Batch Gradient Norm: 0.6488739142991842
Epoch: 538, Batch Gradient Norm after: 0.6488739142991842
Epoch 539/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.01008535183679599
Epoch: 539, Batch Gradient Norm: 0.6464221010849514
Epoch: 539, Batch Gradient Norm after: 0.6464221010849514
Epoch 540/10000, Prediction Accuracy = 59.76153846153847%, Loss = 0.01005084404292015
Epoch: 540, Batch Gradient Norm: 0.6439440734610324
Epoch: 540, Batch Gradient Norm after: 0.6439440734610324
Epoch 541/10000, Prediction Accuracy = 59.25384615384616%, Loss = 0.0101035674317525
Epoch: 541, Batch Gradient Norm: 0.6328855412078105
Epoch: 541, Batch Gradient Norm after: 0.6328855412078105
Epoch 542/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010075193471633472
Epoch: 542, Batch Gradient Norm: 0.6524520829993348
Epoch: 542, Batch Gradient Norm after: 0.6524520829993348
Epoch 543/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.010136020513108144
Epoch: 543, Batch Gradient Norm: 0.6350414308596043
Epoch: 543, Batch Gradient Norm after: 0.6350414308596043
Epoch 544/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010116288868280558
Epoch: 544, Batch Gradient Norm: 0.6557257091647264
Epoch: 544, Batch Gradient Norm after: 0.6557257091647264
Epoch 545/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010116431432274671
Epoch: 545, Batch Gradient Norm: 0.6462865176897877
Epoch: 545, Batch Gradient Norm after: 0.6462865176897877
Epoch 546/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010081525963659469
Epoch: 546, Batch Gradient Norm: 0.6455820923790203
Epoch: 546, Batch Gradient Norm after: 0.6455820923790203
Epoch 547/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010092820757283615
Epoch: 547, Batch Gradient Norm: 0.6355758526276698
Epoch: 547, Batch Gradient Norm after: 0.6355758526276698
Epoch 548/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.010089341622705642
Epoch: 548, Batch Gradient Norm: 0.6408993638054029
Epoch: 548, Batch Gradient Norm after: 0.6408993638054029
Epoch 549/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010080792869512852
Epoch: 549, Batch Gradient Norm: 0.6484482731181784
Epoch: 549, Batch Gradient Norm after: 0.6484482731181784
Epoch 550/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.010066962872560207
Epoch: 550, Batch Gradient Norm: 0.6326615323986673
Epoch: 550, Batch Gradient Norm after: 0.6326615323986673
Epoch 551/10000, Prediction Accuracy = 59.42692307692308%, Loss = 0.010031753936066078
Epoch: 551, Batch Gradient Norm: 0.6527823453838072
Epoch: 551, Batch Gradient Norm after: 0.6527823453838072
Epoch 552/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010170702177744646
Epoch: 552, Batch Gradient Norm: 0.647190411294822
Epoch: 552, Batch Gradient Norm after: 0.647190411294822
Epoch 553/10000, Prediction Accuracy = 59.29615384615386%, Loss = 0.010076448822823854
Epoch: 553, Batch Gradient Norm: 0.6291454327458743
Epoch: 553, Batch Gradient Norm after: 0.6291454327458743
Epoch 554/10000, Prediction Accuracy = 59.25384615384615%, Loss = 0.010116878037269298
Epoch: 554, Batch Gradient Norm: 0.6413761159868127
Epoch: 554, Batch Gradient Norm after: 0.6413761159868127
Epoch 555/10000, Prediction Accuracy = 59.34615384615384%, Loss = 0.010104256897018505
Epoch: 555, Batch Gradient Norm: 0.6414983743948868
Epoch: 555, Batch Gradient Norm after: 0.6414983743948868
Epoch 556/10000, Prediction Accuracy = 59.31538461538462%, Loss = 0.010145524946542887
Epoch: 556, Batch Gradient Norm: 0.6478332766907142
Epoch: 556, Batch Gradient Norm after: 0.6478332766907142
Epoch 557/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010069711062197503
Epoch: 557, Batch Gradient Norm: 0.6431410898904238
Epoch: 557, Batch Gradient Norm after: 0.6431410898904238
Epoch 558/10000, Prediction Accuracy = 59.47692307692307%, Loss = 0.01010642472941142
Epoch: 558, Batch Gradient Norm: 0.6440105274619982
Epoch: 558, Batch Gradient Norm after: 0.6440105274619982
Epoch 559/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.01011062148385323
Epoch: 559, Batch Gradient Norm: 0.6304392136585353
Epoch: 559, Batch Gradient Norm after: 0.6304392136585353
Epoch 560/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010070969637196798
Epoch: 560, Batch Gradient Norm: 0.640086296431024
Epoch: 560, Batch Gradient Norm after: 0.640086296431024
Epoch 561/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010088597782529317
Epoch: 561, Batch Gradient Norm: 0.6403666836701626
Epoch: 561, Batch Gradient Norm after: 0.6403666836701626
Epoch 562/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.01007918375902451
Epoch: 562, Batch Gradient Norm: 0.6458572323205436
Epoch: 562, Batch Gradient Norm after: 0.6458572323205436
Epoch 563/10000, Prediction Accuracy = 59.35000000000001%, Loss = 0.010082986778937854
Epoch: 563, Batch Gradient Norm: 0.6376314364060552
Epoch: 563, Batch Gradient Norm after: 0.6376314364060552
Epoch 564/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.010071917365376767
Epoch: 564, Batch Gradient Norm: 0.6445845769972437
Epoch: 564, Batch Gradient Norm after: 0.6445845769972437
Epoch 565/10000, Prediction Accuracy = 59.69230769230769%, Loss = 0.010058974488996543
Epoch: 565, Batch Gradient Norm: 0.6450169846115805
Epoch: 565, Batch Gradient Norm after: 0.6450169846115805
Epoch 566/10000, Prediction Accuracy = 59.292307692307695%, Loss = 0.01010249712719367
Epoch: 566, Batch Gradient Norm: 0.6330133023116008
Epoch: 566, Batch Gradient Norm after: 0.6330133023116008
Epoch 567/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.010107899514528422
Epoch: 567, Batch Gradient Norm: 0.6256589749550583
Epoch: 567, Batch Gradient Norm after: 0.6256589749550583
Epoch 568/10000, Prediction Accuracy = 59.50384615384617%, Loss = 0.010069277280798325
Epoch: 568, Batch Gradient Norm: 0.6426560283717301
Epoch: 568, Batch Gradient Norm after: 0.6426560283717301
Epoch 569/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010128848469601227
Epoch: 569, Batch Gradient Norm: 0.637629332159298
Epoch: 569, Batch Gradient Norm after: 0.637629332159298
Epoch 570/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.010082334638215028
Epoch: 570, Batch Gradient Norm: 0.6439451826637611
Epoch: 570, Batch Gradient Norm after: 0.6439451826637611
Epoch 571/10000, Prediction Accuracy = 59.611538461538466%, Loss = 0.010137858943870435
Epoch: 571, Batch Gradient Norm: 0.6470136058387443
Epoch: 571, Batch Gradient Norm after: 0.6470136058387443
Epoch 572/10000, Prediction Accuracy = 59.19615384615384%, Loss = 0.010161596923493423
Epoch: 572, Batch Gradient Norm: 0.645754987544394
Epoch: 572, Batch Gradient Norm after: 0.645754987544394
Epoch 573/10000, Prediction Accuracy = 59.4423076923077%, Loss = 0.010095306672155857
Epoch: 573, Batch Gradient Norm: 0.6373843859659816
Epoch: 573, Batch Gradient Norm after: 0.6373843859659816
Epoch 574/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.01006829903389399
Epoch: 574, Batch Gradient Norm: 0.6305094444048313
Epoch: 574, Batch Gradient Norm after: 0.6305094444048313
Epoch 575/10000, Prediction Accuracy = 59.26538461538461%, Loss = 0.010105106191566357
Epoch: 575, Batch Gradient Norm: 0.6448819364161393
Epoch: 575, Batch Gradient Norm after: 0.6448819364161393
Epoch 576/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010096299533660594
Epoch: 576, Batch Gradient Norm: 0.6556331270271469
Epoch: 576, Batch Gradient Norm after: 0.6556331270271469
Epoch 577/10000, Prediction Accuracy = 59.284615384615385%, Loss = 0.01013793538396175
Epoch: 577, Batch Gradient Norm: 0.6377309030672681
Epoch: 577, Batch Gradient Norm after: 0.6377309030672681
Epoch 578/10000, Prediction Accuracy = 59.51538461538461%, Loss = 0.010066552660786189
Epoch: 578, Batch Gradient Norm: 0.6474401173422043
Epoch: 578, Batch Gradient Norm after: 0.6474401173422043
Epoch 579/10000, Prediction Accuracy = 59.74230769230769%, Loss = 0.01007717575591344
Epoch: 579, Batch Gradient Norm: 0.648019218579284
Epoch: 579, Batch Gradient Norm after: 0.648019218579284
Epoch 580/10000, Prediction Accuracy = 59.48846153846153%, Loss = 0.010098299513069483
Epoch: 580, Batch Gradient Norm: 0.6432436046887017
Epoch: 580, Batch Gradient Norm after: 0.6432436046887017
Epoch 581/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.010093462438537525
Epoch: 581, Batch Gradient Norm: 0.6413092292393197
Epoch: 581, Batch Gradient Norm after: 0.6413092292393197
Epoch 582/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010075022108279742
Epoch: 582, Batch Gradient Norm: 0.664972924169814
Epoch: 582, Batch Gradient Norm after: 0.664972924169814
Epoch 583/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010125856560010176
Epoch: 583, Batch Gradient Norm: 0.6394347061837866
Epoch: 583, Batch Gradient Norm after: 0.6394347061837866
Epoch 584/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010084281747157756
Epoch: 584, Batch Gradient Norm: 0.6335103974180718
Epoch: 584, Batch Gradient Norm after: 0.6335103974180718
Epoch 585/10000, Prediction Accuracy = 59.61923076923078%, Loss = 0.010086827481595369
Epoch: 585, Batch Gradient Norm: 0.630640669209955
Epoch: 585, Batch Gradient Norm after: 0.630640669209955
Epoch 586/10000, Prediction Accuracy = 59.63461538461537%, Loss = 0.010048972084545173
Epoch: 586, Batch Gradient Norm: 0.6425734305866344
Epoch: 586, Batch Gradient Norm after: 0.6425734305866344
Epoch 587/10000, Prediction Accuracy = 59.28846153846154%, Loss = 0.010094416471054921
Epoch: 587, Batch Gradient Norm: 0.6467982131898525
Epoch: 587, Batch Gradient Norm after: 0.6467982131898525
Epoch 588/10000, Prediction Accuracy = 59.23076923076923%, Loss = 0.010129366284952713
Epoch: 588, Batch Gradient Norm: 0.636326135963593
Epoch: 588, Batch Gradient Norm after: 0.636326135963593
Epoch 589/10000, Prediction Accuracy = 59.34615384615384%, Loss = 0.010085609526588367
Epoch: 589, Batch Gradient Norm: 0.6556834443768705
Epoch: 589, Batch Gradient Norm after: 0.6556834443768705
Epoch 590/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.010084089536506396
Epoch: 590, Batch Gradient Norm: 0.6359946967532601
Epoch: 590, Batch Gradient Norm after: 0.6359946967532601
Epoch 591/10000, Prediction Accuracy = 59.39230769230768%, Loss = 0.010083142023247022
Epoch: 591, Batch Gradient Norm: 0.6266871454596824
Epoch: 591, Batch Gradient Norm after: 0.6266871454596824
Epoch 592/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010079302108631684
Epoch: 592, Batch Gradient Norm: 0.6544382277913681
Epoch: 592, Batch Gradient Norm after: 0.6544382277913681
Epoch 593/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010070400240902718
Epoch: 593, Batch Gradient Norm: 0.6421842225173803
Epoch: 593, Batch Gradient Norm after: 0.6421842225173803
Epoch 594/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010074190938701997
Epoch: 594, Batch Gradient Norm: 0.644357433965077
Epoch: 594, Batch Gradient Norm after: 0.644357433965077
Epoch 595/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010091829042022046
Epoch: 595, Batch Gradient Norm: 0.6374517540610912
Epoch: 595, Batch Gradient Norm after: 0.6374517540610912
Epoch 596/10000, Prediction Accuracy = 59.51923076923076%, Loss = 0.01010487937869934
Epoch: 596, Batch Gradient Norm: 0.6292317492044232
Epoch: 596, Batch Gradient Norm after: 0.6292317492044232
Epoch 597/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010078374153146377
Epoch: 597, Batch Gradient Norm: 0.6367677501877207
Epoch: 597, Batch Gradient Norm after: 0.6367677501877207
Epoch 598/10000, Prediction Accuracy = 59.565384615384616%, Loss = 0.010073121708746139
Epoch: 598, Batch Gradient Norm: 0.6315812861534338
Epoch: 598, Batch Gradient Norm after: 0.6315812861534338
Epoch 599/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010086615498249348
Epoch: 599, Batch Gradient Norm: 0.6388573782396774
Epoch: 599, Batch Gradient Norm after: 0.6388573782396774
Epoch 600/10000, Prediction Accuracy = 59.23076923076922%, Loss = 0.010094674590688486
Epoch: 600, Batch Gradient Norm: 0.6472895421030976
Epoch: 600, Batch Gradient Norm after: 0.6472895421030976
Epoch 601/10000, Prediction Accuracy = 59.39615384615385%, Loss = 0.010126449167728424
Epoch: 601, Batch Gradient Norm: 0.6383432341637144
Epoch: 601, Batch Gradient Norm after: 0.6383432341637144
Epoch 602/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.01006137436399093
Epoch: 602, Batch Gradient Norm: 0.6378450149371889
Epoch: 602, Batch Gradient Norm after: 0.6378450149371889
Epoch 603/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010137816676153587
Epoch: 603, Batch Gradient Norm: 0.6394405074488223
Epoch: 603, Batch Gradient Norm after: 0.6394405074488223
Epoch 604/10000, Prediction Accuracy = 59.342307692307685%, Loss = 0.010096968151628971
Epoch: 604, Batch Gradient Norm: 0.6272928360900089
Epoch: 604, Batch Gradient Norm after: 0.6272928360900089
Epoch 605/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010083084209607197
Epoch: 605, Batch Gradient Norm: 0.6348775166622896
Epoch: 605, Batch Gradient Norm after: 0.6348775166622896
Epoch 606/10000, Prediction Accuracy = 59.43846153846153%, Loss = 0.010073124932555052
Epoch: 606, Batch Gradient Norm: 0.6542002552949673
Epoch: 606, Batch Gradient Norm after: 0.6542002552949673
Epoch 607/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010111490049614357
Epoch: 607, Batch Gradient Norm: 0.6513940827024408
Epoch: 607, Batch Gradient Norm after: 0.6513940827024408
Epoch 608/10000, Prediction Accuracy = 59.25384615384615%, Loss = 0.010105879619144477
Epoch: 608, Batch Gradient Norm: 0.632997175247022
Epoch: 608, Batch Gradient Norm after: 0.632997175247022
Epoch 609/10000, Prediction Accuracy = 59.46153846153847%, Loss = 0.010110702150716232
Epoch: 609, Batch Gradient Norm: 0.6338367146281674
Epoch: 609, Batch Gradient Norm after: 0.6338367146281674
Epoch 610/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010077682323753834
Epoch: 610, Batch Gradient Norm: 0.638948351996403
Epoch: 610, Batch Gradient Norm after: 0.638948351996403
Epoch 611/10000, Prediction Accuracy = 59.50000000000001%, Loss = 0.010104951018897386
Epoch: 611, Batch Gradient Norm: 0.6482994751667225
Epoch: 611, Batch Gradient Norm after: 0.6482994751667225
Epoch 612/10000, Prediction Accuracy = 59.29230769230771%, Loss = 0.010083523722222218
Epoch: 612, Batch Gradient Norm: 0.6321795599252524
Epoch: 612, Batch Gradient Norm after: 0.6321795599252524
Epoch 613/10000, Prediction Accuracy = 59.23461538461538%, Loss = 0.010080915302611314
Epoch: 613, Batch Gradient Norm: 0.631905968742829
Epoch: 613, Batch Gradient Norm after: 0.631905968742829
Epoch 614/10000, Prediction Accuracy = 59.43076923076922%, Loss = 0.010095218053230872
Epoch: 614, Batch Gradient Norm: 0.6370815583937609
Epoch: 614, Batch Gradient Norm after: 0.6370815583937609
Epoch 615/10000, Prediction Accuracy = 59.476923076923065%, Loss = 0.010089432247556172
Epoch: 615, Batch Gradient Norm: 0.6481067280975075
Epoch: 615, Batch Gradient Norm after: 0.6481067280975075
Epoch 616/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010132779797109274
Epoch: 616, Batch Gradient Norm: 0.6486424942803075
Epoch: 616, Batch Gradient Norm after: 0.6486424942803075
Epoch 617/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.010131478094710754
Epoch: 617, Batch Gradient Norm: 0.6384344105543299
Epoch: 617, Batch Gradient Norm after: 0.6384344105543299
Epoch 618/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.010098027566877695
Epoch: 618, Batch Gradient Norm: 0.6463676858516616
Epoch: 618, Batch Gradient Norm after: 0.6463676858516616
Epoch 619/10000, Prediction Accuracy = 59.07307692307692%, Loss = 0.010150517981785994
Epoch: 619, Batch Gradient Norm: 0.6456273691148835
Epoch: 619, Batch Gradient Norm after: 0.6456273691148835
Epoch 620/10000, Prediction Accuracy = 59.55%, Loss = 0.010088471910701348
Epoch: 620, Batch Gradient Norm: 0.6331936440255225
Epoch: 620, Batch Gradient Norm after: 0.6331936440255225
Epoch 621/10000, Prediction Accuracy = 59.16923076923077%, Loss = 0.010096286423504353
Epoch: 621, Batch Gradient Norm: 0.6314842965992352
Epoch: 621, Batch Gradient Norm after: 0.6314842965992352
Epoch 622/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.010102650867058681
Epoch: 622, Batch Gradient Norm: 0.6514807521063526
Epoch: 622, Batch Gradient Norm after: 0.6514807521063526
Epoch 623/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010097524939248195
Epoch: 623, Batch Gradient Norm: 0.6535318469983792
Epoch: 623, Batch Gradient Norm after: 0.6535318469983792
Epoch 624/10000, Prediction Accuracy = 59.56923076923078%, Loss = 0.010100286382322129
Epoch: 624, Batch Gradient Norm: 0.6502486419622306
Epoch: 624, Batch Gradient Norm after: 0.6502486419622306
Epoch 625/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010140393287516557
Epoch: 625, Batch Gradient Norm: 0.6293230546181676
Epoch: 625, Batch Gradient Norm after: 0.6293230546181676
Epoch 626/10000, Prediction Accuracy = 59.330769230769235%, Loss = 0.010143832160303226
Epoch: 626, Batch Gradient Norm: 0.6404483412776503
Epoch: 626, Batch Gradient Norm after: 0.6404483412776503
Epoch 627/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.0101155021872658
Epoch: 627, Batch Gradient Norm: 0.6345839743897909
Epoch: 627, Batch Gradient Norm after: 0.6345839743897909
Epoch 628/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.010115663377711406
Epoch: 628, Batch Gradient Norm: 0.6557493384865956
Epoch: 628, Batch Gradient Norm after: 0.6557493384865956
Epoch 629/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010163162190180559
Epoch: 629, Batch Gradient Norm: 0.6356460130247605
Epoch: 629, Batch Gradient Norm after: 0.6356460130247605
Epoch 630/10000, Prediction Accuracy = 59.60384615384615%, Loss = 0.010082056244405417
Epoch: 630, Batch Gradient Norm: 0.6441900348291892
Epoch: 630, Batch Gradient Norm after: 0.6441900348291892
Epoch 631/10000, Prediction Accuracy = 59.55384615384616%, Loss = 0.010066041937814308
Epoch: 631, Batch Gradient Norm: 0.6322013195182933
Epoch: 631, Batch Gradient Norm after: 0.6322013195182933
Epoch 632/10000, Prediction Accuracy = 59.57692307692308%, Loss = 0.010073061101138592
Epoch: 632, Batch Gradient Norm: 0.6474135168053066
Epoch: 632, Batch Gradient Norm after: 0.6474135168053066
Epoch 633/10000, Prediction Accuracy = 59.23846153846153%, Loss = 0.010099418819523774
Epoch: 633, Batch Gradient Norm: 0.6360859990007602
Epoch: 633, Batch Gradient Norm after: 0.6360859990007602
Epoch 634/10000, Prediction Accuracy = 59.56923076923078%, Loss = 0.010083863941522745
Epoch: 634, Batch Gradient Norm: 0.6379075324056712
Epoch: 634, Batch Gradient Norm after: 0.6379075324056712
Epoch 635/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.01008646863584335
Epoch: 635, Batch Gradient Norm: 0.6379832786654135
Epoch: 635, Batch Gradient Norm after: 0.6379832786654135
Epoch 636/10000, Prediction Accuracy = 59.442307692307686%, Loss = 0.010081531766515512
Epoch: 636, Batch Gradient Norm: 0.641917358899171
Epoch: 636, Batch Gradient Norm after: 0.641917358899171
Epoch 637/10000, Prediction Accuracy = 59.52307692307693%, Loss = 0.010104373455620728
Epoch: 637, Batch Gradient Norm: 0.6510011592399101
Epoch: 637, Batch Gradient Norm after: 0.6510011592399101
Epoch 638/10000, Prediction Accuracy = 59.13461538461539%, Loss = 0.010132659369936356
Epoch: 638, Batch Gradient Norm: 0.6262991151050368
Epoch: 638, Batch Gradient Norm after: 0.6262991151050368
Epoch 639/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010023657948924946
Epoch: 639, Batch Gradient Norm: 0.6568373953273937
Epoch: 639, Batch Gradient Norm after: 0.6568373953273937
Epoch 640/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.01012450814820253
Epoch: 640, Batch Gradient Norm: 0.6417681913910531
Epoch: 640, Batch Gradient Norm after: 0.6417681913910531
Epoch 641/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.0100665779497761
Epoch: 641, Batch Gradient Norm: 0.6304039836679274
Epoch: 641, Batch Gradient Norm after: 0.6304039836679274
Epoch 642/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.010123377952438135
Epoch: 642, Batch Gradient Norm: 0.6270555365262331
Epoch: 642, Batch Gradient Norm after: 0.6270555365262331
Epoch 643/10000, Prediction Accuracy = 59.29230769230768%, Loss = 0.01009388740819234
Epoch: 643, Batch Gradient Norm: 0.6314603928196051
Epoch: 643, Batch Gradient Norm after: 0.6314603928196051
Epoch 644/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.01008876871604186
Epoch: 644, Batch Gradient Norm: 0.6479569729325436
Epoch: 644, Batch Gradient Norm after: 0.6479569729325436
Epoch 645/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010107487296828857
Epoch: 645, Batch Gradient Norm: 0.6389699009294829
Epoch: 645, Batch Gradient Norm after: 0.6389699009294829
Epoch 646/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010106375011113973
Epoch: 646, Batch Gradient Norm: 0.6419285981599563
Epoch: 646, Batch Gradient Norm after: 0.6419285981599563
Epoch 647/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010060663191744914
Epoch: 647, Batch Gradient Norm: 0.6416679029765114
Epoch: 647, Batch Gradient Norm after: 0.6416679029765114
Epoch 648/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010084922210528301
Epoch: 648, Batch Gradient Norm: 0.6419314594728996
Epoch: 648, Batch Gradient Norm after: 0.6419314594728996
Epoch 649/10000, Prediction Accuracy = 59.55%, Loss = 0.01008749559808236
Epoch: 649, Batch Gradient Norm: 0.6320020946790652
Epoch: 649, Batch Gradient Norm after: 0.6320020946790652
Epoch 650/10000, Prediction Accuracy = 59.48461538461539%, Loss = 0.010070991630737599
Epoch: 650, Batch Gradient Norm: 0.6398968651528822
Epoch: 650, Batch Gradient Norm after: 0.6398968651528822
Epoch 651/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.010141894794427432
Epoch: 651, Batch Gradient Norm: 0.6346127740951192
Epoch: 651, Batch Gradient Norm after: 0.6346127740951192
Epoch 652/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010087323016845264
Epoch: 652, Batch Gradient Norm: 0.6394948185156214
Epoch: 652, Batch Gradient Norm after: 0.6394948185156214
Epoch 653/10000, Prediction Accuracy = 59.24999999999999%, Loss = 0.010170251704179324
Epoch: 653, Batch Gradient Norm: 0.6512373017666765
Epoch: 653, Batch Gradient Norm after: 0.6512373017666765
Epoch 654/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.010141910841831794
Epoch: 654, Batch Gradient Norm: 0.6292614648810277
Epoch: 654, Batch Gradient Norm after: 0.6292614648810277
Epoch 655/10000, Prediction Accuracy = 59.5576923076923%, Loss = 0.010072048466939192
Epoch: 655, Batch Gradient Norm: 0.6510651317682584
Epoch: 655, Batch Gradient Norm after: 0.6510651317682584
Epoch 656/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010123620024667336
Epoch: 656, Batch Gradient Norm: 0.6508981717971022
Epoch: 656, Batch Gradient Norm after: 0.6508981717971022
Epoch 657/10000, Prediction Accuracy = 59.34615384615384%, Loss = 0.010070361698476167
Epoch: 657, Batch Gradient Norm: 0.6416782830044819
Epoch: 657, Batch Gradient Norm after: 0.6416782830044819
Epoch 658/10000, Prediction Accuracy = 59.31538461538461%, Loss = 0.01008093306938043
Epoch: 658, Batch Gradient Norm: 0.6310730755326156
Epoch: 658, Batch Gradient Norm after: 0.6310730755326156
Epoch 659/10000, Prediction Accuracy = 59.157692307692315%, Loss = 0.010112050634164076
Epoch: 659, Batch Gradient Norm: 0.6647344767621229
Epoch: 659, Batch Gradient Norm after: 0.6647344767621229
Epoch 660/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010113086408147445
Epoch: 660, Batch Gradient Norm: 0.6298776744048652
Epoch: 660, Batch Gradient Norm after: 0.6298776744048652
Epoch 661/10000, Prediction Accuracy = 59.26538461538462%, Loss = 0.010059264918359427
Epoch: 661, Batch Gradient Norm: 0.6340630803564666
Epoch: 661, Batch Gradient Norm after: 0.6340630803564666
Epoch 662/10000, Prediction Accuracy = 59.138461538461534%, Loss = 0.010141315655066417
Epoch: 662, Batch Gradient Norm: 0.6331818044642636
Epoch: 662, Batch Gradient Norm after: 0.6331818044642636
Epoch 663/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.010094061995354982
Epoch: 663, Batch Gradient Norm: 0.6401824331765674
Epoch: 663, Batch Gradient Norm after: 0.6401824331765674
Epoch 664/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010137094614597468
Epoch: 664, Batch Gradient Norm: 0.6438724128208696
Epoch: 664, Batch Gradient Norm after: 0.6438724128208696
Epoch 665/10000, Prediction Accuracy = 59.20384615384614%, Loss = 0.010125489189074589
Epoch: 665, Batch Gradient Norm: 0.6653076779220565
Epoch: 665, Batch Gradient Norm after: 0.6653076779220565
Epoch 666/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.010113552570916139
Epoch: 666, Batch Gradient Norm: 0.6515220924472147
Epoch: 666, Batch Gradient Norm after: 0.6515220924472147
Epoch 667/10000, Prediction Accuracy = 59.61923076923077%, Loss = 0.010174708584180245
Epoch: 667, Batch Gradient Norm: 0.6409023350410011
Epoch: 667, Batch Gradient Norm after: 0.6409023350410011
Epoch 668/10000, Prediction Accuracy = 59.33846153846154%, Loss = 0.010062712244689465
Epoch: 668, Batch Gradient Norm: 0.6447029916412574
Epoch: 668, Batch Gradient Norm after: 0.6447029916412574
Epoch 669/10000, Prediction Accuracy = 59.599999999999994%, Loss = 0.010139403434900137
Epoch: 669, Batch Gradient Norm: 0.636330245847299
Epoch: 669, Batch Gradient Norm after: 0.636330245847299
Epoch 670/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.0100876591526545
Epoch: 670, Batch Gradient Norm: 0.6517812406357176
Epoch: 670, Batch Gradient Norm after: 0.6517812406357176
Epoch 671/10000, Prediction Accuracy = 59.15384615384615%, Loss = 0.010124187988157455
Epoch: 671, Batch Gradient Norm: 0.6622675739485135
Epoch: 671, Batch Gradient Norm after: 0.6622675739485135
Epoch 672/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010124889560616933
Epoch: 672, Batch Gradient Norm: 0.640966673605981
Epoch: 672, Batch Gradient Norm after: 0.640966673605981
Epoch 673/10000, Prediction Accuracy = 59.280769230769245%, Loss = 0.010115111533265848
Epoch: 673, Batch Gradient Norm: 0.6338129967681487
Epoch: 673, Batch Gradient Norm after: 0.6338129967681487
Epoch 674/10000, Prediction Accuracy = 59.29615384615384%, Loss = 0.010118112684442447
Epoch: 674, Batch Gradient Norm: 0.63845064536592
Epoch: 674, Batch Gradient Norm after: 0.63845064536592
Epoch 675/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010086170111138087
Epoch: 675, Batch Gradient Norm: 0.6650627203839882
Epoch: 675, Batch Gradient Norm after: 0.6650627203839882
Epoch 676/10000, Prediction Accuracy = 59.06153846153847%, Loss = 0.010151034507613916
Epoch: 676, Batch Gradient Norm: 0.639808591282753
Epoch: 676, Batch Gradient Norm after: 0.639808591282753
Epoch 677/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010070591806792296
Epoch: 677, Batch Gradient Norm: 0.647371708675494
Epoch: 677, Batch Gradient Norm after: 0.647371708675494
Epoch 678/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.010052325060734382
Epoch: 678, Batch Gradient Norm: 0.6329280933998683
Epoch: 678, Batch Gradient Norm after: 0.6329280933998683
Epoch 679/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.01012260359353744
Epoch: 679, Batch Gradient Norm: 0.6361745020914817
Epoch: 679, Batch Gradient Norm after: 0.6361745020914817
Epoch 680/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010126463853969024
Epoch: 680, Batch Gradient Norm: 0.6163357889826236
Epoch: 680, Batch Gradient Norm after: 0.6163357889826236
Epoch 681/10000, Prediction Accuracy = 59.3423076923077%, Loss = 0.010068283487971012
Epoch: 681, Batch Gradient Norm: 0.6455987071253132
Epoch: 681, Batch Gradient Norm after: 0.6455987071253132
Epoch 682/10000, Prediction Accuracy = 59.50769230769232%, Loss = 0.010114219899360951
Epoch: 682, Batch Gradient Norm: 0.6415690728913275
Epoch: 682, Batch Gradient Norm after: 0.6415690728913275
Epoch 683/10000, Prediction Accuracy = 59.24615384615385%, Loss = 0.010152928674450288
Epoch: 683, Batch Gradient Norm: 0.6280046104217076
Epoch: 683, Batch Gradient Norm after: 0.6280046104217076
Epoch 684/10000, Prediction Accuracy = 59.46923076923076%, Loss = 0.010075044674942127
Epoch: 684, Batch Gradient Norm: 0.6319106239027533
Epoch: 684, Batch Gradient Norm after: 0.6319106239027533
Epoch 685/10000, Prediction Accuracy = 59.330769230769235%, Loss = 0.010098976083099842
Epoch: 685, Batch Gradient Norm: 0.6400631944158522
Epoch: 685, Batch Gradient Norm after: 0.6400631944158522
Epoch 686/10000, Prediction Accuracy = 59.58846153846154%, Loss = 0.010080375278798433
Epoch: 686, Batch Gradient Norm: 0.6575766600934246
Epoch: 686, Batch Gradient Norm after: 0.6575766600934246
Epoch 687/10000, Prediction Accuracy = 59.29999999999999%, Loss = 0.010146106235109843
Epoch: 687, Batch Gradient Norm: 0.6378429291650173
Epoch: 687, Batch Gradient Norm after: 0.6378429291650173
Epoch 688/10000, Prediction Accuracy = 59.45384615384616%, Loss = 0.010105592915071892
Epoch: 688, Batch Gradient Norm: 0.635979644136718
Epoch: 688, Batch Gradient Norm after: 0.635979644136718
Epoch 689/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.01006125257565425
Epoch: 689, Batch Gradient Norm: 0.645369784404338
Epoch: 689, Batch Gradient Norm after: 0.645369784404338
Epoch 690/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.010106340050697327
Epoch: 690, Batch Gradient Norm: 0.6407165809579407
Epoch: 690, Batch Gradient Norm after: 0.6407165809579407
Epoch 691/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010123216833632726
Epoch: 691, Batch Gradient Norm: 0.641718831900118
Epoch: 691, Batch Gradient Norm after: 0.641718831900118
Epoch 692/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.01009693813438599
Epoch: 692, Batch Gradient Norm: 0.6303869638987555
Epoch: 692, Batch Gradient Norm after: 0.6303869638987555
Epoch 693/10000, Prediction Accuracy = 59.38461538461539%, Loss = 0.010090343224314543
Epoch: 693, Batch Gradient Norm: 0.6471536420816751
Epoch: 693, Batch Gradient Norm after: 0.6471536420816751
Epoch 694/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.010111391186141051
Epoch: 694, Batch Gradient Norm: 0.6445741667723064
Epoch: 694, Batch Gradient Norm after: 0.6445741667723064
Epoch 695/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.01009490555868699
Epoch: 695, Batch Gradient Norm: 0.639074598136496
Epoch: 695, Batch Gradient Norm after: 0.639074598136496
Epoch 696/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.01003991525906783
Epoch: 696, Batch Gradient Norm: 0.6384693756367442
Epoch: 696, Batch Gradient Norm after: 0.6384693756367442
Epoch 697/10000, Prediction Accuracy = 59.26153846153846%, Loss = 0.010143756508254088
Epoch: 697, Batch Gradient Norm: 0.6296805394059791
Epoch: 697, Batch Gradient Norm after: 0.6296805394059791
Epoch 698/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.01011253477862248
Epoch: 698, Batch Gradient Norm: 0.638615128873184
Epoch: 698, Batch Gradient Norm after: 0.638615128873184
Epoch 699/10000, Prediction Accuracy = 59.29615384615386%, Loss = 0.010108139079350691
Epoch: 699, Batch Gradient Norm: 0.6641663328039766
Epoch: 699, Batch Gradient Norm after: 0.6641663328039766
Epoch 700/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.0101140089906179
Epoch: 700, Batch Gradient Norm: 0.6483196302759282
Epoch: 700, Batch Gradient Norm after: 0.6483196302759282
Epoch 701/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010115277523604723
Epoch: 701, Batch Gradient Norm: 0.6447353958696246
Epoch: 701, Batch Gradient Norm after: 0.6447353958696246
Epoch 702/10000, Prediction Accuracy = 59.27692307692309%, Loss = 0.01010627937144958
Epoch: 702, Batch Gradient Norm: 0.6347012151476773
Epoch: 702, Batch Gradient Norm after: 0.6347012151476773
Epoch 703/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.01009387350999392
Epoch: 703, Batch Gradient Norm: 0.6440482618049213
Epoch: 703, Batch Gradient Norm after: 0.6440482618049213
Epoch 704/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.010069195467692155
Epoch: 704, Batch Gradient Norm: 0.6395145074487971
Epoch: 704, Batch Gradient Norm after: 0.6395145074487971
Epoch 705/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010074617556081368
Epoch: 705, Batch Gradient Norm: 0.6501867996486308
Epoch: 705, Batch Gradient Norm after: 0.6501867996486308
Epoch 706/10000, Prediction Accuracy = 59.330769230769235%, Loss = 0.01013462983358365
Epoch: 706, Batch Gradient Norm: 0.6422265974314932
Epoch: 706, Batch Gradient Norm after: 0.6422265974314932
Epoch 707/10000, Prediction Accuracy = 59.29615384615384%, Loss = 0.010135724495809812
Epoch: 707, Batch Gradient Norm: 0.6601218684193717
Epoch: 707, Batch Gradient Norm after: 0.6601218684193717
Epoch 708/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010140643455088139
Epoch: 708, Batch Gradient Norm: 0.6298261866467959
Epoch: 708, Batch Gradient Norm after: 0.6298261866467959
Epoch 709/10000, Prediction Accuracy = 59.55384615384616%, Loss = 0.01006585295097186
Epoch: 709, Batch Gradient Norm: 0.6486112270513967
Epoch: 709, Batch Gradient Norm after: 0.6486112270513967
Epoch 710/10000, Prediction Accuracy = 59.326923076923094%, Loss = 0.010134186309117537
Epoch: 710, Batch Gradient Norm: 0.65626493046736
Epoch: 710, Batch Gradient Norm after: 0.65626493046736
Epoch 711/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010078222419206914
Epoch: 711, Batch Gradient Norm: 0.6482192397513074
Epoch: 711, Batch Gradient Norm after: 0.6482192397513074
Epoch 712/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010099115709845837
Epoch: 712, Batch Gradient Norm: 0.6514449132183012
Epoch: 712, Batch Gradient Norm after: 0.6514449132183012
Epoch 713/10000, Prediction Accuracy = 59.31923076923078%, Loss = 0.010140940332068847
Epoch: 713, Batch Gradient Norm: 0.6350774954567885
Epoch: 713, Batch Gradient Norm after: 0.6350774954567885
Epoch 714/10000, Prediction Accuracy = 59.63461538461539%, Loss = 0.0100837041838811
Epoch: 714, Batch Gradient Norm: 0.6330130078030416
Epoch: 714, Batch Gradient Norm after: 0.6330130078030416
Epoch 715/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010077406652271748
Epoch: 715, Batch Gradient Norm: 0.6380424943937535
Epoch: 715, Batch Gradient Norm after: 0.6380424943937535
Epoch 716/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010095487277095135
Epoch: 716, Batch Gradient Norm: 0.6404585916476349
Epoch: 716, Batch Gradient Norm after: 0.6404585916476349
Epoch 717/10000, Prediction Accuracy = 59.607692307692304%, Loss = 0.010089468998977771
Epoch: 717, Batch Gradient Norm: 0.6470819583431346
Epoch: 717, Batch Gradient Norm after: 0.6470819583431346
Epoch 718/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010140348727313371
Epoch: 718, Batch Gradient Norm: 0.6485121357443716
Epoch: 718, Batch Gradient Norm after: 0.6485121357443716
Epoch 719/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.01012171941021314
Epoch: 719, Batch Gradient Norm: 0.6248885751340233
Epoch: 719, Batch Gradient Norm after: 0.6248885751340233
Epoch 720/10000, Prediction Accuracy = 59.692307692307686%, Loss = 0.010059514226248631
Epoch: 720, Batch Gradient Norm: 0.645410897174684
Epoch: 720, Batch Gradient Norm after: 0.645410897174684
Epoch 721/10000, Prediction Accuracy = 59.06923076923076%, Loss = 0.010182346862096053
Epoch: 721, Batch Gradient Norm: 0.6368250554641898
Epoch: 721, Batch Gradient Norm after: 0.6368250554641898
Epoch 722/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010087781084271578
Epoch: 722, Batch Gradient Norm: 0.6213569203960655
Epoch: 722, Batch Gradient Norm after: 0.6213569203960655
Epoch 723/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.010118363353495415
Epoch: 723, Batch Gradient Norm: 0.6600138148099968
Epoch: 723, Batch Gradient Norm after: 0.6600138148099968
Epoch 724/10000, Prediction Accuracy = 59.63846153846154%, Loss = 0.010084579770381633
Epoch: 724, Batch Gradient Norm: 0.6584066653465308
Epoch: 724, Batch Gradient Norm after: 0.6584066653465308
Epoch 725/10000, Prediction Accuracy = 59.28461538461538%, Loss = 0.01011035405099392
Epoch: 725, Batch Gradient Norm: 0.6338980799508352
Epoch: 725, Batch Gradient Norm after: 0.6338980799508352
Epoch 726/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010073398239910603
Epoch: 726, Batch Gradient Norm: 0.6334009857532312
Epoch: 726, Batch Gradient Norm after: 0.6334009857532312
Epoch 727/10000, Prediction Accuracy = 59.25384615384615%, Loss = 0.010097023529502062
Epoch: 727, Batch Gradient Norm: 0.6577353740176415
Epoch: 727, Batch Gradient Norm after: 0.6577353740176415
Epoch 728/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010126239906709928
Epoch: 728, Batch Gradient Norm: 0.6471958408275776
Epoch: 728, Batch Gradient Norm after: 0.6471958408275776
Epoch 729/10000, Prediction Accuracy = 59.419230769230765%, Loss = 0.010085479141427921
Epoch: 729, Batch Gradient Norm: 0.6399022994410377
Epoch: 729, Batch Gradient Norm after: 0.6399022994410377
Epoch 730/10000, Prediction Accuracy = 59.62692307692306%, Loss = 0.010103877848730637
Epoch: 730, Batch Gradient Norm: 0.637152852955497
Epoch: 730, Batch Gradient Norm after: 0.637152852955497
Epoch 731/10000, Prediction Accuracy = 59.50769230769232%, Loss = 0.010118938481005339
Epoch: 731, Batch Gradient Norm: 0.6490755260493585
Epoch: 731, Batch Gradient Norm after: 0.6490755260493585
Epoch 732/10000, Prediction Accuracy = 59.24615384615384%, Loss = 0.010103735499657117
Epoch: 732, Batch Gradient Norm: 0.627412210612941
Epoch: 732, Batch Gradient Norm after: 0.627412210612941
Epoch 733/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010042455620490588
Epoch: 733, Batch Gradient Norm: 0.6402886171525453
Epoch: 733, Batch Gradient Norm after: 0.6402886171525453
Epoch 734/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.010055902199103283
Epoch: 734, Batch Gradient Norm: 0.646399063085735
Epoch: 734, Batch Gradient Norm after: 0.646399063085735
Epoch 735/10000, Prediction Accuracy = 59.26923076923076%, Loss = 0.010119197245400686
Epoch: 735, Batch Gradient Norm: 0.6378861298864175
Epoch: 735, Batch Gradient Norm after: 0.6378861298864175
Epoch 736/10000, Prediction Accuracy = 59.26153846153846%, Loss = 0.010138206613751559
Epoch: 736, Batch Gradient Norm: 0.6367376727890564
Epoch: 736, Batch Gradient Norm after: 0.6367376727890564
Epoch 737/10000, Prediction Accuracy = 59.61153846153846%, Loss = 0.010085986855511483
Epoch: 737, Batch Gradient Norm: 0.6532785803444215
Epoch: 737, Batch Gradient Norm after: 0.6532785803444215
Epoch 738/10000, Prediction Accuracy = 59.26153846153847%, Loss = 0.010126113891601562
Epoch: 738, Batch Gradient Norm: 0.6386188509698579
Epoch: 738, Batch Gradient Norm after: 0.6386188509698579
Epoch 739/10000, Prediction Accuracy = 59.350000000000016%, Loss = 0.010136632105478873
Epoch: 739, Batch Gradient Norm: 0.6335886555021002
Epoch: 739, Batch Gradient Norm after: 0.6335886555021002
Epoch 740/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.010116520481040845
Epoch: 740, Batch Gradient Norm: 0.639787638142103
Epoch: 740, Batch Gradient Norm after: 0.639787638142103
Epoch 741/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010089576387634644
Epoch: 741, Batch Gradient Norm: 0.6364265977923539
Epoch: 741, Batch Gradient Norm after: 0.6364265977923539
Epoch 742/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010104519673264943
Epoch: 742, Batch Gradient Norm: 0.6370139828535362
Epoch: 742, Batch Gradient Norm after: 0.6370139828535362
Epoch 743/10000, Prediction Accuracy = 59.21923076923076%, Loss = 0.010119004604908137
Epoch: 743, Batch Gradient Norm: 0.635727049700174
Epoch: 743, Batch Gradient Norm after: 0.635727049700174
Epoch 744/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.01008753220622356
Epoch: 744, Batch Gradient Norm: 0.6288668814285775
Epoch: 744, Batch Gradient Norm after: 0.6288668814285775
Epoch 745/10000, Prediction Accuracy = 59.592307692307706%, Loss = 0.01009088597045495
Epoch: 745, Batch Gradient Norm: 0.6330120695992736
Epoch: 745, Batch Gradient Norm after: 0.6330120695992736
Epoch 746/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010083596938504623
Epoch: 746, Batch Gradient Norm: 0.6285659494562806
Epoch: 746, Batch Gradient Norm after: 0.6285659494562806
Epoch 747/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010054776588311562
Epoch: 747, Batch Gradient Norm: 0.6423660170112067
Epoch: 747, Batch Gradient Norm after: 0.6423660170112067
Epoch 748/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010069022599894267
Epoch: 748, Batch Gradient Norm: 0.6211709363041095
Epoch: 748, Batch Gradient Norm after: 0.6211709363041095
Epoch 749/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.010067774914205074
Epoch: 749, Batch Gradient Norm: 0.649301720971211
Epoch: 749, Batch Gradient Norm after: 0.649301720971211
Epoch 750/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010156326425763277
Epoch: 750, Batch Gradient Norm: 0.6223006186657049
Epoch: 750, Batch Gradient Norm after: 0.6223006186657049
Epoch 751/10000, Prediction Accuracy = 59.3346153846154%, Loss = 0.010074115859774442
Epoch: 751, Batch Gradient Norm: 0.6402357349371249
Epoch: 751, Batch Gradient Norm after: 0.6402357349371249
Epoch 752/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010078824769992095
Epoch: 752, Batch Gradient Norm: 0.6415092134354832
Epoch: 752, Batch Gradient Norm after: 0.6415092134354832
Epoch 753/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010148295702842565
Epoch: 753, Batch Gradient Norm: 0.6467753191370411
Epoch: 753, Batch Gradient Norm after: 0.6467753191370411
Epoch 754/10000, Prediction Accuracy = 59.18846153846154%, Loss = 0.010135183898875346
Epoch: 754, Batch Gradient Norm: 0.6473029676487042
Epoch: 754, Batch Gradient Norm after: 0.6473029676487042
Epoch 755/10000, Prediction Accuracy = 59.21153846153847%, Loss = 0.01010427143997871
Epoch: 755, Batch Gradient Norm: 0.6668362071612498
Epoch: 755, Batch Gradient Norm after: 0.6668362071612498
Epoch 756/10000, Prediction Accuracy = 58.96153846153846%, Loss = 0.010165854930304564
Epoch: 756, Batch Gradient Norm: 0.6402193294485343
Epoch: 756, Batch Gradient Norm after: 0.6402193294485343
Epoch 757/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010100677681083862
Epoch: 757, Batch Gradient Norm: 0.624811911448061
Epoch: 757, Batch Gradient Norm after: 0.624811911448061
Epoch 758/10000, Prediction Accuracy = 59.27692307692307%, Loss = 0.010095932306005405
Epoch: 758, Batch Gradient Norm: 0.6441538933730954
Epoch: 758, Batch Gradient Norm after: 0.6441538933730954
Epoch 759/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.01007845589461235
Epoch: 759, Batch Gradient Norm: 0.6626526072204043
Epoch: 759, Batch Gradient Norm after: 0.6626526072204043
Epoch 760/10000, Prediction Accuracy = 59.223076923076924%, Loss = 0.010151445077588925
Epoch: 760, Batch Gradient Norm: 0.6292957258425554
Epoch: 760, Batch Gradient Norm after: 0.6292957258425554
Epoch 761/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010040955546383675
Epoch: 761, Batch Gradient Norm: 0.6258588338422123
Epoch: 761, Batch Gradient Norm after: 0.6258588338422123
Epoch 762/10000, Prediction Accuracy = 59.603846153846156%, Loss = 0.010082663180163274
Epoch: 762, Batch Gradient Norm: 0.6394698022820929
Epoch: 762, Batch Gradient Norm after: 0.6394698022820929
Epoch 763/10000, Prediction Accuracy = 59.61538461538463%, Loss = 0.010097089366844067
Epoch: 763, Batch Gradient Norm: 0.6389174680385143
Epoch: 763, Batch Gradient Norm after: 0.6389174680385143
Epoch 764/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010092935883081876
Epoch: 764, Batch Gradient Norm: 0.6334191579507873
Epoch: 764, Batch Gradient Norm after: 0.6334191579507873
Epoch 765/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.010109389200806618
Epoch: 765, Batch Gradient Norm: 0.644677420617331
Epoch: 765, Batch Gradient Norm after: 0.644677420617331
Epoch 766/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.010111180563958792
Epoch: 766, Batch Gradient Norm: 0.6310495833953021
Epoch: 766, Batch Gradient Norm after: 0.6310495833953021
Epoch 767/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010078680128432237
Epoch: 767, Batch Gradient Norm: 0.6348197171874903
Epoch: 767, Batch Gradient Norm after: 0.6348197171874903
Epoch 768/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010124234625926385
Epoch: 768, Batch Gradient Norm: 0.6378043585772984
Epoch: 768, Batch Gradient Norm after: 0.6378043585772984
Epoch 769/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010108918667985843
Epoch: 769, Batch Gradient Norm: 0.6348370833227724
Epoch: 769, Batch Gradient Norm after: 0.6348370833227724
Epoch 770/10000, Prediction Accuracy = 59.72692307692307%, Loss = 0.010035540407093672
Epoch: 770, Batch Gradient Norm: 0.6355957323283857
Epoch: 770, Batch Gradient Norm after: 0.6355957323283857
Epoch 771/10000, Prediction Accuracy = 59.25384615384616%, Loss = 0.010112786952119607
Epoch: 771, Batch Gradient Norm: 0.6412770575537158
Epoch: 771, Batch Gradient Norm after: 0.6412770575537158
Epoch 772/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010104883318910232
Epoch: 772, Batch Gradient Norm: 0.6306250612008862
Epoch: 772, Batch Gradient Norm after: 0.6306250612008862
Epoch 773/10000, Prediction Accuracy = 59.5%, Loss = 0.010101458057761192
Epoch: 773, Batch Gradient Norm: 0.6425852951178275
Epoch: 773, Batch Gradient Norm after: 0.6425852951178275
Epoch 774/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010108182278390113
Epoch: 774, Batch Gradient Norm: 0.6337585656701121
Epoch: 774, Batch Gradient Norm after: 0.6337585656701121
Epoch 775/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010093944863631176
Epoch: 775, Batch Gradient Norm: 0.6385848149006357
Epoch: 775, Batch Gradient Norm after: 0.6385848149006357
Epoch 776/10000, Prediction Accuracy = 59.411538461538456%, Loss = 0.01011195986603315
Epoch: 776, Batch Gradient Norm: 0.6477946144867993
Epoch: 776, Batch Gradient Norm after: 0.6477946144867993
Epoch 777/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010113724722312046
Epoch: 777, Batch Gradient Norm: 0.6560275300649686
Epoch: 777, Batch Gradient Norm after: 0.6560275300649686
Epoch 778/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010128390258894516
Epoch: 778, Batch Gradient Norm: 0.6247686183619793
Epoch: 778, Batch Gradient Norm after: 0.6247686183619793
Epoch 779/10000, Prediction Accuracy = 59.59615384615385%, Loss = 0.010068883976111045
Epoch: 779, Batch Gradient Norm: 0.6380518033778343
Epoch: 779, Batch Gradient Norm after: 0.6380518033778343
Epoch 780/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010081715237062711
Epoch: 780, Batch Gradient Norm: 0.6371036743953508
Epoch: 780, Batch Gradient Norm after: 0.6371036743953508
Epoch 781/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010141174810437055
Epoch: 781, Batch Gradient Norm: 0.6465779555291582
Epoch: 781, Batch Gradient Norm after: 0.6465779555291582
Epoch 782/10000, Prediction Accuracy = 59.21153846153846%, Loss = 0.010103046822433289
Epoch: 782, Batch Gradient Norm: 0.6370801240104282
Epoch: 782, Batch Gradient Norm after: 0.6370801240104282
Epoch 783/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.010104182391212536
Epoch: 783, Batch Gradient Norm: 0.6342250252176618
Epoch: 783, Batch Gradient Norm after: 0.6342250252176618
Epoch 784/10000, Prediction Accuracy = 59.23846153846155%, Loss = 0.010113417457502622
Epoch: 784, Batch Gradient Norm: 0.6362522915306924
Epoch: 784, Batch Gradient Norm after: 0.6362522915306924
Epoch 785/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.010094494845431585
Epoch: 785, Batch Gradient Norm: 0.6321397150238761
Epoch: 785, Batch Gradient Norm after: 0.6321397150238761
Epoch 786/10000, Prediction Accuracy = 59.5923076923077%, Loss = 0.010053205948609572
Epoch: 786, Batch Gradient Norm: 0.641045743359941
Epoch: 786, Batch Gradient Norm after: 0.641045743359941
Epoch 787/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010065888771070884
Epoch: 787, Batch Gradient Norm: 0.6372653403152406
Epoch: 787, Batch Gradient Norm after: 0.6372653403152406
Epoch 788/10000, Prediction Accuracy = 59.650000000000006%, Loss = 0.010079465090082241
Epoch: 788, Batch Gradient Norm: 0.6467033029498598
Epoch: 788, Batch Gradient Norm after: 0.6467033029498598
Epoch 789/10000, Prediction Accuracy = 59.03076923076923%, Loss = 0.01014928763302473
Epoch: 789, Batch Gradient Norm: 0.6331153155511889
Epoch: 789, Batch Gradient Norm after: 0.6331153155511889
Epoch 790/10000, Prediction Accuracy = 59.39230769230768%, Loss = 0.010069145319553522
Epoch: 790, Batch Gradient Norm: 0.6418369035984448
Epoch: 790, Batch Gradient Norm after: 0.6418369035984448
Epoch 791/10000, Prediction Accuracy = 59.28846153846154%, Loss = 0.010107603210669298
Epoch: 791, Batch Gradient Norm: 0.6483941443860006
Epoch: 791, Batch Gradient Norm after: 0.6483941443860006
Epoch 792/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010107742407574104
Epoch: 792, Batch Gradient Norm: 0.6452032670790785
Epoch: 792, Batch Gradient Norm after: 0.6452032670790785
Epoch 793/10000, Prediction Accuracy = 59.161538461538456%, Loss = 0.010139655536757065
Epoch: 793, Batch Gradient Norm: 0.6465719787624193
Epoch: 793, Batch Gradient Norm after: 0.6465719787624193
Epoch 794/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.01011028764053033
Epoch: 794, Batch Gradient Norm: 0.6365535064787939
Epoch: 794, Batch Gradient Norm after: 0.6365535064787939
Epoch 795/10000, Prediction Accuracy = 59.28846153846155%, Loss = 0.010120770034308616
Epoch: 795, Batch Gradient Norm: 0.6542190593226511
Epoch: 795, Batch Gradient Norm after: 0.6542190593226511
Epoch 796/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010069723814152755
Epoch: 796, Batch Gradient Norm: 0.6525184559554298
Epoch: 796, Batch Gradient Norm after: 0.6525184559554298
Epoch 797/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.010095122915047865
Epoch: 797, Batch Gradient Norm: 0.6397157413657899
Epoch: 797, Batch Gradient Norm after: 0.6397157413657899
Epoch 798/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.01007923218779839
Epoch: 798, Batch Gradient Norm: 0.6616809548342867
Epoch: 798, Batch Gradient Norm after: 0.6616809548342867
Epoch 799/10000, Prediction Accuracy = 59.292307692307695%, Loss = 0.010072016013929477
Epoch: 799, Batch Gradient Norm: 0.6329919368716486
Epoch: 799, Batch Gradient Norm after: 0.6329919368716486
Epoch 800/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010080512039936505
Epoch: 800, Batch Gradient Norm: 0.6499160036051241
Epoch: 800, Batch Gradient Norm after: 0.6499160036051241
Epoch 801/10000, Prediction Accuracy = 59.22692307692308%, Loss = 0.010116199174752602
Epoch: 801, Batch Gradient Norm: 0.6617797759935051
Epoch: 801, Batch Gradient Norm after: 0.6617797759935051
Epoch 802/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010112268921847526
Epoch: 802, Batch Gradient Norm: 0.6396620304495494
Epoch: 802, Batch Gradient Norm after: 0.6396620304495494
Epoch 803/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010118365431061158
Epoch: 803, Batch Gradient Norm: 0.6434892597930654
Epoch: 803, Batch Gradient Norm after: 0.6434892597930654
Epoch 804/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010109544158554994
Epoch: 804, Batch Gradient Norm: 0.652778674516366
Epoch: 804, Batch Gradient Norm after: 0.652778674516366
Epoch 805/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.01008379330428747
Epoch: 805, Batch Gradient Norm: 0.6351911115789042
Epoch: 805, Batch Gradient Norm after: 0.6351911115789042
Epoch 806/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.01005440721145043
Epoch: 806, Batch Gradient Norm: 0.6534934012263489
Epoch: 806, Batch Gradient Norm after: 0.6534934012263489
Epoch 807/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010158493470114011
Epoch: 807, Batch Gradient Norm: 0.6328490479234893
Epoch: 807, Batch Gradient Norm after: 0.6328490479234893
Epoch 808/10000, Prediction Accuracy = 59.30384615384616%, Loss = 0.010078791600580398
Epoch: 808, Batch Gradient Norm: 0.654209538382702
Epoch: 808, Batch Gradient Norm after: 0.654209538382702
Epoch 809/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010132634582427831
Epoch: 809, Batch Gradient Norm: 0.6427135087398278
Epoch: 809, Batch Gradient Norm after: 0.6427135087398278
Epoch 810/10000, Prediction Accuracy = 59.69615384615384%, Loss = 0.010044098903353397
Epoch: 810, Batch Gradient Norm: 0.6333704209284475
Epoch: 810, Batch Gradient Norm after: 0.6333704209284475
Epoch 811/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.010062920789305981
Epoch: 811, Batch Gradient Norm: 0.640066688855474
Epoch: 811, Batch Gradient Norm after: 0.640066688855474
Epoch 812/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.01003966609445902
Epoch: 812, Batch Gradient Norm: 0.6495784018646248
Epoch: 812, Batch Gradient Norm after: 0.6495784018646248
Epoch 813/10000, Prediction Accuracy = 59.065384615384616%, Loss = 0.01013394108471962
Epoch: 813, Batch Gradient Norm: 0.6447612077898823
Epoch: 813, Batch Gradient Norm after: 0.6447612077898823
Epoch 814/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010074418181410203
Epoch: 814, Batch Gradient Norm: 0.628234538245848
Epoch: 814, Batch Gradient Norm after: 0.628234538245848
Epoch 815/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.010107598124215236
Epoch: 815, Batch Gradient Norm: 0.6278255075429412
Epoch: 815, Batch Gradient Norm after: 0.6278255075429412
Epoch 816/10000, Prediction Accuracy = 59.57307692307692%, Loss = 0.010056634147006732
Epoch: 816, Batch Gradient Norm: 0.6356455409691237
Epoch: 816, Batch Gradient Norm after: 0.6356455409691237
Epoch 817/10000, Prediction Accuracy = 59.43076923076922%, Loss = 0.010118383627671462
Epoch: 817, Batch Gradient Norm: 0.6481040547138249
Epoch: 817, Batch Gradient Norm after: 0.6481040547138249
Epoch 818/10000, Prediction Accuracy = 59.3%, Loss = 0.0100914931927736
Epoch: 818, Batch Gradient Norm: 0.6281464520991732
Epoch: 818, Batch Gradient Norm after: 0.6281464520991732
Epoch 819/10000, Prediction Accuracy = 59.607692307692304%, Loss = 0.010121780519302074
Epoch: 819, Batch Gradient Norm: 0.6423497672849519
Epoch: 819, Batch Gradient Norm after: 0.6423497672849519
Epoch 820/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.010131588348975549
Epoch: 820, Batch Gradient Norm: 0.64118191505906
Epoch: 820, Batch Gradient Norm after: 0.64118191505906
Epoch 821/10000, Prediction Accuracy = 59.630769230769225%, Loss = 0.010086710994633345
Epoch: 821, Batch Gradient Norm: 0.6496454540643284
Epoch: 821, Batch Gradient Norm after: 0.6496454540643284
Epoch 822/10000, Prediction Accuracy = 59.55%, Loss = 0.010089568005731473
Epoch: 822, Batch Gradient Norm: 0.6366813216869079
Epoch: 822, Batch Gradient Norm after: 0.6366813216869079
Epoch 823/10000, Prediction Accuracy = 59.542307692307695%, Loss = 0.010104856167275172
Epoch: 823, Batch Gradient Norm: 0.6344460381237846
Epoch: 823, Batch Gradient Norm after: 0.6344460381237846
Epoch 824/10000, Prediction Accuracy = 59.51923076923077%, Loss = 0.010032419258585343
Epoch: 824, Batch Gradient Norm: 0.6364273425469673
Epoch: 824, Batch Gradient Norm after: 0.6364273425469673
Epoch 825/10000, Prediction Accuracy = 59.60384615384616%, Loss = 0.010059575693538556
Epoch: 825, Batch Gradient Norm: 0.6458319941966234
Epoch: 825, Batch Gradient Norm after: 0.6458319941966234
Epoch 826/10000, Prediction Accuracy = 59.392307692307675%, Loss = 0.010103809933822889
Epoch: 826, Batch Gradient Norm: 0.6252535386910496
Epoch: 826, Batch Gradient Norm after: 0.6252535386910496
Epoch 827/10000, Prediction Accuracy = 59.33076923076924%, Loss = 0.010095341919133296
Epoch: 827, Batch Gradient Norm: 0.6400515046688656
Epoch: 827, Batch Gradient Norm after: 0.6400515046688656
Epoch 828/10000, Prediction Accuracy = 59.419230769230765%, Loss = 0.010077119160156984
Epoch: 828, Batch Gradient Norm: 0.6507286887144627
Epoch: 828, Batch Gradient Norm after: 0.6507286887144627
Epoch 829/10000, Prediction Accuracy = 59.52692307692309%, Loss = 0.010135883895250468
Epoch: 829, Batch Gradient Norm: 0.6392561956464595
Epoch: 829, Batch Gradient Norm after: 0.6392561956464595
Epoch 830/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.010169571695419459
Epoch: 830, Batch Gradient Norm: 0.6322547557853289
Epoch: 830, Batch Gradient Norm after: 0.6322547557853289
Epoch 831/10000, Prediction Accuracy = 59.16153846153846%, Loss = 0.010100262239575386
Epoch: 831, Batch Gradient Norm: 0.6345680311449021
Epoch: 831, Batch Gradient Norm after: 0.6345680311449021
Epoch 832/10000, Prediction Accuracy = 59.29615384615386%, Loss = 0.010104497894644737
Epoch: 832, Batch Gradient Norm: 0.6276280502176991
Epoch: 832, Batch Gradient Norm after: 0.6276280502176991
Epoch 833/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010110030308938943
Epoch: 833, Batch Gradient Norm: 0.6513957146399252
Epoch: 833, Batch Gradient Norm after: 0.6513957146399252
Epoch 834/10000, Prediction Accuracy = 59.24615384615385%, Loss = 0.010123530331139382
Epoch: 834, Batch Gradient Norm: 0.6375898650467485
Epoch: 834, Batch Gradient Norm after: 0.6375898650467485
Epoch 835/10000, Prediction Accuracy = 59.38461538461539%, Loss = 0.010110850947407575
Epoch: 835, Batch Gradient Norm: 0.646746547729146
Epoch: 835, Batch Gradient Norm after: 0.646746547729146
Epoch 836/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.010104340859330617
Epoch: 836, Batch Gradient Norm: 0.6490301790433411
Epoch: 836, Batch Gradient Norm after: 0.6490301790433411
Epoch 837/10000, Prediction Accuracy = 59.22307692307693%, Loss = 0.010104233542313943
Epoch: 837, Batch Gradient Norm: 0.635678625794297
Epoch: 837, Batch Gradient Norm after: 0.635678625794297
Epoch 838/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.010082944009739619
Epoch: 838, Batch Gradient Norm: 0.6362527687388511
Epoch: 838, Batch Gradient Norm after: 0.6362527687388511
Epoch 839/10000, Prediction Accuracy = 59.63076923076923%, Loss = 0.01005284609989478
Epoch: 839, Batch Gradient Norm: 0.6458903337981051
Epoch: 839, Batch Gradient Norm after: 0.6458903337981051
Epoch 840/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.01011440738175924
Epoch: 840, Batch Gradient Norm: 0.6296356017801569
Epoch: 840, Batch Gradient Norm after: 0.6296356017801569
Epoch 841/10000, Prediction Accuracy = 59.61153846153846%, Loss = 0.010052462609914633
Epoch: 841, Batch Gradient Norm: 0.6423389807428926
Epoch: 841, Batch Gradient Norm after: 0.6423389807428926
Epoch 842/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010093537374184681
Epoch: 842, Batch Gradient Norm: 0.6088176887472042
Epoch: 842, Batch Gradient Norm after: 0.6088176887472042
Epoch 843/10000, Prediction Accuracy = 59.638461538461534%, Loss = 0.010049767863865081
Epoch: 843, Batch Gradient Norm: 0.6594143523568491
Epoch: 843, Batch Gradient Norm after: 0.6594143523568491
Epoch 844/10000, Prediction Accuracy = 59.30769230769231%, Loss = 0.010130320604030903
Epoch: 844, Batch Gradient Norm: 0.6513716073749812
Epoch: 844, Batch Gradient Norm after: 0.6513716073749812
Epoch 845/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010128759779036045
Epoch: 845, Batch Gradient Norm: 0.6402491665890498
Epoch: 845, Batch Gradient Norm after: 0.6402491665890498
Epoch 846/10000, Prediction Accuracy = 59.550000000000004%, Loss = 0.010081103071570396
Epoch: 846, Batch Gradient Norm: 0.6420574131057037
Epoch: 846, Batch Gradient Norm after: 0.6420574131057037
Epoch 847/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010107686599859824
Epoch: 847, Batch Gradient Norm: 0.6650657620716794
Epoch: 847, Batch Gradient Norm after: 0.6650657620716794
Epoch 848/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.010142856349165622
Epoch: 848, Batch Gradient Norm: 0.6567085585672181
Epoch: 848, Batch Gradient Norm after: 0.6567085585672181
Epoch 849/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.010114745308573429
Epoch: 849, Batch Gradient Norm: 0.6483460816345062
Epoch: 849, Batch Gradient Norm after: 0.6483460816345062
Epoch 850/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010138953104615211
Epoch: 850, Batch Gradient Norm: 0.6280675254558776
Epoch: 850, Batch Gradient Norm after: 0.6280675254558776
Epoch 851/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010053312119383078
Epoch: 851, Batch Gradient Norm: 0.6367468570113753
Epoch: 851, Batch Gradient Norm after: 0.6367468570113753
Epoch 852/10000, Prediction Accuracy = 59.150000000000006%, Loss = 0.010077460239139887
Epoch: 852, Batch Gradient Norm: 0.628206599502273
Epoch: 852, Batch Gradient Norm after: 0.628206599502273
Epoch 853/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010088377632200718
Epoch: 853, Batch Gradient Norm: 0.6496691477794926
Epoch: 853, Batch Gradient Norm after: 0.6496691477794926
Epoch 854/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.010120456680082358
Epoch: 854, Batch Gradient Norm: 0.6342997884121498
Epoch: 854, Batch Gradient Norm after: 0.6342997884121498
Epoch 855/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.010053705925551744
Epoch: 855, Batch Gradient Norm: 0.6391798059517517
Epoch: 855, Batch Gradient Norm after: 0.6391798059517517
Epoch 856/10000, Prediction Accuracy = 59.08846153846154%, Loss = 0.01009910088032484
Epoch: 856, Batch Gradient Norm: 0.6504031428297525
Epoch: 856, Batch Gradient Norm after: 0.6504031428297525
Epoch 857/10000, Prediction Accuracy = 59.338461538461544%, Loss = 0.010093510150909424
Epoch: 857, Batch Gradient Norm: 0.6458682410464069
Epoch: 857, Batch Gradient Norm after: 0.6458682410464069
Epoch 858/10000, Prediction Accuracy = 59.03461538461538%, Loss = 0.010138970298262743
Epoch: 858, Batch Gradient Norm: 0.6460441002385879
Epoch: 858, Batch Gradient Norm after: 0.6460441002385879
Epoch 859/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010147062918314567
Epoch: 859, Batch Gradient Norm: 0.6372719165850677
Epoch: 859, Batch Gradient Norm after: 0.6372719165850677
Epoch 860/10000, Prediction Accuracy = 59.176923076923075%, Loss = 0.010089249851611944
Epoch: 860, Batch Gradient Norm: 0.6421141081497818
Epoch: 860, Batch Gradient Norm after: 0.6421141081497818
Epoch 861/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.010075253076278247
Epoch: 861, Batch Gradient Norm: 0.6306414070033787
Epoch: 861, Batch Gradient Norm after: 0.6306414070033787
Epoch 862/10000, Prediction Accuracy = 59.63461538461539%, Loss = 0.010107090123570882
Epoch: 862, Batch Gradient Norm: 0.6565184898592227
Epoch: 862, Batch Gradient Norm after: 0.6565184898592227
Epoch 863/10000, Prediction Accuracy = 59.61923076923077%, Loss = 0.01004833670762869
Epoch: 863, Batch Gradient Norm: 0.632397627034497
Epoch: 863, Batch Gradient Norm after: 0.632397627034497
Epoch 864/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.01006780994626192
Epoch: 864, Batch Gradient Norm: 0.6318692708170768
Epoch: 864, Batch Gradient Norm after: 0.6318692708170768
Epoch 865/10000, Prediction Accuracy = 59.62307692307692%, Loss = 0.010119197245400686
Epoch: 865, Batch Gradient Norm: 0.6433639350222415
Epoch: 865, Batch Gradient Norm after: 0.6433639350222415
Epoch 866/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010113116783591418
Epoch: 866, Batch Gradient Norm: 0.6524270316169469
Epoch: 866, Batch Gradient Norm after: 0.6524270316169469
Epoch 867/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010068064054044394
Epoch: 867, Batch Gradient Norm: 0.6354036633215843
Epoch: 867, Batch Gradient Norm after: 0.6354036633215843
Epoch 868/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010055660699995665
Epoch: 868, Batch Gradient Norm: 0.6300191864066864
Epoch: 868, Batch Gradient Norm after: 0.6300191864066864
Epoch 869/10000, Prediction Accuracy = 59.661538461538456%, Loss = 0.010080112789112788
Epoch: 869, Batch Gradient Norm: 0.6485955154309408
Epoch: 869, Batch Gradient Norm after: 0.6485955154309408
Epoch 870/10000, Prediction Accuracy = 59.25%, Loss = 0.010083118811822854
Epoch: 870, Batch Gradient Norm: 0.6507619425146124
Epoch: 870, Batch Gradient Norm after: 0.6507619425146124
Epoch 871/10000, Prediction Accuracy = 59.09615384615385%, Loss = 0.010125767582884202
Epoch: 871, Batch Gradient Norm: 0.6412469214859501
Epoch: 871, Batch Gradient Norm after: 0.6412469214859501
Epoch 872/10000, Prediction Accuracy = 59.39615384615384%, Loss = 0.010094121170158569
Epoch: 872, Batch Gradient Norm: 0.6292634507372904
Epoch: 872, Batch Gradient Norm after: 0.6292634507372904
Epoch 873/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.01008303857480104
Epoch: 873, Batch Gradient Norm: 0.64455303657385
Epoch: 873, Batch Gradient Norm after: 0.64455303657385
Epoch 874/10000, Prediction Accuracy = 59.19615384615384%, Loss = 0.010121525408556828
Epoch: 874, Batch Gradient Norm: 0.6288615845562942
Epoch: 874, Batch Gradient Norm after: 0.6288615845562942
Epoch 875/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010091527150227474
Epoch: 875, Batch Gradient Norm: 0.63028616143414
Epoch: 875, Batch Gradient Norm after: 0.63028616143414
Epoch 876/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010041208221362187
Epoch: 876, Batch Gradient Norm: 0.6463019930103686
Epoch: 876, Batch Gradient Norm after: 0.6463019930103686
Epoch 877/10000, Prediction Accuracy = 59.376923076923084%, Loss = 0.010144847660110546
Epoch: 877, Batch Gradient Norm: 0.6296636935921299
Epoch: 877, Batch Gradient Norm after: 0.6296636935921299
Epoch 878/10000, Prediction Accuracy = 59.45000000000001%, Loss = 0.010117366838340577
Epoch: 878, Batch Gradient Norm: 0.6313320556947091
Epoch: 878, Batch Gradient Norm after: 0.6313320556947091
Epoch 879/10000, Prediction Accuracy = 59.361538461538466%, Loss = 0.010087764463745631
Epoch: 879, Batch Gradient Norm: 0.6247448391133072
Epoch: 879, Batch Gradient Norm after: 0.6247448391133072
Epoch 880/10000, Prediction Accuracy = 59.41923076923078%, Loss = 0.01007495799030249
Epoch: 880, Batch Gradient Norm: 0.6478366634279504
Epoch: 880, Batch Gradient Norm after: 0.6478366634279504
Epoch 881/10000, Prediction Accuracy = 59.55384615384615%, Loss = 0.01005995087325573
Epoch: 881, Batch Gradient Norm: 0.6251987940645705
Epoch: 881, Batch Gradient Norm after: 0.6251987940645705
Epoch 882/10000, Prediction Accuracy = 59.326923076923066%, Loss = 0.010084798201345481
Epoch: 882, Batch Gradient Norm: 0.6405088880649422
Epoch: 882, Batch Gradient Norm after: 0.6405088880649422
Epoch 883/10000, Prediction Accuracy = 59.365384615384606%, Loss = 0.010127418961089391
Epoch: 883, Batch Gradient Norm: 0.6431782802366739
Epoch: 883, Batch Gradient Norm after: 0.6431782802366739
Epoch 884/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010088554153648706
Epoch: 884, Batch Gradient Norm: 0.6429833872550316
Epoch: 884, Batch Gradient Norm after: 0.6429833872550316
Epoch 885/10000, Prediction Accuracy = 59.419230769230765%, Loss = 0.010145585410870038
Epoch: 885, Batch Gradient Norm: 0.6180357951859998
Epoch: 885, Batch Gradient Norm after: 0.6180357951859998
Epoch 886/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010081898134488326
Epoch: 886, Batch Gradient Norm: 0.640285312225619
Epoch: 886, Batch Gradient Norm after: 0.640285312225619
Epoch 887/10000, Prediction Accuracy = 59.31538461538462%, Loss = 0.01012057882662003
Epoch: 887, Batch Gradient Norm: 0.6361736932156309
Epoch: 887, Batch Gradient Norm after: 0.6361736932156309
Epoch 888/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.010116849452830277
Epoch: 888, Batch Gradient Norm: 0.6328530529960765
Epoch: 888, Batch Gradient Norm after: 0.6328530529960765
Epoch 889/10000, Prediction Accuracy = 59.31923076923078%, Loss = 0.010132292643762551
Epoch: 889, Batch Gradient Norm: 0.6552894838490211
Epoch: 889, Batch Gradient Norm after: 0.6552894838490211
Epoch 890/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.010097434027836872
Epoch: 890, Batch Gradient Norm: 0.6350296771001801
Epoch: 890, Batch Gradient Norm after: 0.6350296771001801
Epoch 891/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010064140678598331
Epoch: 891, Batch Gradient Norm: 0.6445442417923987
Epoch: 891, Batch Gradient Norm after: 0.6445442417923987
Epoch 892/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010078986318638692
Epoch: 892, Batch Gradient Norm: 0.6529297973158441
Epoch: 892, Batch Gradient Norm after: 0.6529297973158441
Epoch 893/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.010092802489033112
Epoch: 893, Batch Gradient Norm: 0.6382833270260049
Epoch: 893, Batch Gradient Norm after: 0.6382833270260049
Epoch 894/10000, Prediction Accuracy = 59.10384615384615%, Loss = 0.010094156846977197
Epoch: 894, Batch Gradient Norm: 0.6371650084233128
Epoch: 894, Batch Gradient Norm after: 0.6371650084233128
Epoch 895/10000, Prediction Accuracy = 59.39615384615384%, Loss = 0.010097409956730329
Epoch: 895, Batch Gradient Norm: 0.6420900710384487
Epoch: 895, Batch Gradient Norm after: 0.6420900710384487
Epoch 896/10000, Prediction Accuracy = 59.619230769230775%, Loss = 0.010112810665025162
Epoch: 896, Batch Gradient Norm: 0.6243385535888082
Epoch: 896, Batch Gradient Norm after: 0.6243385535888082
Epoch 897/10000, Prediction Accuracy = 59.419230769230765%, Loss = 0.010111310590918247
Epoch: 897, Batch Gradient Norm: 0.6418246361219205
Epoch: 897, Batch Gradient Norm after: 0.6418246361219205
Epoch 898/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.01008057279082445
Epoch: 898, Batch Gradient Norm: 0.6344072544281419
Epoch: 898, Batch Gradient Norm after: 0.6344072544281419
Epoch 899/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010102547848453889
Epoch: 899, Batch Gradient Norm: 0.6473587373084039
Epoch: 899, Batch Gradient Norm after: 0.6473587373084039
Epoch 900/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010076662955375819
Epoch: 900, Batch Gradient Norm: 0.6372212287217734
Epoch: 900, Batch Gradient Norm after: 0.6372212287217734
Epoch 901/10000, Prediction Accuracy = 59.61538461538461%, Loss = 0.010094571500443496
Epoch: 901, Batch Gradient Norm: 0.6341057193774373
Epoch: 901, Batch Gradient Norm after: 0.6341057193774373
Epoch 902/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.010049990951441802
Epoch: 902, Batch Gradient Norm: 0.6476659971348384
Epoch: 902, Batch Gradient Norm after: 0.6476659971348384
Epoch 903/10000, Prediction Accuracy = 59.43076923076922%, Loss = 0.010090372238594752
Epoch: 903, Batch Gradient Norm: 0.6606298382286868
Epoch: 903, Batch Gradient Norm after: 0.6606298382286868
Epoch 904/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.010117000971849147
Epoch: 904, Batch Gradient Norm: 0.641411907153849
Epoch: 904, Batch Gradient Norm after: 0.641411907153849
Epoch 905/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.010106545944626514
Epoch: 905, Batch Gradient Norm: 0.6326380749074214
Epoch: 905, Batch Gradient Norm after: 0.6326380749074214
Epoch 906/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010096848799059024
Epoch: 906, Batch Gradient Norm: 0.6331161603376058
Epoch: 906, Batch Gradient Norm after: 0.6331161603376058
Epoch 907/10000, Prediction Accuracy = 59.47692307692307%, Loss = 0.010101617815402837
Epoch: 907, Batch Gradient Norm: 0.638279108008169
Epoch: 907, Batch Gradient Norm after: 0.638279108008169
Epoch 908/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010034100152552128
Epoch: 908, Batch Gradient Norm: 0.6501171522561133
Epoch: 908, Batch Gradient Norm after: 0.6501171522561133
Epoch 909/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.010113596128156552
Epoch: 909, Batch Gradient Norm: 0.6436521027439905
Epoch: 909, Batch Gradient Norm after: 0.6436521027439905
Epoch 910/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.010090194141062407
Epoch: 910, Batch Gradient Norm: 0.6474091829117297
Epoch: 910, Batch Gradient Norm after: 0.6474091829117297
Epoch 911/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010078937460023623
Epoch: 911, Batch Gradient Norm: 0.6430192167372952
Epoch: 911, Batch Gradient Norm after: 0.6430192167372952
Epoch 912/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.01010453629379089
Epoch: 912, Batch Gradient Norm: 0.6465391439224502
Epoch: 912, Batch Gradient Norm after: 0.6465391439224502
Epoch 913/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010120149558553329
Epoch: 913, Batch Gradient Norm: 0.6575542640715873
Epoch: 913, Batch Gradient Norm after: 0.6575542640715873
Epoch 914/10000, Prediction Accuracy = 59.39615384615384%, Loss = 0.010114192246244503
Epoch: 914, Batch Gradient Norm: 0.6554156666045854
Epoch: 914, Batch Gradient Norm after: 0.6554156666045854
Epoch 915/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010096949740098072
Epoch: 915, Batch Gradient Norm: 0.6506841800702486
Epoch: 915, Batch Gradient Norm after: 0.6506841800702486
Epoch 916/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010055054122438798
Epoch: 916, Batch Gradient Norm: 0.6383654184275452
Epoch: 916, Batch Gradient Norm after: 0.6383654184275452
Epoch 917/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.01012547744008211
Epoch: 917, Batch Gradient Norm: 0.6678138138292892
Epoch: 917, Batch Gradient Norm after: 0.6678138138292892
Epoch 918/10000, Prediction Accuracy = 59.23461538461539%, Loss = 0.01018323742139798
Epoch: 918, Batch Gradient Norm: 0.6444358483349503
Epoch: 918, Batch Gradient Norm after: 0.6444358483349503
Epoch 919/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010085082684571926
Epoch: 919, Batch Gradient Norm: 0.6334452182136512
Epoch: 919, Batch Gradient Norm after: 0.6334452182136512
Epoch 920/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010024776968818445
Epoch: 920, Batch Gradient Norm: 0.6400939537481899
Epoch: 920, Batch Gradient Norm after: 0.6400939537481899
Epoch 921/10000, Prediction Accuracy = 59.40000000000001%, Loss = 0.010075866602934323
Epoch: 921, Batch Gradient Norm: 0.66287507944279
Epoch: 921, Batch Gradient Norm after: 0.66287507944279
Epoch 922/10000, Prediction Accuracy = 59.43461538461539%, Loss = 0.010119875606435996
Epoch: 922, Batch Gradient Norm: 0.640699195928275
Epoch: 922, Batch Gradient Norm after: 0.640699195928275
Epoch 923/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.010095164466362733
Epoch: 923, Batch Gradient Norm: 0.6506706923404957
Epoch: 923, Batch Gradient Norm after: 0.6506706923404957
Epoch 924/10000, Prediction Accuracy = 59.45769230769232%, Loss = 0.010101741824585658
Epoch: 924, Batch Gradient Norm: 0.6380094786693936
Epoch: 924, Batch Gradient Norm after: 0.6380094786693936
Epoch 925/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010130372829735279
Epoch: 925, Batch Gradient Norm: 0.6353792751766578
Epoch: 925, Batch Gradient Norm after: 0.6353792751766578
Epoch 926/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010072482391618766
Epoch: 926, Batch Gradient Norm: 0.6389656331118609
Epoch: 926, Batch Gradient Norm after: 0.6389656331118609
Epoch 927/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010069689641778286
Epoch: 927, Batch Gradient Norm: 0.627414015464614
Epoch: 927, Batch Gradient Norm after: 0.627414015464614
Epoch 928/10000, Prediction Accuracy = 59.31538461538461%, Loss = 0.010057316806453925
Epoch: 928, Batch Gradient Norm: 0.6485225428104344
Epoch: 928, Batch Gradient Norm after: 0.6485225428104344
Epoch 929/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010131722960907679
Epoch: 929, Batch Gradient Norm: 0.6348547707055793
Epoch: 929, Batch Gradient Norm after: 0.6348547707055793
Epoch 930/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.010062589739950804
Epoch: 930, Batch Gradient Norm: 0.6335297730772437
Epoch: 930, Batch Gradient Norm after: 0.6335297730772437
Epoch 931/10000, Prediction Accuracy = 59.292307692307695%, Loss = 0.010091144376649307
Epoch: 931, Batch Gradient Norm: 0.6453706336865658
Epoch: 931, Batch Gradient Norm after: 0.6453706336865658
Epoch 932/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.0101091036429772
Epoch: 932, Batch Gradient Norm: 0.6401092414497406
Epoch: 932, Batch Gradient Norm after: 0.6401092414497406
Epoch 933/10000, Prediction Accuracy = 59.7%, Loss = 0.010081639799934167
Epoch: 933, Batch Gradient Norm: 0.6368451484040939
Epoch: 933, Batch Gradient Norm after: 0.6368451484040939
Epoch 934/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010083115731294338
Epoch: 934, Batch Gradient Norm: 0.6371506030584705
Epoch: 934, Batch Gradient Norm after: 0.6371506030584705
Epoch 935/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010072472433631238
Epoch: 935, Batch Gradient Norm: 0.6408158961652374
Epoch: 935, Batch Gradient Norm after: 0.6408158961652374
Epoch 936/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010103568148154479
Epoch: 936, Batch Gradient Norm: 0.6423935216458755
Epoch: 936, Batch Gradient Norm after: 0.6423935216458755
Epoch 937/10000, Prediction Accuracy = 59.32307692307693%, Loss = 0.010103041735979227
Epoch: 937, Batch Gradient Norm: 0.6548977550289761
Epoch: 937, Batch Gradient Norm after: 0.6548977550289761
Epoch 938/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.01012128577209436
Epoch: 938, Batch Gradient Norm: 0.6377107250052793
Epoch: 938, Batch Gradient Norm after: 0.6377107250052793
Epoch 939/10000, Prediction Accuracy = 59.15384615384616%, Loss = 0.010118588876838867
Epoch: 939, Batch Gradient Norm: 0.6384527402547681
Epoch: 939, Batch Gradient Norm after: 0.6384527402547681
Epoch 940/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.01010083700888432
Epoch: 940, Batch Gradient Norm: 0.63246842153164
Epoch: 940, Batch Gradient Norm after: 0.63246842153164
Epoch 941/10000, Prediction Accuracy = 59.41153846153847%, Loss = 0.010053902506255187
Epoch: 941, Batch Gradient Norm: 0.6372800599373607
Epoch: 941, Batch Gradient Norm after: 0.6372800599373607
Epoch 942/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010070252447174145
Epoch: 942, Batch Gradient Norm: 0.6398435982403831
Epoch: 942, Batch Gradient Norm after: 0.6398435982403831
Epoch 943/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.01012223013318502
Epoch: 943, Batch Gradient Norm: 0.6497241342448913
Epoch: 943, Batch Gradient Norm after: 0.6497241342448913
Epoch 944/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010138987062069086
Epoch: 944, Batch Gradient Norm: 0.6370883269349753
Epoch: 944, Batch Gradient Norm after: 0.6370883269349753
Epoch 945/10000, Prediction Accuracy = 59.303846153846166%, Loss = 0.010137445221726712
Epoch: 945, Batch Gradient Norm: 0.6244085991010455
Epoch: 945, Batch Gradient Norm after: 0.6244085991010455
Epoch 946/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.010098163038492203
Epoch: 946, Batch Gradient Norm: 0.6455170982835801
Epoch: 946, Batch Gradient Norm after: 0.6455170982835801
Epoch 947/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.01009500463708089
Epoch: 947, Batch Gradient Norm: 0.6553736789440496
Epoch: 947, Batch Gradient Norm after: 0.6553736789440496
Epoch 948/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010098557346142255
Epoch: 948, Batch Gradient Norm: 0.650134737429556
Epoch: 948, Batch Gradient Norm after: 0.650134737429556
Epoch 949/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.010135296947107865
Epoch: 949, Batch Gradient Norm: 0.6461640334096823
Epoch: 949, Batch Gradient Norm after: 0.6461640334096823
Epoch 950/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010086168033572344
Epoch: 950, Batch Gradient Norm: 0.6451134511044959
Epoch: 950, Batch Gradient Norm after: 0.6451134511044959
Epoch 951/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.01010041984801109
Epoch: 951, Batch Gradient Norm: 0.6382569451153394
Epoch: 951, Batch Gradient Norm after: 0.6382569451153394
Epoch 952/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.01009487253255569
Epoch: 952, Batch Gradient Norm: 0.6260499971278582
Epoch: 952, Batch Gradient Norm after: 0.6260499971278582
Epoch 953/10000, Prediction Accuracy = 59.27692307692307%, Loss = 0.01008016959978984
Epoch: 953, Batch Gradient Norm: 0.6479628170112928
Epoch: 953, Batch Gradient Norm after: 0.6479628170112928
Epoch 954/10000, Prediction Accuracy = 59.50384615384616%, Loss = 0.010094721658298602
Epoch: 954, Batch Gradient Norm: 0.6491377115086282
Epoch: 954, Batch Gradient Norm after: 0.6491377115086282
Epoch 955/10000, Prediction Accuracy = 59.40384615384616%, Loss = 0.010071239864023833
Epoch: 955, Batch Gradient Norm: 0.644776619477399
Epoch: 955, Batch Gradient Norm after: 0.644776619477399
Epoch 956/10000, Prediction Accuracy = 59.35000000000001%, Loss = 0.010137262037740303
Epoch: 956, Batch Gradient Norm: 0.6305074766245466
Epoch: 956, Batch Gradient Norm after: 0.6305074766245466
Epoch 957/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.010072431025596766
Epoch: 957, Batch Gradient Norm: 0.6416294607793169
Epoch: 957, Batch Gradient Norm after: 0.6416294607793169
Epoch 958/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010085726658312174
Epoch: 958, Batch Gradient Norm: 0.6216242891553262
Epoch: 958, Batch Gradient Norm after: 0.6216242891553262
Epoch 959/10000, Prediction Accuracy = 59.56153846153847%, Loss = 0.01007363866441525
Epoch: 959, Batch Gradient Norm: 0.6435436560369278
Epoch: 959, Batch Gradient Norm after: 0.6435436560369278
Epoch 960/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010103759140922474
Epoch: 960, Batch Gradient Norm: 0.6590082534288669
Epoch: 960, Batch Gradient Norm after: 0.6590082534288669
Epoch 961/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010117209229904871
Epoch: 961, Batch Gradient Norm: 0.6462319868131587
Epoch: 961, Batch Gradient Norm after: 0.6462319868131587
Epoch 962/10000, Prediction Accuracy = 59.173076923076934%, Loss = 0.01013968584056084
Epoch: 962, Batch Gradient Norm: 0.6614690322308203
Epoch: 962, Batch Gradient Norm after: 0.6614690322308203
Epoch 963/10000, Prediction Accuracy = 59.04615384615385%, Loss = 0.010151457471343188
Epoch: 963, Batch Gradient Norm: 0.6479950647669549
Epoch: 963, Batch Gradient Norm after: 0.6479950647669549
Epoch 964/10000, Prediction Accuracy = 59.24999999999999%, Loss = 0.010138896365578357
Epoch: 964, Batch Gradient Norm: 0.638318812526094
Epoch: 964, Batch Gradient Norm after: 0.638318812526094
Epoch 965/10000, Prediction Accuracy = 59.58846153846154%, Loss = 0.010079191639446296
Epoch: 965, Batch Gradient Norm: 0.6462091722284029
Epoch: 965, Batch Gradient Norm after: 0.6462091722284029
Epoch 966/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.0100867309822486
Epoch: 966, Batch Gradient Norm: 0.6414183857601942
Epoch: 966, Batch Gradient Norm after: 0.6414183857601942
Epoch 967/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010040078741999773
Epoch: 967, Batch Gradient Norm: 0.6310123877172074
Epoch: 967, Batch Gradient Norm after: 0.6310123877172074
Epoch 968/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.010067639012749378
Epoch: 968, Batch Gradient Norm: 0.6401293673680774
Epoch: 968, Batch Gradient Norm after: 0.6401293673680774
Epoch 969/10000, Prediction Accuracy = 59.59615384615384%, Loss = 0.010148400942293497
Epoch: 969, Batch Gradient Norm: 0.6247529419262747
Epoch: 969, Batch Gradient Norm after: 0.6247529419262747
Epoch 970/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010034388575989466
Epoch: 970, Batch Gradient Norm: 0.6491934633216052
Epoch: 970, Batch Gradient Norm after: 0.6491934633216052
Epoch 971/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010055806989280077
Epoch: 971, Batch Gradient Norm: 0.624263390471885
Epoch: 971, Batch Gradient Norm after: 0.624263390471885
Epoch 972/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010100090446380468
Epoch: 972, Batch Gradient Norm: 0.6427025324625918
Epoch: 972, Batch Gradient Norm after: 0.6427025324625918
Epoch 973/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.010124365655848613
Epoch: 973, Batch Gradient Norm: 0.647195244672156
Epoch: 973, Batch Gradient Norm after: 0.647195244672156
Epoch 974/10000, Prediction Accuracy = 59.45%, Loss = 0.010130601791808238
Epoch: 974, Batch Gradient Norm: 0.6356384127943266
Epoch: 974, Batch Gradient Norm after: 0.6356384127943266
Epoch 975/10000, Prediction Accuracy = 59.565384615384616%, Loss = 0.010105733974621845
Epoch: 975, Batch Gradient Norm: 0.6360466981254752
Epoch: 975, Batch Gradient Norm after: 0.6360466981254752
Epoch 976/10000, Prediction Accuracy = 59.23846153846153%, Loss = 0.010107801367457096
Epoch: 976, Batch Gradient Norm: 0.6549703766977144
Epoch: 976, Batch Gradient Norm after: 0.6549703766977144
Epoch 977/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010121502698614048
Epoch: 977, Batch Gradient Norm: 0.6541976523763121
Epoch: 977, Batch Gradient Norm after: 0.6541976523763121
Epoch 978/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010125455518181507
Epoch: 978, Batch Gradient Norm: 0.6291354953469906
Epoch: 978, Batch Gradient Norm after: 0.6291354953469906
Epoch 979/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.01010156637774064
Epoch: 979, Batch Gradient Norm: 0.6453739683372841
Epoch: 979, Batch Gradient Norm after: 0.6453739683372841
Epoch 980/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010082913920856439
Epoch: 980, Batch Gradient Norm: 0.6348036578959629
Epoch: 980, Batch Gradient Norm after: 0.6348036578959629
Epoch 981/10000, Prediction Accuracy = 59.5%, Loss = 0.010047341911838604
Epoch: 981, Batch Gradient Norm: 0.6520729091718424
Epoch: 981, Batch Gradient Norm after: 0.6520729091718424
Epoch 982/10000, Prediction Accuracy = 59.24615384615385%, Loss = 0.010151300937510453
Epoch: 982, Batch Gradient Norm: 0.6369210444118222
Epoch: 982, Batch Gradient Norm after: 0.6369210444118222
Epoch 983/10000, Prediction Accuracy = 59.3346153846154%, Loss = 0.010117053125913326
Epoch: 983, Batch Gradient Norm: 0.6404382441706749
Epoch: 983, Batch Gradient Norm after: 0.6404382441706749
Epoch 984/10000, Prediction Accuracy = 59.580769230769235%, Loss = 0.01009719568089797
Epoch: 984, Batch Gradient Norm: 0.6229476308671397
Epoch: 984, Batch Gradient Norm after: 0.6229476308671397
Epoch 985/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010079587093339516
Epoch: 985, Batch Gradient Norm: 0.6359679266204019
Epoch: 985, Batch Gradient Norm after: 0.6359679266204019
Epoch 986/10000, Prediction Accuracy = 59.38461538461537%, Loss = 0.010077252267644955
Epoch: 986, Batch Gradient Norm: 0.6379825084439252
Epoch: 986, Batch Gradient Norm after: 0.6379825084439252
Epoch 987/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.01010406891313883
Epoch: 987, Batch Gradient Norm: 0.6289993193839065
Epoch: 987, Batch Gradient Norm after: 0.6289993193839065
Epoch 988/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010080648156312795
Epoch: 988, Batch Gradient Norm: 0.6487565339738258
Epoch: 988, Batch Gradient Norm after: 0.6487565339738258
Epoch 989/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010078362833995085
Epoch: 989, Batch Gradient Norm: 0.6308102612484906
Epoch: 989, Batch Gradient Norm after: 0.6308102612484906
Epoch 990/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010053655777413111
Epoch: 990, Batch Gradient Norm: 0.6430745558858899
Epoch: 990, Batch Gradient Norm after: 0.6430745558858899
Epoch 991/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010074657961153068
Epoch: 991, Batch Gradient Norm: 0.6468149902956102
Epoch: 991, Batch Gradient Norm after: 0.6468149902956102
Epoch 992/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010104310842087636
Epoch: 992, Batch Gradient Norm: 0.6437647953336114
Epoch: 992, Batch Gradient Norm after: 0.6437647953336114
Epoch 993/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.010101422810783753
Epoch: 993, Batch Gradient Norm: 0.6501063805295839
Epoch: 993, Batch Gradient Norm after: 0.6501063805295839
Epoch 994/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010119067719922615
Epoch: 994, Batch Gradient Norm: 0.6361431989328689
Epoch: 994, Batch Gradient Norm after: 0.6361431989328689
Epoch 995/10000, Prediction Accuracy = 59.61538461538463%, Loss = 0.01007150486111641
Epoch: 995, Batch Gradient Norm: 0.6376900608319844
Epoch: 995, Batch Gradient Norm after: 0.6376900608319844
Epoch 996/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010096610165559329
Epoch: 996, Batch Gradient Norm: 0.6326901992064502
Epoch: 996, Batch Gradient Norm after: 0.6326901992064502
Epoch 997/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.01009100775879163
Epoch: 997, Batch Gradient Norm: 0.649428321312511
Epoch: 997, Batch Gradient Norm after: 0.649428321312511
Epoch 998/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.010126200576241199
Epoch: 998, Batch Gradient Norm: 0.6539625623896974
Epoch: 998, Batch Gradient Norm after: 0.6539625623896974
Epoch 999/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010124775007940255
Epoch: 999, Batch Gradient Norm: 0.6304237736324108
Epoch: 999, Batch Gradient Norm after: 0.6304237736324108
Epoch 1000/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010064322358140579
Epoch: 1000, Batch Gradient Norm: 0.6483653321715585
Epoch: 1000, Batch Gradient Norm after: 0.6483653321715585
Epoch 1001/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010068885767115997
Epoch: 1001, Batch Gradient Norm: 0.6408563975491203
Epoch: 1001, Batch Gradient Norm after: 0.6408563975491203
Epoch 1002/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010037506500688883
Epoch: 1002, Batch Gradient Norm: 0.6550193763676807
Epoch: 1002, Batch Gradient Norm after: 0.6550193763676807
Epoch 1003/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010065955897936454
Epoch: 1003, Batch Gradient Norm: 0.6435393091632817
Epoch: 1003, Batch Gradient Norm after: 0.6435393091632817
Epoch 1004/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010068222665442871
Epoch: 1004, Batch Gradient Norm: 0.6372791243765791
Epoch: 1004, Batch Gradient Norm after: 0.6372791243765791
Epoch 1005/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010079096357982893
Epoch: 1005, Batch Gradient Norm: 0.6421895957561548
Epoch: 1005, Batch Gradient Norm after: 0.6421895957561548
Epoch 1006/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010097531458506217
Epoch: 1006, Batch Gradient Norm: 0.6364380544041233
Epoch: 1006, Batch Gradient Norm after: 0.6364380544041233
Epoch 1007/10000, Prediction Accuracy = 59.192307692307686%, Loss = 0.01011114388417739
Epoch: 1007, Batch Gradient Norm: 0.6433593164118064
Epoch: 1007, Batch Gradient Norm after: 0.6433593164118064
Epoch 1008/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010081555121220075
Epoch: 1008, Batch Gradient Norm: 0.6547336597887504
Epoch: 1008, Batch Gradient Norm after: 0.6547336597887504
Epoch 1009/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.010091311083390163
Epoch: 1009, Batch Gradient Norm: 0.6515126652300838
Epoch: 1009, Batch Gradient Norm after: 0.6515126652300838
Epoch 1010/10000, Prediction Accuracy = 59.35000000000001%, Loss = 0.010134212386149626
Epoch: 1010, Batch Gradient Norm: 0.6385853449611082
Epoch: 1010, Batch Gradient Norm after: 0.6385853449611082
Epoch 1011/10000, Prediction Accuracy = 59.21923076923076%, Loss = 0.010094095737888263
Epoch: 1011, Batch Gradient Norm: 0.649374791827178
Epoch: 1011, Batch Gradient Norm after: 0.649374791827178
Epoch 1012/10000, Prediction Accuracy = 59.21538461538462%, Loss = 0.010089279224093143
Epoch: 1012, Batch Gradient Norm: 0.6408845790800785
Epoch: 1012, Batch Gradient Norm after: 0.6408845790800785
Epoch 1013/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010111891378003817
Epoch: 1013, Batch Gradient Norm: 0.6451829929777682
Epoch: 1013, Batch Gradient Norm after: 0.6451829929777682
Epoch 1014/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010075870184944226
Epoch: 1014, Batch Gradient Norm: 0.6299792044858726
Epoch: 1014, Batch Gradient Norm after: 0.6299792044858726
Epoch 1015/10000, Prediction Accuracy = 59.56153846153845%, Loss = 0.010072875624665847
Epoch: 1015, Batch Gradient Norm: 0.6345951581796933
Epoch: 1015, Batch Gradient Norm after: 0.6345951581796933
Epoch 1016/10000, Prediction Accuracy = 59.45000000000001%, Loss = 0.010137540789750906
Epoch: 1016, Batch Gradient Norm: 0.6517604827413078
Epoch: 1016, Batch Gradient Norm after: 0.6517604827413078
Epoch 1017/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010122987155157786
Epoch: 1017, Batch Gradient Norm: 0.6251153543971077
Epoch: 1017, Batch Gradient Norm after: 0.6251153543971077
Epoch 1018/10000, Prediction Accuracy = 59.449999999999996%, Loss = 0.010093626781151844
Epoch: 1018, Batch Gradient Norm: 0.6445017482944351
Epoch: 1018, Batch Gradient Norm after: 0.6445017482944351
Epoch 1019/10000, Prediction Accuracy = 59.24615384615384%, Loss = 0.010104847283890614
Epoch: 1019, Batch Gradient Norm: 0.6287470765932535
Epoch: 1019, Batch Gradient Norm after: 0.6287470765932535
Epoch 1020/10000, Prediction Accuracy = 59.4923076923077%, Loss = 0.010075982015293378
Epoch: 1020, Batch Gradient Norm: 0.6556004654626226
Epoch: 1020, Batch Gradient Norm after: 0.6556004654626226
Epoch 1021/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.01012180459040862
Epoch: 1021, Batch Gradient Norm: 0.6359201574889299
Epoch: 1021, Batch Gradient Norm after: 0.6359201574889299
Epoch 1022/10000, Prediction Accuracy = 59.52307692307693%, Loss = 0.010098807298793243
Epoch: 1022, Batch Gradient Norm: 0.644500976054261
Epoch: 1022, Batch Gradient Norm after: 0.644500976054261
Epoch 1023/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010104275380189601
Epoch: 1023, Batch Gradient Norm: 0.6402805236124853
Epoch: 1023, Batch Gradient Norm after: 0.6402805236124853
Epoch 1024/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010049413961286727
Epoch: 1024, Batch Gradient Norm: 0.6397911050404761
Epoch: 1024, Batch Gradient Norm after: 0.6397911050404761
Epoch 1025/10000, Prediction Accuracy = 59.49230769230768%, Loss = 0.010079263852765927
Epoch: 1025, Batch Gradient Norm: 0.6379824305233511
Epoch: 1025, Batch Gradient Norm after: 0.6379824305233511
Epoch 1026/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.010067503111293683
Epoch: 1026, Batch Gradient Norm: 0.6350241131156157
Epoch: 1026, Batch Gradient Norm after: 0.6350241131156157
Epoch 1027/10000, Prediction Accuracy = 59.588461538461544%, Loss = 0.010089935878148446
Epoch: 1027, Batch Gradient Norm: 0.6326354890230812
Epoch: 1027, Batch Gradient Norm after: 0.6326354890230812
Epoch 1028/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010107001361365501
Epoch: 1028, Batch Gradient Norm: 0.6402097689016485
Epoch: 1028, Batch Gradient Norm after: 0.6402097689016485
Epoch 1029/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010128290249178043
Epoch: 1029, Batch Gradient Norm: 0.6470845068128356
Epoch: 1029, Batch Gradient Norm after: 0.6470845068128356
Epoch 1030/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010091501216475781
Epoch: 1030, Batch Gradient Norm: 0.6323410449780985
Epoch: 1030, Batch Gradient Norm after: 0.6323410449780985
Epoch 1031/10000, Prediction Accuracy = 59.626923076923084%, Loss = 0.010071584954857826
Epoch: 1031, Batch Gradient Norm: 0.6450084900523307
Epoch: 1031, Batch Gradient Norm after: 0.6450084900523307
Epoch 1032/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010097725030321341
Epoch: 1032, Batch Gradient Norm: 0.6421608124031166
Epoch: 1032, Batch Gradient Norm after: 0.6421608124031166
Epoch 1033/10000, Prediction Accuracy = 59.7%, Loss = 0.010073821633481063
Epoch: 1033, Batch Gradient Norm: 0.6431350176265943
Epoch: 1033, Batch Gradient Norm after: 0.6431350176265943
Epoch 1034/10000, Prediction Accuracy = 59.4923076923077%, Loss = 0.010074339233911954
Epoch: 1034, Batch Gradient Norm: 0.6322210933404268
Epoch: 1034, Batch Gradient Norm after: 0.6322210933404268
Epoch 1035/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010107704509909336
Epoch: 1035, Batch Gradient Norm: 0.6326146596265824
Epoch: 1035, Batch Gradient Norm after: 0.6326146596265824
Epoch 1036/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.01008763902175885
Epoch: 1036, Batch Gradient Norm: 0.6422306100255218
Epoch: 1036, Batch Gradient Norm after: 0.6422306100255218
Epoch 1037/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010084774560080124
Epoch: 1037, Batch Gradient Norm: 0.6313808835388048
Epoch: 1037, Batch Gradient Norm after: 0.6313808835388048
Epoch 1038/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.010066678604254356
Epoch: 1038, Batch Gradient Norm: 0.6247615174632906
Epoch: 1038, Batch Gradient Norm after: 0.6247615174632906
Epoch 1039/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010090837040199684
Epoch: 1039, Batch Gradient Norm: 0.6357559969833241
Epoch: 1039, Batch Gradient Norm after: 0.6357559969833241
Epoch 1040/10000, Prediction Accuracy = 59.5923076923077%, Loss = 0.010064216688848458
Epoch: 1040, Batch Gradient Norm: 0.639337639502424
Epoch: 1040, Batch Gradient Norm after: 0.639337639502424
Epoch 1041/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010076696554628702
Epoch: 1041, Batch Gradient Norm: 0.6295737480977418
Epoch: 1041, Batch Gradient Norm after: 0.6295737480977418
Epoch 1042/10000, Prediction Accuracy = 59.376923076923084%, Loss = 0.010081681064688243
Epoch: 1042, Batch Gradient Norm: 0.6417044045305367
Epoch: 1042, Batch Gradient Norm after: 0.6417044045305367
Epoch 1043/10000, Prediction Accuracy = 59.3%, Loss = 0.010092900851025032
Epoch: 1043, Batch Gradient Norm: 0.6510257868438883
Epoch: 1043, Batch Gradient Norm after: 0.6510257868438883
Epoch 1044/10000, Prediction Accuracy = 59.3076923076923%, Loss = 0.010095772548363758
Epoch: 1044, Batch Gradient Norm: 0.6529567576371029
Epoch: 1044, Batch Gradient Norm after: 0.6529567576371029
Epoch 1045/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010123809512991171
Epoch: 1045, Batch Gradient Norm: 0.6563387028197601
Epoch: 1045, Batch Gradient Norm after: 0.6563387028197601
Epoch 1046/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.010077200829982758
Epoch: 1046, Batch Gradient Norm: 0.6403656479661336
Epoch: 1046, Batch Gradient Norm after: 0.6403656479661336
Epoch 1047/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.01006107354679933
Epoch: 1047, Batch Gradient Norm: 0.6457593744250022
Epoch: 1047, Batch Gradient Norm after: 0.6457593744250022
Epoch 1048/10000, Prediction Accuracy = 59.215384615384615%, Loss = 0.010102192083230386
Epoch: 1048, Batch Gradient Norm: 0.63841477550458
Epoch: 1048, Batch Gradient Norm after: 0.63841477550458
Epoch 1049/10000, Prediction Accuracy = 59.673076923076934%, Loss = 0.010043543190337144
Epoch: 1049, Batch Gradient Norm: 0.6485981200301614
Epoch: 1049, Batch Gradient Norm after: 0.6485981200301614
Epoch 1050/10000, Prediction Accuracy = 59.43846153846153%, Loss = 0.010152703079466637
Epoch: 1050, Batch Gradient Norm: 0.646423956671065
Epoch: 1050, Batch Gradient Norm after: 0.646423956671065
Epoch 1051/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.01008225160722549
Epoch: 1051, Batch Gradient Norm: 0.6416670079829884
Epoch: 1051, Batch Gradient Norm after: 0.6416670079829884
Epoch 1052/10000, Prediction Accuracy = 59.19615384615385%, Loss = 0.010102657028115712
Epoch: 1052, Batch Gradient Norm: 0.6213805027057917
Epoch: 1052, Batch Gradient Norm after: 0.6213805027057917
Epoch 1053/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.010074064063911255
Epoch: 1053, Batch Gradient Norm: 0.6549532465838532
Epoch: 1053, Batch Gradient Norm after: 0.6549532465838532
Epoch 1054/10000, Prediction Accuracy = 59.17307692307692%, Loss = 0.010078843826284776
Epoch: 1054, Batch Gradient Norm: 0.6238514134007954
Epoch: 1054, Batch Gradient Norm after: 0.6238514134007954
Epoch 1055/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010064674469713982
Epoch: 1055, Batch Gradient Norm: 0.6401088797897025
Epoch: 1055, Batch Gradient Norm after: 0.6401088797897025
Epoch 1056/10000, Prediction Accuracy = 59.21153846153845%, Loss = 0.01009975359416925
Epoch: 1056, Batch Gradient Norm: 0.631657833979141
Epoch: 1056, Batch Gradient Norm after: 0.631657833979141
Epoch 1057/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010088232704080068
Epoch: 1057, Batch Gradient Norm: 0.6415508635941636
Epoch: 1057, Batch Gradient Norm after: 0.6415508635941636
Epoch 1058/10000, Prediction Accuracy = 59.411538461538456%, Loss = 0.01007181377365039
Epoch: 1058, Batch Gradient Norm: 0.6416574728500044
Epoch: 1058, Batch Gradient Norm after: 0.6416574728500044
Epoch 1059/10000, Prediction Accuracy = 59.50384615384615%, Loss = 0.01009152141901163
Epoch: 1059, Batch Gradient Norm: 0.6508909740290825
Epoch: 1059, Batch Gradient Norm after: 0.6508909740290825
Epoch 1060/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.010121636737424593
Epoch: 1060, Batch Gradient Norm: 0.6489409017576213
Epoch: 1060, Batch Gradient Norm after: 0.6489409017576213
Epoch 1061/10000, Prediction Accuracy = 59.5%, Loss = 0.010063289091564141
Epoch: 1061, Batch Gradient Norm: 0.622695423733411
Epoch: 1061, Batch Gradient Norm after: 0.622695423733411
Epoch 1062/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.010064802848948883
Epoch: 1062, Batch Gradient Norm: 0.6297414028423799
Epoch: 1062, Batch Gradient Norm after: 0.6297414028423799
Epoch 1063/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010036165181260843
Epoch: 1063, Batch Gradient Norm: 0.6333090794035434
Epoch: 1063, Batch Gradient Norm after: 0.6333090794035434
Epoch 1064/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010087761239936719
Epoch: 1064, Batch Gradient Norm: 0.6330800823446274
Epoch: 1064, Batch Gradient Norm after: 0.6330800823446274
Epoch 1065/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.01006388384848833
Epoch: 1065, Batch Gradient Norm: 0.6412621603632953
Epoch: 1065, Batch Gradient Norm after: 0.6412621603632953
Epoch 1066/10000, Prediction Accuracy = 59.51538461538461%, Loss = 0.010115004072968777
Epoch: 1066, Batch Gradient Norm: 0.6368867003066448
Epoch: 1066, Batch Gradient Norm after: 0.6368867003066448
Epoch 1067/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010118312274034206
Epoch: 1067, Batch Gradient Norm: 0.6425545154002879
Epoch: 1067, Batch Gradient Norm after: 0.6425545154002879
Epoch 1068/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010093356697605206
Epoch: 1068, Batch Gradient Norm: 0.6369082475652904
Epoch: 1068, Batch Gradient Norm after: 0.6369082475652904
Epoch 1069/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010142839728639675
Epoch: 1069, Batch Gradient Norm: 0.6423019326997869
Epoch: 1069, Batch Gradient Norm after: 0.6423019326997869
Epoch 1070/10000, Prediction Accuracy = 59.43846153846155%, Loss = 0.010097599516694363
Epoch: 1070, Batch Gradient Norm: 0.636379616546958
Epoch: 1070, Batch Gradient Norm after: 0.636379616546958
Epoch 1071/10000, Prediction Accuracy = 59.307692307692314%, Loss = 0.010119988726308713
Epoch: 1071, Batch Gradient Norm: 0.648037956503035
Epoch: 1071, Batch Gradient Norm after: 0.648037956503035
Epoch 1072/10000, Prediction Accuracy = 59.24615384615386%, Loss = 0.010123690518622216
Epoch: 1072, Batch Gradient Norm: 0.6399687312054008
Epoch: 1072, Batch Gradient Norm after: 0.6399687312054008
Epoch 1073/10000, Prediction Accuracy = 59.638461538461534%, Loss = 0.010082789911673619
Epoch: 1073, Batch Gradient Norm: 0.6349458549592071
Epoch: 1073, Batch Gradient Norm after: 0.6349458549592071
Epoch 1074/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010098209103139548
Epoch: 1074, Batch Gradient Norm: 0.6284606622072918
Epoch: 1074, Batch Gradient Norm after: 0.6284606622072918
Epoch 1075/10000, Prediction Accuracy = 59.48076923076922%, Loss = 0.010068567039874883
Epoch: 1075, Batch Gradient Norm: 0.6515431403656631
Epoch: 1075, Batch Gradient Norm after: 0.6515431403656631
Epoch 1076/10000, Prediction Accuracy = 59.41153846153847%, Loss = 0.01010811823205306
Epoch: 1076, Batch Gradient Norm: 0.6422149840393723
Epoch: 1076, Batch Gradient Norm after: 0.6422149840393723
Epoch 1077/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010129995572452363
Epoch: 1077, Batch Gradient Norm: 0.6456165169937658
Epoch: 1077, Batch Gradient Norm after: 0.6456165169937658
Epoch 1078/10000, Prediction Accuracy = 59.150000000000006%, Loss = 0.010078290047553869
Epoch: 1078, Batch Gradient Norm: 0.648848433041932
Epoch: 1078, Batch Gradient Norm after: 0.648848433041932
Epoch 1079/10000, Prediction Accuracy = 59.52307692307693%, Loss = 0.010081339914065141
Epoch: 1079, Batch Gradient Norm: 0.635709692828966
Epoch: 1079, Batch Gradient Norm after: 0.635709692828966
Epoch 1080/10000, Prediction Accuracy = 59.66538461538461%, Loss = 0.01004624323776135
Epoch: 1080, Batch Gradient Norm: 0.6425861409151052
Epoch: 1080, Batch Gradient Norm after: 0.6425861409151052
Epoch 1081/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010103590284975676
Epoch: 1081, Batch Gradient Norm: 0.6293426706079697
Epoch: 1081, Batch Gradient Norm after: 0.6293426706079697
Epoch 1082/10000, Prediction Accuracy = 59.3%, Loss = 0.010137546377686353
Epoch: 1082, Batch Gradient Norm: 0.629326451928195
Epoch: 1082, Batch Gradient Norm after: 0.629326451928195
Epoch 1083/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010091158131567331
Epoch: 1083, Batch Gradient Norm: 0.6352795353413457
Epoch: 1083, Batch Gradient Norm after: 0.6352795353413457
Epoch 1084/10000, Prediction Accuracy = 59.55384615384616%, Loss = 0.010108826037209768
Epoch: 1084, Batch Gradient Norm: 0.628475685756871
Epoch: 1084, Batch Gradient Norm after: 0.628475685756871
Epoch 1085/10000, Prediction Accuracy = 59.526923076923076%, Loss = 0.010066302923055796
Epoch: 1085, Batch Gradient Norm: 0.6503616663459443
Epoch: 1085, Batch Gradient Norm after: 0.6503616663459443
Epoch 1086/10000, Prediction Accuracy = 59.50000000000001%, Loss = 0.0100809635164646
Epoch: 1086, Batch Gradient Norm: 0.6323627804778362
Epoch: 1086, Batch Gradient Norm after: 0.6323627804778362
Epoch 1087/10000, Prediction Accuracy = 59.22307692307691%, Loss = 0.010071979334148077
Epoch: 1087, Batch Gradient Norm: 0.6376180187330812
Epoch: 1087, Batch Gradient Norm after: 0.6376180187330812
Epoch 1088/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010068224169887029
Epoch: 1088, Batch Gradient Norm: 0.6361117533946529
Epoch: 1088, Batch Gradient Norm after: 0.6361117533946529
Epoch 1089/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010119761197039714
Epoch: 1089, Batch Gradient Norm: 0.6494082707270975
Epoch: 1089, Batch Gradient Norm after: 0.6494082707270975
Epoch 1090/10000, Prediction Accuracy = 59.3423076923077%, Loss = 0.01010715624747368
Epoch: 1090, Batch Gradient Norm: 0.6500741924828115
Epoch: 1090, Batch Gradient Norm after: 0.6500741924828115
Epoch 1091/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010085368815522928
Epoch: 1091, Batch Gradient Norm: 0.6360022275933812
Epoch: 1091, Batch Gradient Norm after: 0.6360022275933812
Epoch 1092/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010061630692619544
Epoch: 1092, Batch Gradient Norm: 0.6313862517914133
Epoch: 1092, Batch Gradient Norm after: 0.6313862517914133
Epoch 1093/10000, Prediction Accuracy = 59.646153846153844%, Loss = 0.01006351182093987
Epoch: 1093, Batch Gradient Norm: 0.6349999769141381
Epoch: 1093, Batch Gradient Norm after: 0.6349999769141381
Epoch 1094/10000, Prediction Accuracy = 59.50384615384615%, Loss = 0.010089862518585645
Epoch: 1094, Batch Gradient Norm: 0.637918414765422
Epoch: 1094, Batch Gradient Norm after: 0.637918414765422
Epoch 1095/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.010083734559325071
Epoch: 1095, Batch Gradient Norm: 0.6478593397950042
Epoch: 1095, Batch Gradient Norm after: 0.6478593397950042
Epoch 1096/10000, Prediction Accuracy = 59.20384615384615%, Loss = 0.010135607292445807
Epoch: 1096, Batch Gradient Norm: 0.6313531848661418
Epoch: 1096, Batch Gradient Norm after: 0.6313531848661418
Epoch 1097/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010088003813647307
Epoch: 1097, Batch Gradient Norm: 0.6300778537834729
Epoch: 1097, Batch Gradient Norm after: 0.6300778537834729
Epoch 1098/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.010077510172357926
Epoch: 1098, Batch Gradient Norm: 0.6516196465758529
Epoch: 1098, Batch Gradient Norm after: 0.6516196465758529
Epoch 1099/10000, Prediction Accuracy = 59.5%, Loss = 0.010134188673244072
Epoch: 1099, Batch Gradient Norm: 0.6460377328077654
Epoch: 1099, Batch Gradient Norm after: 0.6460377328077654
Epoch 1100/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010152320520809064
Epoch: 1100, Batch Gradient Norm: 0.6377144098963059
Epoch: 1100, Batch Gradient Norm after: 0.6377144098963059
Epoch 1101/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.01010022427027042
Epoch: 1101, Batch Gradient Norm: 0.6588809477618258
Epoch: 1101, Batch Gradient Norm after: 0.6588809477618258
Epoch 1102/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010104200014701257
Epoch: 1102, Batch Gradient Norm: 0.6442047557342383
Epoch: 1102, Batch Gradient Norm after: 0.6442047557342383
Epoch 1103/10000, Prediction Accuracy = 59.27692307692307%, Loss = 0.010098598324335538
Epoch: 1103, Batch Gradient Norm: 0.6356642820242947
Epoch: 1103, Batch Gradient Norm after: 0.6356642820242947
Epoch 1104/10000, Prediction Accuracy = 59.46923076923076%, Loss = 0.01008489068884116
Epoch: 1104, Batch Gradient Norm: 0.6335086721599956
Epoch: 1104, Batch Gradient Norm after: 0.6335086721599956
Epoch 1105/10000, Prediction Accuracy = 59.43846153846153%, Loss = 0.010038450718499146
Epoch: 1105, Batch Gradient Norm: 0.6249781324998508
Epoch: 1105, Batch Gradient Norm after: 0.6249781324998508
Epoch 1106/10000, Prediction Accuracy = 59.50384615384615%, Loss = 0.010077789497490112
Epoch: 1106, Batch Gradient Norm: 0.6313479731371161
Epoch: 1106, Batch Gradient Norm after: 0.6313479731371161
Epoch 1107/10000, Prediction Accuracy = 59.715384615384615%, Loss = 0.010055507461612042
Epoch: 1107, Batch Gradient Norm: 0.6498998350478294
Epoch: 1107, Batch Gradient Norm after: 0.6498998350478294
Epoch 1108/10000, Prediction Accuracy = 59.50384615384616%, Loss = 0.010087498965171667
Epoch: 1108, Batch Gradient Norm: 0.6475969674306226
Epoch: 1108, Batch Gradient Norm after: 0.6475969674306226
Epoch 1109/10000, Prediction Accuracy = 59.4923076923077%, Loss = 0.010116830396537598
Epoch: 1109, Batch Gradient Norm: 0.6444143711496461
Epoch: 1109, Batch Gradient Norm after: 0.6444143711496461
Epoch 1110/10000, Prediction Accuracy = 59.28846153846155%, Loss = 0.010096558298055943
Epoch: 1110, Batch Gradient Norm: 0.6402304117099514
Epoch: 1110, Batch Gradient Norm after: 0.6402304117099514
Epoch 1111/10000, Prediction Accuracy = 59.54615384615384%, Loss = 0.010068143359743632
Epoch: 1111, Batch Gradient Norm: 0.6293563533922256
Epoch: 1111, Batch Gradient Norm after: 0.6293563533922256
Epoch 1112/10000, Prediction Accuracy = 59.3346153846154%, Loss = 0.010125445201992989
Epoch: 1112, Batch Gradient Norm: 0.6311299075945402
Epoch: 1112, Batch Gradient Norm after: 0.6311299075945402
Epoch 1113/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.01009926930643045
Epoch: 1113, Batch Gradient Norm: 0.6341855231588788
Epoch: 1113, Batch Gradient Norm after: 0.6341855231588788
Epoch 1114/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010089478670404507
Epoch: 1114, Batch Gradient Norm: 0.6303803364032173
Epoch: 1114, Batch Gradient Norm after: 0.6303803364032173
Epoch 1115/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010079005303291174
Epoch: 1115, Batch Gradient Norm: 0.6508845311671863
Epoch: 1115, Batch Gradient Norm after: 0.6508845311671863
Epoch 1116/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010109256809720626
Epoch: 1116, Batch Gradient Norm: 0.6377740351854527
Epoch: 1116, Batch Gradient Norm after: 0.6377740351854527
Epoch 1117/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010080186291955985
Epoch: 1117, Batch Gradient Norm: 0.637067660411509
Epoch: 1117, Batch Gradient Norm after: 0.637067660411509
Epoch 1118/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010073292355697889
Epoch: 1118, Batch Gradient Norm: 0.6546421667678335
Epoch: 1118, Batch Gradient Norm after: 0.6546421667678335
Epoch 1119/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010118239917434178
Epoch: 1119, Batch Gradient Norm: 0.6468309292191504
Epoch: 1119, Batch Gradient Norm after: 0.6468309292191504
Epoch 1120/10000, Prediction Accuracy = 59.49230769230768%, Loss = 0.010094565984148245
Epoch: 1120, Batch Gradient Norm: 0.6419531699030061
Epoch: 1120, Batch Gradient Norm after: 0.6419531699030061
Epoch 1121/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.01010873713172399
Epoch: 1121, Batch Gradient Norm: 0.6512027412329607
Epoch: 1121, Batch Gradient Norm after: 0.6512027412329607
Epoch 1122/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.010116072944723643
Epoch: 1122, Batch Gradient Norm: 0.6428719929920673
Epoch: 1122, Batch Gradient Norm after: 0.6428719929920673
Epoch 1123/10000, Prediction Accuracy = 59.58461538461539%, Loss = 0.010107831384700078
Epoch: 1123, Batch Gradient Norm: 0.6373758191535617
Epoch: 1123, Batch Gradient Norm after: 0.6373758191535617
Epoch 1124/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010073995790802516
Epoch: 1124, Batch Gradient Norm: 0.6311348325327975
Epoch: 1124, Batch Gradient Norm after: 0.6311348325327975
Epoch 1125/10000, Prediction Accuracy = 59.51923076923078%, Loss = 0.010071642983418245
Epoch: 1125, Batch Gradient Norm: 0.6651236449277442
Epoch: 1125, Batch Gradient Norm after: 0.6651236449277442
Epoch 1126/10000, Prediction Accuracy = 59.48461538461539%, Loss = 0.010103940677184325
Epoch: 1126, Batch Gradient Norm: 0.6504989167025181
Epoch: 1126, Batch Gradient Norm after: 0.6504989167025181
Epoch 1127/10000, Prediction Accuracy = 59.45%, Loss = 0.010127728016903767
Epoch: 1127, Batch Gradient Norm: 0.644634913863968
Epoch: 1127, Batch Gradient Norm after: 0.644634913863968
Epoch 1128/10000, Prediction Accuracy = 59.18461538461539%, Loss = 0.010100574519198675
Epoch: 1128, Batch Gradient Norm: 0.6381860390545996
Epoch: 1128, Batch Gradient Norm after: 0.6381860390545996
Epoch 1129/10000, Prediction Accuracy = 59.23846153846154%, Loss = 0.010138624634307165
Epoch: 1129, Batch Gradient Norm: 0.6350461589641915
Epoch: 1129, Batch Gradient Norm after: 0.6350461589641915
Epoch 1130/10000, Prediction Accuracy = 59.565384615384616%, Loss = 0.01005555553218493
Epoch: 1130, Batch Gradient Norm: 0.6492389780720309
Epoch: 1130, Batch Gradient Norm after: 0.6492389780720309
Epoch 1131/10000, Prediction Accuracy = 59.3923076923077%, Loss = 0.010071867503798924
Epoch: 1131, Batch Gradient Norm: 0.6397027262174069
Epoch: 1131, Batch Gradient Norm after: 0.6397027262174069
Epoch 1132/10000, Prediction Accuracy = 59.68076923076923%, Loss = 0.01002772875989859
Epoch: 1132, Batch Gradient Norm: 0.6372343773018807
Epoch: 1132, Batch Gradient Norm after: 0.6372343773018807
Epoch 1133/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.010096677793906285
Epoch: 1133, Batch Gradient Norm: 0.6293604867381227
Epoch: 1133, Batch Gradient Norm after: 0.6293604867381227
Epoch 1134/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.010117430669757036
Epoch: 1134, Batch Gradient Norm: 0.6538624460071235
Epoch: 1134, Batch Gradient Norm after: 0.6538624460071235
Epoch 1135/10000, Prediction Accuracy = 59.176923076923075%, Loss = 0.010081921632473286
Epoch: 1135, Batch Gradient Norm: 0.6366799850996405
Epoch: 1135, Batch Gradient Norm after: 0.6366799850996405
Epoch 1136/10000, Prediction Accuracy = 59.17692307692308%, Loss = 0.010117744740385275
Epoch: 1136, Batch Gradient Norm: 0.6355676189959488
Epoch: 1136, Batch Gradient Norm after: 0.6355676189959488
Epoch 1137/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010104757447082263
Epoch: 1137, Batch Gradient Norm: 0.6384602878673596
Epoch: 1137, Batch Gradient Norm after: 0.6384602878673596
Epoch 1138/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.010094496421515942
Epoch: 1138, Batch Gradient Norm: 0.6597505821733876
Epoch: 1138, Batch Gradient Norm after: 0.6597505821733876
Epoch 1139/10000, Prediction Accuracy = 59.18846153846153%, Loss = 0.010130050019002877
Epoch: 1139, Batch Gradient Norm: 0.627453181731624
Epoch: 1139, Batch Gradient Norm after: 0.627453181731624
Epoch 1140/10000, Prediction Accuracy = 59.5%, Loss = 0.010048794846695203
Epoch: 1140, Batch Gradient Norm: 0.6378905541804988
Epoch: 1140, Batch Gradient Norm after: 0.6378905541804988
Epoch 1141/10000, Prediction Accuracy = 59.361538461538466%, Loss = 0.010066956066741394
Epoch: 1141, Batch Gradient Norm: 0.6295850292713132
Epoch: 1141, Batch Gradient Norm after: 0.6295850292713132
Epoch 1142/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010095753563711276
Epoch: 1142, Batch Gradient Norm: 0.6636507567503214
Epoch: 1142, Batch Gradient Norm after: 0.6636507567503214
Epoch 1143/10000, Prediction Accuracy = 59.35000000000001%, Loss = 0.010096547480386037
Epoch: 1143, Batch Gradient Norm: 0.6181752178278823
Epoch: 1143, Batch Gradient Norm after: 0.6181752178278823
Epoch 1144/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010079161908764105
Epoch: 1144, Batch Gradient Norm: 0.6351891183847297
Epoch: 1144, Batch Gradient Norm after: 0.6351891183847297
Epoch 1145/10000, Prediction Accuracy = 59.51538461538461%, Loss = 0.010109007501831422
Epoch: 1145, Batch Gradient Norm: 0.6545986069805099
Epoch: 1145, Batch Gradient Norm after: 0.6545986069805099
Epoch 1146/10000, Prediction Accuracy = 59.16538461538461%, Loss = 0.01014352969538707
Epoch: 1146, Batch Gradient Norm: 0.6348352358546101
Epoch: 1146, Batch Gradient Norm after: 0.6348352358546101
Epoch 1147/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.01008287315758375
Epoch: 1147, Batch Gradient Norm: 0.6474473554675003
Epoch: 1147, Batch Gradient Norm after: 0.6474473554675003
Epoch 1148/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.010107691256472697
Epoch: 1148, Batch Gradient Norm: 0.6456485053598058
Epoch: 1148, Batch Gradient Norm after: 0.6456485053598058
Epoch 1149/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010093728940074261
Epoch: 1149, Batch Gradient Norm: 0.6352651269603089
Epoch: 1149, Batch Gradient Norm after: 0.6352651269603089
Epoch 1150/10000, Prediction Accuracy = 59.669230769230765%, Loss = 0.010068725149791975
Epoch: 1150, Batch Gradient Norm: 0.6532649737926346
Epoch: 1150, Batch Gradient Norm after: 0.6532649737926346
Epoch 1151/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.010112408835154314
Epoch: 1151, Batch Gradient Norm: 0.6244179806727482
Epoch: 1151, Batch Gradient Norm after: 0.6244179806727482
Epoch 1152/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010052261730799308
Epoch: 1152, Batch Gradient Norm: 0.6505036289169834
Epoch: 1152, Batch Gradient Norm after: 0.6505036289169834
Epoch 1153/10000, Prediction Accuracy = 59.215384615384615%, Loss = 0.010116134268733172
Epoch: 1153, Batch Gradient Norm: 0.6341121653004459
Epoch: 1153, Batch Gradient Norm after: 0.6341121653004459
Epoch 1154/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010090368226743661
Epoch: 1154, Batch Gradient Norm: 0.6212259655609954
Epoch: 1154, Batch Gradient Norm after: 0.6212259655609954
Epoch 1155/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.010042842047718855
Epoch: 1155, Batch Gradient Norm: 0.6542697788255731
Epoch: 1155, Batch Gradient Norm after: 0.6542697788255731
Epoch 1156/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010057192224149521
Epoch: 1156, Batch Gradient Norm: 0.6320889145479419
Epoch: 1156, Batch Gradient Norm after: 0.6320889145479419
Epoch 1157/10000, Prediction Accuracy = 59.35000000000001%, Loss = 0.010089220192569952
Epoch: 1157, Batch Gradient Norm: 0.6326470868210619
Epoch: 1157, Batch Gradient Norm after: 0.6326470868210619
Epoch 1158/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010082211990195971
Epoch: 1158, Batch Gradient Norm: 0.6390515562385846
Epoch: 1158, Batch Gradient Norm after: 0.6390515562385846
Epoch 1159/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.01009583781258418
Epoch: 1159, Batch Gradient Norm: 0.6575786520599772
Epoch: 1159, Batch Gradient Norm after: 0.6575786520599772
Epoch 1160/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.01012781778207192
Epoch: 1160, Batch Gradient Norm: 0.6444809296483163
Epoch: 1160, Batch Gradient Norm after: 0.6444809296483163
Epoch 1161/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.01006850958443605
Epoch: 1161, Batch Gradient Norm: 0.6453487199265517
Epoch: 1161, Batch Gradient Norm after: 0.6453487199265517
Epoch 1162/10000, Prediction Accuracy = 59.43076923076924%, Loss = 0.010070062027527736
Epoch: 1162, Batch Gradient Norm: 0.6373834014527638
Epoch: 1162, Batch Gradient Norm after: 0.6373834014527638
Epoch 1163/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010093020346875373
Epoch: 1163, Batch Gradient Norm: 0.6444552398123699
Epoch: 1163, Batch Gradient Norm after: 0.6444552398123699
Epoch 1164/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010072865451757725
Epoch: 1164, Batch Gradient Norm: 0.6479284370751618
Epoch: 1164, Batch Gradient Norm after: 0.6479284370751618
Epoch 1165/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.010088125601983987
Epoch: 1165, Batch Gradient Norm: 0.6454978602444463
Epoch: 1165, Batch Gradient Norm after: 0.6454978602444463
Epoch 1166/10000, Prediction Accuracy = 59.31538461538462%, Loss = 0.01010649436368392
Epoch: 1166, Batch Gradient Norm: 0.6396210486000864
Epoch: 1166, Batch Gradient Norm after: 0.6396210486000864
Epoch 1167/10000, Prediction Accuracy = 59.4423076923077%, Loss = 0.010084617739686599
Epoch: 1167, Batch Gradient Norm: 0.6364740223262301
Epoch: 1167, Batch Gradient Norm after: 0.6364740223262301
Epoch 1168/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010038387388564073
Epoch: 1168, Batch Gradient Norm: 0.633642175462804
Epoch: 1168, Batch Gradient Norm after: 0.633642175462804
Epoch 1169/10000, Prediction Accuracy = 59.2%, Loss = 0.010088110987383585
Epoch: 1169, Batch Gradient Norm: 0.6509529589484538
Epoch: 1169, Batch Gradient Norm after: 0.6509529589484538
Epoch 1170/10000, Prediction Accuracy = 59.3423076923077%, Loss = 0.01008531623161756
Epoch: 1170, Batch Gradient Norm: 0.6528580942427544
Epoch: 1170, Batch Gradient Norm after: 0.6528580942427544
Epoch 1171/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.010060098523703905
Epoch: 1171, Batch Gradient Norm: 0.6255793805832742
Epoch: 1171, Batch Gradient Norm after: 0.6255793805832742
Epoch 1172/10000, Prediction Accuracy = 59.57307692307693%, Loss = 0.010138065697482
Epoch: 1172, Batch Gradient Norm: 0.6396727530180654
Epoch: 1172, Batch Gradient Norm after: 0.6396727530180654
Epoch 1173/10000, Prediction Accuracy = 59.26923076923076%, Loss = 0.010109834086436491
Epoch: 1173, Batch Gradient Norm: 0.6347857648669566
Epoch: 1173, Batch Gradient Norm after: 0.6347857648669566
Epoch 1174/10000, Prediction Accuracy = 59.30769230769231%, Loss = 0.010102500709203573
Epoch: 1174, Batch Gradient Norm: 0.6438408691597615
Epoch: 1174, Batch Gradient Norm after: 0.6438408691597615
Epoch 1175/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.010083826258778572
Epoch: 1175, Batch Gradient Norm: 0.6522135076745599
Epoch: 1175, Batch Gradient Norm after: 0.6522135076745599
Epoch 1176/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010065100084130581
Epoch: 1176, Batch Gradient Norm: 0.6351989807211467
Epoch: 1176, Batch Gradient Norm after: 0.6351989807211467
Epoch 1177/10000, Prediction Accuracy = 59.60769230769232%, Loss = 0.010095674043091444
Epoch: 1177, Batch Gradient Norm: 0.6335806608085719
Epoch: 1177, Batch Gradient Norm after: 0.6335806608085719
Epoch 1178/10000, Prediction Accuracy = 59.63076923076923%, Loss = 0.010060545343619127
Epoch: 1178, Batch Gradient Norm: 0.6439998930198693
Epoch: 1178, Batch Gradient Norm after: 0.6439998930198693
Epoch 1179/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010163497251386825
Epoch: 1179, Batch Gradient Norm: 0.6433057707716036
Epoch: 1179, Batch Gradient Norm after: 0.6433057707716036
Epoch 1180/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010091100389567705
Epoch: 1180, Batch Gradient Norm: 0.6493810978777415
Epoch: 1180, Batch Gradient Norm after: 0.6493810978777415
Epoch 1181/10000, Prediction Accuracy = 59.369230769230775%, Loss = 0.010164898533660632
Epoch: 1181, Batch Gradient Norm: 0.6450659055766006
Epoch: 1181, Batch Gradient Norm after: 0.6450659055766006
Epoch 1182/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.010084025275248747
Epoch: 1182, Batch Gradient Norm: 0.6412040107805606
Epoch: 1182, Batch Gradient Norm after: 0.6412040107805606
Epoch 1183/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.01006547397432419
Epoch: 1183, Batch Gradient Norm: 0.6379469076367991
Epoch: 1183, Batch Gradient Norm after: 0.6379469076367991
Epoch 1184/10000, Prediction Accuracy = 59.292307692307695%, Loss = 0.010083284945442127
Epoch: 1184, Batch Gradient Norm: 0.6388358282671893
Epoch: 1184, Batch Gradient Norm after: 0.6388358282671893
Epoch 1185/10000, Prediction Accuracy = 59.307692307692314%, Loss = 0.010110092277710255
Epoch: 1185, Batch Gradient Norm: 0.6468312667456725
Epoch: 1185, Batch Gradient Norm after: 0.6468312667456725
Epoch 1186/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010083372776324932
Epoch: 1186, Batch Gradient Norm: 0.6308028988827862
Epoch: 1186, Batch Gradient Norm after: 0.6308028988827862
Epoch 1187/10000, Prediction Accuracy = 59.56153846153845%, Loss = 0.010074853180692745
Epoch: 1187, Batch Gradient Norm: 0.6487589129657285
Epoch: 1187, Batch Gradient Norm after: 0.6487589129657285
Epoch 1188/10000, Prediction Accuracy = 59.39230769230768%, Loss = 0.01007391892087001
Epoch: 1188, Batch Gradient Norm: 0.634591731723185
Epoch: 1188, Batch Gradient Norm after: 0.634591731723185
Epoch 1189/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.010082589963880869
Epoch: 1189, Batch Gradient Norm: 0.6376055476369819
Epoch: 1189, Batch Gradient Norm after: 0.6376055476369819
Epoch 1190/10000, Prediction Accuracy = 59.52307692307693%, Loss = 0.010037936198596772
Epoch: 1190, Batch Gradient Norm: 0.6367475366253635
Epoch: 1190, Batch Gradient Norm after: 0.6367475366253635
Epoch 1191/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.0101111692448075
Epoch: 1191, Batch Gradient Norm: 0.6389152553281475
Epoch: 1191, Batch Gradient Norm after: 0.6389152553281475
Epoch 1192/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.01009888517168852
Epoch: 1192, Batch Gradient Norm: 0.6363315997307268
Epoch: 1192, Batch Gradient Norm after: 0.6363315997307268
Epoch 1193/10000, Prediction Accuracy = 59.42307692307691%, Loss = 0.010066330361251648
Epoch: 1193, Batch Gradient Norm: 0.6457334170669952
Epoch: 1193, Batch Gradient Norm after: 0.6457334170669952
Epoch 1194/10000, Prediction Accuracy = 59.18846153846154%, Loss = 0.010113513741928797
Epoch: 1194, Batch Gradient Norm: 0.6384565814757128
Epoch: 1194, Batch Gradient Norm after: 0.6384565814757128
Epoch 1195/10000, Prediction Accuracy = 59.27307692307692%, Loss = 0.010117842672536006
Epoch: 1195, Batch Gradient Norm: 0.642222044934162
Epoch: 1195, Batch Gradient Norm after: 0.642222044934162
Epoch 1196/10000, Prediction Accuracy = 59.29999999999999%, Loss = 0.01011272519826889
Epoch: 1196, Batch Gradient Norm: 0.6343142428840091
Epoch: 1196, Batch Gradient Norm after: 0.6343142428840091
Epoch 1197/10000, Prediction Accuracy = 59.342307692307685%, Loss = 0.010096135835808057
Epoch: 1197, Batch Gradient Norm: 0.6296353616698154
Epoch: 1197, Batch Gradient Norm after: 0.6296353616698154
Epoch 1198/10000, Prediction Accuracy = 59.62692307692308%, Loss = 0.01010079330836351
Epoch: 1198, Batch Gradient Norm: 0.635927343007601
Epoch: 1198, Batch Gradient Norm after: 0.635927343007601
Epoch 1199/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010066623369661661
Epoch: 1199, Batch Gradient Norm: 0.6410782405025889
Epoch: 1199, Batch Gradient Norm after: 0.6410782405025889
Epoch 1200/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010139573437090103
Epoch: 1200, Batch Gradient Norm: 0.6380097575579395
Epoch: 1200, Batch Gradient Norm after: 0.6380097575579395
Epoch 1201/10000, Prediction Accuracy = 59.68076923076922%, Loss = 0.0100499286244695
Epoch: 1201, Batch Gradient Norm: 0.6417128628714535
Epoch: 1201, Batch Gradient Norm after: 0.6417128628714535
Epoch 1202/10000, Prediction Accuracy = 59.45000000000001%, Loss = 0.010073749491801629
Epoch: 1202, Batch Gradient Norm: 0.650214223102448
Epoch: 1202, Batch Gradient Norm after: 0.650214223102448
Epoch 1203/10000, Prediction Accuracy = 59.411538461538456%, Loss = 0.010099719923276167
Epoch: 1203, Batch Gradient Norm: 0.6328245181970087
Epoch: 1203, Batch Gradient Norm after: 0.6328245181970087
Epoch 1204/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.01010094303637743
Epoch: 1204, Batch Gradient Norm: 0.64214540644132
Epoch: 1204, Batch Gradient Norm after: 0.64214540644132
Epoch 1205/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.010086013219104363
Epoch: 1205, Batch Gradient Norm: 0.6442317801387675
Epoch: 1205, Batch Gradient Norm after: 0.6442317801387675
Epoch 1206/10000, Prediction Accuracy = 59.176923076923075%, Loss = 0.010141431568906857
Epoch: 1206, Batch Gradient Norm: 0.655814105087124
Epoch: 1206, Batch Gradient Norm after: 0.655814105087124
Epoch 1207/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.01013526886415023
Epoch: 1207, Batch Gradient Norm: 0.6330526979924515
Epoch: 1207, Batch Gradient Norm after: 0.6330526979924515
Epoch 1208/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010087802218130002
Epoch: 1208, Batch Gradient Norm: 0.6409066051370678
Epoch: 1208, Batch Gradient Norm after: 0.6409066051370678
Epoch 1209/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010065525626906982
Epoch: 1209, Batch Gradient Norm: 0.63293491670942
Epoch: 1209, Batch Gradient Norm after: 0.63293491670942
Epoch 1210/10000, Prediction Accuracy = 59.446153846153834%, Loss = 0.010068971663713455
Epoch: 1210, Batch Gradient Norm: 0.6529928451696272
Epoch: 1210, Batch Gradient Norm after: 0.6529928451696272
Epoch 1211/10000, Prediction Accuracy = 59.215384615384615%, Loss = 0.010136493696616245
Epoch: 1211, Batch Gradient Norm: 0.6359252493835568
Epoch: 1211, Batch Gradient Norm after: 0.6359252493835568
Epoch 1212/10000, Prediction Accuracy = 59.54230769230768%, Loss = 0.01008591980028611
Epoch: 1212, Batch Gradient Norm: 0.6464099583572341
Epoch: 1212, Batch Gradient Norm after: 0.6464099583572341
Epoch 1213/10000, Prediction Accuracy = 59.70384615384615%, Loss = 0.0100785347704704
Epoch: 1213, Batch Gradient Norm: 0.644253425612702
Epoch: 1213, Batch Gradient Norm after: 0.644253425612702
Epoch 1214/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.010111820669128345
Epoch: 1214, Batch Gradient Norm: 0.6350339953238748
Epoch: 1214, Batch Gradient Norm after: 0.6350339953238748
Epoch 1215/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.010074353275390772
Epoch: 1215, Batch Gradient Norm: 0.6681642725209348
Epoch: 1215, Batch Gradient Norm after: 0.6681642725209348
Epoch 1216/10000, Prediction Accuracy = 59.53846153846153%, Loss = 0.010103120468556881
Epoch: 1216, Batch Gradient Norm: 0.6521010964648793
Epoch: 1216, Batch Gradient Norm after: 0.6521010964648793
Epoch 1217/10000, Prediction Accuracy = 59.165384615384625%, Loss = 0.010107978462026669
Epoch: 1217, Batch Gradient Norm: 0.652535660460721
Epoch: 1217, Batch Gradient Norm after: 0.652535660460721
Epoch 1218/10000, Prediction Accuracy = 59.31538461538461%, Loss = 0.010110802446993498
Epoch: 1218, Batch Gradient Norm: 0.6664302965555143
Epoch: 1218, Batch Gradient Norm after: 0.6664302965555143
Epoch 1219/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010135056307682624
Epoch: 1219, Batch Gradient Norm: 0.6232211504776883
Epoch: 1219, Batch Gradient Norm after: 0.6232211504776883
Epoch 1220/10000, Prediction Accuracy = 59.67307692307691%, Loss = 0.010057471119440518
Epoch: 1220, Batch Gradient Norm: 0.6320497892643967
Epoch: 1220, Batch Gradient Norm after: 0.6320497892643967
Epoch 1221/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010088221671489568
Epoch: 1221, Batch Gradient Norm: 0.6428672542723468
Epoch: 1221, Batch Gradient Norm after: 0.6428672542723468
Epoch 1222/10000, Prediction Accuracy = 59.53076923076924%, Loss = 0.010052719726585425
Epoch: 1222, Batch Gradient Norm: 0.6341418620619215
Epoch: 1222, Batch Gradient Norm after: 0.6341418620619215
Epoch 1223/10000, Prediction Accuracy = 59.31923076923076%, Loss = 0.010072419276604285
Epoch: 1223, Batch Gradient Norm: 0.6382749967200354
Epoch: 1223, Batch Gradient Norm after: 0.6382749967200354
Epoch 1224/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010055753617332531
Epoch: 1224, Batch Gradient Norm: 0.641698437125879
Epoch: 1224, Batch Gradient Norm after: 0.641698437125879
Epoch 1225/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010129792973972283
Epoch: 1225, Batch Gradient Norm: 0.634859843928156
Epoch: 1225, Batch Gradient Norm after: 0.634859843928156
Epoch 1226/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010108607534605723
Epoch: 1226, Batch Gradient Norm: 0.6424457275485258
Epoch: 1226, Batch Gradient Norm after: 0.6424457275485258
Epoch 1227/10000, Prediction Accuracy = 59.71538461538462%, Loss = 0.01013764595756164
Epoch: 1227, Batch Gradient Norm: 0.6564424142839294
Epoch: 1227, Batch Gradient Norm after: 0.6564424142839294
Epoch 1228/10000, Prediction Accuracy = 59.219230769230776%, Loss = 0.010134374722838402
Epoch: 1228, Batch Gradient Norm: 0.6497020845658934
Epoch: 1228, Batch Gradient Norm after: 0.6497020845658934
Epoch 1229/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.010093719197007326
Epoch: 1229, Batch Gradient Norm: 0.6303257349719494
Epoch: 1229, Batch Gradient Norm after: 0.6303257349719494
Epoch 1230/10000, Prediction Accuracy = 59.542307692307695%, Loss = 0.010082017558698472
Epoch: 1230, Batch Gradient Norm: 0.6426785400518067
Epoch: 1230, Batch Gradient Norm after: 0.6426785400518067
Epoch 1231/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010102250111790804
Epoch: 1231, Batch Gradient Norm: 0.6457443370819458
Epoch: 1231, Batch Gradient Norm after: 0.6457443370819458
Epoch 1232/10000, Prediction Accuracy = 59.630769230769225%, Loss = 0.010055220757539455
Epoch: 1232, Batch Gradient Norm: 0.6546729601557779
Epoch: 1232, Batch Gradient Norm after: 0.6546729601557779
Epoch 1233/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010101469305272285
Epoch: 1233, Batch Gradient Norm: 0.6435628464302301
Epoch: 1233, Batch Gradient Norm after: 0.6435628464302301
Epoch 1234/10000, Prediction Accuracy = 59.28846153846155%, Loss = 0.010118289277530633
Epoch: 1234, Batch Gradient Norm: 0.6534681081338211
Epoch: 1234, Batch Gradient Norm after: 0.6534681081338211
Epoch 1235/10000, Prediction Accuracy = 59.33846153846154%, Loss = 0.010102670281552352
Epoch: 1235, Batch Gradient Norm: 0.6381115819800273
Epoch: 1235, Batch Gradient Norm after: 0.6381115819800273
Epoch 1236/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010045734663995413
Epoch: 1236, Batch Gradient Norm: 0.6510486858448661
Epoch: 1236, Batch Gradient Norm after: 0.6510486858448661
Epoch 1237/10000, Prediction Accuracy = 59.599999999999994%, Loss = 0.010088161565363407
Epoch: 1237, Batch Gradient Norm: 0.642249830792509
Epoch: 1237, Batch Gradient Norm after: 0.642249830792509
Epoch 1238/10000, Prediction Accuracy = 59.26153846153846%, Loss = 0.010151760867581917
Epoch: 1238, Batch Gradient Norm: 0.6251943871409924
Epoch: 1238, Batch Gradient Norm after: 0.6251943871409924
Epoch 1239/10000, Prediction Accuracy = 59.45%, Loss = 0.010054922734315578
Epoch: 1239, Batch Gradient Norm: 0.6422805995301282
Epoch: 1239, Batch Gradient Norm after: 0.6422805995301282
Epoch 1240/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010116097374031177
Epoch: 1240, Batch Gradient Norm: 0.664470622464186
Epoch: 1240, Batch Gradient Norm after: 0.664470622464186
Epoch 1241/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.010118621186568188
Epoch: 1241, Batch Gradient Norm: 0.6318465904591628
Epoch: 1241, Batch Gradient Norm after: 0.6318465904591628
Epoch 1242/10000, Prediction Accuracy = 59.42692307692308%, Loss = 0.010080262230565915
Epoch: 1242, Batch Gradient Norm: 0.6416311415889515
Epoch: 1242, Batch Gradient Norm after: 0.6416311415889515
Epoch 1243/10000, Prediction Accuracy = 59.45%, Loss = 0.010094190732790874
Epoch: 1243, Batch Gradient Norm: 0.6567986001085152
Epoch: 1243, Batch Gradient Norm after: 0.6567986001085152
Epoch 1244/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010135118563014727
Epoch: 1244, Batch Gradient Norm: 0.6414665457538629
Epoch: 1244, Batch Gradient Norm after: 0.6414665457538629
Epoch 1245/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010066476865456654
Epoch: 1245, Batch Gradient Norm: 0.6517073910254486
Epoch: 1245, Batch Gradient Norm after: 0.6517073910254486
Epoch 1246/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010080857703892084
Epoch: 1246, Batch Gradient Norm: 0.6358998092152441
Epoch: 1246, Batch Gradient Norm after: 0.6358998092152441
Epoch 1247/10000, Prediction Accuracy = 59.68461538461539%, Loss = 0.010057735829972304
Epoch: 1247, Batch Gradient Norm: 0.6523955468976512
Epoch: 1247, Batch Gradient Norm after: 0.6523955468976512
Epoch 1248/10000, Prediction Accuracy = 59.71153846153846%, Loss = 0.010082818209551848
Epoch: 1248, Batch Gradient Norm: 0.6383961870829656
Epoch: 1248, Batch Gradient Norm after: 0.6383961870829656
Epoch 1249/10000, Prediction Accuracy = 59.088461538461544%, Loss = 0.010104358912660526
Epoch: 1249, Batch Gradient Norm: 0.6431584641729436
Epoch: 1249, Batch Gradient Norm after: 0.6431584641729436
Epoch 1250/10000, Prediction Accuracy = 59.51923076923077%, Loss = 0.010067854578105303
Epoch: 1250, Batch Gradient Norm: 0.6441841343951632
Epoch: 1250, Batch Gradient Norm after: 0.6441841343951632
Epoch 1251/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010120741449869596
Epoch: 1251, Batch Gradient Norm: 0.6461772724062129
Epoch: 1251, Batch Gradient Norm after: 0.6461772724062129
Epoch 1252/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010094831697642803
Epoch: 1252, Batch Gradient Norm: 0.6376555207327829
Epoch: 1252, Batch Gradient Norm after: 0.6376555207327829
Epoch 1253/10000, Prediction Accuracy = 59.62692307692308%, Loss = 0.010082907688159209
Epoch: 1253, Batch Gradient Norm: 0.6261354250552204
Epoch: 1253, Batch Gradient Norm after: 0.6261354250552204
Epoch 1254/10000, Prediction Accuracy = 59.653846153846146%, Loss = 0.010073701851069927
Epoch: 1254, Batch Gradient Norm: 0.6329928694612137
Epoch: 1254, Batch Gradient Norm after: 0.6329928694612137
Epoch 1255/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010034455559574641
Epoch: 1255, Batch Gradient Norm: 0.6265825053815859
Epoch: 1255, Batch Gradient Norm after: 0.6265825053815859
Epoch 1256/10000, Prediction Accuracy = 59.661538461538456%, Loss = 0.010050500384890117
Epoch: 1256, Batch Gradient Norm: 0.6305863749507019
Epoch: 1256, Batch Gradient Norm after: 0.6305863749507019
Epoch 1257/10000, Prediction Accuracy = 59.3923076923077%, Loss = 0.010116774230622329
Epoch: 1257, Batch Gradient Norm: 0.647015207476028
Epoch: 1257, Batch Gradient Norm after: 0.647015207476028
Epoch 1258/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010125554739855804
Epoch: 1258, Batch Gradient Norm: 0.6477706705101672
Epoch: 1258, Batch Gradient Norm after: 0.6477706705101672
Epoch 1259/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.010122953054423515
Epoch: 1259, Batch Gradient Norm: 0.6537041333288253
Epoch: 1259, Batch Gradient Norm after: 0.6537041333288253
Epoch 1260/10000, Prediction Accuracy = 59.307692307692314%, Loss = 0.010112625904954396
Epoch: 1260, Batch Gradient Norm: 0.6340694763376319
Epoch: 1260, Batch Gradient Norm after: 0.6340694763376319
Epoch 1261/10000, Prediction Accuracy = 59.45%, Loss = 0.010070267061774548
Epoch: 1261, Batch Gradient Norm: 0.6429243761842992
Epoch: 1261, Batch Gradient Norm after: 0.6429243761842992
Epoch 1262/10000, Prediction Accuracy = 59.57692307692308%, Loss = 0.010084482268072091
Epoch: 1262, Batch Gradient Norm: 0.6416660265257862
Epoch: 1262, Batch Gradient Norm after: 0.6416660265257862
Epoch 1263/10000, Prediction Accuracy = 59.449999999999996%, Loss = 0.010092452383385254
Epoch: 1263, Batch Gradient Norm: 0.6473614576245456
Epoch: 1263, Batch Gradient Norm after: 0.6473614576245456
Epoch 1264/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.01007142634345935
Epoch: 1264, Batch Gradient Norm: 0.6331320330350396
Epoch: 1264, Batch Gradient Norm after: 0.6331320330350396
Epoch 1265/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010097798604804736
Epoch: 1265, Batch Gradient Norm: 0.6346033063058368
Epoch: 1265, Batch Gradient Norm after: 0.6346033063058368
Epoch 1266/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010078896553470539
Epoch: 1266, Batch Gradient Norm: 0.6486625747737256
Epoch: 1266, Batch Gradient Norm after: 0.6486625747737256
Epoch 1267/10000, Prediction Accuracy = 59.56538461538461%, Loss = 0.010074459876005467
Epoch: 1267, Batch Gradient Norm: 0.6353710129239235
Epoch: 1267, Batch Gradient Norm after: 0.6353710129239235
Epoch 1268/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.01011483922887307
Epoch: 1268, Batch Gradient Norm: 0.6482369148191873
Epoch: 1268, Batch Gradient Norm after: 0.6482369148191873
Epoch 1269/10000, Prediction Accuracy = 59.55%, Loss = 0.010094912364505805
Epoch: 1269, Batch Gradient Norm: 0.6422398794824782
Epoch: 1269, Batch Gradient Norm after: 0.6422398794824782
Epoch 1270/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.01007091117879519
Epoch: 1270, Batch Gradient Norm: 0.6363664511221276
Epoch: 1270, Batch Gradient Norm after: 0.6363664511221276
Epoch 1271/10000, Prediction Accuracy = 59.60000000000001%, Loss = 0.010046244169083925
Epoch: 1271, Batch Gradient Norm: 0.6428217094100838
Epoch: 1271, Batch Gradient Norm after: 0.6428217094100838
Epoch 1272/10000, Prediction Accuracy = 59.376923076923084%, Loss = 0.010093976528598713
Epoch: 1272, Batch Gradient Norm: 0.625142585239302
Epoch: 1272, Batch Gradient Norm after: 0.625142585239302
Epoch 1273/10000, Prediction Accuracy = 59.16153846153846%, Loss = 0.010098923857395466
Epoch: 1273, Batch Gradient Norm: 0.6435196391484347
Epoch: 1273, Batch Gradient Norm after: 0.6435196391484347
Epoch 1274/10000, Prediction Accuracy = 59.380769230769246%, Loss = 0.010104658082127571
Epoch: 1274, Batch Gradient Norm: 0.6426840310656524
Epoch: 1274, Batch Gradient Norm after: 0.6426840310656524
Epoch 1275/10000, Prediction Accuracy = 59.21153846153847%, Loss = 0.010109662651442565
Epoch: 1275, Batch Gradient Norm: 0.646580539364634
Epoch: 1275, Batch Gradient Norm after: 0.646580539364634
Epoch 1276/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010125498215739544
Epoch: 1276, Batch Gradient Norm: 0.6532313539766893
Epoch: 1276, Batch Gradient Norm after: 0.6532313539766893
Epoch 1277/10000, Prediction Accuracy = 59.126923076923084%, Loss = 0.010092866893571157
Epoch: 1277, Batch Gradient Norm: 0.6520553026902429
Epoch: 1277, Batch Gradient Norm after: 0.6520553026902429
Epoch 1278/10000, Prediction Accuracy = 59.21153846153846%, Loss = 0.010114596153681096
Epoch: 1278, Batch Gradient Norm: 0.6458327223562639
Epoch: 1278, Batch Gradient Norm after: 0.6458327223562639
Epoch 1279/10000, Prediction Accuracy = 59.25000000000001%, Loss = 0.010131256941419382
Epoch: 1279, Batch Gradient Norm: 0.6567211018051904
Epoch: 1279, Batch Gradient Norm after: 0.6567211018051904
Epoch 1280/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010113985062791752
Epoch: 1280, Batch Gradient Norm: 0.6297295390471316
Epoch: 1280, Batch Gradient Norm after: 0.6297295390471316
Epoch 1281/10000, Prediction Accuracy = 59.46923076923076%, Loss = 0.010079896005873498
Epoch: 1281, Batch Gradient Norm: 0.6432145889027877
Epoch: 1281, Batch Gradient Norm after: 0.6432145889027877
Epoch 1282/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.010080263949930668
Epoch: 1282, Batch Gradient Norm: 0.6508656233138511
Epoch: 1282, Batch Gradient Norm after: 0.6508656233138511
Epoch 1283/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.01008633065682191
Epoch: 1283, Batch Gradient Norm: 0.6489389107734578
Epoch: 1283, Batch Gradient Norm after: 0.6489389107734578
Epoch 1284/10000, Prediction Accuracy = 59.20769230769231%, Loss = 0.010102458154925933
Epoch: 1284, Batch Gradient Norm: 0.6338263730168617
Epoch: 1284, Batch Gradient Norm after: 0.6338263730168617
Epoch 1285/10000, Prediction Accuracy = 59.60384615384615%, Loss = 0.010073698340700222
Epoch: 1285, Batch Gradient Norm: 0.6403521342203544
Epoch: 1285, Batch Gradient Norm after: 0.6403521342203544
Epoch 1286/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010076147289230274
Epoch: 1286, Batch Gradient Norm: 0.63839316455121
Epoch: 1286, Batch Gradient Norm after: 0.63839316455121
Epoch 1287/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010073112825361582
Epoch: 1287, Batch Gradient Norm: 0.6425493384429288
Epoch: 1287, Batch Gradient Norm after: 0.6425493384429288
Epoch 1288/10000, Prediction Accuracy = 59.53076923076923%, Loss = 0.010075111515246905
Epoch: 1288, Batch Gradient Norm: 0.6521303744385922
Epoch: 1288, Batch Gradient Norm after: 0.6521303744385922
Epoch 1289/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010193369136406826
Epoch: 1289, Batch Gradient Norm: 0.6413277662982505
Epoch: 1289, Batch Gradient Norm after: 0.6413277662982505
Epoch 1290/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010102006391837047
Epoch: 1290, Batch Gradient Norm: 0.6391188011653304
Epoch: 1290, Batch Gradient Norm after: 0.6391188011653304
Epoch 1291/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010117257658678751
Epoch: 1291, Batch Gradient Norm: 0.6506390879860621
Epoch: 1291, Batch Gradient Norm after: 0.6506390879860621
Epoch 1292/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010100017516658856
Epoch: 1292, Batch Gradient Norm: 0.6491723610041948
Epoch: 1292, Batch Gradient Norm after: 0.6491723610041948
Epoch 1293/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.010093331551895691
Epoch: 1293, Batch Gradient Norm: 0.6443952057251517
Epoch: 1293, Batch Gradient Norm after: 0.6443952057251517
Epoch 1294/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.010102753097621294
Epoch: 1294, Batch Gradient Norm: 0.6326934779477406
Epoch: 1294, Batch Gradient Norm after: 0.6326934779477406
Epoch 1295/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.010068196946611771
Epoch: 1295, Batch Gradient Norm: 0.6385101325244107
Epoch: 1295, Batch Gradient Norm after: 0.6385101325244107
Epoch 1296/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010125267104460644
Epoch: 1296, Batch Gradient Norm: 0.643039955796634
Epoch: 1296, Batch Gradient Norm after: 0.643039955796634
Epoch 1297/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010052287879471596
Epoch: 1297, Batch Gradient Norm: 0.6474927603826188
Epoch: 1297, Batch Gradient Norm after: 0.6474927603826188
Epoch 1298/10000, Prediction Accuracy = 59.53846153846153%, Loss = 0.010079997806594921
Epoch: 1298, Batch Gradient Norm: 0.6409804845354935
Epoch: 1298, Batch Gradient Norm after: 0.6409804845354935
Epoch 1299/10000, Prediction Accuracy = 59.33846153846155%, Loss = 0.010068159622068588
Epoch: 1299, Batch Gradient Norm: 0.6479935563119378
Epoch: 1299, Batch Gradient Norm after: 0.6479935563119378
Epoch 1300/10000, Prediction Accuracy = 59.573076923076925%, Loss = 0.010124928103043484
Epoch: 1300, Batch Gradient Norm: 0.6351839714188254
Epoch: 1300, Batch Gradient Norm after: 0.6351839714188254
Epoch 1301/10000, Prediction Accuracy = 59.2576923076923%, Loss = 0.010100958510660209
Epoch: 1301, Batch Gradient Norm: 0.6461024251343481
Epoch: 1301, Batch Gradient Norm after: 0.6461024251343481
Epoch 1302/10000, Prediction Accuracy = 59.542307692307695%, Loss = 0.010077031329274178
Epoch: 1302, Batch Gradient Norm: 0.6368118802587036
Epoch: 1302, Batch Gradient Norm after: 0.6368118802587036
Epoch 1303/10000, Prediction Accuracy = 59.580769230769235%, Loss = 0.010089399937826853
Epoch: 1303, Batch Gradient Norm: 0.6417774391905332
Epoch: 1303, Batch Gradient Norm after: 0.6417774391905332
Epoch 1304/10000, Prediction Accuracy = 59.457692307692305%, Loss = 0.010098763454992037
Epoch: 1304, Batch Gradient Norm: 0.6511742655166485
Epoch: 1304, Batch Gradient Norm after: 0.6511742655166485
Epoch 1305/10000, Prediction Accuracy = 59.449999999999996%, Loss = 0.01012854722256844
Epoch: 1305, Batch Gradient Norm: 0.6488886554363869
Epoch: 1305, Batch Gradient Norm after: 0.6488886554363869
Epoch 1306/10000, Prediction Accuracy = 59.59615384615384%, Loss = 0.01004481609337605
Epoch: 1306, Batch Gradient Norm: 0.6485757618651219
Epoch: 1306, Batch Gradient Norm after: 0.6485757618651219
Epoch 1307/10000, Prediction Accuracy = 59.584615384615375%, Loss = 0.01012023387906643
Epoch: 1307, Batch Gradient Norm: 0.6538736709284845
Epoch: 1307, Batch Gradient Norm after: 0.6538736709284845
Epoch 1308/10000, Prediction Accuracy = 59.58461538461539%, Loss = 0.010124405631079124
Epoch: 1308, Batch Gradient Norm: 0.6302417586711221
Epoch: 1308, Batch Gradient Norm after: 0.6302417586711221
Epoch 1309/10000, Prediction Accuracy = 59.43461538461539%, Loss = 0.01007938406501825
Epoch: 1309, Batch Gradient Norm: 0.6414108447105866
Epoch: 1309, Batch Gradient Norm after: 0.6414108447105866
Epoch 1310/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.01008438433592136
Epoch: 1310, Batch Gradient Norm: 0.6489582711669781
Epoch: 1310, Batch Gradient Norm after: 0.6489582711669781
Epoch 1311/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.010099681810690807
Epoch: 1311, Batch Gradient Norm: 0.638110930690344
Epoch: 1311, Batch Gradient Norm after: 0.638110930690344
Epoch 1312/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010102043429819437
Epoch: 1312, Batch Gradient Norm: 0.6351526422124155
Epoch: 1312, Batch Gradient Norm after: 0.6351526422124155
Epoch 1313/10000, Prediction Accuracy = 59.57692307692308%, Loss = 0.010056587795798596
Epoch: 1313, Batch Gradient Norm: 0.65231912844151
Epoch: 1313, Batch Gradient Norm after: 0.65231912844151
Epoch 1314/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010129095628284492
Epoch: 1314, Batch Gradient Norm: 0.6467976760315729
Epoch: 1314, Batch Gradient Norm after: 0.6467976760315729
Epoch 1315/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010089354303020697
Epoch: 1315, Batch Gradient Norm: 0.6256224337836851
Epoch: 1315, Batch Gradient Norm after: 0.6256224337836851
Epoch 1316/10000, Prediction Accuracy = 59.38846153846153%, Loss = 0.010061938243989762
Epoch: 1316, Batch Gradient Norm: 0.6421506220616852
Epoch: 1316, Batch Gradient Norm after: 0.6421506220616852
Epoch 1317/10000, Prediction Accuracy = 59.3346153846154%, Loss = 0.010145196332954444
Epoch: 1317, Batch Gradient Norm: 0.6388392384664726
Epoch: 1317, Batch Gradient Norm after: 0.6388392384664726
Epoch 1318/10000, Prediction Accuracy = 59.51923076923078%, Loss = 0.0100806626276328
Epoch: 1318, Batch Gradient Norm: 0.6322148189318093
Epoch: 1318, Batch Gradient Norm after: 0.6322148189318093
Epoch 1319/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.01011705119162798
Epoch: 1319, Batch Gradient Norm: 0.6432923555872294
Epoch: 1319, Batch Gradient Norm after: 0.6432923555872294
Epoch 1320/10000, Prediction Accuracy = 59.23461538461538%, Loss = 0.010144130685008489
Epoch: 1320, Batch Gradient Norm: 0.6445444922239235
Epoch: 1320, Batch Gradient Norm after: 0.6445444922239235
Epoch 1321/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010128277425582592
Epoch: 1321, Batch Gradient Norm: 0.6507739534831068
Epoch: 1321, Batch Gradient Norm after: 0.6507739534831068
Epoch 1322/10000, Prediction Accuracy = 59.515384615384605%, Loss = 0.010134930722415447
Epoch: 1322, Batch Gradient Norm: 0.6393487397923496
Epoch: 1322, Batch Gradient Norm after: 0.6393487397923496
Epoch 1323/10000, Prediction Accuracy = 59.43461538461539%, Loss = 0.010133018932090355
Epoch: 1323, Batch Gradient Norm: 0.6346266446136288
Epoch: 1323, Batch Gradient Norm after: 0.6346266446136288
Epoch 1324/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010112884024587961
Epoch: 1324, Batch Gradient Norm: 0.6370031869818207
Epoch: 1324, Batch Gradient Norm after: 0.6370031869818207
Epoch 1325/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.01007802039384842
Epoch: 1325, Batch Gradient Norm: 0.6364492629757027
Epoch: 1325, Batch Gradient Norm after: 0.6364492629757027
Epoch 1326/10000, Prediction Accuracy = 59.34615384615384%, Loss = 0.01008087432441803
Epoch: 1326, Batch Gradient Norm: 0.639899327923946
Epoch: 1326, Batch Gradient Norm after: 0.639899327923946
Epoch 1327/10000, Prediction Accuracy = 59.611538461538466%, Loss = 0.010061266044011483
Epoch: 1327, Batch Gradient Norm: 0.6410126724333325
Epoch: 1327, Batch Gradient Norm after: 0.6410126724333325
Epoch 1328/10000, Prediction Accuracy = 59.26923076923076%, Loss = 0.010098115541040897
Epoch: 1328, Batch Gradient Norm: 0.6489312019065804
Epoch: 1328, Batch Gradient Norm after: 0.6489312019065804
Epoch 1329/10000, Prediction Accuracy = 59.06923076923077%, Loss = 0.010152180535862079
Epoch: 1329, Batch Gradient Norm: 0.6404163897535459
Epoch: 1329, Batch Gradient Norm after: 0.6404163897535459
Epoch 1330/10000, Prediction Accuracy = 59.24615384615384%, Loss = 0.010144157191881767
Epoch: 1330, Batch Gradient Norm: 0.6335697587010934
Epoch: 1330, Batch Gradient Norm after: 0.6335697587010934
Epoch 1331/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010066091727751952
Epoch: 1331, Batch Gradient Norm: 0.6354429170542628
Epoch: 1331, Batch Gradient Norm after: 0.6354429170542628
Epoch 1332/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010064087020089993
Epoch: 1332, Batch Gradient Norm: 0.6428333461276281
Epoch: 1332, Batch Gradient Norm after: 0.6428333461276281
Epoch 1333/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.01010211600134006
Epoch: 1333, Batch Gradient Norm: 0.6301910345076621
Epoch: 1333, Batch Gradient Norm after: 0.6301910345076621
Epoch 1334/10000, Prediction Accuracy = 59.21923076923076%, Loss = 0.01011620332988409
Epoch: 1334, Batch Gradient Norm: 0.6387774625378918
Epoch: 1334, Batch Gradient Norm after: 0.6387774625378918
Epoch 1335/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010055620796405353
Epoch: 1335, Batch Gradient Norm: 0.6487870480441249
Epoch: 1335, Batch Gradient Norm after: 0.6487870480441249
Epoch 1336/10000, Prediction Accuracy = 59.515384615384626%, Loss = 0.01007842573408897
Epoch: 1336, Batch Gradient Norm: 0.6540658444620019
Epoch: 1336, Batch Gradient Norm after: 0.6540658444620019
Epoch 1337/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.01009780211517444
Epoch: 1337, Batch Gradient Norm: 0.6654922063768454
Epoch: 1337, Batch Gradient Norm after: 0.6654922063768454
Epoch 1338/10000, Prediction Accuracy = 59.11153846153846%, Loss = 0.01017102556159863
Epoch: 1338, Batch Gradient Norm: 0.6392377157513859
Epoch: 1338, Batch Gradient Norm after: 0.6392377157513859
Epoch 1339/10000, Prediction Accuracy = 59.16153846153846%, Loss = 0.010138890491082119
Epoch: 1339, Batch Gradient Norm: 0.6452324497385813
Epoch: 1339, Batch Gradient Norm after: 0.6452324497385813
Epoch 1340/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010049275767344695
Epoch: 1340, Batch Gradient Norm: 0.6489089824107638
Epoch: 1340, Batch Gradient Norm after: 0.6489089824107638
Epoch 1341/10000, Prediction Accuracy = 59.31538461538461%, Loss = 0.010080772237135814
Epoch: 1341, Batch Gradient Norm: 0.6415563430497884
Epoch: 1341, Batch Gradient Norm after: 0.6415563430497884
Epoch 1342/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.010078561563904468
Epoch: 1342, Batch Gradient Norm: 0.6354300319216418
Epoch: 1342, Batch Gradient Norm after: 0.6354300319216418
Epoch 1343/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.01008722150268463
Epoch: 1343, Batch Gradient Norm: 0.6330719793046136
Epoch: 1343, Batch Gradient Norm after: 0.6330719793046136
Epoch 1344/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010061288467393471
Epoch: 1344, Batch Gradient Norm: 0.653375072068768
Epoch: 1344, Batch Gradient Norm after: 0.653375072068768
Epoch 1345/10000, Prediction Accuracy = 59.62692307692308%, Loss = 0.010086429520295216
Epoch: 1345, Batch Gradient Norm: 0.6381330706287068
Epoch: 1345, Batch Gradient Norm after: 0.6381330706287068
Epoch 1346/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.010113257341659986
Epoch: 1346, Batch Gradient Norm: 0.6492316714248956
Epoch: 1346, Batch Gradient Norm after: 0.6492316714248956
Epoch 1347/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.010084975940676836
Epoch: 1347, Batch Gradient Norm: 0.6617003493748101
Epoch: 1347, Batch Gradient Norm after: 0.6617003493748101
Epoch 1348/10000, Prediction Accuracy = 59.63076923076923%, Loss = 0.010108997328923298
Epoch: 1348, Batch Gradient Norm: 0.6321205044133219
Epoch: 1348, Batch Gradient Norm after: 0.6321205044133219
Epoch 1349/10000, Prediction Accuracy = 59.35384615384616%, Loss = 0.010078064595850615
Epoch: 1349, Batch Gradient Norm: 0.6478138847643433
Epoch: 1349, Batch Gradient Norm after: 0.6478138847643433
Epoch 1350/10000, Prediction Accuracy = 59.30384615384616%, Loss = 0.010120514780282974
Epoch: 1350, Batch Gradient Norm: 0.6345757863901272
Epoch: 1350, Batch Gradient Norm after: 0.6345757863901272
Epoch 1351/10000, Prediction Accuracy = 59.56153846153847%, Loss = 0.010091519842927273
Epoch: 1351, Batch Gradient Norm: 0.6396075988298193
Epoch: 1351, Batch Gradient Norm after: 0.6396075988298193
Epoch 1352/10000, Prediction Accuracy = 59.35384615384616%, Loss = 0.010089671167616662
Epoch: 1352, Batch Gradient Norm: 0.6531308274864149
Epoch: 1352, Batch Gradient Norm after: 0.6531308274864149
Epoch 1353/10000, Prediction Accuracy = 59.63461538461537%, Loss = 0.01010157411488203
Epoch: 1353, Batch Gradient Norm: 0.6342057392458399
Epoch: 1353, Batch Gradient Norm after: 0.6342057392458399
Epoch 1354/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.010105426279971233
Epoch: 1354, Batch Gradient Norm: 0.6371600229381804
Epoch: 1354, Batch Gradient Norm after: 0.6371600229381804
Epoch 1355/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010089747392787384
Epoch: 1355, Batch Gradient Norm: 0.6465044784356603
Epoch: 1355, Batch Gradient Norm after: 0.6465044784356603
Epoch 1356/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.01011826592282607
Epoch: 1356, Batch Gradient Norm: 0.6325217788130524
Epoch: 1356, Batch Gradient Norm after: 0.6325217788130524
Epoch 1357/10000, Prediction Accuracy = 59.44615384615385%, Loss = 0.010076408847593345
Epoch: 1357, Batch Gradient Norm: 0.6378189946670291
Epoch: 1357, Batch Gradient Norm after: 0.6378189946670291
Epoch 1358/10000, Prediction Accuracy = 59.37692307692306%, Loss = 0.010092648319326915
Epoch: 1358, Batch Gradient Norm: 0.6325936040860813
Epoch: 1358, Batch Gradient Norm after: 0.6325936040860813
Epoch 1359/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.01011610604249514
Epoch: 1359, Batch Gradient Norm: 0.6342324159901614
Epoch: 1359, Batch Gradient Norm after: 0.6342324159901614
Epoch 1360/10000, Prediction Accuracy = 59.542307692307695%, Loss = 0.010095212751856217
Epoch: 1360, Batch Gradient Norm: 0.6488713013556959
Epoch: 1360, Batch Gradient Norm after: 0.6488713013556959
Epoch 1361/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010108300914558081
Epoch: 1361, Batch Gradient Norm: 0.6332821273979882
Epoch: 1361, Batch Gradient Norm after: 0.6332821273979882
Epoch 1362/10000, Prediction Accuracy = 59.51153846153847%, Loss = 0.010081315842958597
Epoch: 1362, Batch Gradient Norm: 0.644712808216063
Epoch: 1362, Batch Gradient Norm after: 0.644712808216063
Epoch 1363/10000, Prediction Accuracy = 59.45384615384616%, Loss = 0.010103542214402786
Epoch: 1363, Batch Gradient Norm: 0.659297366585886
Epoch: 1363, Batch Gradient Norm after: 0.659297366585886
Epoch 1364/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.01009924036379044
Epoch: 1364, Batch Gradient Norm: 0.6376659155695587
Epoch: 1364, Batch Gradient Norm after: 0.6376659155695587
Epoch 1365/10000, Prediction Accuracy = 59.08461538461539%, Loss = 0.010114606828070603
Epoch: 1365, Batch Gradient Norm: 0.6342717929866382
Epoch: 1365, Batch Gradient Norm after: 0.6342717929866382
Epoch 1366/10000, Prediction Accuracy = 59.43846153846154%, Loss = 0.010106626181648327
Epoch: 1366, Batch Gradient Norm: 0.6574701106105684
Epoch: 1366, Batch Gradient Norm after: 0.6574701106105684
Epoch 1367/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010122368470407449
Epoch: 1367, Batch Gradient Norm: 0.6549019278370577
Epoch: 1367, Batch Gradient Norm after: 0.6549019278370577
Epoch 1368/10000, Prediction Accuracy = 59.376923076923084%, Loss = 0.010139311305605449
Epoch: 1368, Batch Gradient Norm: 0.6437207722880682
Epoch: 1368, Batch Gradient Norm after: 0.6437207722880682
Epoch 1369/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010061011434747623
Epoch: 1369, Batch Gradient Norm: 0.6598506514919563
Epoch: 1369, Batch Gradient Norm after: 0.6598506514919563
Epoch 1370/10000, Prediction Accuracy = 59.3576923076923%, Loss = 0.010107047354372648
Epoch: 1370, Batch Gradient Norm: 0.6296115178115983
Epoch: 1370, Batch Gradient Norm after: 0.6296115178115983
Epoch 1371/10000, Prediction Accuracy = 59.66923076923076%, Loss = 0.010053964618306894
Epoch: 1371, Batch Gradient Norm: 0.6487759016963436
Epoch: 1371, Batch Gradient Norm after: 0.6487759016963436
Epoch 1372/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010085230908141686
Epoch: 1372, Batch Gradient Norm: 0.648057607804648
Epoch: 1372, Batch Gradient Norm after: 0.648057607804648
Epoch 1373/10000, Prediction Accuracy = 59.26923076923078%, Loss = 0.010131863733896842
Epoch: 1373, Batch Gradient Norm: 0.6414427925956073
Epoch: 1373, Batch Gradient Norm after: 0.6414427925956073
Epoch 1374/10000, Prediction Accuracy = 59.25384615384615%, Loss = 0.010087000850874644
Epoch: 1374, Batch Gradient Norm: 0.6425648435384418
Epoch: 1374, Batch Gradient Norm after: 0.6425648435384418
Epoch 1375/10000, Prediction Accuracy = 59.25769230769232%, Loss = 0.010111125114445504
Epoch: 1375, Batch Gradient Norm: 0.6177759914007562
Epoch: 1375, Batch Gradient Norm after: 0.6177759914007562
Epoch 1376/10000, Prediction Accuracy = 59.707692307692305%, Loss = 0.010041676031855436
Epoch: 1376, Batch Gradient Norm: 0.6358641427032721
Epoch: 1376, Batch Gradient Norm after: 0.6358641427032721
Epoch 1377/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.010094227126011482
Epoch: 1377, Batch Gradient Norm: 0.6631381834428828
Epoch: 1377, Batch Gradient Norm after: 0.6631381834428828
Epoch 1378/10000, Prediction Accuracy = 59.626923076923084%, Loss = 0.010072684488617457
Epoch: 1378, Batch Gradient Norm: 0.6491728960449228
Epoch: 1378, Batch Gradient Norm after: 0.6491728960449228
Epoch 1379/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.010121514805807518
Epoch: 1379, Batch Gradient Norm: 0.6416321825643586
Epoch: 1379, Batch Gradient Norm after: 0.6416321825643586
Epoch 1380/10000, Prediction Accuracy = 59.16538461538461%, Loss = 0.010084729570035752
Epoch: 1380, Batch Gradient Norm: 0.642571528956474
Epoch: 1380, Batch Gradient Norm after: 0.642571528956474
Epoch 1381/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.010087162686082033
Epoch: 1381, Batch Gradient Norm: 0.6391806727202226
Epoch: 1381, Batch Gradient Norm after: 0.6391806727202226
Epoch 1382/10000, Prediction Accuracy = 59.20769230769231%, Loss = 0.01012216737637153
Epoch: 1382, Batch Gradient Norm: 0.6428679672600754
Epoch: 1382, Batch Gradient Norm after: 0.6428679672600754
Epoch 1383/10000, Prediction Accuracy = 59.35384615384616%, Loss = 0.010146868630097462
Epoch: 1383, Batch Gradient Norm: 0.626109670617191
Epoch: 1383, Batch Gradient Norm after: 0.626109670617191
Epoch 1384/10000, Prediction Accuracy = 59.52307692307692%, Loss = 0.010043061839846464
Epoch: 1384, Batch Gradient Norm: 0.6611484807697805
Epoch: 1384, Batch Gradient Norm after: 0.6611484807697805
Epoch 1385/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010123019966368493
Epoch: 1385, Batch Gradient Norm: 0.6237610500180724
Epoch: 1385, Batch Gradient Norm after: 0.6237610500180724
Epoch 1386/10000, Prediction Accuracy = 59.523076923076914%, Loss = 0.010068738331588415
Epoch: 1387, Batch Gradient Norm: 0.6497064754604589
Epoch: 1387, Batch Gradient Norm after: 0.6497064754604589
Epoch 1388/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.010133302555634426
Epoch: 1388, Batch Gradient Norm: 0.6407991565758832
Epoch: 1388, Batch Gradient Norm after: 0.6407991565758832
Epoch 1389/10000, Prediction Accuracy = 59.584615384615375%, Loss = 0.010055596438738016
Epoch: 1389, Batch Gradient Norm: 0.6338028711224649
Epoch: 1389, Batch Gradient Norm after: 0.6338028711224649
Epoch 1390/10000, Prediction Accuracy = 59.153846153846146%, Loss = 0.010069671445167981
Epoch: 1390, Batch Gradient Norm: 0.6424385970091105
Epoch: 1390, Batch Gradient Norm after: 0.6424385970091105
Epoch 1391/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010118458420038223
Epoch: 1391, Batch Gradient Norm: 0.6465013396495094
Epoch: 1391, Batch Gradient Norm after: 0.6465013396495094
Epoch 1392/10000, Prediction Accuracy = 59.35384615384616%, Loss = 0.010113825591710897
Epoch: 1392, Batch Gradient Norm: 0.652113986263001
Epoch: 1392, Batch Gradient Norm after: 0.652113986263001
Epoch 1393/10000, Prediction Accuracy = 59.31923076923076%, Loss = 0.010112592448981909
Epoch: 1393, Batch Gradient Norm: 0.6357311479884991
Epoch: 1393, Batch Gradient Norm after: 0.6357311479884991
Epoch 1394/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010044729981857996
Epoch: 1394, Batch Gradient Norm: 0.6446623371785473
Epoch: 1394, Batch Gradient Norm after: 0.6446623371785473
Epoch 1395/10000, Prediction Accuracy = 59.2846153846154%, Loss = 0.010141603720302764
Epoch: 1395, Batch Gradient Norm: 0.6389628171971555
Epoch: 1395, Batch Gradient Norm after: 0.6389628171971555
Epoch 1396/10000, Prediction Accuracy = 59.56538461538461%, Loss = 0.010066092587434329
Epoch: 1396, Batch Gradient Norm: 0.6298856939215817
Epoch: 1396, Batch Gradient Norm after: 0.6298856939215817
Epoch 1397/10000, Prediction Accuracy = 59.56153846153846%, Loss = 0.0100873069694409
Epoch: 1397, Batch Gradient Norm: 0.6411653639523344
Epoch: 1397, Batch Gradient Norm after: 0.6411653639523344
Epoch 1398/10000, Prediction Accuracy = 59.13076923076923%, Loss = 0.010117067955434322
Epoch: 1398, Batch Gradient Norm: 0.6351511640977352
Epoch: 1398, Batch Gradient Norm after: 0.6351511640977352
Epoch 1399/10000, Prediction Accuracy = 59.7423076923077%, Loss = 0.010066489474131511
Epoch: 1399, Batch Gradient Norm: 0.6276719535196065
Epoch: 1399, Batch Gradient Norm after: 0.6276719535196065
Epoch 1400/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010038978635118557
Epoch: 1400, Batch Gradient Norm: 0.6357357800416455
Epoch: 1400, Batch Gradient Norm after: 0.6357357800416455
Epoch 1401/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010062369589622203
Epoch: 1401, Batch Gradient Norm: 0.6395039513323487
Epoch: 1401, Batch Gradient Norm after: 0.6395039513323487
Epoch 1402/10000, Prediction Accuracy = 59.6576923076923%, Loss = 0.01004775083408906
Epoch: 1402, Batch Gradient Norm: 0.6383957375195598
Epoch: 1402, Batch Gradient Norm after: 0.6383957375195598
Epoch 1403/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010081428246429333
Epoch: 1403, Batch Gradient Norm: 0.6397373081394042
Epoch: 1403, Batch Gradient Norm after: 0.6397373081394042
Epoch 1404/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010091283215353122
Epoch: 1404, Batch Gradient Norm: 0.6405088431027245
Epoch: 1404, Batch Gradient Norm after: 0.6405088431027245
Epoch 1405/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010170797745768841
Epoch: 1405, Batch Gradient Norm: 0.6518435516155116
Epoch: 1405, Batch Gradient Norm after: 0.6518435516155116
Epoch 1406/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010075840096061047
Epoch: 1406, Batch Gradient Norm: 0.642829085962588
Epoch: 1406, Batch Gradient Norm after: 0.642829085962588
Epoch 1407/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010034602063779648
Epoch: 1407, Batch Gradient Norm: 0.6409684819548171
Epoch: 1407, Batch Gradient Norm after: 0.6409684819548171
Epoch 1408/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010096549486311583
Epoch: 1408, Batch Gradient Norm: 0.644332368037254
Epoch: 1408, Batch Gradient Norm after: 0.644332368037254
Epoch 1409/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010130152106285095
Epoch: 1409, Batch Gradient Norm: 0.6455133637175215
Epoch: 1409, Batch Gradient Norm after: 0.6455133637175215
Epoch 1410/10000, Prediction Accuracy = 59.47692307692307%, Loss = 0.010126572675429858
Epoch: 1410, Batch Gradient Norm: 0.6294058935022523
Epoch: 1410, Batch Gradient Norm after: 0.6294058935022523
Epoch 1411/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.01008062895673972
Epoch: 1411, Batch Gradient Norm: 0.6536506479630231
Epoch: 1411, Batch Gradient Norm after: 0.6536506479630231
Epoch 1412/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.010118357765559968
Epoch: 1412, Batch Gradient Norm: 0.63776629939256
Epoch: 1412, Batch Gradient Norm after: 0.63776629939256
Epoch 1413/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.01010244397016672
Epoch: 1413, Batch Gradient Norm: 0.6440515578730901
Epoch: 1413, Batch Gradient Norm after: 0.6440515578730901
Epoch 1414/10000, Prediction Accuracy = 59.6923076923077%, Loss = 0.010036369714026269
Epoch: 1414, Batch Gradient Norm: 0.6494153897800569
Epoch: 1414, Batch Gradient Norm after: 0.6494153897800569
Epoch 1415/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010105990016689667
Epoch: 1415, Batch Gradient Norm: 0.650247897066344
Epoch: 1415, Batch Gradient Norm after: 0.650247897066344
Epoch 1416/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.010133130977360101
Epoch: 1416, Batch Gradient Norm: 0.6345181870166783
Epoch: 1416, Batch Gradient Norm after: 0.6345181870166783
Epoch 1417/10000, Prediction Accuracy = 59.58461538461539%, Loss = 0.010088712979967777
Epoch: 1417, Batch Gradient Norm: 0.6373398688904738
Epoch: 1417, Batch Gradient Norm after: 0.6373398688904738
Epoch 1418/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010063734335395006
Epoch: 1418, Batch Gradient Norm: 0.6409856018929814
Epoch: 1418, Batch Gradient Norm after: 0.6409856018929814
Epoch 1419/10000, Prediction Accuracy = 59.223076923076924%, Loss = 0.010134789949426284
Epoch: 1419, Batch Gradient Norm: 0.6305797992240856
Epoch: 1419, Batch Gradient Norm after: 0.6305797992240856
Epoch 1420/10000, Prediction Accuracy = 59.56923076923077%, Loss = 0.010067321001910247
Epoch: 1420, Batch Gradient Norm: 0.6341316919461761
Epoch: 1420, Batch Gradient Norm after: 0.6341316919461761
Epoch 1421/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.01015248278585764
Epoch: 1421, Batch Gradient Norm: 0.6465770075237632
Epoch: 1421, Batch Gradient Norm after: 0.6465770075237632
Epoch 1422/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.010101742755908232
Epoch: 1422, Batch Gradient Norm: 0.6403938855627266
Epoch: 1422, Batch Gradient Norm after: 0.6403938855627266
Epoch 1423/10000, Prediction Accuracy = 59.49999999999999%, Loss = 0.010101592669693323
Epoch: 1423, Batch Gradient Norm: 0.6310613884467083
Epoch: 1423, Batch Gradient Norm after: 0.6310613884467083
Epoch 1424/10000, Prediction Accuracy = 59.3423076923077%, Loss = 0.010111339175357269
Epoch: 1424, Batch Gradient Norm: 0.6658643435183824
Epoch: 1424, Batch Gradient Norm after: 0.6658643435183824
Epoch 1425/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010153441546628108
Epoch: 1425, Batch Gradient Norm: 0.6394845227891934
Epoch: 1425, Batch Gradient Norm after: 0.6394845227891934
Epoch 1426/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.010147174390462728
Epoch: 1426, Batch Gradient Norm: 0.6380942899951267
Epoch: 1426, Batch Gradient Norm after: 0.6380942899951267
Epoch 1427/10000, Prediction Accuracy = 59.51538461538461%, Loss = 0.010086986092993846
Epoch: 1427, Batch Gradient Norm: 0.634457853683762
Epoch: 1427, Batch Gradient Norm after: 0.634457853683762
Epoch 1428/10000, Prediction Accuracy = 59.380769230769246%, Loss = 0.010095816392164964
Epoch: 1428, Batch Gradient Norm: 0.6411607076984471
Epoch: 1428, Batch Gradient Norm after: 0.6411607076984471
Epoch 1429/10000, Prediction Accuracy = 59.58846153846153%, Loss = 0.010067495803993482
Epoch: 1429, Batch Gradient Norm: 0.6631529828954835
Epoch: 1429, Batch Gradient Norm after: 0.6631529828954835
Epoch 1430/10000, Prediction Accuracy = 59.12692307692308%, Loss = 0.010141499483814606
Epoch: 1430, Batch Gradient Norm: 0.6589596762336493
Epoch: 1430, Batch Gradient Norm after: 0.6589596762336493
Epoch 1431/10000, Prediction Accuracy = 59.330769230769235%, Loss = 0.010124918574897142
Epoch: 1431, Batch Gradient Norm: 0.6314527010526314
Epoch: 1431, Batch Gradient Norm after: 0.6314527010526314
Epoch 1432/10000, Prediction Accuracy = 59.42307692307692%, Loss = 0.010079121646972803
Epoch: 1432, Batch Gradient Norm: 0.6335150563790727
Epoch: 1432, Batch Gradient Norm after: 0.6335150563790727
Epoch 1433/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010114851479346935
Epoch: 1433, Batch Gradient Norm: 0.6424780691189159
Epoch: 1433, Batch Gradient Norm after: 0.6424780691189159
Epoch 1434/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.010143357114149975
Epoch: 1434, Batch Gradient Norm: 0.6418495818969732
Epoch: 1434, Batch Gradient Norm after: 0.6418495818969732
Epoch 1435/10000, Prediction Accuracy = 59.03076923076923%, Loss = 0.010125386671951184
Epoch: 1435, Batch Gradient Norm: 0.6252746826019568
Epoch: 1435, Batch Gradient Norm after: 0.6252746826019568
Epoch 1436/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010099600499066023
Epoch: 1436, Batch Gradient Norm: 0.6492587602190392
Epoch: 1436, Batch Gradient Norm after: 0.6492587602190392
Epoch 1437/10000, Prediction Accuracy = 59.18076923076923%, Loss = 0.010133920165781792
Epoch: 1437, Batch Gradient Norm: 0.6317957498172481
Epoch: 1437, Batch Gradient Norm after: 0.6317957498172481
Epoch 1438/10000, Prediction Accuracy = 59.54230769230769%, Loss = 0.010081561855398692
Epoch: 1438, Batch Gradient Norm: 0.652529739983055
Epoch: 1438, Batch Gradient Norm after: 0.652529739983055
Epoch 1439/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.01010593155828806
Epoch: 1439, Batch Gradient Norm: 0.6393581773368942
Epoch: 1439, Batch Gradient Norm after: 0.6393581773368942
Epoch 1440/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.0100875307734196
Epoch: 1440, Batch Gradient Norm: 0.6388047825053537
Epoch: 1440, Batch Gradient Norm after: 0.6388047825053537
Epoch 1441/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010065535513254313
Epoch: 1441, Batch Gradient Norm: 0.6356799752114807
Epoch: 1441, Batch Gradient Norm after: 0.6356799752114807
Epoch 1442/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010113270666736823
Epoch: 1442, Batch Gradient Norm: 0.6383012008053465
Epoch: 1442, Batch Gradient Norm after: 0.6383012008053465
Epoch 1443/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010070133237884594
Epoch: 1443, Batch Gradient Norm: 0.6362718530929297
Epoch: 1443, Batch Gradient Norm after: 0.6362718530929297
Epoch 1444/10000, Prediction Accuracy = 59.30769230769231%, Loss = 0.010090113689119998
Epoch: 1444, Batch Gradient Norm: 0.6365542555064017
Epoch: 1444, Batch Gradient Norm after: 0.6365542555064017
Epoch 1445/10000, Prediction Accuracy = 59.388461538461556%, Loss = 0.010111331724776672
Epoch: 1445, Batch Gradient Norm: 0.6527498967957307
Epoch: 1445, Batch Gradient Norm after: 0.6527498967957307
Epoch 1446/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.010103751045580093
Epoch: 1446, Batch Gradient Norm: 0.6423570393421398
Epoch: 1446, Batch Gradient Norm after: 0.6423570393421398
Epoch 1447/10000, Prediction Accuracy = 59.284615384615385%, Loss = 0.01015056691204126
Epoch: 1447, Batch Gradient Norm: 0.6503216774645806
Epoch: 1447, Batch Gradient Norm after: 0.6503216774645806
Epoch 1448/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010071472121545902
Epoch: 1448, Batch Gradient Norm: 0.6486390254034368
Epoch: 1448, Batch Gradient Norm after: 0.6486390254034368
Epoch 1449/10000, Prediction Accuracy = 59.08846153846154%, Loss = 0.010133132051963072
Epoch: 1449, Batch Gradient Norm: 0.6292000697540103
Epoch: 1449, Batch Gradient Norm after: 0.6292000697540103
Epoch 1450/10000, Prediction Accuracy = 59.67307692307692%, Loss = 0.01009129618222897
Epoch: 1450, Batch Gradient Norm: 0.6528667563711316
Epoch: 1450, Batch Gradient Norm after: 0.6528667563711316
Epoch 1451/10000, Prediction Accuracy = 59.45384615384616%, Loss = 0.010103255295409607
Epoch: 1451, Batch Gradient Norm: 0.634574469719924
Epoch: 1451, Batch Gradient Norm after: 0.634574469719924
Epoch 1452/10000, Prediction Accuracy = 59.38461538461539%, Loss = 0.010073021699029665
Epoch: 1452, Batch Gradient Norm: 0.6434796254389278
Epoch: 1452, Batch Gradient Norm after: 0.6434796254389278
Epoch 1453/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.01011528898603641
Epoch: 1453, Batch Gradient Norm: 0.6360695038530165
Epoch: 1453, Batch Gradient Norm after: 0.6360695038530165
Epoch 1454/10000, Prediction Accuracy = 59.56538461538462%, Loss = 0.010054944871136775
Epoch: 1454, Batch Gradient Norm: 0.637560622586626
Epoch: 1454, Batch Gradient Norm after: 0.637560622586626
Epoch 1455/10000, Prediction Accuracy = 59.30384615384616%, Loss = 0.010086303003705464
Epoch: 1455, Batch Gradient Norm: 0.6611571238539684
Epoch: 1455, Batch Gradient Norm after: 0.6611571238539684
Epoch 1456/10000, Prediction Accuracy = 59.22307692307693%, Loss = 0.010176535624151047
Epoch: 1456, Batch Gradient Norm: 0.6351395379043956
Epoch: 1456, Batch Gradient Norm after: 0.6351395379043956
Epoch 1457/10000, Prediction Accuracy = 59.21923076923076%, Loss = 0.010079876519739628
Epoch: 1457, Batch Gradient Norm: 0.6407410488314821
Epoch: 1457, Batch Gradient Norm after: 0.6407410488314821
Epoch 1458/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010100244472806271
Epoch: 1458, Batch Gradient Norm: 0.6550609554043123
Epoch: 1458, Batch Gradient Norm after: 0.6550609554043123
Epoch 1459/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010143911322722068
Epoch: 1459, Batch Gradient Norm: 0.6290479053642273
Epoch: 1459, Batch Gradient Norm after: 0.6290479053642273
Epoch 1460/10000, Prediction Accuracy = 59.446153846153834%, Loss = 0.010042342643898267
Epoch: 1460, Batch Gradient Norm: 0.644475724552432
Epoch: 1460, Batch Gradient Norm after: 0.644475724552432
Epoch 1461/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010094332580383007
Epoch: 1461, Batch Gradient Norm: 0.6473061873892126
Epoch: 1461, Batch Gradient Norm after: 0.6473061873892126
Epoch 1462/10000, Prediction Accuracy = 59.56923076923078%, Loss = 0.010070021837376632
Epoch: 1462, Batch Gradient Norm: 0.6372837655477346
Epoch: 1462, Batch Gradient Norm after: 0.6372837655477346
Epoch 1463/10000, Prediction Accuracy = 59.69230769230769%, Loss = 0.010070085597152893
Epoch: 1463, Batch Gradient Norm: 0.640328771126496
Epoch: 1463, Batch Gradient Norm after: 0.640328771126496
Epoch 1464/10000, Prediction Accuracy = 59.18076923076923%, Loss = 0.010113150955965886
Epoch: 1464, Batch Gradient Norm: 0.6443231316671155
Epoch: 1464, Batch Gradient Norm after: 0.6443231316671155
Epoch 1465/10000, Prediction Accuracy = 59.28076923076924%, Loss = 0.01013958790841011
Epoch: 1465, Batch Gradient Norm: 0.628166385894872
Epoch: 1465, Batch Gradient Norm after: 0.628166385894872
Epoch 1466/10000, Prediction Accuracy = 59.48846153846153%, Loss = 0.010076005943119526
Epoch: 1466, Batch Gradient Norm: 0.647696572529316
Epoch: 1466, Batch Gradient Norm after: 0.647696572529316
Epoch 1467/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010070985827881556
Epoch: 1467, Batch Gradient Norm: 0.6440039137924871
Epoch: 1467, Batch Gradient Norm after: 0.6440039137924871
Epoch 1468/10000, Prediction Accuracy = 59.21153846153846%, Loss = 0.010139027037299596
Epoch: 1468, Batch Gradient Norm: 0.6309174998650848
Epoch: 1468, Batch Gradient Norm after: 0.6309174998650848
Epoch 1469/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010066739570062894
Epoch: 1469, Batch Gradient Norm: 0.6543331996136222
Epoch: 1469, Batch Gradient Norm after: 0.6543331996136222
Epoch 1470/10000, Prediction Accuracy = 59.123076923076916%, Loss = 0.010180634231521534
Epoch: 1470, Batch Gradient Norm: 0.6299734384187514
Epoch: 1470, Batch Gradient Norm after: 0.6299734384187514
Epoch 1471/10000, Prediction Accuracy = 59.48461538461539%, Loss = 0.010073898861614557
Epoch: 1471, Batch Gradient Norm: 0.649531233241055
Epoch: 1471, Batch Gradient Norm after: 0.649531233241055
Epoch 1472/10000, Prediction Accuracy = 59.3076923076923%, Loss = 0.010144222456102189
Epoch: 1472, Batch Gradient Norm: 0.6365503125725359
Epoch: 1472, Batch Gradient Norm after: 0.6365503125725359
Epoch 1473/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010054698428855492
Epoch: 1473, Batch Gradient Norm: 0.6254343127303486
Epoch: 1473, Batch Gradient Norm after: 0.6254343127303486
Epoch 1474/10000, Prediction Accuracy = 59.66538461538461%, Loss = 0.010036968339521151
Epoch: 1474, Batch Gradient Norm: 0.6672149639842574
Epoch: 1474, Batch Gradient Norm after: 0.6672149639842574
Epoch 1475/10000, Prediction Accuracy = 59.115384615384606%, Loss = 0.010156325924281891
Epoch: 1475, Batch Gradient Norm: 0.6443242336050425
Epoch: 1475, Batch Gradient Norm after: 0.6443242336050425
Epoch 1476/10000, Prediction Accuracy = 59.26153846153845%, Loss = 0.010141226606300244
Epoch: 1476, Batch Gradient Norm: 0.6707651320723019
Epoch: 1476, Batch Gradient Norm after: 0.6707651320723019
Epoch 1477/10000, Prediction Accuracy = 59.18076923076924%, Loss = 0.010100736927527647
Epoch: 1477, Batch Gradient Norm: 0.6367556892418824
Epoch: 1477, Batch Gradient Norm after: 0.6367556892418824
Epoch 1478/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010059566523593206
Epoch: 1478, Batch Gradient Norm: 0.6230801581677541
Epoch: 1478, Batch Gradient Norm after: 0.6230801581677541
Epoch 1479/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.01006147673783394
Epoch: 1479, Batch Gradient Norm: 0.6527017134355922
Epoch: 1479, Batch Gradient Norm after: 0.6527017134355922
Epoch 1480/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010103367627240144
Epoch: 1480, Batch Gradient Norm: 0.6511553477113146
Epoch: 1480, Batch Gradient Norm after: 0.6511553477113146
Epoch 1481/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010130111128091812
Epoch: 1481, Batch Gradient Norm: 0.632857057846474
Epoch: 1481, Batch Gradient Norm after: 0.632857057846474
Epoch 1482/10000, Prediction Accuracy = 59.550000000000004%, Loss = 0.01008022053597065
Epoch: 1482, Batch Gradient Norm: 0.651974825190695
Epoch: 1482, Batch Gradient Norm after: 0.651974825190695
Epoch 1483/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.01010403846605466
Epoch: 1483, Batch Gradient Norm: 0.6378326047691827
Epoch: 1483, Batch Gradient Norm after: 0.6378326047691827
Epoch 1484/10000, Prediction Accuracy = 59.70384615384616%, Loss = 0.010083437539063968
Epoch: 1484, Batch Gradient Norm: 0.6453179459757683
Epoch: 1484, Batch Gradient Norm after: 0.6453179459757683
Epoch 1485/10000, Prediction Accuracy = 59.52692307692307%, Loss = 0.010104518527021775
Epoch: 1485, Batch Gradient Norm: 0.631455127339319
Epoch: 1485, Batch Gradient Norm after: 0.631455127339319
Epoch 1486/10000, Prediction Accuracy = 59.26153846153847%, Loss = 0.010115602268622471
Epoch: 1486, Batch Gradient Norm: 0.6435874756449784
Epoch: 1486, Batch Gradient Norm after: 0.6435874756449784
Epoch 1487/10000, Prediction Accuracy = 59.47692307692308%, Loss = 0.01008024188474967
Epoch: 1487, Batch Gradient Norm: 0.6471147570418693
Epoch: 1487, Batch Gradient Norm after: 0.6471147570418693
Epoch 1488/10000, Prediction Accuracy = 59.38461538461539%, Loss = 0.010141463735355781
Epoch: 1488, Batch Gradient Norm: 0.650177115889019
Epoch: 1488, Batch Gradient Norm after: 0.650177115889019
Epoch 1489/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.01010344664637859
Epoch: 1489, Batch Gradient Norm: 0.6548090370339831
Epoch: 1489, Batch Gradient Norm after: 0.6548090370339831
Epoch 1490/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.01011687244933385
Epoch: 1490, Batch Gradient Norm: 0.6272717158599643
Epoch: 1490, Batch Gradient Norm after: 0.6272717158599643
Epoch 1491/10000, Prediction Accuracy = 59.60000000000001%, Loss = 0.010080193241055194
Epoch: 1491, Batch Gradient Norm: 0.6381273537340238
Epoch: 1491, Batch Gradient Norm after: 0.6381273537340238
Epoch 1492/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.010071673143941622
Epoch: 1492, Batch Gradient Norm: 0.6414759354735498
Epoch: 1492, Batch Gradient Norm after: 0.6414759354735498
Epoch 1493/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.010069701175850172
Epoch: 1493, Batch Gradient Norm: 0.6385771176655546
Epoch: 1493, Batch Gradient Norm after: 0.6385771176655546
Epoch 1494/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010100745882552404
Epoch: 1494, Batch Gradient Norm: 0.6382093914945854
Epoch: 1494, Batch Gradient Norm after: 0.6382093914945854
Epoch 1495/10000, Prediction Accuracy = 59.30769230769231%, Loss = 0.010108681610570503
Epoch: 1495, Batch Gradient Norm: 0.6290744056636487
Epoch: 1495, Batch Gradient Norm after: 0.6290744056636487
Epoch 1496/10000, Prediction Accuracy = 59.59615384615385%, Loss = 0.010064684642622104
Epoch: 1496, Batch Gradient Norm: 0.6285786611725831
Epoch: 1496, Batch Gradient Norm after: 0.6285786611725831
Epoch 1497/10000, Prediction Accuracy = 59.45384615384616%, Loss = 0.010023061902477192
Epoch: 1497, Batch Gradient Norm: 0.6362343541156491
Epoch: 1497, Batch Gradient Norm after: 0.6362343541156491
Epoch 1498/10000, Prediction Accuracy = 59.423076923076934%, Loss = 0.010114713070484308
Epoch: 1498, Batch Gradient Norm: 0.654261001559286
Epoch: 1498, Batch Gradient Norm after: 0.654261001559286
Epoch 1499/10000, Prediction Accuracy = 59.41923076923077%, Loss = 0.010110437655105041
Epoch: 1499, Batch Gradient Norm: 0.6410996054093708
Epoch: 1499, Batch Gradient Norm after: 0.6410996054093708
Epoch 1500/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.010066591633053927
Epoch: 1500, Batch Gradient Norm: 0.6513314504074771
Epoch: 1500, Batch Gradient Norm after: 0.6513314504074771
Epoch 1501/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010103024613971893
Epoch: 1501, Batch Gradient Norm: 0.6349393506515287
Epoch: 1501, Batch Gradient Norm after: 0.6349393506515287
Epoch 1502/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010078275934434854
Epoch: 1502, Batch Gradient Norm: 0.6506155067146465
Epoch: 1502, Batch Gradient Norm after: 0.6506155067146465
Epoch 1503/10000, Prediction Accuracy = 59.62692307692306%, Loss = 0.01007858756929636
Epoch: 1503, Batch Gradient Norm: 0.6289634060213057
Epoch: 1503, Batch Gradient Norm after: 0.6289634060213057
Epoch 1504/10000, Prediction Accuracy = 59.342307692307685%, Loss = 0.010072883791648425
Epoch: 1504, Batch Gradient Norm: 0.6465437203420408
Epoch: 1504, Batch Gradient Norm after: 0.6465437203420408
Epoch 1505/10000, Prediction Accuracy = 59.51923076923078%, Loss = 0.010085198025290783
Epoch: 1505, Batch Gradient Norm: 0.6562228014966891
Epoch: 1505, Batch Gradient Norm after: 0.6562228014966891
Epoch 1506/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010085515892849518
Epoch: 1506, Batch Gradient Norm: 0.6246042472220859
Epoch: 1506, Batch Gradient Norm after: 0.6246042472220859
Epoch 1507/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010077084343020733
Epoch: 1507, Batch Gradient Norm: 0.6473561545458534
Epoch: 1507, Batch Gradient Norm after: 0.6473561545458534
Epoch 1508/10000, Prediction Accuracy = 59.284615384615385%, Loss = 0.010075308597431732
Epoch: 1508, Batch Gradient Norm: 0.6664136665603957
Epoch: 1508, Batch Gradient Norm after: 0.6664136665603957
Epoch 1509/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.010075321062826194
Epoch: 1509, Batch Gradient Norm: 0.6431113352397273
Epoch: 1509, Batch Gradient Norm after: 0.6431113352397273
Epoch 1510/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.01010222933613337
Epoch: 1510, Batch Gradient Norm: 0.6255673380585822
Epoch: 1510, Batch Gradient Norm after: 0.6255673380585822
Epoch 1511/10000, Prediction Accuracy = 59.369230769230775%, Loss = 0.010101727926387237
Epoch: 1511, Batch Gradient Norm: 0.6337377278966607
Epoch: 1511, Batch Gradient Norm after: 0.6337377278966607
Epoch 1512/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010041079842127286
Epoch: 1512, Batch Gradient Norm: 0.6523756705871018
Epoch: 1512, Batch Gradient Norm after: 0.6523756705871018
Epoch 1513/10000, Prediction Accuracy = 59.53461538461538%, Loss = 0.010096864273341803
Epoch: 1513, Batch Gradient Norm: 0.6295891546733897
Epoch: 1513, Batch Gradient Norm after: 0.6295891546733897
Epoch 1514/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.01007421429340656
Epoch: 1514, Batch Gradient Norm: 0.6469746849832548
Epoch: 1514, Batch Gradient Norm after: 0.6469746849832548
Epoch 1515/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010096581366199713
Epoch: 1515, Batch Gradient Norm: 0.6373824447318067
Epoch: 1515, Batch Gradient Norm after: 0.6373824447318067
Epoch 1516/10000, Prediction Accuracy = 59.31923076923076%, Loss = 0.010063186359520141
Epoch: 1516, Batch Gradient Norm: 0.6528383863720806
Epoch: 1516, Batch Gradient Norm after: 0.6528383863720806
Epoch 1517/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010087769908400683
Epoch: 1517, Batch Gradient Norm: 0.6369035978703084
Epoch: 1517, Batch Gradient Norm after: 0.6369035978703084
Epoch 1518/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010055418054644879
Epoch: 1518, Batch Gradient Norm: 0.6376331621870767
Epoch: 1518, Batch Gradient Norm after: 0.6376331621870767
Epoch 1519/10000, Prediction Accuracy = 59.580769230769235%, Loss = 0.010082631156994747
Epoch: 1519, Batch Gradient Norm: 0.6333614347240526
Epoch: 1519, Batch Gradient Norm after: 0.6333614347240526
Epoch 1520/10000, Prediction Accuracy = 59.33846153846153%, Loss = 0.010064014806770362
Epoch: 1520, Batch Gradient Norm: 0.6391510574029359
Epoch: 1520, Batch Gradient Norm after: 0.6391510574029359
Epoch 1521/10000, Prediction Accuracy = 59.25%, Loss = 0.010117780703764696
Epoch: 1521, Batch Gradient Norm: 0.6387398895923941
Epoch: 1521, Batch Gradient Norm after: 0.6387398895923941
Epoch 1522/10000, Prediction Accuracy = 59.23846153846153%, Loss = 0.01008384545835165
Epoch: 1522, Batch Gradient Norm: 0.6371629225379045
Epoch: 1522, Batch Gradient Norm after: 0.6371629225379045
Epoch 1523/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010097774390417796
Epoch: 1523, Batch Gradient Norm: 0.6405244500425447
Epoch: 1523, Batch Gradient Norm after: 0.6405244500425447
Epoch 1524/10000, Prediction Accuracy = 59.43461538461539%, Loss = 0.010115496527690154
Epoch: 1524, Batch Gradient Norm: 0.635257012005107
Epoch: 1524, Batch Gradient Norm after: 0.635257012005107
Epoch 1525/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010085125023928972
Epoch: 1525, Batch Gradient Norm: 0.6395407556864626
Epoch: 1525, Batch Gradient Norm after: 0.6395407556864626
Epoch 1526/10000, Prediction Accuracy = 59.111538461538466%, Loss = 0.01010412393281093
Epoch: 1526, Batch Gradient Norm: 0.6458013853419738
Epoch: 1526, Batch Gradient Norm after: 0.6458013853419738
Epoch 1527/10000, Prediction Accuracy = 59.29230769230769%, Loss = 0.01011065816363463
Epoch: 1527, Batch Gradient Norm: 0.6400584426723235
Epoch: 1527, Batch Gradient Norm after: 0.6400584426723235
Epoch 1528/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.010095652837592822
Epoch: 1528, Batch Gradient Norm: 0.6345956110239027
Epoch: 1528, Batch Gradient Norm after: 0.6345956110239027
Epoch 1529/10000, Prediction Accuracy = 59.3423076923077%, Loss = 0.01009616212776074
Epoch: 1529, Batch Gradient Norm: 0.6341915358674948
Epoch: 1529, Batch Gradient Norm after: 0.6341915358674948
Epoch 1530/10000, Prediction Accuracy = 59.5%, Loss = 0.010089571301180583
Epoch: 1530, Batch Gradient Norm: 0.6532366976229887
Epoch: 1530, Batch Gradient Norm after: 0.6532366976229887
Epoch 1531/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010106137523857446
Epoch: 1531, Batch Gradient Norm: 0.6434100913682056
Epoch: 1531, Batch Gradient Norm after: 0.6434100913682056
Epoch 1532/10000, Prediction Accuracy = 59.226923076923086%, Loss = 0.010094206421994247
Epoch: 1532, Batch Gradient Norm: 0.6474944380440422
Epoch: 1532, Batch Gradient Norm after: 0.6474944380440422
Epoch 1533/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010101794408491025
Epoch: 1533, Batch Gradient Norm: 0.6479975925996918
Epoch: 1533, Batch Gradient Norm after: 0.6479975925996918
Epoch 1534/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010108869379529586
Epoch: 1534, Batch Gradient Norm: 0.662508336630109
Epoch: 1534, Batch Gradient Norm after: 0.662508336630109
Epoch 1535/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.01017266153716124
Epoch: 1535, Batch Gradient Norm: 0.6321939284838217
Epoch: 1535, Batch Gradient Norm after: 0.6321939284838217
Epoch 1536/10000, Prediction Accuracy = 59.5923076923077%, Loss = 0.010074682032259611
Epoch: 1536, Batch Gradient Norm: 0.6536709433315941
Epoch: 1536, Batch Gradient Norm after: 0.6536709433315941
Epoch 1537/10000, Prediction Accuracy = 59.51923076923078%, Loss = 0.010078800627245353
Epoch: 1537, Batch Gradient Norm: 0.651720589850791
Epoch: 1537, Batch Gradient Norm after: 0.651720589850791
Epoch 1538/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010070103793763198
Epoch: 1538, Batch Gradient Norm: 0.6482920819583846
Epoch: 1538, Batch Gradient Norm after: 0.6482920819583846
Epoch 1539/10000, Prediction Accuracy = 59.24615384615384%, Loss = 0.010143263838612117
Epoch: 1539, Batch Gradient Norm: 0.6252034840127731
Epoch: 1539, Batch Gradient Norm after: 0.6252034840127731
Epoch 1540/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.010009018417734366
Epoch: 1540, Batch Gradient Norm: 0.6421381921114372
Epoch: 1540, Batch Gradient Norm after: 0.6421381921114372
Epoch 1541/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010118119275340667
Epoch: 1541, Batch Gradient Norm: 0.6482717725447598
Epoch: 1541, Batch Gradient Norm after: 0.6482717725447598
Epoch 1542/10000, Prediction Accuracy = 59.599999999999994%, Loss = 0.01010537133193933
Epoch: 1542, Batch Gradient Norm: 0.6501721614411353
Epoch: 1542, Batch Gradient Norm after: 0.6501721614411353
Epoch 1543/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.01008573998338901
Epoch: 1543, Batch Gradient Norm: 0.6523268123872855
Epoch: 1543, Batch Gradient Norm after: 0.6523268123872855
Epoch 1544/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010111431806133343
Epoch: 1544, Batch Gradient Norm: 0.6451559166646391
Epoch: 1544, Batch Gradient Norm after: 0.6451559166646391
Epoch 1545/10000, Prediction Accuracy = 59.47692307692307%, Loss = 0.010136060201777862
Epoch: 1545, Batch Gradient Norm: 0.646787137256549
Epoch: 1545, Batch Gradient Norm after: 0.646787137256549
Epoch 1546/10000, Prediction Accuracy = 59.476923076923086%, Loss = 0.010088731892980062
Epoch: 1546, Batch Gradient Norm: 0.6446353305591564
Epoch: 1546, Batch Gradient Norm after: 0.6446353305591564
Epoch 1547/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010050360901424518
Epoch: 1547, Batch Gradient Norm: 0.6475658172328375
Epoch: 1547, Batch Gradient Norm after: 0.6475658172328375
Epoch 1548/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.010079859111171503
Epoch: 1548, Batch Gradient Norm: 0.6411246109561645
Epoch: 1548, Batch Gradient Norm after: 0.6411246109561645
Epoch 1549/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010078251361846924
Epoch: 1549, Batch Gradient Norm: 0.6343360110577004
Epoch: 1549, Batch Gradient Norm after: 0.6343360110577004
Epoch 1550/10000, Prediction Accuracy = 59.26538461538462%, Loss = 0.010087922287101928
Epoch: 1550, Batch Gradient Norm: 0.641123156685686
Epoch: 1550, Batch Gradient Norm after: 0.641123156685686
Epoch 1551/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010085041849659039
Epoch: 1551, Batch Gradient Norm: 0.6315170910608572
Epoch: 1551, Batch Gradient Norm after: 0.6315170910608572
Epoch 1552/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010104568388599616
Epoch: 1552, Batch Gradient Norm: 0.6459394419824754
Epoch: 1552, Batch Gradient Norm after: 0.6459394419824754
Epoch 1553/10000, Prediction Accuracy = 59.303846153846166%, Loss = 0.01009317895827385
Epoch: 1553, Batch Gradient Norm: 0.6317439632815708
Epoch: 1553, Batch Gradient Norm after: 0.6317439632815708
Epoch 1554/10000, Prediction Accuracy = 59.342307692307706%, Loss = 0.010076047351153998
Epoch: 1554, Batch Gradient Norm: 0.6342409569466
Epoch: 1554, Batch Gradient Norm after: 0.6342409569466
Epoch 1555/10000, Prediction Accuracy = 59.28846153846155%, Loss = 0.010095499814129792
Epoch: 1555, Batch Gradient Norm: 0.6377452313068431
Epoch: 1555, Batch Gradient Norm after: 0.6377452313068431
Epoch 1556/10000, Prediction Accuracy = 59.23846153846154%, Loss = 0.010085182765928598
Epoch: 1556, Batch Gradient Norm: 0.6488267967008515
Epoch: 1556, Batch Gradient Norm after: 0.6488267967008515
Epoch 1557/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010113571913769612
Epoch: 1557, Batch Gradient Norm: 0.6502976403693675
Epoch: 1557, Batch Gradient Norm after: 0.6502976403693675
Epoch 1558/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010104539660880199
Epoch: 1558, Batch Gradient Norm: 0.6389800641024214
Epoch: 1558, Batch Gradient Norm after: 0.6389800641024214
Epoch 1559/10000, Prediction Accuracy = 59.638461538461556%, Loss = 0.01007534355784838
Epoch: 1559, Batch Gradient Norm: 0.6340189327313431
Epoch: 1559, Batch Gradient Norm after: 0.6340189327313431
Epoch 1560/10000, Prediction Accuracy = 59.58076923076923%, Loss = 0.010047054706284633
Epoch: 1560, Batch Gradient Norm: 0.629006486496004
Epoch: 1560, Batch Gradient Norm after: 0.629006486496004
Epoch 1561/10000, Prediction Accuracy = 59.46153846153847%, Loss = 0.010077868158427568
Epoch: 1561, Batch Gradient Norm: 0.6386183496881166
Epoch: 1561, Batch Gradient Norm after: 0.6386183496881166
Epoch 1562/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010075408320587415
Epoch: 1562, Batch Gradient Norm: 0.6306398260310938
Epoch: 1562, Batch Gradient Norm after: 0.6306398260310938
Epoch 1563/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.010088064564535251
Epoch: 1563, Batch Gradient Norm: 0.6384147521483498
Epoch: 1563, Batch Gradient Norm after: 0.6384147521483498
Epoch 1564/10000, Prediction Accuracy = 59.24230769230768%, Loss = 0.010076341434166981
Epoch: 1564, Batch Gradient Norm: 0.6536695066768968
Epoch: 1564, Batch Gradient Norm after: 0.6536695066768968
Epoch 1565/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.01014835093743526
Epoch: 1565, Batch Gradient Norm: 0.6474719641569872
Epoch: 1565, Batch Gradient Norm after: 0.6474719641569872
Epoch 1566/10000, Prediction Accuracy = 59.50384615384616%, Loss = 0.010103376080783514
Epoch: 1566, Batch Gradient Norm: 0.6304607173341416
Epoch: 1566, Batch Gradient Norm after: 0.6304607173341416
Epoch 1567/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010061642656532617
Epoch: 1567, Batch Gradient Norm: 0.6487866512574816
Epoch: 1567, Batch Gradient Norm after: 0.6487866512574816
Epoch 1568/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.010118279964304887
Epoch: 1568, Batch Gradient Norm: 0.6357210251800028
Epoch: 1568, Batch Gradient Norm after: 0.6357210251800028
Epoch 1569/10000, Prediction Accuracy = 59.31153846153846%, Loss = 0.010088224107256303
Epoch: 1569, Batch Gradient Norm: 0.6366380858867403
Epoch: 1569, Batch Gradient Norm after: 0.6366380858867403
Epoch 1570/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010082494754057664
Epoch: 1570, Batch Gradient Norm: 0.6331500403870304
Epoch: 1570, Batch Gradient Norm after: 0.6331500403870304
Epoch 1571/10000, Prediction Accuracy = 59.19615384615384%, Loss = 0.010142997480355777
Epoch: 1571, Batch Gradient Norm: 0.6392428834226047
Epoch: 1571, Batch Gradient Norm after: 0.6392428834226047
Epoch 1572/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.01008407986507966
Epoch: 1572, Batch Gradient Norm: 0.6551839462791349
Epoch: 1572, Batch Gradient Norm after: 0.6551839462791349
Epoch 1573/10000, Prediction Accuracy = 59.46923076923076%, Loss = 0.010122817797729602
Epoch: 1573, Batch Gradient Norm: 0.6307741893854693
Epoch: 1573, Batch Gradient Norm after: 0.6307741893854693
Epoch 1574/10000, Prediction Accuracy = 59.576923076923066%, Loss = 0.010043192654848099
Epoch: 1574, Batch Gradient Norm: 0.6518184417217243
Epoch: 1574, Batch Gradient Norm after: 0.6518184417217243
Epoch 1575/10000, Prediction Accuracy = 59.32692307692309%, Loss = 0.010088538034604145
Epoch: 1575, Batch Gradient Norm: 0.6312641857622445
Epoch: 1575, Batch Gradient Norm after: 0.6312641857622445
Epoch 1576/10000, Prediction Accuracy = 59.69615384615384%, Loss = 0.010040894294014344
Epoch: 1576, Batch Gradient Norm: 0.6229228769205423
Epoch: 1576, Batch Gradient Norm after: 0.6229228769205423
Epoch 1577/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010040524200751232
Epoch: 1577, Batch Gradient Norm: 0.6323939121548936
Epoch: 1577, Batch Gradient Norm after: 0.6323939121548936
Epoch 1578/10000, Prediction Accuracy = 59.638461538461534%, Loss = 0.010057080823641557
Epoch: 1578, Batch Gradient Norm: 0.6368455133411609
Epoch: 1578, Batch Gradient Norm after: 0.6368455133411609
Epoch 1579/10000, Prediction Accuracy = 59.50384615384615%, Loss = 0.010055296266308198
Epoch: 1579, Batch Gradient Norm: 0.650439884727232
Epoch: 1579, Batch Gradient Norm after: 0.650439884727232
Epoch 1580/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010071819934707422
Epoch: 1580, Batch Gradient Norm: 0.6331553644434106
Epoch: 1580, Batch Gradient Norm after: 0.6331553644434106
Epoch 1581/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010080759270259967
Epoch: 1581, Batch Gradient Norm: 0.631490885041178
Epoch: 1581, Batch Gradient Norm after: 0.631490885041178
Epoch 1582/10000, Prediction Accuracy = 59.342307692307706%, Loss = 0.010108929628936144
Epoch: 1582, Batch Gradient Norm: 0.6379544685166502
Epoch: 1582, Batch Gradient Norm after: 0.6379544685166502
Epoch 1583/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010085612463836487
Epoch: 1583, Batch Gradient Norm: 0.6364653356114243
Epoch: 1583, Batch Gradient Norm after: 0.6364653356114243
Epoch 1584/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.01006756966503767
Epoch: 1584, Batch Gradient Norm: 0.6401714288508039
Epoch: 1584, Batch Gradient Norm after: 0.6401714288508039
Epoch 1585/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010107198156989537
Epoch: 1585, Batch Gradient Norm: 0.6396916865239206
Epoch: 1585, Batch Gradient Norm after: 0.6396916865239206
Epoch 1586/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010090783166770752
Epoch: 1586, Batch Gradient Norm: 0.6384531852823354
Epoch: 1586, Batch Gradient Norm after: 0.6384531852823354
Epoch 1587/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.01008945144712925
Epoch: 1587, Batch Gradient Norm: 0.6449381002015748
Epoch: 1587, Batch Gradient Norm after: 0.6449381002015748
Epoch 1588/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010059219785034657
Epoch: 1588, Batch Gradient Norm: 0.6487009870514718
Epoch: 1588, Batch Gradient Norm after: 0.6487009870514718
Epoch 1589/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.010083406375577817
Epoch: 1589, Batch Gradient Norm: 0.6453173488597648
Epoch: 1589, Batch Gradient Norm after: 0.6453173488597648
Epoch 1590/10000, Prediction Accuracy = 59.55384615384616%, Loss = 0.010088691201347571
Epoch: 1590, Batch Gradient Norm: 0.6345056733824375
Epoch: 1590, Batch Gradient Norm after: 0.6345056733824375
Epoch 1591/10000, Prediction Accuracy = 59.55%, Loss = 0.010045845777942585
Epoch: 1591, Batch Gradient Norm: 0.6372399795622341
Epoch: 1591, Batch Gradient Norm after: 0.6372399795622341
Epoch 1592/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010116148453492384
Epoch: 1592, Batch Gradient Norm: 0.6330281537370316
Epoch: 1592, Batch Gradient Norm after: 0.6330281537370316
Epoch 1593/10000, Prediction Accuracy = 59.45384615384615%, Loss = 0.010035288376876941
Epoch: 1593, Batch Gradient Norm: 0.6236847860680943
Epoch: 1593, Batch Gradient Norm after: 0.6236847860680943
Epoch 1594/10000, Prediction Accuracy = 59.50384615384617%, Loss = 0.010038751893891739
Epoch: 1594, Batch Gradient Norm: 0.641048373795425
Epoch: 1594, Batch Gradient Norm after: 0.641048373795425
Epoch 1595/10000, Prediction Accuracy = 59.469230769230776%, Loss = 0.01005912020515937
Epoch: 1595, Batch Gradient Norm: 0.642274900656076
Epoch: 1595, Batch Gradient Norm after: 0.642274900656076
Epoch 1596/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010126356966793537
Epoch: 1596, Batch Gradient Norm: 0.6296567682794499
Epoch: 1596, Batch Gradient Norm after: 0.6296567682794499
Epoch 1597/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010065974237827154
Epoch: 1597, Batch Gradient Norm: 0.6449018399225789
Epoch: 1597, Batch Gradient Norm after: 0.6449018399225789
Epoch 1598/10000, Prediction Accuracy = 59.40384615384616%, Loss = 0.010097435030799646
Epoch: 1598, Batch Gradient Norm: 0.6490217687677461
Epoch: 1598, Batch Gradient Norm after: 0.6490217687677461
Epoch 1599/10000, Prediction Accuracy = 59.5346153846154%, Loss = 0.010136787134867448
Epoch: 1599, Batch Gradient Norm: 0.6372744333770124
Epoch: 1599, Batch Gradient Norm after: 0.6372744333770124
Epoch 1600/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010091813639379464
Epoch: 1600, Batch Gradient Norm: 0.6392380138229563
Epoch: 1600, Batch Gradient Norm after: 0.6392380138229563
Epoch 1601/10000, Prediction Accuracy = 59.4923076923077%, Loss = 0.010052751176632367
Epoch: 1601, Batch Gradient Norm: 0.630086183342012
Epoch: 1601, Batch Gradient Norm after: 0.630086183342012
Epoch 1602/10000, Prediction Accuracy = 59.51923076923077%, Loss = 0.01013231069709246
Epoch: 1602, Batch Gradient Norm: 0.6363363297705781
Epoch: 1602, Batch Gradient Norm after: 0.6363363297705781
Epoch 1603/10000, Prediction Accuracy = 59.23461538461539%, Loss = 0.010065527991033517
Epoch: 1603, Batch Gradient Norm: 0.6453610325556224
Epoch: 1603, Batch Gradient Norm after: 0.6453610325556224
Epoch 1604/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.01005336893006013
Epoch: 1604, Batch Gradient Norm: 0.639793762577539
Epoch: 1604, Batch Gradient Norm after: 0.639793762577539
Epoch 1605/10000, Prediction Accuracy = 59.23076923076923%, Loss = 0.010094760200725151
Epoch: 1605, Batch Gradient Norm: 0.6425170269371412
Epoch: 1605, Batch Gradient Norm after: 0.6425170269371412
Epoch 1606/10000, Prediction Accuracy = 59.28846153846154%, Loss = 0.010095789885291686
Epoch: 1606, Batch Gradient Norm: 0.6546485054557957
Epoch: 1606, Batch Gradient Norm after: 0.6546485054557957
Epoch 1607/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010095045400353579
Epoch: 1607, Batch Gradient Norm: 0.6495870594146093
Epoch: 1607, Batch Gradient Norm after: 0.6495870594146093
Epoch 1608/10000, Prediction Accuracy = 59.55%, Loss = 0.010072634340478824
Epoch: 1608, Batch Gradient Norm: 0.6421608222930395
Epoch: 1608, Batch Gradient Norm after: 0.6421608222930395
Epoch 1609/10000, Prediction Accuracy = 59.215384615384615%, Loss = 0.010072935587511612
Epoch: 1609, Batch Gradient Norm: 0.6483872785402416
Epoch: 1609, Batch Gradient Norm after: 0.6483872785402416
Epoch 1610/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.0101287650804107
Epoch: 1610, Batch Gradient Norm: 0.6366510354905254
Epoch: 1610, Batch Gradient Norm after: 0.6366510354905254
Epoch 1611/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.01006851395448813
Epoch: 1611, Batch Gradient Norm: 0.659679456592169
Epoch: 1611, Batch Gradient Norm after: 0.659679456592169
Epoch 1612/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.010143721117996253
Epoch: 1612, Batch Gradient Norm: 0.6387763827122621
Epoch: 1612, Batch Gradient Norm after: 0.6387763827122621
Epoch 1613/10000, Prediction Accuracy = 59.342307692307685%, Loss = 0.010097367545733085
Epoch: 1613, Batch Gradient Norm: 0.6359599870414969
Epoch: 1613, Batch Gradient Norm after: 0.6359599870414969
Epoch 1614/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010043062771169039
Epoch: 1614, Batch Gradient Norm: 0.6402586899086258
Epoch: 1614, Batch Gradient Norm after: 0.6402586899086258
Epoch 1615/10000, Prediction Accuracy = 59.73076923076923%, Loss = 0.010075556400876779
Epoch: 1615, Batch Gradient Norm: 0.6587564772862111
Epoch: 1615, Batch Gradient Norm after: 0.6587564772862111
Epoch 1616/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010065998308933698
Epoch: 1616, Batch Gradient Norm: 0.6727749458361557
Epoch: 1616, Batch Gradient Norm after: 0.6727749458361557
Epoch 1617/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010137323361749832
Epoch: 1617, Batch Gradient Norm: 0.624970014007916
Epoch: 1617, Batch Gradient Norm after: 0.624970014007916
Epoch 1618/10000, Prediction Accuracy = 59.67692307692308%, Loss = 0.010010919175468959
Epoch: 1618, Batch Gradient Norm: 0.6350927443355684
Epoch: 1618, Batch Gradient Norm after: 0.6350927443355684
Epoch 1619/10000, Prediction Accuracy = 59.43846153846153%, Loss = 0.010099708317564083
Epoch: 1619, Batch Gradient Norm: 0.643103491582094
Epoch: 1619, Batch Gradient Norm after: 0.643103491582094
Epoch 1620/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010099080677788991
Epoch: 1620, Batch Gradient Norm: 0.6483076520790683
Epoch: 1620, Batch Gradient Norm after: 0.6483076520790683
Epoch 1621/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010060204192996025
Epoch: 1621, Batch Gradient Norm: 0.648481478881385
Epoch: 1621, Batch Gradient Norm after: 0.648481478881385
Epoch 1622/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.01013075073177998
Epoch: 1622, Batch Gradient Norm: 0.629592916483886
Epoch: 1622, Batch Gradient Norm after: 0.629592916483886
Epoch 1623/10000, Prediction Accuracy = 59.153846153846146%, Loss = 0.010095963039650368
Epoch: 1623, Batch Gradient Norm: 0.6359829193555362
Epoch: 1623, Batch Gradient Norm after: 0.6359829193555362
Epoch 1624/10000, Prediction Accuracy = 59.42307692307691%, Loss = 0.010104073139910515
Epoch: 1624, Batch Gradient Norm: 0.6454273560172735
Epoch: 1624, Batch Gradient Norm after: 0.6454273560172735
Epoch 1625/10000, Prediction Accuracy = 59.61153846153846%, Loss = 0.010047135229867239
Epoch: 1625, Batch Gradient Norm: 0.640578691735055
Epoch: 1625, Batch Gradient Norm after: 0.640578691735055
Epoch 1626/10000, Prediction Accuracy = 59.48461538461539%, Loss = 0.010055406448932795
Epoch: 1626, Batch Gradient Norm: 0.6496849599408558
Epoch: 1626, Batch Gradient Norm after: 0.6496849599408558
Epoch 1627/10000, Prediction Accuracy = 59.2%, Loss = 0.010090166344665565
Epoch: 1627, Batch Gradient Norm: 0.6534094790187904
Epoch: 1627, Batch Gradient Norm after: 0.6534094790187904
Epoch 1628/10000, Prediction Accuracy = 59.25769230769231%, Loss = 0.010103866601219544
Epoch: 1628, Batch Gradient Norm: 0.6365914827010576
Epoch: 1628, Batch Gradient Norm after: 0.6365914827010576
Epoch 1629/10000, Prediction Accuracy = 59.20384615384614%, Loss = 0.010087931958528666
Epoch: 1629, Batch Gradient Norm: 0.625971512931247
Epoch: 1629, Batch Gradient Norm after: 0.625971512931247
Epoch 1630/10000, Prediction Accuracy = 59.34615384615385%, Loss = 0.010084506195898239
Epoch: 1630, Batch Gradient Norm: 0.6421004234196633
Epoch: 1630, Batch Gradient Norm after: 0.6421004234196633
Epoch 1631/10000, Prediction Accuracy = 59.26538461538461%, Loss = 0.010092541073950438
Epoch: 1631, Batch Gradient Norm: 0.6535833102666174
Epoch: 1631, Batch Gradient Norm after: 0.6535833102666174
Epoch 1632/10000, Prediction Accuracy = 59.51923076923077%, Loss = 0.010115939335754285
Epoch: 1632, Batch Gradient Norm: 0.6487656962970634
Epoch: 1632, Batch Gradient Norm after: 0.6487656962970634
Epoch 1633/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010074341096557103
Epoch: 1633, Batch Gradient Norm: 0.6433703029591994
Epoch: 1633, Batch Gradient Norm after: 0.6433703029591994
Epoch 1634/10000, Prediction Accuracy = 59.26153846153847%, Loss = 0.010095689732294817
Epoch: 1634, Batch Gradient Norm: 0.6501612261168416
Epoch: 1634, Batch Gradient Norm after: 0.6501612261168416
Epoch 1635/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010124542535497593
Epoch: 1635, Batch Gradient Norm: 0.6332913546318086
Epoch: 1635, Batch Gradient Norm after: 0.6332913546318086
Epoch 1636/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010062957612367777
Epoch: 1636, Batch Gradient Norm: 0.6420073945894713
Epoch: 1636, Batch Gradient Norm after: 0.6420073945894713
Epoch 1637/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010043205550083747
Epoch: 1637, Batch Gradient Norm: 0.6233847091785373
Epoch: 1637, Batch Gradient Norm after: 0.6233847091785373
Epoch 1638/10000, Prediction Accuracy = 59.74230769230769%, Loss = 0.010071461805357384
Epoch: 1638, Batch Gradient Norm: 0.6337145477264828
Epoch: 1638, Batch Gradient Norm after: 0.6337145477264828
Epoch 1639/10000, Prediction Accuracy = 59.392307692307696%, Loss = 0.010142400932426635
Epoch: 1639, Batch Gradient Norm: 0.661863561172297
Epoch: 1639, Batch Gradient Norm after: 0.661863561172297
Epoch 1640/10000, Prediction Accuracy = 59.26923076923077%, Loss = 0.010138763759571772
Epoch: 1640, Batch Gradient Norm: 0.6405388224748532
Epoch: 1640, Batch Gradient Norm after: 0.6405388224748532
Epoch 1641/10000, Prediction Accuracy = 59.40384615384616%, Loss = 0.010108549792606097
Epoch: 1641, Batch Gradient Norm: 0.6394510455419683
Epoch: 1641, Batch Gradient Norm after: 0.6394510455419683
Epoch 1642/10000, Prediction Accuracy = 59.46153846153847%, Loss = 0.010133319104520174
Epoch: 1642, Batch Gradient Norm: 0.6328296813134311
Epoch: 1642, Batch Gradient Norm after: 0.6328296813134311
Epoch 1643/10000, Prediction Accuracy = 59.62307692307692%, Loss = 0.010080032838651767
Epoch: 1643, Batch Gradient Norm: 0.6388359326699445
Epoch: 1643, Batch Gradient Norm after: 0.6388359326699445
Epoch 1644/10000, Prediction Accuracy = 59.23076923076923%, Loss = 0.010124714328692509
Epoch: 1644, Batch Gradient Norm: 0.6440447282442961
Epoch: 1644, Batch Gradient Norm after: 0.6440447282442961
Epoch 1645/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.01012455034427918
Epoch: 1645, Batch Gradient Norm: 0.6449048719232008
Epoch: 1645, Batch Gradient Norm after: 0.6449048719232008
Epoch 1646/10000, Prediction Accuracy = 59.45%, Loss = 0.01007147735128036
Epoch: 1646, Batch Gradient Norm: 0.6384399299821227
Epoch: 1646, Batch Gradient Norm after: 0.6384399299821227
Epoch 1647/10000, Prediction Accuracy = 59.5923076923077%, Loss = 0.010058847972406791
Epoch: 1647, Batch Gradient Norm: 0.6459630642001342
Epoch: 1647, Batch Gradient Norm after: 0.6459630642001342
Epoch 1648/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010115538652126606
Epoch: 1648, Batch Gradient Norm: 0.6344812218691622
Epoch: 1648, Batch Gradient Norm after: 0.6344812218691622
Epoch 1649/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010110560303124098
Epoch: 1649, Batch Gradient Norm: 0.6396920799194444
Epoch: 1649, Batch Gradient Norm after: 0.6396920799194444
Epoch 1650/10000, Prediction Accuracy = 59.146153846153844%, Loss = 0.010081343066233855
Epoch: 1650, Batch Gradient Norm: 0.6320038189881119
Epoch: 1650, Batch Gradient Norm after: 0.6320038189881119
Epoch 1651/10000, Prediction Accuracy = 59.68846153846154%, Loss = 0.010075679335456628
Epoch: 1651, Batch Gradient Norm: 0.6645155619682155
Epoch: 1651, Batch Gradient Norm after: 0.6645155619682155
Epoch 1652/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.010129840543063788
Epoch: 1652, Batch Gradient Norm: 0.6391143551777886
Epoch: 1652, Batch Gradient Norm after: 0.6391143551777886
Epoch 1653/10000, Prediction Accuracy = 59.534615384615385%, Loss = 0.010043777023943571
Epoch: 1653, Batch Gradient Norm: 0.6264030269564984
Epoch: 1653, Batch Gradient Norm after: 0.6264030269564984
Epoch 1654/10000, Prediction Accuracy = 59.54615384615384%, Loss = 0.010049536752586182
Epoch: 1654, Batch Gradient Norm: 0.651593217828165
Epoch: 1654, Batch Gradient Norm after: 0.651593217828165
Epoch 1655/10000, Prediction Accuracy = 59.423076923076934%, Loss = 0.01010116540755217
Epoch: 1655, Batch Gradient Norm: 0.6286436016691063
Epoch: 1655, Batch Gradient Norm after: 0.6286436016691063
Epoch 1656/10000, Prediction Accuracy = 59.611538461538466%, Loss = 0.010073599119025927
Epoch: 1656, Batch Gradient Norm: 0.6523194511844653
Epoch: 1656, Batch Gradient Norm after: 0.6523194511844653
Epoch 1657/10000, Prediction Accuracy = 59.23461538461538%, Loss = 0.010153497354342388
Epoch: 1657, Batch Gradient Norm: 0.6513892491303036
Epoch: 1657, Batch Gradient Norm after: 0.6513892491303036
Epoch 1658/10000, Prediction Accuracy = 59.32692307692308%, Loss = 0.010119620065849561
Epoch: 1658, Batch Gradient Norm: 0.6454472629418425
Epoch: 1658, Batch Gradient Norm after: 0.6454472629418425
Epoch 1659/10000, Prediction Accuracy = 59.55769230769231%, Loss = 0.010066214948892593
Epoch: 1659, Batch Gradient Norm: 0.6352284758909345
Epoch: 1659, Batch Gradient Norm after: 0.6352284758909345
Epoch 1660/10000, Prediction Accuracy = 59.346153846153854%, Loss = 0.01010927135268083
Epoch: 1660, Batch Gradient Norm: 0.6452962082528244
Epoch: 1660, Batch Gradient Norm after: 0.6452962082528244
Epoch 1661/10000, Prediction Accuracy = 59.38461538461539%, Loss = 0.010168077065967597
Epoch: 1661, Batch Gradient Norm: 0.6531679853387541
Epoch: 1661, Batch Gradient Norm after: 0.6531679853387541
Epoch 1662/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010113534660866627
Epoch: 1662, Batch Gradient Norm: 0.6392091588586433
Epoch: 1662, Batch Gradient Norm after: 0.6392091588586433
Epoch 1663/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010070017180763759
Epoch: 1663, Batch Gradient Norm: 0.6388231446199331
Epoch: 1663, Batch Gradient Norm after: 0.6388231446199331
Epoch 1664/10000, Prediction Accuracy = 59.3%, Loss = 0.010093428051242461
Epoch: 1664, Batch Gradient Norm: 0.6465218850545
Epoch: 1664, Batch Gradient Norm after: 0.6465218850545
Epoch 1665/10000, Prediction Accuracy = 59.5%, Loss = 0.010111838650818054
Epoch: 1665, Batch Gradient Norm: 0.6400947458776755
Epoch: 1665, Batch Gradient Norm after: 0.6400947458776755
Epoch 1666/10000, Prediction Accuracy = 59.557692307692314%, Loss = 0.010055952705442905
Epoch: 1666, Batch Gradient Norm: 0.6454781876883275
Epoch: 1666, Batch Gradient Norm after: 0.6454781876883275
Epoch 1667/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.01007141273182172
Epoch: 1667, Batch Gradient Norm: 0.6432836345827997
Epoch: 1667, Batch Gradient Norm after: 0.6432836345827997
Epoch 1668/10000, Prediction Accuracy = 59.619230769230754%, Loss = 0.010053619384192504
Epoch: 1668, Batch Gradient Norm: 0.6423786772324213
Epoch: 1668, Batch Gradient Norm after: 0.6423786772324213
Epoch 1669/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010070342283982497
Epoch: 1669, Batch Gradient Norm: 0.651401194892655
Epoch: 1669, Batch Gradient Norm after: 0.651401194892655
Epoch 1670/10000, Prediction Accuracy = 59.45769230769231%, Loss = 0.01011945278598712
Epoch: 1670, Batch Gradient Norm: 0.6382655113190254
Epoch: 1670, Batch Gradient Norm after: 0.6382655113190254
Epoch 1671/10000, Prediction Accuracy = 59.43076923076923%, Loss = 0.0100989116069216
Epoch: 1671, Batch Gradient Norm: 0.6146954967417143
Epoch: 1671, Batch Gradient Norm after: 0.6146954967417143
Epoch 1672/10000, Prediction Accuracy = 59.45%, Loss = 0.010081717601189246
Epoch: 1672, Batch Gradient Norm: 0.6338446543295755
Epoch: 1672, Batch Gradient Norm after: 0.6338446543295755
Epoch 1673/10000, Prediction Accuracy = 59.5076923076923%, Loss = 0.010051952818265328
Epoch: 1673, Batch Gradient Norm: 0.6502096978059709
Epoch: 1673, Batch Gradient Norm after: 0.6502096978059709
Epoch 1674/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010164630814240528
Epoch: 1674, Batch Gradient Norm: 0.6429453114885237
Epoch: 1674, Batch Gradient Norm after: 0.6429453114885237
Epoch 1675/10000, Prediction Accuracy = 59.54230769230769%, Loss = 0.010111123610001344
Epoch: 1675, Batch Gradient Norm: 0.6238583917479542
Epoch: 1675, Batch Gradient Norm after: 0.6238583917479542
Epoch 1676/10000, Prediction Accuracy = 59.58461538461539%, Loss = 0.010079067773543872
Epoch: 1676, Batch Gradient Norm: 0.6537806039967456
Epoch: 1676, Batch Gradient Norm after: 0.6537806039967456
Epoch 1677/10000, Prediction Accuracy = 59.57692307692309%, Loss = 0.010050186887383461
Epoch: 1677, Batch Gradient Norm: 0.6506762589517252
Epoch: 1677, Batch Gradient Norm after: 0.6506762589517252
Epoch 1678/10000, Prediction Accuracy = 59.557692307692314%, Loss = 0.010070417649470843
Epoch: 1678, Batch Gradient Norm: 0.6476046176263796
Epoch: 1678, Batch Gradient Norm after: 0.6476046176263796
Epoch 1679/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010101486212359024
Epoch: 1679, Batch Gradient Norm: 0.639772942147762
Epoch: 1679, Batch Gradient Norm after: 0.639772942147762
Epoch 1680/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.01006196804631215
Epoch: 1680, Batch Gradient Norm: 0.6596651867157978
Epoch: 1680, Batch Gradient Norm after: 0.6596651867157978
Epoch 1681/10000, Prediction Accuracy = 59.3346153846154%, Loss = 0.010185680925272979
Epoch: 1681, Batch Gradient Norm: 0.6355528342122744
Epoch: 1681, Batch Gradient Norm after: 0.6355528342122744
Epoch 1682/10000, Prediction Accuracy = 59.26538461538462%, Loss = 0.010086023033811497
Epoch: 1682, Batch Gradient Norm: 0.6412132764243019
Epoch: 1682, Batch Gradient Norm after: 0.6412132764243019
Epoch 1683/10000, Prediction Accuracy = 59.4423076923077%, Loss = 0.010098355607344555
Epoch: 1683, Batch Gradient Norm: 0.6322254050804471
Epoch: 1683, Batch Gradient Norm after: 0.6322254050804471
Epoch 1684/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010066321692787684
Epoch: 1684, Batch Gradient Norm: 0.6262887999182378
Epoch: 1684, Batch Gradient Norm after: 0.6262887999182378
Epoch 1685/10000, Prediction Accuracy = 59.54615384615384%, Loss = 0.01005010693692244
Epoch: 1685, Batch Gradient Norm: 0.6316276128044024
Epoch: 1685, Batch Gradient Norm after: 0.6316276128044024
Epoch 1686/10000, Prediction Accuracy = 59.4923076923077%, Loss = 0.010041984944389416
Epoch: 1686, Batch Gradient Norm: 0.6424385864393775
Epoch: 1686, Batch Gradient Norm after: 0.6424385864393775
Epoch 1687/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010132036386774136
Epoch: 1687, Batch Gradient Norm: 0.6414848626803652
Epoch: 1687, Batch Gradient Norm after: 0.6414848626803652
Epoch 1688/10000, Prediction Accuracy = 59.11923076923077%, Loss = 0.010094676811534625
Epoch: 1688, Batch Gradient Norm: 0.6302424012551302
Epoch: 1688, Batch Gradient Norm after: 0.6302424012551302
Epoch 1689/10000, Prediction Accuracy = 59.37692307692306%, Loss = 0.010108704678714275
Epoch: 1689, Batch Gradient Norm: 0.6710742011900915
Epoch: 1689, Batch Gradient Norm after: 0.6710742011900915
Epoch 1690/10000, Prediction Accuracy = 59.32692307692309%, Loss = 0.01014728399996574
Epoch: 1690, Batch Gradient Norm: 0.6381454932501489
Epoch: 1690, Batch Gradient Norm after: 0.6381454932501489
Epoch 1691/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.01008615786066422
Epoch: 1691, Batch Gradient Norm: 0.6479703913379139
Epoch: 1691, Batch Gradient Norm after: 0.6479703913379139
Epoch 1692/10000, Prediction Accuracy = 59.66923076923077%, Loss = 0.010062324098096443
Epoch: 1692, Batch Gradient Norm: 0.6303780016375534
Epoch: 1692, Batch Gradient Norm after: 0.6303780016375534
Epoch 1693/10000, Prediction Accuracy = 59.376923076923084%, Loss = 0.010057160702462379
Epoch: 1693, Batch Gradient Norm: 0.6487053620949773
Epoch: 1693, Batch Gradient Norm after: 0.6487053620949773
Epoch 1694/10000, Prediction Accuracy = 59.300000000000004%, Loss = 0.010089250496373726
Epoch: 1694, Batch Gradient Norm: 0.6291448063772099
Epoch: 1694, Batch Gradient Norm after: 0.6291448063772099
Epoch 1695/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010100149262983065
Epoch: 1695, Batch Gradient Norm: 0.637166974891734
Epoch: 1695, Batch Gradient Norm after: 0.637166974891734
Epoch 1696/10000, Prediction Accuracy = 59.44615384615385%, Loss = 0.01008278181633124
Epoch: 1696, Batch Gradient Norm: 0.6332563436222272
Epoch: 1696, Batch Gradient Norm after: 0.6332563436222272
Epoch 1697/10000, Prediction Accuracy = 59.54615384615385%, Loss = 0.010068627790762829
Epoch: 1697, Batch Gradient Norm: 0.6477871214340302
Epoch: 1697, Batch Gradient Norm after: 0.6477871214340302
Epoch 1698/10000, Prediction Accuracy = 59.403846153846146%, Loss = 0.010135904026146118
Epoch: 1698, Batch Gradient Norm: 0.6473064402597324
Epoch: 1698, Batch Gradient Norm after: 0.6473064402597324
Epoch 1699/10000, Prediction Accuracy = 59.61923076923077%, Loss = 0.010062951737871537
Epoch: 1699, Batch Gradient Norm: 0.6524011004635215
Epoch: 1699, Batch Gradient Norm after: 0.6524011004635215
Epoch 1700/10000, Prediction Accuracy = 59.326923076923066%, Loss = 0.010096356917459231
Epoch: 1700, Batch Gradient Norm: 0.651543551426896
Epoch: 1700, Batch Gradient Norm after: 0.651543551426896
Epoch 1701/10000, Prediction Accuracy = 59.630769230769246%, Loss = 0.010078856936441017
Epoch: 1701, Batch Gradient Norm: 0.6594395462727276
Epoch: 1701, Batch Gradient Norm after: 0.6594395462727276
Epoch 1702/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.010110669482785922
Epoch: 1702, Batch Gradient Norm: 0.641158007154562
Epoch: 1702, Batch Gradient Norm after: 0.641158007154562
Epoch 1703/10000, Prediction Accuracy = 59.67692307692307%, Loss = 0.010028201585205702
Epoch: 1703, Batch Gradient Norm: 0.6425866590107951
Epoch: 1703, Batch Gradient Norm after: 0.6425866590107951
Epoch 1704/10000, Prediction Accuracy = 59.40384615384616%, Loss = 0.010141108328333268
Epoch: 1704, Batch Gradient Norm: 0.6466664210234715
Epoch: 1704, Batch Gradient Norm after: 0.6466664210234715
Epoch 1705/10000, Prediction Accuracy = 59.29615384615384%, Loss = 0.01012984269226973
Epoch: 1705, Batch Gradient Norm: 0.6389934940556907
Epoch: 1705, Batch Gradient Norm after: 0.6389934940556907
Epoch 1706/10000, Prediction Accuracy = 59.45384615384616%, Loss = 0.010072619224397035
Epoch: 1706, Batch Gradient Norm: 0.6542256224536421
Epoch: 1706, Batch Gradient Norm after: 0.6542256224536421
Epoch 1707/10000, Prediction Accuracy = 59.365384615384606%, Loss = 0.010133786126971245
Epoch: 1707, Batch Gradient Norm: 0.6448049868929773
Epoch: 1707, Batch Gradient Norm after: 0.6448049868929773
Epoch 1708/10000, Prediction Accuracy = 59.607692307692304%, Loss = 0.010097730474976392
Epoch: 1708, Batch Gradient Norm: 0.6388979117338893
Epoch: 1708, Batch Gradient Norm after: 0.6388979117338893
Epoch 1709/10000, Prediction Accuracy = 59.53846153846154%, Loss = 0.010079323027569514
Epoch: 1709, Batch Gradient Norm: 0.6319840059348587
Epoch: 1709, Batch Gradient Norm after: 0.6319840059348587
Epoch 1710/10000, Prediction Accuracy = 59.58846153846154%, Loss = 0.01005011166517551
Epoch: 1710, Batch Gradient Norm: 0.6488193178872126
Epoch: 1710, Batch Gradient Norm after: 0.6488193178872126
Epoch 1711/10000, Prediction Accuracy = 59.449999999999996%, Loss = 0.010106427021897756
Epoch: 1711, Batch Gradient Norm: 0.6510604093137742
Epoch: 1711, Batch Gradient Norm after: 0.6510604093137742
Epoch 1712/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.010054349756011596
Epoch: 1712, Batch Gradient Norm: 0.6430950557550809
Epoch: 1712, Batch Gradient Norm after: 0.6430950557550809
Epoch 1713/10000, Prediction Accuracy = 59.369230769230775%, Loss = 0.010108918596345644
Epoch: 1713, Batch Gradient Norm: 0.6330496241034432
Epoch: 1713, Batch Gradient Norm after: 0.6330496241034432
Epoch 1714/10000, Prediction Accuracy = 59.41538461538461%, Loss = 0.01008841388214093
Epoch: 1714, Batch Gradient Norm: 0.6680587360495505
Epoch: 1714, Batch Gradient Norm after: 0.6680587360495505
Epoch 1715/10000, Prediction Accuracy = 59.08076923076922%, Loss = 0.010173373927290622
Epoch: 1715, Batch Gradient Norm: 0.6387591758028898
Epoch: 1715, Batch Gradient Norm after: 0.6387591758028898
Epoch 1716/10000, Prediction Accuracy = 59.69230769230769%, Loss = 0.01005926814216834
Epoch: 1716, Batch Gradient Norm: 0.6374628485639369
Epoch: 1716, Batch Gradient Norm after: 0.6374628485639369
Epoch 1717/10000, Prediction Accuracy = 59.388461538461534%, Loss = 0.010092962748156143
Epoch: 1717, Batch Gradient Norm: 0.6597587828257008
Epoch: 1717, Batch Gradient Norm after: 0.6597587828257008
Epoch 1718/10000, Prediction Accuracy = 59.323076923076925%, Loss = 0.010099831467064528
Epoch: 1718, Batch Gradient Norm: 0.6382310118372765
Epoch: 1718, Batch Gradient Norm after: 0.6382310118372765
Epoch 1719/10000, Prediction Accuracy = 59.642307692307696%, Loss = 0.010077326773450924
Epoch: 1719, Batch Gradient Norm: 0.6436686399649789
Epoch: 1719, Batch Gradient Norm after: 0.6436686399649789
Epoch 1720/10000, Prediction Accuracy = 59.32307692307692%, Loss = 0.010075434397619504
Epoch: 1720, Batch Gradient Norm: 0.6348713727874435
Epoch: 1720, Batch Gradient Norm after: 0.6348713727874435
Epoch 1721/10000, Prediction Accuracy = 59.52307692307693%, Loss = 0.010096389871950332
Epoch: 1721, Batch Gradient Norm: 0.6324431294110103
Epoch: 1721, Batch Gradient Norm after: 0.6324431294110103
Epoch 1722/10000, Prediction Accuracy = 59.646153846153844%, Loss = 0.010070363990962505
Epoch: 1722, Batch Gradient Norm: 0.6453885314836353
Epoch: 1722, Batch Gradient Norm after: 0.6453885314836353
Epoch 1723/10000, Prediction Accuracy = 59.61923076923077%, Loss = 0.010069582468042007
Epoch: 1723, Batch Gradient Norm: 0.6400232029981175
Epoch: 1723, Batch Gradient Norm after: 0.6400232029981175
Epoch 1724/10000, Prediction Accuracy = 59.50000000000001%, Loss = 0.010072858431018315
Epoch: 1724, Batch Gradient Norm: 0.6383667575082471
Epoch: 1724, Batch Gradient Norm after: 0.6383667575082471
Epoch 1725/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010054169580913506
Epoch: 1725, Batch Gradient Norm: 0.6430644466481134
Epoch: 1725, Batch Gradient Norm after: 0.6430644466481134
Epoch 1726/10000, Prediction Accuracy = 59.42692307692308%, Loss = 0.010085119149432732
Epoch: 1726, Batch Gradient Norm: 0.6387074912270979
Epoch: 1726, Batch Gradient Norm after: 0.6387074912270979
Epoch 1727/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.01007092736947995
Epoch: 1727, Batch Gradient Norm: 0.6374510401177121
Epoch: 1727, Batch Gradient Norm after: 0.6374510401177121
Epoch 1728/10000, Prediction Accuracy = 59.18076923076923%, Loss = 0.010068921014093436
Epoch: 1728, Batch Gradient Norm: 0.6444118812213412
Epoch: 1728, Batch Gradient Norm after: 0.6444118812213412
Epoch 1729/10000, Prediction Accuracy = 59.45%, Loss = 0.010066973833510509
Epoch: 1729, Batch Gradient Norm: 0.6532111467163091
Epoch: 1729, Batch Gradient Norm after: 0.6532111467163091
Epoch 1730/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.01008310068685275
Epoch: 1730, Batch Gradient Norm: 0.6398064323866085
Epoch: 1730, Batch Gradient Norm after: 0.6398064323866085
Epoch 1731/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010085175816829387
Epoch: 1731, Batch Gradient Norm: 0.641267388921296
Epoch: 1731, Batch Gradient Norm after: 0.641267388921296
Epoch 1732/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010094727461154644
Epoch: 1732, Batch Gradient Norm: 0.651785544370162
Epoch: 1732, Batch Gradient Norm after: 0.651785544370162
Epoch 1733/10000, Prediction Accuracy = 59.349999999999994%, Loss = 0.010132004220325213
Epoch: 1733, Batch Gradient Norm: 0.6390042264081789
Epoch: 1733, Batch Gradient Norm after: 0.6390042264081789
Epoch 1734/10000, Prediction Accuracy = 59.35384615384615%, Loss = 0.010107208473178057
Epoch: 1734, Batch Gradient Norm: 0.6525008291793398
Epoch: 1734, Batch Gradient Norm after: 0.6525008291793398
Epoch 1735/10000, Prediction Accuracy = 59.373076923076916%, Loss = 0.010132521964036502
Epoch: 1735, Batch Gradient Norm: 0.6299142196712221
Epoch: 1735, Batch Gradient Norm after: 0.6299142196712221
Epoch 1736/10000, Prediction Accuracy = 59.465384615384615%, Loss = 0.01004906750928897
Epoch: 1736, Batch Gradient Norm: 0.6534698288054518
Epoch: 1736, Batch Gradient Norm after: 0.6534698288054518
Epoch 1737/10000, Prediction Accuracy = 59.276923076923076%, Loss = 0.010103957655911263
Epoch: 1737, Batch Gradient Norm: 0.6356211401561004
Epoch: 1737, Batch Gradient Norm after: 0.6356211401561004
Epoch 1738/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010076610658031244
Epoch: 1738, Batch Gradient Norm: 0.6377279283061568
Epoch: 1738, Batch Gradient Norm after: 0.6377279283061568
Epoch 1739/10000, Prediction Accuracy = 59.51538461538461%, Loss = 0.01007886567654518
Epoch: 1739, Batch Gradient Norm: 0.6505552249229783
Epoch: 1739, Batch Gradient Norm after: 0.6505552249229783
Epoch 1740/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010102071870978061
Epoch: 1740, Batch Gradient Norm: 0.6567090663079809
Epoch: 1740, Batch Gradient Norm after: 0.6567090663079809
Epoch 1741/10000, Prediction Accuracy = 59.50384615384614%, Loss = 0.010074505797372414
Epoch: 1741, Batch Gradient Norm: 0.6276483394518346
Epoch: 1741, Batch Gradient Norm after: 0.6276483394518346
Epoch 1742/10000, Prediction Accuracy = 59.315384615384616%, Loss = 0.010026939643117098
Epoch: 1742, Batch Gradient Norm: 0.6633189233937024
Epoch: 1742, Batch Gradient Norm after: 0.6633189233937024
Epoch 1743/10000, Prediction Accuracy = 59.51538461538462%, Loss = 0.010103991398444543
Epoch: 1743, Batch Gradient Norm: 0.649455587462121
Epoch: 1743, Batch Gradient Norm after: 0.649455587462121
Epoch 1744/10000, Prediction Accuracy = 59.06923076923077%, Loss = 0.010122333724911396
Epoch: 1744, Batch Gradient Norm: 0.6490233249740447
Epoch: 1744, Batch Gradient Norm after: 0.6490233249740447
Epoch 1745/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010068145222388782
Epoch: 1745, Batch Gradient Norm: 0.6456858266756145
Epoch: 1745, Batch Gradient Norm after: 0.6456858266756145
Epoch 1746/10000, Prediction Accuracy = 59.36923076923077%, Loss = 0.010117330588400364
Epoch: 1746, Batch Gradient Norm: 0.6477262134714546
Epoch: 1746, Batch Gradient Norm after: 0.6477262134714546
Epoch 1747/10000, Prediction Accuracy = 59.48846153846154%, Loss = 0.010083140805363655
Epoch: 1747, Batch Gradient Norm: 0.6350071083165645
Epoch: 1747, Batch Gradient Norm after: 0.6350071083165645
Epoch 1748/10000, Prediction Accuracy = 59.36153846153846%, Loss = 0.010062960334695302
Epoch: 1748, Batch Gradient Norm: 0.6494864797232048
Epoch: 1748, Batch Gradient Norm after: 0.6494864797232048
Epoch 1749/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.01009460875334648
Epoch: 1749, Batch Gradient Norm: 0.6335142174777659
Epoch: 1749, Batch Gradient Norm after: 0.6335142174777659
Epoch 1750/10000, Prediction Accuracy = 59.30384615384615%, Loss = 0.010072959300417166
Epoch: 1750, Batch Gradient Norm: 0.6362032840986198
Epoch: 1750, Batch Gradient Norm after: 0.6362032840986198
Epoch 1751/10000, Prediction Accuracy = 59.56923076923077%, Loss = 0.010049388099175233
Epoch: 1751, Batch Gradient Norm: 0.652241467395385
Epoch: 1751, Batch Gradient Norm after: 0.652241467395385
Epoch 1752/10000, Prediction Accuracy = 59.27307692307693%, Loss = 0.010123404817512402
Epoch: 1752, Batch Gradient Norm: 0.6389691665849507
Epoch: 1752, Batch Gradient Norm after: 0.6389691665849507
Epoch 1753/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.01009048091677519
Epoch: 1753, Batch Gradient Norm: 0.6469821026865479
Epoch: 1753, Batch Gradient Norm after: 0.6469821026865479
Epoch 1754/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.010058504457657155
Epoch: 1754, Batch Gradient Norm: 0.6527558791114617
Epoch: 1754, Batch Gradient Norm after: 0.6527558791114617
Epoch 1755/10000, Prediction Accuracy = 59.16153846153846%, Loss = 0.010124038618344527
Epoch: 1755, Batch Gradient Norm: 0.6345801308613979
Epoch: 1755, Batch Gradient Norm after: 0.6345801308613979
Epoch 1756/10000, Prediction Accuracy = 59.48076923076922%, Loss = 0.010072683915495872
Epoch: 1756, Batch Gradient Norm: 0.6335914580955687
Epoch: 1756, Batch Gradient Norm after: 0.6335914580955687
Epoch 1757/10000, Prediction Accuracy = 59.53846153846155%, Loss = 0.01006622712772626
Epoch: 1757, Batch Gradient Norm: 0.6380044425805259
Epoch: 1757, Batch Gradient Norm after: 0.6380044425805259
Epoch 1758/10000, Prediction Accuracy = 59.630769230769225%, Loss = 0.010075336752029566
Epoch: 1758, Batch Gradient Norm: 0.6530106076318635
Epoch: 1758, Batch Gradient Norm after: 0.6530106076318635
Epoch 1759/10000, Prediction Accuracy = 59.21923076923077%, Loss = 0.010114624236638729
Epoch: 1759, Batch Gradient Norm: 0.6388231956654608
Epoch: 1759, Batch Gradient Norm after: 0.6388231956654608
Epoch 1760/10000, Prediction Accuracy = 59.46153846153846%, Loss = 0.010069018516402978
Epoch: 1760, Batch Gradient Norm: 0.6543804252427751
Epoch: 1760, Batch Gradient Norm after: 0.6543804252427751
Epoch 1761/10000, Prediction Accuracy = 59.18846153846154%, Loss = 0.010127155611721369
Epoch: 1761, Batch Gradient Norm: 0.6400198190881471
Epoch: 1761, Batch Gradient Norm after: 0.6400198190881471
Epoch 1762/10000, Prediction Accuracy = 59.515384615384626%, Loss = 0.010061661856105695
Epoch: 1762, Batch Gradient Norm: 0.647440312569239
Epoch: 1762, Batch Gradient Norm after: 0.647440312569239
Epoch 1763/10000, Prediction Accuracy = 59.41153846153846%, Loss = 0.010114670516206669
Epoch: 1763, Batch Gradient Norm: 0.6504911180697498
Epoch: 1763, Batch Gradient Norm after: 0.6504911180697498
Epoch 1764/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.01011705591988105
Epoch: 1764, Batch Gradient Norm: 0.657140830816327
Epoch: 1764, Batch Gradient Norm after: 0.657140830816327
Epoch 1765/10000, Prediction Accuracy = 59.29615384615384%, Loss = 0.010134893827713452
Epoch: 1765, Batch Gradient Norm: 0.6296682263280504
Epoch: 1765, Batch Gradient Norm after: 0.6296682263280504
Epoch 1766/10000, Prediction Accuracy = 59.2423076923077%, Loss = 0.010129355610563206
Epoch: 1766, Batch Gradient Norm: 0.6517009554048315
Epoch: 1766, Batch Gradient Norm after: 0.6517009554048315
Epoch 1767/10000, Prediction Accuracy = 59.28461538461538%, Loss = 0.010095558774012785
Epoch: 1767, Batch Gradient Norm: 0.660537434428809
Epoch: 1767, Batch Gradient Norm after: 0.660537434428809
Epoch 1768/10000, Prediction Accuracy = 59.426923076923075%, Loss = 0.01005842500867752
Epoch: 1768, Batch Gradient Norm: 0.6375326365521392
Epoch: 1768, Batch Gradient Norm after: 0.6375326365521392
Epoch 1769/10000, Prediction Accuracy = 59.2%, Loss = 0.010076773926042594
Epoch: 1769, Batch Gradient Norm: 0.6399532764170418
Epoch: 1769, Batch Gradient Norm after: 0.6399532764170418
Epoch 1770/10000, Prediction Accuracy = 59.21153846153847%, Loss = 0.010096307772283371
Epoch: 1770, Batch Gradient Norm: 0.6480038462787593
Epoch: 1770, Batch Gradient Norm after: 0.6480038462787593
Epoch 1771/10000, Prediction Accuracy = 59.3%, Loss = 0.0100923737224478
Epoch: 1771, Batch Gradient Norm: 0.6354313260545379
Epoch: 1771, Batch Gradient Norm after: 0.6354313260545379
Epoch 1772/10000, Prediction Accuracy = 59.65384615384615%, Loss = 0.010062863477147542
Epoch: 1772, Batch Gradient Norm: 0.6427051532489523
Epoch: 1772, Batch Gradient Norm after: 0.6427051532489523
Epoch 1773/10000, Prediction Accuracy = 59.48076923076924%, Loss = 0.010063131339848042
Epoch: 1773, Batch Gradient Norm: 0.6302876967475217
Epoch: 1773, Batch Gradient Norm after: 0.6302876967475217
Epoch 1774/10000, Prediction Accuracy = 59.646153846153844%, Loss = 0.010060096804339152
Epoch: 1774, Batch Gradient Norm: 0.6304522289672427
Epoch: 1774, Batch Gradient Norm after: 0.6304522289672427
Epoch 1775/10000, Prediction Accuracy = 59.47307692307693%, Loss = 0.010079324460373474
Epoch: 1775, Batch Gradient Norm: 0.6430402043029588
Epoch: 1775, Batch Gradient Norm after: 0.6430402043029588
Epoch 1776/10000, Prediction Accuracy = 59.60769230769232%, Loss = 0.010057634745652858
Epoch: 1776, Batch Gradient Norm: 0.644508164571714
Epoch: 1776, Batch Gradient Norm after: 0.644508164571714
Epoch 1777/10000, Prediction Accuracy = 59.31923076923077%, Loss = 0.010075353945677098
Epoch: 1777, Batch Gradient Norm: 0.6466534259582067
Epoch: 1777, Batch Gradient Norm after: 0.6466534259582067
Epoch 1778/10000, Prediction Accuracy = 59.50769230769231%, Loss = 0.010055052044873055
Epoch: 1778, Batch Gradient Norm: 0.6453800047038492
Epoch: 1778, Batch Gradient Norm after: 0.6453800047038492
Epoch 1779/10000, Prediction Accuracy = 59.48846153846155%, Loss = 0.010089687644862212
Epoch: 1779, Batch Gradient Norm: 0.6641000673705334
Epoch: 1779, Batch Gradient Norm after: 0.6641000673705334
Epoch 1780/10000, Prediction Accuracy = 59.434615384615384%, Loss = 0.010110046141422711
Epoch: 1780, Batch Gradient Norm: 0.6409979476159064
Epoch: 1780, Batch Gradient Norm after: 0.6409979476159064
Epoch 1781/10000, Prediction Accuracy = 59.36538461538461%, Loss = 0.01012076036288188
Epoch: 1781, Batch Gradient Norm: 0.6449217518034517
Epoch: 1781, Batch Gradient Norm after: 0.6449217518034517
Epoch 1782/10000, Prediction Accuracy = 59.44230769230769%, Loss = 0.010093183471606327
Epoch: 1782, Batch Gradient Norm: 0.6327135496574897
Epoch: 1782, Batch Gradient Norm after: 0.6327135496574897
Epoch 1783/10000, Prediction Accuracy = 59.46538461538462%, Loss = 0.010043538032242885
Epoch: 1783, Batch Gradient Norm: 0.6497471321608522
Epoch: 1783, Batch Gradient Norm after: 0.6497471321608522
Epoch 1784/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.01007822141624414
Epoch: 1784, Batch Gradient Norm: 0.635646492701671
Epoch: 1784, Batch Gradient Norm after: 0.635646492701671
Epoch 1785/10000, Prediction Accuracy = 59.55384615384615%, Loss = 0.010060519051666442
Epoch: 1785, Batch Gradient Norm: 0.6430432229976155
Epoch: 1785, Batch Gradient Norm after: 0.6430432229976155
Epoch 1786/10000, Prediction Accuracy = 59.46153846153845%, Loss = 0.010088343746387042
Epoch: 1786, Batch Gradient Norm: 0.6408900774983458
Epoch: 1786, Batch Gradient Norm after: 0.6408900774983458
Epoch 1787/10000, Prediction Accuracy = 59.55%, Loss = 0.010085894224735407
Epoch: 1787, Batch Gradient Norm: 0.647443396268297
Epoch: 1787, Batch Gradient Norm after: 0.647443396268297
Epoch 1788/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.010104838328865858
Epoch: 1788, Batch Gradient Norm: 0.6560273036429763
Epoch: 1788, Batch Gradient Norm after: 0.6560273036429763
Epoch 1789/10000, Prediction Accuracy = 59.4076923076923%, Loss = 0.01011105404736904
Epoch: 1789, Batch Gradient Norm: 0.632743197345225
Epoch: 1789, Batch Gradient Norm after: 0.632743197345225
Epoch 1790/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010113209915848879
Epoch: 1790, Batch Gradient Norm: 0.6495083677246545
Epoch: 1790, Batch Gradient Norm after: 0.6495083677246545
Epoch 1791/10000, Prediction Accuracy = 59.5846153846154%, Loss = 0.01006303527034246
Epoch: 1791, Batch Gradient Norm: 0.6410414244959257
Epoch: 1791, Batch Gradient Norm after: 0.6410414244959257
Epoch 1792/10000, Prediction Accuracy = 59.400000000000006%, Loss = 0.010078374153146377
Epoch: 1792, Batch Gradient Norm: 0.6434323683961598
Epoch: 1792, Batch Gradient Norm after: 0.6434323683961598
Epoch 1793/10000, Prediction Accuracy = 59.676923076923075%, Loss = 0.010069885649360143
Epoch: 1793, Batch Gradient Norm: 0.6413840716744991
Epoch: 1793, Batch Gradient Norm after: 0.6413840716744991
Epoch 1794/10000, Prediction Accuracy = 59.353846153846156%, Loss = 0.010071631520986557
Epoch: 1794, Batch Gradient Norm: 0.650959508679768
Epoch: 1794, Batch Gradient Norm after: 0.650959508679768
Epoch 1795/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010105533310427116
Epoch: 1795, Batch Gradient Norm: 0.6418787780834838
Epoch: 1795, Batch Gradient Norm after: 0.6418787780834838
Epoch 1796/10000, Prediction Accuracy = 59.48461538461538%, Loss = 0.0100517740759712
Epoch: 1796, Batch Gradient Norm: 0.6543205926843432
Epoch: 1796, Batch Gradient Norm after: 0.6543205926843432
Epoch 1797/10000, Prediction Accuracy = 59.57307692307692%, Loss = 0.010032496844919829
Epoch: 1797, Batch Gradient Norm: 0.6480133692348107
Epoch: 1797, Batch Gradient Norm after: 0.6480133692348107
Epoch 1798/10000, Prediction Accuracy = 59.49615384615385%, Loss = 0.010053167907664409
Epoch: 1798, Batch Gradient Norm: 0.6336329975656042
Epoch: 1798, Batch Gradient Norm after: 0.6336329975656042
Epoch 1799/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010074764490127563
Epoch: 1799, Batch Gradient Norm: 0.6555368651488448
Epoch: 1799, Batch Gradient Norm after: 0.6555368651488448
Epoch 1800/10000, Prediction Accuracy = 59.28846153846154%, Loss = 0.010152533148916868
Epoch: 1800, Batch Gradient Norm: 0.6349249963451129
Epoch: 1800, Batch Gradient Norm after: 0.6349249963451129
Epoch 1801/10000, Prediction Accuracy = 59.26923076923076%, Loss = 0.01015972181294973
Epoch: 1801, Batch Gradient Norm: 0.6375280816390425
Epoch: 1801, Batch Gradient Norm after: 0.6375280816390425
Epoch 1802/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010057037696242332
Epoch: 1802, Batch Gradient Norm: 0.6313065319605189
Epoch: 1802, Batch Gradient Norm after: 0.6313065319605189
Epoch 1803/10000, Prediction Accuracy = 59.407692307692315%, Loss = 0.010095254374811282
Epoch: 1803, Batch Gradient Norm: 0.6522597138768366
Epoch: 1803, Batch Gradient Norm after: 0.6522597138768366
Epoch 1804/10000, Prediction Accuracy = 59.24615384615384%, Loss = 0.010142639136085143
Epoch: 1804, Batch Gradient Norm: 0.6481003044322087
Epoch: 1804, Batch Gradient Norm after: 0.6481003044322087
Epoch 1805/10000, Prediction Accuracy = 59.31538461538461%, Loss = 0.010128649811332043
Epoch: 1805, Batch Gradient Norm: 0.643831272170864
Epoch: 1805, Batch Gradient Norm after: 0.643831272170864
Epoch 1806/10000, Prediction Accuracy = 59.42692307692308%, Loss = 0.010112965980974527
Epoch: 1806, Batch Gradient Norm: 0.6485205290934352
Epoch: 1806, Batch Gradient Norm after: 0.6485205290934352
Epoch 1807/10000, Prediction Accuracy = 59.24615384615386%, Loss = 0.010115696403842706
Epoch: 1807, Batch Gradient Norm: 0.6339157815791835
Epoch: 1807, Batch Gradient Norm after: 0.6339157815791835
Epoch 1808/10000, Prediction Accuracy = 59.46923076923077%, Loss = 0.010072444279033404
Epoch: 1808, Batch Gradient Norm: 0.6407722574807369
Epoch: 1808, Batch Gradient Norm after: 0.6407722574807369
Epoch 1809/10000, Prediction Accuracy = 59.1576923076923%, Loss = 0.010102474632171484
Epoch: 1809, Batch Gradient Norm: 0.650779107066599
Epoch: 1809, Batch Gradient Norm after: 0.650779107066599
Epoch 1810/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010061963174778681
Epoch: 1810, Batch Gradient Norm: 0.6513263328307447
Epoch: 1810, Batch Gradient Norm after: 0.6513263328307447
Epoch 1811/10000, Prediction Accuracy = 59.353846153846135%, Loss = 0.010107752652122425
Epoch: 1811, Batch Gradient Norm: 0.6553085524143023
Epoch: 1811, Batch Gradient Norm after: 0.6553085524143023
Epoch 1812/10000, Prediction Accuracy = 59.39999999999999%, Loss = 0.010125551229486099
Epoch: 1812, Batch Gradient Norm: 0.6488749062621835
Epoch: 1812, Batch Gradient Norm after: 0.6488749062621835
Epoch 1813/10000, Prediction Accuracy = 59.346153846153854%, Loss = 0.010102100813618073
Epoch: 1813, Batch Gradient Norm: 0.6436949216000457
Epoch: 1813, Batch Gradient Norm after: 0.6436949216000457
Epoch 1814/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010066312737762928
Epoch: 1814, Batch Gradient Norm: 0.6343205258421488
Epoch: 1814, Batch Gradient Norm after: 0.6343205258421488
Epoch 1815/10000, Prediction Accuracy = 59.473076923076924%, Loss = 0.010081556983865224
Epoch: 1815, Batch Gradient Norm: 0.6370803554877325
Epoch: 1815, Batch Gradient Norm after: 0.6370803554877325
Epoch 1816/10000, Prediction Accuracy = 59.28076923076923%, Loss = 0.010101688810839103
Epoch: 1816, Batch Gradient Norm: 0.6537417678675128
Epoch: 1816, Batch Gradient Norm after: 0.6537417678675128
Epoch 1817/10000, Prediction Accuracy = 59.38846153846154%, Loss = 0.010144958559137125
Epoch: 1817, Batch Gradient Norm: 0.6364511503970646
Epoch: 1817, Batch Gradient Norm after: 0.6364511503970646
Epoch 1818/10000, Prediction Accuracy = 59.29615384615385%, Loss = 0.01010122042722427
Epoch: 1818, Batch Gradient Norm: 0.6467572955682069
Epoch: 1818, Batch Gradient Norm after: 0.6467572955682069
Epoch 1819/10000, Prediction Accuracy = 59.369230769230775%, Loss = 0.010093445961291973
Epoch: 1819, Batch Gradient Norm: 0.6502490332306096
Epoch: 1819, Batch Gradient Norm after: 0.6502490332306096
Epoch 1820/10000, Prediction Accuracy = 59.357692307692304%, Loss = 0.010110962634476332
Epoch: 1820, Batch Gradient Norm: 0.6331980293154306
Epoch: 1820, Batch Gradient Norm after: 0.6331980293154306
Epoch 1821/10000, Prediction Accuracy = 59.73846153846154%, Loss = 0.010077178621521363
Epoch: 1821, Batch Gradient Norm: 0.6438088573217989
Epoch: 1821, Batch Gradient Norm after: 0.6438088573217989
Epoch 1822/10000, Prediction Accuracy = 59.2576923076923%, Loss = 0.010102152896042053
Epoch: 1822, Batch Gradient Norm: 0.6486008707843232
Epoch: 1822, Batch Gradient Norm after: 0.6486008707843232
Epoch 1823/10000, Prediction Accuracy = 59.62692307692308%, Loss = 0.010093086112577181
Epoch: 1823, Batch Gradient Norm: 0.6426296490786287
Epoch: 1823, Batch Gradient Norm after: 0.6426296490786287
Epoch 1824/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010086712427437305
Epoch: 1824, Batch Gradient Norm: 0.6621873801007266
Epoch: 1824, Batch Gradient Norm after: 0.6621873801007266
Epoch 1825/10000, Prediction Accuracy = 59.31153846153847%, Loss = 0.010121035676162977
Epoch: 1825, Batch Gradient Norm: 0.632722370220305
Epoch: 1825, Batch Gradient Norm after: 0.632722370220305
Epoch 1826/10000, Prediction Accuracy = 59.334615384615375%, Loss = 0.010112639516592026
Epoch: 1826, Batch Gradient Norm: 0.6379022524236366
Epoch: 1826, Batch Gradient Norm after: 0.6379022524236366
Epoch 1827/10000, Prediction Accuracy = 59.43076923076924%, Loss = 0.010100743590066066
Epoch: 1827, Batch Gradient Norm: 0.6460729812665853
Epoch: 1827, Batch Gradient Norm after: 0.6460729812665853
Epoch 1828/10000, Prediction Accuracy = 59.37307692307692%, Loss = 0.010044823257395854
Epoch: 1828, Batch Gradient Norm: 0.6498507207617317
Epoch: 1828, Batch Gradient Norm after: 0.6498507207617317
Epoch 1829/10000, Prediction Accuracy = 59.19615384615385%, Loss = 0.010107044775325518
Epoch: 1829, Batch Gradient Norm: 0.6435223436760023
Epoch: 1829, Batch Gradient Norm after: 0.6435223436760023
Epoch 1830/10000, Prediction Accuracy = 59.24230769230769%, Loss = 0.010128221044746729
Epoch: 1830, Batch Gradient Norm: 0.6454820612514712
Epoch: 1830, Batch Gradient Norm after: 0.6454820612514712
Epoch 1831/10000, Prediction Accuracy = 59.37692307692308%, Loss = 0.010112364274951128
Epoch: 1831, Batch Gradient Norm: 0.6382704546465627
Epoch: 1831, Batch Gradient Norm after: 0.6382704546465627
Epoch 1832/10000, Prediction Accuracy = 59.619230769230775%, Loss = 0.01006004550995735
Epoch: 1832, Batch Gradient Norm: 0.6393919830149327
Epoch: 1832, Batch Gradient Norm after: 0.6393919830149327
Epoch 1833/10000, Prediction Accuracy = 59.70769230769231%, Loss = 0.010072830204780284
Epoch: 1833, Batch Gradient Norm: 0.6514838200587538
Epoch: 1833, Batch Gradient Norm after: 0.6514838200587538
Epoch 1834/10000, Prediction Accuracy = 59.39615384615384%, Loss = 0.01011528791143344
Epoch: 1834, Batch Gradient Norm: 0.6494320252118633
Epoch: 1834, Batch Gradient Norm after: 0.6494320252118633
Epoch 1835/10000, Prediction Accuracy = 59.396153846153844%, Loss = 0.010094392758149367
Epoch: 1835, Batch Gradient Norm: 0.6448274147798103
Epoch: 1835, Batch Gradient Norm after: 0.6448274147798103
Epoch 1836/10000, Prediction Accuracy = 59.40384615384615%, Loss = 0.010111933717360863
Epoch: 1836, Batch Gradient Norm: 0.6468901421020322
Epoch: 1836, Batch Gradient Norm after: 0.6468901421020322
Epoch 1837/10000, Prediction Accuracy = 59.33076923076923%, Loss = 0.010146638163580345
Epoch: 1837, Batch Gradient Norm: 0.6580577994936438
Epoch: 1837, Batch Gradient Norm after: 0.6580577994936438
Epoch 1838/10000, Prediction Accuracy = 59.576923076923066%, Loss = 0.010091338808146806
Epoch: 1838, Batch Gradient Norm: 0.6416925410734623
Epoch: 1838, Batch Gradient Norm after: 0.6416925410734623
Epoch 1839/10000, Prediction Accuracy = 59.380769230769225%, Loss = 0.010115083808509203
Epoch: 1839, Batch Gradient Norm: 0.6547953838696967
Epoch: 1839, Batch Gradient Norm after: 0.6547953838696967
Epoch 1840/10000, Prediction Accuracy = 59.09615384615385%, Loss = 0.010136081049075494
Epoch: 1840, Batch Gradient Norm: 0.6403900702438495
Epoch: 1840, Batch Gradient Norm after: 0.6403900702438495
Epoch 1841/10000, Prediction Accuracy = 59.35769230769232%, Loss = 0.010105573428938022
Epoch: 1841, Batch Gradient Norm: 0.6394009684522395
Epoch: 1841, Batch Gradient Norm after: 0.6394009684522395
Epoch 1842/10000, Prediction Accuracy = 59.48076923076923%, Loss = 0.010067737446381496
Epoch: 1842, Batch Gradient Norm: 0.6442432545060461
Epoch: 1842, Batch Gradient Norm after: 0.6442432545060461
Epoch 1843/10000, Prediction Accuracy = 59.530769230769224%, Loss = 0.010076242212492686
Epoch: 1843, Batch Gradient Norm: 0.6488099873880634
Epoch: 1843, Batch Gradient Norm after: 0.6488099873880634
Epoch 1844/10000, Prediction Accuracy = 59.630769230769225%, Loss = 0.01004171221015545
Epoch: 1844, Batch Gradient Norm: 0.6554365223635438
Epoch: 1844, Batch Gradient Norm after: 0.6554365223635438
Epoch 1845/10000, Prediction Accuracy = 59.32307692307693%, Loss = 0.010096467315004421
Epoch: 1845, Batch Gradient Norm: 0.6534575420107553
Epoch: 1845, Batch Gradient Norm after: 0.6534575420107553
Epoch 1846/10000, Prediction Accuracy = 59.51153846153846%, Loss = 0.010090322162096318
Epoch: 1846, Batch Gradient Norm: 0.6561078143124945
Epoch: 1846, Batch Gradient Norm after: 0.6561078143124945
Epoch 1847/10000, Prediction Accuracy = 59.44615384615385%, Loss = 0.010141673712776257
Epoch: 1847, Batch Gradient Norm: 0.6451006342637534
Epoch: 1847, Batch Gradient Norm after: 0.6451006342637534
Epoch 1848/10000, Prediction Accuracy = 59.588461538461544%, Loss = 0.010035436098965315
Epoch: 1848, Batch Gradient Norm: 0.6403104125525639
Epoch: 1848, Batch Gradient Norm after: 0.6403104125525639
Epoch 1849/10000, Prediction Accuracy = 59.23846153846153%, Loss = 0.010096040052863268
Epoch: 1849, Batch Gradient Norm: 0.6392190683826212
Epoch: 1849, Batch Gradient Norm after: 0.6392190683826212
Epoch 1850/10000, Prediction Accuracy = 59.49615384615384%, Loss = 0.01009009090753702
Epoch: 1850, Batch Gradient Norm: 0.6556509583628544
Epoch: 1850, Batch Gradient Norm after: 0.6556509583628544
Epoch 1851/10000, Prediction Accuracy = 59.415384615384625%, Loss = 0.01011701300740242
Epoch: 1851, Batch Gradient Norm: 0.6267633571324386
Epoch: 1851, Batch Gradient Norm after: 0.6267633571324386
Epoch 1852/10000, Prediction Accuracy = 59.75769230769231%, Loss = 0.010029734255602727
Epoch: 1852, Batch Gradient Norm: 0.6331865769128108
Epoch: 1852, Batch Gradient Norm after: 0.6331865769128108
Epoch 1853/10000, Prediction Accuracy = 59.33461538461539%, Loss = 0.010069720590343842
Epoch: 1853, Batch Gradient Norm: 0.6358878993166471
Epoch: 1853, Batch Gradient Norm after: 0.6358878993166471
Epoch 1854/10000, Prediction Accuracy = 59.44615384615384%, Loss = 0.010091909637244849
Epoch: 1854, Batch Gradient Norm: 0.6461328379731656
Epoch: 1854, Batch Gradient Norm after: 0.6461328379731656
Epoch 1855/10000, Prediction Accuracy = 59.365384615384606%, Loss = 0.010111748455808712
Epoch: 1855, Batch Gradient Norm: 0.649939606375176
Epoch: 1855, Batch Gradient Norm after: 0.649939606375176
Epoch 1856/10000, Prediction Accuracy = 59.49230769230769%, Loss = 0.010085287432257947
Epoch: 1856, Batch Gradient Norm: 0.650793764642818
Epoch: 1856, Batch Gradient Norm after: 0.650793764642818
Epoch 1857/10000, Prediction Accuracy = 59.38076923076923%, Loss = 0.010114464550637282
Epoch: 1857, Batch Gradient Norm: 0.6407371299856605
Epoch: 1857, Batch Gradient Norm after: 0.6407371299856605
Epoch 1858/10000, Prediction Accuracy = 59.365384615384606%, Loss = 0.010131398144249733
Epoch: 1858, Batch Gradient Norm: 0.638404346034319
Epoch: 1858, Batch Gradient Norm after: 0.638404346034319
Epoch 1859/10000, Prediction Accuracy = 59.56538461538461%, Loss = 0.010053891330384292
Epoch: 1859, Batch Gradient Norm: 0.6345106709284737
Epoch: 1859, Batch Gradient Norm after: 0.6345106709284737
Epoch 1860/10000, Prediction Accuracy = 59.56153846153846%, Loss = 0.010081448735525975
Epoch: 1860, Batch Gradient Norm: 0.6496896024339793
Epoch: 1860, Batch Gradient Norm after: 0.6496896024339793
Epoch 1861/10000, Prediction Accuracy = 59.54615384615385%, Loss = 0.01012856771166508
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode60-dqn/classify_rsmProp.py", line 162, in <module>
    outputs = clf(inputs)
              ^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode60-dqn/classify_rsmProp.py", line 63, in forward
    x = self.linear2(x)
        ^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt