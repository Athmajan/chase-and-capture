Epoch: 0, Batch Gradient Norm: 81.25975487178567
Epoch: 0, Batch Gradient Norm after: 22.360678848474134
Epoch 1/10000, Prediction Accuracy = 21.119999999999997%, Loss = 63.0358268737793
Epoch: 1, Batch Gradient Norm: 80.44307093551686
Epoch: 1, Batch Gradient Norm after: 22.36067965255015
Epoch 2/10000, Prediction Accuracy = 22.695999999999998%, Loss = 61.40454864501953
Epoch: 2, Batch Gradient Norm: 80.03182516209775
Epoch: 2, Batch Gradient Norm after: 22.36067948830245
Epoch 3/10000, Prediction Accuracy = 24.31%, Loss = 59.94353485107422
Epoch: 3, Batch Gradient Norm: 79.98992712666075
Epoch: 3, Batch Gradient Norm after: 22.360679158577938
Epoch 4/10000, Prediction Accuracy = 25.679999999999996%, Loss = 58.639599609375
Epoch: 4, Batch Gradient Norm: 80.17643136462358
Epoch: 4, Batch Gradient Norm after: 22.360681069547837
Epoch 5/10000, Prediction Accuracy = 27.065999999999995%, Loss = 57.45134353637695
Epoch: 5, Batch Gradient Norm: 80.47617587875193
Epoch: 5, Batch Gradient Norm after: 22.360679755603087
Epoch 6/10000, Prediction Accuracy = 28.46%, Loss = 56.35230941772461
Epoch: 6, Batch Gradient Norm: 80.82490982520284
Epoch: 6, Batch Gradient Norm after: 22.360679932379647
Epoch 7/10000, Prediction Accuracy = 29.520000000000003%, Loss = 55.31323394775391
Epoch: 7, Batch Gradient Norm: 81.19851193069938
Epoch: 7, Batch Gradient Norm after: 22.360679159815056
Epoch 8/10000, Prediction Accuracy = 30.29%, Loss = 54.313375091552736
Epoch: 8, Batch Gradient Norm: 81.5849706592566
Epoch: 8, Batch Gradient Norm after: 22.360679574132153
Epoch 9/10000, Prediction Accuracy = 31.026%, Loss = 53.34049606323242
Epoch: 9, Batch Gradient Norm: 81.96349487910985
Epoch: 9, Batch Gradient Norm after: 22.360679718691426
Epoch 10/10000, Prediction Accuracy = 31.595999999999997%, Loss = 52.39063568115235
Epoch: 10, Batch Gradient Norm: 82.32135139442593
Epoch: 10, Batch Gradient Norm after: 22.360680179615358
Epoch 11/10000, Prediction Accuracy = 32.309999999999995%, Loss = 51.46598358154297
Epoch: 11, Batch Gradient Norm: 82.65089102037676
Epoch: 11, Batch Gradient Norm after: 22.360679510201216
Epoch 12/10000, Prediction Accuracy = 32.852%, Loss = 50.56892318725586
Epoch: 12, Batch Gradient Norm: 82.95950541516505
Epoch: 12, Batch Gradient Norm after: 22.360679376376957
Epoch 13/10000, Prediction Accuracy = 33.30800000000001%, Loss = 49.69669647216797
Epoch: 13, Batch Gradient Norm: 83.23337437778696
Epoch: 13, Batch Gradient Norm after: 22.36067942167783
Epoch 14/10000, Prediction Accuracy = 33.656000000000006%, Loss = 48.84497222900391
Epoch: 14, Batch Gradient Norm: 83.45692145816061
Epoch: 14, Batch Gradient Norm after: 22.360680564616032
Epoch 15/10000, Prediction Accuracy = 33.715999999999994%, Loss = 48.01270980834961
Epoch: 15, Batch Gradient Norm: 83.61249907019973
Epoch: 15, Batch Gradient Norm after: 22.36068053275899
Epoch 16/10000, Prediction Accuracy = 34.019999999999996%, Loss = 47.20013580322266
Epoch: 16, Batch Gradient Norm: 83.70665136651515
Epoch: 16, Batch Gradient Norm after: 22.36068065942552
Epoch 17/10000, Prediction Accuracy = 34.212%, Loss = 46.40534362792969
Epoch: 17, Batch Gradient Norm: 83.73493864196358
Epoch: 17, Batch Gradient Norm after: 22.36068162934238
Epoch 18/10000, Prediction Accuracy = 34.498000000000005%, Loss = 45.6303596496582
Epoch: 18, Batch Gradient Norm: 83.70272776294745
Epoch: 18, Batch Gradient Norm after: 22.360680610767673
Epoch 19/10000, Prediction Accuracy = 34.791999999999994%, Loss = 44.87405090332031
Epoch: 19, Batch Gradient Norm: 83.60996974698828
Epoch: 19, Batch Gradient Norm after: 22.360680053233317
Epoch 20/10000, Prediction Accuracy = 34.988%, Loss = 44.132093048095705
Epoch: 20, Batch Gradient Norm: 83.47203867208958
Epoch: 20, Batch Gradient Norm after: 22.36068071902232
Epoch 21/10000, Prediction Accuracy = 35.224%, Loss = 43.40433197021484
Epoch: 21, Batch Gradient Norm: 83.27638881047184
Epoch: 21, Batch Gradient Norm after: 22.36068001823826
Epoch 22/10000, Prediction Accuracy = 35.39%, Loss = 42.69369735717773
Epoch: 22, Batch Gradient Norm: 83.05062173172095
Epoch: 22, Batch Gradient Norm after: 22.360678216884295
Epoch 23/10000, Prediction Accuracy = 35.47%, Loss = 41.992095947265625
Epoch: 23, Batch Gradient Norm: 82.7863739221755
Epoch: 23, Batch Gradient Norm after: 22.360679622584
Epoch 24/10000, Prediction Accuracy = 35.483999999999995%, Loss = 41.30247116088867
Epoch: 24, Batch Gradient Norm: 82.47805028662906
Epoch: 24, Batch Gradient Norm after: 22.360679730370258
Epoch 25/10000, Prediction Accuracy = 35.55799999999999%, Loss = 40.62624664306641
Epoch: 25, Batch Gradient Norm: 82.15521644830856
Epoch: 25, Batch Gradient Norm after: 22.360680110723802
Epoch 26/10000, Prediction Accuracy = 35.552%, Loss = 39.95300827026367
Epoch: 26, Batch Gradient Norm: 81.7982941213639
Epoch: 26, Batch Gradient Norm after: 22.36067969306878
Epoch 27/10000, Prediction Accuracy = 35.626%, Loss = 39.290767669677734
Epoch: 27, Batch Gradient Norm: 81.41151332039644
Epoch: 27, Batch Gradient Norm after: 22.360678333461713
Epoch 28/10000, Prediction Accuracy = 35.751999999999995%, Loss = 38.6359001159668
Epoch: 28, Batch Gradient Norm: 80.99581397538036
Epoch: 28, Batch Gradient Norm after: 22.360679463386095
Epoch 29/10000, Prediction Accuracy = 35.839999999999996%, Loss = 37.98848648071289
Epoch: 29, Batch Gradient Norm: 80.55302810463873
Epoch: 29, Batch Gradient Norm after: 22.360679060025948
Epoch 30/10000, Prediction Accuracy = 35.718%, Loss = 37.351356506347656
Epoch: 30, Batch Gradient Norm: 80.10953789209717
Epoch: 30, Batch Gradient Norm after: 22.360677935884944
Epoch 31/10000, Prediction Accuracy = 35.572%, Loss = 36.71209182739258
Epoch: 31, Batch Gradient Norm: 79.642209647311
Epoch: 31, Batch Gradient Norm after: 22.360679050304558
Epoch 32/10000, Prediction Accuracy = 35.486000000000004%, Loss = 36.08366012573242
Epoch: 32, Batch Gradient Norm: 79.15582858469072
Epoch: 32, Batch Gradient Norm after: 22.360680966086033
Epoch 33/10000, Prediction Accuracy = 35.386%, Loss = 35.46135635375977
Epoch: 33, Batch Gradient Norm: 78.65563252872477
Epoch: 33, Batch Gradient Norm after: 22.360681172308787
Epoch 34/10000, Prediction Accuracy = 35.298%, Loss = 34.841427612304685
Epoch: 34, Batch Gradient Norm: 78.12413514858889
Epoch: 34, Batch Gradient Norm after: 22.36067806471246
Epoch 35/10000, Prediction Accuracy = 35.227999999999994%, Loss = 34.23147277832031
Epoch: 35, Batch Gradient Norm: 77.56942280786838
Epoch: 35, Batch Gradient Norm after: 22.360678771598693
Epoch 36/10000, Prediction Accuracy = 35.2%, Loss = 33.62509689331055
Epoch: 36, Batch Gradient Norm: 76.9965396549518
Epoch: 36, Batch Gradient Norm after: 22.36067955910274
Epoch 37/10000, Prediction Accuracy = 35.266%, Loss = 33.0269416809082
Epoch: 37, Batch Gradient Norm: 76.4111913806444
Epoch: 37, Batch Gradient Norm after: 22.36067831597761
Epoch 38/10000, Prediction Accuracy = 35.236000000000004%, Loss = 32.43389129638672
Epoch: 38, Batch Gradient Norm: 75.82554170786597
Epoch: 38, Batch Gradient Norm after: 22.360678917373143
Epoch 39/10000, Prediction Accuracy = 35.300000000000004%, Loss = 31.845425033569335
Epoch: 39, Batch Gradient Norm: 75.24517031857219
Epoch: 39, Batch Gradient Norm after: 22.360679199134122
Epoch 40/10000, Prediction Accuracy = 35.406%, Loss = 31.256524276733398
Epoch: 40, Batch Gradient Norm: 74.66044108290741
Epoch: 40, Batch Gradient Norm after: 22.36067904991872
Epoch 41/10000, Prediction Accuracy = 35.504000000000005%, Loss = 30.672988891601562
Epoch: 41, Batch Gradient Norm: 74.05747875572429
Epoch: 41, Batch Gradient Norm after: 22.3606799341657
Epoch 42/10000, Prediction Accuracy = 35.522000000000006%, Loss = 30.093593215942384
Epoch: 42, Batch Gradient Norm: 73.44110523284681
Epoch: 42, Batch Gradient Norm after: 22.360681476560444
Epoch 43/10000, Prediction Accuracy = 35.428%, Loss = 29.520362091064452
Epoch: 43, Batch Gradient Norm: 72.80865663066905
Epoch: 43, Batch Gradient Norm after: 22.360678191463567
Epoch 44/10000, Prediction Accuracy = 35.306000000000004%, Loss = 28.94822769165039
Epoch: 44, Batch Gradient Norm: 72.16172548249894
Epoch: 44, Batch Gradient Norm after: 22.360679870753646
Epoch 45/10000, Prediction Accuracy = 35.234%, Loss = 28.380954360961915
Epoch: 45, Batch Gradient Norm: 71.50494070090798
Epoch: 45, Batch Gradient Norm after: 22.360678333592926
Epoch 46/10000, Prediction Accuracy = 35.16%, Loss = 27.821764755249024
Epoch: 46, Batch Gradient Norm: 70.82953590598733
Epoch: 46, Batch Gradient Norm after: 22.360678416264367
Epoch 47/10000, Prediction Accuracy = 35.17%, Loss = 27.26573486328125
Epoch: 47, Batch Gradient Norm: 70.15031890113197
Epoch: 47, Batch Gradient Norm after: 22.36067798298529
Epoch 48/10000, Prediction Accuracy = 35.136%, Loss = 26.714763259887697
Epoch: 48, Batch Gradient Norm: 69.45963822028641
Epoch: 48, Batch Gradient Norm after: 22.36067911615842
Epoch 49/10000, Prediction Accuracy = 35.114%, Loss = 26.170378494262696
Epoch: 49, Batch Gradient Norm: 68.75441266454243
Epoch: 49, Batch Gradient Norm after: 22.36067809565979
Epoch 50/10000, Prediction Accuracy = 35.174%, Loss = 25.626106643676756
Epoch: 50, Batch Gradient Norm: 68.02986879955645
Epoch: 50, Batch Gradient Norm after: 22.360679287122768
Epoch 51/10000, Prediction Accuracy = 35.330000000000005%, Loss = 25.08896675109863
Epoch: 51, Batch Gradient Norm: 67.29092865824397
Epoch: 51, Batch Gradient Norm after: 22.360678901347413
Epoch 52/10000, Prediction Accuracy = 35.444%, Loss = 24.559440994262694
Epoch: 52, Batch Gradient Norm: 66.54561641341331
Epoch: 52, Batch Gradient Norm after: 22.36067822333242
Epoch 53/10000, Prediction Accuracy = 35.678%, Loss = 24.036842346191406
Epoch: 53, Batch Gradient Norm: 65.80857168355435
Epoch: 53, Batch Gradient Norm after: 22.36067842945937
Epoch 54/10000, Prediction Accuracy = 35.908%, Loss = 23.510967254638672
Epoch: 54, Batch Gradient Norm: 65.0591546464407
Epoch: 54, Batch Gradient Norm after: 22.360677541322893
Epoch 55/10000, Prediction Accuracy = 36.076%, Loss = 22.99430046081543
Epoch: 55, Batch Gradient Norm: 64.3078750579188
Epoch: 55, Batch Gradient Norm after: 22.360677593695335
Epoch 56/10000, Prediction Accuracy = 36.354%, Loss = 22.48103904724121
Epoch: 56, Batch Gradient Norm: 63.547998355080914
Epoch: 56, Batch Gradient Norm after: 22.360678742809682
Epoch 57/10000, Prediction Accuracy = 36.428%, Loss = 21.974027252197267
Epoch: 57, Batch Gradient Norm: 62.77683854422991
Epoch: 57, Batch Gradient Norm after: 22.360679678236846
Epoch 58/10000, Prediction Accuracy = 36.646%, Loss = 21.471502685546874
Epoch: 58, Batch Gradient Norm: 62.00004141035091
Epoch: 58, Batch Gradient Norm after: 22.36067863520546
Epoch 59/10000, Prediction Accuracy = 36.872%, Loss = 20.97032241821289
Epoch: 59, Batch Gradient Norm: 61.21003438616166
Epoch: 59, Batch Gradient Norm after: 22.36067763036795
Epoch 60/10000, Prediction Accuracy = 36.932%, Loss = 20.477584075927734
Epoch: 60, Batch Gradient Norm: 60.41782049480683
Epoch: 60, Batch Gradient Norm after: 22.360678253958802
Epoch 61/10000, Prediction Accuracy = 37.040000000000006%, Loss = 19.988092803955077
Epoch: 61, Batch Gradient Norm: 59.61036244757843
Epoch: 61, Batch Gradient Norm after: 22.36067728608393
Epoch 62/10000, Prediction Accuracy = 37.196000000000005%, Loss = 19.513188934326173
Epoch: 62, Batch Gradient Norm: 58.809281082199426
Epoch: 62, Batch Gradient Norm after: 22.36068014365758
Epoch 63/10000, Prediction Accuracy = 37.156000000000006%, Loss = 19.0220458984375
Epoch: 63, Batch Gradient Norm: 58.00149954067315
Epoch: 63, Batch Gradient Norm after: 22.360678120241133
Epoch 64/10000, Prediction Accuracy = 37.338%, Loss = 18.54729309082031
Epoch: 64, Batch Gradient Norm: 57.179330180789904
Epoch: 64, Batch Gradient Norm after: 22.36067751751849
Epoch 65/10000, Prediction Accuracy = 37.4%, Loss = 18.078747940063476
Epoch: 65, Batch Gradient Norm: 56.35861954722308
Epoch: 65, Batch Gradient Norm after: 22.360678404711653
Epoch 66/10000, Prediction Accuracy = 37.534%, Loss = 17.609364318847657
Epoch: 66, Batch Gradient Norm: 55.529853328694486
Epoch: 66, Batch Gradient Norm after: 22.36067800219254
Epoch 67/10000, Prediction Accuracy = 37.614%, Loss = 17.1523494720459
Epoch: 67, Batch Gradient Norm: 54.68559004074657
Epoch: 67, Batch Gradient Norm after: 22.36067859630669
Epoch 68/10000, Prediction Accuracy = 37.784%, Loss = 16.695597457885743
Epoch: 68, Batch Gradient Norm: 53.825659122597635
Epoch: 68, Batch Gradient Norm after: 22.36067884728946
Epoch 69/10000, Prediction Accuracy = 37.80200000000001%, Loss = 16.249916076660156
Epoch: 69, Batch Gradient Norm: 52.974311910354494
Epoch: 69, Batch Gradient Norm after: 22.3606794791043
Epoch 70/10000, Prediction Accuracy = 37.804%, Loss = 15.807400512695313
Epoch: 70, Batch Gradient Norm: 52.10259798117212
Epoch: 70, Batch Gradient Norm after: 22.360677376121366
Epoch 71/10000, Prediction Accuracy = 37.791999999999994%, Loss = 15.363907814025879
Epoch: 71, Batch Gradient Norm: 51.20993028785398
Epoch: 71, Batch Gradient Norm after: 22.360678234735545
Epoch 72/10000, Prediction Accuracy = 37.83%, Loss = 14.937527275085449
Epoch: 72, Batch Gradient Norm: 50.33615999555083
Epoch: 72, Batch Gradient Norm after: 22.360677205487047
Epoch 73/10000, Prediction Accuracy = 37.992%, Loss = 14.512346649169922
Epoch: 73, Batch Gradient Norm: 49.464816037007395
Epoch: 73, Batch Gradient Norm after: 22.360676284869356
Epoch 74/10000, Prediction Accuracy = 38.102%, Loss = 14.09647731781006
Epoch: 74, Batch Gradient Norm: 48.56031792305739
Epoch: 74, Batch Gradient Norm after: 22.36067764083844
Epoch 75/10000, Prediction Accuracy = 38.24400000000001%, Loss = 13.685713958740234
Epoch: 75, Batch Gradient Norm: 47.680198470351705
Epoch: 75, Batch Gradient Norm after: 22.36067792121435
Epoch 76/10000, Prediction Accuracy = 38.35600000000001%, Loss = 13.276525497436523
Epoch: 76, Batch Gradient Norm: 46.794863060094805
Epoch: 76, Batch Gradient Norm after: 22.360677984489335
Epoch 77/10000, Prediction Accuracy = 38.528000000000006%, Loss = 12.875136947631836
Epoch: 77, Batch Gradient Norm: 45.90942250203117
Epoch: 77, Batch Gradient Norm after: 22.360678817935877
Epoch 78/10000, Prediction Accuracy = 38.638%, Loss = 12.483263206481933
Epoch: 78, Batch Gradient Norm: 45.0288967326562
Epoch: 78, Batch Gradient Norm after: 22.360678533625716
Epoch 79/10000, Prediction Accuracy = 38.726%, Loss = 12.096961784362794
Epoch: 79, Batch Gradient Norm: 44.1730354446317
Epoch: 79, Batch Gradient Norm after: 22.360676432439334
Epoch 80/10000, Prediction Accuracy = 38.848%, Loss = 11.71521453857422
Epoch: 80, Batch Gradient Norm: 43.314525057137665
Epoch: 80, Batch Gradient Norm after: 22.36067815675083
Epoch 81/10000, Prediction Accuracy = 39.056%, Loss = 11.33891773223877
Epoch: 81, Batch Gradient Norm: 42.424209739396126
Epoch: 81, Batch Gradient Norm after: 22.360676533861312
Epoch 82/10000, Prediction Accuracy = 39.08%, Loss = 10.970067596435547
Epoch: 82, Batch Gradient Norm: 41.547955684380064
Epoch: 82, Batch Gradient Norm after: 22.360678120382783
Epoch 83/10000, Prediction Accuracy = 39.17%, Loss = 10.60416088104248
Epoch: 83, Batch Gradient Norm: 40.689444134427134
Epoch: 83, Batch Gradient Norm after: 22.360677080850955
Epoch 84/10000, Prediction Accuracy = 39.074%, Loss = 10.250926780700684
Epoch: 84, Batch Gradient Norm: 39.815578242324
Epoch: 84, Batch Gradient Norm after: 22.360676583174296
Epoch 85/10000, Prediction Accuracy = 38.914%, Loss = 9.902617454528809
Epoch: 85, Batch Gradient Norm: 38.94309719254943
Epoch: 85, Batch Gradient Norm after: 22.360676982393233
Epoch 86/10000, Prediction Accuracy = 38.674%, Loss = 9.556778717041016
Epoch: 86, Batch Gradient Norm: 38.07663042153254
Epoch: 86, Batch Gradient Norm after: 22.360678187407533
Epoch 87/10000, Prediction Accuracy = 38.568%, Loss = 9.224815368652344
Epoch: 87, Batch Gradient Norm: 37.18619616235627
Epoch: 87, Batch Gradient Norm after: 22.36067640065794
Epoch 88/10000, Prediction Accuracy = 38.376%, Loss = 8.893381118774414
Epoch: 88, Batch Gradient Norm: 36.312568814314496
Epoch: 88, Batch Gradient Norm after: 22.360677139939884
Epoch 89/10000, Prediction Accuracy = 38.208%, Loss = 8.57076187133789
Epoch: 89, Batch Gradient Norm: 35.433853948337884
Epoch: 89, Batch Gradient Norm after: 22.36067705499588
Epoch 90/10000, Prediction Accuracy = 38.192%, Loss = 8.262827110290527
Epoch: 90, Batch Gradient Norm: 34.54774161944855
Epoch: 90, Batch Gradient Norm after: 22.360677422524077
Epoch 91/10000, Prediction Accuracy = 38.224000000000004%, Loss = 7.9553914070129395
Epoch: 91, Batch Gradient Norm: 33.6349699031642
Epoch: 91, Batch Gradient Norm after: 22.360678494283984
Epoch 92/10000, Prediction Accuracy = 38.2%, Loss = 7.653983783721924
Epoch: 92, Batch Gradient Norm: 32.80192202260351
Epoch: 92, Batch Gradient Norm after: 22.36067790169757
Epoch 93/10000, Prediction Accuracy = 38.334%, Loss = 7.366490745544434
Epoch: 93, Batch Gradient Norm: 31.953799882327914
Epoch: 93, Batch Gradient Norm after: 22.360678520314487
Epoch 94/10000, Prediction Accuracy = 38.498000000000005%, Loss = 7.08900032043457
Epoch: 94, Batch Gradient Norm: 31.05691631340603
Epoch: 94, Batch Gradient Norm after: 22.360679646286446
Epoch 95/10000, Prediction Accuracy = 38.688%, Loss = 6.808465576171875
Epoch: 95, Batch Gradient Norm: 30.147869641384972
Epoch: 95, Batch Gradient Norm after: 22.360677219049883
Epoch 96/10000, Prediction Accuracy = 38.89800000000001%, Loss = 6.538122844696045
Epoch: 96, Batch Gradient Norm: 29.296226856009653
Epoch: 96, Batch Gradient Norm after: 22.36067841406953
Epoch 97/10000, Prediction Accuracy = 39.106%, Loss = 6.281747436523437
Epoch: 97, Batch Gradient Norm: 28.4822904541874
Epoch: 97, Batch Gradient Norm after: 22.360677746339515
Epoch 98/10000, Prediction Accuracy = 39.366%, Loss = 6.026630783081055
Epoch: 98, Batch Gradient Norm: 27.68824419312404
Epoch: 98, Batch Gradient Norm after: 22.36067873680511
Epoch 99/10000, Prediction Accuracy = 39.67%, Loss = 5.771799755096436
Epoch: 99, Batch Gradient Norm: 26.859175889379753
Epoch: 99, Batch Gradient Norm after: 22.36067833819851
Epoch 100/10000, Prediction Accuracy = 39.928000000000004%, Loss = 5.51757173538208
Epoch: 100, Batch Gradient Norm: 26.072137430234896
Epoch: 100, Batch Gradient Norm after: 22.360678449798687
Epoch 101/10000, Prediction Accuracy = 39.92%, Loss = 5.280066204071045
Epoch: 101, Batch Gradient Norm: 25.291656776265583
Epoch: 101, Batch Gradient Norm after: 22.360678739394313
Epoch 102/10000, Prediction Accuracy = 39.75599999999999%, Loss = 5.054471683502197
Epoch: 102, Batch Gradient Norm: 24.4708809666788
Epoch: 102, Batch Gradient Norm after: 22.360679122963777
Epoch 103/10000, Prediction Accuracy = 39.698%, Loss = 4.832591247558594
Epoch: 103, Batch Gradient Norm: 23.705809742029153
Epoch: 103, Batch Gradient Norm after: 22.360678021451218
Epoch 104/10000, Prediction Accuracy = 39.594%, Loss = 4.625747394561768
Epoch: 104, Batch Gradient Norm: 23.030854512175864
Epoch: 104, Batch Gradient Norm after: 22.266609289403043
Epoch 105/10000, Prediction Accuracy = 39.54600000000001%, Loss = 4.420680904388428
Epoch: 105, Batch Gradient Norm: 22.195367114567627
Epoch: 105, Batch Gradient Norm after: 21.837162040976953
Epoch 106/10000, Prediction Accuracy = 39.594%, Loss = 4.220375823974609
Epoch: 106, Batch Gradient Norm: 21.444518014431768
Epoch: 106, Batch Gradient Norm after: 21.404404118549994
Epoch 107/10000, Prediction Accuracy = 39.846000000000004%, Loss = 4.020022535324097
Epoch: 107, Batch Gradient Norm: 20.628978426960813
Epoch: 107, Batch Gradient Norm after: 20.628978426960813
Epoch 108/10000, Prediction Accuracy = 40.046%, Loss = 3.8354235649108888
Epoch: 108, Batch Gradient Norm: 19.87540082469328
Epoch: 108, Batch Gradient Norm after: 19.87540082469328
Epoch 109/10000, Prediction Accuracy = 40.108%, Loss = 3.662310028076172
Epoch: 109, Batch Gradient Norm: 19.206307379980178
Epoch: 109, Batch Gradient Norm after: 19.206307379980178
Epoch 110/10000, Prediction Accuracy = 40.102%, Loss = 3.499114179611206
Epoch: 110, Batch Gradient Norm: 18.556304961712346
Epoch: 110, Batch Gradient Norm after: 18.556304961712346
Epoch 111/10000, Prediction Accuracy = 40.094%, Loss = 3.346510410308838
Epoch: 111, Batch Gradient Norm: 17.973621327916835
Epoch: 111, Batch Gradient Norm after: 17.973621327916835
Epoch 112/10000, Prediction Accuracy = 41.028%, Loss = 3.200325632095337
Epoch: 112, Batch Gradient Norm: 17.387491513405035
Epoch: 112, Batch Gradient Norm after: 17.387491513405035
Epoch 113/10000, Prediction Accuracy = 41.672000000000004%, Loss = 3.0756901264190675
Epoch: 113, Batch Gradient Norm: 16.8402105164926
Epoch: 113, Batch Gradient Norm after: 16.8402105164926
Epoch 114/10000, Prediction Accuracy = 41.510000000000005%, Loss = 2.945462703704834
Epoch: 114, Batch Gradient Norm: 16.234121471341748
Epoch: 114, Batch Gradient Norm after: 16.234121471341748
Epoch 115/10000, Prediction Accuracy = 41.458%, Loss = 2.8279295444488524
Epoch: 115, Batch Gradient Norm: 15.65352818657563
Epoch: 115, Batch Gradient Norm after: 15.65352818657563
Epoch 116/10000, Prediction Accuracy = 41.477999999999994%, Loss = 2.7194856643676757
Epoch: 116, Batch Gradient Norm: 15.15715269165681
Epoch: 116, Batch Gradient Norm after: 15.15715269165681
Epoch 117/10000, Prediction Accuracy = 41.6%, Loss = 2.6191460132598876
Epoch: 117, Batch Gradient Norm: 14.740301762239959
Epoch: 117, Batch Gradient Norm after: 14.740301762239959
Epoch 118/10000, Prediction Accuracy = 41.774%, Loss = 2.5241394996643067
Epoch: 118, Batch Gradient Norm: 14.330956672135127
Epoch: 118, Batch Gradient Norm after: 14.330956672135127
Epoch 119/10000, Prediction Accuracy = 41.821999999999996%, Loss = 2.433150863647461
Epoch: 119, Batch Gradient Norm: 13.918606903545108
Epoch: 119, Batch Gradient Norm after: 13.918606903545108
Epoch 120/10000, Prediction Accuracy = 41.903999999999996%, Loss = 2.347700595855713
Epoch: 120, Batch Gradient Norm: 13.440192753688999
Epoch: 120, Batch Gradient Norm after: 13.440192753688999
Epoch 121/10000, Prediction Accuracy = 42.138%, Loss = 2.2709943771362306
Epoch: 121, Batch Gradient Norm: 13.030389988644888
Epoch: 121, Batch Gradient Norm after: 13.030389988644888
Epoch 122/10000, Prediction Accuracy = 42.226%, Loss = 2.1968424320220947
Epoch: 122, Batch Gradient Norm: 12.613434765319868
Epoch: 122, Batch Gradient Norm after: 12.613434765319868
Epoch 123/10000, Prediction Accuracy = 42.494%, Loss = 2.1256605625152587
Epoch: 123, Batch Gradient Norm: 12.426397252235638
Epoch: 123, Batch Gradient Norm after: 12.426397252235638
Epoch 124/10000, Prediction Accuracy = 42.726%, Loss = 2.06058874130249
Epoch: 124, Batch Gradient Norm: 11.928056458416721
Epoch: 124, Batch Gradient Norm after: 11.928056458416721
Epoch 125/10000, Prediction Accuracy = 42.86%, Loss = 2.002849578857422
Epoch: 125, Batch Gradient Norm: 11.739321911641623
Epoch: 125, Batch Gradient Norm after: 11.739321911641623
Epoch 126/10000, Prediction Accuracy = 42.85%, Loss = 1.9455252885818481
Epoch: 126, Batch Gradient Norm: 11.365674785351153
Epoch: 126, Batch Gradient Norm after: 11.365674785351153
Epoch 127/10000, Prediction Accuracy = 42.815999999999995%, Loss = 1.886103582382202
Epoch: 127, Batch Gradient Norm: 10.830156926786627
Epoch: 127, Batch Gradient Norm after: 10.830156926786627
Epoch 128/10000, Prediction Accuracy = 42.846%, Loss = 1.8297722339630127
Epoch: 128, Batch Gradient Norm: 10.629876324517241
Epoch: 128, Batch Gradient Norm after: 10.629876324517241
Epoch 129/10000, Prediction Accuracy = 42.904%, Loss = 1.787920093536377
Epoch: 129, Batch Gradient Norm: 10.362918227773719
Epoch: 129, Batch Gradient Norm after: 10.362918227773719
Epoch 130/10000, Prediction Accuracy = 43.08%, Loss = 1.7444313049316407
Epoch: 130, Batch Gradient Norm: 10.099494188082021
Epoch: 130, Batch Gradient Norm after: 10.099494188082021
Epoch 131/10000, Prediction Accuracy = 43.338%, Loss = 1.69796781539917
Epoch: 131, Batch Gradient Norm: 10.255478397227392
Epoch: 131, Batch Gradient Norm after: 10.255478397227392
Epoch 132/10000, Prediction Accuracy = 43.522000000000006%, Loss = 1.6637404203414916
Epoch: 132, Batch Gradient Norm: 9.600645864742184
Epoch: 132, Batch Gradient Norm after: 9.600645864742184
Epoch 133/10000, Prediction Accuracy = 43.848%, Loss = 1.6198683500289917
Epoch: 133, Batch Gradient Norm: 9.314520481876869
Epoch: 133, Batch Gradient Norm after: 9.314520481876869
Epoch 134/10000, Prediction Accuracy = 44.035999999999994%, Loss = 1.5839555263519287
Epoch: 134, Batch Gradient Norm: 9.075182691633998
Epoch: 134, Batch Gradient Norm after: 9.075182691633998
Epoch 135/10000, Prediction Accuracy = 43.908%, Loss = 1.5322083234786987
Epoch: 135, Batch Gradient Norm: 8.913506533368581
Epoch: 135, Batch Gradient Norm after: 8.913506533368581
Epoch 136/10000, Prediction Accuracy = 44.12%, Loss = 1.4773006916046143
Epoch: 136, Batch Gradient Norm: 8.604307354680753
Epoch: 136, Batch Gradient Norm after: 8.604307354680753
Epoch 137/10000, Prediction Accuracy = 45.882%, Loss = 1.4269095182418823
Epoch: 137, Batch Gradient Norm: 8.287128437549068
Epoch: 137, Batch Gradient Norm after: 8.287128437549068
Epoch 138/10000, Prediction Accuracy = 46.812%, Loss = 1.3958757400512696
Epoch: 138, Batch Gradient Norm: 8.54876726771789
Epoch: 138, Batch Gradient Norm after: 8.54876726771789
Epoch 139/10000, Prediction Accuracy = 47.044000000000004%, Loss = 1.3689021348953248
Epoch: 139, Batch Gradient Norm: 8.643088774657505
Epoch: 139, Batch Gradient Norm after: 8.643088774657505
Epoch 140/10000, Prediction Accuracy = 47.273999999999994%, Loss = 1.3495858907699585
Epoch: 140, Batch Gradient Norm: 7.972947795155345
Epoch: 140, Batch Gradient Norm after: 7.972947795155345
Epoch 141/10000, Prediction Accuracy = 47.455999999999996%, Loss = 1.3076350688934326
Epoch: 141, Batch Gradient Norm: 7.6200361522445
Epoch: 141, Batch Gradient Norm after: 7.6200361522445
Epoch 142/10000, Prediction Accuracy = 46.565999999999995%, Loss = 1.2662114381790162
Epoch: 142, Batch Gradient Norm: 7.665498703231968
Epoch: 142, Batch Gradient Norm after: 7.665498703231968
Epoch 143/10000, Prediction Accuracy = 46.638%, Loss = 1.2348838090896606
Epoch: 143, Batch Gradient Norm: 6.8990225464596815
Epoch: 143, Batch Gradient Norm after: 6.8990225464596815
Epoch 144/10000, Prediction Accuracy = 47.456%, Loss = 1.2120740175247193
Epoch: 144, Batch Gradient Norm: 6.6610708145803095
Epoch: 144, Batch Gradient Norm after: 6.6610708145803095
Epoch 145/10000, Prediction Accuracy = 48.262%, Loss = 1.1951216220855714
Epoch: 145, Batch Gradient Norm: 6.8249541624115455
Epoch: 145, Batch Gradient Norm after: 6.8249541624115455
Epoch 146/10000, Prediction Accuracy = 48.410000000000004%, Loss = 1.1794701099395752
Epoch: 146, Batch Gradient Norm: 6.14218165559215
Epoch: 146, Batch Gradient Norm after: 6.14218165559215
Epoch 147/10000, Prediction Accuracy = 48.50600000000001%, Loss = 1.1562521934509278
Epoch: 147, Batch Gradient Norm: 6.098100943119435
Epoch: 147, Batch Gradient Norm after: 6.098100943119435
Epoch 148/10000, Prediction Accuracy = 48.727999999999994%, Loss = 1.1382336616516113
Epoch: 148, Batch Gradient Norm: 5.9908052767651565
Epoch: 148, Batch Gradient Norm after: 5.9908052767651565
Epoch 149/10000, Prediction Accuracy = 48.902%, Loss = 1.1202452421188354
Epoch: 149, Batch Gradient Norm: 6.008325576840514
Epoch: 149, Batch Gradient Norm after: 6.008325576840514
Epoch 150/10000, Prediction Accuracy = 48.882000000000005%, Loss = 1.1060483694076537
Epoch: 150, Batch Gradient Norm: 5.741572403304101
Epoch: 150, Batch Gradient Norm after: 5.741572403304101
Epoch 151/10000, Prediction Accuracy = 49.128%, Loss = 1.0951635360717773
Epoch: 151, Batch Gradient Norm: 5.688987964342054
Epoch: 151, Batch Gradient Norm after: 5.688987964342054
Epoch 152/10000, Prediction Accuracy = 48.784%, Loss = 1.077229881286621
Epoch: 152, Batch Gradient Norm: 6.664161697985426
Epoch: 152, Batch Gradient Norm after: 6.664161697985426
Epoch 153/10000, Prediction Accuracy = 48.148%, Loss = 1.0697630643844604
Epoch: 153, Batch Gradient Norm: 6.621445424291898
Epoch: 153, Batch Gradient Norm after: 6.621445424291898
Epoch 154/10000, Prediction Accuracy = 48.384%, Loss = 1.0618865728378295
Epoch: 154, Batch Gradient Norm: 6.854327307208256
Epoch: 154, Batch Gradient Norm after: 6.854327307208256
Epoch 155/10000, Prediction Accuracy = 48.766%, Loss = 1.0433382749557496
Epoch: 155, Batch Gradient Norm: 6.994866983952285
Epoch: 155, Batch Gradient Norm after: 6.994866983952285
Epoch 156/10000, Prediction Accuracy = 49.07000000000001%, Loss = 1.0372431516647338
Epoch: 156, Batch Gradient Norm: 6.85249116126592
Epoch: 156, Batch Gradient Norm after: 6.85249116126592
Epoch 157/10000, Prediction Accuracy = 49.246%, Loss = 1.0263669848442079
Epoch: 157, Batch Gradient Norm: 6.3959572630902
Epoch: 157, Batch Gradient Norm after: 6.3959572630902
Epoch 158/10000, Prediction Accuracy = 48.826%, Loss = 1.0133020758628846
Epoch: 158, Batch Gradient Norm: 6.469794333801849
Epoch: 158, Batch Gradient Norm after: 6.469794333801849
Epoch 159/10000, Prediction Accuracy = 49.33200000000001%, Loss = 1.006760573387146
Epoch: 159, Batch Gradient Norm: 7.073788826151625
Epoch: 159, Batch Gradient Norm after: 7.073788826151625
Epoch 160/10000, Prediction Accuracy = 49.742000000000004%, Loss = 1.0049636363983154
Epoch: 160, Batch Gradient Norm: 5.920519210560351
Epoch: 160, Batch Gradient Norm after: 5.920519210560351
Epoch 161/10000, Prediction Accuracy = 49.854%, Loss = 0.994312858581543
Epoch: 161, Batch Gradient Norm: 5.650126923298092
Epoch: 161, Batch Gradient Norm after: 5.650126923298092
Epoch 162/10000, Prediction Accuracy = 49.898%, Loss = 0.984725558757782
Epoch: 162, Batch Gradient Norm: 5.453103645989654
Epoch: 162, Batch Gradient Norm after: 5.453103645989654
Epoch 163/10000, Prediction Accuracy = 50.184%, Loss = 0.9762735366821289
Epoch: 163, Batch Gradient Norm: 5.619208867424647
Epoch: 163, Batch Gradient Norm after: 5.619208867424647
Epoch 164/10000, Prediction Accuracy = 50.516%, Loss = 0.9715563297271729
Epoch: 164, Batch Gradient Norm: 6.420733206781996
Epoch: 164, Batch Gradient Norm after: 6.420733206781996
Epoch 165/10000, Prediction Accuracy = 50.730000000000004%, Loss = 0.968640661239624
Epoch: 165, Batch Gradient Norm: 5.863681266634591
Epoch: 165, Batch Gradient Norm after: 5.863681266634591
Epoch 166/10000, Prediction Accuracy = 50.824%, Loss = 0.9562353610992431
Epoch: 166, Batch Gradient Norm: 4.825762786777611
Epoch: 166, Batch Gradient Norm after: 4.825762786777611
Epoch 167/10000, Prediction Accuracy = 50.95399999999999%, Loss = 0.9475084662437439
Epoch: 167, Batch Gradient Norm: 4.255155663057436
Epoch: 167, Batch Gradient Norm after: 4.255155663057436
Epoch 168/10000, Prediction Accuracy = 51.098%, Loss = 0.9429400324821472
Epoch: 168, Batch Gradient Norm: 3.751286391027536
Epoch: 168, Batch Gradient Norm after: 3.751286391027536
Epoch 169/10000, Prediction Accuracy = 51.220000000000006%, Loss = 0.9349947452545166
Epoch: 169, Batch Gradient Norm: 4.557998421446023
Epoch: 169, Batch Gradient Norm after: 4.557998421446023
Epoch 170/10000, Prediction Accuracy = 51.38199999999999%, Loss = 0.9398110866546631
Epoch: 170, Batch Gradient Norm: 4.048457784409868
Epoch: 170, Batch Gradient Norm after: 4.048457784409868
Epoch 171/10000, Prediction Accuracy = 51.41799999999999%, Loss = 0.9287193179130554
Epoch: 171, Batch Gradient Norm: 4.525099630934396
Epoch: 171, Batch Gradient Norm after: 4.525099630934396
Epoch 172/10000, Prediction Accuracy = 51.532%, Loss = 0.9228045582771301
Epoch: 172, Batch Gradient Norm: 5.1269773042769025
Epoch: 172, Batch Gradient Norm after: 5.1269773042769025
Epoch 173/10000, Prediction Accuracy = 51.55%, Loss = 0.926843011379242
Epoch: 173, Batch Gradient Norm: 5.180225013119462
Epoch: 173, Batch Gradient Norm after: 5.180225013119462
Epoch 174/10000, Prediction Accuracy = 51.718%, Loss = 0.9150229334831238
Epoch: 174, Batch Gradient Norm: 5.100557789307631
Epoch: 174, Batch Gradient Norm after: 5.100557789307631
Epoch 175/10000, Prediction Accuracy = 51.803999999999995%, Loss = 0.9122971653938293
Epoch: 175, Batch Gradient Norm: 4.689666619663507
Epoch: 175, Batch Gradient Norm after: 4.689666619663507
Epoch 176/10000, Prediction Accuracy = 51.84400000000001%, Loss = 0.9112935900688172
Epoch: 176, Batch Gradient Norm: 5.669027082164799
Epoch: 176, Batch Gradient Norm after: 5.669027082164799
Epoch 177/10000, Prediction Accuracy = 52.077999999999996%, Loss = 0.9078122973442078
Epoch: 177, Batch Gradient Norm: 4.774012077092868
Epoch: 177, Batch Gradient Norm after: 4.774012077092868
Epoch 178/10000, Prediction Accuracy = 52.074%, Loss = 0.897021758556366
Epoch: 178, Batch Gradient Norm: 4.9616528551599295
Epoch: 178, Batch Gradient Norm after: 4.9616528551599295
Epoch 179/10000, Prediction Accuracy = 52.01800000000001%, Loss = 0.898157274723053
Epoch: 179, Batch Gradient Norm: 4.4715747884128625
Epoch: 179, Batch Gradient Norm after: 4.4715747884128625
Epoch 180/10000, Prediction Accuracy = 52.234%, Loss = 0.8976769566535949
Epoch: 180, Batch Gradient Norm: 4.5009647511369435
Epoch: 180, Batch Gradient Norm after: 4.5009647511369435
Epoch 181/10000, Prediction Accuracy = 52.348%, Loss = 0.8877859830856323
Epoch: 181, Batch Gradient Norm: 4.174980562699974
Epoch: 181, Batch Gradient Norm after: 4.174980562699974
Epoch 182/10000, Prediction Accuracy = 52.470000000000006%, Loss = 0.8819398283958435
Epoch: 182, Batch Gradient Norm: 5.345095017391008
Epoch: 182, Batch Gradient Norm after: 5.345095017391008
Epoch 183/10000, Prediction Accuracy = 52.496%, Loss = 0.8914876222610474
Epoch: 183, Batch Gradient Norm: 5.584855482715006
Epoch: 183, Batch Gradient Norm after: 5.584855482715006
Epoch 184/10000, Prediction Accuracy = 52.576%, Loss = 0.8919444561004639
Epoch: 184, Batch Gradient Norm: 4.402352297812142
Epoch: 184, Batch Gradient Norm after: 4.402352297812142
Epoch 185/10000, Prediction Accuracy = 52.65%, Loss = 0.8766290426254273
Epoch: 185, Batch Gradient Norm: 4.408131890339671
Epoch: 185, Batch Gradient Norm after: 4.408131890339671
Epoch 186/10000, Prediction Accuracy = 52.736000000000004%, Loss = 0.8721367955207825
Epoch: 186, Batch Gradient Norm: 4.821325841471137
Epoch: 186, Batch Gradient Norm after: 4.821325841471137
Epoch 187/10000, Prediction Accuracy = 52.784000000000006%, Loss = 0.8723796248435974
Epoch: 187, Batch Gradient Norm: 5.174952610487344
Epoch: 187, Batch Gradient Norm after: 5.174952610487344
Epoch 188/10000, Prediction Accuracy = 52.83%, Loss = 0.8637465715408326
Epoch: 188, Batch Gradient Norm: 6.799919253566484
Epoch: 188, Batch Gradient Norm after: 6.799919253566484
Epoch 189/10000, Prediction Accuracy = 52.888%, Loss = 0.868473219871521
Epoch: 189, Batch Gradient Norm: 6.02182971766292
Epoch: 189, Batch Gradient Norm after: 6.02182971766292
Epoch 190/10000, Prediction Accuracy = 52.998000000000005%, Loss = 0.8637304425239563
Epoch: 190, Batch Gradient Norm: 4.57350040440055
Epoch: 190, Batch Gradient Norm after: 4.57350040440055
Epoch 191/10000, Prediction Accuracy = 53.072%, Loss = 0.8558074712753296
Epoch: 191, Batch Gradient Norm: 5.285742050916689
Epoch: 191, Batch Gradient Norm after: 5.285742050916689
Epoch 192/10000, Prediction Accuracy = 53.128%, Loss = 0.8571768283843995
Epoch: 192, Batch Gradient Norm: 5.494767231580213
Epoch: 192, Batch Gradient Norm after: 5.494767231580213
Epoch 193/10000, Prediction Accuracy = 53.022000000000006%, Loss = 0.8531936407089233
Epoch: 193, Batch Gradient Norm: 6.311484857138541
Epoch: 193, Batch Gradient Norm after: 6.311484857138541
Epoch 194/10000, Prediction Accuracy = 53.279999999999994%, Loss = 0.8530983805656434
Epoch: 194, Batch Gradient Norm: 6.655706558726862
Epoch: 194, Batch Gradient Norm after: 6.655706558726862
Epoch 195/10000, Prediction Accuracy = 53.15%, Loss = 0.8571517586708068
Epoch: 195, Batch Gradient Norm: 5.592618287295451
Epoch: 195, Batch Gradient Norm after: 5.592618287295451
Epoch 196/10000, Prediction Accuracy = 53.378%, Loss = 0.8466658234596253
Epoch: 196, Batch Gradient Norm: 6.078606148092133
Epoch: 196, Batch Gradient Norm after: 6.078606148092133
Epoch 197/10000, Prediction Accuracy = 53.36%, Loss = 0.8489627599716186
Epoch: 197, Batch Gradient Norm: 4.456689082658802
Epoch: 197, Batch Gradient Norm after: 4.456689082658802
Epoch 198/10000, Prediction Accuracy = 53.426%, Loss = 0.836432957649231
Epoch: 198, Batch Gradient Norm: 5.090342773661521
Epoch: 198, Batch Gradient Norm after: 5.090342773661521
Epoch 199/10000, Prediction Accuracy = 53.438%, Loss = 0.8433858036994935
Epoch: 199, Batch Gradient Norm: 4.655484492266137
Epoch: 199, Batch Gradient Norm after: 4.655484492266137
Epoch 200/10000, Prediction Accuracy = 53.65599999999999%, Loss = 0.8353649139404297
Epoch: 200, Batch Gradient Norm: 5.26551284003411
Epoch: 200, Batch Gradient Norm after: 5.26551284003411
Epoch 201/10000, Prediction Accuracy = 53.54600000000001%, Loss = 0.8349958300590515
Epoch: 201, Batch Gradient Norm: 4.689231648002487
Epoch: 201, Batch Gradient Norm after: 4.689231648002487
Epoch 202/10000, Prediction Accuracy = 53.468%, Loss = 0.82730473279953
Epoch: 202, Batch Gradient Norm: 5.083506586107754
Epoch: 202, Batch Gradient Norm after: 5.083506586107754
Epoch 203/10000, Prediction Accuracy = 53.73600000000001%, Loss = 0.8265824556350708
Epoch: 203, Batch Gradient Norm: 5.535100863097947
Epoch: 203, Batch Gradient Norm after: 5.535100863097947
Epoch 204/10000, Prediction Accuracy = 53.782%, Loss = 0.8270226359367371
Epoch: 204, Batch Gradient Norm: 5.133721377102159
Epoch: 204, Batch Gradient Norm after: 5.133721377102159
Epoch 205/10000, Prediction Accuracy = 53.70799999999999%, Loss = 0.8228569269180298
Epoch: 205, Batch Gradient Norm: 5.846677823274857
Epoch: 205, Batch Gradient Norm after: 5.846677823274857
Epoch 206/10000, Prediction Accuracy = 53.910000000000004%, Loss = 0.8234625101089478
Epoch: 206, Batch Gradient Norm: 6.89029744540678
Epoch: 206, Batch Gradient Norm after: 6.89029744540678
Epoch 207/10000, Prediction Accuracy = 53.948%, Loss = 0.8268824458122254
Epoch: 207, Batch Gradient Norm: 6.507298357046784
Epoch: 207, Batch Gradient Norm after: 6.507298357046784
Epoch 208/10000, Prediction Accuracy = 53.922000000000004%, Loss = 0.8186776399612427
Epoch: 208, Batch Gradient Norm: 6.295507168095833
Epoch: 208, Batch Gradient Norm after: 6.295507168095833
Epoch 209/10000, Prediction Accuracy = 53.986000000000004%, Loss = 0.8189302444458008
Epoch: 209, Batch Gradient Norm: 6.448580495775991
Epoch: 209, Batch Gradient Norm after: 6.448580495775991
Epoch 210/10000, Prediction Accuracy = 54.126%, Loss = 0.8177022814750672
Epoch: 210, Batch Gradient Norm: 5.974692363837738
Epoch: 210, Batch Gradient Norm after: 5.974692363837738
Epoch 211/10000, Prediction Accuracy = 54.06999999999999%, Loss = 0.8115348935127258
Epoch: 211, Batch Gradient Norm: 5.668323645368783
Epoch: 211, Batch Gradient Norm after: 5.668323645368783
Epoch 212/10000, Prediction Accuracy = 54.116%, Loss = 0.8093309760093689
Epoch: 212, Batch Gradient Norm: 5.295544651982839
Epoch: 212, Batch Gradient Norm after: 5.295544651982839
Epoch 213/10000, Prediction Accuracy = 54.196000000000005%, Loss = 0.8056137323379516
Epoch: 213, Batch Gradient Norm: 6.297304413820796
Epoch: 213, Batch Gradient Norm after: 6.297304413820796
Epoch 214/10000, Prediction Accuracy = 54.322%, Loss = 0.8136690616607666
Epoch: 214, Batch Gradient Norm: 5.817098127431323
Epoch: 214, Batch Gradient Norm after: 5.817098127431323
Epoch 215/10000, Prediction Accuracy = 54.234%, Loss = 0.8052364468574524
Epoch: 215, Batch Gradient Norm: 5.782014189642502
Epoch: 215, Batch Gradient Norm after: 5.782014189642502
Epoch 216/10000, Prediction Accuracy = 54.352%, Loss = 0.8030902743339539
Epoch: 216, Batch Gradient Norm: 5.553018823727174
Epoch: 216, Batch Gradient Norm after: 5.553018823727174
Epoch 217/10000, Prediction Accuracy = 54.45799999999999%, Loss = 0.8008422493934632
Epoch: 217, Batch Gradient Norm: 4.611048978898657
Epoch: 217, Batch Gradient Norm after: 4.611048978898657
Epoch 218/10000, Prediction Accuracy = 54.43000000000001%, Loss = 0.7970276951789856
Epoch: 218, Batch Gradient Norm: 4.865848723324917
Epoch: 218, Batch Gradient Norm after: 4.865848723324917
Epoch 219/10000, Prediction Accuracy = 54.432%, Loss = 0.7940559983253479
Epoch: 219, Batch Gradient Norm: 5.026746021666435
Epoch: 219, Batch Gradient Norm after: 5.026746021666435
Epoch 220/10000, Prediction Accuracy = 54.513999999999996%, Loss = 0.7954100251197815
Epoch: 220, Batch Gradient Norm: 4.563281761543993
Epoch: 220, Batch Gradient Norm after: 4.563281761543993
Epoch 221/10000, Prediction Accuracy = 54.565999999999995%, Loss = 0.7901903986930847
Epoch: 221, Batch Gradient Norm: 4.642552376027837
Epoch: 221, Batch Gradient Norm after: 4.642552376027837
Epoch 222/10000, Prediction Accuracy = 54.584%, Loss = 0.7896074771881103
Epoch: 222, Batch Gradient Norm: 4.940481826035617
Epoch: 222, Batch Gradient Norm after: 4.940481826035617
Epoch 223/10000, Prediction Accuracy = 54.60600000000001%, Loss = 0.7893356442451477
Epoch: 223, Batch Gradient Norm: 5.253438647663907
Epoch: 223, Batch Gradient Norm after: 5.253438647663907
Epoch 224/10000, Prediction Accuracy = 54.686%, Loss = 0.788815975189209
Epoch: 224, Batch Gradient Norm: 6.145072049758967
Epoch: 224, Batch Gradient Norm after: 6.145072049758967
Epoch 225/10000, Prediction Accuracy = 54.757999999999996%, Loss = 0.7891406893730164
Epoch: 225, Batch Gradient Norm: 6.667193893810059
Epoch: 225, Batch Gradient Norm after: 6.667193893810059
Epoch 226/10000, Prediction Accuracy = 54.706%, Loss = 0.7960378170013428
Epoch: 226, Batch Gradient Norm: 6.226475256071188
Epoch: 226, Batch Gradient Norm after: 6.226475256071188
Epoch 227/10000, Prediction Accuracy = 54.79%, Loss = 0.7883063554763794
Epoch: 227, Batch Gradient Norm: 5.494040312764884
Epoch: 227, Batch Gradient Norm after: 5.494040312764884
Epoch 228/10000, Prediction Accuracy = 54.952%, Loss = 0.7863644957542419
Epoch: 228, Batch Gradient Norm: 5.767815656989113
Epoch: 228, Batch Gradient Norm after: 5.767815656989113
Epoch 229/10000, Prediction Accuracy = 54.827999999999996%, Loss = 0.787678337097168
Epoch: 229, Batch Gradient Norm: 5.151700548652239
Epoch: 229, Batch Gradient Norm after: 5.151700548652239
Epoch 230/10000, Prediction Accuracy = 54.88599999999999%, Loss = 0.7807074546813965
Epoch: 230, Batch Gradient Norm: 5.068292898292837
Epoch: 230, Batch Gradient Norm after: 5.068292898292837
Epoch 231/10000, Prediction Accuracy = 54.934000000000005%, Loss = 0.779349434375763
Epoch: 231, Batch Gradient Norm: 5.789479476270648
Epoch: 231, Batch Gradient Norm after: 5.789479476270648
Epoch 232/10000, Prediction Accuracy = 55.022000000000006%, Loss = 0.779858124256134
Epoch: 232, Batch Gradient Norm: 4.820258094489494
Epoch: 232, Batch Gradient Norm after: 4.820258094489494
Epoch 233/10000, Prediction Accuracy = 55.004000000000005%, Loss = 0.773633337020874
Epoch: 233, Batch Gradient Norm: 4.635041887719374
Epoch: 233, Batch Gradient Norm after: 4.635041887719374
Epoch 234/10000, Prediction Accuracy = 54.986000000000004%, Loss = 0.769864022731781
Epoch: 234, Batch Gradient Norm: 5.793208205345868
Epoch: 234, Batch Gradient Norm after: 5.793208205345868
Epoch 235/10000, Prediction Accuracy = 55.06999999999999%, Loss = 0.776377546787262
Epoch: 235, Batch Gradient Norm: 6.330633559467745
Epoch: 235, Batch Gradient Norm after: 6.330633559467745
Epoch 236/10000, Prediction Accuracy = 55.18000000000001%, Loss = 0.777290427684784
Epoch: 236, Batch Gradient Norm: 6.722789523108653
Epoch: 236, Batch Gradient Norm after: 6.722789523108653
Epoch 237/10000, Prediction Accuracy = 55.15599999999999%, Loss = 0.7744732856750488
Epoch: 237, Batch Gradient Norm: 5.489474619489887
Epoch: 237, Batch Gradient Norm after: 5.489474619489887
Epoch 238/10000, Prediction Accuracy = 55.16600000000001%, Loss = 0.7683319211006164
Epoch: 238, Batch Gradient Norm: 5.443013647216464
Epoch: 238, Batch Gradient Norm after: 5.443013647216464
Epoch 239/10000, Prediction Accuracy = 55.25599999999999%, Loss = 0.7659951686859131
Epoch: 239, Batch Gradient Norm: 6.429444664827373
Epoch: 239, Batch Gradient Norm after: 6.429444664827373
Epoch 240/10000, Prediction Accuracy = 55.436%, Loss = 0.7674229621887207
Epoch: 240, Batch Gradient Norm: 6.721423889751669
Epoch: 240, Batch Gradient Norm after: 6.721423889751669
Epoch 241/10000, Prediction Accuracy = 55.28000000000001%, Loss = 0.7669561266899109
Epoch: 241, Batch Gradient Norm: 7.237796779640272
Epoch: 241, Batch Gradient Norm after: 7.237796779640272
Epoch 242/10000, Prediction Accuracy = 55.376%, Loss = 0.7704747557640076
Epoch: 242, Batch Gradient Norm: 5.701855607451057
Epoch: 242, Batch Gradient Norm after: 5.701855607451057
Epoch 243/10000, Prediction Accuracy = 55.504%, Loss = 0.7630047082901001
Epoch: 243, Batch Gradient Norm: 5.917019598747018
Epoch: 243, Batch Gradient Norm after: 5.917019598747018
Epoch 244/10000, Prediction Accuracy = 55.44199999999999%, Loss = 0.7639554381370545
Epoch: 244, Batch Gradient Norm: 7.141047989421135
Epoch: 244, Batch Gradient Norm after: 7.141047989421135
Epoch 245/10000, Prediction Accuracy = 55.326%, Loss = 0.7682379484176636
Epoch: 245, Batch Gradient Norm: 5.7620510479910525
Epoch: 245, Batch Gradient Norm after: 5.7620510479910525
Epoch 246/10000, Prediction Accuracy = 55.50599999999999%, Loss = 0.75989830493927
Epoch: 246, Batch Gradient Norm: 5.762592432581333
Epoch: 246, Batch Gradient Norm after: 5.762592432581333
Epoch 247/10000, Prediction Accuracy = 55.616%, Loss = 0.7570935726165772
Epoch: 247, Batch Gradient Norm: 6.2769522154253234
Epoch: 247, Batch Gradient Norm after: 6.2769522154253234
Epoch 248/10000, Prediction Accuracy = 55.672000000000004%, Loss = 0.7601248502731324
Epoch: 248, Batch Gradient Norm: 7.417498861519843
Epoch: 248, Batch Gradient Norm after: 7.417498861519843
Epoch 249/10000, Prediction Accuracy = 55.55800000000001%, Loss = 0.7688250780105591
Epoch: 249, Batch Gradient Norm: 5.819524762611765
Epoch: 249, Batch Gradient Norm after: 5.819524762611765
Epoch 250/10000, Prediction Accuracy = 55.648%, Loss = 0.7534267187118531
Epoch: 250, Batch Gradient Norm: 6.3673507209233575
Epoch: 250, Batch Gradient Norm after: 6.3673507209233575
Epoch 251/10000, Prediction Accuracy = 55.694%, Loss = 0.7556546092033386
Epoch: 251, Batch Gradient Norm: 6.242110523147256
Epoch: 251, Batch Gradient Norm after: 6.242110523147256
Epoch 252/10000, Prediction Accuracy = 55.64%, Loss = 0.7559484004974365
Epoch: 252, Batch Gradient Norm: 7.827116783711335
Epoch: 252, Batch Gradient Norm after: 7.827116783711335
Epoch 253/10000, Prediction Accuracy = 55.85000000000001%, Loss = 0.7679075598716736
Epoch: 253, Batch Gradient Norm: 6.138049611200104
Epoch: 253, Batch Gradient Norm after: 6.138049611200104
Epoch 254/10000, Prediction Accuracy = 55.754%, Loss = 0.7492886066436768
Epoch: 254, Batch Gradient Norm: 7.6821678363753385
Epoch: 254, Batch Gradient Norm after: 7.6821678363753385
Epoch 255/10000, Prediction Accuracy = 55.686%, Loss = 0.760348665714264
Epoch: 255, Batch Gradient Norm: 7.220535884530194
Epoch: 255, Batch Gradient Norm after: 7.220535884530194
Epoch 256/10000, Prediction Accuracy = 55.884%, Loss = 0.7522019267082214
Epoch: 256, Batch Gradient Norm: 7.081822392208141
Epoch: 256, Batch Gradient Norm after: 7.081822392208141
Epoch 257/10000, Prediction Accuracy = 55.864%, Loss = 0.7503672003746032
Epoch: 257, Batch Gradient Norm: 5.717031213822266
Epoch: 257, Batch Gradient Norm after: 5.717031213822266
Epoch 258/10000, Prediction Accuracy = 55.902%, Loss = 0.7431306719779969
Epoch: 258, Batch Gradient Norm: 5.5124469948094
Epoch: 258, Batch Gradient Norm after: 5.5124469948094
Epoch 259/10000, Prediction Accuracy = 55.882000000000005%, Loss = 0.7454155206680297
Epoch: 259, Batch Gradient Norm: 6.1842742595180535
Epoch: 259, Batch Gradient Norm after: 6.1842742595180535
Epoch 260/10000, Prediction Accuracy = 55.95399999999999%, Loss = 0.7447951436042786
Epoch: 260, Batch Gradient Norm: 5.805083279228666
Epoch: 260, Batch Gradient Norm after: 5.805083279228666
Epoch 261/10000, Prediction Accuracy = 55.876%, Loss = 0.7412307143211365
Epoch: 261, Batch Gradient Norm: 6.392862087727161
Epoch: 261, Batch Gradient Norm after: 6.392862087727161
Epoch 262/10000, Prediction Accuracy = 55.956%, Loss = 0.7469314932823181
Epoch: 262, Batch Gradient Norm: 6.663982049098913
Epoch: 262, Batch Gradient Norm after: 6.663982049098913
Epoch 263/10000, Prediction Accuracy = 56.053999999999995%, Loss = 0.7409510493278504
Epoch: 263, Batch Gradient Norm: 7.246180978274549
Epoch: 263, Batch Gradient Norm after: 7.246180978274549
Epoch 264/10000, Prediction Accuracy = 56.084%, Loss = 0.7439114093780518
Epoch: 264, Batch Gradient Norm: 8.025737940135103
Epoch: 264, Batch Gradient Norm after: 8.025737940135103
Epoch 265/10000, Prediction Accuracy = 56.076%, Loss = 0.7431045413017273
Epoch: 265, Batch Gradient Norm: 6.8790440331783245
Epoch: 265, Batch Gradient Norm after: 6.8790440331783245
Epoch 266/10000, Prediction Accuracy = 56.056%, Loss = 0.7453675389289856
Epoch: 266, Batch Gradient Norm: 5.769159523915119
Epoch: 266, Batch Gradient Norm after: 5.769159523915119
Epoch 267/10000, Prediction Accuracy = 56.156000000000006%, Loss = 0.7347317576408386
Epoch: 267, Batch Gradient Norm: 5.511035532542971
Epoch: 267, Batch Gradient Norm after: 5.511035532542971
Epoch 268/10000, Prediction Accuracy = 56.096000000000004%, Loss = 0.7340973258018494
Epoch: 268, Batch Gradient Norm: 6.480508711965875
Epoch: 268, Batch Gradient Norm after: 6.480508711965875
Epoch 269/10000, Prediction Accuracy = 56.23599999999999%, Loss = 0.7375198364257812
Epoch: 269, Batch Gradient Norm: 6.482227728143439
Epoch: 269, Batch Gradient Norm after: 6.482227728143439
Epoch 270/10000, Prediction Accuracy = 56.206%, Loss = 0.7358413338661194
Epoch: 270, Batch Gradient Norm: 6.404775302051184
Epoch: 270, Batch Gradient Norm after: 6.404775302051184
Epoch 271/10000, Prediction Accuracy = 56.27%, Loss = 0.7313016772270202
Epoch: 271, Batch Gradient Norm: 8.264188584810576
Epoch: 271, Batch Gradient Norm after: 8.264188584810576
Epoch 272/10000, Prediction Accuracy = 56.275999999999996%, Loss = 0.7393478393554688
Epoch: 272, Batch Gradient Norm: 7.6859381597050636
Epoch: 272, Batch Gradient Norm after: 7.6859381597050636
Epoch 273/10000, Prediction Accuracy = 56.452%, Loss = 0.7290388226509095
Epoch: 273, Batch Gradient Norm: 7.412156716605258
Epoch: 273, Batch Gradient Norm after: 7.412156716605258
Epoch 274/10000, Prediction Accuracy = 56.286%, Loss = 0.7321789979934692
Epoch: 274, Batch Gradient Norm: 6.415971545055207
Epoch: 274, Batch Gradient Norm after: 6.415971545055207
Epoch 275/10000, Prediction Accuracy = 56.431999999999995%, Loss = 0.723774516582489
Epoch: 275, Batch Gradient Norm: 7.116003466285445
Epoch: 275, Batch Gradient Norm after: 7.116003466285445
Epoch 276/10000, Prediction Accuracy = 56.40599999999999%, Loss = 0.73044855594635
Epoch: 276, Batch Gradient Norm: 5.99045310499889
Epoch: 276, Batch Gradient Norm after: 5.99045310499889
Epoch 277/10000, Prediction Accuracy = 56.458000000000006%, Loss = 0.7246953010559082
Epoch: 277, Batch Gradient Norm: 6.165261264715629
Epoch: 277, Batch Gradient Norm after: 6.165261264715629
Epoch 278/10000, Prediction Accuracy = 56.46999999999999%, Loss = 0.72782461643219
Epoch: 278, Batch Gradient Norm: 5.578091871625953
Epoch: 278, Batch Gradient Norm after: 5.578091871625953
Epoch 279/10000, Prediction Accuracy = 56.45399999999999%, Loss = 0.7232592105865479
Epoch: 279, Batch Gradient Norm: 5.785201657965362
Epoch: 279, Batch Gradient Norm after: 5.785201657965362
Epoch 280/10000, Prediction Accuracy = 56.487999999999985%, Loss = 0.7225621581077576
Epoch: 280, Batch Gradient Norm: 6.51408904919019
Epoch: 280, Batch Gradient Norm after: 6.51408904919019
Epoch 281/10000, Prediction Accuracy = 56.448%, Loss = 0.7258752107620239
Epoch: 281, Batch Gradient Norm: 5.863565097462562
Epoch: 281, Batch Gradient Norm after: 5.863565097462562
Epoch 282/10000, Prediction Accuracy = 56.49399999999999%, Loss = 0.7222432017326355
Epoch: 282, Batch Gradient Norm: 5.35941036962737
Epoch: 282, Batch Gradient Norm after: 5.35941036962737
Epoch 283/10000, Prediction Accuracy = 56.507999999999996%, Loss = 0.718642783164978
Epoch: 283, Batch Gradient Norm: 6.032898965741495
Epoch: 283, Batch Gradient Norm after: 6.032898965741495
Epoch 284/10000, Prediction Accuracy = 56.612%, Loss = 0.7191149115562439
Epoch: 284, Batch Gradient Norm: 5.789742583292608
Epoch: 284, Batch Gradient Norm after: 5.789742583292608
Epoch 285/10000, Prediction Accuracy = 56.577999999999996%, Loss = 0.7185165882110596
Epoch: 285, Batch Gradient Norm: 6.72288405400363
Epoch: 285, Batch Gradient Norm after: 6.72288405400363
Epoch 286/10000, Prediction Accuracy = 56.61%, Loss = 0.7243248820304871
Epoch: 286, Batch Gradient Norm: 6.308655300378637
Epoch: 286, Batch Gradient Norm after: 6.308655300378637
Epoch 287/10000, Prediction Accuracy = 56.739999999999995%, Loss = 0.7189765930175781
Epoch: 287, Batch Gradient Norm: 7.291859992282762
Epoch: 287, Batch Gradient Norm after: 7.291859992282762
Epoch 288/10000, Prediction Accuracy = 56.65599999999999%, Loss = 0.7216827273368835
Epoch: 288, Batch Gradient Norm: 6.604360146557609
Epoch: 288, Batch Gradient Norm after: 6.604360146557609
Epoch 289/10000, Prediction Accuracy = 56.802%, Loss = 0.7156807065010071
Epoch: 289, Batch Gradient Norm: 6.799583165572664
Epoch: 289, Batch Gradient Norm after: 6.799583165572664
Epoch 290/10000, Prediction Accuracy = 56.682%, Loss = 0.7159545421600342
Epoch: 290, Batch Gradient Norm: 6.799870760709534
Epoch: 290, Batch Gradient Norm after: 6.799870760709534
Epoch 291/10000, Prediction Accuracy = 56.660000000000004%, Loss = 0.7147417545318604
Epoch: 291, Batch Gradient Norm: 7.710212136558971
Epoch: 291, Batch Gradient Norm after: 7.710212136558971
Epoch 292/10000, Prediction Accuracy = 56.616%, Loss = 0.7204952120780945
Epoch: 292, Batch Gradient Norm: 8.783992120506387
Epoch: 292, Batch Gradient Norm after: 8.783992120506387
Epoch 293/10000, Prediction Accuracy = 56.826%, Loss = 0.7293083429336548
Epoch: 293, Batch Gradient Norm: 7.358605516046925
Epoch: 293, Batch Gradient Norm after: 7.358605516046925
Epoch 294/10000, Prediction Accuracy = 56.669999999999995%, Loss = 0.7211652517318725
Epoch: 294, Batch Gradient Norm: 5.8236760558440785
Epoch: 294, Batch Gradient Norm after: 5.8236760558440785
Epoch 295/10000, Prediction Accuracy = 56.870000000000005%, Loss = 0.7113637089729309
Epoch: 295, Batch Gradient Norm: 5.920217597706066
Epoch: 295, Batch Gradient Norm after: 5.920217597706066
Epoch 296/10000, Prediction Accuracy = 56.98599999999999%, Loss = 0.7103800058364869
Epoch: 296, Batch Gradient Norm: 6.233515839494826
Epoch: 296, Batch Gradient Norm after: 6.233515839494826
Epoch 297/10000, Prediction Accuracy = 56.834%, Loss = 0.708230996131897
Epoch: 297, Batch Gradient Norm: 5.925804165979531
Epoch: 297, Batch Gradient Norm after: 5.925804165979531
Epoch 298/10000, Prediction Accuracy = 56.806%, Loss = 0.7080464363098145
Epoch: 298, Batch Gradient Norm: 5.756111251148124
Epoch: 298, Batch Gradient Norm after: 5.756111251148124
Epoch 299/10000, Prediction Accuracy = 56.838%, Loss = 0.709632670879364
Epoch: 299, Batch Gradient Norm: 6.806626216754409
Epoch: 299, Batch Gradient Norm after: 6.806626216754409
Epoch 300/10000, Prediction Accuracy = 56.99000000000001%, Loss = 0.7100453495979309
Epoch: 300, Batch Gradient Norm: 7.087786181878443
Epoch: 300, Batch Gradient Norm after: 7.087786181878443
Epoch 301/10000, Prediction Accuracy = 56.870000000000005%, Loss = 0.7092077136039734
Epoch: 301, Batch Gradient Norm: 6.431343639292225
Epoch: 301, Batch Gradient Norm after: 6.431343639292225
Epoch 302/10000, Prediction Accuracy = 57.06%, Loss = 0.7047773361206054
Epoch: 302, Batch Gradient Norm: 6.493506204926776
Epoch: 302, Batch Gradient Norm after: 6.493506204926776
Epoch 303/10000, Prediction Accuracy = 56.98%, Loss = 0.7096704363822937
Epoch: 303, Batch Gradient Norm: 6.894353889908002
Epoch: 303, Batch Gradient Norm after: 6.894353889908002
Epoch 304/10000, Prediction Accuracy = 56.952%, Loss = 0.7052902817726135
Epoch: 304, Batch Gradient Norm: 8.221399270794013
Epoch: 304, Batch Gradient Norm after: 8.221399270794013
Epoch 305/10000, Prediction Accuracy = 56.846000000000004%, Loss = 0.7140492677688599
Epoch: 305, Batch Gradient Norm: 7.843034692314779
Epoch: 305, Batch Gradient Norm after: 7.843034692314779
Epoch 306/10000, Prediction Accuracy = 56.98199999999999%, Loss = 0.7078247308731079
Epoch: 306, Batch Gradient Norm: 7.182373620681737
Epoch: 306, Batch Gradient Norm after: 7.182373620681737
Epoch 307/10000, Prediction Accuracy = 57.15599999999999%, Loss = 0.7052594780921936
Epoch: 307, Batch Gradient Norm: 6.577001378243172
Epoch: 307, Batch Gradient Norm after: 6.577001378243172
Epoch 308/10000, Prediction Accuracy = 57.05%, Loss = 0.7024982333183288
Epoch: 308, Batch Gradient Norm: 6.669803351579339
Epoch: 308, Batch Gradient Norm after: 6.669803351579339
Epoch 309/10000, Prediction Accuracy = 57.052%, Loss = 0.700725793838501
Epoch: 309, Batch Gradient Norm: 6.603806455923965
Epoch: 309, Batch Gradient Norm after: 6.603806455923965
Epoch 310/10000, Prediction Accuracy = 57.160000000000004%, Loss = 0.6990628600120544
Epoch: 310, Batch Gradient Norm: 7.234198818277902
Epoch: 310, Batch Gradient Norm after: 7.234198818277902
Epoch 311/10000, Prediction Accuracy = 57.124%, Loss = 0.7019870758056641
Epoch: 311, Batch Gradient Norm: 6.342260336034564
Epoch: 311, Batch Gradient Norm after: 6.342260336034564
Epoch 312/10000, Prediction Accuracy = 57.122%, Loss = 0.6978852391242981
Epoch: 312, Batch Gradient Norm: 7.060732535796367
Epoch: 312, Batch Gradient Norm after: 7.060732535796367
Epoch 313/10000, Prediction Accuracy = 57.25599999999999%, Loss = 0.7011162400245666
Epoch: 313, Batch Gradient Norm: 5.676941311996534
Epoch: 313, Batch Gradient Norm after: 5.676941311996534
Epoch 314/10000, Prediction Accuracy = 57.220000000000006%, Loss = 0.6942817687988281
Epoch: 314, Batch Gradient Norm: 7.289032177316272
Epoch: 314, Batch Gradient Norm after: 7.289032177316272
Epoch 315/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.7017719984054566
Epoch: 315, Batch Gradient Norm: 7.641615751169038
Epoch: 315, Batch Gradient Norm after: 7.641615751169038
Epoch 316/10000, Prediction Accuracy = 57.416%, Loss = 0.7005788207054138
Epoch: 316, Batch Gradient Norm: 6.929297119147143
Epoch: 316, Batch Gradient Norm after: 6.929297119147143
Epoch 317/10000, Prediction Accuracy = 57.105999999999995%, Loss = 0.6970027685165405
Epoch: 317, Batch Gradient Norm: 6.819958776912339
Epoch: 317, Batch Gradient Norm after: 6.819958776912339
Epoch 318/10000, Prediction Accuracy = 57.275999999999996%, Loss = 0.6957133054733277
Epoch: 318, Batch Gradient Norm: 7.343943549137472
Epoch: 318, Batch Gradient Norm after: 7.343943549137472
Epoch 319/10000, Prediction Accuracy = 57.205999999999996%, Loss = 0.6977280378341675
Epoch: 319, Batch Gradient Norm: 8.042767611063935
Epoch: 319, Batch Gradient Norm after: 8.042767611063935
Epoch 320/10000, Prediction Accuracy = 57.364%, Loss = 0.6966705322265625
Epoch: 320, Batch Gradient Norm: 6.268114423274777
Epoch: 320, Batch Gradient Norm after: 6.268114423274777
Epoch 321/10000, Prediction Accuracy = 57.303999999999995%, Loss = 0.689851713180542
Epoch: 321, Batch Gradient Norm: 6.633018855948697
Epoch: 321, Batch Gradient Norm after: 6.633018855948697
Epoch 322/10000, Prediction Accuracy = 57.386%, Loss = 0.6915320873260498
Epoch: 322, Batch Gradient Norm: 6.187547601848922
Epoch: 322, Batch Gradient Norm after: 6.187547601848922
Epoch 323/10000, Prediction Accuracy = 57.278%, Loss = 0.6901949882507324
Epoch: 323, Batch Gradient Norm: 7.838473109388232
Epoch: 323, Batch Gradient Norm after: 7.838473109388232
Epoch 324/10000, Prediction Accuracy = 57.370000000000005%, Loss = 0.6955284714698792
Epoch: 324, Batch Gradient Norm: 7.296136006333656
Epoch: 324, Batch Gradient Norm after: 7.296136006333656
Epoch 325/10000, Prediction Accuracy = 57.366%, Loss = 0.692947506904602
Epoch: 325, Batch Gradient Norm: 6.343872022569512
Epoch: 325, Batch Gradient Norm after: 6.343872022569512
Epoch 326/10000, Prediction Accuracy = 57.376%, Loss = 0.6870150566101074
Epoch: 326, Batch Gradient Norm: 7.532572715541847
Epoch: 326, Batch Gradient Norm after: 7.532572715541847
Epoch 327/10000, Prediction Accuracy = 57.496%, Loss = 0.6902158737182618
Epoch: 327, Batch Gradient Norm: 6.5537024616969655
Epoch: 327, Batch Gradient Norm after: 6.5537024616969655
Epoch 328/10000, Prediction Accuracy = 57.352%, Loss = 0.6876769900321961
Epoch: 328, Batch Gradient Norm: 7.804193019580544
Epoch: 328, Batch Gradient Norm after: 7.804193019580544
Epoch 329/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.6930952429771423
Epoch: 329, Batch Gradient Norm: 8.064701553381045
Epoch: 329, Batch Gradient Norm after: 8.064701553381045
Epoch 330/10000, Prediction Accuracy = 57.354%, Loss = 0.6909087300300598
Epoch: 330, Batch Gradient Norm: 6.340416900695407
Epoch: 330, Batch Gradient Norm after: 6.340416900695407
Epoch 331/10000, Prediction Accuracy = 57.51800000000001%, Loss = 0.6840665578842163
Epoch: 331, Batch Gradient Norm: 6.577503076292451
Epoch: 331, Batch Gradient Norm after: 6.577503076292451
Epoch 332/10000, Prediction Accuracy = 57.484%, Loss = 0.6844051122665405
Epoch: 332, Batch Gradient Norm: 7.155089858873939
Epoch: 332, Batch Gradient Norm after: 7.155089858873939
Epoch 333/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.6868488550186157
Epoch: 333, Batch Gradient Norm: 7.313398136868463
Epoch: 333, Batch Gradient Norm after: 7.313398136868463
Epoch 334/10000, Prediction Accuracy = 57.604%, Loss = 0.6846969366073609
Epoch: 334, Batch Gradient Norm: 5.80084158594417
Epoch: 334, Batch Gradient Norm after: 5.80084158594417
Epoch 335/10000, Prediction Accuracy = 57.50599999999999%, Loss = 0.6803260684013367
Epoch: 335, Batch Gradient Norm: 7.326776320537281
Epoch: 335, Batch Gradient Norm after: 7.326776320537281
Epoch 336/10000, Prediction Accuracy = 57.52%, Loss = 0.6877471089363099
Epoch: 336, Batch Gradient Norm: 6.971772954797184
Epoch: 336, Batch Gradient Norm after: 6.971772954797184
Epoch 337/10000, Prediction Accuracy = 57.477999999999994%, Loss = 0.684634268283844
Epoch: 337, Batch Gradient Norm: 7.248776346504407
Epoch: 337, Batch Gradient Norm after: 7.248776346504407
Epoch 338/10000, Prediction Accuracy = 57.498000000000005%, Loss = 0.6833134055137634
Epoch: 338, Batch Gradient Norm: 5.523536757229153
Epoch: 338, Batch Gradient Norm after: 5.523536757229153
Epoch 339/10000, Prediction Accuracy = 57.538%, Loss = 0.6766966700553894
Epoch: 339, Batch Gradient Norm: 5.997246769758064
Epoch: 339, Batch Gradient Norm after: 5.997246769758064
Epoch 340/10000, Prediction Accuracy = 57.517999999999994%, Loss = 0.6781838059425354
Epoch: 340, Batch Gradient Norm: 6.713846608697572
Epoch: 340, Batch Gradient Norm after: 6.713846608697572
Epoch 341/10000, Prediction Accuracy = 57.522000000000006%, Loss = 0.6807489156723022
Epoch: 341, Batch Gradient Norm: 5.171019888751653
Epoch: 341, Batch Gradient Norm after: 5.171019888751653
Epoch 342/10000, Prediction Accuracy = 57.588%, Loss = 0.6765417218208313
Epoch: 342, Batch Gradient Norm: 6.2873862021971725
Epoch: 342, Batch Gradient Norm after: 6.2873862021971725
Epoch 343/10000, Prediction Accuracy = 57.498000000000005%, Loss = 0.6778603076934815
Epoch: 343, Batch Gradient Norm: 8.159391794152677
Epoch: 343, Batch Gradient Norm after: 8.159391794152677
Epoch 344/10000, Prediction Accuracy = 57.58200000000001%, Loss = 0.6843035697937012
Epoch: 344, Batch Gradient Norm: 7.345720994459385
Epoch: 344, Batch Gradient Norm after: 7.345720994459385
Epoch 345/10000, Prediction Accuracy = 57.589999999999996%, Loss = 0.6806514501571655
Epoch: 345, Batch Gradient Norm: 8.09952479615253
Epoch: 345, Batch Gradient Norm after: 8.09952479615253
Epoch 346/10000, Prediction Accuracy = 57.688%, Loss = 0.6817081212997437
Epoch: 346, Batch Gradient Norm: 7.499057704943354
Epoch: 346, Batch Gradient Norm after: 7.499057704943354
Epoch 347/10000, Prediction Accuracy = 57.574%, Loss = 0.6780133605003357
Epoch: 347, Batch Gradient Norm: 6.418089270745111
Epoch: 347, Batch Gradient Norm after: 6.418089270745111
Epoch 348/10000, Prediction Accuracy = 57.68399999999999%, Loss = 0.6755592346191406
Epoch: 348, Batch Gradient Norm: 7.11499061682504
Epoch: 348, Batch Gradient Norm after: 7.11499061682504
Epoch 349/10000, Prediction Accuracy = 57.628%, Loss = 0.67588130235672
Epoch: 349, Batch Gradient Norm: 7.773742782770738
Epoch: 349, Batch Gradient Norm after: 7.773742782770738
Epoch 350/10000, Prediction Accuracy = 57.64399999999999%, Loss = 0.6762050747871399
Epoch: 350, Batch Gradient Norm: 7.790513759379232
Epoch: 350, Batch Gradient Norm after: 7.790513759379232
Epoch 351/10000, Prediction Accuracy = 57.656000000000006%, Loss = 0.6780515193939209
Epoch: 351, Batch Gradient Norm: 7.3554109689428655
Epoch: 351, Batch Gradient Norm after: 7.3554109689428655
Epoch 352/10000, Prediction Accuracy = 57.74000000000001%, Loss = 0.6753295540809632
Epoch: 352, Batch Gradient Norm: 8.539497673120055
Epoch: 352, Batch Gradient Norm after: 8.539497673120055
Epoch 353/10000, Prediction Accuracy = 57.64399999999999%, Loss = 0.6818026661872864
Epoch: 353, Batch Gradient Norm: 7.7837235797299416
Epoch: 353, Batch Gradient Norm after: 7.7837235797299416
Epoch 354/10000, Prediction Accuracy = 57.7%, Loss = 0.6748197197914123
Epoch: 354, Batch Gradient Norm: 8.017214848043078
Epoch: 354, Batch Gradient Norm after: 8.017214848043078
Epoch 355/10000, Prediction Accuracy = 57.748000000000005%, Loss = 0.6822363138198853
Epoch: 355, Batch Gradient Norm: 7.848230621019952
Epoch: 355, Batch Gradient Norm after: 7.848230621019952
Epoch 356/10000, Prediction Accuracy = 57.75%, Loss = 0.6756866216659546
Epoch: 356, Batch Gradient Norm: 6.812854783744189
Epoch: 356, Batch Gradient Norm after: 6.812854783744189
Epoch 357/10000, Prediction Accuracy = 57.70399999999999%, Loss = 0.6713732719421387
Epoch: 357, Batch Gradient Norm: 6.12769836299698
Epoch: 357, Batch Gradient Norm after: 6.12769836299698
Epoch 358/10000, Prediction Accuracy = 57.824%, Loss = 0.6677405357360839
Epoch: 358, Batch Gradient Norm: 6.028286023798145
Epoch: 358, Batch Gradient Norm after: 6.028286023798145
Epoch 359/10000, Prediction Accuracy = 57.854%, Loss = 0.666874349117279
Epoch: 359, Batch Gradient Norm: 9.05063527276945
Epoch: 359, Batch Gradient Norm after: 9.05063527276945
Epoch 360/10000, Prediction Accuracy = 57.798%, Loss = 0.6817603707313538
Epoch: 360, Batch Gradient Norm: 6.994229224618446
Epoch: 360, Batch Gradient Norm after: 6.994229224618446
Epoch 361/10000, Prediction Accuracy = 57.712%, Loss = 0.6703306913375855
Epoch: 361, Batch Gradient Norm: 7.773481063522377
Epoch: 361, Batch Gradient Norm after: 7.773481063522377
Epoch 362/10000, Prediction Accuracy = 57.831999999999994%, Loss = 0.6747417688369751
Epoch: 362, Batch Gradient Norm: 6.818812354218101
Epoch: 362, Batch Gradient Norm after: 6.818812354218101
Epoch 363/10000, Prediction Accuracy = 57.77%, Loss = 0.6693161964416504
Epoch: 363, Batch Gradient Norm: 6.044967145853985
Epoch: 363, Batch Gradient Norm after: 6.044967145853985
Epoch 364/10000, Prediction Accuracy = 57.86600000000001%, Loss = 0.6657105803489685
Epoch: 364, Batch Gradient Norm: 5.13462953247715
Epoch: 364, Batch Gradient Norm after: 5.13462953247715
Epoch 365/10000, Prediction Accuracy = 57.83200000000001%, Loss = 0.6618735790252686
Epoch: 365, Batch Gradient Norm: 7.331427868607894
Epoch: 365, Batch Gradient Norm after: 7.331427868607894
Epoch 366/10000, Prediction Accuracy = 57.812%, Loss = 0.6708752751350403
Epoch: 366, Batch Gradient Norm: 8.76063352130816
Epoch: 366, Batch Gradient Norm after: 8.76063352130816
Epoch 367/10000, Prediction Accuracy = 57.876%, Loss = 0.6724225878715515
Epoch: 367, Batch Gradient Norm: 6.758858604047675
Epoch: 367, Batch Gradient Norm after: 6.758858604047675
Epoch 368/10000, Prediction Accuracy = 57.8%, Loss = 0.6636951446533204
Epoch: 368, Batch Gradient Norm: 7.27622346347001
Epoch: 368, Batch Gradient Norm after: 7.27622346347001
Epoch 369/10000, Prediction Accuracy = 57.762%, Loss = 0.6627090811729431
Epoch: 369, Batch Gradient Norm: 7.672295552736918
Epoch: 369, Batch Gradient Norm after: 7.672295552736918
Epoch 370/10000, Prediction Accuracy = 57.891999999999996%, Loss = 0.6632415533065796
Epoch: 370, Batch Gradient Norm: 6.670493456494788
Epoch: 370, Batch Gradient Norm after: 6.670493456494788
Epoch 371/10000, Prediction Accuracy = 57.85%, Loss = 0.6616467595100403
Epoch: 371, Batch Gradient Norm: 6.231393919945311
Epoch: 371, Batch Gradient Norm after: 6.231393919945311
Epoch 372/10000, Prediction Accuracy = 57.763999999999996%, Loss = 0.66069495677948
Epoch: 372, Batch Gradient Norm: 6.573213444064568
Epoch: 372, Batch Gradient Norm after: 6.573213444064568
Epoch 373/10000, Prediction Accuracy = 57.884%, Loss = 0.6636816024780273
Epoch: 373, Batch Gradient Norm: 7.242067412532487
Epoch: 373, Batch Gradient Norm after: 7.242067412532487
Epoch 374/10000, Prediction Accuracy = 57.946000000000005%, Loss = 0.6675595879554749
Epoch: 374, Batch Gradient Norm: 5.4645420512874034
Epoch: 374, Batch Gradient Norm after: 5.4645420512874034
Epoch 375/10000, Prediction Accuracy = 57.922000000000004%, Loss = 0.6575401186943054
Epoch: 375, Batch Gradient Norm: 6.352957958485253
Epoch: 375, Batch Gradient Norm after: 6.352957958485253
Epoch 376/10000, Prediction Accuracy = 57.938%, Loss = 0.6578945755958557
Epoch: 376, Batch Gradient Norm: 9.22225551977133
Epoch: 376, Batch Gradient Norm after: 9.22225551977133
Epoch 377/10000, Prediction Accuracy = 57.862%, Loss = 0.6704016923904419
Epoch: 377, Batch Gradient Norm: 9.127810950694835
Epoch: 377, Batch Gradient Norm after: 9.127810950694835
Epoch 378/10000, Prediction Accuracy = 57.8%, Loss = 0.6661108493804931
Epoch: 378, Batch Gradient Norm: 10.227941761757508
Epoch: 378, Batch Gradient Norm after: 10.227941761757508
Epoch 379/10000, Prediction Accuracy = 57.934000000000005%, Loss = 0.6751524806022644
Epoch: 379, Batch Gradient Norm: 8.082379337487088
Epoch: 379, Batch Gradient Norm after: 8.082379337487088
Epoch 380/10000, Prediction Accuracy = 57.919999999999995%, Loss = 0.660651445388794
Epoch: 380, Batch Gradient Norm: 10.112272685401283
Epoch: 380, Batch Gradient Norm after: 10.112272685401283
Epoch 381/10000, Prediction Accuracy = 58.024%, Loss = 0.6691230654716491
Epoch: 381, Batch Gradient Norm: 8.717238232542712
Epoch: 381, Batch Gradient Norm after: 8.717238232542712
Epoch 382/10000, Prediction Accuracy = 57.914%, Loss = 0.6600585103034973
Epoch: 382, Batch Gradient Norm: 8.048023273162379
Epoch: 382, Batch Gradient Norm after: 8.048023273162379
Epoch 383/10000, Prediction Accuracy = 57.95%, Loss = 0.6574185967445374
Epoch: 383, Batch Gradient Norm: 8.48694969929916
Epoch: 383, Batch Gradient Norm after: 8.48694969929916
Epoch 384/10000, Prediction Accuracy = 57.867999999999995%, Loss = 0.660761296749115
Epoch: 384, Batch Gradient Norm: 8.86868296819916
Epoch: 384, Batch Gradient Norm after: 8.86868296819916
Epoch 385/10000, Prediction Accuracy = 58.086%, Loss = 0.6595206975936889
Epoch: 385, Batch Gradient Norm: 7.786092352480817
Epoch: 385, Batch Gradient Norm after: 7.786092352480817
Epoch 386/10000, Prediction Accuracy = 57.972%, Loss = 0.655777096748352
Epoch: 386, Batch Gradient Norm: 7.13203307743026
Epoch: 386, Batch Gradient Norm after: 7.13203307743026
Epoch 387/10000, Prediction Accuracy = 57.93000000000001%, Loss = 0.6538868069648742
Epoch: 387, Batch Gradient Norm: 7.008349012548412
Epoch: 387, Batch Gradient Norm after: 7.008349012548412
Epoch 388/10000, Prediction Accuracy = 57.980000000000004%, Loss = 0.6548935294151306
Epoch: 388, Batch Gradient Norm: 7.618277610754889
Epoch: 388, Batch Gradient Norm after: 7.618277610754889
Epoch 389/10000, Prediction Accuracy = 58.040000000000006%, Loss = 0.6553130507469177
Epoch: 389, Batch Gradient Norm: 5.802202241220809
Epoch: 389, Batch Gradient Norm after: 5.802202241220809
Epoch 390/10000, Prediction Accuracy = 58.024%, Loss = 0.6494606137275696
Epoch: 390, Batch Gradient Norm: 7.420428874823731
Epoch: 390, Batch Gradient Norm after: 7.420428874823731
Epoch 391/10000, Prediction Accuracy = 58.00600000000001%, Loss = 0.6542524099349976
Epoch: 391, Batch Gradient Norm: 5.8367211723446095
Epoch: 391, Batch Gradient Norm after: 5.8367211723446095
Epoch 392/10000, Prediction Accuracy = 58.00599999999999%, Loss = 0.6488978028297424
Epoch: 392, Batch Gradient Norm: 5.942973293524482
Epoch: 392, Batch Gradient Norm after: 5.942973293524482
Epoch 393/10000, Prediction Accuracy = 57.95799999999999%, Loss = 0.6509511351585389
Epoch: 393, Batch Gradient Norm: 6.95030083625443
Epoch: 393, Batch Gradient Norm after: 6.95030083625443
Epoch 394/10000, Prediction Accuracy = 58.096000000000004%, Loss = 0.6525163650512695
Epoch: 394, Batch Gradient Norm: 7.1096709836775585
Epoch: 394, Batch Gradient Norm after: 7.1096709836775585
Epoch 395/10000, Prediction Accuracy = 58.108000000000004%, Loss = 0.6503002643585205
Epoch: 395, Batch Gradient Norm: 7.323728577553645
Epoch: 395, Batch Gradient Norm after: 7.323728577553645
Epoch 396/10000, Prediction Accuracy = 58.128%, Loss = 0.6517737984657288
Epoch: 396, Batch Gradient Norm: 7.085755291369796
Epoch: 396, Batch Gradient Norm after: 7.085755291369796
Epoch 397/10000, Prediction Accuracy = 58.168000000000006%, Loss = 0.6500161051750183
Epoch: 397, Batch Gradient Norm: 7.352354565202911
Epoch: 397, Batch Gradient Norm after: 7.352354565202911
Epoch 398/10000, Prediction Accuracy = 58.072%, Loss = 0.6526943922042847
Epoch: 398, Batch Gradient Norm: 7.099404898569664
Epoch: 398, Batch Gradient Norm after: 7.099404898569664
Epoch 399/10000, Prediction Accuracy = 58.16400000000001%, Loss = 0.6478410720825195
Epoch: 399, Batch Gradient Norm: 6.480560176684726
Epoch: 399, Batch Gradient Norm after: 6.480560176684726
Epoch 400/10000, Prediction Accuracy = 58.116%, Loss = 0.6471446394920349
Epoch: 400, Batch Gradient Norm: 5.775903207407793
Epoch: 400, Batch Gradient Norm after: 5.775903207407793
Epoch 401/10000, Prediction Accuracy = 58.11%, Loss = 0.6456419825553894
Epoch: 401, Batch Gradient Norm: 7.092199256320276
Epoch: 401, Batch Gradient Norm after: 7.092199256320276
Epoch 402/10000, Prediction Accuracy = 58.05%, Loss = 0.6516241908073426
Epoch: 402, Batch Gradient Norm: 7.978224072551098
Epoch: 402, Batch Gradient Norm after: 7.978224072551098
Epoch 403/10000, Prediction Accuracy = 58.24400000000001%, Loss = 0.6520148634910583
Epoch: 403, Batch Gradient Norm: 5.718162011252223
Epoch: 403, Batch Gradient Norm after: 5.718162011252223
Epoch 404/10000, Prediction Accuracy = 58.05799999999999%, Loss = 0.6448526620864868
Epoch: 404, Batch Gradient Norm: 6.248659509233605
Epoch: 404, Batch Gradient Norm after: 6.248659509233605
Epoch 405/10000, Prediction Accuracy = 58.227999999999994%, Loss = 0.6455658793449401
Epoch: 405, Batch Gradient Norm: 7.272580601468875
Epoch: 405, Batch Gradient Norm after: 7.272580601468875
Epoch 406/10000, Prediction Accuracy = 58.11800000000001%, Loss = 0.6469046592712402
Epoch: 406, Batch Gradient Norm: 7.14980263089136
Epoch: 406, Batch Gradient Norm after: 7.14980263089136
Epoch 407/10000, Prediction Accuracy = 58.114%, Loss = 0.6469318985939025
Epoch: 407, Batch Gradient Norm: 8.346629751205706
Epoch: 407, Batch Gradient Norm after: 8.346629751205706
Epoch 408/10000, Prediction Accuracy = 58.18000000000001%, Loss = 0.6519907593727112
Epoch: 408, Batch Gradient Norm: 7.626659825807893
Epoch: 408, Batch Gradient Norm after: 7.626659825807893
Epoch 409/10000, Prediction Accuracy = 58.068000000000005%, Loss = 0.6454105973243713
Epoch: 409, Batch Gradient Norm: 7.224603892968175
Epoch: 409, Batch Gradient Norm after: 7.224603892968175
Epoch 410/10000, Prediction Accuracy = 58.198%, Loss = 0.6458468437194824
Epoch: 410, Batch Gradient Norm: 7.0453521168874245
Epoch: 410, Batch Gradient Norm after: 7.0453521168874245
Epoch 411/10000, Prediction Accuracy = 58.176%, Loss = 0.646471631526947
Epoch: 411, Batch Gradient Norm: 6.5743320808756165
Epoch: 411, Batch Gradient Norm after: 6.5743320808756165
Epoch 412/10000, Prediction Accuracy = 58.169999999999995%, Loss = 0.6429151296615601
Epoch: 412, Batch Gradient Norm: 5.746199390257028
Epoch: 412, Batch Gradient Norm after: 5.746199390257028
Epoch 413/10000, Prediction Accuracy = 58.15999999999999%, Loss = 0.6399293303489685
Epoch: 413, Batch Gradient Norm: 7.357805567221549
Epoch: 413, Batch Gradient Norm after: 7.357805567221549
Epoch 414/10000, Prediction Accuracy = 58.184000000000005%, Loss = 0.6461738348007202
Epoch: 414, Batch Gradient Norm: 7.077637961898761
Epoch: 414, Batch Gradient Norm after: 7.077637961898761
Epoch 415/10000, Prediction Accuracy = 58.27%, Loss = 0.641830050945282
Epoch: 415, Batch Gradient Norm: 8.852003298747915
Epoch: 415, Batch Gradient Norm after: 8.852003298747915
Epoch 416/10000, Prediction Accuracy = 58.3%, Loss = 0.6486067771911621
Epoch: 416, Batch Gradient Norm: 6.850921030950748
Epoch: 416, Batch Gradient Norm after: 6.850921030950748
Epoch 417/10000, Prediction Accuracy = 58.20799999999999%, Loss = 0.6405321955680847
Epoch: 417, Batch Gradient Norm: 8.344913659207863
Epoch: 417, Batch Gradient Norm after: 8.344913659207863
Epoch 418/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.64254709482193
Epoch: 418, Batch Gradient Norm: 8.290030302245643
Epoch: 418, Batch Gradient Norm after: 8.290030302245643
Epoch 419/10000, Prediction Accuracy = 58.21%, Loss = 0.6437296390533447
Epoch: 419, Batch Gradient Norm: 10.27592228341059
Epoch: 419, Batch Gradient Norm after: 10.27592228341059
Epoch 420/10000, Prediction Accuracy = 58.27%, Loss = 0.6538643479347229
Epoch: 420, Batch Gradient Norm: 8.692090437398178
Epoch: 420, Batch Gradient Norm after: 8.692090437398178
Epoch 421/10000, Prediction Accuracy = 58.336%, Loss = 0.6448073148727417
Epoch: 421, Batch Gradient Norm: 7.481139416860241
Epoch: 421, Batch Gradient Norm after: 7.481139416860241
Epoch 422/10000, Prediction Accuracy = 58.263999999999996%, Loss = 0.6408013463020324
Epoch: 422, Batch Gradient Norm: 7.644200032867386
Epoch: 422, Batch Gradient Norm after: 7.644200032867386
Epoch 423/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.6419145822525024
Epoch: 423, Batch Gradient Norm: 7.4974315631913155
Epoch: 423, Batch Gradient Norm after: 7.4974315631913155
Epoch 424/10000, Prediction Accuracy = 58.262%, Loss = 0.6387344837188721
Epoch: 424, Batch Gradient Norm: 7.271076207049679
Epoch: 424, Batch Gradient Norm after: 7.271076207049679
Epoch 425/10000, Prediction Accuracy = 58.286%, Loss = 0.6365875244140625
Epoch: 425, Batch Gradient Norm: 7.46184918279446
Epoch: 425, Batch Gradient Norm after: 7.46184918279446
Epoch 426/10000, Prediction Accuracy = 58.338%, Loss = 0.6368098139762879
Epoch: 426, Batch Gradient Norm: 6.406585479437736
Epoch: 426, Batch Gradient Norm after: 6.406585479437736
Epoch 427/10000, Prediction Accuracy = 58.294000000000004%, Loss = 0.6358446598052978
Epoch: 427, Batch Gradient Norm: 8.325362535124157
Epoch: 427, Batch Gradient Norm after: 8.325362535124157
Epoch 428/10000, Prediction Accuracy = 58.28599999999999%, Loss = 0.6399235486984253
Epoch: 428, Batch Gradient Norm: 10.1470750722357
Epoch: 428, Batch Gradient Norm after: 10.1470750722357
Epoch 429/10000, Prediction Accuracy = 58.29200000000001%, Loss = 0.6405177116394043
Epoch: 429, Batch Gradient Norm: 13.028497798121242
Epoch: 429, Batch Gradient Norm after: 13.028497798121242
Epoch 430/10000, Prediction Accuracy = 58.36800000000001%, Loss = 0.6448590874671936
Epoch: 430, Batch Gradient Norm: 10.180653460503963
Epoch: 430, Batch Gradient Norm after: 10.180653460503963
Epoch 431/10000, Prediction Accuracy = 58.29%, Loss = 0.6400311350822449
Epoch: 431, Batch Gradient Norm: 9.253994531383102
Epoch: 431, Batch Gradient Norm after: 9.253994531383102
Epoch 432/10000, Prediction Accuracy = 58.36%, Loss = 0.6385451555252075
Epoch: 432, Batch Gradient Norm: 8.151649074725228
Epoch: 432, Batch Gradient Norm after: 8.151649074725228
Epoch 433/10000, Prediction Accuracy = 58.334%, Loss = 0.6349340558052063
Epoch: 433, Batch Gradient Norm: 8.265351797060768
Epoch: 433, Batch Gradient Norm after: 8.265351797060768
Epoch 434/10000, Prediction Accuracy = 58.29600000000001%, Loss = 0.6365259408950805
Epoch: 434, Batch Gradient Norm: 10.39787429114557
Epoch: 434, Batch Gradient Norm after: 10.39787429114557
Epoch 435/10000, Prediction Accuracy = 58.48199999999999%, Loss = 0.6479506611824035
Epoch: 435, Batch Gradient Norm: 9.152518601376896
Epoch: 435, Batch Gradient Norm after: 9.152518601376896
Epoch 436/10000, Prediction Accuracy = 58.23199999999999%, Loss = 0.643465518951416
Epoch: 436, Batch Gradient Norm: 9.679691494251943
Epoch: 436, Batch Gradient Norm after: 9.679691494251943
Epoch 437/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.6469770789146423
Epoch: 437, Batch Gradient Norm: 8.901310375490752
Epoch: 437, Batch Gradient Norm after: 8.901310375490752
Epoch 438/10000, Prediction Accuracy = 58.336%, Loss = 0.6426121830940247
Epoch: 438, Batch Gradient Norm: 8.432520968869701
Epoch: 438, Batch Gradient Norm after: 8.432520968869701
Epoch 439/10000, Prediction Accuracy = 58.465999999999994%, Loss = 0.6378015160560608
Epoch: 439, Batch Gradient Norm: 7.718982715915056
Epoch: 439, Batch Gradient Norm after: 7.718982715915056
Epoch 440/10000, Prediction Accuracy = 58.477999999999994%, Loss = 0.635215425491333
Epoch: 440, Batch Gradient Norm: 8.108350381276072
Epoch: 440, Batch Gradient Norm after: 8.108350381276072
Epoch 441/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.636764407157898
Epoch: 441, Batch Gradient Norm: 6.00715427345455
Epoch: 441, Batch Gradient Norm after: 6.00715427345455
Epoch 442/10000, Prediction Accuracy = 58.398%, Loss = 0.6282248020172119
Epoch: 442, Batch Gradient Norm: 8.805823853821927
Epoch: 442, Batch Gradient Norm after: 8.805823853821927
Epoch 443/10000, Prediction Accuracy = 58.455999999999996%, Loss = 0.6397028207778931
Epoch: 443, Batch Gradient Norm: 6.9256797470590765
Epoch: 443, Batch Gradient Norm after: 6.9256797470590765
Epoch 444/10000, Prediction Accuracy = 58.464%, Loss = 0.6307308673858643
Epoch: 444, Batch Gradient Norm: 8.90111659129759
Epoch: 444, Batch Gradient Norm after: 8.90111659129759
Epoch 445/10000, Prediction Accuracy = 58.44%, Loss = 0.6370849728584289
Epoch: 445, Batch Gradient Norm: 7.372301346701073
Epoch: 445, Batch Gradient Norm after: 7.372301346701073
Epoch 446/10000, Prediction Accuracy = 58.45%, Loss = 0.6319393396377564
Epoch: 446, Batch Gradient Norm: 6.669911850733671
Epoch: 446, Batch Gradient Norm after: 6.669911850733671
Epoch 447/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.6273311972618103
Epoch: 447, Batch Gradient Norm: 6.620467346085607
Epoch: 447, Batch Gradient Norm after: 6.620467346085607
Epoch 448/10000, Prediction Accuracy = 58.488%, Loss = 0.6308925986289978
Epoch: 448, Batch Gradient Norm: 6.380012159494238
Epoch: 448, Batch Gradient Norm after: 6.380012159494238
Epoch 449/10000, Prediction Accuracy = 58.426%, Loss = 0.6278244495391846
Epoch: 449, Batch Gradient Norm: 6.858792849227108
Epoch: 449, Batch Gradient Norm after: 6.858792849227108
Epoch 450/10000, Prediction Accuracy = 58.458000000000006%, Loss = 0.6277226090431214
Epoch: 450, Batch Gradient Norm: 7.080246986817482
Epoch: 450, Batch Gradient Norm after: 7.080246986817482
Epoch 451/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.6296074390411377
Epoch: 451, Batch Gradient Norm: 7.629781926469415
Epoch: 451, Batch Gradient Norm after: 7.629781926469415
Epoch 452/10000, Prediction Accuracy = 58.418000000000006%, Loss = 0.6292955040931701
Epoch: 452, Batch Gradient Norm: 7.012566809654921
Epoch: 452, Batch Gradient Norm after: 7.012566809654921
Epoch 453/10000, Prediction Accuracy = 58.556000000000004%, Loss = 0.6271295666694641
Epoch: 453, Batch Gradient Norm: 5.991753764521546
Epoch: 453, Batch Gradient Norm after: 5.991753764521546
Epoch 454/10000, Prediction Accuracy = 58.434000000000005%, Loss = 0.6238906383514404
Epoch: 454, Batch Gradient Norm: 7.508668339382229
Epoch: 454, Batch Gradient Norm after: 7.508668339382229
Epoch 455/10000, Prediction Accuracy = 58.5%, Loss = 0.6263927698135376
Epoch: 455, Batch Gradient Norm: 9.433841241641987
Epoch: 455, Batch Gradient Norm after: 9.433841241641987
Epoch 456/10000, Prediction Accuracy = 58.548%, Loss = 0.6335881590843201
Epoch: 456, Batch Gradient Norm: 6.960010734726802
Epoch: 456, Batch Gradient Norm after: 6.960010734726802
Epoch 457/10000, Prediction Accuracy = 58.486000000000004%, Loss = 0.6240728855133056
Epoch: 457, Batch Gradient Norm: 8.028470694491775
Epoch: 457, Batch Gradient Norm after: 8.028470694491775
Epoch 458/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.6257072925567627
Epoch: 458, Batch Gradient Norm: 8.086222786352641
Epoch: 458, Batch Gradient Norm after: 8.086222786352641
Epoch 459/10000, Prediction Accuracy = 58.52%, Loss = 0.6288742542266845
Epoch: 459, Batch Gradient Norm: 7.88560559410601
Epoch: 459, Batch Gradient Norm after: 7.88560559410601
Epoch 460/10000, Prediction Accuracy = 58.516000000000005%, Loss = 0.6271390318870544
Epoch: 460, Batch Gradient Norm: 8.308601773989418
Epoch: 460, Batch Gradient Norm after: 8.308601773989418
Epoch 461/10000, Prediction Accuracy = 58.504%, Loss = 0.6286085605621338
Epoch: 461, Batch Gradient Norm: 6.724248958798437
Epoch: 461, Batch Gradient Norm after: 6.724248958798437
Epoch 462/10000, Prediction Accuracy = 58.565999999999995%, Loss = 0.6232188463211059
Epoch: 462, Batch Gradient Norm: 8.751029982111092
Epoch: 462, Batch Gradient Norm after: 8.751029982111092
Epoch 463/10000, Prediction Accuracy = 58.674%, Loss = 0.6340571403503418
Epoch: 463, Batch Gradient Norm: 6.525940090429661
Epoch: 463, Batch Gradient Norm after: 6.525940090429661
Epoch 464/10000, Prediction Accuracy = 58.608000000000004%, Loss = 0.6220693469047547
Epoch: 464, Batch Gradient Norm: 8.192148059783456
Epoch: 464, Batch Gradient Norm after: 8.192148059783456
Epoch 465/10000, Prediction Accuracy = 58.59400000000001%, Loss = 0.624522614479065
Epoch: 465, Batch Gradient Norm: 8.939317866531995
Epoch: 465, Batch Gradient Norm after: 8.939317866531995
Epoch 466/10000, Prediction Accuracy = 58.50600000000001%, Loss = 0.6272472381591797
Epoch: 466, Batch Gradient Norm: 7.435008076117649
Epoch: 466, Batch Gradient Norm after: 7.435008076117649
Epoch 467/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.6220724701881408
Epoch: 467, Batch Gradient Norm: 8.38074868184798
Epoch: 467, Batch Gradient Norm after: 8.38074868184798
Epoch 468/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.6276384711265564
Epoch: 468, Batch Gradient Norm: 9.245096784997974
Epoch: 468, Batch Gradient Norm after: 9.245096784997974
Epoch 469/10000, Prediction Accuracy = 58.620000000000005%, Loss = 0.6312567234039307
Epoch: 469, Batch Gradient Norm: 7.456780098676889
Epoch: 469, Batch Gradient Norm after: 7.456780098676889
Epoch 470/10000, Prediction Accuracy = 58.52%, Loss = 0.6216715335845947
Epoch: 470, Batch Gradient Norm: 7.232506901264027
Epoch: 470, Batch Gradient Norm after: 7.232506901264027
Epoch 471/10000, Prediction Accuracy = 58.564%, Loss = 0.6206487894058228
Epoch: 471, Batch Gradient Norm: 7.013003396248601
Epoch: 471, Batch Gradient Norm after: 7.013003396248601
Epoch 472/10000, Prediction Accuracy = 58.576%, Loss = 0.6192382097244262
Epoch: 472, Batch Gradient Norm: 8.715813580350712
Epoch: 472, Batch Gradient Norm after: 8.715813580350712
Epoch 473/10000, Prediction Accuracy = 58.702%, Loss = 0.6233050942420959
Epoch: 473, Batch Gradient Norm: 11.116327549640769
Epoch: 473, Batch Gradient Norm after: 11.116327549640769
Epoch 474/10000, Prediction Accuracy = 58.586%, Loss = 0.6248943567276001
Epoch: 474, Batch Gradient Norm: 12.182634508384318
Epoch: 474, Batch Gradient Norm after: 12.182634508384318
Epoch 475/10000, Prediction Accuracy = 58.726%, Loss = 0.6305393695831298
Epoch: 475, Batch Gradient Norm: 9.6502751139555
Epoch: 475, Batch Gradient Norm after: 9.6502751139555
Epoch 476/10000, Prediction Accuracy = 58.726%, Loss = 0.6228120684623718
Epoch: 476, Batch Gradient Norm: 9.815266968139097
Epoch: 476, Batch Gradient Norm after: 9.815266968139097
Epoch 477/10000, Prediction Accuracy = 58.694%, Loss = 0.6213443398475647
Epoch: 477, Batch Gradient Norm: 7.9688471898369055
Epoch: 477, Batch Gradient Norm after: 7.9688471898369055
Epoch 478/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.6185264348983764
Epoch: 478, Batch Gradient Norm: 8.216963972484239
Epoch: 478, Batch Gradient Norm after: 8.216963972484239
Epoch 479/10000, Prediction Accuracy = 58.69%, Loss = 0.6192925930023193
Epoch: 479, Batch Gradient Norm: 7.7265697849223125
Epoch: 479, Batch Gradient Norm after: 7.7265697849223125
Epoch 480/10000, Prediction Accuracy = 58.59000000000001%, Loss = 0.6190826654434204
Epoch: 480, Batch Gradient Norm: 8.738657652302384
Epoch: 480, Batch Gradient Norm after: 8.738657652302384
Epoch 481/10000, Prediction Accuracy = 58.652%, Loss = 0.6213951706886292
Epoch: 481, Batch Gradient Norm: 7.802276287299722
Epoch: 481, Batch Gradient Norm after: 7.802276287299722
Epoch 482/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.6196946859359741
Epoch: 482, Batch Gradient Norm: 8.722593230739744
Epoch: 482, Batch Gradient Norm after: 8.722593230739744
Epoch 483/10000, Prediction Accuracy = 58.815999999999995%, Loss = 0.6240572333335876
Epoch: 483, Batch Gradient Norm: 8.224732540623537
Epoch: 483, Batch Gradient Norm after: 8.224732540623537
Epoch 484/10000, Prediction Accuracy = 58.746%, Loss = 0.6212282061576844
Epoch: 484, Batch Gradient Norm: 9.522886202700928
Epoch: 484, Batch Gradient Norm after: 9.522886202700928
Epoch 485/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.6271216034889221
Epoch: 485, Batch Gradient Norm: 10.62005579655337
Epoch: 485, Batch Gradient Norm after: 10.62005579655337
Epoch 486/10000, Prediction Accuracy = 58.76400000000001%, Loss = 0.6224080681800842
Epoch: 486, Batch Gradient Norm: 10.365742172220175
Epoch: 486, Batch Gradient Norm after: 10.365742172220175
Epoch 487/10000, Prediction Accuracy = 58.676%, Loss = 0.6258239269256591
Epoch: 487, Batch Gradient Norm: 11.762292609048256
Epoch: 487, Batch Gradient Norm after: 11.762292609048256
Epoch 488/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.6297006607055664
Epoch: 488, Batch Gradient Norm: 9.674817635499629
Epoch: 488, Batch Gradient Norm after: 9.674817635499629
Epoch 489/10000, Prediction Accuracy = 58.754%, Loss = 0.624127459526062
Epoch: 489, Batch Gradient Norm: 9.288608753336986
Epoch: 489, Batch Gradient Norm after: 9.288608753336986
Epoch 490/10000, Prediction Accuracy = 58.75%, Loss = 0.6172824382781983
Epoch: 490, Batch Gradient Norm: 8.62357151932508
Epoch: 490, Batch Gradient Norm after: 8.62357151932508
Epoch 491/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.6143723964691162
Epoch: 491, Batch Gradient Norm: 9.621674761011594
Epoch: 491, Batch Gradient Norm after: 9.621674761011594
Epoch 492/10000, Prediction Accuracy = 58.806%, Loss = 0.6198001861572265
Epoch: 492, Batch Gradient Norm: 9.040760293722295
Epoch: 492, Batch Gradient Norm after: 9.040760293722295
Epoch 493/10000, Prediction Accuracy = 58.822%, Loss = 0.6152011156082153
Epoch: 493, Batch Gradient Norm: 8.474235312783417
Epoch: 493, Batch Gradient Norm after: 8.474235312783417
Epoch 494/10000, Prediction Accuracy = 58.86800000000001%, Loss = 0.6147619724273682
Epoch: 494, Batch Gradient Norm: 7.96294909274219
Epoch: 494, Batch Gradient Norm after: 7.96294909274219
Epoch 495/10000, Prediction Accuracy = 58.83800000000001%, Loss = 0.616947615146637
Epoch: 495, Batch Gradient Norm: 8.299003662080475
Epoch: 495, Batch Gradient Norm after: 8.299003662080475
Epoch 496/10000, Prediction Accuracy = 58.722%, Loss = 0.612899899482727
Epoch: 496, Batch Gradient Norm: 9.203293905852744
Epoch: 496, Batch Gradient Norm after: 9.203293905852744
Epoch 497/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.6172722101211547
Epoch: 497, Batch Gradient Norm: 10.681899498156936
Epoch: 497, Batch Gradient Norm after: 10.681899498156936
Epoch 498/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.6222251415252685
Epoch: 498, Batch Gradient Norm: 10.387490876890315
Epoch: 498, Batch Gradient Norm after: 10.387490876890315
Epoch 499/10000, Prediction Accuracy = 58.852%, Loss = 0.6227821826934814
Epoch: 499, Batch Gradient Norm: 8.689433531957576
Epoch: 499, Batch Gradient Norm after: 8.689433531957576
Epoch 500/10000, Prediction Accuracy = 58.80999999999999%, Loss = 0.6169737935066223
Epoch: 500, Batch Gradient Norm: 8.780621170930626
Epoch: 500, Batch Gradient Norm after: 8.780621170930626
Epoch 501/10000, Prediction Accuracy = 58.85799999999999%, Loss = 0.6163290619850159
Epoch: 501, Batch Gradient Norm: 8.324604808073772
Epoch: 501, Batch Gradient Norm after: 8.324604808073772
Epoch 502/10000, Prediction Accuracy = 58.826%, Loss = 0.617543363571167
Epoch: 502, Batch Gradient Norm: 10.527303216443693
Epoch: 502, Batch Gradient Norm after: 10.527303216443693
Epoch 503/10000, Prediction Accuracy = 58.924%, Loss = 0.6217903375625611
Epoch: 503, Batch Gradient Norm: 8.9268627461048
Epoch: 503, Batch Gradient Norm after: 8.9268627461048
Epoch 504/10000, Prediction Accuracy = 58.818%, Loss = 0.6159628629684448
Epoch: 504, Batch Gradient Norm: 9.119920428558546
Epoch: 504, Batch Gradient Norm after: 9.119920428558546
Epoch 505/10000, Prediction Accuracy = 58.898%, Loss = 0.6144965291023254
Epoch: 505, Batch Gradient Norm: 8.319260635086241
Epoch: 505, Batch Gradient Norm after: 8.319260635086241
Epoch 506/10000, Prediction Accuracy = 58.907999999999994%, Loss = 0.6094798445701599
Epoch: 506, Batch Gradient Norm: 10.841191486191995
Epoch: 506, Batch Gradient Norm after: 10.841191486191995
Epoch 507/10000, Prediction Accuracy = 58.902%, Loss = 0.6209335565567017
Epoch: 507, Batch Gradient Norm: 9.394468713805013
Epoch: 507, Batch Gradient Norm after: 9.394468713805013
Epoch 508/10000, Prediction Accuracy = 58.864%, Loss = 0.6138724684715271
Epoch: 508, Batch Gradient Norm: 9.677377129133633
Epoch: 508, Batch Gradient Norm after: 9.677377129133633
Epoch 509/10000, Prediction Accuracy = 58.94%, Loss = 0.6164838790893554
Epoch: 509, Batch Gradient Norm: 10.830353293548605
Epoch: 509, Batch Gradient Norm after: 10.830353293548605
Epoch 510/10000, Prediction Accuracy = 58.886%, Loss = 0.6250375986099244
Epoch: 510, Batch Gradient Norm: 8.99860243377415
Epoch: 510, Batch Gradient Norm after: 8.99860243377415
Epoch 511/10000, Prediction Accuracy = 58.846000000000004%, Loss = 0.6141750812530518
Epoch: 511, Batch Gradient Norm: 9.679066142656033
Epoch: 511, Batch Gradient Norm after: 9.679066142656033
Epoch 512/10000, Prediction Accuracy = 58.878%, Loss = 0.6167549371719361
Epoch: 512, Batch Gradient Norm: 9.912067975535292
Epoch: 512, Batch Gradient Norm after: 9.912067975535292
Epoch 513/10000, Prediction Accuracy = 58.902%, Loss = 0.6164377331733704
Epoch: 513, Batch Gradient Norm: 10.064802149266795
Epoch: 513, Batch Gradient Norm after: 10.064802149266795
Epoch 514/10000, Prediction Accuracy = 59.02%, Loss = 0.6165709137916565
Epoch: 514, Batch Gradient Norm: 9.335068864164464
Epoch: 514, Batch Gradient Norm after: 9.335068864164464
Epoch 515/10000, Prediction Accuracy = 58.902%, Loss = 0.6158083438873291
Epoch: 515, Batch Gradient Norm: 9.196969206695394
Epoch: 515, Batch Gradient Norm after: 9.196969206695394
Epoch 516/10000, Prediction Accuracy = 58.94000000000001%, Loss = 0.6121903181076049
Epoch: 516, Batch Gradient Norm: 6.954557338905622
Epoch: 516, Batch Gradient Norm after: 6.954557338905622
Epoch 517/10000, Prediction Accuracy = 58.932%, Loss = 0.6063877820968628
Epoch: 517, Batch Gradient Norm: 8.078442984717832
Epoch: 517, Batch Gradient Norm after: 8.078442984717832
Epoch 518/10000, Prediction Accuracy = 58.958000000000006%, Loss = 0.6067339181900024
Epoch: 518, Batch Gradient Norm: 7.233658193582562
Epoch: 518, Batch Gradient Norm after: 7.233658193582562
Epoch 519/10000, Prediction Accuracy = 58.874%, Loss = 0.6056336522102356
Epoch: 519, Batch Gradient Norm: 7.376834207987622
Epoch: 519, Batch Gradient Norm after: 7.376834207987622
Epoch 520/10000, Prediction Accuracy = 58.86200000000001%, Loss = 0.6040698409080505
Epoch: 520, Batch Gradient Norm: 8.187716825708694
Epoch: 520, Batch Gradient Norm after: 8.187716825708694
Epoch 521/10000, Prediction Accuracy = 58.89799999999999%, Loss = 0.6081121921539306
Epoch: 521, Batch Gradient Norm: 6.566273353154902
Epoch: 521, Batch Gradient Norm after: 6.566273353154902
Epoch 522/10000, Prediction Accuracy = 58.95%, Loss = 0.6036303639411926
Epoch: 522, Batch Gradient Norm: 7.791619854629414
Epoch: 522, Batch Gradient Norm after: 7.791619854629414
Epoch 523/10000, Prediction Accuracy = 58.908%, Loss = 0.6065672039985657
Epoch: 523, Batch Gradient Norm: 7.648243802560821
Epoch: 523, Batch Gradient Norm after: 7.648243802560821
Epoch 524/10000, Prediction Accuracy = 58.96199999999999%, Loss = 0.6050476789474487
Epoch: 524, Batch Gradient Norm: 6.517911109308512
Epoch: 524, Batch Gradient Norm after: 6.517911109308512
Epoch 525/10000, Prediction Accuracy = 58.970000000000006%, Loss = 0.6034576058387756
Epoch: 525, Batch Gradient Norm: 6.381279102137897
Epoch: 525, Batch Gradient Norm after: 6.381279102137897
Epoch 526/10000, Prediction Accuracy = 58.968%, Loss = 0.6019254803657532
Epoch: 526, Batch Gradient Norm: 7.564826734116353
Epoch: 526, Batch Gradient Norm after: 7.564826734116353
Epoch 527/10000, Prediction Accuracy = 58.964%, Loss = 0.6060265064239502
Epoch: 527, Batch Gradient Norm: 7.1193790986791115
Epoch: 527, Batch Gradient Norm after: 7.1193790986791115
Epoch 528/10000, Prediction Accuracy = 58.916%, Loss = 0.6031287550926209
Epoch: 528, Batch Gradient Norm: 9.750492781118124
Epoch: 528, Batch Gradient Norm after: 9.750492781118124
Epoch 529/10000, Prediction Accuracy = 58.99400000000001%, Loss = 0.6116005659103394
Epoch: 529, Batch Gradient Norm: 8.946280745722614
Epoch: 529, Batch Gradient Norm after: 8.946280745722614
Epoch 530/10000, Prediction Accuracy = 58.99400000000001%, Loss = 0.6095841765403748
Epoch: 530, Batch Gradient Norm: 10.208553022990298
Epoch: 530, Batch Gradient Norm after: 10.208553022990298
Epoch 531/10000, Prediction Accuracy = 58.962%, Loss = 0.6090488076210022
Epoch: 531, Batch Gradient Norm: 8.712314578405861
Epoch: 531, Batch Gradient Norm after: 8.712314578405861
Epoch 532/10000, Prediction Accuracy = 58.992%, Loss = 0.605062747001648
Epoch: 532, Batch Gradient Norm: 9.692807724227833
Epoch: 532, Batch Gradient Norm after: 9.692807724227833
Epoch 533/10000, Prediction Accuracy = 58.974000000000004%, Loss = 0.6089208602905274
Epoch: 533, Batch Gradient Norm: 8.582349379596797
Epoch: 533, Batch Gradient Norm after: 8.582349379596797
Epoch 534/10000, Prediction Accuracy = 59.024%, Loss = 0.6047493577003479
Epoch: 534, Batch Gradient Norm: 9.575287548876624
Epoch: 534, Batch Gradient Norm after: 9.575287548876624
Epoch 535/10000, Prediction Accuracy = 59.072%, Loss = 0.6114913582801819
Epoch: 535, Batch Gradient Norm: 9.564288520591736
Epoch: 535, Batch Gradient Norm after: 9.564288520591736
Epoch 536/10000, Prediction Accuracy = 58.912%, Loss = 0.6150374054908753
Epoch: 536, Batch Gradient Norm: 9.55443917272795
Epoch: 536, Batch Gradient Norm after: 9.55443917272795
Epoch 537/10000, Prediction Accuracy = 58.93399999999999%, Loss = 0.6119534611701966
Epoch: 537, Batch Gradient Norm: 8.23593253027282
Epoch: 537, Batch Gradient Norm after: 8.23593253027282
Epoch 538/10000, Prediction Accuracy = 59.024%, Loss = 0.6062036156654358
Epoch: 538, Batch Gradient Norm: 8.389912927837994
Epoch: 538, Batch Gradient Norm after: 8.389912927837994
Epoch 539/10000, Prediction Accuracy = 59.14%, Loss = 0.6047383904457092
Epoch: 539, Batch Gradient Norm: 9.556333635261455
Epoch: 539, Batch Gradient Norm after: 9.556333635261455
Epoch 540/10000, Prediction Accuracy = 59.00600000000001%, Loss = 0.606791639328003
Epoch: 540, Batch Gradient Norm: 7.875547606374434
Epoch: 540, Batch Gradient Norm after: 7.875547606374434
Epoch 541/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.6026588916778565
Epoch: 541, Batch Gradient Norm: 8.646277440878626
Epoch: 541, Batch Gradient Norm after: 8.646277440878626
Epoch 542/10000, Prediction Accuracy = 59.040000000000006%, Loss = 0.6072013854980469
Epoch: 542, Batch Gradient Norm: 7.401409177437921
Epoch: 542, Batch Gradient Norm after: 7.401409177437921
Epoch 543/10000, Prediction Accuracy = 59.05999999999999%, Loss = 0.6012082815170288
Epoch: 543, Batch Gradient Norm: 7.67636326708921
Epoch: 543, Batch Gradient Norm after: 7.67636326708921
Epoch 544/10000, Prediction Accuracy = 59.074%, Loss = 0.6018760561943054
Epoch: 544, Batch Gradient Norm: 7.697289105289024
Epoch: 544, Batch Gradient Norm after: 7.697289105289024
Epoch 545/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.5979528188705444
Epoch: 545, Batch Gradient Norm: 7.716530909461236
Epoch: 545, Batch Gradient Norm after: 7.716530909461236
Epoch 546/10000, Prediction Accuracy = 59.008%, Loss = 0.6009166598320007
Epoch: 546, Batch Gradient Norm: 7.471316108716595
Epoch: 546, Batch Gradient Norm after: 7.471316108716595
Epoch 547/10000, Prediction Accuracy = 59.088%, Loss = 0.6005183219909668
Epoch: 547, Batch Gradient Norm: 8.9877949299013
Epoch: 547, Batch Gradient Norm after: 8.9877949299013
Epoch 548/10000, Prediction Accuracy = 59.04200000000001%, Loss = 0.6047043561935425
Epoch: 548, Batch Gradient Norm: 9.22762396165146
Epoch: 548, Batch Gradient Norm after: 9.22762396165146
Epoch 549/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.6022842168807984
Epoch: 549, Batch Gradient Norm: 8.896166322393594
Epoch: 549, Batch Gradient Norm after: 8.896166322393594
Epoch 550/10000, Prediction Accuracy = 59.148%, Loss = 0.6015141129493713
Epoch: 550, Batch Gradient Norm: 9.684959785934137
Epoch: 550, Batch Gradient Norm after: 9.684959785934137
Epoch 551/10000, Prediction Accuracy = 59.068%, Loss = 0.6062968611717224
Epoch: 551, Batch Gradient Norm: 7.713863469408222
Epoch: 551, Batch Gradient Norm after: 7.713863469408222
Epoch 552/10000, Prediction Accuracy = 59.05799999999999%, Loss = 0.5993828296661377
Epoch: 552, Batch Gradient Norm: 9.135521409079951
Epoch: 552, Batch Gradient Norm after: 9.135521409079951
Epoch 553/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.6036844372749328
Epoch: 553, Batch Gradient Norm: 7.613293818542609
Epoch: 553, Batch Gradient Norm after: 7.613293818542609
Epoch 554/10000, Prediction Accuracy = 59.116%, Loss = 0.5980981469154358
Epoch: 554, Batch Gradient Norm: 9.30610877248799
Epoch: 554, Batch Gradient Norm after: 9.30610877248799
Epoch 555/10000, Prediction Accuracy = 59.156000000000006%, Loss = 0.6035525560379028
Epoch: 555, Batch Gradient Norm: 10.109895909396856
Epoch: 555, Batch Gradient Norm after: 10.109895909396856
Epoch 556/10000, Prediction Accuracy = 59.14200000000001%, Loss = 0.6019383907318115
Epoch: 556, Batch Gradient Norm: 9.560613213055396
Epoch: 556, Batch Gradient Norm after: 9.560613213055396
Epoch 557/10000, Prediction Accuracy = 59.194%, Loss = 0.6017617821693421
Epoch: 557, Batch Gradient Norm: 8.906151047337511
Epoch: 557, Batch Gradient Norm after: 8.906151047337511
Epoch 558/10000, Prediction Accuracy = 59.048%, Loss = 0.601228392124176
Epoch: 558, Batch Gradient Norm: 9.344427896545028
Epoch: 558, Batch Gradient Norm after: 9.344427896545028
Epoch 559/10000, Prediction Accuracy = 59.126%, Loss = 0.6005967617034912
Epoch: 559, Batch Gradient Norm: 9.726115828808986
Epoch: 559, Batch Gradient Norm after: 9.726115828808986
Epoch 560/10000, Prediction Accuracy = 59.21600000000001%, Loss = 0.6033735275268555
Epoch: 560, Batch Gradient Norm: 9.758270690944453
Epoch: 560, Batch Gradient Norm after: 9.758270690944453
Epoch 561/10000, Prediction Accuracy = 59.202%, Loss = 0.6047658920288086
Epoch: 561, Batch Gradient Norm: 7.5017739908633265
Epoch: 561, Batch Gradient Norm after: 7.5017739908633265
Epoch 562/10000, Prediction Accuracy = 59.128%, Loss = 0.5946106433868408
Epoch: 562, Batch Gradient Norm: 8.751814954600588
Epoch: 562, Batch Gradient Norm after: 8.751814954600588
Epoch 563/10000, Prediction Accuracy = 59.246%, Loss = 0.5976870894432068
Epoch: 563, Batch Gradient Norm: 8.206370718743292
Epoch: 563, Batch Gradient Norm after: 8.206370718743292
Epoch 564/10000, Prediction Accuracy = 59.077999999999996%, Loss = 0.5977196812629699
Epoch: 564, Batch Gradient Norm: 7.998381787100009
Epoch: 564, Batch Gradient Norm after: 7.998381787100009
Epoch 565/10000, Prediction Accuracy = 59.198%, Loss = 0.5958675146102905
Epoch: 565, Batch Gradient Norm: 9.19024624076532
Epoch: 565, Batch Gradient Norm after: 9.19024624076532
Epoch 566/10000, Prediction Accuracy = 59.11199999999999%, Loss = 0.597821855545044
Epoch: 566, Batch Gradient Norm: 9.342073736897
Epoch: 566, Batch Gradient Norm after: 9.342073736897
Epoch 567/10000, Prediction Accuracy = 59.134%, Loss = 0.598249363899231
Epoch: 567, Batch Gradient Norm: 9.766739690745437
Epoch: 567, Batch Gradient Norm after: 9.766739690745437
Epoch 568/10000, Prediction Accuracy = 59.152%, Loss = 0.5993253946304321
Epoch: 568, Batch Gradient Norm: 6.712852197384017
Epoch: 568, Batch Gradient Norm after: 6.712852197384017
Epoch 569/10000, Prediction Accuracy = 59.138%, Loss = 0.5930890440940857
Epoch: 569, Batch Gradient Norm: 9.080276532572093
Epoch: 569, Batch Gradient Norm after: 9.080276532572093
Epoch 570/10000, Prediction Accuracy = 59.126%, Loss = 0.5983339667320251
Epoch: 570, Batch Gradient Norm: 8.3092959118383
Epoch: 570, Batch Gradient Norm after: 8.3092959118383
Epoch 571/10000, Prediction Accuracy = 59.19000000000001%, Loss = 0.5958328723907471
Epoch: 571, Batch Gradient Norm: 10.072098461617284
Epoch: 571, Batch Gradient Norm after: 10.072098461617284
Epoch 572/10000, Prediction Accuracy = 59.11%, Loss = 0.6017043232917786
Epoch: 572, Batch Gradient Norm: 10.503700985944484
Epoch: 572, Batch Gradient Norm after: 10.503700985944484
Epoch 573/10000, Prediction Accuracy = 59.208000000000006%, Loss = 0.5992856383323669
Epoch: 573, Batch Gradient Norm: 10.751003168711327
Epoch: 573, Batch Gradient Norm after: 10.751003168711327
Epoch 574/10000, Prediction Accuracy = 59.146%, Loss = 0.5989201784133911
Epoch: 574, Batch Gradient Norm: 10.482667758069882
Epoch: 574, Batch Gradient Norm after: 10.482667758069882
Epoch 575/10000, Prediction Accuracy = 59.164%, Loss = 0.5954401731491089
Epoch: 575, Batch Gradient Norm: 7.948692724086523
Epoch: 575, Batch Gradient Norm after: 7.948692724086523
Epoch 576/10000, Prediction Accuracy = 59.17199999999999%, Loss = 0.5921404242515564
Epoch: 576, Batch Gradient Norm: 7.173583856229264
Epoch: 576, Batch Gradient Norm after: 7.173583856229264
Epoch 577/10000, Prediction Accuracy = 59.162%, Loss = 0.5913725733757019
Epoch: 577, Batch Gradient Norm: 5.736612143330842
Epoch: 577, Batch Gradient Norm after: 5.736612143330842
Epoch 578/10000, Prediction Accuracy = 59.124%, Loss = 0.5887842655181885
Epoch: 578, Batch Gradient Norm: 7.000084267303468
Epoch: 578, Batch Gradient Norm after: 7.000084267303468
Epoch 579/10000, Prediction Accuracy = 59.30200000000001%, Loss = 0.5923056960105896
Epoch: 579, Batch Gradient Norm: 6.661701804202888
Epoch: 579, Batch Gradient Norm after: 6.661701804202888
Epoch 580/10000, Prediction Accuracy = 59.227999999999994%, Loss = 0.591299045085907
Epoch: 580, Batch Gradient Norm: 6.6974701754281645
Epoch: 580, Batch Gradient Norm after: 6.6974701754281645
Epoch 581/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.5892291188240051
Epoch: 581, Batch Gradient Norm: 7.202393190053416
Epoch: 581, Batch Gradient Norm after: 7.202393190053416
Epoch 582/10000, Prediction Accuracy = 59.234%, Loss = 0.5886573791503906
Epoch: 582, Batch Gradient Norm: 6.686981033626017
Epoch: 582, Batch Gradient Norm after: 6.686981033626017
Epoch 583/10000, Prediction Accuracy = 59.288%, Loss = 0.58912433385849
Epoch: 583, Batch Gradient Norm: 5.953031843382714
Epoch: 583, Batch Gradient Norm after: 5.953031843382714
Epoch 584/10000, Prediction Accuracy = 59.21199999999999%, Loss = 0.585769248008728
Epoch: 584, Batch Gradient Norm: 6.417745628461814
Epoch: 584, Batch Gradient Norm after: 6.417745628461814
Epoch 585/10000, Prediction Accuracy = 59.193999999999996%, Loss = 0.5865479588508606
Epoch: 585, Batch Gradient Norm: 7.831626038951046
Epoch: 585, Batch Gradient Norm after: 7.831626038951046
Epoch 586/10000, Prediction Accuracy = 59.312%, Loss = 0.5889759421348572
Epoch: 586, Batch Gradient Norm: 9.813584830857952
Epoch: 586, Batch Gradient Norm after: 9.813584830857952
Epoch 587/10000, Prediction Accuracy = 59.206%, Loss = 0.5940986752510071
Epoch: 587, Batch Gradient Norm: 9.926856065235535
Epoch: 587, Batch Gradient Norm after: 9.926856065235535
Epoch 588/10000, Prediction Accuracy = 59.218%, Loss = 0.5948947548866272
Epoch: 588, Batch Gradient Norm: 8.512026424645901
Epoch: 588, Batch Gradient Norm after: 8.512026424645901
Epoch 589/10000, Prediction Accuracy = 59.214%, Loss = 0.5920347213745117
Epoch: 589, Batch Gradient Norm: 8.568521402522245
Epoch: 589, Batch Gradient Norm after: 8.568521402522245
Epoch 590/10000, Prediction Accuracy = 59.152%, Loss = 0.59137362241745
Epoch: 590, Batch Gradient Norm: 11.79258856292843
Epoch: 590, Batch Gradient Norm after: 11.79258856292843
Epoch 591/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.6046452403068543
Epoch: 591, Batch Gradient Norm: 10.887811737684375
Epoch: 591, Batch Gradient Norm after: 10.887811737684375
Epoch 592/10000, Prediction Accuracy = 59.2%, Loss = 0.6051733613014221
Epoch: 592, Batch Gradient Norm: 10.890674700688098
Epoch: 592, Batch Gradient Norm after: 10.890674700688098
Epoch 593/10000, Prediction Accuracy = 59.262%, Loss = 0.6020925998687744
Epoch: 593, Batch Gradient Norm: 8.504389861891648
Epoch: 593, Batch Gradient Norm after: 8.504389861891648
Epoch 594/10000, Prediction Accuracy = 59.238%, Loss = 0.5945405244827271
Epoch: 594, Batch Gradient Norm: 10.46582518141104
Epoch: 594, Batch Gradient Norm after: 10.46582518141104
Epoch 595/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.5974114537239075
Epoch: 595, Batch Gradient Norm: 10.806231942587123
Epoch: 595, Batch Gradient Norm after: 10.806231942587123
Epoch 596/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.6030957579612732
Epoch: 596, Batch Gradient Norm: 8.999942672694244
Epoch: 596, Batch Gradient Norm after: 8.999942672694244
Epoch 597/10000, Prediction Accuracy = 59.338%, Loss = 0.5913737177848816
Epoch: 597, Batch Gradient Norm: 8.624562446888827
Epoch: 597, Batch Gradient Norm after: 8.624562446888827
Epoch 598/10000, Prediction Accuracy = 59.262%, Loss = 0.5902832627296448
Epoch: 598, Batch Gradient Norm: 9.990808590453295
Epoch: 598, Batch Gradient Norm after: 9.990808590453295
Epoch 599/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.596164858341217
Epoch: 599, Batch Gradient Norm: 8.2868485128764
Epoch: 599, Batch Gradient Norm after: 8.2868485128764
Epoch 600/10000, Prediction Accuracy = 59.324%, Loss = 0.5869797110557556
Epoch: 600, Batch Gradient Norm: 9.836331398344795
Epoch: 600, Batch Gradient Norm after: 9.836331398344795
Epoch 601/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.594695258140564
Epoch: 601, Batch Gradient Norm: 7.539657611993324
Epoch: 601, Batch Gradient Norm after: 7.539657611993324
Epoch 602/10000, Prediction Accuracy = 59.33799999999999%, Loss = 0.5857927560806274
Epoch: 602, Batch Gradient Norm: 7.715420326003307
Epoch: 602, Batch Gradient Norm after: 7.715420326003307
Epoch 603/10000, Prediction Accuracy = 59.278%, Loss = 0.5879926443099975
Epoch: 603, Batch Gradient Norm: 9.637091271235914
Epoch: 603, Batch Gradient Norm after: 9.637091271235914
Epoch 604/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.5883245587348938
Epoch: 604, Batch Gradient Norm: 8.639572832273348
Epoch: 604, Batch Gradient Norm after: 8.639572832273348
Epoch 605/10000, Prediction Accuracy = 59.242000000000004%, Loss = 0.5891404747962952
Epoch: 605, Batch Gradient Norm: 8.589000293660801
Epoch: 605, Batch Gradient Norm after: 8.589000293660801
Epoch 606/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.59090176820755
Epoch: 606, Batch Gradient Norm: 8.434657751090565
Epoch: 606, Batch Gradient Norm after: 8.434657751090565
Epoch 607/10000, Prediction Accuracy = 59.188%, Loss = 0.5889659523963928
Epoch: 607, Batch Gradient Norm: 9.088460545024263
Epoch: 607, Batch Gradient Norm after: 9.088460545024263
Epoch 608/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.5895642518997193
Epoch: 608, Batch Gradient Norm: 8.640189312601017
Epoch: 608, Batch Gradient Norm after: 8.640189312601017
Epoch 609/10000, Prediction Accuracy = 59.302%, Loss = 0.5874699234962464
Epoch: 609, Batch Gradient Norm: 7.314834993763388
Epoch: 609, Batch Gradient Norm after: 7.314834993763388
Epoch 610/10000, Prediction Accuracy = 59.326%, Loss = 0.583596658706665
Epoch: 610, Batch Gradient Norm: 8.133726923024
Epoch: 610, Batch Gradient Norm after: 8.133726923024
Epoch 611/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.5865990519523621
Epoch: 611, Batch Gradient Norm: 9.08558975139926
Epoch: 611, Batch Gradient Norm after: 9.08558975139926
Epoch 612/10000, Prediction Accuracy = 59.284000000000006%, Loss = 0.5850420832633972
Epoch: 612, Batch Gradient Norm: 7.131370197141259
Epoch: 612, Batch Gradient Norm after: 7.131370197141259
Epoch 613/10000, Prediction Accuracy = 59.282000000000004%, Loss = 0.5830970883369446
Epoch: 613, Batch Gradient Norm: 6.638793222159178
Epoch: 613, Batch Gradient Norm after: 6.638793222159178
Epoch 614/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.5837265133857727
Epoch: 614, Batch Gradient Norm: 7.166522557120964
Epoch: 614, Batch Gradient Norm after: 7.166522557120964
Epoch 615/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.5832142114639283
Epoch: 615, Batch Gradient Norm: 7.520833704412821
Epoch: 615, Batch Gradient Norm after: 7.520833704412821
Epoch 616/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.5822819471359253
Epoch: 616, Batch Gradient Norm: 6.533635263423817
Epoch: 616, Batch Gradient Norm after: 6.533635263423817
Epoch 617/10000, Prediction Accuracy = 59.374%, Loss = 0.5796938538551331
Epoch: 617, Batch Gradient Norm: 6.535470057674999
Epoch: 617, Batch Gradient Norm after: 6.535470057674999
Epoch 618/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.5812035322189331
Epoch: 618, Batch Gradient Norm: 8.484013628085203
Epoch: 618, Batch Gradient Norm after: 8.484013628085203
Epoch 619/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.5833132863044739
Epoch: 619, Batch Gradient Norm: 9.104759118767952
Epoch: 619, Batch Gradient Norm after: 9.104759118767952
Epoch 620/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.5852655410766602
Epoch: 620, Batch Gradient Norm: 9.622509046139347
Epoch: 620, Batch Gradient Norm after: 9.622509046139347
Epoch 621/10000, Prediction Accuracy = 59.362%, Loss = 0.5859777212142945
Epoch: 621, Batch Gradient Norm: 9.354986481954267
Epoch: 621, Batch Gradient Norm after: 9.354986481954267
Epoch 622/10000, Prediction Accuracy = 59.3%, Loss = 0.5880857706069946
Epoch: 622, Batch Gradient Norm: 9.132645286990218
Epoch: 622, Batch Gradient Norm after: 9.132645286990218
Epoch 623/10000, Prediction Accuracy = 59.306%, Loss = 0.585916006565094
Epoch: 623, Batch Gradient Norm: 8.653667080019153
Epoch: 623, Batch Gradient Norm after: 8.653667080019153
Epoch 624/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.5846974849700928
Epoch: 624, Batch Gradient Norm: 8.023993203583082
Epoch: 624, Batch Gradient Norm after: 8.023993203583082
Epoch 625/10000, Prediction Accuracy = 59.414%, Loss = 0.5840055584907532
Epoch: 625, Batch Gradient Norm: 9.02069962033363
Epoch: 625, Batch Gradient Norm after: 9.02069962033363
Epoch 626/10000, Prediction Accuracy = 59.348%, Loss = 0.583668327331543
Epoch: 626, Batch Gradient Norm: 8.747621393564815
Epoch: 626, Batch Gradient Norm after: 8.747621393564815
Epoch 627/10000, Prediction Accuracy = 59.33%, Loss = 0.5830446243286133
Epoch: 627, Batch Gradient Norm: 8.266923557724416
Epoch: 627, Batch Gradient Norm after: 8.266923557724416
Epoch 628/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.582278025150299
Epoch: 628, Batch Gradient Norm: 7.603072819695175
Epoch: 628, Batch Gradient Norm after: 7.603072819695175
Epoch 629/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.5811921119689941
Epoch: 629, Batch Gradient Norm: 6.5579192965151405
Epoch: 629, Batch Gradient Norm after: 6.5579192965151405
Epoch 630/10000, Prediction Accuracy = 59.348%, Loss = 0.5796637415885926
Epoch: 630, Batch Gradient Norm: 8.31341579504228
Epoch: 630, Batch Gradient Norm after: 8.31341579504228
Epoch 631/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.5812236666679382
Epoch: 631, Batch Gradient Norm: 7.589775302344274
Epoch: 631, Batch Gradient Norm after: 7.589775302344274
Epoch 632/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.5788709640502929
Epoch: 632, Batch Gradient Norm: 9.336901867299169
Epoch: 632, Batch Gradient Norm after: 9.336901867299169
Epoch 633/10000, Prediction Accuracy = 59.438%, Loss = 0.584671700000763
Epoch: 633, Batch Gradient Norm: 10.529545794447152
Epoch: 633, Batch Gradient Norm after: 10.529545794447152
Epoch 634/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.586127495765686
Epoch: 634, Batch Gradient Norm: 11.626186091606005
Epoch: 634, Batch Gradient Norm after: 11.626186091606005
Epoch 635/10000, Prediction Accuracy = 59.362%, Loss = 0.5844356298446656
Epoch: 635, Batch Gradient Norm: 11.111762913408086
Epoch: 635, Batch Gradient Norm after: 11.111762913408086
Epoch 636/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.5840852618217468
Epoch: 636, Batch Gradient Norm: 8.830459640828243
Epoch: 636, Batch Gradient Norm after: 8.830459640828243
Epoch 637/10000, Prediction Accuracy = 59.274%, Loss = 0.5799908995628357
Epoch: 637, Batch Gradient Norm: 9.748109609588269
Epoch: 637, Batch Gradient Norm after: 9.748109609588269
Epoch 638/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.587659215927124
Epoch: 638, Batch Gradient Norm: 9.482927193605972
Epoch: 638, Batch Gradient Norm after: 9.482927193605972
Epoch 639/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.5821077585220337
Epoch: 639, Batch Gradient Norm: 8.664311523356371
Epoch: 639, Batch Gradient Norm after: 8.664311523356371
Epoch 640/10000, Prediction Accuracy = 59.35999999999999%, Loss = 0.5789117574691772
Epoch: 640, Batch Gradient Norm: 8.603411117053069
Epoch: 640, Batch Gradient Norm after: 8.603411117053069
Epoch 641/10000, Prediction Accuracy = 59.39%, Loss = 0.5786235094070434
Epoch: 641, Batch Gradient Norm: 9.388963516680331
Epoch: 641, Batch Gradient Norm after: 9.388963516680331
Epoch 642/10000, Prediction Accuracy = 59.334%, Loss = 0.5839036583900452
Epoch: 642, Batch Gradient Norm: 8.382519129618485
Epoch: 642, Batch Gradient Norm after: 8.382519129618485
Epoch 643/10000, Prediction Accuracy = 59.39399999999999%, Loss = 0.5801990389823913
Epoch: 643, Batch Gradient Norm: 7.434535661737428
Epoch: 643, Batch Gradient Norm after: 7.434535661737428
Epoch 644/10000, Prediction Accuracy = 59.476%, Loss = 0.5789451599121094
Epoch: 644, Batch Gradient Norm: 8.029673049933598
Epoch: 644, Batch Gradient Norm after: 8.029673049933598
Epoch 645/10000, Prediction Accuracy = 59.326%, Loss = 0.5781781435012817
Epoch: 645, Batch Gradient Norm: 8.077887006733606
Epoch: 645, Batch Gradient Norm after: 8.077887006733606
Epoch 646/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.577301812171936
Epoch: 646, Batch Gradient Norm: 7.468343527281105
Epoch: 646, Batch Gradient Norm after: 7.468343527281105
Epoch 647/10000, Prediction Accuracy = 59.33599999999999%, Loss = 0.5770743012428283
Epoch: 647, Batch Gradient Norm: 8.181620375554136
Epoch: 647, Batch Gradient Norm after: 8.181620375554136
Epoch 648/10000, Prediction Accuracy = 59.412%, Loss = 0.5791682124137878
Epoch: 648, Batch Gradient Norm: 7.3369845001007565
Epoch: 648, Batch Gradient Norm after: 7.3369845001007565
Epoch 649/10000, Prediction Accuracy = 59.48%, Loss = 0.5765288472175598
Epoch: 649, Batch Gradient Norm: 7.8824403038894255
Epoch: 649, Batch Gradient Norm after: 7.8824403038894255
Epoch 650/10000, Prediction Accuracy = 59.388%, Loss = 0.5765240669250489
Epoch: 650, Batch Gradient Norm: 7.946938438860113
Epoch: 650, Batch Gradient Norm after: 7.946938438860113
Epoch 651/10000, Prediction Accuracy = 59.41799999999999%, Loss = 0.5766367197036744
Epoch: 651, Batch Gradient Norm: 8.710224485533635
Epoch: 651, Batch Gradient Norm after: 8.710224485533635
Epoch 652/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.5769212365150451
Epoch: 652, Batch Gradient Norm: 8.800931534380222
Epoch: 652, Batch Gradient Norm after: 8.800931534380222
Epoch 653/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.5809839129447937
Epoch: 653, Batch Gradient Norm: 10.680810951661606
Epoch: 653, Batch Gradient Norm after: 10.680810951661606
Epoch 654/10000, Prediction Accuracy = 59.374%, Loss = 0.5882919192314148
Epoch: 654, Batch Gradient Norm: 9.532268102418362
Epoch: 654, Batch Gradient Norm after: 9.532268102418362
Epoch 655/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.5827036499977112
Epoch: 655, Batch Gradient Norm: 10.471615403908269
Epoch: 655, Batch Gradient Norm after: 10.471615403908269
Epoch 656/10000, Prediction Accuracy = 59.36800000000001%, Loss = 0.5896904110908509
Epoch: 656, Batch Gradient Norm: 8.576664884637765
Epoch: 656, Batch Gradient Norm after: 8.576664884637765
Epoch 657/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.5792641043663025
Epoch: 657, Batch Gradient Norm: 8.245183063676611
Epoch: 657, Batch Gradient Norm after: 8.245183063676611
Epoch 658/10000, Prediction Accuracy = 59.374%, Loss = 0.5789607167243958
Epoch: 658, Batch Gradient Norm: 7.515013669416557
Epoch: 658, Batch Gradient Norm after: 7.515013669416557
Epoch 659/10000, Prediction Accuracy = 59.426%, Loss = 0.5755733728408814
Epoch: 659, Batch Gradient Norm: 6.891157232992678
Epoch: 659, Batch Gradient Norm after: 6.891157232992678
Epoch 660/10000, Prediction Accuracy = 59.426%, Loss = 0.5747835040092468
Epoch: 660, Batch Gradient Norm: 7.407276501552695
Epoch: 660, Batch Gradient Norm after: 7.407276501552695
Epoch 661/10000, Prediction Accuracy = 59.374%, Loss = 0.5730844020843506
Epoch: 661, Batch Gradient Norm: 8.413368892927755
Epoch: 661, Batch Gradient Norm after: 8.413368892927755
Epoch 662/10000, Prediction Accuracy = 59.462%, Loss = 0.5731570720672607
Epoch: 662, Batch Gradient Norm: 10.0205269844742
Epoch: 662, Batch Gradient Norm after: 10.0205269844742
Epoch 663/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.5803465604782104
Epoch: 663, Batch Gradient Norm: 8.675491072738996
Epoch: 663, Batch Gradient Norm after: 8.675491072738996
Epoch 664/10000, Prediction Accuracy = 59.338%, Loss = 0.5777482986450195
Epoch: 664, Batch Gradient Norm: 7.019216635145806
Epoch: 664, Batch Gradient Norm after: 7.019216635145806
Epoch 665/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.5724541068077087
Epoch: 665, Batch Gradient Norm: 8.041326745980486
Epoch: 665, Batch Gradient Norm after: 8.041326745980486
Epoch 666/10000, Prediction Accuracy = 59.424%, Loss = 0.574997091293335
Epoch: 666, Batch Gradient Norm: 8.383741973949629
Epoch: 666, Batch Gradient Norm after: 8.383741973949629
Epoch 667/10000, Prediction Accuracy = 59.54599999999999%, Loss = 0.5737870454788208
Epoch: 667, Batch Gradient Norm: 7.614479869140211
Epoch: 667, Batch Gradient Norm after: 7.614479869140211
Epoch 668/10000, Prediction Accuracy = 59.496%, Loss = 0.5735805869102478
Epoch: 668, Batch Gradient Norm: 8.31465254421845
Epoch: 668, Batch Gradient Norm after: 8.31465254421845
Epoch 669/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.5772403120994568
Epoch: 669, Batch Gradient Norm: 8.694501785212132
Epoch: 669, Batch Gradient Norm after: 8.694501785212132
Epoch 670/10000, Prediction Accuracy = 59.528%, Loss = 0.5749170660972596
Epoch: 670, Batch Gradient Norm: 8.597897544402327
Epoch: 670, Batch Gradient Norm after: 8.597897544402327
Epoch 671/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.5782294988632202
Epoch: 671, Batch Gradient Norm: 9.073492589093789
Epoch: 671, Batch Gradient Norm after: 9.073492589093789
Epoch 672/10000, Prediction Accuracy = 59.504%, Loss = 0.5752387881278992
Epoch: 672, Batch Gradient Norm: 8.889920453739771
Epoch: 672, Batch Gradient Norm after: 8.889920453739771
Epoch 673/10000, Prediction Accuracy = 59.38799999999999%, Loss = 0.5761518359184266
Epoch: 673, Batch Gradient Norm: 9.193804809130404
Epoch: 673, Batch Gradient Norm after: 9.193804809130404
Epoch 674/10000, Prediction Accuracy = 59.524%, Loss = 0.5751195788383484
Epoch: 674, Batch Gradient Norm: 8.927550346935734
Epoch: 674, Batch Gradient Norm after: 8.927550346935734
Epoch 675/10000, Prediction Accuracy = 59.418000000000006%, Loss = 0.573917818069458
Epoch: 675, Batch Gradient Norm: 8.37456081217891
Epoch: 675, Batch Gradient Norm after: 8.37456081217891
Epoch 676/10000, Prediction Accuracy = 59.452%, Loss = 0.5718377590179443
Epoch: 676, Batch Gradient Norm: 9.833356786063126
Epoch: 676, Batch Gradient Norm after: 9.833356786063126
Epoch 677/10000, Prediction Accuracy = 59.44%, Loss = 0.5771420359611511
Epoch: 677, Batch Gradient Norm: 8.264881012708
Epoch: 677, Batch Gradient Norm after: 8.264881012708
Epoch 678/10000, Prediction Accuracy = 59.484%, Loss = 0.5767815947532654
Epoch: 678, Batch Gradient Norm: 10.058046554028122
Epoch: 678, Batch Gradient Norm after: 10.058046554028122
Epoch 679/10000, Prediction Accuracy = 59.532%, Loss = 0.5800366997718811
Epoch: 679, Batch Gradient Norm: 10.714372596859105
Epoch: 679, Batch Gradient Norm after: 10.714372596859105
Epoch 680/10000, Prediction Accuracy = 59.45%, Loss = 0.578527820110321
Epoch: 680, Batch Gradient Norm: 7.947929789201593
Epoch: 680, Batch Gradient Norm after: 7.947929789201593
Epoch 681/10000, Prediction Accuracy = 59.548%, Loss = 0.5713197588920593
Epoch: 681, Batch Gradient Norm: 7.766760772625999
Epoch: 681, Batch Gradient Norm after: 7.766760772625999
Epoch 682/10000, Prediction Accuracy = 59.548%, Loss = 0.570186448097229
Epoch: 682, Batch Gradient Norm: 8.180238898007275
Epoch: 682, Batch Gradient Norm after: 8.180238898007275
Epoch 683/10000, Prediction Accuracy = 59.548%, Loss = 0.5721138834953308
Epoch: 683, Batch Gradient Norm: 8.542194255659924
Epoch: 683, Batch Gradient Norm after: 8.542194255659924
Epoch 684/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.5718398451805115
Epoch: 684, Batch Gradient Norm: 8.44028846353983
Epoch: 684, Batch Gradient Norm after: 8.44028846353983
Epoch 685/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.5717206835746765
Epoch: 685, Batch Gradient Norm: 7.879705323660141
Epoch: 685, Batch Gradient Norm after: 7.879705323660141
Epoch 686/10000, Prediction Accuracy = 59.45%, Loss = 0.5697133064270019
Epoch: 686, Batch Gradient Norm: 9.695278814565215
Epoch: 686, Batch Gradient Norm after: 9.695278814565215
Epoch 687/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.571260404586792
Epoch: 687, Batch Gradient Norm: 8.071752051980427
Epoch: 687, Batch Gradient Norm after: 8.071752051980427
Epoch 688/10000, Prediction Accuracy = 59.426%, Loss = 0.5683121681213379
Epoch: 688, Batch Gradient Norm: 9.903117826900301
Epoch: 688, Batch Gradient Norm after: 9.903117826900301
Epoch 689/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.5710720896720887
Epoch: 689, Batch Gradient Norm: 10.069384688238264
Epoch: 689, Batch Gradient Norm after: 10.069384688238264
Epoch 690/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.5743982791900635
Epoch: 690, Batch Gradient Norm: 7.813319021198078
Epoch: 690, Batch Gradient Norm after: 7.813319021198078
Epoch 691/10000, Prediction Accuracy = 59.516%, Loss = 0.5718611121177674
Epoch: 691, Batch Gradient Norm: 6.911082176971159
Epoch: 691, Batch Gradient Norm after: 6.911082176971159
Epoch 692/10000, Prediction Accuracy = 59.464%, Loss = 0.5672333717346192
Epoch: 692, Batch Gradient Norm: 9.107196833662481
Epoch: 692, Batch Gradient Norm after: 9.107196833662481
Epoch 693/10000, Prediction Accuracy = 59.452%, Loss = 0.5768754839897156
Epoch: 693, Batch Gradient Norm: 11.615000385716293
Epoch: 693, Batch Gradient Norm after: 11.615000385716293
Epoch 694/10000, Prediction Accuracy = 59.507999999999996%, Loss = 0.5790376186370849
Epoch: 694, Batch Gradient Norm: 12.365486046383491
Epoch: 694, Batch Gradient Norm after: 12.365486046383491
Epoch 695/10000, Prediction Accuracy = 59.46400000000001%, Loss = 0.5798670172691345
Epoch: 695, Batch Gradient Norm: 11.74903160658982
Epoch: 695, Batch Gradient Norm after: 11.74903160658982
Epoch 696/10000, Prediction Accuracy = 59.532%, Loss = 0.5757054686546326
Epoch: 696, Batch Gradient Norm: 10.75275536678826
Epoch: 696, Batch Gradient Norm after: 10.75275536678826
Epoch 697/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.5734450817108154
Epoch: 697, Batch Gradient Norm: 9.21690392417305
Epoch: 697, Batch Gradient Norm after: 9.21690392417305
Epoch 698/10000, Prediction Accuracy = 59.5%, Loss = 0.5684644222259522
Epoch: 698, Batch Gradient Norm: 8.019429771712284
Epoch: 698, Batch Gradient Norm after: 8.019429771712284
Epoch 699/10000, Prediction Accuracy = 59.528%, Loss = 0.5674726843833924
Epoch: 699, Batch Gradient Norm: 7.610952617177222
Epoch: 699, Batch Gradient Norm after: 7.610952617177222
Epoch 700/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.5687469482421875
Epoch: 700, Batch Gradient Norm: 10.583554343647714
Epoch: 700, Batch Gradient Norm after: 10.583554343647714
Epoch 701/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.5727995038032532
Epoch: 701, Batch Gradient Norm: 9.412837339363277
Epoch: 701, Batch Gradient Norm after: 9.412837339363277
Epoch 702/10000, Prediction Accuracy = 59.588%, Loss = 0.5698530912399292
Epoch: 702, Batch Gradient Norm: 8.041960046222215
Epoch: 702, Batch Gradient Norm after: 8.041960046222215
Epoch 703/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.5669207453727723
Epoch: 703, Batch Gradient Norm: 7.986473683710324
Epoch: 703, Batch Gradient Norm after: 7.986473683710324
Epoch 704/10000, Prediction Accuracy = 59.588%, Loss = 0.5671659588813782
Epoch: 704, Batch Gradient Norm: 9.163589937855777
Epoch: 704, Batch Gradient Norm after: 9.163589937855777
Epoch 705/10000, Prediction Accuracy = 59.564%, Loss = 0.5709305524826049
Epoch: 705, Batch Gradient Norm: 7.383269645455375
Epoch: 705, Batch Gradient Norm after: 7.383269645455375
Epoch 706/10000, Prediction Accuracy = 59.5%, Loss = 0.5689391374588013
Epoch: 706, Batch Gradient Norm: 7.114177926878059
Epoch: 706, Batch Gradient Norm after: 7.114177926878059
Epoch 707/10000, Prediction Accuracy = 59.604000000000006%, Loss = 0.5648818850517273
Epoch: 707, Batch Gradient Norm: 8.540061164060395
Epoch: 707, Batch Gradient Norm after: 8.540061164060395
Epoch 708/10000, Prediction Accuracy = 59.556000000000004%, Loss = 0.5684852480888367
Epoch: 708, Batch Gradient Norm: 8.34820844056141
Epoch: 708, Batch Gradient Norm after: 8.34820844056141
Epoch 709/10000, Prediction Accuracy = 59.472%, Loss = 0.5674931168556213
Epoch: 709, Batch Gradient Norm: 7.659287303282927
Epoch: 709, Batch Gradient Norm after: 7.659287303282927
Epoch 710/10000, Prediction Accuracy = 59.496%, Loss = 0.5662070035934448
Epoch: 710, Batch Gradient Norm: 8.097240228883686
Epoch: 710, Batch Gradient Norm after: 8.097240228883686
Epoch 711/10000, Prediction Accuracy = 59.528%, Loss = 0.5672688126564026
Epoch: 711, Batch Gradient Norm: 7.449669593161824
Epoch: 711, Batch Gradient Norm after: 7.449669593161824
Epoch 712/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.5646617174148559
Epoch: 712, Batch Gradient Norm: 6.0003654851651556
Epoch: 712, Batch Gradient Norm after: 6.0003654851651556
Epoch 713/10000, Prediction Accuracy = 59.548%, Loss = 0.5610927939414978
Epoch: 713, Batch Gradient Norm: 9.445449470617014
Epoch: 713, Batch Gradient Norm after: 9.445449470617014
Epoch 714/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.5708776235580444
Epoch: 714, Batch Gradient Norm: 9.214177652334602
Epoch: 714, Batch Gradient Norm after: 9.214177652334602
Epoch 715/10000, Prediction Accuracy = 59.628%, Loss = 0.5683509469032287
Epoch: 715, Batch Gradient Norm: 10.360578961202963
Epoch: 715, Batch Gradient Norm after: 10.360578961202963
Epoch 716/10000, Prediction Accuracy = 59.562%, Loss = 0.5702071785926819
Epoch: 716, Batch Gradient Norm: 8.172039796361078
Epoch: 716, Batch Gradient Norm after: 8.172039796361078
Epoch 717/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.5643348574638367
Epoch: 717, Batch Gradient Norm: 10.436786935387328
Epoch: 717, Batch Gradient Norm after: 10.436786935387328
Epoch 718/10000, Prediction Accuracy = 59.538%, Loss = 0.5718226313591004
Epoch: 718, Batch Gradient Norm: 9.912944208170057
Epoch: 718, Batch Gradient Norm after: 9.912944208170057
Epoch 719/10000, Prediction Accuracy = 59.56600000000001%, Loss = 0.570184075832367
Epoch: 719, Batch Gradient Norm: 8.308305231808301
Epoch: 719, Batch Gradient Norm after: 8.308305231808301
Epoch 720/10000, Prediction Accuracy = 59.592%, Loss = 0.5638626217842102
Epoch: 720, Batch Gradient Norm: 7.79124280369419
Epoch: 720, Batch Gradient Norm after: 7.79124280369419
Epoch 721/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.5638342022895813
Epoch: 721, Batch Gradient Norm: 10.693537846534467
Epoch: 721, Batch Gradient Norm after: 10.693537846534467
Epoch 722/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.5674427032470704
Epoch: 722, Batch Gradient Norm: 8.284287305207684
Epoch: 722, Batch Gradient Norm after: 8.284287305207684
Epoch 723/10000, Prediction Accuracy = 59.552%, Loss = 0.5620640039443969
Epoch: 723, Batch Gradient Norm: 7.702286286667429
Epoch: 723, Batch Gradient Norm after: 7.702286286667429
Epoch 724/10000, Prediction Accuracy = 59.488%, Loss = 0.5624325752258301
Epoch: 724, Batch Gradient Norm: 8.354377622206124
Epoch: 724, Batch Gradient Norm after: 8.354377622206124
Epoch 725/10000, Prediction Accuracy = 59.564%, Loss = 0.5634206652641296
Epoch: 725, Batch Gradient Norm: 8.359221369186482
Epoch: 725, Batch Gradient Norm after: 8.359221369186482
Epoch 726/10000, Prediction Accuracy = 59.617999999999995%, Loss = 0.5632278680801391
Epoch: 726, Batch Gradient Norm: 9.637503593161327
Epoch: 726, Batch Gradient Norm after: 9.637503593161327
Epoch 727/10000, Prediction Accuracy = 59.472%, Loss = 0.5653842329978943
Epoch: 727, Batch Gradient Norm: 9.223111226122263
Epoch: 727, Batch Gradient Norm after: 9.223111226122263
Epoch 728/10000, Prediction Accuracy = 59.532%, Loss = 0.5642200112342834
Epoch: 728, Batch Gradient Norm: 8.003596065809813
Epoch: 728, Batch Gradient Norm after: 8.003596065809813
Epoch 729/10000, Prediction Accuracy = 59.64399999999999%, Loss = 0.5632017850875854
Epoch: 729, Batch Gradient Norm: 8.320614461185482
Epoch: 729, Batch Gradient Norm after: 8.320614461185482
Epoch 730/10000, Prediction Accuracy = 59.572%, Loss = 0.5655373454093933
Epoch: 730, Batch Gradient Norm: 10.312897345571255
Epoch: 730, Batch Gradient Norm after: 10.312897345571255
Epoch 731/10000, Prediction Accuracy = 59.52%, Loss = 0.5718860030174255
Epoch: 731, Batch Gradient Norm: 8.462467197935275
Epoch: 731, Batch Gradient Norm after: 8.462467197935275
Epoch 732/10000, Prediction Accuracy = 59.64000000000001%, Loss = 0.5625915408134461
Epoch: 732, Batch Gradient Norm: 8.266043730697536
Epoch: 732, Batch Gradient Norm after: 8.266043730697536
Epoch 733/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.5627258658409119
Epoch: 733, Batch Gradient Norm: 11.024625291892272
Epoch: 733, Batch Gradient Norm after: 11.024625291892272
Epoch 734/10000, Prediction Accuracy = 59.58200000000001%, Loss = 0.568382203578949
Epoch: 734, Batch Gradient Norm: 11.485811782911544
Epoch: 734, Batch Gradient Norm after: 11.485811782911544
Epoch 735/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.569017744064331
Epoch: 735, Batch Gradient Norm: 12.285276132934
Epoch: 735, Batch Gradient Norm after: 12.285276132934
Epoch 736/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.567864203453064
Epoch: 736, Batch Gradient Norm: 10.343837667653771
Epoch: 736, Batch Gradient Norm after: 10.343837667653771
Epoch 737/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.5654335856437683
Epoch: 737, Batch Gradient Norm: 9.09343868074188
Epoch: 737, Batch Gradient Norm after: 9.09343868074188
Epoch 738/10000, Prediction Accuracy = 59.512%, Loss = 0.5639952301979065
Epoch: 738, Batch Gradient Norm: 8.246879363105496
Epoch: 738, Batch Gradient Norm after: 8.246879363105496
Epoch 739/10000, Prediction Accuracy = 59.628%, Loss = 0.5616126775741577
Epoch: 739, Batch Gradient Norm: 6.705884588188017
Epoch: 739, Batch Gradient Norm after: 6.705884588188017
Epoch 740/10000, Prediction Accuracy = 59.652%, Loss = 0.5576416730880738
Epoch: 740, Batch Gradient Norm: 8.634121248503924
Epoch: 740, Batch Gradient Norm after: 8.634121248503924
Epoch 741/10000, Prediction Accuracy = 59.666%, Loss = 0.5623826026916504
Epoch: 741, Batch Gradient Norm: 8.216704005044823
Epoch: 741, Batch Gradient Norm after: 8.216704005044823
Epoch 742/10000, Prediction Accuracy = 59.656000000000006%, Loss = 0.5610553741455078
Epoch: 742, Batch Gradient Norm: 7.221507486979076
Epoch: 742, Batch Gradient Norm after: 7.221507486979076
Epoch 743/10000, Prediction Accuracy = 59.614%, Loss = 0.5575257897377014
Epoch: 743, Batch Gradient Norm: 7.912355317767583
Epoch: 743, Batch Gradient Norm after: 7.912355317767583
Epoch 744/10000, Prediction Accuracy = 59.65%, Loss = 0.562153697013855
Epoch: 744, Batch Gradient Norm: 6.868071035983808
Epoch: 744, Batch Gradient Norm after: 6.868071035983808
Epoch 745/10000, Prediction Accuracy = 59.634%, Loss = 0.5571141600608825
Epoch: 745, Batch Gradient Norm: 8.100250298605545
Epoch: 745, Batch Gradient Norm after: 8.100250298605545
Epoch 746/10000, Prediction Accuracy = 59.57800000000001%, Loss = 0.5602587699890137
Epoch: 746, Batch Gradient Norm: 7.5214313608808325
Epoch: 746, Batch Gradient Norm after: 7.5214313608808325
Epoch 747/10000, Prediction Accuracy = 59.6%, Loss = 0.5600152373313904
Epoch: 747, Batch Gradient Norm: 8.32617114677637
Epoch: 747, Batch Gradient Norm after: 8.32617114677637
Epoch 748/10000, Prediction Accuracy = 59.55999999999999%, Loss = 0.558346152305603
Epoch: 748, Batch Gradient Norm: 10.376846010100158
Epoch: 748, Batch Gradient Norm after: 10.376846010100158
Epoch 749/10000, Prediction Accuracy = 59.65%, Loss = 0.56800457239151
Epoch: 749, Batch Gradient Norm: 10.002891504989169
Epoch: 749, Batch Gradient Norm after: 10.002891504989169
Epoch 750/10000, Prediction Accuracy = 59.568%, Loss = 0.5670745253562928
Epoch: 750, Batch Gradient Norm: 9.74619430729341
Epoch: 750, Batch Gradient Norm after: 9.74619430729341
Epoch 751/10000, Prediction Accuracy = 59.67999999999999%, Loss = 0.5644540786743164
Epoch: 751, Batch Gradient Norm: 10.038771804337133
Epoch: 751, Batch Gradient Norm after: 10.038771804337133
Epoch 752/10000, Prediction Accuracy = 59.60999999999999%, Loss = 0.5656614661216736
Epoch: 752, Batch Gradient Norm: 11.416176845097983
Epoch: 752, Batch Gradient Norm after: 11.416176845097983
Epoch 753/10000, Prediction Accuracy = 59.626%, Loss = 0.5708266496658325
Epoch: 753, Batch Gradient Norm: 11.157867276258438
Epoch: 753, Batch Gradient Norm after: 11.157867276258438
Epoch 754/10000, Prediction Accuracy = 59.684000000000005%, Loss = 0.5659669280052185
Epoch: 754, Batch Gradient Norm: 12.272940908652682
Epoch: 754, Batch Gradient Norm after: 12.272940908652682
Epoch 755/10000, Prediction Accuracy = 59.61%, Loss = 0.5723111271858216
Epoch: 755, Batch Gradient Norm: 11.624522216100736
Epoch: 755, Batch Gradient Norm after: 11.624522216100736
Epoch 756/10000, Prediction Accuracy = 59.656000000000006%, Loss = 0.5694920301437378
Epoch: 756, Batch Gradient Norm: 10.800407652931018
Epoch: 756, Batch Gradient Norm after: 10.800407652931018
Epoch 757/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.5667186975479126
Epoch: 757, Batch Gradient Norm: 10.437699904944665
Epoch: 757, Batch Gradient Norm after: 10.437699904944665
Epoch 758/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.566498625278473
Epoch: 758, Batch Gradient Norm: 8.55200125748896
Epoch: 758, Batch Gradient Norm after: 8.55200125748896
Epoch 759/10000, Prediction Accuracy = 59.616%, Loss = 0.5588007092475891
Epoch: 759, Batch Gradient Norm: 8.280561552421691
Epoch: 759, Batch Gradient Norm after: 8.280561552421691
Epoch 760/10000, Prediction Accuracy = 59.664%, Loss = 0.5562662243843078
Epoch: 760, Batch Gradient Norm: 7.385073870658458
Epoch: 760, Batch Gradient Norm after: 7.385073870658458
Epoch 761/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.5556927800178528
Epoch: 761, Batch Gradient Norm: 7.89325402428361
Epoch: 761, Batch Gradient Norm after: 7.89325402428361
Epoch 762/10000, Prediction Accuracy = 59.708000000000006%, Loss = 0.5564058899879456
Epoch: 762, Batch Gradient Norm: 7.245308237056159
Epoch: 762, Batch Gradient Norm after: 7.245308237056159
Epoch 763/10000, Prediction Accuracy = 59.688%, Loss = 0.5534722208976746
Epoch: 763, Batch Gradient Norm: 7.597014954334018
Epoch: 763, Batch Gradient Norm after: 7.597014954334018
Epoch 764/10000, Prediction Accuracy = 59.658%, Loss = 0.5569424271583557
Epoch: 764, Batch Gradient Norm: 9.37364359225519
Epoch: 764, Batch Gradient Norm after: 9.37364359225519
Epoch 765/10000, Prediction Accuracy = 59.686%, Loss = 0.5608341336250305
Epoch: 765, Batch Gradient Norm: 9.799600973559738
Epoch: 765, Batch Gradient Norm after: 9.799600973559738
Epoch 766/10000, Prediction Accuracy = 59.628%, Loss = 0.5588682055473327
Epoch: 766, Batch Gradient Norm: 8.517189115855846
Epoch: 766, Batch Gradient Norm after: 8.517189115855846
Epoch 767/10000, Prediction Accuracy = 59.678%, Loss = 0.5575031518936158
Epoch: 767, Batch Gradient Norm: 9.41431860659963
Epoch: 767, Batch Gradient Norm after: 9.41431860659963
Epoch 768/10000, Prediction Accuracy = 59.652%, Loss = 0.5607447385787964
Epoch: 768, Batch Gradient Norm: 8.072220712727516
Epoch: 768, Batch Gradient Norm after: 8.072220712727516
Epoch 769/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.5556195855140686
Epoch: 769, Batch Gradient Norm: 8.854244360627021
Epoch: 769, Batch Gradient Norm after: 8.854244360627021
Epoch 770/10000, Prediction Accuracy = 59.698%, Loss = 0.5588915109634399
Epoch: 770, Batch Gradient Norm: 8.685065327754938
Epoch: 770, Batch Gradient Norm after: 8.685065327754938
Epoch 771/10000, Prediction Accuracy = 59.628%, Loss = 0.5583917975425721
Epoch: 771, Batch Gradient Norm: 8.214187019028673
Epoch: 771, Batch Gradient Norm after: 8.214187019028673
Epoch 772/10000, Prediction Accuracy = 59.626%, Loss = 0.5567337155342102
Epoch: 772, Batch Gradient Norm: 8.830181517293516
Epoch: 772, Batch Gradient Norm after: 8.830181517293516
Epoch 773/10000, Prediction Accuracy = 59.648%, Loss = 0.5543159961700439
Epoch: 773, Batch Gradient Norm: 8.498732679972504
Epoch: 773, Batch Gradient Norm after: 8.498732679972504
Epoch 774/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.5560826301574707
Epoch: 774, Batch Gradient Norm: 6.658235907060438
Epoch: 774, Batch Gradient Norm after: 6.658235907060438
Epoch 775/10000, Prediction Accuracy = 59.646%, Loss = 0.5523621201515198
Epoch: 775, Batch Gradient Norm: 7.428496172733934
Epoch: 775, Batch Gradient Norm after: 7.428496172733934
Epoch 776/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.5547170042991638
Epoch: 776, Batch Gradient Norm: 8.961752948944216
Epoch: 776, Batch Gradient Norm after: 8.961752948944216
Epoch 777/10000, Prediction Accuracy = 59.678%, Loss = 0.5582816243171692
Epoch: 777, Batch Gradient Norm: 12.679476360374538
Epoch: 777, Batch Gradient Norm after: 12.679476360374538
Epoch 778/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.5687171816825867
Epoch: 778, Batch Gradient Norm: 12.634691530969382
Epoch: 778, Batch Gradient Norm after: 12.634691530969382
Epoch 779/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.5675202131271362
Epoch: 779, Batch Gradient Norm: 13.168376281340974
Epoch: 779, Batch Gradient Norm after: 13.168376281340974
Epoch 780/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.5705341219902038
Epoch: 780, Batch Gradient Norm: 11.512762048809877
Epoch: 780, Batch Gradient Norm after: 11.512762048809877
Epoch 781/10000, Prediction Accuracy = 59.67%, Loss = 0.5644337415695191
Epoch: 781, Batch Gradient Norm: 11.392121962038585
Epoch: 781, Batch Gradient Norm after: 11.392121962038585
Epoch 782/10000, Prediction Accuracy = 59.742%, Loss = 0.5611822962760925
Epoch: 782, Batch Gradient Norm: 7.783298793249665
Epoch: 782, Batch Gradient Norm after: 7.783298793249665
Epoch 783/10000, Prediction Accuracy = 59.738%, Loss = 0.5551838994026184
Epoch: 783, Batch Gradient Norm: 7.028365597147339
Epoch: 783, Batch Gradient Norm after: 7.028365597147339
Epoch 784/10000, Prediction Accuracy = 59.682%, Loss = 0.5536304593086243
Epoch: 784, Batch Gradient Norm: 7.413382456023939
Epoch: 784, Batch Gradient Norm after: 7.413382456023939
Epoch 785/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.5530181407928467
Epoch: 785, Batch Gradient Norm: 8.67973702524888
Epoch: 785, Batch Gradient Norm after: 8.67973702524888
Epoch 786/10000, Prediction Accuracy = 59.662%, Loss = 0.5554224848747253
Epoch: 786, Batch Gradient Norm: 10.75499949099265
Epoch: 786, Batch Gradient Norm after: 10.75499949099265
Epoch 787/10000, Prediction Accuracy = 59.674%, Loss = 0.5627894759178161
Epoch: 787, Batch Gradient Norm: 9.944520438640891
Epoch: 787, Batch Gradient Norm after: 9.944520438640891
Epoch 788/10000, Prediction Accuracy = 59.662%, Loss = 0.5573201298713684
Epoch: 788, Batch Gradient Norm: 9.319601905781074
Epoch: 788, Batch Gradient Norm after: 9.319601905781074
Epoch 789/10000, Prediction Accuracy = 59.61%, Loss = 0.5559433698654175
Epoch: 789, Batch Gradient Norm: 9.122770887129409
Epoch: 789, Batch Gradient Norm after: 9.122770887129409
Epoch 790/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.5527626037597656
Epoch: 790, Batch Gradient Norm: 9.828234624202057
Epoch: 790, Batch Gradient Norm after: 9.828234624202057
Epoch 791/10000, Prediction Accuracy = 59.693999999999996%, Loss = 0.5556856632232666
Epoch: 791, Batch Gradient Norm: 10.67837862208673
Epoch: 791, Batch Gradient Norm after: 10.67837862208673
Epoch 792/10000, Prediction Accuracy = 59.616%, Loss = 0.5579333901405334
Epoch: 792, Batch Gradient Norm: 7.2591898076404755
Epoch: 792, Batch Gradient Norm after: 7.2591898076404755
Epoch 793/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.5526182055473328
Epoch: 793, Batch Gradient Norm: 8.85839568566296
Epoch: 793, Batch Gradient Norm after: 8.85839568566296
Epoch 794/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.5545285224914551
Epoch: 794, Batch Gradient Norm: 9.770701167402843
Epoch: 794, Batch Gradient Norm after: 9.770701167402843
Epoch 795/10000, Prediction Accuracy = 59.742%, Loss = 0.5539891839027404
Epoch: 795, Batch Gradient Norm: 7.779742171490422
Epoch: 795, Batch Gradient Norm after: 7.779742171490422
Epoch 796/10000, Prediction Accuracy = 59.658%, Loss = 0.5506288528442382
Epoch: 796, Batch Gradient Norm: 10.164900142337746
Epoch: 796, Batch Gradient Norm after: 10.164900142337746
Epoch 797/10000, Prediction Accuracy = 59.712%, Loss = 0.5585904240608215
Epoch: 797, Batch Gradient Norm: 9.296592884269142
Epoch: 797, Batch Gradient Norm after: 9.296592884269142
Epoch 798/10000, Prediction Accuracy = 59.652%, Loss = 0.5547608256340026
Epoch: 798, Batch Gradient Norm: 8.661927320921489
Epoch: 798, Batch Gradient Norm after: 8.661927320921489
Epoch 799/10000, Prediction Accuracy = 59.56999999999999%, Loss = 0.5565651535987854
Epoch: 799, Batch Gradient Norm: 10.185357814477419
Epoch: 799, Batch Gradient Norm after: 10.185357814477419
Epoch 800/10000, Prediction Accuracy = 59.758%, Loss = 0.5554356336593628
Epoch: 800, Batch Gradient Norm: 9.616771116273883
Epoch: 800, Batch Gradient Norm after: 9.616771116273883
Epoch 801/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.5579200625419617
Epoch: 801, Batch Gradient Norm: 9.933433085390375
Epoch: 801, Batch Gradient Norm after: 9.933433085390375
Epoch 802/10000, Prediction Accuracy = 59.69799999999999%, Loss = 0.5550335764884948
Epoch: 802, Batch Gradient Norm: 9.22592057429207
Epoch: 802, Batch Gradient Norm after: 9.22592057429207
Epoch 803/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.5565828204154968
Epoch: 803, Batch Gradient Norm: 10.601987073261645
Epoch: 803, Batch Gradient Norm after: 10.601987073261645
Epoch 804/10000, Prediction Accuracy = 59.75%, Loss = 0.5621449828147889
Epoch: 804, Batch Gradient Norm: 9.289409715380444
Epoch: 804, Batch Gradient Norm after: 9.289409715380444
Epoch 805/10000, Prediction Accuracy = 59.79%, Loss = 0.555035400390625
Epoch: 805, Batch Gradient Norm: 7.617827288820396
Epoch: 805, Batch Gradient Norm after: 7.617827288820396
Epoch 806/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.5503981232643127
Epoch: 806, Batch Gradient Norm: 8.457556953195333
Epoch: 806, Batch Gradient Norm after: 8.457556953195333
Epoch 807/10000, Prediction Accuracy = 59.75%, Loss = 0.5520103812217713
Epoch: 807, Batch Gradient Norm: 9.799943507569806
Epoch: 807, Batch Gradient Norm after: 9.799943507569806
Epoch 808/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.5547217488288879
Epoch: 808, Batch Gradient Norm: 7.1796254781778694
Epoch: 808, Batch Gradient Norm after: 7.1796254781778694
Epoch 809/10000, Prediction Accuracy = 59.722%, Loss = 0.5473731279373169
Epoch: 809, Batch Gradient Norm: 9.672118752429187
Epoch: 809, Batch Gradient Norm after: 9.672118752429187
Epoch 810/10000, Prediction Accuracy = 59.662%, Loss = 0.5565340757369995
Epoch: 810, Batch Gradient Norm: 7.487896999436162
Epoch: 810, Batch Gradient Norm after: 7.487896999436162
Epoch 811/10000, Prediction Accuracy = 59.7%, Loss = 0.5476807475090026
Epoch: 811, Batch Gradient Norm: 8.543428037350699
Epoch: 811, Batch Gradient Norm after: 8.543428037350699
Epoch 812/10000, Prediction Accuracy = 59.708000000000006%, Loss = 0.5512454032897949
Epoch: 812, Batch Gradient Norm: 6.708854653378652
Epoch: 812, Batch Gradient Norm after: 6.708854653378652
Epoch 813/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.5469718098640441
Epoch: 813, Batch Gradient Norm: 9.123609102714628
Epoch: 813, Batch Gradient Norm after: 9.123609102714628
Epoch 814/10000, Prediction Accuracy = 59.71%, Loss = 0.5497867226600647
Epoch: 814, Batch Gradient Norm: 11.840316641428249
Epoch: 814, Batch Gradient Norm after: 11.840316641428249
Epoch 815/10000, Prediction Accuracy = 59.71999999999999%, Loss = 0.5601141691207886
Epoch: 815, Batch Gradient Norm: 12.257404676524303
Epoch: 815, Batch Gradient Norm after: 12.257404676524303
Epoch 816/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.5621537804603577
Epoch: 816, Batch Gradient Norm: 15.348963196801922
Epoch: 816, Batch Gradient Norm after: 15.348963196801922
Epoch 817/10000, Prediction Accuracy = 59.734%, Loss = 0.5694730401039123
Epoch: 817, Batch Gradient Norm: 12.237040749845015
Epoch: 817, Batch Gradient Norm after: 12.237040749845015
Epoch 818/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.5603943109512329
Epoch: 818, Batch Gradient Norm: 10.827127087982932
Epoch: 818, Batch Gradient Norm after: 10.827127087982932
Epoch 819/10000, Prediction Accuracy = 59.824%, Loss = 0.557201886177063
Epoch: 819, Batch Gradient Norm: 9.547679756210705
Epoch: 819, Batch Gradient Norm after: 9.547679756210705
Epoch 820/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.550529134273529
Epoch: 820, Batch Gradient Norm: 7.92364865428825
Epoch: 820, Batch Gradient Norm after: 7.92364865428825
Epoch 821/10000, Prediction Accuracy = 59.834%, Loss = 0.5476183295249939
Epoch: 821, Batch Gradient Norm: 9.262990049672338
Epoch: 821, Batch Gradient Norm after: 9.262990049672338
Epoch 822/10000, Prediction Accuracy = 59.824%, Loss = 0.5513017416000366
Epoch: 822, Batch Gradient Norm: 7.522478686505584
Epoch: 822, Batch Gradient Norm after: 7.522478686505584
Epoch 823/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.549533748626709
Epoch: 823, Batch Gradient Norm: 8.423843235259993
Epoch: 823, Batch Gradient Norm after: 8.423843235259993
Epoch 824/10000, Prediction Accuracy = 59.788%, Loss = 0.5489150524139405
Epoch: 824, Batch Gradient Norm: 7.763886417235938
Epoch: 824, Batch Gradient Norm after: 7.763886417235938
Epoch 825/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.5481504321098327
Epoch: 825, Batch Gradient Norm: 8.957603092318115
Epoch: 825, Batch Gradient Norm after: 8.957603092318115
Epoch 826/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.5502244114875794
Epoch: 826, Batch Gradient Norm: 8.353486263987179
Epoch: 826, Batch Gradient Norm after: 8.353486263987179
Epoch 827/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.5516156435012818
Epoch: 827, Batch Gradient Norm: 7.657083413727883
Epoch: 827, Batch Gradient Norm after: 7.657083413727883
Epoch 828/10000, Prediction Accuracy = 59.698%, Loss = 0.5465987920761108
Epoch: 828, Batch Gradient Norm: 7.1986984749551635
Epoch: 828, Batch Gradient Norm after: 7.1986984749551635
Epoch 829/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.5456207633018494
Epoch: 829, Batch Gradient Norm: 8.013563056345934
Epoch: 829, Batch Gradient Norm after: 8.013563056345934
Epoch 830/10000, Prediction Accuracy = 59.738%, Loss = 0.5480767011642456
Epoch: 830, Batch Gradient Norm: 8.669983002206893
Epoch: 830, Batch Gradient Norm after: 8.669983002206893
Epoch 831/10000, Prediction Accuracy = 59.67%, Loss = 0.54792160987854
Epoch: 831, Batch Gradient Norm: 10.838763842430637
Epoch: 831, Batch Gradient Norm after: 10.838763842430637
Epoch 832/10000, Prediction Accuracy = 59.778%, Loss = 0.5522237539291381
Epoch: 832, Batch Gradient Norm: 10.56630389102798
Epoch: 832, Batch Gradient Norm after: 10.56630389102798
Epoch 833/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.5521944046020508
Epoch: 833, Batch Gradient Norm: 8.689873392606012
Epoch: 833, Batch Gradient Norm after: 8.689873392606012
Epoch 834/10000, Prediction Accuracy = 59.714%, Loss = 0.5496215939521789
Epoch: 834, Batch Gradient Norm: 8.229963727877127
Epoch: 834, Batch Gradient Norm after: 8.229963727877127
Epoch 835/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.5490748047828674
Epoch: 835, Batch Gradient Norm: 8.847627754880099
Epoch: 835, Batch Gradient Norm after: 8.847627754880099
Epoch 836/10000, Prediction Accuracy = 59.822%, Loss = 0.5495659351348877
Epoch: 836, Batch Gradient Norm: 9.960032540047159
Epoch: 836, Batch Gradient Norm after: 9.960032540047159
Epoch 837/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.5496405720710754
Epoch: 837, Batch Gradient Norm: 8.339262030344665
Epoch: 837, Batch Gradient Norm after: 8.339262030344665
Epoch 838/10000, Prediction Accuracy = 59.77199999999999%, Loss = 0.5465811610221862
Epoch: 838, Batch Gradient Norm: 6.969535390175876
Epoch: 838, Batch Gradient Norm after: 6.969535390175876
Epoch 839/10000, Prediction Accuracy = 59.714%, Loss = 0.5436084747314454
Epoch: 839, Batch Gradient Norm: 6.917769402189804
Epoch: 839, Batch Gradient Norm after: 6.917769402189804
Epoch 840/10000, Prediction Accuracy = 59.698%, Loss = 0.5433971285820007
Epoch: 840, Batch Gradient Norm: 7.355945794627369
Epoch: 840, Batch Gradient Norm after: 7.355945794627369
Epoch 841/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.5471632361412049
Epoch: 841, Batch Gradient Norm: 8.580863136755095
Epoch: 841, Batch Gradient Norm after: 8.580863136755095
Epoch 842/10000, Prediction Accuracy = 59.742%, Loss = 0.5451648116111756
Epoch: 842, Batch Gradient Norm: 9.694164985168873
Epoch: 842, Batch Gradient Norm after: 9.694164985168873
Epoch 843/10000, Prediction Accuracy = 59.7%, Loss = 0.5477841973304749
Epoch: 843, Batch Gradient Norm: 9.437116296440955
Epoch: 843, Batch Gradient Norm after: 9.437116296440955
Epoch 844/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.5487502932548523
Epoch: 844, Batch Gradient Norm: 9.188379334869948
Epoch: 844, Batch Gradient Norm after: 9.188379334869948
Epoch 845/10000, Prediction Accuracy = 59.836%, Loss = 0.5469208240509034
Epoch: 845, Batch Gradient Norm: 8.560270290824056
Epoch: 845, Batch Gradient Norm after: 8.560270290824056
Epoch 846/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.5464025139808655
Epoch: 846, Batch Gradient Norm: 6.837545536949206
Epoch: 846, Batch Gradient Norm after: 6.837545536949206
Epoch 847/10000, Prediction Accuracy = 59.726%, Loss = 0.5445535540580749
Epoch: 847, Batch Gradient Norm: 8.067113787335055
Epoch: 847, Batch Gradient Norm after: 8.067113787335055
Epoch 848/10000, Prediction Accuracy = 59.8%, Loss = 0.5455401778221131
Epoch: 848, Batch Gradient Norm: 8.260391363848507
Epoch: 848, Batch Gradient Norm after: 8.260391363848507
Epoch 849/10000, Prediction Accuracy = 59.794%, Loss = 0.543229877948761
Epoch: 849, Batch Gradient Norm: 8.9004183710757
Epoch: 849, Batch Gradient Norm after: 8.9004183710757
Epoch 850/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.5448060870170593
Epoch: 850, Batch Gradient Norm: 9.645525875665143
Epoch: 850, Batch Gradient Norm after: 9.645525875665143
Epoch 851/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.5472317576408386
Epoch: 851, Batch Gradient Norm: 10.365335564497306
Epoch: 851, Batch Gradient Norm after: 10.365335564497306
Epoch 852/10000, Prediction Accuracy = 59.818%, Loss = 0.5475360035896302
Epoch: 852, Batch Gradient Norm: 9.38985632925131
Epoch: 852, Batch Gradient Norm after: 9.38985632925131
Epoch 853/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.5455511808395386
Epoch: 853, Batch Gradient Norm: 10.066416062711163
Epoch: 853, Batch Gradient Norm after: 10.066416062711163
Epoch 854/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.5456066846847534
Epoch: 854, Batch Gradient Norm: 8.756788513478757
Epoch: 854, Batch Gradient Norm after: 8.756788513478757
Epoch 855/10000, Prediction Accuracy = 59.798%, Loss = 0.543644642829895
Epoch: 855, Batch Gradient Norm: 7.986889810345221
Epoch: 855, Batch Gradient Norm after: 7.986889810345221
Epoch 856/10000, Prediction Accuracy = 59.726%, Loss = 0.5454938769340515
Epoch: 856, Batch Gradient Norm: 9.40462656905621
Epoch: 856, Batch Gradient Norm after: 9.40462656905621
Epoch 857/10000, Prediction Accuracy = 59.86%, Loss = 0.5463373780250549
Epoch: 857, Batch Gradient Norm: 8.714711760256785
Epoch: 857, Batch Gradient Norm after: 8.714711760256785
Epoch 858/10000, Prediction Accuracy = 59.802%, Loss = 0.5431025862693787
Epoch: 858, Batch Gradient Norm: 9.233213788995862
Epoch: 858, Batch Gradient Norm after: 9.233213788995862
Epoch 859/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.5478296041488647
Epoch: 859, Batch Gradient Norm: 7.899295007884662
Epoch: 859, Batch Gradient Norm after: 7.899295007884662
Epoch 860/10000, Prediction Accuracy = 59.864%, Loss = 0.5428500413894654
Epoch: 860, Batch Gradient Norm: 8.628936608336915
Epoch: 860, Batch Gradient Norm after: 8.628936608336915
Epoch 861/10000, Prediction Accuracy = 59.738%, Loss = 0.5446220993995666
Epoch: 861, Batch Gradient Norm: 10.515126135462173
Epoch: 861, Batch Gradient Norm after: 10.515126135462173
Epoch 862/10000, Prediction Accuracy = 59.758%, Loss = 0.5469394564628601
Epoch: 862, Batch Gradient Norm: 12.557785826668953
Epoch: 862, Batch Gradient Norm after: 12.557785826668953
Epoch 863/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.5497313499450683
Epoch: 863, Batch Gradient Norm: 12.988100524205336
Epoch: 863, Batch Gradient Norm after: 12.988100524205336
Epoch 864/10000, Prediction Accuracy = 59.762%, Loss = 0.5565588355064393
Epoch: 864, Batch Gradient Norm: 11.58527094283206
Epoch: 864, Batch Gradient Norm after: 11.58527094283206
Epoch 865/10000, Prediction Accuracy = 59.898%, Loss = 0.5480932474136353
Epoch: 865, Batch Gradient Norm: 9.557348310115179
Epoch: 865, Batch Gradient Norm after: 9.557348310115179
Epoch 866/10000, Prediction Accuracy = 59.884%, Loss = 0.5431398749351501
Epoch: 866, Batch Gradient Norm: 11.355143598097515
Epoch: 866, Batch Gradient Norm after: 11.355143598097515
Epoch 867/10000, Prediction Accuracy = 59.83%, Loss = 0.5497958540916443
Epoch: 867, Batch Gradient Norm: 11.484005188458696
Epoch: 867, Batch Gradient Norm after: 11.484005188458696
Epoch 868/10000, Prediction Accuracy = 59.786%, Loss = 0.5472588896751404
Epoch: 868, Batch Gradient Norm: 12.170580734685599
Epoch: 868, Batch Gradient Norm after: 12.170580734685599
Epoch 869/10000, Prediction Accuracy = 59.914%, Loss = 0.5481610655784607
Epoch: 869, Batch Gradient Norm: 11.634022043270857
Epoch: 869, Batch Gradient Norm after: 11.634022043270857
Epoch 870/10000, Prediction Accuracy = 59.786%, Loss = 0.548012113571167
Epoch: 870, Batch Gradient Norm: 10.827425963122032
Epoch: 870, Batch Gradient Norm after: 10.827425963122032
Epoch 871/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.5452474236488343
Epoch: 871, Batch Gradient Norm: 9.857600126000351
Epoch: 871, Batch Gradient Norm after: 9.857600126000351
Epoch 872/10000, Prediction Accuracy = 59.898%, Loss = 0.5431837916374207
Epoch: 872, Batch Gradient Norm: 9.514217294538943
Epoch: 872, Batch Gradient Norm after: 9.514217294538943
Epoch 873/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.5441228985786438
Epoch: 873, Batch Gradient Norm: 8.499143668524226
Epoch: 873, Batch Gradient Norm after: 8.499143668524226
Epoch 874/10000, Prediction Accuracy = 59.886%, Loss = 0.5390369057655334
Epoch: 874, Batch Gradient Norm: 7.143917459318412
Epoch: 874, Batch Gradient Norm after: 7.143917459318412
Epoch 875/10000, Prediction Accuracy = 59.78000000000001%, Loss = 0.5377127647399902
Epoch: 875, Batch Gradient Norm: 8.15026306421883
Epoch: 875, Batch Gradient Norm after: 8.15026306421883
Epoch 876/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.5417753338813782
Epoch: 876, Batch Gradient Norm: 11.37253033268173
Epoch: 876, Batch Gradient Norm after: 11.37253033268173
Epoch 877/10000, Prediction Accuracy = 59.806%, Loss = 0.5531139731407165
Epoch: 877, Batch Gradient Norm: 9.72656157370472
Epoch: 877, Batch Gradient Norm after: 9.72656157370472
Epoch 878/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.5450586199760437
Epoch: 878, Batch Gradient Norm: 9.03812190609099
Epoch: 878, Batch Gradient Norm after: 9.03812190609099
Epoch 879/10000, Prediction Accuracy = 59.908%, Loss = 0.5427201867103577
Epoch: 879, Batch Gradient Norm: 9.469698271662708
Epoch: 879, Batch Gradient Norm after: 9.469698271662708
Epoch 880/10000, Prediction Accuracy = 59.788%, Loss = 0.5455830216407775
Epoch: 880, Batch Gradient Norm: 7.998084870709275
Epoch: 880, Batch Gradient Norm after: 7.998084870709275
Epoch 881/10000, Prediction Accuracy = 59.822%, Loss = 0.537843132019043
Epoch: 881, Batch Gradient Norm: 8.195002695956523
Epoch: 881, Batch Gradient Norm after: 8.195002695956523
Epoch 882/10000, Prediction Accuracy = 59.834%, Loss = 0.5406357526779175
Epoch: 882, Batch Gradient Norm: 8.983440927905923
Epoch: 882, Batch Gradient Norm after: 8.983440927905923
Epoch 883/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.5416209101676941
Epoch: 883, Batch Gradient Norm: 11.727923700895056
Epoch: 883, Batch Gradient Norm after: 11.727923700895056
Epoch 884/10000, Prediction Accuracy = 59.86400000000001%, Loss = 0.5459685087203979
Epoch: 884, Batch Gradient Norm: 10.843757412475655
Epoch: 884, Batch Gradient Norm after: 10.843757412475655
Epoch 885/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.5437065124511719
Epoch: 885, Batch Gradient Norm: 11.739046025880553
Epoch: 885, Batch Gradient Norm after: 11.739046025880553
Epoch 886/10000, Prediction Accuracy = 59.896%, Loss = 0.5436816930770874
Epoch: 886, Batch Gradient Norm: 11.350514816095165
Epoch: 886, Batch Gradient Norm after: 11.350514816095165
Epoch 887/10000, Prediction Accuracy = 59.788%, Loss = 0.5458642959594726
Epoch: 887, Batch Gradient Norm: 8.418750070535975
Epoch: 887, Batch Gradient Norm after: 8.418750070535975
Epoch 888/10000, Prediction Accuracy = 59.86999999999999%, Loss = 0.5402252078056335
Epoch: 888, Batch Gradient Norm: 9.80163512236786
Epoch: 888, Batch Gradient Norm after: 9.80163512236786
Epoch 889/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.5423790097236634
Epoch: 889, Batch Gradient Norm: 11.74739480361245
Epoch: 889, Batch Gradient Norm after: 11.74739480361245
Epoch 890/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.5513833284378051
Epoch: 890, Batch Gradient Norm: 12.060398494561376
Epoch: 890, Batch Gradient Norm after: 12.060398494561376
Epoch 891/10000, Prediction Accuracy = 59.854%, Loss = 0.5447092890739441
Epoch: 891, Batch Gradient Norm: 7.534905303276351
Epoch: 891, Batch Gradient Norm after: 7.534905303276351
Epoch 892/10000, Prediction Accuracy = 59.758%, Loss = 0.5368818759918212
Epoch: 892, Batch Gradient Norm: 6.839305499156503
Epoch: 892, Batch Gradient Norm after: 6.839305499156503
Epoch 893/10000, Prediction Accuracy = 59.864%, Loss = 0.535794985294342
Epoch: 893, Batch Gradient Norm: 9.709138184675833
Epoch: 893, Batch Gradient Norm after: 9.709138184675833
Epoch 894/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.5404401540756225
Epoch: 894, Batch Gradient Norm: 8.881802828140794
Epoch: 894, Batch Gradient Norm after: 8.881802828140794
Epoch 895/10000, Prediction Accuracy = 59.864%, Loss = 0.5402867913246154
Epoch: 895, Batch Gradient Norm: 7.063393395491155
Epoch: 895, Batch Gradient Norm after: 7.063393395491155
Epoch 896/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.5359402179718018
Epoch: 896, Batch Gradient Norm: 9.360274491418608
Epoch: 896, Batch Gradient Norm after: 9.360274491418608
Epoch 897/10000, Prediction Accuracy = 59.854%, Loss = 0.5433877348899842
Epoch: 897, Batch Gradient Norm: 9.91517738867731
Epoch: 897, Batch Gradient Norm after: 9.91517738867731
Epoch 898/10000, Prediction Accuracy = 59.864%, Loss = 0.5447686791419983
Epoch: 898, Batch Gradient Norm: 9.838178719726402
Epoch: 898, Batch Gradient Norm after: 9.838178719726402
Epoch 899/10000, Prediction Accuracy = 59.896%, Loss = 0.5424913167953491
Epoch: 899, Batch Gradient Norm: 7.448563175387049
Epoch: 899, Batch Gradient Norm after: 7.448563175387049
Epoch 900/10000, Prediction Accuracy = 59.874%, Loss = 0.5343987822532654
Epoch: 900, Batch Gradient Norm: 8.657708683226227
Epoch: 900, Batch Gradient Norm after: 8.657708683226227
Epoch 901/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.5374883055686951
Epoch: 901, Batch Gradient Norm: 10.8865912016729
Epoch: 901, Batch Gradient Norm after: 10.8865912016729
Epoch 902/10000, Prediction Accuracy = 59.902%, Loss = 0.5438531160354614
Epoch: 902, Batch Gradient Norm: 12.80194683784209
Epoch: 902, Batch Gradient Norm after: 12.80194683784209
Epoch 903/10000, Prediction Accuracy = 59.834%, Loss = 0.5457773923873901
Epoch: 903, Batch Gradient Norm: 10.819052294432064
Epoch: 903, Batch Gradient Norm after: 10.819052294432064
Epoch 904/10000, Prediction Accuracy = 59.874%, Loss = 0.541069495677948
Epoch: 904, Batch Gradient Norm: 9.665296272200514
Epoch: 904, Batch Gradient Norm after: 9.665296272200514
Epoch 905/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.5401290535926819
Epoch: 905, Batch Gradient Norm: 9.044571769663804
Epoch: 905, Batch Gradient Norm after: 9.044571769663804
Epoch 906/10000, Prediction Accuracy = 59.876%, Loss = 0.5375722050666809
Epoch: 906, Batch Gradient Norm: 9.899291781860512
Epoch: 906, Batch Gradient Norm after: 9.899291781860512
Epoch 907/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.5407624483108521
Epoch: 907, Batch Gradient Norm: 10.239037531287932
Epoch: 907, Batch Gradient Norm after: 10.239037531287932
Epoch 908/10000, Prediction Accuracy = 59.89%, Loss = 0.5402710556983947
Epoch: 908, Batch Gradient Norm: 9.387919571431967
Epoch: 908, Batch Gradient Norm after: 9.387919571431967
Epoch 909/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.538397741317749
Epoch: 909, Batch Gradient Norm: 9.008360860172251
Epoch: 909, Batch Gradient Norm after: 9.008360860172251
Epoch 910/10000, Prediction Accuracy = 59.964%, Loss = 0.5369307518005371
Epoch: 910, Batch Gradient Norm: 9.674182966394081
Epoch: 910, Batch Gradient Norm after: 9.674182966394081
Epoch 911/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.5395495891571045
Epoch: 911, Batch Gradient Norm: 10.07636038592178
Epoch: 911, Batch Gradient Norm after: 10.07636038592178
Epoch 912/10000, Prediction Accuracy = 59.866%, Loss = 0.5391025424003602
Epoch: 912, Batch Gradient Norm: 8.480720065772859
Epoch: 912, Batch Gradient Norm after: 8.480720065772859
Epoch 913/10000, Prediction Accuracy = 59.938%, Loss = 0.5346674919128418
Epoch: 913, Batch Gradient Norm: 8.89451573814291
Epoch: 913, Batch Gradient Norm after: 8.89451573814291
Epoch 914/10000, Prediction Accuracy = 59.878%, Loss = 0.5371084451675415
Epoch: 914, Batch Gradient Norm: 8.188842438091907
Epoch: 914, Batch Gradient Norm after: 8.188842438091907
Epoch 915/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.5350037336349487
Epoch: 915, Batch Gradient Norm: 8.266468847027669
Epoch: 915, Batch Gradient Norm after: 8.266468847027669
Epoch 916/10000, Prediction Accuracy = 59.932%, Loss = 0.5348349452018738
Epoch: 916, Batch Gradient Norm: 10.310573215344371
Epoch: 916, Batch Gradient Norm after: 10.310573215344371
Epoch 917/10000, Prediction Accuracy = 59.85%, Loss = 0.542330539226532
Epoch: 917, Batch Gradient Norm: 8.799810341265438
Epoch: 917, Batch Gradient Norm after: 8.799810341265438
Epoch 918/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.5353891372680664
Epoch: 918, Batch Gradient Norm: 7.437043581724624
Epoch: 918, Batch Gradient Norm after: 7.437043581724624
Epoch 919/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.53375643491745
Epoch: 919, Batch Gradient Norm: 7.240275007075897
Epoch: 919, Batch Gradient Norm after: 7.240275007075897
Epoch 920/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.5332903742790223
Epoch: 920, Batch Gradient Norm: 7.656565006988188
Epoch: 920, Batch Gradient Norm after: 7.656565006988188
Epoch 921/10000, Prediction Accuracy = 59.85999999999999%, Loss = 0.5337911248207092
Epoch: 921, Batch Gradient Norm: 6.715398155552584
Epoch: 921, Batch Gradient Norm after: 6.715398155552584
Epoch 922/10000, Prediction Accuracy = 59.9%, Loss = 0.5338837265968323
Epoch: 922, Batch Gradient Norm: 7.829966060432152
Epoch: 922, Batch Gradient Norm after: 7.829966060432152
Epoch 923/10000, Prediction Accuracy = 59.852%, Loss = 0.5347365856170654
Epoch: 923, Batch Gradient Norm: 10.908635360440575
Epoch: 923, Batch Gradient Norm after: 10.908635360440575
Epoch 924/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.5420873880386352
Epoch: 924, Batch Gradient Norm: 7.864308118018488
Epoch: 924, Batch Gradient Norm after: 7.864308118018488
Epoch 925/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.5328868865966797
Epoch: 925, Batch Gradient Norm: 7.335844642152934
Epoch: 925, Batch Gradient Norm after: 7.335844642152934
Epoch 926/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.5326444625854492
Epoch: 926, Batch Gradient Norm: 8.932914451495046
Epoch: 926, Batch Gradient Norm after: 8.932914451495046
Epoch 927/10000, Prediction Accuracy = 59.938%, Loss = 0.5356988668441772
Epoch: 927, Batch Gradient Norm: 8.702688660728818
Epoch: 927, Batch Gradient Norm after: 8.702688660728818
Epoch 928/10000, Prediction Accuracy = 59.855999999999995%, Loss = 0.5359182000160218
Epoch: 928, Batch Gradient Norm: 8.187046148894288
Epoch: 928, Batch Gradient Norm after: 8.187046148894288
Epoch 929/10000, Prediction Accuracy = 59.928%, Loss = 0.5346192955970764
Epoch: 929, Batch Gradient Norm: 9.390923588928668
Epoch: 929, Batch Gradient Norm after: 9.390923588928668
Epoch 930/10000, Prediction Accuracy = 59.886%, Loss = 0.5349754929542542
Epoch: 930, Batch Gradient Norm: 8.752072738567657
Epoch: 930, Batch Gradient Norm after: 8.752072738567657
Epoch 931/10000, Prediction Accuracy = 59.89399999999999%, Loss = 0.5344213604927063
Epoch: 931, Batch Gradient Norm: 12.529897193483052
Epoch: 931, Batch Gradient Norm after: 12.529897193483052
Epoch 932/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.5426400661468506
Epoch: 932, Batch Gradient Norm: 11.870463934589633
Epoch: 932, Batch Gradient Norm after: 11.870463934589633
Epoch 933/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.5401922464370728
Epoch: 933, Batch Gradient Norm: 13.222262975846917
Epoch: 933, Batch Gradient Norm after: 13.222262975846917
Epoch 934/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.5446560621261597
Epoch: 934, Batch Gradient Norm: 9.351161608410658
Epoch: 934, Batch Gradient Norm after: 9.351161608410658
Epoch 935/10000, Prediction Accuracy = 60.104%, Loss = 0.535505759716034
Epoch: 935, Batch Gradient Norm: 9.394255866024244
Epoch: 935, Batch Gradient Norm after: 9.394255866024244
Epoch 936/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.5366423964500427
Epoch: 936, Batch Gradient Norm: 11.156234644233171
Epoch: 936, Batch Gradient Norm after: 11.156234644233171
Epoch 937/10000, Prediction Accuracy = 59.886%, Loss = 0.5394741177558899
Epoch: 937, Batch Gradient Norm: 9.447020704873669
Epoch: 937, Batch Gradient Norm after: 9.447020704873669
Epoch 938/10000, Prediction Accuracy = 59.919999999999995%, Loss = 0.5372340321540833
Epoch: 938, Batch Gradient Norm: 8.033043215067687
Epoch: 938, Batch Gradient Norm after: 8.033043215067687
Epoch 939/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.5310548663139343
Epoch: 939, Batch Gradient Norm: 9.360269743653976
Epoch: 939, Batch Gradient Norm after: 9.360269743653976
Epoch 940/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.5353816866874694
Epoch: 940, Batch Gradient Norm: 8.887173667894436
Epoch: 940, Batch Gradient Norm after: 8.887173667894436
Epoch 941/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.5324503540992737
Epoch: 941, Batch Gradient Norm: 10.026543040441597
Epoch: 941, Batch Gradient Norm after: 10.026543040441597
Epoch 942/10000, Prediction Accuracy = 60.012%, Loss = 0.5361954569816589
Epoch: 942, Batch Gradient Norm: 8.532848236183154
Epoch: 942, Batch Gradient Norm after: 8.532848236183154
Epoch 943/10000, Prediction Accuracy = 59.92%, Loss = 0.5319478034973144
Epoch: 943, Batch Gradient Norm: 9.956017099480519
Epoch: 943, Batch Gradient Norm after: 9.956017099480519
Epoch 944/10000, Prediction Accuracy = 59.906000000000006%, Loss = 0.5369701027870178
Epoch: 944, Batch Gradient Norm: 9.397687848281569
Epoch: 944, Batch Gradient Norm after: 9.397687848281569
Epoch 945/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.5345430135726928
Epoch: 945, Batch Gradient Norm: 9.965033063548203
Epoch: 945, Batch Gradient Norm after: 9.965033063548203
Epoch 946/10000, Prediction Accuracy = 59.862%, Loss = 0.5365517616271973
Epoch: 946, Batch Gradient Norm: 8.29128211905134
Epoch: 946, Batch Gradient Norm after: 8.29128211905134
Epoch 947/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.5333573341369628
Epoch: 947, Batch Gradient Norm: 9.998814126133652
Epoch: 947, Batch Gradient Norm after: 9.998814126133652
Epoch 948/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.5343635320663452
Epoch: 948, Batch Gradient Norm: 9.381153526651161
Epoch: 948, Batch Gradient Norm after: 9.381153526651161
Epoch 949/10000, Prediction Accuracy = 59.959999999999994%, Loss = 0.5340309023857117
Epoch: 949, Batch Gradient Norm: 8.752206634874836
Epoch: 949, Batch Gradient Norm after: 8.752206634874836
Epoch 950/10000, Prediction Accuracy = 59.914%, Loss = 0.533788013458252
Epoch: 950, Batch Gradient Norm: 9.16620348654466
Epoch: 950, Batch Gradient Norm after: 9.16620348654466
Epoch 951/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.5356482863426208
Epoch: 951, Batch Gradient Norm: 9.898992381320827
Epoch: 951, Batch Gradient Norm after: 9.898992381320827
Epoch 952/10000, Prediction Accuracy = 59.94000000000001%, Loss = 0.5345863819122314
Epoch: 952, Batch Gradient Norm: 10.340064707485363
Epoch: 952, Batch Gradient Norm after: 10.340064707485363
Epoch 953/10000, Prediction Accuracy = 59.992%, Loss = 0.5369981288909912
Epoch: 953, Batch Gradient Norm: 10.706338639394417
Epoch: 953, Batch Gradient Norm after: 10.706338639394417
Epoch 954/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.5364268779754638
Epoch: 954, Batch Gradient Norm: 8.050601741575129
Epoch: 954, Batch Gradient Norm after: 8.050601741575129
Epoch 955/10000, Prediction Accuracy = 59.98%, Loss = 0.5296915411949158
Epoch: 955, Batch Gradient Norm: 7.995428053484773
Epoch: 955, Batch Gradient Norm after: 7.995428053484773
Epoch 956/10000, Prediction Accuracy = 59.964%, Loss = 0.5314602136611939
Epoch: 956, Batch Gradient Norm: 8.443465787077033
Epoch: 956, Batch Gradient Norm after: 8.443465787077033
Epoch 957/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.5319284319877624
Epoch: 957, Batch Gradient Norm: 11.669774809335763
Epoch: 957, Batch Gradient Norm after: 11.669774809335763
Epoch 958/10000, Prediction Accuracy = 59.96%, Loss = 0.5409693360328675
Epoch: 958, Batch Gradient Norm: 13.02447114448936
Epoch: 958, Batch Gradient Norm after: 13.02447114448936
Epoch 959/10000, Prediction Accuracy = 59.898%, Loss = 0.5411581158638
Epoch: 959, Batch Gradient Norm: 10.828499278351133
Epoch: 959, Batch Gradient Norm after: 10.828499278351133
Epoch 960/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.5349782943725586
Epoch: 960, Batch Gradient Norm: 9.264090836796317
Epoch: 960, Batch Gradient Norm after: 9.264090836796317
Epoch 961/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.5334628701210022
Epoch: 961, Batch Gradient Norm: 11.537814610954955
Epoch: 961, Batch Gradient Norm after: 11.537814610954955
Epoch 962/10000, Prediction Accuracy = 60.0%, Loss = 0.537064504623413
Epoch: 962, Batch Gradient Norm: 11.188157481476471
Epoch: 962, Batch Gradient Norm after: 11.188157481476471
Epoch 963/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.5353860497474671
Epoch: 963, Batch Gradient Norm: 11.533551766637995
Epoch: 963, Batch Gradient Norm after: 11.533551766637995
Epoch 964/10000, Prediction Accuracy = 59.988%, Loss = 0.536906111240387
Epoch: 964, Batch Gradient Norm: 9.3927445467244
Epoch: 964, Batch Gradient Norm after: 9.3927445467244
Epoch 965/10000, Prediction Accuracy = 60.016%, Loss = 0.5319275498390198
Epoch: 965, Batch Gradient Norm: 10.863115714466197
Epoch: 965, Batch Gradient Norm after: 10.863115714466197
Epoch 966/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.5341438531875611
Epoch: 966, Batch Gradient Norm: 10.131589671081974
Epoch: 966, Batch Gradient Norm after: 10.131589671081974
Epoch 967/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.5362348437309266
Epoch: 967, Batch Gradient Norm: 12.247537463331819
Epoch: 967, Batch Gradient Norm after: 12.247537463331819
Epoch 968/10000, Prediction Accuracy = 60.056%, Loss = 0.5411300778388977
Epoch: 968, Batch Gradient Norm: 10.31155323277998
Epoch: 968, Batch Gradient Norm after: 10.31155323277998
Epoch 969/10000, Prediction Accuracy = 59.98%, Loss = 0.5322511672973633
Epoch: 969, Batch Gradient Norm: 11.099470829406624
Epoch: 969, Batch Gradient Norm after: 11.099470829406624
Epoch 970/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.5366859436035156
Epoch: 970, Batch Gradient Norm: 8.89538042844102
Epoch: 970, Batch Gradient Norm after: 8.89538042844102
Epoch 971/10000, Prediction Accuracy = 59.952%, Loss = 0.531035840511322
Epoch: 971, Batch Gradient Norm: 8.752725062847436
Epoch: 971, Batch Gradient Norm after: 8.752725062847436
Epoch 972/10000, Prediction Accuracy = 60.056000000000004%, Loss = 0.5298459768295288
Epoch: 972, Batch Gradient Norm: 6.737165818687941
Epoch: 972, Batch Gradient Norm after: 6.737165818687941
Epoch 973/10000, Prediction Accuracy = 59.944%, Loss = 0.5267064809799195
Epoch: 973, Batch Gradient Norm: 7.921460186761221
Epoch: 973, Batch Gradient Norm after: 7.921460186761221
Epoch 974/10000, Prediction Accuracy = 59.996%, Loss = 0.5292259812355041
Epoch: 974, Batch Gradient Norm: 11.474663560052832
Epoch: 974, Batch Gradient Norm after: 11.474663560052832
Epoch 975/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.5349629878997803
Epoch: 975, Batch Gradient Norm: 12.097297330708997
Epoch: 975, Batch Gradient Norm after: 12.097297330708997
Epoch 976/10000, Prediction Accuracy = 59.88000000000001%, Loss = 0.5329177379608154
Epoch: 976, Batch Gradient Norm: 9.43484304314695
Epoch: 976, Batch Gradient Norm after: 9.43484304314695
Epoch 977/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.5320127844810486
Epoch: 977, Batch Gradient Norm: 11.402933217999301
Epoch: 977, Batch Gradient Norm after: 11.402933217999301
Epoch 978/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.5321429133415222
Epoch: 978, Batch Gradient Norm: 10.718985016760373
Epoch: 978, Batch Gradient Norm after: 10.718985016760373
Epoch 979/10000, Prediction Accuracy = 59.959999999999994%, Loss = 0.5302265286445618
Epoch: 979, Batch Gradient Norm: 9.11648593442022
Epoch: 979, Batch Gradient Norm after: 9.11648593442022
Epoch 980/10000, Prediction Accuracy = 60.024%, Loss = 0.530221962928772
Epoch: 980, Batch Gradient Norm: 10.63926625580318
Epoch: 980, Batch Gradient Norm after: 10.63926625580318
Epoch 981/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.5335418581962585
Epoch: 981, Batch Gradient Norm: 10.109887114276564
Epoch: 981, Batch Gradient Norm after: 10.109887114276564
Epoch 982/10000, Prediction Accuracy = 59.96%, Loss = 0.5362400531768798
Epoch: 982, Batch Gradient Norm: 8.907463760228431
Epoch: 982, Batch Gradient Norm after: 8.907463760228431
Epoch 983/10000, Prediction Accuracy = 59.962%, Loss = 0.5296862006187439
Epoch: 983, Batch Gradient Norm: 5.793547773054407
Epoch: 983, Batch Gradient Norm after: 5.793547773054407
Epoch 984/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.524167275428772
Epoch: 984, Batch Gradient Norm: 8.777266637189427
Epoch: 984, Batch Gradient Norm after: 8.777266637189427
Epoch 985/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.528210186958313
Epoch: 985, Batch Gradient Norm: 8.723069456913409
Epoch: 985, Batch Gradient Norm after: 8.723069456913409
Epoch 986/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.527756929397583
Epoch: 986, Batch Gradient Norm: 9.841335994622902
Epoch: 986, Batch Gradient Norm after: 9.841335994622902
Epoch 987/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.5309560775756836
Epoch: 987, Batch Gradient Norm: 9.777466295422876
Epoch: 987, Batch Gradient Norm after: 9.777466295422876
Epoch 988/10000, Prediction Accuracy = 59.962%, Loss = 0.5318983197212219
Epoch: 988, Batch Gradient Norm: 8.053302842843987
Epoch: 988, Batch Gradient Norm after: 8.053302842843987
Epoch 989/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.5268969178199768
Epoch: 989, Batch Gradient Norm: 8.499826042451957
Epoch: 989, Batch Gradient Norm after: 8.499826042451957
Epoch 990/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.5288178086280823
Epoch: 990, Batch Gradient Norm: 8.112795466245897
Epoch: 990, Batch Gradient Norm after: 8.112795466245897
Epoch 991/10000, Prediction Accuracy = 60.098%, Loss = 0.5264153480529785
Epoch: 991, Batch Gradient Norm: 8.054005335537111
Epoch: 991, Batch Gradient Norm after: 8.054005335537111
Epoch 992/10000, Prediction Accuracy = 59.992%, Loss = 0.5256675958633423
Epoch: 992, Batch Gradient Norm: 7.126535751563814
Epoch: 992, Batch Gradient Norm after: 7.126535751563814
Epoch 993/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.5241281032562256
Epoch: 993, Batch Gradient Norm: 8.472770060044434
Epoch: 993, Batch Gradient Norm after: 8.472770060044434
Epoch 994/10000, Prediction Accuracy = 60.004%, Loss = 0.5265120983123779
Epoch: 994, Batch Gradient Norm: 8.016078192244144
Epoch: 994, Batch Gradient Norm after: 8.016078192244144
Epoch 995/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.5254536390304565
Epoch: 995, Batch Gradient Norm: 8.29269988805766
Epoch: 995, Batch Gradient Norm after: 8.29269988805766
Epoch 996/10000, Prediction Accuracy = 60.06999999999999%, Loss = 0.5257234334945678
Epoch: 996, Batch Gradient Norm: 8.657812417742598
Epoch: 996, Batch Gradient Norm after: 8.657812417742598
Epoch 997/10000, Prediction Accuracy = 60.008%, Loss = 0.5261791110038757
Epoch: 997, Batch Gradient Norm: 8.829000499107035
Epoch: 997, Batch Gradient Norm after: 8.829000499107035
Epoch 998/10000, Prediction Accuracy = 60.064%, Loss = 0.5294711589813232
Epoch: 998, Batch Gradient Norm: 9.04751691619576
Epoch: 998, Batch Gradient Norm after: 9.04751691619576
Epoch 999/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.5301634430885315
Epoch: 999, Batch Gradient Norm: 8.578246914167519
Epoch: 999, Batch Gradient Norm after: 8.578246914167519
Epoch 1000/10000, Prediction Accuracy = 60.08%, Loss = 0.5278263092041016
Epoch: 1000, Batch Gradient Norm: 8.45825159476006
Epoch: 1000, Batch Gradient Norm after: 8.45825159476006
Epoch 1001/10000, Prediction Accuracy = 60.064%, Loss = 0.5256415009498596
Epoch: 1001, Batch Gradient Norm: 10.8290744358897
Epoch: 1001, Batch Gradient Norm after: 10.8290744358897
Epoch 1002/10000, Prediction Accuracy = 60.044%, Loss = 0.5315802693367004
Epoch: 1002, Batch Gradient Norm: 10.56611347290973
Epoch: 1002, Batch Gradient Norm after: 10.56611347290973
Epoch 1003/10000, Prediction Accuracy = 60.04%, Loss = 0.5305794715881348
Epoch: 1003, Batch Gradient Norm: 10.091405144285146
Epoch: 1003, Batch Gradient Norm after: 10.091405144285146
Epoch 1004/10000, Prediction Accuracy = 60.098%, Loss = 0.5299670815467834
Epoch: 1004, Batch Gradient Norm: 10.17052966565218
Epoch: 1004, Batch Gradient Norm after: 10.17052966565218
Epoch 1005/10000, Prediction Accuracy = 60.02%, Loss = 0.5281064629554748
Epoch: 1005, Batch Gradient Norm: 9.591036562500724
Epoch: 1005, Batch Gradient Norm after: 9.591036562500724
Epoch 1006/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.5294443726539612
Epoch: 1006, Batch Gradient Norm: 7.982056642049704
Epoch: 1006, Batch Gradient Norm after: 7.982056642049704
Epoch 1007/10000, Prediction Accuracy = 60.11%, Loss = 0.5238282442092895
Epoch: 1007, Batch Gradient Norm: 8.952594173075479
Epoch: 1007, Batch Gradient Norm after: 8.952594173075479
Epoch 1008/10000, Prediction Accuracy = 60.074%, Loss = 0.5249483823776245
Epoch: 1008, Batch Gradient Norm: 10.47065134214861
Epoch: 1008, Batch Gradient Norm after: 10.47065134214861
Epoch 1009/10000, Prediction Accuracy = 60.038%, Loss = 0.5278379082679748
Epoch: 1009, Batch Gradient Norm: 12.160847819935892
Epoch: 1009, Batch Gradient Norm after: 12.160847819935892
Epoch 1010/10000, Prediction Accuracy = 60.07199999999999%, Loss = 0.5314460754394531
Epoch: 1010, Batch Gradient Norm: 11.140809406957903
Epoch: 1010, Batch Gradient Norm after: 11.140809406957903
Epoch 1011/10000, Prediction Accuracy = 60.072%, Loss = 0.5292784452438355
Epoch: 1011, Batch Gradient Norm: 10.153296201190821
Epoch: 1011, Batch Gradient Norm after: 10.153296201190821
Epoch 1012/10000, Prediction Accuracy = 60.052%, Loss = 0.526294469833374
Epoch: 1012, Batch Gradient Norm: 9.826998465921191
Epoch: 1012, Batch Gradient Norm after: 9.826998465921191
Epoch 1013/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.5266757249832154
Epoch: 1013, Batch Gradient Norm: 12.481492397224823
Epoch: 1013, Batch Gradient Norm after: 12.481492397224823
Epoch 1014/10000, Prediction Accuracy = 60.08%, Loss = 0.5353856205940246
Epoch: 1014, Batch Gradient Norm: 10.920356142355212
Epoch: 1014, Batch Gradient Norm after: 10.920356142355212
Epoch 1015/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.5297641277313232
Epoch: 1015, Batch Gradient Norm: 11.367668210897342
Epoch: 1015, Batch Gradient Norm after: 11.367668210897342
Epoch 1016/10000, Prediction Accuracy = 60.086%, Loss = 0.5303196668624878
Epoch: 1016, Batch Gradient Norm: 10.346531658032516
Epoch: 1016, Batch Gradient Norm after: 10.346531658032516
Epoch 1017/10000, Prediction Accuracy = 60.098%, Loss = 0.5261317014694213
Epoch: 1017, Batch Gradient Norm: 11.136072874457518
Epoch: 1017, Batch Gradient Norm after: 11.136072874457518
Epoch 1018/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.5287994861602783
Epoch: 1018, Batch Gradient Norm: 11.418705746205893
Epoch: 1018, Batch Gradient Norm after: 11.418705746205893
Epoch 1019/10000, Prediction Accuracy = 60.08%, Loss = 0.5339354157447815
Epoch: 1019, Batch Gradient Norm: 9.644905933076087
Epoch: 1019, Batch Gradient Norm after: 9.644905933076087
Epoch 1020/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.5278174996376037
Epoch: 1020, Batch Gradient Norm: 8.500641135536712
Epoch: 1020, Batch Gradient Norm after: 8.500641135536712
Epoch 1021/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5246466040611267
Epoch: 1021, Batch Gradient Norm: 11.534059630400554
Epoch: 1021, Batch Gradient Norm after: 11.534059630400554
Epoch 1022/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.5327595710754395
Epoch: 1022, Batch Gradient Norm: 8.162842953209095
Epoch: 1022, Batch Gradient Norm after: 8.162842953209095
Epoch 1023/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.5238819003105164
Epoch: 1023, Batch Gradient Norm: 9.750726957370746
Epoch: 1023, Batch Gradient Norm after: 9.750726957370746
Epoch 1024/10000, Prediction Accuracy = 60.13599999999999%, Loss = 0.525440263748169
Epoch: 1024, Batch Gradient Norm: 9.342102971839875
Epoch: 1024, Batch Gradient Norm after: 9.342102971839875
Epoch 1025/10000, Prediction Accuracy = 60.028%, Loss = 0.5267232894897461
Epoch: 1025, Batch Gradient Norm: 8.634517922161905
Epoch: 1025, Batch Gradient Norm after: 8.634517922161905
Epoch 1026/10000, Prediction Accuracy = 60.02%, Loss = 0.5270206451416015
Epoch: 1026, Batch Gradient Norm: 9.107728521316897
Epoch: 1026, Batch Gradient Norm after: 9.107728521316897
Epoch 1027/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.5246447563171387
Epoch: 1027, Batch Gradient Norm: 9.033726366284128
Epoch: 1027, Batch Gradient Norm after: 9.033726366284128
Epoch 1028/10000, Prediction Accuracy = 60.102%, Loss = 0.5245604753494263
Epoch: 1028, Batch Gradient Norm: 8.924024926522323
Epoch: 1028, Batch Gradient Norm after: 8.924024926522323
Epoch 1029/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.5240203619003296
Epoch: 1029, Batch Gradient Norm: 9.238368464649591
Epoch: 1029, Batch Gradient Norm after: 9.238368464649591
Epoch 1030/10000, Prediction Accuracy = 60.04%, Loss = 0.5243598341941833
Epoch: 1030, Batch Gradient Norm: 9.152892141160994
Epoch: 1030, Batch Gradient Norm after: 9.152892141160994
Epoch 1031/10000, Prediction Accuracy = 60.06%, Loss = 0.5253228783607483
Epoch: 1031, Batch Gradient Norm: 9.948197075015061
Epoch: 1031, Batch Gradient Norm after: 9.948197075015061
Epoch 1032/10000, Prediction Accuracy = 59.987999999999985%, Loss = 0.5256630063056946
Epoch: 1032, Batch Gradient Norm: 9.848113721993771
Epoch: 1032, Batch Gradient Norm after: 9.848113721993771
Epoch 1033/10000, Prediction Accuracy = 60.02%, Loss = 0.5272558450698852
Epoch: 1033, Batch Gradient Norm: 9.582176454797656
Epoch: 1033, Batch Gradient Norm after: 9.582176454797656
Epoch 1034/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.525270652770996
Epoch: 1034, Batch Gradient Norm: 7.390734301679817
Epoch: 1034, Batch Gradient Norm after: 7.390734301679817
Epoch 1035/10000, Prediction Accuracy = 60.04%, Loss = 0.5204159021377563
Epoch: 1035, Batch Gradient Norm: 9.031141294413827
Epoch: 1035, Batch Gradient Norm after: 9.031141294413827
Epoch 1036/10000, Prediction Accuracy = 60.146%, Loss = 0.522705340385437
Epoch: 1036, Batch Gradient Norm: 10.537368914709033
Epoch: 1036, Batch Gradient Norm after: 10.537368914709033
Epoch 1037/10000, Prediction Accuracy = 60.040000000000006%, Loss = 0.5259455561637878
Epoch: 1037, Batch Gradient Norm: 8.373564432042546
Epoch: 1037, Batch Gradient Norm after: 8.373564432042546
Epoch 1038/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.5209047198295593
Epoch: 1038, Batch Gradient Norm: 9.503016729201542
Epoch: 1038, Batch Gradient Norm after: 9.503016729201542
Epoch 1039/10000, Prediction Accuracy = 60.146%, Loss = 0.5235415697097778
Epoch: 1039, Batch Gradient Norm: 10.230713103432384
Epoch: 1039, Batch Gradient Norm after: 10.230713103432384
Epoch 1040/10000, Prediction Accuracy = 59.996%, Loss = 0.5249949097633362
Epoch: 1040, Batch Gradient Norm: 10.743558696932075
Epoch: 1040, Batch Gradient Norm after: 10.743558696932075
Epoch 1041/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.5245811939239502
Epoch: 1041, Batch Gradient Norm: 11.32671848429543
Epoch: 1041, Batch Gradient Norm after: 11.32671848429543
Epoch 1042/10000, Prediction Accuracy = 60.05%, Loss = 0.526185154914856
Epoch: 1042, Batch Gradient Norm: 12.620189457335636
Epoch: 1042, Batch Gradient Norm after: 12.620189457335636
Epoch 1043/10000, Prediction Accuracy = 60.072%, Loss = 0.5323064923286438
Epoch: 1043, Batch Gradient Norm: 10.664757969260677
Epoch: 1043, Batch Gradient Norm after: 10.664757969260677
Epoch 1044/10000, Prediction Accuracy = 60.098%, Loss = 0.524846363067627
Epoch: 1044, Batch Gradient Norm: 10.832897124398972
Epoch: 1044, Batch Gradient Norm after: 10.832897124398972
Epoch 1045/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.5292483568191528
Epoch: 1045, Batch Gradient Norm: 8.189194849847325
Epoch: 1045, Batch Gradient Norm after: 8.189194849847325
Epoch 1046/10000, Prediction Accuracy = 60.098%, Loss = 0.5204193711280822
Epoch: 1046, Batch Gradient Norm: 9.58509186836489
Epoch: 1046, Batch Gradient Norm after: 9.58509186836489
Epoch 1047/10000, Prediction Accuracy = 60.098%, Loss = 0.5224908471107483
Epoch: 1047, Batch Gradient Norm: 9.96927076928049
Epoch: 1047, Batch Gradient Norm after: 9.96927076928049
Epoch 1048/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.5241424679756165
Epoch: 1048, Batch Gradient Norm: 10.10800212427427
Epoch: 1048, Batch Gradient Norm after: 10.10800212427427
Epoch 1049/10000, Prediction Accuracy = 60.032%, Loss = 0.5224152565002441
Epoch: 1049, Batch Gradient Norm: 9.55891418018008
Epoch: 1049, Batch Gradient Norm after: 9.55891418018008
Epoch 1050/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.5227747678756713
Epoch: 1050, Batch Gradient Norm: 8.751431913009343
Epoch: 1050, Batch Gradient Norm after: 8.751431913009343
Epoch 1051/10000, Prediction Accuracy = 59.976%, Loss = 0.5211991667747498
Epoch: 1051, Batch Gradient Norm: 8.95410578328339
Epoch: 1051, Batch Gradient Norm after: 8.95410578328339
Epoch 1052/10000, Prediction Accuracy = 60.008%, Loss = 0.5222073435783386
Epoch: 1052, Batch Gradient Norm: 6.655245533936858
Epoch: 1052, Batch Gradient Norm after: 6.655245533936858
Epoch 1053/10000, Prediction Accuracy = 60.126%, Loss = 0.517251455783844
Epoch: 1053, Batch Gradient Norm: 8.366058110484058
Epoch: 1053, Batch Gradient Norm after: 8.366058110484058
Epoch 1054/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.5232635498046875
Epoch: 1054, Batch Gradient Norm: 9.993422517118297
Epoch: 1054, Batch Gradient Norm after: 9.993422517118297
Epoch 1055/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.523283326625824
Epoch: 1055, Batch Gradient Norm: 10.841668144876918
Epoch: 1055, Batch Gradient Norm after: 10.841668144876918
Epoch 1056/10000, Prediction Accuracy = 60.124%, Loss = 0.5266006231307984
Epoch: 1056, Batch Gradient Norm: 12.321908742997099
Epoch: 1056, Batch Gradient Norm after: 12.321908742997099
Epoch 1057/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.5306537032127381
Epoch: 1057, Batch Gradient Norm: 10.437264624575695
Epoch: 1057, Batch Gradient Norm after: 10.437264624575695
Epoch 1058/10000, Prediction Accuracy = 60.048%, Loss = 0.524643075466156
Epoch: 1058, Batch Gradient Norm: 10.413799670465878
Epoch: 1058, Batch Gradient Norm after: 10.413799670465878
Epoch 1059/10000, Prediction Accuracy = 60.206%, Loss = 0.5231354713439942
Epoch: 1059, Batch Gradient Norm: 12.4833049039633
Epoch: 1059, Batch Gradient Norm after: 12.4833049039633
Epoch 1060/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.5294049859046936
Epoch: 1060, Batch Gradient Norm: 13.04606179237897
Epoch: 1060, Batch Gradient Norm after: 13.04606179237897
Epoch 1061/10000, Prediction Accuracy = 60.114%, Loss = 0.5295938849449158
Epoch: 1061, Batch Gradient Norm: 11.471966120546819
Epoch: 1061, Batch Gradient Norm after: 11.471966120546819
Epoch 1062/10000, Prediction Accuracy = 60.072%, Loss = 0.5279138803482055
Epoch: 1062, Batch Gradient Norm: 11.349199929178774
Epoch: 1062, Batch Gradient Norm after: 11.349199929178774
Epoch 1063/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.5250763535499573
Epoch: 1063, Batch Gradient Norm: 9.637771444933703
Epoch: 1063, Batch Gradient Norm after: 9.637771444933703
Epoch 1064/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5221928358078003
Epoch: 1064, Batch Gradient Norm: 8.705416108271185
Epoch: 1064, Batch Gradient Norm after: 8.705416108271185
Epoch 1065/10000, Prediction Accuracy = 60.176%, Loss = 0.5205841660499573
Epoch: 1065, Batch Gradient Norm: 10.15060781574491
Epoch: 1065, Batch Gradient Norm after: 10.15060781574491
Epoch 1066/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.523587989807129
Epoch: 1066, Batch Gradient Norm: 6.571832442811639
Epoch: 1066, Batch Gradient Norm after: 6.571832442811639
Epoch 1067/10000, Prediction Accuracy = 60.166%, Loss = 0.5163151502609253
Epoch: 1067, Batch Gradient Norm: 8.39173455637918
Epoch: 1067, Batch Gradient Norm after: 8.39173455637918
Epoch 1068/10000, Prediction Accuracy = 60.222%, Loss = 0.5197365403175354
Epoch: 1068, Batch Gradient Norm: 10.811512252172943
Epoch: 1068, Batch Gradient Norm after: 10.811512252172943
Epoch 1069/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.526016354560852
Epoch: 1069, Batch Gradient Norm: 13.2870666661097
Epoch: 1069, Batch Gradient Norm after: 13.2870666661097
Epoch 1070/10000, Prediction Accuracy = 60.182%, Loss = 0.5318450331687927
Epoch: 1070, Batch Gradient Norm: 11.031771871126494
Epoch: 1070, Batch Gradient Norm after: 11.031771871126494
Epoch 1071/10000, Prediction Accuracy = 60.11%, Loss = 0.5257719278335571
Epoch: 1071, Batch Gradient Norm: 12.083105489263
Epoch: 1071, Batch Gradient Norm after: 12.083105489263
Epoch 1072/10000, Prediction Accuracy = 60.062%, Loss = 0.5308077812194825
Epoch: 1072, Batch Gradient Norm: 12.117326876066995
Epoch: 1072, Batch Gradient Norm after: 12.117326876066995
Epoch 1073/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.527678656578064
Epoch: 1073, Batch Gradient Norm: 9.565425863705764
Epoch: 1073, Batch Gradient Norm after: 9.565425863705764
Epoch 1074/10000, Prediction Accuracy = 60.076%, Loss = 0.5213182449340821
Epoch: 1074, Batch Gradient Norm: 8.422917172787837
Epoch: 1074, Batch Gradient Norm after: 8.422917172787837
Epoch 1075/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.5173620581626892
Epoch: 1075, Batch Gradient Norm: 10.471450625538246
Epoch: 1075, Batch Gradient Norm after: 10.471450625538246
Epoch 1076/10000, Prediction Accuracy = 60.068%, Loss = 0.5219325423240662
Epoch: 1076, Batch Gradient Norm: 11.046532506120634
Epoch: 1076, Batch Gradient Norm after: 11.046532506120634
Epoch 1077/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5225135326385498
Epoch: 1077, Batch Gradient Norm: 11.82111649840686
Epoch: 1077, Batch Gradient Norm after: 11.82111649840686
Epoch 1078/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.5234069585800171
Epoch: 1078, Batch Gradient Norm: 12.520857834445035
Epoch: 1078, Batch Gradient Norm after: 12.520857834445035
Epoch 1079/10000, Prediction Accuracy = 60.048%, Loss = 0.5259234547615051
Epoch: 1079, Batch Gradient Norm: 12.165820732493637
Epoch: 1079, Batch Gradient Norm after: 12.165820732493637
Epoch 1080/10000, Prediction Accuracy = 60.116%, Loss = 0.5253913879394532
Epoch: 1080, Batch Gradient Norm: 13.975368713531896
Epoch: 1080, Batch Gradient Norm after: 13.975368713531896
Epoch 1081/10000, Prediction Accuracy = 60.14%, Loss = 0.5273049712181092
Epoch: 1081, Batch Gradient Norm: 16.017202608960645
Epoch: 1081, Batch Gradient Norm after: 16.017202608960645
Epoch 1082/10000, Prediction Accuracy = 60.112%, Loss = 0.5379710555076599
Epoch: 1082, Batch Gradient Norm: 10.63890657791402
Epoch: 1082, Batch Gradient Norm after: 10.63890657791402
Epoch 1083/10000, Prediction Accuracy = 60.206%, Loss = 0.5220314860343933
Epoch: 1083, Batch Gradient Norm: 10.760133681197624
Epoch: 1083, Batch Gradient Norm after: 10.760133681197624
Epoch 1084/10000, Prediction Accuracy = 60.152%, Loss = 0.5205701589584351
Epoch: 1084, Batch Gradient Norm: 10.50359089031766
Epoch: 1084, Batch Gradient Norm after: 10.50359089031766
Epoch 1085/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.5236726045608521
Epoch: 1085, Batch Gradient Norm: 10.756379273590749
Epoch: 1085, Batch Gradient Norm after: 10.756379273590749
Epoch 1086/10000, Prediction Accuracy = 60.2%, Loss = 0.5239050030708313
Epoch: 1086, Batch Gradient Norm: 9.738301769252732
Epoch: 1086, Batch Gradient Norm after: 9.738301769252732
Epoch 1087/10000, Prediction Accuracy = 60.11%, Loss = 0.5204959511756897
Epoch: 1087, Batch Gradient Norm: 11.391124399408318
Epoch: 1087, Batch Gradient Norm after: 11.391124399408318
Epoch 1088/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.5237181901931762
Epoch: 1088, Batch Gradient Norm: 11.84894335802621
Epoch: 1088, Batch Gradient Norm after: 11.84894335802621
Epoch 1089/10000, Prediction Accuracy = 60.122%, Loss = 0.5245927810668946
Epoch: 1089, Batch Gradient Norm: 12.09590141527694
Epoch: 1089, Batch Gradient Norm after: 12.09590141527694
Epoch 1090/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.5256214380264282
Epoch: 1090, Batch Gradient Norm: 12.376904906824038
Epoch: 1090, Batch Gradient Norm after: 12.376904906824038
Epoch 1091/10000, Prediction Accuracy = 60.134%, Loss = 0.5235322833061218
Epoch: 1091, Batch Gradient Norm: 10.347662930255643
Epoch: 1091, Batch Gradient Norm after: 10.347662930255643
Epoch 1092/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.5205687761306763
Epoch: 1092, Batch Gradient Norm: 7.980815652136947
Epoch: 1092, Batch Gradient Norm after: 7.980815652136947
Epoch 1093/10000, Prediction Accuracy = 60.19%, Loss = 0.5170560359954834
Epoch: 1093, Batch Gradient Norm: 7.325784935424765
Epoch: 1093, Batch Gradient Norm after: 7.325784935424765
Epoch 1094/10000, Prediction Accuracy = 60.11%, Loss = 0.5144024014472961
Epoch: 1094, Batch Gradient Norm: 7.638027922666797
Epoch: 1094, Batch Gradient Norm after: 7.638027922666797
Epoch 1095/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.514145016670227
Epoch: 1095, Batch Gradient Norm: 8.206323506795297
Epoch: 1095, Batch Gradient Norm after: 8.206323506795297
Epoch 1096/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.5156448245048523
Epoch: 1096, Batch Gradient Norm: 6.288008774633276
Epoch: 1096, Batch Gradient Norm after: 6.288008774633276
Epoch 1097/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.5119766235351563
Epoch: 1097, Batch Gradient Norm: 8.311762947958474
Epoch: 1097, Batch Gradient Norm after: 8.311762947958474
Epoch 1098/10000, Prediction Accuracy = 60.178%, Loss = 0.5160614252090454
Epoch: 1098, Batch Gradient Norm: 10.8338838335478
Epoch: 1098, Batch Gradient Norm after: 10.8338838335478
Epoch 1099/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.5216715335845947
Epoch: 1099, Batch Gradient Norm: 10.309586882122234
Epoch: 1099, Batch Gradient Norm after: 10.309586882122234
Epoch 1100/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.5214384317398071
Epoch: 1100, Batch Gradient Norm: 9.865884188317906
Epoch: 1100, Batch Gradient Norm after: 9.865884188317906
Epoch 1101/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.5196116924285888
Epoch: 1101, Batch Gradient Norm: 9.451235808117897
Epoch: 1101, Batch Gradient Norm after: 9.451235808117897
Epoch 1102/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.5179138660430909
Epoch: 1102, Batch Gradient Norm: 8.302253356974298
Epoch: 1102, Batch Gradient Norm after: 8.302253356974298
Epoch 1103/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.5151010155677795
Epoch: 1103, Batch Gradient Norm: 9.412342177058868
Epoch: 1103, Batch Gradient Norm after: 9.412342177058868
Epoch 1104/10000, Prediction Accuracy = 60.218%, Loss = 0.516789174079895
Epoch: 1104, Batch Gradient Norm: 8.64802798611061
Epoch: 1104, Batch Gradient Norm after: 8.64802798611061
Epoch 1105/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.5146610498428345
Epoch: 1105, Batch Gradient Norm: 9.189123795934494
Epoch: 1105, Batch Gradient Norm after: 9.189123795934494
Epoch 1106/10000, Prediction Accuracy = 60.146%, Loss = 0.5194153785705566
Epoch: 1106, Batch Gradient Norm: 6.960007855061295
Epoch: 1106, Batch Gradient Norm after: 6.960007855061295
Epoch 1107/10000, Prediction Accuracy = 60.186%, Loss = 0.5120245695114136
Epoch: 1107, Batch Gradient Norm: 8.644889109536292
Epoch: 1107, Batch Gradient Norm after: 8.644889109536292
Epoch 1108/10000, Prediction Accuracy = 60.072%, Loss = 0.5170169591903686
Epoch: 1108, Batch Gradient Norm: 10.405865517642818
Epoch: 1108, Batch Gradient Norm after: 10.405865517642818
Epoch 1109/10000, Prediction Accuracy = 60.128%, Loss = 0.5181010603904724
Epoch: 1109, Batch Gradient Norm: 10.823415147561558
Epoch: 1109, Batch Gradient Norm after: 10.823415147561558
Epoch 1110/10000, Prediction Accuracy = 60.174%, Loss = 0.520878529548645
Epoch: 1110, Batch Gradient Norm: 9.587199195616872
Epoch: 1110, Batch Gradient Norm after: 9.587199195616872
Epoch 1111/10000, Prediction Accuracy = 60.122%, Loss = 0.5182802319526673
Epoch: 1111, Batch Gradient Norm: 9.225182159930021
Epoch: 1111, Batch Gradient Norm after: 9.225182159930021
Epoch 1112/10000, Prediction Accuracy = 60.23599999999999%, Loss = 0.5148377299308777
Epoch: 1112, Batch Gradient Norm: 8.817904244187602
Epoch: 1112, Batch Gradient Norm after: 8.817904244187602
Epoch 1113/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.5155089259147644
Epoch: 1113, Batch Gradient Norm: 8.665225042366325
Epoch: 1113, Batch Gradient Norm after: 8.665225042366325
Epoch 1114/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.5153509378433228
Epoch: 1114, Batch Gradient Norm: 11.578710599145651
Epoch: 1114, Batch Gradient Norm after: 11.578710599145651
Epoch 1115/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.520356297492981
Epoch: 1115, Batch Gradient Norm: 13.12179078001423
Epoch: 1115, Batch Gradient Norm after: 13.12179078001423
Epoch 1116/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5236839771270752
Epoch: 1116, Batch Gradient Norm: 12.126015735566144
Epoch: 1116, Batch Gradient Norm after: 12.126015735566144
Epoch 1117/10000, Prediction Accuracy = 60.148%, Loss = 0.5197829127311706
Epoch: 1117, Batch Gradient Norm: 13.385927585343515
Epoch: 1117, Batch Gradient Norm after: 13.385927585343515
Epoch 1118/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.5210662961006165
Epoch: 1118, Batch Gradient Norm: 12.15222253395238
Epoch: 1118, Batch Gradient Norm after: 12.15222253395238
Epoch 1119/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.5195872187614441
Epoch: 1119, Batch Gradient Norm: 11.646521473851806
Epoch: 1119, Batch Gradient Norm after: 11.646521473851806
Epoch 1120/10000, Prediction Accuracy = 60.134%, Loss = 0.5184324026107788
Epoch: 1120, Batch Gradient Norm: 11.428742459539004
Epoch: 1120, Batch Gradient Norm after: 11.428742459539004
Epoch 1121/10000, Prediction Accuracy = 60.14%, Loss = 0.5174224376678467
Epoch: 1121, Batch Gradient Norm: 11.410752701145736
Epoch: 1121, Batch Gradient Norm after: 11.410752701145736
Epoch 1122/10000, Prediction Accuracy = 60.134%, Loss = 0.5206053137779236
Epoch: 1122, Batch Gradient Norm: 10.133754492732995
Epoch: 1122, Batch Gradient Norm after: 10.133754492732995
Epoch 1123/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.5143375873565674
Epoch: 1123, Batch Gradient Norm: 8.213634184373142
Epoch: 1123, Batch Gradient Norm after: 8.213634184373142
Epoch 1124/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.5147938966751099
Epoch: 1124, Batch Gradient Norm: 6.956820029984647
Epoch: 1124, Batch Gradient Norm after: 6.956820029984647
Epoch 1125/10000, Prediction Accuracy = 60.17%, Loss = 0.509948992729187
Epoch: 1125, Batch Gradient Norm: 6.84293502212505
Epoch: 1125, Batch Gradient Norm after: 6.84293502212505
Epoch 1126/10000, Prediction Accuracy = 60.076%, Loss = 0.5110282719135284
Epoch: 1126, Batch Gradient Norm: 8.859485785688097
Epoch: 1126, Batch Gradient Norm after: 8.859485785688097
Epoch 1127/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.5147139430046082
Epoch: 1127, Batch Gradient Norm: 11.344394623578838
Epoch: 1127, Batch Gradient Norm after: 11.344394623578838
Epoch 1128/10000, Prediction Accuracy = 60.164%, Loss = 0.5192776560783386
Epoch: 1128, Batch Gradient Norm: 11.198348997614135
Epoch: 1128, Batch Gradient Norm after: 11.198348997614135
Epoch 1129/10000, Prediction Accuracy = 60.19%, Loss = 0.5205843091011048
Epoch: 1129, Batch Gradient Norm: 9.943450433644733
Epoch: 1129, Batch Gradient Norm after: 9.943450433644733
Epoch 1130/10000, Prediction Accuracy = 60.186%, Loss = 0.516003155708313
Epoch: 1130, Batch Gradient Norm: 9.555232876755106
Epoch: 1130, Batch Gradient Norm after: 9.555232876755106
Epoch 1131/10000, Prediction Accuracy = 60.266%, Loss = 0.5161737561225891
Epoch: 1131, Batch Gradient Norm: 10.881799031388196
Epoch: 1131, Batch Gradient Norm after: 10.881799031388196
Epoch 1132/10000, Prediction Accuracy = 60.124%, Loss = 0.5181501269340515
Epoch: 1132, Batch Gradient Norm: 10.988576025197046
Epoch: 1132, Batch Gradient Norm after: 10.988576025197046
Epoch 1133/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.5167009353637695
Epoch: 1133, Batch Gradient Norm: 12.006782876583504
Epoch: 1133, Batch Gradient Norm after: 12.006782876583504
Epoch 1134/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.5219003796577454
Epoch: 1134, Batch Gradient Norm: 9.99403807316795
Epoch: 1134, Batch Gradient Norm after: 9.99403807316795
Epoch 1135/10000, Prediction Accuracy = 60.176%, Loss = 0.5163650989532471
Epoch: 1135, Batch Gradient Norm: 8.847761318254534
Epoch: 1135, Batch Gradient Norm after: 8.847761318254534
Epoch 1136/10000, Prediction Accuracy = 60.19%, Loss = 0.5120230793952942
Epoch: 1136, Batch Gradient Norm: 10.801341207015176
Epoch: 1136, Batch Gradient Norm after: 10.801341207015176
Epoch 1137/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.5156173229217529
Epoch: 1137, Batch Gradient Norm: 10.407434719871306
Epoch: 1137, Batch Gradient Norm after: 10.407434719871306
Epoch 1138/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.5145859837532043
Epoch: 1138, Batch Gradient Norm: 12.543382775128656
Epoch: 1138, Batch Gradient Norm after: 12.543382775128656
Epoch 1139/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.5209170579910278
Epoch: 1139, Batch Gradient Norm: 10.48372359246109
Epoch: 1139, Batch Gradient Norm after: 10.48372359246109
Epoch 1140/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.5148602843284606
Epoch: 1140, Batch Gradient Norm: 9.109715893735094
Epoch: 1140, Batch Gradient Norm after: 9.109715893735094
Epoch 1141/10000, Prediction Accuracy = 60.2%, Loss = 0.5138792634010315
Epoch: 1141, Batch Gradient Norm: 9.662782161001518
Epoch: 1141, Batch Gradient Norm after: 9.662782161001518
Epoch 1142/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5137511372566224
Epoch: 1142, Batch Gradient Norm: 8.57414803516818
Epoch: 1142, Batch Gradient Norm after: 8.57414803516818
Epoch 1143/10000, Prediction Accuracy = 60.246%, Loss = 0.5113926291465759
Epoch: 1143, Batch Gradient Norm: 8.771568020731708
Epoch: 1143, Batch Gradient Norm after: 8.771568020731708
Epoch 1144/10000, Prediction Accuracy = 60.2%, Loss = 0.5112645149230957
Epoch: 1144, Batch Gradient Norm: 8.941861580260154
Epoch: 1144, Batch Gradient Norm after: 8.941861580260154
Epoch 1145/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.5134152173995972
Epoch: 1145, Batch Gradient Norm: 11.193687922518953
Epoch: 1145, Batch Gradient Norm after: 11.193687922518953
Epoch 1146/10000, Prediction Accuracy = 60.19%, Loss = 0.5179606676101685
Epoch: 1146, Batch Gradient Norm: 11.883908716318457
Epoch: 1146, Batch Gradient Norm after: 11.883908716318457
Epoch 1147/10000, Prediction Accuracy = 60.2%, Loss = 0.515825343132019
Epoch: 1147, Batch Gradient Norm: 12.508686382178565
Epoch: 1147, Batch Gradient Norm after: 12.508686382178565
Epoch 1148/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.5180581212043762
Epoch: 1148, Batch Gradient Norm: 8.631104077870688
Epoch: 1148, Batch Gradient Norm after: 8.631104077870688
Epoch 1149/10000, Prediction Accuracy = 60.25%, Loss = 0.5132547199726105
Epoch: 1149, Batch Gradient Norm: 10.369916298712315
Epoch: 1149, Batch Gradient Norm after: 10.369916298712315
Epoch 1150/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.5144576728343964
Epoch: 1150, Batch Gradient Norm: 9.927502636487205
Epoch: 1150, Batch Gradient Norm after: 9.927502636487205
Epoch 1151/10000, Prediction Accuracy = 60.202%, Loss = 0.5123563408851624
Epoch: 1151, Batch Gradient Norm: 9.659507793197633
Epoch: 1151, Batch Gradient Norm after: 9.659507793197633
Epoch 1152/10000, Prediction Accuracy = 60.218%, Loss = 0.5141220688819885
Epoch: 1152, Batch Gradient Norm: 10.418261144573636
Epoch: 1152, Batch Gradient Norm after: 10.418261144573636
Epoch 1153/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.5143062591552734
Epoch: 1153, Batch Gradient Norm: 9.86785089288258
Epoch: 1153, Batch Gradient Norm after: 9.86785089288258
Epoch 1154/10000, Prediction Accuracy = 60.288%, Loss = 0.5135756969451905
Epoch: 1154, Batch Gradient Norm: 12.558356624544345
Epoch: 1154, Batch Gradient Norm after: 12.558356624544345
Epoch 1155/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.522109079360962
Epoch: 1155, Batch Gradient Norm: 11.528625022994245
Epoch: 1155, Batch Gradient Norm after: 11.528625022994245
Epoch 1156/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.5162006855010987
Epoch: 1156, Batch Gradient Norm: 13.923283333754144
Epoch: 1156, Batch Gradient Norm after: 13.923283333754144
Epoch 1157/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.5270348906517028
Epoch: 1157, Batch Gradient Norm: 9.705045688800757
Epoch: 1157, Batch Gradient Norm after: 9.705045688800757
Epoch 1158/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.5144371032714844
Epoch: 1158, Batch Gradient Norm: 10.08520462726615
Epoch: 1158, Batch Gradient Norm after: 10.08520462726615
Epoch 1159/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.5157768547534942
Epoch: 1159, Batch Gradient Norm: 10.988651549473891
Epoch: 1159, Batch Gradient Norm after: 10.988651549473891
Epoch 1160/10000, Prediction Accuracy = 60.198%, Loss = 0.5149042725563049
Epoch: 1160, Batch Gradient Norm: 10.737015685056276
Epoch: 1160, Batch Gradient Norm after: 10.737015685056276
Epoch 1161/10000, Prediction Accuracy = 60.312%, Loss = 0.5144296646118164
Epoch: 1161, Batch Gradient Norm: 9.953460341435175
Epoch: 1161, Batch Gradient Norm after: 9.953460341435175
Epoch 1162/10000, Prediction Accuracy = 60.278%, Loss = 0.5136189699172974
Epoch: 1162, Batch Gradient Norm: 11.460376205435143
Epoch: 1162, Batch Gradient Norm after: 11.460376205435143
Epoch 1163/10000, Prediction Accuracy = 60.238%, Loss = 0.5191676020622253
Epoch: 1163, Batch Gradient Norm: 10.052873453321364
Epoch: 1163, Batch Gradient Norm after: 10.052873453321364
Epoch 1164/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.5115898013114929
Epoch: 1164, Batch Gradient Norm: 10.272878496613528
Epoch: 1164, Batch Gradient Norm after: 10.272878496613528
Epoch 1165/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.511984783411026
Epoch: 1165, Batch Gradient Norm: 8.380191677807977
Epoch: 1165, Batch Gradient Norm after: 8.380191677807977
Epoch 1166/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5115768313407898
Epoch: 1166, Batch Gradient Norm: 7.479061673836415
Epoch: 1166, Batch Gradient Norm after: 7.479061673836415
Epoch 1167/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.5087551951408387
Epoch: 1167, Batch Gradient Norm: 10.295712822355993
Epoch: 1167, Batch Gradient Norm after: 10.295712822355993
Epoch 1168/10000, Prediction Accuracy = 60.202%, Loss = 0.5152868509292603
Epoch: 1168, Batch Gradient Norm: 10.124908690735328
Epoch: 1168, Batch Gradient Norm after: 10.124908690735328
Epoch 1169/10000, Prediction Accuracy = 60.176%, Loss = 0.514943790435791
Epoch: 1169, Batch Gradient Norm: 10.947359357491482
Epoch: 1169, Batch Gradient Norm after: 10.947359357491482
Epoch 1170/10000, Prediction Accuracy = 60.194%, Loss = 0.5129446029663086
Epoch: 1170, Batch Gradient Norm: 10.157559539819124
Epoch: 1170, Batch Gradient Norm after: 10.157559539819124
Epoch 1171/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.5106430113315582
Epoch: 1171, Batch Gradient Norm: 10.809902322629114
Epoch: 1171, Batch Gradient Norm after: 10.809902322629114
Epoch 1172/10000, Prediction Accuracy = 60.238000000000014%, Loss = 0.5142942190170288
Epoch: 1172, Batch Gradient Norm: 11.38991199602353
Epoch: 1172, Batch Gradient Norm after: 11.38991199602353
Epoch 1173/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.5191601157188416
Epoch: 1173, Batch Gradient Norm: 13.537450318916793
Epoch: 1173, Batch Gradient Norm after: 13.537450318916793
Epoch 1174/10000, Prediction Accuracy = 60.212%, Loss = 0.5182567000389099
Epoch: 1174, Batch Gradient Norm: 11.352029444976331
Epoch: 1174, Batch Gradient Norm after: 11.352029444976331
Epoch 1175/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.5188057065010071
Epoch: 1175, Batch Gradient Norm: 10.760764069007061
Epoch: 1175, Batch Gradient Norm after: 10.760764069007061
Epoch 1176/10000, Prediction Accuracy = 60.2%, Loss = 0.5130495190620422
Epoch: 1176, Batch Gradient Norm: 9.894736982507732
Epoch: 1176, Batch Gradient Norm after: 9.894736982507732
Epoch 1177/10000, Prediction Accuracy = 60.188%, Loss = 0.5131666898727417
Epoch: 1177, Batch Gradient Norm: 11.386084295167857
Epoch: 1177, Batch Gradient Norm after: 11.386084295167857
Epoch 1178/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.5187374234199524
Epoch: 1178, Batch Gradient Norm: 12.289622184010897
Epoch: 1178, Batch Gradient Norm after: 12.289622184010897
Epoch 1179/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.5166783571243286
Epoch: 1179, Batch Gradient Norm: 13.760754547270178
Epoch: 1179, Batch Gradient Norm after: 13.760754547270178
Epoch 1180/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.5183845400810242
Epoch: 1180, Batch Gradient Norm: 10.60726690960617
Epoch: 1180, Batch Gradient Norm after: 10.60726690960617
Epoch 1181/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.5122340083122253
Epoch: 1181, Batch Gradient Norm: 12.827379740929963
Epoch: 1181, Batch Gradient Norm after: 12.827379740929963
Epoch 1182/10000, Prediction Accuracy = 60.234%, Loss = 0.5196086645126343
Epoch: 1182, Batch Gradient Norm: 9.590589957340505
Epoch: 1182, Batch Gradient Norm after: 9.590589957340505
Epoch 1183/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.514255702495575
Epoch: 1183, Batch Gradient Norm: 7.628124786723664
Epoch: 1183, Batch Gradient Norm after: 7.628124786723664
Epoch 1184/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.5077967405319214
Epoch: 1184, Batch Gradient Norm: 8.568474983419202
Epoch: 1184, Batch Gradient Norm after: 8.568474983419202
Epoch 1185/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.5079089999198914
Epoch: 1185, Batch Gradient Norm: 8.042565086935292
Epoch: 1185, Batch Gradient Norm after: 8.042565086935292
Epoch 1186/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.5082257390022278
Epoch: 1186, Batch Gradient Norm: 9.074221857219582
Epoch: 1186, Batch Gradient Norm after: 9.074221857219582
Epoch 1187/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.5088971316814422
Epoch: 1187, Batch Gradient Norm: 9.520581670167283
Epoch: 1187, Batch Gradient Norm after: 9.520581670167283
Epoch 1188/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5097191214561463
Epoch: 1188, Batch Gradient Norm: 10.914939098483584
Epoch: 1188, Batch Gradient Norm after: 10.914939098483584
Epoch 1189/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.5118529319763183
Epoch: 1189, Batch Gradient Norm: 9.765731884749087
Epoch: 1189, Batch Gradient Norm after: 9.765731884749087
Epoch 1190/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.509016215801239
Epoch: 1190, Batch Gradient Norm: 11.264038452929343
Epoch: 1190, Batch Gradient Norm after: 11.264038452929343
Epoch 1191/10000, Prediction Accuracy = 60.25%, Loss = 0.5133442044258117
Epoch: 1191, Batch Gradient Norm: 9.79626669802541
Epoch: 1191, Batch Gradient Norm after: 9.79626669802541
Epoch 1192/10000, Prediction Accuracy = 60.2%, Loss = 0.5088935971260071
Epoch: 1192, Batch Gradient Norm: 9.738743259787737
Epoch: 1192, Batch Gradient Norm after: 9.738743259787737
Epoch 1193/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.5106891691684723
Epoch: 1193, Batch Gradient Norm: 10.110163401695228
Epoch: 1193, Batch Gradient Norm after: 10.110163401695228
Epoch 1194/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.5097080111503601
Epoch: 1194, Batch Gradient Norm: 8.330823684248168
Epoch: 1194, Batch Gradient Norm after: 8.330823684248168
Epoch 1195/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.5056511402130127
Epoch: 1195, Batch Gradient Norm: 11.510487704236152
Epoch: 1195, Batch Gradient Norm after: 11.510487704236152
Epoch 1196/10000, Prediction Accuracy = 60.246%, Loss = 0.517178213596344
Epoch: 1196, Batch Gradient Norm: 10.16560355408346
Epoch: 1196, Batch Gradient Norm after: 10.16560355408346
Epoch 1197/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.5096132755279541
Epoch: 1197, Batch Gradient Norm: 9.352300372009914
Epoch: 1197, Batch Gradient Norm after: 9.352300372009914
Epoch 1198/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.5080988883972168
Epoch: 1198, Batch Gradient Norm: 10.195820160654339
Epoch: 1198, Batch Gradient Norm after: 10.195820160654339
Epoch 1199/10000, Prediction Accuracy = 60.254%, Loss = 0.509478759765625
Epoch: 1199, Batch Gradient Norm: 10.145388917966002
Epoch: 1199, Batch Gradient Norm after: 10.145388917966002
Epoch 1200/10000, Prediction Accuracy = 60.15%, Loss = 0.5098771810531616
Epoch: 1200, Batch Gradient Norm: 12.07232647526943
Epoch: 1200, Batch Gradient Norm after: 12.07232647526943
Epoch 1201/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.5155139088630676
Epoch: 1201, Batch Gradient Norm: 10.730106043627037
Epoch: 1201, Batch Gradient Norm after: 10.730106043627037
Epoch 1202/10000, Prediction Accuracy = 60.234%, Loss = 0.5130649089813233
Epoch: 1202, Batch Gradient Norm: 7.894260143906962
Epoch: 1202, Batch Gradient Norm after: 7.894260143906962
Epoch 1203/10000, Prediction Accuracy = 60.294%, Loss = 0.5049849331378937
Epoch: 1203, Batch Gradient Norm: 7.451460372285243
Epoch: 1203, Batch Gradient Norm after: 7.451460372285243
Epoch 1204/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.5042763352394104
Epoch: 1204, Batch Gradient Norm: 9.046947092087503
Epoch: 1204, Batch Gradient Norm after: 9.046947092087503
Epoch 1205/10000, Prediction Accuracy = 60.178%, Loss = 0.5077830135822297
Epoch: 1205, Batch Gradient Norm: 9.438360877881548
Epoch: 1205, Batch Gradient Norm after: 9.438360877881548
Epoch 1206/10000, Prediction Accuracy = 60.278%, Loss = 0.5096248507499694
Epoch: 1206, Batch Gradient Norm: 10.304972606538994
Epoch: 1206, Batch Gradient Norm after: 10.304972606538994
Epoch 1207/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.5093874216079712
Epoch: 1207, Batch Gradient Norm: 11.122929004552926
Epoch: 1207, Batch Gradient Norm after: 11.122929004552926
Epoch 1208/10000, Prediction Accuracy = 60.27%, Loss = 0.5116965174674988
Epoch: 1208, Batch Gradient Norm: 13.676126541033248
Epoch: 1208, Batch Gradient Norm after: 13.676126541033248
Epoch 1209/10000, Prediction Accuracy = 60.174%, Loss = 0.5156395196914673
Epoch: 1209, Batch Gradient Norm: 13.982325985729679
Epoch: 1209, Batch Gradient Norm after: 13.982325985729679
Epoch 1210/10000, Prediction Accuracy = 60.278%, Loss = 0.5156622052192688
Epoch: 1210, Batch Gradient Norm: 12.13783189173032
Epoch: 1210, Batch Gradient Norm after: 12.13783189173032
Epoch 1211/10000, Prediction Accuracy = 60.194%, Loss = 0.5117975533008575
Epoch: 1211, Batch Gradient Norm: 12.078013098498802
Epoch: 1211, Batch Gradient Norm after: 12.078013098498802
Epoch 1212/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.5136131525039673
Epoch: 1212, Batch Gradient Norm: 12.686525176931454
Epoch: 1212, Batch Gradient Norm after: 12.686525176931454
Epoch 1213/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.5121379494667053
Epoch: 1213, Batch Gradient Norm: 11.31405987021933
Epoch: 1213, Batch Gradient Norm after: 11.31405987021933
Epoch 1214/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.5155541658401489
Epoch: 1214, Batch Gradient Norm: 10.259312556508783
Epoch: 1214, Batch Gradient Norm after: 10.259312556508783
Epoch 1215/10000, Prediction Accuracy = 60.318%, Loss = 0.5107632219791413
Epoch: 1215, Batch Gradient Norm: 10.896050784889278
Epoch: 1215, Batch Gradient Norm after: 10.896050784889278
Epoch 1216/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.5096963405609131
Epoch: 1216, Batch Gradient Norm: 10.163699708615614
Epoch: 1216, Batch Gradient Norm after: 10.163699708615614
Epoch 1217/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.5094902992248536
Epoch: 1217, Batch Gradient Norm: 9.658711664244468
Epoch: 1217, Batch Gradient Norm after: 9.658711664244468
Epoch 1218/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.5060969293117523
Epoch: 1218, Batch Gradient Norm: 10.721572217772737
Epoch: 1218, Batch Gradient Norm after: 10.721572217772737
Epoch 1219/10000, Prediction Accuracy = 60.282%, Loss = 0.5127108097076416
Epoch: 1219, Batch Gradient Norm: 9.690488048512256
Epoch: 1219, Batch Gradient Norm after: 9.690488048512256
Epoch 1220/10000, Prediction Accuracy = 60.294%, Loss = 0.5069575607776642
Epoch: 1220, Batch Gradient Norm: 8.882806035299222
Epoch: 1220, Batch Gradient Norm after: 8.882806035299222
Epoch 1221/10000, Prediction Accuracy = 60.288%, Loss = 0.5062672197818756
Epoch: 1221, Batch Gradient Norm: 10.008427743423532
Epoch: 1221, Batch Gradient Norm after: 10.008427743423532
Epoch 1222/10000, Prediction Accuracy = 60.31%, Loss = 0.5071655154228211
Epoch: 1222, Batch Gradient Norm: 12.537719501418648
Epoch: 1222, Batch Gradient Norm after: 12.537719501418648
Epoch 1223/10000, Prediction Accuracy = 60.27%, Loss = 0.5120693027973175
Epoch: 1223, Batch Gradient Norm: 13.138020365541164
Epoch: 1223, Batch Gradient Norm after: 13.138020365541164
Epoch 1224/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.512068122625351
Epoch: 1224, Batch Gradient Norm: 11.295392925727457
Epoch: 1224, Batch Gradient Norm after: 11.295392925727457
Epoch 1225/10000, Prediction Accuracy = 60.226%, Loss = 0.5083284258842469
Epoch: 1225, Batch Gradient Norm: 8.558656062386982
Epoch: 1225, Batch Gradient Norm after: 8.558656062386982
Epoch 1226/10000, Prediction Accuracy = 60.266%, Loss = 0.504204785823822
Epoch: 1226, Batch Gradient Norm: 9.039172364193277
Epoch: 1226, Batch Gradient Norm after: 9.039172364193277
Epoch 1227/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.5042786478996277
Epoch: 1227, Batch Gradient Norm: 7.710075610770542
Epoch: 1227, Batch Gradient Norm after: 7.710075610770542
Epoch 1228/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.5049823224544525
Epoch: 1228, Batch Gradient Norm: 9.786543122911851
Epoch: 1228, Batch Gradient Norm after: 9.786543122911851
Epoch 1229/10000, Prediction Accuracy = 60.21%, Loss = 0.5055833160877228
Epoch: 1229, Batch Gradient Norm: 11.604010353149501
Epoch: 1229, Batch Gradient Norm after: 11.604010353149501
Epoch 1230/10000, Prediction Accuracy = 60.226%, Loss = 0.5087888717651368
Epoch: 1230, Batch Gradient Norm: 12.713720233852623
Epoch: 1230, Batch Gradient Norm after: 12.713720233852623
Epoch 1231/10000, Prediction Accuracy = 60.290000000000006%, Loss = 0.512763237953186
Epoch: 1231, Batch Gradient Norm: 11.713066342064346
Epoch: 1231, Batch Gradient Norm after: 11.713066342064346
Epoch 1232/10000, Prediction Accuracy = 60.28800000000001%, Loss = 0.5109144926071167
Epoch: 1232, Batch Gradient Norm: 12.709426244713457
Epoch: 1232, Batch Gradient Norm after: 12.709426244713457
Epoch 1233/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.5111634016036988
Epoch: 1233, Batch Gradient Norm: 12.858782544193902
Epoch: 1233, Batch Gradient Norm after: 12.858782544193902
Epoch 1234/10000, Prediction Accuracy = 60.336%, Loss = 0.512014365196228
Epoch: 1234, Batch Gradient Norm: 13.713598773503549
Epoch: 1234, Batch Gradient Norm after: 13.713598773503549
Epoch 1235/10000, Prediction Accuracy = 60.288%, Loss = 0.5144738912582397
Epoch: 1235, Batch Gradient Norm: 16.730549404889196
Epoch: 1235, Batch Gradient Norm after: 16.730549404889196
Epoch 1236/10000, Prediction Accuracy = 60.354%, Loss = 0.523453414440155
Epoch: 1236, Batch Gradient Norm: 12.442935117036672
Epoch: 1236, Batch Gradient Norm after: 12.442935117036672
Epoch 1237/10000, Prediction Accuracy = 60.23%, Loss = 0.5133644342422485
Epoch: 1237, Batch Gradient Norm: 10.149368766111143
Epoch: 1237, Batch Gradient Norm after: 10.149368766111143
Epoch 1238/10000, Prediction Accuracy = 60.222%, Loss = 0.5086055874824524
Epoch: 1238, Batch Gradient Norm: 10.836225048523728
Epoch: 1238, Batch Gradient Norm after: 10.836225048523728
Epoch 1239/10000, Prediction Accuracy = 60.25%, Loss = 0.5121562480926514
Epoch: 1239, Batch Gradient Norm: 10.400893395166037
Epoch: 1239, Batch Gradient Norm after: 10.400893395166037
Epoch 1240/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.5069676160812377
Epoch: 1240, Batch Gradient Norm: 10.331647630787257
Epoch: 1240, Batch Gradient Norm after: 10.331647630787257
Epoch 1241/10000, Prediction Accuracy = 60.326%, Loss = 0.5070842802524567
Epoch: 1241, Batch Gradient Norm: 9.687497441636307
Epoch: 1241, Batch Gradient Norm after: 9.687497441636307
Epoch 1242/10000, Prediction Accuracy = 60.364%, Loss = 0.5073356091976166
Epoch: 1242, Batch Gradient Norm: 8.47523755514033
Epoch: 1242, Batch Gradient Norm after: 8.47523755514033
Epoch 1243/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.5028388202190399
Epoch: 1243, Batch Gradient Norm: 10.14842916621542
Epoch: 1243, Batch Gradient Norm after: 10.14842916621542
Epoch 1244/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.5086603403091431
Epoch: 1244, Batch Gradient Norm: 10.367902224540053
Epoch: 1244, Batch Gradient Norm after: 10.367902224540053
Epoch 1245/10000, Prediction Accuracy = 60.342%, Loss = 0.5066914439201355
Epoch: 1245, Batch Gradient Norm: 6.991474809995214
Epoch: 1245, Batch Gradient Norm after: 6.991474809995214
Epoch 1246/10000, Prediction Accuracy = 60.218%, Loss = 0.5009699940681458
Epoch: 1246, Batch Gradient Norm: 11.640658622427495
Epoch: 1246, Batch Gradient Norm after: 11.640658622427495
Epoch 1247/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.512673532962799
Epoch: 1247, Batch Gradient Norm: 9.029696445772279
Epoch: 1247, Batch Gradient Norm after: 9.029696445772279
Epoch 1248/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.5021362900733948
Epoch: 1248, Batch Gradient Norm: 10.016090329131975
Epoch: 1248, Batch Gradient Norm after: 10.016090329131975
Epoch 1249/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.5068277239799499
Epoch: 1249, Batch Gradient Norm: 9.792771765248359
Epoch: 1249, Batch Gradient Norm after: 9.792771765248359
Epoch 1250/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5052033424377441
Epoch: 1250, Batch Gradient Norm: 12.017147474744037
Epoch: 1250, Batch Gradient Norm after: 12.017147474744037
Epoch 1251/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5085144340991974
Epoch: 1251, Batch Gradient Norm: 9.68113516358464
Epoch: 1251, Batch Gradient Norm after: 9.68113516358464
Epoch 1252/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.5044198751449585
Epoch: 1252, Batch Gradient Norm: 10.436408627725054
Epoch: 1252, Batch Gradient Norm after: 10.436408627725054
Epoch 1253/10000, Prediction Accuracy = 60.456%, Loss = 0.5074626922607421
Epoch: 1253, Batch Gradient Norm: 9.557889059509693
Epoch: 1253, Batch Gradient Norm after: 9.557889059509693
Epoch 1254/10000, Prediction Accuracy = 60.372%, Loss = 0.5036799311637878
Epoch: 1254, Batch Gradient Norm: 6.26031236477941
Epoch: 1254, Batch Gradient Norm after: 6.26031236477941
Epoch 1255/10000, Prediction Accuracy = 60.25%, Loss = 0.49853808283805845
Epoch: 1255, Batch Gradient Norm: 8.65847472147096
Epoch: 1255, Batch Gradient Norm after: 8.65847472147096
Epoch 1256/10000, Prediction Accuracy = 60.314%, Loss = 0.5042604207992554
Epoch: 1256, Batch Gradient Norm: 10.038611968398339
Epoch: 1256, Batch Gradient Norm after: 10.038611968398339
Epoch 1257/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.5052014231681824
Epoch: 1257, Batch Gradient Norm: 11.829441699918828
Epoch: 1257, Batch Gradient Norm after: 11.829441699918828
Epoch 1258/10000, Prediction Accuracy = 60.286%, Loss = 0.5069683611392974
Epoch: 1258, Batch Gradient Norm: 10.865959191648628
Epoch: 1258, Batch Gradient Norm after: 10.865959191648628
Epoch 1259/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.509688401222229
Epoch: 1259, Batch Gradient Norm: 8.857398556332518
Epoch: 1259, Batch Gradient Norm after: 8.857398556332518
Epoch 1260/10000, Prediction Accuracy = 60.386%, Loss = 0.5020449817180633
Epoch: 1260, Batch Gradient Norm: 10.724331641844419
Epoch: 1260, Batch Gradient Norm after: 10.724331641844419
Epoch 1261/10000, Prediction Accuracy = 60.258%, Loss = 0.5062875807285309
Epoch: 1261, Batch Gradient Norm: 8.67554207128641
Epoch: 1261, Batch Gradient Norm after: 8.67554207128641
Epoch 1262/10000, Prediction Accuracy = 60.31%, Loss = 0.5025030732154846
Epoch: 1262, Batch Gradient Norm: 9.29393828461677
Epoch: 1262, Batch Gradient Norm after: 9.29393828461677
Epoch 1263/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.5045214116573333
Epoch: 1263, Batch Gradient Norm: 8.378510701414207
Epoch: 1263, Batch Gradient Norm after: 8.378510701414207
Epoch 1264/10000, Prediction Accuracy = 60.24799999999999%, Loss = 0.5025892376899719
Epoch: 1264, Batch Gradient Norm: 10.85256269265166
Epoch: 1264, Batch Gradient Norm after: 10.85256269265166
Epoch 1265/10000, Prediction Accuracy = 60.324%, Loss = 0.5052013039588928
Epoch: 1265, Batch Gradient Norm: 9.660068606226117
Epoch: 1265, Batch Gradient Norm after: 9.660068606226117
Epoch 1266/10000, Prediction Accuracy = 60.27%, Loss = 0.5027721643447876
Epoch: 1266, Batch Gradient Norm: 9.89968166738806
Epoch: 1266, Batch Gradient Norm after: 9.89968166738806
Epoch 1267/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.5016764760017395
Epoch: 1267, Batch Gradient Norm: 7.448564301219167
Epoch: 1267, Batch Gradient Norm after: 7.448564301219167
Epoch 1268/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.4984131395816803
Epoch: 1268, Batch Gradient Norm: 7.202516721591386
Epoch: 1268, Batch Gradient Norm after: 7.202516721591386
Epoch 1269/10000, Prediction Accuracy = 60.352%, Loss = 0.4991344392299652
Epoch: 1269, Batch Gradient Norm: 10.212179741752223
Epoch: 1269, Batch Gradient Norm after: 10.212179741752223
Epoch 1270/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.5048826932907104
Epoch: 1270, Batch Gradient Norm: 11.331852496233422
Epoch: 1270, Batch Gradient Norm after: 11.331852496233422
Epoch 1271/10000, Prediction Accuracy = 60.178%, Loss = 0.5039275586605072
Epoch: 1271, Batch Gradient Norm: 11.422195896747795
Epoch: 1271, Batch Gradient Norm after: 11.422195896747795
Epoch 1272/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.5075661063194274
Epoch: 1272, Batch Gradient Norm: 14.458238611474238
Epoch: 1272, Batch Gradient Norm after: 14.458238611474238
Epoch 1273/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.513701343536377
Epoch: 1273, Batch Gradient Norm: 15.930416462607552
Epoch: 1273, Batch Gradient Norm after: 15.919151140459181
Epoch 1274/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.5146766364574432
Epoch: 1274, Batch Gradient Norm: 13.738519178684026
Epoch: 1274, Batch Gradient Norm after: 13.738519178684026
Epoch 1275/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.508974677324295
Epoch: 1275, Batch Gradient Norm: 11.514085278378415
Epoch: 1275, Batch Gradient Norm after: 11.514085278378415
Epoch 1276/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.5085047006607055
Epoch: 1276, Batch Gradient Norm: 9.751561265810071
Epoch: 1276, Batch Gradient Norm after: 9.751561265810071
Epoch 1277/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.5055054783821106
Epoch: 1277, Batch Gradient Norm: 12.406320209619144
Epoch: 1277, Batch Gradient Norm after: 12.406320209619144
Epoch 1278/10000, Prediction Accuracy = 60.376%, Loss = 0.5080449223518372
Epoch: 1278, Batch Gradient Norm: 9.154716226041725
Epoch: 1278, Batch Gradient Norm after: 9.154716226041725
Epoch 1279/10000, Prediction Accuracy = 60.416%, Loss = 0.5047075152397156
Epoch: 1279, Batch Gradient Norm: 11.702380515046265
Epoch: 1279, Batch Gradient Norm after: 11.702380515046265
Epoch 1280/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5081765949726105
Epoch: 1280, Batch Gradient Norm: 9.696194825720022
Epoch: 1280, Batch Gradient Norm after: 9.696194825720022
Epoch 1281/10000, Prediction Accuracy = 60.386%, Loss = 0.5049112260341644
Epoch: 1281, Batch Gradient Norm: 10.957327042745282
Epoch: 1281, Batch Gradient Norm after: 10.957327042745282
Epoch 1282/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.5083642482757569
Epoch: 1282, Batch Gradient Norm: 10.217724245634345
Epoch: 1282, Batch Gradient Norm after: 10.217724245634345
Epoch 1283/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.505387008190155
Epoch: 1283, Batch Gradient Norm: 8.963004689044475
Epoch: 1283, Batch Gradient Norm after: 8.963004689044475
Epoch 1284/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.5003577172756195
Epoch: 1284, Batch Gradient Norm: 10.239571698564893
Epoch: 1284, Batch Gradient Norm after: 10.239571698564893
Epoch 1285/10000, Prediction Accuracy = 60.458000000000006%, Loss = 0.5055211782455444
Epoch: 1285, Batch Gradient Norm: 13.281376385334005
Epoch: 1285, Batch Gradient Norm after: 13.281376385334005
Epoch 1286/10000, Prediction Accuracy = 60.334%, Loss = 0.5096181869506836
Epoch: 1286, Batch Gradient Norm: 9.684344773503328
Epoch: 1286, Batch Gradient Norm after: 9.684344773503328
Epoch 1287/10000, Prediction Accuracy = 60.42%, Loss = 0.5006431341171265
Epoch: 1287, Batch Gradient Norm: 11.888050761370527
Epoch: 1287, Batch Gradient Norm after: 11.888050761370527
Epoch 1288/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.5050598978996277
Epoch: 1288, Batch Gradient Norm: 10.9415940098384
Epoch: 1288, Batch Gradient Norm after: 10.9415940098384
Epoch 1289/10000, Prediction Accuracy = 60.374%, Loss = 0.5035519540309906
Epoch: 1289, Batch Gradient Norm: 9.987950546500695
Epoch: 1289, Batch Gradient Norm after: 9.987950546500695
Epoch 1290/10000, Prediction Accuracy = 60.384%, Loss = 0.5015595614910126
Epoch: 1290, Batch Gradient Norm: 10.888252630730397
Epoch: 1290, Batch Gradient Norm after: 10.888252630730397
Epoch 1291/10000, Prediction Accuracy = 60.42%, Loss = 0.5036863088607788
Epoch: 1291, Batch Gradient Norm: 9.371435235539584
Epoch: 1291, Batch Gradient Norm after: 9.371435235539584
Epoch 1292/10000, Prediction Accuracy = 60.44599999999999%, Loss = 0.5018671154975891
Epoch: 1292, Batch Gradient Norm: 9.028402779573105
Epoch: 1292, Batch Gradient Norm after: 9.028402779573105
Epoch 1293/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.5015739977359772
Epoch: 1293, Batch Gradient Norm: 12.934978695430203
Epoch: 1293, Batch Gradient Norm after: 12.934978695430203
Epoch 1294/10000, Prediction Accuracy = 60.306%, Loss = 0.5077127337455749
Epoch: 1294, Batch Gradient Norm: 12.411396637163959
Epoch: 1294, Batch Gradient Norm after: 12.411396637163959
Epoch 1295/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.5041538417339325
Epoch: 1295, Batch Gradient Norm: 11.15490901960957
Epoch: 1295, Batch Gradient Norm after: 11.15490901960957
Epoch 1296/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5039115309715271
Epoch: 1296, Batch Gradient Norm: 11.60116182977031
Epoch: 1296, Batch Gradient Norm after: 11.60116182977031
Epoch 1297/10000, Prediction Accuracy = 60.41600000000001%, Loss = 0.5015899956226348
Epoch: 1297, Batch Gradient Norm: 10.54716693723086
Epoch: 1297, Batch Gradient Norm after: 10.54716693723086
Epoch 1298/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.5004407286643981
Epoch: 1298, Batch Gradient Norm: 9.798607776203458
Epoch: 1298, Batch Gradient Norm after: 9.798607776203458
Epoch 1299/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.4987578153610229
Epoch: 1299, Batch Gradient Norm: 10.666281317696841
Epoch: 1299, Batch Gradient Norm after: 10.666281317696841
Epoch 1300/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.5009647309780121
Epoch: 1300, Batch Gradient Norm: 12.220458917865514
Epoch: 1300, Batch Gradient Norm after: 12.220458917865514
Epoch 1301/10000, Prediction Accuracy = 60.376%, Loss = 0.5060426354408264
Epoch: 1301, Batch Gradient Norm: 9.487526937242816
Epoch: 1301, Batch Gradient Norm after: 9.487526937242816
Epoch 1302/10000, Prediction Accuracy = 60.416%, Loss = 0.50043825507164
Epoch: 1302, Batch Gradient Norm: 8.17553529281836
Epoch: 1302, Batch Gradient Norm after: 8.17553529281836
Epoch 1303/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.49742176532745364
Epoch: 1303, Batch Gradient Norm: 9.756932524746013
Epoch: 1303, Batch Gradient Norm after: 9.756932524746013
Epoch 1304/10000, Prediction Accuracy = 60.3%, Loss = 0.5026228189468384
Epoch: 1304, Batch Gradient Norm: 11.911033557974292
Epoch: 1304, Batch Gradient Norm after: 11.911033557974292
Epoch 1305/10000, Prediction Accuracy = 60.402%, Loss = 0.5039564967155457
Epoch: 1305, Batch Gradient Norm: 12.662379858606041
Epoch: 1305, Batch Gradient Norm after: 12.662379858606041
Epoch 1306/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.5065573751926422
Epoch: 1306, Batch Gradient Norm: 12.339833432326326
Epoch: 1306, Batch Gradient Norm after: 12.339833432326326
Epoch 1307/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.5049243032932281
Epoch: 1307, Batch Gradient Norm: 10.56446257173886
Epoch: 1307, Batch Gradient Norm after: 10.56446257173886
Epoch 1308/10000, Prediction Accuracy = 60.374%, Loss = 0.500390762090683
Epoch: 1308, Batch Gradient Norm: 8.974057652548405
Epoch: 1308, Batch Gradient Norm after: 8.974057652548405
Epoch 1309/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.49804463386535647
Epoch: 1309, Batch Gradient Norm: 11.06868226768833
Epoch: 1309, Batch Gradient Norm after: 11.06868226768833
Epoch 1310/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.5029884576797485
Epoch: 1310, Batch Gradient Norm: 12.009716733735928
Epoch: 1310, Batch Gradient Norm after: 12.009716733735928
Epoch 1311/10000, Prediction Accuracy = 60.33%, Loss = 0.5037464380264283
Epoch: 1311, Batch Gradient Norm: 9.106906935275791
Epoch: 1311, Batch Gradient Norm after: 9.106906935275791
Epoch 1312/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.4995641767978668
Epoch: 1312, Batch Gradient Norm: 9.583563812262383
Epoch: 1312, Batch Gradient Norm after: 9.583563812262383
Epoch 1313/10000, Prediction Accuracy = 60.386%, Loss = 0.4968630731105804
Epoch: 1313, Batch Gradient Norm: 8.725043311663669
Epoch: 1313, Batch Gradient Norm after: 8.725043311663669
Epoch 1314/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.49837281703948977
Epoch: 1314, Batch Gradient Norm: 9.108248500603821
Epoch: 1314, Batch Gradient Norm after: 9.108248500603821
Epoch 1315/10000, Prediction Accuracy = 60.36%, Loss = 0.4967539072036743
Epoch: 1315, Batch Gradient Norm: 11.930149567050037
Epoch: 1315, Batch Gradient Norm after: 11.930149567050037
Epoch 1316/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.5034917116165161
Epoch: 1316, Batch Gradient Norm: 12.268255375648712
Epoch: 1316, Batch Gradient Norm after: 12.268255375648712
Epoch 1317/10000, Prediction Accuracy = 60.48%, Loss = 0.5058321833610535
Epoch: 1317, Batch Gradient Norm: 12.913349052396672
Epoch: 1317, Batch Gradient Norm after: 12.913349052396672
Epoch 1318/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.5073941826820374
Epoch: 1318, Batch Gradient Norm: 13.803952700261627
Epoch: 1318, Batch Gradient Norm after: 13.803952700261627
Epoch 1319/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.5072361588478088
Epoch: 1319, Batch Gradient Norm: 11.5327276744182
Epoch: 1319, Batch Gradient Norm after: 11.5327276744182
Epoch 1320/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.5057852804660797
Epoch: 1320, Batch Gradient Norm: 13.45779315673801
Epoch: 1320, Batch Gradient Norm after: 13.45779315673801
Epoch 1321/10000, Prediction Accuracy = 60.364%, Loss = 0.5094850659370422
Epoch: 1321, Batch Gradient Norm: 12.420685107786849
Epoch: 1321, Batch Gradient Norm after: 12.420685107786849
Epoch 1322/10000, Prediction Accuracy = 60.364%, Loss = 0.5041784822940827
Epoch: 1322, Batch Gradient Norm: 11.620089540132073
Epoch: 1322, Batch Gradient Norm after: 11.620089540132073
Epoch 1323/10000, Prediction Accuracy = 60.338%, Loss = 0.503940749168396
Epoch: 1323, Batch Gradient Norm: 13.270237643273312
Epoch: 1323, Batch Gradient Norm after: 13.270237643273312
Epoch 1324/10000, Prediction Accuracy = 60.424%, Loss = 0.5062346935272217
Epoch: 1324, Batch Gradient Norm: 11.690756919065716
Epoch: 1324, Batch Gradient Norm after: 11.690756919065716
Epoch 1325/10000, Prediction Accuracy = 60.488%, Loss = 0.5014739096164703
Epoch: 1325, Batch Gradient Norm: 10.866290109541168
Epoch: 1325, Batch Gradient Norm after: 10.866290109541168
Epoch 1326/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.5006869316101075
Epoch: 1326, Batch Gradient Norm: 8.183248211167601
Epoch: 1326, Batch Gradient Norm after: 8.183248211167601
Epoch 1327/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.4973629415035248
Epoch: 1327, Batch Gradient Norm: 8.38516485765443
Epoch: 1327, Batch Gradient Norm after: 8.38516485765443
Epoch 1328/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.49742738008499143
Epoch: 1328, Batch Gradient Norm: 9.080020158125075
Epoch: 1328, Batch Gradient Norm after: 9.080020158125075
Epoch 1329/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.4976464152336121
Epoch: 1329, Batch Gradient Norm: 8.903454198048873
Epoch: 1329, Batch Gradient Norm after: 8.903454198048873
Epoch 1330/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.4966567814350128
Epoch: 1330, Batch Gradient Norm: 10.306031404920553
Epoch: 1330, Batch Gradient Norm after: 10.306031404920553
Epoch 1331/10000, Prediction Accuracy = 60.396%, Loss = 0.4987139940261841
Epoch: 1331, Batch Gradient Norm: 10.055744138362842
Epoch: 1331, Batch Gradient Norm after: 10.055744138362842
Epoch 1332/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.4979537844657898
Epoch: 1332, Batch Gradient Norm: 9.938636165498039
Epoch: 1332, Batch Gradient Norm after: 9.938636165498039
Epoch 1333/10000, Prediction Accuracy = 60.362%, Loss = 0.49726501703262327
Epoch: 1333, Batch Gradient Norm: 9.964324260062913
Epoch: 1333, Batch Gradient Norm after: 9.964324260062913
Epoch 1334/10000, Prediction Accuracy = 60.388%, Loss = 0.4999641299247742
Epoch: 1334, Batch Gradient Norm: 9.546580159413738
Epoch: 1334, Batch Gradient Norm after: 9.546580159413738
Epoch 1335/10000, Prediction Accuracy = 60.3%, Loss = 0.4979795515537262
Epoch: 1335, Batch Gradient Norm: 11.83317522173457
Epoch: 1335, Batch Gradient Norm after: 11.83317522173457
Epoch 1336/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.501505708694458
Epoch: 1336, Batch Gradient Norm: 9.335297684895098
Epoch: 1336, Batch Gradient Norm after: 9.335297684895098
Epoch 1337/10000, Prediction Accuracy = 60.444%, Loss = 0.49830871224403384
Epoch: 1337, Batch Gradient Norm: 9.568811662791253
Epoch: 1337, Batch Gradient Norm after: 9.568811662791253
Epoch 1338/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.4977083921432495
Epoch: 1338, Batch Gradient Norm: 9.042987841895911
Epoch: 1338, Batch Gradient Norm after: 9.042987841895911
Epoch 1339/10000, Prediction Accuracy = 60.4%, Loss = 0.4960173964500427
Epoch: 1339, Batch Gradient Norm: 10.29736809835574
Epoch: 1339, Batch Gradient Norm after: 10.29736809835574
Epoch 1340/10000, Prediction Accuracy = 60.471999999999994%, Loss = 0.49725654125213625
Epoch: 1340, Batch Gradient Norm: 11.94781238155113
Epoch: 1340, Batch Gradient Norm after: 11.94781238155113
Epoch 1341/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.4996143102645874
Epoch: 1341, Batch Gradient Norm: 10.096423604368633
Epoch: 1341, Batch Gradient Norm after: 10.096423604368633
Epoch 1342/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.49736970067024233
Epoch: 1342, Batch Gradient Norm: 13.494905404051684
Epoch: 1342, Batch Gradient Norm after: 13.494905404051684
Epoch 1343/10000, Prediction Accuracy = 60.324%, Loss = 0.5076553523540497
Epoch: 1343, Batch Gradient Norm: 13.239944826852566
Epoch: 1343, Batch Gradient Norm after: 13.239944826852566
Epoch 1344/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.5083775281906128
Epoch: 1344, Batch Gradient Norm: 15.479540558884445
Epoch: 1344, Batch Gradient Norm after: 15.479540558884445
Epoch 1345/10000, Prediction Accuracy = 60.42%, Loss = 0.5151524364948272
Epoch: 1345, Batch Gradient Norm: 12.956454699932696
Epoch: 1345, Batch Gradient Norm after: 12.956454699932696
Epoch 1346/10000, Prediction Accuracy = 60.42%, Loss = 0.5079474568367004
Epoch: 1346, Batch Gradient Norm: 10.720621393157279
Epoch: 1346, Batch Gradient Norm after: 10.720621393157279
Epoch 1347/10000, Prediction Accuracy = 60.384%, Loss = 0.5008951544761657
Epoch: 1347, Batch Gradient Norm: 11.680528654549384
Epoch: 1347, Batch Gradient Norm after: 11.680528654549384
Epoch 1348/10000, Prediction Accuracy = 60.402%, Loss = 0.5009837210178375
Epoch: 1348, Batch Gradient Norm: 8.4200489335679
Epoch: 1348, Batch Gradient Norm after: 8.4200489335679
Epoch 1349/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.49742105007171633
Epoch: 1349, Batch Gradient Norm: 7.532735104973754
Epoch: 1349, Batch Gradient Norm after: 7.532735104973754
Epoch 1350/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.49541163444519043
Epoch: 1350, Batch Gradient Norm: 8.518286881372395
Epoch: 1350, Batch Gradient Norm after: 8.518286881372395
Epoch 1351/10000, Prediction Accuracy = 60.388%, Loss = 0.4941103935241699
Epoch: 1351, Batch Gradient Norm: 9.659515633027636
Epoch: 1351, Batch Gradient Norm after: 9.659515633027636
Epoch 1352/10000, Prediction Accuracy = 60.492%, Loss = 0.497246116399765
Epoch: 1352, Batch Gradient Norm: 9.688507352925162
Epoch: 1352, Batch Gradient Norm after: 9.688507352925162
Epoch 1353/10000, Prediction Accuracy = 60.44599999999999%, Loss = 0.4940800070762634
Epoch: 1353, Batch Gradient Norm: 9.389050689075237
Epoch: 1353, Batch Gradient Norm after: 9.389050689075237
Epoch 1354/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.493945974111557
Epoch: 1354, Batch Gradient Norm: 12.692844177796623
Epoch: 1354, Batch Gradient Norm after: 12.692844177796623
Epoch 1355/10000, Prediction Accuracy = 60.428%, Loss = 0.5030050277709961
Epoch: 1355, Batch Gradient Norm: 13.44062317652711
Epoch: 1355, Batch Gradient Norm after: 13.44062317652711
Epoch 1356/10000, Prediction Accuracy = 60.56999999999999%, Loss = 0.5024749577045441
Epoch: 1356, Batch Gradient Norm: 12.708053518387365
Epoch: 1356, Batch Gradient Norm after: 12.708053518387365
Epoch 1357/10000, Prediction Accuracy = 60.46999999999999%, Loss = 0.503948175907135
Epoch: 1357, Batch Gradient Norm: 12.361383548020985
Epoch: 1357, Batch Gradient Norm after: 12.361383548020985
Epoch 1358/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.5035459339618683
Epoch: 1358, Batch Gradient Norm: 10.525183439962348
Epoch: 1358, Batch Gradient Norm after: 10.525183439962348
Epoch 1359/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.4974929690361023
Epoch: 1359, Batch Gradient Norm: 9.087174879866625
Epoch: 1359, Batch Gradient Norm after: 9.087174879866625
Epoch 1360/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.4956868886947632
Epoch: 1360, Batch Gradient Norm: 8.074707640714267
Epoch: 1360, Batch Gradient Norm after: 8.074707640714267
Epoch 1361/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.49415263533592224
Epoch: 1361, Batch Gradient Norm: 8.874631250062922
Epoch: 1361, Batch Gradient Norm after: 8.874631250062922
Epoch 1362/10000, Prediction Accuracy = 60.428%, Loss = 0.4946512758731842
Epoch: 1362, Batch Gradient Norm: 11.176224744238926
Epoch: 1362, Batch Gradient Norm after: 11.176224744238926
Epoch 1363/10000, Prediction Accuracy = 60.480000000000004%, Loss = 0.5010371625423431
Epoch: 1363, Batch Gradient Norm: 11.840207797100822
Epoch: 1363, Batch Gradient Norm after: 11.840207797100822
Epoch 1364/10000, Prediction Accuracy = 60.35%, Loss = 0.5016074180603027
Epoch: 1364, Batch Gradient Norm: 10.359766709093396
Epoch: 1364, Batch Gradient Norm after: 10.359766709093396
Epoch 1365/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.4961317002773285
Epoch: 1365, Batch Gradient Norm: 9.638630821972598
Epoch: 1365, Batch Gradient Norm after: 9.638630821972598
Epoch 1366/10000, Prediction Accuracy = 60.426%, Loss = 0.4952897787094116
Epoch: 1366, Batch Gradient Norm: 11.167693687878385
Epoch: 1366, Batch Gradient Norm after: 11.167693687878385
Epoch 1367/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.5000653028488159
Epoch: 1367, Batch Gradient Norm: 9.446484527644586
Epoch: 1367, Batch Gradient Norm after: 9.446484527644586
Epoch 1368/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.4945672571659088
Epoch: 1368, Batch Gradient Norm: 10.587148477517625
Epoch: 1368, Batch Gradient Norm after: 10.587148477517625
Epoch 1369/10000, Prediction Accuracy = 60.507999999999996%, Loss = 0.497375077009201
Epoch: 1369, Batch Gradient Norm: 8.747300153771414
Epoch: 1369, Batch Gradient Norm after: 8.747300153771414
Epoch 1370/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.49345522522926333
Epoch: 1370, Batch Gradient Norm: 10.724581093876742
Epoch: 1370, Batch Gradient Norm after: 10.724581093876742
Epoch 1371/10000, Prediction Accuracy = 60.44199999999999%, Loss = 0.49928308725357057
Epoch: 1371, Batch Gradient Norm: 12.00009658989218
Epoch: 1371, Batch Gradient Norm after: 12.00009658989218
Epoch 1372/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.5003136932849884
Epoch: 1372, Batch Gradient Norm: 10.532706216662783
Epoch: 1372, Batch Gradient Norm after: 10.532706216662783
Epoch 1373/10000, Prediction Accuracy = 60.436%, Loss = 0.4955652117729187
Epoch: 1373, Batch Gradient Norm: 11.4864052319358
Epoch: 1373, Batch Gradient Norm after: 11.4864052319358
Epoch 1374/10000, Prediction Accuracy = 60.472%, Loss = 0.4996386289596558
Epoch: 1374, Batch Gradient Norm: 12.701899108950279
Epoch: 1374, Batch Gradient Norm after: 12.701899108950279
Epoch 1375/10000, Prediction Accuracy = 60.504%, Loss = 0.5006673157215118
Epoch: 1375, Batch Gradient Norm: 11.171248543882546
Epoch: 1375, Batch Gradient Norm after: 11.171248543882546
Epoch 1376/10000, Prediction Accuracy = 60.462%, Loss = 0.49872761964797974
Epoch: 1376, Batch Gradient Norm: 8.885510360149482
Epoch: 1376, Batch Gradient Norm after: 8.885510360149482
Epoch 1377/10000, Prediction Accuracy = 60.424%, Loss = 0.4954634964466095
Epoch: 1377, Batch Gradient Norm: 9.6987883266732
Epoch: 1377, Batch Gradient Norm after: 9.6987883266732
Epoch 1378/10000, Prediction Accuracy = 60.42%, Loss = 0.4948360025882721
Epoch: 1378, Batch Gradient Norm: 9.045777994468265
Epoch: 1378, Batch Gradient Norm after: 9.045777994468265
Epoch 1379/10000, Prediction Accuracy = 60.50600000000001%, Loss = 0.49274239540100095
Epoch: 1379, Batch Gradient Norm: 11.048792989590583
Epoch: 1379, Batch Gradient Norm after: 11.048792989590583
Epoch 1380/10000, Prediction Accuracy = 60.452%, Loss = 0.49767565727233887
Epoch: 1380, Batch Gradient Norm: 11.272919710985526
Epoch: 1380, Batch Gradient Norm after: 11.272919710985526
Epoch 1381/10000, Prediction Accuracy = 60.462%, Loss = 0.49659664630889894
Epoch: 1381, Batch Gradient Norm: 10.59658354602025
Epoch: 1381, Batch Gradient Norm after: 10.59658354602025
Epoch 1382/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.49631606340408324
Epoch: 1382, Batch Gradient Norm: 10.41581100475611
Epoch: 1382, Batch Gradient Norm after: 10.41581100475611
Epoch 1383/10000, Prediction Accuracy = 60.465999999999994%, Loss = 0.49704834818840027
Epoch: 1383, Batch Gradient Norm: 9.645214568870767
Epoch: 1383, Batch Gradient Norm after: 9.645214568870767
Epoch 1384/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.49531313180923464
Epoch: 1384, Batch Gradient Norm: 7.905229641646483
Epoch: 1384, Batch Gradient Norm after: 7.905229641646483
Epoch 1385/10000, Prediction Accuracy = 60.512%, Loss = 0.4910352826118469
Epoch: 1385, Batch Gradient Norm: 8.12745580608782
Epoch: 1385, Batch Gradient Norm after: 8.12745580608782
Epoch 1386/10000, Prediction Accuracy = 60.444%, Loss = 0.49341326355934145
Epoch: 1386, Batch Gradient Norm: 9.337361624493495
Epoch: 1386, Batch Gradient Norm after: 9.337361624493495
Epoch 1387/10000, Prediction Accuracy = 60.408%, Loss = 0.49430757761001587
Epoch: 1387, Batch Gradient Norm: 9.579023181803553
Epoch: 1387, Batch Gradient Norm after: 9.579023181803553
Epoch 1388/10000, Prediction Accuracy = 60.510000000000005%, Loss = 0.4936153948307037
Epoch: 1388, Batch Gradient Norm: 8.652428713180717
Epoch: 1388, Batch Gradient Norm after: 8.652428713180717
Epoch 1389/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.49109724164009094
Epoch: 1389, Batch Gradient Norm: 11.14743332473008
Epoch: 1389, Batch Gradient Norm after: 11.14743332473008
Epoch 1390/10000, Prediction Accuracy = 60.496%, Loss = 0.49917747974395754
Epoch: 1390, Batch Gradient Norm: 12.015403753625195
Epoch: 1390, Batch Gradient Norm after: 12.015403753625195
Epoch 1391/10000, Prediction Accuracy = 60.414%, Loss = 0.500145435333252
Epoch: 1391, Batch Gradient Norm: 12.137734306967848
Epoch: 1391, Batch Gradient Norm after: 12.137734306967848
Epoch 1392/10000, Prediction Accuracy = 60.438%, Loss = 0.49920843839645385
Epoch: 1392, Batch Gradient Norm: 14.238932162482339
Epoch: 1392, Batch Gradient Norm after: 14.238932162482339
Epoch 1393/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5067421317100524
Epoch: 1393, Batch Gradient Norm: 12.209187029527342
Epoch: 1393, Batch Gradient Norm after: 12.209187029527342
Epoch 1394/10000, Prediction Accuracy = 60.33%, Loss = 0.498676735162735
Epoch: 1394, Batch Gradient Norm: 10.982171368652626
Epoch: 1394, Batch Gradient Norm after: 10.982171368652626
Epoch 1395/10000, Prediction Accuracy = 60.456%, Loss = 0.49893989562988283
Epoch: 1395, Batch Gradient Norm: 11.10628398743405
Epoch: 1395, Batch Gradient Norm after: 11.10628398743405
Epoch 1396/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.49624834060668943
Epoch: 1396, Batch Gradient Norm: 10.001913520439972
Epoch: 1396, Batch Gradient Norm after: 10.001913520439972
Epoch 1397/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.4940214276313782
Epoch: 1397, Batch Gradient Norm: 7.243665143064155
Epoch: 1397, Batch Gradient Norm after: 7.243665143064155
Epoch 1398/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.4905682623386383
Epoch: 1398, Batch Gradient Norm: 8.78570123599688
Epoch: 1398, Batch Gradient Norm after: 8.78570123599688
Epoch 1399/10000, Prediction Accuracy = 60.49399999999999%, Loss = 0.49330662488937377
Epoch: 1399, Batch Gradient Norm: 11.55279635622571
Epoch: 1399, Batch Gradient Norm after: 11.55279635622571
Epoch 1400/10000, Prediction Accuracy = 60.519999999999996%, Loss = 0.49783387780189514
Epoch: 1400, Batch Gradient Norm: 8.430649958859536
Epoch: 1400, Batch Gradient Norm after: 8.430649958859536
Epoch 1401/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.4910447418689728
Epoch: 1401, Batch Gradient Norm: 8.851616137900809
Epoch: 1401, Batch Gradient Norm after: 8.851616137900809
Epoch 1402/10000, Prediction Accuracy = 60.462%, Loss = 0.49362740516662595
Epoch: 1402, Batch Gradient Norm: 8.059378954146197
Epoch: 1402, Batch Gradient Norm after: 8.059378954146197
Epoch 1403/10000, Prediction Accuracy = 60.324%, Loss = 0.4917608439922333
Epoch: 1403, Batch Gradient Norm: 9.33082245874935
Epoch: 1403, Batch Gradient Norm after: 9.33082245874935
Epoch 1404/10000, Prediction Accuracy = 60.46999999999999%, Loss = 0.49255767464637756
Epoch: 1404, Batch Gradient Norm: 9.535659966321747
Epoch: 1404, Batch Gradient Norm after: 9.535659966321747
Epoch 1405/10000, Prediction Accuracy = 60.39%, Loss = 0.49352619647979734
Epoch: 1405, Batch Gradient Norm: 9.0895190979983
Epoch: 1405, Batch Gradient Norm after: 9.0895190979983
Epoch 1406/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.49037861824035645
Epoch: 1406, Batch Gradient Norm: 12.54622933022879
Epoch: 1406, Batch Gradient Norm after: 12.54622933022879
Epoch 1407/10000, Prediction Accuracy = 60.448%, Loss = 0.49613085985183714
Epoch: 1407, Batch Gradient Norm: 11.500223203040239
Epoch: 1407, Batch Gradient Norm after: 11.500223203040239
Epoch 1408/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.4967054784297943
Epoch: 1408, Batch Gradient Norm: 13.416025281017868
Epoch: 1408, Batch Gradient Norm after: 13.416025281017868
Epoch 1409/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.49920222759246824
Epoch: 1409, Batch Gradient Norm: 14.571679524286967
Epoch: 1409, Batch Gradient Norm after: 14.571679524286967
Epoch 1410/10000, Prediction Accuracy = 60.426%, Loss = 0.49999473094940183
Epoch: 1410, Batch Gradient Norm: 12.629165340324896
Epoch: 1410, Batch Gradient Norm after: 12.629165340324896
Epoch 1411/10000, Prediction Accuracy = 60.504%, Loss = 0.49746323227882383
Epoch: 1411, Batch Gradient Norm: 10.22042668785318
Epoch: 1411, Batch Gradient Norm after: 10.22042668785318
Epoch 1412/10000, Prediction Accuracy = 60.402%, Loss = 0.4932527780532837
Epoch: 1412, Batch Gradient Norm: 9.796809431272797
Epoch: 1412, Batch Gradient Norm after: 9.796809431272797
Epoch 1413/10000, Prediction Accuracy = 60.476%, Loss = 0.4913226842880249
Epoch: 1413, Batch Gradient Norm: 8.960751952293458
Epoch: 1413, Batch Gradient Norm after: 8.960751952293458
Epoch 1414/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.49057504534721375
Epoch: 1414, Batch Gradient Norm: 11.256306548739584
Epoch: 1414, Batch Gradient Norm after: 11.256306548739584
Epoch 1415/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.4965758502483368
Epoch: 1415, Batch Gradient Norm: 9.676302895831364
Epoch: 1415, Batch Gradient Norm after: 9.676302895831364
Epoch 1416/10000, Prediction Accuracy = 60.574%, Loss = 0.49104316234588624
Epoch: 1416, Batch Gradient Norm: 11.923758304459094
Epoch: 1416, Batch Gradient Norm after: 11.923758304459094
Epoch 1417/10000, Prediction Accuracy = 60.464%, Loss = 0.4969831705093384
Epoch: 1417, Batch Gradient Norm: 10.656725434854927
Epoch: 1417, Batch Gradient Norm after: 10.656725434854927
Epoch 1418/10000, Prediction Accuracy = 60.36%, Loss = 0.4920951008796692
Epoch: 1418, Batch Gradient Norm: 9.759917941819202
Epoch: 1418, Batch Gradient Norm after: 9.759917941819202
Epoch 1419/10000, Prediction Accuracy = 60.528000000000006%, Loss = 0.4927447438240051
Epoch: 1419, Batch Gradient Norm: 9.558605704791841
Epoch: 1419, Batch Gradient Norm after: 9.558605704791841
Epoch 1420/10000, Prediction Accuracy = 60.54600000000001%, Loss = 0.491548889875412
Epoch: 1420, Batch Gradient Norm: 10.127313235612453
Epoch: 1420, Batch Gradient Norm after: 10.127313235612453
Epoch 1421/10000, Prediction Accuracy = 60.43000000000001%, Loss = 0.4937549889087677
Epoch: 1421, Batch Gradient Norm: 7.4658247590450895
Epoch: 1421, Batch Gradient Norm after: 7.4658247590450895
Epoch 1422/10000, Prediction Accuracy = 60.45799999999999%, Loss = 0.4885538101196289
Epoch: 1422, Batch Gradient Norm: 8.957273531273717
Epoch: 1422, Batch Gradient Norm after: 8.957273531273717
Epoch 1423/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.4898534119129181
Epoch: 1423, Batch Gradient Norm: 8.841211833076997
Epoch: 1423, Batch Gradient Norm after: 8.841211833076997
Epoch 1424/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.48952178955078124
Epoch: 1424, Batch Gradient Norm: 9.792739772205152
Epoch: 1424, Batch Gradient Norm after: 9.792739772205152
Epoch 1425/10000, Prediction Accuracy = 60.44%, Loss = 0.4920660614967346
Epoch: 1425, Batch Gradient Norm: 10.066799337003822
Epoch: 1425, Batch Gradient Norm after: 10.066799337003822
Epoch 1426/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.492362904548645
Epoch: 1426, Batch Gradient Norm: 10.560108891722717
Epoch: 1426, Batch Gradient Norm after: 10.560108891722717
Epoch 1427/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.49287002086639403
Epoch: 1427, Batch Gradient Norm: 8.173924087790002
Epoch: 1427, Batch Gradient Norm after: 8.173924087790002
Epoch 1428/10000, Prediction Accuracy = 60.525999999999996%, Loss = 0.4886856973171234
Epoch: 1428, Batch Gradient Norm: 8.739553179278142
Epoch: 1428, Batch Gradient Norm after: 8.739553179278142
Epoch 1429/10000, Prediction Accuracy = 60.464%, Loss = 0.4891825675964355
Epoch: 1429, Batch Gradient Norm: 10.259789139697947
Epoch: 1429, Batch Gradient Norm after: 10.259789139697947
Epoch 1430/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.4938529312610626
Epoch: 1430, Batch Gradient Norm: 9.113023926821477
Epoch: 1430, Batch Gradient Norm after: 9.113023926821477
Epoch 1431/10000, Prediction Accuracy = 60.458000000000006%, Loss = 0.4895907461643219
Epoch: 1431, Batch Gradient Norm: 9.162876387072542
Epoch: 1431, Batch Gradient Norm after: 9.162876387072542
Epoch 1432/10000, Prediction Accuracy = 60.492%, Loss = 0.48942150473594664
Epoch: 1432, Batch Gradient Norm: 10.353842139537791
Epoch: 1432, Batch Gradient Norm after: 10.353842139537791
Epoch 1433/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.4921336770057678
Epoch: 1433, Batch Gradient Norm: 12.271338870365385
Epoch: 1433, Batch Gradient Norm after: 12.271338870365385
Epoch 1434/10000, Prediction Accuracy = 60.584%, Loss = 0.4957012116909027
Epoch: 1434, Batch Gradient Norm: 11.61271074595345
Epoch: 1434, Batch Gradient Norm after: 11.61271074595345
Epoch 1435/10000, Prediction Accuracy = 60.448%, Loss = 0.4941164553165436
Epoch: 1435, Batch Gradient Norm: 11.439621014883885
Epoch: 1435, Batch Gradient Norm after: 11.439621014883885
Epoch 1436/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.494930624961853
Epoch: 1436, Batch Gradient Norm: 10.429499492695369
Epoch: 1436, Batch Gradient Norm after: 10.429499492695369
Epoch 1437/10000, Prediction Accuracy = 60.486000000000004%, Loss = 0.49111708998680115
Epoch: 1437, Batch Gradient Norm: 8.476010126499467
Epoch: 1437, Batch Gradient Norm after: 8.476010126499467
Epoch 1438/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.4883104622364044
Epoch: 1438, Batch Gradient Norm: 8.963615407701873
Epoch: 1438, Batch Gradient Norm after: 8.963615407701873
Epoch 1439/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.488547545671463
Epoch: 1439, Batch Gradient Norm: 9.803934596097948
Epoch: 1439, Batch Gradient Norm after: 9.803934596097948
Epoch 1440/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.4906285524368286
Epoch: 1440, Batch Gradient Norm: 10.011922938173656
Epoch: 1440, Batch Gradient Norm after: 10.011922938173656
Epoch 1441/10000, Prediction Accuracy = 60.5%, Loss = 0.4902205765247345
Epoch: 1441, Batch Gradient Norm: 10.790931371042698
Epoch: 1441, Batch Gradient Norm after: 10.790931371042698
Epoch 1442/10000, Prediction Accuracy = 60.483999999999995%, Loss = 0.4914725124835968
Epoch: 1442, Batch Gradient Norm: 10.957826924478073
Epoch: 1442, Batch Gradient Norm after: 10.957826924478073
Epoch 1443/10000, Prediction Accuracy = 60.476%, Loss = 0.49190202355384827
Epoch: 1443, Batch Gradient Norm: 11.580426096619778
Epoch: 1443, Batch Gradient Norm after: 11.580426096619778
Epoch 1444/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.4919881522655487
Epoch: 1444, Batch Gradient Norm: 11.675986632259447
Epoch: 1444, Batch Gradient Norm after: 11.675986632259447
Epoch 1445/10000, Prediction Accuracy = 60.512%, Loss = 0.4947122514247894
Epoch: 1445, Batch Gradient Norm: 13.257088425040326
Epoch: 1445, Batch Gradient Norm after: 13.257088425040326
Epoch 1446/10000, Prediction Accuracy = 60.462%, Loss = 0.4976267874240875
Epoch: 1446, Batch Gradient Norm: 11.854715824035447
Epoch: 1446, Batch Gradient Norm after: 11.854715824035447
Epoch 1447/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.49517815113067626
Epoch: 1447, Batch Gradient Norm: 9.56343183678724
Epoch: 1447, Batch Gradient Norm after: 9.56343183678724
Epoch 1448/10000, Prediction Accuracy = 60.414%, Loss = 0.4916392147541046
Epoch: 1448, Batch Gradient Norm: 10.447354822285947
Epoch: 1448, Batch Gradient Norm after: 10.447354822285947
Epoch 1449/10000, Prediction Accuracy = 60.426%, Loss = 0.4921172082424164
Epoch: 1449, Batch Gradient Norm: 10.300837541735536
Epoch: 1449, Batch Gradient Norm after: 10.300837541735536
Epoch 1450/10000, Prediction Accuracy = 60.444%, Loss = 0.4894110500812531
Epoch: 1450, Batch Gradient Norm: 9.955065923390151
Epoch: 1450, Batch Gradient Norm after: 9.955065923390151
Epoch 1451/10000, Prediction Accuracy = 60.488%, Loss = 0.4889157235622406
Epoch: 1451, Batch Gradient Norm: 13.209567845516908
Epoch: 1451, Batch Gradient Norm after: 13.209567845516908
Epoch 1452/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.4960780739784241
Epoch: 1452, Batch Gradient Norm: 11.483503101456517
Epoch: 1452, Batch Gradient Norm after: 11.483503101456517
Epoch 1453/10000, Prediction Accuracy = 60.541999999999994%, Loss = 0.48865262866020204
Epoch: 1453, Batch Gradient Norm: 11.46837209306417
Epoch: 1453, Batch Gradient Norm after: 11.46837209306417
Epoch 1454/10000, Prediction Accuracy = 60.414%, Loss = 0.4920943081378937
Epoch: 1454, Batch Gradient Norm: 12.420663037900454
Epoch: 1454, Batch Gradient Norm after: 12.420663037900454
Epoch 1455/10000, Prediction Accuracy = 60.528%, Loss = 0.4970694243907928
Epoch: 1455, Batch Gradient Norm: 12.438261964763708
Epoch: 1455, Batch Gradient Norm after: 12.438261964763708
Epoch 1456/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.49321892857551575
Epoch: 1456, Batch Gradient Norm: 9.138223772356149
Epoch: 1456, Batch Gradient Norm after: 9.138223772356149
Epoch 1457/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.48680819272994996
Epoch: 1457, Batch Gradient Norm: 10.117267047116346
Epoch: 1457, Batch Gradient Norm after: 10.117267047116346
Epoch 1458/10000, Prediction Accuracy = 60.596000000000004%, Loss = 0.4895149886608124
Epoch: 1458, Batch Gradient Norm: 9.0902453310233
Epoch: 1458, Batch Gradient Norm after: 9.0902453310233
Epoch 1459/10000, Prediction Accuracy = 60.552%, Loss = 0.48771544694900515
Epoch: 1459, Batch Gradient Norm: 9.31387044923227
Epoch: 1459, Batch Gradient Norm after: 9.31387044923227
Epoch 1460/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.4874082863330841
Epoch: 1460, Batch Gradient Norm: 10.440022264294964
Epoch: 1460, Batch Gradient Norm after: 10.440022264294964
Epoch 1461/10000, Prediction Accuracy = 60.54600000000001%, Loss = 0.49105035662651064
Epoch: 1461, Batch Gradient Norm: 11.042670317813323
Epoch: 1461, Batch Gradient Norm after: 11.042670317813323
Epoch 1462/10000, Prediction Accuracy = 60.54200000000001%, Loss = 0.4918730139732361
Epoch: 1462, Batch Gradient Norm: 10.795600576530987
Epoch: 1462, Batch Gradient Norm after: 10.795600576530987
Epoch 1463/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.4885187566280365
Epoch: 1463, Batch Gradient Norm: 10.826279861115689
Epoch: 1463, Batch Gradient Norm after: 10.826279861115689
Epoch 1464/10000, Prediction Accuracy = 60.448%, Loss = 0.4900449633598328
Epoch: 1464, Batch Gradient Norm: 11.797852576194899
Epoch: 1464, Batch Gradient Norm after: 11.797852576194899
Epoch 1465/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.4922513484954834
Epoch: 1465, Batch Gradient Norm: 13.604469525892883
Epoch: 1465, Batch Gradient Norm after: 13.604469525892883
Epoch 1466/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.4936212122440338
Epoch: 1466, Batch Gradient Norm: 11.240545748127513
Epoch: 1466, Batch Gradient Norm after: 11.240545748127513
Epoch 1467/10000, Prediction Accuracy = 60.524%, Loss = 0.49135286808013917
Epoch: 1467, Batch Gradient Norm: 10.380687016502684
Epoch: 1467, Batch Gradient Norm after: 10.380687016502684
Epoch 1468/10000, Prediction Accuracy = 60.56%, Loss = 0.49083044528961184
Epoch: 1468, Batch Gradient Norm: 10.528273763196648
Epoch: 1468, Batch Gradient Norm after: 10.528273763196648
Epoch 1469/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.49055885076522826
Epoch: 1469, Batch Gradient Norm: 10.405557326255334
Epoch: 1469, Batch Gradient Norm after: 10.405557326255334
Epoch 1470/10000, Prediction Accuracy = 60.464%, Loss = 0.4899736702442169
Epoch: 1470, Batch Gradient Norm: 8.543908080138106
Epoch: 1470, Batch Gradient Norm after: 8.543908080138106
Epoch 1471/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.48595237731933594
Epoch: 1471, Batch Gradient Norm: 9.7517440352396
Epoch: 1471, Batch Gradient Norm after: 9.7517440352396
Epoch 1472/10000, Prediction Accuracy = 60.46%, Loss = 0.4889462113380432
Epoch: 1472, Batch Gradient Norm: 9.918534608865805
Epoch: 1472, Batch Gradient Norm after: 9.918534608865805
Epoch 1473/10000, Prediction Accuracy = 60.43800000000001%, Loss = 0.48771694898605344
Epoch: 1473, Batch Gradient Norm: 10.267762175298513
Epoch: 1473, Batch Gradient Norm after: 10.267762175298513
Epoch 1474/10000, Prediction Accuracy = 60.508%, Loss = 0.4892710387706757
Epoch: 1474, Batch Gradient Norm: 10.730359924976934
Epoch: 1474, Batch Gradient Norm after: 10.730359924976934
Epoch 1475/10000, Prediction Accuracy = 60.524%, Loss = 0.48901642560958863
Epoch: 1475, Batch Gradient Norm: 13.598529843846354
Epoch: 1475, Batch Gradient Norm after: 13.598529843846354
Epoch 1476/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.4943689346313477
Epoch: 1476, Batch Gradient Norm: 13.747914585668529
Epoch: 1476, Batch Gradient Norm after: 13.747914585668529
Epoch 1477/10000, Prediction Accuracy = 60.54%, Loss = 0.49262951612472533
Epoch: 1477, Batch Gradient Norm: 12.731253468151625
Epoch: 1477, Batch Gradient Norm after: 12.731253468151625
Epoch 1478/10000, Prediction Accuracy = 60.512%, Loss = 0.49011428356170655
Epoch: 1478, Batch Gradient Norm: 13.171760320583507
Epoch: 1478, Batch Gradient Norm after: 13.171760320583507
Epoch 1479/10000, Prediction Accuracy = 60.564%, Loss = 0.4949783802032471
Epoch: 1479, Batch Gradient Norm: 15.625545957697739
Epoch: 1479, Batch Gradient Norm after: 15.194513262854523
Epoch 1480/10000, Prediction Accuracy = 60.4%, Loss = 0.49650118947029115
Epoch: 1480, Batch Gradient Norm: 12.28543609559067
Epoch: 1480, Batch Gradient Norm after: 12.28543609559067
Epoch 1481/10000, Prediction Accuracy = 60.495999999999995%, Loss = 0.4919956147670746
Epoch: 1481, Batch Gradient Norm: 14.769110566505043
Epoch: 1481, Batch Gradient Norm after: 14.769110566505043
Epoch 1482/10000, Prediction Accuracy = 60.574%, Loss = 0.4971433401107788
Epoch: 1482, Batch Gradient Norm: 13.678228537206621
Epoch: 1482, Batch Gradient Norm after: 13.678228537206621
Epoch 1483/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.49318777918815615
Epoch: 1483, Batch Gradient Norm: 12.941418793070463
Epoch: 1483, Batch Gradient Norm after: 12.941418793070463
Epoch 1484/10000, Prediction Accuracy = 60.508%, Loss = 0.4924686014652252
Epoch: 1484, Batch Gradient Norm: 11.621647697543654
Epoch: 1484, Batch Gradient Norm after: 11.621647697543654
Epoch 1485/10000, Prediction Accuracy = 60.488%, Loss = 0.49001290202140807
Epoch: 1485, Batch Gradient Norm: 10.614607450059331
Epoch: 1485, Batch Gradient Norm after: 10.614607450059331
Epoch 1486/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.48843989372253416
Epoch: 1486, Batch Gradient Norm: 11.544365745510945
Epoch: 1486, Batch Gradient Norm after: 11.544365745510945
Epoch 1487/10000, Prediction Accuracy = 60.548%, Loss = 0.49133671522140504
Epoch: 1487, Batch Gradient Norm: 13.952563106484684
Epoch: 1487, Batch Gradient Norm after: 13.952563106484684
Epoch 1488/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.4962380588054657
Epoch: 1488, Batch Gradient Norm: 12.420698245673444
Epoch: 1488, Batch Gradient Norm after: 12.420698245673444
Epoch 1489/10000, Prediction Accuracy = 60.525999999999996%, Loss = 0.49171808958053587
Epoch: 1489, Batch Gradient Norm: 12.052577018179166
Epoch: 1489, Batch Gradient Norm after: 12.052577018179166
Epoch 1490/10000, Prediction Accuracy = 60.634%, Loss = 0.4912519812583923
Epoch: 1490, Batch Gradient Norm: 12.515334180529376
Epoch: 1490, Batch Gradient Norm after: 12.515334180529376
Epoch 1491/10000, Prediction Accuracy = 60.552%, Loss = 0.49034111499786376
Epoch: 1491, Batch Gradient Norm: 10.512220021285524
Epoch: 1491, Batch Gradient Norm after: 10.512220021285524
Epoch 1492/10000, Prediction Accuracy = 60.516000000000005%, Loss = 0.48799608945846557
Epoch: 1492, Batch Gradient Norm: 7.767127721034614
Epoch: 1492, Batch Gradient Norm after: 7.767127721034614
Epoch 1493/10000, Prediction Accuracy = 60.55800000000001%, Loss = 0.483436381816864
Epoch: 1493, Batch Gradient Norm: 9.934144568742015
Epoch: 1493, Batch Gradient Norm after: 9.934144568742015
Epoch 1494/10000, Prediction Accuracy = 60.511999999999986%, Loss = 0.48690140843391416
Epoch: 1494, Batch Gradient Norm: 9.573096262211344
Epoch: 1494, Batch Gradient Norm after: 9.573096262211344
Epoch 1495/10000, Prediction Accuracy = 60.6%, Loss = 0.4865384042263031
Epoch: 1495, Batch Gradient Norm: 9.69379331365581
Epoch: 1495, Batch Gradient Norm after: 9.69379331365581
Epoch 1496/10000, Prediction Accuracy = 60.565999999999995%, Loss = 0.48810704946517947
Epoch: 1496, Batch Gradient Norm: 9.618657485289797
Epoch: 1496, Batch Gradient Norm after: 9.618657485289797
Epoch 1497/10000, Prediction Accuracy = 60.553999999999995%, Loss = 0.4873779773712158
Epoch: 1497, Batch Gradient Norm: 10.645134733654924
Epoch: 1497, Batch Gradient Norm after: 10.645134733654924
Epoch 1498/10000, Prediction Accuracy = 60.492%, Loss = 0.4884561121463776
Epoch: 1498, Batch Gradient Norm: 10.858960054437311
Epoch: 1498, Batch Gradient Norm after: 10.858960054437311
Epoch 1499/10000, Prediction Accuracy = 60.6%, Loss = 0.48917279243469236
Epoch: 1499, Batch Gradient Norm: 10.457819572438915
Epoch: 1499, Batch Gradient Norm after: 10.457819572438915
Epoch 1500/10000, Prediction Accuracy = 60.56%, Loss = 0.48647804856300353
Epoch: 1500, Batch Gradient Norm: 10.15542608839403
Epoch: 1500, Batch Gradient Norm after: 10.15542608839403
Epoch 1501/10000, Prediction Accuracy = 60.486000000000004%, Loss = 0.4860279083251953
Epoch: 1501, Batch Gradient Norm: 7.464785319377339
Epoch: 1501, Batch Gradient Norm after: 7.464785319377339
Epoch 1502/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.48306400179862974
Epoch: 1502, Batch Gradient Norm: 6.919293327457373
Epoch: 1502, Batch Gradient Norm after: 6.919293327457373
Epoch 1503/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.48208838105201723
Epoch: 1503, Batch Gradient Norm: 9.825138682738311
Epoch: 1503, Batch Gradient Norm after: 9.825138682738311
Epoch 1504/10000, Prediction Accuracy = 60.55800000000001%, Loss = 0.4852315068244934
Epoch: 1504, Batch Gradient Norm: 9.400749744149476
Epoch: 1504, Batch Gradient Norm after: 9.400749744149476
Epoch 1505/10000, Prediction Accuracy = 60.55%, Loss = 0.48408421874046326
Epoch: 1505, Batch Gradient Norm: 10.021941310397175
Epoch: 1505, Batch Gradient Norm after: 10.021941310397175
Epoch 1506/10000, Prediction Accuracy = 60.562%, Loss = 0.48796309232711793
Epoch: 1506, Batch Gradient Norm: 12.143972599833605
Epoch: 1506, Batch Gradient Norm after: 12.143972599833605
Epoch 1507/10000, Prediction Accuracy = 60.553999999999995%, Loss = 0.48958266973495485
Epoch: 1507, Batch Gradient Norm: 13.255055293425388
Epoch: 1507, Batch Gradient Norm after: 13.255055293425388
Epoch 1508/10000, Prediction Accuracy = 60.572%, Loss = 0.49095067381858826
Epoch: 1508, Batch Gradient Norm: 10.2785691122735
Epoch: 1508, Batch Gradient Norm after: 10.2785691122735
Epoch 1509/10000, Prediction Accuracy = 60.51399999999999%, Loss = 0.4847114384174347
Epoch: 1509, Batch Gradient Norm: 10.212267646375256
Epoch: 1509, Batch Gradient Norm after: 10.212267646375256
Epoch 1510/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.48658827543258665
Epoch: 1510, Batch Gradient Norm: 12.686845522625811
Epoch: 1510, Batch Gradient Norm after: 12.686845522625811
Epoch 1511/10000, Prediction Accuracy = 60.45%, Loss = 0.4916038751602173
Epoch: 1511, Batch Gradient Norm: 13.426200085234585
Epoch: 1511, Batch Gradient Norm after: 13.426200085234585
Epoch 1512/10000, Prediction Accuracy = 60.41600000000001%, Loss = 0.4921208381652832
Epoch: 1512, Batch Gradient Norm: 14.377086453496027
Epoch: 1512, Batch Gradient Norm after: 14.377086453496027
Epoch 1513/10000, Prediction Accuracy = 60.58%, Loss = 0.49253061413764954
Epoch: 1513, Batch Gradient Norm: 13.582351978387182
Epoch: 1513, Batch Gradient Norm after: 13.582351978387182
Epoch 1514/10000, Prediction Accuracy = 60.56%, Loss = 0.4921820223331451
Epoch: 1514, Batch Gradient Norm: 11.979796728301286
Epoch: 1514, Batch Gradient Norm after: 11.979796728301286
Epoch 1515/10000, Prediction Accuracy = 60.588%, Loss = 0.4872960984706879
Epoch: 1515, Batch Gradient Norm: 9.645674846989094
Epoch: 1515, Batch Gradient Norm after: 9.645674846989094
Epoch 1516/10000, Prediction Accuracy = 60.608000000000004%, Loss = 0.4834385931491852
Epoch: 1516, Batch Gradient Norm: 9.000967527347193
Epoch: 1516, Batch Gradient Norm after: 9.000967527347193
Epoch 1517/10000, Prediction Accuracy = 60.55800000000001%, Loss = 0.485703581571579
Epoch: 1517, Batch Gradient Norm: 10.88837921369183
Epoch: 1517, Batch Gradient Norm after: 10.88837921369183
Epoch 1518/10000, Prediction Accuracy = 60.488%, Loss = 0.48872204422950744
Epoch: 1518, Batch Gradient Norm: 11.649679219881422
Epoch: 1518, Batch Gradient Norm after: 11.649679219881422
Epoch 1519/10000, Prediction Accuracy = 60.565999999999995%, Loss = 0.48940545320510864
Epoch: 1519, Batch Gradient Norm: 11.220307656682547
Epoch: 1519, Batch Gradient Norm after: 11.220307656682547
Epoch 1520/10000, Prediction Accuracy = 60.49000000000001%, Loss = 0.48695695400238037
Epoch: 1520, Batch Gradient Norm: 11.701212770126569
Epoch: 1520, Batch Gradient Norm after: 11.701212770126569
Epoch 1521/10000, Prediction Accuracy = 60.58600000000001%, Loss = 0.48944480419158937
Epoch: 1521, Batch Gradient Norm: 9.540866460204688
Epoch: 1521, Batch Gradient Norm after: 9.540866460204688
Epoch 1522/10000, Prediction Accuracy = 60.544%, Loss = 0.4848172128200531
Epoch: 1522, Batch Gradient Norm: 8.677269464748647
Epoch: 1522, Batch Gradient Norm after: 8.677269464748647
Epoch 1523/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.48222106099128725
Epoch: 1523, Batch Gradient Norm: 11.480075222253623
Epoch: 1523, Batch Gradient Norm after: 11.480075222253623
Epoch 1524/10000, Prediction Accuracy = 60.48%, Loss = 0.4903630197048187
Epoch: 1524, Batch Gradient Norm: 11.814712688926013
Epoch: 1524, Batch Gradient Norm after: 11.814712688926013
Epoch 1525/10000, Prediction Accuracy = 60.46%, Loss = 0.4868746280670166
Epoch: 1525, Batch Gradient Norm: 9.54847907617606
Epoch: 1525, Batch Gradient Norm after: 9.54847907617606
Epoch 1526/10000, Prediction Accuracy = 60.565999999999995%, Loss = 0.48766895532608034
Epoch: 1526, Batch Gradient Norm: 11.346195214320911
Epoch: 1526, Batch Gradient Norm after: 11.346195214320911
Epoch 1527/10000, Prediction Accuracy = 60.562%, Loss = 0.49126136302948
Epoch: 1527, Batch Gradient Norm: 10.53069785590942
Epoch: 1527, Batch Gradient Norm after: 10.53069785590942
Epoch 1528/10000, Prediction Accuracy = 60.5%, Loss = 0.4888539493083954
Epoch: 1528, Batch Gradient Norm: 10.832413798748103
Epoch: 1528, Batch Gradient Norm after: 10.832413798748103
Epoch 1529/10000, Prediction Accuracy = 60.556%, Loss = 0.4856950998306274
Epoch: 1529, Batch Gradient Norm: 8.588685921561671
Epoch: 1529, Batch Gradient Norm after: 8.588685921561671
Epoch 1530/10000, Prediction Accuracy = 60.64%, Loss = 0.4816818177700043
Epoch: 1530, Batch Gradient Norm: 7.846245674746696
Epoch: 1530, Batch Gradient Norm after: 7.846245674746696
Epoch 1531/10000, Prediction Accuracy = 60.64%, Loss = 0.4806521117687225
Epoch: 1531, Batch Gradient Norm: 8.602175444499208
Epoch: 1531, Batch Gradient Norm after: 8.602175444499208
Epoch 1532/10000, Prediction Accuracy = 60.552%, Loss = 0.48245917558670043
Epoch: 1532, Batch Gradient Norm: 9.661425487048914
Epoch: 1532, Batch Gradient Norm after: 9.661425487048914
Epoch 1533/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.4828134059906006
Epoch: 1533, Batch Gradient Norm: 10.39975847603398
Epoch: 1533, Batch Gradient Norm after: 10.39975847603398
Epoch 1534/10000, Prediction Accuracy = 60.55%, Loss = 0.48688191175460815
Epoch: 1534, Batch Gradient Norm: 9.442477696839756
Epoch: 1534, Batch Gradient Norm after: 9.442477696839756
Epoch 1535/10000, Prediction Accuracy = 60.657999999999994%, Loss = 0.4838425159454346
Epoch: 1535, Batch Gradient Norm: 9.593958203007881
Epoch: 1535, Batch Gradient Norm after: 9.593958203007881
Epoch 1536/10000, Prediction Accuracy = 60.5%, Loss = 0.48334885239601133
Epoch: 1536, Batch Gradient Norm: 9.657780472706792
Epoch: 1536, Batch Gradient Norm after: 9.657780472706792
Epoch 1537/10000, Prediction Accuracy = 60.54600000000001%, Loss = 0.4846272587776184
Epoch: 1537, Batch Gradient Norm: 11.43549870934665
Epoch: 1537, Batch Gradient Norm after: 11.43549870934665
Epoch 1538/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.4864043116569519
Epoch: 1538, Batch Gradient Norm: 10.538265880495228
Epoch: 1538, Batch Gradient Norm after: 10.538265880495228
Epoch 1539/10000, Prediction Accuracy = 60.608000000000004%, Loss = 0.48447909355163576
Epoch: 1539, Batch Gradient Norm: 11.445256872090296
Epoch: 1539, Batch Gradient Norm after: 11.445256872090296
Epoch 1540/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.48646913170814515
Epoch: 1540, Batch Gradient Norm: 11.511475437227878
Epoch: 1540, Batch Gradient Norm after: 11.511475437227878
Epoch 1541/10000, Prediction Accuracy = 60.59599999999999%, Loss = 0.48462581634521484
Epoch: 1541, Batch Gradient Norm: 12.122633737356402
Epoch: 1541, Batch Gradient Norm after: 12.122633737356402
Epoch 1542/10000, Prediction Accuracy = 60.626%, Loss = 0.4875896811485291
Epoch: 1542, Batch Gradient Norm: 13.566610891076161
Epoch: 1542, Batch Gradient Norm after: 13.566610891076161
Epoch 1543/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.4884230852127075
Epoch: 1543, Batch Gradient Norm: 12.945525748637039
Epoch: 1543, Batch Gradient Norm after: 12.945525748637039
Epoch 1544/10000, Prediction Accuracy = 60.55999999999999%, Loss = 0.48801628351211546
Epoch: 1544, Batch Gradient Norm: 12.370536603568912
Epoch: 1544, Batch Gradient Norm after: 12.370536603568912
Epoch 1545/10000, Prediction Accuracy = 60.63399999999999%, Loss = 0.48538984060287477
Epoch: 1545, Batch Gradient Norm: 12.682078960018494
Epoch: 1545, Batch Gradient Norm after: 12.682078960018494
Epoch 1546/10000, Prediction Accuracy = 60.510000000000005%, Loss = 0.48635733127593994
Epoch: 1546, Batch Gradient Norm: 8.627359370251511
Epoch: 1546, Batch Gradient Norm after: 8.627359370251511
Epoch 1547/10000, Prediction Accuracy = 60.584%, Loss = 0.48073590397834776
Epoch: 1547, Batch Gradient Norm: 8.602461629856984
Epoch: 1547, Batch Gradient Norm after: 8.602461629856984
Epoch 1548/10000, Prediction Accuracy = 60.576%, Loss = 0.48090704083442687
Epoch: 1548, Batch Gradient Norm: 8.544491550714323
Epoch: 1548, Batch Gradient Norm after: 8.544491550714323
Epoch 1549/10000, Prediction Accuracy = 60.624%, Loss = 0.4803984522819519
Epoch: 1549, Batch Gradient Norm: 8.326499203335016
Epoch: 1549, Batch Gradient Norm after: 8.326499203335016
Epoch 1550/10000, Prediction Accuracy = 60.598%, Loss = 0.4807241201400757
Epoch: 1550, Batch Gradient Norm: 8.785689206930252
Epoch: 1550, Batch Gradient Norm after: 8.785689206930252
Epoch 1551/10000, Prediction Accuracy = 60.64%, Loss = 0.4817172646522522
Epoch: 1551, Batch Gradient Norm: 8.052033462102214
Epoch: 1551, Batch Gradient Norm after: 8.052033462102214
Epoch 1552/10000, Prediction Accuracy = 60.612%, Loss = 0.4810000956058502
Epoch: 1552, Batch Gradient Norm: 8.10561195027656
Epoch: 1552, Batch Gradient Norm after: 8.10561195027656
Epoch 1553/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.4809908151626587
Epoch: 1553, Batch Gradient Norm: 9.975378321656578
Epoch: 1553, Batch Gradient Norm after: 9.975378321656578
Epoch 1554/10000, Prediction Accuracy = 60.544%, Loss = 0.48417405486106874
Epoch: 1554, Batch Gradient Norm: 10.090282069553616
Epoch: 1554, Batch Gradient Norm after: 10.090282069553616
Epoch 1555/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.48351394534111025
Epoch: 1555, Batch Gradient Norm: 10.622579236543837
Epoch: 1555, Batch Gradient Norm after: 10.622579236543837
Epoch 1556/10000, Prediction Accuracy = 60.588%, Loss = 0.483908885717392
Epoch: 1556, Batch Gradient Norm: 9.331939809495323
Epoch: 1556, Batch Gradient Norm after: 9.331939809495323
Epoch 1557/10000, Prediction Accuracy = 60.57000000000001%, Loss = 0.48112218379974364
Epoch: 1557, Batch Gradient Norm: 9.038086415972005
Epoch: 1557, Batch Gradient Norm after: 9.038086415972005
Epoch 1558/10000, Prediction Accuracy = 60.562%, Loss = 0.47939878702163696
Epoch: 1558, Batch Gradient Norm: 8.345160566526703
Epoch: 1558, Batch Gradient Norm after: 8.345160566526703
Epoch 1559/10000, Prediction Accuracy = 60.620000000000005%, Loss = 0.4813052177429199
Epoch: 1559, Batch Gradient Norm: 9.487618060006993
Epoch: 1559, Batch Gradient Norm after: 9.487618060006993
Epoch 1560/10000, Prediction Accuracy = 60.61600000000001%, Loss = 0.48242950439453125
Epoch: 1560, Batch Gradient Norm: 12.249577865332745
Epoch: 1560, Batch Gradient Norm after: 12.249577865332745
Epoch 1561/10000, Prediction Accuracy = 60.564%, Loss = 0.4901467144489288
Epoch: 1561, Batch Gradient Norm: 8.113267431898096
Epoch: 1561, Batch Gradient Norm after: 8.113267431898096
Epoch 1562/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.48013232946395873
Epoch: 1562, Batch Gradient Norm: 8.834520699572504
Epoch: 1562, Batch Gradient Norm after: 8.834520699572504
Epoch 1563/10000, Prediction Accuracy = 60.524%, Loss = 0.48191534280776976
Epoch: 1563, Batch Gradient Norm: 9.96452744362334
Epoch: 1563, Batch Gradient Norm after: 9.96452744362334
Epoch 1564/10000, Prediction Accuracy = 60.56%, Loss = 0.4810701847076416
Epoch: 1564, Batch Gradient Norm: 11.140609122028035
Epoch: 1564, Batch Gradient Norm after: 11.140609122028035
Epoch 1565/10000, Prediction Accuracy = 60.64%, Loss = 0.48383634686470034
Epoch: 1565, Batch Gradient Norm: 9.499548786335067
Epoch: 1565, Batch Gradient Norm after: 9.499548786335067
Epoch 1566/10000, Prediction Accuracy = 60.622%, Loss = 0.48108163475990295
Epoch: 1566, Batch Gradient Norm: 9.458072865716137
Epoch: 1566, Batch Gradient Norm after: 9.458072865716137
Epoch 1567/10000, Prediction Accuracy = 60.6%, Loss = 0.4817603290081024
Epoch: 1567, Batch Gradient Norm: 8.511942806648262
Epoch: 1567, Batch Gradient Norm after: 8.511942806648262
Epoch 1568/10000, Prediction Accuracy = 60.552%, Loss = 0.4789985179901123
Epoch: 1568, Batch Gradient Norm: 10.942269827780063
Epoch: 1568, Batch Gradient Norm after: 10.942269827780063
Epoch 1569/10000, Prediction Accuracy = 60.598%, Loss = 0.4863323986530304
Epoch: 1569, Batch Gradient Norm: 14.784987884531962
Epoch: 1569, Batch Gradient Norm after: 14.784987884531962
Epoch 1570/10000, Prediction Accuracy = 60.508%, Loss = 0.49460126757621764
Epoch: 1570, Batch Gradient Norm: 13.710035855756397
Epoch: 1570, Batch Gradient Norm after: 13.710035855756397
Epoch 1571/10000, Prediction Accuracy = 60.614%, Loss = 0.4916216850280762
Epoch: 1571, Batch Gradient Norm: 12.636944656507312
Epoch: 1571, Batch Gradient Norm after: 12.636944656507312
Epoch 1572/10000, Prediction Accuracy = 60.52399999999999%, Loss = 0.4875003039836884
Epoch: 1572, Batch Gradient Norm: 10.260787171780146
Epoch: 1572, Batch Gradient Norm after: 10.260787171780146
Epoch 1573/10000, Prediction Accuracy = 60.662%, Loss = 0.4852936327457428
Epoch: 1573, Batch Gradient Norm: 12.958236899380998
Epoch: 1573, Batch Gradient Norm after: 12.958236899380998
Epoch 1574/10000, Prediction Accuracy = 60.52%, Loss = 0.4877426028251648
Epoch: 1574, Batch Gradient Norm: 10.361143882564665
Epoch: 1574, Batch Gradient Norm after: 10.361143882564665
Epoch 1575/10000, Prediction Accuracy = 60.57600000000001%, Loss = 0.48263423442840575
Epoch: 1575, Batch Gradient Norm: 9.183248150075006
Epoch: 1575, Batch Gradient Norm after: 9.183248150075006
Epoch 1576/10000, Prediction Accuracy = 60.628%, Loss = 0.47923709750175475
Epoch: 1576, Batch Gradient Norm: 9.15640516937591
Epoch: 1576, Batch Gradient Norm after: 9.15640516937591
Epoch 1577/10000, Prediction Accuracy = 60.664%, Loss = 0.4814313590526581
Epoch: 1577, Batch Gradient Norm: 10.284105257239283
Epoch: 1577, Batch Gradient Norm after: 10.284105257239283
Epoch 1578/10000, Prediction Accuracy = 60.66799999999999%, Loss = 0.4815894842147827
Epoch: 1578, Batch Gradient Norm: 13.517544646434434
Epoch: 1578, Batch Gradient Norm after: 13.517544646434434
Epoch 1579/10000, Prediction Accuracy = 60.532%, Loss = 0.4896687924861908
Epoch: 1579, Batch Gradient Norm: 11.366156416620724
Epoch: 1579, Batch Gradient Norm after: 11.366156416620724
Epoch 1580/10000, Prediction Accuracy = 60.596000000000004%, Loss = 0.4841517448425293
Epoch: 1580, Batch Gradient Norm: 11.21681834540559
Epoch: 1580, Batch Gradient Norm after: 11.21681834540559
Epoch 1581/10000, Prediction Accuracy = 60.612%, Loss = 0.48375417590141295
Epoch: 1581, Batch Gradient Norm: 10.717660420468633
Epoch: 1581, Batch Gradient Norm after: 10.717660420468633
Epoch 1582/10000, Prediction Accuracy = 60.529999999999994%, Loss = 0.4814342498779297
Epoch: 1582, Batch Gradient Norm: 9.558729657298343
Epoch: 1582, Batch Gradient Norm after: 9.558729657298343
Epoch 1583/10000, Prediction Accuracy = 60.544%, Loss = 0.47929243445396424
Epoch: 1583, Batch Gradient Norm: 8.961136826358008
Epoch: 1583, Batch Gradient Norm after: 8.961136826358008
Epoch 1584/10000, Prediction Accuracy = 60.63599999999999%, Loss = 0.47950608730316163
Epoch: 1584, Batch Gradient Norm: 10.708974026272754
Epoch: 1584, Batch Gradient Norm after: 10.708974026272754
Epoch 1585/10000, Prediction Accuracy = 60.628%, Loss = 0.4819332242012024
Epoch: 1585, Batch Gradient Norm: 9.621830937698508
Epoch: 1585, Batch Gradient Norm after: 9.621830937698508
Epoch 1586/10000, Prediction Accuracy = 60.616%, Loss = 0.479679799079895
Epoch: 1586, Batch Gradient Norm: 10.060823381457269
Epoch: 1586, Batch Gradient Norm after: 10.060823381457269
Epoch 1587/10000, Prediction Accuracy = 60.634%, Loss = 0.48100911974906924
Epoch: 1587, Batch Gradient Norm: 10.125357138270767
Epoch: 1587, Batch Gradient Norm after: 10.125357138270767
Epoch 1588/10000, Prediction Accuracy = 60.688%, Loss = 0.4802574276924133
Epoch: 1588, Batch Gradient Norm: 12.036213386504645
Epoch: 1588, Batch Gradient Norm after: 12.036213386504645
Epoch 1589/10000, Prediction Accuracy = 60.541999999999994%, Loss = 0.48358436226844786
Epoch: 1589, Batch Gradient Norm: 14.906097129793897
Epoch: 1589, Batch Gradient Norm after: 14.906097129793897
Epoch 1590/10000, Prediction Accuracy = 60.638%, Loss = 0.4891571044921875
Epoch: 1590, Batch Gradient Norm: 12.245539465474097
Epoch: 1590, Batch Gradient Norm after: 12.245539465474097
Epoch 1591/10000, Prediction Accuracy = 60.612%, Loss = 0.4835230827331543
Epoch: 1591, Batch Gradient Norm: 11.724069710641631
Epoch: 1591, Batch Gradient Norm after: 11.724069710641631
Epoch 1592/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.48326554894447327
Epoch: 1592, Batch Gradient Norm: 12.496739042621225
Epoch: 1592, Batch Gradient Norm after: 12.496739042621225
Epoch 1593/10000, Prediction Accuracy = 60.664%, Loss = 0.48267862200737
Epoch: 1593, Batch Gradient Norm: 13.46414061706131
Epoch: 1593, Batch Gradient Norm after: 13.46414061706131
Epoch 1594/10000, Prediction Accuracy = 60.608000000000004%, Loss = 0.4873150050640106
Epoch: 1594, Batch Gradient Norm: 12.321152258424092
Epoch: 1594, Batch Gradient Norm after: 12.321152258424092
Epoch 1595/10000, Prediction Accuracy = 60.634%, Loss = 0.4871285200119019
Epoch: 1595, Batch Gradient Norm: 11.793811400768226
Epoch: 1595, Batch Gradient Norm after: 11.793811400768226
Epoch 1596/10000, Prediction Accuracy = 60.628%, Loss = 0.48518756628036497
Epoch: 1596, Batch Gradient Norm: 10.732960873379245
Epoch: 1596, Batch Gradient Norm after: 10.732960873379245
Epoch 1597/10000, Prediction Accuracy = 60.61800000000001%, Loss = 0.48134703040122984
Epoch: 1597, Batch Gradient Norm: 10.188718831385748
Epoch: 1597, Batch Gradient Norm after: 10.188718831385748
Epoch 1598/10000, Prediction Accuracy = 60.517999999999994%, Loss = 0.48103217482566835
Epoch: 1598, Batch Gradient Norm: 11.64366906722499
Epoch: 1598, Batch Gradient Norm after: 11.64366906722499
Epoch 1599/10000, Prediction Accuracy = 60.602%, Loss = 0.4815320074558258
Epoch: 1599, Batch Gradient Norm: 13.480735019539145
Epoch: 1599, Batch Gradient Norm after: 13.480735019539145
Epoch 1600/10000, Prediction Accuracy = 60.676%, Loss = 0.48585754036903384
Epoch: 1600, Batch Gradient Norm: 13.975208166282204
Epoch: 1600, Batch Gradient Norm after: 13.975208166282204
Epoch 1601/10000, Prediction Accuracy = 60.636%, Loss = 0.4860396206378937
Epoch: 1601, Batch Gradient Norm: 12.324482046398005
Epoch: 1601, Batch Gradient Norm after: 12.324482046398005
Epoch 1602/10000, Prediction Accuracy = 60.758%, Loss = 0.48384026288986204
Epoch: 1602, Batch Gradient Norm: 9.708678830737343
Epoch: 1602, Batch Gradient Norm after: 9.708678830737343
Epoch 1603/10000, Prediction Accuracy = 60.664%, Loss = 0.479383784532547
Epoch: 1603, Batch Gradient Norm: 10.009218729990918
Epoch: 1603, Batch Gradient Norm after: 10.009218729990918
Epoch 1604/10000, Prediction Accuracy = 60.70400000000001%, Loss = 0.48061166405677797
Epoch: 1604, Batch Gradient Norm: 9.91477168889169
Epoch: 1604, Batch Gradient Norm after: 9.91477168889169
Epoch 1605/10000, Prediction Accuracy = 60.553999999999995%, Loss = 0.4803702473640442
Epoch: 1605, Batch Gradient Norm: 9.154765746260992
Epoch: 1605, Batch Gradient Norm after: 9.154765746260992
Epoch 1606/10000, Prediction Accuracy = 60.63399999999999%, Loss = 0.47858144640922545
Epoch: 1606, Batch Gradient Norm: 8.13326459822241
Epoch: 1606, Batch Gradient Norm after: 8.13326459822241
Epoch 1607/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.47601492404937745
Epoch: 1607, Batch Gradient Norm: 8.062404717463968
Epoch: 1607, Batch Gradient Norm after: 8.062404717463968
Epoch 1608/10000, Prediction Accuracy = 60.604%, Loss = 0.4767848253250122
Epoch: 1608, Batch Gradient Norm: 9.840413947342944
Epoch: 1608, Batch Gradient Norm after: 9.840413947342944
Epoch 1609/10000, Prediction Accuracy = 60.63199999999999%, Loss = 0.47958511114120483
Epoch: 1609, Batch Gradient Norm: 8.993269211140724
Epoch: 1609, Batch Gradient Norm after: 8.993269211140724
Epoch 1610/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.47861157059669496
Epoch: 1610, Batch Gradient Norm: 9.36426598450719
Epoch: 1610, Batch Gradient Norm after: 9.36426598450719
Epoch 1611/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.4784239411354065
Epoch: 1611, Batch Gradient Norm: 9.351467159498597
Epoch: 1611, Batch Gradient Norm after: 9.351467159498597
Epoch 1612/10000, Prediction Accuracy = 60.712%, Loss = 0.47954198718070984
Epoch: 1612, Batch Gradient Norm: 8.870080804617025
Epoch: 1612, Batch Gradient Norm after: 8.870080804617025
Epoch 1613/10000, Prediction Accuracy = 60.622%, Loss = 0.47734050154685975
Epoch: 1613, Batch Gradient Norm: 10.497977286318056
Epoch: 1613, Batch Gradient Norm after: 10.497977286318056
Epoch 1614/10000, Prediction Accuracy = 60.638%, Loss = 0.47927637696266173
Epoch: 1614, Batch Gradient Norm: 13.627630414130431
Epoch: 1614, Batch Gradient Norm after: 13.627630414130431
Epoch 1615/10000, Prediction Accuracy = 60.698%, Loss = 0.48404315710067747
Epoch: 1615, Batch Gradient Norm: 11.717512766622786
Epoch: 1615, Batch Gradient Norm after: 11.717512766622786
Epoch 1616/10000, Prediction Accuracy = 60.686%, Loss = 0.4831946432590485
Epoch: 1616, Batch Gradient Norm: 13.23924023398902
Epoch: 1616, Batch Gradient Norm after: 13.23924023398902
Epoch 1617/10000, Prediction Accuracy = 60.674%, Loss = 0.4876203179359436
Epoch: 1617, Batch Gradient Norm: 12.802287314212169
Epoch: 1617, Batch Gradient Norm after: 12.802287314212169
Epoch 1618/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.48278923630714415
Epoch: 1618, Batch Gradient Norm: 13.164215402404082
Epoch: 1618, Batch Gradient Norm after: 13.164215402404082
Epoch 1619/10000, Prediction Accuracy = 60.69199999999999%, Loss = 0.4845352053642273
Epoch: 1619, Batch Gradient Norm: 12.658391302956215
Epoch: 1619, Batch Gradient Norm after: 12.658391302956215
Epoch 1620/10000, Prediction Accuracy = 60.69000000000001%, Loss = 0.4845961332321167
Epoch: 1620, Batch Gradient Norm: 12.110723037920637
Epoch: 1620, Batch Gradient Norm after: 12.110723037920637
Epoch 1621/10000, Prediction Accuracy = 60.662%, Loss = 0.47990783452987673
Epoch: 1621, Batch Gradient Norm: 12.146050884755635
Epoch: 1621, Batch Gradient Norm after: 12.146050884755635
Epoch 1622/10000, Prediction Accuracy = 60.62600000000001%, Loss = 0.48097984790802
Epoch: 1622, Batch Gradient Norm: 12.74969992226365
Epoch: 1622, Batch Gradient Norm after: 12.74969992226365
Epoch 1623/10000, Prediction Accuracy = 60.766%, Loss = 0.4830103278160095
Epoch: 1623, Batch Gradient Norm: 11.137249266181072
Epoch: 1623, Batch Gradient Norm after: 11.137249266181072
Epoch 1624/10000, Prediction Accuracy = 60.65599999999999%, Loss = 0.4805025696754456
Epoch: 1624, Batch Gradient Norm: 12.92274483869525
Epoch: 1624, Batch Gradient Norm after: 12.92274483869525
Epoch 1625/10000, Prediction Accuracy = 60.624%, Loss = 0.48412577509880067
Epoch: 1625, Batch Gradient Norm: 13.212173887018336
Epoch: 1625, Batch Gradient Norm after: 13.212173887018336
Epoch 1626/10000, Prediction Accuracy = 60.717999999999996%, Loss = 0.4839616119861603
Epoch: 1626, Batch Gradient Norm: 14.623116961720445
Epoch: 1626, Batch Gradient Norm after: 14.623116961720445
Epoch 1627/10000, Prediction Accuracy = 60.802%, Loss = 0.48537960052490237
Epoch: 1627, Batch Gradient Norm: 12.264811547078077
Epoch: 1627, Batch Gradient Norm after: 12.264811547078077
Epoch 1628/10000, Prediction Accuracy = 60.733999999999995%, Loss = 0.48208399415016173
Epoch: 1628, Batch Gradient Norm: 10.849421410171447
Epoch: 1628, Batch Gradient Norm after: 10.849421410171447
Epoch 1629/10000, Prediction Accuracy = 60.568%, Loss = 0.4791465520858765
Epoch: 1629, Batch Gradient Norm: 9.614984709465329
Epoch: 1629, Batch Gradient Norm after: 9.614984709465329
Epoch 1630/10000, Prediction Accuracy = 60.596000000000004%, Loss = 0.4778037190437317
Epoch: 1630, Batch Gradient Norm: 9.660049056122359
Epoch: 1630, Batch Gradient Norm after: 9.660049056122359
Epoch 1631/10000, Prediction Accuracy = 60.681999999999995%, Loss = 0.47751914262771605
Epoch: 1631, Batch Gradient Norm: 11.304334286970265
Epoch: 1631, Batch Gradient Norm after: 11.304334286970265
Epoch 1632/10000, Prediction Accuracy = 60.714%, Loss = 0.4811182200908661
Epoch: 1632, Batch Gradient Norm: 11.248778566337021
Epoch: 1632, Batch Gradient Norm after: 11.248778566337021
Epoch 1633/10000, Prediction Accuracy = 60.688%, Loss = 0.4805264830589294
Epoch: 1633, Batch Gradient Norm: 11.472469589403865
Epoch: 1633, Batch Gradient Norm after: 11.472469589403865
Epoch 1634/10000, Prediction Accuracy = 60.698%, Loss = 0.4800192713737488
Epoch: 1634, Batch Gradient Norm: 12.053064550438057
Epoch: 1634, Batch Gradient Norm after: 12.053064550438057
Epoch 1635/10000, Prediction Accuracy = 60.662%, Loss = 0.4800275444984436
Epoch: 1635, Batch Gradient Norm: 9.983297076138202
Epoch: 1635, Batch Gradient Norm after: 9.983297076138202
Epoch 1636/10000, Prediction Accuracy = 60.614%, Loss = 0.4786832332611084
Epoch: 1636, Batch Gradient Norm: 10.692267803342684
Epoch: 1636, Batch Gradient Norm after: 10.692267803342684
Epoch 1637/10000, Prediction Accuracy = 60.644000000000005%, Loss = 0.4811170220375061
Epoch: 1637, Batch Gradient Norm: 11.723410515413693
Epoch: 1637, Batch Gradient Norm after: 11.723410515413693
Epoch 1638/10000, Prediction Accuracy = 60.58%, Loss = 0.48135834336280825
Epoch: 1638, Batch Gradient Norm: 10.891881124668402
Epoch: 1638, Batch Gradient Norm after: 10.891881124668402
Epoch 1639/10000, Prediction Accuracy = 60.616%, Loss = 0.4791111767292023
Epoch: 1639, Batch Gradient Norm: 12.541415132305568
Epoch: 1639, Batch Gradient Norm after: 12.541415132305568
Epoch 1640/10000, Prediction Accuracy = 60.64200000000001%, Loss = 0.4808294653892517
Epoch: 1640, Batch Gradient Norm: 10.70787168329791
Epoch: 1640, Batch Gradient Norm after: 10.70787168329791
Epoch 1641/10000, Prediction Accuracy = 60.65%, Loss = 0.47819602489471436
Epoch: 1641, Batch Gradient Norm: 11.621738640678512
Epoch: 1641, Batch Gradient Norm after: 11.621738640678512
Epoch 1642/10000, Prediction Accuracy = 60.669999999999995%, Loss = 0.48063026666641234
Epoch: 1642, Batch Gradient Norm: 12.739685638557777
Epoch: 1642, Batch Gradient Norm after: 12.739685638557777
Epoch 1643/10000, Prediction Accuracy = 60.698%, Loss = 0.4829623758792877
Epoch: 1643, Batch Gradient Norm: 9.249316344533032
Epoch: 1643, Batch Gradient Norm after: 9.249316344533032
Epoch 1644/10000, Prediction Accuracy = 60.74400000000001%, Loss = 0.47643306851387024
Epoch: 1644, Batch Gradient Norm: 10.418650669451965
Epoch: 1644, Batch Gradient Norm after: 10.418650669451965
Epoch 1645/10000, Prediction Accuracy = 60.61399999999999%, Loss = 0.47758241295814513
Epoch: 1645, Batch Gradient Norm: 11.661279388375203
Epoch: 1645, Batch Gradient Norm after: 11.661279388375203
Epoch 1646/10000, Prediction Accuracy = 60.62199999999999%, Loss = 0.4797426700592041
Epoch: 1646, Batch Gradient Norm: 10.832796924518208
Epoch: 1646, Batch Gradient Norm after: 10.832796924518208
Epoch 1647/10000, Prediction Accuracy = 60.702%, Loss = 0.477528578042984
Epoch: 1647, Batch Gradient Norm: 10.518534695241653
Epoch: 1647, Batch Gradient Norm after: 10.518534695241653
Epoch 1648/10000, Prediction Accuracy = 60.694%, Loss = 0.4760821104049683
Epoch: 1648, Batch Gradient Norm: 8.885498432331609
Epoch: 1648, Batch Gradient Norm after: 8.885498432331609
Epoch 1649/10000, Prediction Accuracy = 60.65200000000001%, Loss = 0.4761713743209839
Epoch: 1649, Batch Gradient Norm: 9.886604939757417
Epoch: 1649, Batch Gradient Norm after: 9.886604939757417
Epoch 1650/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.4833317995071411
Epoch: 1650, Batch Gradient Norm: 12.116267243808636
Epoch: 1650, Batch Gradient Norm after: 12.116267243808636
Epoch 1651/10000, Prediction Accuracy = 60.742%, Loss = 0.4821982502937317
Epoch: 1651, Batch Gradient Norm: 9.271081245801614
Epoch: 1651, Batch Gradient Norm after: 9.271081245801614
Epoch 1652/10000, Prediction Accuracy = 60.718%, Loss = 0.47655131816864016
Epoch: 1652, Batch Gradient Norm: 8.836016904946808
Epoch: 1652, Batch Gradient Norm after: 8.836016904946808
Epoch 1653/10000, Prediction Accuracy = 60.65599999999999%, Loss = 0.4738266050815582
Epoch: 1653, Batch Gradient Norm: 10.294935847777317
Epoch: 1653, Batch Gradient Norm after: 10.294935847777317
Epoch 1654/10000, Prediction Accuracy = 60.648%, Loss = 0.4795395314693451
Epoch: 1654, Batch Gradient Norm: 7.000760339364884
Epoch: 1654, Batch Gradient Norm after: 7.000760339364884
Epoch 1655/10000, Prediction Accuracy = 60.65999999999999%, Loss = 0.473750513792038
Epoch: 1655, Batch Gradient Norm: 5.5541774288983925
Epoch: 1655, Batch Gradient Norm after: 5.5541774288983925
Epoch 1656/10000, Prediction Accuracy = 60.648%, Loss = 0.47090811729431153
Epoch: 1656, Batch Gradient Norm: 5.807241910853897
Epoch: 1656, Batch Gradient Norm after: 5.807241910853897
Epoch 1657/10000, Prediction Accuracy = 60.614%, Loss = 0.47250171899795534
Epoch: 1657, Batch Gradient Norm: 9.944602422953126
Epoch: 1657, Batch Gradient Norm after: 9.944602422953126
Epoch 1658/10000, Prediction Accuracy = 60.628%, Loss = 0.4751748263835907
Epoch: 1658, Batch Gradient Norm: 13.70913070253914
Epoch: 1658, Batch Gradient Norm after: 13.70913070253914
Epoch 1659/10000, Prediction Accuracy = 60.681999999999995%, Loss = 0.4809153974056244
Epoch: 1659, Batch Gradient Norm: 12.499073060435542
Epoch: 1659, Batch Gradient Norm after: 12.499073060435542
Epoch 1660/10000, Prediction Accuracy = 60.746%, Loss = 0.47852280735969543
Epoch: 1660, Batch Gradient Norm: 13.362269559978749
Epoch: 1660, Batch Gradient Norm after: 13.362269559978749
Epoch 1661/10000, Prediction Accuracy = 60.68000000000001%, Loss = 0.4811979055404663
Epoch: 1661, Batch Gradient Norm: 12.548759748347663
Epoch: 1661, Batch Gradient Norm after: 12.548759748347663
Epoch 1662/10000, Prediction Accuracy = 60.746%, Loss = 0.47869825959205625
Epoch: 1662, Batch Gradient Norm: 10.468068997003558
Epoch: 1662, Batch Gradient Norm after: 10.468068997003558
Epoch 1663/10000, Prediction Accuracy = 60.605999999999995%, Loss = 0.47605878710746763
Epoch: 1663, Batch Gradient Norm: 11.128311116572553
Epoch: 1663, Batch Gradient Norm after: 11.128311116572553
Epoch 1664/10000, Prediction Accuracy = 60.681999999999995%, Loss = 0.4793148756027222
Epoch: 1664, Batch Gradient Norm: 11.17473878127218
Epoch: 1664, Batch Gradient Norm after: 11.17473878127218
Epoch 1665/10000, Prediction Accuracy = 60.742000000000004%, Loss = 0.4768937349319458
Epoch: 1665, Batch Gradient Norm: 13.625064454691369
Epoch: 1665, Batch Gradient Norm after: 13.625064454691369
Epoch 1666/10000, Prediction Accuracy = 60.696000000000005%, Loss = 0.48253779411315917
Epoch: 1666, Batch Gradient Norm: 13.38042818105959
Epoch: 1666, Batch Gradient Norm after: 13.38042818105959
Epoch 1667/10000, Prediction Accuracy = 60.698%, Loss = 0.4821176290512085
Epoch: 1667, Batch Gradient Norm: 13.992297413563861
Epoch: 1667, Batch Gradient Norm after: 13.992297413563861
Epoch 1668/10000, Prediction Accuracy = 60.66799999999999%, Loss = 0.48209874629974364
Epoch: 1668, Batch Gradient Norm: 10.724788637123424
Epoch: 1668, Batch Gradient Norm after: 10.724788637123424
Epoch 1669/10000, Prediction Accuracy = 60.65599999999999%, Loss = 0.47557572722435
Epoch: 1669, Batch Gradient Norm: 10.4730607803474
Epoch: 1669, Batch Gradient Norm after: 10.4730607803474
Epoch 1670/10000, Prediction Accuracy = 60.730000000000004%, Loss = 0.4759293258190155
Epoch: 1670, Batch Gradient Norm: 10.38354371520357
Epoch: 1670, Batch Gradient Norm after: 10.38354371520357
Epoch 1671/10000, Prediction Accuracy = 60.688%, Loss = 0.477421772480011
Epoch: 1671, Batch Gradient Norm: 10.246021959109305
Epoch: 1671, Batch Gradient Norm after: 10.246021959109305
Epoch 1672/10000, Prediction Accuracy = 60.688%, Loss = 0.4767442047595978
Epoch: 1672, Batch Gradient Norm: 8.478941049173107
Epoch: 1672, Batch Gradient Norm after: 8.478941049173107
Epoch 1673/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.47295747995376586
Epoch: 1673, Batch Gradient Norm: 8.550625198706612
Epoch: 1673, Batch Gradient Norm after: 8.550625198706612
Epoch 1674/10000, Prediction Accuracy = 60.69199999999999%, Loss = 0.4732348144054413
Epoch: 1674, Batch Gradient Norm: 9.210384732520401
Epoch: 1674, Batch Gradient Norm after: 9.210384732520401
Epoch 1675/10000, Prediction Accuracy = 60.712%, Loss = 0.4737837314605713
Epoch: 1675, Batch Gradient Norm: 8.267142829742436
Epoch: 1675, Batch Gradient Norm after: 8.267142829742436
Epoch 1676/10000, Prediction Accuracy = 60.622%, Loss = 0.47377477288246156
Epoch: 1676, Batch Gradient Norm: 8.496331750773379
Epoch: 1676, Batch Gradient Norm after: 8.496331750773379
Epoch 1677/10000, Prediction Accuracy = 60.66799999999999%, Loss = 0.47377667427062986
Epoch: 1677, Batch Gradient Norm: 10.093062460673302
Epoch: 1677, Batch Gradient Norm after: 10.093062460673302
Epoch 1678/10000, Prediction Accuracy = 60.73%, Loss = 0.4757471740245819
Epoch: 1678, Batch Gradient Norm: 10.202222330043583
Epoch: 1678, Batch Gradient Norm after: 10.202222330043583
Epoch 1679/10000, Prediction Accuracy = 60.74400000000001%, Loss = 0.47341325879096985
Epoch: 1679, Batch Gradient Norm: 11.371167143910329
Epoch: 1679, Batch Gradient Norm after: 11.371167143910329
Epoch 1680/10000, Prediction Accuracy = 60.674%, Loss = 0.4804230213165283
Epoch: 1680, Batch Gradient Norm: 12.51242284876079
Epoch: 1680, Batch Gradient Norm after: 12.51242284876079
Epoch 1681/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.48304755091667173
Epoch: 1681, Batch Gradient Norm: 12.67767531494622
Epoch: 1681, Batch Gradient Norm after: 12.67767531494622
Epoch 1682/10000, Prediction Accuracy = 60.738%, Loss = 0.4791213095188141
Epoch: 1682, Batch Gradient Norm: 12.417932151554682
Epoch: 1682, Batch Gradient Norm after: 12.417932151554682
Epoch 1683/10000, Prediction Accuracy = 60.712%, Loss = 0.4782895565032959
Epoch: 1683, Batch Gradient Norm: 12.932713089822265
Epoch: 1683, Batch Gradient Norm after: 12.932713089822265
Epoch 1684/10000, Prediction Accuracy = 60.720000000000006%, Loss = 0.4827293813228607
Epoch: 1684, Batch Gradient Norm: 12.004059408989079
Epoch: 1684, Batch Gradient Norm after: 12.004059408989079
Epoch 1685/10000, Prediction Accuracy = 60.702%, Loss = 0.4786487460136414
Epoch: 1685, Batch Gradient Norm: 10.02962723125541
Epoch: 1685, Batch Gradient Norm after: 10.02962723125541
Epoch 1686/10000, Prediction Accuracy = 60.788%, Loss = 0.474038290977478
Epoch: 1686, Batch Gradient Norm: 9.179684577270798
Epoch: 1686, Batch Gradient Norm after: 9.179684577270798
Epoch 1687/10000, Prediction Accuracy = 60.75600000000001%, Loss = 0.4719952821731567
Epoch: 1687, Batch Gradient Norm: 11.432131733240245
Epoch: 1687, Batch Gradient Norm after: 11.432131733240245
Epoch 1688/10000, Prediction Accuracy = 60.674%, Loss = 0.47793506383895873
Epoch: 1688, Batch Gradient Norm: 10.752325469601379
Epoch: 1688, Batch Gradient Norm after: 10.752325469601379
Epoch 1689/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.4759423851966858
Epoch: 1689, Batch Gradient Norm: 9.688106087366165
Epoch: 1689, Batch Gradient Norm after: 9.688106087366165
Epoch 1690/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.47181742191314696
Epoch: 1690, Batch Gradient Norm: 11.032012646828779
Epoch: 1690, Batch Gradient Norm after: 11.032012646828779
Epoch 1691/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.47601876258850095
Epoch: 1691, Batch Gradient Norm: 10.106643793381455
Epoch: 1691, Batch Gradient Norm after: 10.106643793381455
Epoch 1692/10000, Prediction Accuracy = 60.763999999999996%, Loss = 0.47402713298797605
Epoch: 1692, Batch Gradient Norm: 8.590526293595397
Epoch: 1692, Batch Gradient Norm after: 8.590526293595397
Epoch 1693/10000, Prediction Accuracy = 60.694%, Loss = 0.4717187464237213
Epoch: 1693, Batch Gradient Norm: 10.544037346684945
Epoch: 1693, Batch Gradient Norm after: 10.544037346684945
Epoch 1694/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.47587883472442627
Epoch: 1694, Batch Gradient Norm: 8.855553003589092
Epoch: 1694, Batch Gradient Norm after: 8.855553003589092
Epoch 1695/10000, Prediction Accuracy = 60.705999999999996%, Loss = 0.47483516335487364
Epoch: 1695, Batch Gradient Norm: 9.00737879957123
Epoch: 1695, Batch Gradient Norm after: 9.00737879957123
Epoch 1696/10000, Prediction Accuracy = 60.76800000000001%, Loss = 0.47239089012145996
Epoch: 1696, Batch Gradient Norm: 11.438515517032279
Epoch: 1696, Batch Gradient Norm after: 11.438515517032279
Epoch 1697/10000, Prediction Accuracy = 60.638%, Loss = 0.47791734933853147
Epoch: 1697, Batch Gradient Norm: 8.19669973733126
Epoch: 1697, Batch Gradient Norm after: 8.19669973733126
Epoch 1698/10000, Prediction Accuracy = 60.76800000000001%, Loss = 0.4729102969169617
Epoch: 1698, Batch Gradient Norm: 9.082373301600146
Epoch: 1698, Batch Gradient Norm after: 9.082373301600146
Epoch 1699/10000, Prediction Accuracy = 60.824%, Loss = 0.47303771376609804
Epoch: 1699, Batch Gradient Norm: 7.642100503035557
Epoch: 1699, Batch Gradient Norm after: 7.642100503035557
Epoch 1700/10000, Prediction Accuracy = 60.758%, Loss = 0.4711687624454498
Epoch: 1700, Batch Gradient Norm: 10.043185156155953
Epoch: 1700, Batch Gradient Norm after: 10.043185156155953
Epoch 1701/10000, Prediction Accuracy = 60.70399999999999%, Loss = 0.4779726564884186
Epoch: 1701, Batch Gradient Norm: 9.809460265419869
Epoch: 1701, Batch Gradient Norm after: 9.809460265419869
Epoch 1702/10000, Prediction Accuracy = 60.742%, Loss = 0.4743817925453186
Epoch: 1702, Batch Gradient Norm: 10.866042037514363
Epoch: 1702, Batch Gradient Norm after: 10.866042037514363
Epoch 1703/10000, Prediction Accuracy = 60.70399999999999%, Loss = 0.4746444821357727
Epoch: 1703, Batch Gradient Norm: 8.964896540635218
Epoch: 1703, Batch Gradient Norm after: 8.964896540635218
Epoch 1704/10000, Prediction Accuracy = 60.814%, Loss = 0.4709147036075592
Epoch: 1704, Batch Gradient Norm: 8.87731827469916
Epoch: 1704, Batch Gradient Norm after: 8.87731827469916
Epoch 1705/10000, Prediction Accuracy = 60.67999999999999%, Loss = 0.4705214023590088
Epoch: 1705, Batch Gradient Norm: 8.966397307758777
Epoch: 1705, Batch Gradient Norm after: 8.966397307758777
Epoch 1706/10000, Prediction Accuracy = 60.742%, Loss = 0.4732867956161499
Epoch: 1706, Batch Gradient Norm: 10.121983086545084
Epoch: 1706, Batch Gradient Norm after: 10.121983086545084
Epoch 1707/10000, Prediction Accuracy = 60.75%, Loss = 0.47416870594024657
Epoch: 1707, Batch Gradient Norm: 12.79294436243212
Epoch: 1707, Batch Gradient Norm after: 12.79294436243212
Epoch 1708/10000, Prediction Accuracy = 60.786%, Loss = 0.4783731162548065
Epoch: 1708, Batch Gradient Norm: 13.339952887209765
Epoch: 1708, Batch Gradient Norm after: 13.339952887209765
Epoch 1709/10000, Prediction Accuracy = 60.714%, Loss = 0.4796417236328125
Epoch: 1709, Batch Gradient Norm: 11.532879716372424
Epoch: 1709, Batch Gradient Norm after: 11.532879716372424
Epoch 1710/10000, Prediction Accuracy = 60.734%, Loss = 0.4759629011154175
Epoch: 1710, Batch Gradient Norm: 9.244565646561874
Epoch: 1710, Batch Gradient Norm after: 9.244565646561874
Epoch 1711/10000, Prediction Accuracy = 60.678%, Loss = 0.47284014225006105
Epoch: 1711, Batch Gradient Norm: 8.77123041878551
Epoch: 1711, Batch Gradient Norm after: 8.77123041878551
Epoch 1712/10000, Prediction Accuracy = 60.838%, Loss = 0.4702943801879883
Epoch: 1712, Batch Gradient Norm: 7.344810173982018
Epoch: 1712, Batch Gradient Norm after: 7.344810173982018
Epoch 1713/10000, Prediction Accuracy = 60.664%, Loss = 0.4688582122325897
Epoch: 1713, Batch Gradient Norm: 8.997971941593962
Epoch: 1713, Batch Gradient Norm after: 8.997971941593962
Epoch 1714/10000, Prediction Accuracy = 60.762%, Loss = 0.47192999720573425
Epoch: 1714, Batch Gradient Norm: 9.01726906296282
Epoch: 1714, Batch Gradient Norm after: 9.01726906296282
Epoch 1715/10000, Prediction Accuracy = 60.779999999999994%, Loss = 0.4729923367500305
Epoch: 1715, Batch Gradient Norm: 13.986913439901379
Epoch: 1715, Batch Gradient Norm after: 13.986913439901379
Epoch 1716/10000, Prediction Accuracy = 60.674%, Loss = 0.4837259709835052
Epoch: 1716, Batch Gradient Norm: 11.770084397085355
Epoch: 1716, Batch Gradient Norm after: 11.770084397085355
Epoch 1717/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.4749526262283325
Epoch: 1717, Batch Gradient Norm: 10.701931996930647
Epoch: 1717, Batch Gradient Norm after: 10.701931996930647
Epoch 1718/10000, Prediction Accuracy = 60.698%, Loss = 0.4761487305164337
Epoch: 1718, Batch Gradient Norm: 9.697830686067075
Epoch: 1718, Batch Gradient Norm after: 9.697830686067075
Epoch 1719/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.47265914678573606
Epoch: 1719, Batch Gradient Norm: 12.644559448333132
Epoch: 1719, Batch Gradient Norm after: 12.644559448333132
Epoch 1720/10000, Prediction Accuracy = 60.790000000000006%, Loss = 0.478704971075058
Epoch: 1720, Batch Gradient Norm: 10.968732208570756
Epoch: 1720, Batch Gradient Norm after: 10.968732208570756
Epoch 1721/10000, Prediction Accuracy = 60.660000000000004%, Loss = 0.4762358427047729
Epoch: 1721, Batch Gradient Norm: 10.94790988234544
Epoch: 1721, Batch Gradient Norm after: 10.94790988234544
Epoch 1722/10000, Prediction Accuracy = 60.696000000000005%, Loss = 0.4759726107120514
Epoch: 1722, Batch Gradient Norm: 12.236058235465993
Epoch: 1722, Batch Gradient Norm after: 12.236058235465993
Epoch 1723/10000, Prediction Accuracy = 60.818%, Loss = 0.478409343957901
Epoch: 1723, Batch Gradient Norm: 12.494668804396698
Epoch: 1723, Batch Gradient Norm after: 12.494668804396698
Epoch 1724/10000, Prediction Accuracy = 60.852%, Loss = 0.47850397825241087
Epoch: 1724, Batch Gradient Norm: 12.68740188437387
Epoch: 1724, Batch Gradient Norm after: 12.68740188437387
Epoch 1725/10000, Prediction Accuracy = 60.73%, Loss = 0.47831121683120725
Epoch: 1725, Batch Gradient Norm: 13.691172333311515
Epoch: 1725, Batch Gradient Norm after: 13.691172333311515
Epoch 1726/10000, Prediction Accuracy = 60.702%, Loss = 0.47865869998931887
Epoch: 1726, Batch Gradient Norm: 11.617313353585152
Epoch: 1726, Batch Gradient Norm after: 11.617313353585152
Epoch 1727/10000, Prediction Accuracy = 60.71600000000001%, Loss = 0.4754887938499451
Epoch: 1727, Batch Gradient Norm: 10.967535520686704
Epoch: 1727, Batch Gradient Norm after: 10.967535520686704
Epoch 1728/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.47390311360359194
Epoch: 1728, Batch Gradient Norm: 10.263230112045486
Epoch: 1728, Batch Gradient Norm after: 10.263230112045486
Epoch 1729/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.47213512659072876
Epoch: 1729, Batch Gradient Norm: 12.079128550072225
Epoch: 1729, Batch Gradient Norm after: 12.079128550072225
Epoch 1730/10000, Prediction Accuracy = 60.669999999999995%, Loss = 0.4750145375728607
Epoch: 1730, Batch Gradient Norm: 12.193437975473396
Epoch: 1730, Batch Gradient Norm after: 12.193437975473396
Epoch 1731/10000, Prediction Accuracy = 60.85%, Loss = 0.47497435212135314
Epoch: 1731, Batch Gradient Norm: 11.364839112090854
Epoch: 1731, Batch Gradient Norm after: 11.364839112090854
Epoch 1732/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.477090460062027
Epoch: 1732, Batch Gradient Norm: 11.009783292937174
Epoch: 1732, Batch Gradient Norm after: 11.009783292937174
Epoch 1733/10000, Prediction Accuracy = 60.69200000000001%, Loss = 0.47464362978935243
Epoch: 1733, Batch Gradient Norm: 12.525446588182957
Epoch: 1733, Batch Gradient Norm after: 12.525446588182957
Epoch 1734/10000, Prediction Accuracy = 60.730000000000004%, Loss = 0.4792281031608582
Epoch: 1734, Batch Gradient Norm: 14.458906718623755
Epoch: 1734, Batch Gradient Norm after: 14.458906718623755
Epoch 1735/10000, Prediction Accuracy = 60.746%, Loss = 0.48113174438476564
Epoch: 1735, Batch Gradient Norm: 15.066527547727
Epoch: 1735, Batch Gradient Norm after: 15.066527547727
Epoch 1736/10000, Prediction Accuracy = 60.806%, Loss = 0.4860545516014099
Epoch: 1736, Batch Gradient Norm: 14.193003358013009
Epoch: 1736, Batch Gradient Norm after: 14.193003358013009
Epoch 1737/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.4826457977294922
Epoch: 1737, Batch Gradient Norm: 10.752083774565376
Epoch: 1737, Batch Gradient Norm after: 10.752083774565376
Epoch 1738/10000, Prediction Accuracy = 60.814%, Loss = 0.4733098566532135
Epoch: 1738, Batch Gradient Norm: 10.949662615855132
Epoch: 1738, Batch Gradient Norm after: 10.949662615855132
Epoch 1739/10000, Prediction Accuracy = 60.738%, Loss = 0.4738896906375885
Epoch: 1739, Batch Gradient Norm: 12.667012057035533
Epoch: 1739, Batch Gradient Norm after: 12.667012057035533
Epoch 1740/10000, Prediction Accuracy = 60.786%, Loss = 0.47557846307754514
Epoch: 1740, Batch Gradient Norm: 10.060395716206013
Epoch: 1740, Batch Gradient Norm after: 10.060395716206013
Epoch 1741/10000, Prediction Accuracy = 60.774%, Loss = 0.47149975299835206
Epoch: 1741, Batch Gradient Norm: 9.235690403804906
Epoch: 1741, Batch Gradient Norm after: 9.235690403804906
Epoch 1742/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.47099579572677613
Epoch: 1742, Batch Gradient Norm: 8.122794148781132
Epoch: 1742, Batch Gradient Norm after: 8.122794148781132
Epoch 1743/10000, Prediction Accuracy = 60.754%, Loss = 0.46803433299064634
Epoch: 1743, Batch Gradient Norm: 9.50228193397198
Epoch: 1743, Batch Gradient Norm after: 9.50228193397198
Epoch 1744/10000, Prediction Accuracy = 60.814%, Loss = 0.47082358598709106
Epoch: 1744, Batch Gradient Norm: 11.655197032971095
Epoch: 1744, Batch Gradient Norm after: 11.655197032971095
Epoch 1745/10000, Prediction Accuracy = 60.774%, Loss = 0.4740199029445648
Epoch: 1745, Batch Gradient Norm: 11.73818106432806
Epoch: 1745, Batch Gradient Norm after: 11.73818106432806
Epoch 1746/10000, Prediction Accuracy = 60.746%, Loss = 0.4740776658058167
Epoch: 1746, Batch Gradient Norm: 11.259035494561461
Epoch: 1746, Batch Gradient Norm after: 11.259035494561461
Epoch 1747/10000, Prediction Accuracy = 60.791999999999994%, Loss = 0.47113639712333677
Epoch: 1747, Batch Gradient Norm: 12.685865844183555
Epoch: 1747, Batch Gradient Norm after: 12.685865844183555
Epoch 1748/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.4768690586090088
Epoch: 1748, Batch Gradient Norm: 13.145751582132357
Epoch: 1748, Batch Gradient Norm after: 13.145751582132357
Epoch 1749/10000, Prediction Accuracy = 60.774%, Loss = 0.47397947311401367
Epoch: 1749, Batch Gradient Norm: 12.03031528551619
Epoch: 1749, Batch Gradient Norm after: 12.03031528551619
Epoch 1750/10000, Prediction Accuracy = 60.754%, Loss = 0.4729199707508087
Epoch: 1750, Batch Gradient Norm: 13.201454517902997
Epoch: 1750, Batch Gradient Norm after: 13.201454517902997
Epoch 1751/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.47541751265525817
Epoch: 1751, Batch Gradient Norm: 10.420834512955501
Epoch: 1751, Batch Gradient Norm after: 10.420834512955501
Epoch 1752/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.4710781216621399
Epoch: 1752, Batch Gradient Norm: 10.458455173669726
Epoch: 1752, Batch Gradient Norm after: 10.458455173669726
Epoch 1753/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.473326700925827
Epoch: 1753, Batch Gradient Norm: 9.788984640136452
Epoch: 1753, Batch Gradient Norm after: 9.788984640136452
Epoch 1754/10000, Prediction Accuracy = 60.727999999999994%, Loss = 0.4702811181545258
Epoch: 1754, Batch Gradient Norm: 10.66866098199515
Epoch: 1754, Batch Gradient Norm after: 10.66866098199515
Epoch 1755/10000, Prediction Accuracy = 60.83%, Loss = 0.4712629199028015
Epoch: 1755, Batch Gradient Norm: 8.720490344016742
Epoch: 1755, Batch Gradient Norm after: 8.720490344016742
Epoch 1756/10000, Prediction Accuracy = 60.73599999999999%, Loss = 0.4693489193916321
Epoch: 1756, Batch Gradient Norm: 9.015847190269689
Epoch: 1756, Batch Gradient Norm after: 9.015847190269689
Epoch 1757/10000, Prediction Accuracy = 60.85%, Loss = 0.47004556059837344
Epoch: 1757, Batch Gradient Norm: 8.82450153047453
Epoch: 1757, Batch Gradient Norm after: 8.82450153047453
Epoch 1758/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.4688308835029602
Epoch: 1758, Batch Gradient Norm: 8.715990745107884
Epoch: 1758, Batch Gradient Norm after: 8.715990745107884
Epoch 1759/10000, Prediction Accuracy = 60.786%, Loss = 0.46833638548851014
Epoch: 1759, Batch Gradient Norm: 10.414430141659448
Epoch: 1759, Batch Gradient Norm after: 10.414430141659448
Epoch 1760/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.470671546459198
Epoch: 1760, Batch Gradient Norm: 12.244246445176003
Epoch: 1760, Batch Gradient Norm after: 12.244246445176003
Epoch 1761/10000, Prediction Accuracy = 60.760000000000005%, Loss = 0.47451513409614565
Epoch: 1761, Batch Gradient Norm: 12.027862441318293
Epoch: 1761, Batch Gradient Norm after: 12.027862441318293
Epoch 1762/10000, Prediction Accuracy = 60.815999999999995%, Loss = 0.47150840759277346
Epoch: 1762, Batch Gradient Norm: 8.945634145194228
Epoch: 1762, Batch Gradient Norm after: 8.945634145194228
Epoch 1763/10000, Prediction Accuracy = 60.802%, Loss = 0.46662022471427916
Epoch: 1763, Batch Gradient Norm: 8.806299375225391
Epoch: 1763, Batch Gradient Norm after: 8.806299375225391
Epoch 1764/10000, Prediction Accuracy = 60.763999999999996%, Loss = 0.46834188103675845
Epoch: 1764, Batch Gradient Norm: 8.420192598832031
Epoch: 1764, Batch Gradient Norm after: 8.420192598832031
Epoch 1765/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.4657454788684845
Epoch: 1765, Batch Gradient Norm: 9.779091909465306
Epoch: 1765, Batch Gradient Norm after: 9.779091909465306
Epoch 1766/10000, Prediction Accuracy = 60.852%, Loss = 0.46978235244750977
Epoch: 1766, Batch Gradient Norm: 9.972249153031404
Epoch: 1766, Batch Gradient Norm after: 9.972249153031404
Epoch 1767/10000, Prediction Accuracy = 60.812%, Loss = 0.4691684067249298
Epoch: 1767, Batch Gradient Norm: 8.751579367187729
Epoch: 1767, Batch Gradient Norm after: 8.751579367187729
Epoch 1768/10000, Prediction Accuracy = 60.827999999999996%, Loss = 0.4678966343402863
Epoch: 1768, Batch Gradient Norm: 9.314869582250173
Epoch: 1768, Batch Gradient Norm after: 9.314869582250173
Epoch 1769/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.47098512649536134
Epoch: 1769, Batch Gradient Norm: 10.311472810151011
Epoch: 1769, Batch Gradient Norm after: 10.311472810151011
Epoch 1770/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.47041303515434263
Epoch: 1770, Batch Gradient Norm: 11.221880279889534
Epoch: 1770, Batch Gradient Norm after: 11.221880279889534
Epoch 1771/10000, Prediction Accuracy = 60.724000000000004%, Loss = 0.47224992513656616
Epoch: 1771, Batch Gradient Norm: 11.79800004337001
Epoch: 1771, Batch Gradient Norm after: 11.79800004337001
Epoch 1772/10000, Prediction Accuracy = 60.85799999999999%, Loss = 0.4726291477680206
Epoch: 1772, Batch Gradient Norm: 14.444374239655358
Epoch: 1772, Batch Gradient Norm after: 14.444374239655358
Epoch 1773/10000, Prediction Accuracy = 60.762%, Loss = 0.4771667420864105
Epoch: 1773, Batch Gradient Norm: 11.018425042486719
Epoch: 1773, Batch Gradient Norm after: 11.018425042486719
Epoch 1774/10000, Prediction Accuracy = 60.754%, Loss = 0.47125057578086854
Epoch: 1774, Batch Gradient Norm: 12.426498636765603
Epoch: 1774, Batch Gradient Norm after: 12.426498636765603
Epoch 1775/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.47415981292724607
Epoch: 1775, Batch Gradient Norm: 12.344543272428773
Epoch: 1775, Batch Gradient Norm after: 12.344543272428773
Epoch 1776/10000, Prediction Accuracy = 60.802%, Loss = 0.4735444843769073
Epoch: 1776, Batch Gradient Norm: 13.289364579382042
Epoch: 1776, Batch Gradient Norm after: 13.289364579382042
Epoch 1777/10000, Prediction Accuracy = 60.79%, Loss = 0.4753199577331543
Epoch: 1777, Batch Gradient Norm: 12.959650852082941
Epoch: 1777, Batch Gradient Norm after: 12.959650852082941
Epoch 1778/10000, Prediction Accuracy = 60.769999999999996%, Loss = 0.4755989074707031
Epoch: 1778, Batch Gradient Norm: 13.533567820079906
Epoch: 1778, Batch Gradient Norm after: 13.533567820079906
Epoch 1779/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.4778678178787231
Epoch: 1779, Batch Gradient Norm: 11.857968714642814
Epoch: 1779, Batch Gradient Norm after: 11.857968714642814
Epoch 1780/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.47154952883720397
Epoch: 1780, Batch Gradient Norm: 11.333773927468336
Epoch: 1780, Batch Gradient Norm after: 11.333773927468336
Epoch 1781/10000, Prediction Accuracy = 60.815999999999995%, Loss = 0.47143073081970216
Epoch: 1781, Batch Gradient Norm: 9.273671759016333
Epoch: 1781, Batch Gradient Norm after: 9.273671759016333
Epoch 1782/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.46863890886306764
Epoch: 1782, Batch Gradient Norm: 11.025329441716634
Epoch: 1782, Batch Gradient Norm after: 11.025329441716634
Epoch 1783/10000, Prediction Accuracy = 60.746%, Loss = 0.47103736400604246
Epoch: 1783, Batch Gradient Norm: 9.53557002679836
Epoch: 1783, Batch Gradient Norm after: 9.53557002679836
Epoch 1784/10000, Prediction Accuracy = 60.73199999999999%, Loss = 0.46811453700065614
Epoch: 1784, Batch Gradient Norm: 10.374716389735127
Epoch: 1784, Batch Gradient Norm after: 10.374716389735127
Epoch 1785/10000, Prediction Accuracy = 60.870000000000005%, Loss = 0.46900593638420107
Epoch: 1785, Batch Gradient Norm: 11.290536828719524
Epoch: 1785, Batch Gradient Norm after: 11.290536828719524
Epoch 1786/10000, Prediction Accuracy = 60.757999999999996%, Loss = 0.47075244784355164
Epoch: 1786, Batch Gradient Norm: 9.287697084059474
Epoch: 1786, Batch Gradient Norm after: 9.287697084059474
Epoch 1787/10000, Prediction Accuracy = 60.818%, Loss = 0.4694476962089539
Epoch: 1787, Batch Gradient Norm: 9.41184881217517
Epoch: 1787, Batch Gradient Norm after: 9.41184881217517
Epoch 1788/10000, Prediction Accuracy = 60.862%, Loss = 0.46761331558227537
Epoch: 1788, Batch Gradient Norm: 9.42917663918335
Epoch: 1788, Batch Gradient Norm after: 9.42917663918335
Epoch 1789/10000, Prediction Accuracy = 60.71999999999999%, Loss = 0.468226820230484
Epoch: 1789, Batch Gradient Norm: 12.02966181887688
Epoch: 1789, Batch Gradient Norm after: 12.02966181887688
Epoch 1790/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.47803630232810973
Epoch: 1790, Batch Gradient Norm: 10.178281957447666
Epoch: 1790, Batch Gradient Norm after: 10.178281957447666
Epoch 1791/10000, Prediction Accuracy = 60.83%, Loss = 0.46922353506088255
Epoch: 1791, Batch Gradient Norm: 11.329872793843194
Epoch: 1791, Batch Gradient Norm after: 11.329872793843194
Epoch 1792/10000, Prediction Accuracy = 60.852%, Loss = 0.47081502676010134
Epoch: 1792, Batch Gradient Norm: 13.30971649507255
Epoch: 1792, Batch Gradient Norm after: 13.30971649507255
Epoch 1793/10000, Prediction Accuracy = 60.760000000000005%, Loss = 0.4729999125003815
Epoch: 1793, Batch Gradient Norm: 12.24335888978227
Epoch: 1793, Batch Gradient Norm after: 12.24335888978227
Epoch 1794/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.4713352382183075
Epoch: 1794, Batch Gradient Norm: 9.577914701149927
Epoch: 1794, Batch Gradient Norm after: 9.577914701149927
Epoch 1795/10000, Prediction Accuracy = 60.888%, Loss = 0.46835504174232484
Epoch: 1795, Batch Gradient Norm: 11.148471920334243
Epoch: 1795, Batch Gradient Norm after: 11.148471920334243
Epoch 1796/10000, Prediction Accuracy = 60.86800000000001%, Loss = 0.47085596323013307
Epoch: 1796, Batch Gradient Norm: 11.392043911031797
Epoch: 1796, Batch Gradient Norm after: 11.392043911031797
Epoch 1797/10000, Prediction Accuracy = 60.758%, Loss = 0.470543771982193
Epoch: 1797, Batch Gradient Norm: 12.38624086702922
Epoch: 1797, Batch Gradient Norm after: 12.38624086702922
Epoch 1798/10000, Prediction Accuracy = 60.824%, Loss = 0.471026736497879
Epoch: 1798, Batch Gradient Norm: 10.797045260699736
Epoch: 1798, Batch Gradient Norm after: 10.797045260699736
Epoch 1799/10000, Prediction Accuracy = 60.79599999999999%, Loss = 0.47119341492652894
Epoch: 1799, Batch Gradient Norm: 12.20461451710346
Epoch: 1799, Batch Gradient Norm after: 12.20461451710346
Epoch 1800/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.47009299993515014
Epoch: 1800, Batch Gradient Norm: 11.739520023671426
Epoch: 1800, Batch Gradient Norm after: 11.739520023671426
Epoch 1801/10000, Prediction Accuracy = 60.8%, Loss = 0.4732392907142639
Epoch: 1801, Batch Gradient Norm: 12.59221060415412
Epoch: 1801, Batch Gradient Norm after: 12.59221060415412
Epoch 1802/10000, Prediction Accuracy = 60.8%, Loss = 0.47274442911148074
Epoch: 1802, Batch Gradient Norm: 13.384321235741142
Epoch: 1802, Batch Gradient Norm after: 13.384321235741142
Epoch 1803/10000, Prediction Accuracy = 60.876%, Loss = 0.47380136847496035
Epoch: 1803, Batch Gradient Norm: 9.71491530366595
Epoch: 1803, Batch Gradient Norm after: 9.71491530366595
Epoch 1804/10000, Prediction Accuracy = 60.8%, Loss = 0.4702094614505768
Epoch: 1804, Batch Gradient Norm: 10.476879036528976
Epoch: 1804, Batch Gradient Norm after: 10.476879036528976
Epoch 1805/10000, Prediction Accuracy = 60.83599999999999%, Loss = 0.4679910123348236
Epoch: 1805, Batch Gradient Norm: 8.823134073947305
Epoch: 1805, Batch Gradient Norm after: 8.823134073947305
Epoch 1806/10000, Prediction Accuracy = 60.846000000000004%, Loss = 0.463866251707077
Epoch: 1806, Batch Gradient Norm: 8.373276672405362
Epoch: 1806, Batch Gradient Norm after: 8.373276672405362
Epoch 1807/10000, Prediction Accuracy = 60.786%, Loss = 0.4649127721786499
Epoch: 1807, Batch Gradient Norm: 10.649555784346427
Epoch: 1807, Batch Gradient Norm after: 10.649555784346427
Epoch 1808/10000, Prediction Accuracy = 60.902%, Loss = 0.4679288685321808
Epoch: 1808, Batch Gradient Norm: 10.86903349924126
Epoch: 1808, Batch Gradient Norm after: 10.86903349924126
Epoch 1809/10000, Prediction Accuracy = 60.739999999999995%, Loss = 0.46940307021141053
Epoch: 1809, Batch Gradient Norm: 11.096990029520022
Epoch: 1809, Batch Gradient Norm after: 11.096990029520022
Epoch 1810/10000, Prediction Accuracy = 60.786%, Loss = 0.47134953141212466
Epoch: 1810, Batch Gradient Norm: 13.361824173962118
Epoch: 1810, Batch Gradient Norm after: 13.361824173962118
Epoch 1811/10000, Prediction Accuracy = 60.866%, Loss = 0.4739047050476074
Epoch: 1811, Batch Gradient Norm: 11.116033034977063
Epoch: 1811, Batch Gradient Norm after: 11.116033034977063
Epoch 1812/10000, Prediction Accuracy = 60.81999999999999%, Loss = 0.4697280049324036
Epoch: 1812, Batch Gradient Norm: 9.984331245419488
Epoch: 1812, Batch Gradient Norm after: 9.984331245419488
Epoch 1813/10000, Prediction Accuracy = 60.824%, Loss = 0.4658362567424774
Epoch: 1813, Batch Gradient Norm: 12.271413180518046
Epoch: 1813, Batch Gradient Norm after: 12.271413180518046
Epoch 1814/10000, Prediction Accuracy = 60.746%, Loss = 0.469067370891571
Epoch: 1814, Batch Gradient Norm: 12.435069908142225
Epoch: 1814, Batch Gradient Norm after: 12.435069908142225
Epoch 1815/10000, Prediction Accuracy = 60.834%, Loss = 0.46982610821723936
Epoch: 1815, Batch Gradient Norm: 11.244053005898971
Epoch: 1815, Batch Gradient Norm after: 11.244053005898971
Epoch 1816/10000, Prediction Accuracy = 60.876%, Loss = 0.470025771856308
Epoch: 1816, Batch Gradient Norm: 11.501441316360706
Epoch: 1816, Batch Gradient Norm after: 11.501441316360706
Epoch 1817/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.46980170011520384
Epoch: 1817, Batch Gradient Norm: 11.275988112610603
Epoch: 1817, Batch Gradient Norm after: 11.275988112610603
Epoch 1818/10000, Prediction Accuracy = 60.826%, Loss = 0.4691610217094421
Epoch: 1818, Batch Gradient Norm: 10.688461453249554
Epoch: 1818, Batch Gradient Norm after: 10.688461453249554
Epoch 1819/10000, Prediction Accuracy = 60.870000000000005%, Loss = 0.4671732664108276
Epoch: 1819, Batch Gradient Norm: 10.509923207955561
Epoch: 1819, Batch Gradient Norm after: 10.509923207955561
Epoch 1820/10000, Prediction Accuracy = 60.867999999999995%, Loss = 0.4680486977100372
Epoch: 1820, Batch Gradient Norm: 12.526000078297985
Epoch: 1820, Batch Gradient Norm after: 12.526000078297985
Epoch 1821/10000, Prediction Accuracy = 60.82000000000001%, Loss = 0.4726918458938599
Epoch: 1821, Batch Gradient Norm: 14.310903297657635
Epoch: 1821, Batch Gradient Norm after: 14.310903297657635
Epoch 1822/10000, Prediction Accuracy = 60.92%, Loss = 0.47572410106658936
Epoch: 1822, Batch Gradient Norm: 10.763854044897347
Epoch: 1822, Batch Gradient Norm after: 10.763854044897347
Epoch 1823/10000, Prediction Accuracy = 60.806000000000004%, Loss = 0.46796895265579225
Epoch: 1823, Batch Gradient Norm: 9.420383805280476
Epoch: 1823, Batch Gradient Norm after: 9.420383805280476
Epoch 1824/10000, Prediction Accuracy = 60.774%, Loss = 0.4654246509075165
Epoch: 1824, Batch Gradient Norm: 8.758927169753163
Epoch: 1824, Batch Gradient Norm after: 8.758927169753163
Epoch 1825/10000, Prediction Accuracy = 60.85600000000001%, Loss = 0.46358197927474976
Epoch: 1825, Batch Gradient Norm: 9.296362516182139
Epoch: 1825, Batch Gradient Norm after: 9.296362516182139
Epoch 1826/10000, Prediction Accuracy = 60.798%, Loss = 0.4649216651916504
Epoch: 1826, Batch Gradient Norm: 10.873928995086006
Epoch: 1826, Batch Gradient Norm after: 10.873928995086006
Epoch 1827/10000, Prediction Accuracy = 60.85999999999999%, Loss = 0.4677804112434387
Epoch: 1827, Batch Gradient Norm: 11.258686369048625
Epoch: 1827, Batch Gradient Norm after: 11.258686369048625
Epoch 1828/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.4688725292682648
Epoch: 1828, Batch Gradient Norm: 13.803622450740408
Epoch: 1828, Batch Gradient Norm after: 13.803622450740408
Epoch 1829/10000, Prediction Accuracy = 60.8%, Loss = 0.47259122133255005
Epoch: 1829, Batch Gradient Norm: 10.934080290064417
Epoch: 1829, Batch Gradient Norm after: 10.934080290064417
Epoch 1830/10000, Prediction Accuracy = 60.762%, Loss = 0.47108811140060425
Epoch: 1830, Batch Gradient Norm: 14.08279807077401
Epoch: 1830, Batch Gradient Norm after: 14.08279807077401
Epoch 1831/10000, Prediction Accuracy = 60.822%, Loss = 0.4742557525634766
Epoch: 1831, Batch Gradient Norm: 12.595229374316078
Epoch: 1831, Batch Gradient Norm after: 12.595229374316078
Epoch 1832/10000, Prediction Accuracy = 60.876%, Loss = 0.4722446739673615
Epoch: 1832, Batch Gradient Norm: 12.218075840833308
Epoch: 1832, Batch Gradient Norm after: 12.218075840833308
Epoch 1833/10000, Prediction Accuracy = 60.812%, Loss = 0.4701404571533203
Epoch: 1833, Batch Gradient Norm: 7.63899454387509
Epoch: 1833, Batch Gradient Norm after: 7.63899454387509
Epoch 1834/10000, Prediction Accuracy = 60.895999999999994%, Loss = 0.46365965008735655
Epoch: 1834, Batch Gradient Norm: 7.990124558783484
Epoch: 1834, Batch Gradient Norm after: 7.990124558783484
Epoch 1835/10000, Prediction Accuracy = 60.838%, Loss = 0.4636189043521881
Epoch: 1835, Batch Gradient Norm: 10.235941375618673
Epoch: 1835, Batch Gradient Norm after: 10.235941375618673
Epoch 1836/10000, Prediction Accuracy = 60.852%, Loss = 0.46775396466255187
Epoch: 1836, Batch Gradient Norm: 9.93004525366184
Epoch: 1836, Batch Gradient Norm after: 9.93004525366184
Epoch 1837/10000, Prediction Accuracy = 60.81%, Loss = 0.46513773798942565
Epoch: 1837, Batch Gradient Norm: 12.312675470550642
Epoch: 1837, Batch Gradient Norm after: 12.312675470550642
Epoch 1838/10000, Prediction Accuracy = 60.952%, Loss = 0.47012112736701966
Epoch: 1838, Batch Gradient Norm: 12.627533057658562
Epoch: 1838, Batch Gradient Norm after: 12.627533057658562
Epoch 1839/10000, Prediction Accuracy = 60.866%, Loss = 0.47306249141693113
Epoch: 1839, Batch Gradient Norm: 12.415205341208011
Epoch: 1839, Batch Gradient Norm after: 12.415205341208011
Epoch 1840/10000, Prediction Accuracy = 60.86999999999999%, Loss = 0.47026174068450927
Epoch: 1840, Batch Gradient Norm: 11.837022145598876
Epoch: 1840, Batch Gradient Norm after: 11.837022145598876
Epoch 1841/10000, Prediction Accuracy = 60.85%, Loss = 0.46715832948684693
Epoch: 1841, Batch Gradient Norm: 11.207389820973251
Epoch: 1841, Batch Gradient Norm after: 11.207389820973251
Epoch 1842/10000, Prediction Accuracy = 60.718%, Loss = 0.4664566874504089
Epoch: 1842, Batch Gradient Norm: 11.735953333250402
Epoch: 1842, Batch Gradient Norm after: 11.735953333250402
Epoch 1843/10000, Prediction Accuracy = 60.85999999999999%, Loss = 0.4679184079170227
Epoch: 1843, Batch Gradient Norm: 12.560491598027387
Epoch: 1843, Batch Gradient Norm after: 12.560491598027387
Epoch 1844/10000, Prediction Accuracy = 60.85600000000001%, Loss = 0.47115541696548463
Epoch: 1844, Batch Gradient Norm: 12.946829156391841
Epoch: 1844, Batch Gradient Norm after: 12.946829156391841
Epoch 1845/10000, Prediction Accuracy = 60.864%, Loss = 0.4702854335308075
Epoch: 1845, Batch Gradient Norm: 11.753891287337673
Epoch: 1845, Batch Gradient Norm after: 11.753891287337673
Epoch 1846/10000, Prediction Accuracy = 60.90599999999999%, Loss = 0.4690313398838043
Epoch: 1846, Batch Gradient Norm: 11.941444773920772
Epoch: 1846, Batch Gradient Norm after: 11.941444773920772
Epoch 1847/10000, Prediction Accuracy = 60.874%, Loss = 0.46861469745635986
Epoch: 1847, Batch Gradient Norm: 10.636440657189322
Epoch: 1847, Batch Gradient Norm after: 10.636440657189322
Epoch 1848/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.46455042362213134
Epoch: 1848, Batch Gradient Norm: 12.593914296695244
Epoch: 1848, Batch Gradient Norm after: 12.593914296695244
Epoch 1849/10000, Prediction Accuracy = 60.898%, Loss = 0.47012795209884645
Epoch: 1849, Batch Gradient Norm: 11.427641133881943
Epoch: 1849, Batch Gradient Norm after: 11.427641133881943
Epoch 1850/10000, Prediction Accuracy = 60.924%, Loss = 0.4690063953399658
Epoch: 1850, Batch Gradient Norm: 10.501162755987606
Epoch: 1850, Batch Gradient Norm after: 10.501162755987606
Epoch 1851/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.46518421173095703
Epoch: 1851, Batch Gradient Norm: 9.935468685087644
Epoch: 1851, Batch Gradient Norm after: 9.935468685087644
Epoch 1852/10000, Prediction Accuracy = 60.854000000000006%, Loss = 0.4644163906574249
Epoch: 1852, Batch Gradient Norm: 10.322757060719505
Epoch: 1852, Batch Gradient Norm after: 10.322757060719505
Epoch 1853/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.4649613559246063
Epoch: 1853, Batch Gradient Norm: 11.635660651461242
Epoch: 1853, Batch Gradient Norm after: 11.635660651461242
Epoch 1854/10000, Prediction Accuracy = 60.82000000000001%, Loss = 0.468474292755127
Epoch: 1854, Batch Gradient Norm: 9.223646299221112
Epoch: 1854, Batch Gradient Norm after: 9.223646299221112
Epoch 1855/10000, Prediction Accuracy = 60.872%, Loss = 0.4626133441925049
Epoch: 1855, Batch Gradient Norm: 12.972676329722809
Epoch: 1855, Batch Gradient Norm after: 12.972676329722809
Epoch 1856/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.470857572555542
Epoch: 1856, Batch Gradient Norm: 11.479938386369172
Epoch: 1856, Batch Gradient Norm after: 11.479938386369172
Epoch 1857/10000, Prediction Accuracy = 60.864%, Loss = 0.46678525805473325
Epoch: 1857, Batch Gradient Norm: 9.276640591849963
Epoch: 1857, Batch Gradient Norm after: 9.276640591849963
Epoch 1858/10000, Prediction Accuracy = 60.854000000000006%, Loss = 0.4629914343357086
Epoch: 1858, Batch Gradient Norm: 9.258488343364503
Epoch: 1858, Batch Gradient Norm after: 9.258488343364503
Epoch 1859/10000, Prediction Accuracy = 60.866%, Loss = 0.46379620432853697
Epoch: 1859, Batch Gradient Norm: 12.150888239900945
Epoch: 1859, Batch Gradient Norm after: 12.150888239900945
Epoch 1860/10000, Prediction Accuracy = 60.830000000000005%, Loss = 0.46777546405792236
Epoch: 1860, Batch Gradient Norm: 10.757772797196695
Epoch: 1860, Batch Gradient Norm after: 10.757772797196695
Epoch 1861/10000, Prediction Accuracy = 60.85%, Loss = 0.46639404296875
Epoch: 1861, Batch Gradient Norm: 14.577700325609573
Epoch: 1861, Batch Gradient Norm after: 14.577700325609573
Epoch 1862/10000, Prediction Accuracy = 60.884%, Loss = 0.47599278688430785
Epoch: 1862, Batch Gradient Norm: 13.776901643602164
Epoch: 1862, Batch Gradient Norm after: 13.776901643602164
Epoch 1863/10000, Prediction Accuracy = 60.902%, Loss = 0.473917156457901
Epoch: 1863, Batch Gradient Norm: 12.92068493114345
Epoch: 1863, Batch Gradient Norm after: 12.92068493114345
Epoch 1864/10000, Prediction Accuracy = 60.86999999999999%, Loss = 0.4697878062725067
Epoch: 1864, Batch Gradient Norm: 11.65653337558395
Epoch: 1864, Batch Gradient Norm after: 11.65653337558395
Epoch 1865/10000, Prediction Accuracy = 60.85600000000001%, Loss = 0.46902341246604917
Epoch: 1865, Batch Gradient Norm: 12.779401902138579
Epoch: 1865, Batch Gradient Norm after: 12.779401902138579
Epoch 1866/10000, Prediction Accuracy = 60.82199999999999%, Loss = 0.4694510042667389
Epoch: 1866, Batch Gradient Norm: 11.575712558081332
Epoch: 1866, Batch Gradient Norm after: 11.575712558081332
Epoch 1867/10000, Prediction Accuracy = 60.944%, Loss = 0.46781298518180847
Epoch: 1867, Batch Gradient Norm: 13.082307815025532
Epoch: 1867, Batch Gradient Norm after: 13.082307815025532
Epoch 1868/10000, Prediction Accuracy = 60.786%, Loss = 0.46997442841529846
Epoch: 1868, Batch Gradient Norm: 11.350772289765173
Epoch: 1868, Batch Gradient Norm after: 11.350772289765173
Epoch 1869/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.467479807138443
Epoch: 1869, Batch Gradient Norm: 12.061791873157292
Epoch: 1869, Batch Gradient Norm after: 12.061791873157292
Epoch 1870/10000, Prediction Accuracy = 60.839999999999996%, Loss = 0.46883434653282163
Epoch: 1870, Batch Gradient Norm: 10.83564267918998
Epoch: 1870, Batch Gradient Norm after: 10.83564267918998
Epoch 1871/10000, Prediction Accuracy = 60.83%, Loss = 0.4662084698677063
Epoch: 1871, Batch Gradient Norm: 10.662479594819827
Epoch: 1871, Batch Gradient Norm after: 10.662479594819827
Epoch 1872/10000, Prediction Accuracy = 60.948%, Loss = 0.4665513217449188
Epoch: 1872, Batch Gradient Norm: 9.876559370943362
Epoch: 1872, Batch Gradient Norm after: 9.876559370943362
Epoch 1873/10000, Prediction Accuracy = 60.866%, Loss = 0.46425736546516416
Epoch: 1873, Batch Gradient Norm: 7.470042746788472
Epoch: 1873, Batch Gradient Norm after: 7.470042746788472
Epoch 1874/10000, Prediction Accuracy = 60.918000000000006%, Loss = 0.46025179624557494
Epoch: 1874, Batch Gradient Norm: 11.431986236390353
Epoch: 1874, Batch Gradient Norm after: 11.431986236390353
Epoch 1875/10000, Prediction Accuracy = 60.882000000000005%, Loss = 0.46709818840026857
Epoch: 1875, Batch Gradient Norm: 11.829069067959246
Epoch: 1875, Batch Gradient Norm after: 11.829069067959246
Epoch 1876/10000, Prediction Accuracy = 60.86800000000001%, Loss = 0.46616528630256654
Epoch: 1876, Batch Gradient Norm: 11.824274602589906
Epoch: 1876, Batch Gradient Norm after: 11.824274602589906
Epoch 1877/10000, Prediction Accuracy = 60.831999999999994%, Loss = 0.46727011203765867
Epoch: 1877, Batch Gradient Norm: 12.105854239402277
Epoch: 1877, Batch Gradient Norm after: 12.105854239402277
Epoch 1878/10000, Prediction Accuracy = 60.85200000000001%, Loss = 0.46846439242362975
Epoch: 1878, Batch Gradient Norm: 13.319780341682051
Epoch: 1878, Batch Gradient Norm after: 13.319780341682051
Epoch 1879/10000, Prediction Accuracy = 60.852%, Loss = 0.4721978843212128
Epoch: 1879, Batch Gradient Norm: 10.79003504160357
Epoch: 1879, Batch Gradient Norm after: 10.79003504160357
Epoch 1880/10000, Prediction Accuracy = 60.8%, Loss = 0.4668248951435089
Epoch: 1880, Batch Gradient Norm: 9.876154428023012
Epoch: 1880, Batch Gradient Norm after: 9.876154428023012
Epoch 1881/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.4630555987358093
Epoch: 1881, Batch Gradient Norm: 10.801812653966941
Epoch: 1881, Batch Gradient Norm after: 10.801812653966941
Epoch 1882/10000, Prediction Accuracy = 60.836%, Loss = 0.46406373381614685
Epoch: 1882, Batch Gradient Norm: 9.84861510069094
Epoch: 1882, Batch Gradient Norm after: 9.84861510069094
Epoch 1883/10000, Prediction Accuracy = 60.9%, Loss = 0.46164731979370116
Epoch: 1883, Batch Gradient Norm: 10.846622300296913
Epoch: 1883, Batch Gradient Norm after: 10.846622300296913
Epoch 1884/10000, Prediction Accuracy = 60.9%, Loss = 0.4637749373912811
Epoch: 1884, Batch Gradient Norm: 11.30870303188264
Epoch: 1884, Batch Gradient Norm after: 11.30870303188264
Epoch 1885/10000, Prediction Accuracy = 60.85799999999999%, Loss = 0.4654851138591766
Epoch: 1885, Batch Gradient Norm: 11.338774360539894
Epoch: 1885, Batch Gradient Norm after: 11.338774360539894
Epoch 1886/10000, Prediction Accuracy = 60.854%, Loss = 0.46580745577812194
Epoch: 1886, Batch Gradient Norm: 12.563383727668997
Epoch: 1886, Batch Gradient Norm after: 12.563383727668997
Epoch 1887/10000, Prediction Accuracy = 60.814%, Loss = 0.4714656829833984
Epoch: 1887, Batch Gradient Norm: 9.45632809939529
Epoch: 1887, Batch Gradient Norm after: 9.45632809939529
Epoch 1888/10000, Prediction Accuracy = 60.886%, Loss = 0.46256816387176514
Epoch: 1888, Batch Gradient Norm: 9.557329278164884
Epoch: 1888, Batch Gradient Norm after: 9.557329278164884
Epoch 1889/10000, Prediction Accuracy = 60.96600000000001%, Loss = 0.4633018136024475
Epoch: 1889, Batch Gradient Norm: 11.740259701460978
Epoch: 1889, Batch Gradient Norm after: 11.740259701460978
Epoch 1890/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.46722120642662046
Epoch: 1890, Batch Gradient Norm: 12.886791922402145
Epoch: 1890, Batch Gradient Norm after: 12.886791922402145
Epoch 1891/10000, Prediction Accuracy = 60.902%, Loss = 0.46664523482322695
Epoch: 1891, Batch Gradient Norm: 10.937775855813284
Epoch: 1891, Batch Gradient Norm after: 10.937775855813284
Epoch 1892/10000, Prediction Accuracy = 60.876%, Loss = 0.46409526467323303
Epoch: 1892, Batch Gradient Norm: 11.675277050919535
Epoch: 1892, Batch Gradient Norm after: 11.675277050919535
Epoch 1893/10000, Prediction Accuracy = 60.924%, Loss = 0.463837206363678
Epoch: 1893, Batch Gradient Norm: 9.151391144264668
Epoch: 1893, Batch Gradient Norm after: 9.151391144264668
Epoch 1894/10000, Prediction Accuracy = 60.94599999999999%, Loss = 0.46126739382743837
Epoch: 1894, Batch Gradient Norm: 7.810092276108571
Epoch: 1894, Batch Gradient Norm after: 7.810092276108571
Epoch 1895/10000, Prediction Accuracy = 60.97600000000001%, Loss = 0.46007452011108396
Epoch: 1895, Batch Gradient Norm: 10.503558479627797
Epoch: 1895, Batch Gradient Norm after: 10.503558479627797
Epoch 1896/10000, Prediction Accuracy = 60.87199999999999%, Loss = 0.4627845048904419
Epoch: 1896, Batch Gradient Norm: 12.445086027630477
Epoch: 1896, Batch Gradient Norm after: 12.445086027630477
Epoch 1897/10000, Prediction Accuracy = 60.818000000000005%, Loss = 0.4663350522518158
Epoch: 1897, Batch Gradient Norm: 11.506579921855918
Epoch: 1897, Batch Gradient Norm after: 11.506579921855918
Epoch 1898/10000, Prediction Accuracy = 60.886%, Loss = 0.4632080435752869
Epoch: 1898, Batch Gradient Norm: 12.263843392895415
Epoch: 1898, Batch Gradient Norm after: 12.263843392895415
Epoch 1899/10000, Prediction Accuracy = 60.96%, Loss = 0.4659146904945374
Epoch: 1899, Batch Gradient Norm: 13.171106331354524
Epoch: 1899, Batch Gradient Norm after: 13.171106331354524
Epoch 1900/10000, Prediction Accuracy = 60.94000000000001%, Loss = 0.46851900815963743
Epoch: 1900, Batch Gradient Norm: 10.673191670619591
Epoch: 1900, Batch Gradient Norm after: 10.673191670619591
Epoch 1901/10000, Prediction Accuracy = 60.9%, Loss = 0.46392964124679564
Epoch: 1901, Batch Gradient Norm: 10.169869490271276
Epoch: 1901, Batch Gradient Norm after: 10.169869490271276
Epoch 1902/10000, Prediction Accuracy = 60.934000000000005%, Loss = 0.4638248920440674
Epoch: 1902, Batch Gradient Norm: 8.97416680346245
Epoch: 1902, Batch Gradient Norm after: 8.97416680346245
Epoch 1903/10000, Prediction Accuracy = 60.842%, Loss = 0.46120513081550596
Epoch: 1903, Batch Gradient Norm: 9.42927716008758
Epoch: 1903, Batch Gradient Norm after: 9.42927716008758
Epoch 1904/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.4607710301876068
Epoch: 1904, Batch Gradient Norm: 9.939015313991936
Epoch: 1904, Batch Gradient Norm after: 9.939015313991936
Epoch 1905/10000, Prediction Accuracy = 60.826%, Loss = 0.4619417071342468
Epoch: 1905, Batch Gradient Norm: 10.985792827871855
Epoch: 1905, Batch Gradient Norm after: 10.985792827871855
Epoch 1906/10000, Prediction Accuracy = 60.830000000000005%, Loss = 0.46425479650497437
Epoch: 1906, Batch Gradient Norm: 10.47225082140643
Epoch: 1906, Batch Gradient Norm after: 10.47225082140643
Epoch 1907/10000, Prediction Accuracy = 60.94200000000001%, Loss = 0.4630822122097015
Epoch: 1907, Batch Gradient Norm: 9.965399664882243
Epoch: 1907, Batch Gradient Norm after: 9.965399664882243
Epoch 1908/10000, Prediction Accuracy = 60.89200000000001%, Loss = 0.46062847375869753
Epoch: 1908, Batch Gradient Norm: 10.907960710121197
Epoch: 1908, Batch Gradient Norm after: 10.907960710121197
Epoch 1909/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.4647148013114929
Epoch: 1909, Batch Gradient Norm: 11.378244149514948
Epoch: 1909, Batch Gradient Norm after: 11.378244149514948
Epoch 1910/10000, Prediction Accuracy = 60.842000000000006%, Loss = 0.4622985601425171
Epoch: 1910, Batch Gradient Norm: 11.224361311591016
Epoch: 1910, Batch Gradient Norm after: 11.224361311591016
Epoch 1911/10000, Prediction Accuracy = 60.878%, Loss = 0.4635850667953491
Epoch: 1911, Batch Gradient Norm: 10.30207309681084
Epoch: 1911, Batch Gradient Norm after: 10.30207309681084
Epoch 1912/10000, Prediction Accuracy = 60.84400000000001%, Loss = 0.4618121862411499
Epoch: 1912, Batch Gradient Norm: 10.816407405527679
Epoch: 1912, Batch Gradient Norm after: 10.816407405527679
Epoch 1913/10000, Prediction Accuracy = 60.89200000000001%, Loss = 0.4627051591873169
Epoch: 1913, Batch Gradient Norm: 8.81245653973973
Epoch: 1913, Batch Gradient Norm after: 8.81245653973973
Epoch 1914/10000, Prediction Accuracy = 60.894000000000005%, Loss = 0.46002922058105467
Epoch: 1914, Batch Gradient Norm: 10.937881369354578
Epoch: 1914, Batch Gradient Norm after: 10.937881369354578
Epoch 1915/10000, Prediction Accuracy = 60.888%, Loss = 0.4644037365913391
Epoch: 1915, Batch Gradient Norm: 10.88258712484313
Epoch: 1915, Batch Gradient Norm after: 10.88258712484313
Epoch 1916/10000, Prediction Accuracy = 60.95%, Loss = 0.46231973767280576
Epoch: 1916, Batch Gradient Norm: 8.613700475060039
Epoch: 1916, Batch Gradient Norm after: 8.613700475060039
Epoch 1917/10000, Prediction Accuracy = 60.855999999999995%, Loss = 0.45971094369888305
Epoch: 1917, Batch Gradient Norm: 9.582623953993167
Epoch: 1917, Batch Gradient Norm after: 9.582623953993167
Epoch 1918/10000, Prediction Accuracy = 60.89399999999999%, Loss = 0.45975542068481445
Epoch: 1918, Batch Gradient Norm: 11.028815854120634
Epoch: 1918, Batch Gradient Norm after: 11.028815854120634
Epoch 1919/10000, Prediction Accuracy = 60.94199999999999%, Loss = 0.4622872531414032
Epoch: 1919, Batch Gradient Norm: 10.667365050384465
Epoch: 1919, Batch Gradient Norm after: 10.667365050384465
Epoch 1920/10000, Prediction Accuracy = 60.90599999999999%, Loss = 0.46230649948120117
Epoch: 1920, Batch Gradient Norm: 9.754361531116503
Epoch: 1920, Batch Gradient Norm after: 9.754361531116503
Epoch 1921/10000, Prediction Accuracy = 60.948%, Loss = 0.4612961232662201
Epoch: 1921, Batch Gradient Norm: 9.356560688075016
Epoch: 1921, Batch Gradient Norm after: 9.356560688075016
Epoch 1922/10000, Prediction Accuracy = 60.94599999999999%, Loss = 0.4590278923511505
Epoch: 1922, Batch Gradient Norm: 8.620371833047447
Epoch: 1922, Batch Gradient Norm after: 8.620371833047447
Epoch 1923/10000, Prediction Accuracy = 60.944%, Loss = 0.46039440035820006
Epoch: 1923, Batch Gradient Norm: 9.661134177886641
Epoch: 1923, Batch Gradient Norm after: 9.661134177886641
Epoch 1924/10000, Prediction Accuracy = 60.992%, Loss = 0.4612902462482452
Epoch: 1924, Batch Gradient Norm: 10.741498948912852
Epoch: 1924, Batch Gradient Norm after: 10.741498948912852
Epoch 1925/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.46180384159088134
Epoch: 1925, Batch Gradient Norm: 10.390118677829218
Epoch: 1925, Batch Gradient Norm after: 10.390118677829218
Epoch 1926/10000, Prediction Accuracy = 60.89%, Loss = 0.4606711745262146
Epoch: 1926, Batch Gradient Norm: 9.325851602144441
Epoch: 1926, Batch Gradient Norm after: 9.325851602144441
Epoch 1927/10000, Prediction Accuracy = 60.827999999999996%, Loss = 0.46030896306037905
Epoch: 1927, Batch Gradient Norm: 8.124587330910952
Epoch: 1927, Batch Gradient Norm after: 8.124587330910952
Epoch 1928/10000, Prediction Accuracy = 60.891999999999996%, Loss = 0.4562671482563019
Epoch: 1928, Batch Gradient Norm: 9.98468832484185
Epoch: 1928, Batch Gradient Norm after: 9.98468832484185
Epoch 1929/10000, Prediction Accuracy = 60.912%, Loss = 0.45991736054420473
Epoch: 1929, Batch Gradient Norm: 10.628868310830425
Epoch: 1929, Batch Gradient Norm after: 10.628868310830425
Epoch 1930/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.4604078412055969
Epoch: 1930, Batch Gradient Norm: 11.741505773765606
Epoch: 1930, Batch Gradient Norm after: 11.741505773765606
Epoch 1931/10000, Prediction Accuracy = 60.88199999999999%, Loss = 0.46361268758773805
Epoch: 1931, Batch Gradient Norm: 9.8783742142072
Epoch: 1931, Batch Gradient Norm after: 9.8783742142072
Epoch 1932/10000, Prediction Accuracy = 60.846000000000004%, Loss = 0.4594478905200958
Epoch: 1932, Batch Gradient Norm: 9.87403339112801
Epoch: 1932, Batch Gradient Norm after: 9.87403339112801
Epoch 1933/10000, Prediction Accuracy = 60.862%, Loss = 0.4600511729717255
Epoch: 1933, Batch Gradient Norm: 12.376175884575948
Epoch: 1933, Batch Gradient Norm after: 12.376175884575948
Epoch 1934/10000, Prediction Accuracy = 60.988%, Loss = 0.46323196291923524
Epoch: 1934, Batch Gradient Norm: 14.149703961069102
Epoch: 1934, Batch Gradient Norm after: 14.149703961069102
Epoch 1935/10000, Prediction Accuracy = 60.96999999999999%, Loss = 0.4671199440956116
Epoch: 1935, Batch Gradient Norm: 15.916609542521003
Epoch: 1935, Batch Gradient Norm after: 15.916609542521003
Epoch 1936/10000, Prediction Accuracy = 60.894000000000005%, Loss = 0.47052609324455263
Epoch: 1936, Batch Gradient Norm: 12.731111391157743
Epoch: 1936, Batch Gradient Norm after: 12.731111391157743
Epoch 1937/10000, Prediction Accuracy = 60.872%, Loss = 0.46457965970039367
Epoch: 1937, Batch Gradient Norm: 10.333594795175157
Epoch: 1937, Batch Gradient Norm after: 10.333594795175157
Epoch 1938/10000, Prediction Accuracy = 60.98%, Loss = 0.46073746085166933
Epoch: 1938, Batch Gradient Norm: 9.406272878205014
Epoch: 1938, Batch Gradient Norm after: 9.406272878205014
Epoch 1939/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.4590726912021637
Epoch: 1939, Batch Gradient Norm: 8.152452136139003
Epoch: 1939, Batch Gradient Norm after: 8.152452136139003
Epoch 1940/10000, Prediction Accuracy = 61.038%, Loss = 0.45692330598831177
Epoch: 1940, Batch Gradient Norm: 10.006053839574621
Epoch: 1940, Batch Gradient Norm after: 10.006053839574621
Epoch 1941/10000, Prediction Accuracy = 60.984%, Loss = 0.4605616569519043
Epoch: 1941, Batch Gradient Norm: 11.21970700483268
Epoch: 1941, Batch Gradient Norm after: 11.21970700483268
Epoch 1942/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.4624963700771332
Epoch: 1942, Batch Gradient Norm: 10.945230902466914
Epoch: 1942, Batch Gradient Norm after: 10.945230902466914
Epoch 1943/10000, Prediction Accuracy = 60.924%, Loss = 0.4596336901187897
Epoch: 1943, Batch Gradient Norm: 12.496848413417384
Epoch: 1943, Batch Gradient Norm after: 12.496848413417384
Epoch 1944/10000, Prediction Accuracy = 60.99400000000001%, Loss = 0.46446707248687746
Epoch: 1944, Batch Gradient Norm: 10.803261123876858
Epoch: 1944, Batch Gradient Norm after: 10.803261123876858
Epoch 1945/10000, Prediction Accuracy = 60.94%, Loss = 0.4609213173389435
Epoch: 1945, Batch Gradient Norm: 13.813325912732429
Epoch: 1945, Batch Gradient Norm after: 13.813325912732429
Epoch 1946/10000, Prediction Accuracy = 60.912%, Loss = 0.46574693322181704
Epoch: 1946, Batch Gradient Norm: 15.595599305805191
Epoch: 1946, Batch Gradient Norm after: 15.595599305805191
Epoch 1947/10000, Prediction Accuracy = 60.962%, Loss = 0.469566285610199
Epoch: 1947, Batch Gradient Norm: 15.011382038134752
Epoch: 1947, Batch Gradient Norm after: 15.011382038134752
Epoch 1948/10000, Prediction Accuracy = 60.934000000000005%, Loss = 0.4693653881549835
Epoch: 1948, Batch Gradient Norm: 13.928810477440924
Epoch: 1948, Batch Gradient Norm after: 13.928810477440924
Epoch 1949/10000, Prediction Accuracy = 60.996%, Loss = 0.4672726333141327
Epoch: 1949, Batch Gradient Norm: 14.050295389086633
Epoch: 1949, Batch Gradient Norm after: 14.050295389086633
Epoch 1950/10000, Prediction Accuracy = 60.896%, Loss = 0.4669828534126282
Epoch: 1950, Batch Gradient Norm: 14.649806570118045
Epoch: 1950, Batch Gradient Norm after: 14.649806570118045
Epoch 1951/10000, Prediction Accuracy = 60.984%, Loss = 0.4673572063446045
Epoch: 1951, Batch Gradient Norm: 14.080124615439022
Epoch: 1951, Batch Gradient Norm after: 14.080124615439022
Epoch 1952/10000, Prediction Accuracy = 60.862%, Loss = 0.46508819460868833
Epoch: 1952, Batch Gradient Norm: 13.239060269691878
Epoch: 1952, Batch Gradient Norm after: 13.239060269691878
Epoch 1953/10000, Prediction Accuracy = 60.912%, Loss = 0.46567326188087466
Epoch: 1953, Batch Gradient Norm: 11.528139604849072
Epoch: 1953, Batch Gradient Norm after: 11.528139604849072
Epoch 1954/10000, Prediction Accuracy = 61.028%, Loss = 0.4648971140384674
Epoch: 1954, Batch Gradient Norm: 13.138745707523894
Epoch: 1954, Batch Gradient Norm after: 13.138745707523894
Epoch 1955/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.46452111601829527
Epoch: 1955, Batch Gradient Norm: 10.575450373491973
Epoch: 1955, Batch Gradient Norm after: 10.575450373491973
Epoch 1956/10000, Prediction Accuracy = 60.916%, Loss = 0.4614170789718628
Epoch: 1956, Batch Gradient Norm: 11.167940474384897
Epoch: 1956, Batch Gradient Norm after: 11.167940474384897
Epoch 1957/10000, Prediction Accuracy = 60.916%, Loss = 0.4613195717334747
Epoch: 1957, Batch Gradient Norm: 12.587293947053993
Epoch: 1957, Batch Gradient Norm after: 12.587293947053993
Epoch 1958/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.46208955645561217
Epoch: 1958, Batch Gradient Norm: 10.721527644531914
Epoch: 1958, Batch Gradient Norm after: 10.721527644531914
Epoch 1959/10000, Prediction Accuracy = 60.90599999999999%, Loss = 0.4620418190956116
Epoch: 1959, Batch Gradient Norm: 11.296826279160129
Epoch: 1959, Batch Gradient Norm after: 11.296826279160129
Epoch 1960/10000, Prediction Accuracy = 61.028%, Loss = 0.46133071184158325
Epoch: 1960, Batch Gradient Norm: 11.974970691689752
Epoch: 1960, Batch Gradient Norm after: 11.974970691689752
Epoch 1961/10000, Prediction Accuracy = 60.952%, Loss = 0.46187338829040525
Epoch: 1961, Batch Gradient Norm: 10.929026025591707
Epoch: 1961, Batch Gradient Norm after: 10.929026025591707
Epoch 1962/10000, Prediction Accuracy = 61.053999999999995%, Loss = 0.45980396270751955
Epoch: 1962, Batch Gradient Norm: 11.420018103030865
Epoch: 1962, Batch Gradient Norm after: 11.420018103030865
Epoch 1963/10000, Prediction Accuracy = 61.007999999999996%, Loss = 0.46113510727882384
Epoch: 1963, Batch Gradient Norm: 11.377721918358201
Epoch: 1963, Batch Gradient Norm after: 11.377721918358201
Epoch 1964/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.46207969188690184
Epoch: 1964, Batch Gradient Norm: 8.885881066759152
Epoch: 1964, Batch Gradient Norm after: 8.885881066759152
Epoch 1965/10000, Prediction Accuracy = 60.976%, Loss = 0.4584856152534485
Epoch: 1965, Batch Gradient Norm: 8.431388040896216
Epoch: 1965, Batch Gradient Norm after: 8.431388040896216
Epoch 1966/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.4584847867488861
Epoch: 1966, Batch Gradient Norm: 10.244590319270301
Epoch: 1966, Batch Gradient Norm after: 10.244590319270301
Epoch 1967/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.4619511604309082
Epoch: 1967, Batch Gradient Norm: 13.334049710720901
Epoch: 1967, Batch Gradient Norm after: 13.334049710720901
Epoch 1968/10000, Prediction Accuracy = 60.972%, Loss = 0.4627579391002655
Epoch: 1968, Batch Gradient Norm: 13.819484181641112
Epoch: 1968, Batch Gradient Norm after: 13.819484181641112
Epoch 1969/10000, Prediction Accuracy = 61.008%, Loss = 0.4648677945137024
Epoch: 1969, Batch Gradient Norm: 13.7529982170378
Epoch: 1969, Batch Gradient Norm after: 13.7529982170378
Epoch 1970/10000, Prediction Accuracy = 60.928%, Loss = 0.463944011926651
Epoch: 1970, Batch Gradient Norm: 11.503139256556121
Epoch: 1970, Batch Gradient Norm after: 11.503139256556121
Epoch 1971/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.46208205819129944
Epoch: 1971, Batch Gradient Norm: 10.241682807813532
Epoch: 1971, Batch Gradient Norm after: 10.241682807813532
Epoch 1972/10000, Prediction Accuracy = 61.010000000000005%, Loss = 0.4598396360874176
Epoch: 1972, Batch Gradient Norm: 12.373689998278778
Epoch: 1972, Batch Gradient Norm after: 12.373689998278778
Epoch 1973/10000, Prediction Accuracy = 60.916%, Loss = 0.463906466960907
Epoch: 1973, Batch Gradient Norm: 13.76266358071027
Epoch: 1973, Batch Gradient Norm after: 13.76266358071027
Epoch 1974/10000, Prediction Accuracy = 60.94799999999999%, Loss = 0.4643598198890686
Epoch: 1974, Batch Gradient Norm: 11.876958603326704
Epoch: 1974, Batch Gradient Norm after: 11.876958603326704
Epoch 1975/10000, Prediction Accuracy = 61.004%, Loss = 0.46177309155464175
Epoch: 1975, Batch Gradient Norm: 11.220835113983657
Epoch: 1975, Batch Gradient Norm after: 11.220835113983657
Epoch 1976/10000, Prediction Accuracy = 60.92%, Loss = 0.46086490750312803
Epoch: 1976, Batch Gradient Norm: 8.694933716017403
Epoch: 1976, Batch Gradient Norm after: 8.694933716017403
Epoch 1977/10000, Prediction Accuracy = 61.004%, Loss = 0.4556641817092896
Epoch: 1977, Batch Gradient Norm: 7.938433059561826
Epoch: 1977, Batch Gradient Norm after: 7.938433059561826
Epoch 1978/10000, Prediction Accuracy = 60.89200000000001%, Loss = 0.4561802625656128
Epoch: 1978, Batch Gradient Norm: 8.026129809562097
Epoch: 1978, Batch Gradient Norm after: 8.026129809562097
Epoch 1979/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.4564251720905304
Epoch: 1979, Batch Gradient Norm: 8.802934139040092
Epoch: 1979, Batch Gradient Norm after: 8.802934139040092
Epoch 1980/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.45710024833679197
Epoch: 1980, Batch Gradient Norm: 9.850410393654556
Epoch: 1980, Batch Gradient Norm after: 9.850410393654556
Epoch 1981/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.45891910791397095
Epoch: 1981, Batch Gradient Norm: 9.720533630916833
Epoch: 1981, Batch Gradient Norm after: 9.720533630916833
Epoch 1982/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.45846027135849
Epoch: 1982, Batch Gradient Norm: 11.880600446891677
Epoch: 1982, Batch Gradient Norm after: 11.880600446891677
Epoch 1983/10000, Prediction Accuracy = 60.952%, Loss = 0.4616469144821167
Epoch: 1983, Batch Gradient Norm: 9.577568414233928
Epoch: 1983, Batch Gradient Norm after: 9.577568414233928
Epoch 1984/10000, Prediction Accuracy = 61.03000000000001%, Loss = 0.4551661252975464
Epoch: 1984, Batch Gradient Norm: 10.986237467712403
Epoch: 1984, Batch Gradient Norm after: 10.986237467712403
Epoch 1985/10000, Prediction Accuracy = 60.936%, Loss = 0.4586405515670776
Epoch: 1985, Batch Gradient Norm: 11.368822616298567
Epoch: 1985, Batch Gradient Norm after: 11.368822616298567
Epoch 1986/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.4600332260131836
Epoch: 1986, Batch Gradient Norm: 12.972879723656494
Epoch: 1986, Batch Gradient Norm after: 12.972879723656494
Epoch 1987/10000, Prediction Accuracy = 61.0%, Loss = 0.4626509130001068
Epoch: 1987, Batch Gradient Norm: 8.825021764054684
Epoch: 1987, Batch Gradient Norm after: 8.825021764054684
Epoch 1988/10000, Prediction Accuracy = 60.964%, Loss = 0.4559801697731018
Epoch: 1988, Batch Gradient Norm: 8.851498979852503
Epoch: 1988, Batch Gradient Norm after: 8.851498979852503
Epoch 1989/10000, Prediction Accuracy = 60.932%, Loss = 0.45619672536849976
Epoch: 1989, Batch Gradient Norm: 8.60059341513933
Epoch: 1989, Batch Gradient Norm after: 8.60059341513933
Epoch 1990/10000, Prediction Accuracy = 60.982000000000006%, Loss = 0.45734418630599977
Epoch: 1990, Batch Gradient Norm: 6.994883388414752
Epoch: 1990, Batch Gradient Norm after: 6.994883388414752
Epoch 1991/10000, Prediction Accuracy = 61.028000000000006%, Loss = 0.4529826283454895
Epoch: 1991, Batch Gradient Norm: 9.071631126953458
Epoch: 1991, Batch Gradient Norm after: 9.071631126953458
Epoch 1992/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.45713496804237364
Epoch: 1992, Batch Gradient Norm: 10.163306635648823
Epoch: 1992, Batch Gradient Norm after: 10.163306635648823
Epoch 1993/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.4570424973964691
Epoch: 1993, Batch Gradient Norm: 9.963220484992636
Epoch: 1993, Batch Gradient Norm after: 9.963220484992636
Epoch 1994/10000, Prediction Accuracy = 60.996%, Loss = 0.4606394648551941
Epoch: 1994, Batch Gradient Norm: 8.8770346133987
Epoch: 1994, Batch Gradient Norm after: 8.8770346133987
Epoch 1995/10000, Prediction Accuracy = 60.976%, Loss = 0.4556269347667694
Epoch: 1995, Batch Gradient Norm: 10.647214764131721
Epoch: 1995, Batch Gradient Norm after: 10.647214764131721
Epoch 1996/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.4597271502017975
Epoch: 1996, Batch Gradient Norm: 12.97575412895538
Epoch: 1996, Batch Gradient Norm after: 12.97575412895538
Epoch 1997/10000, Prediction Accuracy = 60.988%, Loss = 0.46405954360961915
Epoch: 1997, Batch Gradient Norm: 11.443155542024586
Epoch: 1997, Batch Gradient Norm after: 11.443155542024586
Epoch 1998/10000, Prediction Accuracy = 60.918000000000006%, Loss = 0.4586659371852875
Epoch: 1998, Batch Gradient Norm: 9.95898786473771
Epoch: 1998, Batch Gradient Norm after: 9.95898786473771
Epoch 1999/10000, Prediction Accuracy = 61.00600000000001%, Loss = 0.4575435876846313
Epoch: 1999, Batch Gradient Norm: 11.616241103654843
Epoch: 1999, Batch Gradient Norm after: 11.616241103654843
Epoch 2000/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.4592127501964569
Epoch: 2000, Batch Gradient Norm: 9.717089491419248
Epoch: 2000, Batch Gradient Norm after: 9.717089491419248
Epoch 2001/10000, Prediction Accuracy = 60.916%, Loss = 0.45571476221084595
Epoch: 2001, Batch Gradient Norm: 9.309962125027969
Epoch: 2001, Batch Gradient Norm after: 9.309962125027969
Epoch 2002/10000, Prediction Accuracy = 60.984%, Loss = 0.45771958231925963
Epoch: 2002, Batch Gradient Norm: 12.45383724803931
Epoch: 2002, Batch Gradient Norm after: 12.45383724803931
Epoch 2003/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.4623378932476044
Epoch: 2003, Batch Gradient Norm: 10.823087281243213
Epoch: 2003, Batch Gradient Norm after: 10.823087281243213
Epoch 2004/10000, Prediction Accuracy = 61.04%, Loss = 0.4613362789154053
Epoch: 2004, Batch Gradient Norm: 10.196745925314204
Epoch: 2004, Batch Gradient Norm after: 10.196745925314204
Epoch 2005/10000, Prediction Accuracy = 60.91600000000001%, Loss = 0.4589744985103607
Epoch: 2005, Batch Gradient Norm: 9.736055192038037
Epoch: 2005, Batch Gradient Norm after: 9.736055192038037
Epoch 2006/10000, Prediction Accuracy = 60.976%, Loss = 0.45770974159240724
Epoch: 2006, Batch Gradient Norm: 9.671511503070063
Epoch: 2006, Batch Gradient Norm after: 9.671511503070063
Epoch 2007/10000, Prediction Accuracy = 61.016000000000005%, Loss = 0.4558807373046875
Epoch: 2007, Batch Gradient Norm: 10.871297751093493
Epoch: 2007, Batch Gradient Norm after: 10.871297751093493
Epoch 2008/10000, Prediction Accuracy = 61.068%, Loss = 0.45791364908218385
Epoch: 2008, Batch Gradient Norm: 7.948661520594984
Epoch: 2008, Batch Gradient Norm after: 7.948661520594984
Epoch 2009/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.4529065668582916
Epoch: 2009, Batch Gradient Norm: 7.757067928972697
Epoch: 2009, Batch Gradient Norm after: 7.757067928972697
Epoch 2010/10000, Prediction Accuracy = 60.992%, Loss = 0.45296770334243774
Epoch: 2010, Batch Gradient Norm: 11.86266294747886
Epoch: 2010, Batch Gradient Norm after: 11.86266294747886
Epoch 2011/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.45964342951774595
Epoch: 2011, Batch Gradient Norm: 11.304707330312734
Epoch: 2011, Batch Gradient Norm after: 11.304707330312734
Epoch 2012/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.4571999728679657
Epoch: 2012, Batch Gradient Norm: 13.452611868785414
Epoch: 2012, Batch Gradient Norm after: 13.452611868785414
Epoch 2013/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.4641211271286011
Epoch: 2013, Batch Gradient Norm: 11.411097582832797
Epoch: 2013, Batch Gradient Norm after: 11.411097582832797
Epoch 2014/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.45849219560623167
Epoch: 2014, Batch Gradient Norm: 10.862098178404374
Epoch: 2014, Batch Gradient Norm after: 10.862098178404374
Epoch 2015/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.45699740648269654
Epoch: 2015, Batch Gradient Norm: 11.347514345376446
Epoch: 2015, Batch Gradient Norm after: 11.347514345376446
Epoch 2016/10000, Prediction Accuracy = 61.062%, Loss = 0.4582456350326538
Epoch: 2016, Batch Gradient Norm: 9.286819413441755
Epoch: 2016, Batch Gradient Norm after: 9.286819413441755
Epoch 2017/10000, Prediction Accuracy = 61.062%, Loss = 0.45799038410186765
Epoch: 2017, Batch Gradient Norm: 11.667554891357081
Epoch: 2017, Batch Gradient Norm after: 11.667554891357081
Epoch 2018/10000, Prediction Accuracy = 61.044%, Loss = 0.4592933177947998
Epoch: 2018, Batch Gradient Norm: 10.311723684352087
Epoch: 2018, Batch Gradient Norm after: 10.311723684352087
Epoch 2019/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.45539318919181826
Epoch: 2019, Batch Gradient Norm: 12.461398240075582
Epoch: 2019, Batch Gradient Norm after: 12.461398240075582
Epoch 2020/10000, Prediction Accuracy = 60.94%, Loss = 0.46058250665664674
Epoch: 2020, Batch Gradient Norm: 11.00871157854262
Epoch: 2020, Batch Gradient Norm after: 11.00871157854262
Epoch 2021/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.45995757579803465
Epoch: 2021, Batch Gradient Norm: 9.11353206950084
Epoch: 2021, Batch Gradient Norm after: 9.11353206950084
Epoch 2022/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.4535664141178131
Epoch: 2022, Batch Gradient Norm: 10.24247492745536
Epoch: 2022, Batch Gradient Norm after: 10.24247492745536
Epoch 2023/10000, Prediction Accuracy = 60.99399999999999%, Loss = 0.4568817675113678
Epoch: 2023, Batch Gradient Norm: 11.014990616283134
Epoch: 2023, Batch Gradient Norm after: 11.014990616283134
Epoch 2024/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.45992844700813296
Epoch: 2024, Batch Gradient Norm: 13.015382322278722
Epoch: 2024, Batch Gradient Norm after: 13.015382322278722
Epoch 2025/10000, Prediction Accuracy = 60.964%, Loss = 0.46069369912147523
Epoch: 2025, Batch Gradient Norm: 12.66653255176739
Epoch: 2025, Batch Gradient Norm after: 12.66653255176739
Epoch 2026/10000, Prediction Accuracy = 61.052%, Loss = 0.4612977683544159
Epoch: 2026, Batch Gradient Norm: 11.020935676462383
Epoch: 2026, Batch Gradient Norm after: 11.020935676462383
Epoch 2027/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.45607913136482237
Epoch: 2027, Batch Gradient Norm: 11.374761960664344
Epoch: 2027, Batch Gradient Norm after: 11.374761960664344
Epoch 2028/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.4581292450428009
Epoch: 2028, Batch Gradient Norm: 9.841453984293233
Epoch: 2028, Batch Gradient Norm after: 9.841453984293233
Epoch 2029/10000, Prediction Accuracy = 60.894000000000005%, Loss = 0.455359673500061
Epoch: 2029, Batch Gradient Norm: 11.117747970047992
Epoch: 2029, Batch Gradient Norm after: 11.117747970047992
Epoch 2030/10000, Prediction Accuracy = 61.0%, Loss = 0.45746389627456663
Epoch: 2030, Batch Gradient Norm: 11.400024318895086
Epoch: 2030, Batch Gradient Norm after: 11.400024318895086
Epoch 2031/10000, Prediction Accuracy = 61.038%, Loss = 0.45802955627441405
Epoch: 2031, Batch Gradient Norm: 13.766294813259053
Epoch: 2031, Batch Gradient Norm after: 13.766294813259053
Epoch 2032/10000, Prediction Accuracy = 60.982000000000006%, Loss = 0.46210837960243223
Epoch: 2032, Batch Gradient Norm: 11.828782670793164
Epoch: 2032, Batch Gradient Norm after: 11.828782670793164
Epoch 2033/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.4597289264202118
Epoch: 2033, Batch Gradient Norm: 14.778296839861808
Epoch: 2033, Batch Gradient Norm after: 14.778296839861808
Epoch 2034/10000, Prediction Accuracy = 60.9%, Loss = 0.46421376466751096
Epoch: 2034, Batch Gradient Norm: 11.833758071261988
Epoch: 2034, Batch Gradient Norm after: 11.833758071261988
Epoch 2035/10000, Prediction Accuracy = 61.007999999999996%, Loss = 0.4569307804107666
Epoch: 2035, Batch Gradient Norm: 10.993164338206885
Epoch: 2035, Batch Gradient Norm after: 10.993164338206885
Epoch 2036/10000, Prediction Accuracy = 60.988%, Loss = 0.45679743885993956
Epoch: 2036, Batch Gradient Norm: 12.28085692823169
Epoch: 2036, Batch Gradient Norm after: 12.28085692823169
Epoch 2037/10000, Prediction Accuracy = 60.988%, Loss = 0.45864729285240174
Epoch: 2037, Batch Gradient Norm: 11.047910889251158
Epoch: 2037, Batch Gradient Norm after: 11.047910889251158
Epoch 2038/10000, Prediction Accuracy = 61.036%, Loss = 0.456997150182724
Epoch: 2038, Batch Gradient Norm: 10.034278957851877
Epoch: 2038, Batch Gradient Norm after: 10.034278957851877
Epoch 2039/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.4548155128955841
Epoch: 2039, Batch Gradient Norm: 11.209100101479892
Epoch: 2039, Batch Gradient Norm after: 11.209100101479892
Epoch 2040/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.4570008456707001
Epoch: 2040, Batch Gradient Norm: 10.756629059776149
Epoch: 2040, Batch Gradient Norm after: 10.756629059776149
Epoch 2041/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.4579362809658051
Epoch: 2041, Batch Gradient Norm: 10.420048058783285
Epoch: 2041, Batch Gradient Norm after: 10.420048058783285
Epoch 2042/10000, Prediction Accuracy = 61.084%, Loss = 0.45501344799995425
Epoch: 2042, Batch Gradient Norm: 10.366986182387777
Epoch: 2042, Batch Gradient Norm after: 10.366986182387777
Epoch 2043/10000, Prediction Accuracy = 61.038%, Loss = 0.45562523007392886
Epoch: 2043, Batch Gradient Norm: 9.226698580915263
Epoch: 2043, Batch Gradient Norm after: 9.226698580915263
Epoch 2044/10000, Prediction Accuracy = 60.944%, Loss = 0.45372692942619325
Epoch: 2044, Batch Gradient Norm: 9.707356324333807
Epoch: 2044, Batch Gradient Norm after: 9.707356324333807
Epoch 2045/10000, Prediction Accuracy = 61.004%, Loss = 0.4554968237876892
Epoch: 2045, Batch Gradient Norm: 12.367812291016898
Epoch: 2045, Batch Gradient Norm after: 12.367812291016898
Epoch 2046/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.45883625745773315
Epoch: 2046, Batch Gradient Norm: 9.904142965037318
Epoch: 2046, Batch Gradient Norm after: 9.904142965037318
Epoch 2047/10000, Prediction Accuracy = 61.012%, Loss = 0.4532167136669159
Epoch: 2047, Batch Gradient Norm: 10.190018174325974
Epoch: 2047, Batch Gradient Norm after: 10.190018174325974
Epoch 2048/10000, Prediction Accuracy = 61.028%, Loss = 0.4544622004032135
Epoch: 2048, Batch Gradient Norm: 8.535553504967138
Epoch: 2048, Batch Gradient Norm after: 8.535553504967138
Epoch 2049/10000, Prediction Accuracy = 60.964%, Loss = 0.4544956386089325
Epoch: 2049, Batch Gradient Norm: 7.488108994412298
Epoch: 2049, Batch Gradient Norm after: 7.488108994412298
Epoch 2050/10000, Prediction Accuracy = 61.064%, Loss = 0.4519657790660858
Epoch: 2050, Batch Gradient Norm: 8.727848610979152
Epoch: 2050, Batch Gradient Norm after: 8.727848610979152
Epoch 2051/10000, Prediction Accuracy = 61.084%, Loss = 0.45530717372894286
Epoch: 2051, Batch Gradient Norm: 7.470141801146269
Epoch: 2051, Batch Gradient Norm after: 7.470141801146269
Epoch 2052/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.4510387420654297
Epoch: 2052, Batch Gradient Norm: 9.47770324405204
Epoch: 2052, Batch Gradient Norm after: 9.47770324405204
Epoch 2053/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.45403783321380614
Epoch: 2053, Batch Gradient Norm: 8.687910147033683
Epoch: 2053, Batch Gradient Norm after: 8.687910147033683
Epoch 2054/10000, Prediction Accuracy = 61.144000000000005%, Loss = 0.4511841058731079
Epoch: 2054, Batch Gradient Norm: 9.299988558597228
Epoch: 2054, Batch Gradient Norm after: 9.299988558597228
Epoch 2055/10000, Prediction Accuracy = 61.084%, Loss = 0.4539445102214813
Epoch: 2055, Batch Gradient Norm: 10.867905729238148
Epoch: 2055, Batch Gradient Norm after: 10.867905729238148
Epoch 2056/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.4547934830188751
Epoch: 2056, Batch Gradient Norm: 10.322988614120758
Epoch: 2056, Batch Gradient Norm after: 10.322988614120758
Epoch 2057/10000, Prediction Accuracy = 60.987999999999985%, Loss = 0.45232508778572084
Epoch: 2057, Batch Gradient Norm: 11.691027758390964
Epoch: 2057, Batch Gradient Norm after: 11.691027758390964
Epoch 2058/10000, Prediction Accuracy = 61.03599999999999%, Loss = 0.45751601457595825
Epoch: 2058, Batch Gradient Norm: 9.84678205894418
Epoch: 2058, Batch Gradient Norm after: 9.84678205894418
Epoch 2059/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.4549129903316498
Epoch: 2059, Batch Gradient Norm: 13.286517235423416
Epoch: 2059, Batch Gradient Norm after: 13.286517235423416
Epoch 2060/10000, Prediction Accuracy = 60.972%, Loss = 0.45998694896698
Epoch: 2060, Batch Gradient Norm: 12.541250223196153
Epoch: 2060, Batch Gradient Norm after: 12.541250223196153
Epoch 2061/10000, Prediction Accuracy = 61.024%, Loss = 0.45913116335868837
Epoch: 2061, Batch Gradient Norm: 9.405097449770716
Epoch: 2061, Batch Gradient Norm after: 9.405097449770716
Epoch 2062/10000, Prediction Accuracy = 61.064%, Loss = 0.454575914144516
Epoch: 2062, Batch Gradient Norm: 8.717833542336793
Epoch: 2062, Batch Gradient Norm after: 8.717833542336793
Epoch 2063/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.45408510565757754
Epoch: 2063, Batch Gradient Norm: 7.86305371760822
Epoch: 2063, Batch Gradient Norm after: 7.86305371760822
Epoch 2064/10000, Prediction Accuracy = 61.064%, Loss = 0.4509250223636627
Epoch: 2064, Batch Gradient Norm: 9.45672991136168
Epoch: 2064, Batch Gradient Norm after: 9.45672991136168
Epoch 2065/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.45392022132873533
Epoch: 2065, Batch Gradient Norm: 9.978228506362385
Epoch: 2065, Batch Gradient Norm after: 9.978228506362385
Epoch 2066/10000, Prediction Accuracy = 60.980000000000004%, Loss = 0.45576797127723695
Epoch: 2066, Batch Gradient Norm: 10.628650922537563
Epoch: 2066, Batch Gradient Norm after: 10.628650922537563
Epoch 2067/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.4544273018836975
Epoch: 2067, Batch Gradient Norm: 10.504264382205386
Epoch: 2067, Batch Gradient Norm after: 10.504264382205386
Epoch 2068/10000, Prediction Accuracy = 61.104%, Loss = 0.45457970499992373
Epoch: 2068, Batch Gradient Norm: 10.64086215404836
Epoch: 2068, Batch Gradient Norm after: 10.64086215404836
Epoch 2069/10000, Prediction Accuracy = 61.048%, Loss = 0.45462462306022644
Epoch: 2069, Batch Gradient Norm: 10.858286120020852
Epoch: 2069, Batch Gradient Norm after: 10.858286120020852
Epoch 2070/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.45443962812423705
Epoch: 2070, Batch Gradient Norm: 11.050111066331786
Epoch: 2070, Batch Gradient Norm after: 11.050111066331786
Epoch 2071/10000, Prediction Accuracy = 61.102%, Loss = 0.4549797773361206
Epoch: 2071, Batch Gradient Norm: 8.73278596617116
Epoch: 2071, Batch Gradient Norm after: 8.73278596617116
Epoch 2072/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.4525335907936096
Epoch: 2072, Batch Gradient Norm: 10.536640801587751
Epoch: 2072, Batch Gradient Norm after: 10.536640801587751
Epoch 2073/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.4549382209777832
Epoch: 2073, Batch Gradient Norm: 11.638473628492639
Epoch: 2073, Batch Gradient Norm after: 11.638473628492639
Epoch 2074/10000, Prediction Accuracy = 61.188%, Loss = 0.4557132005691528
Epoch: 2074, Batch Gradient Norm: 11.183045482363536
Epoch: 2074, Batch Gradient Norm after: 11.183045482363536
Epoch 2075/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.45637896060943606
Epoch: 2075, Batch Gradient Norm: 11.855710573197232
Epoch: 2075, Batch Gradient Norm after: 11.855710573197232
Epoch 2076/10000, Prediction Accuracy = 61.138%, Loss = 0.45439250469207765
Epoch: 2076, Batch Gradient Norm: 9.90377014918578
Epoch: 2076, Batch Gradient Norm after: 9.90377014918578
Epoch 2077/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.4517914950847626
Epoch: 2077, Batch Gradient Norm: 10.506065576870753
Epoch: 2077, Batch Gradient Norm after: 10.506065576870753
Epoch 2078/10000, Prediction Accuracy = 61.086%, Loss = 0.4558268368244171
Epoch: 2078, Batch Gradient Norm: 9.522665933522532
Epoch: 2078, Batch Gradient Norm after: 9.522665933522532
Epoch 2079/10000, Prediction Accuracy = 60.962%, Loss = 0.45385491847991943
Epoch: 2079, Batch Gradient Norm: 10.71276075727044
Epoch: 2079, Batch Gradient Norm after: 10.71276075727044
Epoch 2080/10000, Prediction Accuracy = 61.0%, Loss = 0.4566638648509979
Epoch: 2080, Batch Gradient Norm: 10.505364615740126
Epoch: 2080, Batch Gradient Norm after: 10.505364615740126
Epoch 2081/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.4530913829803467
Epoch: 2081, Batch Gradient Norm: 15.655169571529445
Epoch: 2081, Batch Gradient Norm after: 15.655169571529445
Epoch 2082/10000, Prediction Accuracy = 61.14%, Loss = 0.4643148183822632
Epoch: 2082, Batch Gradient Norm: 14.964734201347602
Epoch: 2082, Batch Gradient Norm after: 14.964734201347602
Epoch 2083/10000, Prediction Accuracy = 60.996%, Loss = 0.46188238859176634
Epoch: 2083, Batch Gradient Norm: 10.377668724779879
Epoch: 2083, Batch Gradient Norm after: 10.377668724779879
Epoch 2084/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.4553591549396515
Epoch: 2084, Batch Gradient Norm: 11.974667844025978
Epoch: 2084, Batch Gradient Norm after: 11.974667844025978
Epoch 2085/10000, Prediction Accuracy = 61.076%, Loss = 0.45590102672576904
Epoch: 2085, Batch Gradient Norm: 9.296188081081299
Epoch: 2085, Batch Gradient Norm after: 9.296188081081299
Epoch 2086/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.4520390391349792
Epoch: 2086, Batch Gradient Norm: 8.84226628074266
Epoch: 2086, Batch Gradient Norm after: 8.84226628074266
Epoch 2087/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.45259042382240294
Epoch: 2087, Batch Gradient Norm: 9.744378963302939
Epoch: 2087, Batch Gradient Norm after: 9.744378963302939
Epoch 2088/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.4534770429134369
Epoch: 2088, Batch Gradient Norm: 10.028389752338834
Epoch: 2088, Batch Gradient Norm after: 10.028389752338834
Epoch 2089/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.4528170883655548
Epoch: 2089, Batch Gradient Norm: 9.51250678786414
Epoch: 2089, Batch Gradient Norm after: 9.51250678786414
Epoch 2090/10000, Prediction Accuracy = 61.093999999999994%, Loss = 0.4510666072368622
Epoch: 2090, Batch Gradient Norm: 10.02227264530351
Epoch: 2090, Batch Gradient Norm after: 10.02227264530351
Epoch 2091/10000, Prediction Accuracy = 61.036%, Loss = 0.4517508089542389
Epoch: 2091, Batch Gradient Norm: 11.942393992398316
Epoch: 2091, Batch Gradient Norm after: 11.942393992398316
Epoch 2092/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.45481751561164857
Epoch: 2092, Batch Gradient Norm: 12.294634507444908
Epoch: 2092, Batch Gradient Norm after: 12.294634507444908
Epoch 2093/10000, Prediction Accuracy = 61.092%, Loss = 0.45834583044052124
Epoch: 2093, Batch Gradient Norm: 13.549797339236074
Epoch: 2093, Batch Gradient Norm after: 13.549797339236074
Epoch 2094/10000, Prediction Accuracy = 61.074%, Loss = 0.45833140015602114
Epoch: 2094, Batch Gradient Norm: 11.202281307403833
Epoch: 2094, Batch Gradient Norm after: 11.202281307403833
Epoch 2095/10000, Prediction Accuracy = 61.028%, Loss = 0.45355710983276365
Epoch: 2095, Batch Gradient Norm: 10.20155816812282
Epoch: 2095, Batch Gradient Norm after: 10.20155816812282
Epoch 2096/10000, Prediction Accuracy = 60.968%, Loss = 0.4534872829914093
Epoch: 2096, Batch Gradient Norm: 11.238577146643859
Epoch: 2096, Batch Gradient Norm after: 11.238577146643859
Epoch 2097/10000, Prediction Accuracy = 61.098%, Loss = 0.45557392835617067
Epoch: 2097, Batch Gradient Norm: 12.562233585851878
Epoch: 2097, Batch Gradient Norm after: 12.562233585851878
Epoch 2098/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.4571031093597412
Epoch: 2098, Batch Gradient Norm: 14.220112079206201
Epoch: 2098, Batch Gradient Norm after: 14.220112079206201
Epoch 2099/10000, Prediction Accuracy = 61.104%, Loss = 0.45867590308189393
Epoch: 2099, Batch Gradient Norm: 15.702883563592636
Epoch: 2099, Batch Gradient Norm after: 15.702883563592636
Epoch 2100/10000, Prediction Accuracy = 61.104%, Loss = 0.4612728297710419
Epoch: 2100, Batch Gradient Norm: 13.168428420034115
Epoch: 2100, Batch Gradient Norm after: 13.168428420034115
Epoch 2101/10000, Prediction Accuracy = 61.076%, Loss = 0.4583595275878906
Epoch: 2101, Batch Gradient Norm: 15.42530895633477
Epoch: 2101, Batch Gradient Norm after: 15.42530895633477
Epoch 2102/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.4609304010868073
Epoch: 2102, Batch Gradient Norm: 14.652510491194242
Epoch: 2102, Batch Gradient Norm after: 14.652510491194242
Epoch 2103/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.4599620997905731
Epoch: 2103, Batch Gradient Norm: 13.168391776349717
Epoch: 2103, Batch Gradient Norm after: 13.168391776349717
Epoch 2104/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.4576738774776459
Epoch: 2104, Batch Gradient Norm: 10.54533689133736
Epoch: 2104, Batch Gradient Norm after: 10.54533689133736
Epoch 2105/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.45332505702972414
Epoch: 2105, Batch Gradient Norm: 11.548298780158623
Epoch: 2105, Batch Gradient Norm after: 11.548298780158623
Epoch 2106/10000, Prediction Accuracy = 61.036%, Loss = 0.45619426369667054
Epoch: 2106, Batch Gradient Norm: 15.555849964791626
Epoch: 2106, Batch Gradient Norm after: 15.555849964791626
Epoch 2107/10000, Prediction Accuracy = 61.04%, Loss = 0.46486756205558777
Epoch: 2107, Batch Gradient Norm: 13.651753393089134
Epoch: 2107, Batch Gradient Norm after: 13.651753393089134
Epoch 2108/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.45887410044670107
Epoch: 2108, Batch Gradient Norm: 11.7087502228595
Epoch: 2108, Batch Gradient Norm after: 11.7087502228595
Epoch 2109/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.4541459321975708
Epoch: 2109, Batch Gradient Norm: 10.374614197103025
Epoch: 2109, Batch Gradient Norm after: 10.374614197103025
Epoch 2110/10000, Prediction Accuracy = 61.114%, Loss = 0.45271568894386294
Epoch: 2110, Batch Gradient Norm: 11.981116640092054
Epoch: 2110, Batch Gradient Norm after: 11.981116640092054
Epoch 2111/10000, Prediction Accuracy = 60.992000000000004%, Loss = 0.4529665052890778
Epoch: 2111, Batch Gradient Norm: 13.258825569527612
Epoch: 2111, Batch Gradient Norm after: 13.258825569527612
Epoch 2112/10000, Prediction Accuracy = 61.19200000000001%, Loss = 0.45644917488098147
Epoch: 2112, Batch Gradient Norm: 13.833453021984342
Epoch: 2112, Batch Gradient Norm after: 13.833453021984342
Epoch 2113/10000, Prediction Accuracy = 61.102%, Loss = 0.4597312271595001
Epoch: 2113, Batch Gradient Norm: 12.196937677152329
Epoch: 2113, Batch Gradient Norm after: 12.196937677152329
Epoch 2114/10000, Prediction Accuracy = 61.07600000000001%, Loss = 0.45522854328155515
Epoch: 2114, Batch Gradient Norm: 11.305418353472321
Epoch: 2114, Batch Gradient Norm after: 11.305418353472321
Epoch 2115/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.4532598376274109
Epoch: 2115, Batch Gradient Norm: 10.46832831250376
Epoch: 2115, Batch Gradient Norm after: 10.46832831250376
Epoch 2116/10000, Prediction Accuracy = 61.098%, Loss = 0.45197705626487733
Epoch: 2116, Batch Gradient Norm: 8.416301873888523
Epoch: 2116, Batch Gradient Norm after: 8.416301873888523
Epoch 2117/10000, Prediction Accuracy = 60.968%, Loss = 0.4485779941082001
Epoch: 2117, Batch Gradient Norm: 10.425267448991479
Epoch: 2117, Batch Gradient Norm after: 10.425267448991479
Epoch 2118/10000, Prediction Accuracy = 61.141999999999996%, Loss = 0.45118057131767275
Epoch: 2118, Batch Gradient Norm: 8.34936626613725
Epoch: 2118, Batch Gradient Norm after: 8.34936626613725
Epoch 2119/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.44970771074295046
Epoch: 2119, Batch Gradient Norm: 9.161392222894353
Epoch: 2119, Batch Gradient Norm after: 9.161392222894353
Epoch 2120/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.45129836797714235
Epoch: 2120, Batch Gradient Norm: 10.064928519797245
Epoch: 2120, Batch Gradient Norm after: 10.064928519797245
Epoch 2121/10000, Prediction Accuracy = 61.11400000000001%, Loss = 0.4504452466964722
Epoch: 2121, Batch Gradient Norm: 8.225318430410818
Epoch: 2121, Batch Gradient Norm after: 8.225318430410818
Epoch 2122/10000, Prediction Accuracy = 61.134%, Loss = 0.4474517822265625
Epoch: 2122, Batch Gradient Norm: 8.455400792529172
Epoch: 2122, Batch Gradient Norm after: 8.455400792529172
Epoch 2123/10000, Prediction Accuracy = 61.07199999999999%, Loss = 0.44804089665412905
Epoch: 2123, Batch Gradient Norm: 9.702696005001327
Epoch: 2123, Batch Gradient Norm after: 9.702696005001327
Epoch 2124/10000, Prediction Accuracy = 61.194%, Loss = 0.450981068611145
Epoch: 2124, Batch Gradient Norm: 10.648877250031902
Epoch: 2124, Batch Gradient Norm after: 10.648877250031902
Epoch 2125/10000, Prediction Accuracy = 61.128%, Loss = 0.45272702574729917
Epoch: 2125, Batch Gradient Norm: 10.408576005305626
Epoch: 2125, Batch Gradient Norm after: 10.408576005305626
Epoch 2126/10000, Prediction Accuracy = 61.072%, Loss = 0.4530159592628479
Epoch: 2126, Batch Gradient Norm: 10.356096298732808
Epoch: 2126, Batch Gradient Norm after: 10.356096298732808
Epoch 2127/10000, Prediction Accuracy = 61.053999999999995%, Loss = 0.45153499245643614
Epoch: 2127, Batch Gradient Norm: 10.81125797944908
Epoch: 2127, Batch Gradient Norm after: 10.81125797944908
Epoch 2128/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.4541256487369537
Epoch: 2128, Batch Gradient Norm: 12.746102694771823
Epoch: 2128, Batch Gradient Norm after: 12.746102694771823
Epoch 2129/10000, Prediction Accuracy = 61.086%, Loss = 0.45693655014038087
Epoch: 2129, Batch Gradient Norm: 12.938844212948807
Epoch: 2129, Batch Gradient Norm after: 12.938844212948807
Epoch 2130/10000, Prediction Accuracy = 60.992%, Loss = 0.45645834803581237
Epoch: 2130, Batch Gradient Norm: 10.813469657229719
Epoch: 2130, Batch Gradient Norm after: 10.813469657229719
Epoch 2131/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.4523580074310303
Epoch: 2131, Batch Gradient Norm: 10.457873113278268
Epoch: 2131, Batch Gradient Norm after: 10.457873113278268
Epoch 2132/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.45086713433265685
Epoch: 2132, Batch Gradient Norm: 10.907391789913925
Epoch: 2132, Batch Gradient Norm after: 10.907391789913925
Epoch 2133/10000, Prediction Accuracy = 60.962%, Loss = 0.4533250093460083
Epoch: 2133, Batch Gradient Norm: 10.783813422140264
Epoch: 2133, Batch Gradient Norm after: 10.783813422140264
Epoch 2134/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.4530192017555237
Epoch: 2134, Batch Gradient Norm: 13.896138036267345
Epoch: 2134, Batch Gradient Norm after: 13.896138036267345
Epoch 2135/10000, Prediction Accuracy = 61.024%, Loss = 0.4605966627597809
Epoch: 2135, Batch Gradient Norm: 12.702169865221753
Epoch: 2135, Batch Gradient Norm after: 12.702169865221753
Epoch 2136/10000, Prediction Accuracy = 61.178%, Loss = 0.45428717136383057
Epoch: 2136, Batch Gradient Norm: 12.115965735859797
Epoch: 2136, Batch Gradient Norm after: 12.115965735859797
Epoch 2137/10000, Prediction Accuracy = 60.999999999999986%, Loss = 0.4531818687915802
Epoch: 2137, Batch Gradient Norm: 10.923072640787453
Epoch: 2137, Batch Gradient Norm after: 10.923072640787453
Epoch 2138/10000, Prediction Accuracy = 61.08%, Loss = 0.45206772685050967
Epoch: 2138, Batch Gradient Norm: 12.519212325619398
Epoch: 2138, Batch Gradient Norm after: 12.519212325619398
Epoch 2139/10000, Prediction Accuracy = 61.036%, Loss = 0.4565537631511688
Epoch: 2139, Batch Gradient Norm: 13.38965857707458
Epoch: 2139, Batch Gradient Norm after: 13.38965857707458
Epoch 2140/10000, Prediction Accuracy = 61.036%, Loss = 0.4578726410865784
Epoch: 2140, Batch Gradient Norm: 12.621751853928577
Epoch: 2140, Batch Gradient Norm after: 12.621751853928577
Epoch 2141/10000, Prediction Accuracy = 61.024%, Loss = 0.45541701316833494
Epoch: 2141, Batch Gradient Norm: 13.393304357230408
Epoch: 2141, Batch Gradient Norm after: 13.393304357230408
Epoch 2142/10000, Prediction Accuracy = 61.098%, Loss = 0.45768519639968874
Epoch: 2142, Batch Gradient Norm: 12.42897178907685
Epoch: 2142, Batch Gradient Norm after: 12.42897178907685
Epoch 2143/10000, Prediction Accuracy = 61.1%, Loss = 0.4535101354122162
Epoch: 2143, Batch Gradient Norm: 14.658641306116703
Epoch: 2143, Batch Gradient Norm after: 14.658641306116703
Epoch 2144/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.45969383120536805
Epoch: 2144, Batch Gradient Norm: 12.971769316480835
Epoch: 2144, Batch Gradient Norm after: 12.971769316480835
Epoch 2145/10000, Prediction Accuracy = 61.028%, Loss = 0.4569563329219818
Epoch: 2145, Batch Gradient Norm: 13.426295781282185
Epoch: 2145, Batch Gradient Norm after: 13.426295781282185
Epoch 2146/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.4570884644985199
Epoch: 2146, Batch Gradient Norm: 13.191095730282553
Epoch: 2146, Batch Gradient Norm after: 13.191095730282553
Epoch 2147/10000, Prediction Accuracy = 61.102%, Loss = 0.45635140538215635
Epoch: 2147, Batch Gradient Norm: 13.384462151806556
Epoch: 2147, Batch Gradient Norm after: 13.384462151806556
Epoch 2148/10000, Prediction Accuracy = 61.052%, Loss = 0.45501431822776794
Epoch: 2148, Batch Gradient Norm: 11.534588904242849
Epoch: 2148, Batch Gradient Norm after: 11.534588904242849
Epoch 2149/10000, Prediction Accuracy = 61.117999999999995%, Loss = 0.4518947541713715
Epoch: 2149, Batch Gradient Norm: 11.842038462391072
Epoch: 2149, Batch Gradient Norm after: 11.842038462391072
Epoch 2150/10000, Prediction Accuracy = 61.124%, Loss = 0.4537681758403778
Epoch: 2150, Batch Gradient Norm: 8.917010891966823
Epoch: 2150, Batch Gradient Norm after: 8.917010891966823
Epoch 2151/10000, Prediction Accuracy = 61.182%, Loss = 0.44812546372413636
Epoch: 2151, Batch Gradient Norm: 9.048002520302953
Epoch: 2151, Batch Gradient Norm after: 9.048002520302953
Epoch 2152/10000, Prediction Accuracy = 61.01800000000001%, Loss = 0.44856555461883546
Epoch: 2152, Batch Gradient Norm: 10.93396149722384
Epoch: 2152, Batch Gradient Norm after: 10.93396149722384
Epoch 2153/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.45025155544281004
Epoch: 2153, Batch Gradient Norm: 10.58681736753473
Epoch: 2153, Batch Gradient Norm after: 10.58681736753473
Epoch 2154/10000, Prediction Accuracy = 61.124%, Loss = 0.4506304442882538
Epoch: 2154, Batch Gradient Norm: 11.064434832445496
Epoch: 2154, Batch Gradient Norm after: 11.064434832445496
Epoch 2155/10000, Prediction Accuracy = 61.092%, Loss = 0.45070632100105285
Epoch: 2155, Batch Gradient Norm: 10.689093208187387
Epoch: 2155, Batch Gradient Norm after: 10.689093208187387
Epoch 2156/10000, Prediction Accuracy = 61.152%, Loss = 0.4500173687934875
Epoch: 2156, Batch Gradient Norm: 10.685795970292373
Epoch: 2156, Batch Gradient Norm after: 10.685795970292373
Epoch 2157/10000, Prediction Accuracy = 61.262%, Loss = 0.4521548092365265
Epoch: 2157, Batch Gradient Norm: 11.118166466232957
Epoch: 2157, Batch Gradient Norm after: 11.118166466232957
Epoch 2158/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.45148348808288574
Epoch: 2158, Batch Gradient Norm: 10.260399280245352
Epoch: 2158, Batch Gradient Norm after: 10.260399280245352
Epoch 2159/10000, Prediction Accuracy = 61.145999999999994%, Loss = 0.44818288683891294
Epoch: 2159, Batch Gradient Norm: 10.493805443067343
Epoch: 2159, Batch Gradient Norm after: 10.493805443067343
Epoch 2160/10000, Prediction Accuracy = 61.08200000000001%, Loss = 0.4489089250564575
Epoch: 2160, Batch Gradient Norm: 11.007460896870242
Epoch: 2160, Batch Gradient Norm after: 11.007460896870242
Epoch 2161/10000, Prediction Accuracy = 61.084%, Loss = 0.450804328918457
Epoch: 2161, Batch Gradient Norm: 11.915543118640858
Epoch: 2161, Batch Gradient Norm after: 11.915543118640858
Epoch 2162/10000, Prediction Accuracy = 61.092%, Loss = 0.452844512462616
Epoch: 2162, Batch Gradient Norm: 11.953564382514445
Epoch: 2162, Batch Gradient Norm after: 11.953564382514445
Epoch 2163/10000, Prediction Accuracy = 61.166%, Loss = 0.45247319936752317
Epoch: 2163, Batch Gradient Norm: 15.509301778547187
Epoch: 2163, Batch Gradient Norm after: 15.509301778547187
Epoch 2164/10000, Prediction Accuracy = 61.056%, Loss = 0.4617185056209564
Epoch: 2164, Batch Gradient Norm: 12.718976059414604
Epoch: 2164, Batch Gradient Norm after: 12.718976059414604
Epoch 2165/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.45612881183624265
Epoch: 2165, Batch Gradient Norm: 13.41146609678528
Epoch: 2165, Batch Gradient Norm after: 13.41146609678528
Epoch 2166/10000, Prediction Accuracy = 61.17%, Loss = 0.4540910661220551
Epoch: 2166, Batch Gradient Norm: 13.114451831130308
Epoch: 2166, Batch Gradient Norm after: 13.114451831130308
Epoch 2167/10000, Prediction Accuracy = 60.984%, Loss = 0.45620843172073366
Epoch: 2167, Batch Gradient Norm: 12.255882554331134
Epoch: 2167, Batch Gradient Norm after: 12.255882554331134
Epoch 2168/10000, Prediction Accuracy = 61.14000000000001%, Loss = 0.45229911208152773
Epoch: 2168, Batch Gradient Norm: 10.640466920210233
Epoch: 2168, Batch Gradient Norm after: 10.640466920210233
Epoch 2169/10000, Prediction Accuracy = 61.074%, Loss = 0.44844016432762146
Epoch: 2169, Batch Gradient Norm: 11.894155177380435
Epoch: 2169, Batch Gradient Norm after: 11.894155177380435
Epoch 2170/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.4516989767551422
Epoch: 2170, Batch Gradient Norm: 12.136227396301797
Epoch: 2170, Batch Gradient Norm after: 12.136227396301797
Epoch 2171/10000, Prediction Accuracy = 61.158%, Loss = 0.45201976895332335
Epoch: 2171, Batch Gradient Norm: 10.93709312578635
Epoch: 2171, Batch Gradient Norm after: 10.93709312578635
Epoch 2172/10000, Prediction Accuracy = 61.157999999999994%, Loss = 0.4496263563632965
Epoch: 2172, Batch Gradient Norm: 10.116724229512338
Epoch: 2172, Batch Gradient Norm after: 10.116724229512338
Epoch 2173/10000, Prediction Accuracy = 61.06%, Loss = 0.4505153477191925
Epoch: 2173, Batch Gradient Norm: 10.128801599123681
Epoch: 2173, Batch Gradient Norm after: 10.128801599123681
Epoch 2174/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.44801167845726014
Epoch: 2174, Batch Gradient Norm: 12.49879951573816
Epoch: 2174, Batch Gradient Norm after: 12.49879951573816
Epoch 2175/10000, Prediction Accuracy = 61.126%, Loss = 0.4540723502635956
Epoch: 2175, Batch Gradient Norm: 12.303877815027354
Epoch: 2175, Batch Gradient Norm after: 12.303877815027354
Epoch 2176/10000, Prediction Accuracy = 61.1%, Loss = 0.4518326461315155
Epoch: 2176, Batch Gradient Norm: 10.017611268454033
Epoch: 2176, Batch Gradient Norm after: 10.017611268454033
Epoch 2177/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.4487378716468811
Epoch: 2177, Batch Gradient Norm: 9.669053226068247
Epoch: 2177, Batch Gradient Norm after: 9.669053226068247
Epoch 2178/10000, Prediction Accuracy = 61.084%, Loss = 0.44803492426872255
Epoch: 2178, Batch Gradient Norm: 8.33867131720293
Epoch: 2178, Batch Gradient Norm after: 8.33867131720293
Epoch 2179/10000, Prediction Accuracy = 61.10999999999999%, Loss = 0.4454402506351471
Epoch: 2179, Batch Gradient Norm: 9.185840655108198
Epoch: 2179, Batch Gradient Norm after: 9.185840655108198
Epoch 2180/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.44568472504615786
Epoch: 2180, Batch Gradient Norm: 8.176731212001677
Epoch: 2180, Batch Gradient Norm after: 8.176731212001677
Epoch 2181/10000, Prediction Accuracy = 61.242%, Loss = 0.4456130385398865
Epoch: 2181, Batch Gradient Norm: 8.327330832727732
Epoch: 2181, Batch Gradient Norm after: 8.327330832727732
Epoch 2182/10000, Prediction Accuracy = 61.124%, Loss = 0.4445506513118744
Epoch: 2182, Batch Gradient Norm: 11.216880834129901
Epoch: 2182, Batch Gradient Norm after: 11.216880834129901
Epoch 2183/10000, Prediction Accuracy = 61.06%, Loss = 0.4499968528747559
Epoch: 2183, Batch Gradient Norm: 11.115115882452553
Epoch: 2183, Batch Gradient Norm after: 11.115115882452553
Epoch 2184/10000, Prediction Accuracy = 61.1%, Loss = 0.4485883593559265
Epoch: 2184, Batch Gradient Norm: 10.990590012879032
Epoch: 2184, Batch Gradient Norm after: 10.990590012879032
Epoch 2185/10000, Prediction Accuracy = 60.99400000000001%, Loss = 0.449092960357666
Epoch: 2185, Batch Gradient Norm: 9.934534575645795
Epoch: 2185, Batch Gradient Norm after: 9.934534575645795
Epoch 2186/10000, Prediction Accuracy = 61.162%, Loss = 0.4482774078845978
Epoch: 2186, Batch Gradient Norm: 13.605354466067748
Epoch: 2186, Batch Gradient Norm after: 13.605354466067748
Epoch 2187/10000, Prediction Accuracy = 61.116%, Loss = 0.45383859872817994
Epoch: 2187, Batch Gradient Norm: 15.029512946003738
Epoch: 2187, Batch Gradient Norm after: 15.005720624291254
Epoch 2188/10000, Prediction Accuracy = 61.136%, Loss = 0.457181578874588
Epoch: 2188, Batch Gradient Norm: 15.673479194845422
Epoch: 2188, Batch Gradient Norm after: 15.673479194845422
Epoch 2189/10000, Prediction Accuracy = 61.19%, Loss = 0.45901761054992674
Epoch: 2189, Batch Gradient Norm: 13.930314091276065
Epoch: 2189, Batch Gradient Norm after: 13.930314091276065
Epoch 2190/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.45504539012908934
Epoch: 2190, Batch Gradient Norm: 14.262437389381608
Epoch: 2190, Batch Gradient Norm after: 14.262437389381608
Epoch 2191/10000, Prediction Accuracy = 61.088%, Loss = 0.4577947795391083
Epoch: 2191, Batch Gradient Norm: 14.05194981433813
Epoch: 2191, Batch Gradient Norm after: 14.05194981433813
Epoch 2192/10000, Prediction Accuracy = 61.136%, Loss = 0.45356589555740356
Epoch: 2192, Batch Gradient Norm: 12.082794021451349
Epoch: 2192, Batch Gradient Norm after: 12.082794021451349
Epoch 2193/10000, Prediction Accuracy = 61.076%, Loss = 0.4511823713779449
Epoch: 2193, Batch Gradient Norm: 11.464699869741953
Epoch: 2193, Batch Gradient Norm after: 11.464699869741953
Epoch 2194/10000, Prediction Accuracy = 61.114%, Loss = 0.4508329689502716
Epoch: 2194, Batch Gradient Norm: 11.372891829819332
Epoch: 2194, Batch Gradient Norm after: 11.372891829819332
Epoch 2195/10000, Prediction Accuracy = 61.124%, Loss = 0.44909714460372924
Epoch: 2195, Batch Gradient Norm: 10.086095267797825
Epoch: 2195, Batch Gradient Norm after: 10.086095267797825
Epoch 2196/10000, Prediction Accuracy = 61.148%, Loss = 0.4496291756629944
Epoch: 2196, Batch Gradient Norm: 12.156246581542781
Epoch: 2196, Batch Gradient Norm after: 12.156246581542781
Epoch 2197/10000, Prediction Accuracy = 61.11999999999999%, Loss = 0.4511893928050995
Epoch: 2197, Batch Gradient Norm: 8.975393502844097
Epoch: 2197, Batch Gradient Norm after: 8.975393502844097
Epoch 2198/10000, Prediction Accuracy = 61.194%, Loss = 0.447180312871933
Epoch: 2198, Batch Gradient Norm: 11.127810154032698
Epoch: 2198, Batch Gradient Norm after: 11.127810154032698
Epoch 2199/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.449433696269989
Epoch: 2199, Batch Gradient Norm: 10.810416582545276
Epoch: 2199, Batch Gradient Norm after: 10.810416582545276
Epoch 2200/10000, Prediction Accuracy = 61.157999999999994%, Loss = 0.4489728808403015
Epoch: 2200, Batch Gradient Norm: 12.841504087072401
Epoch: 2200, Batch Gradient Norm after: 12.841504087072401
Epoch 2201/10000, Prediction Accuracy = 61.15%, Loss = 0.4532234132289886
Epoch: 2201, Batch Gradient Norm: 10.791025241798545
Epoch: 2201, Batch Gradient Norm after: 10.791025241798545
Epoch 2202/10000, Prediction Accuracy = 61.126%, Loss = 0.4490812301635742
Epoch: 2202, Batch Gradient Norm: 11.600812367480435
Epoch: 2202, Batch Gradient Norm after: 11.600812367480435
Epoch 2203/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.4483623743057251
Epoch: 2203, Batch Gradient Norm: 9.882836254353576
Epoch: 2203, Batch Gradient Norm after: 9.882836254353576
Epoch 2204/10000, Prediction Accuracy = 61.19%, Loss = 0.4471189081668854
Epoch: 2204, Batch Gradient Norm: 10.038326053007811
Epoch: 2204, Batch Gradient Norm after: 10.038326053007811
Epoch 2205/10000, Prediction Accuracy = 61.072%, Loss = 0.4461712956428528
Epoch: 2205, Batch Gradient Norm: 11.146983248269164
Epoch: 2205, Batch Gradient Norm after: 11.146983248269164
Epoch 2206/10000, Prediction Accuracy = 61.198%, Loss = 0.4496765613555908
Epoch: 2206, Batch Gradient Norm: 9.511482565573893
Epoch: 2206, Batch Gradient Norm after: 9.511482565573893
Epoch 2207/10000, Prediction Accuracy = 61.124%, Loss = 0.44550493359565735
Epoch: 2207, Batch Gradient Norm: 12.038573304295971
Epoch: 2207, Batch Gradient Norm after: 12.038573304295971
Epoch 2208/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.4521613836288452
Epoch: 2208, Batch Gradient Norm: 11.827009602407248
Epoch: 2208, Batch Gradient Norm after: 11.827009602407248
Epoch 2209/10000, Prediction Accuracy = 61.226%, Loss = 0.45111459493637085
Epoch: 2209, Batch Gradient Norm: 10.338417920912548
Epoch: 2209, Batch Gradient Norm after: 10.338417920912548
Epoch 2210/10000, Prediction Accuracy = 61.178%, Loss = 0.4497465968132019
Epoch: 2210, Batch Gradient Norm: 10.782242499990907
Epoch: 2210, Batch Gradient Norm after: 10.782242499990907
Epoch 2211/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.4480215847492218
Epoch: 2211, Batch Gradient Norm: 10.436704056708452
Epoch: 2211, Batch Gradient Norm after: 10.436704056708452
Epoch 2212/10000, Prediction Accuracy = 61.222%, Loss = 0.4492080569267273
Epoch: 2212, Batch Gradient Norm: 11.538303439107649
Epoch: 2212, Batch Gradient Norm after: 11.538303439107649
Epoch 2213/10000, Prediction Accuracy = 61.19%, Loss = 0.446641343832016
Epoch: 2213, Batch Gradient Norm: 11.076270941492092
Epoch: 2213, Batch Gradient Norm after: 11.076270941492092
Epoch 2214/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.44740777611732485
Epoch: 2214, Batch Gradient Norm: 9.679871928180487
Epoch: 2214, Batch Gradient Norm after: 9.679871928180487
Epoch 2215/10000, Prediction Accuracy = 61.10999999999999%, Loss = 0.4456169903278351
Epoch: 2215, Batch Gradient Norm: 11.687504380222784
Epoch: 2215, Batch Gradient Norm after: 11.687504380222784
Epoch 2216/10000, Prediction Accuracy = 61.145999999999994%, Loss = 0.45253089666366575
Epoch: 2216, Batch Gradient Norm: 11.494206742122882
Epoch: 2216, Batch Gradient Norm after: 11.494206742122882
Epoch 2217/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.44921745657920836
Epoch: 2217, Batch Gradient Norm: 9.447239801543345
Epoch: 2217, Batch Gradient Norm after: 9.447239801543345
Epoch 2218/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.4459741175174713
Epoch: 2218, Batch Gradient Norm: 10.171483285540239
Epoch: 2218, Batch Gradient Norm after: 10.171483285540239
Epoch 2219/10000, Prediction Accuracy = 61.134%, Loss = 0.44664688110351564
Epoch: 2219, Batch Gradient Norm: 10.271657880164614
Epoch: 2219, Batch Gradient Norm after: 10.271657880164614
Epoch 2220/10000, Prediction Accuracy = 61.114%, Loss = 0.44611050486564635
Epoch: 2220, Batch Gradient Norm: 12.952695145147842
Epoch: 2220, Batch Gradient Norm after: 12.952695145147842
Epoch 2221/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.44992339611053467
Epoch: 2221, Batch Gradient Norm: 12.165825617823506
Epoch: 2221, Batch Gradient Norm after: 12.165825617823506
Epoch 2222/10000, Prediction Accuracy = 61.14000000000001%, Loss = 0.45045862793922425
Epoch: 2222, Batch Gradient Norm: 15.290434459790317
Epoch: 2222, Batch Gradient Norm after: 15.290434459790317
Epoch 2223/10000, Prediction Accuracy = 61.124%, Loss = 0.4554359376430511
Epoch: 2223, Batch Gradient Norm: 13.569098289781452
Epoch: 2223, Batch Gradient Norm after: 13.569098289781452
Epoch 2224/10000, Prediction Accuracy = 61.114%, Loss = 0.45149141550064087
Epoch: 2224, Batch Gradient Norm: 13.851627216161454
Epoch: 2224, Batch Gradient Norm after: 13.851627216161454
Epoch 2225/10000, Prediction Accuracy = 61.242%, Loss = 0.45268256068229673
Epoch: 2225, Batch Gradient Norm: 12.716570550398075
Epoch: 2225, Batch Gradient Norm after: 12.716570550398075
Epoch 2226/10000, Prediction Accuracy = 61.13199999999999%, Loss = 0.45209077596664426
Epoch: 2226, Batch Gradient Norm: 9.638079020386684
Epoch: 2226, Batch Gradient Norm after: 9.638079020386684
Epoch 2227/10000, Prediction Accuracy = 61.124%, Loss = 0.4454223334789276
Epoch: 2227, Batch Gradient Norm: 11.970912384109214
Epoch: 2227, Batch Gradient Norm after: 11.970912384109214
Epoch 2228/10000, Prediction Accuracy = 61.222%, Loss = 0.4503054738044739
Epoch: 2228, Batch Gradient Norm: 10.623111159280162
Epoch: 2228, Batch Gradient Norm after: 10.623111159280162
Epoch 2229/10000, Prediction Accuracy = 61.136%, Loss = 0.4489424109458923
Epoch: 2229, Batch Gradient Norm: 11.473492493824905
Epoch: 2229, Batch Gradient Norm after: 11.473492493824905
Epoch 2230/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.448676997423172
Epoch: 2230, Batch Gradient Norm: 10.053765622149259
Epoch: 2230, Batch Gradient Norm after: 10.053765622149259
Epoch 2231/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.446802145242691
Epoch: 2231, Batch Gradient Norm: 12.000105929622665
Epoch: 2231, Batch Gradient Norm after: 12.000105929622665
Epoch 2232/10000, Prediction Accuracy = 61.17399999999999%, Loss = 0.450125652551651
Epoch: 2232, Batch Gradient Norm: 10.455911873617431
Epoch: 2232, Batch Gradient Norm after: 10.455911873617431
Epoch 2233/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.4461861431598663
Epoch: 2233, Batch Gradient Norm: 14.121887380152895
Epoch: 2233, Batch Gradient Norm after: 14.121887380152895
Epoch 2234/10000, Prediction Accuracy = 61.212%, Loss = 0.45267810821533205
Epoch: 2234, Batch Gradient Norm: 14.593946350227217
Epoch: 2234, Batch Gradient Norm after: 14.593946350227217
Epoch 2235/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.45243889689445493
Epoch: 2235, Batch Gradient Norm: 10.63913899650308
Epoch: 2235, Batch Gradient Norm after: 10.63913899650308
Epoch 2236/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.447398316860199
Epoch: 2236, Batch Gradient Norm: 10.26727681891421
Epoch: 2236, Batch Gradient Norm after: 10.26727681891421
Epoch 2237/10000, Prediction Accuracy = 61.092%, Loss = 0.4456076264381409
Epoch: 2237, Batch Gradient Norm: 12.794754986998282
Epoch: 2237, Batch Gradient Norm after: 12.794754986998282
Epoch 2238/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.4471755623817444
Epoch: 2238, Batch Gradient Norm: 9.25630604015455
Epoch: 2238, Batch Gradient Norm after: 9.25630604015455
Epoch 2239/10000, Prediction Accuracy = 61.198%, Loss = 0.4428426742553711
Epoch: 2239, Batch Gradient Norm: 9.19005236875976
Epoch: 2239, Batch Gradient Norm after: 9.19005236875976
Epoch 2240/10000, Prediction Accuracy = 61.242%, Loss = 0.44424070715904235
Epoch: 2240, Batch Gradient Norm: 8.481052106875087
Epoch: 2240, Batch Gradient Norm after: 8.481052106875087
Epoch 2241/10000, Prediction Accuracy = 61.122%, Loss = 0.4434226155281067
Epoch: 2241, Batch Gradient Norm: 9.510244684320917
Epoch: 2241, Batch Gradient Norm after: 9.510244684320917
Epoch 2242/10000, Prediction Accuracy = 61.148%, Loss = 0.44534723162651063
Epoch: 2242, Batch Gradient Norm: 10.430197876848792
Epoch: 2242, Batch Gradient Norm after: 10.430197876848792
Epoch 2243/10000, Prediction Accuracy = 61.14000000000001%, Loss = 0.4454260468482971
Epoch: 2243, Batch Gradient Norm: 8.781563498554462
Epoch: 2243, Batch Gradient Norm after: 8.781563498554462
Epoch 2244/10000, Prediction Accuracy = 61.19599999999999%, Loss = 0.4434878289699554
Epoch: 2244, Batch Gradient Norm: 8.822046730788378
Epoch: 2244, Batch Gradient Norm after: 8.822046730788378
Epoch 2245/10000, Prediction Accuracy = 61.181999999999995%, Loss = 0.44176872372627257
Epoch: 2245, Batch Gradient Norm: 8.527602737408946
Epoch: 2245, Batch Gradient Norm after: 8.527602737408946
Epoch 2246/10000, Prediction Accuracy = 61.102%, Loss = 0.4425089657306671
Epoch: 2246, Batch Gradient Norm: 12.597286049030574
Epoch: 2246, Batch Gradient Norm after: 12.597286049030574
Epoch 2247/10000, Prediction Accuracy = 61.1%, Loss = 0.4490795791149139
Epoch: 2247, Batch Gradient Norm: 11.945856915735378
Epoch: 2247, Batch Gradient Norm after: 11.945856915735378
Epoch 2248/10000, Prediction Accuracy = 61.186%, Loss = 0.44787703156471254
Epoch: 2248, Batch Gradient Norm: 14.878778289966307
Epoch: 2248, Batch Gradient Norm after: 14.878778289966307
Epoch 2249/10000, Prediction Accuracy = 61.11400000000001%, Loss = 0.45262699723243716
Epoch: 2249, Batch Gradient Norm: 12.05410833107681
Epoch: 2249, Batch Gradient Norm after: 12.05410833107681
Epoch 2250/10000, Prediction Accuracy = 61.128%, Loss = 0.4488715589046478
Epoch: 2250, Batch Gradient Norm: 12.470341616634322
Epoch: 2250, Batch Gradient Norm after: 12.470341616634322
Epoch 2251/10000, Prediction Accuracy = 61.117999999999995%, Loss = 0.4501109778881073
Epoch: 2251, Batch Gradient Norm: 14.436212037791698
Epoch: 2251, Batch Gradient Norm after: 14.436212037791698
Epoch 2252/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.45386528968811035
Epoch: 2252, Batch Gradient Norm: 14.490972724324438
Epoch: 2252, Batch Gradient Norm after: 14.490972724324438
Epoch 2253/10000, Prediction Accuracy = 61.152%, Loss = 0.45361114144325254
Epoch: 2253, Batch Gradient Norm: 12.78831021901264
Epoch: 2253, Batch Gradient Norm after: 12.78831021901264
Epoch 2254/10000, Prediction Accuracy = 61.116%, Loss = 0.4487156331539154
Epoch: 2254, Batch Gradient Norm: 10.017412462489492
Epoch: 2254, Batch Gradient Norm after: 10.017412462489492
Epoch 2255/10000, Prediction Accuracy = 61.132000000000005%, Loss = 0.44511094093322756
Epoch: 2255, Batch Gradient Norm: 11.202185365260869
Epoch: 2255, Batch Gradient Norm after: 11.202185365260869
Epoch 2256/10000, Prediction Accuracy = 61.13000000000001%, Loss = 0.44765993356704714
Epoch: 2256, Batch Gradient Norm: 12.835428090307042
Epoch: 2256, Batch Gradient Norm after: 12.835428090307042
Epoch 2257/10000, Prediction Accuracy = 61.14%, Loss = 0.44867711067199706
Epoch: 2257, Batch Gradient Norm: 12.807928057864371
Epoch: 2257, Batch Gradient Norm after: 12.807928057864371
Epoch 2258/10000, Prediction Accuracy = 61.16600000000001%, Loss = 0.44882520437240603
Epoch: 2258, Batch Gradient Norm: 12.16400209396633
Epoch: 2258, Batch Gradient Norm after: 12.16400209396633
Epoch 2259/10000, Prediction Accuracy = 61.156000000000006%, Loss = 0.44757694005966187
Epoch: 2259, Batch Gradient Norm: 9.874234927160073
Epoch: 2259, Batch Gradient Norm after: 9.874234927160073
Epoch 2260/10000, Prediction Accuracy = 61.1%, Loss = 0.4461785852909088
Epoch: 2260, Batch Gradient Norm: 11.332338589773
Epoch: 2260, Batch Gradient Norm after: 11.332338589773
Epoch 2261/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.4454959034919739
Epoch: 2261, Batch Gradient Norm: 10.839217316436049
Epoch: 2261, Batch Gradient Norm after: 10.839217316436049
Epoch 2262/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.4461839199066162
Epoch: 2262, Batch Gradient Norm: 10.422088125295247
Epoch: 2262, Batch Gradient Norm after: 10.422088125295247
Epoch 2263/10000, Prediction Accuracy = 61.15%, Loss = 0.4449710249900818
Epoch: 2263, Batch Gradient Norm: 10.495341560660714
Epoch: 2263, Batch Gradient Norm after: 10.495341560660714
Epoch 2264/10000, Prediction Accuracy = 61.072%, Loss = 0.4482168972492218
Epoch: 2264, Batch Gradient Norm: 8.034399365681404
Epoch: 2264, Batch Gradient Norm after: 8.034399365681404
Epoch 2265/10000, Prediction Accuracy = 61.09799999999999%, Loss = 0.44429339170455934
Epoch: 2265, Batch Gradient Norm: 10.240700783721662
Epoch: 2265, Batch Gradient Norm after: 10.240700783721662
Epoch 2266/10000, Prediction Accuracy = 61.176%, Loss = 0.4453660786151886
Epoch: 2266, Batch Gradient Norm: 9.846180158291917
Epoch: 2266, Batch Gradient Norm after: 9.846180158291917
Epoch 2267/10000, Prediction Accuracy = 61.25%, Loss = 0.4446800172328949
Epoch: 2267, Batch Gradient Norm: 9.357429952486982
Epoch: 2267, Batch Gradient Norm after: 9.357429952486982
Epoch 2268/10000, Prediction Accuracy = 61.257999999999996%, Loss = 0.4434080183506012
Epoch: 2268, Batch Gradient Norm: 9.668830932985143
Epoch: 2268, Batch Gradient Norm after: 9.668830932985143
Epoch 2269/10000, Prediction Accuracy = 61.158%, Loss = 0.44633678793907167
Epoch: 2269, Batch Gradient Norm: 8.897099576409149
Epoch: 2269, Batch Gradient Norm after: 8.897099576409149
Epoch 2270/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.44157432913780215
Epoch: 2270, Batch Gradient Norm: 12.797932083479994
Epoch: 2270, Batch Gradient Norm after: 12.797932083479994
Epoch 2271/10000, Prediction Accuracy = 61.164%, Loss = 0.44825708866119385
Epoch: 2271, Batch Gradient Norm: 11.231063959167749
Epoch: 2271, Batch Gradient Norm after: 11.231063959167749
Epoch 2272/10000, Prediction Accuracy = 61.21%, Loss = 0.4466989994049072
Epoch: 2272, Batch Gradient Norm: 12.076048547368613
Epoch: 2272, Batch Gradient Norm after: 12.076048547368613
Epoch 2273/10000, Prediction Accuracy = 61.093999999999994%, Loss = 0.4473036050796509
Epoch: 2273, Batch Gradient Norm: 15.810222152721964
Epoch: 2273, Batch Gradient Norm after: 15.810222152721964
Epoch 2274/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.4534307062625885
Epoch: 2274, Batch Gradient Norm: 13.149906099776414
Epoch: 2274, Batch Gradient Norm after: 13.149906099776414
Epoch 2275/10000, Prediction Accuracy = 61.146%, Loss = 0.4476303219795227
Epoch: 2275, Batch Gradient Norm: 13.42504240980334
Epoch: 2275, Batch Gradient Norm after: 13.42504240980334
Epoch 2276/10000, Prediction Accuracy = 61.164%, Loss = 0.44917603135108947
Epoch: 2276, Batch Gradient Norm: 11.609503460061537
Epoch: 2276, Batch Gradient Norm after: 11.609503460061537
Epoch 2277/10000, Prediction Accuracy = 61.18000000000001%, Loss = 0.4464367151260376
Epoch: 2277, Batch Gradient Norm: 11.365301238110307
Epoch: 2277, Batch Gradient Norm after: 11.365301238110307
Epoch 2278/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.44559640884399415
Epoch: 2278, Batch Gradient Norm: 11.541184988127288
Epoch: 2278, Batch Gradient Norm after: 11.541184988127288
Epoch 2279/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.4441546738147736
Epoch: 2279, Batch Gradient Norm: 12.0213452528511
Epoch: 2279, Batch Gradient Norm after: 12.0213452528511
Epoch 2280/10000, Prediction Accuracy = 61.246%, Loss = 0.44819435477256775
Epoch: 2280, Batch Gradient Norm: 11.61006837942714
Epoch: 2280, Batch Gradient Norm after: 11.61006837942714
Epoch 2281/10000, Prediction Accuracy = 61.138%, Loss = 0.44507354497909546
Epoch: 2281, Batch Gradient Norm: 13.585500658868355
Epoch: 2281, Batch Gradient Norm after: 13.585500658868355
Epoch 2282/10000, Prediction Accuracy = 61.14200000000001%, Loss = 0.4531147599220276
Epoch: 2282, Batch Gradient Norm: 11.989969301650534
Epoch: 2282, Batch Gradient Norm after: 11.989969301650534
Epoch 2283/10000, Prediction Accuracy = 61.04%, Loss = 0.4464305818080902
Epoch: 2283, Batch Gradient Norm: 9.609138361075715
Epoch: 2283, Batch Gradient Norm after: 9.609138361075715
Epoch 2284/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.44448084235191343
Epoch: 2284, Batch Gradient Norm: 11.192932491830557
Epoch: 2284, Batch Gradient Norm after: 11.192932491830557
Epoch 2285/10000, Prediction Accuracy = 61.2%, Loss = 0.44523402452468874
Epoch: 2285, Batch Gradient Norm: 8.861857513349007
Epoch: 2285, Batch Gradient Norm after: 8.861857513349007
Epoch 2286/10000, Prediction Accuracy = 61.166%, Loss = 0.44045926332473756
Epoch: 2286, Batch Gradient Norm: 9.76719623996337
Epoch: 2286, Batch Gradient Norm after: 9.76719623996337
Epoch 2287/10000, Prediction Accuracy = 61.168000000000006%, Loss = 0.44519189596176145
Epoch: 2287, Batch Gradient Norm: 12.895566739993853
Epoch: 2287, Batch Gradient Norm after: 12.895566739993853
Epoch 2288/10000, Prediction Accuracy = 61.134%, Loss = 0.4487636387348175
Epoch: 2288, Batch Gradient Norm: 13.275012504520783
Epoch: 2288, Batch Gradient Norm after: 13.275012504520783
Epoch 2289/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.4468829333782196
Epoch: 2289, Batch Gradient Norm: 11.169887127317823
Epoch: 2289, Batch Gradient Norm after: 11.169887127317823
Epoch 2290/10000, Prediction Accuracy = 61.178%, Loss = 0.4449524819850922
Epoch: 2290, Batch Gradient Norm: 10.12551387868365
Epoch: 2290, Batch Gradient Norm after: 10.12551387868365
Epoch 2291/10000, Prediction Accuracy = 61.174%, Loss = 0.44269752502441406
Epoch: 2291, Batch Gradient Norm: 10.166565455788584
Epoch: 2291, Batch Gradient Norm after: 10.166565455788584
Epoch 2292/10000, Prediction Accuracy = 61.274%, Loss = 0.4454909682273865
Epoch: 2292, Batch Gradient Norm: 11.422916577635583
Epoch: 2292, Batch Gradient Norm after: 11.422916577635583
Epoch 2293/10000, Prediction Accuracy = 61.16600000000001%, Loss = 0.4459241211414337
Epoch: 2293, Batch Gradient Norm: 9.904023210668974
Epoch: 2293, Batch Gradient Norm after: 9.904023210668974
Epoch 2294/10000, Prediction Accuracy = 61.16799999999999%, Loss = 0.4442788243293762
Epoch: 2294, Batch Gradient Norm: 10.7462526809641
Epoch: 2294, Batch Gradient Norm after: 10.7462526809641
Epoch 2295/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.4462608456611633
Epoch: 2295, Batch Gradient Norm: 10.652800503287366
Epoch: 2295, Batch Gradient Norm after: 10.652800503287366
Epoch 2296/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.44341888427734377
Epoch: 2296, Batch Gradient Norm: 12.968714882853483
Epoch: 2296, Batch Gradient Norm after: 12.968714882853483
Epoch 2297/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.447204053401947
Epoch: 2297, Batch Gradient Norm: 16.036893636777062
Epoch: 2297, Batch Gradient Norm after: 16.036893636777062
Epoch 2298/10000, Prediction Accuracy = 61.145999999999994%, Loss = 0.45346916913986207
Epoch: 2298, Batch Gradient Norm: 13.613816398101859
Epoch: 2298, Batch Gradient Norm after: 13.613816398101859
Epoch 2299/10000, Prediction Accuracy = 61.23599999999999%, Loss = 0.4480097770690918
Epoch: 2299, Batch Gradient Norm: 12.572579779965302
Epoch: 2299, Batch Gradient Norm after: 12.572579779965302
Epoch 2300/10000, Prediction Accuracy = 61.166%, Loss = 0.44819009900093076
Epoch: 2300, Batch Gradient Norm: 13.675992119518822
Epoch: 2300, Batch Gradient Norm after: 13.675992119518822
Epoch 2301/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.4492652177810669
Epoch: 2301, Batch Gradient Norm: 12.213320275757022
Epoch: 2301, Batch Gradient Norm after: 12.213320275757022
Epoch 2302/10000, Prediction Accuracy = 61.29%, Loss = 0.4473270773887634
Epoch: 2302, Batch Gradient Norm: 11.596442228646156
Epoch: 2302, Batch Gradient Norm after: 11.596442228646156
Epoch 2303/10000, Prediction Accuracy = 61.15%, Loss = 0.44359478950500486
Epoch: 2303, Batch Gradient Norm: 11.468794044952924
Epoch: 2303, Batch Gradient Norm after: 11.468794044952924
Epoch 2304/10000, Prediction Accuracy = 61.274%, Loss = 0.44327452778816223
Epoch: 2304, Batch Gradient Norm: 13.653799505367733
Epoch: 2304, Batch Gradient Norm after: 13.653799505367733
Epoch 2305/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.44748951196670533
Epoch: 2305, Batch Gradient Norm: 12.200422103997248
Epoch: 2305, Batch Gradient Norm after: 12.200422103997248
Epoch 2306/10000, Prediction Accuracy = 61.23199999999999%, Loss = 0.444858056306839
Epoch: 2306, Batch Gradient Norm: 14.095170760375904
Epoch: 2306, Batch Gradient Norm after: 14.095170760375904
Epoch 2307/10000, Prediction Accuracy = 61.21%, Loss = 0.4490980267524719
Epoch: 2307, Batch Gradient Norm: 9.39241215537189
Epoch: 2307, Batch Gradient Norm after: 9.39241215537189
Epoch 2308/10000, Prediction Accuracy = 61.302%, Loss = 0.4411943078041077
Epoch: 2308, Batch Gradient Norm: 10.04924641724028
Epoch: 2308, Batch Gradient Norm after: 10.04924641724028
Epoch 2309/10000, Prediction Accuracy = 61.196000000000005%, Loss = 0.4427818536758423
Epoch: 2309, Batch Gradient Norm: 13.039241946095354
Epoch: 2309, Batch Gradient Norm after: 13.039241946095354
Epoch 2310/10000, Prediction Accuracy = 61.294%, Loss = 0.44722241163253784
Epoch: 2310, Batch Gradient Norm: 15.767492854356975
Epoch: 2310, Batch Gradient Norm after: 15.767492854356975
Epoch 2311/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.45111451745033265
Epoch: 2311, Batch Gradient Norm: 12.380003243405191
Epoch: 2311, Batch Gradient Norm after: 12.380003243405191
Epoch 2312/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.44870315194129945
Epoch: 2312, Batch Gradient Norm: 14.104981300604102
Epoch: 2312, Batch Gradient Norm after: 14.104981300604102
Epoch 2313/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.45272001028060915
Epoch: 2313, Batch Gradient Norm: 10.62252907856962
Epoch: 2313, Batch Gradient Norm after: 10.62252907856962
Epoch 2314/10000, Prediction Accuracy = 61.198%, Loss = 0.4437116503715515
Epoch: 2314, Batch Gradient Norm: 10.13916823283391
Epoch: 2314, Batch Gradient Norm after: 10.13916823283391
Epoch 2315/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.4426017165184021
Epoch: 2315, Batch Gradient Norm: 12.36283234618639
Epoch: 2315, Batch Gradient Norm after: 12.36283234618639
Epoch 2316/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.4490169405937195
Epoch: 2316, Batch Gradient Norm: 11.830972419268258
Epoch: 2316, Batch Gradient Norm after: 11.830972419268258
Epoch 2317/10000, Prediction Accuracy = 61.174%, Loss = 0.44371631145477297
Epoch: 2317, Batch Gradient Norm: 8.227477548822234
Epoch: 2317, Batch Gradient Norm after: 8.227477548822234
Epoch 2318/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.4393936336040497
Epoch: 2318, Batch Gradient Norm: 8.523890893489131
Epoch: 2318, Batch Gradient Norm after: 8.523890893489131
Epoch 2319/10000, Prediction Accuracy = 61.15%, Loss = 0.44025840759277346
Epoch: 2319, Batch Gradient Norm: 7.839898840214831
Epoch: 2319, Batch Gradient Norm after: 7.839898840214831
Epoch 2320/10000, Prediction Accuracy = 61.144000000000005%, Loss = 0.43821672797203065
Epoch: 2320, Batch Gradient Norm: 7.32523388832909
Epoch: 2320, Batch Gradient Norm after: 7.32523388832909
Epoch 2321/10000, Prediction Accuracy = 61.148%, Loss = 0.4371077537536621
Epoch: 2321, Batch Gradient Norm: 9.242766782576968
Epoch: 2321, Batch Gradient Norm after: 9.242766782576968
Epoch 2322/10000, Prediction Accuracy = 61.166%, Loss = 0.4422416865825653
Epoch: 2322, Batch Gradient Norm: 10.837927367772958
Epoch: 2322, Batch Gradient Norm after: 10.837927367772958
Epoch 2323/10000, Prediction Accuracy = 61.186%, Loss = 0.4424068510532379
Epoch: 2323, Batch Gradient Norm: 10.599052260303388
Epoch: 2323, Batch Gradient Norm after: 10.599052260303388
Epoch 2324/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.4431548058986664
Epoch: 2324, Batch Gradient Norm: 10.750334335899911
Epoch: 2324, Batch Gradient Norm after: 10.750334335899911
Epoch 2325/10000, Prediction Accuracy = 61.214%, Loss = 0.44294713735580443
Epoch: 2325, Batch Gradient Norm: 12.768118083661063
Epoch: 2325, Batch Gradient Norm after: 12.768118083661063
Epoch 2326/10000, Prediction Accuracy = 61.18599999999999%, Loss = 0.444736123085022
Epoch: 2326, Batch Gradient Norm: 13.265713813015532
Epoch: 2326, Batch Gradient Norm after: 13.265713813015532
Epoch 2327/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.44589648842811586
Epoch: 2327, Batch Gradient Norm: 14.751236498311588
Epoch: 2327, Batch Gradient Norm after: 14.751236498311588
Epoch 2328/10000, Prediction Accuracy = 61.218%, Loss = 0.4500408351421356
Epoch: 2328, Batch Gradient Norm: 14.113189464696758
Epoch: 2328, Batch Gradient Norm after: 14.113189464696758
Epoch 2329/10000, Prediction Accuracy = 61.234%, Loss = 0.44690362811088563
Epoch: 2329, Batch Gradient Norm: 12.260733193887114
Epoch: 2329, Batch Gradient Norm after: 12.260733193887114
Epoch 2330/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.44691654443740847
Epoch: 2330, Batch Gradient Norm: 13.77754439789954
Epoch: 2330, Batch Gradient Norm after: 13.77754439789954
Epoch 2331/10000, Prediction Accuracy = 61.298%, Loss = 0.4474330127239227
Epoch: 2331, Batch Gradient Norm: 15.32502137579412
Epoch: 2331, Batch Gradient Norm after: 15.32502137579412
Epoch 2332/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.45152426362037656
Epoch: 2332, Batch Gradient Norm: 13.489637395029302
Epoch: 2332, Batch Gradient Norm after: 13.489637395029302
Epoch 2333/10000, Prediction Accuracy = 61.188%, Loss = 0.44701606035232544
Epoch: 2333, Batch Gradient Norm: 13.15431251992115
Epoch: 2333, Batch Gradient Norm after: 13.15431251992115
Epoch 2334/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.44526736736297606
Epoch: 2334, Batch Gradient Norm: 11.129027518536237
Epoch: 2334, Batch Gradient Norm after: 11.129027518536237
Epoch 2335/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.44198170900344846
Epoch: 2335, Batch Gradient Norm: 11.455153651997017
Epoch: 2335, Batch Gradient Norm after: 11.455153651997017
Epoch 2336/10000, Prediction Accuracy = 61.181999999999995%, Loss = 0.4418422758579254
Epoch: 2336, Batch Gradient Norm: 11.317618282541332
Epoch: 2336, Batch Gradient Norm after: 11.317618282541332
Epoch 2337/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.4447591185569763
Epoch: 2337, Batch Gradient Norm: 11.226933779995024
Epoch: 2337, Batch Gradient Norm after: 11.226933779995024
Epoch 2338/10000, Prediction Accuracy = 61.267999999999994%, Loss = 0.44375139474868774
Epoch: 2338, Batch Gradient Norm: 12.784646393125495
Epoch: 2338, Batch Gradient Norm after: 12.784646393125495
Epoch 2339/10000, Prediction Accuracy = 61.25%, Loss = 0.44405125379562377
Epoch: 2339, Batch Gradient Norm: 12.7314148923597
Epoch: 2339, Batch Gradient Norm after: 12.7314148923597
Epoch 2340/10000, Prediction Accuracy = 61.14%, Loss = 0.4485961854457855
Epoch: 2340, Batch Gradient Norm: 12.850069251800958
Epoch: 2340, Batch Gradient Norm after: 12.850069251800958
Epoch 2341/10000, Prediction Accuracy = 61.326%, Loss = 0.4455581307411194
Epoch: 2341, Batch Gradient Norm: 13.66270138584368
Epoch: 2341, Batch Gradient Norm after: 13.66270138584368
Epoch 2342/10000, Prediction Accuracy = 61.21600000000001%, Loss = 0.4475739896297455
Epoch: 2342, Batch Gradient Norm: 14.459870222536232
Epoch: 2342, Batch Gradient Norm after: 14.459870222536232
Epoch 2343/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.4467915415763855
Epoch: 2343, Batch Gradient Norm: 10.940916526794334
Epoch: 2343, Batch Gradient Norm after: 10.940916526794334
Epoch 2344/10000, Prediction Accuracy = 61.158%, Loss = 0.44169625639915466
Epoch: 2344, Batch Gradient Norm: 11.217452854648558
Epoch: 2344, Batch Gradient Norm after: 11.217452854648558
Epoch 2345/10000, Prediction Accuracy = 61.162%, Loss = 0.4411636292934418
Epoch: 2345, Batch Gradient Norm: 11.689314982502777
Epoch: 2345, Batch Gradient Norm after: 11.689314982502777
Epoch 2346/10000, Prediction Accuracy = 61.194%, Loss = 0.4418329358100891
Epoch: 2346, Batch Gradient Norm: 11.337873079677736
Epoch: 2346, Batch Gradient Norm after: 11.337873079677736
Epoch 2347/10000, Prediction Accuracy = 61.286%, Loss = 0.441889888048172
Epoch: 2347, Batch Gradient Norm: 10.614542409387282
Epoch: 2347, Batch Gradient Norm after: 10.614542409387282
Epoch 2348/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.4411966562271118
Epoch: 2348, Batch Gradient Norm: 9.244962313622572
Epoch: 2348, Batch Gradient Norm after: 9.244962313622572
Epoch 2349/10000, Prediction Accuracy = 61.30800000000001%, Loss = 0.43810739517211916
Epoch: 2349, Batch Gradient Norm: 10.529925882307621
Epoch: 2349, Batch Gradient Norm after: 10.529925882307621
Epoch 2350/10000, Prediction Accuracy = 61.266%, Loss = 0.4427961230278015
Epoch: 2350, Batch Gradient Norm: 10.214716129273045
Epoch: 2350, Batch Gradient Norm after: 10.214716129273045
Epoch 2351/10000, Prediction Accuracy = 61.16600000000001%, Loss = 0.4400138080120087
Epoch: 2351, Batch Gradient Norm: 8.09600130691572
Epoch: 2351, Batch Gradient Norm after: 8.09600130691572
Epoch 2352/10000, Prediction Accuracy = 61.25%, Loss = 0.43646587133407594
Epoch: 2352, Batch Gradient Norm: 8.220888111834169
Epoch: 2352, Batch Gradient Norm after: 8.220888111834169
Epoch 2353/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.43768559098243714
Epoch: 2353, Batch Gradient Norm: 8.344310839609085
Epoch: 2353, Batch Gradient Norm after: 8.344310839609085
Epoch 2354/10000, Prediction Accuracy = 61.274%, Loss = 0.43911815881729127
Epoch: 2354, Batch Gradient Norm: 8.52546746104655
Epoch: 2354, Batch Gradient Norm after: 8.52546746104655
Epoch 2355/10000, Prediction Accuracy = 61.20400000000001%, Loss = 0.43967310190200803
Epoch: 2355, Batch Gradient Norm: 13.670551755321977
Epoch: 2355, Batch Gradient Norm after: 13.670551755321977
Epoch 2356/10000, Prediction Accuracy = 61.212%, Loss = 0.44787971377372743
Epoch: 2356, Batch Gradient Norm: 12.415667456889208
Epoch: 2356, Batch Gradient Norm after: 12.415667456889208
Epoch 2357/10000, Prediction Accuracy = 61.267999999999994%, Loss = 0.4431167542934418
Epoch: 2357, Batch Gradient Norm: 10.745527540702062
Epoch: 2357, Batch Gradient Norm after: 10.745527540702062
Epoch 2358/10000, Prediction Accuracy = 61.254%, Loss = 0.44147037863731386
Epoch: 2358, Batch Gradient Norm: 12.241832232883121
Epoch: 2358, Batch Gradient Norm after: 12.241832232883121
Epoch 2359/10000, Prediction Accuracy = 61.254%, Loss = 0.44126887917518615
Epoch: 2359, Batch Gradient Norm: 9.71271752288459
Epoch: 2359, Batch Gradient Norm after: 9.71271752288459
Epoch 2360/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.44009293913841246
Epoch: 2360, Batch Gradient Norm: 10.980553667032911
Epoch: 2360, Batch Gradient Norm after: 10.980553667032911
Epoch 2361/10000, Prediction Accuracy = 61.3%, Loss = 0.4407359480857849
Epoch: 2361, Batch Gradient Norm: 11.55654292628364
Epoch: 2361, Batch Gradient Norm after: 11.55654292628364
Epoch 2362/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.4440550744533539
Epoch: 2362, Batch Gradient Norm: 11.812400176155652
Epoch: 2362, Batch Gradient Norm after: 11.812400176155652
Epoch 2363/10000, Prediction Accuracy = 61.35%, Loss = 0.44326801896095275
Epoch: 2363, Batch Gradient Norm: 12.654668388363225
Epoch: 2363, Batch Gradient Norm after: 12.654668388363225
Epoch 2364/10000, Prediction Accuracy = 61.21%, Loss = 0.44331209659576415
Epoch: 2364, Batch Gradient Norm: 10.21358589171005
Epoch: 2364, Batch Gradient Norm after: 10.21358589171005
Epoch 2365/10000, Prediction Accuracy = 61.16799999999999%, Loss = 0.43869287967681886
Epoch: 2365, Batch Gradient Norm: 12.375613659395011
Epoch: 2365, Batch Gradient Norm after: 12.375613659395011
Epoch 2366/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.44347652792930603
Epoch: 2366, Batch Gradient Norm: 13.073728762028422
Epoch: 2366, Batch Gradient Norm after: 13.073728762028422
Epoch 2367/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.442304927110672
Epoch: 2367, Batch Gradient Norm: 11.575055615078782
Epoch: 2367, Batch Gradient Norm after: 11.575055615078782
Epoch 2368/10000, Prediction Accuracy = 61.212%, Loss = 0.4423509776592255
Epoch: 2368, Batch Gradient Norm: 13.784810410052732
Epoch: 2368, Batch Gradient Norm after: 13.784810410052732
Epoch 2369/10000, Prediction Accuracy = 61.148%, Loss = 0.44563300609588624
Epoch: 2369, Batch Gradient Norm: 13.294453350357958
Epoch: 2369, Batch Gradient Norm after: 13.294453350357958
Epoch 2370/10000, Prediction Accuracy = 61.306%, Loss = 0.44506268501281737
Epoch: 2370, Batch Gradient Norm: 13.971066308908004
Epoch: 2370, Batch Gradient Norm after: 13.971066308908004
Epoch 2371/10000, Prediction Accuracy = 61.21%, Loss = 0.44604628086090087
Epoch: 2371, Batch Gradient Norm: 13.043148521299587
Epoch: 2371, Batch Gradient Norm after: 13.043148521299587
Epoch 2372/10000, Prediction Accuracy = 61.266%, Loss = 0.4438170433044434
Epoch: 2372, Batch Gradient Norm: 14.13945230137423
Epoch: 2372, Batch Gradient Norm after: 14.13945230137423
Epoch 2373/10000, Prediction Accuracy = 61.254%, Loss = 0.4442229151725769
Epoch: 2373, Batch Gradient Norm: 11.869303654931551
Epoch: 2373, Batch Gradient Norm after: 11.869303654931551
Epoch 2374/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.4432902693748474
Epoch: 2374, Batch Gradient Norm: 10.863270744672075
Epoch: 2374, Batch Gradient Norm after: 10.863270744672075
Epoch 2375/10000, Prediction Accuracy = 61.222%, Loss = 0.44136364459991456
Epoch: 2375, Batch Gradient Norm: 8.509405511031995
Epoch: 2375, Batch Gradient Norm after: 8.509405511031995
Epoch 2376/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.43734619617462156
Epoch: 2376, Batch Gradient Norm: 12.057256480044382
Epoch: 2376, Batch Gradient Norm after: 12.057256480044382
Epoch 2377/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.4439289152622223
Epoch: 2377, Batch Gradient Norm: 8.844618267519333
Epoch: 2377, Batch Gradient Norm after: 8.844618267519333
Epoch 2378/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.4368189752101898
Epoch: 2378, Batch Gradient Norm: 9.424521741417655
Epoch: 2378, Batch Gradient Norm after: 9.424521741417655
Epoch 2379/10000, Prediction Accuracy = 61.242%, Loss = 0.4393526494503021
Epoch: 2379, Batch Gradient Norm: 8.804084774825242
Epoch: 2379, Batch Gradient Norm after: 8.804084774825242
Epoch 2380/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.4375335514545441
Epoch: 2380, Batch Gradient Norm: 7.98119653000496
Epoch: 2380, Batch Gradient Norm after: 7.98119653000496
Epoch 2381/10000, Prediction Accuracy = 61.282%, Loss = 0.4373597800731659
Epoch: 2381, Batch Gradient Norm: 9.405062845693932
Epoch: 2381, Batch Gradient Norm after: 9.405062845693932
Epoch 2382/10000, Prediction Accuracy = 61.25600000000001%, Loss = 0.4374343276023865
Epoch: 2382, Batch Gradient Norm: 12.247060682037572
Epoch: 2382, Batch Gradient Norm after: 12.247060682037572
Epoch 2383/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.4435216188430786
Epoch: 2383, Batch Gradient Norm: 12.462189230840316
Epoch: 2383, Batch Gradient Norm after: 12.462189230840316
Epoch 2384/10000, Prediction Accuracy = 61.222%, Loss = 0.44364691972732545
Epoch: 2384, Batch Gradient Norm: 13.416973507058703
Epoch: 2384, Batch Gradient Norm after: 13.416973507058703
Epoch 2385/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.4420341193675995
Epoch: 2385, Batch Gradient Norm: 12.697119587771127
Epoch: 2385, Batch Gradient Norm after: 12.697119587771127
Epoch 2386/10000, Prediction Accuracy = 61.312%, Loss = 0.4425868451595306
Epoch: 2386, Batch Gradient Norm: 12.484833610498455
Epoch: 2386, Batch Gradient Norm after: 12.484833610498455
Epoch 2387/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.4419849097728729
Epoch: 2387, Batch Gradient Norm: 11.544901903085636
Epoch: 2387, Batch Gradient Norm after: 11.544901903085636
Epoch 2388/10000, Prediction Accuracy = 61.286%, Loss = 0.44066582918167113
Epoch: 2388, Batch Gradient Norm: 12.414491694071387
Epoch: 2388, Batch Gradient Norm after: 12.414491694071387
Epoch 2389/10000, Prediction Accuracy = 61.214%, Loss = 0.44269842505455015
Epoch: 2389, Batch Gradient Norm: 12.545015957669397
Epoch: 2389, Batch Gradient Norm after: 12.545015957669397
Epoch 2390/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.44142780303955076
Epoch: 2390, Batch Gradient Norm: 13.432837494825488
Epoch: 2390, Batch Gradient Norm after: 13.432837494825488
Epoch 2391/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.4425687909126282
Epoch: 2391, Batch Gradient Norm: 12.325836491542573
Epoch: 2391, Batch Gradient Norm after: 12.325836491542573
Epoch 2392/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.44150463342666624
Epoch: 2392, Batch Gradient Norm: 11.819693607659346
Epoch: 2392, Batch Gradient Norm after: 11.819693607659346
Epoch 2393/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.44195220470428465
Epoch: 2393, Batch Gradient Norm: 14.137933768419979
Epoch: 2393, Batch Gradient Norm after: 14.137933768419979
Epoch 2394/10000, Prediction Accuracy = 61.16600000000001%, Loss = 0.4461434543132782
Epoch: 2394, Batch Gradient Norm: 14.073797098215147
Epoch: 2394, Batch Gradient Norm after: 14.073797098215147
Epoch 2395/10000, Prediction Accuracy = 61.19%, Loss = 0.4465273082256317
Epoch: 2395, Batch Gradient Norm: 13.288604999213876
Epoch: 2395, Batch Gradient Norm after: 13.288604999213876
Epoch 2396/10000, Prediction Accuracy = 61.226%, Loss = 0.44297590255737307
Epoch: 2396, Batch Gradient Norm: 13.54135905125921
Epoch: 2396, Batch Gradient Norm after: 13.54135905125921
Epoch 2397/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.4452800989151001
Epoch: 2397, Batch Gradient Norm: 12.888502282223842
Epoch: 2397, Batch Gradient Norm after: 12.888502282223842
Epoch 2398/10000, Prediction Accuracy = 61.334%, Loss = 0.4411998808383942
Epoch: 2398, Batch Gradient Norm: 11.238590714426866
Epoch: 2398, Batch Gradient Norm after: 11.238590714426866
Epoch 2399/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.4391041576862335
Epoch: 2399, Batch Gradient Norm: 13.062990656239254
Epoch: 2399, Batch Gradient Norm after: 13.062990656239254
Epoch 2400/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.44262661337852477
Epoch: 2400, Batch Gradient Norm: 13.479867415517433
Epoch: 2400, Batch Gradient Norm after: 13.479867415517433
Epoch 2401/10000, Prediction Accuracy = 61.217999999999996%, Loss = 0.44116562604904175
Epoch: 2401, Batch Gradient Norm: 10.931250372523053
Epoch: 2401, Batch Gradient Norm after: 10.931250372523053
Epoch 2402/10000, Prediction Accuracy = 61.286%, Loss = 0.43900033831596375
Epoch: 2402, Batch Gradient Norm: 9.366255567431141
Epoch: 2402, Batch Gradient Norm after: 9.366255567431141
Epoch 2403/10000, Prediction Accuracy = 61.306%, Loss = 0.4393238961696625
Epoch: 2403, Batch Gradient Norm: 9.995335152550334
Epoch: 2403, Batch Gradient Norm after: 9.995335152550334
Epoch 2404/10000, Prediction Accuracy = 61.25999999999999%, Loss = 0.4386834859848022
Epoch: 2404, Batch Gradient Norm: 10.958776773315828
Epoch: 2404, Batch Gradient Norm after: 10.958776773315828
Epoch 2405/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.4393266081809998
Epoch: 2405, Batch Gradient Norm: 13.035791067024606
Epoch: 2405, Batch Gradient Norm after: 13.035791067024606
Epoch 2406/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.44150674939155576
Epoch: 2406, Batch Gradient Norm: 11.404694577138462
Epoch: 2406, Batch Gradient Norm after: 11.404694577138462
Epoch 2407/10000, Prediction Accuracy = 61.262%, Loss = 0.43783111572265626
Epoch: 2407, Batch Gradient Norm: 13.254192997935974
Epoch: 2407, Batch Gradient Norm after: 13.254192997935974
Epoch 2408/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.44156927466392515
Epoch: 2408, Batch Gradient Norm: 13.313455581707464
Epoch: 2408, Batch Gradient Norm after: 13.313455581707464
Epoch 2409/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.4423511564731598
Epoch: 2409, Batch Gradient Norm: 12.717386008690523
Epoch: 2409, Batch Gradient Norm after: 12.717386008690523
Epoch 2410/10000, Prediction Accuracy = 61.282%, Loss = 0.44102579951286314
Epoch: 2410, Batch Gradient Norm: 11.098799166211146
Epoch: 2410, Batch Gradient Norm after: 11.098799166211146
Epoch 2411/10000, Prediction Accuracy = 61.25600000000001%, Loss = 0.43773582577705383
Epoch: 2411, Batch Gradient Norm: 10.148849887223466
Epoch: 2411, Batch Gradient Norm after: 10.148849887223466
Epoch 2412/10000, Prediction Accuracy = 61.314%, Loss = 0.43844144344329833
Epoch: 2412, Batch Gradient Norm: 10.141747479669512
Epoch: 2412, Batch Gradient Norm after: 10.141747479669512
Epoch 2413/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.4387910425662994
Epoch: 2413, Batch Gradient Norm: 9.87279278425016
Epoch: 2413, Batch Gradient Norm after: 9.87279278425016
Epoch 2414/10000, Prediction Accuracy = 61.286%, Loss = 0.43928760290145874
Epoch: 2414, Batch Gradient Norm: 9.458170134447759
Epoch: 2414, Batch Gradient Norm after: 9.458170134447759
Epoch 2415/10000, Prediction Accuracy = 61.324%, Loss = 0.4379881739616394
Epoch: 2415, Batch Gradient Norm: 11.310962308164312
Epoch: 2415, Batch Gradient Norm after: 11.310962308164312
Epoch 2416/10000, Prediction Accuracy = 61.31%, Loss = 0.44023301005363463
Epoch: 2416, Batch Gradient Norm: 13.836320508092973
Epoch: 2416, Batch Gradient Norm after: 13.836320508092973
Epoch 2417/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.44125977754592893
Epoch: 2417, Batch Gradient Norm: 11.745377905684007
Epoch: 2417, Batch Gradient Norm after: 11.745377905684007
Epoch 2418/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.4396811842918396
Epoch: 2418, Batch Gradient Norm: 9.91362149919502
Epoch: 2418, Batch Gradient Norm after: 9.91362149919502
Epoch 2419/10000, Prediction Accuracy = 61.214%, Loss = 0.4390299916267395
Epoch: 2419, Batch Gradient Norm: 10.517352746472199
Epoch: 2419, Batch Gradient Norm after: 10.517352746472199
Epoch 2420/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.4392800569534302
Epoch: 2420, Batch Gradient Norm: 12.305341523522138
Epoch: 2420, Batch Gradient Norm after: 12.305341523522138
Epoch 2421/10000, Prediction Accuracy = 61.274%, Loss = 0.441980642080307
Epoch: 2421, Batch Gradient Norm: 11.094041258006508
Epoch: 2421, Batch Gradient Norm after: 11.094041258006508
Epoch 2422/10000, Prediction Accuracy = 61.298%, Loss = 0.4435502290725708
Epoch: 2422, Batch Gradient Norm: 8.423512223262883
Epoch: 2422, Batch Gradient Norm after: 8.423512223262883
Epoch 2423/10000, Prediction Accuracy = 61.322%, Loss = 0.4355143427848816
Epoch: 2423, Batch Gradient Norm: 13.002590979544824
Epoch: 2423, Batch Gradient Norm after: 13.002590979544824
Epoch 2424/10000, Prediction Accuracy = 61.258%, Loss = 0.442815488576889
Epoch: 2424, Batch Gradient Norm: 13.879423265148043
Epoch: 2424, Batch Gradient Norm after: 13.879423265148043
Epoch 2425/10000, Prediction Accuracy = 61.29599999999999%, Loss = 0.44342533946037294
Epoch: 2425, Batch Gradient Norm: 14.515578364692328
Epoch: 2425, Batch Gradient Norm after: 14.515578364692328
Epoch 2426/10000, Prediction Accuracy = 61.33%, Loss = 0.44672501683235166
Epoch: 2426, Batch Gradient Norm: 10.742947299562736
Epoch: 2426, Batch Gradient Norm after: 10.742947299562736
Epoch 2427/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.4387946903705597
Epoch: 2427, Batch Gradient Norm: 11.312337194612512
Epoch: 2427, Batch Gradient Norm after: 11.312337194612512
Epoch 2428/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.4390312969684601
Epoch: 2428, Batch Gradient Norm: 10.240805034279417
Epoch: 2428, Batch Gradient Norm after: 10.240805034279417
Epoch 2429/10000, Prediction Accuracy = 61.286%, Loss = 0.43862096667289735
Epoch: 2429, Batch Gradient Norm: 11.85991931169482
Epoch: 2429, Batch Gradient Norm after: 11.85991931169482
Epoch 2430/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.43872910737991333
Epoch: 2430, Batch Gradient Norm: 13.016294324843116
Epoch: 2430, Batch Gradient Norm after: 13.016294324843116
Epoch 2431/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.44035980105400085
Epoch: 2431, Batch Gradient Norm: 13.022199426240228
Epoch: 2431, Batch Gradient Norm after: 13.022199426240228
Epoch 2432/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.44239272475242614
Epoch: 2432, Batch Gradient Norm: 11.986729605573196
Epoch: 2432, Batch Gradient Norm after: 11.986729605573196
Epoch 2433/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.4399844765663147
Epoch: 2433, Batch Gradient Norm: 15.024163527098692
Epoch: 2433, Batch Gradient Norm after: 15.024163527098692
Epoch 2434/10000, Prediction Accuracy = 61.214%, Loss = 0.44585791826248167
Epoch: 2434, Batch Gradient Norm: 17.80016730639102
Epoch: 2434, Batch Gradient Norm after: 17.80016730639102
Epoch 2435/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.4486491084098816
Epoch: 2435, Batch Gradient Norm: 13.627740743039892
Epoch: 2435, Batch Gradient Norm after: 13.627740743039892
Epoch 2436/10000, Prediction Accuracy = 61.254%, Loss = 0.4458212971687317
Epoch: 2436, Batch Gradient Norm: 12.49694672373659
Epoch: 2436, Batch Gradient Norm after: 12.49694672373659
Epoch 2437/10000, Prediction Accuracy = 61.3%, Loss = 0.44017197489738463
Epoch: 2437, Batch Gradient Norm: 11.108549717071922
Epoch: 2437, Batch Gradient Norm after: 11.108549717071922
Epoch 2438/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.4370583355426788
Epoch: 2438, Batch Gradient Norm: 11.02251515595887
Epoch: 2438, Batch Gradient Norm after: 11.02251515595887
Epoch 2439/10000, Prediction Accuracy = 61.3%, Loss = 0.4383842945098877
Epoch: 2439, Batch Gradient Norm: 9.935991851966165
Epoch: 2439, Batch Gradient Norm after: 9.935991851966165
Epoch 2440/10000, Prediction Accuracy = 61.31%, Loss = 0.4381146728992462
Epoch: 2440, Batch Gradient Norm: 10.762344860075618
Epoch: 2440, Batch Gradient Norm after: 10.762344860075618
Epoch 2441/10000, Prediction Accuracy = 61.376%, Loss = 0.4369954109191895
Epoch: 2441, Batch Gradient Norm: 13.466298778766804
Epoch: 2441, Batch Gradient Norm after: 13.466298778766804
Epoch 2442/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.44156391024589536
Epoch: 2442, Batch Gradient Norm: 12.109649001934482
Epoch: 2442, Batch Gradient Norm after: 12.109649001934482
Epoch 2443/10000, Prediction Accuracy = 61.374%, Loss = 0.43823521733284
Epoch: 2443, Batch Gradient Norm: 12.692586554526025
Epoch: 2443, Batch Gradient Norm after: 12.692586554526025
Epoch 2444/10000, Prediction Accuracy = 61.226%, Loss = 0.44279865026473997
Epoch: 2444, Batch Gradient Norm: 12.177038382410222
Epoch: 2444, Batch Gradient Norm after: 12.177038382410222
Epoch 2445/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.4442911982536316
Epoch: 2445, Batch Gradient Norm: 14.346140159116292
Epoch: 2445, Batch Gradient Norm after: 14.346140159116292
Epoch 2446/10000, Prediction Accuracy = 61.30800000000001%, Loss = 0.44309248924255373
Epoch: 2446, Batch Gradient Norm: 13.402459649084758
Epoch: 2446, Batch Gradient Norm after: 13.402459649084758
Epoch 2447/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.44210652709007264
Epoch: 2447, Batch Gradient Norm: 10.46141654960837
Epoch: 2447, Batch Gradient Norm after: 10.46141654960837
Epoch 2448/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.4390759229660034
Epoch: 2448, Batch Gradient Norm: 11.733751571361363
Epoch: 2448, Batch Gradient Norm after: 11.733751571361363
Epoch 2449/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.43703740239143374
Epoch: 2449, Batch Gradient Norm: 9.893680539103757
Epoch: 2449, Batch Gradient Norm after: 9.893680539103757
Epoch 2450/10000, Prediction Accuracy = 61.278000000000006%, Loss = 0.438047981262207
Epoch: 2450, Batch Gradient Norm: 8.026375374907646
Epoch: 2450, Batch Gradient Norm after: 8.026375374907646
Epoch 2451/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.4326437175273895
Epoch: 2451, Batch Gradient Norm: 11.26990344062619
Epoch: 2451, Batch Gradient Norm after: 11.26990344062619
Epoch 2452/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.43675212264060975
Epoch: 2452, Batch Gradient Norm: 11.55616888467256
Epoch: 2452, Batch Gradient Norm after: 11.55616888467256
Epoch 2453/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.43671228289604186
Epoch: 2453, Batch Gradient Norm: 9.441404864287458
Epoch: 2453, Batch Gradient Norm after: 9.441404864287458
Epoch 2454/10000, Prediction Accuracy = 61.31400000000001%, Loss = 0.43410776257514955
Epoch: 2454, Batch Gradient Norm: 11.185179245287824
Epoch: 2454, Batch Gradient Norm after: 11.185179245287824
Epoch 2455/10000, Prediction Accuracy = 61.32199999999999%, Loss = 0.4374134361743927
Epoch: 2455, Batch Gradient Norm: 10.639057154264393
Epoch: 2455, Batch Gradient Norm after: 10.639057154264393
Epoch 2456/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.43651978969573973
Epoch: 2456, Batch Gradient Norm: 15.218623451189002
Epoch: 2456, Batch Gradient Norm after: 15.218623451189002
Epoch 2457/10000, Prediction Accuracy = 61.376%, Loss = 0.44461822509765625
Epoch: 2457, Batch Gradient Norm: 15.110403923592274
Epoch: 2457, Batch Gradient Norm after: 15.110403923592274
Epoch 2458/10000, Prediction Accuracy = 61.298%, Loss = 0.44228204488754275
Epoch: 2458, Batch Gradient Norm: 13.607861471281996
Epoch: 2458, Batch Gradient Norm after: 13.607861471281996
Epoch 2459/10000, Prediction Accuracy = 61.278%, Loss = 0.44123865365982057
Epoch: 2459, Batch Gradient Norm: 14.31982663842738
Epoch: 2459, Batch Gradient Norm after: 14.31982663842738
Epoch 2460/10000, Prediction Accuracy = 61.388%, Loss = 0.4424693524837494
Epoch: 2460, Batch Gradient Norm: 19.69450457584878
Epoch: 2460, Batch Gradient Norm after: 18.04160688264259
Epoch 2461/10000, Prediction Accuracy = 61.27199999999999%, Loss = 0.4549973726272583
Epoch: 2461, Batch Gradient Norm: 16.566024142706674
Epoch: 2461, Batch Gradient Norm after: 16.566024142706674
Epoch 2462/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.44434158205986024
Epoch: 2462, Batch Gradient Norm: 11.957688803994444
Epoch: 2462, Batch Gradient Norm after: 11.957688803994444
Epoch 2463/10000, Prediction Accuracy = 61.43399999999999%, Loss = 0.4375505805015564
Epoch: 2463, Batch Gradient Norm: 13.93528660754484
Epoch: 2463, Batch Gradient Norm after: 13.93528660754484
Epoch 2464/10000, Prediction Accuracy = 61.366%, Loss = 0.44324337840080263
Epoch: 2464, Batch Gradient Norm: 12.442805839624466
Epoch: 2464, Batch Gradient Norm after: 12.442805839624466
Epoch 2465/10000, Prediction Accuracy = 61.33399999999999%, Loss = 0.4378494083881378
Epoch: 2465, Batch Gradient Norm: 13.406672579638697
Epoch: 2465, Batch Gradient Norm after: 13.406672579638697
Epoch 2466/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.4391286253929138
Epoch: 2466, Batch Gradient Norm: 9.697389800840627
Epoch: 2466, Batch Gradient Norm after: 9.697389800840627
Epoch 2467/10000, Prediction Accuracy = 61.31%, Loss = 0.4342593431472778
Epoch: 2467, Batch Gradient Norm: 10.047214535830726
Epoch: 2467, Batch Gradient Norm after: 10.047214535830726
Epoch 2468/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.43565548658370973
Epoch: 2468, Batch Gradient Norm: 9.692976859237039
Epoch: 2468, Batch Gradient Norm after: 9.692976859237039
Epoch 2469/10000, Prediction Accuracy = 61.378%, Loss = 0.4347511291503906
Epoch: 2469, Batch Gradient Norm: 10.040668174114868
Epoch: 2469, Batch Gradient Norm after: 10.040668174114868
Epoch 2470/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.435129189491272
Epoch: 2470, Batch Gradient Norm: 11.760834794327897
Epoch: 2470, Batch Gradient Norm after: 11.760834794327897
Epoch 2471/10000, Prediction Accuracy = 61.326%, Loss = 0.43834123611450193
Epoch: 2471, Batch Gradient Norm: 11.842337151071806
Epoch: 2471, Batch Gradient Norm after: 11.842337151071806
Epoch 2472/10000, Prediction Accuracy = 61.278%, Loss = 0.437513130903244
Epoch: 2472, Batch Gradient Norm: 11.15192066851183
Epoch: 2472, Batch Gradient Norm after: 11.15192066851183
Epoch 2473/10000, Prediction Accuracy = 61.406000000000006%, Loss = 0.43629822731018064
Epoch: 2473, Batch Gradient Norm: 9.94233224790337
Epoch: 2473, Batch Gradient Norm after: 9.94233224790337
Epoch 2474/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.435151070356369
Epoch: 2474, Batch Gradient Norm: 10.206974039664313
Epoch: 2474, Batch Gradient Norm after: 10.206974039664313
Epoch 2475/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.43646848797798155
Epoch: 2475, Batch Gradient Norm: 9.775018218801973
Epoch: 2475, Batch Gradient Norm after: 9.775018218801973
Epoch 2476/10000, Prediction Accuracy = 61.31%, Loss = 0.43454455137252807
Epoch: 2476, Batch Gradient Norm: 7.871799710354834
Epoch: 2476, Batch Gradient Norm after: 7.871799710354834
Epoch 2477/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.43193679451942446
Epoch: 2477, Batch Gradient Norm: 11.050344180042464
Epoch: 2477, Batch Gradient Norm after: 11.050344180042464
Epoch 2478/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.4353107988834381
Epoch: 2478, Batch Gradient Norm: 12.051632239683796
Epoch: 2478, Batch Gradient Norm after: 12.051632239683796
Epoch 2479/10000, Prediction Accuracy = 61.388%, Loss = 0.43559719920158385
Epoch: 2479, Batch Gradient Norm: 13.762260493234928
Epoch: 2479, Batch Gradient Norm after: 13.762260493234928
Epoch 2480/10000, Prediction Accuracy = 61.31400000000001%, Loss = 0.44026859998703005
Epoch: 2480, Batch Gradient Norm: 11.5911667453284
Epoch: 2480, Batch Gradient Norm after: 11.5911667453284
Epoch 2481/10000, Prediction Accuracy = 61.3%, Loss = 0.4371031284332275
Epoch: 2481, Batch Gradient Norm: 10.85843139797706
Epoch: 2481, Batch Gradient Norm after: 10.85843139797706
Epoch 2482/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.43478200435638426
Epoch: 2482, Batch Gradient Norm: 12.141087144628678
Epoch: 2482, Batch Gradient Norm after: 12.141087144628678
Epoch 2483/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.4373278856277466
Epoch: 2483, Batch Gradient Norm: 10.952206735664891
Epoch: 2483, Batch Gradient Norm after: 10.952206735664891
Epoch 2484/10000, Prediction Accuracy = 61.424%, Loss = 0.4370144307613373
Epoch: 2484, Batch Gradient Norm: 12.372114929417325
Epoch: 2484, Batch Gradient Norm after: 12.372114929417325
Epoch 2485/10000, Prediction Accuracy = 61.364%, Loss = 0.43739980459213257
Epoch: 2485, Batch Gradient Norm: 13.663792838965593
Epoch: 2485, Batch Gradient Norm after: 13.663792838965593
Epoch 2486/10000, Prediction Accuracy = 61.294%, Loss = 0.4406586825847626
Epoch: 2486, Batch Gradient Norm: 11.081760714350546
Epoch: 2486, Batch Gradient Norm after: 11.081760714350546
Epoch 2487/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.4363445520401001
Epoch: 2487, Batch Gradient Norm: 9.576330587636546
Epoch: 2487, Batch Gradient Norm after: 9.576330587636546
Epoch 2488/10000, Prediction Accuracy = 61.412%, Loss = 0.4336375892162323
Epoch: 2488, Batch Gradient Norm: 8.324069323545451
Epoch: 2488, Batch Gradient Norm after: 8.324069323545451
Epoch 2489/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.43399509191513064
Epoch: 2489, Batch Gradient Norm: 11.601820815138336
Epoch: 2489, Batch Gradient Norm after: 11.601820815138336
Epoch 2490/10000, Prediction Accuracy = 61.348%, Loss = 0.43484860062599184
Epoch: 2490, Batch Gradient Norm: 11.289248170280011
Epoch: 2490, Batch Gradient Norm after: 11.289248170280011
Epoch 2491/10000, Prediction Accuracy = 61.322%, Loss = 0.4385688602924347
Epoch: 2491, Batch Gradient Norm: 10.653160957054133
Epoch: 2491, Batch Gradient Norm after: 10.653160957054133
Epoch 2492/10000, Prediction Accuracy = 61.336%, Loss = 0.43480116724967954
Epoch: 2492, Batch Gradient Norm: 11.299821727140369
Epoch: 2492, Batch Gradient Norm after: 11.299821727140369
Epoch 2493/10000, Prediction Accuracy = 61.364%, Loss = 0.43586581349372866
Epoch: 2493, Batch Gradient Norm: 11.58421838078015
Epoch: 2493, Batch Gradient Norm after: 11.58421838078015
Epoch 2494/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.43556457161903384
Epoch: 2494, Batch Gradient Norm: 12.783180991987289
Epoch: 2494, Batch Gradient Norm after: 12.783180991987289
Epoch 2495/10000, Prediction Accuracy = 61.364%, Loss = 0.438135039806366
Epoch: 2495, Batch Gradient Norm: 12.803048044082741
Epoch: 2495, Batch Gradient Norm after: 12.803048044082741
Epoch 2496/10000, Prediction Accuracy = 61.322%, Loss = 0.4394460380077362
Epoch: 2496, Batch Gradient Norm: 11.968768201925887
Epoch: 2496, Batch Gradient Norm after: 11.968768201925887
Epoch 2497/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.4390374064445496
Epoch: 2497, Batch Gradient Norm: 14.389863303269621
Epoch: 2497, Batch Gradient Norm after: 14.389863303269621
Epoch 2498/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.44300390481948854
Epoch: 2498, Batch Gradient Norm: 11.4856942877991
Epoch: 2498, Batch Gradient Norm after: 11.4856942877991
Epoch 2499/10000, Prediction Accuracy = 61.282%, Loss = 0.4384360432624817
Epoch: 2499, Batch Gradient Norm: 10.759382586358136
Epoch: 2499, Batch Gradient Norm after: 10.759382586358136
Epoch 2500/10000, Prediction Accuracy = 61.419999999999995%, Loss = 0.4357063710689545
Epoch: 2500, Batch Gradient Norm: 11.197032287825039
Epoch: 2500, Batch Gradient Norm after: 11.197032287825039
Epoch 2501/10000, Prediction Accuracy = 61.262%, Loss = 0.4364380180835724
Epoch: 2501, Batch Gradient Norm: 10.63799303653925
Epoch: 2501, Batch Gradient Norm after: 10.63799303653925
Epoch 2502/10000, Prediction Accuracy = 61.286%, Loss = 0.4336686015129089
Epoch: 2502, Batch Gradient Norm: 11.727791742831705
Epoch: 2502, Batch Gradient Norm after: 11.727791742831705
Epoch 2503/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.4352203071117401
Epoch: 2503, Batch Gradient Norm: 10.905451091670134
Epoch: 2503, Batch Gradient Norm after: 10.905451091670134
Epoch 2504/10000, Prediction Accuracy = 61.362%, Loss = 0.43400917053222654
Epoch: 2504, Batch Gradient Norm: 10.51632762033592
Epoch: 2504, Batch Gradient Norm after: 10.51632762033592
Epoch 2505/10000, Prediction Accuracy = 61.322%, Loss = 0.43372020721435545
Epoch: 2505, Batch Gradient Norm: 11.807586902323429
Epoch: 2505, Batch Gradient Norm after: 11.807586902323429
Epoch 2506/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4367745280265808
Epoch: 2506, Batch Gradient Norm: 9.320767625906651
Epoch: 2506, Batch Gradient Norm after: 9.320767625906651
Epoch 2507/10000, Prediction Accuracy = 61.38800000000001%, Loss = 0.4331394612789154
Epoch: 2507, Batch Gradient Norm: 12.256702283351403
Epoch: 2507, Batch Gradient Norm after: 12.256702283351403
Epoch 2508/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.43463624715805055
Epoch: 2508, Batch Gradient Norm: 11.521263310973309
Epoch: 2508, Batch Gradient Norm after: 11.521263310973309
Epoch 2509/10000, Prediction Accuracy = 61.36%, Loss = 0.43420007824897766
Epoch: 2509, Batch Gradient Norm: 11.534347392920399
Epoch: 2509, Batch Gradient Norm after: 11.534347392920399
Epoch 2510/10000, Prediction Accuracy = 61.386%, Loss = 0.43435150384902954
Epoch: 2510, Batch Gradient Norm: 10.079457592789325
Epoch: 2510, Batch Gradient Norm after: 10.079457592789325
Epoch 2511/10000, Prediction Accuracy = 61.33%, Loss = 0.4342509567737579
Epoch: 2511, Batch Gradient Norm: 10.557646893806963
Epoch: 2511, Batch Gradient Norm after: 10.557646893806963
Epoch 2512/10000, Prediction Accuracy = 61.31%, Loss = 0.4334185838699341
Epoch: 2512, Batch Gradient Norm: 11.122100175545253
Epoch: 2512, Batch Gradient Norm after: 11.122100175545253
Epoch 2513/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.4351313054561615
Epoch: 2513, Batch Gradient Norm: 14.993311301157647
Epoch: 2513, Batch Gradient Norm after: 14.993311301157647
Epoch 2514/10000, Prediction Accuracy = 61.33%, Loss = 0.4386241853237152
Epoch: 2514, Batch Gradient Norm: 16.648619261992508
Epoch: 2514, Batch Gradient Norm after: 16.648619261992508
Epoch 2515/10000, Prediction Accuracy = 61.343999999999994%, Loss = 0.4453751266002655
Epoch: 2515, Batch Gradient Norm: 15.56341257228914
Epoch: 2515, Batch Gradient Norm after: 15.56341257228914
Epoch 2516/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.44071089625358584
Epoch: 2516, Batch Gradient Norm: 13.702299634102848
Epoch: 2516, Batch Gradient Norm after: 13.702299634102848
Epoch 2517/10000, Prediction Accuracy = 61.388%, Loss = 0.4388725936412811
Epoch: 2517, Batch Gradient Norm: 12.205232180162271
Epoch: 2517, Batch Gradient Norm after: 12.205232180162271
Epoch 2518/10000, Prediction Accuracy = 61.331999999999994%, Loss = 0.43880621790885926
Epoch: 2518, Batch Gradient Norm: 14.581299487561502
Epoch: 2518, Batch Gradient Norm after: 14.581299487561502
Epoch 2519/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4427667140960693
Epoch: 2519, Batch Gradient Norm: 12.592315186083503
Epoch: 2519, Batch Gradient Norm after: 12.592315186083503
Epoch 2520/10000, Prediction Accuracy = 61.338%, Loss = 0.4374741852283478
Epoch: 2520, Batch Gradient Norm: 11.889936716724092
Epoch: 2520, Batch Gradient Norm after: 11.889936716724092
Epoch 2521/10000, Prediction Accuracy = 61.33799999999999%, Loss = 0.4367227017879486
Epoch: 2521, Batch Gradient Norm: 12.143013201214266
Epoch: 2521, Batch Gradient Norm after: 12.143013201214266
Epoch 2522/10000, Prediction Accuracy = 61.306%, Loss = 0.43871952295303346
Epoch: 2522, Batch Gradient Norm: 10.801137457836603
Epoch: 2522, Batch Gradient Norm after: 10.801137457836603
Epoch 2523/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4358503699302673
Epoch: 2523, Batch Gradient Norm: 10.148341799030753
Epoch: 2523, Batch Gradient Norm after: 10.148341799030753
Epoch 2524/10000, Prediction Accuracy = 61.402%, Loss = 0.4339569270610809
Epoch: 2524, Batch Gradient Norm: 12.727165810210296
Epoch: 2524, Batch Gradient Norm after: 12.727165810210296
Epoch 2525/10000, Prediction Accuracy = 61.35600000000001%, Loss = 0.4368951380252838
Epoch: 2525, Batch Gradient Norm: 16.1481895897062
Epoch: 2525, Batch Gradient Norm after: 16.1481895897062
Epoch 2526/10000, Prediction Accuracy = 61.378%, Loss = 0.44295302629470823
Epoch: 2526, Batch Gradient Norm: 15.75365214980824
Epoch: 2526, Batch Gradient Norm after: 15.719604548196195
Epoch 2527/10000, Prediction Accuracy = 61.398%, Loss = 0.4421320080757141
Epoch: 2527, Batch Gradient Norm: 11.86895405746236
Epoch: 2527, Batch Gradient Norm after: 11.86895405746236
Epoch 2528/10000, Prediction Accuracy = 61.238%, Loss = 0.4352291226387024
Epoch: 2528, Batch Gradient Norm: 15.803979954795626
Epoch: 2528, Batch Gradient Norm after: 15.803979954795626
Epoch 2529/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.4409718453884125
Epoch: 2529, Batch Gradient Norm: 13.390840747536808
Epoch: 2529, Batch Gradient Norm after: 13.390840747536808
Epoch 2530/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.4391486942768097
Epoch: 2530, Batch Gradient Norm: 12.523316618552403
Epoch: 2530, Batch Gradient Norm after: 12.523316618552403
Epoch 2531/10000, Prediction Accuracy = 61.410000000000004%, Loss = 0.4384100317955017
Epoch: 2531, Batch Gradient Norm: 11.58235943177563
Epoch: 2531, Batch Gradient Norm after: 11.58235943177563
Epoch 2532/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.43578591346740725
Epoch: 2532, Batch Gradient Norm: 14.946364926090308
Epoch: 2532, Batch Gradient Norm after: 14.946364926090308
Epoch 2533/10000, Prediction Accuracy = 61.366%, Loss = 0.43891347050666807
Epoch: 2533, Batch Gradient Norm: 12.236574233849124
Epoch: 2533, Batch Gradient Norm after: 12.236574233849124
Epoch 2534/10000, Prediction Accuracy = 61.4%, Loss = 0.43579927682876585
Epoch: 2534, Batch Gradient Norm: 9.351235666538678
Epoch: 2534, Batch Gradient Norm after: 9.351235666538678
Epoch 2535/10000, Prediction Accuracy = 61.262%, Loss = 0.4322111904621124
Epoch: 2535, Batch Gradient Norm: 11.123224503860822
Epoch: 2535, Batch Gradient Norm after: 11.123224503860822
Epoch 2536/10000, Prediction Accuracy = 61.372%, Loss = 0.43499146699905394
Epoch: 2536, Batch Gradient Norm: 10.321028250069721
Epoch: 2536, Batch Gradient Norm after: 10.321028250069721
Epoch 2537/10000, Prediction Accuracy = 61.338%, Loss = 0.43118664622306824
Epoch: 2537, Batch Gradient Norm: 11.604834625727875
Epoch: 2537, Batch Gradient Norm after: 11.604834625727875
Epoch 2538/10000, Prediction Accuracy = 61.372%, Loss = 0.434393048286438
Epoch: 2538, Batch Gradient Norm: 12.947061142407655
Epoch: 2538, Batch Gradient Norm after: 12.947061142407655
Epoch 2539/10000, Prediction Accuracy = 61.318%, Loss = 0.43598759174346924
Epoch: 2539, Batch Gradient Norm: 12.354303372873357
Epoch: 2539, Batch Gradient Norm after: 12.354303372873357
Epoch 2540/10000, Prediction Accuracy = 61.35600000000001%, Loss = 0.43278856873512267
Epoch: 2540, Batch Gradient Norm: 8.606183880723313
Epoch: 2540, Batch Gradient Norm after: 8.606183880723313
Epoch 2541/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.43083520531654357
Epoch: 2541, Batch Gradient Norm: 8.414605434070866
Epoch: 2541, Batch Gradient Norm after: 8.414605434070866
Epoch 2542/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.43089828491210935
Epoch: 2542, Batch Gradient Norm: 10.114652822387553
Epoch: 2542, Batch Gradient Norm after: 10.114652822387553
Epoch 2543/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.4314410924911499
Epoch: 2543, Batch Gradient Norm: 12.27577537670667
Epoch: 2543, Batch Gradient Norm after: 12.27577537670667
Epoch 2544/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.4365513861179352
Epoch: 2544, Batch Gradient Norm: 13.241500606302392
Epoch: 2544, Batch Gradient Norm after: 13.241500606302392
Epoch 2545/10000, Prediction Accuracy = 61.44000000000001%, Loss = 0.43830646872520446
Epoch: 2545, Batch Gradient Norm: 14.216006010325573
Epoch: 2545, Batch Gradient Norm after: 14.216006010325573
Epoch 2546/10000, Prediction Accuracy = 61.33%, Loss = 0.43868841528892516
Epoch: 2546, Batch Gradient Norm: 15.953267231322105
Epoch: 2546, Batch Gradient Norm after: 15.953267231322105
Epoch 2547/10000, Prediction Accuracy = 61.416%, Loss = 0.4410684406757355
Epoch: 2547, Batch Gradient Norm: 13.84687918071194
Epoch: 2547, Batch Gradient Norm after: 13.84687918071194
Epoch 2548/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.4397051990032196
Epoch: 2548, Batch Gradient Norm: 11.581584039191847
Epoch: 2548, Batch Gradient Norm after: 11.581584039191847
Epoch 2549/10000, Prediction Accuracy = 61.386%, Loss = 0.4326974928379059
Epoch: 2549, Batch Gradient Norm: 13.253069099624561
Epoch: 2549, Batch Gradient Norm after: 13.253069099624561
Epoch 2550/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.4367169260978699
Epoch: 2550, Batch Gradient Norm: 12.940244090300276
Epoch: 2550, Batch Gradient Norm after: 12.940244090300276
Epoch 2551/10000, Prediction Accuracy = 61.21600000000001%, Loss = 0.4366099536418915
Epoch: 2551, Batch Gradient Norm: 15.303535605193645
Epoch: 2551, Batch Gradient Norm after: 15.303535605193645
Epoch 2552/10000, Prediction Accuracy = 61.36999999999999%, Loss = 0.4420142829418182
Epoch: 2552, Batch Gradient Norm: 14.83195047906237
Epoch: 2552, Batch Gradient Norm after: 14.83195047906237
Epoch 2553/10000, Prediction Accuracy = 61.41600000000001%, Loss = 0.4379833221435547
Epoch: 2553, Batch Gradient Norm: 11.917505843476215
Epoch: 2553, Batch Gradient Norm after: 11.917505843476215
Epoch 2554/10000, Prediction Accuracy = 61.326%, Loss = 0.43691245913505555
Epoch: 2554, Batch Gradient Norm: 13.482692738721612
Epoch: 2554, Batch Gradient Norm after: 13.482692738721612
Epoch 2555/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.43538008332252504
Epoch: 2555, Batch Gradient Norm: 11.699625667955278
Epoch: 2555, Batch Gradient Norm after: 11.699625667955278
Epoch 2556/10000, Prediction Accuracy = 61.438%, Loss = 0.4357006072998047
Epoch: 2556, Batch Gradient Norm: 13.276802530182728
Epoch: 2556, Batch Gradient Norm after: 13.276802530182728
Epoch 2557/10000, Prediction Accuracy = 61.44200000000001%, Loss = 0.43811378479003904
Epoch: 2557, Batch Gradient Norm: 13.456719312374547
Epoch: 2557, Batch Gradient Norm after: 13.456719312374547
Epoch 2558/10000, Prediction Accuracy = 61.517999999999994%, Loss = 0.4379215598106384
Epoch: 2558, Batch Gradient Norm: 13.461563998401513
Epoch: 2558, Batch Gradient Norm after: 13.461563998401513
Epoch 2559/10000, Prediction Accuracy = 61.39%, Loss = 0.4376899182796478
Epoch: 2559, Batch Gradient Norm: 11.936524858020874
Epoch: 2559, Batch Gradient Norm after: 11.936524858020874
Epoch 2560/10000, Prediction Accuracy = 61.376%, Loss = 0.4340333044528961
Epoch: 2560, Batch Gradient Norm: 11.816616825737665
Epoch: 2560, Batch Gradient Norm after: 11.816616825737665
Epoch 2561/10000, Prediction Accuracy = 61.396%, Loss = 0.43258303999900816
Epoch: 2561, Batch Gradient Norm: 12.987151329893333
Epoch: 2561, Batch Gradient Norm after: 12.987151329893333
Epoch 2562/10000, Prediction Accuracy = 61.36999999999999%, Loss = 0.433454704284668
Epoch: 2562, Batch Gradient Norm: 14.91395610627813
Epoch: 2562, Batch Gradient Norm after: 14.91395610627813
Epoch 2563/10000, Prediction Accuracy = 61.386%, Loss = 0.43732489347457887
Epoch: 2563, Batch Gradient Norm: 14.21072101498616
Epoch: 2563, Batch Gradient Norm after: 14.21072101498616
Epoch 2564/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4371659100055695
Epoch: 2564, Batch Gradient Norm: 11.024326036274394
Epoch: 2564, Batch Gradient Norm after: 11.024326036274394
Epoch 2565/10000, Prediction Accuracy = 61.366%, Loss = 0.43226011395454406
Epoch: 2565, Batch Gradient Norm: 9.888986634139986
Epoch: 2565, Batch Gradient Norm after: 9.888986634139986
Epoch 2566/10000, Prediction Accuracy = 61.4%, Loss = 0.42978727221488955
Epoch: 2566, Batch Gradient Norm: 10.023351791714695
Epoch: 2566, Batch Gradient Norm after: 10.023351791714695
Epoch 2567/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.4338081061840057
Epoch: 2567, Batch Gradient Norm: 9.455715677762875
Epoch: 2567, Batch Gradient Norm after: 9.455715677762875
Epoch 2568/10000, Prediction Accuracy = 61.40999999999999%, Loss = 0.42882617115974425
Epoch: 2568, Batch Gradient Norm: 8.919363826407361
Epoch: 2568, Batch Gradient Norm after: 8.919363826407361
Epoch 2569/10000, Prediction Accuracy = 61.4%, Loss = 0.42836777567863465
Epoch: 2569, Batch Gradient Norm: 9.977581688709861
Epoch: 2569, Batch Gradient Norm after: 9.977581688709861
Epoch 2570/10000, Prediction Accuracy = 61.354%, Loss = 0.42971562743186953
Epoch: 2570, Batch Gradient Norm: 8.941589224164376
Epoch: 2570, Batch Gradient Norm after: 8.941589224164376
Epoch 2571/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.42886302471160886
Epoch: 2571, Batch Gradient Norm: 8.990914482411393
Epoch: 2571, Batch Gradient Norm after: 8.990914482411393
Epoch 2572/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.42950510382652285
Epoch: 2572, Batch Gradient Norm: 10.546342185644608
Epoch: 2572, Batch Gradient Norm after: 10.546342185644608
Epoch 2573/10000, Prediction Accuracy = 61.35%, Loss = 0.43383556604385376
Epoch: 2573, Batch Gradient Norm: 10.710575575009859
Epoch: 2573, Batch Gradient Norm after: 10.710575575009859
Epoch 2574/10000, Prediction Accuracy = 61.31199999999999%, Loss = 0.43269121646881104
Epoch: 2574, Batch Gradient Norm: 11.101599780849744
Epoch: 2574, Batch Gradient Norm after: 11.101599780849744
Epoch 2575/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.43434221744537355
Epoch: 2575, Batch Gradient Norm: 10.01279552101349
Epoch: 2575, Batch Gradient Norm after: 10.01279552101349
Epoch 2576/10000, Prediction Accuracy = 61.376%, Loss = 0.43306995630264283
Epoch: 2576, Batch Gradient Norm: 11.946839149447921
Epoch: 2576, Batch Gradient Norm after: 11.946839149447921
Epoch 2577/10000, Prediction Accuracy = 61.412%, Loss = 0.43512662053108214
Epoch: 2577, Batch Gradient Norm: 10.741382712052143
Epoch: 2577, Batch Gradient Norm after: 10.741382712052143
Epoch 2578/10000, Prediction Accuracy = 61.352%, Loss = 0.4326641082763672
Epoch: 2578, Batch Gradient Norm: 11.385894588713873
Epoch: 2578, Batch Gradient Norm after: 11.385894588713873
Epoch 2579/10000, Prediction Accuracy = 61.444%, Loss = 0.4325827479362488
Epoch: 2579, Batch Gradient Norm: 13.387248769463348
Epoch: 2579, Batch Gradient Norm after: 13.387248769463348
Epoch 2580/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.4394108235836029
Epoch: 2580, Batch Gradient Norm: 13.290531445478072
Epoch: 2580, Batch Gradient Norm after: 13.290531445478072
Epoch 2581/10000, Prediction Accuracy = 61.31999999999999%, Loss = 0.4351218819618225
Epoch: 2581, Batch Gradient Norm: 11.294020464452814
Epoch: 2581, Batch Gradient Norm after: 11.294020464452814
Epoch 2582/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.43491303324699404
Epoch: 2582, Batch Gradient Norm: 13.037788337930365
Epoch: 2582, Batch Gradient Norm after: 13.037788337930365
Epoch 2583/10000, Prediction Accuracy = 61.35%, Loss = 0.43784133195877073
Epoch: 2583, Batch Gradient Norm: 15.884664197894951
Epoch: 2583, Batch Gradient Norm after: 15.884664197894951
Epoch 2584/10000, Prediction Accuracy = 61.36%, Loss = 0.4428888142108917
Epoch: 2584, Batch Gradient Norm: 15.83404760079103
Epoch: 2584, Batch Gradient Norm after: 15.377094262277032
Epoch 2585/10000, Prediction Accuracy = 61.266%, Loss = 0.44744451642036437
Epoch: 2585, Batch Gradient Norm: 14.966788401227948
Epoch: 2585, Batch Gradient Norm after: 14.966788401227948
Epoch 2586/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.4386385500431061
Epoch: 2586, Batch Gradient Norm: 13.574539449412503
Epoch: 2586, Batch Gradient Norm after: 13.574539449412503
Epoch 2587/10000, Prediction Accuracy = 61.386%, Loss = 0.43715088367462157
Epoch: 2587, Batch Gradient Norm: 13.476937168013423
Epoch: 2587, Batch Gradient Norm after: 13.476937168013423
Epoch 2588/10000, Prediction Accuracy = 61.431999999999995%, Loss = 0.433128696680069
Epoch: 2588, Batch Gradient Norm: 13.184096935553919
Epoch: 2588, Batch Gradient Norm after: 13.184096935553919
Epoch 2589/10000, Prediction Accuracy = 61.452%, Loss = 0.43401870131492615
Epoch: 2589, Batch Gradient Norm: 13.92162451958189
Epoch: 2589, Batch Gradient Norm after: 13.92162451958189
Epoch 2590/10000, Prediction Accuracy = 61.348%, Loss = 0.4394184648990631
Epoch: 2590, Batch Gradient Norm: 13.3192138563611
Epoch: 2590, Batch Gradient Norm after: 13.3192138563611
Epoch 2591/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.43596855998039247
Epoch: 2591, Batch Gradient Norm: 14.6965675780333
Epoch: 2591, Batch Gradient Norm after: 14.6965675780333
Epoch 2592/10000, Prediction Accuracy = 61.396%, Loss = 0.4382988750934601
Epoch: 2592, Batch Gradient Norm: 10.897661958266463
Epoch: 2592, Batch Gradient Norm after: 10.897661958266463
Epoch 2593/10000, Prediction Accuracy = 61.386%, Loss = 0.4309259593486786
Epoch: 2593, Batch Gradient Norm: 11.777494783642465
Epoch: 2593, Batch Gradient Norm after: 11.777494783642465
Epoch 2594/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.4330538630485535
Epoch: 2594, Batch Gradient Norm: 11.628896397576463
Epoch: 2594, Batch Gradient Norm after: 11.628896397576463
Epoch 2595/10000, Prediction Accuracy = 61.37399999999999%, Loss = 0.432110857963562
Epoch: 2595, Batch Gradient Norm: 9.487546297309695
Epoch: 2595, Batch Gradient Norm after: 9.487546297309695
Epoch 2596/10000, Prediction Accuracy = 61.378%, Loss = 0.4320824384689331
Epoch: 2596, Batch Gradient Norm: 8.245804215739884
Epoch: 2596, Batch Gradient Norm after: 8.245804215739884
Epoch 2597/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.4287692248821259
Epoch: 2597, Batch Gradient Norm: 7.400769448654632
Epoch: 2597, Batch Gradient Norm after: 7.400769448654632
Epoch 2598/10000, Prediction Accuracy = 61.424%, Loss = 0.42612751126289367
Epoch: 2598, Batch Gradient Norm: 9.469703529176863
Epoch: 2598, Batch Gradient Norm after: 9.469703529176863
Epoch 2599/10000, Prediction Accuracy = 61.516%, Loss = 0.4300257384777069
Epoch: 2599, Batch Gradient Norm: 11.433741514507036
Epoch: 2599, Batch Gradient Norm after: 11.433741514507036
Epoch 2600/10000, Prediction Accuracy = 61.45%, Loss = 0.43063479065895083
Epoch: 2600, Batch Gradient Norm: 11.55218770821713
Epoch: 2600, Batch Gradient Norm after: 11.55218770821713
Epoch 2601/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4312209367752075
Epoch: 2601, Batch Gradient Norm: 13.211269396974075
Epoch: 2601, Batch Gradient Norm after: 13.211269396974075
Epoch 2602/10000, Prediction Accuracy = 61.459999999999994%, Loss = 0.4336825668811798
Epoch: 2602, Batch Gradient Norm: 11.15895086755502
Epoch: 2602, Batch Gradient Norm after: 11.15895086755502
Epoch 2603/10000, Prediction Accuracy = 61.306%, Loss = 0.431977778673172
Epoch: 2603, Batch Gradient Norm: 10.769111388334762
Epoch: 2603, Batch Gradient Norm after: 10.769111388334762
Epoch 2604/10000, Prediction Accuracy = 61.471999999999994%, Loss = 0.43274533152580263
Epoch: 2604, Batch Gradient Norm: 12.210364472592135
Epoch: 2604, Batch Gradient Norm after: 12.210364472592135
Epoch 2605/10000, Prediction Accuracy = 61.367999999999995%, Loss = 0.4330118000507355
Epoch: 2605, Batch Gradient Norm: 12.628190951905962
Epoch: 2605, Batch Gradient Norm after: 12.628190951905962
Epoch 2606/10000, Prediction Accuracy = 61.448%, Loss = 0.43423835635185243
Epoch: 2606, Batch Gradient Norm: 14.154386777262209
Epoch: 2606, Batch Gradient Norm after: 14.154386777262209
Epoch 2607/10000, Prediction Accuracy = 61.504%, Loss = 0.43676968812942507
Epoch: 2607, Batch Gradient Norm: 16.190086438737456
Epoch: 2607, Batch Gradient Norm after: 16.190086438737456
Epoch 2608/10000, Prediction Accuracy = 61.38399999999999%, Loss = 0.43886908888816833
Epoch: 2608, Batch Gradient Norm: 14.407646207401612
Epoch: 2608, Batch Gradient Norm after: 14.407646207401612
Epoch 2609/10000, Prediction Accuracy = 61.36600000000001%, Loss = 0.43716583847999574
Epoch: 2609, Batch Gradient Norm: 18.155795484684294
Epoch: 2609, Batch Gradient Norm after: 18.155795484684294
Epoch 2610/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.4425902545452118
Epoch: 2610, Batch Gradient Norm: 16.59755263121706
Epoch: 2610, Batch Gradient Norm after: 16.284645838507345
Epoch 2611/10000, Prediction Accuracy = 61.354%, Loss = 0.44187707304954527
Epoch: 2611, Batch Gradient Norm: 13.538796706528185
Epoch: 2611, Batch Gradient Norm after: 13.538796706528185
Epoch 2612/10000, Prediction Accuracy = 61.386%, Loss = 0.4337378263473511
Epoch: 2612, Batch Gradient Norm: 12.696214664168178
Epoch: 2612, Batch Gradient Norm after: 12.696214664168178
Epoch 2613/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.4335492491722107
Epoch: 2613, Batch Gradient Norm: 15.214783937655255
Epoch: 2613, Batch Gradient Norm after: 15.214783937655255
Epoch 2614/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.43874263763427734
Epoch: 2614, Batch Gradient Norm: 15.668186933784453
Epoch: 2614, Batch Gradient Norm after: 15.668186933784453
Epoch 2615/10000, Prediction Accuracy = 61.416%, Loss = 0.441315621137619
Epoch: 2615, Batch Gradient Norm: 13.040383137388693
Epoch: 2615, Batch Gradient Norm after: 13.040383137388693
Epoch 2616/10000, Prediction Accuracy = 61.354000000000006%, Loss = 0.43437570333480835
Epoch: 2616, Batch Gradient Norm: 13.196416078628713
Epoch: 2616, Batch Gradient Norm after: 13.196416078628713
Epoch 2617/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.43542771339416503
Epoch: 2617, Batch Gradient Norm: 10.519578524291756
Epoch: 2617, Batch Gradient Norm after: 10.519578524291756
Epoch 2618/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.43069584369659425
Epoch: 2618, Batch Gradient Norm: 12.973745812194641
Epoch: 2618, Batch Gradient Norm after: 12.973745812194641
Epoch 2619/10000, Prediction Accuracy = 61.354000000000006%, Loss = 0.4359125554561615
Epoch: 2619, Batch Gradient Norm: 12.458344235118153
Epoch: 2619, Batch Gradient Norm after: 12.458344235118153
Epoch 2620/10000, Prediction Accuracy = 61.432%, Loss = 0.4335032045841217
Epoch: 2620, Batch Gradient Norm: 13.197780195617188
Epoch: 2620, Batch Gradient Norm after: 13.197780195617188
Epoch 2621/10000, Prediction Accuracy = 61.40400000000001%, Loss = 0.43627769947052003
Epoch: 2621, Batch Gradient Norm: 10.533327668924215
Epoch: 2621, Batch Gradient Norm after: 10.533327668924215
Epoch 2622/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.43294060230255127
Epoch: 2622, Batch Gradient Norm: 9.73421517634527
Epoch: 2622, Batch Gradient Norm after: 9.73421517634527
Epoch 2623/10000, Prediction Accuracy = 61.384%, Loss = 0.4279672265052795
Epoch: 2623, Batch Gradient Norm: 12.975233579644904
Epoch: 2623, Batch Gradient Norm after: 12.975233579644904
Epoch 2624/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.43322070837020876
Epoch: 2624, Batch Gradient Norm: 11.37212516693279
Epoch: 2624, Batch Gradient Norm after: 11.37212516693279
Epoch 2625/10000, Prediction Accuracy = 61.488%, Loss = 0.43053175806999205
Epoch: 2625, Batch Gradient Norm: 10.277940817834304
Epoch: 2625, Batch Gradient Norm after: 10.277940817834304
Epoch 2626/10000, Prediction Accuracy = 61.378%, Loss = 0.42851046323776243
Epoch: 2626, Batch Gradient Norm: 11.941795999518028
Epoch: 2626, Batch Gradient Norm after: 11.941795999518028
Epoch 2627/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.43100458979606626
Epoch: 2627, Batch Gradient Norm: 12.740070294771417
Epoch: 2627, Batch Gradient Norm after: 12.740070294771417
Epoch 2628/10000, Prediction Accuracy = 61.436%, Loss = 0.43302187919616697
Epoch: 2628, Batch Gradient Norm: 13.87277173786002
Epoch: 2628, Batch Gradient Norm after: 13.87277173786002
Epoch 2629/10000, Prediction Accuracy = 61.426%, Loss = 0.43459306955337523
Epoch: 2629, Batch Gradient Norm: 16.247705717495105
Epoch: 2629, Batch Gradient Norm after: 16.247705717495105
Epoch 2630/10000, Prediction Accuracy = 61.408%, Loss = 0.43693512082099917
Epoch: 2630, Batch Gradient Norm: 11.79404544834236
Epoch: 2630, Batch Gradient Norm after: 11.79404544834236
Epoch 2631/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4331969916820526
Epoch: 2631, Batch Gradient Norm: 9.544132531277823
Epoch: 2631, Batch Gradient Norm after: 9.544132531277823
Epoch 2632/10000, Prediction Accuracy = 61.386%, Loss = 0.42774116396903994
Epoch: 2632, Batch Gradient Norm: 9.096811845981744
Epoch: 2632, Batch Gradient Norm after: 9.096811845981744
Epoch 2633/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.4288515269756317
Epoch: 2633, Batch Gradient Norm: 9.191307816169553
Epoch: 2633, Batch Gradient Norm after: 9.191307816169553
Epoch 2634/10000, Prediction Accuracy = 61.476%, Loss = 0.42697409391403196
Epoch: 2634, Batch Gradient Norm: 11.348515504422812
Epoch: 2634, Batch Gradient Norm after: 11.348515504422812
Epoch 2635/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.43214109539985657
Epoch: 2635, Batch Gradient Norm: 11.32648121166731
Epoch: 2635, Batch Gradient Norm after: 11.32648121166731
Epoch 2636/10000, Prediction Accuracy = 61.354%, Loss = 0.4321468472480774
Epoch: 2636, Batch Gradient Norm: 14.049244474712495
Epoch: 2636, Batch Gradient Norm after: 14.049244474712495
Epoch 2637/10000, Prediction Accuracy = 61.384%, Loss = 0.43644769191741944
Epoch: 2637, Batch Gradient Norm: 15.406261793153831
Epoch: 2637, Batch Gradient Norm after: 15.406261793153831
Epoch 2638/10000, Prediction Accuracy = 61.334%, Loss = 0.4366313576698303
Epoch: 2638, Batch Gradient Norm: 13.884758952804196
Epoch: 2638, Batch Gradient Norm after: 13.884758952804196
Epoch 2639/10000, Prediction Accuracy = 61.538%, Loss = 0.4350208520889282
Epoch: 2639, Batch Gradient Norm: 13.764867167847225
Epoch: 2639, Batch Gradient Norm after: 13.764867167847225
Epoch 2640/10000, Prediction Accuracy = 61.45400000000001%, Loss = 0.4349770128726959
Epoch: 2640, Batch Gradient Norm: 14.775037175812853
Epoch: 2640, Batch Gradient Norm after: 14.775037175812853
Epoch 2641/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.43668261766433714
Epoch: 2641, Batch Gradient Norm: 13.527724776089826
Epoch: 2641, Batch Gradient Norm after: 13.527724776089826
Epoch 2642/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.43360854387283326
Epoch: 2642, Batch Gradient Norm: 14.008497807363966
Epoch: 2642, Batch Gradient Norm after: 14.008497807363966
Epoch 2643/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4333301901817322
Epoch: 2643, Batch Gradient Norm: 14.355279484457247
Epoch: 2643, Batch Gradient Norm after: 14.355279484457247
Epoch 2644/10000, Prediction Accuracy = 61.50600000000001%, Loss = 0.43521596789360045
Epoch: 2644, Batch Gradient Norm: 14.668862905254198
Epoch: 2644, Batch Gradient Norm after: 14.668862905254198
Epoch 2645/10000, Prediction Accuracy = 61.29200000000001%, Loss = 0.4358042299747467
Epoch: 2645, Batch Gradient Norm: 13.752326041314843
Epoch: 2645, Batch Gradient Norm after: 13.752326041314843
Epoch 2646/10000, Prediction Accuracy = 61.50600000000001%, Loss = 0.4330614984035492
Epoch: 2646, Batch Gradient Norm: 12.77390427831641
Epoch: 2646, Batch Gradient Norm after: 12.77390427831641
Epoch 2647/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.43071383237838745
Epoch: 2647, Batch Gradient Norm: 15.481765039362825
Epoch: 2647, Batch Gradient Norm after: 15.481765039362825
Epoch 2648/10000, Prediction Accuracy = 61.354%, Loss = 0.4359045684337616
Epoch: 2648, Batch Gradient Norm: 15.32662847835172
Epoch: 2648, Batch Gradient Norm after: 15.32662847835172
Epoch 2649/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4370604634284973
Epoch: 2649, Batch Gradient Norm: 12.509373849207954
Epoch: 2649, Batch Gradient Norm after: 12.509373849207954
Epoch 2650/10000, Prediction Accuracy = 61.504%, Loss = 0.43377443552017214
Epoch: 2650, Batch Gradient Norm: 12.960208723683209
Epoch: 2650, Batch Gradient Norm after: 12.960208723683209
Epoch 2651/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.43188172578811646
Epoch: 2651, Batch Gradient Norm: 8.970959736981708
Epoch: 2651, Batch Gradient Norm after: 8.970959736981708
Epoch 2652/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.4283193707466125
Epoch: 2652, Batch Gradient Norm: 9.778608004873682
Epoch: 2652, Batch Gradient Norm after: 9.778608004873682
Epoch 2653/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4286160945892334
Epoch: 2653, Batch Gradient Norm: 10.4362236073595
Epoch: 2653, Batch Gradient Norm after: 10.4362236073595
Epoch 2654/10000, Prediction Accuracy = 61.443999999999996%, Loss = 0.4293428361415863
Epoch: 2654, Batch Gradient Norm: 9.921282514398461
Epoch: 2654, Batch Gradient Norm after: 9.921282514398461
Epoch 2655/10000, Prediction Accuracy = 61.431999999999995%, Loss = 0.4268356680870056
Epoch: 2655, Batch Gradient Norm: 11.155281725468967
Epoch: 2655, Batch Gradient Norm after: 11.155281725468967
Epoch 2656/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.4283598005771637
Epoch: 2656, Batch Gradient Norm: 12.807189731877036
Epoch: 2656, Batch Gradient Norm after: 12.807189731877036
Epoch 2657/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4346823990345001
Epoch: 2657, Batch Gradient Norm: 11.670019308550879
Epoch: 2657, Batch Gradient Norm after: 11.670019308550879
Epoch 2658/10000, Prediction Accuracy = 61.428%, Loss = 0.43093246817588804
Epoch: 2658, Batch Gradient Norm: 11.562201360848844
Epoch: 2658, Batch Gradient Norm after: 11.562201360848844
Epoch 2659/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.43138944506645205
Epoch: 2659, Batch Gradient Norm: 11.778731348578571
Epoch: 2659, Batch Gradient Norm after: 11.778731348578571
Epoch 2660/10000, Prediction Accuracy = 61.362%, Loss = 0.4313370168209076
Epoch: 2660, Batch Gradient Norm: 11.165852648885917
Epoch: 2660, Batch Gradient Norm after: 11.165852648885917
Epoch 2661/10000, Prediction Accuracy = 61.492%, Loss = 0.4288589179515839
Epoch: 2661, Batch Gradient Norm: 12.442072812481321
Epoch: 2661, Batch Gradient Norm after: 12.442072812481321
Epoch 2662/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.4321584105491638
Epoch: 2662, Batch Gradient Norm: 11.51431212081085
Epoch: 2662, Batch Gradient Norm after: 11.51431212081085
Epoch 2663/10000, Prediction Accuracy = 61.46%, Loss = 0.4299250960350037
Epoch: 2663, Batch Gradient Norm: 12.90341488117795
Epoch: 2663, Batch Gradient Norm after: 12.90341488117795
Epoch 2664/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.43338821530342103
Epoch: 2664, Batch Gradient Norm: 12.475474169950303
Epoch: 2664, Batch Gradient Norm after: 12.475474169950303
Epoch 2665/10000, Prediction Accuracy = 61.438%, Loss = 0.43115063309669494
Epoch: 2665, Batch Gradient Norm: 12.980898757268637
Epoch: 2665, Batch Gradient Norm after: 12.980898757268637
Epoch 2666/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.43327802419662476
Epoch: 2666, Batch Gradient Norm: 13.776370604300665
Epoch: 2666, Batch Gradient Norm after: 13.776370604300665
Epoch 2667/10000, Prediction Accuracy = 61.42%, Loss = 0.4328453838825226
Epoch: 2667, Batch Gradient Norm: 13.457608954793187
Epoch: 2667, Batch Gradient Norm after: 13.457608954793187
Epoch 2668/10000, Prediction Accuracy = 61.424%, Loss = 0.4328918635845184
Epoch: 2668, Batch Gradient Norm: 15.916300407019019
Epoch: 2668, Batch Gradient Norm after: 15.916300407019019
Epoch 2669/10000, Prediction Accuracy = 61.376%, Loss = 0.43773104548454284
Epoch: 2669, Batch Gradient Norm: 11.0202733887794
Epoch: 2669, Batch Gradient Norm after: 11.0202733887794
Epoch 2670/10000, Prediction Accuracy = 61.50999999999999%, Loss = 0.4271387755870819
Epoch: 2670, Batch Gradient Norm: 11.029315082464697
Epoch: 2670, Batch Gradient Norm after: 11.029315082464697
Epoch 2671/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.428714382648468
Epoch: 2671, Batch Gradient Norm: 10.278483943019467
Epoch: 2671, Batch Gradient Norm after: 10.278483943019467
Epoch 2672/10000, Prediction Accuracy = 61.465999999999994%, Loss = 0.42679665684700013
Epoch: 2672, Batch Gradient Norm: 11.094713453139473
Epoch: 2672, Batch Gradient Norm after: 11.094713453139473
Epoch 2673/10000, Prediction Accuracy = 61.49399999999999%, Loss = 0.430552214384079
Epoch: 2673, Batch Gradient Norm: 11.918120134724264
Epoch: 2673, Batch Gradient Norm after: 11.918120134724264
Epoch 2674/10000, Prediction Accuracy = 61.488%, Loss = 0.4308218479156494
Epoch: 2674, Batch Gradient Norm: 10.434540234784565
Epoch: 2674, Batch Gradient Norm after: 10.434540234784565
Epoch 2675/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4292702078819275
Epoch: 2675, Batch Gradient Norm: 11.569964286659879
Epoch: 2675, Batch Gradient Norm after: 11.569964286659879
Epoch 2676/10000, Prediction Accuracy = 61.49000000000001%, Loss = 0.4288711488246918
Epoch: 2676, Batch Gradient Norm: 12.268588195232434
Epoch: 2676, Batch Gradient Norm after: 12.268588195232434
Epoch 2677/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.42969446778297427
Epoch: 2677, Batch Gradient Norm: 12.756595755719898
Epoch: 2677, Batch Gradient Norm after: 12.756595755719898
Epoch 2678/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.43136581778526306
Epoch: 2678, Batch Gradient Norm: 13.983786230888134
Epoch: 2678, Batch Gradient Norm after: 13.983786230888134
Epoch 2679/10000, Prediction Accuracy = 61.424%, Loss = 0.43611512184143064
Epoch: 2679, Batch Gradient Norm: 14.86039728857948
Epoch: 2679, Batch Gradient Norm after: 14.86039728857948
Epoch 2680/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.4344123721122742
Epoch: 2680, Batch Gradient Norm: 14.17787293807057
Epoch: 2680, Batch Gradient Norm after: 14.17787293807057
Epoch 2681/10000, Prediction Accuracy = 61.476%, Loss = 0.43184198141098024
Epoch: 2681, Batch Gradient Norm: 12.559215624721123
Epoch: 2681, Batch Gradient Norm after: 12.559215624721123
Epoch 2682/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.4302273690700531
Epoch: 2682, Batch Gradient Norm: 12.227825836902419
Epoch: 2682, Batch Gradient Norm after: 12.227825836902419
Epoch 2683/10000, Prediction Accuracy = 61.483999999999995%, Loss = 0.42993534207344053
Epoch: 2683, Batch Gradient Norm: 12.336959090013735
Epoch: 2683, Batch Gradient Norm after: 12.336959090013735
Epoch 2684/10000, Prediction Accuracy = 61.568%, Loss = 0.4288013935089111
Epoch: 2684, Batch Gradient Norm: 12.005820951684651
Epoch: 2684, Batch Gradient Norm after: 12.005820951684651
Epoch 2685/10000, Prediction Accuracy = 61.496%, Loss = 0.430917763710022
Epoch: 2685, Batch Gradient Norm: 15.447797978235394
Epoch: 2685, Batch Gradient Norm after: 15.447797978235394
Epoch 2686/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4349934935569763
Epoch: 2686, Batch Gradient Norm: 16.218669859345994
Epoch: 2686, Batch Gradient Norm after: 16.218669859345994
Epoch 2687/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4369125425815582
Epoch: 2687, Batch Gradient Norm: 14.221844125707957
Epoch: 2687, Batch Gradient Norm after: 14.221844125707957
Epoch 2688/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4335853934288025
Epoch: 2688, Batch Gradient Norm: 11.604783568182809
Epoch: 2688, Batch Gradient Norm after: 11.604783568182809
Epoch 2689/10000, Prediction Accuracy = 61.49000000000001%, Loss = 0.4298245370388031
Epoch: 2689, Batch Gradient Norm: 11.768525640964818
Epoch: 2689, Batch Gradient Norm after: 11.768525640964818
Epoch 2690/10000, Prediction Accuracy = 61.398%, Loss = 0.42868573069572447
Epoch: 2690, Batch Gradient Norm: 9.833280452775991
Epoch: 2690, Batch Gradient Norm after: 9.833280452775991
Epoch 2691/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.42695953249931334
Epoch: 2691, Batch Gradient Norm: 11.42341539500661
Epoch: 2691, Batch Gradient Norm after: 11.42341539500661
Epoch 2692/10000, Prediction Accuracy = 61.472%, Loss = 0.4292367100715637
Epoch: 2692, Batch Gradient Norm: 10.91813578717011
Epoch: 2692, Batch Gradient Norm after: 10.91813578717011
Epoch 2693/10000, Prediction Accuracy = 61.525999999999996%, Loss = 0.4277344226837158
Epoch: 2693, Batch Gradient Norm: 11.28496131380388
Epoch: 2693, Batch Gradient Norm after: 11.28496131380388
Epoch 2694/10000, Prediction Accuracy = 61.536%, Loss = 0.4288110375404358
Epoch: 2694, Batch Gradient Norm: 11.462926080688575
Epoch: 2694, Batch Gradient Norm after: 11.462926080688575
Epoch 2695/10000, Prediction Accuracy = 61.484%, Loss = 0.4287404239177704
Epoch: 2695, Batch Gradient Norm: 10.492928078799677
Epoch: 2695, Batch Gradient Norm after: 10.492928078799677
Epoch 2696/10000, Prediction Accuracy = 61.44200000000001%, Loss = 0.42755441665649413
Epoch: 2696, Batch Gradient Norm: 12.379988034646276
Epoch: 2696, Batch Gradient Norm after: 12.379988034646276
Epoch 2697/10000, Prediction Accuracy = 61.444%, Loss = 0.4289312124252319
Epoch: 2697, Batch Gradient Norm: 12.58054598800016
Epoch: 2697, Batch Gradient Norm after: 12.58054598800016
Epoch 2698/10000, Prediction Accuracy = 61.49399999999999%, Loss = 0.42943293452262876
Epoch: 2698, Batch Gradient Norm: 11.640724230013749
Epoch: 2698, Batch Gradient Norm after: 11.640724230013749
Epoch 2699/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.42852063179016114
Epoch: 2699, Batch Gradient Norm: 9.781714281610208
Epoch: 2699, Batch Gradient Norm after: 9.781714281610208
Epoch 2700/10000, Prediction Accuracy = 61.53399999999999%, Loss = 0.428578519821167
Epoch: 2700, Batch Gradient Norm: 10.673886580981003
Epoch: 2700, Batch Gradient Norm after: 10.673886580981003
Epoch 2701/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.42806971073150635
Epoch: 2701, Batch Gradient Norm: 15.130744407421545
Epoch: 2701, Batch Gradient Norm after: 15.130744407421545
Epoch 2702/10000, Prediction Accuracy = 61.5%, Loss = 0.43412638902664186
Epoch: 2702, Batch Gradient Norm: 11.814438862721556
Epoch: 2702, Batch Gradient Norm after: 11.814438862721556
Epoch 2703/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.42798041105270385
Epoch: 2703, Batch Gradient Norm: 11.159233088729437
Epoch: 2703, Batch Gradient Norm after: 11.159233088729437
Epoch 2704/10000, Prediction Accuracy = 61.528%, Loss = 0.4276441097259521
Epoch: 2704, Batch Gradient Norm: 11.595098092635851
Epoch: 2704, Batch Gradient Norm after: 11.595098092635851
Epoch 2705/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.4271796941757202
Epoch: 2705, Batch Gradient Norm: 11.101494010259083
Epoch: 2705, Batch Gradient Norm after: 11.101494010259083
Epoch 2706/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.42572306394577025
Epoch: 2706, Batch Gradient Norm: 10.199299219385374
Epoch: 2706, Batch Gradient Norm after: 10.199299219385374
Epoch 2707/10000, Prediction Accuracy = 61.525999999999996%, Loss = 0.42538503408432005
Epoch: 2707, Batch Gradient Norm: 10.51669741021692
Epoch: 2707, Batch Gradient Norm after: 10.51669741021692
Epoch 2708/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.42431487441062926
Epoch: 2708, Batch Gradient Norm: 11.7806738224431
Epoch: 2708, Batch Gradient Norm after: 11.7806738224431
Epoch 2709/10000, Prediction Accuracy = 61.528%, Loss = 0.4273643672466278
Epoch: 2709, Batch Gradient Norm: 10.378331991190015
Epoch: 2709, Batch Gradient Norm after: 10.378331991190015
Epoch 2710/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.4251662909984589
Epoch: 2710, Batch Gradient Norm: 9.657223568349616
Epoch: 2710, Batch Gradient Norm after: 9.657223568349616
Epoch 2711/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.4275708973407745
Epoch: 2711, Batch Gradient Norm: 10.694604329898624
Epoch: 2711, Batch Gradient Norm after: 10.694604329898624
Epoch 2712/10000, Prediction Accuracy = 61.468%, Loss = 0.42638221979141233
Epoch: 2712, Batch Gradient Norm: 11.853377456563457
Epoch: 2712, Batch Gradient Norm after: 11.853377456563457
Epoch 2713/10000, Prediction Accuracy = 61.467999999999996%, Loss = 0.4262762188911438
Epoch: 2713, Batch Gradient Norm: 10.068484110573413
Epoch: 2713, Batch Gradient Norm after: 10.068484110573413
Epoch 2714/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4266382396221161
Epoch: 2714, Batch Gradient Norm: 10.19412420335913
Epoch: 2714, Batch Gradient Norm after: 10.19412420335913
Epoch 2715/10000, Prediction Accuracy = 61.46%, Loss = 0.42690995931625364
Epoch: 2715, Batch Gradient Norm: 12.132326625529316
Epoch: 2715, Batch Gradient Norm after: 12.132326625529316
Epoch 2716/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.427716064453125
Epoch: 2716, Batch Gradient Norm: 10.201158723932489
Epoch: 2716, Batch Gradient Norm after: 10.201158723932489
Epoch 2717/10000, Prediction Accuracy = 61.532%, Loss = 0.4253819704055786
Epoch: 2717, Batch Gradient Norm: 10.326060906408566
Epoch: 2717, Batch Gradient Norm after: 10.326060906408566
Epoch 2718/10000, Prediction Accuracy = 61.598%, Loss = 0.42622604966163635
Epoch: 2718, Batch Gradient Norm: 14.31542542741082
Epoch: 2718, Batch Gradient Norm after: 14.31542542741082
Epoch 2719/10000, Prediction Accuracy = 61.540000000000006%, Loss = 0.43057951927185056
Epoch: 2719, Batch Gradient Norm: 12.965369430082912
Epoch: 2719, Batch Gradient Norm after: 12.965369430082912
Epoch 2720/10000, Prediction Accuracy = 61.576%, Loss = 0.42830405235290525
Epoch: 2720, Batch Gradient Norm: 12.787858078834198
Epoch: 2720, Batch Gradient Norm after: 12.787858078834198
Epoch 2721/10000, Prediction Accuracy = 61.414%, Loss = 0.427949994802475
Epoch: 2721, Batch Gradient Norm: 10.302084936882075
Epoch: 2721, Batch Gradient Norm after: 10.302084936882075
Epoch 2722/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.424201512336731
Epoch: 2722, Batch Gradient Norm: 11.84000908402385
Epoch: 2722, Batch Gradient Norm after: 11.84000908402385
Epoch 2723/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.42746599912643435
Epoch: 2723, Batch Gradient Norm: 12.242835393633396
Epoch: 2723, Batch Gradient Norm after: 12.242835393633396
Epoch 2724/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4263406813144684
Epoch: 2724, Batch Gradient Norm: 8.837777264460922
Epoch: 2724, Batch Gradient Norm after: 8.837777264460922
Epoch 2725/10000, Prediction Accuracy = 61.553999999999995%, Loss = 0.4220955431461334
Epoch: 2725, Batch Gradient Norm: 11.332835995326187
Epoch: 2725, Batch Gradient Norm after: 11.332835995326187
Epoch 2726/10000, Prediction Accuracy = 61.492%, Loss = 0.4268870949745178
Epoch: 2726, Batch Gradient Norm: 11.791305180659355
Epoch: 2726, Batch Gradient Norm after: 11.791305180659355
Epoch 2727/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.42749407291412356
Epoch: 2727, Batch Gradient Norm: 13.19276359271038
Epoch: 2727, Batch Gradient Norm after: 13.19276359271038
Epoch 2728/10000, Prediction Accuracy = 61.556%, Loss = 0.43017011880874634
Epoch: 2728, Batch Gradient Norm: 16.20557449143131
Epoch: 2728, Batch Gradient Norm after: 16.20557449143131
Epoch 2729/10000, Prediction Accuracy = 61.504%, Loss = 0.43578886389732363
Epoch: 2729, Batch Gradient Norm: 11.766010546751719
Epoch: 2729, Batch Gradient Norm after: 11.766010546751719
Epoch 2730/10000, Prediction Accuracy = 61.544%, Loss = 0.42843422293663025
Epoch: 2730, Batch Gradient Norm: 11.55197297641111
Epoch: 2730, Batch Gradient Norm after: 11.55197297641111
Epoch 2731/10000, Prediction Accuracy = 61.484%, Loss = 0.4266403019428253
Epoch: 2731, Batch Gradient Norm: 12.608041153227035
Epoch: 2731, Batch Gradient Norm after: 12.608041153227035
Epoch 2732/10000, Prediction Accuracy = 61.42%, Loss = 0.4288453280925751
Epoch: 2732, Batch Gradient Norm: 13.64054747834014
Epoch: 2732, Batch Gradient Norm after: 13.64054747834014
Epoch 2733/10000, Prediction Accuracy = 61.398%, Loss = 0.4290564298629761
Epoch: 2733, Batch Gradient Norm: 14.782202973876595
Epoch: 2733, Batch Gradient Norm after: 14.782202973876595
Epoch 2734/10000, Prediction Accuracy = 61.48%, Loss = 0.4338660418987274
Epoch: 2734, Batch Gradient Norm: 12.571369271744826
Epoch: 2734, Batch Gradient Norm after: 12.571369271744826
Epoch 2735/10000, Prediction Accuracy = 61.538%, Loss = 0.4293157458305359
Epoch: 2735, Batch Gradient Norm: 10.271242199463194
Epoch: 2735, Batch Gradient Norm after: 10.271242199463194
Epoch 2736/10000, Prediction Accuracy = 61.586%, Loss = 0.42511374950408937
Epoch: 2736, Batch Gradient Norm: 12.3127499517129
Epoch: 2736, Batch Gradient Norm after: 12.3127499517129
Epoch 2737/10000, Prediction Accuracy = 61.512%, Loss = 0.43026353120803834
Epoch: 2737, Batch Gradient Norm: 12.375177995023225
Epoch: 2737, Batch Gradient Norm after: 12.375177995023225
Epoch 2738/10000, Prediction Accuracy = 61.65%, Loss = 0.426774388551712
Epoch: 2738, Batch Gradient Norm: 10.26769169005217
Epoch: 2738, Batch Gradient Norm after: 10.26769169005217
Epoch 2739/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.42558999061584474
Epoch: 2739, Batch Gradient Norm: 10.704692930097362
Epoch: 2739, Batch Gradient Norm after: 10.704692930097362
Epoch 2740/10000, Prediction Accuracy = 61.572%, Loss = 0.4255469262599945
Epoch: 2740, Batch Gradient Norm: 9.937181739446448
Epoch: 2740, Batch Gradient Norm after: 9.937181739446448
Epoch 2741/10000, Prediction Accuracy = 61.604%, Loss = 0.4237129032611847
Epoch: 2741, Batch Gradient Norm: 10.278596335728494
Epoch: 2741, Batch Gradient Norm after: 10.278596335728494
Epoch 2742/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.42486347556114196
Epoch: 2742, Batch Gradient Norm: 8.825875739431911
Epoch: 2742, Batch Gradient Norm after: 8.825875739431911
Epoch 2743/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.4216174840927124
Epoch: 2743, Batch Gradient Norm: 8.848590291811915
Epoch: 2743, Batch Gradient Norm after: 8.848590291811915
Epoch 2744/10000, Prediction Accuracy = 61.702%, Loss = 0.42148791551589965
Epoch: 2744, Batch Gradient Norm: 10.734635909929313
Epoch: 2744, Batch Gradient Norm after: 10.734635909929313
Epoch 2745/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.42571851015090945
Epoch: 2745, Batch Gradient Norm: 12.393314468979272
Epoch: 2745, Batch Gradient Norm after: 12.393314468979272
Epoch 2746/10000, Prediction Accuracy = 61.541999999999994%, Loss = 0.4288892149925232
Epoch: 2746, Batch Gradient Norm: 12.50058616986997
Epoch: 2746, Batch Gradient Norm after: 12.50058616986997
Epoch 2747/10000, Prediction Accuracy = 61.55%, Loss = 0.42763214707374575
Epoch: 2747, Batch Gradient Norm: 12.734851541356214
Epoch: 2747, Batch Gradient Norm after: 12.734851541356214
Epoch 2748/10000, Prediction Accuracy = 61.548%, Loss = 0.4270244538784027
Epoch: 2748, Batch Gradient Norm: 11.034928989270382
Epoch: 2748, Batch Gradient Norm after: 11.034928989270382
Epoch 2749/10000, Prediction Accuracy = 61.50999999999999%, Loss = 0.426931095123291
Epoch: 2749, Batch Gradient Norm: 9.435304519759452
Epoch: 2749, Batch Gradient Norm after: 9.435304519759452
Epoch 2750/10000, Prediction Accuracy = 61.55799999999999%, Loss = 0.42237833738327024
Epoch: 2750, Batch Gradient Norm: 11.207072963090026
Epoch: 2750, Batch Gradient Norm after: 11.207072963090026
Epoch 2751/10000, Prediction Accuracy = 61.53399999999999%, Loss = 0.42405312657356264
Epoch: 2751, Batch Gradient Norm: 8.99440832435463
Epoch: 2751, Batch Gradient Norm after: 8.99440832435463
Epoch 2752/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.42318437099456785
Epoch: 2752, Batch Gradient Norm: 10.605021089110277
Epoch: 2752, Batch Gradient Norm after: 10.605021089110277
Epoch 2753/10000, Prediction Accuracy = 61.538%, Loss = 0.42493385076522827
Epoch: 2753, Batch Gradient Norm: 11.232078785402022
Epoch: 2753, Batch Gradient Norm after: 11.232078785402022
Epoch 2754/10000, Prediction Accuracy = 61.516%, Loss = 0.42336413264274597
Epoch: 2754, Batch Gradient Norm: 9.507035931639685
Epoch: 2754, Batch Gradient Norm after: 9.507035931639685
Epoch 2755/10000, Prediction Accuracy = 61.48599999999999%, Loss = 0.42255117297172545
Epoch: 2755, Batch Gradient Norm: 12.60998413318287
Epoch: 2755, Batch Gradient Norm after: 12.60998413318287
Epoch 2756/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4259282350540161
Epoch: 2756, Batch Gradient Norm: 11.399406022425504
Epoch: 2756, Batch Gradient Norm after: 11.399406022425504
Epoch 2757/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.42536233067512513
Epoch: 2757, Batch Gradient Norm: 16.196215192413813
Epoch: 2757, Batch Gradient Norm after: 16.196215192413813
Epoch 2758/10000, Prediction Accuracy = 61.488%, Loss = 0.43496874570846555
Epoch: 2758, Batch Gradient Norm: 14.510757369470745
Epoch: 2758, Batch Gradient Norm after: 14.510757369470745
Epoch 2759/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.42875474095344546
Epoch: 2759, Batch Gradient Norm: 12.815338872175735
Epoch: 2759, Batch Gradient Norm after: 12.815338872175735
Epoch 2760/10000, Prediction Accuracy = 61.516000000000005%, Loss = 0.42744205594062806
Epoch: 2760, Batch Gradient Norm: 12.973112194764694
Epoch: 2760, Batch Gradient Norm after: 12.973112194764694
Epoch 2761/10000, Prediction Accuracy = 61.574%, Loss = 0.42897787094116213
Epoch: 2761, Batch Gradient Norm: 10.97381320914068
Epoch: 2761, Batch Gradient Norm after: 10.97381320914068
Epoch 2762/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.42527684569358826
Epoch: 2762, Batch Gradient Norm: 11.441359119352033
Epoch: 2762, Batch Gradient Norm after: 11.441359119352033
Epoch 2763/10000, Prediction Accuracy = 61.49000000000001%, Loss = 0.4250901162624359
Epoch: 2763, Batch Gradient Norm: 12.508942044593462
Epoch: 2763, Batch Gradient Norm after: 12.508942044593462
Epoch 2764/10000, Prediction Accuracy = 61.61%, Loss = 0.42680950164794923
Epoch: 2764, Batch Gradient Norm: 12.613015098197817
Epoch: 2764, Batch Gradient Norm after: 12.613015098197817
Epoch 2765/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.42716001272201537
Epoch: 2765, Batch Gradient Norm: 12.221603061812116
Epoch: 2765, Batch Gradient Norm after: 12.221603061812116
Epoch 2766/10000, Prediction Accuracy = 61.536%, Loss = 0.4266279637813568
Epoch: 2766, Batch Gradient Norm: 13.307638410852775
Epoch: 2766, Batch Gradient Norm after: 13.307638410852775
Epoch 2767/10000, Prediction Accuracy = 61.55400000000001%, Loss = 0.4299650251865387
Epoch: 2767, Batch Gradient Norm: 12.690084905402806
Epoch: 2767, Batch Gradient Norm after: 12.690084905402806
Epoch 2768/10000, Prediction Accuracy = 61.51800000000001%, Loss = 0.42569360733032224
Epoch: 2768, Batch Gradient Norm: 12.300854306250038
Epoch: 2768, Batch Gradient Norm after: 12.300854306250038
Epoch 2769/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4251718640327454
Epoch: 2769, Batch Gradient Norm: 9.469117426694078
Epoch: 2769, Batch Gradient Norm after: 9.469117426694078
Epoch 2770/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.42236303091049193
Epoch: 2770, Batch Gradient Norm: 9.518697182949417
Epoch: 2770, Batch Gradient Norm after: 9.518697182949417
Epoch 2771/10000, Prediction Accuracy = 61.622%, Loss = 0.4208677768707275
Epoch: 2771, Batch Gradient Norm: 10.146379926368459
Epoch: 2771, Batch Gradient Norm after: 10.146379926368459
Epoch 2772/10000, Prediction Accuracy = 61.562%, Loss = 0.4224547863006592
Epoch: 2772, Batch Gradient Norm: 13.145378670897687
Epoch: 2772, Batch Gradient Norm after: 13.145378670897687
Epoch 2773/10000, Prediction Accuracy = 61.548%, Loss = 0.4275350034236908
Epoch: 2773, Batch Gradient Norm: 10.331658510539345
Epoch: 2773, Batch Gradient Norm after: 10.331658510539345
Epoch 2774/10000, Prediction Accuracy = 61.564%, Loss = 0.4250779688358307
Epoch: 2774, Batch Gradient Norm: 9.194608447911362
Epoch: 2774, Batch Gradient Norm after: 9.194608447911362
Epoch 2775/10000, Prediction Accuracy = 61.56%, Loss = 0.4218056619167328
Epoch: 2775, Batch Gradient Norm: 12.84217272558317
Epoch: 2775, Batch Gradient Norm after: 12.84217272558317
Epoch 2776/10000, Prediction Accuracy = 61.61%, Loss = 0.42714176177978513
Epoch: 2776, Batch Gradient Norm: 13.137845615525338
Epoch: 2776, Batch Gradient Norm after: 13.137845615525338
Epoch 2777/10000, Prediction Accuracy = 61.528%, Loss = 0.42672659158706666
Epoch: 2777, Batch Gradient Norm: 13.201982863592903
Epoch: 2777, Batch Gradient Norm after: 13.201982863592903
Epoch 2778/10000, Prediction Accuracy = 61.576%, Loss = 0.42819978594779967
Epoch: 2778, Batch Gradient Norm: 14.23982192991633
Epoch: 2778, Batch Gradient Norm after: 14.23982192991633
Epoch 2779/10000, Prediction Accuracy = 61.59000000000001%, Loss = 0.4288601577281952
Epoch: 2779, Batch Gradient Norm: 10.717079476695279
Epoch: 2779, Batch Gradient Norm after: 10.717079476695279
Epoch 2780/10000, Prediction Accuracy = 61.598%, Loss = 0.4225423276424408
Epoch: 2780, Batch Gradient Norm: 11.796467968327445
Epoch: 2780, Batch Gradient Norm after: 11.796467968327445
Epoch 2781/10000, Prediction Accuracy = 61.556%, Loss = 0.42561944127082824
Epoch: 2781, Batch Gradient Norm: 10.759512629778579
Epoch: 2781, Batch Gradient Norm after: 10.759512629778579
Epoch 2782/10000, Prediction Accuracy = 61.562%, Loss = 0.42375180721282957
Epoch: 2782, Batch Gradient Norm: 10.900015419139038
Epoch: 2782, Batch Gradient Norm after: 10.900015419139038
Epoch 2783/10000, Prediction Accuracy = 61.51400000000001%, Loss = 0.42458441853523254
Epoch: 2783, Batch Gradient Norm: 11.852296755182243
Epoch: 2783, Batch Gradient Norm after: 11.852296755182243
Epoch 2784/10000, Prediction Accuracy = 61.646%, Loss = 0.4249221920967102
Epoch: 2784, Batch Gradient Norm: 11.662743710525964
Epoch: 2784, Batch Gradient Norm after: 11.662743710525964
Epoch 2785/10000, Prediction Accuracy = 61.674%, Loss = 0.4251418113708496
Epoch: 2785, Batch Gradient Norm: 12.381441249072957
Epoch: 2785, Batch Gradient Norm after: 12.381441249072957
Epoch 2786/10000, Prediction Accuracy = 61.562%, Loss = 0.42674133777618406
Epoch: 2786, Batch Gradient Norm: 9.518721510644307
Epoch: 2786, Batch Gradient Norm after: 9.518721510644307
Epoch 2787/10000, Prediction Accuracy = 61.61%, Loss = 0.4227972269058228
Epoch: 2787, Batch Gradient Norm: 10.588606894832608
Epoch: 2787, Batch Gradient Norm after: 10.588606894832608
Epoch 2788/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.42312361001968385
Epoch: 2788, Batch Gradient Norm: 9.814367741139032
Epoch: 2788, Batch Gradient Norm after: 9.814367741139032
Epoch 2789/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.41999120712280275
Epoch: 2789, Batch Gradient Norm: 8.384622835076776
Epoch: 2789, Batch Gradient Norm after: 8.384622835076776
Epoch 2790/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.41931074261665346
Epoch: 2790, Batch Gradient Norm: 8.667788205822795
Epoch: 2790, Batch Gradient Norm after: 8.667788205822795
Epoch 2791/10000, Prediction Accuracy = 61.568%, Loss = 0.4211151361465454
Epoch: 2791, Batch Gradient Norm: 9.843002581729296
Epoch: 2791, Batch Gradient Norm after: 9.843002581729296
Epoch 2792/10000, Prediction Accuracy = 61.512%, Loss = 0.4232570886611938
Epoch: 2792, Batch Gradient Norm: 9.800459029902632
Epoch: 2792, Batch Gradient Norm after: 9.800459029902632
Epoch 2793/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4233274281024933
Epoch: 2793, Batch Gradient Norm: 9.0623552522999
Epoch: 2793, Batch Gradient Norm after: 9.0623552522999
Epoch 2794/10000, Prediction Accuracy = 61.64%, Loss = 0.4200074732303619
Epoch: 2794, Batch Gradient Norm: 12.93801401319939
Epoch: 2794, Batch Gradient Norm after: 12.93801401319939
Epoch 2795/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.4260681807994843
Epoch: 2795, Batch Gradient Norm: 16.52830330750133
Epoch: 2795, Batch Gradient Norm after: 16.52830330750133
Epoch 2796/10000, Prediction Accuracy = 61.572%, Loss = 0.4339480817317963
Epoch: 2796, Batch Gradient Norm: 12.73317841650027
Epoch: 2796, Batch Gradient Norm after: 12.73317841650027
Epoch 2797/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.42635133266448977
Epoch: 2797, Batch Gradient Norm: 10.239116856737136
Epoch: 2797, Batch Gradient Norm after: 10.239116856737136
Epoch 2798/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.42233026027679443
Epoch: 2798, Batch Gradient Norm: 10.492624074793506
Epoch: 2798, Batch Gradient Norm after: 10.492624074793506
Epoch 2799/10000, Prediction Accuracy = 61.524%, Loss = 0.42155482769012453
Epoch: 2799, Batch Gradient Norm: 12.945997595674301
Epoch: 2799, Batch Gradient Norm after: 12.945997595674301
Epoch 2800/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.42752133011817933
Epoch: 2800, Batch Gradient Norm: 12.713302398670095
Epoch: 2800, Batch Gradient Norm after: 12.713302398670095
Epoch 2801/10000, Prediction Accuracy = 61.612%, Loss = 0.4249500632286072
Epoch: 2801, Batch Gradient Norm: 11.390060771635987
Epoch: 2801, Batch Gradient Norm after: 11.390060771635987
Epoch 2802/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4241697371006012
Epoch: 2802, Batch Gradient Norm: 10.372940624084757
Epoch: 2802, Batch Gradient Norm after: 10.372940624084757
Epoch 2803/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4230246961116791
Epoch: 2803, Batch Gradient Norm: 11.118612533218425
Epoch: 2803, Batch Gradient Norm after: 11.118612533218425
Epoch 2804/10000, Prediction Accuracy = 61.55%, Loss = 0.4236384749412537
Epoch: 2804, Batch Gradient Norm: 9.588133753215516
Epoch: 2804, Batch Gradient Norm after: 9.588133753215516
Epoch 2805/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4215954065322876
Epoch: 2805, Batch Gradient Norm: 13.0537058053256
Epoch: 2805, Batch Gradient Norm after: 13.0537058053256
Epoch 2806/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4252139151096344
Epoch: 2806, Batch Gradient Norm: 10.232631135362979
Epoch: 2806, Batch Gradient Norm after: 10.232631135362979
Epoch 2807/10000, Prediction Accuracy = 61.648%, Loss = 0.42406784892082217
Epoch: 2807, Batch Gradient Norm: 12.505823248528078
Epoch: 2807, Batch Gradient Norm after: 12.505823248528078
Epoch 2808/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.42642465233802795
Epoch: 2808, Batch Gradient Norm: 10.709728912804653
Epoch: 2808, Batch Gradient Norm after: 10.709728912804653
Epoch 2809/10000, Prediction Accuracy = 61.61%, Loss = 0.4239192962646484
Epoch: 2809, Batch Gradient Norm: 8.101204705287872
Epoch: 2809, Batch Gradient Norm after: 8.101204705287872
Epoch 2810/10000, Prediction Accuracy = 61.584%, Loss = 0.4182584285736084
Epoch: 2810, Batch Gradient Norm: 10.415470919052792
Epoch: 2810, Batch Gradient Norm after: 10.415470919052792
Epoch 2811/10000, Prediction Accuracy = 61.52%, Loss = 0.42184402942657473
Epoch: 2811, Batch Gradient Norm: 9.532330890869664
Epoch: 2811, Batch Gradient Norm after: 9.532330890869664
Epoch 2812/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.4212149977684021
Epoch: 2812, Batch Gradient Norm: 10.788193485542521
Epoch: 2812, Batch Gradient Norm after: 10.788193485542521
Epoch 2813/10000, Prediction Accuracy = 61.612%, Loss = 0.42219380140304563
Epoch: 2813, Batch Gradient Norm: 11.156381643985224
Epoch: 2813, Batch Gradient Norm after: 11.156381643985224
Epoch 2814/10000, Prediction Accuracy = 61.602%, Loss = 0.42353113293647765
Epoch: 2814, Batch Gradient Norm: 11.724205584354191
Epoch: 2814, Batch Gradient Norm after: 11.724205584354191
Epoch 2815/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.42464863061904906
Epoch: 2815, Batch Gradient Norm: 12.214493134204602
Epoch: 2815, Batch Gradient Norm after: 12.214493134204602
Epoch 2816/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.4250070214271545
Epoch: 2816, Batch Gradient Norm: 11.200955114726124
Epoch: 2816, Batch Gradient Norm after: 11.200955114726124
Epoch 2817/10000, Prediction Accuracy = 61.588%, Loss = 0.4231749653816223
Epoch: 2817, Batch Gradient Norm: 9.88844686602835
Epoch: 2817, Batch Gradient Norm after: 9.88844686602835
Epoch 2818/10000, Prediction Accuracy = 61.714%, Loss = 0.42103737592697144
Epoch: 2818, Batch Gradient Norm: 10.851073764638226
Epoch: 2818, Batch Gradient Norm after: 10.851073764638226
Epoch 2819/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.42203959822654724
Epoch: 2819, Batch Gradient Norm: 13.231033898283467
Epoch: 2819, Batch Gradient Norm after: 13.231033898283467
Epoch 2820/10000, Prediction Accuracy = 61.652%, Loss = 0.4259441316127777
Epoch: 2820, Batch Gradient Norm: 12.906171102231799
Epoch: 2820, Batch Gradient Norm after: 12.906171102231799
Epoch 2821/10000, Prediction Accuracy = 61.75%, Loss = 0.4246200442314148
Epoch: 2821, Batch Gradient Norm: 9.34831809463542
Epoch: 2821, Batch Gradient Norm after: 9.34831809463542
Epoch 2822/10000, Prediction Accuracy = 61.58%, Loss = 0.4202640295028687
Epoch: 2822, Batch Gradient Norm: 9.721231787831867
Epoch: 2822, Batch Gradient Norm after: 9.721231787831867
Epoch 2823/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.42219179272651675
Epoch: 2823, Batch Gradient Norm: 9.510452193390877
Epoch: 2823, Batch Gradient Norm after: 9.510452193390877
Epoch 2824/10000, Prediction Accuracy = 61.634%, Loss = 0.4187133312225342
Epoch: 2824, Batch Gradient Norm: 10.018654521262288
Epoch: 2824, Batch Gradient Norm after: 10.018654521262288
Epoch 2825/10000, Prediction Accuracy = 61.574%, Loss = 0.42246098518371583
Epoch: 2825, Batch Gradient Norm: 11.639912118853688
Epoch: 2825, Batch Gradient Norm after: 11.639912118853688
Epoch 2826/10000, Prediction Accuracy = 61.666%, Loss = 0.4230477690696716
Epoch: 2826, Batch Gradient Norm: 11.395428851573403
Epoch: 2826, Batch Gradient Norm after: 11.395428851573403
Epoch 2827/10000, Prediction Accuracy = 61.636%, Loss = 0.4222091495990753
Epoch: 2827, Batch Gradient Norm: 11.91948609954906
Epoch: 2827, Batch Gradient Norm after: 11.91948609954906
Epoch 2828/10000, Prediction Accuracy = 61.698%, Loss = 0.42367965579032896
Epoch: 2828, Batch Gradient Norm: 14.43579038032377
Epoch: 2828, Batch Gradient Norm after: 14.43579038032377
Epoch 2829/10000, Prediction Accuracy = 61.61%, Loss = 0.4301139712333679
Epoch: 2829, Batch Gradient Norm: 14.352905843344836
Epoch: 2829, Batch Gradient Norm after: 14.352905843344836
Epoch 2830/10000, Prediction Accuracy = 61.6%, Loss = 0.4275739431381226
Epoch: 2830, Batch Gradient Norm: 11.64816422813088
Epoch: 2830, Batch Gradient Norm after: 11.64816422813088
Epoch 2831/10000, Prediction Accuracy = 61.69%, Loss = 0.42185998558998106
Epoch: 2831, Batch Gradient Norm: 10.833982112986623
Epoch: 2831, Batch Gradient Norm after: 10.833982112986623
Epoch 2832/10000, Prediction Accuracy = 61.576%, Loss = 0.4197585225105286
Epoch: 2832, Batch Gradient Norm: 12.662067019112829
Epoch: 2832, Batch Gradient Norm after: 12.662067019112829
Epoch 2833/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.4250019252300262
Epoch: 2833, Batch Gradient Norm: 11.114766388624842
Epoch: 2833, Batch Gradient Norm after: 11.114766388624842
Epoch 2834/10000, Prediction Accuracy = 61.55400000000001%, Loss = 0.42351712584495543
Epoch: 2834, Batch Gradient Norm: 9.244587702462384
Epoch: 2834, Batch Gradient Norm after: 9.244587702462384
Epoch 2835/10000, Prediction Accuracy = 61.556%, Loss = 0.4184359431266785
Epoch: 2835, Batch Gradient Norm: 11.145549266353907
Epoch: 2835, Batch Gradient Norm after: 11.145549266353907
Epoch 2836/10000, Prediction Accuracy = 61.674%, Loss = 0.42236738801002505
Epoch: 2836, Batch Gradient Norm: 9.67622443064457
Epoch: 2836, Batch Gradient Norm after: 9.67622443064457
Epoch 2837/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.420133513212204
Epoch: 2837, Batch Gradient Norm: 10.275414522951856
Epoch: 2837, Batch Gradient Norm after: 10.275414522951856
Epoch 2838/10000, Prediction Accuracy = 61.49000000000001%, Loss = 0.4198767840862274
Epoch: 2838, Batch Gradient Norm: 10.888581441885835
Epoch: 2838, Batch Gradient Norm after: 10.888581441885835
Epoch 2839/10000, Prediction Accuracy = 61.6%, Loss = 0.42519883513450624
Epoch: 2839, Batch Gradient Norm: 12.979353347473648
Epoch: 2839, Batch Gradient Norm after: 12.979353347473648
Epoch 2840/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4277454435825348
Epoch: 2840, Batch Gradient Norm: 12.947587381427356
Epoch: 2840, Batch Gradient Norm after: 12.947587381427356
Epoch 2841/10000, Prediction Accuracy = 61.674%, Loss = 0.42375755310058594
Epoch: 2841, Batch Gradient Norm: 12.553164661882615
Epoch: 2841, Batch Gradient Norm after: 12.553164661882615
Epoch 2842/10000, Prediction Accuracy = 61.586%, Loss = 0.4233732998371124
Epoch: 2842, Batch Gradient Norm: 12.672330115401802
Epoch: 2842, Batch Gradient Norm after: 12.672330115401802
Epoch 2843/10000, Prediction Accuracy = 61.55%, Loss = 0.42305595278739927
Epoch: 2843, Batch Gradient Norm: 12.007064523400517
Epoch: 2843, Batch Gradient Norm after: 12.007064523400517
Epoch 2844/10000, Prediction Accuracy = 61.552%, Loss = 0.4235083222389221
Epoch: 2844, Batch Gradient Norm: 10.731447575815823
Epoch: 2844, Batch Gradient Norm after: 10.731447575815823
Epoch 2845/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.42349063158035277
Epoch: 2845, Batch Gradient Norm: 14.154868669614359
Epoch: 2845, Batch Gradient Norm after: 14.154868669614359
Epoch 2846/10000, Prediction Accuracy = 61.512%, Loss = 0.42854673862457277
Epoch: 2846, Batch Gradient Norm: 11.582766447466351
Epoch: 2846, Batch Gradient Norm after: 11.582766447466351
Epoch 2847/10000, Prediction Accuracy = 61.626%, Loss = 0.42456196546554564
Epoch: 2847, Batch Gradient Norm: 13.065996453878272
Epoch: 2847, Batch Gradient Norm after: 13.065996453878272
Epoch 2848/10000, Prediction Accuracy = 61.6%, Loss = 0.4258548140525818
Epoch: 2848, Batch Gradient Norm: 12.653518809692168
Epoch: 2848, Batch Gradient Norm after: 12.653518809692168
Epoch 2849/10000, Prediction Accuracy = 61.58399999999999%, Loss = 0.42604562640190125
Epoch: 2849, Batch Gradient Norm: 12.265592793492406
Epoch: 2849, Batch Gradient Norm after: 12.265592793492406
Epoch 2850/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.42338783740997316
Epoch: 2850, Batch Gradient Norm: 13.911762691990303
Epoch: 2850, Batch Gradient Norm after: 13.911762691990303
Epoch 2851/10000, Prediction Accuracy = 61.624%, Loss = 0.4249757766723633
Epoch: 2851, Batch Gradient Norm: 11.884730391597946
Epoch: 2851, Batch Gradient Norm after: 11.884730391597946
Epoch 2852/10000, Prediction Accuracy = 61.562%, Loss = 0.42191295623779296
Epoch: 2852, Batch Gradient Norm: 10.789419964037016
Epoch: 2852, Batch Gradient Norm after: 10.789419964037016
Epoch 2853/10000, Prediction Accuracy = 61.614%, Loss = 0.42044873237609864
Epoch: 2853, Batch Gradient Norm: 9.418765312557332
Epoch: 2853, Batch Gradient Norm after: 9.418765312557332
Epoch 2854/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4197522222995758
Epoch: 2854, Batch Gradient Norm: 11.710300395938837
Epoch: 2854, Batch Gradient Norm after: 11.710300395938837
Epoch 2855/10000, Prediction Accuracy = 61.584%, Loss = 0.4230303168296814
Epoch: 2855, Batch Gradient Norm: 10.473397938211287
Epoch: 2855, Batch Gradient Norm after: 10.473397938211287
Epoch 2856/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.4193718612194061
Epoch: 2856, Batch Gradient Norm: 13.60601621690372
Epoch: 2856, Batch Gradient Norm after: 13.60601621690372
Epoch 2857/10000, Prediction Accuracy = 61.586%, Loss = 0.4262793958187103
Epoch: 2857, Batch Gradient Norm: 15.632254544712847
Epoch: 2857, Batch Gradient Norm after: 15.632254544712847
Epoch 2858/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4264663815498352
Epoch: 2858, Batch Gradient Norm: 14.880350722072453
Epoch: 2858, Batch Gradient Norm after: 14.880350722072453
Epoch 2859/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.42566899061203
Epoch: 2859, Batch Gradient Norm: 16.280289352658432
Epoch: 2859, Batch Gradient Norm after: 16.280289352658432
Epoch 2860/10000, Prediction Accuracy = 61.592%, Loss = 0.42745959758758545
Epoch: 2860, Batch Gradient Norm: 12.856966524880791
Epoch: 2860, Batch Gradient Norm after: 12.856966524880791
Epoch 2861/10000, Prediction Accuracy = 61.516%, Loss = 0.42515428066253663
Epoch: 2861, Batch Gradient Norm: 12.541052772401125
Epoch: 2861, Batch Gradient Norm after: 12.541052772401125
Epoch 2862/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.42353443503379823
Epoch: 2862, Batch Gradient Norm: 11.188224874091961
Epoch: 2862, Batch Gradient Norm after: 11.188224874091961
Epoch 2863/10000, Prediction Accuracy = 61.67%, Loss = 0.422846782207489
Epoch: 2863, Batch Gradient Norm: 11.381159923079485
Epoch: 2863, Batch Gradient Norm after: 11.381159923079485
Epoch 2864/10000, Prediction Accuracy = 61.7%, Loss = 0.42387847900390624
Epoch: 2864, Batch Gradient Norm: 13.626047011529973
Epoch: 2864, Batch Gradient Norm after: 13.626047011529973
Epoch 2865/10000, Prediction Accuracy = 61.694%, Loss = 0.42284095883369444
Epoch: 2865, Batch Gradient Norm: 12.128969019181
Epoch: 2865, Batch Gradient Norm after: 12.128969019181
Epoch 2866/10000, Prediction Accuracy = 61.614%, Loss = 0.4227206587791443
Epoch: 2866, Batch Gradient Norm: 12.792252878697361
Epoch: 2866, Batch Gradient Norm after: 12.792252878697361
Epoch 2867/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4248643755912781
Epoch: 2867, Batch Gradient Norm: 12.555933196452699
Epoch: 2867, Batch Gradient Norm after: 12.555933196452699
Epoch 2868/10000, Prediction Accuracy = 61.73%, Loss = 0.4225819230079651
Epoch: 2868, Batch Gradient Norm: 11.795522757151005
Epoch: 2868, Batch Gradient Norm after: 11.795522757151005
Epoch 2869/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.420896577835083
Epoch: 2869, Batch Gradient Norm: 13.114043863602783
Epoch: 2869, Batch Gradient Norm after: 13.114043863602783
Epoch 2870/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.42553861141204835
Epoch: 2870, Batch Gradient Norm: 11.013004496216507
Epoch: 2870, Batch Gradient Norm after: 11.013004496216507
Epoch 2871/10000, Prediction Accuracy = 61.709999999999994%, Loss = 0.42423654198646543
Epoch: 2871, Batch Gradient Norm: 10.75913137701707
Epoch: 2871, Batch Gradient Norm after: 10.75913137701707
Epoch 2872/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.4202061653137207
Epoch: 2872, Batch Gradient Norm: 12.11654411439549
Epoch: 2872, Batch Gradient Norm after: 12.11654411439549
Epoch 2873/10000, Prediction Accuracy = 61.678%, Loss = 0.4253727912902832
Epoch: 2873, Batch Gradient Norm: 13.082568766887249
Epoch: 2873, Batch Gradient Norm after: 13.082568766887249
Epoch 2874/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.429279226064682
Epoch: 2874, Batch Gradient Norm: 11.014399697306985
Epoch: 2874, Batch Gradient Norm after: 11.014399697306985
Epoch 2875/10000, Prediction Accuracy = 61.548%, Loss = 0.4207690417766571
Epoch: 2875, Batch Gradient Norm: 8.94657063663709
Epoch: 2875, Batch Gradient Norm after: 8.94657063663709
Epoch 2876/10000, Prediction Accuracy = 61.624%, Loss = 0.417597895860672
Epoch: 2876, Batch Gradient Norm: 8.628184797513978
Epoch: 2876, Batch Gradient Norm after: 8.628184797513978
Epoch 2877/10000, Prediction Accuracy = 61.648%, Loss = 0.4167735457420349
Epoch: 2877, Batch Gradient Norm: 8.870021267242818
Epoch: 2877, Batch Gradient Norm after: 8.870021267242818
Epoch 2878/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.41784554719924927
Epoch: 2878, Batch Gradient Norm: 6.747478907954056
Epoch: 2878, Batch Gradient Norm after: 6.747478907954056
Epoch 2879/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.41435940861701964
Epoch: 2879, Batch Gradient Norm: 8.856237502576015
Epoch: 2879, Batch Gradient Norm after: 8.856237502576015
Epoch 2880/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4185988485813141
Epoch: 2880, Batch Gradient Norm: 7.792419345528842
Epoch: 2880, Batch Gradient Norm after: 7.792419345528842
Epoch 2881/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.4158434808254242
Epoch: 2881, Batch Gradient Norm: 9.613668448486925
Epoch: 2881, Batch Gradient Norm after: 9.613668448486925
Epoch 2882/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.41865183115005494
Epoch: 2882, Batch Gradient Norm: 12.064670777543787
Epoch: 2882, Batch Gradient Norm after: 12.064670777543787
Epoch 2883/10000, Prediction Accuracy = 61.614%, Loss = 0.42286189198493956
Epoch: 2883, Batch Gradient Norm: 13.024781239751231
Epoch: 2883, Batch Gradient Norm after: 13.024781239751231
Epoch 2884/10000, Prediction Accuracy = 61.676%, Loss = 0.42415274381637574
Epoch: 2884, Batch Gradient Norm: 12.079914298930637
Epoch: 2884, Batch Gradient Norm after: 12.079914298930637
Epoch 2885/10000, Prediction Accuracy = 61.588%, Loss = 0.42210707664489744
Epoch: 2885, Batch Gradient Norm: 12.342130194296441
Epoch: 2885, Batch Gradient Norm after: 12.342130194296441
Epoch 2886/10000, Prediction Accuracy = 61.572%, Loss = 0.4216329872608185
Epoch: 2886, Batch Gradient Norm: 13.06769839100547
Epoch: 2886, Batch Gradient Norm after: 13.06769839100547
Epoch 2887/10000, Prediction Accuracy = 61.602%, Loss = 0.42249549031257627
Epoch: 2887, Batch Gradient Norm: 12.353724205909312
Epoch: 2887, Batch Gradient Norm after: 12.353724205909312
Epoch 2888/10000, Prediction Accuracy = 61.63000000000001%, Loss = 0.4210940897464752
Epoch: 2888, Batch Gradient Norm: 13.463743482453154
Epoch: 2888, Batch Gradient Norm after: 13.463743482453154
Epoch 2889/10000, Prediction Accuracy = 61.584%, Loss = 0.42224411964416503
Epoch: 2889, Batch Gradient Norm: 13.165398506634059
Epoch: 2889, Batch Gradient Norm after: 13.165398506634059
Epoch 2890/10000, Prediction Accuracy = 61.58%, Loss = 0.4207270801067352
Epoch: 2890, Batch Gradient Norm: 10.135326451075471
Epoch: 2890, Batch Gradient Norm after: 10.135326451075471
Epoch 2891/10000, Prediction Accuracy = 61.682%, Loss = 0.41765509843826293
Epoch: 2891, Batch Gradient Norm: 14.568949535659057
Epoch: 2891, Batch Gradient Norm after: 14.568949535659057
Epoch 2892/10000, Prediction Accuracy = 61.626%, Loss = 0.42510027885437013
Epoch: 2892, Batch Gradient Norm: 16.511799932188435
Epoch: 2892, Batch Gradient Norm after: 16.511799932188435
Epoch 2893/10000, Prediction Accuracy = 61.652%, Loss = 0.426768434047699
Epoch: 2893, Batch Gradient Norm: 12.238393097744739
Epoch: 2893, Batch Gradient Norm after: 12.238393097744739
Epoch 2894/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.42126842141151427
Epoch: 2894, Batch Gradient Norm: 10.456474960307848
Epoch: 2894, Batch Gradient Norm after: 10.456474960307848
Epoch 2895/10000, Prediction Accuracy = 61.538%, Loss = 0.4188608467578888
Epoch: 2895, Batch Gradient Norm: 11.136869683816665
Epoch: 2895, Batch Gradient Norm after: 11.136869683816665
Epoch 2896/10000, Prediction Accuracy = 61.702%, Loss = 0.41937975883483886
Epoch: 2896, Batch Gradient Norm: 12.3184951778616
Epoch: 2896, Batch Gradient Norm after: 12.3184951778616
Epoch 2897/10000, Prediction Accuracy = 61.626%, Loss = 0.4225793361663818
Epoch: 2897, Batch Gradient Norm: 12.01298337029878
Epoch: 2897, Batch Gradient Norm after: 12.01298337029878
Epoch 2898/10000, Prediction Accuracy = 61.614%, Loss = 0.421557742357254
Epoch: 2898, Batch Gradient Norm: 12.8048456731969
Epoch: 2898, Batch Gradient Norm after: 12.8048456731969
Epoch 2899/10000, Prediction Accuracy = 61.592%, Loss = 0.42190577983856203
Epoch: 2899, Batch Gradient Norm: 16.332988318677756
Epoch: 2899, Batch Gradient Norm after: 16.332988318677756
Epoch 2900/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4278587281703949
Epoch: 2900, Batch Gradient Norm: 11.939806547367398
Epoch: 2900, Batch Gradient Norm after: 11.939806547367398
Epoch 2901/10000, Prediction Accuracy = 61.592%, Loss = 0.42169770002365115
Epoch: 2901, Batch Gradient Norm: 10.900776513940503
Epoch: 2901, Batch Gradient Norm after: 10.900776513940503
Epoch 2902/10000, Prediction Accuracy = 61.576%, Loss = 0.4193216383457184
Epoch: 2902, Batch Gradient Norm: 13.269678906852043
Epoch: 2902, Batch Gradient Norm after: 13.269678906852043
Epoch 2903/10000, Prediction Accuracy = 61.626%, Loss = 0.4228220522403717
Epoch: 2903, Batch Gradient Norm: 12.741845600600039
Epoch: 2903, Batch Gradient Norm after: 12.741845600600039
Epoch 2904/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4215210735797882
Epoch: 2904, Batch Gradient Norm: 14.099400930552106
Epoch: 2904, Batch Gradient Norm after: 14.099400930552106
Epoch 2905/10000, Prediction Accuracy = 61.688%, Loss = 0.42685632705688475
Epoch: 2905, Batch Gradient Norm: 12.804966937808414
Epoch: 2905, Batch Gradient Norm after: 12.804966937808414
Epoch 2906/10000, Prediction Accuracy = 61.574%, Loss = 0.4231043875217438
Epoch: 2906, Batch Gradient Norm: 12.6016602736647
Epoch: 2906, Batch Gradient Norm after: 12.6016602736647
Epoch 2907/10000, Prediction Accuracy = 61.628%, Loss = 0.4205170512199402
Epoch: 2907, Batch Gradient Norm: 9.656537535895822
Epoch: 2907, Batch Gradient Norm after: 9.656537535895822
Epoch 2908/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.4184410810470581
Epoch: 2908, Batch Gradient Norm: 11.42424084478058
Epoch: 2908, Batch Gradient Norm after: 11.42424084478058
Epoch 2909/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4185773253440857
Epoch: 2909, Batch Gradient Norm: 10.137249502749118
Epoch: 2909, Batch Gradient Norm after: 10.137249502749118
Epoch 2910/10000, Prediction Accuracy = 61.65%, Loss = 0.41793338656425477
Epoch: 2910, Batch Gradient Norm: 13.315659388742393
Epoch: 2910, Batch Gradient Norm after: 13.315659388742393
Epoch 2911/10000, Prediction Accuracy = 61.64%, Loss = 0.4214173197746277
Epoch: 2911, Batch Gradient Norm: 10.351208601445851
Epoch: 2911, Batch Gradient Norm after: 10.351208601445851
Epoch 2912/10000, Prediction Accuracy = 61.574%, Loss = 0.4191420316696167
Epoch: 2912, Batch Gradient Norm: 10.510948803816577
Epoch: 2912, Batch Gradient Norm after: 10.510948803816577
Epoch 2913/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.4179818868637085
Epoch: 2913, Batch Gradient Norm: 8.77238045248532
Epoch: 2913, Batch Gradient Norm after: 8.77238045248532
Epoch 2914/10000, Prediction Accuracy = 61.67%, Loss = 0.4162119448184967
Epoch: 2914, Batch Gradient Norm: 10.91600989754077
Epoch: 2914, Batch Gradient Norm after: 10.91600989754077
Epoch 2915/10000, Prediction Accuracy = 61.7%, Loss = 0.41717216968536375
Epoch: 2915, Batch Gradient Norm: 12.49806352572518
Epoch: 2915, Batch Gradient Norm after: 12.49806352572518
Epoch 2916/10000, Prediction Accuracy = 61.652%, Loss = 0.42037380337715147
Epoch: 2916, Batch Gradient Norm: 11.965790054373585
Epoch: 2916, Batch Gradient Norm after: 11.965790054373585
Epoch 2917/10000, Prediction Accuracy = 61.67%, Loss = 0.4203289270401001
Epoch: 2917, Batch Gradient Norm: 10.312212928354288
Epoch: 2917, Batch Gradient Norm after: 10.312212928354288
Epoch 2918/10000, Prediction Accuracy = 61.664%, Loss = 0.41803374886512756
Epoch: 2918, Batch Gradient Norm: 9.613268565836087
Epoch: 2918, Batch Gradient Norm after: 9.613268565836087
Epoch 2919/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.4183955192565918
Epoch: 2919, Batch Gradient Norm: 9.997731709862574
Epoch: 2919, Batch Gradient Norm after: 9.997731709862574
Epoch 2920/10000, Prediction Accuracy = 61.782%, Loss = 0.4163392722606659
Epoch: 2920, Batch Gradient Norm: 12.62807400495937
Epoch: 2920, Batch Gradient Norm after: 12.62807400495937
Epoch 2921/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.4210945129394531
Epoch: 2921, Batch Gradient Norm: 14.685840058375343
Epoch: 2921, Batch Gradient Norm after: 14.685840058375343
Epoch 2922/10000, Prediction Accuracy = 61.64%, Loss = 0.4219327509403229
Epoch: 2922, Batch Gradient Norm: 11.24063775724282
Epoch: 2922, Batch Gradient Norm after: 11.24063775724282
Epoch 2923/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.42161825895309446
Epoch: 2923, Batch Gradient Norm: 12.388935167731136
Epoch: 2923, Batch Gradient Norm after: 12.388935167731136
Epoch 2924/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4197396576404572
Epoch: 2924, Batch Gradient Norm: 11.224168694031968
Epoch: 2924, Batch Gradient Norm after: 11.224168694031968
Epoch 2925/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.4192641317844391
Epoch: 2925, Batch Gradient Norm: 10.527572649272228
Epoch: 2925, Batch Gradient Norm after: 10.527572649272228
Epoch 2926/10000, Prediction Accuracy = 61.56%, Loss = 0.4181075692176819
Epoch: 2926, Batch Gradient Norm: 12.975915226895903
Epoch: 2926, Batch Gradient Norm after: 12.975915226895903
Epoch 2927/10000, Prediction Accuracy = 61.664%, Loss = 0.42472142577171323
Epoch: 2927, Batch Gradient Norm: 9.970174925559549
Epoch: 2927, Batch Gradient Norm after: 9.970174925559549
Epoch 2928/10000, Prediction Accuracy = 61.598%, Loss = 0.41862237453460693
Epoch: 2928, Batch Gradient Norm: 9.447666271018008
Epoch: 2928, Batch Gradient Norm after: 9.447666271018008
Epoch 2929/10000, Prediction Accuracy = 61.614%, Loss = 0.4162489652633667
Epoch: 2929, Batch Gradient Norm: 10.386759411225414
Epoch: 2929, Batch Gradient Norm after: 10.386759411225414
Epoch 2930/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.4202447414398193
Epoch: 2930, Batch Gradient Norm: 11.352046729234264
Epoch: 2930, Batch Gradient Norm after: 11.352046729234264
Epoch 2931/10000, Prediction Accuracy = 61.592%, Loss = 0.4178131103515625
Epoch: 2931, Batch Gradient Norm: 13.023712889007431
Epoch: 2931, Batch Gradient Norm after: 13.023712889007431
Epoch 2932/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.42260549664497377
Epoch: 2932, Batch Gradient Norm: 14.327316400212974
Epoch: 2932, Batch Gradient Norm after: 14.327316400212974
Epoch 2933/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.4219523072242737
Epoch: 2933, Batch Gradient Norm: 12.696303545835029
Epoch: 2933, Batch Gradient Norm after: 12.696303545835029
Epoch 2934/10000, Prediction Accuracy = 61.67199999999999%, Loss = 0.4194077789783478
Epoch: 2934, Batch Gradient Norm: 13.965062755302249
Epoch: 2934, Batch Gradient Norm after: 13.965062755302249
Epoch 2935/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.42245633006095884
Epoch: 2935, Batch Gradient Norm: 14.459063385913023
Epoch: 2935, Batch Gradient Norm after: 14.459063385913023
Epoch 2936/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.42317587733268736
Epoch: 2936, Batch Gradient Norm: 14.429225574010736
Epoch: 2936, Batch Gradient Norm after: 14.429225574010736
Epoch 2937/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.42427425980567934
Epoch: 2937, Batch Gradient Norm: 14.541823902831002
Epoch: 2937, Batch Gradient Norm after: 14.541823902831002
Epoch 2938/10000, Prediction Accuracy = 61.758%, Loss = 0.4238617539405823
Epoch: 2938, Batch Gradient Norm: 12.865205366616166
Epoch: 2938, Batch Gradient Norm after: 12.865205366616166
Epoch 2939/10000, Prediction Accuracy = 61.67%, Loss = 0.42024142742156984
Epoch: 2939, Batch Gradient Norm: 13.138654436387736
Epoch: 2939, Batch Gradient Norm after: 13.138654436387736
Epoch 2940/10000, Prediction Accuracy = 61.73%, Loss = 0.42048934698104856
Epoch: 2940, Batch Gradient Norm: 13.996775592478789
Epoch: 2940, Batch Gradient Norm after: 13.996775592478789
Epoch 2941/10000, Prediction Accuracy = 61.7%, Loss = 0.4230367839336395
Epoch: 2941, Batch Gradient Norm: 14.839461233829779
Epoch: 2941, Batch Gradient Norm after: 14.839461233829779
Epoch 2942/10000, Prediction Accuracy = 61.572%, Loss = 0.42780903577804563
Epoch: 2942, Batch Gradient Norm: 13.26140449131186
Epoch: 2942, Batch Gradient Norm after: 13.26140449131186
Epoch 2943/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.421927410364151
Epoch: 2943, Batch Gradient Norm: 12.996167685180131
Epoch: 2943, Batch Gradient Norm after: 12.996167685180131
Epoch 2944/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.42263008952140807
Epoch: 2944, Batch Gradient Norm: 11.89578925293896
Epoch: 2944, Batch Gradient Norm after: 11.89578925293896
Epoch 2945/10000, Prediction Accuracy = 61.686%, Loss = 0.41711058616638186
Epoch: 2945, Batch Gradient Norm: 12.274583373429374
Epoch: 2945, Batch Gradient Norm after: 12.274583373429374
Epoch 2946/10000, Prediction Accuracy = 61.64%, Loss = 0.4193996012210846
Epoch: 2946, Batch Gradient Norm: 12.27672742532174
Epoch: 2946, Batch Gradient Norm after: 12.27672742532174
Epoch 2947/10000, Prediction Accuracy = 61.73%, Loss = 0.41986018419265747
Epoch: 2947, Batch Gradient Norm: 13.203767687888643
Epoch: 2947, Batch Gradient Norm after: 13.203767687888643
Epoch 2948/10000, Prediction Accuracy = 61.714%, Loss = 0.4194765329360962
Epoch: 2948, Batch Gradient Norm: 12.16557434876344
Epoch: 2948, Batch Gradient Norm after: 12.16557434876344
Epoch 2949/10000, Prediction Accuracy = 61.67%, Loss = 0.41921823024749755
Epoch: 2949, Batch Gradient Norm: 12.820718826674213
Epoch: 2949, Batch Gradient Norm after: 12.820718826674213
Epoch 2950/10000, Prediction Accuracy = 61.67%, Loss = 0.4192818462848663
Epoch: 2950, Batch Gradient Norm: 10.278773531101313
Epoch: 2950, Batch Gradient Norm after: 10.278773531101313
Epoch 2951/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.41712345480918883
Epoch: 2951, Batch Gradient Norm: 9.553023525205168
Epoch: 2951, Batch Gradient Norm after: 9.553023525205168
Epoch 2952/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.41526004672050476
Epoch: 2952, Batch Gradient Norm: 9.46391580260199
Epoch: 2952, Batch Gradient Norm after: 9.46391580260199
Epoch 2953/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4153690993785858
Epoch: 2953, Batch Gradient Norm: 12.431449495897635
Epoch: 2953, Batch Gradient Norm after: 12.431449495897635
Epoch 2954/10000, Prediction Accuracy = 61.73%, Loss = 0.41851955056190493
Epoch: 2954, Batch Gradient Norm: 11.485302131655653
Epoch: 2954, Batch Gradient Norm after: 11.485302131655653
Epoch 2955/10000, Prediction Accuracy = 61.722%, Loss = 0.4175655961036682
Epoch: 2955, Batch Gradient Norm: 12.27479679089449
Epoch: 2955, Batch Gradient Norm after: 12.27479679089449
Epoch 2956/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.41828447580337524
Epoch: 2956, Batch Gradient Norm: 10.640083389146
Epoch: 2956, Batch Gradient Norm after: 10.640083389146
Epoch 2957/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.41649611592292785
Epoch: 2957, Batch Gradient Norm: 11.929877631032069
Epoch: 2957, Batch Gradient Norm after: 11.929877631032069
Epoch 2958/10000, Prediction Accuracy = 61.812%, Loss = 0.41688554883003237
Epoch: 2958, Batch Gradient Norm: 10.560145896680382
Epoch: 2958, Batch Gradient Norm after: 10.560145896680382
Epoch 2959/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.41684845089912415
Epoch: 2959, Batch Gradient Norm: 9.669239830187296
Epoch: 2959, Batch Gradient Norm after: 9.669239830187296
Epoch 2960/10000, Prediction Accuracy = 61.75%, Loss = 0.4156924784183502
Epoch: 2960, Batch Gradient Norm: 11.861670048929103
Epoch: 2960, Batch Gradient Norm after: 11.861670048929103
Epoch 2961/10000, Prediction Accuracy = 61.730000000000004%, Loss = 0.418902325630188
Epoch: 2961, Batch Gradient Norm: 12.150186058434592
Epoch: 2961, Batch Gradient Norm after: 12.150186058434592
Epoch 2962/10000, Prediction Accuracy = 61.83%, Loss = 0.41696539521217346
Epoch: 2962, Batch Gradient Norm: 10.128726583438189
Epoch: 2962, Batch Gradient Norm after: 10.128726583438189
Epoch 2963/10000, Prediction Accuracy = 61.7%, Loss = 0.4140931487083435
Epoch: 2963, Batch Gradient Norm: 11.409295594222334
Epoch: 2963, Batch Gradient Norm after: 11.409295594222334
Epoch 2964/10000, Prediction Accuracy = 61.67199999999999%, Loss = 0.4186641573905945
Epoch: 2964, Batch Gradient Norm: 13.892915910802268
Epoch: 2964, Batch Gradient Norm after: 13.892915910802268
Epoch 2965/10000, Prediction Accuracy = 61.674%, Loss = 0.42625707387924194
Epoch: 2965, Batch Gradient Norm: 12.676179285095833
Epoch: 2965, Batch Gradient Norm after: 12.676179285095833
Epoch 2966/10000, Prediction Accuracy = 61.694%, Loss = 0.4181497752666473
Epoch: 2966, Batch Gradient Norm: 10.939025488086667
Epoch: 2966, Batch Gradient Norm after: 10.939025488086667
Epoch 2967/10000, Prediction Accuracy = 61.7%, Loss = 0.4171945631504059
Epoch: 2967, Batch Gradient Norm: 12.06986367389656
Epoch: 2967, Batch Gradient Norm after: 12.06986367389656
Epoch 2968/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.41885130405426024
Epoch: 2968, Batch Gradient Norm: 12.78027877441765
Epoch: 2968, Batch Gradient Norm after: 12.78027877441765
Epoch 2969/10000, Prediction Accuracy = 61.74799999999999%, Loss = 0.41944544315338134
Epoch: 2969, Batch Gradient Norm: 11.60165920380069
Epoch: 2969, Batch Gradient Norm after: 11.60165920380069
Epoch 2970/10000, Prediction Accuracy = 61.67%, Loss = 0.41686745285987853
Epoch: 2970, Batch Gradient Norm: 12.531305738468738
Epoch: 2970, Batch Gradient Norm after: 12.531305738468738
Epoch 2971/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4179681777954102
Epoch: 2971, Batch Gradient Norm: 10.733642718687676
Epoch: 2971, Batch Gradient Norm after: 10.733642718687676
Epoch 2972/10000, Prediction Accuracy = 61.761999999999986%, Loss = 0.41837523579597474
Epoch: 2972, Batch Gradient Norm: 9.138741411148853
Epoch: 2972, Batch Gradient Norm after: 9.138741411148853
Epoch 2973/10000, Prediction Accuracy = 61.78399999999999%, Loss = 0.41258005499839784
Epoch: 2973, Batch Gradient Norm: 11.915077090549973
Epoch: 2973, Batch Gradient Norm after: 11.915077090549973
Epoch 2974/10000, Prediction Accuracy = 61.666%, Loss = 0.4184498906135559
Epoch: 2974, Batch Gradient Norm: 11.498652991722343
Epoch: 2974, Batch Gradient Norm after: 11.498652991722343
Epoch 2975/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4183335304260254
Epoch: 2975, Batch Gradient Norm: 9.59743834744759
Epoch: 2975, Batch Gradient Norm after: 9.59743834744759
Epoch 2976/10000, Prediction Accuracy = 61.79599999999999%, Loss = 0.4144521415233612
Epoch: 2976, Batch Gradient Norm: 10.579575756271849
Epoch: 2976, Batch Gradient Norm after: 10.579575756271849
Epoch 2977/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.41755807399749756
Epoch: 2977, Batch Gradient Norm: 12.29387545856702
Epoch: 2977, Batch Gradient Norm after: 12.29387545856702
Epoch 2978/10000, Prediction Accuracy = 61.774%, Loss = 0.41847166419029236
Epoch: 2978, Batch Gradient Norm: 13.168855950453494
Epoch: 2978, Batch Gradient Norm after: 13.168855950453494
Epoch 2979/10000, Prediction Accuracy = 61.878%, Loss = 0.4191608726978302
Epoch: 2979, Batch Gradient Norm: 15.389923192856916
Epoch: 2979, Batch Gradient Norm after: 15.389923192856916
Epoch 2980/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.42693854570388795
Epoch: 2980, Batch Gradient Norm: 16.249124522928938
Epoch: 2980, Batch Gradient Norm after: 16.10900783543979
Epoch 2981/10000, Prediction Accuracy = 61.72800000000001%, Loss = 0.42579773664474485
Epoch: 2981, Batch Gradient Norm: 13.573870414045478
Epoch: 2981, Batch Gradient Norm after: 13.573870414045478
Epoch 2982/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4194816052913666
Epoch: 2982, Batch Gradient Norm: 15.04850436059872
Epoch: 2982, Batch Gradient Norm after: 15.04850436059872
Epoch 2983/10000, Prediction Accuracy = 61.746%, Loss = 0.4251014828681946
Epoch: 2983, Batch Gradient Norm: 14.27407067925841
Epoch: 2983, Batch Gradient Norm after: 14.27407067925841
Epoch 2984/10000, Prediction Accuracy = 61.749999999999986%, Loss = 0.423232889175415
Epoch: 2984, Batch Gradient Norm: 14.197969653848856
Epoch: 2984, Batch Gradient Norm after: 14.197969653848856
Epoch 2985/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.42237432599067687
Epoch: 2985, Batch Gradient Norm: 13.545447011007711
Epoch: 2985, Batch Gradient Norm after: 13.545447011007711
Epoch 2986/10000, Prediction Accuracy = 61.754%, Loss = 0.42043986916542053
Epoch: 2986, Batch Gradient Norm: 13.080994024098354
Epoch: 2986, Batch Gradient Norm after: 13.080994024098354
Epoch 2987/10000, Prediction Accuracy = 61.698%, Loss = 0.4182162702083588
Epoch: 2987, Batch Gradient Norm: 11.571994531231478
Epoch: 2987, Batch Gradient Norm after: 11.571994531231478
Epoch 2988/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4170210838317871
Epoch: 2988, Batch Gradient Norm: 14.978368155845057
Epoch: 2988, Batch Gradient Norm after: 14.978368155845057
Epoch 2989/10000, Prediction Accuracy = 61.73%, Loss = 0.42129374742507936
Epoch: 2989, Batch Gradient Norm: 13.544238520049733
Epoch: 2989, Batch Gradient Norm after: 13.544238520049733
Epoch 2990/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.41904841661453246
Epoch: 2990, Batch Gradient Norm: 11.336652533316897
Epoch: 2990, Batch Gradient Norm after: 11.336652533316897
Epoch 2991/10000, Prediction Accuracy = 61.722%, Loss = 0.4142844557762146
Epoch: 2991, Batch Gradient Norm: 14.471953507120599
Epoch: 2991, Batch Gradient Norm after: 14.471953507120599
Epoch 2992/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.42098885774612427
Epoch: 2992, Batch Gradient Norm: 14.859915924762662
Epoch: 2992, Batch Gradient Norm after: 14.859915924762662
Epoch 2993/10000, Prediction Accuracy = 61.653999999999996%, Loss = 0.4260014533996582
Epoch: 2993, Batch Gradient Norm: 13.73037515070937
Epoch: 2993, Batch Gradient Norm after: 13.73037515070937
Epoch 2994/10000, Prediction Accuracy = 61.648%, Loss = 0.41998606324195864
Epoch: 2994, Batch Gradient Norm: 11.876956681728203
Epoch: 2994, Batch Gradient Norm after: 11.876956681728203
Epoch 2995/10000, Prediction Accuracy = 61.698%, Loss = 0.417851322889328
Epoch: 2995, Batch Gradient Norm: 13.300640508357093
Epoch: 2995, Batch Gradient Norm after: 13.300640508357093
Epoch 2996/10000, Prediction Accuracy = 61.75%, Loss = 0.4197741448879242
Epoch: 2996, Batch Gradient Norm: 12.287473595175268
Epoch: 2996, Batch Gradient Norm after: 12.287473595175268
Epoch 2997/10000, Prediction Accuracy = 61.714%, Loss = 0.4203063130378723
Epoch: 2997, Batch Gradient Norm: 12.19086041187563
Epoch: 2997, Batch Gradient Norm after: 12.19086041187563
Epoch 2998/10000, Prediction Accuracy = 61.722%, Loss = 0.4174176216125488
Epoch: 2998, Batch Gradient Norm: 14.295937479307822
Epoch: 2998, Batch Gradient Norm after: 14.295937479307822
Epoch 2999/10000, Prediction Accuracy = 61.826%, Loss = 0.42273709177970886
Epoch: 2999, Batch Gradient Norm: 13.278706818899199
Epoch: 2999, Batch Gradient Norm after: 13.278706818899199
Epoch 3000/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.41927369236946105
Epoch: 3000, Batch Gradient Norm: 13.364690123277416
Epoch: 3000, Batch Gradient Norm after: 13.364690123277416
Epoch 3001/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4209102690219879
Epoch: 3001, Batch Gradient Norm: 10.47407831743646
Epoch: 3001, Batch Gradient Norm after: 10.47407831743646
Epoch 3002/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4146375238895416
Epoch: 3002, Batch Gradient Norm: 12.496575891722456
Epoch: 3002, Batch Gradient Norm after: 12.496575891722456
Epoch 3003/10000, Prediction Accuracy = 61.75600000000001%, Loss = 0.41957967877388
Epoch: 3003, Batch Gradient Norm: 11.9887321194953
Epoch: 3003, Batch Gradient Norm after: 11.9887321194953
Epoch 3004/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.41669432520866395
Epoch: 3004, Batch Gradient Norm: 12.79778852469643
Epoch: 3004, Batch Gradient Norm after: 12.79778852469643
Epoch 3005/10000, Prediction Accuracy = 61.628%, Loss = 0.41862592101097107
Epoch: 3005, Batch Gradient Norm: 10.539246412250819
Epoch: 3005, Batch Gradient Norm after: 10.539246412250819
Epoch 3006/10000, Prediction Accuracy = 61.802%, Loss = 0.41268156170845033
Epoch: 3006, Batch Gradient Norm: 11.88813850606166
Epoch: 3006, Batch Gradient Norm after: 11.88813850606166
Epoch 3007/10000, Prediction Accuracy = 61.806%, Loss = 0.4162406146526337
Epoch: 3007, Batch Gradient Norm: 10.343184177914136
Epoch: 3007, Batch Gradient Norm after: 10.343184177914136
Epoch 3008/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4144680261611938
Epoch: 3008, Batch Gradient Norm: 10.59374193631667
Epoch: 3008, Batch Gradient Norm after: 10.59374193631667
Epoch 3009/10000, Prediction Accuracy = 61.742%, Loss = 0.41434067487716675
Epoch: 3009, Batch Gradient Norm: 12.420702157953468
Epoch: 3009, Batch Gradient Norm after: 12.420702157953468
Epoch 3010/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.41679124236106874
Epoch: 3010, Batch Gradient Norm: 14.070423138983703
Epoch: 3010, Batch Gradient Norm after: 14.070423138983703
Epoch 3011/10000, Prediction Accuracy = 61.698%, Loss = 0.4183117985725403
Epoch: 3011, Batch Gradient Norm: 10.828548865158789
Epoch: 3011, Batch Gradient Norm after: 10.828548865158789
Epoch 3012/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.41537935733795167
Epoch: 3012, Batch Gradient Norm: 12.593044713454972
Epoch: 3012, Batch Gradient Norm after: 12.593044713454972
Epoch 3013/10000, Prediction Accuracy = 61.774%, Loss = 0.4178495228290558
Epoch: 3013, Batch Gradient Norm: 11.4534141111203
Epoch: 3013, Batch Gradient Norm after: 11.4534141111203
Epoch 3014/10000, Prediction Accuracy = 61.814%, Loss = 0.416778039932251
Epoch: 3014, Batch Gradient Norm: 11.055444012796386
Epoch: 3014, Batch Gradient Norm after: 11.055444012796386
Epoch 3015/10000, Prediction Accuracy = 61.746%, Loss = 0.4149084746837616
Epoch: 3015, Batch Gradient Norm: 9.73913187699889
Epoch: 3015, Batch Gradient Norm after: 9.73913187699889
Epoch 3016/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4125230073928833
Epoch: 3016, Batch Gradient Norm: 11.943094882350822
Epoch: 3016, Batch Gradient Norm after: 11.943094882350822
Epoch 3017/10000, Prediction Accuracy = 61.798%, Loss = 0.4169050335884094
Epoch: 3017, Batch Gradient Norm: 15.307058139495767
Epoch: 3017, Batch Gradient Norm after: 15.307058139495767
Epoch 3018/10000, Prediction Accuracy = 61.812%, Loss = 0.4203058540821075
Epoch: 3018, Batch Gradient Norm: 15.047896097305227
Epoch: 3018, Batch Gradient Norm after: 15.047896097305227
Epoch 3019/10000, Prediction Accuracy = 61.794%, Loss = 0.418482232093811
Epoch: 3019, Batch Gradient Norm: 14.67099890586467
Epoch: 3019, Batch Gradient Norm after: 14.67099890586467
Epoch 3020/10000, Prediction Accuracy = 61.8%, Loss = 0.4187254667282104
Epoch: 3020, Batch Gradient Norm: 11.287506816483033
Epoch: 3020, Batch Gradient Norm after: 11.287506816483033
Epoch 3021/10000, Prediction Accuracy = 61.698%, Loss = 0.415632027387619
Epoch: 3021, Batch Gradient Norm: 14.700134088507067
Epoch: 3021, Batch Gradient Norm after: 14.700134088507067
Epoch 3022/10000, Prediction Accuracy = 61.745999999999995%, Loss = 0.41978649497032167
Epoch: 3022, Batch Gradient Norm: 15.588032109409333
Epoch: 3022, Batch Gradient Norm after: 15.588032109409333
Epoch 3023/10000, Prediction Accuracy = 61.79%, Loss = 0.4245623528957367
Epoch: 3023, Batch Gradient Norm: 13.658408729425076
Epoch: 3023, Batch Gradient Norm after: 13.658408729425076
Epoch 3024/10000, Prediction Accuracy = 61.786%, Loss = 0.4175525426864624
Epoch: 3024, Batch Gradient Norm: 11.472808867886913
Epoch: 3024, Batch Gradient Norm after: 11.472808867886913
Epoch 3025/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.416158390045166
Epoch: 3025, Batch Gradient Norm: 13.683706583575383
Epoch: 3025, Batch Gradient Norm after: 13.683706583575383
Epoch 3026/10000, Prediction Accuracy = 61.79%, Loss = 0.4183061718940735
Epoch: 3026, Batch Gradient Norm: 11.261882757317414
Epoch: 3026, Batch Gradient Norm after: 11.261882757317414
Epoch 3027/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.41625062227249143
Epoch: 3027, Batch Gradient Norm: 10.742333049509961
Epoch: 3027, Batch Gradient Norm after: 10.742333049509961
Epoch 3028/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.41759185791015624
Epoch: 3028, Batch Gradient Norm: 12.028078980195106
Epoch: 3028, Batch Gradient Norm after: 12.028078980195106
Epoch 3029/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4157578647136688
Epoch: 3029, Batch Gradient Norm: 12.931463551003532
Epoch: 3029, Batch Gradient Norm after: 12.931463551003532
Epoch 3030/10000, Prediction Accuracy = 61.746%, Loss = 0.42050867080688475
Epoch: 3030, Batch Gradient Norm: 11.607817102421778
Epoch: 3030, Batch Gradient Norm after: 11.607817102421778
Epoch 3031/10000, Prediction Accuracy = 61.778%, Loss = 0.4162450790405273
Epoch: 3031, Batch Gradient Norm: 11.472104368472587
Epoch: 3031, Batch Gradient Norm after: 11.472104368472587
Epoch 3032/10000, Prediction Accuracy = 61.69%, Loss = 0.4161918103694916
Epoch: 3032, Batch Gradient Norm: 10.342373654231823
Epoch: 3032, Batch Gradient Norm after: 10.342373654231823
Epoch 3033/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.41322516798973086
Epoch: 3033, Batch Gradient Norm: 9.735957412471969
Epoch: 3033, Batch Gradient Norm after: 9.735957412471969
Epoch 3034/10000, Prediction Accuracy = 61.822%, Loss = 0.4132236182689667
Epoch: 3034, Batch Gradient Norm: 12.015975871603692
Epoch: 3034, Batch Gradient Norm after: 12.015975871603692
Epoch 3035/10000, Prediction Accuracy = 61.709999999999994%, Loss = 0.41948193311691284
Epoch: 3035, Batch Gradient Norm: 12.279280028196652
Epoch: 3035, Batch Gradient Norm after: 12.279280028196652
Epoch 3036/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4178642511367798
Epoch: 3036, Batch Gradient Norm: 13.659861252702568
Epoch: 3036, Batch Gradient Norm after: 13.659861252702568
Epoch 3037/10000, Prediction Accuracy = 61.774%, Loss = 0.4181596219539642
Epoch: 3037, Batch Gradient Norm: 10.661151629575423
Epoch: 3037, Batch Gradient Norm after: 10.661151629575423
Epoch 3038/10000, Prediction Accuracy = 61.843999999999994%, Loss = 0.41245612502098083
Epoch: 3038, Batch Gradient Norm: 11.169218231951524
Epoch: 3038, Batch Gradient Norm after: 11.169218231951524
Epoch 3039/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.41447784900665285
Epoch: 3039, Batch Gradient Norm: 14.583809175746685
Epoch: 3039, Batch Gradient Norm after: 14.583809175746685
Epoch 3040/10000, Prediction Accuracy = 61.778%, Loss = 0.419519966840744
Epoch: 3040, Batch Gradient Norm: 11.356482209943707
Epoch: 3040, Batch Gradient Norm after: 11.356482209943707
Epoch 3041/10000, Prediction Accuracy = 61.788%, Loss = 0.41418705582618714
Epoch: 3041, Batch Gradient Norm: 10.719854615443591
Epoch: 3041, Batch Gradient Norm after: 10.719854615443591
Epoch 3042/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.41444326639175416
Epoch: 3042, Batch Gradient Norm: 10.301491341423835
Epoch: 3042, Batch Gradient Norm after: 10.301491341423835
Epoch 3043/10000, Prediction Accuracy = 61.766%, Loss = 0.41545520424842836
Epoch: 3043, Batch Gradient Norm: 8.539102631720397
Epoch: 3043, Batch Gradient Norm after: 8.539102631720397
Epoch 3044/10000, Prediction Accuracy = 61.798%, Loss = 0.4113777995109558
Epoch: 3044, Batch Gradient Norm: 9.227694455905867
Epoch: 3044, Batch Gradient Norm after: 9.227694455905867
Epoch 3045/10000, Prediction Accuracy = 61.818%, Loss = 0.41071838736534116
Epoch: 3045, Batch Gradient Norm: 10.72867627433384
Epoch: 3045, Batch Gradient Norm after: 10.72867627433384
Epoch 3046/10000, Prediction Accuracy = 61.734%, Loss = 0.414627742767334
Epoch: 3046, Batch Gradient Norm: 11.211765873321548
Epoch: 3046, Batch Gradient Norm after: 11.211765873321548
Epoch 3047/10000, Prediction Accuracy = 61.86199999999999%, Loss = 0.4149736166000366
Epoch: 3047, Batch Gradient Norm: 11.871630971690864
Epoch: 3047, Batch Gradient Norm after: 11.871630971690864
Epoch 3048/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4152175009250641
Epoch: 3048, Batch Gradient Norm: 9.59544210704488
Epoch: 3048, Batch Gradient Norm after: 9.59544210704488
Epoch 3049/10000, Prediction Accuracy = 61.682%, Loss = 0.4126332879066467
Epoch: 3049, Batch Gradient Norm: 9.862203844108473
Epoch: 3049, Batch Gradient Norm after: 9.862203844108473
Epoch 3050/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4122460842132568
Epoch: 3050, Batch Gradient Norm: 10.615262888554662
Epoch: 3050, Batch Gradient Norm after: 10.615262888554662
Epoch 3051/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.4140324115753174
Epoch: 3051, Batch Gradient Norm: 12.662484683528456
Epoch: 3051, Batch Gradient Norm after: 12.662484683528456
Epoch 3052/10000, Prediction Accuracy = 61.698%, Loss = 0.41744003295898435
Epoch: 3052, Batch Gradient Norm: 12.591851629325733
Epoch: 3052, Batch Gradient Norm after: 12.591851629325733
Epoch 3053/10000, Prediction Accuracy = 61.878%, Loss = 0.41541674733161926
Epoch: 3053, Batch Gradient Norm: 12.641765007908466
Epoch: 3053, Batch Gradient Norm after: 12.641765007908466
Epoch 3054/10000, Prediction Accuracy = 61.786%, Loss = 0.4166830837726593
Epoch: 3054, Batch Gradient Norm: 12.509808833260692
Epoch: 3054, Batch Gradient Norm after: 12.509808833260692
Epoch 3055/10000, Prediction Accuracy = 61.712%, Loss = 0.4169519543647766
Epoch: 3055, Batch Gradient Norm: 13.043821062923874
Epoch: 3055, Batch Gradient Norm after: 13.043821062923874
Epoch 3056/10000, Prediction Accuracy = 61.842%, Loss = 0.41526720523834226
Epoch: 3056, Batch Gradient Norm: 10.950730026064688
Epoch: 3056, Batch Gradient Norm after: 10.950730026064688
Epoch 3057/10000, Prediction Accuracy = 61.666%, Loss = 0.4140059590339661
Epoch: 3057, Batch Gradient Norm: 11.927016774190971
Epoch: 3057, Batch Gradient Norm after: 11.927016774190971
Epoch 3058/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.41470261216163634
Epoch: 3058, Batch Gradient Norm: 12.445479220541833
Epoch: 3058, Batch Gradient Norm after: 12.445479220541833
Epoch 3059/10000, Prediction Accuracy = 61.758%, Loss = 0.41676905155181887
Epoch: 3059, Batch Gradient Norm: 9.671112091138843
Epoch: 3059, Batch Gradient Norm after: 9.671112091138843
Epoch 3060/10000, Prediction Accuracy = 61.788%, Loss = 0.4098212420940399
Epoch: 3060, Batch Gradient Norm: 10.97928345147536
Epoch: 3060, Batch Gradient Norm after: 10.97928345147536
Epoch 3061/10000, Prediction Accuracy = 61.848%, Loss = 0.4120837986469269
Epoch: 3061, Batch Gradient Norm: 10.165909463453701
Epoch: 3061, Batch Gradient Norm after: 10.165909463453701
Epoch 3062/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4122177243232727
Epoch: 3062, Batch Gradient Norm: 15.057633914254398
Epoch: 3062, Batch Gradient Norm after: 15.057633914254398
Epoch 3063/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.41892998218536376
Epoch: 3063, Batch Gradient Norm: 12.779938698611305
Epoch: 3063, Batch Gradient Norm after: 12.779938698611305
Epoch 3064/10000, Prediction Accuracy = 61.7%, Loss = 0.4160210072994232
Epoch: 3064, Batch Gradient Norm: 11.533776895563568
Epoch: 3064, Batch Gradient Norm after: 11.533776895563568
Epoch 3065/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.41590332984924316
Epoch: 3065, Batch Gradient Norm: 8.965335881713278
Epoch: 3065, Batch Gradient Norm after: 8.965335881713278
Epoch 3066/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.41022119522094724
Epoch: 3066, Batch Gradient Norm: 12.026491599781492
Epoch: 3066, Batch Gradient Norm after: 12.026491599781492
Epoch 3067/10000, Prediction Accuracy = 61.866%, Loss = 0.41481807827949524
Epoch: 3067, Batch Gradient Norm: 13.234040545771402
Epoch: 3067, Batch Gradient Norm after: 13.234040545771402
Epoch 3068/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.41638765335083006
Epoch: 3068, Batch Gradient Norm: 10.39666954559464
Epoch: 3068, Batch Gradient Norm after: 10.39666954559464
Epoch 3069/10000, Prediction Accuracy = 61.806%, Loss = 0.41244940757751464
Epoch: 3069, Batch Gradient Norm: 11.637047347108492
Epoch: 3069, Batch Gradient Norm after: 11.637047347108492
Epoch 3070/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.41582667231559756
Epoch: 3070, Batch Gradient Norm: 11.176798147672454
Epoch: 3070, Batch Gradient Norm after: 11.176798147672454
Epoch 3071/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4131554067134857
Epoch: 3071, Batch Gradient Norm: 11.652945154383069
Epoch: 3071, Batch Gradient Norm after: 11.652945154383069
Epoch 3072/10000, Prediction Accuracy = 61.878%, Loss = 0.41303821206092833
Epoch: 3072, Batch Gradient Norm: 11.699419248696767
Epoch: 3072, Batch Gradient Norm after: 11.699419248696767
Epoch 3073/10000, Prediction Accuracy = 61.8%, Loss = 0.4139811933040619
Epoch: 3073, Batch Gradient Norm: 12.516035549341249
Epoch: 3073, Batch Gradient Norm after: 12.516035549341249
Epoch 3074/10000, Prediction Accuracy = 61.806%, Loss = 0.41756240129470823
Epoch: 3074, Batch Gradient Norm: 13.344584392499174
Epoch: 3074, Batch Gradient Norm after: 13.344584392499174
Epoch 3075/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.41592358350753783
Epoch: 3075, Batch Gradient Norm: 12.515233601282064
Epoch: 3075, Batch Gradient Norm after: 12.515233601282064
Epoch 3076/10000, Prediction Accuracy = 61.86%, Loss = 0.41554691791534426
Epoch: 3076, Batch Gradient Norm: 14.370818222530112
Epoch: 3076, Batch Gradient Norm after: 14.370818222530112
Epoch 3077/10000, Prediction Accuracy = 61.874%, Loss = 0.4217315077781677
Epoch: 3077, Batch Gradient Norm: 12.114718738665625
Epoch: 3077, Batch Gradient Norm after: 12.114718738665625
Epoch 3078/10000, Prediction Accuracy = 61.8%, Loss = 0.4135418713092804
Epoch: 3078, Batch Gradient Norm: 12.41283564512216
Epoch: 3078, Batch Gradient Norm after: 12.41283564512216
Epoch 3079/10000, Prediction Accuracy = 61.86400000000001%, Loss = 0.41651359796524046
Epoch: 3079, Batch Gradient Norm: 12.024051272602701
Epoch: 3079, Batch Gradient Norm after: 12.024051272602701
Epoch 3080/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4141657710075378
Epoch: 3080, Batch Gradient Norm: 10.86155944042324
Epoch: 3080, Batch Gradient Norm after: 10.86155944042324
Epoch 3081/10000, Prediction Accuracy = 61.824%, Loss = 0.41247996091842654
Epoch: 3081, Batch Gradient Norm: 9.492886451246244
Epoch: 3081, Batch Gradient Norm after: 9.492886451246244
Epoch 3082/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4118470847606659
Epoch: 3082, Batch Gradient Norm: 12.06755992072838
Epoch: 3082, Batch Gradient Norm after: 12.06755992072838
Epoch 3083/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.41448949575424193
Epoch: 3083, Batch Gradient Norm: 14.985593916177516
Epoch: 3083, Batch Gradient Norm after: 14.985593916177516
Epoch 3084/10000, Prediction Accuracy = 61.83%, Loss = 0.41716882586479187
Epoch: 3084, Batch Gradient Norm: 18.91936480173401
Epoch: 3084, Batch Gradient Norm after: 18.91936480173401
Epoch 3085/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.42556535005569457
Epoch: 3085, Batch Gradient Norm: 19.235308139914466
Epoch: 3085, Batch Gradient Norm after: 19.198725207872293
Epoch 3086/10000, Prediction Accuracy = 61.85%, Loss = 0.42608016133308413
Epoch: 3086, Batch Gradient Norm: 12.883401438168423
Epoch: 3086, Batch Gradient Norm after: 12.883401438168423
Epoch 3087/10000, Prediction Accuracy = 61.68000000000001%, Loss = 0.41737439632415774
Epoch: 3087, Batch Gradient Norm: 12.015166341366173
Epoch: 3087, Batch Gradient Norm after: 12.015166341366173
Epoch 3088/10000, Prediction Accuracy = 61.782%, Loss = 0.4155264854431152
Epoch: 3088, Batch Gradient Norm: 11.465396489009628
Epoch: 3088, Batch Gradient Norm after: 11.465396489009628
Epoch 3089/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4151043832302094
Epoch: 3089, Batch Gradient Norm: 12.251778008356954
Epoch: 3089, Batch Gradient Norm after: 12.251778008356954
Epoch 3090/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.41407800316810606
Epoch: 3090, Batch Gradient Norm: 13.657448603933544
Epoch: 3090, Batch Gradient Norm after: 13.657448603933544
Epoch 3091/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.41673451066017153
Epoch: 3091, Batch Gradient Norm: 15.289201226554646
Epoch: 3091, Batch Gradient Norm after: 15.289201226554646
Epoch 3092/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4178925812244415
Epoch: 3092, Batch Gradient Norm: 15.46695736745808
Epoch: 3092, Batch Gradient Norm after: 15.46695736745808
Epoch 3093/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4194145381450653
Epoch: 3093, Batch Gradient Norm: 12.880782473435193
Epoch: 3093, Batch Gradient Norm after: 12.880782473435193
Epoch 3094/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.41673576831817627
Epoch: 3094, Batch Gradient Norm: 13.162059405868536
Epoch: 3094, Batch Gradient Norm after: 13.162059405868536
Epoch 3095/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.4151708006858826
Epoch: 3095, Batch Gradient Norm: 12.01952815533852
Epoch: 3095, Batch Gradient Norm after: 12.01952815533852
Epoch 3096/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.4146650195121765
Epoch: 3096, Batch Gradient Norm: 11.237039273665964
Epoch: 3096, Batch Gradient Norm after: 11.237039273665964
Epoch 3097/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4138361752033234
Epoch: 3097, Batch Gradient Norm: 9.875831161727408
Epoch: 3097, Batch Gradient Norm after: 9.875831161727408
Epoch 3098/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.41077563166618347
Epoch: 3098, Batch Gradient Norm: 9.08409853221183
Epoch: 3098, Batch Gradient Norm after: 9.08409853221183
Epoch 3099/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.40998045206069944
Epoch: 3099, Batch Gradient Norm: 9.2042725430537
Epoch: 3099, Batch Gradient Norm after: 9.2042725430537
Epoch 3100/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.40896965861320494
Epoch: 3100, Batch Gradient Norm: 9.664542646144632
Epoch: 3100, Batch Gradient Norm after: 9.664542646144632
Epoch 3101/10000, Prediction Accuracy = 61.836%, Loss = 0.4099688708782196
Epoch: 3101, Batch Gradient Norm: 9.723212851365872
Epoch: 3101, Batch Gradient Norm after: 9.723212851365872
Epoch 3102/10000, Prediction Accuracy = 61.928%, Loss = 0.41084068417549136
Epoch: 3102, Batch Gradient Norm: 11.553953926451806
Epoch: 3102, Batch Gradient Norm after: 11.553953926451806
Epoch 3103/10000, Prediction Accuracy = 61.806%, Loss = 0.4116471290588379
Epoch: 3103, Batch Gradient Norm: 14.364033930265654
Epoch: 3103, Batch Gradient Norm after: 14.364033930265654
Epoch 3104/10000, Prediction Accuracy = 61.814%, Loss = 0.4189308166503906
Epoch: 3104, Batch Gradient Norm: 13.013909193177142
Epoch: 3104, Batch Gradient Norm after: 13.013909193177142
Epoch 3105/10000, Prediction Accuracy = 61.78800000000001%, Loss = 0.41555107831954957
Epoch: 3105, Batch Gradient Norm: 10.030101874062767
Epoch: 3105, Batch Gradient Norm after: 10.030101874062767
Epoch 3106/10000, Prediction Accuracy = 61.834%, Loss = 0.4121177077293396
Epoch: 3106, Batch Gradient Norm: 12.116081954146914
Epoch: 3106, Batch Gradient Norm after: 12.116081954146914
Epoch 3107/10000, Prediction Accuracy = 61.766%, Loss = 0.4144570350646973
Epoch: 3107, Batch Gradient Norm: 12.874188986579858
Epoch: 3107, Batch Gradient Norm after: 12.874188986579858
Epoch 3108/10000, Prediction Accuracy = 61.814%, Loss = 0.41444718837738037
Epoch: 3108, Batch Gradient Norm: 13.403887792634427
Epoch: 3108, Batch Gradient Norm after: 13.403887792634427
Epoch 3109/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.41710459589958193
Epoch: 3109, Batch Gradient Norm: 11.529229449104628
Epoch: 3109, Batch Gradient Norm after: 11.529229449104628
Epoch 3110/10000, Prediction Accuracy = 61.854%, Loss = 0.4124328911304474
Epoch: 3110, Batch Gradient Norm: 13.1515211338738
Epoch: 3110, Batch Gradient Norm after: 13.1515211338738
Epoch 3111/10000, Prediction Accuracy = 61.848%, Loss = 0.4156228840351105
Epoch: 3111, Batch Gradient Norm: 16.71744325278079
Epoch: 3111, Batch Gradient Norm after: 16.693143308049443
Epoch 3112/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.4212773978710175
Epoch: 3112, Batch Gradient Norm: 14.963717761463036
Epoch: 3112, Batch Gradient Norm after: 14.963717761463036
Epoch 3113/10000, Prediction Accuracy = 61.824%, Loss = 0.418132483959198
Epoch: 3113, Batch Gradient Norm: 10.576315536741872
Epoch: 3113, Batch Gradient Norm after: 10.576315536741872
Epoch 3114/10000, Prediction Accuracy = 61.824%, Loss = 0.41094653606414794
Epoch: 3114, Batch Gradient Norm: 10.32945877593197
Epoch: 3114, Batch Gradient Norm after: 10.32945877593197
Epoch 3115/10000, Prediction Accuracy = 61.8%, Loss = 0.411997389793396
Epoch: 3115, Batch Gradient Norm: 13.484037537042656
Epoch: 3115, Batch Gradient Norm after: 13.484037537042656
Epoch 3116/10000, Prediction Accuracy = 61.766%, Loss = 0.41437416076660155
Epoch: 3116, Batch Gradient Norm: 11.709434894975741
Epoch: 3116, Batch Gradient Norm after: 11.709434894975741
Epoch 3117/10000, Prediction Accuracy = 61.872%, Loss = 0.41303921341896055
Epoch: 3117, Batch Gradient Norm: 11.133635052554727
Epoch: 3117, Batch Gradient Norm after: 11.133635052554727
Epoch 3118/10000, Prediction Accuracy = 61.852%, Loss = 0.4145318329334259
Epoch: 3118, Batch Gradient Norm: 13.221102233966986
Epoch: 3118, Batch Gradient Norm after: 13.221102233966986
Epoch 3119/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4150041937828064
Epoch: 3119, Batch Gradient Norm: 14.83630609119445
Epoch: 3119, Batch Gradient Norm after: 14.83630609119445
Epoch 3120/10000, Prediction Accuracy = 61.788%, Loss = 0.41879077553749083
Epoch: 3120, Batch Gradient Norm: 15.177271317297816
Epoch: 3120, Batch Gradient Norm after: 14.984724484374583
Epoch 3121/10000, Prediction Accuracy = 61.838%, Loss = 0.41571993231773374
Epoch: 3121, Batch Gradient Norm: 13.461184507291897
Epoch: 3121, Batch Gradient Norm after: 13.461184507291897
Epoch 3122/10000, Prediction Accuracy = 61.836%, Loss = 0.4156711459159851
Epoch: 3122, Batch Gradient Norm: 12.4232054490441
Epoch: 3122, Batch Gradient Norm after: 12.4232054490441
Epoch 3123/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.4135927021503448
Epoch: 3123, Batch Gradient Norm: 11.51680200672871
Epoch: 3123, Batch Gradient Norm after: 11.51680200672871
Epoch 3124/10000, Prediction Accuracy = 61.794%, Loss = 0.4116992294788361
Epoch: 3124, Batch Gradient Norm: 11.04906802449884
Epoch: 3124, Batch Gradient Norm after: 11.04906802449884
Epoch 3125/10000, Prediction Accuracy = 61.75%, Loss = 0.4124970197677612
Epoch: 3125, Batch Gradient Norm: 14.402395420394841
Epoch: 3125, Batch Gradient Norm after: 14.402395420394841
Epoch 3126/10000, Prediction Accuracy = 61.908%, Loss = 0.4169835686683655
Epoch: 3126, Batch Gradient Norm: 12.353350318182029
Epoch: 3126, Batch Gradient Norm after: 12.353350318182029
Epoch 3127/10000, Prediction Accuracy = 61.822%, Loss = 0.41361833810806276
Epoch: 3127, Batch Gradient Norm: 9.154375406814198
Epoch: 3127, Batch Gradient Norm after: 9.154375406814198
Epoch 3128/10000, Prediction Accuracy = 61.826%, Loss = 0.408875572681427
Epoch: 3128, Batch Gradient Norm: 11.302219699658663
Epoch: 3128, Batch Gradient Norm after: 11.302219699658663
Epoch 3129/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.41355336308479307
Epoch: 3129, Batch Gradient Norm: 12.194813703149313
Epoch: 3129, Batch Gradient Norm after: 12.194813703149313
Epoch 3130/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.4180759072303772
Epoch: 3130, Batch Gradient Norm: 11.708130801393574
Epoch: 3130, Batch Gradient Norm after: 11.708130801393574
Epoch 3131/10000, Prediction Accuracy = 61.84000000000001%, Loss = 0.4123476266860962
Epoch: 3131, Batch Gradient Norm: 13.760015922638136
Epoch: 3131, Batch Gradient Norm after: 13.760015922638136
Epoch 3132/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.41741669178009033
Epoch: 3132, Batch Gradient Norm: 11.302375078947005
Epoch: 3132, Batch Gradient Norm after: 11.302375078947005
Epoch 3133/10000, Prediction Accuracy = 61.86800000000001%, Loss = 0.4144935250282288
Epoch: 3133, Batch Gradient Norm: 9.825442443213165
Epoch: 3133, Batch Gradient Norm after: 9.825442443213165
Epoch 3134/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4108074724674225
Epoch: 3134, Batch Gradient Norm: 11.643092732472484
Epoch: 3134, Batch Gradient Norm after: 11.643092732472484
Epoch 3135/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.41380438208580017
Epoch: 3135, Batch Gradient Norm: 11.59511113971494
Epoch: 3135, Batch Gradient Norm after: 11.59511113971494
Epoch 3136/10000, Prediction Accuracy = 61.786%, Loss = 0.41245266795158386
Epoch: 3136, Batch Gradient Norm: 11.945766068568984
Epoch: 3136, Batch Gradient Norm after: 11.945766068568984
Epoch 3137/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.41156473755836487
Epoch: 3137, Batch Gradient Norm: 12.308553927828486
Epoch: 3137, Batch Gradient Norm after: 12.308553927828486
Epoch 3138/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.415070641040802
Epoch: 3138, Batch Gradient Norm: 13.287895037131563
Epoch: 3138, Batch Gradient Norm after: 13.287895037131563
Epoch 3139/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.415254807472229
Epoch: 3139, Batch Gradient Norm: 12.25632257225023
Epoch: 3139, Batch Gradient Norm after: 12.25632257225023
Epoch 3140/10000, Prediction Accuracy = 61.84000000000001%, Loss = 0.4124941170215607
Epoch: 3140, Batch Gradient Norm: 13.334141454998125
Epoch: 3140, Batch Gradient Norm after: 13.334141454998125
Epoch 3141/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4147721529006958
Epoch: 3141, Batch Gradient Norm: 10.549831617329088
Epoch: 3141, Batch Gradient Norm after: 10.549831617329088
Epoch 3142/10000, Prediction Accuracy = 61.848%, Loss = 0.4098304629325867
Epoch: 3142, Batch Gradient Norm: 11.101539880541486
Epoch: 3142, Batch Gradient Norm after: 11.101539880541486
Epoch 3143/10000, Prediction Accuracy = 61.852%, Loss = 0.40923468470573426
Epoch: 3143, Batch Gradient Norm: 10.575108134232098
Epoch: 3143, Batch Gradient Norm after: 10.575108134232098
Epoch 3144/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4091376543045044
Epoch: 3144, Batch Gradient Norm: 9.056601488000894
Epoch: 3144, Batch Gradient Norm after: 9.056601488000894
Epoch 3145/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4074600517749786
Epoch: 3145, Batch Gradient Norm: 9.819120958573684
Epoch: 3145, Batch Gradient Norm after: 9.819120958573684
Epoch 3146/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.40826369524002076
Epoch: 3146, Batch Gradient Norm: 11.169730500949584
Epoch: 3146, Batch Gradient Norm after: 11.169730500949584
Epoch 3147/10000, Prediction Accuracy = 61.872%, Loss = 0.41040533781051636
Epoch: 3147, Batch Gradient Norm: 9.658522156290125
Epoch: 3147, Batch Gradient Norm after: 9.658522156290125
Epoch 3148/10000, Prediction Accuracy = 61.926%, Loss = 0.4079813599586487
Epoch: 3148, Batch Gradient Norm: 12.44072061393919
Epoch: 3148, Batch Gradient Norm after: 12.44072061393919
Epoch 3149/10000, Prediction Accuracy = 61.782000000000004%, Loss = 0.4119300842285156
Epoch: 3149, Batch Gradient Norm: 12.912747167841278
Epoch: 3149, Batch Gradient Norm after: 12.912747167841278
Epoch 3150/10000, Prediction Accuracy = 61.742000000000004%, Loss = 0.41067875623703004
Epoch: 3150, Batch Gradient Norm: 11.698587035805417
Epoch: 3150, Batch Gradient Norm after: 11.698587035805417
Epoch 3151/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.4147095739841461
Epoch: 3151, Batch Gradient Norm: 10.516647693684996
Epoch: 3151, Batch Gradient Norm after: 10.516647693684996
Epoch 3152/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.41038696765899657
Epoch: 3152, Batch Gradient Norm: 13.028900020358057
Epoch: 3152, Batch Gradient Norm after: 13.028900020358057
Epoch 3153/10000, Prediction Accuracy = 61.88000000000001%, Loss = 0.41292564868927
Epoch: 3153, Batch Gradient Norm: 15.209098753819513
Epoch: 3153, Batch Gradient Norm after: 15.209098753819513
Epoch 3154/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4178048253059387
Epoch: 3154, Batch Gradient Norm: 13.48916526621394
Epoch: 3154, Batch Gradient Norm after: 13.48916526621394
Epoch 3155/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.4134390950202942
Epoch: 3155, Batch Gradient Norm: 11.251236662406038
Epoch: 3155, Batch Gradient Norm after: 11.251236662406038
Epoch 3156/10000, Prediction Accuracy = 61.884%, Loss = 0.4106014370918274
Epoch: 3156, Batch Gradient Norm: 11.136275771369462
Epoch: 3156, Batch Gradient Norm after: 11.136275771369462
Epoch 3157/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.41012988090515134
Epoch: 3157, Batch Gradient Norm: 11.103486640368544
Epoch: 3157, Batch Gradient Norm after: 11.103486640368544
Epoch 3158/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.413407164812088
Epoch: 3158, Batch Gradient Norm: 8.597650397085646
Epoch: 3158, Batch Gradient Norm after: 8.597650397085646
Epoch 3159/10000, Prediction Accuracy = 61.94%, Loss = 0.40774014592170715
Epoch: 3159, Batch Gradient Norm: 9.625501199483667
Epoch: 3159, Batch Gradient Norm after: 9.625501199483667
Epoch 3160/10000, Prediction Accuracy = 61.81400000000001%, Loss = 0.4094131886959076
Epoch: 3160, Batch Gradient Norm: 11.126013862801665
Epoch: 3160, Batch Gradient Norm after: 11.126013862801665
Epoch 3161/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.40975804924964904
Epoch: 3161, Batch Gradient Norm: 13.97287794574526
Epoch: 3161, Batch Gradient Norm after: 13.97287794574526
Epoch 3162/10000, Prediction Accuracy = 61.914%, Loss = 0.4154029250144958
Epoch: 3162, Batch Gradient Norm: 11.967735784589816
Epoch: 3162, Batch Gradient Norm after: 11.967735784589816
Epoch 3163/10000, Prediction Accuracy = 61.964%, Loss = 0.4105853259563446
Epoch: 3163, Batch Gradient Norm: 11.638659599094293
Epoch: 3163, Batch Gradient Norm after: 11.638659599094293
Epoch 3164/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.41362352967262267
Epoch: 3164, Batch Gradient Norm: 11.006180355648745
Epoch: 3164, Batch Gradient Norm after: 11.006180355648745
Epoch 3165/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4116308629512787
Epoch: 3165, Batch Gradient Norm: 8.970393567047369
Epoch: 3165, Batch Gradient Norm after: 8.970393567047369
Epoch 3166/10000, Prediction Accuracy = 61.762%, Loss = 0.4064851224422455
Epoch: 3166, Batch Gradient Norm: 8.373736088676884
Epoch: 3166, Batch Gradient Norm after: 8.373736088676884
Epoch 3167/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.40561893582344055
Epoch: 3167, Batch Gradient Norm: 11.172022453200928
Epoch: 3167, Batch Gradient Norm after: 11.172022453200928
Epoch 3168/10000, Prediction Accuracy = 61.77%, Loss = 0.41281297206878664
Epoch: 3168, Batch Gradient Norm: 10.062707853223507
Epoch: 3168, Batch Gradient Norm after: 10.062707853223507
Epoch 3169/10000, Prediction Accuracy = 61.928%, Loss = 0.4082897067070007
Epoch: 3169, Batch Gradient Norm: 11.56207517104349
Epoch: 3169, Batch Gradient Norm after: 11.56207517104349
Epoch 3170/10000, Prediction Accuracy = 61.898%, Loss = 0.4113334774971008
Epoch: 3170, Batch Gradient Norm: 10.363063301426454
Epoch: 3170, Batch Gradient Norm after: 10.363063301426454
Epoch 3171/10000, Prediction Accuracy = 61.722%, Loss = 0.4094159841537476
Epoch: 3171, Batch Gradient Norm: 9.021505119196917
Epoch: 3171, Batch Gradient Norm after: 9.021505119196917
Epoch 3172/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.40814716219902036
Epoch: 3172, Batch Gradient Norm: 9.18229230142457
Epoch: 3172, Batch Gradient Norm after: 9.18229230142457
Epoch 3173/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4069543480873108
Epoch: 3173, Batch Gradient Norm: 8.771876862430204
Epoch: 3173, Batch Gradient Norm after: 8.771876862430204
Epoch 3174/10000, Prediction Accuracy = 61.85%, Loss = 0.40552743673324587
Epoch: 3174, Batch Gradient Norm: 10.698601777560052
Epoch: 3174, Batch Gradient Norm after: 10.698601777560052
Epoch 3175/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4114964842796326
Epoch: 3175, Batch Gradient Norm: 12.085353853482317
Epoch: 3175, Batch Gradient Norm after: 12.085353853482317
Epoch 3176/10000, Prediction Accuracy = 61.824%, Loss = 0.41103546023368837
Epoch: 3176, Batch Gradient Norm: 12.342653125551069
Epoch: 3176, Batch Gradient Norm after: 12.342653125551069
Epoch 3177/10000, Prediction Accuracy = 61.87400000000001%, Loss = 0.4106464982032776
Epoch: 3177, Batch Gradient Norm: 11.495711858089827
Epoch: 3177, Batch Gradient Norm after: 11.495711858089827
Epoch 3178/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.4103572010993958
Epoch: 3178, Batch Gradient Norm: 11.727801425153482
Epoch: 3178, Batch Gradient Norm after: 11.727801425153482
Epoch 3179/10000, Prediction Accuracy = 61.694%, Loss = 0.4091397821903229
Epoch: 3179, Batch Gradient Norm: 16.634249132631332
Epoch: 3179, Batch Gradient Norm after: 16.634249132631332
Epoch 3180/10000, Prediction Accuracy = 61.85%, Loss = 0.41796669363975525
Epoch: 3180, Batch Gradient Norm: 16.3084058985599
Epoch: 3180, Batch Gradient Norm after: 16.3084058985599
Epoch 3181/10000, Prediction Accuracy = 61.726%, Loss = 0.419921213388443
Epoch: 3181, Batch Gradient Norm: 12.71504518987104
Epoch: 3181, Batch Gradient Norm after: 12.71504518987104
Epoch 3182/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.41088477373123167
Epoch: 3182, Batch Gradient Norm: 10.697905421274646
Epoch: 3182, Batch Gradient Norm after: 10.697905421274646
Epoch 3183/10000, Prediction Accuracy = 61.83%, Loss = 0.4089671313762665
Epoch: 3183, Batch Gradient Norm: 11.162794477251214
Epoch: 3183, Batch Gradient Norm after: 11.162794477251214
Epoch 3184/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40934387445449827
Epoch: 3184, Batch Gradient Norm: 10.302231531523894
Epoch: 3184, Batch Gradient Norm after: 10.302231531523894
Epoch 3185/10000, Prediction Accuracy = 61.924%, Loss = 0.40907823443412783
Epoch: 3185, Batch Gradient Norm: 10.850180972914831
Epoch: 3185, Batch Gradient Norm after: 10.850180972914831
Epoch 3186/10000, Prediction Accuracy = 61.912%, Loss = 0.40787249207496645
Epoch: 3186, Batch Gradient Norm: 11.215909547878766
Epoch: 3186, Batch Gradient Norm after: 11.215909547878766
Epoch 3187/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4132830500602722
Epoch: 3187, Batch Gradient Norm: 13.860170015913637
Epoch: 3187, Batch Gradient Norm after: 13.860170015913637
Epoch 3188/10000, Prediction Accuracy = 61.888%, Loss = 0.414630138874054
Epoch: 3188, Batch Gradient Norm: 13.33121598576245
Epoch: 3188, Batch Gradient Norm after: 13.33121598576245
Epoch 3189/10000, Prediction Accuracy = 61.908%, Loss = 0.41229389905929564
Epoch: 3189, Batch Gradient Norm: 15.98493406618819
Epoch: 3189, Batch Gradient Norm after: 15.98493406618819
Epoch 3190/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4213444828987122
Epoch: 3190, Batch Gradient Norm: 16.329434891372916
Epoch: 3190, Batch Gradient Norm after: 16.329434891372916
Epoch 3191/10000, Prediction Accuracy = 61.814%, Loss = 0.4151931285858154
Epoch: 3191, Batch Gradient Norm: 16.04180336399882
Epoch: 3191, Batch Gradient Norm after: 16.04180336399882
Epoch 3192/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.4144334435462952
Epoch: 3192, Batch Gradient Norm: 11.311994348885758
Epoch: 3192, Batch Gradient Norm after: 11.311994348885758
Epoch 3193/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4096941530704498
Epoch: 3193, Batch Gradient Norm: 11.570411386925405
Epoch: 3193, Batch Gradient Norm after: 11.570411386925405
Epoch 3194/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.40817098021507264
Epoch: 3194, Batch Gradient Norm: 12.89462238082116
Epoch: 3194, Batch Gradient Norm after: 12.89462238082116
Epoch 3195/10000, Prediction Accuracy = 61.908%, Loss = 0.4098284125328064
Epoch: 3195, Batch Gradient Norm: 11.407772409880957
Epoch: 3195, Batch Gradient Norm after: 11.407772409880957
Epoch 3196/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.4080682575702667
Epoch: 3196, Batch Gradient Norm: 13.509342452819435
Epoch: 3196, Batch Gradient Norm after: 13.509342452819435
Epoch 3197/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4099536776542664
Epoch: 3197, Batch Gradient Norm: 13.699426319107179
Epoch: 3197, Batch Gradient Norm after: 13.699426319107179
Epoch 3198/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.41176244616508484
Epoch: 3198, Batch Gradient Norm: 13.189323526732968
Epoch: 3198, Batch Gradient Norm after: 13.189323526732968
Epoch 3199/10000, Prediction Accuracy = 61.876%, Loss = 0.41271970272064207
Epoch: 3199, Batch Gradient Norm: 11.825830212563394
Epoch: 3199, Batch Gradient Norm after: 11.825830212563394
Epoch 3200/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.40920931100845337
Epoch: 3200, Batch Gradient Norm: 13.305944061973328
Epoch: 3200, Batch Gradient Norm after: 13.305944061973328
Epoch 3201/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.41312089562416077
Epoch: 3201, Batch Gradient Norm: 17.685104271330125
Epoch: 3201, Batch Gradient Norm after: 17.417206509532242
Epoch 3202/10000, Prediction Accuracy = 61.896%, Loss = 0.4196826934814453
Epoch: 3202, Batch Gradient Norm: 20.394958526556334
Epoch: 3202, Batch Gradient Norm after: 19.282687652603336
Epoch 3203/10000, Prediction Accuracy = 61.876%, Loss = 0.42599939107894896
Epoch: 3203, Batch Gradient Norm: 17.15275914278438
Epoch: 3203, Batch Gradient Norm after: 17.15275914278438
Epoch 3204/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.418075031042099
Epoch: 3204, Batch Gradient Norm: 15.66319112039855
Epoch: 3204, Batch Gradient Norm after: 15.66319112039855
Epoch 3205/10000, Prediction Accuracy = 61.988000000000014%, Loss = 0.4154483020305634
Epoch: 3205, Batch Gradient Norm: 13.40033569406926
Epoch: 3205, Batch Gradient Norm after: 13.40033569406926
Epoch 3206/10000, Prediction Accuracy = 61.9%, Loss = 0.413672012090683
Epoch: 3206, Batch Gradient Norm: 12.416683713618223
Epoch: 3206, Batch Gradient Norm after: 12.416683713618223
Epoch 3207/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.4112565219402313
Epoch: 3207, Batch Gradient Norm: 13.693322030141077
Epoch: 3207, Batch Gradient Norm after: 13.693322030141077
Epoch 3208/10000, Prediction Accuracy = 61.86%, Loss = 0.4134811043739319
Epoch: 3208, Batch Gradient Norm: 13.324411003275726
Epoch: 3208, Batch Gradient Norm after: 13.324411003275726
Epoch 3209/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.41174736618995667
Epoch: 3209, Batch Gradient Norm: 12.822474807597608
Epoch: 3209, Batch Gradient Norm after: 12.822474807597608
Epoch 3210/10000, Prediction Accuracy = 61.852%, Loss = 0.4118244409561157
Epoch: 3210, Batch Gradient Norm: 11.69222090679333
Epoch: 3210, Batch Gradient Norm after: 11.69222090679333
Epoch 3211/10000, Prediction Accuracy = 61.848%, Loss = 0.4116445302963257
Epoch: 3211, Batch Gradient Norm: 12.537906876600724
Epoch: 3211, Batch Gradient Norm after: 12.537906876600724
Epoch 3212/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.4107998549938202
Epoch: 3212, Batch Gradient Norm: 11.512763808842697
Epoch: 3212, Batch Gradient Norm after: 11.512763808842697
Epoch 3213/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.40825764536857606
Epoch: 3213, Batch Gradient Norm: 11.602289517731487
Epoch: 3213, Batch Gradient Norm after: 11.602289517731487
Epoch 3214/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.410218745470047
Epoch: 3214, Batch Gradient Norm: 10.152656228732718
Epoch: 3214, Batch Gradient Norm after: 10.152656228732718
Epoch 3215/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.407243949174881
Epoch: 3215, Batch Gradient Norm: 10.204767653711576
Epoch: 3215, Batch Gradient Norm after: 10.204767653711576
Epoch 3216/10000, Prediction Accuracy = 61.876%, Loss = 0.4067579388618469
Epoch: 3216, Batch Gradient Norm: 12.724444645138538
Epoch: 3216, Batch Gradient Norm after: 12.724444645138538
Epoch 3217/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4091645121574402
Epoch: 3217, Batch Gradient Norm: 13.67456368237807
Epoch: 3217, Batch Gradient Norm after: 13.67456368237807
Epoch 3218/10000, Prediction Accuracy = 61.89%, Loss = 0.4114235281944275
Epoch: 3218, Batch Gradient Norm: 12.28560013513339
Epoch: 3218, Batch Gradient Norm after: 12.28560013513339
Epoch 3219/10000, Prediction Accuracy = 61.872%, Loss = 0.41146476864814757
Epoch: 3219, Batch Gradient Norm: 12.678147532208653
Epoch: 3219, Batch Gradient Norm after: 12.678147532208653
Epoch 3220/10000, Prediction Accuracy = 61.822%, Loss = 0.4096523761749268
Epoch: 3220, Batch Gradient Norm: 11.30179080598358
Epoch: 3220, Batch Gradient Norm after: 11.30179080598358
Epoch 3221/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.4087218463420868
Epoch: 3221, Batch Gradient Norm: 12.023833009992057
Epoch: 3221, Batch Gradient Norm after: 12.023833009992057
Epoch 3222/10000, Prediction Accuracy = 61.83%, Loss = 0.40931381583213805
Epoch: 3222, Batch Gradient Norm: 14.822596901603706
Epoch: 3222, Batch Gradient Norm after: 14.822596901603706
Epoch 3223/10000, Prediction Accuracy = 61.878%, Loss = 0.4145368874073029
Epoch: 3223, Batch Gradient Norm: 14.619223724644819
Epoch: 3223, Batch Gradient Norm after: 14.619223724644819
Epoch 3224/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4124894022941589
Epoch: 3224, Batch Gradient Norm: 12.501866182023688
Epoch: 3224, Batch Gradient Norm after: 12.501866182023688
Epoch 3225/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.40758273005485535
Epoch: 3225, Batch Gradient Norm: 10.246204524016731
Epoch: 3225, Batch Gradient Norm after: 10.246204524016731
Epoch 3226/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4071840465068817
Epoch: 3226, Batch Gradient Norm: 9.347661619083059
Epoch: 3226, Batch Gradient Norm after: 9.347661619083059
Epoch 3227/10000, Prediction Accuracy = 61.943999999999996%, Loss = 0.4054270625114441
Epoch: 3227, Batch Gradient Norm: 9.691760504984298
Epoch: 3227, Batch Gradient Norm after: 9.691760504984298
Epoch 3228/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.40583573579788207
Epoch: 3228, Batch Gradient Norm: 10.312033913116585
Epoch: 3228, Batch Gradient Norm after: 10.312033913116585
Epoch 3229/10000, Prediction Accuracy = 61.85%, Loss = 0.40768673419952395
Epoch: 3229, Batch Gradient Norm: 11.0182849818913
Epoch: 3229, Batch Gradient Norm after: 11.0182849818913
Epoch 3230/10000, Prediction Accuracy = 61.888%, Loss = 0.4057876944541931
Epoch: 3230, Batch Gradient Norm: 12.126261343181955
Epoch: 3230, Batch Gradient Norm after: 12.126261343181955
Epoch 3231/10000, Prediction Accuracy = 61.778%, Loss = 0.41044139862060547
Epoch: 3231, Batch Gradient Norm: 13.55536441473453
Epoch: 3231, Batch Gradient Norm after: 13.55536441473453
Epoch 3232/10000, Prediction Accuracy = 61.95%, Loss = 0.4104564607143402
Epoch: 3232, Batch Gradient Norm: 10.722886391934802
Epoch: 3232, Batch Gradient Norm after: 10.722886391934802
Epoch 3233/10000, Prediction Accuracy = 61.884%, Loss = 0.40712510943412783
Epoch: 3233, Batch Gradient Norm: 11.497387443166769
Epoch: 3233, Batch Gradient Norm after: 11.497387443166769
Epoch 3234/10000, Prediction Accuracy = 61.98599999999999%, Loss = 0.40712507367134093
Epoch: 3234, Batch Gradient Norm: 15.065520545839622
Epoch: 3234, Batch Gradient Norm after: 15.065520545839622
Epoch 3235/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4124732851982117
Epoch: 3235, Batch Gradient Norm: 13.734690396682247
Epoch: 3235, Batch Gradient Norm after: 13.734690396682247
Epoch 3236/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4125904619693756
Epoch: 3236, Batch Gradient Norm: 11.93765606227025
Epoch: 3236, Batch Gradient Norm after: 11.93765606227025
Epoch 3237/10000, Prediction Accuracy = 61.85999999999999%, Loss = 0.40972267985343935
Epoch: 3237, Batch Gradient Norm: 12.35790238807683
Epoch: 3237, Batch Gradient Norm after: 12.35790238807683
Epoch 3238/10000, Prediction Accuracy = 61.96%, Loss = 0.40971359610557556
Epoch: 3238, Batch Gradient Norm: 10.625862948070631
Epoch: 3238, Batch Gradient Norm after: 10.625862948070631
Epoch 3239/10000, Prediction Accuracy = 62.004%, Loss = 0.4068569719791412
Epoch: 3239, Batch Gradient Norm: 12.974233655679482
Epoch: 3239, Batch Gradient Norm after: 12.974233655679482
Epoch 3240/10000, Prediction Accuracy = 61.928%, Loss = 0.4116129219532013
Epoch: 3240, Batch Gradient Norm: 11.01667342503377
Epoch: 3240, Batch Gradient Norm after: 11.01667342503377
Epoch 3241/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.4066940188407898
Epoch: 3241, Batch Gradient Norm: 11.575531079487563
Epoch: 3241, Batch Gradient Norm after: 11.575531079487563
Epoch 3242/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.4077556788921356
Epoch: 3242, Batch Gradient Norm: 14.490211011526345
Epoch: 3242, Batch Gradient Norm after: 14.3870606356575
Epoch 3243/10000, Prediction Accuracy = 61.84599999999999%, Loss = 0.41369972825050355
Epoch: 3243, Batch Gradient Norm: 14.73269153196926
Epoch: 3243, Batch Gradient Norm after: 14.73269153196926
Epoch 3244/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.41164422035217285
Epoch: 3244, Batch Gradient Norm: 12.118083921249216
Epoch: 3244, Batch Gradient Norm after: 12.118083921249216
Epoch 3245/10000, Prediction Accuracy = 61.938%, Loss = 0.4095472753047943
Epoch: 3245, Batch Gradient Norm: 13.13514648202093
Epoch: 3245, Batch Gradient Norm after: 13.13514648202093
Epoch 3246/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.40835166573524473
Epoch: 3246, Batch Gradient Norm: 13.208871325518105
Epoch: 3246, Batch Gradient Norm after: 13.208871325518105
Epoch 3247/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.40894684195518494
Epoch: 3247, Batch Gradient Norm: 12.086944495932936
Epoch: 3247, Batch Gradient Norm after: 12.086944495932936
Epoch 3248/10000, Prediction Accuracy = 61.814%, Loss = 0.4084101259708405
Epoch: 3248, Batch Gradient Norm: 12.895830578576735
Epoch: 3248, Batch Gradient Norm after: 12.895830578576735
Epoch 3249/10000, Prediction Accuracy = 61.926%, Loss = 0.40827503204345705
Epoch: 3249, Batch Gradient Norm: 11.98882635540783
Epoch: 3249, Batch Gradient Norm after: 11.98882635540783
Epoch 3250/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.40906103849411013
Epoch: 3250, Batch Gradient Norm: 10.672656203474158
Epoch: 3250, Batch Gradient Norm after: 10.672656203474158
Epoch 3251/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.40789098143577573
Epoch: 3251, Batch Gradient Norm: 12.430309580260804
Epoch: 3251, Batch Gradient Norm after: 12.430309580260804
Epoch 3252/10000, Prediction Accuracy = 61.93599999999999%, Loss = 0.4088214635848999
Epoch: 3252, Batch Gradient Norm: 11.078566070122422
Epoch: 3252, Batch Gradient Norm after: 11.078566070122422
Epoch 3253/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.40770916938781737
Epoch: 3253, Batch Gradient Norm: 11.650828315219828
Epoch: 3253, Batch Gradient Norm after: 11.650828315219828
Epoch 3254/10000, Prediction Accuracy = 61.972%, Loss = 0.40610164403915405
Epoch: 3254, Batch Gradient Norm: 13.07209738257552
Epoch: 3254, Batch Gradient Norm after: 13.07209738257552
Epoch 3255/10000, Prediction Accuracy = 61.962%, Loss = 0.40837247371673585
Epoch: 3255, Batch Gradient Norm: 13.220355161620068
Epoch: 3255, Batch Gradient Norm after: 13.220355161620068
Epoch 3256/10000, Prediction Accuracy = 61.976%, Loss = 0.40854268670082095
Epoch: 3256, Batch Gradient Norm: 12.427785239026113
Epoch: 3256, Batch Gradient Norm after: 12.427785239026113
Epoch 3257/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.40846061110496523
Epoch: 3257, Batch Gradient Norm: 13.563993412879842
Epoch: 3257, Batch Gradient Norm after: 13.563993412879842
Epoch 3258/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.41090108156204225
Epoch: 3258, Batch Gradient Norm: 13.93329100169049
Epoch: 3258, Batch Gradient Norm after: 13.93329100169049
Epoch 3259/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.4117422759532928
Epoch: 3259, Batch Gradient Norm: 13.020390651316912
Epoch: 3259, Batch Gradient Norm after: 13.020390651316912
Epoch 3260/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.4097072422504425
Epoch: 3260, Batch Gradient Norm: 11.376643996087052
Epoch: 3260, Batch Gradient Norm after: 11.376643996087052
Epoch 3261/10000, Prediction Accuracy = 61.992%, Loss = 0.40608630776405336
Epoch: 3261, Batch Gradient Norm: 10.051094320416036
Epoch: 3261, Batch Gradient Norm after: 10.051094320416036
Epoch 3262/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.4067470610141754
Epoch: 3262, Batch Gradient Norm: 10.868476302125636
Epoch: 3262, Batch Gradient Norm after: 10.868476302125636
Epoch 3263/10000, Prediction Accuracy = 61.822%, Loss = 0.4060782611370087
Epoch: 3263, Batch Gradient Norm: 10.272233609036503
Epoch: 3263, Batch Gradient Norm after: 10.272233609036503
Epoch 3264/10000, Prediction Accuracy = 61.852%, Loss = 0.40378912091255187
Epoch: 3264, Batch Gradient Norm: 10.78655569721101
Epoch: 3264, Batch Gradient Norm after: 10.78655569721101
Epoch 3265/10000, Prediction Accuracy = 61.896%, Loss = 0.4078946352005005
Epoch: 3265, Batch Gradient Norm: 12.13929140228158
Epoch: 3265, Batch Gradient Norm after: 12.13929140228158
Epoch 3266/10000, Prediction Accuracy = 62.004000000000005%, Loss = 0.40492802262306216
Epoch: 3266, Batch Gradient Norm: 11.131376452777172
Epoch: 3266, Batch Gradient Norm after: 11.131376452777172
Epoch 3267/10000, Prediction Accuracy = 61.872%, Loss = 0.40656707882881166
Epoch: 3267, Batch Gradient Norm: 13.571004430350357
Epoch: 3267, Batch Gradient Norm after: 13.571004430350357
Epoch 3268/10000, Prediction Accuracy = 61.874%, Loss = 0.41302528977394104
Epoch: 3268, Batch Gradient Norm: 14.808533674447274
Epoch: 3268, Batch Gradient Norm after: 14.808533674447274
Epoch 3269/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4115250587463379
Epoch: 3269, Batch Gradient Norm: 13.942508798331941
Epoch: 3269, Batch Gradient Norm after: 13.942508798331941
Epoch 3270/10000, Prediction Accuracy = 61.888%, Loss = 0.40993582606315615
Epoch: 3270, Batch Gradient Norm: 11.063170298712897
Epoch: 3270, Batch Gradient Norm after: 11.063170298712897
Epoch 3271/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.4043911337852478
Epoch: 3271, Batch Gradient Norm: 10.13522326600989
Epoch: 3271, Batch Gradient Norm after: 10.13522326600989
Epoch 3272/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.4048060655593872
Epoch: 3272, Batch Gradient Norm: 9.704890061909891
Epoch: 3272, Batch Gradient Norm after: 9.704890061909891
Epoch 3273/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40446149110794066
Epoch: 3273, Batch Gradient Norm: 9.644605771347365
Epoch: 3273, Batch Gradient Norm after: 9.644605771347365
Epoch 3274/10000, Prediction Accuracy = 61.94%, Loss = 0.4022687911987305
Epoch: 3274, Batch Gradient Norm: 8.963573343083347
Epoch: 3274, Batch Gradient Norm after: 8.963573343083347
Epoch 3275/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4036149501800537
Epoch: 3275, Batch Gradient Norm: 9.129549739870685
Epoch: 3275, Batch Gradient Norm after: 9.129549739870685
Epoch 3276/10000, Prediction Accuracy = 61.952%, Loss = 0.40284534096717833
Epoch: 3276, Batch Gradient Norm: 11.807740341151032
Epoch: 3276, Batch Gradient Norm after: 11.807740341151032
Epoch 3277/10000, Prediction Accuracy = 61.98%, Loss = 0.407024610042572
Epoch: 3277, Batch Gradient Norm: 11.522685840332239
Epoch: 3277, Batch Gradient Norm after: 11.522685840332239
Epoch 3278/10000, Prediction Accuracy = 61.826%, Loss = 0.40675336718559263
Epoch: 3278, Batch Gradient Norm: 11.120292637552536
Epoch: 3278, Batch Gradient Norm after: 11.120292637552536
Epoch 3279/10000, Prediction Accuracy = 61.896%, Loss = 0.40368114709854125
Epoch: 3279, Batch Gradient Norm: 11.400323977543097
Epoch: 3279, Batch Gradient Norm after: 11.400323977543097
Epoch 3280/10000, Prediction Accuracy = 61.94200000000001%, Loss = 0.4078167319297791
Epoch: 3280, Batch Gradient Norm: 9.625101029631482
Epoch: 3280, Batch Gradient Norm after: 9.625101029631482
Epoch 3281/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4036791563034058
Epoch: 3281, Batch Gradient Norm: 9.768557158691456
Epoch: 3281, Batch Gradient Norm after: 9.768557158691456
Epoch 3282/10000, Prediction Accuracy = 62.0%, Loss = 0.40367034673690794
Epoch: 3282, Batch Gradient Norm: 12.599501774976828
Epoch: 3282, Batch Gradient Norm after: 12.599501774976828
Epoch 3283/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4098144292831421
Epoch: 3283, Batch Gradient Norm: 13.499772535050413
Epoch: 3283, Batch Gradient Norm after: 13.499772535050413
Epoch 3284/10000, Prediction Accuracy = 61.886%, Loss = 0.409297913312912
Epoch: 3284, Batch Gradient Norm: 15.71646998383722
Epoch: 3284, Batch Gradient Norm after: 15.52680122327308
Epoch 3285/10000, Prediction Accuracy = 61.878%, Loss = 0.41298319697380065
Epoch: 3285, Batch Gradient Norm: 13.947144150342213
Epoch: 3285, Batch Gradient Norm after: 13.947144150342213
Epoch 3286/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.4105978488922119
Epoch: 3286, Batch Gradient Norm: 13.715655838227603
Epoch: 3286, Batch Gradient Norm after: 13.715655838227603
Epoch 3287/10000, Prediction Accuracy = 61.854000000000006%, Loss = 0.4096165120601654
Epoch: 3287, Batch Gradient Norm: 11.229100370141332
Epoch: 3287, Batch Gradient Norm after: 11.229100370141332
Epoch 3288/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.40529497861862185
Epoch: 3288, Batch Gradient Norm: 12.965185524014556
Epoch: 3288, Batch Gradient Norm after: 12.965185524014556
Epoch 3289/10000, Prediction Accuracy = 61.98%, Loss = 0.4081039309501648
Epoch: 3289, Batch Gradient Norm: 12.545862230140358
Epoch: 3289, Batch Gradient Norm after: 12.545862230140358
Epoch 3290/10000, Prediction Accuracy = 61.826%, Loss = 0.4074700355529785
Epoch: 3290, Batch Gradient Norm: 9.174481331274572
Epoch: 3290, Batch Gradient Norm after: 9.174481331274572
Epoch 3291/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4026487827301025
Epoch: 3291, Batch Gradient Norm: 12.25862961279446
Epoch: 3291, Batch Gradient Norm after: 12.25862961279446
Epoch 3292/10000, Prediction Accuracy = 61.92800000000001%, Loss = 0.4074757039546967
Epoch: 3292, Batch Gradient Norm: 11.412522991862328
Epoch: 3292, Batch Gradient Norm after: 11.412522991862328
Epoch 3293/10000, Prediction Accuracy = 61.812%, Loss = 0.40726153254508973
Epoch: 3293, Batch Gradient Norm: 10.143175040058114
Epoch: 3293, Batch Gradient Norm after: 10.143175040058114
Epoch 3294/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.40456266403198243
Epoch: 3294, Batch Gradient Norm: 11.179605063654769
Epoch: 3294, Batch Gradient Norm after: 11.179605063654769
Epoch 3295/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.40499187707901
Epoch: 3295, Batch Gradient Norm: 9.85465266459578
Epoch: 3295, Batch Gradient Norm after: 9.85465266459578
Epoch 3296/10000, Prediction Accuracy = 61.92%, Loss = 0.4035769820213318
Epoch: 3296, Batch Gradient Norm: 10.72384947975637
Epoch: 3296, Batch Gradient Norm after: 10.72384947975637
Epoch 3297/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.4041297435760498
Epoch: 3297, Batch Gradient Norm: 10.473334142935782
Epoch: 3297, Batch Gradient Norm after: 10.473334142935782
Epoch 3298/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.4050209105014801
Epoch: 3298, Batch Gradient Norm: 8.7223278392141
Epoch: 3298, Batch Gradient Norm after: 8.7223278392141
Epoch 3299/10000, Prediction Accuracy = 61.996%, Loss = 0.401032018661499
Epoch: 3299, Batch Gradient Norm: 8.883164568446162
Epoch: 3299, Batch Gradient Norm after: 8.883164568446162
Epoch 3300/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.40134140849113464
Epoch: 3300, Batch Gradient Norm: 10.190543429507816
Epoch: 3300, Batch Gradient Norm after: 10.190543429507816
Epoch 3301/10000, Prediction Accuracy = 61.98%, Loss = 0.4033398687839508
Epoch: 3301, Batch Gradient Norm: 11.653372508706793
Epoch: 3301, Batch Gradient Norm after: 11.653372508706793
Epoch 3302/10000, Prediction Accuracy = 62.056%, Loss = 0.4035774767398834
Epoch: 3302, Batch Gradient Norm: 11.749938831421911
Epoch: 3302, Batch Gradient Norm after: 11.749938831421911
Epoch 3303/10000, Prediction Accuracy = 61.992%, Loss = 0.4046865403652191
Epoch: 3303, Batch Gradient Norm: 14.193198574798526
Epoch: 3303, Batch Gradient Norm after: 14.193198574798526
Epoch 3304/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.4083303391933441
Epoch: 3304, Batch Gradient Norm: 13.76173982261443
Epoch: 3304, Batch Gradient Norm after: 13.76173982261443
Epoch 3305/10000, Prediction Accuracy = 61.976%, Loss = 0.4083164811134338
Epoch: 3305, Batch Gradient Norm: 12.96155902488423
Epoch: 3305, Batch Gradient Norm after: 12.96155902488423
Epoch 3306/10000, Prediction Accuracy = 61.972%, Loss = 0.407141375541687
Epoch: 3306, Batch Gradient Norm: 14.9514442931208
Epoch: 3306, Batch Gradient Norm after: 14.9514442931208
Epoch 3307/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.41003702878952025
Epoch: 3307, Batch Gradient Norm: 13.874265473145725
Epoch: 3307, Batch Gradient Norm after: 13.874265473145725
Epoch 3308/10000, Prediction Accuracy = 61.898%, Loss = 0.4097996771335602
Epoch: 3308, Batch Gradient Norm: 14.113828286041093
Epoch: 3308, Batch Gradient Norm after: 14.113828286041093
Epoch 3309/10000, Prediction Accuracy = 61.948%, Loss = 0.4104761600494385
Epoch: 3309, Batch Gradient Norm: 16.32739232932272
Epoch: 3309, Batch Gradient Norm after: 16.032205651452305
Epoch 3310/10000, Prediction Accuracy = 61.876%, Loss = 0.4139875292778015
Epoch: 3310, Batch Gradient Norm: 14.466043872655783
Epoch: 3310, Batch Gradient Norm after: 14.466043872655783
Epoch 3311/10000, Prediction Accuracy = 61.864%, Loss = 0.40897742509841917
Epoch: 3311, Batch Gradient Norm: 16.47259908891452
Epoch: 3311, Batch Gradient Norm after: 16.47259908891452
Epoch 3312/10000, Prediction Accuracy = 61.896%, Loss = 0.4131136417388916
Epoch: 3312, Batch Gradient Norm: 14.72824619338327
Epoch: 3312, Batch Gradient Norm after: 14.72824619338327
Epoch 3313/10000, Prediction Accuracy = 61.95%, Loss = 0.4115748882293701
Epoch: 3313, Batch Gradient Norm: 13.237810114769726
Epoch: 3313, Batch Gradient Norm after: 13.237810114769726
Epoch 3314/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.4074545443058014
Epoch: 3314, Batch Gradient Norm: 12.730817994054625
Epoch: 3314, Batch Gradient Norm after: 12.730817994054625
Epoch 3315/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4097509324550629
Epoch: 3315, Batch Gradient Norm: 14.273310733532613
Epoch: 3315, Batch Gradient Norm after: 14.273310733532613
Epoch 3316/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4077689230442047
Epoch: 3316, Batch Gradient Norm: 14.221866714845001
Epoch: 3316, Batch Gradient Norm after: 14.221866714845001
Epoch 3317/10000, Prediction Accuracy = 61.902%, Loss = 0.41028417348861695
Epoch: 3317, Batch Gradient Norm: 16.939113540807696
Epoch: 3317, Batch Gradient Norm after: 16.939113540807696
Epoch 3318/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.41359922885894773
Epoch: 3318, Batch Gradient Norm: 13.541923589483652
Epoch: 3318, Batch Gradient Norm after: 13.541923589483652
Epoch 3319/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.41039586067199707
Epoch: 3319, Batch Gradient Norm: 18.33621016899296
Epoch: 3319, Batch Gradient Norm after: 16.798348531752783
Epoch 3320/10000, Prediction Accuracy = 61.948%, Loss = 0.4206832230091095
Epoch: 3320, Batch Gradient Norm: 18.056571026935067
Epoch: 3320, Batch Gradient Norm after: 18.056571026935067
Epoch 3321/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.41861063838005064
Epoch: 3321, Batch Gradient Norm: 13.54164712424798
Epoch: 3321, Batch Gradient Norm after: 13.54164712424798
Epoch 3322/10000, Prediction Accuracy = 61.876%, Loss = 0.41102057695388794
Epoch: 3322, Batch Gradient Norm: 11.935333347168958
Epoch: 3322, Batch Gradient Norm after: 11.935333347168958
Epoch 3323/10000, Prediction Accuracy = 61.94%, Loss = 0.40692052245140076
Epoch: 3323, Batch Gradient Norm: 14.227416587088165
Epoch: 3323, Batch Gradient Norm after: 14.227416587088165
Epoch 3324/10000, Prediction Accuracy = 61.932%, Loss = 0.41040372252464297
Epoch: 3324, Batch Gradient Norm: 13.816755127313584
Epoch: 3324, Batch Gradient Norm after: 13.816755127313584
Epoch 3325/10000, Prediction Accuracy = 61.95%, Loss = 0.4105550289154053
Epoch: 3325, Batch Gradient Norm: 14.017612359892672
Epoch: 3325, Batch Gradient Norm after: 14.017612359892672
Epoch 3326/10000, Prediction Accuracy = 61.96999999999999%, Loss = 0.4089114487171173
Epoch: 3326, Batch Gradient Norm: 12.90029098516877
Epoch: 3326, Batch Gradient Norm after: 12.90029098516877
Epoch 3327/10000, Prediction Accuracy = 61.89%, Loss = 0.4079318940639496
Epoch: 3327, Batch Gradient Norm: 10.883442859986609
Epoch: 3327, Batch Gradient Norm after: 10.883442859986609
Epoch 3328/10000, Prediction Accuracy = 61.924%, Loss = 0.40345176458358767
Epoch: 3328, Batch Gradient Norm: 11.029175868179795
Epoch: 3328, Batch Gradient Norm after: 11.029175868179795
Epoch 3329/10000, Prediction Accuracy = 61.964%, Loss = 0.4035478413105011
Epoch: 3329, Batch Gradient Norm: 8.76892507525217
Epoch: 3329, Batch Gradient Norm after: 8.76892507525217
Epoch 3330/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4018920183181763
Epoch: 3330, Batch Gradient Norm: 8.33263759175785
Epoch: 3330, Batch Gradient Norm after: 8.33263759175785
Epoch 3331/10000, Prediction Accuracy = 61.92%, Loss = 0.4030027031898499
Epoch: 3331, Batch Gradient Norm: 11.608428625240364
Epoch: 3331, Batch Gradient Norm after: 11.608428625240364
Epoch 3332/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.40480418801307677
Epoch: 3332, Batch Gradient Norm: 11.298492441281022
Epoch: 3332, Batch Gradient Norm after: 11.298492441281022
Epoch 3333/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.40531991720199584
Epoch: 3333, Batch Gradient Norm: 12.871066408373519
Epoch: 3333, Batch Gradient Norm after: 12.871066408373519
Epoch 3334/10000, Prediction Accuracy = 62.0%, Loss = 0.4056344985961914
Epoch: 3334, Batch Gradient Norm: 15.067153600938166
Epoch: 3334, Batch Gradient Norm after: 15.067153600938166
Epoch 3335/10000, Prediction Accuracy = 61.972%, Loss = 0.4071308374404907
Epoch: 3335, Batch Gradient Norm: 13.379418786578862
Epoch: 3335, Batch Gradient Norm after: 13.379418786578862
Epoch 3336/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.40567449331283567
Epoch: 3336, Batch Gradient Norm: 15.288514238013935
Epoch: 3336, Batch Gradient Norm after: 15.288514238013935
Epoch 3337/10000, Prediction Accuracy = 62.038%, Loss = 0.40898764729499815
Epoch: 3337, Batch Gradient Norm: 14.824517716673657
Epoch: 3337, Batch Gradient Norm after: 14.824517716673657
Epoch 3338/10000, Prediction Accuracy = 62.04%, Loss = 0.4100997090339661
Epoch: 3338, Batch Gradient Norm: 13.725593160246309
Epoch: 3338, Batch Gradient Norm after: 13.725593160246309
Epoch 3339/10000, Prediction Accuracy = 62.112%, Loss = 0.4082135260105133
Epoch: 3339, Batch Gradient Norm: 14.20845761177039
Epoch: 3339, Batch Gradient Norm after: 14.20845761177039
Epoch 3340/10000, Prediction Accuracy = 61.982000000000006%, Loss = 0.4089008331298828
Epoch: 3340, Batch Gradient Norm: 13.169761494936106
Epoch: 3340, Batch Gradient Norm after: 13.169761494936106
Epoch 3341/10000, Prediction Accuracy = 61.862%, Loss = 0.4069684624671936
Epoch: 3341, Batch Gradient Norm: 13.630742097152856
Epoch: 3341, Batch Gradient Norm after: 13.630742097152856
Epoch 3342/10000, Prediction Accuracy = 61.916%, Loss = 0.4068852484226227
Epoch: 3342, Batch Gradient Norm: 14.686303729923546
Epoch: 3342, Batch Gradient Norm after: 14.686303729923546
Epoch 3343/10000, Prediction Accuracy = 61.86%, Loss = 0.40809241533279417
Epoch: 3343, Batch Gradient Norm: 8.60009152111129
Epoch: 3343, Batch Gradient Norm after: 8.60009152111129
Epoch 3344/10000, Prediction Accuracy = 61.898%, Loss = 0.40065357089042664
Epoch: 3344, Batch Gradient Norm: 11.322219811371339
Epoch: 3344, Batch Gradient Norm after: 11.322219811371339
Epoch 3345/10000, Prediction Accuracy = 62.05%, Loss = 0.40370343923568724
Epoch: 3345, Batch Gradient Norm: 11.351306282057122
Epoch: 3345, Batch Gradient Norm after: 11.351306282057122
Epoch 3346/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.40442546606063845
Epoch: 3346, Batch Gradient Norm: 11.149338505298894
Epoch: 3346, Batch Gradient Norm after: 11.149338505298894
Epoch 3347/10000, Prediction Accuracy = 61.9%, Loss = 0.4036432147026062
Epoch: 3347, Batch Gradient Norm: 13.733037908720732
Epoch: 3347, Batch Gradient Norm after: 13.733037908720732
Epoch 3348/10000, Prediction Accuracy = 61.886%, Loss = 0.40746802687644956
Epoch: 3348, Batch Gradient Norm: 12.944121460594602
Epoch: 3348, Batch Gradient Norm after: 12.944121460594602
Epoch 3349/10000, Prediction Accuracy = 62.076%, Loss = 0.4068749785423279
Epoch: 3349, Batch Gradient Norm: 11.878592083227597
Epoch: 3349, Batch Gradient Norm after: 11.878592083227597
Epoch 3350/10000, Prediction Accuracy = 61.9%, Loss = 0.4123939394950867
Epoch: 3350, Batch Gradient Norm: 13.375287630518734
Epoch: 3350, Batch Gradient Norm after: 13.375287630518734
Epoch 3351/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4067194640636444
Epoch: 3351, Batch Gradient Norm: 14.21966603382998
Epoch: 3351, Batch Gradient Norm after: 14.21966603382998
Epoch 3352/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.40891222953796386
Epoch: 3352, Batch Gradient Norm: 12.368472965270199
Epoch: 3352, Batch Gradient Norm after: 12.368472965270199
Epoch 3353/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.4055618643760681
Epoch: 3353, Batch Gradient Norm: 11.49829030254387
Epoch: 3353, Batch Gradient Norm after: 11.49829030254387
Epoch 3354/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4036134958267212
Epoch: 3354, Batch Gradient Norm: 12.714083999119142
Epoch: 3354, Batch Gradient Norm after: 12.714083999119142
Epoch 3355/10000, Prediction Accuracy = 61.92%, Loss = 0.404703414440155
Epoch: 3355, Batch Gradient Norm: 11.391397190282042
Epoch: 3355, Batch Gradient Norm after: 11.391397190282042
Epoch 3356/10000, Prediction Accuracy = 62.032%, Loss = 0.4037518084049225
Epoch: 3356, Batch Gradient Norm: 12.433162809978855
Epoch: 3356, Batch Gradient Norm after: 12.433162809978855
Epoch 3357/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40507677793502805
Epoch: 3357, Batch Gradient Norm: 10.985251258295323
Epoch: 3357, Batch Gradient Norm after: 10.985251258295323
Epoch 3358/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.4024361491203308
Epoch: 3358, Batch Gradient Norm: 11.216924024205856
Epoch: 3358, Batch Gradient Norm after: 11.216924024205856
Epoch 3359/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.4022712230682373
Epoch: 3359, Batch Gradient Norm: 10.000678074706773
Epoch: 3359, Batch Gradient Norm after: 10.000678074706773
Epoch 3360/10000, Prediction Accuracy = 62.016%, Loss = 0.4013941824436188
Epoch: 3360, Batch Gradient Norm: 11.648870842514555
Epoch: 3360, Batch Gradient Norm after: 11.648870842514555
Epoch 3361/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.40334475636482237
Epoch: 3361, Batch Gradient Norm: 12.67787244835226
Epoch: 3361, Batch Gradient Norm after: 12.67787244835226
Epoch 3362/10000, Prediction Accuracy = 61.984%, Loss = 0.40498111844062806
Epoch: 3362, Batch Gradient Norm: 12.801150965847079
Epoch: 3362, Batch Gradient Norm after: 12.801150965847079
Epoch 3363/10000, Prediction Accuracy = 61.952%, Loss = 0.40336319208145144
Epoch: 3363, Batch Gradient Norm: 14.471516600671933
Epoch: 3363, Batch Gradient Norm after: 14.471516600671933
Epoch 3364/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.40769180059432986
Epoch: 3364, Batch Gradient Norm: 16.76392151292357
Epoch: 3364, Batch Gradient Norm after: 16.531622603939088
Epoch 3365/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.41123088002204894
Epoch: 3365, Batch Gradient Norm: 14.805753436092315
Epoch: 3365, Batch Gradient Norm after: 14.805753436092315
Epoch 3366/10000, Prediction Accuracy = 62.038%, Loss = 0.407393878698349
Epoch: 3366, Batch Gradient Norm: 14.504960673799593
Epoch: 3366, Batch Gradient Norm after: 14.504960673799593
Epoch 3367/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.40574702620506287
Epoch: 3367, Batch Gradient Norm: 11.761154276924172
Epoch: 3367, Batch Gradient Norm after: 11.761154276924172
Epoch 3368/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.4042026996612549
Epoch: 3368, Batch Gradient Norm: 11.083354991654721
Epoch: 3368, Batch Gradient Norm after: 11.083354991654721
Epoch 3369/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.40338494777679446
Epoch: 3369, Batch Gradient Norm: 14.045571297648324
Epoch: 3369, Batch Gradient Norm after: 14.045571297648324
Epoch 3370/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4106459438800812
Epoch: 3370, Batch Gradient Norm: 14.59849987483371
Epoch: 3370, Batch Gradient Norm after: 14.59849987483371
Epoch 3371/10000, Prediction Accuracy = 62.04599999999999%, Loss = 0.4113677442073822
Epoch: 3371, Batch Gradient Norm: 13.31618634295449
Epoch: 3371, Batch Gradient Norm after: 13.31618634295449
Epoch 3372/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.4062776267528534
Epoch: 3372, Batch Gradient Norm: 10.872979701007942
Epoch: 3372, Batch Gradient Norm after: 10.872979701007942
Epoch 3373/10000, Prediction Accuracy = 62.052%, Loss = 0.4018901944160461
Epoch: 3373, Batch Gradient Norm: 10.883211614727704
Epoch: 3373, Batch Gradient Norm after: 10.883211614727704
Epoch 3374/10000, Prediction Accuracy = 62.03599999999999%, Loss = 0.40267221331596376
Epoch: 3374, Batch Gradient Norm: 11.354234571786945
Epoch: 3374, Batch Gradient Norm after: 11.354234571786945
Epoch 3375/10000, Prediction Accuracy = 61.964%, Loss = 0.40208445191383363
Epoch: 3375, Batch Gradient Norm: 13.315965794418593
Epoch: 3375, Batch Gradient Norm after: 13.315965794418593
Epoch 3376/10000, Prediction Accuracy = 62.04%, Loss = 0.40770745277404785
Epoch: 3376, Batch Gradient Norm: 13.015133707760912
Epoch: 3376, Batch Gradient Norm after: 13.015133707760912
Epoch 3377/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.40510364770889284
Epoch: 3377, Batch Gradient Norm: 14.766999031892587
Epoch: 3377, Batch Gradient Norm after: 14.766999031892587
Epoch 3378/10000, Prediction Accuracy = 62.008%, Loss = 0.4101709008216858
Epoch: 3378, Batch Gradient Norm: 12.287674958047456
Epoch: 3378, Batch Gradient Norm after: 12.287674958047456
Epoch 3379/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.40338751673698425
Epoch: 3379, Batch Gradient Norm: 13.040247919311787
Epoch: 3379, Batch Gradient Norm after: 13.040247919311787
Epoch 3380/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.40454342365264895
Epoch: 3380, Batch Gradient Norm: 12.866325185654896
Epoch: 3380, Batch Gradient Norm after: 12.866325185654896
Epoch 3381/10000, Prediction Accuracy = 61.976%, Loss = 0.40371394753456114
Epoch: 3381, Batch Gradient Norm: 10.370287421741375
Epoch: 3381, Batch Gradient Norm after: 10.370287421741375
Epoch 3382/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.3995190322399139
Epoch: 3382, Batch Gradient Norm: 13.286264243024378
Epoch: 3382, Batch Gradient Norm after: 13.286264243024378
Epoch 3383/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.40501587390899657
Epoch: 3383, Batch Gradient Norm: 13.72234484459903
Epoch: 3383, Batch Gradient Norm after: 13.72234484459903
Epoch 3384/10000, Prediction Accuracy = 62.056%, Loss = 0.4065291941165924
Epoch: 3384, Batch Gradient Norm: 11.163815433545285
Epoch: 3384, Batch Gradient Norm after: 11.163815433545285
Epoch 3385/10000, Prediction Accuracy = 61.948%, Loss = 0.4023277461528778
Epoch: 3385, Batch Gradient Norm: 12.299536607450825
Epoch: 3385, Batch Gradient Norm after: 12.299536607450825
Epoch 3386/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.40633185505867003
Epoch: 3386, Batch Gradient Norm: 13.590695995167723
Epoch: 3386, Batch Gradient Norm after: 13.590695995167723
Epoch 3387/10000, Prediction Accuracy = 61.982000000000006%, Loss = 0.4064043700695038
Epoch: 3387, Batch Gradient Norm: 13.385408083230129
Epoch: 3387, Batch Gradient Norm after: 13.385408083230129
Epoch 3388/10000, Prediction Accuracy = 61.93400000000001%, Loss = 0.4054814040660858
Epoch: 3388, Batch Gradient Norm: 11.500667557794026
Epoch: 3388, Batch Gradient Norm after: 11.500667557794026
Epoch 3389/10000, Prediction Accuracy = 62.08%, Loss = 0.4020558476448059
Epoch: 3389, Batch Gradient Norm: 11.92947060450728
Epoch: 3389, Batch Gradient Norm after: 11.92947060450728
Epoch 3390/10000, Prediction Accuracy = 61.878%, Loss = 0.40530667304992674
Epoch: 3390, Batch Gradient Norm: 14.071024790808096
Epoch: 3390, Batch Gradient Norm after: 14.071024790808096
Epoch 3391/10000, Prediction Accuracy = 61.984%, Loss = 0.4063351690769196
Epoch: 3391, Batch Gradient Norm: 10.602125980582906
Epoch: 3391, Batch Gradient Norm after: 10.602125980582906
Epoch 3392/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.40173088312149047
Epoch: 3392, Batch Gradient Norm: 12.063187316409119
Epoch: 3392, Batch Gradient Norm after: 12.063187316409119
Epoch 3393/10000, Prediction Accuracy = 61.968%, Loss = 0.404484885931015
Epoch: 3393, Batch Gradient Norm: 15.549471726900153
Epoch: 3393, Batch Gradient Norm after: 15.549471726900153
Epoch 3394/10000, Prediction Accuracy = 61.984%, Loss = 0.40846415758132937
Epoch: 3394, Batch Gradient Norm: 16.799125396711556
Epoch: 3394, Batch Gradient Norm after: 16.799125396711556
Epoch 3395/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.41149606704711916
Epoch: 3395, Batch Gradient Norm: 15.159913099306497
Epoch: 3395, Batch Gradient Norm after: 15.159913099306497
Epoch 3396/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.40763579607009887
Epoch: 3396, Batch Gradient Norm: 13.477592570903058
Epoch: 3396, Batch Gradient Norm after: 13.477592570903058
Epoch 3397/10000, Prediction Accuracy = 61.92%, Loss = 0.4054637014865875
Epoch: 3397, Batch Gradient Norm: 12.92346629681539
Epoch: 3397, Batch Gradient Norm after: 12.92346629681539
Epoch 3398/10000, Prediction Accuracy = 61.98%, Loss = 0.40500286817550657
Epoch: 3398, Batch Gradient Norm: 13.33256722232389
Epoch: 3398, Batch Gradient Norm after: 13.33256722232389
Epoch 3399/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4062561810016632
Epoch: 3399, Batch Gradient Norm: 10.850736611357288
Epoch: 3399, Batch Gradient Norm after: 10.850736611357288
Epoch 3400/10000, Prediction Accuracy = 61.976%, Loss = 0.4013438820838928
Epoch: 3400, Batch Gradient Norm: 10.431694915116942
Epoch: 3400, Batch Gradient Norm after: 10.431694915116942
Epoch 3401/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4016763210296631
Epoch: 3401, Batch Gradient Norm: 8.609878619110036
Epoch: 3401, Batch Gradient Norm after: 8.609878619110036
Epoch 3402/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40054035782814024
Epoch: 3402, Batch Gradient Norm: 9.315583367324775
Epoch: 3402, Batch Gradient Norm after: 9.315583367324775
Epoch 3403/10000, Prediction Accuracy = 62.048%, Loss = 0.39993993639945985
Epoch: 3403, Batch Gradient Norm: 10.7595616303259
Epoch: 3403, Batch Gradient Norm after: 10.7595616303259
Epoch 3404/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.40180805921554563
Epoch: 3404, Batch Gradient Norm: 11.772265181581956
Epoch: 3404, Batch Gradient Norm after: 11.772265181581956
Epoch 3405/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.40271416306495667
Epoch: 3405, Batch Gradient Norm: 12.96628734652639
Epoch: 3405, Batch Gradient Norm after: 12.96628734652639
Epoch 3406/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.40244855284690856
Epoch: 3406, Batch Gradient Norm: 11.448436852268932
Epoch: 3406, Batch Gradient Norm after: 11.448436852268932
Epoch 3407/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.40123172998428347
Epoch: 3407, Batch Gradient Norm: 13.91120457975887
Epoch: 3407, Batch Gradient Norm after: 13.91120457975887
Epoch 3408/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.40444694757461547
Epoch: 3408, Batch Gradient Norm: 12.981164846017752
Epoch: 3408, Batch Gradient Norm after: 12.981164846017752
Epoch 3409/10000, Prediction Accuracy = 62.024%, Loss = 0.4030440390110016
Epoch: 3409, Batch Gradient Norm: 12.865165653939552
Epoch: 3409, Batch Gradient Norm after: 12.865165653939552
Epoch 3410/10000, Prediction Accuracy = 62.07800000000001%, Loss = 0.403659725189209
Epoch: 3410, Batch Gradient Norm: 15.282443359671207
Epoch: 3410, Batch Gradient Norm after: 15.282443359671207
Epoch 3411/10000, Prediction Accuracy = 62.004%, Loss = 0.4061204791069031
Epoch: 3411, Batch Gradient Norm: 15.229752106389624
Epoch: 3411, Batch Gradient Norm after: 15.229752106389624
Epoch 3412/10000, Prediction Accuracy = 61.93399999999999%, Loss = 0.4062584638595581
Epoch: 3412, Batch Gradient Norm: 12.702095783353194
Epoch: 3412, Batch Gradient Norm after: 12.702095783353194
Epoch 3413/10000, Prediction Accuracy = 62.024%, Loss = 0.40322163701057434
Epoch: 3413, Batch Gradient Norm: 12.343773137314503
Epoch: 3413, Batch Gradient Norm after: 12.343773137314503
Epoch 3414/10000, Prediction Accuracy = 62.088%, Loss = 0.40302648544311526
Epoch: 3414, Batch Gradient Norm: 12.24299297017506
Epoch: 3414, Batch Gradient Norm after: 12.24299297017506
Epoch 3415/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.4034074544906616
Epoch: 3415, Batch Gradient Norm: 12.709636620304508
Epoch: 3415, Batch Gradient Norm after: 12.709636620304508
Epoch 3416/10000, Prediction Accuracy = 62.052%, Loss = 0.40224632024765017
Epoch: 3416, Batch Gradient Norm: 13.416962604997448
Epoch: 3416, Batch Gradient Norm after: 13.416962604997448
Epoch 3417/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.4044476687908173
Epoch: 3417, Batch Gradient Norm: 11.162657522620199
Epoch: 3417, Batch Gradient Norm after: 11.162657522620199
Epoch 3418/10000, Prediction Accuracy = 62.074%, Loss = 0.40300593972206117
Epoch: 3418, Batch Gradient Norm: 12.422367021958962
Epoch: 3418, Batch Gradient Norm after: 12.422367021958962
Epoch 3419/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4018587827682495
Epoch: 3419, Batch Gradient Norm: 14.503066173449822
Epoch: 3419, Batch Gradient Norm after: 14.503066173449822
Epoch 3420/10000, Prediction Accuracy = 62.104%, Loss = 0.4036992907524109
Epoch: 3420, Batch Gradient Norm: 12.13318660590854
Epoch: 3420, Batch Gradient Norm after: 12.13318660590854
Epoch 3421/10000, Prediction Accuracy = 61.980000000000004%, Loss = 0.4019900977611542
Epoch: 3421, Batch Gradient Norm: 8.681576779172666
Epoch: 3421, Batch Gradient Norm after: 8.681576779172666
Epoch 3422/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.39648728966712954
Epoch: 3422, Batch Gradient Norm: 7.9599291990963605
Epoch: 3422, Batch Gradient Norm after: 7.9599291990963605
Epoch 3423/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.3977601706981659
Epoch: 3423, Batch Gradient Norm: 8.430034856931156
Epoch: 3423, Batch Gradient Norm after: 8.430034856931156
Epoch 3424/10000, Prediction Accuracy = 61.878%, Loss = 0.3979856908321381
Epoch: 3424, Batch Gradient Norm: 7.993207443436957
Epoch: 3424, Batch Gradient Norm after: 7.993207443436957
Epoch 3425/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.3978211939334869
Epoch: 3425, Batch Gradient Norm: 10.645766484118031
Epoch: 3425, Batch Gradient Norm after: 10.645766484118031
Epoch 3426/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.3992388308048248
Epoch: 3426, Batch Gradient Norm: 10.270104687887068
Epoch: 3426, Batch Gradient Norm after: 10.270104687887068
Epoch 3427/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.39898295402526857
Epoch: 3427, Batch Gradient Norm: 10.9228208579645
Epoch: 3427, Batch Gradient Norm after: 10.9228208579645
Epoch 3428/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4020296573638916
Epoch: 3428, Batch Gradient Norm: 11.184507964710642
Epoch: 3428, Batch Gradient Norm after: 11.184507964710642
Epoch 3429/10000, Prediction Accuracy = 62.128%, Loss = 0.40040774941444396
Epoch: 3429, Batch Gradient Norm: 11.604238633147261
Epoch: 3429, Batch Gradient Norm after: 11.604238633147261
Epoch 3430/10000, Prediction Accuracy = 62.0%, Loss = 0.40150185227394103
Epoch: 3430, Batch Gradient Norm: 12.21087781605168
Epoch: 3430, Batch Gradient Norm after: 12.21087781605168
Epoch 3431/10000, Prediction Accuracy = 62.088%, Loss = 0.4030728280544281
Epoch: 3431, Batch Gradient Norm: 12.084540163314557
Epoch: 3431, Batch Gradient Norm after: 12.084540163314557
Epoch 3432/10000, Prediction Accuracy = 61.98%, Loss = 0.40245004296302794
Epoch: 3432, Batch Gradient Norm: 12.05236420376762
Epoch: 3432, Batch Gradient Norm after: 12.05236420376762
Epoch 3433/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.40392786264419556
Epoch: 3433, Batch Gradient Norm: 10.419947135643042
Epoch: 3433, Batch Gradient Norm after: 10.419947135643042
Epoch 3434/10000, Prediction Accuracy = 62.004%, Loss = 0.39955838918685915
Epoch: 3434, Batch Gradient Norm: 12.628541858898146
Epoch: 3434, Batch Gradient Norm after: 12.628541858898146
Epoch 3435/10000, Prediction Accuracy = 62.05999999999999%, Loss = 0.4023315072059631
Epoch: 3435, Batch Gradient Norm: 11.215736352351701
Epoch: 3435, Batch Gradient Norm after: 11.215736352351701
Epoch 3436/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.3989266693592072
Epoch: 3436, Batch Gradient Norm: 11.734420741218994
Epoch: 3436, Batch Gradient Norm after: 11.734420741218994
Epoch 3437/10000, Prediction Accuracy = 62.08%, Loss = 0.4005048930644989
Epoch: 3437, Batch Gradient Norm: 12.613658155971594
Epoch: 3437, Batch Gradient Norm after: 12.613658155971594
Epoch 3438/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.4021738350391388
Epoch: 3438, Batch Gradient Norm: 12.296405236919231
Epoch: 3438, Batch Gradient Norm after: 12.296405236919231
Epoch 3439/10000, Prediction Accuracy = 62.03399999999999%, Loss = 0.40264315605163575
Epoch: 3439, Batch Gradient Norm: 15.105224667699128
Epoch: 3439, Batch Gradient Norm after: 15.105224667699128
Epoch 3440/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.40648903250694274
Epoch: 3440, Batch Gradient Norm: 12.713005725586
Epoch: 3440, Batch Gradient Norm after: 12.713005725586
Epoch 3441/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.40558457374572754
Epoch: 3441, Batch Gradient Norm: 13.00988074023784
Epoch: 3441, Batch Gradient Norm after: 13.00988074023784
Epoch 3442/10000, Prediction Accuracy = 62.074%, Loss = 0.40179545283317564
Epoch: 3442, Batch Gradient Norm: 14.458047639179796
Epoch: 3442, Batch Gradient Norm after: 14.458047639179796
Epoch 3443/10000, Prediction Accuracy = 62.088%, Loss = 0.4048793375492096
Epoch: 3443, Batch Gradient Norm: 13.35401228272742
Epoch: 3443, Batch Gradient Norm after: 13.35401228272742
Epoch 3444/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.40545372366905214
Epoch: 3444, Batch Gradient Norm: 15.437608355380307
Epoch: 3444, Batch Gradient Norm after: 15.437608355380307
Epoch 3445/10000, Prediction Accuracy = 61.959999999999994%, Loss = 0.4064304769039154
Epoch: 3445, Batch Gradient Norm: 11.492273840171917
Epoch: 3445, Batch Gradient Norm after: 11.492273840171917
Epoch 3446/10000, Prediction Accuracy = 62.074%, Loss = 0.40234861969947816
Epoch: 3446, Batch Gradient Norm: 12.403082935769632
Epoch: 3446, Batch Gradient Norm after: 12.403082935769632
Epoch 3447/10000, Prediction Accuracy = 62.128%, Loss = 0.4032041788101196
Epoch: 3447, Batch Gradient Norm: 14.502155589677043
Epoch: 3447, Batch Gradient Norm after: 14.502155589677043
Epoch 3448/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.40516930222511294
Epoch: 3448, Batch Gradient Norm: 13.655590531328865
Epoch: 3448, Batch Gradient Norm after: 13.655590531328865
Epoch 3449/10000, Prediction Accuracy = 62.077999999999996%, Loss = 0.4031986117362976
Epoch: 3449, Batch Gradient Norm: 16.349292934778596
Epoch: 3449, Batch Gradient Norm after: 16.349292934778596
Epoch 3450/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.408074551820755
Epoch: 3450, Batch Gradient Norm: 16.312962334071557
Epoch: 3450, Batch Gradient Norm after: 16.312962334071557
Epoch 3451/10000, Prediction Accuracy = 62.05%, Loss = 0.40669944882392883
Epoch: 3451, Batch Gradient Norm: 13.597930981282921
Epoch: 3451, Batch Gradient Norm after: 13.597930981282921
Epoch 3452/10000, Prediction Accuracy = 62.02%, Loss = 0.4025868594646454
Epoch: 3452, Batch Gradient Norm: 13.771461436731288
Epoch: 3452, Batch Gradient Norm after: 13.771461436731288
Epoch 3453/10000, Prediction Accuracy = 61.972%, Loss = 0.40540567636489866
Epoch: 3453, Batch Gradient Norm: 10.858676143377188
Epoch: 3453, Batch Gradient Norm after: 10.858676143377188
Epoch 3454/10000, Prediction Accuracy = 62.01400000000001%, Loss = 0.401592618227005
Epoch: 3454, Batch Gradient Norm: 11.840624769175816
Epoch: 3454, Batch Gradient Norm after: 11.840624769175816
Epoch 3455/10000, Prediction Accuracy = 62.03399999999999%, Loss = 0.402083021402359
Epoch: 3455, Batch Gradient Norm: 12.819374529135686
Epoch: 3455, Batch Gradient Norm after: 12.819374529135686
Epoch 3456/10000, Prediction Accuracy = 62.124%, Loss = 0.4033572018146515
Epoch: 3456, Batch Gradient Norm: 12.58733757092739
Epoch: 3456, Batch Gradient Norm after: 12.58733757092739
Epoch 3457/10000, Prediction Accuracy = 61.984%, Loss = 0.4011236608028412
Epoch: 3457, Batch Gradient Norm: 8.781488481905932
Epoch: 3457, Batch Gradient Norm after: 8.781488481905932
Epoch 3458/10000, Prediction Accuracy = 62.064%, Loss = 0.39554824829101565
Epoch: 3458, Batch Gradient Norm: 8.094476053421328
Epoch: 3458, Batch Gradient Norm after: 8.094476053421328
Epoch 3459/10000, Prediction Accuracy = 62.08200000000001%, Loss = 0.39548534750938413
Epoch: 3459, Batch Gradient Norm: 10.561017571950467
Epoch: 3459, Batch Gradient Norm after: 10.561017571950467
Epoch 3460/10000, Prediction Accuracy = 61.958000000000006%, Loss = 0.3973921716213226
Epoch: 3460, Batch Gradient Norm: 8.860201675594444
Epoch: 3460, Batch Gradient Norm after: 8.860201675594444
Epoch 3461/10000, Prediction Accuracy = 62.086%, Loss = 0.3989067256450653
Epoch: 3461, Batch Gradient Norm: 11.527616094349726
Epoch: 3461, Batch Gradient Norm after: 11.527616094349726
Epoch 3462/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.39893828630447387
Epoch: 3462, Batch Gradient Norm: 11.27088406892132
Epoch: 3462, Batch Gradient Norm after: 11.27088406892132
Epoch 3463/10000, Prediction Accuracy = 62.056%, Loss = 0.4016632378101349
Epoch: 3463, Batch Gradient Norm: 11.314458852969352
Epoch: 3463, Batch Gradient Norm after: 11.314458852969352
Epoch 3464/10000, Prediction Accuracy = 62.06600000000001%, Loss = 0.40079185962677
Epoch: 3464, Batch Gradient Norm: 9.54829323384849
Epoch: 3464, Batch Gradient Norm after: 9.54829323384849
Epoch 3465/10000, Prediction Accuracy = 61.995999999999995%, Loss = 0.39675849080085757
Epoch: 3465, Batch Gradient Norm: 10.536615403239052
Epoch: 3465, Batch Gradient Norm after: 10.536615403239052
Epoch 3466/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.39703804850578306
Epoch: 3466, Batch Gradient Norm: 9.813978360296641
Epoch: 3466, Batch Gradient Norm after: 9.813978360296641
Epoch 3467/10000, Prediction Accuracy = 62.008%, Loss = 0.40243707299232484
Epoch: 3467, Batch Gradient Norm: 9.248593613766065
Epoch: 3467, Batch Gradient Norm after: 9.248593613766065
Epoch 3468/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.39707004427909853
Epoch: 3468, Batch Gradient Norm: 11.001082980451214
Epoch: 3468, Batch Gradient Norm after: 11.001082980451214
Epoch 3469/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.399093759059906
Epoch: 3469, Batch Gradient Norm: 12.320052618401288
Epoch: 3469, Batch Gradient Norm after: 12.320052618401288
Epoch 3470/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.40217960476875303
Epoch: 3470, Batch Gradient Norm: 11.121272630839814
Epoch: 3470, Batch Gradient Norm after: 11.121272630839814
Epoch 3471/10000, Prediction Accuracy = 61.988%, Loss = 0.40016459822654726
Epoch: 3471, Batch Gradient Norm: 15.321422062290026
Epoch: 3471, Batch Gradient Norm after: 15.321422062290026
Epoch 3472/10000, Prediction Accuracy = 61.907999999999994%, Loss = 0.4023595035076141
Epoch: 3472, Batch Gradient Norm: 13.456802199241563
Epoch: 3472, Batch Gradient Norm after: 13.456802199241563
Epoch 3473/10000, Prediction Accuracy = 61.956%, Loss = 0.4012010872364044
Epoch: 3473, Batch Gradient Norm: 10.959087707208061
Epoch: 3473, Batch Gradient Norm after: 10.959087707208061
Epoch 3474/10000, Prediction Accuracy = 62.048%, Loss = 0.39789520502090453
Epoch: 3474, Batch Gradient Norm: 8.800513928702252
Epoch: 3474, Batch Gradient Norm after: 8.800513928702252
Epoch 3475/10000, Prediction Accuracy = 62.064%, Loss = 0.39608052372932434
Epoch: 3475, Batch Gradient Norm: 9.859848144313416
Epoch: 3475, Batch Gradient Norm after: 9.859848144313416
Epoch 3476/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.3978171765804291
Epoch: 3476, Batch Gradient Norm: 12.273348679785718
Epoch: 3476, Batch Gradient Norm after: 12.273348679785718
Epoch 3477/10000, Prediction Accuracy = 62.102%, Loss = 0.39991087913513185
Epoch: 3477, Batch Gradient Norm: 11.096545706996874
Epoch: 3477, Batch Gradient Norm after: 11.096545706996874
Epoch 3478/10000, Prediction Accuracy = 62.11%, Loss = 0.400580632686615
Epoch: 3478, Batch Gradient Norm: 11.62545733335799
Epoch: 3478, Batch Gradient Norm after: 11.62545733335799
Epoch 3479/10000, Prediction Accuracy = 62.072%, Loss = 0.40110703110694884
Epoch: 3479, Batch Gradient Norm: 11.460658939018536
Epoch: 3479, Batch Gradient Norm after: 11.460658939018536
Epoch 3480/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.39838985204696653
Epoch: 3480, Batch Gradient Norm: 12.190520420699904
Epoch: 3480, Batch Gradient Norm after: 12.190520420699904
Epoch 3481/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40229002237319944
Epoch: 3481, Batch Gradient Norm: 12.590418244372886
Epoch: 3481, Batch Gradient Norm after: 12.590418244372886
Epoch 3482/10000, Prediction Accuracy = 62.019999999999996%, Loss = 0.4016939878463745
Epoch: 3482, Batch Gradient Norm: 13.455470493927573
Epoch: 3482, Batch Gradient Norm after: 13.455470493927573
Epoch 3483/10000, Prediction Accuracy = 62.092%, Loss = 0.40139045715332033
Epoch: 3483, Batch Gradient Norm: 15.844965006065056
Epoch: 3483, Batch Gradient Norm after: 15.844965006065056
Epoch 3484/10000, Prediction Accuracy = 62.05800000000001%, Loss = 0.4047731578350067
Epoch: 3484, Batch Gradient Norm: 12.71065215341515
Epoch: 3484, Batch Gradient Norm after: 12.71065215341515
Epoch 3485/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.40169132351875303
Epoch: 3485, Batch Gradient Norm: 11.07544117932465
Epoch: 3485, Batch Gradient Norm after: 11.07544117932465
Epoch 3486/10000, Prediction Accuracy = 62.102%, Loss = 0.3978513479232788
Epoch: 3486, Batch Gradient Norm: 10.627361341805242
Epoch: 3486, Batch Gradient Norm after: 10.627361341805242
Epoch 3487/10000, Prediction Accuracy = 62.122%, Loss = 0.39780173897743226
Epoch: 3487, Batch Gradient Norm: 12.282801709892787
Epoch: 3487, Batch Gradient Norm after: 12.282801709892787
Epoch 3488/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.400688624382019
Epoch: 3488, Batch Gradient Norm: 11.111936959809315
Epoch: 3488, Batch Gradient Norm after: 11.111936959809315
Epoch 3489/10000, Prediction Accuracy = 62.077999999999996%, Loss = 0.3988906443119049
Epoch: 3489, Batch Gradient Norm: 9.126572912033255
Epoch: 3489, Batch Gradient Norm after: 9.126572912033255
Epoch 3490/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.3965125441551208
Epoch: 3490, Batch Gradient Norm: 12.086884415587624
Epoch: 3490, Batch Gradient Norm after: 12.086884415587624
Epoch 3491/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.4001741588115692
Epoch: 3491, Batch Gradient Norm: 9.814219576349803
Epoch: 3491, Batch Gradient Norm after: 9.814219576349803
Epoch 3492/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.39703001976013186
Epoch: 3492, Batch Gradient Norm: 10.428889488949824
Epoch: 3492, Batch Gradient Norm after: 10.428889488949824
Epoch 3493/10000, Prediction Accuracy = 61.996%, Loss = 0.3964842796325684
Epoch: 3493, Batch Gradient Norm: 10.040387037629284
Epoch: 3493, Batch Gradient Norm after: 10.040387037629284
Epoch 3494/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.39711910486221313
Epoch: 3494, Batch Gradient Norm: 10.733361176984745
Epoch: 3494, Batch Gradient Norm after: 10.733361176984745
Epoch 3495/10000, Prediction Accuracy = 62.062%, Loss = 0.3961299121379852
Epoch: 3495, Batch Gradient Norm: 14.397521302964979
Epoch: 3495, Batch Gradient Norm after: 14.397521302964979
Epoch 3496/10000, Prediction Accuracy = 62.06999999999999%, Loss = 0.40212632417678834
Epoch: 3496, Batch Gradient Norm: 11.5947679869082
Epoch: 3496, Batch Gradient Norm after: 11.5947679869082
Epoch 3497/10000, Prediction Accuracy = 62.166%, Loss = 0.4007267117500305
Epoch: 3497, Batch Gradient Norm: 10.119154043796552
Epoch: 3497, Batch Gradient Norm after: 10.119154043796552
Epoch 3498/10000, Prediction Accuracy = 62.012%, Loss = 0.39878369569778443
Epoch: 3498, Batch Gradient Norm: 10.928248483682168
Epoch: 3498, Batch Gradient Norm after: 10.928248483682168
Epoch 3499/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.3979021072387695
Epoch: 3499, Batch Gradient Norm: 10.786186374683506
Epoch: 3499, Batch Gradient Norm after: 10.786186374683506
Epoch 3500/10000, Prediction Accuracy = 62.128%, Loss = 0.3974026381969452
Epoch: 3500, Batch Gradient Norm: 9.737408569878951
Epoch: 3500, Batch Gradient Norm after: 9.737408569878951
Epoch 3501/10000, Prediction Accuracy = 62.138%, Loss = 0.39609606862068175
Epoch: 3501, Batch Gradient Norm: 12.28324426403436
Epoch: 3501, Batch Gradient Norm after: 12.28324426403436
Epoch 3502/10000, Prediction Accuracy = 62.06%, Loss = 0.39928961992263795
Epoch: 3502, Batch Gradient Norm: 12.849170555760963
Epoch: 3502, Batch Gradient Norm after: 12.849170555760963
Epoch 3503/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.3998257339000702
Epoch: 3503, Batch Gradient Norm: 10.924631431853006
Epoch: 3503, Batch Gradient Norm after: 10.924631431853006
Epoch 3504/10000, Prediction Accuracy = 62.08%, Loss = 0.39695162177085874
Epoch: 3504, Batch Gradient Norm: 12.130518408076055
Epoch: 3504, Batch Gradient Norm after: 12.130518408076055
Epoch 3505/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.40064107775688174
Epoch: 3505, Batch Gradient Norm: 13.913501559082833
Epoch: 3505, Batch Gradient Norm after: 13.913501559082833
Epoch 3506/10000, Prediction Accuracy = 62.1%, Loss = 0.4018032133579254
Epoch: 3506, Batch Gradient Norm: 14.324892105092587
Epoch: 3506, Batch Gradient Norm after: 14.324892105092587
Epoch 3507/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.40273823738098147
Epoch: 3507, Batch Gradient Norm: 12.718572786048139
Epoch: 3507, Batch Gradient Norm after: 12.718572786048139
Epoch 3508/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.39959522485733034
Epoch: 3508, Batch Gradient Norm: 13.025815227971778
Epoch: 3508, Batch Gradient Norm after: 13.025815227971778
Epoch 3509/10000, Prediction Accuracy = 62.032%, Loss = 0.4019729971885681
Epoch: 3509, Batch Gradient Norm: 13.32789182582742
Epoch: 3509, Batch Gradient Norm after: 13.32789182582742
Epoch 3510/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.40046237111091615
Epoch: 3510, Batch Gradient Norm: 11.7588713850698
Epoch: 3510, Batch Gradient Norm after: 11.7588713850698
Epoch 3511/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.3995294988155365
Epoch: 3511, Batch Gradient Norm: 12.075787298493271
Epoch: 3511, Batch Gradient Norm after: 12.075787298493271
Epoch 3512/10000, Prediction Accuracy = 62.088%, Loss = 0.39799435138702394
Epoch: 3512, Batch Gradient Norm: 11.20827167126709
Epoch: 3512, Batch Gradient Norm after: 11.20827167126709
Epoch 3513/10000, Prediction Accuracy = 62.06%, Loss = 0.3959228217601776
Epoch: 3513, Batch Gradient Norm: 11.944996772279909
Epoch: 3513, Batch Gradient Norm after: 11.944996772279909
Epoch 3514/10000, Prediction Accuracy = 61.968%, Loss = 0.40070409178733823
Epoch: 3514, Batch Gradient Norm: 9.31877642128774
Epoch: 3514, Batch Gradient Norm after: 9.31877642128774
Epoch 3515/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.39536519050598146
Epoch: 3515, Batch Gradient Norm: 8.877811104140985
Epoch: 3515, Batch Gradient Norm after: 8.877811104140985
Epoch 3516/10000, Prediction Accuracy = 61.972%, Loss = 0.39383533000946047
Epoch: 3516, Batch Gradient Norm: 12.27584994741795
Epoch: 3516, Batch Gradient Norm after: 12.27584994741795
Epoch 3517/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.40140295028686523
Epoch: 3517, Batch Gradient Norm: 12.719651152529092
Epoch: 3517, Batch Gradient Norm after: 12.719651152529092
Epoch 3518/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.3983511686325073
Epoch: 3518, Batch Gradient Norm: 11.602164832939751
Epoch: 3518, Batch Gradient Norm after: 11.602164832939751
Epoch 3519/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.3982043623924255
Epoch: 3519, Batch Gradient Norm: 12.225015647565613
Epoch: 3519, Batch Gradient Norm after: 12.225015647565613
Epoch 3520/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.39883547425270083
Epoch: 3520, Batch Gradient Norm: 11.433169015699567
Epoch: 3520, Batch Gradient Norm after: 11.433169015699567
Epoch 3521/10000, Prediction Accuracy = 62.112%, Loss = 0.3967763245105743
Epoch: 3521, Batch Gradient Norm: 12.243912545068424
Epoch: 3521, Batch Gradient Norm after: 12.243912545068424
Epoch 3522/10000, Prediction Accuracy = 62.114%, Loss = 0.40109161138534544
Epoch: 3522, Batch Gradient Norm: 11.172041793411788
Epoch: 3522, Batch Gradient Norm after: 11.172041793411788
Epoch 3523/10000, Prediction Accuracy = 62.036%, Loss = 0.3988657295703888
Epoch: 3523, Batch Gradient Norm: 10.30565318453692
Epoch: 3523, Batch Gradient Norm after: 10.30565318453692
Epoch 3524/10000, Prediction Accuracy = 62.074%, Loss = 0.39674839973449705
Epoch: 3524, Batch Gradient Norm: 8.196948791692238
Epoch: 3524, Batch Gradient Norm after: 8.196948791692238
Epoch 3525/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.39377424120903015
Epoch: 3525, Batch Gradient Norm: 8.791094166745541
Epoch: 3525, Batch Gradient Norm after: 8.791094166745541
Epoch 3526/10000, Prediction Accuracy = 62.074%, Loss = 0.39316150546073914
Epoch: 3526, Batch Gradient Norm: 12.44544406108754
Epoch: 3526, Batch Gradient Norm after: 12.44544406108754
Epoch 3527/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.3977371871471405
Epoch: 3527, Batch Gradient Norm: 9.873818043286603
Epoch: 3527, Batch Gradient Norm after: 9.873818043286603
Epoch 3528/10000, Prediction Accuracy = 62.06%, Loss = 0.39711514115333557
Epoch: 3528, Batch Gradient Norm: 10.421862846435449
Epoch: 3528, Batch Gradient Norm after: 10.421862846435449
Epoch 3529/10000, Prediction Accuracy = 62.126%, Loss = 0.3950496971607208
Epoch: 3529, Batch Gradient Norm: 11.75976160196413
Epoch: 3529, Batch Gradient Norm after: 11.75976160196413
Epoch 3530/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.39723028540611266
Epoch: 3530, Batch Gradient Norm: 10.77301264231455
Epoch: 3530, Batch Gradient Norm after: 10.77301264231455
Epoch 3531/10000, Prediction Accuracy = 62.086%, Loss = 0.3964617669582367
Epoch: 3531, Batch Gradient Norm: 14.858221080884206
Epoch: 3531, Batch Gradient Norm after: 14.858221080884206
Epoch 3532/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4008338630199432
Epoch: 3532, Batch Gradient Norm: 12.335403791957544
Epoch: 3532, Batch Gradient Norm after: 12.335403791957544
Epoch 3533/10000, Prediction Accuracy = 62.04%, Loss = 0.40023391842842104
Epoch: 3533, Batch Gradient Norm: 13.755748837042042
Epoch: 3533, Batch Gradient Norm after: 13.755748837042042
Epoch 3534/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.4013229787349701
Epoch: 3534, Batch Gradient Norm: 14.114311404866204
Epoch: 3534, Batch Gradient Norm after: 14.114311404866204
Epoch 3535/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4015828788280487
Epoch: 3535, Batch Gradient Norm: 13.889642439881227
Epoch: 3535, Batch Gradient Norm after: 13.889642439881227
Epoch 3536/10000, Prediction Accuracy = 61.976%, Loss = 0.40160155296325684
Epoch: 3536, Batch Gradient Norm: 14.78972132239839
Epoch: 3536, Batch Gradient Norm after: 14.78972132239839
Epoch 3537/10000, Prediction Accuracy = 62.05%, Loss = 0.40530177354812624
Epoch: 3537, Batch Gradient Norm: 13.51789166223499
Epoch: 3537, Batch Gradient Norm after: 13.51789166223499
Epoch 3538/10000, Prediction Accuracy = 62.1%, Loss = 0.4003734469413757
Epoch: 3538, Batch Gradient Norm: 11.353361253188229
Epoch: 3538, Batch Gradient Norm after: 11.353361253188229
Epoch 3539/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.3977549850940704
Epoch: 3539, Batch Gradient Norm: 14.819807131210919
Epoch: 3539, Batch Gradient Norm after: 14.819807131210919
Epoch 3540/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.4017708361148834
Epoch: 3540, Batch Gradient Norm: 10.603715160463077
Epoch: 3540, Batch Gradient Norm after: 10.603715160463077
Epoch 3541/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.39715748429298403
Epoch: 3541, Batch Gradient Norm: 10.799162052872804
Epoch: 3541, Batch Gradient Norm after: 10.799162052872804
Epoch 3542/10000, Prediction Accuracy = 62.012%, Loss = 0.3974094033241272
Epoch: 3542, Batch Gradient Norm: 13.80728558181777
Epoch: 3542, Batch Gradient Norm after: 13.80728558181777
Epoch 3543/10000, Prediction Accuracy = 62.0%, Loss = 0.4017096281051636
Epoch: 3543, Batch Gradient Norm: 15.907835752035647
Epoch: 3543, Batch Gradient Norm after: 15.907835752035647
Epoch 3544/10000, Prediction Accuracy = 62.056%, Loss = 0.40236289501190187
Epoch: 3544, Batch Gradient Norm: 18.098519379760454
Epoch: 3544, Batch Gradient Norm after: 18.098519379760454
Epoch 3545/10000, Prediction Accuracy = 62.14000000000001%, Loss = 0.40550132989883425
Epoch: 3545, Batch Gradient Norm: 19.99696056033047
Epoch: 3545, Batch Gradient Norm after: 19.634646871186188
Epoch 3546/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4130495488643646
Epoch: 3546, Batch Gradient Norm: 19.607118370783265
Epoch: 3546, Batch Gradient Norm after: 19.438555126124225
Epoch 3547/10000, Prediction Accuracy = 62.102%, Loss = 0.41049366593360903
Epoch: 3547, Batch Gradient Norm: 17.598097875935363
Epoch: 3547, Batch Gradient Norm after: 17.598097875935363
Epoch 3548/10000, Prediction Accuracy = 62.016000000000005%, Loss = 0.4086418867111206
Epoch: 3548, Batch Gradient Norm: 15.801990366105406
Epoch: 3548, Batch Gradient Norm after: 15.801990366105406
Epoch 3549/10000, Prediction Accuracy = 62.164%, Loss = 0.40329061150550843
Epoch: 3549, Batch Gradient Norm: 13.31423442838863
Epoch: 3549, Batch Gradient Norm after: 13.31423442838863
Epoch 3550/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4014574527740479
Epoch: 3550, Batch Gradient Norm: 13.813288892362108
Epoch: 3550, Batch Gradient Norm after: 13.813288892362108
Epoch 3551/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.40046167373657227
Epoch: 3551, Batch Gradient Norm: 11.548683373821401
Epoch: 3551, Batch Gradient Norm after: 11.548683373821401
Epoch 3552/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.39826284646987914
Epoch: 3552, Batch Gradient Norm: 9.695021307113445
Epoch: 3552, Batch Gradient Norm after: 9.695021307113445
Epoch 3553/10000, Prediction Accuracy = 62.102%, Loss = 0.3943446397781372
Epoch: 3553, Batch Gradient Norm: 9.836103612205248
Epoch: 3553, Batch Gradient Norm after: 9.836103612205248
Epoch 3554/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.39610496163368225
Epoch: 3554, Batch Gradient Norm: 12.499061892876632
Epoch: 3554, Batch Gradient Norm after: 12.499061892876632
Epoch 3555/10000, Prediction Accuracy = 62.148%, Loss = 0.39744300246238706
Epoch: 3555, Batch Gradient Norm: 13.592162886012991
Epoch: 3555, Batch Gradient Norm after: 13.592162886012991
Epoch 3556/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.39997406005859376
Epoch: 3556, Batch Gradient Norm: 13.645853419168025
Epoch: 3556, Batch Gradient Norm after: 13.645853419168025
Epoch 3557/10000, Prediction Accuracy = 62.084%, Loss = 0.39944940209388735
Epoch: 3557, Batch Gradient Norm: 8.937737948844784
Epoch: 3557, Batch Gradient Norm after: 8.937737948844784
Epoch 3558/10000, Prediction Accuracy = 62.198%, Loss = 0.3930976092815399
Epoch: 3558, Batch Gradient Norm: 12.195271102077742
Epoch: 3558, Batch Gradient Norm after: 12.195271102077742
Epoch 3559/10000, Prediction Accuracy = 62.072%, Loss = 0.3971846878528595
Epoch: 3559, Batch Gradient Norm: 14.69958681438118
Epoch: 3559, Batch Gradient Norm after: 14.69958681438118
Epoch 3560/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.40028690099716185
Epoch: 3560, Batch Gradient Norm: 13.991454282541316
Epoch: 3560, Batch Gradient Norm after: 13.991454282541316
Epoch 3561/10000, Prediction Accuracy = 62.064%, Loss = 0.4010878145694733
Epoch: 3561, Batch Gradient Norm: 14.183908247723686
Epoch: 3561, Batch Gradient Norm after: 14.183908247723686
Epoch 3562/10000, Prediction Accuracy = 62.092%, Loss = 0.4002086341381073
Epoch: 3562, Batch Gradient Norm: 14.656924576998698
Epoch: 3562, Batch Gradient Norm after: 14.656924576998698
Epoch 3563/10000, Prediction Accuracy = 62.008%, Loss = 0.3998497068881989
Epoch: 3563, Batch Gradient Norm: 16.72941890654623
Epoch: 3563, Batch Gradient Norm after: 16.153096577237513
Epoch 3564/10000, Prediction Accuracy = 62.124%, Loss = 0.40417303442955016
Epoch: 3564, Batch Gradient Norm: 13.325791418826212
Epoch: 3564, Batch Gradient Norm after: 13.325791418826212
Epoch 3565/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.40331098437309265
Epoch: 3565, Batch Gradient Norm: 13.406999331595316
Epoch: 3565, Batch Gradient Norm after: 13.406999331595316
Epoch 3566/10000, Prediction Accuracy = 62.126%, Loss = 0.39923626780509947
Epoch: 3566, Batch Gradient Norm: 11.976359910110775
Epoch: 3566, Batch Gradient Norm after: 11.976359910110775
Epoch 3567/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.3974481463432312
Epoch: 3567, Batch Gradient Norm: 14.028118228447015
Epoch: 3567, Batch Gradient Norm after: 14.028118228447015
Epoch 3568/10000, Prediction Accuracy = 62.102%, Loss = 0.400211238861084
Epoch: 3568, Batch Gradient Norm: 10.326056405070505
Epoch: 3568, Batch Gradient Norm after: 10.326056405070505
Epoch 3569/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.3931733131408691
Epoch: 3569, Batch Gradient Norm: 8.944392928880237
Epoch: 3569, Batch Gradient Norm after: 8.944392928880237
Epoch 3570/10000, Prediction Accuracy = 62.214%, Loss = 0.39482455849647524
Epoch: 3570, Batch Gradient Norm: 10.738500280803764
Epoch: 3570, Batch Gradient Norm after: 10.738500280803764
Epoch 3571/10000, Prediction Accuracy = 62.136%, Loss = 0.39450193047523496
Epoch: 3571, Batch Gradient Norm: 12.175801182013975
Epoch: 3571, Batch Gradient Norm after: 12.175801182013975
Epoch 3572/10000, Prediction Accuracy = 62.13399999999999%, Loss = 0.3973137378692627
Epoch: 3572, Batch Gradient Norm: 14.618908959812579
Epoch: 3572, Batch Gradient Norm after: 14.618908959812579
Epoch 3573/10000, Prediction Accuracy = 62.077999999999996%, Loss = 0.40251938700675965
Epoch: 3573, Batch Gradient Norm: 13.169925585836332
Epoch: 3573, Batch Gradient Norm after: 13.169925585836332
Epoch 3574/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.39897695779800413
Epoch: 3574, Batch Gradient Norm: 15.555748025065927
Epoch: 3574, Batch Gradient Norm after: 15.555748025065927
Epoch 3575/10000, Prediction Accuracy = 62.15%, Loss = 0.4017042458057404
Epoch: 3575, Batch Gradient Norm: 12.35668581769143
Epoch: 3575, Batch Gradient Norm after: 12.35668581769143
Epoch 3576/10000, Prediction Accuracy = 62.102%, Loss = 0.3969018578529358
Epoch: 3576, Batch Gradient Norm: 11.198263857270309
Epoch: 3576, Batch Gradient Norm after: 11.198263857270309
Epoch 3577/10000, Prediction Accuracy = 62.146%, Loss = 0.39486839175224303
Epoch: 3577, Batch Gradient Norm: 9.595369492129626
Epoch: 3577, Batch Gradient Norm after: 9.595369492129626
Epoch 3578/10000, Prediction Accuracy = 62.068%, Loss = 0.394548225402832
Epoch: 3578, Batch Gradient Norm: 11.223826991233276
Epoch: 3578, Batch Gradient Norm after: 11.223826991233276
Epoch 3579/10000, Prediction Accuracy = 62.234%, Loss = 0.39490711092948916
Epoch: 3579, Batch Gradient Norm: 9.870016694199442
Epoch: 3579, Batch Gradient Norm after: 9.870016694199442
Epoch 3580/10000, Prediction Accuracy = 62.188%, Loss = 0.3931137561798096
Epoch: 3580, Batch Gradient Norm: 10.087881985527655
Epoch: 3580, Batch Gradient Norm after: 10.087881985527655
Epoch 3581/10000, Prediction Accuracy = 62.102%, Loss = 0.39445520639419557
Epoch: 3581, Batch Gradient Norm: 11.759991578071936
Epoch: 3581, Batch Gradient Norm after: 11.759991578071936
Epoch 3582/10000, Prediction Accuracy = 62.038%, Loss = 0.3967824041843414
Epoch: 3582, Batch Gradient Norm: 11.818739661372067
Epoch: 3582, Batch Gradient Norm after: 11.818739661372067
Epoch 3583/10000, Prediction Accuracy = 62.10999999999999%, Loss = 0.3961461067199707
Epoch: 3583, Batch Gradient Norm: 8.787912121670796
Epoch: 3583, Batch Gradient Norm after: 8.787912121670796
Epoch 3584/10000, Prediction Accuracy = 62.234%, Loss = 0.39197460412979124
Epoch: 3584, Batch Gradient Norm: 8.88215488986741
Epoch: 3584, Batch Gradient Norm after: 8.88215488986741
Epoch 3585/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.3942786633968353
Epoch: 3585, Batch Gradient Norm: 9.44871946921323
Epoch: 3585, Batch Gradient Norm after: 9.44871946921323
Epoch 3586/10000, Prediction Accuracy = 62.286%, Loss = 0.39444954991340636
Epoch: 3586, Batch Gradient Norm: 12.152464996669922
Epoch: 3586, Batch Gradient Norm after: 12.152464996669922
Epoch 3587/10000, Prediction Accuracy = 62.178%, Loss = 0.394910728931427
Epoch: 3587, Batch Gradient Norm: 14.049193379956938
Epoch: 3587, Batch Gradient Norm after: 14.049193379956938
Epoch 3588/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.3988936245441437
Epoch: 3588, Batch Gradient Norm: 16.032401731344606
Epoch: 3588, Batch Gradient Norm after: 16.032401731344606
Epoch 3589/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4029033362865448
Epoch: 3589, Batch Gradient Norm: 16.294526980236792
Epoch: 3589, Batch Gradient Norm after: 16.294526980236792
Epoch 3590/10000, Prediction Accuracy = 62.178%, Loss = 0.4030440926551819
Epoch: 3590, Batch Gradient Norm: 13.687855561509874
Epoch: 3590, Batch Gradient Norm after: 13.687855561509874
Epoch 3591/10000, Prediction Accuracy = 62.246%, Loss = 0.39928542375564574
Epoch: 3591, Batch Gradient Norm: 13.421522340745705
Epoch: 3591, Batch Gradient Norm after: 13.421522340745705
Epoch 3592/10000, Prediction Accuracy = 62.19%, Loss = 0.3976219415664673
Epoch: 3592, Batch Gradient Norm: 14.71336021734502
Epoch: 3592, Batch Gradient Norm after: 14.71336021734502
Epoch 3593/10000, Prediction Accuracy = 62.14200000000001%, Loss = 0.401605612039566
Epoch: 3593, Batch Gradient Norm: 9.530849732103059
Epoch: 3593, Batch Gradient Norm after: 9.530849732103059
Epoch 3594/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.3918842554092407
Epoch: 3594, Batch Gradient Norm: 9.49645590895904
Epoch: 3594, Batch Gradient Norm after: 9.49645590895904
Epoch 3595/10000, Prediction Accuracy = 62.076%, Loss = 0.3951002836227417
Epoch: 3595, Batch Gradient Norm: 10.519738546344449
Epoch: 3595, Batch Gradient Norm after: 10.519738546344449
Epoch 3596/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.3949303925037384
Epoch: 3596, Batch Gradient Norm: 10.83820622845631
Epoch: 3596, Batch Gradient Norm after: 10.83820622845631
Epoch 3597/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.3939541220664978
Epoch: 3597, Batch Gradient Norm: 10.380026041049952
Epoch: 3597, Batch Gradient Norm after: 10.380026041049952
Epoch 3598/10000, Prediction Accuracy = 62.122%, Loss = 0.39526782035827634
Epoch: 3598, Batch Gradient Norm: 9.795117251530753
Epoch: 3598, Batch Gradient Norm after: 9.795117251530753
Epoch 3599/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.3935096263885498
Epoch: 3599, Batch Gradient Norm: 9.114396803267123
Epoch: 3599, Batch Gradient Norm after: 9.114396803267123
Epoch 3600/10000, Prediction Accuracy = 62.08%, Loss = 0.3908024489879608
Epoch: 3600, Batch Gradient Norm: 10.441090245160972
Epoch: 3600, Batch Gradient Norm after: 10.441090245160972
Epoch 3601/10000, Prediction Accuracy = 62.134%, Loss = 0.39436370730400083
Epoch: 3601, Batch Gradient Norm: 9.1232818459586
Epoch: 3601, Batch Gradient Norm after: 9.1232818459586
Epoch 3602/10000, Prediction Accuracy = 62.15%, Loss = 0.3911606013774872
Epoch: 3602, Batch Gradient Norm: 9.371077164441557
Epoch: 3602, Batch Gradient Norm after: 9.371077164441557
Epoch 3603/10000, Prediction Accuracy = 62.164%, Loss = 0.39383029341697695
Epoch: 3603, Batch Gradient Norm: 10.215284257937306
Epoch: 3603, Batch Gradient Norm after: 10.215284257937306
Epoch 3604/10000, Prediction Accuracy = 62.278%, Loss = 0.39415987133979796
Epoch: 3604, Batch Gradient Norm: 11.961586652517125
Epoch: 3604, Batch Gradient Norm after: 11.961586652517125
Epoch 3605/10000, Prediction Accuracy = 62.186%, Loss = 0.39595096111297606
Epoch: 3605, Batch Gradient Norm: 13.674600099005648
Epoch: 3605, Batch Gradient Norm after: 13.674600099005648
Epoch 3606/10000, Prediction Accuracy = 62.15%, Loss = 0.4016581177711487
Epoch: 3606, Batch Gradient Norm: 16.45895906697621
Epoch: 3606, Batch Gradient Norm after: 16.10586683576766
Epoch 3607/10000, Prediction Accuracy = 62.174%, Loss = 0.4026048302650452
Epoch: 3607, Batch Gradient Norm: 15.289966928860883
Epoch: 3607, Batch Gradient Norm after: 15.289966928860883
Epoch 3608/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.402678519487381
Epoch: 3608, Batch Gradient Norm: 16.391135622077
Epoch: 3608, Batch Gradient Norm after: 16.391135622077
Epoch 3609/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.40240535140037537
Epoch: 3609, Batch Gradient Norm: 15.167530939867937
Epoch: 3609, Batch Gradient Norm after: 15.167530939867937
Epoch 3610/10000, Prediction Accuracy = 62.05%, Loss = 0.3998889088630676
Epoch: 3610, Batch Gradient Norm: 16.805802158176135
Epoch: 3610, Batch Gradient Norm after: 16.805802158176135
Epoch 3611/10000, Prediction Accuracy = 62.206%, Loss = 0.40549625754356383
Epoch: 3611, Batch Gradient Norm: 16.98316044334514
Epoch: 3611, Batch Gradient Norm after: 16.98316044334514
Epoch 3612/10000, Prediction Accuracy = 62.148%, Loss = 0.40261991024017335
Epoch: 3612, Batch Gradient Norm: 15.801029086589667
Epoch: 3612, Batch Gradient Norm after: 15.69598845033353
Epoch 3613/10000, Prediction Accuracy = 62.15999999999999%, Loss = 0.4019473671913147
Epoch: 3613, Batch Gradient Norm: 14.64398682006784
Epoch: 3613, Batch Gradient Norm after: 14.64398682006784
Epoch 3614/10000, Prediction Accuracy = 62.152%, Loss = 0.4003196358680725
Epoch: 3614, Batch Gradient Norm: 13.616647364072273
Epoch: 3614, Batch Gradient Norm after: 13.616647364072273
Epoch 3615/10000, Prediction Accuracy = 62.044%, Loss = 0.39793508052825927
Epoch: 3615, Batch Gradient Norm: 14.06111448877768
Epoch: 3615, Batch Gradient Norm after: 14.06111448877768
Epoch 3616/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.39828858971595765
Epoch: 3616, Batch Gradient Norm: 14.463082771086942
Epoch: 3616, Batch Gradient Norm after: 14.463082771086942
Epoch 3617/10000, Prediction Accuracy = 62.13000000000001%, Loss = 0.400403767824173
Epoch: 3617, Batch Gradient Norm: 13.218738422804092
Epoch: 3617, Batch Gradient Norm after: 13.218738422804092
Epoch 3618/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.3985781133174896
Epoch: 3618, Batch Gradient Norm: 11.45808470830302
Epoch: 3618, Batch Gradient Norm after: 11.45808470830302
Epoch 3619/10000, Prediction Accuracy = 62.04%, Loss = 0.39403821229934693
Epoch: 3619, Batch Gradient Norm: 13.286346752697545
Epoch: 3619, Batch Gradient Norm after: 13.286346752697545
Epoch 3620/10000, Prediction Accuracy = 62.092000000000006%, Loss = 0.3973114788532257
Epoch: 3620, Batch Gradient Norm: 10.624665047217826
Epoch: 3620, Batch Gradient Norm after: 10.624665047217826
Epoch 3621/10000, Prediction Accuracy = 62.134%, Loss = 0.395929342508316
Epoch: 3621, Batch Gradient Norm: 12.155248727930816
Epoch: 3621, Batch Gradient Norm after: 12.155248727930816
Epoch 3622/10000, Prediction Accuracy = 62.072%, Loss = 0.39550226330757143
Epoch: 3622, Batch Gradient Norm: 13.083459858470315
Epoch: 3622, Batch Gradient Norm after: 13.083459858470315
Epoch 3623/10000, Prediction Accuracy = 62.116%, Loss = 0.39659035205841064
Epoch: 3623, Batch Gradient Norm: 12.674731194505034
Epoch: 3623, Batch Gradient Norm after: 12.674731194505034
Epoch 3624/10000, Prediction Accuracy = 62.15%, Loss = 0.39605446457862853
Epoch: 3624, Batch Gradient Norm: 11.599482710957744
Epoch: 3624, Batch Gradient Norm after: 11.599482710957744
Epoch 3625/10000, Prediction Accuracy = 62.11600000000001%, Loss = 0.39567508101463317
Epoch: 3625, Batch Gradient Norm: 10.130577922479736
Epoch: 3625, Batch Gradient Norm after: 10.130577922479736
Epoch 3626/10000, Prediction Accuracy = 62.012%, Loss = 0.39330512285232544
Epoch: 3626, Batch Gradient Norm: 10.213886528232184
Epoch: 3626, Batch Gradient Norm after: 10.213886528232184
Epoch 3627/10000, Prediction Accuracy = 62.11%, Loss = 0.3920145809650421
Epoch: 3627, Batch Gradient Norm: 12.957858757653947
Epoch: 3627, Batch Gradient Norm after: 12.957858757653947
Epoch 3628/10000, Prediction Accuracy = 62.11%, Loss = 0.3959804177284241
Epoch: 3628, Batch Gradient Norm: 12.271249737169102
Epoch: 3628, Batch Gradient Norm after: 12.271249737169102
Epoch 3629/10000, Prediction Accuracy = 62.212%, Loss = 0.39454511404037473
Epoch: 3629, Batch Gradient Norm: 10.403818484833062
Epoch: 3629, Batch Gradient Norm after: 10.403818484833062
Epoch 3630/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.39354172348976135
Epoch: 3630, Batch Gradient Norm: 10.299834562332897
Epoch: 3630, Batch Gradient Norm after: 10.299834562332897
Epoch 3631/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.39216539859771726
Epoch: 3631, Batch Gradient Norm: 9.581958215771099
Epoch: 3631, Batch Gradient Norm after: 9.581958215771099
Epoch 3632/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.39117575287818906
Epoch: 3632, Batch Gradient Norm: 12.273893358320633
Epoch: 3632, Batch Gradient Norm after: 12.273893358320633
Epoch 3633/10000, Prediction Accuracy = 62.16400000000001%, Loss = 0.3943057119846344
Epoch: 3633, Batch Gradient Norm: 13.504381572379168
Epoch: 3633, Batch Gradient Norm after: 13.504381572379168
Epoch 3634/10000, Prediction Accuracy = 62.194%, Loss = 0.39850212931632994
Epoch: 3634, Batch Gradient Norm: 13.31509940963796
Epoch: 3634, Batch Gradient Norm after: 13.31509940963796
Epoch 3635/10000, Prediction Accuracy = 62.14%, Loss = 0.3961101174354553
Epoch: 3635, Batch Gradient Norm: 14.652076916987053
Epoch: 3635, Batch Gradient Norm after: 14.652076916987053
Epoch 3636/10000, Prediction Accuracy = 62.2%, Loss = 0.39784793853759765
Epoch: 3636, Batch Gradient Norm: 15.365630217722991
Epoch: 3636, Batch Gradient Norm after: 15.365630217722991
Epoch 3637/10000, Prediction Accuracy = 62.202%, Loss = 0.40011951327323914
Epoch: 3637, Batch Gradient Norm: 14.109880935366764
Epoch: 3637, Batch Gradient Norm after: 14.109880935366764
Epoch 3638/10000, Prediction Accuracy = 62.126%, Loss = 0.39653216004371644
Epoch: 3638, Batch Gradient Norm: 15.053098376423462
Epoch: 3638, Batch Gradient Norm after: 15.053098376423462
Epoch 3639/10000, Prediction Accuracy = 62.136%, Loss = 0.39892744421958926
Epoch: 3639, Batch Gradient Norm: 13.989308269318535
Epoch: 3639, Batch Gradient Norm after: 13.989308269318535
Epoch 3640/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.3979294538497925
Epoch: 3640, Batch Gradient Norm: 12.370337016011176
Epoch: 3640, Batch Gradient Norm after: 12.370337016011176
Epoch 3641/10000, Prediction Accuracy = 62.10600000000001%, Loss = 0.39669046401977537
Epoch: 3641, Batch Gradient Norm: 12.447545043437792
Epoch: 3641, Batch Gradient Norm after: 12.447545043437792
Epoch 3642/10000, Prediction Accuracy = 62.18000000000001%, Loss = 0.3965515196323395
Epoch: 3642, Batch Gradient Norm: 12.593871802464571
Epoch: 3642, Batch Gradient Norm after: 12.593871802464571
Epoch 3643/10000, Prediction Accuracy = 62.134%, Loss = 0.39840927720069885
Epoch: 3643, Batch Gradient Norm: 13.689392341033647
Epoch: 3643, Batch Gradient Norm after: 13.689392341033647
Epoch 3644/10000, Prediction Accuracy = 62.036%, Loss = 0.3990171790122986
Epoch: 3644, Batch Gradient Norm: 14.326643203540092
Epoch: 3644, Batch Gradient Norm after: 14.326643203540092
Epoch 3645/10000, Prediction Accuracy = 62.14799999999999%, Loss = 0.3987832427024841
Epoch: 3645, Batch Gradient Norm: 12.99872457060883
Epoch: 3645, Batch Gradient Norm after: 12.99872457060883
Epoch 3646/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.3975373864173889
Epoch: 3646, Batch Gradient Norm: 11.579196311834401
Epoch: 3646, Batch Gradient Norm after: 11.579196311834401
Epoch 3647/10000, Prediction Accuracy = 62.230000000000004%, Loss = 0.3927634537220001
Epoch: 3647, Batch Gradient Norm: 10.111755581573664
Epoch: 3647, Batch Gradient Norm after: 10.111755581573664
Epoch 3648/10000, Prediction Accuracy = 62.23%, Loss = 0.39296104907989504
Epoch: 3648, Batch Gradient Norm: 13.145090474541336
Epoch: 3648, Batch Gradient Norm after: 13.145090474541336
Epoch 3649/10000, Prediction Accuracy = 62.104%, Loss = 0.3956080198287964
Epoch: 3649, Batch Gradient Norm: 15.807991203113398
Epoch: 3649, Batch Gradient Norm after: 15.807991203113398
Epoch 3650/10000, Prediction Accuracy = 62.148%, Loss = 0.3978484392166138
Epoch: 3650, Batch Gradient Norm: 13.212335004423476
Epoch: 3650, Batch Gradient Norm after: 13.212335004423476
Epoch 3651/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.395563018321991
Epoch: 3651, Batch Gradient Norm: 10.408476349797144
Epoch: 3651, Batch Gradient Norm after: 10.408476349797144
Epoch 3652/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.3929444968700409
Epoch: 3652, Batch Gradient Norm: 11.374010959926014
Epoch: 3652, Batch Gradient Norm after: 11.374010959926014
Epoch 3653/10000, Prediction Accuracy = 62.198%, Loss = 0.3941437304019928
Epoch: 3653, Batch Gradient Norm: 11.264294949908892
Epoch: 3653, Batch Gradient Norm after: 11.264294949908892
Epoch 3654/10000, Prediction Accuracy = 62.164%, Loss = 0.3933957278728485
Epoch: 3654, Batch Gradient Norm: 12.043431119574631
Epoch: 3654, Batch Gradient Norm after: 12.043431119574631
Epoch 3655/10000, Prediction Accuracy = 62.138%, Loss = 0.39392022490501405
Epoch: 3655, Batch Gradient Norm: 11.823830308002723
Epoch: 3655, Batch Gradient Norm after: 11.823830308002723
Epoch 3656/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.39279189705848694
Epoch: 3656, Batch Gradient Norm: 12.638788467550304
Epoch: 3656, Batch Gradient Norm after: 12.638788467550304
Epoch 3657/10000, Prediction Accuracy = 62.193999999999996%, Loss = 0.39435920119285583
Epoch: 3657, Batch Gradient Norm: 12.770593946281465
Epoch: 3657, Batch Gradient Norm after: 12.770593946281465
Epoch 3658/10000, Prediction Accuracy = 62.164%, Loss = 0.3925331115722656
Epoch: 3658, Batch Gradient Norm: 14.377027599236769
Epoch: 3658, Batch Gradient Norm after: 14.377027599236769
Epoch 3659/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.39761722683906553
Epoch: 3659, Batch Gradient Norm: 11.465332396295551
Epoch: 3659, Batch Gradient Norm after: 11.465332396295551
Epoch 3660/10000, Prediction Accuracy = 62.11%, Loss = 0.39496746063232424
Epoch: 3660, Batch Gradient Norm: 10.051518698139251
Epoch: 3660, Batch Gradient Norm after: 10.051518698139251
Epoch 3661/10000, Prediction Accuracy = 62.134%, Loss = 0.3914004027843475
Epoch: 3661, Batch Gradient Norm: 9.459749842142243
Epoch: 3661, Batch Gradient Norm after: 9.459749842142243
Epoch 3662/10000, Prediction Accuracy = 62.1%, Loss = 0.39003536105155945
Epoch: 3662, Batch Gradient Norm: 9.84921281972328
Epoch: 3662, Batch Gradient Norm after: 9.84921281972328
Epoch 3663/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.39362757205963134
Epoch: 3663, Batch Gradient Norm: 13.51929272180074
Epoch: 3663, Batch Gradient Norm after: 13.51929272180074
Epoch 3664/10000, Prediction Accuracy = 62.138%, Loss = 0.3951539397239685
Epoch: 3664, Batch Gradient Norm: 9.88391493915413
Epoch: 3664, Batch Gradient Norm after: 9.88391493915413
Epoch 3665/10000, Prediction Accuracy = 62.234%, Loss = 0.3900932610034943
Epoch: 3665, Batch Gradient Norm: 11.546568137839055
Epoch: 3665, Batch Gradient Norm after: 11.546568137839055
Epoch 3666/10000, Prediction Accuracy = 62.04%, Loss = 0.394877552986145
Epoch: 3666, Batch Gradient Norm: 12.859020813063907
Epoch: 3666, Batch Gradient Norm after: 12.859020813063907
Epoch 3667/10000, Prediction Accuracy = 62.246%, Loss = 0.39693816304206847
Epoch: 3667, Batch Gradient Norm: 13.373833800161714
Epoch: 3667, Batch Gradient Norm after: 13.373833800161714
Epoch 3668/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.3950700879096985
Epoch: 3668, Batch Gradient Norm: 11.976728190176283
Epoch: 3668, Batch Gradient Norm after: 11.976728190176283
Epoch 3669/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.39293469190597535
Epoch: 3669, Batch Gradient Norm: 11.493261468186498
Epoch: 3669, Batch Gradient Norm after: 11.493261468186498
Epoch 3670/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.3923721432685852
Epoch: 3670, Batch Gradient Norm: 12.613790437440814
Epoch: 3670, Batch Gradient Norm after: 12.613790437440814
Epoch 3671/10000, Prediction Accuracy = 62.222%, Loss = 0.3931201994419098
Epoch: 3671, Batch Gradient Norm: 14.198323021028559
Epoch: 3671, Batch Gradient Norm after: 14.198323021028559
Epoch 3672/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.3942117810249329
Epoch: 3672, Batch Gradient Norm: 11.682428462973055
Epoch: 3672, Batch Gradient Norm after: 11.682428462973055
Epoch 3673/10000, Prediction Accuracy = 62.21%, Loss = 0.3917591392993927
Epoch: 3673, Batch Gradient Norm: 12.13551248944353
Epoch: 3673, Batch Gradient Norm after: 12.13551248944353
Epoch 3674/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.3931889057159424
Epoch: 3674, Batch Gradient Norm: 11.120484626072447
Epoch: 3674, Batch Gradient Norm after: 11.120484626072447
Epoch 3675/10000, Prediction Accuracy = 62.17199999999999%, Loss = 0.3926095426082611
Epoch: 3675, Batch Gradient Norm: 10.99020293437849
Epoch: 3675, Batch Gradient Norm after: 10.99020293437849
Epoch 3676/10000, Prediction Accuracy = 62.214%, Loss = 0.3921187877655029
Epoch: 3676, Batch Gradient Norm: 10.143740297955736
Epoch: 3676, Batch Gradient Norm after: 10.143740297955736
Epoch 3677/10000, Prediction Accuracy = 62.112%, Loss = 0.3922043442726135
Epoch: 3677, Batch Gradient Norm: 13.082844320857115
Epoch: 3677, Batch Gradient Norm after: 13.082844320857115
Epoch 3678/10000, Prediction Accuracy = 62.157999999999994%, Loss = 0.3945887446403503
Epoch: 3678, Batch Gradient Norm: 12.123501826568786
Epoch: 3678, Batch Gradient Norm after: 12.123501826568786
Epoch 3679/10000, Prediction Accuracy = 62.166%, Loss = 0.39403350949287413
Epoch: 3679, Batch Gradient Norm: 12.379926470397905
Epoch: 3679, Batch Gradient Norm after: 12.379926470397905
Epoch 3680/10000, Prediction Accuracy = 62.152%, Loss = 0.3936945736408234
Epoch: 3680, Batch Gradient Norm: 12.42579241908707
Epoch: 3680, Batch Gradient Norm after: 12.42579241908707
Epoch 3681/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.3940685451030731
Epoch: 3681, Batch Gradient Norm: 12.67737632434028
Epoch: 3681, Batch Gradient Norm after: 12.67737632434028
Epoch 3682/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.39668106436729433
Epoch: 3682, Batch Gradient Norm: 10.792667081186313
Epoch: 3682, Batch Gradient Norm after: 10.792667081186313
Epoch 3683/10000, Prediction Accuracy = 62.186%, Loss = 0.39324278831481935
Epoch: 3683, Batch Gradient Norm: 15.779706384159141
Epoch: 3683, Batch Gradient Norm after: 15.779706384159141
Epoch 3684/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.3999895632266998
Epoch: 3684, Batch Gradient Norm: 13.457267018631084
Epoch: 3684, Batch Gradient Norm after: 13.457267018631084
Epoch 3685/10000, Prediction Accuracy = 62.134%, Loss = 0.3953521132469177
Epoch: 3685, Batch Gradient Norm: 12.603807690185942
Epoch: 3685, Batch Gradient Norm after: 12.603807690185942
Epoch 3686/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.39386415481567383
Epoch: 3686, Batch Gradient Norm: 13.419060073290865
Epoch: 3686, Batch Gradient Norm after: 13.419060073290865
Epoch 3687/10000, Prediction Accuracy = 62.17%, Loss = 0.3937466382980347
Epoch: 3687, Batch Gradient Norm: 15.296866907292657
Epoch: 3687, Batch Gradient Norm after: 15.296866907292657
Epoch 3688/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.39766511917114256
Epoch: 3688, Batch Gradient Norm: 14.891026047101981
Epoch: 3688, Batch Gradient Norm after: 14.891026047101981
Epoch 3689/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.3974481523036957
Epoch: 3689, Batch Gradient Norm: 13.178994386901556
Epoch: 3689, Batch Gradient Norm after: 13.178994386901556
Epoch 3690/10000, Prediction Accuracy = 62.146%, Loss = 0.39450379014015197
Epoch: 3690, Batch Gradient Norm: 11.62573257479133
Epoch: 3690, Batch Gradient Norm after: 11.62573257479133
Epoch 3691/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.3940256953239441
Epoch: 3691, Batch Gradient Norm: 12.634573950021379
Epoch: 3691, Batch Gradient Norm after: 12.634573950021379
Epoch 3692/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.39467719197273254
Epoch: 3692, Batch Gradient Norm: 12.88042956388973
Epoch: 3692, Batch Gradient Norm after: 12.88042956388973
Epoch 3693/10000, Prediction Accuracy = 62.17%, Loss = 0.39584035873413087
Epoch: 3693, Batch Gradient Norm: 10.27623715484923
Epoch: 3693, Batch Gradient Norm after: 10.27623715484923
Epoch 3694/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.39180207848548887
Epoch: 3694, Batch Gradient Norm: 11.615676447505482
Epoch: 3694, Batch Gradient Norm after: 11.615676447505482
Epoch 3695/10000, Prediction Accuracy = 62.176%, Loss = 0.39212419986724856
Epoch: 3695, Batch Gradient Norm: 10.998420028845455
Epoch: 3695, Batch Gradient Norm after: 10.998420028845455
Epoch 3696/10000, Prediction Accuracy = 62.134%, Loss = 0.39263848066329954
Epoch: 3696, Batch Gradient Norm: 12.479967299519593
Epoch: 3696, Batch Gradient Norm after: 12.479967299519593
Epoch 3697/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.3950184643268585
Epoch: 3697, Batch Gradient Norm: 11.991308663408484
Epoch: 3697, Batch Gradient Norm after: 11.991308663408484
Epoch 3698/10000, Prediction Accuracy = 62.274%, Loss = 0.39433481097221373
Epoch: 3698, Batch Gradient Norm: 13.242209694265828
Epoch: 3698, Batch Gradient Norm after: 13.242209694265828
Epoch 3699/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.3965268790721893
Epoch: 3699, Batch Gradient Norm: 12.578701351340959
Epoch: 3699, Batch Gradient Norm after: 12.578701351340959
Epoch 3700/10000, Prediction Accuracy = 62.202%, Loss = 0.3931601524353027
Epoch: 3700, Batch Gradient Norm: 12.999503868864627
Epoch: 3700, Batch Gradient Norm after: 12.999503868864627
Epoch 3701/10000, Prediction Accuracy = 62.064%, Loss = 0.39480720162391664
Epoch: 3701, Batch Gradient Norm: 16.367092512013844
Epoch: 3701, Batch Gradient Norm after: 16.367092512013844
Epoch 3702/10000, Prediction Accuracy = 62.23599999999999%, Loss = 0.4019827783107758
Epoch: 3702, Batch Gradient Norm: 15.05384445614458
Epoch: 3702, Batch Gradient Norm after: 15.05384445614458
Epoch 3703/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.3978506028652191
Epoch: 3703, Batch Gradient Norm: 12.710209412139172
Epoch: 3703, Batch Gradient Norm after: 12.710209412139172
Epoch 3704/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.39343262910842897
Epoch: 3704, Batch Gradient Norm: 12.035104303229268
Epoch: 3704, Batch Gradient Norm after: 12.035104303229268
Epoch 3705/10000, Prediction Accuracy = 62.186%, Loss = 0.3957029402256012
Epoch: 3705, Batch Gradient Norm: 13.167079541836669
Epoch: 3705, Batch Gradient Norm after: 13.167079541836669
Epoch 3706/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.3944034993648529
Epoch: 3706, Batch Gradient Norm: 13.364686657410722
Epoch: 3706, Batch Gradient Norm after: 13.364686657410722
Epoch 3707/10000, Prediction Accuracy = 62.162%, Loss = 0.3951663613319397
Epoch: 3707, Batch Gradient Norm: 13.529173572842957
Epoch: 3707, Batch Gradient Norm after: 13.529173572842957
Epoch 3708/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.3965412199497223
Epoch: 3708, Batch Gradient Norm: 13.043682831828916
Epoch: 3708, Batch Gradient Norm after: 13.043682831828916
Epoch 3709/10000, Prediction Accuracy = 62.29%, Loss = 0.39450002908706666
Epoch: 3709, Batch Gradient Norm: 13.540544371848663
Epoch: 3709, Batch Gradient Norm after: 13.540544371848663
Epoch 3710/10000, Prediction Accuracy = 62.21400000000001%, Loss = 0.39478822946548464
Epoch: 3710, Batch Gradient Norm: 14.145984895715639
Epoch: 3710, Batch Gradient Norm after: 14.145984895715639
Epoch 3711/10000, Prediction Accuracy = 62.148%, Loss = 0.39399378895759585
Epoch: 3711, Batch Gradient Norm: 12.48337530065054
Epoch: 3711, Batch Gradient Norm after: 12.48337530065054
Epoch 3712/10000, Prediction Accuracy = 62.226%, Loss = 0.39272732734680177
Epoch: 3712, Batch Gradient Norm: 11.417133642482266
Epoch: 3712, Batch Gradient Norm after: 11.417133642482266
Epoch 3713/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.3918183267116547
Epoch: 3713, Batch Gradient Norm: 13.575827722027137
Epoch: 3713, Batch Gradient Norm after: 13.575827722027137
Epoch 3714/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.3961603343486786
Epoch: 3714, Batch Gradient Norm: 12.82331974722414
Epoch: 3714, Batch Gradient Norm after: 12.82331974722414
Epoch 3715/10000, Prediction Accuracy = 62.162%, Loss = 0.391622930765152
Epoch: 3715, Batch Gradient Norm: 13.69736808916498
Epoch: 3715, Batch Gradient Norm after: 13.69736808916498
Epoch 3716/10000, Prediction Accuracy = 62.212%, Loss = 0.3928312838077545
Epoch: 3716, Batch Gradient Norm: 11.706515841282693
Epoch: 3716, Batch Gradient Norm after: 11.706515841282693
Epoch 3717/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.3926818311214447
Epoch: 3717, Batch Gradient Norm: 10.145089175228941
Epoch: 3717, Batch Gradient Norm after: 10.145089175228941
Epoch 3718/10000, Prediction Accuracy = 62.182%, Loss = 0.39041834473609927
Epoch: 3718, Batch Gradient Norm: 9.826810811785386
Epoch: 3718, Batch Gradient Norm after: 9.826810811785386
Epoch 3719/10000, Prediction Accuracy = 62.202%, Loss = 0.3905429780483246
Epoch: 3719, Batch Gradient Norm: 13.20311569618827
Epoch: 3719, Batch Gradient Norm after: 13.20311569618827
Epoch 3720/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.39179232716560364
Epoch: 3720, Batch Gradient Norm: 14.428190627798324
Epoch: 3720, Batch Gradient Norm after: 14.428190627798324
Epoch 3721/10000, Prediction Accuracy = 62.19%, Loss = 0.3960817039012909
Epoch: 3721, Batch Gradient Norm: 13.963488462350766
Epoch: 3721, Batch Gradient Norm after: 13.963488462350766
Epoch 3722/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.39383472204208375
Epoch: 3722, Batch Gradient Norm: 12.037154631932337
Epoch: 3722, Batch Gradient Norm after: 12.037154631932337
Epoch 3723/10000, Prediction Accuracy = 62.322%, Loss = 0.3905246317386627
Epoch: 3723, Batch Gradient Norm: 11.040855709939194
Epoch: 3723, Batch Gradient Norm after: 11.040855709939194
Epoch 3724/10000, Prediction Accuracy = 62.176%, Loss = 0.39061413407325746
Epoch: 3724, Batch Gradient Norm: 12.303315962390306
Epoch: 3724, Batch Gradient Norm after: 12.303315962390306
Epoch 3725/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.39271260499954225
Epoch: 3725, Batch Gradient Norm: 12.623605238336008
Epoch: 3725, Batch Gradient Norm after: 12.623605238336008
Epoch 3726/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.3934830605983734
Epoch: 3726, Batch Gradient Norm: 15.077833424321595
Epoch: 3726, Batch Gradient Norm after: 15.077833424321595
Epoch 3727/10000, Prediction Accuracy = 62.24000000000001%, Loss = 0.3942317426204681
Epoch: 3727, Batch Gradient Norm: 14.216475667647543
Epoch: 3727, Batch Gradient Norm after: 14.216475667647543
Epoch 3728/10000, Prediction Accuracy = 62.178%, Loss = 0.39563597440719606
Epoch: 3728, Batch Gradient Norm: 12.80249955350844
Epoch: 3728, Batch Gradient Norm after: 12.80249955350844
Epoch 3729/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.39221656918525694
Epoch: 3729, Batch Gradient Norm: 13.337352949652281
Epoch: 3729, Batch Gradient Norm after: 13.337352949652281
Epoch 3730/10000, Prediction Accuracy = 62.194%, Loss = 0.3932175874710083
Epoch: 3730, Batch Gradient Norm: 14.224169837149184
Epoch: 3730, Batch Gradient Norm after: 14.224169837149184
Epoch 3731/10000, Prediction Accuracy = 62.198%, Loss = 0.3974912464618683
Epoch: 3731, Batch Gradient Norm: 12.675426546782703
Epoch: 3731, Batch Gradient Norm after: 12.675426546782703
Epoch 3732/10000, Prediction Accuracy = 62.33%, Loss = 0.39176546931266787
Epoch: 3732, Batch Gradient Norm: 12.17875736288443
Epoch: 3732, Batch Gradient Norm after: 12.17875736288443
Epoch 3733/10000, Prediction Accuracy = 62.222%, Loss = 0.39347326159477236
Epoch: 3733, Batch Gradient Norm: 11.185737495773433
Epoch: 3733, Batch Gradient Norm after: 11.185737495773433
Epoch 3734/10000, Prediction Accuracy = 62.322%, Loss = 0.3902894496917725
Epoch: 3734, Batch Gradient Norm: 13.22166329445599
Epoch: 3734, Batch Gradient Norm after: 13.22166329445599
Epoch 3735/10000, Prediction Accuracy = 62.068%, Loss = 0.3955752968788147
Epoch: 3735, Batch Gradient Norm: 12.34525386890259
Epoch: 3735, Batch Gradient Norm after: 12.34525386890259
Epoch 3736/10000, Prediction Accuracy = 62.282%, Loss = 0.3948543190956116
Epoch: 3736, Batch Gradient Norm: 12.520269509367608
Epoch: 3736, Batch Gradient Norm after: 12.520269509367608
Epoch 3737/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.3920748770236969
Epoch: 3737, Batch Gradient Norm: 11.797677176658123
Epoch: 3737, Batch Gradient Norm after: 11.797677176658123
Epoch 3738/10000, Prediction Accuracy = 62.102%, Loss = 0.39154231548309326
Epoch: 3738, Batch Gradient Norm: 12.484762533880916
Epoch: 3738, Batch Gradient Norm after: 12.484762533880916
Epoch 3739/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.3909067928791046
Epoch: 3739, Batch Gradient Norm: 11.141582778696936
Epoch: 3739, Batch Gradient Norm after: 11.141582778696936
Epoch 3740/10000, Prediction Accuracy = 62.254%, Loss = 0.39265021681785583
Epoch: 3740, Batch Gradient Norm: 10.60071120243804
Epoch: 3740, Batch Gradient Norm after: 10.60071120243804
Epoch 3741/10000, Prediction Accuracy = 62.158%, Loss = 0.3902351140975952
Epoch: 3741, Batch Gradient Norm: 10.963815589745598
Epoch: 3741, Batch Gradient Norm after: 10.963815589745598
Epoch 3742/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.3917398929595947
Epoch: 3742, Batch Gradient Norm: 10.264600229892356
Epoch: 3742, Batch Gradient Norm after: 10.264600229892356
Epoch 3743/10000, Prediction Accuracy = 62.226%, Loss = 0.3899761140346527
Epoch: 3743, Batch Gradient Norm: 8.848672409675247
Epoch: 3743, Batch Gradient Norm after: 8.848672409675247
Epoch 3744/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.3883218228816986
Epoch: 3744, Batch Gradient Norm: 11.579223980906402
Epoch: 3744, Batch Gradient Norm after: 11.579223980906402
Epoch 3745/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.39161003828048707
Epoch: 3745, Batch Gradient Norm: 12.688076624711334
Epoch: 3745, Batch Gradient Norm after: 12.688076624711334
Epoch 3746/10000, Prediction Accuracy = 62.202%, Loss = 0.39209147691726687
Epoch: 3746, Batch Gradient Norm: 14.782208406555808
Epoch: 3746, Batch Gradient Norm after: 14.782208406555808
Epoch 3747/10000, Prediction Accuracy = 62.278%, Loss = 0.3946539044380188
Epoch: 3747, Batch Gradient Norm: 14.457582400448095
Epoch: 3747, Batch Gradient Norm after: 14.457582400448095
Epoch 3748/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.3929951310157776
Epoch: 3748, Batch Gradient Norm: 13.88714619173558
Epoch: 3748, Batch Gradient Norm after: 13.88714619173558
Epoch 3749/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.39265176057815554
Epoch: 3749, Batch Gradient Norm: 13.398769818865883
Epoch: 3749, Batch Gradient Norm after: 13.398769818865883
Epoch 3750/10000, Prediction Accuracy = 62.26400000000001%, Loss = 0.3940981924533844
Epoch: 3750, Batch Gradient Norm: 11.793691145326175
Epoch: 3750, Batch Gradient Norm after: 11.793691145326175
Epoch 3751/10000, Prediction Accuracy = 62.148%, Loss = 0.3905576169490814
Epoch: 3751, Batch Gradient Norm: 15.812127368645763
Epoch: 3751, Batch Gradient Norm after: 15.812127368645763
Epoch 3752/10000, Prediction Accuracy = 62.212%, Loss = 0.3984504222869873
Epoch: 3752, Batch Gradient Norm: 16.042091588741183
Epoch: 3752, Batch Gradient Norm after: 16.042091588741183
Epoch 3753/10000, Prediction Accuracy = 62.262%, Loss = 0.3998776197433472
Epoch: 3753, Batch Gradient Norm: 12.47158233232133
Epoch: 3753, Batch Gradient Norm after: 12.47158233232133
Epoch 3754/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.3930054843425751
Epoch: 3754, Batch Gradient Norm: 12.589910821058352
Epoch: 3754, Batch Gradient Norm after: 12.589910821058352
Epoch 3755/10000, Prediction Accuracy = 62.162%, Loss = 0.3947973906993866
Epoch: 3755, Batch Gradient Norm: 14.229062584825835
Epoch: 3755, Batch Gradient Norm after: 14.229062584825835
Epoch 3756/10000, Prediction Accuracy = 62.222%, Loss = 0.39308207035064696
Epoch: 3756, Batch Gradient Norm: 16.04616714105073
Epoch: 3756, Batch Gradient Norm after: 16.04616714105073
Epoch 3757/10000, Prediction Accuracy = 62.148%, Loss = 0.3989956796169281
Epoch: 3757, Batch Gradient Norm: 17.147316636975823
Epoch: 3757, Batch Gradient Norm after: 17.147316636975823
Epoch 3758/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.3993596136569977
Epoch: 3758, Batch Gradient Norm: 16.750506960697916
Epoch: 3758, Batch Gradient Norm after: 16.750506960697916
Epoch 3759/10000, Prediction Accuracy = 62.122%, Loss = 0.3997768759727478
Epoch: 3759, Batch Gradient Norm: 14.925672572649441
Epoch: 3759, Batch Gradient Norm after: 14.925672572649441
Epoch 3760/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.3965326607227325
Epoch: 3760, Batch Gradient Norm: 13.74097125864087
Epoch: 3760, Batch Gradient Norm after: 13.74097125864087
Epoch 3761/10000, Prediction Accuracy = 62.2%, Loss = 0.3920365393161774
Epoch: 3761, Batch Gradient Norm: 13.695621255535427
Epoch: 3761, Batch Gradient Norm after: 13.695621255535427
Epoch 3762/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.39185729026794436
Epoch: 3762, Batch Gradient Norm: 14.63212974717103
Epoch: 3762, Batch Gradient Norm after: 14.63212974717103
Epoch 3763/10000, Prediction Accuracy = 62.188%, Loss = 0.3956942319869995
Epoch: 3763, Batch Gradient Norm: 16.74309768545849
Epoch: 3763, Batch Gradient Norm after: 16.01919246249679
Epoch 3764/10000, Prediction Accuracy = 62.166%, Loss = 0.4009288489818573
Epoch: 3764, Batch Gradient Norm: 16.33387842931996
Epoch: 3764, Batch Gradient Norm after: 15.877468902779738
Epoch 3765/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.3972553193569183
Epoch: 3765, Batch Gradient Norm: 14.043406367570926
Epoch: 3765, Batch Gradient Norm after: 14.043406367570926
Epoch 3766/10000, Prediction Accuracy = 62.13399999999999%, Loss = 0.3949315071105957
Epoch: 3766, Batch Gradient Norm: 16.07788918462962
Epoch: 3766, Batch Gradient Norm after: 16.07788918462962
Epoch 3767/10000, Prediction Accuracy = 62.3%, Loss = 0.3970461428165436
Epoch: 3767, Batch Gradient Norm: 14.455079729301183
Epoch: 3767, Batch Gradient Norm after: 14.455079729301183
Epoch 3768/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.39275259971618653
Epoch: 3768, Batch Gradient Norm: 12.40282611228741
Epoch: 3768, Batch Gradient Norm after: 12.40282611228741
Epoch 3769/10000, Prediction Accuracy = 62.126%, Loss = 0.3906336009502411
Epoch: 3769, Batch Gradient Norm: 10.799724682047302
Epoch: 3769, Batch Gradient Norm after: 10.799724682047302
Epoch 3770/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.38869285583496094
Epoch: 3770, Batch Gradient Norm: 10.383129072871823
Epoch: 3770, Batch Gradient Norm after: 10.383129072871823
Epoch 3771/10000, Prediction Accuracy = 62.136%, Loss = 0.38930187225341795
Epoch: 3771, Batch Gradient Norm: 10.157711991126233
Epoch: 3771, Batch Gradient Norm after: 10.157711991126233
Epoch 3772/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.3880685031414032
Epoch: 3772, Batch Gradient Norm: 10.968336638889149
Epoch: 3772, Batch Gradient Norm after: 10.968336638889149
Epoch 3773/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.3881659388542175
Epoch: 3773, Batch Gradient Norm: 12.408405599949516
Epoch: 3773, Batch Gradient Norm after: 12.408405599949516
Epoch 3774/10000, Prediction Accuracy = 62.212%, Loss = 0.38980293869972227
Epoch: 3774, Batch Gradient Norm: 15.41789496301678
Epoch: 3774, Batch Gradient Norm after: 15.41789496301678
Epoch 3775/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.3940994918346405
Epoch: 3775, Batch Gradient Norm: 16.622119378672345
Epoch: 3775, Batch Gradient Norm after: 16.622119378672345
Epoch 3776/10000, Prediction Accuracy = 62.21%, Loss = 0.39545833468437197
Epoch: 3776, Batch Gradient Norm: 17.372709045886037
Epoch: 3776, Batch Gradient Norm after: 16.989903418968975
Epoch 3777/10000, Prediction Accuracy = 62.134%, Loss = 0.39892889857292174
Epoch: 3777, Batch Gradient Norm: 14.534056992200759
Epoch: 3777, Batch Gradient Norm after: 14.534056992200759
Epoch 3778/10000, Prediction Accuracy = 62.318%, Loss = 0.39468362331390383
Epoch: 3778, Batch Gradient Norm: 11.043375481918574
Epoch: 3778, Batch Gradient Norm after: 11.043375481918574
Epoch 3779/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.3878647446632385
Epoch: 3779, Batch Gradient Norm: 10.690986881094393
Epoch: 3779, Batch Gradient Norm after: 10.690986881094393
Epoch 3780/10000, Prediction Accuracy = 62.2%, Loss = 0.3901566982269287
Epoch: 3780, Batch Gradient Norm: 7.565864645682905
Epoch: 3780, Batch Gradient Norm after: 7.565864645682905
Epoch 3781/10000, Prediction Accuracy = 62.21%, Loss = 0.38438721299171447
Epoch: 3781, Batch Gradient Norm: 9.503035883796793
Epoch: 3781, Batch Gradient Norm after: 9.503035883796793
Epoch 3782/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.3863108217716217
Epoch: 3782, Batch Gradient Norm: 9.818894972235023
Epoch: 3782, Batch Gradient Norm after: 9.818894972235023
Epoch 3783/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.3902649343013763
Epoch: 3783, Batch Gradient Norm: 10.116420507197331
Epoch: 3783, Batch Gradient Norm after: 10.116420507197331
Epoch 3784/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.3866982877254486
Epoch: 3784, Batch Gradient Norm: 11.27559099760488
Epoch: 3784, Batch Gradient Norm after: 11.27559099760488
Epoch 3785/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.38830423951148985
Epoch: 3785, Batch Gradient Norm: 10.68603052143519
Epoch: 3785, Batch Gradient Norm after: 10.68603052143519
Epoch 3786/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.3889137268066406
Epoch: 3786, Batch Gradient Norm: 10.165728512510464
Epoch: 3786, Batch Gradient Norm after: 10.165728512510464
Epoch 3787/10000, Prediction Accuracy = 62.209999999999994%, Loss = 0.38792405128479
Epoch: 3787, Batch Gradient Norm: 12.024644246350238
Epoch: 3787, Batch Gradient Norm after: 12.024644246350238
Epoch 3788/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.38978148698806764
Epoch: 3788, Batch Gradient Norm: 12.386257549580783
Epoch: 3788, Batch Gradient Norm after: 12.386257549580783
Epoch 3789/10000, Prediction Accuracy = 62.076%, Loss = 0.3892125070095062
Epoch: 3789, Batch Gradient Norm: 11.813353678814249
Epoch: 3789, Batch Gradient Norm after: 11.813353678814249
Epoch 3790/10000, Prediction Accuracy = 62.25%, Loss = 0.3910775125026703
Epoch: 3790, Batch Gradient Norm: 15.957516856920336
Epoch: 3790, Batch Gradient Norm after: 15.957516856920336
Epoch 3791/10000, Prediction Accuracy = 62.302%, Loss = 0.3941918730735779
Epoch: 3791, Batch Gradient Norm: 14.362776519230925
Epoch: 3791, Batch Gradient Norm after: 14.362776519230925
Epoch 3792/10000, Prediction Accuracy = 62.27199999999999%, Loss = 0.3927650451660156
Epoch: 3792, Batch Gradient Norm: 16.736603390111565
Epoch: 3792, Batch Gradient Norm after: 16.39603239478329
Epoch 3793/10000, Prediction Accuracy = 62.194%, Loss = 0.39689786434173585
Epoch: 3793, Batch Gradient Norm: 14.583688952922481
Epoch: 3793, Batch Gradient Norm after: 14.583688952922481
Epoch 3794/10000, Prediction Accuracy = 62.24799999999999%, Loss = 0.3936264395713806
Epoch: 3794, Batch Gradient Norm: 16.01907442105115
Epoch: 3794, Batch Gradient Norm after: 16.01907442105115
Epoch 3795/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.3958456218242645
Epoch: 3795, Batch Gradient Norm: 14.31922374891162
Epoch: 3795, Batch Gradient Norm after: 14.31922374891162
Epoch 3796/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.3928943157196045
Epoch: 3796, Batch Gradient Norm: 16.648030433132252
Epoch: 3796, Batch Gradient Norm after: 16.23016401655168
Epoch 3797/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.3970733225345612
Epoch: 3797, Batch Gradient Norm: 13.389233854858979
Epoch: 3797, Batch Gradient Norm after: 13.389233854858979
Epoch 3798/10000, Prediction Accuracy = 62.202%, Loss = 0.3917052984237671
Epoch: 3798, Batch Gradient Norm: 13.064589559264522
Epoch: 3798, Batch Gradient Norm after: 13.064589559264522
Epoch 3799/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.3912071824073792
Epoch: 3799, Batch Gradient Norm: 12.530759416146147
Epoch: 3799, Batch Gradient Norm after: 12.530759416146147
Epoch 3800/10000, Prediction Accuracy = 62.25600000000001%, Loss = 0.3900415897369385
Epoch: 3800, Batch Gradient Norm: 13.958914503740957
Epoch: 3800, Batch Gradient Norm after: 13.958914503740957
Epoch 3801/10000, Prediction Accuracy = 62.238%, Loss = 0.39448603987693787
Epoch: 3801, Batch Gradient Norm: 12.824859045617266
Epoch: 3801, Batch Gradient Norm after: 12.824859045617266
Epoch 3802/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.39060577750205994
Epoch: 3802, Batch Gradient Norm: 12.765665336842247
Epoch: 3802, Batch Gradient Norm after: 12.765665336842247
Epoch 3803/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.3896917819976807
Epoch: 3803, Batch Gradient Norm: 12.217753359578515
Epoch: 3803, Batch Gradient Norm after: 12.217753359578515
Epoch 3804/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.3909402251243591
Epoch: 3804, Batch Gradient Norm: 15.033726042132962
Epoch: 3804, Batch Gradient Norm after: 15.033726042132962
Epoch 3805/10000, Prediction Accuracy = 62.214%, Loss = 0.39278510212898254
Epoch: 3805, Batch Gradient Norm: 11.54869973954262
Epoch: 3805, Batch Gradient Norm after: 11.54869973954262
Epoch 3806/10000, Prediction Accuracy = 62.164%, Loss = 0.38987215161323546
Epoch: 3806, Batch Gradient Norm: 13.238080281124267
Epoch: 3806, Batch Gradient Norm after: 13.238080281124267
Epoch 3807/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.38910770416259766
Epoch: 3807, Batch Gradient Norm: 13.616186331071454
Epoch: 3807, Batch Gradient Norm after: 13.616186331071454
Epoch 3808/10000, Prediction Accuracy = 62.23%, Loss = 0.39254496097564695
Epoch: 3808, Batch Gradient Norm: 12.082202471611739
Epoch: 3808, Batch Gradient Norm after: 12.082202471611739
Epoch 3809/10000, Prediction Accuracy = 62.312%, Loss = 0.38924564719200133
Epoch: 3809, Batch Gradient Norm: 16.43510071144715
Epoch: 3809, Batch Gradient Norm after: 16.43510071144715
Epoch 3810/10000, Prediction Accuracy = 62.246%, Loss = 0.39516717195510864
Epoch: 3810, Batch Gradient Norm: 14.403222525557492
Epoch: 3810, Batch Gradient Norm after: 14.403222525557492
Epoch 3811/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.3922322988510132
Epoch: 3811, Batch Gradient Norm: 14.009932106632494
Epoch: 3811, Batch Gradient Norm after: 14.009932106632494
Epoch 3812/10000, Prediction Accuracy = 62.27%, Loss = 0.3909936547279358
Epoch: 3812, Batch Gradient Norm: 13.178481254058543
Epoch: 3812, Batch Gradient Norm after: 13.178481254058543
Epoch 3813/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.39034133553504946
Epoch: 3813, Batch Gradient Norm: 13.518729620104088
Epoch: 3813, Batch Gradient Norm after: 13.518729620104088
Epoch 3814/10000, Prediction Accuracy = 62.178%, Loss = 0.39080855846405027
Epoch: 3814, Batch Gradient Norm: 13.029781461437516
Epoch: 3814, Batch Gradient Norm after: 13.029781461437516
Epoch 3815/10000, Prediction Accuracy = 62.314%, Loss = 0.39095396399497984
Epoch: 3815, Batch Gradient Norm: 15.506426234580058
Epoch: 3815, Batch Gradient Norm after: 15.506426234580058
Epoch 3816/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.3965986967086792
Epoch: 3816, Batch Gradient Norm: 12.910298129998397
Epoch: 3816, Batch Gradient Norm after: 12.910298129998397
Epoch 3817/10000, Prediction Accuracy = 62.3%, Loss = 0.39014877676963805
Epoch: 3817, Batch Gradient Norm: 13.203417733764883
Epoch: 3817, Batch Gradient Norm after: 13.203417733764883
Epoch 3818/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.38980414867401125
Epoch: 3818, Batch Gradient Norm: 11.411356534425916
Epoch: 3818, Batch Gradient Norm after: 11.411356534425916
Epoch 3819/10000, Prediction Accuracy = 62.376%, Loss = 0.3874340295791626
Epoch: 3819, Batch Gradient Norm: 14.013647554212612
Epoch: 3819, Batch Gradient Norm after: 14.013647554212612
Epoch 3820/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.3918766021728516
Epoch: 3820, Batch Gradient Norm: 12.423107926193653
Epoch: 3820, Batch Gradient Norm after: 12.423107926193653
Epoch 3821/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.39058205485343933
Epoch: 3821, Batch Gradient Norm: 13.38648964833862
Epoch: 3821, Batch Gradient Norm after: 13.38648964833862
Epoch 3822/10000, Prediction Accuracy = 62.246%, Loss = 0.39133440852165224
Epoch: 3822, Batch Gradient Norm: 13.137390696465491
Epoch: 3822, Batch Gradient Norm after: 13.137390696465491
Epoch 3823/10000, Prediction Accuracy = 62.198%, Loss = 0.3899197816848755
Epoch: 3823, Batch Gradient Norm: 16.975524944633815
Epoch: 3823, Batch Gradient Norm after: 16.490094902043047
Epoch 3824/10000, Prediction Accuracy = 62.254%, Loss = 0.39823451042175295
Epoch: 3824, Batch Gradient Norm: 19.077980543538985
Epoch: 3824, Batch Gradient Norm after: 19.074715226758983
Epoch 3825/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.39977837204933164
Epoch: 3825, Batch Gradient Norm: 16.45530705212958
Epoch: 3825, Batch Gradient Norm after: 16.45530705212958
Epoch 3826/10000, Prediction Accuracy = 62.288%, Loss = 0.3942273914813995
Epoch: 3826, Batch Gradient Norm: 15.256182446214028
Epoch: 3826, Batch Gradient Norm after: 15.256182446214028
Epoch 3827/10000, Prediction Accuracy = 62.2%, Loss = 0.39498613476753236
Epoch: 3827, Batch Gradient Norm: 13.099243053075009
Epoch: 3827, Batch Gradient Norm after: 13.099243053075009
Epoch 3828/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.38957459330558775
Epoch: 3828, Batch Gradient Norm: 14.562062256437828
Epoch: 3828, Batch Gradient Norm after: 14.562062256437828
Epoch 3829/10000, Prediction Accuracy = 62.214%, Loss = 0.39251199960708616
Epoch: 3829, Batch Gradient Norm: 11.761020106121503
Epoch: 3829, Batch Gradient Norm after: 11.761020106121503
Epoch 3830/10000, Prediction Accuracy = 62.148%, Loss = 0.3891835331916809
Epoch: 3830, Batch Gradient Norm: 12.8443116664467
Epoch: 3830, Batch Gradient Norm after: 12.8443116664467
Epoch 3831/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.3897298097610474
Epoch: 3831, Batch Gradient Norm: 12.23436167500038
Epoch: 3831, Batch Gradient Norm after: 12.23436167500038
Epoch 3832/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.3884893298149109
Epoch: 3832, Batch Gradient Norm: 15.337136512438327
Epoch: 3832, Batch Gradient Norm after: 15.337136512438327
Epoch 3833/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.39273186326026915
Epoch: 3833, Batch Gradient Norm: 13.145805575457484
Epoch: 3833, Batch Gradient Norm after: 13.145805575457484
Epoch 3834/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.39101132154464724
Epoch: 3834, Batch Gradient Norm: 13.582566740889828
Epoch: 3834, Batch Gradient Norm after: 13.582566740889828
Epoch 3835/10000, Prediction Accuracy = 62.186%, Loss = 0.392042875289917
Epoch: 3835, Batch Gradient Norm: 14.066753474917066
Epoch: 3835, Batch Gradient Norm after: 14.066753474917066
Epoch 3836/10000, Prediction Accuracy = 62.19%, Loss = 0.39057198762893675
Epoch: 3836, Batch Gradient Norm: 10.94610095454045
Epoch: 3836, Batch Gradient Norm after: 10.94610095454045
Epoch 3837/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.3865895211696625
Epoch: 3837, Batch Gradient Norm: 12.915529108719067
Epoch: 3837, Batch Gradient Norm after: 12.915529108719067
Epoch 3838/10000, Prediction Accuracy = 62.322%, Loss = 0.3907823622226715
Epoch: 3838, Batch Gradient Norm: 14.991079647119927
Epoch: 3838, Batch Gradient Norm after: 14.991079647119927
Epoch 3839/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.3961579501628876
Epoch: 3839, Batch Gradient Norm: 13.675056183184294
Epoch: 3839, Batch Gradient Norm after: 13.675056183184294
Epoch 3840/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.3907411515712738
Epoch: 3840, Batch Gradient Norm: 9.865954096408865
Epoch: 3840, Batch Gradient Norm after: 9.865954096408865
Epoch 3841/10000, Prediction Accuracy = 62.23%, Loss = 0.3861173689365387
Epoch: 3841, Batch Gradient Norm: 12.969637430998244
Epoch: 3841, Batch Gradient Norm after: 12.969637430998244
Epoch 3842/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.38881604075431825
Epoch: 3842, Batch Gradient Norm: 12.24945542792452
Epoch: 3842, Batch Gradient Norm after: 12.24945542792452
Epoch 3843/10000, Prediction Accuracy = 62.362%, Loss = 0.38961527347564695
Epoch: 3843, Batch Gradient Norm: 14.30471836274697
Epoch: 3843, Batch Gradient Norm after: 14.30471836274697
Epoch 3844/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.39382253885269164
Epoch: 3844, Batch Gradient Norm: 12.402632984371861
Epoch: 3844, Batch Gradient Norm after: 12.402632984371861
Epoch 3845/10000, Prediction Accuracy = 62.242%, Loss = 0.39142250418663027
Epoch: 3845, Batch Gradient Norm: 9.391310022521616
Epoch: 3845, Batch Gradient Norm after: 9.391310022521616
Epoch 3846/10000, Prediction Accuracy = 62.3%, Loss = 0.38508530259132384
Epoch: 3846, Batch Gradient Norm: 9.539974695902258
Epoch: 3846, Batch Gradient Norm after: 9.539974695902258
Epoch 3847/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.3857632398605347
Epoch: 3847, Batch Gradient Norm: 10.122965592763455
Epoch: 3847, Batch Gradient Norm after: 10.122965592763455
Epoch 3848/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.38799487352371215
Epoch: 3848, Batch Gradient Norm: 11.258719828279533
Epoch: 3848, Batch Gradient Norm after: 11.258719828279533
Epoch 3849/10000, Prediction Accuracy = 62.33%, Loss = 0.3871959984302521
Epoch: 3849, Batch Gradient Norm: 16.27068453908572
Epoch: 3849, Batch Gradient Norm after: 16.27068453908572
Epoch 3850/10000, Prediction Accuracy = 62.288%, Loss = 0.39411144852638247
Epoch: 3850, Batch Gradient Norm: 15.95885687233859
Epoch: 3850, Batch Gradient Norm after: 15.95885687233859
Epoch 3851/10000, Prediction Accuracy = 62.15999999999999%, Loss = 0.39592031836509706
Epoch: 3851, Batch Gradient Norm: 14.845516948717274
Epoch: 3851, Batch Gradient Norm after: 14.845516948717274
Epoch 3852/10000, Prediction Accuracy = 62.30999999999999%, Loss = 0.3933199763298035
Epoch: 3852, Batch Gradient Norm: 15.953382413518367
Epoch: 3852, Batch Gradient Norm after: 15.953382413518367
Epoch 3853/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.3954099118709564
Epoch: 3853, Batch Gradient Norm: 14.32334757286133
Epoch: 3853, Batch Gradient Norm after: 14.32334757286133
Epoch 3854/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.39362590909004214
Epoch: 3854, Batch Gradient Norm: 13.591002107688439
Epoch: 3854, Batch Gradient Norm after: 13.591002107688439
Epoch 3855/10000, Prediction Accuracy = 62.20400000000001%, Loss = 0.39020482897758485
Epoch: 3855, Batch Gradient Norm: 11.845719430176912
Epoch: 3855, Batch Gradient Norm after: 11.845719430176912
Epoch 3856/10000, Prediction Accuracy = 62.322%, Loss = 0.3872017920017242
Epoch: 3856, Batch Gradient Norm: 16.5130621550115
Epoch: 3856, Batch Gradient Norm after: 16.5130621550115
Epoch 3857/10000, Prediction Accuracy = 62.326%, Loss = 0.39390941262245177
Epoch: 3857, Batch Gradient Norm: 13.92673168275845
Epoch: 3857, Batch Gradient Norm after: 13.92673168275845
Epoch 3858/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.3922590970993042
Epoch: 3858, Batch Gradient Norm: 14.853345714078548
Epoch: 3858, Batch Gradient Norm after: 14.853345714078548
Epoch 3859/10000, Prediction Accuracy = 62.288%, Loss = 0.39372256994247434
Epoch: 3859, Batch Gradient Norm: 13.563543602547938
Epoch: 3859, Batch Gradient Norm after: 13.563543602547938
Epoch 3860/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.38931936025619507
Epoch: 3860, Batch Gradient Norm: 13.796235894739528
Epoch: 3860, Batch Gradient Norm after: 13.796235894739528
Epoch 3861/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.3933446526527405
Epoch: 3861, Batch Gradient Norm: 13.047382691304323
Epoch: 3861, Batch Gradient Norm after: 13.047382691304323
Epoch 3862/10000, Prediction Accuracy = 62.24399999999999%, Loss = 0.38955329060554506
Epoch: 3862, Batch Gradient Norm: 11.393822128242677
Epoch: 3862, Batch Gradient Norm after: 11.393822128242677
Epoch 3863/10000, Prediction Accuracy = 62.23%, Loss = 0.3850834727287292
Epoch: 3863, Batch Gradient Norm: 12.705948426379374
Epoch: 3863, Batch Gradient Norm after: 12.705948426379374
Epoch 3864/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.3896582007408142
Epoch: 3864, Batch Gradient Norm: 13.759480342744999
Epoch: 3864, Batch Gradient Norm after: 13.759480342744999
Epoch 3865/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.3880284190177917
Epoch: 3865, Batch Gradient Norm: 14.011887826366536
Epoch: 3865, Batch Gradient Norm after: 14.011887826366536
Epoch 3866/10000, Prediction Accuracy = 62.298%, Loss = 0.38977810740470886
Epoch: 3866, Batch Gradient Norm: 14.350371428650396
Epoch: 3866, Batch Gradient Norm after: 14.350371428650396
Epoch 3867/10000, Prediction Accuracy = 62.38599999999999%, Loss = 0.3909052312374115
Epoch: 3867, Batch Gradient Norm: 13.283907043975281
Epoch: 3867, Batch Gradient Norm after: 13.283907043975281
Epoch 3868/10000, Prediction Accuracy = 62.222%, Loss = 0.38831594586372375
Epoch: 3868, Batch Gradient Norm: 11.591180222336497
Epoch: 3868, Batch Gradient Norm after: 11.591180222336497
Epoch 3869/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.38699214458465575
Epoch: 3869, Batch Gradient Norm: 11.592320924465302
Epoch: 3869, Batch Gradient Norm after: 11.592320924465302
Epoch 3870/10000, Prediction Accuracy = 62.3%, Loss = 0.387039452791214
Epoch: 3870, Batch Gradient Norm: 13.329855212369415
Epoch: 3870, Batch Gradient Norm after: 13.329855212369415
Epoch 3871/10000, Prediction Accuracy = 62.294%, Loss = 0.3889961004257202
Epoch: 3871, Batch Gradient Norm: 15.109724178321406
Epoch: 3871, Batch Gradient Norm after: 15.109724178321406
Epoch 3872/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.3926894128322601
Epoch: 3872, Batch Gradient Norm: 13.377314903661958
Epoch: 3872, Batch Gradient Norm after: 13.377314903661958
Epoch 3873/10000, Prediction Accuracy = 62.202%, Loss = 0.38764455914497375
Epoch: 3873, Batch Gradient Norm: 12.708323413936496
Epoch: 3873, Batch Gradient Norm after: 12.708323413936496
Epoch 3874/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.38803953528404234
Epoch: 3874, Batch Gradient Norm: 12.37285664740959
Epoch: 3874, Batch Gradient Norm after: 12.37285664740959
Epoch 3875/10000, Prediction Accuracy = 62.278%, Loss = 0.38721056580543517
Epoch: 3875, Batch Gradient Norm: 10.41994945059367
Epoch: 3875, Batch Gradient Norm after: 10.41994945059367
Epoch 3876/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.3862256407737732
Epoch: 3876, Batch Gradient Norm: 10.63140560011162
Epoch: 3876, Batch Gradient Norm after: 10.63140560011162
Epoch 3877/10000, Prediction Accuracy = 62.314%, Loss = 0.38533568382263184
Epoch: 3877, Batch Gradient Norm: 11.300067836497066
Epoch: 3877, Batch Gradient Norm after: 11.300067836497066
Epoch 3878/10000, Prediction Accuracy = 62.33%, Loss = 0.3853332459926605
Epoch: 3878, Batch Gradient Norm: 11.736606384386844
Epoch: 3878, Batch Gradient Norm after: 11.736606384386844
Epoch 3879/10000, Prediction Accuracy = 62.37400000000001%, Loss = 0.3857323467731476
Epoch: 3879, Batch Gradient Norm: 12.25810252537138
Epoch: 3879, Batch Gradient Norm after: 12.25810252537138
Epoch 3880/10000, Prediction Accuracy = 62.15400000000001%, Loss = 0.38708900809288027
Epoch: 3880, Batch Gradient Norm: 15.076750205859707
Epoch: 3880, Batch Gradient Norm after: 15.076750205859707
Epoch 3881/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.3915840208530426
Epoch: 3881, Batch Gradient Norm: 10.634490131760536
Epoch: 3881, Batch Gradient Norm after: 10.634490131760536
Epoch 3882/10000, Prediction Accuracy = 62.286%, Loss = 0.38508598804473876
Epoch: 3882, Batch Gradient Norm: 13.091597224636276
Epoch: 3882, Batch Gradient Norm after: 13.091597224636276
Epoch 3883/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.3904715836048126
Epoch: 3883, Batch Gradient Norm: 11.8419869038193
Epoch: 3883, Batch Gradient Norm after: 11.8419869038193
Epoch 3884/10000, Prediction Accuracy = 62.330000000000005%, Loss = 0.3866704165935516
Epoch: 3884, Batch Gradient Norm: 10.399898211294731
Epoch: 3884, Batch Gradient Norm after: 10.399898211294731
Epoch 3885/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.3861788034439087
Epoch: 3885, Batch Gradient Norm: 9.101924290195512
Epoch: 3885, Batch Gradient Norm after: 9.101924290195512
Epoch 3886/10000, Prediction Accuracy = 62.234%, Loss = 0.3878594696521759
Epoch: 3886, Batch Gradient Norm: 8.81232074692443
Epoch: 3886, Batch Gradient Norm after: 8.81232074692443
Epoch 3887/10000, Prediction Accuracy = 62.3%, Loss = 0.3836730241775513
Epoch: 3887, Batch Gradient Norm: 9.700079751541459
Epoch: 3887, Batch Gradient Norm after: 9.700079751541459
Epoch 3888/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.3831749320030212
Epoch: 3888, Batch Gradient Norm: 9.77286777198601
Epoch: 3888, Batch Gradient Norm after: 9.77286777198601
Epoch 3889/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.38538066744804383
Epoch: 3889, Batch Gradient Norm: 9.714169143021808
Epoch: 3889, Batch Gradient Norm after: 9.714169143021808
Epoch 3890/10000, Prediction Accuracy = 62.31%, Loss = 0.3850728750228882
Epoch: 3890, Batch Gradient Norm: 11.475374434494462
Epoch: 3890, Batch Gradient Norm after: 11.475374434494462
Epoch 3891/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.38509983420372007
Epoch: 3891, Batch Gradient Norm: 11.451195968507701
Epoch: 3891, Batch Gradient Norm after: 11.451195968507701
Epoch 3892/10000, Prediction Accuracy = 62.366%, Loss = 0.3851251184940338
Epoch: 3892, Batch Gradient Norm: 12.365000698011087
Epoch: 3892, Batch Gradient Norm after: 12.365000698011087
Epoch 3893/10000, Prediction Accuracy = 62.242%, Loss = 0.39295593500137327
Epoch: 3893, Batch Gradient Norm: 14.483180138709875
Epoch: 3893, Batch Gradient Norm after: 14.483180138709875
Epoch 3894/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.3911100089550018
Epoch: 3894, Batch Gradient Norm: 14.055456009705352
Epoch: 3894, Batch Gradient Norm after: 14.055456009705352
Epoch 3895/10000, Prediction Accuracy = 62.31999999999999%, Loss = 0.39212522506713865
Epoch: 3895, Batch Gradient Norm: 11.12410026277588
Epoch: 3895, Batch Gradient Norm after: 11.12410026277588
Epoch 3896/10000, Prediction Accuracy = 62.288%, Loss = 0.38692315220832824
Epoch: 3896, Batch Gradient Norm: 10.534146693480947
Epoch: 3896, Batch Gradient Norm after: 10.534146693480947
Epoch 3897/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.38530372977256777
Epoch: 3897, Batch Gradient Norm: 14.210659887892152
Epoch: 3897, Batch Gradient Norm after: 14.210659887892152
Epoch 3898/10000, Prediction Accuracy = 62.146%, Loss = 0.3899340987205505
Epoch: 3898, Batch Gradient Norm: 16.283360748672592
Epoch: 3898, Batch Gradient Norm after: 16.283360748672592
Epoch 3899/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.39728249311447145
Epoch: 3899, Batch Gradient Norm: 14.668605726476756
Epoch: 3899, Batch Gradient Norm after: 14.668605726476756
Epoch 3900/10000, Prediction Accuracy = 62.36%, Loss = 0.3915486395359039
Epoch: 3900, Batch Gradient Norm: 15.255742915741004
Epoch: 3900, Batch Gradient Norm after: 15.255742915741004
Epoch 3901/10000, Prediction Accuracy = 62.374%, Loss = 0.39112367033958434
Epoch: 3901, Batch Gradient Norm: 15.012997618610123
Epoch: 3901, Batch Gradient Norm after: 15.012997618610123
Epoch 3902/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.38991618156433105
Epoch: 3902, Batch Gradient Norm: 14.028594506537384
Epoch: 3902, Batch Gradient Norm after: 14.028594506537384
Epoch 3903/10000, Prediction Accuracy = 62.202%, Loss = 0.39131134152412417
Epoch: 3903, Batch Gradient Norm: 13.468558827597171
Epoch: 3903, Batch Gradient Norm after: 13.468558827597171
Epoch 3904/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.38822328448295595
Epoch: 3904, Batch Gradient Norm: 14.251766120541273
Epoch: 3904, Batch Gradient Norm after: 14.251766120541273
Epoch 3905/10000, Prediction Accuracy = 62.214%, Loss = 0.38964650630950926
Epoch: 3905, Batch Gradient Norm: 13.007042501201381
Epoch: 3905, Batch Gradient Norm after: 13.007042501201381
Epoch 3906/10000, Prediction Accuracy = 62.326%, Loss = 0.38652573227882386
Epoch: 3906, Batch Gradient Norm: 14.274367237865073
Epoch: 3906, Batch Gradient Norm after: 14.274367237865073
Epoch 3907/10000, Prediction Accuracy = 62.324%, Loss = 0.3877253353595734
Epoch: 3907, Batch Gradient Norm: 11.079004364005815
Epoch: 3907, Batch Gradient Norm after: 11.079004364005815
Epoch 3908/10000, Prediction Accuracy = 62.324%, Loss = 0.3848201334476471
Epoch: 3908, Batch Gradient Norm: 12.226890195307977
Epoch: 3908, Batch Gradient Norm after: 12.226890195307977
Epoch 3909/10000, Prediction Accuracy = 62.33%, Loss = 0.3899086892604828
Epoch: 3909, Batch Gradient Norm: 13.562900764969228
Epoch: 3909, Batch Gradient Norm after: 13.562900764969228
Epoch 3910/10000, Prediction Accuracy = 62.38000000000001%, Loss = 0.39022287130355837
Epoch: 3910, Batch Gradient Norm: 12.573368639830978
Epoch: 3910, Batch Gradient Norm after: 12.573368639830978
Epoch 3911/10000, Prediction Accuracy = 62.336%, Loss = 0.3880526125431061
Epoch: 3911, Batch Gradient Norm: 16.105223082823546
Epoch: 3911, Batch Gradient Norm after: 16.105223082823546
Epoch 3912/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.3919563293457031
Epoch: 3912, Batch Gradient Norm: 17.222304765903026
Epoch: 3912, Batch Gradient Norm after: 17.222304765903026
Epoch 3913/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.39479456543922425
Epoch: 3913, Batch Gradient Norm: 14.31940381957432
Epoch: 3913, Batch Gradient Norm after: 14.31940381957432
Epoch 3914/10000, Prediction Accuracy = 62.35799999999999%, Loss = 0.388323712348938
Epoch: 3914, Batch Gradient Norm: 12.121973890532647
Epoch: 3914, Batch Gradient Norm after: 12.121973890532647
Epoch 3915/10000, Prediction Accuracy = 62.222%, Loss = 0.3859414041042328
Epoch: 3915, Batch Gradient Norm: 11.093555953176773
Epoch: 3915, Batch Gradient Norm after: 11.093555953176773
Epoch 3916/10000, Prediction Accuracy = 62.182%, Loss = 0.38566269278526305
Epoch: 3916, Batch Gradient Norm: 10.57582519704519
Epoch: 3916, Batch Gradient Norm after: 10.57582519704519
Epoch 3917/10000, Prediction Accuracy = 62.46%, Loss = 0.3845295548439026
Epoch: 3917, Batch Gradient Norm: 12.898017899708293
Epoch: 3917, Batch Gradient Norm after: 12.898017899708293
Epoch 3918/10000, Prediction Accuracy = 62.23%, Loss = 0.38684225678443906
Epoch: 3918, Batch Gradient Norm: 14.697491074697133
Epoch: 3918, Batch Gradient Norm after: 14.697491074697133
Epoch 3919/10000, Prediction Accuracy = 62.222%, Loss = 0.3904495000839233
Epoch: 3919, Batch Gradient Norm: 12.47146204936909
Epoch: 3919, Batch Gradient Norm after: 12.47146204936909
Epoch 3920/10000, Prediction Accuracy = 62.234%, Loss = 0.386683851480484
Epoch: 3920, Batch Gradient Norm: 15.672866590191639
Epoch: 3920, Batch Gradient Norm after: 15.672866590191639
Epoch 3921/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.39392260909080506
Epoch: 3921, Batch Gradient Norm: 17.446653505243642
Epoch: 3921, Batch Gradient Norm after: 17.446653505243642
Epoch 3922/10000, Prediction Accuracy = 62.348%, Loss = 0.3935131847858429
Epoch: 3922, Batch Gradient Norm: 14.771285648060605
Epoch: 3922, Batch Gradient Norm after: 14.771285648060605
Epoch 3923/10000, Prediction Accuracy = 62.31%, Loss = 0.3881851673126221
Epoch: 3923, Batch Gradient Norm: 12.895670498161335
Epoch: 3923, Batch Gradient Norm after: 12.895670498161335
Epoch 3924/10000, Prediction Accuracy = 62.314%, Loss = 0.387135249376297
Epoch: 3924, Batch Gradient Norm: 12.087528661351136
Epoch: 3924, Batch Gradient Norm after: 12.087528661351136
Epoch 3925/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.38487015962600707
Epoch: 3925, Batch Gradient Norm: 13.46971255778639
Epoch: 3925, Batch Gradient Norm after: 13.46971255778639
Epoch 3926/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.38854941725730896
Epoch: 3926, Batch Gradient Norm: 16.152300668437995
Epoch: 3926, Batch Gradient Norm after: 15.38053540960851
Epoch 3927/10000, Prediction Accuracy = 62.378%, Loss = 0.3949446201324463
Epoch: 3927, Batch Gradient Norm: 17.04300254657503
Epoch: 3927, Batch Gradient Norm after: 17.04300254657503
Epoch 3928/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.3940302193164825
Epoch: 3928, Batch Gradient Norm: 15.668663970951336
Epoch: 3928, Batch Gradient Norm after: 15.668663970951336
Epoch 3929/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.3924281120300293
Epoch: 3929, Batch Gradient Norm: 15.655673575669748
Epoch: 3929, Batch Gradient Norm after: 15.655673575669748
Epoch 3930/10000, Prediction Accuracy = 62.348%, Loss = 0.3913536429405212
Epoch: 3930, Batch Gradient Norm: 15.311811057337982
Epoch: 3930, Batch Gradient Norm after: 15.311811057337982
Epoch 3931/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.3903278410434723
Epoch: 3931, Batch Gradient Norm: 13.443068977634683
Epoch: 3931, Batch Gradient Norm after: 13.443068977634683
Epoch 3932/10000, Prediction Accuracy = 62.33%, Loss = 0.3880932033061981
Epoch: 3932, Batch Gradient Norm: 12.550381308204937
Epoch: 3932, Batch Gradient Norm after: 12.550381308204937
Epoch 3933/10000, Prediction Accuracy = 62.31%, Loss = 0.38732980489730834
Epoch: 3933, Batch Gradient Norm: 9.085331751865178
Epoch: 3933, Batch Gradient Norm after: 9.085331751865178
Epoch 3934/10000, Prediction Accuracy = 62.263999999999996%, Loss = 0.382020628452301
Epoch: 3934, Batch Gradient Norm: 8.377086587610542
Epoch: 3934, Batch Gradient Norm after: 8.377086587610542
Epoch 3935/10000, Prediction Accuracy = 62.33599999999999%, Loss = 0.3814274430274963
Epoch: 3935, Batch Gradient Norm: 10.930281706556185
Epoch: 3935, Batch Gradient Norm after: 10.930281706556185
Epoch 3936/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.3830873966217041
Epoch: 3936, Batch Gradient Norm: 9.99346966276024
Epoch: 3936, Batch Gradient Norm after: 9.99346966276024
Epoch 3937/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.38230634331703184
Epoch: 3937, Batch Gradient Norm: 10.04876497266573
Epoch: 3937, Batch Gradient Norm after: 10.04876497266573
Epoch 3938/10000, Prediction Accuracy = 62.374%, Loss = 0.3838539123535156
Epoch: 3938, Batch Gradient Norm: 10.784460434007547
Epoch: 3938, Batch Gradient Norm after: 10.784460434007547
Epoch 3939/10000, Prediction Accuracy = 62.274%, Loss = 0.3826973021030426
Epoch: 3939, Batch Gradient Norm: 8.679224042290787
Epoch: 3939, Batch Gradient Norm after: 8.679224042290787
Epoch 3940/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.3802061676979065
Epoch: 3940, Batch Gradient Norm: 7.813081892910351
Epoch: 3940, Batch Gradient Norm after: 7.813081892910351
Epoch 3941/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.3799440860748291
Epoch: 3941, Batch Gradient Norm: 9.579741162548368
Epoch: 3941, Batch Gradient Norm after: 9.579741162548368
Epoch 3942/10000, Prediction Accuracy = 62.334%, Loss = 0.38233253359794617
Epoch: 3942, Batch Gradient Norm: 11.125245175298641
Epoch: 3942, Batch Gradient Norm after: 11.125245175298641
Epoch 3943/10000, Prediction Accuracy = 62.254%, Loss = 0.38535378575325013
Epoch: 3943, Batch Gradient Norm: 12.461077652672468
Epoch: 3943, Batch Gradient Norm after: 12.461077652672468
Epoch 3944/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.38589749336242674
Epoch: 3944, Batch Gradient Norm: 15.02572361434109
Epoch: 3944, Batch Gradient Norm after: 15.02572361434109
Epoch 3945/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.390135669708252
Epoch: 3945, Batch Gradient Norm: 15.715453776748943
Epoch: 3945, Batch Gradient Norm after: 15.715453776748943
Epoch 3946/10000, Prediction Accuracy = 62.214%, Loss = 0.3920864284038544
Epoch: 3946, Batch Gradient Norm: 17.791472161664203
Epoch: 3946, Batch Gradient Norm after: 17.151468734992932
Epoch 3947/10000, Prediction Accuracy = 62.222%, Loss = 0.39447363615036013
Epoch: 3947, Batch Gradient Norm: 17.016880143393134
Epoch: 3947, Batch Gradient Norm after: 17.016880143393134
Epoch 3948/10000, Prediction Accuracy = 62.254%, Loss = 0.3916878759860992
Epoch: 3948, Batch Gradient Norm: 15.46734922127282
Epoch: 3948, Batch Gradient Norm after: 15.46734922127282
Epoch 3949/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.3895210921764374
Epoch: 3949, Batch Gradient Norm: 14.128419718141538
Epoch: 3949, Batch Gradient Norm after: 14.128419718141538
Epoch 3950/10000, Prediction Accuracy = 62.468%, Loss = 0.38803977966308595
Epoch: 3950, Batch Gradient Norm: 11.736467188501335
Epoch: 3950, Batch Gradient Norm after: 11.736467188501335
Epoch 3951/10000, Prediction Accuracy = 62.15400000000001%, Loss = 0.3876201272010803
Epoch: 3951, Batch Gradient Norm: 14.24834132116151
Epoch: 3951, Batch Gradient Norm after: 14.24834132116151
Epoch 3952/10000, Prediction Accuracy = 62.35%, Loss = 0.38981889486312865
Epoch: 3952, Batch Gradient Norm: 12.170642937519174
Epoch: 3952, Batch Gradient Norm after: 12.170642937519174
Epoch 3953/10000, Prediction Accuracy = 62.25%, Loss = 0.3851765334606171
Epoch: 3953, Batch Gradient Norm: 12.413142106344308
Epoch: 3953, Batch Gradient Norm after: 12.413142106344308
Epoch 3954/10000, Prediction Accuracy = 62.286%, Loss = 0.38586090207099916
Epoch: 3954, Batch Gradient Norm: 8.71915467545043
Epoch: 3954, Batch Gradient Norm after: 8.71915467545043
Epoch 3955/10000, Prediction Accuracy = 62.234%, Loss = 0.38084354996681213
Epoch: 3955, Batch Gradient Norm: 7.687305265025061
Epoch: 3955, Batch Gradient Norm after: 7.687305265025061
Epoch 3956/10000, Prediction Accuracy = 62.324%, Loss = 0.3789899408817291
Epoch: 3956, Batch Gradient Norm: 10.181152397730896
Epoch: 3956, Batch Gradient Norm after: 10.181152397730896
Epoch 3957/10000, Prediction Accuracy = 62.245999999999995%, Loss = 0.38180909752845765
Epoch: 3957, Batch Gradient Norm: 14.425502931013042
Epoch: 3957, Batch Gradient Norm after: 14.425502931013042
Epoch 3958/10000, Prediction Accuracy = 62.462%, Loss = 0.38858076333999636
Epoch: 3958, Batch Gradient Norm: 17.63080746636078
Epoch: 3958, Batch Gradient Norm after: 17.63080746636078
Epoch 3959/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.39328227043151853
Epoch: 3959, Batch Gradient Norm: 15.824041870808776
Epoch: 3959, Batch Gradient Norm after: 15.824041870808776
Epoch 3960/10000, Prediction Accuracy = 62.27%, Loss = 0.3886392116546631
Epoch: 3960, Batch Gradient Norm: 15.350281733879106
Epoch: 3960, Batch Gradient Norm after: 15.350281733879106
Epoch 3961/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.39022072553634646
Epoch: 3961, Batch Gradient Norm: 14.682838556762523
Epoch: 3961, Batch Gradient Norm after: 14.681405290315238
Epoch 3962/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.3866969466209412
Epoch: 3962, Batch Gradient Norm: 14.204530977669272
Epoch: 3962, Batch Gradient Norm after: 14.204530977669272
Epoch 3963/10000, Prediction Accuracy = 62.29%, Loss = 0.38657912611961365
Epoch: 3963, Batch Gradient Norm: 12.526581210903887
Epoch: 3963, Batch Gradient Norm after: 12.526581210903887
Epoch 3964/10000, Prediction Accuracy = 62.25%, Loss = 0.38593496680259703
Epoch: 3964, Batch Gradient Norm: 10.944453504521164
Epoch: 3964, Batch Gradient Norm after: 10.944453504521164
Epoch 3965/10000, Prediction Accuracy = 62.27%, Loss = 0.38381811380386355
Epoch: 3965, Batch Gradient Norm: 11.704031331590452
Epoch: 3965, Batch Gradient Norm after: 11.704031331590452
Epoch 3966/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.3843501329421997
Epoch: 3966, Batch Gradient Norm: 11.566354861114167
Epoch: 3966, Batch Gradient Norm after: 11.566354861114167
Epoch 3967/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.382970404624939
Epoch: 3967, Batch Gradient Norm: 11.18861798577573
Epoch: 3967, Batch Gradient Norm after: 11.18861798577573
Epoch 3968/10000, Prediction Accuracy = 62.288%, Loss = 0.3839577078819275
Epoch: 3968, Batch Gradient Norm: 11.983859772936736
Epoch: 3968, Batch Gradient Norm after: 11.983859772936736
Epoch 3969/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.3856223404407501
Epoch: 3969, Batch Gradient Norm: 12.172935462554735
Epoch: 3969, Batch Gradient Norm after: 12.172935462554735
Epoch 3970/10000, Prediction Accuracy = 62.202%, Loss = 0.38544034361839297
Epoch: 3970, Batch Gradient Norm: 14.083129743695814
Epoch: 3970, Batch Gradient Norm after: 14.083129743695814
Epoch 3971/10000, Prediction Accuracy = 62.348%, Loss = 0.38660852909088134
Epoch: 3971, Batch Gradient Norm: 12.595413295032971
Epoch: 3971, Batch Gradient Norm after: 12.595413295032971
Epoch 3972/10000, Prediction Accuracy = 62.29%, Loss = 0.38571404814720156
Epoch: 3972, Batch Gradient Norm: 10.962726841386392
Epoch: 3972, Batch Gradient Norm after: 10.962726841386392
Epoch 3973/10000, Prediction Accuracy = 62.370000000000005%, Loss = 0.3834379553794861
Epoch: 3973, Batch Gradient Norm: 11.261431942882034
Epoch: 3973, Batch Gradient Norm after: 11.261431942882034
Epoch 3974/10000, Prediction Accuracy = 62.286%, Loss = 0.3829689800739288
Epoch: 3974, Batch Gradient Norm: 11.657840291508503
Epoch: 3974, Batch Gradient Norm after: 11.657840291508503
Epoch 3975/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.3821876525878906
Epoch: 3975, Batch Gradient Norm: 12.996380989845122
Epoch: 3975, Batch Gradient Norm after: 12.996380989845122
Epoch 3976/10000, Prediction Accuracy = 62.348%, Loss = 0.3852430164813995
Epoch: 3976, Batch Gradient Norm: 11.585442761834434
Epoch: 3976, Batch Gradient Norm after: 11.585442761834434
Epoch 3977/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.38235245943069457
Epoch: 3977, Batch Gradient Norm: 12.345010004832195
Epoch: 3977, Batch Gradient Norm after: 12.345010004832195
Epoch 3978/10000, Prediction Accuracy = 62.27%, Loss = 0.38748964071273806
Epoch: 3978, Batch Gradient Norm: 10.97663323752107
Epoch: 3978, Batch Gradient Norm after: 10.97663323752107
Epoch 3979/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.3812653422355652
Epoch: 3979, Batch Gradient Norm: 11.425553038415782
Epoch: 3979, Batch Gradient Norm after: 11.425553038415782
Epoch 3980/10000, Prediction Accuracy = 62.402%, Loss = 0.3816054105758667
Epoch: 3980, Batch Gradient Norm: 13.926945350341752
Epoch: 3980, Batch Gradient Norm after: 13.926945350341752
Epoch 3981/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.38652026653289795
Epoch: 3981, Batch Gradient Norm: 14.161024918485694
Epoch: 3981, Batch Gradient Norm after: 14.161024918485694
Epoch 3982/10000, Prediction Accuracy = 62.35%, Loss = 0.38868439197540283
Epoch: 3982, Batch Gradient Norm: 15.057445152727889
Epoch: 3982, Batch Gradient Norm after: 15.057445152727889
Epoch 3983/10000, Prediction Accuracy = 62.366%, Loss = 0.3875965178012848
Epoch: 3983, Batch Gradient Norm: 16.306765103978236
Epoch: 3983, Batch Gradient Norm after: 16.306765103978236
Epoch 3984/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.3898235559463501
Epoch: 3984, Batch Gradient Norm: 15.899032096920584
Epoch: 3984, Batch Gradient Norm after: 15.899032096920584
Epoch 3985/10000, Prediction Accuracy = 62.23%, Loss = 0.3912147581577301
Epoch: 3985, Batch Gradient Norm: 14.908041470568346
Epoch: 3985, Batch Gradient Norm after: 14.908041470568346
Epoch 3986/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.38654226064682007
Epoch: 3986, Batch Gradient Norm: 12.727871398897106
Epoch: 3986, Batch Gradient Norm after: 12.727871398897106
Epoch 3987/10000, Prediction Accuracy = 62.374%, Loss = 0.3874216556549072
Epoch: 3987, Batch Gradient Norm: 10.98125230419218
Epoch: 3987, Batch Gradient Norm after: 10.98125230419218
Epoch 3988/10000, Prediction Accuracy = 62.306%, Loss = 0.3815037727355957
Epoch: 3988, Batch Gradient Norm: 13.257105261363254
Epoch: 3988, Batch Gradient Norm after: 13.257105261363254
Epoch 3989/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.38548778295516967
Epoch: 3989, Batch Gradient Norm: 11.264707598807162
Epoch: 3989, Batch Gradient Norm after: 11.264707598807162
Epoch 3990/10000, Prediction Accuracy = 62.364%, Loss = 0.3818951189517975
Epoch: 3990, Batch Gradient Norm: 10.310916148734725
Epoch: 3990, Batch Gradient Norm after: 10.310916148734725
Epoch 3991/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.38246641755104066
Epoch: 3991, Batch Gradient Norm: 11.003893969430724
Epoch: 3991, Batch Gradient Norm after: 11.003893969430724
Epoch 3992/10000, Prediction Accuracy = 62.27%, Loss = 0.38291155099868773
Epoch: 3992, Batch Gradient Norm: 11.897757016908477
Epoch: 3992, Batch Gradient Norm after: 11.897757016908477
Epoch 3993/10000, Prediction Accuracy = 62.364%, Loss = 0.3834182918071747
Epoch: 3993, Batch Gradient Norm: 12.62325499446285
Epoch: 3993, Batch Gradient Norm after: 12.62325499446285
Epoch 3994/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.38558310866355894
Epoch: 3994, Batch Gradient Norm: 12.166459811305621
Epoch: 3994, Batch Gradient Norm after: 12.166459811305621
Epoch 3995/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.3831807792186737
Epoch: 3995, Batch Gradient Norm: 12.000221859611317
Epoch: 3995, Batch Gradient Norm after: 12.000221859611317
Epoch 3996/10000, Prediction Accuracy = 62.262%, Loss = 0.38325863480567934
Epoch: 3996, Batch Gradient Norm: 12.829225698296547
Epoch: 3996, Batch Gradient Norm after: 12.829225698296547
Epoch 3997/10000, Prediction Accuracy = 62.386%, Loss = 0.3857563555240631
Epoch: 3997, Batch Gradient Norm: 12.282195104709446
Epoch: 3997, Batch Gradient Norm after: 12.282195104709446
Epoch 3998/10000, Prediction Accuracy = 62.29%, Loss = 0.3849846959114075
Epoch: 3998, Batch Gradient Norm: 13.49847290263992
Epoch: 3998, Batch Gradient Norm after: 13.49847290263992
Epoch 3999/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.38464773297309873
Epoch: 3999, Batch Gradient Norm: 14.446767636583845
Epoch: 3999, Batch Gradient Norm after: 14.446767636583845
Epoch 4000/10000, Prediction Accuracy = 62.438%, Loss = 0.3869291126728058
Epoch: 4000, Batch Gradient Norm: 13.133144345724407
Epoch: 4000, Batch Gradient Norm after: 13.133144345724407
Epoch 4001/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.38626623153686523
Epoch: 4001, Batch Gradient Norm: 11.526043194166663
Epoch: 4001, Batch Gradient Norm after: 11.526043194166663
Epoch 4002/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.3829336166381836
Epoch: 4002, Batch Gradient Norm: 14.355363116900634
Epoch: 4002, Batch Gradient Norm after: 14.355363116900634
Epoch 4003/10000, Prediction Accuracy = 62.388%, Loss = 0.38670321106910704
Epoch: 4003, Batch Gradient Norm: 14.540447240142592
Epoch: 4003, Batch Gradient Norm after: 14.540447240142592
Epoch 4004/10000, Prediction Accuracy = 62.408%, Loss = 0.3859215319156647
Epoch: 4004, Batch Gradient Norm: 14.043877743096306
Epoch: 4004, Batch Gradient Norm after: 14.043877743096306
Epoch 4005/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.38432554602622987
Epoch: 4005, Batch Gradient Norm: 13.144867061081621
Epoch: 4005, Batch Gradient Norm after: 13.144867061081621
Epoch 4006/10000, Prediction Accuracy = 62.36999999999999%, Loss = 0.38367798924446106
Epoch: 4006, Batch Gradient Norm: 12.826607745200054
Epoch: 4006, Batch Gradient Norm after: 12.826607745200054
Epoch 4007/10000, Prediction Accuracy = 62.386%, Loss = 0.3882420539855957
Epoch: 4007, Batch Gradient Norm: 16.252639266016043
Epoch: 4007, Batch Gradient Norm after: 16.252639266016043
Epoch 4008/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.3921890318393707
Epoch: 4008, Batch Gradient Norm: 15.835077175588196
Epoch: 4008, Batch Gradient Norm after: 15.835077175588196
Epoch 4009/10000, Prediction Accuracy = 62.318%, Loss = 0.3894573628902435
Epoch: 4009, Batch Gradient Norm: 13.85885657922922
Epoch: 4009, Batch Gradient Norm after: 13.85885657922922
Epoch 4010/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.3853473961353302
Epoch: 4010, Batch Gradient Norm: 13.05685965595787
Epoch: 4010, Batch Gradient Norm after: 13.05685965595787
Epoch 4011/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.3858018219470978
Epoch: 4011, Batch Gradient Norm: 12.624028292757677
Epoch: 4011, Batch Gradient Norm after: 12.624028292757677
Epoch 4012/10000, Prediction Accuracy = 62.3%, Loss = 0.3837925851345062
Epoch: 4012, Batch Gradient Norm: 10.74030636282943
Epoch: 4012, Batch Gradient Norm after: 10.74030636282943
Epoch 4013/10000, Prediction Accuracy = 62.312%, Loss = 0.38017197847366335
Epoch: 4013, Batch Gradient Norm: 12.748976961924027
Epoch: 4013, Batch Gradient Norm after: 12.748976961924027
Epoch 4014/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.3854540526866913
Epoch: 4014, Batch Gradient Norm: 13.499040254496519
Epoch: 4014, Batch Gradient Norm after: 13.499040254496519
Epoch 4015/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.3844838201999664
Epoch: 4015, Batch Gradient Norm: 13.466542056558913
Epoch: 4015, Batch Gradient Norm after: 13.466542056558913
Epoch 4016/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.3878353118896484
Epoch: 4016, Batch Gradient Norm: 15.736526836159149
Epoch: 4016, Batch Gradient Norm after: 15.736526836159149
Epoch 4017/10000, Prediction Accuracy = 62.312%, Loss = 0.38789219260215757
Epoch: 4017, Batch Gradient Norm: 15.631204827865366
Epoch: 4017, Batch Gradient Norm after: 15.631204827865366
Epoch 4018/10000, Prediction Accuracy = 62.45%, Loss = 0.3890077233314514
Epoch: 4018, Batch Gradient Norm: 13.061282786344442
Epoch: 4018, Batch Gradient Norm after: 13.061282786344442
Epoch 4019/10000, Prediction Accuracy = 62.39%, Loss = 0.3837083101272583
Epoch: 4019, Batch Gradient Norm: 11.130990978687338
Epoch: 4019, Batch Gradient Norm after: 11.130990978687338
Epoch 4020/10000, Prediction Accuracy = 62.39%, Loss = 0.3822347819805145
Epoch: 4020, Batch Gradient Norm: 12.308725555046038
Epoch: 4020, Batch Gradient Norm after: 12.308725555046038
Epoch 4021/10000, Prediction Accuracy = 62.414%, Loss = 0.3833233594894409
Epoch: 4021, Batch Gradient Norm: 9.984586333165675
Epoch: 4021, Batch Gradient Norm after: 9.984586333165675
Epoch 4022/10000, Prediction Accuracy = 62.31%, Loss = 0.37865020632743834
Epoch: 4022, Batch Gradient Norm: 11.224293485492705
Epoch: 4022, Batch Gradient Norm after: 11.224293485492705
Epoch 4023/10000, Prediction Accuracy = 62.396%, Loss = 0.3834425449371338
Epoch: 4023, Batch Gradient Norm: 15.002492431801707
Epoch: 4023, Batch Gradient Norm after: 15.002492431801707
Epoch 4024/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.38640283346176146
Epoch: 4024, Batch Gradient Norm: 16.587983606731918
Epoch: 4024, Batch Gradient Norm after: 16.587983606731918
Epoch 4025/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.388854044675827
Epoch: 4025, Batch Gradient Norm: 14.663587538691369
Epoch: 4025, Batch Gradient Norm after: 14.663587538691369
Epoch 4026/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.38882548809051515
Epoch: 4026, Batch Gradient Norm: 12.270841800407299
Epoch: 4026, Batch Gradient Norm after: 12.270841800407299
Epoch 4027/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.38198179602622984
Epoch: 4027, Batch Gradient Norm: 16.099191028632376
Epoch: 4027, Batch Gradient Norm after: 16.099191028632376
Epoch 4028/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.390722918510437
Epoch: 4028, Batch Gradient Norm: 11.527694114228378
Epoch: 4028, Batch Gradient Norm after: 11.527694114228378
Epoch 4029/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.383055317401886
Epoch: 4029, Batch Gradient Norm: 12.39291523959132
Epoch: 4029, Batch Gradient Norm after: 12.39291523959132
Epoch 4030/10000, Prediction Accuracy = 62.364%, Loss = 0.3826975166797638
Epoch: 4030, Batch Gradient Norm: 13.088156307710305
Epoch: 4030, Batch Gradient Norm after: 13.088156307710305
Epoch 4031/10000, Prediction Accuracy = 62.326%, Loss = 0.3826000809669495
Epoch: 4031, Batch Gradient Norm: 11.126992545216833
Epoch: 4031, Batch Gradient Norm after: 11.126992545216833
Epoch 4032/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.3807743191719055
Epoch: 4032, Batch Gradient Norm: 9.773301213787546
Epoch: 4032, Batch Gradient Norm after: 9.773301213787546
Epoch 4033/10000, Prediction Accuracy = 62.342000000000006%, Loss = 0.379634416103363
Epoch: 4033, Batch Gradient Norm: 9.570934873384568
Epoch: 4033, Batch Gradient Norm after: 9.570934873384568
Epoch 4034/10000, Prediction Accuracy = 62.412%, Loss = 0.37879046201705935
Epoch: 4034, Batch Gradient Norm: 11.322535675663218
Epoch: 4034, Batch Gradient Norm after: 11.322535675663218
Epoch 4035/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.38195335268974306
Epoch: 4035, Batch Gradient Norm: 12.856065653783098
Epoch: 4035, Batch Gradient Norm after: 12.856065653783098
Epoch 4036/10000, Prediction Accuracy = 62.366%, Loss = 0.3847123980522156
Epoch: 4036, Batch Gradient Norm: 10.676867519870145
Epoch: 4036, Batch Gradient Norm after: 10.676867519870145
Epoch 4037/10000, Prediction Accuracy = 62.448%, Loss = 0.38116012811660765
Epoch: 4037, Batch Gradient Norm: 13.164358506592604
Epoch: 4037, Batch Gradient Norm after: 13.164358506592604
Epoch 4038/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.38346577286720274
Epoch: 4038, Batch Gradient Norm: 11.835067156065216
Epoch: 4038, Batch Gradient Norm after: 11.835067156065216
Epoch 4039/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.38124255537986756
Epoch: 4039, Batch Gradient Norm: 9.797290728726404
Epoch: 4039, Batch Gradient Norm after: 9.797290728726404
Epoch 4040/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.38001789450645446
Epoch: 4040, Batch Gradient Norm: 9.96767429646145
Epoch: 4040, Batch Gradient Norm after: 9.96767429646145
Epoch 4041/10000, Prediction Accuracy = 62.32199999999999%, Loss = 0.3776177644729614
Epoch: 4041, Batch Gradient Norm: 10.064010627059085
Epoch: 4041, Batch Gradient Norm after: 10.064010627059085
Epoch 4042/10000, Prediction Accuracy = 62.266%, Loss = 0.3790490090847015
Epoch: 4042, Batch Gradient Norm: 11.180785638970953
Epoch: 4042, Batch Gradient Norm after: 11.180785638970953
Epoch 4043/10000, Prediction Accuracy = 62.412%, Loss = 0.381215900182724
Epoch: 4043, Batch Gradient Norm: 11.322601679541632
Epoch: 4043, Batch Gradient Norm after: 11.322601679541632
Epoch 4044/10000, Prediction Accuracy = 62.462%, Loss = 0.3790196061134338
Epoch: 4044, Batch Gradient Norm: 13.051154460955516
Epoch: 4044, Batch Gradient Norm after: 13.051154460955516
Epoch 4045/10000, Prediction Accuracy = 62.372%, Loss = 0.382335764169693
Epoch: 4045, Batch Gradient Norm: 14.105843576169498
Epoch: 4045, Batch Gradient Norm after: 14.105843576169498
Epoch 4046/10000, Prediction Accuracy = 62.378%, Loss = 0.3824321150779724
Epoch: 4046, Batch Gradient Norm: 17.630327374819952
Epoch: 4046, Batch Gradient Norm after: 17.35274121588191
Epoch 4047/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.38897123336791994
Epoch: 4047, Batch Gradient Norm: 15.048018165900517
Epoch: 4047, Batch Gradient Norm after: 15.048018165900517
Epoch 4048/10000, Prediction Accuracy = 62.424%, Loss = 0.38461499810218813
Epoch: 4048, Batch Gradient Norm: 17.741855540730178
Epoch: 4048, Batch Gradient Norm after: 17.12714557590316
Epoch 4049/10000, Prediction Accuracy = 62.33%, Loss = 0.38864932060241697
Epoch: 4049, Batch Gradient Norm: 13.946225158487273
Epoch: 4049, Batch Gradient Norm after: 13.946225158487273
Epoch 4050/10000, Prediction Accuracy = 62.348%, Loss = 0.3858591139316559
Epoch: 4050, Batch Gradient Norm: 17.990190291060628
Epoch: 4050, Batch Gradient Norm after: 17.990190291060628
Epoch 4051/10000, Prediction Accuracy = 62.322%, Loss = 0.390265679359436
Epoch: 4051, Batch Gradient Norm: 15.483513999984766
Epoch: 4051, Batch Gradient Norm after: 15.483513999984766
Epoch 4052/10000, Prediction Accuracy = 62.396%, Loss = 0.38801183104515075
Epoch: 4052, Batch Gradient Norm: 13.60041991427532
Epoch: 4052, Batch Gradient Norm after: 13.60041991427532
Epoch 4053/10000, Prediction Accuracy = 62.326%, Loss = 0.38245858550071715
Epoch: 4053, Batch Gradient Norm: 12.409768093013723
Epoch: 4053, Batch Gradient Norm after: 12.409768093013723
Epoch 4054/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.3844262361526489
Epoch: 4054, Batch Gradient Norm: 14.60458818261383
Epoch: 4054, Batch Gradient Norm after: 14.60458818261383
Epoch 4055/10000, Prediction Accuracy = 62.262%, Loss = 0.3850592076778412
Epoch: 4055, Batch Gradient Norm: 14.897796130004721
Epoch: 4055, Batch Gradient Norm after: 14.897796130004721
Epoch 4056/10000, Prediction Accuracy = 62.278%, Loss = 0.3845792531967163
Epoch: 4056, Batch Gradient Norm: 16.503862985215054
Epoch: 4056, Batch Gradient Norm after: 16.091208097850423
Epoch 4057/10000, Prediction Accuracy = 62.44%, Loss = 0.38778017163276673
Epoch: 4057, Batch Gradient Norm: 14.978039925844065
Epoch: 4057, Batch Gradient Norm after: 14.978039925844065
Epoch 4058/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.3859691798686981
Epoch: 4058, Batch Gradient Norm: 14.145897698643068
Epoch: 4058, Batch Gradient Norm after: 14.145897698643068
Epoch 4059/10000, Prediction Accuracy = 62.384%, Loss = 0.38163082003593446
Epoch: 4059, Batch Gradient Norm: 14.163079344009677
Epoch: 4059, Batch Gradient Norm after: 14.163079344009677
Epoch 4060/10000, Prediction Accuracy = 62.428%, Loss = 0.3840664982795715
Epoch: 4060, Batch Gradient Norm: 11.273239755701331
Epoch: 4060, Batch Gradient Norm after: 11.273239755701331
Epoch 4061/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.3822230875492096
Epoch: 4061, Batch Gradient Norm: 11.8493980887574
Epoch: 4061, Batch Gradient Norm after: 11.8493980887574
Epoch 4062/10000, Prediction Accuracy = 62.398%, Loss = 0.3795605540275574
Epoch: 4062, Batch Gradient Norm: 13.179534588745826
Epoch: 4062, Batch Gradient Norm after: 13.179534588745826
Epoch 4063/10000, Prediction Accuracy = 62.314%, Loss = 0.382767653465271
Epoch: 4063, Batch Gradient Norm: 11.040819143184143
Epoch: 4063, Batch Gradient Norm after: 11.040819143184143
Epoch 4064/10000, Prediction Accuracy = 62.408%, Loss = 0.37991599440574647
Epoch: 4064, Batch Gradient Norm: 10.605846609731643
Epoch: 4064, Batch Gradient Norm after: 10.605846609731643
Epoch 4065/10000, Prediction Accuracy = 62.318%, Loss = 0.3796573102474213
Epoch: 4065, Batch Gradient Norm: 14.536902680089772
Epoch: 4065, Batch Gradient Norm after: 14.536902680089772
Epoch 4066/10000, Prediction Accuracy = 62.424%, Loss = 0.38712419867515563
Epoch: 4066, Batch Gradient Norm: 14.47640864331508
Epoch: 4066, Batch Gradient Norm after: 14.47640864331508
Epoch 4067/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.3874417424201965
Epoch: 4067, Batch Gradient Norm: 14.185614897783012
Epoch: 4067, Batch Gradient Norm after: 14.185614897783012
Epoch 4068/10000, Prediction Accuracy = 62.476%, Loss = 0.3840146422386169
Epoch: 4068, Batch Gradient Norm: 11.316698934195795
Epoch: 4068, Batch Gradient Norm after: 11.316698934195795
Epoch 4069/10000, Prediction Accuracy = 62.464%, Loss = 0.38056031465530393
Epoch: 4069, Batch Gradient Norm: 10.38258230016703
Epoch: 4069, Batch Gradient Norm after: 10.38258230016703
Epoch 4070/10000, Prediction Accuracy = 62.354%, Loss = 0.3792675375938416
Epoch: 4070, Batch Gradient Norm: 13.565774004809048
Epoch: 4070, Batch Gradient Norm after: 13.565774004809048
Epoch 4071/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.3830149173736572
Epoch: 4071, Batch Gradient Norm: 12.437919534637361
Epoch: 4071, Batch Gradient Norm after: 12.437919534637361
Epoch 4072/10000, Prediction Accuracy = 62.462%, Loss = 0.38273277282714846
Epoch: 4072, Batch Gradient Norm: 10.792003956491955
Epoch: 4072, Batch Gradient Norm after: 10.792003956491955
Epoch 4073/10000, Prediction Accuracy = 62.376%, Loss = 0.38099135756492614
Epoch: 4073, Batch Gradient Norm: 12.89037976313562
Epoch: 4073, Batch Gradient Norm after: 12.89037976313562
Epoch 4074/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.38177133798599244
Epoch: 4074, Batch Gradient Norm: 13.748132103170327
Epoch: 4074, Batch Gradient Norm after: 13.748132103170327
Epoch 4075/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.38121321201324465
Epoch: 4075, Batch Gradient Norm: 13.950284671135542
Epoch: 4075, Batch Gradient Norm after: 13.950284671135542
Epoch 4076/10000, Prediction Accuracy = 62.286%, Loss = 0.38432635068893434
Epoch: 4076, Batch Gradient Norm: 14.65331553178644
Epoch: 4076, Batch Gradient Norm after: 14.65331553178644
Epoch 4077/10000, Prediction Accuracy = 62.44%, Loss = 0.38679436445236204
Epoch: 4077, Batch Gradient Norm: 14.699343314177991
Epoch: 4077, Batch Gradient Norm after: 14.699343314177991
Epoch 4078/10000, Prediction Accuracy = 62.35%, Loss = 0.3840237855911255
Epoch: 4078, Batch Gradient Norm: 14.288989367153388
Epoch: 4078, Batch Gradient Norm after: 14.288989367153388
Epoch 4079/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.38726807832717897
Epoch: 4079, Batch Gradient Norm: 14.39270222748955
Epoch: 4079, Batch Gradient Norm after: 14.39270222748955
Epoch 4080/10000, Prediction Accuracy = 62.290000000000006%, Loss = 0.3858845829963684
Epoch: 4080, Batch Gradient Norm: 13.555425014237485
Epoch: 4080, Batch Gradient Norm after: 13.555425014237485
Epoch 4081/10000, Prediction Accuracy = 62.35%, Loss = 0.3835752308368683
Epoch: 4081, Batch Gradient Norm: 12.335887345775108
Epoch: 4081, Batch Gradient Norm after: 12.335887345775108
Epoch 4082/10000, Prediction Accuracy = 62.378%, Loss = 0.38204194903373717
Epoch: 4082, Batch Gradient Norm: 12.658276416313813
Epoch: 4082, Batch Gradient Norm after: 12.658276416313813
Epoch 4083/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.38330559730529784
Epoch: 4083, Batch Gradient Norm: 14.791361402311443
Epoch: 4083, Batch Gradient Norm after: 14.791361402311443
Epoch 4084/10000, Prediction Accuracy = 62.396%, Loss = 0.3874679744243622
Epoch: 4084, Batch Gradient Norm: 13.165809452059143
Epoch: 4084, Batch Gradient Norm after: 13.165809452059143
Epoch 4085/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.38413876891136167
Epoch: 4085, Batch Gradient Norm: 13.13812382885195
Epoch: 4085, Batch Gradient Norm after: 13.13812382885195
Epoch 4086/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.3836928248405457
Epoch: 4086, Batch Gradient Norm: 11.716666887668252
Epoch: 4086, Batch Gradient Norm after: 11.716666887668252
Epoch 4087/10000, Prediction Accuracy = 62.398%, Loss = 0.37958517074584963
Epoch: 4087, Batch Gradient Norm: 10.934578110605674
Epoch: 4087, Batch Gradient Norm after: 10.934578110605674
Epoch 4088/10000, Prediction Accuracy = 62.262%, Loss = 0.3792679667472839
Epoch: 4088, Batch Gradient Norm: 11.46287261864727
Epoch: 4088, Batch Gradient Norm after: 11.46287261864727
Epoch 4089/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.37966814637184143
Epoch: 4089, Batch Gradient Norm: 8.979095002340825
Epoch: 4089, Batch Gradient Norm after: 8.979095002340825
Epoch 4090/10000, Prediction Accuracy = 62.38799999999999%, Loss = 0.3783096134662628
Epoch: 4090, Batch Gradient Norm: 9.630776985311698
Epoch: 4090, Batch Gradient Norm after: 9.630776985311698
Epoch 4091/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.37876430749893186
Epoch: 4091, Batch Gradient Norm: 9.964890409812776
Epoch: 4091, Batch Gradient Norm after: 9.964890409812776
Epoch 4092/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.37798250913619996
Epoch: 4092, Batch Gradient Norm: 10.165947258421642
Epoch: 4092, Batch Gradient Norm after: 10.165947258421642
Epoch 4093/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.3777832090854645
Epoch: 4093, Batch Gradient Norm: 12.724517238058583
Epoch: 4093, Batch Gradient Norm after: 12.724517238058583
Epoch 4094/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.3826776385307312
Epoch: 4094, Batch Gradient Norm: 12.396051474421853
Epoch: 4094, Batch Gradient Norm after: 12.396051474421853
Epoch 4095/10000, Prediction Accuracy = 62.386%, Loss = 0.38028712272644044
Epoch: 4095, Batch Gradient Norm: 9.580118169441242
Epoch: 4095, Batch Gradient Norm after: 9.580118169441242
Epoch 4096/10000, Prediction Accuracy = 62.306000000000004%, Loss = 0.37688738107681274
Epoch: 4096, Batch Gradient Norm: 11.009506789512631
Epoch: 4096, Batch Gradient Norm after: 11.009506789512631
Epoch 4097/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.38015480637550353
Epoch: 4097, Batch Gradient Norm: 10.437318447288765
Epoch: 4097, Batch Gradient Norm after: 10.437318447288765
Epoch 4098/10000, Prediction Accuracy = 62.398%, Loss = 0.37764008045196534
Epoch: 4098, Batch Gradient Norm: 10.053933161658955
Epoch: 4098, Batch Gradient Norm after: 10.053933161658955
Epoch 4099/10000, Prediction Accuracy = 62.43599999999999%, Loss = 0.37703400254249575
Epoch: 4099, Batch Gradient Norm: 13.355470120351006
Epoch: 4099, Batch Gradient Norm after: 13.355470120351006
Epoch 4100/10000, Prediction Accuracy = 62.53599999999999%, Loss = 0.3800447344779968
Epoch: 4100, Batch Gradient Norm: 12.284476413011499
Epoch: 4100, Batch Gradient Norm after: 12.284476413011499
Epoch 4101/10000, Prediction Accuracy = 62.472%, Loss = 0.3781982660293579
Epoch: 4101, Batch Gradient Norm: 12.52876523667243
Epoch: 4101, Batch Gradient Norm after: 12.52876523667243
Epoch 4102/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.3793146193027496
Epoch: 4102, Batch Gradient Norm: 11.697498644065288
Epoch: 4102, Batch Gradient Norm after: 11.697498644065288
Epoch 4103/10000, Prediction Accuracy = 62.386%, Loss = 0.3796774446964264
Epoch: 4103, Batch Gradient Norm: 9.38904985510639
Epoch: 4103, Batch Gradient Norm after: 9.38904985510639
Epoch 4104/10000, Prediction Accuracy = 62.452%, Loss = 0.3780126929283142
Epoch: 4104, Batch Gradient Norm: 11.621034570620639
Epoch: 4104, Batch Gradient Norm after: 11.621034570620639
Epoch 4105/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.3799035906791687
Epoch: 4105, Batch Gradient Norm: 10.8692476243856
Epoch: 4105, Batch Gradient Norm after: 10.8692476243856
Epoch 4106/10000, Prediction Accuracy = 62.31199999999999%, Loss = 0.3782354772090912
Epoch: 4106, Batch Gradient Norm: 9.815033825531675
Epoch: 4106, Batch Gradient Norm after: 9.815033825531675
Epoch 4107/10000, Prediction Accuracy = 62.414%, Loss = 0.3758556008338928
Epoch: 4107, Batch Gradient Norm: 10.723303283213081
Epoch: 4107, Batch Gradient Norm after: 10.723303283213081
Epoch 4108/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.37864646315574646
Epoch: 4108, Batch Gradient Norm: 8.81879790411928
Epoch: 4108, Batch Gradient Norm after: 8.81879790411928
Epoch 4109/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.37868030071258546
Epoch: 4109, Batch Gradient Norm: 11.047189665540223
Epoch: 4109, Batch Gradient Norm after: 11.047189665540223
Epoch 4110/10000, Prediction Accuracy = 62.426%, Loss = 0.3799140632152557
Epoch: 4110, Batch Gradient Norm: 13.141130776158153
Epoch: 4110, Batch Gradient Norm after: 13.141130776158153
Epoch 4111/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.3807185411453247
Epoch: 4111, Batch Gradient Norm: 13.317003424871524
Epoch: 4111, Batch Gradient Norm after: 13.317003424871524
Epoch 4112/10000, Prediction Accuracy = 62.39000000000001%, Loss = 0.38334755301475526
Epoch: 4112, Batch Gradient Norm: 13.159376177581894
Epoch: 4112, Batch Gradient Norm after: 13.159376177581894
Epoch 4113/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.3807219862937927
Epoch: 4113, Batch Gradient Norm: 15.309493355662282
Epoch: 4113, Batch Gradient Norm after: 15.309493355662282
Epoch 4114/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.38207504749298093
Epoch: 4114, Batch Gradient Norm: 11.60976556164254
Epoch: 4114, Batch Gradient Norm after: 11.60976556164254
Epoch 4115/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.3777359962463379
Epoch: 4115, Batch Gradient Norm: 14.677286987772964
Epoch: 4115, Batch Gradient Norm after: 14.677286987772964
Epoch 4116/10000, Prediction Accuracy = 62.396%, Loss = 0.3832506716251373
Epoch: 4116, Batch Gradient Norm: 12.286839310336688
Epoch: 4116, Batch Gradient Norm after: 12.286839310336688
Epoch 4117/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.3790880382061005
Epoch: 4117, Batch Gradient Norm: 12.987897834298897
Epoch: 4117, Batch Gradient Norm after: 12.987897834298897
Epoch 4118/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.380174720287323
Epoch: 4118, Batch Gradient Norm: 14.702100060005586
Epoch: 4118, Batch Gradient Norm after: 14.702100060005586
Epoch 4119/10000, Prediction Accuracy = 62.386%, Loss = 0.3834883213043213
Epoch: 4119, Batch Gradient Norm: 13.36675220037113
Epoch: 4119, Batch Gradient Norm after: 13.36675220037113
Epoch 4120/10000, Prediction Accuracy = 62.354%, Loss = 0.38067545294761657
Epoch: 4120, Batch Gradient Norm: 10.020096217940207
Epoch: 4120, Batch Gradient Norm after: 10.020096217940207
Epoch 4121/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.37751791477203367
Epoch: 4121, Batch Gradient Norm: 12.291602204278702
Epoch: 4121, Batch Gradient Norm after: 12.291602204278702
Epoch 4122/10000, Prediction Accuracy = 62.40999999999999%, Loss = 0.3775744915008545
Epoch: 4122, Batch Gradient Norm: 11.75499113959607
Epoch: 4122, Batch Gradient Norm after: 11.75499113959607
Epoch 4123/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.37775514721870423
Epoch: 4123, Batch Gradient Norm: 12.079321251062481
Epoch: 4123, Batch Gradient Norm after: 12.079321251062481
Epoch 4124/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.38012959957122805
Epoch: 4124, Batch Gradient Norm: 11.69280671112274
Epoch: 4124, Batch Gradient Norm after: 11.69280671112274
Epoch 4125/10000, Prediction Accuracy = 62.338%, Loss = 0.3789534270763397
Epoch: 4125, Batch Gradient Norm: 10.766543359716119
Epoch: 4125, Batch Gradient Norm after: 10.766543359716119
Epoch 4126/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.37853075861930846
Epoch: 4126, Batch Gradient Norm: 11.923556704348425
Epoch: 4126, Batch Gradient Norm after: 11.923556704348425
Epoch 4127/10000, Prediction Accuracy = 62.388%, Loss = 0.37996580004692077
Epoch: 4127, Batch Gradient Norm: 12.452611306683037
Epoch: 4127, Batch Gradient Norm after: 12.452611306683037
Epoch 4128/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.38148008584976195
Epoch: 4128, Batch Gradient Norm: 12.052422549059013
Epoch: 4128, Batch Gradient Norm after: 12.052422549059013
Epoch 4129/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.37951101660728453
Epoch: 4129, Batch Gradient Norm: 13.542470051150898
Epoch: 4129, Batch Gradient Norm after: 13.542470051150898
Epoch 4130/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.3803254127502441
Epoch: 4130, Batch Gradient Norm: 11.630674718739767
Epoch: 4130, Batch Gradient Norm after: 11.630674718739767
Epoch 4131/10000, Prediction Accuracy = 62.45400000000001%, Loss = 0.377597975730896
Epoch: 4131, Batch Gradient Norm: 10.853631609461349
Epoch: 4131, Batch Gradient Norm after: 10.853631609461349
Epoch 4132/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.37997822761535643
Epoch: 4132, Batch Gradient Norm: 13.842031708895847
Epoch: 4132, Batch Gradient Norm after: 13.842031708895847
Epoch 4133/10000, Prediction Accuracy = 62.452%, Loss = 0.38309585452079775
Epoch: 4133, Batch Gradient Norm: 18.337932879057544
Epoch: 4133, Batch Gradient Norm after: 18.337932879057544
Epoch 4134/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.38942148089408873
Epoch: 4134, Batch Gradient Norm: 14.528383356789053
Epoch: 4134, Batch Gradient Norm after: 14.528383356789053
Epoch 4135/10000, Prediction Accuracy = 62.398%, Loss = 0.38232395648956297
Epoch: 4135, Batch Gradient Norm: 10.528201850056591
Epoch: 4135, Batch Gradient Norm after: 10.528201850056591
Epoch 4136/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.3780417084693909
Epoch: 4136, Batch Gradient Norm: 10.047740657768484
Epoch: 4136, Batch Gradient Norm after: 10.047740657768484
Epoch 4137/10000, Prediction Accuracy = 62.412%, Loss = 0.37826029062271116
Epoch: 4137, Batch Gradient Norm: 10.709416292813358
Epoch: 4137, Batch Gradient Norm after: 10.709416292813358
Epoch 4138/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.37849292159080505
Epoch: 4138, Batch Gradient Norm: 13.155748984856144
Epoch: 4138, Batch Gradient Norm after: 13.155748984856144
Epoch 4139/10000, Prediction Accuracy = 62.33%, Loss = 0.38161065578460696
Epoch: 4139, Batch Gradient Norm: 11.675799654452117
Epoch: 4139, Batch Gradient Norm after: 11.675799654452117
Epoch 4140/10000, Prediction Accuracy = 62.46%, Loss = 0.378016597032547
Epoch: 4140, Batch Gradient Norm: 8.948099714611935
Epoch: 4140, Batch Gradient Norm after: 8.948099714611935
Epoch 4141/10000, Prediction Accuracy = 62.436%, Loss = 0.3755480170249939
Epoch: 4141, Batch Gradient Norm: 8.89480154106709
Epoch: 4141, Batch Gradient Norm after: 8.89480154106709
Epoch 4142/10000, Prediction Accuracy = 62.388%, Loss = 0.3753219962120056
Epoch: 4142, Batch Gradient Norm: 10.707880603414209
Epoch: 4142, Batch Gradient Norm after: 10.707880603414209
Epoch 4143/10000, Prediction Accuracy = 62.436%, Loss = 0.3773083031177521
Epoch: 4143, Batch Gradient Norm: 8.41458984098587
Epoch: 4143, Batch Gradient Norm after: 8.41458984098587
Epoch 4144/10000, Prediction Accuracy = 62.471999999999994%, Loss = 0.372957181930542
Epoch: 4144, Batch Gradient Norm: 8.964250936151124
Epoch: 4144, Batch Gradient Norm after: 8.964250936151124
Epoch 4145/10000, Prediction Accuracy = 62.366%, Loss = 0.3757196366786957
Epoch: 4145, Batch Gradient Norm: 7.762462588637038
Epoch: 4145, Batch Gradient Norm after: 7.762462588637038
Epoch 4146/10000, Prediction Accuracy = 62.342%, Loss = 0.3747318863868713
Epoch: 4146, Batch Gradient Norm: 8.947411808440604
Epoch: 4146, Batch Gradient Norm after: 8.947411808440604
Epoch 4147/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.3759265959262848
Epoch: 4147, Batch Gradient Norm: 12.599452230427334
Epoch: 4147, Batch Gradient Norm after: 12.599452230427334
Epoch 4148/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.38170782327651975
Epoch: 4148, Batch Gradient Norm: 11.789267825814488
Epoch: 4148, Batch Gradient Norm after: 11.789267825814488
Epoch 4149/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.37884004712104796
Epoch: 4149, Batch Gradient Norm: 13.72598828921534
Epoch: 4149, Batch Gradient Norm after: 13.72598828921534
Epoch 4150/10000, Prediction Accuracy = 62.428%, Loss = 0.38161718249320986
Epoch: 4150, Batch Gradient Norm: 12.613479911582422
Epoch: 4150, Batch Gradient Norm after: 12.613479911582422
Epoch 4151/10000, Prediction Accuracy = 62.384%, Loss = 0.37879862189292907
Epoch: 4151, Batch Gradient Norm: 13.813296698353891
Epoch: 4151, Batch Gradient Norm after: 13.813296698353891
Epoch 4152/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.38100308179855347
Epoch: 4152, Batch Gradient Norm: 10.571450405499657
Epoch: 4152, Batch Gradient Norm after: 10.571450405499657
Epoch 4153/10000, Prediction Accuracy = 62.462%, Loss = 0.3760372996330261
Epoch: 4153, Batch Gradient Norm: 9.892176716981398
Epoch: 4153, Batch Gradient Norm after: 9.892176716981398
Epoch 4154/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.37706333994865415
Epoch: 4154, Batch Gradient Norm: 11.770279838228209
Epoch: 4154, Batch Gradient Norm after: 11.770279838228209
Epoch 4155/10000, Prediction Accuracy = 62.43799999999999%, Loss = 0.3780871629714966
Epoch: 4155, Batch Gradient Norm: 10.195692562653344
Epoch: 4155, Batch Gradient Norm after: 10.195692562653344
Epoch 4156/10000, Prediction Accuracy = 62.396%, Loss = 0.37550294399261475
Epoch: 4156, Batch Gradient Norm: 12.229106424766158
Epoch: 4156, Batch Gradient Norm after: 12.229106424766158
Epoch 4157/10000, Prediction Accuracy = 62.45%, Loss = 0.3803201258182526
Epoch: 4157, Batch Gradient Norm: 13.431199476637929
Epoch: 4157, Batch Gradient Norm after: 13.431199476637929
Epoch 4158/10000, Prediction Accuracy = 62.528%, Loss = 0.3821397304534912
Epoch: 4158, Batch Gradient Norm: 14.303352440275974
Epoch: 4158, Batch Gradient Norm after: 14.303352440275974
Epoch 4159/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.38285539746284486
Epoch: 4159, Batch Gradient Norm: 16.91274181140018
Epoch: 4159, Batch Gradient Norm after: 16.186530413589605
Epoch 4160/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.38476441502571107
Epoch: 4160, Batch Gradient Norm: 16.576042803438934
Epoch: 4160, Batch Gradient Norm after: 16.28152248538265
Epoch 4161/10000, Prediction Accuracy = 62.462%, Loss = 0.38599576354026793
Epoch: 4161, Batch Gradient Norm: 17.900595650002053
Epoch: 4161, Batch Gradient Norm after: 17.900595650002053
Epoch 4162/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.3863027572631836
Epoch: 4162, Batch Gradient Norm: 14.865615903323864
Epoch: 4162, Batch Gradient Norm after: 14.865615903323864
Epoch 4163/10000, Prediction Accuracy = 62.44%, Loss = 0.38174076080322267
Epoch: 4163, Batch Gradient Norm: 13.387096124893356
Epoch: 4163, Batch Gradient Norm after: 13.387096124893356
Epoch 4164/10000, Prediction Accuracy = 62.438%, Loss = 0.3792815148830414
Epoch: 4164, Batch Gradient Norm: 12.97603863962029
Epoch: 4164, Batch Gradient Norm after: 12.97603863962029
Epoch 4165/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.3845261693000793
Epoch: 4165, Batch Gradient Norm: 12.04927671589311
Epoch: 4165, Batch Gradient Norm after: 12.04927671589311
Epoch 4166/10000, Prediction Accuracy = 62.4%, Loss = 0.3778932511806488
Epoch: 4166, Batch Gradient Norm: 11.864920070124432
Epoch: 4166, Batch Gradient Norm after: 11.864920070124432
Epoch 4167/10000, Prediction Accuracy = 62.426%, Loss = 0.37718050479888915
Epoch: 4167, Batch Gradient Norm: 11.754323017142067
Epoch: 4167, Batch Gradient Norm after: 11.754323017142067
Epoch 4168/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.3774948716163635
Epoch: 4168, Batch Gradient Norm: 12.433117422228026
Epoch: 4168, Batch Gradient Norm after: 12.433117422228026
Epoch 4169/10000, Prediction Accuracy = 62.39%, Loss = 0.38040669560432433
Epoch: 4169, Batch Gradient Norm: 12.573448420676295
Epoch: 4169, Batch Gradient Norm after: 12.573448420676295
Epoch 4170/10000, Prediction Accuracy = 62.414%, Loss = 0.3805343329906464
Epoch: 4170, Batch Gradient Norm: 15.68497639080774
Epoch: 4170, Batch Gradient Norm after: 15.68497639080774
Epoch 4171/10000, Prediction Accuracy = 62.436%, Loss = 0.38309242129325866
Epoch: 4171, Batch Gradient Norm: 15.036347169114906
Epoch: 4171, Batch Gradient Norm after: 15.036347169114906
Epoch 4172/10000, Prediction Accuracy = 62.33399999999999%, Loss = 0.38422838449478147
Epoch: 4172, Batch Gradient Norm: 14.060281622619485
Epoch: 4172, Batch Gradient Norm after: 14.060281622619485
Epoch 4173/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.38317906856536865
Epoch: 4173, Batch Gradient Norm: 15.1984185329148
Epoch: 4173, Batch Gradient Norm after: 15.1984185329148
Epoch 4174/10000, Prediction Accuracy = 62.302%, Loss = 0.3868171811103821
Epoch: 4174, Batch Gradient Norm: 16.810383731180778
Epoch: 4174, Batch Gradient Norm after: 16.810383731180778
Epoch 4175/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.3870822489261627
Epoch: 4175, Batch Gradient Norm: 20.929397400590435
Epoch: 4175, Batch Gradient Norm after: 18.735231148459647
Epoch 4176/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.3911412298679352
Epoch: 4176, Batch Gradient Norm: 17.155101953887936
Epoch: 4176, Batch Gradient Norm after: 17.155101953887936
Epoch 4177/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.3848425388336182
Epoch: 4177, Batch Gradient Norm: 17.191424568744807
Epoch: 4177, Batch Gradient Norm after: 17.191424568744807
Epoch 4178/10000, Prediction Accuracy = 62.476%, Loss = 0.38542593717575074
Epoch: 4178, Batch Gradient Norm: 18.767019578475622
Epoch: 4178, Batch Gradient Norm after: 18.767019578475622
Epoch 4179/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.38791710138320923
Epoch: 4179, Batch Gradient Norm: 16.96201729606233
Epoch: 4179, Batch Gradient Norm after: 16.96201729606233
Epoch 4180/10000, Prediction Accuracy = 62.508%, Loss = 0.3868092894554138
Epoch: 4180, Batch Gradient Norm: 19.853764933315937
Epoch: 4180, Batch Gradient Norm after: 19.810613585250408
Epoch 4181/10000, Prediction Accuracy = 62.436%, Loss = 0.3966693103313446
Epoch: 4181, Batch Gradient Norm: 18.886443400655217
Epoch: 4181, Batch Gradient Norm after: 18.80868073195045
Epoch 4182/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.38780805468559265
Epoch: 4182, Batch Gradient Norm: 16.69129907780702
Epoch: 4182, Batch Gradient Norm after: 16.69129907780702
Epoch 4183/10000, Prediction Accuracy = 62.416%, Loss = 0.3852642774581909
Epoch: 4183, Batch Gradient Norm: 12.965849194133263
Epoch: 4183, Batch Gradient Norm after: 12.965849194133263
Epoch 4184/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.37738527059555055
Epoch: 4184, Batch Gradient Norm: 10.955658698537519
Epoch: 4184, Batch Gradient Norm after: 10.955658698537519
Epoch 4185/10000, Prediction Accuracy = 62.448%, Loss = 0.37573620676994324
Epoch: 4185, Batch Gradient Norm: 12.815996784109378
Epoch: 4185, Batch Gradient Norm after: 12.815996784109378
Epoch 4186/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.3796832740306854
Epoch: 4186, Batch Gradient Norm: 16.801474627605536
Epoch: 4186, Batch Gradient Norm after: 16.801474627605536
Epoch 4187/10000, Prediction Accuracy = 62.348%, Loss = 0.3844111323356628
Epoch: 4187, Batch Gradient Norm: 14.822955861414657
Epoch: 4187, Batch Gradient Norm after: 14.822955861414657
Epoch 4188/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.3806102752685547
Epoch: 4188, Batch Gradient Norm: 11.790165320627938
Epoch: 4188, Batch Gradient Norm after: 11.790165320627938
Epoch 4189/10000, Prediction Accuracy = 62.51800000000001%, Loss = 0.37852923274040223
Epoch: 4189, Batch Gradient Norm: 12.609632655006523
Epoch: 4189, Batch Gradient Norm after: 12.609632655006523
Epoch 4190/10000, Prediction Accuracy = 62.39%, Loss = 0.37690783739089967
Epoch: 4190, Batch Gradient Norm: 11.837889925150861
Epoch: 4190, Batch Gradient Norm after: 11.837889925150861
Epoch 4191/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.37757357954978943
Epoch: 4191, Batch Gradient Norm: 11.522674455340866
Epoch: 4191, Batch Gradient Norm after: 11.522674455340866
Epoch 4192/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.3762849986553192
Epoch: 4192, Batch Gradient Norm: 9.886860373306359
Epoch: 4192, Batch Gradient Norm after: 9.886860373306359
Epoch 4193/10000, Prediction Accuracy = 62.36%, Loss = 0.37647961974143984
Epoch: 4193, Batch Gradient Norm: 13.014559433863685
Epoch: 4193, Batch Gradient Norm after: 13.014559433863685
Epoch 4194/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.37876567244529724
Epoch: 4194, Batch Gradient Norm: 11.730695120010083
Epoch: 4194, Batch Gradient Norm after: 11.730695120010083
Epoch 4195/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.3749210715293884
Epoch: 4195, Batch Gradient Norm: 13.55448928939656
Epoch: 4195, Batch Gradient Norm after: 13.55448928939656
Epoch 4196/10000, Prediction Accuracy = 62.45%, Loss = 0.3794657289981842
Epoch: 4196, Batch Gradient Norm: 12.593569309353104
Epoch: 4196, Batch Gradient Norm after: 12.593569309353104
Epoch 4197/10000, Prediction Accuracy = 62.414%, Loss = 0.37848982214927673
Epoch: 4197, Batch Gradient Norm: 10.553569062965138
Epoch: 4197, Batch Gradient Norm after: 10.553569062965138
Epoch 4198/10000, Prediction Accuracy = 62.352%, Loss = 0.3744494080543518
Epoch: 4198, Batch Gradient Norm: 11.918026766722733
Epoch: 4198, Batch Gradient Norm after: 11.918026766722733
Epoch 4199/10000, Prediction Accuracy = 62.5%, Loss = 0.3757335007190704
Epoch: 4199, Batch Gradient Norm: 12.484919423284154
Epoch: 4199, Batch Gradient Norm after: 12.484919423284154
Epoch 4200/10000, Prediction Accuracy = 62.378%, Loss = 0.3808834791183472
Epoch: 4200, Batch Gradient Norm: 11.792906622547294
Epoch: 4200, Batch Gradient Norm after: 11.792906622547294
Epoch 4201/10000, Prediction Accuracy = 62.464%, Loss = 0.37889739871025085
Epoch: 4201, Batch Gradient Norm: 13.704217405364984
Epoch: 4201, Batch Gradient Norm after: 13.704217405364984
Epoch 4202/10000, Prediction Accuracy = 62.504000000000005%, Loss = 0.3795651912689209
Epoch: 4202, Batch Gradient Norm: 9.450901169591106
Epoch: 4202, Batch Gradient Norm after: 9.450901169591106
Epoch 4203/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.37267066836357116
Epoch: 4203, Batch Gradient Norm: 7.794607985444411
Epoch: 4203, Batch Gradient Norm after: 7.794607985444411
Epoch 4204/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.3734622895717621
Epoch: 4204, Batch Gradient Norm: 12.039903356883952
Epoch: 4204, Batch Gradient Norm after: 12.039903356883952
Epoch 4205/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.37902318835258486
Epoch: 4205, Batch Gradient Norm: 12.793821356776185
Epoch: 4205, Batch Gradient Norm after: 12.793821356776185
Epoch 4206/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.3799465775489807
Epoch: 4206, Batch Gradient Norm: 14.610966477723574
Epoch: 4206, Batch Gradient Norm after: 14.610966477723574
Epoch 4207/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.3822861909866333
Epoch: 4207, Batch Gradient Norm: 14.000109660249114
Epoch: 4207, Batch Gradient Norm after: 14.000109660249114
Epoch 4208/10000, Prediction Accuracy = 62.536%, Loss = 0.3788725912570953
Epoch: 4208, Batch Gradient Norm: 13.898088419282015
Epoch: 4208, Batch Gradient Norm after: 13.898088419282015
Epoch 4209/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.3810965299606323
Epoch: 4209, Batch Gradient Norm: 13.956920490382178
Epoch: 4209, Batch Gradient Norm after: 13.956920490382178
Epoch 4210/10000, Prediction Accuracy = 62.426%, Loss = 0.3814186155796051
Epoch: 4210, Batch Gradient Norm: 13.29453406811292
Epoch: 4210, Batch Gradient Norm after: 13.29453406811292
Epoch 4211/10000, Prediction Accuracy = 62.43799999999999%, Loss = 0.38171389102935793
Epoch: 4211, Batch Gradient Norm: 15.193920289716107
Epoch: 4211, Batch Gradient Norm after: 15.193920289716107
Epoch 4212/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.38486890196800233
Epoch: 4212, Batch Gradient Norm: 13.83378430677912
Epoch: 4212, Batch Gradient Norm after: 13.83378430677912
Epoch 4213/10000, Prediction Accuracy = 62.384%, Loss = 0.3789857864379883
Epoch: 4213, Batch Gradient Norm: 10.221122757408803
Epoch: 4213, Batch Gradient Norm after: 10.221122757408803
Epoch 4214/10000, Prediction Accuracy = 62.512%, Loss = 0.3735931754112244
Epoch: 4214, Batch Gradient Norm: 8.435372807023384
Epoch: 4214, Batch Gradient Norm after: 8.435372807023384
Epoch 4215/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.3722029089927673
Epoch: 4215, Batch Gradient Norm: 10.230093259270316
Epoch: 4215, Batch Gradient Norm after: 10.230093259270316
Epoch 4216/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.3754709780216217
Epoch: 4216, Batch Gradient Norm: 12.708259698815857
Epoch: 4216, Batch Gradient Norm after: 12.708259698815857
Epoch 4217/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.3777038872241974
Epoch: 4217, Batch Gradient Norm: 12.281041305671986
Epoch: 4217, Batch Gradient Norm after: 12.281041305671986
Epoch 4218/10000, Prediction Accuracy = 62.352%, Loss = 0.3767473638057709
Epoch: 4218, Batch Gradient Norm: 10.976869561180473
Epoch: 4218, Batch Gradient Norm after: 10.976869561180473
Epoch 4219/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.3739147663116455
Epoch: 4219, Batch Gradient Norm: 10.226866768255725
Epoch: 4219, Batch Gradient Norm after: 10.226866768255725
Epoch 4220/10000, Prediction Accuracy = 62.484%, Loss = 0.3748979687690735
Epoch: 4220, Batch Gradient Norm: 12.195684224694524
Epoch: 4220, Batch Gradient Norm after: 12.195684224694524
Epoch 4221/10000, Prediction Accuracy = 62.462%, Loss = 0.37764182686805725
Epoch: 4221, Batch Gradient Norm: 13.62687831937668
Epoch: 4221, Batch Gradient Norm after: 13.62687831937668
Epoch 4222/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.3796136438846588
Epoch: 4222, Batch Gradient Norm: 15.422047010413198
Epoch: 4222, Batch Gradient Norm after: 15.157959239387095
Epoch 4223/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.38114418387413024
Epoch: 4223, Batch Gradient Norm: 14.162214628705268
Epoch: 4223, Batch Gradient Norm after: 14.162214628705268
Epoch 4224/10000, Prediction Accuracy = 62.362%, Loss = 0.3797961413860321
Epoch: 4224, Batch Gradient Norm: 13.989927257609137
Epoch: 4224, Batch Gradient Norm after: 13.989927257609137
Epoch 4225/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.3802270948886871
Epoch: 4225, Batch Gradient Norm: 11.640179260524283
Epoch: 4225, Batch Gradient Norm after: 11.640179260524283
Epoch 4226/10000, Prediction Accuracy = 62.51800000000001%, Loss = 0.3765227198600769
Epoch: 4226, Batch Gradient Norm: 12.224625591788783
Epoch: 4226, Batch Gradient Norm after: 12.224625591788783
Epoch 4227/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.37778160572052
Epoch: 4227, Batch Gradient Norm: 13.94029698037657
Epoch: 4227, Batch Gradient Norm after: 13.94029698037657
Epoch 4228/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.38028537631034853
Epoch: 4228, Batch Gradient Norm: 12.510926440538809
Epoch: 4228, Batch Gradient Norm after: 12.510926440538809
Epoch 4229/10000, Prediction Accuracy = 62.524%, Loss = 0.3757925510406494
Epoch: 4229, Batch Gradient Norm: 14.522370518914988
Epoch: 4229, Batch Gradient Norm after: 14.522370518914988
Epoch 4230/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.3820246636867523
Epoch: 4230, Batch Gradient Norm: 12.734064657531572
Epoch: 4230, Batch Gradient Norm after: 12.734064657531572
Epoch 4231/10000, Prediction Accuracy = 62.472%, Loss = 0.37758506536483766
Epoch: 4231, Batch Gradient Norm: 9.760161729670251
Epoch: 4231, Batch Gradient Norm after: 9.760161729670251
Epoch 4232/10000, Prediction Accuracy = 62.48%, Loss = 0.3732980012893677
Epoch: 4232, Batch Gradient Norm: 10.086388986226346
Epoch: 4232, Batch Gradient Norm after: 10.086388986226346
Epoch 4233/10000, Prediction Accuracy = 62.464%, Loss = 0.37235129475593565
Epoch: 4233, Batch Gradient Norm: 9.404708379077158
Epoch: 4233, Batch Gradient Norm after: 9.404708379077158
Epoch 4234/10000, Prediction Accuracy = 62.56600000000001%, Loss = 0.3739882171154022
Epoch: 4234, Batch Gradient Norm: 11.418006789823869
Epoch: 4234, Batch Gradient Norm after: 11.418006789823869
Epoch 4235/10000, Prediction Accuracy = 62.464%, Loss = 0.37622371315956116
Epoch: 4235, Batch Gradient Norm: 13.105503088194874
Epoch: 4235, Batch Gradient Norm after: 13.105503088194874
Epoch 4236/10000, Prediction Accuracy = 62.5%, Loss = 0.3783868789672852
Epoch: 4236, Batch Gradient Norm: 11.150877613204756
Epoch: 4236, Batch Gradient Norm after: 11.150877613204756
Epoch 4237/10000, Prediction Accuracy = 62.476%, Loss = 0.3742357075214386
Epoch: 4237, Batch Gradient Norm: 14.828027468852252
Epoch: 4237, Batch Gradient Norm after: 14.828027468852252
Epoch 4238/10000, Prediction Accuracy = 62.58399999999999%, Loss = 0.3792355239391327
Epoch: 4238, Batch Gradient Norm: 13.441638397471905
Epoch: 4238, Batch Gradient Norm after: 13.441638397471905
Epoch 4239/10000, Prediction Accuracy = 62.511999999999986%, Loss = 0.3816268563270569
Epoch: 4239, Batch Gradient Norm: 16.089063844293616
Epoch: 4239, Batch Gradient Norm after: 16.089063844293616
Epoch 4240/10000, Prediction Accuracy = 62.472%, Loss = 0.38259388208389283
Epoch: 4240, Batch Gradient Norm: 14.265827363884851
Epoch: 4240, Batch Gradient Norm after: 14.265827363884851
Epoch 4241/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.3796342253684998
Epoch: 4241, Batch Gradient Norm: 11.727459605699275
Epoch: 4241, Batch Gradient Norm after: 11.727459605699275
Epoch 4242/10000, Prediction Accuracy = 62.39000000000001%, Loss = 0.37543157339096067
Epoch: 4242, Batch Gradient Norm: 13.29470477752642
Epoch: 4242, Batch Gradient Norm after: 13.29470477752642
Epoch 4243/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.3770324349403381
Epoch: 4243, Batch Gradient Norm: 13.773179841188384
Epoch: 4243, Batch Gradient Norm after: 13.773179841188384
Epoch 4244/10000, Prediction Accuracy = 62.42%, Loss = 0.3789362132549286
Epoch: 4244, Batch Gradient Norm: 11.852722863465834
Epoch: 4244, Batch Gradient Norm after: 11.852722863465834
Epoch 4245/10000, Prediction Accuracy = 62.548%, Loss = 0.37546974420547485
Epoch: 4245, Batch Gradient Norm: 12.202801655010443
Epoch: 4245, Batch Gradient Norm after: 12.202801655010443
Epoch 4246/10000, Prediction Accuracy = 62.488%, Loss = 0.37623459100723267
Epoch: 4246, Batch Gradient Norm: 13.19962097595285
Epoch: 4246, Batch Gradient Norm after: 13.19962097595285
Epoch 4247/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.3767740726470947
Epoch: 4247, Batch Gradient Norm: 13.60420812060673
Epoch: 4247, Batch Gradient Norm after: 13.60420812060673
Epoch 4248/10000, Prediction Accuracy = 62.43599999999999%, Loss = 0.3776314198970795
Epoch: 4248, Batch Gradient Norm: 14.324939139298017
Epoch: 4248, Batch Gradient Norm after: 14.324939139298017
Epoch 4249/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.37795292735099795
Epoch: 4249, Batch Gradient Norm: 11.952739755456
Epoch: 4249, Batch Gradient Norm after: 11.952739755456
Epoch 4250/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.3747465074062347
Epoch: 4250, Batch Gradient Norm: 10.311044331872997
Epoch: 4250, Batch Gradient Norm after: 10.311044331872997
Epoch 4251/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.3711799144744873
Epoch: 4251, Batch Gradient Norm: 10.488462656205755
Epoch: 4251, Batch Gradient Norm after: 10.488462656205755
Epoch 4252/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.3737306833267212
Epoch: 4252, Batch Gradient Norm: 12.243618574831263
Epoch: 4252, Batch Gradient Norm after: 12.243618574831263
Epoch 4253/10000, Prediction Accuracy = 62.576%, Loss = 0.37445151805877686
Epoch: 4253, Batch Gradient Norm: 16.2702782561285
Epoch: 4253, Batch Gradient Norm after: 16.2702782561285
Epoch 4254/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.38104055523872377
Epoch: 4254, Batch Gradient Norm: 11.908526728114783
Epoch: 4254, Batch Gradient Norm after: 11.908526728114783
Epoch 4255/10000, Prediction Accuracy = 62.56999999999999%, Loss = 0.3743959963321686
Epoch: 4255, Batch Gradient Norm: 11.583500121518329
Epoch: 4255, Batch Gradient Norm after: 11.583500121518329
Epoch 4256/10000, Prediction Accuracy = 62.576%, Loss = 0.3746859490871429
Epoch: 4256, Batch Gradient Norm: 11.527922540624841
Epoch: 4256, Batch Gradient Norm after: 11.527922540624841
Epoch 4257/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.37561383843421936
Epoch: 4257, Batch Gradient Norm: 13.032085054841042
Epoch: 4257, Batch Gradient Norm after: 13.032085054841042
Epoch 4258/10000, Prediction Accuracy = 62.464%, Loss = 0.3782535672187805
Epoch: 4258, Batch Gradient Norm: 12.77287457943371
Epoch: 4258, Batch Gradient Norm after: 12.77287457943371
Epoch 4259/10000, Prediction Accuracy = 62.588%, Loss = 0.37761388421058656
Epoch: 4259, Batch Gradient Norm: 14.155404460781112
Epoch: 4259, Batch Gradient Norm after: 14.155404460781112
Epoch 4260/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.3774870753288269
Epoch: 4260, Batch Gradient Norm: 12.26190253251655
Epoch: 4260, Batch Gradient Norm after: 12.26190253251655
Epoch 4261/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.37550772428512574
Epoch: 4261, Batch Gradient Norm: 10.974834952675385
Epoch: 4261, Batch Gradient Norm after: 10.974834952675385
Epoch 4262/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3730157732963562
Epoch: 4262, Batch Gradient Norm: 10.093153912393717
Epoch: 4262, Batch Gradient Norm after: 10.093153912393717
Epoch 4263/10000, Prediction Accuracy = 62.6%, Loss = 0.3717285871505737
Epoch: 4263, Batch Gradient Norm: 10.826333688033515
Epoch: 4263, Batch Gradient Norm after: 10.826333688033515
Epoch 4264/10000, Prediction Accuracy = 62.462%, Loss = 0.3736748158931732
Epoch: 4264, Batch Gradient Norm: 10.142187982195308
Epoch: 4264, Batch Gradient Norm after: 10.142187982195308
Epoch 4265/10000, Prediction Accuracy = 62.5%, Loss = 0.37101184725761416
Epoch: 4265, Batch Gradient Norm: 8.385768337788381
Epoch: 4265, Batch Gradient Norm after: 8.385768337788381
Epoch 4266/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.3699621021747589
Epoch: 4266, Batch Gradient Norm: 10.086979729624323
Epoch: 4266, Batch Gradient Norm after: 10.086979729624323
Epoch 4267/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.3767863631248474
Epoch: 4267, Batch Gradient Norm: 12.787657843589875
Epoch: 4267, Batch Gradient Norm after: 12.787657843589875
Epoch 4268/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.37653560638427735
Epoch: 4268, Batch Gradient Norm: 13.869268006741535
Epoch: 4268, Batch Gradient Norm after: 13.869268006741535
Epoch 4269/10000, Prediction Accuracy = 62.426%, Loss = 0.3791902601718903
Epoch: 4269, Batch Gradient Norm: 16.590195114865548
Epoch: 4269, Batch Gradient Norm after: 16.590195114865548
Epoch 4270/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.38139996528625486
Epoch: 4270, Batch Gradient Norm: 17.0471973651168
Epoch: 4270, Batch Gradient Norm after: 17.0471973651168
Epoch 4271/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.3824756443500519
Epoch: 4271, Batch Gradient Norm: 12.946222427691335
Epoch: 4271, Batch Gradient Norm after: 12.946222427691335
Epoch 4272/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.37525359392166135
Epoch: 4272, Batch Gradient Norm: 12.552095406248139
Epoch: 4272, Batch Gradient Norm after: 12.552095406248139
Epoch 4273/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.37712892293930056
Epoch: 4273, Batch Gradient Norm: 11.968534504111592
Epoch: 4273, Batch Gradient Norm after: 11.968534504111592
Epoch 4274/10000, Prediction Accuracy = 62.588%, Loss = 0.37513707876205443
Epoch: 4274, Batch Gradient Norm: 15.970695208184615
Epoch: 4274, Batch Gradient Norm after: 15.970695208184615
Epoch 4275/10000, Prediction Accuracy = 62.495999999999995%, Loss = 0.3793883085250854
Epoch: 4275, Batch Gradient Norm: 16.45932318262418
Epoch: 4275, Batch Gradient Norm after: 16.45932318262418
Epoch 4276/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.3807988703250885
Epoch: 4276, Batch Gradient Norm: 14.222381816952476
Epoch: 4276, Batch Gradient Norm after: 14.222381816952476
Epoch 4277/10000, Prediction Accuracy = 62.424%, Loss = 0.3764637649059296
Epoch: 4277, Batch Gradient Norm: 15.083262571627465
Epoch: 4277, Batch Gradient Norm after: 15.083262571627465
Epoch 4278/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.377690190076828
Epoch: 4278, Batch Gradient Norm: 13.190714856369098
Epoch: 4278, Batch Gradient Norm after: 13.190714856369098
Epoch 4279/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.37562822103500365
Epoch: 4279, Batch Gradient Norm: 11.596905066318739
Epoch: 4279, Batch Gradient Norm after: 11.596905066318739
Epoch 4280/10000, Prediction Accuracy = 62.428%, Loss = 0.37437863945961
Epoch: 4280, Batch Gradient Norm: 12.034390567030185
Epoch: 4280, Batch Gradient Norm after: 12.034390567030185
Epoch 4281/10000, Prediction Accuracy = 62.556%, Loss = 0.37610526084899903
Epoch: 4281, Batch Gradient Norm: 11.18097622012317
Epoch: 4281, Batch Gradient Norm after: 11.18097622012317
Epoch 4282/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.37218838930130005
Epoch: 4282, Batch Gradient Norm: 10.689856317635291
Epoch: 4282, Batch Gradient Norm after: 10.689856317635291
Epoch 4283/10000, Prediction Accuracy = 62.44000000000001%, Loss = 0.372151654958725
Epoch: 4283, Batch Gradient Norm: 10.326495795733667
Epoch: 4283, Batch Gradient Norm after: 10.326495795733667
Epoch 4284/10000, Prediction Accuracy = 62.516000000000005%, Loss = 0.37272773385047914
Epoch: 4284, Batch Gradient Norm: 8.436554927946613
Epoch: 4284, Batch Gradient Norm after: 8.436554927946613
Epoch 4285/10000, Prediction Accuracy = 62.636%, Loss = 0.36976009607315063
Epoch: 4285, Batch Gradient Norm: 11.410343845958767
Epoch: 4285, Batch Gradient Norm after: 11.410343845958767
Epoch 4286/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.37512434720993043
Epoch: 4286, Batch Gradient Norm: 12.124622395315269
Epoch: 4286, Batch Gradient Norm after: 12.124622395315269
Epoch 4287/10000, Prediction Accuracy = 62.574%, Loss = 0.37408925890922545
Epoch: 4287, Batch Gradient Norm: 12.705108067250572
Epoch: 4287, Batch Gradient Norm after: 12.705108067250572
Epoch 4288/10000, Prediction Accuracy = 62.576%, Loss = 0.37344972491264344
Epoch: 4288, Batch Gradient Norm: 12.694994213238173
Epoch: 4288, Batch Gradient Norm after: 12.694994213238173
Epoch 4289/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.3734266996383667
Epoch: 4289, Batch Gradient Norm: 16.640579090069007
Epoch: 4289, Batch Gradient Norm after: 16.640579090069007
Epoch 4290/10000, Prediction Accuracy = 62.562%, Loss = 0.3796409845352173
Epoch: 4290, Batch Gradient Norm: 15.356924389394676
Epoch: 4290, Batch Gradient Norm after: 15.356924389394676
Epoch 4291/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.3788071572780609
Epoch: 4291, Batch Gradient Norm: 14.523151222451624
Epoch: 4291, Batch Gradient Norm after: 14.523151222451624
Epoch 4292/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.37668712735176085
Epoch: 4292, Batch Gradient Norm: 11.186694383985227
Epoch: 4292, Batch Gradient Norm after: 11.186694383985227
Epoch 4293/10000, Prediction Accuracy = 62.33599999999999%, Loss = 0.37343961000442505
Epoch: 4293, Batch Gradient Norm: 12.442628421854954
Epoch: 4293, Batch Gradient Norm after: 12.442628421854954
Epoch 4294/10000, Prediction Accuracy = 62.45%, Loss = 0.376028835773468
Epoch: 4294, Batch Gradient Norm: 11.31198812334297
Epoch: 4294, Batch Gradient Norm after: 11.31198812334297
Epoch 4295/10000, Prediction Accuracy = 62.504%, Loss = 0.37423108220100404
Epoch: 4295, Batch Gradient Norm: 11.088325260123574
Epoch: 4295, Batch Gradient Norm after: 11.088325260123574
Epoch 4296/10000, Prediction Accuracy = 62.46600000000001%, Loss = 0.37271645069122317
Epoch: 4296, Batch Gradient Norm: 10.607944831219354
Epoch: 4296, Batch Gradient Norm after: 10.607944831219354
Epoch 4297/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.37379324436187744
Epoch: 4297, Batch Gradient Norm: 9.050974667403215
Epoch: 4297, Batch Gradient Norm after: 9.050974667403215
Epoch 4298/10000, Prediction Accuracy = 62.436%, Loss = 0.37056286334991456
Epoch: 4298, Batch Gradient Norm: 10.354251736903002
Epoch: 4298, Batch Gradient Norm after: 10.354251736903002
Epoch 4299/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3705443501472473
Epoch: 4299, Batch Gradient Norm: 10.758001659305988
Epoch: 4299, Batch Gradient Norm after: 10.758001659305988
Epoch 4300/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.37388880252838136
Epoch: 4300, Batch Gradient Norm: 10.513272992154551
Epoch: 4300, Batch Gradient Norm after: 10.513272992154551
Epoch 4301/10000, Prediction Accuracy = 62.484%, Loss = 0.37293612360954287
Epoch: 4301, Batch Gradient Norm: 9.578110250406622
Epoch: 4301, Batch Gradient Norm after: 9.578110250406622
Epoch 4302/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.3716108679771423
Epoch: 4302, Batch Gradient Norm: 11.863923160246669
Epoch: 4302, Batch Gradient Norm after: 11.863923160246669
Epoch 4303/10000, Prediction Accuracy = 62.519999999999996%, Loss = 0.37250959873199463
Epoch: 4303, Batch Gradient Norm: 10.70696753921966
Epoch: 4303, Batch Gradient Norm after: 10.70696753921966
Epoch 4304/10000, Prediction Accuracy = 62.54799999999999%, Loss = 0.37047884464263914
Epoch: 4304, Batch Gradient Norm: 11.846789318035455
Epoch: 4304, Batch Gradient Norm after: 11.846789318035455
Epoch 4305/10000, Prediction Accuracy = 62.444%, Loss = 0.3720341444015503
Epoch: 4305, Batch Gradient Norm: 11.529266185961596
Epoch: 4305, Batch Gradient Norm after: 11.529266185961596
Epoch 4306/10000, Prediction Accuracy = 62.532%, Loss = 0.37157960534095763
Epoch: 4306, Batch Gradient Norm: 11.889105975131876
Epoch: 4306, Batch Gradient Norm after: 11.889105975131876
Epoch 4307/10000, Prediction Accuracy = 62.574%, Loss = 0.37392076253890993
Epoch: 4307, Batch Gradient Norm: 10.658720516465062
Epoch: 4307, Batch Gradient Norm after: 10.658720516465062
Epoch 4308/10000, Prediction Accuracy = 62.552%, Loss = 0.3734555423259735
Epoch: 4308, Batch Gradient Norm: 12.339274763236643
Epoch: 4308, Batch Gradient Norm after: 12.339274763236643
Epoch 4309/10000, Prediction Accuracy = 62.55400000000001%, Loss = 0.37428765892982485
Epoch: 4309, Batch Gradient Norm: 13.571944262579299
Epoch: 4309, Batch Gradient Norm after: 13.571944262579299
Epoch 4310/10000, Prediction Accuracy = 62.52%, Loss = 0.37482995390892027
Epoch: 4310, Batch Gradient Norm: 14.143359413270597
Epoch: 4310, Batch Gradient Norm after: 14.143359413270597
Epoch 4311/10000, Prediction Accuracy = 62.492%, Loss = 0.3770886600017548
Epoch: 4311, Batch Gradient Norm: 13.744612374727266
Epoch: 4311, Batch Gradient Norm after: 13.744612374727266
Epoch 4312/10000, Prediction Accuracy = 62.538%, Loss = 0.3754607081413269
Epoch: 4312, Batch Gradient Norm: 11.790773208013956
Epoch: 4312, Batch Gradient Norm after: 11.790773208013956
Epoch 4313/10000, Prediction Accuracy = 62.576%, Loss = 0.3738299310207367
Epoch: 4313, Batch Gradient Norm: 10.127758050914613
Epoch: 4313, Batch Gradient Norm after: 10.127758050914613
Epoch 4314/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.37213443517684935
Epoch: 4314, Batch Gradient Norm: 14.649025839623924
Epoch: 4314, Batch Gradient Norm after: 14.649025839623924
Epoch 4315/10000, Prediction Accuracy = 62.396%, Loss = 0.3767363965511322
Epoch: 4315, Batch Gradient Norm: 15.062530670551316
Epoch: 4315, Batch Gradient Norm after: 15.062530670551316
Epoch 4316/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.37819740176200867
Epoch: 4316, Batch Gradient Norm: 14.705257836644375
Epoch: 4316, Batch Gradient Norm after: 14.705257836644375
Epoch 4317/10000, Prediction Accuracy = 62.544%, Loss = 0.3774192690849304
Epoch: 4317, Batch Gradient Norm: 16.521577713653127
Epoch: 4317, Batch Gradient Norm after: 16.521577713653127
Epoch 4318/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.37995373606681826
Epoch: 4318, Batch Gradient Norm: 18.210553573798514
Epoch: 4318, Batch Gradient Norm after: 17.901405788159554
Epoch 4319/10000, Prediction Accuracy = 62.50999999999999%, Loss = 0.38129520416259766
Epoch: 4319, Batch Gradient Norm: 14.691799123069185
Epoch: 4319, Batch Gradient Norm after: 14.691799123069185
Epoch 4320/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.37692420482635497
Epoch: 4320, Batch Gradient Norm: 11.483084568422946
Epoch: 4320, Batch Gradient Norm after: 11.483084568422946
Epoch 4321/10000, Prediction Accuracy = 62.45400000000001%, Loss = 0.3745617032051086
Epoch: 4321, Batch Gradient Norm: 11.251359571658067
Epoch: 4321, Batch Gradient Norm after: 11.251359571658067
Epoch 4322/10000, Prediction Accuracy = 62.53000000000001%, Loss = 0.37212846279144285
Epoch: 4322, Batch Gradient Norm: 13.333566056653929
Epoch: 4322, Batch Gradient Norm after: 13.333566056653929
Epoch 4323/10000, Prediction Accuracy = 62.5%, Loss = 0.3735681712627411
Epoch: 4323, Batch Gradient Norm: 11.416483080849643
Epoch: 4323, Batch Gradient Norm after: 11.416483080849643
Epoch 4324/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.37368794679641726
Epoch: 4324, Batch Gradient Norm: 11.33852597238805
Epoch: 4324, Batch Gradient Norm after: 11.33852597238805
Epoch 4325/10000, Prediction Accuracy = 62.4%, Loss = 0.3726871728897095
Epoch: 4325, Batch Gradient Norm: 12.719432700370426
Epoch: 4325, Batch Gradient Norm after: 12.719432700370426
Epoch 4326/10000, Prediction Accuracy = 62.564%, Loss = 0.3753538250923157
Epoch: 4326, Batch Gradient Norm: 12.552442694655939
Epoch: 4326, Batch Gradient Norm after: 12.552442694655939
Epoch 4327/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.3718856394290924
Epoch: 4327, Batch Gradient Norm: 13.194977595994093
Epoch: 4327, Batch Gradient Norm after: 13.194977595994093
Epoch 4328/10000, Prediction Accuracy = 62.492%, Loss = 0.37420580387115476
Epoch: 4328, Batch Gradient Norm: 14.670520756100167
Epoch: 4328, Batch Gradient Norm after: 14.670520756100167
Epoch 4329/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.3764866471290588
Epoch: 4329, Batch Gradient Norm: 14.84543889142811
Epoch: 4329, Batch Gradient Norm after: 14.84543889142811
Epoch 4330/10000, Prediction Accuracy = 62.492000000000004%, Loss = 0.38141135573387147
Epoch: 4330, Batch Gradient Norm: 16.906715718106838
Epoch: 4330, Batch Gradient Norm after: 16.906715718106838
Epoch 4331/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.3809480905532837
Epoch: 4331, Batch Gradient Norm: 19.197821827081835
Epoch: 4331, Batch Gradient Norm after: 17.551451044574286
Epoch 4332/10000, Prediction Accuracy = 62.44599999999999%, Loss = 0.38415061831474306
Epoch: 4332, Batch Gradient Norm: 18.968463931750776
Epoch: 4332, Batch Gradient Norm after: 18.968463931750776
Epoch 4333/10000, Prediction Accuracy = 62.476%, Loss = 0.38485315442085266
Epoch: 4333, Batch Gradient Norm: 17.531450188781577
Epoch: 4333, Batch Gradient Norm after: 17.531450188781577
Epoch 4334/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.3827980875968933
Epoch: 4334, Batch Gradient Norm: 11.822376322943368
Epoch: 4334, Batch Gradient Norm after: 11.822376322943368
Epoch 4335/10000, Prediction Accuracy = 62.586%, Loss = 0.37410051822662355
Epoch: 4335, Batch Gradient Norm: 13.387893972843743
Epoch: 4335, Batch Gradient Norm after: 13.387893972843743
Epoch 4336/10000, Prediction Accuracy = 62.528%, Loss = 0.3740278959274292
Epoch: 4336, Batch Gradient Norm: 16.814591834589923
Epoch: 4336, Batch Gradient Norm after: 16.814591834589923
Epoch 4337/10000, Prediction Accuracy = 62.488%, Loss = 0.3789813220500946
Epoch: 4337, Batch Gradient Norm: 16.667980878898994
Epoch: 4337, Batch Gradient Norm after: 16.667980878898994
Epoch 4338/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.3783424854278564
Epoch: 4338, Batch Gradient Norm: 13.625371122710634
Epoch: 4338, Batch Gradient Norm after: 13.625371122710634
Epoch 4339/10000, Prediction Accuracy = 62.576%, Loss = 0.37615346908569336
Epoch: 4339, Batch Gradient Norm: 14.63498216657134
Epoch: 4339, Batch Gradient Norm after: 14.63498216657134
Epoch 4340/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.3771394670009613
Epoch: 4340, Batch Gradient Norm: 16.057527375756035
Epoch: 4340, Batch Gradient Norm after: 16.057527375756035
Epoch 4341/10000, Prediction Accuracy = 62.472%, Loss = 0.3811060845851898
Epoch: 4341, Batch Gradient Norm: 16.329396121800166
Epoch: 4341, Batch Gradient Norm after: 16.329396121800166
Epoch 4342/10000, Prediction Accuracy = 62.574%, Loss = 0.3801617443561554
Epoch: 4342, Batch Gradient Norm: 15.381504708817127
Epoch: 4342, Batch Gradient Norm after: 15.381504708817127
Epoch 4343/10000, Prediction Accuracy = 62.584%, Loss = 0.37815220952033995
Epoch: 4343, Batch Gradient Norm: 15.630998648152238
Epoch: 4343, Batch Gradient Norm after: 15.630998648152238
Epoch 4344/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.38036893010139466
Epoch: 4344, Batch Gradient Norm: 16.55150309838169
Epoch: 4344, Batch Gradient Norm after: 16.55150309838169
Epoch 4345/10000, Prediction Accuracy = 62.602%, Loss = 0.38119844198226926
Epoch: 4345, Batch Gradient Norm: 11.980351253303738
Epoch: 4345, Batch Gradient Norm after: 11.980351253303738
Epoch 4346/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.37446596622467043
Epoch: 4346, Batch Gradient Norm: 10.414025448424955
Epoch: 4346, Batch Gradient Norm after: 10.414025448424955
Epoch 4347/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.37042399644851687
Epoch: 4347, Batch Gradient Norm: 11.405288915729214
Epoch: 4347, Batch Gradient Norm after: 11.405288915729214
Epoch 4348/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.37207964062690735
Epoch: 4348, Batch Gradient Norm: 12.493549858580929
Epoch: 4348, Batch Gradient Norm after: 12.493549858580929
Epoch 4349/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.3730353832244873
Epoch: 4349, Batch Gradient Norm: 11.392203414147819
Epoch: 4349, Batch Gradient Norm after: 11.392203414147819
Epoch 4350/10000, Prediction Accuracy = 62.45799999999999%, Loss = 0.3745403826236725
Epoch: 4350, Batch Gradient Norm: 13.784236782195892
Epoch: 4350, Batch Gradient Norm after: 13.784236782195892
Epoch 4351/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.37474897503852844
Epoch: 4351, Batch Gradient Norm: 14.79554162718467
Epoch: 4351, Batch Gradient Norm after: 14.79554162718467
Epoch 4352/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.37487504482269285
Epoch: 4352, Batch Gradient Norm: 13.412581336777297
Epoch: 4352, Batch Gradient Norm after: 13.412581336777297
Epoch 4353/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.3731440782546997
Epoch: 4353, Batch Gradient Norm: 15.053456499154011
Epoch: 4353, Batch Gradient Norm after: 15.053456499154011
Epoch 4354/10000, Prediction Accuracy = 62.41399999999999%, Loss = 0.37754592299461365
Epoch: 4354, Batch Gradient Norm: 16.046979296577145
Epoch: 4354, Batch Gradient Norm after: 16.046979296577145
Epoch 4355/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.3775448739528656
Epoch: 4355, Batch Gradient Norm: 14.60979374157557
Epoch: 4355, Batch Gradient Norm after: 14.60979374157557
Epoch 4356/10000, Prediction Accuracy = 62.464%, Loss = 0.377643084526062
Epoch: 4356, Batch Gradient Norm: 13.754782292281494
Epoch: 4356, Batch Gradient Norm after: 13.754782292281494
Epoch 4357/10000, Prediction Accuracy = 62.55%, Loss = 0.3794329822063446
Epoch: 4357, Batch Gradient Norm: 15.192108677753481
Epoch: 4357, Batch Gradient Norm after: 15.192108677753481
Epoch 4358/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.3785856783390045
Epoch: 4358, Batch Gradient Norm: 12.518338980843055
Epoch: 4358, Batch Gradient Norm after: 12.518338980843055
Epoch 4359/10000, Prediction Accuracy = 62.53399999999999%, Loss = 0.3735395908355713
Epoch: 4359, Batch Gradient Norm: 14.673833665600576
Epoch: 4359, Batch Gradient Norm after: 14.673833665600576
Epoch 4360/10000, Prediction Accuracy = 62.538%, Loss = 0.37788387537002566
Epoch: 4360, Batch Gradient Norm: 13.701639112981306
Epoch: 4360, Batch Gradient Norm after: 13.701639112981306
Epoch 4361/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.3741611659526825
Epoch: 4361, Batch Gradient Norm: 13.369188868908585
Epoch: 4361, Batch Gradient Norm after: 13.369188868908585
Epoch 4362/10000, Prediction Accuracy = 62.65%, Loss = 0.3764794111251831
Epoch: 4362, Batch Gradient Norm: 13.936407783900654
Epoch: 4362, Batch Gradient Norm after: 13.936407783900654
Epoch 4363/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.3738385200500488
Epoch: 4363, Batch Gradient Norm: 10.257975883027278
Epoch: 4363, Batch Gradient Norm after: 10.257975883027278
Epoch 4364/10000, Prediction Accuracy = 62.516000000000005%, Loss = 0.3718708693981171
Epoch: 4364, Batch Gradient Norm: 12.432537915081873
Epoch: 4364, Batch Gradient Norm after: 12.432537915081873
Epoch 4365/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.37458027005195615
Epoch: 4365, Batch Gradient Norm: 13.036817814442578
Epoch: 4365, Batch Gradient Norm after: 13.036817814442578
Epoch 4366/10000, Prediction Accuracy = 62.564%, Loss = 0.3741402268409729
Epoch: 4366, Batch Gradient Norm: 11.857258889639533
Epoch: 4366, Batch Gradient Norm after: 11.857258889639533
Epoch 4367/10000, Prediction Accuracy = 62.508%, Loss = 0.37276619076728823
Epoch: 4367, Batch Gradient Norm: 11.927243376530367
Epoch: 4367, Batch Gradient Norm after: 11.927243376530367
Epoch 4368/10000, Prediction Accuracy = 62.483999999999995%, Loss = 0.37192532420158386
Epoch: 4368, Batch Gradient Norm: 11.612336255191481
Epoch: 4368, Batch Gradient Norm after: 11.612336255191481
Epoch 4369/10000, Prediction Accuracy = 62.50600000000001%, Loss = 0.3712169051170349
Epoch: 4369, Batch Gradient Norm: 11.812313796165778
Epoch: 4369, Batch Gradient Norm after: 11.812313796165778
Epoch 4370/10000, Prediction Accuracy = 62.616%, Loss = 0.3728159427642822
Epoch: 4370, Batch Gradient Norm: 10.294400671885231
Epoch: 4370, Batch Gradient Norm after: 10.294400671885231
Epoch 4371/10000, Prediction Accuracy = 62.58%, Loss = 0.3712509453296661
Epoch: 4371, Batch Gradient Norm: 11.312981684324205
Epoch: 4371, Batch Gradient Norm after: 11.312981684324205
Epoch 4372/10000, Prediction Accuracy = 62.628%, Loss = 0.37261914610862734
Epoch: 4372, Batch Gradient Norm: 11.035751412643878
Epoch: 4372, Batch Gradient Norm after: 11.035751412643878
Epoch 4373/10000, Prediction Accuracy = 62.638%, Loss = 0.3696004569530487
Epoch: 4373, Batch Gradient Norm: 13.083067431818403
Epoch: 4373, Batch Gradient Norm after: 13.083067431818403
Epoch 4374/10000, Prediction Accuracy = 62.46%, Loss = 0.37320316433906553
Epoch: 4374, Batch Gradient Norm: 11.193048127242342
Epoch: 4374, Batch Gradient Norm after: 11.193048127242342
Epoch 4375/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.3717311441898346
Epoch: 4375, Batch Gradient Norm: 10.786980885237508
Epoch: 4375, Batch Gradient Norm after: 10.786980885237508
Epoch 4376/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.37020928263664243
Epoch: 4376, Batch Gradient Norm: 10.513343140058748
Epoch: 4376, Batch Gradient Norm after: 10.513343140058748
Epoch 4377/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.37018882632255556
Epoch: 4377, Batch Gradient Norm: 10.663449257453127
Epoch: 4377, Batch Gradient Norm after: 10.663449257453127
Epoch 4378/10000, Prediction Accuracy = 62.614%, Loss = 0.3679589807987213
Epoch: 4378, Batch Gradient Norm: 12.45688055060051
Epoch: 4378, Batch Gradient Norm after: 12.45688055060051
Epoch 4379/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.369837099313736
Epoch: 4379, Batch Gradient Norm: 9.208495391289402
Epoch: 4379, Batch Gradient Norm after: 9.208495391289402
Epoch 4380/10000, Prediction Accuracy = 62.622%, Loss = 0.3675354778766632
Epoch: 4380, Batch Gradient Norm: 10.186078181127899
Epoch: 4380, Batch Gradient Norm after: 10.186078181127899
Epoch 4381/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.36986133456230164
Epoch: 4381, Batch Gradient Norm: 9.451490953886676
Epoch: 4381, Batch Gradient Norm after: 9.451490953886676
Epoch 4382/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.37015012502670286
Epoch: 4382, Batch Gradient Norm: 9.755313984042115
Epoch: 4382, Batch Gradient Norm after: 9.755313984042115
Epoch 4383/10000, Prediction Accuracy = 62.536%, Loss = 0.3704447329044342
Epoch: 4383, Batch Gradient Norm: 11.417846178802474
Epoch: 4383, Batch Gradient Norm after: 11.417846178802474
Epoch 4384/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.3695980668067932
Epoch: 4384, Batch Gradient Norm: 13.597261066287352
Epoch: 4384, Batch Gradient Norm after: 13.597261066287352
Epoch 4385/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.37314743995666505
Epoch: 4385, Batch Gradient Norm: 13.031078593416867
Epoch: 4385, Batch Gradient Norm after: 13.031078593416867
Epoch 4386/10000, Prediction Accuracy = 62.48%, Loss = 0.37217419147491454
Epoch: 4386, Batch Gradient Norm: 10.858388335942864
Epoch: 4386, Batch Gradient Norm after: 10.858388335942864
Epoch 4387/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.37143065333366393
Epoch: 4387, Batch Gradient Norm: 13.466229331909439
Epoch: 4387, Batch Gradient Norm after: 13.466229331909439
Epoch 4388/10000, Prediction Accuracy = 62.54%, Loss = 0.3714087903499603
Epoch: 4388, Batch Gradient Norm: 13.21911019869425
Epoch: 4388, Batch Gradient Norm after: 13.21911019869425
Epoch 4389/10000, Prediction Accuracy = 62.492%, Loss = 0.37494056224822997
Epoch: 4389, Batch Gradient Norm: 12.184108657225442
Epoch: 4389, Batch Gradient Norm after: 12.184108657225442
Epoch 4390/10000, Prediction Accuracy = 62.436%, Loss = 0.3716392397880554
Epoch: 4390, Batch Gradient Norm: 10.166388914588488
Epoch: 4390, Batch Gradient Norm after: 10.166388914588488
Epoch 4391/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.3699641823768616
Epoch: 4391, Batch Gradient Norm: 8.04459246967135
Epoch: 4391, Batch Gradient Norm after: 8.04459246967135
Epoch 4392/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.36781010031700134
Epoch: 4392, Batch Gradient Norm: 10.607629705863696
Epoch: 4392, Batch Gradient Norm after: 10.607629705863696
Epoch 4393/10000, Prediction Accuracy = 62.648%, Loss = 0.36876190900802613
Epoch: 4393, Batch Gradient Norm: 14.263583365204292
Epoch: 4393, Batch Gradient Norm after: 14.263583365204292
Epoch 4394/10000, Prediction Accuracy = 62.402%, Loss = 0.3747377753257751
Epoch: 4394, Batch Gradient Norm: 12.276961947214396
Epoch: 4394, Batch Gradient Norm after: 12.276961947214396
Epoch 4395/10000, Prediction Accuracy = 62.580000000000005%, Loss = 0.3714455783367157
Epoch: 4395, Batch Gradient Norm: 13.790511418841115
Epoch: 4395, Batch Gradient Norm after: 13.790511418841115
Epoch 4396/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.3738785803318024
Epoch: 4396, Batch Gradient Norm: 14.749639980844924
Epoch: 4396, Batch Gradient Norm after: 14.749639980844924
Epoch 4397/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.37505685091018676
Epoch: 4397, Batch Gradient Norm: 12.429254364883874
Epoch: 4397, Batch Gradient Norm after: 12.429254364883874
Epoch 4398/10000, Prediction Accuracy = 62.612%, Loss = 0.3736609220504761
Epoch: 4398, Batch Gradient Norm: 11.803834386156364
Epoch: 4398, Batch Gradient Norm after: 11.803834386156364
Epoch 4399/10000, Prediction Accuracy = 62.544000000000004%, Loss = 0.37083699107170104
Epoch: 4399, Batch Gradient Norm: 13.262197492384031
Epoch: 4399, Batch Gradient Norm after: 13.262197492384031
Epoch 4400/10000, Prediction Accuracy = 62.55%, Loss = 0.37246585488319395
Epoch: 4400, Batch Gradient Norm: 12.080392573905398
Epoch: 4400, Batch Gradient Norm after: 12.080392573905398
Epoch 4401/10000, Prediction Accuracy = 62.616%, Loss = 0.37092223167419436
Epoch: 4401, Batch Gradient Norm: 11.495987210116935
Epoch: 4401, Batch Gradient Norm after: 11.495987210116935
Epoch 4402/10000, Prediction Accuracy = 62.53000000000001%, Loss = 0.371576315164566
Epoch: 4402, Batch Gradient Norm: 11.496239731057031
Epoch: 4402, Batch Gradient Norm after: 11.496239731057031
Epoch 4403/10000, Prediction Accuracy = 62.664%, Loss = 0.3701870620250702
Epoch: 4403, Batch Gradient Norm: 11.508482915127002
Epoch: 4403, Batch Gradient Norm after: 11.508482915127002
Epoch 4404/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.37121307849884033
Epoch: 4404, Batch Gradient Norm: 11.92901655754688
Epoch: 4404, Batch Gradient Norm after: 11.92901655754688
Epoch 4405/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.3717345356941223
Epoch: 4405, Batch Gradient Norm: 13.894556987618772
Epoch: 4405, Batch Gradient Norm after: 13.894556987618772
Epoch 4406/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.3736105799674988
Epoch: 4406, Batch Gradient Norm: 13.14438143993529
Epoch: 4406, Batch Gradient Norm after: 13.14438143993529
Epoch 4407/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.37372986078262327
Epoch: 4407, Batch Gradient Norm: 10.252634802016821
Epoch: 4407, Batch Gradient Norm after: 10.252634802016821
Epoch 4408/10000, Prediction Accuracy = 62.588%, Loss = 0.3698261618614197
Epoch: 4408, Batch Gradient Norm: 11.238697443530535
Epoch: 4408, Batch Gradient Norm after: 11.238697443530535
Epoch 4409/10000, Prediction Accuracy = 62.61%, Loss = 0.36979749202728274
Epoch: 4409, Batch Gradient Norm: 11.080408869846265
Epoch: 4409, Batch Gradient Norm after: 11.080408869846265
Epoch 4410/10000, Prediction Accuracy = 62.53399999999999%, Loss = 0.3690595328807831
Epoch: 4410, Batch Gradient Norm: 11.922568886522368
Epoch: 4410, Batch Gradient Norm after: 11.922568886522368
Epoch 4411/10000, Prediction Accuracy = 62.53399999999999%, Loss = 0.36894147396087645
Epoch: 4411, Batch Gradient Norm: 11.058082455365454
Epoch: 4411, Batch Gradient Norm after: 11.058082455365454
Epoch 4412/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.36916530728340147
Epoch: 4412, Batch Gradient Norm: 10.37822738986089
Epoch: 4412, Batch Gradient Norm after: 10.37822738986089
Epoch 4413/10000, Prediction Accuracy = 62.55%, Loss = 0.3706827461719513
Epoch: 4413, Batch Gradient Norm: 13.009838020946837
Epoch: 4413, Batch Gradient Norm after: 13.009838020946837
Epoch 4414/10000, Prediction Accuracy = 62.572%, Loss = 0.37145928144454954
Epoch: 4414, Batch Gradient Norm: 15.173287015768759
Epoch: 4414, Batch Gradient Norm after: 15.173287015768759
Epoch 4415/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.37400381565093993
Epoch: 4415, Batch Gradient Norm: 12.659048005770893
Epoch: 4415, Batch Gradient Norm after: 12.659048005770893
Epoch 4416/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.3710748732089996
Epoch: 4416, Batch Gradient Norm: 13.347733780955803
Epoch: 4416, Batch Gradient Norm after: 13.347733780955803
Epoch 4417/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.3743147969245911
Epoch: 4417, Batch Gradient Norm: 14.398653823026576
Epoch: 4417, Batch Gradient Norm after: 14.398653823026576
Epoch 4418/10000, Prediction Accuracy = 62.648%, Loss = 0.3724353551864624
Epoch: 4418, Batch Gradient Norm: 11.060197129688138
Epoch: 4418, Batch Gradient Norm after: 11.060197129688138
Epoch 4419/10000, Prediction Accuracy = 62.562%, Loss = 0.37052599191665647
Epoch: 4419, Batch Gradient Norm: 11.28973862048826
Epoch: 4419, Batch Gradient Norm after: 11.28973862048826
Epoch 4420/10000, Prediction Accuracy = 62.552%, Loss = 0.3694959878921509
Epoch: 4420, Batch Gradient Norm: 12.190813224162358
Epoch: 4420, Batch Gradient Norm after: 12.190813224162358
Epoch 4421/10000, Prediction Accuracy = 62.652%, Loss = 0.37086278200149536
Epoch: 4421, Batch Gradient Norm: 14.669570238457899
Epoch: 4421, Batch Gradient Norm after: 14.669570238457899
Epoch 4422/10000, Prediction Accuracy = 62.584%, Loss = 0.37539111971855166
Epoch: 4422, Batch Gradient Norm: 10.904805316679294
Epoch: 4422, Batch Gradient Norm after: 10.904805316679294
Epoch 4423/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.36873136162757875
Epoch: 4423, Batch Gradient Norm: 9.221254396285367
Epoch: 4423, Batch Gradient Norm after: 9.221254396285367
Epoch 4424/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.36790044903755187
Epoch: 4424, Batch Gradient Norm: 11.889190347562534
Epoch: 4424, Batch Gradient Norm after: 11.889190347562534
Epoch 4425/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.3710625827312469
Epoch: 4425, Batch Gradient Norm: 10.74718556718642
Epoch: 4425, Batch Gradient Norm after: 10.74718556718642
Epoch 4426/10000, Prediction Accuracy = 62.71200000000001%, Loss = 0.36933578848838805
Epoch: 4426, Batch Gradient Norm: 9.25164012861766
Epoch: 4426, Batch Gradient Norm after: 9.25164012861766
Epoch 4427/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.3669073939323425
Epoch: 4427, Batch Gradient Norm: 9.736530933787629
Epoch: 4427, Batch Gradient Norm after: 9.736530933787629
Epoch 4428/10000, Prediction Accuracy = 62.604%, Loss = 0.3664123058319092
Epoch: 4428, Batch Gradient Norm: 9.508643027586837
Epoch: 4428, Batch Gradient Norm after: 9.508643027586837
Epoch 4429/10000, Prediction Accuracy = 62.662%, Loss = 0.3666008234024048
Epoch: 4429, Batch Gradient Norm: 12.438489329513988
Epoch: 4429, Batch Gradient Norm after: 12.438489329513988
Epoch 4430/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.37032017707824705
Epoch: 4430, Batch Gradient Norm: 13.178037624014262
Epoch: 4430, Batch Gradient Norm after: 13.178037624014262
Epoch 4431/10000, Prediction Accuracy = 62.495999999999995%, Loss = 0.37571591734886167
Epoch: 4431, Batch Gradient Norm: 12.946234922156668
Epoch: 4431, Batch Gradient Norm after: 12.946234922156668
Epoch 4432/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.37243573665618895
Epoch: 4432, Batch Gradient Norm: 14.289019381882492
Epoch: 4432, Batch Gradient Norm after: 14.289019381882492
Epoch 4433/10000, Prediction Accuracy = 62.69%, Loss = 0.37296282649040224
Epoch: 4433, Batch Gradient Norm: 13.411510853549338
Epoch: 4433, Batch Gradient Norm after: 13.411510853549338
Epoch 4434/10000, Prediction Accuracy = 62.666%, Loss = 0.37099209427833557
Epoch: 4434, Batch Gradient Norm: 13.196752192273568
Epoch: 4434, Batch Gradient Norm after: 13.196752192273568
Epoch 4435/10000, Prediction Accuracy = 62.669999999999995%, Loss = 0.37079030871391294
Epoch: 4435, Batch Gradient Norm: 13.10205445783951
Epoch: 4435, Batch Gradient Norm after: 13.10205445783951
Epoch 4436/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.37320044040679934
Epoch: 4436, Batch Gradient Norm: 11.023112419930337
Epoch: 4436, Batch Gradient Norm after: 11.023112419930337
Epoch 4437/10000, Prediction Accuracy = 62.62399999999999%, Loss = 0.3679616451263428
Epoch: 4437, Batch Gradient Norm: 10.816908679761786
Epoch: 4437, Batch Gradient Norm after: 10.816908679761786
Epoch 4438/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.3681790351867676
Epoch: 4438, Batch Gradient Norm: 12.194755059591815
Epoch: 4438, Batch Gradient Norm after: 12.194755059591815
Epoch 4439/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.3705876588821411
Epoch: 4439, Batch Gradient Norm: 10.365529815898169
Epoch: 4439, Batch Gradient Norm after: 10.365529815898169
Epoch 4440/10000, Prediction Accuracy = 62.556%, Loss = 0.3687708556652069
Epoch: 4440, Batch Gradient Norm: 12.344827640796307
Epoch: 4440, Batch Gradient Norm after: 12.344827640796307
Epoch 4441/10000, Prediction Accuracy = 62.512%, Loss = 0.3716390550136566
Epoch: 4441, Batch Gradient Norm: 11.96829467272311
Epoch: 4441, Batch Gradient Norm after: 11.96829467272311
Epoch 4442/10000, Prediction Accuracy = 62.61%, Loss = 0.3684880375862122
Epoch: 4442, Batch Gradient Norm: 13.805955025644465
Epoch: 4442, Batch Gradient Norm after: 13.805955025644465
Epoch 4443/10000, Prediction Accuracy = 62.56400000000001%, Loss = 0.37485707402229307
Epoch: 4443, Batch Gradient Norm: 11.49871207386731
Epoch: 4443, Batch Gradient Norm after: 11.49871207386731
Epoch 4444/10000, Prediction Accuracy = 62.516%, Loss = 0.36980674862861634
Epoch: 4444, Batch Gradient Norm: 10.836446085644639
Epoch: 4444, Batch Gradient Norm after: 10.836446085644639
Epoch 4445/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.36821833848953245
Epoch: 4445, Batch Gradient Norm: 12.052620534488915
Epoch: 4445, Batch Gradient Norm after: 12.052620534488915
Epoch 4446/10000, Prediction Accuracy = 62.698%, Loss = 0.3697129786014557
Epoch: 4446, Batch Gradient Norm: 15.985943377864698
Epoch: 4446, Batch Gradient Norm after: 15.985943377864698
Epoch 4447/10000, Prediction Accuracy = 62.602%, Loss = 0.3764944612979889
Epoch: 4447, Batch Gradient Norm: 17.29278590096585
Epoch: 4447, Batch Gradient Norm after: 17.29278590096585
Epoch 4448/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.3768123507499695
Epoch: 4448, Batch Gradient Norm: 17.587978253615933
Epoch: 4448, Batch Gradient Norm after: 17.4176786774023
Epoch 4449/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3767202913761139
Epoch: 4449, Batch Gradient Norm: 14.892067950606254
Epoch: 4449, Batch Gradient Norm after: 14.892067950606254
Epoch 4450/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.37219439148902894
Epoch: 4450, Batch Gradient Norm: 11.091082759516944
Epoch: 4450, Batch Gradient Norm after: 11.091082759516944
Epoch 4451/10000, Prediction Accuracy = 62.782%, Loss = 0.3688492953777313
Epoch: 4451, Batch Gradient Norm: 12.017397403605871
Epoch: 4451, Batch Gradient Norm after: 12.017397403605871
Epoch 4452/10000, Prediction Accuracy = 62.56200000000001%, Loss = 0.3712283134460449
Epoch: 4452, Batch Gradient Norm: 13.908895982404218
Epoch: 4452, Batch Gradient Norm after: 13.908895982404218
Epoch 4453/10000, Prediction Accuracy = 62.572%, Loss = 0.37330418825149536
Epoch: 4453, Batch Gradient Norm: 13.027562302091118
Epoch: 4453, Batch Gradient Norm after: 13.027562302091118
Epoch 4454/10000, Prediction Accuracy = 62.5%, Loss = 0.3724353194236755
Epoch: 4454, Batch Gradient Norm: 13.690928090930438
Epoch: 4454, Batch Gradient Norm after: 13.690928090930438
Epoch 4455/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.3699361026287079
Epoch: 4455, Batch Gradient Norm: 13.075735710027885
Epoch: 4455, Batch Gradient Norm after: 13.075735710027885
Epoch 4456/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3715377330780029
Epoch: 4456, Batch Gradient Norm: 13.234524771026582
Epoch: 4456, Batch Gradient Norm after: 13.234524771026582
Epoch 4457/10000, Prediction Accuracy = 62.56%, Loss = 0.3737901449203491
Epoch: 4457, Batch Gradient Norm: 11.250251203199525
Epoch: 4457, Batch Gradient Norm after: 11.250251203199525
Epoch 4458/10000, Prediction Accuracy = 62.638%, Loss = 0.3685014069080353
Epoch: 4458, Batch Gradient Norm: 10.419844154302487
Epoch: 4458, Batch Gradient Norm after: 10.419844154302487
Epoch 4459/10000, Prediction Accuracy = 62.552%, Loss = 0.36575195789337156
Epoch: 4459, Batch Gradient Norm: 12.739628368575753
Epoch: 4459, Batch Gradient Norm after: 12.739628368575753
Epoch 4460/10000, Prediction Accuracy = 62.524%, Loss = 0.37200188636779785
Epoch: 4460, Batch Gradient Norm: 13.479227668116161
Epoch: 4460, Batch Gradient Norm after: 13.479227668116161
Epoch 4461/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.3708822190761566
Epoch: 4461, Batch Gradient Norm: 11.90532794294375
Epoch: 4461, Batch Gradient Norm after: 11.90532794294375
Epoch 4462/10000, Prediction Accuracy = 62.564%, Loss = 0.3686807036399841
Epoch: 4462, Batch Gradient Norm: 13.217822455315682
Epoch: 4462, Batch Gradient Norm after: 13.217822455315682
Epoch 4463/10000, Prediction Accuracy = 62.448%, Loss = 0.37005234956741334
Epoch: 4463, Batch Gradient Norm: 13.840303277201782
Epoch: 4463, Batch Gradient Norm after: 13.840303277201782
Epoch 4464/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.3728197753429413
Epoch: 4464, Batch Gradient Norm: 15.21239720822785
Epoch: 4464, Batch Gradient Norm after: 15.21239720822785
Epoch 4465/10000, Prediction Accuracy = 62.682%, Loss = 0.3722525298595428
Epoch: 4465, Batch Gradient Norm: 15.620819703595584
Epoch: 4465, Batch Gradient Norm after: 15.620819703595584
Epoch 4466/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.3737360596656799
Epoch: 4466, Batch Gradient Norm: 12.571927899645765
Epoch: 4466, Batch Gradient Norm after: 12.571927899645765
Epoch 4467/10000, Prediction Accuracy = 62.648%, Loss = 0.36934362053871156
Epoch: 4467, Batch Gradient Norm: 11.880741096998582
Epoch: 4467, Batch Gradient Norm after: 11.880741096998582
Epoch 4468/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.36916476488113403
Epoch: 4468, Batch Gradient Norm: 10.230350010469778
Epoch: 4468, Batch Gradient Norm after: 10.230350010469778
Epoch 4469/10000, Prediction Accuracy = 62.598%, Loss = 0.3681492805480957
Epoch: 4469, Batch Gradient Norm: 11.35013484162259
Epoch: 4469, Batch Gradient Norm after: 11.35013484162259
Epoch 4470/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.36917256712913515
Epoch: 4470, Batch Gradient Norm: 12.095806150375218
Epoch: 4470, Batch Gradient Norm after: 12.095806150375218
Epoch 4471/10000, Prediction Accuracy = 62.664%, Loss = 0.3708302974700928
Epoch: 4471, Batch Gradient Norm: 13.732320399331885
Epoch: 4471, Batch Gradient Norm after: 13.732320399331885
Epoch 4472/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.37092626094818115
Epoch: 4472, Batch Gradient Norm: 15.036417465771182
Epoch: 4472, Batch Gradient Norm after: 15.036417465771182
Epoch 4473/10000, Prediction Accuracy = 62.556000000000004%, Loss = 0.374855101108551
Epoch: 4473, Batch Gradient Norm: 12.183235250760761
Epoch: 4473, Batch Gradient Norm after: 12.183235250760761
Epoch 4474/10000, Prediction Accuracy = 62.60999999999999%, Loss = 0.36881694197654724
Epoch: 4474, Batch Gradient Norm: 11.34750198113758
Epoch: 4474, Batch Gradient Norm after: 11.34750198113758
Epoch 4475/10000, Prediction Accuracy = 62.56600000000001%, Loss = 0.36755427718162537
Epoch: 4475, Batch Gradient Norm: 11.681270143505522
Epoch: 4475, Batch Gradient Norm after: 11.681270143505522
Epoch 4476/10000, Prediction Accuracy = 62.588%, Loss = 0.3676910221576691
Epoch: 4476, Batch Gradient Norm: 13.529486730595828
Epoch: 4476, Batch Gradient Norm after: 13.529486730595828
Epoch 4477/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.369294011592865
Epoch: 4477, Batch Gradient Norm: 13.03770342730725
Epoch: 4477, Batch Gradient Norm after: 13.03770342730725
Epoch 4478/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.36925299763679503
Epoch: 4478, Batch Gradient Norm: 15.358499951392176
Epoch: 4478, Batch Gradient Norm after: 15.358499951392176
Epoch 4479/10000, Prediction Accuracy = 62.65%, Loss = 0.37371698021888733
Epoch: 4479, Batch Gradient Norm: 12.767278906071885
Epoch: 4479, Batch Gradient Norm after: 12.767278906071885
Epoch 4480/10000, Prediction Accuracy = 62.626%, Loss = 0.3689349293708801
Epoch: 4480, Batch Gradient Norm: 13.25756324550863
Epoch: 4480, Batch Gradient Norm after: 13.25756324550863
Epoch 4481/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.36978837847709656
Epoch: 4481, Batch Gradient Norm: 13.445786516023643
Epoch: 4481, Batch Gradient Norm after: 13.445786516023643
Epoch 4482/10000, Prediction Accuracy = 62.65999999999999%, Loss = 0.3707712352275848
Epoch: 4482, Batch Gradient Norm: 13.475516218914823
Epoch: 4482, Batch Gradient Norm after: 13.475516218914823
Epoch 4483/10000, Prediction Accuracy = 62.664%, Loss = 0.3700281023979187
Epoch: 4483, Batch Gradient Norm: 12.778770432022116
Epoch: 4483, Batch Gradient Norm after: 12.778770432022116
Epoch 4484/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.37051469683647154
Epoch: 4484, Batch Gradient Norm: 13.815420776446294
Epoch: 4484, Batch Gradient Norm after: 13.815420776446294
Epoch 4485/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.37296419143676757
Epoch: 4485, Batch Gradient Norm: 16.503882002313127
Epoch: 4485, Batch Gradient Norm after: 16.503882002313127
Epoch 4486/10000, Prediction Accuracy = 62.738%, Loss = 0.3760181427001953
Epoch: 4486, Batch Gradient Norm: 16.517681541438506
Epoch: 4486, Batch Gradient Norm after: 16.517681541438506
Epoch 4487/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.37781564593315126
Epoch: 4487, Batch Gradient Norm: 17.071106520466994
Epoch: 4487, Batch Gradient Norm after: 17.071106520466994
Epoch 4488/10000, Prediction Accuracy = 62.54599999999999%, Loss = 0.3763969123363495
Epoch: 4488, Batch Gradient Norm: 14.352364885252452
Epoch: 4488, Batch Gradient Norm after: 14.352364885252452
Epoch 4489/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.37167707085609436
Epoch: 4489, Batch Gradient Norm: 12.090544794455518
Epoch: 4489, Batch Gradient Norm after: 12.090544794455518
Epoch 4490/10000, Prediction Accuracy = 62.562%, Loss = 0.37036635875701907
Epoch: 4490, Batch Gradient Norm: 10.872306165709874
Epoch: 4490, Batch Gradient Norm after: 10.872306165709874
Epoch 4491/10000, Prediction Accuracy = 62.604%, Loss = 0.368500155210495
Epoch: 4491, Batch Gradient Norm: 12.049200624959747
Epoch: 4491, Batch Gradient Norm after: 12.049200624959747
Epoch 4492/10000, Prediction Accuracy = 62.572%, Loss = 0.3699399888515472
Epoch: 4492, Batch Gradient Norm: 11.187032075449876
Epoch: 4492, Batch Gradient Norm after: 11.187032075449876
Epoch 4493/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.36840252876281737
Epoch: 4493, Batch Gradient Norm: 12.276500295488805
Epoch: 4493, Batch Gradient Norm after: 12.276500295488805
Epoch 4494/10000, Prediction Accuracy = 62.512%, Loss = 0.36955074071884153
Epoch: 4494, Batch Gradient Norm: 13.804442726676651
Epoch: 4494, Batch Gradient Norm after: 13.804442726676651
Epoch 4495/10000, Prediction Accuracy = 62.6%, Loss = 0.3711488425731659
Epoch: 4495, Batch Gradient Norm: 12.769709227490857
Epoch: 4495, Batch Gradient Norm after: 12.769709227490857
Epoch 4496/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.36959159970283506
Epoch: 4496, Batch Gradient Norm: 9.421450318241055
Epoch: 4496, Batch Gradient Norm after: 9.421450318241055
Epoch 4497/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.36533504724502563
Epoch: 4497, Batch Gradient Norm: 11.504943944114718
Epoch: 4497, Batch Gradient Norm after: 11.504943944114718
Epoch 4498/10000, Prediction Accuracy = 62.58399999999999%, Loss = 0.3693339109420776
Epoch: 4498, Batch Gradient Norm: 11.225091684618054
Epoch: 4498, Batch Gradient Norm after: 11.225091684618054
Epoch 4499/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.36877602338790894
Epoch: 4499, Batch Gradient Norm: 11.267806703593473
Epoch: 4499, Batch Gradient Norm after: 11.267806703593473
Epoch 4500/10000, Prediction Accuracy = 62.634%, Loss = 0.3662811040878296
Epoch: 4500, Batch Gradient Norm: 11.811217221024236
Epoch: 4500, Batch Gradient Norm after: 11.811217221024236
Epoch 4501/10000, Prediction Accuracy = 62.65%, Loss = 0.36677756905555725
Epoch: 4501, Batch Gradient Norm: 13.22524803781767
Epoch: 4501, Batch Gradient Norm after: 13.22524803781767
Epoch 4502/10000, Prediction Accuracy = 62.645999999999994%, Loss = 0.368783050775528
Epoch: 4502, Batch Gradient Norm: 13.364063184754155
Epoch: 4502, Batch Gradient Norm after: 13.364063184754155
Epoch 4503/10000, Prediction Accuracy = 62.678%, Loss = 0.36900562047958374
Epoch: 4503, Batch Gradient Norm: 15.021404248616713
Epoch: 4503, Batch Gradient Norm after: 15.021404248616713
Epoch 4504/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.3714905261993408
Epoch: 4504, Batch Gradient Norm: 13.589365441480133
Epoch: 4504, Batch Gradient Norm after: 13.589365441480133
Epoch 4505/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.36883561611175536
Epoch: 4505, Batch Gradient Norm: 13.141575319324222
Epoch: 4505, Batch Gradient Norm after: 13.141575319324222
Epoch 4506/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.3687646806240082
Epoch: 4506, Batch Gradient Norm: 14.045052425011589
Epoch: 4506, Batch Gradient Norm after: 14.045052425011589
Epoch 4507/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.37013280391693115
Epoch: 4507, Batch Gradient Norm: 14.453180501089843
Epoch: 4507, Batch Gradient Norm after: 14.453180501089843
Epoch 4508/10000, Prediction Accuracy = 62.552%, Loss = 0.3706331253051758
Epoch: 4508, Batch Gradient Norm: 13.574100335152883
Epoch: 4508, Batch Gradient Norm after: 13.574100335152883
Epoch 4509/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.36941736936569214
Epoch: 4509, Batch Gradient Norm: 12.670679203228959
Epoch: 4509, Batch Gradient Norm after: 12.670679203228959
Epoch 4510/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.3687531054019928
Epoch: 4510, Batch Gradient Norm: 11.214215843738797
Epoch: 4510, Batch Gradient Norm after: 11.214215843738797
Epoch 4511/10000, Prediction Accuracy = 62.658%, Loss = 0.367375385761261
Epoch: 4511, Batch Gradient Norm: 15.175056276906014
Epoch: 4511, Batch Gradient Norm after: 15.175056276906014
Epoch 4512/10000, Prediction Accuracy = 62.714%, Loss = 0.37379215359687806
Epoch: 4512, Batch Gradient Norm: 14.142844983269612
Epoch: 4512, Batch Gradient Norm after: 14.142844983269612
Epoch 4513/10000, Prediction Accuracy = 62.666%, Loss = 0.3703105032444
Epoch: 4513, Batch Gradient Norm: 15.192838264369776
Epoch: 4513, Batch Gradient Norm after: 15.192838264369776
Epoch 4514/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.37272305488586427
Epoch: 4514, Batch Gradient Norm: 14.550183829528802
Epoch: 4514, Batch Gradient Norm after: 14.550183829528802
Epoch 4515/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.37304850220680236
Epoch: 4515, Batch Gradient Norm: 12.550568503160124
Epoch: 4515, Batch Gradient Norm after: 12.550568503160124
Epoch 4516/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3693869233131409
Epoch: 4516, Batch Gradient Norm: 11.459059232514043
Epoch: 4516, Batch Gradient Norm after: 11.459059232514043
Epoch 4517/10000, Prediction Accuracy = 62.634%, Loss = 0.3675500154495239
Epoch: 4517, Batch Gradient Norm: 11.236557074457984
Epoch: 4517, Batch Gradient Norm after: 11.236557074457984
Epoch 4518/10000, Prediction Accuracy = 62.678%, Loss = 0.36506372690200806
Epoch: 4518, Batch Gradient Norm: 14.92450818344924
Epoch: 4518, Batch Gradient Norm after: 14.92450818344924
Epoch 4519/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.37346715331077573
Epoch: 4519, Batch Gradient Norm: 14.339629266228277
Epoch: 4519, Batch Gradient Norm after: 14.339629266228277
Epoch 4520/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.37328038215637205
Epoch: 4520, Batch Gradient Norm: 11.40076406765199
Epoch: 4520, Batch Gradient Norm after: 11.40076406765199
Epoch 4521/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.36637580394744873
Epoch: 4521, Batch Gradient Norm: 12.273994710866333
Epoch: 4521, Batch Gradient Norm after: 12.273994710866333
Epoch 4522/10000, Prediction Accuracy = 62.532%, Loss = 0.36936333775520325
Epoch: 4522, Batch Gradient Norm: 12.250445670380863
Epoch: 4522, Batch Gradient Norm after: 12.250445670380863
Epoch 4523/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.36709892749786377
Epoch: 4523, Batch Gradient Norm: 14.437423921790767
Epoch: 4523, Batch Gradient Norm after: 14.437423921790767
Epoch 4524/10000, Prediction Accuracy = 62.568%, Loss = 0.37281823754310606
Epoch: 4524, Batch Gradient Norm: 13.01145045785485
Epoch: 4524, Batch Gradient Norm after: 13.01145045785485
Epoch 4525/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.3696527659893036
Epoch: 4525, Batch Gradient Norm: 13.6527376699648
Epoch: 4525, Batch Gradient Norm after: 13.6527376699648
Epoch 4526/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.37044445872306825
Epoch: 4526, Batch Gradient Norm: 13.817547735093916
Epoch: 4526, Batch Gradient Norm after: 13.817547735093916
Epoch 4527/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.36989580392837523
Epoch: 4527, Batch Gradient Norm: 15.85748955413601
Epoch: 4527, Batch Gradient Norm after: 15.85748955413601
Epoch 4528/10000, Prediction Accuracy = 62.662%, Loss = 0.3735948920249939
Epoch: 4528, Batch Gradient Norm: 14.872394681299557
Epoch: 4528, Batch Gradient Norm after: 14.872394681299557
Epoch 4529/10000, Prediction Accuracy = 62.67%, Loss = 0.3727780759334564
Epoch: 4529, Batch Gradient Norm: 11.330930638274474
Epoch: 4529, Batch Gradient Norm after: 11.330930638274474
Epoch 4530/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.3660299003124237
Epoch: 4530, Batch Gradient Norm: 11.736168351381906
Epoch: 4530, Batch Gradient Norm after: 11.736168351381906
Epoch 4531/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.36823996901512146
Epoch: 4531, Batch Gradient Norm: 12.155023182811842
Epoch: 4531, Batch Gradient Norm after: 12.155023182811842
Epoch 4532/10000, Prediction Accuracy = 62.562%, Loss = 0.36987813711166384
Epoch: 4532, Batch Gradient Norm: 13.130082079284703
Epoch: 4532, Batch Gradient Norm after: 13.130082079284703
Epoch 4533/10000, Prediction Accuracy = 62.676%, Loss = 0.3701846718788147
Epoch: 4533, Batch Gradient Norm: 13.970045734683666
Epoch: 4533, Batch Gradient Norm after: 13.970045734683666
Epoch 4534/10000, Prediction Accuracy = 62.67%, Loss = 0.3689262390136719
Epoch: 4534, Batch Gradient Norm: 12.269910592419002
Epoch: 4534, Batch Gradient Norm after: 12.269910592419002
Epoch 4535/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.3664771318435669
Epoch: 4535, Batch Gradient Norm: 11.63810334881971
Epoch: 4535, Batch Gradient Norm after: 11.63810334881971
Epoch 4536/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.36805187463760375
Epoch: 4536, Batch Gradient Norm: 11.323505270429523
Epoch: 4536, Batch Gradient Norm after: 11.323505270429523
Epoch 4537/10000, Prediction Accuracy = 62.636%, Loss = 0.368412572145462
Epoch: 4537, Batch Gradient Norm: 13.792981979243146
Epoch: 4537, Batch Gradient Norm after: 13.792981979243146
Epoch 4538/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.36792155504226687
Epoch: 4538, Batch Gradient Norm: 13.053867463388732
Epoch: 4538, Batch Gradient Norm after: 13.053867463388732
Epoch 4539/10000, Prediction Accuracy = 62.548%, Loss = 0.36755610108375547
Epoch: 4539, Batch Gradient Norm: 12.219662658229831
Epoch: 4539, Batch Gradient Norm after: 12.219662658229831
Epoch 4540/10000, Prediction Accuracy = 62.702%, Loss = 0.36724738478660585
Epoch: 4540, Batch Gradient Norm: 12.096228265902235
Epoch: 4540, Batch Gradient Norm after: 12.096228265902235
Epoch 4541/10000, Prediction Accuracy = 62.604%, Loss = 0.36617985367774963
Epoch: 4541, Batch Gradient Norm: 12.196782386956386
Epoch: 4541, Batch Gradient Norm after: 12.196782386956386
Epoch 4542/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.36930217742919924
Epoch: 4542, Batch Gradient Norm: 10.926759730604125
Epoch: 4542, Batch Gradient Norm after: 10.926759730604125
Epoch 4543/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.36381853818893434
Epoch: 4543, Batch Gradient Norm: 12.978700757218942
Epoch: 4543, Batch Gradient Norm after: 12.978700757218942
Epoch 4544/10000, Prediction Accuracy = 62.626%, Loss = 0.3683516323566437
Epoch: 4544, Batch Gradient Norm: 11.247256895870354
Epoch: 4544, Batch Gradient Norm after: 11.247256895870354
Epoch 4545/10000, Prediction Accuracy = 62.646%, Loss = 0.36553303599357606
Epoch: 4545, Batch Gradient Norm: 16.72895534670952
Epoch: 4545, Batch Gradient Norm after: 16.72895534670952
Epoch 4546/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.37354850172996523
Epoch: 4546, Batch Gradient Norm: 17.762006458046482
Epoch: 4546, Batch Gradient Norm after: 17.691336422891187
Epoch 4547/10000, Prediction Accuracy = 62.69%, Loss = 0.37537562251091006
Epoch: 4547, Batch Gradient Norm: 14.859157386900764
Epoch: 4547, Batch Gradient Norm after: 14.859157386900764
Epoch 4548/10000, Prediction Accuracy = 62.666%, Loss = 0.37108861207962035
Epoch: 4548, Batch Gradient Norm: 15.535827954193978
Epoch: 4548, Batch Gradient Norm after: 15.535827954193978
Epoch 4549/10000, Prediction Accuracy = 62.556%, Loss = 0.37404589653015136
Epoch: 4549, Batch Gradient Norm: 18.337893573016988
Epoch: 4549, Batch Gradient Norm after: 18.337893573016988
Epoch 4550/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.3768049955368042
Epoch: 4550, Batch Gradient Norm: 14.95913922696607
Epoch: 4550, Batch Gradient Norm after: 14.95913922696607
Epoch 4551/10000, Prediction Accuracy = 62.80200000000001%, Loss = 0.36955565214157104
Epoch: 4551, Batch Gradient Norm: 13.196091764052175
Epoch: 4551, Batch Gradient Norm after: 13.196091764052175
Epoch 4552/10000, Prediction Accuracy = 62.622%, Loss = 0.36960087418556214
Epoch: 4552, Batch Gradient Norm: 12.10910511010826
Epoch: 4552, Batch Gradient Norm after: 12.10910511010826
Epoch 4553/10000, Prediction Accuracy = 62.73%, Loss = 0.36700956225395204
Epoch: 4553, Batch Gradient Norm: 15.852499505126941
Epoch: 4553, Batch Gradient Norm after: 15.852499505126941
Epoch 4554/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.3724498629570007
Epoch: 4554, Batch Gradient Norm: 15.249535537164546
Epoch: 4554, Batch Gradient Norm after: 15.249535537164546
Epoch 4555/10000, Prediction Accuracy = 62.702%, Loss = 0.37276920676231384
Epoch: 4555, Batch Gradient Norm: 12.59953901264845
Epoch: 4555, Batch Gradient Norm after: 12.59953901264845
Epoch 4556/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.36817623376846315
Epoch: 4556, Batch Gradient Norm: 10.570243614638057
Epoch: 4556, Batch Gradient Norm after: 10.570243614638057
Epoch 4557/10000, Prediction Accuracy = 62.584%, Loss = 0.3655189275741577
Epoch: 4557, Batch Gradient Norm: 9.38969043126485
Epoch: 4557, Batch Gradient Norm after: 9.38969043126485
Epoch 4558/10000, Prediction Accuracy = 62.734%, Loss = 0.36402319073677064
Epoch: 4558, Batch Gradient Norm: 11.594169806074472
Epoch: 4558, Batch Gradient Norm after: 11.594169806074472
Epoch 4559/10000, Prediction Accuracy = 62.64%, Loss = 0.36478300094604493
Epoch: 4559, Batch Gradient Norm: 14.802693316452885
Epoch: 4559, Batch Gradient Norm after: 14.802693316452885
Epoch 4560/10000, Prediction Accuracy = 62.712%, Loss = 0.37050623297691343
Epoch: 4560, Batch Gradient Norm: 15.790511148361247
Epoch: 4560, Batch Gradient Norm after: 15.790511148361247
Epoch 4561/10000, Prediction Accuracy = 62.602%, Loss = 0.3689665913581848
Epoch: 4561, Batch Gradient Norm: 12.12440363360811
Epoch: 4561, Batch Gradient Norm after: 12.12440363360811
Epoch 4562/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.36658631563186644
Epoch: 4562, Batch Gradient Norm: 10.608349401465107
Epoch: 4562, Batch Gradient Norm after: 10.608349401465107
Epoch 4563/10000, Prediction Accuracy = 62.790000000000006%, Loss = 0.36293327808380127
Epoch: 4563, Batch Gradient Norm: 11.874758876871113
Epoch: 4563, Batch Gradient Norm after: 11.874758876871113
Epoch 4564/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.364250248670578
Epoch: 4564, Batch Gradient Norm: 12.218291086866902
Epoch: 4564, Batch Gradient Norm after: 12.218291086866902
Epoch 4565/10000, Prediction Accuracy = 62.602%, Loss = 0.36675424575805665
Epoch: 4565, Batch Gradient Norm: 12.273217884897704
Epoch: 4565, Batch Gradient Norm after: 12.273217884897704
Epoch 4566/10000, Prediction Accuracy = 62.61%, Loss = 0.36609130501747134
Epoch: 4566, Batch Gradient Norm: 13.376647511957412
Epoch: 4566, Batch Gradient Norm after: 13.376647511957412
Epoch 4567/10000, Prediction Accuracy = 62.602%, Loss = 0.3680748581886292
Epoch: 4567, Batch Gradient Norm: 12.604682292941284
Epoch: 4567, Batch Gradient Norm after: 12.604682292941284
Epoch 4568/10000, Prediction Accuracy = 62.69200000000001%, Loss = 0.3688837945461273
Epoch: 4568, Batch Gradient Norm: 10.476677824205415
Epoch: 4568, Batch Gradient Norm after: 10.476677824205415
Epoch 4569/10000, Prediction Accuracy = 62.6%, Loss = 0.3657748818397522
Epoch: 4569, Batch Gradient Norm: 13.607799373199272
Epoch: 4569, Batch Gradient Norm after: 13.607799373199272
Epoch 4570/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.3703848302364349
Epoch: 4570, Batch Gradient Norm: 17.19980184839882
Epoch: 4570, Batch Gradient Norm after: 17.19980184839882
Epoch 4571/10000, Prediction Accuracy = 62.612%, Loss = 0.37360310554504395
Epoch: 4571, Batch Gradient Norm: 14.99188490107588
Epoch: 4571, Batch Gradient Norm after: 14.99188490107588
Epoch 4572/10000, Prediction Accuracy = 62.666%, Loss = 0.3677804350852966
Epoch: 4572, Batch Gradient Norm: 15.594159918961847
Epoch: 4572, Batch Gradient Norm after: 15.594159918961847
Epoch 4573/10000, Prediction Accuracy = 62.698%, Loss = 0.37195175886154175
Epoch: 4573, Batch Gradient Norm: 15.714010683524343
Epoch: 4573, Batch Gradient Norm after: 15.714010683524343
Epoch 4574/10000, Prediction Accuracy = 62.636%, Loss = 0.36995776295661925
Epoch: 4574, Batch Gradient Norm: 16.499229365803476
Epoch: 4574, Batch Gradient Norm after: 16.499229365803476
Epoch 4575/10000, Prediction Accuracy = 62.698%, Loss = 0.37278791666030886
Epoch: 4575, Batch Gradient Norm: 15.52186901614591
Epoch: 4575, Batch Gradient Norm after: 15.52186901614591
Epoch 4576/10000, Prediction Accuracy = 62.686%, Loss = 0.36951322555541993
Epoch: 4576, Batch Gradient Norm: 14.791105760842234
Epoch: 4576, Batch Gradient Norm after: 14.791105760842234
Epoch 4577/10000, Prediction Accuracy = 62.67199999999999%, Loss = 0.371932715177536
Epoch: 4577, Batch Gradient Norm: 15.135022126934532
Epoch: 4577, Batch Gradient Norm after: 15.135022126934532
Epoch 4578/10000, Prediction Accuracy = 62.64799999999999%, Loss = 0.37106443047523496
Epoch: 4578, Batch Gradient Norm: 12.136259217369492
Epoch: 4578, Batch Gradient Norm after: 12.136259217369492
Epoch 4579/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.3664615035057068
Epoch: 4579, Batch Gradient Norm: 13.467806284413065
Epoch: 4579, Batch Gradient Norm after: 13.467806284413065
Epoch 4580/10000, Prediction Accuracy = 62.71%, Loss = 0.36895239949226377
Epoch: 4580, Batch Gradient Norm: 12.830875380534676
Epoch: 4580, Batch Gradient Norm after: 12.830875380534676
Epoch 4581/10000, Prediction Accuracy = 62.67%, Loss = 0.3690981507301331
Epoch: 4581, Batch Gradient Norm: 11.70177482396501
Epoch: 4581, Batch Gradient Norm after: 11.70177482396501
Epoch 4582/10000, Prediction Accuracy = 62.71200000000001%, Loss = 0.36513314247131345
Epoch: 4582, Batch Gradient Norm: 13.80159007198796
Epoch: 4582, Batch Gradient Norm after: 13.80159007198796
Epoch 4583/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.36745611429214475
Epoch: 4583, Batch Gradient Norm: 11.266362514694805
Epoch: 4583, Batch Gradient Norm after: 11.266362514694805
Epoch 4584/10000, Prediction Accuracy = 62.55%, Loss = 0.3640342831611633
Epoch: 4584, Batch Gradient Norm: 10.624655443780233
Epoch: 4584, Batch Gradient Norm after: 10.624655443780233
Epoch 4585/10000, Prediction Accuracy = 62.652%, Loss = 0.36423616409301757
Epoch: 4585, Batch Gradient Norm: 11.241308843530742
Epoch: 4585, Batch Gradient Norm after: 11.241308843530742
Epoch 4586/10000, Prediction Accuracy = 62.76400000000001%, Loss = 0.36404308676719666
Epoch: 4586, Batch Gradient Norm: 14.173238653792362
Epoch: 4586, Batch Gradient Norm after: 14.173238653792362
Epoch 4587/10000, Prediction Accuracy = 62.604%, Loss = 0.36974565386772157
Epoch: 4587, Batch Gradient Norm: 14.866709798709605
Epoch: 4587, Batch Gradient Norm after: 14.866709798709605
Epoch 4588/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.36900444626808165
Epoch: 4588, Batch Gradient Norm: 12.291815545680144
Epoch: 4588, Batch Gradient Norm after: 12.291815545680144
Epoch 4589/10000, Prediction Accuracy = 62.669999999999995%, Loss = 0.3682979464530945
Epoch: 4589, Batch Gradient Norm: 14.424423790448392
Epoch: 4589, Batch Gradient Norm after: 14.424423790448392
Epoch 4590/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.37055196762084963
Epoch: 4590, Batch Gradient Norm: 15.15805076406512
Epoch: 4590, Batch Gradient Norm after: 15.15805076406512
Epoch 4591/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.37320526242256163
Epoch: 4591, Batch Gradient Norm: 16.609064537687736
Epoch: 4591, Batch Gradient Norm after: 16.609064537687736
Epoch 4592/10000, Prediction Accuracy = 62.712%, Loss = 0.37154741287231446
Epoch: 4592, Batch Gradient Norm: 15.288541058046597
Epoch: 4592, Batch Gradient Norm after: 15.288541058046597
Epoch 4593/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.36944433450698855
Epoch: 4593, Batch Gradient Norm: 15.2901266566528
Epoch: 4593, Batch Gradient Norm after: 15.2901266566528
Epoch 4594/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.36960911750793457
Epoch: 4594, Batch Gradient Norm: 15.20818123053567
Epoch: 4594, Batch Gradient Norm after: 15.20818123053567
Epoch 4595/10000, Prediction Accuracy = 62.56999999999999%, Loss = 0.37093167304992675
Epoch: 4595, Batch Gradient Norm: 10.891640374787746
Epoch: 4595, Batch Gradient Norm after: 10.891640374787746
Epoch 4596/10000, Prediction Accuracy = 62.71%, Loss = 0.36553993821144104
Epoch: 4596, Batch Gradient Norm: 10.612692769559322
Epoch: 4596, Batch Gradient Norm after: 10.612692769559322
Epoch 4597/10000, Prediction Accuracy = 62.69200000000001%, Loss = 0.36518858671188353
Epoch: 4597, Batch Gradient Norm: 10.260735804826588
Epoch: 4597, Batch Gradient Norm after: 10.260735804826588
Epoch 4598/10000, Prediction Accuracy = 62.61999999999999%, Loss = 0.36369024515151976
Epoch: 4598, Batch Gradient Norm: 10.025868502397639
Epoch: 4598, Batch Gradient Norm after: 10.025868502397639
Epoch 4599/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.36497130393981936
Epoch: 4599, Batch Gradient Norm: 10.487557671718136
Epoch: 4599, Batch Gradient Norm after: 10.487557671718136
Epoch 4600/10000, Prediction Accuracy = 62.726%, Loss = 0.3626895010471344
Epoch: 4600, Batch Gradient Norm: 9.512554254388107
Epoch: 4600, Batch Gradient Norm after: 9.512554254388107
Epoch 4601/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.362520432472229
Epoch: 4601, Batch Gradient Norm: 12.677322822349108
Epoch: 4601, Batch Gradient Norm after: 12.677322822349108
Epoch 4602/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3653294682502747
Epoch: 4602, Batch Gradient Norm: 13.825387839234725
Epoch: 4602, Batch Gradient Norm after: 13.825387839234725
Epoch 4603/10000, Prediction Accuracy = 62.788%, Loss = 0.36913002729415895
Epoch: 4603, Batch Gradient Norm: 12.492815454018846
Epoch: 4603, Batch Gradient Norm after: 12.492815454018846
Epoch 4604/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.36492571234703064
Epoch: 4604, Batch Gradient Norm: 13.120377144899464
Epoch: 4604, Batch Gradient Norm after: 13.120377144899464
Epoch 4605/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.36742827892303465
Epoch: 4605, Batch Gradient Norm: 11.551344694115294
Epoch: 4605, Batch Gradient Norm after: 11.551344694115294
Epoch 4606/10000, Prediction Accuracy = 62.790000000000006%, Loss = 0.3634097516536713
Epoch: 4606, Batch Gradient Norm: 13.274092770284478
Epoch: 4606, Batch Gradient Norm after: 13.274092770284478
Epoch 4607/10000, Prediction Accuracy = 62.7%, Loss = 0.36456077694892886
Epoch: 4607, Batch Gradient Norm: 10.001141128690222
Epoch: 4607, Batch Gradient Norm after: 10.001141128690222
Epoch 4608/10000, Prediction Accuracy = 62.75%, Loss = 0.3646697461605072
Epoch: 4608, Batch Gradient Norm: 9.79777127940451
Epoch: 4608, Batch Gradient Norm after: 9.79777127940451
Epoch 4609/10000, Prediction Accuracy = 62.734%, Loss = 0.3622822523117065
Epoch: 4609, Batch Gradient Norm: 9.842543547897723
Epoch: 4609, Batch Gradient Norm after: 9.842543547897723
Epoch 4610/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.36158434152603147
Epoch: 4610, Batch Gradient Norm: 11.473653296346452
Epoch: 4610, Batch Gradient Norm after: 11.473653296346452
Epoch 4611/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.36709612011909487
Epoch: 4611, Batch Gradient Norm: 11.875680082595892
Epoch: 4611, Batch Gradient Norm after: 11.875680082595892
Epoch 4612/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3644793689250946
Epoch: 4612, Batch Gradient Norm: 12.775210842630013
Epoch: 4612, Batch Gradient Norm after: 12.775210842630013
Epoch 4613/10000, Prediction Accuracy = 62.6%, Loss = 0.3657481610774994
Epoch: 4613, Batch Gradient Norm: 11.695232769458743
Epoch: 4613, Batch Gradient Norm after: 11.695232769458743
Epoch 4614/10000, Prediction Accuracy = 62.602%, Loss = 0.3632152020931244
Epoch: 4614, Batch Gradient Norm: 13.077182685722303
Epoch: 4614, Batch Gradient Norm after: 13.077182685722303
Epoch 4615/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.3671365439891815
Epoch: 4615, Batch Gradient Norm: 12.972147095766333
Epoch: 4615, Batch Gradient Norm after: 12.972147095766333
Epoch 4616/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.36615070700645447
Epoch: 4616, Batch Gradient Norm: 12.545772499295733
Epoch: 4616, Batch Gradient Norm after: 12.545772499295733
Epoch 4617/10000, Prediction Accuracy = 62.722%, Loss = 0.36453838348388673
Epoch: 4617, Batch Gradient Norm: 14.281182577958237
Epoch: 4617, Batch Gradient Norm after: 14.281182577958237
Epoch 4618/10000, Prediction Accuracy = 62.722%, Loss = 0.3673014760017395
Epoch: 4618, Batch Gradient Norm: 15.343762221504342
Epoch: 4618, Batch Gradient Norm after: 15.343762221504342
Epoch 4619/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.3722184062004089
Epoch: 4619, Batch Gradient Norm: 16.37978384560366
Epoch: 4619, Batch Gradient Norm after: 16.37978384560366
Epoch 4620/10000, Prediction Accuracy = 62.758%, Loss = 0.3717349827289581
Epoch: 4620, Batch Gradient Norm: 17.0575829924532
Epoch: 4620, Batch Gradient Norm after: 17.0575829924532
Epoch 4621/10000, Prediction Accuracy = 62.774%, Loss = 0.3714005708694458
Epoch: 4621, Batch Gradient Norm: 16.062696691023213
Epoch: 4621, Batch Gradient Norm after: 16.062696691023213
Epoch 4622/10000, Prediction Accuracy = 62.528%, Loss = 0.37146441340446473
Epoch: 4622, Batch Gradient Norm: 12.694979343262972
Epoch: 4622, Batch Gradient Norm after: 12.694979343262972
Epoch 4623/10000, Prediction Accuracy = 62.69%, Loss = 0.3677766501903534
Epoch: 4623, Batch Gradient Norm: 12.780084660558416
Epoch: 4623, Batch Gradient Norm after: 12.780084660558416
Epoch 4624/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.3649004638195038
Epoch: 4624, Batch Gradient Norm: 12.743690820139097
Epoch: 4624, Batch Gradient Norm after: 12.743690820139097
Epoch 4625/10000, Prediction Accuracy = 62.69%, Loss = 0.3661673843860626
Epoch: 4625, Batch Gradient Norm: 12.01609803116619
Epoch: 4625, Batch Gradient Norm after: 12.01609803116619
Epoch 4626/10000, Prediction Accuracy = 62.645999999999994%, Loss = 0.36804911494255066
Epoch: 4626, Batch Gradient Norm: 12.34230507971297
Epoch: 4626, Batch Gradient Norm after: 12.34230507971297
Epoch 4627/10000, Prediction Accuracy = 62.78599999999999%, Loss = 0.3674734890460968
Epoch: 4627, Batch Gradient Norm: 10.715064438366506
Epoch: 4627, Batch Gradient Norm after: 10.715064438366506
Epoch 4628/10000, Prediction Accuracy = 62.65%, Loss = 0.3629241526126862
Epoch: 4628, Batch Gradient Norm: 11.043933136381739
Epoch: 4628, Batch Gradient Norm after: 11.043933136381739
Epoch 4629/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.36302504539489744
Epoch: 4629, Batch Gradient Norm: 11.69561397773865
Epoch: 4629, Batch Gradient Norm after: 11.69561397773865
Epoch 4630/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.36449801325798037
Epoch: 4630, Batch Gradient Norm: 13.049256960134311
Epoch: 4630, Batch Gradient Norm after: 13.049256960134311
Epoch 4631/10000, Prediction Accuracy = 62.694%, Loss = 0.3674450218677521
Epoch: 4631, Batch Gradient Norm: 17.529316834325588
Epoch: 4631, Batch Gradient Norm after: 17.529316834325588
Epoch 4632/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.37006407380104067
Epoch: 4632, Batch Gradient Norm: 18.26098950576809
Epoch: 4632, Batch Gradient Norm after: 18.26098950576809
Epoch 4633/10000, Prediction Accuracy = 62.733999999999995%, Loss = 0.37461864948272705
Epoch: 4633, Batch Gradient Norm: 15.276163968480798
Epoch: 4633, Batch Gradient Norm after: 15.276163968480798
Epoch 4634/10000, Prediction Accuracy = 62.73%, Loss = 0.36962292790412904
Epoch: 4634, Batch Gradient Norm: 16.64329082772576
Epoch: 4634, Batch Gradient Norm after: 16.64329082772576
Epoch 4635/10000, Prediction Accuracy = 62.616%, Loss = 0.37147149443626404
Epoch: 4635, Batch Gradient Norm: 14.400217187497127
Epoch: 4635, Batch Gradient Norm after: 14.400217187497127
Epoch 4636/10000, Prediction Accuracy = 62.694%, Loss = 0.36865065693855287
Epoch: 4636, Batch Gradient Norm: 13.589481362729481
Epoch: 4636, Batch Gradient Norm after: 13.589481362729481
Epoch 4637/10000, Prediction Accuracy = 62.60200000000001%, Loss = 0.36760656237602235
Epoch: 4637, Batch Gradient Norm: 15.20957724755716
Epoch: 4637, Batch Gradient Norm after: 15.20957724755716
Epoch 4638/10000, Prediction Accuracy = 62.712%, Loss = 0.36952539086341857
Epoch: 4638, Batch Gradient Norm: 14.174448824604184
Epoch: 4638, Batch Gradient Norm after: 14.174448824604184
Epoch 4639/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.36996703743934634
Epoch: 4639, Batch Gradient Norm: 13.22627542095005
Epoch: 4639, Batch Gradient Norm after: 13.22627542095005
Epoch 4640/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.36587126851081847
Epoch: 4640, Batch Gradient Norm: 13.881770071178702
Epoch: 4640, Batch Gradient Norm after: 13.881770071178702
Epoch 4641/10000, Prediction Accuracy = 62.664%, Loss = 0.3669844150543213
Epoch: 4641, Batch Gradient Norm: 12.596600821795866
Epoch: 4641, Batch Gradient Norm after: 12.596600821795866
Epoch 4642/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.3658178269863129
Epoch: 4642, Batch Gradient Norm: 13.64233843451667
Epoch: 4642, Batch Gradient Norm after: 13.64233843451667
Epoch 4643/10000, Prediction Accuracy = 62.722%, Loss = 0.36918293833732607
Epoch: 4643, Batch Gradient Norm: 11.893893079828171
Epoch: 4643, Batch Gradient Norm after: 11.893893079828171
Epoch 4644/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.3637120723724365
Epoch: 4644, Batch Gradient Norm: 15.39750558755653
Epoch: 4644, Batch Gradient Norm after: 15.39750558755653
Epoch 4645/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3704188346862793
Epoch: 4645, Batch Gradient Norm: 18.335263323012967
Epoch: 4645, Batch Gradient Norm after: 18.335263323012967
Epoch 4646/10000, Prediction Accuracy = 62.712%, Loss = 0.37490314841270445
Epoch: 4646, Batch Gradient Norm: 16.44658497657658
Epoch: 4646, Batch Gradient Norm after: 16.44658497657658
Epoch 4647/10000, Prediction Accuracy = 62.622%, Loss = 0.37317338585853577
Epoch: 4647, Batch Gradient Norm: 15.756852271303552
Epoch: 4647, Batch Gradient Norm after: 15.756852271303552
Epoch 4648/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.3677078425884247
Epoch: 4648, Batch Gradient Norm: 15.85026041372691
Epoch: 4648, Batch Gradient Norm after: 15.85026041372691
Epoch 4649/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.36743208169937136
Epoch: 4649, Batch Gradient Norm: 17.144362184795995
Epoch: 4649, Batch Gradient Norm after: 17.144362184795995
Epoch 4650/10000, Prediction Accuracy = 62.67%, Loss = 0.3718310475349426
Epoch: 4650, Batch Gradient Norm: 18.23179896615182
Epoch: 4650, Batch Gradient Norm after: 18.23179896615182
Epoch 4651/10000, Prediction Accuracy = 62.702%, Loss = 0.372121250629425
Epoch: 4651, Batch Gradient Norm: 17.67399094829167
Epoch: 4651, Batch Gradient Norm after: 17.67074194075791
Epoch 4652/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.37361607551574705
Epoch: 4652, Batch Gradient Norm: 16.676651089468113
Epoch: 4652, Batch Gradient Norm after: 16.526393669062173
Epoch 4653/10000, Prediction Accuracy = 62.790000000000006%, Loss = 0.36931646466255186
Epoch: 4653, Batch Gradient Norm: 14.691872800834656
Epoch: 4653, Batch Gradient Norm after: 14.691872800834656
Epoch 4654/10000, Prediction Accuracy = 62.688%, Loss = 0.3669623970985413
Epoch: 4654, Batch Gradient Norm: 14.638625670705153
Epoch: 4654, Batch Gradient Norm after: 14.638625670705153
Epoch 4655/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.36624190807342527
Epoch: 4655, Batch Gradient Norm: 13.342184483836178
Epoch: 4655, Batch Gradient Norm after: 13.342184483836178
Epoch 4656/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.3645784020423889
Epoch: 4656, Batch Gradient Norm: 11.504200359679341
Epoch: 4656, Batch Gradient Norm after: 11.504200359679341
Epoch 4657/10000, Prediction Accuracy = 62.730000000000004%, Loss = 0.3655106067657471
Epoch: 4657, Batch Gradient Norm: 10.694061087426329
Epoch: 4657, Batch Gradient Norm after: 10.694061087426329
Epoch 4658/10000, Prediction Accuracy = 62.826%, Loss = 0.36517258286476134
Epoch: 4658, Batch Gradient Norm: 14.13406235884929
Epoch: 4658, Batch Gradient Norm after: 14.13406235884929
Epoch 4659/10000, Prediction Accuracy = 62.714%, Loss = 0.3672490894794464
Epoch: 4659, Batch Gradient Norm: 15.610717252216284
Epoch: 4659, Batch Gradient Norm after: 15.610717252216284
Epoch 4660/10000, Prediction Accuracy = 62.712%, Loss = 0.37038999795913696
Epoch: 4660, Batch Gradient Norm: 14.173402465638334
Epoch: 4660, Batch Gradient Norm after: 14.173402465638334
Epoch 4661/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.3657275378704071
Epoch: 4661, Batch Gradient Norm: 15.07231710927246
Epoch: 4661, Batch Gradient Norm after: 15.07231710927246
Epoch 4662/10000, Prediction Accuracy = 62.754%, Loss = 0.36715697050094603
Epoch: 4662, Batch Gradient Norm: 15.984392692273856
Epoch: 4662, Batch Gradient Norm after: 15.34036700141922
Epoch 4663/10000, Prediction Accuracy = 62.698%, Loss = 0.3688278436660767
Epoch: 4663, Batch Gradient Norm: 14.08679224641615
Epoch: 4663, Batch Gradient Norm after: 14.08679224641615
Epoch 4664/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3673112511634827
Epoch: 4664, Batch Gradient Norm: 15.29149549555536
Epoch: 4664, Batch Gradient Norm after: 15.29149549555536
Epoch 4665/10000, Prediction Accuracy = 62.676%, Loss = 0.3689479172229767
Epoch: 4665, Batch Gradient Norm: 14.501567416550495
Epoch: 4665, Batch Gradient Norm after: 14.501567416550495
Epoch 4666/10000, Prediction Accuracy = 62.648%, Loss = 0.3658761322498322
Epoch: 4666, Batch Gradient Norm: 13.605319061437902
Epoch: 4666, Batch Gradient Norm after: 13.605319061437902
Epoch 4667/10000, Prediction Accuracy = 62.64%, Loss = 0.3666697859764099
Epoch: 4667, Batch Gradient Norm: 15.939548759537281
Epoch: 4667, Batch Gradient Norm after: 15.939548759537281
Epoch 4668/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.3681550920009613
Epoch: 4668, Batch Gradient Norm: 13.595962147480636
Epoch: 4668, Batch Gradient Norm after: 13.595962147480636
Epoch 4669/10000, Prediction Accuracy = 62.75600000000001%, Loss = 0.36399929523468016
Epoch: 4669, Batch Gradient Norm: 17.263705672720636
Epoch: 4669, Batch Gradient Norm after: 17.219442891422425
Epoch 4670/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.3721589267253876
Epoch: 4670, Batch Gradient Norm: 16.276571543752496
Epoch: 4670, Batch Gradient Norm after: 16.276571543752496
Epoch 4671/10000, Prediction Accuracy = 62.714%, Loss = 0.3722299158573151
Epoch: 4671, Batch Gradient Norm: 17.46806201425709
Epoch: 4671, Batch Gradient Norm after: 17.46806201425709
Epoch 4672/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.37087265849113465
Epoch: 4672, Batch Gradient Norm: 12.268796020749605
Epoch: 4672, Batch Gradient Norm after: 12.268796020749605
Epoch 4673/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.36229653358459474
Epoch: 4673, Batch Gradient Norm: 10.9619295638154
Epoch: 4673, Batch Gradient Norm after: 10.9619295638154
Epoch 4674/10000, Prediction Accuracy = 62.754%, Loss = 0.3618858814239502
Epoch: 4674, Batch Gradient Norm: 12.957825934117878
Epoch: 4674, Batch Gradient Norm after: 12.957825934117878
Epoch 4675/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.3639631688594818
Epoch: 4675, Batch Gradient Norm: 12.550245490163544
Epoch: 4675, Batch Gradient Norm after: 12.550245490163544
Epoch 4676/10000, Prediction Accuracy = 62.712%, Loss = 0.36615540981292727
Epoch: 4676, Batch Gradient Norm: 12.051888749496976
Epoch: 4676, Batch Gradient Norm after: 12.051888749496976
Epoch 4677/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.36403220891952515
Epoch: 4677, Batch Gradient Norm: 11.809498500041842
Epoch: 4677, Batch Gradient Norm after: 11.809498500041842
Epoch 4678/10000, Prediction Accuracy = 62.69000000000001%, Loss = 0.36226590275764464
Epoch: 4678, Batch Gradient Norm: 12.946009397437196
Epoch: 4678, Batch Gradient Norm after: 12.946009397437196
Epoch 4679/10000, Prediction Accuracy = 62.524%, Loss = 0.3645288825035095
Epoch: 4679, Batch Gradient Norm: 13.588785171731024
Epoch: 4679, Batch Gradient Norm after: 13.588785171731024
Epoch 4680/10000, Prediction Accuracy = 62.782%, Loss = 0.3638369500637054
Epoch: 4680, Batch Gradient Norm: 14.833523982664508
Epoch: 4680, Batch Gradient Norm after: 14.833523982664508
Epoch 4681/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.36745615005493165
Epoch: 4681, Batch Gradient Norm: 16.20159820546823
Epoch: 4681, Batch Gradient Norm after: 16.20159820546823
Epoch 4682/10000, Prediction Accuracy = 62.814%, Loss = 0.37170419096946716
Epoch: 4682, Batch Gradient Norm: 13.83357380569554
Epoch: 4682, Batch Gradient Norm after: 13.83357380569554
Epoch 4683/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.3639743208885193
Epoch: 4683, Batch Gradient Norm: 12.803113456322656
Epoch: 4683, Batch Gradient Norm after: 12.803113456322656
Epoch 4684/10000, Prediction Accuracy = 62.702%, Loss = 0.3636363983154297
Epoch: 4684, Batch Gradient Norm: 11.700520013566296
Epoch: 4684, Batch Gradient Norm after: 11.700520013566296
Epoch 4685/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.3619258880615234
Epoch: 4685, Batch Gradient Norm: 11.89670299631348
Epoch: 4685, Batch Gradient Norm after: 11.89670299631348
Epoch 4686/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.36426650285720824
Epoch: 4686, Batch Gradient Norm: 12.114050847434726
Epoch: 4686, Batch Gradient Norm after: 12.114050847434726
Epoch 4687/10000, Prediction Accuracy = 62.688%, Loss = 0.36377577781677245
Epoch: 4687, Batch Gradient Norm: 11.68705332840905
Epoch: 4687, Batch Gradient Norm after: 11.68705332840905
Epoch 4688/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.36262967586517336
Epoch: 4688, Batch Gradient Norm: 13.561743378086069
Epoch: 4688, Batch Gradient Norm after: 13.561743378086069
Epoch 4689/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3644287407398224
Epoch: 4689, Batch Gradient Norm: 15.936840986661045
Epoch: 4689, Batch Gradient Norm after: 15.936840986661045
Epoch 4690/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.3683642566204071
Epoch: 4690, Batch Gradient Norm: 17.621759408777447
Epoch: 4690, Batch Gradient Norm after: 17.024621053627648
Epoch 4691/10000, Prediction Accuracy = 62.762%, Loss = 0.3715278685092926
Epoch: 4691, Batch Gradient Norm: 13.821172659820025
Epoch: 4691, Batch Gradient Norm after: 13.821172659820025
Epoch 4692/10000, Prediction Accuracy = 62.722%, Loss = 0.36638453006744387
Epoch: 4692, Batch Gradient Norm: 14.775996395722991
Epoch: 4692, Batch Gradient Norm after: 14.775996395722991
Epoch 4693/10000, Prediction Accuracy = 62.662%, Loss = 0.3660445511341095
Epoch: 4693, Batch Gradient Norm: 13.743491499284742
Epoch: 4693, Batch Gradient Norm after: 13.743491499284742
Epoch 4694/10000, Prediction Accuracy = 62.722%, Loss = 0.366420978307724
Epoch: 4694, Batch Gradient Norm: 13.814899197252005
Epoch: 4694, Batch Gradient Norm after: 13.814899197252005
Epoch 4695/10000, Prediction Accuracy = 62.782%, Loss = 0.365312659740448
Epoch: 4695, Batch Gradient Norm: 14.259488569474227
Epoch: 4695, Batch Gradient Norm after: 14.259488569474227
Epoch 4696/10000, Prediction Accuracy = 62.646%, Loss = 0.37157506346702573
Epoch: 4696, Batch Gradient Norm: 14.845721274093801
Epoch: 4696, Batch Gradient Norm after: 14.845721274093801
Epoch 4697/10000, Prediction Accuracy = 62.826%, Loss = 0.36603994369506837
Epoch: 4697, Batch Gradient Norm: 12.15471176115646
Epoch: 4697, Batch Gradient Norm after: 12.15471176115646
Epoch 4698/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.3656231760978699
Epoch: 4698, Batch Gradient Norm: 12.499171898726052
Epoch: 4698, Batch Gradient Norm after: 12.499171898726052
Epoch 4699/10000, Prediction Accuracy = 62.818%, Loss = 0.36478169560432433
Epoch: 4699, Batch Gradient Norm: 16.321036915684243
Epoch: 4699, Batch Gradient Norm after: 16.321036915684243
Epoch 4700/10000, Prediction Accuracy = 62.706%, Loss = 0.3683011829853058
Epoch: 4700, Batch Gradient Norm: 16.829944755259927
Epoch: 4700, Batch Gradient Norm after: 16.829944755259927
Epoch 4701/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.36991732716560366
Epoch: 4701, Batch Gradient Norm: 13.422865161057974
Epoch: 4701, Batch Gradient Norm after: 13.422865161057974
Epoch 4702/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.3657942354679108
Epoch: 4702, Batch Gradient Norm: 13.274501832031099
Epoch: 4702, Batch Gradient Norm after: 13.274501832031099
Epoch 4703/10000, Prediction Accuracy = 62.638%, Loss = 0.3651063144207001
Epoch: 4703, Batch Gradient Norm: 13.31987791497529
Epoch: 4703, Batch Gradient Norm after: 13.31987791497529
Epoch 4704/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.3651165783405304
Epoch: 4704, Batch Gradient Norm: 18.991039605136752
Epoch: 4704, Batch Gradient Norm after: 18.44512470041545
Epoch 4705/10000, Prediction Accuracy = 62.694%, Loss = 0.3766120493412018
Epoch: 4705, Batch Gradient Norm: 19.38565730595223
Epoch: 4705, Batch Gradient Norm after: 18.21446917511737
Epoch 4706/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.37448769211769106
Epoch: 4706, Batch Gradient Norm: 15.903602780967223
Epoch: 4706, Batch Gradient Norm after: 15.903602780967223
Epoch 4707/10000, Prediction Accuracy = 62.7%, Loss = 0.3663861572742462
Epoch: 4707, Batch Gradient Norm: 12.760826180387458
Epoch: 4707, Batch Gradient Norm after: 12.760826180387458
Epoch 4708/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.361430698633194
Epoch: 4708, Batch Gradient Norm: 11.719610422930879
Epoch: 4708, Batch Gradient Norm after: 11.719610422930879
Epoch 4709/10000, Prediction Accuracy = 62.693999999999996%, Loss = 0.3602282702922821
Epoch: 4709, Batch Gradient Norm: 12.622878354902305
Epoch: 4709, Batch Gradient Norm after: 12.622878354902305
Epoch 4710/10000, Prediction Accuracy = 62.714%, Loss = 0.36411954164505006
Epoch: 4710, Batch Gradient Norm: 12.58580166797722
Epoch: 4710, Batch Gradient Norm after: 12.58580166797722
Epoch 4711/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.36133278012275694
Epoch: 4711, Batch Gradient Norm: 11.00408366823839
Epoch: 4711, Batch Gradient Norm after: 11.00408366823839
Epoch 4712/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.36047911643981934
Epoch: 4712, Batch Gradient Norm: 11.989603541855159
Epoch: 4712, Batch Gradient Norm after: 11.989603541855159
Epoch 4713/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.3624653398990631
Epoch: 4713, Batch Gradient Norm: 12.061752211554122
Epoch: 4713, Batch Gradient Norm after: 12.061752211554122
Epoch 4714/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.3637200057506561
Epoch: 4714, Batch Gradient Norm: 13.994005921349567
Epoch: 4714, Batch Gradient Norm after: 13.994005921349567
Epoch 4715/10000, Prediction Accuracy = 62.718%, Loss = 0.36399867534637453
Epoch: 4715, Batch Gradient Norm: 15.030930415310813
Epoch: 4715, Batch Gradient Norm after: 15.030930415310813
Epoch 4716/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.3670643150806427
Epoch: 4716, Batch Gradient Norm: 17.15365612981377
Epoch: 4716, Batch Gradient Norm after: 17.15365612981377
Epoch 4717/10000, Prediction Accuracy = 62.694%, Loss = 0.36884449124336244
Epoch: 4717, Batch Gradient Norm: 14.579061405696068
Epoch: 4717, Batch Gradient Norm after: 14.579061405696068
Epoch 4718/10000, Prediction Accuracy = 62.746%, Loss = 0.3655813992023468
Epoch: 4718, Batch Gradient Norm: 15.024275883433578
Epoch: 4718, Batch Gradient Norm after: 15.024275883433578
Epoch 4719/10000, Prediction Accuracy = 62.7%, Loss = 0.36696159839630127
Epoch: 4719, Batch Gradient Norm: 13.697607722766119
Epoch: 4719, Batch Gradient Norm after: 13.697607722766119
Epoch 4720/10000, Prediction Accuracy = 62.774%, Loss = 0.36675548553466797
Epoch: 4720, Batch Gradient Norm: 13.900499638369336
Epoch: 4720, Batch Gradient Norm after: 13.900499638369336
Epoch 4721/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.3666099190711975
Epoch: 4721, Batch Gradient Norm: 15.886972425827425
Epoch: 4721, Batch Gradient Norm after: 15.886972425827425
Epoch 4722/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.36780930161476133
Epoch: 4722, Batch Gradient Norm: 16.276709302218098
Epoch: 4722, Batch Gradient Norm after: 16.276709302218098
Epoch 4723/10000, Prediction Accuracy = 62.662%, Loss = 0.37014655470848085
Epoch: 4723, Batch Gradient Norm: 15.901116919306743
Epoch: 4723, Batch Gradient Norm after: 15.901116919306743
Epoch 4724/10000, Prediction Accuracy = 62.786%, Loss = 0.36751521825790406
Epoch: 4724, Batch Gradient Norm: 14.651948822335838
Epoch: 4724, Batch Gradient Norm after: 14.651948822335838
Epoch 4725/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3658905982971191
Epoch: 4725, Batch Gradient Norm: 13.05866744409986
Epoch: 4725, Batch Gradient Norm after: 13.05866744409986
Epoch 4726/10000, Prediction Accuracy = 62.686%, Loss = 0.3619937300682068
Epoch: 4726, Batch Gradient Norm: 11.79308556460509
Epoch: 4726, Batch Gradient Norm after: 11.79308556460509
Epoch 4727/10000, Prediction Accuracy = 62.682%, Loss = 0.36177493929862975
Epoch: 4727, Batch Gradient Norm: 12.853689897804045
Epoch: 4727, Batch Gradient Norm after: 12.853689897804045
Epoch 4728/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.36053343415260314
Epoch: 4728, Batch Gradient Norm: 14.540805296940459
Epoch: 4728, Batch Gradient Norm after: 14.540805296940459
Epoch 4729/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.36360296607017517
Epoch: 4729, Batch Gradient Norm: 12.048993423784305
Epoch: 4729, Batch Gradient Norm after: 12.048993423784305
Epoch 4730/10000, Prediction Accuracy = 62.7%, Loss = 0.36001019477844237
Epoch: 4730, Batch Gradient Norm: 13.080391959584814
Epoch: 4730, Batch Gradient Norm after: 13.080391959584814
Epoch 4731/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3630404531955719
Epoch: 4731, Batch Gradient Norm: 11.259687060421669
Epoch: 4731, Batch Gradient Norm after: 11.259687060421669
Epoch 4732/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3618306338787079
Epoch: 4732, Batch Gradient Norm: 13.525514696491548
Epoch: 4732, Batch Gradient Norm after: 13.525514696491548
Epoch 4733/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.36247738003730773
Epoch: 4733, Batch Gradient Norm: 12.277537586886755
Epoch: 4733, Batch Gradient Norm after: 12.277537586886755
Epoch 4734/10000, Prediction Accuracy = 62.746%, Loss = 0.36312450766563414
Epoch: 4734, Batch Gradient Norm: 11.374150234112081
Epoch: 4734, Batch Gradient Norm after: 11.374150234112081
Epoch 4735/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3602423191070557
Epoch: 4735, Batch Gradient Norm: 9.879309610328882
Epoch: 4735, Batch Gradient Norm after: 9.879309610328882
Epoch 4736/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.3589782118797302
Epoch: 4736, Batch Gradient Norm: 12.127346178930761
Epoch: 4736, Batch Gradient Norm after: 12.127346178930761
Epoch 4737/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.3611066281795502
Epoch: 4737, Batch Gradient Norm: 11.530307779231414
Epoch: 4737, Batch Gradient Norm after: 11.530307779231414
Epoch 4738/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.36270856857299805
Epoch: 4738, Batch Gradient Norm: 10.016876731427038
Epoch: 4738, Batch Gradient Norm after: 10.016876731427038
Epoch 4739/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.36072016358375547
Epoch: 4739, Batch Gradient Norm: 11.170411884974227
Epoch: 4739, Batch Gradient Norm after: 11.170411884974227
Epoch 4740/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.3596278548240662
Epoch: 4740, Batch Gradient Norm: 8.740118749336245
Epoch: 4740, Batch Gradient Norm after: 8.740118749336245
Epoch 4741/10000, Prediction Accuracy = 62.79200000000001%, Loss = 0.3563356876373291
Epoch: 4741, Batch Gradient Norm: 11.965805290209891
Epoch: 4741, Batch Gradient Norm after: 11.965805290209891
Epoch 4742/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.36080902218818667
Epoch: 4742, Batch Gradient Norm: 12.349629045780285
Epoch: 4742, Batch Gradient Norm after: 12.349629045780285
Epoch 4743/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.3639702022075653
Epoch: 4743, Batch Gradient Norm: 15.715513602143455
Epoch: 4743, Batch Gradient Norm after: 15.715513602143455
Epoch 4744/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3699768304824829
Epoch: 4744, Batch Gradient Norm: 16.246946526559615
Epoch: 4744, Batch Gradient Norm after: 16.246946526559615
Epoch 4745/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.37054657340049746
Epoch: 4745, Batch Gradient Norm: 18.543282262784214
Epoch: 4745, Batch Gradient Norm after: 18.543282262784214
Epoch 4746/10000, Prediction Accuracy = 62.79%, Loss = 0.3735152781009674
Epoch: 4746, Batch Gradient Norm: 18.010634888850344
Epoch: 4746, Batch Gradient Norm after: 17.44884547679093
Epoch 4747/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.37337595224380493
Epoch: 4747, Batch Gradient Norm: 17.31631298596713
Epoch: 4747, Batch Gradient Norm after: 17.31631298596713
Epoch 4748/10000, Prediction Accuracy = 62.702%, Loss = 0.37028759717941284
Epoch: 4748, Batch Gradient Norm: 15.148567739766245
Epoch: 4748, Batch Gradient Norm after: 15.148567739766245
Epoch 4749/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.3661410689353943
Epoch: 4749, Batch Gradient Norm: 14.956115234945429
Epoch: 4749, Batch Gradient Norm after: 14.956115234945429
Epoch 4750/10000, Prediction Accuracy = 62.698%, Loss = 0.36583017706871035
Epoch: 4750, Batch Gradient Norm: 13.613994054804376
Epoch: 4750, Batch Gradient Norm after: 13.613994054804376
Epoch 4751/10000, Prediction Accuracy = 62.824%, Loss = 0.36322547793388366
Epoch: 4751, Batch Gradient Norm: 14.548588189270198
Epoch: 4751, Batch Gradient Norm after: 14.548588189270198
Epoch 4752/10000, Prediction Accuracy = 62.730000000000004%, Loss = 0.3647447466850281
Epoch: 4752, Batch Gradient Norm: 13.70703769942503
Epoch: 4752, Batch Gradient Norm after: 13.70703769942503
Epoch 4753/10000, Prediction Accuracy = 62.702%, Loss = 0.3662781655788422
Epoch: 4753, Batch Gradient Norm: 13.375934368305149
Epoch: 4753, Batch Gradient Norm after: 13.375934368305149
Epoch 4754/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3624097228050232
Epoch: 4754, Batch Gradient Norm: 10.727645986783944
Epoch: 4754, Batch Gradient Norm after: 10.727645986783944
Epoch 4755/10000, Prediction Accuracy = 62.7%, Loss = 0.3580478370189667
Epoch: 4755, Batch Gradient Norm: 10.830342589506508
Epoch: 4755, Batch Gradient Norm after: 10.830342589506508
Epoch 4756/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.3614558815956116
Epoch: 4756, Batch Gradient Norm: 10.931000109694184
Epoch: 4756, Batch Gradient Norm after: 10.931000109694184
Epoch 4757/10000, Prediction Accuracy = 62.75600000000001%, Loss = 0.36211487650871277
Epoch: 4757, Batch Gradient Norm: 12.292155476779566
Epoch: 4757, Batch Gradient Norm after: 12.292155476779566
Epoch 4758/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.36133984923362733
Epoch: 4758, Batch Gradient Norm: 13.075628711976746
Epoch: 4758, Batch Gradient Norm after: 13.075628711976746
Epoch 4759/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.36320366263389586
Epoch: 4759, Batch Gradient Norm: 14.688172951180537
Epoch: 4759, Batch Gradient Norm after: 14.688172951180537
Epoch 4760/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.36413944363594053
Epoch: 4760, Batch Gradient Norm: 13.674661145180453
Epoch: 4760, Batch Gradient Norm after: 13.674661145180453
Epoch 4761/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.3627818703651428
Epoch: 4761, Batch Gradient Norm: 12.356591164038422
Epoch: 4761, Batch Gradient Norm after: 12.356591164038422
Epoch 4762/10000, Prediction Accuracy = 62.754%, Loss = 0.3632771670818329
Epoch: 4762, Batch Gradient Norm: 13.329009777201927
Epoch: 4762, Batch Gradient Norm after: 13.329009777201927
Epoch 4763/10000, Prediction Accuracy = 62.89%, Loss = 0.36203954815864564
Epoch: 4763, Batch Gradient Norm: 13.60601629192809
Epoch: 4763, Batch Gradient Norm after: 13.60601629192809
Epoch 4764/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.36239861845970156
Epoch: 4764, Batch Gradient Norm: 14.813092317190632
Epoch: 4764, Batch Gradient Norm after: 14.813092317190632
Epoch 4765/10000, Prediction Accuracy = 62.786%, Loss = 0.3629405379295349
Epoch: 4765, Batch Gradient Norm: 13.85714987069441
Epoch: 4765, Batch Gradient Norm after: 13.85714987069441
Epoch 4766/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.3621838390827179
Epoch: 4766, Batch Gradient Norm: 12.777273496815983
Epoch: 4766, Batch Gradient Norm after: 12.777273496815983
Epoch 4767/10000, Prediction Accuracy = 62.75999999999999%, Loss = 0.3623753249645233
Epoch: 4767, Batch Gradient Norm: 11.245330095584377
Epoch: 4767, Batch Gradient Norm after: 11.245330095584377
Epoch 4768/10000, Prediction Accuracy = 62.79%, Loss = 0.36039140820503235
Epoch: 4768, Batch Gradient Norm: 12.061601190665206
Epoch: 4768, Batch Gradient Norm after: 12.061601190665206
Epoch 4769/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.3603470265865326
Epoch: 4769, Batch Gradient Norm: 10.877504875440078
Epoch: 4769, Batch Gradient Norm after: 10.877504875440078
Epoch 4770/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.35910109281539915
Epoch: 4770, Batch Gradient Norm: 12.209881913502764
Epoch: 4770, Batch Gradient Norm after: 12.209881913502764
Epoch 4771/10000, Prediction Accuracy = 62.834%, Loss = 0.36168078184127805
Epoch: 4771, Batch Gradient Norm: 13.193355114775091
Epoch: 4771, Batch Gradient Norm after: 13.193355114775091
Epoch 4772/10000, Prediction Accuracy = 62.718%, Loss = 0.36350674033164976
Epoch: 4772, Batch Gradient Norm: 11.977416039736493
Epoch: 4772, Batch Gradient Norm after: 11.977416039736493
Epoch 4773/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.36130803227424624
Epoch: 4773, Batch Gradient Norm: 11.771767974557488
Epoch: 4773, Batch Gradient Norm after: 11.771767974557488
Epoch 4774/10000, Prediction Accuracy = 62.664%, Loss = 0.3616960346698761
Epoch: 4774, Batch Gradient Norm: 11.752835187973435
Epoch: 4774, Batch Gradient Norm after: 11.752835187973435
Epoch 4775/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.35919840931892394
Epoch: 4775, Batch Gradient Norm: 10.43407483859816
Epoch: 4775, Batch Gradient Norm after: 10.43407483859816
Epoch 4776/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.3593928039073944
Epoch: 4776, Batch Gradient Norm: 12.902223565758833
Epoch: 4776, Batch Gradient Norm after: 12.902223565758833
Epoch 4777/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.3617887616157532
Epoch: 4777, Batch Gradient Norm: 11.940460958845938
Epoch: 4777, Batch Gradient Norm after: 11.940460958845938
Epoch 4778/10000, Prediction Accuracy = 62.88799999999999%, Loss = 0.3610540390014648
Epoch: 4778, Batch Gradient Norm: 12.384206075352106
Epoch: 4778, Batch Gradient Norm after: 12.384206075352106
Epoch 4779/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.3622786164283752
Epoch: 4779, Batch Gradient Norm: 15.01749106037331
Epoch: 4779, Batch Gradient Norm after: 15.01749106037331
Epoch 4780/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.36793155074119566
Epoch: 4780, Batch Gradient Norm: 15.637733490440109
Epoch: 4780, Batch Gradient Norm after: 15.637733490440109
Epoch 4781/10000, Prediction Accuracy = 62.726%, Loss = 0.36380294561386106
Epoch: 4781, Batch Gradient Norm: 11.738847435303748
Epoch: 4781, Batch Gradient Norm after: 11.738847435303748
Epoch 4782/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.36165469884872437
Epoch: 4782, Batch Gradient Norm: 10.585677181717816
Epoch: 4782, Batch Gradient Norm after: 10.585677181717816
Epoch 4783/10000, Prediction Accuracy = 62.714%, Loss = 0.3589074730873108
Epoch: 4783, Batch Gradient Norm: 10.615620959887572
Epoch: 4783, Batch Gradient Norm after: 10.615620959887572
Epoch 4784/10000, Prediction Accuracy = 62.852%, Loss = 0.3607421636581421
Epoch: 4784, Batch Gradient Norm: 11.478626292200953
Epoch: 4784, Batch Gradient Norm after: 11.478626292200953
Epoch 4785/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3611092627048492
Epoch: 4785, Batch Gradient Norm: 10.958868157516003
Epoch: 4785, Batch Gradient Norm after: 10.958868157516003
Epoch 4786/10000, Prediction Accuracy = 62.812%, Loss = 0.35907702445983886
Epoch: 4786, Batch Gradient Norm: 15.444013309012169
Epoch: 4786, Batch Gradient Norm after: 15.444013309012169
Epoch 4787/10000, Prediction Accuracy = 62.83599999999999%, Loss = 0.36412152647972107
Epoch: 4787, Batch Gradient Norm: 14.57434141035827
Epoch: 4787, Batch Gradient Norm after: 14.57434141035827
Epoch 4788/10000, Prediction Accuracy = 62.774%, Loss = 0.36259661316871644
Epoch: 4788, Batch Gradient Norm: 14.518027601004492
Epoch: 4788, Batch Gradient Norm after: 14.518027601004492
Epoch 4789/10000, Prediction Accuracy = 62.814%, Loss = 0.3628487825393677
Epoch: 4789, Batch Gradient Norm: 15.448550651666574
Epoch: 4789, Batch Gradient Norm after: 15.448550651666574
Epoch 4790/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.36381588578224183
Epoch: 4790, Batch Gradient Norm: 16.30282346088741
Epoch: 4790, Batch Gradient Norm after: 15.864656691806008
Epoch 4791/10000, Prediction Accuracy = 62.790000000000006%, Loss = 0.3643134534358978
Epoch: 4791, Batch Gradient Norm: 12.274808838199283
Epoch: 4791, Batch Gradient Norm after: 12.274808838199283
Epoch 4792/10000, Prediction Accuracy = 62.80799999999999%, Loss = 0.35877264142036436
Epoch: 4792, Batch Gradient Norm: 12.239779522292045
Epoch: 4792, Batch Gradient Norm after: 12.239779522292045
Epoch 4793/10000, Prediction Accuracy = 62.738%, Loss = 0.3588467240333557
Epoch: 4793, Batch Gradient Norm: 13.662684164506887
Epoch: 4793, Batch Gradient Norm after: 13.662684164506887
Epoch 4794/10000, Prediction Accuracy = 62.81%, Loss = 0.3613728940486908
Epoch: 4794, Batch Gradient Norm: 13.906760391215968
Epoch: 4794, Batch Gradient Norm after: 13.906760391215968
Epoch 4795/10000, Prediction Accuracy = 62.86%, Loss = 0.3613083064556122
Epoch: 4795, Batch Gradient Norm: 12.021184614793235
Epoch: 4795, Batch Gradient Norm after: 12.021184614793235
Epoch 4796/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.36074864864349365
Epoch: 4796, Batch Gradient Norm: 15.410012375179525
Epoch: 4796, Batch Gradient Norm after: 15.410012375179525
Epoch 4797/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3648810267448425
Epoch: 4797, Batch Gradient Norm: 14.68278541496158
Epoch: 4797, Batch Gradient Norm after: 14.68278541496158
Epoch 4798/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.36549238562583924
Epoch: 4798, Batch Gradient Norm: 15.061832796638644
Epoch: 4798, Batch Gradient Norm after: 15.061832796638644
Epoch 4799/10000, Prediction Accuracy = 62.774%, Loss = 0.36355380415916444
Epoch: 4799, Batch Gradient Norm: 13.18568469058919
Epoch: 4799, Batch Gradient Norm after: 13.18568469058919
Epoch 4800/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3609171688556671
Epoch: 4800, Batch Gradient Norm: 14.15110233070769
Epoch: 4800, Batch Gradient Norm after: 14.15110233070769
Epoch 4801/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.3624886333942413
Epoch: 4801, Batch Gradient Norm: 10.920583720993534
Epoch: 4801, Batch Gradient Norm after: 10.920583720993534
Epoch 4802/10000, Prediction Accuracy = 62.814%, Loss = 0.36202715039253236
Epoch: 4802, Batch Gradient Norm: 13.469015850910312
Epoch: 4802, Batch Gradient Norm after: 13.469015850910312
Epoch 4803/10000, Prediction Accuracy = 62.826%, Loss = 0.36115615367889403
Epoch: 4803, Batch Gradient Norm: 10.696912552057794
Epoch: 4803, Batch Gradient Norm after: 10.696912552057794
Epoch 4804/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.3571864187717438
Epoch: 4804, Batch Gradient Norm: 11.583395844769196
Epoch: 4804, Batch Gradient Norm after: 11.583395844769196
Epoch 4805/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.35805270075798035
Epoch: 4805, Batch Gradient Norm: 12.5831978275765
Epoch: 4805, Batch Gradient Norm after: 12.5831978275765
Epoch 4806/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.35999119877815244
Epoch: 4806, Batch Gradient Norm: 12.825697162209144
Epoch: 4806, Batch Gradient Norm after: 12.825697162209144
Epoch 4807/10000, Prediction Accuracy = 62.774%, Loss = 0.3604222655296326
Epoch: 4807, Batch Gradient Norm: 12.636818421357292
Epoch: 4807, Batch Gradient Norm after: 12.636818421357292
Epoch 4808/10000, Prediction Accuracy = 62.762%, Loss = 0.3596886098384857
Epoch: 4808, Batch Gradient Norm: 11.84443361083834
Epoch: 4808, Batch Gradient Norm after: 11.84443361083834
Epoch 4809/10000, Prediction Accuracy = 62.83200000000001%, Loss = 0.3592768907546997
Epoch: 4809, Batch Gradient Norm: 12.027039349098485
Epoch: 4809, Batch Gradient Norm after: 12.027039349098485
Epoch 4810/10000, Prediction Accuracy = 62.78599999999999%, Loss = 0.3586611211299896
Epoch: 4810, Batch Gradient Norm: 10.83762327767844
Epoch: 4810, Batch Gradient Norm after: 10.83762327767844
Epoch 4811/10000, Prediction Accuracy = 62.75%, Loss = 0.3579275608062744
Epoch: 4811, Batch Gradient Norm: 13.25060552299493
Epoch: 4811, Batch Gradient Norm after: 13.25060552299493
Epoch 4812/10000, Prediction Accuracy = 62.806000000000004%, Loss = 0.360737019777298
Epoch: 4812, Batch Gradient Norm: 10.797712566208913
Epoch: 4812, Batch Gradient Norm after: 10.797712566208913
Epoch 4813/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.3567339539527893
Epoch: 4813, Batch Gradient Norm: 13.140615402729566
Epoch: 4813, Batch Gradient Norm after: 13.140615402729566
Epoch 4814/10000, Prediction Accuracy = 62.798%, Loss = 0.36164519786834715
Epoch: 4814, Batch Gradient Norm: 15.238317929998761
Epoch: 4814, Batch Gradient Norm after: 15.238317929998761
Epoch 4815/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.3644529163837433
Epoch: 4815, Batch Gradient Norm: 14.861626184776597
Epoch: 4815, Batch Gradient Norm after: 14.861626184776597
Epoch 4816/10000, Prediction Accuracy = 62.662%, Loss = 0.3655384540557861
Epoch: 4816, Batch Gradient Norm: 15.92254594873098
Epoch: 4816, Batch Gradient Norm after: 15.92254594873098
Epoch 4817/10000, Prediction Accuracy = 62.782%, Loss = 0.3659887731075287
Epoch: 4817, Batch Gradient Norm: 10.563663996590678
Epoch: 4817, Batch Gradient Norm after: 10.563663996590678
Epoch 4818/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.3586129009723663
Epoch: 4818, Batch Gradient Norm: 10.027765173716519
Epoch: 4818, Batch Gradient Norm after: 10.027765173716519
Epoch 4819/10000, Prediction Accuracy = 62.806%, Loss = 0.35746642351150515
Epoch: 4819, Batch Gradient Norm: 12.86230317923811
Epoch: 4819, Batch Gradient Norm after: 12.86230317923811
Epoch 4820/10000, Prediction Accuracy = 62.652%, Loss = 0.36146692037582395
Epoch: 4820, Batch Gradient Norm: 10.816472701393579
Epoch: 4820, Batch Gradient Norm after: 10.816472701393579
Epoch 4821/10000, Prediction Accuracy = 62.9%, Loss = 0.35781762599945066
Epoch: 4821, Batch Gradient Norm: 13.670341111157622
Epoch: 4821, Batch Gradient Norm after: 13.670341111157622
Epoch 4822/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.36276894211769106
Epoch: 4822, Batch Gradient Norm: 13.342232315282004
Epoch: 4822, Batch Gradient Norm after: 13.342232315282004
Epoch 4823/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.36147966980934143
Epoch: 4823, Batch Gradient Norm: 14.644853402545444
Epoch: 4823, Batch Gradient Norm after: 14.644853402545444
Epoch 4824/10000, Prediction Accuracy = 62.85%, Loss = 0.36123829483985903
Epoch: 4824, Batch Gradient Norm: 12.270960346382683
Epoch: 4824, Batch Gradient Norm after: 12.270960346382683
Epoch 4825/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.3601529061794281
Epoch: 4825, Batch Gradient Norm: 13.312606830393591
Epoch: 4825, Batch Gradient Norm after: 13.312606830393591
Epoch 4826/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3594877600669861
Epoch: 4826, Batch Gradient Norm: 11.985750445814746
Epoch: 4826, Batch Gradient Norm after: 11.985750445814746
Epoch 4827/10000, Prediction Accuracy = 62.838%, Loss = 0.3594970226287842
Epoch: 4827, Batch Gradient Norm: 10.140788617745276
Epoch: 4827, Batch Gradient Norm after: 10.140788617745276
Epoch 4828/10000, Prediction Accuracy = 62.726%, Loss = 0.35786465406417844
Epoch: 4828, Batch Gradient Norm: 10.178211693338508
Epoch: 4828, Batch Gradient Norm after: 10.178211693338508
Epoch 4829/10000, Prediction Accuracy = 62.61%, Loss = 0.35686670541763305
Epoch: 4829, Batch Gradient Norm: 12.570106502023435
Epoch: 4829, Batch Gradient Norm after: 12.570106502023435
Epoch 4830/10000, Prediction Accuracy = 62.824%, Loss = 0.3603555202484131
Epoch: 4830, Batch Gradient Norm: 11.97311471460001
Epoch: 4830, Batch Gradient Norm after: 11.97311471460001
Epoch 4831/10000, Prediction Accuracy = 62.834%, Loss = 0.36181713342666627
Epoch: 4831, Batch Gradient Norm: 12.207423008367147
Epoch: 4831, Batch Gradient Norm after: 12.207423008367147
Epoch 4832/10000, Prediction Accuracy = 62.75%, Loss = 0.3582225739955902
Epoch: 4832, Batch Gradient Norm: 14.320004870028338
Epoch: 4832, Batch Gradient Norm after: 14.320004870028338
Epoch 4833/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.36280837059021
Epoch: 4833, Batch Gradient Norm: 15.156386528421875
Epoch: 4833, Batch Gradient Norm after: 15.156386528421875
Epoch 4834/10000, Prediction Accuracy = 62.754%, Loss = 0.3619545757770538
Epoch: 4834, Batch Gradient Norm: 18.126683412626885
Epoch: 4834, Batch Gradient Norm after: 18.105542378511178
Epoch 4835/10000, Prediction Accuracy = 62.734%, Loss = 0.3687274754047394
Epoch: 4835, Batch Gradient Norm: 14.499954769472103
Epoch: 4835, Batch Gradient Norm after: 14.499954769472103
Epoch 4836/10000, Prediction Accuracy = 62.758%, Loss = 0.36376659870147704
Epoch: 4836, Batch Gradient Norm: 14.583750733366147
Epoch: 4836, Batch Gradient Norm after: 14.583750733366147
Epoch 4837/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.36095391511917113
Epoch: 4837, Batch Gradient Norm: 14.16661888633222
Epoch: 4837, Batch Gradient Norm after: 14.16661888633222
Epoch 4838/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.36169666051864624
Epoch: 4838, Batch Gradient Norm: 13.86341478058972
Epoch: 4838, Batch Gradient Norm after: 13.86341478058972
Epoch 4839/10000, Prediction Accuracy = 62.89000000000001%, Loss = 0.3612474977970123
Epoch: 4839, Batch Gradient Norm: 14.1392851520502
Epoch: 4839, Batch Gradient Norm after: 14.1392851520502
Epoch 4840/10000, Prediction Accuracy = 62.745999999999995%, Loss = 0.36057373881340027
Epoch: 4840, Batch Gradient Norm: 10.717929545677766
Epoch: 4840, Batch Gradient Norm after: 10.717929545677766
Epoch 4841/10000, Prediction Accuracy = 62.852%, Loss = 0.35642498135566714
Epoch: 4841, Batch Gradient Norm: 11.992910704460538
Epoch: 4841, Batch Gradient Norm after: 11.992910704460538
Epoch 4842/10000, Prediction Accuracy = 62.852%, Loss = 0.3575762212276459
Epoch: 4842, Batch Gradient Norm: 12.70020835978465
Epoch: 4842, Batch Gradient Norm after: 12.70020835978465
Epoch 4843/10000, Prediction Accuracy = 62.824%, Loss = 0.3597471594810486
Epoch: 4843, Batch Gradient Norm: 10.380960586933394
Epoch: 4843, Batch Gradient Norm after: 10.380960586933394
Epoch 4844/10000, Prediction Accuracy = 62.862%, Loss = 0.35594968795776366
Epoch: 4844, Batch Gradient Norm: 13.567594820226352
Epoch: 4844, Batch Gradient Norm after: 13.567594820226352
Epoch 4845/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.36053899526596067
Epoch: 4845, Batch Gradient Norm: 13.235774778190512
Epoch: 4845, Batch Gradient Norm after: 13.235774778190512
Epoch 4846/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.35867854952812195
Epoch: 4846, Batch Gradient Norm: 12.145753100244953
Epoch: 4846, Batch Gradient Norm after: 12.145753100244953
Epoch 4847/10000, Prediction Accuracy = 62.77199999999999%, Loss = 0.3587826073169708
Epoch: 4847, Batch Gradient Norm: 11.523186088395569
Epoch: 4847, Batch Gradient Norm after: 11.523186088395569
Epoch 4848/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.35957087874412536
Epoch: 4848, Batch Gradient Norm: 11.577729904843025
Epoch: 4848, Batch Gradient Norm after: 11.577729904843025
Epoch 4849/10000, Prediction Accuracy = 62.767999999999994%, Loss = 0.3564657151699066
Epoch: 4849, Batch Gradient Norm: 9.454653431971055
Epoch: 4849, Batch Gradient Norm after: 9.454653431971055
Epoch 4850/10000, Prediction Accuracy = 62.852%, Loss = 0.35471264719963075
Epoch: 4850, Batch Gradient Norm: 9.962321713749105
Epoch: 4850, Batch Gradient Norm after: 9.962321713749105
Epoch 4851/10000, Prediction Accuracy = 62.836%, Loss = 0.35623825192451475
Epoch: 4851, Batch Gradient Norm: 12.877194866025961
Epoch: 4851, Batch Gradient Norm after: 12.877194866025961
Epoch 4852/10000, Prediction Accuracy = 62.852%, Loss = 0.35881932973861697
Epoch: 4852, Batch Gradient Norm: 13.679570246435947
Epoch: 4852, Batch Gradient Norm after: 13.679570246435947
Epoch 4853/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.35940657258033754
Epoch: 4853, Batch Gradient Norm: 15.096799953266276
Epoch: 4853, Batch Gradient Norm after: 15.096799953266276
Epoch 4854/10000, Prediction Accuracy = 62.866%, Loss = 0.36295239329338075
Epoch: 4854, Batch Gradient Norm: 14.371511943019037
Epoch: 4854, Batch Gradient Norm after: 14.371511943019037
Epoch 4855/10000, Prediction Accuracy = 62.884%, Loss = 0.3620271921157837
Epoch: 4855, Batch Gradient Norm: 13.363958076210318
Epoch: 4855, Batch Gradient Norm after: 13.363958076210318
Epoch 4856/10000, Prediction Accuracy = 62.898%, Loss = 0.3618864893913269
Epoch: 4856, Batch Gradient Norm: 10.847593761026474
Epoch: 4856, Batch Gradient Norm after: 10.847593761026474
Epoch 4857/10000, Prediction Accuracy = 62.798%, Loss = 0.35500040650367737
Epoch: 4857, Batch Gradient Norm: 10.555643976346357
Epoch: 4857, Batch Gradient Norm after: 10.555643976346357
Epoch 4858/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.35610286593437196
Epoch: 4858, Batch Gradient Norm: 14.43009556620009
Epoch: 4858, Batch Gradient Norm after: 14.43009556620009
Epoch 4859/10000, Prediction Accuracy = 62.818%, Loss = 0.3617969810962677
Epoch: 4859, Batch Gradient Norm: 15.325767778708073
Epoch: 4859, Batch Gradient Norm after: 15.325767778708073
Epoch 4860/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.36064797043800356
Epoch: 4860, Batch Gradient Norm: 14.894266431627702
Epoch: 4860, Batch Gradient Norm after: 14.894266431627702
Epoch 4861/10000, Prediction Accuracy = 62.7%, Loss = 0.3624995529651642
Epoch: 4861, Batch Gradient Norm: 14.394892374179062
Epoch: 4861, Batch Gradient Norm after: 14.394892374179062
Epoch 4862/10000, Prediction Accuracy = 62.874%, Loss = 0.36083528995513914
Epoch: 4862, Batch Gradient Norm: 15.300434350284963
Epoch: 4862, Batch Gradient Norm after: 15.300434350284963
Epoch 4863/10000, Prediction Accuracy = 62.848%, Loss = 0.3611002802848816
Epoch: 4863, Batch Gradient Norm: 13.067312963089929
Epoch: 4863, Batch Gradient Norm after: 13.067312963089929
Epoch 4864/10000, Prediction Accuracy = 62.786%, Loss = 0.3597750306129456
Epoch: 4864, Batch Gradient Norm: 13.720344040895043
Epoch: 4864, Batch Gradient Norm after: 13.720344040895043
Epoch 4865/10000, Prediction Accuracy = 62.876%, Loss = 0.3596330165863037
Epoch: 4865, Batch Gradient Norm: 14.544661667529507
Epoch: 4865, Batch Gradient Norm after: 14.544661667529507
Epoch 4866/10000, Prediction Accuracy = 62.812%, Loss = 0.3600919783115387
Epoch: 4866, Batch Gradient Norm: 12.691259494234941
Epoch: 4866, Batch Gradient Norm after: 12.691259494234941
Epoch 4867/10000, Prediction Accuracy = 62.964%, Loss = 0.3598669171333313
Epoch: 4867, Batch Gradient Norm: 12.698766173982833
Epoch: 4867, Batch Gradient Norm after: 12.698766173982833
Epoch 4868/10000, Prediction Accuracy = 62.83200000000001%, Loss = 0.35696046948432925
Epoch: 4868, Batch Gradient Norm: 14.132096497606575
Epoch: 4868, Batch Gradient Norm after: 14.132096497606575
Epoch 4869/10000, Prediction Accuracy = 62.834%, Loss = 0.3602872550487518
Epoch: 4869, Batch Gradient Norm: 17.95560415838206
Epoch: 4869, Batch Gradient Norm after: 17.86007298547373
Epoch 4870/10000, Prediction Accuracy = 62.92%, Loss = 0.3670871675014496
Epoch: 4870, Batch Gradient Norm: 13.207718383814461
Epoch: 4870, Batch Gradient Norm after: 13.207718383814461
Epoch 4871/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.36083453297615053
Epoch: 4871, Batch Gradient Norm: 15.07565692763381
Epoch: 4871, Batch Gradient Norm after: 15.07565692763381
Epoch 4872/10000, Prediction Accuracy = 62.89399999999999%, Loss = 0.360668009519577
Epoch: 4872, Batch Gradient Norm: 15.246678683047833
Epoch: 4872, Batch Gradient Norm after: 15.246678683047833
Epoch 4873/10000, Prediction Accuracy = 62.78799999999999%, Loss = 0.361787748336792
Epoch: 4873, Batch Gradient Norm: 14.793480768492662
Epoch: 4873, Batch Gradient Norm after: 14.793480768492662
Epoch 4874/10000, Prediction Accuracy = 62.85999999999999%, Loss = 0.3603267133235931
Epoch: 4874, Batch Gradient Norm: 15.8672562338162
Epoch: 4874, Batch Gradient Norm after: 15.8672562338162
Epoch 4875/10000, Prediction Accuracy = 63.004%, Loss = 0.36432088613510133
Epoch: 4875, Batch Gradient Norm: 11.491661046240537
Epoch: 4875, Batch Gradient Norm after: 11.491661046240537
Epoch 4876/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.3566738724708557
Epoch: 4876, Batch Gradient Norm: 12.901782608004545
Epoch: 4876, Batch Gradient Norm after: 12.901782608004545
Epoch 4877/10000, Prediction Accuracy = 62.928%, Loss = 0.35995269417762754
Epoch: 4877, Batch Gradient Norm: 13.481524571213052
Epoch: 4877, Batch Gradient Norm after: 13.481524571213052
Epoch 4878/10000, Prediction Accuracy = 62.864%, Loss = 0.3591536581516266
Epoch: 4878, Batch Gradient Norm: 9.951451520512164
Epoch: 4878, Batch Gradient Norm after: 9.951451520512164
Epoch 4879/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.3558698773384094
Epoch: 4879, Batch Gradient Norm: 13.03252462262085
Epoch: 4879, Batch Gradient Norm after: 13.03252462262085
Epoch 4880/10000, Prediction Accuracy = 62.886%, Loss = 0.3611564576625824
Epoch: 4880, Batch Gradient Norm: 12.57917342914452
Epoch: 4880, Batch Gradient Norm after: 12.57917342914452
Epoch 4881/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.36015615463256834
Epoch: 4881, Batch Gradient Norm: 14.342267428578632
Epoch: 4881, Batch Gradient Norm after: 14.342267428578632
Epoch 4882/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.3633825182914734
Epoch: 4882, Batch Gradient Norm: 13.02181537024328
Epoch: 4882, Batch Gradient Norm after: 13.02181537024328
Epoch 4883/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.35938220024108886
Epoch: 4883, Batch Gradient Norm: 15.77166083376533
Epoch: 4883, Batch Gradient Norm after: 15.77166083376533
Epoch 4884/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.36147215962409973
Epoch: 4884, Batch Gradient Norm: 14.065721782784724
Epoch: 4884, Batch Gradient Norm after: 14.065721782784724
Epoch 4885/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3604898571968079
Epoch: 4885, Batch Gradient Norm: 11.049069847244015
Epoch: 4885, Batch Gradient Norm after: 11.049069847244015
Epoch 4886/10000, Prediction Accuracy = 62.786%, Loss = 0.3563010275363922
Epoch: 4886, Batch Gradient Norm: 12.707675523500077
Epoch: 4886, Batch Gradient Norm after: 12.707675523500077
Epoch 4887/10000, Prediction Accuracy = 62.89%, Loss = 0.358690470457077
Epoch: 4887, Batch Gradient Norm: 11.817606430081181
Epoch: 4887, Batch Gradient Norm after: 11.817606430081181
Epoch 4888/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.3597375452518463
Epoch: 4888, Batch Gradient Norm: 13.020076249150435
Epoch: 4888, Batch Gradient Norm after: 13.020076249150435
Epoch 4889/10000, Prediction Accuracy = 62.838%, Loss = 0.3597630143165588
Epoch: 4889, Batch Gradient Norm: 14.476485324508978
Epoch: 4889, Batch Gradient Norm after: 14.476485324508978
Epoch 4890/10000, Prediction Accuracy = 62.824%, Loss = 0.3590750992298126
Epoch: 4890, Batch Gradient Norm: 13.841250002007113
Epoch: 4890, Batch Gradient Norm after: 13.841250002007113
Epoch 4891/10000, Prediction Accuracy = 62.888%, Loss = 0.3596955180168152
Epoch: 4891, Batch Gradient Norm: 12.849969130299872
Epoch: 4891, Batch Gradient Norm after: 12.849969130299872
Epoch 4892/10000, Prediction Accuracy = 62.822%, Loss = 0.3579515516757965
Epoch: 4892, Batch Gradient Norm: 14.188951964315006
Epoch: 4892, Batch Gradient Norm after: 14.188951964315006
Epoch 4893/10000, Prediction Accuracy = 62.774%, Loss = 0.36406511068344116
Epoch: 4893, Batch Gradient Norm: 12.897248351000917
Epoch: 4893, Batch Gradient Norm after: 12.897248351000917
Epoch 4894/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.35808088183403014
Epoch: 4894, Batch Gradient Norm: 12.398231656636543
Epoch: 4894, Batch Gradient Norm after: 12.398231656636543
Epoch 4895/10000, Prediction Accuracy = 62.898%, Loss = 0.35706745386123656
Epoch: 4895, Batch Gradient Norm: 10.882367187909166
Epoch: 4895, Batch Gradient Norm after: 10.882367187909166
Epoch 4896/10000, Prediction Accuracy = 62.912%, Loss = 0.356617683172226
Epoch: 4896, Batch Gradient Norm: 11.952527041882595
Epoch: 4896, Batch Gradient Norm after: 11.952527041882595
Epoch 4897/10000, Prediction Accuracy = 62.822%, Loss = 0.35766281485557555
Epoch: 4897, Batch Gradient Norm: 15.286303031868862
Epoch: 4897, Batch Gradient Norm after: 15.286303031868862
Epoch 4898/10000, Prediction Accuracy = 62.908%, Loss = 0.3624695479869843
Epoch: 4898, Batch Gradient Norm: 14.749140708278466
Epoch: 4898, Batch Gradient Norm after: 14.749140708278466
Epoch 4899/10000, Prediction Accuracy = 62.83%, Loss = 0.36203219890594485
Epoch: 4899, Batch Gradient Norm: 15.500860032621363
Epoch: 4899, Batch Gradient Norm after: 15.500860032621363
Epoch 4900/10000, Prediction Accuracy = 62.83800000000001%, Loss = 0.3613599479198456
Epoch: 4900, Batch Gradient Norm: 12.166486785708752
Epoch: 4900, Batch Gradient Norm after: 12.166486785708752
Epoch 4901/10000, Prediction Accuracy = 62.996%, Loss = 0.3569833815097809
Epoch: 4901, Batch Gradient Norm: 10.940658226675772
Epoch: 4901, Batch Gradient Norm after: 10.940658226675772
Epoch 4902/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.35466583371162413
Epoch: 4902, Batch Gradient Norm: 11.610533246625518
Epoch: 4902, Batch Gradient Norm after: 11.610533246625518
Epoch 4903/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.3564150869846344
Epoch: 4903, Batch Gradient Norm: 14.336862098126389
Epoch: 4903, Batch Gradient Norm after: 14.336862098126389
Epoch 4904/10000, Prediction Accuracy = 62.858000000000004%, Loss = 0.36165851354599
Epoch: 4904, Batch Gradient Norm: 13.764285453875049
Epoch: 4904, Batch Gradient Norm after: 13.764285453875049
Epoch 4905/10000, Prediction Accuracy = 62.89200000000001%, Loss = 0.36036166548728943
Epoch: 4905, Batch Gradient Norm: 11.641850106713168
Epoch: 4905, Batch Gradient Norm after: 11.641850106713168
Epoch 4906/10000, Prediction Accuracy = 62.824%, Loss = 0.3565931797027588
Epoch: 4906, Batch Gradient Norm: 11.433567343077826
Epoch: 4906, Batch Gradient Norm after: 11.433567343077826
Epoch 4907/10000, Prediction Accuracy = 62.89%, Loss = 0.3574510157108307
Epoch: 4907, Batch Gradient Norm: 15.20290688345306
Epoch: 4907, Batch Gradient Norm after: 15.20290688345306
Epoch 4908/10000, Prediction Accuracy = 62.854%, Loss = 0.36226674914360046
Epoch: 4908, Batch Gradient Norm: 13.759389384198055
Epoch: 4908, Batch Gradient Norm after: 13.759389384198055
Epoch 4909/10000, Prediction Accuracy = 62.862%, Loss = 0.36092802286148074
Epoch: 4909, Batch Gradient Norm: 15.534720405842465
Epoch: 4909, Batch Gradient Norm after: 15.534720405842465
Epoch 4910/10000, Prediction Accuracy = 62.896%, Loss = 0.36498684883117677
Epoch: 4910, Batch Gradient Norm: 15.875660034503097
Epoch: 4910, Batch Gradient Norm after: 15.875660034503097
Epoch 4911/10000, Prediction Accuracy = 62.76400000000001%, Loss = 0.3622665822505951
Epoch: 4911, Batch Gradient Norm: 11.95660169353308
Epoch: 4911, Batch Gradient Norm after: 11.95660169353308
Epoch 4912/10000, Prediction Accuracy = 62.836%, Loss = 0.3571180641651154
Epoch: 4912, Batch Gradient Norm: 12.83603695389668
Epoch: 4912, Batch Gradient Norm after: 12.83603695389668
Epoch 4913/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.35874212980270387
Epoch: 4913, Batch Gradient Norm: 13.684683204256123
Epoch: 4913, Batch Gradient Norm after: 13.684683204256123
Epoch 4914/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.3585459113121033
Epoch: 4914, Batch Gradient Norm: 12.029308355019891
Epoch: 4914, Batch Gradient Norm after: 12.029308355019891
Epoch 4915/10000, Prediction Accuracy = 62.822%, Loss = 0.356646466255188
Epoch: 4915, Batch Gradient Norm: 10.909949405455508
Epoch: 4915, Batch Gradient Norm after: 10.909949405455508
Epoch 4916/10000, Prediction Accuracy = 62.96%, Loss = 0.355556720495224
Epoch: 4916, Batch Gradient Norm: 11.635093830017459
Epoch: 4916, Batch Gradient Norm after: 11.635093830017459
Epoch 4917/10000, Prediction Accuracy = 62.75%, Loss = 0.35567726492881774
Epoch: 4917, Batch Gradient Norm: 12.724654621117708
Epoch: 4917, Batch Gradient Norm after: 12.724654621117708
Epoch 4918/10000, Prediction Accuracy = 62.834%, Loss = 0.35864253640174865
Epoch: 4918, Batch Gradient Norm: 12.718933696837029
Epoch: 4918, Batch Gradient Norm after: 12.718933696837029
Epoch 4919/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.35789873003959655
Epoch: 4919, Batch Gradient Norm: 14.750090471221164
Epoch: 4919, Batch Gradient Norm after: 14.750090471221164
Epoch 4920/10000, Prediction Accuracy = 62.822%, Loss = 0.36194377541542055
Epoch: 4920, Batch Gradient Norm: 11.586280257313279
Epoch: 4920, Batch Gradient Norm after: 11.586280257313279
Epoch 4921/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3564449608325958
Epoch: 4921, Batch Gradient Norm: 10.309970776836943
Epoch: 4921, Batch Gradient Norm after: 10.309970776836943
Epoch 4922/10000, Prediction Accuracy = 62.842%, Loss = 0.35378206372261045
Epoch: 4922, Batch Gradient Norm: 11.70254770233427
Epoch: 4922, Batch Gradient Norm after: 11.70254770233427
Epoch 4923/10000, Prediction Accuracy = 62.79%, Loss = 0.35626745223999023
Epoch: 4923, Batch Gradient Norm: 14.330414603465968
Epoch: 4923, Batch Gradient Norm after: 14.330414603465968
Epoch 4924/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.35846285820007323
Epoch: 4924, Batch Gradient Norm: 15.22030624991383
Epoch: 4924, Batch Gradient Norm after: 15.22030624991383
Epoch 4925/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.35769309997558596
Epoch: 4925, Batch Gradient Norm: 13.175808201799926
Epoch: 4925, Batch Gradient Norm after: 13.175808201799926
Epoch 4926/10000, Prediction Accuracy = 62.83%, Loss = 0.3570732891559601
Epoch: 4926, Batch Gradient Norm: 13.809559288250458
Epoch: 4926, Batch Gradient Norm after: 13.809559288250458
Epoch 4927/10000, Prediction Accuracy = 62.822%, Loss = 0.3587559700012207
Epoch: 4927, Batch Gradient Norm: 16.722581276130715
Epoch: 4927, Batch Gradient Norm after: 16.722581276130715
Epoch 4928/10000, Prediction Accuracy = 62.788%, Loss = 0.3632074475288391
Epoch: 4928, Batch Gradient Norm: 15.554611316157589
Epoch: 4928, Batch Gradient Norm after: 15.554611316157589
Epoch 4929/10000, Prediction Accuracy = 62.786%, Loss = 0.3637531936168671
Epoch: 4929, Batch Gradient Norm: 18.215205814904902
Epoch: 4929, Batch Gradient Norm after: 18.215205814904902
Epoch 4930/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.36698625683784486
Epoch: 4930, Batch Gradient Norm: 17.987997843305692
Epoch: 4930, Batch Gradient Norm after: 17.92002727777733
Epoch 4931/10000, Prediction Accuracy = 62.818%, Loss = 0.36618671417236326
Epoch: 4931, Batch Gradient Norm: 18.470089275966536
Epoch: 4931, Batch Gradient Norm after: 18.470089275966536
Epoch 4932/10000, Prediction Accuracy = 62.77%, Loss = 0.36874206066131593
Epoch: 4932, Batch Gradient Norm: 19.68795508504922
Epoch: 4932, Batch Gradient Norm after: 19.68795508504922
Epoch 4933/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.37010031938552856
Epoch: 4933, Batch Gradient Norm: 18.355361181680703
Epoch: 4933, Batch Gradient Norm after: 18.355361181680703
Epoch 4934/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.37160850167274473
Epoch: 4934, Batch Gradient Norm: 13.63861643429857
Epoch: 4934, Batch Gradient Norm after: 13.63861643429857
Epoch 4935/10000, Prediction Accuracy = 62.924%, Loss = 0.36090118885040284
Epoch: 4935, Batch Gradient Norm: 13.993991082786033
Epoch: 4935, Batch Gradient Norm after: 13.993991082786033
Epoch 4936/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.35853257179260256
Epoch: 4936, Batch Gradient Norm: 13.252746326922502
Epoch: 4936, Batch Gradient Norm after: 13.252746326922502
Epoch 4937/10000, Prediction Accuracy = 62.86400000000001%, Loss = 0.35971771478652953
Epoch: 4937, Batch Gradient Norm: 11.661424030846467
Epoch: 4937, Batch Gradient Norm after: 11.661424030846467
Epoch 4938/10000, Prediction Accuracy = 62.94%, Loss = 0.356528514623642
Epoch: 4938, Batch Gradient Norm: 10.318900528457114
Epoch: 4938, Batch Gradient Norm after: 10.318900528457114
Epoch 4939/10000, Prediction Accuracy = 62.824%, Loss = 0.35386891961097716
Epoch: 4939, Batch Gradient Norm: 14.329469066925506
Epoch: 4939, Batch Gradient Norm after: 14.329469066925506
Epoch 4940/10000, Prediction Accuracy = 62.794%, Loss = 0.35989726781845094
Epoch: 4940, Batch Gradient Norm: 13.4828559785237
Epoch: 4940, Batch Gradient Norm after: 13.4828559785237
Epoch 4941/10000, Prediction Accuracy = 62.767999999999994%, Loss = 0.35938583612442015
Epoch: 4941, Batch Gradient Norm: 10.731530690351866
Epoch: 4941, Batch Gradient Norm after: 10.731530690351866
Epoch 4942/10000, Prediction Accuracy = 62.754%, Loss = 0.35521234273910524
Epoch: 4942, Batch Gradient Norm: 11.680188315911732
Epoch: 4942, Batch Gradient Norm after: 11.680188315911732
Epoch 4943/10000, Prediction Accuracy = 62.8%, Loss = 0.3557345986366272
Epoch: 4943, Batch Gradient Norm: 11.732715190981216
Epoch: 4943, Batch Gradient Norm after: 11.732715190981216
Epoch 4944/10000, Prediction Accuracy = 62.89399999999999%, Loss = 0.3549356162548065
Epoch: 4944, Batch Gradient Norm: 12.14758432784124
Epoch: 4944, Batch Gradient Norm after: 12.14758432784124
Epoch 4945/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.3562249898910522
Epoch: 4945, Batch Gradient Norm: 10.183160930387832
Epoch: 4945, Batch Gradient Norm after: 10.183160930387832
Epoch 4946/10000, Prediction Accuracy = 62.92999999999999%, Loss = 0.35555121302604675
Epoch: 4946, Batch Gradient Norm: 10.448042164653796
Epoch: 4946, Batch Gradient Norm after: 10.448042164653796
Epoch 4947/10000, Prediction Accuracy = 62.85600000000001%, Loss = 0.3546143352985382
Epoch: 4947, Batch Gradient Norm: 14.468202367610429
Epoch: 4947, Batch Gradient Norm after: 14.468202367610429
Epoch 4948/10000, Prediction Accuracy = 62.9%, Loss = 0.36163222789764404
Epoch: 4948, Batch Gradient Norm: 13.435020697249533
Epoch: 4948, Batch Gradient Norm after: 13.435020697249533
Epoch 4949/10000, Prediction Accuracy = 62.91600000000001%, Loss = 0.3574307143688202
Epoch: 4949, Batch Gradient Norm: 13.965050891386404
Epoch: 4949, Batch Gradient Norm after: 13.965050891386404
Epoch 4950/10000, Prediction Accuracy = 62.92600000000001%, Loss = 0.3595857858657837
Epoch: 4950, Batch Gradient Norm: 14.043530053633148
Epoch: 4950, Batch Gradient Norm after: 14.043530053633148
Epoch 4951/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.35930862426757815
Epoch: 4951, Batch Gradient Norm: 13.875761378368054
Epoch: 4951, Batch Gradient Norm after: 13.875761378368054
Epoch 4952/10000, Prediction Accuracy = 62.842%, Loss = 0.35797343850135804
Epoch: 4952, Batch Gradient Norm: 13.340935838757447
Epoch: 4952, Batch Gradient Norm after: 13.340935838757447
Epoch 4953/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.35664166808128356
Epoch: 4953, Batch Gradient Norm: 12.359223433421862
Epoch: 4953, Batch Gradient Norm after: 12.359223433421862
Epoch 4954/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.35697346925735474
Epoch: 4954, Batch Gradient Norm: 12.512651658933411
Epoch: 4954, Batch Gradient Norm after: 12.512651658933411
Epoch 4955/10000, Prediction Accuracy = 62.758%, Loss = 0.3565488874912262
Epoch: 4955, Batch Gradient Norm: 14.306693013710321
Epoch: 4955, Batch Gradient Norm after: 14.306693013710321
Epoch 4956/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.3620091438293457
Epoch: 4956, Batch Gradient Norm: 9.600315062345846
Epoch: 4956, Batch Gradient Norm after: 9.600315062345846
Epoch 4957/10000, Prediction Accuracy = 62.902%, Loss = 0.3510964035987854
Epoch: 4957, Batch Gradient Norm: 11.780519705051617
Epoch: 4957, Batch Gradient Norm after: 11.780519705051617
Epoch 4958/10000, Prediction Accuracy = 62.794%, Loss = 0.35612890124320984
Epoch: 4958, Batch Gradient Norm: 10.709717036867271
Epoch: 4958, Batch Gradient Norm after: 10.709717036867271
Epoch 4959/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.35340712070465086
Epoch: 4959, Batch Gradient Norm: 15.106054284213267
Epoch: 4959, Batch Gradient Norm after: 15.106054284213267
Epoch 4960/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.35960400104522705
Epoch: 4960, Batch Gradient Norm: 16.051239998364576
Epoch: 4960, Batch Gradient Norm after: 15.97971101015404
Epoch 4961/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.3609273672103882
Epoch: 4961, Batch Gradient Norm: 15.110327237898554
Epoch: 4961, Batch Gradient Norm after: 15.110327237898554
Epoch 4962/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3583477556705475
Epoch: 4962, Batch Gradient Norm: 14.030711919271287
Epoch: 4962, Batch Gradient Norm after: 14.030711919271287
Epoch 4963/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.35687544345855715
Epoch: 4963, Batch Gradient Norm: 11.632811907727971
Epoch: 4963, Batch Gradient Norm after: 11.632811907727971
Epoch 4964/10000, Prediction Accuracy = 62.77%, Loss = 0.3564266085624695
Epoch: 4964, Batch Gradient Norm: 10.434322968887649
Epoch: 4964, Batch Gradient Norm after: 10.434322968887649
Epoch 4965/10000, Prediction Accuracy = 62.93399999999999%, Loss = 0.35473082065582273
Epoch: 4965, Batch Gradient Norm: 11.423715850585749
Epoch: 4965, Batch Gradient Norm after: 11.423715850585749
Epoch 4966/10000, Prediction Accuracy = 62.862%, Loss = 0.35405277609825136
Epoch: 4966, Batch Gradient Norm: 15.675886325485214
Epoch: 4966, Batch Gradient Norm after: 15.675886325485214
Epoch 4967/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.36006855964660645
Epoch: 4967, Batch Gradient Norm: 14.926645988460704
Epoch: 4967, Batch Gradient Norm after: 14.926645988460704
Epoch 4968/10000, Prediction Accuracy = 62.94%, Loss = 0.3575369417667389
Epoch: 4968, Batch Gradient Norm: 10.461975750867827
Epoch: 4968, Batch Gradient Norm after: 10.461975750867827
Epoch 4969/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.35272365212440493
Epoch: 4969, Batch Gradient Norm: 11.671335919502225
Epoch: 4969, Batch Gradient Norm after: 11.671335919502225
Epoch 4970/10000, Prediction Accuracy = 62.86%, Loss = 0.35298478603363037
Epoch: 4970, Batch Gradient Norm: 12.819201561506766
Epoch: 4970, Batch Gradient Norm after: 12.819201561506766
Epoch 4971/10000, Prediction Accuracy = 62.806%, Loss = 0.35717416405677793
Epoch: 4971, Batch Gradient Norm: 13.806384671971134
Epoch: 4971, Batch Gradient Norm after: 13.806384671971134
Epoch 4972/10000, Prediction Accuracy = 62.852%, Loss = 0.35843493342399596
Epoch: 4972, Batch Gradient Norm: 12.539248996691358
Epoch: 4972, Batch Gradient Norm after: 12.539248996691358
Epoch 4973/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3552566945552826
Epoch: 4973, Batch Gradient Norm: 14.45940935898269
Epoch: 4973, Batch Gradient Norm after: 14.45940935898269
Epoch 4974/10000, Prediction Accuracy = 62.83599999999999%, Loss = 0.35752707719802856
Epoch: 4974, Batch Gradient Norm: 14.961851895093268
Epoch: 4974, Batch Gradient Norm after: 14.961851895093268
Epoch 4975/10000, Prediction Accuracy = 62.902%, Loss = 0.3601192235946655
Epoch: 4975, Batch Gradient Norm: 16.431763923300387
Epoch: 4975, Batch Gradient Norm after: 16.431763923300387
Epoch 4976/10000, Prediction Accuracy = 62.822%, Loss = 0.36211493611335754
Epoch: 4976, Batch Gradient Norm: 16.643312496334755
Epoch: 4976, Batch Gradient Norm after: 16.643312496334755
Epoch 4977/10000, Prediction Accuracy = 62.93399999999999%, Loss = 0.36349415183067324
Epoch: 4977, Batch Gradient Norm: 10.492001496970826
Epoch: 4977, Batch Gradient Norm after: 10.492001496970826
Epoch 4978/10000, Prediction Accuracy = 62.9%, Loss = 0.35236180424690244
Epoch: 4978, Batch Gradient Norm: 11.973025258723206
Epoch: 4978, Batch Gradient Norm after: 11.973025258723206
Epoch 4979/10000, Prediction Accuracy = 62.78000000000001%, Loss = 0.35447114109992983
Epoch: 4979, Batch Gradient Norm: 13.688710511583384
Epoch: 4979, Batch Gradient Norm after: 13.688710511583384
Epoch 4980/10000, Prediction Accuracy = 62.848%, Loss = 0.35890639424324033
Epoch: 4980, Batch Gradient Norm: 12.629668538372048
Epoch: 4980, Batch Gradient Norm after: 12.629668538372048
Epoch 4981/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3542465090751648
Epoch: 4981, Batch Gradient Norm: 11.187464341530369
Epoch: 4981, Batch Gradient Norm after: 11.187464341530369
Epoch 4982/10000, Prediction Accuracy = 62.85%, Loss = 0.3544703483581543
Epoch: 4982, Batch Gradient Norm: 11.148669156050474
Epoch: 4982, Batch Gradient Norm after: 11.148669156050474
Epoch 4983/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.3554531931877136
Epoch: 4983, Batch Gradient Norm: 10.57872707631129
Epoch: 4983, Batch Gradient Norm after: 10.57872707631129
Epoch 4984/10000, Prediction Accuracy = 62.842%, Loss = 0.3539625108242035
Epoch: 4984, Batch Gradient Norm: 12.291872808660159
Epoch: 4984, Batch Gradient Norm after: 12.291872808660159
Epoch 4985/10000, Prediction Accuracy = 62.896%, Loss = 0.3540491104125977
Epoch: 4985, Batch Gradient Norm: 16.05658297078983
Epoch: 4985, Batch Gradient Norm after: 16.05658297078983
Epoch 4986/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3609337568283081
Epoch: 4986, Batch Gradient Norm: 13.575684104861118
Epoch: 4986, Batch Gradient Norm after: 13.575684104861118
Epoch 4987/10000, Prediction Accuracy = 62.89%, Loss = 0.3594130873680115
Epoch: 4987, Batch Gradient Norm: 10.00359724870839
Epoch: 4987, Batch Gradient Norm after: 10.00359724870839
Epoch 4988/10000, Prediction Accuracy = 62.956%, Loss = 0.3536223590373993
Epoch: 4988, Batch Gradient Norm: 12.630029689193883
Epoch: 4988, Batch Gradient Norm after: 12.630029689193883
Epoch 4989/10000, Prediction Accuracy = 62.94799999999999%, Loss = 0.35469093918800354
Epoch: 4989, Batch Gradient Norm: 12.50211472742186
Epoch: 4989, Batch Gradient Norm after: 12.50211472742186
Epoch 4990/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3560229420661926
Epoch: 4990, Batch Gradient Norm: 14.048068853100627
Epoch: 4990, Batch Gradient Norm after: 14.048068853100627
Epoch 4991/10000, Prediction Accuracy = 62.884%, Loss = 0.3573238492012024
Epoch: 4991, Batch Gradient Norm: 14.301443476731919
Epoch: 4991, Batch Gradient Norm after: 14.301443476731919
Epoch 4992/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.3580435931682587
Epoch: 4992, Batch Gradient Norm: 11.029772456608857
Epoch: 4992, Batch Gradient Norm after: 11.029772456608857
Epoch 4993/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.35524905323982237
Epoch: 4993, Batch Gradient Norm: 9.326481603187382
Epoch: 4993, Batch Gradient Norm after: 9.326481603187382
Epoch 4994/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.35184294581413267
Epoch: 4994, Batch Gradient Norm: 11.200534149856239
Epoch: 4994, Batch Gradient Norm after: 11.200534149856239
Epoch 4995/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.3557185232639313
Epoch: 4995, Batch Gradient Norm: 10.615091318498221
Epoch: 4995, Batch Gradient Norm after: 10.615091318498221
Epoch 4996/10000, Prediction Accuracy = 62.938%, Loss = 0.351694130897522
Epoch: 4996, Batch Gradient Norm: 9.909631701484745
Epoch: 4996, Batch Gradient Norm after: 9.909631701484745
Epoch 4997/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.35356601476669314
Epoch: 4997, Batch Gradient Norm: 11.766665949666262
Epoch: 4997, Batch Gradient Norm after: 11.766665949666262
Epoch 4998/10000, Prediction Accuracy = 62.896%, Loss = 0.35432352423667907
Epoch: 4998, Batch Gradient Norm: 13.71616486815274
Epoch: 4998, Batch Gradient Norm after: 13.71616486815274
Epoch 4999/10000, Prediction Accuracy = 62.943999999999996%, Loss = 0.3565945506095886
Epoch: 4999, Batch Gradient Norm: 16.342671813320823
Epoch: 4999, Batch Gradient Norm after: 16.342671813320823
Epoch 5000/10000, Prediction Accuracy = 62.962%, Loss = 0.3627431333065033
Epoch: 5000, Batch Gradient Norm: 13.385878401220856
Epoch: 5000, Batch Gradient Norm after: 13.385878401220856
Epoch 5001/10000, Prediction Accuracy = 62.90999999999999%, Loss = 0.3581691861152649
Epoch: 5001, Batch Gradient Norm: 12.378263913754386
Epoch: 5001, Batch Gradient Norm after: 12.378263913754386
Epoch 5002/10000, Prediction Accuracy = 62.842%, Loss = 0.35539458990097045
Epoch: 5002, Batch Gradient Norm: 14.045143873201466
Epoch: 5002, Batch Gradient Norm after: 14.045143873201466
Epoch 5003/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.35895193219184873
Epoch: 5003, Batch Gradient Norm: 13.579319966692212
Epoch: 5003, Batch Gradient Norm after: 13.579319966692212
Epoch 5004/10000, Prediction Accuracy = 62.988%, Loss = 0.35626575350761414
Epoch: 5004, Batch Gradient Norm: 12.756237864462975
Epoch: 5004, Batch Gradient Norm after: 12.756237864462975
Epoch 5005/10000, Prediction Accuracy = 62.98%, Loss = 0.35638700127601625
Epoch: 5005, Batch Gradient Norm: 15.299433433861532
Epoch: 5005, Batch Gradient Norm after: 15.299433433861532
Epoch 5006/10000, Prediction Accuracy = 62.878%, Loss = 0.35788780450820923
Epoch: 5006, Batch Gradient Norm: 17.556597334011194
Epoch: 5006, Batch Gradient Norm after: 17.556597334011194
Epoch 5007/10000, Prediction Accuracy = 62.86400000000001%, Loss = 0.36195058822631837
Epoch: 5007, Batch Gradient Norm: 14.636027927895094
Epoch: 5007, Batch Gradient Norm after: 14.636027927895094
Epoch 5008/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3596318304538727
Epoch: 5008, Batch Gradient Norm: 14.616366441276185
Epoch: 5008, Batch Gradient Norm after: 14.616366441276185
Epoch 5009/10000, Prediction Accuracy = 62.888%, Loss = 0.358652675151825
Epoch: 5009, Batch Gradient Norm: 14.291441735643868
Epoch: 5009, Batch Gradient Norm after: 14.291441735643868
Epoch 5010/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.35807780027389524
Epoch: 5010, Batch Gradient Norm: 12.806467609218465
Epoch: 5010, Batch Gradient Norm after: 12.806467609218465
Epoch 5011/10000, Prediction Accuracy = 62.896%, Loss = 0.3573746681213379
Epoch: 5011, Batch Gradient Norm: 13.782369885256728
Epoch: 5011, Batch Gradient Norm after: 13.782369885256728
Epoch 5012/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.35896467566490176
Epoch: 5012, Batch Gradient Norm: 13.400574828021506
Epoch: 5012, Batch Gradient Norm after: 13.400574828021506
Epoch 5013/10000, Prediction Accuracy = 62.848%, Loss = 0.35536582469940187
Epoch: 5013, Batch Gradient Norm: 12.389441574724119
Epoch: 5013, Batch Gradient Norm after: 12.389441574724119
Epoch 5014/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.35467163324356077
Epoch: 5014, Batch Gradient Norm: 10.801546395913778
Epoch: 5014, Batch Gradient Norm after: 10.801546395913778
Epoch 5015/10000, Prediction Accuracy = 62.9%, Loss = 0.353332382440567
Epoch: 5015, Batch Gradient Norm: 10.647141311092645
Epoch: 5015, Batch Gradient Norm after: 10.647141311092645
Epoch 5016/10000, Prediction Accuracy = 62.788%, Loss = 0.35294594764709475
Epoch: 5016, Batch Gradient Norm: 12.501104426323847
Epoch: 5016, Batch Gradient Norm after: 12.501104426323847
Epoch 5017/10000, Prediction Accuracy = 62.864%, Loss = 0.3608550190925598
Epoch: 5017, Batch Gradient Norm: 13.60000751238953
Epoch: 5017, Batch Gradient Norm after: 13.60000751238953
Epoch 5018/10000, Prediction Accuracy = 62.928%, Loss = 0.3577671766281128
Epoch: 5018, Batch Gradient Norm: 13.131005939973374
Epoch: 5018, Batch Gradient Norm after: 13.131005939973374
Epoch 5019/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.35700627565383913
Epoch: 5019, Batch Gradient Norm: 17.832931688858892
Epoch: 5019, Batch Gradient Norm after: 17.65836356759172
Epoch 5020/10000, Prediction Accuracy = 62.95%, Loss = 0.36340853571891785
Epoch: 5020, Batch Gradient Norm: 16.459351131375193
Epoch: 5020, Batch Gradient Norm after: 16.459351131375193
Epoch 5021/10000, Prediction Accuracy = 62.926%, Loss = 0.3588457465171814
Epoch: 5021, Batch Gradient Norm: 13.4635165528923
Epoch: 5021, Batch Gradient Norm after: 13.4635165528923
Epoch 5022/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.3562264919281006
Epoch: 5022, Batch Gradient Norm: 12.800175733288812
Epoch: 5022, Batch Gradient Norm after: 12.800175733288812
Epoch 5023/10000, Prediction Accuracy = 62.85%, Loss = 0.35582517385482787
Epoch: 5023, Batch Gradient Norm: 12.517217470910023
Epoch: 5023, Batch Gradient Norm after: 12.517217470910023
Epoch 5024/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3539475917816162
Epoch: 5024, Batch Gradient Norm: 14.96960996598023
Epoch: 5024, Batch Gradient Norm after: 14.96960996598023
Epoch 5025/10000, Prediction Accuracy = 62.866%, Loss = 0.36024847626686096
Epoch: 5025, Batch Gradient Norm: 11.75824130730912
Epoch: 5025, Batch Gradient Norm after: 11.75824130730912
Epoch 5026/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.352588015794754
Epoch: 5026, Batch Gradient Norm: 12.340651987013304
Epoch: 5026, Batch Gradient Norm after: 12.340651987013304
Epoch 5027/10000, Prediction Accuracy = 62.971999999999994%, Loss = 0.35536378622055054
Epoch: 5027, Batch Gradient Norm: 16.120253336455292
Epoch: 5027, Batch Gradient Norm after: 16.06091263502747
Epoch 5028/10000, Prediction Accuracy = 62.912%, Loss = 0.3600680589675903
Epoch: 5028, Batch Gradient Norm: 16.44364592584176
Epoch: 5028, Batch Gradient Norm after: 16.44364592584176
Epoch 5029/10000, Prediction Accuracy = 62.898%, Loss = 0.3585118234157562
Epoch: 5029, Batch Gradient Norm: 12.242224855109608
Epoch: 5029, Batch Gradient Norm after: 12.242224855109608
Epoch 5030/10000, Prediction Accuracy = 62.85799999999999%, Loss = 0.3562135577201843
Epoch: 5030, Batch Gradient Norm: 12.51949666933946
Epoch: 5030, Batch Gradient Norm after: 12.51949666933946
Epoch 5031/10000, Prediction Accuracy = 62.984%, Loss = 0.3532167375087738
Epoch: 5031, Batch Gradient Norm: 13.560825529921052
Epoch: 5031, Batch Gradient Norm after: 13.560825529921052
Epoch 5032/10000, Prediction Accuracy = 62.867999999999995%, Loss = 0.3549696087837219
Epoch: 5032, Batch Gradient Norm: 13.530085714277321
Epoch: 5032, Batch Gradient Norm after: 13.530085714277321
Epoch 5033/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.35462731719017027
Epoch: 5033, Batch Gradient Norm: 12.416141848064962
Epoch: 5033, Batch Gradient Norm after: 12.416141848064962
Epoch 5034/10000, Prediction Accuracy = 62.9%, Loss = 0.35524786710739137
Epoch: 5034, Batch Gradient Norm: 13.20446517979023
Epoch: 5034, Batch Gradient Norm after: 13.20446517979023
Epoch 5035/10000, Prediction Accuracy = 63.052%, Loss = 0.3529368221759796
Epoch: 5035, Batch Gradient Norm: 12.965520705272217
Epoch: 5035, Batch Gradient Norm after: 12.965520705272217
Epoch 5036/10000, Prediction Accuracy = 62.92999999999999%, Loss = 0.35445343852043154
Epoch: 5036, Batch Gradient Norm: 12.918768636033233
Epoch: 5036, Batch Gradient Norm after: 12.918768636033233
Epoch 5037/10000, Prediction Accuracy = 62.952%, Loss = 0.3540982484817505
Epoch: 5037, Batch Gradient Norm: 11.25789095117779
Epoch: 5037, Batch Gradient Norm after: 11.25789095117779
Epoch 5038/10000, Prediction Accuracy = 63.028%, Loss = 0.3534880459308624
Epoch: 5038, Batch Gradient Norm: 9.476940300462095
Epoch: 5038, Batch Gradient Norm after: 9.476940300462095
Epoch 5039/10000, Prediction Accuracy = 62.88000000000001%, Loss = 0.34977670311927794
Epoch: 5039, Batch Gradient Norm: 11.471414722553487
Epoch: 5039, Batch Gradient Norm after: 11.471414722553487
Epoch 5040/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.35420915484428406
Epoch: 5040, Batch Gradient Norm: 12.273401183704927
Epoch: 5040, Batch Gradient Norm after: 12.273401183704927
Epoch 5041/10000, Prediction Accuracy = 62.906000000000006%, Loss = 0.35244112014770507
Epoch: 5041, Batch Gradient Norm: 14.580646878903087
Epoch: 5041, Batch Gradient Norm after: 14.580646878903087
Epoch 5042/10000, Prediction Accuracy = 62.96200000000001%, Loss = 0.3594684362411499
Epoch: 5042, Batch Gradient Norm: 12.95186483609034
Epoch: 5042, Batch Gradient Norm after: 12.95186483609034
Epoch 5043/10000, Prediction Accuracy = 62.948%, Loss = 0.35321609377861024
Epoch: 5043, Batch Gradient Norm: 12.978991765741574
Epoch: 5043, Batch Gradient Norm after: 12.978991765741574
Epoch 5044/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3538232326507568
Epoch: 5044, Batch Gradient Norm: 11.941347695494633
Epoch: 5044, Batch Gradient Norm after: 11.941347695494633
Epoch 5045/10000, Prediction Accuracy = 63.016000000000005%, Loss = 0.35225236415863037
Epoch: 5045, Batch Gradient Norm: 10.422959776033103
Epoch: 5045, Batch Gradient Norm after: 10.422959776033103
Epoch 5046/10000, Prediction Accuracy = 62.806%, Loss = 0.35182400345802306
Epoch: 5046, Batch Gradient Norm: 11.40788806782423
Epoch: 5046, Batch Gradient Norm after: 11.40788806782423
Epoch 5047/10000, Prediction Accuracy = 62.936%, Loss = 0.35195119976997374
Epoch: 5047, Batch Gradient Norm: 10.123285146101153
Epoch: 5047, Batch Gradient Norm after: 10.123285146101153
Epoch 5048/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3515589237213135
Epoch: 5048, Batch Gradient Norm: 11.845979875271635
Epoch: 5048, Batch Gradient Norm after: 11.845979875271635
Epoch 5049/10000, Prediction Accuracy = 62.89%, Loss = 0.3533743619918823
Epoch: 5049, Batch Gradient Norm: 14.597407026722479
Epoch: 5049, Batch Gradient Norm after: 14.597407026722479
Epoch 5050/10000, Prediction Accuracy = 62.878%, Loss = 0.35715368390083313
Epoch: 5050, Batch Gradient Norm: 15.340586981532468
Epoch: 5050, Batch Gradient Norm after: 15.340586981532468
Epoch 5051/10000, Prediction Accuracy = 62.838%, Loss = 0.3565567910671234
Epoch: 5051, Batch Gradient Norm: 10.643704290616574
Epoch: 5051, Batch Gradient Norm after: 10.643704290616574
Epoch 5052/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.35098171830177305
Epoch: 5052, Batch Gradient Norm: 10.483343062109546
Epoch: 5052, Batch Gradient Norm after: 10.483343062109546
Epoch 5053/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.35065804719924926
Epoch: 5053, Batch Gradient Norm: 10.753227498986547
Epoch: 5053, Batch Gradient Norm after: 10.753227498986547
Epoch 5054/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.35284733176231386
Epoch: 5054, Batch Gradient Norm: 11.171694270597557
Epoch: 5054, Batch Gradient Norm after: 11.171694270597557
Epoch 5055/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3506262958049774
Epoch: 5055, Batch Gradient Norm: 15.140309046096588
Epoch: 5055, Batch Gradient Norm after: 15.140309046096588
Epoch 5056/10000, Prediction Accuracy = 63.032%, Loss = 0.3568093001842499
Epoch: 5056, Batch Gradient Norm: 17.39444282210889
Epoch: 5056, Batch Gradient Norm after: 17.39444282210889
Epoch 5057/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.3603822410106659
Epoch: 5057, Batch Gradient Norm: 19.845998898476175
Epoch: 5057, Batch Gradient Norm after: 17.928868229063177
Epoch 5058/10000, Prediction Accuracy = 62.984%, Loss = 0.36585999727249147
Epoch: 5058, Batch Gradient Norm: 20.68402046053667
Epoch: 5058, Batch Gradient Norm after: 20.322380346312755
Epoch 5059/10000, Prediction Accuracy = 62.908%, Loss = 0.3682811975479126
Epoch: 5059, Batch Gradient Norm: 20.234028206095633
Epoch: 5059, Batch Gradient Norm after: 18.145139171972588
Epoch 5060/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.36457656025886537
Epoch: 5060, Batch Gradient Norm: 18.8137140993894
Epoch: 5060, Batch Gradient Norm after: 18.8137140993894
Epoch 5061/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3622057497501373
Epoch: 5061, Batch Gradient Norm: 15.817677607223965
Epoch: 5061, Batch Gradient Norm after: 15.817677607223965
Epoch 5062/10000, Prediction Accuracy = 62.876%, Loss = 0.35765625834465026
Epoch: 5062, Batch Gradient Norm: 15.42808915285413
Epoch: 5062, Batch Gradient Norm after: 15.42808915285413
Epoch 5063/10000, Prediction Accuracy = 63.06%, Loss = 0.35527609586715697
Epoch: 5063, Batch Gradient Norm: 14.746307165599646
Epoch: 5063, Batch Gradient Norm after: 14.746307165599646
Epoch 5064/10000, Prediction Accuracy = 63.004000000000005%, Loss = 0.35638176202774047
Epoch: 5064, Batch Gradient Norm: 13.885020630333305
Epoch: 5064, Batch Gradient Norm after: 13.885020630333305
Epoch 5065/10000, Prediction Accuracy = 62.988%, Loss = 0.35546120405197146
Epoch: 5065, Batch Gradient Norm: 12.554681824075391
Epoch: 5065, Batch Gradient Norm after: 12.554681824075391
Epoch 5066/10000, Prediction Accuracy = 62.992%, Loss = 0.3553277015686035
Epoch: 5066, Batch Gradient Norm: 15.448944356301917
Epoch: 5066, Batch Gradient Norm after: 15.448944356301917
Epoch 5067/10000, Prediction Accuracy = 63.007999999999996%, Loss = 0.3572918355464935
Epoch: 5067, Batch Gradient Norm: 14.190853357239764
Epoch: 5067, Batch Gradient Norm after: 14.190853357239764
Epoch 5068/10000, Prediction Accuracy = 62.992000000000004%, Loss = 0.35322771668434144
Epoch: 5068, Batch Gradient Norm: 12.354857837011458
Epoch: 5068, Batch Gradient Norm after: 12.354857837011458
Epoch 5069/10000, Prediction Accuracy = 62.786%, Loss = 0.3552480280399323
Epoch: 5069, Batch Gradient Norm: 10.428731559730672
Epoch: 5069, Batch Gradient Norm after: 10.428731559730672
Epoch 5070/10000, Prediction Accuracy = 62.86%, Loss = 0.35320021510124205
Epoch: 5070, Batch Gradient Norm: 13.028265884117255
Epoch: 5070, Batch Gradient Norm after: 13.028265884117255
Epoch 5071/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.35415477156639097
Epoch: 5071, Batch Gradient Norm: 12.845837427909748
Epoch: 5071, Batch Gradient Norm after: 12.845837427909748
Epoch 5072/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3535391569137573
Epoch: 5072, Batch Gradient Norm: 11.090051866897259
Epoch: 5072, Batch Gradient Norm after: 11.090051866897259
Epoch 5073/10000, Prediction Accuracy = 63.00999999999999%, Loss = 0.35183905363082885
Epoch: 5073, Batch Gradient Norm: 12.61452484657119
Epoch: 5073, Batch Gradient Norm after: 12.61452484657119
Epoch 5074/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.3521016001701355
Epoch: 5074, Batch Gradient Norm: 12.428503393260184
Epoch: 5074, Batch Gradient Norm after: 12.428503393260184
Epoch 5075/10000, Prediction Accuracy = 62.826%, Loss = 0.3542462944984436
Epoch: 5075, Batch Gradient Norm: 13.3003099216089
Epoch: 5075, Batch Gradient Norm after: 13.3003099216089
Epoch 5076/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.352432918548584
Epoch: 5076, Batch Gradient Norm: 11.504599750282026
Epoch: 5076, Batch Gradient Norm after: 11.504599750282026
Epoch 5077/10000, Prediction Accuracy = 62.928%, Loss = 0.3507968783378601
Epoch: 5077, Batch Gradient Norm: 13.20554099519522
Epoch: 5077, Batch Gradient Norm after: 13.20554099519522
Epoch 5078/10000, Prediction Accuracy = 62.95%, Loss = 0.35323222279548644
Epoch: 5078, Batch Gradient Norm: 13.318142746829574
Epoch: 5078, Batch Gradient Norm after: 13.318142746829574
Epoch 5079/10000, Prediction Accuracy = 62.928%, Loss = 0.3547807216644287
Epoch: 5079, Batch Gradient Norm: 13.548988516279689
Epoch: 5079, Batch Gradient Norm after: 13.548988516279689
Epoch 5080/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.3545045077800751
Epoch: 5080, Batch Gradient Norm: 13.201050918056366
Epoch: 5080, Batch Gradient Norm after: 13.201050918056366
Epoch 5081/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.354286789894104
Epoch: 5081, Batch Gradient Norm: 11.620467106213248
Epoch: 5081, Batch Gradient Norm after: 11.620467106213248
Epoch 5082/10000, Prediction Accuracy = 62.962%, Loss = 0.35208823084831237
Epoch: 5082, Batch Gradient Norm: 11.728409108150348
Epoch: 5082, Batch Gradient Norm after: 11.728409108150348
Epoch 5083/10000, Prediction Accuracy = 62.886%, Loss = 0.3563310444355011
Epoch: 5083, Batch Gradient Norm: 11.448268334210347
Epoch: 5083, Batch Gradient Norm after: 11.448268334210347
Epoch 5084/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.35062445402145387
Epoch: 5084, Batch Gradient Norm: 11.87607386838675
Epoch: 5084, Batch Gradient Norm after: 11.87607386838675
Epoch 5085/10000, Prediction Accuracy = 62.936%, Loss = 0.35265780687332154
Epoch: 5085, Batch Gradient Norm: 12.510984104254005
Epoch: 5085, Batch Gradient Norm after: 12.510984104254005
Epoch 5086/10000, Prediction Accuracy = 62.952%, Loss = 0.3542879939079285
Epoch: 5086, Batch Gradient Norm: 12.870880704032396
Epoch: 5086, Batch Gradient Norm after: 12.870880704032396
Epoch 5087/10000, Prediction Accuracy = 62.943999999999996%, Loss = 0.35441851019859316
Epoch: 5087, Batch Gradient Norm: 12.906089857016301
Epoch: 5087, Batch Gradient Norm after: 12.906089857016301
Epoch 5088/10000, Prediction Accuracy = 62.94%, Loss = 0.35287309288978574
Epoch: 5088, Batch Gradient Norm: 12.370395912969292
Epoch: 5088, Batch Gradient Norm after: 12.370395912969292
Epoch 5089/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.35382124185562136
Epoch: 5089, Batch Gradient Norm: 11.204751981202577
Epoch: 5089, Batch Gradient Norm after: 11.204751981202577
Epoch 5090/10000, Prediction Accuracy = 62.824%, Loss = 0.3516660213470459
Epoch: 5090, Batch Gradient Norm: 13.545502801261266
Epoch: 5090, Batch Gradient Norm after: 13.545502801261266
Epoch 5091/10000, Prediction Accuracy = 63.03399999999999%, Loss = 0.3547901511192322
Epoch: 5091, Batch Gradient Norm: 13.539191497850876
Epoch: 5091, Batch Gradient Norm after: 13.539191497850876
Epoch 5092/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.35269143581390383
Epoch: 5092, Batch Gradient Norm: 12.41534141623861
Epoch: 5092, Batch Gradient Norm after: 12.41534141623861
Epoch 5093/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3532393157482147
Epoch: 5093, Batch Gradient Norm: 13.86804733782375
Epoch: 5093, Batch Gradient Norm after: 13.86804733782375
Epoch 5094/10000, Prediction Accuracy = 62.89%, Loss = 0.35648916363716127
Epoch: 5094, Batch Gradient Norm: 13.865254576617293
Epoch: 5094, Batch Gradient Norm after: 13.865254576617293
Epoch 5095/10000, Prediction Accuracy = 62.962%, Loss = 0.35397266745567324
Epoch: 5095, Batch Gradient Norm: 15.375791716610532
Epoch: 5095, Batch Gradient Norm after: 15.375791716610532
Epoch 5096/10000, Prediction Accuracy = 62.992%, Loss = 0.35642189979553224
Epoch: 5096, Batch Gradient Norm: 14.529486880784242
Epoch: 5096, Batch Gradient Norm after: 14.529486880784242
Epoch 5097/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3542564272880554
Epoch: 5097, Batch Gradient Norm: 13.410573922278873
Epoch: 5097, Batch Gradient Norm after: 13.410573922278873
Epoch 5098/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3536775290966034
Epoch: 5098, Batch Gradient Norm: 13.808472192705185
Epoch: 5098, Batch Gradient Norm after: 13.808472192705185
Epoch 5099/10000, Prediction Accuracy = 63.068%, Loss = 0.3523290157318115
Epoch: 5099, Batch Gradient Norm: 16.83314508626923
Epoch: 5099, Batch Gradient Norm after: 16.83314508626923
Epoch 5100/10000, Prediction Accuracy = 62.968%, Loss = 0.36048274040222167
Epoch: 5100, Batch Gradient Norm: 13.320356381095715
Epoch: 5100, Batch Gradient Norm after: 13.320356381095715
Epoch 5101/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.3550961434841156
Epoch: 5101, Batch Gradient Norm: 13.26663493442219
Epoch: 5101, Batch Gradient Norm after: 13.26663493442219
Epoch 5102/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.35451499819755555
Epoch: 5102, Batch Gradient Norm: 10.92027244910812
Epoch: 5102, Batch Gradient Norm after: 10.92027244910812
Epoch 5103/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.34899708032608034
Epoch: 5103, Batch Gradient Norm: 12.268250158954757
Epoch: 5103, Batch Gradient Norm after: 12.268250158954757
Epoch 5104/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.35053293108940126
Epoch: 5104, Batch Gradient Norm: 13.832573070621528
Epoch: 5104, Batch Gradient Norm after: 13.832573070621528
Epoch 5105/10000, Prediction Accuracy = 62.99399999999999%, Loss = 0.35380292534828184
Epoch: 5105, Batch Gradient Norm: 13.889926721106558
Epoch: 5105, Batch Gradient Norm after: 13.889926721106558
Epoch 5106/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.354484897851944
Epoch: 5106, Batch Gradient Norm: 13.020419404850912
Epoch: 5106, Batch Gradient Norm after: 13.020419404850912
Epoch 5107/10000, Prediction Accuracy = 62.98%, Loss = 0.3512276649475098
Epoch: 5107, Batch Gradient Norm: 14.122624318654456
Epoch: 5107, Batch Gradient Norm after: 14.122624318654456
Epoch 5108/10000, Prediction Accuracy = 62.906000000000006%, Loss = 0.3532895267009735
Epoch: 5108, Batch Gradient Norm: 16.009309508112395
Epoch: 5108, Batch Gradient Norm after: 16.009309508112395
Epoch 5109/10000, Prediction Accuracy = 62.89%, Loss = 0.3576634585857391
Epoch: 5109, Batch Gradient Norm: 13.168593808887419
Epoch: 5109, Batch Gradient Norm after: 13.168593808887419
Epoch 5110/10000, Prediction Accuracy = 62.972%, Loss = 0.35405611991882324
Epoch: 5110, Batch Gradient Norm: 11.254645567866216
Epoch: 5110, Batch Gradient Norm after: 11.254645567866216
Epoch 5111/10000, Prediction Accuracy = 62.936%, Loss = 0.3520002007484436
Epoch: 5111, Batch Gradient Norm: 11.429947973604476
Epoch: 5111, Batch Gradient Norm after: 11.429947973604476
Epoch 5112/10000, Prediction Accuracy = 63.0%, Loss = 0.3504415571689606
Epoch: 5112, Batch Gradient Norm: 11.418208685963936
Epoch: 5112, Batch Gradient Norm after: 11.418208685963936
Epoch 5113/10000, Prediction Accuracy = 63.05%, Loss = 0.3522798538208008
Epoch: 5113, Batch Gradient Norm: 13.295299491293253
Epoch: 5113, Batch Gradient Norm after: 13.295299491293253
Epoch 5114/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.3538402259349823
Epoch: 5114, Batch Gradient Norm: 13.245019786808479
Epoch: 5114, Batch Gradient Norm after: 13.245019786808479
Epoch 5115/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3546090483665466
Epoch: 5115, Batch Gradient Norm: 12.520136907026293
Epoch: 5115, Batch Gradient Norm after: 12.520136907026293
Epoch 5116/10000, Prediction Accuracy = 62.94200000000001%, Loss = 0.3522008001804352
Epoch: 5116, Batch Gradient Norm: 14.380566389065585
Epoch: 5116, Batch Gradient Norm after: 14.380566389065585
Epoch 5117/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.35447105169296267
Epoch: 5117, Batch Gradient Norm: 13.140595223050266
Epoch: 5117, Batch Gradient Norm after: 13.140595223050266
Epoch 5118/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.35169353485107424
Epoch: 5118, Batch Gradient Norm: 13.461751337917939
Epoch: 5118, Batch Gradient Norm after: 13.461751337917939
Epoch 5119/10000, Prediction Accuracy = 62.944%, Loss = 0.35295950770378115
Epoch: 5119, Batch Gradient Norm: 13.385837790647823
Epoch: 5119, Batch Gradient Norm after: 13.385837790647823
Epoch 5120/10000, Prediction Accuracy = 63.04%, Loss = 0.35483107566833494
Epoch: 5120, Batch Gradient Norm: 13.888912144324838
Epoch: 5120, Batch Gradient Norm after: 13.888912144324838
Epoch 5121/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.35357418060302737
Epoch: 5121, Batch Gradient Norm: 13.98818969776456
Epoch: 5121, Batch Gradient Norm after: 13.98818969776456
Epoch 5122/10000, Prediction Accuracy = 62.91600000000001%, Loss = 0.3521653115749359
Epoch: 5122, Batch Gradient Norm: 13.22853053126827
Epoch: 5122, Batch Gradient Norm after: 13.22853053126827
Epoch 5123/10000, Prediction Accuracy = 63.004%, Loss = 0.3553514063358307
Epoch: 5123, Batch Gradient Norm: 12.380925226987717
Epoch: 5123, Batch Gradient Norm after: 12.380925226987717
Epoch 5124/10000, Prediction Accuracy = 63.036%, Loss = 0.3533782482147217
Epoch: 5124, Batch Gradient Norm: 12.301297570624424
Epoch: 5124, Batch Gradient Norm after: 12.301297570624424
Epoch 5125/10000, Prediction Accuracy = 62.912%, Loss = 0.35234647393226626
Epoch: 5125, Batch Gradient Norm: 12.559749401443858
Epoch: 5125, Batch Gradient Norm after: 12.559749401443858
Epoch 5126/10000, Prediction Accuracy = 63.048%, Loss = 0.35017778277397155
Epoch: 5126, Batch Gradient Norm: 14.049891822008597
Epoch: 5126, Batch Gradient Norm after: 14.049891822008597
Epoch 5127/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3538320899009705
Epoch: 5127, Batch Gradient Norm: 16.421645604048916
Epoch: 5127, Batch Gradient Norm after: 16.421645604048916
Epoch 5128/10000, Prediction Accuracy = 62.914%, Loss = 0.3579184651374817
Epoch: 5128, Batch Gradient Norm: 16.10745666541648
Epoch: 5128, Batch Gradient Norm after: 16.10745666541648
Epoch 5129/10000, Prediction Accuracy = 62.95%, Loss = 0.355252605676651
Epoch: 5129, Batch Gradient Norm: 18.363055475108553
Epoch: 5129, Batch Gradient Norm after: 18.363055475108553
Epoch 5130/10000, Prediction Accuracy = 62.992000000000004%, Loss = 0.3624325156211853
Epoch: 5130, Batch Gradient Norm: 14.41545752988899
Epoch: 5130, Batch Gradient Norm after: 14.41545752988899
Epoch 5131/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3547970294952393
Epoch: 5131, Batch Gradient Norm: 14.442456861382492
Epoch: 5131, Batch Gradient Norm after: 14.442456861382492
Epoch 5132/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.35416216850280763
Epoch: 5132, Batch Gradient Norm: 12.568953749781794
Epoch: 5132, Batch Gradient Norm after: 12.568953749781794
Epoch 5133/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.3511644244194031
Epoch: 5133, Batch Gradient Norm: 14.724453561153505
Epoch: 5133, Batch Gradient Norm after: 14.724453561153505
Epoch 5134/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.35385231375694276
Epoch: 5134, Batch Gradient Norm: 14.000279431183522
Epoch: 5134, Batch Gradient Norm after: 14.000279431183522
Epoch 5135/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.35411731600761415
Epoch: 5135, Batch Gradient Norm: 13.10812995687219
Epoch: 5135, Batch Gradient Norm after: 13.10812995687219
Epoch 5136/10000, Prediction Accuracy = 63.016000000000005%, Loss = 0.35302805304527285
Epoch: 5136, Batch Gradient Norm: 12.44925861773748
Epoch: 5136, Batch Gradient Norm after: 12.44925861773748
Epoch 5137/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.35069891810417175
Epoch: 5137, Batch Gradient Norm: 12.489956427425135
Epoch: 5137, Batch Gradient Norm after: 12.489956427425135
Epoch 5138/10000, Prediction Accuracy = 62.95%, Loss = 0.35309417843818663
Epoch: 5138, Batch Gradient Norm: 14.981955655523139
Epoch: 5138, Batch Gradient Norm after: 14.981955655523139
Epoch 5139/10000, Prediction Accuracy = 62.984%, Loss = 0.3524734199047089
Epoch: 5139, Batch Gradient Norm: 11.723564927564256
Epoch: 5139, Batch Gradient Norm after: 11.723564927564256
Epoch 5140/10000, Prediction Accuracy = 62.996%, Loss = 0.3529939830303192
Epoch: 5140, Batch Gradient Norm: 12.046666430269237
Epoch: 5140, Batch Gradient Norm after: 12.046666430269237
Epoch 5141/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.3526393949985504
Epoch: 5141, Batch Gradient Norm: 13.809338434496926
Epoch: 5141, Batch Gradient Norm after: 13.809338434496926
Epoch 5142/10000, Prediction Accuracy = 63.02199999999999%, Loss = 0.35336769223213194
Epoch: 5142, Batch Gradient Norm: 15.845777884357593
Epoch: 5142, Batch Gradient Norm after: 15.845777884357593
Epoch 5143/10000, Prediction Accuracy = 62.95%, Loss = 0.3555411219596863
Epoch: 5143, Batch Gradient Norm: 13.813414133125374
Epoch: 5143, Batch Gradient Norm after: 13.813414133125374
Epoch 5144/10000, Prediction Accuracy = 63.024%, Loss = 0.35393351316452026
Epoch: 5144, Batch Gradient Norm: 11.013735927151606
Epoch: 5144, Batch Gradient Norm after: 11.013735927151606
Epoch 5145/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.34951211810112
Epoch: 5145, Batch Gradient Norm: 13.017650596524739
Epoch: 5145, Batch Gradient Norm after: 13.017650596524739
Epoch 5146/10000, Prediction Accuracy = 63.08399999999999%, Loss = 0.3512616574764252
Epoch: 5146, Batch Gradient Norm: 12.074261808743525
Epoch: 5146, Batch Gradient Norm after: 12.074261808743525
Epoch 5147/10000, Prediction Accuracy = 62.928%, Loss = 0.35202054381370546
Epoch: 5147, Batch Gradient Norm: 15.443419304741127
Epoch: 5147, Batch Gradient Norm after: 15.443419304741127
Epoch 5148/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.35654409527778624
Epoch: 5148, Batch Gradient Norm: 19.807575946943352
Epoch: 5148, Batch Gradient Norm after: 19.097131968549867
Epoch 5149/10000, Prediction Accuracy = 62.914%, Loss = 0.3644213259220123
Epoch: 5149, Batch Gradient Norm: 20.578988179248505
Epoch: 5149, Batch Gradient Norm after: 19.00179909892046
Epoch 5150/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.367531555891037
Epoch: 5150, Batch Gradient Norm: 20.238698159258774
Epoch: 5150, Batch Gradient Norm after: 20.157455014211813
Epoch 5151/10000, Prediction Accuracy = 63.004%, Loss = 0.3644766926765442
Epoch: 5151, Batch Gradient Norm: 17.451948898531548
Epoch: 5151, Batch Gradient Norm after: 17.451948898531548
Epoch 5152/10000, Prediction Accuracy = 62.908%, Loss = 0.3609391272068024
Epoch: 5152, Batch Gradient Norm: 16.26001574341013
Epoch: 5152, Batch Gradient Norm after: 16.26001574341013
Epoch 5153/10000, Prediction Accuracy = 62.938%, Loss = 0.35799855589866636
Epoch: 5153, Batch Gradient Norm: 15.261373292530886
Epoch: 5153, Batch Gradient Norm after: 15.261373292530886
Epoch 5154/10000, Prediction Accuracy = 62.94%, Loss = 0.3565202236175537
Epoch: 5154, Batch Gradient Norm: 16.16651417515692
Epoch: 5154, Batch Gradient Norm after: 16.16651417515692
Epoch 5155/10000, Prediction Accuracy = 62.936%, Loss = 0.35588021874427794
Epoch: 5155, Batch Gradient Norm: 14.55332016448767
Epoch: 5155, Batch Gradient Norm after: 14.55332016448767
Epoch 5156/10000, Prediction Accuracy = 63.0%, Loss = 0.35330691933631897
Epoch: 5156, Batch Gradient Norm: 13.071044077989258
Epoch: 5156, Batch Gradient Norm after: 13.071044077989258
Epoch 5157/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.35449341535568235
Epoch: 5157, Batch Gradient Norm: 13.48788679896607
Epoch: 5157, Batch Gradient Norm after: 13.48788679896607
Epoch 5158/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.35250899791717527
Epoch: 5158, Batch Gradient Norm: 12.708518675932835
Epoch: 5158, Batch Gradient Norm after: 12.708518675932835
Epoch 5159/10000, Prediction Accuracy = 63.1%, Loss = 0.35137826204299927
Epoch: 5159, Batch Gradient Norm: 11.935604455364153
Epoch: 5159, Batch Gradient Norm after: 11.935604455364153
Epoch 5160/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.35093953013420104
Epoch: 5160, Batch Gradient Norm: 12.45765882402375
Epoch: 5160, Batch Gradient Norm after: 12.45765882402375
Epoch 5161/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.35161514282226564
Epoch: 5161, Batch Gradient Norm: 12.771340321690538
Epoch: 5161, Batch Gradient Norm after: 12.771340321690538
Epoch 5162/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.35195412635803225
Epoch: 5162, Batch Gradient Norm: 13.473001890959788
Epoch: 5162, Batch Gradient Norm after: 13.473001890959788
Epoch 5163/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.35309996604919436
Epoch: 5163, Batch Gradient Norm: 14.884466624306862
Epoch: 5163, Batch Gradient Norm after: 14.884466624306862
Epoch 5164/10000, Prediction Accuracy = 63.128%, Loss = 0.3546599090099335
Epoch: 5164, Batch Gradient Norm: 14.75437751939793
Epoch: 5164, Batch Gradient Norm after: 14.75437751939793
Epoch 5165/10000, Prediction Accuracy = 62.984%, Loss = 0.35409330725669863
Epoch: 5165, Batch Gradient Norm: 12.33596956836972
Epoch: 5165, Batch Gradient Norm after: 12.33596956836972
Epoch 5166/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.35129453539848327
Epoch: 5166, Batch Gradient Norm: 12.437869750828467
Epoch: 5166, Batch Gradient Norm after: 12.437869750828467
Epoch 5167/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.3515497922897339
Epoch: 5167, Batch Gradient Norm: 10.329393674808385
Epoch: 5167, Batch Gradient Norm after: 10.329393674808385
Epoch 5168/10000, Prediction Accuracy = 62.932%, Loss = 0.351037985086441
Epoch: 5168, Batch Gradient Norm: 11.71373873949752
Epoch: 5168, Batch Gradient Norm after: 11.71373873949752
Epoch 5169/10000, Prediction Accuracy = 63.104000000000006%, Loss = 0.35094602704048156
Epoch: 5169, Batch Gradient Norm: 8.690234446661078
Epoch: 5169, Batch Gradient Norm after: 8.690234446661078
Epoch 5170/10000, Prediction Accuracy = 62.948%, Loss = 0.3465203523635864
Epoch: 5170, Batch Gradient Norm: 9.21454072405696
Epoch: 5170, Batch Gradient Norm after: 9.21454072405696
Epoch 5171/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.34605648517608645
Epoch: 5171, Batch Gradient Norm: 13.640544779396931
Epoch: 5171, Batch Gradient Norm after: 13.640544779396931
Epoch 5172/10000, Prediction Accuracy = 62.93000000000001%, Loss = 0.35313248038291933
Epoch: 5172, Batch Gradient Norm: 13.352863371953358
Epoch: 5172, Batch Gradient Norm after: 13.352863371953358
Epoch 5173/10000, Prediction Accuracy = 63.008%, Loss = 0.35203106999397277
Epoch: 5173, Batch Gradient Norm: 11.249806078259843
Epoch: 5173, Batch Gradient Norm after: 11.249806078259843
Epoch 5174/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3481154918670654
Epoch: 5174, Batch Gradient Norm: 11.020520682370973
Epoch: 5174, Batch Gradient Norm after: 11.020520682370973
Epoch 5175/10000, Prediction Accuracy = 62.928%, Loss = 0.346856015920639
Epoch: 5175, Batch Gradient Norm: 13.595757158693404
Epoch: 5175, Batch Gradient Norm after: 13.595757158693404
Epoch 5176/10000, Prediction Accuracy = 63.074%, Loss = 0.3529146134853363
Epoch: 5176, Batch Gradient Norm: 13.174616752879201
Epoch: 5176, Batch Gradient Norm after: 13.174616752879201
Epoch 5177/10000, Prediction Accuracy = 63.004%, Loss = 0.35090423822402955
Epoch: 5177, Batch Gradient Norm: 11.138972015697787
Epoch: 5177, Batch Gradient Norm after: 11.138972015697787
Epoch 5178/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.34746927618980405
Epoch: 5178, Batch Gradient Norm: 13.082659895222148
Epoch: 5178, Batch Gradient Norm after: 13.082659895222148
Epoch 5179/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.35077418088912965
Epoch: 5179, Batch Gradient Norm: 15.070713174043112
Epoch: 5179, Batch Gradient Norm after: 15.070713174043112
Epoch 5180/10000, Prediction Accuracy = 63.1%, Loss = 0.3548291981220245
Epoch: 5180, Batch Gradient Norm: 14.024934442351928
Epoch: 5180, Batch Gradient Norm after: 14.024934442351928
Epoch 5181/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.35254337787628176
Epoch: 5181, Batch Gradient Norm: 13.700016086448837
Epoch: 5181, Batch Gradient Norm after: 13.700016086448837
Epoch 5182/10000, Prediction Accuracy = 63.05%, Loss = 0.3517531335353851
Epoch: 5182, Batch Gradient Norm: 14.919677132288877
Epoch: 5182, Batch Gradient Norm after: 14.919677132288877
Epoch 5183/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.35191087126731874
Epoch: 5183, Batch Gradient Norm: 16.599166827054656
Epoch: 5183, Batch Gradient Norm after: 16.445446722551235
Epoch 5184/10000, Prediction Accuracy = 62.888%, Loss = 0.35714573264122007
Epoch: 5184, Batch Gradient Norm: 14.166768515687933
Epoch: 5184, Batch Gradient Norm after: 14.166768515687933
Epoch 5185/10000, Prediction Accuracy = 63.138%, Loss = 0.35383111238479614
Epoch: 5185, Batch Gradient Norm: 13.999893280334941
Epoch: 5185, Batch Gradient Norm after: 13.999893280334941
Epoch 5186/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3529912352561951
Epoch: 5186, Batch Gradient Norm: 11.92847114838094
Epoch: 5186, Batch Gradient Norm after: 11.92847114838094
Epoch 5187/10000, Prediction Accuracy = 63.004%, Loss = 0.34878122210502627
Epoch: 5187, Batch Gradient Norm: 12.242402051809865
Epoch: 5187, Batch Gradient Norm after: 12.242402051809865
Epoch 5188/10000, Prediction Accuracy = 63.03000000000001%, Loss = 0.34812151193618773
Epoch: 5188, Batch Gradient Norm: 15.674118276673243
Epoch: 5188, Batch Gradient Norm after: 15.674118276673243
Epoch 5189/10000, Prediction Accuracy = 63.024%, Loss = 0.3534960687160492
Epoch: 5189, Batch Gradient Norm: 13.395324250499154
Epoch: 5189, Batch Gradient Norm after: 13.395324250499154
Epoch 5190/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.3498195469379425
Epoch: 5190, Batch Gradient Norm: 14.847414364856881
Epoch: 5190, Batch Gradient Norm after: 14.847414364856881
Epoch 5191/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.35383187532424926
Epoch: 5191, Batch Gradient Norm: 11.443100070296984
Epoch: 5191, Batch Gradient Norm after: 11.443100070296984
Epoch 5192/10000, Prediction Accuracy = 63.024%, Loss = 0.34900912642478943
Epoch: 5192, Batch Gradient Norm: 10.852458683005718
Epoch: 5192, Batch Gradient Norm after: 10.852458683005718
Epoch 5193/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.34977816343307494
Epoch: 5193, Batch Gradient Norm: 10.39665967127979
Epoch: 5193, Batch Gradient Norm after: 10.39665967127979
Epoch 5194/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.3494096279144287
Epoch: 5194, Batch Gradient Norm: 12.15947379643886
Epoch: 5194, Batch Gradient Norm after: 12.15947379643886
Epoch 5195/10000, Prediction Accuracy = 63.104%, Loss = 0.3490705132484436
Epoch: 5195, Batch Gradient Norm: 10.224837799362746
Epoch: 5195, Batch Gradient Norm after: 10.224837799362746
Epoch 5196/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.34699041247367857
Epoch: 5196, Batch Gradient Norm: 10.603303829298317
Epoch: 5196, Batch Gradient Norm after: 10.603303829298317
Epoch 5197/10000, Prediction Accuracy = 62.988%, Loss = 0.34684130549430847
Epoch: 5197, Batch Gradient Norm: 13.70278228133787
Epoch: 5197, Batch Gradient Norm after: 13.70278228133787
Epoch 5198/10000, Prediction Accuracy = 63.03399999999999%, Loss = 0.35036706924438477
Epoch: 5198, Batch Gradient Norm: 11.669667303768676
Epoch: 5198, Batch Gradient Norm after: 11.669667303768676
Epoch 5199/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.3482476532459259
Epoch: 5199, Batch Gradient Norm: 11.526721832174331
Epoch: 5199, Batch Gradient Norm after: 11.526721832174331
Epoch 5200/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.347007691860199
Epoch: 5200, Batch Gradient Norm: 12.608024765610711
Epoch: 5200, Batch Gradient Norm after: 12.608024765610711
Epoch 5201/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.3483436584472656
Epoch: 5201, Batch Gradient Norm: 12.610679994903053
Epoch: 5201, Batch Gradient Norm after: 12.610679994903053
Epoch 5202/10000, Prediction Accuracy = 62.96600000000001%, Loss = 0.3509567677974701
Epoch: 5202, Batch Gradient Norm: 10.458669087467763
Epoch: 5202, Batch Gradient Norm after: 10.458669087467763
Epoch 5203/10000, Prediction Accuracy = 62.980000000000004%, Loss = 0.34958612322807314
Epoch: 5203, Batch Gradient Norm: 13.855506722134937
Epoch: 5203, Batch Gradient Norm after: 13.855506722134937
Epoch 5204/10000, Prediction Accuracy = 62.902%, Loss = 0.35561161637306216
Epoch: 5204, Batch Gradient Norm: 13.077133658019509
Epoch: 5204, Batch Gradient Norm after: 13.077133658019509
Epoch 5205/10000, Prediction Accuracy = 62.912%, Loss = 0.3507286846637726
Epoch: 5205, Batch Gradient Norm: 10.505785060681866
Epoch: 5205, Batch Gradient Norm after: 10.505785060681866
Epoch 5206/10000, Prediction Accuracy = 63.117999999999995%, Loss = 0.3482197761535645
Epoch: 5206, Batch Gradient Norm: 11.175504561282292
Epoch: 5206, Batch Gradient Norm after: 11.175504561282292
Epoch 5207/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.348550146818161
Epoch: 5207, Batch Gradient Norm: 10.827443258971481
Epoch: 5207, Batch Gradient Norm after: 10.827443258971481
Epoch 5208/10000, Prediction Accuracy = 63.074%, Loss = 0.3505571007728577
Epoch: 5208, Batch Gradient Norm: 9.240923471115538
Epoch: 5208, Batch Gradient Norm after: 9.240923471115538
Epoch 5209/10000, Prediction Accuracy = 63.08599999999999%, Loss = 0.3477896749973297
Epoch: 5209, Batch Gradient Norm: 11.074254159446628
Epoch: 5209, Batch Gradient Norm after: 11.074254159446628
Epoch 5210/10000, Prediction Accuracy = 62.964%, Loss = 0.3490888297557831
Epoch: 5210, Batch Gradient Norm: 14.806670251586084
Epoch: 5210, Batch Gradient Norm after: 14.806670251586084
Epoch 5211/10000, Prediction Accuracy = 63.0%, Loss = 0.35279688239097595
Epoch: 5211, Batch Gradient Norm: 16.442078731561036
Epoch: 5211, Batch Gradient Norm after: 16.442078731561036
Epoch 5212/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.35390132665634155
Epoch: 5212, Batch Gradient Norm: 16.759467555305523
Epoch: 5212, Batch Gradient Norm after: 16.663862201413064
Epoch 5213/10000, Prediction Accuracy = 62.984%, Loss = 0.35746713280677794
Epoch: 5213, Batch Gradient Norm: 16.79894910666415
Epoch: 5213, Batch Gradient Norm after: 16.79894910666415
Epoch 5214/10000, Prediction Accuracy = 63.13599999999999%, Loss = 0.3550940930843353
Epoch: 5214, Batch Gradient Norm: 17.61018424835576
Epoch: 5214, Batch Gradient Norm after: 17.61018424835576
Epoch 5215/10000, Prediction Accuracy = 63.052%, Loss = 0.35929507613182066
Epoch: 5215, Batch Gradient Norm: 14.853506678981805
Epoch: 5215, Batch Gradient Norm after: 14.853506678981805
Epoch 5216/10000, Prediction Accuracy = 63.032%, Loss = 0.3531247854232788
Epoch: 5216, Batch Gradient Norm: 12.87403672344923
Epoch: 5216, Batch Gradient Norm after: 12.87403672344923
Epoch 5217/10000, Prediction Accuracy = 62.964%, Loss = 0.3494845390319824
Epoch: 5217, Batch Gradient Norm: 11.30084956441799
Epoch: 5217, Batch Gradient Norm after: 11.30084956441799
Epoch 5218/10000, Prediction Accuracy = 62.936%, Loss = 0.3498364627361298
Epoch: 5218, Batch Gradient Norm: 13.823392105511436
Epoch: 5218, Batch Gradient Norm after: 13.823392105511436
Epoch 5219/10000, Prediction Accuracy = 62.996%, Loss = 0.35101394057273866
Epoch: 5219, Batch Gradient Norm: 15.909965557587121
Epoch: 5219, Batch Gradient Norm after: 15.909965557587121
Epoch 5220/10000, Prediction Accuracy = 63.112%, Loss = 0.35267354249954225
Epoch: 5220, Batch Gradient Norm: 15.076812895397019
Epoch: 5220, Batch Gradient Norm after: 15.076812895397019
Epoch 5221/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.35343345403671267
Epoch: 5221, Batch Gradient Norm: 10.350112307661758
Epoch: 5221, Batch Gradient Norm after: 10.350112307661758
Epoch 5222/10000, Prediction Accuracy = 63.074%, Loss = 0.34575300812721255
Epoch: 5222, Batch Gradient Norm: 10.926527406148342
Epoch: 5222, Batch Gradient Norm after: 10.926527406148342
Epoch 5223/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3473052799701691
Epoch: 5223, Batch Gradient Norm: 9.849403902991979
Epoch: 5223, Batch Gradient Norm after: 9.849403902991979
Epoch 5224/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.3446726441383362
Epoch: 5224, Batch Gradient Norm: 10.645817674232113
Epoch: 5224, Batch Gradient Norm after: 10.645817674232113
Epoch 5225/10000, Prediction Accuracy = 63.048%, Loss = 0.3460998833179474
Epoch: 5225, Batch Gradient Norm: 13.160369570371751
Epoch: 5225, Batch Gradient Norm after: 13.160369570371751
Epoch 5226/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.34955921173095705
Epoch: 5226, Batch Gradient Norm: 13.116697220140335
Epoch: 5226, Batch Gradient Norm after: 13.116697220140335
Epoch 5227/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3497989535331726
Epoch: 5227, Batch Gradient Norm: 12.470360627838204
Epoch: 5227, Batch Gradient Norm after: 12.470360627838204
Epoch 5228/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.34937929511070254
Epoch: 5228, Batch Gradient Norm: 14.981902815300264
Epoch: 5228, Batch Gradient Norm after: 14.981902815300264
Epoch 5229/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.352204692363739
Epoch: 5229, Batch Gradient Norm: 15.976525270005446
Epoch: 5229, Batch Gradient Norm after: 15.976525270005446
Epoch 5230/10000, Prediction Accuracy = 62.968%, Loss = 0.3543012857437134
Epoch: 5230, Batch Gradient Norm: 15.964121890826686
Epoch: 5230, Batch Gradient Norm after: 15.964121890826686
Epoch 5231/10000, Prediction Accuracy = 63.136%, Loss = 0.3547270119190216
Epoch: 5231, Batch Gradient Norm: 14.631393658241047
Epoch: 5231, Batch Gradient Norm after: 14.631393658241047
Epoch 5232/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.35172813534736636
Epoch: 5232, Batch Gradient Norm: 15.982921823452429
Epoch: 5232, Batch Gradient Norm after: 15.59850723837127
Epoch 5233/10000, Prediction Accuracy = 63.242%, Loss = 0.351663863658905
Epoch: 5233, Batch Gradient Norm: 16.432965779886977
Epoch: 5233, Batch Gradient Norm after: 16.432965779886977
Epoch 5234/10000, Prediction Accuracy = 63.008%, Loss = 0.3544029474258423
Epoch: 5234, Batch Gradient Norm: 13.53809027668988
Epoch: 5234, Batch Gradient Norm after: 13.53809027668988
Epoch 5235/10000, Prediction Accuracy = 63.11%, Loss = 0.35390353202819824
Epoch: 5235, Batch Gradient Norm: 16.81230391688317
Epoch: 5235, Batch Gradient Norm after: 15.962497599355336
Epoch 5236/10000, Prediction Accuracy = 63.02%, Loss = 0.3528025209903717
Epoch: 5236, Batch Gradient Norm: 19.038343401012987
Epoch: 5236, Batch Gradient Norm after: 19.038343401012987
Epoch 5237/10000, Prediction Accuracy = 62.992000000000004%, Loss = 0.36233500242233274
Epoch: 5237, Batch Gradient Norm: 16.13576872762362
Epoch: 5237, Batch Gradient Norm after: 16.13576872762362
Epoch 5238/10000, Prediction Accuracy = 63.098%, Loss = 0.3514083087444305
Epoch: 5238, Batch Gradient Norm: 14.10347138815677
Epoch: 5238, Batch Gradient Norm after: 14.10347138815677
Epoch 5239/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.3498198390007019
Epoch: 5239, Batch Gradient Norm: 14.363063940069328
Epoch: 5239, Batch Gradient Norm after: 14.363063940069328
Epoch 5240/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.3505968153476715
Epoch: 5240, Batch Gradient Norm: 11.772319547258048
Epoch: 5240, Batch Gradient Norm after: 11.772319547258048
Epoch 5241/10000, Prediction Accuracy = 63.016%, Loss = 0.34724995493888855
Epoch: 5241, Batch Gradient Norm: 11.96611244300937
Epoch: 5241, Batch Gradient Norm after: 11.96611244300937
Epoch 5242/10000, Prediction Accuracy = 63.038%, Loss = 0.3476339399814606
Epoch: 5242, Batch Gradient Norm: 11.984475054387985
Epoch: 5242, Batch Gradient Norm after: 11.984475054387985
Epoch 5243/10000, Prediction Accuracy = 63.072%, Loss = 0.34637336134910585
Epoch: 5243, Batch Gradient Norm: 14.472514709211781
Epoch: 5243, Batch Gradient Norm after: 14.472514709211781
Epoch 5244/10000, Prediction Accuracy = 63.06400000000001%, Loss = 0.35478039979934695
Epoch: 5244, Batch Gradient Norm: 14.587518291993183
Epoch: 5244, Batch Gradient Norm after: 14.587518291993183
Epoch 5245/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.3516546726226807
Epoch: 5245, Batch Gradient Norm: 14.995722177791931
Epoch: 5245, Batch Gradient Norm after: 14.995722177791931
Epoch 5246/10000, Prediction Accuracy = 63.074%, Loss = 0.3528590738773346
Epoch: 5246, Batch Gradient Norm: 13.765824749690447
Epoch: 5246, Batch Gradient Norm after: 13.765824749690447
Epoch 5247/10000, Prediction Accuracy = 63.08%, Loss = 0.3520184874534607
Epoch: 5247, Batch Gradient Norm: 18.50230227685051
Epoch: 5247, Batch Gradient Norm after: 18.50230227685051
Epoch 5248/10000, Prediction Accuracy = 63.056%, Loss = 0.3593963623046875
Epoch: 5248, Batch Gradient Norm: 17.151259208078802
Epoch: 5248, Batch Gradient Norm after: 16.73606475833552
Epoch 5249/10000, Prediction Accuracy = 63.076%, Loss = 0.3583588063716888
Epoch: 5249, Batch Gradient Norm: 16.23384071657373
Epoch: 5249, Batch Gradient Norm after: 16.23384071657373
Epoch 5250/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3524542272090912
Epoch: 5250, Batch Gradient Norm: 15.400138260946473
Epoch: 5250, Batch Gradient Norm after: 15.400138260946473
Epoch 5251/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.35346331596374514
Epoch: 5251, Batch Gradient Norm: 14.714365353230257
Epoch: 5251, Batch Gradient Norm after: 14.714365353230257
Epoch 5252/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.3529808759689331
Epoch: 5252, Batch Gradient Norm: 12.72528317236727
Epoch: 5252, Batch Gradient Norm after: 12.72528317236727
Epoch 5253/10000, Prediction Accuracy = 62.874%, Loss = 0.349921840429306
Epoch: 5253, Batch Gradient Norm: 14.004046778581166
Epoch: 5253, Batch Gradient Norm after: 14.004046778581166
Epoch 5254/10000, Prediction Accuracy = 63.117999999999995%, Loss = 0.35127283334732057
Epoch: 5254, Batch Gradient Norm: 12.267822366914269
Epoch: 5254, Batch Gradient Norm after: 12.267822366914269
Epoch 5255/10000, Prediction Accuracy = 63.126%, Loss = 0.3469537436962128
Epoch: 5255, Batch Gradient Norm: 11.534184132458336
Epoch: 5255, Batch Gradient Norm after: 11.534184132458336
Epoch 5256/10000, Prediction Accuracy = 63.11%, Loss = 0.34754509925842286
Epoch: 5256, Batch Gradient Norm: 14.052268515542377
Epoch: 5256, Batch Gradient Norm after: 14.052268515542377
Epoch 5257/10000, Prediction Accuracy = 63.072%, Loss = 0.3492021381855011
Epoch: 5257, Batch Gradient Norm: 13.475080379397365
Epoch: 5257, Batch Gradient Norm after: 13.475080379397365
Epoch 5258/10000, Prediction Accuracy = 63.08399999999999%, Loss = 0.34908402562141416
Epoch: 5258, Batch Gradient Norm: 12.934473756810563
Epoch: 5258, Batch Gradient Norm after: 12.934473756810563
Epoch 5259/10000, Prediction Accuracy = 63.068%, Loss = 0.3490336060523987
Epoch: 5259, Batch Gradient Norm: 13.393666097686477
Epoch: 5259, Batch Gradient Norm after: 13.393666097686477
Epoch 5260/10000, Prediction Accuracy = 62.94%, Loss = 0.3510798692703247
Epoch: 5260, Batch Gradient Norm: 12.452282045840862
Epoch: 5260, Batch Gradient Norm after: 12.452282045840862
Epoch 5261/10000, Prediction Accuracy = 63.044000000000004%, Loss = 0.34910306334495544
Epoch: 5261, Batch Gradient Norm: 12.191199353619174
Epoch: 5261, Batch Gradient Norm after: 12.191199353619174
Epoch 5262/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3502986431121826
Epoch: 5262, Batch Gradient Norm: 14.539172384093805
Epoch: 5262, Batch Gradient Norm after: 14.539172384093805
Epoch 5263/10000, Prediction Accuracy = 63.112%, Loss = 0.3507235527038574
Epoch: 5263, Batch Gradient Norm: 13.006910910215373
Epoch: 5263, Batch Gradient Norm after: 13.006910910215373
Epoch 5264/10000, Prediction Accuracy = 63.16400000000001%, Loss = 0.3483204782009125
Epoch: 5264, Batch Gradient Norm: 13.623094891529744
Epoch: 5264, Batch Gradient Norm after: 13.623094891529744
Epoch 5265/10000, Prediction Accuracy = 62.995999999999995%, Loss = 0.3481859862804413
Epoch: 5265, Batch Gradient Norm: 11.878767986279245
Epoch: 5265, Batch Gradient Norm after: 11.878767986279245
Epoch 5266/10000, Prediction Accuracy = 62.996%, Loss = 0.347716748714447
Epoch: 5266, Batch Gradient Norm: 13.578034026543921
Epoch: 5266, Batch Gradient Norm after: 13.578034026543921
Epoch 5267/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.35154837369918823
Epoch: 5267, Batch Gradient Norm: 13.384684203744843
Epoch: 5267, Batch Gradient Norm after: 13.384684203744843
Epoch 5268/10000, Prediction Accuracy = 62.98%, Loss = 0.3499827742576599
Epoch: 5268, Batch Gradient Norm: 17.39797899563251
Epoch: 5268, Batch Gradient Norm after: 17.21578698897021
Epoch 5269/10000, Prediction Accuracy = 63.012%, Loss = 0.3573511064052582
Epoch: 5269, Batch Gradient Norm: 14.659668360750306
Epoch: 5269, Batch Gradient Norm after: 14.659668360750306
Epoch 5270/10000, Prediction Accuracy = 63.15%, Loss = 0.3511963963508606
Epoch: 5270, Batch Gradient Norm: 15.43782477516216
Epoch: 5270, Batch Gradient Norm after: 15.43782477516216
Epoch 5271/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.350506854057312
Epoch: 5271, Batch Gradient Norm: 15.656456354005094
Epoch: 5271, Batch Gradient Norm after: 14.716727673403263
Epoch 5272/10000, Prediction Accuracy = 62.95%, Loss = 0.35516048669815065
Epoch: 5272, Batch Gradient Norm: 13.995387086537985
Epoch: 5272, Batch Gradient Norm after: 13.995387086537985
Epoch 5273/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.35071701407432554
Epoch: 5273, Batch Gradient Norm: 15.390184954312657
Epoch: 5273, Batch Gradient Norm after: 15.390184954312657
Epoch 5274/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.35009393095970154
Epoch: 5274, Batch Gradient Norm: 14.569617234886028
Epoch: 5274, Batch Gradient Norm after: 14.569617234886028
Epoch 5275/10000, Prediction Accuracy = 63.062%, Loss = 0.3496515452861786
Epoch: 5275, Batch Gradient Norm: 14.183866133315295
Epoch: 5275, Batch Gradient Norm after: 14.183866133315295
Epoch 5276/10000, Prediction Accuracy = 63.048%, Loss = 0.3483771920204163
Epoch: 5276, Batch Gradient Norm: 11.593622542940464
Epoch: 5276, Batch Gradient Norm after: 11.593622542940464
Epoch 5277/10000, Prediction Accuracy = 63.092%, Loss = 0.34624903202056884
Epoch: 5277, Batch Gradient Norm: 11.74947585734462
Epoch: 5277, Batch Gradient Norm after: 11.74947585734462
Epoch 5278/10000, Prediction Accuracy = 63.016000000000005%, Loss = 0.34698845744132994
Epoch: 5278, Batch Gradient Norm: 13.452947635538823
Epoch: 5278, Batch Gradient Norm after: 13.452947635538823
Epoch 5279/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.34689375162124636
Epoch: 5279, Batch Gradient Norm: 15.675654590887985
Epoch: 5279, Batch Gradient Norm after: 15.675654590887985
Epoch 5280/10000, Prediction Accuracy = 63.138%, Loss = 0.3511819064617157
Epoch: 5280, Batch Gradient Norm: 17.72294588202801
Epoch: 5280, Batch Gradient Norm after: 17.235311123012984
Epoch 5281/10000, Prediction Accuracy = 62.996%, Loss = 0.35606907606124877
Epoch: 5281, Batch Gradient Norm: 15.21437845520208
Epoch: 5281, Batch Gradient Norm after: 15.21437845520208
Epoch 5282/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.35169642567634585
Epoch: 5282, Batch Gradient Norm: 14.301668825584787
Epoch: 5282, Batch Gradient Norm after: 14.301668825584787
Epoch 5283/10000, Prediction Accuracy = 63.086%, Loss = 0.35148410201072694
Epoch: 5283, Batch Gradient Norm: 17.66980166947737
Epoch: 5283, Batch Gradient Norm after: 17.66980166947737
Epoch 5284/10000, Prediction Accuracy = 62.888%, Loss = 0.3584309697151184
Epoch: 5284, Batch Gradient Norm: 17.535486698556273
Epoch: 5284, Batch Gradient Norm after: 17.51037655326688
Epoch 5285/10000, Prediction Accuracy = 62.948%, Loss = 0.35375168919563293
Epoch: 5285, Batch Gradient Norm: 17.78112858952284
Epoch: 5285, Batch Gradient Norm after: 17.78112858952284
Epoch 5286/10000, Prediction Accuracy = 62.987999999999985%, Loss = 0.3557228446006775
Epoch: 5286, Batch Gradient Norm: 15.074364268302352
Epoch: 5286, Batch Gradient Norm after: 15.074364268302352
Epoch 5287/10000, Prediction Accuracy = 62.962%, Loss = 0.3510971188545227
Epoch: 5287, Batch Gradient Norm: 16.25590973036184
Epoch: 5287, Batch Gradient Norm after: 16.25590973036184
Epoch 5288/10000, Prediction Accuracy = 62.902%, Loss = 0.3568502008914948
Epoch: 5288, Batch Gradient Norm: 13.959946067111703
Epoch: 5288, Batch Gradient Norm after: 13.959946067111703
Epoch 5289/10000, Prediction Accuracy = 62.98599999999999%, Loss = 0.3501943528652191
Epoch: 5289, Batch Gradient Norm: 13.02496958627953
Epoch: 5289, Batch Gradient Norm after: 13.02496958627953
Epoch 5290/10000, Prediction Accuracy = 63.06200000000001%, Loss = 0.3499296844005585
Epoch: 5290, Batch Gradient Norm: 12.132940667134273
Epoch: 5290, Batch Gradient Norm after: 12.132940667134273
Epoch 5291/10000, Prediction Accuracy = 63.052%, Loss = 0.34869418740272523
Epoch: 5291, Batch Gradient Norm: 13.328360351967792
Epoch: 5291, Batch Gradient Norm after: 13.328360351967792
Epoch 5292/10000, Prediction Accuracy = 63.07800000000001%, Loss = 0.3475168466567993
Epoch: 5292, Batch Gradient Norm: 14.069065816073234
Epoch: 5292, Batch Gradient Norm after: 14.069065816073234
Epoch 5293/10000, Prediction Accuracy = 63.074%, Loss = 0.3516145348548889
Epoch: 5293, Batch Gradient Norm: 12.459085427215902
Epoch: 5293, Batch Gradient Norm after: 12.459085427215902
Epoch 5294/10000, Prediction Accuracy = 63.134%, Loss = 0.3477217495441437
Epoch: 5294, Batch Gradient Norm: 12.23192779154434
Epoch: 5294, Batch Gradient Norm after: 12.23192779154434
Epoch 5295/10000, Prediction Accuracy = 63.06%, Loss = 0.3476965487003326
Epoch: 5295, Batch Gradient Norm: 14.07786184596502
Epoch: 5295, Batch Gradient Norm after: 14.07786184596502
Epoch 5296/10000, Prediction Accuracy = 63.052%, Loss = 0.34922454953193666
Epoch: 5296, Batch Gradient Norm: 11.529062340363895
Epoch: 5296, Batch Gradient Norm after: 11.529062340363895
Epoch 5297/10000, Prediction Accuracy = 63.104%, Loss = 0.34752174019813536
Epoch: 5297, Batch Gradient Norm: 10.404697097176117
Epoch: 5297, Batch Gradient Norm after: 10.404697097176117
Epoch 5298/10000, Prediction Accuracy = 63.136%, Loss = 0.3437283098697662
Epoch: 5298, Batch Gradient Norm: 12.39988155670241
Epoch: 5298, Batch Gradient Norm after: 12.39988155670241
Epoch 5299/10000, Prediction Accuracy = 63.09000000000001%, Loss = 0.34492053985595705
Epoch: 5299, Batch Gradient Norm: 13.913753515165753
Epoch: 5299, Batch Gradient Norm after: 13.913753515165753
Epoch 5300/10000, Prediction Accuracy = 63.004%, Loss = 0.3488976716995239
Epoch: 5300, Batch Gradient Norm: 10.83195393782385
Epoch: 5300, Batch Gradient Norm after: 10.83195393782385
Epoch 5301/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.34291980862617494
Epoch: 5301, Batch Gradient Norm: 9.036350927407783
Epoch: 5301, Batch Gradient Norm after: 9.036350927407783
Epoch 5302/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3462251365184784
Epoch: 5302, Batch Gradient Norm: 11.835461804066542
Epoch: 5302, Batch Gradient Norm after: 11.835461804066542
Epoch 5303/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3465483009815216
Epoch: 5303, Batch Gradient Norm: 12.541514010874504
Epoch: 5303, Batch Gradient Norm after: 12.541514010874504
Epoch 5304/10000, Prediction Accuracy = 63.084%, Loss = 0.3467701733112335
Epoch: 5304, Batch Gradient Norm: 12.869758285097847
Epoch: 5304, Batch Gradient Norm after: 12.869758285097847
Epoch 5305/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.3474611520767212
Epoch: 5305, Batch Gradient Norm: 10.897112865465633
Epoch: 5305, Batch Gradient Norm after: 10.897112865465633
Epoch 5306/10000, Prediction Accuracy = 63.104%, Loss = 0.34776471853256224
Epoch: 5306, Batch Gradient Norm: 11.121809868779927
Epoch: 5306, Batch Gradient Norm after: 11.121809868779927
Epoch 5307/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.34447471499443055
Epoch: 5307, Batch Gradient Norm: 10.66249613496607
Epoch: 5307, Batch Gradient Norm after: 10.66249613496607
Epoch 5308/10000, Prediction Accuracy = 63.05%, Loss = 0.3461336851119995
Epoch: 5308, Batch Gradient Norm: 10.857653745794243
Epoch: 5308, Batch Gradient Norm after: 10.857653745794243
Epoch 5309/10000, Prediction Accuracy = 63.06600000000001%, Loss = 0.3468673348426819
Epoch: 5309, Batch Gradient Norm: 11.775292287111057
Epoch: 5309, Batch Gradient Norm after: 11.775292287111057
Epoch 5310/10000, Prediction Accuracy = 63.072%, Loss = 0.3468282878398895
Epoch: 5310, Batch Gradient Norm: 13.495714967540726
Epoch: 5310, Batch Gradient Norm after: 13.495714967540726
Epoch 5311/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.3479234933853149
Epoch: 5311, Batch Gradient Norm: 15.744523561434743
Epoch: 5311, Batch Gradient Norm after: 15.744523561434743
Epoch 5312/10000, Prediction Accuracy = 63.074%, Loss = 0.35359988212585447
Epoch: 5312, Batch Gradient Norm: 16.13393477657082
Epoch: 5312, Batch Gradient Norm after: 16.13393477657082
Epoch 5313/10000, Prediction Accuracy = 62.972%, Loss = 0.3518081605434418
Epoch: 5313, Batch Gradient Norm: 13.84596426047953
Epoch: 5313, Batch Gradient Norm after: 13.84596426047953
Epoch 5314/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.34825599789619444
Epoch: 5314, Batch Gradient Norm: 12.168202161624576
Epoch: 5314, Batch Gradient Norm after: 12.168202161624576
Epoch 5315/10000, Prediction Accuracy = 63.116%, Loss = 0.3467930257320404
Epoch: 5315, Batch Gradient Norm: 12.777529016001637
Epoch: 5315, Batch Gradient Norm after: 12.777529016001637
Epoch 5316/10000, Prediction Accuracy = 63.112%, Loss = 0.3462762892246246
Epoch: 5316, Batch Gradient Norm: 13.276677993906036
Epoch: 5316, Batch Gradient Norm after: 13.276677993906036
Epoch 5317/10000, Prediction Accuracy = 63.102%, Loss = 0.3485743820667267
Epoch: 5317, Batch Gradient Norm: 11.288813038111261
Epoch: 5317, Batch Gradient Norm after: 11.288813038111261
Epoch 5318/10000, Prediction Accuracy = 62.91799999999999%, Loss = 0.3471439123153687
Epoch: 5318, Batch Gradient Norm: 16.406008068057282
Epoch: 5318, Batch Gradient Norm after: 16.406008068057282
Epoch 5319/10000, Prediction Accuracy = 63.12800000000001%, Loss = 0.35196938514709475
Epoch: 5319, Batch Gradient Norm: 15.823375735520774
Epoch: 5319, Batch Gradient Norm after: 15.823375735520774
Epoch 5320/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.35063851475715635
Epoch: 5320, Batch Gradient Norm: 13.21646378540654
Epoch: 5320, Batch Gradient Norm after: 13.21646378540654
Epoch 5321/10000, Prediction Accuracy = 63.122%, Loss = 0.3471711099147797
Epoch: 5321, Batch Gradient Norm: 13.492057747357912
Epoch: 5321, Batch Gradient Norm after: 13.492057747357912
Epoch 5322/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3484162211418152
Epoch: 5322, Batch Gradient Norm: 14.229009472753152
Epoch: 5322, Batch Gradient Norm after: 14.229009472753152
Epoch 5323/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.34721890091896057
Epoch: 5323, Batch Gradient Norm: 10.65392350168979
Epoch: 5323, Batch Gradient Norm after: 10.65392350168979
Epoch 5324/10000, Prediction Accuracy = 63.05%, Loss = 0.3461411833763123
Epoch: 5324, Batch Gradient Norm: 13.29458139092545
Epoch: 5324, Batch Gradient Norm after: 13.29458139092545
Epoch 5325/10000, Prediction Accuracy = 63.02%, Loss = 0.3520736455917358
Epoch: 5325, Batch Gradient Norm: 11.021026863120134
Epoch: 5325, Batch Gradient Norm after: 11.021026863120134
Epoch 5326/10000, Prediction Accuracy = 63.114%, Loss = 0.34685524702072146
Epoch: 5326, Batch Gradient Norm: 10.590493678107473
Epoch: 5326, Batch Gradient Norm after: 10.590493678107473
Epoch 5327/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.34408706426620483
Epoch: 5327, Batch Gradient Norm: 11.132553525943944
Epoch: 5327, Batch Gradient Norm after: 11.132553525943944
Epoch 5328/10000, Prediction Accuracy = 62.99399999999999%, Loss = 0.3457785427570343
Epoch: 5328, Batch Gradient Norm: 11.02141626487188
Epoch: 5328, Batch Gradient Norm after: 11.02141626487188
Epoch 5329/10000, Prediction Accuracy = 63.176%, Loss = 0.3443962574005127
Epoch: 5329, Batch Gradient Norm: 10.763241939473263
Epoch: 5329, Batch Gradient Norm after: 10.763241939473263
Epoch 5330/10000, Prediction Accuracy = 63.146%, Loss = 0.34421181082725527
Epoch: 5330, Batch Gradient Norm: 10.248519367976348
Epoch: 5330, Batch Gradient Norm after: 10.248519367976348
Epoch 5331/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.34497653841972353
Epoch: 5331, Batch Gradient Norm: 11.724788622625212
Epoch: 5331, Batch Gradient Norm after: 11.724788622625212
Epoch 5332/10000, Prediction Accuracy = 63.084%, Loss = 0.34564473628997805
Epoch: 5332, Batch Gradient Norm: 10.321462216644493
Epoch: 5332, Batch Gradient Norm after: 10.321462216644493
Epoch 5333/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.34328741431236265
Epoch: 5333, Batch Gradient Norm: 13.070832431688588
Epoch: 5333, Batch Gradient Norm after: 13.070832431688588
Epoch 5334/10000, Prediction Accuracy = 63.146%, Loss = 0.34501715302467345
Epoch: 5334, Batch Gradient Norm: 16.7353514354834
Epoch: 5334, Batch Gradient Norm after: 16.7353514354834
Epoch 5335/10000, Prediction Accuracy = 63.138%, Loss = 0.3527414917945862
Epoch: 5335, Batch Gradient Norm: 13.84348567904803
Epoch: 5335, Batch Gradient Norm after: 13.84348567904803
Epoch 5336/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.34815839529037473
Epoch: 5336, Batch Gradient Norm: 13.9814888637639
Epoch: 5336, Batch Gradient Norm after: 13.9814888637639
Epoch 5337/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.34722156524658204
Epoch: 5337, Batch Gradient Norm: 17.100735087145797
Epoch: 5337, Batch Gradient Norm after: 17.100735087145797
Epoch 5338/10000, Prediction Accuracy = 63.10799999999999%, Loss = 0.3520375430583954
Epoch: 5338, Batch Gradient Norm: 18.085514025638002
Epoch: 5338, Batch Gradient Norm after: 17.94461310388819
Epoch 5339/10000, Prediction Accuracy = 63.068%, Loss = 0.3556573748588562
Epoch: 5339, Batch Gradient Norm: 14.222507603600512
Epoch: 5339, Batch Gradient Norm after: 14.222507603600512
Epoch 5340/10000, Prediction Accuracy = 63.114%, Loss = 0.3493854343891144
Epoch: 5340, Batch Gradient Norm: 12.00658621333866
Epoch: 5340, Batch Gradient Norm after: 12.00658621333866
Epoch 5341/10000, Prediction Accuracy = 63.028%, Loss = 0.34659276604652406
Epoch: 5341, Batch Gradient Norm: 12.71886736419174
Epoch: 5341, Batch Gradient Norm after: 12.71886736419174
Epoch 5342/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.34760466814041135
Epoch: 5342, Batch Gradient Norm: 14.376696667053928
Epoch: 5342, Batch Gradient Norm after: 14.376696667053928
Epoch 5343/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3497644603252411
Epoch: 5343, Batch Gradient Norm: 13.987063034290582
Epoch: 5343, Batch Gradient Norm after: 13.987063034290582
Epoch 5344/10000, Prediction Accuracy = 63.128%, Loss = 0.3470336675643921
Epoch: 5344, Batch Gradient Norm: 12.100014724324275
Epoch: 5344, Batch Gradient Norm after: 12.100014724324275
Epoch 5345/10000, Prediction Accuracy = 63.04%, Loss = 0.34780367016792296
Epoch: 5345, Batch Gradient Norm: 14.140372678025736
Epoch: 5345, Batch Gradient Norm after: 14.140372678025736
Epoch 5346/10000, Prediction Accuracy = 63.11%, Loss = 0.3489125072956085
Epoch: 5346, Batch Gradient Norm: 13.043915973958427
Epoch: 5346, Batch Gradient Norm after: 13.043915973958427
Epoch 5347/10000, Prediction Accuracy = 63.122%, Loss = 0.34731786251068114
Epoch: 5347, Batch Gradient Norm: 13.44913664331671
Epoch: 5347, Batch Gradient Norm after: 13.44913664331671
Epoch 5348/10000, Prediction Accuracy = 63.062%, Loss = 0.3475554585456848
Epoch: 5348, Batch Gradient Norm: 14.365008884178334
Epoch: 5348, Batch Gradient Norm after: 14.365008884178334
Epoch 5349/10000, Prediction Accuracy = 63.174%, Loss = 0.34753612279891966
Epoch: 5349, Batch Gradient Norm: 10.942625496006674
Epoch: 5349, Batch Gradient Norm after: 10.942625496006674
Epoch 5350/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3428362846374512
Epoch: 5350, Batch Gradient Norm: 11.2860178715376
Epoch: 5350, Batch Gradient Norm after: 11.2860178715376
Epoch 5351/10000, Prediction Accuracy = 63.05%, Loss = 0.3441683888435364
Epoch: 5351, Batch Gradient Norm: 13.03838759144248
Epoch: 5351, Batch Gradient Norm after: 13.03838759144248
Epoch 5352/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.34742493033409116
Epoch: 5352, Batch Gradient Norm: 10.481121192032534
Epoch: 5352, Batch Gradient Norm after: 10.481121192032534
Epoch 5353/10000, Prediction Accuracy = 63.064%, Loss = 0.3458146870136261
Epoch: 5353, Batch Gradient Norm: 12.71052013412849
Epoch: 5353, Batch Gradient Norm after: 12.71052013412849
Epoch 5354/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.3462324857711792
Epoch: 5354, Batch Gradient Norm: 13.102643285706048
Epoch: 5354, Batch Gradient Norm after: 13.102643285706048
Epoch 5355/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.34590407013893126
Epoch: 5355, Batch Gradient Norm: 15.558822137181085
Epoch: 5355, Batch Gradient Norm after: 15.558822137181085
Epoch 5356/10000, Prediction Accuracy = 63.040000000000006%, Loss = 0.3491249859333038
Epoch: 5356, Batch Gradient Norm: 15.579608708442981
Epoch: 5356, Batch Gradient Norm after: 15.579608708442981
Epoch 5357/10000, Prediction Accuracy = 62.98%, Loss = 0.3500935435295105
Epoch: 5357, Batch Gradient Norm: 19.19193752767508
Epoch: 5357, Batch Gradient Norm after: 18.035861957623037
Epoch 5358/10000, Prediction Accuracy = 63.05799999999999%, Loss = 0.3584984838962555
Epoch: 5358, Batch Gradient Norm: 16.505526631072446
Epoch: 5358, Batch Gradient Norm after: 16.083922999622242
Epoch 5359/10000, Prediction Accuracy = 63.086%, Loss = 0.35245318412780763
Epoch: 5359, Batch Gradient Norm: 17.155203140555088
Epoch: 5359, Batch Gradient Norm after: 17.155203140555088
Epoch 5360/10000, Prediction Accuracy = 63.062%, Loss = 0.35102075934410093
Epoch: 5360, Batch Gradient Norm: 16.445800722335257
Epoch: 5360, Batch Gradient Norm after: 16.445800722335257
Epoch 5361/10000, Prediction Accuracy = 63.076%, Loss = 0.3566405653953552
Epoch: 5361, Batch Gradient Norm: 16.03271260180978
Epoch: 5361, Batch Gradient Norm after: 16.03271260180978
Epoch 5362/10000, Prediction Accuracy = 63.186%, Loss = 0.3487737953662872
Epoch: 5362, Batch Gradient Norm: 13.306261248409164
Epoch: 5362, Batch Gradient Norm after: 13.306261248409164
Epoch 5363/10000, Prediction Accuracy = 63.174%, Loss = 0.3457366585731506
Epoch: 5363, Batch Gradient Norm: 12.090567610790215
Epoch: 5363, Batch Gradient Norm after: 12.090567610790215
Epoch 5364/10000, Prediction Accuracy = 63.178%, Loss = 0.34382672905921935
Epoch: 5364, Batch Gradient Norm: 9.62883433906504
Epoch: 5364, Batch Gradient Norm after: 9.62883433906504
Epoch 5365/10000, Prediction Accuracy = 63.178%, Loss = 0.3419415593147278
Epoch: 5365, Batch Gradient Norm: 11.102916173764452
Epoch: 5365, Batch Gradient Norm after: 11.102916173764452
Epoch 5366/10000, Prediction Accuracy = 63.14%, Loss = 0.3436667263507843
Epoch: 5366, Batch Gradient Norm: 14.031866234568625
Epoch: 5366, Batch Gradient Norm after: 14.031866234568625
Epoch 5367/10000, Prediction Accuracy = 63.146%, Loss = 0.3454967737197876
Epoch: 5367, Batch Gradient Norm: 11.987534457190577
Epoch: 5367, Batch Gradient Norm after: 11.987534457190577
Epoch 5368/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.3447880148887634
Epoch: 5368, Batch Gradient Norm: 12.610874316402308
Epoch: 5368, Batch Gradient Norm after: 12.610874316402308
Epoch 5369/10000, Prediction Accuracy = 63.164%, Loss = 0.34442129731178284
Epoch: 5369, Batch Gradient Norm: 14.26211594131284
Epoch: 5369, Batch Gradient Norm after: 14.26211594131284
Epoch 5370/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3462835311889648
Epoch: 5370, Batch Gradient Norm: 12.481896585530704
Epoch: 5370, Batch Gradient Norm after: 12.481896585530704
Epoch 5371/10000, Prediction Accuracy = 63.162%, Loss = 0.3460793733596802
Epoch: 5371, Batch Gradient Norm: 13.505816299863922
Epoch: 5371, Batch Gradient Norm after: 13.505816299863922
Epoch 5372/10000, Prediction Accuracy = 63.212%, Loss = 0.34547955393791197
Epoch: 5372, Batch Gradient Norm: 12.856606126024987
Epoch: 5372, Batch Gradient Norm after: 12.856606126024987
Epoch 5373/10000, Prediction Accuracy = 62.988%, Loss = 0.3462019801139832
Epoch: 5373, Batch Gradient Norm: 11.730718506833398
Epoch: 5373, Batch Gradient Norm after: 11.730718506833398
Epoch 5374/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3441280722618103
Epoch: 5374, Batch Gradient Norm: 14.053911079311343
Epoch 5375/10000, Prediction Accuracy = 63.038%, Loss = 0.3460664451122284
Epoch: 5375, Batch Gradient Norm: 14.560287761651061
Epoch: 5375, Batch Gradient Norm after: 14.560287761651061
Epoch 5376/10000, Prediction Accuracy = 63.072%, Loss = 0.34775310158729555
Epoch: 5376, Batch Gradient Norm: 13.401492183246386
Epoch: 5376, Batch Gradient Norm after: 13.401492183246386
Epoch 5377/10000, Prediction Accuracy = 63.036%, Loss = 0.3471256375312805
Epoch: 5377, Batch Gradient Norm: 13.140128744845686
Epoch: 5377, Batch Gradient Norm after: 13.140128744845686
Epoch 5378/10000, Prediction Accuracy = 63.15%, Loss = 0.3464557647705078
Epoch: 5378, Batch Gradient Norm: 15.101769683230689
Epoch: 5378, Batch Gradient Norm after: 15.101769683230689
Epoch 5379/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.3500832200050354
Epoch: 5379, Batch Gradient Norm: 14.47759722011822
Epoch: 5379, Batch Gradient Norm after: 14.47759722011822
Epoch 5380/10000, Prediction Accuracy = 63.092000000000006%, Loss = 0.3479892373085022
Epoch: 5380, Batch Gradient Norm: 13.096458141483874
Epoch: 5380, Batch Gradient Norm after: 13.096458141483874
Epoch 5381/10000, Prediction Accuracy = 63.212%, Loss = 0.345621794462204
Epoch: 5381, Batch Gradient Norm: 12.545096924899735
Epoch: 5381, Batch Gradient Norm after: 12.545096924899735
Epoch 5382/10000, Prediction Accuracy = 63.21%, Loss = 0.3433953166007996
Epoch: 5382, Batch Gradient Norm: 13.496228698072638
Epoch: 5382, Batch Gradient Norm after: 13.496228698072638
Epoch 5383/10000, Prediction Accuracy = 63.084%, Loss = 0.34971935153007505
Epoch: 5383, Batch Gradient Norm: 15.216778538378497
Epoch: 5383, Batch Gradient Norm after: 15.216778538378497
Epoch 5384/10000, Prediction Accuracy = 63.217999999999996%, Loss = 0.34955414533615115
Epoch: 5384, Batch Gradient Norm: 16.766824023396325
Epoch: 5384, Batch Gradient Norm after: 16.766824023396325
Epoch 5385/10000, Prediction Accuracy = 63.086%, Loss = 0.350051212310791
Epoch: 5385, Batch Gradient Norm: 15.418592770259007
Epoch: 5385, Batch Gradient Norm after: 15.418592770259007
Epoch 5386/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.34813334345817565
Epoch: 5386, Batch Gradient Norm: 17.205899627004396
Epoch: 5386, Batch Gradient Norm after: 17.205899627004396
Epoch 5387/10000, Prediction Accuracy = 63.052%, Loss = 0.35496958494186404
Epoch: 5387, Batch Gradient Norm: 14.783104993127857
Epoch: 5387, Batch Gradient Norm after: 14.783104993127857
Epoch 5388/10000, Prediction Accuracy = 63.194%, Loss = 0.34820746779441836
Epoch: 5388, Batch Gradient Norm: 14.022835202113058
Epoch: 5388, Batch Gradient Norm after: 14.022835202113058
Epoch 5389/10000, Prediction Accuracy = 63.028%, Loss = 0.3478428065776825
Epoch: 5389, Batch Gradient Norm: 14.075890631338885
Epoch: 5389, Batch Gradient Norm after: 14.075890631338885
Epoch 5390/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.3484600424766541
Epoch: 5390, Batch Gradient Norm: 12.907095145594905
Epoch: 5390, Batch Gradient Norm after: 12.907095145594905
Epoch 5391/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3468717455863953
Epoch: 5391, Batch Gradient Norm: 14.84001048010233
Epoch: 5391, Batch Gradient Norm after: 14.84001048010233
Epoch 5392/10000, Prediction Accuracy = 63.157999999999994%, Loss = 0.34926868677139283
Epoch: 5392, Batch Gradient Norm: 11.101593613913156
Epoch: 5392, Batch Gradient Norm after: 11.101593613913156
Epoch 5393/10000, Prediction Accuracy = 63.274%, Loss = 0.34296382069587705
Epoch: 5393, Batch Gradient Norm: 10.550544589385446
Epoch: 5393, Batch Gradient Norm after: 10.550544589385446
Epoch 5394/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3429030179977417
Epoch: 5394, Batch Gradient Norm: 10.169367194301245
Epoch: 5394, Batch Gradient Norm after: 10.169367194301245
Epoch 5395/10000, Prediction Accuracy = 63.262%, Loss = 0.34317214488983155
Epoch: 5395, Batch Gradient Norm: 13.829835482336376
Epoch: 5395, Batch Gradient Norm after: 13.829835482336376
Epoch 5396/10000, Prediction Accuracy = 63.162%, Loss = 0.34470751881599426
Epoch: 5396, Batch Gradient Norm: 11.270330880452429
Epoch: 5396, Batch Gradient Norm after: 11.270330880452429
Epoch 5397/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.3434246420860291
Epoch: 5397, Batch Gradient Norm: 12.339593239074645
Epoch: 5397, Batch Gradient Norm after: 12.339593239074645
Epoch 5398/10000, Prediction Accuracy = 63.152%, Loss = 0.34483805298805237
Epoch: 5398, Batch Gradient Norm: 13.706722688490624
Epoch: 5398, Batch Gradient Norm after: 13.706722688490624
Epoch 5399/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.34773707389831543
Epoch: 5399, Batch Gradient Norm: 13.93515029568267
Epoch: 5399, Batch Gradient Norm after: 13.93515029568267
Epoch 5400/10000, Prediction Accuracy = 63.11%, Loss = 0.34789668321609496
Epoch: 5400, Batch Gradient Norm: 16.035576721417332
Epoch: 5400, Batch Gradient Norm after: 16.035576721417332
Epoch 5401/10000, Prediction Accuracy = 63.032%, Loss = 0.35030454993247984
Epoch: 5401, Batch Gradient Norm: 18.0482743443676
Epoch: 5401, Batch Gradient Norm after: 18.0482743443676
Epoch 5402/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3525057196617126
Epoch: 5402, Batch Gradient Norm: 16.282220970573164
Epoch: 5402, Batch Gradient Norm after: 16.282220970573164
Epoch 5403/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.34909397959709165
Epoch: 5403, Batch Gradient Norm: 15.848264875631276
Epoch: 5403, Batch Gradient Norm after: 15.848264875631276
Epoch 5404/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.35106507539749143
Epoch: 5404, Batch Gradient Norm: 12.98897863036128
Epoch: 5404, Batch Gradient Norm after: 12.98897863036128
Epoch 5405/10000, Prediction Accuracy = 63.134%, Loss = 0.34559465050697324
Epoch: 5405, Batch Gradient Norm: 13.28555160739704
Epoch: 5405, Batch Gradient Norm after: 13.28555160739704
Epoch 5406/10000, Prediction Accuracy = 63.134%, Loss = 0.34476142525672915
Epoch: 5406, Batch Gradient Norm: 12.740074258660249
Epoch: 5406, Batch Gradient Norm after: 12.740074258660249
Epoch 5407/10000, Prediction Accuracy = 63.186%, Loss = 0.3463387131690979
Epoch: 5407, Batch Gradient Norm: 14.034936950843194
Epoch: 5407, Batch Gradient Norm after: 14.034936950843194
Epoch 5408/10000, Prediction Accuracy = 63.212%, Loss = 0.34670884609222413
Epoch: 5408, Batch Gradient Norm: 12.257297795063202
Epoch: 5408, Batch Gradient Norm after: 12.257297795063202
Epoch 5409/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.3468981385231018
Epoch: 5409, Batch Gradient Norm: 13.997723322117011
Epoch: 5409, Batch Gradient Norm after: 13.997723322117011
Epoch 5410/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.3485440492630005
Epoch: 5410, Batch Gradient Norm: 12.554275863897086
Epoch: 5410, Batch Gradient Norm after: 12.554275863897086
Epoch 5411/10000, Prediction Accuracy = 63.214%, Loss = 0.34477896690368653
Epoch: 5411, Batch Gradient Norm: 10.438716985902293
Epoch: 5411, Batch Gradient Norm after: 10.438716985902293
Epoch 5412/10000, Prediction Accuracy = 63.162%, Loss = 0.3417055785655975
Epoch: 5412, Batch Gradient Norm: 10.421000967245275
Epoch: 5412, Batch Gradient Norm after: 10.421000967245275
Epoch 5413/10000, Prediction Accuracy = 63.14%, Loss = 0.34087459444999696
Epoch: 5413, Batch Gradient Norm: 11.199307586560549
Epoch: 5413, Batch Gradient Norm after: 11.199307586560549
Epoch 5414/10000, Prediction Accuracy = 63.04%, Loss = 0.3423882782459259
Epoch: 5414, Batch Gradient Norm: 12.85019518207027
Epoch: 5414, Batch Gradient Norm after: 12.85019518207027
Epoch 5415/10000, Prediction Accuracy = 63.092%, Loss = 0.34296207427978515
Epoch: 5415, Batch Gradient Norm: 14.41389152223291
Epoch: 5415, Batch Gradient Norm after: 14.41389152223291
Epoch 5416/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.34847261309623717
Epoch: 5416, Batch Gradient Norm: 14.186056543773919
Epoch: 5416, Batch Gradient Norm after: 14.186056543773919
Epoch 5417/10000, Prediction Accuracy = 63.008%, Loss = 0.34760661125183107
Epoch: 5417, Batch Gradient Norm: 14.308661710261575
Epoch: 5417, Batch Gradient Norm after: 14.308661710261575
Epoch 5418/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.346463268995285
Epoch: 5418, Batch Gradient Norm: 16.61748727901507
Epoch: 5418, Batch Gradient Norm after: 16.61748727901507
Epoch 5419/10000, Prediction Accuracy = 63.138%, Loss = 0.35163902044296264
Epoch: 5419, Batch Gradient Norm: 13.779314961461282
Epoch: 5419, Batch Gradient Norm after: 13.779314961461282
Epoch 5420/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.34576376080513
Epoch: 5420, Batch Gradient Norm: 11.944945440905267
Epoch: 5420, Batch Gradient Norm after: 11.944945440905267
Epoch 5421/10000, Prediction Accuracy = 63.126%, Loss = 0.3432012677192688
Epoch: 5421, Batch Gradient Norm: 10.62931160866369
Epoch: 5421, Batch Gradient Norm after: 10.62931160866369
Epoch 5422/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.3422935903072357
Epoch: 5422, Batch Gradient Norm: 9.608442133582217
Epoch: 5422, Batch Gradient Norm after: 9.608442133582217
Epoch 5423/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3402727425098419
Epoch: 5423, Batch Gradient Norm: 9.670711765020704
Epoch: 5423, Batch Gradient Norm after: 9.670711765020704
Epoch 5424/10000, Prediction Accuracy = 63.278%, Loss = 0.34162926077842715
Epoch: 5424, Batch Gradient Norm: 11.47108784267737
Epoch: 5424, Batch Gradient Norm after: 11.47108784267737
Epoch 5425/10000, Prediction Accuracy = 63.146%, Loss = 0.3429228365421295
Epoch: 5425, Batch Gradient Norm: 12.890776295785775
Epoch: 5425, Batch Gradient Norm after: 12.890776295785775
Epoch 5426/10000, Prediction Accuracy = 63.124%, Loss = 0.3445015847682953
Epoch: 5426, Batch Gradient Norm: 12.153827474357092
Epoch: 5426, Batch Gradient Norm after: 12.153827474357092
Epoch 5427/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.34492290019989014
Epoch: 5427, Batch Gradient Norm: 16.578774609158724
Epoch: 5427, Batch Gradient Norm after: 16.578774609158724
Epoch 5428/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3495956063270569
Epoch: 5428, Batch Gradient Norm: 17.90794609223368
Epoch: 5428, Batch Gradient Norm after: 17.90794609223368
Epoch 5429/10000, Prediction Accuracy = 63.124%, Loss = 0.35357173085212706
Epoch: 5429, Batch Gradient Norm: 14.897754319639708
Epoch: 5429, Batch Gradient Norm after: 14.897754319639708
Epoch 5430/10000, Prediction Accuracy = 63.158%, Loss = 0.3493067681789398
Epoch: 5430, Batch Gradient Norm: 16.46892036119777
Epoch: 5430, Batch Gradient Norm after: 16.46892036119777
Epoch 5431/10000, Prediction Accuracy = 63.10799999999999%, Loss = 0.34847397208213804
Epoch: 5431, Batch Gradient Norm: 18.081610196740254
Epoch: 5431, Batch Gradient Norm after: 17.154398765740762
Epoch 5432/10000, Prediction Accuracy = 63.114%, Loss = 0.3519364595413208
Epoch: 5432, Batch Gradient Norm: 17.17993841523308
Epoch: 5432, Batch Gradient Norm after: 17.106317002488
Epoch 5433/10000, Prediction Accuracy = 63.16799999999999%, Loss = 0.34969920516014097
Epoch: 5433, Batch Gradient Norm: 13.315166541647379
Epoch: 5433, Batch Gradient Norm after: 13.315166541647379
Epoch 5434/10000, Prediction Accuracy = 63.402%, Loss = 0.34451369047164915
Epoch: 5434, Batch Gradient Norm: 13.999379391133466
Epoch: 5434, Batch Gradient Norm after: 13.999379391133466
Epoch 5435/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3453871369361877
Epoch: 5435, Batch Gradient Norm: 10.99517592506272
Epoch: 5435, Batch Gradient Norm after: 10.99517592506272
Epoch 5436/10000, Prediction Accuracy = 63.148%, Loss = 0.33992048501968386
Epoch: 5436, Batch Gradient Norm: 13.27451916028192
Epoch: 5436, Batch Gradient Norm after: 13.27451916028192
Epoch 5437/10000, Prediction Accuracy = 63.188%, Loss = 0.34475414752960204
Epoch: 5437, Batch Gradient Norm: 13.407649759703986
Epoch: 5437, Batch Gradient Norm after: 13.407649759703986
Epoch 5438/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.34816727638244627
Epoch: 5438, Batch Gradient Norm: 11.725345089831336
Epoch: 5438, Batch Gradient Norm after: 11.725345089831336
Epoch 5439/10000, Prediction Accuracy = 63.098%, Loss = 0.34435796141624453
Epoch: 5439, Batch Gradient Norm: 15.530289654305914
Epoch: 5439, Batch Gradient Norm after: 15.530289654305914
Epoch 5440/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3512682497501373
Epoch: 5440, Batch Gradient Norm: 11.52124618172148
Epoch: 5440, Batch Gradient Norm after: 11.52124618172148
Epoch 5441/10000, Prediction Accuracy = 63.152%, Loss = 0.34516586661338805
Epoch: 5441, Batch Gradient Norm: 12.171298288832155
Epoch: 5441, Batch Gradient Norm after: 12.171298288832155
Epoch 5442/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.3431906223297119
Epoch: 5442, Batch Gradient Norm: 15.267856922956947
Epoch: 5442, Batch Gradient Norm after: 15.267856922956947
Epoch 5443/10000, Prediction Accuracy = 63.064%, Loss = 0.35039040446281433
Epoch: 5443, Batch Gradient Norm: 16.46202594283267
Epoch: 5443, Batch Gradient Norm after: 16.46202594283267
Epoch 5444/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.35078064203262327
Epoch: 5444, Batch Gradient Norm: 14.146678859882588
Epoch: 5444, Batch Gradient Norm after: 14.146678859882588
Epoch 5445/10000, Prediction Accuracy = 63.217999999999996%, Loss = 0.3449679672718048
Epoch: 5445, Batch Gradient Norm: 12.972185493084096
Epoch: 5445, Batch Gradient Norm after: 12.972185493084096
Epoch 5446/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.34344157576560974
Epoch: 5446, Batch Gradient Norm: 13.094623244655063
Epoch: 5446, Batch Gradient Norm after: 13.094623244655063
Epoch 5447/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.34502986669540403
Epoch: 5447, Batch Gradient Norm: 12.586321050323889
Epoch: 5447, Batch Gradient Norm after: 12.586321050323889
Epoch 5448/10000, Prediction Accuracy = 63.302%, Loss = 0.3436133205890656
Epoch: 5448, Batch Gradient Norm: 12.828987832615981
Epoch: 5448, Batch Gradient Norm after: 12.828987832615981
Epoch 5449/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.34370497465133665
Epoch: 5449, Batch Gradient Norm: 13.225256577652472
Epoch: 5449, Batch Gradient Norm after: 13.225256577652472
Epoch 5450/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.3449219107627869
Epoch: 5450, Batch Gradient Norm: 15.634630876809418
Epoch: 5450, Batch Gradient Norm after: 15.634630876809418
Epoch 5451/10000, Prediction Accuracy = 63.18599999999999%, Loss = 0.34930306673049927
Epoch: 5451, Batch Gradient Norm: 12.800214246721385
Epoch: 5451, Batch Gradient Norm after: 12.800214246721385
Epoch 5452/10000, Prediction Accuracy = 63.15%, Loss = 0.3455347537994385
Epoch: 5452, Batch Gradient Norm: 13.652273191254544
Epoch: 5452, Batch Gradient Norm after: 13.652273191254544
Epoch 5453/10000, Prediction Accuracy = 63.15999999999999%, Loss = 0.34411917328834535
Epoch: 5453, Batch Gradient Norm: 11.515223377691377
Epoch: 5453, Batch Gradient Norm after: 11.515223377691377
Epoch 5454/10000, Prediction Accuracy = 63.222%, Loss = 0.34224599599838257
Epoch: 5454, Batch Gradient Norm: 12.323124018592726
Epoch: 5454, Batch Gradient Norm after: 12.323124018592726
Epoch 5455/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.3452785909175873
Epoch: 5455, Batch Gradient Norm: 11.06083109156156
Epoch: 5455, Batch Gradient Norm after: 11.06083109156156
Epoch 5456/10000, Prediction Accuracy = 63.25%, Loss = 0.34401957392692567
Epoch: 5456, Batch Gradient Norm: 12.804734898387279
Epoch: 5456, Batch Gradient Norm after: 12.804734898387279
Epoch 5457/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.34339640736579896
Epoch: 5457, Batch Gradient Norm: 12.82242513651988
Epoch: 5457, Batch Gradient Norm after: 12.82242513651988
Epoch 5458/10000, Prediction Accuracy = 63.1%, Loss = 0.3434006929397583
Epoch: 5458, Batch Gradient Norm: 11.536854974228383
Epoch: 5458, Batch Gradient Norm after: 11.536854974228383
Epoch 5459/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3454568088054657
Epoch: 5459, Batch Gradient Norm: 12.735424652464854
Epoch: 5459, Batch Gradient Norm after: 12.735424652464854
Epoch 5460/10000, Prediction Accuracy = 63.174%, Loss = 0.34319445490837097
Epoch: 5460, Batch Gradient Norm: 12.02876328746748
Epoch: 5460, Batch Gradient Norm after: 12.02876328746748
Epoch 5461/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.34270421266555784
Epoch: 5461, Batch Gradient Norm: 9.358274398198768
Epoch: 5461, Batch Gradient Norm after: 9.358274398198768
Epoch 5462/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.33854698538780215
Epoch: 5462, Batch Gradient Norm: 14.089492641000765
Epoch: 5462, Batch Gradient Norm after: 14.089492641000765
Epoch 5463/10000, Prediction Accuracy = 63.152%, Loss = 0.34453383684158323
Epoch: 5463, Batch Gradient Norm: 14.820986785159072
Epoch: 5463, Batch Gradient Norm after: 14.820986785159072
Epoch 5464/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.3476431488990784
Epoch: 5464, Batch Gradient Norm: 13.452873155871448
Epoch: 5464, Batch Gradient Norm after: 13.452873155871448
Epoch 5465/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.34573432207107546
Epoch: 5465, Batch Gradient Norm: 12.132673192835146
Epoch: 5465, Batch Gradient Norm after: 12.132673192835146
Epoch 5466/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3426706314086914
Epoch: 5466, Batch Gradient Norm: 11.595409893467938
Epoch: 5466, Batch Gradient Norm after: 11.595409893467938
Epoch 5467/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.3421474754810333
Epoch: 5467, Batch Gradient Norm: 10.290678093119455
Epoch: 5467, Batch Gradient Norm after: 10.290678093119455
Epoch 5468/10000, Prediction Accuracy = 63.17%, Loss = 0.34088910818099977
Epoch: 5468, Batch Gradient Norm: 12.944634948511649
Epoch: 5468, Batch Gradient Norm after: 12.944634948511649
Epoch 5469/10000, Prediction Accuracy = 63.086%, Loss = 0.3421800971031189
Epoch: 5469, Batch Gradient Norm: 14.539858951781909
Epoch: 5469, Batch Gradient Norm after: 14.539858951781909
Epoch 5470/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.34910562038421633
Epoch: 5470, Batch Gradient Norm: 14.961735320935645
Epoch: 5470, Batch Gradient Norm after: 14.961735320935645
Epoch 5471/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3469520449638367
Epoch: 5471, Batch Gradient Norm: 12.127290417251633
Epoch: 5471, Batch Gradient Norm after: 12.127290417251633
Epoch 5472/10000, Prediction Accuracy = 63.186%, Loss = 0.34284947514534
Epoch: 5472, Batch Gradient Norm: 12.133725475333787
Epoch: 5472, Batch Gradient Norm after: 12.133725475333787
Epoch 5473/10000, Prediction Accuracy = 63.134%, Loss = 0.343431282043457
Epoch: 5473, Batch Gradient Norm: 15.090066409401711
Epoch: 5473, Batch Gradient Norm after: 15.090066409401711
Epoch 5474/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.34640330672264097
Epoch: 5474, Batch Gradient Norm: 14.614454664753934
Epoch: 5474, Batch Gradient Norm after: 14.614454664753934
Epoch 5475/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.3446495771408081
Epoch: 5475, Batch Gradient Norm: 14.503405342354608
Epoch: 5475, Batch Gradient Norm after: 14.503405342354608
Epoch 5476/10000, Prediction Accuracy = 63.314%, Loss = 0.3463116347789764
Epoch: 5476, Batch Gradient Norm: 11.327681481326165
Epoch: 5476, Batch Gradient Norm after: 11.327681481326165
Epoch 5477/10000, Prediction Accuracy = 63.3%, Loss = 0.3411149561405182
Epoch: 5477, Batch Gradient Norm: 13.233782655658805
Epoch: 5477, Batch Gradient Norm after: 13.233782655658805
Epoch 5478/10000, Prediction Accuracy = 63.18599999999999%, Loss = 0.34284300804138185
Epoch: 5478, Batch Gradient Norm: 13.65423985378174
Epoch: 5478, Batch Gradient Norm after: 13.65423985378174
Epoch 5479/10000, Prediction Accuracy = 63.298%, Loss = 0.3440476953983307
Epoch: 5479, Batch Gradient Norm: 12.861364666502586
Epoch: 5479, Batch Gradient Norm after: 12.861364666502586
Epoch 5480/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3439046621322632
Epoch: 5480, Batch Gradient Norm: 11.473537502705193
Epoch: 5480, Batch Gradient Norm after: 11.473537502705193
Epoch 5481/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3424553036689758
Epoch: 5481, Batch Gradient Norm: 9.852441183795117
Epoch: 5481, Batch Gradient Norm after: 9.852441183795117
Epoch 5482/10000, Prediction Accuracy = 63.334%, Loss = 0.341861629486084
Epoch: 5482, Batch Gradient Norm: 13.020883226890655
Epoch: 5482, Batch Gradient Norm after: 13.020883226890655
Epoch 5483/10000, Prediction Accuracy = 63.152%, Loss = 0.34590198993682864
Epoch: 5483, Batch Gradient Norm: 15.622455764766748
Epoch: 5483, Batch Gradient Norm after: 15.622455764766748
Epoch 5484/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.3468825578689575
Epoch: 5484, Batch Gradient Norm: 16.051962024605533
Epoch: 5484, Batch Gradient Norm after: 16.051962024605533
Epoch 5485/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.34690543413162234
Epoch: 5485, Batch Gradient Norm: 13.018182917050877
Epoch: 5485, Batch Gradient Norm after: 13.018182917050877
Epoch 5486/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.3443605601787567
Epoch: 5486, Batch Gradient Norm: 14.913550100301846
Epoch: 5486, Batch Gradient Norm after: 14.913550100301846
Epoch 5487/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.3447559535503387
Epoch: 5487, Batch Gradient Norm: 11.185088080911706
Epoch: 5487, Batch Gradient Norm after: 11.185088080911706
Epoch 5488/10000, Prediction Accuracy = 63.164%, Loss = 0.34122158885002135
Epoch: 5488, Batch Gradient Norm: 13.181578122527192
Epoch: 5488, Batch Gradient Norm after: 13.181578122527192
Epoch 5489/10000, Prediction Accuracy = 63.136%, Loss = 0.3448687672615051
Epoch: 5489, Batch Gradient Norm: 13.255889310312003
Epoch: 5489, Batch Gradient Norm after: 13.255889310312003
Epoch 5490/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.34324930906295775
Epoch: 5490, Batch Gradient Norm: 13.70322644376036
Epoch: 5490, Batch Gradient Norm after: 13.70322644376036
Epoch 5491/10000, Prediction Accuracy = 63.222%, Loss = 0.3468953609466553
Epoch: 5491, Batch Gradient Norm: 11.692930900657423
Epoch: 5491, Batch Gradient Norm after: 11.692930900657423
Epoch 5492/10000, Prediction Accuracy = 63.193999999999996%, Loss = 0.3412525415420532
Epoch: 5492, Batch Gradient Norm: 14.036842591957102
Epoch: 5492, Batch Gradient Norm after: 14.036842591957102
Epoch 5493/10000, Prediction Accuracy = 63.162%, Loss = 0.3448427259922028
Epoch: 5493, Batch Gradient Norm: 15.212407448646685
Epoch: 5493, Batch Gradient Norm after: 15.212407448646685
Epoch 5494/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.34601929783821106
Epoch: 5494, Batch Gradient Norm: 16.261104083092167
Epoch: 5494, Batch Gradient Norm after: 16.261104083092167
Epoch 5495/10000, Prediction Accuracy = 63.209999999999994%, Loss = 0.3462912976741791
Epoch: 5495, Batch Gradient Norm: 17.721769224576114
Epoch: 5495, Batch Gradient Norm after: 17.721769224576114
Epoch 5496/10000, Prediction Accuracy = 63.148%, Loss = 0.3505950689315796
Epoch: 5496, Batch Gradient Norm: 21.128655555009765
Epoch: 5496, Batch Gradient Norm after: 20.53100750879619
Epoch 5497/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.35680878162384033
Epoch: 5497, Batch Gradient Norm: 18.13797116060663
Epoch: 5497, Batch Gradient Norm after: 18.015808579162606
Epoch 5498/10000, Prediction Accuracy = 63.188%, Loss = 0.3496545791625977
Epoch: 5498, Batch Gradient Norm: 14.759951684350083
Epoch: 5498, Batch Gradient Norm after: 14.759951684350083
Epoch 5499/10000, Prediction Accuracy = 63.146%, Loss = 0.34478192329406737
Epoch: 5499, Batch Gradient Norm: 16.57725177038512
Epoch: 5499, Batch Gradient Norm after: 16.09904525833897
Epoch 5500/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3470068097114563
Epoch: 5500, Batch Gradient Norm: 14.586470774690245
Epoch: 5500, Batch Gradient Norm after: 14.586470774690245
Epoch 5501/10000, Prediction Accuracy = 63.176%, Loss = 0.346376895904541
Epoch: 5501, Batch Gradient Norm: 16.461245766280072
Epoch: 5501, Batch Gradient Norm after: 16.461245766280072
Epoch 5502/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.352110493183136
Epoch: 5502, Batch Gradient Norm: 14.951851746765456
Epoch: 5502, Batch Gradient Norm after: 14.951851746765456
Epoch 5503/10000, Prediction Accuracy = 63.138%, Loss = 0.34752010107040404
Epoch: 5503, Batch Gradient Norm: 13.978102826268849
Epoch: 5503, Batch Gradient Norm after: 13.978102826268849
Epoch 5504/10000, Prediction Accuracy = 63.17%, Loss = 0.344863361120224
Epoch: 5504, Batch Gradient Norm: 16.744721190952106
Epoch: 5504, Batch Gradient Norm after: 16.744721190952106
Epoch 5505/10000, Prediction Accuracy = 63.182%, Loss = 0.34784031510353086
Epoch: 5505, Batch Gradient Norm: 15.791310664031004
Epoch: 5505, Batch Gradient Norm after: 15.791310664031004
Epoch 5506/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.34580514430999754
Epoch: 5506, Batch Gradient Norm: 15.053317816264675
Epoch: 5506, Batch Gradient Norm after: 15.053317816264675
Epoch 5507/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.347272002696991
Epoch: 5507, Batch Gradient Norm: 17.46273521335604
Epoch: 5507, Batch Gradient Norm after: 17.46273521335604
Epoch 5508/10000, Prediction Accuracy = 63.226%, Loss = 0.35204154849052427
Epoch: 5508, Batch Gradient Norm: 17.382773930466904
Epoch: 5508, Batch Gradient Norm after: 17.382773930466904
Epoch 5509/10000, Prediction Accuracy = 63.25%, Loss = 0.3484306871891022
Epoch: 5509, Batch Gradient Norm: 16.427009442946456
Epoch: 5509, Batch Gradient Norm after: 16.427009442946456
Epoch 5510/10000, Prediction Accuracy = 63.186%, Loss = 0.34734131693840026
Epoch: 5510, Batch Gradient Norm: 17.84882098934429
Epoch: 5510, Batch Gradient Norm after: 17.53381797068868
Epoch 5511/10000, Prediction Accuracy = 63.134%, Loss = 0.35015198588371277
Epoch: 5511, Batch Gradient Norm: 16.35818923702932
Epoch: 5511, Batch Gradient Norm after: 16.35818923702932
Epoch 5512/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.34982000589370726
Epoch: 5512, Batch Gradient Norm: 17.66018698845819
Epoch: 5512, Batch Gradient Norm after: 17.66018698845819
Epoch 5513/10000, Prediction Accuracy = 63.3%, Loss = 0.34923004508018496
Epoch: 5513, Batch Gradient Norm: 16.264562904094486
Epoch: 5513, Batch Gradient Norm after: 16.264562904094486
Epoch 5514/10000, Prediction Accuracy = 63.122%, Loss = 0.349375331401825
Epoch: 5514, Batch Gradient Norm: 15.202869356921695
Epoch: 5514, Batch Gradient Norm after: 15.202869356921695
Epoch 5515/10000, Prediction Accuracy = 63.092000000000006%, Loss = 0.34482028484344485
Epoch: 5515, Batch Gradient Norm: 14.769241544900492
Epoch: 5515, Batch Gradient Norm after: 14.769241544900492
Epoch 5516/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.34416770935058594
Epoch: 5516, Batch Gradient Norm: 13.250625665129146
Epoch: 5516, Batch Gradient Norm after: 13.250625665129146
Epoch 5517/10000, Prediction Accuracy = 63.14200000000001%, Loss = 0.3429452359676361
Epoch: 5517, Batch Gradient Norm: 13.908268998778986
Epoch: 5517, Batch Gradient Norm after: 13.908268998778986
Epoch 5518/10000, Prediction Accuracy = 63.10999999999999%, Loss = 0.34579753279685976
Epoch: 5518, Batch Gradient Norm: 15.205372083892328
Epoch: 5518, Batch Gradient Norm after: 15.205372083892328
Epoch 5519/10000, Prediction Accuracy = 63.19599999999999%, Loss = 0.3457187354564667
Epoch: 5519, Batch Gradient Norm: 17.28220447633016
Epoch: 5519, Batch Gradient Norm after: 17.28220447633016
Epoch 5520/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.35041680932044983
Epoch: 5520, Batch Gradient Norm: 16.559042252843916
Epoch: 5520, Batch Gradient Norm after: 16.559042252843916
Epoch 5521/10000, Prediction Accuracy = 63.193999999999996%, Loss = 0.34781879782676695
Epoch: 5521, Batch Gradient Norm: 17.680190170687577
Epoch: 5521, Batch Gradient Norm after: 17.680190170687577
Epoch 5522/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.35041211247444154
Epoch: 5522, Batch Gradient Norm: 15.07575938432346
Epoch: 5522, Batch Gradient Norm after: 15.07575938432346
Epoch 5523/10000, Prediction Accuracy = 63.202%, Loss = 0.3446991503238678
Epoch: 5523, Batch Gradient Norm: 13.464474020814478
Epoch: 5523, Batch Gradient Norm after: 13.464474020814478
Epoch 5524/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.3448024570941925
Epoch: 5524, Batch Gradient Norm: 12.212195939570002
Epoch: 5524, Batch Gradient Norm after: 12.212195939570002
Epoch 5525/10000, Prediction Accuracy = 63.146%, Loss = 0.34106615781784055
Epoch: 5525, Batch Gradient Norm: 11.577787084207921
Epoch: 5525, Batch Gradient Norm after: 11.577787084207921
Epoch 5526/10000, Prediction Accuracy = 63.354%, Loss = 0.34011924266815186
Epoch: 5526, Batch Gradient Norm: 10.73798990166464
Epoch: 5526, Batch Gradient Norm after: 10.73798990166464
Epoch 5527/10000, Prediction Accuracy = 63.164%, Loss = 0.341231107711792
Epoch: 5527, Batch Gradient Norm: 12.247562063071951
Epoch: 5527, Batch Gradient Norm after: 12.247562063071951
Epoch 5528/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.34194029569625856
Epoch: 5528, Batch Gradient Norm: 14.51837999267434
Epoch: 5528, Batch Gradient Norm after: 14.51837999267434
Epoch 5529/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.34179999232292174
Epoch: 5529, Batch Gradient Norm: 14.648371704604013
Epoch: 5529, Batch Gradient Norm after: 14.648371704604013
Epoch 5530/10000, Prediction Accuracy = 63.262%, Loss = 0.3448981761932373
Epoch: 5530, Batch Gradient Norm: 13.7828619533741
Epoch: 5530, Batch Gradient Norm after: 13.7828619533741
Epoch 5531/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.34380006790161133
Epoch: 5531, Batch Gradient Norm: 14.518954476538822
Epoch: 5531, Batch Gradient Norm after: 14.518954476538822
Epoch 5532/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3425135314464569
Epoch: 5532, Batch Gradient Norm: 14.325931212052362
Epoch: 5532, Batch Gradient Norm after: 14.325931212052362
Epoch 5533/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.34300724864006044
Epoch: 5533, Batch Gradient Norm: 11.446589182852163
Epoch: 5533, Batch Gradient Norm after: 11.446589182852163
Epoch 5534/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.34138875603675845
Epoch: 5534, Batch Gradient Norm: 11.428275912607978
Epoch: 5534, Batch Gradient Norm after: 11.428275912607978
Epoch 5535/10000, Prediction Accuracy = 63.258%, Loss = 0.3415463149547577
Epoch: 5535, Batch Gradient Norm: 12.495921166976261
Epoch: 5535, Batch Gradient Norm after: 12.495921166976261
Epoch 5536/10000, Prediction Accuracy = 63.226%, Loss = 0.34226514101028443
Epoch: 5536, Batch Gradient Norm: 13.264624213837514
Epoch: 5536, Batch Gradient Norm after: 13.264624213837514
Epoch 5537/10000, Prediction Accuracy = 63.20400000000001%, Loss = 0.3424775660037994
Epoch: 5537, Batch Gradient Norm: 15.231152565677089
Epoch: 5537, Batch Gradient Norm after: 15.231152565677089
Epoch 5538/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3495827317237854
Epoch: 5538, Batch Gradient Norm: 14.529464385944928
Epoch: 5538, Batch Gradient Norm after: 14.529464385944928
Epoch 5539/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3439175128936768
Epoch: 5539, Batch Gradient Norm: 14.303562286094666
Epoch: 5539, Batch Gradient Norm after: 14.303562286094666
Epoch 5540/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.34618467688560484
Epoch: 5540, Batch Gradient Norm: 14.179907279067386
Epoch: 5540, Batch Gradient Norm after: 14.179907279067386
Epoch 5541/10000, Prediction Accuracy = 63.286%, Loss = 0.34466745853424074
Epoch: 5541, Batch Gradient Norm: 13.451366813552518
Epoch: 5541, Batch Gradient Norm after: 13.451366813552518
Epoch 5542/10000, Prediction Accuracy = 63.242%, Loss = 0.34180404543876647
Epoch: 5542, Batch Gradient Norm: 13.193551388945275
Epoch: 5542, Batch Gradient Norm after: 13.193551388945275
Epoch 5543/10000, Prediction Accuracy = 63.162%, Loss = 0.343193393945694
Epoch: 5543, Batch Gradient Norm: 10.800214435737054
Epoch: 5543, Batch Gradient Norm after: 10.800214435737054
Epoch 5544/10000, Prediction Accuracy = 63.354%, Loss = 0.33856111764907837
Epoch: 5544, Batch Gradient Norm: 11.030716967929461
Epoch: 5544, Batch Gradient Norm after: 11.030716967929461
Epoch 5545/10000, Prediction Accuracy = 63.34799999999999%, Loss = 0.3404600262641907
Epoch: 5545, Batch Gradient Norm: 14.887765429412852
Epoch: 5545, Batch Gradient Norm after: 14.887765429412852
Epoch 5546/10000, Prediction Accuracy = 63.214%, Loss = 0.34840832352638246
Epoch: 5546, Batch Gradient Norm: 14.63733322056601
Epoch: 5546, Batch Gradient Norm after: 14.63733322056601
Epoch 5547/10000, Prediction Accuracy = 63.27%, Loss = 0.343615460395813
Epoch: 5547, Batch Gradient Norm: 15.996351684428847
Epoch: 5547, Batch Gradient Norm after: 15.996351684428847
Epoch 5548/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3465337336063385
Epoch: 5548, Batch Gradient Norm: 14.11305645615517
Epoch: 5548, Batch Gradient Norm after: 14.11305645615517
Epoch 5549/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.34247115850448606
Epoch: 5549, Batch Gradient Norm: 15.692174238100208
Epoch: 5549, Batch Gradient Norm after: 15.692174238100208
Epoch 5550/10000, Prediction Accuracy = 63.222%, Loss = 0.3455170750617981
Epoch: 5550, Batch Gradient Norm: 12.810383660119406
Epoch: 5550, Batch Gradient Norm after: 12.810383660119406
Epoch 5551/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3398858428001404
Epoch: 5551, Batch Gradient Norm: 12.426800251865973
Epoch: 5551, Batch Gradient Norm after: 12.426800251865973
Epoch 5552/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3399499595165253
Epoch: 5552, Batch Gradient Norm: 10.708747305031881
Epoch: 5552, Batch Gradient Norm after: 10.708747305031881
Epoch 5553/10000, Prediction Accuracy = 63.214%, Loss = 0.33833914399147036
Epoch: 5553, Batch Gradient Norm: 9.123071796003376
Epoch: 5553, Batch Gradient Norm after: 9.123071796003376
Epoch 5554/10000, Prediction Accuracy = 63.33%, Loss = 0.3389767110347748
Epoch: 5554, Batch Gradient Norm: 9.500259096695785
Epoch: 5554, Batch Gradient Norm after: 9.500259096695785
Epoch 5555/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3398604154586792
Epoch: 5555, Batch Gradient Norm: 10.538626735461376
Epoch: 5555, Batch Gradient Norm after: 10.538626735461376
Epoch 5556/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.33746362328529356
Epoch: 5556, Batch Gradient Norm: 11.42293880810592
Epoch: 5556, Batch Gradient Norm after: 11.42293880810592
Epoch 5557/10000, Prediction Accuracy = 63.312%, Loss = 0.3388511955738068
Epoch: 5557, Batch Gradient Norm: 12.104672334960368
Epoch: 5557, Batch Gradient Norm after: 12.104672334960368
Epoch 5558/10000, Prediction Accuracy = 63.28000000000001%, Loss = 0.3408044159412384
Epoch: 5558, Batch Gradient Norm: 14.35042025618555
Epoch: 5558, Batch Gradient Norm after: 14.35042025618555
Epoch 5559/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.3456328630447388
Epoch: 5559, Batch Gradient Norm: 12.682333705761259
Epoch: 5559, Batch Gradient Norm after: 12.682333705761259
Epoch 5560/10000, Prediction Accuracy = 63.282000000000004%, Loss = 0.3418851673603058
Epoch: 5560, Batch Gradient Norm: 11.641943028777069
Epoch: 5560, Batch Gradient Norm after: 11.641943028777069
Epoch 5561/10000, Prediction Accuracy = 63.274%, Loss = 0.33896890878677366
Epoch: 5561, Batch Gradient Norm: 12.476924355423595
Epoch: 5561, Batch Gradient Norm after: 12.476924355423595
Epoch 5562/10000, Prediction Accuracy = 63.222%, Loss = 0.34225393533706666
Epoch: 5562, Batch Gradient Norm: 13.121388408238477
Epoch: 5562, Batch Gradient Norm after: 13.121388408238477
Epoch 5563/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.3413504183292389
Epoch: 5563, Batch Gradient Norm: 14.073084324843531
Epoch: 5563, Batch Gradient Norm after: 14.073084324843531
Epoch 5564/10000, Prediction Accuracy = 63.388%, Loss = 0.3414384722709656
Epoch: 5564, Batch Gradient Norm: 15.606342931598954
Epoch: 5564, Batch Gradient Norm after: 15.606342931598954
Epoch 5565/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.3461477518081665
Epoch: 5565, Batch Gradient Norm: 14.222803602934729
Epoch: 5565, Batch Gradient Norm after: 14.222803602934729
Epoch 5566/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.343982470035553
Epoch: 5566, Batch Gradient Norm: 14.604730983462662
Epoch: 5566, Batch Gradient Norm after: 14.604730983462662
Epoch 5567/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3442539215087891
Epoch: 5567, Batch Gradient Norm: 16.499447468648594
Epoch: 5567, Batch Gradient Norm after: 16.499447468648594
Epoch 5568/10000, Prediction Accuracy = 63.20400000000001%, Loss = 0.346263587474823
Epoch: 5568, Batch Gradient Norm: 15.573973635707382
Epoch: 5568, Batch Gradient Norm after: 15.573973635707382
Epoch 5569/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.34712429642677306
Epoch: 5569, Batch Gradient Norm: 15.13421112334637
Epoch: 5569, Batch Gradient Norm after: 15.13421112334637
Epoch 5570/10000, Prediction Accuracy = 63.174%, Loss = 0.34376848936080934
Epoch: 5570, Batch Gradient Norm: 16.700493652322386
Epoch: 5570, Batch Gradient Norm after: 16.700493652322386
Epoch 5571/10000, Prediction Accuracy = 63.254%, Loss = 0.34575003385543823
Epoch: 5571, Batch Gradient Norm: 11.941385215621342
Epoch: 5571, Batch Gradient Norm after: 11.941385215621342
Epoch 5572/10000, Prediction Accuracy = 63.342%, Loss = 0.33972967863082887
Epoch: 5572, Batch Gradient Norm: 11.212504777657333
Epoch: 5572, Batch Gradient Norm after: 11.212504777657333
Epoch 5573/10000, Prediction Accuracy = 63.326%, Loss = 0.33766870498657225
Epoch: 5573, Batch Gradient Norm: 10.488263068085121
Epoch: 5573, Batch Gradient Norm after: 10.488263068085121
Epoch 5574/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3389925003051758
Epoch: 5574, Batch Gradient Norm: 12.38294256220762
Epoch: 5574, Batch Gradient Norm after: 12.38294256220762
Epoch 5575/10000, Prediction Accuracy = 63.338%, Loss = 0.3417137563228607
Epoch: 5575, Batch Gradient Norm: 13.94214218937368
Epoch: 5575, Batch Gradient Norm after: 13.94214218937368
Epoch 5576/10000, Prediction Accuracy = 63.138%, Loss = 0.34382748007774355
Epoch: 5576, Batch Gradient Norm: 17.826848613424744
Epoch: 5576, Batch Gradient Norm after: 17.0820503077327
Epoch 5577/10000, Prediction Accuracy = 63.378%, Loss = 0.34957789778709414
Epoch: 5577, Batch Gradient Norm: 17.453193227218822
Epoch: 5577, Batch Gradient Norm after: 17.209618274025758
Epoch 5578/10000, Prediction Accuracy = 63.23%, Loss = 0.3471609830856323
Epoch: 5578, Batch Gradient Norm: 17.504038045281934
Epoch: 5578, Batch Gradient Norm after: 16.266586473189015
Epoch 5579/10000, Prediction Accuracy = 63.17%, Loss = 0.350432687997818
Epoch: 5579, Batch Gradient Norm: 18.027134273367242
Epoch: 5579, Batch Gradient Norm after: 17.4876674577689
Epoch 5580/10000, Prediction Accuracy = 63.374%, Loss = 0.3490454614162445
Epoch: 5580, Batch Gradient Norm: 15.938483945493017
Epoch: 5580, Batch Gradient Norm after: 15.938483945493017
Epoch 5581/10000, Prediction Accuracy = 63.206%, Loss = 0.34600051045417785
Epoch: 5581, Batch Gradient Norm: 15.487849562495802
Epoch: 5581, Batch Gradient Norm after: 15.487849562495802
Epoch 5582/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.3457935869693756
Epoch: 5582, Batch Gradient Norm: 16.36654213585595
Epoch: 5582, Batch Gradient Norm after: 16.36654213585595
Epoch 5583/10000, Prediction Accuracy = 63.258%, Loss = 0.34730095267295835
Epoch: 5583, Batch Gradient Norm: 14.609506078222669
Epoch: 5583, Batch Gradient Norm after: 14.609506078222669
Epoch 5584/10000, Prediction Accuracy = 63.14200000000001%, Loss = 0.3447263062000275
Epoch: 5584, Batch Gradient Norm: 14.04648696073052
Epoch: 5584, Batch Gradient Norm after: 14.04648696073052
Epoch 5585/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3457189738750458
Epoch: 5585, Batch Gradient Norm: 15.48256077552739
Epoch: 5585, Batch Gradient Norm after: 15.48256077552739
Epoch 5586/10000, Prediction Accuracy = 63.088%, Loss = 0.3511992871761322
Epoch: 5586, Batch Gradient Norm: 14.5327571786877
Epoch: 5586, Batch Gradient Norm after: 14.5327571786877
Epoch 5587/10000, Prediction Accuracy = 63.238%, Loss = 0.3435601532459259
Epoch: 5587, Batch Gradient Norm: 11.719342366288215
Epoch: 5587, Batch Gradient Norm after: 11.719342366288215
Epoch 5588/10000, Prediction Accuracy = 63.188%, Loss = 0.3403928279876709
Epoch: 5588, Batch Gradient Norm: 12.50973794100413
Epoch: 5588, Batch Gradient Norm after: 12.50973794100413
Epoch 5589/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.33819152116775514
Epoch: 5589, Batch Gradient Norm: 14.763121335796217
Epoch: 5589, Batch Gradient Norm after: 14.763121335796217
Epoch 5590/10000, Prediction Accuracy = 63.166%, Loss = 0.3427903473377228
Epoch: 5590, Batch Gradient Norm: 14.233347308364861
Epoch: 5590, Batch Gradient Norm after: 14.233347308364861
Epoch 5591/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.3408534526824951
Epoch: 5591, Batch Gradient Norm: 15.833924495755895
Epoch: 5591, Batch Gradient Norm after: 15.833924495755895
Epoch 5592/10000, Prediction Accuracy = 63.13599999999999%, Loss = 0.34298760294914243
Epoch: 5592, Batch Gradient Norm: 15.619335083715903
Epoch: 5592, Batch Gradient Norm after: 15.619335083715903
Epoch 5593/10000, Prediction Accuracy = 63.21%, Loss = 0.3442253053188324
Epoch: 5593, Batch Gradient Norm: 15.579828213976809
Epoch: 5593, Batch Gradient Norm after: 15.579828213976809
Epoch 5594/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.3424068093299866
Epoch: 5594, Batch Gradient Norm: 12.964924618604252
Epoch: 5594, Batch Gradient Norm after: 12.964924618604252
Epoch 5595/10000, Prediction Accuracy = 63.376%, Loss = 0.33972095251083373
Epoch: 5595, Batch Gradient Norm: 11.262796880019899
Epoch: 5595, Batch Gradient Norm after: 11.262796880019899
Epoch 5596/10000, Prediction Accuracy = 63.30999999999999%, Loss = 0.337714684009552
Epoch: 5596, Batch Gradient Norm: 11.939524106106635
Epoch: 5596, Batch Gradient Norm after: 11.939524106106635
Epoch 5597/10000, Prediction Accuracy = 63.194%, Loss = 0.33807823061943054
Epoch: 5597, Batch Gradient Norm: 14.250948487778984
Epoch: 5597, Batch Gradient Norm after: 14.250948487778984
Epoch 5598/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.34044442772865297
Epoch: 5598, Batch Gradient Norm: 11.540774754278065
Epoch: 5598, Batch Gradient Norm after: 11.540774754278065
Epoch 5599/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3388269543647766
Epoch: 5599, Batch Gradient Norm: 12.06876315096169
Epoch: 5599, Batch Gradient Norm after: 12.06876315096169
Epoch 5600/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.33685058951377866
Epoch: 5600, Batch Gradient Norm: 11.459754967318664
Epoch: 5600, Batch Gradient Norm after: 11.459754967318664
Epoch 5601/10000, Prediction Accuracy = 63.29%, Loss = 0.3377496063709259
Epoch: 5601, Batch Gradient Norm: 14.6440206824187
Epoch: 5601, Batch Gradient Norm after: 14.6440206824187
Epoch 5602/10000, Prediction Accuracy = 63.30800000000001%, Loss = 0.3430372714996338
Epoch: 5602, Batch Gradient Norm: 14.076598629073514
Epoch: 5602, Batch Gradient Norm after: 14.076598629073514
Epoch 5603/10000, Prediction Accuracy = 63.31%, Loss = 0.3419682800769806
Epoch: 5603, Batch Gradient Norm: 11.57940280605259
Epoch: 5603, Batch Gradient Norm after: 11.57940280605259
Epoch 5604/10000, Prediction Accuracy = 63.282000000000004%, Loss = 0.3382751166820526
Epoch: 5604, Batch Gradient Norm: 12.892091825957493
Epoch: 5604, Batch Gradient Norm after: 12.892091825957493
Epoch 5605/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3410541176795959
Epoch: 5605, Batch Gradient Norm: 13.162706506595757
Epoch: 5605, Batch Gradient Norm after: 13.162706506595757
Epoch 5606/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.34064791202545164
Epoch: 5606, Batch Gradient Norm: 12.39420239247385
Epoch: 5606, Batch Gradient Norm after: 12.39420239247385
Epoch 5607/10000, Prediction Accuracy = 63.35999999999999%, Loss = 0.34028889536857604
Epoch: 5607, Batch Gradient Norm: 11.238744171326365
Epoch: 5607, Batch Gradient Norm after: 11.238744171326365
Epoch 5608/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.33787832260131834
Epoch: 5608, Batch Gradient Norm: 10.240813334487527
Epoch: 5608, Batch Gradient Norm after: 10.240813334487527
Epoch 5609/10000, Prediction Accuracy = 63.176%, Loss = 0.33597723841667176
Epoch: 5609, Batch Gradient Norm: 13.390390948643711
Epoch: 5609, Batch Gradient Norm after: 13.390390948643711
Epoch 5610/10000, Prediction Accuracy = 63.336%, Loss = 0.34266866445541383
Epoch: 5610, Batch Gradient Norm: 14.969417196625713
Epoch: 5610, Batch Gradient Norm after: 14.969417196625713
Epoch 5611/10000, Prediction Accuracy = 63.318%, Loss = 0.34287900328636167
Epoch: 5611, Batch Gradient Norm: 16.59161548023239
Epoch: 5611, Batch Gradient Norm after: 16.59161548023239
Epoch 5612/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3456515550613403
Epoch: 5612, Batch Gradient Norm: 15.677031580720957
Epoch: 5612, Batch Gradient Norm after: 15.677031580720957
Epoch 5613/10000, Prediction Accuracy = 63.35%, Loss = 0.3436014652252197
Epoch: 5613, Batch Gradient Norm: 12.785601284449069
Epoch: 5613, Batch Gradient Norm after: 12.785601284449069
Epoch 5614/10000, Prediction Accuracy = 63.282000000000004%, Loss = 0.3422537982463837
Epoch: 5614, Batch Gradient Norm: 14.742960022658995
Epoch: 5614, Batch Gradient Norm after: 14.742960022658995
Epoch 5615/10000, Prediction Accuracy = 63.318%, Loss = 0.3432778775691986
Epoch: 5615, Batch Gradient Norm: 13.85763330864424
Epoch: 5615, Batch Gradient Norm after: 13.85763330864424
Epoch 5616/10000, Prediction Accuracy = 63.40999999999999%, Loss = 0.34098913073539733
Epoch: 5616, Batch Gradient Norm: 14.941523247163484
Epoch: 5616, Batch Gradient Norm after: 14.941523247163484
Epoch 5617/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.34022338390350343
Epoch: 5617, Batch Gradient Norm: 15.607467202156656
Epoch: 5617, Batch Gradient Norm after: 15.607467202156656
Epoch 5618/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.3421526372432709
Epoch: 5618, Batch Gradient Norm: 16.774916308440094
Epoch: 5618, Batch Gradient Norm after: 16.774916308440094
Epoch 5619/10000, Prediction Accuracy = 63.3%, Loss = 0.3451786577701569
Epoch: 5619, Batch Gradient Norm: 14.498845922537187
Epoch: 5619, Batch Gradient Norm after: 14.498845922537187
Epoch 5620/10000, Prediction Accuracy = 63.286%, Loss = 0.3413983225822449
Epoch: 5620, Batch Gradient Norm: 11.189559375930495
Epoch: 5620, Batch Gradient Norm after: 11.189559375930495
Epoch 5621/10000, Prediction Accuracy = 63.314%, Loss = 0.3391937553882599
Epoch: 5621, Batch Gradient Norm: 12.892419761959767
Epoch: 5621, Batch Gradient Norm after: 12.892419761959767
Epoch 5622/10000, Prediction Accuracy = 63.326%, Loss = 0.33911298513412474
Epoch: 5622, Batch Gradient Norm: 10.636068705956294
Epoch: 5622, Batch Gradient Norm after: 10.636068705956294
Epoch 5623/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.33618972897529603
Epoch: 5623, Batch Gradient Norm: 12.365788943870166
Epoch: 5623, Batch Gradient Norm after: 12.365788943870166
Epoch 5624/10000, Prediction Accuracy = 63.278%, Loss = 0.342656409740448
Epoch: 5624, Batch Gradient Norm: 14.080087566517951
Epoch: 5624, Batch Gradient Norm after: 14.080087566517951
Epoch 5625/10000, Prediction Accuracy = 63.32199999999999%, Loss = 0.3418669879436493
Epoch: 5625, Batch Gradient Norm: 12.986992776101024
Epoch: 5625, Batch Gradient Norm after: 12.986992776101024
Epoch 5626/10000, Prediction Accuracy = 63.331999999999994%, Loss = 0.3398102104663849
Epoch: 5626, Batch Gradient Norm: 12.567697317989838
Epoch: 5626, Batch Gradient Norm after: 12.567697317989838
Epoch 5627/10000, Prediction Accuracy = 63.262%, Loss = 0.3380428612232208
Epoch: 5627, Batch Gradient Norm: 14.041638283720983
Epoch: 5627, Batch Gradient Norm after: 14.041638283720983
Epoch 5628/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.34189714789390563
Epoch: 5628, Batch Gradient Norm: 12.371794946813894
Epoch: 5628, Batch Gradient Norm after: 12.371794946813894
Epoch 5629/10000, Prediction Accuracy = 63.428%, Loss = 0.33963931202888487
Epoch: 5629, Batch Gradient Norm: 12.408706694857088
Epoch: 5629, Batch Gradient Norm after: 12.408706694857088
Epoch 5630/10000, Prediction Accuracy = 63.226%, Loss = 0.34122189283370974
Epoch: 5630, Batch Gradient Norm: 12.573465975965716
Epoch: 5630, Batch Gradient Norm after: 12.573465975965716
Epoch 5631/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.3376983642578125
Epoch: 5631, Batch Gradient Norm: 10.643501086277544
Epoch: 5631, Batch Gradient Norm after: 10.643501086277544
Epoch 5632/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.33582786917686464
Epoch: 5632, Batch Gradient Norm: 11.395104073297054
Epoch: 5632, Batch Gradient Norm after: 11.395104073297054
Epoch 5633/10000, Prediction Accuracy = 63.312%, Loss = 0.33842750191688536
Epoch: 5633, Batch Gradient Norm: 11.319777463291361
Epoch: 5633, Batch Gradient Norm after: 11.319777463291361
Epoch 5634/10000, Prediction Accuracy = 63.25%, Loss = 0.33832101821899413
Epoch: 5634, Batch Gradient Norm: 10.277461149999255
Epoch: 5634, Batch Gradient Norm after: 10.277461149999255
Epoch 5635/10000, Prediction Accuracy = 63.18799999999999%, Loss = 0.33738742470741273
Epoch: 5635, Batch Gradient Norm: 11.869956468872497
Epoch: 5635, Batch Gradient Norm after: 11.869956468872497
Epoch 5636/10000, Prediction Accuracy = 63.278%, Loss = 0.33869712948799136
Epoch: 5636, Batch Gradient Norm: 10.18429513143051
Epoch: 5636, Batch Gradient Norm after: 10.18429513143051
Epoch 5637/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3379405975341797
Epoch: 5637, Batch Gradient Norm: 11.640249353663053
Epoch: 5637, Batch Gradient Norm after: 11.640249353663053
Epoch 5638/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.3377715051174164
Epoch: 5638, Batch Gradient Norm: 12.622410529135726
Epoch: 5638, Batch Gradient Norm after: 12.622410529135726
Epoch 5639/10000, Prediction Accuracy = 63.352%, Loss = 0.3367870688438416
Epoch: 5639, Batch Gradient Norm: 12.806610599885802
Epoch: 5639, Batch Gradient Norm after: 12.806610599885802
Epoch 5640/10000, Prediction Accuracy = 63.354%, Loss = 0.3389352262020111
Epoch: 5640, Batch Gradient Norm: 13.86350232459287
Epoch: 5640, Batch Gradient Norm after: 13.86350232459287
Epoch 5641/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.33797062039375303
Epoch: 5641, Batch Gradient Norm: 13.737927978160323
Epoch: 5641, Batch Gradient Norm after: 13.737927978160323
Epoch 5642/10000, Prediction Accuracy = 63.27%, Loss = 0.34002506732940674
Epoch: 5642, Batch Gradient Norm: 14.748646810287536
Epoch: 5642, Batch Gradient Norm after: 14.748646810287536
Epoch 5643/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.34155806303024294
Epoch: 5643, Batch Gradient Norm: 14.220921123717275
Epoch: 5643, Batch Gradient Norm after: 14.220921123717275
Epoch 5644/10000, Prediction Accuracy = 63.402%, Loss = 0.34331275820732116
Epoch: 5644, Batch Gradient Norm: 12.119554180158952
Epoch: 5644, Batch Gradient Norm after: 12.119554180158952
Epoch 5645/10000, Prediction Accuracy = 63.29%, Loss = 0.3381194889545441
Epoch: 5645, Batch Gradient Norm: 16.137435543386133
Epoch: 5645, Batch Gradient Norm after: 16.137435543386133
Epoch 5646/10000, Prediction Accuracy = 63.206%, Loss = 0.3476662874221802
Epoch: 5646, Batch Gradient Norm: 16.50307372787803
Epoch: 5646, Batch Gradient Norm after: 16.50307372787803
Epoch 5647/10000, Prediction Accuracy = 63.366%, Loss = 0.345453667640686
Epoch: 5647, Batch Gradient Norm: 18.01748919052636
Epoch: 5647, Batch Gradient Norm after: 18.01748919052636
Epoch 5648/10000, Prediction Accuracy = 63.298%, Loss = 0.3486762404441833
Epoch: 5648, Batch Gradient Norm: 20.503176644152088
Epoch: 5648, Batch Gradient Norm after: 18.719908657897136
Epoch 5649/10000, Prediction Accuracy = 63.209999999999994%, Loss = 0.35363603234291074
Epoch: 5649, Batch Gradient Norm: 18.9597675326871
Epoch: 5649, Batch Gradient Norm after: 18.785540827194975
Epoch 5650/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.3488806188106537
Epoch: 5650, Batch Gradient Norm: 14.377529652414585
Epoch: 5650, Batch Gradient Norm after: 14.377529652414585
Epoch 5651/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.34006489515304567
Epoch: 5651, Batch Gradient Norm: 13.711338404453427
Epoch: 5651, Batch Gradient Norm after: 13.711338404453427
Epoch 5652/10000, Prediction Accuracy = 63.39%, Loss = 0.3405530333518982
Epoch: 5652, Batch Gradient Norm: 14.336414407296274
Epoch: 5652, Batch Gradient Norm after: 14.336414407296274
Epoch 5653/10000, Prediction Accuracy = 63.298%, Loss = 0.3430417597293854
Epoch: 5653, Batch Gradient Norm: 13.485539505529454
Epoch: 5653, Batch Gradient Norm after: 13.485539505529454
Epoch 5654/10000, Prediction Accuracy = 63.266%, Loss = 0.3381746709346771
Epoch: 5654, Batch Gradient Norm: 14.50474126818337
Epoch: 5654, Batch Gradient Norm after: 14.50474126818337
Epoch 5655/10000, Prediction Accuracy = 63.382000000000005%, Loss = 0.3413726449012756
Epoch: 5655, Batch Gradient Norm: 15.668992572546726
Epoch: 5655, Batch Gradient Norm after: 15.668992572546726
Epoch 5656/10000, Prediction Accuracy = 63.338%, Loss = 0.3415838062763214
Epoch: 5656, Batch Gradient Norm: 16.826621812029977
Epoch: 5656, Batch Gradient Norm after: 16.826621812029977
Epoch 5657/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.34331763386726377
Epoch: 5657, Batch Gradient Norm: 14.783943743146827
Epoch: 5657, Batch Gradient Norm after: 14.783943743146827
Epoch 5658/10000, Prediction Accuracy = 63.298%, Loss = 0.3408586919307709
Epoch: 5658, Batch Gradient Norm: 14.390199645454993
Epoch: 5658, Batch Gradient Norm after: 14.390199645454993
Epoch 5659/10000, Prediction Accuracy = 63.45%, Loss = 0.34139648675918577
Epoch: 5659, Batch Gradient Norm: 14.417915922721358
Epoch: 5659, Batch Gradient Norm after: 14.417915922721358
Epoch 5660/10000, Prediction Accuracy = 63.318%, Loss = 0.3419730603694916
Epoch: 5660, Batch Gradient Norm: 12.847394812909315
Epoch: 5660, Batch Gradient Norm after: 12.847394812909315
Epoch 5661/10000, Prediction Accuracy = 63.342%, Loss = 0.34216315150260923
Epoch: 5661, Batch Gradient Norm: 9.59642203282959
Epoch: 5661, Batch Gradient Norm after: 9.59642203282959
Epoch 5662/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.3350277066230774
Epoch: 5662, Batch Gradient Norm: 10.324436656640783
Epoch: 5662, Batch Gradient Norm after: 10.324436656640783
Epoch 5663/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3348811030387878
Epoch: 5663, Batch Gradient Norm: 10.691948782594485
Epoch: 5663, Batch Gradient Norm after: 10.691948782594485
Epoch 5664/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.3368375599384308
Epoch: 5664, Batch Gradient Norm: 11.92635772757331
Epoch: 5664, Batch Gradient Norm after: 11.92635772757331
Epoch 5665/10000, Prediction Accuracy = 63.36%, Loss = 0.33709325790405276
Epoch: 5665, Batch Gradient Norm: 15.316925014448563
Epoch: 5665, Batch Gradient Norm after: 15.316925014448563
Epoch 5666/10000, Prediction Accuracy = 63.352%, Loss = 0.3416935861110687
Epoch: 5666, Batch Gradient Norm: 16.264042239145635
Epoch: 5666, Batch Gradient Norm after: 16.264042239145635
Epoch 5667/10000, Prediction Accuracy = 63.274%, Loss = 0.345284241437912
Epoch: 5667, Batch Gradient Norm: 17.079661620951356
Epoch: 5667, Batch Gradient Norm after: 16.63665695283219
Epoch 5668/10000, Prediction Accuracy = 63.372%, Loss = 0.3429788827896118
Epoch: 5668, Batch Gradient Norm: 15.73414101127294
Epoch: 5668, Batch Gradient Norm after: 15.73414101127294
Epoch 5669/10000, Prediction Accuracy = 63.314%, Loss = 0.3409532904624939
Epoch: 5669, Batch Gradient Norm: 14.063728261153255
Epoch: 5669, Batch Gradient Norm after: 14.063728261153255
Epoch 5670/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.3397119164466858
Epoch: 5670, Batch Gradient Norm: 11.656021911177437
Epoch: 5670, Batch Gradient Norm after: 11.656021911177437
Epoch 5671/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3367709696292877
Epoch: 5671, Batch Gradient Norm: 9.42372948252418
Epoch: 5671, Batch Gradient Norm after: 9.42372948252418
Epoch 5672/10000, Prediction Accuracy = 63.36%, Loss = 0.33323427438735964
Epoch: 5672, Batch Gradient Norm: 11.386828607843185
Epoch: 5672, Batch Gradient Norm after: 11.386828607843185
Epoch 5673/10000, Prediction Accuracy = 63.21%, Loss = 0.33903650641441346
Epoch: 5673, Batch Gradient Norm: 12.858581323953214
Epoch: 5673, Batch Gradient Norm after: 12.858581323953214
Epoch 5674/10000, Prediction Accuracy = 63.266%, Loss = 0.34151182174682615
Epoch: 5674, Batch Gradient Norm: 13.032485029192188
Epoch: 5674, Batch Gradient Norm after: 13.032485029192188
Epoch 5675/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.33781089186668395
Epoch: 5675, Batch Gradient Norm: 13.416675241297387
Epoch: 5675, Batch Gradient Norm after: 13.416675241297387
Epoch 5676/10000, Prediction Accuracy = 63.288%, Loss = 0.34019858837127687
Epoch: 5676, Batch Gradient Norm: 10.830190535463743
Epoch: 5676, Batch Gradient Norm after: 10.830190535463743
Epoch 5677/10000, Prediction Accuracy = 63.444%, Loss = 0.3357895493507385
Epoch: 5677, Batch Gradient Norm: 13.973899980858945
Epoch: 5677, Batch Gradient Norm after: 13.973899980858945
Epoch 5678/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.34251760244369506
Epoch: 5678, Batch Gradient Norm: 12.570323171228251
Epoch: 5678, Batch Gradient Norm after: 12.570323171228251
Epoch 5679/10000, Prediction Accuracy = 63.254%, Loss = 0.34084432721138
Epoch: 5679, Batch Gradient Norm: 11.709896756846833
Epoch: 5679, Batch Gradient Norm after: 11.709896756846833
Epoch 5680/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.33536783456802366
Epoch: 5680, Batch Gradient Norm: 10.369361440891186
Epoch: 5680, Batch Gradient Norm after: 10.369361440891186
Epoch 5681/10000, Prediction Accuracy = 63.36%, Loss = 0.3355857789516449
Epoch: 5681, Batch Gradient Norm: 13.526371709105579
Epoch: 5681, Batch Gradient Norm after: 13.526371709105579
Epoch 5682/10000, Prediction Accuracy = 63.27%, Loss = 0.3410106062889099
Epoch: 5682, Batch Gradient Norm: 12.926351933515722
Epoch: 5682, Batch Gradient Norm after: 12.926351933515722
Epoch 5683/10000, Prediction Accuracy = 63.288%, Loss = 0.33731989860534667
Epoch: 5683, Batch Gradient Norm: 11.879813699103584
Epoch: 5683, Batch Gradient Norm after: 11.879813699103584
Epoch 5684/10000, Prediction Accuracy = 63.34599999999999%, Loss = 0.3389113128185272
Epoch: 5684, Batch Gradient Norm: 11.192714587415802
Epoch: 5684, Batch Gradient Norm after: 11.192714587415802
Epoch 5685/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.33535093665122984
Epoch: 5685, Batch Gradient Norm: 14.264442987904282
Epoch: 5685, Batch Gradient Norm after: 14.264442987904282
Epoch 5686/10000, Prediction Accuracy = 63.41600000000001%, Loss = 0.33993632197380064
Epoch: 5686, Batch Gradient Norm: 15.877595362995997
Epoch: 5686, Batch Gradient Norm after: 15.877595362995997
Epoch 5687/10000, Prediction Accuracy = 63.443999999999996%, Loss = 0.34182158708572385
Epoch: 5687, Batch Gradient Norm: 14.976040203658561
Epoch: 5687, Batch Gradient Norm after: 14.976040203658561
Epoch 5688/10000, Prediction Accuracy = 63.438%, Loss = 0.34003445506095886
Epoch: 5688, Batch Gradient Norm: 14.127801387317897
Epoch: 5688, Batch Gradient Norm after: 14.127801387317897
Epoch 5689/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.33995345830917356
Epoch: 5689, Batch Gradient Norm: 12.398310784568315
Epoch: 5689, Batch Gradient Norm after: 12.398310784568315
Epoch 5690/10000, Prediction Accuracy = 63.352%, Loss = 0.33698886036872866
Epoch: 5690, Batch Gradient Norm: 12.090702782077937
Epoch: 5690, Batch Gradient Norm after: 12.090702782077937
Epoch 5691/10000, Prediction Accuracy = 63.407999999999994%, Loss = 0.33587891459465025
Epoch: 5691, Batch Gradient Norm: 11.315108197804117
Epoch: 5691, Batch Gradient Norm after: 11.315108197804117
Epoch 5692/10000, Prediction Accuracy = 63.354%, Loss = 0.3359070360660553
Epoch: 5692, Batch Gradient Norm: 13.1747946641673
Epoch: 5692, Batch Gradient Norm after: 13.1747946641673
Epoch 5693/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.33909791707992554
Epoch: 5693, Batch Gradient Norm: 13.525796896712892
Epoch: 5693, Batch Gradient Norm after: 13.525796896712892
Epoch 5694/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.33771814703941344
Epoch: 5694, Batch Gradient Norm: 13.581992277464359
Epoch: 5694, Batch Gradient Norm after: 13.581992277464359
Epoch 5695/10000, Prediction Accuracy = 63.406000000000006%, Loss = 0.3373138904571533
Epoch: 5695, Batch Gradient Norm: 10.990560152420933
Epoch: 5695, Batch Gradient Norm after: 10.990560152420933
Epoch 5696/10000, Prediction Accuracy = 63.412%, Loss = 0.33741800785064696
Epoch: 5696, Batch Gradient Norm: 14.898659353916543
Epoch: 5696, Batch Gradient Norm after: 14.898659353916543
Epoch 5697/10000, Prediction Accuracy = 63.33399999999999%, Loss = 0.3399496376514435
Epoch: 5697, Batch Gradient Norm: 13.471145265674425
Epoch: 5697, Batch Gradient Norm after: 13.471145265674425
Epoch 5698/10000, Prediction Accuracy = 63.27%, Loss = 0.33885214328765867
Epoch: 5698, Batch Gradient Norm: 12.571653032021853
Epoch: 5698, Batch Gradient Norm after: 12.571653032021853
Epoch 5699/10000, Prediction Accuracy = 63.33800000000001%, Loss = 0.33802309036254885
Epoch: 5699, Batch Gradient Norm: 12.810151467437104
Epoch: 5699, Batch Gradient Norm after: 12.810151467437104
Epoch 5700/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3397270679473877
Epoch: 5700, Batch Gradient Norm: 14.47934437999662
Epoch: 5700, Batch Gradient Norm after: 14.47934437999662
Epoch 5701/10000, Prediction Accuracy = 63.21199999999999%, Loss = 0.33948882222175597
Epoch: 5701, Batch Gradient Norm: 12.50352781542652
Epoch: 5701, Batch Gradient Norm after: 12.50352781542652
Epoch 5702/10000, Prediction Accuracy = 63.398%, Loss = 0.33747472763061526
Epoch: 5702, Batch Gradient Norm: 10.460151320975898
Epoch: 5702, Batch Gradient Norm after: 10.460151320975898
Epoch 5703/10000, Prediction Accuracy = 63.3%, Loss = 0.33636896014213563
Epoch: 5703, Batch Gradient Norm: 11.93207413659776
Epoch: 5703, Batch Gradient Norm after: 11.93207413659776
Epoch 5704/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3369600296020508
Epoch: 5704, Batch Gradient Norm: 12.600252636728221
Epoch: 5704, Batch Gradient Norm after: 12.600252636728221
Epoch 5705/10000, Prediction Accuracy = 63.282%, Loss = 0.3401314616203308
Epoch: 5705, Batch Gradient Norm: 14.348418809559766
Epoch: 5705, Batch Gradient Norm after: 14.348418809559766
Epoch 5706/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.338202702999115
Epoch: 5706, Batch Gradient Norm: 11.701034153468463
Epoch: 5706, Batch Gradient Norm after: 11.701034153468463
Epoch 5707/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.3391734898090363
Epoch: 5707, Batch Gradient Norm: 10.654653375211364
Epoch: 5707, Batch Gradient Norm after: 10.654653375211364
Epoch 5708/10000, Prediction Accuracy = 63.348%, Loss = 0.335305255651474
Epoch: 5708, Batch Gradient Norm: 11.637290812681268
Epoch: 5708, Batch Gradient Norm after: 11.637290812681268
Epoch 5709/10000, Prediction Accuracy = 63.431999999999995%, Loss = 0.3367944240570068
Epoch: 5709, Batch Gradient Norm: 13.786738098952535
Epoch: 5709, Batch Gradient Norm after: 13.786738098952535
Epoch 5710/10000, Prediction Accuracy = 63.496%, Loss = 0.33729897141456605
Epoch: 5710, Batch Gradient Norm: 11.416769420279076
Epoch: 5710, Batch Gradient Norm after: 11.416769420279076
Epoch 5711/10000, Prediction Accuracy = 63.544%, Loss = 0.3338285148143768
Epoch: 5711, Batch Gradient Norm: 12.899601170377895
Epoch: 5711, Batch Gradient Norm after: 12.899601170377895
Epoch 5712/10000, Prediction Accuracy = 63.41799999999999%, Loss = 0.3367413103580475
Epoch: 5712, Batch Gradient Norm: 12.929913105101319
Epoch: 5712, Batch Gradient Norm after: 12.929913105101319
Epoch 5713/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.33533493280410764
Epoch: 5713, Batch Gradient Norm: 14.861442652113308
Epoch: 5713, Batch Gradient Norm after: 14.861442652113308
Epoch 5714/10000, Prediction Accuracy = 63.254%, Loss = 0.34073954820632935
Epoch: 5714, Batch Gradient Norm: 11.857977042172609
Epoch: 5714, Batch Gradient Norm after: 11.857977042172609
Epoch 5715/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3362428545951843
Epoch: 5715, Batch Gradient Norm: 11.55091859379718
Epoch: 5715, Batch Gradient Norm after: 11.55091859379718
Epoch 5716/10000, Prediction Accuracy = 63.408%, Loss = 0.3360186338424683
Epoch: 5716, Batch Gradient Norm: 13.240456001972031
Epoch: 5716, Batch Gradient Norm after: 13.240456001972031
Epoch 5717/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.34045291543006895
Epoch: 5717, Batch Gradient Norm: 11.667975656312718
Epoch: 5717, Batch Gradient Norm after: 11.667975656312718
Epoch 5718/10000, Prediction Accuracy = 63.384%, Loss = 0.3364965498447418
Epoch: 5718, Batch Gradient Norm: 11.302620293243763
Epoch: 5718, Batch Gradient Norm after: 11.302620293243763
Epoch 5719/10000, Prediction Accuracy = 63.23%, Loss = 0.3376631736755371
Epoch: 5719, Batch Gradient Norm: 13.16460534795345
Epoch: 5719, Batch Gradient Norm after: 13.16460534795345
Epoch 5720/10000, Prediction Accuracy = 63.424%, Loss = 0.33625662326812744
Epoch: 5720, Batch Gradient Norm: 12.626778680458129
Epoch: 5720, Batch Gradient Norm after: 12.626778680458129
Epoch 5721/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3377721905708313
Epoch: 5721, Batch Gradient Norm: 12.423927461142782
Epoch: 5721, Batch Gradient Norm after: 12.423927461142782
Epoch 5722/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.334960526227951
Epoch: 5722, Batch Gradient Norm: 15.310638800520998
Epoch: 5722, Batch Gradient Norm after: 15.310638800520998
Epoch 5723/10000, Prediction Accuracy = 63.398%, Loss = 0.3399638831615448
Epoch: 5723, Batch Gradient Norm: 14.247724504997612
Epoch: 5723, Batch Gradient Norm after: 14.247724504997612
Epoch 5724/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.33989914655685427
Epoch: 5724, Batch Gradient Norm: 10.958664121511879
Epoch: 5724, Batch Gradient Norm after: 10.958664121511879
Epoch 5725/10000, Prediction Accuracy = 63.428%, Loss = 0.33504329323768617
Epoch: 5725, Batch Gradient Norm: 13.054563962667835
Epoch: 5725, Batch Gradient Norm after: 13.054563962667835
Epoch 5726/10000, Prediction Accuracy = 63.348%, Loss = 0.3373330056667328
Epoch: 5726, Batch Gradient Norm: 14.890437011846357
Epoch: 5726, Batch Gradient Norm after: 14.890437011846357
Epoch 5727/10000, Prediction Accuracy = 63.298%, Loss = 0.3407123386859894
Epoch: 5727, Batch Gradient Norm: 14.155147193324588
Epoch: 5727, Batch Gradient Norm after: 14.155147193324588
Epoch 5728/10000, Prediction Accuracy = 63.38599999999999%, Loss = 0.33962472677230837
Epoch: 5728, Batch Gradient Norm: 13.640591526983087
Epoch: 5728, Batch Gradient Norm after: 13.640591526983087
Epoch 5729/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.33864139914512636
Epoch: 5729, Batch Gradient Norm: 12.965908659239217
Epoch: 5729, Batch Gradient Norm after: 12.965908659239217
Epoch 5730/10000, Prediction Accuracy = 63.38800000000001%, Loss = 0.3374164938926697
Epoch: 5730, Batch Gradient Norm: 10.48383085666385
Epoch: 5730, Batch Gradient Norm after: 10.48383085666385
Epoch 5731/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.3328713536262512
Epoch: 5731, Batch Gradient Norm: 11.488143985702353
Epoch: 5731, Batch Gradient Norm after: 11.488143985702353
Epoch 5732/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3362878501415253
Epoch: 5732, Batch Gradient Norm: 13.840841419778814
Epoch: 5732, Batch Gradient Norm after: 13.840841419778814
Epoch 5733/10000, Prediction Accuracy = 63.42%, Loss = 0.33650888204574586
Epoch: 5733, Batch Gradient Norm: 12.446301286892709
Epoch: 5733, Batch Gradient Norm after: 12.446301286892709
Epoch 5734/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.33672531843185427
Epoch: 5734, Batch Gradient Norm: 12.902323729933968
Epoch: 5734, Batch Gradient Norm after: 12.902323729933968
Epoch 5735/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3355797827243805
Epoch: 5735, Batch Gradient Norm: 12.516920977911173
Epoch: 5735, Batch Gradient Norm after: 12.516920977911173
Epoch 5736/10000, Prediction Accuracy = 63.452%, Loss = 0.33544197082519533
Epoch: 5736, Batch Gradient Norm: 10.305579173981132
Epoch: 5736, Batch Gradient Norm after: 10.305579173981132
Epoch 5737/10000, Prediction Accuracy = 63.326%, Loss = 0.3328457295894623
Epoch: 5737, Batch Gradient Norm: 13.794244960718682
Epoch: 5737, Batch Gradient Norm after: 13.794244960718682
Epoch 5738/10000, Prediction Accuracy = 63.5%, Loss = 0.33809277415275574
Epoch: 5738, Batch Gradient Norm: 14.868988196254334
Epoch: 5738, Batch Gradient Norm after: 14.868988196254334
Epoch 5739/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3387868344783783
Epoch: 5739, Batch Gradient Norm: 11.624108672795526
Epoch: 5739, Batch Gradient Norm after: 11.624108672795526
Epoch 5740/10000, Prediction Accuracy = 63.548%, Loss = 0.3342975676059723
Epoch: 5740, Batch Gradient Norm: 15.809417197321773
Epoch: 5740, Batch Gradient Norm after: 15.809417197321773
Epoch 5741/10000, Prediction Accuracy = 63.35799999999999%, Loss = 0.34147401452064513
Epoch: 5741, Batch Gradient Norm: 14.804025571447644
Epoch: 5741, Batch Gradient Norm after: 14.804025571447644
Epoch 5742/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.3392723560333252
Epoch: 5742, Batch Gradient Norm: 17.978513922798715
Epoch: 5742, Batch Gradient Norm after: 17.477266842416263
Epoch 5743/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3469711899757385
Epoch: 5743, Batch Gradient Norm: 17.392640568704895
Epoch: 5743, Batch Gradient Norm after: 17.392640568704895
Epoch 5744/10000, Prediction Accuracy = 63.398%, Loss = 0.3429021656513214
Epoch: 5744, Batch Gradient Norm: 13.55236489155248
Epoch: 5744, Batch Gradient Norm after: 13.55236489155248
Epoch 5745/10000, Prediction Accuracy = 63.456%, Loss = 0.33682088255882264
Epoch: 5745, Batch Gradient Norm: 13.06225299416892
Epoch: 5745, Batch Gradient Norm after: 13.06225299416892
Epoch 5746/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.335366302728653
Epoch: 5746, Batch Gradient Norm: 14.682457291958627
Epoch: 5746, Batch Gradient Norm after: 14.682457291958627
Epoch 5747/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.3374905467033386
Epoch: 5747, Batch Gradient Norm: 15.41798689485937
Epoch: 5747, Batch Gradient Norm after: 15.41798689485937
Epoch 5748/10000, Prediction Accuracy = 63.43399999999999%, Loss = 0.33983645439147947
Epoch: 5748, Batch Gradient Norm: 16.441047777960666
Epoch: 5748, Batch Gradient Norm after: 15.293099891988861
Epoch 5749/10000, Prediction Accuracy = 63.45%, Loss = 0.3416386365890503
Epoch: 5749, Batch Gradient Norm: 15.0229698265928
Epoch: 5749, Batch Gradient Norm after: 15.0229698265928
Epoch 5750/10000, Prediction Accuracy = 63.396%, Loss = 0.3399036943912506
Epoch: 5750, Batch Gradient Norm: 17.46853675260073
Epoch: 5750, Batch Gradient Norm after: 17.46853675260073
Epoch 5751/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3437386333942413
Epoch: 5751, Batch Gradient Norm: 14.42203491551158
Epoch: 5751, Batch Gradient Norm after: 14.42203491551158
Epoch 5752/10000, Prediction Accuracy = 63.43399999999999%, Loss = 0.3371658563613892
Epoch: 5752, Batch Gradient Norm: 11.509986160011433
Epoch: 5752, Batch Gradient Norm after: 11.509986160011433
Epoch 5753/10000, Prediction Accuracy = 63.424%, Loss = 0.3320759356021881
Epoch: 5753, Batch Gradient Norm: 15.712781377218349
Epoch: 5753, Batch Gradient Norm after: 15.712781377218349
Epoch 5754/10000, Prediction Accuracy = 63.33399999999999%, Loss = 0.3399901568889618
Epoch: 5754, Batch Gradient Norm: 13.533614453039661
Epoch: 5754, Batch Gradient Norm after: 13.533614453039661
Epoch 5755/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.33712915778160096
Epoch: 5755, Batch Gradient Norm: 15.089633685736231
Epoch: 5755, Batch Gradient Norm after: 15.089633685736231
Epoch 5756/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.34368879795074464
Epoch: 5756, Batch Gradient Norm: 13.53848071664002
Epoch: 5756, Batch Gradient Norm after: 13.53848071664002
Epoch 5757/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.3372992992401123
Epoch: 5757, Batch Gradient Norm: 12.646270679692794
Epoch: 5757, Batch Gradient Norm after: 12.646270679692794
Epoch 5758/10000, Prediction Accuracy = 63.424%, Loss = 0.33615906834602355
Epoch: 5758, Batch Gradient Norm: 14.044825296493341
Epoch: 5758, Batch Gradient Norm after: 14.044825296493341
Epoch 5759/10000, Prediction Accuracy = 63.386%, Loss = 0.3356593608856201
Epoch: 5759, Batch Gradient Norm: 14.893377134847189
Epoch: 5759, Batch Gradient Norm after: 14.893377134847189
Epoch 5760/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.3384413242340088
Epoch: 5760, Batch Gradient Norm: 13.431147339141232
Epoch: 5760, Batch Gradient Norm after: 13.431147339141232
Epoch 5761/10000, Prediction Accuracy = 63.45799999999999%, Loss = 0.3354876637458801
Epoch: 5761, Batch Gradient Norm: 11.415332156975161
Epoch: 5761, Batch Gradient Norm after: 11.415332156975161
Epoch 5762/10000, Prediction Accuracy = 63.438%, Loss = 0.3386532127857208
Epoch: 5762, Batch Gradient Norm: 12.000930720695756
Epoch: 5762, Batch Gradient Norm after: 12.000930720695756
Epoch 5763/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.33471809029579164
Epoch: 5763, Batch Gradient Norm: 13.638942474924496
Epoch: 5763, Batch Gradient Norm after: 13.638942474924496
Epoch 5764/10000, Prediction Accuracy = 63.352%, Loss = 0.33630279898643495
Epoch: 5764, Batch Gradient Norm: 13.615231198004189
Epoch: 5764, Batch Gradient Norm after: 13.615231198004189
Epoch 5765/10000, Prediction Accuracy = 63.324%, Loss = 0.337956166267395
Epoch: 5765, Batch Gradient Norm: 13.596637895869748
Epoch: 5765, Batch Gradient Norm after: 13.596637895869748
Epoch 5766/10000, Prediction Accuracy = 63.314%, Loss = 0.33585495948791505
Epoch: 5766, Batch Gradient Norm: 13.355810258841787
Epoch: 5766, Batch Gradient Norm after: 13.355810258841787
Epoch 5767/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.3347983658313751
Epoch: 5767, Batch Gradient Norm: 9.795591257318359
Epoch: 5767, Batch Gradient Norm after: 9.795591257318359
Epoch 5768/10000, Prediction Accuracy = 63.472%, Loss = 0.333516925573349
Epoch: 5768, Batch Gradient Norm: 10.55110908804402
Epoch: 5768, Batch Gradient Norm after: 10.55110908804402
Epoch 5769/10000, Prediction Accuracy = 63.436%, Loss = 0.3330391764640808
Epoch: 5769, Batch Gradient Norm: 11.823319392623949
Epoch: 5769, Batch Gradient Norm after: 11.823319392623949
Epoch 5770/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.333386093378067
Epoch: 5770, Batch Gradient Norm: 12.253474133791368
Epoch: 5770, Batch Gradient Norm after: 12.253474133791368
Epoch 5771/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.33359711766242983
Epoch: 5771, Batch Gradient Norm: 11.037119428665962
Epoch: 5771, Batch Gradient Norm after: 11.037119428665962
Epoch 5772/10000, Prediction Accuracy = 63.246%, Loss = 0.33439563512802123
Epoch: 5772, Batch Gradient Norm: 16.33473629052078
Epoch: 5772, Batch Gradient Norm after: 16.33473629052078
Epoch 5773/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.34121943116188047
Epoch: 5773, Batch Gradient Norm: 16.48226493485546
Epoch: 5773, Batch Gradient Norm after: 16.48226493485546
Epoch 5774/10000, Prediction Accuracy = 63.362%, Loss = 0.343231338262558
Epoch: 5774, Batch Gradient Norm: 15.261749782027119
Epoch: 5774, Batch Gradient Norm after: 15.261749782027119
Epoch 5775/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.33982519507408143
Epoch: 5775, Batch Gradient Norm: 12.406828280998472
Epoch: 5775, Batch Gradient Norm after: 12.406828280998472
Epoch 5776/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.33746759295463563
Epoch: 5776, Batch Gradient Norm: 13.573537826779988
Epoch: 5776, Batch Gradient Norm after: 13.573537826779988
Epoch 5777/10000, Prediction Accuracy = 63.382000000000005%, Loss = 0.33722156286239624
Epoch: 5777, Batch Gradient Norm: 13.075954736506354
Epoch: 5777, Batch Gradient Norm after: 13.075954736506354
Epoch 5778/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.33817715048789976
Epoch: 5778, Batch Gradient Norm: 12.378141253748298
Epoch: 5778, Batch Gradient Norm after: 12.378141253748298
Epoch 5779/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.335184371471405
Epoch: 5779, Batch Gradient Norm: 11.567897981164165
Epoch: 5779, Batch Gradient Norm after: 11.567897981164165
Epoch 5780/10000, Prediction Accuracy = 63.362%, Loss = 0.3344803750514984
Epoch: 5780, Batch Gradient Norm: 13.785309703602804
Epoch: 5780, Batch Gradient Norm after: 13.785309703602804
Epoch 5781/10000, Prediction Accuracy = 63.354%, Loss = 0.33840433359146116
Epoch: 5781, Batch Gradient Norm: 14.248355385035365
Epoch: 5781, Batch Gradient Norm after: 14.248355385035365
Epoch 5782/10000, Prediction Accuracy = 63.342%, Loss = 0.3385724604129791
Epoch: 5782, Batch Gradient Norm: 14.856014791085096
Epoch: 5782, Batch Gradient Norm after: 14.856014791085096
Epoch 5783/10000, Prediction Accuracy = 63.254%, Loss = 0.33981006145477294
Epoch: 5783, Batch Gradient Norm: 16.377394218641207
Epoch: 5783, Batch Gradient Norm after: 16.377394218641207
Epoch 5784/10000, Prediction Accuracy = 63.412%, Loss = 0.34067440032958984
Epoch: 5784, Batch Gradient Norm: 13.892425308252061
Epoch: 5784, Batch Gradient Norm after: 13.892425308252061
Epoch 5785/10000, Prediction Accuracy = 63.480000000000004%, Loss = 0.33549765348434446
Epoch: 5785, Batch Gradient Norm: 12.833559487792334
Epoch: 5785, Batch Gradient Norm after: 12.833559487792334
Epoch 5786/10000, Prediction Accuracy = 63.41200000000001%, Loss = 0.33458245992660524
Epoch: 5786, Batch Gradient Norm: 15.175253270291643
Epoch: 5786, Batch Gradient Norm after: 15.175253270291643
Epoch 5787/10000, Prediction Accuracy = 63.416%, Loss = 0.3391628861427307
Epoch: 5787, Batch Gradient Norm: 18.44974613101895
Epoch: 5787, Batch Gradient Norm after: 18.44974613101895
Epoch 5788/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.34344438910484315
Epoch: 5788, Batch Gradient Norm: 16.35305420399356
Epoch: 5788, Batch Gradient Norm after: 16.35305420399356
Epoch 5789/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.33995280265808103
Epoch: 5789, Batch Gradient Norm: 14.015200449909445
Epoch: 5789, Batch Gradient Norm after: 14.015200449909445
Epoch 5790/10000, Prediction Accuracy = 63.396%, Loss = 0.3374206244945526
Epoch: 5790, Batch Gradient Norm: 11.998550473956621
Epoch: 5790, Batch Gradient Norm after: 11.998550473956621
Epoch 5791/10000, Prediction Accuracy = 63.54%, Loss = 0.3351316094398499
Epoch: 5791, Batch Gradient Norm: 12.667562967502027
Epoch: 5791, Batch Gradient Norm after: 12.667562967502027
Epoch 5792/10000, Prediction Accuracy = 63.322%, Loss = 0.3354834318161011
Epoch: 5792, Batch Gradient Norm: 11.215895884357135
Epoch: 5792, Batch Gradient Norm after: 11.215895884357135
Epoch 5793/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.3364054441452026
Epoch: 5793, Batch Gradient Norm: 11.923115125210037
Epoch: 5793, Batch Gradient Norm after: 11.923115125210037
Epoch 5794/10000, Prediction Accuracy = 63.35799999999999%, Loss = 0.3374513328075409
Epoch: 5794, Batch Gradient Norm: 14.04556832037382
Epoch: 5794, Batch Gradient Norm after: 14.04556832037382
Epoch 5795/10000, Prediction Accuracy = 63.336%, Loss = 0.33865087628364565
Epoch: 5795, Batch Gradient Norm: 17.413643044871193
Epoch: 5795, Batch Gradient Norm after: 17.06867166462034
Epoch 5796/10000, Prediction Accuracy = 63.406000000000006%, Loss = 0.3426874279975891
Epoch: 5796, Batch Gradient Norm: 15.322802123882532
Epoch: 5796, Batch Gradient Norm after: 15.322802123882532
Epoch 5797/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3403376340866089
Epoch: 5797, Batch Gradient Norm: 12.778889475941847
Epoch: 5797, Batch Gradient Norm after: 12.778889475941847
Epoch 5798/10000, Prediction Accuracy = 63.39%, Loss = 0.33494855761528014
Epoch: 5798, Batch Gradient Norm: 13.34365744220649
Epoch: 5798, Batch Gradient Norm after: 13.34365744220649
Epoch 5799/10000, Prediction Accuracy = 63.378%, Loss = 0.33529782891273496
Epoch: 5799, Batch Gradient Norm: 11.780789053577092
Epoch: 5799, Batch Gradient Norm after: 11.780789053577092
Epoch 5800/10000, Prediction Accuracy = 63.48%, Loss = 0.33436339497566225
Epoch: 5800, Batch Gradient Norm: 12.99879911677022
Epoch: 5800, Batch Gradient Norm after: 12.99879911677022
Epoch 5801/10000, Prediction Accuracy = 63.314%, Loss = 0.3363761603832245
Epoch: 5801, Batch Gradient Norm: 13.119584534393779
Epoch: 5801, Batch Gradient Norm after: 13.119584534393779
Epoch 5802/10000, Prediction Accuracy = 63.31999999999999%, Loss = 0.33592234253883363
Epoch: 5802, Batch Gradient Norm: 10.879496642085039
Epoch: 5802, Batch Gradient Norm after: 10.879496642085039
Epoch 5803/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3351089656352997
Epoch: 5803, Batch Gradient Norm: 16.614850667256697
Epoch: 5803, Batch Gradient Norm after: 16.331427729760044
Epoch 5804/10000, Prediction Accuracy = 63.48%, Loss = 0.3403684973716736
Epoch: 5804, Batch Gradient Norm: 14.47214335520035
Epoch: 5804, Batch Gradient Norm after: 14.47214335520035
Epoch 5805/10000, Prediction Accuracy = 63.39200000000001%, Loss = 0.3391895413398743
Epoch: 5805, Batch Gradient Norm: 14.075514877927374
Epoch: 5805, Batch Gradient Norm after: 14.075514877927374
Epoch 5806/10000, Prediction Accuracy = 63.426%, Loss = 0.33646302819252016
Epoch: 5806, Batch Gradient Norm: 15.442991906382073
Epoch: 5806, Batch Gradient Norm after: 15.442991906382073
Epoch 5807/10000, Prediction Accuracy = 63.324%, Loss = 0.3428756892681122
Epoch: 5807, Batch Gradient Norm: 15.599898080817942
Epoch: 5807, Batch Gradient Norm after: 15.599898080817942
Epoch 5808/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.33913673758506774
Epoch: 5808, Batch Gradient Norm: 13.619866114457997
Epoch: 5808, Batch Gradient Norm after: 13.619866114457997
Epoch 5809/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.33833165764808654
Epoch: 5809, Batch Gradient Norm: 11.551948181127305
Epoch: 5809, Batch Gradient Norm after: 11.551948181127305
Epoch 5810/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.33649289011955263
Epoch: 5810, Batch Gradient Norm: 11.306295788362178
Epoch: 5810, Batch Gradient Norm after: 11.306295788362178
Epoch 5811/10000, Prediction Accuracy = 63.402%, Loss = 0.3336597681045532
Epoch: 5811, Batch Gradient Norm: 12.423703247861202
Epoch: 5811, Batch Gradient Norm after: 12.423703247861202
Epoch 5812/10000, Prediction Accuracy = 63.45400000000001%, Loss = 0.338439416885376
Epoch: 5812, Batch Gradient Norm: 12.821340787315398
Epoch: 5812, Batch Gradient Norm after: 12.821340787315398
Epoch 5813/10000, Prediction Accuracy = 63.448%, Loss = 0.3380599915981293
Epoch: 5813, Batch Gradient Norm: 14.436912793972617
Epoch: 5813, Batch Gradient Norm after: 14.436912793972617
Epoch 5814/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.33692235350608823
Epoch: 5814, Batch Gradient Norm: 11.80045476147719
Epoch: 5814, Batch Gradient Norm after: 11.80045476147719
Epoch 5815/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.33466848731040955
Epoch: 5815, Batch Gradient Norm: 13.104963423550775
Epoch: 5815, Batch Gradient Norm after: 13.104963423550775
Epoch 5816/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.33762088418006897
Epoch: 5816, Batch Gradient Norm: 14.866543740874896
Epoch: 5816, Batch Gradient Norm after: 14.866543740874896
Epoch 5817/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.33730425238609313
Epoch: 5817, Batch Gradient Norm: 11.63362904071392
Epoch: 5817, Batch Gradient Norm after: 11.63362904071392
Epoch 5818/10000, Prediction Accuracy = 63.464%, Loss = 0.3316240727901459
Epoch: 5818, Batch Gradient Norm: 11.939296681092655
Epoch: 5818, Batch Gradient Norm after: 11.939296681092655
Epoch 5819/10000, Prediction Accuracy = 63.36999999999999%, Loss = 0.33436842560768126
Epoch: 5819, Batch Gradient Norm: 12.894909951424898
Epoch: 5819, Batch Gradient Norm after: 12.894909951424898
Epoch 5820/10000, Prediction Accuracy = 63.336%, Loss = 0.3351873576641083
Epoch: 5820, Batch Gradient Norm: 16.46041098514194
Epoch: 5820, Batch Gradient Norm after: 16.46041098514194
Epoch 5821/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.33901716470718385
Epoch: 5821, Batch Gradient Norm: 15.140598774095192
Epoch: 5821, Batch Gradient Norm after: 15.140598774095192
Epoch 5822/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3368032395839691
Epoch: 5822, Batch Gradient Norm: 14.916475599804876
Epoch: 5822, Batch Gradient Norm after: 14.916475599804876
Epoch 5823/10000, Prediction Accuracy = 63.46%, Loss = 0.3366787016391754
Epoch: 5823, Batch Gradient Norm: 14.991171864951536
Epoch: 5823, Batch Gradient Norm after: 14.991171864951536
Epoch 5824/10000, Prediction Accuracy = 63.38000000000001%, Loss = 0.3361985981464386
Epoch: 5824, Batch Gradient Norm: 17.255450378004923
Epoch: 5824, Batch Gradient Norm after: 17.255450378004923
Epoch 5825/10000, Prediction Accuracy = 63.30999999999999%, Loss = 0.34096395373344424
Epoch: 5825, Batch Gradient Norm: 16.072499015062746
Epoch: 5825, Batch Gradient Norm after: 16.072499015062746
Epoch 5826/10000, Prediction Accuracy = 63.424%, Loss = 0.33954558372497556
Epoch: 5826, Batch Gradient Norm: 14.169760934136443
Epoch: 5826, Batch Gradient Norm after: 14.169760934136443
Epoch 5827/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3363975346088409
Epoch: 5827, Batch Gradient Norm: 14.141632687044208
Epoch: 5827, Batch Gradient Norm after: 14.141632687044208
Epoch 5828/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.33770076632499696
Epoch: 5828, Batch Gradient Norm: 17.46548185734
Epoch: 5828, Batch Gradient Norm after: 17.001828001257536
Epoch 5829/10000, Prediction Accuracy = 63.33%, Loss = 0.3430822670459747
Epoch: 5829, Batch Gradient Norm: 18.01837282831453
Epoch: 5829, Batch Gradient Norm after: 17.831952747413524
Epoch 5830/10000, Prediction Accuracy = 63.364%, Loss = 0.345463365316391
Epoch: 5830, Batch Gradient Norm: 13.042656529387077
Epoch: 5830, Batch Gradient Norm after: 13.042656529387077
Epoch 5831/10000, Prediction Accuracy = 63.416%, Loss = 0.3359454035758972
Epoch: 5831, Batch Gradient Norm: 16.05840125115374
Epoch: 5831, Batch Gradient Norm after: 16.05840125115374
Epoch 5832/10000, Prediction Accuracy = 63.41399999999999%, Loss = 0.33900200128555297
Epoch: 5832, Batch Gradient Norm: 14.544833653511326
Epoch: 5832, Batch Gradient Norm after: 14.544833653511326
Epoch 5833/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.336960768699646
Epoch: 5833, Batch Gradient Norm: 15.27726159344987
Epoch: 5833, Batch Gradient Norm after: 15.27726159344987
Epoch 5834/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3405612111091614
Epoch: 5834, Batch Gradient Norm: 15.520388263368496
Epoch: 5834, Batch Gradient Norm after: 15.520388263368496
Epoch 5835/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.33826072216033937
Epoch: 5835, Batch Gradient Norm: 11.445025323717687
Epoch: 5835, Batch Gradient Norm after: 11.445025323717687
Epoch 5836/10000, Prediction Accuracy = 63.342%, Loss = 0.3333082556724548
Epoch: 5836, Batch Gradient Norm: 11.50544699464336
Epoch: 5836, Batch Gradient Norm after: 11.50544699464336
Epoch 5837/10000, Prediction Accuracy = 63.302%, Loss = 0.3328519999980927
Epoch: 5837, Batch Gradient Norm: 10.403692469048789
Epoch: 5837, Batch Gradient Norm after: 10.403692469048789
Epoch 5838/10000, Prediction Accuracy = 63.552%, Loss = 0.3327788829803467
Epoch: 5838, Batch Gradient Norm: 12.459019384070324
Epoch: 5838, Batch Gradient Norm after: 12.459019384070324
Epoch 5839/10000, Prediction Accuracy = 63.448%, Loss = 0.3349712550640106
Epoch: 5839, Batch Gradient Norm: 13.122856689578658
Epoch: 5839, Batch Gradient Norm after: 13.122856689578658
Epoch 5840/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3344274997711182
Epoch: 5840, Batch Gradient Norm: 10.793479813279403
Epoch: 5840, Batch Gradient Norm after: 10.793479813279403
Epoch 5841/10000, Prediction Accuracy = 63.492%, Loss = 0.3332818627357483
Epoch: 5841, Batch Gradient Norm: 11.07709671633663
Epoch: 5841, Batch Gradient Norm after: 11.07709671633663
Epoch 5842/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.3314409375190735
Epoch: 5842, Batch Gradient Norm: 12.392090439020732
Epoch: 5842, Batch Gradient Norm after: 12.392090439020732
Epoch 5843/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.3339578866958618
Epoch: 5843, Batch Gradient Norm: 12.625717425040586
Epoch: 5843, Batch Gradient Norm after: 12.625717425040586
Epoch 5844/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.33338114619255066
Epoch: 5844, Batch Gradient Norm: 12.522599322009276
Epoch: 5844, Batch Gradient Norm after: 12.522599322009276
Epoch 5845/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.333730548620224
Epoch: 5845, Batch Gradient Norm: 12.29392854052581
Epoch: 5845, Batch Gradient Norm after: 12.29392854052581
Epoch 5846/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3334851384162903
Epoch: 5846, Batch Gradient Norm: 12.28793734519574
Epoch: 5846, Batch Gradient Norm after: 12.28793734519574
Epoch 5847/10000, Prediction Accuracy = 63.436%, Loss = 0.33354034423828127
Epoch: 5847, Batch Gradient Norm: 12.241054084988875
Epoch: 5847, Batch Gradient Norm after: 12.241054084988875
Epoch 5848/10000, Prediction Accuracy = 63.386%, Loss = 0.3334493637084961
Epoch: 5848, Batch Gradient Norm: 16.962670009576406
Epoch: 5848, Batch Gradient Norm after: 16.782454556446726
Epoch 5849/10000, Prediction Accuracy = 63.39%, Loss = 0.3397678077220917
Epoch: 5849, Batch Gradient Norm: 19.35456831116284
Epoch: 5849, Batch Gradient Norm after: 18.819377652855252
Epoch 5850/10000, Prediction Accuracy = 63.38599999999999%, Loss = 0.3472296714782715
Epoch: 5850, Batch Gradient Norm: 16.124128323371615
Epoch: 5850, Batch Gradient Norm after: 16.124128323371615
Epoch 5851/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.33894469738006594
Epoch: 5851, Batch Gradient Norm: 15.782177317707744
Epoch: 5851, Batch Gradient Norm after: 15.782177317707744
Epoch 5852/10000, Prediction Accuracy = 63.35%, Loss = 0.3389779686927795
Epoch: 5852, Batch Gradient Norm: 13.123843468181773
Epoch: 5852, Batch Gradient Norm after: 13.123843468181773
Epoch 5853/10000, Prediction Accuracy = 63.584%, Loss = 0.33405309319496157
Epoch: 5853, Batch Gradient Norm: 10.058728708897863
Epoch: 5853, Batch Gradient Norm after: 10.058728708897863
Epoch 5854/10000, Prediction Accuracy = 63.504000000000005%, Loss = 0.3304001986980438
Epoch: 5854, Batch Gradient Norm: 14.195808783767907
Epoch: 5854, Batch Gradient Norm after: 14.195808783767907
Epoch 5855/10000, Prediction Accuracy = 63.366%, Loss = 0.3371427595615387
Epoch: 5855, Batch Gradient Norm: 16.89210746730738
Epoch: 5855, Batch Gradient Norm after: 16.89210746730738
Epoch 5856/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.3387652635574341
Epoch: 5856, Batch Gradient Norm: 18.166795592354884
Epoch: 5856, Batch Gradient Norm after: 17.705379464330065
Epoch 5857/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.34247429966926574
Epoch: 5857, Batch Gradient Norm: 16.64828309443347
Epoch: 5857, Batch Gradient Norm after: 16.278190263539965
Epoch 5858/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.33872851729393005
Epoch: 5858, Batch Gradient Norm: 14.194661670686461
Epoch: 5858, Batch Gradient Norm after: 14.194661670686461
Epoch 5859/10000, Prediction Accuracy = 63.412%, Loss = 0.3353560507297516
Epoch: 5859, Batch Gradient Norm: 11.263076842729289
Epoch: 5859, Batch Gradient Norm after: 11.263076842729289
Epoch 5860/10000, Prediction Accuracy = 63.467999999999996%, Loss = 0.3346534729003906
Epoch: 5860, Batch Gradient Norm: 11.183504716962394
Epoch: 5860, Batch Gradient Norm after: 11.183504716962394
Epoch 5861/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3336623787879944
Epoch: 5861, Batch Gradient Norm: 13.344066420708032
Epoch: 5861, Batch Gradient Norm after: 13.344066420708032
Epoch 5862/10000, Prediction Accuracy = 63.448%, Loss = 0.33612473011016847
Epoch: 5862, Batch Gradient Norm: 14.425552854360523
Epoch: 5862, Batch Gradient Norm after: 14.419230869361462
Epoch 5863/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.33750260472297666
Epoch: 5863, Batch Gradient Norm: 17.265981587528614
Epoch: 5863, Batch Gradient Norm after: 17.265981587528614
Epoch 5864/10000, Prediction Accuracy = 63.428%, Loss = 0.34079933166503906
Epoch: 5864, Batch Gradient Norm: 16.301581110554253
Epoch: 5864, Batch Gradient Norm after: 16.301581110554253
Epoch 5865/10000, Prediction Accuracy = 63.524%, Loss = 0.3376586675643921
Epoch: 5865, Batch Gradient Norm: 14.522287415221356
Epoch: 5865, Batch Gradient Norm after: 14.522287415221356
Epoch 5866/10000, Prediction Accuracy = 63.45%, Loss = 0.3375484228134155
Epoch: 5866, Batch Gradient Norm: 12.821988366738342
Epoch: 5866, Batch Gradient Norm after: 12.821988366738342
Epoch 5867/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3343267679214478
Epoch: 5867, Batch Gradient Norm: 9.944654580970274
Epoch: 5867, Batch Gradient Norm after: 9.944654580970274
Epoch 5868/10000, Prediction Accuracy = 63.544%, Loss = 0.3318542897701263
Epoch: 5868, Batch Gradient Norm: 12.648264435339186
Epoch: 5868, Batch Gradient Norm after: 12.648264435339186
Epoch 5869/10000, Prediction Accuracy = 63.42%, Loss = 0.33380518555641175
Epoch: 5869, Batch Gradient Norm: 11.240284740354767
Epoch: 5869, Batch Gradient Norm after: 11.240284740354767
Epoch 5870/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3293265223503113
Epoch: 5870, Batch Gradient Norm: 12.540653953951583
Epoch: 5870, Batch Gradient Norm after: 12.540653953951583
Epoch 5871/10000, Prediction Accuracy = 63.516000000000005%, Loss = 0.33559768199920653
Epoch: 5871, Batch Gradient Norm: 14.529564560680207
Epoch: 5871, Batch Gradient Norm after: 14.529564560680207
Epoch 5872/10000, Prediction Accuracy = 63.482000000000006%, Loss = 0.3366358816623688
Epoch: 5872, Batch Gradient Norm: 14.273933807359652
Epoch: 5872, Batch Gradient Norm after: 14.273933807359652
Epoch 5873/10000, Prediction Accuracy = 63.386%, Loss = 0.3378682494163513
Epoch: 5873, Batch Gradient Norm: 16.112007146714664
Epoch: 5873, Batch Gradient Norm after: 16.112007146714664
Epoch 5874/10000, Prediction Accuracy = 63.464%, Loss = 0.33980185389518736
Epoch: 5874, Batch Gradient Norm: 13.957788306448208
Epoch: 5874, Batch Gradient Norm after: 13.957788306448208
Epoch 5875/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.33540151119232176
Epoch: 5875, Batch Gradient Norm: 12.372084198312942
Epoch: 5875, Batch Gradient Norm after: 12.372084198312942
Epoch 5876/10000, Prediction Accuracy = 63.412%, Loss = 0.3332250118255615
Epoch: 5876, Batch Gradient Norm: 13.57001406961849
Epoch: 5876, Batch Gradient Norm after: 13.57001406961849
Epoch 5877/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3349581897258759
Epoch: 5877, Batch Gradient Norm: 15.243748793509976
Epoch: 5877, Batch Gradient Norm after: 15.243748793509976
Epoch 5878/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3383433699607849
Epoch: 5878, Batch Gradient Norm: 17.100103044626113
Epoch: 5878, Batch Gradient Norm after: 17.100103044626113
Epoch 5879/10000, Prediction Accuracy = 63.472%, Loss = 0.3412074029445648
Epoch: 5879, Batch Gradient Norm: 13.173004518269044
Epoch: 5879, Batch Gradient Norm after: 13.173004518269044
Epoch 5880/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.33452104330062865
Epoch: 5880, Batch Gradient Norm: 12.547877882974788
Epoch: 5880, Batch Gradient Norm after: 12.547877882974788
Epoch 5881/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.33347375988960265
Epoch: 5881, Batch Gradient Norm: 12.698843309951808
Epoch: 5881, Batch Gradient Norm after: 12.698843309951808
Epoch 5882/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3314709961414337
Epoch: 5882, Batch Gradient Norm: 14.64925717512974
Epoch: 5882, Batch Gradient Norm after: 14.64925717512974
Epoch 5883/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.33491879105567934
Epoch: 5883, Batch Gradient Norm: 14.539651769695832
Epoch: 5883, Batch Gradient Norm after: 14.539651769695832
Epoch 5884/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.33266353607177734
Epoch: 5884, Batch Gradient Norm: 13.25331372751062
Epoch: 5884, Batch Gradient Norm after: 13.25331372751062
Epoch 5885/10000, Prediction Accuracy = 63.528000000000006%, Loss = 0.33171032667160033
Epoch: 5885, Batch Gradient Norm: 15.939772248536125
Epoch: 5885, Batch Gradient Norm after: 15.6209013679337
Epoch 5886/10000, Prediction Accuracy = 63.324%, Loss = 0.33629565238952636
Epoch: 5886, Batch Gradient Norm: 14.046301092092184
Epoch: 5886, Batch Gradient Norm after: 14.046301092092184
Epoch 5887/10000, Prediction Accuracy = 63.562%, Loss = 0.33302833437919616
Epoch: 5887, Batch Gradient Norm: 15.93766576490594
Epoch: 5887, Batch Gradient Norm after: 15.93766576490594
Epoch 5888/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.3361190617084503
Epoch: 5888, Batch Gradient Norm: 16.421219930406203
Epoch: 5888, Batch Gradient Norm after: 16.421219930406203
Epoch 5889/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.33734055161476134
Epoch: 5889, Batch Gradient Norm: 16.056697290724767
Epoch: 5889, Batch Gradient Norm after: 16.056697290724767
Epoch 5890/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.3377643883228302
Epoch: 5890, Batch Gradient Norm: 17.1689920822733
Epoch: 5890, Batch Gradient Norm after: 17.1689920822733
Epoch 5891/10000, Prediction Accuracy = 63.60200000000001%, Loss = 0.33909080028533933
Epoch: 5891, Batch Gradient Norm: 19.880090546147624
Epoch: 5891, Batch Gradient Norm after: 19.880090546147624
Epoch 5892/10000, Prediction Accuracy = 63.338%, Loss = 0.3475957453250885
Epoch: 5892, Batch Gradient Norm: 16.736678586790738
Epoch: 5892, Batch Gradient Norm after: 16.53787481964927
Epoch 5893/10000, Prediction Accuracy = 63.36999999999999%, Loss = 0.3364169478416443
Epoch: 5893, Batch Gradient Norm: 15.39044735653425
Epoch: 5893, Batch Gradient Norm after: 15.39044735653425
Epoch 5894/10000, Prediction Accuracy = 63.508%, Loss = 0.3367604911327362
Epoch: 5894, Batch Gradient Norm: 16.93679492402987
Epoch: 5894, Batch Gradient Norm after: 16.93679492402987
Epoch 5895/10000, Prediction Accuracy = 63.496%, Loss = 0.3402997851371765
Epoch: 5895, Batch Gradient Norm: 14.700200832460531
Epoch: 5895, Batch Gradient Norm after: 14.700200832460531
Epoch 5896/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.335030859708786
Epoch: 5896, Batch Gradient Norm: 14.270138757296777
Epoch: 5896, Batch Gradient Norm after: 14.270138757296777
Epoch 5897/10000, Prediction Accuracy = 63.456%, Loss = 0.3351606011390686
Epoch: 5897, Batch Gradient Norm: 10.023961637266373
Epoch: 5897, Batch Gradient Norm after: 10.023961637266373
Epoch 5898/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3297586441040039
Epoch: 5898, Batch Gradient Norm: 8.964134166019079
Epoch: 5898, Batch Gradient Norm after: 8.964134166019079
Epoch 5899/10000, Prediction Accuracy = 63.512%, Loss = 0.32874916195869447
Epoch: 5899, Batch Gradient Norm: 11.84825854937883
Epoch: 5899, Batch Gradient Norm after: 11.84825854937883
Epoch 5900/10000, Prediction Accuracy = 63.424%, Loss = 0.33202531933784485
Epoch: 5900, Batch Gradient Norm: 13.806107128197064
Epoch: 5900, Batch Gradient Norm after: 13.806107128197064
Epoch 5901/10000, Prediction Accuracy = 63.484%, Loss = 0.33497726917266846
Epoch: 5901, Batch Gradient Norm: 14.984060181139975
Epoch: 5901, Batch Gradient Norm after: 14.984060181139975
Epoch 5902/10000, Prediction Accuracy = 63.416%, Loss = 0.33430569171905516
Epoch: 5902, Batch Gradient Norm: 12.955283213124929
Epoch: 5902, Batch Gradient Norm after: 12.955283213124929
Epoch 5903/10000, Prediction Accuracy = 63.528%, Loss = 0.3317837715148926
Epoch: 5903, Batch Gradient Norm: 12.62123002492507
Epoch: 5903, Batch Gradient Norm after: 12.62123002492507
Epoch 5904/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.33195152282714846
Epoch: 5904, Batch Gradient Norm: 12.337542570618153
Epoch: 5904, Batch Gradient Norm after: 12.337542570618153
Epoch 5905/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.3313657879829407
Epoch: 5905, Batch Gradient Norm: 12.737159713980928
Epoch: 5905, Batch Gradient Norm after: 12.737159713980928
Epoch 5906/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.33542874455451965
Epoch: 5906, Batch Gradient Norm: 12.884696693941626
Epoch: 5906, Batch Gradient Norm after: 12.884696693941626
Epoch 5907/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.3311279296875
Epoch: 5907, Batch Gradient Norm: 11.367047829657526
Epoch: 5907, Batch Gradient Norm after: 11.367047829657526
Epoch 5908/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.33217552304267883
Epoch: 5908, Batch Gradient Norm: 9.533751971299
Epoch: 5908, Batch Gradient Norm after: 9.533751971299
Epoch 5909/10000, Prediction Accuracy = 63.386%, Loss = 0.3290886640548706
Epoch: 5909, Batch Gradient Norm: 9.592061916960517
Epoch: 5909, Batch Gradient Norm after: 9.592061916960517
Epoch 5910/10000, Prediction Accuracy = 63.58399999999999%, Loss = 0.3291162073612213
Epoch: 5910, Batch Gradient Norm: 11.84948326736695
Epoch: 5910, Batch Gradient Norm after: 11.84948326736695
Epoch 5911/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.33019272089004514
Epoch: 5911, Batch Gradient Norm: 10.50555934743265
Epoch: 5911, Batch Gradient Norm after: 10.50555934743265
Epoch 5912/10000, Prediction Accuracy = 63.50999999999999%, Loss = 0.32942465543746946
Epoch: 5912, Batch Gradient Norm: 11.92055372518647
Epoch: 5912, Batch Gradient Norm after: 11.92055372518647
Epoch 5913/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3314464449882507
Epoch: 5913, Batch Gradient Norm: 12.396628216415381
Epoch: 5913, Batch Gradient Norm after: 12.396628216415381
Epoch 5914/10000, Prediction Accuracy = 63.488%, Loss = 0.332163542509079
Epoch: 5914, Batch Gradient Norm: 12.92918517018268
Epoch: 5914, Batch Gradient Norm after: 12.92918517018268
Epoch 5915/10000, Prediction Accuracy = 63.496%, Loss = 0.3338385045528412
Epoch: 5915, Batch Gradient Norm: 15.503286942026502
Epoch: 5915, Batch Gradient Norm after: 15.503286942026502
Epoch 5916/10000, Prediction Accuracy = 63.436%, Loss = 0.3368968069553375
Epoch: 5916, Batch Gradient Norm: 12.895611621727868
Epoch: 5916, Batch Gradient Norm after: 12.895611621727868
Epoch 5917/10000, Prediction Accuracy = 63.438%, Loss = 0.3327587127685547
Epoch: 5917, Batch Gradient Norm: 10.301713246429212
Epoch: 5917, Batch Gradient Norm after: 10.301713246429212
Epoch 5918/10000, Prediction Accuracy = 63.44%, Loss = 0.33010935187339785
Epoch: 5918, Batch Gradient Norm: 11.557368544939502
Epoch: 5918, Batch Gradient Norm after: 11.557368544939502
Epoch 5919/10000, Prediction Accuracy = 63.564%, Loss = 0.33133623003959656
Epoch: 5919, Batch Gradient Norm: 11.438757588005634
Epoch: 5919, Batch Gradient Norm after: 11.438757588005634
Epoch 5920/10000, Prediction Accuracy = 63.5%, Loss = 0.3326733708381653
Epoch: 5920, Batch Gradient Norm: 12.241822635042329
Epoch: 5920, Batch Gradient Norm after: 12.241822635042329
Epoch 5921/10000, Prediction Accuracy = 63.376%, Loss = 0.3315310120582581
Epoch: 5921, Batch Gradient Norm: 12.967405384210355
Epoch: 5921, Batch Gradient Norm after: 12.967405384210355
Epoch 5922/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.3314801275730133
Epoch: 5922, Batch Gradient Norm: 12.870319097850818
Epoch: 5922, Batch Gradient Norm after: 12.870319097850818
Epoch 5923/10000, Prediction Accuracy = 63.653999999999996%, Loss = 0.3308630704879761
Epoch: 5923, Batch Gradient Norm: 14.149244677997027
Epoch: 5923, Batch Gradient Norm after: 14.149244677997027
Epoch 5924/10000, Prediction Accuracy = 63.472%, Loss = 0.3329379677772522
Epoch: 5924, Batch Gradient Norm: 13.828724490510082
Epoch: 5924, Batch Gradient Norm after: 13.828724490510082
Epoch 5925/10000, Prediction Accuracy = 63.516%, Loss = 0.3326330006122589
Epoch: 5925, Batch Gradient Norm: 15.438726471049657
Epoch: 5925, Batch Gradient Norm after: 15.438726471049657
Epoch 5926/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.3362281620502472
Epoch: 5926, Batch Gradient Norm: 14.811822546673097
Epoch: 5926, Batch Gradient Norm after: 14.811822546673097
Epoch 5927/10000, Prediction Accuracy = 63.64%, Loss = 0.3363962948322296
Epoch: 5927, Batch Gradient Norm: 13.765168748129824
Epoch: 5927, Batch Gradient Norm after: 13.765168748129824
Epoch 5928/10000, Prediction Accuracy = 63.459999999999994%, Loss = 0.33361110687255857
Epoch: 5928, Batch Gradient Norm: 13.541349070869883
Epoch: 5928, Batch Gradient Norm after: 13.541349070869883
Epoch 5929/10000, Prediction Accuracy = 63.25600000000001%, Loss = 0.3355784356594086
Epoch: 5929, Batch Gradient Norm: 12.15926593510828
Epoch: 5929, Batch Gradient Norm after: 12.15926593510828
Epoch 5930/10000, Prediction Accuracy = 63.46999999999999%, Loss = 0.3357417404651642
Epoch: 5930, Batch Gradient Norm: 12.60110646433329
Epoch: 5930, Batch Gradient Norm after: 12.60110646433329
Epoch 5931/10000, Prediction Accuracy = 63.504%, Loss = 0.33262641429901124
Epoch: 5931, Batch Gradient Norm: 13.523472579241956
Epoch: 5931, Batch Gradient Norm after: 13.523472579241956
Epoch 5932/10000, Prediction Accuracy = 63.462%, Loss = 0.3323837459087372
Epoch: 5932, Batch Gradient Norm: 12.602392194762974
Epoch: 5932, Batch Gradient Norm after: 12.602392194762974
Epoch 5933/10000, Prediction Accuracy = 63.507999999999996%, Loss = 0.3331520974636078
Epoch: 5933, Batch Gradient Norm: 12.858521475712891
Epoch: 5933, Batch Gradient Norm after: 12.858521475712891
Epoch 5934/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3323367714881897
Epoch: 5934, Batch Gradient Norm: 12.25394819067544
Epoch: 5934, Batch Gradient Norm after: 12.25394819067544
Epoch 5935/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.33446829915046694
Epoch: 5935, Batch Gradient Norm: 13.142502770805663
Epoch: 5935, Batch Gradient Norm after: 13.142502770805663
Epoch 5936/10000, Prediction Accuracy = 63.48%, Loss = 0.33345813751220704
Epoch: 5936, Batch Gradient Norm: 14.75021547114784
Epoch: 5936, Batch Gradient Norm after: 14.75021547114784
Epoch 5937/10000, Prediction Accuracy = 63.48599999999999%, Loss = 0.3350774824619293
Epoch: 5937, Batch Gradient Norm: 13.279763466007985
Epoch: 5937, Batch Gradient Norm after: 13.279763466007985
Epoch 5938/10000, Prediction Accuracy = 63.354000000000006%, Loss = 0.33372859954833983
Epoch: 5938, Batch Gradient Norm: 11.564953184636227
Epoch: 5938, Batch Gradient Norm after: 11.564953184636227
Epoch 5939/10000, Prediction Accuracy = 63.55999999999999%, Loss = 0.3306992471218109
Epoch: 5939, Batch Gradient Norm: 12.281778712654697
Epoch: 5939, Batch Gradient Norm after: 12.281778712654697
Epoch 5940/10000, Prediction Accuracy = 63.504%, Loss = 0.33294416069984434
Epoch: 5940, Batch Gradient Norm: 12.195733008772326
Epoch: 5940, Batch Gradient Norm after: 12.195733008772326
Epoch 5941/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.33148435354232786
Epoch: 5941, Batch Gradient Norm: 15.122526546544163
Epoch: 5941, Batch Gradient Norm after: 15.075016049713176
Epoch 5942/10000, Prediction Accuracy = 63.407999999999994%, Loss = 0.335498583316803
Epoch: 5942, Batch Gradient Norm: 15.377953381750341
Epoch: 5942, Batch Gradient Norm after: 15.377953381750341
Epoch 5943/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.33535916805267335
Epoch: 5943, Batch Gradient Norm: 14.609384401997517
Epoch: 5943, Batch Gradient Norm after: 14.609384401997517
Epoch 5944/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.335754531621933
Epoch: 5944, Batch Gradient Norm: 14.47449050102687
Epoch: 5944, Batch Gradient Norm after: 14.47449050102687
Epoch 5945/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3332372188568115
Epoch: 5945, Batch Gradient Norm: 10.742638857980552
Epoch: 5945, Batch Gradient Norm after: 10.742638857980552
Epoch 5946/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.3278236508369446
Epoch: 5946, Batch Gradient Norm: 11.437231174796
Epoch: 5946, Batch Gradient Norm after: 11.437231174796
Epoch 5947/10000, Prediction Accuracy = 63.464%, Loss = 0.3294974148273468
Epoch: 5947, Batch Gradient Norm: 12.253168243960763
Epoch: 5947, Batch Gradient Norm after: 12.253168243960763
Epoch 5948/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.3298119485378265
Epoch: 5948, Batch Gradient Norm: 13.909734379907988
Epoch: 5948, Batch Gradient Norm after: 13.909734379907988
Epoch 5949/10000, Prediction Accuracy = 63.552%, Loss = 0.33393112421035764
Epoch: 5949, Batch Gradient Norm: 14.00029547011471
Epoch: 5949, Batch Gradient Norm after: 14.00029547011471
Epoch 5950/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3323440968990326
Epoch: 5950, Batch Gradient Norm: 11.468046691288501
Epoch: 5950, Batch Gradient Norm after: 11.468046691288501
Epoch 5951/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.33114575743675234
Epoch: 5951, Batch Gradient Norm: 12.123329573491473
Epoch: 5951, Batch Gradient Norm after: 12.123329573491473
Epoch 5952/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.33007111549377444
Epoch: 5952, Batch Gradient Norm: 11.773628208409521
Epoch: 5952, Batch Gradient Norm after: 11.773628208409521
Epoch 5953/10000, Prediction Accuracy = 63.538%, Loss = 0.3326988577842712
Epoch: 5953, Batch Gradient Norm: 10.800148350891064
Epoch: 5953, Batch Gradient Norm after: 10.800148350891064
Epoch 5954/10000, Prediction Accuracy = 63.646%, Loss = 0.32815741896629336
Epoch: 5954, Batch Gradient Norm: 13.979256482053549
Epoch: 5954, Batch Gradient Norm after: 13.979256482053549
Epoch 5955/10000, Prediction Accuracy = 63.408%, Loss = 0.3329076588153839
Epoch: 5955, Batch Gradient Norm: 15.921915038636335
Epoch: 5955, Batch Gradient Norm after: 15.921915038636335
Epoch 5956/10000, Prediction Accuracy = 63.40999999999999%, Loss = 0.33725112676620483
Epoch: 5956, Batch Gradient Norm: 16.04918784688979
Epoch: 5956, Batch Gradient Norm after: 16.04918784688979
Epoch 5957/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.33643257021903994
Epoch: 5957, Batch Gradient Norm: 16.613512472874987
Epoch: 5957, Batch Gradient Norm after: 16.613512472874987
Epoch 5958/10000, Prediction Accuracy = 63.428%, Loss = 0.3400032341480255
Epoch: 5958, Batch Gradient Norm: 16.77377759429529
Epoch: 5958, Batch Gradient Norm after: 16.77377759429529
Epoch 5959/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.33994752168655396
Epoch: 5959, Batch Gradient Norm: 14.335216937184958
Epoch: 5959, Batch Gradient Norm after: 14.335216937184958
Epoch 5960/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.33482748866081236
Epoch: 5960, Batch Gradient Norm: 15.747093865048233
Epoch: 5960, Batch Gradient Norm after: 15.747093865048233
Epoch 5961/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.33530920147895815
Epoch: 5961, Batch Gradient Norm: 15.507924631110813
Epoch: 5961, Batch Gradient Norm after: 15.507924631110813
Epoch 5962/10000, Prediction Accuracy = 63.408%, Loss = 0.3371312141418457
Epoch: 5962, Batch Gradient Norm: 14.757395347053102
Epoch: 5962, Batch Gradient Norm after: 14.757395347053102
Epoch 5963/10000, Prediction Accuracy = 63.488%, Loss = 0.33669867515563967
Epoch: 5963, Batch Gradient Norm: 12.341201321671274
Epoch: 5963, Batch Gradient Norm after: 12.341201321671274
Epoch 5964/10000, Prediction Accuracy = 63.53399999999999%, Loss = 0.333896142244339
Epoch: 5964, Batch Gradient Norm: 13.854853887387458
Epoch: 5964, Batch Gradient Norm after: 13.854853887387458
Epoch 5965/10000, Prediction Accuracy = 63.492000000000004%, Loss = 0.332366007566452
Epoch: 5965, Batch Gradient Norm: 13.081985779862457
Epoch: 5965, Batch Gradient Norm after: 13.081985779862457
Epoch 5966/10000, Prediction Accuracy = 63.641999999999996%, Loss = 0.332234263420105
Epoch: 5966, Batch Gradient Norm: 12.66261627924392
Epoch: 5966, Batch Gradient Norm after: 12.66261627924392
Epoch 5967/10000, Prediction Accuracy = 63.376%, Loss = 0.3337219297885895
Epoch: 5967, Batch Gradient Norm: 12.620863167547043
Epoch: 5967, Batch Gradient Norm after: 12.620863167547043
Epoch 5968/10000, Prediction Accuracy = 63.49399999999999%, Loss = 0.3301119089126587
Epoch: 5968, Batch Gradient Norm: 13.778936806843955
Epoch: 5968, Batch Gradient Norm after: 13.778936806843955
Epoch 5969/10000, Prediction Accuracy = 63.507999999999996%, Loss = 0.3359105229377747
Epoch: 5969, Batch Gradient Norm: 11.82328405205535
Epoch: 5969, Batch Gradient Norm after: 11.82328405205535
Epoch 5970/10000, Prediction Accuracy = 63.517999999999994%, Loss = 0.3292174577713013
Epoch: 5970, Batch Gradient Norm: 11.78869970242014
Epoch: 5970, Batch Gradient Norm after: 11.78869970242014
Epoch 5971/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3309970498085022
Epoch: 5971, Batch Gradient Norm: 9.896665836052128
Epoch: 5971, Batch Gradient Norm after: 9.896665836052128
Epoch 5972/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.32865822315216064
Epoch: 5972, Batch Gradient Norm: 10.839511100947746
Epoch: 5972, Batch Gradient Norm after: 10.839511100947746
Epoch 5973/10000, Prediction Accuracy = 63.462%, Loss = 0.32845730781555177
Epoch: 5973, Batch Gradient Norm: 11.86027056587642
Epoch: 5973, Batch Gradient Norm after: 11.86027056587642
Epoch 5974/10000, Prediction Accuracy = 63.524%, Loss = 0.3289397478103638
Epoch: 5974, Batch Gradient Norm: 12.141847560081766
Epoch: 5974, Batch Gradient Norm after: 12.141847560081766
Epoch 5975/10000, Prediction Accuracy = 63.674%, Loss = 0.3308892548084259
Epoch: 5975, Batch Gradient Norm: 15.021978425029692
Epoch: 5975, Batch Gradient Norm after: 15.021978425029692
Epoch 5976/10000, Prediction Accuracy = 63.5%, Loss = 0.33450186252593994
Epoch: 5976, Batch Gradient Norm: 17.116236302177438
Epoch: 5976, Batch Gradient Norm after: 17.116236302177438
Epoch 5977/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.33812007308006287
Epoch: 5977, Batch Gradient Norm: 14.908441106814271
Epoch: 5977, Batch Gradient Norm after: 14.908441106814271
Epoch 5978/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3341793417930603
Epoch: 5978, Batch Gradient Norm: 11.309600734951609
Epoch: 5978, Batch Gradient Norm after: 11.309600734951609
Epoch 5979/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.33024234175682066
Epoch: 5979, Batch Gradient Norm: 9.310619262526252
Epoch: 5979, Batch Gradient Norm after: 9.310619262526252
Epoch 5980/10000, Prediction Accuracy = 63.438%, Loss = 0.3282474339008331
Epoch: 5980, Batch Gradient Norm: 12.281241166326593
Epoch: 5980, Batch Gradient Norm after: 12.281241166326593
Epoch 5981/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.3330393433570862
Epoch: 5981, Batch Gradient Norm: 11.663252500190326
Epoch: 5981, Batch Gradient Norm after: 11.663252500190326
Epoch 5982/10000, Prediction Accuracy = 63.516%, Loss = 0.3308275640010834
Epoch: 5982, Batch Gradient Norm: 12.102935724701336
Epoch: 5982, Batch Gradient Norm after: 12.102935724701336
Epoch 5983/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.331655877828598
Epoch: 5983, Batch Gradient Norm: 14.28592604789883
Epoch: 5983, Batch Gradient Norm after: 14.28592604789883
Epoch 5984/10000, Prediction Accuracy = 63.562%, Loss = 0.33397993445396423
Epoch: 5984, Batch Gradient Norm: 11.400881878633772
Epoch: 5984, Batch Gradient Norm after: 11.400881878633772
Epoch 5985/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3315272033214569
Epoch: 5985, Batch Gradient Norm: 14.05200031273793
Epoch: 5985, Batch Gradient Norm after: 14.05200031273793
Epoch 5986/10000, Prediction Accuracy = 63.65%, Loss = 0.33135128021240234
Epoch: 5986, Batch Gradient Norm: 10.83886753328126
Epoch: 5986, Batch Gradient Norm after: 10.83886753328126
Epoch 5987/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.32818426489830016
Epoch: 5987, Batch Gradient Norm: 10.991675653709258
Epoch: 5987, Batch Gradient Norm after: 10.991675653709258
Epoch 5988/10000, Prediction Accuracy = 63.586%, Loss = 0.3296901762485504
Epoch: 5988, Batch Gradient Norm: 13.354318713637825
Epoch: 5988, Batch Gradient Norm after: 13.354318713637825
Epoch 5989/10000, Prediction Accuracy = 63.576%, Loss = 0.33178605437278746
Epoch: 5989, Batch Gradient Norm: 17.43133702328556
Epoch: 5989, Batch Gradient Norm after: 17.43133702328556
Epoch 5990/10000, Prediction Accuracy = 63.55%, Loss = 0.33843317031860354
Epoch: 5990, Batch Gradient Norm: 15.74244842196966
Epoch: 5990, Batch Gradient Norm after: 15.74244842196966
Epoch 5991/10000, Prediction Accuracy = 63.528%, Loss = 0.3339099586009979
Epoch: 5991, Batch Gradient Norm: 17.497908699728605
Epoch: 5991, Batch Gradient Norm after: 17.497908699728605
Epoch 5992/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.338182258605957
Epoch: 5992, Batch Gradient Norm: 14.478748832549417
Epoch: 5992, Batch Gradient Norm after: 14.478748832549417
Epoch 5993/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.333637410402298
Epoch: 5993, Batch Gradient Norm: 12.091953424056761
Epoch: 5993, Batch Gradient Norm after: 12.091953424056761
Epoch 5994/10000, Prediction Accuracy = 63.588%, Loss = 0.32910211086273194
Epoch: 5994, Batch Gradient Norm: 13.315704002030714
Epoch: 5994, Batch Gradient Norm after: 13.315704002030714
Epoch 5995/10000, Prediction Accuracy = 63.424%, Loss = 0.3353858768939972
Epoch: 5995, Batch Gradient Norm: 14.174232243251408
Epoch: 5995, Batch Gradient Norm after: 14.174232243251408
Epoch 5996/10000, Prediction Accuracy = 63.508%, Loss = 0.3332591116428375
Epoch: 5996, Batch Gradient Norm: 12.266057672611627
Epoch: 5996, Batch Gradient Norm after: 12.266057672611627
Epoch 5997/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.3316202282905579
Epoch: 5997, Batch Gradient Norm: 12.621901893163399
Epoch: 5997, Batch Gradient Norm after: 12.621901893163399
Epoch 5998/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.3329355537891388
Epoch: 5998, Batch Gradient Norm: 14.206422672071556
Epoch: 5998, Batch Gradient Norm after: 14.206422672071556
Epoch 5999/10000, Prediction Accuracy = 63.504%, Loss = 0.33459314703941345
Epoch: 5999, Batch Gradient Norm: 16.318770589900684
Epoch: 5999, Batch Gradient Norm after: 16.318770589900684
Epoch 6000/10000, Prediction Accuracy = 63.524%, Loss = 0.3352403104305267
Epoch: 6000, Batch Gradient Norm: 17.171667604960714
Epoch: 6000, Batch Gradient Norm after: 17.171667604960714
Epoch 6001/10000, Prediction Accuracy = 63.46%, Loss = 0.3388586401939392
Epoch: 6001, Batch Gradient Norm: 18.697019952092727
Epoch: 6001, Batch Gradient Norm after: 17.759587811857152
Epoch 6002/10000, Prediction Accuracy = 63.374%, Loss = 0.3413680613040924
Epoch: 6002, Batch Gradient Norm: 15.418243227405338
Epoch: 6002, Batch Gradient Norm after: 15.418243227405338
Epoch 6003/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.334396892786026
Epoch: 6003, Batch Gradient Norm: 14.915707540795207
Epoch: 6003, Batch Gradient Norm after: 14.915707540795207
Epoch 6004/10000, Prediction Accuracy = 63.604%, Loss = 0.33425645232200624
Epoch: 6004, Batch Gradient Norm: 15.375764988490102
Epoch: 6004, Batch Gradient Norm after: 15.375764988490102
Epoch 6005/10000, Prediction Accuracy = 63.538%, Loss = 0.33531683683395386
Epoch: 6005, Batch Gradient Norm: 12.525584506115
Epoch: 6005, Batch Gradient Norm after: 12.525584506115
Epoch 6006/10000, Prediction Accuracy = 63.636%, Loss = 0.33130083084106443
Epoch: 6006, Batch Gradient Norm: 13.219891816878363
Epoch: 6006, Batch Gradient Norm after: 13.219891816878363
Epoch 6007/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.33168326020240785
Epoch: 6007, Batch Gradient Norm: 14.13687561440743
Epoch: 6007, Batch Gradient Norm after: 14.13687561440743
Epoch 6008/10000, Prediction Accuracy = 63.564%, Loss = 0.3315402626991272
Epoch: 6008, Batch Gradient Norm: 13.586038286901339
Epoch: 6008, Batch Gradient Norm after: 13.586038286901339
Epoch 6009/10000, Prediction Accuracy = 63.50200000000001%, Loss = 0.33245614171028137
Epoch: 6009, Batch Gradient Norm: 12.682630148367966
Epoch: 6009, Batch Gradient Norm after: 12.682630148367966
Epoch 6010/10000, Prediction Accuracy = 63.548%, Loss = 0.32963536977767943
Epoch: 6010, Batch Gradient Norm: 14.474914475287992
Epoch: 6010, Batch Gradient Norm after: 14.474914475287992
Epoch 6011/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.3339653968811035
Epoch: 6011, Batch Gradient Norm: 14.851916790274927
Epoch: 6011, Batch Gradient Norm after: 14.851916790274927
Epoch 6012/10000, Prediction Accuracy = 63.564%, Loss = 0.3343821823596954
Epoch: 6012, Batch Gradient Norm: 16.485731104011773
Epoch: 6012, Batch Gradient Norm after: 16.485731104011773
Epoch 6013/10000, Prediction Accuracy = 63.47800000000001%, Loss = 0.336250364780426
Epoch: 6013, Batch Gradient Norm: 15.172644950375947
Epoch: 6013, Batch Gradient Norm after: 15.172644950375947
Epoch 6014/10000, Prediction Accuracy = 63.495999999999995%, Loss = 0.33333284258842466
Epoch: 6014, Batch Gradient Norm: 14.343484644654295
Epoch: 6014, Batch Gradient Norm after: 14.343484644654295
Epoch 6015/10000, Prediction Accuracy = 63.38599999999999%, Loss = 0.3332971572875977
Epoch: 6015, Batch Gradient Norm: 14.834127474499148
Epoch: 6015, Batch Gradient Norm after: 14.834127474499148
Epoch 6016/10000, Prediction Accuracy = 63.574%, Loss = 0.33183686137199403
Epoch: 6016, Batch Gradient Norm: 13.987226573102964
Epoch: 6016, Batch Gradient Norm after: 13.987226573102964
Epoch 6017/10000, Prediction Accuracy = 63.622%, Loss = 0.3318119585514069
Epoch: 6017, Batch Gradient Norm: 16.519107989624217
Epoch: 6017, Batch Gradient Norm after: 16.519107989624217
Epoch 6018/10000, Prediction Accuracy = 63.398%, Loss = 0.3365509033203125
Epoch: 6018, Batch Gradient Norm: 17.26005652175801
Epoch: 6018, Batch Gradient Norm after: 16.99839770994659
Epoch 6019/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3332356870174408
Epoch: 6019, Batch Gradient Norm: 16.495157972994274
Epoch: 6019, Batch Gradient Norm after: 16.495157972994274
Epoch 6020/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3366472899913788
Epoch: 6020, Batch Gradient Norm: 16.208258502655568
Epoch: 6020, Batch Gradient Norm after: 16.208258502655568
Epoch 6021/10000, Prediction Accuracy = 63.548%, Loss = 0.33331441283226015
Epoch: 6021, Batch Gradient Norm: 14.602778221810992
Epoch: 6021, Batch Gradient Norm after: 14.602778221810992
Epoch 6022/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3343673348426819
Epoch: 6022, Batch Gradient Norm: 14.937147508726259
Epoch: 6022, Batch Gradient Norm after: 14.937147508726259
Epoch 6023/10000, Prediction Accuracy = 63.58%, Loss = 0.332936292886734
Epoch: 6023, Batch Gradient Norm: 15.192471900013548
Epoch: 6023, Batch Gradient Norm after: 15.192471900013548
Epoch 6024/10000, Prediction Accuracy = 63.431999999999995%, Loss = 0.3345073640346527
Epoch: 6024, Batch Gradient Norm: 14.466783515576594
Epoch: 6024, Batch Gradient Norm after: 14.466783515576594
Epoch 6025/10000, Prediction Accuracy = 63.564%, Loss = 0.33253214359283445
Epoch: 6025, Batch Gradient Norm: 15.46900165229699
Epoch: 6025, Batch Gradient Norm after: 15.46900165229699
Epoch 6026/10000, Prediction Accuracy = 63.519999999999996%, Loss = 0.3322801351547241
Epoch: 6026, Batch Gradient Norm: 13.309232892747314
Epoch: 6026, Batch Gradient Norm after: 13.309232892747314
Epoch 6027/10000, Prediction Accuracy = 63.482000000000006%, Loss = 0.3302042782306671
Epoch: 6027, Batch Gradient Norm: 15.40542325007592
Epoch: 6027, Batch Gradient Norm after: 15.40542325007592
Epoch 6028/10000, Prediction Accuracy = 63.556%, Loss = 0.3334690809249878
Epoch: 6028, Batch Gradient Norm: 17.536939987752977
Epoch: 6028, Batch Gradient Norm after: 16.970877842087052
Epoch 6029/10000, Prediction Accuracy = 63.598%, Loss = 0.33512153625488283
Epoch: 6029, Batch Gradient Norm: 16.44020776932756
Epoch: 6029, Batch Gradient Norm after: 16.44020776932756
Epoch 6030/10000, Prediction Accuracy = 63.6%, Loss = 0.33421778678894043
Epoch: 6030, Batch Gradient Norm: 15.046311025626352
Epoch: 6030, Batch Gradient Norm after: 15.046311025626352
Epoch 6031/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.3337971866130829
Epoch: 6031, Batch Gradient Norm: 14.10149892381187
Epoch: 6031, Batch Gradient Norm after: 14.10149892381187
Epoch 6032/10000, Prediction Accuracy = 63.492000000000004%, Loss = 0.3342202067375183
Epoch: 6032, Batch Gradient Norm: 12.797409988631077
Epoch: 6032, Batch Gradient Norm after: 12.797409988631077
Epoch 6033/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.33188487887382506
Epoch: 6033, Batch Gradient Norm: 11.555092972897414
Epoch: 6033, Batch Gradient Norm after: 11.555092972897414
Epoch 6034/10000, Prediction Accuracy = 63.504%, Loss = 0.3276247501373291
Epoch: 6034, Batch Gradient Norm: 11.396005894605688
Epoch: 6034, Batch Gradient Norm after: 11.396005894605688
Epoch 6035/10000, Prediction Accuracy = 63.572%, Loss = 0.32828248739242555
Epoch: 6035, Batch Gradient Norm: 11.150721140829667
Epoch: 6035, Batch Gradient Norm after: 11.150721140829667
Epoch 6036/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3274088740348816
Epoch: 6036, Batch Gradient Norm: 10.316393499940567
Epoch: 6036, Batch Gradient Norm after: 10.316393499940567
Epoch 6037/10000, Prediction Accuracy = 63.61999999999999%, Loss = 0.32759777903556825
Epoch: 6037, Batch Gradient Norm: 12.461769335007856
Epoch: 6037, Batch Gradient Norm after: 12.461769335007856
Epoch 6038/10000, Prediction Accuracy = 63.538%, Loss = 0.3296875238418579
Epoch: 6038, Batch Gradient Norm: 14.926220577692575
Epoch: 6038, Batch Gradient Norm after: 14.926220577692575
Epoch 6039/10000, Prediction Accuracy = 63.532%, Loss = 0.335142719745636
Epoch: 6039, Batch Gradient Norm: 16.579018664398614
Epoch: 6039, Batch Gradient Norm after: 16.579018664398614
Epoch 6040/10000, Prediction Accuracy = 63.218%, Loss = 0.33858534693717957
Epoch: 6040, Batch Gradient Norm: 14.227388451496823
Epoch: 6040, Batch Gradient Norm after: 14.227388451496823
Epoch 6041/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.33235780000686643
Epoch: 6041, Batch Gradient Norm: 13.007813804888468
Epoch: 6041, Batch Gradient Norm after: 13.007813804888468
Epoch 6042/10000, Prediction Accuracy = 63.626%, Loss = 0.3298093616962433
Epoch: 6042, Batch Gradient Norm: 15.71580477515996
Epoch: 6042, Batch Gradient Norm after: 15.71580477515996
Epoch 6043/10000, Prediction Accuracy = 63.653999999999996%, Loss = 0.3320674657821655
Epoch: 6043, Batch Gradient Norm: 14.975871823293014
Epoch: 6043, Batch Gradient Norm after: 14.975871823293014
Epoch 6044/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.33270946741104124
Epoch: 6044, Batch Gradient Norm: 13.707130906655921
Epoch: 6044, Batch Gradient Norm after: 13.707130906655921
Epoch 6045/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3312735319137573
Epoch: 6045, Batch Gradient Norm: 12.298451277876161
Epoch: 6045, Batch Gradient Norm after: 12.298451277876161
Epoch 6046/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.3290661633014679
Epoch: 6046, Batch Gradient Norm: 13.079060481976086
Epoch: 6046, Batch Gradient Norm after: 13.079060481976086
Epoch 6047/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3320732474327087
Epoch: 6047, Batch Gradient Norm: 13.012766162187543
Epoch: 6047, Batch Gradient Norm after: 13.012766162187543
Epoch 6048/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.3296986699104309
Epoch: 6048, Batch Gradient Norm: 14.719868350653684
Epoch: 6048, Batch Gradient Norm after: 14.719868350653684
Epoch 6049/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.33239240646362306
Epoch: 6049, Batch Gradient Norm: 16.54850082993904
Epoch: 6049, Batch Gradient Norm after: 16.54850082993904
Epoch 6050/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.33552579283714296
Epoch: 6050, Batch Gradient Norm: 16.460720710110635
Epoch: 6050, Batch Gradient Norm after: 16.460720710110635
Epoch 6051/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3336977601051331
Epoch: 6051, Batch Gradient Norm: 15.041486936562729
Epoch: 6051, Batch Gradient Norm after: 15.041486936562729
Epoch 6052/10000, Prediction Accuracy = 63.61%, Loss = 0.3324785828590393
Epoch: 6052, Batch Gradient Norm: 17.497500594071067
Epoch: 6052, Batch Gradient Norm after: 17.497500594071067
Epoch 6053/10000, Prediction Accuracy = 63.55%, Loss = 0.33421153426170347
Epoch: 6053, Batch Gradient Norm: 18.566747283911386
Epoch: 6053, Batch Gradient Norm after: 18.566747283911386
Epoch 6054/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.33835187554359436
Epoch: 6054, Batch Gradient Norm: 17.73133455382011
Epoch: 6054, Batch Gradient Norm after: 17.73133455382011
Epoch 6055/10000, Prediction Accuracy = 63.556%, Loss = 0.33813772201538084
Epoch: 6055, Batch Gradient Norm: 15.237675582734566
Epoch: 6055, Batch Gradient Norm after: 15.237675582734566
Epoch 6056/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3342153072357178
Epoch: 6056, Batch Gradient Norm: 15.808734579862717
Epoch: 6056, Batch Gradient Norm after: 15.808734579862717
Epoch 6057/10000, Prediction Accuracy = 63.588%, Loss = 0.3331882119178772
Epoch: 6057, Batch Gradient Norm: 15.840980136133005
Epoch: 6057, Batch Gradient Norm after: 15.840980136133005
Epoch 6058/10000, Prediction Accuracy = 63.562%, Loss = 0.3326807141304016
Epoch: 6058, Batch Gradient Norm: 16.664925959750285
Epoch: 6058, Batch Gradient Norm after: 16.664925959750285
Epoch 6059/10000, Prediction Accuracy = 63.55%, Loss = 0.33513776063919065
Epoch: 6059, Batch Gradient Norm: 16.540463739904087
Epoch: 6059, Batch Gradient Norm after: 16.540463739904087
Epoch 6060/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.3324303090572357
Epoch: 6060, Batch Gradient Norm: 12.409334469792206
Epoch: 6060, Batch Gradient Norm after: 12.409334469792206
Epoch 6061/10000, Prediction Accuracy = 63.524%, Loss = 0.32821348309516907
Epoch: 6061, Batch Gradient Norm: 13.143397200129494
Epoch: 6061, Batch Gradient Norm after: 13.143397200129494
Epoch 6062/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.33015757203102114
Epoch: 6062, Batch Gradient Norm: 14.352989424596906
Epoch: 6062, Batch Gradient Norm after: 14.352989424596906
Epoch 6063/10000, Prediction Accuracy = 63.548%, Loss = 0.3314930617809296
Epoch: 6063, Batch Gradient Norm: 15.862472757970135
Epoch: 6063, Batch Gradient Norm after: 15.862472757970135
Epoch 6064/10000, Prediction Accuracy = 63.61%, Loss = 0.3339199125766754
Epoch: 6064, Batch Gradient Norm: 15.965565912065985
Epoch: 6064, Batch Gradient Norm after: 15.965565912065985
Epoch 6065/10000, Prediction Accuracy = 63.5%, Loss = 0.33369970321655273
Epoch: 6065, Batch Gradient Norm: 12.746895604741951
Epoch: 6065, Batch Gradient Norm after: 12.746895604741951
Epoch 6066/10000, Prediction Accuracy = 63.592%, Loss = 0.329474276304245
Epoch: 6066, Batch Gradient Norm: 13.078297892752817
Epoch: 6066, Batch Gradient Norm after: 13.078297892752817
Epoch 6067/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3280995607376099
Epoch: 6067, Batch Gradient Norm: 13.472881271306912
Epoch: 6067, Batch Gradient Norm after: 13.472881271306912
Epoch 6068/10000, Prediction Accuracy = 63.54%, Loss = 0.3286083936691284
Epoch: 6068, Batch Gradient Norm: 12.738168094668426
Epoch: 6068, Batch Gradient Norm after: 12.738168094668426
Epoch 6069/10000, Prediction Accuracy = 63.60999999999999%, Loss = 0.3291483700275421
Epoch: 6069, Batch Gradient Norm: 11.955930710429723
Epoch: 6069, Batch Gradient Norm after: 11.955930710429723
Epoch 6070/10000, Prediction Accuracy = 63.638%, Loss = 0.3282695710659027
Epoch: 6070, Batch Gradient Norm: 15.25084427679755
Epoch: 6070, Batch Gradient Norm after: 15.25084427679755
Epoch 6071/10000, Prediction Accuracy = 63.762%, Loss = 0.3329149544239044
Epoch: 6071, Batch Gradient Norm: 21.2017071187212
Epoch: 6071, Batch Gradient Norm after: 20.590911673236484
Epoch 6072/10000, Prediction Accuracy = 63.63000000000001%, Loss = 0.3424647331237793
Epoch: 6072, Batch Gradient Norm: 17.622417586750174
Epoch: 6072, Batch Gradient Norm after: 17.476804044315728
Epoch 6073/10000, Prediction Accuracy = 63.602%, Loss = 0.3366444408893585
Epoch: 6073, Batch Gradient Norm: 18.604824061572035
Epoch: 6073, Batch Gradient Norm after: 18.376449207502425
Epoch 6074/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.3366407692432404
Epoch: 6074, Batch Gradient Norm: 18.10354955792215
Epoch: 6074, Batch Gradient Norm after: 18.10354955792215
Epoch 6075/10000, Prediction Accuracy = 63.754000000000005%, Loss = 0.33823416829109193
Epoch: 6075, Batch Gradient Norm: 16.59633044544815
Epoch: 6075, Batch Gradient Norm after: 16.59633044544815
Epoch 6076/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.3364737033843994
Epoch: 6076, Batch Gradient Norm: 16.811712685076383
Epoch: 6076, Batch Gradient Norm after: 16.811712685076383
Epoch 6077/10000, Prediction Accuracy = 63.614%, Loss = 0.3358822584152222
Epoch: 6077, Batch Gradient Norm: 17.3293366379268
Epoch: 6077, Batch Gradient Norm after: 17.316942318971453
Epoch 6078/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3375874400138855
Epoch: 6078, Batch Gradient Norm: 17.560073891060508
Epoch: 6078, Batch Gradient Norm after: 17.560073891060508
Epoch 6079/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.33637229204177854
Epoch: 6079, Batch Gradient Norm: 17.706602367789614
Epoch: 6079, Batch Gradient Norm after: 17.706602367789614
Epoch 6080/10000, Prediction Accuracy = 63.548%, Loss = 0.33733671307563784
Epoch: 6080, Batch Gradient Norm: 17.292001680808994
Epoch: 6080, Batch Gradient Norm after: 17.064154365836878
Epoch 6081/10000, Prediction Accuracy = 63.58800000000001%, Loss = 0.336028641462326
Epoch: 6081, Batch Gradient Norm: 14.57341273898501
Epoch: 6081, Batch Gradient Norm after: 14.57341273898501
Epoch 6082/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3302449703216553
Epoch: 6082, Batch Gradient Norm: 13.71603627289019
Epoch: 6082, Batch Gradient Norm after: 13.71603627289019
Epoch 6083/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3298346996307373
Epoch: 6083, Batch Gradient Norm: 12.33004029802167
Epoch: 6083, Batch Gradient Norm after: 12.33004029802167
Epoch 6084/10000, Prediction Accuracy = 63.72600000000001%, Loss = 0.3282534182071686
Epoch: 6084, Batch Gradient Norm: 9.399857258882962
Epoch: 6084, Batch Gradient Norm after: 9.399857258882962
Epoch 6085/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.3266309261322021
Epoch: 6085, Batch Gradient Norm: 9.66793866997461
Epoch: 6085, Batch Gradient Norm after: 9.66793866997461
Epoch 6086/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.32551499605178835
Epoch: 6086, Batch Gradient Norm: 14.776305511374913
Epoch: 6086, Batch Gradient Norm after: 14.776305511374913
Epoch 6087/10000, Prediction Accuracy = 63.556%, Loss = 0.3314226150512695
Epoch: 6087, Batch Gradient Norm: 12.857135228936095
Epoch: 6087, Batch Gradient Norm after: 12.857135228936095
Epoch 6088/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.3317501485347748
Epoch: 6088, Batch Gradient Norm: 8.628418276662241
Epoch: 6088, Batch Gradient Norm after: 8.628418276662241
Epoch 6089/10000, Prediction Accuracy = 63.612%, Loss = 0.3216438710689545
Epoch: 6089, Batch Gradient Norm: 11.282414036531275
Epoch: 6089, Batch Gradient Norm after: 11.282414036531275
Epoch 6090/10000, Prediction Accuracy = 63.624%, Loss = 0.3272505164146423
Epoch: 6090, Batch Gradient Norm: 12.716481287168293
Epoch: 6090, Batch Gradient Norm after: 12.716481287168293
Epoch 6091/10000, Prediction Accuracy = 63.64799999999999%, Loss = 0.32942891120910645
Epoch: 6091, Batch Gradient Norm: 14.62957329921426
Epoch: 6091, Batch Gradient Norm after: 14.62957329921426
Epoch 6092/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.3333456993103027
Epoch: 6092, Batch Gradient Norm: 14.471680865536245
Epoch: 6092, Batch Gradient Norm after: 14.471680865536245
Epoch 6093/10000, Prediction Accuracy = 63.612%, Loss = 0.33070309162139894
Epoch: 6093, Batch Gradient Norm: 14.780321998149933
Epoch: 6093, Batch Gradient Norm after: 14.780321998149933
Epoch 6094/10000, Prediction Accuracy = 63.588%, Loss = 0.3301728188991547
Epoch: 6094, Batch Gradient Norm: 14.195449036630112
Epoch: 6094, Batch Gradient Norm after: 14.195449036630112
Epoch 6095/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.32895727157592775
Epoch: 6095, Batch Gradient Norm: 13.895767875683198
Epoch: 6095, Batch Gradient Norm after: 13.895767875683198
Epoch 6096/10000, Prediction Accuracy = 63.6%, Loss = 0.330363804101944
Epoch: 6096, Batch Gradient Norm: 15.984108628278236
Epoch: 6096, Batch Gradient Norm after: 15.984108628278236
Epoch 6097/10000, Prediction Accuracy = 63.58%, Loss = 0.33388425707817077
Epoch: 6097, Batch Gradient Norm: 17.986281380953233
Epoch: 6097, Batch Gradient Norm after: 17.986281380953233
Epoch 6098/10000, Prediction Accuracy = 63.58399999999999%, Loss = 0.3366471350193024
Epoch: 6098, Batch Gradient Norm: 14.96974694905128
Epoch: 6098, Batch Gradient Norm after: 14.96974694905128
Epoch 6099/10000, Prediction Accuracy = 63.53399999999999%, Loss = 0.33390216827392577
Epoch: 6099, Batch Gradient Norm: 10.940415332588387
Epoch: 6099, Batch Gradient Norm after: 10.940415332588387
Epoch 6100/10000, Prediction Accuracy = 63.6%, Loss = 0.3273219347000122
Epoch: 6100, Batch Gradient Norm: 10.620022440896761
Epoch: 6100, Batch Gradient Norm after: 10.620022440896761
Epoch 6101/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.32398658990859985
Epoch: 6101, Batch Gradient Norm: 12.619916448040854
Epoch: 6101, Batch Gradient Norm after: 12.619916448040854
Epoch 6102/10000, Prediction Accuracy = 63.614%, Loss = 0.3278502643108368
Epoch: 6102, Batch Gradient Norm: 14.542030777992895
Epoch: 6102, Batch Gradient Norm after: 14.542030777992895
Epoch 6103/10000, Prediction Accuracy = 63.614%, Loss = 0.33019617199897766
Epoch: 6103, Batch Gradient Norm: 11.913922111009416
Epoch: 6103, Batch Gradient Norm after: 11.913922111009416
Epoch 6104/10000, Prediction Accuracy = 63.602%, Loss = 0.3252623438835144
Epoch: 6104, Batch Gradient Norm: 12.483709715268787
Epoch: 6104, Batch Gradient Norm after: 12.483709715268787
Epoch 6105/10000, Prediction Accuracy = 63.71199999999999%, Loss = 0.32891793847084044
Epoch: 6105, Batch Gradient Norm: 13.05738586282269
Epoch: 6105, Batch Gradient Norm after: 13.05738586282269
Epoch 6106/10000, Prediction Accuracy = 63.576%, Loss = 0.3302621304988861
Epoch: 6106, Batch Gradient Norm: 14.20806885426615
Epoch: 6106, Batch Gradient Norm after: 14.20806885426615
Epoch 6107/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.33126166462898254
Epoch: 6107, Batch Gradient Norm: 17.25253777731259
Epoch: 6107, Batch Gradient Norm after: 17.25253777731259
Epoch 6108/10000, Prediction Accuracy = 63.66600000000001%, Loss = 0.33571756482124326
Epoch: 6108, Batch Gradient Norm: 12.971512183676586
Epoch: 6108, Batch Gradient Norm after: 12.971512183676586
Epoch 6109/10000, Prediction Accuracy = 63.628%, Loss = 0.3286400020122528
Epoch: 6109, Batch Gradient Norm: 15.414256639571146
Epoch: 6109, Batch Gradient Norm after: 15.414256639571146
Epoch 6110/10000, Prediction Accuracy = 63.532%, Loss = 0.3322105944156647
Epoch: 6110, Batch Gradient Norm: 17.58184462388188
Epoch: 6110, Batch Gradient Norm after: 17.58184462388188
Epoch 6111/10000, Prediction Accuracy = 63.536%, Loss = 0.3371038854122162
Epoch: 6111, Batch Gradient Norm: 19.236652214603033
Epoch: 6111, Batch Gradient Norm after: 18.917756675624762
Epoch 6112/10000, Prediction Accuracy = 63.682%, Loss = 0.33785783052444457
Epoch: 6112, Batch Gradient Norm: 13.834178957303175
Epoch: 6112, Batch Gradient Norm after: 13.834178957303175
Epoch 6113/10000, Prediction Accuracy = 63.720000000000006%, Loss = 0.32698769569396974
Epoch: 6113, Batch Gradient Norm: 14.050076815848461
Epoch: 6113, Batch Gradient Norm after: 14.050076815848461
Epoch 6114/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.32827305793762207
Epoch: 6114, Batch Gradient Norm: 13.818706933254719
Epoch: 6114, Batch Gradient Norm after: 13.818706933254719
Epoch 6115/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.32876273393630984
Epoch: 6115, Batch Gradient Norm: 12.236374243881524
Epoch: 6115, Batch Gradient Norm after: 12.236374243881524
Epoch 6116/10000, Prediction Accuracy = 63.66400000000001%, Loss = 0.3323476791381836
Epoch: 6116, Batch Gradient Norm: 12.564144424047976
Epoch: 6116, Batch Gradient Norm after: 12.564144424047976
Epoch 6117/10000, Prediction Accuracy = 63.6%, Loss = 0.32861433625221254
Epoch: 6117, Batch Gradient Norm: 13.252371568167218
Epoch: 6117, Batch Gradient Norm after: 13.252371568167218
Epoch 6118/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.33050936460494995
Epoch: 6118, Batch Gradient Norm: 13.095666708396404
Epoch: 6118, Batch Gradient Norm after: 13.095666708396404
Epoch 6119/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.327888023853302
Epoch: 6119, Batch Gradient Norm: 14.331172217728703
Epoch: 6119, Batch Gradient Norm after: 14.331172217728703
Epoch 6120/10000, Prediction Accuracy = 63.58800000000001%, Loss = 0.32964004278182985
Epoch: 6120, Batch Gradient Norm: 15.392171334356892
Epoch: 6120, Batch Gradient Norm after: 15.392171334356892
Epoch 6121/10000, Prediction Accuracy = 63.648%, Loss = 0.3297343611717224
Epoch: 6121, Batch Gradient Norm: 15.470394593338698
Epoch: 6121, Batch Gradient Norm after: 15.470394593338698
Epoch 6122/10000, Prediction Accuracy = 63.65%, Loss = 0.33115296959877016
Epoch: 6122, Batch Gradient Norm: 12.453118767428247
Epoch: 6122, Batch Gradient Norm after: 12.453118767428247
Epoch 6123/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.32924275398254393
Epoch: 6123, Batch Gradient Norm: 15.977131786196368
Epoch: 6123, Batch Gradient Norm after: 15.977131786196368
Epoch 6124/10000, Prediction Accuracy = 63.648%, Loss = 0.3314265787601471
Epoch: 6124, Batch Gradient Norm: 14.281142368145233
Epoch: 6124, Batch Gradient Norm after: 14.281142368145233
Epoch 6125/10000, Prediction Accuracy = 63.798%, Loss = 0.3286419212818146
Epoch: 6125, Batch Gradient Norm: 13.554348357541446
Epoch: 6125, Batch Gradient Norm after: 13.554348357541446
Epoch 6126/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.32730778455734255
Epoch: 6126, Batch Gradient Norm: 13.648404531808147
Epoch: 6126, Batch Gradient Norm after: 13.648404531808147
Epoch 6127/10000, Prediction Accuracy = 63.544%, Loss = 0.32978388071060183
Epoch: 6127, Batch Gradient Norm: 15.334256461920774
Epoch: 6127, Batch Gradient Norm after: 15.334256461920774
Epoch 6128/10000, Prediction Accuracy = 63.628%, Loss = 0.3305303633213043
Epoch: 6128, Batch Gradient Norm: 14.874450887012527
Epoch: 6128, Batch Gradient Norm after: 14.874450887012527
Epoch 6129/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.33229732513427734
Epoch: 6129, Batch Gradient Norm: 16.45454196639961
Epoch: 6129, Batch Gradient Norm after: 16.45454196639961
Epoch 6130/10000, Prediction Accuracy = 63.448%, Loss = 0.3347002506256104
Epoch: 6130, Batch Gradient Norm: 15.321576988075224
Epoch: 6130, Batch Gradient Norm after: 15.321576988075224
Epoch 6131/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.33271296620368956
Epoch: 6131, Batch Gradient Norm: 15.938094173306665
Epoch: 6131, Batch Gradient Norm after: 15.938094173306665
Epoch 6132/10000, Prediction Accuracy = 63.538%, Loss = 0.3320539236068726
Epoch: 6132, Batch Gradient Norm: 14.593405691025394
Epoch: 6132, Batch Gradient Norm after: 14.593405691025394
Epoch 6133/10000, Prediction Accuracy = 63.598%, Loss = 0.33266666531562805
Epoch: 6133, Batch Gradient Norm: 13.334467762581923
Epoch: 6133, Batch Gradient Norm after: 13.334467762581923
Epoch 6134/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.33088732361793516
Epoch: 6134, Batch Gradient Norm: 11.866078103958435
Epoch: 6134, Batch Gradient Norm after: 11.866078103958435
Epoch 6135/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3278149843215942
Epoch: 6135, Batch Gradient Norm: 12.981649650397715
Epoch: 6135, Batch Gradient Norm after: 12.981649650397715
Epoch 6136/10000, Prediction Accuracy = 63.602%, Loss = 0.3272049903869629
Epoch: 6136, Batch Gradient Norm: 15.885294767875031
Epoch: 6136, Batch Gradient Norm after: 15.885294767875031
Epoch 6137/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.33256134390830994
Epoch: 6137, Batch Gradient Norm: 12.88705028131707
Epoch: 6137, Batch Gradient Norm after: 12.88705028131707
Epoch 6138/10000, Prediction Accuracy = 63.775999999999996%, Loss = 0.32682972550392153
Epoch: 6138, Batch Gradient Norm: 14.825144690916224
Epoch: 6138, Batch Gradient Norm after: 14.825144690916224
Epoch 6139/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.32950093746185305
Epoch: 6139, Batch Gradient Norm: 16.913632607060798
Epoch: 6139, Batch Gradient Norm after: 16.913632607060798
Epoch 6140/10000, Prediction Accuracy = 63.576%, Loss = 0.33528622388839724
Epoch: 6140, Batch Gradient Norm: 15.523381535992517
Epoch: 6140, Batch Gradient Norm after: 15.523381535992517
Epoch 6141/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.3309819340705872
Epoch: 6141, Batch Gradient Norm: 16.769960884431406
Epoch: 6141, Batch Gradient Norm after: 16.769960884431406
Epoch 6142/10000, Prediction Accuracy = 63.584%, Loss = 0.3338606536388397
Epoch: 6142, Batch Gradient Norm: 15.902635506195791
Epoch: 6142, Batch Gradient Norm after: 15.902635506195791
Epoch 6143/10000, Prediction Accuracy = 63.67%, Loss = 0.3324536979198456
Epoch: 6143, Batch Gradient Norm: 13.726853642149173
Epoch: 6143, Batch Gradient Norm after: 13.726853642149173
Epoch 6144/10000, Prediction Accuracy = 63.636%, Loss = 0.32916526198387147
Epoch: 6144, Batch Gradient Norm: 9.642735414752842
Epoch: 6144, Batch Gradient Norm after: 9.642735414752842
Epoch 6145/10000, Prediction Accuracy = 63.66600000000001%, Loss = 0.3242518544197083
Epoch: 6145, Batch Gradient Norm: 12.12576120904828
Epoch: 6145, Batch Gradient Norm after: 12.12576120904828
Epoch 6146/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.327723628282547
Epoch: 6146, Batch Gradient Norm: 12.317159870754727
Epoch: 6146, Batch Gradient Norm after: 12.317159870754727
Epoch 6147/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.326945823431015
Epoch: 6147, Batch Gradient Norm: 9.883494802542296
Epoch: 6147, Batch Gradient Norm after: 9.883494802542296
Epoch 6148/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.32475524544715884
Epoch: 6148, Batch Gradient Norm: 12.470423474395337
Epoch: 6148, Batch Gradient Norm after: 12.470423474395337
Epoch 6149/10000, Prediction Accuracy = 63.617999999999995%, Loss = 0.3295039117336273
Epoch: 6149, Batch Gradient Norm: 13.590607558568056
Epoch: 6149, Batch Gradient Norm after: 13.590607558568056
Epoch 6150/10000, Prediction Accuracy = 63.672000000000004%, Loss = 0.32841427326202394
Epoch: 6150, Batch Gradient Norm: 14.331985413125372
Epoch: 6150, Batch Gradient Norm after: 14.331985413125372
Epoch 6151/10000, Prediction Accuracy = 63.614%, Loss = 0.33060444593429567
Epoch: 6151, Batch Gradient Norm: 12.815923399503623
Epoch: 6151, Batch Gradient Norm after: 12.815923399503623
Epoch 6152/10000, Prediction Accuracy = 63.6%, Loss = 0.3287644863128662
Epoch: 6152, Batch Gradient Norm: 11.063312505515801
Epoch: 6152, Batch Gradient Norm after: 11.063312505515801
Epoch 6153/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.32351893186569214
Epoch: 6153, Batch Gradient Norm: 12.106726008307342
Epoch: 6153, Batch Gradient Norm after: 12.106726008307342
Epoch 6154/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.3279064953327179
Epoch: 6154, Batch Gradient Norm: 15.006489037412672
Epoch: 6154, Batch Gradient Norm after: 15.006489037412672
Epoch 6155/10000, Prediction Accuracy = 63.678%, Loss = 0.32887675166130065
Epoch: 6155, Batch Gradient Norm: 15.121343261915811
Epoch: 6155, Batch Gradient Norm after: 15.121343261915811
Epoch 6156/10000, Prediction Accuracy = 63.636%, Loss = 0.329357773065567
Epoch: 6156, Batch Gradient Norm: 13.745874366072792
Epoch: 6156, Batch Gradient Norm after: 13.745874366072792
Epoch 6157/10000, Prediction Accuracy = 63.55%, Loss = 0.33188289403915405
Epoch: 6157, Batch Gradient Norm: 16.121649676280004
Epoch: 6157, Batch Gradient Norm after: 16.121649676280004
Epoch 6158/10000, Prediction Accuracy = 63.617999999999995%, Loss = 0.33178119659423827
Epoch: 6158, Batch Gradient Norm: 14.858723429511972
Epoch: 6158, Batch Gradient Norm after: 14.858723429511972
Epoch 6159/10000, Prediction Accuracy = 63.6%, Loss = 0.3307534992694855
Epoch: 6159, Batch Gradient Norm: 12.608076352733773
Epoch: 6159, Batch Gradient Norm after: 12.608076352733773
Epoch 6160/10000, Prediction Accuracy = 63.612%, Loss = 0.3271166682243347
Epoch: 6160, Batch Gradient Norm: 12.284696979411711
Epoch: 6160, Batch Gradient Norm after: 12.284696979411711
Epoch 6161/10000, Prediction Accuracy = 63.666%, Loss = 0.325832211971283
Epoch: 6161, Batch Gradient Norm: 12.931217621573683
Epoch: 6161, Batch Gradient Norm after: 12.931217621573683
Epoch 6162/10000, Prediction Accuracy = 63.69199999999999%, Loss = 0.32642878890037536
Epoch: 6162, Batch Gradient Norm: 12.428712756670832
Epoch: 6162, Batch Gradient Norm after: 12.428712756670832
Epoch 6163/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.3273069500923157
Epoch: 6163, Batch Gradient Norm: 10.74847639433565
Epoch: 6163, Batch Gradient Norm after: 10.74847639433565
Epoch 6164/10000, Prediction Accuracy = 63.61%, Loss = 0.32748807668685914
Epoch: 6164, Batch Gradient Norm: 11.872129911023345
Epoch: 6164, Batch Gradient Norm after: 11.872129911023345
Epoch 6165/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.32658531069755553
Epoch: 6165, Batch Gradient Norm: 8.89410904710502
Epoch: 6165, Batch Gradient Norm after: 8.89410904710502
Epoch 6166/10000, Prediction Accuracy = 63.624%, Loss = 0.3238495051860809
Epoch: 6166, Batch Gradient Norm: 14.55182669259513
Epoch: 6166, Batch Gradient Norm after: 14.55182669259513
Epoch 6167/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.32934553027153013
Epoch: 6167, Batch Gradient Norm: 17.460354727117753
Epoch: 6167, Batch Gradient Norm after: 17.310087480850008
Epoch 6168/10000, Prediction Accuracy = 63.646%, Loss = 0.33360591530799866
Epoch: 6168, Batch Gradient Norm: 14.411475636764077
Epoch: 6168, Batch Gradient Norm after: 14.411475636764077
Epoch 6169/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.3317346751689911
Epoch: 6169, Batch Gradient Norm: 16.814157178080027
Epoch: 6169, Batch Gradient Norm after: 16.814157178080027
Epoch 6170/10000, Prediction Accuracy = 63.71999999999999%, Loss = 0.33387049436569216
Epoch: 6170, Batch Gradient Norm: 16.76904090902916
Epoch: 6170, Batch Gradient Norm after: 16.76904090902916
Epoch 6171/10000, Prediction Accuracy = 63.65%, Loss = 0.33218454718589785
Epoch: 6171, Batch Gradient Norm: 16.45090643069788
Epoch: 6171, Batch Gradient Norm after: 16.45090643069788
Epoch 6172/10000, Prediction Accuracy = 63.656000000000006%, Loss = 0.3319580614566803
Epoch: 6172, Batch Gradient Norm: 14.911892678317924
Epoch: 6172, Batch Gradient Norm after: 14.911892678317924
Epoch 6173/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.32956241369247435
Epoch: 6173, Batch Gradient Norm: 13.8092976181489
Epoch: 6173, Batch Gradient Norm after: 13.8092976181489
Epoch 6174/10000, Prediction Accuracy = 63.658%, Loss = 0.3276886999607086
Epoch: 6174, Batch Gradient Norm: 11.275975828620458
Epoch: 6174, Batch Gradient Norm after: 11.275975828620458
Epoch 6175/10000, Prediction Accuracy = 63.678%, Loss = 0.32630489468574525
Epoch: 6175, Batch Gradient Norm: 15.014026517525213
Epoch: 6175, Batch Gradient Norm after: 15.014026517525213
Epoch 6176/10000, Prediction Accuracy = 63.686%, Loss = 0.33257052302360535
Epoch: 6176, Batch Gradient Norm: 14.703243171580054
Epoch: 6176, Batch Gradient Norm after: 14.703243171580054
Epoch 6177/10000, Prediction Accuracy = 63.702%, Loss = 0.3273000597953796
Epoch: 6177, Batch Gradient Norm: 13.981933204292742
Epoch: 6177, Batch Gradient Norm after: 13.981933204292742
Epoch 6178/10000, Prediction Accuracy = 63.638%, Loss = 0.3263100802898407
Epoch: 6178, Batch Gradient Norm: 12.189261995494338
Epoch: 6178, Batch Gradient Norm after: 12.189261995494338
Epoch 6179/10000, Prediction Accuracy = 63.74400000000001%, Loss = 0.3266115665435791
Epoch: 6179, Batch Gradient Norm: 13.363985005257298
Epoch: 6179, Batch Gradient Norm after: 13.363985005257298
Epoch 6180/10000, Prediction Accuracy = 63.73%, Loss = 0.3262315034866333
Epoch: 6180, Batch Gradient Norm: 16.114351137135323
Epoch: 6180, Batch Gradient Norm after: 16.114351137135323
Epoch 6181/10000, Prediction Accuracy = 63.67%, Loss = 0.33167784214019774
Epoch: 6181, Batch Gradient Norm: 15.730986081678028
Epoch: 6181, Batch Gradient Norm after: 15.730986081678028
Epoch 6182/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3341654181480408
Epoch: 6182, Batch Gradient Norm: 15.831617714125771
Epoch: 6182, Batch Gradient Norm after: 15.831617714125771
Epoch 6183/10000, Prediction Accuracy = 63.70799999999999%, Loss = 0.33262400031089784
Epoch: 6183, Batch Gradient Norm: 13.33415131151715
Epoch: 6183, Batch Gradient Norm after: 13.33415131151715
Epoch 6184/10000, Prediction Accuracy = 63.602%, Loss = 0.3259332120418549
Epoch: 6184, Batch Gradient Norm: 15.479836304712794
Epoch: 6184, Batch Gradient Norm after: 15.479836304712794
Epoch 6185/10000, Prediction Accuracy = 63.767999999999994%, Loss = 0.3303822696208954
Epoch: 6185, Batch Gradient Norm: 14.141981088441181
Epoch: 6185, Batch Gradient Norm after: 14.141981088441181
Epoch 6186/10000, Prediction Accuracy = 63.64200000000001%, Loss = 0.32790862321853637
Epoch: 6186, Batch Gradient Norm: 14.381713729882797
Epoch: 6186, Batch Gradient Norm after: 14.381713729882797
Epoch 6187/10000, Prediction Accuracy = 63.574%, Loss = 0.33083529472351075
Epoch: 6187, Batch Gradient Norm: 14.058151907594114
Epoch: 6187, Batch Gradient Norm after: 14.058151907594114
Epoch 6188/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3286868453025818
Epoch: 6188, Batch Gradient Norm: 13.88336498523742
Epoch: 6188, Batch Gradient Norm after: 13.88336498523742
Epoch 6189/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.32882219552993774
Epoch: 6189, Batch Gradient Norm: 10.46101051329618
Epoch: 6189, Batch Gradient Norm after: 10.46101051329618
Epoch 6190/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3239880621433258
Epoch: 6190, Batch Gradient Norm: 9.86100084510496
Epoch: 6190, Batch Gradient Norm after: 9.86100084510496
Epoch 6191/10000, Prediction Accuracy = 63.528%, Loss = 0.32386484146118166
Epoch: 6191, Batch Gradient Norm: 11.981384574878675
Epoch: 6191, Batch Gradient Norm after: 11.981384574878675
Epoch 6192/10000, Prediction Accuracy = 63.662%, Loss = 0.3248469471931458
Epoch: 6192, Batch Gradient Norm: 14.423566428819043
Epoch: 6192, Batch Gradient Norm after: 14.423566428819043
Epoch 6193/10000, Prediction Accuracy = 63.75599999999999%, Loss = 0.3268210530281067
Epoch: 6193, Batch Gradient Norm: 14.131697018339699
Epoch: 6193, Batch Gradient Norm after: 14.131697018339699
Epoch 6194/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3286141574382782
Epoch: 6194, Batch Gradient Norm: 14.517175835573449
Epoch: 6194, Batch Gradient Norm after: 14.517175835573449
Epoch 6195/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.3278837203979492
Epoch: 6195, Batch Gradient Norm: 14.64252819182091
Epoch: 6195, Batch Gradient Norm after: 14.64252819182091
Epoch 6196/10000, Prediction Accuracy = 63.69%, Loss = 0.3284785389900208
Epoch: 6196, Batch Gradient Norm: 11.453205733828804
Epoch: 6196, Batch Gradient Norm after: 11.453205733828804
Epoch 6197/10000, Prediction Accuracy = 63.69200000000001%, Loss = 0.32487844228744506
Epoch: 6197, Batch Gradient Norm: 10.023805680005632
Epoch: 6197, Batch Gradient Norm after: 10.023805680005632
Epoch 6198/10000, Prediction Accuracy = 63.428%, Loss = 0.32398966550827024
Epoch: 6198, Batch Gradient Norm: 12.873619901934607
Epoch: 6198, Batch Gradient Norm after: 12.873619901934607
Epoch 6199/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.3284570038318634
Epoch: 6199, Batch Gradient Norm: 15.854667280292077
Epoch: 6199, Batch Gradient Norm after: 15.854667280292077
Epoch 6200/10000, Prediction Accuracy = 63.678%, Loss = 0.3300277054309845
Epoch: 6200, Batch Gradient Norm: 14.147806203708063
Epoch: 6200, Batch Gradient Norm after: 14.147806203708063
Epoch 6201/10000, Prediction Accuracy = 63.424%, Loss = 0.3278713524341583
Epoch: 6201, Batch Gradient Norm: 13.893151390368486
Epoch: 6201, Batch Gradient Norm after: 13.893151390368486
Epoch 6202/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.32706313133239745
Epoch: 6202, Batch Gradient Norm: 13.038032962659148
Epoch: 6202, Batch Gradient Norm after: 13.038032962659148
Epoch 6203/10000, Prediction Accuracy = 63.726%, Loss = 0.3265946865081787
Epoch: 6203, Batch Gradient Norm: 12.27625296038068
Epoch: 6203, Batch Gradient Norm after: 12.27625296038068
Epoch 6204/10000, Prediction Accuracy = 63.674%, Loss = 0.32534810304641726
Epoch: 6204, Batch Gradient Norm: 11.846949917685581
Epoch: 6204, Batch Gradient Norm after: 11.846949917685581
Epoch 6205/10000, Prediction Accuracy = 63.60999999999999%, Loss = 0.3269895911216736
Epoch: 6205, Batch Gradient Norm: 12.292491253649313
Epoch: 6205, Batch Gradient Norm after: 12.292491253649313
Epoch 6206/10000, Prediction Accuracy = 63.612%, Loss = 0.32747073769569396
Epoch: 6206, Batch Gradient Norm: 12.851522155460255
Epoch: 6206, Batch Gradient Norm after: 12.851522155460255
Epoch 6207/10000, Prediction Accuracy = 63.636%, Loss = 0.3258020758628845
Epoch: 6207, Batch Gradient Norm: 14.18130286126337
Epoch: 6207, Batch Gradient Norm after: 14.18130286126337
Epoch 6208/10000, Prediction Accuracy = 63.641999999999996%, Loss = 0.3274422287940979
Epoch: 6208, Batch Gradient Norm: 14.861966232913232
Epoch: 6208, Batch Gradient Norm after: 14.861966232913232
Epoch 6209/10000, Prediction Accuracy = 63.544%, Loss = 0.33015064597129823
Epoch: 6209, Batch Gradient Norm: 12.59497424848263
Epoch: 6209, Batch Gradient Norm after: 12.59497424848263
Epoch 6210/10000, Prediction Accuracy = 63.722%, Loss = 0.32515714764595033
Epoch: 6210, Batch Gradient Norm: 15.056663072773997
Epoch: 6210, Batch Gradient Norm after: 15.056663072773997
Epoch 6211/10000, Prediction Accuracy = 63.552%, Loss = 0.33308970332145693
Epoch: 6211, Batch Gradient Norm: 14.238822787338083
Epoch: 6211, Batch Gradient Norm after: 14.238822787338083
Epoch 6212/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.32844231128692625
Epoch: 6212, Batch Gradient Norm: 12.844135928476677
Epoch: 6212, Batch Gradient Norm after: 12.844135928476677
Epoch 6213/10000, Prediction Accuracy = 63.598%, Loss = 0.3255613386631012
Epoch: 6213, Batch Gradient Norm: 13.050452494497812
Epoch: 6213, Batch Gradient Norm after: 13.050452494497812
Epoch 6214/10000, Prediction Accuracy = 63.676%, Loss = 0.32805901765823364
Epoch: 6214, Batch Gradient Norm: 15.37500406276741
Epoch: 6214, Batch Gradient Norm after: 15.37500406276741
Epoch 6215/10000, Prediction Accuracy = 63.602%, Loss = 0.3310347437858582
Epoch: 6215, Batch Gradient Norm: 18.59128601836644
Epoch: 6215, Batch Gradient Norm after: 18.59128601836644
Epoch 6216/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.33317965269088745
Epoch: 6216, Batch Gradient Norm: 16.30670644328355
Epoch: 6216, Batch Gradient Norm after: 16.30670644328355
Epoch 6217/10000, Prediction Accuracy = 63.778%, Loss = 0.33022417426109313
Epoch: 6217, Batch Gradient Norm: 15.136941631775146
Epoch: 6217, Batch Gradient Norm after: 15.136941631775146
Epoch 6218/10000, Prediction Accuracy = 63.636%, Loss = 0.32639355063438413
Epoch: 6218, Batch Gradient Norm: 15.650422575759483
Epoch: 6218, Batch Gradient Norm after: 15.650422575759483
Epoch 6219/10000, Prediction Accuracy = 63.602%, Loss = 0.33166012167930603
Epoch: 6219, Batch Gradient Norm: 13.287821182175868
Epoch: 6219, Batch Gradient Norm after: 13.287821182175868
Epoch 6220/10000, Prediction Accuracy = 63.714%, Loss = 0.3273309588432312
Epoch: 6220, Batch Gradient Norm: 14.02249538329925
Epoch: 6220, Batch Gradient Norm after: 14.02249538329925
Epoch 6221/10000, Prediction Accuracy = 63.688%, Loss = 0.3276576459407806
Epoch: 6221, Batch Gradient Norm: 12.160856874424557
Epoch: 6221, Batch Gradient Norm after: 12.160856874424557
Epoch 6222/10000, Prediction Accuracy = 63.588%, Loss = 0.3253922700881958
Epoch: 6222, Batch Gradient Norm: 12.064834546297329
Epoch: 6222, Batch Gradient Norm after: 12.064834546297329
Epoch 6223/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.32365740537643434
Epoch: 6223, Batch Gradient Norm: 12.264536241751362
Epoch: 6223, Batch Gradient Norm after: 12.264536241751362
Epoch 6224/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3272096157073975
Epoch: 6224, Batch Gradient Norm: 14.757272476462132
Epoch: 6224, Batch Gradient Norm after: 14.757272476462132
Epoch 6225/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.3266338765621185
Epoch: 6225, Batch Gradient Norm: 18.35403149278374
Epoch: 6225, Batch Gradient Norm after: 18.35403149278374
Epoch 6226/10000, Prediction Accuracy = 63.638%, Loss = 0.33201115727424624
Epoch: 6226, Batch Gradient Norm: 12.269130004767279
Epoch: 6226, Batch Gradient Norm after: 12.269130004767279
Epoch 6227/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.3268642842769623
Epoch: 6227, Batch Gradient Norm: 15.595299953504169
Epoch: 6227, Batch Gradient Norm after: 15.595299953504169
Epoch 6228/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.33048295974731445
Epoch: 6228, Batch Gradient Norm: 17.226674831269033
Epoch: 6228, Batch Gradient Norm after: 17.226674831269033
Epoch 6229/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.3312721073627472
Epoch: 6229, Batch Gradient Norm: 15.055364121397707
Epoch: 6229, Batch Gradient Norm after: 15.055364121397707
Epoch 6230/10000, Prediction Accuracy = 63.702%, Loss = 0.32785576581954956
Epoch: 6230, Batch Gradient Norm: 12.934437217739633
Epoch: 6230, Batch Gradient Norm after: 12.934437217739633
Epoch 6231/10000, Prediction Accuracy = 63.56600000000001%, Loss = 0.32553682923316957
Epoch: 6231, Batch Gradient Norm: 15.080528108756605
Epoch: 6231, Batch Gradient Norm after: 15.080528108756605
Epoch 6232/10000, Prediction Accuracy = 63.598%, Loss = 0.33308848142623904
Epoch: 6232, Batch Gradient Norm: 13.123269117172493
Epoch: 6232, Batch Gradient Norm after: 13.123269117172493
Epoch 6233/10000, Prediction Accuracy = 63.622%, Loss = 0.3254149317741394
Epoch: 6233, Batch Gradient Norm: 14.489381828935526
Epoch: 6233, Batch Gradient Norm after: 14.489381828935526
Epoch 6234/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.32792900800704955
Epoch: 6234, Batch Gradient Norm: 15.457518300502079
Epoch: 6234, Batch Gradient Norm after: 15.457518300502079
Epoch 6235/10000, Prediction Accuracy = 63.664%, Loss = 0.32992759346961975
Epoch: 6235, Batch Gradient Norm: 17.891495505310242
Epoch: 6235, Batch Gradient Norm after: 17.891495505310242
Epoch 6236/10000, Prediction Accuracy = 63.653999999999996%, Loss = 0.3365859091281891
Epoch: 6236, Batch Gradient Norm: 18.07756718809106
Epoch: 6236, Batch Gradient Norm after: 17.744501207282063
Epoch 6237/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.33313941955566406
Epoch: 6237, Batch Gradient Norm: 14.619803636466452
Epoch: 6237, Batch Gradient Norm after: 14.619803636466452
Epoch 6238/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.3290018141269684
Epoch: 6238, Batch Gradient Norm: 14.619160131915358
Epoch: 6238, Batch Gradient Norm after: 14.619160131915358
Epoch 6239/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.3283402144908905
Epoch: 6239, Batch Gradient Norm: 14.491863130826665
Epoch: 6239, Batch Gradient Norm after: 14.491863130826665
Epoch 6240/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.32903832793235777
Epoch: 6240, Batch Gradient Norm: 13.02291352884518
Epoch: 6240, Batch Gradient Norm after: 13.02291352884518
Epoch 6241/10000, Prediction Accuracy = 63.726%, Loss = 0.327041494846344
Epoch: 6241, Batch Gradient Norm: 12.185481464282933
Epoch: 6241, Batch Gradient Norm after: 12.185481464282933
Epoch 6242/10000, Prediction Accuracy = 63.714%, Loss = 0.3257891178131104
Epoch: 6242, Batch Gradient Norm: 11.91564327282118
Epoch: 6242, Batch Gradient Norm after: 11.91564327282118
Epoch 6243/10000, Prediction Accuracy = 63.616%, Loss = 0.32404390573501585
Epoch: 6243, Batch Gradient Norm: 13.237003027057044
Epoch: 6243, Batch Gradient Norm after: 13.237003027057044
Epoch 6244/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.3286949276924133
Epoch: 6244, Batch Gradient Norm: 13.881689551767495
Epoch: 6244, Batch Gradient Norm after: 13.881689551767495
Epoch 6245/10000, Prediction Accuracy = 63.653999999999996%, Loss = 0.3254986107349396
Epoch: 6245, Batch Gradient Norm: 12.397518426638907
Epoch: 6245, Batch Gradient Norm after: 12.397518426638907
Epoch 6246/10000, Prediction Accuracy = 63.645999999999994%, Loss = 0.32651911973953246
Epoch: 6246, Batch Gradient Norm: 12.472883523180478
Epoch: 6246, Batch Gradient Norm after: 12.472883523180478
Epoch 6247/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.3255782902240753
Epoch: 6247, Batch Gradient Norm: 10.56990904444727
Epoch: 6247, Batch Gradient Norm after: 10.56990904444727
Epoch 6248/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.32173445224761965
Epoch: 6248, Batch Gradient Norm: 12.99994528215721
Epoch: 6248, Batch Gradient Norm after: 12.99994528215721
Epoch 6249/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.32497346997261045
Epoch: 6249, Batch Gradient Norm: 14.113637110016361
Epoch: 6249, Batch Gradient Norm after: 14.113637110016361
Epoch 6250/10000, Prediction Accuracy = 63.734%, Loss = 0.32957481741905215
Epoch: 6250, Batch Gradient Norm: 15.299918597501076
Epoch: 6250, Batch Gradient Norm after: 15.299918597501076
Epoch 6251/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.3305849492549896
Epoch: 6251, Batch Gradient Norm: 14.293658975335237
Epoch: 6251, Batch Gradient Norm after: 14.293658975335237
Epoch 6252/10000, Prediction Accuracy = 63.69199999999999%, Loss = 0.3272181868553162
Epoch: 6252, Batch Gradient Norm: 13.134842529505564
Epoch: 6252, Batch Gradient Norm after: 13.134842529505564
Epoch 6253/10000, Prediction Accuracy = 63.638%, Loss = 0.32639960050582884
Epoch: 6253, Batch Gradient Norm: 13.228545181581351
Epoch: 6253, Batch Gradient Norm after: 13.228545181581351
Epoch 6254/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.32574755549430845
Epoch: 6254, Batch Gradient Norm: 12.94487527356879
Epoch: 6254, Batch Gradient Norm after: 12.94487527356879
Epoch 6255/10000, Prediction Accuracy = 63.73%, Loss = 0.32445144057273867
Epoch: 6255, Batch Gradient Norm: 14.650729908537333
Epoch: 6255, Batch Gradient Norm after: 14.650729908537333
Epoch 6256/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.3254644572734833
Epoch: 6256, Batch Gradient Norm: 13.044364967667297
Epoch: 6256, Batch Gradient Norm after: 13.044364967667297
Epoch 6257/10000, Prediction Accuracy = 63.638%, Loss = 0.32521284818649293
Epoch: 6257, Batch Gradient Norm: 11.746228819693266
Epoch: 6257, Batch Gradient Norm after: 11.746228819693266
Epoch 6258/10000, Prediction Accuracy = 63.786%, Loss = 0.32308834195137026
Epoch: 6258, Batch Gradient Norm: 13.841096810486524
Epoch: 6258, Batch Gradient Norm after: 13.841096810486524
Epoch 6259/10000, Prediction Accuracy = 63.60999999999999%, Loss = 0.3263492286205292
Epoch: 6259, Batch Gradient Norm: 13.633350872595807
Epoch: 6259, Batch Gradient Norm after: 13.633350872595807
Epoch 6260/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.3251647174358368
Epoch: 6260, Batch Gradient Norm: 15.444773398649684
Epoch: 6260, Batch Gradient Norm after: 15.444773398649684
Epoch 6261/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.3283605337142944
Epoch: 6261, Batch Gradient Norm: 12.563224809644739
Epoch: 6261, Batch Gradient Norm after: 12.563224809644739
Epoch 6262/10000, Prediction Accuracy = 63.84400000000001%, Loss = 0.32548118233680723
Epoch: 6262, Batch Gradient Norm: 11.61226211237856
Epoch: 6262, Batch Gradient Norm after: 11.61226211237856
Epoch 6263/10000, Prediction Accuracy = 63.730000000000004%, Loss = 0.3251899182796478
Epoch: 6263, Batch Gradient Norm: 15.604069211354368
Epoch: 6263, Batch Gradient Norm after: 15.604069211354368
Epoch 6264/10000, Prediction Accuracy = 63.638%, Loss = 0.32856541872024536
Epoch: 6264, Batch Gradient Norm: 14.151673458815983
Epoch: 6264, Batch Gradient Norm after: 14.151673458815983
Epoch 6265/10000, Prediction Accuracy = 63.714%, Loss = 0.3268748879432678
Epoch: 6265, Batch Gradient Norm: 13.62873578224559
Epoch: 6265, Batch Gradient Norm after: 13.62873578224559
Epoch 6266/10000, Prediction Accuracy = 63.75600000000001%, Loss = 0.32511473894119264
Epoch: 6266, Batch Gradient Norm: 15.337551376957816
Epoch: 6266, Batch Gradient Norm after: 15.337551376957816
Epoch 6267/10000, Prediction Accuracy = 63.602%, Loss = 0.3266772449016571
Epoch: 6267, Batch Gradient Norm: 15.39452078725586
Epoch: 6267, Batch Gradient Norm after: 15.39452078725586
Epoch 6268/10000, Prediction Accuracy = 63.662%, Loss = 0.3302021324634552
Epoch: 6268, Batch Gradient Norm: 13.752397602043336
Epoch: 6268, Batch Gradient Norm after: 13.752397602043336
Epoch 6269/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.32599303126335144
Epoch: 6269, Batch Gradient Norm: 11.260818892669631
Epoch: 6269, Batch Gradient Norm after: 11.260818892669631
Epoch 6270/10000, Prediction Accuracy = 63.652%, Loss = 0.3240308344364166
Epoch: 6270, Batch Gradient Norm: 13.072218725380461
Epoch: 6270, Batch Gradient Norm after: 13.072218725380461
Epoch 6271/10000, Prediction Accuracy = 63.626%, Loss = 0.32662464380264283
Epoch: 6271, Batch Gradient Norm: 10.579831421178836
Epoch: 6271, Batch Gradient Norm after: 10.579831421178836
Epoch 6272/10000, Prediction Accuracy = 63.678%, Loss = 0.32284811735153196
Epoch: 6272, Batch Gradient Norm: 13.120763680422286
Epoch: 6272, Batch Gradient Norm after: 13.120763680422286
Epoch 6273/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3267554581165314
Epoch: 6273, Batch Gradient Norm: 16.851546795823584
Epoch: 6273, Batch Gradient Norm after: 16.851546795823584
Epoch 6274/10000, Prediction Accuracy = 63.617999999999995%, Loss = 0.3287306010723114
Epoch: 6274, Batch Gradient Norm: 18.414829656395522
Epoch: 6274, Batch Gradient Norm after: 17.637765885738048
Epoch 6275/10000, Prediction Accuracy = 63.686%, Loss = 0.32961684465408325
Epoch: 6275, Batch Gradient Norm: 14.140781814501612
Epoch: 6275, Batch Gradient Norm after: 14.140781814501612
Epoch 6276/10000, Prediction Accuracy = 63.628%, Loss = 0.3269052803516388
Epoch: 6276, Batch Gradient Norm: 15.589341789697036
Epoch: 6276, Batch Gradient Norm after: 15.589341789697036
Epoch 6277/10000, Prediction Accuracy = 63.874%, Loss = 0.32918472290039064
Epoch: 6277, Batch Gradient Norm: 16.479477494511492
Epoch: 6277, Batch Gradient Norm after: 16.479477494511492
Epoch 6278/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.3289072036743164
Epoch: 6278, Batch Gradient Norm: 13.030086492801308
Epoch: 6278, Batch Gradient Norm after: 13.030086492801308
Epoch 6279/10000, Prediction Accuracy = 63.71%, Loss = 0.32521076798439025
Epoch: 6279, Batch Gradient Norm: 9.808751105321612
Epoch: 6279, Batch Gradient Norm after: 9.808751105321612
Epoch 6280/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.320644348859787
Epoch: 6280, Batch Gradient Norm: 12.757878134577753
Epoch: 6280, Batch Gradient Norm after: 12.757878134577753
Epoch 6281/10000, Prediction Accuracy = 63.67%, Loss = 0.32782312035560607
Epoch: 6281, Batch Gradient Norm: 13.274886625323354
Epoch: 6281, Batch Gradient Norm after: 13.274886625323354
Epoch 6282/10000, Prediction Accuracy = 63.669999999999995%, Loss = 0.32384868860244753
Epoch: 6282, Batch Gradient Norm: 13.834470494574312
Epoch: 6282, Batch Gradient Norm after: 13.834470494574312
Epoch 6283/10000, Prediction Accuracy = 63.69199999999999%, Loss = 0.3233526885509491
Epoch: 6283, Batch Gradient Norm: 17.241480906353694
Epoch: 6283, Batch Gradient Norm after: 16.502563188535774
Epoch 6284/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.3304018497467041
Epoch: 6284, Batch Gradient Norm: 14.39073860658803
Epoch: 6284, Batch Gradient Norm after: 14.39073860658803
Epoch 6285/10000, Prediction Accuracy = 63.775999999999996%, Loss = 0.32612184882164
Epoch: 6285, Batch Gradient Norm: 14.053946995561244
Epoch: 6285, Batch Gradient Norm after: 14.053946995561244
Epoch 6286/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.3250251173973083
Epoch: 6286, Batch Gradient Norm: 16.306274460313105
Epoch: 6286, Batch Gradient Norm after: 16.306274460313105
Epoch 6287/10000, Prediction Accuracy = 63.688%, Loss = 0.3282355546951294
Epoch: 6287, Batch Gradient Norm: 20.19316155274324
Epoch: 6287, Batch Gradient Norm after: 19.814370928712957
Epoch 6288/10000, Prediction Accuracy = 63.662%, Loss = 0.338697749376297
Epoch: 6288, Batch Gradient Norm: 18.095629028832196
Epoch: 6288, Batch Gradient Norm after: 15.717914961415456
Epoch 6289/10000, Prediction Accuracy = 63.614%, Loss = 0.3341837406158447
Epoch: 6289, Batch Gradient Norm: 14.99347023371198
Epoch: 6289, Batch Gradient Norm after: 14.99347023371198
Epoch 6290/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.3274962306022644
Epoch: 6290, Batch Gradient Norm: 11.527063423193308
Epoch: 6290, Batch Gradient Norm after: 11.527063423193308
Epoch 6291/10000, Prediction Accuracy = 63.61%, Loss = 0.3237047791481018
Epoch: 6291, Batch Gradient Norm: 11.164771713554638
Epoch: 6291, Batch Gradient Norm after: 11.164771713554638
Epoch 6292/10000, Prediction Accuracy = 63.722%, Loss = 0.32359548211097716
Epoch: 6292, Batch Gradient Norm: 13.614689933637633
Epoch: 6292, Batch Gradient Norm after: 13.614689933637633
Epoch 6293/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3289436995983124
Epoch: 6293, Batch Gradient Norm: 15.447805819268252
Epoch: 6293, Batch Gradient Norm after: 15.447805819268252
Epoch 6294/10000, Prediction Accuracy = 63.831999999999994%, Loss = 0.32873556613922117
Epoch: 6294, Batch Gradient Norm: 14.524582150648923
Epoch: 6294, Batch Gradient Norm after: 14.524582150648923
Epoch 6295/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.3270085990428925
Epoch: 6295, Batch Gradient Norm: 16.028426472501597
Epoch: 6295, Batch Gradient Norm after: 16.028426472501597
Epoch 6296/10000, Prediction Accuracy = 63.75599999999999%, Loss = 0.3305402934551239
Epoch: 6296, Batch Gradient Norm: 16.767134896542167
Epoch: 6296, Batch Gradient Norm after: 16.767134896542167
Epoch 6297/10000, Prediction Accuracy = 63.694%, Loss = 0.3319679796695709
Epoch: 6297, Batch Gradient Norm: 17.717693723888843
Epoch: 6297, Batch Gradient Norm after: 17.717693723888843
Epoch 6298/10000, Prediction Accuracy = 63.572%, Loss = 0.333961421251297
Epoch: 6298, Batch Gradient Norm: 19.0012495603604
Epoch: 6298, Batch Gradient Norm after: 18.619372481480287
Epoch 6299/10000, Prediction Accuracy = 63.751999999999995%, Loss = 0.33521540760993956
Epoch: 6299, Batch Gradient Norm: 17.368866417229654
Epoch: 6299, Batch Gradient Norm after: 17.368866417229654
Epoch 6300/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.3332483768463135
Epoch: 6300, Batch Gradient Norm: 16.00952374243166
Epoch: 6300, Batch Gradient Norm after: 16.00952374243166
Epoch 6301/10000, Prediction Accuracy = 63.798%, Loss = 0.33062363862991334
Epoch: 6301, Batch Gradient Norm: 14.410085157867199
Epoch: 6301, Batch Gradient Norm after: 14.410085157867199
Epoch 6302/10000, Prediction Accuracy = 63.758%, Loss = 0.3261755645275116
Epoch: 6302, Batch Gradient Norm: 13.483019005088385
Epoch: 6302, Batch Gradient Norm after: 13.483019005088385
Epoch 6303/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.32506829500198364
Epoch: 6303, Batch Gradient Norm: 15.860574074766424
Epoch: 6303, Batch Gradient Norm after: 15.860574074766424
Epoch 6304/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.3280318081378937
Epoch: 6304, Batch Gradient Norm: 15.77672038133206
Epoch: 6304, Batch Gradient Norm after: 15.77672038133206
Epoch 6305/10000, Prediction Accuracy = 63.782000000000004%, Loss = 0.32998040318489075
Epoch: 6305, Batch Gradient Norm: 17.95532460208614
Epoch: 6305, Batch Gradient Norm after: 17.644747502572848
Epoch 6306/10000, Prediction Accuracy = 63.626%, Loss = 0.33202669620513914
Epoch: 6306, Batch Gradient Norm: 16.106788277297817
Epoch: 6306, Batch Gradient Norm after: 16.106788277297817
Epoch 6307/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.33002920150756837
Epoch: 6307, Batch Gradient Norm: 14.73034955829478
Epoch: 6307, Batch Gradient Norm after: 14.73034955829478
Epoch 6308/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.32553781270980836
Epoch: 6308, Batch Gradient Norm: 15.010155363868174
Epoch: 6308, Batch Gradient Norm after: 15.010155363868174
Epoch 6309/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.3265844464302063
Epoch: 6309, Batch Gradient Norm: 15.891475113326736
Epoch: 6309, Batch Gradient Norm after: 15.891475113326736
Epoch 6310/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.32887020111083987
Epoch: 6310, Batch Gradient Norm: 14.318683358552741
Epoch: 6310, Batch Gradient Norm after: 14.318683358552741
Epoch 6311/10000, Prediction Accuracy = 63.602%, Loss = 0.32687833309173586
Epoch: 6311, Batch Gradient Norm: 12.955845761934933
Epoch: 6311, Batch Gradient Norm after: 12.955845761934933
Epoch 6312/10000, Prediction Accuracy = 63.878%, Loss = 0.3262837529182434
Epoch: 6312, Batch Gradient Norm: 11.816478145849263
Epoch: 6312, Batch Gradient Norm after: 11.816478145849263
Epoch 6313/10000, Prediction Accuracy = 63.814%, Loss = 0.32426074147224426
Epoch: 6313, Batch Gradient Norm: 10.834857905313488
Epoch: 6313, Batch Gradient Norm after: 10.834857905313488
Epoch 6314/10000, Prediction Accuracy = 63.774%, Loss = 0.32192996740341184
Epoch: 6314, Batch Gradient Norm: 10.806571755209019
Epoch: 6314, Batch Gradient Norm after: 10.806571755209019
Epoch 6315/10000, Prediction Accuracy = 63.718%, Loss = 0.32131080627441405
Epoch: 6315, Batch Gradient Norm: 12.539003077849543
Epoch: 6315, Batch Gradient Norm after: 12.539003077849543
Epoch 6316/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.3240352749824524
Epoch: 6316, Batch Gradient Norm: 12.616619287061038
Epoch: 6316, Batch Gradient Norm after: 12.616619287061038
Epoch 6317/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.32228867411613465
Epoch: 6317, Batch Gradient Norm: 10.464458299153645
Epoch: 6317, Batch Gradient Norm after: 10.464458299153645
Epoch 6318/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.32043021321296694
Epoch: 6318, Batch Gradient Norm: 12.498317118721978
Epoch: 6318, Batch Gradient Norm after: 12.498317118721978
Epoch 6319/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3219092786312103
Epoch: 6319, Batch Gradient Norm: 13.583170424047596
Epoch: 6319, Batch Gradient Norm after: 13.583170424047596
Epoch 6320/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.3253981530666351
Epoch: 6320, Batch Gradient Norm: 15.0192670945894
Epoch: 6320, Batch Gradient Norm after: 15.0192670945894
Epoch 6321/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.32557525038719176
Epoch: 6321, Batch Gradient Norm: 12.947828898650291
Epoch: 6321, Batch Gradient Norm after: 12.947828898650291
Epoch 6322/10000, Prediction Accuracy = 63.614%, Loss = 0.32371618747711184
Epoch: 6322, Batch Gradient Norm: 11.241017186098654
Epoch: 6322, Batch Gradient Norm after: 11.241017186098654
Epoch 6323/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.3211671710014343
Epoch: 6323, Batch Gradient Norm: 12.275726100468583
Epoch: 6323, Batch Gradient Norm after: 12.275726100468583
Epoch 6324/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.32414469718933103
Epoch: 6324, Batch Gradient Norm: 12.49696753316257
Epoch: 6324, Batch Gradient Norm after: 12.49696753316257
Epoch 6325/10000, Prediction Accuracy = 63.702%, Loss = 0.3237151324748993
Epoch: 6325, Batch Gradient Norm: 10.954056723727719
Epoch: 6325, Batch Gradient Norm after: 10.954056723727719
Epoch 6326/10000, Prediction Accuracy = 63.672000000000004%, Loss = 0.3229100465774536
Epoch: 6326, Batch Gradient Norm: 11.993657624998168
Epoch: 6326, Batch Gradient Norm after: 11.993657624998168
Epoch 6327/10000, Prediction Accuracy = 63.63800000000001%, Loss = 0.32377819418907167
Epoch: 6327, Batch Gradient Norm: 15.381510668689149
Epoch: 6327, Batch Gradient Norm after: 15.381510668689149
Epoch 6328/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.32624015808105467
Epoch: 6328, Batch Gradient Norm: 13.855070564740359
Epoch: 6328, Batch Gradient Norm after: 13.855070564740359
Epoch 6329/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.32484047412872313
Epoch: 6329, Batch Gradient Norm: 11.809411179453287
Epoch: 6329, Batch Gradient Norm after: 11.809411179453287
Epoch 6330/10000, Prediction Accuracy = 63.676%, Loss = 0.3217753529548645
Epoch: 6330, Batch Gradient Norm: 10.875773059912353
Epoch: 6330, Batch Gradient Norm after: 10.875773059912353
Epoch 6331/10000, Prediction Accuracy = 63.666%, Loss = 0.321677428483963
Epoch: 6331, Batch Gradient Norm: 9.96390346439615
Epoch: 6331, Batch Gradient Norm after: 9.96390346439615
Epoch 6332/10000, Prediction Accuracy = 63.742000000000004%, Loss = 0.3212165892124176
Epoch: 6332, Batch Gradient Norm: 10.293976970742797
Epoch: 6332, Batch Gradient Norm after: 10.293976970742797
Epoch 6333/10000, Prediction Accuracy = 63.864%, Loss = 0.319619220495224
Epoch: 6333, Batch Gradient Norm: 9.756197438686081
Epoch: 6333, Batch Gradient Norm after: 9.756197438686081
Epoch 6334/10000, Prediction Accuracy = 63.760000000000005%, Loss = 0.3247561573982239
Epoch: 6334, Batch Gradient Norm: 12.370166954793161
Epoch: 6334, Batch Gradient Norm after: 12.370166954793161
Epoch 6335/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.3226842820644379
Epoch: 6335, Batch Gradient Norm: 14.836488481939881
Epoch: 6335, Batch Gradient Norm after: 14.836488481939881
Epoch 6336/10000, Prediction Accuracy = 63.694%, Loss = 0.32910553812980653
Epoch: 6336, Batch Gradient Norm: 16.415319194013602
Epoch: 6336, Batch Gradient Norm after: 16.415319194013602
Epoch 6337/10000, Prediction Accuracy = 63.678%, Loss = 0.3298273146152496
Epoch: 6337, Batch Gradient Norm: 16.60073133765591
Epoch: 6337, Batch Gradient Norm after: 16.60073133765591
Epoch 6338/10000, Prediction Accuracy = 63.734%, Loss = 0.3286843657493591
Epoch: 6338, Batch Gradient Norm: 14.018515675637312
Epoch: 6338, Batch Gradient Norm after: 14.018515675637312
Epoch 6339/10000, Prediction Accuracy = 63.69199999999999%, Loss = 0.32449692487716675
Epoch: 6339, Batch Gradient Norm: 13.547030064229153
Epoch: 6339, Batch Gradient Norm after: 13.547030064229153
Epoch 6340/10000, Prediction Accuracy = 63.80799999999999%, Loss = 0.32398781180381775
Epoch: 6340, Batch Gradient Norm: 14.101637532760858
Epoch: 6340, Batch Gradient Norm after: 14.101637532760858
Epoch 6341/10000, Prediction Accuracy = 63.7%, Loss = 0.32400875091552733
Epoch: 6341, Batch Gradient Norm: 15.40130350645529
Epoch: 6341, Batch Gradient Norm after: 15.40130350645529
Epoch 6342/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.3273409068584442
Epoch: 6342, Batch Gradient Norm: 13.540415216332233
Epoch: 6342, Batch Gradient Norm after: 13.540415216332233
Epoch 6343/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.32614280581474303
Epoch: 6343, Batch Gradient Norm: 11.87227582628756
Epoch: 6343, Batch Gradient Norm after: 11.87227582628756
Epoch 6344/10000, Prediction Accuracy = 63.842000000000006%, Loss = 0.3194657385349274
Epoch: 6344, Batch Gradient Norm: 13.798554787738409
Epoch: 6344, Batch Gradient Norm after: 13.798554787738409
Epoch 6345/10000, Prediction Accuracy = 63.78799999999999%, Loss = 0.32665804624557493
Epoch: 6345, Batch Gradient Norm: 15.639978248959185
Epoch: 6345, Batch Gradient Norm after: 15.639978248959185
Epoch 6346/10000, Prediction Accuracy = 63.778%, Loss = 0.3291249454021454
Epoch: 6346, Batch Gradient Norm: 16.508385526031585
Epoch: 6346, Batch Gradient Norm after: 16.508385526031585
Epoch 6347/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.32866120934486387
Epoch: 6347, Batch Gradient Norm: 16.90254289307981
Epoch: 6347, Batch Gradient Norm after: 16.90254289307981
Epoch 6348/10000, Prediction Accuracy = 63.512%, Loss = 0.32726786732673646
Epoch: 6348, Batch Gradient Norm: 18.836166612960366
Epoch: 6348, Batch Gradient Norm after: 18.481120372847382
Epoch 6349/10000, Prediction Accuracy = 63.831999999999994%, Loss = 0.33459013104438784
Epoch: 6349, Batch Gradient Norm: 17.283342950680694
Epoch: 6349, Batch Gradient Norm after: 16.625226165853867
Epoch 6350/10000, Prediction Accuracy = 63.71200000000001%, Loss = 0.3316160261631012
Epoch: 6350, Batch Gradient Norm: 16.393876875333053
Epoch: 6350, Batch Gradient Norm after: 16.393876875333053
Epoch 6351/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3285131096839905
Epoch: 6351, Batch Gradient Norm: 14.7201158764841
Epoch: 6351, Batch Gradient Norm after: 14.7201158764841
Epoch 6352/10000, Prediction Accuracy = 63.682%, Loss = 0.3257803738117218
Epoch: 6352, Batch Gradient Norm: 15.357353023672303
Epoch: 6352, Batch Gradient Norm after: 15.357353023672303
Epoch 6353/10000, Prediction Accuracy = 63.778%, Loss = 0.32529997229576113
Epoch: 6353, Batch Gradient Norm: 16.392685745557575
Epoch: 6353, Batch Gradient Norm after: 16.392685745557575
Epoch 6354/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.32874945402145384
Epoch: 6354, Batch Gradient Norm: 15.811413289516524
Epoch: 6354, Batch Gradient Norm after: 15.811413289516524
Epoch 6355/10000, Prediction Accuracy = 63.8%, Loss = 0.3271893560886383
Epoch: 6355, Batch Gradient Norm: 14.681862744177616
Epoch: 6355, Batch Gradient Norm after: 14.681862744177616
Epoch 6356/10000, Prediction Accuracy = 63.64200000000001%, Loss = 0.3246810495853424
Epoch: 6356, Batch Gradient Norm: 14.163616866997355
Epoch: 6356, Batch Gradient Norm after: 14.163616866997355
Epoch 6357/10000, Prediction Accuracy = 63.922000000000004%, Loss = 0.32613958716392516
Epoch: 6357, Batch Gradient Norm: 12.669091598765426
Epoch: 6357, Batch Gradient Norm after: 12.669091598765426
Epoch 6358/10000, Prediction Accuracy = 63.854%, Loss = 0.3215418756008148
Epoch: 6358, Batch Gradient Norm: 12.458046609401553
Epoch: 6358, Batch Gradient Norm after: 12.458046609401553
Epoch 6359/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.32268713116645814
Epoch: 6359, Batch Gradient Norm: 10.219017882840891
Epoch: 6359, Batch Gradient Norm after: 10.219017882840891
Epoch 6360/10000, Prediction Accuracy = 63.907999999999994%, Loss = 0.3208871066570282
Epoch: 6360, Batch Gradient Norm: 9.753556011709229
Epoch: 6360, Batch Gradient Norm after: 9.753556011709229
Epoch 6361/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.32127622961997987
Epoch: 6361, Batch Gradient Norm: 10.033735034378488
Epoch: 6361, Batch Gradient Norm after: 10.033735034378488
Epoch 6362/10000, Prediction Accuracy = 63.89200000000001%, Loss = 0.31883950233459474
Epoch: 6362, Batch Gradient Norm: 9.449181591121977
Epoch: 6362, Batch Gradient Norm after: 9.449181591121977
Epoch 6363/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.31922648549079896
Epoch: 6363, Batch Gradient Norm: 9.453620220800685
Epoch: 6363, Batch Gradient Norm after: 9.453620220800685
Epoch 6364/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.3216513991355896
Epoch: 6364, Batch Gradient Norm: 11.737355619461306
Epoch: 6364, Batch Gradient Norm after: 11.737355619461306
Epoch 6365/10000, Prediction Accuracy = 63.78000000000001%, Loss = 0.3203271090984344
Epoch: 6365, Batch Gradient Norm: 14.317953205708973
Epoch: 6365, Batch Gradient Norm after: 14.317953205708973
Epoch 6366/10000, Prediction Accuracy = 63.712%, Loss = 0.32560561299324037
Epoch: 6366, Batch Gradient Norm: 12.52687221185517
Epoch: 6366, Batch Gradient Norm after: 12.52687221185517
Epoch 6367/10000, Prediction Accuracy = 63.824%, Loss = 0.3227112412452698
Epoch: 6367, Batch Gradient Norm: 14.834802418647723
Epoch: 6367, Batch Gradient Norm after: 14.834802418647723
Epoch 6368/10000, Prediction Accuracy = 63.767999999999994%, Loss = 0.32443549036979674
Epoch: 6368, Batch Gradient Norm: 14.215083014544318
Epoch: 6368, Batch Gradient Norm after: 14.215083014544318
Epoch 6369/10000, Prediction Accuracy = 63.722%, Loss = 0.3240732789039612
Epoch: 6369, Batch Gradient Norm: 14.46944101274855
Epoch: 6369, Batch Gradient Norm after: 14.46944101274855
Epoch 6370/10000, Prediction Accuracy = 63.912%, Loss = 0.32352309823036196
Epoch: 6370, Batch Gradient Norm: 15.559249056872837
Epoch: 6370, Batch Gradient Norm after: 15.559249056872837
Epoch 6371/10000, Prediction Accuracy = 63.686%, Loss = 0.32432134747505187
Epoch: 6371, Batch Gradient Norm: 14.505053277888276
Epoch: 6371, Batch Gradient Norm after: 14.505053277888276
Epoch 6372/10000, Prediction Accuracy = 63.879999999999995%, Loss = 0.32415685057640076
Epoch: 6372, Batch Gradient Norm: 11.697183800891883
Epoch: 6372, Batch Gradient Norm after: 11.697183800891883
Epoch 6373/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.3220060169696808
Epoch: 6373, Batch Gradient Norm: 12.776371092495827
Epoch: 6373, Batch Gradient Norm after: 12.776371092495827
Epoch 6374/10000, Prediction Accuracy = 63.79%, Loss = 0.32262125611305237
Epoch: 6374, Batch Gradient Norm: 13.183623378601508
Epoch: 6374, Batch Gradient Norm after: 13.183623378601508
Epoch 6375/10000, Prediction Accuracy = 63.944%, Loss = 0.32149815559387207
Epoch: 6375, Batch Gradient Norm: 15.138754860663756
Epoch: 6375, Batch Gradient Norm after: 15.138754860663756
Epoch 6376/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.323067307472229
Epoch: 6376, Batch Gradient Norm: 15.893347496914213
Epoch: 6376, Batch Gradient Norm after: 15.893347496914213
Epoch 6377/10000, Prediction Accuracy = 63.914%, Loss = 0.32536089420318604
Epoch: 6377, Batch Gradient Norm: 12.182995685617012
Epoch: 6377, Batch Gradient Norm after: 12.182995685617012
Epoch 6378/10000, Prediction Accuracy = 63.73%, Loss = 0.3215110719203949
Epoch: 6378, Batch Gradient Norm: 11.338062023998688
Epoch: 6378, Batch Gradient Norm after: 11.338062023998688
Epoch 6379/10000, Prediction Accuracy = 63.712%, Loss = 0.32111371159553526
Epoch: 6379, Batch Gradient Norm: 11.959131760331621
Epoch: 6379, Batch Gradient Norm after: 11.959131760331621
Epoch 6380/10000, Prediction Accuracy = 63.934000000000005%, Loss = 0.32050660252571106
Epoch: 6380, Batch Gradient Norm: 13.188239984024138
Epoch: 6380, Batch Gradient Norm after: 13.188239984024138
Epoch 6381/10000, Prediction Accuracy = 63.86600000000001%, Loss = 0.3260386765003204
Epoch: 6381, Batch Gradient Norm: 16.91329460089377
Epoch: 6381, Batch Gradient Norm after: 16.91329460089377
Epoch 6382/10000, Prediction Accuracy = 63.746%, Loss = 0.32952292561531066
Epoch: 6382, Batch Gradient Norm: 17.973503433328883
Epoch: 6382, Batch Gradient Norm after: 16.92236290155521
Epoch 6383/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.32906795740127565
Epoch: 6383, Batch Gradient Norm: 14.05291406398931
Epoch: 6383, Batch Gradient Norm after: 14.05291406398931
Epoch 6384/10000, Prediction Accuracy = 63.678%, Loss = 0.3268272340297699
Epoch: 6384, Batch Gradient Norm: 15.79730750618389
Epoch: 6384, Batch Gradient Norm after: 15.79730750618389
Epoch 6385/10000, Prediction Accuracy = 63.86999999999999%, Loss = 0.32584637999534605
Epoch: 6385, Batch Gradient Norm: 12.329109789555348
Epoch: 6385, Batch Gradient Norm after: 12.329109789555348
Epoch 6386/10000, Prediction Accuracy = 63.686000000000014%, Loss = 0.3224037230014801
Epoch: 6386, Batch Gradient Norm: 14.329186921587377
Epoch: 6386, Batch Gradient Norm after: 14.329186921587377
Epoch 6387/10000, Prediction Accuracy = 63.69200000000001%, Loss = 0.32627453804016116
Epoch: 6387, Batch Gradient Norm: 14.9586398650132
Epoch: 6387, Batch Gradient Norm after: 14.9586398650132
Epoch 6388/10000, Prediction Accuracy = 63.788%, Loss = 0.32610068917274476
Epoch: 6388, Batch Gradient Norm: 13.560555830375671
Epoch: 6388, Batch Gradient Norm after: 13.560555830375671
Epoch 6389/10000, Prediction Accuracy = 63.69599999999999%, Loss = 0.3251750826835632
Epoch: 6389, Batch Gradient Norm: 12.898430484115247
Epoch: 6389, Batch Gradient Norm after: 12.898430484115247
Epoch 6390/10000, Prediction Accuracy = 63.836%, Loss = 0.3218901872634888
Epoch: 6390, Batch Gradient Norm: 13.538942784832793
Epoch: 6390, Batch Gradient Norm after: 13.538942784832793
Epoch 6391/10000, Prediction Accuracy = 63.802%, Loss = 0.3258635699748993
Epoch: 6391, Batch Gradient Norm: 15.793108530098346
Epoch: 6391, Batch Gradient Norm after: 15.793108530098346
Epoch 6392/10000, Prediction Accuracy = 63.812%, Loss = 0.3244650661945343
Epoch: 6392, Batch Gradient Norm: 16.540042728269263
Epoch: 6392, Batch Gradient Norm after: 16.540042728269263
Epoch 6393/10000, Prediction Accuracy = 63.85%, Loss = 0.32503358721733094
Epoch: 6393, Batch Gradient Norm: 15.28094099268482
Epoch: 6393, Batch Gradient Norm after: 15.28094099268482
Epoch 6394/10000, Prediction Accuracy = 63.831999999999994%, Loss = 0.32294795513153074
Epoch: 6394, Batch Gradient Norm: 14.320337380026768
Epoch: 6394, Batch Gradient Norm after: 14.320337380026768
Epoch 6395/10000, Prediction Accuracy = 63.77%, Loss = 0.3234050154685974
Epoch: 6395, Batch Gradient Norm: 14.538277173141298
Epoch: 6395, Batch Gradient Norm after: 14.538277173141298
Epoch 6396/10000, Prediction Accuracy = 63.83399999999999%, Loss = 0.3229739129543304
Epoch: 6396, Batch Gradient Norm: 14.103468087625096
Epoch: 6396, Batch Gradient Norm after: 14.103468087625096
Epoch 6397/10000, Prediction Accuracy = 63.737999999999985%, Loss = 0.3287541329860687
Epoch: 6397, Batch Gradient Norm: 14.035271398100846
Epoch: 6397, Batch Gradient Norm after: 14.035271398100846
Epoch 6398/10000, Prediction Accuracy = 63.80200000000001%, Loss = 0.3251798748970032
Epoch: 6398, Batch Gradient Norm: 11.164975099488617
Epoch: 6398, Batch Gradient Norm after: 11.164975099488617
Epoch 6399/10000, Prediction Accuracy = 63.751999999999995%, Loss = 0.32112898826599123
Epoch: 6399, Batch Gradient Norm: 9.222789094829084
Epoch: 6399, Batch Gradient Norm after: 9.222789094829084
Epoch 6400/10000, Prediction Accuracy = 63.674%, Loss = 0.32146674394607544
Epoch: 6400, Batch Gradient Norm: 11.10507940756126
Epoch: 6400, Batch Gradient Norm after: 11.10507940756126
Epoch 6401/10000, Prediction Accuracy = 63.69%, Loss = 0.3224323332309723
Epoch: 6401, Batch Gradient Norm: 13.256894873186836
Epoch: 6401, Batch Gradient Norm after: 13.256894873186836
Epoch 6402/10000, Prediction Accuracy = 63.698%, Loss = 0.32236906290054324
Epoch: 6402, Batch Gradient Norm: 12.013158723547415
Epoch: 6402, Batch Gradient Norm after: 12.013158723547415
Epoch 6403/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.3212107062339783
Epoch: 6403, Batch Gradient Norm: 12.772519512512691
Epoch: 6403, Batch Gradient Norm after: 12.772519512512691
Epoch 6404/10000, Prediction Accuracy = 63.876%, Loss = 0.3222492575645447
Epoch: 6404, Batch Gradient Norm: 15.81107120327727
Epoch: 6404, Batch Gradient Norm after: 15.81107120327727
Epoch 6405/10000, Prediction Accuracy = 63.854%, Loss = 0.32828043699264525
Epoch: 6405, Batch Gradient Norm: 17.975006337810978
Epoch: 6405, Batch Gradient Norm after: 17.975006337810978
Epoch 6406/10000, Prediction Accuracy = 63.75999999999999%, Loss = 0.3282083749771118
Epoch: 6406, Batch Gradient Norm: 15.894240310111442
Epoch: 6406, Batch Gradient Norm after: 15.894240310111442
Epoch 6407/10000, Prediction Accuracy = 63.812%, Loss = 0.3263772130012512
Epoch: 6407, Batch Gradient Norm: 18.555789049257484
Epoch: 6407, Batch Gradient Norm after: 18.555789049257484
Epoch 6408/10000, Prediction Accuracy = 63.778%, Loss = 0.3285478353500366
Epoch: 6408, Batch Gradient Norm: 17.6341130459466
Epoch: 6408, Batch Gradient Norm after: 17.6341130459466
Epoch 6409/10000, Prediction Accuracy = 63.67%, Loss = 0.32945179343223574
Epoch: 6409, Batch Gradient Norm: 14.464949505258978
Epoch: 6409, Batch Gradient Norm after: 14.464949505258978
Epoch 6410/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.3232622742652893
Epoch: 6410, Batch Gradient Norm: 13.171193886634809
Epoch: 6410, Batch Gradient Norm after: 13.171193886634809
Epoch 6411/10000, Prediction Accuracy = 63.814%, Loss = 0.32431886792182923
Epoch: 6411, Batch Gradient Norm: 11.789974988053443
Epoch: 6411, Batch Gradient Norm after: 11.789974988053443
Epoch 6412/10000, Prediction Accuracy = 63.907999999999994%, Loss = 0.3207817256450653
Epoch: 6412, Batch Gradient Norm: 10.217905947028807
Epoch: 6412, Batch Gradient Norm after: 10.217905947028807
Epoch 6413/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.3189845681190491
Epoch: 6413, Batch Gradient Norm: 10.84846518819855
Epoch: 6413, Batch Gradient Norm after: 10.84846518819855
Epoch 6414/10000, Prediction Accuracy = 63.758%, Loss = 0.32271480560302734
Epoch: 6414, Batch Gradient Norm: 10.69053905218861
Epoch: 6414, Batch Gradient Norm after: 10.69053905218861
Epoch 6415/10000, Prediction Accuracy = 63.95%, Loss = 0.32127335071563723
Epoch: 6415, Batch Gradient Norm: 12.161780514519497
Epoch: 6415, Batch Gradient Norm after: 12.161780514519497
Epoch 6416/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.32172195315361024
Epoch: 6416, Batch Gradient Norm: 12.428185234315752
Epoch: 6416, Batch Gradient Norm after: 12.428185234315752
Epoch 6417/10000, Prediction Accuracy = 63.81999999999999%, Loss = 0.32146849632263186
Epoch: 6417, Batch Gradient Norm: 12.081086762970365
Epoch: 6417, Batch Gradient Norm after: 12.081086762970365
Epoch 6418/10000, Prediction Accuracy = 63.622%, Loss = 0.32155426144599913
Epoch: 6418, Batch Gradient Norm: 10.378284862265142
Epoch: 6418, Batch Gradient Norm after: 10.378284862265142
Epoch 6419/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.318787032365799
Epoch: 6419, Batch Gradient Norm: 11.461533521586297
Epoch: 6419, Batch Gradient Norm after: 11.461533521586297
Epoch 6420/10000, Prediction Accuracy = 63.786%, Loss = 0.31976261734962463
Epoch: 6420, Batch Gradient Norm: 11.867712647913043
Epoch: 6420, Batch Gradient Norm after: 11.867712647913043
Epoch 6421/10000, Prediction Accuracy = 63.822%, Loss = 0.3210813760757446
Epoch: 6421, Batch Gradient Norm: 10.039184484708198
Epoch: 6421, Batch Gradient Norm after: 10.039184484708198
Epoch 6422/10000, Prediction Accuracy = 63.746%, Loss = 0.3173880100250244
Epoch: 6422, Batch Gradient Norm: 11.291772186917516
Epoch: 6422, Batch Gradient Norm after: 11.291772186917516
Epoch 6423/10000, Prediction Accuracy = 63.854%, Loss = 0.3174997091293335
Epoch: 6423, Batch Gradient Norm: 14.125104890243328
Epoch: 6423, Batch Gradient Norm after: 14.125104890243328
Epoch 6424/10000, Prediction Accuracy = 63.834%, Loss = 0.32389895915985106
Epoch: 6424, Batch Gradient Norm: 14.874935360269227
Epoch: 6424, Batch Gradient Norm after: 14.874935360269227
Epoch 6425/10000, Prediction Accuracy = 63.82199999999999%, Loss = 0.32570170164108275
Epoch: 6425, Batch Gradient Norm: 16.014462637374262
Epoch: 6425, Batch Gradient Norm after: 16.014462637374262
Epoch 6426/10000, Prediction Accuracy = 63.812%, Loss = 0.3261176645755768
Epoch: 6426, Batch Gradient Norm: 14.715554120462972
Epoch: 6426, Batch Gradient Norm after: 14.715554120462972
Epoch 6427/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.32547128200531006
Epoch: 6427, Batch Gradient Norm: 14.779012883124043
Epoch: 6427, Batch Gradient Norm after: 14.779012883124043
Epoch 6428/10000, Prediction Accuracy = 63.774%, Loss = 0.32343730330467224
Epoch: 6428, Batch Gradient Norm: 13.64058216139407
Epoch: 6428, Batch Gradient Norm after: 13.64058216139407
Epoch 6429/10000, Prediction Accuracy = 63.91600000000001%, Loss = 0.32327157258987427
Epoch: 6429, Batch Gradient Norm: 13.826853625296176
Epoch: 6429, Batch Gradient Norm after: 13.826853625296176
Epoch 6430/10000, Prediction Accuracy = 63.94000000000001%, Loss = 0.32085150480270386
Epoch: 6430, Batch Gradient Norm: 12.775404157735819
Epoch: 6430, Batch Gradient Norm after: 12.775404157735819
Epoch 6431/10000, Prediction Accuracy = 63.766%, Loss = 0.3224580824375153
Epoch: 6431, Batch Gradient Norm: 12.325452687167035
Epoch: 6431, Batch Gradient Norm after: 12.325452687167035
Epoch 6432/10000, Prediction Accuracy = 63.761999999999986%, Loss = 0.3215946614742279
Epoch: 6432, Batch Gradient Norm: 12.723929212477843
Epoch: 6432, Batch Gradient Norm after: 12.723929212477843
Epoch 6433/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.32171869874000547
Epoch: 6433, Batch Gradient Norm: 11.427034863506034
Epoch: 6433, Batch Gradient Norm after: 11.427034863506034
Epoch 6434/10000, Prediction Accuracy = 63.766%, Loss = 0.3197199583053589
Epoch: 6434, Batch Gradient Norm: 11.460880399634156
Epoch: 6434, Batch Gradient Norm after: 11.460880399634156
Epoch 6435/10000, Prediction Accuracy = 63.604%, Loss = 0.31926130056381224
Epoch: 6435, Batch Gradient Norm: 12.894349732992545
Epoch: 6435, Batch Gradient Norm after: 12.894349732992545
Epoch 6436/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.32120829820632935
Epoch: 6436, Batch Gradient Norm: 11.682059160180197
Epoch: 6436, Batch Gradient Norm after: 11.682059160180197
Epoch 6437/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.31843520402908326
Epoch: 6437, Batch Gradient Norm: 11.710122418219933
Epoch: 6437, Batch Gradient Norm after: 11.710122418219933
Epoch 6438/10000, Prediction Accuracy = 63.78399999999999%, Loss = 0.3204361915588379
Epoch: 6438, Batch Gradient Norm: 14.751039831406617
Epoch: 6438, Batch Gradient Norm after: 14.751039831406617
Epoch 6439/10000, Prediction Accuracy = 63.84400000000001%, Loss = 0.3237601459026337
Epoch: 6439, Batch Gradient Norm: 14.493054885553848
Epoch: 6439, Batch Gradient Norm after: 14.493054885553848
Epoch 6440/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.3247613251209259
Epoch: 6440, Batch Gradient Norm: 14.565712276392865
Epoch: 6440, Batch Gradient Norm after: 14.565712276392865
Epoch 6441/10000, Prediction Accuracy = 63.676%, Loss = 0.3230680823326111
Epoch: 6441, Batch Gradient Norm: 11.862121856041185
Epoch: 6441, Batch Gradient Norm after: 11.862121856041185
Epoch 6442/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.31919832825660704
Epoch: 6442, Batch Gradient Norm: 11.805415300372191
Epoch: 6442, Batch Gradient Norm after: 11.805415300372191
Epoch 6443/10000, Prediction Accuracy = 63.872%, Loss = 0.31844736337661744
Epoch: 6443, Batch Gradient Norm: 15.724913938868925
Epoch: 6443, Batch Gradient Norm after: 15.724913938868925
Epoch 6444/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.32564510107040406
Epoch: 6444, Batch Gradient Norm: 15.207556806507217
Epoch: 6444, Batch Gradient Norm after: 15.207556806507217
Epoch 6445/10000, Prediction Accuracy = 63.876%, Loss = 0.32668803334236146
Epoch: 6445, Batch Gradient Norm: 11.619338536716032
Epoch: 6445, Batch Gradient Norm after: 11.619338536716032
Epoch 6446/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.32061017155647276
Epoch: 6446, Batch Gradient Norm: 11.627140028959305
Epoch: 6446, Batch Gradient Norm after: 11.627140028959305
Epoch 6447/10000, Prediction Accuracy = 63.872%, Loss = 0.3204127848148346
Epoch: 6447, Batch Gradient Norm: 14.011556248160453
Epoch: 6447, Batch Gradient Norm after: 14.011556248160453
Epoch 6448/10000, Prediction Accuracy = 63.836%, Loss = 0.3236213207244873
Epoch: 6448, Batch Gradient Norm: 15.027266936400313
Epoch: 6448, Batch Gradient Norm after: 15.027266936400313
Epoch 6449/10000, Prediction Accuracy = 63.77%, Loss = 0.32422350645065307
Epoch: 6449, Batch Gradient Norm: 15.575577975128782
Epoch: 6449, Batch Gradient Norm after: 15.575577975128782
Epoch 6450/10000, Prediction Accuracy = 63.754%, Loss = 0.32518246173858645
Epoch: 6450, Batch Gradient Norm: 13.76466828147253
Epoch: 6450, Batch Gradient Norm after: 13.76466828147253
Epoch 6451/10000, Prediction Accuracy = 63.912%, Loss = 0.32082041501998904
Epoch: 6451, Batch Gradient Norm: 12.213017628124796
Epoch: 6451, Batch Gradient Norm after: 12.213017628124796
Epoch 6452/10000, Prediction Accuracy = 63.831999999999994%, Loss = 0.3197091519832611
Epoch: 6452, Batch Gradient Norm: 13.075365279657886
Epoch: 6452, Batch Gradient Norm after: 13.075365279657886
Epoch 6453/10000, Prediction Accuracy = 63.77600000000001%, Loss = 0.31943055987358093
Epoch: 6453, Batch Gradient Norm: 12.19765993775749
Epoch: 6453, Batch Gradient Norm after: 12.19765993775749
Epoch 6454/10000, Prediction Accuracy = 63.769999999999996%, Loss = 0.3211665630340576
Epoch: 6454, Batch Gradient Norm: 12.131123705797263
Epoch: 6454, Batch Gradient Norm after: 12.131123705797263
Epoch 6455/10000, Prediction Accuracy = 63.8%, Loss = 0.3232696712017059
Epoch: 6455, Batch Gradient Norm: 11.7277519225412
Epoch: 6455, Batch Gradient Norm after: 11.7277519225412
Epoch 6456/10000, Prediction Accuracy = 63.90599999999999%, Loss = 0.32041080594062804
Epoch: 6456, Batch Gradient Norm: 10.420205331052642
Epoch: 6456, Batch Gradient Norm after: 10.420205331052642
Epoch 6457/10000, Prediction Accuracy = 63.812%, Loss = 0.31768932938575745
Epoch: 6457, Batch Gradient Norm: 9.737892528953347
Epoch: 6457, Batch Gradient Norm after: 9.737892528953347
Epoch 6458/10000, Prediction Accuracy = 63.75599999999999%, Loss = 0.3168645977973938
Epoch: 6458, Batch Gradient Norm: 11.320385122165813
Epoch: 6458, Batch Gradient Norm after: 11.320385122165813
Epoch 6459/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.3181169807910919
Epoch: 6459, Batch Gradient Norm: 13.581111237540066
Epoch: 6459, Batch Gradient Norm after: 13.581111237540066
Epoch 6460/10000, Prediction Accuracy = 63.81999999999999%, Loss = 0.32193857431411743
Epoch: 6460, Batch Gradient Norm: 16.26827795756329
Epoch: 6460, Batch Gradient Norm after: 16.26827795756329
Epoch 6461/10000, Prediction Accuracy = 63.754%, Loss = 0.32727733850479124
Epoch: 6461, Batch Gradient Norm: 13.68802246327772
Epoch: 6461, Batch Gradient Norm after: 13.68802246327772
Epoch 6462/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.3224013328552246
Epoch: 6462, Batch Gradient Norm: 11.97545945511456
Epoch: 6462, Batch Gradient Norm after: 11.97545945511456
Epoch 6463/10000, Prediction Accuracy = 63.948%, Loss = 0.3199003279209137
Epoch: 6463, Batch Gradient Norm: 12.249179750252544
Epoch: 6463, Batch Gradient Norm after: 12.249179750252544
Epoch 6464/10000, Prediction Accuracy = 63.826%, Loss = 0.31926732063293456
Epoch: 6464, Batch Gradient Norm: 14.490872003321389
Epoch: 6464, Batch Gradient Norm after: 14.490872003321389
Epoch 6465/10000, Prediction Accuracy = 63.574%, Loss = 0.32288784384727476
Epoch: 6465, Batch Gradient Norm: 17.549783213474505
Epoch: 6465, Batch Gradient Norm after: 17.549783213474505
Epoch 6466/10000, Prediction Accuracy = 63.872%, Loss = 0.32908007502555847
Epoch: 6466, Batch Gradient Norm: 14.62137412063282
Epoch: 6466, Batch Gradient Norm after: 14.62137412063282
Epoch 6467/10000, Prediction Accuracy = 63.754%, Loss = 0.32597991824150085
Epoch: 6467, Batch Gradient Norm: 16.541008594026344
Epoch: 6467, Batch Gradient Norm after: 16.541008594026344
Epoch 6468/10000, Prediction Accuracy = 63.84400000000001%, Loss = 0.32740716338157655
Epoch: 6468, Batch Gradient Norm: 14.371567355102625
Epoch: 6468, Batch Gradient Norm after: 14.371567355102625
Epoch 6469/10000, Prediction Accuracy = 63.71600000000001%, Loss = 0.3227943181991577
Epoch: 6469, Batch Gradient Norm: 13.145458874104104
Epoch: 6469, Batch Gradient Norm after: 13.145458874104104
Epoch 6470/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3245824217796326
Epoch: 6470, Batch Gradient Norm: 15.34038694968869
Epoch: 6470, Batch Gradient Norm after: 15.34038694968869
Epoch 6471/10000, Prediction Accuracy = 63.71%, Loss = 0.32273473143577575
Epoch: 6471, Batch Gradient Norm: 17.649378919284008
Epoch: 6471, Batch Gradient Norm after: 17.649378919284008
Epoch 6472/10000, Prediction Accuracy = 63.788%, Loss = 0.3284258782863617
Epoch: 6472, Batch Gradient Norm: 12.922318774098674
Epoch: 6472, Batch Gradient Norm after: 12.922318774098674
Epoch 6473/10000, Prediction Accuracy = 63.71200000000001%, Loss = 0.3213226139545441
Epoch: 6473, Batch Gradient Norm: 12.355826616596017
Epoch: 6473, Batch Gradient Norm after: 12.355826616596017
Epoch 6474/10000, Prediction Accuracy = 63.926%, Loss = 0.3203666269779205
Epoch: 6474, Batch Gradient Norm: 15.18300976250639
Epoch: 6474, Batch Gradient Norm after: 15.18300976250639
Epoch 6475/10000, Prediction Accuracy = 63.758%, Loss = 0.32384254336357116
Epoch: 6475, Batch Gradient Norm: 13.570932239235585
Epoch: 6475, Batch Gradient Norm after: 13.570932239235585
Epoch 6476/10000, Prediction Accuracy = 63.818%, Loss = 0.3234946131706238
Epoch: 6476, Batch Gradient Norm: 13.428576598229549
Epoch: 6476, Batch Gradient Norm after: 13.428576598229549
Epoch 6477/10000, Prediction Accuracy = 63.86800000000001%, Loss = 0.3230107545852661
Epoch: 6477, Batch Gradient Norm: 14.441847372357001
Epoch: 6477, Batch Gradient Norm after: 14.441847372357001
Epoch 6478/10000, Prediction Accuracy = 63.976%, Loss = 0.32549737095832826
Epoch: 6478, Batch Gradient Norm: 14.631951137193075
Epoch: 6478, Batch Gradient Norm after: 14.631951137193075
Epoch 6479/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.32492347359657286
Epoch: 6479, Batch Gradient Norm: 14.353223533741604
Epoch: 6479, Batch Gradient Norm after: 14.353223533741604
Epoch 6480/10000, Prediction Accuracy = 63.83%, Loss = 0.32471067309379575
Epoch: 6480, Batch Gradient Norm: 17.83316992827285
Epoch: 6480, Batch Gradient Norm after: 17.80414664113756
Epoch 6481/10000, Prediction Accuracy = 63.824%, Loss = 0.32697339057922364
Epoch: 6481, Batch Gradient Norm: 15.20254849476206
Epoch: 6481, Batch Gradient Norm after: 15.20254849476206
Epoch 6482/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.32516870498657224
Epoch: 6482, Batch Gradient Norm: 15.969093644201463
Epoch: 6482, Batch Gradient Norm after: 15.969093644201463
Epoch 6483/10000, Prediction Accuracy = 63.812%, Loss = 0.32774672508239744
Epoch: 6483, Batch Gradient Norm: 14.799854670879824
Epoch: 6483, Batch Gradient Norm after: 14.799854670879824
Epoch 6484/10000, Prediction Accuracy = 63.89%, Loss = 0.32250932455062864
Epoch: 6484, Batch Gradient Norm: 14.190566933430604
Epoch: 6484, Batch Gradient Norm after: 14.190566933430604
Epoch 6485/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.32268728613853453
Epoch: 6485, Batch Gradient Norm: 15.447516464035525
Epoch: 6485, Batch Gradient Norm after: 15.447516464035525
Epoch 6486/10000, Prediction Accuracy = 63.815999999999995%, Loss = 0.325236040353775
Epoch: 6486, Batch Gradient Norm: 15.602291823611932
Epoch: 6486, Batch Gradient Norm after: 15.602291823611932
Epoch 6487/10000, Prediction Accuracy = 63.66600000000001%, Loss = 0.32512436509132386
Epoch: 6487, Batch Gradient Norm: 17.243258719800636
Epoch: 6487, Batch Gradient Norm after: 17.243258719800636
Epoch 6488/10000, Prediction Accuracy = 63.884%, Loss = 0.3260892152786255
Epoch: 6488, Batch Gradient Norm: 16.684032684760787
Epoch: 6488, Batch Gradient Norm after: 16.684032684760787
Epoch 6489/10000, Prediction Accuracy = 63.74400000000001%, Loss = 0.3278689682483673
Epoch: 6489, Batch Gradient Norm: 18.20083007560356
Epoch: 6489, Batch Gradient Norm after: 17.800165760712638
Epoch 6490/10000, Prediction Accuracy = 63.69%, Loss = 0.33051827549934387
Epoch: 6490, Batch Gradient Norm: 16.45199966154712
Epoch: 6490, Batch Gradient Norm after: 16.45199966154712
Epoch 6491/10000, Prediction Accuracy = 63.814%, Loss = 0.3271360695362091
Epoch: 6491, Batch Gradient Norm: 19.5708634184973
Epoch: 6491, Batch Gradient Norm after: 19.5708634184973
Epoch 6492/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.33153423070907595
Epoch: 6492, Batch Gradient Norm: 19.453125400982287
Epoch: 6492, Batch Gradient Norm after: 19.453125400982287
Epoch 6493/10000, Prediction Accuracy = 63.842%, Loss = 0.33176671862602236
Epoch: 6493, Batch Gradient Norm: 18.393114536082066
Epoch: 6493, Batch Gradient Norm after: 18.393114536082066
Epoch 6494/10000, Prediction Accuracy = 63.775999999999996%, Loss = 0.32945730686187746
Epoch: 6494, Batch Gradient Norm: 16.908936850550894
Epoch: 6494, Batch Gradient Norm after: 16.908936850550894
Epoch 6495/10000, Prediction Accuracy = 63.75%, Loss = 0.326292484998703
Epoch: 6495, Batch Gradient Norm: 17.177235628435312
Epoch: 6495, Batch Gradient Norm after: 16.36542613907096
Epoch 6496/10000, Prediction Accuracy = 63.946000000000005%, Loss = 0.324737012386322
Epoch: 6496, Batch Gradient Norm: 17.42138689019844
Epoch: 6496, Batch Gradient Norm after: 17.42138689019844
Epoch 6497/10000, Prediction Accuracy = 63.955999999999996%, Loss = 0.3254160642623901
Epoch: 6497, Batch Gradient Norm: 15.740410496865277
Epoch: 6497, Batch Gradient Norm after: 15.740410496865277
Epoch 6498/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.325186151266098
Epoch: 6498, Batch Gradient Norm: 11.754600334069691
Epoch: 6498, Batch Gradient Norm after: 11.754600334069691
Epoch 6499/10000, Prediction Accuracy = 63.798%, Loss = 0.31918424367904663
Epoch: 6499, Batch Gradient Norm: 13.176658674815393
Epoch: 6499, Batch Gradient Norm after: 13.176658674815393
Epoch 6500/10000, Prediction Accuracy = 63.884%, Loss = 0.32016188502311704
Epoch: 6500, Batch Gradient Norm: 14.259648930286605
Epoch: 6500, Batch Gradient Norm after: 14.259648930286605
Epoch 6501/10000, Prediction Accuracy = 63.85%, Loss = 0.3226729393005371
Epoch: 6501, Batch Gradient Norm: 14.450055321233695
Epoch: 6501, Batch Gradient Norm after: 14.450055321233695
Epoch 6502/10000, Prediction Accuracy = 63.916%, Loss = 0.3226382613182068
Epoch: 6502, Batch Gradient Norm: 14.438914422018415
Epoch: 6502, Batch Gradient Norm after: 14.438914422018415
Epoch 6503/10000, Prediction Accuracy = 63.8%, Loss = 0.32041009664535525
Epoch: 6503, Batch Gradient Norm: 15.707200519451094
Epoch: 6503, Batch Gradient Norm after: 15.707200519451094
Epoch 6504/10000, Prediction Accuracy = 63.970000000000006%, Loss = 0.3234373450279236
Epoch: 6504, Batch Gradient Norm: 16.00844485830323
Epoch: 6504, Batch Gradient Norm after: 16.00844485830323
Epoch 6505/10000, Prediction Accuracy = 63.862%, Loss = 0.32292298674583436
Epoch: 6505, Batch Gradient Norm: 14.365561871683939
Epoch: 6505, Batch Gradient Norm after: 14.365561871683939
Epoch 6506/10000, Prediction Accuracy = 63.83599999999999%, Loss = 0.3210076093673706
Epoch: 6506, Batch Gradient Norm: 16.504808429390366
Epoch: 6506, Batch Gradient Norm after: 16.504808429390366
Epoch 6507/10000, Prediction Accuracy = 63.812%, Loss = 0.32706109881401063
Epoch: 6507, Batch Gradient Norm: 14.366789524691823
Epoch: 6507, Batch Gradient Norm after: 14.366789524691823
Epoch 6508/10000, Prediction Accuracy = 63.852%, Loss = 0.32155648469924925
Epoch: 6508, Batch Gradient Norm: 14.270631823725763
Epoch: 6508, Batch Gradient Norm after: 14.270631823725763
Epoch 6509/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.3207492113113403
Epoch: 6509, Batch Gradient Norm: 14.239011494297623
Epoch: 6509, Batch Gradient Norm after: 14.239011494297623
Epoch 6510/10000, Prediction Accuracy = 63.766%, Loss = 0.32128448486328126
Epoch: 6510, Batch Gradient Norm: 12.874663808694352
Epoch: 6510, Batch Gradient Norm after: 12.874663808694352
Epoch 6511/10000, Prediction Accuracy = 63.858000000000004%, Loss = 0.32060490250587464
Epoch: 6511, Batch Gradient Norm: 12.48983627870745
Epoch: 6511, Batch Gradient Norm after: 12.48983627870745
Epoch 6512/10000, Prediction Accuracy = 63.85600000000001%, Loss = 0.3189422428607941
Epoch: 6512, Batch Gradient Norm: 12.202918920018197
Epoch: 6512, Batch Gradient Norm after: 12.202918920018197
Epoch 6513/10000, Prediction Accuracy = 63.876%, Loss = 0.31894275546073914
Epoch: 6513, Batch Gradient Norm: 15.344425263071289
Epoch: 6513, Batch Gradient Norm after: 15.344425263071289
Epoch 6514/10000, Prediction Accuracy = 63.90599999999999%, Loss = 0.32189509868621824
Epoch: 6514, Batch Gradient Norm: 12.037819875651653
Epoch: 6514, Batch Gradient Norm after: 12.037819875651653
Epoch 6515/10000, Prediction Accuracy = 63.834%, Loss = 0.31886530518531797
Epoch: 6515, Batch Gradient Norm: 11.209312836871316
Epoch: 6515, Batch Gradient Norm after: 11.209312836871316
Epoch 6516/10000, Prediction Accuracy = 64.074%, Loss = 0.3187182366847992
Epoch: 6516, Batch Gradient Norm: 14.776494457375824
Epoch: 6516, Batch Gradient Norm after: 14.776494457375824
Epoch 6517/10000, Prediction Accuracy = 63.85999999999999%, Loss = 0.3206251382827759
Epoch: 6517, Batch Gradient Norm: 13.728562661657318
Epoch: 6517, Batch Gradient Norm after: 13.728562661657318
Epoch 6518/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.3183390736579895
Epoch: 6518, Batch Gradient Norm: 16.200526196208433
Epoch: 6518, Batch Gradient Norm after: 16.200526196208433
Epoch 6519/10000, Prediction Accuracy = 63.918000000000006%, Loss = 0.32408747673034666
Epoch: 6519, Batch Gradient Norm: 16.990847625360164
Epoch: 6519, Batch Gradient Norm after: 16.990847625360164
Epoch 6520/10000, Prediction Accuracy = 63.84599999999999%, Loss = 0.32498525977134707
Epoch: 6520, Batch Gradient Norm: 17.544224819543167
Epoch: 6520, Batch Gradient Norm after: 17.395606642181036
Epoch 6521/10000, Prediction Accuracy = 63.85799999999999%, Loss = 0.32537208795547484
Epoch: 6521, Batch Gradient Norm: 14.846617569953821
Epoch: 6521, Batch Gradient Norm after: 14.846617569953821
Epoch 6522/10000, Prediction Accuracy = 63.786%, Loss = 0.3260831892490387
Epoch: 6522, Batch Gradient Norm: 16.920913033429827
Epoch: 6522, Batch Gradient Norm after: 16.920913033429827
Epoch 6523/10000, Prediction Accuracy = 63.818%, Loss = 0.32484089136123656
Epoch: 6523, Batch Gradient Norm: 15.422123683281306
Epoch: 6523, Batch Gradient Norm after: 15.422123683281306
Epoch 6524/10000, Prediction Accuracy = 63.802%, Loss = 0.3247872829437256
Epoch: 6524, Batch Gradient Norm: 14.306142877131098
Epoch: 6524, Batch Gradient Norm after: 14.306142877131098
Epoch 6525/10000, Prediction Accuracy = 63.702%, Loss = 0.32184775471687316
Epoch: 6525, Batch Gradient Norm: 19.711353773079466
Epoch: 6525, Batch Gradient Norm after: 19.548840492881716
Epoch 6526/10000, Prediction Accuracy = 63.766%, Loss = 0.3301476776599884
Epoch: 6526, Batch Gradient Norm: 16.81530848530783
Epoch: 6526, Batch Gradient Norm after: 16.81530848530783
Epoch 6527/10000, Prediction Accuracy = 63.826%, Loss = 0.3255102813243866
Epoch: 6527, Batch Gradient Norm: 15.093376391419902
Epoch: 6527, Batch Gradient Norm after: 15.093376391419902
Epoch 6528/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.32432539463043214
Epoch: 6528, Batch Gradient Norm: 12.810628111961607
Epoch: 6528, Batch Gradient Norm after: 12.810628111961607
Epoch 6529/10000, Prediction Accuracy = 63.862%, Loss = 0.3212231516838074
Epoch: 6529, Batch Gradient Norm: 14.492854299088371
Epoch: 6529, Batch Gradient Norm after: 14.492854299088371
Epoch 6530/10000, Prediction Accuracy = 63.854%, Loss = 0.3216606676578522
Epoch: 6530, Batch Gradient Norm: 13.023046952870576
Epoch: 6530, Batch Gradient Norm after: 13.023046952870576
Epoch 6531/10000, Prediction Accuracy = 63.842000000000006%, Loss = 0.3222850739955902
Epoch: 6531, Batch Gradient Norm: 14.03554677642053
Epoch: 6531, Batch Gradient Norm after: 14.03554677642053
Epoch 6532/10000, Prediction Accuracy = 63.842%, Loss = 0.31977576613426206
Epoch: 6532, Batch Gradient Norm: 11.995249826327182
Epoch: 6532, Batch Gradient Norm after: 11.995249826327182
Epoch 6533/10000, Prediction Accuracy = 63.95%, Loss = 0.3177710473537445
Epoch: 6533, Batch Gradient Norm: 11.082090472152263
Epoch: 6533, Batch Gradient Norm after: 11.082090472152263
Epoch 6534/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.31868460178375246
Epoch: 6534, Batch Gradient Norm: 13.36007248192868
Epoch: 6534, Batch Gradient Norm after: 13.36007248192868
Epoch 6535/10000, Prediction Accuracy = 63.767999999999994%, Loss = 0.321368807554245
Epoch: 6535, Batch Gradient Norm: 12.226445616548764
Epoch: 6535, Batch Gradient Norm after: 12.226445616548764
Epoch 6536/10000, Prediction Accuracy = 63.818%, Loss = 0.31846800446510315
Epoch: 6536, Batch Gradient Norm: 12.721724035157983
Epoch: 6536, Batch Gradient Norm after: 12.721724035157983
Epoch 6537/10000, Prediction Accuracy = 63.888%, Loss = 0.3202295660972595
Epoch: 6537, Batch Gradient Norm: 13.49857506754191
Epoch: 6537, Batch Gradient Norm after: 13.49857506754191
Epoch 6538/10000, Prediction Accuracy = 63.919999999999995%, Loss = 0.3205085456371307
Epoch: 6538, Batch Gradient Norm: 15.881850322046876
Epoch: 6538, Batch Gradient Norm after: 15.881850322046876
Epoch 6539/10000, Prediction Accuracy = 63.698%, Loss = 0.3246730089187622
Epoch: 6539, Batch Gradient Norm: 14.397882202435369
Epoch: 6539, Batch Gradient Norm after: 14.397882202435369
Epoch 6540/10000, Prediction Accuracy = 63.89000000000001%, Loss = 0.3197403073310852
Epoch: 6540, Batch Gradient Norm: 15.593930088419626
Epoch: 6540, Batch Gradient Norm after: 15.593930088419626
Epoch 6541/10000, Prediction Accuracy = 63.99400000000001%, Loss = 0.32288939952850343
Epoch: 6541, Batch Gradient Norm: 15.350309879825513
Epoch: 6541, Batch Gradient Norm after: 15.350309879825513
Epoch 6542/10000, Prediction Accuracy = 63.788%, Loss = 0.32401793599128725
Epoch: 6542, Batch Gradient Norm: 13.33445682530763
Epoch: 6542, Batch Gradient Norm after: 13.33445682530763
Epoch 6543/10000, Prediction Accuracy = 63.824%, Loss = 0.3202061355113983
Epoch: 6543, Batch Gradient Norm: 14.277558604921019
Epoch: 6543, Batch Gradient Norm after: 14.277558604921019
Epoch 6544/10000, Prediction Accuracy = 63.754000000000005%, Loss = 0.32183019518852235
Epoch: 6544, Batch Gradient Norm: 15.000569757440472
Epoch: 6544, Batch Gradient Norm after: 15.000569757440472
Epoch 6545/10000, Prediction Accuracy = 63.924%, Loss = 0.3236509084701538
Epoch: 6545, Batch Gradient Norm: 16.071046819451713
Epoch: 6545, Batch Gradient Norm after: 16.071046819451713
Epoch 6546/10000, Prediction Accuracy = 63.878%, Loss = 0.32326008677482604
Epoch: 6546, Batch Gradient Norm: 16.671655315995956
Epoch: 6546, Batch Gradient Norm after: 16.671655315995956
Epoch 6547/10000, Prediction Accuracy = 63.790000000000006%, Loss = 0.32392354011535646
Epoch: 6547, Batch Gradient Norm: 15.767244349136854
Epoch: 6547, Batch Gradient Norm after: 15.767244349136854
Epoch 6548/10000, Prediction Accuracy = 63.874%, Loss = 0.32150046825408934
Epoch: 6548, Batch Gradient Norm: 12.573371781352584
Epoch: 6548, Batch Gradient Norm after: 12.573371781352584
Epoch 6549/10000, Prediction Accuracy = 63.8%, Loss = 0.31913201212882997
Epoch: 6549, Batch Gradient Norm: 13.230152912747899
Epoch: 6549, Batch Gradient Norm after: 13.230152912747899
Epoch 6550/10000, Prediction Accuracy = 63.89%, Loss = 0.32152411341667175
Epoch: 6550, Batch Gradient Norm: 13.301393408701617
Epoch: 6550, Batch Gradient Norm after: 13.301393408701617
Epoch 6551/10000, Prediction Accuracy = 63.962%, Loss = 0.31848504543304446
Epoch: 6551, Batch Gradient Norm: 11.981749534379775
Epoch: 6551, Batch Gradient Norm after: 11.981749534379775
Epoch 6552/10000, Prediction Accuracy = 63.782000000000004%, Loss = 0.31787529587745667
Epoch: 6552, Batch Gradient Norm: 17.450260326850604
Epoch: 6552, Batch Gradient Norm after: 17.450260326850604
Epoch 6553/10000, Prediction Accuracy = 63.85200000000001%, Loss = 0.3296014368534088
Epoch: 6553, Batch Gradient Norm: 15.842933726292255
Epoch: 6553, Batch Gradient Norm after: 15.811188684458482
Epoch 6554/10000, Prediction Accuracy = 63.92%, Loss = 0.32074776887893675
Epoch: 6554, Batch Gradient Norm: 16.728496326640737
Epoch: 6554, Batch Gradient Norm after: 16.427493278077304
Epoch 6555/10000, Prediction Accuracy = 63.757999999999996%, Loss = 0.32376099228858946
Epoch: 6555, Batch Gradient Norm: 14.206070003151176
Epoch: 6555, Batch Gradient Norm after: 14.206070003151176
Epoch 6556/10000, Prediction Accuracy = 63.95%, Loss = 0.31845842599868773
Epoch: 6556, Batch Gradient Norm: 13.75555212058397
Epoch: 6556, Batch Gradient Norm after: 13.75555212058397
Epoch 6557/10000, Prediction Accuracy = 63.95399999999999%, Loss = 0.3194567382335663
Epoch: 6557, Batch Gradient Norm: 12.415960977369092
Epoch: 6557, Batch Gradient Norm after: 12.415960977369092
Epoch 6558/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.31944094896316527
Epoch: 6558, Batch Gradient Norm: 10.922282059275394
Epoch: 6558, Batch Gradient Norm after: 10.922282059275394
Epoch 6559/10000, Prediction Accuracy = 63.879999999999995%, Loss = 0.3147313237190247
Epoch: 6559, Batch Gradient Norm: 9.168186169991818
Epoch: 6559, Batch Gradient Norm after: 9.168186169991818
Epoch 6560/10000, Prediction Accuracy = 63.89200000000001%, Loss = 0.31367320418357847
Epoch: 6560, Batch Gradient Norm: 10.199654862134073
Epoch: 6560, Batch Gradient Norm after: 10.199654862134073
Epoch 6561/10000, Prediction Accuracy = 63.89%, Loss = 0.316877681016922
Epoch: 6561, Batch Gradient Norm: 10.589033229751918
Epoch: 6561, Batch Gradient Norm after: 10.589033229751918
Epoch 6562/10000, Prediction Accuracy = 63.93400000000001%, Loss = 0.3166300058364868
Epoch: 6562, Batch Gradient Norm: 11.687159245583894
Epoch: 6562, Batch Gradient Norm after: 11.687159245583894
Epoch 6563/10000, Prediction Accuracy = 63.769999999999996%, Loss = 0.31783134341239927
Epoch: 6563, Batch Gradient Norm: 13.561739491945364
Epoch: 6563, Batch Gradient Norm after: 13.561739491945364
Epoch 6564/10000, Prediction Accuracy = 63.818000000000005%, Loss = 0.32317072749137876
Epoch: 6564, Batch Gradient Norm: 14.587708374779965
Epoch: 6564, Batch Gradient Norm after: 14.587708374779965
Epoch 6565/10000, Prediction Accuracy = 63.878%, Loss = 0.32295417189598086
Epoch: 6565, Batch Gradient Norm: 14.638397680413407
Epoch: 6565, Batch Gradient Norm after: 14.638397680413407
Epoch 6566/10000, Prediction Accuracy = 63.926%, Loss = 0.3212557792663574
Epoch: 6566, Batch Gradient Norm: 15.464809457406579
Epoch: 6566, Batch Gradient Norm after: 15.464809457406579
Epoch 6567/10000, Prediction Accuracy = 63.90599999999999%, Loss = 0.32303295135498045
Epoch: 6567, Batch Gradient Norm: 12.428832098269487
Epoch: 6567, Batch Gradient Norm after: 12.428832098269487
Epoch 6568/10000, Prediction Accuracy = 63.81%, Loss = 0.31987686157226564
Epoch: 6568, Batch Gradient Norm: 13.678480643177103
Epoch: 6568, Batch Gradient Norm after: 13.678480643177103
Epoch 6569/10000, Prediction Accuracy = 63.872%, Loss = 0.3218535900115967
Epoch: 6569, Batch Gradient Norm: 13.820653304731275
Epoch: 6569, Batch Gradient Norm after: 13.820653304731275
Epoch 6570/10000, Prediction Accuracy = 63.864%, Loss = 0.31963621973991396
Epoch: 6570, Batch Gradient Norm: 13.03186760268564
Epoch: 6570, Batch Gradient Norm after: 13.03186760268564
Epoch 6571/10000, Prediction Accuracy = 63.90599999999999%, Loss = 0.3186440408229828
Epoch: 6571, Batch Gradient Norm: 12.07578048548273
Epoch: 6571, Batch Gradient Norm after: 12.07578048548273
Epoch 6572/10000, Prediction Accuracy = 64.01199999999999%, Loss = 0.31754053235054014
Epoch: 6572, Batch Gradient Norm: 14.78997104642378
Epoch: 6572, Batch Gradient Norm after: 14.78997104642378
Epoch 6573/10000, Prediction Accuracy = 63.802%, Loss = 0.3240398943424225
Epoch: 6573, Batch Gradient Norm: 14.183188750029583
Epoch: 6573, Batch Gradient Norm after: 14.183188750029583
Epoch 6574/10000, Prediction Accuracy = 63.874%, Loss = 0.318431955575943
Epoch: 6574, Batch Gradient Norm: 14.191785887160583
Epoch: 6574, Batch Gradient Norm after: 14.191785887160583
Epoch 6575/10000, Prediction Accuracy = 63.79%, Loss = 0.32029740810394286
Epoch: 6575, Batch Gradient Norm: 12.790556203161916
Epoch: 6575, Batch Gradient Norm after: 12.790556203161916
Epoch 6576/10000, Prediction Accuracy = 63.918000000000006%, Loss = 0.31896598935127257
Epoch: 6576, Batch Gradient Norm: 11.230611873873391
Epoch: 6576, Batch Gradient Norm after: 11.230611873873391
Epoch 6577/10000, Prediction Accuracy = 63.996%, Loss = 0.31542190313339236
Epoch: 6577, Batch Gradient Norm: 14.859212543001115
Epoch: 6577, Batch Gradient Norm after: 14.859212543001115
Epoch 6578/10000, Prediction Accuracy = 63.936%, Loss = 0.3203976035118103
Epoch: 6578, Batch Gradient Norm: 12.48542900724063
Epoch: 6578, Batch Gradient Norm after: 12.48542900724063
Epoch 6579/10000, Prediction Accuracy = 64.01400000000001%, Loss = 0.3164464831352234
Epoch: 6579, Batch Gradient Norm: 13.593161081219582
Epoch: 6579, Batch Gradient Norm after: 13.593161081219582
Epoch 6580/10000, Prediction Accuracy = 63.95%, Loss = 0.31840404868125916
Epoch: 6580, Batch Gradient Norm: 14.032486869360703
Epoch: 6580, Batch Gradient Norm after: 14.032486869360703
Epoch 6581/10000, Prediction Accuracy = 63.854%, Loss = 0.3183997392654419
Epoch: 6581, Batch Gradient Norm: 13.687101829754237
Epoch: 6581, Batch Gradient Norm after: 13.687101829754237
Epoch 6582/10000, Prediction Accuracy = 63.94200000000001%, Loss = 0.3195346474647522
Epoch: 6582, Batch Gradient Norm: 14.242780252761353
Epoch: 6582, Batch Gradient Norm after: 14.242780252761353
Epoch 6583/10000, Prediction Accuracy = 63.89%, Loss = 0.318999844789505
Epoch: 6583, Batch Gradient Norm: 15.11265901929296
Epoch: 6583, Batch Gradient Norm after: 15.11265901929296
Epoch 6584/10000, Prediction Accuracy = 63.85600000000001%, Loss = 0.3212342858314514
Epoch: 6584, Batch Gradient Norm: 11.777162370347387
Epoch: 6584, Batch Gradient Norm after: 11.777162370347387
Epoch 6585/10000, Prediction Accuracy = 63.848%, Loss = 0.3178333342075348
Epoch: 6585, Batch Gradient Norm: 13.493015672252435
Epoch: 6585, Batch Gradient Norm after: 13.493015672252435
Epoch 6586/10000, Prediction Accuracy = 63.757999999999996%, Loss = 0.321098780632019
Epoch: 6586, Batch Gradient Norm: 15.871165352886013
Epoch: 6586, Batch Gradient Norm after: 15.871165352886013
Epoch 6587/10000, Prediction Accuracy = 63.788%, Loss = 0.32067192196846006
Epoch: 6587, Batch Gradient Norm: 14.19392671772261
Epoch: 6587, Batch Gradient Norm after: 14.19392671772261
Epoch 6588/10000, Prediction Accuracy = 63.894000000000005%, Loss = 0.3191268861293793
Epoch: 6588, Batch Gradient Norm: 11.937738067390596
Epoch: 6588, Batch Gradient Norm after: 11.937738067390596
Epoch 6589/10000, Prediction Accuracy = 63.862%, Loss = 0.31689101457595825
Epoch: 6589, Batch Gradient Norm: 12.26170037694224
Epoch: 6589, Batch Gradient Norm after: 12.26170037694224
Epoch 6590/10000, Prediction Accuracy = 63.870000000000005%, Loss = 0.318217134475708
Epoch: 6590, Batch Gradient Norm: 12.460136867112114
Epoch: 6590, Batch Gradient Norm after: 12.460136867112114
Epoch 6591/10000, Prediction Accuracy = 63.89399999999999%, Loss = 0.31648101806640627
Epoch: 6591, Batch Gradient Norm: 11.659734978453118
Epoch: 6591, Batch Gradient Norm after: 11.659734978453118
Epoch 6592/10000, Prediction Accuracy = 63.98599999999999%, Loss = 0.31694868206977844
Epoch: 6592, Batch Gradient Norm: 12.413476316708497
Epoch: 6592, Batch Gradient Norm after: 12.413476316708497
Epoch 6593/10000, Prediction Accuracy = 63.84400000000001%, Loss = 0.31782554984092715
Epoch: 6593, Batch Gradient Norm: 11.719898800372617
Epoch: 6593, Batch Gradient Norm after: 11.719898800372617
Epoch 6594/10000, Prediction Accuracy = 63.822%, Loss = 0.31751930713653564
Epoch: 6594, Batch Gradient Norm: 12.365049250250014
Epoch: 6594, Batch Gradient Norm after: 12.365049250250014
Epoch 6595/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.3178021490573883
Epoch: 6595, Batch Gradient Norm: 14.78587470697618
Epoch: 6595, Batch Gradient Norm after: 14.78587470697618
Epoch 6596/10000, Prediction Accuracy = 63.98%, Loss = 0.3203729808330536
Epoch: 6596, Batch Gradient Norm: 16.431257895446066
Epoch: 6596, Batch Gradient Norm after: 16.431257895446066
Epoch 6597/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.3216849625110626
Epoch: 6597, Batch Gradient Norm: 17.15464031226893
Epoch: 6597, Batch Gradient Norm after: 17.15464031226893
Epoch 6598/10000, Prediction Accuracy = 63.91400000000001%, Loss = 0.3233943998813629
Epoch: 6598, Batch Gradient Norm: 16.665914411660975
Epoch: 6598, Batch Gradient Norm after: 16.665914411660975
Epoch 6599/10000, Prediction Accuracy = 63.894000000000005%, Loss = 0.32279430627822875
Epoch: 6599, Batch Gradient Norm: 16.572928010146203
Epoch: 6599, Batch Gradient Norm after: 16.572928010146203
Epoch 6600/10000, Prediction Accuracy = 63.95799999999999%, Loss = 0.32278999090194704
Epoch: 6600, Batch Gradient Norm: 16.889020799064404
Epoch: 6600, Batch Gradient Norm after: 16.061097868616077
Epoch 6601/10000, Prediction Accuracy = 64.018%, Loss = 0.32005004286766053
Epoch: 6601, Batch Gradient Norm: 16.86560129806573
Epoch: 6601, Batch Gradient Norm after: 16.86560129806573
Epoch 6602/10000, Prediction Accuracy = 63.912%, Loss = 0.3248927414417267
Epoch: 6602, Batch Gradient Norm: 15.283865851293891
Epoch: 6602, Batch Gradient Norm after: 15.283865851293891
Epoch 6603/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.3194749474525452
Epoch: 6603, Batch Gradient Norm: 16.461092042113567
Epoch: 6603, Batch Gradient Norm after: 16.461092042113567
Epoch 6604/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.3222032606601715
Epoch: 6604, Batch Gradient Norm: 13.738227505695123
Epoch: 6604, Batch Gradient Norm after: 13.738227505695123
Epoch 6605/10000, Prediction Accuracy = 63.922000000000004%, Loss = 0.3181771159172058
Epoch: 6605, Batch Gradient Norm: 12.53518899457401
Epoch: 6605, Batch Gradient Norm after: 12.53518899457401
Epoch 6606/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.3181549906730652
Epoch: 6606, Batch Gradient Norm: 11.524871385293174
Epoch: 6606, Batch Gradient Norm after: 11.524871385293174
Epoch 6607/10000, Prediction Accuracy = 63.983999999999995%, Loss = 0.3162703514099121
Epoch: 6607, Batch Gradient Norm: 11.95878023357895
Epoch: 6607, Batch Gradient Norm after: 11.95878023357895
Epoch 6608/10000, Prediction Accuracy = 63.924%, Loss = 0.3184638261795044
Epoch: 6608, Batch Gradient Norm: 12.93738515148616
Epoch: 6608, Batch Gradient Norm after: 12.93738515148616
Epoch 6609/10000, Prediction Accuracy = 63.96%, Loss = 0.3173574686050415
Epoch: 6609, Batch Gradient Norm: 12.739008294042018
Epoch: 6609, Batch Gradient Norm after: 12.739008294042018
Epoch 6610/10000, Prediction Accuracy = 63.895999999999994%, Loss = 0.3158397853374481
Epoch: 6610, Batch Gradient Norm: 12.118002267517937
Epoch: 6610, Batch Gradient Norm after: 12.118002267517937
Epoch 6611/10000, Prediction Accuracy = 63.982000000000006%, Loss = 0.3160639226436615
Epoch: 6611, Batch Gradient Norm: 13.075870292645636
Epoch: 6611, Batch Gradient Norm after: 13.075870292645636
Epoch 6612/10000, Prediction Accuracy = 63.88000000000001%, Loss = 0.3172824144363403
Epoch: 6612, Batch Gradient Norm: 14.20965896080149
Epoch: 6612, Batch Gradient Norm after: 14.20965896080149
Epoch 6613/10000, Prediction Accuracy = 63.754%, Loss = 0.32011947631835935
Epoch: 6613, Batch Gradient Norm: 12.936822859156925
Epoch: 6613, Batch Gradient Norm after: 12.936822859156925
Epoch 6614/10000, Prediction Accuracy = 63.85799999999999%, Loss = 0.321104633808136
Epoch: 6614, Batch Gradient Norm: 11.873519653919585
Epoch: 6614, Batch Gradient Norm after: 11.873519653919585
Epoch 6615/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.31841486096382143
Epoch: 6615, Batch Gradient Norm: 13.927061117729766
Epoch: 6615, Batch Gradient Norm after: 13.927061117729766
Epoch 6616/10000, Prediction Accuracy = 63.943999999999996%, Loss = 0.31937684416770934
Epoch: 6616, Batch Gradient Norm: 14.177174770357345
Epoch: 6616, Batch Gradient Norm after: 14.177174770357345
Epoch 6617/10000, Prediction Accuracy = 63.906000000000006%, Loss = 0.31808029413223265
Epoch: 6617, Batch Gradient Norm: 12.528199199036706
Epoch: 6617, Batch Gradient Norm after: 12.528199199036706
Epoch 6618/10000, Prediction Accuracy = 63.932%, Loss = 0.31937832832336427
Epoch: 6618, Batch Gradient Norm: 13.853793310216698
Epoch: 6618, Batch Gradient Norm after: 13.853793310216698
Epoch 6619/10000, Prediction Accuracy = 63.906000000000006%, Loss = 0.318534380197525
Epoch: 6619, Batch Gradient Norm: 12.800414166871896
Epoch: 6619, Batch Gradient Norm after: 12.800414166871896
Epoch 6620/10000, Prediction Accuracy = 63.977999999999994%, Loss = 0.31853153109550475
Epoch: 6620, Batch Gradient Norm: 12.85130253318311
Epoch: 6620, Batch Gradient Norm after: 12.85130253318311
Epoch 6621/10000, Prediction Accuracy = 63.92%, Loss = 0.3160442650318146
Epoch: 6621, Batch Gradient Norm: 11.350829806517435
Epoch: 6621, Batch Gradient Norm after: 11.350829806517435
Epoch 6622/10000, Prediction Accuracy = 63.848%, Loss = 0.3166452944278717
Epoch: 6622, Batch Gradient Norm: 13.668599222038779
Epoch: 6622, Batch Gradient Norm after: 13.668599222038779
Epoch 6623/10000, Prediction Accuracy = 64.08%, Loss = 0.3175391316413879
Epoch: 6623, Batch Gradient Norm: 13.051250202968632
Epoch: 6623, Batch Gradient Norm after: 13.051250202968632
Epoch 6624/10000, Prediction Accuracy = 63.879999999999995%, Loss = 0.3180625379085541
Epoch: 6624, Batch Gradient Norm: 11.758680943704682
Epoch: 6624, Batch Gradient Norm after: 11.758680943704682
Epoch 6625/10000, Prediction Accuracy = 63.912%, Loss = 0.3153462648391724
Epoch: 6625, Batch Gradient Norm: 11.35538526449782
Epoch: 6625, Batch Gradient Norm after: 11.35538526449782
Epoch 6626/10000, Prediction Accuracy = 63.94199999999999%, Loss = 0.31518937945365905
Epoch: 6626, Batch Gradient Norm: 11.381910230841477
Epoch: 6626, Batch Gradient Norm after: 11.381910230841477
Epoch 6627/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.316136908531189
Epoch: 6627, Batch Gradient Norm: 12.206624859273346
Epoch: 6627, Batch Gradient Norm after: 12.206624859273346
Epoch 6628/10000, Prediction Accuracy = 63.88800000000001%, Loss = 0.31888617277145387
Epoch: 6628, Batch Gradient Norm: 11.085385274887894
Epoch: 6628, Batch Gradient Norm after: 11.085385274887894
Epoch 6629/10000, Prediction Accuracy = 63.955999999999996%, Loss = 0.31611881852149964
Epoch: 6629, Batch Gradient Norm: 13.56574769809501
Epoch: 6629, Batch Gradient Norm after: 13.56574769809501
Epoch 6630/10000, Prediction Accuracy = 63.83999999999999%, Loss = 0.3191124975681305
Epoch: 6630, Batch Gradient Norm: 12.727322436882837
Epoch: 6630, Batch Gradient Norm after: 12.727322436882837
Epoch 6631/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.316509348154068
Epoch: 6631, Batch Gradient Norm: 12.092275056052241
Epoch: 6631, Batch Gradient Norm after: 12.092275056052241
Epoch 6632/10000, Prediction Accuracy = 63.902%, Loss = 0.3146447420120239
Epoch: 6632, Batch Gradient Norm: 10.300571932772563
Epoch: 6632, Batch Gradient Norm after: 10.300571932772563
Epoch 6633/10000, Prediction Accuracy = 63.838%, Loss = 0.3143609642982483
Epoch: 6633, Batch Gradient Norm: 13.599823049674718
Epoch: 6633, Batch Gradient Norm after: 13.599823049674718
Epoch 6634/10000, Prediction Accuracy = 63.95799999999999%, Loss = 0.31823051571846006
Epoch: 6634, Batch Gradient Norm: 14.027479149963893
Epoch: 6634, Batch Gradient Norm after: 14.027479149963893
Epoch 6635/10000, Prediction Accuracy = 63.970000000000006%, Loss = 0.31743146777153014
Epoch: 6635, Batch Gradient Norm: 15.966579400319024
Epoch: 6635, Batch Gradient Norm after: 15.966579400319024
Epoch 6636/10000, Prediction Accuracy = 63.806000000000004%, Loss = 0.32313752770423887
Epoch: 6636, Batch Gradient Norm: 12.908937803861834
Epoch: 6636, Batch Gradient Norm after: 12.908937803861834
Epoch 6637/10000, Prediction Accuracy = 63.962%, Loss = 0.3164291143417358
Epoch: 6637, Batch Gradient Norm: 10.848419692749225
Epoch: 6637, Batch Gradient Norm after: 10.848419692749225
Epoch 6638/10000, Prediction Accuracy = 63.806000000000004%, Loss = 0.3164631128311157
Epoch: 6638, Batch Gradient Norm: 14.325917281163564
Epoch: 6638, Batch Gradient Norm after: 14.325917281163564
Epoch 6639/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.31941884756088257
Epoch: 6639, Batch Gradient Norm: 12.44768689447312
Epoch: 6639, Batch Gradient Norm after: 12.44768689447312
Epoch 6640/10000, Prediction Accuracy = 64.03%, Loss = 0.3141120433807373
Epoch: 6640, Batch Gradient Norm: 12.424630392879113
Epoch: 6640, Batch Gradient Norm after: 12.424630392879113
Epoch 6641/10000, Prediction Accuracy = 63.94%, Loss = 0.3170072019100189
Epoch: 6641, Batch Gradient Norm: 11.820696524514856
Epoch: 6641, Batch Gradient Norm after: 11.820696524514856
Epoch 6642/10000, Prediction Accuracy = 63.876%, Loss = 0.31533451080322267
Epoch: 6642, Batch Gradient Norm: 12.580092794834506
Epoch: 6642, Batch Gradient Norm after: 12.580092794834506
Epoch 6643/10000, Prediction Accuracy = 63.852%, Loss = 0.3173476755619049
Epoch: 6643, Batch Gradient Norm: 14.890659202634616
Epoch: 6643, Batch Gradient Norm after: 14.890659202634616
Epoch 6644/10000, Prediction Accuracy = 63.79%, Loss = 0.323849493265152
Epoch: 6644, Batch Gradient Norm: 12.23097624885454
Epoch: 6644, Batch Gradient Norm after: 12.23097624885454
Epoch 6645/10000, Prediction Accuracy = 63.903999999999996%, Loss = 0.31549137830734253
Epoch: 6645, Batch Gradient Norm: 13.247501941981715
Epoch: 6645, Batch Gradient Norm after: 13.247501941981715
Epoch 6646/10000, Prediction Accuracy = 63.967999999999996%, Loss = 0.3193890154361725
Epoch: 6646, Batch Gradient Norm: 13.437096818215405
Epoch: 6646, Batch Gradient Norm after: 13.437096818215405
Epoch 6647/10000, Prediction Accuracy = 64.034%, Loss = 0.3173189103603363
Epoch: 6647, Batch Gradient Norm: 10.507310151363797
Epoch: 6647, Batch Gradient Norm after: 10.507310151363797
Epoch 6648/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.3153377115726471
Epoch: 6648, Batch Gradient Norm: 10.959231993521962
Epoch: 6648, Batch Gradient Norm after: 10.959231993521962
Epoch 6649/10000, Prediction Accuracy = 64.03%, Loss = 0.3144435346126556
Epoch: 6649, Batch Gradient Norm: 13.9364706465107
Epoch: 6649, Batch Gradient Norm after: 13.9364706465107
Epoch 6650/10000, Prediction Accuracy = 63.92999999999999%, Loss = 0.31939737200737
Epoch: 6650, Batch Gradient Norm: 15.252604904981341
Epoch: 6650, Batch Gradient Norm after: 15.252604904981341
Epoch 6651/10000, Prediction Accuracy = 63.924%, Loss = 0.3192368268966675
Epoch: 6651, Batch Gradient Norm: 15.58895805893768
Epoch: 6651, Batch Gradient Norm after: 15.58895805893768
Epoch 6652/10000, Prediction Accuracy = 63.936%, Loss = 0.32058714628219603
Epoch: 6652, Batch Gradient Norm: 13.673187677958346
Epoch: 6652, Batch Gradient Norm after: 13.673187677958346
Epoch 6653/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.316421765089035
Epoch: 6653, Batch Gradient Norm: 15.244158535387974
Epoch: 6653, Batch Gradient Norm after: 15.244158535387974
Epoch 6654/10000, Prediction Accuracy = 63.852%, Loss = 0.320031875371933
Epoch: 6654, Batch Gradient Norm: 16.060862839289435
Epoch: 6654, Batch Gradient Norm after: 16.060862839289435
Epoch 6655/10000, Prediction Accuracy = 63.826%, Loss = 0.32149123549461367
Epoch: 6655, Batch Gradient Norm: 12.15095262561853
Epoch: 6655, Batch Gradient Norm after: 12.15095262561853
Epoch 6656/10000, Prediction Accuracy = 63.976%, Loss = 0.31698375940322876
Epoch: 6656, Batch Gradient Norm: 12.455820133854147
Epoch: 6656, Batch Gradient Norm after: 12.455820133854147
Epoch 6657/10000, Prediction Accuracy = 63.86800000000001%, Loss = 0.31760660409927366
Epoch: 6657, Batch Gradient Norm: 14.35296169880973
Epoch: 6657, Batch Gradient Norm after: 14.35296169880973
Epoch 6658/10000, Prediction Accuracy = 63.834%, Loss = 0.32311689853668213
Epoch: 6658, Batch Gradient Norm: 14.026600901894886
Epoch: 6658, Batch Gradient Norm after: 14.026600901894886
Epoch 6659/10000, Prediction Accuracy = 63.932%, Loss = 0.3205786108970642
Epoch: 6659, Batch Gradient Norm: 15.264669806704601
Epoch: 6659, Batch Gradient Norm after: 15.264669806704601
Epoch 6660/10000, Prediction Accuracy = 63.970000000000006%, Loss = 0.31825328469276426
Epoch: 6660, Batch Gradient Norm: 15.173884206306226
Epoch: 6660, Batch Gradient Norm after: 15.173884206306226
Epoch 6661/10000, Prediction Accuracy = 63.86%, Loss = 0.31957945227622986
Epoch: 6661, Batch Gradient Norm: 16.940364386326475
Epoch: 6661, Batch Gradient Norm after: 16.940364386326475
Epoch 6662/10000, Prediction Accuracy = 64.042%, Loss = 0.32262571454048156
Epoch: 6662, Batch Gradient Norm: 16.849085067949297
Epoch: 6662, Batch Gradient Norm after: 16.849085067949297
Epoch 6663/10000, Prediction Accuracy = 63.95399999999999%, Loss = 0.3245855450630188
Epoch: 6663, Batch Gradient Norm: 16.201428703118594
Epoch: 6663, Batch Gradient Norm after: 16.201428703118594
Epoch 6664/10000, Prediction Accuracy = 64.088%, Loss = 0.3202681660652161
Epoch: 6664, Batch Gradient Norm: 17.00940238982968
Epoch: 6664, Batch Gradient Norm after: 17.00940238982968
Epoch 6665/10000, Prediction Accuracy = 63.90599999999999%, Loss = 0.32257975935935973
Epoch: 6665, Batch Gradient Norm: 16.912542962541398
Epoch: 6665, Batch Gradient Norm after: 16.912542962541398
Epoch 6666/10000, Prediction Accuracy = 63.802%, Loss = 0.3225118935108185
Epoch: 6666, Batch Gradient Norm: 19.893481324889663
Epoch: 6666, Batch Gradient Norm after: 18.897671268891624
Epoch 6667/10000, Prediction Accuracy = 64.03999999999999%, Loss = 0.32816195487976074
Epoch: 6667, Batch Gradient Norm: 14.541529046405094
Epoch: 6667, Batch Gradient Norm after: 14.541529046405094
Epoch 6668/10000, Prediction Accuracy = 63.9%, Loss = 0.31804071068763734
Epoch: 6668, Batch Gradient Norm: 13.534512676652794
Epoch: 6668, Batch Gradient Norm after: 13.534512676652794
Epoch 6669/10000, Prediction Accuracy = 63.96600000000001%, Loss = 0.3154128909111023
Epoch: 6669, Batch Gradient Norm: 13.90246075050077
Epoch: 6669, Batch Gradient Norm after: 13.90246075050077
Epoch 6670/10000, Prediction Accuracy = 63.91799999999999%, Loss = 0.3175641596317291
Epoch: 6670, Batch Gradient Norm: 12.80756627047957
Epoch: 6670, Batch Gradient Norm after: 12.80756627047957
Epoch 6671/10000, Prediction Accuracy = 63.876%, Loss = 0.31650453209877016
Epoch: 6671, Batch Gradient Norm: 12.978132569721431
Epoch: 6671, Batch Gradient Norm after: 12.978132569721431
Epoch 6672/10000, Prediction Accuracy = 64.072%, Loss = 0.3166878461837769
Epoch: 6672, Batch Gradient Norm: 14.647809516782704
Epoch: 6672, Batch Gradient Norm after: 14.647809516782704
Epoch 6673/10000, Prediction Accuracy = 63.867999999999995%, Loss = 0.3178135335445404
Epoch: 6673, Batch Gradient Norm: 13.539880661440366
Epoch: 6673, Batch Gradient Norm after: 13.539880661440366
Epoch 6674/10000, Prediction Accuracy = 63.95%, Loss = 0.3165598750114441
Epoch: 6674, Batch Gradient Norm: 15.13319879978194
Epoch: 6674, Batch Gradient Norm after: 15.13319879978194
Epoch 6675/10000, Prediction Accuracy = 63.86%, Loss = 0.3200585901737213
Epoch: 6675, Batch Gradient Norm: 15.326016480502387
Epoch: 6675, Batch Gradient Norm after: 15.326016480502387
Epoch 6676/10000, Prediction Accuracy = 64.02000000000001%, Loss = 0.31878921389579773
Epoch: 6676, Batch Gradient Norm: 14.48469096939419
Epoch: 6676, Batch Gradient Norm after: 14.48469096939419
Epoch 6677/10000, Prediction Accuracy = 63.976%, Loss = 0.31964179277420046
Epoch: 6677, Batch Gradient Norm: 15.872589486239514
Epoch: 6677, Batch Gradient Norm after: 15.826678928421755
Epoch 6678/10000, Prediction Accuracy = 63.814%, Loss = 0.322231662273407
Epoch: 6678, Batch Gradient Norm: 14.68475800331941
Epoch: 6678, Batch Gradient Norm after: 14.68475800331941
Epoch 6679/10000, Prediction Accuracy = 63.814%, Loss = 0.320062917470932
Epoch: 6679, Batch Gradient Norm: 15.073362801000394
Epoch: 6679, Batch Gradient Norm after: 15.073362801000394
Epoch 6680/10000, Prediction Accuracy = 63.874%, Loss = 0.3197528958320618
Epoch: 6680, Batch Gradient Norm: 14.462680704017243
Epoch: 6680, Batch Gradient Norm after: 14.462680704017243
Epoch 6681/10000, Prediction Accuracy = 63.964%, Loss = 0.3179549813270569
Epoch: 6681, Batch Gradient Norm: 15.230012631705637
Epoch: 6681, Batch Gradient Norm after: 15.230012631705637
Epoch 6682/10000, Prediction Accuracy = 63.958000000000006%, Loss = 0.3200328290462494
Epoch: 6682, Batch Gradient Norm: 19.96727898517878
Epoch: 6682, Batch Gradient Norm after: 18.565969963832107
Epoch 6683/10000, Prediction Accuracy = 63.94200000000001%, Loss = 0.32685989141464233
Epoch: 6683, Batch Gradient Norm: 15.79759503621926
Epoch: 6683, Batch Gradient Norm after: 15.79759503621926
Epoch 6684/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.32045149207115176
Epoch: 6684, Batch Gradient Norm: 15.654350647634477
Epoch: 6684, Batch Gradient Norm after: 15.654350647634477
Epoch 6685/10000, Prediction Accuracy = 63.946000000000005%, Loss = 0.3232280433177948
Epoch: 6685, Batch Gradient Norm: 12.946948630215369
Epoch: 6685, Batch Gradient Norm after: 12.946948630215369
Epoch 6686/10000, Prediction Accuracy = 63.94%, Loss = 0.3161781430244446
Epoch: 6686, Batch Gradient Norm: 10.731784408376113
Epoch: 6686, Batch Gradient Norm after: 10.731784408376113
Epoch 6687/10000, Prediction Accuracy = 63.958000000000006%, Loss = 0.3131342113018036
Epoch: 6687, Batch Gradient Norm: 12.790293660895712
Epoch: 6687, Batch Gradient Norm after: 12.790293660895712
Epoch 6688/10000, Prediction Accuracy = 63.926%, Loss = 0.31420997977256776
Epoch: 6688, Batch Gradient Norm: 12.270328759271239
Epoch: 6688, Batch Gradient Norm after: 12.270328759271239
Epoch 6689/10000, Prediction Accuracy = 63.866%, Loss = 0.3159527063369751
Epoch: 6689, Batch Gradient Norm: 12.400353791352703
Epoch: 6689, Batch Gradient Norm after: 12.400353791352703
Epoch 6690/10000, Prediction Accuracy = 63.959999999999994%, Loss = 0.3160913050174713
Epoch: 6690, Batch Gradient Norm: 14.47527567651039
Epoch: 6690, Batch Gradient Norm after: 14.47527567651039
Epoch 6691/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.3209437131881714
Epoch: 6691, Batch Gradient Norm: 14.844119127069325
Epoch: 6691, Batch Gradient Norm after: 14.844119127069325
Epoch 6692/10000, Prediction Accuracy = 63.91600000000001%, Loss = 0.3196679651737213
Epoch: 6692, Batch Gradient Norm: 12.902176517791082
Epoch: 6692, Batch Gradient Norm after: 12.902176517791082
Epoch 6693/10000, Prediction Accuracy = 64.03%, Loss = 0.3170736074447632
Epoch: 6693, Batch Gradient Norm: 15.151265466289345
Epoch: 6693, Batch Gradient Norm after: 15.151265466289345
Epoch 6694/10000, Prediction Accuracy = 64.1%, Loss = 0.3180527567863464
Epoch: 6694, Batch Gradient Norm: 12.977401439613597
Epoch: 6694, Batch Gradient Norm after: 12.977401439613597
Epoch 6695/10000, Prediction Accuracy = 63.99400000000001%, Loss = 0.31638199687004087
Epoch: 6695, Batch Gradient Norm: 11.22667485754649
Epoch: 6695, Batch Gradient Norm after: 11.22667485754649
Epoch 6696/10000, Prediction Accuracy = 64.034%, Loss = 0.3126009464263916
Epoch: 6696, Batch Gradient Norm: 11.843470890839262
Epoch: 6696, Batch Gradient Norm after: 11.843470890839262
Epoch 6697/10000, Prediction Accuracy = 63.934000000000005%, Loss = 0.31695389151573183
Epoch: 6697, Batch Gradient Norm: 13.956381383370427
Epoch: 6697, Batch Gradient Norm after: 13.956381383370427
Epoch 6698/10000, Prediction Accuracy = 63.896%, Loss = 0.31613526940345765
Epoch: 6698, Batch Gradient Norm: 13.66913379535098
Epoch: 6698, Batch Gradient Norm after: 13.66913379535098
Epoch 6699/10000, Prediction Accuracy = 63.914%, Loss = 0.31842421293258666
Epoch: 6699, Batch Gradient Norm: 16.459283292519327
Epoch: 6699, Batch Gradient Norm after: 16.459283292519327
Epoch 6700/10000, Prediction Accuracy = 63.916%, Loss = 0.32295397520065305
Epoch: 6700, Batch Gradient Norm: 17.182421012880113
Epoch: 6700, Batch Gradient Norm after: 16.699780262720573
Epoch 6701/10000, Prediction Accuracy = 63.86%, Loss = 0.32212039828300476
Epoch: 6701, Batch Gradient Norm: 18.35722832516236
Epoch: 6701, Batch Gradient Norm after: 18.35722832516236
Epoch 6702/10000, Prediction Accuracy = 64.124%, Loss = 0.3240612208843231
Epoch: 6702, Batch Gradient Norm: 15.34978711631142
Epoch: 6702, Batch Gradient Norm after: 15.34978711631142
Epoch 6703/10000, Prediction Accuracy = 63.918000000000006%, Loss = 0.3193139433860779
Epoch: 6703, Batch Gradient Norm: 15.998014017557146
Epoch: 6703, Batch Gradient Norm after: 15.998014017557146
Epoch 6704/10000, Prediction Accuracy = 63.968%, Loss = 0.31959585547447206
Epoch: 6704, Batch Gradient Norm: 16.459269809584683
Epoch: 6704, Batch Gradient Norm after: 16.459269809584683
Epoch 6705/10000, Prediction Accuracy = 63.95399999999999%, Loss = 0.31985239386558534
Epoch: 6705, Batch Gradient Norm: 17.05399493475387
Epoch: 6705, Batch Gradient Norm after: 17.05399493475387
Epoch 6706/10000, Prediction Accuracy = 64.008%, Loss = 0.320428603887558
Epoch: 6706, Batch Gradient Norm: 16.593331828967802
Epoch: 6706, Batch Gradient Norm after: 16.593331828967802
Epoch 6707/10000, Prediction Accuracy = 63.94%, Loss = 0.32114432454109193
Epoch: 6707, Batch Gradient Norm: 12.090716639980077
Epoch: 6707, Batch Gradient Norm after: 12.090716639980077
Epoch 6708/10000, Prediction Accuracy = 64.006%, Loss = 0.31429328918457033
Epoch: 6708, Batch Gradient Norm: 11.40499159414037
Epoch: 6708, Batch Gradient Norm after: 11.40499159414037
Epoch 6709/10000, Prediction Accuracy = 64.03799999999998%, Loss = 0.3130674660205841
Epoch: 6709, Batch Gradient Norm: 11.644019031843365
Epoch: 6709, Batch Gradient Norm after: 11.644019031843365
Epoch 6710/10000, Prediction Accuracy = 64.01400000000001%, Loss = 0.31455164551734927
Epoch: 6710, Batch Gradient Norm: 9.381072799855906
Epoch: 6710, Batch Gradient Norm after: 9.381072799855906
Epoch 6711/10000, Prediction Accuracy = 63.95799999999999%, Loss = 0.31392560601234437
Epoch: 6711, Batch Gradient Norm: 12.277004550326582
Epoch: 6711, Batch Gradient Norm after: 12.277004550326582
Epoch 6712/10000, Prediction Accuracy = 63.95%, Loss = 0.317356276512146
Epoch: 6712, Batch Gradient Norm: 15.31058277547227
Epoch: 6712, Batch Gradient Norm after: 15.31058277547227
Epoch 6713/10000, Prediction Accuracy = 63.936%, Loss = 0.31889666318893434
Epoch: 6713, Batch Gradient Norm: 16.121628764943583
Epoch: 6713, Batch Gradient Norm after: 16.121628764943583
Epoch 6714/10000, Prediction Accuracy = 64.05199999999999%, Loss = 0.31964091658592225
Epoch: 6714, Batch Gradient Norm: 13.234260450131032
Epoch: 6714, Batch Gradient Norm after: 13.234260450131032
Epoch 6715/10000, Prediction Accuracy = 63.976%, Loss = 0.3139160811901093
Epoch: 6715, Batch Gradient Norm: 14.703175830669197
Epoch: 6715, Batch Gradient Norm after: 14.703175830669197
Epoch 6716/10000, Prediction Accuracy = 64.02599999999998%, Loss = 0.31656644940376283
Epoch: 6716, Batch Gradient Norm: 11.550123933116927
Epoch: 6716, Batch Gradient Norm after: 11.550123933116927
Epoch 6717/10000, Prediction Accuracy = 64.002%, Loss = 0.3140909433364868
Epoch: 6717, Batch Gradient Norm: 11.562288422258709
Epoch: 6717, Batch Gradient Norm after: 11.562288422258709
Epoch 6718/10000, Prediction Accuracy = 64.066%, Loss = 0.3110214591026306
Epoch: 6718, Batch Gradient Norm: 12.725057661583392
Epoch: 6718, Batch Gradient Norm after: 12.725057661583392
Epoch 6719/10000, Prediction Accuracy = 63.922000000000004%, Loss = 0.31595982909202575
Epoch: 6719, Batch Gradient Norm: 13.785272595978297
Epoch: 6719, Batch Gradient Norm after: 13.785272595978297
Epoch 6720/10000, Prediction Accuracy = 64.032%, Loss = 0.3162447392940521
Epoch: 6720, Batch Gradient Norm: 15.381944175812952
Epoch: 6720, Batch Gradient Norm after: 15.381944175812952
Epoch 6721/10000, Prediction Accuracy = 64.13000000000001%, Loss = 0.3182861626148224
Epoch: 6721, Batch Gradient Norm: 12.93919393696745
Epoch: 6721, Batch Gradient Norm after: 12.93919393696745
Epoch 6722/10000, Prediction Accuracy = 63.998000000000005%, Loss = 0.31477928161621094
Epoch: 6722, Batch Gradient Norm: 11.796607478946463
Epoch: 6722, Batch Gradient Norm after: 11.796607478946463
Epoch 6723/10000, Prediction Accuracy = 64.038%, Loss = 0.31196109652519227
Epoch: 6723, Batch Gradient Norm: 11.629224276682427
Epoch: 6723, Batch Gradient Norm after: 11.629224276682427
Epoch 6724/10000, Prediction Accuracy = 64.152%, Loss = 0.31514729261398317
Epoch: 6724, Batch Gradient Norm: 10.273188657420157
Epoch: 6724, Batch Gradient Norm after: 10.273188657420157
Epoch 6725/10000, Prediction Accuracy = 63.95%, Loss = 0.311907684803009
Epoch: 6725, Batch Gradient Norm: 8.873060886953992
Epoch: 6725, Batch Gradient Norm after: 8.873060886953992
Epoch 6726/10000, Prediction Accuracy = 64.02000000000001%, Loss = 0.3104339301586151
Epoch: 6726, Batch Gradient Norm: 11.594365133770946
Epoch: 6726, Batch Gradient Norm after: 11.594365133770946
Epoch 6727/10000, Prediction Accuracy = 63.98%, Loss = 0.3142652869224548
Epoch: 6727, Batch Gradient Norm: 11.791636459607343
Epoch: 6727, Batch Gradient Norm after: 11.791636459607343
Epoch 6728/10000, Prediction Accuracy = 64.042%, Loss = 0.313818359375
Epoch: 6728, Batch Gradient Norm: 12.32553428582455
Epoch: 6728, Batch Gradient Norm after: 12.32553428582455
Epoch 6729/10000, Prediction Accuracy = 64.086%, Loss = 0.3140588879585266
Epoch: 6729, Batch Gradient Norm: 11.302901968089467
Epoch: 6729, Batch Gradient Norm after: 11.302901968089467
Epoch 6730/10000, Prediction Accuracy = 63.972%, Loss = 0.31286938190460206
Epoch: 6730, Batch Gradient Norm: 12.993920619016325
Epoch: 6730, Batch Gradient Norm after: 12.993920619016325
Epoch 6731/10000, Prediction Accuracy = 63.964%, Loss = 0.3140843272209167
Epoch: 6731, Batch Gradient Norm: 13.766082435916667
Epoch: 6731, Batch Gradient Norm after: 13.766082435916667
Epoch 6732/10000, Prediction Accuracy = 64.002%, Loss = 0.31802800893783567
Epoch: 6732, Batch Gradient Norm: 13.460791433849423
Epoch: 6732, Batch Gradient Norm after: 13.460791433849423
Epoch 6733/10000, Prediction Accuracy = 63.870000000000005%, Loss = 0.31623533368110657
Epoch: 6733, Batch Gradient Norm: 12.190606714902056
Epoch: 6733, Batch Gradient Norm after: 12.190606714902056
Epoch 6734/10000, Prediction Accuracy = 64.084%, Loss = 0.31395148634910586
Epoch: 6734, Batch Gradient Norm: 10.824921428536369
Epoch: 6734, Batch Gradient Norm after: 10.824921428536369
Epoch 6735/10000, Prediction Accuracy = 63.989999999999995%, Loss = 0.31254740357398986
Epoch: 6735, Batch Gradient Norm: 11.652124734951764
Epoch: 6735, Batch Gradient Norm after: 11.652124734951764
Epoch 6736/10000, Prediction Accuracy = 64.038%, Loss = 0.31550944447517393
Epoch: 6736, Batch Gradient Norm: 14.240443144051605
Epoch: 6736, Batch Gradient Norm after: 14.240443144051605
Epoch 6737/10000, Prediction Accuracy = 63.794000000000004%, Loss = 0.3188522636890411
Epoch: 6737, Batch Gradient Norm: 15.50164786558455
Epoch: 6737, Batch Gradient Norm after: 15.50164786558455
Epoch 6738/10000, Prediction Accuracy = 63.94000000000001%, Loss = 0.3189252018928528
Epoch: 6738, Batch Gradient Norm: 15.126509364004532
Epoch: 6738, Batch Gradient Norm after: 15.126509364004532
Epoch 6739/10000, Prediction Accuracy = 63.912%, Loss = 0.32183008790016177
Epoch: 6739, Batch Gradient Norm: 15.865105016090363
Epoch: 6739, Batch Gradient Norm after: 15.865105016090363
Epoch 6740/10000, Prediction Accuracy = 63.896%, Loss = 0.3174705505371094
Epoch: 6740, Batch Gradient Norm: 19.143792876626968
Epoch: 6740, Batch Gradient Norm after: 19.143792876626968
Epoch 6741/10000, Prediction Accuracy = 63.93000000000001%, Loss = 0.32448647618293763
Epoch: 6741, Batch Gradient Norm: 15.555656787753875
Epoch: 6741, Batch Gradient Norm after: 15.555656787753875
Epoch 6742/10000, Prediction Accuracy = 64.036%, Loss = 0.3184613585472107
Epoch: 6742, Batch Gradient Norm: 14.000697758023762
Epoch: 6742, Batch Gradient Norm after: 14.000697758023762
Epoch 6743/10000, Prediction Accuracy = 64.026%, Loss = 0.3164880335330963
Epoch: 6743, Batch Gradient Norm: 16.092410360525328
Epoch: 6743, Batch Gradient Norm after: 16.092410360525328
Epoch 6744/10000, Prediction Accuracy = 63.962%, Loss = 0.3193528771400452
Epoch: 6744, Batch Gradient Norm: 18.423893412429216
Epoch: 6744, Batch Gradient Norm after: 18.423893412429216
Epoch 6745/10000, Prediction Accuracy = 63.866%, Loss = 0.3230466663837433
Epoch: 6745, Batch Gradient Norm: 16.087058597346193
Epoch: 6745, Batch Gradient Norm after: 16.087058597346193
Epoch 6746/10000, Prediction Accuracy = 64.062%, Loss = 0.3198772370815277
Epoch: 6746, Batch Gradient Norm: 14.574045808832167
Epoch: 6746, Batch Gradient Norm after: 14.574045808832167
Epoch 6747/10000, Prediction Accuracy = 63.919999999999995%, Loss = 0.319147652387619
Epoch: 6747, Batch Gradient Norm: 14.333122420180691
Epoch: 6747, Batch Gradient Norm after: 14.333122420180691
Epoch 6748/10000, Prediction Accuracy = 64.044%, Loss = 0.31560965776443484
Epoch: 6748, Batch Gradient Norm: 13.986271719572924
Epoch: 6748, Batch Gradient Norm after: 13.986271719572924
Epoch 6749/10000, Prediction Accuracy = 64.068%, Loss = 0.3155688941478729
Epoch: 6749, Batch Gradient Norm: 12.497587245856979
Epoch: 6749, Batch Gradient Norm after: 12.497587245856979
Epoch 6750/10000, Prediction Accuracy = 64.09%, Loss = 0.31499934792518614
Epoch: 6750, Batch Gradient Norm: 13.384021242579776
Epoch: 6750, Batch Gradient Norm after: 13.384021242579776
Epoch 6751/10000, Prediction Accuracy = 64.03999999999999%, Loss = 0.314932656288147
Epoch: 6751, Batch Gradient Norm: 14.498562134655387
Epoch: 6751, Batch Gradient Norm after: 14.498562134655387
Epoch 6752/10000, Prediction Accuracy = 63.984%, Loss = 0.316238933801651
Epoch: 6752, Batch Gradient Norm: 14.706800609193596
Epoch: 6752, Batch Gradient Norm after: 14.706800609193596
Epoch 6753/10000, Prediction Accuracy = 63.968%, Loss = 0.31750324964523313
Epoch: 6753, Batch Gradient Norm: 14.741294481652758
Epoch: 6753, Batch Gradient Norm after: 14.741294481652758
Epoch 6754/10000, Prediction Accuracy = 63.968%, Loss = 0.31731459498405457
Epoch: 6754, Batch Gradient Norm: 17.264157018821123
Epoch: 6754, Batch Gradient Norm after: 17.14480980630184
Epoch 6755/10000, Prediction Accuracy = 64.068%, Loss = 0.31959742307662964
Epoch: 6755, Batch Gradient Norm: 14.88470345603392
Epoch: 6755, Batch Gradient Norm after: 14.88470345603392
Epoch 6756/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.3219562888145447
Epoch: 6756, Batch Gradient Norm: 11.9330403380319
Epoch: 6756, Batch Gradient Norm after: 11.9330403380319
Epoch 6757/10000, Prediction Accuracy = 63.88399999999999%, Loss = 0.3161758303642273
Epoch: 6757, Batch Gradient Norm: 15.735782065909929
Epoch: 6757, Batch Gradient Norm after: 15.735782065909929
Epoch 6758/10000, Prediction Accuracy = 63.85799999999999%, Loss = 0.3207546412944794
Epoch: 6758, Batch Gradient Norm: 14.302233454426833
Epoch: 6758, Batch Gradient Norm after: 14.302233454426833
Epoch 6759/10000, Prediction Accuracy = 64.092%, Loss = 0.3159507930278778
Epoch: 6759, Batch Gradient Norm: 12.3737466158055
Epoch: 6759, Batch Gradient Norm after: 12.3737466158055
Epoch 6760/10000, Prediction Accuracy = 64.124%, Loss = 0.31241796612739564
Epoch: 6760, Batch Gradient Norm: 13.533644992975368
Epoch: 6760, Batch Gradient Norm after: 13.533644992975368
Epoch 6761/10000, Prediction Accuracy = 63.992000000000004%, Loss = 0.31548786759376524
Epoch: 6761, Batch Gradient Norm: 14.697740372808745
Epoch: 6761, Batch Gradient Norm after: 14.697740372808745
Epoch 6762/10000, Prediction Accuracy = 64.00800000000001%, Loss = 0.31574195623397827
Epoch: 6762, Batch Gradient Norm: 14.468664491691388
Epoch: 6762, Batch Gradient Norm after: 14.468664491691388
Epoch 6763/10000, Prediction Accuracy = 64.03599999999999%, Loss = 0.3157746493816376
Epoch: 6763, Batch Gradient Norm: 14.448669803909159
Epoch: 6763, Batch Gradient Norm after: 14.448669803909159
Epoch 6764/10000, Prediction Accuracy = 64.058%, Loss = 0.3146837055683136
Epoch: 6764, Batch Gradient Norm: 16.14935278974669
Epoch: 6764, Batch Gradient Norm after: 16.14935278974669
Epoch 6765/10000, Prediction Accuracy = 64.104%, Loss = 0.3172275424003601
Epoch: 6765, Batch Gradient Norm: 16.646102144528346
Epoch: 6765, Batch Gradient Norm after: 16.646102144528346
Epoch 6766/10000, Prediction Accuracy = 63.976%, Loss = 0.31812999248504636
Epoch: 6766, Batch Gradient Norm: 14.008441996567067
Epoch: 6766, Batch Gradient Norm after: 14.008441996567067
Epoch 6767/10000, Prediction Accuracy = 63.967999999999996%, Loss = 0.31771364212036135
Epoch: 6767, Batch Gradient Norm: 13.122891421939068
Epoch: 6767, Batch Gradient Norm after: 13.122891421939068
Epoch 6768/10000, Prediction Accuracy = 64.05000000000001%, Loss = 0.3151650071144104
Epoch: 6768, Batch Gradient Norm: 14.752452567562141
Epoch: 6768, Batch Gradient Norm after: 14.752452567562141
Epoch 6769/10000, Prediction Accuracy = 63.962%, Loss = 0.3189281940460205
Epoch: 6769, Batch Gradient Norm: 14.00713701386684
Epoch: 6769, Batch Gradient Norm after: 14.00713701386684
Epoch 6770/10000, Prediction Accuracy = 64.118%, Loss = 0.3156454384326935
Epoch: 6770, Batch Gradient Norm: 15.087275185555079
Epoch: 6770, Batch Gradient Norm after: 15.087275185555079
Epoch 6771/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3188654124736786
Epoch: 6771, Batch Gradient Norm: 17.06128847362302
Epoch: 6771, Batch Gradient Norm after: 17.01672072651648
Epoch 6772/10000, Prediction Accuracy = 64.168%, Loss = 0.32161001563072206
Epoch: 6772, Batch Gradient Norm: 16.736187160732122
Epoch: 6772, Batch Gradient Norm after: 16.736187160732122
Epoch 6773/10000, Prediction Accuracy = 63.989999999999995%, Loss = 0.3189762711524963
Epoch: 6773, Batch Gradient Norm: 15.34598293305521
Epoch: 6773, Batch Gradient Norm after: 15.34598293305521
Epoch 6774/10000, Prediction Accuracy = 64.07399999999998%, Loss = 0.31747748255729674
Epoch: 6774, Batch Gradient Norm: 16.778486830907173
Epoch: 6774, Batch Gradient Norm after: 16.778486830907173
Epoch 6775/10000, Prediction Accuracy = 63.99000000000001%, Loss = 0.321282285451889
Epoch: 6775, Batch Gradient Norm: 19.088499898176767
Epoch: 6775, Batch Gradient Norm after: 19.088499898176767
Epoch 6776/10000, Prediction Accuracy = 63.964%, Loss = 0.32331610918045045
Epoch: 6776, Batch Gradient Norm: 18.34198429984768
Epoch: 6776, Batch Gradient Norm after: 18.34198429984768
Epoch 6777/10000, Prediction Accuracy = 63.944%, Loss = 0.32188568115234373
Epoch: 6777, Batch Gradient Norm: 15.074288794043811
Epoch: 6777, Batch Gradient Norm after: 15.074288794043811
Epoch 6778/10000, Prediction Accuracy = 64.078%, Loss = 0.3177244246006012
Epoch: 6778, Batch Gradient Norm: 14.656668674077743
Epoch: 6778, Batch Gradient Norm after: 14.656668674077743
Epoch 6779/10000, Prediction Accuracy = 64.078%, Loss = 0.3167499125003815
Epoch: 6779, Batch Gradient Norm: 16.35974177733571
Epoch: 6779, Batch Gradient Norm after: 16.35974177733571
Epoch 6780/10000, Prediction Accuracy = 64.078%, Loss = 0.3179528295993805
Epoch: 6780, Batch Gradient Norm: 14.317882264386565
Epoch: 6780, Batch Gradient Norm after: 14.317882264386565
Epoch 6781/10000, Prediction Accuracy = 63.912%, Loss = 0.3176377356052399
Epoch: 6781, Batch Gradient Norm: 16.87225174057014
Epoch: 6781, Batch Gradient Norm after: 16.87225174057014
Epoch 6782/10000, Prediction Accuracy = 64.018%, Loss = 0.32025210857391356
Epoch: 6782, Batch Gradient Norm: 18.298572658494145
Epoch: 6782, Batch Gradient Norm after: 17.014634586529205
Epoch 6783/10000, Prediction Accuracy = 63.992000000000004%, Loss = 0.3206102252006531
Epoch: 6783, Batch Gradient Norm: 14.307099819578914
Epoch: 6783, Batch Gradient Norm after: 14.307099819578914
Epoch 6784/10000, Prediction Accuracy = 64.06199999999998%, Loss = 0.31650314331054685
Epoch: 6784, Batch Gradient Norm: 11.288050784184065
Epoch: 6784, Batch Gradient Norm after: 11.288050784184065
Epoch 6785/10000, Prediction Accuracy = 64.026%, Loss = 0.3111242651939392
Epoch: 6785, Batch Gradient Norm: 11.880779048918775
Epoch: 6785, Batch Gradient Norm after: 11.880779048918775
Epoch 6786/10000, Prediction Accuracy = 64.03599999999999%, Loss = 0.3118138492107391
Epoch: 6786, Batch Gradient Norm: 12.835568705224308
Epoch: 6786, Batch Gradient Norm after: 12.835568705224308
Epoch 6787/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.31313309669494627
Epoch: 6787, Batch Gradient Norm: 10.936359591867836
Epoch: 6787, Batch Gradient Norm after: 10.936359591867836
Epoch 6788/10000, Prediction Accuracy = 64.12%, Loss = 0.31046581268310547
Epoch: 6788, Batch Gradient Norm: 12.522860517203373
Epoch: 6788, Batch Gradient Norm after: 12.522860517203373
Epoch 6789/10000, Prediction Accuracy = 64.03%, Loss = 0.3143846333026886
Epoch: 6789, Batch Gradient Norm: 12.575714129922488
Epoch: 6789, Batch Gradient Norm after: 12.575714129922488
Epoch 6790/10000, Prediction Accuracy = 64.132%, Loss = 0.31405912041664125
Epoch: 6790, Batch Gradient Norm: 14.10888508810928
Epoch: 6790, Batch Gradient Norm after: 14.10888508810928
Epoch 6791/10000, Prediction Accuracy = 63.988%, Loss = 0.31493879556655885
Epoch: 6791, Batch Gradient Norm: 15.16947030055262
Epoch: 6791, Batch Gradient Norm after: 15.16947030055262
Epoch 6792/10000, Prediction Accuracy = 63.998000000000005%, Loss = 0.3165163159370422
Epoch: 6792, Batch Gradient Norm: 14.401560474695971
Epoch: 6792, Batch Gradient Norm after: 14.401560474695971
Epoch 6793/10000, Prediction Accuracy = 63.964%, Loss = 0.3138220489025116
Epoch: 6793, Batch Gradient Norm: 14.838458495607803
Epoch: 6793, Batch Gradient Norm after: 14.838458495607803
Epoch 6794/10000, Prediction Accuracy = 63.96999999999999%, Loss = 0.3166507065296173
Epoch: 6794, Batch Gradient Norm: 12.226044574420133
Epoch: 6794, Batch Gradient Norm after: 12.226044574420133
Epoch 6795/10000, Prediction Accuracy = 64.098%, Loss = 0.31317869424819944
Epoch: 6795, Batch Gradient Norm: 10.582362455329383
Epoch: 6795, Batch Gradient Norm after: 10.582362455329383
Epoch 6796/10000, Prediction Accuracy = 64.038%, Loss = 0.3123686611652374
Epoch: 6796, Batch Gradient Norm: 12.685294133723296
Epoch: 6796, Batch Gradient Norm after: 12.685294133723296
Epoch 6797/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.3130798935890198
Epoch: 6797, Batch Gradient Norm: 11.334866709456286
Epoch: 6797, Batch Gradient Norm after: 11.334866709456286
Epoch 6798/10000, Prediction Accuracy = 64.08%, Loss = 0.31010028123855593
Epoch: 6798, Batch Gradient Norm: 13.299260439423021
Epoch: 6798, Batch Gradient Norm after: 13.299260439423021
Epoch 6799/10000, Prediction Accuracy = 64.076%, Loss = 0.31241453886032106
Epoch: 6799, Batch Gradient Norm: 18.393148212047393
Epoch: 6799, Batch Gradient Norm after: 18.393148212047393
Epoch 6800/10000, Prediction Accuracy = 63.948%, Loss = 0.3211636424064636
Epoch: 6800, Batch Gradient Norm: 20.356115265113818
Epoch: 6800, Batch Gradient Norm after: 19.166838383830864
Epoch 6801/10000, Prediction Accuracy = 64.026%, Loss = 0.32510963678359983
Epoch: 6801, Batch Gradient Norm: 18.2891156657447
Epoch: 6801, Batch Gradient Norm after: 18.2891156657447
Epoch 6802/10000, Prediction Accuracy = 64.09400000000001%, Loss = 0.32025503516197207
Epoch: 6802, Batch Gradient Norm: 15.73745755192306
Epoch: 6802, Batch Gradient Norm after: 15.73745755192306
Epoch 6803/10000, Prediction Accuracy = 63.980000000000004%, Loss = 0.31720834970474243
Epoch: 6803, Batch Gradient Norm: 15.979099658489782
Epoch: 6803, Batch Gradient Norm after: 15.979099658489782
Epoch 6804/10000, Prediction Accuracy = 63.9%, Loss = 0.31770884394645693
Epoch: 6804, Batch Gradient Norm: 17.503454720930343
Epoch: 6804, Batch Gradient Norm after: 17.503454720930343
Epoch 6805/10000, Prediction Accuracy = 63.99000000000001%, Loss = 0.3184610068798065
Epoch: 6805, Batch Gradient Norm: 16.566506835027884
Epoch: 6805, Batch Gradient Norm after: 16.566506835027884
Epoch 6806/10000, Prediction Accuracy = 64.046%, Loss = 0.3179370403289795
Epoch: 6806, Batch Gradient Norm: 20.146707708537807
Epoch: 6806, Batch Gradient Norm after: 20.055594192242243
Epoch 6807/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.324978107213974
Epoch: 6807, Batch Gradient Norm: 17.46310382429564
Epoch: 6807, Batch Gradient Norm after: 17.46310382429564
Epoch 6808/10000, Prediction Accuracy = 63.96%, Loss = 0.3166976869106293
Epoch: 6808, Batch Gradient Norm: 17.641991177250734
Epoch: 6808, Batch Gradient Norm after: 17.641991177250734
Epoch 6809/10000, Prediction Accuracy = 64.01%, Loss = 0.3187072157859802
Epoch: 6809, Batch Gradient Norm: 17.147326608284896
Epoch: 6809, Batch Gradient Norm after: 16.931991585673597
Epoch 6810/10000, Prediction Accuracy = 64.00399999999999%, Loss = 0.318364417552948
Epoch: 6810, Batch Gradient Norm: 15.19728259191591
Epoch: 6810, Batch Gradient Norm after: 15.19728259191591
Epoch 6811/10000, Prediction Accuracy = 64.088%, Loss = 0.31722918152809143
Epoch: 6811, Batch Gradient Norm: 15.535685853592344
Epoch: 6811, Batch Gradient Norm after: 15.535685853592344
Epoch 6812/10000, Prediction Accuracy = 63.943999999999996%, Loss = 0.3168393552303314
Epoch: 6812, Batch Gradient Norm: 16.887469543600467
Epoch: 6812, Batch Gradient Norm after: 16.887469543600467
Epoch 6813/10000, Prediction Accuracy = 63.96199999999999%, Loss = 0.3223512589931488
Epoch: 6813, Batch Gradient Norm: 12.169582743050603
Epoch: 6813, Batch Gradient Norm after: 12.169582743050603
Epoch 6814/10000, Prediction Accuracy = 64.066%, Loss = 0.31508159041404726
Epoch: 6814, Batch Gradient Norm: 11.45377522468951
Epoch: 6814, Batch Gradient Norm after: 11.45377522468951
Epoch 6815/10000, Prediction Accuracy = 64.04799999999999%, Loss = 0.3120963335037231
Epoch: 6815, Batch Gradient Norm: 12.913503122710164
Epoch: 6815, Batch Gradient Norm after: 12.913503122710164
Epoch 6816/10000, Prediction Accuracy = 63.980000000000004%, Loss = 0.31574620604515075
Epoch: 6816, Batch Gradient Norm: 14.509734130042283
Epoch: 6816, Batch Gradient Norm after: 14.509734130042283
Epoch 6817/10000, Prediction Accuracy = 64.12199999999999%, Loss = 0.3167419731616974
Epoch: 6817, Batch Gradient Norm: 16.968734430479174
Epoch: 6817, Batch Gradient Norm after: 16.968734430479174
Epoch 6818/10000, Prediction Accuracy = 63.956%, Loss = 0.31874403953552244
Epoch: 6818, Batch Gradient Norm: 18.597836073153655
Epoch: 6818, Batch Gradient Norm after: 18.18430835841922
Epoch 6819/10000, Prediction Accuracy = 64.076%, Loss = 0.3211579382419586
Epoch: 6819, Batch Gradient Norm: 16.119139633762014
Epoch: 6819, Batch Gradient Norm after: 16.119139633762014
Epoch 6820/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.31879538893699644
Epoch: 6820, Batch Gradient Norm: 13.087777868348716
Epoch: 6820, Batch Gradient Norm after: 13.087777868348716
Epoch 6821/10000, Prediction Accuracy = 64.05999999999999%, Loss = 0.3173409640789032
Epoch: 6821, Batch Gradient Norm: 12.270670594771623
Epoch: 6821, Batch Gradient Norm after: 12.270670594771623
Epoch 6822/10000, Prediction Accuracy = 64.01%, Loss = 0.31417313814163206
Epoch: 6822, Batch Gradient Norm: 11.856074385993265
Epoch: 6822, Batch Gradient Norm after: 11.856074385993265
Epoch 6823/10000, Prediction Accuracy = 64.066%, Loss = 0.3122474193572998
Epoch: 6823, Batch Gradient Norm: 13.8194749295668
Epoch: 6823, Batch Gradient Norm after: 13.8194749295668
Epoch 6824/10000, Prediction Accuracy = 64.086%, Loss = 0.3149160504341125
Epoch: 6824, Batch Gradient Norm: 14.561498958614035
Epoch: 6824, Batch Gradient Norm after: 14.561498958614035
Epoch 6825/10000, Prediction Accuracy = 64.022%, Loss = 0.3154252767562866
Epoch: 6825, Batch Gradient Norm: 13.214466952237194
Epoch: 6825, Batch Gradient Norm after: 13.214466952237194
Epoch 6826/10000, Prediction Accuracy = 64.026%, Loss = 0.31387965083122255
Epoch: 6826, Batch Gradient Norm: 11.17731509046881
Epoch: 6826, Batch Gradient Norm after: 11.17731509046881
Epoch 6827/10000, Prediction Accuracy = 64.05199999999999%, Loss = 0.31342331767082215
Epoch: 6827, Batch Gradient Norm: 14.197139106804677
Epoch: 6827, Batch Gradient Norm after: 14.197139106804677
Epoch 6828/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.3140481114387512
Epoch: 6828, Batch Gradient Norm: 15.550733329203643
Epoch: 6828, Batch Gradient Norm after: 15.550733329203643
Epoch 6829/10000, Prediction Accuracy = 63.989999999999995%, Loss = 0.31736624240875244
Epoch: 6829, Batch Gradient Norm: 13.760458534590086
Epoch: 6829, Batch Gradient Norm after: 13.760458534590086
Epoch 6830/10000, Prediction Accuracy = 64.03%, Loss = 0.3149264931678772
Epoch: 6830, Batch Gradient Norm: 15.024949476826869
Epoch: 6830, Batch Gradient Norm after: 15.024949476826869
Epoch 6831/10000, Prediction Accuracy = 63.958000000000006%, Loss = 0.316476172208786
Epoch: 6831, Batch Gradient Norm: 16.457445113932163
Epoch: 6831, Batch Gradient Norm after: 16.457445113932163
Epoch 6832/10000, Prediction Accuracy = 63.968%, Loss = 0.31882768869400024
Epoch: 6832, Batch Gradient Norm: 14.76108337546103
Epoch: 6832, Batch Gradient Norm after: 14.76108337546103
Epoch 6833/10000, Prediction Accuracy = 64.122%, Loss = 0.3146696984767914
Epoch: 6833, Batch Gradient Norm: 15.01082545748844
Epoch: 6833, Batch Gradient Norm after: 15.01082545748844
Epoch 6834/10000, Prediction Accuracy = 64.08%, Loss = 0.314218807220459
Epoch: 6834, Batch Gradient Norm: 14.128407739493202
Epoch: 6834, Batch Gradient Norm after: 14.128407739493202
Epoch 6835/10000, Prediction Accuracy = 64.018%, Loss = 0.31457693576812745
Epoch: 6835, Batch Gradient Norm: 12.94283532981827
Epoch: 6835, Batch Gradient Norm after: 12.94283532981827
Epoch 6836/10000, Prediction Accuracy = 64.02000000000001%, Loss = 0.31364486217498777
Epoch: 6836, Batch Gradient Norm: 13.45562288639642
Epoch: 6836, Batch Gradient Norm after: 13.45562288639642
Epoch 6837/10000, Prediction Accuracy = 63.94%, Loss = 0.3121016561985016
Epoch: 6837, Batch Gradient Norm: 13.560010844178162
Epoch: 6837, Batch Gradient Norm after: 13.560010844178162
Epoch 6838/10000, Prediction Accuracy = 64.018%, Loss = 0.3137884736061096
Epoch: 6838, Batch Gradient Norm: 12.237467577995352
Epoch: 6838, Batch Gradient Norm after: 12.237467577995352
Epoch 6839/10000, Prediction Accuracy = 64.124%, Loss = 0.3132074952125549
Epoch: 6839, Batch Gradient Norm: 14.0928345642347
Epoch: 6839, Batch Gradient Norm after: 14.0928345642347
Epoch 6840/10000, Prediction Accuracy = 64.14000000000001%, Loss = 0.3129106104373932
Epoch: 6840, Batch Gradient Norm: 12.394956118212267
Epoch: 6840, Batch Gradient Norm after: 12.394956118212267
Epoch 6841/10000, Prediction Accuracy = 64.03799999999998%, Loss = 0.31090734004974363
Epoch: 6841, Batch Gradient Norm: 12.89923349613269
Epoch: 6841, Batch Gradient Norm after: 12.89923349613269
Epoch 6842/10000, Prediction Accuracy = 64.064%, Loss = 0.31494998931884766
Epoch: 6842, Batch Gradient Norm: 12.735917094876307
Epoch: 6842, Batch Gradient Norm after: 12.735917094876307
Epoch 6843/10000, Prediction Accuracy = 64.124%, Loss = 0.31218274831771853
Epoch: 6843, Batch Gradient Norm: 16.485528261504143
Epoch: 6843, Batch Gradient Norm after: 16.485528261504143
Epoch 6844/10000, Prediction Accuracy = 64.166%, Loss = 0.31726318001747134
Epoch: 6844, Batch Gradient Norm: 18.745091340406145
Epoch: 6844, Batch Gradient Norm after: 18.745091340406145
Epoch 6845/10000, Prediction Accuracy = 64.042%, Loss = 0.32198300361633303
Epoch: 6845, Batch Gradient Norm: 14.336137951482051
Epoch: 6845, Batch Gradient Norm after: 14.336137951482051
Epoch 6846/10000, Prediction Accuracy = 63.988%, Loss = 0.3156570553779602
Epoch: 6846, Batch Gradient Norm: 13.039453240487047
Epoch: 6846, Batch Gradient Norm after: 13.039453240487047
Epoch 6847/10000, Prediction Accuracy = 64.024%, Loss = 0.3116869628429413
Epoch: 6847, Batch Gradient Norm: 15.634705440235066
Epoch: 6847, Batch Gradient Norm after: 15.634705440235066
Epoch 6848/10000, Prediction Accuracy = 64.012%, Loss = 0.314538711309433
Epoch: 6848, Batch Gradient Norm: 15.09277913841775
Epoch: 6848, Batch Gradient Norm after: 15.09277913841775
Epoch 6849/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.3152665138244629
Epoch: 6849, Batch Gradient Norm: 16.128926232841316
Epoch: 6849, Batch Gradient Norm after: 16.128926232841316
Epoch 6850/10000, Prediction Accuracy = 64.118%, Loss = 0.3159094512462616
Epoch: 6850, Batch Gradient Norm: 18.58635482890345
Epoch: 6850, Batch Gradient Norm after: 18.315656592409418
Epoch 6851/10000, Prediction Accuracy = 64.152%, Loss = 0.3184590399265289
Epoch: 6851, Batch Gradient Norm: 16.585946547661162
Epoch: 6851, Batch Gradient Norm after: 16.585946547661162
Epoch 6852/10000, Prediction Accuracy = 64.036%, Loss = 0.3178101897239685
Epoch: 6852, Batch Gradient Norm: 13.845649700608254
Epoch: 6852, Batch Gradient Norm after: 13.845649700608254
Epoch 6853/10000, Prediction Accuracy = 64.092%, Loss = 0.31306531429290774
Epoch: 6853, Batch Gradient Norm: 15.079340928846156
Epoch: 6853, Batch Gradient Norm after: 15.079340928846156
Epoch 6854/10000, Prediction Accuracy = 64.01%, Loss = 0.31652971506118777
Epoch: 6854, Batch Gradient Norm: 13.84146172135357
Epoch: 6854, Batch Gradient Norm after: 13.84146172135357
Epoch 6855/10000, Prediction Accuracy = 64.008%, Loss = 0.3122620046138763
Epoch: 6855, Batch Gradient Norm: 13.830310274072312
Epoch: 6855, Batch Gradient Norm after: 13.830310274072312
Epoch 6856/10000, Prediction Accuracy = 63.964%, Loss = 0.314736407995224
Epoch: 6856, Batch Gradient Norm: 12.267402076307413
Epoch: 6856, Batch Gradient Norm after: 12.267402076307413
Epoch 6857/10000, Prediction Accuracy = 64.06%, Loss = 0.3108261704444885
Epoch: 6857, Batch Gradient Norm: 15.342515034792093
Epoch: 6857, Batch Gradient Norm after: 15.342515034792093
Epoch 6858/10000, Prediction Accuracy = 64.094%, Loss = 0.31616023778915403
Epoch: 6858, Batch Gradient Norm: 16.490632596841046
Epoch: 6858, Batch Gradient Norm after: 16.490632596841046
Epoch 6859/10000, Prediction Accuracy = 63.93399999999999%, Loss = 0.3188449203968048
Epoch: 6859, Batch Gradient Norm: 15.783425784360979
Epoch: 6859, Batch Gradient Norm after: 15.783425784360979
Epoch 6860/10000, Prediction Accuracy = 64.002%, Loss = 0.3191578686237335
Epoch: 6860, Batch Gradient Norm: 12.761359352963886
Epoch: 6860, Batch Gradient Norm after: 12.761359352963886
Epoch 6861/10000, Prediction Accuracy = 64.144%, Loss = 0.3124724268913269
Epoch: 6861, Batch Gradient Norm: 12.707876052460195
Epoch: 6861, Batch Gradient Norm after: 12.707876052460195
Epoch 6862/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.3131073832511902
Epoch: 6862, Batch Gradient Norm: 16.103904913400527
Epoch: 6862, Batch Gradient Norm after: 16.103904913400527
Epoch 6863/10000, Prediction Accuracy = 63.99400000000001%, Loss = 0.3167928814888
Epoch: 6863, Batch Gradient Norm: 14.381711068020131
Epoch: 6863, Batch Gradient Norm after: 14.381711068020131
Epoch 6864/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.3146877110004425
Epoch: 6864, Batch Gradient Norm: 15.272824105328068
Epoch: 6864, Batch Gradient Norm after: 15.272824105328068
Epoch 6865/10000, Prediction Accuracy = 64.078%, Loss = 0.3175443410873413
Epoch: 6865, Batch Gradient Norm: 18.495724391329944
Epoch: 6865, Batch Gradient Norm after: 17.625064275063526
Epoch 6866/10000, Prediction Accuracy = 64.076%, Loss = 0.32117793560028074
Epoch: 6866, Batch Gradient Norm: 17.687971583759563
Epoch: 6866, Batch Gradient Norm after: 17.687971583759563
Epoch 6867/10000, Prediction Accuracy = 64.112%, Loss = 0.3201237380504608
Epoch: 6867, Batch Gradient Norm: 15.997027996819629
Epoch: 6867, Batch Gradient Norm after: 15.997027996819629
Epoch 6868/10000, Prediction Accuracy = 64.13%, Loss = 0.3163039267063141
Epoch: 6868, Batch Gradient Norm: 17.14497482957297
Epoch: 6868, Batch Gradient Norm after: 17.14497482957297
Epoch 6869/10000, Prediction Accuracy = 63.95399999999999%, Loss = 0.31784710884094236
Epoch: 6869, Batch Gradient Norm: 13.321286749731842
Epoch: 6869, Batch Gradient Norm after: 13.321286749731842
Epoch 6870/10000, Prediction Accuracy = 64.10999999999999%, Loss = 0.31111855506896974
Epoch: 6870, Batch Gradient Norm: 14.588495007847687
Epoch: 6870, Batch Gradient Norm after: 14.588495007847687
Epoch 6871/10000, Prediction Accuracy = 64.17800000000001%, Loss = 0.31448503732681277
Epoch: 6871, Batch Gradient Norm: 13.306697733768722
Epoch: 6871, Batch Gradient Norm after: 13.306697733768722
Epoch 6872/10000, Prediction Accuracy = 64.04%, Loss = 0.31241256594657896
Epoch: 6872, Batch Gradient Norm: 17.032628436947896
Epoch: 6872, Batch Gradient Norm after: 16.31414090653308
Epoch 6873/10000, Prediction Accuracy = 64.102%, Loss = 0.3175917148590088
Epoch: 6873, Batch Gradient Norm: 13.732987656420004
Epoch: 6873, Batch Gradient Norm after: 13.732987656420004
Epoch 6874/10000, Prediction Accuracy = 63.992000000000004%, Loss = 0.31383088827133176
Epoch: 6874, Batch Gradient Norm: 17.682105816551925
Epoch: 6874, Batch Gradient Norm after: 17.58109081331688
Epoch 6875/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.31868704557418825
Epoch: 6875, Batch Gradient Norm: 16.973207074970485
Epoch: 6875, Batch Gradient Norm after: 16.973207074970485
Epoch 6876/10000, Prediction Accuracy = 64.13199999999999%, Loss = 0.31529021859169004
Epoch: 6876, Batch Gradient Norm: 14.561979672724638
Epoch: 6876, Batch Gradient Norm after: 14.561979672724638
Epoch 6877/10000, Prediction Accuracy = 63.95%, Loss = 0.31463799476623533
Epoch: 6877, Batch Gradient Norm: 12.497870645631302
Epoch: 6877, Batch Gradient Norm after: 12.497870645631302
Epoch 6878/10000, Prediction Accuracy = 64.248%, Loss = 0.3140498101711273
Epoch: 6878, Batch Gradient Norm: 12.78267601714299
Epoch: 6878, Batch Gradient Norm after: 12.78267601714299
Epoch 6879/10000, Prediction Accuracy = 64.024%, Loss = 0.31136436462402345
Epoch: 6879, Batch Gradient Norm: 12.694322472937474
Epoch: 6879, Batch Gradient Norm after: 12.694322472937474
Epoch 6880/10000, Prediction Accuracy = 64.0%, Loss = 0.3116262793540955
Epoch: 6880, Batch Gradient Norm: 16.025702360038437
Epoch: 6880, Batch Gradient Norm after: 16.025702360038437
Epoch 6881/10000, Prediction Accuracy = 64.118%, Loss = 0.3151834547519684
Epoch: 6881, Batch Gradient Norm: 17.00785840584771
Epoch: 6881, Batch Gradient Norm after: 16.809369378504787
Epoch 6882/10000, Prediction Accuracy = 64.05%, Loss = 0.3163583278656006
Epoch: 6882, Batch Gradient Norm: 13.608573061811454
Epoch: 6882, Batch Gradient Norm after: 13.608573061811454
Epoch 6883/10000, Prediction Accuracy = 64.06%, Loss = 0.3132450580596924
Epoch: 6883, Batch Gradient Norm: 12.939346712350554
Epoch: 6883, Batch Gradient Norm after: 12.939346712350554
Epoch 6884/10000, Prediction Accuracy = 64.08200000000001%, Loss = 0.3117103576660156
Epoch: 6884, Batch Gradient Norm: 12.83463242575638
Epoch: 6884, Batch Gradient Norm after: 12.83463242575638
Epoch 6885/10000, Prediction Accuracy = 64.23000000000002%, Loss = 0.3105994284152985
Epoch: 6885, Batch Gradient Norm: 13.466905853785427
Epoch: 6885, Batch Gradient Norm after: 13.466905853785427
Epoch 6886/10000, Prediction Accuracy = 64.162%, Loss = 0.31110026240348815
Epoch: 6886, Batch Gradient Norm: 14.1840438809017
Epoch: 6886, Batch Gradient Norm after: 14.1840438809017
Epoch 6887/10000, Prediction Accuracy = 64.152%, Loss = 0.31673166155815125
Epoch: 6887, Batch Gradient Norm: 12.620277164781996
Epoch: 6887, Batch Gradient Norm after: 12.620277164781996
Epoch 6888/10000, Prediction Accuracy = 64.0%, Loss = 0.3113778293132782
Epoch: 6888, Batch Gradient Norm: 14.150980642998913
Epoch: 6888, Batch Gradient Norm after: 14.150980642998913
Epoch 6889/10000, Prediction Accuracy = 64.058%, Loss = 0.31217291951179504
Epoch: 6889, Batch Gradient Norm: 12.397214256021337
Epoch: 6889, Batch Gradient Norm after: 12.397214256021337
Epoch 6890/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3111248970031738
Epoch: 6890, Batch Gradient Norm: 13.31617364902272
Epoch: 6890, Batch Gradient Norm after: 13.31617364902272
Epoch 6891/10000, Prediction Accuracy = 64.032%, Loss = 0.3117960631847382
Epoch: 6891, Batch Gradient Norm: 15.010488908830956
Epoch: 6891, Batch Gradient Norm after: 15.010488908830956
Epoch 6892/10000, Prediction Accuracy = 64.16%, Loss = 0.31415098905563354
Epoch: 6892, Batch Gradient Norm: 15.367475599107328
Epoch: 6892, Batch Gradient Norm after: 15.367475599107328
Epoch 6893/10000, Prediction Accuracy = 64.102%, Loss = 0.31678621768951415
Epoch: 6893, Batch Gradient Norm: 17.14447714374474
Epoch: 6893, Batch Gradient Norm after: 17.14447714374474
Epoch 6894/10000, Prediction Accuracy = 64.018%, Loss = 0.317803692817688
Epoch: 6894, Batch Gradient Norm: 15.543425680956641
Epoch: 6894, Batch Gradient Norm after: 15.543425680956641
Epoch 6895/10000, Prediction Accuracy = 64.072%, Loss = 0.3179750978946686
Epoch: 6895, Batch Gradient Norm: 14.311681285848598
Epoch: 6895, Batch Gradient Norm after: 14.311681285848598
Epoch 6896/10000, Prediction Accuracy = 64.034%, Loss = 0.31225486993789675
Epoch: 6896, Batch Gradient Norm: 13.444628048714176
Epoch: 6896, Batch Gradient Norm after: 13.444628048714176
Epoch 6897/10000, Prediction Accuracy = 63.902%, Loss = 0.3142510652542114
Epoch: 6897, Batch Gradient Norm: 15.524351292384505
Epoch: 6897, Batch Gradient Norm after: 15.524351292384505
Epoch 6898/10000, Prediction Accuracy = 64.054%, Loss = 0.3148542582988739
Epoch: 6898, Batch Gradient Norm: 13.212910549364361
Epoch: 6898, Batch Gradient Norm after: 13.212910549364361
Epoch 6899/10000, Prediction Accuracy = 64.088%, Loss = 0.31305673718452454
Epoch: 6899, Batch Gradient Norm: 12.87353690300454
Epoch: 6899, Batch Gradient Norm after: 12.87353690300454
Epoch 6900/10000, Prediction Accuracy = 64.03%, Loss = 0.3112531125545502
Epoch: 6900, Batch Gradient Norm: 15.390165325349235
Epoch: 6900, Batch Gradient Norm after: 15.390165325349235
Epoch 6901/10000, Prediction Accuracy = 64.168%, Loss = 0.3137857377529144
Epoch: 6901, Batch Gradient Norm: 13.227013519470765
Epoch: 6901, Batch Gradient Norm after: 13.227013519470765
Epoch 6902/10000, Prediction Accuracy = 64.146%, Loss = 0.3103921890258789
Epoch: 6902, Batch Gradient Norm: 13.878853522359822
Epoch: 6902, Batch Gradient Norm after: 13.878853522359822
Epoch 6903/10000, Prediction Accuracy = 64.06%, Loss = 0.3106774866580963
Epoch: 6903, Batch Gradient Norm: 13.057968711447824
Epoch: 6903, Batch Gradient Norm after: 13.057968711447824
Epoch 6904/10000, Prediction Accuracy = 64.014%, Loss = 0.31397001147270204
Epoch: 6904, Batch Gradient Norm: 14.07560455589585
Epoch: 6904, Batch Gradient Norm after: 14.07560455589585
Epoch 6905/10000, Prediction Accuracy = 64.186%, Loss = 0.31472432017326357
Epoch: 6905, Batch Gradient Norm: 13.023374744241773
Epoch: 6905, Batch Gradient Norm after: 13.023374744241773
Epoch 6906/10000, Prediction Accuracy = 64.11%, Loss = 0.3110381603240967
Epoch: 6906, Batch Gradient Norm: 12.948244259446689
Epoch: 6906, Batch Gradient Norm after: 12.948244259446689
Epoch 6907/10000, Prediction Accuracy = 64.06000000000002%, Loss = 0.311098313331604
Epoch: 6907, Batch Gradient Norm: 10.96294931414618
Epoch: 6907, Batch Gradient Norm after: 10.96294931414618
Epoch 6908/10000, Prediction Accuracy = 64.10600000000001%, Loss = 0.30779512524604796
Epoch: 6908, Batch Gradient Norm: 13.981041313111218
Epoch: 6908, Batch Gradient Norm after: 13.981041313111218
Epoch 6909/10000, Prediction Accuracy = 64.216%, Loss = 0.3118412017822266
Epoch: 6909, Batch Gradient Norm: 12.977062121572233
Epoch: 6909, Batch Gradient Norm after: 12.977062121572233
Epoch 6910/10000, Prediction Accuracy = 64.038%, Loss = 0.31046183705329894
Epoch: 6910, Batch Gradient Norm: 14.105407447975121
Epoch: 6910, Batch Gradient Norm after: 14.105407447975121
Epoch 6911/10000, Prediction Accuracy = 64.062%, Loss = 0.3150111734867096
Epoch: 6911, Batch Gradient Norm: 13.380515679103523
Epoch: 6911, Batch Gradient Norm after: 13.380515679103523
Epoch 6912/10000, Prediction Accuracy = 64.092%, Loss = 0.3131799876689911
Epoch: 6912, Batch Gradient Norm: 13.783372115296018
Epoch: 6912, Batch Gradient Norm after: 13.783372115296018
Epoch 6913/10000, Prediction Accuracy = 64.162%, Loss = 0.312682169675827
Epoch: 6913, Batch Gradient Norm: 14.583997075610466
Epoch: 6913, Batch Gradient Norm after: 14.583997075610466
Epoch 6914/10000, Prediction Accuracy = 64.114%, Loss = 0.31273452043533323
Epoch: 6914, Batch Gradient Norm: 16.476381182300454
Epoch: 6914, Batch Gradient Norm after: 16.476381182300454
Epoch 6915/10000, Prediction Accuracy = 64.06%, Loss = 0.3186695516109467
Epoch: 6915, Batch Gradient Norm: 13.834163786836887
Epoch: 6915, Batch Gradient Norm after: 13.834163786836887
Epoch 6916/10000, Prediction Accuracy = 64.114%, Loss = 0.313422691822052
Epoch: 6916, Batch Gradient Norm: 14.882405838795284
Epoch: 6916, Batch Gradient Norm after: 14.882405838795284
Epoch 6917/10000, Prediction Accuracy = 64.16400000000002%, Loss = 0.3186161041259766
Epoch: 6917, Batch Gradient Norm: 15.044186583566175
Epoch: 6917, Batch Gradient Norm after: 15.044186583566175
Epoch 6918/10000, Prediction Accuracy = 64.072%, Loss = 0.3126079201698303
Epoch: 6918, Batch Gradient Norm: 14.803702206099665
Epoch: 6918, Batch Gradient Norm after: 14.803702206099665
Epoch 6919/10000, Prediction Accuracy = 64.11600000000001%, Loss = 0.3118276000022888
Epoch: 6919, Batch Gradient Norm: 16.838599491414197
Epoch: 6919, Batch Gradient Norm after: 16.838599491414197
Epoch 6920/10000, Prediction Accuracy = 64.092%, Loss = 0.3165605068206787
Epoch: 6920, Batch Gradient Norm: 17.23802959152544
Epoch: 6920, Batch Gradient Norm after: 17.23802959152544
Epoch 6921/10000, Prediction Accuracy = 64.07000000000001%, Loss = 0.31787593960762023
Epoch: 6921, Batch Gradient Norm: 19.008504917071633
Epoch: 6921, Batch Gradient Norm after: 18.958849247037644
Epoch 6922/10000, Prediction Accuracy = 64.136%, Loss = 0.31912313103675843
Epoch: 6922, Batch Gradient Norm: 16.800688395388924
Epoch: 6922, Batch Gradient Norm after: 16.800688395388924
Epoch 6923/10000, Prediction Accuracy = 63.975999999999985%, Loss = 0.31706246733665466
Epoch: 6923, Batch Gradient Norm: 18.483584730331707
Epoch: 6923, Batch Gradient Norm after: 18.010990155131296
Epoch 6924/10000, Prediction Accuracy = 64.03799999999998%, Loss = 0.3213637053966522
Epoch: 6924, Batch Gradient Norm: 13.336606937352698
Epoch: 6924, Batch Gradient Norm after: 13.336606937352698
Epoch 6925/10000, Prediction Accuracy = 64.11200000000001%, Loss = 0.3119045555591583
Epoch: 6925, Batch Gradient Norm: 13.129290978894854
Epoch: 6925, Batch Gradient Norm after: 13.129290978894854
Epoch 6926/10000, Prediction Accuracy = 63.89%, Loss = 0.31411474347114565
Epoch: 6926, Batch Gradient Norm: 12.07556336819318
Epoch: 6926, Batch Gradient Norm after: 12.07556336819318
Epoch 6927/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3122107207775116
Epoch: 6927, Batch Gradient Norm: 11.341321952759323
Epoch: 6927, Batch Gradient Norm after: 11.341321952759323
Epoch 6928/10000, Prediction Accuracy = 63.96600000000001%, Loss = 0.31225480437278746
Epoch: 6928, Batch Gradient Norm: 13.635210413730597
Epoch: 6928, Batch Gradient Norm after: 13.635210413730597
Epoch 6929/10000, Prediction Accuracy = 63.955999999999996%, Loss = 0.31093682050704957
Epoch: 6929, Batch Gradient Norm: 12.489525349527785
Epoch: 6929, Batch Gradient Norm after: 12.489525349527785
Epoch 6930/10000, Prediction Accuracy = 64.12%, Loss = 0.3109234213829041
Epoch: 6930, Batch Gradient Norm: 10.513205958087573
Epoch: 6930, Batch Gradient Norm after: 10.513205958087573
Epoch 6931/10000, Prediction Accuracy = 64.15400000000001%, Loss = 0.30694485306739805
Epoch: 6931, Batch Gradient Norm: 12.99205430689786
Epoch: 6931, Batch Gradient Norm after: 12.99205430689786
Epoch 6932/10000, Prediction Accuracy = 64.076%, Loss = 0.31110711097717286
Epoch: 6932, Batch Gradient Norm: 11.665684029581357
Epoch: 6932, Batch Gradient Norm after: 11.665684029581357
Epoch 6933/10000, Prediction Accuracy = 64.0%, Loss = 0.31088306903839114
Epoch: 6933, Batch Gradient Norm: 12.509929760921075
Epoch: 6933, Batch Gradient Norm after: 12.509929760921075
Epoch 6934/10000, Prediction Accuracy = 64.152%, Loss = 0.30998033881187437
Epoch: 6934, Batch Gradient Norm: 12.174154478556382
Epoch: 6934, Batch Gradient Norm after: 12.174154478556382
Epoch 6935/10000, Prediction Accuracy = 64.084%, Loss = 0.3101172149181366
Epoch: 6935, Batch Gradient Norm: 15.678107486288715
Epoch: 6935, Batch Gradient Norm after: 15.678107486288715
Epoch 6936/10000, Prediction Accuracy = 64.00399999999999%, Loss = 0.3153091073036194
Epoch: 6936, Batch Gradient Norm: 13.482588318479673
Epoch: 6936, Batch Gradient Norm after: 13.482588318479673
Epoch 6937/10000, Prediction Accuracy = 64.076%, Loss = 0.31208503246307373
Epoch: 6937, Batch Gradient Norm: 10.466073183965085
Epoch: 6937, Batch Gradient Norm after: 10.466073183965085
Epoch 6938/10000, Prediction Accuracy = 64.23999999999998%, Loss = 0.30728754997253416
Epoch: 6938, Batch Gradient Norm: 13.176775768745625
Epoch: 6938, Batch Gradient Norm after: 13.176775768745625
Epoch 6939/10000, Prediction Accuracy = 64.088%, Loss = 0.3108354151248932
Epoch: 6939, Batch Gradient Norm: 12.333890384375088
Epoch: 6939, Batch Gradient Norm after: 12.333890384375088
Epoch 6940/10000, Prediction Accuracy = 64.08000000000001%, Loss = 0.31254156231880187
Epoch: 6940, Batch Gradient Norm: 11.283602075062804
Epoch: 6940, Batch Gradient Norm after: 11.283602075062804
Epoch 6941/10000, Prediction Accuracy = 64.056%, Loss = 0.30870879888534547
Epoch: 6941, Batch Gradient Norm: 10.638798491661776
Epoch: 6941, Batch Gradient Norm after: 10.638798491661776
Epoch 6942/10000, Prediction Accuracy = 64.074%, Loss = 0.30786666870117185
Epoch: 6942, Batch Gradient Norm: 12.635084387836152
Epoch: 6942, Batch Gradient Norm after: 12.635084387836152
Epoch 6943/10000, Prediction Accuracy = 64.13199999999999%, Loss = 0.3122914135456085
Epoch: 6943, Batch Gradient Norm: 14.229279441887165
Epoch: 6943, Batch Gradient Norm after: 14.229279441887165
Epoch 6944/10000, Prediction Accuracy = 64.122%, Loss = 0.3135696887969971
Epoch: 6944, Batch Gradient Norm: 11.04377153214972
Epoch: 6944, Batch Gradient Norm after: 11.04377153214972
Epoch 6945/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.31037868857383727
Epoch: 6945, Batch Gradient Norm: 10.866511995263497
Epoch: 6945, Batch Gradient Norm after: 10.866511995263497
Epoch 6946/10000, Prediction Accuracy = 64.114%, Loss = 0.3089490532875061
Epoch: 6946, Batch Gradient Norm: 14.281813517530702
Epoch: 6946, Batch Gradient Norm after: 14.281813517530702
Epoch 6947/10000, Prediction Accuracy = 64.088%, Loss = 0.3123739302158356
Epoch: 6947, Batch Gradient Norm: 12.937744212085441
Epoch: 6947, Batch Gradient Norm after: 12.937744212085441
Epoch 6948/10000, Prediction Accuracy = 64.148%, Loss = 0.3099956214427948
Epoch: 6948, Batch Gradient Norm: 13.559375273141658
Epoch: 6948, Batch Gradient Norm after: 13.559375273141658
Epoch 6949/10000, Prediction Accuracy = 64.14399999999999%, Loss = 0.3097478151321411
Epoch: 6949, Batch Gradient Norm: 12.427498204951103
Epoch: 6949, Batch Gradient Norm after: 12.427498204951103
Epoch 6950/10000, Prediction Accuracy = 64.00999999999999%, Loss = 0.31108996272087097
Epoch: 6950, Batch Gradient Norm: 12.989695931462004
Epoch: 6950, Batch Gradient Norm after: 12.989695931462004
Epoch 6951/10000, Prediction Accuracy = 64.048%, Loss = 0.3109308540821075
Epoch: 6951, Batch Gradient Norm: 11.526230154477336
Epoch: 6951, Batch Gradient Norm after: 11.526230154477336
Epoch 6952/10000, Prediction Accuracy = 64.042%, Loss = 0.3106747031211853
Epoch: 6952, Batch Gradient Norm: 13.227834228978383
Epoch: 6952, Batch Gradient Norm after: 13.227834228978383
Epoch 6953/10000, Prediction Accuracy = 63.988%, Loss = 0.30982065200805664
Epoch: 6953, Batch Gradient Norm: 15.154177382383285
Epoch: 6953, Batch Gradient Norm after: 15.154177382383285
Epoch 6954/10000, Prediction Accuracy = 64.138%, Loss = 0.31258043050765993
Epoch: 6954, Batch Gradient Norm: 13.684233451591053
Epoch: 6954, Batch Gradient Norm after: 13.684233451591053
Epoch 6955/10000, Prediction Accuracy = 63.948%, Loss = 0.31257343888282774
Epoch: 6955, Batch Gradient Norm: 11.44806502976546
Epoch: 6955, Batch Gradient Norm after: 11.44806502976546
Epoch 6956/10000, Prediction Accuracy = 64.054%, Loss = 0.3093407154083252
Epoch: 6956, Batch Gradient Norm: 12.389154921665467
Epoch: 6956, Batch Gradient Norm after: 12.389154921665467
Epoch 6957/10000, Prediction Accuracy = 64.16999999999999%, Loss = 0.3101311981678009
Epoch: 6957, Batch Gradient Norm: 11.834951671233137
Epoch: 6957, Batch Gradient Norm after: 11.834951671233137
Epoch 6958/10000, Prediction Accuracy = 64.17999999999999%, Loss = 0.3085304319858551
Epoch: 6958, Batch Gradient Norm: 13.72716435811606
Epoch: 6958, Batch Gradient Norm after: 13.72716435811606
Epoch 6959/10000, Prediction Accuracy = 64.098%, Loss = 0.31183982491493223
Epoch: 6959, Batch Gradient Norm: 10.513638861968422
Epoch: 6959, Batch Gradient Norm after: 10.513638861968422
Epoch 6960/10000, Prediction Accuracy = 63.958000000000006%, Loss = 0.3077832818031311
Epoch: 6960, Batch Gradient Norm: 14.091566978781723
Epoch: 6960, Batch Gradient Norm after: 14.091566978781723
Epoch 6961/10000, Prediction Accuracy = 64.142%, Loss = 0.31092530488967896
Epoch: 6961, Batch Gradient Norm: 10.582861421002056
Epoch: 6961, Batch Gradient Norm after: 10.582861421002056
Epoch 6962/10000, Prediction Accuracy = 64.124%, Loss = 0.3091427743434906
Epoch: 6962, Batch Gradient Norm: 10.811334621584045
Epoch: 6962, Batch Gradient Norm after: 10.811334621584045
Epoch 6963/10000, Prediction Accuracy = 64.13%, Loss = 0.30753999948501587
Epoch: 6963, Batch Gradient Norm: 13.5443022782244
Epoch: 6963, Batch Gradient Norm after: 13.5443022782244
Epoch 6964/10000, Prediction Accuracy = 64.18199999999999%, Loss = 0.309137761592865
Epoch: 6964, Batch Gradient Norm: 12.854398857561993
Epoch: 6964, Batch Gradient Norm after: 12.854398857561993
Epoch 6965/10000, Prediction Accuracy = 64.026%, Loss = 0.31112404465675353
Epoch: 6965, Batch Gradient Norm: 13.563530071457832
Epoch: 6965, Batch Gradient Norm after: 13.563530071457832
Epoch 6966/10000, Prediction Accuracy = 64.214%, Loss = 0.31055461764335635
Epoch: 6966, Batch Gradient Norm: 12.84159246162959
Epoch: 6966, Batch Gradient Norm after: 12.84159246162959
Epoch 6967/10000, Prediction Accuracy = 64.142%, Loss = 0.3098331391811371
Epoch: 6967, Batch Gradient Norm: 14.046722702208791
Epoch: 6967, Batch Gradient Norm after: 14.046722702208791
Epoch 6968/10000, Prediction Accuracy = 64.158%, Loss = 0.31173508167266845
Epoch: 6968, Batch Gradient Norm: 14.485537782409882
Epoch: 6968, Batch Gradient Norm after: 14.485537782409882
Epoch 6969/10000, Prediction Accuracy = 64.116%, Loss = 0.3131076157093048
Epoch: 6969, Batch Gradient Norm: 12.671449641502276
Epoch: 6969, Batch Gradient Norm after: 12.671449641502276
Epoch 6970/10000, Prediction Accuracy = 64.168%, Loss = 0.30887422561645506
Epoch: 6970, Batch Gradient Norm: 13.147389940508349
Epoch: 6970, Batch Gradient Norm after: 13.147389940508349
Epoch 6971/10000, Prediction Accuracy = 63.983999999999995%, Loss = 0.3132322788238525
Epoch: 6971, Batch Gradient Norm: 14.137685538454543
Epoch: 6971, Batch Gradient Norm after: 14.137685538454543
Epoch 6972/10000, Prediction Accuracy = 64.09400000000001%, Loss = 0.3128617823123932
Epoch: 6972, Batch Gradient Norm: 18.184426840373025
Epoch: 6972, Batch Gradient Norm after: 18.184426840373025
Epoch 6973/10000, Prediction Accuracy = 64.09200000000001%, Loss = 0.3177303671836853
Epoch: 6973, Batch Gradient Norm: 15.100162191952203
Epoch: 6973, Batch Gradient Norm after: 15.100162191952203
Epoch 6974/10000, Prediction Accuracy = 64.064%, Loss = 0.31358407735824584
Epoch: 6974, Batch Gradient Norm: 14.850148313517924
Epoch: 6974, Batch Gradient Norm after: 14.850148313517924
Epoch 6975/10000, Prediction Accuracy = 64.17399999999999%, Loss = 0.312363064289093
Epoch: 6975, Batch Gradient Norm: 13.832246434403647
Epoch: 6975, Batch Gradient Norm after: 13.832246434403647
Epoch 6976/10000, Prediction Accuracy = 63.907999999999994%, Loss = 0.31211656928062437
Epoch: 6976, Batch Gradient Norm: 13.201078492293552
Epoch: 6976, Batch Gradient Norm after: 13.201078492293552
Epoch 6977/10000, Prediction Accuracy = 64.296%, Loss = 0.31009989976882935
Epoch: 6977, Batch Gradient Norm: 11.947830879600257
Epoch: 6977, Batch Gradient Norm after: 11.947830879600257
Epoch 6978/10000, Prediction Accuracy = 64.03999999999999%, Loss = 0.31243237257003786
Epoch: 6978, Batch Gradient Norm: 13.763435417599405
Epoch: 6978, Batch Gradient Norm after: 13.763435417599405
Epoch 6979/10000, Prediction Accuracy = 64.078%, Loss = 0.3131893515586853
Epoch: 6979, Batch Gradient Norm: 16.49197267424685
Epoch: 6979, Batch Gradient Norm after: 16.49197267424685
Epoch 6980/10000, Prediction Accuracy = 64.076%, Loss = 0.31454861760139463
Epoch: 6980, Batch Gradient Norm: 17.7366308279581
Epoch: 6980, Batch Gradient Norm after: 17.7366308279581
Epoch 6981/10000, Prediction Accuracy = 64.20199999999998%, Loss = 0.31663464307785033
Epoch: 6981, Batch Gradient Norm: 12.63678259891938
Epoch: 6981, Batch Gradient Norm after: 12.63678259891938
Epoch 6982/10000, Prediction Accuracy = 64.148%, Loss = 0.3109819948673248
Epoch: 6982, Batch Gradient Norm: 12.717117446292448
Epoch: 6982, Batch Gradient Norm after: 12.717117446292448
Epoch 6983/10000, Prediction Accuracy = 64.142%, Loss = 0.3085430920124054
Epoch: 6983, Batch Gradient Norm: 13.66466823804119
Epoch: 6983, Batch Gradient Norm after: 13.66466823804119
Epoch 6984/10000, Prediction Accuracy = 64.05%, Loss = 0.3113065481185913
Epoch: 6984, Batch Gradient Norm: 11.242922623780869
Epoch: 6984, Batch Gradient Norm after: 11.242922623780869
Epoch 6985/10000, Prediction Accuracy = 64.23800000000001%, Loss = 0.30773202180862425
Epoch: 6985, Batch Gradient Norm: 10.863743681995935
Epoch: 6985, Batch Gradient Norm after: 10.863743681995935
Epoch 6986/10000, Prediction Accuracy = 64.136%, Loss = 0.30667443871498107
Epoch: 6986, Batch Gradient Norm: 12.413371322623835
Epoch: 6986, Batch Gradient Norm after: 12.413371322623835
Epoch 6987/10000, Prediction Accuracy = 64.114%, Loss = 0.31187222599983216
Epoch: 6987, Batch Gradient Norm: 14.111376468350063
Epoch: 6987, Batch Gradient Norm after: 14.111376468350063
Epoch 6988/10000, Prediction Accuracy = 64.08200000000001%, Loss = 0.3140270411968231
Epoch: 6988, Batch Gradient Norm: 14.49024236034969
Epoch: 6988, Batch Gradient Norm after: 14.49024236034969
Epoch 6989/10000, Prediction Accuracy = 64.078%, Loss = 0.31432282328605654
Epoch: 6989, Batch Gradient Norm: 12.970653287520962
Epoch: 6989, Batch Gradient Norm after: 12.970653287520962
Epoch 6990/10000, Prediction Accuracy = 64.13199999999999%, Loss = 0.3102890610694885
Epoch: 6990, Batch Gradient Norm: 15.802761147132745
Epoch: 6990, Batch Gradient Norm after: 15.802761147132745
Epoch 6991/10000, Prediction Accuracy = 64.152%, Loss = 0.31365776658058164
Epoch: 6991, Batch Gradient Norm: 17.884803901256827
Epoch: 6991, Batch Gradient Norm after: 17.884803901256827
Epoch 6992/10000, Prediction Accuracy = 63.96600000000001%, Loss = 0.31724236011505125
Epoch: 6992, Batch Gradient Norm: 18.82321201665126
Epoch: 6992, Batch Gradient Norm after: 18.29564830789157
Epoch 6993/10000, Prediction Accuracy = 63.99400000000001%, Loss = 0.32001867294311526
Epoch: 6993, Batch Gradient Norm: 18.315649730458833
Epoch: 6993, Batch Gradient Norm after: 18.315649730458833
Epoch 6994/10000, Prediction Accuracy = 64.30600000000001%, Loss = 0.3163922429084778
Epoch: 6994, Batch Gradient Norm: 16.104994610968234
Epoch: 6994, Batch Gradient Norm after: 16.104994610968234
Epoch 6995/10000, Prediction Accuracy = 64.024%, Loss = 0.31378166675567626
Epoch: 6995, Batch Gradient Norm: 16.604163383013905
Epoch: 6995, Batch Gradient Norm after: 16.604163383013905
Epoch 6996/10000, Prediction Accuracy = 64.208%, Loss = 0.3144024908542633
Epoch: 6996, Batch Gradient Norm: 13.125883992298938
Epoch: 6996, Batch Gradient Norm after: 13.125883992298938
Epoch 6997/10000, Prediction Accuracy = 64.116%, Loss = 0.31066684126853944
Epoch: 6997, Batch Gradient Norm: 13.714232504716385
Epoch: 6997, Batch Gradient Norm after: 13.714232504716385
Epoch 6998/10000, Prediction Accuracy = 64.148%, Loss = 0.3118156552314758
Epoch: 6998, Batch Gradient Norm: 11.258023283185516
Epoch: 6998, Batch Gradient Norm after: 11.258023283185516
Epoch 6999/10000, Prediction Accuracy = 64.018%, Loss = 0.30703004002571105
Epoch: 6999, Batch Gradient Norm: 10.387004194412864
Epoch: 6999, Batch Gradient Norm after: 10.387004194412864
Epoch 7000/10000, Prediction Accuracy = 64.11800000000001%, Loss = 0.30780845880508423
Epoch: 7000, Batch Gradient Norm: 11.106406487453908
Epoch: 7000, Batch Gradient Norm after: 11.106406487453908
Epoch 7001/10000, Prediction Accuracy = 64.142%, Loss = 0.3085052192211151
Epoch: 7001, Batch Gradient Norm: 12.084482546754334
Epoch: 7001, Batch Gradient Norm after: 12.084482546754334
Epoch 7002/10000, Prediction Accuracy = 64.308%, Loss = 0.30843361020088195
Epoch: 7002, Batch Gradient Norm: 14.629784063676606
Epoch: 7002, Batch Gradient Norm after: 14.629784063676606
Epoch 7003/10000, Prediction Accuracy = 64.138%, Loss = 0.3118306815624237
Epoch: 7003, Batch Gradient Norm: 15.830569652824913
Epoch: 7003, Batch Gradient Norm after: 15.830569652824913
Epoch 7004/10000, Prediction Accuracy = 64.16799999999999%, Loss = 0.3120876610279083
Epoch: 7004, Batch Gradient Norm: 21.188965179195815
Epoch: 7004, Batch Gradient Norm after: 18.869650539378465
Epoch 7005/10000, Prediction Accuracy = 64.14%, Loss = 0.32371230125427247
Epoch: 7005, Batch Gradient Norm: 20.14019059779287
Epoch: 7005, Batch Gradient Norm after: 18.527150414116882
Epoch 7006/10000, Prediction Accuracy = 63.986000000000004%, Loss = 0.32171587347984315
Epoch: 7006, Batch Gradient Norm: 16.83393945489176
Epoch: 7006, Batch Gradient Norm after: 16.83393945489176
Epoch 7007/10000, Prediction Accuracy = 64.17%, Loss = 0.31508262157440187
Epoch: 7007, Batch Gradient Norm: 14.694445930657428
Epoch: 7007, Batch Gradient Norm after: 14.694445930657428
Epoch 7008/10000, Prediction Accuracy = 64.16999999999999%, Loss = 0.3117725968360901
Epoch: 7008, Batch Gradient Norm: 14.70954885196709
Epoch: 7008, Batch Gradient Norm after: 14.70954885196709
Epoch 7009/10000, Prediction Accuracy = 64.148%, Loss = 0.31171717047691344
Epoch: 7009, Batch Gradient Norm: 14.556250347464053
Epoch: 7009, Batch Gradient Norm after: 14.556250347464053
Epoch 7010/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.3101942539215088
Epoch: 7010, Batch Gradient Norm: 12.272923958470507
Epoch: 7010, Batch Gradient Norm after: 12.272923958470507
Epoch 7011/10000, Prediction Accuracy = 64.116%, Loss = 0.3106779634952545
Epoch: 7011, Batch Gradient Norm: 12.591245883831718
Epoch: 7011, Batch Gradient Norm after: 12.591245883831718
Epoch 7012/10000, Prediction Accuracy = 64.222%, Loss = 0.3078223943710327
Epoch: 7012, Batch Gradient Norm: 11.716764560684677
Epoch: 7012, Batch Gradient Norm after: 11.716764560684677
Epoch 7013/10000, Prediction Accuracy = 64.124%, Loss = 0.30904932618141173
Epoch: 7013, Batch Gradient Norm: 13.388858688783051
Epoch: 7013, Batch Gradient Norm after: 13.388858688783051
Epoch 7014/10000, Prediction Accuracy = 64.16%, Loss = 0.308569198846817
Epoch: 7014, Batch Gradient Norm: 16.161041983899867
Epoch: 7014, Batch Gradient Norm after: 16.161041983899867
Epoch 7015/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3147795081138611
Epoch: 7015, Batch Gradient Norm: 17.217930489681695
Epoch: 7015, Batch Gradient Norm after: 17.217930489681695
Epoch 7016/10000, Prediction Accuracy = 64.088%, Loss = 0.3151259422302246
Epoch: 7016, Batch Gradient Norm: 17.82887015504466
Epoch: 7016, Batch Gradient Norm after: 17.82887015504466
Epoch 7017/10000, Prediction Accuracy = 64.122%, Loss = 0.3137317657470703
Epoch: 7017, Batch Gradient Norm: 13.027022103845995
Epoch: 7017, Batch Gradient Norm after: 13.027022103845995
Epoch 7018/10000, Prediction Accuracy = 64.178%, Loss = 0.3095979630947113
Epoch: 7018, Batch Gradient Norm: 14.924132096208265
Epoch: 7018, Batch Gradient Norm after: 14.924132096208265
Epoch 7019/10000, Prediction Accuracy = 64.0%, Loss = 0.3104978740215302
Epoch: 7019, Batch Gradient Norm: 11.98059469164575
Epoch: 7019, Batch Gradient Norm after: 11.98059469164575
Epoch 7020/10000, Prediction Accuracy = 64.238%, Loss = 0.30863032937049867
Epoch: 7020, Batch Gradient Norm: 10.79488747639558
Epoch: 7020, Batch Gradient Norm after: 10.79488747639558
Epoch 7021/10000, Prediction Accuracy = 64.288%, Loss = 0.30673080682754517
Epoch: 7021, Batch Gradient Norm: 15.523064830424174
Epoch: 7021, Batch Gradient Norm after: 15.523064830424174
Epoch 7022/10000, Prediction Accuracy = 64.14600000000002%, Loss = 0.31154314875602723
Epoch: 7022, Batch Gradient Norm: 17.794274224621926
Epoch: 7022, Batch Gradient Norm after: 17.375099376220117
Epoch 7023/10000, Prediction Accuracy = 64.18%, Loss = 0.3174673497676849
Epoch: 7023, Batch Gradient Norm: 15.875004507477634
Epoch: 7023, Batch Gradient Norm after: 15.875004507477634
Epoch 7024/10000, Prediction Accuracy = 64.146%, Loss = 0.3114025115966797
Epoch: 7024, Batch Gradient Norm: 14.75023923924424
Epoch: 7024, Batch Gradient Norm after: 14.75023923924424
Epoch 7025/10000, Prediction Accuracy = 64.106%, Loss = 0.31162059903144834
Epoch: 7025, Batch Gradient Norm: 12.79294039527222
Epoch: 7025, Batch Gradient Norm after: 12.79294039527222
Epoch 7026/10000, Prediction Accuracy = 64.28%, Loss = 0.3083791732788086
Epoch: 7026, Batch Gradient Norm: 12.861166122651486
Epoch: 7026, Batch Gradient Norm after: 12.861166122651486
Epoch 7027/10000, Prediction Accuracy = 64.128%, Loss = 0.31146039962768557
Epoch: 7027, Batch Gradient Norm: 13.233140530761881
Epoch: 7027, Batch Gradient Norm after: 13.233140530761881
Epoch 7028/10000, Prediction Accuracy = 64.288%, Loss = 0.31052209734916686
Epoch: 7028, Batch Gradient Norm: 11.862407453612086
Epoch: 7028, Batch Gradient Norm after: 11.862407453612086
Epoch 7029/10000, Prediction Accuracy = 64.126%, Loss = 0.30729228258132935
Epoch: 7029, Batch Gradient Norm: 14.065998821462998
Epoch: 7029, Batch Gradient Norm after: 14.065998821462998
Epoch 7030/10000, Prediction Accuracy = 64.182%, Loss = 0.31017419695854187
Epoch: 7030, Batch Gradient Norm: 15.410570076904493
Epoch: 7030, Batch Gradient Norm after: 15.410570076904493
Epoch 7031/10000, Prediction Accuracy = 64.148%, Loss = 0.31311235427856443
Epoch: 7031, Batch Gradient Norm: 14.672099313243013
Epoch: 7031, Batch Gradient Norm after: 14.672099313243013
Epoch 7032/10000, Prediction Accuracy = 64.11600000000001%, Loss = 0.31049891710281374
Epoch: 7032, Batch Gradient Norm: 14.531654781702578
Epoch: 7032, Batch Gradient Norm after: 14.531654781702578
Epoch 7033/10000, Prediction Accuracy = 64.17%, Loss = 0.31096304655075074
Epoch: 7033, Batch Gradient Norm: 15.22337715854135
Epoch: 7033, Batch Gradient Norm after: 15.22337715854135
Epoch 7034/10000, Prediction Accuracy = 64.212%, Loss = 0.31140629649162294
Epoch: 7034, Batch Gradient Norm: 14.099738540497816
Epoch: 7034, Batch Gradient Norm after: 14.099738540497816
Epoch 7035/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3104149758815765
Epoch: 7035, Batch Gradient Norm: 13.636089237244098
Epoch: 7035, Batch Gradient Norm after: 13.636089237244098
Epoch 7036/10000, Prediction Accuracy = 64.244%, Loss = 0.31120632886886596
Epoch: 7036, Batch Gradient Norm: 13.614538281874157
Epoch: 7036, Batch Gradient Norm after: 13.614538281874157
Epoch 7037/10000, Prediction Accuracy = 64.19200000000001%, Loss = 0.3107279181480408
Epoch: 7037, Batch Gradient Norm: 13.723055784300044
Epoch: 7037, Batch Gradient Norm after: 13.723055784300044
Epoch 7038/10000, Prediction Accuracy = 64.26599999999999%, Loss = 0.31013438701629636
Epoch: 7038, Batch Gradient Norm: 13.405722218575644
Epoch: 7038, Batch Gradient Norm after: 13.405722218575644
Epoch 7039/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.3110533177852631
Epoch: 7039, Batch Gradient Norm: 11.050694345563015
Epoch: 7039, Batch Gradient Norm after: 11.050694345563015
Epoch 7040/10000, Prediction Accuracy = 64.20400000000001%, Loss = 0.30868821144104003
Epoch: 7040, Batch Gradient Norm: 11.645882953956582
Epoch: 7040, Batch Gradient Norm after: 11.645882953956582
Epoch 7041/10000, Prediction Accuracy = 64.234%, Loss = 0.30607653260231016
Epoch: 7041, Batch Gradient Norm: 15.058432451809475
Epoch: 7041, Batch Gradient Norm after: 15.058432451809475
Epoch 7042/10000, Prediction Accuracy = 64.118%, Loss = 0.3131397783756256
Epoch: 7042, Batch Gradient Norm: 14.825978329615142
Epoch: 7042, Batch Gradient Norm after: 14.825978329615142
Epoch 7043/10000, Prediction Accuracy = 64.19800000000001%, Loss = 0.3124867260456085
Epoch: 7043, Batch Gradient Norm: 14.61657831119187
Epoch: 7043, Batch Gradient Norm after: 14.61657831119187
Epoch 7044/10000, Prediction Accuracy = 64.28999999999999%, Loss = 0.31370740532875063
Epoch: 7044, Batch Gradient Norm: 15.590979845848185
Epoch: 7044, Batch Gradient Norm after: 15.590979845848185
Epoch 7045/10000, Prediction Accuracy = 64.26599999999999%, Loss = 0.3126727879047394
Epoch: 7045, Batch Gradient Norm: 12.59030569746887
Epoch: 7045, Batch Gradient Norm after: 12.59030569746887
Epoch 7046/10000, Prediction Accuracy = 64.24600000000001%, Loss = 0.308419531583786
Epoch: 7046, Batch Gradient Norm: 13.64759155161702
Epoch: 7046, Batch Gradient Norm after: 13.64759155161702
Epoch 7047/10000, Prediction Accuracy = 64.13000000000001%, Loss = 0.30794658660888674
Epoch: 7047, Batch Gradient Norm: 13.87145219878294
Epoch: 7047, Batch Gradient Norm after: 13.87145219878294
Epoch 7048/10000, Prediction Accuracy = 64.23800000000001%, Loss = 0.30800790786743165
Epoch: 7048, Batch Gradient Norm: 15.310072836062304
Epoch: 7048, Batch Gradient Norm after: 15.310072836062304
Epoch 7049/10000, Prediction Accuracy = 64.17%, Loss = 0.31224889159202573
Epoch: 7049, Batch Gradient Norm: 15.43714179611931
Epoch: 7049, Batch Gradient Norm after: 15.43714179611931
Epoch 7050/10000, Prediction Accuracy = 64.232%, Loss = 0.31360386610031127
Epoch: 7050, Batch Gradient Norm: 16.32864831010445
Epoch: 7050, Batch Gradient Norm after: 16.32864831010445
Epoch 7051/10000, Prediction Accuracy = 64.09%, Loss = 0.3147133946418762
Epoch: 7051, Batch Gradient Norm: 15.189712608731385
Epoch: 7051, Batch Gradient Norm after: 15.189712608731385
Epoch 7052/10000, Prediction Accuracy = 64.05799999999999%, Loss = 0.3139856934547424
Epoch: 7052, Batch Gradient Norm: 15.134434939831575
Epoch: 7052, Batch Gradient Norm after: 15.134434939831575
Epoch 7053/10000, Prediction Accuracy = 64.13799999999999%, Loss = 0.31412688493728635
Epoch: 7053, Batch Gradient Norm: 14.281353025376932
Epoch: 7053, Batch Gradient Norm after: 14.281353025376932
Epoch 7054/10000, Prediction Accuracy = 64.24199999999999%, Loss = 0.3097635626792908
Epoch: 7054, Batch Gradient Norm: 16.28760741842116
Epoch: 7054, Batch Gradient Norm after: 16.28760741842116
Epoch 7055/10000, Prediction Accuracy = 64.28999999999999%, Loss = 0.31252871751785277
Epoch: 7055, Batch Gradient Norm: 17.458652861504778
Epoch: 7055, Batch Gradient Norm after: 17.42123940663315
Epoch 7056/10000, Prediction Accuracy = 64.024%, Loss = 0.31703363060951234
Epoch: 7056, Batch Gradient Norm: 18.418584398234373
Epoch: 7056, Batch Gradient Norm after: 18.418584398234373
Epoch 7057/10000, Prediction Accuracy = 64.16400000000002%, Loss = 0.3140325903892517
Epoch: 7057, Batch Gradient Norm: 20.236376839797746
Epoch: 7057, Batch Gradient Norm after: 19.76719113665323
Epoch 7058/10000, Prediction Accuracy = 64.102%, Loss = 0.3180911421775818
Epoch: 7058, Batch Gradient Norm: 15.441746530442341
Epoch: 7058, Batch Gradient Norm after: 15.441746530442341
Epoch 7059/10000, Prediction Accuracy = 64.148%, Loss = 0.31053274869918823
Epoch: 7059, Batch Gradient Norm: 17.157650579983
Epoch: 7059, Batch Gradient Norm after: 17.157650579983
Epoch 7060/10000, Prediction Accuracy = 64.0%, Loss = 0.3144926965236664
Epoch: 7060, Batch Gradient Norm: 16.388206935046576
Epoch: 7060, Batch Gradient Norm after: 16.388206935046576
Epoch 7061/10000, Prediction Accuracy = 64.096%, Loss = 0.3132238209247589
Epoch: 7061, Batch Gradient Norm: 16.332844623505338
Epoch: 7061, Batch Gradient Norm after: 16.332844623505338
Epoch 7062/10000, Prediction Accuracy = 64.372%, Loss = 0.31341609358787537
Epoch: 7062, Batch Gradient Norm: 16.34178809156864
Epoch: 7062, Batch Gradient Norm after: 16.34178809156864
Epoch 7063/10000, Prediction Accuracy = 64.28%, Loss = 0.313307386636734
Epoch: 7063, Batch Gradient Norm: 16.063676267938956
Epoch: 7063, Batch Gradient Norm after: 16.063676267938956
Epoch 7064/10000, Prediction Accuracy = 64.11600000000001%, Loss = 0.31450246572494506
Epoch: 7064, Batch Gradient Norm: 17.700937343961975
Epoch: 7064, Batch Gradient Norm after: 17.51075124337345
Epoch 7065/10000, Prediction Accuracy = 64.152%, Loss = 0.3138612687587738
Epoch: 7065, Batch Gradient Norm: 17.445395163754853
Epoch: 7065, Batch Gradient Norm after: 17.445395163754853
Epoch 7066/10000, Prediction Accuracy = 64.32000000000001%, Loss = 0.31174878478050233
Epoch: 7066, Batch Gradient Norm: 16.15207833610854
Epoch: 7066, Batch Gradient Norm after: 16.15207833610854
Epoch 7067/10000, Prediction Accuracy = 64.178%, Loss = 0.312235689163208
Epoch: 7067, Batch Gradient Norm: 14.882514874827796
Epoch: 7067, Batch Gradient Norm after: 14.882514874827796
Epoch 7068/10000, Prediction Accuracy = 64.27199999999999%, Loss = 0.3108633518218994
Epoch: 7068, Batch Gradient Norm: 14.445345628466152
Epoch: 7068, Batch Gradient Norm after: 14.445345628466152
Epoch 7069/10000, Prediction Accuracy = 64.2%, Loss = 0.313627815246582
Epoch: 7069, Batch Gradient Norm: 13.831534621452295
Epoch: 7069, Batch Gradient Norm after: 13.831534621452295
Epoch 7070/10000, Prediction Accuracy = 64.148%, Loss = 0.311068069934845
Epoch: 7070, Batch Gradient Norm: 16.468938124516967
Epoch: 7070, Batch Gradient Norm after: 16.468938124516967
Epoch 7071/10000, Prediction Accuracy = 64.124%, Loss = 0.31652969121932983
Epoch: 7071, Batch Gradient Norm: 15.770719000436438
Epoch: 7071, Batch Gradient Norm after: 15.770719000436438
Epoch 7072/10000, Prediction Accuracy = 64.12%, Loss = 0.31227385997772217
Epoch: 7072, Batch Gradient Norm: 12.692159569758447
Epoch: 7072, Batch Gradient Norm after: 12.692159569758447
Epoch 7073/10000, Prediction Accuracy = 64.3%, Loss = 0.3117245078086853
Epoch: 7073, Batch Gradient Norm: 12.80649732654048
Epoch: 7073, Batch Gradient Norm after: 12.80649732654048
Epoch 7074/10000, Prediction Accuracy = 64.186%, Loss = 0.31013251543045045
Epoch: 7074, Batch Gradient Norm: 15.070164215802624
Epoch: 7074, Batch Gradient Norm after: 15.070164215802624
Epoch 7075/10000, Prediction Accuracy = 64.11200000000001%, Loss = 0.3106456995010376
Epoch: 7075, Batch Gradient Norm: 15.036412424761714
Epoch: 7075, Batch Gradient Norm after: 15.036412424761714
Epoch 7076/10000, Prediction Accuracy = 64.24000000000001%, Loss = 0.31101977825164795
Epoch: 7076, Batch Gradient Norm: 14.82057430771926
Epoch: 7076, Batch Gradient Norm after: 14.82057430771926
Epoch 7077/10000, Prediction Accuracy = 64.224%, Loss = 0.31128052473068235
Epoch: 7077, Batch Gradient Norm: 15.09618926186183
Epoch: 7077, Batch Gradient Norm after: 15.09618926186183
Epoch 7078/10000, Prediction Accuracy = 64.02%, Loss = 0.3127085566520691
Epoch: 7078, Batch Gradient Norm: 12.883830537981487
Epoch: 7078, Batch Gradient Norm after: 12.883830537981487
Epoch 7079/10000, Prediction Accuracy = 64.15400000000001%, Loss = 0.3081205487251282
Epoch: 7079, Batch Gradient Norm: 11.299128326452616
Epoch: 7079, Batch Gradient Norm after: 11.299128326452616
Epoch 7080/10000, Prediction Accuracy = 64.30600000000001%, Loss = 0.3052812278270721
Epoch: 7080, Batch Gradient Norm: 13.121709349813521
Epoch: 7080, Batch Gradient Norm after: 13.121709349813521
Epoch 7081/10000, Prediction Accuracy = 64.154%, Loss = 0.307713919878006
Epoch: 7081, Batch Gradient Norm: 15.831195057204647
Epoch: 7081, Batch Gradient Norm after: 15.71488678749987
Epoch 7082/10000, Prediction Accuracy = 64.186%, Loss = 0.3119642913341522
Epoch: 7082, Batch Gradient Norm: 16.23156777021696
Epoch: 7082, Batch Gradient Norm after: 16.23156777021696
Epoch 7083/10000, Prediction Accuracy = 64.146%, Loss = 0.3126956343650818
Epoch: 7083, Batch Gradient Norm: 12.467526631102777
Epoch: 7083, Batch Gradient Norm after: 12.467526631102777
Epoch 7084/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.30673936009407043
Epoch: 7084, Batch Gradient Norm: 14.442688225702064
Epoch: 7084, Batch Gradient Norm after: 14.442688225702064
Epoch 7085/10000, Prediction Accuracy = 64.284%, Loss = 0.309385883808136
Epoch: 7085, Batch Gradient Norm: 16.183534023253987
Epoch: 7085, Batch Gradient Norm after: 16.183534023253987
Epoch 7086/10000, Prediction Accuracy = 64.054%, Loss = 0.3131580054759979
Epoch: 7086, Batch Gradient Norm: 14.096792417078316
Epoch: 7086, Batch Gradient Norm after: 14.096792417078316
Epoch 7087/10000, Prediction Accuracy = 64.196%, Loss = 0.3090983211994171
Epoch: 7087, Batch Gradient Norm: 15.681401214616953
Epoch: 7087, Batch Gradient Norm after: 15.681401214616953
Epoch 7088/10000, Prediction Accuracy = 64.274%, Loss = 0.3102215051651001
Epoch: 7088, Batch Gradient Norm: 15.711674609399328
Epoch: 7088, Batch Gradient Norm after: 15.711674609399328
Epoch 7089/10000, Prediction Accuracy = 63.999999999999986%, Loss = 0.31033499240875245
Epoch: 7089, Batch Gradient Norm: 13.772152925703498
Epoch: 7089, Batch Gradient Norm after: 13.772152925703498
Epoch 7090/10000, Prediction Accuracy = 64.20599999999999%, Loss = 0.310126930475235
Epoch: 7090, Batch Gradient Norm: 11.891252224842148
Epoch: 7090, Batch Gradient Norm after: 11.891252224842148
Epoch 7091/10000, Prediction Accuracy = 64.286%, Loss = 0.3058779060840607
Epoch: 7091, Batch Gradient Norm: 14.697440342226859
Epoch: 7091, Batch Gradient Norm after: 14.697440342226859
Epoch 7092/10000, Prediction Accuracy = 64.196%, Loss = 0.3105567157268524
Epoch: 7092, Batch Gradient Norm: 14.897055170571482
Epoch: 7092, Batch Gradient Norm after: 14.897055170571482
Epoch 7093/10000, Prediction Accuracy = 64.288%, Loss = 0.3103285372257233
Epoch: 7093, Batch Gradient Norm: 13.955376517303918
Epoch: 7093, Batch Gradient Norm after: 13.955376517303918
Epoch 7094/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.3094019293785095
Epoch: 7094, Batch Gradient Norm: 15.564014037954903
Epoch: 7094, Batch Gradient Norm after: 15.564014037954903
Epoch 7095/10000, Prediction Accuracy = 64.10799999999999%, Loss = 0.31089420318603517
Epoch: 7095, Batch Gradient Norm: 14.697382607891303
Epoch: 7095, Batch Gradient Norm after: 14.697382607891303
Epoch 7096/10000, Prediction Accuracy = 64.26%, Loss = 0.31082746386528015
Epoch: 7096, Batch Gradient Norm: 15.421268751816115
Epoch: 7096, Batch Gradient Norm after: 15.421268751816115
Epoch 7097/10000, Prediction Accuracy = 64.19399999999999%, Loss = 0.3104568898677826
Epoch: 7097, Batch Gradient Norm: 12.739739870033311
Epoch: 7097, Batch Gradient Norm after: 12.739739870033311
Epoch 7098/10000, Prediction Accuracy = 64.17%, Loss = 0.3082645654678345
Epoch: 7098, Batch Gradient Norm: 14.403773572026372
Epoch: 7098, Batch Gradient Norm after: 14.403773572026372
Epoch 7099/10000, Prediction Accuracy = 64.188%, Loss = 0.31026298403739927
Epoch: 7099, Batch Gradient Norm: 13.946893638067515
Epoch: 7099, Batch Gradient Norm after: 13.946893638067515
Epoch 7100/10000, Prediction Accuracy = 64.20800000000001%, Loss = 0.30972492694854736
Epoch: 7100, Batch Gradient Norm: 12.779253578379157
Epoch: 7100, Batch Gradient Norm after: 12.779253578379157
Epoch 7101/10000, Prediction Accuracy = 64.308%, Loss = 0.30897215008735657
Epoch: 7101, Batch Gradient Norm: 12.604566370681615
Epoch: 7101, Batch Gradient Norm after: 12.604566370681615
Epoch 7102/10000, Prediction Accuracy = 64.28399999999999%, Loss = 0.30650943517684937
Epoch: 7102, Batch Gradient Norm: 12.573113886099023
Epoch: 7102, Batch Gradient Norm after: 12.573113886099023
Epoch 7103/10000, Prediction Accuracy = 64.264%, Loss = 0.3054632544517517
Epoch: 7103, Batch Gradient Norm: 10.881056174452704
Epoch: 7103, Batch Gradient Norm after: 10.881056174452704
Epoch 7104/10000, Prediction Accuracy = 64.204%, Loss = 0.3054603040218353
Epoch: 7104, Batch Gradient Norm: 11.584494968839582
Epoch: 7104, Batch Gradient Norm after: 11.584494968839582
Epoch 7105/10000, Prediction Accuracy = 64.134%, Loss = 0.30546478033065794
Epoch: 7105, Batch Gradient Norm: 11.726222601181204
Epoch: 7105, Batch Gradient Norm after: 11.726222601181204
Epoch 7106/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.30707454681396484
Epoch: 7106, Batch Gradient Norm: 13.116152477695458
Epoch: 7106, Batch Gradient Norm after: 13.116152477695458
Epoch 7107/10000, Prediction Accuracy = 64.24199999999999%, Loss = 0.3078352451324463
Epoch: 7107, Batch Gradient Norm: 14.60334648459846
Epoch: 7107, Batch Gradient Norm after: 14.60334648459846
Epoch 7108/10000, Prediction Accuracy = 64.30199999999999%, Loss = 0.30796594023704527
Epoch: 7108, Batch Gradient Norm: 15.777859989822495
Epoch: 7108, Batch Gradient Norm after: 15.521165992587813
Epoch 7109/10000, Prediction Accuracy = 64.26599999999999%, Loss = 0.3098666727542877
Epoch: 7109, Batch Gradient Norm: 16.444551245088945
Epoch: 7109, Batch Gradient Norm after: 16.444551245088945
Epoch 7110/10000, Prediction Accuracy = 64.22%, Loss = 0.3121503472328186
Epoch: 7110, Batch Gradient Norm: 15.915505107239882
Epoch: 7110, Batch Gradient Norm after: 15.915505107239882
Epoch 7111/10000, Prediction Accuracy = 64.234%, Loss = 0.3144860088825226
Epoch: 7111, Batch Gradient Norm: 12.796258572823588
Epoch: 7111, Batch Gradient Norm after: 12.796258572823588
Epoch 7112/10000, Prediction Accuracy = 64.25200000000001%, Loss = 0.30734034776687624
Epoch: 7112, Batch Gradient Norm: 14.796219095479366
Epoch: 7112, Batch Gradient Norm after: 14.796219095479366
Epoch 7113/10000, Prediction Accuracy = 64.20400000000001%, Loss = 0.311848908662796
Epoch: 7113, Batch Gradient Norm: 17.89764310699298
Epoch: 7113, Batch Gradient Norm after: 17.89764310699298
Epoch 7114/10000, Prediction Accuracy = 64.24%, Loss = 0.3158698439598083
Epoch: 7114, Batch Gradient Norm: 17.210039339314964
Epoch: 7114, Batch Gradient Norm after: 17.210039339314964
Epoch 7115/10000, Prediction Accuracy = 64.28200000000001%, Loss = 0.312727028131485
Epoch: 7115, Batch Gradient Norm: 17.222902097037547
Epoch: 7115, Batch Gradient Norm after: 17.222902097037547
Epoch 7116/10000, Prediction Accuracy = 64.184%, Loss = 0.31298325657844545
Epoch: 7116, Batch Gradient Norm: 18.530578041241515
Epoch: 7116, Batch Gradient Norm after: 18.530578041241515
Epoch 7117/10000, Prediction Accuracy = 64.19800000000001%, Loss = 0.3168880999088287
Epoch: 7117, Batch Gradient Norm: 15.634752584938257
Epoch: 7117, Batch Gradient Norm after: 15.634752584938257
Epoch 7118/10000, Prediction Accuracy = 64.19399999999999%, Loss = 0.3118170738220215
Epoch: 7118, Batch Gradient Norm: 14.084124829758622
Epoch: 7118, Batch Gradient Norm after: 14.084124829758622
Epoch 7119/10000, Prediction Accuracy = 64.16399999999999%, Loss = 0.30877572298049927
Epoch: 7119, Batch Gradient Norm: 16.45645195826709
Epoch: 7119, Batch Gradient Norm after: 16.45645195826709
Epoch 7120/10000, Prediction Accuracy = 64.274%, Loss = 0.31014434099197385
Epoch: 7120, Batch Gradient Norm: 16.790379360428243
Epoch: 7120, Batch Gradient Norm after: 16.790379360428243
Epoch 7121/10000, Prediction Accuracy = 64.238%, Loss = 0.3128542423248291
Epoch: 7121, Batch Gradient Norm: 18.11091400583287
Epoch: 7121, Batch Gradient Norm after: 17.691376452525283
Epoch 7122/10000, Prediction Accuracy = 64.39000000000001%, Loss = 0.3137519061565399
Epoch: 7122, Batch Gradient Norm: 14.387196624373088
Epoch: 7122, Batch Gradient Norm after: 14.387196624373088
Epoch 7123/10000, Prediction Accuracy = 64.21%, Loss = 0.30854191780090334
Epoch: 7123, Batch Gradient Norm: 16.188017128833426
Epoch: 7123, Batch Gradient Norm after: 16.188017128833426
Epoch 7124/10000, Prediction Accuracy = 64.134%, Loss = 0.3128730356693268
Epoch: 7124, Batch Gradient Norm: 16.046051258900647
Epoch: 7124, Batch Gradient Norm after: 16.046051258900647
Epoch 7125/10000, Prediction Accuracy = 64.274%, Loss = 0.3113616585731506
Epoch: 7125, Batch Gradient Norm: 14.033574759980947
Epoch: 7125, Batch Gradient Norm after: 14.033574759980947
Epoch 7126/10000, Prediction Accuracy = 64.272%, Loss = 0.3071968972682953
Epoch: 7126, Batch Gradient Norm: 14.874458241371949
Epoch: 7126, Batch Gradient Norm after: 14.874458241371949
Epoch 7127/10000, Prediction Accuracy = 64.18800000000002%, Loss = 0.30980847477912904
Epoch: 7127, Batch Gradient Norm: 14.75341236514558
Epoch: 7127, Batch Gradient Norm after: 14.75341236514558
Epoch 7128/10000, Prediction Accuracy = 64.27799999999999%, Loss = 0.3092983841896057
Epoch: 7128, Batch Gradient Norm: 12.135182764169594
Epoch: 7128, Batch Gradient Norm after: 12.135182764169594
Epoch 7129/10000, Prediction Accuracy = 64.28%, Loss = 0.3048971056938171
Epoch: 7129, Batch Gradient Norm: 14.799130183654997
Epoch: 7129, Batch Gradient Norm after: 14.799130183654997
Epoch 7130/10000, Prediction Accuracy = 64.246%, Loss = 0.3115168035030365
Epoch: 7130, Batch Gradient Norm: 18.414037230841235
Epoch: 7130, Batch Gradient Norm after: 18.414037230841235
Epoch 7131/10000, Prediction Accuracy = 64.13%, Loss = 0.3177932262420654
Epoch: 7131, Batch Gradient Norm: 16.28601282886039
Epoch: 7131, Batch Gradient Norm after: 16.28601282886039
Epoch 7132/10000, Prediction Accuracy = 64.298%, Loss = 0.31079208850860596
Epoch: 7132, Batch Gradient Norm: 14.9710217471431
Epoch: 7132, Batch Gradient Norm after: 14.9710217471431
Epoch 7133/10000, Prediction Accuracy = 64.17999999999999%, Loss = 0.30806326270103457
Epoch: 7133, Batch Gradient Norm: 17.536880483886126
Epoch: 7133, Batch Gradient Norm after: 17.536880483886126
Epoch 7134/10000, Prediction Accuracy = 64.402%, Loss = 0.31397225260734557
Epoch: 7134, Batch Gradient Norm: 19.40031940201771
Epoch: 7134, Batch Gradient Norm after: 19.024861405908492
Epoch 7135/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.3148182690143585
Epoch: 7135, Batch Gradient Norm: 16.166621541394967
Epoch: 7135, Batch Gradient Norm after: 16.166621541394967
Epoch 7136/10000, Prediction Accuracy = 64.14000000000001%, Loss = 0.310384064912796
Epoch: 7136, Batch Gradient Norm: 14.578944220726438
Epoch: 7136, Batch Gradient Norm after: 14.578944220726438
Epoch 7137/10000, Prediction Accuracy = 64.112%, Loss = 0.3087845742702484
Epoch: 7137, Batch Gradient Norm: 17.270808686734195
Epoch: 7137, Batch Gradient Norm after: 17.270808686734195
Epoch 7138/10000, Prediction Accuracy = 64.16%, Loss = 0.3120217740535736
Epoch: 7138, Batch Gradient Norm: 13.098995863764504
Epoch: 7138, Batch Gradient Norm after: 13.098995863764504
Epoch 7139/10000, Prediction Accuracy = 64.21200000000002%, Loss = 0.30550298690795896
Epoch: 7139, Batch Gradient Norm: 10.74992784197417
Epoch: 7139, Batch Gradient Norm after: 10.74992784197417
Epoch 7140/10000, Prediction Accuracy = 64.284%, Loss = 0.3034051477909088
Epoch: 7140, Batch Gradient Norm: 13.411408135415696
Epoch: 7140, Batch Gradient Norm after: 13.411408135415696
Epoch 7141/10000, Prediction Accuracy = 64.098%, Loss = 0.3094674825668335
Epoch: 7141, Batch Gradient Norm: 11.630999544141364
Epoch: 7141, Batch Gradient Norm after: 11.630999544141364
Epoch 7142/10000, Prediction Accuracy = 64.236%, Loss = 0.3032552480697632
Epoch: 7142, Batch Gradient Norm: 12.978285727424089
Epoch: 7142, Batch Gradient Norm after: 12.978285727424089
Epoch 7143/10000, Prediction Accuracy = 64.17800000000001%, Loss = 0.30724998116493224
Epoch: 7143, Batch Gradient Norm: 14.23046435016305
Epoch: 7143, Batch Gradient Norm after: 14.23046435016305
Epoch 7144/10000, Prediction Accuracy = 64.074%, Loss = 0.30760128498077394
Epoch: 7144, Batch Gradient Norm: 12.941869824089112
Epoch: 7144, Batch Gradient Norm after: 12.941869824089112
Epoch 7145/10000, Prediction Accuracy = 64.156%, Loss = 0.3082525312900543
Epoch: 7145, Batch Gradient Norm: 12.446403650931924
Epoch: 7145, Batch Gradient Norm after: 12.446403650931924
Epoch 7146/10000, Prediction Accuracy = 64.228%, Loss = 0.3050740838050842
Epoch: 7146, Batch Gradient Norm: 13.13102467038894
Epoch: 7146, Batch Gradient Norm after: 13.13102467038894
Epoch 7147/10000, Prediction Accuracy = 64.22%, Loss = 0.3089342057704926
Epoch: 7147, Batch Gradient Norm: 13.005433364610102
Epoch: 7147, Batch Gradient Norm after: 13.005433364610102
Epoch 7148/10000, Prediction Accuracy = 64.19800000000001%, Loss = 0.3069798111915588
Epoch: 7148, Batch Gradient Norm: 15.411833128483062
Epoch: 7148, Batch Gradient Norm after: 15.411833128483062
Epoch 7149/10000, Prediction Accuracy = 64.252%, Loss = 0.3097578465938568
Epoch: 7149, Batch Gradient Norm: 15.700266999512992
Epoch: 7149, Batch Gradient Norm after: 15.700266999512992
Epoch 7150/10000, Prediction Accuracy = 64.33599999999998%, Loss = 0.30968504548072817
Epoch: 7150, Batch Gradient Norm: 15.135457453485396
Epoch: 7150, Batch Gradient Norm after: 15.135457453485396
Epoch 7151/10000, Prediction Accuracy = 64.25%, Loss = 0.30882909893989563
Epoch: 7151, Batch Gradient Norm: 12.470002145308989
Epoch: 7151, Batch Gradient Norm after: 12.470002145308989
Epoch 7152/10000, Prediction Accuracy = 64.162%, Loss = 0.3063172876834869
Epoch: 7152, Batch Gradient Norm: 15.398306971918773
Epoch: 7152, Batch Gradient Norm after: 15.398306971918773
Epoch 7153/10000, Prediction Accuracy = 64.29599999999999%, Loss = 0.31096630692481997
Epoch: 7153, Batch Gradient Norm: 16.048120228800403
Epoch: 7153, Batch Gradient Norm after: 16.048120228800403
Epoch 7154/10000, Prediction Accuracy = 64.14399999999999%, Loss = 0.31151830554008486
Epoch: 7154, Batch Gradient Norm: 17.159315523305498
Epoch: 7154, Batch Gradient Norm after: 17.159315523305498
Epoch 7155/10000, Prediction Accuracy = 64.292%, Loss = 0.313407301902771
Epoch: 7155, Batch Gradient Norm: 14.505544695530897
Epoch: 7155, Batch Gradient Norm after: 14.505544695530897
Epoch 7156/10000, Prediction Accuracy = 64.172%, Loss = 0.3107589781284332
Epoch: 7156, Batch Gradient Norm: 14.737926319579532
Epoch: 7156, Batch Gradient Norm after: 14.737926319579532
Epoch 7157/10000, Prediction Accuracy = 64.142%, Loss = 0.3105266571044922
Epoch: 7157, Batch Gradient Norm: 16.633313436864878
Epoch: 7157, Batch Gradient Norm after: 16.633313436864878
Epoch 7158/10000, Prediction Accuracy = 64.09200000000001%, Loss = 0.31041600108146666
Epoch: 7158, Batch Gradient Norm: 14.357885358748623
Epoch: 7158, Batch Gradient Norm after: 14.357885358748623
Epoch 7159/10000, Prediction Accuracy = 64.128%, Loss = 0.3095086395740509
Epoch: 7159, Batch Gradient Norm: 13.9204609072712
Epoch: 7159, Batch Gradient Norm after: 13.9204609072712
Epoch 7160/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.30872238874435426
Epoch: 7160, Batch Gradient Norm: 13.530643098061685
Epoch: 7160, Batch Gradient Norm after: 13.530643098061685
Epoch 7161/10000, Prediction Accuracy = 64.28%, Loss = 0.30700679421424865
Epoch: 7161, Batch Gradient Norm: 14.023188903672064
Epoch: 7161, Batch Gradient Norm after: 14.023188903672064
Epoch 7162/10000, Prediction Accuracy = 64.012%, Loss = 0.3090148866176605
Epoch: 7162, Batch Gradient Norm: 13.798723940244727
Epoch: 7162, Batch Gradient Norm after: 13.798723940244727
Epoch 7163/10000, Prediction Accuracy = 64.236%, Loss = 0.30952478051185606
Epoch: 7163, Batch Gradient Norm: 12.781361183941216
Epoch: 7163, Batch Gradient Norm after: 12.781361183941216
Epoch 7164/10000, Prediction Accuracy = 64.11200000000001%, Loss = 0.30643919110298157
Epoch: 7164, Batch Gradient Norm: 13.561652957598847
Epoch: 7164, Batch Gradient Norm after: 13.561652957598847
Epoch 7165/10000, Prediction Accuracy = 64.11%, Loss = 0.30915995836257937
Epoch: 7165, Batch Gradient Norm: 15.501281101053598
Epoch: 7165, Batch Gradient Norm after: 15.501281101053598
Epoch 7166/10000, Prediction Accuracy = 64.104%, Loss = 0.31114606857299804
Epoch: 7166, Batch Gradient Norm: 16.306165566465175
Epoch: 7166, Batch Gradient Norm after: 16.306165566465175
Epoch 7167/10000, Prediction Accuracy = 64.23400000000001%, Loss = 0.310959655046463
Epoch: 7167, Batch Gradient Norm: 15.200430688788575
Epoch: 7167, Batch Gradient Norm after: 15.200430688788575
Epoch 7168/10000, Prediction Accuracy = 64.056%, Loss = 0.3086493372917175
Epoch: 7168, Batch Gradient Norm: 15.60599091867066
Epoch: 7168, Batch Gradient Norm after: 15.60599091867066
Epoch 7169/10000, Prediction Accuracy = 64.20400000000001%, Loss = 0.30901169776916504
Epoch: 7169, Batch Gradient Norm: 13.20116085176503
Epoch: 7169, Batch Gradient Norm after: 13.20116085176503
Epoch 7170/10000, Prediction Accuracy = 64.13799999999999%, Loss = 0.306895500421524
Epoch: 7170, Batch Gradient Norm: 13.506718382103625
Epoch: 7170, Batch Gradient Norm after: 13.506718382103625
Epoch 7171/10000, Prediction Accuracy = 64.24000000000001%, Loss = 0.3091328084468842
Epoch: 7171, Batch Gradient Norm: 13.641384850325936
Epoch: 7171, Batch Gradient Norm after: 13.641384850325936
Epoch 7172/10000, Prediction Accuracy = 64.276%, Loss = 0.307439386844635
Epoch: 7172, Batch Gradient Norm: 14.847769739925376
Epoch: 7172, Batch Gradient Norm after: 14.847769739925376
Epoch 7173/10000, Prediction Accuracy = 64.156%, Loss = 0.31177058815956116
Epoch: 7173, Batch Gradient Norm: 12.053937612764297
Epoch: 7173, Batch Gradient Norm after: 12.053937612764297
Epoch 7174/10000, Prediction Accuracy = 64.22599999999998%, Loss = 0.304141503572464
Epoch: 7174, Batch Gradient Norm: 12.99740854598914
Epoch: 7174, Batch Gradient Norm after: 12.99740854598914
Epoch 7175/10000, Prediction Accuracy = 64.054%, Loss = 0.30709009766578677
Epoch: 7175, Batch Gradient Norm: 13.298925150105045
Epoch: 7175, Batch Gradient Norm after: 13.298925150105045
Epoch 7176/10000, Prediction Accuracy = 64.408%, Loss = 0.30884645581245423
Epoch: 7176, Batch Gradient Norm: 14.54601931923284
Epoch: 7176, Batch Gradient Norm after: 14.54601931923284
Epoch 7177/10000, Prediction Accuracy = 64.28%, Loss = 0.3066149353981018
Epoch: 7177, Batch Gradient Norm: 12.860993488228516
Epoch: 7177, Batch Gradient Norm after: 12.860993488228516
Epoch 7178/10000, Prediction Accuracy = 64.126%, Loss = 0.3096094071865082
Epoch: 7178, Batch Gradient Norm: 11.652377879478648
Epoch: 7178, Batch Gradient Norm after: 11.652377879478648
Epoch 7179/10000, Prediction Accuracy = 64.136%, Loss = 0.30383577942848206
Epoch: 7179, Batch Gradient Norm: 12.651747785707434
Epoch: 7179, Batch Gradient Norm after: 12.651747785707434
Epoch 7180/10000, Prediction Accuracy = 64.218%, Loss = 0.3052453875541687
Epoch: 7180, Batch Gradient Norm: 14.12364206224141
Epoch: 7180, Batch Gradient Norm after: 14.12364206224141
Epoch 7181/10000, Prediction Accuracy = 64.17999999999999%, Loss = 0.3071682214736938
Epoch: 7181, Batch Gradient Norm: 14.120561609345035
Epoch: 7181, Batch Gradient Norm after: 14.120561609345035
Epoch 7182/10000, Prediction Accuracy = 64.252%, Loss = 0.30778629183769224
Epoch: 7182, Batch Gradient Norm: 13.646648932451274
Epoch: 7182, Batch Gradient Norm after: 13.646648932451274
Epoch 7183/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.3068182170391083
Epoch: 7183, Batch Gradient Norm: 12.153413054954806
Epoch: 7183, Batch Gradient Norm after: 12.153413054954806
Epoch 7184/10000, Prediction Accuracy = 64.22%, Loss = 0.30562934279441833
Epoch: 7184, Batch Gradient Norm: 11.79309634652405
Epoch: 7184, Batch Gradient Norm after: 11.79309634652405
Epoch 7185/10000, Prediction Accuracy = 64.192%, Loss = 0.3048441171646118
Epoch: 7185, Batch Gradient Norm: 14.724394157039832
Epoch: 7185, Batch Gradient Norm after: 14.724394157039832
Epoch 7186/10000, Prediction Accuracy = 64.224%, Loss = 0.30793473720550535
Epoch: 7186, Batch Gradient Norm: 15.409174068257892
Epoch: 7186, Batch Gradient Norm after: 15.409174068257892
Epoch 7187/10000, Prediction Accuracy = 64.27799999999999%, Loss = 0.30938578248023985
Epoch: 7187, Batch Gradient Norm: 12.800181628506342
Epoch: 7187, Batch Gradient Norm after: 12.800181628506342
Epoch 7188/10000, Prediction Accuracy = 64.25800000000001%, Loss = 0.3052720189094543
Epoch: 7188, Batch Gradient Norm: 11.398444543269633
Epoch: 7188, Batch Gradient Norm after: 11.398444543269633
Epoch 7189/10000, Prediction Accuracy = 64.27%, Loss = 0.30347868204116824
Epoch: 7189, Batch Gradient Norm: 11.759548165839869
Epoch: 7189, Batch Gradient Norm after: 11.759548165839869
Epoch 7190/10000, Prediction Accuracy = 64.214%, Loss = 0.30321879386901857
Epoch: 7190, Batch Gradient Norm: 14.407529682616538
Epoch: 7190, Batch Gradient Norm after: 14.407529682616538
Epoch 7191/10000, Prediction Accuracy = 64.39200000000001%, Loss = 0.30807012915611265
Epoch: 7191, Batch Gradient Norm: 13.017571429113131
Epoch: 7191, Batch Gradient Norm after: 13.017571429113131
Epoch 7192/10000, Prediction Accuracy = 64.24600000000001%, Loss = 0.30778194665908815
Epoch: 7192, Batch Gradient Norm: 12.025718453518708
Epoch: 7192, Batch Gradient Norm after: 12.025718453518708
Epoch 7193/10000, Prediction Accuracy = 64.35400000000001%, Loss = 0.3038331866264343
Epoch: 7193, Batch Gradient Norm: 12.916466031033927
Epoch: 7193, Batch Gradient Norm after: 12.916466031033927
Epoch 7194/10000, Prediction Accuracy = 64.11600000000001%, Loss = 0.30637975335121154
Epoch: 7194, Batch Gradient Norm: 13.25191402693952
Epoch: 7194, Batch Gradient Norm after: 13.25191402693952
Epoch 7195/10000, Prediction Accuracy = 64.246%, Loss = 0.3080573737621307
Epoch: 7195, Batch Gradient Norm: 16.459259698478245
Epoch: 7195, Batch Gradient Norm after: 16.459259698478245
Epoch 7196/10000, Prediction Accuracy = 64.22%, Loss = 0.30893290638923643
Epoch: 7196, Batch Gradient Norm: 16.751876292270474
Epoch: 7196, Batch Gradient Norm after: 16.751876292270474
Epoch 7197/10000, Prediction Accuracy = 64.25800000000001%, Loss = 0.31017049551010134
Epoch: 7197, Batch Gradient Norm: 16.165473571183064
Epoch: 7197, Batch Gradient Norm after: 16.165473571183064
Epoch 7198/10000, Prediction Accuracy = 64.182%, Loss = 0.31108600497245786
Epoch: 7198, Batch Gradient Norm: 16.185476862594793
Epoch: 7198, Batch Gradient Norm after: 16.185476862594793
Epoch 7199/10000, Prediction Accuracy = 64.19800000000001%, Loss = 0.30985628366470336
Epoch: 7199, Batch Gradient Norm: 18.86096503746291
Epoch: 7199, Batch Gradient Norm after: 18.86096503746291
Epoch 7200/10000, Prediction Accuracy = 64.22800000000001%, Loss = 0.3128700375556946
Epoch: 7200, Batch Gradient Norm: 14.759209597922311
Epoch: 7200, Batch Gradient Norm after: 14.759209597922311
Epoch 7201/10000, Prediction Accuracy = 64.164%, Loss = 0.30749787092208863
Epoch: 7201, Batch Gradient Norm: 12.99030422241049
Epoch: 7201, Batch Gradient Norm after: 12.99030422241049
Epoch 7202/10000, Prediction Accuracy = 64.244%, Loss = 0.30582643747329713
Epoch: 7202, Batch Gradient Norm: 13.872505062172893
Epoch: 7202, Batch Gradient Norm after: 13.872505062172893
Epoch 7203/10000, Prediction Accuracy = 64.212%, Loss = 0.30887231826782224
Epoch: 7203, Batch Gradient Norm: 12.705803874991771
Epoch: 7203, Batch Gradient Norm after: 12.705803874991771
Epoch 7204/10000, Prediction Accuracy = 64.284%, Loss = 0.3055630147457123
Epoch: 7204, Batch Gradient Norm: 11.930769098092723
Epoch: 7204, Batch Gradient Norm after: 11.930769098092723
Epoch 7205/10000, Prediction Accuracy = 64.33600000000001%, Loss = 0.30439041256904603
Epoch: 7205, Batch Gradient Norm: 12.353678208343759
Epoch: 7205, Batch Gradient Norm after: 12.353678208343759
Epoch 7206/10000, Prediction Accuracy = 64.20599999999999%, Loss = 0.30702129006385803
Epoch: 7206, Batch Gradient Norm: 14.01839061453547
Epoch: 7206, Batch Gradient Norm after: 14.01839061453547
Epoch 7207/10000, Prediction Accuracy = 64.298%, Loss = 0.30831327438354494
Epoch: 7207, Batch Gradient Norm: 15.11444752147135
Epoch: 7207, Batch Gradient Norm after: 15.11444752147135
Epoch 7208/10000, Prediction Accuracy = 64.33200000000001%, Loss = 0.310080748796463
Epoch: 7208, Batch Gradient Norm: 14.980103209242966
Epoch: 7208, Batch Gradient Norm after: 14.980103209242966
Epoch 7209/10000, Prediction Accuracy = 64.12199999999999%, Loss = 0.3094574987888336
Epoch: 7209, Batch Gradient Norm: 18.15990877028486
Epoch: 7209, Batch Gradient Norm after: 18.15990877028486
Epoch 7210/10000, Prediction Accuracy = 64.322%, Loss = 0.3152718961238861
Epoch: 7210, Batch Gradient Norm: 16.83631508519544
Epoch: 7210, Batch Gradient Norm after: 16.83631508519544
Epoch 7211/10000, Prediction Accuracy = 64.28599999999999%, Loss = 0.31117860674858094
Epoch: 7211, Batch Gradient Norm: 18.020823634503103
Epoch: 7211, Batch Gradient Norm after: 18.020823634503103
Epoch 7212/10000, Prediction Accuracy = 64.43%, Loss = 0.31229588985443113
Epoch: 7212, Batch Gradient Norm: 16.644880714625554
Epoch: 7212, Batch Gradient Norm after: 16.644880714625554
Epoch 7213/10000, Prediction Accuracy = 64.13000000000001%, Loss = 0.3105776786804199
Epoch: 7213, Batch Gradient Norm: 15.837999627456705
Epoch: 7213, Batch Gradient Norm after: 15.837999627456705
Epoch 7214/10000, Prediction Accuracy = 64.266%, Loss = 0.3099039375782013
Epoch: 7214, Batch Gradient Norm: 11.529566636381068
Epoch: 7214, Batch Gradient Norm after: 11.529566636381068
Epoch 7215/10000, Prediction Accuracy = 64.37%, Loss = 0.30476707220077515
Epoch: 7215, Batch Gradient Norm: 12.580418089473216
Epoch: 7215, Batch Gradient Norm after: 12.580418089473216
Epoch 7216/10000, Prediction Accuracy = 64.23800000000001%, Loss = 0.3054143309593201
Epoch: 7216, Batch Gradient Norm: 10.052814664756538
Epoch: 7216, Batch Gradient Norm after: 10.052814664756538
Epoch 7217/10000, Prediction Accuracy = 64.312%, Loss = 0.3010313451290131
Epoch: 7217, Batch Gradient Norm: 12.100926257019067
Epoch: 7217, Batch Gradient Norm after: 12.100926257019067
Epoch 7218/10000, Prediction Accuracy = 64.30800000000002%, Loss = 0.30396938920021055
Epoch: 7218, Batch Gradient Norm: 11.579993338283819
Epoch: 7218, Batch Gradient Norm after: 11.579993338283819
Epoch 7219/10000, Prediction Accuracy = 64.286%, Loss = 0.30352833271026614
Epoch: 7219, Batch Gradient Norm: 13.420842958416717
Epoch: 7219, Batch Gradient Norm after: 13.420842958416717
Epoch 7220/10000, Prediction Accuracy = 64.32%, Loss = 0.30461515188217164
Epoch: 7220, Batch Gradient Norm: 13.214505652081133
Epoch: 7220, Batch Gradient Norm after: 13.214505652081133
Epoch 7221/10000, Prediction Accuracy = 64.328%, Loss = 0.30549474358558654
Epoch: 7221, Batch Gradient Norm: 13.000263652800568
Epoch: 7221, Batch Gradient Norm after: 13.000263652800568
Epoch 7222/10000, Prediction Accuracy = 64.232%, Loss = 0.3053743541240692
Epoch: 7222, Batch Gradient Norm: 10.704202128996696
Epoch: 7222, Batch Gradient Norm after: 10.704202128996696
Epoch 7223/10000, Prediction Accuracy = 64.308%, Loss = 0.30365448594093325
Epoch: 7223, Batch Gradient Norm: 12.492901613922331
Epoch: 7223, Batch Gradient Norm after: 12.492901613922331
Epoch 7224/10000, Prediction Accuracy = 64.254%, Loss = 0.3068244636058807
Epoch: 7224, Batch Gradient Norm: 11.00144069206503
Epoch: 7224, Batch Gradient Norm after: 11.00144069206503
Epoch 7225/10000, Prediction Accuracy = 64.216%, Loss = 0.3056021392345428
Epoch: 7225, Batch Gradient Norm: 13.841943928721824
Epoch: 7225, Batch Gradient Norm after: 13.841943928721824
Epoch 7226/10000, Prediction Accuracy = 64.18199999999999%, Loss = 0.30820807814598083
Epoch: 7226, Batch Gradient Norm: 15.165313045658426
Epoch: 7226, Batch Gradient Norm after: 15.165313045658426
Epoch 7227/10000, Prediction Accuracy = 64.284%, Loss = 0.3091273009777069
Epoch: 7227, Batch Gradient Norm: 14.731081386258277
Epoch: 7227, Batch Gradient Norm after: 14.731081386258277
Epoch 7228/10000, Prediction Accuracy = 64.204%, Loss = 0.3081052482128143
Epoch: 7228, Batch Gradient Norm: 12.478038556359856
Epoch: 7228, Batch Gradient Norm after: 12.478038556359856
Epoch 7229/10000, Prediction Accuracy = 64.26599999999999%, Loss = 0.3041575372219086
Epoch: 7229, Batch Gradient Norm: 14.351419469536314
Epoch: 7229, Batch Gradient Norm after: 14.351419469536314
Epoch 7230/10000, Prediction Accuracy = 64.316%, Loss = 0.3058411955833435
Epoch: 7230, Batch Gradient Norm: 13.442517167444539
Epoch: 7230, Batch Gradient Norm after: 13.442517167444539
Epoch 7231/10000, Prediction Accuracy = 64.22999999999999%, Loss = 0.30644973516464236
Epoch: 7231, Batch Gradient Norm: 14.093899514787195
Epoch: 7231, Batch Gradient Norm after: 14.093899514787195
Epoch 7232/10000, Prediction Accuracy = 64.24199999999999%, Loss = 0.307528555393219
Epoch: 7232, Batch Gradient Norm: 12.09889996493563
Epoch: 7232, Batch Gradient Norm after: 12.09889996493563
Epoch 7233/10000, Prediction Accuracy = 64.298%, Loss = 0.3043818771839142
Epoch: 7233, Batch Gradient Norm: 13.261676407118216
Epoch: 7233, Batch Gradient Norm after: 13.261676407118216
Epoch 7234/10000, Prediction Accuracy = 64.39200000000001%, Loss = 0.30665306448936464
Epoch: 7234, Batch Gradient Norm: 14.432366206456644
Epoch: 7234, Batch Gradient Norm after: 14.432366206456644
Epoch 7235/10000, Prediction Accuracy = 64.17999999999999%, Loss = 0.3059010565280914
Epoch: 7235, Batch Gradient Norm: 13.815008880063802
Epoch: 7235, Batch Gradient Norm after: 13.815008880063802
Epoch 7236/10000, Prediction Accuracy = 64.342%, Loss = 0.30480790734291074
Epoch: 7236, Batch Gradient Norm: 10.586893965751976
Epoch: 7236, Batch Gradient Norm after: 10.586893965751976
Epoch 7237/10000, Prediction Accuracy = 64.272%, Loss = 0.30384241938591006
Epoch: 7237, Batch Gradient Norm: 11.723825421330744
Epoch: 7237, Batch Gradient Norm after: 11.723825421330744
Epoch 7238/10000, Prediction Accuracy = 64.30199999999999%, Loss = 0.3026520133018494
Epoch: 7238, Batch Gradient Norm: 12.409876451506056
Epoch: 7238, Batch Gradient Norm after: 12.409876451506056
Epoch 7239/10000, Prediction Accuracy = 64.28%, Loss = 0.3044020652770996
Epoch: 7239, Batch Gradient Norm: 13.703992105053722
Epoch: 7239, Batch Gradient Norm after: 13.703992105053722
Epoch 7240/10000, Prediction Accuracy = 64.422%, Loss = 0.30400580167770386
Epoch: 7240, Batch Gradient Norm: 16.33436315693878
Epoch: 7240, Batch Gradient Norm after: 16.33436315693878
Epoch 7241/10000, Prediction Accuracy = 64.224%, Loss = 0.3108056366443634
Epoch: 7241, Batch Gradient Norm: 17.675838379431834
Epoch: 7241, Batch Gradient Norm after: 17.675838379431834
Epoch 7242/10000, Prediction Accuracy = 64.232%, Loss = 0.31033395528793334
Epoch: 7242, Batch Gradient Norm: 16.394591303077686
Epoch: 7242, Batch Gradient Norm after: 16.394591303077686
Epoch 7243/10000, Prediction Accuracy = 64.336%, Loss = 0.30757989883422854
Epoch: 7243, Batch Gradient Norm: 15.414546001830786
Epoch: 7243, Batch Gradient Norm after: 15.414546001830786
Epoch 7244/10000, Prediction Accuracy = 64.22999999999999%, Loss = 0.30789350867271426
Epoch: 7244, Batch Gradient Norm: 13.204146358765456
Epoch: 7244, Batch Gradient Norm after: 13.204146358765456
Epoch 7245/10000, Prediction Accuracy = 64.344%, Loss = 0.3060006558895111
Epoch: 7245, Batch Gradient Norm: 12.925275136597609
Epoch: 7245, Batch Gradient Norm after: 12.925275136597609
Epoch 7246/10000, Prediction Accuracy = 64.244%, Loss = 0.30503820180892943
Epoch: 7246, Batch Gradient Norm: 15.728900918845085
Epoch: 7246, Batch Gradient Norm after: 15.728900918845085
Epoch 7247/10000, Prediction Accuracy = 64.264%, Loss = 0.3090150892734528
Epoch: 7247, Batch Gradient Norm: 14.426094086112217
Epoch: 7247, Batch Gradient Norm after: 14.426094086112217
Epoch 7248/10000, Prediction Accuracy = 64.37%, Loss = 0.3059622645378113
Epoch: 7248, Batch Gradient Norm: 15.887847306205865
Epoch: 7248, Batch Gradient Norm after: 15.887847306205865
Epoch 7249/10000, Prediction Accuracy = 64.322%, Loss = 0.31017279624938965
Epoch: 7249, Batch Gradient Norm: 18.20873469905643
Epoch: 7249, Batch Gradient Norm after: 17.328235026576753
Epoch 7250/10000, Prediction Accuracy = 64.26400000000001%, Loss = 0.31404905915260317
Epoch: 7250, Batch Gradient Norm: 15.692062912741847
Epoch: 7250, Batch Gradient Norm after: 15.692062912741847
Epoch 7251/10000, Prediction Accuracy = 64.13399999999999%, Loss = 0.3088447391986847
Epoch: 7251, Batch Gradient Norm: 17.116054070606875
Epoch: 7251, Batch Gradient Norm after: 17.116054070606875
Epoch 7252/10000, Prediction Accuracy = 64.16600000000001%, Loss = 0.3102673768997192
Epoch: 7252, Batch Gradient Norm: 16.96984002707847
Epoch: 7252, Batch Gradient Norm after: 16.269041781421112
Epoch 7253/10000, Prediction Accuracy = 64.34799999999998%, Loss = 0.30861185789108275
Epoch: 7253, Batch Gradient Norm: 16.4164649505674
Epoch: 7253, Batch Gradient Norm after: 16.4164649505674
Epoch 7254/10000, Prediction Accuracy = 64.45%, Loss = 0.3088950514793396
Epoch: 7254, Batch Gradient Norm: 13.95057249303912
Epoch: 7254, Batch Gradient Norm after: 13.95057249303912
Epoch 7255/10000, Prediction Accuracy = 64.33800000000001%, Loss = 0.30714291930198667
Epoch: 7255, Batch Gradient Norm: 15.923685397577497
Epoch: 7255, Batch Gradient Norm after: 15.923685397577497
Epoch 7256/10000, Prediction Accuracy = 64.308%, Loss = 0.30762864351272584
Epoch: 7256, Batch Gradient Norm: 15.526015313818592
Epoch: 7256, Batch Gradient Norm after: 15.526015313818592
Epoch 7257/10000, Prediction Accuracy = 64.386%, Loss = 0.30664584040641785
Epoch: 7257, Batch Gradient Norm: 15.398751460813335
Epoch: 7257, Batch Gradient Norm after: 15.398751460813335
Epoch 7258/10000, Prediction Accuracy = 64.238%, Loss = 0.3085167109966278
Epoch: 7258, Batch Gradient Norm: 12.60369508790716
Epoch: 7258, Batch Gradient Norm after: 12.60369508790716
Epoch 7259/10000, Prediction Accuracy = 64.286%, Loss = 0.3057450294494629
Epoch: 7259, Batch Gradient Norm: 11.917396276983727
Epoch: 7259, Batch Gradient Norm after: 11.917396276983727
Epoch 7260/10000, Prediction Accuracy = 64.19800000000001%, Loss = 0.30358762145042417
Epoch: 7260, Batch Gradient Norm: 14.196512794299315
Epoch: 7260, Batch Gradient Norm after: 14.196512794299315
Epoch 7261/10000, Prediction Accuracy = 64.218%, Loss = 0.30950409173965454
Epoch: 7261, Batch Gradient Norm: 13.595287048410778
Epoch: 7261, Batch Gradient Norm after: 13.595287048410778
Epoch 7262/10000, Prediction Accuracy = 64.27799999999999%, Loss = 0.30561027526855467
Epoch: 7262, Batch Gradient Norm: 12.760075249001966
Epoch: 7262, Batch Gradient Norm after: 12.760075249001966
Epoch 7263/10000, Prediction Accuracy = 64.326%, Loss = 0.30644155144691465
Epoch: 7263, Batch Gradient Norm: 14.806542281136688
Epoch: 7263, Batch Gradient Norm after: 14.806542281136688
Epoch 7264/10000, Prediction Accuracy = 64.232%, Loss = 0.30781657695770265
Epoch: 7264, Batch Gradient Norm: 13.99173873576654
Epoch: 7264, Batch Gradient Norm after: 13.99173873576654
Epoch 7265/10000, Prediction Accuracy = 64.29400000000001%, Loss = 0.30755985975265504
Epoch: 7265, Batch Gradient Norm: 14.010855218057033
Epoch: 7265, Batch Gradient Norm after: 14.010855218057033
Epoch 7266/10000, Prediction Accuracy = 64.32000000000001%, Loss = 0.3058736264705658
Epoch: 7266, Batch Gradient Norm: 12.55056011532119
Epoch: 7266, Batch Gradient Norm after: 12.55056011532119
Epoch 7267/10000, Prediction Accuracy = 64.256%, Loss = 0.3022366762161255
Epoch: 7267, Batch Gradient Norm: 14.133513354301659
Epoch: 7267, Batch Gradient Norm after: 14.133513354301659
Epoch 7268/10000, Prediction Accuracy = 64.30199999999999%, Loss = 0.3055712044239044
Epoch: 7268, Batch Gradient Norm: 15.449899047193668
Epoch: 7268, Batch Gradient Norm after: 15.449899047193668
Epoch 7269/10000, Prediction Accuracy = 64.134%, Loss = 0.3129042565822601
Epoch: 7269, Batch Gradient Norm: 17.804988014775148
Epoch: 7269, Batch Gradient Norm after: 17.79445436056762
Epoch 7270/10000, Prediction Accuracy = 64.252%, Loss = 0.31045992374420167
Epoch: 7270, Batch Gradient Norm: 14.46232144010334
Epoch: 7270, Batch Gradient Norm after: 14.46232144010334
Epoch 7271/10000, Prediction Accuracy = 64.38000000000001%, Loss = 0.30691147446632383
Epoch: 7271, Batch Gradient Norm: 12.411346170629695
Epoch: 7271, Batch Gradient Norm after: 12.411346170629695
Epoch 7272/10000, Prediction Accuracy = 64.38%, Loss = 0.30372580885887146
Epoch: 7272, Batch Gradient Norm: 13.944613597581561
Epoch: 7272, Batch Gradient Norm after: 13.944613597581561
Epoch 7273/10000, Prediction Accuracy = 64.294%, Loss = 0.3047787606716156
Epoch: 7273, Batch Gradient Norm: 15.139712887369779
Epoch: 7273, Batch Gradient Norm after: 15.139712887369779
Epoch 7274/10000, Prediction Accuracy = 64.26%, Loss = 0.30781174302101133
Epoch: 7274, Batch Gradient Norm: 15.552342547007617
Epoch: 7274, Batch Gradient Norm after: 15.552342547007617
Epoch 7275/10000, Prediction Accuracy = 64.32000000000001%, Loss = 0.305857914686203
Epoch: 7275, Batch Gradient Norm: 14.561331086011554
Epoch: 7275, Batch Gradient Norm after: 14.561331086011554
Epoch 7276/10000, Prediction Accuracy = 64.434%, Loss = 0.3063255250453949
Epoch: 7276, Batch Gradient Norm: 13.356373397059379
Epoch: 7276, Batch Gradient Norm after: 13.356373397059379
Epoch 7277/10000, Prediction Accuracy = 64.292%, Loss = 0.3061040759086609
Epoch: 7277, Batch Gradient Norm: 15.410700734998453
Epoch: 7277, Batch Gradient Norm after: 15.410700734998453
Epoch 7278/10000, Prediction Accuracy = 64.28%, Loss = 0.31095221638679504
Epoch: 7278, Batch Gradient Norm: 12.715486634037546
Epoch: 7278, Batch Gradient Norm after: 12.715486634037546
Epoch 7279/10000, Prediction Accuracy = 64.33200000000001%, Loss = 0.30341110825538636
Epoch: 7279, Batch Gradient Norm: 11.858020093438576
Epoch: 7279, Batch Gradient Norm after: 11.858020093438576
Epoch 7280/10000, Prediction Accuracy = 64.312%, Loss = 0.3043109536170959
Epoch: 7280, Batch Gradient Norm: 12.468450030708977
Epoch: 7280, Batch Gradient Norm after: 12.468450030708977
Epoch 7281/10000, Prediction Accuracy = 64.364%, Loss = 0.3044303834438324
Epoch: 7281, Batch Gradient Norm: 12.174918905743459
Epoch: 7281, Batch Gradient Norm after: 12.174918905743459
Epoch 7282/10000, Prediction Accuracy = 64.324%, Loss = 0.30256646275520327
Epoch: 7282, Batch Gradient Norm: 10.645018479286303
Epoch: 7282, Batch Gradient Norm after: 10.645018479286303
Epoch 7283/10000, Prediction Accuracy = 64.434%, Loss = 0.30117306113243103
Epoch: 7283, Batch Gradient Norm: 13.849919502904406
Epoch: 7283, Batch Gradient Norm after: 13.849919502904406
Epoch 7284/10000, Prediction Accuracy = 64.248%, Loss = 0.306255042552948
Epoch: 7284, Batch Gradient Norm: 15.132989008680699
Epoch: 7284, Batch Gradient Norm after: 15.132989008680699
Epoch 7285/10000, Prediction Accuracy = 64.36800000000001%, Loss = 0.3078435122966766
Epoch: 7285, Batch Gradient Norm: 15.249186868089868
Epoch: 7285, Batch Gradient Norm after: 15.249186868089868
Epoch 7286/10000, Prediction Accuracy = 64.194%, Loss = 0.3102757573127747
Epoch: 7286, Batch Gradient Norm: 17.819932076734794
Epoch: 7286, Batch Gradient Norm after: 17.631458873202092
Epoch 7287/10000, Prediction Accuracy = 64.406%, Loss = 0.3093430697917938
Epoch: 7287, Batch Gradient Norm: 18.3915322663525
Epoch: 7287, Batch Gradient Norm after: 18.3915322663525
Epoch 7288/10000, Prediction Accuracy = 64.428%, Loss = 0.31019397377967833
Epoch: 7288, Batch Gradient Norm: 17.467252024363898
Epoch: 7288, Batch Gradient Norm after: 17.467252024363898
Epoch 7289/10000, Prediction Accuracy = 64.32999999999998%, Loss = 0.309319806098938
Epoch: 7289, Batch Gradient Norm: 20.303960004736243
Epoch: 7289, Batch Gradient Norm after: 18.81937990540303
Epoch 7290/10000, Prediction Accuracy = 64.412%, Loss = 0.31354486346244814
Epoch: 7290, Batch Gradient Norm: 19.484592070764617
Epoch: 7290, Batch Gradient Norm after: 18.890188431038254
Epoch 7291/10000, Prediction Accuracy = 64.28%, Loss = 0.31317713260650637
Epoch: 7291, Batch Gradient Norm: 16.30185755232277
Epoch: 7291, Batch Gradient Norm after: 16.30185755232277
Epoch 7292/10000, Prediction Accuracy = 64.19%, Loss = 0.30701971650123594
Epoch: 7292, Batch Gradient Norm: 14.60017729214044
Epoch: 7292, Batch Gradient Norm after: 14.60017729214044
Epoch 7293/10000, Prediction Accuracy = 64.304%, Loss = 0.30644250512123106
Epoch: 7293, Batch Gradient Norm: 15.316312182583264
Epoch: 7293, Batch Gradient Norm after: 15.316312182583264
Epoch 7294/10000, Prediction Accuracy = 64.274%, Loss = 0.3074381113052368
Epoch: 7294, Batch Gradient Norm: 14.182582214869404
Epoch: 7294, Batch Gradient Norm after: 14.182582214869404
Epoch 7295/10000, Prediction Accuracy = 64.282%, Loss = 0.30605739951133726
Epoch: 7295, Batch Gradient Norm: 14.434849810094384
Epoch: 7295, Batch Gradient Norm after: 14.307339754600578
Epoch 7296/10000, Prediction Accuracy = 64.312%, Loss = 0.30691688060760497
Epoch: 7296, Batch Gradient Norm: 15.055198195845314
Epoch: 7296, Batch Gradient Norm after: 15.055198195845314
Epoch 7297/10000, Prediction Accuracy = 64.45%, Loss = 0.3064539015293121
Epoch: 7297, Batch Gradient Norm: 13.012472732680752
Epoch: 7297, Batch Gradient Norm after: 13.012472732680752
Epoch 7298/10000, Prediction Accuracy = 64.28599999999999%, Loss = 0.303209525346756
Epoch: 7298, Batch Gradient Norm: 12.494711851700021
Epoch: 7298, Batch Gradient Norm after: 12.494711851700021
Epoch 7299/10000, Prediction Accuracy = 64.36%, Loss = 0.30385537147521974
Epoch: 7299, Batch Gradient Norm: 11.176382749251875
Epoch: 7299, Batch Gradient Norm after: 11.176382749251875
Epoch 7300/10000, Prediction Accuracy = 64.48400000000001%, Loss = 0.30131895542144777
Epoch: 7300, Batch Gradient Norm: 13.801767483797336
Epoch: 7300, Batch Gradient Norm after: 13.801767483797336
Epoch 7301/10000, Prediction Accuracy = 64.334%, Loss = 0.3043991565704346
Epoch: 7301, Batch Gradient Norm: 13.878597331656355
Epoch: 7301, Batch Gradient Norm after: 13.878597331656355
Epoch 7302/10000, Prediction Accuracy = 64.348%, Loss = 0.3048888087272644
Epoch: 7302, Batch Gradient Norm: 13.066382564394917
Epoch: 7302, Batch Gradient Norm after: 13.066382564394917
Epoch 7303/10000, Prediction Accuracy = 64.328%, Loss = 0.3032706916332245
Epoch: 7303, Batch Gradient Norm: 14.357616530606377
Epoch: 7303, Batch Gradient Norm after: 14.357616530606377
Epoch 7304/10000, Prediction Accuracy = 64.196%, Loss = 0.3069035291671753
Epoch: 7304, Batch Gradient Norm: 14.10117978339856
Epoch: 7304, Batch Gradient Norm after: 14.10117978339856
Epoch 7305/10000, Prediction Accuracy = 64.356%, Loss = 0.30476675033569334
Epoch: 7305, Batch Gradient Norm: 14.838989048941457
Epoch: 7305, Batch Gradient Norm after: 14.838989048941457
Epoch 7306/10000, Prediction Accuracy = 64.356%, Loss = 0.30798713564872743
Epoch: 7306, Batch Gradient Norm: 14.213912050595495
Epoch: 7306, Batch Gradient Norm after: 14.213912050595495
Epoch 7307/10000, Prediction Accuracy = 64.46799999999999%, Loss = 0.3079721510410309
Epoch: 7307, Batch Gradient Norm: 13.345790664835132
Epoch: 7307, Batch Gradient Norm after: 13.345790664835132
Epoch 7308/10000, Prediction Accuracy = 64.366%, Loss = 0.3051945924758911
Epoch: 7308, Batch Gradient Norm: 16.90400425022575
Epoch: 7308, Batch Gradient Norm after: 16.560901601313212
Epoch 7309/10000, Prediction Accuracy = 64.286%, Loss = 0.30625011324882506
Epoch: 7309, Batch Gradient Norm: 16.260211685964737
Epoch: 7309, Batch Gradient Norm after: 16.260211685964737
Epoch 7310/10000, Prediction Accuracy = 64.35400000000001%, Loss = 0.3090877592563629
Epoch: 7310, Batch Gradient Norm: 16.809720143808562
Epoch: 7310, Batch Gradient Norm after: 16.809720143808562
Epoch 7311/10000, Prediction Accuracy = 64.34799999999998%, Loss = 0.3100963234901428
Epoch: 7311, Batch Gradient Norm: 17.330336796829396
Epoch: 7311, Batch Gradient Norm after: 16.66803211312866
Epoch 7312/10000, Prediction Accuracy = 64.41%, Loss = 0.3079810976982117
Epoch: 7312, Batch Gradient Norm: 15.730775325508262
Epoch: 7312, Batch Gradient Norm after: 15.71516631693646
Epoch 7313/10000, Prediction Accuracy = 64.29400000000001%, Loss = 0.3073145687580109
Epoch: 7313, Batch Gradient Norm: 13.624707520544801
Epoch: 7313, Batch Gradient Norm after: 13.624707520544801
Epoch 7314/10000, Prediction Accuracy = 64.48599999999999%, Loss = 0.3042227327823639
Epoch: 7314, Batch Gradient Norm: 13.136148381084377
Epoch: 7314, Batch Gradient Norm after: 13.136148381084377
Epoch 7315/10000, Prediction Accuracy = 64.38399999999999%, Loss = 0.3024794578552246
Epoch: 7315, Batch Gradient Norm: 11.59830195067888
Epoch: 7315, Batch Gradient Norm after: 11.59830195067888
Epoch 7316/10000, Prediction Accuracy = 64.462%, Loss = 0.3020057678222656
Epoch: 7316, Batch Gradient Norm: 15.645827220285751
Epoch: 7316, Batch Gradient Norm after: 15.645827220285751
Epoch 7317/10000, Prediction Accuracy = 64.27799999999999%, Loss = 0.3073925793170929
Epoch: 7317, Batch Gradient Norm: 17.072760792751385
Epoch: 7317, Batch Gradient Norm after: 17.072760792751385
Epoch 7318/10000, Prediction Accuracy = 64.32399999999998%, Loss = 0.3111860632896423
Epoch: 7318, Batch Gradient Norm: 17.62414126493194
Epoch: 7318, Batch Gradient Norm after: 17.62414126493194
Epoch 7319/10000, Prediction Accuracy = 64.42599999999999%, Loss = 0.3094362676143646
Epoch: 7319, Batch Gradient Norm: 17.887478688117977
Epoch: 7319, Batch Gradient Norm after: 17.88053659942501
Epoch 7320/10000, Prediction Accuracy = 64.376%, Loss = 0.30900992155075074
Epoch: 7320, Batch Gradient Norm: 17.32789337579946
Epoch: 7320, Batch Gradient Norm after: 17.32789337579946
Epoch 7321/10000, Prediction Accuracy = 64.18%, Loss = 0.31097633838653566
Epoch: 7321, Batch Gradient Norm: 16.066889099607483
Epoch: 7321, Batch Gradient Norm after: 16.066889099607483
Epoch 7322/10000, Prediction Accuracy = 64.232%, Loss = 0.3098840653896332
Epoch: 7322, Batch Gradient Norm: 12.158418474621682
Epoch: 7322, Batch Gradient Norm after: 12.158418474621682
Epoch 7323/10000, Prediction Accuracy = 64.23800000000001%, Loss = 0.3027550637722015
Epoch: 7323, Batch Gradient Norm: 11.623437673233326
Epoch: 7323, Batch Gradient Norm after: 11.623437673233326
Epoch 7324/10000, Prediction Accuracy = 64.22399999999999%, Loss = 0.30348957777023317
Epoch: 7324, Batch Gradient Norm: 13.626129649496107
Epoch: 7324, Batch Gradient Norm after: 13.626129649496107
Epoch 7325/10000, Prediction Accuracy = 64.252%, Loss = 0.3061620891094208
Epoch: 7325, Batch Gradient Norm: 17.30510119931342
Epoch: 7325, Batch Gradient Norm after: 17.30510119931342
Epoch 7326/10000, Prediction Accuracy = 64.39%, Loss = 0.3099508166313171
Epoch: 7326, Batch Gradient Norm: 16.11004771047639
Epoch: 7326, Batch Gradient Norm after: 16.11004771047639
Epoch 7327/10000, Prediction Accuracy = 64.358%, Loss = 0.30641295313835143
Epoch: 7327, Batch Gradient Norm: 14.635134578230911
Epoch: 7327, Batch Gradient Norm after: 14.635134578230911
Epoch 7328/10000, Prediction Accuracy = 64.19200000000001%, Loss = 0.3071061372756958
Epoch: 7328, Batch Gradient Norm: 13.320810796138415
Epoch: 7328, Batch Gradient Norm after: 13.320810796138415
Epoch 7329/10000, Prediction Accuracy = 64.348%, Loss = 0.30405332446098327
Epoch: 7329, Batch Gradient Norm: 15.260593236652218
Epoch: 7329, Batch Gradient Norm after: 15.260593236652218
Epoch 7330/10000, Prediction Accuracy = 64.35%, Loss = 0.3048143744468689
Epoch: 7330, Batch Gradient Norm: 14.798397984539918
Epoch: 7330, Batch Gradient Norm after: 14.798397984539918
Epoch 7331/10000, Prediction Accuracy = 64.40400000000001%, Loss = 0.3048398017883301
Epoch: 7331, Batch Gradient Norm: 16.253157936347993
Epoch: 7331, Batch Gradient Norm after: 16.253157936347993
Epoch 7332/10000, Prediction Accuracy = 64.424%, Loss = 0.30515331625938413
Epoch: 7332, Batch Gradient Norm: 15.331046616744073
Epoch: 7332, Batch Gradient Norm after: 15.331046616744073
Epoch 7333/10000, Prediction Accuracy = 64.35%, Loss = 0.3091228723526001
Epoch: 7333, Batch Gradient Norm: 14.668899484495341
Epoch: 7333, Batch Gradient Norm after: 14.668899484495341
Epoch 7334/10000, Prediction Accuracy = 64.344%, Loss = 0.3064213514328003
Epoch: 7334, Batch Gradient Norm: 13.226084271959879
Epoch: 7334, Batch Gradient Norm after: 13.226084271959879
Epoch 7335/10000, Prediction Accuracy = 64.212%, Loss = 0.30865992307662965
Epoch: 7335, Batch Gradient Norm: 12.746808645865585
Epoch: 7335, Batch Gradient Norm after: 12.746808645865585
Epoch 7336/10000, Prediction Accuracy = 64.374%, Loss = 0.30431734919548037
Epoch: 7336, Batch Gradient Norm: 13.376694719585968
Epoch: 7336, Batch Gradient Norm after: 13.376694719585968
Epoch 7337/10000, Prediction Accuracy = 64.366%, Loss = 0.3035548746585846
Epoch: 7337, Batch Gradient Norm: 10.041241413866063
Epoch: 7337, Batch Gradient Norm after: 10.041241413866063
Epoch 7338/10000, Prediction Accuracy = 64.27000000000001%, Loss = 0.29871660470962524
Epoch: 7338, Batch Gradient Norm: 11.077736789917736
Epoch: 7338, Batch Gradient Norm after: 11.077736789917736
Epoch 7339/10000, Prediction Accuracy = 64.23599999999999%, Loss = 0.301185667514801
Epoch: 7339, Batch Gradient Norm: 12.400574793261073
Epoch: 7339, Batch Gradient Norm after: 12.400574793261073
Epoch 7340/10000, Prediction Accuracy = 64.23599999999999%, Loss = 0.3035737335681915
Epoch: 7340, Batch Gradient Norm: 15.71682645301926
Epoch: 7340, Batch Gradient Norm after: 15.71682645301926
Epoch 7341/10000, Prediction Accuracy = 64.298%, Loss = 0.30849661827087405
Epoch: 7341, Batch Gradient Norm: 17.920470213379396
Epoch: 7341, Batch Gradient Norm after: 17.920470213379396
Epoch 7342/10000, Prediction Accuracy = 64.348%, Loss = 0.30974170565605164
Epoch: 7342, Batch Gradient Norm: 16.626999383309993
Epoch: 7342, Batch Gradient Norm after: 16.626999383309993
Epoch 7343/10000, Prediction Accuracy = 64.28999999999999%, Loss = 0.309231036901474
Epoch: 7343, Batch Gradient Norm: 14.749570058551319
Epoch: 7343, Batch Gradient Norm after: 14.749570058551319
Epoch 7344/10000, Prediction Accuracy = 64.33200000000001%, Loss = 0.30352399349212644
Epoch: 7344, Batch Gradient Norm: 13.270604077697914
Epoch: 7344, Batch Gradient Norm after: 13.270604077697914
Epoch 7345/10000, Prediction Accuracy = 64.428%, Loss = 0.3045901656150818
Epoch: 7345, Batch Gradient Norm: 14.199556758294458
Epoch: 7345, Batch Gradient Norm after: 14.199556758294458
Epoch 7346/10000, Prediction Accuracy = 64.35999999999999%, Loss = 0.30538337826728823
Epoch: 7346, Batch Gradient Norm: 12.937217152819596
Epoch: 7346, Batch Gradient Norm after: 12.937217152819596
Epoch 7347/10000, Prediction Accuracy = 64.32%, Loss = 0.30429546236991883
Epoch: 7347, Batch Gradient Norm: 13.103855014578256
Epoch: 7347, Batch Gradient Norm after: 13.103855014578256
Epoch 7348/10000, Prediction Accuracy = 64.38199999999999%, Loss = 0.3057056188583374
Epoch: 7348, Batch Gradient Norm: 13.493154437047252
Epoch: 7348, Batch Gradient Norm after: 13.493154437047252
Epoch 7349/10000, Prediction Accuracy = 64.37%, Loss = 0.3044244349002838
Epoch: 7349, Batch Gradient Norm: 13.780289972175614
Epoch: 7349, Batch Gradient Norm after: 13.780289972175614
Epoch 7350/10000, Prediction Accuracy = 64.47%, Loss = 0.30429883003234864
Epoch: 7350, Batch Gradient Norm: 15.748833185622622
Epoch: 7350, Batch Gradient Norm after: 15.748833185622622
Epoch 7351/10000, Prediction Accuracy = 64.252%, Loss = 0.30843613743782045
Epoch: 7351, Batch Gradient Norm: 15.533686499352866
Epoch: 7351, Batch Gradient Norm after: 15.533686499352866
Epoch 7352/10000, Prediction Accuracy = 64.294%, Loss = 0.3076527714729309
Epoch: 7352, Batch Gradient Norm: 18.131112727272797
Epoch: 7352, Batch Gradient Norm after: 18.131112727272797
Epoch 7353/10000, Prediction Accuracy = 64.292%, Loss = 0.3115591168403625
Epoch: 7353, Batch Gradient Norm: 12.341102937436565
Epoch: 7353, Batch Gradient Norm after: 12.341102937436565
Epoch 7354/10000, Prediction Accuracy = 64.33200000000001%, Loss = 0.30132265090942384
Epoch: 7354, Batch Gradient Norm: 14.798260217231212
Epoch: 7354, Batch Gradient Norm after: 14.798260217231212
Epoch 7355/10000, Prediction Accuracy = 64.454%, Loss = 0.3062728226184845
Epoch: 7355, Batch Gradient Norm: 16.41626989872763
Epoch: 7355, Batch Gradient Norm after: 16.358346688436857
Epoch 7356/10000, Prediction Accuracy = 64.356%, Loss = 0.30900301337242125
Epoch: 7356, Batch Gradient Norm: 13.14134653191514
Epoch: 7356, Batch Gradient Norm after: 13.14134653191514
Epoch 7357/10000, Prediction Accuracy = 64.33800000000001%, Loss = 0.3041031897068024
Epoch: 7357, Batch Gradient Norm: 12.977770602026839
Epoch: 7357, Batch Gradient Norm after: 12.977770602026839
Epoch 7358/10000, Prediction Accuracy = 64.35600000000001%, Loss = 0.3023788869380951
Epoch: 7358, Batch Gradient Norm: 12.50169319008312
Epoch: 7358, Batch Gradient Norm after: 12.50169319008312
Epoch 7359/10000, Prediction Accuracy = 64.344%, Loss = 0.30369032025337217
Epoch: 7359, Batch Gradient Norm: 14.761422817523497
Epoch: 7359, Batch Gradient Norm after: 14.761422817523497
Epoch 7360/10000, Prediction Accuracy = 64.29400000000001%, Loss = 0.3070900499820709
Epoch: 7360, Batch Gradient Norm: 13.47107676695768
Epoch: 7360, Batch Gradient Norm after: 13.47107676695768
Epoch 7361/10000, Prediction Accuracy = 64.376%, Loss = 0.303882372379303
Epoch: 7361, Batch Gradient Norm: 14.319880508773204
Epoch: 7361, Batch Gradient Norm after: 14.319880508773204
Epoch 7362/10000, Prediction Accuracy = 64.31%, Loss = 0.3021222412586212
Epoch: 7362, Batch Gradient Norm: 13.340053745396453
Epoch: 7362, Batch Gradient Norm after: 13.340053745396453
Epoch 7363/10000, Prediction Accuracy = 64.26400000000001%, Loss = 0.3024079382419586
Epoch: 7363, Batch Gradient Norm: 12.481427474688772
Epoch: 7363, Batch Gradient Norm after: 12.481427474688772
Epoch 7364/10000, Prediction Accuracy = 64.24199999999999%, Loss = 0.30187009572982787
Epoch: 7364, Batch Gradient Norm: 13.07086633524816
Epoch: 7364, Batch Gradient Norm after: 13.07086633524816
Epoch 7365/10000, Prediction Accuracy = 64.38399999999999%, Loss = 0.3027556538581848
Epoch: 7365, Batch Gradient Norm: 14.48494626578118
Epoch: 7365, Batch Gradient Norm after: 14.48494626578118
Epoch 7366/10000, Prediction Accuracy = 64.5%, Loss = 0.3057567238807678
Epoch: 7366, Batch Gradient Norm: 15.345920727101982
Epoch: 7366, Batch Gradient Norm after: 15.345920727101982
Epoch 7367/10000, Prediction Accuracy = 64.332%, Loss = 0.30703134536743165
Epoch: 7367, Batch Gradient Norm: 14.995279876315184
Epoch: 7367, Batch Gradient Norm after: 14.995279876315184
Epoch 7368/10000, Prediction Accuracy = 64.48400000000001%, Loss = 0.3045181453227997
Epoch: 7368, Batch Gradient Norm: 17.616465376137214
Epoch: 7368, Batch Gradient Norm after: 17.616465376137214
Epoch 7369/10000, Prediction Accuracy = 64.28%, Loss = 0.3078299582004547
Epoch: 7369, Batch Gradient Norm: 15.49325262340739
Epoch: 7369, Batch Gradient Norm after: 15.49325262340739
Epoch 7370/10000, Prediction Accuracy = 64.22%, Loss = 0.30665590763092043
Epoch: 7370, Batch Gradient Norm: 13.797786008702634
Epoch: 7370, Batch Gradient Norm after: 13.797786008702634
Epoch 7371/10000, Prediction Accuracy = 64.316%, Loss = 0.30619744658470155
Epoch: 7371, Batch Gradient Norm: 12.8577393104284
Epoch: 7371, Batch Gradient Norm after: 12.8577393104284
Epoch 7372/10000, Prediction Accuracy = 64.376%, Loss = 0.30228346586227417
Epoch: 7372, Batch Gradient Norm: 13.874515215753663
Epoch: 7372, Batch Gradient Norm after: 13.874515215753663
Epoch 7373/10000, Prediction Accuracy = 64.232%, Loss = 0.3060678541660309
Epoch: 7373, Batch Gradient Norm: 15.64763020405172
Epoch: 7373, Batch Gradient Norm after: 15.64763020405172
Epoch 7374/10000, Prediction Accuracy = 64.45599999999999%, Loss = 0.30683866143226624
Epoch: 7374, Batch Gradient Norm: 15.785790224212583
Epoch: 7374, Batch Gradient Norm after: 15.785790224212583
Epoch 7375/10000, Prediction Accuracy = 64.188%, Loss = 0.30903242230415345
Epoch: 7375, Batch Gradient Norm: 12.958571767596593
Epoch: 7375, Batch Gradient Norm after: 12.958571767596593
Epoch 7376/10000, Prediction Accuracy = 64.48800000000001%, Loss = 0.30211368203163147
Epoch: 7376, Batch Gradient Norm: 13.933355251470914
Epoch: 7376, Batch Gradient Norm after: 13.933355251470914
Epoch 7377/10000, Prediction Accuracy = 64.28%, Loss = 0.3061412572860718
Epoch: 7377, Batch Gradient Norm: 15.363871695431325
Epoch: 7377, Batch Gradient Norm after: 15.363871695431325
Epoch 7378/10000, Prediction Accuracy = 64.292%, Loss = 0.3059503436088562
Epoch: 7378, Batch Gradient Norm: 17.036051124630895
Epoch: 7378, Batch Gradient Norm after: 17.036051124630895
Epoch 7379/10000, Prediction Accuracy = 64.37%, Loss = 0.3069844484329224
Epoch: 7379, Batch Gradient Norm: 13.261229498004436
Epoch: 7379, Batch Gradient Norm after: 13.261229498004436
Epoch 7380/10000, Prediction Accuracy = 64.34%, Loss = 0.3010073721408844
Epoch: 7380, Batch Gradient Norm: 12.551611997881142
Epoch: 7380, Batch Gradient Norm after: 12.551611997881142
Epoch 7381/10000, Prediction Accuracy = 64.352%, Loss = 0.3039736211299896
Epoch: 7381, Batch Gradient Norm: 13.039350477465998
Epoch: 7381, Batch Gradient Norm after: 13.039350477465998
Epoch 7382/10000, Prediction Accuracy = 64.46000000000001%, Loss = 0.30156312584877015
Epoch: 7382, Batch Gradient Norm: 13.650459166312606
Epoch: 7382, Batch Gradient Norm after: 13.650459166312606
Epoch 7383/10000, Prediction Accuracy = 64.476%, Loss = 0.30434940457344056
Epoch: 7383, Batch Gradient Norm: 13.669852501931375
Epoch: 7383, Batch Gradient Norm after: 13.669852501931375
Epoch 7384/10000, Prediction Accuracy = 64.36000000000001%, Loss = 0.3051347494125366
Epoch: 7384, Batch Gradient Norm: 17.08137155995436
Epoch: 7384, Batch Gradient Norm after: 17.08137155995436
Epoch 7385/10000, Prediction Accuracy = 64.40599999999999%, Loss = 0.30861859321594237
Epoch: 7385, Batch Gradient Norm: 14.996594436098947
Epoch: 7385, Batch Gradient Norm after: 14.996594436098947
Epoch 7386/10000, Prediction Accuracy = 64.186%, Loss = 0.3057757139205933
Epoch: 7386, Batch Gradient Norm: 14.730342143683313
Epoch: 7386, Batch Gradient Norm after: 14.730342143683313
Epoch 7387/10000, Prediction Accuracy = 64.23799999999999%, Loss = 0.3056656181812286
Epoch: 7387, Batch Gradient Norm: 13.265725151294799
Epoch: 7387, Batch Gradient Norm after: 13.265725151294799
Epoch 7388/10000, Prediction Accuracy = 64.41%, Loss = 0.30054550170898436
Epoch: 7388, Batch Gradient Norm: 12.281038439785965
Epoch: 7388, Batch Gradient Norm after: 12.281038439785965
Epoch 7389/10000, Prediction Accuracy = 64.426%, Loss = 0.3055645763874054
Epoch: 7389, Batch Gradient Norm: 14.351379845135915
Epoch: 7389, Batch Gradient Norm after: 14.351379845135915
Epoch 7390/10000, Prediction Accuracy = 64.39%, Loss = 0.3054019570350647
Epoch: 7390, Batch Gradient Norm: 14.146955655693187
Epoch: 7390, Batch Gradient Norm after: 14.146955655693187
Epoch 7391/10000, Prediction Accuracy = 64.372%, Loss = 0.30359503626823425
Epoch: 7391, Batch Gradient Norm: 12.731283265941116
Epoch: 7391, Batch Gradient Norm after: 12.731283265941116
Epoch 7392/10000, Prediction Accuracy = 64.30999999999999%, Loss = 0.30053141713142395
Epoch: 7392, Batch Gradient Norm: 13.65865700299355
Epoch: 7392, Batch Gradient Norm after: 13.65865700299355
Epoch 7393/10000, Prediction Accuracy = 64.254%, Loss = 0.3044584572315216
Epoch: 7393, Batch Gradient Norm: 15.225455156001
Epoch: 7393, Batch Gradient Norm after: 15.225455156001
Epoch 7394/10000, Prediction Accuracy = 64.28799999999998%, Loss = 0.30826970338821413
Epoch: 7394, Batch Gradient Norm: 15.464486335437813
Epoch: 7394, Batch Gradient Norm after: 15.464486335437813
Epoch 7395/10000, Prediction Accuracy = 64.332%, Loss = 0.30457082986831663
Epoch: 7395, Batch Gradient Norm: 16.307112184981758
Epoch: 7395, Batch Gradient Norm after: 16.307112184981758
Epoch 7396/10000, Prediction Accuracy = 64.266%, Loss = 0.31020039319992065
Epoch: 7396, Batch Gradient Norm: 15.208964610880876
Epoch: 7396, Batch Gradient Norm after: 15.208964610880876
Epoch 7397/10000, Prediction Accuracy = 64.458%, Loss = 0.304767370223999
Epoch: 7397, Batch Gradient Norm: 13.664955694180126
Epoch: 7397, Batch Gradient Norm after: 13.664955694180126
Epoch 7398/10000, Prediction Accuracy = 64.444%, Loss = 0.3056728899478912
Epoch: 7398, Batch Gradient Norm: 13.205388791503509
Epoch: 7398, Batch Gradient Norm after: 13.205388791503509
Epoch 7399/10000, Prediction Accuracy = 64.388%, Loss = 0.30372993350028993
Epoch: 7399, Batch Gradient Norm: 12.209356152050415
Epoch: 7399, Batch Gradient Norm after: 12.209356152050415
Epoch 7400/10000, Prediction Accuracy = 64.38199999999999%, Loss = 0.3011023163795471
Epoch: 7400, Batch Gradient Norm: 13.452274719124869
Epoch: 7400, Batch Gradient Norm after: 13.452274719124869
Epoch 7401/10000, Prediction Accuracy = 64.41999999999999%, Loss = 0.3049401819705963
Epoch: 7401, Batch Gradient Norm: 17.676918808256616
Epoch: 7401, Batch Gradient Norm after: 17.57355906996698
Epoch 7402/10000, Prediction Accuracy = 64.39399999999999%, Loss = 0.3107761263847351
Epoch: 7402, Batch Gradient Norm: 18.68393309561699
Epoch: 7402, Batch Gradient Norm after: 18.515082996127163
Epoch 7403/10000, Prediction Accuracy = 64.232%, Loss = 0.3083894789218903
Epoch: 7403, Batch Gradient Norm: 15.560248087660785
Epoch: 7403, Batch Gradient Norm after: 15.560248087660785
Epoch 7404/10000, Prediction Accuracy = 64.43800000000002%, Loss = 0.3047767817974091
Epoch: 7404, Batch Gradient Norm: 11.288698549574478
Epoch: 7404, Batch Gradient Norm after: 11.288698549574478
Epoch 7405/10000, Prediction Accuracy = 64.47999999999999%, Loss = 0.30029483437538146
Epoch: 7405, Batch Gradient Norm: 15.900067330673922
Epoch: 7405, Batch Gradient Norm after: 15.900067330673922
Epoch 7406/10000, Prediction Accuracy = 64.328%, Loss = 0.30667566061019896
Epoch: 7406, Batch Gradient Norm: 15.330240422967293
Epoch: 7406, Batch Gradient Norm after: 15.330240422967293
Epoch 7407/10000, Prediction Accuracy = 64.34400000000001%, Loss = 0.30637669563293457
Epoch: 7407, Batch Gradient Norm: 15.339138047284598
Epoch: 7407, Batch Gradient Norm after: 15.339138047284598
Epoch 7408/10000, Prediction Accuracy = 64.45000000000002%, Loss = 0.30408085584640504
Epoch: 7408, Batch Gradient Norm: 14.833247662459591
Epoch: 7408, Batch Gradient Norm after: 14.833247662459591
Epoch 7409/10000, Prediction Accuracy = 64.244%, Loss = 0.30517261028289794
Epoch: 7409, Batch Gradient Norm: 13.757194257870543
Epoch: 7409, Batch Gradient Norm after: 13.757194257870543
Epoch 7410/10000, Prediction Accuracy = 64.43999999999998%, Loss = 0.3031761169433594
Epoch: 7410, Batch Gradient Norm: 14.442211514646186
Epoch: 7410, Batch Gradient Norm after: 14.442211514646186
Epoch 7411/10000, Prediction Accuracy = 64.34400000000001%, Loss = 0.3061340570449829
Epoch: 7411, Batch Gradient Norm: 13.077052785613619
Epoch: 7411, Batch Gradient Norm after: 13.077052785613619
Epoch 7412/10000, Prediction Accuracy = 64.244%, Loss = 0.3020483672618866
Epoch: 7412, Batch Gradient Norm: 13.000551316212773
Epoch: 7412, Batch Gradient Norm after: 13.000551316212773
Epoch 7413/10000, Prediction Accuracy = 64.36200000000001%, Loss = 0.3020143866539001
Epoch: 7413, Batch Gradient Norm: 15.276608363494574
Epoch: 7413, Batch Gradient Norm after: 15.276608363494574
Epoch 7414/10000, Prediction Accuracy = 64.362%, Loss = 0.3036290228366852
Epoch: 7414, Batch Gradient Norm: 14.577301405859934
Epoch: 7414, Batch Gradient Norm after: 14.577301405859934
Epoch 7415/10000, Prediction Accuracy = 64.412%, Loss = 0.3039697289466858
Epoch: 7415, Batch Gradient Norm: 14.92780507316532
Epoch: 7415, Batch Gradient Norm after: 14.92780507316532
Epoch 7416/10000, Prediction Accuracy = 64.35%, Loss = 0.30466272234916686
Epoch: 7416, Batch Gradient Norm: 13.807578343959023
Epoch: 7416, Batch Gradient Norm after: 13.807578343959023
Epoch 7417/10000, Prediction Accuracy = 64.45%, Loss = 0.30140756368637084
Epoch: 7417, Batch Gradient Norm: 14.754147344141495
Epoch: 7417, Batch Gradient Norm after: 14.754147344141495
Epoch 7418/10000, Prediction Accuracy = 64.386%, Loss = 0.30364813208580016
Epoch: 7418, Batch Gradient Norm: 12.629687087526412
Epoch: 7418, Batch Gradient Norm after: 12.629687087526412
Epoch 7419/10000, Prediction Accuracy = 64.39999999999999%, Loss = 0.30114338994026185
Epoch: 7419, Batch Gradient Norm: 12.66753147973842
Epoch: 7419, Batch Gradient Norm after: 12.66753147973842
Epoch 7420/10000, Prediction Accuracy = 64.51400000000001%, Loss = 0.30179981589317323
Epoch: 7420, Batch Gradient Norm: 11.482703725707243
Epoch: 7420, Batch Gradient Norm after: 11.482703725707243
Epoch 7421/10000, Prediction Accuracy = 64.37200000000001%, Loss = 0.2998659431934357
Epoch: 7421, Batch Gradient Norm: 10.926593529073436
Epoch: 7421, Batch Gradient Norm after: 10.926593529073436
Epoch 7422/10000, Prediction Accuracy = 64.39399999999999%, Loss = 0.29778777360916137
Epoch: 7422, Batch Gradient Norm: 12.257444372260704
Epoch: 7422, Batch Gradient Norm after: 12.257444372260704
Epoch 7423/10000, Prediction Accuracy = 64.292%, Loss = 0.302357679605484
Epoch: 7423, Batch Gradient Norm: 14.80612762534926
Epoch: 7423, Batch Gradient Norm after: 14.80612762534926
Epoch 7424/10000, Prediction Accuracy = 64.458%, Loss = 0.30343763828277587
Epoch: 7424, Batch Gradient Norm: 15.373536704544598
Epoch: 7424, Batch Gradient Norm after: 15.373536704544598
Epoch 7425/10000, Prediction Accuracy = 64.4%, Loss = 0.30615745186805726
Epoch: 7425, Batch Gradient Norm: 15.786454002353157
Epoch: 7425, Batch Gradient Norm after: 15.786454002353157
Epoch 7426/10000, Prediction Accuracy = 64.304%, Loss = 0.30853061079978944
Epoch: 7426, Batch Gradient Norm: 20.45637002043012
Epoch: 7426, Batch Gradient Norm after: 19.81323335505934
Epoch 7427/10000, Prediction Accuracy = 64.39%, Loss = 0.3135219573974609
Epoch: 7427, Batch Gradient Norm: 16.569965986358255
Epoch: 7427, Batch Gradient Norm after: 16.569965986358255
Epoch 7428/10000, Prediction Accuracy = 64.388%, Loss = 0.30726651549339296
Epoch: 7428, Batch Gradient Norm: 15.533484299168816
Epoch: 7428, Batch Gradient Norm after: 15.533484299168816
Epoch 7429/10000, Prediction Accuracy = 64.36600000000001%, Loss = 0.30698304176330565
Epoch: 7429, Batch Gradient Norm: 12.484641715859478
Epoch: 7429, Batch Gradient Norm after: 12.484641715859478
Epoch 7430/10000, Prediction Accuracy = 64.376%, Loss = 0.30271347165107726
Epoch: 7430, Batch Gradient Norm: 13.409464262385777
Epoch: 7430, Batch Gradient Norm after: 13.409464262385777
Epoch 7431/10000, Prediction Accuracy = 64.358%, Loss = 0.30680217146873473
Epoch: 7431, Batch Gradient Norm: 12.838591712630901
Epoch: 7431, Batch Gradient Norm after: 12.838591712630901
Epoch 7432/10000, Prediction Accuracy = 64.416%, Loss = 0.29941425919532777
Epoch: 7432, Batch Gradient Norm: 16.19946531363917
Epoch: 7432, Batch Gradient Norm after: 16.19946531363917
Epoch 7433/10000, Prediction Accuracy = 64.44999999999999%, Loss = 0.30472729206085203
Epoch: 7433, Batch Gradient Norm: 15.414112661524406
Epoch: 7433, Batch Gradient Norm after: 15.414112661524406
Epoch 7434/10000, Prediction Accuracy = 64.348%, Loss = 0.3047286868095398
Epoch: 7434, Batch Gradient Norm: 15.136978162209918
Epoch: 7434, Batch Gradient Norm after: 15.136978162209918
Epoch 7435/10000, Prediction Accuracy = 64.29599999999999%, Loss = 0.3084712505340576
Epoch: 7435, Batch Gradient Norm: 14.554681004431963
Epoch: 7435, Batch Gradient Norm after: 14.554681004431963
Epoch 7436/10000, Prediction Accuracy = 64.406%, Loss = 0.3047829866409302
Epoch: 7436, Batch Gradient Norm: 13.152529567163565
Epoch: 7436, Batch Gradient Norm after: 13.152529567163565
Epoch 7437/10000, Prediction Accuracy = 64.468%, Loss = 0.3014613926410675
Epoch: 7437, Batch Gradient Norm: 13.059726642211848
Epoch: 7437, Batch Gradient Norm after: 13.059726642211848
Epoch 7438/10000, Prediction Accuracy = 64.354%, Loss = 0.30344055891036986
Epoch: 7438, Batch Gradient Norm: 17.979036999201707
Epoch: 7438, Batch Gradient Norm after: 17.383643071723416
Epoch 7439/10000, Prediction Accuracy = 64.46600000000001%, Loss = 0.30834500789642333
Epoch: 7439, Batch Gradient Norm: 18.89399458327828
Epoch: 7439, Batch Gradient Norm after: 18.89399458327828
Epoch 7440/10000, Prediction Accuracy = 64.362%, Loss = 0.3120245635509491
Epoch: 7440, Batch Gradient Norm: 14.381256387953643
Epoch: 7440, Batch Gradient Norm after: 14.381256387953643
Epoch 7441/10000, Prediction Accuracy = 64.39%, Loss = 0.30229040384292605
Epoch: 7441, Batch Gradient Norm: 15.141646359547916
Epoch: 7441, Batch Gradient Norm after: 15.141646359547916
Epoch 7442/10000, Prediction Accuracy = 64.44199999999998%, Loss = 0.3045163631439209
Epoch: 7442, Batch Gradient Norm: 14.858503933545462
Epoch: 7442, Batch Gradient Norm after: 14.858503933545462
Epoch 7443/10000, Prediction Accuracy = 64.318%, Loss = 0.30374596118927
Epoch: 7443, Batch Gradient Norm: 12.608510502172294
Epoch: 7443, Batch Gradient Norm after: 12.608510502172294
Epoch 7444/10000, Prediction Accuracy = 64.40400000000001%, Loss = 0.2990374743938446
Epoch: 7444, Batch Gradient Norm: 12.90181190139915
Epoch: 7444, Batch Gradient Norm after: 12.90181190139915
Epoch 7445/10000, Prediction Accuracy = 64.352%, Loss = 0.3022237539291382
Epoch: 7445, Batch Gradient Norm: 14.160713097155334
Epoch: 7445, Batch Gradient Norm after: 14.160713097155334
Epoch 7446/10000, Prediction Accuracy = 64.338%, Loss = 0.3010808050632477
Epoch: 7446, Batch Gradient Norm: 12.130223173732288
Epoch: 7446, Batch Gradient Norm after: 12.130223173732288
Epoch 7447/10000, Prediction Accuracy = 64.39200000000001%, Loss = 0.3004402041435242
Epoch: 7447, Batch Gradient Norm: 14.719820000469081
Epoch: 7447, Batch Gradient Norm after: 14.719820000469081
Epoch 7448/10000, Prediction Accuracy = 64.35%, Loss = 0.3048000156879425
Epoch: 7448, Batch Gradient Norm: 13.650925261941058
Epoch: 7448, Batch Gradient Norm after: 13.650925261941058
Epoch 7449/10000, Prediction Accuracy = 64.5%, Loss = 0.29947800636291505
Epoch: 7449, Batch Gradient Norm: 14.30946955674059
Epoch: 7449, Batch Gradient Norm after: 14.30946955674059
Epoch 7450/10000, Prediction Accuracy = 64.306%, Loss = 0.30150275230407714
Epoch: 7450, Batch Gradient Norm: 16.04874584487796
Epoch: 7450, Batch Gradient Norm after: 16.04874584487796
Epoch 7451/10000, Prediction Accuracy = 64.376%, Loss = 0.3039140820503235
Epoch: 7451, Batch Gradient Norm: 13.641858519065352
Epoch: 7451, Batch Gradient Norm after: 13.641858519065352
Epoch 7452/10000, Prediction Accuracy = 64.378%, Loss = 0.3010892033576965
Epoch: 7452, Batch Gradient Norm: 15.683390203056346
Epoch: 7452, Batch Gradient Norm after: 15.683390203056346
Epoch 7453/10000, Prediction Accuracy = 64.542%, Loss = 0.3037201464176178
Epoch: 7453, Batch Gradient Norm: 11.68363124517253
Epoch: 7453, Batch Gradient Norm after: 11.68363124517253
Epoch 7454/10000, Prediction Accuracy = 64.436%, Loss = 0.29891362190246584
Epoch: 7454, Batch Gradient Norm: 13.321850507522475
Epoch: 7454, Batch Gradient Norm after: 13.321850507522475
Epoch 7455/10000, Prediction Accuracy = 64.426%, Loss = 0.3015545725822449
Epoch: 7455, Batch Gradient Norm: 11.835650674576799
Epoch: 7455, Batch Gradient Norm after: 11.835650674576799
Epoch 7456/10000, Prediction Accuracy = 64.464%, Loss = 0.3003827154636383
Epoch: 7456, Batch Gradient Norm: 11.734637887684245
Epoch: 7456, Batch Gradient Norm after: 11.734637887684245
Epoch 7457/10000, Prediction Accuracy = 64.33599999999998%, Loss = 0.2992385923862457
Epoch: 7457, Batch Gradient Norm: 12.765162423939723
Epoch: 7457, Batch Gradient Norm after: 12.765162423939723
Epoch 7458/10000, Prediction Accuracy = 64.59%, Loss = 0.3030485510826111
Epoch: 7458, Batch Gradient Norm: 12.149930291676585
Epoch: 7458, Batch Gradient Norm after: 12.149930291676585
Epoch 7459/10000, Prediction Accuracy = 64.512%, Loss = 0.3005365073680878
Epoch: 7459, Batch Gradient Norm: 12.823448530273966
Epoch: 7459, Batch Gradient Norm after: 12.823448530273966
Epoch 7460/10000, Prediction Accuracy = 64.518%, Loss = 0.30063229203224184
Epoch: 7460, Batch Gradient Norm: 16.051869815827967
Epoch: 7460, Batch Gradient Norm after: 16.051869815827967
Epoch 7461/10000, Prediction Accuracy = 64.36800000000001%, Loss = 0.30453450679779054
Epoch: 7461, Batch Gradient Norm: 14.814894186351678
Epoch: 7461, Batch Gradient Norm after: 14.814894186351678
Epoch 7462/10000, Prediction Accuracy = 64.402%, Loss = 0.3033653676509857
Epoch: 7462, Batch Gradient Norm: 17.124537114617812
Epoch: 7462, Batch Gradient Norm after: 17.124537114617812
Epoch 7463/10000, Prediction Accuracy = 64.476%, Loss = 0.3096507787704468
Epoch: 7463, Batch Gradient Norm: 14.0123012786351
Epoch: 7463, Batch Gradient Norm after: 14.0123012786351
Epoch 7464/10000, Prediction Accuracy = 64.362%, Loss = 0.3034207224845886
Epoch: 7464, Batch Gradient Norm: 19.280710809695776
Epoch: 7464, Batch Gradient Norm after: 19.280710809695776
Epoch 7465/10000, Prediction Accuracy = 64.3%, Loss = 0.3133571982383728
Epoch: 7465, Batch Gradient Norm: 19.109545105367204
Epoch: 7465, Batch Gradient Norm after: 18.221650061601768
Epoch 7466/10000, Prediction Accuracy = 64.46799999999999%, Loss = 0.3092286646366119
Epoch: 7466, Batch Gradient Norm: 13.965649403664006
Epoch: 7466, Batch Gradient Norm after: 13.965649403664006
Epoch 7467/10000, Prediction Accuracy = 64.362%, Loss = 0.30342180132865904
Epoch: 7467, Batch Gradient Norm: 13.70816968835989
Epoch: 7467, Batch Gradient Norm after: 13.70816968835989
Epoch 7468/10000, Prediction Accuracy = 64.23400000000001%, Loss = 0.30540173053741454
Epoch: 7468, Batch Gradient Norm: 14.34038665219771
Epoch: 7468, Batch Gradient Norm after: 14.34038665219771
Epoch 7469/10000, Prediction Accuracy = 64.48400000000001%, Loss = 0.3032867252826691
Epoch: 7469, Batch Gradient Norm: 15.226644988284335
Epoch: 7469, Batch Gradient Norm after: 15.226644988284335
Epoch 7470/10000, Prediction Accuracy = 64.31400000000001%, Loss = 0.3029987752437592
Epoch: 7470, Batch Gradient Norm: 16.41332188147654
Epoch: 7470, Batch Gradient Norm after: 16.41332188147654
Epoch 7471/10000, Prediction Accuracy = 64.45%, Loss = 0.3037661612033844
Epoch: 7471, Batch Gradient Norm: 15.983096850366495
Epoch: 7471, Batch Gradient Norm after: 15.983096850366495
Epoch 7472/10000, Prediction Accuracy = 64.48%, Loss = 0.3091073513031006
Epoch: 7472, Batch Gradient Norm: 17.047959574569948
Epoch: 7472, Batch Gradient Norm after: 17.047959574569948
Epoch 7473/10000, Prediction Accuracy = 64.458%, Loss = 0.30889796614646914
Epoch: 7473, Batch Gradient Norm: 19.261452880218535
Epoch: 7473, Batch Gradient Norm after: 19.11896648535625
Epoch 7474/10000, Prediction Accuracy = 64.438%, Loss = 0.3106915712356567
Epoch: 7474, Batch Gradient Norm: 17.55221898380878
Epoch: 7474, Batch Gradient Norm after: 17.513682620753332
Epoch 7475/10000, Prediction Accuracy = 64.15799999999999%, Loss = 0.31042136549949645
Epoch: 7475, Batch Gradient Norm: 19.24922226693652
Epoch: 7475, Batch Gradient Norm after: 19.24922226693652
Epoch 7476/10000, Prediction Accuracy = 64.36%, Loss = 0.30986536741256715
Epoch: 7476, Batch Gradient Norm: 16.13499412906773
Epoch: 7476, Batch Gradient Norm after: 16.13499412906773
Epoch 7477/10000, Prediction Accuracy = 64.34%, Loss = 0.30621521472930907
Epoch: 7477, Batch Gradient Norm: 13.285776678160678
Epoch: 7477, Batch Gradient Norm after: 13.285776678160678
Epoch 7478/10000, Prediction Accuracy = 64.44800000000001%, Loss = 0.30090956687927245
Epoch: 7478, Batch Gradient Norm: 12.95131581784949
Epoch: 7478, Batch Gradient Norm after: 12.95131581784949
Epoch 7479/10000, Prediction Accuracy = 64.32000000000001%, Loss = 0.3023409307003021
Epoch: 7479, Batch Gradient Norm: 13.239264345121638
Epoch: 7479, Batch Gradient Norm after: 13.239264345121638
Epoch 7480/10000, Prediction Accuracy = 64.476%, Loss = 0.29960072636604307
Epoch: 7480, Batch Gradient Norm: 16.303608004217423
Epoch: 7480, Batch Gradient Norm after: 16.303608004217423
Epoch 7481/10000, Prediction Accuracy = 64.43799999999999%, Loss = 0.3054434597492218
Epoch: 7481, Batch Gradient Norm: 18.80565283098887
Epoch: 7481, Batch Gradient Norm after: 18.678283380292022
Epoch 7482/10000, Prediction Accuracy = 64.37800000000001%, Loss = 0.3111121833324432
Epoch: 7482, Batch Gradient Norm: 17.383996874975164
Epoch: 7482, Batch Gradient Norm after: 17.383996874975164
Epoch 7483/10000, Prediction Accuracy = 64.33200000000001%, Loss = 0.3079673707485199
Epoch: 7483, Batch Gradient Norm: 17.78282681635428
Epoch: 7483, Batch Gradient Norm after: 17.78282681635428
Epoch 7484/10000, Prediction Accuracy = 64.30199999999999%, Loss = 0.30883132815361025
Epoch: 7484, Batch Gradient Norm: 19.287272021350674
Epoch: 7484, Batch Gradient Norm after: 18.913305170621253
Epoch 7485/10000, Prediction Accuracy = 64.43599999999999%, Loss = 0.3104982852935791
Epoch: 7485, Batch Gradient Norm: 17.27490583242296
Epoch: 7485, Batch Gradient Norm after: 17.27490583242296
Epoch 7486/10000, Prediction Accuracy = 64.416%, Loss = 0.30650209784507754
Epoch: 7486, Batch Gradient Norm: 15.563168013784063
Epoch: 7486, Batch Gradient Norm after: 15.563168013784063
Epoch 7487/10000, Prediction Accuracy = 64.398%, Loss = 0.3033111095428467
Epoch: 7487, Batch Gradient Norm: 14.873612175340433
Epoch: 7487, Batch Gradient Norm after: 14.873612175340433
Epoch 7488/10000, Prediction Accuracy = 64.316%, Loss = 0.3058854520320892
Epoch: 7488, Batch Gradient Norm: 14.837735162409015
Epoch: 7488, Batch Gradient Norm after: 14.837735162409015
Epoch 7489/10000, Prediction Accuracy = 64.40200000000002%, Loss = 0.30289228558540343
Epoch: 7489, Batch Gradient Norm: 14.895208163879825
Epoch: 7489, Batch Gradient Norm after: 14.895208163879825
Epoch 7490/10000, Prediction Accuracy = 64.50999999999999%, Loss = 0.3037440598011017
Epoch: 7490, Batch Gradient Norm: 13.56250731640024
Epoch: 7490, Batch Gradient Norm after: 13.56250731640024
Epoch 7491/10000, Prediction Accuracy = 64.462%, Loss = 0.30296499729156495
Epoch: 7491, Batch Gradient Norm: 14.044414524745827
Epoch: 7491, Batch Gradient Norm after: 14.044414524745827
Epoch 7492/10000, Prediction Accuracy = 64.38%, Loss = 0.30331566333770754
Epoch: 7492, Batch Gradient Norm: 14.857299435486723
Epoch: 7492, Batch Gradient Norm after: 14.857299435486723
Epoch 7493/10000, Prediction Accuracy = 64.22200000000001%, Loss = 0.3036137819290161
Epoch: 7493, Batch Gradient Norm: 16.33336818944588
Epoch: 7493, Batch Gradient Norm after: 16.33336818944588
Epoch 7494/10000, Prediction Accuracy = 64.538%, Loss = 0.30866390466690063
Epoch: 7494, Batch Gradient Norm: 15.540198806010867
Epoch: 7494, Batch Gradient Norm after: 15.540198806010867
Epoch 7495/10000, Prediction Accuracy = 64.402%, Loss = 0.3040989637374878
Epoch: 7495, Batch Gradient Norm: 16.65907545289766
Epoch: 7495, Batch Gradient Norm after: 16.65907545289766
Epoch 7496/10000, Prediction Accuracy = 64.38400000000001%, Loss = 0.30489055514335633
Epoch: 7496, Batch Gradient Norm: 15.568238781215953
Epoch: 7496, Batch Gradient Norm after: 15.568238781215953
Epoch 7497/10000, Prediction Accuracy = 64.44800000000001%, Loss = 0.3037418842315674
Epoch: 7497, Batch Gradient Norm: 14.380626171106933
Epoch: 7497, Batch Gradient Norm after: 14.380626171106933
Epoch 7498/10000, Prediction Accuracy = 64.374%, Loss = 0.30510241389274595
Epoch: 7498, Batch Gradient Norm: 14.916250251613171
Epoch: 7498, Batch Gradient Norm after: 14.916250251613171
Epoch 7499/10000, Prediction Accuracy = 64.43%, Loss = 0.30421882271766665
Epoch: 7499, Batch Gradient Norm: 13.733114093304629
Epoch: 7499, Batch Gradient Norm after: 13.733114093304629
Epoch 7500/10000, Prediction Accuracy = 64.386%, Loss = 0.30196733474731446
Epoch: 7500, Batch Gradient Norm: 12.639081148578084
Epoch: 7500, Batch Gradient Norm after: 12.639081148578084
Epoch 7501/10000, Prediction Accuracy = 64.488%, Loss = 0.29969499111175535
Epoch: 7501, Batch Gradient Norm: 12.686148707989766
Epoch: 7501, Batch Gradient Norm after: 12.686148707989766
Epoch 7502/10000, Prediction Accuracy = 64.406%, Loss = 0.29982406497001646
Epoch: 7502, Batch Gradient Norm: 14.691911393273621
Epoch: 7502, Batch Gradient Norm after: 14.691911393273621
Epoch 7503/10000, Prediction Accuracy = 64.46%, Loss = 0.3039330780506134
Epoch: 7503, Batch Gradient Norm: 11.833419450178088
Epoch: 7503, Batch Gradient Norm after: 11.833419450178088
Epoch 7504/10000, Prediction Accuracy = 64.454%, Loss = 0.3002906084060669
Epoch: 7504, Batch Gradient Norm: 12.832005044121523
Epoch: 7504, Batch Gradient Norm after: 12.832005044121523
Epoch 7505/10000, Prediction Accuracy = 64.38399999999999%, Loss = 0.3003812491893768
Epoch: 7505, Batch Gradient Norm: 15.575825841368992
Epoch: 7505, Batch Gradient Norm after: 15.575825841368992
Epoch 7506/10000, Prediction Accuracy = 64.326%, Loss = 0.30298963785171507
Epoch: 7506, Batch Gradient Norm: 15.385117102346753
Epoch: 7506, Batch Gradient Norm after: 15.385117102346753
Epoch 7507/10000, Prediction Accuracy = 64.474%, Loss = 0.3058294653892517
Epoch: 7507, Batch Gradient Norm: 14.49728811765234
Epoch: 7507, Batch Gradient Norm after: 14.49728811765234
Epoch 7508/10000, Prediction Accuracy = 64.48799999999999%, Loss = 0.299316281080246
Epoch: 7508, Batch Gradient Norm: 13.811626928035516
Epoch: 7508, Batch Gradient Norm after: 13.811626928035516
Epoch 7509/10000, Prediction Accuracy = 64.518%, Loss = 0.3003456473350525
Epoch: 7509, Batch Gradient Norm: 14.824066389982814
Epoch: 7509, Batch Gradient Norm after: 14.824066389982814
Epoch 7510/10000, Prediction Accuracy = 64.474%, Loss = 0.3045614421367645
Epoch: 7510, Batch Gradient Norm: 16.380889462607474
Epoch: 7510, Batch Gradient Norm after: 15.970177873016917
Epoch 7511/10000, Prediction Accuracy = 64.28999999999999%, Loss = 0.3075419306755066
Epoch: 7511, Batch Gradient Norm: 21.740373982078296
Epoch: 7511, Batch Gradient Norm after: 19.70373775848469
Epoch 7512/10000, Prediction Accuracy = 64.322%, Loss = 0.31229184865951537
Epoch: 7512, Batch Gradient Norm: 16.86645148308956
Epoch: 7512, Batch Gradient Norm after: 16.86645148308956
Epoch 7513/10000, Prediction Accuracy = 64.366%, Loss = 0.3062500298023224
Epoch: 7513, Batch Gradient Norm: 16.74346558337951
Epoch: 7513, Batch Gradient Norm after: 16.74346558337951
Epoch 7514/10000, Prediction Accuracy = 64.374%, Loss = 0.3053783893585205
Epoch: 7514, Batch Gradient Norm: 14.242090058969
Epoch: 7514, Batch Gradient Norm after: 14.242090058969
Epoch 7515/10000, Prediction Accuracy = 64.46400000000001%, Loss = 0.30404176712036135
Epoch: 7515, Batch Gradient Norm: 12.844740589222603
Epoch: 7515, Batch Gradient Norm after: 12.844740589222603
Epoch 7516/10000, Prediction Accuracy = 64.402%, Loss = 0.3022079408168793
Epoch: 7516, Batch Gradient Norm: 15.637020703800976
Epoch: 7516, Batch Gradient Norm after: 15.637020703800976
Epoch 7517/10000, Prediction Accuracy = 64.41%, Loss = 0.3031331181526184
Epoch: 7517, Batch Gradient Norm: 11.732403006871964
Epoch: 7517, Batch Gradient Norm after: 11.732403006871964
Epoch 7518/10000, Prediction Accuracy = 64.37400000000001%, Loss = 0.2975963234901428
Epoch: 7518, Batch Gradient Norm: 12.062715650030233
Epoch: 7518, Batch Gradient Norm after: 12.062715650030233
Epoch 7519/10000, Prediction Accuracy = 64.50000000000001%, Loss = 0.29709649085998535
Epoch: 7519, Batch Gradient Norm: 17.093856102571173
Epoch: 7519, Batch Gradient Norm after: 17.093856102571173
Epoch 7520/10000, Prediction Accuracy = 64.218%, Loss = 0.3085102677345276
Epoch: 7520, Batch Gradient Norm: 18.21217548798535
Epoch: 7520, Batch Gradient Norm after: 18.21217548798535
Epoch 7521/10000, Prediction Accuracy = 64.532%, Loss = 0.3061675548553467
Epoch: 7521, Batch Gradient Norm: 19.336296213920072
Epoch: 7521, Batch Gradient Norm after: 19.336296213920072
Epoch 7522/10000, Prediction Accuracy = 64.396%, Loss = 0.3078840434551239
Epoch: 7522, Batch Gradient Norm: 17.02872295737081
Epoch: 7522, Batch Gradient Norm after: 17.02872295737081
Epoch 7523/10000, Prediction Accuracy = 64.49600000000001%, Loss = 0.30449260473251344
Epoch: 7523, Batch Gradient Norm: 15.993070529948334
Epoch: 7523, Batch Gradient Norm after: 15.993070529948334
Epoch 7524/10000, Prediction Accuracy = 64.456%, Loss = 0.30396364331245423
Epoch: 7524, Batch Gradient Norm: 15.380933934483368
Epoch: 7524, Batch Gradient Norm after: 15.380933934483368
Epoch 7525/10000, Prediction Accuracy = 64.514%, Loss = 0.301676470041275
Epoch: 7525, Batch Gradient Norm: 13.54409092433086
Epoch: 7525, Batch Gradient Norm after: 13.54409092433086
Epoch 7526/10000, Prediction Accuracy = 64.354%, Loss = 0.3034182369709015
Epoch: 7526, Batch Gradient Norm: 14.190750439540805
Epoch: 7526, Batch Gradient Norm after: 14.190750439540805
Epoch 7527/10000, Prediction Accuracy = 64.268%, Loss = 0.30142644643783567
Epoch: 7527, Batch Gradient Norm: 15.575025024255563
Epoch: 7527, Batch Gradient Norm after: 15.575025024255563
Epoch 7528/10000, Prediction Accuracy = 64.386%, Loss = 0.3022481918334961
Epoch: 7528, Batch Gradient Norm: 17.65030656135612
Epoch: 7528, Batch Gradient Norm after: 17.65030656135612
Epoch 7529/10000, Prediction Accuracy = 64.466%, Loss = 0.3064283072948456
Epoch: 7529, Batch Gradient Norm: 15.606599651488482
Epoch: 7529, Batch Gradient Norm after: 15.606599651488482
Epoch 7530/10000, Prediction Accuracy = 64.326%, Loss = 0.30619394183158877
Epoch: 7530, Batch Gradient Norm: 17.890625777796853
Epoch: 7530, Batch Gradient Norm after: 17.88260073047341
Epoch 7531/10000, Prediction Accuracy = 64.45599999999999%, Loss = 0.30905150175094603
Epoch: 7531, Batch Gradient Norm: 14.621133360139513
Epoch: 7531, Batch Gradient Norm after: 14.621133360139513
Epoch 7532/10000, Prediction Accuracy = 64.48800000000001%, Loss = 0.3007499635219574
Epoch: 7532, Batch Gradient Norm: 13.182540622418589
Epoch: 7532, Batch Gradient Norm after: 13.182540622418589
Epoch 7533/10000, Prediction Accuracy = 64.566%, Loss = 0.3001259803771973
Epoch: 7533, Batch Gradient Norm: 11.223783095629097
Epoch: 7533, Batch Gradient Norm after: 11.223783095629097
Epoch 7534/10000, Prediction Accuracy = 64.53999999999999%, Loss = 0.29617480635643006
Epoch: 7534, Batch Gradient Norm: 10.922574307081186
Epoch: 7534, Batch Gradient Norm after: 10.922574307081186
Epoch 7535/10000, Prediction Accuracy = 64.39999999999999%, Loss = 0.2980500638484955
Epoch: 7535, Batch Gradient Norm: 13.139408952822595
Epoch: 7535, Batch Gradient Norm after: 13.139408952822595
Epoch 7536/10000, Prediction Accuracy = 64.576%, Loss = 0.2984055757522583
Epoch: 7536, Batch Gradient Norm: 13.829883885964648
Epoch: 7536, Batch Gradient Norm after: 13.829883885964648
Epoch 7537/10000, Prediction Accuracy = 64.552%, Loss = 0.3016569912433624
Epoch: 7537, Batch Gradient Norm: 15.922453893692708
Epoch: 7537, Batch Gradient Norm after: 15.922453893692708
Epoch 7538/10000, Prediction Accuracy = 64.406%, Loss = 0.30191074013710023
Epoch: 7538, Batch Gradient Norm: 15.955488364294082
Epoch: 7538, Batch Gradient Norm after: 15.955488364294082
Epoch 7539/10000, Prediction Accuracy = 64.41799999999999%, Loss = 0.30268571972846986
Epoch: 7539, Batch Gradient Norm: 18.039121127421495
Epoch: 7539, Batch Gradient Norm after: 17.734770962133446
Epoch 7540/10000, Prediction Accuracy = 64.5%, Loss = 0.30671698451042173
Epoch: 7540, Batch Gradient Norm: 18.970392948482576
Epoch: 7540, Batch Gradient Norm after: 18.970392948482576
Epoch 7541/10000, Prediction Accuracy = 64.434%, Loss = 0.3115616261959076
Epoch: 7541, Batch Gradient Norm: 16.331139213210246
Epoch: 7541, Batch Gradient Norm after: 16.331139213210246
Epoch 7542/10000, Prediction Accuracy = 64.58600000000001%, Loss = 0.30224722623825073
Epoch: 7542, Batch Gradient Norm: 16.43118621321415
Epoch: 7542, Batch Gradient Norm after: 16.43118621321415
Epoch 7543/10000, Prediction Accuracy = 64.47800000000001%, Loss = 0.3017492055892944
Epoch: 7543, Batch Gradient Norm: 16.227489805467588
Epoch: 7543, Batch Gradient Norm after: 16.227489805467588
Epoch 7544/10000, Prediction Accuracy = 64.388%, Loss = 0.30472267270088194
Epoch: 7544, Batch Gradient Norm: 16.077043508606444
Epoch: 7544, Batch Gradient Norm after: 16.077043508606444
Epoch 7545/10000, Prediction Accuracy = 64.414%, Loss = 0.30557042360305786
Epoch: 7545, Batch Gradient Norm: 12.247602732249293
Epoch: 7545, Batch Gradient Norm after: 12.247602732249293
Epoch 7546/10000, Prediction Accuracy = 64.522%, Loss = 0.29786195755004885
Epoch: 7546, Batch Gradient Norm: 12.941571390669258
Epoch: 7546, Batch Gradient Norm after: 12.941571390669258
Epoch 7547/10000, Prediction Accuracy = 64.51199999999999%, Loss = 0.29922921061515806
Epoch: 7547, Batch Gradient Norm: 11.953480337656588
Epoch: 7547, Batch Gradient Norm after: 11.953480337656588
Epoch 7548/10000, Prediction Accuracy = 64.38%, Loss = 0.3008763790130615
Epoch: 7548, Batch Gradient Norm: 11.344315798300704
Epoch: 7548, Batch Gradient Norm after: 11.344315798300704
Epoch 7549/10000, Prediction Accuracy = 64.48400000000001%, Loss = 0.2973873496055603
Epoch: 7549, Batch Gradient Norm: 13.528708231752699
Epoch: 7549, Batch Gradient Norm after: 13.528708231752699
Epoch 7550/10000, Prediction Accuracy = 64.50800000000001%, Loss = 0.30094969272613525
Epoch: 7550, Batch Gradient Norm: 14.332100793151588
Epoch: 7550, Batch Gradient Norm after: 14.332100793151588
Epoch 7551/10000, Prediction Accuracy = 64.528%, Loss = 0.3025978446006775
Epoch: 7551, Batch Gradient Norm: 15.704961078854492
Epoch: 7551, Batch Gradient Norm after: 15.704961078854492
Epoch 7552/10000, Prediction Accuracy = 64.454%, Loss = 0.30600671768188475
Epoch: 7552, Batch Gradient Norm: 17.011662244941668
Epoch: 7552, Batch Gradient Norm after: 17.011662244941668
Epoch 7553/10000, Prediction Accuracy = 64.578%, Loss = 0.3077950716018677
Epoch: 7553, Batch Gradient Norm: 16.400105027963313
Epoch: 7553, Batch Gradient Norm after: 16.400105027963313
Epoch 7554/10000, Prediction Accuracy = 64.446%, Loss = 0.30579532980918883
Epoch: 7554, Batch Gradient Norm: 15.350004581538652
Epoch: 7554, Batch Gradient Norm after: 15.350004581538652
Epoch 7555/10000, Prediction Accuracy = 64.3%, Loss = 0.3057710647583008
Epoch: 7555, Batch Gradient Norm: 15.293424838899666
Epoch: 7555, Batch Gradient Norm after: 15.293424838899666
Epoch 7556/10000, Prediction Accuracy = 64.438%, Loss = 0.30549865365028384
Epoch: 7556, Batch Gradient Norm: 14.409760728148798
Epoch: 7556, Batch Gradient Norm after: 14.409760728148798
Epoch 7557/10000, Prediction Accuracy = 64.412%, Loss = 0.2997209429740906
Epoch: 7557, Batch Gradient Norm: 16.143126765158677
Epoch: 7557, Batch Gradient Norm after: 16.143126765158677
Epoch 7558/10000, Prediction Accuracy = 64.34400000000001%, Loss = 0.30262988805770874
Epoch: 7558, Batch Gradient Norm: 15.183589001969418
Epoch: 7558, Batch Gradient Norm after: 15.183589001969418
Epoch 7559/10000, Prediction Accuracy = 64.39000000000001%, Loss = 0.30422248840332033
Epoch: 7559, Batch Gradient Norm: 13.249861082951066
Epoch: 7559, Batch Gradient Norm after: 13.249861082951066
Epoch 7560/10000, Prediction Accuracy = 64.366%, Loss = 0.30048711895942687
Epoch: 7560, Batch Gradient Norm: 14.336606397451646
Epoch: 7560, Batch Gradient Norm after: 14.336606397451646
Epoch 7561/10000, Prediction Accuracy = 64.45200000000001%, Loss = 0.3016915738582611
Epoch: 7561, Batch Gradient Norm: 13.945175224433981
Epoch: 7561, Batch Gradient Norm after: 13.945175224433981
Epoch 7562/10000, Prediction Accuracy = 64.58000000000001%, Loss = 0.30147129893302915
Epoch: 7562, Batch Gradient Norm: 15.141916517669316
Epoch: 7562, Batch Gradient Norm after: 15.141916517669316
Epoch 7563/10000, Prediction Accuracy = 64.476%, Loss = 0.3021141469478607
Epoch: 7563, Batch Gradient Norm: 13.583879131220895
Epoch: 7563, Batch Gradient Norm after: 13.583879131220895
Epoch 7564/10000, Prediction Accuracy = 64.364%, Loss = 0.30039235949516296
Epoch: 7564, Batch Gradient Norm: 13.845755510611436
Epoch: 7564, Batch Gradient Norm after: 13.845755510611436
Epoch 7565/10000, Prediction Accuracy = 64.34599999999999%, Loss = 0.3003908693790436
Epoch: 7565, Batch Gradient Norm: 12.239330878751623
Epoch: 7565, Batch Gradient Norm after: 12.239330878751623
Epoch 7566/10000, Prediction Accuracy = 64.352%, Loss = 0.30108982920646665
Epoch: 7566, Batch Gradient Norm: 14.390088905475869
Epoch: 7566, Batch Gradient Norm after: 14.390088905475869
Epoch 7567/10000, Prediction Accuracy = 64.44%, Loss = 0.3029313325881958
Epoch: 7567, Batch Gradient Norm: 15.643360709029846
Epoch: 7567, Batch Gradient Norm after: 15.643360709029846
Epoch 7568/10000, Prediction Accuracy = 64.376%, Loss = 0.30072376132011414
Epoch: 7568, Batch Gradient Norm: 11.949194509243656
Epoch: 7568, Batch Gradient Norm after: 11.949194509243656
Epoch 7569/10000, Prediction Accuracy = 64.66%, Loss = 0.29840818643569944
Epoch: 7569, Batch Gradient Norm: 13.611259834391078
Epoch: 7569, Batch Gradient Norm after: 13.611259834391078
Epoch 7570/10000, Prediction Accuracy = 64.57000000000001%, Loss = 0.30065408945083616
Epoch: 7570, Batch Gradient Norm: 15.605386530651263
Epoch: 7570, Batch Gradient Norm after: 15.605386530651263
Epoch 7571/10000, Prediction Accuracy = 64.44800000000001%, Loss = 0.30262898206710814
Epoch: 7571, Batch Gradient Norm: 15.502320154199388
Epoch: 7571, Batch Gradient Norm after: 15.502320154199388
Epoch 7572/10000, Prediction Accuracy = 64.556%, Loss = 0.30070515871047976
Epoch: 7572, Batch Gradient Norm: 17.358197674319563
Epoch: 7572, Batch Gradient Norm after: 17.358197674319563
Epoch 7573/10000, Prediction Accuracy = 64.534%, Loss = 0.30601908564567565
Epoch: 7573, Batch Gradient Norm: 15.7106439328856
Epoch: 7573, Batch Gradient Norm after: 15.7106439328856
Epoch 7574/10000, Prediction Accuracy = 64.488%, Loss = 0.3030130326747894
Epoch: 7574, Batch Gradient Norm: 15.958205228959931
Epoch: 7574, Batch Gradient Norm after: 15.958205228959931
Epoch 7575/10000, Prediction Accuracy = 64.442%, Loss = 0.30326480269432066
Epoch: 7575, Batch Gradient Norm: 17.278851858074177
Epoch: 7575, Batch Gradient Norm after: 17.278851858074177
Epoch 7576/10000, Prediction Accuracy = 64.42399999999999%, Loss = 0.3039476752281189
Epoch: 7576, Batch Gradient Norm: 14.89586761872228
Epoch: 7576, Batch Gradient Norm after: 14.89586761872228
Epoch 7577/10000, Prediction Accuracy = 64.574%, Loss = 0.30107465386390686
Epoch: 7577, Batch Gradient Norm: 13.32558788643123
Epoch: 7577, Batch Gradient Norm after: 13.32558788643123
Epoch 7578/10000, Prediction Accuracy = 64.454%, Loss = 0.3002724885940552
Epoch: 7578, Batch Gradient Norm: 11.592765769980293
Epoch: 7578, Batch Gradient Norm after: 11.592765769980293
Epoch 7579/10000, Prediction Accuracy = 64.52000000000001%, Loss = 0.2995702803134918
Epoch: 7579, Batch Gradient Norm: 11.71939448970035
Epoch: 7579, Batch Gradient Norm after: 11.71939448970035
Epoch 7580/10000, Prediction Accuracy = 64.51599999999999%, Loss = 0.3004069924354553
Epoch: 7580, Batch Gradient Norm: 10.216305441670318
Epoch: 7580, Batch Gradient Norm after: 10.216305441670318
Epoch 7581/10000, Prediction Accuracy = 64.512%, Loss = 0.2965364694595337
Epoch: 7581, Batch Gradient Norm: 12.172927854833732
Epoch: 7581, Batch Gradient Norm after: 12.172927854833732
Epoch 7582/10000, Prediction Accuracy = 64.44000000000001%, Loss = 0.2990453064441681
Epoch: 7582, Batch Gradient Norm: 11.753179367225455
Epoch: 7582, Batch Gradient Norm after: 11.753179367225455
Epoch 7583/10000, Prediction Accuracy = 64.51%, Loss = 0.29761945009231566
Epoch: 7583, Batch Gradient Norm: 13.519734253675814
Epoch: 7583, Batch Gradient Norm after: 13.519734253675814
Epoch 7584/10000, Prediction Accuracy = 64.41000000000001%, Loss = 0.30216172337532043
Epoch: 7584, Batch Gradient Norm: 13.353262515487943
Epoch: 7584, Batch Gradient Norm after: 13.353262515487943
Epoch 7585/10000, Prediction Accuracy = 64.38199999999999%, Loss = 0.29971925020217893
Epoch: 7585, Batch Gradient Norm: 13.964030557248156
Epoch: 7585, Batch Gradient Norm after: 13.964030557248156
Epoch 7586/10000, Prediction Accuracy = 64.618%, Loss = 0.29934201836586
Epoch: 7586, Batch Gradient Norm: 16.4582361297119
Epoch: 7586, Batch Gradient Norm after: 16.4582361297119
Epoch 7587/10000, Prediction Accuracy = 64.524%, Loss = 0.30184690952301024
Epoch: 7587, Batch Gradient Norm: 15.209826615316498
Epoch: 7587, Batch Gradient Norm after: 15.209826615316498
Epoch 7588/10000, Prediction Accuracy = 64.438%, Loss = 0.3027777671813965
Epoch: 7588, Batch Gradient Norm: 15.175459868463305
Epoch: 7588, Batch Gradient Norm after: 15.175459868463305
Epoch 7589/10000, Prediction Accuracy = 64.17999999999999%, Loss = 0.30056663155555724
Epoch: 7589, Batch Gradient Norm: 13.283445661130035
Epoch: 7589, Batch Gradient Norm after: 13.283445661130035
Epoch 7590/10000, Prediction Accuracy = 64.53%, Loss = 0.29884549379348757
Epoch: 7590, Batch Gradient Norm: 14.503004060361095
Epoch: 7590, Batch Gradient Norm after: 14.503004060361095
Epoch 7591/10000, Prediction Accuracy = 64.43199999999999%, Loss = 0.3007191479206085
Epoch: 7591, Batch Gradient Norm: 12.655316218086064
Epoch: 7591, Batch Gradient Norm after: 12.655316218086064
Epoch 7592/10000, Prediction Accuracy = 64.508%, Loss = 0.29804263114929197
Epoch: 7592, Batch Gradient Norm: 12.75537524855652
Epoch: 7592, Batch Gradient Norm after: 12.75537524855652
Epoch 7593/10000, Prediction Accuracy = 64.414%, Loss = 0.29843445420265197
Epoch: 7593, Batch Gradient Norm: 10.42409030714312
Epoch: 7593, Batch Gradient Norm after: 10.42409030714312
Epoch 7594/10000, Prediction Accuracy = 64.58600000000001%, Loss = 0.2950351476669312
Epoch: 7594, Batch Gradient Norm: 11.51451865180862
Epoch: 7594, Batch Gradient Norm after: 11.51451865180862
Epoch 7595/10000, Prediction Accuracy = 64.632%, Loss = 0.29564074277877805
Epoch: 7595, Batch Gradient Norm: 12.034184999395247
Epoch: 7595, Batch Gradient Norm after: 12.034184999395247
Epoch 7596/10000, Prediction Accuracy = 64.478%, Loss = 0.2973483562469482
Epoch: 7596, Batch Gradient Norm: 11.617646383100276
Epoch: 7596, Batch Gradient Norm after: 11.617646383100276
Epoch 7597/10000, Prediction Accuracy = 64.47%, Loss = 0.29966365694999697
Epoch: 7597, Batch Gradient Norm: 13.708285575119808
Epoch: 7597, Batch Gradient Norm after: 13.708285575119808
Epoch 7598/10000, Prediction Accuracy = 64.588%, Loss = 0.29836020469665525
Epoch: 7598, Batch Gradient Norm: 13.451938047336053
Epoch: 7598, Batch Gradient Norm after: 13.451938047336053
Epoch 7599/10000, Prediction Accuracy = 64.506%, Loss = 0.29955102801322936
Epoch: 7599, Batch Gradient Norm: 14.653951358191021
Epoch: 7599, Batch Gradient Norm after: 14.653951358191021
Epoch 7600/10000, Prediction Accuracy = 64.41600000000001%, Loss = 0.3032227396965027
Epoch: 7600, Batch Gradient Norm: 14.384340287537132
Epoch: 7600, Batch Gradient Norm after: 14.384340287537132
Epoch 7601/10000, Prediction Accuracy = 64.394%, Loss = 0.29980307817459106
Epoch: 7601, Batch Gradient Norm: 17.90980761851113
Epoch: 7601, Batch Gradient Norm after: 17.90980761851113
Epoch 7602/10000, Prediction Accuracy = 64.482%, Loss = 0.3060922741889954
Epoch: 7602, Batch Gradient Norm: 17.297129075545104
Epoch: 7602, Batch Gradient Norm after: 17.297129075545104
Epoch 7603/10000, Prediction Accuracy = 64.342%, Loss = 0.3040671467781067
Epoch: 7603, Batch Gradient Norm: 15.976388785257983
Epoch: 7603, Batch Gradient Norm after: 15.976388785257983
Epoch 7604/10000, Prediction Accuracy = 64.364%, Loss = 0.3030535340309143
Epoch: 7604, Batch Gradient Norm: 16.1543098367514
Epoch: 7604, Batch Gradient Norm after: 16.1543098367514
Epoch 7605/10000, Prediction Accuracy = 64.49199999999999%, Loss = 0.3020827293395996
Epoch: 7605, Batch Gradient Norm: 12.484237983081927
Epoch: 7605, Batch Gradient Norm after: 12.484237983081927
Epoch 7606/10000, Prediction Accuracy = 64.414%, Loss = 0.30005107522010804
Epoch: 7606, Batch Gradient Norm: 14.103235104621907
Epoch: 7606, Batch Gradient Norm after: 14.103235104621907
Epoch 7607/10000, Prediction Accuracy = 64.562%, Loss = 0.2983984172344208
Epoch: 7607, Batch Gradient Norm: 14.87064908670364
Epoch: 7607, Batch Gradient Norm after: 14.87064908670364
Epoch 7608/10000, Prediction Accuracy = 64.48799999999999%, Loss = 0.3008475720882416
Epoch: 7608, Batch Gradient Norm: 13.820119451398067
Epoch: 7608, Batch Gradient Norm after: 13.820119451398067
Epoch 7609/10000, Prediction Accuracy = 64.458%, Loss = 0.2978098213672638
Epoch: 7609, Batch Gradient Norm: 13.67881298157935
Epoch: 7609, Batch Gradient Norm after: 13.67881298157935
Epoch 7610/10000, Prediction Accuracy = 64.53%, Loss = 0.3006208300590515
Epoch: 7610, Batch Gradient Norm: 14.12122528461082
Epoch: 7610, Batch Gradient Norm after: 14.12122528461082
Epoch 7611/10000, Prediction Accuracy = 64.458%, Loss = 0.2999128997325897
Epoch: 7611, Batch Gradient Norm: 13.451735553659692
Epoch: 7611, Batch Gradient Norm after: 13.451735553659692
Epoch 7612/10000, Prediction Accuracy = 64.522%, Loss = 0.298576283454895
Epoch: 7612, Batch Gradient Norm: 14.61762261601513
Epoch: 7612, Batch Gradient Norm after: 14.61762261601513
Epoch 7613/10000, Prediction Accuracy = 64.436%, Loss = 0.29956403374671936
Epoch: 7613, Batch Gradient Norm: 17.0660814589267
Epoch: 7613, Batch Gradient Norm after: 17.0660814589267
Epoch 7614/10000, Prediction Accuracy = 64.604%, Loss = 0.3033673882484436
Epoch: 7614, Batch Gradient Norm: 14.743860494206832
Epoch: 7614, Batch Gradient Norm after: 14.743860494206832
Epoch 7615/10000, Prediction Accuracy = 64.418%, Loss = 0.30354090332984923
Epoch: 7615, Batch Gradient Norm: 16.73723458021261
Epoch: 7615, Batch Gradient Norm after: 16.73723458021261
Epoch 7616/10000, Prediction Accuracy = 64.362%, Loss = 0.3037383794784546
Epoch: 7616, Batch Gradient Norm: 14.105273781907016
Epoch: 7616, Batch Gradient Norm after: 14.105273781907016
Epoch 7617/10000, Prediction Accuracy = 64.55000000000001%, Loss = 0.2981809675693512
Epoch: 7617, Batch Gradient Norm: 14.280466148758268
Epoch: 7617, Batch Gradient Norm after: 14.280466148758268
Epoch 7618/10000, Prediction Accuracy = 64.49600000000001%, Loss = 0.3000620245933533
Epoch: 7618, Batch Gradient Norm: 15.949671475808465
Epoch: 7618, Batch Gradient Norm after: 15.949671475808465
Epoch 7619/10000, Prediction Accuracy = 64.44200000000001%, Loss = 0.30386594533920286
Epoch: 7619, Batch Gradient Norm: 16.186800774680265
Epoch: 7619, Batch Gradient Norm after: 16.186800774680265
Epoch 7620/10000, Prediction Accuracy = 64.43800000000002%, Loss = 0.3036414921283722
Epoch: 7620, Batch Gradient Norm: 13.184543296964868
Epoch: 7620, Batch Gradient Norm after: 13.184543296964868
Epoch 7621/10000, Prediction Accuracy = 64.646%, Loss = 0.2985937535762787
Epoch: 7621, Batch Gradient Norm: 12.503064958905435
Epoch: 7621, Batch Gradient Norm after: 12.503064958905435
Epoch 7622/10000, Prediction Accuracy = 64.452%, Loss = 0.29759055376052856
Epoch: 7622, Batch Gradient Norm: 13.593157892431474
Epoch: 7622, Batch Gradient Norm after: 13.593157892431474
Epoch 7623/10000, Prediction Accuracy = 64.34599999999999%, Loss = 0.30070316791534424
Epoch: 7623, Batch Gradient Norm: 14.994923068968074
Epoch: 7623, Batch Gradient Norm after: 14.994923068968074
Epoch 7624/10000, Prediction Accuracy = 64.4%, Loss = 0.30397720336914064
Epoch: 7624, Batch Gradient Norm: 15.760167407508503
Epoch: 7624, Batch Gradient Norm after: 15.760167407508503
Epoch 7625/10000, Prediction Accuracy = 64.34200000000001%, Loss = 0.30241244435310366
Epoch: 7625, Batch Gradient Norm: 14.957338597012587
Epoch: 7625, Batch Gradient Norm after: 14.957338597012587
Epoch 7626/10000, Prediction Accuracy = 64.48%, Loss = 0.30338712930679324
Epoch: 7626, Batch Gradient Norm: 15.410367067670903
Epoch: 7626, Batch Gradient Norm after: 15.410367067670903
Epoch 7627/10000, Prediction Accuracy = 64.448%, Loss = 0.30032681226730346
Epoch: 7627, Batch Gradient Norm: 19.354126339150987
Epoch: 7627, Batch Gradient Norm after: 19.354126339150987
Epoch 7628/10000, Prediction Accuracy = 64.502%, Loss = 0.3078575789928436
Epoch: 7628, Batch Gradient Norm: 20.142877753981296
Epoch: 7628, Batch Gradient Norm after: 20.142877753981296
Epoch 7629/10000, Prediction Accuracy = 64.41199999999999%, Loss = 0.30891165137290955
Epoch: 7629, Batch Gradient Norm: 18.464450010155076
Epoch: 7629, Batch Gradient Norm after: 18.464450010155076
Epoch 7630/10000, Prediction Accuracy = 64.478%, Loss = 0.30552432537078855
Epoch: 7630, Batch Gradient Norm: 17.084820354112065
Epoch: 7630, Batch Gradient Norm after: 17.084820354112065
Epoch 7631/10000, Prediction Accuracy = 64.54599999999999%, Loss = 0.30678852200508117
Epoch: 7631, Batch Gradient Norm: 14.61710764498574
Epoch: 7631, Batch Gradient Norm after: 14.61710764498574
Epoch 7632/10000, Prediction Accuracy = 64.468%, Loss = 0.30168572068214417
Epoch: 7632, Batch Gradient Norm: 13.953846788844698
Epoch: 7632, Batch Gradient Norm after: 13.953846788844698
Epoch 7633/10000, Prediction Accuracy = 64.546%, Loss = 0.2998843789100647
Epoch: 7633, Batch Gradient Norm: 13.72802861923979
Epoch: 7633, Batch Gradient Norm after: 13.72802861923979
Epoch 7634/10000, Prediction Accuracy = 64.45599999999999%, Loss = 0.2991597235202789
Epoch: 7634, Batch Gradient Norm: 12.335057295344077
Epoch: 7634, Batch Gradient Norm after: 12.335057295344077
Epoch 7635/10000, Prediction Accuracy = 64.494%, Loss = 0.30048800706863404
Epoch: 7635, Batch Gradient Norm: 15.390449781816377
Epoch: 7635, Batch Gradient Norm after: 15.390449781816377
Epoch 7636/10000, Prediction Accuracy = 64.54%, Loss = 0.30168030261993406
Epoch: 7636, Batch Gradient Norm: 15.15868099982038
Epoch: 7636, Batch Gradient Norm after: 15.15868099982038
Epoch 7637/10000, Prediction Accuracy = 64.506%, Loss = 0.29962494373321535
Epoch: 7637, Batch Gradient Norm: 15.617876649337852
Epoch: 7637, Batch Gradient Norm after: 15.617876649337852
Epoch 7638/10000, Prediction Accuracy = 64.362%, Loss = 0.3027603328227997
Epoch: 7638, Batch Gradient Norm: 16.690344081112304
Epoch: 7638, Batch Gradient Norm after: 16.690344081112304
Epoch 7639/10000, Prediction Accuracy = 64.446%, Loss = 0.30349533557891845
Epoch: 7639, Batch Gradient Norm: 15.502420410772194
Epoch: 7639, Batch Gradient Norm after: 15.502420410772194
Epoch 7640/10000, Prediction Accuracy = 64.44800000000001%, Loss = 0.3049103498458862
Epoch: 7640, Batch Gradient Norm: 14.286340633053904
Epoch: 7640, Batch Gradient Norm after: 14.286340633053904
Epoch 7641/10000, Prediction Accuracy = 64.51%, Loss = 0.30654619336128236
Epoch: 7641, Batch Gradient Norm: 12.938629334412186
Epoch: 7641, Batch Gradient Norm after: 12.938629334412186
Epoch 7642/10000, Prediction Accuracy = 64.47999999999999%, Loss = 0.2988732397556305
Epoch: 7642, Batch Gradient Norm: 13.147748945562196
Epoch: 7642, Batch Gradient Norm after: 13.147748945562196
Epoch 7643/10000, Prediction Accuracy = 64.602%, Loss = 0.2986240088939667
Epoch: 7643, Batch Gradient Norm: 13.871194982741681
Epoch: 7643, Batch Gradient Norm after: 13.871194982741681
Epoch 7644/10000, Prediction Accuracy = 64.354%, Loss = 0.29949318170547484
Epoch: 7644, Batch Gradient Norm: 17.12074667045277
Epoch: 7644, Batch Gradient Norm after: 17.12074667045277
Epoch 7645/10000, Prediction Accuracy = 64.52000000000001%, Loss = 0.3030812323093414
Epoch: 7645, Batch Gradient Norm: 16.54700622158306
Epoch: 7645, Batch Gradient Norm after: 16.54700622158306
Epoch 7646/10000, Prediction Accuracy = 64.336%, Loss = 0.3088917672634125
Epoch: 7646, Batch Gradient Norm: 16.29561161135781
Epoch: 7646, Batch Gradient Norm after: 16.29561161135781
Epoch 7647/10000, Prediction Accuracy = 64.44000000000001%, Loss = 0.30461923480033876
Epoch: 7647, Batch Gradient Norm: 13.074652099510258
Epoch: 7647, Batch Gradient Norm after: 13.074652099510258
Epoch 7648/10000, Prediction Accuracy = 64.578%, Loss = 0.29636738300323484
Epoch: 7648, Batch Gradient Norm: 12.040698380669163
Epoch: 7648, Batch Gradient Norm after: 12.040698380669163
Epoch 7649/10000, Prediction Accuracy = 64.682%, Loss = 0.29671829342842104
Epoch: 7649, Batch Gradient Norm: 10.871528025775387
Epoch: 7649, Batch Gradient Norm after: 10.871528025775387
Epoch 7650/10000, Prediction Accuracy = 64.506%, Loss = 0.29708858132362365
Epoch: 7650, Batch Gradient Norm: 10.651384296333948
Epoch: 7650, Batch Gradient Norm after: 10.651384296333948
Epoch 7651/10000, Prediction Accuracy = 64.56400000000001%, Loss = 0.29362953901290895
Epoch: 7651, Batch Gradient Norm: 11.392795836651636
Epoch: 7651, Batch Gradient Norm after: 11.392795836651636
Epoch 7652/10000, Prediction Accuracy = 64.404%, Loss = 0.2969250321388245
Epoch: 7652, Batch Gradient Norm: 11.77362720826599
Epoch: 7652, Batch Gradient Norm after: 11.77362720826599
Epoch 7653/10000, Prediction Accuracy = 64.50800000000001%, Loss = 0.2945202112197876
Epoch: 7653, Batch Gradient Norm: 13.905322666577161
Epoch: 7653, Batch Gradient Norm after: 13.905322666577161
Epoch 7654/10000, Prediction Accuracy = 64.53%, Loss = 0.2991849958896637
Epoch: 7654, Batch Gradient Norm: 16.254246332682204
Epoch: 7654, Batch Gradient Norm after: 16.254246332682204
Epoch 7655/10000, Prediction Accuracy = 64.464%, Loss = 0.29962806701660155
Epoch: 7655, Batch Gradient Norm: 17.876760043444172
Epoch: 7655, Batch Gradient Norm after: 17.876760043444172
Epoch 7656/10000, Prediction Accuracy = 64.382%, Loss = 0.30489521026611327
Epoch: 7656, Batch Gradient Norm: 17.52711603547509
Epoch: 7656, Batch Gradient Norm after: 17.52711603547509
Epoch 7657/10000, Prediction Accuracy = 64.492%, Loss = 0.3023258924484253
Epoch: 7657, Batch Gradient Norm: 15.495447945163518
Epoch: 7657, Batch Gradient Norm after: 15.495447945163518
Epoch 7658/10000, Prediction Accuracy = 64.502%, Loss = 0.29942795634269714
Epoch: 7658, Batch Gradient Norm: 13.343081460502
Epoch: 7658, Batch Gradient Norm after: 13.343081460502
Epoch 7659/10000, Prediction Accuracy = 64.648%, Loss = 0.2979434907436371
Epoch: 7659, Batch Gradient Norm: 14.062485092140468
Epoch: 7659, Batch Gradient Norm after: 14.062485092140468
Epoch 7660/10000, Prediction Accuracy = 64.4%, Loss = 0.30104663372039797
Epoch: 7660, Batch Gradient Norm: 13.02648453721461
Epoch: 7660, Batch Gradient Norm after: 13.02648453721461
Epoch 7661/10000, Prediction Accuracy = 64.47800000000001%, Loss = 0.29913211464881895
Epoch: 7661, Batch Gradient Norm: 13.512983400273512
Epoch: 7661, Batch Gradient Norm after: 13.512983400273512
Epoch 7662/10000, Prediction Accuracy = 64.59400000000001%, Loss = 0.2986984491348267
Epoch: 7662, Batch Gradient Norm: 14.377014676251557
Epoch: 7662, Batch Gradient Norm after: 14.377014676251557
Epoch 7663/10000, Prediction Accuracy = 64.494%, Loss = 0.29964513182640073
Epoch: 7663, Batch Gradient Norm: 16.734370182283584
Epoch: 7663, Batch Gradient Norm after: 16.734370182283584
Epoch 7664/10000, Prediction Accuracy = 64.53200000000001%, Loss = 0.3019116520881653
Epoch: 7664, Batch Gradient Norm: 15.77859887519664
Epoch: 7664, Batch Gradient Norm after: 15.77859887519664
Epoch 7665/10000, Prediction Accuracy = 64.462%, Loss = 0.30218344926834106
Epoch: 7665, Batch Gradient Norm: 16.052984496346376
Epoch: 7665, Batch Gradient Norm after: 16.052984496346376
Epoch 7666/10000, Prediction Accuracy = 64.652%, Loss = 0.30056278109550477
Epoch: 7666, Batch Gradient Norm: 16.495943162462794
Epoch: 7666, Batch Gradient Norm after: 16.495943162462794
Epoch 7667/10000, Prediction Accuracy = 64.57399999999998%, Loss = 0.30350462794303895
Epoch: 7667, Batch Gradient Norm: 17.817592929496843
Epoch: 7667, Batch Gradient Norm after: 17.817592929496843
Epoch 7668/10000, Prediction Accuracy = 64.41%, Loss = 0.3028129994869232
Epoch: 7668, Batch Gradient Norm: 15.6596937072296
Epoch: 7668, Batch Gradient Norm after: 15.6596937072296
Epoch 7669/10000, Prediction Accuracy = 64.558%, Loss = 0.3016305804252625
Epoch: 7669, Batch Gradient Norm: 13.086449234537659
Epoch: 7669, Batch Gradient Norm after: 13.086449234537659
Epoch 7670/10000, Prediction Accuracy = 64.48400000000001%, Loss = 0.2983886480331421
Epoch: 7670, Batch Gradient Norm: 15.220876536417173
Epoch: 7670, Batch Gradient Norm after: 15.220876536417173
Epoch 7671/10000, Prediction Accuracy = 64.65%, Loss = 0.299325293302536
Epoch: 7671, Batch Gradient Norm: 17.249318001647275
Epoch: 7671, Batch Gradient Norm after: 17.249318001647275
Epoch 7672/10000, Prediction Accuracy = 64.676%, Loss = 0.3026233732700348
Epoch: 7672, Batch Gradient Norm: 14.634466888100505
Epoch: 7672, Batch Gradient Norm after: 14.634466888100505
Epoch 7673/10000, Prediction Accuracy = 64.73%, Loss = 0.3012955725193024
Epoch: 7673, Batch Gradient Norm: 15.010165769359062
Epoch: 7673, Batch Gradient Norm after: 15.010165769359062
Epoch 7674/10000, Prediction Accuracy = 64.564%, Loss = 0.30045992136001587
Epoch: 7674, Batch Gradient Norm: 13.482115935098625
Epoch: 7674, Batch Gradient Norm after: 13.482115935098625
Epoch 7675/10000, Prediction Accuracy = 64.716%, Loss = 0.2996784746646881
Epoch: 7675, Batch Gradient Norm: 15.600748514503394
Epoch: 7675, Batch Gradient Norm after: 15.600748514503394
Epoch 7676/10000, Prediction Accuracy = 64.54%, Loss = 0.2992559611797333
Epoch: 7676, Batch Gradient Norm: 12.82146643078406
Epoch: 7676, Batch Gradient Norm after: 12.82146643078406
Epoch 7677/10000, Prediction Accuracy = 64.628%, Loss = 0.29590972065925597
Epoch: 7677, Batch Gradient Norm: 13.863330289978464
Epoch: 7677, Batch Gradient Norm after: 13.863330289978464
Epoch 7678/10000, Prediction Accuracy = 64.344%, Loss = 0.2987047016620636
Epoch: 7678, Batch Gradient Norm: 11.917789795690942
Epoch: 7678, Batch Gradient Norm after: 11.917789795690942
Epoch 7679/10000, Prediction Accuracy = 64.446%, Loss = 0.29672290086746217
Epoch: 7679, Batch Gradient Norm: 14.745548145508
Epoch: 7679, Batch Gradient Norm after: 14.745548145508
Epoch 7680/10000, Prediction Accuracy = 64.534%, Loss = 0.3005105912685394
Epoch: 7680, Batch Gradient Norm: 16.954252513208605
Epoch: 7680, Batch Gradient Norm after: 16.954252513208605
Epoch 7681/10000, Prediction Accuracy = 64.54%, Loss = 0.3038103818893433
Epoch: 7681, Batch Gradient Norm: 14.564943764431462
Epoch: 7681, Batch Gradient Norm after: 14.564943764431462
Epoch 7682/10000, Prediction Accuracy = 64.63000000000001%, Loss = 0.29807387590408324
Epoch: 7682, Batch Gradient Norm: 11.790989124064227
Epoch: 7682, Batch Gradient Norm after: 11.790989124064227
Epoch 7683/10000, Prediction Accuracy = 64.34800000000001%, Loss = 0.2978241264820099
Epoch: 7683, Batch Gradient Norm: 11.859667331031911
Epoch: 7683, Batch Gradient Norm after: 11.859667331031911
Epoch 7684/10000, Prediction Accuracy = 64.564%, Loss = 0.2948027551174164
Epoch: 7684, Batch Gradient Norm: 12.524068591614585
Epoch: 7684, Batch Gradient Norm after: 12.524068591614585
Epoch 7685/10000, Prediction Accuracy = 64.51000000000002%, Loss = 0.297391676902771
Epoch: 7685, Batch Gradient Norm: 12.672152637999988
Epoch: 7685, Batch Gradient Norm after: 12.672152637999988
Epoch 7686/10000, Prediction Accuracy = 64.534%, Loss = 0.29861901998519896
Epoch: 7686, Batch Gradient Norm: 11.298444461869675
Epoch: 7686, Batch Gradient Norm after: 11.298444461869675
Epoch 7687/10000, Prediction Accuracy = 64.584%, Loss = 0.2967341125011444
Epoch: 7687, Batch Gradient Norm: 13.531507120280313
Epoch: 7687, Batch Gradient Norm after: 13.531507120280313
Epoch 7688/10000, Prediction Accuracy = 64.512%, Loss = 0.2970352530479431
Epoch: 7688, Batch Gradient Norm: 13.656808558321597
Epoch: 7688, Batch Gradient Norm after: 13.656808558321597
Epoch 7689/10000, Prediction Accuracy = 64.572%, Loss = 0.2971869885921478
Epoch: 7689, Batch Gradient Norm: 13.755846597678396
Epoch: 7689, Batch Gradient Norm after: 13.755846597678396
Epoch 7690/10000, Prediction Accuracy = 64.42%, Loss = 0.30078468918800355
Epoch: 7690, Batch Gradient Norm: 13.895236062975018
Epoch: 7690, Batch Gradient Norm after: 13.895236062975018
Epoch 7691/10000, Prediction Accuracy = 64.49000000000001%, Loss = 0.29931222796440127
Epoch: 7691, Batch Gradient Norm: 13.577282826668437
Epoch: 7691, Batch Gradient Norm after: 13.577282826668437
Epoch 7692/10000, Prediction Accuracy = 64.518%, Loss = 0.29788129329681395
Epoch: 7692, Batch Gradient Norm: 15.505135304514306
Epoch: 7692, Batch Gradient Norm after: 15.505135304514306
Epoch 7693/10000, Prediction Accuracy = 64.604%, Loss = 0.29895237684249876
Epoch: 7693, Batch Gradient Norm: 14.005960942547107
Epoch: 7693, Batch Gradient Norm after: 14.005960942547107
Epoch 7694/10000, Prediction Accuracy = 64.328%, Loss = 0.29909608960151673
Epoch: 7694, Batch Gradient Norm: 15.837121287565944
Epoch: 7694, Batch Gradient Norm after: 15.837121287565944
Epoch 7695/10000, Prediction Accuracy = 64.60799999999999%, Loss = 0.3026661217212677
Epoch: 7695, Batch Gradient Norm: 15.92246568485285
Epoch: 7695, Batch Gradient Norm after: 15.92246568485285
Epoch 7696/10000, Prediction Accuracy = 64.43800000000002%, Loss = 0.30180822014808656
Epoch: 7696, Batch Gradient Norm: 16.541715695698944
Epoch: 7696, Batch Gradient Norm after: 16.541715695698944
Epoch 7697/10000, Prediction Accuracy = 64.506%, Loss = 0.3021854221820831
Epoch: 7697, Batch Gradient Norm: 16.302832361399055
Epoch: 7697, Batch Gradient Norm after: 16.302832361399055
Epoch 7698/10000, Prediction Accuracy = 64.612%, Loss = 0.30248731970787046
Epoch: 7698, Batch Gradient Norm: 18.315615720337245
Epoch: 7698, Batch Gradient Norm after: 18.315615720337245
Epoch 7699/10000, Prediction Accuracy = 64.616%, Loss = 0.3047141134738922
Epoch: 7699, Batch Gradient Norm: 17.896537671594604
Epoch: 7699, Batch Gradient Norm after: 17.896537671594604
Epoch 7700/10000, Prediction Accuracy = 64.52799999999999%, Loss = 0.3031574249267578
Epoch: 7700, Batch Gradient Norm: 14.706772801898223
Epoch: 7700, Batch Gradient Norm after: 14.706772801898223
Epoch 7701/10000, Prediction Accuracy = 64.63199999999999%, Loss = 0.29910929799079894
Epoch: 7701, Batch Gradient Norm: 15.13386668382687
Epoch: 7701, Batch Gradient Norm after: 15.13386668382687
Epoch 7702/10000, Prediction Accuracy = 64.546%, Loss = 0.3009601891040802
Epoch: 7702, Batch Gradient Norm: 14.884742749314153
Epoch: 7702, Batch Gradient Norm after: 14.884742749314153
Epoch 7703/10000, Prediction Accuracy = 64.49%, Loss = 0.3021468997001648
Epoch: 7703, Batch Gradient Norm: 14.2236239095479
Epoch: 7703, Batch Gradient Norm after: 14.2236239095479
Epoch 7704/10000, Prediction Accuracy = 64.59200000000001%, Loss = 0.29784446954727173
Epoch: 7704, Batch Gradient Norm: 13.213199575493597
Epoch: 7704, Batch Gradient Norm after: 13.213199575493597
Epoch 7705/10000, Prediction Accuracy = 64.58000000000001%, Loss = 0.2975907564163208
Epoch: 7705, Batch Gradient Norm: 12.272154832846637
Epoch: 7705, Batch Gradient Norm after: 12.272154832846637
Epoch 7706/10000, Prediction Accuracy = 64.53%, Loss = 0.2963169991970062
Epoch: 7706, Batch Gradient Norm: 15.503723637542594
Epoch: 7706, Batch Gradient Norm after: 15.503723637542594
Epoch 7707/10000, Prediction Accuracy = 64.452%, Loss = 0.301199346780777
Epoch: 7707, Batch Gradient Norm: 16.95089030725781
Epoch: 7707, Batch Gradient Norm after: 16.90494261494463
Epoch 7708/10000, Prediction Accuracy = 64.638%, Loss = 0.30273337960243224
Epoch: 7708, Batch Gradient Norm: 17.88798777116725
Epoch: 7708, Batch Gradient Norm after: 17.88798777116725
Epoch 7709/10000, Prediction Accuracy = 64.464%, Loss = 0.3044136822223663
Epoch: 7709, Batch Gradient Norm: 17.144439095069266
Epoch: 7709, Batch Gradient Norm after: 17.144439095069266
Epoch 7710/10000, Prediction Accuracy = 64.586%, Loss = 0.30329756140708924
Epoch: 7710, Batch Gradient Norm: 13.067235590181504
Epoch: 7710, Batch Gradient Norm after: 13.067235590181504
Epoch 7711/10000, Prediction Accuracy = 64.474%, Loss = 0.29547218680381776
Epoch: 7711, Batch Gradient Norm: 16.6849561883795
Epoch: 7711, Batch Gradient Norm after: 16.6849561883795
Epoch 7712/10000, Prediction Accuracy = 64.434%, Loss = 0.3031455993652344
Epoch: 7712, Batch Gradient Norm: 14.415148320894973
Epoch: 7712, Batch Gradient Norm after: 14.415148320894973
Epoch 7713/10000, Prediction Accuracy = 64.57600000000001%, Loss = 0.29786946177482604
Epoch: 7713, Batch Gradient Norm: 14.048211747569663
Epoch: 7713, Batch Gradient Norm after: 14.048211747569663
Epoch 7714/10000, Prediction Accuracy = 64.428%, Loss = 0.29670228958129885
Epoch: 7714, Batch Gradient Norm: 15.249687294575065
Epoch: 7714, Batch Gradient Norm after: 15.249687294575065
Epoch 7715/10000, Prediction Accuracy = 64.336%, Loss = 0.3005469560623169
Epoch: 7715, Batch Gradient Norm: 15.842365655581874
Epoch: 7715, Batch Gradient Norm after: 15.842365655581874
Epoch 7716/10000, Prediction Accuracy = 64.63799999999999%, Loss = 0.3020492970943451
Epoch: 7716, Batch Gradient Norm: 17.24146292730243
Epoch: 7716, Batch Gradient Norm after: 17.24146292730243
Epoch 7717/10000, Prediction Accuracy = 64.51%, Loss = 0.30518742799758913
Epoch: 7717, Batch Gradient Norm: 15.32012392508612
Epoch: 7717, Batch Gradient Norm after: 15.32012392508612
Epoch 7718/10000, Prediction Accuracy = 64.566%, Loss = 0.29943509101867677
Epoch: 7718, Batch Gradient Norm: 13.425327583231056
Epoch: 7718, Batch Gradient Norm after: 13.425327583231056
Epoch 7719/10000, Prediction Accuracy = 64.46400000000001%, Loss = 0.3020852029323578
Epoch: 7719, Batch Gradient Norm: 13.854548366832066
Epoch: 7719, Batch Gradient Norm after: 13.854548366832066
Epoch 7720/10000, Prediction Accuracy = 64.56200000000001%, Loss = 0.29817736744880674
Epoch: 7720, Batch Gradient Norm: 14.045171890066964
Epoch: 7720, Batch Gradient Norm after: 14.045171890066964
Epoch 7721/10000, Prediction Accuracy = 64.50399999999999%, Loss = 0.2965949773788452
Epoch: 7721, Batch Gradient Norm: 13.644175839499477
Epoch: 7721, Batch Gradient Norm after: 13.644175839499477
Epoch 7722/10000, Prediction Accuracy = 64.476%, Loss = 0.29815263152122495
Epoch: 7722, Batch Gradient Norm: 15.321111206525746
Epoch: 7722, Batch Gradient Norm after: 15.321111206525746
Epoch 7723/10000, Prediction Accuracy = 64.506%, Loss = 0.3014179289340973
Epoch: 7723, Batch Gradient Norm: 13.914631246301612
Epoch: 7723, Batch Gradient Norm after: 13.914631246301612
Epoch 7724/10000, Prediction Accuracy = 64.402%, Loss = 0.29652584791183473
Epoch: 7724, Batch Gradient Norm: 17.491868705204446
Epoch: 7724, Batch Gradient Norm after: 16.657465105968857
Epoch 7725/10000, Prediction Accuracy = 64.536%, Loss = 0.3032020926475525
Epoch: 7725, Batch Gradient Norm: 24.02020423345364
Epoch: 7725, Batch Gradient Norm after: 20.86669059302133
Epoch 7726/10000, Prediction Accuracy = 64.51200000000001%, Loss = 0.3143795013427734
Epoch: 7726, Batch Gradient Norm: 20.790658907517614
Epoch: 7726, Batch Gradient Norm after: 18.854748712438397
Epoch 7727/10000, Prediction Accuracy = 64.53%, Loss = 0.30670523047447207
Epoch: 7727, Batch Gradient Norm: 19.858431807395082
Epoch: 7727, Batch Gradient Norm after: 19.365029773730537
Epoch 7728/10000, Prediction Accuracy = 64.446%, Loss = 0.3068592011928558
Epoch: 7728, Batch Gradient Norm: 17.91889903149681
Epoch: 7728, Batch Gradient Norm after: 17.91889903149681
Epoch 7729/10000, Prediction Accuracy = 64.46400000000001%, Loss = 0.30406705737113954
Epoch: 7729, Batch Gradient Norm: 17.23423116150605
Epoch: 7729, Batch Gradient Norm after: 17.23423116150605
Epoch 7730/10000, Prediction Accuracy = 64.532%, Loss = 0.3048909306526184
Epoch: 7730, Batch Gradient Norm: 16.935229748046307
Epoch: 7730, Batch Gradient Norm after: 16.450964784603
Epoch 7731/10000, Prediction Accuracy = 64.446%, Loss = 0.301556921005249
Epoch: 7731, Batch Gradient Norm: 14.068071760244822
Epoch: 7731, Batch Gradient Norm after: 14.068071760244822
Epoch 7732/10000, Prediction Accuracy = 64.636%, Loss = 0.29716031551361083
Epoch: 7732, Batch Gradient Norm: 13.13599495371871
Epoch: 7732, Batch Gradient Norm after: 13.13599495371871
Epoch 7733/10000, Prediction Accuracy = 64.58%, Loss = 0.2957650661468506
Epoch: 7733, Batch Gradient Norm: 13.416788053103405
Epoch: 7733, Batch Gradient Norm after: 13.416788053103405
Epoch 7734/10000, Prediction Accuracy = 64.612%, Loss = 0.2996630609035492
Epoch: 7734, Batch Gradient Norm: 15.71899744011474
Epoch: 7734, Batch Gradient Norm after: 15.63534301422428
Epoch 7735/10000, Prediction Accuracy = 64.572%, Loss = 0.2987828433513641
Epoch: 7735, Batch Gradient Norm: 17.667030100826274
Epoch: 7735, Batch Gradient Norm after: 17.667030100826274
Epoch 7736/10000, Prediction Accuracy = 64.438%, Loss = 0.303340357542038
Epoch: 7736, Batch Gradient Norm: 15.739984435244715
Epoch: 7736, Batch Gradient Norm after: 15.739984435244715
Epoch 7737/10000, Prediction Accuracy = 64.55799999999999%, Loss = 0.300725531578064
Epoch: 7737, Batch Gradient Norm: 14.604763953810092
Epoch: 7737, Batch Gradient Norm after: 14.604763953810092
Epoch 7738/10000, Prediction Accuracy = 64.66400000000002%, Loss = 0.2978549003601074
Epoch: 7738, Batch Gradient Norm: 15.510802123523261
Epoch: 7738, Batch Gradient Norm after: 15.510802123523261
Epoch 7739/10000, Prediction Accuracy = 64.526%, Loss = 0.30036598443984985
Epoch: 7739, Batch Gradient Norm: 13.710071556372313
Epoch: 7739, Batch Gradient Norm after: 13.710071556372313
Epoch 7740/10000, Prediction Accuracy = 64.614%, Loss = 0.29833335280418394
Epoch: 7740, Batch Gradient Norm: 12.995081027718259
Epoch: 7740, Batch Gradient Norm after: 12.995081027718259
Epoch 7741/10000, Prediction Accuracy = 64.64%, Loss = 0.2980993211269379
Epoch: 7741, Batch Gradient Norm: 15.574776742954324
Epoch: 7741, Batch Gradient Norm after: 15.574776742954324
Epoch 7742/10000, Prediction Accuracy = 64.662%, Loss = 0.3000834047794342
Epoch: 7742, Batch Gradient Norm: 17.061034541997035
Epoch: 7742, Batch Gradient Norm after: 17.061034541997035
Epoch 7743/10000, Prediction Accuracy = 64.55799999999999%, Loss = 0.30128014087677
Epoch: 7743, Batch Gradient Norm: 14.196528850584361
Epoch: 7743, Batch Gradient Norm after: 14.196528850584361
Epoch 7744/10000, Prediction Accuracy = 64.566%, Loss = 0.29583951234817507
Epoch: 7744, Batch Gradient Norm: 13.916180147205953
Epoch: 7744, Batch Gradient Norm after: 13.916180147205953
Epoch 7745/10000, Prediction Accuracy = 64.454%, Loss = 0.30014036893844603
Epoch: 7745, Batch Gradient Norm: 15.853719927337227
Epoch: 7745, Batch Gradient Norm after: 15.853719927337227
Epoch 7746/10000, Prediction Accuracy = 64.588%, Loss = 0.30076864957809446
Epoch: 7746, Batch Gradient Norm: 14.77666264438007
Epoch: 7746, Batch Gradient Norm after: 14.77666264438007
Epoch 7747/10000, Prediction Accuracy = 64.6%, Loss = 0.29754768013954164
Epoch: 7747, Batch Gradient Norm: 14.982621212915003
Epoch: 7747, Batch Gradient Norm after: 14.982621212915003
Epoch 7748/10000, Prediction Accuracy = 64.526%, Loss = 0.30055056810379027
Epoch: 7748, Batch Gradient Norm: 13.822263441787445
Epoch: 7748, Batch Gradient Norm after: 13.822263441787445
Epoch 7749/10000, Prediction Accuracy = 64.61600000000001%, Loss = 0.2972740411758423
Epoch: 7749, Batch Gradient Norm: 11.842041282971488
Epoch: 7749, Batch Gradient Norm after: 11.842041282971488
Epoch 7750/10000, Prediction Accuracy = 64.5%, Loss = 0.29576738476753234
Epoch: 7750, Batch Gradient Norm: 13.306109826607797
Epoch: 7750, Batch Gradient Norm after: 13.306109826607797
Epoch 7751/10000, Prediction Accuracy = 64.50999999999999%, Loss = 0.29660224318504336
Epoch: 7751, Batch Gradient Norm: 16.861674776359393
Epoch: 7751, Batch Gradient Norm after: 16.861674776359393
Epoch 7752/10000, Prediction Accuracy = 64.6%, Loss = 0.30016512870788575
Epoch: 7752, Batch Gradient Norm: 17.540954883925224
Epoch: 7752, Batch Gradient Norm after: 17.540954883925224
Epoch 7753/10000, Prediction Accuracy = 64.522%, Loss = 0.30097495317459105
Epoch: 7753, Batch Gradient Norm: 16.890069311537182
Epoch: 7753, Batch Gradient Norm after: 16.890069311537182
Epoch 7754/10000, Prediction Accuracy = 64.654%, Loss = 0.3003504276275635
Epoch: 7754, Batch Gradient Norm: 15.120704544720306
Epoch: 7754, Batch Gradient Norm after: 15.120704544720306
Epoch 7755/10000, Prediction Accuracy = 64.59799999999998%, Loss = 0.2980427324771881
Epoch: 7755, Batch Gradient Norm: 14.880971794579409
Epoch: 7755, Batch Gradient Norm after: 14.880971794579409
Epoch 7756/10000, Prediction Accuracy = 64.56400000000001%, Loss = 0.2982372581958771
Epoch: 7756, Batch Gradient Norm: 18.124225000472745
Epoch: 7756, Batch Gradient Norm after: 18.124225000472745
Epoch 7757/10000, Prediction Accuracy = 64.574%, Loss = 0.301550954580307
Epoch: 7757, Batch Gradient Norm: 16.391199952665563
Epoch: 7757, Batch Gradient Norm after: 16.391199952665563
Epoch 7758/10000, Prediction Accuracy = 64.59599999999999%, Loss = 0.30048959255218505
Epoch: 7758, Batch Gradient Norm: 16.715194008671023
Epoch: 7758, Batch Gradient Norm after: 16.715194008671023
Epoch 7759/10000, Prediction Accuracy = 64.63799999999999%, Loss = 0.3016119062900543
Epoch: 7759, Batch Gradient Norm: 13.116921264422583
Epoch: 7759, Batch Gradient Norm after: 13.116921264422583
Epoch 7760/10000, Prediction Accuracy = 64.592%, Loss = 0.2953345596790314
Epoch: 7760, Batch Gradient Norm: 14.936111027504756
Epoch: 7760, Batch Gradient Norm after: 14.936111027504756
Epoch 7761/10000, Prediction Accuracy = 64.568%, Loss = 0.2981379747390747
Epoch: 7761, Batch Gradient Norm: 11.90497053422864
Epoch: 7761, Batch Gradient Norm after: 11.90497053422864
Epoch 7762/10000, Prediction Accuracy = 64.676%, Loss = 0.2955669164657593
Epoch: 7762, Batch Gradient Norm: 14.989277830152538
Epoch: 7762, Batch Gradient Norm after: 14.989277830152538
Epoch 7763/10000, Prediction Accuracy = 64.634%, Loss = 0.2973582923412323
Epoch: 7763, Batch Gradient Norm: 14.489290882544694
Epoch: 7763, Batch Gradient Norm after: 14.489290882544694
Epoch 7764/10000, Prediction Accuracy = 64.552%, Loss = 0.2986673414707184
Epoch: 7764, Batch Gradient Norm: 14.994168098921273
Epoch: 7764, Batch Gradient Norm after: 14.994168098921273
Epoch 7765/10000, Prediction Accuracy = 64.578%, Loss = 0.29848950505256655
Epoch: 7765, Batch Gradient Norm: 14.528502335953663
Epoch: 7765, Batch Gradient Norm after: 14.528502335953663
Epoch 7766/10000, Prediction Accuracy = 64.56800000000001%, Loss = 0.29855270981788634
Epoch: 7766, Batch Gradient Norm: 16.97182227931052
Epoch: 7766, Batch Gradient Norm after: 16.97182227931052
Epoch 7767/10000, Prediction Accuracy = 64.53%, Loss = 0.3056961536407471
Epoch: 7767, Batch Gradient Norm: 16.10310403277358
Epoch: 7767, Batch Gradient Norm after: 16.10310403277358
Epoch 7768/10000, Prediction Accuracy = 64.46%, Loss = 0.30193541646003724
Epoch: 7768, Batch Gradient Norm: 14.025341914116133
Epoch: 7768, Batch Gradient Norm after: 14.025341914116133
Epoch 7769/10000, Prediction Accuracy = 64.47200000000001%, Loss = 0.2970310151576996
Epoch: 7769, Batch Gradient Norm: 12.636327791882003
Epoch: 7769, Batch Gradient Norm after: 12.636327791882003
Epoch 7770/10000, Prediction Accuracy = 64.618%, Loss = 0.29490050077438357
Epoch: 7770, Batch Gradient Norm: 13.605542185701967
Epoch: 7770, Batch Gradient Norm after: 13.605542185701967
Epoch 7771/10000, Prediction Accuracy = 64.498%, Loss = 0.30067577958106995
Epoch: 7771, Batch Gradient Norm: 12.606253478155441
Epoch: 7771, Batch Gradient Norm after: 12.606253478155441
Epoch 7772/10000, Prediction Accuracy = 64.56800000000001%, Loss = 0.29754220843315127
Epoch: 7772, Batch Gradient Norm: 14.348155094360724
Epoch: 7772, Batch Gradient Norm after: 14.348155094360724
Epoch 7773/10000, Prediction Accuracy = 64.62%, Loss = 0.29661707282066346
Epoch: 7773, Batch Gradient Norm: 19.008551984419253
Epoch: 7773, Batch Gradient Norm after: 19.008551984419253
Epoch 7774/10000, Prediction Accuracy = 64.32000000000001%, Loss = 0.30815443992614744
Epoch: 7774, Batch Gradient Norm: 15.426982975677479
Epoch: 7774, Batch Gradient Norm after: 15.426982975677479
Epoch 7775/10000, Prediction Accuracy = 64.622%, Loss = 0.2982204020023346
Epoch: 7775, Batch Gradient Norm: 14.274856836038722
Epoch: 7775, Batch Gradient Norm after: 14.274856836038722
Epoch 7776/10000, Prediction Accuracy = 64.564%, Loss = 0.2974411904811859
Epoch: 7776, Batch Gradient Norm: 15.35906241964488
Epoch: 7776, Batch Gradient Norm after: 15.35906241964488
Epoch 7777/10000, Prediction Accuracy = 64.66199999999999%, Loss = 0.29774990677833557
Epoch: 7777, Batch Gradient Norm: 14.17592369949809
Epoch: 7777, Batch Gradient Norm after: 14.17592369949809
Epoch 7778/10000, Prediction Accuracy = 64.644%, Loss = 0.29669387340545655
Epoch: 7778, Batch Gradient Norm: 14.660648697662799
Epoch: 7778, Batch Gradient Norm after: 14.660648697662799
Epoch 7779/10000, Prediction Accuracy = 64.52799999999999%, Loss = 0.29785135984420774
Epoch: 7779, Batch Gradient Norm: 14.285119632423061
Epoch: 7779, Batch Gradient Norm after: 14.285119632423061
Epoch 7780/10000, Prediction Accuracy = 64.614%, Loss = 0.30112618803977964
Epoch: 7780, Batch Gradient Norm: 14.38997994627694
Epoch: 7780, Batch Gradient Norm after: 14.38997994627694
Epoch 7781/10000, Prediction Accuracy = 64.566%, Loss = 0.2977030038833618
Epoch: 7781, Batch Gradient Norm: 10.58168880277037
Epoch: 7781, Batch Gradient Norm after: 10.58168880277037
Epoch 7782/10000, Prediction Accuracy = 64.686%, Loss = 0.29178565740585327
Epoch: 7782, Batch Gradient Norm: 14.996440789626343
Epoch: 7782, Batch Gradient Norm after: 14.996440789626343
Epoch 7783/10000, Prediction Accuracy = 64.694%, Loss = 0.29985188841819765
Epoch: 7783, Batch Gradient Norm: 15.080971027235229
Epoch: 7783, Batch Gradient Norm after: 15.080971027235229
Epoch 7784/10000, Prediction Accuracy = 64.69200000000001%, Loss = 0.29702102541923525
Epoch: 7784, Batch Gradient Norm: 12.609085584724733
Epoch: 7784, Batch Gradient Norm after: 12.609085584724733
Epoch 7785/10000, Prediction Accuracy = 64.546%, Loss = 0.29528979063034055
Epoch: 7785, Batch Gradient Norm: 13.5869790078845
Epoch: 7785, Batch Gradient Norm after: 13.5869790078845
Epoch 7786/10000, Prediction Accuracy = 64.55199999999999%, Loss = 0.29603165984153745
Epoch: 7786, Batch Gradient Norm: 11.669050460951983
Epoch: 7786, Batch Gradient Norm after: 11.669050460951983
Epoch 7787/10000, Prediction Accuracy = 64.494%, Loss = 0.2944155991077423
Epoch: 7787, Batch Gradient Norm: 14.270075829932578
Epoch: 7787, Batch Gradient Norm after: 14.270075829932578
Epoch 7788/10000, Prediction Accuracy = 64.478%, Loss = 0.2998079121112823
Epoch: 7788, Batch Gradient Norm: 15.435865804613538
Epoch: 7788, Batch Gradient Norm after: 15.435865804613538
Epoch 7789/10000, Prediction Accuracy = 64.52399999999999%, Loss = 0.2988231718540192
Epoch: 7789, Batch Gradient Norm: 13.174266857350304
Epoch: 7789, Batch Gradient Norm after: 13.174266857350304
Epoch 7790/10000, Prediction Accuracy = 64.46199999999999%, Loss = 0.29447824954986573
Epoch: 7790, Batch Gradient Norm: 13.711302058193201
Epoch: 7790, Batch Gradient Norm after: 13.711302058193201
Epoch 7791/10000, Prediction Accuracy = 64.614%, Loss = 0.2957558512687683
Epoch: 7791, Batch Gradient Norm: 16.948151817416225
Epoch: 7791, Batch Gradient Norm after: 15.952249586237809
Epoch 7792/10000, Prediction Accuracy = 64.592%, Loss = 0.2990727424621582
Epoch: 7792, Batch Gradient Norm: 14.493698065262938
Epoch: 7792, Batch Gradient Norm after: 14.493698065262938
Epoch 7793/10000, Prediction Accuracy = 64.476%, Loss = 0.2980614960193634
Epoch: 7793, Batch Gradient Norm: 16.91524641106686
Epoch: 7793, Batch Gradient Norm after: 16.91524641106686
Epoch 7794/10000, Prediction Accuracy = 64.518%, Loss = 0.3005208671092987
Epoch: 7794, Batch Gradient Norm: 17.589706953107385
Epoch: 7794, Batch Gradient Norm after: 17.589706953107385
Epoch 7795/10000, Prediction Accuracy = 64.624%, Loss = 0.3016685307025909
Epoch: 7795, Batch Gradient Norm: 17.570294492175133
Epoch: 7795, Batch Gradient Norm after: 17.570294492175133
Epoch 7796/10000, Prediction Accuracy = 64.656%, Loss = 0.3012098968029022
Epoch: 7796, Batch Gradient Norm: 15.80467962857664
Epoch: 7796, Batch Gradient Norm after: 15.80467962857664
Epoch 7797/10000, Prediction Accuracy = 64.614%, Loss = 0.29623265862464904
Epoch: 7797, Batch Gradient Norm: 17.01936438444314
Epoch: 7797, Batch Gradient Norm after: 17.01936438444314
Epoch 7798/10000, Prediction Accuracy = 64.674%, Loss = 0.30277469754219055
Epoch: 7798, Batch Gradient Norm: 16.361312556500977
Epoch: 7798, Batch Gradient Norm after: 16.361312556500977
Epoch 7799/10000, Prediction Accuracy = 64.562%, Loss = 0.30053873658180236
Epoch: 7799, Batch Gradient Norm: 14.194437776533949
Epoch: 7799, Batch Gradient Norm after: 14.194437776533949
Epoch 7800/10000, Prediction Accuracy = 64.674%, Loss = 0.29665628671646116
Epoch: 7800, Batch Gradient Norm: 12.810432140522627
Epoch: 7800, Batch Gradient Norm after: 12.810432140522627
Epoch 7801/10000, Prediction Accuracy = 64.664%, Loss = 0.29556772112846375
Epoch: 7801, Batch Gradient Norm: 13.332162671768836
Epoch: 7801, Batch Gradient Norm after: 13.332162671768836
Epoch 7802/10000, Prediction Accuracy = 64.696%, Loss = 0.2950893878936768
Epoch: 7802, Batch Gradient Norm: 13.343817336141512
Epoch: 7802, Batch Gradient Norm after: 13.343817336141512
Epoch 7803/10000, Prediction Accuracy = 64.53%, Loss = 0.2954840183258057
Epoch: 7803, Batch Gradient Norm: 12.49843086394956
Epoch: 7803, Batch Gradient Norm after: 12.49843086394956
Epoch 7804/10000, Prediction Accuracy = 64.546%, Loss = 0.294609934091568
Epoch: 7804, Batch Gradient Norm: 11.703235176170079
Epoch: 7804, Batch Gradient Norm after: 11.703235176170079
Epoch 7805/10000, Prediction Accuracy = 64.75%, Loss = 0.2926767110824585
Epoch: 7805, Batch Gradient Norm: 14.292662659074074
Epoch: 7805, Batch Gradient Norm after: 14.292662659074074
Epoch 7806/10000, Prediction Accuracy = 64.654%, Loss = 0.2984329581260681
Epoch: 7806, Batch Gradient Norm: 15.61220685949275
Epoch: 7806, Batch Gradient Norm after: 15.61220685949275
Epoch 7807/10000, Prediction Accuracy = 64.712%, Loss = 0.2969414234161377
Epoch: 7807, Batch Gradient Norm: 13.201627391580798
Epoch: 7807, Batch Gradient Norm after: 13.201627391580798
Epoch 7808/10000, Prediction Accuracy = 64.622%, Loss = 0.2947091817855835
Epoch: 7808, Batch Gradient Norm: 13.048710816794653
Epoch: 7808, Batch Gradient Norm after: 13.048710816794653
Epoch 7809/10000, Prediction Accuracy = 64.78999999999999%, Loss = 0.29408764839172363
Epoch: 7809, Batch Gradient Norm: 13.081065673427299
Epoch: 7809, Batch Gradient Norm after: 13.081065673427299
Epoch 7810/10000, Prediction Accuracy = 64.61600000000001%, Loss = 0.2945405185222626
Epoch: 7810, Batch Gradient Norm: 12.98898243714448
Epoch: 7810, Batch Gradient Norm after: 12.98898243714448
Epoch 7811/10000, Prediction Accuracy = 64.566%, Loss = 0.2937114179134369
Epoch: 7811, Batch Gradient Norm: 13.25968130459733
Epoch: 7811, Batch Gradient Norm after: 13.25968130459733
Epoch 7812/10000, Prediction Accuracy = 64.646%, Loss = 0.2934822678565979
Epoch: 7812, Batch Gradient Norm: 14.40002070044807
Epoch: 7812, Batch Gradient Norm after: 14.40002070044807
Epoch 7813/10000, Prediction Accuracy = 64.726%, Loss = 0.2951678395271301
Epoch: 7813, Batch Gradient Norm: 15.710050176502149
Epoch: 7813, Batch Gradient Norm after: 15.710050176502149
Epoch 7814/10000, Prediction Accuracy = 64.64000000000001%, Loss = 0.2981963038444519
Epoch: 7814, Batch Gradient Norm: 16.020583665014733
Epoch: 7814, Batch Gradient Norm after: 16.020583665014733
Epoch 7815/10000, Prediction Accuracy = 64.53999999999999%, Loss = 0.29869748950004577
Epoch: 7815, Batch Gradient Norm: 16.570389813897574
Epoch: 7815, Batch Gradient Norm after: 16.483191590163084
Epoch 7816/10000, Prediction Accuracy = 64.488%, Loss = 0.2984925925731659
Epoch: 7816, Batch Gradient Norm: 14.010784679182576
Epoch: 7816, Batch Gradient Norm after: 14.010784679182576
Epoch 7817/10000, Prediction Accuracy = 64.542%, Loss = 0.295149564743042
Epoch: 7817, Batch Gradient Norm: 16.769235334437308
Epoch: 7817, Batch Gradient Norm after: 16.607714267902146
Epoch 7818/10000, Prediction Accuracy = 64.50800000000001%, Loss = 0.3006187975406647
Epoch: 7818, Batch Gradient Norm: 14.952803490795745
Epoch: 7818, Batch Gradient Norm after: 14.952803490795745
Epoch 7819/10000, Prediction Accuracy = 64.53200000000001%, Loss = 0.2961824655532837
Epoch: 7819, Batch Gradient Norm: 10.964116093783609
Epoch: 7819, Batch Gradient Norm after: 10.964116093783609
Epoch 7820/10000, Prediction Accuracy = 64.516%, Loss = 0.2928357064723969
Epoch: 7820, Batch Gradient Norm: 11.287836224736312
Epoch: 7820, Batch Gradient Norm after: 11.287836224736312
Epoch 7821/10000, Prediction Accuracy = 64.62800000000001%, Loss = 0.2929671823978424
Epoch: 7821, Batch Gradient Norm: 11.48312273093196
Epoch: 7821, Batch Gradient Norm after: 11.48312273093196
Epoch 7822/10000, Prediction Accuracy = 64.512%, Loss = 0.29408048987388613
Epoch: 7822, Batch Gradient Norm: 10.599018072534038
Epoch: 7822, Batch Gradient Norm after: 10.599018072534038
Epoch 7823/10000, Prediction Accuracy = 64.576%, Loss = 0.29277206063270567
Epoch: 7823, Batch Gradient Norm: 13.955643087316643
Epoch: 7823, Batch Gradient Norm after: 13.955643087316643
Epoch 7824/10000, Prediction Accuracy = 64.752%, Loss = 0.29889650344848634
Epoch: 7824, Batch Gradient Norm: 15.355000736336159
Epoch: 7824, Batch Gradient Norm after: 15.355000736336159
Epoch 7825/10000, Prediction Accuracy = 64.672%, Loss = 0.2981959342956543
Epoch: 7825, Batch Gradient Norm: 14.388006640066063
Epoch: 7825, Batch Gradient Norm after: 14.388006640066063
Epoch 7826/10000, Prediction Accuracy = 64.66%, Loss = 0.29372201561927797
Epoch: 7826, Batch Gradient Norm: 13.120166095096366
Epoch: 7826, Batch Gradient Norm after: 13.120166095096366
Epoch 7827/10000, Prediction Accuracy = 64.782%, Loss = 0.2919375240802765
Epoch: 7827, Batch Gradient Norm: 12.419657735156497
Epoch: 7827, Batch Gradient Norm after: 12.419657735156497
Epoch 7828/10000, Prediction Accuracy = 64.47%, Loss = 0.2940189361572266
Epoch: 7828, Batch Gradient Norm: 16.162619543569264
Epoch: 7828, Batch Gradient Norm after: 16.162619543569264
Epoch 7829/10000, Prediction Accuracy = 64.622%, Loss = 0.29739624857902525
Epoch: 7829, Batch Gradient Norm: 16.639811011566817
Epoch: 7829, Batch Gradient Norm after: 16.639811011566817
Epoch 7830/10000, Prediction Accuracy = 64.716%, Loss = 0.2993377447128296
Epoch: 7830, Batch Gradient Norm: 17.17043765924824
Epoch: 7830, Batch Gradient Norm after: 17.17043765924824
Epoch 7831/10000, Prediction Accuracy = 64.596%, Loss = 0.30122373104095457
Epoch: 7831, Batch Gradient Norm: 17.53883611042174
Epoch: 7831, Batch Gradient Norm after: 17.53883611042174
Epoch 7832/10000, Prediction Accuracy = 64.694%, Loss = 0.2973606765270233
Epoch: 7832, Batch Gradient Norm: 16.71673241142418
Epoch: 7832, Batch Gradient Norm after: 16.71673241142418
Epoch 7833/10000, Prediction Accuracy = 64.61800000000001%, Loss = 0.2986863136291504
Epoch: 7833, Batch Gradient Norm: 15.717925574310502
Epoch: 7833, Batch Gradient Norm after: 15.717925574310502
Epoch 7834/10000, Prediction Accuracy = 64.58200000000001%, Loss = 0.2971729815006256
Epoch: 7834, Batch Gradient Norm: 14.98010484606459
Epoch: 7834, Batch Gradient Norm after: 14.98010484606459
Epoch 7835/10000, Prediction Accuracy = 64.64399999999999%, Loss = 0.29702791571617126
Epoch: 7835, Batch Gradient Norm: 12.600842986817858
Epoch: 7835, Batch Gradient Norm after: 12.600842986817858
Epoch 7836/10000, Prediction Accuracy = 64.61800000000001%, Loss = 0.29498488306999204
Epoch: 7836, Batch Gradient Norm: 15.385764614218717
Epoch: 7836, Batch Gradient Norm after: 15.385764614218717
Epoch 7837/10000, Prediction Accuracy = 64.52000000000001%, Loss = 0.2982369065284729
Epoch: 7837, Batch Gradient Norm: 12.883584577446781
Epoch: 7837, Batch Gradient Norm after: 12.883584577446781
Epoch 7838/10000, Prediction Accuracy = 64.752%, Loss = 0.2966946303844452
Epoch: 7838, Batch Gradient Norm: 13.472557524746039
Epoch: 7838, Batch Gradient Norm after: 13.472557524746039
Epoch 7839/10000, Prediction Accuracy = 64.524%, Loss = 0.29663410782814026
Epoch: 7839, Batch Gradient Norm: 14.267761436000637
Epoch: 7839, Batch Gradient Norm after: 14.267761436000637
Epoch 7840/10000, Prediction Accuracy = 64.59400000000001%, Loss = 0.29490503668785095
Epoch: 7840, Batch Gradient Norm: 14.741490716633997
Epoch: 7840, Batch Gradient Norm after: 14.741490716633997
Epoch 7841/10000, Prediction Accuracy = 64.492%, Loss = 0.2959658861160278
Epoch: 7841, Batch Gradient Norm: 14.312824493283912
Epoch: 7841, Batch Gradient Norm after: 14.312824493283912
Epoch 7842/10000, Prediction Accuracy = 64.574%, Loss = 0.29831514358520506
Epoch: 7842, Batch Gradient Norm: 13.607117538208234
Epoch: 7842, Batch Gradient Norm after: 13.607117538208234
Epoch 7843/10000, Prediction Accuracy = 64.674%, Loss = 0.2951778173446655
Epoch: 7843, Batch Gradient Norm: 11.192672608388083
Epoch: 7843, Batch Gradient Norm after: 11.192672608388083
Epoch 7844/10000, Prediction Accuracy = 64.648%, Loss = 0.2932827711105347
Epoch: 7844, Batch Gradient Norm: 14.039496864015096
Epoch: 7844, Batch Gradient Norm after: 14.039496864015096
Epoch 7845/10000, Prediction Accuracy = 64.544%, Loss = 0.2963036000728607
Epoch: 7845, Batch Gradient Norm: 12.786636864612248
Epoch: 7845, Batch Gradient Norm after: 12.786636864612248
Epoch 7846/10000, Prediction Accuracy = 64.646%, Loss = 0.29238943457603456
Epoch: 7846, Batch Gradient Norm: 14.66885684579242
Epoch: 7846, Batch Gradient Norm after: 14.66885684579242
Epoch 7847/10000, Prediction Accuracy = 64.64000000000001%, Loss = 0.296209716796875
Epoch: 7847, Batch Gradient Norm: 13.854568163704887
Epoch: 7847, Batch Gradient Norm after: 13.854568163704887
Epoch 7848/10000, Prediction Accuracy = 64.616%, Loss = 0.294515323638916
Epoch: 7848, Batch Gradient Norm: 11.424365803957816
Epoch: 7848, Batch Gradient Norm after: 11.424365803957816
Epoch 7849/10000, Prediction Accuracy = 64.646%, Loss = 0.2915651798248291
Epoch: 7849, Batch Gradient Norm: 13.405654412604425
Epoch: 7849, Batch Gradient Norm after: 13.405654412604425
Epoch 7850/10000, Prediction Accuracy = 64.662%, Loss = 0.2958869099617004
Epoch: 7850, Batch Gradient Norm: 14.920050311627586
Epoch: 7850, Batch Gradient Norm after: 14.920050311627586
Epoch 7851/10000, Prediction Accuracy = 64.56%, Loss = 0.2990402400493622
Epoch: 7851, Batch Gradient Norm: 11.51531768927405
Epoch: 7851, Batch Gradient Norm after: 11.51531768927405
Epoch 7852/10000, Prediction Accuracy = 64.486%, Loss = 0.2924666404724121
Epoch: 7852, Batch Gradient Norm: 11.503396615337422
Epoch: 7852, Batch Gradient Norm after: 11.503396615337422
Epoch 7853/10000, Prediction Accuracy = 64.64000000000001%, Loss = 0.2922949314117432
Epoch: 7853, Batch Gradient Norm: 12.103110817968526
Epoch: 7853, Batch Gradient Norm after: 12.103110817968526
Epoch 7854/10000, Prediction Accuracy = 64.776%, Loss = 0.29399721026420594
Epoch: 7854, Batch Gradient Norm: 13.170247750625423
Epoch: 7854, Batch Gradient Norm after: 13.170247750625423
Epoch 7855/10000, Prediction Accuracy = 64.536%, Loss = 0.29359795451164244
Epoch: 7855, Batch Gradient Norm: 15.036058044219374
Epoch: 7855, Batch Gradient Norm after: 15.036058044219374
Epoch 7856/10000, Prediction Accuracy = 64.614%, Loss = 0.2972499907016754
Epoch: 7856, Batch Gradient Norm: 17.85260319175273
Epoch: 7856, Batch Gradient Norm after: 17.85260319175273
Epoch 7857/10000, Prediction Accuracy = 64.708%, Loss = 0.2992527961730957
Epoch: 7857, Batch Gradient Norm: 18.379216962317074
Epoch: 7857, Batch Gradient Norm after: 18.379216962317074
Epoch 7858/10000, Prediction Accuracy = 64.55%, Loss = 0.3036799967288971
Epoch: 7858, Batch Gradient Norm: 15.466469874405433
Epoch: 7858, Batch Gradient Norm after: 15.466469874405433
Epoch 7859/10000, Prediction Accuracy = 64.64200000000001%, Loss = 0.2997662603855133
Epoch: 7859, Batch Gradient Norm: 12.800200672500713
Epoch: 7859, Batch Gradient Norm after: 12.800200672500713
Epoch 7860/10000, Prediction Accuracy = 64.52400000000002%, Loss = 0.294278085231781
Epoch: 7860, Batch Gradient Norm: 12.025895131763663
Epoch: 7860, Batch Gradient Norm after: 12.025895131763663
Epoch 7861/10000, Prediction Accuracy = 64.68%, Loss = 0.2932710587978363
Epoch: 7861, Batch Gradient Norm: 12.776558482918373
Epoch: 7861, Batch Gradient Norm after: 12.776558482918373
Epoch 7862/10000, Prediction Accuracy = 64.73%, Loss = 0.2928361833095551
Epoch: 7862, Batch Gradient Norm: 13.116666584637484
Epoch: 7862, Batch Gradient Norm after: 13.116666584637484
Epoch 7863/10000, Prediction Accuracy = 64.64399999999999%, Loss = 0.2920120894908905
Epoch: 7863, Batch Gradient Norm: 12.107538777931723
Epoch: 7863, Batch Gradient Norm after: 12.107538777931723
Epoch 7864/10000, Prediction Accuracy = 64.598%, Loss = 0.2943278789520264
Epoch: 7864, Batch Gradient Norm: 14.085627917348152
Epoch: 7864, Batch Gradient Norm after: 14.085627917348152
Epoch 7865/10000, Prediction Accuracy = 64.566%, Loss = 0.2953658878803253
Epoch: 7865, Batch Gradient Norm: 12.888698491636584
Epoch: 7865, Batch Gradient Norm after: 12.888698491636584
Epoch 7866/10000, Prediction Accuracy = 64.68200000000002%, Loss = 0.292440927028656
Epoch: 7866, Batch Gradient Norm: 12.256185748772161
Epoch: 7866, Batch Gradient Norm after: 12.256185748772161
Epoch 7867/10000, Prediction Accuracy = 64.732%, Loss = 0.2921792149543762
Epoch: 7867, Batch Gradient Norm: 13.934683753032784
Epoch: 7867, Batch Gradient Norm after: 13.934683753032784
Epoch 7868/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.2945773720741272
Epoch: 7868, Batch Gradient Norm: 15.172583909008054
Epoch: 7868, Batch Gradient Norm after: 15.172583909008054
Epoch 7869/10000, Prediction Accuracy = 64.67999999999999%, Loss = 0.29815974831581116
Epoch: 7869, Batch Gradient Norm: 14.27666100203998
Epoch: 7869, Batch Gradient Norm after: 14.27666100203998
Epoch 7870/10000, Prediction Accuracy = 64.58%, Loss = 0.29403053522109984
Epoch: 7870, Batch Gradient Norm: 14.397490097596581
Epoch: 7870, Batch Gradient Norm after: 14.397490097596581
Epoch 7871/10000, Prediction Accuracy = 64.69800000000001%, Loss = 0.29440792202949523
Epoch: 7871, Batch Gradient Norm: 15.31075097946297
Epoch: 7871, Batch Gradient Norm after: 15.31075097946297
Epoch 7872/10000, Prediction Accuracy = 64.65%, Loss = 0.2969901502132416
Epoch: 7872, Batch Gradient Norm: 16.143139451718245
Epoch: 7872, Batch Gradient Norm after: 16.143139451718245
Epoch 7873/10000, Prediction Accuracy = 64.774%, Loss = 0.29841322898864747
Epoch: 7873, Batch Gradient Norm: 15.747455280079391
Epoch: 7873, Batch Gradient Norm after: 15.747455280079391
Epoch 7874/10000, Prediction Accuracy = 64.67399999999999%, Loss = 0.29724454283714297
Epoch: 7874, Batch Gradient Norm: 13.915511608312883
Epoch: 7874, Batch Gradient Norm after: 13.915511608312883
Epoch 7875/10000, Prediction Accuracy = 64.614%, Loss = 0.2937365770339966
Epoch: 7875, Batch Gradient Norm: 14.383447737041305
Epoch: 7875, Batch Gradient Norm after: 14.383447737041305
Epoch 7876/10000, Prediction Accuracy = 64.67999999999999%, Loss = 0.2960615038871765
Epoch: 7876, Batch Gradient Norm: 13.328921958826601
Epoch: 7876, Batch Gradient Norm after: 13.328921958826601
Epoch 7877/10000, Prediction Accuracy = 64.642%, Loss = 0.2950936436653137
Epoch: 7877, Batch Gradient Norm: 14.714454688222595
Epoch: 7877, Batch Gradient Norm after: 14.714454688222595
Epoch 7878/10000, Prediction Accuracy = 64.78800000000001%, Loss = 0.2948709070682526
Epoch: 7878, Batch Gradient Norm: 15.138707550772608
Epoch: 7878, Batch Gradient Norm after: 15.138707550772608
Epoch 7879/10000, Prediction Accuracy = 64.59400000000001%, Loss = 0.2998919069766998
Epoch: 7879, Batch Gradient Norm: 13.200112313890152
Epoch: 7879, Batch Gradient Norm after: 13.200112313890152
Epoch 7880/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.29474706649780275
Epoch: 7880, Batch Gradient Norm: 14.245906540058048
Epoch: 7880, Batch Gradient Norm after: 14.245906540058048
Epoch 7881/10000, Prediction Accuracy = 64.702%, Loss = 0.2987674713134766
Epoch: 7881, Batch Gradient Norm: 14.096009766792697
Epoch: 7881, Batch Gradient Norm after: 14.096009766792697
Epoch 7882/10000, Prediction Accuracy = 64.606%, Loss = 0.2969901621341705
Epoch: 7882, Batch Gradient Norm: 15.509456038639748
Epoch: 7882, Batch Gradient Norm after: 15.509456038639748
Epoch 7883/10000, Prediction Accuracy = 64.648%, Loss = 0.29749747514724734
Epoch: 7883, Batch Gradient Norm: 16.0695640697429
Epoch: 7883, Batch Gradient Norm after: 16.0695640697429
Epoch 7884/10000, Prediction Accuracy = 64.564%, Loss = 0.29677813649177553
Epoch: 7884, Batch Gradient Norm: 16.00414381370931
Epoch: 7884, Batch Gradient Norm after: 16.00414381370931
Epoch 7885/10000, Prediction Accuracy = 64.532%, Loss = 0.2994146466255188
Epoch: 7885, Batch Gradient Norm: 14.693900895392247
Epoch: 7885, Batch Gradient Norm after: 14.693900895392247
Epoch 7886/10000, Prediction Accuracy = 64.69200000000001%, Loss = 0.29494765400886536
Epoch: 7886, Batch Gradient Norm: 13.047857411358994
Epoch: 7886, Batch Gradient Norm after: 13.047857411358994
Epoch 7887/10000, Prediction Accuracy = 64.67%, Loss = 0.2932907700538635
Epoch: 7887, Batch Gradient Norm: 12.314392660793942
Epoch: 7887, Batch Gradient Norm after: 12.314392660793942
Epoch 7888/10000, Prediction Accuracy = 64.596%, Loss = 0.2948072552680969
Epoch: 7888, Batch Gradient Norm: 12.316366628788913
Epoch: 7888, Batch Gradient Norm after: 12.316366628788913
Epoch 7889/10000, Prediction Accuracy = 64.53800000000001%, Loss = 0.2928905725479126
Epoch: 7889, Batch Gradient Norm: 15.712336590633399
Epoch: 7889, Batch Gradient Norm after: 15.712336590633399
Epoch 7890/10000, Prediction Accuracy = 64.76200000000001%, Loss = 0.29670531749725343
Epoch: 7890, Batch Gradient Norm: 16.404880250207736
Epoch: 7890, Batch Gradient Norm after: 16.404880250207736
Epoch 7891/10000, Prediction Accuracy = 64.65200000000002%, Loss = 0.2987754464149475
Epoch: 7891, Batch Gradient Norm: 11.837270864937157
Epoch: 7891, Batch Gradient Norm after: 11.837270864937157
Epoch 7892/10000, Prediction Accuracy = 64.64000000000001%, Loss = 0.2920592248439789
Epoch: 7892, Batch Gradient Norm: 12.020849605594172
Epoch: 7892, Batch Gradient Norm after: 12.020849605594172
Epoch 7893/10000, Prediction Accuracy = 64.654%, Loss = 0.2931480407714844
Epoch: 7893, Batch Gradient Norm: 13.037875788195178
Epoch: 7893, Batch Gradient Norm after: 13.037875788195178
Epoch 7894/10000, Prediction Accuracy = 64.834%, Loss = 0.29259621500968935
Epoch: 7894, Batch Gradient Norm: 12.694765795301665
Epoch: 7894, Batch Gradient Norm after: 12.694765795301665
Epoch 7895/10000, Prediction Accuracy = 64.732%, Loss = 0.29731630682945254
Epoch: 7895, Batch Gradient Norm: 13.337477626966862
Epoch: 7895, Batch Gradient Norm after: 13.337477626966862
Epoch 7896/10000, Prediction Accuracy = 64.672%, Loss = 0.2949131608009338
Epoch: 7896, Batch Gradient Norm: 14.018209321012396
Epoch: 7896, Batch Gradient Norm after: 14.018209321012396
Epoch 7897/10000, Prediction Accuracy = 64.718%, Loss = 0.29622005224227904
Epoch: 7897, Batch Gradient Norm: 15.312920635798916
Epoch: 7897, Batch Gradient Norm after: 15.312920635798916
Epoch 7898/10000, Prediction Accuracy = 64.596%, Loss = 0.29712753295898436
Epoch: 7898, Batch Gradient Norm: 16.764804327487408
Epoch: 7898, Batch Gradient Norm after: 16.764804327487408
Epoch 7899/10000, Prediction Accuracy = 64.628%, Loss = 0.2971951127052307
Epoch: 7899, Batch Gradient Norm: 14.179582681578216
Epoch: 7899, Batch Gradient Norm after: 14.179582681578216
Epoch 7900/10000, Prediction Accuracy = 64.53999999999999%, Loss = 0.29786098599433897
Epoch: 7900, Batch Gradient Norm: 14.023901659351848
Epoch: 7900, Batch Gradient Norm after: 14.023901659351848
Epoch 7901/10000, Prediction Accuracy = 64.542%, Loss = 0.29704962968826293
Epoch: 7901, Batch Gradient Norm: 14.911480314992291
Epoch: 7901, Batch Gradient Norm after: 14.911480314992291
Epoch 7902/10000, Prediction Accuracy = 64.576%, Loss = 0.2979596138000488
Epoch: 7902, Batch Gradient Norm: 14.121955753063922
Epoch: 7902, Batch Gradient Norm after: 14.121955753063922
Epoch 7903/10000, Prediction Accuracy = 64.678%, Loss = 0.2939351797103882
Epoch: 7903, Batch Gradient Norm: 14.857439058252393
Epoch: 7903, Batch Gradient Norm after: 14.857439058252393
Epoch 7904/10000, Prediction Accuracy = 64.63199999999999%, Loss = 0.2974473536014557
Epoch: 7904, Batch Gradient Norm: 15.515042829659745
Epoch: 7904, Batch Gradient Norm after: 15.515042829659745
Epoch 7905/10000, Prediction Accuracy = 64.71%, Loss = 0.29864932894706725
Epoch: 7905, Batch Gradient Norm: 18.091193202092587
Epoch: 7905, Batch Gradient Norm after: 18.091193202092587
Epoch 7906/10000, Prediction Accuracy = 64.78599999999999%, Loss = 0.30106462836265563
Epoch: 7906, Batch Gradient Norm: 19.27034905275777
Epoch: 7906, Batch Gradient Norm after: 18.632620309264272
Epoch 7907/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.3017119407653809
Epoch: 7907, Batch Gradient Norm: 18.892207189411867
Epoch: 7907, Batch Gradient Norm after: 18.892207189411867
Epoch 7908/10000, Prediction Accuracy = 64.59400000000001%, Loss = 0.30156750679016114
Epoch: 7908, Batch Gradient Norm: 15.727374141562542
Epoch: 7908, Batch Gradient Norm after: 15.727374141562542
Epoch 7909/10000, Prediction Accuracy = 64.71%, Loss = 0.2963461399078369
Epoch: 7909, Batch Gradient Norm: 14.569454165984132
Epoch: 7909, Batch Gradient Norm after: 14.569454165984132
Epoch 7910/10000, Prediction Accuracy = 64.75%, Loss = 0.2950035810470581
Epoch: 7910, Batch Gradient Norm: 16.3358111799002
Epoch: 7910, Batch Gradient Norm after: 16.3358111799002
Epoch 7911/10000, Prediction Accuracy = 64.646%, Loss = 0.29569846391677856
Epoch: 7911, Batch Gradient Norm: 14.819492685046573
Epoch: 7911, Batch Gradient Norm after: 14.819492685046573
Epoch 7912/10000, Prediction Accuracy = 64.63999999999999%, Loss = 0.29544134736061095
Epoch: 7912, Batch Gradient Norm: 16.146364531852992
Epoch: 7912, Batch Gradient Norm after: 16.146364531852992
Epoch 7913/10000, Prediction Accuracy = 64.654%, Loss = 0.29485095739364625
Epoch: 7913, Batch Gradient Norm: 16.003147316591008
Epoch: 7913, Batch Gradient Norm after: 16.003147316591008
Epoch 7914/10000, Prediction Accuracy = 64.78400000000002%, Loss = 0.295885968208313
Epoch: 7914, Batch Gradient Norm: 16.064055906173962
Epoch: 7914, Batch Gradient Norm after: 16.064055906173962
Epoch 7915/10000, Prediction Accuracy = 64.744%, Loss = 0.2979830622673035
Epoch: 7915, Batch Gradient Norm: 15.791028255226225
Epoch: 7915, Batch Gradient Norm after: 15.791028255226225
Epoch 7916/10000, Prediction Accuracy = 64.782%, Loss = 0.2968106806278229
Epoch: 7916, Batch Gradient Norm: 14.92095943732959
Epoch: 7916, Batch Gradient Norm after: 14.92095943732959
Epoch 7917/10000, Prediction Accuracy = 64.81%, Loss = 0.2969465434551239
Epoch: 7917, Batch Gradient Norm: 14.528128153324799
Epoch: 7917, Batch Gradient Norm after: 14.528128153324799
Epoch 7918/10000, Prediction Accuracy = 64.824%, Loss = 0.29462929368019103
Epoch: 7918, Batch Gradient Norm: 14.139772884133654
Epoch: 7918, Batch Gradient Norm after: 14.139772884133654
Epoch 7919/10000, Prediction Accuracy = 64.63%, Loss = 0.2951505720615387
Epoch: 7919, Batch Gradient Norm: 14.561511604329976
Epoch: 7919, Batch Gradient Norm after: 14.561511604329976
Epoch 7920/10000, Prediction Accuracy = 64.75800000000001%, Loss = 0.29567630887031554
Epoch: 7920, Batch Gradient Norm: 11.429300809204957
Epoch: 7920, Batch Gradient Norm after: 11.429300809204957
Epoch 7921/10000, Prediction Accuracy = 64.77799999999999%, Loss = 0.29296478629112244
Epoch: 7921, Batch Gradient Norm: 12.445184817675392
Epoch: 7921, Batch Gradient Norm after: 12.445184817675392
Epoch 7922/10000, Prediction Accuracy = 64.55199999999999%, Loss = 0.2940911531448364
Epoch: 7922, Batch Gradient Norm: 14.923300839637532
Epoch: 7922, Batch Gradient Norm after: 14.923300839637532
Epoch 7923/10000, Prediction Accuracy = 64.602%, Loss = 0.2968934953212738
Epoch: 7923, Batch Gradient Norm: 15.163858569255982
Epoch: 7923, Batch Gradient Norm after: 15.163858569255982
Epoch 7924/10000, Prediction Accuracy = 64.828%, Loss = 0.29717360734939574
Epoch: 7924, Batch Gradient Norm: 13.33239785572206
Epoch: 7924, Batch Gradient Norm after: 13.33239785572206
Epoch 7925/10000, Prediction Accuracy = 64.77%, Loss = 0.2913221180438995
Epoch: 7925, Batch Gradient Norm: 11.979277659892697
Epoch: 7925, Batch Gradient Norm after: 11.979277659892697
Epoch 7926/10000, Prediction Accuracy = 64.736%, Loss = 0.29180939197540284
Epoch: 7926, Batch Gradient Norm: 12.276836658821702
Epoch: 7926, Batch Gradient Norm after: 12.276836658821702
Epoch 7927/10000, Prediction Accuracy = 64.496%, Loss = 0.29383044242858886
Epoch: 7927, Batch Gradient Norm: 13.301964688337424
Epoch: 7927, Batch Gradient Norm after: 13.301964688337424
Epoch 7928/10000, Prediction Accuracy = 64.614%, Loss = 0.296215283870697
Epoch: 7928, Batch Gradient Norm: 11.632009234730951
Epoch: 7928, Batch Gradient Norm after: 11.632009234730951
Epoch 7929/10000, Prediction Accuracy = 64.6%, Loss = 0.2915916919708252
Epoch: 7929, Batch Gradient Norm: 14.246717873196653
Epoch: 7929, Batch Gradient Norm after: 14.246717873196653
Epoch 7930/10000, Prediction Accuracy = 64.672%, Loss = 0.29605849981307986
Epoch: 7930, Batch Gradient Norm: 13.459618264553463
Epoch: 7930, Batch Gradient Norm after: 13.459618264553463
Epoch 7931/10000, Prediction Accuracy = 64.682%, Loss = 0.29319688081741335
Epoch: 7931, Batch Gradient Norm: 13.900332631778467
Epoch: 7931, Batch Gradient Norm after: 13.900332631778467
Epoch 7932/10000, Prediction Accuracy = 64.71199999999999%, Loss = 0.2948533475399017
Epoch: 7932, Batch Gradient Norm: 16.58349511215383
Epoch: 7932, Batch Gradient Norm after: 16.58349511215383
Epoch 7933/10000, Prediction Accuracy = 64.728%, Loss = 0.2967729032039642
Epoch: 7933, Batch Gradient Norm: 16.016280776165708
Epoch: 7933, Batch Gradient Norm after: 16.016280776165708
Epoch 7934/10000, Prediction Accuracy = 64.654%, Loss = 0.29475300908088686
Epoch: 7934, Batch Gradient Norm: 15.763307377912586
Epoch: 7934, Batch Gradient Norm after: 15.763307377912586
Epoch 7935/10000, Prediction Accuracy = 64.74600000000001%, Loss = 0.29961119294166566
Epoch: 7935, Batch Gradient Norm: 14.798780812765049
Epoch: 7935, Batch Gradient Norm after: 14.798780812765049
Epoch 7936/10000, Prediction Accuracy = 64.814%, Loss = 0.2954255521297455
Epoch: 7936, Batch Gradient Norm: 13.891783830417408
Epoch: 7936, Batch Gradient Norm after: 13.891783830417408
Epoch 7937/10000, Prediction Accuracy = 64.69200000000001%, Loss = 0.29575851559638977
Epoch: 7937, Batch Gradient Norm: 13.074376992645485
Epoch: 7937, Batch Gradient Norm after: 13.074376992645485
Epoch 7938/10000, Prediction Accuracy = 64.638%, Loss = 0.2941261112689972
Epoch: 7938, Batch Gradient Norm: 11.294545131262508
Epoch: 7938, Batch Gradient Norm after: 11.294545131262508
Epoch 7939/10000, Prediction Accuracy = 64.772%, Loss = 0.29078595638275145
Epoch: 7939, Batch Gradient Norm: 13.83665472907161
Epoch: 7939, Batch Gradient Norm after: 13.83665472907161
Epoch 7940/10000, Prediction Accuracy = 64.728%, Loss = 0.29295640587806704
Epoch: 7940, Batch Gradient Norm: 17.305461423804665
Epoch: 7940, Batch Gradient Norm after: 17.305461423804665
Epoch 7941/10000, Prediction Accuracy = 64.54999999999998%, Loss = 0.29795350432395934
Epoch: 7941, Batch Gradient Norm: 14.393629861200315
Epoch: 7941, Batch Gradient Norm after: 14.393629861200315
Epoch 7942/10000, Prediction Accuracy = 64.70000000000002%, Loss = 0.2954272091388702
Epoch: 7942, Batch Gradient Norm: 11.286168594355352
Epoch: 7942, Batch Gradient Norm after: 11.286168594355352
Epoch 7943/10000, Prediction Accuracy = 64.652%, Loss = 0.2929274022579193
Epoch: 7943, Batch Gradient Norm: 13.661137053988051
Epoch: 7943, Batch Gradient Norm after: 13.661137053988051
Epoch 7944/10000, Prediction Accuracy = 64.62%, Loss = 0.29335758090019226
Epoch: 7944, Batch Gradient Norm: 16.196953249903956
Epoch: 7944, Batch Gradient Norm after: 16.17834614799932
Epoch 7945/10000, Prediction Accuracy = 64.768%, Loss = 0.29644123911857606
Epoch: 7945, Batch Gradient Norm: 17.320770448561674
Epoch: 7945, Batch Gradient Norm after: 17.320770448561674
Epoch 7946/10000, Prediction Accuracy = 64.876%, Loss = 0.2977238357067108
Epoch: 7946, Batch Gradient Norm: 15.441603747198535
Epoch: 7946, Batch Gradient Norm after: 15.441603747198535
Epoch 7947/10000, Prediction Accuracy = 64.69800000000001%, Loss = 0.3003982365131378
Epoch: 7947, Batch Gradient Norm: 17.32015046727366
Epoch: 7947, Batch Gradient Norm after: 17.32015046727366
Epoch 7948/10000, Prediction Accuracy = 64.738%, Loss = 0.2974343955516815
Epoch: 7948, Batch Gradient Norm: 15.52257815420112
Epoch: 7948, Batch Gradient Norm after: 15.52257815420112
Epoch 7949/10000, Prediction Accuracy = 64.838%, Loss = 0.2950085520744324
Epoch: 7949, Batch Gradient Norm: 16.991044838591208
Epoch: 7949, Batch Gradient Norm after: 16.991044838591208
Epoch 7950/10000, Prediction Accuracy = 64.636%, Loss = 0.29987160563468934
Epoch: 7950, Batch Gradient Norm: 14.752274358131597
Epoch: 7950, Batch Gradient Norm after: 14.752274358131597
Epoch 7951/10000, Prediction Accuracy = 64.734%, Loss = 0.29479292035102844
Epoch: 7951, Batch Gradient Norm: 12.132478047553537
Epoch: 7951, Batch Gradient Norm after: 12.132478047553537
Epoch 7952/10000, Prediction Accuracy = 64.69000000000001%, Loss = 0.2920386791229248
Epoch: 7952, Batch Gradient Norm: 14.45778684978984
Epoch: 7952, Batch Gradient Norm after: 14.45778684978984
Epoch 7953/10000, Prediction Accuracy = 64.646%, Loss = 0.29387022852897643
Epoch: 7953, Batch Gradient Norm: 16.301516936888323
Epoch: 7953, Batch Gradient Norm after: 16.301516936888323
Epoch 7954/10000, Prediction Accuracy = 64.752%, Loss = 0.29627203941345215
Epoch: 7954, Batch Gradient Norm: 15.939661386375523
Epoch: 7954, Batch Gradient Norm after: 15.939661386375523
Epoch 7955/10000, Prediction Accuracy = 64.752%, Loss = 0.29761711359024046
Epoch: 7955, Batch Gradient Norm: 15.782153436675895
Epoch: 7955, Batch Gradient Norm after: 15.782153436675895
Epoch 7956/10000, Prediction Accuracy = 64.55%, Loss = 0.29868828058242797
Epoch: 7956, Batch Gradient Norm: 15.457080055234302
Epoch: 7956, Batch Gradient Norm after: 15.457080055234302
Epoch 7957/10000, Prediction Accuracy = 64.742%, Loss = 0.29706956148147584
Epoch: 7957, Batch Gradient Norm: 14.123613833835478
Epoch: 7957, Batch Gradient Norm after: 14.123613833835478
Epoch 7958/10000, Prediction Accuracy = 64.684%, Loss = 0.2934567928314209
Epoch: 7958, Batch Gradient Norm: 15.519962334070964
Epoch: 7958, Batch Gradient Norm after: 15.519962334070964
Epoch 7959/10000, Prediction Accuracy = 64.59400000000001%, Loss = 0.2973230302333832
Epoch: 7959, Batch Gradient Norm: 15.379538320627702
Epoch: 7959, Batch Gradient Norm after: 15.379538320627702
Epoch 7960/10000, Prediction Accuracy = 64.69%, Loss = 0.2954230487346649
Epoch: 7960, Batch Gradient Norm: 16.88648609296707
Epoch: 7960, Batch Gradient Norm after: 16.88648609296707
Epoch 7961/10000, Prediction Accuracy = 64.74000000000001%, Loss = 0.2968234956264496
Epoch: 7961, Batch Gradient Norm: 16.643962582990806
Epoch: 7961, Batch Gradient Norm after: 16.643962582990806
Epoch 7962/10000, Prediction Accuracy = 64.678%, Loss = 0.29832082986831665
Epoch: 7962, Batch Gradient Norm: 17.086458704882865
Epoch: 7962, Batch Gradient Norm after: 17.086458704882865
Epoch 7963/10000, Prediction Accuracy = 64.676%, Loss = 0.29719704389572144
Epoch: 7963, Batch Gradient Norm: 12.818313041237328
Epoch: 7963, Batch Gradient Norm after: 12.818313041237328
Epoch 7964/10000, Prediction Accuracy = 64.638%, Loss = 0.2924493491649628
Epoch: 7964, Batch Gradient Norm: 16.352315858841482
Epoch: 7964, Batch Gradient Norm after: 16.352315858841482
Epoch 7965/10000, Prediction Accuracy = 64.564%, Loss = 0.2997438132762909
Epoch: 7965, Batch Gradient Norm: 16.086668954678405
Epoch: 7965, Batch Gradient Norm after: 16.086668954678405
Epoch 7966/10000, Prediction Accuracy = 64.81800000000001%, Loss = 0.29662013053894043
Epoch: 7966, Batch Gradient Norm: 15.270721830174658
Epoch: 7966, Batch Gradient Norm after: 15.270721830174658
Epoch 7967/10000, Prediction Accuracy = 64.654%, Loss = 0.2965308427810669
Epoch: 7967, Batch Gradient Norm: 15.879596083458505
Epoch: 7967, Batch Gradient Norm after: 15.879596083458505
Epoch 7968/10000, Prediction Accuracy = 64.66600000000001%, Loss = 0.2950869143009186
Epoch: 7968, Batch Gradient Norm: 15.647460082702253
Epoch: 7968, Batch Gradient Norm after: 15.647460082702253
Epoch 7969/10000, Prediction Accuracy = 64.78200000000001%, Loss = 0.29597367644309996
Epoch: 7969, Batch Gradient Norm: 12.867367877755647
Epoch: 7969, Batch Gradient Norm after: 12.867367877755647
Epoch 7970/10000, Prediction Accuracy = 64.718%, Loss = 0.29200454950332644
Epoch: 7970, Batch Gradient Norm: 12.230760700229327
Epoch: 7970, Batch Gradient Norm after: 12.230760700229327
Epoch 7971/10000, Prediction Accuracy = 64.662%, Loss = 0.29051194190979
Epoch: 7971, Batch Gradient Norm: 15.82782312416513
Epoch: 7971, Batch Gradient Norm after: 15.82782312416513
Epoch 7972/10000, Prediction Accuracy = 64.654%, Loss = 0.2959462344646454
Epoch: 7972, Batch Gradient Norm: 16.21469626384127
Epoch: 7972, Batch Gradient Norm after: 16.21469626384127
Epoch 7973/10000, Prediction Accuracy = 64.63399999999999%, Loss = 0.29902464151382446
Epoch: 7973, Batch Gradient Norm: 17.740129705466472
Epoch: 7973, Batch Gradient Norm after: 17.740129705466472
Epoch 7974/10000, Prediction Accuracy = 64.642%, Loss = 0.3002042531967163
Epoch: 7974, Batch Gradient Norm: 13.862864845016013
Epoch: 7974, Batch Gradient Norm after: 13.862864845016013
Epoch 7975/10000, Prediction Accuracy = 64.726%, Loss = 0.2954309284687042
Epoch: 7975, Batch Gradient Norm: 14.13338264802602
Epoch: 7975, Batch Gradient Norm after: 14.13338264802602
Epoch 7976/10000, Prediction Accuracy = 64.7%, Loss = 0.2969751119613647
Epoch: 7976, Batch Gradient Norm: 13.95528073257694
Epoch: 7976, Batch Gradient Norm after: 13.95528073257694
Epoch 7977/10000, Prediction Accuracy = 64.672%, Loss = 0.2954841375350952
Epoch: 7977, Batch Gradient Norm: 14.735803263098495
Epoch: 7977, Batch Gradient Norm after: 14.735803263098495
Epoch 7978/10000, Prediction Accuracy = 64.61600000000001%, Loss = 0.2960015654563904
Epoch: 7978, Batch Gradient Norm: 14.209359827846063
Epoch: 7978, Batch Gradient Norm after: 14.209359827846063
Epoch 7979/10000, Prediction Accuracy = 64.692%, Loss = 0.2959055960178375
Epoch: 7979, Batch Gradient Norm: 14.707893002653554
Epoch: 7979, Batch Gradient Norm after: 14.707893002653554
Epoch 7980/10000, Prediction Accuracy = 64.648%, Loss = 0.2956349551677704
Epoch: 7980, Batch Gradient Norm: 14.892802345959565
Epoch: 7980, Batch Gradient Norm after: 14.892802345959565
Epoch 7981/10000, Prediction Accuracy = 64.786%, Loss = 0.29583982229232786
Epoch: 7981, Batch Gradient Norm: 13.469446588102818
Epoch: 7981, Batch Gradient Norm after: 13.469446588102818
Epoch 7982/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.2964454233646393
Epoch: 7982, Batch Gradient Norm: 12.40783649944944
Epoch: 7982, Batch Gradient Norm after: 12.40783649944944
Epoch 7983/10000, Prediction Accuracy = 64.72%, Loss = 0.29571889638900756
Epoch: 7983, Batch Gradient Norm: 12.32740848134921
Epoch: 7983, Batch Gradient Norm after: 12.32740848134921
Epoch 7984/10000, Prediction Accuracy = 64.74799999999999%, Loss = 0.29132862091064454
Epoch: 7984, Batch Gradient Norm: 12.567902371653808
Epoch: 7984, Batch Gradient Norm after: 12.567902371653808
Epoch 7985/10000, Prediction Accuracy = 64.66999999999999%, Loss = 0.2957196176052094
Epoch: 7985, Batch Gradient Norm: 13.496757583301866
Epoch: 7985, Batch Gradient Norm after: 13.496757583301866
Epoch 7986/10000, Prediction Accuracy = 64.798%, Loss = 0.2937339961528778
Epoch: 7986, Batch Gradient Norm: 16.636524108922917
Epoch: 7986, Batch Gradient Norm after: 16.636524108922917
Epoch 7987/10000, Prediction Accuracy = 64.66999999999999%, Loss = 0.2992001295089722
Epoch: 7987, Batch Gradient Norm: 15.87909947071525
Epoch: 7987, Batch Gradient Norm after: 15.87909947071525
Epoch 7988/10000, Prediction Accuracy = 64.552%, Loss = 0.2981336772441864
Epoch: 7988, Batch Gradient Norm: 17.999778116244464
Epoch: 7988, Batch Gradient Norm after: 17.999778116244464
Epoch 7989/10000, Prediction Accuracy = 64.664%, Loss = 0.29822723269462587
Epoch: 7989, Batch Gradient Norm: 19.103628807296907
Epoch: 7989, Batch Gradient Norm after: 19.103628807296907
Epoch 7990/10000, Prediction Accuracy = 64.63600000000001%, Loss = 0.3015532732009888
Epoch: 7990, Batch Gradient Norm: 16.914203857564324
Epoch: 7990, Batch Gradient Norm after: 16.914203857564324
Epoch 7991/10000, Prediction Accuracy = 64.798%, Loss = 0.29614222049713135
Epoch: 7991, Batch Gradient Norm: 16.578189042484937
Epoch: 7991, Batch Gradient Norm after: 16.578189042484937
Epoch 7992/10000, Prediction Accuracy = 64.586%, Loss = 0.2967213988304138
Epoch: 7992, Batch Gradient Norm: 14.564600546410743
Epoch: 7992, Batch Gradient Norm after: 14.564600546410743
Epoch 7993/10000, Prediction Accuracy = 64.716%, Loss = 0.29452266693115237
Epoch: 7993, Batch Gradient Norm: 15.095013345200455
Epoch: 7993, Batch Gradient Norm after: 15.095013345200455
Epoch 7994/10000, Prediction Accuracy = 64.676%, Loss = 0.293549519777298
Epoch: 7994, Batch Gradient Norm: 15.760246520814444
Epoch: 7994, Batch Gradient Norm after: 15.760246520814444
Epoch 7995/10000, Prediction Accuracy = 64.826%, Loss = 0.2933491289615631
Epoch: 7995, Batch Gradient Norm: 14.604111281010839
Epoch: 7995, Batch Gradient Norm after: 14.604111281010839
Epoch 7996/10000, Prediction Accuracy = 64.78399999999999%, Loss = 0.2936142861843109
Epoch: 7996, Batch Gradient Norm: 16.276453087574726
Epoch: 7996, Batch Gradient Norm after: 16.276453087574726
Epoch 7997/10000, Prediction Accuracy = 64.648%, Loss = 0.29706707000732424
Epoch: 7997, Batch Gradient Norm: 12.993099922937532
Epoch: 7997, Batch Gradient Norm after: 12.993099922937532
Epoch 7998/10000, Prediction Accuracy = 64.664%, Loss = 0.2912799954414368
Epoch: 7998, Batch Gradient Norm: 14.263762486799115
Epoch: 7998, Batch Gradient Norm after: 14.263762486799115
Epoch 7999/10000, Prediction Accuracy = 64.696%, Loss = 0.29325618147850036
Epoch: 7999, Batch Gradient Norm: 15.975770195191012
Epoch: 7999, Batch Gradient Norm after: 15.975770195191012
Epoch 8000/10000, Prediction Accuracy = 64.63399999999999%, Loss = 0.2961051344871521
Epoch: 8000, Batch Gradient Norm: 16.112729623919613
Epoch: 8000, Batch Gradient Norm after: 16.112729623919613
Epoch 8001/10000, Prediction Accuracy = 64.75200000000001%, Loss = 0.29530115723609923
Epoch: 8001, Batch Gradient Norm: 18.11996113145134
Epoch: 8001, Batch Gradient Norm after: 17.96898250171935
Epoch 8002/10000, Prediction Accuracy = 64.908%, Loss = 0.29798869490623475
Epoch: 8002, Batch Gradient Norm: 18.3398049638188
Epoch: 8002, Batch Gradient Norm after: 18.011507669932424
Epoch 8003/10000, Prediction Accuracy = 64.636%, Loss = 0.2994195282459259
Epoch: 8003, Batch Gradient Norm: 19.313947262908616
Epoch: 8003, Batch Gradient Norm after: 19.313947262908616
Epoch 8004/10000, Prediction Accuracy = 64.684%, Loss = 0.3003075659275055
Epoch: 8004, Batch Gradient Norm: 17.359566617396183
Epoch: 8004, Batch Gradient Norm after: 17.359566617396183
Epoch 8005/10000, Prediction Accuracy = 64.816%, Loss = 0.2977167546749115
Epoch: 8005, Batch Gradient Norm: 15.944595024042654
Epoch: 8005, Batch Gradient Norm after: 15.944595024042654
Epoch 8006/10000, Prediction Accuracy = 64.72%, Loss = 0.3013671815395355
Epoch: 8006, Batch Gradient Norm: 18.634459323457992
Epoch: 8006, Batch Gradient Norm after: 18.58096135954394
Epoch 8007/10000, Prediction Accuracy = 64.80000000000001%, Loss = 0.29956272840499876
Epoch: 8007, Batch Gradient Norm: 15.186710407059763
Epoch: 8007, Batch Gradient Norm after: 15.186710407059763
Epoch 8008/10000, Prediction Accuracy = 64.684%, Loss = 0.2963786482810974
Epoch: 8008, Batch Gradient Norm: 13.006080032494221
Epoch: 8008, Batch Gradient Norm after: 13.006080032494221
Epoch 8009/10000, Prediction Accuracy = 64.84200000000001%, Loss = 0.2923725426197052
Epoch: 8009, Batch Gradient Norm: 14.173577076637784
Epoch: 8009, Batch Gradient Norm after: 14.173577076637784
Epoch 8010/10000, Prediction Accuracy = 64.65799999999999%, Loss = 0.2953768253326416
Epoch: 8010, Batch Gradient Norm: 17.628145410221947
Epoch: 8010, Batch Gradient Norm after: 17.432079804701
Epoch 8011/10000, Prediction Accuracy = 64.876%, Loss = 0.29738959670066833
Epoch: 8011, Batch Gradient Norm: 15.956403462095617
Epoch: 8011, Batch Gradient Norm after: 15.956403462095617
Epoch 8012/10000, Prediction Accuracy = 64.688%, Loss = 0.29548817276954653
Epoch: 8012, Batch Gradient Norm: 15.41621365880987
Epoch: 8012, Batch Gradient Norm after: 15.41621365880987
Epoch 8013/10000, Prediction Accuracy = 64.76599999999999%, Loss = 0.2955273449420929
Epoch: 8013, Batch Gradient Norm: 18.71895182682349
Epoch: 8013, Batch Gradient Norm after: 18.71895182682349
Epoch 8014/10000, Prediction Accuracy = 64.71%, Loss = 0.29873759150505064
Epoch: 8014, Batch Gradient Norm: 18.674846954196
Epoch: 8014, Batch Gradient Norm after: 18.674846954196
Epoch 8015/10000, Prediction Accuracy = 64.652%, Loss = 0.30055695176124575
Epoch: 8015, Batch Gradient Norm: 14.24742975885485
Epoch: 8015, Batch Gradient Norm after: 14.24742975885485
Epoch 8016/10000, Prediction Accuracy = 64.75999999999999%, Loss = 0.29247761964797975
Epoch: 8016, Batch Gradient Norm: 15.130968048758927
Epoch: 8016, Batch Gradient Norm after: 15.130968048758927
Epoch 8017/10000, Prediction Accuracy = 64.732%, Loss = 0.2939114153385162
Epoch: 8017, Batch Gradient Norm: 16.684585635740934
Epoch: 8017, Batch Gradient Norm after: 16.684585635740934
Epoch 8018/10000, Prediction Accuracy = 64.702%, Loss = 0.2959496319293976
Epoch: 8018, Batch Gradient Norm: 15.903801835228428
Epoch: 8018, Batch Gradient Norm after: 15.903801835228428
Epoch 8019/10000, Prediction Accuracy = 64.644%, Loss = 0.29587363004684447
Epoch: 8019, Batch Gradient Norm: 14.561587490747707
Epoch: 8019, Batch Gradient Norm after: 14.561587490747707
Epoch 8020/10000, Prediction Accuracy = 64.724%, Loss = 0.2933432996273041
Epoch: 8020, Batch Gradient Norm: 15.850449729650403
Epoch: 8020, Batch Gradient Norm after: 15.850449729650403
Epoch 8021/10000, Prediction Accuracy = 64.606%, Loss = 0.29877259731292727
Epoch: 8021, Batch Gradient Norm: 15.510321097640025
Epoch: 8021, Batch Gradient Norm after: 15.510321097640025
Epoch 8022/10000, Prediction Accuracy = 64.646%, Loss = 0.29462496638298036
Epoch: 8022, Batch Gradient Norm: 15.27785723285747
Epoch: 8022, Batch Gradient Norm after: 15.27785723285747
Epoch 8023/10000, Prediction Accuracy = 64.91%, Loss = 0.2922452211380005
Epoch: 8023, Batch Gradient Norm: 17.229536149627343
Epoch: 8023, Batch Gradient Norm after: 16.93069387206584
Epoch 8024/10000, Prediction Accuracy = 64.732%, Loss = 0.2958108365535736
Epoch: 8024, Batch Gradient Norm: 19.426169024251852
Epoch: 8024, Batch Gradient Norm after: 19.061898874760672
Epoch 8025/10000, Prediction Accuracy = 64.664%, Loss = 0.2988810956478119
Epoch: 8025, Batch Gradient Norm: 16.049451804050314
Epoch: 8025, Batch Gradient Norm after: 16.049451804050314
Epoch 8026/10000, Prediction Accuracy = 64.72999999999999%, Loss = 0.2958123445510864
Epoch: 8026, Batch Gradient Norm: 17.677857942642145
Epoch: 8026, Batch Gradient Norm after: 17.677857942642145
Epoch 8027/10000, Prediction Accuracy = 64.75800000000001%, Loss = 0.2969407081604004
Epoch: 8027, Batch Gradient Norm: 16.673387457019306
Epoch: 8027, Batch Gradient Norm after: 16.673387457019306
Epoch 8028/10000, Prediction Accuracy = 64.878%, Loss = 0.29537783861160277
Epoch: 8028, Batch Gradient Norm: 14.72290480255553
Epoch: 8028, Batch Gradient Norm after: 14.72290480255553
Epoch 8029/10000, Prediction Accuracy = 64.7%, Loss = 0.29373939633369445
Epoch: 8029, Batch Gradient Norm: 15.505749537400716
Epoch: 8029, Batch Gradient Norm after: 15.505749537400716
Epoch 8030/10000, Prediction Accuracy = 64.678%, Loss = 0.29361655116081237
Epoch: 8030, Batch Gradient Norm: 14.831228550787735
Epoch: 8030, Batch Gradient Norm after: 14.831228550787735
Epoch 8031/10000, Prediction Accuracy = 64.74%, Loss = 0.29234030842781067
Epoch: 8031, Batch Gradient Norm: 14.861005150674451
Epoch: 8031, Batch Gradient Norm after: 14.861005150674451
Epoch 8032/10000, Prediction Accuracy = 64.76%, Loss = 0.2927632749080658
Epoch: 8032, Batch Gradient Norm: 17.436954548410323
Epoch: 8032, Batch Gradient Norm after: 17.344439397352904
Epoch 8033/10000, Prediction Accuracy = 64.60999999999999%, Loss = 0.29903845191001893
Epoch: 8033, Batch Gradient Norm: 15.67601481525064
Epoch: 8033, Batch Gradient Norm after: 15.67601481525064
Epoch 8034/10000, Prediction Accuracy = 64.85400000000001%, Loss = 0.29343377947807314
Epoch: 8034, Batch Gradient Norm: 14.282782703804193
Epoch: 8034, Batch Gradient Norm after: 14.282782703804193
Epoch 8035/10000, Prediction Accuracy = 64.70000000000002%, Loss = 0.2924670338630676
Epoch: 8035, Batch Gradient Norm: 12.214417297297855
Epoch: 8035, Batch Gradient Norm after: 12.214417297297855
Epoch 8036/10000, Prediction Accuracy = 64.824%, Loss = 0.2912369132041931
Epoch: 8036, Batch Gradient Norm: 15.702052555227965
Epoch: 8036, Batch Gradient Norm after: 15.702052555227965
Epoch 8037/10000, Prediction Accuracy = 64.734%, Loss = 0.29573906064033506
Epoch: 8037, Batch Gradient Norm: 16.177692406309806
Epoch: 8037, Batch Gradient Norm after: 16.177692406309806
Epoch 8038/10000, Prediction Accuracy = 64.782%, Loss = 0.2944421887397766
Epoch: 8038, Batch Gradient Norm: 14.23569871057219
Epoch: 8038, Batch Gradient Norm after: 14.23569871057219
Epoch 8039/10000, Prediction Accuracy = 64.76199999999999%, Loss = 0.292932391166687
Epoch: 8039, Batch Gradient Norm: 14.670625802215264
Epoch: 8039, Batch Gradient Norm after: 14.670625802215264
Epoch 8040/10000, Prediction Accuracy = 64.686%, Loss = 0.2925056040287018
Epoch: 8040, Batch Gradient Norm: 14.226075132144228
Epoch: 8040, Batch Gradient Norm after: 14.226075132144228
Epoch 8041/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.2922326385974884
Epoch: 8041, Batch Gradient Norm: 14.075806627483518
Epoch: 8041, Batch Gradient Norm after: 14.075806627483518
Epoch 8042/10000, Prediction Accuracy = 64.752%, Loss = 0.29357311725616453
Epoch: 8042, Batch Gradient Norm: 14.818958479622905
Epoch: 8042, Batch Gradient Norm after: 14.818958479622905
Epoch 8043/10000, Prediction Accuracy = 64.62400000000001%, Loss = 0.29207040667533873
Epoch: 8043, Batch Gradient Norm: 12.536539164794299
Epoch: 8043, Batch Gradient Norm after: 12.536539164794299
Epoch 8044/10000, Prediction Accuracy = 64.892%, Loss = 0.2895816743373871
Epoch: 8044, Batch Gradient Norm: 12.269683482813454
Epoch: 8044, Batch Gradient Norm after: 12.269683482813454
Epoch 8045/10000, Prediction Accuracy = 64.838%, Loss = 0.2907079756259918
Epoch: 8045, Batch Gradient Norm: 14.0224525312502
Epoch: 8045, Batch Gradient Norm after: 14.0224525312502
Epoch 8046/10000, Prediction Accuracy = 64.946%, Loss = 0.29170753359794616
Epoch: 8046, Batch Gradient Norm: 17.063282053916854
Epoch: 8046, Batch Gradient Norm after: 17.063282053916854
Epoch 8047/10000, Prediction Accuracy = 64.68199999999999%, Loss = 0.2972315728664398
Epoch: 8047, Batch Gradient Norm: 15.999235790719446
Epoch: 8047, Batch Gradient Norm after: 15.72843293386737
Epoch 8048/10000, Prediction Accuracy = 64.78%, Loss = 0.29417191743850707
Epoch: 8048, Batch Gradient Norm: 14.907176564393538
Epoch: 8048, Batch Gradient Norm after: 14.907176564393538
Epoch 8049/10000, Prediction Accuracy = 64.81400000000001%, Loss = 0.2936438798904419
Epoch: 8049, Batch Gradient Norm: 11.736821109620601
Epoch: 8049, Batch Gradient Norm after: 11.736821109620601
Epoch 8050/10000, Prediction Accuracy = 64.68599999999999%, Loss = 0.29013381004333494
Epoch: 8050, Batch Gradient Norm: 12.823631456667997
Epoch: 8050, Batch Gradient Norm after: 12.823631456667997
Epoch 8051/10000, Prediction Accuracy = 64.8%, Loss = 0.2900760293006897
Epoch: 8051, Batch Gradient Norm: 14.078805845646526
Epoch: 8051, Batch Gradient Norm after: 14.078805845646526
Epoch 8052/10000, Prediction Accuracy = 64.74600000000001%, Loss = 0.2920797228813171
Epoch: 8052, Batch Gradient Norm: 13.010826393771033
Epoch: 8052, Batch Gradient Norm after: 13.010826393771033
Epoch 8053/10000, Prediction Accuracy = 64.84799999999998%, Loss = 0.29016374349594115
Epoch: 8053, Batch Gradient Norm: 16.129714748983982
Epoch: 8053, Batch Gradient Norm after: 16.129714748983982
Epoch 8054/10000, Prediction Accuracy = 64.61800000000001%, Loss = 0.2989963710308075
Epoch: 8054, Batch Gradient Norm: 16.376022197733512
Epoch: 8054, Batch Gradient Norm after: 16.376022197733512
Epoch 8055/10000, Prediction Accuracy = 64.72999999999999%, Loss = 0.2948599517345428
Epoch: 8055, Batch Gradient Norm: 16.244188680639926
Epoch: 8055, Batch Gradient Norm after: 16.244188680639926
Epoch 8056/10000, Prediction Accuracy = 64.634%, Loss = 0.2969824492931366
Epoch: 8056, Batch Gradient Norm: 13.442197551604476
Epoch: 8056, Batch Gradient Norm after: 13.442197551604476
Epoch 8057/10000, Prediction Accuracy = 64.756%, Loss = 0.29297717213630675
Epoch: 8057, Batch Gradient Norm: 13.97701104971783
Epoch: 8057, Batch Gradient Norm after: 13.97701104971783
Epoch 8058/10000, Prediction Accuracy = 64.84799999999998%, Loss = 0.2907160758972168
Epoch: 8058, Batch Gradient Norm: 15.945556024043494
Epoch: 8058, Batch Gradient Norm after: 15.945556024043494
Epoch 8059/10000, Prediction Accuracy = 64.732%, Loss = 0.2945986270904541
Epoch: 8059, Batch Gradient Norm: 15.065102585909852
Epoch: 8059, Batch Gradient Norm after: 15.065102585909852
Epoch 8060/10000, Prediction Accuracy = 64.696%, Loss = 0.29394657015800474
Epoch: 8060, Batch Gradient Norm: 13.495325025710356
Epoch: 8060, Batch Gradient Norm after: 13.495325025710356
Epoch 8061/10000, Prediction Accuracy = 64.804%, Loss = 0.2926453948020935
Epoch: 8061, Batch Gradient Norm: 14.021306952964824
Epoch: 8061, Batch Gradient Norm after: 14.021306952964824
Epoch 8062/10000, Prediction Accuracy = 64.90599999999999%, Loss = 0.2920156896114349
Epoch: 8062, Batch Gradient Norm: 16.541355880374336
Epoch: 8062, Batch Gradient Norm after: 16.541355880374336
Epoch 8063/10000, Prediction Accuracy = 64.74000000000001%, Loss = 0.2970256507396698
Epoch: 8063, Batch Gradient Norm: 13.846096099696833
Epoch: 8063, Batch Gradient Norm after: 13.846096099696833
Epoch 8064/10000, Prediction Accuracy = 64.776%, Loss = 0.2911488234996796
Epoch: 8064, Batch Gradient Norm: 14.235476077986295
Epoch: 8064, Batch Gradient Norm after: 14.235476077986295
Epoch 8065/10000, Prediction Accuracy = 64.788%, Loss = 0.2939732551574707
Epoch: 8065, Batch Gradient Norm: 16.220357492442865
Epoch: 8065, Batch Gradient Norm after: 16.220357492442865
Epoch 8066/10000, Prediction Accuracy = 64.794%, Loss = 0.29379191994667053
Epoch: 8066, Batch Gradient Norm: 16.824205040593686
Epoch: 8066, Batch Gradient Norm after: 16.824205040593686
Epoch 8067/10000, Prediction Accuracy = 64.672%, Loss = 0.29336169362068176
Epoch: 8067, Batch Gradient Norm: 16.19636282913949
Epoch: 8067, Batch Gradient Norm after: 16.19636282913949
Epoch 8068/10000, Prediction Accuracy = 64.806%, Loss = 0.29463712573051454
Epoch: 8068, Batch Gradient Norm: 16.273953515647488
Epoch: 8068, Batch Gradient Norm after: 16.273953515647488
Epoch 8069/10000, Prediction Accuracy = 64.77399999999999%, Loss = 0.2945633172988892
Epoch: 8069, Batch Gradient Norm: 15.702891490652641
Epoch: 8069, Batch Gradient Norm after: 15.702891490652641
Epoch 8070/10000, Prediction Accuracy = 64.822%, Loss = 0.2938322424888611
Epoch: 8070, Batch Gradient Norm: 13.440432148164216
Epoch: 8070, Batch Gradient Norm after: 13.440432148164216
Epoch 8071/10000, Prediction Accuracy = 64.83000000000001%, Loss = 0.2907719075679779
Epoch: 8071, Batch Gradient Norm: 11.718123194431323
Epoch: 8071, Batch Gradient Norm after: 11.718123194431323
Epoch 8072/10000, Prediction Accuracy = 64.876%, Loss = 0.29078291058540345
Epoch: 8072, Batch Gradient Norm: 11.497099269735122
Epoch: 8072, Batch Gradient Norm after: 11.497099269735122
Epoch 8073/10000, Prediction Accuracy = 64.76%, Loss = 0.28731473684310915
Epoch: 8073, Batch Gradient Norm: 12.416076320837046
Epoch: 8073, Batch Gradient Norm after: 12.416076320837046
Epoch 8074/10000, Prediction Accuracy = 64.65%, Loss = 0.2932006478309631
Epoch: 8074, Batch Gradient Norm: 14.734415245788497
Epoch: 8074, Batch Gradient Norm after: 14.734415245788497
Epoch 8075/10000, Prediction Accuracy = 64.738%, Loss = 0.29309722781181335
Epoch: 8075, Batch Gradient Norm: 15.886894657381182
Epoch: 8075, Batch Gradient Norm after: 15.886894657381182
Epoch 8076/10000, Prediction Accuracy = 64.848%, Loss = 0.29528324007987977
Epoch: 8076, Batch Gradient Norm: 13.434848701602322
Epoch: 8076, Batch Gradient Norm after: 13.434848701602322
Epoch 8077/10000, Prediction Accuracy = 64.824%, Loss = 0.28982595205307005
Epoch: 8077, Batch Gradient Norm: 13.620231611759587
Epoch: 8077, Batch Gradient Norm after: 13.620231611759587
Epoch 8078/10000, Prediction Accuracy = 64.742%, Loss = 0.2917139232158661
Epoch: 8078, Batch Gradient Norm: 11.748274562895205
Epoch: 8078, Batch Gradient Norm after: 11.748274562895205
Epoch 8079/10000, Prediction Accuracy = 64.87%, Loss = 0.28994019627571105
Epoch: 8079, Batch Gradient Norm: 14.269611348330203
Epoch: 8079, Batch Gradient Norm after: 14.269611348330203
Epoch 8080/10000, Prediction Accuracy = 64.78200000000001%, Loss = 0.2913614630699158
Epoch: 8080, Batch Gradient Norm: 12.315226311491955
Epoch: 8080, Batch Gradient Norm after: 12.315226311491955
Epoch 8081/10000, Prediction Accuracy = 64.812%, Loss = 0.29281249046325686
Epoch: 8081, Batch Gradient Norm: 15.236101042176085
Epoch: 8081, Batch Gradient Norm after: 15.236101042176085
Epoch 8082/10000, Prediction Accuracy = 64.88%, Loss = 0.29473623633384705
Epoch: 8082, Batch Gradient Norm: 17.76402858486128
Epoch: 8082, Batch Gradient Norm after: 17.76402858486128
Epoch 8083/10000, Prediction Accuracy = 64.844%, Loss = 0.29523796439170835
Epoch: 8083, Batch Gradient Norm: 13.963403916161369
Epoch: 8083, Batch Gradient Norm after: 13.963403916161369
Epoch 8084/10000, Prediction Accuracy = 64.798%, Loss = 0.29268192052841185
Epoch: 8084, Batch Gradient Norm: 16.322552004545546
Epoch: 8084, Batch Gradient Norm after: 16.322552004545546
Epoch 8085/10000, Prediction Accuracy = 64.786%, Loss = 0.2956859290599823
Epoch: 8085, Batch Gradient Norm: 16.169586699614058
Epoch: 8085, Batch Gradient Norm after: 15.936263158002085
Epoch 8086/10000, Prediction Accuracy = 64.66999999999999%, Loss = 0.29309152364730834
Epoch: 8086, Batch Gradient Norm: 14.187116809701234
Epoch: 8086, Batch Gradient Norm after: 14.187116809701234
Epoch 8087/10000, Prediction Accuracy = 64.78%, Loss = 0.2917598009109497
Epoch: 8087, Batch Gradient Norm: 13.479611846024332
Epoch: 8087, Batch Gradient Norm after: 13.479611846024332
Epoch 8088/10000, Prediction Accuracy = 64.80799999999999%, Loss = 0.2903338611125946
Epoch: 8088, Batch Gradient Norm: 14.888465765136422
Epoch: 8088, Batch Gradient Norm after: 14.888465765136422
Epoch 8089/10000, Prediction Accuracy = 64.83%, Loss = 0.2940835297107697
Epoch: 8089, Batch Gradient Norm: 16.9249869961164
Epoch: 8089, Batch Gradient Norm after: 16.9249869961164
Epoch 8090/10000, Prediction Accuracy = 64.75%, Loss = 0.2955829918384552
Epoch: 8090, Batch Gradient Norm: 15.114558361089133
Epoch: 8090, Batch Gradient Norm after: 15.114558361089133
Epoch 8091/10000, Prediction Accuracy = 64.73400000000001%, Loss = 0.29314240217208865
Epoch: 8091, Batch Gradient Norm: 14.622029820390173
Epoch: 8091, Batch Gradient Norm after: 14.622029820390173
Epoch 8092/10000, Prediction Accuracy = 64.846%, Loss = 0.29390786290168763
Epoch: 8092, Batch Gradient Norm: 13.839105090548768
Epoch: 8092, Batch Gradient Norm after: 13.839105090548768
Epoch 8093/10000, Prediction Accuracy = 64.74600000000001%, Loss = 0.2957733154296875
Epoch: 8093, Batch Gradient Norm: 15.268410311212596
Epoch: 8093, Batch Gradient Norm after: 15.268410311212596
Epoch 8094/10000, Prediction Accuracy = 64.75%, Loss = 0.29513317346572876
Epoch: 8094, Batch Gradient Norm: 17.609629273328725
Epoch: 8094, Batch Gradient Norm after: 17.609629273328725
Epoch 8095/10000, Prediction Accuracy = 64.852%, Loss = 0.2965360939502716
Epoch: 8095, Batch Gradient Norm: 15.880957914561366
Epoch: 8095, Batch Gradient Norm after: 15.880957914561366
Epoch 8096/10000, Prediction Accuracy = 64.718%, Loss = 0.2949913263320923
Epoch: 8096, Batch Gradient Norm: 13.522635796576516
Epoch: 8096, Batch Gradient Norm after: 13.522635796576516
Epoch 8097/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.29192450642585754
Epoch: 8097, Batch Gradient Norm: 10.486394147406012
Epoch: 8097, Batch Gradient Norm after: 10.486394147406012
Epoch 8098/10000, Prediction Accuracy = 64.906%, Loss = 0.2868021190166473
Epoch: 8098, Batch Gradient Norm: 11.586794692561131
Epoch: 8098, Batch Gradient Norm after: 11.586794692561131
Epoch 8099/10000, Prediction Accuracy = 64.696%, Loss = 0.2914783775806427
Epoch: 8099, Batch Gradient Norm: 11.411932476039851
Epoch: 8099, Batch Gradient Norm after: 11.411932476039851
Epoch 8100/10000, Prediction Accuracy = 64.77%, Loss = 0.28896665573120117
Epoch: 8100, Batch Gradient Norm: 15.41982306209151
Epoch: 8100, Batch Gradient Norm after: 15.41982306209151
Epoch 8101/10000, Prediction Accuracy = 64.82000000000001%, Loss = 0.29251679182052615
Epoch: 8101, Batch Gradient Norm: 14.625740561108508
Epoch: 8101, Batch Gradient Norm after: 14.625740561108508
Epoch 8102/10000, Prediction Accuracy = 64.83%, Loss = 0.2907774388790131
Epoch: 8102, Batch Gradient Norm: 14.343739156060835
Epoch: 8102, Batch Gradient Norm after: 14.343739156060835
Epoch 8103/10000, Prediction Accuracy = 64.74199999999999%, Loss = 0.29202004671096804
Epoch: 8103, Batch Gradient Norm: 14.192776759871643
Epoch: 8103, Batch Gradient Norm after: 14.192776759871643
Epoch 8104/10000, Prediction Accuracy = 64.846%, Loss = 0.2894061505794525
Epoch: 8104, Batch Gradient Norm: 13.873412812087683
Epoch: 8104, Batch Gradient Norm after: 13.873412812087683
Epoch 8105/10000, Prediction Accuracy = 64.84%, Loss = 0.2909631371498108
Epoch: 8105, Batch Gradient Norm: 14.356850394174788
Epoch: 8105, Batch Gradient Norm after: 14.356850394174788
Epoch 8106/10000, Prediction Accuracy = 64.812%, Loss = 0.2915475845336914
Epoch: 8106, Batch Gradient Norm: 13.545103967120061
Epoch: 8106, Batch Gradient Norm after: 13.545103967120061
Epoch 8107/10000, Prediction Accuracy = 64.76%, Loss = 0.2904778003692627
Epoch: 8107, Batch Gradient Norm: 15.546150824556134
Epoch: 8107, Batch Gradient Norm after: 15.546150824556134
Epoch 8108/10000, Prediction Accuracy = 64.87599999999999%, Loss = 0.29533362984657285
Epoch: 8108, Batch Gradient Norm: 14.915753915011473
Epoch: 8108, Batch Gradient Norm after: 14.915753915011473
Epoch 8109/10000, Prediction Accuracy = 64.658%, Loss = 0.2939846575260162
Epoch: 8109, Batch Gradient Norm: 13.595149986164957
Epoch: 8109, Batch Gradient Norm after: 13.595149986164957
Epoch 8110/10000, Prediction Accuracy = 65.00800000000001%, Loss = 0.29100264310836793
Epoch: 8110, Batch Gradient Norm: 13.229997139474145
Epoch: 8110, Batch Gradient Norm after: 13.229997139474145
Epoch 8111/10000, Prediction Accuracy = 64.852%, Loss = 0.28985016942024233
Epoch: 8111, Batch Gradient Norm: 13.250026471350276
Epoch: 8111, Batch Gradient Norm after: 13.250026471350276
Epoch 8112/10000, Prediction Accuracy = 64.792%, Loss = 0.2914489388465881
Epoch: 8112, Batch Gradient Norm: 16.06380573836853
Epoch: 8112, Batch Gradient Norm after: 16.06380573836853
Epoch 8113/10000, Prediction Accuracy = 64.832%, Loss = 0.2955112397670746
Epoch: 8113, Batch Gradient Norm: 17.568273749664638
Epoch: 8113, Batch Gradient Norm after: 17.568273749664638
Epoch 8114/10000, Prediction Accuracy = 64.74600000000001%, Loss = 0.295745187997818
Epoch: 8114, Batch Gradient Norm: 16.40591175176689
Epoch: 8114, Batch Gradient Norm after: 16.40591175176689
Epoch 8115/10000, Prediction Accuracy = 64.83200000000001%, Loss = 0.2956774413585663
Epoch: 8115, Batch Gradient Norm: 16.0919202629829
Epoch: 8115, Batch Gradient Norm after: 16.0919202629829
Epoch 8116/10000, Prediction Accuracy = 64.772%, Loss = 0.29890832901000974
Epoch: 8116, Batch Gradient Norm: 15.19266874274556
Epoch: 8116, Batch Gradient Norm after: 15.19266874274556
Epoch 8117/10000, Prediction Accuracy = 64.774%, Loss = 0.2924524784088135
Epoch: 8117, Batch Gradient Norm: 16.543991508348906
Epoch: 8117, Batch Gradient Norm after: 16.543991508348906
Epoch 8118/10000, Prediction Accuracy = 64.602%, Loss = 0.29409416317939757
Epoch: 8118, Batch Gradient Norm: 16.85545377911311
Epoch: 8118, Batch Gradient Norm after: 16.65375860633257
Epoch 8119/10000, Prediction Accuracy = 64.878%, Loss = 0.2948834478855133
Epoch: 8119, Batch Gradient Norm: 15.613003153176798
Epoch: 8119, Batch Gradient Norm after: 15.613003153176798
Epoch 8120/10000, Prediction Accuracy = 64.7%, Loss = 0.2942897379398346
Epoch: 8120, Batch Gradient Norm: 15.317452253072654
Epoch: 8120, Batch Gradient Norm after: 15.317452253072654
Epoch 8121/10000, Prediction Accuracy = 64.8%, Loss = 0.29294057488441466
Epoch: 8121, Batch Gradient Norm: 14.402502240691975
Epoch: 8121, Batch Gradient Norm after: 14.402502240691975
Epoch 8122/10000, Prediction Accuracy = 64.77799999999999%, Loss = 0.2912633538246155
Epoch: 8122, Batch Gradient Norm: 12.955684620366776
Epoch: 8122, Batch Gradient Norm after: 12.955684620366776
Epoch 8123/10000, Prediction Accuracy = 64.646%, Loss = 0.29089051485061646
Epoch: 8123, Batch Gradient Norm: 13.034840869518089
Epoch: 8123, Batch Gradient Norm after: 13.034840869518089
Epoch 8124/10000, Prediction Accuracy = 64.89%, Loss = 0.29033045172691346
Epoch: 8124, Batch Gradient Norm: 15.196920194609067
Epoch: 8124, Batch Gradient Norm after: 15.196920194609067
Epoch 8125/10000, Prediction Accuracy = 64.85999999999999%, Loss = 0.2920739114284515
Epoch: 8125, Batch Gradient Norm: 14.995407011565897
Epoch: 8125, Batch Gradient Norm after: 14.995407011565897
Epoch 8126/10000, Prediction Accuracy = 64.728%, Loss = 0.2921130955219269
Epoch: 8126, Batch Gradient Norm: 14.337121312287472
Epoch: 8126, Batch Gradient Norm after: 14.337121312287472
Epoch 8127/10000, Prediction Accuracy = 64.752%, Loss = 0.29293119311332705
Epoch: 8127, Batch Gradient Norm: 10.747046120423542
Epoch: 8127, Batch Gradient Norm after: 10.747046120423542
Epoch 8128/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.2879677414894104
Epoch: 8128, Batch Gradient Norm: 15.832694389112193
Epoch: 8128, Batch Gradient Norm after: 15.722517791666412
Epoch 8129/10000, Prediction Accuracy = 64.732%, Loss = 0.29763281941413877
Epoch: 8129, Batch Gradient Norm: 14.510381533280206
Epoch: 8129, Batch Gradient Norm after: 14.510381533280206
Epoch 8130/10000, Prediction Accuracy = 64.79599999999999%, Loss = 0.29595131874084474
Epoch: 8130, Batch Gradient Norm: 11.886396558436417
Epoch: 8130, Batch Gradient Norm after: 11.886396558436417
Epoch 8131/10000, Prediction Accuracy = 64.852%, Loss = 0.28831635117530824
Epoch: 8131, Batch Gradient Norm: 15.470342267896635
Epoch: 8131, Batch Gradient Norm after: 15.470342267896635
Epoch 8132/10000, Prediction Accuracy = 64.822%, Loss = 0.29613858461380005
Epoch: 8132, Batch Gradient Norm: 14.247734507901576
Epoch: 8132, Batch Gradient Norm after: 14.247734507901576
Epoch 8133/10000, Prediction Accuracy = 64.8%, Loss = 0.2926481306552887
Epoch: 8133, Batch Gradient Norm: 14.97097454977167
Epoch: 8133, Batch Gradient Norm after: 14.97097454977167
Epoch 8134/10000, Prediction Accuracy = 64.798%, Loss = 0.29010515809059145
Epoch: 8134, Batch Gradient Norm: 15.07622718059975
Epoch: 8134, Batch Gradient Norm after: 15.07622718059975
Epoch 8135/10000, Prediction Accuracy = 64.874%, Loss = 0.29073208570480347
Epoch: 8135, Batch Gradient Norm: 18.751115397936953
Epoch: 8135, Batch Gradient Norm after: 18.131350052401604
Epoch 8136/10000, Prediction Accuracy = 64.842%, Loss = 0.29832584261894224
Epoch: 8136, Batch Gradient Norm: 17.071936185263077
Epoch: 8136, Batch Gradient Norm after: 17.071936185263077
Epoch 8137/10000, Prediction Accuracy = 64.946%, Loss = 0.29643569588661195
Epoch: 8137, Batch Gradient Norm: 16.300753676330658
Epoch: 8137, Batch Gradient Norm after: 16.300753676330658
Epoch 8138/10000, Prediction Accuracy = 64.792%, Loss = 0.29682897925376894
Epoch: 8138, Batch Gradient Norm: 14.619040115658741
Epoch: 8138, Batch Gradient Norm after: 14.619040115658741
Epoch 8139/10000, Prediction Accuracy = 64.726%, Loss = 0.29086252450942995
Epoch: 8139, Batch Gradient Norm: 14.71021714946859
Epoch: 8139, Batch Gradient Norm after: 14.71021714946859
Epoch 8140/10000, Prediction Accuracy = 64.836%, Loss = 0.2916217386722565
Epoch: 8140, Batch Gradient Norm: 14.641808944245808
Epoch: 8140, Batch Gradient Norm after: 14.641808944245808
Epoch 8141/10000, Prediction Accuracy = 64.75%, Loss = 0.289942866563797
Epoch: 8141, Batch Gradient Norm: 15.745164470509613
Epoch: 8141, Batch Gradient Norm after: 15.745164470509613
Epoch 8142/10000, Prediction Accuracy = 64.722%, Loss = 0.29414343237876894
Epoch: 8142, Batch Gradient Norm: 14.26903693213774
Epoch: 8142, Batch Gradient Norm after: 14.26903693213774
Epoch 8143/10000, Prediction Accuracy = 64.72200000000001%, Loss = 0.2915189862251282
Epoch: 8143, Batch Gradient Norm: 16.176694838519932
Epoch: 8143, Batch Gradient Norm after: 16.176694838519932
Epoch 8144/10000, Prediction Accuracy = 64.81%, Loss = 0.29482600688934324
Epoch: 8144, Batch Gradient Norm: 15.75692445769994
Epoch: 8144, Batch Gradient Norm after: 15.75692445769994
Epoch 8145/10000, Prediction Accuracy = 64.904%, Loss = 0.2915510356426239
Epoch: 8145, Batch Gradient Norm: 15.760368227612586
Epoch: 8145, Batch Gradient Norm after: 15.760368227612586
Epoch 8146/10000, Prediction Accuracy = 64.69800000000001%, Loss = 0.2948927581310272
Epoch: 8146, Batch Gradient Norm: 15.268137649568818
Epoch: 8146, Batch Gradient Norm after: 15.268137649568818
Epoch 8147/10000, Prediction Accuracy = 64.772%, Loss = 0.29309019446372986
Epoch: 8147, Batch Gradient Norm: 15.350127987857494
Epoch: 8147, Batch Gradient Norm after: 15.350127987857494
Epoch 8148/10000, Prediction Accuracy = 64.71000000000001%, Loss = 0.29397475719451904
Epoch: 8148, Batch Gradient Norm: 16.90100502378859
Epoch: 8148, Batch Gradient Norm after: 16.90100502378859
Epoch 8149/10000, Prediction Accuracy = 64.898%, Loss = 0.2945374310016632
Epoch: 8149, Batch Gradient Norm: 15.82333121062044
Epoch: 8149, Batch Gradient Norm after: 15.82333121062044
Epoch 8150/10000, Prediction Accuracy = 64.68%, Loss = 0.2934598445892334
Epoch: 8150, Batch Gradient Norm: 16.06280576573186
Epoch: 8150, Batch Gradient Norm after: 16.06280576573186
Epoch 8151/10000, Prediction Accuracy = 64.848%, Loss = 0.293748551607132
Epoch: 8151, Batch Gradient Norm: 17.31570191150847
Epoch: 8151, Batch Gradient Norm after: 17.31570191150847
Epoch 8152/10000, Prediction Accuracy = 64.90599999999999%, Loss = 0.2943919479846954
Epoch: 8152, Batch Gradient Norm: 17.210576481832813
Epoch: 8152, Batch Gradient Norm after: 17.210576481832813
Epoch 8153/10000, Prediction Accuracy = 64.77199999999999%, Loss = 0.295375669002533
Epoch: 8153, Batch Gradient Norm: 15.542013509093515
Epoch: 8153, Batch Gradient Norm after: 15.542013509093515
Epoch 8154/10000, Prediction Accuracy = 64.936%, Loss = 0.2928441822528839
Epoch: 8154, Batch Gradient Norm: 12.824035992736203
Epoch: 8154, Batch Gradient Norm after: 12.824035992736203
Epoch 8155/10000, Prediction Accuracy = 64.848%, Loss = 0.28978853225708007
Epoch: 8155, Batch Gradient Norm: 14.903642395385956
Epoch: 8155, Batch Gradient Norm after: 14.903642395385956
Epoch 8156/10000, Prediction Accuracy = 64.71%, Loss = 0.2925894379615784
Epoch: 8156, Batch Gradient Norm: 15.462439344907876
Epoch: 8156, Batch Gradient Norm after: 15.462439344907876
Epoch 8157/10000, Prediction Accuracy = 64.65199999999999%, Loss = 0.29285486340522765
Epoch: 8157, Batch Gradient Norm: 14.648621187570496
Epoch: 8157, Batch Gradient Norm after: 14.648621187570496
Epoch 8158/10000, Prediction Accuracy = 64.664%, Loss = 0.29435359835624697
Epoch: 8158, Batch Gradient Norm: 15.409324952581144
Epoch: 8158, Batch Gradient Norm after: 15.409324952581144
Epoch 8159/10000, Prediction Accuracy = 64.684%, Loss = 0.2922241985797882
Epoch: 8159, Batch Gradient Norm: 15.00954086965756
Epoch: 8159, Batch Gradient Norm after: 15.00954086965756
Epoch 8160/10000, Prediction Accuracy = 64.77799999999999%, Loss = 0.2917898058891296
Epoch: 8160, Batch Gradient Norm: 15.74821612147819
Epoch: 8160, Batch Gradient Norm after: 15.74821612147819
Epoch 8161/10000, Prediction Accuracy = 64.866%, Loss = 0.29168141484260557
Epoch: 8161, Batch Gradient Norm: 14.796834633208608
Epoch: 8161, Batch Gradient Norm after: 14.796834633208608
Epoch 8162/10000, Prediction Accuracy = 64.9%, Loss = 0.28982519507408144
Epoch: 8162, Batch Gradient Norm: 14.26883918718269
Epoch: 8162, Batch Gradient Norm after: 14.26883918718269
Epoch 8163/10000, Prediction Accuracy = 64.768%, Loss = 0.2900072157382965
Epoch: 8163, Batch Gradient Norm: 12.349276006483526
Epoch: 8163, Batch Gradient Norm after: 12.349276006483526
Epoch 8164/10000, Prediction Accuracy = 64.708%, Loss = 0.28824920058250425
Epoch: 8164, Batch Gradient Norm: 13.55064526482053
Epoch: 8164, Batch Gradient Norm after: 13.55064526482053
Epoch 8165/10000, Prediction Accuracy = 64.84200000000001%, Loss = 0.29156320095062255
Epoch: 8165, Batch Gradient Norm: 14.978314627042407
Epoch: 8165, Batch Gradient Norm after: 14.978314627042407
Epoch 8166/10000, Prediction Accuracy = 64.716%, Loss = 0.2913787603378296
Epoch: 8166, Batch Gradient Norm: 13.312493036209396
Epoch: 8166, Batch Gradient Norm after: 13.312493036209396
Epoch 8167/10000, Prediction Accuracy = 64.846%, Loss = 0.289574408531189
Epoch: 8167, Batch Gradient Norm: 12.122939506116957
Epoch: 8167, Batch Gradient Norm after: 12.122939506116957
Epoch 8168/10000, Prediction Accuracy = 64.78%, Loss = 0.28780112862586976
Epoch: 8168, Batch Gradient Norm: 12.233811564840584
Epoch: 8168, Batch Gradient Norm after: 12.233811564840584
Epoch 8169/10000, Prediction Accuracy = 64.762%, Loss = 0.2896804213523865
Epoch: 8169, Batch Gradient Norm: 12.565640370743232
Epoch: 8169, Batch Gradient Norm after: 12.565640370743232
Epoch 8170/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.2903606057167053
Epoch: 8170, Batch Gradient Norm: 14.164468581003243
Epoch: 8170, Batch Gradient Norm after: 14.164468581003243
Epoch 8171/10000, Prediction Accuracy = 64.828%, Loss = 0.2913796007633209
Epoch: 8171, Batch Gradient Norm: 12.255497943734111
Epoch: 8171, Batch Gradient Norm after: 12.255497943734111
Epoch 8172/10000, Prediction Accuracy = 64.864%, Loss = 0.2892506420612335
Epoch: 8172, Batch Gradient Norm: 12.12162926923233
Epoch: 8172, Batch Gradient Norm after: 12.12162926923233
Epoch 8173/10000, Prediction Accuracy = 64.73%, Loss = 0.28921186923980713
Epoch: 8173, Batch Gradient Norm: 13.687594679333314
Epoch: 8173, Batch Gradient Norm after: 13.687594679333314
Epoch 8174/10000, Prediction Accuracy = 64.922%, Loss = 0.28998520970344543
Epoch: 8174, Batch Gradient Norm: 17.243877711860964
Epoch: 8174, Batch Gradient Norm after: 17.243877711860964
Epoch 8175/10000, Prediction Accuracy = 64.874%, Loss = 0.29378451108932496
Epoch: 8175, Batch Gradient Norm: 17.756306101609475
Epoch: 8175, Batch Gradient Norm after: 17.756306101609475
Epoch 8176/10000, Prediction Accuracy = 64.65%, Loss = 0.29526999592781067
Epoch: 8176, Batch Gradient Norm: 15.269613509404435
Epoch: 8176, Batch Gradient Norm after: 15.269613509404435
Epoch 8177/10000, Prediction Accuracy = 64.818%, Loss = 0.29068478345870974
Epoch: 8177, Batch Gradient Norm: 14.10384807446261
Epoch: 8177, Batch Gradient Norm after: 14.10384807446261
Epoch 8178/10000, Prediction Accuracy = 64.76199999999999%, Loss = 0.2906490325927734
Epoch: 8178, Batch Gradient Norm: 12.681382468243285
Epoch: 8178, Batch Gradient Norm after: 12.681382468243285
Epoch 8179/10000, Prediction Accuracy = 64.83200000000001%, Loss = 0.29318422079086304
Epoch: 8179, Batch Gradient Norm: 12.26783619520452
Epoch: 8179, Batch Gradient Norm after: 12.26783619520452
Epoch 8180/10000, Prediction Accuracy = 64.802%, Loss = 0.2902398586273193
Epoch: 8180, Batch Gradient Norm: 11.98663538651772
Epoch: 8180, Batch Gradient Norm after: 11.98663538651772
Epoch 8181/10000, Prediction Accuracy = 64.964%, Loss = 0.28765261769294737
Epoch: 8181, Batch Gradient Norm: 12.370919711067295
Epoch: 8181, Batch Gradient Norm after: 12.370919711067295
Epoch 8182/10000, Prediction Accuracy = 64.804%, Loss = 0.2892860949039459
Epoch: 8182, Batch Gradient Norm: 11.502568503075269
Epoch: 8182, Batch Gradient Norm after: 11.502568503075269
Epoch 8183/10000, Prediction Accuracy = 64.8%, Loss = 0.2913719654083252
Epoch: 8183, Batch Gradient Norm: 11.437497692676057
Epoch: 8183, Batch Gradient Norm after: 11.437497692676057
Epoch 8184/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.2897260129451752
Epoch: 8184, Batch Gradient Norm: 12.036309360669327
Epoch: 8184, Batch Gradient Norm after: 12.036309360669327
Epoch 8185/10000, Prediction Accuracy = 64.842%, Loss = 0.287203186750412
Epoch: 8185, Batch Gradient Norm: 13.612740964659517
Epoch: 8185, Batch Gradient Norm after: 13.612740964659517
Epoch 8186/10000, Prediction Accuracy = 64.892%, Loss = 0.28915550708770754
Epoch: 8186, Batch Gradient Norm: 16.755968603565222
Epoch: 8186, Batch Gradient Norm after: 16.421558231852472
Epoch 8187/10000, Prediction Accuracy = 64.806%, Loss = 0.2925057649612427
Epoch: 8187, Batch Gradient Norm: 17.931307192015908
Epoch: 8187, Batch Gradient Norm after: 17.751477377447518
Epoch 8188/10000, Prediction Accuracy = 64.798%, Loss = 0.2949187934398651
Epoch: 8188, Batch Gradient Norm: 19.62667339335445
Epoch: 8188, Batch Gradient Norm after: 19.17230591068097
Epoch 8189/10000, Prediction Accuracy = 64.598%, Loss = 0.29757818579673767
Epoch: 8189, Batch Gradient Norm: 16.657007902545388
Epoch: 8189, Batch Gradient Norm after: 16.657007902545388
Epoch 8190/10000, Prediction Accuracy = 64.612%, Loss = 0.29397052526474
Epoch: 8190, Batch Gradient Norm: 18.153028698185484
Epoch: 8190, Batch Gradient Norm after: 18.153028698185484
Epoch 8191/10000, Prediction Accuracy = 64.688%, Loss = 0.2966970086097717
Epoch: 8191, Batch Gradient Norm: 16.630348184315704
Epoch: 8191, Batch Gradient Norm after: 16.630348184315704
Epoch 8192/10000, Prediction Accuracy = 64.718%, Loss = 0.29506248235702515
Epoch: 8192, Batch Gradient Norm: 17.284512542013616
Epoch: 8192, Batch Gradient Norm after: 17.284512542013616
Epoch 8193/10000, Prediction Accuracy = 64.686%, Loss = 0.2936230003833771
Epoch: 8193, Batch Gradient Norm: 16.86344330952857
Epoch: 8193, Batch Gradient Norm after: 16.86344330952857
Epoch 8194/10000, Prediction Accuracy = 64.714%, Loss = 0.2941377103328705
Epoch: 8194, Batch Gradient Norm: 15.468221376924584
Epoch: 8194, Batch Gradient Norm after: 15.468221376924584
Epoch 8195/10000, Prediction Accuracy = 64.756%, Loss = 0.2918290078639984
Epoch: 8195, Batch Gradient Norm: 15.988014046182865
Epoch: 8195, Batch Gradient Norm after: 15.988014046182865
Epoch 8196/10000, Prediction Accuracy = 64.782%, Loss = 0.2922540307044983
Epoch: 8196, Batch Gradient Norm: 15.928412131513094
Epoch: 8196, Batch Gradient Norm after: 15.928412131513094
Epoch 8197/10000, Prediction Accuracy = 64.94000000000001%, Loss = 0.29217563271522523
Epoch: 8197, Batch Gradient Norm: 14.11391958455115
Epoch: 8197, Batch Gradient Norm after: 14.11391958455115
Epoch 8198/10000, Prediction Accuracy = 64.896%, Loss = 0.288917201757431
Epoch: 8198, Batch Gradient Norm: 12.718502547793651
Epoch: 8198, Batch Gradient Norm after: 12.718502547793651
Epoch 8199/10000, Prediction Accuracy = 64.73400000000001%, Loss = 0.2878831624984741
Epoch: 8199, Batch Gradient Norm: 17.066606219637375
Epoch: 8199, Batch Gradient Norm after: 17.066606219637375
Epoch 8200/10000, Prediction Accuracy = 64.68199999999999%, Loss = 0.2935240089893341
Epoch: 8200, Batch Gradient Norm: 15.594598413819622
Epoch: 8200, Batch Gradient Norm after: 15.594598413819622
Epoch 8201/10000, Prediction Accuracy = 64.904%, Loss = 0.28997759222984315
Epoch: 8201, Batch Gradient Norm: 16.932445410680785
Epoch: 8201, Batch Gradient Norm after: 16.932445410680785
Epoch 8202/10000, Prediction Accuracy = 64.804%, Loss = 0.29338210821151733
Epoch: 8202, Batch Gradient Norm: 15.394065717281359
Epoch: 8202, Batch Gradient Norm after: 15.394065717281359
Epoch 8203/10000, Prediction Accuracy = 64.804%, Loss = 0.29150413870811465
Epoch: 8203, Batch Gradient Norm: 14.362520192600297
Epoch: 8203, Batch Gradient Norm after: 14.362520192600297
Epoch 8204/10000, Prediction Accuracy = 64.97%, Loss = 0.2902743875980377
Epoch: 8204, Batch Gradient Norm: 17.15785595414293
Epoch: 8204, Batch Gradient Norm after: 17.15785595414293
Epoch 8205/10000, Prediction Accuracy = 64.81%, Loss = 0.2937918484210968
Epoch: 8205, Batch Gradient Norm: 15.99170182167837
Epoch: 8205, Batch Gradient Norm after: 15.99170182167837
Epoch 8206/10000, Prediction Accuracy = 64.708%, Loss = 0.2924378991127014
Epoch: 8206, Batch Gradient Norm: 14.050511877699712
Epoch: 8206, Batch Gradient Norm after: 14.050511877699712
Epoch 8207/10000, Prediction Accuracy = 64.91999999999999%, Loss = 0.2908872485160828
Epoch: 8207, Batch Gradient Norm: 14.403821218305806
Epoch: 8207, Batch Gradient Norm after: 14.403821218305806
Epoch 8208/10000, Prediction Accuracy = 64.98%, Loss = 0.29073156118392945
Epoch: 8208, Batch Gradient Norm: 13.730696027740537
Epoch: 8208, Batch Gradient Norm after: 13.730696027740537
Epoch 8209/10000, Prediction Accuracy = 64.846%, Loss = 0.29000691771507264
Epoch: 8209, Batch Gradient Norm: 12.950364000965662
Epoch: 8209, Batch Gradient Norm after: 12.950364000965662
Epoch 8210/10000, Prediction Accuracy = 64.798%, Loss = 0.28652132153511045
Epoch: 8210, Batch Gradient Norm: 12.84164862955054
Epoch: 8210, Batch Gradient Norm after: 12.84164862955054
Epoch 8211/10000, Prediction Accuracy = 64.928%, Loss = 0.2856705129146576
Epoch: 8211, Batch Gradient Norm: 11.054865106316132
Epoch: 8211, Batch Gradient Norm after: 11.054865106316132
Epoch 8212/10000, Prediction Accuracy = 64.902%, Loss = 0.2856670796871185
Epoch: 8212, Batch Gradient Norm: 12.971304682536182
Epoch: 8212, Batch Gradient Norm after: 12.971304682536182
Epoch 8213/10000, Prediction Accuracy = 64.96%, Loss = 0.28657644987106323
Epoch: 8213, Batch Gradient Norm: 13.425449208729441
Epoch: 8213, Batch Gradient Norm after: 13.425449208729441
Epoch 8214/10000, Prediction Accuracy = 64.94399999999999%, Loss = 0.2887516260147095
Epoch: 8214, Batch Gradient Norm: 14.242230469973354
Epoch: 8214, Batch Gradient Norm after: 14.242230469973354
Epoch 8215/10000, Prediction Accuracy = 64.844%, Loss = 0.28859850764274597
Epoch: 8215, Batch Gradient Norm: 14.563990823935747
Epoch: 8215, Batch Gradient Norm after: 14.563990823935747
Epoch 8216/10000, Prediction Accuracy = 64.826%, Loss = 0.2923014581203461
Epoch: 8216, Batch Gradient Norm: 16.738941676292317
Epoch: 8216, Batch Gradient Norm after: 16.738941676292317
Epoch 8217/10000, Prediction Accuracy = 64.74600000000001%, Loss = 0.2953105688095093
Epoch: 8217, Batch Gradient Norm: 14.587963670303457
Epoch: 8217, Batch Gradient Norm after: 14.587963670303457
Epoch 8218/10000, Prediction Accuracy = 64.85%, Loss = 0.2898725211620331
Epoch: 8218, Batch Gradient Norm: 15.406025128236921
Epoch: 8218, Batch Gradient Norm after: 15.406025128236921
Epoch 8219/10000, Prediction Accuracy = 64.938%, Loss = 0.2924001157283783
Epoch: 8219, Batch Gradient Norm: 14.50051756925845
Epoch: 8219, Batch Gradient Norm after: 14.50051756925845
Epoch 8220/10000, Prediction Accuracy = 64.864%, Loss = 0.28917239904403685
Epoch: 8220, Batch Gradient Norm: 14.015799554835498
Epoch: 8220, Batch Gradient Norm after: 14.015799554835498
Epoch 8221/10000, Prediction Accuracy = 64.846%, Loss = 0.28949680328369143
Epoch: 8221, Batch Gradient Norm: 15.75556600825387
Epoch: 8221, Batch Gradient Norm after: 15.75556600825387
Epoch 8222/10000, Prediction Accuracy = 64.81000000000002%, Loss = 0.29390166997909545
Epoch: 8222, Batch Gradient Norm: 14.430769384659236
Epoch: 8222, Batch Gradient Norm after: 14.430769384659236
Epoch 8223/10000, Prediction Accuracy = 64.844%, Loss = 0.28976110816001893
Epoch: 8223, Batch Gradient Norm: 17.266266197470546
Epoch: 8223, Batch Gradient Norm after: 17.266266197470546
Epoch 8224/10000, Prediction Accuracy = 64.71799999999999%, Loss = 0.2946373522281647
Epoch: 8224, Batch Gradient Norm: 17.830309218850278
Epoch: 8224, Batch Gradient Norm after: 17.830309218850278
Epoch 8225/10000, Prediction Accuracy = 64.87%, Loss = 0.2933365225791931
Epoch: 8225, Batch Gradient Norm: 15.484871791837955
Epoch: 8225, Batch Gradient Norm after: 15.484871791837955
Epoch 8226/10000, Prediction Accuracy = 64.902%, Loss = 0.2919105291366577
Epoch: 8226, Batch Gradient Norm: 14.741418651788967
Epoch: 8226, Batch Gradient Norm after: 14.741418651788967
Epoch 8227/10000, Prediction Accuracy = 64.824%, Loss = 0.2900769114494324
Epoch: 8227, Batch Gradient Norm: 12.506431897095347
Epoch: 8227, Batch Gradient Norm after: 12.506431897095347
Epoch 8228/10000, Prediction Accuracy = 64.934%, Loss = 0.2876405298709869
Epoch: 8228, Batch Gradient Norm: 14.475008565545897
Epoch: 8228, Batch Gradient Norm after: 14.475008565545897
Epoch 8229/10000, Prediction Accuracy = 64.75200000000001%, Loss = 0.2901124835014343
Epoch: 8229, Batch Gradient Norm: 14.864942395581012
Epoch: 8229, Batch Gradient Norm after: 14.864942395581012
Epoch 8230/10000, Prediction Accuracy = 64.854%, Loss = 0.2914921760559082
Epoch: 8230, Batch Gradient Norm: 13.917048818530404
Epoch: 8230, Batch Gradient Norm after: 13.917048818530404
Epoch 8231/10000, Prediction Accuracy = 64.954%, Loss = 0.2868418514728546
Epoch: 8231, Batch Gradient Norm: 14.443022071889871
Epoch: 8231, Batch Gradient Norm after: 14.443022071889871
Epoch 8232/10000, Prediction Accuracy = 64.94000000000001%, Loss = 0.2912509858608246
Epoch: 8232, Batch Gradient Norm: 16.59584733796198
Epoch: 8232, Batch Gradient Norm after: 16.50891370019187
Epoch 8233/10000, Prediction Accuracy = 64.852%, Loss = 0.29247410893440245
Epoch: 8233, Batch Gradient Norm: 14.297515809387646
Epoch: 8233, Batch Gradient Norm after: 14.297515809387646
Epoch 8234/10000, Prediction Accuracy = 64.784%, Loss = 0.2893787741661072
Epoch: 8234, Batch Gradient Norm: 19.32278458485112
Epoch: 8234, Batch Gradient Norm after: 19.21567604664924
Epoch 8235/10000, Prediction Accuracy = 64.92399999999999%, Loss = 0.2946383595466614
Epoch: 8235, Batch Gradient Norm: 22.877143193955792
Epoch: 8235, Batch Gradient Norm after: 20.111158475733415
Epoch 8236/10000, Prediction Accuracy = 64.91799999999999%, Loss = 0.3031537115573883
Epoch: 8236, Batch Gradient Norm: 21.279043737804834
Epoch: 8236, Batch Gradient Norm after: 20.075050522650848
Epoch 8237/10000, Prediction Accuracy = 64.896%, Loss = 0.29807891845703127
Epoch: 8237, Batch Gradient Norm: 18.324887612393212
Epoch: 8237, Batch Gradient Norm after: 17.909386401870822
Epoch 8238/10000, Prediction Accuracy = 64.672%, Loss = 0.2952520430088043
Epoch: 8238, Batch Gradient Norm: 17.40954075506813
Epoch: 8238, Batch Gradient Norm after: 17.40954075506813
Epoch 8239/10000, Prediction Accuracy = 64.92200000000001%, Loss = 0.29380324482917786
Epoch: 8239, Batch Gradient Norm: 15.481807255495626
Epoch: 8239, Batch Gradient Norm after: 15.481807255495626
Epoch 8240/10000, Prediction Accuracy = 64.76199999999999%, Loss = 0.29209492802619935
Epoch: 8240, Batch Gradient Norm: 16.400111167383727
Epoch: 8240, Batch Gradient Norm after: 16.400111167383727
Epoch 8241/10000, Prediction Accuracy = 64.756%, Loss = 0.29439263939857485
Epoch: 8241, Batch Gradient Norm: 17.56877649124881
Epoch: 8241, Batch Gradient Norm after: 17.56877649124881
Epoch 8242/10000, Prediction Accuracy = 64.776%, Loss = 0.2944507718086243
Epoch: 8242, Batch Gradient Norm: 14.927285091330127
Epoch: 8242, Batch Gradient Norm after: 14.927285091330127
Epoch 8243/10000, Prediction Accuracy = 64.88000000000001%, Loss = 0.28927762508392335
Epoch: 8243, Batch Gradient Norm: 15.282005458322622
Epoch: 8243, Batch Gradient Norm after: 15.282005458322622
Epoch 8244/10000, Prediction Accuracy = 64.8%, Loss = 0.2883393406867981
Epoch: 8244, Batch Gradient Norm: 12.421737862487412
Epoch: 8244, Batch Gradient Norm after: 12.421737862487412
Epoch 8245/10000, Prediction Accuracy = 64.876%, Loss = 0.2868351459503174
Epoch: 8245, Batch Gradient Norm: 12.592332179647027
Epoch: 8245, Batch Gradient Norm after: 12.592332179647027
Epoch 8246/10000, Prediction Accuracy = 64.782%, Loss = 0.2869623601436615
Epoch: 8246, Batch Gradient Norm: 12.66905685912452
Epoch: 8246, Batch Gradient Norm after: 12.66905685912452
Epoch 8247/10000, Prediction Accuracy = 64.914%, Loss = 0.28804964423179624
Epoch: 8247, Batch Gradient Norm: 14.244392734093456
Epoch: 8247, Batch Gradient Norm after: 14.244392734093456
Epoch 8248/10000, Prediction Accuracy = 64.856%, Loss = 0.290315979719162
Epoch: 8248, Batch Gradient Norm: 15.848968119788918
Epoch: 8248, Batch Gradient Norm after: 15.848968119788918
Epoch 8249/10000, Prediction Accuracy = 64.806%, Loss = 0.29348695278167725
Epoch: 8249, Batch Gradient Norm: 17.04073356352996
Epoch: 8249, Batch Gradient Norm after: 17.04073356352996
Epoch 8250/10000, Prediction Accuracy = 64.934%, Loss = 0.29219244718551635
Epoch: 8250, Batch Gradient Norm: 15.957968950258369
Epoch: 8250, Batch Gradient Norm after: 15.957968950258369
Epoch 8251/10000, Prediction Accuracy = 64.922%, Loss = 0.2909676134586334
Epoch: 8251, Batch Gradient Norm: 15.376419667511827
Epoch: 8251, Batch Gradient Norm after: 15.376419667511827
Epoch 8252/10000, Prediction Accuracy = 64.862%, Loss = 0.2920437455177307
Epoch: 8252, Batch Gradient Norm: 13.719364332956031
Epoch: 8252, Batch Gradient Norm after: 13.719364332956031
Epoch 8253/10000, Prediction Accuracy = 64.876%, Loss = 0.2900957643985748
Epoch: 8253, Batch Gradient Norm: 13.36654785681773
Epoch: 8253, Batch Gradient Norm after: 13.36654785681773
Epoch 8254/10000, Prediction Accuracy = 64.768%, Loss = 0.28903573751449585
Epoch: 8254, Batch Gradient Norm: 14.24420344735363
Epoch: 8254, Batch Gradient Norm after: 14.24420344735363
Epoch 8255/10000, Prediction Accuracy = 64.814%, Loss = 0.2924703240394592
Epoch: 8255, Batch Gradient Norm: 13.034907584113391
Epoch: 8255, Batch Gradient Norm after: 13.034907584113391
Epoch 8256/10000, Prediction Accuracy = 64.952%, Loss = 0.28898817896842954
Epoch: 8256, Batch Gradient Norm: 16.261917296879115
Epoch: 8256, Batch Gradient Norm after: 16.261917296879115
Epoch 8257/10000, Prediction Accuracy = 64.752%, Loss = 0.29105848670005796
Epoch: 8257, Batch Gradient Norm: 14.352963981839734
Epoch: 8257, Batch Gradient Norm after: 14.352963981839734
Epoch 8258/10000, Prediction Accuracy = 64.83200000000001%, Loss = 0.29146322011947634
Epoch: 8258, Batch Gradient Norm: 12.829682672396645
Epoch: 8258, Batch Gradient Norm after: 12.829682672396645
Epoch 8259/10000, Prediction Accuracy = 64.87%, Loss = 0.2865693747997284
Epoch: 8259, Batch Gradient Norm: 13.353795390330003
Epoch: 8259, Batch Gradient Norm after: 13.353795390330003
Epoch 8260/10000, Prediction Accuracy = 64.856%, Loss = 0.28944194316864014
Epoch: 8260, Batch Gradient Norm: 11.401490841013748
Epoch: 8260, Batch Gradient Norm after: 11.401490841013748
Epoch 8261/10000, Prediction Accuracy = 64.904%, Loss = 0.2874326169490814
Epoch: 8261, Batch Gradient Norm: 11.272170723409745
Epoch: 8261, Batch Gradient Norm after: 11.272170723409745
Epoch 8262/10000, Prediction Accuracy = 64.9%, Loss = 0.2855492472648621
Epoch: 8262, Batch Gradient Norm: 12.048779615993096
Epoch: 8262, Batch Gradient Norm after: 12.048779615993096
Epoch 8263/10000, Prediction Accuracy = 64.982%, Loss = 0.28845858573913574
Epoch: 8263, Batch Gradient Norm: 14.912819836041628
Epoch: 8263, Batch Gradient Norm after: 14.912819836041628
Epoch 8264/10000, Prediction Accuracy = 64.83200000000001%, Loss = 0.28925169706344606
Epoch: 8264, Batch Gradient Norm: 15.27042483061921
Epoch: 8264, Batch Gradient Norm after: 15.27042483061921
Epoch 8265/10000, Prediction Accuracy = 64.91%, Loss = 0.29143884778022766
Epoch: 8265, Batch Gradient Norm: 15.434578967460194
Epoch: 8265, Batch Gradient Norm after: 15.434578967460194
Epoch 8266/10000, Prediction Accuracy = 64.792%, Loss = 0.29161508083343507
Epoch: 8266, Batch Gradient Norm: 13.148791543719845
Epoch: 8266, Batch Gradient Norm after: 13.148791543719845
Epoch 8267/10000, Prediction Accuracy = 64.98400000000001%, Loss = 0.28551421165466306
Epoch: 8267, Batch Gradient Norm: 11.173383876671295
Epoch: 8267, Batch Gradient Norm after: 11.173383876671295
Epoch 8268/10000, Prediction Accuracy = 64.84%, Loss = 0.28548175692558286
Epoch: 8268, Batch Gradient Norm: 13.475251699655207
Epoch: 8268, Batch Gradient Norm after: 13.475251699655207
Epoch 8269/10000, Prediction Accuracy = 64.886%, Loss = 0.28947689533233645
Epoch: 8269, Batch Gradient Norm: 15.044944266413125
Epoch: 8269, Batch Gradient Norm after: 15.044944266413125
Epoch 8270/10000, Prediction Accuracy = 64.984%, Loss = 0.2868407487869263
Epoch: 8270, Batch Gradient Norm: 16.499250203311046
Epoch: 8270, Batch Gradient Norm after: 16.499250203311046
Epoch 8271/10000, Prediction Accuracy = 64.9%, Loss = 0.2924949169158936
Epoch: 8271, Batch Gradient Norm: 14.047221922209426
Epoch: 8271, Batch Gradient Norm after: 14.047221922209426
Epoch 8272/10000, Prediction Accuracy = 64.926%, Loss = 0.28602094054222105
Epoch: 8272, Batch Gradient Norm: 13.631655421560028
Epoch: 8272, Batch Gradient Norm after: 13.631655421560028
Epoch 8273/10000, Prediction Accuracy = 64.92600000000002%, Loss = 0.2860487401485443
Epoch: 8273, Batch Gradient Norm: 13.882938111562432
Epoch: 8273, Batch Gradient Norm after: 13.882938111562432
Epoch 8274/10000, Prediction Accuracy = 64.856%, Loss = 0.28984405398368834
Epoch: 8274, Batch Gradient Norm: 19.04644304785164
Epoch: 8274, Batch Gradient Norm after: 18.61343615523894
Epoch 8275/10000, Prediction Accuracy = 64.98599999999999%, Loss = 0.29786699414253237
Epoch: 8275, Batch Gradient Norm: 15.99308736068503
Epoch: 8275, Batch Gradient Norm after: 15.99308736068503
Epoch 8276/10000, Prediction Accuracy = 64.864%, Loss = 0.2944673001766205
Epoch: 8276, Batch Gradient Norm: 15.104257679918659
Epoch: 8276, Batch Gradient Norm after: 15.104257679918659
Epoch 8277/10000, Prediction Accuracy = 64.994%, Loss = 0.28949207067489624
Epoch: 8277, Batch Gradient Norm: 15.038867900170757
Epoch: 8277, Batch Gradient Norm after: 15.038867900170757
Epoch 8278/10000, Prediction Accuracy = 64.82000000000001%, Loss = 0.29256553053855894
Epoch: 8278, Batch Gradient Norm: 15.054596479043903
Epoch: 8278, Batch Gradient Norm after: 15.054596479043903
Epoch 8279/10000, Prediction Accuracy = 64.88%, Loss = 0.28880231380462645
Epoch: 8279, Batch Gradient Norm: 14.019275072516223
Epoch: 8279, Batch Gradient Norm after: 14.019275072516223
Epoch 8280/10000, Prediction Accuracy = 64.97%, Loss = 0.2884796142578125
Epoch: 8280, Batch Gradient Norm: 16.53260639198031
Epoch: 8280, Batch Gradient Norm after: 16.53260639198031
Epoch 8281/10000, Prediction Accuracy = 64.93%, Loss = 0.29078990817070005
Epoch: 8281, Batch Gradient Norm: 15.311109339466412
Epoch: 8281, Batch Gradient Norm after: 15.311109339466412
Epoch 8282/10000, Prediction Accuracy = 65.02000000000001%, Loss = 0.28939459323883054
Epoch: 8282, Batch Gradient Norm: 14.74225293929649
Epoch: 8282, Batch Gradient Norm after: 14.74225293929649
Epoch 8283/10000, Prediction Accuracy = 64.914%, Loss = 0.29216961860656737
Epoch: 8283, Batch Gradient Norm: 13.77833081687329
Epoch: 8283, Batch Gradient Norm after: 13.77833081687329
Epoch 8284/10000, Prediction Accuracy = 64.79400000000001%, Loss = 0.28730120658874514
Epoch: 8284, Batch Gradient Norm: 12.709205900874727
Epoch: 8284, Batch Gradient Norm after: 12.709205900874727
Epoch 8285/10000, Prediction Accuracy = 64.904%, Loss = 0.2860581874847412
Epoch: 8285, Batch Gradient Norm: 12.728928768135606
Epoch: 8285, Batch Gradient Norm after: 12.728928768135606
Epoch 8286/10000, Prediction Accuracy = 64.874%, Loss = 0.2870976984500885
Epoch: 8286, Batch Gradient Norm: 13.008336244371886
Epoch: 8286, Batch Gradient Norm after: 13.008336244371886
Epoch 8287/10000, Prediction Accuracy = 64.77000000000001%, Loss = 0.2891576409339905
Epoch: 8287, Batch Gradient Norm: 12.73529955022328
Epoch: 8287, Batch Gradient Norm after: 12.73529955022328
Epoch 8288/10000, Prediction Accuracy = 64.898%, Loss = 0.28772215247154237
Epoch: 8288, Batch Gradient Norm: 12.771976607829272
Epoch: 8288, Batch Gradient Norm after: 12.771976607829272
Epoch 8289/10000, Prediction Accuracy = 64.94000000000001%, Loss = 0.28728281855583193
Epoch: 8289, Batch Gradient Norm: 12.42130788385301
Epoch: 8289, Batch Gradient Norm after: 12.42130788385301
Epoch 8290/10000, Prediction Accuracy = 65.018%, Loss = 0.2872817039489746
Epoch: 8290, Batch Gradient Norm: 11.600979311231555
Epoch: 8290, Batch Gradient Norm after: 11.600979311231555
Epoch 8291/10000, Prediction Accuracy = 64.84200000000001%, Loss = 0.28666253089904786
Epoch: 8291, Batch Gradient Norm: 14.80198182048858
Epoch: 8291, Batch Gradient Norm after: 14.80198182048858
Epoch 8292/10000, Prediction Accuracy = 64.76200000000001%, Loss = 0.288326358795166
Epoch: 8292, Batch Gradient Norm: 17.213418172236665
Epoch: 8292, Batch Gradient Norm after: 17.213418172236665
Epoch 8293/10000, Prediction Accuracy = 64.96199999999999%, Loss = 0.29116724133491517
Epoch: 8293, Batch Gradient Norm: 15.058976569874957
Epoch: 8293, Batch Gradient Norm after: 15.058976569874957
Epoch 8294/10000, Prediction Accuracy = 64.75800000000001%, Loss = 0.2903566062450409
Epoch: 8294, Batch Gradient Norm: 16.379446209149926
Epoch: 8294, Batch Gradient Norm after: 16.379446209149926
Epoch 8295/10000, Prediction Accuracy = 64.85999999999999%, Loss = 0.29192426800727844
Epoch: 8295, Batch Gradient Norm: 17.56747200864084
Epoch: 8295, Batch Gradient Norm after: 17.56747200864084
Epoch 8296/10000, Prediction Accuracy = 64.67%, Loss = 0.2918649673461914
Epoch: 8296, Batch Gradient Norm: 14.192222966088888
Epoch: 8296, Batch Gradient Norm after: 14.192222966088888
Epoch 8297/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.28806053996086123
Epoch: 8297, Batch Gradient Norm: 14.113739282585266
Epoch: 8297, Batch Gradient Norm after: 14.090202697712915
Epoch 8298/10000, Prediction Accuracy = 64.90599999999999%, Loss = 0.2900572121143341
Epoch: 8298, Batch Gradient Norm: 13.810632398965959
Epoch: 8298, Batch Gradient Norm after: 13.810632398965959
Epoch 8299/10000, Prediction Accuracy = 64.874%, Loss = 0.28936646580696107
Epoch: 8299, Batch Gradient Norm: 13.173356806100617
Epoch: 8299, Batch Gradient Norm after: 13.173356806100617
Epoch 8300/10000, Prediction Accuracy = 64.958%, Loss = 0.28678092956542967
Epoch: 8300, Batch Gradient Norm: 14.159466842741356
Epoch: 8300, Batch Gradient Norm after: 14.159466842741356
Epoch 8301/10000, Prediction Accuracy = 64.848%, Loss = 0.29165102243423463
Epoch: 8301, Batch Gradient Norm: 12.46257180509292
Epoch: 8301, Batch Gradient Norm after: 12.46257180509292
Epoch 8302/10000, Prediction Accuracy = 64.94%, Loss = 0.2851479947566986
Epoch: 8302, Batch Gradient Norm: 13.672199008660154
Epoch: 8302, Batch Gradient Norm after: 13.672199008660154
Epoch 8303/10000, Prediction Accuracy = 64.924%, Loss = 0.28827370405197145
Epoch: 8303, Batch Gradient Norm: 15.28315282734946
Epoch: 8303, Batch Gradient Norm after: 15.28315282734946
Epoch 8304/10000, Prediction Accuracy = 64.85800000000002%, Loss = 0.2910042107105255
Epoch: 8304, Batch Gradient Norm: 15.110385189760796
Epoch: 8304, Batch Gradient Norm after: 15.110385189760796
Epoch 8305/10000, Prediction Accuracy = 64.852%, Loss = 0.28824257254600527
Epoch: 8305, Batch Gradient Norm: 15.57757884309071
Epoch: 8305, Batch Gradient Norm after: 15.57757884309071
Epoch 8306/10000, Prediction Accuracy = 64.88199999999999%, Loss = 0.28975706100463866
Epoch: 8306, Batch Gradient Norm: 15.811177189172147
Epoch: 8306, Batch Gradient Norm after: 15.811177189172147
Epoch 8307/10000, Prediction Accuracy = 64.934%, Loss = 0.29077590703964235
Epoch: 8307, Batch Gradient Norm: 13.114634233888873
Epoch: 8307, Batch Gradient Norm after: 13.114634233888873
Epoch 8308/10000, Prediction Accuracy = 64.886%, Loss = 0.28989713191986083
Epoch: 8308, Batch Gradient Norm: 12.454615004019152
Epoch: 8308, Batch Gradient Norm after: 12.454615004019152
Epoch 8309/10000, Prediction Accuracy = 65.036%, Loss = 0.28743932247161863
Epoch: 8309, Batch Gradient Norm: 12.453639955445565
Epoch: 8309, Batch Gradient Norm after: 12.453639955445565
Epoch 8310/10000, Prediction Accuracy = 64.926%, Loss = 0.2892507791519165
Epoch: 8310, Batch Gradient Norm: 11.666053697163479
Epoch: 8310, Batch Gradient Norm after: 11.666053697163479
Epoch 8311/10000, Prediction Accuracy = 64.876%, Loss = 0.28581382632255553
Epoch: 8311, Batch Gradient Norm: 11.543304348786432
Epoch: 8311, Batch Gradient Norm after: 11.543304348786432
Epoch 8312/10000, Prediction Accuracy = 65.096%, Loss = 0.2863435447216034
Epoch: 8312, Batch Gradient Norm: 14.29522801840708
Epoch: 8312, Batch Gradient Norm after: 14.29522801840708
Epoch 8313/10000, Prediction Accuracy = 64.862%, Loss = 0.2911823809146881
Epoch: 8313, Batch Gradient Norm: 17.092095857493675
Epoch: 8313, Batch Gradient Norm after: 17.092095857493675
Epoch 8314/10000, Prediction Accuracy = 64.828%, Loss = 0.29173173308372496
Epoch: 8314, Batch Gradient Norm: 15.330295945449114
Epoch: 8314, Batch Gradient Norm after: 15.330295945449114
Epoch 8315/10000, Prediction Accuracy = 64.89399999999999%, Loss = 0.2900678634643555
Epoch: 8315, Batch Gradient Norm: 12.895480024616566
Epoch: 8315, Batch Gradient Norm after: 12.895480024616566
Epoch 8316/10000, Prediction Accuracy = 64.84%, Loss = 0.2854962944984436
Epoch: 8316, Batch Gradient Norm: 11.427980361633436
Epoch: 8316, Batch Gradient Norm after: 11.427980361633436
Epoch 8317/10000, Prediction Accuracy = 65.04599999999999%, Loss = 0.2866968035697937
Epoch: 8317, Batch Gradient Norm: 14.220974866055283
Epoch: 8317, Batch Gradient Norm after: 14.220974866055283
Epoch 8318/10000, Prediction Accuracy = 65.03%, Loss = 0.28635030388832095
Epoch: 8318, Batch Gradient Norm: 15.982051231154003
Epoch: 8318, Batch Gradient Norm after: 15.982051231154003
Epoch 8319/10000, Prediction Accuracy = 64.964%, Loss = 0.29057320952415466
Epoch: 8319, Batch Gradient Norm: 13.735716378618651
Epoch: 8319, Batch Gradient Norm after: 13.735716378618651
Epoch 8320/10000, Prediction Accuracy = 64.924%, Loss = 0.2860999941825867
Epoch: 8320, Batch Gradient Norm: 13.703183639813448
Epoch: 8320, Batch Gradient Norm after: 13.703183639813448
Epoch 8321/10000, Prediction Accuracy = 64.96799999999999%, Loss = 0.290796285867691
Epoch: 8321, Batch Gradient Norm: 12.447977204620758
Epoch: 8321, Batch Gradient Norm after: 12.447977204620758
Epoch 8322/10000, Prediction Accuracy = 65.092%, Loss = 0.2847965657711029
Epoch: 8322, Batch Gradient Norm: 16.070449774505036
Epoch: 8322, Batch Gradient Norm after: 15.821991546795006
Epoch 8323/10000, Prediction Accuracy = 64.99%, Loss = 0.28980796933174136
Epoch: 8323, Batch Gradient Norm: 13.297458637073229
Epoch: 8323, Batch Gradient Norm after: 13.297458637073229
Epoch 8324/10000, Prediction Accuracy = 65.03%, Loss = 0.2854434072971344
Epoch: 8324, Batch Gradient Norm: 13.74691621156325
Epoch: 8324, Batch Gradient Norm after: 13.74691621156325
Epoch 8325/10000, Prediction Accuracy = 65.07%, Loss = 0.28555408120155334
Epoch: 8325, Batch Gradient Norm: 14.40438256330126
Epoch: 8325, Batch Gradient Norm after: 14.40438256330126
Epoch 8326/10000, Prediction Accuracy = 64.92%, Loss = 0.2873165667057037
Epoch: 8326, Batch Gradient Norm: 15.642009345173035
Epoch: 8326, Batch Gradient Norm after: 15.642009345173035
Epoch 8327/10000, Prediction Accuracy = 64.94%, Loss = 0.2900074660778046
Epoch: 8327, Batch Gradient Norm: 15.938034549481253
Epoch: 8327, Batch Gradient Norm after: 15.938034549481253
Epoch 8328/10000, Prediction Accuracy = 65.004%, Loss = 0.28807424306869506
Epoch: 8328, Batch Gradient Norm: 17.19124854728385
Epoch: 8328, Batch Gradient Norm after: 17.19124854728385
Epoch 8329/10000, Prediction Accuracy = 64.84200000000001%, Loss = 0.2938796103000641
Epoch: 8329, Batch Gradient Norm: 15.960443182937462
Epoch: 8329, Batch Gradient Norm after: 15.960443182937462
Epoch 8330/10000, Prediction Accuracy = 64.89%, Loss = 0.2897571504116058
Epoch: 8330, Batch Gradient Norm: 14.761140867184201
Epoch: 8330, Batch Gradient Norm after: 14.761140867184201
Epoch 8331/10000, Prediction Accuracy = 65.01000000000002%, Loss = 0.28630504608154295
Epoch: 8331, Batch Gradient Norm: 12.344654620423542
Epoch: 8331, Batch Gradient Norm after: 12.344654620423542
Epoch 8332/10000, Prediction Accuracy = 64.872%, Loss = 0.2862694799900055
Epoch: 8332, Batch Gradient Norm: 12.019709638160098
Epoch: 8332, Batch Gradient Norm after: 12.019709638160098
Epoch 8333/10000, Prediction Accuracy = 64.976%, Loss = 0.2855471193790436
Epoch: 8333, Batch Gradient Norm: 15.902797946152083
Epoch: 8333, Batch Gradient Norm after: 15.902797946152083
Epoch 8334/10000, Prediction Accuracy = 64.804%, Loss = 0.29116840958595275
Epoch: 8334, Batch Gradient Norm: 17.286220374129638
Epoch: 8334, Batch Gradient Norm after: 16.93518270689964
Epoch 8335/10000, Prediction Accuracy = 64.868%, Loss = 0.29116804003715513
Epoch: 8335, Batch Gradient Norm: 17.43176629820038
Epoch: 8335, Batch Gradient Norm after: 17.43176629820038
Epoch 8336/10000, Prediction Accuracy = 64.82000000000001%, Loss = 0.29296759366989134
Epoch: 8336, Batch Gradient Norm: 18.813460825786088
Epoch: 8336, Batch Gradient Norm after: 18.813460825786088
Epoch 8337/10000, Prediction Accuracy = 64.79599999999999%, Loss = 0.2925959169864655
Epoch: 8337, Batch Gradient Norm: 17.20437868930519
Epoch: 8337, Batch Gradient Norm after: 17.20437868930519
Epoch 8338/10000, Prediction Accuracy = 64.988%, Loss = 0.29083324074745176
Epoch: 8338, Batch Gradient Norm: 16.908339496448505
Epoch: 8338, Batch Gradient Norm after: 16.908339496448505
Epoch 8339/10000, Prediction Accuracy = 64.77000000000001%, Loss = 0.2913209140300751
Epoch: 8339, Batch Gradient Norm: 19.335730458083923
Epoch: 8339, Batch Gradient Norm after: 18.088109672682386
Epoch 8340/10000, Prediction Accuracy = 65.01399999999998%, Loss = 0.2933076500892639
Epoch: 8340, Batch Gradient Norm: 17.481236015844754
Epoch: 8340, Batch Gradient Norm after: 17.481236015844754
Epoch 8341/10000, Prediction Accuracy = 64.92999999999999%, Loss = 0.29114181399345396
Epoch: 8341, Batch Gradient Norm: 16.930886450740765
Epoch: 8341, Batch Gradient Norm after: 16.930886450740765
Epoch 8342/10000, Prediction Accuracy = 65.01400000000001%, Loss = 0.2895104467868805
Epoch: 8342, Batch Gradient Norm: 17.635530852538388
Epoch: 8342, Batch Gradient Norm after: 17.635530852538388
Epoch 8343/10000, Prediction Accuracy = 65.01599999999999%, Loss = 0.2955780863761902
Epoch: 8343, Batch Gradient Norm: 16.592997903644186
Epoch: 8343, Batch Gradient Norm after: 16.592997903644186
Epoch 8344/10000, Prediction Accuracy = 64.99%, Loss = 0.29102579355239866
Epoch: 8344, Batch Gradient Norm: 16.09842834380871
Epoch: 8344, Batch Gradient Norm after: 16.09842834380871
Epoch 8345/10000, Prediction Accuracy = 64.908%, Loss = 0.2905817925930023
Epoch: 8345, Batch Gradient Norm: 15.333062572271022
Epoch: 8345, Batch Gradient Norm after: 15.333062572271022
Epoch 8346/10000, Prediction Accuracy = 64.90799999999999%, Loss = 0.2889951586723328
Epoch: 8346, Batch Gradient Norm: 14.418733597554615
Epoch: 8346, Batch Gradient Norm after: 14.418733597554615
Epoch 8347/10000, Prediction Accuracy = 64.84400000000001%, Loss = 0.28918920159339906
Epoch: 8347, Batch Gradient Norm: 12.649140102441732
Epoch: 8347, Batch Gradient Norm after: 12.649140102441732
Epoch 8348/10000, Prediction Accuracy = 64.982%, Loss = 0.2867577850818634
Epoch: 8348, Batch Gradient Norm: 14.236702934043208
Epoch: 8348, Batch Gradient Norm after: 14.236702934043208
Epoch 8349/10000, Prediction Accuracy = 64.78399999999999%, Loss = 0.28926767110824586
Epoch: 8349, Batch Gradient Norm: 12.481965119682807
Epoch: 8349, Batch Gradient Norm after: 12.481965119682807
Epoch 8350/10000, Prediction Accuracy = 64.964%, Loss = 0.2842361330986023
Epoch: 8350, Batch Gradient Norm: 13.77140193581916
Epoch: 8350, Batch Gradient Norm after: 13.77140193581916
Epoch 8351/10000, Prediction Accuracy = 64.95%, Loss = 0.2864062011241913
Epoch: 8351, Batch Gradient Norm: 16.31115725055133
Epoch: 8351, Batch Gradient Norm after: 16.31115725055133
Epoch 8352/10000, Prediction Accuracy = 65.05199999999999%, Loss = 0.29058762192726134
Epoch: 8352, Batch Gradient Norm: 16.78628050289021
Epoch: 8352, Batch Gradient Norm after: 16.78628050289021
Epoch 8353/10000, Prediction Accuracy = 64.968%, Loss = 0.28938775062561034
Epoch: 8353, Batch Gradient Norm: 16.628758668964085
Epoch: 8353, Batch Gradient Norm after: 16.628758668964085
Epoch 8354/10000, Prediction Accuracy = 64.89999999999999%, Loss = 0.2942479372024536
Epoch: 8354, Batch Gradient Norm: 13.937591717085834
Epoch: 8354, Batch Gradient Norm after: 13.937591717085834
Epoch 8355/10000, Prediction Accuracy = 65.0%, Loss = 0.28636290431022643
Epoch: 8355, Batch Gradient Norm: 13.50519557149821
Epoch: 8355, Batch Gradient Norm after: 13.50519557149821
Epoch 8356/10000, Prediction Accuracy = 64.83200000000001%, Loss = 0.2866361618041992
Epoch: 8356, Batch Gradient Norm: 11.477486294150262
Epoch: 8356, Batch Gradient Norm after: 11.477486294150262
Epoch 8357/10000, Prediction Accuracy = 64.99799999999999%, Loss = 0.2872564733028412
Epoch: 8357, Batch Gradient Norm: 11.468385090561517
Epoch: 8357, Batch Gradient Norm after: 11.468385090561517
Epoch 8358/10000, Prediction Accuracy = 64.898%, Loss = 0.28395835161209104
Epoch: 8358, Batch Gradient Norm: 11.465821586281324
Epoch: 8358, Batch Gradient Norm after: 11.465821586281324
Epoch 8359/10000, Prediction Accuracy = 64.876%, Loss = 0.28416178822517396
Epoch: 8359, Batch Gradient Norm: 13.587243704531575
Epoch: 8359, Batch Gradient Norm after: 13.587243704531575
Epoch 8360/10000, Prediction Accuracy = 64.922%, Loss = 0.2885804772377014
Epoch: 8360, Batch Gradient Norm: 16.946705728577943
Epoch: 8360, Batch Gradient Norm after: 16.946705728577943
Epoch 8361/10000, Prediction Accuracy = 64.938%, Loss = 0.2897734522819519
Epoch: 8361, Batch Gradient Norm: 17.205644925153436
Epoch: 8361, Batch Gradient Norm after: 17.205644925153436
Epoch 8362/10000, Prediction Accuracy = 64.824%, Loss = 0.29055718779563905
Epoch: 8362, Batch Gradient Norm: 14.838498563832266
Epoch: 8362, Batch Gradient Norm after: 14.838498563832266
Epoch 8363/10000, Prediction Accuracy = 64.91199999999999%, Loss = 0.2884936809539795
Epoch: 8363, Batch Gradient Norm: 15.054001565161741
Epoch: 8363, Batch Gradient Norm after: 15.054001565161741
Epoch 8364/10000, Prediction Accuracy = 64.986%, Loss = 0.2884182810783386
Epoch: 8364, Batch Gradient Norm: 12.990706265896879
Epoch: 8364, Batch Gradient Norm after: 12.990706265896879
Epoch 8365/10000, Prediction Accuracy = 64.94200000000001%, Loss = 0.2862421989440918
Epoch: 8365, Batch Gradient Norm: 12.457854111042709
Epoch: 8365, Batch Gradient Norm after: 12.457854111042709
Epoch 8366/10000, Prediction Accuracy = 64.964%, Loss = 0.28599857687950136
Epoch: 8366, Batch Gradient Norm: 14.55166314825854
Epoch: 8366, Batch Gradient Norm after: 14.55166314825854
Epoch 8367/10000, Prediction Accuracy = 64.976%, Loss = 0.28760831356048583
Epoch: 8367, Batch Gradient Norm: 16.742205982642467
Epoch: 8367, Batch Gradient Norm after: 16.742205982642467
Epoch 8368/10000, Prediction Accuracy = 65.002%, Loss = 0.2920490324497223
Epoch: 8368, Batch Gradient Norm: 14.801642346555452
Epoch: 8368, Batch Gradient Norm after: 14.801642346555452
Epoch 8369/10000, Prediction Accuracy = 64.81%, Loss = 0.2892759442329407
Epoch: 8369, Batch Gradient Norm: 14.020546297385296
Epoch: 8369, Batch Gradient Norm after: 14.020546297385296
Epoch 8370/10000, Prediction Accuracy = 65.03599999999999%, Loss = 0.29118756055831907
Epoch: 8370, Batch Gradient Norm: 14.731030312092864
Epoch: 8370, Batch Gradient Norm after: 14.731030312092864
Epoch 8371/10000, Prediction Accuracy = 64.902%, Loss = 0.28861223459243773
Epoch: 8371, Batch Gradient Norm: 14.778987709058205
Epoch: 8371, Batch Gradient Norm after: 14.778987709058205
Epoch 8372/10000, Prediction Accuracy = 64.95599999999999%, Loss = 0.28973146677017214
Epoch: 8372, Batch Gradient Norm: 18.39646633841229
Epoch: 8372, Batch Gradient Norm after: 18.323057686464576
Epoch 8373/10000, Prediction Accuracy = 64.922%, Loss = 0.29489773511886597
Epoch: 8373, Batch Gradient Norm: 17.520751948459925
Epoch: 8373, Batch Gradient Norm after: 17.520751948459925
Epoch 8374/10000, Prediction Accuracy = 64.934%, Loss = 0.29343498945236207
Epoch: 8374, Batch Gradient Norm: 19.033821915502635
Epoch: 8374, Batch Gradient Norm after: 18.76767521438579
Epoch 8375/10000, Prediction Accuracy = 64.846%, Loss = 0.29441277384757997
Epoch: 8375, Batch Gradient Norm: 15.879310136522117
Epoch: 8375, Batch Gradient Norm after: 15.879310136522117
Epoch 8376/10000, Prediction Accuracy = 64.862%, Loss = 0.2891660749912262
Epoch: 8376, Batch Gradient Norm: 13.20984814341486
Epoch: 8376, Batch Gradient Norm after: 13.20984814341486
Epoch 8377/10000, Prediction Accuracy = 64.926%, Loss = 0.2857510566711426
Epoch: 8377, Batch Gradient Norm: 12.710851510729892
Epoch: 8377, Batch Gradient Norm after: 12.710851510729892
Epoch 8378/10000, Prediction Accuracy = 64.964%, Loss = 0.28592680096626283
Epoch: 8378, Batch Gradient Norm: 14.456258723949544
Epoch: 8378, Batch Gradient Norm after: 14.456258723949544
Epoch 8379/10000, Prediction Accuracy = 64.924%, Loss = 0.2890822649002075
Epoch: 8379, Batch Gradient Norm: 13.364647607475927
Epoch: 8379, Batch Gradient Norm after: 13.364647607475927
Epoch 8380/10000, Prediction Accuracy = 65.018%, Loss = 0.2851603627204895
Epoch: 8380, Batch Gradient Norm: 18.05733293570779
Epoch: 8380, Batch Gradient Norm after: 18.05733293570779
Epoch 8381/10000, Prediction Accuracy = 64.98599999999999%, Loss = 0.2930692732334137
Epoch: 8381, Batch Gradient Norm: 16.056475302490423
Epoch: 8381, Batch Gradient Norm after: 16.056475302490423
Epoch 8382/10000, Prediction Accuracy = 64.99%, Loss = 0.29109988212585447
Epoch: 8382, Batch Gradient Norm: 13.3624460600283
Epoch: 8382, Batch Gradient Norm after: 13.3624460600283
Epoch 8383/10000, Prediction Accuracy = 64.99199999999999%, Loss = 0.28569973111152647
Epoch: 8383, Batch Gradient Norm: 11.31066205775929
Epoch: 8383, Batch Gradient Norm after: 11.31066205775929
Epoch 8384/10000, Prediction Accuracy = 64.92%, Loss = 0.28232102394104003
Epoch: 8384, Batch Gradient Norm: 11.08804342587324
Epoch: 8384, Batch Gradient Norm after: 11.08804342587324
Epoch 8385/10000, Prediction Accuracy = 65.01%, Loss = 0.2824076533317566
Epoch: 8385, Batch Gradient Norm: 12.317120094898163
Epoch: 8385, Batch Gradient Norm after: 12.317120094898163
Epoch 8386/10000, Prediction Accuracy = 65.00000000000001%, Loss = 0.28322145342826843
Epoch: 8386, Batch Gradient Norm: 14.361120501015954
Epoch: 8386, Batch Gradient Norm after: 14.361120501015954
Epoch 8387/10000, Prediction Accuracy = 65.008%, Loss = 0.2864410042762756
Epoch: 8387, Batch Gradient Norm: 11.271581557593244
Epoch: 8387, Batch Gradient Norm after: 11.271581557593244
Epoch 8388/10000, Prediction Accuracy = 64.87%, Loss = 0.28639233112335205
Epoch: 8388, Batch Gradient Norm: 12.811408106196204
Epoch: 8388, Batch Gradient Norm after: 12.811408106196204
Epoch 8389/10000, Prediction Accuracy = 64.89000000000001%, Loss = 0.2863151848316193
Epoch: 8389, Batch Gradient Norm: 12.618444018070448
Epoch: 8389, Batch Gradient Norm after: 12.618444018070448
Epoch 8390/10000, Prediction Accuracy = 65.04400000000001%, Loss = 0.28383672833442686
Epoch: 8390, Batch Gradient Norm: 14.445679506908236
Epoch: 8390, Batch Gradient Norm after: 14.445679506908236
Epoch 8391/10000, Prediction Accuracy = 64.902%, Loss = 0.2862407624721527
Epoch: 8391, Batch Gradient Norm: 15.941875718163894
Epoch: 8391, Batch Gradient Norm after: 15.941875718163894
Epoch 8392/10000, Prediction Accuracy = 65.242%, Loss = 0.2872195065021515
Epoch: 8392, Batch Gradient Norm: 15.28354200182537
Epoch: 8392, Batch Gradient Norm after: 15.28354200182537
Epoch 8393/10000, Prediction Accuracy = 64.90400000000001%, Loss = 0.28805622458457947
Epoch: 8393, Batch Gradient Norm: 14.295343827269965
Epoch: 8393, Batch Gradient Norm after: 14.295343827269965
Epoch 8394/10000, Prediction Accuracy = 64.958%, Loss = 0.2879435956478119
Epoch: 8394, Batch Gradient Norm: 14.484641167536147
Epoch: 8394, Batch Gradient Norm after: 14.484641167536147
Epoch 8395/10000, Prediction Accuracy = 64.964%, Loss = 0.2879509210586548
Epoch: 8395, Batch Gradient Norm: 13.262339442908013
Epoch: 8395, Batch Gradient Norm after: 13.262339442908013
Epoch 8396/10000, Prediction Accuracy = 64.97%, Loss = 0.2851892113685608
Epoch: 8396, Batch Gradient Norm: 13.751770777203356
Epoch: 8396, Batch Gradient Norm after: 13.751770777203356
Epoch 8397/10000, Prediction Accuracy = 64.986%, Loss = 0.2859521985054016
Epoch: 8397, Batch Gradient Norm: 13.485970884317405
Epoch: 8397, Batch Gradient Norm after: 13.485970884317405
Epoch 8398/10000, Prediction Accuracy = 64.91%, Loss = 0.28630931973457335
Epoch: 8398, Batch Gradient Norm: 15.132883575431146
Epoch: 8398, Batch Gradient Norm after: 15.132883575431146
Epoch 8399/10000, Prediction Accuracy = 64.89000000000001%, Loss = 0.29021493792533876
Epoch: 8399, Batch Gradient Norm: 15.164028878021352
Epoch: 8399, Batch Gradient Norm after: 15.164028878021352
Epoch 8400/10000, Prediction Accuracy = 64.972%, Loss = 0.28976776599884035
Epoch: 8400, Batch Gradient Norm: 15.329181577609079
Epoch: 8400, Batch Gradient Norm after: 15.329181577609079
Epoch 8401/10000, Prediction Accuracy = 64.994%, Loss = 0.2872228503227234
Epoch: 8401, Batch Gradient Norm: 15.030980370759856
Epoch: 8401, Batch Gradient Norm after: 15.030980370759856
Epoch 8402/10000, Prediction Accuracy = 64.97999999999999%, Loss = 0.289366626739502
Epoch: 8402, Batch Gradient Norm: 14.365294821702543
Epoch: 8402, Batch Gradient Norm after: 14.365294821702543
Epoch 8403/10000, Prediction Accuracy = 64.94999999999999%, Loss = 0.2856239080429077
Epoch: 8403, Batch Gradient Norm: 14.07910279393162
Epoch: 8403, Batch Gradient Norm after: 14.07910279393162
Epoch 8404/10000, Prediction Accuracy = 65.002%, Loss = 0.28749072551727295
Epoch: 8404, Batch Gradient Norm: 14.041291816397802
Epoch: 8404, Batch Gradient Norm after: 14.041291816397802
Epoch 8405/10000, Prediction Accuracy = 64.66999999999999%, Loss = 0.2906777739524841
Epoch: 8405, Batch Gradient Norm: 13.222084921152314
Epoch: 8405, Batch Gradient Norm after: 13.222084921152314
Epoch 8406/10000, Prediction Accuracy = 64.88%, Loss = 0.29051029682159424
Epoch: 8406, Batch Gradient Norm: 15.925518541839589
Epoch: 8406, Batch Gradient Norm after: 15.925518541839589
Epoch 8407/10000, Prediction Accuracy = 64.886%, Loss = 0.28866214752197267
Epoch: 8407, Batch Gradient Norm: 15.962636963211919
Epoch: 8407, Batch Gradient Norm after: 15.962636963211919
Epoch 8408/10000, Prediction Accuracy = 64.934%, Loss = 0.28969683647155764
Epoch: 8408, Batch Gradient Norm: 18.65587698023217
Epoch: 8408, Batch Gradient Norm after: 18.65587698023217
Epoch 8409/10000, Prediction Accuracy = 64.96799999999999%, Loss = 0.2924609661102295
Epoch: 8409, Batch Gradient Norm: 15.43718187760501
Epoch: 8409, Batch Gradient Norm after: 15.43718187760501
Epoch 8410/10000, Prediction Accuracy = 65.07000000000001%, Loss = 0.28813138604164124
Epoch: 8410, Batch Gradient Norm: 14.866753751251355
Epoch: 8410, Batch Gradient Norm after: 14.866753751251355
Epoch 8411/10000, Prediction Accuracy = 64.89399999999999%, Loss = 0.28744022250175477
Epoch: 8411, Batch Gradient Norm: 11.082685587951662
Epoch: 8411, Batch Gradient Norm after: 11.082685587951662
Epoch 8412/10000, Prediction Accuracy = 64.91%, Loss = 0.2830879271030426
Epoch: 8412, Batch Gradient Norm: 10.929532415749904
Epoch: 8412, Batch Gradient Norm after: 10.929532415749904
Epoch 8413/10000, Prediction Accuracy = 64.852%, Loss = 0.2838965475559235
Epoch: 8413, Batch Gradient Norm: 14.938835152170823
Epoch: 8413, Batch Gradient Norm after: 14.938835152170823
Epoch 8414/10000, Prediction Accuracy = 64.996%, Loss = 0.29212177991867067
Epoch: 8414, Batch Gradient Norm: 12.387105589668883
Epoch: 8414, Batch Gradient Norm after: 12.387105589668883
Epoch 8415/10000, Prediction Accuracy = 64.852%, Loss = 0.2848378121852875
Epoch: 8415, Batch Gradient Norm: 12.909570121161783
Epoch: 8415, Batch Gradient Norm after: 12.909570121161783
Epoch 8416/10000, Prediction Accuracy = 65.036%, Loss = 0.28516151905059817
Epoch: 8416, Batch Gradient Norm: 12.626320807322935
Epoch: 8416, Batch Gradient Norm after: 12.626320807322935
Epoch 8417/10000, Prediction Accuracy = 65.152%, Loss = 0.2838031411170959
Epoch: 8417, Batch Gradient Norm: 13.27650802147639
Epoch: 8417, Batch Gradient Norm after: 13.27650802147639
Epoch 8418/10000, Prediction Accuracy = 64.95599999999999%, Loss = 0.2890981435775757
Epoch: 8418, Batch Gradient Norm: 14.641255024406956
Epoch: 8418, Batch Gradient Norm after: 14.641255024406956
Epoch 8419/10000, Prediction Accuracy = 64.97200000000001%, Loss = 0.28607524037361143
Epoch: 8419, Batch Gradient Norm: 17.11435961167972
Epoch: 8419, Batch Gradient Norm after: 17.11435961167972
Epoch 8420/10000, Prediction Accuracy = 64.89%, Loss = 0.29096202850341796
Epoch: 8420, Batch Gradient Norm: 16.21803546995277
Epoch: 8420, Batch Gradient Norm after: 16.21803546995277
Epoch 8421/10000, Prediction Accuracy = 64.75%, Loss = 0.28993496894836424
Epoch: 8421, Batch Gradient Norm: 16.815181220754987
Epoch: 8421, Batch Gradient Norm after: 16.815181220754987
Epoch 8422/10000, Prediction Accuracy = 64.93599999999999%, Loss = 0.2918181300163269
Epoch: 8422, Batch Gradient Norm: 15.550918881639575
Epoch: 8422, Batch Gradient Norm after: 15.550918881639575
Epoch 8423/10000, Prediction Accuracy = 65.002%, Loss = 0.2904519259929657
Epoch: 8423, Batch Gradient Norm: 17.23319127204406
Epoch: 8423, Batch Gradient Norm after: 17.23319127204406
Epoch 8424/10000, Prediction Accuracy = 65.014%, Loss = 0.29128903746604917
Epoch: 8424, Batch Gradient Norm: 17.618386211699473
Epoch: 8424, Batch Gradient Norm after: 17.618386211699473
Epoch 8425/10000, Prediction Accuracy = 64.976%, Loss = 0.2903922498226166
Epoch: 8425, Batch Gradient Norm: 17.037959361072875
Epoch: 8425, Batch Gradient Norm after: 17.037959361072875
Epoch 8426/10000, Prediction Accuracy = 64.962%, Loss = 0.29081188440322875
Epoch: 8426, Batch Gradient Norm: 14.788517642612169
Epoch: 8426, Batch Gradient Norm after: 14.788517642612169
Epoch 8427/10000, Prediction Accuracy = 64.84%, Loss = 0.29215468764305114
Epoch: 8427, Batch Gradient Norm: 14.407041756798641
Epoch: 8427, Batch Gradient Norm after: 14.407041756798641
Epoch 8428/10000, Prediction Accuracy = 64.954%, Loss = 0.28746782541275023
Epoch: 8428, Batch Gradient Norm: 14.56749612237499
Epoch: 8428, Batch Gradient Norm after: 14.56749612237499
Epoch 8429/10000, Prediction Accuracy = 64.896%, Loss = 0.29009140133857725
Epoch: 8429, Batch Gradient Norm: 16.38611906576843
Epoch: 8429, Batch Gradient Norm after: 16.38611906576843
Epoch 8430/10000, Prediction Accuracy = 64.748%, Loss = 0.2923323571681976
Epoch: 8430, Batch Gradient Norm: 19.884757933650068
Epoch: 8430, Batch Gradient Norm after: 19.621199877417116
Epoch 8431/10000, Prediction Accuracy = 64.992%, Loss = 0.2960219979286194
Epoch: 8431, Batch Gradient Norm: 20.296562847428206
Epoch: 8431, Batch Gradient Norm after: 18.284042315742752
Epoch 8432/10000, Prediction Accuracy = 64.85%, Loss = 0.29953774213790896
Epoch: 8432, Batch Gradient Norm: 20.15921857245221
Epoch: 8432, Batch Gradient Norm after: 19.297380861928794
Epoch 8433/10000, Prediction Accuracy = 64.84%, Loss = 0.2980757176876068
Epoch: 8433, Batch Gradient Norm: 15.896679253679952
Epoch: 8433, Batch Gradient Norm after: 15.896679253679952
Epoch 8434/10000, Prediction Accuracy = 64.938%, Loss = 0.2902842700481415
Epoch: 8434, Batch Gradient Norm: 12.804252990692266
Epoch: 8434, Batch Gradient Norm after: 12.804252990692266
Epoch 8435/10000, Prediction Accuracy = 64.96400000000001%, Loss = 0.2901473045349121
Epoch: 8435, Batch Gradient Norm: 12.417819167577933
Epoch: 8435, Batch Gradient Norm after: 12.417819167577933
Epoch 8436/10000, Prediction Accuracy = 64.8%, Loss = 0.2873213291168213
Epoch: 8436, Batch Gradient Norm: 11.751180757133532
Epoch: 8436, Batch Gradient Norm after: 11.751180757133532
Epoch 8437/10000, Prediction Accuracy = 65.09400000000001%, Loss = 0.2823730528354645
Epoch: 8437, Batch Gradient Norm: 13.45093878030745
Epoch: 8437, Batch Gradient Norm after: 13.45093878030745
Epoch 8438/10000, Prediction Accuracy = 64.932%, Loss = 0.28527380228042604
Epoch: 8438, Batch Gradient Norm: 14.03238451684145
Epoch: 8438, Batch Gradient Norm after: 14.03238451684145
Epoch 8439/10000, Prediction Accuracy = 65.054%, Loss = 0.2866134703159332
Epoch: 8439, Batch Gradient Norm: 17.742085059299
Epoch: 8439, Batch Gradient Norm after: 17.31937567012355
Epoch 8440/10000, Prediction Accuracy = 64.912%, Loss = 0.29314419627189636
Epoch: 8440, Batch Gradient Norm: 18.597533790583473
Epoch: 8440, Batch Gradient Norm after: 18.597533790583473
Epoch 8441/10000, Prediction Accuracy = 64.94200000000001%, Loss = 0.29491845369338987
Epoch: 8441, Batch Gradient Norm: 17.399764538117523
Epoch: 8441, Batch Gradient Norm after: 17.399764538117523
Epoch 8442/10000, Prediction Accuracy = 65.058%, Loss = 0.2893721342086792
Epoch: 8442, Batch Gradient Norm: 17.659108675411805
Epoch: 8442, Batch Gradient Norm after: 17.659108675411805
Epoch 8443/10000, Prediction Accuracy = 64.936%, Loss = 0.28897937536239626
Epoch: 8443, Batch Gradient Norm: 13.694621319648025
Epoch: 8443, Batch Gradient Norm after: 13.694621319648025
Epoch 8444/10000, Prediction Accuracy = 64.89000000000001%, Loss = 0.2845837950706482
Epoch: 8444, Batch Gradient Norm: 15.614628296933105
Epoch: 8444, Batch Gradient Norm after: 15.614628296933105
Epoch 8445/10000, Prediction Accuracy = 64.90599999999999%, Loss = 0.28782097101211546
Epoch: 8445, Batch Gradient Norm: 14.068026053923221
Epoch: 8445, Batch Gradient Norm after: 14.068026053923221
Epoch 8446/10000, Prediction Accuracy = 65.006%, Loss = 0.28527352809906004
Epoch: 8446, Batch Gradient Norm: 15.508655661256197
Epoch: 8446, Batch Gradient Norm after: 15.508655661256197
Epoch 8447/10000, Prediction Accuracy = 65.12200000000001%, Loss = 0.286991149187088
Epoch: 8447, Batch Gradient Norm: 12.783236754769131
Epoch: 8447, Batch Gradient Norm after: 12.783236754769131
Epoch 8448/10000, Prediction Accuracy = 65.16799999999999%, Loss = 0.28383834958076476
Epoch: 8448, Batch Gradient Norm: 11.843888367774822
Epoch: 8448, Batch Gradient Norm after: 11.843888367774822
Epoch 8449/10000, Prediction Accuracy = 64.896%, Loss = 0.28412787318229676
Epoch: 8449, Batch Gradient Norm: 12.139413159031225
Epoch: 8449, Batch Gradient Norm after: 12.139413159031225
Epoch 8450/10000, Prediction Accuracy = 65.03599999999999%, Loss = 0.2833298921585083
Epoch: 8450, Batch Gradient Norm: 12.535274254605064
Epoch: 8450, Batch Gradient Norm after: 12.535274254605064
Epoch 8451/10000, Prediction Accuracy = 65.132%, Loss = 0.28407939672470095
Epoch: 8451, Batch Gradient Norm: 11.543772468236824
Epoch: 8451, Batch Gradient Norm after: 11.543772468236824
Epoch 8452/10000, Prediction Accuracy = 65.08%, Loss = 0.28445231914520264
Epoch: 8452, Batch Gradient Norm: 13.47687634185861
Epoch: 8452, Batch Gradient Norm after: 13.47687634185861
Epoch 8453/10000, Prediction Accuracy = 64.974%, Loss = 0.2870701730251312
Epoch: 8453, Batch Gradient Norm: 11.571849136513148
Epoch: 8453, Batch Gradient Norm after: 11.571849136513148
Epoch 8454/10000, Prediction Accuracy = 65.04599999999999%, Loss = 0.2821003258228302
Epoch: 8454, Batch Gradient Norm: 16.843403638483196
Epoch: 8454, Batch Gradient Norm after: 16.843403638483196
Epoch 8455/10000, Prediction Accuracy = 64.86%, Loss = 0.28938424587249756
Epoch: 8455, Batch Gradient Norm: 16.356868584731906
Epoch: 8455, Batch Gradient Norm after: 16.356868584731906
Epoch 8456/10000, Prediction Accuracy = 65.05600000000001%, Loss = 0.28750919699668886
Epoch: 8456, Batch Gradient Norm: 13.30008534140683
Epoch: 8456, Batch Gradient Norm after: 13.30008534140683
Epoch 8457/10000, Prediction Accuracy = 64.792%, Loss = 0.28881064653396604
Epoch: 8457, Batch Gradient Norm: 11.944983435856036
Epoch: 8457, Batch Gradient Norm after: 11.944983435856036
Epoch 8458/10000, Prediction Accuracy = 64.9%, Loss = 0.28686975240707396
Epoch: 8458, Batch Gradient Norm: 14.361386147579458
Epoch: 8458, Batch Gradient Norm after: 14.361386147579458
Epoch 8459/10000, Prediction Accuracy = 64.862%, Loss = 0.28633785247802734
Epoch: 8459, Batch Gradient Norm: 16.42711836007703
Epoch: 8459, Batch Gradient Norm after: 16.42711836007703
Epoch 8460/10000, Prediction Accuracy = 64.978%, Loss = 0.28940644264221194
Epoch: 8460, Batch Gradient Norm: 16.215012721395045
Epoch: 8460, Batch Gradient Norm after: 16.215012721395045
Epoch 8461/10000, Prediction Accuracy = 65.13600000000001%, Loss = 0.2874071538448334
Epoch: 8461, Batch Gradient Norm: 17.507674297147307
Epoch: 8461, Batch Gradient Norm after: 17.507674297147307
Epoch 8462/10000, Prediction Accuracy = 64.926%, Loss = 0.29056215286254883
Epoch: 8462, Batch Gradient Norm: 16.415308240396335
Epoch: 8462, Batch Gradient Norm after: 16.184075938181763
Epoch 8463/10000, Prediction Accuracy = 65.14200000000001%, Loss = 0.29055830240249636
Epoch: 8463, Batch Gradient Norm: 14.097682801606629
Epoch: 8463, Batch Gradient Norm after: 14.097682801606629
Epoch 8464/10000, Prediction Accuracy = 65.14000000000001%, Loss = 0.28717575669288636
Epoch: 8464, Batch Gradient Norm: 13.141233439432165
Epoch: 8464, Batch Gradient Norm after: 13.141233439432165
Epoch 8465/10000, Prediction Accuracy = 65.126%, Loss = 0.28478339314460754
Epoch: 8465, Batch Gradient Norm: 12.07341908416454
Epoch: 8465, Batch Gradient Norm after: 12.07341908416454
Epoch 8466/10000, Prediction Accuracy = 64.982%, Loss = 0.28292577266693114
Epoch: 8466, Batch Gradient Norm: 13.149802776970098
Epoch: 8466, Batch Gradient Norm after: 13.149802776970098
Epoch 8467/10000, Prediction Accuracy = 65.08%, Loss = 0.2838315963745117
Epoch: 8467, Batch Gradient Norm: 14.297875135756481
Epoch: 8467, Batch Gradient Norm after: 14.297875135756481
Epoch 8468/10000, Prediction Accuracy = 65.06800000000001%, Loss = 0.2878772258758545
Epoch: 8468, Batch Gradient Norm: 16.930022848949985
Epoch: 8468, Batch Gradient Norm after: 16.930022848949985
Epoch 8469/10000, Prediction Accuracy = 65.082%, Loss = 0.28970409631729127
Epoch: 8469, Batch Gradient Norm: 18.6490455669187
Epoch: 8469, Batch Gradient Norm after: 18.607066545652877
Epoch 8470/10000, Prediction Accuracy = 65.01%, Loss = 0.29327550530433655
Epoch: 8470, Batch Gradient Norm: 15.80517482278479
Epoch: 8470, Batch Gradient Norm after: 15.80517482278479
Epoch 8471/10000, Prediction Accuracy = 64.91999999999999%, Loss = 0.288439017534256
Epoch: 8471, Batch Gradient Norm: 16.725808794708886
Epoch: 8471, Batch Gradient Norm after: 16.725808794708886
Epoch 8472/10000, Prediction Accuracy = 65.034%, Loss = 0.29053595662117004
Epoch: 8472, Batch Gradient Norm: 17.664713921410797
Epoch: 8472, Batch Gradient Norm after: 17.664713921410797
Epoch 8473/10000, Prediction Accuracy = 65.09200000000001%, Loss = 0.2910321235656738
Epoch: 8473, Batch Gradient Norm: 17.325589100552392
Epoch: 8473, Batch Gradient Norm after: 17.325589100552392
Epoch 8474/10000, Prediction Accuracy = 64.924%, Loss = 0.29516834020614624
Epoch: 8474, Batch Gradient Norm: 16.44554506692978
Epoch: 8474, Batch Gradient Norm after: 16.44554506692978
Epoch 8475/10000, Prediction Accuracy = 64.918%, Loss = 0.28963698744773864
Epoch: 8475, Batch Gradient Norm: 13.170800997068042
Epoch: 8475, Batch Gradient Norm after: 13.170800997068042
Epoch 8476/10000, Prediction Accuracy = 64.99199999999999%, Loss = 0.2848232567310333
Epoch: 8476, Batch Gradient Norm: 15.131177242244533
Epoch: 8476, Batch Gradient Norm after: 15.131177242244533
Epoch 8477/10000, Prediction Accuracy = 64.942%, Loss = 0.2880139946937561
Epoch: 8477, Batch Gradient Norm: 14.061285538872024
Epoch: 8477, Batch Gradient Norm after: 14.061285538872024
Epoch 8478/10000, Prediction Accuracy = 64.90800000000002%, Loss = 0.2860520124435425
Epoch: 8478, Batch Gradient Norm: 13.23944520243128
Epoch: 8478, Batch Gradient Norm after: 13.23944520243128
Epoch 8479/10000, Prediction Accuracy = 64.964%, Loss = 0.28696449398994445
Epoch: 8479, Batch Gradient Norm: 13.438164341827349
Epoch: 8479, Batch Gradient Norm after: 13.438164341827349
Epoch 8480/10000, Prediction Accuracy = 64.998%, Loss = 0.2859214603900909
Epoch: 8480, Batch Gradient Norm: 16.896236539973167
Epoch: 8480, Batch Gradient Norm after: 16.896236539973167
Epoch 8481/10000, Prediction Accuracy = 65.166%, Loss = 0.2903769314289093
Epoch: 8481, Batch Gradient Norm: 17.14041625075844
Epoch: 8481, Batch Gradient Norm after: 17.14041625075844
Epoch 8482/10000, Prediction Accuracy = 64.92999999999999%, Loss = 0.2881740152835846
Epoch: 8482, Batch Gradient Norm: 16.185995972339317
Epoch: 8482, Batch Gradient Norm after: 16.185995972339317
Epoch 8483/10000, Prediction Accuracy = 64.99%, Loss = 0.28961063623428346
Epoch: 8483, Batch Gradient Norm: 18.476851636414274
Epoch: 8483, Batch Gradient Norm after: 17.858489105840963
Epoch 8484/10000, Prediction Accuracy = 65.078%, Loss = 0.29148282408714293
Epoch: 8484, Batch Gradient Norm: 17.219976482617863
Epoch: 8484, Batch Gradient Norm after: 17.219976482617863
Epoch 8485/10000, Prediction Accuracy = 65.032%, Loss = 0.2891960799694061
Epoch: 8485, Batch Gradient Norm: 15.436639569494192
Epoch: 8485, Batch Gradient Norm after: 15.436639569494192
Epoch 8486/10000, Prediction Accuracy = 64.874%, Loss = 0.28747721314430236
Epoch: 8486, Batch Gradient Norm: 18.98227599970499
Epoch: 8486, Batch Gradient Norm after: 18.98227599970499
Epoch 8487/10000, Prediction Accuracy = 64.85600000000001%, Loss = 0.29232717752456666
Epoch: 8487, Batch Gradient Norm: 21.38227220655637
Epoch: 8487, Batch Gradient Norm after: 21.003899558551304
Epoch 8488/10000, Prediction Accuracy = 64.932%, Loss = 0.29689571261405945
Epoch: 8488, Batch Gradient Norm: 19.142548518306842
Epoch: 8488, Batch Gradient Norm after: 18.526821653059976
Epoch 8489/10000, Prediction Accuracy = 64.994%, Loss = 0.29419047832489015
Epoch: 8489, Batch Gradient Norm: 19.59305870745474
Epoch: 8489, Batch Gradient Norm after: 18.665148444469953
Epoch 8490/10000, Prediction Accuracy = 65.076%, Loss = 0.29274486303329467
Epoch: 8490, Batch Gradient Norm: 19.011692598958806
Epoch: 8490, Batch Gradient Norm after: 19.011692598958806
Epoch 8491/10000, Prediction Accuracy = 65.08200000000001%, Loss = 0.29468340873718263
Epoch: 8491, Batch Gradient Norm: 15.519615415873126
Epoch: 8491, Batch Gradient Norm after: 15.519615415873126
Epoch 8492/10000, Prediction Accuracy = 64.81400000000001%, Loss = 0.2914781212806702
Epoch: 8492, Batch Gradient Norm: 14.950602119277493
Epoch: 8492, Batch Gradient Norm after: 14.950602119277493
Epoch 8493/10000, Prediction Accuracy = 64.88600000000001%, Loss = 0.28709514141082765
Epoch: 8493, Batch Gradient Norm: 15.810890673166366
Epoch: 8493, Batch Gradient Norm after: 15.810890673166366
Epoch 8494/10000, Prediction Accuracy = 64.95200000000001%, Loss = 0.28905473947525023
Epoch: 8494, Batch Gradient Norm: 15.015590635437865
Epoch: 8494, Batch Gradient Norm after: 15.015590635437865
Epoch 8495/10000, Prediction Accuracy = 65.02600000000001%, Loss = 0.2887668967247009
Epoch: 8495, Batch Gradient Norm: 15.853857975844372
Epoch: 8495, Batch Gradient Norm after: 15.853857975844372
Epoch 8496/10000, Prediction Accuracy = 65.042%, Loss = 0.29149343967437746
Epoch: 8496, Batch Gradient Norm: 17.45311370849474
Epoch: 8496, Batch Gradient Norm after: 17.45311370849474
Epoch 8497/10000, Prediction Accuracy = 64.95400000000001%, Loss = 0.29153640270233155
Epoch: 8497, Batch Gradient Norm: 18.492007625522877
Epoch: 8497, Batch Gradient Norm after: 18.492007625522877
Epoch 8498/10000, Prediction Accuracy = 64.96400000000001%, Loss = 0.29034581780433655
Epoch: 8498, Batch Gradient Norm: 20.233613378611278
Epoch: 8498, Batch Gradient Norm after: 20.17620281935919
Epoch 8499/10000, Prediction Accuracy = 64.886%, Loss = 0.2972383499145508
Epoch: 8499, Batch Gradient Norm: 14.452576285902431
Epoch: 8499, Batch Gradient Norm after: 14.452576285902431
Epoch 8500/10000, Prediction Accuracy = 65.15200000000002%, Loss = 0.28633110523223876
Epoch: 8500, Batch Gradient Norm: 16.005806148022053
Epoch: 8500, Batch Gradient Norm after: 16.005806148022053
Epoch 8501/10000, Prediction Accuracy = 64.958%, Loss = 0.28780643343925477
Epoch: 8501, Batch Gradient Norm: 16.983665205747073
Epoch: 8501, Batch Gradient Norm after: 16.983665205747073
Epoch 8502/10000, Prediction Accuracy = 64.80199999999999%, Loss = 0.2949430286884308
Epoch: 8502, Batch Gradient Norm: 18.644994955538742
Epoch: 8502, Batch Gradient Norm after: 18.340769095863916
Epoch 8503/10000, Prediction Accuracy = 64.926%, Loss = 0.29417407512664795
Epoch: 8503, Batch Gradient Norm: 16.58871861565081
Epoch: 8503, Batch Gradient Norm after: 16.58871861565081
Epoch 8504/10000, Prediction Accuracy = 65.05000000000001%, Loss = 0.2873607695102692
Epoch: 8504, Batch Gradient Norm: 13.290341741013693
Epoch: 8504, Batch Gradient Norm after: 13.290341741013693
Epoch 8505/10000, Prediction Accuracy = 65.064%, Loss = 0.2847540259361267
Epoch: 8505, Batch Gradient Norm: 13.712317545855631
Epoch: 8505, Batch Gradient Norm after: 13.712317545855631
Epoch 8506/10000, Prediction Accuracy = 64.90200000000002%, Loss = 0.2848536312580109
Epoch: 8506, Batch Gradient Norm: 14.239186548081845
Epoch: 8506, Batch Gradient Norm after: 14.239186548081845
Epoch 8507/10000, Prediction Accuracy = 64.97999999999999%, Loss = 0.28684484362602236
Epoch: 8507, Batch Gradient Norm: 12.795517951827335
Epoch: 8507, Batch Gradient Norm after: 12.795517951827335
Epoch 8508/10000, Prediction Accuracy = 64.97999999999999%, Loss = 0.2845721185207367
Epoch: 8508, Batch Gradient Norm: 10.928645545770042
Epoch: 8508, Batch Gradient Norm after: 10.928645545770042
Epoch 8509/10000, Prediction Accuracy = 65.00999999999999%, Loss = 0.28314117193222044
Epoch: 8509, Batch Gradient Norm: 12.825110910090572
Epoch: 8509, Batch Gradient Norm after: 12.825110910090572
Epoch 8510/10000, Prediction Accuracy = 65.016%, Loss = 0.2843594551086426
Epoch: 8510, Batch Gradient Norm: 13.2920414792233
Epoch: 8510, Batch Gradient Norm after: 13.2920414792233
Epoch 8511/10000, Prediction Accuracy = 65.024%, Loss = 0.2858115255832672
Epoch: 8511, Batch Gradient Norm: 13.769899783482352
Epoch: 8511, Batch Gradient Norm after: 13.769899783482352
Epoch 8512/10000, Prediction Accuracy = 65.048%, Loss = 0.2836011409759521
Epoch: 8512, Batch Gradient Norm: 15.53442894696239
Epoch: 8512, Batch Gradient Norm after: 15.53442894696239
Epoch 8513/10000, Prediction Accuracy = 64.92800000000001%, Loss = 0.2890990853309631
Epoch: 8513, Batch Gradient Norm: 16.849337791464592
Epoch: 8513, Batch Gradient Norm after: 16.849337791464592
Epoch 8514/10000, Prediction Accuracy = 64.996%, Loss = 0.29042922258377074
Epoch: 8514, Batch Gradient Norm: 14.960035040227798
Epoch: 8514, Batch Gradient Norm after: 14.960035040227798
Epoch 8515/10000, Prediction Accuracy = 65.046%, Loss = 0.2860850393772125
Epoch: 8515, Batch Gradient Norm: 15.951417429589839
Epoch: 8515, Batch Gradient Norm after: 15.951417429589839
Epoch 8516/10000, Prediction Accuracy = 64.93199999999999%, Loss = 0.2882071614265442
Epoch: 8516, Batch Gradient Norm: 14.692828099892614
Epoch: 8516, Batch Gradient Norm after: 14.692828099892614
Epoch 8517/10000, Prediction Accuracy = 65.028%, Loss = 0.2849609911441803
Epoch: 8517, Batch Gradient Norm: 14.848323657745317
Epoch: 8517, Batch Gradient Norm after: 14.848323657745317
Epoch 8518/10000, Prediction Accuracy = 65.052%, Loss = 0.2883639752864838
Epoch: 8518, Batch Gradient Norm: 14.293617892253755
Epoch: 8518, Batch Gradient Norm after: 14.293617892253755
Epoch 8519/10000, Prediction Accuracy = 65.03%, Loss = 0.28741263747215273
Epoch: 8519, Batch Gradient Norm: 13.910342406257714
Epoch: 8519, Batch Gradient Norm after: 13.910342406257714
Epoch 8520/10000, Prediction Accuracy = 64.992%, Loss = 0.2841191828250885
Epoch: 8520, Batch Gradient Norm: 14.145929995773805
Epoch: 8520, Batch Gradient Norm after: 14.145929995773805
Epoch 8521/10000, Prediction Accuracy = 65.078%, Loss = 0.28293816447257997
Epoch: 8521, Batch Gradient Norm: 14.673912569679125
Epoch: 8521, Batch Gradient Norm after: 14.673912569679125
Epoch 8522/10000, Prediction Accuracy = 65.104%, Loss = 0.2850224137306213
Epoch: 8522, Batch Gradient Norm: 15.093234124179368
Epoch: 8522, Batch Gradient Norm after: 15.093234124179368
Epoch 8523/10000, Prediction Accuracy = 64.89399999999999%, Loss = 0.2854256868362427
Epoch: 8523, Batch Gradient Norm: 18.33225841709933
Epoch: 8523, Batch Gradient Norm after: 17.41072695625799
Epoch 8524/10000, Prediction Accuracy = 64.888%, Loss = 0.29177549481391907
Epoch: 8524, Batch Gradient Norm: 22.40796840454171
Epoch: 8524, Batch Gradient Norm after: 20.15054003978529
Epoch 8525/10000, Prediction Accuracy = 64.866%, Loss = 0.30050535798072814
Epoch: 8525, Batch Gradient Norm: 17.461608215477128
Epoch: 8525, Batch Gradient Norm after: 17.461608215477128
Epoch 8526/10000, Prediction Accuracy = 64.956%, Loss = 0.2911031663417816
Epoch: 8526, Batch Gradient Norm: 15.39710984430342
Epoch: 8526, Batch Gradient Norm after: 15.39710984430342
Epoch 8527/10000, Prediction Accuracy = 64.96400000000001%, Loss = 0.28944777250289916
Epoch: 8527, Batch Gradient Norm: 14.25967926060761
Epoch: 8527, Batch Gradient Norm after: 14.25967926060761
Epoch 8528/10000, Prediction Accuracy = 64.886%, Loss = 0.28651407957077024
Epoch: 8528, Batch Gradient Norm: 13.207351108827611
Epoch: 8528, Batch Gradient Norm after: 13.207351108827611
Epoch 8529/10000, Prediction Accuracy = 64.94800000000001%, Loss = 0.28681864142417907
Epoch: 8529, Batch Gradient Norm: 15.731715467759248
Epoch: 8529, Batch Gradient Norm after: 15.731715467759248
Epoch 8530/10000, Prediction Accuracy = 65.04799999999999%, Loss = 0.28551815152168275
Epoch: 8530, Batch Gradient Norm: 16.451667411655478
Epoch: 8530, Batch Gradient Norm after: 16.30507600863373
Epoch 8531/10000, Prediction Accuracy = 65.05999999999999%, Loss = 0.28644859790802
Epoch: 8531, Batch Gradient Norm: 12.55491650307628
Epoch: 8531, Batch Gradient Norm after: 12.55491650307628
Epoch 8532/10000, Prediction Accuracy = 64.952%, Loss = 0.2832594752311707
Epoch: 8532, Batch Gradient Norm: 11.853591285960437
Epoch: 8532, Batch Gradient Norm after: 11.853591285960437
Epoch 8533/10000, Prediction Accuracy = 64.934%, Loss = 0.2837352991104126
Epoch: 8533, Batch Gradient Norm: 15.801303506840075
Epoch: 8533, Batch Gradient Norm after: 15.801303506840075
Epoch 8534/10000, Prediction Accuracy = 65.016%, Loss = 0.28760884404182435
Epoch: 8534, Batch Gradient Norm: 16.51423259204895
Epoch: 8534, Batch Gradient Norm after: 16.51423259204895
Epoch 8535/10000, Prediction Accuracy = 65.126%, Loss = 0.28740690350532533
Epoch: 8535, Batch Gradient Norm: 15.580389555957614
Epoch: 8535, Batch Gradient Norm after: 15.580389555957614
Epoch 8536/10000, Prediction Accuracy = 64.962%, Loss = 0.28556494116783143
Epoch: 8536, Batch Gradient Norm: 17.12193256624211
Epoch: 8536, Batch Gradient Norm after: 17.12193256624211
Epoch 8537/10000, Prediction Accuracy = 64.916%, Loss = 0.2887167513370514
Epoch: 8537, Batch Gradient Norm: 13.3685561888851
Epoch: 8537, Batch Gradient Norm after: 13.3685561888851
Epoch 8538/10000, Prediction Accuracy = 64.872%, Loss = 0.2875185370445251
Epoch: 8538, Batch Gradient Norm: 17.775819379518264
Epoch: 8538, Batch Gradient Norm after: 17.775819379518264
Epoch 8539/10000, Prediction Accuracy = 65.114%, Loss = 0.2874020397663116
Epoch: 8539, Batch Gradient Norm: 18.446385099011547
Epoch: 8539, Batch Gradient Norm after: 18.446385099011547
Epoch 8540/10000, Prediction Accuracy = 65.022%, Loss = 0.28974621295928954
Epoch: 8540, Batch Gradient Norm: 18.48346493347161
Epoch: 8540, Batch Gradient Norm after: 18.48346493347161
Epoch 8541/10000, Prediction Accuracy = 64.856%, Loss = 0.29200044870376585
Epoch: 8541, Batch Gradient Norm: 16.489394619610703
Epoch: 8541, Batch Gradient Norm after: 16.489394619610703
Epoch 8542/10000, Prediction Accuracy = 65.09%, Loss = 0.28816230297088624
Epoch: 8542, Batch Gradient Norm: 13.955173582247374
Epoch: 8542, Batch Gradient Norm after: 13.955173582247374
Epoch 8543/10000, Prediction Accuracy = 65.01%, Loss = 0.2844762086868286
Epoch: 8543, Batch Gradient Norm: 18.579695991798218
Epoch: 8543, Batch Gradient Norm after: 18.579695991798218
Epoch 8544/10000, Prediction Accuracy = 65.248%, Loss = 0.2904466152191162
Epoch: 8544, Batch Gradient Norm: 19.18926019894458
Epoch: 8544, Batch Gradient Norm after: 18.369426309177843
Epoch 8545/10000, Prediction Accuracy = 64.932%, Loss = 0.2942714810371399
Epoch: 8545, Batch Gradient Norm: 15.612494175384649
Epoch: 8545, Batch Gradient Norm after: 15.612494175384649
Epoch 8546/10000, Prediction Accuracy = 64.99199999999999%, Loss = 0.2887039065361023
Epoch: 8546, Batch Gradient Norm: 14.781080049185718
Epoch: 8546, Batch Gradient Norm after: 14.781080049185718
Epoch 8547/10000, Prediction Accuracy = 65.016%, Loss = 0.2862683296203613
Epoch: 8547, Batch Gradient Norm: 15.53500896987862
Epoch: 8547, Batch Gradient Norm after: 15.53500896987862
Epoch 8548/10000, Prediction Accuracy = 64.92600000000002%, Loss = 0.2857385814189911
Epoch: 8548, Batch Gradient Norm: 13.041381122914622
Epoch: 8548, Batch Gradient Norm after: 13.041381122914622
Epoch 8549/10000, Prediction Accuracy = 65.11%, Loss = 0.2833297550678253
Epoch: 8549, Batch Gradient Norm: 19.070343313684443
Epoch: 8549, Batch Gradient Norm after: 17.66280926450095
Epoch 8550/10000, Prediction Accuracy = 64.90799999999999%, Loss = 0.2933472752571106
Epoch: 8550, Batch Gradient Norm: 20.62862352735374
Epoch: 8550, Batch Gradient Norm after: 19.487855005872962
Epoch 8551/10000, Prediction Accuracy = 65.01599999999999%, Loss = 0.29522526264190674
Epoch: 8551, Batch Gradient Norm: 18.27552380349148
Epoch: 8551, Batch Gradient Norm after: 18.27552380349148
Epoch 8552/10000, Prediction Accuracy = 64.93%, Loss = 0.2933720827102661
Epoch: 8552, Batch Gradient Norm: 18.952491516596144
Epoch: 8552, Batch Gradient Norm after: 18.67382354356466
Epoch 8553/10000, Prediction Accuracy = 65.04%, Loss = 0.29145789742469785
Epoch: 8553, Batch Gradient Norm: 17.447824867476662
Epoch: 8553, Batch Gradient Norm after: 16.81919144857442
Epoch 8554/10000, Prediction Accuracy = 65.07600000000001%, Loss = 0.28893433809280394
Epoch: 8554, Batch Gradient Norm: 16.932444161091226
Epoch: 8554, Batch Gradient Norm after: 16.932444161091226
Epoch 8555/10000, Prediction Accuracy = 65.10600000000001%, Loss = 0.28987480998039244
Epoch: 8555, Batch Gradient Norm: 17.788020678540292
Epoch: 8555, Batch Gradient Norm after: 17.788020678540292
Epoch 8556/10000, Prediction Accuracy = 64.958%, Loss = 0.2877247154712677
Epoch: 8556, Batch Gradient Norm: 16.47763973188006
Epoch: 8556, Batch Gradient Norm after: 16.47763973188006
Epoch 8557/10000, Prediction Accuracy = 65.048%, Loss = 0.2862288594245911
Epoch: 8557, Batch Gradient Norm: 19.262246228887527
Epoch: 8557, Batch Gradient Norm after: 19.101587548687203
Epoch 8558/10000, Prediction Accuracy = 64.924%, Loss = 0.2904560148715973
Epoch: 8558, Batch Gradient Norm: 17.83654018785505
Epoch: 8558, Batch Gradient Norm after: 17.83654018785505
Epoch 8559/10000, Prediction Accuracy = 65.16799999999998%, Loss = 0.2886357307434082
Epoch: 8559, Batch Gradient Norm: 15.756278624436195
Epoch: 8559, Batch Gradient Norm after: 15.756278624436195
Epoch 8560/10000, Prediction Accuracy = 65.048%, Loss = 0.2843752145767212
Epoch: 8560, Batch Gradient Norm: 17.703121701736986
Epoch: 8560, Batch Gradient Norm after: 17.703121701736986
Epoch 8561/10000, Prediction Accuracy = 65.24600000000001%, Loss = 0.28908787965774535
Epoch: 8561, Batch Gradient Norm: 18.572633538259154
Epoch: 8561, Batch Gradient Norm after: 18.572633538259154
Epoch 8562/10000, Prediction Accuracy = 65.098%, Loss = 0.2909165024757385
Epoch: 8562, Batch Gradient Norm: 16.241509082447983
Epoch: 8562, Batch Gradient Norm after: 16.241509082447983
Epoch 8563/10000, Prediction Accuracy = 65.138%, Loss = 0.2879736006259918
Epoch: 8563, Batch Gradient Norm: 13.69576320614749
Epoch: 8563, Batch Gradient Norm after: 13.69576320614749
Epoch 8564/10000, Prediction Accuracy = 64.92999999999999%, Loss = 0.28535484671592715
Epoch: 8564, Batch Gradient Norm: 13.679268962513166
Epoch: 8564, Batch Gradient Norm after: 13.679268962513166
Epoch 8565/10000, Prediction Accuracy = 64.984%, Loss = 0.2824161291122437
Epoch: 8565, Batch Gradient Norm: 13.616905062791371
Epoch: 8565, Batch Gradient Norm after: 13.616905062791371
Epoch 8566/10000, Prediction Accuracy = 65.012%, Loss = 0.2828130006790161
Epoch: 8566, Batch Gradient Norm: 14.445823672299372
Epoch: 8566, Batch Gradient Norm after: 14.445823672299372
Epoch 8567/10000, Prediction Accuracy = 64.98599999999999%, Loss = 0.284605610370636
Epoch: 8567, Batch Gradient Norm: 14.496950900070084
Epoch: 8567, Batch Gradient Norm after: 14.496950900070084
Epoch 8568/10000, Prediction Accuracy = 65.12400000000001%, Loss = 0.28555413484573366
Epoch: 8568, Batch Gradient Norm: 17.854505931025436
Epoch: 8568, Batch Gradient Norm after: 17.854505931025436
Epoch 8569/10000, Prediction Accuracy = 65.02000000000001%, Loss = 0.28991971015930174
Epoch: 8569, Batch Gradient Norm: 17.240880026413382
Epoch: 8569, Batch Gradient Norm after: 17.240880026413382
Epoch 8570/10000, Prediction Accuracy = 64.97%, Loss = 0.28828967213630674
Epoch: 8570, Batch Gradient Norm: 14.416492550655411
Epoch: 8570, Batch Gradient Norm after: 14.416492550655411
Epoch 8571/10000, Prediction Accuracy = 65.05600000000001%, Loss = 0.2834783673286438
Epoch: 8571, Batch Gradient Norm: 15.263511048743919
Epoch: 8571, Batch Gradient Norm after: 15.263511048743919
Epoch 8572/10000, Prediction Accuracy = 65.03%, Loss = 0.2870731770992279
Epoch: 8572, Batch Gradient Norm: 15.496240414113197
Epoch: 8572, Batch Gradient Norm after: 15.496240414113197
Epoch 8573/10000, Prediction Accuracy = 65.15%, Loss = 0.28643893003463744
Epoch: 8573, Batch Gradient Norm: 15.256710745553157
Epoch: 8573, Batch Gradient Norm after: 15.256710745553157
Epoch 8574/10000, Prediction Accuracy = 65.07%, Loss = 0.2866394817829132
Epoch: 8574, Batch Gradient Norm: 14.007181513978805
Epoch: 8574, Batch Gradient Norm after: 14.007181513978805
Epoch 8575/10000, Prediction Accuracy = 64.984%, Loss = 0.2859112799167633
Epoch: 8575, Batch Gradient Norm: 15.375882581983202
Epoch: 8575, Batch Gradient Norm after: 15.375882581983202
Epoch 8576/10000, Prediction Accuracy = 65.068%, Loss = 0.28726868629455565
Epoch: 8576, Batch Gradient Norm: 14.793375278998987
Epoch: 8576, Batch Gradient Norm after: 14.793375278998987
Epoch 8577/10000, Prediction Accuracy = 65.072%, Loss = 0.28474453687667844
Epoch: 8577, Batch Gradient Norm: 14.009665471708583
Epoch: 8577, Batch Gradient Norm after: 14.009665471708583
Epoch 8578/10000, Prediction Accuracy = 65.11%, Loss = 0.2831682741641998
Epoch: 8578, Batch Gradient Norm: 16.177585037352944
Epoch: 8578, Batch Gradient Norm after: 16.177585037352944
Epoch 8579/10000, Prediction Accuracy = 64.864%, Loss = 0.2862739622592926
Epoch: 8579, Batch Gradient Norm: 11.943503377109419
Epoch: 8579, Batch Gradient Norm after: 11.943503377109419
Epoch 8580/10000, Prediction Accuracy = 65.08200000000001%, Loss = 0.2823763608932495
Epoch: 8580, Batch Gradient Norm: 11.324318790863392
Epoch: 8580, Batch Gradient Norm after: 11.324318790863392
Epoch 8581/10000, Prediction Accuracy = 65.13199999999999%, Loss = 0.2810811877250671
Epoch: 8581, Batch Gradient Norm: 11.751197131478985
Epoch: 8581, Batch Gradient Norm after: 11.751197131478985
Epoch 8582/10000, Prediction Accuracy = 64.988%, Loss = 0.28131012320518495
Epoch: 8582, Batch Gradient Norm: 12.968770873022935
Epoch: 8582, Batch Gradient Norm after: 12.968770873022935
Epoch 8583/10000, Prediction Accuracy = 65.122%, Loss = 0.28261940479278563
Epoch: 8583, Batch Gradient Norm: 12.988161981475578
Epoch: 8583, Batch Gradient Norm after: 12.988161981475578
Epoch 8584/10000, Prediction Accuracy = 65.01%, Loss = 0.2837436258792877
Epoch: 8584, Batch Gradient Norm: 14.929159593407281
Epoch: 8584, Batch Gradient Norm after: 14.929159593407281
Epoch 8585/10000, Prediction Accuracy = 65.11%, Loss = 0.28594693541526794
Epoch: 8585, Batch Gradient Norm: 14.127619057692677
Epoch: 8585, Batch Gradient Norm after: 14.127619057692677
Epoch 8586/10000, Prediction Accuracy = 65.08800000000001%, Loss = 0.2844529926776886
Epoch: 8586, Batch Gradient Norm: 14.33207136592604
Epoch: 8586, Batch Gradient Norm after: 14.33207136592604
Epoch 8587/10000, Prediction Accuracy = 65.00800000000001%, Loss = 0.2827895939350128
Epoch: 8587, Batch Gradient Norm: 16.407236321733244
Epoch: 8587, Batch Gradient Norm after: 16.407236321733244
Epoch 8588/10000, Prediction Accuracy = 65.088%, Loss = 0.285379159450531
Epoch: 8588, Batch Gradient Norm: 18.21283367060376
Epoch: 8588, Batch Gradient Norm after: 18.21283367060376
Epoch 8589/10000, Prediction Accuracy = 64.93199999999999%, Loss = 0.29068689942359927
Epoch: 8589, Batch Gradient Norm: 15.690889842561496
Epoch: 8589, Batch Gradient Norm after: 15.690889842561496
Epoch 8590/10000, Prediction Accuracy = 65.05199999999999%, Loss = 0.2870422065258026
Epoch: 8590, Batch Gradient Norm: 14.547381508756263
Epoch: 8590, Batch Gradient Norm after: 14.547381508756263
Epoch 8591/10000, Prediction Accuracy = 64.964%, Loss = 0.28469310998916625
Epoch: 8591, Batch Gradient Norm: 14.478517355290618
Epoch: 8591, Batch Gradient Norm after: 14.478517355290618
Epoch 8592/10000, Prediction Accuracy = 65.024%, Loss = 0.2837271153926849
Epoch: 8592, Batch Gradient Norm: 12.985623079782966
Epoch: 8592, Batch Gradient Norm after: 12.985623079782966
Epoch 8593/10000, Prediction Accuracy = 64.97200000000001%, Loss = 0.2843183934688568
Epoch: 8593, Batch Gradient Norm: 14.205676308167497
Epoch: 8593, Batch Gradient Norm after: 14.205676308167497
Epoch 8594/10000, Prediction Accuracy = 65.09200000000001%, Loss = 0.28493579626083376
Epoch: 8594, Batch Gradient Norm: 14.67808270893486
Epoch: 8594, Batch Gradient Norm after: 14.67808270893486
Epoch 8595/10000, Prediction Accuracy = 64.992%, Loss = 0.288654500246048
Epoch: 8595, Batch Gradient Norm: 15.461706302664318
Epoch: 8595, Batch Gradient Norm after: 15.461706302664318
Epoch 8596/10000, Prediction Accuracy = 65.086%, Loss = 0.286516934633255
Epoch: 8596, Batch Gradient Norm: 14.872652995916862
Epoch: 8596, Batch Gradient Norm after: 14.872652995916862
Epoch 8597/10000, Prediction Accuracy = 65.016%, Loss = 0.284971821308136
Epoch: 8597, Batch Gradient Norm: 13.459051568402074
Epoch: 8597, Batch Gradient Norm after: 13.459051568402074
Epoch 8598/10000, Prediction Accuracy = 65.05199999999999%, Loss = 0.282417356967926
Epoch: 8598, Batch Gradient Norm: 15.92058013805124
Epoch: 8598, Batch Gradient Norm after: 15.92058013805124
Epoch 8599/10000, Prediction Accuracy = 64.99999999999999%, Loss = 0.28943353295326235
Epoch: 8599, Batch Gradient Norm: 16.878556191056173
Epoch: 8599, Batch Gradient Norm after: 16.878556191056173
Epoch 8600/10000, Prediction Accuracy = 65.014%, Loss = 0.2922389626502991
Epoch: 8600, Batch Gradient Norm: 18.01768413820644
Epoch: 8600, Batch Gradient Norm after: 18.01768413820644
Epoch 8601/10000, Prediction Accuracy = 64.92999999999999%, Loss = 0.29024829268455504
Epoch: 8601, Batch Gradient Norm: 15.494686023817508
Epoch: 8601, Batch Gradient Norm after: 15.494686023817508
Epoch 8602/10000, Prediction Accuracy = 65.086%, Loss = 0.2877774238586426
Epoch: 8602, Batch Gradient Norm: 15.501200895639977
Epoch: 8602, Batch Gradient Norm after: 15.501200895639977
Epoch 8603/10000, Prediction Accuracy = 65.03999999999999%, Loss = 0.28906131982803346
Epoch: 8603, Batch Gradient Norm: 15.198246319589224
Epoch: 8603, Batch Gradient Norm after: 15.198246319589224
Epoch 8604/10000, Prediction Accuracy = 64.862%, Loss = 0.28515943288803103
Epoch: 8604, Batch Gradient Norm: 17.41499668285584
Epoch: 8604, Batch Gradient Norm after: 17.41499668285584
Epoch 8605/10000, Prediction Accuracy = 64.908%, Loss = 0.29287356734275816
Epoch: 8605, Batch Gradient Norm: 16.608038461678056
Epoch: 8605, Batch Gradient Norm after: 16.608038461678056
Epoch 8606/10000, Prediction Accuracy = 64.968%, Loss = 0.2860844492912292
Epoch: 8606, Batch Gradient Norm: 13.174121581742853
Epoch: 8606, Batch Gradient Norm after: 13.174121581742853
Epoch 8607/10000, Prediction Accuracy = 65.10999999999999%, Loss = 0.283191055059433
Epoch: 8607, Batch Gradient Norm: 12.745306362412858
Epoch: 8607, Batch Gradient Norm after: 12.745306362412858
Epoch 8608/10000, Prediction Accuracy = 65.14599999999999%, Loss = 0.28164719939231875
Epoch: 8608, Batch Gradient Norm: 13.607654526004112
Epoch: 8608, Batch Gradient Norm after: 13.607654526004112
Epoch 8609/10000, Prediction Accuracy = 65.206%, Loss = 0.2817462205886841
Epoch: 8609, Batch Gradient Norm: 13.841381404960732
Epoch: 8609, Batch Gradient Norm after: 13.841381404960732
Epoch 8610/10000, Prediction Accuracy = 65.08%, Loss = 0.2840843677520752
Epoch: 8610, Batch Gradient Norm: 12.059771722517187
Epoch: 8610, Batch Gradient Norm after: 12.059771722517187
Epoch 8611/10000, Prediction Accuracy = 65.05000000000001%, Loss = 0.28089296221733095
Epoch: 8611, Batch Gradient Norm: 15.197559665442313
Epoch: 8611, Batch Gradient Norm after: 15.197559665442313
Epoch 8612/10000, Prediction Accuracy = 65.08%, Loss = 0.2873941957950592
Epoch: 8612, Batch Gradient Norm: 14.560735736639433
Epoch: 8612, Batch Gradient Norm after: 14.560735736639433
Epoch 8613/10000, Prediction Accuracy = 64.92%, Loss = 0.28680952191352843
Epoch: 8613, Batch Gradient Norm: 15.049393018858105
Epoch: 8613, Batch Gradient Norm after: 15.049393018858105
Epoch 8614/10000, Prediction Accuracy = 64.91799999999999%, Loss = 0.28582038879394533
Epoch: 8614, Batch Gradient Norm: 11.944927638447254
Epoch: 8614, Batch Gradient Norm after: 11.944927638447254
Epoch 8615/10000, Prediction Accuracy = 65.122%, Loss = 0.27965477108955383
Epoch: 8615, Batch Gradient Norm: 13.352170889754412
Epoch: 8615, Batch Gradient Norm after: 13.352170889754412
Epoch 8616/10000, Prediction Accuracy = 64.91%, Loss = 0.2863845586776733
Epoch: 8616, Batch Gradient Norm: 14.753386807676248
Epoch: 8616, Batch Gradient Norm after: 14.753386807676248
Epoch 8617/10000, Prediction Accuracy = 65.002%, Loss = 0.2869054198265076
Epoch: 8617, Batch Gradient Norm: 14.57962582674455
Epoch: 8617, Batch Gradient Norm after: 14.57962582674455
Epoch 8618/10000, Prediction Accuracy = 65.144%, Loss = 0.28284813165664674
Epoch: 8618, Batch Gradient Norm: 11.243250186086957
Epoch: 8618, Batch Gradient Norm after: 11.243250186086957
Epoch 8619/10000, Prediction Accuracy = 65.098%, Loss = 0.2782954275608063
Epoch: 8619, Batch Gradient Norm: 13.452934817228966
Epoch: 8619, Batch Gradient Norm after: 13.452934817228966
Epoch 8620/10000, Prediction Accuracy = 64.89%, Loss = 0.28496257662773133
Epoch: 8620, Batch Gradient Norm: 13.3820868240353
Epoch: 8620, Batch Gradient Norm after: 13.3820868240353
Epoch 8621/10000, Prediction Accuracy = 65.06%, Loss = 0.2860895872116089
Epoch: 8621, Batch Gradient Norm: 17.245389631853083
Epoch: 8621, Batch Gradient Norm after: 17.245389631853083
Epoch 8622/10000, Prediction Accuracy = 64.91600000000001%, Loss = 0.2869295597076416
Epoch: 8622, Batch Gradient Norm: 16.81508770025486
Epoch: 8622, Batch Gradient Norm after: 16.81508770025486
Epoch 8623/10000, Prediction Accuracy = 65.196%, Loss = 0.2878612458705902
Epoch: 8623, Batch Gradient Norm: 16.195363553625
Epoch: 8623, Batch Gradient Norm after: 15.884250832290762
Epoch 8624/10000, Prediction Accuracy = 65.102%, Loss = 0.2836609363555908
Epoch: 8624, Batch Gradient Norm: 13.914987209891006
Epoch: 8624, Batch Gradient Norm after: 13.914987209891006
Epoch 8625/10000, Prediction Accuracy = 65.098%, Loss = 0.2826467573642731
Epoch: 8625, Batch Gradient Norm: 12.779359153232765
Epoch: 8625, Batch Gradient Norm after: 12.779359153232765
Epoch 8626/10000, Prediction Accuracy = 65.178%, Loss = 0.2827450633049011
Epoch: 8626, Batch Gradient Norm: 15.972703994687535
Epoch: 8626, Batch Gradient Norm after: 15.972703994687535
Epoch 8627/10000, Prediction Accuracy = 65.136%, Loss = 0.2843315601348877
Epoch: 8627, Batch Gradient Norm: 13.682667003717544
Epoch: 8627, Batch Gradient Norm after: 13.682667003717544
Epoch 8628/10000, Prediction Accuracy = 65.03%, Loss = 0.2844128429889679
Epoch: 8628, Batch Gradient Norm: 14.278880081138176
Epoch: 8628, Batch Gradient Norm after: 14.278880081138176
Epoch 8629/10000, Prediction Accuracy = 65.10999999999999%, Loss = 0.28728567957878115
Epoch: 8629, Batch Gradient Norm: 15.83935488709578
Epoch: 8629, Batch Gradient Norm after: 15.83935488709578
Epoch 8630/10000, Prediction Accuracy = 65.03600000000002%, Loss = 0.28493847250938414
Epoch: 8630, Batch Gradient Norm: 15.997506420772007
Epoch: 8630, Batch Gradient Norm after: 15.997506420772007
Epoch 8631/10000, Prediction Accuracy = 65.10799999999999%, Loss = 0.2850920021533966
Epoch: 8631, Batch Gradient Norm: 13.598212004207827
Epoch: 8631, Batch Gradient Norm after: 13.598212004207827
Epoch 8632/10000, Prediction Accuracy = 64.966%, Loss = 0.28558189868927003
Epoch: 8632, Batch Gradient Norm: 16.16366545965847
Epoch: 8632, Batch Gradient Norm after: 16.16366545965847
Epoch 8633/10000, Prediction Accuracy = 65.22200000000001%, Loss = 0.28608499765396117
Epoch: 8633, Batch Gradient Norm: 14.625489737788591
Epoch: 8633, Batch Gradient Norm after: 14.625489737788591
Epoch 8634/10000, Prediction Accuracy = 65.19399999999999%, Loss = 0.28229655623435973
Epoch: 8634, Batch Gradient Norm: 14.073947537542963
Epoch: 8634, Batch Gradient Norm after: 14.073947537542963
Epoch 8635/10000, Prediction Accuracy = 65.036%, Loss = 0.2849690794944763
Epoch: 8635, Batch Gradient Norm: 13.77582965439396
Epoch: 8635, Batch Gradient Norm after: 13.77582965439396
Epoch 8636/10000, Prediction Accuracy = 65.058%, Loss = 0.28357304334640504
Epoch: 8636, Batch Gradient Norm: 17.488520035887493
Epoch: 8636, Batch Gradient Norm after: 17.488520035887493
Epoch 8637/10000, Prediction Accuracy = 65.006%, Loss = 0.2918925166130066
Epoch: 8637, Batch Gradient Norm: 16.880003402617792
Epoch: 8637, Batch Gradient Norm after: 16.880003402617792
Epoch 8638/10000, Prediction Accuracy = 65.048%, Loss = 0.2878373205661774
Epoch: 8638, Batch Gradient Norm: 14.314601166585113
Epoch: 8638, Batch Gradient Norm after: 14.314601166585113
Epoch 8639/10000, Prediction Accuracy = 65.076%, Loss = 0.28430997133255004
Epoch: 8639, Batch Gradient Norm: 12.795478468347996
Epoch: 8639, Batch Gradient Norm after: 12.795478468347996
Epoch 8640/10000, Prediction Accuracy = 65.042%, Loss = 0.283671909570694
Epoch: 8640, Batch Gradient Norm: 14.056819932539936
Epoch: 8640, Batch Gradient Norm after: 14.056819932539936
Epoch 8641/10000, Prediction Accuracy = 65.08599999999998%, Loss = 0.2834886610507965
Epoch: 8641, Batch Gradient Norm: 15.94066964983935
Epoch: 8641, Batch Gradient Norm after: 15.94066964983935
Epoch 8642/10000, Prediction Accuracy = 65.018%, Loss = 0.28568816781044004
Epoch: 8642, Batch Gradient Norm: 14.960902376361526
Epoch: 8642, Batch Gradient Norm after: 14.960902376361526
Epoch 8643/10000, Prediction Accuracy = 65.09400000000001%, Loss = 0.28221636414527895
Epoch: 8643, Batch Gradient Norm: 15.637404085179242
Epoch: 8643, Batch Gradient Norm after: 15.637404085179242
Epoch 8644/10000, Prediction Accuracy = 64.90599999999999%, Loss = 0.2850237250328064
Epoch: 8644, Batch Gradient Norm: 13.08312314657872
Epoch: 8644, Batch Gradient Norm after: 13.08312314657872
Epoch 8645/10000, Prediction Accuracy = 65.17999999999999%, Loss = 0.2807766139507294
Epoch: 8645, Batch Gradient Norm: 12.146768047169706
Epoch: 8645, Batch Gradient Norm after: 12.146768047169706
Epoch 8646/10000, Prediction Accuracy = 65.048%, Loss = 0.2826824724674225
Epoch: 8646, Batch Gradient Norm: 12.614118501527594
Epoch: 8646, Batch Gradient Norm after: 12.614118501527594
Epoch 8647/10000, Prediction Accuracy = 65.134%, Loss = 0.2811331212520599
Epoch: 8647, Batch Gradient Norm: 11.521171042285756
Epoch: 8647, Batch Gradient Norm after: 11.521171042285756
Epoch 8648/10000, Prediction Accuracy = 65.01%, Loss = 0.28115994930267335
Epoch: 8648, Batch Gradient Norm: 13.651147255264734
Epoch: 8648, Batch Gradient Norm after: 13.651147255264734
Epoch 8649/10000, Prediction Accuracy = 65.076%, Loss = 0.2821999490261078
Epoch: 8649, Batch Gradient Norm: 13.639982042682123
Epoch: 8649, Batch Gradient Norm after: 13.639982042682123
Epoch 8650/10000, Prediction Accuracy = 65.15599999999999%, Loss = 0.28082035779953
Epoch: 8650, Batch Gradient Norm: 15.308275418072508
Epoch: 8650, Batch Gradient Norm after: 15.308275418072508
Epoch 8651/10000, Prediction Accuracy = 65.01199999999999%, Loss = 0.28525635600090027
Epoch: 8651, Batch Gradient Norm: 15.194770733802981
Epoch: 8651, Batch Gradient Norm after: 15.194770733802981
Epoch 8652/10000, Prediction Accuracy = 65.218%, Loss = 0.283963268995285
Epoch: 8652, Batch Gradient Norm: 16.410679544622443
Epoch: 8652, Batch Gradient Norm after: 16.410679544622443
Epoch 8653/10000, Prediction Accuracy = 65.12199999999999%, Loss = 0.2845889627933502
Epoch: 8653, Batch Gradient Norm: 16.9929196077367
Epoch: 8653, Batch Gradient Norm after: 16.9929196077367
Epoch 8654/10000, Prediction Accuracy = 65.228%, Loss = 0.2872165322303772
Epoch: 8654, Batch Gradient Norm: 16.79689411342434
Epoch: 8654, Batch Gradient Norm after: 16.79689411342434
Epoch 8655/10000, Prediction Accuracy = 65.03800000000001%, Loss = 0.28751721382141116
Epoch: 8655, Batch Gradient Norm: 15.978641674461938
Epoch: 8655, Batch Gradient Norm after: 15.978641674461938
Epoch 8656/10000, Prediction Accuracy = 65.05%, Loss = 0.2863633632659912
Epoch: 8656, Batch Gradient Norm: 17.95200192186439
Epoch: 8656, Batch Gradient Norm after: 17.742889971839254
Epoch 8657/10000, Prediction Accuracy = 65.16400000000002%, Loss = 0.286712646484375
Epoch: 8657, Batch Gradient Norm: 18.02116247240304
Epoch: 8657, Batch Gradient Norm after: 18.02116247240304
Epoch 8658/10000, Prediction Accuracy = 64.87800000000001%, Loss = 0.2889610528945923
Epoch: 8658, Batch Gradient Norm: 17.847690358623904
Epoch: 8658, Batch Gradient Norm after: 17.847690358623904
Epoch 8659/10000, Prediction Accuracy = 65.15%, Loss = 0.2886780023574829
Epoch: 8659, Batch Gradient Norm: 18.70328615104921
Epoch: 8659, Batch Gradient Norm after: 17.9957520543164
Epoch 8660/10000, Prediction Accuracy = 65.148%, Loss = 0.2887809455394745
Epoch: 8660, Batch Gradient Norm: 13.916235588610249
Epoch: 8660, Batch Gradient Norm after: 13.916235588610249
Epoch 8661/10000, Prediction Accuracy = 64.902%, Loss = 0.2822790741920471
Epoch: 8661, Batch Gradient Norm: 14.464476812391021
Epoch: 8661, Batch Gradient Norm after: 14.464476812391021
Epoch 8662/10000, Prediction Accuracy = 64.938%, Loss = 0.2855012059211731
Epoch: 8662, Batch Gradient Norm: 15.046763114485653
Epoch: 8662, Batch Gradient Norm after: 15.046763114485653
Epoch 8663/10000, Prediction Accuracy = 65.0%, Loss = 0.28746902346611025
Epoch: 8663, Batch Gradient Norm: 17.29061102933502
Epoch: 8663, Batch Gradient Norm after: 16.985425599704875
Epoch 8664/10000, Prediction Accuracy = 65.072%, Loss = 0.2895192265510559
Epoch: 8664, Batch Gradient Norm: 17.90920427969062
Epoch: 8664, Batch Gradient Norm after: 17.77855546072844
Epoch 8665/10000, Prediction Accuracy = 65.018%, Loss = 0.2886763095855713
Epoch: 8665, Batch Gradient Norm: 14.852604113867734
Epoch: 8665, Batch Gradient Norm after: 14.852604113867734
Epoch 8666/10000, Prediction Accuracy = 65.20599999999999%, Loss = 0.2845828890800476
Epoch: 8666, Batch Gradient Norm: 16.07327471845016
Epoch: 8666, Batch Gradient Norm after: 16.07327471845016
Epoch 8667/10000, Prediction Accuracy = 65.1%, Loss = 0.2832191467285156
Epoch: 8667, Batch Gradient Norm: 15.39221513825801
Epoch: 8667, Batch Gradient Norm after: 15.39221513825801
Epoch 8668/10000, Prediction Accuracy = 65.166%, Loss = 0.2832644522190094
Epoch: 8668, Batch Gradient Norm: 17.660731223503063
Epoch: 8668, Batch Gradient Norm after: 17.500886540507995
Epoch 8669/10000, Prediction Accuracy = 65.078%, Loss = 0.28672303557395934
Epoch: 8669, Batch Gradient Norm: 17.364414913016642
Epoch: 8669, Batch Gradient Norm after: 17.364414913016642
Epoch 8670/10000, Prediction Accuracy = 65.06800000000001%, Loss = 0.29062235951423643
Epoch: 8670, Batch Gradient Norm: 14.743850060016749
Epoch: 8670, Batch Gradient Norm after: 14.743850060016749
Epoch 8671/10000, Prediction Accuracy = 65.18599999999999%, Loss = 0.28429049253463745
Epoch: 8671, Batch Gradient Norm: 15.208313535648129
Epoch: 8671, Batch Gradient Norm after: 15.208313535648129
Epoch 8672/10000, Prediction Accuracy = 65.068%, Loss = 0.28560327291488646
Epoch: 8672, Batch Gradient Norm: 14.618261562976377
Epoch: 8672, Batch Gradient Norm after: 14.618261562976377
Epoch 8673/10000, Prediction Accuracy = 65.11399999999999%, Loss = 0.2827314019203186
Epoch: 8673, Batch Gradient Norm: 14.063977450527581
Epoch: 8673, Batch Gradient Norm after: 14.063977450527581
Epoch 8674/10000, Prediction Accuracy = 65.132%, Loss = 0.2823993623256683
Epoch: 8674, Batch Gradient Norm: 13.098318211266657
Epoch: 8674, Batch Gradient Norm after: 13.098318211266657
Epoch 8675/10000, Prediction Accuracy = 65.194%, Loss = 0.2791526436805725
Epoch: 8675, Batch Gradient Norm: 13.969135767414452
Epoch: 8675, Batch Gradient Norm after: 13.969135767414452
Epoch 8676/10000, Prediction Accuracy = 65.154%, Loss = 0.28259323835372924
Epoch: 8676, Batch Gradient Norm: 14.921195292168173
Epoch: 8676, Batch Gradient Norm after: 14.921195292168173
Epoch 8677/10000, Prediction Accuracy = 65.05999999999999%, Loss = 0.28345572352409365
Epoch: 8677, Batch Gradient Norm: 14.607331572973063
Epoch: 8677, Batch Gradient Norm after: 14.607331572973063
Epoch 8678/10000, Prediction Accuracy = 65.082%, Loss = 0.284844172000885
Epoch: 8678, Batch Gradient Norm: 12.074987792919787
Epoch: 8678, Batch Gradient Norm after: 12.074987792919787
Epoch 8679/10000, Prediction Accuracy = 65.174%, Loss = 0.28020007610321046
Epoch: 8679, Batch Gradient Norm: 14.176060246837851
Epoch: 8679, Batch Gradient Norm after: 14.176060246837851
Epoch 8680/10000, Prediction Accuracy = 64.96200000000002%, Loss = 0.2828163981437683
Epoch: 8680, Batch Gradient Norm: 12.492218395800542
Epoch: 8680, Batch Gradient Norm after: 12.492218395800542
Epoch 8681/10000, Prediction Accuracy = 65.05799999999999%, Loss = 0.2812889516353607
Epoch: 8681, Batch Gradient Norm: 11.662092353451849
Epoch: 8681, Batch Gradient Norm after: 11.662092353451849
Epoch 8682/10000, Prediction Accuracy = 65.176%, Loss = 0.2793749451637268
Epoch: 8682, Batch Gradient Norm: 12.45539458571472
Epoch: 8682, Batch Gradient Norm after: 12.45539458571472
Epoch 8683/10000, Prediction Accuracy = 65.076%, Loss = 0.2813574433326721
Epoch: 8683, Batch Gradient Norm: 13.966509492084011
Epoch: 8683, Batch Gradient Norm after: 13.966509492084011
Epoch 8684/10000, Prediction Accuracy = 65.08400000000002%, Loss = 0.2808025419712067
Epoch: 8684, Batch Gradient Norm: 17.356173516963548
Epoch: 8684, Batch Gradient Norm after: 17.356173516963548
Epoch 8685/10000, Prediction Accuracy = 65.09200000000001%, Loss = 0.28915077447891235
Epoch: 8685, Batch Gradient Norm: 18.296188715980062
Epoch: 8685, Batch Gradient Norm after: 18.296188715980062
Epoch 8686/10000, Prediction Accuracy = 65.11600000000001%, Loss = 0.28787697553634645
Epoch: 8686, Batch Gradient Norm: 17.935532450687074
Epoch: 8686, Batch Gradient Norm after: 17.935532450687074
Epoch 8687/10000, Prediction Accuracy = 65.06%, Loss = 0.287779438495636
Epoch: 8687, Batch Gradient Norm: 17.833800625587003
Epoch: 8687, Batch Gradient Norm after: 16.62237112489009
Epoch 8688/10000, Prediction Accuracy = 65.054%, Loss = 0.287860506772995
Epoch: 8688, Batch Gradient Norm: 15.776815325050801
Epoch: 8688, Batch Gradient Norm after: 15.776815325050801
Epoch 8689/10000, Prediction Accuracy = 65.19200000000001%, Loss = 0.28644862174987795
Epoch: 8689, Batch Gradient Norm: 16.434316881111613
Epoch: 8689, Batch Gradient Norm after: 16.434316881111613
Epoch 8690/10000, Prediction Accuracy = 65.17%, Loss = 0.287478905916214
Epoch: 8690, Batch Gradient Norm: 14.631277214887898
Epoch: 8690, Batch Gradient Norm after: 14.631277214887898
Epoch 8691/10000, Prediction Accuracy = 64.964%, Loss = 0.2860231935977936
Epoch: 8691, Batch Gradient Norm: 12.61630114760583
Epoch: 8691, Batch Gradient Norm after: 12.61630114760583
Epoch 8692/10000, Prediction Accuracy = 65.23400000000001%, Loss = 0.2818800151348114
Epoch: 8692, Batch Gradient Norm: 13.061699344025971
Epoch: 8692, Batch Gradient Norm after: 13.061699344025971
Epoch 8693/10000, Prediction Accuracy = 65.03%, Loss = 0.28033804297447207
Epoch: 8693, Batch Gradient Norm: 13.427626538774371
Epoch: 8693, Batch Gradient Norm after: 13.427626538774371
Epoch 8694/10000, Prediction Accuracy = 65.13%, Loss = 0.2817267417907715
Epoch: 8694, Batch Gradient Norm: 12.828856326068726
Epoch: 8694, Batch Gradient Norm after: 12.828856326068726
Epoch 8695/10000, Prediction Accuracy = 65.116%, Loss = 0.2806934952735901
Epoch: 8695, Batch Gradient Norm: 13.54115418457507
Epoch: 8695, Batch Gradient Norm after: 13.54115418457507
Epoch 8696/10000, Prediction Accuracy = 65.12800000000001%, Loss = 0.2803914129734039
Epoch: 8696, Batch Gradient Norm: 14.80710393231551
Epoch: 8696, Batch Gradient Norm after: 14.80710393231551
Epoch 8697/10000, Prediction Accuracy = 65.104%, Loss = 0.28388752937316897
Epoch: 8697, Batch Gradient Norm: 18.81617264985668
Epoch: 8697, Batch Gradient Norm after: 18.382398097421113
Epoch 8698/10000, Prediction Accuracy = 65.128%, Loss = 0.28893699645996096
Epoch: 8698, Batch Gradient Norm: 15.931013095508392
Epoch: 8698, Batch Gradient Norm after: 15.931013095508392
Epoch 8699/10000, Prediction Accuracy = 65.09599999999999%, Loss = 0.2838085234165192
Epoch: 8699, Batch Gradient Norm: 15.483019388036228
Epoch: 8699, Batch Gradient Norm after: 15.483019388036228
Epoch 8700/10000, Prediction Accuracy = 65.066%, Loss = 0.2876048982143402
Epoch: 8700, Batch Gradient Norm: 14.856373637723918
Epoch: 8700, Batch Gradient Norm after: 14.856373637723918
Epoch 8701/10000, Prediction Accuracy = 65.118%, Loss = 0.28264633417129514
Epoch: 8701, Batch Gradient Norm: 13.879724529222818
Epoch: 8701, Batch Gradient Norm after: 13.879724529222818
Epoch 8702/10000, Prediction Accuracy = 65.072%, Loss = 0.28320655822753904
Epoch: 8702, Batch Gradient Norm: 14.50295719981914
Epoch: 8702, Batch Gradient Norm after: 14.50295719981914
Epoch 8703/10000, Prediction Accuracy = 65.326%, Loss = 0.2819617986679077
Epoch: 8703, Batch Gradient Norm: 12.855184079612432
Epoch: 8703, Batch Gradient Norm after: 12.855184079612432
Epoch 8704/10000, Prediction Accuracy = 65.088%, Loss = 0.28198346495628357
Epoch: 8704, Batch Gradient Norm: 12.534277478304778
Epoch: 8704, Batch Gradient Norm after: 12.534277478304778
Epoch 8705/10000, Prediction Accuracy = 65.202%, Loss = 0.27914887070655825
Epoch: 8705, Batch Gradient Norm: 11.99568241331392
Epoch: 8705, Batch Gradient Norm after: 11.99568241331392
Epoch 8706/10000, Prediction Accuracy = 65.134%, Loss = 0.2779135644435883
Epoch: 8706, Batch Gradient Norm: 16.42800999081479
Epoch: 8706, Batch Gradient Norm after: 16.410303061906237
Epoch 8707/10000, Prediction Accuracy = 65.244%, Loss = 0.2832311451435089
Epoch: 8707, Batch Gradient Norm: 14.364502582320156
Epoch: 8707, Batch Gradient Norm after: 14.364502582320156
Epoch 8708/10000, Prediction Accuracy = 65.146%, Loss = 0.2817864418029785
Epoch: 8708, Batch Gradient Norm: 13.459418613265287
Epoch: 8708, Batch Gradient Norm after: 13.459418613265287
Epoch 8709/10000, Prediction Accuracy = 65.03%, Loss = 0.28209437131881715
Epoch: 8709, Batch Gradient Norm: 14.59740648388917
Epoch: 8709, Batch Gradient Norm after: 14.59740648388917
Epoch 8710/10000, Prediction Accuracy = 65.046%, Loss = 0.2846267342567444
Epoch: 8710, Batch Gradient Norm: 17.22329523237812
Epoch: 8710, Batch Gradient Norm after: 17.134838229043364
Epoch 8711/10000, Prediction Accuracy = 65.188%, Loss = 0.2852927207946777
Epoch: 8711, Batch Gradient Norm: 15.96164786726659
Epoch: 8711, Batch Gradient Norm after: 15.96164786726659
Epoch 8712/10000, Prediction Accuracy = 65.164%, Loss = 0.2846971094608307
Epoch: 8712, Batch Gradient Norm: 16.203216256195958
Epoch: 8712, Batch Gradient Norm after: 16.203216256195958
Epoch 8713/10000, Prediction Accuracy = 65.02%, Loss = 0.2835013449192047
Epoch: 8713, Batch Gradient Norm: 12.52654167364876
Epoch: 8713, Batch Gradient Norm after: 12.52654167364876
Epoch 8714/10000, Prediction Accuracy = 65.118%, Loss = 0.28132076263427735
Epoch: 8714, Batch Gradient Norm: 14.692563441265783
Epoch: 8714, Batch Gradient Norm after: 14.692563441265783
Epoch 8715/10000, Prediction Accuracy = 64.994%, Loss = 0.2847596526145935
Epoch: 8715, Batch Gradient Norm: 16.017363051433005
Epoch: 8715, Batch Gradient Norm after: 16.017363051433005
Epoch 8716/10000, Prediction Accuracy = 65.11800000000001%, Loss = 0.2848466157913208
Epoch: 8716, Batch Gradient Norm: 14.780027471775782
Epoch: 8716, Batch Gradient Norm after: 14.780027471775782
Epoch 8717/10000, Prediction Accuracy = 65.12199999999999%, Loss = 0.2839718580245972
Epoch: 8717, Batch Gradient Norm: 15.921161486392094
Epoch: 8717, Batch Gradient Norm after: 15.921161486392094
Epoch 8718/10000, Prediction Accuracy = 65.11%, Loss = 0.2864752173423767
Epoch: 8718, Batch Gradient Norm: 16.021042852756448
Epoch: 8718, Batch Gradient Norm after: 16.021042852756448
Epoch 8719/10000, Prediction Accuracy = 65.08%, Loss = 0.2849606215953827
Epoch: 8719, Batch Gradient Norm: 14.791967452292152
Epoch: 8719, Batch Gradient Norm after: 14.791967452292152
Epoch 8720/10000, Prediction Accuracy = 65.15400000000001%, Loss = 0.2864328622817993
Epoch: 8720, Batch Gradient Norm: 18.769071281725015
Epoch: 8720, Batch Gradient Norm after: 18.769071281725015
Epoch 8721/10000, Prediction Accuracy = 64.99%, Loss = 0.29067962169647216
Epoch: 8721, Batch Gradient Norm: 16.394312908675776
Epoch: 8721, Batch Gradient Norm after: 16.394312908675776
Epoch 8722/10000, Prediction Accuracy = 65.21000000000001%, Loss = 0.2854688405990601
Epoch: 8722, Batch Gradient Norm: 17.36862271271748
Epoch: 8722, Batch Gradient Norm after: 17.36862271271748
Epoch 8723/10000, Prediction Accuracy = 65.072%, Loss = 0.2879142165184021
Epoch: 8723, Batch Gradient Norm: 19.19969128436358
Epoch: 8723, Batch Gradient Norm after: 19.19969128436358
Epoch 8724/10000, Prediction Accuracy = 65.31200000000001%, Loss = 0.2900453686714172
Epoch: 8724, Batch Gradient Norm: 12.360498073066536
Epoch: 8724, Batch Gradient Norm after: 12.360498073066536
Epoch 8725/10000, Prediction Accuracy = 65.15599999999999%, Loss = 0.28113717436790464
Epoch: 8725, Batch Gradient Norm: 15.01390176160266
Epoch: 8725, Batch Gradient Norm after: 15.01390176160266
Epoch 8726/10000, Prediction Accuracy = 65.276%, Loss = 0.2847613275051117
Epoch: 8726, Batch Gradient Norm: 17.919259393446875
Epoch: 8726, Batch Gradient Norm after: 17.831785048999173
Epoch 8727/10000, Prediction Accuracy = 65.136%, Loss = 0.28761231899261475
Epoch: 8727, Batch Gradient Norm: 17.66626426154961
Epoch: 8727, Batch Gradient Norm after: 17.66626426154961
Epoch 8728/10000, Prediction Accuracy = 65.056%, Loss = 0.28767173290252684
Epoch: 8728, Batch Gradient Norm: 16.597197210461648
Epoch: 8728, Batch Gradient Norm after: 16.597197210461648
Epoch 8729/10000, Prediction Accuracy = 65.03%, Loss = 0.2859838306903839
Epoch: 8729, Batch Gradient Norm: 14.761596191191153
Epoch: 8729, Batch Gradient Norm after: 14.761596191191153
Epoch 8730/10000, Prediction Accuracy = 64.996%, Loss = 0.2850688576698303
Epoch: 8730, Batch Gradient Norm: 16.17909535931062
Epoch: 8730, Batch Gradient Norm after: 16.17909535931062
Epoch 8731/10000, Prediction Accuracy = 65.088%, Loss = 0.2844452321529388
Epoch: 8731, Batch Gradient Norm: 17.020071637779473
Epoch: 8731, Batch Gradient Norm after: 17.020071637779473
Epoch 8732/10000, Prediction Accuracy = 65.08%, Loss = 0.28551783561706545
Epoch: 8732, Batch Gradient Norm: 17.35545314394287
Epoch: 8732, Batch Gradient Norm after: 17.324266521472836
Epoch 8733/10000, Prediction Accuracy = 65.12800000000001%, Loss = 0.284989994764328
Epoch: 8733, Batch Gradient Norm: 18.6175084521911
Epoch: 8733, Batch Gradient Norm after: 18.152404295152287
Epoch 8734/10000, Prediction Accuracy = 65.256%, Loss = 0.28576103448867796
Epoch: 8734, Batch Gradient Norm: 19.756960776650644
Epoch: 8734, Batch Gradient Norm after: 18.11768246245249
Epoch 8735/10000, Prediction Accuracy = 65.204%, Loss = 0.28904551863670347
Epoch: 8735, Batch Gradient Norm: 14.605279940966367
Epoch: 8735, Batch Gradient Norm after: 14.605279940966367
Epoch 8736/10000, Prediction Accuracy = 65.17999999999999%, Loss = 0.28265859484672545
Epoch: 8736, Batch Gradient Norm: 13.71881296969578
Epoch: 8736, Batch Gradient Norm after: 13.71881296969578
Epoch 8737/10000, Prediction Accuracy = 65.22999999999999%, Loss = 0.2828665912151337
Epoch: 8737, Batch Gradient Norm: 14.116267129383084
Epoch: 8737, Batch Gradient Norm after: 14.116267129383084
Epoch 8738/10000, Prediction Accuracy = 65.22200000000001%, Loss = 0.28397979736328127
Epoch: 8738, Batch Gradient Norm: 16.642211143878907
Epoch: 8738, Batch Gradient Norm after: 16.642211143878907
Epoch 8739/10000, Prediction Accuracy = 65.104%, Loss = 0.2872910976409912
Epoch: 8739, Batch Gradient Norm: 12.272628229795092
Epoch: 8739, Batch Gradient Norm after: 12.272628229795092
Epoch 8740/10000, Prediction Accuracy = 65.16799999999999%, Loss = 0.282867032289505
Epoch: 8740, Batch Gradient Norm: 12.87842350145486
Epoch: 8740, Batch Gradient Norm after: 12.87842350145486
Epoch 8741/10000, Prediction Accuracy = 64.986%, Loss = 0.2857282876968384
Epoch: 8741, Batch Gradient Norm: 14.39758759129101
Epoch: 8741, Batch Gradient Norm after: 14.39758759129101
Epoch 8742/10000, Prediction Accuracy = 65.212%, Loss = 0.2807020664215088
Epoch: 8742, Batch Gradient Norm: 13.92235217333373
Epoch: 8742, Batch Gradient Norm after: 13.92235217333373
Epoch 8743/10000, Prediction Accuracy = 65.208%, Loss = 0.2807567358016968
Epoch: 8743, Batch Gradient Norm: 15.607472166290778
Epoch: 8743, Batch Gradient Norm after: 15.607472166290778
Epoch 8744/10000, Prediction Accuracy = 65.252%, Loss = 0.2805392622947693
Epoch: 8744, Batch Gradient Norm: 20.881017690782695
Epoch: 8744, Batch Gradient Norm after: 20.282517923337117
Epoch 8745/10000, Prediction Accuracy = 65.124%, Loss = 0.2912594318389893
Epoch: 8745, Batch Gradient Norm: 17.94066333749742
Epoch: 8745, Batch Gradient Norm after: 17.07490978977321
Epoch 8746/10000, Prediction Accuracy = 65.29%, Loss = 0.28416520953178404
Epoch: 8746, Batch Gradient Norm: 15.765853211349654
Epoch: 8746, Batch Gradient Norm after: 15.765853211349654
Epoch 8747/10000, Prediction Accuracy = 65.018%, Loss = 0.28676327466964724
Epoch: 8747, Batch Gradient Norm: 14.690278810627326
Epoch: 8747, Batch Gradient Norm after: 14.690278810627326
Epoch 8748/10000, Prediction Accuracy = 65.152%, Loss = 0.28356249928474425
Epoch: 8748, Batch Gradient Norm: 12.866686749933526
Epoch: 8748, Batch Gradient Norm after: 12.866686749933526
Epoch 8749/10000, Prediction Accuracy = 65.206%, Loss = 0.2785514831542969
Epoch: 8749, Batch Gradient Norm: 14.125218740040072
Epoch: 8749, Batch Gradient Norm after: 14.125218740040072
Epoch 8750/10000, Prediction Accuracy = 65.12199999999999%, Loss = 0.283761590719223
Epoch: 8750, Batch Gradient Norm: 13.961182396261004
Epoch: 8750, Batch Gradient Norm after: 13.961182396261004
Epoch 8751/10000, Prediction Accuracy = 64.926%, Loss = 0.2846617341041565
Epoch: 8751, Batch Gradient Norm: 15.816658546202408
Epoch: 8751, Batch Gradient Norm after: 15.816658546202408
Epoch 8752/10000, Prediction Accuracy = 65.28600000000002%, Loss = 0.28199819922447206
Epoch: 8752, Batch Gradient Norm: 19.876563736162332
Epoch: 8752, Batch Gradient Norm after: 18.682935911418646
Epoch 8753/10000, Prediction Accuracy = 65.086%, Loss = 0.28987611532211305
Epoch: 8753, Batch Gradient Norm: 17.065805865594335
Epoch: 8753, Batch Gradient Norm after: 17.065805865594335
Epoch 8754/10000, Prediction Accuracy = 65.08399999999999%, Loss = 0.2883103549480438
Epoch: 8754, Batch Gradient Norm: 14.835746743248384
Epoch: 8754, Batch Gradient Norm after: 14.835746743248384
Epoch 8755/10000, Prediction Accuracy = 65.29400000000001%, Loss = 0.28168281316757204
Epoch: 8755, Batch Gradient Norm: 15.134087665392299
Epoch: 8755, Batch Gradient Norm after: 15.134087665392299
Epoch 8756/10000, Prediction Accuracy = 65.244%, Loss = 0.283282595872879
Epoch: 8756, Batch Gradient Norm: 16.891349888519287
Epoch: 8756, Batch Gradient Norm after: 16.891349888519287
Epoch 8757/10000, Prediction Accuracy = 65.188%, Loss = 0.28569496870040895
Epoch: 8757, Batch Gradient Norm: 13.438140561716398
Epoch: 8757, Batch Gradient Norm after: 13.438140561716398
Epoch 8758/10000, Prediction Accuracy = 65.20000000000002%, Loss = 0.2800311505794525
Epoch: 8758, Batch Gradient Norm: 14.771026020090668
Epoch: 8758, Batch Gradient Norm after: 14.771026020090668
Epoch 8759/10000, Prediction Accuracy = 65.27000000000001%, Loss = 0.2821711838245392
Epoch: 8759, Batch Gradient Norm: 13.89550620261705
Epoch: 8759, Batch Gradient Norm after: 13.89550620261705
Epoch 8760/10000, Prediction Accuracy = 65.214%, Loss = 0.28081231117248534
Epoch: 8760, Batch Gradient Norm: 13.608410014019478
Epoch: 8760, Batch Gradient Norm after: 13.608410014019478
Epoch 8761/10000, Prediction Accuracy = 65.13399999999999%, Loss = 0.28211883902549745
Epoch: 8761, Batch Gradient Norm: 13.332011913540782
Epoch: 8761, Batch Gradient Norm after: 13.332011913540782
Epoch 8762/10000, Prediction Accuracy = 65.274%, Loss = 0.27990849018096925
Epoch: 8762, Batch Gradient Norm: 12.330662562285603
Epoch: 8762, Batch Gradient Norm after: 12.330662562285603
Epoch 8763/10000, Prediction Accuracy = 65.246%, Loss = 0.2825814068317413
Epoch: 8763, Batch Gradient Norm: 15.58200881396088
Epoch: 8763, Batch Gradient Norm after: 15.58200881396088
Epoch 8764/10000, Prediction Accuracy = 65.2%, Loss = 0.2832109272480011
Epoch: 8764, Batch Gradient Norm: 15.483770744288954
Epoch: 8764, Batch Gradient Norm after: 15.483770744288954
Epoch 8765/10000, Prediction Accuracy = 65.268%, Loss = 0.2837855935096741
Epoch: 8765, Batch Gradient Norm: 14.378700049098281
Epoch: 8765, Batch Gradient Norm after: 14.378700049098281
Epoch 8766/10000, Prediction Accuracy = 65.16400000000002%, Loss = 0.28373640179634096
Epoch: 8766, Batch Gradient Norm: 13.152239056600635
Epoch: 8766, Batch Gradient Norm after: 13.152239056600635
Epoch 8767/10000, Prediction Accuracy = 65.14599999999999%, Loss = 0.2833924233913422
Epoch: 8767, Batch Gradient Norm: 13.654108769680722
Epoch: 8767, Batch Gradient Norm after: 13.654108769680722
Epoch 8768/10000, Prediction Accuracy = 65.224%, Loss = 0.2834407567977905
Epoch: 8768, Batch Gradient Norm: 15.553102155688798
Epoch: 8768, Batch Gradient Norm after: 15.553102155688798
Epoch 8769/10000, Prediction Accuracy = 65.114%, Loss = 0.2841149091720581
Epoch: 8769, Batch Gradient Norm: 14.565179791539741
Epoch: 8769, Batch Gradient Norm after: 14.565179791539741
Epoch 8770/10000, Prediction Accuracy = 65.152%, Loss = 0.28261190056800845
Epoch: 8770, Batch Gradient Norm: 14.436168088075908
Epoch: 8770, Batch Gradient Norm after: 14.436168088075908
Epoch 8771/10000, Prediction Accuracy = 65.06599999999999%, Loss = 0.28180516958236695
Epoch: 8771, Batch Gradient Norm: 12.544167129560213
Epoch: 8771, Batch Gradient Norm after: 12.544167129560213
Epoch 8772/10000, Prediction Accuracy = 65.19800000000001%, Loss = 0.277374678850174
Epoch: 8772, Batch Gradient Norm: 14.645757314116302
Epoch: 8772, Batch Gradient Norm after: 14.645757314116302
Epoch 8773/10000, Prediction Accuracy = 65.106%, Loss = 0.2831540048122406
Epoch: 8773, Batch Gradient Norm: 12.144347422917148
Epoch: 8773, Batch Gradient Norm after: 12.144347422917148
Epoch 8774/10000, Prediction Accuracy = 65.164%, Loss = 0.27905644178390504
Epoch: 8774, Batch Gradient Norm: 13.232222113528215
Epoch: 8774, Batch Gradient Norm after: 13.232222113528215
Epoch 8775/10000, Prediction Accuracy = 65.44399999999999%, Loss = 0.27826192378997805
Epoch: 8775, Batch Gradient Norm: 12.735095677048733
Epoch: 8775, Batch Gradient Norm after: 12.735095677048733
Epoch 8776/10000, Prediction Accuracy = 65.184%, Loss = 0.27962899804115293
Epoch: 8776, Batch Gradient Norm: 14.00124601721757
Epoch: 8776, Batch Gradient Norm after: 14.00124601721757
Epoch 8777/10000, Prediction Accuracy = 65.098%, Loss = 0.2824378848075867
Epoch: 8777, Batch Gradient Norm: 13.179872332093382
Epoch: 8777, Batch Gradient Norm after: 13.179872332093382
Epoch 8778/10000, Prediction Accuracy = 65.10600000000001%, Loss = 0.28166874647140505
Epoch: 8778, Batch Gradient Norm: 18.648496866767044
Epoch: 8778, Batch Gradient Norm after: 18.360781310335295
Epoch 8779/10000, Prediction Accuracy = 65.176%, Loss = 0.28453243970870973
Epoch: 8779, Batch Gradient Norm: 14.787166933600737
Epoch: 8779, Batch Gradient Norm after: 14.787166933600737
Epoch 8780/10000, Prediction Accuracy = 65.192%, Loss = 0.2805663704872131
Epoch: 8780, Batch Gradient Norm: 15.351010156074956
Epoch: 8780, Batch Gradient Norm after: 15.351010156074956
Epoch 8781/10000, Prediction Accuracy = 65.23%, Loss = 0.28269082903862
Epoch: 8781, Batch Gradient Norm: 13.574149863279795
Epoch: 8781, Batch Gradient Norm after: 13.574149863279795
Epoch 8782/10000, Prediction Accuracy = 65.30199999999999%, Loss = 0.2797576427459717
Epoch: 8782, Batch Gradient Norm: 14.790784596272287
Epoch: 8782, Batch Gradient Norm after: 14.790784596272287
Epoch 8783/10000, Prediction Accuracy = 65.21200000000002%, Loss = 0.28339475989341734
Epoch: 8783, Batch Gradient Norm: 17.24005208289914
Epoch: 8783, Batch Gradient Norm after: 17.135464403921212
Epoch 8784/10000, Prediction Accuracy = 65.238%, Loss = 0.2861282229423523
Epoch: 8784, Batch Gradient Norm: 20.377441719740588
Epoch: 8784, Batch Gradient Norm after: 19.978473665084525
Epoch 8785/10000, Prediction Accuracy = 65.19399999999999%, Loss = 0.2891547024250031
Epoch: 8785, Batch Gradient Norm: 16.84474391044482
Epoch: 8785, Batch Gradient Norm after: 16.84474391044482
Epoch 8786/10000, Prediction Accuracy = 65.14200000000001%, Loss = 0.28554423451423644
Epoch: 8786, Batch Gradient Norm: 17.80215707582383
Epoch: 8786, Batch Gradient Norm after: 17.790813588820615
Epoch 8787/10000, Prediction Accuracy = 65.32199999999999%, Loss = 0.2870500206947327
Epoch: 8787, Batch Gradient Norm: 15.797914786680694
Epoch: 8787, Batch Gradient Norm after: 15.797914786680694
Epoch 8788/10000, Prediction Accuracy = 65.114%, Loss = 0.28696736693382263
Epoch: 8788, Batch Gradient Norm: 16.077108848978945
Epoch: 8788, Batch Gradient Norm after: 16.077108848978945
Epoch 8789/10000, Prediction Accuracy = 65.088%, Loss = 0.28315072059631347
Epoch: 8789, Batch Gradient Norm: 16.224046548554956
Epoch: 8789, Batch Gradient Norm after: 16.224046548554956
Epoch 8790/10000, Prediction Accuracy = 65.166%, Loss = 0.2840030133724213
Epoch: 8790, Batch Gradient Norm: 14.964533219678106
Epoch: 8790, Batch Gradient Norm after: 14.964533219678106
Epoch 8791/10000, Prediction Accuracy = 65.15%, Loss = 0.2820778667926788
Epoch: 8791, Batch Gradient Norm: 13.929705393126017
Epoch: 8791, Batch Gradient Norm after: 13.929705393126017
Epoch 8792/10000, Prediction Accuracy = 65.176%, Loss = 0.2815368115901947
Epoch: 8792, Batch Gradient Norm: 12.566758951441217
Epoch: 8792, Batch Gradient Norm after: 12.566758951441217
Epoch 8793/10000, Prediction Accuracy = 65.12800000000001%, Loss = 0.281681627035141
Epoch: 8793, Batch Gradient Norm: 12.437180213479676
Epoch: 8793, Batch Gradient Norm after: 12.437180213479676
Epoch 8794/10000, Prediction Accuracy = 65.056%, Loss = 0.28112266063690183
Epoch: 8794, Batch Gradient Norm: 13.598267524053593
Epoch: 8794, Batch Gradient Norm after: 13.598267524053593
Epoch 8795/10000, Prediction Accuracy = 65.102%, Loss = 0.28272231221199035
Epoch: 8795, Batch Gradient Norm: 15.433955551639263
Epoch: 8795, Batch Gradient Norm after: 15.433955551639263
Epoch 8796/10000, Prediction Accuracy = 65.348%, Loss = 0.2821527779102325
Epoch: 8796, Batch Gradient Norm: 17.98375458003301
Epoch: 8796, Batch Gradient Norm after: 17.98375458003301
Epoch 8797/10000, Prediction Accuracy = 65.118%, Loss = 0.2871615350246429
Epoch: 8797, Batch Gradient Norm: 14.743850631957764
Epoch: 8797, Batch Gradient Norm after: 14.743850631957764
Epoch 8798/10000, Prediction Accuracy = 65.092%, Loss = 0.28070977330207825
Epoch: 8798, Batch Gradient Norm: 12.215279038022475
Epoch: 8798, Batch Gradient Norm after: 12.215279038022475
Epoch 8799/10000, Prediction Accuracy = 65.14000000000001%, Loss = 0.28060402870178225
Epoch: 8799, Batch Gradient Norm: 12.41582381655036
Epoch: 8799, Batch Gradient Norm after: 12.41582381655036
Epoch 8800/10000, Prediction Accuracy = 65.248%, Loss = 0.27861266732215884
Epoch: 8800, Batch Gradient Norm: 13.692573576012073
Epoch: 8800, Batch Gradient Norm after: 13.692573576012073
Epoch 8801/10000, Prediction Accuracy = 65.156%, Loss = 0.28224616646766665
Epoch: 8801, Batch Gradient Norm: 15.648792137532357
Epoch: 8801, Batch Gradient Norm after: 15.648792137532357
Epoch 8802/10000, Prediction Accuracy = 65.196%, Loss = 0.2848917365074158
Epoch: 8802, Batch Gradient Norm: 16.63159068116714
Epoch: 8802, Batch Gradient Norm after: 16.63159068116714
Epoch 8803/10000, Prediction Accuracy = 65.05%, Loss = 0.2852666199207306
Epoch: 8803, Batch Gradient Norm: 12.726702748232755
Epoch: 8803, Batch Gradient Norm after: 12.726702748232755
Epoch 8804/10000, Prediction Accuracy = 65.27799999999999%, Loss = 0.2805844068527222
Epoch: 8804, Batch Gradient Norm: 13.207344059643686
Epoch: 8804, Batch Gradient Norm after: 13.207344059643686
Epoch 8805/10000, Prediction Accuracy = 65.27%, Loss = 0.2799078166484833
Epoch: 8805, Batch Gradient Norm: 10.037676390176506
Epoch: 8805, Batch Gradient Norm after: 10.037676390176506
Epoch 8806/10000, Prediction Accuracy = 65.30600000000001%, Loss = 0.27533688545227053
Epoch: 8806, Batch Gradient Norm: 11.812133795302831
Epoch: 8806, Batch Gradient Norm after: 11.812133795302831
Epoch 8807/10000, Prediction Accuracy = 65.186%, Loss = 0.28075518608093264
Epoch: 8807, Batch Gradient Norm: 12.888126160715593
Epoch: 8807, Batch Gradient Norm after: 12.888126160715593
Epoch 8808/10000, Prediction Accuracy = 65.29%, Loss = 0.2792221486568451
Epoch: 8808, Batch Gradient Norm: 12.730823430162788
Epoch: 8808, Batch Gradient Norm after: 12.730823430162788
Epoch 8809/10000, Prediction Accuracy = 65.27000000000001%, Loss = 0.27874534726142886
Epoch: 8809, Batch Gradient Norm: 12.743137347592983
Epoch: 8809, Batch Gradient Norm after: 12.743137347592983
Epoch 8810/10000, Prediction Accuracy = 65.16399999999999%, Loss = 0.27888927459716795
Epoch: 8810, Batch Gradient Norm: 14.357755416145855
Epoch: 8810, Batch Gradient Norm after: 14.357755416145855
Epoch 8811/10000, Prediction Accuracy = 65.35999999999999%, Loss = 0.280802971124649
Epoch: 8811, Batch Gradient Norm: 13.513041857524891
Epoch: 8811, Batch Gradient Norm after: 13.513041857524891
Epoch 8812/10000, Prediction Accuracy = 65.22800000000001%, Loss = 0.27843462228775023
Epoch: 8812, Batch Gradient Norm: 15.9038289726332
Epoch: 8812, Batch Gradient Norm after: 15.9038289726332
Epoch 8813/10000, Prediction Accuracy = 65.228%, Loss = 0.28413293361663816
Epoch: 8813, Batch Gradient Norm: 14.065339667752466
Epoch: 8813, Batch Gradient Norm after: 14.065339667752466
Epoch 8814/10000, Prediction Accuracy = 65.182%, Loss = 0.28451297879219056
Epoch: 8814, Batch Gradient Norm: 12.410072938304362
Epoch: 8814, Batch Gradient Norm after: 12.410072938304362
Epoch 8815/10000, Prediction Accuracy = 65.21399999999998%, Loss = 0.2788800299167633
Epoch: 8815, Batch Gradient Norm: 15.425300883291376
Epoch: 8815, Batch Gradient Norm after: 15.425300883291376
Epoch 8816/10000, Prediction Accuracy = 65.13199999999999%, Loss = 0.2836239397525787
Epoch: 8816, Batch Gradient Norm: 17.558957569178965
Epoch: 8816, Batch Gradient Norm after: 17.558957569178965
Epoch 8817/10000, Prediction Accuracy = 65.132%, Loss = 0.28940104842185976
Epoch: 8817, Batch Gradient Norm: 17.884822572130748
Epoch: 8817, Batch Gradient Norm after: 17.790677547602257
Epoch 8818/10000, Prediction Accuracy = 65.108%, Loss = 0.28565216064453125
Epoch: 8818, Batch Gradient Norm: 14.672253326497865
Epoch: 8818, Batch Gradient Norm after: 14.672253326497865
Epoch 8819/10000, Prediction Accuracy = 65.214%, Loss = 0.28105451464653014
Epoch: 8819, Batch Gradient Norm: 13.174648932927514
Epoch: 8819, Batch Gradient Norm after: 13.174648932927514
Epoch 8820/10000, Prediction Accuracy = 65.19399999999999%, Loss = 0.28023576736450195
Epoch: 8820, Batch Gradient Norm: 12.23170163852095
Epoch: 8820, Batch Gradient Norm after: 12.23170163852095
Epoch 8821/10000, Prediction Accuracy = 65.204%, Loss = 0.27872451543807986
Epoch: 8821, Batch Gradient Norm: 10.767126236970157
Epoch: 8821, Batch Gradient Norm after: 10.767126236970157
Epoch 8822/10000, Prediction Accuracy = 65.288%, Loss = 0.27784045338630675
Epoch: 8822, Batch Gradient Norm: 11.884108817539179
Epoch: 8822, Batch Gradient Norm after: 11.884108817539179
Epoch 8823/10000, Prediction Accuracy = 65.352%, Loss = 0.27796446681022646
Epoch: 8823, Batch Gradient Norm: 15.085943106069644
Epoch: 8823, Batch Gradient Norm after: 15.085943106069644
Epoch 8824/10000, Prediction Accuracy = 65.26199999999999%, Loss = 0.28162142634391785
Epoch: 8824, Batch Gradient Norm: 16.47566172514804
Epoch: 8824, Batch Gradient Norm after: 16.47566172514804
Epoch 8825/10000, Prediction Accuracy = 65.20599999999999%, Loss = 0.2812669396400452
Epoch: 8825, Batch Gradient Norm: 14.740345717051003
Epoch: 8825, Batch Gradient Norm after: 14.740345717051003
Epoch 8826/10000, Prediction Accuracy = 65.25%, Loss = 0.27986271381378175
Epoch: 8826, Batch Gradient Norm: 15.604139568773173
Epoch: 8826, Batch Gradient Norm after: 15.604139568773173
Epoch 8827/10000, Prediction Accuracy = 65.016%, Loss = 0.2820910930633545
Epoch: 8827, Batch Gradient Norm: 15.657346898210957
Epoch: 8827, Batch Gradient Norm after: 15.657346898210957
Epoch 8828/10000, Prediction Accuracy = 65.254%, Loss = 0.2852628767490387
Epoch: 8828, Batch Gradient Norm: 14.654930749298899
Epoch: 8828, Batch Gradient Norm after: 14.654930749298899
Epoch 8829/10000, Prediction Accuracy = 65.288%, Loss = 0.28202722072601316
Epoch: 8829, Batch Gradient Norm: 15.484033585080478
Epoch: 8829, Batch Gradient Norm after: 15.484033585080478
Epoch 8830/10000, Prediction Accuracy = 65.348%, Loss = 0.2815811216831207
Epoch: 8830, Batch Gradient Norm: 13.772147455518635
Epoch: 8830, Batch Gradient Norm after: 13.772147455518635
Epoch 8831/10000, Prediction Accuracy = 65.21000000000001%, Loss = 0.28125251531600953
Epoch: 8831, Batch Gradient Norm: 16.876636766300532
Epoch: 8831, Batch Gradient Norm after: 16.876636766300532
Epoch 8832/10000, Prediction Accuracy = 65.292%, Loss = 0.28315929174423216
Epoch: 8832, Batch Gradient Norm: 15.752317136025763
Epoch: 8832, Batch Gradient Norm after: 15.752317136025763
Epoch 8833/10000, Prediction Accuracy = 65.226%, Loss = 0.2812153100967407
Epoch: 8833, Batch Gradient Norm: 15.320278821473737
Epoch: 8833, Batch Gradient Norm after: 15.320278821473737
Epoch 8834/10000, Prediction Accuracy = 65.088%, Loss = 0.2813100218772888
Epoch: 8834, Batch Gradient Norm: 16.63815073989099
Epoch: 8834, Batch Gradient Norm after: 16.63815073989099
Epoch 8835/10000, Prediction Accuracy = 65.106%, Loss = 0.28494819402694704
Epoch: 8835, Batch Gradient Norm: 18.29743769313014
Epoch: 8835, Batch Gradient Norm after: 18.29743769313014
Epoch 8836/10000, Prediction Accuracy = 65.058%, Loss = 0.2879145979881287
Epoch: 8836, Batch Gradient Norm: 16.78046441825783
Epoch: 8836, Batch Gradient Norm after: 16.78046441825783
Epoch 8837/10000, Prediction Accuracy = 65.17800000000001%, Loss = 0.28477766513824465
Epoch: 8837, Batch Gradient Norm: 14.433913373137036
Epoch: 8837, Batch Gradient Norm after: 14.433913373137036
Epoch 8838/10000, Prediction Accuracy = 65.344%, Loss = 0.2806843757629395
Epoch: 8838, Batch Gradient Norm: 12.487795215993899
Epoch: 8838, Batch Gradient Norm after: 12.487795215993899
Epoch 8839/10000, Prediction Accuracy = 65.22200000000001%, Loss = 0.2788736164569855
Epoch: 8839, Batch Gradient Norm: 14.868190665704882
Epoch: 8839, Batch Gradient Norm after: 14.868190665704882
Epoch 8840/10000, Prediction Accuracy = 65.15%, Loss = 0.2805492341518402
Epoch: 8840, Batch Gradient Norm: 12.22478551497313
Epoch: 8840, Batch Gradient Norm after: 12.22478551497313
Epoch 8841/10000, Prediction Accuracy = 65.324%, Loss = 0.27848888635635377
Epoch: 8841, Batch Gradient Norm: 14.820640335380212
Epoch: 8841, Batch Gradient Norm after: 14.820640335380212
Epoch 8842/10000, Prediction Accuracy = 65.186%, Loss = 0.2813809335231781
Epoch: 8842, Batch Gradient Norm: 17.551543887940593
Epoch: 8842, Batch Gradient Norm after: 16.972280282524558
Epoch 8843/10000, Prediction Accuracy = 65.128%, Loss = 0.28521170616149905
Epoch: 8843, Batch Gradient Norm: 17.99278967716937
Epoch: 8843, Batch Gradient Norm after: 17.99278967716937
Epoch 8844/10000, Prediction Accuracy = 65.24199999999999%, Loss = 0.28210778832435607
Epoch: 8844, Batch Gradient Norm: 18.02006984716591
Epoch: 8844, Batch Gradient Norm after: 18.02006984716591
Epoch 8845/10000, Prediction Accuracy = 65.066%, Loss = 0.2871553063392639
Epoch: 8845, Batch Gradient Norm: 18.463070441593118
Epoch: 8845, Batch Gradient Norm after: 18.04298606139407
Epoch 8846/10000, Prediction Accuracy = 65.09400000000001%, Loss = 0.2874265193939209
Epoch: 8846, Batch Gradient Norm: 19.287353502117995
Epoch: 8846, Batch Gradient Norm after: 19.18994483587866
Epoch 8847/10000, Prediction Accuracy = 65.14%, Loss = 0.2867411315441132
Epoch: 8847, Batch Gradient Norm: 16.63628069425744
Epoch: 8847, Batch Gradient Norm after: 16.63628069425744
Epoch 8848/10000, Prediction Accuracy = 65.054%, Loss = 0.2840971887111664
Epoch: 8848, Batch Gradient Norm: 15.946949505422984
Epoch: 8848, Batch Gradient Norm after: 15.946949505422984
Epoch 8849/10000, Prediction Accuracy = 65.10599999999998%, Loss = 0.28121548891067505
Epoch: 8849, Batch Gradient Norm: 17.61001185304043
Epoch: 8849, Batch Gradient Norm after: 17.61001185304043
Epoch 8850/10000, Prediction Accuracy = 65.04599999999999%, Loss = 0.28758063316345217
Epoch: 8850, Batch Gradient Norm: 14.108546648718159
Epoch: 8850, Batch Gradient Norm after: 14.108546648718159
Epoch 8851/10000, Prediction Accuracy = 65.20000000000002%, Loss = 0.28102715611457824
Epoch: 8851, Batch Gradient Norm: 15.651169476240703
Epoch: 8851, Batch Gradient Norm after: 15.651169476240703
Epoch 8852/10000, Prediction Accuracy = 65.21799999999999%, Loss = 0.2808358371257782
Epoch: 8852, Batch Gradient Norm: 14.243860995224203
Epoch: 8852, Batch Gradient Norm after: 14.243860995224203
Epoch 8853/10000, Prediction Accuracy = 65.11600000000001%, Loss = 0.2828972816467285
Epoch: 8853, Batch Gradient Norm: 17.69180605651271
Epoch: 8853, Batch Gradient Norm after: 17.508430456936896
Epoch 8854/10000, Prediction Accuracy = 65.21399999999998%, Loss = 0.2842546463012695
Epoch: 8854, Batch Gradient Norm: 16.758386958679257
Epoch: 8854, Batch Gradient Norm after: 16.758386958679257
Epoch 8855/10000, Prediction Accuracy = 65.066%, Loss = 0.28387044072151185
Epoch: 8855, Batch Gradient Norm: 15.518736602513792
Epoch: 8855, Batch Gradient Norm after: 15.518736602513792
Epoch 8856/10000, Prediction Accuracy = 65.02%, Loss = 0.2825871706008911
Epoch: 8856, Batch Gradient Norm: 15.309408406753017
Epoch: 8856, Batch Gradient Norm after: 15.309408406753017
Epoch 8857/10000, Prediction Accuracy = 65.06800000000001%, Loss = 0.2819461107254028
Epoch: 8857, Batch Gradient Norm: 14.535485601878674
Epoch: 8857, Batch Gradient Norm after: 14.535485601878674
Epoch 8858/10000, Prediction Accuracy = 65.188%, Loss = 0.28132851123809816
Epoch: 8858, Batch Gradient Norm: 16.96315696023553
Epoch: 8858, Batch Gradient Norm after: 16.96315696023553
Epoch 8859/10000, Prediction Accuracy = 65.29799999999999%, Loss = 0.2856284320354462
Epoch: 8859, Batch Gradient Norm: 16.8737600335791
Epoch: 8859, Batch Gradient Norm after: 16.8737600335791
Epoch 8860/10000, Prediction Accuracy = 65.256%, Loss = 0.2831889927387238
Epoch: 8860, Batch Gradient Norm: 14.540162472325996
Epoch: 8860, Batch Gradient Norm after: 14.540162472325996
Epoch 8861/10000, Prediction Accuracy = 65.18599999999999%, Loss = 0.28110278248786924
Epoch: 8861, Batch Gradient Norm: 15.824180698846165
Epoch: 8861, Batch Gradient Norm after: 15.824180698846165
Epoch 8862/10000, Prediction Accuracy = 65.31800000000001%, Loss = 0.2832192361354828
Epoch: 8862, Batch Gradient Norm: 13.595983915922588
Epoch: 8862, Batch Gradient Norm after: 13.595983915922588
Epoch 8863/10000, Prediction Accuracy = 65.316%, Loss = 0.27963054180145264
Epoch: 8863, Batch Gradient Norm: 15.836410130087838
Epoch: 8863, Batch Gradient Norm after: 15.836410130087838
Epoch 8864/10000, Prediction Accuracy = 65.16%, Loss = 0.28233707547187803
Epoch: 8864, Batch Gradient Norm: 18.63986247034846
Epoch: 8864, Batch Gradient Norm after: 18.63986247034846
Epoch 8865/10000, Prediction Accuracy = 65.01400000000001%, Loss = 0.28936620950698855
Epoch: 8865, Batch Gradient Norm: 17.659212281921864
Epoch: 8865, Batch Gradient Norm after: 17.659212281921864
Epoch 8866/10000, Prediction Accuracy = 65.16999999999999%, Loss = 0.28269365429878235
Epoch: 8866, Batch Gradient Norm: 17.166456662144363
Epoch: 8866, Batch Gradient Norm after: 17.166456662144363
Epoch 8867/10000, Prediction Accuracy = 65.208%, Loss = 0.286185497045517
Epoch: 8867, Batch Gradient Norm: 13.104745782153264
Epoch: 8867, Batch Gradient Norm after: 13.104745782153264
Epoch 8868/10000, Prediction Accuracy = 65.072%, Loss = 0.28142794966697693
Epoch: 8868, Batch Gradient Norm: 13.339370241772748
Epoch: 8868, Batch Gradient Norm after: 13.339370241772748
Epoch 8869/10000, Prediction Accuracy = 65.272%, Loss = 0.279289311170578
Epoch: 8869, Batch Gradient Norm: 14.035525518942947
Epoch: 8869, Batch Gradient Norm after: 14.035525518942947
Epoch 8870/10000, Prediction Accuracy = 65.254%, Loss = 0.2795876681804657
Epoch: 8870, Batch Gradient Norm: 14.049636207813537
Epoch: 8870, Batch Gradient Norm after: 14.049636207813537
Epoch 8871/10000, Prediction Accuracy = 65.2%, Loss = 0.27881709337234495
Epoch: 8871, Batch Gradient Norm: 16.892687045452213
Epoch: 8871, Batch Gradient Norm after: 16.892687045452213
Epoch 8872/10000, Prediction Accuracy = 65.268%, Loss = 0.28488911390304567
Epoch: 8872, Batch Gradient Norm: 16.9830359333222
Epoch: 8872, Batch Gradient Norm after: 16.9830359333222
Epoch 8873/10000, Prediction Accuracy = 65.226%, Loss = 0.2838039815425873
Epoch: 8873, Batch Gradient Norm: 13.115731664595147
Epoch: 8873, Batch Gradient Norm after: 13.115731664595147
Epoch 8874/10000, Prediction Accuracy = 65.272%, Loss = 0.2804435849189758
Epoch: 8874, Batch Gradient Norm: 13.364879836302178
Epoch: 8874, Batch Gradient Norm after: 13.364879836302178
Epoch 8875/10000, Prediction Accuracy = 65.19%, Loss = 0.2788710057735443
Epoch: 8875, Batch Gradient Norm: 15.518532867780708
Epoch: 8875, Batch Gradient Norm after: 15.518532867780708
Epoch 8876/10000, Prediction Accuracy = 65.254%, Loss = 0.28253265023231505
Epoch: 8876, Batch Gradient Norm: 15.270738084995678
Epoch: 8876, Batch Gradient Norm after: 15.270738084995678
Epoch 8877/10000, Prediction Accuracy = 65.23%, Loss = 0.28286500573158263
Epoch: 8877, Batch Gradient Norm: 17.610733095179583
Epoch: 8877, Batch Gradient Norm after: 17.610733095179583
Epoch 8878/10000, Prediction Accuracy = 65.224%, Loss = 0.2831088900566101
Epoch: 8878, Batch Gradient Norm: 18.2504620409115
Epoch: 8878, Batch Gradient Norm after: 18.2504620409115
Epoch 8879/10000, Prediction Accuracy = 65.214%, Loss = 0.28424403071403503
Epoch: 8879, Batch Gradient Norm: 17.298770333431868
Epoch: 8879, Batch Gradient Norm after: 17.298770333431868
Epoch 8880/10000, Prediction Accuracy = 65.056%, Loss = 0.2852224111557007
Epoch: 8880, Batch Gradient Norm: 18.69298910227465
Epoch: 8880, Batch Gradient Norm after: 18.2726552781972
Epoch 8881/10000, Prediction Accuracy = 65.202%, Loss = 0.28487821817398074
Epoch: 8881, Batch Gradient Norm: 14.812519358054361
Epoch: 8881, Batch Gradient Norm after: 14.812519358054361
Epoch 8882/10000, Prediction Accuracy = 65.37199999999999%, Loss = 0.2794420659542084
Epoch: 8882, Batch Gradient Norm: 18.344694723422208
Epoch: 8882, Batch Gradient Norm after: 18.22500328245217
Epoch 8883/10000, Prediction Accuracy = 65.152%, Loss = 0.28568974137306213
Epoch: 8883, Batch Gradient Norm: 15.83585072225307
Epoch: 8883, Batch Gradient Norm after: 15.83585072225307
Epoch 8884/10000, Prediction Accuracy = 65.196%, Loss = 0.2829982876777649
Epoch: 8884, Batch Gradient Norm: 16.008169171695346
Epoch: 8884, Batch Gradient Norm after: 16.008169171695346
Epoch 8885/10000, Prediction Accuracy = 65.128%, Loss = 0.2815353453159332
Epoch: 8885, Batch Gradient Norm: 17.428640506234927
Epoch: 8885, Batch Gradient Norm after: 17.428640506234927
Epoch 8886/10000, Prediction Accuracy = 65.234%, Loss = 0.28314988017082215
Epoch: 8886, Batch Gradient Norm: 16.667379318871344
Epoch: 8886, Batch Gradient Norm after: 16.667379318871344
Epoch 8887/10000, Prediction Accuracy = 65.202%, Loss = 0.28210513591766356
Epoch: 8887, Batch Gradient Norm: 15.920003373341967
Epoch: 8887, Batch Gradient Norm after: 15.920003373341967
Epoch 8888/10000, Prediction Accuracy = 65.306%, Loss = 0.2829758048057556
Epoch: 8888, Batch Gradient Norm: 15.771333317885388
Epoch: 8888, Batch Gradient Norm after: 15.771333317885388
Epoch 8889/10000, Prediction Accuracy = 65.196%, Loss = 0.2823028266429901
Epoch: 8889, Batch Gradient Norm: 16.48294011886031
Epoch: 8889, Batch Gradient Norm after: 16.48294011886031
Epoch 8890/10000, Prediction Accuracy = 65.21399999999998%, Loss = 0.2821357011795044
Epoch: 8890, Batch Gradient Norm: 14.78140197673732
Epoch: 8890, Batch Gradient Norm after: 14.78140197673732
Epoch 8891/10000, Prediction Accuracy = 65.176%, Loss = 0.2807121634483337
Epoch: 8891, Batch Gradient Norm: 15.580471375417977
Epoch: 8891, Batch Gradient Norm after: 15.580471375417977
Epoch 8892/10000, Prediction Accuracy = 65.178%, Loss = 0.2810563981533051
Epoch: 8892, Batch Gradient Norm: 13.90307507588202
Epoch: 8892, Batch Gradient Norm after: 13.90307507588202
Epoch 8893/10000, Prediction Accuracy = 65.198%, Loss = 0.28180308938026427
Epoch: 8893, Batch Gradient Norm: 14.943401014395198
Epoch: 8893, Batch Gradient Norm after: 14.943401014395198
Epoch 8894/10000, Prediction Accuracy = 65.348%, Loss = 0.2829644620418549
Epoch: 8894, Batch Gradient Norm: 17.664785322877787
Epoch: 8894, Batch Gradient Norm after: 17.664785322877787
Epoch 8895/10000, Prediction Accuracy = 65.24%, Loss = 0.28366572260856626
Epoch: 8895, Batch Gradient Norm: 13.676537106339607
Epoch: 8895, Batch Gradient Norm after: 13.676537106339607
Epoch 8896/10000, Prediction Accuracy = 65.41799999999999%, Loss = 0.27687368392944334
Epoch: 8896, Batch Gradient Norm: 16.261060885241516
Epoch: 8896, Batch Gradient Norm after: 16.261060885241516
Epoch 8897/10000, Prediction Accuracy = 65.32%, Loss = 0.2797561526298523
Epoch: 8897, Batch Gradient Norm: 14.682347656457821
Epoch: 8897, Batch Gradient Norm after: 14.682347656457821
Epoch 8898/10000, Prediction Accuracy = 65.184%, Loss = 0.2802082121372223
Epoch: 8898, Batch Gradient Norm: 13.374488219863329
Epoch: 8898, Batch Gradient Norm after: 13.374488219863329
Epoch 8899/10000, Prediction Accuracy = 65.322%, Loss = 0.27831771969795227
Epoch: 8899, Batch Gradient Norm: 12.318143271315257
Epoch: 8899, Batch Gradient Norm after: 12.318143271315257
Epoch 8900/10000, Prediction Accuracy = 65.35%, Loss = 0.2781973123550415
Epoch: 8900, Batch Gradient Norm: 14.68707889071428
Epoch: 8900, Batch Gradient Norm after: 14.68707889071428
Epoch 8901/10000, Prediction Accuracy = 65.236%, Loss = 0.27877299189567567
Epoch: 8901, Batch Gradient Norm: 15.955741279136335
Epoch: 8901, Batch Gradient Norm after: 15.955741279136335
Epoch 8902/10000, Prediction Accuracy = 65.25%, Loss = 0.2829130172729492
Epoch: 8902, Batch Gradient Norm: 15.349862219898286
Epoch: 8902, Batch Gradient Norm after: 15.349862219898286
Epoch 8903/10000, Prediction Accuracy = 65.324%, Loss = 0.28234370350837706
Epoch: 8903, Batch Gradient Norm: 15.386273108742325
Epoch: 8903, Batch Gradient Norm after: 15.386273108742325
Epoch 8904/10000, Prediction Accuracy = 65.20399999999998%, Loss = 0.27992063760757446
Epoch: 8904, Batch Gradient Norm: 12.906965763415993
Epoch: 8904, Batch Gradient Norm after: 12.906965763415993
Epoch 8905/10000, Prediction Accuracy = 65.368%, Loss = 0.277873158454895
Epoch: 8905, Batch Gradient Norm: 12.746403196966495
Epoch: 8905, Batch Gradient Norm after: 12.746403196966495
Epoch 8906/10000, Prediction Accuracy = 65.208%, Loss = 0.2764564216136932
Epoch: 8906, Batch Gradient Norm: 13.089658813090015
Epoch: 8906, Batch Gradient Norm after: 13.089658813090015
Epoch 8907/10000, Prediction Accuracy = 65.352%, Loss = 0.27836745977401733
Epoch: 8907, Batch Gradient Norm: 15.580453864898146
Epoch: 8907, Batch Gradient Norm after: 15.580453864898146
Epoch 8908/10000, Prediction Accuracy = 65.436%, Loss = 0.27964194416999816
Epoch: 8908, Batch Gradient Norm: 19.426366352877505
Epoch: 8908, Batch Gradient Norm after: 18.33372994983344
Epoch 8909/10000, Prediction Accuracy = 65.3%, Loss = 0.2870252072811127
Epoch: 8909, Batch Gradient Norm: 20.94509569161224
Epoch: 8909, Batch Gradient Norm after: 19.340049774892027
Epoch 8910/10000, Prediction Accuracy = 65.244%, Loss = 0.2904277861118317
Epoch: 8910, Batch Gradient Norm: 24.41088516926012
Epoch: 8910, Batch Gradient Norm after: 21.06694256614376
Epoch 8911/10000, Prediction Accuracy = 65.036%, Loss = 0.294097501039505
Epoch: 8911, Batch Gradient Norm: 20.652756893806167
Epoch: 8911, Batch Gradient Norm after: 18.80993139178572
Epoch 8912/10000, Prediction Accuracy = 65.106%, Loss = 0.2901990532875061
Epoch: 8912, Batch Gradient Norm: 17.35693555130189
Epoch: 8912, Batch Gradient Norm after: 17.35693555130189
Epoch 8913/10000, Prediction Accuracy = 65.18999999999998%, Loss = 0.28289785981178284
Epoch: 8913, Batch Gradient Norm: 20.070151884334646
Epoch: 8913, Batch Gradient Norm after: 19.157261682227347
Epoch 8914/10000, Prediction Accuracy = 65.146%, Loss = 0.289668607711792
Epoch: 8914, Batch Gradient Norm: 20.04269073501135
Epoch: 8914, Batch Gradient Norm after: 19.069343164538058
Epoch 8915/10000, Prediction Accuracy = 65.18800000000002%, Loss = 0.2900651156902313
Epoch: 8915, Batch Gradient Norm: 19.467147274479636
Epoch: 8915, Batch Gradient Norm after: 19.379999633866053
Epoch 8916/10000, Prediction Accuracy = 65.226%, Loss = 0.2872914493083954
Epoch: 8916, Batch Gradient Norm: 21.167598350060768
Epoch: 8916, Batch Gradient Norm after: 19.981425817326024
Epoch 8917/10000, Prediction Accuracy = 65.168%, Loss = 0.2904861867427826
Epoch: 8917, Batch Gradient Norm: 16.965412658388594
Epoch: 8917, Batch Gradient Norm after: 16.965412658388594
Epoch 8918/10000, Prediction Accuracy = 65.24%, Loss = 0.2833147406578064
Epoch: 8918, Batch Gradient Norm: 15.096538261582888
Epoch: 8918, Batch Gradient Norm after: 15.096538261582888
Epoch 8919/10000, Prediction Accuracy = 65.026%, Loss = 0.28107708096504214
Epoch: 8919, Batch Gradient Norm: 14.399142547202993
Epoch: 8919, Batch Gradient Norm after: 14.399142547202993
Epoch 8920/10000, Prediction Accuracy = 65.15599999999999%, Loss = 0.27909674048423766
Epoch: 8920, Batch Gradient Norm: 13.452713099959631
Epoch: 8920, Batch Gradient Norm after: 13.452713099959631
Epoch 8921/10000, Prediction Accuracy = 65.252%, Loss = 0.27982689142227174
Epoch: 8921, Batch Gradient Norm: 15.877340439396637
Epoch: 8921, Batch Gradient Norm after: 15.877340439396637
Epoch 8922/10000, Prediction Accuracy = 65.26399999999998%, Loss = 0.2828458666801453
Epoch: 8922, Batch Gradient Norm: 15.039474244482241
Epoch: 8922, Batch Gradient Norm after: 15.039474244482241
Epoch 8923/10000, Prediction Accuracy = 65.17999999999999%, Loss = 0.2800894618034363
Epoch: 8923, Batch Gradient Norm: 16.764218420832126
Epoch: 8923, Batch Gradient Norm after: 16.764218420832126
Epoch 8924/10000, Prediction Accuracy = 65.28200000000001%, Loss = 0.28226367235183714
Epoch: 8924, Batch Gradient Norm: 16.39557782832869
Epoch: 8924, Batch Gradient Norm after: 16.39557782832869
Epoch 8925/10000, Prediction Accuracy = 65.188%, Loss = 0.2878287732601166
Epoch: 8925, Batch Gradient Norm: 17.477892364826726
Epoch: 8925, Batch Gradient Norm after: 17.273297927342878
Epoch 8926/10000, Prediction Accuracy = 65.208%, Loss = 0.28442259430885314
Epoch: 8926, Batch Gradient Norm: 14.788197128095392
Epoch: 8926, Batch Gradient Norm after: 14.788197128095392
Epoch 8927/10000, Prediction Accuracy = 65.166%, Loss = 0.2822770357131958
Epoch: 8927, Batch Gradient Norm: 12.167859604525962
Epoch: 8927, Batch Gradient Norm after: 12.167859604525962
Epoch 8928/10000, Prediction Accuracy = 65.22%, Loss = 0.27801698446273804
Epoch: 8928, Batch Gradient Norm: 13.940429656521573
Epoch: 8928, Batch Gradient Norm after: 13.940429656521573
Epoch 8929/10000, Prediction Accuracy = 65.36999999999999%, Loss = 0.2784466862678528
Epoch: 8929, Batch Gradient Norm: 16.135283723373313
Epoch: 8929, Batch Gradient Norm after: 16.135283723373313
Epoch 8930/10000, Prediction Accuracy = 65.33200000000001%, Loss = 0.281675124168396
Epoch: 8930, Batch Gradient Norm: 16.064486618741686
Epoch: 8930, Batch Gradient Norm after: 16.064486618741686
Epoch 8931/10000, Prediction Accuracy = 65.316%, Loss = 0.28110981583595274
Epoch: 8931, Batch Gradient Norm: 15.696079645079678
Epoch: 8931, Batch Gradient Norm after: 15.696079645079678
Epoch 8932/10000, Prediction Accuracy = 65.152%, Loss = 0.2814114987850189
Epoch: 8932, Batch Gradient Norm: 14.82279249123929
Epoch: 8932, Batch Gradient Norm after: 14.82279249123929
Epoch 8933/10000, Prediction Accuracy = 65.338%, Loss = 0.2810658097267151
Epoch: 8933, Batch Gradient Norm: 13.77880099324433
Epoch: 8933, Batch Gradient Norm after: 13.77880099324433
Epoch 8934/10000, Prediction Accuracy = 65.362%, Loss = 0.2778727293014526
Epoch: 8934, Batch Gradient Norm: 15.773848872974709
Epoch: 8934, Batch Gradient Norm after: 15.773848872974709
Epoch 8935/10000, Prediction Accuracy = 65.33000000000001%, Loss = 0.28066274523735046
Epoch: 8935, Batch Gradient Norm: 16.60496556515059
Epoch: 8935, Batch Gradient Norm after: 16.60496556515059
Epoch 8936/10000, Prediction Accuracy = 65.21600000000001%, Loss = 0.2868913412094116
Epoch: 8936, Batch Gradient Norm: 19.22551832444833
Epoch: 8936, Batch Gradient Norm after: 19.20765638718899
Epoch 8937/10000, Prediction Accuracy = 65.158%, Loss = 0.28569799065589907
Epoch: 8937, Batch Gradient Norm: 19.89922640416826
Epoch: 8937, Batch Gradient Norm after: 19.89922640416826
Epoch 8938/10000, Prediction Accuracy = 65.20000000000002%, Loss = 0.28843759894371035
Epoch: 8938, Batch Gradient Norm: 18.50445822675653
Epoch: 8938, Batch Gradient Norm after: 18.50445822675653
Epoch 8939/10000, Prediction Accuracy = 65.128%, Loss = 0.28389679789543154
Epoch: 8939, Batch Gradient Norm: 15.130334058799995
Epoch: 8939, Batch Gradient Norm after: 15.130334058799995
Epoch 8940/10000, Prediction Accuracy = 65.38399999999999%, Loss = 0.2790316104888916
Epoch: 8940, Batch Gradient Norm: 17.232120467344963
Epoch: 8940, Batch Gradient Norm after: 17.232120467344963
Epoch 8941/10000, Prediction Accuracy = 65.36%, Loss = 0.28430320620536803
Epoch: 8941, Batch Gradient Norm: 17.309582871042743
Epoch: 8941, Batch Gradient Norm after: 17.309582871042743
Epoch 8942/10000, Prediction Accuracy = 65.32000000000001%, Loss = 0.28385571837425233
Epoch: 8942, Batch Gradient Norm: 15.427683848255901
Epoch: 8942, Batch Gradient Norm after: 15.427683848255901
Epoch 8943/10000, Prediction Accuracy = 65.262%, Loss = 0.2815505564212799
Epoch: 8943, Batch Gradient Norm: 13.019188320020813
Epoch: 8943, Batch Gradient Norm after: 13.019188320020813
Epoch 8944/10000, Prediction Accuracy = 65.196%, Loss = 0.27921555638313295
Epoch: 8944, Batch Gradient Norm: 14.450619805564992
Epoch: 8944, Batch Gradient Norm after: 14.450619805564992
Epoch 8945/10000, Prediction Accuracy = 65.28800000000001%, Loss = 0.28088962435722353
Epoch: 8945, Batch Gradient Norm: 13.964378042329242
Epoch: 8945, Batch Gradient Norm after: 13.964378042329242
Epoch 8946/10000, Prediction Accuracy = 65.342%, Loss = 0.2805409789085388
Epoch: 8946, Batch Gradient Norm: 14.332790795523396
Epoch: 8946, Batch Gradient Norm after: 14.332790795523396
Epoch 8947/10000, Prediction Accuracy = 65.468%, Loss = 0.27751352190971373
Epoch: 8947, Batch Gradient Norm: 15.686339214932273
Epoch: 8947, Batch Gradient Norm after: 15.686339214932273
Epoch 8948/10000, Prediction Accuracy = 65.314%, Loss = 0.2819766640663147
Epoch: 8948, Batch Gradient Norm: 14.876334435684477
Epoch: 8948, Batch Gradient Norm after: 14.876334435684477
Epoch 8949/10000, Prediction Accuracy = 65.422%, Loss = 0.278561532497406
Epoch: 8949, Batch Gradient Norm: 16.342074947821658
Epoch: 8949, Batch Gradient Norm after: 16.342074947821658
Epoch 8950/10000, Prediction Accuracy = 65.15800000000002%, Loss = 0.283966600894928
Epoch: 8950, Batch Gradient Norm: 14.21620105331438
Epoch: 8950, Batch Gradient Norm after: 14.21620105331438
Epoch 8951/10000, Prediction Accuracy = 65.28999999999999%, Loss = 0.2788275957107544
Epoch: 8951, Batch Gradient Norm: 15.056434109361694
Epoch: 8951, Batch Gradient Norm after: 15.056434109361694
Epoch 8952/10000, Prediction Accuracy = 65.402%, Loss = 0.2801262676715851
Epoch: 8952, Batch Gradient Norm: 20.355549499321185
Epoch: 8952, Batch Gradient Norm after: 19.496632755641638
Epoch 8953/10000, Prediction Accuracy = 65.24199999999999%, Loss = 0.2848023295402527
Epoch: 8953, Batch Gradient Norm: 18.852503449107108
Epoch: 8953, Batch Gradient Norm after: 18.276825993157647
Epoch 8954/10000, Prediction Accuracy = 65.154%, Loss = 0.28757878541946413
Epoch: 8954, Batch Gradient Norm: 16.018150869743568
Epoch: 8954, Batch Gradient Norm after: 16.018150869743568
Epoch 8955/10000, Prediction Accuracy = 65.13399999999999%, Loss = 0.2855105459690094
Epoch: 8955, Batch Gradient Norm: 16.700950376801767
Epoch: 8955, Batch Gradient Norm after: 16.700950376801767
Epoch 8956/10000, Prediction Accuracy = 65.21000000000001%, Loss = 0.2832173526287079
Epoch: 8956, Batch Gradient Norm: 14.748924215748172
Epoch: 8956, Batch Gradient Norm after: 14.748924215748172
Epoch 8957/10000, Prediction Accuracy = 65.246%, Loss = 0.27774627804756163
Epoch: 8957, Batch Gradient Norm: 14.53908615755727
Epoch: 8957, Batch Gradient Norm after: 14.53908615755727
Epoch 8958/10000, Prediction Accuracy = 65.162%, Loss = 0.2801303744316101
Epoch: 8958, Batch Gradient Norm: 13.665765481962458
Epoch: 8958, Batch Gradient Norm after: 13.665765481962458
Epoch 8959/10000, Prediction Accuracy = 65.262%, Loss = 0.27782013416290285
Epoch: 8959, Batch Gradient Norm: 14.955376557329657
Epoch: 8959, Batch Gradient Norm after: 14.955376557329657
Epoch 8960/10000, Prediction Accuracy = 65.394%, Loss = 0.27754625082015993
Epoch: 8960, Batch Gradient Norm: 13.562742611571595
Epoch: 8960, Batch Gradient Norm after: 13.562742611571595
Epoch 8961/10000, Prediction Accuracy = 65.22%, Loss = 0.2770862400531769
Epoch: 8961, Batch Gradient Norm: 13.234093619754747
Epoch: 8961, Batch Gradient Norm after: 13.234093619754747
Epoch 8962/10000, Prediction Accuracy = 65.39200000000001%, Loss = 0.27722066044807436
Epoch: 8962, Batch Gradient Norm: 15.133593327185245
Epoch: 8962, Batch Gradient Norm after: 15.133593327185245
Epoch 8963/10000, Prediction Accuracy = 65.402%, Loss = 0.2792523741722107
Epoch: 8963, Batch Gradient Norm: 18.689762029236427
Epoch: 8963, Batch Gradient Norm after: 18.31169942676047
Epoch 8964/10000, Prediction Accuracy = 65.262%, Loss = 0.2875689446926117
Epoch: 8964, Batch Gradient Norm: 18.037966643056148
Epoch: 8964, Batch Gradient Norm after: 17.339009987020837
Epoch 8965/10000, Prediction Accuracy = 65.31400000000001%, Loss = 0.28212444186210633
Epoch: 8965, Batch Gradient Norm: 17.983689988305397
Epoch: 8965, Batch Gradient Norm after: 17.503230559155842
Epoch 8966/10000, Prediction Accuracy = 65.30799999999999%, Loss = 0.2849701762199402
Epoch: 8966, Batch Gradient Norm: 14.962491325293758
Epoch: 8966, Batch Gradient Norm after: 14.962491325293758
Epoch 8967/10000, Prediction Accuracy = 65.38399999999999%, Loss = 0.28135358095169066
Epoch: 8967, Batch Gradient Norm: 14.287043650128524
Epoch: 8967, Batch Gradient Norm after: 14.287043650128524
Epoch 8968/10000, Prediction Accuracy = 65.154%, Loss = 0.2807080149650574
Epoch: 8968, Batch Gradient Norm: 14.169405795645657
Epoch: 8968, Batch Gradient Norm after: 14.169405795645657
Epoch 8969/10000, Prediction Accuracy = 65.41999999999999%, Loss = 0.27911428213119505
Epoch: 8969, Batch Gradient Norm: 13.547292602788236
Epoch: 8969, Batch Gradient Norm after: 13.547292602788236
Epoch 8970/10000, Prediction Accuracy = 65.312%, Loss = 0.27851338386535646
Epoch: 8970, Batch Gradient Norm: 14.179955261676346
Epoch: 8970, Batch Gradient Norm after: 14.179955261676346
Epoch 8971/10000, Prediction Accuracy = 65.31%, Loss = 0.27727876901626586
Epoch: 8971, Batch Gradient Norm: 13.174658350708581
Epoch: 8971, Batch Gradient Norm after: 13.174658350708581
Epoch 8972/10000, Prediction Accuracy = 65.196%, Loss = 0.2759026825428009
Epoch: 8972, Batch Gradient Norm: 10.645030145093514
Epoch: 8972, Batch Gradient Norm after: 10.645030145093514
Epoch 8973/10000, Prediction Accuracy = 65.256%, Loss = 0.27462381720542905
Epoch: 8973, Batch Gradient Norm: 14.466829751385488
Epoch: 8973, Batch Gradient Norm after: 14.466829751385488
Epoch 8974/10000, Prediction Accuracy = 65.27000000000001%, Loss = 0.2802309453487396
Epoch: 8974, Batch Gradient Norm: 15.426040683851573
Epoch: 8974, Batch Gradient Norm after: 15.426040683851573
Epoch 8975/10000, Prediction Accuracy = 65.13%, Loss = 0.2796431124210358
Epoch: 8975, Batch Gradient Norm: 16.62732353604113
Epoch: 8975, Batch Gradient Norm after: 16.62732353604113
Epoch 8976/10000, Prediction Accuracy = 65.13199999999999%, Loss = 0.2803929388523102
Epoch: 8976, Batch Gradient Norm: 16.1910730222516
Epoch: 8976, Batch Gradient Norm after: 16.1910730222516
Epoch 8977/10000, Prediction Accuracy = 65.078%, Loss = 0.2838263213634491
Epoch: 8977, Batch Gradient Norm: 14.834592182390404
Epoch: 8977, Batch Gradient Norm after: 14.834592182390404
Epoch 8978/10000, Prediction Accuracy = 65.18800000000002%, Loss = 0.278941810131073
Epoch: 8978, Batch Gradient Norm: 16.963096277303613
Epoch: 8978, Batch Gradient Norm after: 16.963096277303613
Epoch 8979/10000, Prediction Accuracy = 65.264%, Loss = 0.2838835299015045
Epoch: 8979, Batch Gradient Norm: 16.755960800205127
Epoch: 8979, Batch Gradient Norm after: 16.755960800205127
Epoch 8980/10000, Prediction Accuracy = 65.2%, Loss = 0.2848087430000305
Epoch: 8980, Batch Gradient Norm: 15.59808265805349
Epoch: 8980, Batch Gradient Norm after: 15.59808265805349
Epoch 8981/10000, Prediction Accuracy = 65.07999999999998%, Loss = 0.27949714064598086
Epoch: 8981, Batch Gradient Norm: 14.458076869308194
Epoch: 8981, Batch Gradient Norm after: 14.458076869308194
Epoch 8982/10000, Prediction Accuracy = 65.28600000000002%, Loss = 0.2788339853286743
Epoch: 8982, Batch Gradient Norm: 13.765968275819738
Epoch: 8982, Batch Gradient Norm after: 13.765968275819738
Epoch 8983/10000, Prediction Accuracy = 65.272%, Loss = 0.2786899387836456
Epoch: 8983, Batch Gradient Norm: 13.029711491820004
Epoch: 8983, Batch Gradient Norm after: 13.029711491820004
Epoch 8984/10000, Prediction Accuracy = 65.322%, Loss = 0.2768286943435669
Epoch: 8984, Batch Gradient Norm: 14.773722797979762
Epoch: 8984, Batch Gradient Norm after: 14.773722797979762
Epoch 8985/10000, Prediction Accuracy = 65.328%, Loss = 0.2787005305290222
Epoch: 8985, Batch Gradient Norm: 14.383477863513368
Epoch: 8985, Batch Gradient Norm after: 14.383477863513368
Epoch 8986/10000, Prediction Accuracy = 65.304%, Loss = 0.2789044439792633
Epoch: 8986, Batch Gradient Norm: 16.114845257596137
Epoch: 8986, Batch Gradient Norm after: 15.891495905614253
Epoch 8987/10000, Prediction Accuracy = 65.298%, Loss = 0.2801365315914154
Epoch: 8987, Batch Gradient Norm: 16.209552321505225
Epoch: 8987, Batch Gradient Norm after: 16.209552321505225
Epoch 8988/10000, Prediction Accuracy = 65.326%, Loss = 0.2787189602851868
Epoch: 8988, Batch Gradient Norm: 15.597269497511911
Epoch: 8988, Batch Gradient Norm after: 15.597269497511911
Epoch 8989/10000, Prediction Accuracy = 65.27000000000001%, Loss = 0.2815407454967499
Epoch: 8989, Batch Gradient Norm: 14.828391803675094
Epoch: 8989, Batch Gradient Norm after: 14.828391803675094
Epoch 8990/10000, Prediction Accuracy = 65.22200000000001%, Loss = 0.28012369871139525
Epoch: 8990, Batch Gradient Norm: 14.42509748366712
Epoch: 8990, Batch Gradient Norm after: 14.42509748366712
Epoch 8991/10000, Prediction Accuracy = 65.228%, Loss = 0.27928953170776366
Epoch: 8991, Batch Gradient Norm: 13.759989940046188
Epoch: 8991, Batch Gradient Norm after: 13.759989940046188
Epoch 8992/10000, Prediction Accuracy = 65.23599999999999%, Loss = 0.27779326438903806
Epoch: 8992, Batch Gradient Norm: 13.469941959046285
Epoch: 8992, Batch Gradient Norm after: 13.469941959046285
Epoch 8993/10000, Prediction Accuracy = 65.184%, Loss = 0.2792697012424469
Epoch: 8993, Batch Gradient Norm: 15.05688940024597
Epoch: 8993, Batch Gradient Norm after: 15.05688940024597
Epoch 8994/10000, Prediction Accuracy = 65.18%, Loss = 0.27950698137283325
Epoch: 8994, Batch Gradient Norm: 15.753910928892948
Epoch: 8994, Batch Gradient Norm after: 15.753910928892948
Epoch 8995/10000, Prediction Accuracy = 65.31599999999999%, Loss = 0.2803232729434967
Epoch: 8995, Batch Gradient Norm: 11.71484752494099
Epoch: 8995, Batch Gradient Norm after: 11.71484752494099
Epoch 8996/10000, Prediction Accuracy = 65.332%, Loss = 0.2754372298717499
Epoch: 8996, Batch Gradient Norm: 11.17979461321389
Epoch: 8996, Batch Gradient Norm after: 11.17979461321389
Epoch 8997/10000, Prediction Accuracy = 65.352%, Loss = 0.2741523921489716
Epoch: 8997, Batch Gradient Norm: 11.714041577017333
Epoch: 8997, Batch Gradient Norm after: 11.714041577017333
Epoch 8998/10000, Prediction Accuracy = 65.316%, Loss = 0.27477921843528746
Epoch: 8998, Batch Gradient Norm: 14.009064038041537
Epoch: 8998, Batch Gradient Norm after: 14.009064038041537
Epoch 8999/10000, Prediction Accuracy = 65.254%, Loss = 0.2784495115280151
Epoch: 8999, Batch Gradient Norm: 16.714115875218624
Epoch: 8999, Batch Gradient Norm after: 16.160455121587955
Epoch 9000/10000, Prediction Accuracy = 65.22999999999999%, Loss = 0.28306750059127805
Epoch: 9000, Batch Gradient Norm: 16.95100081774245
Epoch: 9000, Batch Gradient Norm after: 16.95100081774245
Epoch 9001/10000, Prediction Accuracy = 65.34599999999999%, Loss = 0.28198575377464297
Epoch: 9001, Batch Gradient Norm: 15.263550737540179
Epoch: 9001, Batch Gradient Norm after: 15.263550737540179
Epoch 9002/10000, Prediction Accuracy = 65.30000000000001%, Loss = 0.2819319307804108
Epoch: 9002, Batch Gradient Norm: 15.669250953170408
Epoch: 9002, Batch Gradient Norm after: 15.669250953170408
Epoch 9003/10000, Prediction Accuracy = 65.372%, Loss = 0.27960861325263975
Epoch: 9003, Batch Gradient Norm: 13.612844590337442
Epoch: 9003, Batch Gradient Norm after: 13.612844590337442
Epoch 9004/10000, Prediction Accuracy = 65.348%, Loss = 0.2783129274845123
Epoch: 9004, Batch Gradient Norm: 17.19046179323293
Epoch: 9004, Batch Gradient Norm after: 17.19046179323293
Epoch 9005/10000, Prediction Accuracy = 65.168%, Loss = 0.28362047076225283
Epoch: 9005, Batch Gradient Norm: 15.712551171884872
Epoch: 9005, Batch Gradient Norm after: 15.712551171884872
Epoch 9006/10000, Prediction Accuracy = 65.268%, Loss = 0.2805339276790619
Epoch: 9006, Batch Gradient Norm: 13.047069880386852
Epoch: 9006, Batch Gradient Norm after: 13.047069880386852
Epoch 9007/10000, Prediction Accuracy = 65.19399999999999%, Loss = 0.27702866196632386
Epoch: 9007, Batch Gradient Norm: 14.832290546376091
Epoch: 9007, Batch Gradient Norm after: 14.832290546376091
Epoch 9008/10000, Prediction Accuracy = 65.186%, Loss = 0.2826333582401276
Epoch: 9008, Batch Gradient Norm: 15.356735072740408
Epoch: 9008, Batch Gradient Norm after: 15.356735072740408
Epoch 9009/10000, Prediction Accuracy = 65.078%, Loss = 0.2804307281970978
Epoch: 9009, Batch Gradient Norm: 13.174868842642313
Epoch: 9009, Batch Gradient Norm after: 13.174868842642313
Epoch 9010/10000, Prediction Accuracy = 65.21600000000001%, Loss = 0.2762989103794098
Epoch: 9010, Batch Gradient Norm: 14.540906485792439
Epoch: 9010, Batch Gradient Norm after: 14.540906485792439
Epoch 9011/10000, Prediction Accuracy = 65.11999999999999%, Loss = 0.27878921627998354
Epoch: 9011, Batch Gradient Norm: 12.663792791500166
Epoch: 9011, Batch Gradient Norm after: 12.663792791500166
Epoch 9012/10000, Prediction Accuracy = 65.3%, Loss = 0.27741416096687316
Epoch: 9012, Batch Gradient Norm: 11.496795100511362
Epoch: 9012, Batch Gradient Norm after: 11.496795100511362
Epoch 9013/10000, Prediction Accuracy = 65.306%, Loss = 0.2774911344051361
Epoch: 9013, Batch Gradient Norm: 13.805738151921261
Epoch: 9013, Batch Gradient Norm after: 13.805738151921261
Epoch 9014/10000, Prediction Accuracy = 65.284%, Loss = 0.2784546256065369
Epoch: 9014, Batch Gradient Norm: 16.50686538360306
Epoch: 9014, Batch Gradient Norm after: 16.50686538360306
Epoch 9015/10000, Prediction Accuracy = 65.232%, Loss = 0.28287158608436586
Epoch: 9015, Batch Gradient Norm: 14.076293246071963
Epoch: 9015, Batch Gradient Norm after: 14.076293246071963
Epoch 9016/10000, Prediction Accuracy = 65.304%, Loss = 0.2761811316013336
Epoch: 9016, Batch Gradient Norm: 15.498947553756558
Epoch: 9016, Batch Gradient Norm after: 15.498947553756558
Epoch 9017/10000, Prediction Accuracy = 65.4%, Loss = 0.27919420003890993
Epoch: 9017, Batch Gradient Norm: 13.923217513989009
Epoch: 9017, Batch Gradient Norm after: 13.923217513989009
Epoch 9018/10000, Prediction Accuracy = 65.2%, Loss = 0.2770895779132843
Epoch: 9018, Batch Gradient Norm: 12.464317646894752
Epoch: 9018, Batch Gradient Norm after: 12.464317646894752
Epoch 9019/10000, Prediction Accuracy = 65.288%, Loss = 0.27774657011032106
Epoch: 9019, Batch Gradient Norm: 12.843940444355843
Epoch: 9019, Batch Gradient Norm after: 12.843940444355843
Epoch 9020/10000, Prediction Accuracy = 65.272%, Loss = 0.2748553931713104
Epoch: 9020, Batch Gradient Norm: 14.286151806077049
Epoch: 9020, Batch Gradient Norm after: 14.286151806077049
Epoch 9021/10000, Prediction Accuracy = 65.23400000000001%, Loss = 0.27851736545562744
Epoch: 9021, Batch Gradient Norm: 14.881349448903268
Epoch: 9021, Batch Gradient Norm after: 14.881349448903268
Epoch 9022/10000, Prediction Accuracy = 65.24%, Loss = 0.2809576392173767
Epoch: 9022, Batch Gradient Norm: 18.26656203169061
Epoch: 9022, Batch Gradient Norm after: 18.263973654086435
Epoch 9023/10000, Prediction Accuracy = 65.232%, Loss = 0.2845716655254364
Epoch: 9023, Batch Gradient Norm: 15.474564605064419
Epoch: 9023, Batch Gradient Norm after: 15.474564605064419
Epoch 9024/10000, Prediction Accuracy = 65.232%, Loss = 0.28007517457008363
Epoch: 9024, Batch Gradient Norm: 14.497756494484092
Epoch: 9024, Batch Gradient Norm after: 14.497756494484092
Epoch 9025/10000, Prediction Accuracy = 65.414%, Loss = 0.27782598733901975
Epoch: 9025, Batch Gradient Norm: 14.913695839678958
Epoch: 9025, Batch Gradient Norm after: 14.913695839678958
Epoch 9026/10000, Prediction Accuracy = 65.278%, Loss = 0.27668806314468386
Epoch: 9026, Batch Gradient Norm: 14.766648747883258
Epoch: 9026, Batch Gradient Norm after: 14.766648747883258
Epoch 9027/10000, Prediction Accuracy = 65.37%, Loss = 0.2773595154285431
Epoch: 9027, Batch Gradient Norm: 16.853476837479448
Epoch: 9027, Batch Gradient Norm after: 16.853476837479448
Epoch 9028/10000, Prediction Accuracy = 65.26599999999999%, Loss = 0.28028516173362733
Epoch: 9028, Batch Gradient Norm: 18.13558750788959
Epoch: 9028, Batch Gradient Norm after: 17.827251163801492
Epoch 9029/10000, Prediction Accuracy = 65.33%, Loss = 0.2828164339065552
Epoch: 9029, Batch Gradient Norm: 16.896129570065632
Epoch: 9029, Batch Gradient Norm after: 16.896129570065632
Epoch 9030/10000, Prediction Accuracy = 65.37800000000001%, Loss = 0.2806037366390228
Epoch: 9030, Batch Gradient Norm: 15.989746204307135
Epoch: 9030, Batch Gradient Norm after: 15.989746204307135
Epoch 9031/10000, Prediction Accuracy = 65.342%, Loss = 0.2818315029144287
Epoch: 9031, Batch Gradient Norm: 17.08242830872356
Epoch: 9031, Batch Gradient Norm after: 17.08242830872356
Epoch 9032/10000, Prediction Accuracy = 65.31200000000001%, Loss = 0.2851427853107452
Epoch: 9032, Batch Gradient Norm: 17.35318709690987
Epoch: 9032, Batch Gradient Norm after: 17.35318709690987
Epoch 9033/10000, Prediction Accuracy = 65.14%, Loss = 0.2858901798725128
Epoch: 9033, Batch Gradient Norm: 16.809657945008933
Epoch: 9033, Batch Gradient Norm after: 16.809657945008933
Epoch 9034/10000, Prediction Accuracy = 65.294%, Loss = 0.28129807114601135
Epoch: 9034, Batch Gradient Norm: 16.637494012881408
Epoch: 9034, Batch Gradient Norm after: 16.637494012881408
Epoch 9035/10000, Prediction Accuracy = 65.224%, Loss = 0.28213115930557253
Epoch: 9035, Batch Gradient Norm: 14.696810017019468
Epoch: 9035, Batch Gradient Norm after: 14.696810017019468
Epoch 9036/10000, Prediction Accuracy = 65.38199999999999%, Loss = 0.27828606963157654
Epoch: 9036, Batch Gradient Norm: 15.679560993597608
Epoch: 9036, Batch Gradient Norm after: 15.679560993597608
Epoch 9037/10000, Prediction Accuracy = 65.28399999999999%, Loss = 0.28077408075332644
Epoch: 9037, Batch Gradient Norm: 14.67335317153061
Epoch: 9037, Batch Gradient Norm after: 14.67335317153061
Epoch 9038/10000, Prediction Accuracy = 65.25399999999999%, Loss = 0.2834862172603607
Epoch: 9038, Batch Gradient Norm: 16.796466488602604
Epoch: 9038, Batch Gradient Norm after: 16.796466488602604
Epoch 9039/10000, Prediction Accuracy = 65.408%, Loss = 0.2786805748939514
Epoch: 9039, Batch Gradient Norm: 14.067683472527833
Epoch: 9039, Batch Gradient Norm after: 14.067683472527833
Epoch 9040/10000, Prediction Accuracy = 65.28%, Loss = 0.28074418306350707
Epoch: 9040, Batch Gradient Norm: 15.159044019411445
Epoch: 9040, Batch Gradient Norm after: 15.159044019411445
Epoch 9041/10000, Prediction Accuracy = 65.452%, Loss = 0.2795553386211395
Epoch: 9041, Batch Gradient Norm: 13.676772673798013
Epoch: 9041, Batch Gradient Norm after: 13.676772673798013
Epoch 9042/10000, Prediction Accuracy = 65.352%, Loss = 0.27535171508789064
Epoch: 9042, Batch Gradient Norm: 14.69290461196322
Epoch: 9042, Batch Gradient Norm after: 14.69290461196322
Epoch 9043/10000, Prediction Accuracy = 65.414%, Loss = 0.2767516613006592
Epoch: 9043, Batch Gradient Norm: 12.978220135664422
Epoch: 9043, Batch Gradient Norm after: 12.978220135664422
Epoch 9044/10000, Prediction Accuracy = 65.366%, Loss = 0.275956404209137
Epoch: 9044, Batch Gradient Norm: 13.699037975894244
Epoch: 9044, Batch Gradient Norm after: 13.699037975894244
Epoch 9045/10000, Prediction Accuracy = 65.526%, Loss = 0.2754507303237915
Epoch: 9045, Batch Gradient Norm: 12.332242645626067
Epoch: 9045, Batch Gradient Norm after: 12.332242645626067
Epoch 9046/10000, Prediction Accuracy = 65.22999999999999%, Loss = 0.27847033739089966
Epoch: 9046, Batch Gradient Norm: 16.284984879091652
Epoch: 9046, Batch Gradient Norm after: 16.284984879091652
Epoch 9047/10000, Prediction Accuracy = 65.36399999999999%, Loss = 0.27824742794036866
Epoch: 9047, Batch Gradient Norm: 15.595057746075456
Epoch: 9047, Batch Gradient Norm after: 15.595057746075456
Epoch 9048/10000, Prediction Accuracy = 65.39599999999999%, Loss = 0.27976537942886354
Epoch: 9048, Batch Gradient Norm: 15.523129526675481
Epoch: 9048, Batch Gradient Norm after: 15.523129526675481
Epoch 9049/10000, Prediction Accuracy = 65.38399999999999%, Loss = 0.27856178879737853
Epoch: 9049, Batch Gradient Norm: 15.06377142934013
Epoch: 9049, Batch Gradient Norm after: 15.06377142934013
Epoch 9050/10000, Prediction Accuracy = 65.342%, Loss = 0.2795227825641632
Epoch: 9050, Batch Gradient Norm: 19.778693024844024
Epoch: 9050, Batch Gradient Norm after: 18.258436597575514
Epoch 9051/10000, Prediction Accuracy = 65.436%, Loss = 0.2856054961681366
Epoch: 9051, Batch Gradient Norm: 21.330191254030346
Epoch: 9051, Batch Gradient Norm after: 19.386858437980923
Epoch 9052/10000, Prediction Accuracy = 65.288%, Loss = 0.2876028835773468
Epoch: 9052, Batch Gradient Norm: 16.748103599615273
Epoch: 9052, Batch Gradient Norm after: 16.71191951450033
Epoch 9053/10000, Prediction Accuracy = 65.366%, Loss = 0.2814225196838379
Epoch: 9053, Batch Gradient Norm: 14.203884394377198
Epoch: 9053, Batch Gradient Norm after: 14.203884394377198
Epoch 9054/10000, Prediction Accuracy = 65.248%, Loss = 0.27607172131538393
Epoch: 9054, Batch Gradient Norm: 14.21638453406779
Epoch: 9054, Batch Gradient Norm after: 14.21638453406779
Epoch 9055/10000, Prediction Accuracy = 65.19200000000001%, Loss = 0.27872015833854674
Epoch: 9055, Batch Gradient Norm: 14.288037623539482
Epoch: 9055, Batch Gradient Norm after: 14.288037623539482
Epoch 9056/10000, Prediction Accuracy = 65.356%, Loss = 0.27744223475456237
Epoch: 9056, Batch Gradient Norm: 15.450080942755001
Epoch: 9056, Batch Gradient Norm after: 15.450080942755001
Epoch 9057/10000, Prediction Accuracy = 65.376%, Loss = 0.27927393317222593
Epoch: 9057, Batch Gradient Norm: 14.648698206797066
Epoch: 9057, Batch Gradient Norm after: 14.648698206797066
Epoch 9058/10000, Prediction Accuracy = 65.262%, Loss = 0.27809585332870485
Epoch: 9058, Batch Gradient Norm: 13.888447094292928
Epoch: 9058, Batch Gradient Norm after: 13.888447094292928
Epoch 9059/10000, Prediction Accuracy = 65.462%, Loss = 0.27853638529777525
Epoch: 9059, Batch Gradient Norm: 13.92629883591124
Epoch: 9059, Batch Gradient Norm after: 13.92629883591124
Epoch 9060/10000, Prediction Accuracy = 65.16400000000002%, Loss = 0.2794605493545532
Epoch: 9060, Batch Gradient Norm: 15.475482785813233
Epoch: 9060, Batch Gradient Norm after: 15.475482785813233
Epoch 9061/10000, Prediction Accuracy = 65.38600000000001%, Loss = 0.28018473982810976
Epoch: 9061, Batch Gradient Norm: 14.64997650151776
Epoch: 9061, Batch Gradient Norm after: 14.64997650151776
Epoch 9062/10000, Prediction Accuracy = 65.332%, Loss = 0.2798015236854553
Epoch: 9062, Batch Gradient Norm: 13.987277558519985
Epoch: 9062, Batch Gradient Norm after: 13.987277558519985
Epoch 9063/10000, Prediction Accuracy = 65.36%, Loss = 0.2794511795043945
Epoch: 9063, Batch Gradient Norm: 13.996426915468978
Epoch: 9063, Batch Gradient Norm after: 13.996426915468978
Epoch 9064/10000, Prediction Accuracy = 65.10799999999999%, Loss = 0.2810250759124756
Epoch: 9064, Batch Gradient Norm: 11.738839727429646
Epoch: 9064, Batch Gradient Norm after: 11.738839727429646
Epoch 9065/10000, Prediction Accuracy = 65.34400000000001%, Loss = 0.27423765063285827
Epoch: 9065, Batch Gradient Norm: 12.213464209818236
Epoch: 9065, Batch Gradient Norm after: 12.213464209818236
Epoch 9066/10000, Prediction Accuracy = 65.308%, Loss = 0.27703975439071654
Epoch: 9066, Batch Gradient Norm: 14.27822228061605
Epoch: 9066, Batch Gradient Norm after: 14.27822228061605
Epoch 9067/10000, Prediction Accuracy = 65.262%, Loss = 0.27674232721328734
Epoch: 9067, Batch Gradient Norm: 15.953084951595619
Epoch: 9067, Batch Gradient Norm after: 15.953084951595619
Epoch 9068/10000, Prediction Accuracy = 65.348%, Loss = 0.2787726938724518
Epoch: 9068, Batch Gradient Norm: 16.860479520144562
Epoch: 9068, Batch Gradient Norm after: 16.860479520144562
Epoch 9069/10000, Prediction Accuracy = 65.27000000000001%, Loss = 0.2791976034641266
Epoch: 9069, Batch Gradient Norm: 17.905915857993865
Epoch: 9069, Batch Gradient Norm after: 17.905915857993865
Epoch 9070/10000, Prediction Accuracy = 65.486%, Loss = 0.2808138906955719
Epoch: 9070, Batch Gradient Norm: 17.63383302747238
Epoch: 9070, Batch Gradient Norm after: 17.411177169894007
Epoch 9071/10000, Prediction Accuracy = 65.41999999999999%, Loss = 0.2799674987792969
Epoch: 9071, Batch Gradient Norm: 15.289090832987826
Epoch: 9071, Batch Gradient Norm after: 15.289090832987826
Epoch 9072/10000, Prediction Accuracy = 65.3%, Loss = 0.2779871642589569
Epoch: 9072, Batch Gradient Norm: 15.182614915026347
Epoch: 9072, Batch Gradient Norm after: 15.182614915026347
Epoch 9073/10000, Prediction Accuracy = 65.316%, Loss = 0.2779125928878784
Epoch: 9073, Batch Gradient Norm: 15.09798841701088
Epoch: 9073, Batch Gradient Norm after: 15.09798841701088
Epoch 9074/10000, Prediction Accuracy = 65.452%, Loss = 0.27534046173095705
Epoch: 9074, Batch Gradient Norm: 16.531183394156105
Epoch: 9074, Batch Gradient Norm after: 16.531183394156105
Epoch 9075/10000, Prediction Accuracy = 65.29400000000001%, Loss = 0.28013946413993834
Epoch: 9075, Batch Gradient Norm: 12.207535539482588
Epoch: 9075, Batch Gradient Norm after: 12.207535539482588
Epoch 9076/10000, Prediction Accuracy = 65.41399999999999%, Loss = 0.2738702595233917
Epoch: 9076, Batch Gradient Norm: 14.224490735014385
Epoch: 9076, Batch Gradient Norm after: 14.224490735014385
Epoch 9077/10000, Prediction Accuracy = 65.53799999999998%, Loss = 0.27735388875007627
Epoch: 9077, Batch Gradient Norm: 14.186567932543582
Epoch: 9077, Batch Gradient Norm after: 14.186567932543582
Epoch 9078/10000, Prediction Accuracy = 65.11%, Loss = 0.2807504296302795
Epoch: 9078, Batch Gradient Norm: 14.103617503084187
Epoch: 9078, Batch Gradient Norm after: 14.103617503084187
Epoch 9079/10000, Prediction Accuracy = 65.326%, Loss = 0.280282998085022
Epoch: 9079, Batch Gradient Norm: 13.27748017898574
Epoch: 9079, Batch Gradient Norm after: 13.27748017898574
Epoch 9080/10000, Prediction Accuracy = 65.24600000000001%, Loss = 0.27483071088790895
Epoch: 9080, Batch Gradient Norm: 12.796568630755258
Epoch: 9080, Batch Gradient Norm after: 12.796568630755258
Epoch 9081/10000, Prediction Accuracy = 65.334%, Loss = 0.2761043131351471
Epoch: 9081, Batch Gradient Norm: 14.38984680143858
Epoch: 9081, Batch Gradient Norm after: 14.38984680143858
Epoch 9082/10000, Prediction Accuracy = 65.21600000000001%, Loss = 0.2772256672382355
Epoch: 9082, Batch Gradient Norm: 15.164165950013658
Epoch: 9082, Batch Gradient Norm after: 15.164165950013658
Epoch 9083/10000, Prediction Accuracy = 65.36800000000001%, Loss = 0.27973100543022156
Epoch: 9083, Batch Gradient Norm: 14.84775744442705
Epoch: 9083, Batch Gradient Norm after: 14.84775744442705
Epoch 9084/10000, Prediction Accuracy = 65.34%, Loss = 0.27858819961547854
Epoch: 9084, Batch Gradient Norm: 14.663230926737352
Epoch: 9084, Batch Gradient Norm after: 14.663230926737352
Epoch 9085/10000, Prediction Accuracy = 65.45%, Loss = 0.2763786256313324
Epoch: 9085, Batch Gradient Norm: 16.15246857636161
Epoch: 9085, Batch Gradient Norm after: 16.15246857636161
Epoch 9086/10000, Prediction Accuracy = 65.388%, Loss = 0.2791181027889252
Epoch: 9086, Batch Gradient Norm: 13.014033126662328
Epoch: 9086, Batch Gradient Norm after: 13.014033126662328
Epoch 9087/10000, Prediction Accuracy = 65.29399999999998%, Loss = 0.2758298933506012
Epoch: 9087, Batch Gradient Norm: 13.780270902588548
Epoch: 9087, Batch Gradient Norm after: 13.780270902588548
Epoch 9088/10000, Prediction Accuracy = 65.33000000000001%, Loss = 0.27617530822753905
Epoch: 9088, Batch Gradient Norm: 14.927880987482268
Epoch: 9088, Batch Gradient Norm after: 14.927880987482268
Epoch 9089/10000, Prediction Accuracy = 65.38%, Loss = 0.2800951361656189
Epoch: 9089, Batch Gradient Norm: 15.28742422924654
Epoch: 9089, Batch Gradient Norm after: 15.28742422924654
Epoch 9090/10000, Prediction Accuracy = 65.364%, Loss = 0.27916913032531737
Epoch: 9090, Batch Gradient Norm: 19.625591494613396
Epoch: 9090, Batch Gradient Norm after: 18.995906550571476
Epoch 9091/10000, Prediction Accuracy = 65.286%, Loss = 0.2879329979419708
Epoch: 9091, Batch Gradient Norm: 16.796411208820633
Epoch: 9091, Batch Gradient Norm after: 16.796411208820633
Epoch 9092/10000, Prediction Accuracy = 65.086%, Loss = 0.284215635061264
Epoch: 9092, Batch Gradient Norm: 16.99698323838739
Epoch: 9092, Batch Gradient Norm after: 16.996654247843775
Epoch 9093/10000, Prediction Accuracy = 65.292%, Loss = 0.28002484440803527
Epoch: 9093, Batch Gradient Norm: 13.462995003832727
Epoch: 9093, Batch Gradient Norm after: 13.462995003832727
Epoch 9094/10000, Prediction Accuracy = 65.304%, Loss = 0.27819653153419494
Epoch: 9094, Batch Gradient Norm: 14.184560513484804
Epoch: 9094, Batch Gradient Norm after: 14.184560513484804
Epoch 9095/10000, Prediction Accuracy = 65.41000000000001%, Loss = 0.2787842035293579
Epoch: 9095, Batch Gradient Norm: 12.977704528904901
Epoch: 9095, Batch Gradient Norm after: 12.977704528904901
Epoch 9096/10000, Prediction Accuracy = 65.446%, Loss = 0.2766465902328491
Epoch: 9096, Batch Gradient Norm: 13.554263447341281
Epoch: 9096, Batch Gradient Norm after: 13.554263447341281
Epoch 9097/10000, Prediction Accuracy = 65.45200000000001%, Loss = 0.2742376565933228
Epoch: 9097, Batch Gradient Norm: 16.40555003863989
Epoch: 9097, Batch Gradient Norm after: 16.40555003863989
Epoch 9098/10000, Prediction Accuracy = 65.404%, Loss = 0.2799148678779602
Epoch: 9098, Batch Gradient Norm: 17.460961408714624
Epoch: 9098, Batch Gradient Norm after: 17.210513698085414
Epoch 9099/10000, Prediction Accuracy = 65.408%, Loss = 0.2818677663803101
Epoch: 9099, Batch Gradient Norm: 14.735853163100268
Epoch: 9099, Batch Gradient Norm after: 14.735853163100268
Epoch 9100/10000, Prediction Accuracy = 65.27799999999999%, Loss = 0.27989766001701355
Epoch: 9100, Batch Gradient Norm: 16.834788293187025
Epoch: 9100, Batch Gradient Norm after: 16.834788293187025
Epoch 9101/10000, Prediction Accuracy = 65.30999999999999%, Loss = 0.2800194799900055
Epoch: 9101, Batch Gradient Norm: 16.311383530164058
Epoch: 9101, Batch Gradient Norm after: 16.311383530164058
Epoch 9102/10000, Prediction Accuracy = 65.446%, Loss = 0.2798783242702484
Epoch: 9102, Batch Gradient Norm: 18.46733225768882
Epoch: 9102, Batch Gradient Norm after: 18.46733225768882
Epoch 9103/10000, Prediction Accuracy = 65.488%, Loss = 0.2830309748649597
Epoch: 9103, Batch Gradient Norm: 16.253326135846336
Epoch: 9103, Batch Gradient Norm after: 16.253326135846336
Epoch 9104/10000, Prediction Accuracy = 65.352%, Loss = 0.2806392252445221
Epoch: 9104, Batch Gradient Norm: 15.79108749619954
Epoch: 9104, Batch Gradient Norm after: 15.79108749619954
Epoch 9105/10000, Prediction Accuracy = 65.338%, Loss = 0.280782413482666
Epoch: 9105, Batch Gradient Norm: 13.200673777927776
Epoch: 9105, Batch Gradient Norm after: 13.200673777927776
Epoch 9106/10000, Prediction Accuracy = 65.41799999999999%, Loss = 0.27575241327285765
Epoch: 9106, Batch Gradient Norm: 13.861713503167218
Epoch: 9106, Batch Gradient Norm after: 13.861713503167218
Epoch 9107/10000, Prediction Accuracy = 65.312%, Loss = 0.27690528631210326
Epoch: 9107, Batch Gradient Norm: 14.048751844894866
Epoch: 9107, Batch Gradient Norm after: 14.048751844894866
Epoch 9108/10000, Prediction Accuracy = 65.46799999999999%, Loss = 0.2775022864341736
Epoch: 9108, Batch Gradient Norm: 16.908558781647965
Epoch: 9108, Batch Gradient Norm after: 16.908558781647965
Epoch 9109/10000, Prediction Accuracy = 65.34%, Loss = 0.2817269265651703
Epoch: 9109, Batch Gradient Norm: 14.536339016125277
Epoch: 9109, Batch Gradient Norm after: 14.536339016125277
Epoch 9110/10000, Prediction Accuracy = 65.274%, Loss = 0.27629624009132386
Epoch: 9110, Batch Gradient Norm: 13.303987421393842
Epoch: 9110, Batch Gradient Norm after: 13.303987421393842
Epoch 9111/10000, Prediction Accuracy = 65.398%, Loss = 0.2761087238788605
Epoch: 9111, Batch Gradient Norm: 14.656741272640593
Epoch: 9111, Batch Gradient Norm after: 14.656741272640593
Epoch 9112/10000, Prediction Accuracy = 65.336%, Loss = 0.2809779107570648
Epoch: 9112, Batch Gradient Norm: 16.3641234154678
Epoch: 9112, Batch Gradient Norm after: 16.3641234154678
Epoch 9113/10000, Prediction Accuracy = 65.488%, Loss = 0.28112303018569945
Epoch: 9113, Batch Gradient Norm: 15.08863283913889
Epoch: 9113, Batch Gradient Norm after: 15.08863283913889
Epoch 9114/10000, Prediction Accuracy = 65.32%, Loss = 0.27610790729522705
Epoch: 9114, Batch Gradient Norm: 13.100105131996411
Epoch: 9114, Batch Gradient Norm after: 13.100105131996411
Epoch 9115/10000, Prediction Accuracy = 65.454%, Loss = 0.27359555959701537
Epoch: 9115, Batch Gradient Norm: 15.217424541500009
Epoch: 9115, Batch Gradient Norm after: 15.217424541500009
Epoch 9116/10000, Prediction Accuracy = 65.404%, Loss = 0.27558125257492067
Epoch: 9116, Batch Gradient Norm: 14.695674384633358
Epoch: 9116, Batch Gradient Norm after: 14.695674384633358
Epoch 9117/10000, Prediction Accuracy = 65.24%, Loss = 0.2769108474254608
Epoch: 9117, Batch Gradient Norm: 13.50080102103433
Epoch: 9117, Batch Gradient Norm after: 13.50080102103433
Epoch 9118/10000, Prediction Accuracy = 65.36600000000001%, Loss = 0.2752843499183655
Epoch: 9118, Batch Gradient Norm: 13.428812099159208
Epoch: 9118, Batch Gradient Norm after: 13.428812099159208
Epoch 9119/10000, Prediction Accuracy = 65.334%, Loss = 0.27703598141670227
Epoch: 9119, Batch Gradient Norm: 14.369065465780047
Epoch: 9119, Batch Gradient Norm after: 14.369065465780047
Epoch 9120/10000, Prediction Accuracy = 65.328%, Loss = 0.27568777203559874
Epoch: 9120, Batch Gradient Norm: 11.856443608225465
Epoch: 9120, Batch Gradient Norm after: 11.856443608225465
Epoch 9121/10000, Prediction Accuracy = 65.41000000000001%, Loss = 0.2739550232887268
Epoch: 9121, Batch Gradient Norm: 11.457975327377092
Epoch: 9121, Batch Gradient Norm after: 11.457975327377092
Epoch 9122/10000, Prediction Accuracy = 65.46799999999999%, Loss = 0.2748613297939301
Epoch: 9122, Batch Gradient Norm: 14.334247136865498
Epoch: 9122, Batch Gradient Norm after: 14.334247136865498
Epoch 9123/10000, Prediction Accuracy = 65.444%, Loss = 0.27752678990364077
Epoch: 9123, Batch Gradient Norm: 16.22731796321165
Epoch: 9123, Batch Gradient Norm after: 16.22731796321165
Epoch 9124/10000, Prediction Accuracy = 65.494%, Loss = 0.2785035014152527
Epoch: 9124, Batch Gradient Norm: 13.007992758992875
Epoch: 9124, Batch Gradient Norm after: 13.007992758992875
Epoch 9125/10000, Prediction Accuracy = 65.314%, Loss = 0.27500582933425904
Epoch: 9125, Batch Gradient Norm: 11.276373878965057
Epoch: 9125, Batch Gradient Norm after: 11.276373878965057
Epoch 9126/10000, Prediction Accuracy = 65.37800000000001%, Loss = 0.27590863704681395
Epoch: 9126, Batch Gradient Norm: 13.521515607573948
Epoch: 9126, Batch Gradient Norm after: 13.521515607573948
Epoch 9127/10000, Prediction Accuracy = 65.27%, Loss = 0.27715946435928346
Epoch: 9127, Batch Gradient Norm: 15.896708354701133
Epoch: 9127, Batch Gradient Norm after: 15.825031099831142
Epoch 9128/10000, Prediction Accuracy = 65.26599999999999%, Loss = 0.27781819105148314
Epoch: 9128, Batch Gradient Norm: 18.422708872265268
Epoch: 9128, Batch Gradient Norm after: 17.710615425756924
Epoch 9129/10000, Prediction Accuracy = 65.492%, Loss = 0.2822597086429596
Epoch: 9129, Batch Gradient Norm: 20.76960448860625
Epoch: 9129, Batch Gradient Norm after: 20.056925650748568
Epoch 9130/10000, Prediction Accuracy = 65.286%, Loss = 0.28501412868499754
Epoch: 9130, Batch Gradient Norm: 18.148088005131502
Epoch: 9130, Batch Gradient Norm after: 18.148088005131502
Epoch 9131/10000, Prediction Accuracy = 65.34200000000001%, Loss = 0.2804864406585693
Epoch: 9131, Batch Gradient Norm: 15.616109347967239
Epoch: 9131, Batch Gradient Norm after: 15.616109347967239
Epoch 9132/10000, Prediction Accuracy = 65.33399999999999%, Loss = 0.27919360995292664
Epoch: 9132, Batch Gradient Norm: 13.170175947223921
Epoch: 9132, Batch Gradient Norm after: 13.170175947223921
Epoch 9133/10000, Prediction Accuracy = 65.372%, Loss = 0.27394902110099795
Epoch: 9133, Batch Gradient Norm: 15.456747664429765
Epoch: 9133, Batch Gradient Norm after: 15.24261642989691
Epoch 9134/10000, Prediction Accuracy = 65.49199999999999%, Loss = 0.2781775951385498
Epoch: 9134, Batch Gradient Norm: 15.405106877575571
Epoch: 9134, Batch Gradient Norm after: 15.405106877575571
Epoch 9135/10000, Prediction Accuracy = 65.268%, Loss = 0.27793471813201903
Epoch: 9135, Batch Gradient Norm: 12.13883134381248
Epoch: 9135, Batch Gradient Norm after: 12.13883134381248
Epoch 9136/10000, Prediction Accuracy = 65.46199999999999%, Loss = 0.2733657479286194
Epoch: 9136, Batch Gradient Norm: 15.105399146240124
Epoch: 9136, Batch Gradient Norm after: 15.105399146240124
Epoch 9137/10000, Prediction Accuracy = 65.43199999999999%, Loss = 0.27566691637039187
Epoch: 9137, Batch Gradient Norm: 13.847521522911725
Epoch: 9137, Batch Gradient Norm after: 13.847521522911725
Epoch 9138/10000, Prediction Accuracy = 65.28999999999999%, Loss = 0.2747525930404663
Epoch: 9138, Batch Gradient Norm: 14.106383768795192
Epoch: 9138, Batch Gradient Norm after: 14.106383768795192
Epoch 9139/10000, Prediction Accuracy = 65.104%, Loss = 0.2792059898376465
Epoch: 9139, Batch Gradient Norm: 16.932939204936957
Epoch: 9139, Batch Gradient Norm after: 16.932939204936957
Epoch 9140/10000, Prediction Accuracy = 65.29400000000001%, Loss = 0.2800679922103882
Epoch: 9140, Batch Gradient Norm: 20.68034201698545
Epoch: 9140, Batch Gradient Norm after: 20.0761325067612
Epoch 9141/10000, Prediction Accuracy = 65.31800000000001%, Loss = 0.2846777379512787
Epoch: 9141, Batch Gradient Norm: 19.363433402345454
Epoch: 9141, Batch Gradient Norm after: 19.363433402345454
Epoch 9142/10000, Prediction Accuracy = 65.39599999999999%, Loss = 0.28351265788078306
Epoch: 9142, Batch Gradient Norm: 18.24888696428149
Epoch: 9142, Batch Gradient Norm after: 18.24888696428149
Epoch 9143/10000, Prediction Accuracy = 65.422%, Loss = 0.2814665138721466
Epoch: 9143, Batch Gradient Norm: 18.23693971848311
Epoch: 9143, Batch Gradient Norm after: 18.23693971848311
Epoch 9144/10000, Prediction Accuracy = 65.376%, Loss = 0.28224324584007265
Epoch: 9144, Batch Gradient Norm: 16.702252395761473
Epoch: 9144, Batch Gradient Norm after: 16.702252395761473
Epoch 9145/10000, Prediction Accuracy = 65.474%, Loss = 0.2775981307029724
Epoch: 9145, Batch Gradient Norm: 15.662645844329763
Epoch: 9145, Batch Gradient Norm after: 15.662645844329763
Epoch 9146/10000, Prediction Accuracy = 65.33200000000001%, Loss = 0.2790587663650513
Epoch: 9146, Batch Gradient Norm: 15.568517658788926
Epoch: 9146, Batch Gradient Norm after: 15.568517658788926
Epoch 9147/10000, Prediction Accuracy = 65.322%, Loss = 0.28000414967536924
Epoch: 9147, Batch Gradient Norm: 15.418633459972193
Epoch: 9147, Batch Gradient Norm after: 15.418633459972193
Epoch 9148/10000, Prediction Accuracy = 65.41600000000001%, Loss = 0.2779623746871948
Epoch: 9148, Batch Gradient Norm: 15.619253371913166
Epoch: 9148, Batch Gradient Norm after: 15.619253371913166
Epoch 9149/10000, Prediction Accuracy = 65.488%, Loss = 0.2755855143070221
Epoch: 9149, Batch Gradient Norm: 15.732358611342855
Epoch: 9149, Batch Gradient Norm after: 15.732358611342855
Epoch 9150/10000, Prediction Accuracy = 65.52000000000001%, Loss = 0.2783806025981903
Epoch: 9150, Batch Gradient Norm: 14.444913912199484
Epoch: 9150, Batch Gradient Norm after: 14.444913912199484
Epoch 9151/10000, Prediction Accuracy = 65.50399999999999%, Loss = 0.2775086462497711
Epoch: 9151, Batch Gradient Norm: 16.765467198104112
Epoch: 9151, Batch Gradient Norm after: 16.765467198104112
Epoch 9152/10000, Prediction Accuracy = 65.41399999999999%, Loss = 0.27859225273132326
Epoch: 9152, Batch Gradient Norm: 17.486948602318908
Epoch: 9152, Batch Gradient Norm after: 17.486948602318908
Epoch 9153/10000, Prediction Accuracy = 65.364%, Loss = 0.2785287857055664
Epoch: 9153, Batch Gradient Norm: 13.46059924164328
Epoch: 9153, Batch Gradient Norm after: 13.46059924164328
Epoch 9154/10000, Prediction Accuracy = 65.36600000000001%, Loss = 0.2769291400909424
Epoch: 9154, Batch Gradient Norm: 12.346068243508146
Epoch: 9154, Batch Gradient Norm after: 12.346068243508146
Epoch 9155/10000, Prediction Accuracy = 65.518%, Loss = 0.27397588491439817
Epoch: 9155, Batch Gradient Norm: 13.43222139646057
Epoch: 9155, Batch Gradient Norm after: 13.43222139646057
Epoch 9156/10000, Prediction Accuracy = 65.418%, Loss = 0.2742599129676819
Epoch: 9156, Batch Gradient Norm: 13.267290572863336
Epoch: 9156, Batch Gradient Norm after: 13.267290572863336
Epoch 9157/10000, Prediction Accuracy = 65.374%, Loss = 0.27553372979164126
Epoch: 9157, Batch Gradient Norm: 14.581454206020245
Epoch: 9157, Batch Gradient Norm after: 14.581454206020245
Epoch 9158/10000, Prediction Accuracy = 65.344%, Loss = 0.2778183460235596
Epoch: 9158, Batch Gradient Norm: 14.599980051836694
Epoch: 9158, Batch Gradient Norm after: 14.599980051836694
Epoch 9159/10000, Prediction Accuracy = 65.32000000000001%, Loss = 0.27749302983283997
Epoch: 9159, Batch Gradient Norm: 13.2965011899754
Epoch: 9159, Batch Gradient Norm after: 13.2965011899754
Epoch 9160/10000, Prediction Accuracy = 65.39599999999999%, Loss = 0.27625276446342467
Epoch: 9160, Batch Gradient Norm: 17.112627322180142
Epoch: 9160, Batch Gradient Norm after: 17.112627322180142
Epoch 9161/10000, Prediction Accuracy = 65.54799999999999%, Loss = 0.2769669473171234
Epoch: 9161, Batch Gradient Norm: 16.168601747020947
Epoch: 9161, Batch Gradient Norm after: 16.168601747020947
Epoch 9162/10000, Prediction Accuracy = 65.348%, Loss = 0.27739569544792175
Epoch: 9162, Batch Gradient Norm: 15.908072177933409
Epoch: 9162, Batch Gradient Norm after: 15.908072177933409
Epoch 9163/10000, Prediction Accuracy = 65.41%, Loss = 0.2778137862682343
Epoch: 9163, Batch Gradient Norm: 15.788576840963492
Epoch: 9163, Batch Gradient Norm after: 15.788576840963492
Epoch 9164/10000, Prediction Accuracy = 65.338%, Loss = 0.27774214148521426
Epoch: 9164, Batch Gradient Norm: 16.98253787108312
Epoch: 9164, Batch Gradient Norm after: 16.98253787108312
Epoch 9165/10000, Prediction Accuracy = 65.436%, Loss = 0.2815566539764404
Epoch: 9165, Batch Gradient Norm: 13.514311222612143
Epoch: 9165, Batch Gradient Norm after: 13.514311222612143
Epoch 9166/10000, Prediction Accuracy = 65.41799999999999%, Loss = 0.27469871640205384
Epoch: 9166, Batch Gradient Norm: 13.013621838915492
Epoch: 9166, Batch Gradient Norm after: 13.013621838915492
Epoch 9167/10000, Prediction Accuracy = 65.48%, Loss = 0.27509581446647646
Epoch: 9167, Batch Gradient Norm: 12.603596776833731
Epoch: 9167, Batch Gradient Norm after: 12.603596776833731
Epoch 9168/10000, Prediction Accuracy = 65.42%, Loss = 0.27377307415008545
Epoch: 9168, Batch Gradient Norm: 13.403116990526357
Epoch: 9168, Batch Gradient Norm after: 13.403116990526357
Epoch 9169/10000, Prediction Accuracy = 65.334%, Loss = 0.2743471682071686
Epoch: 9169, Batch Gradient Norm: 15.374641049211595
Epoch: 9169, Batch Gradient Norm after: 15.374641049211595
Epoch 9170/10000, Prediction Accuracy = 65.45200000000001%, Loss = 0.27644010782241824
Epoch: 9170, Batch Gradient Norm: 18.97114013315848
Epoch: 9170, Batch Gradient Norm after: 18.97114013315848
Epoch 9171/10000, Prediction Accuracy = 65.426%, Loss = 0.2799937129020691
Epoch: 9171, Batch Gradient Norm: 19.354678182526357
Epoch: 9171, Batch Gradient Norm after: 18.98122788029086
Epoch 9172/10000, Prediction Accuracy = 65.392%, Loss = 0.2826611280441284
Epoch: 9172, Batch Gradient Norm: 15.765897278679182
Epoch: 9172, Batch Gradient Norm after: 15.765897278679182
Epoch 9173/10000, Prediction Accuracy = 65.536%, Loss = 0.278518944978714
Epoch: 9173, Batch Gradient Norm: 17.845374547729307
Epoch: 9173, Batch Gradient Norm after: 17.845374547729307
Epoch 9174/10000, Prediction Accuracy = 65.14200000000001%, Loss = 0.2847649037837982
Epoch: 9174, Batch Gradient Norm: 17.305725736508982
Epoch: 9174, Batch Gradient Norm after: 17.305725736508982
Epoch 9175/10000, Prediction Accuracy = 65.362%, Loss = 0.2819450259208679
Epoch: 9175, Batch Gradient Norm: 14.870047146771697
Epoch: 9175, Batch Gradient Norm after: 14.870047146771697
Epoch 9176/10000, Prediction Accuracy = 65.566%, Loss = 0.2783825099468231
Epoch: 9176, Batch Gradient Norm: 15.952673990421047
Epoch: 9176, Batch Gradient Norm after: 15.952673990421047
Epoch 9177/10000, Prediction Accuracy = 65.31%, Loss = 0.27750363945961
Epoch: 9177, Batch Gradient Norm: 16.878508449108725
Epoch: 9177, Batch Gradient Norm after: 16.878508449108725
Epoch 9178/10000, Prediction Accuracy = 65.318%, Loss = 0.2808448851108551
Epoch: 9178, Batch Gradient Norm: 16.55412002453761
Epoch: 9178, Batch Gradient Norm after: 16.55412002453761
Epoch 9179/10000, Prediction Accuracy = 65.31%, Loss = 0.2820857524871826
Epoch: 9179, Batch Gradient Norm: 17.48009625384494
Epoch: 9179, Batch Gradient Norm after: 17.48009625384494
Epoch 9180/10000, Prediction Accuracy = 65.30199999999999%, Loss = 0.28162744641304016
Epoch: 9180, Batch Gradient Norm: 20.22117883447265
Epoch: 9180, Batch Gradient Norm after: 19.66972335241549
Epoch 9181/10000, Prediction Accuracy = 65.276%, Loss = 0.28708388805389407
Epoch: 9181, Batch Gradient Norm: 19.339992247459925
Epoch: 9181, Batch Gradient Norm after: 18.45520471120122
Epoch 9182/10000, Prediction Accuracy = 65.248%, Loss = 0.28424187898635866
Epoch: 9182, Batch Gradient Norm: 17.975351816217568
Epoch: 9182, Batch Gradient Norm after: 17.975351816217568
Epoch 9183/10000, Prediction Accuracy = 65.44%, Loss = 0.2806259572505951
Epoch: 9183, Batch Gradient Norm: 18.45789924210794
Epoch: 9183, Batch Gradient Norm after: 18.45789924210794
Epoch 9184/10000, Prediction Accuracy = 65.244%, Loss = 0.2816697359085083
Epoch: 9184, Batch Gradient Norm: 16.118170575968485
Epoch: 9184, Batch Gradient Norm after: 16.118170575968485
Epoch 9185/10000, Prediction Accuracy = 65.428%, Loss = 0.2762296497821808
Epoch: 9185, Batch Gradient Norm: 15.776851487464572
Epoch: 9185, Batch Gradient Norm after: 15.776851487464572
Epoch 9186/10000, Prediction Accuracy = 65.502%, Loss = 0.2776880323886871
Epoch: 9186, Batch Gradient Norm: 15.16199545369669
Epoch: 9186, Batch Gradient Norm after: 15.16199545369669
Epoch 9187/10000, Prediction Accuracy = 65.398%, Loss = 0.27651030421257017
Epoch: 9187, Batch Gradient Norm: 15.094402612812335
Epoch: 9187, Batch Gradient Norm after: 15.094402612812335
Epoch 9188/10000, Prediction Accuracy = 65.49199999999999%, Loss = 0.2778013527393341
Epoch: 9188, Batch Gradient Norm: 19.278336801084667
Epoch: 9188, Batch Gradient Norm after: 18.67670619467879
Epoch 9189/10000, Prediction Accuracy = 65.184%, Loss = 0.2830101132392883
Epoch: 9189, Batch Gradient Norm: 20.65998945037685
Epoch: 9189, Batch Gradient Norm after: 20.520694827107736
Epoch 9190/10000, Prediction Accuracy = 65.47999999999999%, Loss = 0.28230721354484556
Epoch: 9190, Batch Gradient Norm: 17.396688174355912
Epoch: 9190, Batch Gradient Norm after: 17.396688174355912
Epoch 9191/10000, Prediction Accuracy = 65.604%, Loss = 0.2778201878070831
Epoch: 9191, Batch Gradient Norm: 16.357201014700436
Epoch: 9191, Batch Gradient Norm after: 16.357201014700436
Epoch 9192/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.27649758458137513
Epoch: 9192, Batch Gradient Norm: 22.706680501321856
Epoch: 9192, Batch Gradient Norm after: 19.509773947154144
Epoch 9193/10000, Prediction Accuracy = 65.404%, Loss = 0.2885126888751984
Epoch: 9193, Batch Gradient Norm: 19.473952494521548
Epoch: 9193, Batch Gradient Norm after: 18.557298306770605
Epoch 9194/10000, Prediction Accuracy = 65.436%, Loss = 0.2841284990310669
Epoch: 9194, Batch Gradient Norm: 15.396612519344586
Epoch: 9194, Batch Gradient Norm after: 15.396612519344586
Epoch 9195/10000, Prediction Accuracy = 65.506%, Loss = 0.27748778462409973
Epoch: 9195, Batch Gradient Norm: 13.651572035692608
Epoch: 9195, Batch Gradient Norm after: 13.651572035692608
Epoch 9196/10000, Prediction Accuracy = 65.35999999999999%, Loss = 0.27488443851470945
Epoch: 9196, Batch Gradient Norm: 12.832929644874753
Epoch: 9196, Batch Gradient Norm after: 12.832929644874753
Epoch 9197/10000, Prediction Accuracy = 65.57399999999998%, Loss = 0.2778123438358307
Epoch: 9197, Batch Gradient Norm: 14.288197391406758
Epoch: 9197, Batch Gradient Norm after: 14.288197391406758
Epoch 9198/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.2759357750415802
Epoch: 9198, Batch Gradient Norm: 13.947453024986743
Epoch: 9198, Batch Gradient Norm after: 13.947453024986743
Epoch 9199/10000, Prediction Accuracy = 65.372%, Loss = 0.27495976686477663
Epoch: 9199, Batch Gradient Norm: 16.565937121393596
Epoch: 9199, Batch Gradient Norm after: 16.565937121393596
Epoch 9200/10000, Prediction Accuracy = 65.292%, Loss = 0.28116074204444885
Epoch: 9200, Batch Gradient Norm: 14.16968735077253
Epoch: 9200, Batch Gradient Norm after: 14.16968735077253
Epoch 9201/10000, Prediction Accuracy = 65.494%, Loss = 0.27495583295822146
Epoch: 9201, Batch Gradient Norm: 13.496974145954498
Epoch: 9201, Batch Gradient Norm after: 13.496974145954498
Epoch 9202/10000, Prediction Accuracy = 65.416%, Loss = 0.2736117959022522
Epoch: 9202, Batch Gradient Norm: 16.270500503207916
Epoch: 9202, Batch Gradient Norm after: 16.270500503207916
Epoch 9203/10000, Prediction Accuracy = 65.48599999999999%, Loss = 0.2767463564872742
Epoch: 9203, Batch Gradient Norm: 14.591649299788632
Epoch: 9203, Batch Gradient Norm after: 14.591649299788632
Epoch 9204/10000, Prediction Accuracy = 65.54%, Loss = 0.2771656155586243
Epoch: 9204, Batch Gradient Norm: 14.890299755463822
Epoch: 9204, Batch Gradient Norm after: 14.890299755463822
Epoch 9205/10000, Prediction Accuracy = 65.274%, Loss = 0.2759185552597046
Epoch: 9205, Batch Gradient Norm: 14.563097435482018
Epoch: 9205, Batch Gradient Norm after: 14.563097435482018
Epoch 9206/10000, Prediction Accuracy = 65.504%, Loss = 0.2748320043087006
Epoch: 9206, Batch Gradient Norm: 16.723951504799704
Epoch: 9206, Batch Gradient Norm after: 16.723951504799704
Epoch 9207/10000, Prediction Accuracy = 65.412%, Loss = 0.27907068729400636
Epoch: 9207, Batch Gradient Norm: 15.497770843420776
Epoch: 9207, Batch Gradient Norm after: 15.497770843420776
Epoch 9208/10000, Prediction Accuracy = 65.34200000000001%, Loss = 0.2757387399673462
Epoch: 9208, Batch Gradient Norm: 16.85545086541512
Epoch: 9208, Batch Gradient Norm after: 16.089095599297075
Epoch 9209/10000, Prediction Accuracy = 65.454%, Loss = 0.27734182476997377
Epoch: 9209, Batch Gradient Norm: 15.81807968267984
Epoch: 9209, Batch Gradient Norm after: 15.81807968267984
Epoch 9210/10000, Prediction Accuracy = 65.428%, Loss = 0.2797831654548645
Epoch: 9210, Batch Gradient Norm: 18.332846496412948
Epoch: 9210, Batch Gradient Norm after: 17.89077060964304
Epoch 9211/10000, Prediction Accuracy = 65.424%, Loss = 0.2779879629611969
Epoch: 9211, Batch Gradient Norm: 15.546392544633019
Epoch: 9211, Batch Gradient Norm after: 15.546392544633019
Epoch 9212/10000, Prediction Accuracy = 65.286%, Loss = 0.27462189793586733
Epoch: 9212, Batch Gradient Norm: 14.96261620901964
Epoch: 9212, Batch Gradient Norm after: 14.96261620901964
Epoch 9213/10000, Prediction Accuracy = 65.398%, Loss = 0.2755198121070862
Epoch: 9213, Batch Gradient Norm: 15.680592725518872
Epoch: 9213, Batch Gradient Norm after: 15.680592725518872
Epoch 9214/10000, Prediction Accuracy = 65.49600000000001%, Loss = 0.2752068817615509
Epoch: 9214, Batch Gradient Norm: 17.530392970689867
Epoch: 9214, Batch Gradient Norm after: 16.17655123129057
Epoch 9215/10000, Prediction Accuracy = 65.454%, Loss = 0.27659276127815247
Epoch: 9215, Batch Gradient Norm: 15.575633895948808
Epoch: 9215, Batch Gradient Norm after: 15.575633895948808
Epoch 9216/10000, Prediction Accuracy = 65.348%, Loss = 0.276718932390213
Epoch: 9216, Batch Gradient Norm: 16.323392912360784
Epoch: 9216, Batch Gradient Norm after: 16.323392912360784
Epoch 9217/10000, Prediction Accuracy = 65.30199999999999%, Loss = 0.27833081483840943
Epoch: 9217, Batch Gradient Norm: 18.020163584465703
Epoch: 9217, Batch Gradient Norm after: 17.630953328591165
Epoch 9218/10000, Prediction Accuracy = 65.324%, Loss = 0.2796318352222443
Epoch: 9218, Batch Gradient Norm: 14.975300813496432
Epoch: 9218, Batch Gradient Norm after: 14.975300813496432
Epoch 9219/10000, Prediction Accuracy = 65.326%, Loss = 0.2801307559013367
Epoch: 9219, Batch Gradient Norm: 14.990505204527764
Epoch: 9219, Batch Gradient Norm after: 14.990505204527764
Epoch 9220/10000, Prediction Accuracy = 65.4%, Loss = 0.27689494490623473
Epoch: 9220, Batch Gradient Norm: 16.305237726848265
Epoch: 9220, Batch Gradient Norm after: 16.305237726848265
Epoch 9221/10000, Prediction Accuracy = 65.316%, Loss = 0.2772729516029358
Epoch: 9221, Batch Gradient Norm: 16.514111389225928
Epoch: 9221, Batch Gradient Norm after: 16.514111389225928
Epoch 9222/10000, Prediction Accuracy = 65.368%, Loss = 0.27862991094589235
Epoch: 9222, Batch Gradient Norm: 16.789837433844514
Epoch: 9222, Batch Gradient Norm after: 16.789837433844514
Epoch 9223/10000, Prediction Accuracy = 65.39%, Loss = 0.2787281274795532
Epoch: 9223, Batch Gradient Norm: 17.30784584269126
Epoch: 9223, Batch Gradient Norm after: 17.30784584269126
Epoch 9224/10000, Prediction Accuracy = 65.312%, Loss = 0.2810795307159424
Epoch: 9224, Batch Gradient Norm: 14.783229175304312
Epoch: 9224, Batch Gradient Norm after: 14.783229175304312
Epoch 9225/10000, Prediction Accuracy = 65.31400000000001%, Loss = 0.2770781576633453
Epoch: 9225, Batch Gradient Norm: 14.79718981509639
Epoch: 9225, Batch Gradient Norm after: 14.79718981509639
Epoch 9226/10000, Prediction Accuracy = 65.43%, Loss = 0.2769570231437683
Epoch: 9226, Batch Gradient Norm: 15.42537256437608
Epoch: 9226, Batch Gradient Norm after: 15.42537256437608
Epoch 9227/10000, Prediction Accuracy = 65.362%, Loss = 0.2758538782596588
Epoch: 9227, Batch Gradient Norm: 13.852180125432414
Epoch: 9227, Batch Gradient Norm after: 13.852180125432414
Epoch 9228/10000, Prediction Accuracy = 65.444%, Loss = 0.2743318259716034
Epoch: 9228, Batch Gradient Norm: 14.460908304076288
Epoch: 9228, Batch Gradient Norm after: 14.460908304076288
Epoch 9229/10000, Prediction Accuracy = 65.48400000000001%, Loss = 0.2746558368206024
Epoch: 9229, Batch Gradient Norm: 15.529845088385024
Epoch: 9229, Batch Gradient Norm after: 15.529845088385024
Epoch 9230/10000, Prediction Accuracy = 65.464%, Loss = 0.2752301275730133
Epoch: 9230, Batch Gradient Norm: 16.230430544011053
Epoch: 9230, Batch Gradient Norm after: 16.230430544011053
Epoch 9231/10000, Prediction Accuracy = 65.424%, Loss = 0.278368616104126
Epoch: 9231, Batch Gradient Norm: 19.137947260668543
Epoch: 9231, Batch Gradient Norm after: 18.295154729235552
Epoch 9232/10000, Prediction Accuracy = 65.278%, Loss = 0.2837353885173798
Epoch: 9232, Batch Gradient Norm: 18.657510878036106
Epoch: 9232, Batch Gradient Norm after: 18.657510878036106
Epoch 9233/10000, Prediction Accuracy = 65.40200000000002%, Loss = 0.27927619218826294
Epoch: 9233, Batch Gradient Norm: 15.885977553947683
Epoch: 9233, Batch Gradient Norm after: 15.885977553947683
Epoch 9234/10000, Prediction Accuracy = 65.552%, Loss = 0.2764362096786499
Epoch: 9234, Batch Gradient Norm: 14.35861517907228
Epoch: 9234, Batch Gradient Norm after: 14.35861517907228
Epoch 9235/10000, Prediction Accuracy = 65.36800000000001%, Loss = 0.2787323176860809
Epoch: 9235, Batch Gradient Norm: 13.702257688101858
Epoch: 9235, Batch Gradient Norm after: 13.702257688101858
Epoch 9236/10000, Prediction Accuracy = 65.518%, Loss = 0.2738559007644653
Epoch: 9236, Batch Gradient Norm: 14.899059985238752
Epoch: 9236, Batch Gradient Norm after: 14.899059985238752
Epoch 9237/10000, Prediction Accuracy = 65.336%, Loss = 0.2821558237075806
Epoch: 9237, Batch Gradient Norm: 16.439285889386987
Epoch: 9237, Batch Gradient Norm after: 16.439285889386987
Epoch 9238/10000, Prediction Accuracy = 65.21199999999999%, Loss = 0.27978459000587463
Epoch: 9238, Batch Gradient Norm: 14.44559423773044
Epoch: 9238, Batch Gradient Norm after: 14.44559423773044
Epoch 9239/10000, Prediction Accuracy = 65.434%, Loss = 0.27457013726234436
Epoch: 9239, Batch Gradient Norm: 11.95814930607105
Epoch: 9239, Batch Gradient Norm after: 11.95814930607105
Epoch 9240/10000, Prediction Accuracy = 65.38199999999999%, Loss = 0.27321265935897826
Epoch: 9240, Batch Gradient Norm: 12.351409132444838
Epoch: 9240, Batch Gradient Norm after: 12.351409132444838
Epoch 9241/10000, Prediction Accuracy = 65.276%, Loss = 0.2738367259502411
Epoch: 9241, Batch Gradient Norm: 16.24303076459846
Epoch: 9241, Batch Gradient Norm after: 16.24303076459846
Epoch 9242/10000, Prediction Accuracy = 65.34400000000001%, Loss = 0.27813478112220763
Epoch: 9242, Batch Gradient Norm: 20.551148767643188
Epoch: 9242, Batch Gradient Norm after: 20.29892177631929
Epoch 9243/10000, Prediction Accuracy = 65.47%, Loss = 0.2828680992126465
Epoch: 9243, Batch Gradient Norm: 18.02759619662136
Epoch: 9243, Batch Gradient Norm after: 17.693468940190346
Epoch 9244/10000, Prediction Accuracy = 65.20400000000001%, Loss = 0.2835544288158417
Epoch: 9244, Batch Gradient Norm: 19.07359960410745
Epoch: 9244, Batch Gradient Norm after: 18.343278987980526
Epoch 9245/10000, Prediction Accuracy = 65.372%, Loss = 0.2812125861644745
Epoch: 9245, Batch Gradient Norm: 16.91997370860372
Epoch: 9245, Batch Gradient Norm after: 16.91997370860372
Epoch 9246/10000, Prediction Accuracy = 65.40599999999999%, Loss = 0.2800754249095917
Epoch: 9246, Batch Gradient Norm: 16.69709704322087
Epoch: 9246, Batch Gradient Norm after: 16.69709704322087
Epoch 9247/10000, Prediction Accuracy = 65.398%, Loss = 0.2781523883342743
Epoch: 9247, Batch Gradient Norm: 15.705186402153766
Epoch: 9247, Batch Gradient Norm after: 15.705186402153766
Epoch 9248/10000, Prediction Accuracy = 65.41400000000002%, Loss = 0.27791576385498046
Epoch: 9248, Batch Gradient Norm: 14.66086755862568
Epoch: 9248, Batch Gradient Norm after: 14.66086755862568
Epoch 9249/10000, Prediction Accuracy = 65.322%, Loss = 0.27608434557914735
Epoch: 9249, Batch Gradient Norm: 15.801700402006876
Epoch: 9249, Batch Gradient Norm after: 15.801700402006876
Epoch 9250/10000, Prediction Accuracy = 65.27%, Loss = 0.27647669315338136
Epoch: 9250, Batch Gradient Norm: 12.848134352686653
Epoch: 9250, Batch Gradient Norm after: 12.848134352686653
Epoch 9251/10000, Prediction Accuracy = 65.586%, Loss = 0.2725850820541382
Epoch: 9251, Batch Gradient Norm: 13.318552598666649
Epoch: 9251, Batch Gradient Norm after: 13.318552598666649
Epoch 9252/10000, Prediction Accuracy = 65.6%, Loss = 0.27304638624191285
Epoch: 9252, Batch Gradient Norm: 12.969008818055858
Epoch: 9252, Batch Gradient Norm after: 12.969008818055858
Epoch 9253/10000, Prediction Accuracy = 65.44800000000001%, Loss = 0.2743918657302856
Epoch: 9253, Batch Gradient Norm: 13.59757413737991
Epoch: 9253, Batch Gradient Norm after: 13.59757413737991
Epoch 9254/10000, Prediction Accuracy = 65.356%, Loss = 0.2757365942001343
Epoch: 9254, Batch Gradient Norm: 13.423212268501183
Epoch: 9254, Batch Gradient Norm after: 13.423212268501183
Epoch 9255/10000, Prediction Accuracy = 65.48%, Loss = 0.27588142156600953
Epoch: 9255, Batch Gradient Norm: 14.356489776014723
Epoch: 9255, Batch Gradient Norm after: 14.356489776014723
Epoch 9256/10000, Prediction Accuracy = 65.372%, Loss = 0.27549620270729064
Epoch: 9256, Batch Gradient Norm: 14.925187871726754
Epoch: 9256, Batch Gradient Norm after: 14.925187871726754
Epoch 9257/10000, Prediction Accuracy = 65.512%, Loss = 0.2739418983459473
Epoch: 9257, Batch Gradient Norm: 15.417436722531193
Epoch: 9257, Batch Gradient Norm after: 15.417436722531193
Epoch 9258/10000, Prediction Accuracy = 65.454%, Loss = 0.28030866384506226
Epoch: 9258, Batch Gradient Norm: 15.455728370902634
Epoch: 9258, Batch Gradient Norm after: 15.455728370902634
Epoch 9259/10000, Prediction Accuracy = 65.38600000000001%, Loss = 0.27575446367263795
Epoch: 9259, Batch Gradient Norm: 15.157973760627492
Epoch: 9259, Batch Gradient Norm after: 15.157973760627492
Epoch 9260/10000, Prediction Accuracy = 65.46199999999999%, Loss = 0.27502575516700745
Epoch: 9260, Batch Gradient Norm: 16.529696710684934
Epoch: 9260, Batch Gradient Norm after: 16.529696710684934
Epoch 9261/10000, Prediction Accuracy = 65.45800000000001%, Loss = 0.2771154999732971
Epoch: 9261, Batch Gradient Norm: 17.560429955461753
Epoch: 9261, Batch Gradient Norm after: 17.35989027037836
Epoch 9262/10000, Prediction Accuracy = 65.50399999999999%, Loss = 0.27892040014266967
Epoch: 9262, Batch Gradient Norm: 15.918176949530661
Epoch: 9262, Batch Gradient Norm after: 15.918176949530661
Epoch 9263/10000, Prediction Accuracy = 65.412%, Loss = 0.2757339417934418
Epoch: 9263, Batch Gradient Norm: 12.364347227541733
Epoch: 9263, Batch Gradient Norm after: 12.364347227541733
Epoch 9264/10000, Prediction Accuracy = 65.42%, Loss = 0.2742848634719849
Epoch: 9264, Batch Gradient Norm: 11.428170908462167
Epoch: 9264, Batch Gradient Norm after: 11.428170908462167
Epoch 9265/10000, Prediction Accuracy = 65.386%, Loss = 0.27164092659950256
Epoch: 9265, Batch Gradient Norm: 13.451584300344408
Epoch: 9265, Batch Gradient Norm after: 13.451584300344408
Epoch 9266/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.27282707691192626
Epoch: 9266, Batch Gradient Norm: 11.26081495692972
Epoch: 9266, Batch Gradient Norm after: 11.26081495692972
Epoch 9267/10000, Prediction Accuracy = 65.444%, Loss = 0.27150769233703614
Epoch: 9267, Batch Gradient Norm: 11.723617180292019
Epoch: 9267, Batch Gradient Norm after: 11.723617180292019
Epoch 9268/10000, Prediction Accuracy = 65.52%, Loss = 0.2706485986709595
Epoch: 9268, Batch Gradient Norm: 16.06757772346047
Epoch: 9268, Batch Gradient Norm after: 15.593770837096386
Epoch 9269/10000, Prediction Accuracy = 65.488%, Loss = 0.27847155928611755
Epoch: 9269, Batch Gradient Norm: 16.77446047434615
Epoch: 9269, Batch Gradient Norm after: 16.77446047434615
Epoch 9270/10000, Prediction Accuracy = 65.352%, Loss = 0.2773789167404175
Epoch: 9270, Batch Gradient Norm: 14.719870264851505
Epoch: 9270, Batch Gradient Norm after: 14.719870264851505
Epoch 9271/10000, Prediction Accuracy = 65.394%, Loss = 0.2770041346549988
Epoch: 9271, Batch Gradient Norm: 13.28641081237449
Epoch: 9271, Batch Gradient Norm after: 13.28641081237449
Epoch 9272/10000, Prediction Accuracy = 65.312%, Loss = 0.2744298934936523
Epoch: 9272, Batch Gradient Norm: 14.55055671307854
Epoch: 9272, Batch Gradient Norm after: 14.55055671307854
Epoch 9273/10000, Prediction Accuracy = 65.34200000000001%, Loss = 0.2760241270065308
Epoch: 9273, Batch Gradient Norm: 13.26786759397544
Epoch: 9273, Batch Gradient Norm after: 13.26786759397544
Epoch 9274/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.27143242955207825
Epoch: 9274, Batch Gradient Norm: 13.549719300798232
Epoch: 9274, Batch Gradient Norm after: 13.549719300798232
Epoch 9275/10000, Prediction Accuracy = 65.398%, Loss = 0.27621728777885435
Epoch: 9275, Batch Gradient Norm: 16.616941782847295
Epoch: 9275, Batch Gradient Norm after: 16.616941782847295
Epoch 9276/10000, Prediction Accuracy = 65.47%, Loss = 0.27799721956253054
Epoch: 9276, Batch Gradient Norm: 13.100911874391985
Epoch: 9276, Batch Gradient Norm after: 13.100911874391985
Epoch 9277/10000, Prediction Accuracy = 65.426%, Loss = 0.27153258919715884
Epoch: 9277, Batch Gradient Norm: 12.27482036853945
Epoch: 9277, Batch Gradient Norm after: 12.27482036853945
Epoch 9278/10000, Prediction Accuracy = 65.65%, Loss = 0.2715996026992798
Epoch: 9278, Batch Gradient Norm: 11.575028009029856
Epoch: 9278, Batch Gradient Norm after: 11.575028009029856
Epoch 9279/10000, Prediction Accuracy = 65.556%, Loss = 0.2713889181613922
Epoch: 9279, Batch Gradient Norm: 14.335375814413737
Epoch: 9279, Batch Gradient Norm after: 14.335375814413737
Epoch 9280/10000, Prediction Accuracy = 65.45%, Loss = 0.2742088198661804
Epoch: 9280, Batch Gradient Norm: 14.689946376921029
Epoch: 9280, Batch Gradient Norm after: 14.689946376921029
Epoch 9281/10000, Prediction Accuracy = 65.412%, Loss = 0.27647464275360106
Epoch: 9281, Batch Gradient Norm: 15.577190144983252
Epoch: 9281, Batch Gradient Norm after: 15.577190144983252
Epoch 9282/10000, Prediction Accuracy = 65.414%, Loss = 0.2752253353595734
Epoch: 9282, Batch Gradient Norm: 16.08052275992423
Epoch: 9282, Batch Gradient Norm after: 16.08052275992423
Epoch 9283/10000, Prediction Accuracy = 65.46600000000001%, Loss = 0.2758564054965973
Epoch: 9283, Batch Gradient Norm: 16.984833571271256
Epoch: 9283, Batch Gradient Norm after: 16.984833571271256
Epoch 9284/10000, Prediction Accuracy = 65.22999999999999%, Loss = 0.2803525388240814
Epoch: 9284, Batch Gradient Norm: 15.211423537432747
Epoch: 9284, Batch Gradient Norm after: 15.211423537432747
Epoch 9285/10000, Prediction Accuracy = 65.41199999999999%, Loss = 0.27697221636772157
Epoch: 9285, Batch Gradient Norm: 18.950822310417404
Epoch: 9285, Batch Gradient Norm after: 18.950822310417404
Epoch 9286/10000, Prediction Accuracy = 65.494%, Loss = 0.2824768006801605
Epoch: 9286, Batch Gradient Norm: 17.823791754010347
Epoch: 9286, Batch Gradient Norm after: 17.823791754010347
Epoch 9287/10000, Prediction Accuracy = 65.546%, Loss = 0.28036150336265564
Epoch: 9287, Batch Gradient Norm: 19.574109028266356
Epoch: 9287, Batch Gradient Norm after: 19.14444391686246
Epoch 9288/10000, Prediction Accuracy = 65.216%, Loss = 0.28146607279777525
Epoch: 9288, Batch Gradient Norm: 20.567309259238513
Epoch: 9288, Batch Gradient Norm after: 20.43850971202703
Epoch 9289/10000, Prediction Accuracy = 65.54400000000001%, Loss = 0.28176259994506836
Epoch: 9289, Batch Gradient Norm: 18.65168081826522
Epoch: 9289, Batch Gradient Norm after: 18.59052204176181
Epoch 9290/10000, Prediction Accuracy = 65.266%, Loss = 0.2821681022644043
Epoch: 9290, Batch Gradient Norm: 15.06904840451807
Epoch: 9290, Batch Gradient Norm after: 15.06904840451807
Epoch 9291/10000, Prediction Accuracy = 65.292%, Loss = 0.27591889500617983
Epoch: 9291, Batch Gradient Norm: 12.966857411438639
Epoch: 9291, Batch Gradient Norm after: 12.966857411438639
Epoch 9292/10000, Prediction Accuracy = 65.52000000000001%, Loss = 0.2735531210899353
Epoch: 9292, Batch Gradient Norm: 12.720286532049121
Epoch: 9292, Batch Gradient Norm after: 12.720286532049121
Epoch 9293/10000, Prediction Accuracy = 65.276%, Loss = 0.2750631272792816
Epoch: 9293, Batch Gradient Norm: 11.972486110544994
Epoch: 9293, Batch Gradient Norm after: 11.972486110544994
Epoch 9294/10000, Prediction Accuracy = 65.5%, Loss = 0.2712286591529846
Epoch: 9294, Batch Gradient Norm: 13.3107650677398
Epoch: 9294, Batch Gradient Norm after: 13.3107650677398
Epoch 9295/10000, Prediction Accuracy = 65.556%, Loss = 0.27520684599876405
Epoch: 9295, Batch Gradient Norm: 14.779278652552659
Epoch: 9295, Batch Gradient Norm after: 14.779278652552659
Epoch 9296/10000, Prediction Accuracy = 65.516%, Loss = 0.27342583537101744
Epoch: 9296, Batch Gradient Norm: 14.513653504545998
Epoch: 9296, Batch Gradient Norm after: 14.513653504545998
Epoch 9297/10000, Prediction Accuracy = 65.55199999999999%, Loss = 0.27596275210380555
Epoch: 9297, Batch Gradient Norm: 13.325782950017176
Epoch: 9297, Batch Gradient Norm after: 13.325782950017176
Epoch 9298/10000, Prediction Accuracy = 65.53399999999999%, Loss = 0.27394009828567506
Epoch: 9298, Batch Gradient Norm: 13.110130056674953
Epoch: 9298, Batch Gradient Norm after: 13.110130056674953
Epoch 9299/10000, Prediction Accuracy = 65.526%, Loss = 0.2727987229824066
Epoch: 9299, Batch Gradient Norm: 11.942922169429341
Epoch: 9299, Batch Gradient Norm after: 11.942922169429341
Epoch 9300/10000, Prediction Accuracy = 65.51599999999999%, Loss = 0.26917982697486875
Epoch: 9300, Batch Gradient Norm: 13.365872015783662
Epoch: 9300, Batch Gradient Norm after: 13.365872015783662
Epoch 9301/10000, Prediction Accuracy = 65.388%, Loss = 0.2732388436794281
Epoch: 9301, Batch Gradient Norm: 14.199223439923653
Epoch: 9301, Batch Gradient Norm after: 14.199223439923653
Epoch 9302/10000, Prediction Accuracy = 65.398%, Loss = 0.276055771112442
Epoch: 9302, Batch Gradient Norm: 14.896636519771958
Epoch: 9302, Batch Gradient Norm after: 14.896636519771958
Epoch 9303/10000, Prediction Accuracy = 65.444%, Loss = 0.2764274299144745
Epoch: 9303, Batch Gradient Norm: 12.45791267989481
Epoch: 9303, Batch Gradient Norm after: 12.45791267989481
Epoch 9304/10000, Prediction Accuracy = 65.40599999999999%, Loss = 0.2723601460456848
Epoch: 9304, Batch Gradient Norm: 14.204458464000682
Epoch: 9304, Batch Gradient Norm after: 14.204458464000682
Epoch 9305/10000, Prediction Accuracy = 65.474%, Loss = 0.2716656386852264
Epoch: 9305, Batch Gradient Norm: 11.36058568574459
Epoch: 9305, Batch Gradient Norm after: 11.36058568574459
Epoch 9306/10000, Prediction Accuracy = 65.364%, Loss = 0.27198724150657655
Epoch: 9306, Batch Gradient Norm: 14.223982164163129
Epoch: 9306, Batch Gradient Norm after: 14.223982164163129
Epoch 9307/10000, Prediction Accuracy = 65.49%, Loss = 0.2731235444545746
Epoch: 9307, Batch Gradient Norm: 14.69774324748666
Epoch: 9307, Batch Gradient Norm after: 14.69774324748666
Epoch 9308/10000, Prediction Accuracy = 65.564%, Loss = 0.2730140149593353
Epoch: 9308, Batch Gradient Norm: 18.27123245277694
Epoch: 9308, Batch Gradient Norm after: 18.27123245277694
Epoch 9309/10000, Prediction Accuracy = 65.224%, Loss = 0.28253948092460635
Epoch: 9309, Batch Gradient Norm: 17.738639963233204
Epoch: 9309, Batch Gradient Norm after: 17.50589492805605
Epoch 9310/10000, Prediction Accuracy = 65.49799999999999%, Loss = 0.2788774609565735
Epoch: 9310, Batch Gradient Norm: 18.510696471436937
Epoch: 9310, Batch Gradient Norm after: 18.510696471436937
Epoch 9311/10000, Prediction Accuracy = 65.40599999999999%, Loss = 0.2789721727371216
Epoch: 9311, Batch Gradient Norm: 15.750824795454406
Epoch: 9311, Batch Gradient Norm after: 15.750824795454406
Epoch 9312/10000, Prediction Accuracy = 65.55799999999999%, Loss = 0.27819434404373167
Epoch: 9312, Batch Gradient Norm: 15.752381171358563
Epoch: 9312, Batch Gradient Norm after: 15.752381171358563
Epoch 9313/10000, Prediction Accuracy = 65.424%, Loss = 0.27607297897338867
Epoch: 9313, Batch Gradient Norm: 14.727413490901792
Epoch: 9313, Batch Gradient Norm after: 14.727413490901792
Epoch 9314/10000, Prediction Accuracy = 65.424%, Loss = 0.27327866554260255
Epoch: 9314, Batch Gradient Norm: 17.619657324511838
Epoch: 9314, Batch Gradient Norm after: 17.619657324511838
Epoch 9315/10000, Prediction Accuracy = 65.534%, Loss = 0.276069313287735
Epoch: 9315, Batch Gradient Norm: 12.402399497158378
Epoch: 9315, Batch Gradient Norm after: 12.402399497158378
Epoch 9316/10000, Prediction Accuracy = 65.442%, Loss = 0.271957540512085
Epoch: 9316, Batch Gradient Norm: 13.694018974949692
Epoch: 9316, Batch Gradient Norm after: 13.694018974949692
Epoch 9317/10000, Prediction Accuracy = 65.524%, Loss = 0.2747324347496033
Epoch: 9317, Batch Gradient Norm: 15.5454214828602
Epoch: 9317, Batch Gradient Norm after: 15.5454214828602
Epoch 9318/10000, Prediction Accuracy = 65.518%, Loss = 0.27678931355476377
Epoch: 9318, Batch Gradient Norm: 17.457596303488998
Epoch: 9318, Batch Gradient Norm after: 17.173485628370305
Epoch 9319/10000, Prediction Accuracy = 65.53999999999999%, Loss = 0.27806739807128905
Epoch: 9319, Batch Gradient Norm: 13.0946322982613
Epoch: 9319, Batch Gradient Norm after: 13.0946322982613
Epoch 9320/10000, Prediction Accuracy = 65.46000000000001%, Loss = 0.27124438285827634
Epoch: 9320, Batch Gradient Norm: 13.449584568158066
Epoch: 9320, Batch Gradient Norm after: 13.449584568158066
Epoch 9321/10000, Prediction Accuracy = 65.532%, Loss = 0.2734231114387512
Epoch: 9321, Batch Gradient Norm: 15.981758653931887
Epoch: 9321, Batch Gradient Norm after: 15.981758653931887
Epoch 9322/10000, Prediction Accuracy = 65.50399999999999%, Loss = 0.27715904712677003
Epoch: 9322, Batch Gradient Norm: 14.796893613610518
Epoch: 9322, Batch Gradient Norm after: 14.796893613610518
Epoch 9323/10000, Prediction Accuracy = 65.304%, Loss = 0.2786913216114044
Epoch: 9323, Batch Gradient Norm: 12.025702228498385
Epoch: 9323, Batch Gradient Norm after: 12.025702228498385
Epoch 9324/10000, Prediction Accuracy = 65.55999999999999%, Loss = 0.2713705122470856
Epoch: 9324, Batch Gradient Norm: 13.23577395321597
Epoch: 9324, Batch Gradient Norm after: 13.23577395321597
Epoch 9325/10000, Prediction Accuracy = 65.41%, Loss = 0.2738309681415558
Epoch: 9325, Batch Gradient Norm: 14.072238400154289
Epoch: 9325, Batch Gradient Norm after: 14.072238400154289
Epoch 9326/10000, Prediction Accuracy = 65.464%, Loss = 0.27331130504608153
Epoch: 9326, Batch Gradient Norm: 15.284398880661577
Epoch: 9326, Batch Gradient Norm after: 15.284398880661577
Epoch 9327/10000, Prediction Accuracy = 65.458%, Loss = 0.2747686505317688
Epoch: 9327, Batch Gradient Norm: 14.25978312304564
Epoch: 9327, Batch Gradient Norm after: 14.25978312304564
Epoch 9328/10000, Prediction Accuracy = 65.47999999999999%, Loss = 0.2741865038871765
Epoch: 9328, Batch Gradient Norm: 17.757890840293733
Epoch: 9328, Batch Gradient Norm after: 17.757890840293733
Epoch 9329/10000, Prediction Accuracy = 65.36%, Loss = 0.2810835003852844
Epoch: 9329, Batch Gradient Norm: 17.005932119822496
Epoch: 9329, Batch Gradient Norm after: 17.005932119822496
Epoch 9330/10000, Prediction Accuracy = 65.55%, Loss = 0.27769752144813536
Epoch: 9330, Batch Gradient Norm: 16.399145426036338
Epoch: 9330, Batch Gradient Norm after: 16.399145426036338
Epoch 9331/10000, Prediction Accuracy = 65.374%, Loss = 0.27773427963256836
Epoch: 9331, Batch Gradient Norm: 16.549785949369603
Epoch: 9331, Batch Gradient Norm after: 16.549785949369603
Epoch 9332/10000, Prediction Accuracy = 65.40799999999999%, Loss = 0.2762048006057739
Epoch: 9332, Batch Gradient Norm: 15.33214368443932
Epoch: 9332, Batch Gradient Norm after: 15.33214368443932
Epoch 9333/10000, Prediction Accuracy = 65.308%, Loss = 0.2754092991352081
Epoch: 9333, Batch Gradient Norm: 15.470800643598489
Epoch: 9333, Batch Gradient Norm after: 15.470800643598489
Epoch 9334/10000, Prediction Accuracy = 65.36600000000001%, Loss = 0.27745245695114135
Epoch: 9334, Batch Gradient Norm: 14.875428194498104
Epoch: 9334, Batch Gradient Norm after: 14.875428194498104
Epoch 9335/10000, Prediction Accuracy = 65.422%, Loss = 0.2734509527683258
Epoch: 9335, Batch Gradient Norm: 13.491148556616402
Epoch: 9335, Batch Gradient Norm after: 13.491148556616402
Epoch 9336/10000, Prediction Accuracy = 65.408%, Loss = 0.2788577675819397
Epoch: 9336, Batch Gradient Norm: 13.555656215003022
Epoch: 9336, Batch Gradient Norm after: 13.555656215003022
Epoch 9337/10000, Prediction Accuracy = 65.45399999999998%, Loss = 0.2773634374141693
Epoch: 9337, Batch Gradient Norm: 16.044908389635864
Epoch: 9337, Batch Gradient Norm after: 16.044908389635864
Epoch 9338/10000, Prediction Accuracy = 65.558%, Loss = 0.2764488995075226
Epoch: 9338, Batch Gradient Norm: 16.05495608878718
Epoch: 9338, Batch Gradient Norm after: 16.05495608878718
Epoch 9339/10000, Prediction Accuracy = 65.616%, Loss = 0.27500617504119873
Epoch: 9339, Batch Gradient Norm: 15.658107756777705
Epoch: 9339, Batch Gradient Norm after: 15.658107756777705
Epoch 9340/10000, Prediction Accuracy = 65.46%, Loss = 0.2741649329662323
Epoch: 9340, Batch Gradient Norm: 18.727518100662234
Epoch: 9340, Batch Gradient Norm after: 18.727518100662234
Epoch 9341/10000, Prediction Accuracy = 65.348%, Loss = 0.279657381772995
Epoch: 9341, Batch Gradient Norm: 16.905566023527946
Epoch: 9341, Batch Gradient Norm after: 16.905566023527946
Epoch 9342/10000, Prediction Accuracy = 65.624%, Loss = 0.275791996717453
Epoch: 9342, Batch Gradient Norm: 16.781316616167395
Epoch: 9342, Batch Gradient Norm after: 16.781316616167395
Epoch 9343/10000, Prediction Accuracy = 65.45%, Loss = 0.2767547905445099
Epoch: 9343, Batch Gradient Norm: 19.922268952075303
Epoch: 9343, Batch Gradient Norm after: 19.482282456886196
Epoch 9344/10000, Prediction Accuracy = 65.39%, Loss = 0.28242777585983275
Epoch: 9344, Batch Gradient Norm: 19.43587806541777
Epoch: 9344, Batch Gradient Norm after: 19.038147065170932
Epoch 9345/10000, Prediction Accuracy = 65.52799999999999%, Loss = 0.2813511252403259
Epoch: 9345, Batch Gradient Norm: 16.481010336256936
Epoch: 9345, Batch Gradient Norm after: 16.481010336256936
Epoch 9346/10000, Prediction Accuracy = 65.46000000000001%, Loss = 0.2775578320026398
Epoch: 9346, Batch Gradient Norm: 16.20094653843546
Epoch: 9346, Batch Gradient Norm after: 16.20094653843546
Epoch 9347/10000, Prediction Accuracy = 65.46799999999999%, Loss = 0.2774081230163574
Epoch: 9347, Batch Gradient Norm: 16.738894575781575
Epoch: 9347, Batch Gradient Norm after: 16.738894575781575
Epoch 9348/10000, Prediction Accuracy = 65.596%, Loss = 0.28014275431632996
Epoch: 9348, Batch Gradient Norm: 14.602910267551477
Epoch: 9348, Batch Gradient Norm after: 14.602910267551477
Epoch 9349/10000, Prediction Accuracy = 65.392%, Loss = 0.27634269595146177
Epoch: 9349, Batch Gradient Norm: 12.740818287129557
Epoch: 9349, Batch Gradient Norm after: 12.740818287129557
Epoch 9350/10000, Prediction Accuracy = 65.44%, Loss = 0.2721020996570587
Epoch: 9350, Batch Gradient Norm: 12.87358849623975
Epoch: 9350, Batch Gradient Norm after: 12.87358849623975
Epoch 9351/10000, Prediction Accuracy = 65.348%, Loss = 0.2766446590423584
Epoch: 9351, Batch Gradient Norm: 13.604559696229469
Epoch: 9351, Batch Gradient Norm after: 13.604559696229469
Epoch 9352/10000, Prediction Accuracy = 65.32%, Loss = 0.275479257106781
Epoch: 9352, Batch Gradient Norm: 13.968142909838228
Epoch: 9352, Batch Gradient Norm after: 13.968142909838228
Epoch 9353/10000, Prediction Accuracy = 65.556%, Loss = 0.27777225971221925
Epoch: 9353, Batch Gradient Norm: 16.13153921160742
Epoch: 9353, Batch Gradient Norm after: 16.13153921160742
Epoch 9354/10000, Prediction Accuracy = 65.47%, Loss = 0.27676003575325014
Epoch: 9354, Batch Gradient Norm: 17.509458983079618
Epoch: 9354, Batch Gradient Norm after: 17.509458983079618
Epoch 9355/10000, Prediction Accuracy = 65.6%, Loss = 0.27707549929618835
Epoch: 9355, Batch Gradient Norm: 17.375472007185223
Epoch: 9355, Batch Gradient Norm after: 17.375472007185223
Epoch 9356/10000, Prediction Accuracy = 65.412%, Loss = 0.2770837128162384
Epoch: 9356, Batch Gradient Norm: 15.802645397613574
Epoch: 9356, Batch Gradient Norm after: 15.802645397613574
Epoch 9357/10000, Prediction Accuracy = 65.65%, Loss = 0.275070583820343
Epoch: 9357, Batch Gradient Norm: 14.38848338808204
Epoch: 9357, Batch Gradient Norm after: 14.38848338808204
Epoch 9358/10000, Prediction Accuracy = 65.47%, Loss = 0.2754634261131287
Epoch: 9358, Batch Gradient Norm: 16.595039113492234
Epoch: 9358, Batch Gradient Norm after: 16.595039113492234
Epoch 9359/10000, Prediction Accuracy = 65.45000000000002%, Loss = 0.27627053260803225
Epoch: 9359, Batch Gradient Norm: 13.971401311074244
Epoch: 9359, Batch Gradient Norm after: 13.971401311074244
Epoch 9360/10000, Prediction Accuracy = 65.512%, Loss = 0.27160600423812864
Epoch: 9360, Batch Gradient Norm: 14.01231841910958
Epoch: 9360, Batch Gradient Norm after: 14.01231841910958
Epoch 9361/10000, Prediction Accuracy = 65.55799999999999%, Loss = 0.27419331669807434
Epoch: 9361, Batch Gradient Norm: 14.171315239608099
Epoch: 9361, Batch Gradient Norm after: 14.171315239608099
Epoch 9362/10000, Prediction Accuracy = 65.378%, Loss = 0.27361072301864625
Epoch: 9362, Batch Gradient Norm: 13.248380673290429
Epoch: 9362, Batch Gradient Norm after: 13.248380673290429
Epoch 9363/10000, Prediction Accuracy = 65.506%, Loss = 0.2719468891620636
Epoch: 9363, Batch Gradient Norm: 13.960146940865116
Epoch: 9363, Batch Gradient Norm after: 13.960146940865116
Epoch 9364/10000, Prediction Accuracy = 65.44800000000001%, Loss = 0.2718961715698242
Epoch: 9364, Batch Gradient Norm: 14.927261003633658
Epoch: 9364, Batch Gradient Norm after: 14.927261003633658
Epoch 9365/10000, Prediction Accuracy = 65.54599999999999%, Loss = 0.2749112665653229
Epoch: 9365, Batch Gradient Norm: 12.978757492427725
Epoch: 9365, Batch Gradient Norm after: 12.978757492427725
Epoch 9366/10000, Prediction Accuracy = 65.434%, Loss = 0.27189531922340393
Epoch: 9366, Batch Gradient Norm: 15.541434710418862
Epoch: 9366, Batch Gradient Norm after: 15.541434710418862
Epoch 9367/10000, Prediction Accuracy = 65.43799999999999%, Loss = 0.2765369415283203
Epoch: 9367, Batch Gradient Norm: 19.415418870272493
Epoch: 9367, Batch Gradient Norm after: 19.347372537008003
Epoch 9368/10000, Prediction Accuracy = 65.494%, Loss = 0.28214186429977417
Epoch: 9368, Batch Gradient Norm: 19.74841646442804
Epoch: 9368, Batch Gradient Norm after: 18.851636472334494
Epoch 9369/10000, Prediction Accuracy = 65.328%, Loss = 0.28094324469566345
Epoch: 9369, Batch Gradient Norm: 19.06441160974304
Epoch: 9369, Batch Gradient Norm after: 19.06441160974304
Epoch 9370/10000, Prediction Accuracy = 65.47800000000001%, Loss = 0.27962064146995547
Epoch: 9370, Batch Gradient Norm: 18.582706748502225
Epoch: 9370, Batch Gradient Norm after: 18.582706748502225
Epoch 9371/10000, Prediction Accuracy = 65.424%, Loss = 0.2788205802440643
Epoch: 9371, Batch Gradient Norm: 15.793341458068175
Epoch: 9371, Batch Gradient Norm after: 15.793341458068175
Epoch 9372/10000, Prediction Accuracy = 65.4%, Loss = 0.2759199023246765
Epoch: 9372, Batch Gradient Norm: 14.95000120447272
Epoch: 9372, Batch Gradient Norm after: 14.95000120447272
Epoch 9373/10000, Prediction Accuracy = 65.458%, Loss = 0.2735224008560181
Epoch: 9373, Batch Gradient Norm: 18.41640845267704
Epoch: 9373, Batch Gradient Norm after: 18.41640845267704
Epoch 9374/10000, Prediction Accuracy = 65.48%, Loss = 0.27995572090148924
Epoch: 9374, Batch Gradient Norm: 19.775020175877668
Epoch: 9374, Batch Gradient Norm after: 19.775020175877668
Epoch 9375/10000, Prediction Accuracy = 65.36%, Loss = 0.2853574752807617
Epoch: 9375, Batch Gradient Norm: 20.434280652936888
Epoch: 9375, Batch Gradient Norm after: 19.103768988355416
Epoch 9376/10000, Prediction Accuracy = 65.41600000000001%, Loss = 0.2836789727210999
Epoch: 9376, Batch Gradient Norm: 17.209799031499525
Epoch: 9376, Batch Gradient Norm after: 17.209799031499525
Epoch 9377/10000, Prediction Accuracy = 65.416%, Loss = 0.27761186957359313
Epoch: 9377, Batch Gradient Norm: 15.575639551339197
Epoch: 9377, Batch Gradient Norm after: 15.575639551339197
Epoch 9378/10000, Prediction Accuracy = 65.648%, Loss = 0.2728822350502014
Epoch: 9378, Batch Gradient Norm: 15.76076569944018
Epoch: 9378, Batch Gradient Norm after: 15.76076569944018
Epoch 9379/10000, Prediction Accuracy = 65.47%, Loss = 0.27561141848564147
Epoch: 9379, Batch Gradient Norm: 14.676092889653981
Epoch: 9379, Batch Gradient Norm after: 14.676092889653981
Epoch 9380/10000, Prediction Accuracy = 65.50999999999999%, Loss = 0.27213533520698546
Epoch: 9380, Batch Gradient Norm: 13.118434396005494
Epoch: 9380, Batch Gradient Norm after: 13.118434396005494
Epoch 9381/10000, Prediction Accuracy = 65.634%, Loss = 0.27046036124229433
Epoch: 9381, Batch Gradient Norm: 15.90695212093476
Epoch: 9381, Batch Gradient Norm after: 15.90695212093476
Epoch 9382/10000, Prediction Accuracy = 65.5%, Loss = 0.27426925897598264
Epoch: 9382, Batch Gradient Norm: 14.482471692364669
Epoch: 9382, Batch Gradient Norm after: 14.482471692364669
Epoch 9383/10000, Prediction Accuracy = 65.49000000000001%, Loss = 0.27575393915176394
Epoch: 9383, Batch Gradient Norm: 15.250263287811565
Epoch: 9383, Batch Gradient Norm after: 15.250263287811565
Epoch 9384/10000, Prediction Accuracy = 65.572%, Loss = 0.27810977697372435
Epoch: 9384, Batch Gradient Norm: 17.432625294104046
Epoch: 9384, Batch Gradient Norm after: 17.00924400042571
Epoch 9385/10000, Prediction Accuracy = 65.588%, Loss = 0.2762613773345947
Epoch: 9385, Batch Gradient Norm: 17.4615753983067
Epoch: 9385, Batch Gradient Norm after: 17.3422182174402
Epoch 9386/10000, Prediction Accuracy = 65.34200000000001%, Loss = 0.28172060251235964
Epoch: 9386, Batch Gradient Norm: 14.567870644827446
Epoch: 9386, Batch Gradient Norm after: 14.567870644827446
Epoch 9387/10000, Prediction Accuracy = 65.53999999999999%, Loss = 0.27336058020591736
Epoch: 9387, Batch Gradient Norm: 14.489053971128286
Epoch: 9387, Batch Gradient Norm after: 14.489053971128286
Epoch 9388/10000, Prediction Accuracy = 65.35%, Loss = 0.27403836846351626
Epoch: 9388, Batch Gradient Norm: 15.674591106584149
Epoch: 9388, Batch Gradient Norm after: 15.674591106584149
Epoch 9389/10000, Prediction Accuracy = 65.368%, Loss = 0.2753335177898407
Epoch: 9389, Batch Gradient Norm: 17.302435042633753
Epoch: 9389, Batch Gradient Norm after: 17.302435042633753
Epoch 9390/10000, Prediction Accuracy = 65.47200000000001%, Loss = 0.2788164496421814
Epoch: 9390, Batch Gradient Norm: 15.639409110101852
Epoch: 9390, Batch Gradient Norm after: 15.639409110101852
Epoch 9391/10000, Prediction Accuracy = 65.496%, Loss = 0.27748669385910035
Epoch: 9391, Batch Gradient Norm: 18.103672068570148
Epoch: 9391, Batch Gradient Norm after: 18.103672068570148
Epoch 9392/10000, Prediction Accuracy = 65.378%, Loss = 0.28004845380783083
Epoch: 9392, Batch Gradient Norm: 15.325258293024035
Epoch: 9392, Batch Gradient Norm after: 15.325258293024035
Epoch 9393/10000, Prediction Accuracy = 65.36800000000001%, Loss = 0.27792162299156187
Epoch: 9393, Batch Gradient Norm: 14.510581588369519
Epoch: 9393, Batch Gradient Norm after: 14.510581588369519
Epoch 9394/10000, Prediction Accuracy = 65.542%, Loss = 0.2739686191082001
Epoch: 9394, Batch Gradient Norm: 15.021380618957075
Epoch: 9394, Batch Gradient Norm after: 15.021380618957075
Epoch 9395/10000, Prediction Accuracy = 65.628%, Loss = 0.2737574279308319
Epoch: 9395, Batch Gradient Norm: 14.680318025099362
Epoch: 9395, Batch Gradient Norm after: 14.680318025099362
Epoch 9396/10000, Prediction Accuracy = 65.594%, Loss = 0.27290700674057006
Epoch: 9396, Batch Gradient Norm: 14.2118829503649
Epoch: 9396, Batch Gradient Norm after: 14.2118829503649
Epoch 9397/10000, Prediction Accuracy = 65.522%, Loss = 0.2742536187171936
Epoch: 9397, Batch Gradient Norm: 14.261596011569363
Epoch: 9397, Batch Gradient Norm after: 14.261596011569363
Epoch 9398/10000, Prediction Accuracy = 65.55999999999999%, Loss = 0.2737753868103027
Epoch: 9398, Batch Gradient Norm: 15.259432962967182
Epoch: 9398, Batch Gradient Norm after: 15.259432962967182
Epoch 9399/10000, Prediction Accuracy = 65.426%, Loss = 0.2752650558948517
Epoch: 9399, Batch Gradient Norm: 13.412870620946522
Epoch: 9399, Batch Gradient Norm after: 13.412870620946522
Epoch 9400/10000, Prediction Accuracy = 65.39000000000001%, Loss = 0.27208539843559265
Epoch: 9400, Batch Gradient Norm: 15.567640625605573
Epoch: 9400, Batch Gradient Norm after: 15.567640625605573
Epoch 9401/10000, Prediction Accuracy = 65.53%, Loss = 0.2732711911201477
Epoch: 9401, Batch Gradient Norm: 14.080171189727773
Epoch: 9401, Batch Gradient Norm after: 14.080171189727773
Epoch 9402/10000, Prediction Accuracy = 65.418%, Loss = 0.2743608593940735
Epoch: 9402, Batch Gradient Norm: 13.243291595902901
Epoch: 9402, Batch Gradient Norm after: 13.243291595902901
Epoch 9403/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.2711270391941071
Epoch: 9403, Batch Gradient Norm: 14.110194915008753
Epoch: 9403, Batch Gradient Norm after: 14.110194915008753
Epoch 9404/10000, Prediction Accuracy = 65.54%, Loss = 0.2724070310592651
Epoch: 9404, Batch Gradient Norm: 14.787533048500778
Epoch: 9404, Batch Gradient Norm after: 14.787533048500778
Epoch 9405/10000, Prediction Accuracy = 65.54%, Loss = 0.27337626814842225
Epoch: 9405, Batch Gradient Norm: 14.778340261134163
Epoch: 9405, Batch Gradient Norm after: 14.778340261134163
Epoch 9406/10000, Prediction Accuracy = 65.422%, Loss = 0.27322606444358827
Epoch: 9406, Batch Gradient Norm: 13.349319450863007
Epoch: 9406, Batch Gradient Norm after: 13.349319450863007
Epoch 9407/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.27138583064079286
Epoch: 9407, Batch Gradient Norm: 15.826356792610746
Epoch: 9407, Batch Gradient Norm after: 15.826356792610746
Epoch 9408/10000, Prediction Accuracy = 65.434%, Loss = 0.2776517987251282
Epoch: 9408, Batch Gradient Norm: 15.424003416252534
Epoch: 9408, Batch Gradient Norm after: 15.424003416252534
Epoch 9409/10000, Prediction Accuracy = 65.38399999999999%, Loss = 0.2764224112033844
Epoch: 9409, Batch Gradient Norm: 19.347373251210445
Epoch: 9409, Batch Gradient Norm after: 18.869725044191014
Epoch 9410/10000, Prediction Accuracy = 65.28999999999999%, Loss = 0.2818718135356903
Epoch: 9410, Batch Gradient Norm: 18.80695354088392
Epoch: 9410, Batch Gradient Norm after: 18.80695354088392
Epoch 9411/10000, Prediction Accuracy = 65.56%, Loss = 0.2785878598690033
Epoch: 9411, Batch Gradient Norm: 15.436814195113257
Epoch: 9411, Batch Gradient Norm after: 15.436814195113257
Epoch 9412/10000, Prediction Accuracy = 65.34%, Loss = 0.27760422229766846
Epoch: 9412, Batch Gradient Norm: 14.320756556425753
Epoch: 9412, Batch Gradient Norm after: 14.320756556425753
Epoch 9413/10000, Prediction Accuracy = 65.54999999999998%, Loss = 0.2712447226047516
Epoch: 9413, Batch Gradient Norm: 13.033925877002352
Epoch: 9413, Batch Gradient Norm after: 13.033925877002352
Epoch 9414/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.2712434589862823
Epoch: 9414, Batch Gradient Norm: 14.690445740867924
Epoch: 9414, Batch Gradient Norm after: 14.690445740867924
Epoch 9415/10000, Prediction Accuracy = 65.574%, Loss = 0.27353258728981017
Epoch: 9415, Batch Gradient Norm: 16.937913121279053
Epoch: 9415, Batch Gradient Norm after: 16.937913121279053
Epoch 9416/10000, Prediction Accuracy = 65.424%, Loss = 0.2778195023536682
Epoch: 9416, Batch Gradient Norm: 15.040492856605171
Epoch: 9416, Batch Gradient Norm after: 15.040492856605171
Epoch 9417/10000, Prediction Accuracy = 65.548%, Loss = 0.2735355317592621
Epoch: 9417, Batch Gradient Norm: 18.003999657976987
Epoch: 9417, Batch Gradient Norm after: 18.003999657976987
Epoch 9418/10000, Prediction Accuracy = 65.53%, Loss = 0.27779528498649597
Epoch: 9418, Batch Gradient Norm: 14.852708271395286
Epoch: 9418, Batch Gradient Norm after: 14.852708271395286
Epoch 9419/10000, Prediction Accuracy = 65.664%, Loss = 0.2738520741462708
Epoch: 9419, Batch Gradient Norm: 12.745330948154198
Epoch: 9419, Batch Gradient Norm after: 12.745330948154198
Epoch 9420/10000, Prediction Accuracy = 65.46599999999998%, Loss = 0.27078187465667725
Epoch: 9420, Batch Gradient Norm: 16.05227737043184
Epoch: 9420, Batch Gradient Norm after: 16.05227737043184
Epoch 9421/10000, Prediction Accuracy = 65.39000000000001%, Loss = 0.27824729681015015
Epoch: 9421, Batch Gradient Norm: 15.173990566234979
Epoch: 9421, Batch Gradient Norm after: 15.173990566234979
Epoch 9422/10000, Prediction Accuracy = 65.486%, Loss = 0.27364346385002136
Epoch: 9422, Batch Gradient Norm: 13.834469634619257
Epoch: 9422, Batch Gradient Norm after: 13.834469634619257
Epoch 9423/10000, Prediction Accuracy = 65.48400000000001%, Loss = 0.2725384533405304
Epoch: 9423, Batch Gradient Norm: 12.95629585488835
Epoch: 9423, Batch Gradient Norm after: 12.95629585488835
Epoch 9424/10000, Prediction Accuracy = 65.628%, Loss = 0.27240356206893923
Epoch: 9424, Batch Gradient Norm: 13.895344376132494
Epoch: 9424, Batch Gradient Norm after: 13.895344376132494
Epoch 9425/10000, Prediction Accuracy = 65.576%, Loss = 0.27100747227668764
Epoch: 9425, Batch Gradient Norm: 16.569563000286013
Epoch: 9425, Batch Gradient Norm after: 16.569563000286013
Epoch 9426/10000, Prediction Accuracy = 65.424%, Loss = 0.2756601214408875
Epoch: 9426, Batch Gradient Norm: 16.210496819779554
Epoch: 9426, Batch Gradient Norm after: 16.210496819779554
Epoch 9427/10000, Prediction Accuracy = 65.542%, Loss = 0.2754898011684418
Epoch: 9427, Batch Gradient Norm: 13.152444269428099
Epoch: 9427, Batch Gradient Norm after: 13.152444269428099
Epoch 9428/10000, Prediction Accuracy = 65.564%, Loss = 0.2728295624256134
Epoch: 9428, Batch Gradient Norm: 13.68174341452248
Epoch: 9428, Batch Gradient Norm after: 13.68174341452248
Epoch 9429/10000, Prediction Accuracy = 65.426%, Loss = 0.27187572717666625
Epoch: 9429, Batch Gradient Norm: 15.309789571816147
Epoch: 9429, Batch Gradient Norm after: 15.309789571816147
Epoch 9430/10000, Prediction Accuracy = 65.498%, Loss = 0.27281156182289124
Epoch: 9430, Batch Gradient Norm: 17.34932071602585
Epoch: 9430, Batch Gradient Norm after: 17.34932071602585
Epoch 9431/10000, Prediction Accuracy = 65.526%, Loss = 0.2752263367176056
Epoch: 9431, Batch Gradient Norm: 15.540892994974577
Epoch: 9431, Batch Gradient Norm after: 15.540892994974577
Epoch 9432/10000, Prediction Accuracy = 65.578%, Loss = 0.27764673829078673
Epoch: 9432, Batch Gradient Norm: 11.886439390347885
Epoch: 9432, Batch Gradient Norm after: 11.886439390347885
Epoch 9433/10000, Prediction Accuracy = 65.658%, Loss = 0.2697525918483734
Epoch: 9433, Batch Gradient Norm: 15.038950519176373
Epoch: 9433, Batch Gradient Norm after: 15.038950519176373
Epoch 9434/10000, Prediction Accuracy = 65.49999999999999%, Loss = 0.2726868987083435
Epoch: 9434, Batch Gradient Norm: 12.852289690882836
Epoch: 9434, Batch Gradient Norm after: 12.852289690882836
Epoch 9435/10000, Prediction Accuracy = 65.54%, Loss = 0.2706051170825958
Epoch: 9435, Batch Gradient Norm: 12.96059571014698
Epoch: 9435, Batch Gradient Norm after: 12.96059571014698
Epoch 9436/10000, Prediction Accuracy = 65.622%, Loss = 0.2742512345314026
Epoch: 9436, Batch Gradient Norm: 12.88792460438881
Epoch: 9436, Batch Gradient Norm after: 12.88792460438881
Epoch 9437/10000, Prediction Accuracy = 65.438%, Loss = 0.2709288716316223
Epoch: 9437, Batch Gradient Norm: 12.240694000645709
Epoch: 9437, Batch Gradient Norm after: 12.240694000645709
Epoch 9438/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.2727700471878052
Epoch: 9438, Batch Gradient Norm: 14.755168419315996
Epoch: 9438, Batch Gradient Norm after: 14.755168419315996
Epoch 9439/10000, Prediction Accuracy = 65.55999999999999%, Loss = 0.27259581685066225
Epoch: 9439, Batch Gradient Norm: 14.41323659912707
Epoch: 9439, Batch Gradient Norm after: 14.41323659912707
Epoch 9440/10000, Prediction Accuracy = 65.72999999999999%, Loss = 0.27110363245010377
Epoch: 9440, Batch Gradient Norm: 13.814710452037852
Epoch: 9440, Batch Gradient Norm after: 13.814710452037852
Epoch 9441/10000, Prediction Accuracy = 65.63400000000001%, Loss = 0.2692251205444336
Epoch: 9441, Batch Gradient Norm: 14.672947085510573
Epoch: 9441, Batch Gradient Norm after: 14.672947085510573
Epoch 9442/10000, Prediction Accuracy = 65.49600000000001%, Loss = 0.2723386883735657
Epoch: 9442, Batch Gradient Norm: 16.743306923662004
Epoch: 9442, Batch Gradient Norm after: 16.743306923662004
Epoch 9443/10000, Prediction Accuracy = 65.47%, Loss = 0.274469131231308
Epoch: 9443, Batch Gradient Norm: 17.451504968287036
Epoch: 9443, Batch Gradient Norm after: 17.451504968287036
Epoch 9444/10000, Prediction Accuracy = 65.488%, Loss = 0.27809709310531616
Epoch: 9444, Batch Gradient Norm: 18.807342257741823
Epoch: 9444, Batch Gradient Norm after: 17.290099892872004
Epoch 9445/10000, Prediction Accuracy = 65.398%, Loss = 0.2777204096317291
Epoch: 9445, Batch Gradient Norm: 20.203324090655578
Epoch: 9445, Batch Gradient Norm after: 19.632393469181107
Epoch 9446/10000, Prediction Accuracy = 65.40599999999999%, Loss = 0.28364922404289244
Epoch: 9446, Batch Gradient Norm: 17.060454903875467
Epoch: 9446, Batch Gradient Norm after: 17.060454903875467
Epoch 9447/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.27318336367607116
Epoch: 9447, Batch Gradient Norm: 16.872409926613056
Epoch: 9447, Batch Gradient Norm after: 16.75841114608813
Epoch 9448/10000, Prediction Accuracy = 65.52799999999999%, Loss = 0.27434147596359254
Epoch: 9448, Batch Gradient Norm: 16.4339978564976
Epoch: 9448, Batch Gradient Norm after: 16.4339978564976
Epoch 9449/10000, Prediction Accuracy = 65.40200000000002%, Loss = 0.2731209218502045
Epoch: 9449, Batch Gradient Norm: 18.41089410856939
Epoch: 9449, Batch Gradient Norm after: 18.01829575718336
Epoch 9450/10000, Prediction Accuracy = 65.402%, Loss = 0.2771491348743439
Epoch: 9450, Batch Gradient Norm: 12.836998564685569
Epoch: 9450, Batch Gradient Norm after: 12.836998564685569
Epoch 9451/10000, Prediction Accuracy = 65.61599999999999%, Loss = 0.2707508683204651
Epoch: 9451, Batch Gradient Norm: 12.888123598382117
Epoch: 9451, Batch Gradient Norm after: 12.888123598382117
Epoch 9452/10000, Prediction Accuracy = 65.502%, Loss = 0.26976712942123415
Epoch: 9452, Batch Gradient Norm: 15.59432522071285
Epoch: 9452, Batch Gradient Norm after: 15.59432522071285
Epoch 9453/10000, Prediction Accuracy = 65.64999999999999%, Loss = 0.274174177646637
Epoch: 9453, Batch Gradient Norm: 14.522358006889739
Epoch: 9453, Batch Gradient Norm after: 14.522358006889739
Epoch 9454/10000, Prediction Accuracy = 65.526%, Loss = 0.275372314453125
Epoch: 9454, Batch Gradient Norm: 15.437210623018375
Epoch: 9454, Batch Gradient Norm after: 15.437210623018375
Epoch 9455/10000, Prediction Accuracy = 65.49600000000001%, Loss = 0.27502075433731077
Epoch: 9455, Batch Gradient Norm: 16.445489580989452
Epoch: 9455, Batch Gradient Norm after: 16.445489580989452
Epoch 9456/10000, Prediction Accuracy = 65.48%, Loss = 0.27785874009132383
Epoch: 9456, Batch Gradient Norm: 16.05587185196384
Epoch: 9456, Batch Gradient Norm after: 16.05587185196384
Epoch 9457/10000, Prediction Accuracy = 65.658%, Loss = 0.27537248134613035
Epoch: 9457, Batch Gradient Norm: 14.556220097253279
Epoch: 9457, Batch Gradient Norm after: 14.556220097253279
Epoch 9458/10000, Prediction Accuracy = 65.588%, Loss = 0.2751434803009033
Epoch: 9458, Batch Gradient Norm: 15.585056820875707
Epoch: 9458, Batch Gradient Norm after: 15.585056820875707
Epoch 9459/10000, Prediction Accuracy = 65.48599999999999%, Loss = 0.27806426882743834
Epoch: 9459, Batch Gradient Norm: 14.930894536627425
Epoch: 9459, Batch Gradient Norm after: 14.930894536627425
Epoch 9460/10000, Prediction Accuracy = 65.63%, Loss = 0.2738016128540039
Epoch: 9460, Batch Gradient Norm: 14.608337996852518
Epoch: 9460, Batch Gradient Norm after: 14.608337996852518
Epoch 9461/10000, Prediction Accuracy = 65.602%, Loss = 0.27270413637161256
Epoch: 9461, Batch Gradient Norm: 13.796095546151172
Epoch: 9461, Batch Gradient Norm after: 13.796095546151172
Epoch 9462/10000, Prediction Accuracy = 65.51599999999999%, Loss = 0.2731185436248779
Epoch: 9462, Batch Gradient Norm: 15.465066261787586
Epoch: 9462, Batch Gradient Norm after: 15.465066261787586
Epoch 9463/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.27244395613670347
Epoch: 9463, Batch Gradient Norm: 18.06160448573992
Epoch: 9463, Batch Gradient Norm after: 18.016122924478786
Epoch 9464/10000, Prediction Accuracy = 65.54999999999998%, Loss = 0.2766298472881317
Epoch: 9464, Batch Gradient Norm: 16.35318604553749
Epoch: 9464, Batch Gradient Norm after: 16.35318604553749
Epoch 9465/10000, Prediction Accuracy = 65.518%, Loss = 0.27450606822967527
Epoch: 9465, Batch Gradient Norm: 15.357871009356815
Epoch: 9465, Batch Gradient Norm after: 15.357871009356815
Epoch 9466/10000, Prediction Accuracy = 65.426%, Loss = 0.2726259589195251
Epoch: 9466, Batch Gradient Norm: 16.67028810470837
Epoch: 9466, Batch Gradient Norm after: 16.67028810470837
Epoch 9467/10000, Prediction Accuracy = 65.41999999999999%, Loss = 0.2748590350151062
Epoch: 9467, Batch Gradient Norm: 13.83550656540225
Epoch: 9467, Batch Gradient Norm after: 13.83550656540225
Epoch 9468/10000, Prediction Accuracy = 65.636%, Loss = 0.2711535215377808
Epoch: 9468, Batch Gradient Norm: 13.162511708491422
Epoch: 9468, Batch Gradient Norm after: 13.162511708491422
Epoch 9469/10000, Prediction Accuracy = 65.48%, Loss = 0.26876558661460875
Epoch: 9469, Batch Gradient Norm: 12.163967179657737
Epoch: 9469, Batch Gradient Norm after: 12.163967179657737
Epoch 9470/10000, Prediction Accuracy = 65.59400000000001%, Loss = 0.2678936064243317
Epoch: 9470, Batch Gradient Norm: 15.587239257079867
Epoch: 9470, Batch Gradient Norm after: 15.587239257079867
Epoch 9471/10000, Prediction Accuracy = 65.59%, Loss = 0.2732446014881134
Epoch: 9471, Batch Gradient Norm: 14.916249285459338
Epoch: 9471, Batch Gradient Norm after: 14.916249285459338
Epoch 9472/10000, Prediction Accuracy = 65.65799999999999%, Loss = 0.27205172181129456
Epoch: 9472, Batch Gradient Norm: 15.314820380178409
Epoch: 9472, Batch Gradient Norm after: 15.314820380178409
Epoch 9473/10000, Prediction Accuracy = 65.618%, Loss = 0.2740380346775055
Epoch: 9473, Batch Gradient Norm: 19.77812104830289
Epoch: 9473, Batch Gradient Norm after: 19.77812104830289
Epoch 9474/10000, Prediction Accuracy = 65.24999999999999%, Loss = 0.2842781364917755
Epoch: 9474, Batch Gradient Norm: 18.72360549666047
Epoch: 9474, Batch Gradient Norm after: 18.72360549666047
Epoch 9475/10000, Prediction Accuracy = 65.526%, Loss = 0.2773561716079712
Epoch: 9475, Batch Gradient Norm: 17.779210271962935
Epoch: 9475, Batch Gradient Norm after: 17.779210271962935
Epoch 9476/10000, Prediction Accuracy = 65.476%, Loss = 0.2754905164241791
Epoch: 9476, Batch Gradient Norm: 20.09189160961001
Epoch: 9476, Batch Gradient Norm after: 18.859693709812365
Epoch 9477/10000, Prediction Accuracy = 65.364%, Loss = 0.2777072310447693
Epoch: 9477, Batch Gradient Norm: 21.14734334964445
Epoch: 9477, Batch Gradient Norm after: 19.82739904524201
Epoch 9478/10000, Prediction Accuracy = 65.38399999999999%, Loss = 0.28270026445388796
Epoch: 9478, Batch Gradient Norm: 22.30371434827479
Epoch: 9478, Batch Gradient Norm after: 21.572392943789183
Epoch 9479/10000, Prediction Accuracy = 65.372%, Loss = 0.28444159030914307
Epoch: 9479, Batch Gradient Norm: 22.860185753979742
Epoch: 9479, Batch Gradient Norm after: 20.898073291906652
Epoch 9480/10000, Prediction Accuracy = 65.45599999999999%, Loss = 0.2867891192436218
Epoch: 9480, Batch Gradient Norm: 22.088127453425905
Epoch: 9480, Batch Gradient Norm after: 20.196400482233084
Epoch 9481/10000, Prediction Accuracy = 65.55799999999999%, Loss = 0.28126928210258484
Epoch: 9481, Batch Gradient Norm: 17.44720756341005
Epoch: 9481, Batch Gradient Norm after: 17.44720756341005
Epoch 9482/10000, Prediction Accuracy = 65.602%, Loss = 0.2764612793922424
Epoch: 9482, Batch Gradient Norm: 16.59756011055403
Epoch: 9482, Batch Gradient Norm after: 16.59756011055403
Epoch 9483/10000, Prediction Accuracy = 65.45%, Loss = 0.2745976150035858
Epoch: 9483, Batch Gradient Norm: 14.536412438317647
Epoch: 9483, Batch Gradient Norm after: 14.536412438317647
Epoch 9484/10000, Prediction Accuracy = 65.482%, Loss = 0.27152477502822875
Epoch: 9484, Batch Gradient Norm: 15.574766895223686
Epoch: 9484, Batch Gradient Norm after: 15.574766895223686
Epoch 9485/10000, Prediction Accuracy = 65.46200000000002%, Loss = 0.2725802183151245
Epoch: 9485, Batch Gradient Norm: 16.869811173961036
Epoch: 9485, Batch Gradient Norm after: 16.869811173961036
Epoch 9486/10000, Prediction Accuracy = 65.434%, Loss = 0.27993648648262026
Epoch: 9486, Batch Gradient Norm: 15.468268698257685
Epoch: 9486, Batch Gradient Norm after: 15.468268698257685
Epoch 9487/10000, Prediction Accuracy = 65.57%, Loss = 0.2729685604572296
Epoch: 9487, Batch Gradient Norm: 17.39143006369123
Epoch: 9487, Batch Gradient Norm after: 17.39143006369123
Epoch 9488/10000, Prediction Accuracy = 65.72999999999999%, Loss = 0.2751276075839996
Epoch: 9488, Batch Gradient Norm: 15.699379419452265
Epoch: 9488, Batch Gradient Norm after: 15.699379419452265
Epoch 9489/10000, Prediction Accuracy = 65.672%, Loss = 0.27240135073661803
Epoch: 9489, Batch Gradient Norm: 16.754703578408144
Epoch: 9489, Batch Gradient Norm after: 16.754703578408144
Epoch 9490/10000, Prediction Accuracy = 65.692%, Loss = 0.2740560114383698
Epoch: 9490, Batch Gradient Norm: 13.313435415887325
Epoch: 9490, Batch Gradient Norm after: 13.313435415887325
Epoch 9491/10000, Prediction Accuracy = 65.524%, Loss = 0.26993508338928224
Epoch: 9491, Batch Gradient Norm: 13.91378160188783
Epoch: 9491, Batch Gradient Norm after: 13.91378160188783
Epoch 9492/10000, Prediction Accuracy = 65.572%, Loss = 0.2712946474552155
Epoch: 9492, Batch Gradient Norm: 15.669632808748942
Epoch: 9492, Batch Gradient Norm after: 15.669632808748942
Epoch 9493/10000, Prediction Accuracy = 65.70000000000002%, Loss = 0.27364463210105894
Epoch: 9493, Batch Gradient Norm: 17.07841329624202
Epoch: 9493, Batch Gradient Norm after: 17.07841329624202
Epoch 9494/10000, Prediction Accuracy = 65.554%, Loss = 0.2741082191467285
Epoch: 9494, Batch Gradient Norm: 13.063728699986468
Epoch: 9494, Batch Gradient Norm after: 13.063728699986468
Epoch 9495/10000, Prediction Accuracy = 65.752%, Loss = 0.26976078748703003
Epoch: 9495, Batch Gradient Norm: 14.059958726419193
Epoch: 9495, Batch Gradient Norm after: 14.059958726419193
Epoch 9496/10000, Prediction Accuracy = 65.426%, Loss = 0.2719484865665436
Epoch: 9496, Batch Gradient Norm: 15.32802385689829
Epoch: 9496, Batch Gradient Norm after: 15.32802385689829
Epoch 9497/10000, Prediction Accuracy = 65.596%, Loss = 0.2730278491973877
Epoch: 9497, Batch Gradient Norm: 15.530888361367804
Epoch: 9497, Batch Gradient Norm after: 15.530888361367804
Epoch 9498/10000, Prediction Accuracy = 65.788%, Loss = 0.27101756930351256
Epoch: 9498, Batch Gradient Norm: 13.591153364135499
Epoch: 9498, Batch Gradient Norm after: 13.591153364135499
Epoch 9499/10000, Prediction Accuracy = 65.606%, Loss = 0.2722954094409943
Epoch: 9499, Batch Gradient Norm: 16.171799591447073
Epoch: 9499, Batch Gradient Norm after: 16.171799591447073
Epoch 9500/10000, Prediction Accuracy = 65.47%, Loss = 0.2778348386287689
Epoch: 9500, Batch Gradient Norm: 17.776209363066062
Epoch: 9500, Batch Gradient Norm after: 17.38591847714374
Epoch 9501/10000, Prediction Accuracy = 65.614%, Loss = 0.27908812165260316
Epoch: 9501, Batch Gradient Norm: 16.421888811759157
Epoch: 9501, Batch Gradient Norm after: 16.421888811759157
Epoch 9502/10000, Prediction Accuracy = 65.562%, Loss = 0.27470843195915223
Epoch: 9502, Batch Gradient Norm: 15.324785049494347
Epoch: 9502, Batch Gradient Norm after: 15.324785049494347
Epoch 9503/10000, Prediction Accuracy = 65.56199999999998%, Loss = 0.27215912342071535
Epoch: 9503, Batch Gradient Norm: 15.63208752759766
Epoch: 9503, Batch Gradient Norm after: 15.63208752759766
Epoch 9504/10000, Prediction Accuracy = 65.412%, Loss = 0.2745330810546875
Epoch: 9504, Batch Gradient Norm: 14.989329855232606
Epoch: 9504, Batch Gradient Norm after: 14.989329855232606
Epoch 9505/10000, Prediction Accuracy = 65.62800000000001%, Loss = 0.27099335193634033
Epoch: 9505, Batch Gradient Norm: 13.357852187383196
Epoch: 9505, Batch Gradient Norm after: 13.357852187383196
Epoch 9506/10000, Prediction Accuracy = 65.542%, Loss = 0.26890186667442323
Epoch: 9506, Batch Gradient Norm: 14.911213121430384
Epoch: 9506, Batch Gradient Norm after: 14.911213121430384
Epoch 9507/10000, Prediction Accuracy = 65.546%, Loss = 0.2743923723697662
Epoch: 9507, Batch Gradient Norm: 17.8258053655146
Epoch: 9507, Batch Gradient Norm after: 17.76313728506971
Epoch 9508/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.2751014053821564
Epoch: 9508, Batch Gradient Norm: 17.038108022652825
Epoch: 9508, Batch Gradient Norm after: 17.038108022652825
Epoch 9509/10000, Prediction Accuracy = 65.458%, Loss = 0.2759407043457031
Epoch: 9509, Batch Gradient Norm: 16.042129818748883
Epoch: 9509, Batch Gradient Norm after: 16.042129818748883
Epoch 9510/10000, Prediction Accuracy = 65.586%, Loss = 0.2730726897716522
Epoch: 9510, Batch Gradient Norm: 15.279278663994777
Epoch: 9510, Batch Gradient Norm after: 15.279278663994777
Epoch 9511/10000, Prediction Accuracy = 65.506%, Loss = 0.273737496137619
Epoch: 9511, Batch Gradient Norm: 13.166133279758586
Epoch: 9511, Batch Gradient Norm after: 13.166133279758586
Epoch 9512/10000, Prediction Accuracy = 65.6%, Loss = 0.2712009012699127
Epoch: 9512, Batch Gradient Norm: 18.067259366514048
Epoch: 9512, Batch Gradient Norm after: 18.067259366514048
Epoch 9513/10000, Prediction Accuracy = 65.368%, Loss = 0.27829758524894715
Epoch: 9513, Batch Gradient Norm: 16.07022550783004
Epoch: 9513, Batch Gradient Norm after: 16.07022550783004
Epoch 9514/10000, Prediction Accuracy = 65.832%, Loss = 0.2739673376083374
Epoch: 9514, Batch Gradient Norm: 17.3933196699065
Epoch: 9514, Batch Gradient Norm after: 17.11790641180673
Epoch 9515/10000, Prediction Accuracy = 65.69800000000001%, Loss = 0.2763711452484131
Epoch: 9515, Batch Gradient Norm: 17.734078977222715
Epoch: 9515, Batch Gradient Norm after: 17.734078977222715
Epoch 9516/10000, Prediction Accuracy = 65.63399999999999%, Loss = 0.2763235688209534
Epoch: 9516, Batch Gradient Norm: 19.260494542408413
Epoch: 9516, Batch Gradient Norm after: 18.977644653099645
Epoch 9517/10000, Prediction Accuracy = 65.578%, Loss = 0.2795547604560852
Epoch: 9517, Batch Gradient Norm: 18.62817448600822
Epoch: 9517, Batch Gradient Norm after: 18.62817448600822
Epoch 9518/10000, Prediction Accuracy = 65.652%, Loss = 0.2774424314498901
Epoch: 9518, Batch Gradient Norm: 16.9043337432044
Epoch: 9518, Batch Gradient Norm after: 16.9043337432044
Epoch 9519/10000, Prediction Accuracy = 65.378%, Loss = 0.279573518037796
Epoch: 9519, Batch Gradient Norm: 16.745317237536646
Epoch: 9519, Batch Gradient Norm after: 16.745317237536646
Epoch 9520/10000, Prediction Accuracy = 65.58%, Loss = 0.2772914290428162
Epoch: 9520, Batch Gradient Norm: 16.63173476367757
Epoch: 9520, Batch Gradient Norm after: 16.63173476367757
Epoch 9521/10000, Prediction Accuracy = 65.372%, Loss = 0.2785393834114075
Epoch: 9521, Batch Gradient Norm: 14.230689584383224
Epoch: 9521, Batch Gradient Norm after: 14.230689584383224
Epoch 9522/10000, Prediction Accuracy = 65.64399999999999%, Loss = 0.27559974789619446
Epoch: 9522, Batch Gradient Norm: 15.168641448945165
Epoch: 9522, Batch Gradient Norm after: 15.168641448945165
Epoch 9523/10000, Prediction Accuracy = 65.684%, Loss = 0.27175875306129454
Epoch: 9523, Batch Gradient Norm: 17.20273906339867
Epoch: 9523, Batch Gradient Norm after: 17.20273906339867
Epoch 9524/10000, Prediction Accuracy = 65.58000000000001%, Loss = 0.2756638526916504
Epoch: 9524, Batch Gradient Norm: 18.972103614544988
Epoch: 9524, Batch Gradient Norm after: 18.972103614544988
Epoch 9525/10000, Prediction Accuracy = 65.60600000000001%, Loss = 0.27728731632232667
Epoch: 9525, Batch Gradient Norm: 19.047245897959407
Epoch: 9525, Batch Gradient Norm after: 17.54502530651316
Epoch 9526/10000, Prediction Accuracy = 65.46799999999999%, Loss = 0.2760610044002533
Epoch: 9526, Batch Gradient Norm: 17.55602245425699
Epoch: 9526, Batch Gradient Norm after: 17.55602245425699
Epoch 9527/10000, Prediction Accuracy = 65.598%, Loss = 0.27728787064552307
Epoch: 9527, Batch Gradient Norm: 20.30694426853133
Epoch: 9527, Batch Gradient Norm after: 20.05898648430369
Epoch 9528/10000, Prediction Accuracy = 65.426%, Loss = 0.2796622276306152
Epoch: 9528, Batch Gradient Norm: 18.843566036315817
Epoch: 9528, Batch Gradient Norm after: 18.19348442176138
Epoch 9529/10000, Prediction Accuracy = 65.506%, Loss = 0.27925959825515745
Epoch: 9529, Batch Gradient Norm: 18.101196866279412
Epoch: 9529, Batch Gradient Norm after: 18.101196866279412
Epoch 9530/10000, Prediction Accuracy = 65.56400000000001%, Loss = 0.2777542233467102
Epoch: 9530, Batch Gradient Norm: 17.406966525292685
Epoch: 9530, Batch Gradient Norm after: 17.406966525292685
Epoch 9531/10000, Prediction Accuracy = 65.54599999999999%, Loss = 0.27343729734420774
Epoch: 9531, Batch Gradient Norm: 14.356576071209172
Epoch: 9531, Batch Gradient Norm after: 14.356576071209172
Epoch 9532/10000, Prediction Accuracy = 65.676%, Loss = 0.2714563965797424
Epoch: 9532, Batch Gradient Norm: 12.701571451585464
Epoch: 9532, Batch Gradient Norm after: 12.701571451585464
Epoch 9533/10000, Prediction Accuracy = 65.47%, Loss = 0.27218097448349
Epoch: 9533, Batch Gradient Norm: 11.41107523657872
Epoch: 9533, Batch Gradient Norm after: 11.41107523657872
Epoch 9534/10000, Prediction Accuracy = 65.816%, Loss = 0.2678448557853699
Epoch: 9534, Batch Gradient Norm: 14.500675003806384
Epoch: 9534, Batch Gradient Norm after: 14.500675003806384
Epoch 9535/10000, Prediction Accuracy = 65.508%, Loss = 0.2730933248996735
Epoch: 9535, Batch Gradient Norm: 15.738072124917094
Epoch: 9535, Batch Gradient Norm after: 15.738072124917094
Epoch 9536/10000, Prediction Accuracy = 65.678%, Loss = 0.271516227722168
Epoch: 9536, Batch Gradient Norm: 14.279935441166236
Epoch: 9536, Batch Gradient Norm after: 14.279935441166236
Epoch 9537/10000, Prediction Accuracy = 65.41400000000002%, Loss = 0.27404074668884276
Epoch: 9537, Batch Gradient Norm: 14.879791150646609
Epoch: 9537, Batch Gradient Norm after: 14.879791150646609
Epoch 9538/10000, Prediction Accuracy = 65.626%, Loss = 0.2713648021221161
Epoch: 9538, Batch Gradient Norm: 17.45639753663454
Epoch: 9538, Batch Gradient Norm after: 17.45639753663454
Epoch 9539/10000, Prediction Accuracy = 65.48800000000001%, Loss = 0.2751430094242096
Epoch: 9539, Batch Gradient Norm: 17.13096607889846
Epoch: 9539, Batch Gradient Norm after: 17.13096607889846
Epoch 9540/10000, Prediction Accuracy = 65.54599999999999%, Loss = 0.2733632504940033
Epoch: 9540, Batch Gradient Norm: 16.522056258369556
Epoch: 9540, Batch Gradient Norm after: 16.415699573539538
Epoch 9541/10000, Prediction Accuracy = 65.626%, Loss = 0.2721737504005432
Epoch: 9541, Batch Gradient Norm: 19.617699634486925
Epoch: 9541, Batch Gradient Norm after: 18.828468063928984
Epoch 9542/10000, Prediction Accuracy = 65.51399999999998%, Loss = 0.2792899191379547
Epoch: 9542, Batch Gradient Norm: 19.13754226199936
Epoch: 9542, Batch Gradient Norm after: 18.674846644397363
Epoch 9543/10000, Prediction Accuracy = 65.564%, Loss = 0.27718201875686643
Epoch: 9543, Batch Gradient Norm: 17.12617204425754
Epoch: 9543, Batch Gradient Norm after: 17.12617204425754
Epoch 9544/10000, Prediction Accuracy = 65.532%, Loss = 0.27384659051895144
Epoch: 9544, Batch Gradient Norm: 15.952473308260638
Epoch: 9544, Batch Gradient Norm after: 15.952473308260638
Epoch 9545/10000, Prediction Accuracy = 65.648%, Loss = 0.2714852809906006
Epoch: 9545, Batch Gradient Norm: 16.555724109302947
Epoch: 9545, Batch Gradient Norm after: 16.555724109302947
Epoch 9546/10000, Prediction Accuracy = 65.534%, Loss = 0.2721816301345825
Epoch: 9546, Batch Gradient Norm: 18.148195145092696
Epoch: 9546, Batch Gradient Norm after: 17.380949667588883
Epoch 9547/10000, Prediction Accuracy = 65.742%, Loss = 0.2756935119628906
Epoch: 9547, Batch Gradient Norm: 15.016418065306754
Epoch: 9547, Batch Gradient Norm after: 15.016418065306754
Epoch 9548/10000, Prediction Accuracy = 65.49%, Loss = 0.2751228094100952
Epoch: 9548, Batch Gradient Norm: 12.587890077168234
Epoch: 9548, Batch Gradient Norm after: 12.587890077168234
Epoch 9549/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.26934518218040465
Epoch: 9549, Batch Gradient Norm: 10.861712724295021
Epoch: 9549, Batch Gradient Norm after: 10.861712724295021
Epoch 9550/10000, Prediction Accuracy = 65.608%, Loss = 0.26647868752479553
Epoch: 9550, Batch Gradient Norm: 11.146420570898183
Epoch: 9550, Batch Gradient Norm after: 11.146420570898183
Epoch 9551/10000, Prediction Accuracy = 65.63399999999999%, Loss = 0.2669395565986633
Epoch: 9551, Batch Gradient Norm: 14.236932417854561
Epoch: 9551, Batch Gradient Norm after: 14.236932417854561
Epoch 9552/10000, Prediction Accuracy = 65.59%, Loss = 0.27368820905685426
Epoch: 9552, Batch Gradient Norm: 13.395342653606043
Epoch: 9552, Batch Gradient Norm after: 13.395342653606043
Epoch 9553/10000, Prediction Accuracy = 65.672%, Loss = 0.2700589418411255
Epoch: 9553, Batch Gradient Norm: 13.38999484829021
Epoch: 9553, Batch Gradient Norm after: 13.38999484829021
Epoch 9554/10000, Prediction Accuracy = 65.666%, Loss = 0.2710295021533966
Epoch: 9554, Batch Gradient Norm: 13.788067670331898
Epoch: 9554, Batch Gradient Norm after: 13.788067670331898
Epoch 9555/10000, Prediction Accuracy = 65.56800000000001%, Loss = 0.27063884735107424
Epoch: 9555, Batch Gradient Norm: 15.243109540165857
Epoch: 9555, Batch Gradient Norm after: 15.243109540165857
Epoch 9556/10000, Prediction Accuracy = 65.51%, Loss = 0.27312257289886477
Epoch: 9556, Batch Gradient Norm: 15.0423298209257
Epoch: 9556, Batch Gradient Norm after: 15.0423298209257
Epoch 9557/10000, Prediction Accuracy = 65.702%, Loss = 0.271193665266037
Epoch: 9557, Batch Gradient Norm: 16.244733797936167
Epoch: 9557, Batch Gradient Norm after: 16.244733797936167
Epoch 9558/10000, Prediction Accuracy = 65.672%, Loss = 0.273525333404541
Epoch: 9558, Batch Gradient Norm: 20.871838261600377
Epoch: 9558, Batch Gradient Norm after: 19.448378156581533
Epoch 9559/10000, Prediction Accuracy = 65.622%, Loss = 0.28163362145423887
Epoch: 9559, Batch Gradient Norm: 21.85686322207542
Epoch: 9559, Batch Gradient Norm after: 21.16063898685756
Epoch 9560/10000, Prediction Accuracy = 65.61800000000001%, Loss = 0.2793064832687378
Epoch: 9560, Batch Gradient Norm: 22.590776541266596
Epoch: 9560, Batch Gradient Norm after: 21.539224309848052
Epoch 9561/10000, Prediction Accuracy = 65.544%, Loss = 0.28268625140190123
Epoch: 9561, Batch Gradient Norm: 21.242862906958866
Epoch: 9561, Batch Gradient Norm after: 19.87550367786014
Epoch 9562/10000, Prediction Accuracy = 65.574%, Loss = 0.2818999409675598
Epoch: 9562, Batch Gradient Norm: 16.0222074987644
Epoch: 9562, Batch Gradient Norm after: 16.0222074987644
Epoch 9563/10000, Prediction Accuracy = 65.47200000000001%, Loss = 0.27302377820014956
Epoch: 9563, Batch Gradient Norm: 15.640962029443685
Epoch: 9563, Batch Gradient Norm after: 15.640962029443685
Epoch 9564/10000, Prediction Accuracy = 65.43%, Loss = 0.27250691652297976
Epoch: 9564, Batch Gradient Norm: 16.685704031786585
Epoch: 9564, Batch Gradient Norm after: 16.685704031786585
Epoch 9565/10000, Prediction Accuracy = 65.366%, Loss = 0.27727305293083193
Epoch: 9565, Batch Gradient Norm: 14.095882777564473
Epoch: 9565, Batch Gradient Norm after: 14.095882777564473
Epoch 9566/10000, Prediction Accuracy = 65.634%, Loss = 0.26914642453193666
Epoch: 9566, Batch Gradient Norm: 14.094670426893394
Epoch: 9566, Batch Gradient Norm after: 14.094670426893394
Epoch 9567/10000, Prediction Accuracy = 65.62199999999999%, Loss = 0.2718964576721191
Epoch: 9567, Batch Gradient Norm: 12.77058669820794
Epoch: 9567, Batch Gradient Norm after: 12.77058669820794
Epoch 9568/10000, Prediction Accuracy = 65.65%, Loss = 0.27161933183670045
Epoch: 9568, Batch Gradient Norm: 16.309392375207814
Epoch: 9568, Batch Gradient Norm after: 16.309392375207814
Epoch 9569/10000, Prediction Accuracy = 65.506%, Loss = 0.2736341178417206
Epoch: 9569, Batch Gradient Norm: 16.967740466102516
Epoch: 9569, Batch Gradient Norm after: 16.967740466102516
Epoch 9570/10000, Prediction Accuracy = 65.46000000000001%, Loss = 0.2741121113300323
Epoch: 9570, Batch Gradient Norm: 15.791173160059524
Epoch: 9570, Batch Gradient Norm after: 15.791173160059524
Epoch 9571/10000, Prediction Accuracy = 65.226%, Loss = 0.2766007661819458
Epoch: 9571, Batch Gradient Norm: 14.430459443660633
Epoch: 9571, Batch Gradient Norm after: 14.430459443660633
Epoch 9572/10000, Prediction Accuracy = 65.55199999999999%, Loss = 0.2716323912143707
Epoch: 9572, Batch Gradient Norm: 15.414644292136929
Epoch: 9572, Batch Gradient Norm after: 15.414644292136929
Epoch 9573/10000, Prediction Accuracy = 65.558%, Loss = 0.27407575845718385
Epoch: 9573, Batch Gradient Norm: 13.197286398667757
Epoch: 9573, Batch Gradient Norm after: 13.197286398667757
Epoch 9574/10000, Prediction Accuracy = 65.7%, Loss = 0.2691402018070221
Epoch: 9574, Batch Gradient Norm: 16.537864831486143
Epoch: 9574, Batch Gradient Norm after: 16.537864831486143
Epoch 9575/10000, Prediction Accuracy = 65.64200000000001%, Loss = 0.27428972721099854
Epoch: 9575, Batch Gradient Norm: 16.919798769598724
Epoch: 9575, Batch Gradient Norm after: 16.676561174714614
Epoch 9576/10000, Prediction Accuracy = 65.56%, Loss = 0.2726567566394806
Epoch: 9576, Batch Gradient Norm: 16.492056310923477
Epoch: 9576, Batch Gradient Norm after: 16.492056310923477
Epoch 9577/10000, Prediction Accuracy = 65.666%, Loss = 0.2724844217300415
Epoch: 9577, Batch Gradient Norm: 17.536781158779927
Epoch: 9577, Batch Gradient Norm after: 17.536781158779927
Epoch 9578/10000, Prediction Accuracy = 65.696%, Loss = 0.2758933424949646
Epoch: 9578, Batch Gradient Norm: 15.441193470654577
Epoch: 9578, Batch Gradient Norm after: 15.441193470654577
Epoch 9579/10000, Prediction Accuracy = 65.63%, Loss = 0.2715330541133881
Epoch: 9579, Batch Gradient Norm: 16.64029802755532
Epoch: 9579, Batch Gradient Norm after: 16.64029802755532
Epoch 9580/10000, Prediction Accuracy = 65.78599999999999%, Loss = 0.2723141074180603
Epoch: 9580, Batch Gradient Norm: 18.336547643239452
Epoch: 9580, Batch Gradient Norm after: 18.336547643239452
Epoch 9581/10000, Prediction Accuracy = 65.652%, Loss = 0.27755202651023864
Epoch: 9581, Batch Gradient Norm: 17.009526541413763
Epoch: 9581, Batch Gradient Norm after: 17.009526541413763
Epoch 9582/10000, Prediction Accuracy = 65.6%, Loss = 0.2748312592506409
Epoch: 9582, Batch Gradient Norm: 15.100053018860555
Epoch: 9582, Batch Gradient Norm after: 15.100053018860555
Epoch 9583/10000, Prediction Accuracy = 65.61399999999999%, Loss = 0.2711796760559082
Epoch: 9583, Batch Gradient Norm: 21.22905653186812
Epoch: 9583, Batch Gradient Norm after: 20.358066169776578
Epoch 9584/10000, Prediction Accuracy = 65.572%, Loss = 0.28211585283279417
Epoch: 9584, Batch Gradient Norm: 21.48342924244911
Epoch: 9584, Batch Gradient Norm after: 20.228989439637353
Epoch 9585/10000, Prediction Accuracy = 65.578%, Loss = 0.2828550398349762
Epoch: 9585, Batch Gradient Norm: 20.072588609316902
Epoch: 9585, Batch Gradient Norm after: 19.686002126207697
Epoch 9586/10000, Prediction Accuracy = 65.63799999999999%, Loss = 0.2773443818092346
Epoch: 9586, Batch Gradient Norm: 18.64312809897695
Epoch: 9586, Batch Gradient Norm after: 18.50400465171509
Epoch 9587/10000, Prediction Accuracy = 65.672%, Loss = 0.2767303168773651
Epoch: 9587, Batch Gradient Norm: 15.70799282444192
Epoch: 9587, Batch Gradient Norm after: 15.70799282444192
Epoch 9588/10000, Prediction Accuracy = 65.78%, Loss = 0.271521782875061
Epoch: 9588, Batch Gradient Norm: 18.112370747695742
Epoch: 9588, Batch Gradient Norm after: 18.112370747695742
Epoch 9589/10000, Prediction Accuracy = 65.574%, Loss = 0.27406933903694153
Epoch: 9589, Batch Gradient Norm: 17.35172476671991
Epoch: 9589, Batch Gradient Norm after: 17.015856883451722
Epoch 9590/10000, Prediction Accuracy = 65.556%, Loss = 0.27769344449043276
Epoch: 9590, Batch Gradient Norm: 14.15000034706269
Epoch: 9590, Batch Gradient Norm after: 14.15000034706269
Epoch 9591/10000, Prediction Accuracy = 65.594%, Loss = 0.2694736301898956
Epoch: 9591, Batch Gradient Norm: 14.729836628839907
Epoch: 9591, Batch Gradient Norm after: 14.729836628839907
Epoch 9592/10000, Prediction Accuracy = 65.588%, Loss = 0.270386278629303
Epoch: 9592, Batch Gradient Norm: 12.387922446319564
Epoch: 9592, Batch Gradient Norm after: 12.387922446319564
Epoch 9593/10000, Prediction Accuracy = 65.53599999999999%, Loss = 0.26800752282142637
Epoch: 9593, Batch Gradient Norm: 13.582918466588634
Epoch: 9593, Batch Gradient Norm after: 13.582918466588634
Epoch 9594/10000, Prediction Accuracy = 65.638%, Loss = 0.2705069363117218
Epoch: 9594, Batch Gradient Norm: 12.884193567386353
Epoch: 9594, Batch Gradient Norm after: 12.884193567386353
Epoch 9595/10000, Prediction Accuracy = 65.52400000000002%, Loss = 0.2707241952419281
Epoch: 9595, Batch Gradient Norm: 16.88482617177733
Epoch: 9595, Batch Gradient Norm after: 16.88482617177733
Epoch 9596/10000, Prediction Accuracy = 65.606%, Loss = 0.27592753171920775
Epoch: 9596, Batch Gradient Norm: 17.68847902757508
Epoch: 9596, Batch Gradient Norm after: 17.68847902757508
Epoch 9597/10000, Prediction Accuracy = 65.75%, Loss = 0.2744328439235687
Epoch: 9597, Batch Gradient Norm: 15.63759178033913
Epoch: 9597, Batch Gradient Norm after: 15.63759178033913
Epoch 9598/10000, Prediction Accuracy = 65.634%, Loss = 0.2699171543121338
Epoch: 9598, Batch Gradient Norm: 16.13545793862415
Epoch: 9598, Batch Gradient Norm after: 16.13545793862415
Epoch 9599/10000, Prediction Accuracy = 65.654%, Loss = 0.2705578148365021
Epoch: 9599, Batch Gradient Norm: 14.145194248412114
Epoch: 9599, Batch Gradient Norm after: 14.145194248412114
Epoch 9600/10000, Prediction Accuracy = 65.586%, Loss = 0.2710178971290588
Epoch: 9600, Batch Gradient Norm: 13.4012413724362
Epoch: 9600, Batch Gradient Norm after: 13.4012413724362
Epoch 9601/10000, Prediction Accuracy = 65.556%, Loss = 0.2693912386894226
Epoch: 9601, Batch Gradient Norm: 10.526916682589826
Epoch: 9601, Batch Gradient Norm after: 10.526916682589826
Epoch 9602/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.2674369513988495
Epoch: 9602, Batch Gradient Norm: 12.128194983257117
Epoch: 9602, Batch Gradient Norm after: 12.128194983257117
Epoch 9603/10000, Prediction Accuracy = 65.796%, Loss = 0.26765158772468567
Epoch: 9603, Batch Gradient Norm: 16.184307939631243
Epoch: 9603, Batch Gradient Norm after: 16.184307939631243
Epoch 9604/10000, Prediction Accuracy = 65.474%, Loss = 0.2746307492256165
Epoch: 9604, Batch Gradient Norm: 17.21060854011691
Epoch: 9604, Batch Gradient Norm after: 17.21060854011691
Epoch 9605/10000, Prediction Accuracy = 65.71%, Loss = 0.2755190312862396
Epoch: 9605, Batch Gradient Norm: 13.30765818974711
Epoch: 9605, Batch Gradient Norm after: 13.30765818974711
Epoch 9606/10000, Prediction Accuracy = 65.72%, Loss = 0.26984286308288574
Epoch: 9606, Batch Gradient Norm: 15.022282972910409
Epoch: 9606, Batch Gradient Norm after: 15.022282972910409
Epoch 9607/10000, Prediction Accuracy = 65.7%, Loss = 0.2722244918346405
Epoch: 9607, Batch Gradient Norm: 15.31681853374516
Epoch: 9607, Batch Gradient Norm after: 15.31681853374516
Epoch 9608/10000, Prediction Accuracy = 65.658%, Loss = 0.2708456635475159
Epoch: 9608, Batch Gradient Norm: 15.516967356136085
Epoch: 9608, Batch Gradient Norm after: 15.516967356136085
Epoch 9609/10000, Prediction Accuracy = 65.542%, Loss = 0.2701669216156006
Epoch: 9609, Batch Gradient Norm: 14.847290965704737
Epoch: 9609, Batch Gradient Norm after: 14.847290965704737
Epoch 9610/10000, Prediction Accuracy = 65.714%, Loss = 0.26919329166412354
Epoch: 9610, Batch Gradient Norm: 14.942851363997873
Epoch: 9610, Batch Gradient Norm after: 14.942851363997873
Epoch 9611/10000, Prediction Accuracy = 65.634%, Loss = 0.27147040963172914
Epoch: 9611, Batch Gradient Norm: 15.642724555341916
Epoch: 9611, Batch Gradient Norm after: 15.642724555341916
Epoch 9612/10000, Prediction Accuracy = 65.606%, Loss = 0.27334744930267335
Epoch: 9612, Batch Gradient Norm: 15.500688528967904
Epoch: 9612, Batch Gradient Norm after: 15.500688528967904
Epoch 9613/10000, Prediction Accuracy = 65.77199999999999%, Loss = 0.27085336446762087
Epoch: 9613, Batch Gradient Norm: 16.47671045444711
Epoch: 9613, Batch Gradient Norm after: 16.47671045444711
Epoch 9614/10000, Prediction Accuracy = 65.74199999999999%, Loss = 0.2705539226531982
Epoch: 9614, Batch Gradient Norm: 15.501538218389467
Epoch: 9614, Batch Gradient Norm after: 15.501538218389467
Epoch 9615/10000, Prediction Accuracy = 65.536%, Loss = 0.27279749512672424
Epoch: 9615, Batch Gradient Norm: 17.149839629082727
Epoch: 9615, Batch Gradient Norm after: 17.149839629082727
Epoch 9616/10000, Prediction Accuracy = 65.452%, Loss = 0.27469362616539
Epoch: 9616, Batch Gradient Norm: 17.285122076921002
Epoch: 9616, Batch Gradient Norm after: 17.285122076921002
Epoch 9617/10000, Prediction Accuracy = 65.496%, Loss = 0.27428954243659975
Epoch: 9617, Batch Gradient Norm: 17.05589403411487
Epoch: 9617, Batch Gradient Norm after: 17.05589403411487
Epoch 9618/10000, Prediction Accuracy = 65.72%, Loss = 0.2741800010204315
Epoch: 9618, Batch Gradient Norm: 17.05735953743305
Epoch: 9618, Batch Gradient Norm after: 17.05735953743305
Epoch 9619/10000, Prediction Accuracy = 65.65%, Loss = 0.2751742362976074
Epoch: 9619, Batch Gradient Norm: 15.926880201324776
Epoch: 9619, Batch Gradient Norm after: 15.926880201324776
Epoch 9620/10000, Prediction Accuracy = 65.708%, Loss = 0.27163257002830504
Epoch: 9620, Batch Gradient Norm: 14.318434382920756
Epoch: 9620, Batch Gradient Norm after: 14.318434382920756
Epoch 9621/10000, Prediction Accuracy = 65.61%, Loss = 0.2685470461845398
Epoch: 9621, Batch Gradient Norm: 13.009873079709608
Epoch: 9621, Batch Gradient Norm after: 13.009873079709608
Epoch 9622/10000, Prediction Accuracy = 65.704%, Loss = 0.2698688328266144
Epoch: 9622, Batch Gradient Norm: 12.862102684392172
Epoch: 9622, Batch Gradient Norm after: 12.862102684392172
Epoch 9623/10000, Prediction Accuracy = 65.55%, Loss = 0.27072558403015134
Epoch: 9623, Batch Gradient Norm: 12.987479010653521
Epoch: 9623, Batch Gradient Norm after: 12.987479010653521
Epoch 9624/10000, Prediction Accuracy = 65.662%, Loss = 0.2662293553352356
Epoch: 9624, Batch Gradient Norm: 12.411472751147175
Epoch: 9624, Batch Gradient Norm after: 12.411472751147175
Epoch 9625/10000, Prediction Accuracy = 65.708%, Loss = 0.26636476516723634
Epoch: 9625, Batch Gradient Norm: 14.427703108847815
Epoch: 9625, Batch Gradient Norm after: 14.427703108847815
Epoch 9626/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.2707560837268829
Epoch: 9626, Batch Gradient Norm: 14.206783282318035
Epoch: 9626, Batch Gradient Norm after: 14.206783282318035
Epoch 9627/10000, Prediction Accuracy = 65.598%, Loss = 0.27003859877586367
Epoch: 9627, Batch Gradient Norm: 14.07787680546557
Epoch: 9627, Batch Gradient Norm after: 14.07787680546557
Epoch 9628/10000, Prediction Accuracy = 65.816%, Loss = 0.26801562309265137
Epoch: 9628, Batch Gradient Norm: 15.665239692163096
Epoch: 9628, Batch Gradient Norm after: 15.665239692163096
Epoch 9629/10000, Prediction Accuracy = 65.674%, Loss = 0.27000118494033815
Epoch: 9629, Batch Gradient Norm: 18.149118429952093
Epoch: 9629, Batch Gradient Norm after: 18.149118429952093
Epoch 9630/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.27486258149147036
Epoch: 9630, Batch Gradient Norm: 16.73168236253822
Epoch: 9630, Batch Gradient Norm after: 16.72033460855398
Epoch 9631/10000, Prediction Accuracy = 65.652%, Loss = 0.27242264747619627
Epoch: 9631, Batch Gradient Norm: 18.792120510606413
Epoch: 9631, Batch Gradient Norm after: 18.20754282358946
Epoch 9632/10000, Prediction Accuracy = 65.61%, Loss = 0.2739648699760437
Epoch: 9632, Batch Gradient Norm: 17.123710790786035
Epoch: 9632, Batch Gradient Norm after: 17.123710790786035
Epoch 9633/10000, Prediction Accuracy = 65.63%, Loss = 0.27354287505149844
Epoch: 9633, Batch Gradient Norm: 15.217747545603283
Epoch: 9633, Batch Gradient Norm after: 15.217747545603283
Epoch 9634/10000, Prediction Accuracy = 65.58000000000001%, Loss = 0.2727180480957031
Epoch: 9634, Batch Gradient Norm: 13.881853211655535
Epoch: 9634, Batch Gradient Norm after: 13.881853211655535
Epoch 9635/10000, Prediction Accuracy = 65.706%, Loss = 0.2677198588848114
Epoch: 9635, Batch Gradient Norm: 14.529960534478708
Epoch: 9635, Batch Gradient Norm after: 14.529960534478708
Epoch 9636/10000, Prediction Accuracy = 65.6%, Loss = 0.27206997871398925
Epoch: 9636, Batch Gradient Norm: 14.650184091499108
Epoch: 9636, Batch Gradient Norm after: 14.650184091499108
Epoch 9637/10000, Prediction Accuracy = 65.758%, Loss = 0.272779643535614
Epoch: 9637, Batch Gradient Norm: 13.173808696735007
Epoch: 9637, Batch Gradient Norm after: 13.173808696735007
Epoch 9638/10000, Prediction Accuracy = 65.64200000000001%, Loss = 0.26874130964279175
Epoch: 9638, Batch Gradient Norm: 13.508305836271179
Epoch: 9638, Batch Gradient Norm after: 13.508305836271179
Epoch 9639/10000, Prediction Accuracy = 65.664%, Loss = 0.2673370599746704
Epoch: 9639, Batch Gradient Norm: 15.583545589363771
Epoch: 9639, Batch Gradient Norm after: 15.583545589363771
Epoch 9640/10000, Prediction Accuracy = 65.666%, Loss = 0.27021077275276184
Epoch: 9640, Batch Gradient Norm: 14.733723085679172
Epoch: 9640, Batch Gradient Norm after: 14.733723085679172
Epoch 9641/10000, Prediction Accuracy = 65.708%, Loss = 0.26984825134277346
Epoch: 9641, Batch Gradient Norm: 15.58554763119583
Epoch: 9641, Batch Gradient Norm after: 15.58554763119583
Epoch 9642/10000, Prediction Accuracy = 65.582%, Loss = 0.2724311888217926
Epoch: 9642, Batch Gradient Norm: 15.859133213459797
Epoch: 9642, Batch Gradient Norm after: 15.859133213459797
Epoch 9643/10000, Prediction Accuracy = 65.562%, Loss = 0.27365683317184447
Epoch: 9643, Batch Gradient Norm: 14.985443894326874
Epoch: 9643, Batch Gradient Norm after: 14.985443894326874
Epoch 9644/10000, Prediction Accuracy = 65.496%, Loss = 0.273029226064682
Epoch: 9644, Batch Gradient Norm: 18.942042892752145
Epoch: 9644, Batch Gradient Norm after: 18.942042892752145
Epoch 9645/10000, Prediction Accuracy = 65.59%, Loss = 0.2759714663028717
Epoch: 9645, Batch Gradient Norm: 18.28833121243409
Epoch: 9645, Batch Gradient Norm after: 18.28833121243409
Epoch 9646/10000, Prediction Accuracy = 65.648%, Loss = 0.2737279534339905
Epoch: 9646, Batch Gradient Norm: 17.81526394790122
Epoch: 9646, Batch Gradient Norm after: 17.81526394790122
Epoch 9647/10000, Prediction Accuracy = 65.536%, Loss = 0.2766638219356537
Epoch: 9647, Batch Gradient Norm: 15.917661617462155
Epoch: 9647, Batch Gradient Norm after: 15.917661617462155
Epoch 9648/10000, Prediction Accuracy = 65.724%, Loss = 0.27319923639297483
Epoch: 9648, Batch Gradient Norm: 15.855923872739055
Epoch: 9648, Batch Gradient Norm after: 15.855923872739055
Epoch 9649/10000, Prediction Accuracy = 65.594%, Loss = 0.2716291546821594
Epoch: 9649, Batch Gradient Norm: 15.731495459588935
Epoch: 9649, Batch Gradient Norm after: 15.731495459588935
Epoch 9650/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.27182610630989074
Epoch: 9650, Batch Gradient Norm: 13.56150877922165
Epoch: 9650, Batch Gradient Norm after: 13.56150877922165
Epoch 9651/10000, Prediction Accuracy = 65.66%, Loss = 0.26764232516288755
Epoch: 9651, Batch Gradient Norm: 17.891658101795667
Epoch: 9651, Batch Gradient Norm after: 17.891658101795667
Epoch 9652/10000, Prediction Accuracy = 65.69200000000001%, Loss = 0.27506091594696047
Epoch: 9652, Batch Gradient Norm: 16.935222774317044
Epoch: 9652, Batch Gradient Norm after: 16.935222774317044
Epoch 9653/10000, Prediction Accuracy = 65.58%, Loss = 0.27089184522628784
Epoch: 9653, Batch Gradient Norm: 16.512825737333806
Epoch: 9653, Batch Gradient Norm after: 16.512825737333806
Epoch 9654/10000, Prediction Accuracy = 65.71400000000001%, Loss = 0.2720225274562836
Epoch: 9654, Batch Gradient Norm: 15.3403192074328
Epoch: 9654, Batch Gradient Norm after: 15.3403192074328
Epoch 9655/10000, Prediction Accuracy = 65.52599999999998%, Loss = 0.27090664505958556
Epoch: 9655, Batch Gradient Norm: 12.001874594526944
Epoch: 9655, Batch Gradient Norm after: 12.001874594526944
Epoch 9656/10000, Prediction Accuracy = 65.752%, Loss = 0.2664350032806396
Epoch: 9656, Batch Gradient Norm: 13.346900746003588
Epoch: 9656, Batch Gradient Norm after: 13.346900746003588
Epoch 9657/10000, Prediction Accuracy = 65.628%, Loss = 0.26908754706382754
Epoch: 9657, Batch Gradient Norm: 14.78372423706994
Epoch: 9657, Batch Gradient Norm after: 14.78372423706994
Epoch 9658/10000, Prediction Accuracy = 65.68800000000002%, Loss = 0.2702758491039276
Epoch: 9658, Batch Gradient Norm: 13.684519537309733
Epoch: 9658, Batch Gradient Norm after: 13.684519537309733
Epoch 9659/10000, Prediction Accuracy = 65.522%, Loss = 0.26889427900314333
Epoch: 9659, Batch Gradient Norm: 13.967498050183812
Epoch: 9659, Batch Gradient Norm after: 13.967498050183812
Epoch 9660/10000, Prediction Accuracy = 65.64999999999999%, Loss = 0.26819294691085815
Epoch: 9660, Batch Gradient Norm: 17.775233595362398
Epoch: 9660, Batch Gradient Norm after: 17.775233595362398
Epoch 9661/10000, Prediction Accuracy = 65.55%, Loss = 0.2738874077796936
Epoch: 9661, Batch Gradient Norm: 16.341401917828172
Epoch: 9661, Batch Gradient Norm after: 16.341401917828172
Epoch 9662/10000, Prediction Accuracy = 65.622%, Loss = 0.27080875635147095
Epoch: 9662, Batch Gradient Norm: 14.501155100837929
Epoch: 9662, Batch Gradient Norm after: 14.501155100837929
Epoch 9663/10000, Prediction Accuracy = 65.69800000000001%, Loss = 0.27018892765045166
Epoch: 9663, Batch Gradient Norm: 13.660731549470807
Epoch: 9663, Batch Gradient Norm after: 13.660731549470807
Epoch 9664/10000, Prediction Accuracy = 65.68800000000002%, Loss = 0.2687608599662781
Epoch: 9664, Batch Gradient Norm: 15.211331986531372
Epoch: 9664, Batch Gradient Norm after: 15.211331986531372
Epoch 9665/10000, Prediction Accuracy = 65.694%, Loss = 0.27043159008026124
Epoch: 9665, Batch Gradient Norm: 13.930651391113182
Epoch: 9665, Batch Gradient Norm after: 13.930651391113182
Epoch 9666/10000, Prediction Accuracy = 65.674%, Loss = 0.2703670680522919
Epoch: 9666, Batch Gradient Norm: 15.639943923857105
Epoch: 9666, Batch Gradient Norm after: 15.639943923857105
Epoch 9667/10000, Prediction Accuracy = 65.744%, Loss = 0.27191661596298217
Epoch: 9667, Batch Gradient Norm: 15.780306267863367
Epoch: 9667, Batch Gradient Norm after: 15.780306267863367
Epoch 9668/10000, Prediction Accuracy = 65.56200000000001%, Loss = 0.27163911461830137
Epoch: 9668, Batch Gradient Norm: 13.817380694409223
Epoch: 9668, Batch Gradient Norm after: 13.817380694409223
Epoch 9669/10000, Prediction Accuracy = 65.74000000000001%, Loss = 0.2702931523323059
Epoch: 9669, Batch Gradient Norm: 11.08395389669679
Epoch: 9669, Batch Gradient Norm after: 11.08395389669679
Epoch 9670/10000, Prediction Accuracy = 65.768%, Loss = 0.2656425595283508
Epoch: 9670, Batch Gradient Norm: 11.98516345851285
Epoch: 9670, Batch Gradient Norm after: 11.98516345851285
Epoch 9671/10000, Prediction Accuracy = 65.714%, Loss = 0.26729326844215395
Epoch: 9671, Batch Gradient Norm: 10.248104702623467
Epoch: 9671, Batch Gradient Norm after: 10.248104702623467
Epoch 9672/10000, Prediction Accuracy = 65.69800000000001%, Loss = 0.26626173257827757
Epoch: 9672, Batch Gradient Norm: 10.210445163161832
Epoch: 9672, Batch Gradient Norm after: 10.210445163161832
Epoch 9673/10000, Prediction Accuracy = 65.83000000000001%, Loss = 0.26593115329742434
Epoch: 9673, Batch Gradient Norm: 15.555478916102373
Epoch: 9673, Batch Gradient Norm after: 15.555478916102373
Epoch 9674/10000, Prediction Accuracy = 65.602%, Loss = 0.2716097295284271
Epoch: 9674, Batch Gradient Norm: 19.942647960171602
Epoch: 9674, Batch Gradient Norm after: 19.56912264692372
Epoch 9675/10000, Prediction Accuracy = 65.57199999999999%, Loss = 0.27785575985908506
Epoch: 9675, Batch Gradient Norm: 15.350862944699601
Epoch: 9675, Batch Gradient Norm after: 15.350862944699601
Epoch 9676/10000, Prediction Accuracy = 65.60999999999999%, Loss = 0.27252050042152404
Epoch: 9676, Batch Gradient Norm: 14.26067974315862
Epoch: 9676, Batch Gradient Norm after: 14.26067974315862
Epoch 9677/10000, Prediction Accuracy = 65.728%, Loss = 0.27009424567222595
Epoch: 9677, Batch Gradient Norm: 16.033118017870226
Epoch: 9677, Batch Gradient Norm after: 16.033118017870226
Epoch 9678/10000, Prediction Accuracy = 65.764%, Loss = 0.2727347433567047
Epoch: 9678, Batch Gradient Norm: 13.60645646355882
Epoch: 9678, Batch Gradient Norm after: 13.60645646355882
Epoch 9679/10000, Prediction Accuracy = 65.602%, Loss = 0.2683801412582397
Epoch: 9679, Batch Gradient Norm: 11.757152210902177
Epoch: 9679, Batch Gradient Norm after: 11.757152210902177
Epoch 9680/10000, Prediction Accuracy = 65.732%, Loss = 0.2669013559818268
Epoch: 9680, Batch Gradient Norm: 14.615498109097333
Epoch: 9680, Batch Gradient Norm after: 14.615498109097333
Epoch 9681/10000, Prediction Accuracy = 65.77799999999999%, Loss = 0.2687291979789734
Epoch: 9681, Batch Gradient Norm: 16.186267137445064
Epoch: 9681, Batch Gradient Norm after: 16.186267137445064
Epoch 9682/10000, Prediction Accuracy = 65.428%, Loss = 0.2778079867362976
Epoch: 9682, Batch Gradient Norm: 16.207295858071145
Epoch: 9682, Batch Gradient Norm after: 16.207295858071145
Epoch 9683/10000, Prediction Accuracy = 65.61600000000001%, Loss = 0.2723798990249634
Epoch: 9683, Batch Gradient Norm: 18.45056834647619
Epoch: 9683, Batch Gradient Norm after: 18.45056834647619
Epoch 9684/10000, Prediction Accuracy = 65.65599999999999%, Loss = 0.2746265769004822
Epoch: 9684, Batch Gradient Norm: 16.402622143647136
Epoch: 9684, Batch Gradient Norm after: 16.402622143647136
Epoch 9685/10000, Prediction Accuracy = 65.48599999999999%, Loss = 0.2725507736206055
Epoch: 9685, Batch Gradient Norm: 16.75979635682105
Epoch: 9685, Batch Gradient Norm after: 16.75979635682105
Epoch 9686/10000, Prediction Accuracy = 65.766%, Loss = 0.2723831295967102
Epoch: 9686, Batch Gradient Norm: 15.579773830322159
Epoch: 9686, Batch Gradient Norm after: 15.579773830322159
Epoch 9687/10000, Prediction Accuracy = 65.696%, Loss = 0.27018054723739626
Epoch: 9687, Batch Gradient Norm: 16.336664806789614
Epoch: 9687, Batch Gradient Norm after: 16.336664806789614
Epoch 9688/10000, Prediction Accuracy = 65.554%, Loss = 0.273011314868927
Epoch: 9688, Batch Gradient Norm: 14.753780596910786
Epoch: 9688, Batch Gradient Norm after: 14.753780596910786
Epoch 9689/10000, Prediction Accuracy = 65.596%, Loss = 0.26938119530677795
Epoch: 9689, Batch Gradient Norm: 15.092475194112076
Epoch: 9689, Batch Gradient Norm after: 15.092475194112076
Epoch 9690/10000, Prediction Accuracy = 65.73599999999999%, Loss = 0.27058298587799073
Epoch: 9690, Batch Gradient Norm: 18.499143055826192
Epoch: 9690, Batch Gradient Norm after: 18.384171922128115
Epoch 9691/10000, Prediction Accuracy = 65.416%, Loss = 0.27878755927085874
Epoch: 9691, Batch Gradient Norm: 21.137646886997103
Epoch: 9691, Batch Gradient Norm after: 20.048343233099615
Epoch 9692/10000, Prediction Accuracy = 65.606%, Loss = 0.28187955021858213
Epoch: 9692, Batch Gradient Norm: 22.930517027449138
Epoch: 9692, Batch Gradient Norm after: 20.6417840675555
Epoch 9693/10000, Prediction Accuracy = 65.45%, Loss = 0.28568143844604493
Epoch: 9693, Batch Gradient Norm: 19.40218171107254
Epoch: 9693, Batch Gradient Norm after: 19.351832463693263
Epoch 9694/10000, Prediction Accuracy = 65.654%, Loss = 0.2812960624694824
Epoch: 9694, Batch Gradient Norm: 14.023693495083903
Epoch: 9694, Batch Gradient Norm after: 14.023693495083903
Epoch 9695/10000, Prediction Accuracy = 65.66000000000001%, Loss = 0.270777827501297
Epoch: 9695, Batch Gradient Norm: 15.807342058215976
Epoch: 9695, Batch Gradient Norm after: 15.807342058215976
Epoch 9696/10000, Prediction Accuracy = 65.64399999999999%, Loss = 0.2713606238365173
Epoch: 9696, Batch Gradient Norm: 16.656349312769613
Epoch: 9696, Batch Gradient Norm after: 16.656349312769613
Epoch 9697/10000, Prediction Accuracy = 65.55799999999999%, Loss = 0.2796507000923157
Epoch: 9697, Batch Gradient Norm: 16.70962305532511
Epoch: 9697, Batch Gradient Norm after: 16.70962305532511
Epoch 9698/10000, Prediction Accuracy = 65.63400000000001%, Loss = 0.2753509819507599
Epoch: 9698, Batch Gradient Norm: 16.43376216424297
Epoch: 9698, Batch Gradient Norm after: 16.43376216424297
Epoch 9699/10000, Prediction Accuracy = 65.684%, Loss = 0.27261889576911924
Epoch: 9699, Batch Gradient Norm: 16.01195636341678
Epoch: 9699, Batch Gradient Norm after: 16.01195636341678
Epoch 9700/10000, Prediction Accuracy = 65.622%, Loss = 0.271674245595932
Epoch: 9700, Batch Gradient Norm: 15.341641910102654
Epoch: 9700, Batch Gradient Norm after: 15.341641910102654
Epoch 9701/10000, Prediction Accuracy = 65.55799999999999%, Loss = 0.27231389880180357
Epoch: 9701, Batch Gradient Norm: 16.45607526702959
Epoch: 9701, Batch Gradient Norm after: 16.45607526702959
Epoch 9702/10000, Prediction Accuracy = 65.58599999999998%, Loss = 0.2715665280818939
Epoch: 9702, Batch Gradient Norm: 16.050384614146825
Epoch: 9702, Batch Gradient Norm after: 16.050384614146825
Epoch 9703/10000, Prediction Accuracy = 65.614%, Loss = 0.27142749428749086
Epoch: 9703, Batch Gradient Norm: 16.335001063126477
Epoch: 9703, Batch Gradient Norm after: 16.335001063126477
Epoch 9704/10000, Prediction Accuracy = 65.69399999999999%, Loss = 0.27526567578315736
Epoch: 9704, Batch Gradient Norm: 17.285178285773977
Epoch: 9704, Batch Gradient Norm after: 16.977035090455605
Epoch 9705/10000, Prediction Accuracy = 65.66799999999999%, Loss = 0.2723942339420319
Epoch: 9705, Batch Gradient Norm: 16.913862193211777
Epoch: 9705, Batch Gradient Norm after: 16.913862193211777
Epoch 9706/10000, Prediction Accuracy = 65.646%, Loss = 0.27420578598976136
Epoch: 9706, Batch Gradient Norm: 13.269959620097964
Epoch: 9706, Batch Gradient Norm after: 13.269959620097964
Epoch 9707/10000, Prediction Accuracy = 65.644%, Loss = 0.2677954018115997
Epoch: 9707, Batch Gradient Norm: 13.750413505823145
Epoch: 9707, Batch Gradient Norm after: 13.750413505823145
Epoch 9708/10000, Prediction Accuracy = 65.706%, Loss = 0.26947562098503114
Epoch: 9708, Batch Gradient Norm: 16.111462512032773
Epoch: 9708, Batch Gradient Norm after: 16.10258506603927
Epoch 9709/10000, Prediction Accuracy = 65.624%, Loss = 0.269927853345871
Epoch: 9709, Batch Gradient Norm: 16.11191077679212
Epoch: 9709, Batch Gradient Norm after: 16.11191077679212
Epoch 9710/10000, Prediction Accuracy = 65.60799999999999%, Loss = 0.2705983817577362
Epoch: 9710, Batch Gradient Norm: 13.15107707499151
Epoch: 9710, Batch Gradient Norm after: 13.15107707499151
Epoch 9711/10000, Prediction Accuracy = 65.588%, Loss = 0.266222220659256
Epoch: 9711, Batch Gradient Norm: 12.491487412225693
Epoch: 9711, Batch Gradient Norm after: 12.491487412225693
Epoch 9712/10000, Prediction Accuracy = 65.79400000000001%, Loss = 0.26734121441841124
Epoch: 9712, Batch Gradient Norm: 13.240533890860783
Epoch: 9712, Batch Gradient Norm after: 13.240533890860783
Epoch 9713/10000, Prediction Accuracy = 65.656%, Loss = 0.2676162123680115
Epoch: 9713, Batch Gradient Norm: 14.194447691411757
Epoch: 9713, Batch Gradient Norm after: 14.194447691411757
Epoch 9714/10000, Prediction Accuracy = 65.696%, Loss = 0.2690952479839325
Epoch: 9714, Batch Gradient Norm: 16.306752794028622
Epoch: 9714, Batch Gradient Norm after: 16.306752794028622
Epoch 9715/10000, Prediction Accuracy = 65.74199999999999%, Loss = 0.26937482357025144
Epoch: 9715, Batch Gradient Norm: 17.11386702392529
Epoch: 9715, Batch Gradient Norm after: 17.11386702392529
Epoch 9716/10000, Prediction Accuracy = 65.86999999999999%, Loss = 0.27244803309440613
Epoch: 9716, Batch Gradient Norm: 18.065466324909323
Epoch: 9716, Batch Gradient Norm after: 17.728244048260603
Epoch 9717/10000, Prediction Accuracy = 65.634%, Loss = 0.27393475770950315
Epoch: 9717, Batch Gradient Norm: 14.929611804089365
Epoch: 9717, Batch Gradient Norm after: 14.929611804089365
Epoch 9718/10000, Prediction Accuracy = 65.762%, Loss = 0.26781007647514343
Epoch: 9718, Batch Gradient Norm: 13.231721983629061
Epoch: 9718, Batch Gradient Norm after: 13.231721983629061
Epoch 9719/10000, Prediction Accuracy = 65.724%, Loss = 0.2672705888748169
Epoch: 9719, Batch Gradient Norm: 13.62407368841323
Epoch: 9719, Batch Gradient Norm after: 13.62407368841323
Epoch 9720/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.2680719971656799
Epoch: 9720, Batch Gradient Norm: 14.615560809647604
Epoch: 9720, Batch Gradient Norm after: 14.615560809647604
Epoch 9721/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.2737449765205383
Epoch: 9721, Batch Gradient Norm: 15.182608869789098
Epoch: 9721, Batch Gradient Norm after: 15.182608869789098
Epoch 9722/10000, Prediction Accuracy = 65.68599999999999%, Loss = 0.27055165767669676
Epoch: 9722, Batch Gradient Norm: 13.718424647058889
Epoch: 9722, Batch Gradient Norm after: 13.718424647058889
Epoch 9723/10000, Prediction Accuracy = 65.61999999999999%, Loss = 0.26957701444625853
Epoch: 9723, Batch Gradient Norm: 12.47627999907048
Epoch: 9723, Batch Gradient Norm after: 12.47627999907048
Epoch 9724/10000, Prediction Accuracy = 65.9%, Loss = 0.2660837709903717
Epoch: 9724, Batch Gradient Norm: 13.221423267392918
Epoch: 9724, Batch Gradient Norm after: 13.221423267392918
Epoch 9725/10000, Prediction Accuracy = 65.752%, Loss = 0.2649019479751587
Epoch: 9725, Batch Gradient Norm: 16.375952191796394
Epoch: 9725, Batch Gradient Norm after: 16.360449998088615
Epoch 9726/10000, Prediction Accuracy = 65.672%, Loss = 0.2711151599884033
Epoch: 9726, Batch Gradient Norm: 16.078772587390638
Epoch: 9726, Batch Gradient Norm after: 16.078772587390638
Epoch 9727/10000, Prediction Accuracy = 65.612%, Loss = 0.27060556411743164
Epoch: 9727, Batch Gradient Norm: 16.796544429229062
Epoch: 9727, Batch Gradient Norm after: 16.796544429229062
Epoch 9728/10000, Prediction Accuracy = 65.64399999999999%, Loss = 0.2730766415596008
Epoch: 9728, Batch Gradient Norm: 13.760786493435027
Epoch: 9728, Batch Gradient Norm after: 13.760786493435027
Epoch 9729/10000, Prediction Accuracy = 65.69%, Loss = 0.27185007333755495
Epoch: 9729, Batch Gradient Norm: 13.973058789500586
Epoch: 9729, Batch Gradient Norm after: 13.973058789500586
Epoch 9730/10000, Prediction Accuracy = 65.626%, Loss = 0.26962417364120483
Epoch: 9730, Batch Gradient Norm: 14.48491624742728
Epoch: 9730, Batch Gradient Norm after: 14.48491624742728
Epoch 9731/10000, Prediction Accuracy = 65.65%, Loss = 0.27025129199028014
Epoch: 9731, Batch Gradient Norm: 14.848246054800166
Epoch: 9731, Batch Gradient Norm after: 14.848246054800166
Epoch 9732/10000, Prediction Accuracy = 65.66600000000001%, Loss = 0.27091463208198546
Epoch: 9732, Batch Gradient Norm: 19.17284453907748
Epoch: 9732, Batch Gradient Norm after: 19.17284453907748
Epoch 9733/10000, Prediction Accuracy = 65.736%, Loss = 0.2747649312019348
Epoch: 9733, Batch Gradient Norm: 15.771933051044563
Epoch: 9733, Batch Gradient Norm after: 15.771933051044563
Epoch 9734/10000, Prediction Accuracy = 65.65%, Loss = 0.2698036849498749
Epoch: 9734, Batch Gradient Norm: 13.977045692220939
Epoch: 9734, Batch Gradient Norm after: 13.977045692220939
Epoch 9735/10000, Prediction Accuracy = 65.82000000000001%, Loss = 0.26779584884643554
Epoch: 9735, Batch Gradient Norm: 13.613687858662233
Epoch: 9735, Batch Gradient Norm after: 13.613687858662233
Epoch 9736/10000, Prediction Accuracy = 65.648%, Loss = 0.2696371853351593
Epoch: 9736, Batch Gradient Norm: 15.938569295313386
Epoch: 9736, Batch Gradient Norm after: 15.938569295313386
Epoch 9737/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.2718237221240997
Epoch: 9737, Batch Gradient Norm: 15.236848579178503
Epoch: 9737, Batch Gradient Norm after: 15.236848579178503
Epoch 9738/10000, Prediction Accuracy = 65.552%, Loss = 0.270168399810791
Epoch: 9738, Batch Gradient Norm: 16.81548700589626
Epoch: 9738, Batch Gradient Norm after: 16.81548700589626
Epoch 9739/10000, Prediction Accuracy = 65.672%, Loss = 0.2709904074668884
Epoch: 9739, Batch Gradient Norm: 14.929951318830469
Epoch: 9739, Batch Gradient Norm after: 14.929951318830469
Epoch 9740/10000, Prediction Accuracy = 65.68800000000002%, Loss = 0.2695214807987213
Epoch: 9740, Batch Gradient Norm: 13.83516304063919
Epoch: 9740, Batch Gradient Norm after: 13.83516304063919
Epoch 9741/10000, Prediction Accuracy = 65.8%, Loss = 0.2673011600971222
Epoch: 9741, Batch Gradient Norm: 13.36805268682736
Epoch: 9741, Batch Gradient Norm after: 13.36805268682736
Epoch 9742/10000, Prediction Accuracy = 65.64%, Loss = 0.2675468623638153
Epoch: 9742, Batch Gradient Norm: 15.0311314443415
Epoch: 9742, Batch Gradient Norm after: 15.0311314443415
Epoch 9743/10000, Prediction Accuracy = 65.714%, Loss = 0.269105464220047
Epoch: 9743, Batch Gradient Norm: 15.073887509025699
Epoch: 9743, Batch Gradient Norm after: 15.073887509025699
Epoch 9744/10000, Prediction Accuracy = 65.60400000000001%, Loss = 0.27042909264564513
Epoch: 9744, Batch Gradient Norm: 15.35103664463598
Epoch: 9744, Batch Gradient Norm after: 15.35103664463598
Epoch 9745/10000, Prediction Accuracy = 65.67399999999999%, Loss = 0.26978365182876585
Epoch: 9745, Batch Gradient Norm: 14.112371512890325
Epoch: 9745, Batch Gradient Norm after: 14.112371512890325
Epoch 9746/10000, Prediction Accuracy = 65.69399999999999%, Loss = 0.2688528001308441
Epoch: 9746, Batch Gradient Norm: 16.51779042787988
Epoch: 9746, Batch Gradient Norm after: 16.51779042787988
Epoch 9747/10000, Prediction Accuracy = 65.614%, Loss = 0.27160604596138
Epoch: 9747, Batch Gradient Norm: 14.983115474363373
Epoch: 9747, Batch Gradient Norm after: 14.983115474363373
Epoch 9748/10000, Prediction Accuracy = 65.638%, Loss = 0.270252126455307
Epoch: 9748, Batch Gradient Norm: 12.206784709362116
Epoch: 9748, Batch Gradient Norm after: 12.206784709362116
Epoch 9749/10000, Prediction Accuracy = 65.79%, Loss = 0.26642017960548403
Epoch: 9749, Batch Gradient Norm: 14.02154692195914
Epoch: 9749, Batch Gradient Norm after: 14.02154692195914
Epoch 9750/10000, Prediction Accuracy = 65.7%, Loss = 0.2694032311439514
Epoch: 9750, Batch Gradient Norm: 17.424684637746356
Epoch: 9750, Batch Gradient Norm after: 17.424684637746356
Epoch 9751/10000, Prediction Accuracy = 65.666%, Loss = 0.2777307450771332
Epoch: 9751, Batch Gradient Norm: 16.000241931854465
Epoch: 9751, Batch Gradient Norm after: 16.000241931854465
Epoch 9752/10000, Prediction Accuracy = 65.574%, Loss = 0.26832192540168764
Epoch: 9752, Batch Gradient Norm: 15.658024537603952
Epoch: 9752, Batch Gradient Norm after: 15.658024537603952
Epoch 9753/10000, Prediction Accuracy = 65.7%, Loss = 0.26994178891181947
Epoch: 9753, Batch Gradient Norm: 16.828879474864134
Epoch: 9753, Batch Gradient Norm after: 16.828879474864134
Epoch 9754/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.27035529017448423
Epoch: 9754, Batch Gradient Norm: 17.176397758230042
Epoch: 9754, Batch Gradient Norm after: 17.176397758230042
Epoch 9755/10000, Prediction Accuracy = 65.598%, Loss = 0.2731692254543304
Epoch: 9755, Batch Gradient Norm: 14.902940815534514
Epoch: 9755, Batch Gradient Norm after: 14.902940815534514
Epoch 9756/10000, Prediction Accuracy = 65.75800000000001%, Loss = 0.26816601753234864
Epoch: 9756, Batch Gradient Norm: 11.131015154209882
Epoch: 9756, Batch Gradient Norm after: 11.131015154209882
Epoch 9757/10000, Prediction Accuracy = 65.918%, Loss = 0.2632256269454956
Epoch: 9757, Batch Gradient Norm: 12.332566127411882
Epoch: 9757, Batch Gradient Norm after: 12.332566127411882
Epoch 9758/10000, Prediction Accuracy = 65.93400000000001%, Loss = 0.2639742612838745
Epoch: 9758, Batch Gradient Norm: 13.995204812064891
Epoch: 9758, Batch Gradient Norm after: 13.995204812064891
Epoch 9759/10000, Prediction Accuracy = 65.632%, Loss = 0.26832489371299745
Epoch: 9759, Batch Gradient Norm: 15.498429945631125
Epoch: 9759, Batch Gradient Norm after: 15.498429945631125
Epoch 9760/10000, Prediction Accuracy = 65.69199999999998%, Loss = 0.26852613091468813
Epoch: 9760, Batch Gradient Norm: 18.006933081542428
Epoch: 9760, Batch Gradient Norm after: 18.006933081542428
Epoch 9761/10000, Prediction Accuracy = 65.53399999999999%, Loss = 0.27377696633338927
Epoch: 9761, Batch Gradient Norm: 18.334426533743784
Epoch: 9761, Batch Gradient Norm after: 18.334426533743784
Epoch 9762/10000, Prediction Accuracy = 65.60400000000001%, Loss = 0.27532315254211426
Epoch: 9762, Batch Gradient Norm: 19.063842993969942
Epoch: 9762, Batch Gradient Norm after: 18.510948370903353
Epoch 9763/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.2753188252449036
Epoch: 9763, Batch Gradient Norm: 17.6809216805656
Epoch: 9763, Batch Gradient Norm after: 17.6809216805656
Epoch 9764/10000, Prediction Accuracy = 65.872%, Loss = 0.2729112863540649
Epoch: 9764, Batch Gradient Norm: 16.499573697640606
Epoch: 9764, Batch Gradient Norm after: 16.499573697640606
Epoch 9765/10000, Prediction Accuracy = 65.58599999999998%, Loss = 0.2745682060718536
Epoch: 9765, Batch Gradient Norm: 16.856875440315722
Epoch: 9765, Batch Gradient Norm after: 16.856875440315722
Epoch 9766/10000, Prediction Accuracy = 65.71199999999999%, Loss = 0.270192414522171
Epoch: 9766, Batch Gradient Norm: 15.344414330608759
Epoch: 9766, Batch Gradient Norm after: 15.344414330608759
Epoch 9767/10000, Prediction Accuracy = 65.75800000000001%, Loss = 0.27038757801055907
Epoch: 9767, Batch Gradient Norm: 16.705196266111752
Epoch: 9767, Batch Gradient Norm after: 16.705196266111752
Epoch 9768/10000, Prediction Accuracy = 65.624%, Loss = 0.2714246153831482
Epoch: 9768, Batch Gradient Norm: 15.552765492312087
Epoch: 9768, Batch Gradient Norm after: 15.552765492312087
Epoch 9769/10000, Prediction Accuracy = 65.74199999999999%, Loss = 0.270284628868103
Epoch: 9769, Batch Gradient Norm: 16.782538906361527
Epoch: 9769, Batch Gradient Norm after: 16.782538906361527
Epoch 9770/10000, Prediction Accuracy = 65.71600000000001%, Loss = 0.2705318868160248
Epoch: 9770, Batch Gradient Norm: 17.788633907848276
Epoch: 9770, Batch Gradient Norm after: 17.788633907848276
Epoch 9771/10000, Prediction Accuracy = 65.656%, Loss = 0.27733269333839417
Epoch: 9771, Batch Gradient Norm: 18.8369651058443
Epoch: 9771, Batch Gradient Norm after: 18.8369651058443
Epoch 9772/10000, Prediction Accuracy = 65.708%, Loss = 0.274896639585495
Epoch: 9772, Batch Gradient Norm: 19.809104648796552
Epoch: 9772, Batch Gradient Norm after: 19.809104648796552
Epoch 9773/10000, Prediction Accuracy = 65.572%, Loss = 0.27623882293701174
Epoch: 9773, Batch Gradient Norm: 16.081899117301422
Epoch: 9773, Batch Gradient Norm after: 16.081899117301422
Epoch 9774/10000, Prediction Accuracy = 65.69000000000001%, Loss = 0.27149924635887146
Epoch: 9774, Batch Gradient Norm: 15.246168608743645
Epoch: 9774, Batch Gradient Norm after: 15.246168608743645
Epoch 9775/10000, Prediction Accuracy = 65.852%, Loss = 0.2683513879776001
Epoch: 9775, Batch Gradient Norm: 12.54388063336725
Epoch: 9775, Batch Gradient Norm after: 12.54388063336725
Epoch 9776/10000, Prediction Accuracy = 65.852%, Loss = 0.26567885279655457
Epoch: 9776, Batch Gradient Norm: 9.755161988969718
Epoch: 9776, Batch Gradient Norm after: 9.755161988969718
Epoch 9777/10000, Prediction Accuracy = 65.938%, Loss = 0.2608762741088867
Epoch: 9777, Batch Gradient Norm: 14.562189139784456
Epoch: 9777, Batch Gradient Norm after: 14.562189139784456
Epoch 9778/10000, Prediction Accuracy = 65.572%, Loss = 0.2708897769451141
Epoch: 9778, Batch Gradient Norm: 15.761032596967908
Epoch: 9778, Batch Gradient Norm after: 15.761032596967908
Epoch 9779/10000, Prediction Accuracy = 65.66399999999999%, Loss = 0.27193593978881836
Epoch: 9779, Batch Gradient Norm: 15.630248669436249
Epoch: 9779, Batch Gradient Norm after: 15.630248669436249
Epoch 9780/10000, Prediction Accuracy = 65.708%, Loss = 0.26889272332191466
Epoch: 9780, Batch Gradient Norm: 15.261624545396119
Epoch: 9780, Batch Gradient Norm after: 15.261624545396119
Epoch 9781/10000, Prediction Accuracy = 65.692%, Loss = 0.26909822821617124
Epoch: 9781, Batch Gradient Norm: 13.65190744908701
Epoch: 9781, Batch Gradient Norm after: 13.65190744908701
Epoch 9782/10000, Prediction Accuracy = 65.75%, Loss = 0.2687914609909058
Epoch: 9782, Batch Gradient Norm: 14.386530230026468
Epoch: 9782, Batch Gradient Norm after: 14.386530230026468
Epoch 9783/10000, Prediction Accuracy = 65.68599999999999%, Loss = 0.2684584319591522
Epoch: 9783, Batch Gradient Norm: 16.962593821299933
Epoch: 9783, Batch Gradient Norm after: 16.962593821299933
Epoch 9784/10000, Prediction Accuracy = 65.918%, Loss = 0.2706143379211426
Epoch: 9784, Batch Gradient Norm: 12.11439015127285
Epoch: 9784, Batch Gradient Norm after: 12.11439015127285
Epoch 9785/10000, Prediction Accuracy = 65.746%, Loss = 0.26546268463134765
Epoch: 9785, Batch Gradient Norm: 16.094208883811206
Epoch: 9785, Batch Gradient Norm after: 16.094208883811206
Epoch 9786/10000, Prediction Accuracy = 65.736%, Loss = 0.2734255850315094
Epoch: 9786, Batch Gradient Norm: 15.945225361089784
Epoch: 9786, Batch Gradient Norm after: 15.945225361089784
Epoch 9787/10000, Prediction Accuracy = 65.77799999999999%, Loss = 0.26982818841934203
Epoch: 9787, Batch Gradient Norm: 17.750543614999504
Epoch: 9787, Batch Gradient Norm after: 17.63634505315889
Epoch 9788/10000, Prediction Accuracy = 65.632%, Loss = 0.2736883580684662
Epoch: 9788, Batch Gradient Norm: 15.426055502984147
Epoch: 9788, Batch Gradient Norm after: 15.426055502984147
Epoch 9789/10000, Prediction Accuracy = 65.696%, Loss = 0.27054409980773925
Epoch: 9789, Batch Gradient Norm: 17.838868270499088
Epoch: 9789, Batch Gradient Norm after: 17.838868270499088
Epoch 9790/10000, Prediction Accuracy = 65.81199999999998%, Loss = 0.27212578654289243
Epoch: 9790, Batch Gradient Norm: 18.675734225961296
Epoch: 9790, Batch Gradient Norm after: 18.558808274860393
Epoch 9791/10000, Prediction Accuracy = 65.66799999999999%, Loss = 0.27420780062675476
Epoch: 9791, Batch Gradient Norm: 15.528700507050823
Epoch: 9791, Batch Gradient Norm after: 15.528700507050823
Epoch 9792/10000, Prediction Accuracy = 65.832%, Loss = 0.2681561648845673
Epoch: 9792, Batch Gradient Norm: 15.818377840734133
Epoch: 9792, Batch Gradient Norm after: 15.818377840734133
Epoch 9793/10000, Prediction Accuracy = 65.66%, Loss = 0.26937581300735475
Epoch: 9793, Batch Gradient Norm: 17.314037382642816
Epoch: 9793, Batch Gradient Norm after: 16.92568457218801
Epoch 9794/10000, Prediction Accuracy = 65.63%, Loss = 0.2714917063713074
Epoch: 9794, Batch Gradient Norm: 17.980183908430636
Epoch: 9794, Batch Gradient Norm after: 17.980183908430636
Epoch 9795/10000, Prediction Accuracy = 65.784%, Loss = 0.2707077205181122
Epoch: 9795, Batch Gradient Norm: 18.680783453664638
Epoch: 9795, Batch Gradient Norm after: 18.680783453664638
Epoch 9796/10000, Prediction Accuracy = 65.864%, Loss = 0.27247229814529417
Epoch: 9796, Batch Gradient Norm: 19.45217864831956
Epoch: 9796, Batch Gradient Norm after: 19.45217864831956
Epoch 9797/10000, Prediction Accuracy = 65.63199999999999%, Loss = 0.27682803869247435
Epoch: 9797, Batch Gradient Norm: 16.48419977521954
Epoch: 9797, Batch Gradient Norm after: 16.48419977521954
Epoch 9798/10000, Prediction Accuracy = 65.84400000000001%, Loss = 0.2711242914199829
Epoch: 9798, Batch Gradient Norm: 13.858418319578872
Epoch: 9798, Batch Gradient Norm after: 13.858418319578872
Epoch 9799/10000, Prediction Accuracy = 65.864%, Loss = 0.2664126455783844
Epoch: 9799, Batch Gradient Norm: 14.256718816402945
Epoch: 9799, Batch Gradient Norm after: 14.256718816402945
Epoch 9800/10000, Prediction Accuracy = 65.728%, Loss = 0.2692219614982605
Epoch: 9800, Batch Gradient Norm: 12.898639169606858
Epoch: 9800, Batch Gradient Norm after: 12.898639169606858
Epoch 9801/10000, Prediction Accuracy = 65.702%, Loss = 0.2673389077186584
Epoch: 9801, Batch Gradient Norm: 15.125625777555136
Epoch: 9801, Batch Gradient Norm after: 15.125625777555136
Epoch 9802/10000, Prediction Accuracy = 65.68199999999999%, Loss = 0.2696575582027435
Epoch: 9802, Batch Gradient Norm: 15.273601219136664
Epoch: 9802, Batch Gradient Norm after: 15.273601219136664
Epoch 9803/10000, Prediction Accuracy = 65.764%, Loss = 0.2725322723388672
Epoch: 9803, Batch Gradient Norm: 15.704815919885553
Epoch: 9803, Batch Gradient Norm after: 15.704815919885553
Epoch 9804/10000, Prediction Accuracy = 65.698%, Loss = 0.2684469699859619
Epoch: 9804, Batch Gradient Norm: 14.750831327126658
Epoch: 9804, Batch Gradient Norm after: 14.750831327126658
Epoch 9805/10000, Prediction Accuracy = 65.79599999999999%, Loss = 0.26961780786514283
Epoch: 9805, Batch Gradient Norm: 14.991167658637512
Epoch: 9805, Batch Gradient Norm after: 14.991167658637512
Epoch 9806/10000, Prediction Accuracy = 65.63%, Loss = 0.2716635465621948
Epoch: 9806, Batch Gradient Norm: 14.927565090129079
Epoch: 9806, Batch Gradient Norm after: 14.927565090129079
Epoch 9807/10000, Prediction Accuracy = 65.834%, Loss = 0.2677869737148285
Epoch: 9807, Batch Gradient Norm: 14.239805740413168
Epoch: 9807, Batch Gradient Norm after: 14.239805740413168
Epoch 9808/10000, Prediction Accuracy = 65.73400000000001%, Loss = 0.27073289155960084
Epoch: 9808, Batch Gradient Norm: 14.801165844603721
Epoch: 9808, Batch Gradient Norm after: 14.801165844603721
Epoch 9809/10000, Prediction Accuracy = 65.76599999999999%, Loss = 0.2716709077358246
Epoch: 9809, Batch Gradient Norm: 14.127219694041417
Epoch: 9809, Batch Gradient Norm after: 14.127219694041417
Epoch 9810/10000, Prediction Accuracy = 65.84400000000001%, Loss = 0.2684690058231354
Epoch: 9810, Batch Gradient Norm: 13.452542208945498
Epoch: 9810, Batch Gradient Norm after: 13.452542208945498
Epoch 9811/10000, Prediction Accuracy = 65.836%, Loss = 0.2653984367847443
Epoch: 9811, Batch Gradient Norm: 12.07505288438749
Epoch: 9811, Batch Gradient Norm after: 12.07505288438749
Epoch 9812/10000, Prediction Accuracy = 65.816%, Loss = 0.2668061971664429
Epoch: 9812, Batch Gradient Norm: 12.79895945982303
Epoch: 9812, Batch Gradient Norm after: 12.79895945982303
Epoch 9813/10000, Prediction Accuracy = 65.798%, Loss = 0.2672262489795685
Epoch: 9813, Batch Gradient Norm: 14.345058866620475
Epoch: 9813, Batch Gradient Norm after: 14.345058866620475
Epoch 9814/10000, Prediction Accuracy = 65.61%, Loss = 0.2694612443447113
Epoch: 9814, Batch Gradient Norm: 15.542985114578103
Epoch: 9814, Batch Gradient Norm after: 15.542985114578103
Epoch 9815/10000, Prediction Accuracy = 65.72999999999999%, Loss = 0.2689432382583618
Epoch: 9815, Batch Gradient Norm: 14.080213023873549
Epoch: 9815, Batch Gradient Norm after: 14.080213023873549
Epoch 9816/10000, Prediction Accuracy = 65.662%, Loss = 0.2681609332561493
Epoch: 9816, Batch Gradient Norm: 16.036203169687926
Epoch: 9816, Batch Gradient Norm after: 16.036203169687926
Epoch 9817/10000, Prediction Accuracy = 65.676%, Loss = 0.26818217635154723
Epoch: 9817, Batch Gradient Norm: 16.465203083124738
Epoch: 9817, Batch Gradient Norm after: 16.465203083124738
Epoch 9818/10000, Prediction Accuracy = 65.58200000000001%, Loss = 0.27064310312271117
Epoch: 9818, Batch Gradient Norm: 16.282836495697417
Epoch: 9818, Batch Gradient Norm after: 16.282836495697417
Epoch 9819/10000, Prediction Accuracy = 65.724%, Loss = 0.26910645365715025
Epoch: 9819, Batch Gradient Norm: 14.953464233004203
Epoch: 9819, Batch Gradient Norm after: 14.953464233004203
Epoch 9820/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.26687241196632383
Epoch: 9820, Batch Gradient Norm: 17.77409205505105
Epoch: 9820, Batch Gradient Norm after: 17.77409205505105
Epoch 9821/10000, Prediction Accuracy = 65.76399999999998%, Loss = 0.2715330719947815
Epoch: 9821, Batch Gradient Norm: 16.623667988603234
Epoch: 9821, Batch Gradient Norm after: 16.623667988603234
Epoch 9822/10000, Prediction Accuracy = 65.752%, Loss = 0.27337883710861205
Epoch: 9822, Batch Gradient Norm: 16.259656346259938
Epoch: 9822, Batch Gradient Norm after: 16.259656346259938
Epoch 9823/10000, Prediction Accuracy = 65.768%, Loss = 0.27008905410766604
Epoch: 9823, Batch Gradient Norm: 13.807635289226255
Epoch: 9823, Batch Gradient Norm after: 13.807635289226255
Epoch 9824/10000, Prediction Accuracy = 65.802%, Loss = 0.2661817491054535
Epoch: 9824, Batch Gradient Norm: 14.025611163710934
Epoch: 9824, Batch Gradient Norm after: 14.025611163710934
Epoch 9825/10000, Prediction Accuracy = 65.862%, Loss = 0.2662739038467407
Epoch: 9825, Batch Gradient Norm: 14.193832866732519
Epoch: 9825, Batch Gradient Norm after: 14.193832866732519
Epoch 9826/10000, Prediction Accuracy = 65.826%, Loss = 0.2673297107219696
Epoch: 9826, Batch Gradient Norm: 15.55332338687226
Epoch: 9826, Batch Gradient Norm after: 15.55332338687226
Epoch 9827/10000, Prediction Accuracy = 65.678%, Loss = 0.2682696282863617
Epoch: 9827, Batch Gradient Norm: 16.86938234056639
Epoch: 9827, Batch Gradient Norm after: 16.86938234056639
Epoch 9828/10000, Prediction Accuracy = 65.83%, Loss = 0.27094247937202454
Epoch: 9828, Batch Gradient Norm: 14.342140170922038
Epoch: 9828, Batch Gradient Norm after: 14.342140170922038
Epoch 9829/10000, Prediction Accuracy = 65.61800000000001%, Loss = 0.2673910915851593
Epoch: 9829, Batch Gradient Norm: 17.472201566018104
Epoch: 9829, Batch Gradient Norm after: 17.472201566018104
Epoch 9830/10000, Prediction Accuracy = 65.734%, Loss = 0.27280545234680176
Epoch: 9830, Batch Gradient Norm: 20.16349551455748
Epoch: 9830, Batch Gradient Norm after: 18.649960230516065
Epoch 9831/10000, Prediction Accuracy = 65.684%, Loss = 0.2747734010219574
Epoch: 9831, Batch Gradient Norm: 14.656405611748522
Epoch: 9831, Batch Gradient Norm after: 14.656405611748522
Epoch 9832/10000, Prediction Accuracy = 65.804%, Loss = 0.2679852247238159
Epoch: 9832, Batch Gradient Norm: 15.170185216939709
Epoch: 9832, Batch Gradient Norm after: 15.170185216939709
Epoch 9833/10000, Prediction Accuracy = 65.648%, Loss = 0.26758516430854795
Epoch: 9833, Batch Gradient Norm: 16.242346684432786
Epoch: 9833, Batch Gradient Norm after: 16.242346684432786
Epoch 9834/10000, Prediction Accuracy = 65.714%, Loss = 0.2706508219242096
Epoch: 9834, Batch Gradient Norm: 14.527489072332578
Epoch: 9834, Batch Gradient Norm after: 14.527489072332578
Epoch 9835/10000, Prediction Accuracy = 65.71000000000001%, Loss = 0.26507660150527956
Epoch: 9835, Batch Gradient Norm: 12.127859252521604
Epoch: 9835, Batch Gradient Norm after: 12.127859252521604
Epoch 9836/10000, Prediction Accuracy = 65.73599999999999%, Loss = 0.26489596366882323
Epoch: 9836, Batch Gradient Norm: 15.062288619862766
Epoch: 9836, Batch Gradient Norm after: 15.062288619862766
Epoch 9837/10000, Prediction Accuracy = 65.522%, Loss = 0.271724534034729
Epoch: 9837, Batch Gradient Norm: 12.567470563399324
Epoch: 9837, Batch Gradient Norm after: 12.567470563399324
Epoch 9838/10000, Prediction Accuracy = 65.816%, Loss = 0.2657212197780609
Epoch: 9838, Batch Gradient Norm: 10.923963853205896
Epoch: 9838, Batch Gradient Norm after: 10.923963853205896
Epoch 9839/10000, Prediction Accuracy = 65.752%, Loss = 0.2631505846977234
Epoch: 9839, Batch Gradient Norm: 11.527650450685112
Epoch: 9839, Batch Gradient Norm after: 11.527650450685112
Epoch 9840/10000, Prediction Accuracy = 65.766%, Loss = 0.26341678500175475
Epoch: 9840, Batch Gradient Norm: 13.869966790141879
Epoch: 9840, Batch Gradient Norm after: 13.869966790141879
Epoch 9841/10000, Prediction Accuracy = 65.84%, Loss = 0.26613983511924744
Epoch: 9841, Batch Gradient Norm: 16.55665507025818
Epoch: 9841, Batch Gradient Norm after: 16.55665507025818
Epoch 9842/10000, Prediction Accuracy = 65.822%, Loss = 0.27106757164001466
Epoch: 9842, Batch Gradient Norm: 15.986115512198843
Epoch: 9842, Batch Gradient Norm after: 15.986115512198843
Epoch 9843/10000, Prediction Accuracy = 65.66999999999999%, Loss = 0.2696871221065521
Epoch: 9843, Batch Gradient Norm: 13.892386229331532
Epoch: 9843, Batch Gradient Norm after: 13.892386229331532
Epoch 9844/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.2670222342014313
Epoch: 9844, Batch Gradient Norm: 13.332957081890767
Epoch: 9844, Batch Gradient Norm after: 13.332957081890767
Epoch 9845/10000, Prediction Accuracy = 65.80199999999999%, Loss = 0.2675962567329407
Epoch: 9845, Batch Gradient Norm: 14.158155525059858
Epoch: 9845, Batch Gradient Norm after: 14.158155525059858
Epoch 9846/10000, Prediction Accuracy = 65.736%, Loss = 0.26723849177360537
Epoch: 9846, Batch Gradient Norm: 15.0059153260847
Epoch: 9846, Batch Gradient Norm after: 15.0059153260847
Epoch 9847/10000, Prediction Accuracy = 65.792%, Loss = 0.26746244430541993
Epoch: 9847, Batch Gradient Norm: 12.057005446603846
Epoch: 9847, Batch Gradient Norm after: 12.057005446603846
Epoch 9848/10000, Prediction Accuracy = 65.702%, Loss = 0.2658612012863159
Epoch: 9848, Batch Gradient Norm: 14.602936504385644
Epoch: 9848, Batch Gradient Norm after: 14.602936504385644
Epoch 9849/10000, Prediction Accuracy = 65.792%, Loss = 0.26508763432502747
Epoch: 9849, Batch Gradient Norm: 14.370148837129893
Epoch: 9849, Batch Gradient Norm after: 14.370148837129893
Epoch 9850/10000, Prediction Accuracy = 65.8%, Loss = 0.2655926048755646
Epoch: 9850, Batch Gradient Norm: 11.898713888310299
Epoch: 9850, Batch Gradient Norm after: 11.898713888310299
Epoch 9851/10000, Prediction Accuracy = 65.90799999999999%, Loss = 0.2628325641155243
Epoch: 9851, Batch Gradient Norm: 10.969877586720026
Epoch: 9851, Batch Gradient Norm after: 10.969877586720026
Epoch 9852/10000, Prediction Accuracy = 65.85000000000001%, Loss = 0.26123151183128357
Epoch: 9852, Batch Gradient Norm: 12.67176196504496
Epoch: 9852, Batch Gradient Norm after: 12.67176196504496
Epoch 9853/10000, Prediction Accuracy = 65.696%, Loss = 0.2656723618507385
Epoch: 9853, Batch Gradient Norm: 15.280064205265505
Epoch: 9853, Batch Gradient Norm after: 15.280064205265505
Epoch 9854/10000, Prediction Accuracy = 65.79400000000001%, Loss = 0.267166668176651
Epoch: 9854, Batch Gradient Norm: 17.025881993010046
Epoch: 9854, Batch Gradient Norm after: 17.025881993010046
Epoch 9855/10000, Prediction Accuracy = 65.822%, Loss = 0.2722873747348785
Epoch: 9855, Batch Gradient Norm: 15.513104849249526
Epoch: 9855, Batch Gradient Norm after: 15.513104849249526
Epoch 9856/10000, Prediction Accuracy = 65.764%, Loss = 0.26699026823043825
Epoch: 9856, Batch Gradient Norm: 16.838902412035818
Epoch: 9856, Batch Gradient Norm after: 16.838902412035818
Epoch 9857/10000, Prediction Accuracy = 65.68799999999999%, Loss = 0.27384171485900877
Epoch: 9857, Batch Gradient Norm: 16.92249294277707
Epoch: 9857, Batch Gradient Norm after: 16.92249294277707
Epoch 9858/10000, Prediction Accuracy = 65.652%, Loss = 0.27345011234283445
Epoch: 9858, Batch Gradient Norm: 17.30328518838425
Epoch: 9858, Batch Gradient Norm after: 17.30328518838425
Epoch 9859/10000, Prediction Accuracy = 65.574%, Loss = 0.2733130693435669
Epoch: 9859, Batch Gradient Norm: 15.84404906799614
Epoch: 9859, Batch Gradient Norm after: 15.84404906799614
Epoch 9860/10000, Prediction Accuracy = 65.696%, Loss = 0.2702346920967102
Epoch: 9860, Batch Gradient Norm: 14.581051093300823
Epoch: 9860, Batch Gradient Norm after: 14.581051093300823
Epoch 9861/10000, Prediction Accuracy = 65.826%, Loss = 0.2678532779216766
Epoch: 9861, Batch Gradient Norm: 18.96767295018838
Epoch: 9861, Batch Gradient Norm after: 18.72954267341287
Epoch 9862/10000, Prediction Accuracy = 65.652%, Loss = 0.2717375934123993
Epoch: 9862, Batch Gradient Norm: 18.808487752811306
Epoch: 9862, Batch Gradient Norm after: 18.241615365011945
Epoch 9863/10000, Prediction Accuracy = 65.848%, Loss = 0.2762103617191315
Epoch: 9863, Batch Gradient Norm: 14.618714197051345
Epoch: 9863, Batch Gradient Norm after: 14.618714197051345
Epoch 9864/10000, Prediction Accuracy = 65.614%, Loss = 0.27054303884506226
Epoch: 9864, Batch Gradient Norm: 12.36325688790758
Epoch: 9864, Batch Gradient Norm after: 12.36325688790758
Epoch 9865/10000, Prediction Accuracy = 65.738%, Loss = 0.2688171207904816
Epoch: 9865, Batch Gradient Norm: 12.752844599854246
Epoch: 9865, Batch Gradient Norm after: 12.752844599854246
Epoch 9866/10000, Prediction Accuracy = 65.77%, Loss = 0.2660682737827301
Epoch: 9866, Batch Gradient Norm: 14.949862609849434
Epoch: 9866, Batch Gradient Norm after: 14.949862609849434
Epoch 9867/10000, Prediction Accuracy = 65.66600000000001%, Loss = 0.2691631019115448
Epoch: 9867, Batch Gradient Norm: 14.407664414437344
Epoch: 9867, Batch Gradient Norm after: 14.407664414437344
Epoch 9868/10000, Prediction Accuracy = 65.862%, Loss = 0.2669402003288269
Epoch: 9868, Batch Gradient Norm: 15.755225035264864
Epoch: 9868, Batch Gradient Norm after: 15.755225035264864
Epoch 9869/10000, Prediction Accuracy = 65.768%, Loss = 0.26912163496017455
Epoch: 9869, Batch Gradient Norm: 16.84441879125698
Epoch: 9869, Batch Gradient Norm after: 16.84441879125698
Epoch 9870/10000, Prediction Accuracy = 65.816%, Loss = 0.26940869688987734
Epoch: 9870, Batch Gradient Norm: 17.963138406049932
Epoch: 9870, Batch Gradient Norm after: 17.68051246965401
Epoch 9871/10000, Prediction Accuracy = 65.64599999999999%, Loss = 0.2728010296821594
Epoch: 9871, Batch Gradient Norm: 18.49822882560538
Epoch: 9871, Batch Gradient Norm after: 18.305166258394287
Epoch 9872/10000, Prediction Accuracy = 65.742%, Loss = 0.27178396582603453
Epoch: 9872, Batch Gradient Norm: 14.718070824483226
Epoch: 9872, Batch Gradient Norm after: 14.718070824483226
Epoch 9873/10000, Prediction Accuracy = 65.92%, Loss = 0.26754510402679443
Epoch: 9873, Batch Gradient Norm: 15.800103987148143
Epoch: 9873, Batch Gradient Norm after: 15.800103987148143
Epoch 9874/10000, Prediction Accuracy = 65.862%, Loss = 0.26680023670196534
Epoch: 9874, Batch Gradient Norm: 18.70479297999082
Epoch: 9874, Batch Gradient Norm after: 18.50383353405507
Epoch 9875/10000, Prediction Accuracy = 65.67%, Loss = 0.27164894342422485
Epoch: 9875, Batch Gradient Norm: 16.03610835094903
Epoch: 9875, Batch Gradient Norm after: 16.03610835094903
Epoch 9876/10000, Prediction Accuracy = 65.854%, Loss = 0.2665054738521576
Epoch: 9876, Batch Gradient Norm: 16.42095414710862
Epoch: 9876, Batch Gradient Norm after: 16.42095414710862
Epoch 9877/10000, Prediction Accuracy = 65.77799999999999%, Loss = 0.26950129866600037
Epoch: 9877, Batch Gradient Norm: 16.375491642139444
Epoch: 9877, Batch Gradient Norm after: 16.375491642139444
Epoch 9878/10000, Prediction Accuracy = 65.796%, Loss = 0.27140634655952456
Epoch: 9878, Batch Gradient Norm: 14.408307854164208
Epoch: 9878, Batch Gradient Norm after: 14.408307854164208
Epoch 9879/10000, Prediction Accuracy = 65.76599999999999%, Loss = 0.26889575123786924
Epoch: 9879, Batch Gradient Norm: 14.32760706262184
Epoch: 9879, Batch Gradient Norm after: 14.32760706262184
Epoch 9880/10000, Prediction Accuracy = 65.58000000000001%, Loss = 0.27034379839897155
Epoch: 9880, Batch Gradient Norm: 16.69681259882296
Epoch: 9880, Batch Gradient Norm after: 16.69681259882296
Epoch 9881/10000, Prediction Accuracy = 65.622%, Loss = 0.2706673264503479
Epoch: 9881, Batch Gradient Norm: 15.121188746565338
Epoch: 9881, Batch Gradient Norm after: 15.121188746565338
Epoch 9882/10000, Prediction Accuracy = 65.69%, Loss = 0.26946520805358887
Epoch: 9882, Batch Gradient Norm: 10.630219155301619
Epoch: 9882, Batch Gradient Norm after: 10.630219155301619
Epoch 9883/10000, Prediction Accuracy = 66.024%, Loss = 0.2615864992141724
Epoch: 9883, Batch Gradient Norm: 12.176400907434898
Epoch: 9883, Batch Gradient Norm after: 12.176400907434898
Epoch 9884/10000, Prediction Accuracy = 65.76599999999999%, Loss = 0.2681383967399597
Epoch: 9884, Batch Gradient Norm: 11.969111170376296
Epoch: 9884, Batch Gradient Norm after: 11.969111170376296
Epoch 9885/10000, Prediction Accuracy = 65.818%, Loss = 0.26307246685028074
Epoch: 9885, Batch Gradient Norm: 14.312783888728655
Epoch: 9885, Batch Gradient Norm after: 14.312783888728655
Epoch 9886/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.2692570090293884
Epoch: 9886, Batch Gradient Norm: 16.173125172937247
Epoch: 9886, Batch Gradient Norm after: 16.173125172937247
Epoch 9887/10000, Prediction Accuracy = 65.77000000000001%, Loss = 0.2705087125301361
Epoch: 9887, Batch Gradient Norm: 14.672462477265665
Epoch: 9887, Batch Gradient Norm after: 14.672462477265665
Epoch 9888/10000, Prediction Accuracy = 65.74%, Loss = 0.2680511176586151
Epoch: 9888, Batch Gradient Norm: 15.666240876828526
Epoch: 9888, Batch Gradient Norm after: 15.666240876828526
Epoch 9889/10000, Prediction Accuracy = 65.65799999999999%, Loss = 0.2715657114982605
Epoch: 9889, Batch Gradient Norm: 14.467168587024878
Epoch: 9889, Batch Gradient Norm after: 14.467168587024878
Epoch 9890/10000, Prediction Accuracy = 65.922%, Loss = 0.26561814546585083
Epoch: 9890, Batch Gradient Norm: 16.53721612286146
Epoch: 9890, Batch Gradient Norm after: 16.53721612286146
Epoch 9891/10000, Prediction Accuracy = 65.788%, Loss = 0.2710117220878601
Epoch: 9891, Batch Gradient Norm: 15.127174169488013
Epoch: 9891, Batch Gradient Norm after: 15.127174169488013
Epoch 9892/10000, Prediction Accuracy = 65.762%, Loss = 0.2688398420810699
Epoch: 9892, Batch Gradient Norm: 18.405362487926496
Epoch: 9892, Batch Gradient Norm after: 18.405362487926496
Epoch 9893/10000, Prediction Accuracy = 65.71600000000001%, Loss = 0.27212460041046144
Epoch: 9893, Batch Gradient Norm: 16.812831403006804
Epoch: 9893, Batch Gradient Norm after: 16.812831403006804
Epoch 9894/10000, Prediction Accuracy = 65.89%, Loss = 0.26908616423606874
Epoch: 9894, Batch Gradient Norm: 14.675582197833863
Epoch: 9894, Batch Gradient Norm after: 14.675582197833863
Epoch 9895/10000, Prediction Accuracy = 65.848%, Loss = 0.26658156514167786
Epoch: 9895, Batch Gradient Norm: 18.331822315461245
Epoch: 9895, Batch Gradient Norm after: 18.331822315461245
Epoch 9896/10000, Prediction Accuracy = 65.78%, Loss = 0.27584110498428344
Epoch: 9896, Batch Gradient Norm: 14.874254397864798
Epoch: 9896, Batch Gradient Norm after: 14.874254397864798
Epoch 9897/10000, Prediction Accuracy = 65.7%, Loss = 0.2679798662662506
Epoch: 9897, Batch Gradient Norm: 16.495552898547253
Epoch: 9897, Batch Gradient Norm after: 16.495552898547253
Epoch 9898/10000, Prediction Accuracy = 65.674%, Loss = 0.2716711461544037
Epoch: 9898, Batch Gradient Norm: 18.25002050704515
Epoch: 9898, Batch Gradient Norm after: 18.169324416795735
Epoch 9899/10000, Prediction Accuracy = 65.63199999999999%, Loss = 0.2712289810180664
Epoch: 9899, Batch Gradient Norm: 16.89551480534056
Epoch: 9899, Batch Gradient Norm after: 16.89551480534056
Epoch 9900/10000, Prediction Accuracy = 65.636%, Loss = 0.2700740575790405
Epoch: 9900, Batch Gradient Norm: 17.381529458564636
Epoch: 9900, Batch Gradient Norm after: 17.381529458564636
Epoch 9901/10000, Prediction Accuracy = 65.798%, Loss = 0.2711122572422028
Epoch: 9901, Batch Gradient Norm: 17.454393508320663
Epoch: 9901, Batch Gradient Norm after: 17.454393508320663
Epoch 9902/10000, Prediction Accuracy = 65.72999999999999%, Loss = 0.2695057809352875
Epoch: 9902, Batch Gradient Norm: 16.29937735715112
Epoch: 9902, Batch Gradient Norm after: 16.29937735715112
Epoch 9903/10000, Prediction Accuracy = 65.66%, Loss = 0.27057037949562074
Epoch: 9903, Batch Gradient Norm: 13.930911521999285
Epoch: 9903, Batch Gradient Norm after: 13.930911521999285
Epoch 9904/10000, Prediction Accuracy = 65.704%, Loss = 0.26604520082473754
Epoch: 9904, Batch Gradient Norm: 15.144236433681176
Epoch: 9904, Batch Gradient Norm after: 15.144236433681176
Epoch 9905/10000, Prediction Accuracy = 65.746%, Loss = 0.2680286169052124
Epoch: 9905, Batch Gradient Norm: 13.799951458763616
Epoch: 9905, Batch Gradient Norm after: 13.799951458763616
Epoch 9906/10000, Prediction Accuracy = 65.73400000000001%, Loss = 0.26750620603561404
Epoch: 9906, Batch Gradient Norm: 15.256304425516912
Epoch: 9906, Batch Gradient Norm after: 15.256304425516912
Epoch 9907/10000, Prediction Accuracy = 65.70599999999999%, Loss = 0.26674200892448424
Epoch: 9907, Batch Gradient Norm: 15.668492126842247
Epoch: 9907, Batch Gradient Norm after: 15.668492126842247
Epoch 9908/10000, Prediction Accuracy = 65.70599999999999%, Loss = 0.2707840859889984
Epoch: 9908, Batch Gradient Norm: 18.7756371364453
Epoch: 9908, Batch Gradient Norm after: 18.737295609499405
Epoch 9909/10000, Prediction Accuracy = 65.902%, Loss = 0.271663635969162
Epoch: 9909, Batch Gradient Norm: 17.078341357461866
Epoch: 9909, Batch Gradient Norm after: 17.078341357461866
Epoch 9910/10000, Prediction Accuracy = 65.83800000000001%, Loss = 0.26879984736442564
Epoch: 9910, Batch Gradient Norm: 22.221575962540516
Epoch: 9910, Batch Gradient Norm after: 20.331174340497636
Epoch 9911/10000, Prediction Accuracy = 65.69200000000001%, Loss = 0.2804545879364014
Epoch: 9911, Batch Gradient Norm: 22.391573602010457
Epoch: 9911, Batch Gradient Norm after: 19.633371122128747
Epoch 9912/10000, Prediction Accuracy = 65.85600000000001%, Loss = 0.2758337378501892
Epoch: 9912, Batch Gradient Norm: 19.037553602294008
Epoch: 9912, Batch Gradient Norm after: 18.725654587827048
Epoch 9913/10000, Prediction Accuracy = 65.72999999999999%, Loss = 0.27383652329444885
Epoch: 9913, Batch Gradient Norm: 21.664493311028778
Epoch: 9913, Batch Gradient Norm after: 19.85575666058353
Epoch 9914/10000, Prediction Accuracy = 65.99799999999999%, Loss = 0.2765803337097168
Epoch: 9914, Batch Gradient Norm: 18.40212791696468
Epoch: 9914, Batch Gradient Norm after: 18.220965738667672
Epoch 9915/10000, Prediction Accuracy = 65.826%, Loss = 0.272745156288147
Epoch: 9915, Batch Gradient Norm: 22.243471031899755
Epoch: 9915, Batch Gradient Norm after: 20.97300053483667
Epoch 9916/10000, Prediction Accuracy = 65.674%, Loss = 0.27948930859565735
Epoch: 9916, Batch Gradient Norm: 20.8650070362877
Epoch: 9916, Batch Gradient Norm after: 20.289368475013045
Epoch 9917/10000, Prediction Accuracy = 65.764%, Loss = 0.2759042978286743
Epoch: 9917, Batch Gradient Norm: 17.92460998305122
Epoch: 9917, Batch Gradient Norm after: 17.92460998305122
Epoch 9918/10000, Prediction Accuracy = 65.866%, Loss = 0.2737758636474609
Epoch: 9918, Batch Gradient Norm: 14.739423161745234
Epoch: 9918, Batch Gradient Norm after: 14.739423161745234
Epoch 9919/10000, Prediction Accuracy = 65.83%, Loss = 0.26699377298355104
Epoch: 9919, Batch Gradient Norm: 14.097886539504213
Epoch: 9919, Batch Gradient Norm after: 14.097886539504213
Epoch 9920/10000, Prediction Accuracy = 65.966%, Loss = 0.26754565834999083
Epoch: 9920, Batch Gradient Norm: 15.749827619390201
Epoch: 9920, Batch Gradient Norm after: 15.749827619390201
Epoch 9921/10000, Prediction Accuracy = 65.538%, Loss = 0.2683205246925354
Epoch: 9921, Batch Gradient Norm: 16.658198982466356
Epoch: 9921, Batch Gradient Norm after: 16.52630041847571
Epoch 9922/10000, Prediction Accuracy = 65.61%, Loss = 0.2702814042568207
Epoch: 9922, Batch Gradient Norm: 17.536417324512637
Epoch: 9922, Batch Gradient Norm after: 17.536417324512637
Epoch 9923/10000, Prediction Accuracy = 65.55999999999999%, Loss = 0.27114891409873965
Epoch: 9923, Batch Gradient Norm: 17.316602253657507
Epoch: 9923, Batch Gradient Norm after: 17.316602253657507
Epoch 9924/10000, Prediction Accuracy = 65.672%, Loss = 0.2697016417980194
Epoch: 9924, Batch Gradient Norm: 21.036308439081793
Epoch: 9924, Batch Gradient Norm after: 20.572751963665358
Epoch 9925/10000, Prediction Accuracy = 65.94000000000001%, Loss = 0.2748462677001953
Epoch: 9925, Batch Gradient Norm: 16.5488625063031
Epoch: 9925, Batch Gradient Norm after: 16.5488625063031
Epoch 9926/10000, Prediction Accuracy = 65.79%, Loss = 0.2710070013999939
Epoch: 9926, Batch Gradient Norm: 13.962225826138281
Epoch: 9926, Batch Gradient Norm after: 13.962225826138281
Epoch 9927/10000, Prediction Accuracy = 65.874%, Loss = 0.2652474582195282
Epoch: 9927, Batch Gradient Norm: 14.20784937300682
Epoch: 9927, Batch Gradient Norm after: 14.20784937300682
Epoch 9928/10000, Prediction Accuracy = 65.776%, Loss = 0.2666573464870453
Epoch: 9928, Batch Gradient Norm: 13.155481088541054
Epoch: 9928, Batch Gradient Norm after: 13.155481088541054
Epoch 9929/10000, Prediction Accuracy = 65.91%, Loss = 0.2655154287815094
Epoch: 9929, Batch Gradient Norm: 11.899362681431219
Epoch: 9929, Batch Gradient Norm after: 11.899362681431219
Epoch 9930/10000, Prediction Accuracy = 65.91%, Loss = 0.2641568064689636
Epoch: 9930, Batch Gradient Norm: 14.825548493174802
Epoch: 9930, Batch Gradient Norm after: 14.825548493174802
Epoch 9931/10000, Prediction Accuracy = 65.80600000000001%, Loss = 0.2709410548210144
Epoch: 9931, Batch Gradient Norm: 17.429145832749214
Epoch: 9931, Batch Gradient Norm after: 17.228132033453893
Epoch 9932/10000, Prediction Accuracy = 65.86%, Loss = 0.2697247087955475
Epoch: 9932, Batch Gradient Norm: 17.041728328656244
Epoch: 9932, Batch Gradient Norm after: 17.041728328656244
Epoch 9933/10000, Prediction Accuracy = 65.682%, Loss = 0.2712464928627014
Epoch: 9933, Batch Gradient Norm: 16.26407819893603
Epoch: 9933, Batch Gradient Norm after: 16.26407819893603
Epoch 9934/10000, Prediction Accuracy = 65.832%, Loss = 0.2681016087532043
Epoch: 9934, Batch Gradient Norm: 18.238605389200792
Epoch: 9934, Batch Gradient Norm after: 18.137421429956337
Epoch 9935/10000, Prediction Accuracy = 65.738%, Loss = 0.2732884526252747
Epoch: 9935, Batch Gradient Norm: 16.809991684405183
Epoch: 9935, Batch Gradient Norm after: 16.809991684405183
Epoch 9936/10000, Prediction Accuracy = 65.744%, Loss = 0.2703582584857941
Epoch: 9936, Batch Gradient Norm: 17.8992213092632
Epoch: 9936, Batch Gradient Norm after: 17.8992213092632
Epoch 9937/10000, Prediction Accuracy = 65.78%, Loss = 0.2734639644622803
Epoch: 9937, Batch Gradient Norm: 18.536384547281035
Epoch: 9937, Batch Gradient Norm after: 18.536384547281035
Epoch 9938/10000, Prediction Accuracy = 65.8%, Loss = 0.27303234934806825
Epoch: 9938, Batch Gradient Norm: 17.21002197499175
Epoch: 9938, Batch Gradient Norm after: 17.21002197499175
Epoch 9939/10000, Prediction Accuracy = 65.77%, Loss = 0.2706234216690063
Epoch: 9939, Batch Gradient Norm: 16.03092498723731
Epoch: 9939, Batch Gradient Norm after: 16.03092498723731
Epoch 9940/10000, Prediction Accuracy = 65.918%, Loss = 0.2684178948402405
Epoch: 9940, Batch Gradient Norm: 13.502668666571308
Epoch: 9940, Batch Gradient Norm after: 13.502668666571308
Epoch 9941/10000, Prediction Accuracy = 65.622%, Loss = 0.26789541244506837
Epoch: 9941, Batch Gradient Norm: 12.04098353358404
Epoch: 9941, Batch Gradient Norm after: 12.04098353358404
Epoch 9942/10000, Prediction Accuracy = 65.992%, Loss = 0.26387197971343995
Epoch: 9942, Batch Gradient Norm: 16.503620638733295
Epoch: 9942, Batch Gradient Norm after: 16.503620638733295
Epoch 9943/10000, Prediction Accuracy = 65.82199999999999%, Loss = 0.26872804164886477
Epoch: 9943, Batch Gradient Norm: 15.162840762475044
Epoch: 9943, Batch Gradient Norm after: 15.162840762475044
Epoch 9944/10000, Prediction Accuracy = 65.762%, Loss = 0.2668576776981354
Epoch: 9944, Batch Gradient Norm: 17.191856876723897
Epoch: 9944, Batch Gradient Norm after: 17.191856876723897
Epoch 9945/10000, Prediction Accuracy = 65.65599999999999%, Loss = 0.2716490089893341
Epoch: 9945, Batch Gradient Norm: 16.975349621946904
Epoch: 9945, Batch Gradient Norm after: 16.975349621946904
Epoch 9946/10000, Prediction Accuracy = 65.966%, Loss = 0.2699618399143219
Epoch: 9946, Batch Gradient Norm: 15.648096302046419
Epoch: 9946, Batch Gradient Norm after: 15.454719650274038
Epoch 9947/10000, Prediction Accuracy = 65.822%, Loss = 0.26807847023010256
Epoch: 9947, Batch Gradient Norm: 12.894515579894303
Epoch: 9947, Batch Gradient Norm after: 12.894515579894303
Epoch 9948/10000, Prediction Accuracy = 65.69800000000001%, Loss = 0.26698614954948424
Epoch: 9948, Batch Gradient Norm: 15.359127045074802
Epoch: 9948, Batch Gradient Norm after: 15.359127045074802
Epoch 9949/10000, Prediction Accuracy = 65.93199999999999%, Loss = 0.26927947998046875
Epoch: 9949, Batch Gradient Norm: 12.395582782187912
Epoch: 9949, Batch Gradient Norm after: 12.395582782187912
Epoch 9950/10000, Prediction Accuracy = 65.756%, Loss = 0.2684148907661438
Epoch: 9950, Batch Gradient Norm: 13.651237555446189
Epoch: 9950, Batch Gradient Norm after: 13.651237555446189
Epoch 9951/10000, Prediction Accuracy = 65.8%, Loss = 0.2654469132423401
Epoch: 9951, Batch Gradient Norm: 14.340715706359095
Epoch: 9951, Batch Gradient Norm after: 14.340715706359095
Epoch 9952/10000, Prediction Accuracy = 65.868%, Loss = 0.2676654517650604
Epoch: 9952, Batch Gradient Norm: 13.583133111296425
Epoch: 9952, Batch Gradient Norm after: 13.583133111296425
Epoch 9953/10000, Prediction Accuracy = 65.85%, Loss = 0.2671934962272644
Epoch: 9953, Batch Gradient Norm: 15.442592041278466
Epoch: 9953, Batch Gradient Norm after: 15.442592041278466
Epoch 9954/10000, Prediction Accuracy = 65.832%, Loss = 0.269016695022583
Epoch: 9954, Batch Gradient Norm: 16.66616908406211
Epoch: 9954, Batch Gradient Norm after: 16.66616908406211
Epoch 9955/10000, Prediction Accuracy = 65.77799999999999%, Loss = 0.2722554624080658
Epoch: 9955, Batch Gradient Norm: 15.038163797739163
Epoch: 9955, Batch Gradient Norm after: 15.038163797739163
Epoch 9956/10000, Prediction Accuracy = 65.926%, Loss = 0.2673841893672943
Epoch: 9956, Batch Gradient Norm: 15.389122059484244
Epoch: 9956, Batch Gradient Norm after: 15.389122059484244
Epoch 9957/10000, Prediction Accuracy = 65.86800000000001%, Loss = 0.2656168580055237
Epoch: 9957, Batch Gradient Norm: 17.55526617676029
Epoch: 9957, Batch Gradient Norm after: 17.55526617676029
Epoch 9958/10000, Prediction Accuracy = 65.848%, Loss = 0.26919260025024416
Epoch: 9958, Batch Gradient Norm: 17.10862556607818
Epoch: 9958, Batch Gradient Norm after: 17.10862556607818
Epoch 9959/10000, Prediction Accuracy = 65.84200000000001%, Loss = 0.2711959183216095
Epoch: 9959, Batch Gradient Norm: 13.718577375507243
Epoch: 9959, Batch Gradient Norm after: 13.718577375507243
Epoch 9960/10000, Prediction Accuracy = 65.852%, Loss = 0.26531335711479187
Epoch: 9960, Batch Gradient Norm: 13.165399710243387
Epoch: 9960, Batch Gradient Norm after: 13.165399710243387
Epoch 9961/10000, Prediction Accuracy = 65.846%, Loss = 0.2655653655529022
Epoch: 9961, Batch Gradient Norm: 12.651258240165026
Epoch: 9961, Batch Gradient Norm after: 12.651258240165026
Epoch 9962/10000, Prediction Accuracy = 65.944%, Loss = 0.2650322914123535
Epoch: 9962, Batch Gradient Norm: 13.516405005375987
Epoch: 9962, Batch Gradient Norm after: 13.516405005375987
Epoch 9963/10000, Prediction Accuracy = 65.806%, Loss = 0.2661495268344879
Epoch: 9963, Batch Gradient Norm: 16.356476546468624
Epoch: 9963, Batch Gradient Norm after: 16.356476546468624
Epoch 9964/10000, Prediction Accuracy = 65.742%, Loss = 0.2681544303894043
Epoch: 9964, Batch Gradient Norm: 14.842189902292173
Epoch: 9964, Batch Gradient Norm after: 14.842189902292173
Epoch 9965/10000, Prediction Accuracy = 65.782%, Loss = 0.26570573449134827
Epoch: 9965, Batch Gradient Norm: 12.930561408775699
Epoch: 9965, Batch Gradient Norm after: 12.930561408775699
Epoch 9966/10000, Prediction Accuracy = 65.962%, Loss = 0.265066397190094
Epoch: 9966, Batch Gradient Norm: 14.176552471429083
Epoch: 9966, Batch Gradient Norm after: 14.176552471429083
Epoch 9967/10000, Prediction Accuracy = 65.86%, Loss = 0.26614081859588623
Epoch: 9967, Batch Gradient Norm: 15.435293632803074
Epoch: 9967, Batch Gradient Norm after: 15.435293632803074
Epoch 9968/10000, Prediction Accuracy = 65.886%, Loss = 0.26935980916023256
Epoch: 9968, Batch Gradient Norm: 16.604029552630777
Epoch: 9968, Batch Gradient Norm after: 16.604029552630777
Epoch 9969/10000, Prediction Accuracy = 65.718%, Loss = 0.269349604845047
Epoch: 9969, Batch Gradient Norm: 13.68552402535909
Epoch: 9969, Batch Gradient Norm after: 13.68552402535909
Epoch 9970/10000, Prediction Accuracy = 65.974%, Loss = 0.2636721730232239
Epoch: 9970, Batch Gradient Norm: 16.40764218509234
Epoch: 9970, Batch Gradient Norm after: 16.40764218509234
Epoch 9971/10000, Prediction Accuracy = 65.858%, Loss = 0.2701224207878113
Epoch: 9971, Batch Gradient Norm: 17.11745678048261
Epoch: 9971, Batch Gradient Norm after: 16.835343099630776
Epoch 9972/10000, Prediction Accuracy = 65.892%, Loss = 0.26940324902534485
Epoch: 9972, Batch Gradient Norm: 14.883213081737388
Epoch: 9972, Batch Gradient Norm after: 14.883213081737388
Epoch 9973/10000, Prediction Accuracy = 65.85799999999999%, Loss = 0.2663008153438568
Epoch: 9973, Batch Gradient Norm: 16.02455397169282
Epoch: 9973, Batch Gradient Norm after: 16.02455397169282
Epoch 9974/10000, Prediction Accuracy = 65.84400000000001%, Loss = 0.2700274348258972
Epoch: 9974, Batch Gradient Norm: 17.99868052291921
Epoch: 9974, Batch Gradient Norm after: 17.633289088825475
Epoch 9975/10000, Prediction Accuracy = 65.602%, Loss = 0.2733703553676605
Epoch: 9975, Batch Gradient Norm: 14.174371507937629
Epoch: 9975, Batch Gradient Norm after: 14.174371507937629
Epoch 9976/10000, Prediction Accuracy = 65.708%, Loss = 0.2662943243980408
Epoch: 9976, Batch Gradient Norm: 15.884755333080946
Epoch: 9976, Batch Gradient Norm after: 15.884755333080946
Epoch 9977/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.2684971272945404
Epoch: 9977, Batch Gradient Norm: 14.325667966419543
Epoch: 9977, Batch Gradient Norm after: 14.325667966419543
Epoch 9978/10000, Prediction Accuracy = 65.78800000000001%, Loss = 0.26566134095191957
Epoch: 9978, Batch Gradient Norm: 14.606611300804259
Epoch: 9978, Batch Gradient Norm after: 14.606611300804259
Epoch 9979/10000, Prediction Accuracy = 65.778%, Loss = 0.2684644103050232
Epoch: 9979, Batch Gradient Norm: 13.740928759828337
Epoch: 9979, Batch Gradient Norm after: 13.740928759828337
Epoch 9980/10000, Prediction Accuracy = 65.81%, Loss = 0.26396368741989135
Epoch: 9980, Batch Gradient Norm: 19.005196464559127
Epoch: 9980, Batch Gradient Norm after: 18.237998032435033
Epoch 9981/10000, Prediction Accuracy = 65.72200000000001%, Loss = 0.27169640064239503
Epoch: 9981, Batch Gradient Norm: 17.83839548431286
Epoch: 9981, Batch Gradient Norm after: 17.811703283480387
Epoch 9982/10000, Prediction Accuracy = 65.99600000000001%, Loss = 0.27050315141677855
Epoch: 9982, Batch Gradient Norm: 19.643510741825597
Epoch: 9982, Batch Gradient Norm after: 19.34584413921357
Epoch 9983/10000, Prediction Accuracy = 65.708%, Loss = 0.27440242767333983
Epoch: 9983, Batch Gradient Norm: 17.733130580271908
Epoch: 9983, Batch Gradient Norm after: 17.733130580271908
Epoch 9984/10000, Prediction Accuracy = 65.832%, Loss = 0.27069218158721925
Epoch: 9984, Batch Gradient Norm: 16.58824718937893
Epoch: 9984, Batch Gradient Norm after: 16.58824718937893
Epoch 9985/10000, Prediction Accuracy = 65.824%, Loss = 0.26786401867866516
Epoch: 9985, Batch Gradient Norm: 15.937138792798514
Epoch: 9985, Batch Gradient Norm after: 15.937138792798514
Epoch 9986/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.2669002771377563
Epoch: 9986, Batch Gradient Norm: 17.609174367062845
Epoch: 9986, Batch Gradient Norm after: 17.609174367062845
Epoch 9987/10000, Prediction Accuracy = 65.69200000000001%, Loss = 0.2742219090461731
Epoch: 9987, Batch Gradient Norm: 15.660156152439214
Epoch: 9987, Batch Gradient Norm after: 15.660156152439214
Epoch 9988/10000, Prediction Accuracy = 65.834%, Loss = 0.2670172691345215
Epoch: 9988, Batch Gradient Norm: 13.490400458595333
Epoch: 9988, Batch Gradient Norm after: 13.490400458595333
Epoch 9989/10000, Prediction Accuracy = 65.974%, Loss = 0.26553580164909363
Epoch: 9989, Batch Gradient Norm: 12.355108903558111
Epoch: 9989, Batch Gradient Norm after: 12.355108903558111
Epoch 9990/10000, Prediction Accuracy = 66.11%, Loss = 0.26252198219299316
Epoch: 9990, Batch Gradient Norm: 12.232186861264417
Epoch: 9990, Batch Gradient Norm after: 12.232186861264417
Epoch 9991/10000, Prediction Accuracy = 65.71%, Loss = 0.2685598850250244
Epoch: 9991, Batch Gradient Norm: 13.509667933146833
Epoch: 9991, Batch Gradient Norm after: 13.509667933146833
Epoch 9992/10000, Prediction Accuracy = 65.74600000000001%, Loss = 0.2690592348575592
Epoch: 9992, Batch Gradient Norm: 14.521977186856914
Epoch: 9992, Batch Gradient Norm after: 14.521977186856914
Epoch 9993/10000, Prediction Accuracy = 65.684%, Loss = 0.2652518093585968
Epoch: 9993, Batch Gradient Norm: 15.088851712660743
Epoch: 9993, Batch Gradient Norm after: 15.088851712660743
Epoch 9994/10000, Prediction Accuracy = 65.886%, Loss = 0.2663298904895782
Epoch: 9994, Batch Gradient Norm: 14.838811026505116
Epoch: 9994, Batch Gradient Norm after: 14.838811026505116
Epoch 9995/10000, Prediction Accuracy = 65.762%, Loss = 0.26889234185218813
Epoch: 9995, Batch Gradient Norm: 15.474723740857504
Epoch: 9995, Batch Gradient Norm after: 15.474723740857504
Epoch 9996/10000, Prediction Accuracy = 65.896%, Loss = 0.26866563558578493
Epoch: 9996, Batch Gradient Norm: 15.003990642695719
Epoch: 9996, Batch Gradient Norm after: 15.003990642695719
Epoch 9997/10000, Prediction Accuracy = 65.69800000000001%, Loss = 0.2665814757347107
Epoch: 9997, Batch Gradient Norm: 14.087279492156382
Epoch: 9997, Batch Gradient Norm after: 14.087279492156382
Epoch 9998/10000, Prediction Accuracy = 65.74199999999999%, Loss = 0.2636440098285675
Epoch: 9998, Batch Gradient Norm: 16.33619237487668
Epoch: 9998, Batch Gradient Norm after: 16.33619237487668
Epoch 9999/10000, Prediction Accuracy = 65.85400000000001%, Loss = 0.2665819823741913
Epoch: 9999, Batch Gradient Norm: 18.42141920103294
Epoch: 9999, Batch Gradient Norm after: 18.42141920103294
Epoch 10000/10000, Prediction Accuracy = 65.95199999999998%, Loss = 0.2697666883468628