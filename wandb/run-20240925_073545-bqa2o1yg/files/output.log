Epoch: 0, Batch Gradient Norm: 29.832463845097674
Epoch: 0, Batch Gradient Norm after: 22.36067571397725
Epoch 1/10000, Prediction Accuracy = 22.326%, Loss = 61.55699691772461
Epoch: 1, Batch Gradient Norm: 56.42507243191495
Epoch: 1, Batch Gradient Norm after: 22.36067676655417
Epoch 2/10000, Prediction Accuracy = 22.326%, Loss = 55.58273468017578
Epoch: 2, Batch Gradient Norm: 85.41771550451035
Epoch: 2, Batch Gradient Norm after: 22.360679313348424
Epoch 3/10000, Prediction Accuracy = 22.326%, Loss = 48.2430061340332
Epoch: 3, Batch Gradient Norm: 108.34596539711019
Epoch: 3, Batch Gradient Norm after: 22.360680413843937
Epoch 4/10000, Prediction Accuracy = 22.326%, Loss = 40.52728500366211
Epoch: 4, Batch Gradient Norm: 122.86437664214557
Epoch: 4, Batch Gradient Norm after: 22.360679914768276
Epoch 5/10000, Prediction Accuracy = 22.326%, Loss = 33.00720291137695
Epoch: 5, Batch Gradient Norm: 127.38130248573805
Epoch: 5, Batch Gradient Norm after: 22.360679892621658
Epoch 6/10000, Prediction Accuracy = 22.326%, Loss = 26.002618026733398
Epoch: 6, Batch Gradient Norm: 120.28035098152633
Epoch: 6, Batch Gradient Norm after: 22.36068005047927
Epoch 7/10000, Prediction Accuracy = 22.326%, Loss = 19.90205612182617
Epoch: 7, Batch Gradient Norm: 100.75162399047332
Epoch: 7, Batch Gradient Norm after: 22.360679251451902
Epoch 8/10000, Prediction Accuracy = 22.326%, Loss = 15.064786529541015
Epoch: 8, Batch Gradient Norm: 70.41655598715585
Epoch: 8, Batch Gradient Norm after: 22.36067883362941
Epoch 9/10000, Prediction Accuracy = 22.326%, Loss = 11.802758598327637
Epoch: 9, Batch Gradient Norm: 37.202632179948104
Epoch: 9, Batch Gradient Norm after: 22.36067761617574
Epoch 10/10000, Prediction Accuracy = 22.326%, Loss = 10.104920959472656
Epoch: 10, Batch Gradient Norm: 15.13312580337526
Epoch: 10, Batch Gradient Norm after: 15.13312580337526
Epoch 11/10000, Prediction Accuracy = 22.326%, Loss = 9.36374111175537
Epoch: 11, Batch Gradient Norm: 8.399719601457042
Epoch: 11, Batch Gradient Norm after: 8.399719601457042
Epoch 12/10000, Prediction Accuracy = 22.391999999999996%, Loss = 8.987593460083009
Epoch: 12, Batch Gradient Norm: 7.089458187027941
Epoch: 12, Batch Gradient Norm after: 7.089458187027941
Epoch 13/10000, Prediction Accuracy = 23.69%, Loss = 8.705070495605469
Epoch: 13, Batch Gradient Norm: 6.761854213184967
Epoch: 13, Batch Gradient Norm after: 6.761854213184967
Epoch 14/10000, Prediction Accuracy = 23.4%, Loss = 8.45399513244629
Epoch: 14, Batch Gradient Norm: 6.605013631159774
Epoch: 14, Batch Gradient Norm after: 6.605013631159774
Epoch 15/10000, Prediction Accuracy = 23.314%, Loss = 8.22327117919922
Epoch: 15, Batch Gradient Norm: 6.504163721719893
Epoch: 15, Batch Gradient Norm after: 6.504163721719893
Epoch 16/10000, Prediction Accuracy = 23.302%, Loss = 8.00721254348755
Epoch: 16, Batch Gradient Norm: 6.420381074801872
Epoch: 16, Batch Gradient Norm after: 6.420381074801872
Epoch 17/10000, Prediction Accuracy = 23.37%, Loss = 7.803244876861572
Epoch: 17, Batch Gradient Norm: 6.339832225868154
Epoch: 17, Batch Gradient Norm after: 6.339832225868154
Epoch 18/10000, Prediction Accuracy = 23.492%, Loss = 7.6086126327514645
Epoch: 18, Batch Gradient Norm: 6.270059309096889
Epoch: 18, Batch Gradient Norm after: 6.270059309096889
Epoch 19/10000, Prediction Accuracy = 23.636%, Loss = 7.422282409667969
Epoch: 19, Batch Gradient Norm: 6.204777152176102
Epoch: 19, Batch Gradient Norm after: 6.204777152176102
Epoch 20/10000, Prediction Accuracy = 23.9%, Loss = 7.244837379455566
Epoch: 20, Batch Gradient Norm: 6.15012539003367
Epoch: 20, Batch Gradient Norm after: 6.15012539003367
Epoch 21/10000, Prediction Accuracy = 24.092000000000002%, Loss = 7.073914527893066
Epoch: 21, Batch Gradient Norm: 6.077636879780526
Epoch: 21, Batch Gradient Norm after: 6.077636879780526
Epoch 22/10000, Prediction Accuracy = 24.322%, Loss = 6.9080709457397464
Epoch: 22, Batch Gradient Norm: 5.988661256980466
Epoch: 22, Batch Gradient Norm after: 5.988661256980466
Epoch 23/10000, Prediction Accuracy = 24.57%, Loss = 6.748618507385254
Epoch: 23, Batch Gradient Norm: 5.881096561724006
Epoch: 23, Batch Gradient Norm after: 5.881096561724006
Epoch 24/10000, Prediction Accuracy = 24.756%, Loss = 6.596685218811035
Epoch: 24, Batch Gradient Norm: 5.681006324698364
Epoch: 24, Batch Gradient Norm after: 5.681006324698364
Epoch 25/10000, Prediction Accuracy = 24.942%, Loss = 6.452398014068604
Epoch: 25, Batch Gradient Norm: 5.537850749979633
Epoch: 25, Batch Gradient Norm after: 5.537850749979633
Epoch 26/10000, Prediction Accuracy = 25.038000000000004%, Loss = 6.3190484046936035
Epoch: 26, Batch Gradient Norm: 5.308465110618937
Epoch: 26, Batch Gradient Norm after: 5.308465110618937
Epoch 27/10000, Prediction Accuracy = 25.208%, Loss = 6.196385383605957
Epoch: 27, Batch Gradient Norm: 5.081896412287756
Epoch: 27, Batch Gradient Norm after: 5.081896412287756
Epoch 28/10000, Prediction Accuracy = 25.304000000000002%, Loss = 6.0848993301391605
Epoch: 28, Batch Gradient Norm: 4.825323761919539
Epoch: 28, Batch Gradient Norm after: 4.825323761919539
Epoch 29/10000, Prediction Accuracy = 25.406%, Loss = 5.9842836380004885
Epoch: 29, Batch Gradient Norm: 4.565479403797129
Epoch: 29, Batch Gradient Norm after: 4.565479403797129
Epoch 30/10000, Prediction Accuracy = 25.4%, Loss = 5.89330415725708
Epoch: 30, Batch Gradient Norm: 4.290333719900642
Epoch: 30, Batch Gradient Norm after: 4.290333719900642
Epoch 31/10000, Prediction Accuracy = 25.576%, Loss = 5.81197566986084
Epoch: 31, Batch Gradient Norm: 4.051606108221341
Epoch: 31, Batch Gradient Norm after: 4.051606108221341
Epoch 32/10000, Prediction Accuracy = 26.115999999999996%, Loss = 5.740273094177246
Epoch: 32, Batch Gradient Norm: 3.8037070546306015
Epoch: 32, Batch Gradient Norm after: 3.8037070546306015
Epoch 33/10000, Prediction Accuracy = 26.354000000000003%, Loss = 5.677310276031494
Epoch: 33, Batch Gradient Norm: 3.5992642712167315
Epoch: 33, Batch Gradient Norm after: 3.5992642712167315
Epoch 34/10000, Prediction Accuracy = 26.786%, Loss = 5.622169876098633
Epoch: 34, Batch Gradient Norm: 3.443646633496879
Epoch: 34, Batch Gradient Norm after: 3.443646633496879
Epoch 35/10000, Prediction Accuracy = 27.166000000000004%, Loss = 5.572673606872558
Epoch: 35, Batch Gradient Norm: 3.394227944139519
Epoch: 35, Batch Gradient Norm after: 3.394227944139519
Epoch 36/10000, Prediction Accuracy = 27.669999999999998%, Loss = 5.529016780853271
Epoch: 36, Batch Gradient Norm: 3.0749738868814633
Epoch: 36, Batch Gradient Norm after: 3.0749738868814633
Epoch 37/10000, Prediction Accuracy = 28.148000000000003%, Loss = 5.489202785491943
Epoch: 37, Batch Gradient Norm: 3.269962633272549
Epoch: 37, Batch Gradient Norm after: 3.269962633272549
Epoch 38/10000, Prediction Accuracy = 28.386000000000003%, Loss = 5.453758239746094
Epoch: 38, Batch Gradient Norm: 2.961014921910225
Epoch: 38, Batch Gradient Norm after: 2.961014921910225
Epoch 39/10000, Prediction Accuracy = 28.977999999999998%, Loss = 5.420077896118164
Epoch: 39, Batch Gradient Norm: 3.767989839225929
Epoch: 39, Batch Gradient Norm after: 3.767989839225929
Epoch 40/10000, Prediction Accuracy = 29.038%, Loss = 5.390257835388184
Epoch: 40, Batch Gradient Norm: 4.217918998278708
Epoch: 40, Batch Gradient Norm after: 4.217918998278708
Epoch 41/10000, Prediction Accuracy = 29.35%, Loss = 5.361827945709228
Epoch: 41, Batch Gradient Norm: 6.567761354310972
Epoch: 41, Batch Gradient Norm after: 6.567761354310972
Epoch 42/10000, Prediction Accuracy = 29.635999999999996%, Loss = 5.340003395080567
Epoch: 42, Batch Gradient Norm: 5.757431527922058
Epoch: 42, Batch Gradient Norm after: 5.757431527922058
Epoch 43/10000, Prediction Accuracy = 29.915999999999997%, Loss = 5.310784530639649
Epoch: 43, Batch Gradient Norm: 6.420755691276439
Epoch: 43, Batch Gradient Norm after: 6.420755691276439
Epoch 44/10000, Prediction Accuracy = 30.153999999999996%, Loss = 5.286092376708984
Epoch: 44, Batch Gradient Norm: 4.479034281065401
Epoch: 44, Batch Gradient Norm after: 4.479034281065401
Epoch 45/10000, Prediction Accuracy = 30.474%, Loss = 5.255158424377441
Epoch: 45, Batch Gradient Norm: 5.465080262362118
Epoch: 45, Batch Gradient Norm after: 5.465080262362118
Epoch 46/10000, Prediction Accuracy = 30.742%, Loss = 5.231621551513672
Epoch: 46, Batch Gradient Norm: 4.515546358892743
Epoch: 46, Batch Gradient Norm after: 4.515546358892743
Epoch 47/10000, Prediction Accuracy = 31.165999999999997%, Loss = 5.203914642333984
Epoch: 47, Batch Gradient Norm: 6.33710938687855
Epoch: 47, Batch Gradient Norm after: 6.33710938687855
Epoch 48/10000, Prediction Accuracy = 31.302%, Loss = 5.183055210113525
Epoch: 48, Batch Gradient Norm: 6.047006742151023
Epoch: 48, Batch Gradient Norm after: 6.047006742151023
Epoch 49/10000, Prediction Accuracy = 31.708%, Loss = 5.157007789611816
Epoch: 49, Batch Gradient Norm: 8.062821932629392
Epoch: 49, Batch Gradient Norm after: 8.062821932629392
Epoch 50/10000, Prediction Accuracy = 31.856%, Loss = 5.13820743560791
Epoch: 50, Batch Gradient Norm: 6.725442513055287
Epoch: 50, Batch Gradient Norm after: 6.725442513055287
Epoch 51/10000, Prediction Accuracy = 32.214%, Loss = 5.108323860168457
Epoch: 51, Batch Gradient Norm: 7.692392486067979
Epoch: 51, Batch Gradient Norm after: 7.692392486067979
Epoch 52/10000, Prediction Accuracy = 32.298%, Loss = 5.0858429908752445
Epoch: 52, Batch Gradient Norm: 5.967708319412983
Epoch: 52, Batch Gradient Norm after: 5.967708319412983
Epoch 53/10000, Prediction Accuracy = 32.574%, Loss = 5.054818344116211
Epoch: 53, Batch Gradient Norm: 7.266517918277244
Epoch: 53, Batch Gradient Norm after: 7.266517918277244
Epoch 54/10000, Prediction Accuracy = 32.674%, Loss = 5.032723522186279
Epoch: 54, Batch Gradient Norm: 5.981119268228015
Epoch: 54, Batch Gradient Norm after: 5.981119268228015
Epoch 55/10000, Prediction Accuracy = 32.972%, Loss = 5.002923202514649
Epoch: 55, Batch Gradient Norm: 7.908409534723448
Epoch: 55, Batch Gradient Norm after: 7.908409534723448
Epoch 56/10000, Prediction Accuracy = 32.958%, Loss = 4.982622337341309
Epoch: 56, Batch Gradient Norm: 7.083396389166072
Epoch: 56, Batch Gradient Norm after: 7.083396389166072
Epoch 57/10000, Prediction Accuracy = 33.378%, Loss = 4.953579711914062
Epoch: 57, Batch Gradient Norm: 8.93057268636838
Epoch: 57, Batch Gradient Norm after: 8.93057268636838
Epoch 58/10000, Prediction Accuracy = 33.288%, Loss = 4.933476829528809
Epoch: 58, Batch Gradient Norm: 7.311430955950398
Epoch: 58, Batch Gradient Norm after: 7.311430955950398
Epoch 59/10000, Prediction Accuracy = 33.620000000000005%, Loss = 4.900961685180664
Epoch: 59, Batch Gradient Norm: 8.530347237760285
Epoch: 59, Batch Gradient Norm after: 8.530347237760285
Epoch 60/10000, Prediction Accuracy = 33.565999999999995%, Loss = 4.878026103973388
Epoch: 60, Batch Gradient Norm: 6.698865891294772
Epoch: 60, Batch Gradient Norm after: 6.698865891294772
Epoch 61/10000, Prediction Accuracy = 33.89%, Loss = 4.844786930084228
Epoch: 61, Batch Gradient Norm: 8.249742613406838
Epoch: 61, Batch Gradient Norm after: 8.249742613406838
Epoch 62/10000, Prediction Accuracy = 33.854%, Loss = 4.8222246170043945
Epoch: 62, Batch Gradient Norm: 7.025206668220077
Epoch: 62, Batch Gradient Norm after: 7.025206668220077
Epoch 63/10000, Prediction Accuracy = 34.17%, Loss = 4.790392684936523
Epoch: 63, Batch Gradient Norm: 9.252854273832918
Epoch: 63, Batch Gradient Norm after: 9.252854273832918
Epoch 64/10000, Prediction Accuracy = 34.07%, Loss = 4.769995498657226
Epoch: 64, Batch Gradient Norm: 8.245318348293713
Epoch: 64, Batch Gradient Norm after: 8.245318348293713
Epoch 65/10000, Prediction Accuracy = 34.224000000000004%, Loss = 4.738261699676514
Epoch: 65, Batch Gradient Norm: 10.102003573189585
Epoch: 65, Batch Gradient Norm after: 10.102003573189585
Epoch 66/10000, Prediction Accuracy = 34.072%, Loss = 4.717109203338623
Epoch: 66, Batch Gradient Norm: 8.12551229445735
Epoch: 66, Batch Gradient Norm after: 8.12551229445735
Epoch 67/10000, Prediction Accuracy = 34.23799999999999%, Loss = 4.681083297729492
Epoch: 67, Batch Gradient Norm: 9.2690100678568
Epoch: 67, Batch Gradient Norm after: 9.2690100678568
Epoch 68/10000, Prediction Accuracy = 34.304%, Loss = 4.65634593963623
Epoch: 68, Batch Gradient Norm: 7.404288786406395
Epoch: 68, Batch Gradient Norm after: 7.404288786406395
Epoch 69/10000, Prediction Accuracy = 34.414%, Loss = 4.621026039123535
Epoch: 69, Batch Gradient Norm: 9.147507183033708
Epoch: 69, Batch Gradient Norm after: 9.147507183033708
Epoch 70/10000, Prediction Accuracy = 34.461999999999996%, Loss = 4.597851657867432
Epoch: 70, Batch Gradient Norm: 8.068275301492127
Epoch: 70, Batch Gradient Norm after: 8.068275301492127
Epoch 71/10000, Prediction Accuracy = 34.534000000000006%, Loss = 4.565155696868897
Epoch: 71, Batch Gradient Norm: 10.286612484131417
Epoch: 71, Batch Gradient Norm after: 10.286612484131417
Epoch 72/10000, Prediction Accuracy = 34.58%, Loss = 4.544338226318359
Epoch: 72, Batch Gradient Norm: 9.075661326987692
Epoch: 72, Batch Gradient Norm after: 9.075661326987692
Epoch 73/10000, Prediction Accuracy = 34.74%, Loss = 4.510676383972168
Epoch: 73, Batch Gradient Norm: 10.664699128796368
Epoch: 73, Batch Gradient Norm after: 10.664699128796368
Epoch 74/10000, Prediction Accuracy = 34.678%, Loss = 4.487653064727783
Epoch: 74, Batch Gradient Norm: 8.682442626778709
Epoch: 74, Batch Gradient Norm after: 8.682442626778709
Epoch 75/10000, Prediction Accuracy = 35.028%, Loss = 4.450869369506836
Epoch: 75, Batch Gradient Norm: 10.044207649835133
Epoch: 75, Batch Gradient Norm after: 10.044207649835133
Epoch 76/10000, Prediction Accuracy = 34.995999999999995%, Loss = 4.426340484619141
Epoch: 76, Batch Gradient Norm: 8.519640771879395
Epoch: 76, Batch Gradient Norm after: 8.519640771879395
Epoch 77/10000, Prediction Accuracy = 35.2%, Loss = 4.391587543487549
Epoch: 77, Batch Gradient Norm: 10.514668494447431
Epoch: 77, Batch Gradient Norm after: 10.514668494447431
Epoch 78/10000, Prediction Accuracy = 35.156%, Loss = 4.36971082687378
Epoch: 78, Batch Gradient Norm: 9.402736505104597
Epoch: 78, Batch Gradient Norm after: 9.402736505104597
Epoch 79/10000, Prediction Accuracy = 35.153999999999996%, Loss = 4.336594867706299
Epoch: 79, Batch Gradient Norm: 11.364209361195078
Epoch: 79, Batch Gradient Norm after: 11.364209361195078
Epoch 80/10000, Prediction Accuracy = 35.22%, Loss = 4.315284156799317
Epoch: 80, Batch Gradient Norm: 9.784877675316087
Epoch: 80, Batch Gradient Norm after: 9.784877675316087
Epoch 81/10000, Prediction Accuracy = 35.239999999999995%, Loss = 4.280149650573731
Epoch: 81, Batch Gradient Norm: 11.247839191283306
Epoch: 81, Batch Gradient Norm after: 11.247839191283306
Epoch 82/10000, Prediction Accuracy = 35.318%, Loss = 4.256944847106934
Epoch: 82, Batch Gradient Norm: 9.43923583481998
Epoch: 82, Batch Gradient Norm after: 9.43923583481998
Epoch 83/10000, Prediction Accuracy = 35.364%, Loss = 4.221374225616455
Epoch: 83, Batch Gradient Norm: 11.083016372129547
Epoch: 83, Batch Gradient Norm after: 11.083016372129547
Epoch 84/10000, Prediction Accuracy = 35.4%, Loss = 4.1988935470581055
Epoch: 84, Batch Gradient Norm: 9.766653964648075
Epoch: 84, Batch Gradient Norm after: 9.766653964648075
Epoch 85/10000, Prediction Accuracy = 35.45%, Loss = 4.165646266937256
Epoch: 85, Batch Gradient Norm: 11.785593267436186
Epoch: 85, Batch Gradient Norm after: 11.785593267436186
Epoch 86/10000, Prediction Accuracy = 35.544%, Loss = 4.145201301574707
Epoch: 86, Batch Gradient Norm: 10.528342346196572
Epoch: 86, Batch Gradient Norm after: 10.528342346196572
Epoch 87/10000, Prediction Accuracy = 35.56%, Loss = 4.112294387817383
Epoch: 87, Batch Gradient Norm: 12.26144830469786
Epoch: 87, Batch Gradient Norm after: 12.26144830469786
Epoch 88/10000, Prediction Accuracy = 35.658%, Loss = 4.091352558135986
Epoch: 88, Batch Gradient Norm: 10.568200267210264
Epoch: 88, Batch Gradient Norm after: 10.568200267210264
Epoch 89/10000, Prediction Accuracy = 35.732%, Loss = 4.056910419464112
Epoch: 89, Batch Gradient Norm: 12.108098975308845
Epoch: 89, Batch Gradient Norm after: 12.108098975308845
Epoch 90/10000, Prediction Accuracy = 35.88%, Loss = 4.035420322418213
Epoch: 90, Batch Gradient Norm: 10.456531598887935
Epoch: 90, Batch Gradient Norm after: 10.456531598887935
Epoch 91/10000, Prediction Accuracy = 35.822%, Loss = 4.001855373382568
Epoch: 91, Batch Gradient Norm: 12.185592436400277
Epoch: 91, Batch Gradient Norm after: 12.185592436400277
Epoch 92/10000, Prediction Accuracy = 35.938%, Loss = 3.981554126739502
Epoch: 92, Batch Gradient Norm: 10.838807682767767
Epoch: 92, Batch Gradient Norm after: 10.838807682767767
Epoch 93/10000, Prediction Accuracy = 35.866%, Loss = 3.9496294498443603
Epoch: 93, Batch Gradient Norm: 12.925595682894059
Epoch: 93, Batch Gradient Norm after: 12.925595682894059
Epoch 94/10000, Prediction Accuracy = 35.916000000000004%, Loss = 3.9313873291015624
Epoch: 94, Batch Gradient Norm: 12.097684918736798
Epoch: 94, Batch Gradient Norm after: 12.097684918736798
Epoch 95/10000, Prediction Accuracy = 35.904%, Loss = 3.90181303024292
Epoch: 95, Batch Gradient Norm: 13.70454471769557
Epoch: 95, Batch Gradient Norm after: 13.70454471769557
Epoch 96/10000, Prediction Accuracy = 36.006%, Loss = 3.882670545578003
Epoch: 96, Batch Gradient Norm: 11.545985358907721
Epoch: 96, Batch Gradient Norm after: 11.545985358907721
Epoch 97/10000, Prediction Accuracy = 35.93%, Loss = 3.84803729057312
Epoch: 97, Batch Gradient Norm: 12.75645596483261
Epoch: 97, Batch Gradient Norm after: 12.75645596483261
Epoch 98/10000, Prediction Accuracy = 36.078%, Loss = 3.8272862434387207
Epoch: 98, Batch Gradient Norm: 10.955786060581511
Epoch: 98, Batch Gradient Norm after: 10.955786060581511
Epoch 99/10000, Prediction Accuracy = 36.102%, Loss = 3.7953070163726808
Epoch: 99, Batch Gradient Norm: 12.960107617217341
Epoch: 99, Batch Gradient Norm after: 12.960107617217341
Epoch 100/10000, Prediction Accuracy = 36.15%, Loss = 3.7780097007751463
Epoch: 100, Batch Gradient Norm: 12.209222801404872
Epoch: 100, Batch Gradient Norm after: 12.209222801404872
Epoch 101/10000, Prediction Accuracy = 36.089999999999996%, Loss = 3.7504224300384523
Epoch: 101, Batch Gradient Norm: 14.687596866159916
Epoch: 101, Batch Gradient Norm after: 14.687596866159916
Epoch 102/10000, Prediction Accuracy = 36.214000000000006%, Loss = 3.736586093902588
Epoch: 102, Batch Gradient Norm: 13.050705474546811
Epoch: 102, Batch Gradient Norm after: 13.050705474546811
Epoch 103/10000, Prediction Accuracy = 36.308%, Loss = 3.7055637359619142
Epoch: 103, Batch Gradient Norm: 14.29298890437329
Epoch: 103, Batch Gradient Norm after: 14.29298890437329
Epoch 104/10000, Prediction Accuracy = 36.412%, Loss = 3.687025308609009
Epoch: 104, Batch Gradient Norm: 11.936754502010361
Epoch: 104, Batch Gradient Norm after: 11.936754502010361
Epoch 105/10000, Prediction Accuracy = 36.354%, Loss = 3.65420880317688
Epoch: 105, Batch Gradient Norm: 13.537803051921061
Epoch: 105, Batch Gradient Norm after: 13.537803051921061
Epoch 106/10000, Prediction Accuracy = 36.519999999999996%, Loss = 3.637159299850464
Epoch: 106, Batch Gradient Norm: 12.188288056964083
Epoch: 106, Batch Gradient Norm after: 12.188288056964083
Epoch 107/10000, Prediction Accuracy = 36.495999999999995%, Loss = 3.60911169052124
Epoch: 107, Batch Gradient Norm: 14.713190616180366
Epoch: 107, Batch Gradient Norm after: 14.713190616180366
Epoch 108/10000, Prediction Accuracy = 36.628%, Loss = 3.596656322479248
Epoch: 108, Batch Gradient Norm: 13.636304211270657
Epoch: 108, Batch Gradient Norm after: 13.636304211270657
Epoch 109/10000, Prediction Accuracy = 36.618%, Loss = 3.570028781890869
Epoch: 109, Batch Gradient Norm: 15.5828043554081
Epoch: 109, Batch Gradient Norm after: 15.5828043554081
Epoch 110/10000, Prediction Accuracy = 36.730000000000004%, Loss = 3.55656418800354
Epoch: 110, Batch Gradient Norm: 13.483351306652514
Epoch: 110, Batch Gradient Norm after: 13.483351306652514
Epoch 111/10000, Prediction Accuracy = 36.702000000000005%, Loss = 3.5263033390045164
Epoch: 111, Batch Gradient Norm: 14.876416685027392
Epoch: 111, Batch Gradient Norm after: 14.876416685027392
Epoch 112/10000, Prediction Accuracy = 36.778%, Loss = 3.5108110427856447
Epoch: 112, Batch Gradient Norm: 12.890761199495543
Epoch: 112, Batch Gradient Norm after: 12.890761199495543
Epoch 113/10000, Prediction Accuracy = 36.784000000000006%, Loss = 3.482218074798584
Epoch: 113, Batch Gradient Norm: 14.995224970012421
Epoch: 113, Batch Gradient Norm after: 14.995224970012421
Epoch 114/10000, Prediction Accuracy = 36.842%, Loss = 3.4699330806732176
Epoch: 114, Batch Gradient Norm: 13.813204353291
Epoch: 114, Batch Gradient Norm after: 13.813204353291
Epoch 115/10000, Prediction Accuracy = 36.82%, Loss = 3.4451038360595705
Epoch: 115, Batch Gradient Norm: 16.089124246442797
Epoch: 115, Batch Gradient Norm after: 16.089124246442797
Epoch 116/10000, Prediction Accuracy = 36.95399999999999%, Loss = 3.434688377380371
Epoch: 116, Batch Gradient Norm: 14.539214085188517
Epoch: 116, Batch Gradient Norm after: 14.539214085188517
Epoch 117/10000, Prediction Accuracy = 36.936%, Loss = 3.4087923526763917
Epoch: 117, Batch Gradient Norm: 16.267986631651112
Epoch: 117, Batch Gradient Norm after: 16.267986631651112
Epoch 118/10000, Prediction Accuracy = 37.048%, Loss = 3.397018051147461
Epoch: 118, Batch Gradient Norm: 14.13741044843213
Epoch: 118, Batch Gradient Norm after: 14.13741044843213
Epoch 119/10000, Prediction Accuracy = 37.022000000000006%, Loss = 3.3696527004241945
Epoch: 119, Batch Gradient Norm: 15.789962375209747
Epoch: 119, Batch Gradient Norm after: 15.789962375209747
Epoch 120/10000, Prediction Accuracy = 37.17%, Loss = 3.3579153537750246
Epoch: 120, Batch Gradient Norm: 14.061799889142572
Epoch: 120, Batch Gradient Norm after: 14.061799889142572
Epoch 121/10000, Prediction Accuracy = 37.08%, Loss = 3.333069944381714
Epoch: 121, Batch Gradient Norm: 16.27545590610248
Epoch: 121, Batch Gradient Norm after: 16.27545590610248
Epoch 122/10000, Prediction Accuracy = 37.254%, Loss = 3.3241944313049316
Epoch: 122, Batch Gradient Norm: 14.949232380674864
Epoch: 122, Batch Gradient Norm after: 14.949232380674864
Epoch 123/10000, Prediction Accuracy = 37.17%, Loss = 3.30147442817688
Epoch: 123, Batch Gradient Norm: 17.087433458248306
Epoch: 123, Batch Gradient Norm after: 17.087433458248306
Epoch 124/10000, Prediction Accuracy = 37.30200000000001%, Loss = 3.2933933258056642
Epoch: 124, Batch Gradient Norm: 15.336379406461797
Epoch: 124, Batch Gradient Norm after: 15.336379406461797
Epoch 125/10000, Prediction Accuracy = 37.3%, Loss = 3.2695149421691894
Epoch: 125, Batch Gradient Norm: 17.01145438643622
Epoch: 125, Batch Gradient Norm after: 17.01145438643622
Epoch 126/10000, Prediction Accuracy = 37.370000000000005%, Loss = 3.2602384090423584
Epoch: 126, Batch Gradient Norm: 14.917405764265252
Epoch: 126, Batch Gradient Norm after: 14.917405764265252
Epoch 127/10000, Prediction Accuracy = 37.392%, Loss = 3.2359246253967284
Epoch: 127, Batch Gradient Norm: 16.680351684316744
Epoch: 127, Batch Gradient Norm after: 16.680351684316744
Epoch 128/10000, Prediction Accuracy = 37.44599999999999%, Loss = 3.2273812294006348
Epoch: 128, Batch Gradient Norm: 15.030003947411531
Epoch: 128, Batch Gradient Norm after: 15.030003947411531
Epoch 129/10000, Prediction Accuracy = 37.492000000000004%, Loss = 3.205565643310547
Epoch: 129, Batch Gradient Norm: 17.232640929025383
Epoch: 129, Batch Gradient Norm after: 17.232640929025383
Epoch 130/10000, Prediction Accuracy = 37.518%, Loss = 3.1994350433349608
Epoch: 130, Batch Gradient Norm: 15.837416529983457
Epoch: 130, Batch Gradient Norm after: 15.837416529983457
Epoch 131/10000, Prediction Accuracy = 37.564%, Loss = 3.1790599822998047
Epoch: 131, Batch Gradient Norm: 17.902103193546264
Epoch: 131, Batch Gradient Norm after: 17.902103193546264
Epoch 132/10000, Prediction Accuracy = 37.542%, Loss = 3.173334550857544
Epoch: 132, Batch Gradient Norm: 16.09696117316809
Epoch: 132, Batch Gradient Norm after: 16.09696117316809
Epoch 133/10000, Prediction Accuracy = 37.628%, Loss = 3.1518917083740234
Epoch: 133, Batch Gradient Norm: 17.724148498842272
Epoch: 133, Batch Gradient Norm after: 17.724148498842272
Epoch 134/10000, Prediction Accuracy = 37.629999999999995%, Loss = 3.145001029968262
Epoch: 134, Batch Gradient Norm: 15.697691356442293
Epoch: 134, Batch Gradient Norm after: 15.697691356442293
Epoch 135/10000, Prediction Accuracy = 37.668%, Loss = 3.1236057758331297
Epoch: 135, Batch Gradient Norm: 17.47927355633432
Epoch: 135, Batch Gradient Norm after: 17.47927355633432
Epoch 136/10000, Prediction Accuracy = 37.672000000000004%, Loss = 3.1176331520080565
Epoch: 136, Batch Gradient Norm: 15.86664721230758
Epoch: 136, Batch Gradient Norm after: 15.86664721230758
Epoch 137/10000, Prediction Accuracy = 37.714%, Loss = 3.09851279258728
Epoch: 137, Batch Gradient Norm: 18.023687546134074
Epoch: 137, Batch Gradient Norm after: 18.023687546134074
Epoch 138/10000, Prediction Accuracy = 37.727999999999994%, Loss = 3.094553565979004
Epoch: 138, Batch Gradient Norm: 16.597619564459034
Epoch: 138, Batch Gradient Norm after: 16.597619564459034
Epoch 139/10000, Prediction Accuracy = 37.786%, Loss = 3.0765798568725584
Epoch: 139, Batch Gradient Norm: 18.304042095996024
Epoch: 139, Batch Gradient Norm after: 18.25123177343538
Epoch 140/10000, Prediction Accuracy = 37.794%, Loss = 3.071604537963867
Epoch: 140, Batch Gradient Norm: 16.621444825318395
Epoch: 140, Batch Gradient Norm after: 16.621444825318395
Epoch 141/10000, Prediction Accuracy = 37.864%, Loss = 3.0531846046447755
Epoch: 141, Batch Gradient Norm: 18.31735598360308
Epoch: 141, Batch Gradient Norm after: 18.269409867198885
Epoch 142/10000, Prediction Accuracy = 37.856%, Loss = 3.048567199707031
Epoch: 142, Batch Gradient Norm: 16.695138290471792
Epoch: 142, Batch Gradient Norm after: 16.695138290471792
Epoch 143/10000, Prediction Accuracy = 37.976%, Loss = 3.030924367904663
Epoch: 143, Batch Gradient Norm: 18.33789648343197
Epoch: 143, Batch Gradient Norm after: 18.270705557588222
Epoch 144/10000, Prediction Accuracy = 37.874%, Loss = 3.026560735702515
Epoch: 144, Batch Gradient Norm: 16.79373336159112
Epoch: 144, Batch Gradient Norm after: 16.79373336159112
Epoch 145/10000, Prediction Accuracy = 38.022%, Loss = 3.009769058227539
Epoch: 145, Batch Gradient Norm: 18.337663645717445
Epoch: 145, Batch Gradient Norm after: 18.223346392337074
Epoch 146/10000, Prediction Accuracy = 37.974000000000004%, Loss = 3.0054334163665772
Epoch: 146, Batch Gradient Norm: 16.966466491681945
Epoch: 146, Batch Gradient Norm after: 16.966466491681945
Epoch 147/10000, Prediction Accuracy = 38.056%, Loss = 2.9897905826568603
Epoch: 147, Batch Gradient Norm: 18.344773563657892
Epoch: 147, Batch Gradient Norm after: 18.147227122831573
Epoch 148/10000, Prediction Accuracy = 38.062%, Loss = 2.9852297782897947
Epoch: 148, Batch Gradient Norm: 17.3379753384819
Epoch: 148, Batch Gradient Norm after: 17.3379753384819
Epoch 149/10000, Prediction Accuracy = 38.134%, Loss = 2.971359062194824
Epoch: 149, Batch Gradient Norm: 18.20993703320992
Epoch: 149, Batch Gradient Norm after: 17.851850330571803
Epoch 150/10000, Prediction Accuracy = 38.104%, Loss = 2.965441846847534
Epoch: 150, Batch Gradient Norm: 17.28895265739652
Epoch: 150, Batch Gradient Norm after: 17.28895265739652
Epoch 151/10000, Prediction Accuracy = 38.204%, Loss = 2.9523812770843505
Epoch: 151, Batch Gradient Norm: 18.3529142907837
Epoch: 151, Batch Gradient Norm after: 17.945850130038277
Epoch 152/10000, Prediction Accuracy = 38.15599999999999%, Loss = 2.947502088546753
Epoch: 152, Batch Gradient Norm: 17.90370051768725
Epoch: 152, Batch Gradient Norm after: 17.821028158700365
Epoch 153/10000, Prediction Accuracy = 38.254%, Loss = 2.9364968299865724
Epoch: 153, Batch Gradient Norm: 18.390887100984614
Epoch: 153, Batch Gradient Norm after: 17.93231236301419
Epoch 154/10000, Prediction Accuracy = 38.20399999999999%, Loss = 2.9299859523773195
Epoch: 154, Batch Gradient Norm: 18.174885190728105
Epoch: 154, Batch Gradient Norm after: 17.99416269003402
Epoch 155/10000, Prediction Accuracy = 38.338%, Loss = 2.920215702056885
Epoch: 155, Batch Gradient Norm: 18.65401351295878
Epoch: 155, Batch Gradient Norm after: 18.28538876221427
Epoch 156/10000, Prediction Accuracy = 38.291999999999994%, Loss = 2.9140486240386965
Epoch: 156, Batch Gradient Norm: 18.891731395380006
Epoch: 156, Batch Gradient Norm after: 18.51890570338158
Epoch 157/10000, Prediction Accuracy = 38.406%, Loss = 2.9062310218811036
Epoch: 157, Batch Gradient Norm: 18.994323879555452
Epoch: 157, Batch Gradient Norm after: 18.82777221903691
Epoch 158/10000, Prediction Accuracy = 38.342%, Loss = 2.8990727424621583
Epoch: 158, Batch Gradient Norm: 19.36009655171795
Epoch: 158, Batch Gradient Norm after: 18.915413343359525
Epoch 159/10000, Prediction Accuracy = 38.476%, Loss = 2.892067384719849
Epoch: 159, Batch Gradient Norm: 19.04183289111802
Epoch: 159, Batch Gradient Norm after: 18.98376790728455
Epoch 160/10000, Prediction Accuracy = 38.41%, Loss = 2.88370623588562
Epoch: 160, Batch Gradient Norm: 19.192204220070998
Epoch: 160, Batch Gradient Norm after: 18.81229896196773
Epoch 161/10000, Prediction Accuracy = 38.59%, Loss = 2.8762410163879393
Epoch: 161, Batch Gradient Norm: 18.861460984762534
Epoch: 161, Batch Gradient Norm after: 18.76660291009783
Epoch 162/10000, Prediction Accuracy = 38.455999999999996%, Loss = 2.8681817054748535
Epoch: 162, Batch Gradient Norm: 18.757632331051834
Epoch: 162, Batch Gradient Norm after: 18.503972701926557
Epoch 163/10000, Prediction Accuracy = 38.646%, Loss = 2.8600996017456053
Epoch: 163, Batch Gradient Norm: 18.703730695149854
Epoch: 163, Batch Gradient Norm after: 18.488838968553473
Epoch 164/10000, Prediction Accuracy = 38.568%, Loss = 2.8533390045166014
Epoch: 164, Batch Gradient Norm: 18.68171430546796
Epoch: 164, Batch Gradient Norm after: 18.412397126526013
Epoch 165/10000, Prediction Accuracy = 38.73%, Loss = 2.845802116394043
Epoch: 165, Batch Gradient Norm: 18.909539364668817
Epoch: 165, Batch Gradient Norm after: 18.68850150355481
Epoch 166/10000, Prediction Accuracy = 38.660000000000004%, Loss = 2.8402029991149904
Epoch: 166, Batch Gradient Norm: 19.386761962956832
Epoch: 166, Batch Gradient Norm after: 18.92095502083548
Epoch 167/10000, Prediction Accuracy = 38.798%, Loss = 2.8346200466156004
Epoch: 167, Batch Gradient Norm: 19.325275137595263
Epoch: 167, Batch Gradient Norm after: 19.288484911395084
Epoch 168/10000, Prediction Accuracy = 38.727999999999994%, Loss = 2.8283395290374758
Epoch: 168, Batch Gradient Norm: 19.861417645708705
Epoch: 168, Batch Gradient Norm after: 19.374426556472194
Epoch 169/10000, Prediction Accuracy = 38.842%, Loss = 2.8233268737792967
Epoch: 169, Batch Gradient Norm: 19.165454200555956
Epoch: 169, Batch Gradient Norm after: 19.115758716600546
Epoch 170/10000, Prediction Accuracy = 38.762%, Loss = 2.8150569438934325
Epoch: 170, Batch Gradient Norm: 19.625733812682657
Epoch: 170, Batch Gradient Norm after: 19.15760264186191
Epoch 171/10000, Prediction Accuracy = 38.914%, Loss = 2.809963512420654
Epoch: 171, Batch Gradient Norm: 19.268083287361975
Epoch: 171, Batch Gradient Norm after: 19.268083287361975
Epoch 172/10000, Prediction Accuracy = 38.876000000000005%, Loss = 2.8031246662139893
Epoch: 172, Batch Gradient Norm: 19.789065898174233
Epoch: 172, Batch Gradient Norm after: 19.31166595696955
Epoch 173/10000, Prediction Accuracy = 38.946000000000005%, Loss = 2.798470640182495
Epoch: 173, Batch Gradient Norm: 19.304511031695895
Epoch: 173, Batch Gradient Norm after: 19.304511031695895
Epoch 174/10000, Prediction Accuracy = 38.942%, Loss = 2.7913763523101807
Epoch: 174, Batch Gradient Norm: 19.7890798378443
Epoch: 174, Batch Gradient Norm after: 19.318404007094937
Epoch 175/10000, Prediction Accuracy = 39.00599999999999%, Loss = 2.786830949783325
Epoch: 175, Batch Gradient Norm: 19.317911579336677
Epoch: 175, Batch Gradient Norm after: 19.31624198762935
Epoch 176/10000, Prediction Accuracy = 39.007999999999996%, Loss = 2.780015563964844
Epoch: 176, Batch Gradient Norm: 19.813735360893862
Epoch: 176, Batch Gradient Norm after: 19.345307287963315
Epoch 177/10000, Prediction Accuracy = 39.07%, Loss = 2.775701904296875
Epoch: 177, Batch Gradient Norm: 19.387760632054995
Epoch: 177, Batch Gradient Norm after: 19.379381840300383
Epoch 178/10000, Prediction Accuracy = 39.086%, Loss = 2.769241762161255
Epoch: 178, Batch Gradient Norm: 19.88633741004385
Epoch: 178, Batch Gradient Norm after: 19.424759649315256
Epoch 179/10000, Prediction Accuracy = 39.128%, Loss = 2.7651336193084717
Epoch: 179, Batch Gradient Norm: 19.342945316432225
Epoch: 179, Batch Gradient Norm after: 19.295905484189873
Epoch 180/10000, Prediction Accuracy = 39.122%, Loss = 2.758457565307617
Epoch: 180, Batch Gradient Norm: 19.878449699632135
Epoch: 180, Batch Gradient Norm after: 19.413024177226067
Epoch 181/10000, Prediction Accuracy = 39.192%, Loss = 2.7546316623687743
Epoch: 181, Batch Gradient Norm: 19.430887261106456
Epoch: 181, Batch Gradient Norm after: 19.375521145322487
Epoch 182/10000, Prediction Accuracy = 39.162%, Loss = 2.7484500408172607
Epoch: 182, Batch Gradient Norm: 20.00051601318238
Epoch: 182, Batch Gradient Norm after: 19.545406832697438
Epoch 183/10000, Prediction Accuracy = 39.254%, Loss = 2.7449405670166014
Epoch: 183, Batch Gradient Norm: 19.317509719195975
Epoch: 183, Batch Gradient Norm after: 19.17977607258674
Epoch 184/10000, Prediction Accuracy = 39.242000000000004%, Loss = 2.738114070892334
Epoch: 184, Batch Gradient Norm: 19.98350676548185
Epoch: 184, Batch Gradient Norm after: 19.514206885553595
Epoch 185/10000, Prediction Accuracy = 39.336%, Loss = 2.735044240951538
Epoch: 185, Batch Gradient Norm: 19.4115754039081
Epoch: 185, Batch Gradient Norm after: 19.25088345225866
Epoch 186/10000, Prediction Accuracy = 39.322%, Loss = 2.7287466526031494
Epoch: 186, Batch Gradient Norm: 20.146888352542806
Epoch: 186, Batch Gradient Norm after: 19.680862930617213
Epoch 187/10000, Prediction Accuracy = 39.378%, Loss = 2.7260809421539305
Epoch: 187, Batch Gradient Norm: 19.341678724974052
Epoch: 187, Batch Gradient Norm after: 19.092400750507107
Epoch 188/10000, Prediction Accuracy = 39.356%, Loss = 2.7191519260406496
Epoch: 188, Batch Gradient Norm: 20.160155728306243
Epoch: 188, Batch Gradient Norm after: 19.686239239925985
Epoch 189/10000, Prediction Accuracy = 39.422%, Loss = 2.7169064044952393
Epoch: 189, Batch Gradient Norm: 19.399289787646737
Epoch: 189, Batch Gradient Norm after: 19.105867456299165
Epoch 190/10000, Prediction Accuracy = 39.402%, Loss = 2.7102682113647463
Epoch: 190, Batch Gradient Norm: 20.31506912559431
Epoch: 190, Batch Gradient Norm after: 19.837919095528395
Epoch 191/10000, Prediction Accuracy = 39.504000000000005%, Loss = 2.7085169315338136
Epoch: 191, Batch Gradient Norm: 19.305749657836255
Epoch: 191, Batch Gradient Norm after: 18.898200012990355
Epoch 192/10000, Prediction Accuracy = 39.470000000000006%, Loss = 2.7011380195617676
Epoch: 192, Batch Gradient Norm: 20.341430724784093
Epoch: 192, Batch Gradient Norm after: 19.84774006248341
Epoch 193/10000, Prediction Accuracy = 39.55%, Loss = 2.699898862838745
Epoch: 193, Batch Gradient Norm: 19.38988426077037
Epoch: 193, Batch Gradient Norm after: 18.940111636848098
Epoch 194/10000, Prediction Accuracy = 39.495999999999995%, Loss = 2.692893362045288
Epoch: 194, Batch Gradient Norm: 20.542273552455192
Epoch: 194, Batch Gradient Norm after: 19.970302970951018
Epoch 195/10000, Prediction Accuracy = 39.598%, Loss = 2.6921518802642823
Epoch: 195, Batch Gradient Norm: 19.57341168679176
Epoch: 195, Batch Gradient Norm after: 19.14518513152613
Epoch 196/10000, Prediction Accuracy = 39.553999999999995%, Loss = 2.6851696968078613
Epoch: 196, Batch Gradient Norm: 20.825273435405713
Epoch: 196, Batch Gradient Norm after: 20.11025703934447
Epoch 197/10000, Prediction Accuracy = 39.688%, Loss = 2.6849466800689696
Epoch: 197, Batch Gradient Norm: 20.004999541322643
Epoch: 197, Batch Gradient Norm after: 19.737707871505588
Epoch 198/10000, Prediction Accuracy = 39.63%, Loss = 2.6784736633300783
Epoch: 198, Batch Gradient Norm: 21.293751741307357
Epoch: 198, Batch Gradient Norm after: 20.509234262894505
Epoch 199/10000, Prediction Accuracy = 39.769999999999996%, Loss = 2.6785701274871827
Epoch: 199, Batch Gradient Norm: 19.931055641820695
Epoch: 199, Batch Gradient Norm after: 19.678019511792503
Epoch 200/10000, Prediction Accuracy = 39.716%, Loss = 2.6703298091888428
Epoch: 200, Batch Gradient Norm: 21.153105733972588
Epoch: 200, Batch Gradient Norm after: 20.36381182957324
Epoch 201/10000, Prediction Accuracy = 39.842%, Loss = 2.670319128036499
Epoch: 201, Batch Gradient Norm: 20.11301591008991
Epoch: 201, Batch Gradient Norm after: 19.952349716044758
Epoch 202/10000, Prediction Accuracy = 39.754000000000005%, Loss = 2.6632678508758545
Epoch: 202, Batch Gradient Norm: 21.29250567005566
Epoch: 202, Batch Gradient Norm after: 20.521647225896853
Epoch 203/10000, Prediction Accuracy = 39.878%, Loss = 2.663199615478516
Epoch: 203, Batch Gradient Norm: 19.937027790401338
Epoch: 203, Batch Gradient Norm after: 19.73288645329582
Epoch 204/10000, Prediction Accuracy = 39.8%, Loss = 2.6551719665527345
Epoch: 204, Batch Gradient Norm: 21.095565991692027
Epoch: 204, Batch Gradient Norm after: 20.306670826459733
Epoch 205/10000, Prediction Accuracy = 39.93%, Loss = 2.6550940036773683
Epoch: 205, Batch Gradient Norm: 20.222242268777944
Epoch: 205, Batch Gradient Norm after: 20.122183332178547
Epoch 206/10000, Prediction Accuracy = 39.89%, Loss = 2.6487604141235352
Epoch: 206, Batch Gradient Norm: 21.3698856571677
Epoch: 206, Batch Gradient Norm after: 20.604886799021664
Epoch 207/10000, Prediction Accuracy = 39.977999999999994%, Loss = 2.648735523223877
Epoch: 207, Batch Gradient Norm: 19.915687087023716
Epoch: 207, Batch Gradient Norm after: 19.714012546998934
Epoch 208/10000, Prediction Accuracy = 39.936%, Loss = 2.6405889987945557
Epoch: 208, Batch Gradient Norm: 21.051481567680963
Epoch: 208, Batch Gradient Norm after: 20.259265819600834
Epoch 209/10000, Prediction Accuracy = 40.036%, Loss = 2.6405568599700926
Epoch: 209, Batch Gradient Norm: 20.34648762941827
Epoch: 209, Batch Gradient Norm after: 20.283295781861053
Epoch 210/10000, Prediction Accuracy = 40.016%, Loss = 2.634963941574097
Epoch: 210, Batch Gradient Norm: 21.482798781891788
Epoch: 210, Batch Gradient Norm after: 20.722580045227826
Epoch 211/10000, Prediction Accuracy = 40.09%, Loss = 2.635048007965088
Epoch: 211, Batch Gradient Norm: 19.86288970067618
Epoch: 211, Batch Gradient Norm after: 19.629677059909486
Epoch 212/10000, Prediction Accuracy = 40.098%, Loss = 2.6265026092529298
Epoch: 212, Batch Gradient Norm: 21.001200766253685
Epoch: 212, Batch Gradient Norm after: 20.217706043674145
Epoch 213/10000, Prediction Accuracy = 40.148%, Loss = 2.6265970706939696
Epoch: 213, Batch Gradient Norm: 20.431639925744346
Epoch: 213, Batch Gradient Norm after: 20.367522640028593
Epoch 214/10000, Prediction Accuracy = 40.17%, Loss = 2.6215876579284667
Epoch: 214, Batch Gradient Norm: 21.601227396478475
Epoch: 214, Batch Gradient Norm after: 20.824329987034364
Epoch 215/10000, Prediction Accuracy = 40.22%, Loss = 2.621890163421631
Epoch: 215, Batch Gradient Norm: 19.893734845242005
Epoch: 215, Batch Gradient Norm after: 19.639203144134786
Epoch 216/10000, Prediction Accuracy = 40.212%, Loss = 2.6132235527038574
Epoch: 216, Batch Gradient Norm: 21.08272912817914
Epoch: 216, Batch Gradient Norm after: 20.262025311345038
Epoch 217/10000, Prediction Accuracy = 40.248000000000005%, Loss = 2.6135924816131593
Epoch: 217, Batch Gradient Norm: 20.531076101014385
Epoch: 217, Batch Gradient Norm after: 20.48231873411516
Epoch 218/10000, Prediction Accuracy = 40.286%, Loss = 2.608802556991577
Epoch: 218, Batch Gradient Norm: 21.7741910081102
Epoch: 218, Batch Gradient Norm after: 20.99619353333921
Epoch 219/10000, Prediction Accuracy = 40.29600000000001%, Loss = 2.6094758987426756
Epoch: 219, Batch Gradient Norm: 19.813080709947855
Epoch: 219, Batch Gradient Norm after: 19.492342742599718
Epoch 220/10000, Prediction Accuracy = 40.354%, Loss = 2.600162982940674
Epoch: 220, Batch Gradient Norm: 21.079914930832203
Epoch: 220, Batch Gradient Norm after: 20.241194145071336
Epoch 221/10000, Prediction Accuracy = 40.33%, Loss = 2.600819158554077
Epoch: 221, Batch Gradient Norm: 20.547541077833834
Epoch: 221, Batch Gradient Norm after: 20.473666239143483
Epoch 222/10000, Prediction Accuracy = 40.39399999999999%, Loss = 2.5962549209594727
Epoch: 222, Batch Gradient Norm: 21.901575563146267
Epoch: 222, Batch Gradient Norm after: 21.108035154458527
Epoch 223/10000, Prediction Accuracy = 40.397999999999996%, Loss = 2.597391891479492
Epoch: 223, Batch Gradient Norm: 19.81780153719144
Epoch: 223, Batch Gradient Norm after: 19.436093232450002
Epoch 224/10000, Prediction Accuracy = 40.452000000000005%, Loss = 2.587837648391724
Epoch: 224, Batch Gradient Norm: 21.1831694378679
Epoch: 224, Batch Gradient Norm after: 20.33100744531002
Epoch 225/10000, Prediction Accuracy = 40.47%, Loss = 2.588924264907837
Epoch: 225, Batch Gradient Norm: 20.707425314340462
Epoch: 225, Batch Gradient Norm after: 20.618248628548024
Epoch 226/10000, Prediction Accuracy = 40.504000000000005%, Loss = 2.5846951961517335
Epoch: 226, Batch Gradient Norm: 22.170547008224833
Epoch: 226, Batch Gradient Norm after: 21.328890310349085
Epoch 227/10000, Prediction Accuracy = 40.532000000000004%, Loss = 2.58628249168396
Epoch: 227, Batch Gradient Norm: 19.85257799229261
Epoch: 227, Batch Gradient Norm after: 19.4534194123227
Epoch 228/10000, Prediction Accuracy = 40.565999999999995%, Loss = 2.576088809967041
Epoch: 228, Batch Gradient Norm: 21.275755096580987
Epoch: 228, Batch Gradient Norm after: 20.424793332595897
Epoch 229/10000, Prediction Accuracy = 40.56400000000001%, Loss = 2.5774160861968993
Epoch: 229, Batch Gradient Norm: 20.775293899623588
Epoch: 229, Batch Gradient Norm after: 20.656510116476714
Epoch 230/10000, Prediction Accuracy = 40.60799999999999%, Loss = 2.5732235431671144
Epoch: 230, Batch Gradient Norm: 22.32880495979396
Epoch: 230, Batch Gradient Norm after: 21.423720493911105
Epoch 231/10000, Prediction Accuracy = 40.598%, Loss = 2.5752061367034913
Epoch: 231, Batch Gradient Norm: 20.070693864758766
Epoch: 231, Batch Gradient Norm after: 19.724508917352125
Epoch 232/10000, Prediction Accuracy = 40.642%, Loss = 2.5653018951416016
Epoch: 232, Batch Gradient Norm: 21.545275886401647
Epoch: 232, Batch Gradient Norm after: 20.70619449253104
Epoch 233/10000, Prediction Accuracy = 40.644%, Loss = 2.5669344902038573
Epoch: 233, Batch Gradient Norm: 20.52522699593449
Epoch: 233, Batch Gradient Norm after: 20.292812694153927
Epoch 234/10000, Prediction Accuracy = 40.646%, Loss = 2.561127185821533
Epoch: 234, Batch Gradient Norm: 22.113960812420764
Epoch: 234, Batch Gradient Norm after: 21.27613928809869
Epoch 235/10000, Prediction Accuracy = 40.71%, Loss = 2.563290500640869
Epoch: 235, Batch Gradient Norm: 19.995482728683893
Epoch: 235, Batch Gradient Norm after: 19.536311443664683
Epoch 236/10000, Prediction Accuracy = 40.674%, Loss = 2.5539739608764647
Epoch: 236, Batch Gradient Norm: 21.580949702670505
Epoch: 236, Batch Gradient Norm after: 20.71733033873926
Epoch 237/10000, Prediction Accuracy = 40.763999999999996%, Loss = 2.5560847759246825
Epoch: 237, Batch Gradient Norm: 20.667642282273114
Epoch: 237, Batch Gradient Norm after: 20.404096667625474
Epoch 238/10000, Prediction Accuracy = 40.738%, Loss = 2.55071382522583
Epoch: 238, Batch Gradient Norm: 22.42461670705211
Epoch: 238, Batch Gradient Norm after: 21.454015583247998
Epoch 239/10000, Prediction Accuracy = 40.818%, Loss = 2.553561305999756
Epoch: 239, Batch Gradient Norm: 20.396352254520068
Epoch: 239, Batch Gradient Norm after: 20.040463279898052
Epoch 240/10000, Prediction Accuracy = 40.763999999999996%, Loss = 2.544460391998291
Epoch: 240, Batch Gradient Norm: 22.114730968012324
Epoch: 240, Batch Gradient Norm after: 21.254966725595892
Epoch 241/10000, Prediction Accuracy = 40.86%, Loss = 2.547100305557251
Epoch: 241, Batch Gradient Norm: 20.155835937629853
Epoch: 241, Batch Gradient Norm after: 19.66074462982994
Epoch 242/10000, Prediction Accuracy = 40.828%, Loss = 2.5383955001831056
Epoch: 242, Batch Gradient Norm: 21.94269977072429
Epoch: 242, Batch Gradient Norm after: 21.06066491928889
Epoch 243/10000, Prediction Accuracy = 40.9%, Loss = 2.5412702560424805
Epoch: 243, Batch Gradient Norm: 20.45694678927195
Epoch: 243, Batch Gradient Norm after: 20.01517554253266
Epoch 244/10000, Prediction Accuracy = 40.80800000000001%, Loss = 2.5341119289398195
Epoch: 244, Batch Gradient Norm: 22.380893253365635
Epoch: 244, Batch Gradient Norm after: 21.389174276193035
Epoch 245/10000, Prediction Accuracy = 40.910000000000004%, Loss = 2.53751859664917
Epoch: 245, Batch Gradient Norm: 20.58130052010361
Epoch: 245, Batch Gradient Norm after: 20.183212003179836
Epoch 246/10000, Prediction Accuracy = 40.886%, Loss = 2.529403257369995
Epoch: 246, Batch Gradient Norm: 22.544270163592355
Epoch: 246, Batch Gradient Norm after: 21.50060122626585
Epoch 247/10000, Prediction Accuracy = 40.954%, Loss = 2.5330188274383545
Epoch: 247, Batch Gradient Norm: 20.688526928141165
Epoch: 247, Batch Gradient Norm after: 20.34224062435578
Epoch 248/10000, Prediction Accuracy = 40.956%, Loss = 2.5246901988983153
Epoch: 248, Batch Gradient Norm: 22.66308081208681
Epoch: 248, Batch Gradient Norm after: 21.589013193313228
Epoch 249/10000, Prediction Accuracy = 40.968%, Loss = 2.5284021377563475
Epoch: 249, Batch Gradient Norm: 20.79336417533996
Epoch: 249, Batch Gradient Norm after: 20.48871761432952
Epoch 250/10000, Prediction Accuracy = 40.976%, Loss = 2.520074415206909
Epoch: 250, Batch Gradient Norm: 22.94969900898299
Epoch: 250, Batch Gradient Norm after: 21.782248756468626
Epoch 251/10000, Prediction Accuracy = 41.016%, Loss = 2.5244369506835938
Epoch: 251, Batch Gradient Norm: 20.974761206836984
Epoch: 251, Batch Gradient Norm after: 20.780686941086007
Epoch 252/10000, Prediction Accuracy = 41.032%, Loss = 2.5157061576843263
Epoch: 252, Batch Gradient Norm: 22.854243478597223
Epoch: 252, Batch Gradient Norm after: 21.752582764202028
Epoch 253/10000, Prediction Accuracy = 41.028000000000006%, Loss = 2.519233465194702
Epoch: 253, Batch Gradient Norm: 20.762799630607102
Epoch: 253, Batch Gradient Norm after: 20.525910806704847
Epoch 254/10000, Prediction Accuracy = 41.058%, Loss = 2.510196018218994
Epoch: 254, Batch Gradient Norm: 22.57523554064846
Epoch: 254, Batch Gradient Norm after: 21.582462047827118
Epoch 255/10000, Prediction Accuracy = 41.05800000000001%, Loss = 2.513463354110718
Epoch: 255, Batch Gradient Norm: 20.519417403459077
Epoch: 255, Batch Gradient Norm after: 20.170680496384463
Epoch 256/10000, Prediction Accuracy = 41.114000000000004%, Loss = 2.504662036895752
Epoch: 256, Batch Gradient Norm: 22.36821920043102
Epoch: 256, Batch Gradient Norm after: 21.428753968768554
Epoch 257/10000, Prediction Accuracy = 41.083999999999996%, Loss = 2.5080429553985595
Epoch: 257, Batch Gradient Norm: 20.43924164200511
Epoch: 257, Batch Gradient Norm after: 19.997711698350155
Epoch 258/10000, Prediction Accuracy = 41.116%, Loss = 2.4997048377990723
Epoch: 258, Batch Gradient Norm: 22.38463212358163
Epoch: 258, Batch Gradient Norm after: 21.416676745406946
Epoch 259/10000, Prediction Accuracy = 41.138%, Loss = 2.5033801555633546
Epoch: 259, Batch Gradient Norm: 20.57952653803399
Epoch: 259, Batch Gradient Norm after: 20.131945462495608
Epoch 260/10000, Prediction Accuracy = 41.196%, Loss = 2.4954957485198976
Epoch: 260, Batch Gradient Norm: 22.666451255626203
Epoch: 260, Batch Gradient Norm after: 21.57643056001396
Epoch 261/10000, Prediction Accuracy = 41.184000000000005%, Loss = 2.4997161388397218
Epoch: 261, Batch Gradient Norm: 20.93171619166826
Epoch: 261, Batch Gradient Norm after: 20.591056828515168
Epoch 262/10000, Prediction Accuracy = 41.236%, Loss = 2.4920310020446776
Epoch: 262, Batch Gradient Norm: 23.13005731392306
Epoch: 262, Batch Gradient Norm after: 21.875548676899562
Epoch 263/10000, Prediction Accuracy = 41.20399999999999%, Loss = 2.4967222213745117
Epoch: 263, Batch Gradient Norm: 21.241287788755578
Epoch: 263, Batch Gradient Norm after: 21.04974445348954
Epoch 264/10000, Prediction Accuracy = 41.275999999999996%, Loss = 2.4884713172912596
Epoch: 264, Batch Gradient Norm: 23.23360186158108
Epoch: 264, Batch Gradient Norm after: 21.996063023812447
Epoch 265/10000, Prediction Accuracy = 41.256%, Loss = 2.492592716217041
Epoch: 265, Batch Gradient Norm: 21.011050732379687
Epoch: 265, Batch Gradient Norm after: 20.769269196972836
Epoch 266/10000, Prediction Accuracy = 41.298%, Loss = 2.4832666397094725
Epoch: 266, Batch Gradient Norm: 23.051257415458895
Epoch: 266, Batch Gradient Norm after: 21.86757186608329
Epoch 267/10000, Prediction Accuracy = 41.286%, Loss = 2.4875005722045898
Epoch: 267, Batch Gradient Norm: 21.136968742387985
Epoch: 267, Batch Gradient Norm after: 20.94427044020438
Epoch 268/10000, Prediction Accuracy = 41.321999999999996%, Loss = 2.479201602935791
Epoch: 268, Batch Gradient Norm: 23.141311123679124
Epoch: 268, Batch Gradient Norm after: 21.947396200644615
Epoch 269/10000, Prediction Accuracy = 41.31599999999999%, Loss = 2.4834107875823976
Epoch: 269, Batch Gradient Norm: 21.097658361587925
Epoch: 269, Batch Gradient Norm after: 20.90813914234667
Epoch 270/10000, Prediction Accuracy = 41.376%, Loss = 2.474702024459839
Epoch: 270, Batch Gradient Norm: 23.101293951525886
Epoch: 270, Batch Gradient Norm after: 21.924842328130897
Epoch 271/10000, Prediction Accuracy = 41.354%, Loss = 2.4788920402526857
Epoch: 271, Batch Gradient Norm: 21.128170327528412
Epoch: 271, Batch Gradient Norm after: 20.955099973010185
Epoch 272/10000, Prediction Accuracy = 41.408%, Loss = 2.470453977584839
Epoch: 272, Batch Gradient Norm: 23.11152733566343
Epoch: 272, Batch Gradient Norm after: 21.940609422789112
Epoch 273/10000, Prediction Accuracy = 41.396%, Loss = 2.4746374130249023
Epoch: 273, Batch Gradient Norm: 21.12309268737363
Epoch: 273, Batch Gradient Norm after: 20.95100654688958
Epoch 274/10000, Prediction Accuracy = 41.465999999999994%, Loss = 2.466156005859375
Epoch: 274, Batch Gradient Norm: 23.105159048037713
Epoch: 274, Batch Gradient Norm after: 21.941211338748374
Epoch 275/10000, Prediction Accuracy = 41.444%, Loss = 2.470337724685669
Epoch: 275, Batch Gradient Norm: 21.103959150991926
Epoch: 275, Batch Gradient Norm after: 20.92559829909543
Epoch 276/10000, Prediction Accuracy = 41.525999999999996%, Loss = 2.4618584632873537
Epoch: 276, Batch Gradient Norm: 23.084593167070608
Epoch: 276, Batch Gradient Norm after: 21.930448433251158
Epoch 277/10000, Prediction Accuracy = 41.510000000000005%, Loss = 2.4660528659820558
Epoch: 277, Batch Gradient Norm: 21.09148033408586
Epoch: 277, Batch Gradient Norm after: 20.90207108512029
Epoch 278/10000, Prediction Accuracy = 41.54600000000001%, Loss = 2.4576606273651125
Epoch: 278, Batch Gradient Norm: 23.088956517471846
Epoch: 278, Batch Gradient Norm after: 21.92912286498576
Epoch 279/10000, Prediction Accuracy = 41.528%, Loss = 2.46188645362854
Epoch: 279, Batch Gradient Norm: 21.1455991610794
Epoch: 279, Batch Gradient Norm after: 20.957867652794302
Epoch 280/10000, Prediction Accuracy = 41.598%, Loss = 2.453691577911377
Epoch: 280, Batch Gradient Norm: 23.138748915963408
Epoch: 280, Batch Gradient Norm after: 21.965912449070945
Epoch 281/10000, Prediction Accuracy = 41.6%, Loss = 2.4579319953918457
Epoch: 281, Batch Gradient Norm: 21.126250872086466
Epoch: 281, Batch Gradient Norm after: 20.91834590397797
Epoch 282/10000, Prediction Accuracy = 41.620000000000005%, Loss = 2.449527931213379
Epoch: 282, Batch Gradient Norm: 23.149896106413777
Epoch: 282, Batch Gradient Norm after: 21.969579190832505
Epoch 283/10000, Prediction Accuracy = 41.622%, Loss = 2.453855562210083
Epoch: 283, Batch Gradient Norm: 21.187007614054675
Epoch: 283, Batch Gradient Norm after: 20.975135841747164
Epoch 284/10000, Prediction Accuracy = 41.644000000000005%, Loss = 2.4456504821777343
Epoch: 284, Batch Gradient Norm: 23.185348219017627
Epoch: 284, Batch Gradient Norm after: 22.003589603244144
Epoch 285/10000, Prediction Accuracy = 41.64%, Loss = 2.4499414920806886
Epoch: 285, Batch Gradient Norm: 21.141653709124455
Epoch: 285, Batch Gradient Norm after: 20.887537285633375
Epoch 286/10000, Prediction Accuracy = 41.666%, Loss = 2.4414696216583254
Epoch: 286, Batch Gradient Norm: 23.18386826888239
Epoch: 286, Batch Gradient Norm after: 21.995092602097586
Epoch 287/10000, Prediction Accuracy = 41.676%, Loss = 2.445903778076172
Epoch: 287, Batch Gradient Norm: 21.20136889105436
Epoch: 287, Batch Gradient Norm after: 20.9487452048863
Epoch 288/10000, Prediction Accuracy = 41.702000000000005%, Loss = 2.4376744747161867
Epoch: 288, Batch Gradient Norm: 23.427091497942754
Epoch: 288, Batch Gradient Norm after: 22.08200162214697
Epoch 289/10000, Prediction Accuracy = 41.702%, Loss = 2.442775774002075
Epoch: 289, Batch Gradient Norm: 21.286050947209233
Epoch: 289, Batch Gradient Norm after: 21.05298606648287
Epoch 290/10000, Prediction Accuracy = 41.751999999999995%, Loss = 2.434004878997803
Epoch: 290, Batch Gradient Norm: 23.369405193933392
Epoch: 290, Batch Gradient Norm after: 22.128673063011348
Epoch 291/10000, Prediction Accuracy = 41.717999999999996%, Loss = 2.4386239051818848
Epoch: 291, Batch Gradient Norm: 21.39204385892951
Epoch: 291, Batch Gradient Norm after: 21.179483352300494
Epoch 292/10000, Prediction Accuracy = 41.78%, Loss = 2.430431842803955
Epoch: 292, Batch Gradient Norm: 23.405626902260874
Epoch: 292, Batch Gradient Norm after: 22.17868018865651
Epoch 293/10000, Prediction Accuracy = 41.760000000000005%, Loss = 2.434867095947266
Epoch: 293, Batch Gradient Norm: 21.283982955941973
Epoch: 293, Batch Gradient Norm after: 21.019691948442166
Epoch 294/10000, Prediction Accuracy = 41.815999999999995%, Loss = 2.426263952255249
Epoch: 294, Batch Gradient Norm: 23.37880283652999
Epoch: 294, Batch Gradient Norm after: 22.146226541650538
Epoch 295/10000, Prediction Accuracy = 41.8%, Loss = 2.430951738357544
Epoch: 295, Batch Gradient Norm: 21.442718246847996
Epoch: 295, Batch Gradient Norm after: 21.21204707446382
Epoch 296/10000, Prediction Accuracy = 41.833999999999996%, Loss = 2.4229267597198487
Epoch: 296, Batch Gradient Norm: 23.4238533411114
Epoch: 296, Batch Gradient Norm after: 22.214520218796924
Epoch 297/10000, Prediction Accuracy = 41.83200000000001%, Loss = 2.427311372756958
Epoch: 297, Batch Gradient Norm: 21.25024381761679
Epoch: 297, Batch Gradient Norm after: 20.93919054046439
Epoch 298/10000, Prediction Accuracy = 41.884%, Loss = 2.4185459136962892
Epoch: 298, Batch Gradient Norm: 23.388677914915693
Epoch: 298, Batch Gradient Norm after: 22.151713486226885
Epoch 299/10000, Prediction Accuracy = 41.86%, Loss = 2.423387956619263
Epoch: 299, Batch Gradient Norm: 21.491700467516317
Epoch: 299, Batch Gradient Norm after: 21.242630996218633
Epoch 300/10000, Prediction Accuracy = 41.924%, Loss = 2.4155595302581787
Epoch: 300, Batch Gradient Norm: 23.456726137012478
Epoch: 300, Batch Gradient Norm after: 22.25226647004201
Epoch 301/10000, Prediction Accuracy = 41.908%, Loss = 2.419894218444824
Epoch: 301, Batch Gradient Norm: 21.240448898532637
Epoch: 301, Batch Gradient Norm after: 20.83362414105566
Epoch 302/10000, Prediction Accuracy = 41.968%, Loss = 2.4110677242279053
Epoch: 302, Batch Gradient Norm: 23.401554343517603
Epoch: 302, Batch Gradient Norm after: 22.150214242146546
Epoch 303/10000, Prediction Accuracy = 41.944%, Loss = 2.4159493446350098
Epoch: 303, Batch Gradient Norm: 21.610289703287997
Epoch: 303, Batch Gradient Norm after: 21.34340625040587
Epoch 304/10000, Prediction Accuracy = 41.982000000000006%, Loss = 2.408522939682007
Epoch: 304, Batch Gradient Norm: 23.51951268314754
Epoch: 304, Batch Gradient Norm after: 22.316308270390365
Epoch 305/10000, Prediction Accuracy = 41.971999999999994%, Loss = 2.4127312183380125
Epoch: 305, Batch Gradient Norm: 21.065791564128855
Epoch: 305, Batch Gradient Norm after: 20.587599611071635
Epoch 306/10000, Prediction Accuracy = 42.076%, Loss = 2.403184747695923
Epoch: 306, Batch Gradient Norm: 23.403178038896446
Epoch: 306, Batch Gradient Norm after: 22.118153513933482
Epoch 307/10000, Prediction Accuracy = 42.00599999999999%, Loss = 2.408615827560425
Epoch: 307, Batch Gradient Norm: 21.75571102456425
Epoch: 307, Batch Gradient Norm after: 21.48579387236266
Epoch 308/10000, Prediction Accuracy = 42.10000000000001%, Loss = 2.401703929901123
Epoch: 308, Batch Gradient Norm: 23.62331936576031
Epoch: 308, Batch Gradient Norm after: 22.360678564316412
Epoch 309/10000, Prediction Accuracy = 42.068000000000005%, Loss = 2.4058117866516113
Epoch: 309, Batch Gradient Norm: 21.139251905625997
Epoch: 309, Batch Gradient Norm after: 20.627012293367102
Epoch 310/10000, Prediction Accuracy = 42.164%, Loss = 2.396182346343994
Epoch: 310, Batch Gradient Norm: 23.457357861656163
Epoch: 310, Batch Gradient Norm after: 22.170498659620893
Epoch 311/10000, Prediction Accuracy = 42.098%, Loss = 2.4015870571136473
Epoch: 311, Batch Gradient Norm: 21.718039441406553
Epoch: 311, Batch Gradient Norm after: 21.36722666950295
Epoch 312/10000, Prediction Accuracy = 42.24400000000001%, Loss = 2.3944087982177735
Epoch: 312, Batch Gradient Norm: 23.669686652902456
Epoch: 312, Batch Gradient Norm after: 22.360677833846907
Epoch 313/10000, Prediction Accuracy = 42.156%, Loss = 2.3987847805023192
Epoch: 313, Batch Gradient Norm: 21.314586781488813
Epoch: 313, Batch Gradient Norm after: 20.802136678157026
Epoch 314/10000, Prediction Accuracy = 42.286%, Loss = 2.3896178245544433
Epoch: 314, Batch Gradient Norm: 23.559382781150436
Epoch: 314, Batch Gradient Norm after: 22.27287667098877
Epoch 315/10000, Prediction Accuracy = 42.206%, Loss = 2.394835948944092
Epoch: 315, Batch Gradient Norm: 21.54818237902829
Epoch: 315, Batch Gradient Norm after: 21.07149848734442
Epoch 316/10000, Prediction Accuracy = 42.321999999999996%, Loss = 2.3868077754974366
Epoch: 316, Batch Gradient Norm: 23.674127814743997
Epoch: 316, Batch Gradient Norm after: 22.360678057603486
Epoch 317/10000, Prediction Accuracy = 42.254%, Loss = 2.3917123317718505
Epoch: 317, Batch Gradient Norm: 21.405065966617187
Epoch: 317, Batch Gradient Norm after: 20.838937199646903
Epoch 318/10000, Prediction Accuracy = 42.388%, Loss = 2.382851219177246
Epoch: 318, Batch Gradient Norm: 23.65368152190303
Epoch: 318, Batch Gradient Norm after: 22.3383154558729
Epoch 319/10000, Prediction Accuracy = 42.328%, Loss = 2.3881062984466555
Epoch: 319, Batch Gradient Norm: 21.51050669242568
Epoch: 319, Batch Gradient Norm after: 20.931550578752994
Epoch 320/10000, Prediction Accuracy = 42.434000000000005%, Loss = 2.3796555042266845
Epoch: 320, Batch Gradient Norm: 23.72631371376369
Epoch: 320, Batch Gradient Norm after: 22.360679652641988
Epoch 321/10000, Prediction Accuracy = 42.37%, Loss = 2.3848105907440185
Epoch: 321, Batch Gradient Norm: 21.620856983288125
Epoch: 321, Batch Gradient Norm after: 21.037602779179437
Epoch 322/10000, Prediction Accuracy = 42.467999999999996%, Loss = 2.376486349105835
Epoch: 322, Batch Gradient Norm: 23.807666284972964
Epoch: 322, Batch Gradient Norm after: 22.36067956791342
Epoch 323/10000, Prediction Accuracy = 42.414%, Loss = 2.3815269947052
Epoch: 323, Batch Gradient Norm: 21.88523303152494
Epoch: 323, Batch Gradient Norm after: 21.369710745458356
Epoch 324/10000, Prediction Accuracy = 42.48800000000001%, Loss = 2.3738174438476562
Epoch: 324, Batch Gradient Norm: 23.935318616247216
Epoch: 324, Batch Gradient Norm after: 22.360676026041507
Epoch 325/10000, Prediction Accuracy = 42.444%, Loss = 2.378438138961792
Epoch: 325, Batch Gradient Norm: 22.262146676990486
Epoch: 325, Batch Gradient Norm after: 21.746608559430218
Epoch 326/10000, Prediction Accuracy = 42.513999999999996%, Loss = 2.3715077877044677
Epoch: 326, Batch Gradient Norm: 23.81977174969947
Epoch: 326, Batch Gradient Norm after: 22.360677694180627
Epoch 327/10000, Prediction Accuracy = 42.482%, Loss = 2.374593639373779
Epoch: 327, Batch Gradient Norm: 21.967771892237636
Epoch: 327, Batch Gradient Norm after: 21.46791206375721
Epoch 328/10000, Prediction Accuracy = 42.562%, Loss = 2.3671123027801513
Epoch: 328, Batch Gradient Norm: 23.951781719295045
Epoch: 328, Batch Gradient Norm after: 22.360678116175375
Epoch 329/10000, Prediction Accuracy = 42.488%, Loss = 2.3715457916259766
Epoch: 329, Batch Gradient Norm: 22.35908400385194
Epoch: 329, Batch Gradient Norm after: 21.831076428621795
Epoch 330/10000, Prediction Accuracy = 42.576%, Loss = 2.364918851852417
Epoch: 330, Batch Gradient Norm: 23.77369668452965
Epoch: 330, Batch Gradient Norm after: 22.360677643880734
Epoch 331/10000, Prediction Accuracy = 42.49%, Loss = 2.3675498008728026
Epoch: 331, Batch Gradient Norm: 21.935977996389962
Epoch: 331, Batch Gradient Norm after: 21.411675474904648
Epoch 332/10000, Prediction Accuracy = 42.586%, Loss = 2.360163450241089
Epoch: 332, Batch Gradient Norm: 23.963304129983563
Epoch: 332, Batch Gradient Norm after: 22.360679592748877
Epoch 333/10000, Prediction Accuracy = 42.524%, Loss = 2.364718770980835
Epoch: 333, Batch Gradient Norm: 22.42045286263265
Epoch: 333, Batch Gradient Norm after: 21.860316653173637
Epoch 334/10000, Prediction Accuracy = 42.602000000000004%, Loss = 2.3582723140716553
Epoch: 334, Batch Gradient Norm: 23.754365555071487
Epoch: 334, Batch Gradient Norm after: 22.36067773505896
Epoch 335/10000, Prediction Accuracy = 42.582%, Loss = 2.3606284618377686
Epoch: 335, Batch Gradient Norm: 21.961350213159083
Epoch: 335, Batch Gradient Norm after: 21.39087591132035
Epoch 336/10000, Prediction Accuracy = 42.656%, Loss = 2.353408145904541
Epoch: 336, Batch Gradient Norm: 23.968802321383322
Epoch: 336, Batch Gradient Norm after: 22.360677175310432
Epoch 337/10000, Prediction Accuracy = 42.62%, Loss = 2.3578632354736326
Epoch: 337, Batch Gradient Norm: 22.538527810179296
Epoch: 337, Batch Gradient Norm after: 21.915939822174025
Epoch 338/10000, Prediction Accuracy = 42.70399999999999%, Loss = 2.3518134117126466
Epoch: 338, Batch Gradient Norm: 23.72933999871597
Epoch: 338, Batch Gradient Norm after: 22.34505253189527
Epoch 339/10000, Prediction Accuracy = 42.644000000000005%, Loss = 2.3537206172943117
Epoch: 339, Batch Gradient Norm: 22.061680080991668
Epoch: 339, Batch Gradient Norm after: 21.42450245610877
Epoch 340/10000, Prediction Accuracy = 42.716%, Loss = 2.346924591064453
Epoch: 340, Batch Gradient Norm: 23.94684610796157
Epoch: 340, Batch Gradient Norm after: 22.360678143141445
Epoch 341/10000, Prediction Accuracy = 42.726%, Loss = 2.351006841659546
Epoch: 341, Batch Gradient Norm: 22.60396193361392
Epoch: 341, Batch Gradient Norm after: 21.913252465761726
Epoch 342/10000, Prediction Accuracy = 42.739999999999995%, Loss = 2.345280170440674
Epoch: 342, Batch Gradient Norm: 23.73051734522105
Epoch: 342, Batch Gradient Norm after: 22.33422804924264
Epoch 343/10000, Prediction Accuracy = 42.75599999999999%, Loss = 2.3469945430755614
Epoch: 343, Batch Gradient Norm: 22.209138749525078
Epoch: 343, Batch Gradient Norm after: 21.515864965294092
Epoch 344/10000, Prediction Accuracy = 42.767999999999994%, Loss = 2.340724802017212
Epoch: 344, Batch Gradient Norm: 23.912340161825945
Epoch: 344, Batch Gradient Norm after: 22.36067849150436
Epoch 345/10000, Prediction Accuracy = 42.786%, Loss = 2.3442280769348143
Epoch: 345, Batch Gradient Norm: 22.597364465664413
Epoch: 345, Batch Gradient Norm after: 21.845680573989064
Epoch 346/10000, Prediction Accuracy = 42.82000000000001%, Loss = 2.338630962371826
Epoch: 346, Batch Gradient Norm: 23.764987945712658
Epoch: 346, Batch Gradient Norm after: 22.349362334377044
Epoch 347/10000, Prediction Accuracy = 42.796%, Loss = 2.3404799461364747
Epoch: 347, Batch Gradient Norm: 22.30753167215697
Epoch: 347, Batch Gradient Norm after: 21.53792831180343
Epoch 348/10000, Prediction Accuracy = 42.846000000000004%, Loss = 2.334428310394287
Epoch: 348, Batch Gradient Norm: 23.90477233355767
Epoch: 348, Batch Gradient Norm after: 22.360678021667365
Epoch 349/10000, Prediction Accuracy = 42.814%, Loss = 2.3376028537750244
Epoch: 349, Batch Gradient Norm: 22.82804566770152
Epoch: 349, Batch Gradient Norm after: 21.936761858786472
Epoch 350/10000, Prediction Accuracy = 42.843999999999994%, Loss = 2.332813358306885
Epoch: 350, Batch Gradient Norm: 23.702809383674992
Epoch: 350, Batch Gradient Norm after: 22.29357233356463
Epoch 351/10000, Prediction Accuracy = 42.858000000000004%, Loss = 2.3336719036102296
Epoch: 351, Batch Gradient Norm: 22.541649085731542
Epoch: 351, Batch Gradient Norm after: 21.672858264468186
Epoch 352/10000, Prediction Accuracy = 42.888%, Loss = 2.328651857376099
Epoch: 352, Batch Gradient Norm: 23.83438790243855
Epoch: 352, Batch Gradient Norm after: 22.36067986986802
Epoch 353/10000, Prediction Accuracy = 42.858%, Loss = 2.3308836936950685
Epoch: 353, Batch Gradient Norm: 22.5533450969185
Epoch: 353, Batch Gradient Norm after: 21.649461712373505
Epoch 354/10000, Prediction Accuracy = 42.896%, Loss = 2.3254623889923094
Epoch: 354, Batch Gradient Norm: 23.846335076796244
Epoch: 354, Batch Gradient Norm after: 22.360677252856952
Epoch 355/10000, Prediction Accuracy = 42.903999999999996%, Loss = 2.3276835441589356
Epoch: 355, Batch Gradient Norm: 22.644909824791974
Epoch: 355, Batch Gradient Norm after: 21.687223875405838
Epoch 356/10000, Prediction Accuracy = 42.922000000000004%, Loss = 2.322512483596802
Epoch: 356, Batch Gradient Norm: 23.82942594073425
Epoch: 356, Batch Gradient Norm after: 22.34077017482549
Epoch 357/10000, Prediction Accuracy = 42.910000000000004%, Loss = 2.3243999004364015
Epoch: 357, Batch Gradient Norm: 22.77673727189688
Epoch: 357, Batch Gradient Norm after: 21.765952615427565
Epoch 358/10000, Prediction Accuracy = 42.928%, Loss = 2.3197030544281008
Epoch: 358, Batch Gradient Norm: 23.798252006055723
Epoch: 358, Batch Gradient Norm after: 22.321905440133822
Epoch 359/10000, Prediction Accuracy = 42.934000000000005%, Loss = 2.321081829071045
Epoch: 359, Batch Gradient Norm: 22.828249360424692
Epoch: 359, Batch Gradient Norm after: 21.775611967094534
Epoch 360/10000, Prediction Accuracy = 42.965999999999994%, Loss = 2.316679334640503
Epoch: 360, Batch Gradient Norm: 23.79952523851107
Epoch: 360, Batch Gradient Norm after: 22.31934526209298
Epoch 361/10000, Prediction Accuracy = 42.948%, Loss = 2.317876625061035
Epoch: 361, Batch Gradient Norm: 22.898129384755205
Epoch: 361, Batch Gradient Norm after: 21.794728012148983
Epoch 362/10000, Prediction Accuracy = 42.992%, Loss = 2.3137230396270754
Epoch: 362, Batch Gradient Norm: 23.79751259399112
Epoch: 362, Batch Gradient Norm after: 22.300233821174626
Epoch 363/10000, Prediction Accuracy = 42.972%, Loss = 2.314696455001831
Epoch: 363, Batch Gradient Norm: 23.04560551779808
Epoch: 363, Batch Gradient Norm after: 21.89434395355885
Epoch 364/10000, Prediction Accuracy = 43.024%, Loss = 2.3110491275787353
Epoch: 364, Batch Gradient Norm: 23.764290559225305
Epoch: 364, Batch Gradient Norm after: 22.24961189782563
Epoch 365/10000, Prediction Accuracy = 43.019999999999996%, Loss = 2.3114327430725097
Epoch: 365, Batch Gradient Norm: 23.29459562777611
Epoch: 365, Batch Gradient Norm after: 22.087780354173535
Epoch 366/10000, Prediction Accuracy = 43.034000000000006%, Loss = 2.308698558807373
Epoch: 366, Batch Gradient Norm: 23.69938592885718
Epoch: 366, Batch Gradient Norm after: 22.159053638030567
Epoch 367/10000, Prediction Accuracy = 43.06%, Loss = 2.3080613136291506
Epoch: 367, Batch Gradient Norm: 23.52392029083242
Epoch: 367, Batch Gradient Norm after: 22.29652865125323
Epoch 368/10000, Prediction Accuracy = 43.046%, Loss = 2.3062996864318848
Epoch: 368, Batch Gradient Norm: 23.65073032484123
Epoch: 368, Batch Gradient Norm after: 22.076076564757873
Epoch 369/10000, Prediction Accuracy = 43.088%, Loss = 2.3047410011291505
Epoch: 369, Batch Gradient Norm: 23.275534619378213
Epoch: 369, Batch Gradient Norm after: 22.12543426702949
Epoch 370/10000, Prediction Accuracy = 43.04%, Loss = 2.302367401123047
Epoch: 370, Batch Gradient Norm: 23.73084616786305
Epoch: 370, Batch Gradient Norm after: 22.169990219433597
Epoch 371/10000, Prediction Accuracy = 43.126%, Loss = 2.301921272277832
Epoch: 371, Batch Gradient Norm: 23.58098965966728
Epoch: 371, Batch Gradient Norm after: 22.315146328849533
Epoch 372/10000, Prediction Accuracy = 43.052%, Loss = 2.300270175933838
Epoch: 372, Batch Gradient Norm: 23.68918308497609
Epoch: 372, Batch Gradient Norm after: 22.110072742699668
Epoch 373/10000, Prediction Accuracy = 43.138%, Loss = 2.298676776885986
Epoch: 373, Batch Gradient Norm: 23.550007845644313
Epoch: 373, Batch Gradient Norm after: 22.269545598263246
Epoch 374/10000, Prediction Accuracy = 43.096%, Loss = 2.2970633506774902
Epoch: 374, Batch Gradient Norm: 23.67221218486697
Epoch: 374, Batch Gradient Norm after: 22.072238584292545
Epoch 375/10000, Prediction Accuracy = 43.166000000000004%, Loss = 2.2955322742462156
Epoch: 375, Batch Gradient Norm: 23.514523859649792
Epoch: 375, Batch Gradient Norm after: 22.231476689555578
Epoch 376/10000, Prediction Accuracy = 43.122%, Loss = 2.293858528137207
Epoch: 376, Batch Gradient Norm: 23.703263142928
Epoch: 376, Batch Gradient Norm after: 22.090078190606633
Epoch 377/10000, Prediction Accuracy = 43.18599999999999%, Loss = 2.2925768375396727
Epoch: 377, Batch Gradient Norm: 23.601576676734112
Epoch: 377, Batch Gradient Norm after: 22.275891409282078
Epoch 378/10000, Prediction Accuracy = 43.162%, Loss = 2.291093873977661
Epoch: 378, Batch Gradient Norm: 23.723522675004588
Epoch: 378, Batch Gradient Norm after: 22.091726703013617
Epoch 379/10000, Prediction Accuracy = 43.22%, Loss = 2.289605951309204
Epoch: 379, Batch Gradient Norm: 23.630807353905794
Epoch: 379, Batch Gradient Norm after: 22.28597717220928
Epoch 380/10000, Prediction Accuracy = 43.209999999999994%, Loss = 2.2881582736968995
Epoch: 380, Batch Gradient Norm: 23.746506579807352
Epoch: 380, Batch Gradient Norm after: 22.105136756974655
Epoch 381/10000, Prediction Accuracy = 43.260000000000005%, Loss = 2.2866557121276854
Epoch: 381, Batch Gradient Norm: 23.670089904617658
Epoch: 381, Batch Gradient Norm after: 22.308512478347453
Epoch 382/10000, Prediction Accuracy = 43.234%, Loss = 2.2852648735046386
Epoch: 382, Batch Gradient Norm: 23.783440254792268
Epoch: 382, Batch Gradient Norm after: 22.13756151261241
Epoch 383/10000, Prediction Accuracy = 43.284%, Loss = 2.283762502670288
Epoch: 383, Batch Gradient Norm: 23.72519632562653
Epoch: 383, Batch Gradient Norm after: 22.34843420067266
Epoch 384/10000, Prediction Accuracy = 43.239999999999995%, Loss = 2.282426452636719
Epoch: 384, Batch Gradient Norm: 23.83757091989338
Epoch: 384, Batch Gradient Norm after: 22.191879032851585
Epoch 385/10000, Prediction Accuracy = 43.294000000000004%, Loss = 2.28096923828125
Epoch: 385, Batch Gradient Norm: 23.803051497994318
Epoch: 385, Batch Gradient Norm after: 22.360676053510357
Epoch 386/10000, Prediction Accuracy = 43.262%, Loss = 2.2797033309936525
Epoch: 386, Batch Gradient Norm: 23.835334737706113
Epoch: 386, Batch Gradient Norm after: 22.173619658283283
Epoch 387/10000, Prediction Accuracy = 43.286%, Loss = 2.277973461151123
Epoch: 387, Batch Gradient Norm: 23.803408315744296
Epoch: 387, Batch Gradient Norm after: 22.360679662807055
Epoch 388/10000, Prediction Accuracy = 43.29600000000001%, Loss = 2.276722240447998
Epoch: 388, Batch Gradient Norm: 23.862263603732373
Epoch: 388, Batch Gradient Norm after: 22.195321676165676
Epoch 389/10000, Prediction Accuracy = 43.32000000000001%, Loss = 2.275102949142456
Epoch: 389, Batch Gradient Norm: 23.854472129776976
Epoch: 389, Batch Gradient Norm after: 22.360679801899664
Epoch 390/10000, Prediction Accuracy = 43.324%, Loss = 2.2739500522613527
Epoch: 390, Batch Gradient Norm: 23.861483513931823
Epoch: 390, Batch Gradient Norm after: 22.17886425108171
Epoch 391/10000, Prediction Accuracy = 43.336%, Loss = 2.272151565551758
Epoch: 391, Batch Gradient Norm: 23.860213073563433
Epoch: 391, Batch Gradient Norm after: 22.360678735834682
Epoch 392/10000, Prediction Accuracy = 43.355999999999995%, Loss = 2.2710244178771974
Epoch: 392, Batch Gradient Norm: 23.88833922108446
Epoch: 392, Batch Gradient Norm after: 22.1968403479943
Epoch 393/10000, Prediction Accuracy = 43.373999999999995%, Loss = 2.2693248748779298
Epoch: 393, Batch Gradient Norm: 23.915132952767898
Epoch: 393, Batch Gradient Norm after: 22.36067774650296
Epoch 394/10000, Prediction Accuracy = 43.384%, Loss = 2.2682966709136965
Epoch: 394, Batch Gradient Norm: 23.878916487609104
Epoch: 394, Batch Gradient Norm after: 22.171994375890577
Epoch 395/10000, Prediction Accuracy = 43.39199999999999%, Loss = 2.2663782596588136
Epoch: 395, Batch Gradient Norm: 23.921015657982462
Epoch: 395, Batch Gradient Norm after: 22.36067778199666
Epoch 396/10000, Prediction Accuracy = 43.416%, Loss = 2.2654133319854735
Epoch: 396, Batch Gradient Norm: 23.916619426436018
Epoch: 396, Batch Gradient Norm after: 22.197025174667417
Epoch 397/10000, Prediction Accuracy = 43.418%, Loss = 2.263599395751953
Epoch: 397, Batch Gradient Norm: 23.975955477765627
Epoch: 397, Batch Gradient Norm after: 22.36067854429101
Epoch 398/10000, Prediction Accuracy = 43.44%, Loss = 2.262689447402954
Epoch: 398, Batch Gradient Norm: 23.916368120363266
Epoch: 398, Batch Gradient Norm after: 22.17784503485625
Epoch 399/10000, Prediction Accuracy = 43.436%, Loss = 2.260695743560791
Epoch: 399, Batch Gradient Norm: 23.985952203193946
Epoch: 399, Batch Gradient Norm after: 22.36067738681118
Epoch 400/10000, Prediction Accuracy = 43.471999999999994%, Loss = 2.2598161697387695
Epoch: 400, Batch Gradient Norm: 23.948101313207154
Epoch: 400, Batch Gradient Norm after: 22.19832919027642
Epoch 401/10000, Prediction Accuracy = 43.486%, Loss = 2.257923221588135
Epoch: 401, Batch Gradient Norm: 24.04913081756942
Epoch: 401, Batch Gradient Norm after: 22.36067632152676
Epoch 402/10000, Prediction Accuracy = 43.476%, Loss = 2.2571367740631105
Epoch: 402, Batch Gradient Norm: 23.944515300124745
Epoch: 402, Batch Gradient Norm after: 22.17494383791966
Epoch 403/10000, Prediction Accuracy = 43.528000000000006%, Loss = 2.2550546169281005
Epoch: 403, Batch Gradient Norm: 24.056972447571304
Epoch: 403, Batch Gradient Norm after: 22.360677892503336
Epoch 404/10000, Prediction Accuracy = 43.486000000000004%, Loss = 2.254265832901001
Epoch: 404, Batch Gradient Norm: 23.98054039931092
Epoch: 404, Batch Gradient Norm after: 22.197897467735586
Epoch 405/10000, Prediction Accuracy = 43.552%, Loss = 2.2523240089416503
Epoch: 405, Batch Gradient Norm: 24.12469018350805
Epoch: 405, Batch Gradient Norm after: 22.360677165631365
Epoch 406/10000, Prediction Accuracy = 43.508%, Loss = 2.2516219139099123
Epoch: 406, Batch Gradient Norm: 23.974526409903472
Epoch: 406, Batch Gradient Norm after: 22.17330753996558
Epoch 407/10000, Prediction Accuracy = 43.58800000000001%, Loss = 2.249436616897583
Epoch: 407, Batch Gradient Norm: 24.143541448420848
Epoch: 407, Batch Gradient Norm after: 22.36067569566137
Epoch 408/10000, Prediction Accuracy = 43.522000000000006%, Loss = 2.248816633224487
Epoch: 408, Batch Gradient Norm: 24.002597966274035
Epoch: 408, Batch Gradient Norm after: 22.186180236852287
Epoch 409/10000, Prediction Accuracy = 43.598%, Loss = 2.2466819286346436
Epoch: 409, Batch Gradient Norm: 24.20426741628872
Epoch: 409, Batch Gradient Norm after: 22.360676970438064
Epoch 410/10000, Prediction Accuracy = 43.534000000000006%, Loss = 2.2461627960205077
Epoch: 410, Batch Gradient Norm: 24.00777568255209
Epoch: 410, Batch Gradient Norm after: 22.1717466779509
Epoch 411/10000, Prediction Accuracy = 43.614%, Loss = 2.243858480453491
Epoch: 411, Batch Gradient Norm: 24.231989583102642
Epoch: 411, Batch Gradient Norm after: 22.36067882084857
Epoch 412/10000, Prediction Accuracy = 43.525999999999996%, Loss = 2.243412208557129
Epoch: 412, Batch Gradient Norm: 24.034775000702666
Epoch: 412, Batch Gradient Norm after: 22.182449532900975
Epoch 413/10000, Prediction Accuracy = 43.644%, Loss = 2.2411128520965575
Epoch: 413, Batch Gradient Norm: 24.29386954068789
Epoch: 413, Batch Gradient Norm after: 22.360676990094227
Epoch 414/10000, Prediction Accuracy = 43.568%, Loss = 2.240784025192261
Epoch: 414, Batch Gradient Norm: 24.044966044652412
Epoch: 414, Batch Gradient Norm after: 22.165173040566383
Epoch 415/10000, Prediction Accuracy = 43.668%, Loss = 2.23830189704895
Epoch: 415, Batch Gradient Norm: 24.309214573092984
Epoch: 415, Batch Gradient Norm after: 22.360676978564303
Epoch 416/10000, Prediction Accuracy = 43.577999999999996%, Loss = 2.237991714477539
Epoch: 416, Batch Gradient Norm: 24.08657295135756
Epoch: 416, Batch Gradient Norm after: 22.188100995019795
Epoch 417/10000, Prediction Accuracy = 43.702%, Loss = 2.2356108665466308
Epoch: 417, Batch Gradient Norm: 24.378829095860052
Epoch: 417, Batch Gradient Norm after: 22.360678549979983
Epoch 418/10000, Prediction Accuracy = 43.574%, Loss = 2.235422945022583
Epoch: 418, Batch Gradient Norm: 24.09328681743146
Epoch: 418, Batch Gradient Norm after: 22.168137178734895
Epoch 419/10000, Prediction Accuracy = 43.712%, Loss = 2.232835578918457
Epoch: 419, Batch Gradient Norm: 24.38701985323542
Epoch: 419, Batch Gradient Norm after: 22.360677651977504
Epoch 420/10000, Prediction Accuracy = 43.626%, Loss = 2.2326536178588867
Epoch: 420, Batch Gradient Norm: 24.137504511541
Epoch: 420, Batch Gradient Norm after: 22.203022112432016
Epoch 421/10000, Prediction Accuracy = 43.732000000000006%, Loss = 2.2302082538604737
Epoch: 421, Batch Gradient Norm: 24.583548282303816
Epoch: 421, Batch Gradient Norm after: 22.35690335749651
Epoch 422/10000, Prediction Accuracy = 43.660000000000004%, Loss = 2.2305412769317625
Epoch: 422, Batch Gradient Norm: 24.151851375776744
Epoch: 422, Batch Gradient Norm after: 22.16529561459323
Epoch 423/10000, Prediction Accuracy = 43.744%, Loss = 2.227495527267456
Epoch: 423, Batch Gradient Norm: 24.470444143816735
Epoch: 423, Batch Gradient Norm after: 22.360678527682293
Epoch 424/10000, Prediction Accuracy = 43.69199999999999%, Loss = 2.227388525009155
Epoch: 424, Batch Gradient Norm: 24.185494951597622
Epoch: 424, Batch Gradient Norm after: 22.20582824740404
Epoch 425/10000, Prediction Accuracy = 43.74999999999999%, Loss = 2.224862241744995
Epoch: 425, Batch Gradient Norm: 24.572588446117468
Epoch: 425, Batch Gradient Norm after: 22.36067899358685
Epoch 426/10000, Prediction Accuracy = 43.75%, Loss = 2.224990653991699
Epoch: 426, Batch Gradient Norm: 24.170462674682227
Epoch: 426, Batch Gradient Norm after: 22.159054741053055
Epoch 427/10000, Prediction Accuracy = 43.75%, Loss = 2.2220417976379396
Epoch: 427, Batch Gradient Norm: 24.544551548823012
Epoch: 427, Batch Gradient Norm after: 22.36067632093491
Epoch 428/10000, Prediction Accuracy = 43.766%, Loss = 2.222137451171875
Epoch: 428, Batch Gradient Norm: 24.242634584275997
Epoch: 428, Batch Gradient Norm after: 22.21848191257315
Epoch 429/10000, Prediction Accuracy = 43.77400000000001%, Loss = 2.2195598125457763
Epoch: 429, Batch Gradient Norm: 24.66213164585453
Epoch: 429, Batch Gradient Norm after: 22.354400517259585
Epoch 430/10000, Prediction Accuracy = 43.782%, Loss = 2.2197986125946043
Epoch: 430, Batch Gradient Norm: 24.213500205336583
Epoch: 430, Batch Gradient Norm after: 22.16055480966411
Epoch 431/10000, Prediction Accuracy = 43.81%, Loss = 2.2167380809783936
Epoch: 431, Batch Gradient Norm: 24.64090713526662
Epoch: 431, Batch Gradient Norm after: 22.360678696019264
Epoch 432/10000, Prediction Accuracy = 43.798%, Loss = 2.216995048522949
Epoch: 432, Batch Gradient Norm: 24.277363681226316
Epoch: 432, Batch Gradient Norm after: 22.208744507211907
Epoch 433/10000, Prediction Accuracy = 43.83%, Loss = 2.214262104034424
Epoch: 433, Batch Gradient Norm: 24.75278461692926
Epoch: 433, Batch Gradient Norm after: 22.360679549781935
Epoch 434/10000, Prediction Accuracy = 43.826%, Loss = 2.2146699905395506
Epoch: 434, Batch Gradient Norm: 24.251503534305055
Epoch: 434, Batch Gradient Norm after: 22.15228565296064
Epoch 435/10000, Prediction Accuracy = 43.846000000000004%, Loss = 2.2114511013031004
Epoch: 435, Batch Gradient Norm: 24.72125120221739
Epoch: 435, Batch Gradient Norm after: 22.360679402727893
Epoch 436/10000, Prediction Accuracy = 43.843999999999994%, Loss = 2.211847496032715
Epoch: 436, Batch Gradient Norm: 24.334994240200366
Epoch: 436, Batch Gradient Norm after: 22.216417239437757
Epoch 437/10000, Prediction Accuracy = 43.844%, Loss = 2.2090364933013915
Epoch: 437, Batch Gradient Norm: 24.843563863624453
Epoch: 437, Batch Gradient Norm after: 22.354382692514076
Epoch 438/10000, Prediction Accuracy = 43.86%, Loss = 2.20955810546875
Epoch: 438, Batch Gradient Norm: 24.303071554418647
Epoch: 438, Batch Gradient Norm after: 22.15495785380929
Epoch 439/10000, Prediction Accuracy = 43.858000000000004%, Loss = 2.2062263965606688
Epoch: 439, Batch Gradient Norm: 24.824520838997522
Epoch: 439, Batch Gradient Norm after: 22.360678243065582
Epoch 440/10000, Prediction Accuracy = 43.872%, Loss = 2.2067903518676757
Epoch: 440, Batch Gradient Norm: 24.37576928133089
Epoch: 440, Batch Gradient Norm after: 22.204601416717107
Epoch 441/10000, Prediction Accuracy = 43.89%, Loss = 2.2038087844848633
Epoch: 441, Batch Gradient Norm: 24.929413585174025
Epoch: 441, Batch Gradient Norm after: 22.360679264776508
Epoch 442/10000, Prediction Accuracy = 43.870000000000005%, Loss = 2.2044707775115966
Epoch: 442, Batch Gradient Norm: 24.357581423381408
Epoch: 442, Batch Gradient Norm after: 22.161583993407366
Epoch 443/10000, Prediction Accuracy = 43.916%, Loss = 2.2010815143585205
Epoch: 443, Batch Gradient Norm: 24.917473327066613
Epoch: 443, Batch Gradient Norm after: 22.360678909197024
Epoch 444/10000, Prediction Accuracy = 43.882000000000005%, Loss = 2.201756811141968
Epoch: 444, Batch Gradient Norm: 24.44056676296006
Epoch: 444, Batch Gradient Norm after: 22.206502003236647
Epoch 445/10000, Prediction Accuracy = 43.907999999999994%, Loss = 2.198700189590454
Epoch: 445, Batch Gradient Norm: 25.0600420519106
Epoch: 445, Batch Gradient Norm after: 22.3486246798577
Epoch 446/10000, Prediction Accuracy = 43.902%, Loss = 2.1995771884918214
Epoch: 446, Batch Gradient Norm: 24.427412092179804
Epoch: 446, Batch Gradient Norm after: 22.162814735836008
Epoch 447/10000, Prediction Accuracy = 43.964%, Loss = 2.195986604690552
Epoch: 447, Batch Gradient Norm: 25.03641059270287
Epoch: 447, Batch Gradient Norm after: 22.36067772998349
Epoch 448/10000, Prediction Accuracy = 43.932%, Loss = 2.196809482574463
Epoch: 448, Batch Gradient Norm: 24.48345388148162
Epoch: 448, Batch Gradient Norm after: 22.201180339956064
Epoch 449/10000, Prediction Accuracy = 43.964000000000006%, Loss = 2.1935175895690917
Epoch: 449, Batch Gradient Norm: 25.12811531203675
Epoch: 449, Batch Gradient Norm after: 22.36067946717633
Epoch 450/10000, Prediction Accuracy = 43.97200000000001%, Loss = 2.1944657802581786
Epoch: 450, Batch Gradient Norm: 24.4708786568911
Epoch: 450, Batch Gradient Norm after: 22.16277324087888
Epoch 451/10000, Prediction Accuracy = 43.986000000000004%, Loss = 2.190797805786133
Epoch: 451, Batch Gradient Norm: 25.116274342263072
Epoch: 451, Batch Gradient Norm after: 22.360678843474375
Epoch 452/10000, Prediction Accuracy = 43.971999999999994%, Loss = 2.1917677879333497
Epoch: 452, Batch Gradient Norm: 24.532801525757282
Epoch: 452, Batch Gradient Norm after: 22.204486097393946
Epoch 453/10000, Prediction Accuracy = 43.982%, Loss = 2.1883766651153564
Epoch: 453, Batch Gradient Norm: 25.222622468251146
Epoch: 453, Batch Gradient Norm after: 22.360677801698788
Epoch 454/10000, Prediction Accuracy = 43.967999999999996%, Loss = 2.1894764423370363
Epoch: 454, Batch Gradient Norm: 24.514605494470114
Epoch: 454, Batch Gradient Norm after: 22.162826577619136
Epoch 455/10000, Prediction Accuracy = 44.028%, Loss = 2.1856804370880125
Epoch: 455, Batch Gradient Norm: 25.210401916355146
Epoch: 455, Batch Gradient Norm after: 22.360678610790732
Epoch 456/10000, Prediction Accuracy = 43.984%, Loss = 2.186807155609131
Epoch: 456, Batch Gradient Norm: 24.57911924676186
Epoch: 456, Batch Gradient Norm after: 22.2031224885319
Epoch 457/10000, Prediction Accuracy = 44.04200000000001%, Loss = 2.183268404006958
Epoch: 457, Batch Gradient Norm: 25.31550333579866
Epoch: 457, Batch Gradient Norm after: 22.36067854811449
Epoch 458/10000, Prediction Accuracy = 44.007999999999996%, Loss = 2.1845506191253663
Epoch: 458, Batch Gradient Norm: 24.565095149709986
Epoch: 458, Batch Gradient Norm after: 22.16028415790191
Epoch 459/10000, Prediction Accuracy = 44.064%, Loss = 2.180585527420044
Epoch: 459, Batch Gradient Norm: 25.29080858390254
Epoch: 459, Batch Gradient Norm after: 22.36067936027201
Epoch 460/10000, Prediction Accuracy = 44.029999999999994%, Loss = 2.1818448066711427
Epoch: 460, Batch Gradient Norm: 24.646597450661783
Epoch: 460, Batch Gradient Norm after: 22.217970550354146
Epoch 461/10000, Prediction Accuracy = 44.08%, Loss = 2.1782649517059327
Epoch: 461, Batch Gradient Norm: 25.424280687689045
Epoch: 461, Batch Gradient Norm after: 22.360678156424576
Epoch 462/10000, Prediction Accuracy = 44.05%, Loss = 2.1796849727630616
Epoch: 462, Batch Gradient Norm: 24.614130090791654
Epoch: 462, Batch Gradient Norm after: 22.156009386595766
Epoch 463/10000, Prediction Accuracy = 44.11%, Loss = 2.1755533695220945
Epoch: 463, Batch Gradient Norm: 25.366460433251515
Epoch: 463, Batch Gradient Norm after: 22.360676003158165
Epoch 464/10000, Prediction Accuracy = 44.077999999999996%, Loss = 2.176904058456421
Epoch: 464, Batch Gradient Norm: 24.72398946157994
Epoch: 464, Batch Gradient Norm after: 22.23384617940212
Epoch 465/10000, Prediction Accuracy = 44.120000000000005%, Loss = 2.173362112045288
Epoch: 465, Batch Gradient Norm: 25.531780816817182
Epoch: 465, Batch Gradient Norm after: 22.360678160857994
Epoch 466/10000, Prediction Accuracy = 44.098%, Loss = 2.174890327453613
Epoch: 466, Batch Gradient Norm: 24.65344493655645
Epoch: 466, Batch Gradient Norm after: 22.144049684923313
Epoch 467/10000, Prediction Accuracy = 44.14%, Loss = 2.1705501079559326
Epoch: 467, Batch Gradient Norm: 25.440721289622374
Epoch: 467, Batch Gradient Norm after: 22.360676948005015
Epoch 468/10000, Prediction Accuracy = 44.132%, Loss = 2.1720041275024413
Epoch: 468, Batch Gradient Norm: 24.785999243419237
Epoch: 468, Batch Gradient Norm after: 22.249705740042643
Epoch 469/10000, Prediction Accuracy = 44.152%, Loss = 2.1684447288513184
Epoch: 469, Batch Gradient Norm: 25.623609768727423
Epoch: 469, Batch Gradient Norm after: 22.35102417651407
Epoch 470/10000, Prediction Accuracy = 44.14%, Loss = 2.1700740814208985
Epoch: 470, Batch Gradient Norm: 24.71973552378219
Epoch: 470, Batch Gradient Norm after: 22.15678444814618
Epoch 471/10000, Prediction Accuracy = 44.18%, Loss = 2.1656518936157227
Epoch: 471, Batch Gradient Norm: 25.54870331536213
Epoch: 471, Batch Gradient Norm after: 22.360678882435966
Epoch 472/10000, Prediction Accuracy = 44.166%, Loss = 2.167262887954712
Epoch: 472, Batch Gradient Norm: 24.835220265589026
Epoch: 472, Batch Gradient Norm after: 22.246023757034543
Epoch 473/10000, Prediction Accuracy = 44.198%, Loss = 2.163506841659546
Epoch: 473, Batch Gradient Norm: 25.764352005420946
Epoch: 473, Batch Gradient Norm after: 22.35937409113434
Epoch 474/10000, Prediction Accuracy = 44.17999999999999%, Loss = 2.1654420375823973
Epoch: 474, Batch Gradient Norm: 24.773181038922438
Epoch: 474, Batch Gradient Norm after: 22.14647103344306
Epoch 475/10000, Prediction Accuracy = 44.209999999999994%, Loss = 2.1607452392578126
Epoch: 475, Batch Gradient Norm: 25.6483065809203
Epoch: 475, Batch Gradient Norm after: 22.360678360519213
Epoch 476/10000, Prediction Accuracy = 44.2%, Loss = 2.1624871253967286
Epoch: 476, Batch Gradient Norm: 24.905615590877883
Epoch: 476, Batch Gradient Norm after: 22.254280759941786
Epoch 477/10000, Prediction Accuracy = 44.239999999999995%, Loss = 2.1586883544921873
Epoch: 477, Batch Gradient Norm: 25.830838887759626
Epoch: 477, Batch Gradient Norm after: 22.350883552355306
Epoch 478/10000, Prediction Accuracy = 44.215999999999994%, Loss = 2.160582160949707
Epoch: 478, Batch Gradient Norm: 24.843734711938332
Epoch: 478, Batch Gradient Norm after: 22.15361003092622
Epoch 479/10000, Prediction Accuracy = 44.254000000000005%, Loss = 2.1559235572814943
Epoch: 479, Batch Gradient Norm: 25.753469292714772
Epoch: 479, Batch Gradient Norm after: 22.360678238471777
Epoch 480/10000, Prediction Accuracy = 44.24%, Loss = 2.157783603668213
Epoch: 480, Batch Gradient Norm: 24.965771625748552
Epoch: 480, Batch Gradient Norm after: 22.248896911623508
Epoch 481/10000, Prediction Accuracy = 44.290000000000006%, Loss = 2.153827667236328
Epoch: 481, Batch Gradient Norm: 25.93989392874809
Epoch: 481, Batch Gradient Norm after: 22.360676640241376
Epoch 482/10000, Prediction Accuracy = 44.251999999999995%, Loss = 2.1559054374694826
Epoch: 482, Batch Gradient Norm: 24.88213333935301
Epoch: 482, Batch Gradient Norm after: 22.144273434270307
Epoch 483/10000, Prediction Accuracy = 44.300000000000004%, Loss = 2.151031494140625
Epoch: 483, Batch Gradient Norm: 25.805690461409316
Epoch: 483, Batch Gradient Norm after: 22.360679306777207
Epoch 484/10000, Prediction Accuracy = 44.248%, Loss = 2.1529332637786864
Epoch: 484, Batch Gradient Norm: 25.04464615271965
Epoch: 484, Batch Gradient Norm after: 22.275071145648404
Epoch 485/10000, Prediction Accuracy = 44.316%, Loss = 2.14910249710083
Epoch: 485, Batch Gradient Norm: 26.026481387988934
Epoch: 485, Batch Gradient Norm after: 22.349613028714575
Epoch 486/10000, Prediction Accuracy = 44.268%, Loss = 2.151189947128296
Epoch: 486, Batch Gradient Norm: 24.941277546731225
Epoch: 486, Batch Gradient Norm after: 22.152138670015017
Epoch 487/10000, Prediction Accuracy = 44.336%, Loss = 2.1462449550628664
Epoch: 487, Batch Gradient Norm: 25.899031724670483
Epoch: 487, Batch Gradient Norm after: 22.36067922900714
Epoch 488/10000, Prediction Accuracy = 44.290000000000006%, Loss = 2.1482500553131105
Epoch: 488, Batch Gradient Norm: 25.095797859717134
Epoch: 488, Batch Gradient Norm after: 22.26894561031629
Epoch 489/10000, Prediction Accuracy = 44.342%, Loss = 2.144305372238159
Epoch: 489, Batch Gradient Norm: 26.129057960132112
Epoch: 489, Batch Gradient Norm after: 22.357817453627863
Epoch 490/10000, Prediction Accuracy = 44.318%, Loss = 2.1465620040893554
Epoch: 490, Batch Gradient Norm: 24.979850033536014
Epoch: 490, Batch Gradient Norm after: 22.14007899641637
Epoch 491/10000, Prediction Accuracy = 44.36%, Loss = 2.1414028644561767
Epoch: 491, Batch Gradient Norm: 25.978917332378185
Epoch: 491, Batch Gradient Norm after: 22.360679958066758
Epoch 492/10000, Prediction Accuracy = 44.344%, Loss = 2.143556022644043
Epoch: 492, Batch Gradient Norm: 25.158839681165222
Epoch: 492, Batch Gradient Norm after: 22.26310787616737
Epoch 493/10000, Prediction Accuracy = 44.384%, Loss = 2.1395495414733885
Epoch: 493, Batch Gradient Norm: 26.201615545616193
Epoch: 493, Batch Gradient Norm after: 22.360676736007317
Epoch 494/10000, Prediction Accuracy = 44.35%, Loss = 2.1418279647827148
Epoch: 494, Batch Gradient Norm: 25.04955702471192
Epoch: 494, Batch Gradient Norm after: 22.155853281529453
Epoch 495/10000, Prediction Accuracy = 44.39%, Loss = 2.136680746078491
Epoch: 495, Batch Gradient Norm: 26.084813975835193
Epoch: 495, Batch Gradient Norm after: 22.360679616725104
Epoch 496/10000, Prediction Accuracy = 44.366%, Loss = 2.138956069946289
Epoch: 496, Batch Gradient Norm: 25.206315291153352
Epoch: 496, Batch Gradient Norm after: 22.26579789295238
Epoch 497/10000, Prediction Accuracy = 44.42%, Loss = 2.134764051437378
Epoch: 497, Batch Gradient Norm: 26.296374094659427
Epoch: 497, Batch Gradient Norm after: 22.36067700932359
Epoch 498/10000, Prediction Accuracy = 44.382%, Loss = 2.1372186183929442
Epoch: 498, Batch Gradient Norm: 25.104385302328346
Epoch: 498, Batch Gradient Norm after: 22.15577606422218
Epoch 499/10000, Prediction Accuracy = 44.42999999999999%, Loss = 2.1319344997406007
Epoch: 499, Batch Gradient Norm: 26.171719806204628
Epoch: 499, Batch Gradient Norm after: 22.360676284720984
Epoch 500/10000, Prediction Accuracy = 44.386%, Loss = 2.134317493438721
Epoch: 500, Batch Gradient Norm: 25.265314415893226
Epoch: 500, Batch Gradient Norm after: 22.261315441526694
Epoch 501/10000, Prediction Accuracy = 44.448%, Loss = 2.130044460296631
Epoch: 501, Batch Gradient Norm: 26.375257235333795
Epoch: 501, Batch Gradient Norm after: 22.360677870913875
Epoch 502/10000, Prediction Accuracy = 44.406%, Loss = 2.1325599193572997
Epoch: 502, Batch Gradient Norm: 25.16849770767346
Epoch: 502, Batch Gradient Norm after: 22.168537027615983
Epoch 503/10000, Prediction Accuracy = 44.466%, Loss = 2.127252721786499
Epoch: 503, Batch Gradient Norm: 26.288976996677732
Epoch: 503, Batch Gradient Norm after: 22.36067911603661
Epoch 504/10000, Prediction Accuracy = 44.436%, Loss = 2.129786491394043
Epoch: 504, Batch Gradient Norm: 25.30749979039039
Epoch: 504, Batch Gradient Norm after: 22.26412807084879
Epoch 505/10000, Prediction Accuracy = 44.478%, Loss = 2.1252737998962403
Epoch: 505, Batch Gradient Norm: 26.50249938457041
Epoch: 505, Batch Gradient Norm after: 22.360679412118188
Epoch 506/10000, Prediction Accuracy = 44.455999999999996%, Loss = 2.1280604362487794
Epoch: 506, Batch Gradient Norm: 25.20782639483943
Epoch: 506, Batch Gradient Norm after: 22.15299330412045
Epoch 507/10000, Prediction Accuracy = 44.494%, Loss = 2.122469902038574
Epoch: 507, Batch Gradient Norm: 26.367819379797925
Epoch: 507, Batch Gradient Norm after: 22.360677944761832
Epoch 508/10000, Prediction Accuracy = 44.465999999999994%, Loss = 2.12514066696167
Epoch: 508, Batch Gradient Norm: 25.387699334940386
Epoch: 508, Batch Gradient Norm after: 22.25717661740415
Epoch 509/10000, Prediction Accuracy = 44.49999999999999%, Loss = 2.1206412315368652
Epoch: 509, Batch Gradient Norm: 26.557108999525944
Epoch: 509, Batch Gradient Norm after: 22.36067830970544
Epoch 510/10000, Prediction Accuracy = 44.480000000000004%, Loss = 2.1233582496643066
Epoch: 510, Batch Gradient Norm: 25.30251719623637
Epoch: 510, Batch Gradient Norm after: 22.185846308422715
Epoch 511/10000, Prediction Accuracy = 44.508%, Loss = 2.117909383773804
Epoch: 511, Batch Gradient Norm: 26.518445878211597
Epoch: 511, Batch Gradient Norm after: 22.360676904538646
Epoch 512/10000, Prediction Accuracy = 44.508%, Loss = 2.120789384841919
Epoch: 512, Batch Gradient Norm: 25.403538233167684
Epoch: 512, Batch Gradient Norm after: 22.250787132361427
Epoch 513/10000, Prediction Accuracy = 44.523999999999994%, Loss = 2.1158323764801024
Epoch: 513, Batch Gradient Norm: 26.683361777968646
Epoch: 513, Batch Gradient Norm after: 22.36068090882481
Epoch 514/10000, Prediction Accuracy = 44.558%, Loss = 2.118927478790283
Epoch: 514, Batch Gradient Norm: 25.343900817058785
Epoch: 514, Batch Gradient Norm after: 22.17125949360012
Epoch 515/10000, Prediction Accuracy = 44.534%, Loss = 2.1131893157958985
Epoch: 515, Batch Gradient Norm: 26.588899298892628
Epoch: 515, Batch Gradient Norm after: 22.360681081194016
Epoch 516/10000, Prediction Accuracy = 44.58800000000001%, Loss = 2.1161572456359865
Epoch: 516, Batch Gradient Norm: 25.48865338314923
Epoch: 516, Batch Gradient Norm after: 22.25843875025955
Epoch 517/10000, Prediction Accuracy = 44.532%, Loss = 2.111284351348877
Epoch: 517, Batch Gradient Norm: 26.772194140026247
Epoch: 517, Batch Gradient Norm after: 22.36067807298107
Epoch 518/10000, Prediction Accuracy = 44.588%, Loss = 2.1143673419952393
Epoch: 518, Batch Gradient Norm: 25.4080707870283
Epoch: 518, Batch Gradient Norm after: 22.178075317406485
Epoch 519/10000, Prediction Accuracy = 44.568%, Loss = 2.1086018085479736
Epoch: 519, Batch Gradient Norm: 26.70138413529721
Epoch: 519, Batch Gradient Norm after: 22.360679345687622
Epoch 520/10000, Prediction Accuracy = 44.604%, Loss = 2.1117008209228514
Epoch: 520, Batch Gradient Norm: 25.535554311953785
Epoch: 520, Batch Gradient Norm after: 22.259893526797562
Epoch 521/10000, Prediction Accuracy = 44.589999999999996%, Loss = 2.1066566467285157
Epoch: 521, Batch Gradient Norm: 26.89280350975637
Epoch: 521, Batch Gradient Norm after: 22.36067928909579
Epoch 522/10000, Prediction Accuracy = 44.620000000000005%, Loss = 2.1099729537963867
Epoch: 522, Batch Gradient Norm: 25.45138114376426
Epoch: 522, Batch Gradient Norm after: 22.167169895094016
Epoch 523/10000, Prediction Accuracy = 44.61%, Loss = 2.1039753913879395
Epoch: 523, Batch Gradient Norm: 26.776105711008093
Epoch: 523, Batch Gradient Norm after: 22.360679612126745
Epoch 524/10000, Prediction Accuracy = 44.624%, Loss = 2.1071900367736816
Epoch: 524, Batch Gradient Norm: 25.611868063611855
Epoch: 524, Batch Gradient Norm after: 22.253559780413475
Epoch 525/10000, Prediction Accuracy = 44.632%, Loss = 2.102164793014526
Epoch: 525, Batch Gradient Norm: 26.941555865415637
Epoch: 525, Batch Gradient Norm after: 22.360677076282958
Epoch 526/10000, Prediction Accuracy = 44.632%, Loss = 2.1053912162780763
Epoch: 526, Batch Gradient Norm: 25.546384699578883
Epoch: 526, Batch Gradient Norm after: 22.199639466537658
Epoch 527/10000, Prediction Accuracy = 44.64%, Loss = 2.0995659828186035
Epoch: 527, Batch Gradient Norm: 26.92712887487621
Epoch: 527, Batch Gradient Norm after: 22.360678875366666
Epoch 528/10000, Prediction Accuracy = 44.644000000000005%, Loss = 2.1029646396636963
Epoch: 528, Batch Gradient Norm: 25.622524899730603
Epoch: 528, Batch Gradient Norm after: 22.241426318791618
Epoch 529/10000, Prediction Accuracy = 44.669999999999995%, Loss = 2.0974680900573732
Epoch: 529, Batch Gradient Norm: 27.05813860430879
Epoch: 529, Batch Gradient Norm after: 22.36067807516261
Epoch 530/10000, Prediction Accuracy = 44.666%, Loss = 2.101057529449463
Epoch: 530, Batch Gradient Norm: 25.591106249583614
Epoch: 530, Batch Gradient Norm after: 22.186370197185113
Epoch 531/10000, Prediction Accuracy = 44.684000000000005%, Loss = 2.0949889183044434
Epoch: 531, Batch Gradient Norm: 27.01095714490926
Epoch: 531, Batch Gradient Norm after: 22.36067765297321
Epoch 532/10000, Prediction Accuracy = 44.678%, Loss = 2.0985111713409426
Epoch: 532, Batch Gradient Norm: 25.690945450583193
Epoch: 532, Batch Gradient Norm after: 22.252192212217675
Epoch 533/10000, Prediction Accuracy = 44.684%, Loss = 2.093000316619873
Epoch: 533, Batch Gradient Norm: 27.180434880708923
Epoch: 533, Batch Gradient Norm after: 22.360679406492164
Epoch 534/10000, Prediction Accuracy = 44.692%, Loss = 2.096739339828491
Epoch: 534, Batch Gradient Norm: 25.62464992503892
Epoch: 534, Batch Gradient Norm after: 22.17089662340471
Epoch 535/10000, Prediction Accuracy = 44.67399999999999%, Loss = 2.0904120922088625
Epoch: 535, Batch Gradient Norm: 27.06951964427647
Epoch: 535, Batch Gradient Norm after: 22.360678403908818
Epoch 536/10000, Prediction Accuracy = 44.716%, Loss = 2.094014024734497
Epoch: 536, Batch Gradient Norm: 25.783712722868113
Epoch: 536, Batch Gradient Norm after: 22.24761196322741
Epoch 537/10000, Prediction Accuracy = 44.690000000000005%, Loss = 2.0886229515075683
Epoch: 537, Batch Gradient Norm: 27.21552764039184
Epoch: 537, Batch Gradient Norm after: 22.360676954849325
Epoch 538/10000, Prediction Accuracy = 44.726%, Loss = 2.0921772003173826
Epoch: 538, Batch Gradient Norm: 25.73293282640298
Epoch: 538, Batch Gradient Norm after: 22.21177320125376
Epoch 539/10000, Prediction Accuracy = 44.722%, Loss = 2.086103630065918
Epoch: 539, Batch Gradient Norm: 27.236295831975742
Epoch: 539, Batch Gradient Norm after: 22.3606786887971
Epoch 540/10000, Prediction Accuracy = 44.738%, Loss = 2.0899133682250977
Epoch: 540, Batch Gradient Norm: 25.78641827141239
Epoch: 540, Batch Gradient Norm after: 22.232233340901978
Epoch 541/10000, Prediction Accuracy = 44.728%, Loss = 2.083963632583618
Epoch: 541, Batch Gradient Norm: 27.32667991847928
Epoch: 541, Batch Gradient Norm after: 22.360678093898795
Epoch 542/10000, Prediction Accuracy = 44.742000000000004%, Loss = 2.0878808975219725
Epoch: 542, Batch Gradient Norm: 25.78012257747581
Epoch: 542, Batch Gradient Norm after: 22.204954494541425
Epoch 543/10000, Prediction Accuracy = 44.724000000000004%, Loss = 2.081616830825806
Epoch: 543, Batch Gradient Norm: 27.324520343401616
Epoch: 543, Batch Gradient Norm after: 22.36067838628199
Epoch 544/10000, Prediction Accuracy = 44.739999999999995%, Loss = 2.0855399131774903
Epoch: 544, Batch Gradient Norm: 25.849587930656444
Epoch: 544, Batch Gradient Norm after: 22.237619422283768
Epoch 545/10000, Prediction Accuracy = 44.714000000000006%, Loss = 2.079527711868286
Epoch: 545, Batch Gradient Norm: 27.433448160616425
Epoch: 545, Batch Gradient Norm after: 22.36067616013197
Epoch 546/10000, Prediction Accuracy = 44.751999999999995%, Loss = 2.0835790157318117
Epoch: 546, Batch Gradient Norm: 25.83630419494631
Epoch: 546, Batch Gradient Norm after: 22.200282838155378
Epoch 547/10000, Prediction Accuracy = 44.727999999999994%, Loss = 2.077145147323608
Epoch: 547, Batch Gradient Norm: 27.40366979500415
Epoch: 547, Batch Gradient Norm after: 22.360677799452418
Epoch 548/10000, Prediction Accuracy = 44.758%, Loss = 2.081149864196777
Epoch: 548, Batch Gradient Norm: 25.934320124795576
Epoch: 548, Batch Gradient Norm after: 22.2544425236378
Epoch 549/10000, Prediction Accuracy = 44.73799999999999%, Loss = 2.075152635574341
Epoch: 549, Batch Gradient Norm: 27.54896352959649
Epoch: 549, Batch Gradient Norm after: 22.36067558409684
Epoch 550/10000, Prediction Accuracy = 44.774%, Loss = 2.0793370246887206
Epoch: 550, Batch Gradient Norm: 25.88425838868548
Epoch: 550, Batch Gradient Norm after: 22.191091171921208
Epoch 551/10000, Prediction Accuracy = 44.751999999999995%, Loss = 2.07265305519104
Epoch: 551, Batch Gradient Norm: 27.48352526555663
Epoch: 551, Batch Gradient Norm after: 22.360678545190925
Epoch 552/10000, Prediction Accuracy = 44.775999999999996%, Loss = 2.076792097091675
Epoch: 552, Batch Gradient Norm: 26.009297882487683
Epoch: 552, Batch Gradient Norm after: 22.2491153910722
Epoch 553/10000, Prediction Accuracy = 44.754%, Loss = 2.070766496658325
Epoch: 553, Batch Gradient Norm: 27.624944830351527
Epoch: 553, Batch Gradient Norm after: 22.360678612441962
Epoch 554/10000, Prediction Accuracy = 44.784000000000006%, Loss = 2.074961757659912
Epoch: 554, Batch Gradient Norm: 25.95791857362437
Epoch: 554, Batch Gradient Norm after: 22.202972600240013
Epoch 555/10000, Prediction Accuracy = 44.762%, Loss = 2.068267011642456
Epoch: 555, Batch Gradient Norm: 27.615815466004108
Epoch: 555, Batch Gradient Norm after: 22.360678774967337
Epoch 556/10000, Prediction Accuracy = 44.79599999999999%, Loss = 2.0725904941558837
Epoch: 556, Batch Gradient Norm: 26.0416978949187
Epoch: 556, Batch Gradient Norm after: 22.24319974499867
Epoch 557/10000, Prediction Accuracy = 44.76800000000001%, Loss = 2.0662373542785644
Epoch: 557, Batch Gradient Norm: 27.74543130249721
Epoch: 557, Batch Gradient Norm after: 22.360675340067065
Epoch 558/10000, Prediction Accuracy = 44.797999999999995%, Loss = 2.070729207992554
Epoch: 558, Batch Gradient Norm: 26.009646819451874
Epoch: 558, Batch Gradient Norm after: 22.19411969477081
Epoch 559/10000, Prediction Accuracy = 44.79%, Loss = 2.0638166427612306
Epoch: 559, Batch Gradient Norm: 27.692899685215654
Epoch: 559, Batch Gradient Norm after: 22.360678736890225
Epoch 560/10000, Prediction Accuracy = 44.796%, Loss = 2.068226623535156
Epoch: 560, Batch Gradient Norm: 26.125508139861736
Epoch: 560, Batch Gradient Norm after: 22.248456259558708
Epoch 561/10000, Prediction Accuracy = 44.802%, Loss = 2.061913585662842
Epoch: 561, Batch Gradient Norm: 27.829099878052443
Epoch: 561, Batch Gradient Norm after: 22.360677509547692
Epoch 562/10000, Prediction Accuracy = 44.790000000000006%, Loss = 2.0663944244384767
Epoch: 562, Batch Gradient Norm: 26.09188565500413
Epoch: 562, Batch Gradient Norm after: 22.20826199158179
Epoch 563/10000, Prediction Accuracy = 44.82%, Loss = 2.059481477737427
Epoch: 563, Batch Gradient Norm: 27.822061961079402
Epoch: 563, Batch Gradient Norm after: 22.360677521061003
Epoch 564/10000, Prediction Accuracy = 44.812%, Loss = 2.0640456676483154
Epoch: 564, Batch Gradient Norm: 26.167578394400092
Epoch: 564, Batch Gradient Norm after: 22.24261373759068
Epoch 565/10000, Prediction Accuracy = 44.834%, Loss = 2.0574547290802
Epoch: 565, Batch Gradient Norm: 27.947583714082324
Epoch: 565, Batch Gradient Norm after: 22.36067619881708
Epoch 566/10000, Prediction Accuracy = 44.83%, Loss = 2.06217246055603
Epoch: 566, Batch Gradient Norm: 26.140526250505292
Epoch: 566, Batch Gradient Norm after: 22.195193680165513
Epoch 567/10000, Prediction Accuracy = 44.824%, Loss = 2.055066728591919
Epoch: 567, Batch Gradient Norm: 27.890702661861333
Epoch: 567, Batch Gradient Norm after: 22.360675326192396
Epoch 568/10000, Prediction Accuracy = 44.846000000000004%, Loss = 2.059702968597412
Epoch: 568, Batch Gradient Norm: 26.260065932271175
Epoch: 568, Batch Gradient Norm after: 22.245384818103116
Epoch 569/10000, Prediction Accuracy = 44.842%, Loss = 2.053188133239746
Epoch: 569, Batch Gradient Norm: 28.01142164680177
Epoch: 569, Batch Gradient Norm after: 22.360674722178643
Epoch 570/10000, Prediction Accuracy = 44.858%, Loss = 2.0578330516815186
Epoch: 570, Batch Gradient Norm: 26.233070079323383
Epoch: 570, Batch Gradient Norm after: 22.220018549241274
Epoch 571/10000, Prediction Accuracy = 44.848%, Loss = 2.050805425643921
Epoch: 571, Batch Gradient Norm: 28.030690198140675
Epoch: 571, Batch Gradient Norm after: 22.360676068381856
Epoch 572/10000, Prediction Accuracy = 44.85600000000001%, Loss = 2.0556072473526
Epoch: 572, Batch Gradient Norm: 26.293646444229577
Epoch: 572, Batch Gradient Norm after: 22.240815357544914
Epoch 573/10000, Prediction Accuracy = 44.864%, Loss = 2.0487411499023436
Epoch: 573, Batch Gradient Norm: 28.12985664888161
Epoch: 573, Batch Gradient Norm after: 22.36067847525
Epoch 574/10000, Prediction Accuracy = 44.854%, Loss = 2.053670072555542
Epoch: 574, Batch Gradient Norm: 26.281910809248465
Epoch: 574, Batch Gradient Norm after: 22.208833138038475
Epoch 575/10000, Prediction Accuracy = 44.86%, Loss = 2.0464261054992674
Epoch: 575, Batch Gradient Norm: 28.107382750301557
Epoch: 575, Batch Gradient Norm after: 22.36067845328791
Epoch 576/10000, Prediction Accuracy = 44.866%, Loss = 2.051323080062866
Epoch: 576, Batch Gradient Norm: 26.376003189459972
Epoch: 576, Batch Gradient Norm after: 22.245901117925786
Epoch 577/10000, Prediction Accuracy = 44.873999999999995%, Loss = 2.044487190246582
Epoch: 577, Batch Gradient Norm: 28.217169952131563
Epoch: 577, Batch Gradient Norm after: 22.36067967433761
Epoch 578/10000, Prediction Accuracy = 44.858000000000004%, Loss = 2.04943425655365
Epoch: 578, Batch Gradient Norm: 26.3559145528779
Epoch: 578, Batch Gradient Norm after: 22.216847048399124
Epoch 579/10000, Prediction Accuracy = 44.86999999999999%, Loss = 2.0421568393707275
Epoch: 579, Batch Gradient Norm: 28.225087744292047
Epoch: 579, Batch Gradient Norm after: 22.360679435494482
Epoch 580/10000, Prediction Accuracy = 44.874%, Loss = 2.047193145751953
Epoch: 580, Batch Gradient Norm: 26.42514494652109
Epoch: 580, Batch Gradient Norm after: 22.2448767246806
Epoch 581/10000, Prediction Accuracy = 44.878%, Loss = 2.0401439666748047
Epoch: 581, Batch Gradient Norm: 28.328469701950173
Epoch: 581, Batch Gradient Norm after: 22.360678552144424
Epoch 582/10000, Prediction Accuracy = 44.888%, Loss = 2.0452858209609985
Epoch: 582, Batch Gradient Norm: 26.416274338993993
Epoch: 582, Batch Gradient Norm after: 22.214726534034316
Epoch 583/10000, Prediction Accuracy = 44.89%, Loss = 2.037852334976196
Epoch: 583, Batch Gradient Norm: 28.316610883256146
Epoch: 583, Batch Gradient Norm after: 22.36067641958993
Epoch 584/10000, Prediction Accuracy = 44.924%, Loss = 2.0429980039596556
Epoch: 584, Batch Gradient Norm: 26.501323498644588
Epoch: 584, Batch Gradient Norm after: 22.243841156398613
Epoch 585/10000, Prediction Accuracy = 44.891999999999996%, Loss = 2.035891580581665
Epoch: 585, Batch Gradient Norm: 28.41951575560228
Epoch: 585, Batch Gradient Norm after: 22.360678064682432
Epoch 586/10000, Prediction Accuracy = 44.942%, Loss = 2.0411062479019164
Epoch: 586, Batch Gradient Norm: 26.488297583137165
Epoch: 586, Batch Gradient Norm after: 22.22006820098785
Epoch 587/10000, Prediction Accuracy = 44.914%, Loss = 2.0336069583892824
Epoch: 587, Batch Gradient Norm: 28.422823353840666
Epoch: 587, Batch Gradient Norm after: 22.36067591602745
Epoch 588/10000, Prediction Accuracy = 44.936%, Loss = 2.0388832330703734
Epoch: 588, Batch Gradient Norm: 26.56561232945899
Epoch: 588, Batch Gradient Norm after: 22.247219431843632
Epoch 589/10000, Prediction Accuracy = 44.916%, Loss = 2.0316341400146483
Epoch: 589, Batch Gradient Norm: 28.51719205699099
Epoch: 589, Batch Gradient Norm after: 22.360676012782644
Epoch 590/10000, Prediction Accuracy = 44.94%, Loss = 2.036985754966736
Epoch: 590, Batch Gradient Norm: 26.557364224375558
Epoch: 590, Batch Gradient Norm after: 22.22162663124893
Epoch 591/10000, Prediction Accuracy = 44.928%, Loss = 2.029382109642029
Epoch: 591, Batch Gradient Norm: 28.519184784181537
Epoch: 591, Batch Gradient Norm after: 22.3606786546347
Epoch 592/10000, Prediction Accuracy = 44.942%, Loss = 2.034748911857605
Epoch: 592, Batch Gradient Norm: 26.629584620154645
Epoch: 592, Batch Gradient Norm after: 22.249055834353012
Epoch 593/10000, Prediction Accuracy = 44.952%, Loss = 2.0274116516113283
Epoch: 593, Batch Gradient Norm: 28.61572282031047
Epoch: 593, Batch Gradient Norm after: 22.360680263617954
Epoch 594/10000, Prediction Accuracy = 44.952%, Loss = 2.032869029045105
Epoch: 594, Batch Gradient Norm: 26.62395289197776
Epoch: 594, Batch Gradient Norm after: 22.22356030252785
Epoch 595/10000, Prediction Accuracy = 44.96%, Loss = 2.0251691579818725
Epoch: 595, Batch Gradient Norm: 28.60822255828026
Epoch: 595, Batch Gradient Norm after: 22.360677263555054
Epoch 596/10000, Prediction Accuracy = 44.955999999999996%, Loss = 2.030623984336853
Epoch: 596, Batch Gradient Norm: 26.7020040901099
Epoch: 596, Batch Gradient Norm after: 22.245286959423428
Epoch 597/10000, Prediction Accuracy = 44.955999999999996%, Loss = 2.0232343673706055
Epoch: 597, Batch Gradient Norm: 28.68742136653645
Epoch: 597, Batch Gradient Norm after: 22.36067763692385
Epoch 598/10000, Prediction Accuracy = 44.97%, Loss = 2.028687000274658
Epoch: 598, Batch Gradient Norm: 26.70648604884675
Epoch: 598, Batch Gradient Norm after: 22.240625031886328
Epoch 599/10000, Prediction Accuracy = 44.964%, Loss = 2.021037721633911
Epoch: 599, Batch Gradient Norm: 28.74220960666481
Epoch: 599, Batch Gradient Norm after: 22.36067885650947
Epoch 600/10000, Prediction Accuracy = 44.980000000000004%, Loss = 2.0266590118408203
Epoch: 600, Batch Gradient Norm: 26.734501156731817
Epoch: 600, Batch Gradient Norm after: 22.23852196806983
Epoch 601/10000, Prediction Accuracy = 44.972%, Loss = 2.0189360857009886
Epoch: 601, Batch Gradient Norm: 28.786442350933527
Epoch: 601, Batch Gradient Norm after: 22.36067892730393
Epoch 602/10000, Prediction Accuracy = 44.97600000000001%, Loss = 2.024596071243286
Epoch: 602, Batch Gradient Norm: 26.771286924663166
Epoch: 602, Batch Gradient Norm after: 22.24334864200456
Epoch 603/10000, Prediction Accuracy = 44.974%, Loss = 2.0168619394302367
Epoch: 603, Batch Gradient Norm: 28.84286216150164
Epoch: 603, Batch Gradient Norm after: 22.360677231441947
Epoch 604/10000, Prediction Accuracy = 44.992000000000004%, Loss = 2.0225833892822265
Epoch: 604, Batch Gradient Norm: 26.795984054098916
Epoch: 604, Batch Gradient Norm after: 22.23849423052515
Epoch 605/10000, Prediction Accuracy = 44.996%, Loss = 2.0147624254226684
Epoch: 605, Batch Gradient Norm: 28.872256267091302
Epoch: 605, Batch Gradient Norm after: 22.36067730233393
Epoch 606/10000, Prediction Accuracy = 45.008%, Loss = 2.020487380027771
Epoch: 606, Batch Gradient Norm: 26.845116119857373
Epoch: 606, Batch Gradient Norm after: 22.25123415657529
Epoch 607/10000, Prediction Accuracy = 44.992%, Loss = 2.0127479076385497
Epoch: 607, Batch Gradient Norm: 28.947781193845266
Epoch: 607, Batch Gradient Norm after: 22.36067548705026
Epoch 608/10000, Prediction Accuracy = 45.029999999999994%, Loss = 2.018546724319458
Epoch: 608, Batch Gradient Norm: 26.85320728751512
Epoch: 608, Batch Gradient Norm after: 22.235125381013564
Epoch 609/10000, Prediction Accuracy = 45.007999999999996%, Loss = 2.0105865001678467
Epoch: 609, Batch Gradient Norm: 28.95559030494985
Epoch: 609, Batch Gradient Norm after: 22.36067845916946
Epoch 610/10000, Prediction Accuracy = 45.024%, Loss = 2.0163806915283202
Epoch: 610, Batch Gradient Norm: 26.92394265379787
Epoch: 610, Batch Gradient Norm after: 22.260858505353763
Epoch 611/10000, Prediction Accuracy = 45.012%, Loss = 2.0086486101150514
Epoch: 611, Batch Gradient Norm: 29.06174304364974
Epoch: 611, Batch Gradient Norm after: 22.360676825604592
Epoch 612/10000, Prediction Accuracy = 45.029999999999994%, Loss = 2.0145512580871583
Epoch: 612, Batch Gradient Norm: 26.905432306200076
Epoch: 612, Batch Gradient Norm after: 22.22703347068504
Epoch 613/10000, Prediction Accuracy = 45.036%, Loss = 2.006406378746033
Epoch: 613, Batch Gradient Norm: 29.031555204295397
Epoch: 613, Batch Gradient Norm after: 22.36067763162885
Epoch 614/10000, Prediction Accuracy = 45.044%, Loss = 2.012255907058716
Epoch: 614, Batch Gradient Norm: 27.01604137865783
Epoch: 614, Batch Gradient Norm after: 22.25466145518581
Epoch 615/10000, Prediction Accuracy = 45.044%, Loss = 2.004603409767151
Epoch: 615, Batch Gradient Norm: 29.10810760243592
Epoch: 615, Batch Gradient Norm after: 22.360676931546468
Epoch 616/10000, Prediction Accuracy = 45.054%, Loss = 2.0103394746780396
Epoch: 616, Batch Gradient Norm: 27.02319023016621
Epoch: 616, Batch Gradient Norm after: 22.26284823271753
Epoch 617/10000, Prediction Accuracy = 45.054%, Loss = 2.0024574518203737
Epoch: 617, Batch Gradient Norm: 29.208676134505094
Epoch: 617, Batch Gradient Norm after: 22.360678987387328
Epoch 618/10000, Prediction Accuracy = 45.054%, Loss = 2.008516240119934
Epoch: 618, Batch Gradient Norm: 27.015200125544062
Epoch: 618, Batch Gradient Norm after: 22.23718827554532
Epoch 619/10000, Prediction Accuracy = 45.062%, Loss = 2.0002639293670654
Epoch: 619, Batch Gradient Norm: 29.19528747844122
Epoch: 619, Batch Gradient Norm after: 22.360678795187688
Epoch 620/10000, Prediction Accuracy = 45.048%, Loss = 2.006319189071655
Epoch: 620, Batch Gradient Norm: 27.111830790194656
Epoch: 620, Batch Gradient Norm after: 22.252341227275206
Epoch 621/10000, Prediction Accuracy = 45.077999999999996%, Loss = 1.9984247922897338
Epoch: 621, Batch Gradient Norm: 29.233014667107636
Epoch: 621, Batch Gradient Norm after: 22.360678310346515
Epoch 622/10000, Prediction Accuracy = 45.065999999999995%, Loss = 2.0043065309524537
Epoch: 622, Batch Gradient Norm: 27.154481779520484
Epoch: 622, Batch Gradient Norm after: 22.250737938033524
Epoch 623/10000, Prediction Accuracy = 45.077999999999996%, Loss = 1.9964225530624389
Epoch: 623, Batch Gradient Norm: 29.258626712126194
Epoch: 623, Batch Gradient Norm after: 22.360680778535535
Epoch 624/10000, Prediction Accuracy = 45.05400000000001%, Loss = 2.002230167388916
Epoch: 624, Batch Gradient Norm: 27.209841252321564
Epoch: 624, Batch Gradient Norm after: 22.243860631368666
Epoch 625/10000, Prediction Accuracy = 45.084%, Loss = 1.9944660186767578
Epoch: 625, Batch Gradient Norm: 29.2473148992693
Epoch: 625, Batch Gradient Norm after: 22.360680443107196
Epoch 626/10000, Prediction Accuracy = 45.065999999999995%, Loss = 2.0000386238098145
Epoch: 626, Batch Gradient Norm: 27.30740803212624
Epoch: 626, Batch Gradient Norm after: 22.220518560482237
Epoch 627/10000, Prediction Accuracy = 45.089999999999996%, Loss = 1.9926520586013794
Epoch: 627, Batch Gradient Norm: 29.14197996209952
Epoch: 627, Batch Gradient Norm after: 22.360679498281307
Epoch 628/10000, Prediction Accuracy = 45.082%, Loss = 1.9975172519683837
Epoch: 628, Batch Gradient Norm: 27.386433299709516
Epoch: 628, Batch Gradient Norm after: 22.29311364043143
Epoch 629/10000, Prediction Accuracy = 45.088%, Loss = 1.9907591104507447
Epoch: 629, Batch Gradient Norm: 29.325142692073747
Epoch: 629, Batch Gradient Norm after: 22.360679690024
Epoch 630/10000, Prediction Accuracy = 45.092%, Loss = 1.9960055351257324
Epoch: 630, Batch Gradient Norm: 27.39075884183515
Epoch: 630, Batch Gradient Norm after: 22.221578450481736
Epoch 631/10000, Prediction Accuracy = 45.104000000000006%, Loss = 1.98867347240448
Epoch: 631, Batch Gradient Norm: 29.19743853481342
Epoch: 631, Batch Gradient Norm after: 22.360679105907447
Epoch 632/10000, Prediction Accuracy = 45.116%, Loss = 1.9934180498123169
Epoch: 632, Batch Gradient Norm: 27.45802159514605
Epoch: 632, Batch Gradient Norm after: 22.331349501324883
Epoch 633/10000, Prediction Accuracy = 45.12%, Loss = 1.9867279291152955
Epoch: 633, Batch Gradient Norm: 29.50103363763138
Epoch: 633, Batch Gradient Norm after: 22.36067718715708
Epoch 634/10000, Prediction Accuracy = 45.117999999999995%, Loss = 1.992339301109314
Epoch: 634, Batch Gradient Norm: 27.374419005060968
Epoch: 634, Batch Gradient Norm after: 22.249196394853154
Epoch 635/10000, Prediction Accuracy = 45.141999999999996%, Loss = 1.9843594312667847
Epoch: 635, Batch Gradient Norm: 29.466738116918982
Epoch: 635, Batch Gradient Norm after: 22.360677516154556
Epoch 636/10000, Prediction Accuracy = 45.126%, Loss = 1.990103816986084
Epoch: 636, Batch Gradient Norm: 27.48399766605098
Epoch: 636, Batch Gradient Norm after: 22.2313266595756
Epoch 637/10000, Prediction Accuracy = 45.136%, Loss = 1.9826099157333374
Epoch: 637, Batch Gradient Norm: 29.35224466216644
Epoch: 637, Batch Gradient Norm after: 22.3606770349023
Epoch 638/10000, Prediction Accuracy = 45.142%, Loss = 1.987558078765869
Epoch: 638, Batch Gradient Norm: 27.54664766678969
Epoch: 638, Batch Gradient Norm after: 22.32676439303575
Epoch 639/10000, Prediction Accuracy = 45.145999999999994%, Loss = 1.98066143989563
Epoch: 639, Batch Gradient Norm: 29.625891026472758
Epoch: 639, Batch Gradient Norm after: 22.360676600071056
Epoch 640/10000, Prediction Accuracy = 45.153999999999996%, Loss = 1.9864043951034547
Epoch: 640, Batch Gradient Norm: 27.48338054730354
Epoch: 640, Batch Gradient Norm after: 22.24055395553396
Epoch 641/10000, Prediction Accuracy = 45.16%, Loss = 1.9783795118331908
Epoch: 641, Batch Gradient Norm: 29.558438430218583
Epoch: 641, Batch Gradient Norm after: 22.3606809745905
Epoch 642/10000, Prediction Accuracy = 45.164%, Loss = 1.9840474367141723
Epoch: 642, Batch Gradient Norm: 27.587523312514218
Epoch: 642, Batch Gradient Norm after: 22.259099807830697
Epoch 643/10000, Prediction Accuracy = 45.176%, Loss = 1.976598310470581
Epoch: 643, Batch Gradient Norm: 29.561144168153806
Epoch: 643, Batch Gradient Norm after: 22.360680229211674
Epoch 644/10000, Prediction Accuracy = 45.182%, Loss = 1.9819323778152467
Epoch: 644, Batch Gradient Norm: 27.625878081389875
Epoch: 644, Batch Gradient Norm after: 22.29130203857894
Epoch 645/10000, Prediction Accuracy = 45.19%, Loss = 1.9745991945266723
Epoch: 645, Batch Gradient Norm: 29.689057495640988
Epoch: 645, Batch Gradient Norm after: 22.360677556298835
Epoch 646/10000, Prediction Accuracy = 45.196000000000005%, Loss = 1.9802587270736693
Epoch: 646, Batch Gradient Norm: 27.64428241635348
Epoch: 646, Batch Gradient Norm after: 22.245933873065585
Epoch 647/10000, Prediction Accuracy = 45.20399999999999%, Loss = 1.9725881338119506
Epoch: 647, Batch Gradient Norm: 29.62802611033709
Epoch: 647, Batch Gradient Norm after: 22.360680512031525
Epoch 648/10000, Prediction Accuracy = 45.212%, Loss = 1.9779400110244751
Epoch: 648, Batch Gradient Norm: 27.691709554019692
Epoch: 648, Batch Gradient Norm after: 22.31465460019264
Epoch 649/10000, Prediction Accuracy = 45.212%, Loss = 1.970625138282776
Epoch: 649, Batch Gradient Norm: 29.853307892663565
Epoch: 649, Batch Gradient Norm after: 22.360677196543655
Epoch 650/10000, Prediction Accuracy = 45.227999999999994%, Loss = 1.9766187429428101
Epoch: 650, Batch Gradient Norm: 27.65550011066598
Epoch: 650, Batch Gradient Norm after: 22.246994462372378
Epoch 651/10000, Prediction Accuracy = 45.224%, Loss = 1.9684378147125243
Epoch: 651, Batch Gradient Norm: 29.803836472721933
Epoch: 651, Batch Gradient Norm after: 22.3606789109069
Epoch 652/10000, Prediction Accuracy = 45.242000000000004%, Loss = 1.9743502378463744
Epoch: 652, Batch Gradient Norm: 27.742632973054253
Epoch: 652, Batch Gradient Norm after: 22.265237899946253
Epoch 653/10000, Prediction Accuracy = 45.242%, Loss = 1.966625690460205
Epoch: 653, Batch Gradient Norm: 29.8200596735639
Epoch: 653, Batch Gradient Norm after: 22.360679822601092
Epoch 654/10000, Prediction Accuracy = 45.256%, Loss = 1.9723220586776733
Epoch: 654, Batch Gradient Norm: 27.779772709683943
Epoch: 654, Batch Gradient Norm after: 22.28237939221964
Epoch 655/10000, Prediction Accuracy = 45.25600000000001%, Loss = 1.9646570920944213
Epoch: 655, Batch Gradient Norm: 29.905894156007214
Epoch: 655, Batch Gradient Norm after: 22.360677169282003
Epoch 656/10000, Prediction Accuracy = 45.274%, Loss = 1.970527172088623
Epoch: 656, Batch Gradient Norm: 27.80903145876743
Epoch: 656, Batch Gradient Norm after: 22.26683805363536
Epoch 657/10000, Prediction Accuracy = 45.264%, Loss = 1.9626741409301758
Epoch: 657, Batch Gradient Norm: 29.913336137073014
Epoch: 657, Batch Gradient Norm after: 22.36067955054923
Epoch 658/10000, Prediction Accuracy = 45.290000000000006%, Loss = 1.9684516191482544
Epoch: 658, Batch Gradient Norm: 27.8468620809717
Epoch: 658, Batch Gradient Norm after: 22.29218589936137
Epoch 659/10000, Prediction Accuracy = 45.272%, Loss = 1.9607177019119262
Epoch: 659, Batch Gradient Norm: 30.024648215548822
Epoch: 659, Batch Gradient Norm after: 22.360676335629545
Epoch 660/10000, Prediction Accuracy = 45.30200000000001%, Loss = 1.966750717163086
Epoch: 660, Batch Gradient Norm: 27.88547282569576
Epoch: 660, Batch Gradient Norm after: 22.263333536882357
Epoch 661/10000, Prediction Accuracy = 45.279999999999994%, Loss = 1.9587728261947632
Epoch: 661, Batch Gradient Norm: 30.017643367649054
Epoch: 661, Batch Gradient Norm after: 22.360679564092344
Epoch 662/10000, Prediction Accuracy = 45.3%, Loss = 1.9646286249160767
Epoch: 662, Batch Gradient Norm: 27.91445840437821
Epoch: 662, Batch Gradient Norm after: 22.29950669519652
Epoch 663/10000, Prediction Accuracy = 45.308%, Loss = 1.956784725189209
Epoch: 663, Batch Gradient Norm: 30.141952375929197
Epoch: 663, Batch Gradient Norm after: 22.360679235355196
Epoch 664/10000, Prediction Accuracy = 45.304%, Loss = 1.9629845380783082
Epoch: 664, Batch Gradient Norm: 27.936832791587886
Epoch: 664, Batch Gradient Norm after: 22.255901623869857
Epoch 665/10000, Prediction Accuracy = 45.31400000000001%, Loss = 1.9548089742660522
Epoch: 665, Batch Gradient Norm: 30.069853643288987
Epoch: 665, Batch Gradient Norm after: 22.360675532366702
Epoch 666/10000, Prediction Accuracy = 45.318%, Loss = 1.9606661319732666
Epoch: 666, Batch Gradient Norm: 27.984361051345797
Epoch: 666, Batch Gradient Norm after: 22.3314503144544
Epoch 667/10000, Prediction Accuracy = 45.334%, Loss = 1.952890682220459
Epoch: 667, Batch Gradient Norm: 30.316756627770946
Epoch: 667, Batch Gradient Norm after: 22.360677673040584
Epoch 668/10000, Prediction Accuracy = 45.336%, Loss = 1.9594696521759034
Epoch: 668, Batch Gradient Norm: 27.949214439677128
Epoch: 668, Batch Gradient Norm after: 22.251682936331598
Epoch 669/10000, Prediction Accuracy = 45.334%, Loss = 1.9507575511932373
Epoch: 669, Batch Gradient Norm: 30.22146429196672
Epoch: 669, Batch Gradient Norm after: 22.360677310486235
Epoch 670/10000, Prediction Accuracy = 45.358000000000004%, Loss = 1.9570923566818237
Epoch: 670, Batch Gradient Norm: 28.0366001847415
Epoch: 670, Batch Gradient Norm after: 22.29964825947576
Epoch 671/10000, Prediction Accuracy = 45.352%, Loss = 1.9489919185638427
Epoch: 671, Batch Gradient Norm: 30.326785020030133
Epoch: 671, Batch Gradient Norm after: 22.360679447033082
Epoch 672/10000, Prediction Accuracy = 45.368%, Loss = 1.955419158935547
Epoch: 672, Batch Gradient Norm: 28.060784438100097
Epoch: 672, Batch Gradient Norm after: 22.259075760684564
Epoch 673/10000, Prediction Accuracy = 45.362%, Loss = 1.9470468759536743
Epoch: 673, Batch Gradient Norm: 30.267667453965245
Epoch: 673, Batch Gradient Norm after: 22.36067754275035
Epoch 674/10000, Prediction Accuracy = 45.372%, Loss = 1.953163456916809
Epoch: 674, Batch Gradient Norm: 28.10620917848766
Epoch: 674, Batch Gradient Norm after: 22.324358102225887
Epoch 675/10000, Prediction Accuracy = 45.38%, Loss = 1.9451366424560548
Epoch: 675, Batch Gradient Norm: 30.49412389230655
Epoch: 675, Batch Gradient Norm after: 22.360678870951137
Epoch 676/10000, Prediction Accuracy = 45.384%, Loss = 1.9518989086151124
Epoch: 676, Batch Gradient Norm: 28.086496690528165
Epoch: 676, Batch Gradient Norm after: 22.244383099832174
Epoch 677/10000, Prediction Accuracy = 45.376%, Loss = 1.9430711269378662
Epoch: 677, Batch Gradient Norm: 30.379705582247375
Epoch: 677, Batch Gradient Norm after: 22.3606771264392
Epoch 678/10000, Prediction Accuracy = 45.384%, Loss = 1.949453854560852
Epoch: 678, Batch Gradient Norm: 28.167567763969696
Epoch: 678, Batch Gradient Norm after: 22.31196951168495
Epoch 679/10000, Prediction Accuracy = 45.378%, Loss = 1.941285753250122
Epoch: 679, Batch Gradient Norm: 30.548141800457664
Epoch: 679, Batch Gradient Norm after: 22.360677862053443
Epoch 680/10000, Prediction Accuracy = 45.382000000000005%, Loss = 1.9480153560638427
Epoch: 680, Batch Gradient Norm: 28.188737124393285
Epoch: 680, Batch Gradient Norm after: 22.246677156271836
Epoch 681/10000, Prediction Accuracy = 45.372%, Loss = 1.9393474817276002
Epoch: 681, Batch Gradient Norm: 30.429303553042452
Epoch: 681, Batch Gradient Norm after: 22.360679266901435
Epoch 682/10000, Prediction Accuracy = 45.39%, Loss = 1.9455540657043457
Epoch: 682, Batch Gradient Norm: 28.24072131557551
Epoch: 682, Batch Gradient Norm after: 22.347361218139408
Epoch 683/10000, Prediction Accuracy = 45.37%, Loss = 1.937464189529419
Epoch: 683, Batch Gradient Norm: 30.74111920917192
Epoch: 683, Batch Gradient Norm after: 22.360677339569342
Epoch 684/10000, Prediction Accuracy = 45.394%, Loss = 1.944624423980713
Epoch: 684, Batch Gradient Norm: 28.16685358437041
Epoch: 684, Batch Gradient Norm after: 22.263158340319805
Epoch 685/10000, Prediction Accuracy = 45.376%, Loss = 1.9352379322052002
Epoch: 685, Batch Gradient Norm: 30.690951174343994
Epoch: 685, Batch Gradient Norm after: 22.360677700605514
Epoch 686/10000, Prediction Accuracy = 45.408%, Loss = 1.942417597770691
Epoch: 686, Batch Gradient Norm: 28.28400386063127
Epoch: 686, Batch Gradient Norm after: 22.25169360024235
Epoch 687/10000, Prediction Accuracy = 45.394000000000005%, Loss = 1.9336017608642577
Epoch: 687, Batch Gradient Norm: 30.587306812455985
Epoch: 687, Batch Gradient Norm after: 22.360677008216612
Epoch 688/10000, Prediction Accuracy = 45.42%, Loss = 1.9400259733200074
Epoch: 688, Batch Gradient Norm: 28.33227632964185
Epoch: 688, Batch Gradient Norm after: 22.338875229716074
Epoch 689/10000, Prediction Accuracy = 45.396%, Loss = 1.9317067384719848
Epoch: 689, Batch Gradient Norm: 30.87536489750031
Epoch: 689, Batch Gradient Norm after: 22.360677079035025
Epoch 690/10000, Prediction Accuracy = 45.432%, Loss = 1.9389996767044066
Epoch: 690, Batch Gradient Norm: 28.287563231425153
Epoch: 690, Batch Gradient Norm after: 22.253435245171282
Epoch 691/10000, Prediction Accuracy = 45.41%, Loss = 1.9295799016952515
Epoch: 691, Batch Gradient Norm: 30.778384070558737
Epoch: 691, Batch Gradient Norm after: 22.360678989687727
Epoch 692/10000, Prediction Accuracy = 45.444%, Loss = 1.9366225957870484
Epoch: 692, Batch Gradient Norm: 28.38793794962303
Epoch: 692, Batch Gradient Norm after: 22.290286209620408
Epoch 693/10000, Prediction Accuracy = 45.422%, Loss = 1.9278833150863648
Epoch: 693, Batch Gradient Norm: 30.839362352886994
Epoch: 693, Batch Gradient Norm after: 22.360675525236704
Epoch 694/10000, Prediction Accuracy = 45.432%, Loss = 1.9348201751708984
Epoch: 694, Batch Gradient Norm: 28.416919460717363
Epoch: 694, Batch Gradient Norm after: 22.279912227289117
Epoch 695/10000, Prediction Accuracy = 45.45%, Loss = 1.9259816884994507
Epoch: 695, Batch Gradient Norm: 30.853314589760572
Epoch: 695, Batch Gradient Norm after: 22.36067682547562
Epoch 696/10000, Prediction Accuracy = 45.448%, Loss = 1.9328506469726563
Epoch: 696, Batch Gradient Norm: 28.454165594256757
Epoch: 696, Batch Gradient Norm after: 22.30758088080967
Epoch 697/10000, Prediction Accuracy = 45.444%, Loss = 1.9240821599960327
Epoch: 697, Batch Gradient Norm: 30.981507813888218
Epoch: 697, Batch Gradient Norm after: 22.360677451800903
Epoch 698/10000, Prediction Accuracy = 45.458000000000006%, Loss = 1.9312716007232666
Epoch: 698, Batch Gradient Norm: 28.48363324728958
Epoch: 698, Batch Gradient Norm after: 22.261043584352986
Epoch 699/10000, Prediction Accuracy = 45.454%, Loss = 1.9221919059753418
Epoch: 699, Batch Gradient Norm: 30.89987382423937
Epoch: 699, Batch Gradient Norm after: 22.360676943084968
Epoch 700/10000, Prediction Accuracy = 45.472%, Loss = 1.9289684295654297
Epoch: 700, Batch Gradient Norm: 28.529165273551424
Epoch: 700, Batch Gradient Norm after: 22.336415304764945
Epoch 701/10000, Prediction Accuracy = 45.472%, Loss = 1.9203204393386841
Epoch: 701, Batch Gradient Norm: 31.15115338319641
Epoch: 701, Batch Gradient Norm after: 22.360676843531486
Epoch 702/10000, Prediction Accuracy = 45.477999999999994%, Loss = 1.9278449296951294
Epoch: 702, Batch Gradient Norm: 28.517823989703544
Epoch: 702, Batch Gradient Norm after: 22.250160898192398
Epoch 703/10000, Prediction Accuracy = 45.477999999999994%, Loss = 1.9183226585388184
Epoch: 703, Batch Gradient Norm: 30.995492163418433
Epoch: 703, Batch Gradient Norm after: 22.36067700112722
Epoch 704/10000, Prediction Accuracy = 45.492%, Loss = 1.9252985715866089
Epoch: 704, Batch Gradient Norm: 28.59736980279173
Epoch: 704, Batch Gradient Norm after: 22.34024458963051
Epoch 705/10000, Prediction Accuracy = 45.488%, Loss = 1.9165595054626465
Epoch: 705, Batch Gradient Norm: 31.24801696746031
Epoch: 705, Batch Gradient Norm after: 22.36067765788039
Epoch 706/10000, Prediction Accuracy = 45.49%, Loss = 1.9241828918457031
Epoch: 706, Batch Gradient Norm: 28.58985210744435
Epoch: 706, Batch Gradient Norm after: 22.248801549267778
Epoch 707/10000, Prediction Accuracy = 45.502%, Loss = 1.9145829677581787
Epoch: 707, Batch Gradient Norm: 31.077303840298615
Epoch: 707, Batch Gradient Norm after: 22.360679786337396
Epoch 708/10000, Prediction Accuracy = 45.501999999999995%, Loss = 1.921586012840271
Epoch: 708, Batch Gradient Norm: 28.664221015450188
Epoch: 708, Batch Gradient Norm after: 22.352998497244737
Epoch 709/10000, Prediction Accuracy = 45.501999999999995%, Loss = 1.9128056049346924
Epoch: 709, Batch Gradient Norm: 31.381212129552253
Epoch: 709, Batch Gradient Norm after: 22.360678775062567
Epoch 710/10000, Prediction Accuracy = 45.513999999999996%, Loss = 1.920641827583313
Epoch: 710, Batch Gradient Norm: 28.619910366237317
Epoch: 710, Batch Gradient Norm after: 22.25883807942919
Epoch 711/10000, Prediction Accuracy = 45.513999999999996%, Loss = 1.9107152938842773
Epoch: 711, Batch Gradient Norm: 31.259168392946414
Epoch: 711, Batch Gradient Norm after: 22.360679065464318
Epoch 712/10000, Prediction Accuracy = 45.534%, Loss = 1.918212103843689
Epoch: 712, Batch Gradient Norm: 28.71923985457827
Epoch: 712, Batch Gradient Norm after: 22.305915013631747
Epoch 713/10000, Prediction Accuracy = 45.510000000000005%, Loss = 1.9090476512908936
Epoch: 713, Batch Gradient Norm: 31.350486365771637
Epoch: 713, Batch Gradient Norm after: 22.36067841444022
Epoch 714/10000, Prediction Accuracy = 45.544%, Loss = 1.9165348768234254
Epoch: 714, Batch Gradient Norm: 28.748093201961623
Epoch: 714, Batch Gradient Norm after: 22.29543726893039
Epoch 715/10000, Prediction Accuracy = 45.516000000000005%, Loss = 1.9071732044219971
Epoch: 715, Batch Gradient Norm: 31.367464601440243
Epoch: 715, Batch Gradient Norm after: 22.360677882145193
Epoch 716/10000, Prediction Accuracy = 45.562%, Loss = 1.914620327949524
Epoch: 716, Batch Gradient Norm: 28.7828260464788
Epoch: 716, Batch Gradient Norm after: 22.31153119930085
Epoch 717/10000, Prediction Accuracy = 45.52400000000001%, Loss = 1.905316138267517
Epoch: 717, Batch Gradient Norm: 31.462470157094305
Epoch: 717, Batch Gradient Norm after: 22.360677286499097
Epoch 718/10000, Prediction Accuracy = 45.564%, Loss = 1.9129744529724122
Epoch: 718, Batch Gradient Norm: 28.81462865822625
Epoch: 718, Batch Gradient Norm after: 22.287542926322274
Epoch 719/10000, Prediction Accuracy = 45.53%, Loss = 1.9034553050994873
Epoch: 719, Batch Gradient Norm: 31.442510064249227
Epoch: 719, Batch Gradient Norm after: 22.36067891487034
Epoch 720/10000, Prediction Accuracy = 45.572%, Loss = 1.9109246969223022
Epoch: 720, Batch Gradient Norm: 28.851883757057426
Epoch: 720, Batch Gradient Norm after: 22.32442701327104
Epoch 721/10000, Prediction Accuracy = 45.524%, Loss = 1.9015924215316773
Epoch: 721, Batch Gradient Norm: 31.590142528160655
Epoch: 721, Batch Gradient Norm after: 22.36067653529132
Epoch 722/10000, Prediction Accuracy = 45.583999999999996%, Loss = 1.9094787120819092
Epoch: 722, Batch Gradient Norm: 28.878872770062284
Epoch: 722, Batch Gradient Norm after: 22.267798662626404
Epoch 723/10000, Prediction Accuracy = 45.544000000000004%, Loss = 1.8997440814971924
Epoch: 723, Batch Gradient Norm: 31.466349713417763
Epoch: 723, Batch Gradient Norm after: 22.360680123175523
Epoch 724/10000, Prediction Accuracy = 45.604%, Loss = 1.907090711593628
Epoch: 724, Batch Gradient Norm: 28.925268387984747
Epoch: 724, Batch Gradient Norm after: 22.36067666580466
Epoch 725/10000, Prediction Accuracy = 45.544%, Loss = 1.8978891849517823
Epoch: 725, Batch Gradient Norm: 31.76999974689779
Epoch: 725, Batch Gradient Norm after: 22.3606768641584
Epoch 726/10000, Prediction Accuracy = 45.602%, Loss = 1.9062032222747802
Epoch: 726, Batch Gradient Norm: 28.880660720693232
Epoch: 726, Batch Gradient Norm after: 22.275505283514278
Epoch 727/10000, Prediction Accuracy = 45.564%, Loss = 1.8958284616470338
Epoch: 727, Batch Gradient Norm: 31.668719472682426
Epoch: 727, Batch Gradient Norm after: 22.36067700829985
Epoch 728/10000, Prediction Accuracy = 45.6%, Loss = 1.9038811683654786
Epoch: 728, Batch Gradient Norm: 28.980883230424887
Epoch: 728, Batch Gradient Norm after: 22.30832318211622
Epoch 729/10000, Prediction Accuracy = 45.572%, Loss = 1.8941914558410644
Epoch: 729, Batch Gradient Norm: 31.710002481364878
Epoch: 729, Batch Gradient Norm after: 22.360677341290977
Epoch 730/10000, Prediction Accuracy = 45.61200000000001%, Loss = 1.9020530700683593
Epoch: 730, Batch Gradient Norm: 29.011675725742535
Epoch: 730, Batch Gradient Norm after: 22.31323786855581
Epoch 731/10000, Prediction Accuracy = 45.578%, Loss = 1.8923486471176147
Epoch: 731, Batch Gradient Norm: 31.77378021470784
Epoch: 731, Batch Gradient Norm after: 22.36067726853677
Epoch 732/10000, Prediction Accuracy = 45.61%, Loss = 1.9003331899642943
Epoch: 732, Batch Gradient Norm: 29.045135351667646
Epoch: 732, Batch Gradient Norm after: 22.305950820024623
Epoch 733/10000, Prediction Accuracy = 45.593999999999994%, Loss = 1.8905165433883666
Epoch: 733, Batch Gradient Norm: 31.80271517315964
Epoch: 733, Batch Gradient Norm after: 22.36067892881234
Epoch 734/10000, Prediction Accuracy = 45.61%, Loss = 1.898483657836914
Epoch: 734, Batch Gradient Norm: 29.078220776163533
Epoch: 734, Batch Gradient Norm after: 22.316763831910865
Epoch 735/10000, Prediction Accuracy = 45.60999999999999%, Loss = 1.8886831521987915
Epoch: 735, Batch Gradient Norm: 31.884890095901515
Epoch: 735, Batch Gradient Norm after: 22.360676086484848
Epoch 736/10000, Prediction Accuracy = 45.61%, Loss = 1.8968237400054933
Epoch: 736, Batch Gradient Norm: 29.112213482328897
Epoch: 736, Batch Gradient Norm after: 22.29818278855561
Epoch 737/10000, Prediction Accuracy = 45.618%, Loss = 1.886855411529541
Epoch: 737, Batch Gradient Norm: 31.883096264639175
Epoch: 737, Batch Gradient Norm after: 22.360678511506077
Epoch 738/10000, Prediction Accuracy = 45.626%, Loss = 1.894874906539917
Epoch: 738, Batch Gradient Norm: 29.14734910788423
Epoch: 738, Batch Gradient Norm after: 22.319980373372005
Epoch 739/10000, Prediction Accuracy = 45.61%, Loss = 1.8850412607192992
Epoch: 739, Batch Gradient Norm: 31.995738134733372
Epoch: 739, Batch Gradient Norm after: 22.360676944413083
Epoch 740/10000, Prediction Accuracy = 45.628%, Loss = 1.893330717086792
Epoch: 740, Batch Gradient Norm: 29.18014460060778
Epoch: 740, Batch Gradient Norm after: 22.285535470074233
Epoch 741/10000, Prediction Accuracy = 45.614%, Loss = 1.883233380317688
Epoch: 741, Batch Gradient Norm: 31.94112412655261
Epoch: 741, Batch Gradient Norm after: 22.360678503208582
Epoch 742/10000, Prediction Accuracy = 45.635999999999996%, Loss = 1.8912003993988038
Epoch: 742, Batch Gradient Norm: 29.214311395794617
Epoch: 742, Batch Gradient Norm after: 22.343987512181517
Epoch 743/10000, Prediction Accuracy = 45.634%, Loss = 1.8814104795455933
Epoch: 743, Batch Gradient Norm: 32.16612455903547
Epoch: 743, Batch Gradient Norm after: 22.3606785481317
Epoch 744/10000, Prediction Accuracy = 45.658%, Loss = 1.8900609254837035
Epoch: 744, Batch Gradient Norm: 29.214346886483064
Epoch: 744, Batch Gradient Norm after: 22.27627106756476
Epoch 745/10000, Prediction Accuracy = 45.636%, Loss = 1.8795006990432739
Epoch: 745, Batch Gradient Norm: 32.06203879123962
Epoch: 745, Batch Gradient Norm after: 22.360677925315194
Epoch 746/10000, Prediction Accuracy = 45.65400000000001%, Loss = 1.8877322196960449
Epoch: 746, Batch Gradient Norm: 29.283147720031586
Epoch: 746, Batch Gradient Norm after: 22.336872923683355
Epoch 747/10000, Prediction Accuracy = 45.653999999999996%, Loss = 1.8777713775634766
Epoch: 747, Batch Gradient Norm: 32.244314480040096
Epoch: 747, Batch Gradient Norm after: 22.360677763415207
Epoch 748/10000, Prediction Accuracy = 45.66199999999999%, Loss = 1.886443305015564
Epoch: 748, Batch Gradient Norm: 29.302415331789607
Epoch: 748, Batch Gradient Norm after: 22.26886327058171
Epoch 749/10000, Prediction Accuracy = 45.664%, Loss = 1.8759490966796875
Epoch: 749, Batch Gradient Norm: 32.1043886869639
Epoch: 749, Batch Gradient Norm after: 22.360676209035194
Epoch 750/10000, Prediction Accuracy = 45.658%, Loss = 1.884013843536377
Epoch: 750, Batch Gradient Norm: 29.34830086051683
Epoch: 750, Batch Gradient Norm after: 22.36067904862384
Epoch 751/10000, Prediction Accuracy = 45.676%, Loss = 1.874141812324524
Epoch: 751, Batch Gradient Norm: 32.401469033382355
Epoch: 751, Batch Gradient Norm after: 22.360679979342194
Epoch 752/10000, Prediction Accuracy = 45.668%, Loss = 1.8831354141235352
Epoch: 752, Batch Gradient Norm: 29.307980478170332
Epoch: 752, Batch Gradient Norm after: 22.295574677408666
Epoch 753/10000, Prediction Accuracy = 45.678%, Loss = 1.8721120357513428
Epoch: 753, Batch Gradient Norm: 32.36735687103123
Epoch: 753, Batch Gradient Norm after: 22.360676323236966
Epoch 754/10000, Prediction Accuracy = 45.68000000000001%, Loss = 1.8810790061950684
Epoch: 754, Batch Gradient Norm: 29.411436647989294
Epoch: 754, Batch Gradient Norm after: 22.287146227217523
Epoch 755/10000, Prediction Accuracy = 45.69%, Loss = 1.8705373287200928
Epoch: 755, Batch Gradient Norm: 32.28326397716841
Epoch: 755, Batch Gradient Norm after: 22.36067854125348
Epoch 756/10000, Prediction Accuracy = 45.676%, Loss = 1.878853154182434
Epoch: 756, Batch Gradient Norm: 29.448471038394928
Epoch: 756, Batch Gradient Norm after: 22.360677967341086
Epoch 757/10000, Prediction Accuracy = 45.702%, Loss = 1.8687191724777221
Epoch: 757, Batch Gradient Norm: 32.56043039688293
Epoch: 757, Batch Gradient Norm after: 22.360679855980386
Epoch 758/10000, Prediction Accuracy = 45.676%, Loss = 1.8779135704040528
Epoch: 758, Batch Gradient Norm: 29.39783128536593
Epoch: 758, Batch Gradient Norm after: 22.305512116308464
Epoch 759/10000, Prediction Accuracy = 45.69199999999999%, Loss = 1.8666842937469483
Epoch: 759, Batch Gradient Norm: 32.56853210975926
Epoch: 759, Batch Gradient Norm after: 22.360678702655957
Epoch 760/10000, Prediction Accuracy = 45.686%, Loss = 1.8760092973709106
Epoch: 760, Batch Gradient Norm: 29.477612517943317
Epoch: 760, Batch Gradient Norm after: 22.291920771582575
Epoch 761/10000, Prediction Accuracy = 45.720000000000006%, Loss = 1.8650436878204346
Epoch: 761, Batch Gradient Norm: 32.499466218956215
Epoch: 761, Batch Gradient Norm after: 22.36067630787071
Epoch 762/10000, Prediction Accuracy = 45.696%, Loss = 1.8738572597503662
Epoch: 762, Batch Gradient Norm: 29.542699995334335
Epoch: 762, Batch Gradient Norm after: 22.33039042252111
Epoch 763/10000, Prediction Accuracy = 45.726%, Loss = 1.8633562326431274
Epoch: 763, Batch Gradient Norm: 32.61644509297708
Epoch: 763, Batch Gradient Norm after: 22.360677983998762
Epoch 764/10000, Prediction Accuracy = 45.71%, Loss = 1.8723596811294556
Epoch: 764, Batch Gradient Norm: 29.57740486447241
Epoch: 764, Batch Gradient Norm after: 22.28996869907288
Epoch 765/10000, Prediction Accuracy = 45.738%, Loss = 1.861583662033081
Epoch: 765, Batch Gradient Norm: 32.53764296060521
Epoch: 765, Batch Gradient Norm after: 22.360678492599757
Epoch 766/10000, Prediction Accuracy = 45.716%, Loss = 1.870176100730896
Epoch: 766, Batch Gradient Norm: 29.61047933226347
Epoch: 766, Batch Gradient Norm after: 22.360679549515005
Epoch 767/10000, Prediction Accuracy = 45.74600000000001%, Loss = 1.8597749710083007
Epoch: 767, Batch Gradient Norm: 32.81571092543136
Epoch: 767, Batch Gradient Norm after: 22.360678267534887
Epoch 768/10000, Prediction Accuracy = 45.724000000000004%, Loss = 1.8692495822906494
Epoch: 768, Batch Gradient Norm: 29.540758430574574
Epoch: 768, Batch Gradient Norm after: 22.31059761419663
Epoch 769/10000, Prediction Accuracy = 45.756%, Loss = 1.8576727628707885
Epoch: 769, Batch Gradient Norm: 32.84207473165114
Epoch: 769, Batch Gradient Norm after: 22.360679403134718
Epoch 770/10000, Prediction Accuracy = 45.730000000000004%, Loss = 1.867423939704895
Epoch: 770, Batch Gradient Norm: 29.60713960459339
Epoch: 770, Batch Gradient Norm after: 22.31085628889765
Epoch 771/10000, Prediction Accuracy = 45.784000000000006%, Loss = 1.8559932947158813
Epoch: 771, Batch Gradient Norm: 32.86457276471817
Epoch: 771, Batch Gradient Norm after: 22.36067826154162
Epoch 772/10000, Prediction Accuracy = 45.726%, Loss = 1.8656033515930175
Epoch: 772, Batch Gradient Norm: 29.670428411039126
Epoch: 772, Batch Gradient Norm after: 22.30157598229215
Epoch 773/10000, Prediction Accuracy = 45.778000000000006%, Loss = 1.8543017148971557
Epoch: 773, Batch Gradient Norm: 32.83133840776333
Epoch: 773, Batch Gradient Norm after: 22.360676745629153
Epoch 774/10000, Prediction Accuracy = 45.739999999999995%, Loss = 1.8635852336883545
Epoch: 774, Batch Gradient Norm: 29.749317430085174
Epoch: 774, Batch Gradient Norm after: 22.311255703772822
Epoch 775/10000, Prediction Accuracy = 45.784000000000006%, Loss = 1.8526585578918457
Epoch: 775, Batch Gradient Norm: 32.84616187009577
Epoch: 775, Batch Gradient Norm after: 22.36067895390564
Epoch 776/10000, Prediction Accuracy = 45.745999999999995%, Loss = 1.861744999885559
Epoch: 776, Batch Gradient Norm: 29.782543217187524
Epoch: 776, Batch Gradient Norm after: 22.32444420978538
Epoch 777/10000, Prediction Accuracy = 45.79600000000001%, Loss = 1.8508872270584107
Epoch: 777, Batch Gradient Norm: 32.93842459859581
Epoch: 777, Batch Gradient Norm after: 22.36067813198338
Epoch 778/10000, Prediction Accuracy = 45.756%, Loss = 1.8601742506027221
Epoch: 778, Batch Gradient Norm: 29.816225302627572
Epoch: 778, Batch Gradient Norm after: 22.29539773289159
Epoch 779/10000, Prediction Accuracy = 45.816%, Loss = 1.8491246223449707
Epoch: 779, Batch Gradient Norm: 32.89459776433116
Epoch: 779, Batch Gradient Norm after: 22.3606783894072
Epoch 780/10000, Prediction Accuracy = 45.756%, Loss = 1.8581181764602661
Epoch: 780, Batch Gradient Norm: 29.845413933408892
Epoch: 780, Batch Gradient Norm after: 22.34836841602066
Epoch 781/10000, Prediction Accuracy = 45.839999999999996%, Loss = 1.8473431587219238
Epoch: 781, Batch Gradient Norm: 33.11672992268408
Epoch: 781, Batch Gradient Norm after: 22.360676431221716
Epoch 782/10000, Prediction Accuracy = 45.766000000000005%, Loss = 1.8570059776306151
Epoch: 782, Batch Gradient Norm: 29.83140731946886
Epoch: 782, Batch Gradient Norm after: 22.2939475178153
Epoch 783/10000, Prediction Accuracy = 45.856%, Loss = 1.8454509735107423
Epoch: 783, Batch Gradient Norm: 33.06447820553241
Epoch: 783, Batch Gradient Norm after: 22.36067674456898
Epoch 784/10000, Prediction Accuracy = 45.775999999999996%, Loss = 1.8549317121505737
Epoch: 784, Batch Gradient Norm: 29.9114458726354
Epoch: 784, Batch Gradient Norm after: 22.312190641380198
Epoch 785/10000, Prediction Accuracy = 45.86%, Loss = 1.8438244819641114
Epoch: 785, Batch Gradient Norm: 33.1015108451495
Epoch: 785, Batch Gradient Norm after: 22.360677334995284
Epoch 786/10000, Prediction Accuracy = 45.786%, Loss = 1.8531676292419434
Epoch: 786, Batch Gradient Norm: 29.944483522235657
Epoch: 786, Batch Gradient Norm after: 22.325495808148794
Epoch 787/10000, Prediction Accuracy = 45.876%, Loss = 1.84206383228302
Epoch: 787, Batch Gradient Norm: 33.186444708798916
Epoch: 787, Batch Gradient Norm after: 22.360677820224915
Epoch 788/10000, Prediction Accuracy = 45.8%, Loss = 1.851570153236389
Epoch: 788, Batch Gradient Norm: 29.978988329289802
Epoch: 788, Batch Gradient Norm after: 22.308834842147434
Epoch 789/10000, Prediction Accuracy = 45.903999999999996%, Loss = 1.8403234481811523
Epoch: 789, Batch Gradient Norm: 33.182689004335955
Epoch: 789, Batch Gradient Norm after: 22.360677873909154
Epoch 790/10000, Prediction Accuracy = 45.834%, Loss = 1.849674129486084
Epoch: 790, Batch Gradient Norm: 30.012014339823228
Epoch: 790, Batch Gradient Norm after: 22.337995459372692
Epoch 791/10000, Prediction Accuracy = 45.914%, Loss = 1.8385592937469482
Epoch: 791, Batch Gradient Norm: 33.324808831902786
Epoch: 791, Batch Gradient Norm after: 22.36067577034109
Epoch 792/10000, Prediction Accuracy = 45.858%, Loss = 1.8483077049255372
Epoch: 792, Batch Gradient Norm: 30.05003568505723
Epoch: 792, Batch Gradient Norm after: 22.284366299905326
Epoch 793/10000, Prediction Accuracy = 45.934000000000005%, Loss = 1.8368405818939209
Epoch: 793, Batch Gradient Norm: 33.198935402136335
Epoch: 793, Batch Gradient Norm after: 22.360676624964302
Epoch 794/10000, Prediction Accuracy = 45.855999999999995%, Loss = 1.8459789752960205
Epoch: 794, Batch Gradient Norm: 30.08185339161868
Epoch: 794, Batch Gradient Norm after: 22.36067683546739
Epoch 795/10000, Prediction Accuracy = 45.942%, Loss = 1.8350435256958009
Epoch: 795, Batch Gradient Norm: 33.46250154577577
Epoch: 795, Batch Gradient Norm after: 22.36067663913819
Epoch 796/10000, Prediction Accuracy = 45.876%, Loss = 1.8450376987457275
Epoch: 796, Batch Gradient Norm: 30.093019431851776
Epoch: 796, Batch Gradient Norm after: 22.295400535413894
Epoch 797/10000, Prediction Accuracy = 45.936%, Loss = 1.833263635635376
Epoch: 797, Batch Gradient Norm: 33.36675523188074
Epoch: 797, Batch Gradient Norm after: 22.360676810054084
Epoch 798/10000, Prediction Accuracy = 45.892%, Loss = 1.842846441268921
Epoch: 798, Batch Gradient Norm: 30.14752330592577
Epoch: 798, Batch Gradient Norm after: 22.3546792224955
Epoch 799/10000, Prediction Accuracy = 45.934000000000005%, Loss = 1.831567072868347
Epoch: 799, Batch Gradient Norm: 33.5840674257326
Epoch: 799, Batch Gradient Norm after: 22.360681050446605
Epoch 800/10000, Prediction Accuracy = 45.903999999999996%, Loss = 1.8417378664016724
Epoch: 800, Batch Gradient Norm: 30.138387203426188
Epoch: 800, Batch Gradient Norm after: 22.302056097899634
Epoch 801/10000, Prediction Accuracy = 45.942%, Loss = 1.8297100782394409
Epoch: 801, Batch Gradient Norm: 33.514827953206655
Epoch: 801, Batch Gradient Norm after: 22.360679688095658
Epoch 802/10000, Prediction Accuracy = 45.912%, Loss = 1.8396400690078736
Epoch: 802, Batch Gradient Norm: 30.215755631237986
Epoch: 802, Batch Gradient Norm after: 22.328626284046525
Epoch 803/10000, Prediction Accuracy = 45.949999999999996%, Loss = 1.8280970573425293
Epoch: 803, Batch Gradient Norm: 33.57930021829345
Epoch: 803, Batch Gradient Norm after: 22.360676966024375
Epoch 804/10000, Prediction Accuracy = 45.914%, Loss = 1.8380241394042969
Epoch: 804, Batch Gradient Norm: 30.251806616294058
Epoch: 804, Batch Gradient Norm after: 22.320253506316597
Epoch 805/10000, Prediction Accuracy = 45.954%, Loss = 1.8263794898986816
Epoch: 805, Batch Gradient Norm: 33.5943376594926
Epoch: 805, Batch Gradient Norm after: 22.360677992022726
Epoch 806/10000, Prediction Accuracy = 45.92399999999999%, Loss = 1.8362385988235475
Epoch: 806, Batch Gradient Norm: 30.28181765768093
Epoch: 806, Batch Gradient Norm after: 22.34003554963313
Epoch 807/10000, Prediction Accuracy = 45.958%, Loss = 1.8246408462524415
Epoch: 807, Batch Gradient Norm: 33.71173079851442
Epoch: 807, Batch Gradient Norm after: 22.3606780788336
Epoch 808/10000, Prediction Accuracy = 45.942%, Loss = 1.834790015220642
Epoch: 808, Batch Gradient Norm: 30.319072524487527
Epoch: 808, Batch Gradient Norm after: 22.302496551181978
Epoch 809/10000, Prediction Accuracy = 45.971999999999994%, Loss = 1.8229517221450806
Epoch: 809, Batch Gradient Norm: 33.632304954845075
Epoch: 809, Batch Gradient Norm after: 22.360678439562186
Epoch 810/10000, Prediction Accuracy = 45.962%, Loss = 1.8326668977737426
Epoch: 810, Batch Gradient Norm: 30.345389059615997
Epoch: 810, Batch Gradient Norm after: 22.360675567107943
Epoch 811/10000, Prediction Accuracy = 45.986000000000004%, Loss = 1.8211945056915284
Epoch: 811, Batch Gradient Norm: 33.866609576474694
Epoch: 811, Batch Gradient Norm after: 22.360676804534332
Epoch 812/10000, Prediction Accuracy = 45.970000000000006%, Loss = 1.8316434144973754
Epoch: 812, Batch Gradient Norm: 30.342762055557735
Epoch: 812, Batch Gradient Norm after: 22.30078278893734
Epoch 813/10000, Prediction Accuracy = 45.988%, Loss = 1.8193757772445678
Epoch: 813, Batch Gradient Norm: 33.788891344051336
Epoch: 813, Batch Gradient Norm after: 22.36067885487167
Epoch 814/10000, Prediction Accuracy = 45.974000000000004%, Loss = 1.8295193910598755
Epoch: 814, Batch Gradient Norm: 30.414248810521777
Epoch: 814, Batch Gradient Norm after: 22.33446409210023
Epoch 815/10000, Prediction Accuracy = 45.992000000000004%, Loss = 1.8177823781967164
Epoch: 815, Batch Gradient Norm: 33.88648618229093
Epoch: 815, Batch Gradient Norm after: 22.3606780144422
Epoch 816/10000, Prediction Accuracy = 45.972%, Loss = 1.8280328035354614
Epoch: 816, Batch Gradient Norm: 30.449439625033218
Epoch: 816, Batch Gradient Norm after: 22.31168342901204
Epoch 817/10000, Prediction Accuracy = 46.004%, Loss = 1.8160843133926392
Epoch: 817, Batch Gradient Norm: 33.85630088007763
Epoch: 817, Batch Gradient Norm after: 22.360679322909387
Epoch 818/10000, Prediction Accuracy = 45.978%, Loss = 1.8260848760604858
Epoch: 818, Batch Gradient Norm: 30.481460245131903
Epoch: 818, Batch Gradient Norm after: 22.34847193922035
Epoch 819/10000, Prediction Accuracy = 46.010000000000005%, Loss = 1.8143474578857421
Epoch: 819, Batch Gradient Norm: 34.03336230157807
Epoch: 819, Batch Gradient Norm after: 22.360677856754585
Epoch 820/10000, Prediction Accuracy = 45.99400000000001%, Loss = 1.8248902797698974
Epoch: 820, Batch Gradient Norm: 30.51686862911345
Epoch: 820, Batch Gradient Norm after: 22.290710259788586
Epoch 821/10000, Prediction Accuracy = 46.010000000000005%, Loss = 1.812659215927124
Epoch: 821, Batch Gradient Norm: 33.885380031511545
Epoch: 821, Batch Gradient Norm after: 22.36067801143887
Epoch 822/10000, Prediction Accuracy = 45.996%, Loss = 1.8225501298904419
Epoch: 822, Batch Gradient Norm: 30.54669002013582
Epoch: 822, Batch Gradient Norm after: 22.36067631020579
Epoch 823/10000, Prediction Accuracy = 46.019999999999996%, Loss = 1.8109096050262452
Epoch: 823, Batch Gradient Norm: 34.11950786724639
Epoch: 823, Batch Gradient Norm after: 22.360677583430775
Epoch 824/10000, Prediction Accuracy = 46.013999999999996%, Loss = 1.8215567827224732
Epoch: 824, Batch Gradient Norm: 30.594801358045103
Epoch: 824, Batch Gradient Norm after: 22.290756722490656
Epoch 825/10000, Prediction Accuracy = 46.038%, Loss = 1.809286069869995
Epoch: 825, Batch Gradient Norm: 33.96973327959836
Epoch: 825, Batch Gradient Norm after: 22.36067617551978
Epoch 826/10000, Prediction Accuracy = 46.008%, Loss = 1.8192256212234497
Epoch: 826, Batch Gradient Norm: 30.615261325505525
Epoch: 826, Batch Gradient Norm after: 22.360675419449663
Epoch 827/10000, Prediction Accuracy = 46.048%, Loss = 1.807527780532837
Epoch: 827, Batch Gradient Norm: 34.20260000298409
Epoch: 827, Batch Gradient Norm after: 22.360677959305008
Epoch 828/10000, Prediction Accuracy = 46.01200000000001%, Loss = 1.8182272434234619
Epoch: 828, Batch Gradient Norm: 30.65756633129472
Epoch: 828, Batch Gradient Norm after: 22.297176135020653
Epoch 829/10000, Prediction Accuracy = 46.072%, Loss = 1.8059026718139648
Epoch: 829, Batch Gradient Norm: 34.08187567529232
Epoch: 829, Batch Gradient Norm after: 22.360678166367855
Epoch 830/10000, Prediction Accuracy = 46.022000000000006%, Loss = 1.8159892797470092
Epoch: 830, Batch Gradient Norm: 30.683471855935675
Epoch: 830, Batch Gradient Norm after: 22.360677527731056
Epoch 831/10000, Prediction Accuracy = 46.068%, Loss = 1.8041619777679443
Epoch: 831, Batch Gradient Norm: 34.30736363748616
Epoch: 831, Batch Gradient Norm after: 22.360677422841565
Epoch 832/10000, Prediction Accuracy = 46.03000000000001%, Loss = 1.8149839401245118
Epoch: 832, Batch Gradient Norm: 30.732008942930428
Epoch: 832, Batch Gradient Norm after: 22.293989595563225
Epoch 833/10000, Prediction Accuracy = 46.07%, Loss = 1.8025327682495118
Epoch: 833, Batch Gradient Norm: 34.16581349174063
Epoch: 833, Batch Gradient Norm after: 22.36067933031685
Epoch 834/10000, Prediction Accuracy = 46.032%, Loss = 1.8126632928848267
Epoch: 834, Batch Gradient Norm: 30.754795026434014
Epoch: 834, Batch Gradient Norm after: 22.360675117385235
Epoch 835/10000, Prediction Accuracy = 46.08%, Loss = 1.8007835626602173
Epoch: 835, Batch Gradient Norm: 34.38311989839685
Epoch: 835, Batch Gradient Norm after: 22.36067715592449
Epoch 836/10000, Prediction Accuracy = 46.050000000000004%, Loss = 1.811617636680603
Epoch: 836, Batch Gradient Norm: 30.796921927507327
Epoch: 836, Batch Gradient Norm after: 22.30819917115528
Epoch 837/10000, Prediction Accuracy = 46.072%, Loss = 1.799176573753357
Epoch: 837, Batch Gradient Norm: 34.30984166897917
Epoch: 837, Batch Gradient Norm after: 22.36067592359425
Epoch 838/10000, Prediction Accuracy = 46.052%, Loss = 1.8095567226409912
Epoch: 838, Batch Gradient Norm: 30.822132022206635
Epoch: 838, Batch Gradient Norm after: 22.36067728764137
Epoch 839/10000, Prediction Accuracy = 46.077999999999996%, Loss = 1.7974676132202148
Epoch: 839, Batch Gradient Norm: 34.53017590309118
Epoch: 839, Batch Gradient Norm after: 22.360677533102844
Epoch 840/10000, Prediction Accuracy = 46.062%, Loss = 1.8085132122039795
Epoch: 840, Batch Gradient Norm: 30.865270782317985
Epoch: 840, Batch Gradient Norm after: 22.291530926696296
Epoch 841/10000, Prediction Accuracy = 46.077999999999996%, Loss = 1.7958369970321655
Epoch: 841, Batch Gradient Norm: 34.3523837129514
Epoch: 841, Batch Gradient Norm after: 22.360678844904967
Epoch 842/10000, Prediction Accuracy = 46.077999999999996%, Loss = 1.8060926437377929
Epoch: 842, Batch Gradient Norm: 30.887559645872276
Epoch: 842, Batch Gradient Norm after: 22.360676426108647
Epoch 843/10000, Prediction Accuracy = 46.08%, Loss = 1.7941004514694214
Epoch: 843, Batch Gradient Norm: 34.56163336163258
Epoch: 843, Batch Gradient Norm after: 22.360677338136792
Epoch 844/10000, Prediction Accuracy = 46.07599999999999%, Loss = 1.8050529956817627
Epoch: 844, Batch Gradient Norm: 30.9337698572428
Epoch: 844, Batch Gradient Norm after: 22.323208055513817
Epoch 845/10000, Prediction Accuracy = 46.093999999999994%, Loss = 1.7925139904022216
Epoch: 845, Batch Gradient Norm: 34.55851093170881
Epoch: 845, Batch Gradient Norm after: 22.360677283082083
Epoch 846/10000, Prediction Accuracy = 46.088%, Loss = 1.803247594833374
Epoch: 846, Batch Gradient Norm: 30.965738374985378
Epoch: 846, Batch Gradient Norm after: 22.351364613190743
Epoch 847/10000, Prediction Accuracy = 46.10000000000001%, Loss = 1.790836548805237
Epoch: 847, Batch Gradient Norm: 34.7150488125748
Epoch: 847, Batch Gradient Norm after: 22.360677984039814
Epoch 848/10000, Prediction Accuracy = 46.094%, Loss = 1.8020151853561401
Epoch: 848, Batch Gradient Norm: 31.007102316509435
Epoch: 848, Batch Gradient Norm after: 22.30300867461785
Epoch 849/10000, Prediction Accuracy = 46.10600000000001%, Loss = 1.7892269134521483
Epoch: 849, Batch Gradient Norm: 34.59327800475644
Epoch: 849, Batch Gradient Norm after: 22.360679537579713
Epoch 850/10000, Prediction Accuracy = 46.1%, Loss = 1.7997934103012085
Epoch: 850, Batch Gradient Norm: 31.027386095595816
Epoch: 850, Batch Gradient Norm after: 22.360674997518803
Epoch 851/10000, Prediction Accuracy = 46.11%, Loss = 1.7875128984451294
Epoch: 851, Batch Gradient Norm: 34.80397489273804
Epoch: 851, Batch Gradient Norm after: 22.360680259293773
Epoch 852/10000, Prediction Accuracy = 46.108%, Loss = 1.7987518072128297
Epoch: 852, Batch Gradient Norm: 31.075505257160472
Epoch: 852, Batch Gradient Norm after: 22.305709874597838
Epoch 853/10000, Prediction Accuracy = 46.10600000000001%, Loss = 1.7859357595443726
Epoch: 853, Batch Gradient Norm: 34.703656317996554
Epoch: 853, Batch Gradient Norm after: 22.360677530547513
Epoch 854/10000, Prediction Accuracy = 46.118%, Loss = 1.7966350078582765
Epoch: 854, Batch Gradient Norm: 31.098088630856655
Epoch: 854, Batch Gradient Norm after: 22.36067798842531
Epoch 855/10000, Prediction Accuracy = 46.106%, Loss = 1.7842441558837892
Epoch: 855, Batch Gradient Norm: 34.923761528469264
Epoch: 855, Batch Gradient Norm after: 22.360678902997186
Epoch 856/10000, Prediction Accuracy = 46.124%, Loss = 1.7956583738327025
Epoch: 856, Batch Gradient Norm: 31.136169065563127
Epoch: 856, Batch Gradient Norm after: 22.299074419986635
Epoch 857/10000, Prediction Accuracy = 46.09400000000001%, Loss = 1.782640814781189
Epoch: 857, Batch Gradient Norm: 34.79007752257383
Epoch: 857, Batch Gradient Norm after: 22.360679393054113
Epoch 858/10000, Prediction Accuracy = 46.120000000000005%, Loss = 1.7934142351150513
Epoch: 858, Batch Gradient Norm: 31.166592930707612
Epoch: 858, Batch Gradient Norm after: 22.360677694885318
Epoch 859/10000, Prediction Accuracy = 46.08800000000001%, Loss = 1.7809669971466064
Epoch: 859, Batch Gradient Norm: 35.008235519405844
Epoch: 859, Batch Gradient Norm after: 22.360677246066455
Epoch 860/10000, Prediction Accuracy = 46.124%, Loss = 1.7924175262451172
Epoch: 860, Batch Gradient Norm: 31.217611055577592
Epoch: 860, Batch Gradient Norm after: 22.300818585697076
Epoch 861/10000, Prediction Accuracy = 46.11%, Loss = 1.7794076204299927
Epoch: 861, Batch Gradient Norm: 34.8898962148146
Epoch: 861, Batch Gradient Norm after: 22.360677392482096
Epoch 862/10000, Prediction Accuracy = 46.12%, Loss = 1.7902269601821899
Epoch: 862, Batch Gradient Norm: 31.23770516474653
Epoch: 862, Batch Gradient Norm after: 22.3606759093735
Epoch 863/10000, Prediction Accuracy = 46.12%, Loss = 1.7777092695236205
Epoch: 863, Batch Gradient Norm: 35.12686296460704
Epoch: 863, Batch Gradient Norm after: 22.360679318139283
Epoch 864/10000, Prediction Accuracy = 46.120000000000005%, Loss = 1.7893114805221557
Epoch: 864, Batch Gradient Norm: 31.27851543122207
Epoch: 864, Batch Gradient Norm after: 22.298190269926096
Epoch 865/10000, Prediction Accuracy = 46.105999999999995%, Loss = 1.7761279582977294
Epoch: 865, Batch Gradient Norm: 35.00157814612878
Epoch: 865, Batch Gradient Norm after: 22.360677583018663
Epoch 866/10000, Prediction Accuracy = 46.118%, Loss = 1.7871128797531128
Epoch: 866, Batch Gradient Norm: 31.304810768631683
Epoch: 866, Batch Gradient Norm after: 22.360677495317113
Epoch 867/10000, Prediction Accuracy = 46.114%, Loss = 1.7744574785232543
Epoch: 867, Batch Gradient Norm: 35.24987320965473
Epoch: 867, Batch Gradient Norm after: 22.36067707159255
Epoch 868/10000, Prediction Accuracy = 46.13%, Loss = 1.786229681968689
Epoch: 868, Batch Gradient Norm: 31.31818391402879
Epoch: 868, Batch Gradient Norm after: 22.30421993606889
Epoch 869/10000, Prediction Accuracy = 46.106%, Loss = 1.7727985858917237
Epoch: 869, Batch Gradient Norm: 35.16654797319358
Epoch: 869, Batch Gradient Norm after: 22.36067777281279
Epoch 870/10000, Prediction Accuracy = 46.138%, Loss = 1.784192395210266
Epoch: 870, Batch Gradient Norm: 31.372667634373762
Epoch: 870, Batch Gradient Norm after: 22.346275364890488
Epoch 871/10000, Prediction Accuracy = 46.11%, Loss = 1.771235418319702
Epoch: 871, Batch Gradient Norm: 35.319603636165866
Epoch: 871, Batch Gradient Norm after: 22.360677073635944
Epoch 872/10000, Prediction Accuracy = 46.141999999999996%, Loss = 1.7830054759979248
Epoch: 872, Batch Gradient Norm: 31.418455281736456
Epoch: 872, Batch Gradient Norm after: 22.300640278154123
Epoch 873/10000, Prediction Accuracy = 46.108%, Loss = 1.7696744441986083
Epoch: 873, Batch Gradient Norm: 35.19831639133599
Epoch: 873, Batch Gradient Norm after: 22.3606796985645
Epoch 874/10000, Prediction Accuracy = 46.15%, Loss = 1.7808493852615357
Epoch: 874, Batch Gradient Norm: 31.434366687035418
Epoch: 874, Batch Gradient Norm after: 22.360676971251234
Epoch 875/10000, Prediction Accuracy = 46.116%, Loss = 1.7679893016815185
Epoch: 875, Batch Gradient Norm: 35.440119348791825
Epoch: 875, Batch Gradient Norm after: 22.360679184496213
Epoch 876/10000, Prediction Accuracy = 46.160000000000004%, Loss = 1.7799755096435548
Epoch: 876, Batch Gradient Norm: 31.442720993603288
Epoch: 876, Batch Gradient Norm after: 22.316046663167576
Epoch 877/10000, Prediction Accuracy = 46.108%, Loss = 1.766342830657959
Epoch: 877, Batch Gradient Norm: 35.42104231333668
Epoch: 877, Batch Gradient Norm after: 22.360677078981283
Epoch 878/10000, Prediction Accuracy = 46.162%, Loss = 1.7781880378723145
Epoch: 878, Batch Gradient Norm: 31.520147503624575
Epoch: 878, Batch Gradient Norm after: 22.31566458922069
Epoch 879/10000, Prediction Accuracy = 46.122%, Loss = 1.7648718357086182
Epoch: 879, Batch Gradient Norm: 35.40034555643786
Epoch: 879, Batch Gradient Norm after: 22.36067577505504
Epoch 880/10000, Prediction Accuracy = 46.17%, Loss = 1.7763858079910277
Epoch: 880, Batch Gradient Norm: 31.54433030065557
Epoch: 880, Batch Gradient Norm after: 22.354427115338023
Epoch 881/10000, Prediction Accuracy = 46.132%, Loss = 1.763229250907898
Epoch: 881, Batch Gradient Norm: 35.593437830636425
Epoch: 881, Batch Gradient Norm after: 22.36067915664009
Epoch 882/10000, Prediction Accuracy = 46.172000000000004%, Loss = 1.7753539323806762
Epoch: 882, Batch Gradient Norm: 31.551931715428935
Epoch: 882, Batch Gradient Norm after: 22.310254999484222
Epoch 883/10000, Prediction Accuracy = 46.144%, Loss = 1.76157386302948
Epoch: 883, Batch Gradient Norm: 35.53495966210236
Epoch: 883, Batch Gradient Norm after: 22.360677234563255
Epoch 884/10000, Prediction Accuracy = 46.17400000000001%, Loss = 1.7734416484832765
Epoch: 884, Batch Gradient Norm: 31.618637020394473
Epoch: 884, Batch Gradient Norm after: 22.330889110140458
Epoch 885/10000, Prediction Accuracy = 46.158%, Loss = 1.760071873664856
Epoch: 885, Batch Gradient Norm: 35.60202566480048
Epoch: 885, Batch Gradient Norm after: 22.360677387075718
Epoch 886/10000, Prediction Accuracy = 46.169999999999995%, Loss = 1.7719714879989623
Epoch: 886, Batch Gradient Norm: 31.651543348952263
Epoch: 886, Batch Gradient Norm after: 22.32448547257208
Epoch 887/10000, Prediction Accuracy = 46.166%, Loss = 1.7584911823272704
Epoch: 887, Batch Gradient Norm: 35.63790071616695
Epoch: 887, Batch Gradient Norm after: 22.360676684271116
Epoch 888/10000, Prediction Accuracy = 46.174%, Loss = 1.7703678846359252
Epoch: 888, Batch Gradient Norm: 31.686252649282363
Epoch: 888, Batch Gradient Norm after: 22.330495074955408
Epoch 889/10000, Prediction Accuracy = 46.162%, Loss = 1.7569039106369018
Epoch: 889, Batch Gradient Norm: 35.70884118036897
Epoch: 889, Batch Gradient Norm after: 22.36067763041468
Epoch 890/10000, Prediction Accuracy = 46.184%, Loss = 1.768905472755432
Epoch: 890, Batch Gradient Norm: 31.720068164429378
Epoch: 890, Batch Gradient Norm after: 22.324021053574846
Epoch 891/10000, Prediction Accuracy = 46.168%, Loss = 1.7553272485733031
Epoch: 891, Batch Gradient Norm: 35.74056958843677
Epoch: 891, Batch Gradient Norm after: 22.360678131929884
Epoch 892/10000, Prediction Accuracy = 46.181999999999995%, Loss = 1.7673089742660522
Epoch: 892, Batch Gradient Norm: 31.753963583832753
Epoch: 892, Batch Gradient Norm after: 22.326162199338906
Epoch 893/10000, Prediction Accuracy = 46.164%, Loss = 1.7537424325942994
Epoch: 893, Batch Gradient Norm: 35.79959427102068
Epoch: 893, Batch Gradient Norm after: 22.360677219861635
Epoch 894/10000, Prediction Accuracy = 46.19199999999999%, Loss = 1.7658098936080933
Epoch: 894, Batch Gradient Norm: 31.78696747222401
Epoch: 894, Batch Gradient Norm after: 22.3279096080113
Epoch 895/10000, Prediction Accuracy = 46.17%, Loss = 1.752164602279663
Epoch: 895, Batch Gradient Norm: 35.85772737038876
Epoch: 895, Batch Gradient Norm after: 22.360677355253735
Epoch 896/10000, Prediction Accuracy = 46.202%, Loss = 1.7643241167068482
Epoch: 896, Batch Gradient Norm: 31.820243220141904
Epoch: 896, Batch Gradient Norm after: 22.322846057144424
Epoch 897/10000, Prediction Accuracy = 46.196000000000005%, Loss = 1.7505958557128907
Epoch: 897, Batch Gradient Norm: 35.887552712074935
Epoch: 897, Batch Gradient Norm after: 22.36067787388462
Epoch 898/10000, Prediction Accuracy = 46.206%, Loss = 1.7627254486083985
Epoch: 898, Batch Gradient Norm: 31.85871512236286
Epoch: 898, Batch Gradient Norm after: 22.32784023107199
Epoch 899/10000, Prediction Accuracy = 46.18999999999999%, Loss = 1.7490307331085204
Epoch: 899, Batch Gradient Norm: 35.95522930205901
Epoch: 899, Batch Gradient Norm after: 22.36067842826704
Epoch 900/10000, Prediction Accuracy = 46.234%, Loss = 1.7612642049789429
Epoch: 900, Batch Gradient Norm: 31.89494808316792
Epoch: 900, Batch Gradient Norm after: 22.32509388076302
Epoch 901/10000, Prediction Accuracy = 46.204%, Loss = 1.747471523284912
Epoch: 901, Batch Gradient Norm: 35.987919654343735
Epoch: 901, Batch Gradient Norm after: 22.36067596016785
Epoch 902/10000, Prediction Accuracy = 46.26%, Loss = 1.7596881866455079
Epoch: 902, Batch Gradient Norm: 31.92812034460088
Epoch: 902, Batch Gradient Norm after: 22.33366162217169
Epoch 903/10000, Prediction Accuracy = 46.22%, Loss = 1.745917010307312
Epoch: 903, Batch Gradient Norm: 36.05845095597512
Epoch: 903, Batch Gradient Norm after: 22.360677091153867
Epoch 904/10000, Prediction Accuracy = 46.262%, Loss = 1.7582630395889283
Epoch: 904, Batch Gradient Norm: 31.964591771020853
Epoch: 904, Batch Gradient Norm after: 22.328753592029923
Epoch 905/10000, Prediction Accuracy = 46.22%, Loss = 1.744367504119873
Epoch: 905, Batch Gradient Norm: 36.09059647756146
Epoch: 905, Batch Gradient Norm after: 22.360678641405304
Epoch 906/10000, Prediction Accuracy = 46.276%, Loss = 1.7566752672195434
Epoch: 906, Batch Gradient Norm: 31.998781518124613
Epoch: 906, Batch Gradient Norm after: 22.33637750004919
Epoch 907/10000, Prediction Accuracy = 46.248000000000005%, Loss = 1.7428123235702515
Epoch: 907, Batch Gradient Norm: 36.16841798976143
Epoch: 907, Batch Gradient Norm after: 22.360677640025177
Epoch 908/10000, Prediction Accuracy = 46.282000000000004%, Loss = 1.7552674293518067
Epoch: 908, Batch Gradient Norm: 32.03545138611756
Epoch: 908, Batch Gradient Norm after: 22.327534266481575
Epoch 909/10000, Prediction Accuracy = 46.269999999999996%, Loss = 1.741285228729248
Epoch: 909, Batch Gradient Norm: 36.177325908430454
Epoch: 909, Batch Gradient Norm after: 22.36067665975078
Epoch 910/10000, Prediction Accuracy = 46.3%, Loss = 1.7536235332489014
Epoch: 910, Batch Gradient Norm: 32.06372784339814
Epoch: 910, Batch Gradient Norm after: 22.35106050290899
Epoch 911/10000, Prediction Accuracy = 46.284%, Loss = 1.7397059202194214
Epoch: 911, Batch Gradient Norm: 36.32218179607279
Epoch: 911, Batch Gradient Norm after: 22.360678534112278
Epoch 912/10000, Prediction Accuracy = 46.312%, Loss = 1.7524653673171997
Epoch: 912, Batch Gradient Norm: 32.07739070962043
Epoch: 912, Batch Gradient Norm after: 22.32783678251826
Epoch 913/10000, Prediction Accuracy = 46.29600000000001%, Loss = 1.7381027221679688
Epoch: 913, Batch Gradient Norm: 36.325411225245624
Epoch: 913, Batch Gradient Norm after: 22.360676367126114
Epoch 914/10000, Prediction Accuracy = 46.31%, Loss = 1.7508238554000854
Epoch: 914, Batch Gradient Norm: 32.13830608364971
Epoch: 914, Batch Gradient Norm after: 22.328698559899948
Epoch 915/10000, Prediction Accuracy = 46.306000000000004%, Loss = 1.7366591215133667
Epoch: 915, Batch Gradient Norm: 36.324216677857756
Epoch: 915, Batch Gradient Norm after: 22.36067892767476
Epoch 916/10000, Prediction Accuracy = 46.316%, Loss = 1.7491563320159913
Epoch: 916, Batch Gradient Norm: 32.1644924940356
Epoch: 916, Batch Gradient Norm after: 22.348266465827553
Epoch 917/10000, Prediction Accuracy = 46.322%, Loss = 1.735104513168335
Epoch: 917, Batch Gradient Norm: 36.45065496482884
Epoch: 917, Batch Gradient Norm after: 22.3606778853095
Epoch 918/10000, Prediction Accuracy = 46.321999999999996%, Loss = 1.7479376077651978
Epoch: 918, Batch Gradient Norm: 32.19302920013966
Epoch: 918, Batch Gradient Norm after: 22.32449419580423
Epoch 919/10000, Prediction Accuracy = 46.318%, Loss = 1.7335665225982666
Epoch: 919, Batch Gradient Norm: 36.423440459379464
Epoch: 919, Batch Gradient Norm after: 22.36067781986782
Epoch 920/10000, Prediction Accuracy = 46.338%, Loss = 1.746181583404541
Epoch: 920, Batch Gradient Norm: 32.23004867092043
Epoch: 920, Batch Gradient Norm after: 22.350850628726505
Epoch 921/10000, Prediction Accuracy = 46.318%, Loss = 1.7320391416549683
Epoch: 921, Batch Gradient Norm: 36.56655564970295
Epoch: 921, Batch Gradient Norm after: 22.360680423121327
Epoch 922/10000, Prediction Accuracy = 46.348%, Loss = 1.7450294494628906
Epoch: 922, Batch Gradient Norm: 32.226749435013026
Epoch: 922, Batch Gradient Norm after: 22.33916588887185
Epoch 923/10000, Prediction Accuracy = 46.32%, Loss = 1.730411171913147
Epoch: 923, Batch Gradient Norm: 36.63276620393584
Epoch: 923, Batch Gradient Norm after: 22.360678397550533
Epoch 924/10000, Prediction Accuracy = 46.366%, Loss = 1.7436344861984252
Epoch: 924, Batch Gradient Norm: 32.24445809427921
Epoch: 924, Batch Gradient Norm after: 22.336259998874528
Epoch 925/10000, Prediction Accuracy = 46.324%, Loss = 1.7288314580917359
Epoch: 925, Batch Gradient Norm: 36.69392952301843
Epoch: 925, Batch Gradient Norm after: 22.360677970557337
Epoch 926/10000, Prediction Accuracy = 46.378%, Loss = 1.742208766937256
Epoch: 926, Batch Gradient Norm: 32.25906263277409
Epoch: 926, Batch Gradient Norm after: 22.329281066521414
Epoch 927/10000, Prediction Accuracy = 46.32000000000001%, Loss = 1.7272588729858398
Epoch: 927, Batch Gradient Norm: 36.71731762724902
Epoch: 927, Batch Gradient Norm after: 22.36067653995379
Epoch 928/10000, Prediction Accuracy = 46.388%, Loss = 1.7406485080718994
Epoch: 928, Batch Gradient Norm: 32.32430196991942
Epoch: 928, Batch Gradient Norm after: 22.33083760017181
Epoch 929/10000, Prediction Accuracy = 46.333999999999996%, Loss = 1.725847315788269
Epoch: 929, Batch Gradient Norm: 36.76389805626638
Epoch: 929, Batch Gradient Norm after: 22.360675718549913
Epoch 930/10000, Prediction Accuracy = 46.402%, Loss = 1.7391773223876954
Epoch: 930, Batch Gradient Norm: 32.35688764603546
Epoch: 930, Batch Gradient Norm after: 22.330813461772166
Epoch 931/10000, Prediction Accuracy = 46.321999999999996%, Loss = 1.7243363380432128
Epoch: 931, Batch Gradient Norm: 36.824253921941136
Epoch: 931, Batch Gradient Norm after: 22.360675754687133
Epoch 932/10000, Prediction Accuracy = 46.398%, Loss = 1.737747359275818
Epoch: 932, Batch Gradient Norm: 32.374008927011054
Epoch: 932, Batch Gradient Norm after: 22.332442802837456
Epoch 933/10000, Prediction Accuracy = 46.338%, Loss = 1.7227784395217896
Epoch: 933, Batch Gradient Norm: 36.898863926608094
Epoch: 933, Batch Gradient Norm after: 22.360679761957236
Epoch 934/10000, Prediction Accuracy = 46.410000000000004%, Loss = 1.736379289627075
Epoch: 934, Batch Gradient Norm: 32.38154154315786
Epoch: 934, Batch Gradient Norm after: 22.32251128491554
Epoch 935/10000, Prediction Accuracy = 46.348%, Loss = 1.721190595626831
Epoch: 935, Batch Gradient Norm: 36.9115497715762
Epoch: 935, Batch Gradient Norm after: 22.3606760242196
Epoch 936/10000, Prediction Accuracy = 46.414%, Loss = 1.7348129749298096
Epoch: 936, Batch Gradient Norm: 32.45720808125196
Epoch: 936, Batch Gradient Norm after: 22.325082001462487
Epoch 937/10000, Prediction Accuracy = 46.348%, Loss = 1.719830870628357
Epoch: 937, Batch Gradient Norm: 36.951434823366675
Epoch: 937, Batch Gradient Norm after: 22.36067667086413
Epoch 938/10000, Prediction Accuracy = 46.416000000000004%, Loss = 1.7333208799362183
Epoch: 938, Batch Gradient Norm: 32.494777403348266
Epoch: 938, Batch Gradient Norm after: 22.325327578749224
Epoch 939/10000, Prediction Accuracy = 46.366%, Loss = 1.7183570384979248
Epoch: 939, Batch Gradient Norm: 36.98440211865938
Epoch: 939, Batch Gradient Norm after: 22.360677179898303
Epoch 940/10000, Prediction Accuracy = 46.42%, Loss = 1.7318182706832885
Epoch: 940, Batch Gradient Norm: 32.55375314911751
Epoch: 940, Batch Gradient Norm after: 22.319131052925094
Epoch 941/10000, Prediction Accuracy = 46.386%, Loss = 1.7169425249099732
Epoch: 941, Batch Gradient Norm: 36.96846791389643
Epoch: 941, Batch Gradient Norm after: 22.360677724276908
Epoch 942/10000, Prediction Accuracy = 46.436%, Loss = 1.7301507711410522
Epoch: 942, Batch Gradient Norm: 32.61662320625972
Epoch: 942, Batch Gradient Norm after: 22.32425237229446
Epoch 943/10000, Prediction Accuracy = 46.4%, Loss = 1.7155462503433228
Epoch: 943, Batch Gradient Norm: 36.98554420101125
Epoch: 943, Batch Gradient Norm after: 22.36067601810078
Epoch 944/10000, Prediction Accuracy = 46.43599999999999%, Loss = 1.72859046459198
Epoch: 944, Batch Gradient Norm: 32.64737095178624
Epoch: 944, Batch Gradient Norm after: 22.335771875568504
Epoch 945/10000, Prediction Accuracy = 46.426%, Loss = 1.7140505075454713
Epoch: 945, Batch Gradient Norm: 37.083770866872165
Epoch: 945, Batch Gradient Norm after: 22.3606780328828
Epoch 946/10000, Prediction Accuracy = 46.442%, Loss = 1.7273338556289672
Epoch: 946, Batch Gradient Norm: 32.688558308173434
Epoch: 946, Batch Gradient Norm after: 22.31019710017926
Epoch 947/10000, Prediction Accuracy = 46.444%, Loss = 1.7126144886016845
Epoch: 947, Batch Gradient Norm: 37.01902331603851
Epoch: 947, Batch Gradient Norm after: 22.360679861209974
Epoch 948/10000, Prediction Accuracy = 46.44199999999999%, Loss = 1.7255184412002564
Epoch: 948, Batch Gradient Norm: 32.700968400467325
Epoch: 948, Batch Gradient Norm after: 22.36067792184298
Epoch 949/10000, Prediction Accuracy = 46.43599999999999%, Loss = 1.7110752582550048
Epoch: 949, Batch Gradient Norm: 37.268296844303684
Epoch: 949, Batch Gradient Norm after: 22.360677341883953
Epoch 950/10000, Prediction Accuracy = 46.459999999999994%, Loss = 1.7247889518737793
Epoch: 950, Batch Gradient Norm: 32.663209329200136
Epoch: 950, Batch Gradient Norm after: 22.327254144111844
Epoch 951/10000, Prediction Accuracy = 46.45%, Loss = 1.709397315979004
Epoch: 951, Batch Gradient Norm: 37.29364212883232
Epoch: 951, Batch Gradient Norm after: 22.360679273898484
Epoch 952/10000, Prediction Accuracy = 46.474%, Loss = 1.7233027696609498
Epoch: 952, Batch Gradient Norm: 32.71948772086988
Epoch: 952, Batch Gradient Norm after: 22.328817106366394
Epoch 953/10000, Prediction Accuracy = 46.462%, Loss = 1.7080146074295044
Epoch: 953, Batch Gradient Norm: 37.34027217625013
Epoch: 953, Batch Gradient Norm after: 22.360677138982783
Epoch 954/10000, Prediction Accuracy = 46.480000000000004%, Loss = 1.721879768371582
Epoch: 954, Batch Gradient Norm: 32.75755643062387
Epoch: 954, Batch Gradient Norm after: 22.33233568980665
Epoch 955/10000, Prediction Accuracy = 46.472%, Loss = 1.706554913520813
Epoch: 955, Batch Gradient Norm: 37.38920149849068
Epoch: 955, Batch Gradient Norm after: 22.360679536776814
Epoch 956/10000, Prediction Accuracy = 46.478%, Loss = 1.7204663515090943
Epoch: 956, Batch Gradient Norm: 32.783051127579654
Epoch: 956, Batch Gradient Norm after: 22.33515901557002
Epoch 957/10000, Prediction Accuracy = 46.474000000000004%, Loss = 1.705067229270935
Epoch: 957, Batch Gradient Norm: 37.460906921579586
Epoch: 957, Batch Gradient Norm after: 22.360676886017878
Epoch 958/10000, Prediction Accuracy = 46.466%, Loss = 1.7191231489181518
Epoch: 958, Batch Gradient Norm: 32.789307629918454
Epoch: 958, Batch Gradient Norm after: 22.325955091854684
Epoch 959/10000, Prediction Accuracy = 46.482000000000006%, Loss = 1.7035329341888428
Epoch: 959, Batch Gradient Norm: 37.471931518868644
Epoch: 959, Batch Gradient Norm after: 22.36067859906071
Epoch 960/10000, Prediction Accuracy = 46.458000000000006%, Loss = 1.7175833940505982
Epoch: 960, Batch Gradient Norm: 32.87349004454035
Epoch: 960, Batch Gradient Norm after: 22.324694624509505
Epoch 961/10000, Prediction Accuracy = 46.489999999999995%, Loss = 1.7022422313690186
Epoch: 961, Batch Gradient Norm: 37.47776429091839
Epoch: 961, Batch Gradient Norm after: 22.360677830911637
Epoch 962/10000, Prediction Accuracy = 46.454%, Loss = 1.7160215377807617
Epoch: 962, Batch Gradient Norm: 32.955718873639626
Epoch: 962, Batch Gradient Norm after: 22.31261812889088
Epoch 963/10000, Prediction Accuracy = 46.5%, Loss = 1.700950288772583
Epoch: 963, Batch Gradient Norm: 37.397578978897776
Epoch: 963, Batch Gradient Norm after: 22.3606777722406
Epoch 964/10000, Prediction Accuracy = 46.45799999999999%, Loss = 1.7141632556915283
Epoch: 964, Batch Gradient Norm: 32.96186809338626
Epoch: 964, Batch Gradient Norm after: 22.360678011936034
Epoch 965/10000, Prediction Accuracy = 46.513999999999996%, Loss = 1.699421262741089
Epoch: 965, Batch Gradient Norm: 37.61582780169349
Epoch: 965, Batch Gradient Norm after: 22.36067705011456
Epoch 966/10000, Prediction Accuracy = 46.46%, Loss = 1.71338050365448
Epoch: 966, Batch Gradient Norm: 32.96666073850595
Epoch: 966, Batch Gradient Norm after: 22.334933632659634
Epoch 967/10000, Prediction Accuracy = 46.513999999999996%, Loss = 1.6979178667068482
Epoch: 967, Batch Gradient Norm: 37.65311839381624
Epoch: 967, Batch Gradient Norm after: 22.360678125761574
Epoch 968/10000, Prediction Accuracy = 46.462%, Loss = 1.7119528532028199
Epoch: 968, Batch Gradient Norm: 33.02130115123429
Epoch: 968, Batch Gradient Norm after: 22.33286002633227
Epoch 969/10000, Prediction Accuracy = 46.528%, Loss = 1.6965399503707885
Epoch: 969, Batch Gradient Norm: 37.651280572335224
Epoch: 969, Batch Gradient Norm after: 22.360676872805733
Epoch 970/10000, Prediction Accuracy = 46.468%, Loss = 1.7104009628295898
Epoch: 970, Batch Gradient Norm: 33.08379011094266
Epoch: 970, Batch Gradient Norm after: 22.3309403426821
Epoch 971/10000, Prediction Accuracy = 46.53000000000001%, Loss = 1.695195746421814
Epoch: 971, Batch Gradient Norm: 37.6424537474712
Epoch: 971, Batch Gradient Norm after: 22.360679202426617
Epoch 972/10000, Prediction Accuracy = 46.476%, Loss = 1.7088217258453369
Epoch: 972, Batch Gradient Norm: 33.10901321670818
Epoch: 972, Batch Gradient Norm after: 22.354690973010477
Epoch 973/10000, Prediction Accuracy = 46.525999999999996%, Loss = 1.6937370538711547
Epoch: 973, Batch Gradient Norm: 37.7923517448035
Epoch: 973, Batch Gradient Norm after: 22.360677758177005
Epoch 974/10000, Prediction Accuracy = 46.472%, Loss = 1.7078048467636109
Epoch: 974, Batch Gradient Norm: 33.1192479573507
Epoch: 974, Batch Gradient Norm after: 22.33423003420677
Epoch 975/10000, Prediction Accuracy = 46.54600000000001%, Loss = 1.6922494649887085
Epoch: 975, Batch Gradient Norm: 37.806717296807165
Epoch: 975, Batch Gradient Norm after: 22.36067838610606
Epoch 976/10000, Prediction Accuracy = 46.464%, Loss = 1.706308937072754
Epoch: 976, Batch Gradient Norm: 33.186034614927884
Epoch: 976, Batch Gradient Norm after: 22.323345708495427
Epoch 977/10000, Prediction Accuracy = 46.56%, Loss = 1.6909372329711914
Epoch: 977, Batch Gradient Norm: 37.75560445051929
Epoch: 977, Batch Gradient Norm after: 22.36067728152874
Epoch 978/10000, Prediction Accuracy = 46.47%, Loss = 1.7045913934707642
Epoch: 978, Batch Gradient Norm: 33.20497136637504
Epoch: 978, Batch Gradient Norm after: 22.360678015611917
Epoch 979/10000, Prediction Accuracy = 46.562%, Loss = 1.6894644260406495
Epoch: 979, Batch Gradient Norm: 37.959723047572396
Epoch: 979, Batch Gradient Norm after: 22.36067897111107
Epoch 980/10000, Prediction Accuracy = 46.474%, Loss = 1.7037787437438965
Epoch: 980, Batch Gradient Norm: 33.16366154478894
Epoch: 980, Batch Gradient Norm after: 22.329249251421114
Epoch 981/10000, Prediction Accuracy = 46.571999999999996%, Loss = 1.6878260612487792
Epoch: 981, Batch Gradient Norm: 37.97005539795116
Epoch: 981, Batch Gradient Norm after: 22.36067818182452
Epoch 982/10000, Prediction Accuracy = 46.480000000000004%, Loss = 1.702272152900696
Epoch: 982, Batch Gradient Norm: 33.25013396380632
Epoch: 982, Batch Gradient Norm after: 22.33030979866628
Epoch 983/10000, Prediction Accuracy = 46.56999999999999%, Loss = 1.686599349975586
Epoch: 983, Batch Gradient Norm: 37.984412209809406
Epoch: 983, Batch Gradient Norm after: 22.360677872424112
Epoch 984/10000, Prediction Accuracy = 46.476%, Loss = 1.7007769346237183
Epoch: 984, Batch Gradient Norm: 33.31147422340578
Epoch: 984, Batch Gradient Norm after: 22.328123276638507
Epoch 985/10000, Prediction Accuracy = 46.574%, Loss = 1.6852752685546875
Epoch: 985, Batch Gradient Norm: 37.97113564900851
Epoch: 985, Batch Gradient Norm after: 22.360679099041008
Epoch 986/10000, Prediction Accuracy = 46.48199999999999%, Loss = 1.6991990804672241
Epoch: 986, Batch Gradient Norm: 33.333946608540174
Epoch: 986, Batch Gradient Norm after: 22.355758413344837
Epoch 987/10000, Prediction Accuracy = 46.565999999999995%, Loss = 1.6838262319564818
Epoch: 987, Batch Gradient Norm: 38.14212559080148
Epoch: 987, Batch Gradient Norm after: 22.360677815139976
Epoch 988/10000, Prediction Accuracy = 46.492000000000004%, Loss = 1.6982748746871947
Epoch: 988, Batch Gradient Norm: 33.318901184870604
Epoch: 988, Batch Gradient Norm after: 22.338510470768792
Epoch 989/10000, Prediction Accuracy = 46.562%, Loss = 1.682280921936035
Epoch: 989, Batch Gradient Norm: 38.19452077643815
Epoch: 989, Batch Gradient Norm after: 22.36067851972921
Epoch 990/10000, Prediction Accuracy = 46.492000000000004%, Loss = 1.6969412088394165
Epoch: 990, Batch Gradient Norm: 33.3454671672668
Epoch: 990, Batch Gradient Norm after: 22.336428710412743
Epoch 991/10000, Prediction Accuracy = 46.562%, Loss = 1.680859637260437
Epoch: 991, Batch Gradient Norm: 38.2255975084137
Epoch: 991, Batch Gradient Norm after: 22.360676289932275
Epoch 992/10000, Prediction Accuracy = 46.506%, Loss = 1.6955408096313476
Epoch: 992, Batch Gradient Norm: 33.39471118860769
Epoch: 992, Batch Gradient Norm after: 22.341883402291394
Epoch 993/10000, Prediction Accuracy = 46.568%, Loss = 1.6795297145843506
Epoch: 993, Batch Gradient Norm: 38.29495878520797
Epoch: 993, Batch Gradient Norm after: 22.36067811129701
Epoch 994/10000, Prediction Accuracy = 46.516%, Loss = 1.694263505935669
Epoch: 994, Batch Gradient Norm: 33.394710288240596
Epoch: 994, Batch Gradient Norm after: 22.332281703213837
Epoch 995/10000, Prediction Accuracy = 46.562%, Loss = 1.6780403852462769
Epoch: 995, Batch Gradient Norm: 38.28469831271632
Epoch: 995, Batch Gradient Norm after: 22.360678105554573
Epoch 996/10000, Prediction Accuracy = 46.514%, Loss = 1.6927152872085571
Epoch: 996, Batch Gradient Norm: 33.505150663696995
Epoch: 996, Batch Gradient Norm after: 22.330004587494336
Epoch 997/10000, Prediction Accuracy = 46.576%, Loss = 1.676898217201233
Epoch: 997, Batch Gradient Norm: 38.25052976613854
Epoch: 997, Batch Gradient Norm after: 22.36067747537369
Epoch 998/10000, Prediction Accuracy = 46.524%, Loss = 1.6910918474197387
Epoch: 998, Batch Gradient Norm: 33.526743106644034
Epoch: 998, Batch Gradient Norm after: 22.360679069225394
Epoch 999/10000, Prediction Accuracy = 46.596%, Loss = 1.675481128692627
Epoch: 999, Batch Gradient Norm: 38.41304643638381
Epoch: 999, Batch Gradient Norm after: 22.360678946346155
Epoch 1000/10000, Prediction Accuracy = 46.522000000000006%, Loss = 1.6901712656021117
Epoch: 1000, Batch Gradient Norm: 33.517599992544646
Epoch: 1000, Batch Gradient Norm after: 22.34206663190218
Epoch 1001/10000, Prediction Accuracy = 46.608%, Loss = 1.6739723682403564
Epoch: 1001, Batch Gradient Norm: 38.444880601919685
Epoch: 1001, Batch Gradient Norm after: 22.360677130421738
Epoch 1002/10000, Prediction Accuracy = 46.536%, Loss = 1.688783097267151
Epoch: 1002, Batch Gradient Norm: 33.567193518411415
Epoch: 1002, Batch Gradient Norm after: 22.34417437351303
Epoch 1003/10000, Prediction Accuracy = 46.618%, Loss = 1.6726485252380372
Epoch: 1003, Batch Gradient Norm: 38.48326636678586
Epoch: 1003, Batch Gradient Norm after: 22.360678337655088
Epoch 1004/10000, Prediction Accuracy = 46.536%, Loss = 1.6874216079711915
Epoch: 1004, Batch Gradient Norm: 33.612168966578146
Epoch: 1004, Batch Gradient Norm after: 22.335081369565103
Epoch 1005/10000, Prediction Accuracy = 46.606%, Loss = 1.6713046789169312
Epoch: 1005, Batch Gradient Norm: 38.48329673574862
Epoch: 1005, Batch Gradient Norm after: 22.36067713456199
Epoch 1006/10000, Prediction Accuracy = 46.519999999999996%, Loss = 1.6859137773513795
Epoch: 1006, Batch Gradient Norm: 33.67151634914253
Epoch: 1006, Batch Gradient Norm after: 22.33759040408269
Epoch 1007/10000, Prediction Accuracy = 46.622%, Loss = 1.669999861717224
Epoch: 1007, Batch Gradient Norm: 38.48785624449153
Epoch: 1007, Batch Gradient Norm after: 22.360677529526974
Epoch 1008/10000, Prediction Accuracy = 46.532%, Loss = 1.6844375371932983
Epoch: 1008, Batch Gradient Norm: 33.705447187162314
Epoch: 1008, Batch Gradient Norm after: 22.34979677160529
Epoch 1009/10000, Prediction Accuracy = 46.628%, Loss = 1.6686262845993043
Epoch: 1009, Batch Gradient Norm: 38.589908090192715
Epoch: 1009, Batch Gradient Norm after: 22.360678879595657
Epoch 1010/10000, Prediction Accuracy = 46.529999999999994%, Loss = 1.6833085775375367
Epoch: 1010, Batch Gradient Norm: 33.74865356564231
Epoch: 1010, Batch Gradient Norm after: 22.325176740802775
Epoch 1011/10000, Prediction Accuracy = 46.63600000000001%, Loss = 1.6673100471496582
Epoch: 1011, Batch Gradient Norm: 38.51617552061924
Epoch: 1011, Batch Gradient Norm after: 22.360679508058826
Epoch 1012/10000, Prediction Accuracy = 46.51800000000001%, Loss = 1.6815484046936036
Epoch: 1012, Batch Gradient Norm: 33.74910565771215
Epoch: 1012, Batch Gradient Norm after: 22.360679200661934
Epoch 1013/10000, Prediction Accuracy = 46.644%, Loss = 1.6658283948898316
Epoch: 1013, Batch Gradient Norm: 38.66669780024383
Epoch: 1013, Batch Gradient Norm after: 22.360677870881155
Epoch 1014/10000, Prediction Accuracy = 46.506%, Loss = 1.680610227584839
Epoch: 1014, Batch Gradient Norm: 33.801383441727964
Epoch: 1014, Batch Gradient Norm after: 22.34341219400414
Epoch 1015/10000, Prediction Accuracy = 46.656%, Loss = 1.6645412921905518
Epoch: 1015, Batch Gradient Norm: 38.69711648226017
Epoch: 1015, Batch Gradient Norm after: 22.360678746839103
Epoch 1016/10000, Prediction Accuracy = 46.514%, Loss = 1.679235076904297
Epoch: 1016, Batch Gradient Norm: 33.82747876497388
Epoch: 1016, Batch Gradient Norm after: 22.35652176907415
Epoch 1017/10000, Prediction Accuracy = 46.65%, Loss = 1.6631603002548219
Epoch: 1017, Batch Gradient Norm: 38.80506398130091
Epoch: 1017, Batch Gradient Norm after: 22.360678587895553
Epoch 1018/10000, Prediction Accuracy = 46.525999999999996%, Loss = 1.6781301259994508
Epoch: 1018, Batch Gradient Norm: 33.84140584203735
Epoch: 1018, Batch Gradient Norm after: 22.349861983837123
Epoch 1019/10000, Prediction Accuracy = 46.646%, Loss = 1.6617335081100464
Epoch: 1019, Batch Gradient Norm: 38.86825244419618
Epoch: 1019, Batch Gradient Norm after: 22.36067740482409
Epoch 1020/10000, Prediction Accuracy = 46.52400000000001%, Loss = 1.6768819332122802
Epoch: 1020, Batch Gradient Norm: 33.84715659927156
Epoch: 1020, Batch Gradient Norm after: 22.340887518809094
Epoch 1021/10000, Prediction Accuracy = 46.638%, Loss = 1.660299825668335
Epoch: 1021, Batch Gradient Norm: 38.87693296819514
Epoch: 1021, Batch Gradient Norm after: 22.360677333425713
Epoch 1022/10000, Prediction Accuracy = 46.53000000000001%, Loss = 1.6754524946212768
Epoch: 1022, Batch Gradient Norm: 33.91832753929191
Epoch: 1022, Batch Gradient Norm after: 22.347670068695834
Epoch 1023/10000, Prediction Accuracy = 46.64%, Loss = 1.6590689420700073
Epoch: 1023, Batch Gradient Norm: 38.93019630794466
Epoch: 1023, Batch Gradient Norm after: 22.360677062036817
Epoch 1024/10000, Prediction Accuracy = 46.541999999999994%, Loss = 1.6741748571395874
Epoch: 1024, Batch Gradient Norm: 33.94288915469875
Epoch: 1024, Batch Gradient Norm after: 22.34991829267914
Epoch 1025/10000, Prediction Accuracy = 46.636%, Loss = 1.6576933145523072
Epoch: 1025, Batch Gradient Norm: 39.00858585452766
Epoch: 1025, Batch Gradient Norm after: 22.360677799010638
Epoch 1026/10000, Prediction Accuracy = 46.554%, Loss = 1.6729909420013427
Epoch: 1026, Batch Gradient Norm: 33.91843038933618
Epoch: 1026, Batch Gradient Norm after: 22.33007841463252
Epoch 1027/10000, Prediction Accuracy = 46.636%, Loss = 1.656175136566162
Epoch: 1027, Batch Gradient Norm: 38.9678104445258
Epoch: 1027, Batch Gradient Norm after: 22.36067656469095
Epoch 1028/10000, Prediction Accuracy = 46.564%, Loss = 1.6713977098464965
Epoch: 1028, Batch Gradient Norm: 34.04201220020288
Epoch: 1028, Batch Gradient Norm after: 22.34013671180948
Epoch 1029/10000, Prediction Accuracy = 46.634%, Loss = 1.6551332235336305
Epoch: 1029, Batch Gradient Norm: 39.00329887059787
Epoch: 1029, Batch Gradient Norm after: 22.360679220160545
Epoch 1030/10000, Prediction Accuracy = 46.564%, Loss = 1.6700591087341308
Epoch: 1030, Batch Gradient Norm: 34.07352484393168
Epoch: 1030, Batch Gradient Norm after: 22.338306825532328
Epoch 1031/10000, Prediction Accuracy = 46.624%, Loss = 1.6537865161895753
Epoch: 1031, Batch Gradient Norm: 39.04702046691758
Epoch: 1031, Batch Gradient Norm after: 22.360678642046675
Epoch 1032/10000, Prediction Accuracy = 46.576%, Loss = 1.6687739133834838
Epoch: 1032, Batch Gradient Norm: 34.10407896428366
Epoch: 1032, Batch Gradient Norm after: 22.34136384615495
Epoch 1033/10000, Prediction Accuracy = 46.63%, Loss = 1.652444887161255
Epoch: 1033, Batch Gradient Norm: 39.102573961702895
Epoch: 1033, Batch Gradient Norm after: 22.360677029580565
Epoch 1034/10000, Prediction Accuracy = 46.58%, Loss = 1.6675364255905152
Epoch: 1034, Batch Gradient Norm: 34.13775156056437
Epoch: 1034, Batch Gradient Norm after: 22.339809823864805
Epoch 1035/10000, Prediction Accuracy = 46.63%, Loss = 1.6511117219924927
Epoch: 1035, Batch Gradient Norm: 39.13718114836657
Epoch: 1035, Batch Gradient Norm after: 22.360678270882364
Epoch 1036/10000, Prediction Accuracy = 46.580000000000005%, Loss = 1.666205143928528
Epoch: 1036, Batch Gradient Norm: 34.160482432332984
Epoch: 1036, Batch Gradient Norm after: 22.348625530506336
Epoch 1037/10000, Prediction Accuracy = 46.644%, Loss = 1.649764633178711
Epoch: 1037, Batch Gradient Norm: 39.22842831814333
Epoch: 1037, Batch Gradient Norm after: 22.360675398758847
Epoch 1038/10000, Prediction Accuracy = 46.578%, Loss = 1.6650832414627075
Epoch: 1038, Batch Gradient Norm: 34.16092400923011
Epoch: 1038, Batch Gradient Norm after: 22.34905789969804
Epoch 1039/10000, Prediction Accuracy = 46.646%, Loss = 1.6483384609222411
Epoch: 1039, Batch Gradient Norm: 39.31739308993908
Epoch: 1039, Batch Gradient Norm after: 22.360675906116203
Epoch 1040/10000, Prediction Accuracy = 46.574%, Loss = 1.6639517068862915
Epoch: 1040, Batch Gradient Norm: 34.137350012449296
Epoch: 1040, Batch Gradient Norm after: 22.331764193284368
Epoch 1041/10000, Prediction Accuracy = 46.632%, Loss = 1.6468365430831908
Epoch: 1041, Batch Gradient Norm: 39.29693395635127
Epoch: 1041, Batch Gradient Norm after: 22.36067800525984
Epoch 1042/10000, Prediction Accuracy = 46.594%, Loss = 1.6624434471130372
Epoch: 1042, Batch Gradient Norm: 34.245661788411844
Epoch: 1042, Batch Gradient Norm after: 22.34793531513499
Epoch 1043/10000, Prediction Accuracy = 46.62200000000001%, Loss = 1.6457746267318725
Epoch: 1043, Batch Gradient Norm: 39.39289308200922
Epoch: 1043, Batch Gradient Norm after: 22.360677133091468
Epoch 1044/10000, Prediction Accuracy = 46.598%, Loss = 1.6613451004028321
Epoch: 1044, Batch Gradient Norm: 34.19522965930335
Epoch: 1044, Batch Gradient Norm after: 22.33162510030023
Epoch 1045/10000, Prediction Accuracy = 46.626000000000005%, Loss = 1.6442002534866333
Epoch: 1045, Batch Gradient Norm: 39.36847238380416
Epoch: 1045, Batch Gradient Norm after: 22.36067829157352
Epoch 1046/10000, Prediction Accuracy = 46.602%, Loss = 1.6598300218582154
Epoch: 1046, Batch Gradient Norm: 34.317051217922675
Epoch: 1046, Batch Gradient Norm after: 22.355366716503067
Epoch 1047/10000, Prediction Accuracy = 46.641999999999996%, Loss = 1.6431740999221802
Epoch: 1047, Batch Gradient Norm: 39.49806066914657
Epoch: 1047, Batch Gradient Norm after: 22.36067825347202
Epoch 1048/10000, Prediction Accuracy = 46.598%, Loss = 1.6588464498519897
Epoch: 1048, Batch Gradient Norm: 34.23050553583552
Epoch: 1048, Batch Gradient Norm after: 22.32239372456291
Epoch 1049/10000, Prediction Accuracy = 46.65%, Loss = 1.64148690700531
Epoch: 1049, Batch Gradient Norm: 39.39611941851089
Epoch: 1049, Batch Gradient Norm after: 22.36067771836318
Epoch 1050/10000, Prediction Accuracy = 46.598%, Loss = 1.6570646286010742
Epoch: 1050, Batch Gradient Norm: 34.363102708456864
Epoch: 1050, Batch Gradient Norm after: 22.36067632565353
Epoch 1051/10000, Prediction Accuracy = 46.651999999999994%, Loss = 1.6405020952224731
Epoch: 1051, Batch Gradient Norm: 39.56049444929915
Epoch: 1051, Batch Gradient Norm after: 22.36067742277972
Epoch 1052/10000, Prediction Accuracy = 46.608%, Loss = 1.6562088489532472
Epoch: 1052, Batch Gradient Norm: 34.314738377680584
Epoch: 1052, Batch Gradient Norm after: 22.33194801282372
Epoch 1053/10000, Prediction Accuracy = 46.658%, Loss = 1.6389524459838867
Epoch: 1053, Batch Gradient Norm: 39.52004808131887
Epoch: 1053, Batch Gradient Norm after: 22.360677153520605
Epoch 1054/10000, Prediction Accuracy = 46.60999999999999%, Loss = 1.6546498775482177
Epoch: 1054, Batch Gradient Norm: 34.42698439949568
Epoch: 1054, Batch Gradient Norm after: 22.360676228160692
Epoch 1055/10000, Prediction Accuracy = 46.664%, Loss = 1.6379077911376954
Epoch: 1055, Batch Gradient Norm: 39.66880847643465
Epoch: 1055, Batch Gradient Norm after: 22.360678032184687
Epoch 1056/10000, Prediction Accuracy = 46.61%, Loss = 1.653761076927185
Epoch: 1056, Batch Gradient Norm: 34.339214235248654
Epoch: 1056, Batch Gradient Norm after: 22.320583577189435
Epoch 1057/10000, Prediction Accuracy = 46.664%, Loss = 1.6362223148345947
Epoch: 1057, Batch Gradient Norm: 39.547227822288555
Epoch: 1057, Batch Gradient Norm after: 22.36067885538727
Epoch 1058/10000, Prediction Accuracy = 46.60600000000001%, Loss = 1.651922631263733
Epoch: 1058, Batch Gradient Norm: 34.48089389327657
Epoch: 1058, Batch Gradient Norm after: 22.36067758129445
Epoch 1059/10000, Prediction Accuracy = 46.676%, Loss = 1.6352713584899903
Epoch: 1059, Batch Gradient Norm: 39.70918625061156
Epoch: 1059, Batch Gradient Norm after: 22.360677840590192
Epoch 1060/10000, Prediction Accuracy = 46.612%, Loss = 1.6510717868804932
Epoch: 1060, Batch Gradient Norm: 34.45808267074818
Epoch: 1060, Batch Gradient Norm after: 22.33903783423893
Epoch 1061/10000, Prediction Accuracy = 46.681999999999995%, Loss = 1.6338078022003173
Epoch: 1061, Batch Gradient Norm: 39.71319257157569
Epoch: 1061, Batch Gradient Norm after: 22.360677915982404
Epoch 1062/10000, Prediction Accuracy = 46.60600000000001%, Loss = 1.649677610397339
Epoch: 1062, Batch Gradient Norm: 34.540136325748875
Epoch: 1062, Batch Gradient Norm after: 22.356152777514204
Epoch 1063/10000, Prediction Accuracy = 46.682%, Loss = 1.6326913118362427
Epoch: 1063, Batch Gradient Norm: 39.83784193635762
Epoch: 1063, Batch Gradient Norm after: 22.36067901293436
Epoch 1064/10000, Prediction Accuracy = 46.617999999999995%, Loss = 1.6487106800079345
Epoch: 1064, Batch Gradient Norm: 34.4479285008156
Epoch: 1064, Batch Gradient Norm after: 22.31726705787387
Epoch 1065/10000, Prediction Accuracy = 46.696%, Loss = 1.6310051679611206
Epoch: 1065, Batch Gradient Norm: 39.698184005669056
Epoch: 1065, Batch Gradient Norm after: 22.360677463261204
Epoch 1066/10000, Prediction Accuracy = 46.611999999999995%, Loss = 1.6468096733093263
Epoch: 1066, Batch Gradient Norm: 34.597869521469974
Epoch: 1066, Batch Gradient Norm after: 22.3606788800276
Epoch 1067/10000, Prediction Accuracy = 46.694%, Loss = 1.6300835609436035
Epoch: 1067, Batch Gradient Norm: 39.85386120646373
Epoch: 1067, Batch Gradient Norm after: 22.360680014238536
Epoch 1068/10000, Prediction Accuracy = 46.636%, Loss = 1.645970630645752
Epoch: 1068, Batch Gradient Norm: 34.608208007731946
Epoch: 1068, Batch Gradient Norm after: 22.346398160317214
Epoch 1069/10000, Prediction Accuracy = 46.69%, Loss = 1.6287363767623901
Epoch: 1069, Batch Gradient Norm: 39.90687837761049
Epoch: 1069, Batch Gradient Norm after: 22.360678125366405
Epoch 1070/10000, Prediction Accuracy = 46.641999999999996%, Loss = 1.6447572231292724
Epoch: 1070, Batch Gradient Norm: 34.63109528739702
Epoch: 1070, Batch Gradient Norm after: 22.34393615319965
Epoch 1071/10000, Prediction Accuracy = 46.69199999999999%, Loss = 1.627421236038208
Epoch: 1071, Batch Gradient Norm: 39.93861516786483
Epoch: 1071, Batch Gradient Norm after: 22.36067955132687
Epoch 1072/10000, Prediction Accuracy = 46.641999999999996%, Loss = 1.643451714515686
Epoch: 1072, Batch Gradient Norm: 34.67652386288749
Epoch: 1072, Batch Gradient Norm after: 22.347527685291066
Epoch 1073/10000, Prediction Accuracy = 46.708000000000006%, Loss = 1.6261794805526733
Epoch: 1073, Batch Gradient Norm: 40.0069152501575
Epoch: 1073, Batch Gradient Norm after: 22.360680079306572
Epoch 1074/10000, Prediction Accuracy = 46.647999999999996%, Loss = 1.6422924280166626
Epoch: 1074, Batch Gradient Norm: 34.675711654202814
Epoch: 1074, Batch Gradient Norm after: 22.337190391833747
Epoch 1075/10000, Prediction Accuracy = 46.7%, Loss = 1.6247942447662354
Epoch: 1075, Batch Gradient Norm: 40.0029737329748
Epoch: 1075, Batch Gradient Norm after: 22.360677203295594
Epoch 1076/10000, Prediction Accuracy = 46.652%, Loss = 1.6408725023269652
Epoch: 1076, Batch Gradient Norm: 34.777879999394585
Epoch: 1076, Batch Gradient Norm after: 22.358124228900127
Epoch 1077/10000, Prediction Accuracy = 46.712%, Loss = 1.6237348794937134
Epoch: 1077, Batch Gradient Norm: 40.151708603106734
Epoch: 1077, Batch Gradient Norm after: 22.360679143587685
Epoch 1078/10000, Prediction Accuracy = 46.66%, Loss = 1.6399990797042847
Epoch: 1078, Batch Gradient Norm: 34.65445412997287
Epoch: 1078, Batch Gradient Norm after: 22.309625932022627
Epoch 1079/10000, Prediction Accuracy = 46.715999999999994%, Loss = 1.6219680786132813
Epoch: 1079, Batch Gradient Norm: 39.958439165773875
Epoch: 1079, Batch Gradient Norm after: 22.360678347904493
Epoch 1080/10000, Prediction Accuracy = 46.674%, Loss = 1.6379375219345094
Epoch: 1080, Batch Gradient Norm: 34.819641061108825
Epoch: 1080, Batch Gradient Norm after: 22.36067840255825
Epoch 1081/10000, Prediction Accuracy = 46.714%, Loss = 1.6211195707321167
Epoch: 1081, Batch Gradient Norm: 40.12513756940574
Epoch: 1081, Batch Gradient Norm after: 22.36067997964519
Epoch 1082/10000, Prediction Accuracy = 46.68%, Loss = 1.6371382236480714
Epoch: 1082, Batch Gradient Norm: 34.87202771326487
Epoch: 1082, Batch Gradient Norm after: 22.358035165960096
Epoch 1083/10000, Prediction Accuracy = 46.721999999999994%, Loss = 1.6199355602264405
Epoch: 1083, Batch Gradient Norm: 40.271435680561375
Epoch: 1083, Batch Gradient Norm after: 22.36067898591744
Epoch 1084/10000, Prediction Accuracy = 46.684000000000005%, Loss = 1.6362542152404784
Epoch: 1084, Batch Gradient Norm: 34.76094489141567
Epoch: 1084, Batch Gradient Norm after: 22.31349631470413
Epoch 1085/10000, Prediction Accuracy = 46.726%, Loss = 1.6182172775268555
Epoch: 1085, Batch Gradient Norm: 40.098883696098625
Epoch: 1085, Batch Gradient Norm after: 22.360678744146448
Epoch 1086/10000, Prediction Accuracy = 46.67399999999999%, Loss = 1.6342617988586425
Epoch: 1086, Batch Gradient Norm: 34.9186352692665
Epoch: 1086, Batch Gradient Norm after: 22.36067829380374
Epoch 1087/10000, Prediction Accuracy = 46.721999999999994%, Loss = 1.6173383712768554
Epoch: 1087, Batch Gradient Norm: 40.25885014258826
Epoch: 1087, Batch Gradient Norm after: 22.36067694171283
Epoch 1088/10000, Prediction Accuracy = 46.684000000000005%, Loss = 1.6334392786026002
Epoch: 1088, Batch Gradient Norm: 34.97307609819257
Epoch: 1088, Batch Gradient Norm after: 22.35770479902187
Epoch 1089/10000, Prediction Accuracy = 46.732%, Loss = 1.6161456823348999
Epoch: 1089, Batch Gradient Norm: 40.39430719166359
Epoch: 1089, Batch Gradient Norm after: 22.360679312547113
Epoch 1090/10000, Prediction Accuracy = 46.688%, Loss = 1.6325416088104248
Epoch: 1090, Batch Gradient Norm: 34.875856161346356
Epoch: 1090, Batch Gradient Norm after: 22.318705687078694
Epoch 1091/10000, Prediction Accuracy = 46.746%, Loss = 1.614482593536377
Epoch: 1091, Batch Gradient Norm: 40.24189358588568
Epoch: 1091, Batch Gradient Norm after: 22.360679444743873
Epoch 1092/10000, Prediction Accuracy = 46.702%, Loss = 1.6306466817855836
Epoch: 1092, Batch Gradient Norm: 35.01931392121888
Epoch: 1092, Batch Gradient Norm after: 22.360679208088385
Epoch 1093/10000, Prediction Accuracy = 46.763999999999996%, Loss = 1.6135766983032227
Epoch: 1093, Batch Gradient Norm: 40.39396460217497
Epoch: 1093, Batch Gradient Norm after: 22.360679288703942
Epoch 1094/10000, Prediction Accuracy = 46.7%, Loss = 1.6298316717147827
Epoch: 1094, Batch Gradient Norm: 35.05200852061504
Epoch: 1094, Batch Gradient Norm after: 22.35435321029704
Epoch 1095/10000, Prediction Accuracy = 46.766000000000005%, Loss = 1.612331795692444
Epoch: 1095, Batch Gradient Norm: 40.50787886628755
Epoch: 1095, Batch Gradient Norm after: 22.360680727468953
Epoch 1096/10000, Prediction Accuracy = 46.715999999999994%, Loss = 1.628852868080139
Epoch: 1096, Batch Gradient Norm: 34.982542576778435
Epoch: 1096, Batch Gradient Norm after: 22.322389184935687
Epoch 1097/10000, Prediction Accuracy = 46.769999999999996%, Loss = 1.6107501983642578
Epoch: 1097, Batch Gradient Norm: 40.39803527267897
Epoch: 1097, Batch Gradient Norm after: 22.360679860342998
Epoch 1098/10000, Prediction Accuracy = 46.721999999999994%, Loss = 1.6270920753479003
Epoch: 1098, Batch Gradient Norm: 35.122372065683166
Epoch: 1098, Batch Gradient Norm after: 22.360678650205244
Epoch 1099/10000, Prediction Accuracy = 46.779999999999994%, Loss = 1.6098313093185426
Epoch: 1099, Batch Gradient Norm: 40.5502965318719
Epoch: 1099, Batch Gradient Norm after: 22.36067969776037
Epoch 1100/10000, Prediction Accuracy = 46.715999999999994%, Loss = 1.626264762878418
Epoch: 1100, Batch Gradient Norm: 35.11571036303397
Epoch: 1100, Batch Gradient Norm after: 22.34342607167324
Epoch 1101/10000, Prediction Accuracy = 46.782%, Loss = 1.6084595918655396
Epoch: 1101, Batch Gradient Norm: 40.56459885210188
Epoch: 1101, Batch Gradient Norm after: 22.36067932884284
Epoch 1102/10000, Prediction Accuracy = 46.730000000000004%, Loss = 1.6249433517456056
Epoch: 1102, Batch Gradient Norm: 35.20300671931705
Epoch: 1102, Batch Gradient Norm after: 22.360676078374745
Epoch 1103/10000, Prediction Accuracy = 46.796%, Loss = 1.6073895692825317
Epoch: 1103, Batch Gradient Norm: 40.684125014554176
Epoch: 1103, Batch Gradient Norm after: 22.360680004254043
Epoch 1104/10000, Prediction Accuracy = 46.736000000000004%, Loss = 1.6239976406097412
Epoch: 1104, Batch Gradient Norm: 35.11164810830525
Epoch: 1104, Batch Gradient Norm after: 22.32367427306457
Epoch 1105/10000, Prediction Accuracy = 46.797999999999995%, Loss = 1.6057546615600586
Epoch: 1105, Batch Gradient Norm: 40.54608149478711
Epoch: 1105, Batch Gradient Norm after: 22.360676888534
Epoch 1106/10000, Prediction Accuracy = 46.756%, Loss = 1.6221490859985352
Epoch: 1106, Batch Gradient Norm: 35.237655024914936
Epoch: 1106, Batch Gradient Norm after: 22.36067801943086
Epoch 1107/10000, Prediction Accuracy = 46.81%, Loss = 1.6048079967498778
Epoch: 1107, Batch Gradient Norm: 40.66945082223393
Epoch: 1107, Batch Gradient Norm after: 22.360677146422802
Epoch 1108/10000, Prediction Accuracy = 46.769999999999996%, Loss = 1.6212336540222168
Epoch: 1108, Batch Gradient Norm: 35.287062562641104
Epoch: 1108, Batch Gradient Norm after: 22.36067811010582
Epoch 1109/10000, Prediction Accuracy = 46.826%, Loss = 1.6036519050598144
Epoch: 1109, Batch Gradient Norm: 40.77408446645928
Epoch: 1109, Batch Gradient Norm after: 22.360680238522352
Epoch 1110/10000, Prediction Accuracy = 46.786%, Loss = 1.6202591419219972
Epoch: 1110, Batch Gradient Norm: 35.25730760793813
Epoch: 1110, Batch Gradient Norm after: 22.34238773604348
Epoch 1111/10000, Prediction Accuracy = 46.824%, Loss = 1.6022459268569946
Epoch: 1111, Batch Gradient Norm: 40.74356912838348
Epoch: 1111, Batch Gradient Norm after: 22.36067920840154
Epoch 1112/10000, Prediction Accuracy = 46.790000000000006%, Loss = 1.6187799453735352
Epoch: 1112, Batch Gradient Norm: 35.3461302789829
Epoch: 1112, Batch Gradient Norm after: 22.36067736530808
Epoch 1113/10000, Prediction Accuracy = 46.836%, Loss = 1.601185417175293
Epoch: 1113, Batch Gradient Norm: 40.845902420818135
Epoch: 1113, Batch Gradient Norm after: 22.360678395058535
Epoch 1114/10000, Prediction Accuracy = 46.788000000000004%, Loss = 1.6177870988845826
Epoch: 1114, Batch Gradient Norm: 35.334333406708446
Epoch: 1114, Batch Gradient Norm after: 22.345612652231384
Epoch 1115/10000, Prediction Accuracy = 46.854%, Loss = 1.5998364210128784
Epoch: 1115, Batch Gradient Norm: 40.8411963104906
Epoch: 1115, Batch Gradient Norm after: 22.36067943948973
Epoch 1116/10000, Prediction Accuracy = 46.784000000000006%, Loss = 1.616421389579773
Epoch: 1116, Batch Gradient Norm: 35.41002608186387
Epoch: 1116, Batch Gradient Norm after: 22.360675018471294
Epoch 1117/10000, Prediction Accuracy = 46.85600000000001%, Loss = 1.5987401247024535
Epoch: 1117, Batch Gradient Norm: 40.93477787755325
Epoch: 1117, Batch Gradient Norm after: 22.360678416100967
Epoch 1118/10000, Prediction Accuracy = 46.786%, Loss = 1.615400743484497
Epoch: 1118, Batch Gradient Norm: 35.38469628020743
Epoch: 1118, Batch Gradient Norm after: 22.341993974984415
Epoch 1119/10000, Prediction Accuracy = 46.86%, Loss = 1.5973381519317627
Epoch: 1119, Batch Gradient Norm: 40.89509874844768
Epoch: 1119, Batch Gradient Norm after: 22.360678490212962
Epoch 1120/10000, Prediction Accuracy = 46.794%, Loss = 1.6139425754547119
Epoch: 1120, Batch Gradient Norm: 35.474427497542855
Epoch: 1120, Batch Gradient Norm after: 22.360679701377393
Epoch 1121/10000, Prediction Accuracy = 46.872%, Loss = 1.5962965488433838
Epoch: 1121, Batch Gradient Norm: 40.98775452022965
Epoch: 1121, Batch Gradient Norm after: 22.360678253482572
Epoch 1122/10000, Prediction Accuracy = 46.797999999999995%, Loss = 1.612935757637024
Epoch: 1122, Batch Gradient Norm: 35.49798795663273
Epoch: 1122, Batch Gradient Norm after: 22.35506524886219
Epoch 1123/10000, Prediction Accuracy = 46.874%, Loss = 1.5950568675994874
Epoch: 1123, Batch Gradient Norm: 41.048341427013
Epoch: 1123, Batch Gradient Norm after: 22.36067793578795
Epoch 1124/10000, Prediction Accuracy = 46.797999999999995%, Loss = 1.6117951154708863
Epoch: 1124, Batch Gradient Norm: 35.50496315856514
Epoch: 1124, Batch Gradient Norm after: 22.34843391547639
Epoch 1125/10000, Prediction Accuracy = 46.888%, Loss = 1.5937641382217407
Epoch: 1125, Batch Gradient Norm: 41.06944247258555
Epoch: 1125, Batch Gradient Norm after: 22.36067873868619
Epoch 1126/10000, Prediction Accuracy = 46.806%, Loss = 1.610536313056946
Epoch: 1126, Batch Gradient Norm: 35.55757549583486
Epoch: 1126, Batch Gradient Norm after: 22.353186666098516
Epoch 1127/10000, Prediction Accuracy = 46.894000000000005%, Loss = 1.5925954103469848
Epoch: 1127, Batch Gradient Norm: 41.12705625281676
Epoch: 1127, Batch Gradient Norm after: 22.360677102125987
Epoch 1128/10000, Prediction Accuracy = 46.809999999999995%, Loss = 1.6094069242477418
Epoch: 1128, Batch Gradient Norm: 35.57908912175085
Epoch: 1128, Batch Gradient Norm after: 22.348649869976388
Epoch 1129/10000, Prediction Accuracy = 46.903999999999996%, Loss = 1.5913411855697632
Epoch: 1129, Batch Gradient Norm: 41.15279573605484
Epoch: 1129, Batch Gradient Norm after: 22.360678310196317
Epoch 1130/10000, Prediction Accuracy = 46.812000000000005%, Loss = 1.6081659317016601
Epoch: 1130, Batch Gradient Norm: 35.63227050733793
Epoch: 1130, Batch Gradient Norm after: 22.356040363075248
Epoch 1131/10000, Prediction Accuracy = 46.916%, Loss = 1.5902080535888672
Epoch: 1131, Batch Gradient Norm: 41.221341607220374
Epoch: 1131, Batch Gradient Norm after: 22.36067866229193
Epoch 1132/10000, Prediction Accuracy = 46.818%, Loss = 1.6070844173431396
Epoch: 1132, Batch Gradient Norm: 35.616146619950406
Epoch: 1132, Batch Gradient Norm after: 22.343236773218596
Epoch 1133/10000, Prediction Accuracy = 46.926%, Loss = 1.5888487100601196
Epoch: 1133, Batch Gradient Norm: 41.19522341040736
Epoch: 1133, Batch Gradient Norm after: 22.360676965258147
Epoch 1134/10000, Prediction Accuracy = 46.816%, Loss = 1.605673885345459
Epoch: 1134, Batch Gradient Norm: 35.701891758509795
Epoch: 1134, Batch Gradient Norm after: 22.36067882347097
Epoch 1135/10000, Prediction Accuracy = 46.936%, Loss = 1.5878042697906494
Epoch: 1135, Batch Gradient Norm: 41.3004964326494
Epoch: 1135, Batch Gradient Norm after: 22.36067662445679
Epoch 1136/10000, Prediction Accuracy = 46.826%, Loss = 1.6047256231307983
Epoch: 1136, Batch Gradient Norm: 35.67526670236366
Epoch: 1136, Batch Gradient Norm after: 22.341438053449888
Epoch 1137/10000, Prediction Accuracy = 46.936%, Loss = 1.5864168405532837
Epoch: 1137, Batch Gradient Norm: 41.265469903764625
Epoch: 1137, Batch Gradient Norm after: 22.360677975137314
Epoch 1138/10000, Prediction Accuracy = 46.834%, Loss = 1.603291654586792
Epoch: 1138, Batch Gradient Norm: 35.756626904469606
Epoch: 1138, Batch Gradient Norm after: 22.360678493533023
Epoch 1139/10000, Prediction Accuracy = 46.924%, Loss = 1.585388970375061
Epoch: 1139, Batch Gradient Norm: 41.36146406567005
Epoch: 1139, Batch Gradient Norm after: 22.360677313465164
Epoch 1140/10000, Prediction Accuracy = 46.83999999999999%, Loss = 1.6023115158081054
Epoch: 1140, Batch Gradient Norm: 35.77622762659611
Epoch: 1140, Batch Gradient Norm after: 22.353661375329015
Epoch 1141/10000, Prediction Accuracy = 46.938%, Loss = 1.5841601848602296
Epoch: 1141, Batch Gradient Norm: 41.40248062015522
Epoch: 1141, Batch Gradient Norm after: 22.360678258798302
Epoch 1142/10000, Prediction Accuracy = 46.84400000000001%, Loss = 1.6011229753494263
Epoch: 1142, Batch Gradient Norm: 35.808288143168
Epoch: 1142, Batch Gradient Norm after: 22.353960465379604
Epoch 1143/10000, Prediction Accuracy = 46.944%, Loss = 1.582964587211609
Epoch: 1143, Batch Gradient Norm: 41.44579174948416
Epoch: 1143, Batch Gradient Norm after: 22.36067665069306
Epoch 1144/10000, Prediction Accuracy = 46.852000000000004%, Loss = 1.5999558448791504
Epoch: 1144, Batch Gradient Norm: 35.821529694664
Epoch: 1144, Batch Gradient Norm after: 22.3502259277084
Epoch 1145/10000, Prediction Accuracy = 46.940000000000005%, Loss = 1.581736946105957
Epoch: 1145, Batch Gradient Norm: 41.4604673899436
Epoch: 1145, Batch Gradient Norm after: 22.36067875812664
Epoch 1146/10000, Prediction Accuracy = 46.85%, Loss = 1.598708200454712
Epoch: 1146, Batch Gradient Norm: 35.887825319926456
Epoch: 1146, Batch Gradient Norm after: 22.360679418768093
Epoch 1147/10000, Prediction Accuracy = 46.948%, Loss = 1.5806459188461304
Epoch: 1147, Batch Gradient Norm: 41.52771549517342
Epoch: 1147, Batch Gradient Norm after: 22.360677808689964
Epoch 1148/10000, Prediction Accuracy = 46.86200000000001%, Loss = 1.597642183303833
Epoch: 1148, Batch Gradient Norm: 35.88425986432474
Epoch: 1148, Batch Gradient Norm after: 22.352210835167728
Epoch 1149/10000, Prediction Accuracy = 46.962%, Loss = 1.5793600797653198
Epoch: 1149, Batch Gradient Norm: 41.51914226022784
Epoch: 1149, Batch Gradient Norm after: 22.360678226826725
Epoch 1150/10000, Prediction Accuracy = 46.864%, Loss = 1.5963194608688354
Epoch: 1150, Batch Gradient Norm: 35.938834937272055
Epoch: 1150, Batch Gradient Norm after: 22.360678023728596
Epoch 1151/10000, Prediction Accuracy = 46.96999999999999%, Loss = 1.578253483772278
Epoch: 1151, Batch Gradient Norm: 41.58079510038633
Epoch: 1151, Batch Gradient Norm after: 22.36067726496152
Epoch 1152/10000, Prediction Accuracy = 46.874%, Loss = 1.5952492713928224
Epoch: 1152, Batch Gradient Norm: 35.97883597981396
Epoch: 1152, Batch Gradient Norm after: 22.36067793123275
Epoch 1153/10000, Prediction Accuracy = 46.980000000000004%, Loss = 1.577083444595337
Epoch: 1153, Batch Gradient Norm: 41.63396168701579
Epoch: 1153, Batch Gradient Norm after: 22.360679253997855
Epoch 1154/10000, Prediction Accuracy = 46.886%, Loss = 1.5941481828689574
Epoch: 1154, Batch Gradient Norm: 36.00718019139452
Epoch: 1154, Batch Gradient Norm after: 22.36056246897954
Epoch 1155/10000, Prediction Accuracy = 46.984%, Loss = 1.5758939027786254
Epoch: 1155, Batch Gradient Norm: 41.66969270357648
Epoch: 1155, Batch Gradient Norm after: 22.360676963624375
Epoch 1156/10000, Prediction Accuracy = 46.886%, Loss = 1.5929999828338623
Epoch: 1156, Batch Gradient Norm: 36.036438346767355
Epoch: 1156, Batch Gradient Norm after: 22.36010804123563
Epoch 1157/10000, Prediction Accuracy = 47.001999999999995%, Loss = 1.5747198104858398
Epoch: 1157, Batch Gradient Norm: 41.716386697743765
Epoch: 1157, Batch Gradient Norm after: 22.360678090343423
Epoch 1158/10000, Prediction Accuracy = 46.896%, Loss = 1.591858983039856
Epoch: 1158, Batch Gradient Norm: 36.046814286117964
Epoch: 1158, Batch Gradient Norm after: 22.35397195920259
Epoch 1159/10000, Prediction Accuracy = 47.013999999999996%, Loss = 1.5734779119491578
Epoch: 1159, Batch Gradient Norm: 41.71213743067899
Epoch: 1159, Batch Gradient Norm after: 22.360678891566803
Epoch 1160/10000, Prediction Accuracy = 46.900000000000006%, Loss = 1.5905660390853882
Epoch: 1160, Batch Gradient Norm: 36.0902597986718
Epoch: 1160, Batch Gradient Norm after: 22.360677541532972
Epoch 1161/10000, Prediction Accuracy = 47.018%, Loss = 1.5723465442657472
Epoch: 1161, Batch Gradient Norm: 41.747111381011834
Epoch: 1161, Batch Gradient Norm after: 22.36067723863267
Epoch 1162/10000, Prediction Accuracy = 46.898%, Loss = 1.589410924911499
Epoch: 1162, Batch Gradient Norm: 36.11321744186602
Epoch: 1162, Batch Gradient Norm after: 22.36068126445477
Epoch 1163/10000, Prediction Accuracy = 47.010000000000005%, Loss = 1.5711628437042235
Epoch: 1163, Batch Gradient Norm: 41.7925521514402
Epoch: 1163, Batch Gradient Norm after: 22.360680448959602
Epoch 1164/10000, Prediction Accuracy = 46.908%, Loss = 1.5882968187332154
Epoch: 1164, Batch Gradient Norm: 36.145388568360104
Epoch: 1164, Batch Gradient Norm after: 22.36067643427603
Epoch 1165/10000, Prediction Accuracy = 47.028000000000006%, Loss = 1.5699909448623657
Epoch: 1165, Batch Gradient Norm: 41.83133514891602
Epoch: 1165, Batch Gradient Norm after: 22.360677125084166
Epoch 1166/10000, Prediction Accuracy = 46.924%, Loss = 1.5871638059616089
Epoch: 1166, Batch Gradient Norm: 36.17797995216461
Epoch: 1166, Batch Gradient Norm after: 22.360678309522736
Epoch 1167/10000, Prediction Accuracy = 47.036%, Loss = 1.5688302755355834
Epoch: 1167, Batch Gradient Norm: 41.88075490258511
Epoch: 1167, Batch Gradient Norm after: 22.360678075810952
Epoch 1168/10000, Prediction Accuracy = 46.936%, Loss = 1.5860609054565429
Epoch: 1168, Batch Gradient Norm: 36.20999519986428
Epoch: 1168, Batch Gradient Norm after: 22.36067582059782
Epoch 1169/10000, Prediction Accuracy = 47.044%, Loss = 1.567665123939514
Epoch: 1169, Batch Gradient Norm: 41.92191666574479
Epoch: 1169, Batch Gradient Norm after: 22.36067721593505
Epoch 1170/10000, Prediction Accuracy = 46.938%, Loss = 1.5849177598953248
Epoch: 1170, Batch Gradient Norm: 36.2359800195794
Epoch: 1170, Batch Gradient Norm after: 22.36067752771367
Epoch 1171/10000, Prediction Accuracy = 47.044%, Loss = 1.5664888381958009
Epoch: 1171, Batch Gradient Norm: 41.97290476980741
Epoch: 1171, Batch Gradient Norm after: 22.360677829993417
Epoch 1172/10000, Prediction Accuracy = 46.944%, Loss = 1.5838152647018433
Epoch: 1172, Batch Gradient Norm: 36.25392691980665
Epoch: 1172, Batch Gradient Norm after: 22.356392544270314
Epoch 1173/10000, Prediction Accuracy = 47.06%, Loss = 1.565287446975708
Epoch: 1173, Batch Gradient Norm: 41.99867395616445
Epoch: 1173, Batch Gradient Norm after: 22.360678341292775
Epoch 1174/10000, Prediction Accuracy = 46.95399999999999%, Loss = 1.5826383590698243
Epoch: 1174, Batch Gradient Norm: 36.296201084658236
Epoch: 1174, Batch Gradient Norm after: 22.360676935707342
Epoch 1175/10000, Prediction Accuracy = 47.068%, Loss = 1.5641624927520752
Epoch: 1175, Batch Gradient Norm: 42.01655264847694
Epoch: 1175, Batch Gradient Norm after: 22.36067668798751
Epoch 1176/10000, Prediction Accuracy = 46.966%, Loss = 1.5814361333847047
Epoch: 1176, Batch Gradient Norm: 36.32053144145238
Epoch: 1176, Batch Gradient Norm after: 22.360678206620378
Epoch 1177/10000, Prediction Accuracy = 47.077999999999996%, Loss = 1.5629926204681397
Epoch: 1177, Batch Gradient Norm: 42.04445941048934
Epoch: 1177, Batch Gradient Norm after: 22.360677394262826
Epoch 1178/10000, Prediction Accuracy = 46.97%, Loss = 1.5802670001983643
Epoch: 1178, Batch Gradient Norm: 36.34678254889198
Epoch: 1178, Batch Gradient Norm after: 22.360674931423116
Epoch 1179/10000, Prediction Accuracy = 47.086%, Loss = 1.561824369430542
Epoch: 1179, Batch Gradient Norm: 42.06152075645476
Epoch: 1179, Batch Gradient Norm after: 22.360677842221534
Epoch 1180/10000, Prediction Accuracy = 46.977999999999994%, Loss = 1.5790767908096313
Epoch: 1180, Batch Gradient Norm: 36.3741148007569
Epoch: 1180, Batch Gradient Norm after: 22.360678044277723
Epoch 1181/10000, Prediction Accuracy = 47.096%, Loss = 1.5606746196746826
Epoch: 1181, Batch Gradient Norm: 42.118671957717616
Epoch: 1181, Batch Gradient Norm after: 22.360678816179757
Epoch 1182/10000, Prediction Accuracy = 46.992%, Loss = 1.5780002355575562
Epoch: 1182, Batch Gradient Norm: 36.409668945827754
Epoch: 1182, Batch Gradient Norm after: 22.36067731941656
Epoch 1183/10000, Prediction Accuracy = 47.102%, Loss = 1.5595405101776123
Epoch: 1183, Batch Gradient Norm: 42.17249661017675
Epoch: 1183, Batch Gradient Norm after: 22.360676498940744
Epoch 1184/10000, Prediction Accuracy = 47.001999999999995%, Loss = 1.5769533157348632
Epoch: 1184, Batch Gradient Norm: 36.43165695791565
Epoch: 1184, Batch Gradient Norm after: 22.357642162501282
Epoch 1185/10000, Prediction Accuracy = 47.114%, Loss = 1.5583673477172852
Epoch: 1185, Batch Gradient Norm: 42.213224937821124
Epoch: 1185, Batch Gradient Norm after: 22.36067747537805
Epoch 1186/10000, Prediction Accuracy = 47.019999999999996%, Loss = 1.5758219003677367
Epoch: 1186, Batch Gradient Norm: 36.46294656883023
Epoch: 1186, Batch Gradient Norm after: 22.35781831651906
Epoch 1187/10000, Prediction Accuracy = 47.116%, Loss = 1.5572153091430665
Epoch: 1187, Batch Gradient Norm: 42.2473287707059
Epoch: 1187, Batch Gradient Norm after: 22.36067699792375
Epoch 1188/10000, Prediction Accuracy = 47.034000000000006%, Loss = 1.5746987104415893
Epoch: 1188, Batch Gradient Norm: 36.49861294485076
Epoch: 1188, Batch Gradient Norm after: 22.36067621849421
Epoch 1189/10000, Prediction Accuracy = 47.132%, Loss = 1.5560900449752808
Epoch: 1189, Batch Gradient Norm: 42.308754839894405
Epoch: 1189, Batch Gradient Norm after: 22.360676768482605
Epoch 1190/10000, Prediction Accuracy = 47.044000000000004%, Loss = 1.5736538171768188
Epoch: 1190, Batch Gradient Norm: 36.48892962026517
Epoch: 1190, Batch Gradient Norm after: 22.350344318173153
Epoch 1191/10000, Prediction Accuracy = 47.134%, Loss = 1.5548179388046264
Epoch: 1191, Batch Gradient Norm: 42.29171609125821
Epoch: 1191, Batch Gradient Norm after: 22.36067700635022
Epoch 1192/10000, Prediction Accuracy = 47.05%, Loss = 1.5723419189453125
Epoch: 1192, Batch Gradient Norm: 36.54979385754004
Epoch: 1192, Batch Gradient Norm after: 22.360676756400125
Epoch 1193/10000, Prediction Accuracy = 47.138%, Loss = 1.5537644386291505
Epoch: 1193, Batch Gradient Norm: 42.359798909660526
Epoch: 1193, Batch Gradient Norm after: 22.36067782384571
Epoch 1194/10000, Prediction Accuracy = 47.062%, Loss = 1.5713154077529907
Epoch: 1194, Batch Gradient Norm: 36.576442181152125
Epoch: 1194, Batch Gradient Norm after: 22.358384819397088
Epoch 1195/10000, Prediction Accuracy = 47.14399999999999%, Loss = 1.5526198148727417
Epoch: 1195, Batch Gradient Norm: 42.40284448438224
Epoch: 1195, Batch Gradient Norm after: 22.36067866267786
Epoch 1196/10000, Prediction Accuracy = 47.068%, Loss = 1.5702102184295654
Epoch: 1196, Batch Gradient Norm: 36.597964422228564
Epoch: 1196, Batch Gradient Norm after: 22.355082545598144
Epoch 1197/10000, Prediction Accuracy = 47.146%, Loss = 1.551456594467163
Epoch: 1197, Batch Gradient Norm: 42.42572497782419
Epoch: 1197, Batch Gradient Norm after: 22.36067793830161
Epoch 1198/10000, Prediction Accuracy = 47.08200000000001%, Loss = 1.5690582275390625
Epoch: 1198, Batch Gradient Norm: 36.64702322212455
Epoch: 1198, Batch Gradient Norm after: 22.360677016776616
Epoch 1199/10000, Prediction Accuracy = 47.158%, Loss = 1.5503880739212037
Epoch: 1199, Batch Gradient Norm: 42.50449124994764
Epoch: 1199, Batch Gradient Norm after: 22.360676082583204
Epoch 1200/10000, Prediction Accuracy = 47.096%, Loss = 1.568088698387146
Epoch: 1200, Batch Gradient Norm: 36.621766128887984
Epoch: 1200, Batch Gradient Norm after: 22.345809101609
Epoch 1201/10000, Prediction Accuracy = 47.17%, Loss = 1.549077534675598
Epoch: 1201, Batch Gradient Norm: 42.45375542231342
Epoch: 1201, Batch Gradient Norm after: 22.360677441006438
Epoch 1202/10000, Prediction Accuracy = 47.094%, Loss = 1.5666929006576538
Epoch: 1202, Batch Gradient Norm: 36.687081835363465
Epoch: 1202, Batch Gradient Norm after: 22.360679227393973
Epoch 1203/10000, Prediction Accuracy = 47.174%, Loss = 1.5480606317520142
Epoch: 1203, Batch Gradient Norm: 42.525525272577774
Epoch: 1203, Batch Gradient Norm after: 22.360678028016086
Epoch 1204/10000, Prediction Accuracy = 47.09400000000001%, Loss = 1.5657191753387452
Epoch: 1204, Batch Gradient Norm: 36.728255795939276
Epoch: 1204, Batch Gradient Norm after: 22.360678887654423
Epoch 1205/10000, Prediction Accuracy = 47.184000000000005%, Loss = 1.5469669342041015
Epoch: 1205, Batch Gradient Norm: 42.57780287531197
Epoch: 1205, Batch Gradient Norm after: 22.360677994905704
Epoch 1206/10000, Prediction Accuracy = 47.094%, Loss = 1.5646772384643555
Epoch: 1206, Batch Gradient Norm: 36.76127180355797
Epoch: 1206, Batch Gradient Norm after: 22.360676706936736
Epoch 1207/10000, Prediction Accuracy = 47.19200000000001%, Loss = 1.5458547115325927
Epoch: 1207, Batch Gradient Norm: 42.623102216119385
Epoch: 1207, Batch Gradient Norm after: 22.360678681202717
Epoch 1208/10000, Prediction Accuracy = 47.10799999999999%, Loss = 1.5636192560195923
Epoch: 1208, Batch Gradient Norm: 36.78161595433897
Epoch: 1208, Batch Gradient Norm after: 22.358948373946564
Epoch 1209/10000, Prediction Accuracy = 47.198%, Loss = 1.544714593887329
Epoch: 1209, Batch Gradient Norm: 42.65183929714918
Epoch: 1209, Batch Gradient Norm after: 22.360677536347815
Epoch 1210/10000, Prediction Accuracy = 47.116%, Loss = 1.5625044584274292
Epoch: 1210, Batch Gradient Norm: 36.80938751047932
Epoch: 1210, Batch Gradient Norm after: 22.36067755172066
Epoch 1211/10000, Prediction Accuracy = 47.202%, Loss = 1.543596911430359
Epoch: 1211, Batch Gradient Norm: 42.689862139099645
Epoch: 1211, Batch Gradient Norm after: 22.360680015367294
Epoch 1212/10000, Prediction Accuracy = 47.138%, Loss = 1.5614277839660644
Epoch: 1212, Batch Gradient Norm: 36.83476706817393
Epoch: 1212, Batch Gradient Norm after: 22.360678585298928
Epoch 1213/10000, Prediction Accuracy = 47.205999999999996%, Loss = 1.542467761039734
Epoch: 1213, Batch Gradient Norm: 42.71762440012939
Epoch: 1213, Batch Gradient Norm after: 22.360679598182642
Epoch 1214/10000, Prediction Accuracy = 47.148%, Loss = 1.5603083610534667
Epoch: 1214, Batch Gradient Norm: 36.860602618677646
Epoch: 1214, Batch Gradient Norm after: 22.360679014343773
Epoch 1215/10000, Prediction Accuracy = 47.21%, Loss = 1.5413689613342285
Epoch: 1215, Batch Gradient Norm: 42.755945198243246
Epoch: 1215, Batch Gradient Norm after: 22.36067688206208
Epoch 1216/10000, Prediction Accuracy = 47.15%, Loss = 1.5592255353927613
Epoch: 1216, Batch Gradient Norm: 36.889320124375814
Epoch: 1216, Batch Gradient Norm after: 22.36067932588905
Epoch 1217/10000, Prediction Accuracy = 47.226000000000006%, Loss = 1.5402653694152832
Epoch: 1217, Batch Gradient Norm: 42.81469823675491
Epoch: 1217, Batch Gradient Norm after: 22.360677780037452
Epoch 1218/10000, Prediction Accuracy = 47.158%, Loss = 1.5582327842712402
Epoch: 1218, Batch Gradient Norm: 36.88726125512723
Epoch: 1218, Batch Gradient Norm after: 22.353058374497635
Epoch 1219/10000, Prediction Accuracy = 47.227999999999994%, Loss = 1.539069414138794
Epoch: 1219, Batch Gradient Norm: 42.797145575053115
Epoch: 1219, Batch Gradient Norm after: 22.360679153881538
Epoch 1220/10000, Prediction Accuracy = 47.164%, Loss = 1.5569726467132567
Epoch: 1220, Batch Gradient Norm: 36.931808558532644
Epoch: 1220, Batch Gradient Norm after: 22.360679229840386
Epoch 1221/10000, Prediction Accuracy = 47.244%, Loss = 1.538027858734131
Epoch: 1221, Batch Gradient Norm: 42.854860842991876
Epoch: 1221, Batch Gradient Norm after: 22.360677638426466
Epoch 1222/10000, Prediction Accuracy = 47.178000000000004%, Loss = 1.5559690952301026
Epoch: 1222, Batch Gradient Norm: 36.96203982840241
Epoch: 1222, Batch Gradient Norm after: 22.360678671138697
Epoch 1223/10000, Prediction Accuracy = 47.238%, Loss = 1.5369495153427124
Epoch: 1223, Batch Gradient Norm: 42.900681923968556
Epoch: 1223, Batch Gradient Norm after: 22.36067585978709
Epoch 1224/10000, Prediction Accuracy = 47.176%, Loss = 1.554940414428711
Epoch: 1224, Batch Gradient Norm: 36.984907955275176
Epoch: 1224, Batch Gradient Norm after: 22.35895580227953
Epoch 1225/10000, Prediction Accuracy = 47.25%, Loss = 1.5358344793319703
Epoch: 1225, Batch Gradient Norm: 42.91936101354794
Epoch: 1225, Batch Gradient Norm after: 22.36067660946985
Epoch 1226/10000, Prediction Accuracy = 47.18%, Loss = 1.553810453414917
Epoch: 1226, Batch Gradient Norm: 37.013712900916225
Epoch: 1226, Batch Gradient Norm after: 22.360678274676662
Epoch 1227/10000, Prediction Accuracy = 47.256%, Loss = 1.5347379922866822
Epoch: 1227, Batch Gradient Norm: 42.95863095914284
Epoch: 1227, Batch Gradient Norm after: 22.36067913982407
Epoch 1228/10000, Prediction Accuracy = 47.186%, Loss = 1.5527426958084107
Epoch: 1228, Batch Gradient Norm: 37.036856421114
Epoch: 1228, Batch Gradient Norm after: 22.36067847766991
Epoch 1229/10000, Prediction Accuracy = 47.264%, Loss = 1.5336406707763672
Epoch: 1229, Batch Gradient Norm: 42.98570981588712
Epoch: 1229, Batch Gradient Norm after: 22.360679371981888
Epoch 1230/10000, Prediction Accuracy = 47.196000000000005%, Loss = 1.5516520261764526
Epoch: 1230, Batch Gradient Norm: 37.05611758180146
Epoch: 1230, Batch Gradient Norm after: 22.36068084919482
Epoch 1231/10000, Prediction Accuracy = 47.28%, Loss = 1.5325301885604858
Epoch: 1231, Batch Gradient Norm: 43.001311624168046
Epoch: 1231, Batch Gradient Norm after: 22.360676331450765
Epoch 1232/10000, Prediction Accuracy = 47.198%, Loss = 1.550526237487793
Epoch: 1232, Batch Gradient Norm: 37.07741978099611
Epoch: 1232, Batch Gradient Norm after: 22.360678218692048
Epoch 1233/10000, Prediction Accuracy = 47.286%, Loss = 1.5314314126968385
Epoch: 1233, Batch Gradient Norm: 43.026929901726724
Epoch: 1233, Batch Gradient Norm after: 22.360677322775295
Epoch 1234/10000, Prediction Accuracy = 47.224000000000004%, Loss = 1.549418830871582
Epoch: 1234, Batch Gradient Norm: 37.09602471083722
Epoch: 1234, Batch Gradient Norm after: 22.360677729106705
Epoch 1235/10000, Prediction Accuracy = 47.292%, Loss = 1.5303104162216186
Epoch: 1235, Batch Gradient Norm: 43.04427800604109
Epoch: 1235, Batch Gradient Norm after: 22.36067693588659
Epoch 1236/10000, Prediction Accuracy = 47.222%, Loss = 1.5482941627502442
Epoch: 1236, Batch Gradient Norm: 37.11810259697417
Epoch: 1236, Batch Gradient Norm after: 22.360681193694397
Epoch 1237/10000, Prediction Accuracy = 47.312%, Loss = 1.5292167663574219
Epoch: 1237, Batch Gradient Norm: 43.06464844355901
Epoch: 1237, Batch Gradient Norm after: 22.360676386828978
Epoch 1238/10000, Prediction Accuracy = 47.224000000000004%, Loss = 1.5471856355667115
Epoch: 1238, Batch Gradient Norm: 37.14181028996858
Epoch: 1238, Batch Gradient Norm after: 22.360677878943235
Epoch 1239/10000, Prediction Accuracy = 47.322%, Loss = 1.528133487701416
Epoch: 1239, Batch Gradient Norm: 43.09520345322315
Epoch: 1239, Batch Gradient Norm after: 22.360678382083123
Epoch 1240/10000, Prediction Accuracy = 47.227999999999994%, Loss = 1.5461229801177978
Epoch: 1240, Batch Gradient Norm: 37.160202518292614
Epoch: 1240, Batch Gradient Norm after: 22.360677449602075
Epoch 1241/10000, Prediction Accuracy = 47.332%, Loss = 1.527048444747925
Epoch: 1241, Batch Gradient Norm: 43.11637639927981
Epoch: 1241, Batch Gradient Norm after: 22.360678548819607
Epoch 1242/10000, Prediction Accuracy = 47.244%, Loss = 1.5450200080871581
Epoch: 1242, Batch Gradient Norm: 37.182514481096995
Epoch: 1242, Batch Gradient Norm after: 22.360678944623256
Epoch 1243/10000, Prediction Accuracy = 47.33%, Loss = 1.5259600400924682
Epoch: 1243, Batch Gradient Norm: 43.14304195956918
Epoch: 1243, Batch Gradient Norm after: 22.360678030868073
Epoch 1244/10000, Prediction Accuracy = 47.252%, Loss = 1.5439502239227294
Epoch: 1244, Batch Gradient Norm: 37.19726263260801
Epoch: 1244, Batch Gradient Norm after: 22.360679138577538
Epoch 1245/10000, Prediction Accuracy = 47.332%, Loss = 1.5248589038848877
Epoch: 1245, Batch Gradient Norm: 43.159058942392804
Epoch: 1245, Batch Gradient Norm after: 22.360675402552395
Epoch 1246/10000, Prediction Accuracy = 47.266000000000005%, Loss = 1.5428576231002809
Epoch: 1246, Batch Gradient Norm: 37.22713934818142
Epoch: 1246, Batch Gradient Norm after: 22.360678870869602
Epoch 1247/10000, Prediction Accuracy = 47.339999999999996%, Loss = 1.5237992525100708
Epoch: 1247, Batch Gradient Norm: 43.19641697635902
Epoch: 1247, Batch Gradient Norm after: 22.360677274978954
Epoch 1248/10000, Prediction Accuracy = 47.276%, Loss = 1.541825270652771
Epoch: 1248, Batch Gradient Norm: 37.251440997655784
Epoch: 1248, Batch Gradient Norm after: 22.360677179037555
Epoch 1249/10000, Prediction Accuracy = 47.344%, Loss = 1.5227261781692505
Epoch: 1249, Batch Gradient Norm: 43.23494144525539
Epoch: 1249, Batch Gradient Norm after: 22.36067854344134
Epoch 1250/10000, Prediction Accuracy = 47.286%, Loss = 1.5407933712005615
Epoch: 1250, Batch Gradient Norm: 37.27018470652895
Epoch: 1250, Batch Gradient Norm after: 22.360678486693146
Epoch 1251/10000, Prediction Accuracy = 47.362%, Loss = 1.5216548919677735
Epoch: 1251, Batch Gradient Norm: 43.26156051670805
Epoch: 1251, Batch Gradient Norm after: 22.36067633759362
Epoch 1252/10000, Prediction Accuracy = 47.29600000000001%, Loss = 1.5397363185882569
Epoch: 1252, Batch Gradient Norm: 37.294887168179805
Epoch: 1252, Batch Gradient Norm after: 22.36068102526527
Epoch 1253/10000, Prediction Accuracy = 47.372%, Loss = 1.5205903768539428
Epoch: 1253, Batch Gradient Norm: 43.30168889128183
Epoch: 1253, Batch Gradient Norm after: 22.360676658697287
Epoch 1254/10000, Prediction Accuracy = 47.302%, Loss = 1.5387194156646729
Epoch: 1254, Batch Gradient Norm: 37.32415330016727
Epoch: 1254, Batch Gradient Norm after: 22.36067849267119
Epoch 1255/10000, Prediction Accuracy = 47.368%, Loss = 1.519537353515625
Epoch: 1255, Batch Gradient Norm: 43.34675848107276
Epoch: 1255, Batch Gradient Norm after: 22.36067849992096
Epoch 1256/10000, Prediction Accuracy = 47.3%, Loss = 1.537717628479004
Epoch: 1256, Batch Gradient Norm: 37.35160017761383
Epoch: 1256, Batch Gradient Norm after: 22.360677867214573
Epoch 1257/10000, Prediction Accuracy = 47.376%, Loss = 1.5184773206710815
Epoch: 1257, Batch Gradient Norm: 43.40052763731496
Epoch: 1257, Batch Gradient Norm after: 22.360677552155273
Epoch 1258/10000, Prediction Accuracy = 47.314%, Loss = 1.536767029762268
Epoch: 1258, Batch Gradient Norm: 37.38720551498001
Epoch: 1258, Batch Gradient Norm after: 22.360678597052853
Epoch 1259/10000, Prediction Accuracy = 47.388000000000005%, Loss = 1.517459797859192
Epoch: 1259, Batch Gradient Norm: 43.46715923281703
Epoch: 1259, Batch Gradient Norm after: 22.360677487007987
Epoch 1260/10000, Prediction Accuracy = 47.309999999999995%, Loss = 1.5358347177505494
Epoch: 1260, Batch Gradient Norm: 37.42144393945213
Epoch: 1260, Batch Gradient Norm after: 22.360677578940273
Epoch 1261/10000, Prediction Accuracy = 47.392%, Loss = 1.5164256811141967
Epoch: 1261, Batch Gradient Norm: 43.51960729467508
Epoch: 1261, Batch Gradient Norm after: 22.360677220762696
Epoch 1262/10000, Prediction Accuracy = 47.309999999999995%, Loss = 1.5348675966262817
Epoch: 1262, Batch Gradient Norm: 37.449620227241134
Epoch 1263/10000, Prediction Accuracy = 47.406%, Loss = 1.5153903007507323
Epoch: 1263, Batch Gradient Norm: 43.56010772077612
Epoch: 1263, Batch Gradient Norm after: 22.36067728859326
Epoch 1264/10000, Prediction Accuracy = 47.328%, Loss = 1.5338567733764648
Epoch: 1264, Batch Gradient Norm: 37.47479879390091
Epoch: 1264, Batch Gradient Norm after: 22.36067846934931
Epoch 1265/10000, Prediction Accuracy = 47.402%, Loss = 1.5143360137939452
Epoch: 1265, Batch Gradient Norm: 43.5892535678719
Epoch: 1265, Batch Gradient Norm after: 22.360678403472175
Epoch 1266/10000, Prediction Accuracy = 47.344%, Loss = 1.5328239679336548
Epoch: 1266, Batch Gradient Norm: 37.49625206914177
Epoch: 1266, Batch Gradient Norm after: 22.360677534554096
Epoch 1267/10000, Prediction Accuracy = 47.410000000000004%, Loss = 1.5132755517959595
Epoch: 1267, Batch Gradient Norm: 43.59651478054153
Epoch: 1267, Batch Gradient Norm after: 22.360676950701162
Epoch 1268/10000, Prediction Accuracy = 47.36%, Loss = 1.531711721420288
Epoch: 1268, Batch Gradient Norm: 37.51234371714067
Epoch: 1268, Batch Gradient Norm after: 22.360678343839897
Epoch 1269/10000, Prediction Accuracy = 47.416000000000004%, Loss = 1.5121957063674927
Epoch: 1269, Batch Gradient Norm: 43.58823824948115
Epoch: 1269, Batch Gradient Norm after: 22.360678416802358
Epoch 1270/10000, Prediction Accuracy = 47.362%, Loss = 1.5305469751358032
Epoch: 1270, Batch Gradient Norm: 37.531828649823126
Epoch: 1270, Batch Gradient Norm after: 22.36067537333169
Epoch 1271/10000, Prediction Accuracy = 47.42400000000001%, Loss = 1.511138129234314
Epoch: 1271, Batch Gradient Norm: 43.594762762870545
Epoch: 1271, Batch Gradient Norm after: 22.360679489663475
Epoch 1272/10000, Prediction Accuracy = 47.378%, Loss = 1.5294378042221068
Epoch: 1272, Batch Gradient Norm: 37.548486059113785
Epoch: 1272, Batch Gradient Norm after: 22.36067863475464
Epoch 1273/10000, Prediction Accuracy = 47.416000000000004%, Loss = 1.510075831413269
Epoch: 1273, Batch Gradient Norm: 43.59008130274051
Epoch: 1273, Batch Gradient Norm after: 22.360679062487435
Epoch 1274/10000, Prediction Accuracy = 47.385999999999996%, Loss = 1.528277540206909
Epoch: 1274, Batch Gradient Norm: 37.55918499533707
Epoch: 1274, Batch Gradient Norm after: 22.36067800747941
Epoch 1275/10000, Prediction Accuracy = 47.42399999999999%, Loss = 1.5089855670928956
Epoch: 1275, Batch Gradient Norm: 43.57892425825233
Epoch: 1275, Batch Gradient Norm after: 22.360677852707166
Epoch 1276/10000, Prediction Accuracy = 47.392%, Loss = 1.5271142959594726
Epoch: 1276, Batch Gradient Norm: 37.572130858140305
Epoch: 1276, Batch Gradient Norm after: 22.36067962446906
Epoch 1277/10000, Prediction Accuracy = 47.428000000000004%, Loss = 1.507924485206604
Epoch: 1277, Batch Gradient Norm: 43.57702209656909
Epoch: 1277, Batch Gradient Norm after: 22.360678560751822
Epoch 1278/10000, Prediction Accuracy = 47.412%, Loss = 1.526003909111023
Epoch: 1278, Batch Gradient Norm: 37.59003223188282
Epoch: 1278, Batch Gradient Norm after: 22.360677196674843
Epoch 1279/10000, Prediction Accuracy = 47.446%, Loss = 1.5068788528442383
Epoch: 1279, Batch Gradient Norm: 43.601042992428816
Epoch: 1279, Batch Gradient Norm after: 22.36067733402284
Epoch 1280/10000, Prediction Accuracy = 47.436%, Loss = 1.5249698400497436
Epoch: 1280, Batch Gradient Norm: 37.61796564450678
Epoch: 1280, Batch Gradient Norm after: 22.360678106031223
Epoch 1281/10000, Prediction Accuracy = 47.468%, Loss = 1.5058615684509278
Epoch: 1281, Batch Gradient Norm: 43.621465221517475
Epoch: 1281, Batch Gradient Norm after: 22.360678556765226
Epoch 1282/10000, Prediction Accuracy = 47.44%, Loss = 1.5239349365234376
Epoch: 1282, Batch Gradient Norm: 37.6379415186421
Epoch: 1282, Batch Gradient Norm after: 22.36067679250121
Epoch 1283/10000, Prediction Accuracy = 47.470000000000006%, Loss = 1.5048304557800294
Epoch: 1283, Batch Gradient Norm: 43.63319055742325
Epoch: 1283, Batch Gradient Norm after: 22.360680073110014
Epoch 1284/10000, Prediction Accuracy = 47.44199999999999%, Loss = 1.5228743076324462
Epoch: 1284, Batch Gradient Norm: 37.651374951592274
Epoch: 1284, Batch Gradient Norm after: 22.36067752701098
Epoch 1285/10000, Prediction Accuracy = 47.464%, Loss = 1.5037782907485961
Epoch: 1285, Batch Gradient Norm: 43.616298926351895
Epoch: 1285, Batch Gradient Norm after: 22.360678510733756
Epoch 1286/10000, Prediction Accuracy = 47.45%, Loss = 1.5217426538467407
Epoch: 1286, Batch Gradient Norm: 37.66242671020554
Epoch: 1286, Batch Gradient Norm after: 22.3606770945089
Epoch 1287/10000, Prediction Accuracy = 47.47%, Loss = 1.5027312994003297
Epoch: 1287, Batch Gradient Norm: 43.6197770759708
Epoch: 1287, Batch Gradient Norm after: 22.360677344170703
Epoch 1288/10000, Prediction Accuracy = 47.464%, Loss = 1.520664668083191
Epoch: 1288, Batch Gradient Norm: 37.678618289427284
Epoch: 1288, Batch Gradient Norm after: 22.360675922195878
Epoch 1289/10000, Prediction Accuracy = 47.470000000000006%, Loss = 1.5017041206359862
Epoch: 1289, Batch Gradient Norm: 43.634741710192884
Epoch: 1289, Batch Gradient Norm after: 22.360678035962064
Epoch 1290/10000, Prediction Accuracy = 47.456%, Loss = 1.5196297883987426
Epoch: 1290, Batch Gradient Norm: 37.70092442016018
Epoch: 1290, Batch Gradient Norm after: 22.36067741057562
Epoch 1291/10000, Prediction Accuracy = 47.470000000000006%, Loss = 1.500679111480713
Epoch: 1291, Batch Gradient Norm: 43.64941595140645
Epoch: 1291, Batch Gradient Norm after: 22.36067990709446
Epoch 1292/10000, Prediction Accuracy = 47.470000000000006%, Loss = 1.518589735031128
Epoch: 1292, Batch Gradient Norm: 37.72022149598237
Epoch: 1292, Batch Gradient Norm after: 22.36067727699872
Epoch 1293/10000, Prediction Accuracy = 47.482%, Loss = 1.4996589183807374
Epoch: 1293, Batch Gradient Norm: 43.65898786654859
Epoch: 1293, Batch Gradient Norm after: 22.360679138179197
Epoch 1294/10000, Prediction Accuracy = 47.468%, Loss = 1.5175374746322632
Epoch: 1294, Batch Gradient Norm: 37.734870569348196
Epoch: 1294, Batch Gradient Norm after: 22.360680884749605
Epoch 1295/10000, Prediction Accuracy = 47.486000000000004%, Loss = 1.4986249208450317
Epoch: 1295, Batch Gradient Norm: 43.65775863140237
Epoch: 1295, Batch Gradient Norm after: 22.36067943011401
Epoch 1296/10000, Prediction Accuracy = 47.474000000000004%, Loss = 1.516443943977356
Epoch: 1296, Batch Gradient Norm: 37.74913143185271
Epoch: 1296, Batch Gradient Norm after: 22.360677105855135
Epoch 1297/10000, Prediction Accuracy = 47.49%, Loss = 1.4976094722747804
Epoch: 1297, Batch Gradient Norm: 43.65822121364116
Epoch: 1297, Batch Gradient Norm after: 22.360677540202477
Epoch 1298/10000, Prediction Accuracy = 47.482%, Loss = 1.5153859615325929
Epoch: 1298, Batch Gradient Norm: 37.76045827641903
Epoch: 1298, Batch Gradient Norm after: 22.360677392804078
Epoch 1299/10000, Prediction Accuracy = 47.482000000000006%, Loss = 1.4965957164764405
Epoch: 1299, Batch Gradient Norm: 43.64797427597991
Epoch: 1299, Batch Gradient Norm after: 22.360678458727023
Epoch 1300/10000, Prediction Accuracy = 47.486000000000004%, Loss = 1.5142857313156128
Epoch: 1300, Batch Gradient Norm: 37.77175997277203
Epoch: 1300, Batch Gradient Norm after: 22.360679856415796
Epoch 1301/10000, Prediction Accuracy = 47.502%, Loss = 1.495576596260071
Epoch: 1301, Batch Gradient Norm: 43.63963090857013
Epoch: 1301, Batch Gradient Norm after: 22.360678976179294
Epoch 1302/10000, Prediction Accuracy = 47.49%, Loss = 1.5131811857223512
Epoch: 1302, Batch Gradient Norm: 37.790472869341336
Epoch: 1302, Batch Gradient Norm after: 22.36067760001082
Epoch 1303/10000, Prediction Accuracy = 47.51199999999999%, Loss = 1.4945810317993165
Epoch: 1303, Batch Gradient Norm: 43.641000067692694
Epoch: 1303, Batch Gradient Norm after: 22.360678474482977
Epoch 1304/10000, Prediction Accuracy = 47.504%, Loss = 1.5121300220489502
Epoch: 1304, Batch Gradient Norm: 37.80279891788533
Epoch: 1304, Batch Gradient Norm after: 22.360679198468855
Epoch 1305/10000, Prediction Accuracy = 47.51%, Loss = 1.4935809135437013
Epoch: 1305, Batch Gradient Norm: 43.64243684600516
Epoch: 1305, Batch Gradient Norm after: 22.360678295874933
Epoch 1306/10000, Prediction Accuracy = 47.504000000000005%, Loss = 1.5110753536224366
Epoch: 1306, Batch Gradient Norm: 37.812831397940904
Epoch: 1306, Batch Gradient Norm after: 22.36067816234681
Epoch 1307/10000, Prediction Accuracy = 47.519999999999996%, Loss = 1.4925660848617555
Epoch: 1307, Batch Gradient Norm: 43.63668654548455
Epoch: 1307, Batch Gradient Norm after: 22.360676520357046
Epoch 1308/10000, Prediction Accuracy = 47.519999999999996%, Loss = 1.5099963665008544
Epoch: 1308, Batch Gradient Norm: 37.81859024602383
Epoch: 1308, Batch Gradient Norm after: 22.360676864816657
Epoch 1309/10000, Prediction Accuracy = 47.52400000000001%, Loss = 1.4915348291397095
Epoch: 1309, Batch Gradient Norm: 43.614020069542995
Epoch: 1309, Batch Gradient Norm after: 22.3606796850892
Epoch 1310/10000, Prediction Accuracy = 47.534%, Loss = 1.5088773488998413
Epoch: 1310, Batch Gradient Norm: 37.82202411636733
Epoch: 1310, Batch Gradient Norm after: 22.360677750892137
Epoch 1311/10000, Prediction Accuracy = 47.518%, Loss = 1.4905133724212647
Epoch: 1311, Batch Gradient Norm: 43.60705064885462
Epoch: 1311, Batch Gradient Norm after: 22.36067791363188
Epoch 1312/10000, Prediction Accuracy = 47.541999999999994%, Loss = 1.5078238487243651
Epoch: 1312, Batch Gradient Norm: 37.83414041535659
Epoch: 1312, Batch Gradient Norm after: 22.360678977575255
Epoch 1313/10000, Prediction Accuracy = 47.538%, Loss = 1.4895087242126466
Epoch: 1313, Batch Gradient Norm: 43.597275249009925
Epoch: 1313, Batch Gradient Norm after: 22.360676787439434
Epoch 1314/10000, Prediction Accuracy = 47.541999999999994%, Loss = 1.506741452217102
Epoch: 1314, Batch Gradient Norm: 37.84592995230119
Epoch: 1314, Batch Gradient Norm after: 22.360678557892463
Epoch 1315/10000, Prediction Accuracy = 47.56%, Loss = 1.488529586791992
Epoch: 1315, Batch Gradient Norm: 43.60522740098041
Epoch: 1315, Batch Gradient Norm after: 22.360679142366603
Epoch 1316/10000, Prediction Accuracy = 47.544%, Loss = 1.505731964111328
Epoch: 1316, Batch Gradient Norm: 37.86274760584952
Epoch: 1316, Batch Gradient Norm after: 22.360678005843646
Epoch 1317/10000, Prediction Accuracy = 47.562000000000005%, Loss = 1.4875632524490356
Epoch: 1317, Batch Gradient Norm: 43.624870633353645
Epoch: 1317, Batch Gradient Norm after: 22.360678041202114
Epoch 1318/10000, Prediction Accuracy = 47.556000000000004%, Loss = 1.5047732591629028
Epoch: 1318, Batch Gradient Norm: 37.88253051113923
Epoch: 1318, Batch Gradient Norm after: 22.36067862359995
Epoch 1319/10000, Prediction Accuracy = 47.562%, Loss = 1.4866077661514283
Epoch: 1319, Batch Gradient Norm: 43.66199207550357
Epoch: 1319, Batch Gradient Norm after: 22.360677109918754
Epoch 1320/10000, Prediction Accuracy = 47.56%, Loss = 1.5038624048233031
Epoch: 1320, Batch Gradient Norm: 37.905675264986705
Epoch: 1320, Batch Gradient Norm after: 22.360678903362146
Epoch 1321/10000, Prediction Accuracy = 47.57%, Loss = 1.4856684446334838
Epoch: 1321, Batch Gradient Norm: 43.69655054562155
Epoch: 1321, Batch Gradient Norm after: 22.360675983718714
Epoch 1322/10000, Prediction Accuracy = 47.568%, Loss = 1.502958345413208
Epoch: 1322, Batch Gradient Norm: 37.9263834668708
Epoch: 1322, Batch Gradient Norm after: 22.360678326913042
Epoch 1323/10000, Prediction Accuracy = 47.568%, Loss = 1.4847163677215576
Epoch: 1323, Batch Gradient Norm: 43.71534703246634
Epoch: 1323, Batch Gradient Norm after: 22.360678952759173
Epoch 1324/10000, Prediction Accuracy = 47.574%, Loss = 1.5020009279251099
Epoch: 1324, Batch Gradient Norm: 37.945154800031524
Epoch: 1324, Batch Gradient Norm after: 22.360677439047326
Epoch 1325/10000, Prediction Accuracy = 47.572%, Loss = 1.4837533235549927
Epoch: 1325, Batch Gradient Norm: 43.73450540293713
Epoch: 1325, Batch Gradient Norm after: 22.36067828223639
Epoch 1326/10000, Prediction Accuracy = 47.58200000000001%, Loss = 1.5010345935821534
Epoch: 1326, Batch Gradient Norm: 37.960783987800646
Epoch: 1326, Batch Gradient Norm after: 22.360679092504643
Epoch 1327/10000, Prediction Accuracy = 47.586%, Loss = 1.4828006505966187
Epoch: 1327, Batch Gradient Norm: 43.74110360906806
Epoch: 1327, Batch Gradient Norm after: 22.360677372049082
Epoch 1328/10000, Prediction Accuracy = 47.586%, Loss = 1.500052237510681
Epoch: 1328, Batch Gradient Norm: 37.97290306215211
Epoch: 1328, Batch Gradient Norm after: 22.36067771745638
Epoch 1329/10000, Prediction Accuracy = 47.598%, Loss = 1.4818369626998902
Epoch: 1329, Batch Gradient Norm: 43.74054636304076
Epoch: 1329, Batch Gradient Norm after: 22.360677842201003
Epoch 1330/10000, Prediction Accuracy = 47.602%, Loss = 1.499039316177368
Epoch: 1330, Batch Gradient Norm: 37.9863229368196
Epoch: 1330, Batch Gradient Norm after: 22.360678640536246
Epoch 1331/10000, Prediction Accuracy = 47.611999999999995%, Loss = 1.4808668851852418
Epoch: 1331, Batch Gradient Norm: 43.7427213853852
Epoch: 1331, Batch Gradient Norm after: 22.360680881215245
Epoch 1332/10000, Prediction Accuracy = 47.616%, Loss = 1.4980289459228515
Epoch: 1332, Batch Gradient Norm: 37.99860224399735
Epoch: 1332, Batch Gradient Norm after: 22.360676713482036
Epoch 1333/10000, Prediction Accuracy = 47.620000000000005%, Loss = 1.4799092769622804
Epoch: 1333, Batch Gradient Norm: 43.7528074843284
Epoch: 1333, Batch Gradient Norm after: 22.360678445452592
Epoch 1334/10000, Prediction Accuracy = 47.617999999999995%, Loss = 1.4970532655715942
Epoch: 1334, Batch Gradient Norm: 38.01199486356968
Epoch: 1334, Batch Gradient Norm after: 22.360677438774367
Epoch 1335/10000, Prediction Accuracy = 47.617999999999995%, Loss = 1.478953766822815
Epoch: 1335, Batch Gradient Norm: 43.76298308458219
Epoch: 1335, Batch Gradient Norm after: 22.360678450189575
Epoch 1336/10000, Prediction Accuracy = 47.617999999999995%, Loss = 1.49607355594635
Epoch: 1336, Batch Gradient Norm: 38.0289633391914
Epoch: 1336, Batch Gradient Norm after: 22.36067712550694
Epoch 1337/10000, Prediction Accuracy = 47.624%, Loss = 1.4780049085617066
Epoch: 1337, Batch Gradient Norm: 43.7705021228528
Epoch: 1337, Batch Gradient Norm after: 22.360677764866793
Epoch 1338/10000, Prediction Accuracy = 47.612%, Loss = 1.4950859308242799
Epoch: 1338, Batch Gradient Norm: 38.042015826472955
Epoch: 1338, Batch Gradient Norm after: 22.360678317496134
Epoch 1339/10000, Prediction Accuracy = 47.62%, Loss = 1.4770637273788452
Epoch: 1339, Batch Gradient Norm: 43.78400106795942
Epoch: 1339, Batch Gradient Norm after: 22.360677759106167
Epoch 1340/10000, Prediction Accuracy = 47.614%, Loss = 1.4941309452056886
Epoch: 1340, Batch Gradient Norm: 38.0592392103597
Epoch: 1340, Batch Gradient Norm after: 22.360678385931937
Epoch 1341/10000, Prediction Accuracy = 47.626%, Loss = 1.4761214971542358
Epoch: 1341, Batch Gradient Norm: 43.78566386412836
Epoch: 1341, Batch Gradient Norm after: 22.360676742785465
Epoch 1342/10000, Prediction Accuracy = 47.624%, Loss = 1.4931435108184814
Epoch: 1342, Batch Gradient Norm: 38.07412330178806
Epoch: 1342, Batch Gradient Norm after: 22.360680047840493
Epoch 1343/10000, Prediction Accuracy = 47.641999999999996%, Loss = 1.47516667842865
Epoch: 1343, Batch Gradient Norm: 43.79273306439582
Epoch: 1343, Batch Gradient Norm after: 22.36067601976053
Epoch 1344/10000, Prediction Accuracy = 47.624%, Loss = 1.4921605348587037
Epoch: 1344, Batch Gradient Norm: 38.08661269465014
Epoch: 1344, Batch Gradient Norm after: 22.36067679189625
Epoch 1345/10000, Prediction Accuracy = 47.636%, Loss = 1.4742381572723389
Epoch: 1345, Batch Gradient Norm: 43.800146182958535
Epoch: 1345, Batch Gradient Norm after: 22.36067612327157
Epoch 1346/10000, Prediction Accuracy = 47.656%, Loss = 1.4911899089813232
Epoch: 1346, Batch Gradient Norm: 38.101366585537924
Epoch: 1346, Batch Gradient Norm after: 22.360679315170696
Epoch 1347/10000, Prediction Accuracy = 47.642%, Loss = 1.4732985734939574
Epoch: 1347, Batch Gradient Norm: 43.811351165431795
Epoch: 1347, Batch Gradient Norm after: 22.360676735203285
Epoch 1348/10000, Prediction Accuracy = 47.65%, Loss = 1.4902411222457885
Epoch: 1348, Batch Gradient Norm: 38.113674552401385
Epoch: 1348, Batch Gradient Norm after: 22.360678561991435
Epoch 1349/10000, Prediction Accuracy = 47.644%, Loss = 1.472357201576233
Epoch: 1349, Batch Gradient Norm: 43.82366036165557
Epoch: 1349, Batch Gradient Norm after: 22.360676202918846
Epoch 1350/10000, Prediction Accuracy = 47.646%, Loss = 1.4892954349517822
Epoch: 1350, Batch Gradient Norm: 38.12918419037149
Epoch: 1350, Batch Gradient Norm after: 22.360678545941624
Epoch 1351/10000, Prediction Accuracy = 47.65%, Loss = 1.471433401107788
Epoch: 1351, Batch Gradient Norm: 43.838464621061625
Epoch: 1351, Batch Gradient Norm after: 22.360678818673165
Epoch 1352/10000, Prediction Accuracy = 47.666%, Loss = 1.4883687019348144
Epoch: 1352, Batch Gradient Norm: 38.14782446182245
Epoch: 1352, Batch Gradient Norm after: 22.360676440897233
Epoch 1353/10000, Prediction Accuracy = 47.662000000000006%, Loss = 1.4705077886581421
Epoch: 1353, Batch Gradient Norm: 43.85459829215888
Epoch: 1353, Batch Gradient Norm after: 22.360677684921576
Epoch 1354/10000, Prediction Accuracy = 47.666%, Loss = 1.4874390602111816
Epoch: 1354, Batch Gradient Norm: 38.162745007645796
Epoch: 1354, Batch Gradient Norm after: 22.360677374740245
Epoch 1355/10000, Prediction Accuracy = 47.669999999999995%, Loss = 1.469583559036255
Epoch: 1355, Batch Gradient Norm: 43.86545452161234
Epoch: 1355, Batch Gradient Norm after: 22.360676888032692
Epoch 1356/10000, Prediction Accuracy = 47.668%, Loss = 1.4864952325820924
Epoch: 1356, Batch Gradient Norm: 38.18093699826071
Epoch: 1356, Batch Gradient Norm after: 22.36067790775461
Epoch 1357/10000, Prediction Accuracy = 47.676%, Loss = 1.468658685684204
Epoch: 1357, Batch Gradient Norm: 43.88156745155619
Epoch: 1357, Batch Gradient Norm after: 22.360678734254567
Epoch 1358/10000, Prediction Accuracy = 47.682%, Loss = 1.48556067943573
Epoch: 1358, Batch Gradient Norm: 38.20152194816125
Epoch: 1358, Batch Gradient Norm after: 22.360677902843605
Epoch 1359/10000, Prediction Accuracy = 47.68599999999999%, Loss = 1.467747449874878
Epoch: 1359, Batch Gradient Norm: 43.88932441087254
Epoch: 1359, Batch Gradient Norm after: 22.36067787639147
Epoch 1360/10000, Prediction Accuracy = 47.702%, Loss = 1.484610414505005
Epoch: 1360, Batch Gradient Norm: 38.21099429127209
Epoch: 1360, Batch Gradient Norm after: 22.36067591156276
Epoch 1361/10000, Prediction Accuracy = 47.692%, Loss = 1.4668160200119018
Epoch: 1361, Batch Gradient Norm: 43.88183295825609
Epoch: 1361, Batch Gradient Norm after: 22.360675341056844
Epoch 1362/10000, Prediction Accuracy = 47.705999999999996%, Loss = 1.4836127281188964
Epoch: 1362, Batch Gradient Norm: 38.22029980453007
Epoch: 1362, Batch Gradient Norm after: 22.360676846494325
Epoch 1363/10000, Prediction Accuracy = 47.692%, Loss = 1.465871810913086
Epoch: 1363, Batch Gradient Norm: 43.85883380354477
Epoch: 1363, Batch Gradient Norm after: 22.360675810634497
Epoch 1364/10000, Prediction Accuracy = 47.727999999999994%, Loss = 1.4825660705566406
Epoch: 1364, Batch Gradient Norm: 38.23184173821621
Epoch: 1364, Batch Gradient Norm after: 22.360677793052247
Epoch 1365/10000, Prediction Accuracy = 47.708000000000006%, Loss = 1.464930248260498
Epoch: 1365, Batch Gradient Norm: 43.84448059575452
Epoch: 1365, Batch Gradient Norm after: 22.360677391378907
Epoch 1366/10000, Prediction Accuracy = 47.733999999999995%, Loss = 1.4815485000610351
Epoch: 1366, Batch Gradient Norm: 38.239284061714514
Epoch: 1366, Batch Gradient Norm after: 22.36067741419363
Epoch 1367/10000, Prediction Accuracy = 47.722%, Loss = 1.4639965295791626
Epoch: 1367, Batch Gradient Norm: 43.833453678039085
Epoch: 1367, Batch Gradient Norm after: 22.360675322845808
Epoch 1368/10000, Prediction Accuracy = 47.736000000000004%, Loss = 1.4805418252944946
Epoch: 1368, Batch Gradient Norm: 38.24718498388061
Epoch: 1368, Batch Gradient Norm after: 22.3606789944234
Epoch 1369/10000, Prediction Accuracy = 47.717999999999996%, Loss = 1.463068413734436
Epoch: 1369, Batch Gradient Norm: 43.81958810379157
Epoch: 1369, Batch Gradient Norm after: 22.360677728625788
Epoch 1370/10000, Prediction Accuracy = 47.75%, Loss = 1.479549765586853
Epoch: 1370, Batch Gradient Norm: 38.25864283296753
Epoch: 1370, Batch Gradient Norm after: 22.360678683648015
Epoch 1371/10000, Prediction Accuracy = 47.732%, Loss = 1.4621635913848876
Epoch: 1371, Batch Gradient Norm: 43.81755853389283
Epoch: 1371, Batch Gradient Norm after: 22.360676877248157
Epoch 1372/10000, Prediction Accuracy = 47.763999999999996%, Loss = 1.478581976890564
Epoch: 1372, Batch Gradient Norm: 38.26963419721113
Epoch: 1372, Batch Gradient Norm after: 22.360678282752843
Epoch 1373/10000, Prediction Accuracy = 47.75%, Loss = 1.461245322227478
Epoch: 1373, Batch Gradient Norm: 43.8158067768258
Epoch: 1373, Batch Gradient Norm after: 22.36067580691643
Epoch 1374/10000, Prediction Accuracy = 47.76800000000001%, Loss = 1.4776148796081543
Epoch: 1374, Batch Gradient Norm: 38.285034743659715
Epoch: 1374, Batch Gradient Norm after: 22.360677846672964
Epoch 1375/10000, Prediction Accuracy = 47.763999999999996%, Loss = 1.4603494167327882
Epoch: 1375, Batch Gradient Norm: 43.8173550163348
Epoch: 1375, Batch Gradient Norm after: 22.36067675497093
Epoch 1376/10000, Prediction Accuracy = 47.782000000000004%, Loss = 1.4766772031784057
Epoch: 1376, Batch Gradient Norm: 38.29593893930553
Epoch: 1376, Batch Gradient Norm after: 22.36067845450983
Epoch 1377/10000, Prediction Accuracy = 47.766%, Loss = 1.4594518661499023
Epoch: 1377, Batch Gradient Norm: 43.81408022268207
Epoch: 1377, Batch Gradient Norm after: 22.36067403651206
Epoch 1378/10000, Prediction Accuracy = 47.786%, Loss = 1.4757247686386108
Epoch: 1378, Batch Gradient Norm: 38.30735289602718
Epoch: 1378, Batch Gradient Norm after: 22.360677927040484
Epoch 1379/10000, Prediction Accuracy = 47.773999999999994%, Loss = 1.458552074432373
Epoch: 1379, Batch Gradient Norm: 43.804685408324026
Epoch: 1379, Batch Gradient Norm after: 22.36067521147139
Epoch 1380/10000, Prediction Accuracy = 47.804%, Loss = 1.474739170074463
Epoch: 1380, Batch Gradient Norm: 38.31928166914444
Epoch: 1380, Batch Gradient Norm after: 22.36067670358033
Epoch 1381/10000, Prediction Accuracy = 47.774%, Loss = 1.4576602697372436
Epoch: 1381, Batch Gradient Norm: 43.79469050396834
Epoch: 1381, Batch Gradient Norm after: 22.360676845678917
Epoch 1382/10000, Prediction Accuracy = 47.812%, Loss = 1.4737759828567505
Epoch: 1382, Batch Gradient Norm: 38.326548106519624
Epoch: 1382, Batch Gradient Norm after: 22.36067906289513
Epoch 1383/10000, Prediction Accuracy = 47.78000000000001%, Loss = 1.456760573387146
Epoch: 1383, Batch Gradient Norm: 43.78171563318577
Epoch: 1383, Batch Gradient Norm after: 22.360677600745902
Epoch 1384/10000, Prediction Accuracy = 47.81399999999999%, Loss = 1.4727897882461547
Epoch: 1384, Batch Gradient Norm: 38.332553343282186
Epoch: 1384, Batch Gradient Norm after: 22.360679967297663
Epoch 1385/10000, Prediction Accuracy = 47.798%, Loss = 1.455854344367981
Epoch: 1385, Batch Gradient Norm: 43.76384376109575
Epoch: 1385, Batch Gradient Norm after: 22.360677352381067
Epoch 1386/10000, Prediction Accuracy = 47.815999999999995%, Loss = 1.4717983961105348
Epoch: 1386, Batch Gradient Norm: 38.34228707065037
Epoch: 1386, Batch Gradient Norm after: 22.360677363752263
Epoch 1387/10000, Prediction Accuracy = 47.794%, Loss = 1.4549484968185424
Epoch: 1387, Batch Gradient Norm: 43.73468693164894
Epoch: 1387, Batch Gradient Norm after: 22.36067541242303
Epoch 1388/10000, Prediction Accuracy = 47.818%, Loss = 1.47077317237854
Epoch: 1388, Batch Gradient Norm: 38.34379156684193
Epoch: 1388, Batch Gradient Norm after: 22.36067784699466
Epoch 1389/10000, Prediction Accuracy = 47.797999999999995%, Loss = 1.4540401697158813
Epoch: 1389, Batch Gradient Norm: 43.696234174968666
Epoch: 1389, Batch Gradient Norm after: 22.360676943327768
Epoch 1390/10000, Prediction Accuracy = 47.824%, Loss = 1.4697251081466676
Epoch: 1390, Batch Gradient Norm: 38.34849312735168
Epoch: 1390, Batch Gradient Norm after: 22.36067891069555
Epoch 1391/10000, Prediction Accuracy = 47.8%, Loss = 1.4531380653381347
Epoch: 1391, Batch Gradient Norm: 43.650065736574085
Epoch: 1391, Batch Gradient Norm after: 22.360676549437432
Epoch 1392/10000, Prediction Accuracy = 47.832%, Loss = 1.4686673164367676
Epoch: 1392, Batch Gradient Norm: 38.35320955672407
Epoch: 1392, Batch Gradient Norm after: 22.360679278536143
Epoch 1393/10000, Prediction Accuracy = 47.808%, Loss = 1.4522422790527343
Epoch: 1393, Batch Gradient Norm: 43.60899736587744
Epoch: 1393, Batch Gradient Norm after: 22.36067764090222
Epoch 1394/10000, Prediction Accuracy = 47.84000000000001%, Loss = 1.4676092863082886
Epoch: 1394, Batch Gradient Norm: 38.35783468874937
Epoch: 1394, Batch Gradient Norm after: 22.360677593734298
Epoch 1395/10000, Prediction Accuracy = 47.81400000000001%, Loss = 1.4513484716415406
Epoch: 1395, Batch Gradient Norm: 43.56190120055152
Epoch: 1395, Batch Gradient Norm after: 22.360677131579724
Epoch 1396/10000, Prediction Accuracy = 47.854%, Loss = 1.4665297508239745
Epoch: 1396, Batch Gradient Norm: 38.36047449052141
Epoch: 1396, Batch Gradient Norm after: 22.360679976206498
Epoch 1397/10000, Prediction Accuracy = 47.827999999999996%, Loss = 1.4504586458206177
Epoch: 1397, Batch Gradient Norm: 43.51727429432853
Epoch: 1397, Batch Gradient Norm after: 22.36067911107255
Epoch 1398/10000, Prediction Accuracy = 47.872%, Loss = 1.4654884338378906
Epoch: 1398, Batch Gradient Norm: 38.36087224974143
Epoch: 1398, Batch Gradient Norm after: 22.360679084557038
Epoch 1399/10000, Prediction Accuracy = 47.842%, Loss = 1.4495760917663574
Epoch: 1399, Batch Gradient Norm: 43.484604180259616
Epoch: 1399, Batch Gradient Norm after: 22.360676845738332
Epoch 1400/10000, Prediction Accuracy = 47.87%, Loss = 1.4644947528839112
Epoch: 1400, Batch Gradient Norm: 38.36437356430904
Epoch: 1400, Batch Gradient Norm after: 22.360679621361772
Epoch 1401/10000, Prediction Accuracy = 47.846%, Loss = 1.448708176612854
Epoch: 1401, Batch Gradient Norm: 43.45101807257308
Epoch: 1401, Batch Gradient Norm after: 22.36067580864807
Epoch 1402/10000, Prediction Accuracy = 47.884%, Loss = 1.4634951591491698
Epoch: 1402, Batch Gradient Norm: 38.37318752821951
Epoch: 1402, Batch Gradient Norm after: 22.360679882511878
Epoch 1403/10000, Prediction Accuracy = 47.852%, Loss = 1.4478409051895142
Epoch: 1403, Batch Gradient Norm: 43.41799245022429
Epoch: 1403, Batch Gradient Norm after: 22.360676123037273
Epoch 1404/10000, Prediction Accuracy = 47.89%, Loss = 1.4625055551528932
Epoch: 1404, Batch Gradient Norm: 38.37638510108016
Epoch: 1404, Batch Gradient Norm after: 22.3606779331663
Epoch 1405/10000, Prediction Accuracy = 47.862%, Loss = 1.4469681978225708
Epoch: 1405, Batch Gradient Norm: 43.3965917580751
Epoch: 1405, Batch Gradient Norm after: 22.360675681895355
Epoch 1406/10000, Prediction Accuracy = 47.902%, Loss = 1.4615564584732055
Epoch: 1406, Batch Gradient Norm: 38.385458025031014
Epoch: 1406, Batch Gradient Norm after: 22.360679830974593
Epoch 1407/10000, Prediction Accuracy = 47.872%, Loss = 1.4461186885833741
Epoch: 1407, Batch Gradient Norm: 43.382151143002844
Epoch: 1407, Batch Gradient Norm after: 22.36067778479384
Epoch 1408/10000, Prediction Accuracy = 47.904%, Loss = 1.4606380939483643
Epoch: 1408, Batch Gradient Norm: 38.390501308506884
Epoch: 1408, Batch Gradient Norm after: 22.360678595101216
Epoch 1409/10000, Prediction Accuracy = 47.87%, Loss = 1.4452747583389283
Epoch: 1409, Batch Gradient Norm: 43.37388558330855
Epoch: 1409, Batch Gradient Norm after: 22.36067774758662
Epoch 1410/10000, Prediction Accuracy = 47.910000000000004%, Loss = 1.4597461700439454
Epoch: 1410, Batch Gradient Norm: 38.40183182083119
Epoch: 1410, Batch Gradient Norm after: 22.36067898163903
Epoch 1411/10000, Prediction Accuracy = 47.882000000000005%, Loss = 1.4444304704666138
Epoch: 1411, Batch Gradient Norm: 43.37279096243564
Epoch: 1411, Batch Gradient Norm after: 22.360677965840097
Epoch 1412/10000, Prediction Accuracy = 47.916000000000004%, Loss = 1.4588634490966796
Epoch: 1412, Batch Gradient Norm: 38.41083603468385
Epoch: 1412, Batch Gradient Norm after: 22.360677461953923
Epoch 1413/10000, Prediction Accuracy = 47.882%, Loss = 1.4435965776443482
Epoch: 1413, Batch Gradient Norm: 43.37738802861576
Epoch: 1413, Batch Gradient Norm after: 22.36067685999436
Epoch 1414/10000, Prediction Accuracy = 47.914%, Loss = 1.4580031633377075
Epoch: 1414, Batch Gradient Norm: 38.42432526848888
Epoch: 1414, Batch Gradient Norm after: 22.360680096710475
Epoch 1415/10000, Prediction Accuracy = 47.894%, Loss = 1.4427667617797852
Epoch: 1415, Batch Gradient Norm: 43.38533340946383
Epoch: 1415, Batch Gradient Norm after: 22.360676410338648
Epoch 1416/10000, Prediction Accuracy = 47.928%, Loss = 1.457157039642334
Epoch: 1416, Batch Gradient Norm: 38.431802352550974
Epoch: 1416, Batch Gradient Norm after: 22.360676412371856
Epoch 1417/10000, Prediction Accuracy = 47.912000000000006%, Loss = 1.4419317722320557
Epoch: 1417, Batch Gradient Norm: 43.385444965516115
Epoch: 1417, Batch Gradient Norm after: 22.36067769926666
Epoch 1418/10000, Prediction Accuracy = 47.92999999999999%, Loss = 1.4562925815582275
Epoch: 1418, Batch Gradient Norm: 38.44473406289304
Epoch: 1418, Batch Gradient Norm after: 22.360680363540162
Epoch 1419/10000, Prediction Accuracy = 47.91799999999999%, Loss = 1.4411112785339355
Epoch: 1419, Batch Gradient Norm: 43.385900025713674
Epoch: 1419, Batch Gradient Norm after: 22.36067733109049
Epoch 1420/10000, Prediction Accuracy = 47.94%, Loss = 1.4554192543029785
Epoch: 1420, Batch Gradient Norm: 38.45416383895571
Epoch: 1420, Batch Gradient Norm after: 22.360678795861805
Epoch 1421/10000, Prediction Accuracy = 47.92%, Loss = 1.4402865171432495
Epoch: 1421, Batch Gradient Norm: 43.36907603079782
Epoch: 1421, Batch Gradient Norm after: 22.360677424493733
Epoch 1422/10000, Prediction Accuracy = 47.94799999999999%, Loss = 1.4544986963272095
Epoch: 1422, Batch Gradient Norm: 38.46545732761311
Epoch: 1422, Batch Gradient Norm after: 22.360680581785214
Epoch 1423/10000, Prediction Accuracy = 47.928000000000004%, Loss = 1.4394501447677612
Epoch: 1423, Batch Gradient Norm: 43.352618858331546
Epoch: 1423, Batch Gradient Norm after: 22.360677153674235
Epoch 1424/10000, Prediction Accuracy = 47.952%, Loss = 1.4535950899124146
Epoch: 1424, Batch Gradient Norm: 38.47553765066019
Epoch: 1424, Batch Gradient Norm after: 22.360677662255632
Epoch 1425/10000, Prediction Accuracy = 47.942%, Loss = 1.4386161804199218
Epoch: 1425, Batch Gradient Norm: 43.33697092785406
Epoch: 1425, Batch Gradient Norm after: 22.360678759380832
Epoch 1426/10000, Prediction Accuracy = 47.959999999999994%, Loss = 1.4526907205581665
Epoch: 1426, Batch Gradient Norm: 38.486403485931426
Epoch: 1426, Batch Gradient Norm after: 22.3606777912348
Epoch 1427/10000, Prediction Accuracy = 47.952000000000005%, Loss = 1.4377809524536134
Epoch: 1427, Batch Gradient Norm: 43.31977754380838
Epoch: 1427, Batch Gradient Norm after: 22.360679146772057
Epoch 1428/10000, Prediction Accuracy = 47.968%, Loss = 1.4517787456512452
Epoch: 1428, Batch Gradient Norm: 38.495434935265195
Epoch: 1428, Batch Gradient Norm after: 22.360677254996617
Epoch 1429/10000, Prediction Accuracy = 47.96%, Loss = 1.4369534730911255
Epoch: 1429, Batch Gradient Norm: 43.30421219365118
Epoch: 1429, Batch Gradient Norm after: 22.360679003976283
Epoch 1430/10000, Prediction Accuracy = 47.986000000000004%, Loss = 1.450891661643982
Epoch: 1430, Batch Gradient Norm: 38.50317322464737
Epoch: 1430, Batch Gradient Norm after: 22.360679425138937
Epoch 1431/10000, Prediction Accuracy = 47.972%, Loss = 1.4361384391784668
Epoch: 1431, Batch Gradient Norm: 43.295388582178
Epoch: 1431, Batch Gradient Norm after: 22.360678085479655
Epoch 1432/10000, Prediction Accuracy = 47.996%, Loss = 1.4500144958496093
Epoch: 1432, Batch Gradient Norm: 38.508247770942276
Epoch: 1432, Batch Gradient Norm after: 22.360676503747143
Epoch 1433/10000, Prediction Accuracy = 47.976%, Loss = 1.435327172279358
Epoch: 1433, Batch Gradient Norm: 43.29613251924665
Epoch: 1433, Batch Gradient Norm after: 22.360676952885342
Epoch 1434/10000, Prediction Accuracy = 48.007999999999996%, Loss = 1.449175763130188
Epoch: 1434, Batch Gradient Norm: 38.51474972456649
Epoch: 1434, Batch Gradient Norm after: 22.360678157976086
Epoch 1435/10000, Prediction Accuracy = 47.983999999999995%, Loss = 1.4345266342163085
Epoch: 1435, Batch Gradient Norm: 43.30696735572053
Epoch: 1435, Batch Gradient Norm after: 22.360678370021464
Epoch 1436/10000, Prediction Accuracy = 48.014%, Loss = 1.448362135887146
Epoch: 1436, Batch Gradient Norm: 38.524655927569775
Epoch: 1436, Batch Gradient Norm after: 22.36067820757346
Epoch 1437/10000, Prediction Accuracy = 47.998000000000005%, Loss = 1.4337180137634278
Epoch: 1437, Batch Gradient Norm: 43.31769094263263
Epoch: 1437, Batch Gradient Norm after: 22.360678832542725
Epoch 1438/10000, Prediction Accuracy = 48.017999999999994%, Loss = 1.447555923461914
Epoch: 1438, Batch Gradient Norm: 38.53253525010265
Epoch: 1438, Batch Gradient Norm after: 22.360676283261874
Epoch 1439/10000, Prediction Accuracy = 48.0%, Loss = 1.4329144954681396
Epoch: 1439, Batch Gradient Norm: 43.328750816149544
Epoch: 1439, Batch Gradient Norm after: 22.360676706852242
Epoch 1440/10000, Prediction Accuracy = 48.01800000000001%, Loss = 1.4467599630355834
Epoch: 1440, Batch Gradient Norm: 38.5442271433627
Epoch: 1440, Batch Gradient Norm after: 22.36067777660052
Epoch 1441/10000, Prediction Accuracy = 48.013999999999996%, Loss = 1.4321117162704469
Epoch: 1441, Batch Gradient Norm: 43.33675286821714
Epoch: 1441, Batch Gradient Norm after: 22.36067953254731
Epoch 1442/10000, Prediction Accuracy = 48.022%, Loss = 1.445965790748596
Epoch: 1442, Batch Gradient Norm: 38.5560564694856
Epoch: 1442, Batch Gradient Norm after: 22.360676607624352
Epoch 1443/10000, Prediction Accuracy = 48.025999999999996%, Loss = 1.4313200473785401
Epoch: 1443, Batch Gradient Norm: 43.34668851721238
Epoch: 1443, Batch Gradient Norm after: 22.36067691071569
Epoch 1444/10000, Prediction Accuracy = 48.022%, Loss = 1.4451774835586548
Epoch: 1444, Batch Gradient Norm: 38.56561698917844
Epoch: 1444, Batch Gradient Norm after: 22.360676669458453
Epoch 1445/10000, Prediction Accuracy = 48.025999999999996%, Loss = 1.430529284477234
Epoch: 1445, Batch Gradient Norm: 43.36908848727593
Epoch: 1445, Batch Gradient Norm after: 22.3606797665067
Epoch 1446/10000, Prediction Accuracy = 48.03%, Loss = 1.4444175720214845
Epoch: 1446, Batch Gradient Norm: 38.57807724086909
Epoch: 1446, Batch Gradient Norm after: 22.36067656532242
Epoch 1447/10000, Prediction Accuracy = 48.029999999999994%, Loss = 1.4297280788421631
Epoch: 1447, Batch Gradient Norm: 43.396550639359084
Epoch: 1447, Batch Gradient Norm after: 22.360678156640716
Epoch 1448/10000, Prediction Accuracy = 48.048%, Loss = 1.443677306175232
Epoch: 1448, Batch Gradient Norm: 38.589137422138684
Epoch: 1448, Batch Gradient Norm after: 22.36067523167875
Epoch 1449/10000, Prediction Accuracy = 48.04%, Loss = 1.4289372444152832
Epoch: 1449, Batch Gradient Norm: 43.41486283092599
Epoch: 1449, Batch Gradient Norm after: 22.36067856710009
Epoch 1450/10000, Prediction Accuracy = 48.056%, Loss = 1.4428961515426635
Epoch: 1450, Batch Gradient Norm: 38.59893094746697
Epoch: 1450, Batch Gradient Norm after: 22.360677473654107
Epoch 1451/10000, Prediction Accuracy = 48.05%, Loss = 1.428149652481079
Epoch: 1451, Batch Gradient Norm: 43.42411217835077
Epoch: 1451, Batch Gradient Norm after: 22.36067772481502
Epoch 1452/10000, Prediction Accuracy = 48.05800000000001%, Loss = 1.4420888662338256
Epoch: 1452, Batch Gradient Norm: 38.608115626299366
Epoch: 1452, Batch Gradient Norm after: 22.360676522806166
Epoch 1453/10000, Prediction Accuracy = 48.048%, Loss = 1.427359652519226
Epoch: 1453, Batch Gradient Norm: 43.429875923978265
Epoch: 1453, Batch Gradient Norm after: 22.36067882168383
Epoch 1454/10000, Prediction Accuracy = 48.074%, Loss = 1.4412635803222655
Epoch: 1454, Batch Gradient Norm: 38.619968602559915
Epoch: 1454, Batch Gradient Norm after: 22.3606772080999
Epoch 1455/10000, Prediction Accuracy = 48.058%, Loss = 1.4265532493591309
Epoch: 1455, Batch Gradient Norm: 43.43779433388235
Epoch: 1455, Batch Gradient Norm after: 22.36067796511178
Epoch 1456/10000, Prediction Accuracy = 48.078%, Loss = 1.440456223487854
Epoch: 1456, Batch Gradient Norm: 38.63189823189735
Epoch: 1456, Batch Gradient Norm after: 22.360679187321193
Epoch 1457/10000, Prediction Accuracy = 48.062%, Loss = 1.4257683753967285
Epoch: 1457, Batch Gradient Norm: 43.437932523358775
Epoch: 1457, Batch Gradient Norm after: 22.36067944712253
Epoch 1458/10000, Prediction Accuracy = 48.08%, Loss = 1.4396401643753052
Epoch: 1458, Batch Gradient Norm: 38.64316483943305
Epoch: 1458, Batch Gradient Norm after: 22.360677524635474
Epoch 1459/10000, Prediction Accuracy = 48.064%, Loss = 1.4249739408493043
Epoch: 1459, Batch Gradient Norm: 43.44409868885878
Epoch: 1459, Batch Gradient Norm after: 22.360678013850123
Epoch 1460/10000, Prediction Accuracy = 48.086%, Loss = 1.438831663131714
Epoch: 1460, Batch Gradient Norm: 38.64872220042738
Epoch: 1460, Batch Gradient Norm after: 22.360678597437055
Epoch 1461/10000, Prediction Accuracy = 48.065999999999995%, Loss = 1.4241869926452637
Epoch: 1461, Batch Gradient Norm: 43.44776370659007
Epoch: 1461, Batch Gradient Norm after: 22.36067779244559
Epoch 1462/10000, Prediction Accuracy = 48.08800000000001%, Loss = 1.4380179405212403
Epoch: 1462, Batch Gradient Norm: 38.658846666194805
Epoch: 1462, Batch Gradient Norm after: 22.360678411715476
Epoch 1463/10000, Prediction Accuracy = 48.078%, Loss = 1.4234017133712769
Epoch: 1463, Batch Gradient Norm: 43.44935621516394
Epoch: 1463, Batch Gradient Norm after: 22.360678629972714
Epoch 1464/10000, Prediction Accuracy = 48.08%, Loss = 1.4371997594833374
Epoch: 1464, Batch Gradient Norm: 38.67003318769435
Epoch: 1464, Batch Gradient Norm after: 22.360678631192954
Epoch 1465/10000, Prediction Accuracy = 48.094%, Loss = 1.4226175785064696
Epoch: 1465, Batch Gradient Norm: 43.4518553803542
Epoch: 1465, Batch Gradient Norm after: 22.360677564436394
Epoch 1466/10000, Prediction Accuracy = 48.084%, Loss = 1.4363949775695801
Epoch: 1466, Batch Gradient Norm: 38.67874128391487
Epoch: 1466, Batch Gradient Norm after: 22.360677004105288
Epoch 1467/10000, Prediction Accuracy = 48.094%, Loss = 1.4218263626098633
Epoch: 1467, Batch Gradient Norm: 43.44945304675607
Epoch: 1467, Batch Gradient Norm after: 22.360677670380827
Epoch 1468/10000, Prediction Accuracy = 48.08%, Loss = 1.4355685234069824
Epoch: 1468, Batch Gradient Norm: 38.68766713441088
Epoch: 1468, Batch Gradient Norm after: 22.36067868789945
Epoch 1469/10000, Prediction Accuracy = 48.102%, Loss = 1.421044158935547
Epoch: 1469, Batch Gradient Norm: 43.44942913663742
Epoch: 1469, Batch Gradient Norm after: 22.360679111039666
Epoch 1470/10000, Prediction Accuracy = 48.086%, Loss = 1.4347545623779296
Epoch: 1470, Batch Gradient Norm: 38.6980308808665
Epoch: 1470, Batch Gradient Norm after: 22.36067801279437
Epoch 1471/10000, Prediction Accuracy = 48.10600000000001%, Loss = 1.420262336730957
Epoch: 1471, Batch Gradient Norm: 43.45894230612785
Epoch: 1471, Batch Gradient Norm after: 22.360676487824584
Epoch 1472/10000, Prediction Accuracy = 48.098%, Loss = 1.43397855758667
Epoch: 1472, Batch Gradient Norm: 38.70606567622391
Epoch: 1472, Batch Gradient Norm after: 22.360675993733455
Epoch 1473/10000, Prediction Accuracy = 48.122%, Loss = 1.4194846630096436
Epoch: 1473, Batch Gradient Norm: 43.47484063859929
Epoch: 1473, Batch Gradient Norm after: 22.360678552855703
Epoch 1474/10000, Prediction Accuracy = 48.116%, Loss = 1.4332223653793335
Epoch: 1474, Batch Gradient Norm: 38.71882141644839
Epoch: 1474, Batch Gradient Norm after: 22.360677670710587
Epoch 1475/10000, Prediction Accuracy = 48.132%, Loss = 1.4187233924865723
Epoch: 1475, Batch Gradient Norm: 43.50083382243271
Epoch: 1475, Batch Gradient Norm after: 22.360678136396057
Epoch 1476/10000, Prediction Accuracy = 48.116%, Loss = 1.432489275932312
Epoch: 1476, Batch Gradient Norm: 38.72712431647353
Epoch: 1476, Batch Gradient Norm after: 22.360679925304694
Epoch 1477/10000, Prediction Accuracy = 48.132%, Loss = 1.4179503202438355
Epoch: 1477, Batch Gradient Norm: 43.52700799155765
Epoch: 1477, Batch Gradient Norm after: 22.36067768513527
Epoch 1478/10000, Prediction Accuracy = 48.124%, Loss = 1.4317679405212402
Epoch: 1478, Batch Gradient Norm: 38.73916394476802
Epoch: 1478, Batch Gradient Norm after: 22.360677931679724
Epoch 1479/10000, Prediction Accuracy = 48.138%, Loss = 1.4171876430511474
Epoch: 1479, Batch Gradient Norm: 43.55794511133028
Epoch: 1479, Batch Gradient Norm after: 22.360678383432333
Epoch 1480/10000, Prediction Accuracy = 48.124%, Loss = 1.4310685634613036
Epoch: 1480, Batch Gradient Norm: 38.7488375134385
Epoch: 1480, Batch Gradient Norm after: 22.360678492396858
Epoch 1481/10000, Prediction Accuracy = 48.134%, Loss = 1.4164271116256715
Epoch: 1481, Batch Gradient Norm: 43.595146367138796
Epoch: 1481, Batch Gradient Norm after: 22.360676346704192
Epoch 1482/10000, Prediction Accuracy = 48.138%, Loss = 1.4303822278976441
Epoch: 1482, Batch Gradient Norm: 38.76068267821259
Epoch: 1482, Batch Gradient Norm after: 22.360678214751427
Epoch 1483/10000, Prediction Accuracy = 48.14%, Loss = 1.4156678915023804
Epoch: 1483, Batch Gradient Norm: 43.61878447500519
Epoch: 1483, Batch Gradient Norm after: 22.360679912976313
Epoch 1484/10000, Prediction Accuracy = 48.152%, Loss = 1.429651665687561
Epoch: 1484, Batch Gradient Norm: 38.77130372374415
Epoch: 1484, Batch Gradient Norm after: 22.360677718840908
Epoch 1485/10000, Prediction Accuracy = 48.142%, Loss = 1.414897871017456
Epoch: 1485, Batch Gradient Norm: 43.637315501879556
Epoch: 1485, Batch Gradient Norm after: 22.360677518051038
Epoch 1486/10000, Prediction Accuracy = 48.154%, Loss = 1.4289108514785767
Epoch: 1486, Batch Gradient Norm: 38.77874189808506
Epoch: 1486, Batch Gradient Norm after: 22.360678424303558
Epoch 1487/10000, Prediction Accuracy = 48.156%, Loss = 1.4141220092773437
Epoch: 1487, Batch Gradient Norm: 43.65088472444141
Epoch: 1487, Batch Gradient Norm after: 22.360678026980253
Epoch 1488/10000, Prediction Accuracy = 48.168%, Loss = 1.42815043926239
Epoch: 1488, Batch Gradient Norm: 38.78911545580418
Epoch: 1488, Batch Gradient Norm after: 22.360677576929913
Epoch 1489/10000, Prediction Accuracy = 48.164%, Loss = 1.4133556365966797
Epoch: 1489, Batch Gradient Norm: 43.67546356241188
Epoch: 1489, Batch Gradient Norm after: 22.36067817752709
Epoch 1490/10000, Prediction Accuracy = 48.19200000000001%, Loss = 1.4274218082427979
Epoch: 1490, Batch Gradient Norm: 38.80135434112363
Epoch: 1490, Batch Gradient Norm after: 22.360677984396
Epoch 1491/10000, Prediction Accuracy = 48.178%, Loss = 1.412593412399292
Epoch: 1491, Batch Gradient Norm: 43.69751850437477
Epoch: 1491, Batch Gradient Norm after: 22.360678607506383
Epoch 1492/10000, Prediction Accuracy = 48.202%, Loss = 1.4267047882080077
Epoch: 1492, Batch Gradient Norm: 38.812316824602505
Epoch: 1492, Batch Gradient Norm after: 22.360677551569463
Epoch 1493/10000, Prediction Accuracy = 48.188%, Loss = 1.4118383884429933
Epoch: 1493, Batch Gradient Norm: 43.721227724320364
Epoch: 1493, Batch Gradient Norm after: 22.360678999067176
Epoch 1494/10000, Prediction Accuracy = 48.206%, Loss = 1.425979995727539
Epoch: 1494, Batch Gradient Norm: 38.823535682508464
Epoch: 1494, Batch Gradient Norm after: 22.360678134924427
Epoch 1495/10000, Prediction Accuracy = 48.196%, Loss = 1.411086344718933
Epoch: 1495, Batch Gradient Norm: 43.75515224651841
Epoch: 1495, Batch Gradient Norm after: 22.360679884602778
Epoch 1496/10000, Prediction Accuracy = 48.204%, Loss = 1.4252899646759034
Epoch: 1496, Batch Gradient Norm: 38.837112966189046
Epoch: 1496, Batch Gradient Norm after: 22.360676976160935
Epoch 1497/10000, Prediction Accuracy = 48.2%, Loss = 1.4103265047073363
Epoch: 1497, Batch Gradient Norm: 43.79570796446305
Epoch: 1497, Batch Gradient Norm after: 22.360679115972427
Epoch 1498/10000, Prediction Accuracy = 48.222%, Loss = 1.424611711502075
Epoch: 1498, Batch Gradient Norm: 38.85109671178841
Epoch: 1498, Batch Gradient Norm after: 22.360678710754364
Epoch 1499/10000, Prediction Accuracy = 48.224%, Loss = 1.4095799207687378
Epoch: 1499, Batch Gradient Norm: 43.83104832798822
Epoch: 1499, Batch Gradient Norm after: 22.360678236804738
Epoch 1500/10000, Prediction Accuracy = 48.22599999999999%, Loss = 1.4239368200302125
Epoch: 1500, Batch Gradient Norm: 38.864057243636864
Epoch: 1500, Batch Gradient Norm after: 22.360678525530936
Epoch 1501/10000, Prediction Accuracy = 48.232%, Loss = 1.4088233709335327
Epoch: 1501, Batch Gradient Norm: 43.86886264703196
Epoch: 1501, Batch Gradient Norm after: 22.360678348745868
Epoch 1502/10000, Prediction Accuracy = 48.242%, Loss = 1.4232474327087403
Epoch: 1502, Batch Gradient Norm: 38.880052661643866
Epoch: 1502, Batch Gradient Norm after: 22.36067880003594
Epoch 1503/10000, Prediction Accuracy = 48.232%, Loss = 1.4080668449401856
Epoch: 1503, Batch Gradient Norm: 43.900797513419036
Epoch: 1503, Batch Gradient Norm after: 22.3606799311242
Epoch 1504/10000, Prediction Accuracy = 48.264%, Loss = 1.422548532485962
Epoch: 1504, Batch Gradient Norm: 38.89273701953729
Epoch: 1504, Batch Gradient Norm after: 22.36067853221552
Epoch 1505/10000, Prediction Accuracy = 48.245999999999995%, Loss = 1.4073092460632324
Epoch: 1505, Batch Gradient Norm: 43.933927578364695
Epoch: 1505, Batch Gradient Norm after: 22.36067719420789
Epoch 1506/10000, Prediction Accuracy = 48.268%, Loss = 1.4218542337417603
Epoch: 1506, Batch Gradient Norm: 38.906955792815296
Epoch: 1506, Batch Gradient Norm after: 22.36067846425091
Epoch 1507/10000, Prediction Accuracy = 48.258%, Loss = 1.4065592527389525
Epoch: 1507, Batch Gradient Norm: 43.95561753295993
Epoch: 1507, Batch Gradient Norm after: 22.360675645019782
Epoch 1508/10000, Prediction Accuracy = 48.284%, Loss = 1.4211445093154906
Epoch: 1508, Batch Gradient Norm: 38.91493105737807
Epoch: 1508, Batch Gradient Norm after: 22.360678930062836
Epoch 1509/10000, Prediction Accuracy = 48.272000000000006%, Loss = 1.4058080911636353
Epoch: 1509, Batch Gradient Norm: 43.96412967675399
Epoch: 1509, Batch Gradient Norm after: 22.36067818672984
Epoch 1510/10000, Prediction Accuracy = 48.278%, Loss = 1.4203809738159179
Epoch: 1510, Batch Gradient Norm: 38.928150616310106
Epoch: 1510, Batch Gradient Norm after: 22.36067824336789
Epoch 1511/10000, Prediction Accuracy = 48.27600000000001%, Loss = 1.405049467086792
Epoch: 1511, Batch Gradient Norm: 43.98960917015391
Epoch: 1511, Batch Gradient Norm after: 22.360679735791802
Epoch 1512/10000, Prediction Accuracy = 48.288000000000004%, Loss = 1.419674777984619
Epoch: 1512, Batch Gradient Norm: 38.94285488899504
Epoch: 1512, Batch Gradient Norm after: 22.36067931347617
Epoch 1513/10000, Prediction Accuracy = 48.282%, Loss = 1.4043034315109253
Epoch: 1513, Batch Gradient Norm: 44.004537502086336
Epoch: 1513, Batch Gradient Norm after: 22.360677366122278
Epoch 1514/10000, Prediction Accuracy = 48.285999999999994%, Loss = 1.4189230680465699
Epoch: 1514, Batch Gradient Norm: 38.95215409242211
Epoch: 1514, Batch Gradient Norm after: 22.36067858328729
Epoch 1515/10000, Prediction Accuracy = 48.294000000000004%, Loss = 1.4035401821136475
Epoch: 1515, Batch Gradient Norm: 44.010713040770206
Epoch: 1515, Batch Gradient Norm after: 22.360678029420406
Epoch 1516/10000, Prediction Accuracy = 48.30200000000001%, Loss = 1.4181670188903808
Epoch: 1516, Batch Gradient Norm: 38.961104959080224
Epoch: 1516, Batch Gradient Norm after: 22.360677266855543
Epoch 1517/10000, Prediction Accuracy = 48.294000000000004%, Loss = 1.402798628807068
Epoch: 1517, Batch Gradient Norm: 44.01815608137319
Epoch: 1517, Batch Gradient Norm after: 22.360676114809728
Epoch 1518/10000, Prediction Accuracy = 48.315999999999995%, Loss = 1.4174142599105835
Epoch: 1518, Batch Gradient Norm: 38.97109282189375
Epoch: 1518, Batch Gradient Norm after: 22.36067606965713
Epoch 1519/10000, Prediction Accuracy = 48.3%, Loss = 1.4020458459854126
Epoch: 1519, Batch Gradient Norm: 44.028869882720535
Epoch: 1519, Batch Gradient Norm after: 22.360678311511855
Epoch 1520/10000, Prediction Accuracy = 48.324%, Loss = 1.416670274734497
Epoch: 1520, Batch Gradient Norm: 38.98117003316962
Epoch: 1520, Batch Gradient Norm after: 22.360679890669672
Epoch 1521/10000, Prediction Accuracy = 48.3%, Loss = 1.401309895515442
Epoch: 1521, Batch Gradient Norm: 44.046909058841
Epoch: 1521, Batch Gradient Norm after: 22.360678173667853
Epoch 1522/10000, Prediction Accuracy = 48.332%, Loss = 1.4159398794174194
Epoch: 1522, Batch Gradient Norm: 38.98774532244981
Epoch: 1522, Batch Gradient Norm after: 22.36067948840666
Epoch 1523/10000, Prediction Accuracy = 48.303999999999995%, Loss = 1.4005694150924684
Epoch: 1523, Batch Gradient Norm: 44.0558307233419
Epoch: 1523, Batch Gradient Norm after: 22.360676852402797
Epoch 1524/10000, Prediction Accuracy = 48.327999999999996%, Loss = 1.4151888847351075
Epoch: 1524, Batch Gradient Norm: 38.995960406178995
Epoch: 1524, Batch Gradient Norm after: 22.36067986842172
Epoch 1525/10000, Prediction Accuracy = 48.31%, Loss = 1.3998200178146363
Epoch: 1525, Batch Gradient Norm: 44.05820437020812
Epoch: 1525, Batch Gradient Norm after: 22.36067629537271
Epoch 1526/10000, Prediction Accuracy = 48.336%, Loss = 1.4144193649291992
Epoch: 1526, Batch Gradient Norm: 39.00768028021687
Epoch: 1526, Batch Gradient Norm after: 22.360677025515617
Epoch 1527/10000, Prediction Accuracy = 48.308%, Loss = 1.399068546295166
Epoch: 1527, Batch Gradient Norm: 44.06221778059997
Epoch: 1527, Batch Gradient Norm after: 22.360678361083508
Epoch 1528/10000, Prediction Accuracy = 48.342%, Loss = 1.4136574268341064
Epoch: 1528, Batch Gradient Norm: 39.01366066497747
Epoch: 1528, Batch Gradient Norm after: 22.36067701457943
Epoch 1529/10000, Prediction Accuracy = 48.312000000000005%, Loss = 1.3983113288879394
Epoch: 1529, Batch Gradient Norm: 44.07083777922795
Epoch: 1529, Batch Gradient Norm after: 22.360676546228834
Epoch 1530/10000, Prediction Accuracy = 48.35199999999999%, Loss = 1.4129090070724488
Epoch: 1530, Batch Gradient Norm: 39.02413641567526
Epoch: 1530, Batch Gradient Norm after: 22.360677643248426
Epoch 1531/10000, Prediction Accuracy = 48.303999999999995%, Loss = 1.3975661754608155
Epoch: 1531, Batch Gradient Norm: 44.08753134469141
Epoch: 1531, Batch Gradient Norm after: 22.3606756614789
Epoch 1532/10000, Prediction Accuracy = 48.352%, Loss = 1.4121836185455323
Epoch: 1532, Batch Gradient Norm: 39.031158375323436
Epoch: 1532, Batch Gradient Norm after: 22.360678702619403
Epoch 1533/10000, Prediction Accuracy = 48.3%, Loss = 1.3968169450759889
Epoch: 1533, Batch Gradient Norm: 44.10302498915032
Epoch: 1533, Batch Gradient Norm after: 22.36067676325006
Epoch 1534/10000, Prediction Accuracy = 48.378%, Loss = 1.411452555656433
Epoch: 1534, Batch Gradient Norm: 39.039901267910665
Epoch: 1534, Batch Gradient Norm after: 22.360677655995996
Epoch 1535/10000, Prediction Accuracy = 48.312%, Loss = 1.3960815906524657
Epoch: 1535, Batch Gradient Norm: 44.13105762763478
Epoch: 1535, Batch Gradient Norm after: 22.360678758727385
Epoch 1536/10000, Prediction Accuracy = 48.386%, Loss = 1.4107689619064332
Epoch: 1536, Batch Gradient Norm: 39.04882934952316
Epoch: 1536, Batch Gradient Norm after: 22.360678704825006
Epoch 1537/10000, Prediction Accuracy = 48.327999999999996%, Loss = 1.395344042778015
Epoch: 1537, Batch Gradient Norm: 44.15591223930941
Epoch: 1537, Batch Gradient Norm after: 22.360678133953105
Epoch 1538/10000, Prediction Accuracy = 48.4%, Loss = 1.4100754499435424
Epoch: 1538, Batch Gradient Norm: 39.05837352936555
Epoch: 1538, Batch Gradient Norm after: 22.360678121655017
Epoch 1539/10000, Prediction Accuracy = 48.354%, Loss = 1.3946194171905517
Epoch: 1539, Batch Gradient Norm: 44.185161167205735
Epoch: 1539, Batch Gradient Norm after: 22.360681023634278
Epoch 1540/10000, Prediction Accuracy = 48.414%, Loss = 1.4094080209732056
Epoch: 1540, Batch Gradient Norm: 39.069646017108575
Epoch: 1540, Batch Gradient Norm after: 22.360677607102943
Epoch 1541/10000, Prediction Accuracy = 48.364000000000004%, Loss = 1.393899917602539
Epoch: 1541, Batch Gradient Norm: 44.22219621234841
Epoch: 1541, Batch Gradient Norm after: 22.36067783961851
Epoch 1542/10000, Prediction Accuracy = 48.432%, Loss = 1.4087548017501832
Epoch: 1542, Batch Gradient Norm: 39.08280189039127
Epoch: 1542, Batch Gradient Norm after: 22.36067757949296
Epoch 1543/10000, Prediction Accuracy = 48.382%, Loss = 1.3931737899780274
Epoch: 1543, Batch Gradient Norm: 44.2618304053528
Epoch: 1543, Batch Gradient Norm after: 22.36067549973784
Epoch 1544/10000, Prediction Accuracy = 48.438%, Loss = 1.4081167936325074
Epoch: 1544, Batch Gradient Norm: 39.0965217323317
Epoch: 1544, Batch Gradient Norm after: 22.360677025717038
Epoch 1545/10000, Prediction Accuracy = 48.379999999999995%, Loss = 1.3924543380737304
Epoch: 1545, Batch Gradient Norm: 44.30123446934274
Epoch: 1545, Batch Gradient Norm after: 22.360676561874453
Epoch 1546/10000, Prediction Accuracy = 48.446%, Loss = 1.4074721813201905
Epoch: 1546, Batch Gradient Norm: 39.10848026436617
Epoch: 1546, Batch Gradient Norm after: 22.36067768746839
Epoch 1547/10000, Prediction Accuracy = 48.382%, Loss = 1.3917220115661622
Epoch: 1547, Batch Gradient Norm: 44.335713194621306
Epoch: 1547, Batch Gradient Norm after: 22.36067813047361
Epoch 1548/10000, Prediction Accuracy = 48.44199999999999%, Loss = 1.406816053390503
Epoch: 1548, Batch Gradient Norm: 39.12057029092735
Epoch: 1548, Batch Gradient Norm after: 22.360675900330556
Epoch 1549/10000, Prediction Accuracy = 48.404%, Loss = 1.3909906148910522
Epoch: 1549, Batch Gradient Norm: 44.34935135593775
Epoch: 1549, Batch Gradient Norm after: 22.360680218245964
Epoch 1550/10000, Prediction Accuracy = 48.45399999999999%, Loss = 1.4060831546783448
Epoch: 1550, Batch Gradient Norm: 39.1262179533406
Epoch: 1550, Batch Gradient Norm after: 22.360678915791443
Epoch 1551/10000, Prediction Accuracy = 48.412000000000006%, Loss = 1.3902495145797729
Epoch: 1551, Batch Gradient Norm: 44.364019830256126
Epoch: 1551, Batch Gradient Norm after: 22.360679393890276
Epoch 1552/10000, Prediction Accuracy = 48.464000000000006%, Loss = 1.4053619861602784
Epoch: 1552, Batch Gradient Norm: 39.13237729768908
Epoch: 1552, Batch Gradient Norm after: 22.360677893753465
Epoch 1553/10000, Prediction Accuracy = 48.426%, Loss = 1.3895196437835693
Epoch: 1553, Batch Gradient Norm: 44.38880932336522
Epoch: 1553, Batch Gradient Norm after: 22.360678696486755
Epoch 1554/10000, Prediction Accuracy = 48.480000000000004%, Loss = 1.4046725749969482
Epoch: 1554, Batch Gradient Norm: 39.14395920511025
Epoch: 1554, Batch Gradient Norm after: 22.36067868115061
Epoch 1555/10000, Prediction Accuracy = 48.43%, Loss = 1.3887848138809205
Epoch: 1555, Batch Gradient Norm: 44.41715174951996
Epoch: 1555, Batch Gradient Norm after: 22.3606773505674
Epoch 1556/10000, Prediction Accuracy = 48.496%, Loss = 1.4039974212646484
Epoch: 1556, Batch Gradient Norm: 39.15629814000664
Epoch: 1556, Batch Gradient Norm after: 22.36067908819784
Epoch 1557/10000, Prediction Accuracy = 48.458000000000006%, Loss = 1.3880608558654786
Epoch: 1557, Batch Gradient Norm: 44.42886549759428
Epoch: 1557, Batch Gradient Norm after: 22.360678606530243
Epoch 1558/10000, Prediction Accuracy = 48.504000000000005%, Loss = 1.4032955408096313
Epoch: 1558, Batch Gradient Norm: 39.16610114197611
Epoch: 1558, Batch Gradient Norm after: 22.36067709297465
Epoch 1559/10000, Prediction Accuracy = 48.46%, Loss = 1.3873319149017334
Epoch: 1559, Batch Gradient Norm: 44.438013828749355
Epoch: 1559, Batch Gradient Norm after: 22.360675997465023
Epoch 1560/10000, Prediction Accuracy = 48.517999999999994%, Loss = 1.4025750875473022
Epoch: 1560, Batch Gradient Norm: 39.17426292320269
Epoch: 1560, Batch Gradient Norm after: 22.36067536868557
Epoch 1561/10000, Prediction Accuracy = 48.462%, Loss = 1.3865976572036742
Epoch: 1561, Batch Gradient Norm: 44.445426492787604
Epoch: 1561, Batch Gradient Norm after: 22.36067935766735
Epoch 1562/10000, Prediction Accuracy = 48.525999999999996%, Loss = 1.4018439769744873
Epoch: 1562, Batch Gradient Norm: 39.18032886528905
Epoch: 1562, Batch Gradient Norm after: 22.36067736648577
Epoch 1563/10000, Prediction Accuracy = 48.459999999999994%, Loss = 1.3858644485473632
Epoch: 1563, Batch Gradient Norm: 44.45858870216085
Epoch: 1563, Batch Gradient Norm after: 22.360677720781695
Epoch 1564/10000, Prediction Accuracy = 48.528000000000006%, Loss = 1.4011244297027587
Epoch: 1564, Batch Gradient Norm: 39.18872365917757
Epoch: 1564, Batch Gradient Norm after: 22.360676644000694
Epoch 1565/10000, Prediction Accuracy = 48.484%, Loss = 1.3851272821426392
Epoch: 1565, Batch Gradient Norm: 44.472338398392274
Epoch: 1565, Batch Gradient Norm after: 22.360678732884235
Epoch 1566/10000, Prediction Accuracy = 48.529999999999994%, Loss = 1.400406002998352
Epoch: 1566, Batch Gradient Norm: 39.19643146950266
Epoch: 1566, Batch Gradient Norm after: 22.360676355212313
Epoch 1567/10000, Prediction Accuracy = 48.492000000000004%, Loss = 1.3843820333480834
Epoch: 1567, Batch Gradient Norm: 44.48074095283641
Epoch: 1567, Batch Gradient Norm after: 22.360677148878278
Epoch 1568/10000, Prediction Accuracy = 48.54%, Loss = 1.3996631383895874
Epoch: 1568, Batch Gradient Norm: 39.20341615668329
Epoch: 1568, Batch Gradient Norm after: 22.36067700273022
Epoch 1569/10000, Prediction Accuracy = 48.50599999999999%, Loss = 1.3836376428604127
Epoch: 1569, Batch Gradient Norm: 44.484964040844076
Epoch: 1569, Batch Gradient Norm after: 22.360675108910502
Epoch 1570/10000, Prediction Accuracy = 48.547999999999995%, Loss = 1.398921585083008
Epoch: 1570, Batch Gradient Norm: 39.213627741274244
Epoch: 1570, Batch Gradient Norm after: 22.360676590708337
Epoch 1571/10000, Prediction Accuracy = 48.507999999999996%, Loss = 1.3829125881195068
Epoch: 1571, Batch Gradient Norm: 44.49803793030308
Epoch: 1571, Batch Gradient Norm after: 22.360677526018517
Epoch 1572/10000, Prediction Accuracy = 48.552%, Loss = 1.3982019186019898
Epoch: 1572, Batch Gradient Norm: 39.22217371102074
Epoch: 1572, Batch Gradient Norm after: 22.360677422232463
Epoch 1573/10000, Prediction Accuracy = 48.53%, Loss = 1.3821943044662475
Epoch: 1573, Batch Gradient Norm: 44.51877690292452
Epoch: 1573, Batch Gradient Norm after: 22.360676244481922
Epoch 1574/10000, Prediction Accuracy = 48.57600000000001%, Loss = 1.3975087642669677
Epoch: 1574, Batch Gradient Norm: 39.22929614445073
Epoch: 1574, Batch Gradient Norm after: 22.360677338026747
Epoch 1575/10000, Prediction Accuracy = 48.544%, Loss = 1.3814690113067627
Epoch: 1575, Batch Gradient Norm: 44.54714153589109
Epoch: 1575, Batch Gradient Norm after: 22.360675987969874
Epoch 1576/10000, Prediction Accuracy = 48.583999999999996%, Loss = 1.3968447685241698
Epoch: 1576, Batch Gradient Norm: 39.242489228028596
Epoch: 1576, Batch Gradient Norm after: 22.360678364449154
Epoch 1577/10000, Prediction Accuracy = 48.559999999999995%, Loss = 1.3807547330856322
Epoch: 1577, Batch Gradient Norm: 44.582380699722925
Epoch: 1577, Batch Gradient Norm after: 22.36067642722687
Epoch 1578/10000, Prediction Accuracy = 48.58%, Loss = 1.3961998701095581
Epoch: 1578, Batch Gradient Norm: 39.25162822013101
Epoch: 1578, Batch Gradient Norm after: 22.36067572467634
Epoch 1579/10000, Prediction Accuracy = 48.566%, Loss = 1.380023980140686
Epoch: 1579, Batch Gradient Norm: 44.612027709791306
Epoch: 1579, Batch Gradient Norm after: 22.36067728942233
Epoch 1580/10000, Prediction Accuracy = 48.59400000000001%, Loss = 1.3955390930175782
Epoch: 1580, Batch Gradient Norm: 39.26111360336882
Epoch: 1580, Batch Gradient Norm after: 22.360679832186683
Epoch 1581/10000, Prediction Accuracy = 48.566%, Loss = 1.3793099641799926
Epoch: 1581, Batch Gradient Norm: 44.637440924588475
Epoch: 1581, Batch Gradient Norm after: 22.36067953020596
Epoch 1582/10000, Prediction Accuracy = 48.604%, Loss = 1.3948678016662597
Epoch: 1582, Batch Gradient Norm: 39.2714335025922
Epoch: 1582, Batch Gradient Norm after: 22.36067547548534
Epoch 1583/10000, Prediction Accuracy = 48.562%, Loss = 1.37859046459198
Epoch: 1583, Batch Gradient Norm: 44.646743024533876
Epoch: 1583, Batch Gradient Norm after: 22.36067529961671
Epoch 1584/10000, Prediction Accuracy = 48.612%, Loss = 1.3941373825073242
Epoch: 1584, Batch Gradient Norm: 39.27973215037673
Epoch: 1584, Batch Gradient Norm after: 22.36067935366309
Epoch 1585/10000, Prediction Accuracy = 48.57%, Loss = 1.3778475999832154
Epoch: 1585, Batch Gradient Norm: 44.642453320032814
Epoch: 1585, Batch Gradient Norm after: 22.360677630269056
Epoch 1586/10000, Prediction Accuracy = 48.626%, Loss = 1.3933595657348632
Epoch: 1586, Batch Gradient Norm: 39.28461913396489
Epoch: 1586, Batch Gradient Norm after: 22.360678974614594
Epoch 1587/10000, Prediction Accuracy = 48.59%, Loss = 1.377114748954773
Epoch: 1587, Batch Gradient Norm: 44.63250879533139
Epoch: 1587, Batch Gradient Norm after: 22.360677345858832
Epoch 1588/10000, Prediction Accuracy = 48.634%, Loss = 1.3925808191299438
Epoch: 1588, Batch Gradient Norm: 39.28852908944108
Epoch: 1588, Batch Gradient Norm after: 22.360677381905905
Epoch 1589/10000, Prediction Accuracy = 48.61%, Loss = 1.376369261741638
Epoch: 1589, Batch Gradient Norm: 44.6260323661431
Epoch: 1589, Batch Gradient Norm after: 22.36067734803757
Epoch 1590/10000, Prediction Accuracy = 48.644000000000005%, Loss = 1.391801619529724
Epoch: 1590, Batch Gradient Norm: 39.29035740022586
Epoch: 1590, Batch Gradient Norm after: 22.36067818305357
Epoch 1591/10000, Prediction Accuracy = 48.624%, Loss = 1.3756427526474
Epoch: 1591, Batch Gradient Norm: 44.621482043080384
Epoch: 1591, Batch Gradient Norm after: 22.36067660144846
Epoch 1592/10000, Prediction Accuracy = 48.642%, Loss = 1.3910568475723266
Epoch: 1592, Batch Gradient Norm: 39.29287791650212
Epoch: 1592, Batch Gradient Norm after: 22.360676000591088
Epoch 1593/10000, Prediction Accuracy = 48.632%, Loss = 1.3749096870422364
Epoch: 1593, Batch Gradient Norm: 44.61821161146063
Epoch: 1593, Batch Gradient Norm after: 22.360677347131787
Epoch 1594/10000, Prediction Accuracy = 48.658%, Loss = 1.3902913093566895
Epoch: 1594, Batch Gradient Norm: 39.30012878408328
Epoch: 1594, Batch Gradient Norm after: 22.36067756845378
Epoch 1595/10000, Prediction Accuracy = 48.664%, Loss = 1.3741766691207886
Epoch: 1595, Batch Gradient Norm: 44.617327311298304
Epoch: 1595, Batch Gradient Norm after: 22.3606763594381
Epoch 1596/10000, Prediction Accuracy = 48.666%, Loss = 1.3895322322845458
Epoch: 1596, Batch Gradient Norm: 39.307593949206485
Epoch: 1596, Batch Gradient Norm after: 22.36067460355499
Epoch 1597/10000, Prediction Accuracy = 48.684%, Loss = 1.3734694719314575
Epoch: 1597, Batch Gradient Norm: 44.62592995806406
Epoch: 1597, Batch Gradient Norm after: 22.360677170430794
Epoch 1598/10000, Prediction Accuracy = 48.682%, Loss = 1.388818621635437
Epoch: 1598, Batch Gradient Norm: 39.31536021907233
Epoch: 1598, Batch Gradient Norm after: 22.360674311247415
Epoch 1599/10000, Prediction Accuracy = 48.702%, Loss = 1.3727535963058473
Epoch: 1599, Batch Gradient Norm: 44.64105228213676
Epoch: 1599, Batch Gradient Norm after: 22.36067633023536
Epoch 1600/10000, Prediction Accuracy = 48.696%, Loss = 1.3881218433380127
Epoch: 1600, Batch Gradient Norm: 39.32317107369424
Epoch: 1600, Batch Gradient Norm after: 22.36067697836661
Epoch 1601/10000, Prediction Accuracy = 48.708000000000006%, Loss = 1.3720394134521485
Epoch: 1601, Batch Gradient Norm: 44.65732931258048
Epoch: 1601, Batch Gradient Norm after: 22.36067741742099
Epoch 1602/10000, Prediction Accuracy = 48.709999999999994%, Loss = 1.3874299287796021
Epoch: 1602, Batch Gradient Norm: 39.32989279218271
Epoch: 1602, Batch Gradient Norm after: 22.360677312884057
Epoch 1603/10000, Prediction Accuracy = 48.71999999999999%, Loss = 1.3713305234909057
Epoch: 1603, Batch Gradient Norm: 44.673820465594396
Epoch: 1603, Batch Gradient Norm after: 22.36067836481743
Epoch 1604/10000, Prediction Accuracy = 48.708%, Loss = 1.3867335557937621
Epoch: 1604, Batch Gradient Norm: 39.33535709768211
Epoch: 1604, Batch Gradient Norm after: 22.360676434470996
Epoch 1605/10000, Prediction Accuracy = 48.724000000000004%, Loss = 1.370619249343872
Epoch: 1605, Batch Gradient Norm: 44.69341150474455
Epoch: 1605, Batch Gradient Norm after: 22.360675967220025
Epoch 1606/10000, Prediction Accuracy = 48.714%, Loss = 1.3860543012619018
Epoch: 1606, Batch Gradient Norm: 39.34475071474561
Epoch: 1606, Batch Gradient Norm after: 22.360677265324014
Epoch 1607/10000, Prediction Accuracy = 48.727999999999994%, Loss = 1.3699183940887452
Epoch: 1607, Batch Gradient Norm: 44.7142058188504
Epoch: 1607, Batch Gradient Norm after: 22.360677161323316
Epoch 1608/10000, Prediction Accuracy = 48.721999999999994%, Loss = 1.38537917137146
Epoch: 1608, Batch Gradient Norm: 39.35267290294297
Epoch: 1608, Batch Gradient Norm after: 22.360676953286546
Epoch 1609/10000, Prediction Accuracy = 48.734%, Loss = 1.3692142248153687
Epoch: 1609, Batch Gradient Norm: 44.729773355555906
Epoch: 1609, Batch Gradient Norm after: 22.360674923528258
Epoch 1610/10000, Prediction Accuracy = 48.724%, Loss = 1.3846969127655029
Epoch: 1610, Batch Gradient Norm: 39.36497218884138
Epoch: 1610, Batch Gradient Norm after: 22.360677009990038
Epoch 1611/10000, Prediction Accuracy = 48.744%, Loss = 1.3685166835784912
Epoch: 1611, Batch Gradient Norm: 44.73298497840014
Epoch: 1611, Batch Gradient Norm after: 22.36067767634535
Epoch 1612/10000, Prediction Accuracy = 48.730000000000004%, Loss = 1.383985161781311
Epoch: 1612, Batch Gradient Norm: 39.368389350625435
Epoch: 1612, Batch Gradient Norm after: 22.360677700303516
Epoch 1613/10000, Prediction Accuracy = 48.748%, Loss = 1.3677926778793335
Epoch: 1613, Batch Gradient Norm: 44.72326419940632
Epoch: 1613, Batch Gradient Norm after: 22.360676662920188
Epoch 1614/10000, Prediction Accuracy = 48.74399999999999%, Loss = 1.3832266569137572
Epoch: 1614, Batch Gradient Norm: 39.37386400280541
Epoch: 1614, Batch Gradient Norm after: 22.360676062390827
Epoch 1615/10000, Prediction Accuracy = 48.75%, Loss = 1.3670867919921874
Epoch: 1615, Batch Gradient Norm: 44.72617795457417
Epoch: 1615, Batch Gradient Norm after: 22.360679992397312
Epoch 1616/10000, Prediction Accuracy = 48.754%, Loss = 1.3824946403503418
Epoch: 1616, Batch Gradient Norm: 39.37973084270803
Epoch: 1616, Batch Gradient Norm after: 22.360677880547808
Epoch 1617/10000, Prediction Accuracy = 48.756%, Loss = 1.3663676261901856
Epoch: 1617, Batch Gradient Norm: 44.73357552447825
Epoch: 1617, Batch Gradient Norm after: 22.360677996009734
Epoch 1618/10000, Prediction Accuracy = 48.767999999999994%, Loss = 1.3817814111709594
Epoch: 1618, Batch Gradient Norm: 39.390757569069784
Epoch: 1618, Batch Gradient Norm after: 22.36067899667661
Epoch 1619/10000, Prediction Accuracy = 48.786%, Loss = 1.3656672954559326
Epoch: 1619, Batch Gradient Norm: 44.74306806379827
Epoch: 1619, Batch Gradient Norm after: 22.36067771127517
Epoch 1620/10000, Prediction Accuracy = 48.778%, Loss = 1.3810914278030395
Epoch: 1620, Batch Gradient Norm: 39.387661194577944
Epoch: 1620, Batch Gradient Norm after: 22.36067766639367
Epoch 1621/10000, Prediction Accuracy = 48.791999999999994%, Loss = 1.3649410486221314
Epoch: 1621, Batch Gradient Norm: 44.72014192995242
Epoch: 1621, Batch Gradient Norm after: 22.360678046841887
Epoch 1622/10000, Prediction Accuracy = 48.784%, Loss = 1.3802836656570434
Epoch: 1622, Batch Gradient Norm: 39.39052103838358
Epoch: 1622, Batch Gradient Norm after: 22.360678622889708
Epoch 1623/10000, Prediction Accuracy = 48.797999999999995%, Loss = 1.3642245054244995
Epoch: 1623, Batch Gradient Norm: 44.70072537904643
Epoch: 1623, Batch Gradient Norm after: 22.360679063453375
Epoch 1624/10000, Prediction Accuracy = 48.798%, Loss = 1.37950541973114
Epoch: 1624, Batch Gradient Norm: 39.39028125948598
Epoch: 1624, Batch Gradient Norm after: 22.360676640501616
Epoch 1625/10000, Prediction Accuracy = 48.80800000000001%, Loss = 1.363503646850586
Epoch: 1625, Batch Gradient Norm: 44.67158818723191
Epoch: 1625, Batch Gradient Norm after: 22.36067545662972
Epoch 1626/10000, Prediction Accuracy = 48.812%, Loss = 1.3786901235580444
Epoch: 1626, Batch Gradient Norm: 39.39084528253476
Epoch: 1626, Batch Gradient Norm after: 22.360675980487876
Epoch 1627/10000, Prediction Accuracy = 48.82%, Loss = 1.3627964019775392
Epoch: 1627, Batch Gradient Norm: 44.650308835253334
Epoch: 1627, Batch Gradient Norm after: 22.360674616817622
Epoch 1628/10000, Prediction Accuracy = 48.822%, Loss = 1.3778936624526978
Epoch: 1628, Batch Gradient Norm: 39.398359686812036
Epoch: 1628, Batch Gradient Norm after: 22.3606755446407
Epoch 1629/10000, Prediction Accuracy = 48.826%, Loss = 1.3620926856994628
Epoch: 1629, Batch Gradient Norm: 44.63150920836815
Epoch: 1629, Batch Gradient Norm after: 22.360678144441042
Epoch 1630/10000, Prediction Accuracy = 48.839999999999996%, Loss = 1.3771114349365234
Epoch: 1630, Batch Gradient Norm: 39.40049336504442
Epoch: 1630, Batch Gradient Norm after: 22.360676649502068
Epoch 1631/10000, Prediction Accuracy = 48.833999999999996%, Loss = 1.3613788843154908
Epoch: 1631, Batch Gradient Norm: 44.61789491153677
Epoch: 1631, Batch Gradient Norm after: 22.360676593880473
Epoch 1632/10000, Prediction Accuracy = 48.846000000000004%, Loss = 1.3763605117797852
Epoch: 1632, Batch Gradient Norm: 39.405039061879855
Epoch: 1632, Batch Gradient Norm after: 22.36067597656174
Epoch 1633/10000, Prediction Accuracy = 48.852%, Loss = 1.3606882333755492
Epoch: 1633, Batch Gradient Norm: 44.62411792412072
Epoch: 1633, Batch Gradient Norm after: 22.360678257276156
Epoch 1634/10000, Prediction Accuracy = 48.85%, Loss = 1.375670027732849
Epoch: 1634, Batch Gradient Norm: 39.40988774664612
Epoch: 1634, Batch Gradient Norm after: 22.360676381316193
Epoch 1635/10000, Prediction Accuracy = 48.856%, Loss = 1.3600073099136352
Epoch: 1635, Batch Gradient Norm: 44.640935926853416
Epoch: 1635, Batch Gradient Norm after: 22.360676588961702
Epoch 1636/10000, Prediction Accuracy = 48.852%, Loss = 1.3750139236450196
Epoch: 1636, Batch Gradient Norm: 39.41723373892692
Epoch: 1636, Batch Gradient Norm after: 22.360678679045204
Epoch 1637/10000, Prediction Accuracy = 48.864000000000004%, Loss = 1.3593113422393799
Epoch: 1637, Batch Gradient Norm: 44.6520306578028
Epoch: 1637, Batch Gradient Norm after: 22.360676978312302
Epoch 1638/10000, Prediction Accuracy = 48.862%, Loss = 1.3743385791778564
Epoch: 1638, Batch Gradient Norm: 39.426249417505964
Epoch: 1638, Batch Gradient Norm after: 22.36067388610563
Epoch 1639/10000, Prediction Accuracy = 48.86800000000001%, Loss = 1.3586302280426026
Epoch: 1639, Batch Gradient Norm: 44.66998345835896
Epoch: 1639, Batch Gradient Norm after: 22.360675257115
Epoch 1640/10000, Prediction Accuracy = 48.876%, Loss = 1.3736953020095826
Epoch: 1640, Batch Gradient Norm: 39.43470274541432
Epoch: 1640, Batch Gradient Norm after: 22.360677005498136
Epoch 1641/10000, Prediction Accuracy = 48.888%, Loss = 1.357945156097412
Epoch: 1641, Batch Gradient Norm: 44.69461857657381
Epoch: 1641, Batch Gradient Norm after: 22.360675860623978
Epoch 1642/10000, Prediction Accuracy = 48.89%, Loss = 1.3730649948120117
Epoch: 1642, Batch Gradient Norm: 39.442692081814926
Epoch: 1642, Batch Gradient Norm after: 22.360676349127996
Epoch 1643/10000, Prediction Accuracy = 48.894%, Loss = 1.3572662353515625
Epoch: 1643, Batch Gradient Norm: 44.713254026713
Epoch: 1643, Batch Gradient Norm after: 22.3606758168547
Epoch 1644/10000, Prediction Accuracy = 48.88799999999999%, Loss = 1.3724240064620972
Epoch: 1644, Batch Gradient Norm: 39.451379943050554
Epoch: 1644, Batch Gradient Norm after: 22.36067752247438
Epoch 1645/10000, Prediction Accuracy = 48.906%, Loss = 1.356586718559265
Epoch: 1645, Batch Gradient Norm: 44.71796362911952
Epoch: 1645, Batch Gradient Norm after: 22.3606799825163
Epoch 1646/10000, Prediction Accuracy = 48.894%, Loss = 1.3717403650283813
Epoch: 1646, Batch Gradient Norm: 39.45839019807968
Epoch: 1646, Batch Gradient Norm after: 22.36067604495687
Epoch 1647/10000, Prediction Accuracy = 48.908%, Loss = 1.3559052228927613
Epoch: 1647, Batch Gradient Norm: 44.72974726944158
Epoch: 1647, Batch Gradient Norm after: 22.36067865186611
Epoch 1648/10000, Prediction Accuracy = 48.910000000000004%, Loss = 1.3710686922073365
Epoch: 1648, Batch Gradient Norm: 39.46724860061047
Epoch: 1648, Batch Gradient Norm after: 22.360675997098912
Epoch 1649/10000, Prediction Accuracy = 48.916%, Loss = 1.3552340269088745
Epoch: 1649, Batch Gradient Norm: 44.7462778219861
Epoch: 1649, Batch Gradient Norm after: 22.36067842172476
Epoch 1650/10000, Prediction Accuracy = 48.916000000000004%, Loss = 1.3704201459884644
Epoch: 1650, Batch Gradient Norm: 39.47524449409058
Epoch: 1650, Batch Gradient Norm after: 22.360677835926847
Epoch 1651/10000, Prediction Accuracy = 48.918%, Loss = 1.3545595169067384
Epoch: 1651, Batch Gradient Norm: 44.77022276292248
Epoch: 1651, Batch Gradient Norm after: 22.360675272878726
Epoch 1652/10000, Prediction Accuracy = 48.928%, Loss = 1.3697768926620484
Epoch: 1652, Batch Gradient Norm: 39.48505012043314
Epoch: 1652, Batch Gradient Norm after: 22.360677282099704
Epoch 1653/10000, Prediction Accuracy = 48.922%, Loss = 1.3538832426071168
Epoch: 1653, Batch Gradient Norm: 44.79263606457199
Epoch: 1653, Batch Gradient Norm after: 22.360678387991296
Epoch 1654/10000, Prediction Accuracy = 48.934000000000005%, Loss = 1.3691462993621826
Epoch: 1654, Batch Gradient Norm: 39.49347695117975
Epoch: 1654, Batch Gradient Norm after: 22.36067783823626
Epoch 1655/10000, Prediction Accuracy = 48.918%, Loss = 1.3532092094421386
Epoch: 1655, Batch Gradient Norm: 44.79953892304927
Epoch: 1655, Batch Gradient Norm after: 22.36067761552746
Epoch 1656/10000, Prediction Accuracy = 48.926%, Loss = 1.3684671640396118
Epoch: 1656, Batch Gradient Norm: 39.49823942411182
Epoch: 1656, Batch Gradient Norm after: 22.360676724687412
Epoch 1657/10000, Prediction Accuracy = 48.934%, Loss = 1.3525273084640503
Epoch: 1657, Batch Gradient Norm: 44.80851266885501
Epoch: 1657, Batch Gradient Norm after: 22.36067674135562
Epoch 1658/10000, Prediction Accuracy = 48.938%, Loss = 1.3677810192108155
Epoch: 1658, Batch Gradient Norm: 39.5052410612444
Epoch: 1658, Batch Gradient Norm after: 22.36067679830463
Epoch 1659/10000, Prediction Accuracy = 48.94%, Loss = 1.3518589973449706
Epoch: 1659, Batch Gradient Norm: 44.81170404472655
Epoch: 1659, Batch Gradient Norm after: 22.36067886235752
Epoch 1660/10000, Prediction Accuracy = 48.95399999999999%, Loss = 1.3670907974243165
Epoch: 1660, Batch Gradient Norm: 39.511732301866665
Epoch: 1660, Batch Gradient Norm after: 22.36067915754206
Epoch 1661/10000, Prediction Accuracy = 48.95199999999999%, Loss = 1.3511822462081908
Epoch: 1661, Batch Gradient Norm: 44.81384358213786
Epoch: 1661, Batch Gradient Norm after: 22.360678920905556
Epoch 1662/10000, Prediction Accuracy = 48.964%, Loss = 1.3663785934448243
Epoch: 1662, Batch Gradient Norm: 39.516661347683545
Epoch: 1662, Batch Gradient Norm after: 22.36067645745353
Epoch 1663/10000, Prediction Accuracy = 48.968%, Loss = 1.3504761695861816
Epoch: 1663, Batch Gradient Norm: 44.79691529999475
Epoch: 1663, Batch Gradient Norm after: 22.360677278679784
Epoch 1664/10000, Prediction Accuracy = 48.964000000000006%, Loss = 1.365634274482727
Epoch: 1664, Batch Gradient Norm: 39.52012679954851
Epoch: 1664, Batch Gradient Norm after: 22.36067715554506
Epoch 1665/10000, Prediction Accuracy = 48.978%, Loss = 1.349790835380554
Epoch: 1665, Batch Gradient Norm: 44.77865831566893
Epoch: 1665, Batch Gradient Norm after: 22.36067843448803
Epoch 1666/10000, Prediction Accuracy = 48.972%, Loss = 1.3648831367492675
Epoch: 1666, Batch Gradient Norm: 39.52321092439459
Epoch: 1666, Batch Gradient Norm after: 22.360676162121443
Epoch 1667/10000, Prediction Accuracy = 49.001999999999995%, Loss = 1.3491070032119752
Epoch: 1667, Batch Gradient Norm: 44.76483572713171
Epoch: 1667, Batch Gradient Norm after: 22.36067865560878
Epoch 1668/10000, Prediction Accuracy = 48.982%, Loss = 1.364147424697876
Epoch: 1668, Batch Gradient Norm: 39.523659916529915
Epoch: 1668, Batch Gradient Norm after: 22.360675750153437
Epoch 1669/10000, Prediction Accuracy = 49.001999999999995%, Loss = 1.3484225034713746
Epoch: 1669, Batch Gradient Norm: 44.749482640076145
Epoch: 1669, Batch Gradient Norm after: 22.360676782759747
Epoch 1670/10000, Prediction Accuracy = 48.983999999999995%, Loss = 1.3633991003036499
Epoch: 1670, Batch Gradient Norm: 39.528856373930346
Epoch: 1670, Batch Gradient Norm after: 22.360674947019582
Epoch 1671/10000, Prediction Accuracy = 49.010000000000005%, Loss = 1.347742986679077
Epoch: 1671, Batch Gradient Norm: 44.74242989668769
Epoch: 1671, Batch Gradient Norm after: 22.36067664277499
Epoch 1672/10000, Prediction Accuracy = 48.982%, Loss = 1.3626864910125733
Epoch: 1672, Batch Gradient Norm: 39.53225044928094
Epoch: 1672, Batch Gradient Norm after: 22.360677730624
Epoch 1673/10000, Prediction Accuracy = 49.024%, Loss = 1.3470642566680908
Epoch: 1673, Batch Gradient Norm: 44.74187435033711
Epoch: 1673, Batch Gradient Norm after: 22.3606760692441
Epoch 1674/10000, Prediction Accuracy = 48.986000000000004%, Loss = 1.3619993686676026
Epoch: 1674, Batch Gradient Norm: 39.53807882932816
Epoch: 1674, Batch Gradient Norm after: 22.360676702658438
Epoch 1675/10000, Prediction Accuracy = 49.022000000000006%, Loss = 1.3463972091674805
Epoch: 1675, Batch Gradient Norm: 44.73657585222715
Epoch: 1675, Batch Gradient Norm after: 22.36067651026427
Epoch 1676/10000, Prediction Accuracy = 48.995999999999995%, Loss = 1.3612951993942262
Epoch: 1676, Batch Gradient Norm: 39.54301935254785
Epoch: 1676, Batch Gradient Norm after: 22.360678121040284
Epoch 1677/10000, Prediction Accuracy = 49.028%, Loss = 1.3457221269607544
Epoch: 1677, Batch Gradient Norm: 44.73761961609284
Epoch: 1677, Batch Gradient Norm after: 22.360676129554527
Epoch 1678/10000, Prediction Accuracy = 48.992000000000004%, Loss = 1.3606077671051025
Epoch: 1678, Batch Gradient Norm: 39.548101950118195
Epoch: 1678, Batch Gradient Norm after: 22.3606775462921
Epoch 1679/10000, Prediction Accuracy = 49.035999999999994%, Loss = 1.3450506925582886
Epoch: 1679, Batch Gradient Norm: 44.7292843049031
Epoch: 1679, Batch Gradient Norm after: 22.360679112940055
Epoch 1680/10000, Prediction Accuracy = 49.004000000000005%, Loss = 1.3598995208740234
Epoch: 1680, Batch Gradient Norm: 39.55033298764121
Epoch: 1680, Batch Gradient Norm after: 22.360676573656157
Epoch 1681/10000, Prediction Accuracy = 49.042%, Loss = 1.3443755865097047
Epoch: 1681, Batch Gradient Norm: 44.72798518313502
Epoch: 1681, Batch Gradient Norm after: 22.360677621317897
Epoch 1682/10000, Prediction Accuracy = 49.017999999999994%, Loss = 1.3592026472091674
Epoch: 1682, Batch Gradient Norm: 39.55754353096928
Epoch: 1682, Batch Gradient Norm after: 22.360677015212524
Epoch 1683/10000, Prediction Accuracy = 49.048%, Loss = 1.3437095403671264
Epoch: 1683, Batch Gradient Norm: 44.72583685701306
Epoch: 1683, Batch Gradient Norm after: 22.360678470678923
Epoch 1684/10000, Prediction Accuracy = 49.034%, Loss = 1.3585111856460572
Epoch: 1684, Batch Gradient Norm: 39.56361491992515
Epoch: 1684, Batch Gradient Norm after: 22.360677363206804
Epoch 1685/10000, Prediction Accuracy = 49.052%, Loss = 1.3430450439453125
Epoch: 1685, Batch Gradient Norm: 44.71809967618182
Epoch: 1685, Batch Gradient Norm after: 22.360678307694883
Epoch 1686/10000, Prediction Accuracy = 49.053999999999995%, Loss = 1.357804012298584
Epoch: 1686, Batch Gradient Norm: 39.56576827872851
Epoch: 1686, Batch Gradient Norm after: 22.360679487104356
Epoch 1687/10000, Prediction Accuracy = 49.05800000000001%, Loss = 1.342374587059021
Epoch: 1687, Batch Gradient Norm: 44.7159258145537
Epoch: 1687, Batch Gradient Norm after: 22.36067881417516
Epoch 1688/10000, Prediction Accuracy = 49.059999999999995%, Loss = 1.3571074247360229
Epoch: 1688, Batch Gradient Norm: 39.57279794040503
Epoch: 1688, Batch Gradient Norm after: 22.360676790129215
Epoch 1689/10000, Prediction Accuracy = 49.065999999999995%, Loss = 1.3417119026184081
Epoch: 1689, Batch Gradient Norm: 44.70440652326124
Epoch: 1689, Batch Gradient Norm after: 22.360678467422016
Epoch 1690/10000, Prediction Accuracy = 49.062%, Loss = 1.3563967227935791
Epoch: 1690, Batch Gradient Norm: 39.57508178146132
Epoch: 1690, Batch Gradient Norm after: 22.360676662766355
Epoch 1691/10000, Prediction Accuracy = 49.068%, Loss = 1.341042733192444
Epoch: 1691, Batch Gradient Norm: 44.697634100175776
Epoch: 1691, Batch Gradient Norm after: 22.360675737261506
Epoch 1692/10000, Prediction Accuracy = 49.05999999999999%, Loss = 1.3556837081909179
Epoch: 1692, Batch Gradient Norm: 39.57928834283029
Epoch: 1692, Batch Gradient Norm after: 22.360674550419596
Epoch 1693/10000, Prediction Accuracy = 49.092%, Loss = 1.3403838634490968
Epoch: 1693, Batch Gradient Norm: 44.68664633278339
Epoch: 1693, Batch Gradient Norm after: 22.36067729500857
Epoch 1694/10000, Prediction Accuracy = 49.065999999999995%, Loss = 1.3549720287322997
Epoch: 1694, Batch Gradient Norm: 39.58115142320787
Epoch: 1694, Batch Gradient Norm after: 22.360678521610723
Epoch 1695/10000, Prediction Accuracy = 49.096%, Loss = 1.339710807800293
Epoch: 1695, Batch Gradient Norm: 44.657124597272535
Epoch: 1695, Batch Gradient Norm after: 22.360675900784393
Epoch 1696/10000, Prediction Accuracy = 49.081999999999994%, Loss = 1.3542033672332763
Epoch: 1696, Batch Gradient Norm: 39.58217735660796
Epoch: 1696, Batch Gradient Norm after: 22.36067716625691
Epoch 1697/10000, Prediction Accuracy = 49.098%, Loss = 1.3390345096588134
Epoch: 1697, Batch Gradient Norm: 44.62619058417443
Epoch: 1697, Batch Gradient Norm after: 22.36067684960619
Epoch 1698/10000, Prediction Accuracy = 49.083999999999996%, Loss = 1.3534357070922851
Epoch: 1698, Batch Gradient Norm: 39.579823501815184
Epoch: 1698, Batch Gradient Norm after: 22.360679127192814
Epoch 1699/10000, Prediction Accuracy = 49.128%, Loss = 1.3383678674697876
Epoch: 1699, Batch Gradient Norm: 44.610718978667876
Epoch: 1699, Batch Gradient Norm after: 22.360675452768582
Epoch 1700/10000, Prediction Accuracy = 49.098%, Loss = 1.352708601951599
Epoch: 1700, Batch Gradient Norm: 39.58434381930505
Epoch: 1700, Batch Gradient Norm after: 22.360675909717077
Epoch 1701/10000, Prediction Accuracy = 49.146%, Loss = 1.337707757949829
Epoch: 1701, Batch Gradient Norm: 44.599011916929086
Epoch: 1701, Batch Gradient Norm after: 22.36067703440932
Epoch 1702/10000, Prediction Accuracy = 49.112%, Loss = 1.3520130872726441
Epoch: 1702, Batch Gradient Norm: 39.58868837619829
Epoch: 1702, Batch Gradient Norm after: 22.360678411520194
Epoch 1703/10000, Prediction Accuracy = 49.150000000000006%, Loss = 1.3370516061782838
Epoch: 1703, Batch Gradient Norm: 44.58433346766698
Epoch: 1703, Batch Gradient Norm after: 22.360676023596362
Epoch 1704/10000, Prediction Accuracy = 49.114%, Loss = 1.3513161897659303
Epoch: 1704, Batch Gradient Norm: 39.589302498564464
Epoch: 1704, Batch Gradient Norm after: 22.360677113303044
Epoch 1705/10000, Prediction Accuracy = 49.16799999999999%, Loss = 1.3363927125930786
Epoch: 1705, Batch Gradient Norm: 44.55962174323177
Epoch: 1705, Batch Gradient Norm after: 22.360677484979092
Epoch 1706/10000, Prediction Accuracy = 49.13000000000001%, Loss = 1.3505774974822997
Epoch: 1706, Batch Gradient Norm: 39.59183435196213
Epoch: 1706, Batch Gradient Norm after: 22.360677509836343
Epoch 1707/10000, Prediction Accuracy = 49.17999999999999%, Loss = 1.335733413696289
Epoch: 1707, Batch Gradient Norm: 44.52875455241218
Epoch: 1707, Batch Gradient Norm after: 22.36067655565553
Epoch 1708/10000, Prediction Accuracy = 49.14%, Loss = 1.3498173713684083
Epoch: 1708, Batch Gradient Norm: 39.59348638437716
Epoch: 1708, Batch Gradient Norm after: 22.360678234811402
Epoch 1709/10000, Prediction Accuracy = 49.196000000000005%, Loss = 1.335068416595459
Epoch: 1709, Batch Gradient Norm: 44.49541635336018
Epoch: 1709, Batch Gradient Norm after: 22.36067601400108
Epoch 1710/10000, Prediction Accuracy = 49.152%, Loss = 1.3490544080734252
Epoch: 1710, Batch Gradient Norm: 39.595287744513854
Epoch: 1710, Batch Gradient Norm after: 22.360677393649716
Epoch 1711/10000, Prediction Accuracy = 49.217999999999996%, Loss = 1.3344117879867554
Epoch: 1711, Batch Gradient Norm: 44.47014839732971
Epoch: 1711, Batch Gradient Norm after: 22.360676136970582
Epoch 1712/10000, Prediction Accuracy = 49.166%, Loss = 1.3483174324035645
Epoch: 1712, Batch Gradient Norm: 39.5961305467379
Epoch: 1712, Batch Gradient Norm after: 22.360677241777577
Epoch 1713/10000, Prediction Accuracy = 49.236000000000004%, Loss = 1.3337623834609986
Epoch: 1713, Batch Gradient Norm: 44.44349876399972
Epoch: 1713, Batch Gradient Norm after: 22.36067538390725
Epoch 1714/10000, Prediction Accuracy = 49.186%, Loss = 1.3475810766220093
Epoch: 1714, Batch Gradient Norm: 39.59811088552988
Epoch: 1714, Batch Gradient Norm after: 22.360677002911604
Epoch 1715/10000, Prediction Accuracy = 49.242%, Loss = 1.3331219911575318
Epoch: 1715, Batch Gradient Norm: 44.41534591874912
Epoch: 1715, Batch Gradient Norm after: 22.360676119568506
Epoch 1716/10000, Prediction Accuracy = 49.190000000000005%, Loss = 1.3468379735946656
Epoch: 1716, Batch Gradient Norm: 39.59984017643052
Epoch: 1716, Batch Gradient Norm after: 22.360677382800034
Epoch 1717/10000, Prediction Accuracy = 49.248000000000005%, Loss = 1.3324753999710084
Epoch: 1717, Batch Gradient Norm: 44.3953981417639
Epoch: 1717, Batch Gradient Norm after: 22.36067751874538
Epoch 1718/10000, Prediction Accuracy = 49.202000000000005%, Loss = 1.3461257457733153
Epoch: 1718, Batch Gradient Norm: 39.602818266972804
Epoch: 1718, Batch Gradient Norm after: 22.360677448080278
Epoch 1719/10000, Prediction Accuracy = 49.248000000000005%, Loss = 1.3318280696868896
Epoch: 1719, Batch Gradient Norm: 44.372531869383806
Epoch: 1719, Batch Gradient Norm after: 22.360675750530945
Epoch 1720/10000, Prediction Accuracy = 49.21%, Loss = 1.3454144477844239
Epoch: 1720, Batch Gradient Norm: 39.606089842819536
Epoch: 1720, Batch Gradient Norm after: 22.360678112705074
Epoch 1721/10000, Prediction Accuracy = 49.245999999999995%, Loss = 1.3311872243881226
Epoch: 1721, Batch Gradient Norm: 44.35357619025436
Epoch: 1721, Batch Gradient Norm after: 22.36067672247882
Epoch 1722/10000, Prediction Accuracy = 49.222%, Loss = 1.3446981191635132
Epoch: 1722, Batch Gradient Norm: 39.612407245438945
Epoch: 1722, Batch Gradient Norm after: 22.360679486424992
Epoch 1723/10000, Prediction Accuracy = 49.254%, Loss = 1.3305453777313232
Epoch: 1723, Batch Gradient Norm: 44.3297303536592
Epoch: 1723, Batch Gradient Norm after: 22.3606754824496
Epoch 1724/10000, Prediction Accuracy = 49.232%, Loss = 1.3440036535263062
Epoch: 1724, Batch Gradient Norm: 39.614076832062
Epoch: 1724, Batch Gradient Norm after: 22.36067877460589
Epoch 1725/10000, Prediction Accuracy = 49.266000000000005%, Loss = 1.3299067258834838
Epoch: 1725, Batch Gradient Norm: 44.308061402109765
Epoch: 1725, Batch Gradient Norm after: 22.360678152342327
Epoch 1726/10000, Prediction Accuracy = 49.239999999999995%, Loss = 1.343297529220581
Epoch: 1726, Batch Gradient Norm: 39.61662575143037
Epoch: 1726, Batch Gradient Norm after: 22.360676502039226
Epoch 1727/10000, Prediction Accuracy = 49.275999999999996%, Loss = 1.329267716407776
Epoch: 1727, Batch Gradient Norm: 44.293873574423074
Epoch: 1727, Batch Gradient Norm after: 22.360676736350285
Epoch 1728/10000, Prediction Accuracy = 49.25%, Loss = 1.3426189184188844
Epoch: 1728, Batch Gradient Norm: 39.618751315894364
Epoch: 1728, Batch Gradient Norm after: 22.360677982390627
Epoch 1729/10000, Prediction Accuracy = 49.288%, Loss = 1.3286261081695556
Epoch: 1729, Batch Gradient Norm: 44.285687497336006
Epoch: 1729, Batch Gradient Norm after: 22.360676513958346
Epoch 1730/10000, Prediction Accuracy = 49.257999999999996%, Loss = 1.3419641971588134
Epoch: 1730, Batch Gradient Norm: 39.621941313426035
Epoch: 1730, Batch Gradient Norm after: 22.36067729790798
Epoch 1731/10000, Prediction Accuracy = 49.296%, Loss = 1.3279921293258667
Epoch: 1731, Batch Gradient Norm: 44.278475480951535
Epoch: 1731, Batch Gradient Norm after: 22.360678927383134
Epoch 1732/10000, Prediction Accuracy = 49.272000000000006%, Loss = 1.3413036346435547
Epoch: 1732, Batch Gradient Norm: 39.62262058749537
Epoch: 1732, Batch Gradient Norm after: 22.36067704781253
Epoch 1733/10000, Prediction Accuracy = 49.29600000000001%, Loss = 1.327366089820862
Epoch: 1733, Batch Gradient Norm: 44.26834748850625
Epoch: 1733, Batch Gradient Norm after: 22.360679504805496
Epoch 1734/10000, Prediction Accuracy = 49.282%, Loss = 1.3406399011611938
Epoch: 1734, Batch Gradient Norm: 39.62025348641223
Epoch: 1734, Batch Gradient Norm after: 22.360675191326944
Epoch 1735/10000, Prediction Accuracy = 49.303999999999995%, Loss = 1.3267411470413208
Epoch: 1735, Batch Gradient Norm: 44.25157944278048
Epoch: 1735, Batch Gradient Norm after: 22.36067657957738
Epoch 1736/10000, Prediction Accuracy = 49.288%, Loss = 1.3399604320526124
Epoch: 1736, Batch Gradient Norm: 39.621603772371245
Epoch: 1736, Batch Gradient Norm after: 22.36067683490358
Epoch 1737/10000, Prediction Accuracy = 49.31400000000001%, Loss = 1.326111125946045
Epoch: 1737, Batch Gradient Norm: 44.241052414359864
Epoch: 1737, Batch Gradient Norm after: 22.360674857948975
Epoch 1738/10000, Prediction Accuracy = 49.297999999999995%, Loss = 1.3392849206924438
Epoch: 1738, Batch Gradient Norm: 39.62250411634518
Epoch: 1738, Batch Gradient Norm after: 22.360678089956654
Epoch 1739/10000, Prediction Accuracy = 49.314%, Loss = 1.3254819869995118
Epoch: 1739, Batch Gradient Norm: 44.22518176078559
Epoch: 1739, Batch Gradient Norm after: 22.360675924142935
Epoch 1740/10000, Prediction Accuracy = 49.315999999999995%, Loss = 1.3385955333709716
Epoch: 1740, Batch Gradient Norm: 39.62020397291795
Epoch: 1740, Batch Gradient Norm after: 22.360676974348046
Epoch 1741/10000, Prediction Accuracy = 49.331999999999994%, Loss = 1.324860644340515
Epoch: 1741, Batch Gradient Norm: 44.20960668648525
Epoch: 1741, Batch Gradient Norm after: 22.360676390595163
Epoch 1742/10000, Prediction Accuracy = 49.321999999999996%, Loss = 1.337917184829712
Epoch: 1742, Batch Gradient Norm: 39.6242729998038
Epoch: 1742, Batch Gradient Norm after: 22.360678984357527
Epoch 1743/10000, Prediction Accuracy = 49.352%, Loss = 1.3242420911788941
Epoch: 1743, Batch Gradient Norm: 44.210311646252215
Epoch: 1743, Batch Gradient Norm after: 22.360676266223628
Epoch 1744/10000, Prediction Accuracy = 49.332%, Loss = 1.3372897386550904
Epoch: 1744, Batch Gradient Norm: 39.62916078563301
Epoch: 1744, Batch Gradient Norm after: 22.360677847286542
Epoch 1745/10000, Prediction Accuracy = 49.372%, Loss = 1.323621439933777
Epoch: 1745, Batch Gradient Norm: 44.209688547659
Epoch: 1745, Batch Gradient Norm after: 22.36067698157017
Epoch 1746/10000, Prediction Accuracy = 49.348000000000006%, Loss = 1.336672019958496
Epoch: 1746, Batch Gradient Norm: 39.63118709033265
Epoch: 1746, Batch Gradient Norm after: 22.360677404178322
Epoch 1747/10000, Prediction Accuracy = 49.39%, Loss = 1.32299907207489
Epoch: 1747, Batch Gradient Norm: 44.19930138346957
Epoch: 1747, Batch Gradient Norm after: 22.360675784426554
Epoch 1748/10000, Prediction Accuracy = 49.362%, Loss = 1.3360111236572265
Epoch: 1748, Batch Gradient Norm: 39.63388439942527
Epoch: 1748, Batch Gradient Norm after: 22.360676624294946
Epoch 1749/10000, Prediction Accuracy = 49.402%, Loss = 1.322385573387146
Epoch: 1749, Batch Gradient Norm: 44.208243096366
Epoch: 1749, Batch Gradient Norm after: 22.360674864750127
Epoch 1750/10000, Prediction Accuracy = 49.374%, Loss = 1.3354088544845581
Epoch: 1750, Batch Gradient Norm: 39.638516033137776
Epoch: 1750, Batch Gradient Norm after: 22.360678082827285
Epoch 1751/10000, Prediction Accuracy = 49.412%, Loss = 1.3217750310897827
Epoch: 1751, Batch Gradient Norm: 44.23279160742456
Epoch: 1751, Batch Gradient Norm after: 22.36067512306985
Epoch 1752/10000, Prediction Accuracy = 49.388%, Loss = 1.3348482131958008
Epoch: 1752, Batch Gradient Norm: 39.642870103931855
Epoch: 1752, Batch Gradient Norm after: 22.360678142604463
Epoch 1753/10000, Prediction Accuracy = 49.42%, Loss = 1.321158528327942
Epoch: 1753, Batch Gradient Norm: 44.24559499062057
Epoch: 1753, Batch Gradient Norm after: 22.360677119538227
Epoch 1754/10000, Prediction Accuracy = 49.4%, Loss = 1.3342624187469483
Epoch: 1754, Batch Gradient Norm: 39.64320876092742
Epoch: 1754, Batch Gradient Norm after: 22.360677436261877
Epoch 1755/10000, Prediction Accuracy = 49.432%, Loss = 1.3205433130264281
Epoch: 1755, Batch Gradient Norm: 44.261819356584745
Epoch: 1755, Batch Gradient Norm after: 22.360677080485274
Epoch 1756/10000, Prediction Accuracy = 49.410000000000004%, Loss = 1.3336800813674927
Epoch: 1756, Batch Gradient Norm: 39.64358623757631
Epoch: 1756, Batch Gradient Norm after: 22.360676033158015
Epoch 1757/10000, Prediction Accuracy = 49.443999999999996%, Loss = 1.3199279069900514
Epoch: 1757, Batch Gradient Norm: 44.26927333221145
Epoch: 1757, Batch Gradient Norm after: 22.360678861888758
Epoch 1758/10000, Prediction Accuracy = 49.424%, Loss = 1.3330868005752563
Epoch: 1758, Batch Gradient Norm: 39.64468622875498
Epoch: 1758, Batch Gradient Norm after: 22.360676108982755
Epoch 1759/10000, Prediction Accuracy = 49.45399999999999%, Loss = 1.3193101406097412
Epoch: 1759, Batch Gradient Norm: 44.27376130793701
Epoch: 1759, Batch Gradient Norm after: 22.36067457170802
Epoch 1760/10000, Prediction Accuracy = 49.446%, Loss = 1.33248131275177
Epoch: 1760, Batch Gradient Norm: 39.64906243873864
Epoch: 1760, Batch Gradient Norm after: 22.360676621617344
Epoch 1761/10000, Prediction Accuracy = 49.466%, Loss = 1.3186936378479004
Epoch: 1761, Batch Gradient Norm: 44.27235561539992
Epoch: 1761, Batch Gradient Norm after: 22.360676529454196
Epoch 1762/10000, Prediction Accuracy = 49.45%, Loss = 1.331851315498352
Epoch: 1762, Batch Gradient Norm: 39.651471781998104
Epoch: 1762, Batch Gradient Norm after: 22.360676475280286
Epoch 1763/10000, Prediction Accuracy = 49.48800000000001%, Loss = 1.3180768966674805
Epoch: 1763, Batch Gradient Norm: 44.27840349462354
Epoch: 1763, Batch Gradient Norm after: 22.360676620088636
Epoch 1764/10000, Prediction Accuracy = 49.45799999999999%, Loss = 1.3312490224838256
Epoch: 1764, Batch Gradient Norm: 39.652497161228496
Epoch: 1764, Batch Gradient Norm after: 22.360677132493514
Epoch 1765/10000, Prediction Accuracy = 49.49400000000001%, Loss = 1.3174664497375488
Epoch: 1765, Batch Gradient Norm: 44.28506818629949
Epoch: 1765, Batch Gradient Norm after: 22.360676471277202
Epoch 1766/10000, Prediction Accuracy = 49.476%, Loss = 1.3306304216384888
Epoch: 1766, Batch Gradient Norm: 39.65760157115974
Epoch: 1766, Batch Gradient Norm after: 22.36067623430046
Epoch 1767/10000, Prediction Accuracy = 49.5%, Loss = 1.3168386936187744
Epoch: 1767, Batch Gradient Norm: 44.285566229621935
Epoch: 1767, Batch Gradient Norm after: 22.360677728927286
Epoch 1768/10000, Prediction Accuracy = 49.48799999999999%, Loss = 1.3300055027008058
Epoch: 1768, Batch Gradient Norm: 39.661335450569005
Epoch: 1768, Batch Gradient Norm after: 22.360675843444586
Epoch 1769/10000, Prediction Accuracy = 49.519999999999996%, Loss = 1.316230845451355
Epoch: 1769, Batch Gradient Norm: 44.28553345334174
Epoch: 1769, Batch Gradient Norm after: 22.360677071251732
Epoch 1770/10000, Prediction Accuracy = 49.494%, Loss = 1.3293856382369995
Epoch: 1770, Batch Gradient Norm: 39.66489132977051
Epoch: 1770, Batch Gradient Norm after: 22.360677607166977
Epoch 1771/10000, Prediction Accuracy = 49.522%, Loss = 1.3156174898147583
Epoch: 1771, Batch Gradient Norm: 44.28515796880147
Epoch: 1771, Batch Gradient Norm after: 22.3606783797864
Epoch 1772/10000, Prediction Accuracy = 49.496%, Loss = 1.3287630081176758
Epoch: 1772, Batch Gradient Norm: 39.6652515337625
Epoch: 1772, Batch Gradient Norm after: 22.360677780065316
Epoch 1773/10000, Prediction Accuracy = 49.523999999999994%, Loss = 1.3150135278701782
Epoch: 1773, Batch Gradient Norm: 44.28733387251239
Epoch: 1773, Batch Gradient Norm after: 22.360674736158597
Epoch 1774/10000, Prediction Accuracy = 49.508%, Loss = 1.3281514406204225
Epoch: 1774, Batch Gradient Norm: 39.666664696461176
Epoch: 1774, Batch Gradient Norm after: 22.3606760873037
Epoch 1775/10000, Prediction Accuracy = 49.528%, Loss = 1.3144020557403564
Epoch: 1775, Batch Gradient Norm: 44.3001843399524
Epoch: 1775, Batch Gradient Norm after: 22.36067520736462
Epoch 1776/10000, Prediction Accuracy = 49.513999999999996%, Loss = 1.3275704383850098
Epoch: 1776, Batch Gradient Norm: 39.66949915646095
Epoch: 1776, Batch Gradient Norm after: 22.36067684473361
Epoch 1777/10000, Prediction Accuracy = 49.536%, Loss = 1.3137874841690063
Epoch: 1777, Batch Gradient Norm: 44.30588526624193
Epoch: 1777, Batch Gradient Norm after: 22.360675977855202
Epoch 1778/10000, Prediction Accuracy = 49.526%, Loss = 1.326971745491028
Epoch: 1778, Batch Gradient Norm: 39.67069219335208
Epoch: 1778, Batch Gradient Norm after: 22.36067515650156
Epoch 1779/10000, Prediction Accuracy = 49.546%, Loss = 1.3131813049316405
Epoch: 1779, Batch Gradient Norm: 44.30868089783064
Epoch: 1779, Batch Gradient Norm after: 22.36067782013902
Epoch 1780/10000, Prediction Accuracy = 49.523999999999994%, Loss = 1.3263721942901612
Epoch: 1780, Batch Gradient Norm: 39.67073331476664
Epoch: 1780, Batch Gradient Norm after: 22.360676935476384
Epoch 1781/10000, Prediction Accuracy = 49.562%, Loss = 1.31257483959198
Epoch: 1781, Batch Gradient Norm: 44.30247201069408
Epoch: 1781, Batch Gradient Norm after: 22.360677834990224
Epoch 1782/10000, Prediction Accuracy = 49.538%, Loss = 1.3257416009902954
Epoch: 1782, Batch Gradient Norm: 39.673535704833014
Epoch: 1782, Batch Gradient Norm after: 22.360678793908477
Epoch 1783/10000, Prediction Accuracy = 49.57199999999999%, Loss = 1.3119616270065309
Epoch: 1783, Batch Gradient Norm: 44.29453504950454
Epoch: 1783, Batch Gradient Norm after: 22.360675587143213
Epoch 1784/10000, Prediction Accuracy = 49.55%, Loss = 1.325107717514038
Epoch: 1784, Batch Gradient Norm: 39.673610569563415
Epoch: 1784, Batch Gradient Norm after: 22.360677230445063
Epoch 1785/10000, Prediction Accuracy = 49.59%, Loss = 1.3113515615463256
Epoch: 1785, Batch Gradient Norm: 44.291926245213524
Epoch: 1785, Batch Gradient Norm after: 22.36067658298589
Epoch 1786/10000, Prediction Accuracy = 49.572%, Loss = 1.324488592147827
Epoch: 1786, Batch Gradient Norm: 39.677380786844495
Epoch: 1786, Batch Gradient Norm after: 22.360677101452374
Epoch 1787/10000, Prediction Accuracy = 49.592000000000006%, Loss = 1.3107413291931151
Epoch: 1787, Batch Gradient Norm: 44.28380502231142
Epoch: 1787, Batch Gradient Norm after: 22.360677155341623
Epoch 1788/10000, Prediction Accuracy = 49.564%, Loss = 1.3238651037216187
Epoch: 1788, Batch Gradient Norm: 39.680024235704266
Epoch: 1788, Batch Gradient Norm after: 22.360676207291878
Epoch 1789/10000, Prediction Accuracy = 49.592%, Loss = 1.3101369857788085
Epoch: 1789, Batch Gradient Norm: 44.275912301124414
Epoch: 1789, Batch Gradient Norm after: 22.36067642559426
Epoch 1790/10000, Prediction Accuracy = 49.572%, Loss = 1.3232327222824096
Epoch: 1790, Batch Gradient Norm: 39.68244645082764
Epoch: 1790, Batch Gradient Norm after: 22.3606767958364
Epoch 1791/10000, Prediction Accuracy = 49.592%, Loss = 1.3095297813415527
Epoch: 1791, Batch Gradient Norm: 44.265446904391275
Epoch: 1791, Batch Gradient Norm after: 22.360678149961892
Epoch 1792/10000, Prediction Accuracy = 49.578%, Loss = 1.3225931167602538
Epoch: 1792, Batch Gradient Norm: 39.68298660997116
Epoch: 1792, Batch Gradient Norm after: 22.360676066118945
Epoch 1793/10000, Prediction Accuracy = 49.604%, Loss = 1.3089278221130372
Epoch: 1793, Batch Gradient Norm: 44.261853961172115
Epoch: 1793, Batch Gradient Norm after: 22.36067602814707
Epoch 1794/10000, Prediction Accuracy = 49.577999999999996%, Loss = 1.3219772577285767
Epoch: 1794, Batch Gradient Norm: 39.68165398081203
Epoch: 1794, Batch Gradient Norm after: 22.36067691038029
Epoch 1795/10000, Prediction Accuracy = 49.61%, Loss = 1.3083257913589477
Epoch: 1795, Batch Gradient Norm: 44.271956128440216
Epoch: 1795, Batch Gradient Norm after: 22.36067659598475
Epoch 1796/10000, Prediction Accuracy = 49.574%, Loss = 1.3214003086090087
Epoch: 1796, Batch Gradient Norm: 39.6866848673504
Epoch: 1796, Batch Gradient Norm after: 22.360676176517888
Epoch 1797/10000, Prediction Accuracy = 49.622%, Loss = 1.3077313899993896
Epoch: 1797, Batch Gradient Norm: 44.27270326860961
Epoch: 1797, Batch Gradient Norm after: 22.360678052465012
Epoch 1798/10000, Prediction Accuracy = 49.578%, Loss = 1.320791244506836
Epoch: 1798, Batch Gradient Norm: 39.68335705919388
Epoch: 1798, Batch Gradient Norm after: 22.360676212407594
Epoch 1799/10000, Prediction Accuracy = 49.63399999999999%, Loss = 1.3071346759796143
Epoch: 1799, Batch Gradient Norm: 44.27591896804236
Epoch: 1799, Batch Gradient Norm after: 22.360677652200774
Epoch 1800/10000, Prediction Accuracy = 49.592%, Loss = 1.3202049493789674
Epoch: 1800, Batch Gradient Norm: 39.684970707484176
Epoch: 1800, Batch Gradient Norm after: 22.360677363441585
Epoch 1801/10000, Prediction Accuracy = 49.636%, Loss = 1.3065369129180908
Epoch: 1801, Batch Gradient Norm: 44.27618887026348
Epoch: 1801, Batch Gradient Norm after: 22.360677499628455
Epoch 1802/10000, Prediction Accuracy = 49.604%, Loss = 1.3195990800857544
Epoch: 1802, Batch Gradient Norm: 39.68636595181442
Epoch: 1802, Batch Gradient Norm after: 22.360678111878524
Epoch 1803/10000, Prediction Accuracy = 49.644%, Loss = 1.3059382677078246
Epoch: 1803, Batch Gradient Norm: 44.28000511402907
Epoch: 1803, Batch Gradient Norm after: 22.36067794699104
Epoch 1804/10000, Prediction Accuracy = 49.604%, Loss = 1.319005060195923
Epoch: 1804, Batch Gradient Norm: 39.68628292422665
Epoch: 1804, Batch Gradient Norm after: 22.360677038562883
Epoch 1805/10000, Prediction Accuracy = 49.64%, Loss = 1.3053405046463014
Epoch: 1805, Batch Gradient Norm: 44.29148568035066
Epoch: 1805, Batch Gradient Norm after: 22.36067643287717
Epoch 1806/10000, Prediction Accuracy = 49.63%, Loss = 1.3184364080429076
Epoch: 1806, Batch Gradient Norm: 39.69082146519185
Epoch: 1806, Batch Gradient Norm after: 22.360675147956957
Epoch 1807/10000, Prediction Accuracy = 49.658%, Loss = 1.3047502994537354
Epoch: 1807, Batch Gradient Norm: 44.299412214684935
Epoch: 1807, Batch Gradient Norm after: 22.360677828647276
Epoch 1808/10000, Prediction Accuracy = 49.63999999999999%, Loss = 1.3178534507751465
Epoch: 1808, Batch Gradient Norm: 39.68964722688286
Epoch: 1808, Batch Gradient Norm after: 22.360674913176
Epoch 1809/10000, Prediction Accuracy = 49.668000000000006%, Loss = 1.3041543006896972
Epoch: 1809, Batch Gradient Norm: 44.30723556634319
Epoch: 1809, Batch Gradient Norm after: 22.360677581347023
Epoch 1810/10000, Prediction Accuracy = 49.66199999999999%, Loss = 1.317276167869568
Epoch: 1810, Batch Gradient Norm: 39.693196588426076
Epoch: 1810, Batch Gradient Norm after: 22.360676623690054
Epoch 1811/10000, Prediction Accuracy = 49.682%, Loss = 1.3035539388656616
Epoch: 1811, Batch Gradient Norm: 44.32091204440203
Epoch: 1811, Batch Gradient Norm after: 22.360677228432106
Epoch 1812/10000, Prediction Accuracy = 49.678000000000004%, Loss = 1.3167191028594971
Epoch: 1812, Batch Gradient Norm: 39.69358569034877
Epoch: 1812, Batch Gradient Norm after: 22.360676966464244
Epoch 1813/10000, Prediction Accuracy = 49.684000000000005%, Loss = 1.3029553413391113
Epoch: 1813, Batch Gradient Norm: 44.31341322378502
Epoch: 1813, Batch Gradient Norm after: 22.360677435009297
Epoch 1814/10000, Prediction Accuracy = 49.68000000000001%, Loss = 1.316100001335144
Epoch: 1814, Batch Gradient Norm: 39.696054428692634
Epoch: 1814, Batch Gradient Norm after: 22.36067759777451
Epoch 1815/10000, Prediction Accuracy = 49.698%, Loss = 1.302361249923706
Epoch: 1815, Batch Gradient Norm: 44.28776815376663
Epoch: 1815, Batch Gradient Norm after: 22.36067656274016
Epoch 1816/10000, Prediction Accuracy = 49.678000000000004%, Loss = 1.3154260158538817
Epoch: 1816, Batch Gradient Norm: 39.69330803413031
Epoch: 1816, Batch Gradient Norm after: 22.36067752177035
Epoch 1817/10000, Prediction Accuracy = 49.696000000000005%, Loss = 1.3017579317092896
Epoch: 1817, Batch Gradient Norm: 44.26000187904979
Epoch: 1817, Batch Gradient Norm after: 22.36067810588788
Epoch 1818/10000, Prediction Accuracy = 49.69199999999999%, Loss = 1.3147348403930663
Epoch: 1818, Batch Gradient Norm: 39.69246148792374
Epoch: 1818, Batch Gradient Norm after: 22.360676842684132
Epoch 1819/10000, Prediction Accuracy = 49.7%, Loss = 1.301156187057495
Epoch: 1819, Batch Gradient Norm: 44.22998064741841
Epoch: 1819, Batch Gradient Norm after: 22.36067816160567
Epoch 1820/10000, Prediction Accuracy = 49.71%, Loss = 1.3140463352203369
Epoch: 1820, Batch Gradient Norm: 39.69471020606024
Epoch: 1820, Batch Gradient Norm after: 22.360677307530548
Epoch 1821/10000, Prediction Accuracy = 49.708000000000006%, Loss = 1.3005597114562988
Epoch: 1821, Batch Gradient Norm: 44.204537261259
Epoch: 1821, Batch Gradient Norm after: 22.360676312575407
Epoch 1822/10000, Prediction Accuracy = 49.717999999999996%, Loss = 1.3133681535720825
Epoch: 1822, Batch Gradient Norm: 39.69299569387935
Epoch: 1822, Batch Gradient Norm after: 22.360676221209182
Epoch 1823/10000, Prediction Accuracy = 49.71%, Loss = 1.2999626874923706
Epoch: 1823, Batch Gradient Norm: 44.16889856417071
Epoch: 1823, Batch Gradient Norm after: 22.36067795943313
Epoch 1824/10000, Prediction Accuracy = 49.734%, Loss = 1.3126738786697387
Epoch: 1824, Batch Gradient Norm: 39.69734406261595
Epoch: 1824, Batch Gradient Norm after: 22.360676962452935
Epoch 1825/10000, Prediction Accuracy = 49.716%, Loss = 1.2993794918060302
Epoch: 1825, Batch Gradient Norm: 44.14000179341847
Epoch: 1825, Batch Gradient Norm after: 22.360679057103184
Epoch 1826/10000, Prediction Accuracy = 49.745999999999995%, Loss = 1.3119935989379883
Epoch: 1826, Batch Gradient Norm: 39.69848283084979
Epoch: 1826, Batch Gradient Norm after: 22.360677698900563
Epoch 1827/10000, Prediction Accuracy = 49.72%, Loss = 1.2987849712371826
Epoch: 1827, Batch Gradient Norm: 44.111982934731714
Epoch: 1827, Batch Gradient Norm after: 22.360678916379985
Epoch 1828/10000, Prediction Accuracy = 49.754000000000005%, Loss = 1.3113037109375
Epoch: 1828, Batch Gradient Norm: 39.69724960234316
Epoch: 1828, Batch Gradient Norm after: 22.360676732166663
Epoch 1829/10000, Prediction Accuracy = 49.726%, Loss = 1.29819757938385
Epoch: 1829, Batch Gradient Norm: 44.08978928739109
Epoch: 1829, Batch Gradient Norm after: 22.360677852410554
Epoch 1830/10000, Prediction Accuracy = 49.766000000000005%, Loss = 1.3106486797332764
Epoch: 1830, Batch Gradient Norm: 39.699056213460054
Epoch: 1830, Batch Gradient Norm after: 22.360678010604172
Epoch 1831/10000, Prediction Accuracy = 49.71%, Loss = 1.2976184129714965
Epoch: 1831, Batch Gradient Norm: 44.07948708618353
Epoch: 1831, Batch Gradient Norm after: 22.36067732150947
Epoch 1832/10000, Prediction Accuracy = 49.78%, Loss = 1.310034704208374
Epoch: 1832, Batch Gradient Norm: 39.69700049153585
Epoch: 1832, Batch Gradient Norm after: 22.36067634477187
Epoch 1833/10000, Prediction Accuracy = 49.722%, Loss = 1.2970333337783813
Epoch: 1833, Batch Gradient Norm: 44.062254613132396
Epoch: 1833, Batch Gradient Norm after: 22.36068011765416
Epoch 1834/10000, Prediction Accuracy = 49.788000000000004%, Loss = 1.3094094038009643
Epoch: 1834, Batch Gradient Norm: 39.698452916348636
Epoch: 1834, Batch Gradient Norm after: 22.360676600869567
Epoch 1835/10000, Prediction Accuracy = 49.739999999999995%, Loss = 1.2964453935623168
Epoch: 1835, Batch Gradient Norm: 44.044972412933106
Epoch: 1835, Batch Gradient Norm after: 22.36067669684381
Epoch 1836/10000, Prediction Accuracy = 49.804%, Loss = 1.3087661504745483
Epoch: 1836, Batch Gradient Norm: 39.69880103395444
Epoch: 1836, Batch Gradient Norm after: 22.360677387761623
Epoch 1837/10000, Prediction Accuracy = 49.751999999999995%, Loss = 1.295859980583191
Epoch: 1837, Batch Gradient Norm: 44.03601912549851
Epoch: 1837, Batch Gradient Norm after: 22.360678076182595
Epoch 1838/10000, Prediction Accuracy = 49.82000000000001%, Loss = 1.3081622123718262
Epoch: 1838, Batch Gradient Norm: 39.70056525827697
Epoch: 1838, Batch Gradient Norm after: 22.36067585616345
Epoch 1839/10000, Prediction Accuracy = 49.782%, Loss = 1.295279049873352
Epoch: 1839, Batch Gradient Norm: 44.01853554837019
Epoch: 1839, Batch Gradient Norm after: 22.360677742320874
Epoch 1840/10000, Prediction Accuracy = 49.84%, Loss = 1.307524275779724
Epoch: 1840, Batch Gradient Norm: 39.701552236137154
Epoch: 1840, Batch Gradient Norm after: 22.360676440083736
Epoch 1841/10000, Prediction Accuracy = 49.798%, Loss = 1.2947023153305053
Epoch: 1841, Batch Gradient Norm: 43.995432770589616
Epoch: 1841, Batch Gradient Norm after: 22.360680278899146
Epoch 1842/10000, Prediction Accuracy = 49.852%, Loss = 1.3068650007247924
Epoch: 1842, Batch Gradient Norm: 39.698658798454716
Epoch: 1842, Batch Gradient Norm after: 22.360676402196844
Epoch 1843/10000, Prediction Accuracy = 49.814%, Loss = 1.2941203117370605
Epoch: 1843, Batch Gradient Norm: 43.972983271246626
Epoch: 1843, Batch Gradient Norm after: 22.36067649521096
Epoch 1844/10000, Prediction Accuracy = 49.866%, Loss = 1.3062124252319336
Epoch: 1844, Batch Gradient Norm: 39.7007112353351
Epoch: 1844, Batch Gradient Norm after: 22.360676899991724
Epoch 1845/10000, Prediction Accuracy = 49.827999999999996%, Loss = 1.2935385704040527
Epoch: 1845, Batch Gradient Norm: 43.94865066515343
Epoch: 1845, Batch Gradient Norm after: 22.36067817052295
Epoch 1846/10000, Prediction Accuracy = 49.884%, Loss = 1.3055577993392944
Epoch: 1846, Batch Gradient Norm: 39.70026936335428
Epoch: 1846, Batch Gradient Norm after: 22.360677412517393
Epoch 1847/10000, Prediction Accuracy = 49.846%, Loss = 1.2929593324661255
Epoch: 1847, Batch Gradient Norm: 43.93149625838093
Epoch: 1847, Batch Gradient Norm after: 22.36067788400255
Epoch 1848/10000, Prediction Accuracy = 49.896%, Loss = 1.3049193143844604
Epoch: 1848, Batch Gradient Norm: 39.70022567916897
Epoch: 1848, Batch Gradient Norm after: 22.36067569308462
Epoch 1849/10000, Prediction Accuracy = 49.862%, Loss = 1.2923733472824097
Epoch: 1849, Batch Gradient Norm: 43.90959817663452
Epoch: 1849, Batch Gradient Norm after: 22.36067796550901
Epoch 1850/10000, Prediction Accuracy = 49.897999999999996%, Loss = 1.3042756557464599
Epoch: 1850, Batch Gradient Norm: 39.702745470731784
Epoch: 1850, Batch Gradient Norm after: 22.360676988496376
Epoch 1851/10000, Prediction Accuracy = 49.866%, Loss = 1.2917945384979248
Epoch: 1851, Batch Gradient Norm: 43.88207243252024
Epoch: 1851, Batch Gradient Norm after: 22.360674804628804
Epoch 1852/10000, Prediction Accuracy = 49.918%, Loss = 1.3036255359649658
Epoch: 1852, Batch Gradient Norm: 39.700012947579665
Epoch: 1852, Batch Gradient Norm after: 22.360677031278446
Epoch 1853/10000, Prediction Accuracy = 49.872%, Loss = 1.2912164688110352
Epoch: 1853, Batch Gradient Norm: 43.85699428232147
Epoch: 1853, Batch Gradient Norm after: 22.360678254022222
Epoch 1854/10000, Prediction Accuracy = 49.916%, Loss = 1.3029586553573609
Epoch: 1854, Batch Gradient Norm: 39.700023788900594
Epoch: 1854, Batch Gradient Norm after: 22.36067812146103
Epoch 1855/10000, Prediction Accuracy = 49.89%, Loss = 1.2906404256820678
Epoch: 1855, Batch Gradient Norm: 43.84166208187814
Epoch: 1855, Batch Gradient Norm after: 22.360677041269632
Epoch 1856/10000, Prediction Accuracy = 49.922000000000004%, Loss = 1.3023387670516968
Epoch: 1856, Batch Gradient Norm: 39.70178243890247
Epoch: 1856, Batch Gradient Norm after: 22.36067581108965
Epoch 1857/10000, Prediction Accuracy = 49.898%, Loss = 1.2900597333908081
Epoch: 1857, Batch Gradient Norm: 43.824439995065205
Epoch: 1857, Batch Gradient Norm after: 22.360676129592616
Epoch 1858/10000, Prediction Accuracy = 49.928%, Loss = 1.3017324209213257
Epoch: 1858, Batch Gradient Norm: 39.69988873622779
Epoch: 1858, Batch Gradient Norm after: 22.360677397241886
Epoch 1859/10000, Prediction Accuracy = 49.908%, Loss = 1.2894861698150635
Epoch: 1859, Batch Gradient Norm: 43.80327402089769
Epoch: 1859, Batch Gradient Norm after: 22.360675203371127
Epoch 1860/10000, Prediction Accuracy = 49.94000000000001%, Loss = 1.3011045455932617
Epoch: 1860, Batch Gradient Norm: 39.700242252354855
Epoch: 1860, Batch Gradient Norm after: 22.360679031798448
Epoch 1861/10000, Prediction Accuracy = 49.906%, Loss = 1.2889127254486084
Epoch: 1861, Batch Gradient Norm: 43.786161197472204
Epoch: 1861, Batch Gradient Norm after: 22.36067382587667
Epoch 1862/10000, Prediction Accuracy = 49.944%, Loss = 1.3004723072052002
Epoch: 1862, Batch Gradient Norm: 39.70158994816144
Epoch: 1862, Batch Gradient Norm after: 22.360678096636203
Epoch 1863/10000, Prediction Accuracy = 49.924%, Loss = 1.2883408308029174
Epoch: 1863, Batch Gradient Norm: 43.779498301335785
Epoch: 1863, Batch Gradient Norm after: 22.36067616867245
Epoch 1864/10000, Prediction Accuracy = 49.946000000000005%, Loss = 1.2998731851577758
Epoch: 1864, Batch Gradient Norm: 39.70599932609154
Epoch: 1864, Batch Gradient Norm after: 22.360678440360996
Epoch 1865/10000, Prediction Accuracy = 49.922000000000004%, Loss = 1.2877797603607177
Epoch: 1865, Batch Gradient Norm: 43.77140807555254
Epoch: 1865, Batch Gradient Norm after: 22.360676519117572
Epoch 1866/10000, Prediction Accuracy = 49.95%, Loss = 1.2992710590362548
Epoch: 1866, Batch Gradient Norm: 39.704654350347425
Epoch: 1866, Batch Gradient Norm after: 22.360677512833664
Epoch 1867/10000, Prediction Accuracy = 49.938%, Loss = 1.2872100591659545
Epoch: 1867, Batch Gradient Norm: 43.75518857132981
Epoch: 1867, Batch Gradient Norm after: 22.360676816025336
Epoch 1868/10000, Prediction Accuracy = 49.962%, Loss = 1.2986576557159424
Epoch: 1868, Batch Gradient Norm: 39.703414890602694
Epoch: 1868, Batch Gradient Norm after: 22.360678154769992
Epoch 1869/10000, Prediction Accuracy = 49.95399999999999%, Loss = 1.2866353750228883
Epoch: 1869, Batch Gradient Norm: 43.74455947635456
Epoch: 1869, Batch Gradient Norm after: 22.360676719264106
Epoch 1870/10000, Prediction Accuracy = 49.966%, Loss = 1.2980383396148683
Epoch: 1870, Batch Gradient Norm: 39.70488980117552
Epoch: 1870, Batch Gradient Norm after: 22.36067765166568
Epoch 1871/10000, Prediction Accuracy = 49.962%, Loss = 1.2860648393630982
Epoch: 1871, Batch Gradient Norm: 43.73210553413554
Epoch: 1871, Batch Gradient Norm after: 22.36067715108108
Epoch 1872/10000, Prediction Accuracy = 49.96999999999999%, Loss = 1.2974298238754272
Epoch: 1872, Batch Gradient Norm: 39.7026365169441
Epoch: 1872, Batch Gradient Norm after: 22.36067781462815
Epoch 1873/10000, Prediction Accuracy = 49.982%, Loss = 1.2854992866516113
Epoch: 1873, Batch Gradient Norm: 43.722100249165315
Epoch: 1873, Batch Gradient Norm after: 22.360678359072363
Epoch 1874/10000, Prediction Accuracy = 49.986%, Loss = 1.2968436241149903
Epoch: 1874, Batch Gradient Norm: 39.70130212920494
Epoch: 1874, Batch Gradient Norm after: 22.360679300872643
Epoch 1875/10000, Prediction Accuracy = 49.995999999999995%, Loss = 1.2849335193634033
Epoch: 1875, Batch Gradient Norm: 43.72224166365891
Epoch: 1875, Batch Gradient Norm after: 22.36067533851221
Epoch 1876/10000, Prediction Accuracy = 49.989999999999995%, Loss = 1.2962845802307128
Epoch: 1876, Batch Gradient Norm: 39.70199047189202
Epoch: 1876, Batch Gradient Norm after: 22.360679529485882
Epoch 1877/10000, Prediction Accuracy = 50.0%, Loss = 1.2843719482421876
Epoch: 1877, Batch Gradient Norm: 43.720782186556995
Epoch: 1877, Batch Gradient Norm after: 22.360676377534823
Epoch 1878/10000, Prediction Accuracy = 49.99400000000001%, Loss = 1.2957211017608643
Epoch: 1878, Batch Gradient Norm: 39.70158518307342
Epoch: 1878, Batch Gradient Norm after: 22.36067814682335
Epoch 1879/10000, Prediction Accuracy = 50.004%, Loss = 1.2838018894195558
Epoch: 1879, Batch Gradient Norm: 43.72398974548829
Epoch: 1879, Batch Gradient Norm after: 22.360676670303352
Epoch 1880/10000, Prediction Accuracy = 50.004%, Loss = 1.2951629400253295
Epoch: 1880, Batch Gradient Norm: 39.703102493985156
Epoch: 1880, Batch Gradient Norm after: 22.36067846629679
Epoch 1881/10000, Prediction Accuracy = 50.010000000000005%, Loss = 1.2832315921783448
Epoch: 1881, Batch Gradient Norm: 43.72270034305512
Epoch: 1881, Batch Gradient Norm after: 22.360678284296878
Epoch 1882/10000, Prediction Accuracy = 50.00600000000001%, Loss = 1.2946033000946044
Epoch: 1882, Batch Gradient Norm: 39.700609798548356
Epoch: 1882, Batch Gradient Norm after: 22.36067635662246
Epoch 1883/10000, Prediction Accuracy = 50.025999999999996%, Loss = 1.2826693773269653
Epoch: 1883, Batch Gradient Norm: 43.712304427655525
Epoch: 1883, Batch Gradient Norm after: 22.36067845145301
Epoch 1884/10000, Prediction Accuracy = 50.008%, Loss = 1.2940109968185425
Epoch: 1884, Batch Gradient Norm: 39.69611896476327
Epoch: 1884, Batch Gradient Norm after: 22.360678392636636
Epoch 1885/10000, Prediction Accuracy = 50.034000000000006%, Loss = 1.2821006298065185
Epoch: 1885, Batch Gradient Norm: 43.69334841731313
Epoch: 1885, Batch Gradient Norm after: 22.360675268273358
Epoch 1886/10000, Prediction Accuracy = 50.028000000000006%, Loss = 1.2933959245681763
Epoch: 1886, Batch Gradient Norm: 39.69777653087458
Epoch: 1886, Batch Gradient Norm after: 22.36068032281472
Epoch 1887/10000, Prediction Accuracy = 50.044%, Loss = 1.2815329074859618
Epoch: 1887, Batch Gradient Norm: 43.680294515554955
Epoch: 1887, Batch Gradient Norm after: 22.360677296930415
Epoch 1888/10000, Prediction Accuracy = 50.034000000000006%, Loss = 1.2927937984466553
Epoch: 1888, Batch Gradient Norm: 39.696266671784194
Epoch: 1888, Batch Gradient Norm after: 22.360679178834964
Epoch 1889/10000, Prediction Accuracy = 50.05800000000001%, Loss = 1.2809709787368775
Epoch: 1889, Batch Gradient Norm: 43.671008662913444
Epoch: 1889, Batch Gradient Norm after: 22.360677089737248
Epoch 1890/10000, Prediction Accuracy = 50.036%, Loss = 1.2922075986862183
Epoch: 1890, Batch Gradient Norm: 39.69579880209252
Epoch: 1890, Batch Gradient Norm after: 22.360679491395025
Epoch 1891/10000, Prediction Accuracy = 50.058%, Loss = 1.280414056777954
Epoch: 1891, Batch Gradient Norm: 43.656038933689025
Epoch: 1891, Batch Gradient Norm after: 22.360676837638096
Epoch 1892/10000, Prediction Accuracy = 50.036%, Loss = 1.2916056632995605
Epoch: 1892, Batch Gradient Norm: 39.69819920780424
Epoch: 1892, Batch Gradient Norm after: 22.360678314506952
Epoch 1893/10000, Prediction Accuracy = 50.065999999999995%, Loss = 1.2798528909683227
Epoch: 1893, Batch Gradient Norm: 43.64326824120105
Epoch: 1893, Batch Gradient Norm after: 22.360676150970047
Epoch 1894/10000, Prediction Accuracy = 50.056%, Loss = 1.291001296043396
Epoch: 1894, Batch Gradient Norm: 39.69926156305529
Epoch: 1894, Batch Gradient Norm after: 22.360679360710474
Epoch 1895/10000, Prediction Accuracy = 50.088%, Loss = 1.2792973279953004
Epoch: 1895, Batch Gradient Norm: 43.633174187787276
Epoch: 1895, Batch Gradient Norm after: 22.360677846554317
Epoch 1896/10000, Prediction Accuracy = 50.06600000000001%, Loss = 1.2904019594192504
Epoch: 1896, Batch Gradient Norm: 39.69729795561598
Epoch: 1896, Batch Gradient Norm after: 22.360679197335674
Epoch 1897/10000, Prediction Accuracy = 50.102000000000004%, Loss = 1.2787365436553955
Epoch: 1897, Batch Gradient Norm: 43.62818165225859
Epoch: 1897, Batch Gradient Norm after: 22.360676582369774
Epoch 1898/10000, Prediction Accuracy = 50.074%, Loss = 1.2898326158523559
Epoch: 1898, Batch Gradient Norm: 39.69857338645825
Epoch: 1898, Batch Gradient Norm after: 22.360678240905173
Epoch 1899/10000, Prediction Accuracy = 50.12%, Loss = 1.2781776905059814
Epoch: 1899, Batch Gradient Norm: 43.618911401526525
Epoch: 1899, Batch Gradient Norm after: 22.36067906951977
Epoch 1900/10000, Prediction Accuracy = 50.092%, Loss = 1.2892451763153077
Epoch: 1900, Batch Gradient Norm: 39.69747024742162
Epoch: 1900, Batch Gradient Norm after: 22.360677425249364
Epoch 1901/10000, Prediction Accuracy = 50.138%, Loss = 1.277622389793396
Epoch: 1901, Batch Gradient Norm: 43.62188900381627
Epoch: 1901, Batch Gradient Norm after: 22.360680071384728
Epoch 1902/10000, Prediction Accuracy = 50.092%, Loss = 1.2886991024017334
Epoch: 1902, Batch Gradient Norm: 39.69653009740405
Epoch: 1902, Batch Gradient Norm after: 22.36067570411157
Epoch 1903/10000, Prediction Accuracy = 50.142%, Loss = 1.2770617485046387
Epoch: 1903, Batch Gradient Norm: 43.61620850636993
Epoch: 1903, Batch Gradient Norm after: 22.36067736634532
Epoch 1904/10000, Prediction Accuracy = 50.096000000000004%, Loss = 1.288125205039978
Epoch: 1904, Batch Gradient Norm: 39.6961882035185
Epoch: 1904, Batch Gradient Norm after: 22.360676779265855
Epoch 1905/10000, Prediction Accuracy = 50.14999999999999%, Loss = 1.2765164136886598
Epoch: 1905, Batch Gradient Norm: 43.60989699673542
Epoch: 1905, Batch Gradient Norm after: 22.360678865627573
Epoch 1906/10000, Prediction Accuracy = 50.105999999999995%, Loss = 1.2875580072402955
Epoch: 1906, Batch Gradient Norm: 39.698277013543596
Epoch: 1906, Batch Gradient Norm after: 22.360677351176545
Epoch 1907/10000, Prediction Accuracy = 50.154%, Loss = 1.2759546279907226
Epoch: 1907, Batch Gradient Norm: 43.614296117493474
Epoch: 1907, Batch Gradient Norm after: 22.360675417184257
Epoch 1908/10000, Prediction Accuracy = 50.122%, Loss = 1.2870123863220215
Epoch: 1908, Batch Gradient Norm: 39.69553252513444
Epoch: 1908, Batch Gradient Norm after: 22.360679178039064
Epoch 1909/10000, Prediction Accuracy = 50.154%, Loss = 1.2754034280776978
Epoch: 1909, Batch Gradient Norm: 43.6157269952014
Epoch: 1909, Batch Gradient Norm after: 22.36067743831023
Epoch 1910/10000, Prediction Accuracy = 50.14%, Loss = 1.2864622831344605
Epoch: 1910, Batch Gradient Norm: 39.697682280075085
Epoch: 1910, Batch Gradient Norm after: 22.36067661064877
Epoch 1911/10000, Prediction Accuracy = 50.174%, Loss = 1.2748556852340698
Epoch: 1911, Batch Gradient Norm: 43.62665806795869
Epoch: 1911, Batch Gradient Norm after: 22.36067745290634
Epoch 1912/10000, Prediction Accuracy = 50.14%, Loss = 1.285926604270935
Epoch: 1912, Batch Gradient Norm: 39.69830955416856
Epoch: 1912, Batch Gradient Norm after: 22.360675589977824
Epoch 1913/10000, Prediction Accuracy = 50.176%, Loss = 1.2743051052093506
Epoch: 1913, Batch Gradient Norm: 43.63662059528076
Epoch: 1913, Batch Gradient Norm after: 22.360676521298846
Epoch 1914/10000, Prediction Accuracy = 50.158%, Loss = 1.285398817062378
Epoch: 1914, Batch Gradient Norm: 39.69747049489268
Epoch: 1914, Batch Gradient Norm after: 22.360677836313187
Epoch 1915/10000, Prediction Accuracy = 50.182%, Loss = 1.2737531423568726
Epoch: 1915, Batch Gradient Norm: 43.65205330338886
Epoch: 1915, Batch Gradient Norm after: 22.360678463440284
Epoch 1916/10000, Prediction Accuracy = 50.162%, Loss = 1.2848907470703126
Epoch: 1916, Batch Gradient Norm: 39.698120010067896
Epoch: 1916, Batch Gradient Norm after: 22.36067700466253
Epoch 1917/10000, Prediction Accuracy = 50.2%, Loss = 1.2732118129730225
Epoch: 1917, Batch Gradient Norm: 43.66490241325832
Epoch: 1917, Batch Gradient Norm after: 22.360677704346678
Epoch 1918/10000, Prediction Accuracy = 50.17%, Loss = 1.2843925476074218
Epoch: 1918, Batch Gradient Norm: 39.69476756611637
Epoch: 1918, Batch Gradient Norm after: 22.360678067786036
Epoch 1919/10000, Prediction Accuracy = 50.214%, Loss = 1.2726566553115846
Epoch: 1919, Batch Gradient Norm: 43.674677368067485
Epoch: 1919, Batch Gradient Norm after: 22.360676293076096
Epoch 1920/10000, Prediction Accuracy = 50.178000000000004%, Loss = 1.283873963356018
Epoch: 1920, Batch Gradient Norm: 39.69677954174882
Epoch: 1920, Batch Gradient Norm after: 22.360680254079455
Epoch 1921/10000, Prediction Accuracy = 50.242%, Loss = 1.272100043296814
Epoch: 1921, Batch Gradient Norm: 43.66551440312864
Epoch: 1921, Batch Gradient Norm after: 22.360676314624993
Epoch 1922/10000, Prediction Accuracy = 50.19199999999999%, Loss = 1.2832868099212646
Epoch: 1922, Batch Gradient Norm: 39.698511993358295
Epoch: 1922, Batch Gradient Norm after: 22.360676897706437
Epoch 1923/10000, Prediction Accuracy = 50.248000000000005%, Loss = 1.2715517044067384
Epoch: 1923, Batch Gradient Norm: 43.64728370167705
Epoch: 1923, Batch Gradient Norm after: 22.360678601650413
Epoch 1924/10000, Prediction Accuracy = 50.198%, Loss = 1.282685899734497
Epoch: 1924, Batch Gradient Norm: 39.69740553380676
Epoch: 1924, Batch Gradient Norm after: 22.360679958274915
Epoch 1925/10000, Prediction Accuracy = 50.256%, Loss = 1.271004867553711
Epoch: 1925, Batch Gradient Norm: 43.64317216420037
Epoch: 1925, Batch Gradient Norm after: 22.36067908680459
Epoch 1926/10000, Prediction Accuracy = 50.212%, Loss = 1.2821099996566772
Epoch: 1926, Batch Gradient Norm: 39.700628568039015
Epoch: 1926, Batch Gradient Norm after: 22.36067729958976
Epoch 1927/10000, Prediction Accuracy = 50.266000000000005%, Loss = 1.2704469680786132
Epoch: 1927, Batch Gradient Norm: 43.632917234849444
Epoch: 1927, Batch Gradient Norm after: 22.360677994156173
Epoch 1928/10000, Prediction Accuracy = 50.230000000000004%, Loss = 1.2815195322036743
Epoch: 1928, Batch Gradient Norm: 39.700643559057966
Epoch: 1928, Batch Gradient Norm after: 22.360679292459043
Epoch 1929/10000, Prediction Accuracy = 50.29%, Loss = 1.2698978900909423
Epoch: 1929, Batch Gradient Norm: 43.626255607232736
Epoch: 1929, Batch Gradient Norm after: 22.360676775899613
Epoch 1930/10000, Prediction Accuracy = 50.24399999999999%, Loss = 1.280948233604431
Epoch: 1930, Batch Gradient Norm: 39.70181328200858
Epoch: 1930, Batch Gradient Norm after: 22.360676753844324
Epoch 1931/10000, Prediction Accuracy = 50.294%, Loss = 1.269344449043274
Epoch: 1931, Batch Gradient Norm: 43.61308691953838
Epoch: 1931, Batch Gradient Norm after: 22.360679367904925
Epoch 1932/10000, Prediction Accuracy = 50.262%, Loss = 1.280361008644104
Epoch: 1932, Batch Gradient Norm: 39.70101345424794
Epoch: 1932, Batch Gradient Norm after: 22.360676589391105
Epoch 1933/10000, Prediction Accuracy = 50.312%, Loss = 1.2687910795211792
Epoch: 1933, Batch Gradient Norm: 43.61350265313411
Epoch: 1933, Batch Gradient Norm after: 22.360676198600594
Epoch 1934/10000, Prediction Accuracy = 50.269999999999996%, Loss = 1.2797959566116333
Epoch: 1934, Batch Gradient Norm: 39.700789328129545
Epoch: 1934, Batch Gradient Norm after: 22.360677489772623
Epoch 1935/10000, Prediction Accuracy = 50.328%, Loss = 1.2682458400726317
Epoch: 1935, Batch Gradient Norm: 43.61174386917446
Epoch: 1935, Batch Gradient Norm after: 22.360679516963053
Epoch 1936/10000, Prediction Accuracy = 50.291999999999994%, Loss = 1.279233694076538
Epoch: 1936, Batch Gradient Norm: 39.70193685865445
Epoch: 1936, Batch Gradient Norm after: 22.360678614075198
Epoch 1937/10000, Prediction Accuracy = 50.334%, Loss = 1.2677042007446289
Epoch: 1937, Batch Gradient Norm: 43.606896906633615
Epoch: 1937, Batch Gradient Norm after: 22.360677632198133
Epoch 1938/10000, Prediction Accuracy = 50.30200000000001%, Loss = 1.2786818742752075
Epoch: 1938, Batch Gradient Norm: 39.70331441932775
Epoch: 1938, Batch Gradient Norm after: 22.360676013475484
Epoch 1939/10000, Prediction Accuracy = 50.338%, Loss = 1.2671613931655883
Epoch: 1939, Batch Gradient Norm: 43.604190881951155
Epoch: 1939, Batch Gradient Norm after: 22.360677302704797
Epoch 1940/10000, Prediction Accuracy = 50.31400000000001%, Loss = 1.2781232118606567
Epoch: 1940, Batch Gradient Norm: 39.70282732373375
Epoch: 1940, Batch Gradient Norm after: 22.360676461628948
Epoch 1941/10000, Prediction Accuracy = 50.361999999999995%, Loss = 1.2666161060333252
Epoch: 1941, Batch Gradient Norm: 43.598733120364386
Epoch: 1941, Batch Gradient Norm after: 22.360676903032804
Epoch 1942/10000, Prediction Accuracy = 50.33%, Loss = 1.2775546550750732
Epoch: 1942, Batch Gradient Norm: 39.70406747035653
Epoch: 1942, Batch Gradient Norm after: 22.360676589124363
Epoch 1943/10000, Prediction Accuracy = 50.376000000000005%, Loss = 1.266071343421936
Epoch: 1943, Batch Gradient Norm: 43.59065614537226
Epoch: 1943, Batch Gradient Norm after: 22.360677242721856
Epoch 1944/10000, Prediction Accuracy = 50.342%, Loss = 1.2769923210144043
Epoch: 1944, Batch Gradient Norm: 39.70458776168243
Epoch: 1944, Batch Gradient Norm after: 22.36067644529916
Epoch 1945/10000, Prediction Accuracy = 50.4%, Loss = 1.265529704093933
Epoch: 1945, Batch Gradient Norm: 43.59292859912595
Epoch: 1945, Batch Gradient Norm after: 22.360679511419807
Epoch 1946/10000, Prediction Accuracy = 50.364%, Loss = 1.276446795463562
Epoch: 1946, Batch Gradient Norm: 39.703099816100426
Epoch: 1946, Batch Gradient Norm after: 22.36067788173785
Epoch 1947/10000, Prediction Accuracy = 50.406000000000006%, Loss = 1.2649864196777343
Epoch: 1947, Batch Gradient Norm: 43.59021818732805
Epoch: 1947, Batch Gradient Norm after: 22.360679180905848
Epoch 1948/10000, Prediction Accuracy = 50.374%, Loss = 1.2758984565734863
Epoch: 1948, Batch Gradient Norm: 39.7025434623435
Epoch: 1948, Batch Gradient Norm after: 22.360676512902142
Epoch 1949/10000, Prediction Accuracy = 50.41199999999999%, Loss = 1.2644405364990234
Epoch: 1949, Batch Gradient Norm: 43.601715727948815
Epoch: 1949, Batch Gradient Norm after: 22.3606785750295
Epoch 1950/10000, Prediction Accuracy = 50.394%, Loss = 1.2753847122192383
Epoch: 1950, Batch Gradient Norm: 39.70323626204927
Epoch: 1950, Batch Gradient Norm after: 22.360676372510397
Epoch 1951/10000, Prediction Accuracy = 50.416%, Loss = 1.2638983726501465
Epoch: 1951, Batch Gradient Norm: 43.60867055236657
Epoch: 1951, Batch Gradient Norm after: 22.360676408270017
Epoch 1952/10000, Prediction Accuracy = 50.406%, Loss = 1.2748698711395263
Epoch: 1952, Batch Gradient Norm: 39.70182516693205
Epoch: 1952, Batch Gradient Norm after: 22.36067843766381
Epoch 1953/10000, Prediction Accuracy = 50.42%, Loss = 1.2633541107177735
Epoch: 1953, Batch Gradient Norm: 43.618359130177474
Epoch: 1953, Batch Gradient Norm after: 22.36067549330148
Epoch 1954/10000, Prediction Accuracy = 50.404%, Loss = 1.2743581295013429
Epoch: 1954, Batch Gradient Norm: 39.697107120907766
Epoch: 1954, Batch Gradient Norm after: 22.360677374656817
Epoch 1955/10000, Prediction Accuracy = 50.42%, Loss = 1.2628145456314086
Epoch: 1955, Batch Gradient Norm: 43.62571117593467
Epoch: 1955, Batch Gradient Norm after: 22.360675754018768
Epoch 1956/10000, Prediction Accuracy = 50.403999999999996%, Loss = 1.2738391399383544
Epoch: 1956, Batch Gradient Norm: 39.697228905803705
Epoch: 1956, Batch Gradient Norm after: 22.360678255167038
Epoch 1957/10000, Prediction Accuracy = 50.42399999999999%, Loss = 1.262272596359253
Epoch: 1957, Batch Gradient Norm: 43.64174802821509
Epoch: 1957, Batch Gradient Norm after: 22.360678185900717
Epoch 1958/10000, Prediction Accuracy = 50.416000000000004%, Loss = 1.2733350038528441
Epoch: 1958, Batch Gradient Norm: 39.695873170825294
Epoch: 1958, Batch Gradient Norm after: 22.360679000005998
Epoch 1959/10000, Prediction Accuracy = 50.426%, Loss = 1.2617305755615233
Epoch: 1959, Batch Gradient Norm: 43.648845541623885
Epoch: 1959, Batch Gradient Norm after: 22.36067752575254
Epoch 1960/10000, Prediction Accuracy = 50.43000000000001%, Loss = 1.2728251695632935
Epoch: 1960, Batch Gradient Norm: 39.69533537216031
Epoch: 1960, Batch Gradient Norm after: 22.360676072038636
Epoch 1961/10000, Prediction Accuracy = 50.432%, Loss = 1.261196208000183
Epoch: 1961, Batch Gradient Norm: 43.66710784810501
Epoch: 1961, Batch Gradient Norm after: 22.36067644036881
Epoch 1962/10000, Prediction Accuracy = 50.438%, Loss = 1.2723355531692504
Epoch: 1962, Batch Gradient Norm: 39.69321073699038
Epoch: 1962, Batch Gradient Norm after: 22.360677042069902
Epoch 1963/10000, Prediction Accuracy = 50.434%, Loss = 1.2606644868850707
Epoch: 1963, Batch Gradient Norm: 43.682955714424175
Epoch: 1963, Batch Gradient Norm after: 22.36067798147797
Epoch 1964/10000, Prediction Accuracy = 50.465999999999994%, Loss = 1.2718346118927002
Epoch: 1964, Batch Gradient Norm: 39.69148536989536
Epoch: 1964, Batch Gradient Norm after: 22.360678822665918
Epoch 1965/10000, Prediction Accuracy = 50.446%, Loss = 1.2601235151290893
Epoch: 1965, Batch Gradient Norm: 43.699428584364995
Epoch: 1965, Batch Gradient Norm after: 22.36067670579782
Epoch 1966/10000, Prediction Accuracy = 50.476%, Loss = 1.2713515996932983
Epoch: 1966, Batch Gradient Norm: 39.69008583535396
Epoch: 1966, Batch Gradient Norm after: 22.36067581976228
Epoch 1967/10000, Prediction Accuracy = 50.444%, Loss = 1.259588646888733
Epoch: 1967, Batch Gradient Norm: 43.72244028672621
Epoch: 1967, Batch Gradient Norm after: 22.360677593706527
Epoch 1968/10000, Prediction Accuracy = 50.478%, Loss = 1.2708776950836183
Epoch: 1968, Batch Gradient Norm: 39.68930735077416
Epoch: 1968, Batch Gradient Norm after: 22.360676835747764
Epoch 1969/10000, Prediction Accuracy = 50.446%, Loss = 1.259052038192749
Epoch: 1969, Batch Gradient Norm: 43.73930868888185
Epoch: 1969, Batch Gradient Norm after: 22.360677626978703
Epoch 1970/10000, Prediction Accuracy = 50.492%, Loss = 1.270401692390442
Epoch: 1970, Batch Gradient Norm: 39.686220208703254
Epoch: 1970, Batch Gradient Norm after: 22.360675804007194
Epoch 1971/10000, Prediction Accuracy = 50.458000000000006%, Loss = 1.2585180759429933
Epoch: 1971, Batch Gradient Norm: 43.756150183774665
Epoch: 1971, Batch Gradient Norm after: 22.36067791119189
Epoch 1972/10000, Prediction Accuracy = 50.51%, Loss = 1.2699237585067749
Epoch: 1972, Batch Gradient Norm: 39.68663949146661
Epoch: 1972, Batch Gradient Norm after: 22.36067758083038
Epoch 1973/10000, Prediction Accuracy = 50.454%, Loss = 1.2579789638519288
Epoch: 1973, Batch Gradient Norm: 43.76176812076266
Epoch: 1973, Batch Gradient Norm after: 22.360676701529968
Epoch 1974/10000, Prediction Accuracy = 50.522000000000006%, Loss = 1.2694071292877198
Epoch: 1974, Batch Gradient Norm: 39.686016086610806
Epoch: 1974, Batch Gradient Norm after: 22.360678261906195
Epoch 1975/10000, Prediction Accuracy = 50.474000000000004%, Loss = 1.2574382543563842
Epoch: 1975, Batch Gradient Norm: 43.7735638674768
Epoch: 1975, Batch Gradient Norm after: 22.36067680171226
Epoch 1976/10000, Prediction Accuracy = 50.523999999999994%, Loss = 1.2689049005508424
Epoch: 1976, Batch Gradient Norm: 39.68393618354299
Epoch: 1976, Batch Gradient Norm after: 22.360677429200056
Epoch 1977/10000, Prediction Accuracy = 50.474000000000004%, Loss = 1.2569032192230225
Epoch: 1977, Batch Gradient Norm: 43.78416858520758
Epoch: 1977, Batch Gradient Norm after: 22.360676670769735
Epoch 1978/10000, Prediction Accuracy = 50.528%, Loss = 1.268405556678772
Epoch: 1978, Batch Gradient Norm: 39.68253454224889
Epoch: 1978, Batch Gradient Norm after: 22.360675577857563
Epoch 1979/10000, Prediction Accuracy = 50.489999999999995%, Loss = 1.256360650062561
Epoch: 1979, Batch Gradient Norm: 43.78770318213012
Epoch: 1979, Batch Gradient Norm after: 22.360676192419113
Epoch 1980/10000, Prediction Accuracy = 50.54600000000001%, Loss = 1.2678812980651855
Epoch: 1980, Batch Gradient Norm: 39.68169290491487
Epoch: 1980, Batch Gradient Norm after: 22.36067549751373
Epoch 1981/10000, Prediction Accuracy = 50.50600000000001%, Loss = 1.2558209896087646
Epoch: 1981, Batch Gradient Norm: 43.7804773351241
Epoch: 1981, Batch Gradient Norm after: 22.3606754869127
Epoch 1982/10000, Prediction Accuracy = 50.556%, Loss = 1.267333722114563
Epoch: 1982, Batch Gradient Norm: 39.679607680370175
Epoch: 1982, Batch Gradient Norm after: 22.360675945142095
Epoch 1983/10000, Prediction Accuracy = 50.516000000000005%, Loss = 1.25528724193573
Epoch: 1983, Batch Gradient Norm: 43.77432256675619
Epoch: 1983, Batch Gradient Norm after: 22.36067724420771
Epoch 1984/10000, Prediction Accuracy = 50.577999999999996%, Loss = 1.266769027709961
Epoch: 1984, Batch Gradient Norm: 39.6771396243049
Epoch: 1984, Batch Gradient Norm after: 22.360676301723068
Epoch 1985/10000, Prediction Accuracy = 50.528%, Loss = 1.2547432899475097
Epoch: 1985, Batch Gradient Norm: 43.76585678795441
Epoch: 1985, Batch Gradient Norm after: 22.360675774422603
Epoch 1986/10000, Prediction Accuracy = 50.574%, Loss = 1.2662168741226196
Epoch: 1986, Batch Gradient Norm: 39.67788989572762
Epoch: 1986, Batch Gradient Norm after: 22.360676188220296
Epoch 1987/10000, Prediction Accuracy = 50.528000000000006%, Loss = 1.2542064428329467
Epoch: 1987, Batch Gradient Norm: 43.755459699274574
Epoch: 1987, Batch Gradient Norm after: 22.360677916574154
Epoch 1988/10000, Prediction Accuracy = 50.589999999999996%, Loss = 1.2656429529190063
Epoch: 1988, Batch Gradient Norm: 39.679488902961666
Epoch: 1988, Batch Gradient Norm after: 22.360677197872878
Epoch 1989/10000, Prediction Accuracy = 50.528000000000006%, Loss = 1.2536727666854859
Epoch: 1989, Batch Gradient Norm: 43.74571893483935
Epoch: 1989, Batch Gradient Norm after: 22.360676664125407
Epoch 1990/10000, Prediction Accuracy = 50.606%, Loss = 1.2650729894638062
Epoch: 1990, Batch Gradient Norm: 39.67951565686595
Epoch: 1990, Batch Gradient Norm after: 22.360676465793276
Epoch 1991/10000, Prediction Accuracy = 50.534%, Loss = 1.2531376600265502
Epoch: 1991, Batch Gradient Norm: 43.73305547609751
Epoch: 1991, Batch Gradient Norm after: 22.360675837092828
Epoch 1992/10000, Prediction Accuracy = 50.614000000000004%, Loss = 1.2645015954971313
Epoch: 1992, Batch Gradient Norm: 39.67922239013751
Epoch: 1992, Batch Gradient Norm after: 22.36068110343201
Epoch 1993/10000, Prediction Accuracy = 50.534000000000006%, Loss = 1.252603244781494
Epoch: 1993, Batch Gradient Norm: 43.72153693887214
Epoch: 1993, Batch Gradient Norm after: 22.360675843854217
Epoch 1994/10000, Prediction Accuracy = 50.626%, Loss = 1.2639499187469483
Epoch: 1994, Batch Gradient Norm: 39.68106511377644
Epoch: 1994, Batch Gradient Norm after: 22.360677451783328
Epoch 1995/10000, Prediction Accuracy = 50.542%, Loss = 1.2520706176757812
Epoch: 1995, Batch Gradient Norm: 43.71089153916884
Epoch: 1995, Batch Gradient Norm after: 22.360676206435098
Epoch 1996/10000, Prediction Accuracy = 50.636%, Loss = 1.2633853197097777
Epoch: 1996, Batch Gradient Norm: 39.68502657948786
Epoch: 1996, Batch Gradient Norm after: 22.360678747538607
Epoch 1997/10000, Prediction Accuracy = 50.546%, Loss = 1.2515391111373901
Epoch: 1997, Batch Gradient Norm: 43.70337928839112
Epoch: 1997, Batch Gradient Norm after: 22.360676740590385
Epoch 1998/10000, Prediction Accuracy = 50.641999999999996%, Loss = 1.2628389120101928
Epoch: 1998, Batch Gradient Norm: 39.683915973564766
Epoch: 1998, Batch Gradient Norm after: 22.360677670174606
Epoch 1999/10000, Prediction Accuracy = 50.556%, Loss = 1.251005482673645
Epoch: 1999, Batch Gradient Norm: 43.70042487456477
Epoch: 1999, Batch Gradient Norm after: 22.360675856728466
Epoch 2000/10000, Prediction Accuracy = 50.652%, Loss = 1.2623012065887451
Epoch: 2000, Batch Gradient Norm: 39.683173492141236
Epoch: 2000, Batch Gradient Norm after: 22.36067807829028
Epoch 2001/10000, Prediction Accuracy = 50.582%, Loss = 1.2504757165908813
Epoch: 2001, Batch Gradient Norm: 43.68329176425423
Epoch: 2001, Batch Gradient Norm after: 22.360676129360943
Epoch 2002/10000, Prediction Accuracy = 50.656000000000006%, Loss = 1.2617289304733277
Epoch: 2002, Batch Gradient Norm: 39.68233359273968
Epoch: 2002, Batch Gradient Norm after: 22.36067693318839
Epoch 2003/10000, Prediction Accuracy = 50.593999999999994%, Loss = 1.249948787689209
Epoch: 2003, Batch Gradient Norm: 43.67397494291309
Epoch: 2003, Batch Gradient Norm after: 22.360675926035633
Epoch 2004/10000, Prediction Accuracy = 50.664%, Loss = 1.261172604560852
Epoch: 2004, Batch Gradient Norm: 39.680148277445106
Epoch: 2004, Batch Gradient Norm after: 22.360677211169723
Epoch 2005/10000, Prediction Accuracy = 50.606%, Loss = 1.2494173288345336
Epoch: 2005, Batch Gradient Norm: 43.66568649629065
Epoch: 2005, Batch Gradient Norm after: 22.360677373982142
Epoch 2006/10000, Prediction Accuracy = 50.658%, Loss = 1.2606305122375487
Epoch: 2006, Batch Gradient Norm: 39.68151248310561
Epoch: 2006, Batch Gradient Norm after: 22.360677566785746
Epoch 2007/10000, Prediction Accuracy = 50.616%, Loss = 1.2488898277282714
Epoch: 2007, Batch Gradient Norm: 43.66373999506043
Epoch: 2007, Batch Gradient Norm after: 22.360677161662927
Epoch 2008/10000, Prediction Accuracy = 50.66%, Loss = 1.260091209411621
Epoch: 2008, Batch Gradient Norm: 39.681965238347836
Epoch: 2008, Batch Gradient Norm after: 22.360677293519597
Epoch 2009/10000, Prediction Accuracy = 50.626%, Loss = 1.2483556747436524
Epoch: 2009, Batch Gradient Norm: 43.65528248059102
Epoch: 2009, Batch Gradient Norm after: 22.360677935121757
Epoch 2010/10000, Prediction Accuracy = 50.682%, Loss = 1.259543776512146
Epoch: 2010, Batch Gradient Norm: 39.68322570088497
Epoch: 2010, Batch Gradient Norm after: 22.360677311819305
Epoch 2011/10000, Prediction Accuracy = 50.642%, Loss = 1.2478363513946533
Epoch: 2011, Batch Gradient Norm: 43.65112955704164
Epoch: 2011, Batch Gradient Norm after: 22.360676362755434
Epoch 2012/10000, Prediction Accuracy = 50.694%, Loss = 1.259002947807312
Epoch: 2012, Batch Gradient Norm: 39.68423508624581
Epoch: 2012, Batch Gradient Norm after: 22.360677113467553
Epoch 2013/10000, Prediction Accuracy = 50.644%, Loss = 1.2473105907440185
Epoch: 2013, Batch Gradient Norm: 43.637277433776
Epoch: 2013, Batch Gradient Norm after: 22.360676761998096
Epoch 2014/10000, Prediction Accuracy = 50.714000000000006%, Loss = 1.2584407091140748
Epoch: 2014, Batch Gradient Norm: 39.68340017206402
Epoch: 2014, Batch Gradient Norm after: 22.360677159916232
Epoch 2015/10000, Prediction Accuracy = 50.64%, Loss = 1.2467890739440919
Epoch: 2015, Batch Gradient Norm: 43.627810456437146
Epoch: 2015, Batch Gradient Norm after: 22.360677986286273
Epoch 2016/10000, Prediction Accuracy = 50.714%, Loss = 1.2578852891921997
Epoch: 2016, Batch Gradient Norm: 39.683141392592134
Epoch: 2016, Batch Gradient Norm after: 22.360676291426753
Epoch 2017/10000, Prediction Accuracy = 50.644%, Loss = 1.2462661981582641
Epoch: 2017, Batch Gradient Norm: 43.6224685360914
Epoch: 2017, Batch Gradient Norm after: 22.36067796791066
Epoch 2018/10000, Prediction Accuracy = 50.709999999999994%, Loss = 1.2573536872863769
Epoch: 2018, Batch Gradient Norm: 39.68318220896702
Epoch: 2018, Batch Gradient Norm after: 22.36067906113722
Epoch 2019/10000, Prediction Accuracy = 50.652%, Loss = 1.245743703842163
Epoch: 2019, Batch Gradient Norm: 43.62339840895421
Epoch: 2019, Batch Gradient Norm after: 22.36067694718062
Epoch 2020/10000, Prediction Accuracy = 50.720000000000006%, Loss = 1.2568407773971557
Epoch: 2020, Batch Gradient Norm: 39.67846602273961
Epoch: 2020, Batch Gradient Norm after: 22.36067716673887
Epoch 2021/10000, Prediction Accuracy = 50.666%, Loss = 1.2452250242233276
Epoch: 2021, Batch Gradient Norm: 43.62853378094586
Epoch: 2021, Batch Gradient Norm after: 22.360675260539757
Epoch 2022/10000, Prediction Accuracy = 50.738%, Loss = 1.2563351392745972
Epoch: 2022, Batch Gradient Norm: 39.6746519777525
Epoch: 2022, Batch Gradient Norm after: 22.360677615955233
Epoch 2023/10000, Prediction Accuracy = 50.678%, Loss = 1.2447068452835084
Epoch: 2023, Batch Gradient Norm: 43.62373128794363
Epoch: 2023, Batch Gradient Norm after: 22.360676093718762
Epoch 2024/10000, Prediction Accuracy = 50.758%, Loss = 1.2558122873306274
Epoch: 2024, Batch Gradient Norm: 39.670527712079085
Epoch: 2024, Batch Gradient Norm after: 22.360676197414087
Epoch 2025/10000, Prediction Accuracy = 50.688%, Loss = 1.2441861629486084
Epoch: 2025, Batch Gradient Norm: 43.61553005578415
Epoch: 2025, Batch Gradient Norm after: 22.360675987937185
Epoch 2026/10000, Prediction Accuracy = 50.772000000000006%, Loss = 1.255279803276062
Epoch: 2026, Batch Gradient Norm: 39.66737758453627
Epoch: 2026, Batch Gradient Norm after: 22.36067825464183
Epoch 2027/10000, Prediction Accuracy = 50.71%, Loss = 1.2436672925949097
Epoch: 2027, Batch Gradient Norm: 43.59601390922277
Epoch: 2027, Batch Gradient Norm after: 22.360678104401227
Epoch 2028/10000, Prediction Accuracy = 50.786%, Loss = 1.2547101974487305
Epoch: 2028, Batch Gradient Norm: 39.66616604493167
Epoch: 2028, Batch Gradient Norm after: 22.36067819944785
Epoch 2029/10000, Prediction Accuracy = 50.720000000000006%, Loss = 1.243143129348755
Epoch: 2029, Batch Gradient Norm: 43.57646309553995
Epoch: 2029, Batch Gradient Norm after: 22.360676475212173
Epoch 2030/10000, Prediction Accuracy = 50.804%, Loss = 1.2541337728500366
Epoch: 2030, Batch Gradient Norm: 39.666565447133465
Epoch: 2030, Batch Gradient Norm after: 22.360677272645386
Epoch 2031/10000, Prediction Accuracy = 50.726%, Loss = 1.2426228761672973
Epoch: 2031, Batch Gradient Norm: 43.55688897214404
Epoch: 2031, Batch Gradient Norm after: 22.36067691561415
Epoch 2032/10000, Prediction Accuracy = 50.818%, Loss = 1.2535545349121093
Epoch: 2032, Batch Gradient Norm: 39.6658996637638
Epoch: 2032, Batch Gradient Norm after: 22.36067800399239
Epoch 2033/10000, Prediction Accuracy = 50.739999999999995%, Loss = 1.2421041965484618
Epoch: 2033, Batch Gradient Norm: 43.53660189345251
Epoch: 2033, Batch Gradient Norm after: 22.360676739123033
Epoch 2034/10000, Prediction Accuracy = 50.83200000000001%, Loss = 1.2529828548431396
Epoch: 2034, Batch Gradient Norm: 39.665911842609276
Epoch: 2034, Batch Gradient Norm after: 22.360676656405985
Epoch 2035/10000, Prediction Accuracy = 50.75600000000001%, Loss = 1.241583251953125
Epoch: 2035, Batch Gradient Norm: 43.51161343959438
Epoch: 2035, Batch Gradient Norm after: 22.360675431774755
Epoch 2036/10000, Prediction Accuracy = 50.848%, Loss = 1.2524086236953735
Epoch: 2036, Batch Gradient Norm: 39.663494845506506
Epoch: 2036, Batch Gradient Norm after: 22.360678710792698
Epoch 2037/10000, Prediction Accuracy = 50.762%, Loss = 1.2410761356353759
Epoch: 2037, Batch Gradient Norm: 43.486425415631246
Epoch: 2037, Batch Gradient Norm after: 22.360677440562075
Epoch 2038/10000, Prediction Accuracy = 50.843999999999994%, Loss = 1.2518335103988647
Epoch: 2038, Batch Gradient Norm: 39.660291496215194
Epoch: 2038, Batch Gradient Norm after: 22.360679557372205
Epoch 2039/10000, Prediction Accuracy = 50.776%, Loss = 1.240555214881897
Epoch: 2039, Batch Gradient Norm: 43.456140026145846
Epoch: 2039, Batch Gradient Norm after: 22.360676719552544
Epoch 2040/10000, Prediction Accuracy = 50.846%, Loss = 1.251246976852417
Epoch: 2040, Batch Gradient Norm: 39.659859713103884
Epoch: 2040, Batch Gradient Norm after: 22.36067924662366
Epoch 2041/10000, Prediction Accuracy = 50.792%, Loss = 1.2400348663330079
Epoch: 2041, Batch Gradient Norm: 43.44162165650325
Epoch: 2041, Batch Gradient Norm after: 22.36067805044088
Epoch 2042/10000, Prediction Accuracy = 50.85799999999999%, Loss = 1.2506914377212524
Epoch: 2042, Batch Gradient Norm: 39.658703376727686
Epoch: 2042, Batch Gradient Norm after: 22.360678674998415
Epoch 2043/10000, Prediction Accuracy = 50.822%, Loss = 1.2395257472991943
Epoch: 2043, Batch Gradient Norm: 43.429421754135305
Epoch: 2043, Batch Gradient Norm after: 22.360677414342753
Epoch 2044/10000, Prediction Accuracy = 50.862%, Loss = 1.2501448154449464
Epoch: 2044, Batch Gradient Norm: 39.65669614769668
Epoch: 2044, Batch Gradient Norm after: 22.36067746722537
Epoch 2045/10000, Prediction Accuracy = 50.846000000000004%, Loss = 1.2390093803405762
Epoch: 2045, Batch Gradient Norm: 43.41839650594161
Epoch: 2045, Batch Gradient Norm after: 22.360675294637428
Epoch 2046/10000, Prediction Accuracy = 50.86%, Loss = 1.249611234664917
Epoch: 2046, Batch Gradient Norm: 39.65899932137332
Epoch: 2046, Batch Gradient Norm after: 22.36067766786558
Epoch 2047/10000, Prediction Accuracy = 50.87%, Loss = 1.2384936094284058
Epoch: 2047, Batch Gradient Norm: 43.41895339993164
Epoch: 2047, Batch Gradient Norm after: 22.360675794342217
Epoch 2048/10000, Prediction Accuracy = 50.873999999999995%, Loss = 1.2491036891937255
Epoch: 2048, Batch Gradient Norm: 39.65728945143006
Epoch: 2048, Batch Gradient Norm after: 22.360677713803963
Epoch 2049/10000, Prediction Accuracy = 50.872%, Loss = 1.2379855155944823
Epoch: 2049, Batch Gradient Norm: 43.41330456705799
Epoch: 2049, Batch Gradient Norm after: 22.36067578325662
Epoch 2050/10000, Prediction Accuracy = 50.884%, Loss = 1.2485714435577393
Epoch: 2050, Batch Gradient Norm: 39.65712483851884
Epoch: 2050, Batch Gradient Norm after: 22.36067875941244
Epoch 2051/10000, Prediction Accuracy = 50.884%, Loss = 1.2374739408493043
Epoch: 2051, Batch Gradient Norm: 43.40584809000735
Epoch: 2051, Batch Gradient Norm after: 22.360676531271064
Epoch 2052/10000, Prediction Accuracy = 50.906%, Loss = 1.2480420112609862
Epoch: 2052, Batch Gradient Norm: 39.6535356601068
Epoch: 2052, Batch Gradient Norm after: 22.36067868109549
Epoch 2053/10000, Prediction Accuracy = 50.894%, Loss = 1.2369666814804077
Epoch: 2053, Batch Gradient Norm: 43.415974364028926
Epoch: 2053, Batch Gradient Norm after: 22.360677656013557
Epoch 2054/10000, Prediction Accuracy = 50.928%, Loss = 1.2475495338439941
Epoch: 2054, Batch Gradient Norm: 39.65239182191303
Epoch: 2054, Batch Gradient Norm after: 22.36067863990538
Epoch 2055/10000, Prediction Accuracy = 50.89999999999999%, Loss = 1.2364562034606934
Epoch: 2055, Batch Gradient Norm: 43.412859379842345
Epoch: 2055, Batch Gradient Norm after: 22.36067638834912
Epoch 2056/10000, Prediction Accuracy = 50.94799999999999%, Loss = 1.247032356262207
Epoch: 2056, Batch Gradient Norm: 39.64864851573262
Epoch: 2056, Batch Gradient Norm after: 22.360678577048887
Epoch 2057/10000, Prediction Accuracy = 50.908%, Loss = 1.235940170288086
Epoch: 2057, Batch Gradient Norm: 43.40496413945649
Epoch: 2057, Batch Gradient Norm after: 22.36067597658199
Epoch 2058/10000, Prediction Accuracy = 50.964%, Loss = 1.24651095867157
Epoch: 2058, Batch Gradient Norm: 39.64397987389288
Epoch: 2058, Batch Gradient Norm after: 22.360677111799557
Epoch 2059/10000, Prediction Accuracy = 50.91799999999999%, Loss = 1.235428762435913
Epoch: 2059, Batch Gradient Norm: 43.39987277622309
Epoch: 2059, Batch Gradient Norm after: 22.36067913001238
Epoch 2060/10000, Prediction Accuracy = 50.992000000000004%, Loss = 1.2459936141967773
Epoch: 2060, Batch Gradient Norm: 39.64224813486722
Epoch: 2060, Batch Gradient Norm after: 22.360675513240178
Epoch 2061/10000, Prediction Accuracy = 50.936%, Loss = 1.2349144697189331
Epoch: 2061, Batch Gradient Norm: 43.38736232762547
Epoch: 2061, Batch Gradient Norm after: 22.360675831500057
Epoch 2062/10000, Prediction Accuracy = 50.99400000000001%, Loss = 1.2454478979110717
Epoch: 2062, Batch Gradient Norm: 39.63772108301805
Epoch: 2062, Batch Gradient Norm after: 22.360679246117275
Epoch 2063/10000, Prediction Accuracy = 50.946%, Loss = 1.234407901763916
Epoch: 2063, Batch Gradient Norm: 43.37214127243487
Epoch: 2063, Batch Gradient Norm after: 22.360676956007616
Epoch 2064/10000, Prediction Accuracy = 51.006%, Loss = 1.244895100593567
Epoch: 2064, Batch Gradient Norm: 39.63705803851818
Epoch: 2064, Batch Gradient Norm after: 22.3606795310975
Epoch 2065/10000, Prediction Accuracy = 50.948%, Loss = 1.2338984966278077
Epoch: 2065, Batch Gradient Norm: 43.35627406263699
Epoch: 2065, Batch Gradient Norm after: 22.36067821365241
Epoch 2066/10000, Prediction Accuracy = 51.02%, Loss = 1.2443501472473144
Epoch: 2066, Batch Gradient Norm: 39.63709064591473
Epoch: 2066, Batch Gradient Norm after: 22.360678830229926
Epoch 2067/10000, Prediction Accuracy = 50.958%, Loss = 1.233393621444702
Epoch: 2067, Batch Gradient Norm: 43.339317624598316
Epoch: 2067, Batch Gradient Norm after: 22.36067709412137
Epoch 2068/10000, Prediction Accuracy = 51.04%, Loss = 1.2438142061233521
Epoch: 2068, Batch Gradient Norm: 39.635102349686214
Epoch: 2068, Batch Gradient Norm after: 22.36067874101762
Epoch 2069/10000, Prediction Accuracy = 50.968%, Loss = 1.232884168624878
Epoch: 2069, Batch Gradient Norm: 43.31471814130722
Epoch: 2069, Batch Gradient Norm after: 22.360677436069135
Epoch 2070/10000, Prediction Accuracy = 51.048%, Loss = 1.243241024017334
Epoch: 2070, Batch Gradient Norm: 39.63404961105934
Epoch: 2070, Batch Gradient Norm after: 22.360676961356223
Epoch 2071/10000, Prediction Accuracy = 50.974000000000004%, Loss = 1.2323781251907349
Epoch: 2071, Batch Gradient Norm: 43.29259111300812
Epoch: 2071, Batch Gradient Norm after: 22.360677732140775
Epoch 2072/10000, Prediction Accuracy = 51.044%, Loss = 1.2426778316497802
Epoch: 2072, Batch Gradient Norm: 39.63241019030414
Epoch: 2072, Batch Gradient Norm after: 22.3606757960726
Epoch 2073/10000, Prediction Accuracy = 50.980000000000004%, Loss = 1.231877374649048
Epoch: 2073, Batch Gradient Norm: 43.271597253273356
Epoch: 2073, Batch Gradient Norm after: 22.36067668265719
Epoch 2074/10000, Prediction Accuracy = 51.050000000000004%, Loss = 1.242115569114685
Epoch: 2074, Batch Gradient Norm: 39.632785896073834
Epoch: 2074, Batch Gradient Norm after: 22.360676437154225
Epoch 2075/10000, Prediction Accuracy = 50.988%, Loss = 1.231376838684082
Epoch: 2075, Batch Gradient Norm: 43.24647913941231
Epoch: 2075, Batch Gradient Norm after: 22.360677273916732
Epoch 2076/10000, Prediction Accuracy = 51.062%, Loss = 1.2415562629699708
Epoch: 2076, Batch Gradient Norm: 39.63178406342658
Epoch: 2076, Batch Gradient Norm after: 22.360677552203
Epoch 2077/10000, Prediction Accuracy = 51.0%, Loss = 1.230875563621521
Epoch: 2077, Batch Gradient Norm: 43.2241149015616
Epoch: 2077, Batch Gradient Norm after: 22.360677199996093
Epoch 2078/10000, Prediction Accuracy = 51.05799999999999%, Loss = 1.2409902811050415
Epoch: 2078, Batch Gradient Norm: 39.62906814027252
Epoch: 2078, Batch Gradient Norm after: 22.360679687372574
Epoch 2079/10000, Prediction Accuracy = 51.024%, Loss = 1.230373787879944
Epoch: 2079, Batch Gradient Norm: 43.2077216538723
Epoch: 2079, Batch Gradient Norm after: 22.360677201056223
Epoch 2080/10000, Prediction Accuracy = 51.05799999999999%, Loss = 1.240451693534851
Epoch: 2080, Batch Gradient Norm: 39.62607576304966
Epoch: 2080, Batch Gradient Norm after: 22.360678227801362
Epoch 2081/10000, Prediction Accuracy = 51.028000000000006%, Loss = 1.2298747301101685
Epoch: 2081, Batch Gradient Norm: 43.191419498415364
Epoch: 2081, Batch Gradient Norm after: 22.360676209757315
Epoch 2082/10000, Prediction Accuracy = 51.064%, Loss = 1.2399030685424806
Epoch: 2082, Batch Gradient Norm: 39.627193343280766
Epoch: 2082, Batch Gradient Norm after: 22.360678963651452
Epoch 2083/10000, Prediction Accuracy = 51.041999999999994%, Loss = 1.2293752193450929
Epoch: 2083, Batch Gradient Norm: 43.17215147674159
Epoch: 2083, Batch Gradient Norm after: 22.36067680710769
Epoch 2084/10000, Prediction Accuracy = 51.071999999999996%, Loss = 1.2393532991409302
Epoch: 2084, Batch Gradient Norm: 39.62623525699129
Epoch: 2084, Batch Gradient Norm after: 22.360677091878234
Epoch 2085/10000, Prediction Accuracy = 51.053999999999995%, Loss = 1.2288756608963012
Epoch: 2085, Batch Gradient Norm: 43.160592908585855
Epoch: 2085, Batch Gradient Norm after: 22.360676905341673
Epoch 2086/10000, Prediction Accuracy = 51.093999999999994%, Loss = 1.2388155937194825
Epoch: 2086, Batch Gradient Norm: 39.623522737948555
Epoch: 2086, Batch Gradient Norm after: 22.36067701947651
Epoch 2087/10000, Prediction Accuracy = 51.06400000000001%, Loss = 1.2283788681030274
Epoch: 2087, Batch Gradient Norm: 43.14060864556458
Epoch: 2087, Batch Gradient Norm after: 22.36067675307539
Epoch 2088/10000, Prediction Accuracy = 51.096000000000004%, Loss = 1.2382616996765137
Epoch: 2088, Batch Gradient Norm: 39.62583479633907
Epoch: 2088, Batch Gradient Norm after: 22.36067686290605
Epoch 2089/10000, Prediction Accuracy = 51.088%, Loss = 1.2278819322586059
Epoch: 2089, Batch Gradient Norm: 43.123388789609116
Epoch: 2089, Batch Gradient Norm after: 22.36067624563877
Epoch 2090/10000, Prediction Accuracy = 51.1%, Loss = 1.2377262592315674
Epoch: 2090, Batch Gradient Norm: 39.62125384164237
Epoch: 2090, Batch Gradient Norm after: 22.36067770055048
Epoch 2091/10000, Prediction Accuracy = 51.10600000000001%, Loss = 1.2273804903030396
Epoch: 2091, Batch Gradient Norm: 43.10585755297279
Epoch: 2091, Batch Gradient Norm after: 22.360678197326603
Epoch 2092/10000, Prediction Accuracy = 51.10799999999999%, Loss = 1.2371767044067383
Epoch: 2092, Batch Gradient Norm: 39.62022202384226
Epoch: 2092, Batch Gradient Norm after: 22.36067874320498
Epoch 2093/10000, Prediction Accuracy = 51.126%, Loss = 1.226888394355774
Epoch: 2093, Batch Gradient Norm: 43.08318253884554
Epoch: 2093, Batch Gradient Norm after: 22.360675361882254
Epoch 2094/10000, Prediction Accuracy = 51.116%, Loss = 1.236613392829895
Epoch: 2094, Batch Gradient Norm: 39.6210643721331
Epoch: 2094, Batch Gradient Norm after: 22.360679178495413
Epoch 2095/10000, Prediction Accuracy = 51.134%, Loss = 1.2264003515243531
Epoch: 2095, Batch Gradient Norm: 43.06267838580033
Epoch: 2095, Batch Gradient Norm after: 22.360674815287826
Epoch 2096/10000, Prediction Accuracy = 51.129999999999995%, Loss = 1.2360549926757813
Epoch: 2096, Batch Gradient Norm: 39.62334427966132
Epoch: 2096, Batch Gradient Norm after: 22.36067901118947
Epoch 2097/10000, Prediction Accuracy = 51.148%, Loss = 1.2259015798568726
Epoch: 2097, Batch Gradient Norm: 43.043925012554816
Epoch: 2097, Batch Gradient Norm after: 22.36067642969244
Epoch 2098/10000, Prediction Accuracy = 51.132%, Loss = 1.2355051040649414
Epoch: 2098, Batch Gradient Norm: 39.62151987206794
Epoch: 2098, Batch Gradient Norm after: 22.360678568510455
Epoch 2099/10000, Prediction Accuracy = 51.172000000000004%, Loss = 1.2254112482070922
Epoch: 2099, Batch Gradient Norm: 43.02225950690322
Epoch: 2099, Batch Gradient Norm after: 22.36067647771612
Epoch 2100/10000, Prediction Accuracy = 51.138%, Loss = 1.234948182106018
Epoch: 2100, Batch Gradient Norm: 39.6215945065994
Epoch: 2100, Batch Gradient Norm after: 22.36067824752573
Epoch 2101/10000, Prediction Accuracy = 51.202%, Loss = 1.2249126195907594
Epoch: 2101, Batch Gradient Norm: 42.99771879126534
Epoch: 2101, Batch Gradient Norm after: 22.360678318613406
Epoch 2102/10000, Prediction Accuracy = 51.158%, Loss = 1.234387469291687
Epoch: 2102, Batch Gradient Norm: 39.62473367601586
Epoch: 2102, Batch Gradient Norm after: 22.360678444274285
Epoch 2103/10000, Prediction Accuracy = 51.206%, Loss = 1.2244247198104858
Epoch: 2103, Batch Gradient Norm: 42.97602515967289
Epoch: 2103, Batch Gradient Norm after: 22.360674777825974
Epoch 2104/10000, Prediction Accuracy = 51.17%, Loss = 1.233832836151123
Epoch: 2104, Batch Gradient Norm: 39.62432273115606
Epoch: 2104, Batch Gradient Norm after: 22.360677308343778
Epoch 2105/10000, Prediction Accuracy = 51.208000000000006%, Loss = 1.2239402532577515
Epoch: 2105, Batch Gradient Norm: 42.9604519384797
Epoch: 2105, Batch Gradient Norm after: 22.360676625298165
Epoch 2106/10000, Prediction Accuracy = 51.174%, Loss = 1.23330557346344
Epoch: 2106, Batch Gradient Norm: 39.6209667638042
Epoch: 2106, Batch Gradient Norm after: 22.360676563152175
Epoch 2107/10000, Prediction Accuracy = 51.215999999999994%, Loss = 1.223453164100647
Epoch: 2107, Batch Gradient Norm: 42.93832134721161
Epoch: 2107, Batch Gradient Norm after: 22.360677171545348
Epoch 2108/10000, Prediction Accuracy = 51.174%, Loss = 1.2327599763870238
Epoch: 2108, Batch Gradient Norm: 39.61999940901749
Epoch: 2108, Batch Gradient Norm after: 22.360678661266945
Epoch 2109/10000, Prediction Accuracy = 51.226%, Loss = 1.2229676961898803
Epoch: 2109, Batch Gradient Norm: 42.92138403058661
Epoch: 2109, Batch Gradient Norm after: 22.36067611149055
Epoch 2110/10000, Prediction Accuracy = 51.178%, Loss = 1.232235836982727
Epoch: 2110, Batch Gradient Norm: 39.61987790817891
Epoch: 2110, Batch Gradient Norm after: 22.36067824760575
Epoch 2111/10000, Prediction Accuracy = 51.248000000000005%, Loss = 1.2224884986877442
Epoch: 2111, Batch Gradient Norm: 42.898682300772975
Epoch: 2111, Batch Gradient Norm after: 22.360677314566345
Epoch 2112/10000, Prediction Accuracy = 51.178%, Loss = 1.2316756010055543
Epoch: 2112, Batch Gradient Norm: 39.620109337151185
Epoch: 2112, Batch Gradient Norm after: 22.360678113714002
Epoch 2113/10000, Prediction Accuracy = 51.257999999999996%, Loss = 1.2219967126846314
Epoch: 2113, Batch Gradient Norm: 42.87863365646806
Epoch: 2113, Batch Gradient Norm after: 22.360676849957674
Epoch 2114/10000, Prediction Accuracy = 51.182%, Loss = 1.231144332885742
Epoch: 2114, Batch Gradient Norm: 39.62078176126675
Epoch: 2114, Batch Gradient Norm after: 22.360675397463165
Epoch 2115/10000, Prediction Accuracy = 51.254%, Loss = 1.2215112447738647
Epoch: 2115, Batch Gradient Norm: 42.862473804720686
Epoch: 2115, Batch Gradient Norm after: 22.360678248616825
Epoch 2116/10000, Prediction Accuracy = 51.188%, Loss = 1.2305985689163208
Epoch: 2116, Batch Gradient Norm: 39.62212016277868
Epoch: 2116, Batch Gradient Norm after: 22.360676445743085
Epoch 2117/10000, Prediction Accuracy = 51.267999999999994%, Loss = 1.2210298299789428
Epoch: 2117, Batch Gradient Norm: 42.84117943143813
Epoch: 2117, Batch Gradient Norm after: 22.36067878431775
Epoch 2118/10000, Prediction Accuracy = 51.196%, Loss = 1.2300499200820922
Epoch: 2118, Batch Gradient Norm: 39.623128246223196
Epoch: 2118, Batch Gradient Norm after: 22.36067649407436
Epoch 2119/10000, Prediction Accuracy = 51.26400000000001%, Loss = 1.2205430746078492
Epoch: 2119, Batch Gradient Norm: 42.82710381247591
Epoch: 2119, Batch Gradient Norm after: 22.360677099748617
Epoch 2120/10000, Prediction Accuracy = 51.20399999999999%, Loss = 1.2295134305953979
Epoch: 2120, Batch Gradient Norm: 39.623328153425376
Epoch: 2120, Batch Gradient Norm after: 22.3606777800296
Epoch 2121/10000, Prediction Accuracy = 51.27%, Loss = 1.2200529336929322
Epoch: 2121, Batch Gradient Norm: 42.810302470856634
Epoch: 2121, Batch Gradient Norm after: 22.36067727809957
Epoch 2122/10000, Prediction Accuracy = 51.208000000000006%, Loss = 1.2289644956588746
Epoch: 2122, Batch Gradient Norm: 39.624225281421765
Epoch: 2122, Batch Gradient Norm after: 22.360676749233313
Epoch 2123/10000, Prediction Accuracy = 51.294000000000004%, Loss = 1.2195649147033691
Epoch: 2123, Batch Gradient Norm: 42.790247246513474
Epoch: 2123, Batch Gradient Norm after: 22.360676537514202
Epoch 2124/10000, Prediction Accuracy = 51.220000000000006%, Loss = 1.2284164667129516
Epoch: 2124, Batch Gradient Norm: 39.62350479726549
Epoch: 2124, Batch Gradient Norm after: 22.36067563224357
Epoch 2125/10000, Prediction Accuracy = 51.31%, Loss = 1.2190802574157715
Epoch: 2125, Batch Gradient Norm: 42.768760544742484
Epoch: 2125, Batch Gradient Norm after: 22.360678742289647
Epoch 2126/10000, Prediction Accuracy = 51.24400000000001%, Loss = 1.2278683662414551
Epoch: 2126, Batch Gradient Norm: 39.62523336034588
Epoch: 2126, Batch Gradient Norm after: 22.360674726719168
Epoch 2127/10000, Prediction Accuracy = 51.29999999999999%, Loss = 1.2186055421829223
Epoch: 2127, Batch Gradient Norm: 42.7507221863091
Epoch: 2127, Batch Gradient Norm after: 22.360678981452374
Epoch 2128/10000, Prediction Accuracy = 51.25600000000001%, Loss = 1.227329134941101
Epoch: 2128, Batch Gradient Norm: 39.62436964235705
Epoch: 2128, Batch Gradient Norm after: 22.36067732785464
Epoch 2129/10000, Prediction Accuracy = 51.3%, Loss = 1.218122386932373
Epoch: 2129, Batch Gradient Norm: 42.73207450991578
Epoch: 2129, Batch Gradient Norm after: 22.360677681268047
Epoch 2130/10000, Prediction Accuracy = 51.272000000000006%, Loss = 1.2267925500869752
Epoch: 2130, Batch Gradient Norm: 39.625180719565904
Epoch: 2130, Batch Gradient Norm after: 22.36067827088253
Epoch 2131/10000, Prediction Accuracy = 51.312%, Loss = 1.217637801170349
Epoch: 2131, Batch Gradient Norm: 42.71841331234568
Epoch: 2131, Batch Gradient Norm after: 22.360677016496847
Epoch 2132/10000, Prediction Accuracy = 51.290000000000006%, Loss = 1.2262665033340454
Epoch: 2132, Batch Gradient Norm: 39.62689788078988
Epoch: 2132, Batch Gradient Norm after: 22.36067752937331
Epoch 2133/10000, Prediction Accuracy = 51.315999999999995%, Loss = 1.2171514987945558
Epoch: 2133, Batch Gradient Norm: 42.710524868981054
Epoch: 2133, Batch Gradient Norm after: 22.36067905506461
Epoch 2134/10000, Prediction Accuracy = 51.306000000000004%, Loss = 1.2257640838623047
Epoch: 2134, Batch Gradient Norm: 39.62466602992315
Epoch: 2134, Batch Gradient Norm after: 22.360675073205982
Epoch 2135/10000, Prediction Accuracy = 51.33%, Loss = 1.2166646003723145
Epoch: 2135, Batch Gradient Norm: 42.70879912652568
Epoch: 2135, Batch Gradient Norm after: 22.36067815337436
Epoch 2136/10000, Prediction Accuracy = 51.318000000000005%, Loss = 1.225270962715149
Epoch: 2136, Batch Gradient Norm: 39.62421031589682
Epoch: 2136, Batch Gradient Norm after: 22.360676770743247
Epoch 2137/10000, Prediction Accuracy = 51.35%, Loss = 1.216182541847229
Epoch: 2137, Batch Gradient Norm: 42.706648877362944
Epoch: 2137, Batch Gradient Norm after: 22.360678116493123
Epoch 2138/10000, Prediction Accuracy = 51.32400000000001%, Loss = 1.224787998199463
Epoch: 2138, Batch Gradient Norm: 39.623140454861016
Epoch: 2138, Batch Gradient Norm after: 22.3606763304354
Epoch 2139/10000, Prediction Accuracy = 51.36800000000001%, Loss = 1.2156983852386474
Epoch: 2139, Batch Gradient Norm: 42.70006377562874
Epoch: 2139, Batch Gradient Norm after: 22.360679353259936
Epoch 2140/10000, Prediction Accuracy = 51.33%, Loss = 1.2242860317230224
Epoch: 2140, Batch Gradient Norm: 39.617500276678875
Epoch: 2140, Batch Gradient Norm after: 22.36067712156695
Epoch 2141/10000, Prediction Accuracy = 51.386%, Loss = 1.2152172088623048
Epoch: 2141, Batch Gradient Norm: 42.69454061900961
Epoch: 2141, Batch Gradient Norm after: 22.360678690814755
Epoch 2142/10000, Prediction Accuracy = 51.336%, Loss = 1.2237942457199096
Epoch: 2142, Batch Gradient Norm: 39.612594369707374
Epoch: 2142, Batch Gradient Norm after: 22.360676245028884
Epoch 2143/10000, Prediction Accuracy = 51.38199999999999%, Loss = 1.2147347688674928
Epoch: 2143, Batch Gradient Norm: 42.695986833016455
Epoch: 2143, Batch Gradient Norm after: 22.360675997096173
Epoch 2144/10000, Prediction Accuracy = 51.338%, Loss = 1.2233179807662964
Epoch: 2144, Batch Gradient Norm: 39.60753378904615
Epoch: 2144, Batch Gradient Norm after: 22.36067645566732
Epoch 2145/10000, Prediction Accuracy = 51.39%, Loss = 1.2142540216445923
Epoch: 2145, Batch Gradient Norm: 42.68775466618576
Epoch: 2145, Batch Gradient Norm after: 22.36067568360155
Epoch 2146/10000, Prediction Accuracy = 51.35%, Loss = 1.222832703590393
Epoch: 2146, Batch Gradient Norm: 39.60638173733561
Epoch: 2146, Batch Gradient Norm after: 22.360679651667812
Epoch 2147/10000, Prediction Accuracy = 51.403999999999996%, Loss = 1.213763689994812
Epoch: 2147, Batch Gradient Norm: 42.68149092925305
Epoch: 2147, Batch Gradient Norm after: 22.36067721609415
Epoch 2148/10000, Prediction Accuracy = 51.36%, Loss = 1.2223274946212768
Epoch: 2148, Batch Gradient Norm: 39.60039079753248
Epoch: 2148, Batch Gradient Norm after: 22.360676579972953
Epoch 2149/10000, Prediction Accuracy = 51.418000000000006%, Loss = 1.2132805109024047
Epoch: 2149, Batch Gradient Norm: 42.675430634293726
Epoch: 2149, Batch Gradient Norm after: 22.360677277313194
Epoch 2150/10000, Prediction Accuracy = 51.372%, Loss = 1.221838617324829
Epoch: 2150, Batch Gradient Norm: 39.59629728372331
Epoch: 2150, Batch Gradient Norm after: 22.360675709756666
Epoch 2151/10000, Prediction Accuracy = 51.428%, Loss = 1.2127987623214722
Epoch: 2151, Batch Gradient Norm: 42.66477876514792
Epoch: 2151, Batch Gradient Norm after: 22.36067874076689
Epoch 2152/10000, Prediction Accuracy = 51.384%, Loss = 1.2213273763656616
Epoch: 2152, Batch Gradient Norm: 39.59413644815922
Epoch: 2152, Batch Gradient Norm after: 22.360676749258477
Epoch 2153/10000, Prediction Accuracy = 51.438%, Loss = 1.2123185396194458
Epoch: 2153, Batch Gradient Norm: 42.65544596133865
Epoch: 2153, Batch Gradient Norm after: 22.360676033042076
Epoch 2154/10000, Prediction Accuracy = 51.414%, Loss = 1.2208303213119507
Epoch: 2154, Batch Gradient Norm: 39.591497860652105
Epoch: 2154, Batch Gradient Norm after: 22.360678550036788
Epoch 2155/10000, Prediction Accuracy = 51.434000000000005%, Loss = 1.211835217475891
Epoch: 2155, Batch Gradient Norm: 42.64793991431042
Epoch: 2155, Batch Gradient Norm after: 22.360676816853005
Epoch 2156/10000, Prediction Accuracy = 51.424%, Loss = 1.2203195571899415
Epoch: 2156, Batch Gradient Norm: 39.5904391952009
Epoch: 2156, Batch Gradient Norm after: 22.360676891193744
Epoch 2157/10000, Prediction Accuracy = 51.43399999999999%, Loss = 1.2113529443740845
Epoch: 2157, Batch Gradient Norm: 42.63374465779437
Epoch: 2157, Batch Gradient Norm after: 22.36067657446961
Epoch 2158/10000, Prediction Accuracy = 51.434000000000005%, Loss = 1.2197964429855346
Epoch: 2158, Batch Gradient Norm: 39.59098795742205
Epoch: 2158, Batch Gradient Norm after: 22.360675090982767
Epoch 2159/10000, Prediction Accuracy = 51.448%, Loss = 1.2108713865280152
Epoch: 2159, Batch Gradient Norm: 42.61709634387842
Epoch: 2159, Batch Gradient Norm after: 22.360677323749442
Epoch 2160/10000, Prediction Accuracy = 51.44%, Loss = 1.2192691326141358
Epoch: 2160, Batch Gradient Norm: 39.58756913488933
Epoch: 2160, Batch Gradient Norm after: 22.360676962828
Epoch 2161/10000, Prediction Accuracy = 51.462%, Loss = 1.2103950023651122
Epoch: 2161, Batch Gradient Norm: 42.60665187973737
Epoch: 2161, Batch Gradient Norm after: 22.36067720038853
Epoch 2162/10000, Prediction Accuracy = 51.45399999999999%, Loss = 1.2187736988067628
Epoch: 2162, Batch Gradient Norm: 39.585132886633055
Epoch: 2162, Batch Gradient Norm after: 22.360674792306558
Epoch 2163/10000, Prediction Accuracy = 51.480000000000004%, Loss = 1.2099111795425415
Epoch: 2163, Batch Gradient Norm: 42.587810117271694
Epoch: 2163, Batch Gradient Norm after: 22.36067781681908
Epoch 2164/10000, Prediction Accuracy = 51.45799999999999%, Loss = 1.2182521104812623
Epoch: 2164, Batch Gradient Norm: 39.58662452203863
Epoch: 2164, Batch Gradient Norm after: 22.3606772203337
Epoch 2165/10000, Prediction Accuracy = 51.501999999999995%, Loss = 1.20943284034729
Epoch: 2165, Batch Gradient Norm: 42.57039704486983
Epoch: 2165, Batch Gradient Norm after: 22.360677593609893
Epoch 2166/10000, Prediction Accuracy = 51.474000000000004%, Loss = 1.2177272319793702
Epoch: 2166, Batch Gradient Norm: 39.58322489596979
Epoch: 2166, Batch Gradient Norm after: 22.36067666311804
Epoch 2167/10000, Prediction Accuracy = 51.5%, Loss = 1.2089585781097412
Epoch: 2167, Batch Gradient Norm: 42.545246283079116
Epoch: 2167, Batch Gradient Norm after: 22.36067682809285
Epoch 2168/10000, Prediction Accuracy = 51.489999999999995%, Loss = 1.217184853553772
Epoch: 2168, Batch Gradient Norm: 39.58489423791491
Epoch: 2168, Batch Gradient Norm after: 22.36067638500013
Epoch 2169/10000, Prediction Accuracy = 51.49399999999999%, Loss = 1.208493185043335
Epoch: 2169, Batch Gradient Norm: 42.519758567396075
Epoch: 2169, Batch Gradient Norm after: 22.360677020278136
Epoch 2170/10000, Prediction Accuracy = 51.512%, Loss = 1.2166492700576783
Epoch: 2170, Batch Gradient Norm: 39.58374246424463
Epoch: 2170, Batch Gradient Norm after: 22.36067838180393
Epoch 2171/10000, Prediction Accuracy = 51.508%, Loss = 1.2080204963684082
Epoch: 2171, Batch Gradient Norm: 42.49746573261787
Epoch: 2171, Batch Gradient Norm after: 22.360675820473205
Epoch 2172/10000, Prediction Accuracy = 51.548%, Loss = 1.2161161422729492
Epoch: 2172, Batch Gradient Norm: 39.58757508535233
Epoch: 2172, Batch Gradient Norm after: 22.36067442707393
Epoch 2173/10000, Prediction Accuracy = 51.525999999999996%, Loss = 1.2075430393218993
Epoch: 2173, Batch Gradient Norm: 42.46766641663112
Epoch: 2173, Batch Gradient Norm after: 22.36067475659693
Epoch 2174/10000, Prediction Accuracy = 51.55%, Loss = 1.2155692100524902
Epoch: 2174, Batch Gradient Norm: 39.59080891952788
Epoch: 2174, Batch Gradient Norm after: 22.36067605343256
Epoch 2175/10000, Prediction Accuracy = 51.54%, Loss = 1.2070853948593139
Epoch: 2175, Batch Gradient Norm: 42.435258570498
Epoch: 2175, Batch Gradient Norm after: 22.360675987835464
Epoch 2176/10000, Prediction Accuracy = 51.565999999999995%, Loss = 1.2149989128112793
Epoch: 2176, Batch Gradient Norm: 39.59637190522078
Epoch: 2176, Batch Gradient Norm after: 22.36067564523817
Epoch 2177/10000, Prediction Accuracy = 51.552%, Loss = 1.206620740890503
Epoch: 2177, Batch Gradient Norm: 42.41154621650974
Epoch: 2177, Batch Gradient Norm after: 22.360675946355737
Epoch 2178/10000, Prediction Accuracy = 51.568000000000005%, Loss = 1.2144577503204346
Epoch: 2178, Batch Gradient Norm: 39.595935900417224
Epoch: 2178, Batch Gradient Norm after: 22.36067438472627
Epoch 2179/10000, Prediction Accuracy = 51.55799999999999%, Loss = 1.2061566352844237
Epoch: 2179, Batch Gradient Norm: 42.39059061366272
Epoch: 2179, Batch Gradient Norm after: 22.36067518696939
Epoch 2180/10000, Prediction Accuracy = 51.568%, Loss = 1.213927936553955
Epoch: 2180, Batch Gradient Norm: 39.5959721490555
Epoch: 2180, Batch Gradient Norm after: 22.360674702881255
Epoch 2181/10000, Prediction Accuracy = 51.581999999999994%, Loss = 1.2056891202926636
Epoch: 2181, Batch Gradient Norm: 42.371257996085845
Epoch: 2181, Batch Gradient Norm after: 22.360675407317267
Epoch 2182/10000, Prediction Accuracy = 51.584%, Loss = 1.2134129524230957
Epoch: 2182, Batch Gradient Norm: 39.59630661871344
Epoch: 2182, Batch Gradient Norm after: 22.360675836005164
Epoch 2183/10000, Prediction Accuracy = 51.59400000000001%, Loss = 1.2052231311798096
Epoch: 2183, Batch Gradient Norm: 42.350565097522214
Epoch: 2183, Batch Gradient Norm after: 22.360675757178466
Epoch 2184/10000, Prediction Accuracy = 51.58%, Loss = 1.2128830432891846
Epoch: 2184, Batch Gradient Norm: 39.59489864338892
Epoch: 2184, Batch Gradient Norm after: 22.360675921694597
Epoch 2185/10000, Prediction Accuracy = 51.604%, Loss = 1.204754400253296
Epoch: 2185, Batch Gradient Norm: 42.33201255679226
Epoch: 2185, Batch Gradient Norm after: 22.36067464135691
Epoch 2186/10000, Prediction Accuracy = 51.592%, Loss = 1.2123688459396362
Epoch: 2186, Batch Gradient Norm: 39.5969420536982
Epoch: 2186, Batch Gradient Norm after: 22.360677400512923
Epoch 2187/10000, Prediction Accuracy = 51.602%, Loss = 1.2042904853820802
Epoch: 2187, Batch Gradient Norm: 42.309005107825946
Epoch: 2187, Batch Gradient Norm after: 22.360673945250703
Epoch 2188/10000, Prediction Accuracy = 51.60600000000001%, Loss = 1.211842918395996
Epoch: 2188, Batch Gradient Norm: 39.59807872579276
Epoch: 2188, Batch Gradient Norm after: 22.36067766297799
Epoch 2189/10000, Prediction Accuracy = 51.61199999999999%, Loss = 1.2038273334503173
Epoch: 2189, Batch Gradient Norm: 42.282891817624225
Epoch: 2189, Batch Gradient Norm after: 22.36067596380517
Epoch 2190/10000, Prediction Accuracy = 51.602%, Loss = 1.2113235950469972
Epoch: 2190, Batch Gradient Norm: 39.59532645593774
Epoch: 2190, Batch Gradient Norm after: 22.360677673527544
Epoch 2191/10000, Prediction Accuracy = 51.61600000000001%, Loss = 1.2033721923828125
Epoch: 2191, Batch Gradient Norm: 42.26554498870914
Epoch: 2191, Batch Gradient Norm after: 22.360676347788292
Epoch 2192/10000, Prediction Accuracy = 51.622%, Loss = 1.210815978050232
Epoch: 2192, Batch Gradient Norm: 39.5939544483018
Epoch: 2192, Batch Gradient Norm after: 22.360678707993433
Epoch 2193/10000, Prediction Accuracy = 51.616%, Loss = 1.2029114246368409
Epoch: 2193, Batch Gradient Norm: 42.24445933315456
Epoch: 2193, Batch Gradient Norm after: 22.36067736055256
Epoch 2194/10000, Prediction Accuracy = 51.632000000000005%, Loss = 1.2102999687194824
Epoch: 2194, Batch Gradient Norm: 39.59543079075773
Epoch: 2194, Batch Gradient Norm after: 22.36067535554482
Epoch 2195/10000, Prediction Accuracy = 51.632000000000005%, Loss = 1.2024551153182983
Epoch: 2195, Batch Gradient Norm: 42.22488749509185
Epoch: 2195, Batch Gradient Norm after: 22.360677357470514
Epoch 2196/10000, Prediction Accuracy = 51.63000000000001%, Loss = 1.2097999334335328
Epoch: 2196, Batch Gradient Norm: 39.59425863431178
Epoch: 2196, Batch Gradient Norm after: 22.360678954473943
Epoch 2197/10000, Prediction Accuracy = 51.629999999999995%, Loss = 1.2019980430603028
Epoch: 2197, Batch Gradient Norm: 42.206092322093355
Epoch: 2197, Batch Gradient Norm after: 22.360677502260735
Epoch 2198/10000, Prediction Accuracy = 51.641999999999996%, Loss = 1.2092882156372071
Epoch: 2198, Batch Gradient Norm: 39.592123402942754
Epoch: 2198, Batch Gradient Norm after: 22.36067847634735
Epoch 2199/10000, Prediction Accuracy = 51.638%, Loss = 1.2015350580215454
Epoch: 2199, Batch Gradient Norm: 42.187084206470445
Epoch: 2199, Batch Gradient Norm after: 22.360679079446868
Epoch 2200/10000, Prediction Accuracy = 51.65599999999999%, Loss = 1.2087748050689697
Epoch: 2200, Batch Gradient Norm: 39.5898573725064
Epoch: 2200, Batch Gradient Norm after: 22.360679099940864
Epoch 2201/10000, Prediction Accuracy = 51.662%, Loss = 1.201076602935791
Epoch: 2201, Batch Gradient Norm: 42.17216814802199
Epoch: 2201, Batch Gradient Norm after: 22.36067775776345
Epoch 2202/10000, Prediction Accuracy = 51.668000000000006%, Loss = 1.2082708835601808
Epoch: 2202, Batch Gradient Norm: 39.588421150474446
Epoch: 2202, Batch Gradient Norm after: 22.36068061091409
Epoch 2203/10000, Prediction Accuracy = 51.668000000000006%, Loss = 1.2006128787994386
Epoch: 2203, Batch Gradient Norm: 42.16565792023491
Epoch: 2203, Batch Gradient Norm after: 22.36067799494127
Epoch 2204/10000, Prediction Accuracy = 51.676%, Loss = 1.2077804327011108
Epoch: 2204, Batch Gradient Norm: 39.584175636163565
Epoch: 2204, Batch Gradient Norm after: 22.360676325629033
Epoch 2205/10000, Prediction Accuracy = 51.67999999999999%, Loss = 1.2001507759094239
Epoch: 2205, Batch Gradient Norm: 42.15706349187259
Epoch: 2205, Batch Gradient Norm after: 22.360677750447742
Epoch 2206/10000, Prediction Accuracy = 51.678%, Loss = 1.2073020219802857
Epoch: 2206, Batch Gradient Norm: 39.57992706483189
Epoch: 2206, Batch Gradient Norm after: 22.360680543276427
Epoch 2207/10000, Prediction Accuracy = 51.698%, Loss = 1.1996870756149292
Epoch: 2207, Batch Gradient Norm: 42.14836867019381
Epoch: 2207, Batch Gradient Norm after: 22.36067798982994
Epoch 2208/10000, Prediction Accuracy = 51.7%, Loss = 1.2068297386169433
Epoch: 2208, Batch Gradient Norm: 39.57494352455664
Epoch: 2208, Batch Gradient Norm after: 22.36067878898933
Epoch 2209/10000, Prediction Accuracy = 51.70799999999999%, Loss = 1.1992227554321289
Epoch: 2209, Batch Gradient Norm: 42.140305306195174
Epoch: 2209, Batch Gradient Norm after: 22.36067724248742
Epoch 2210/10000, Prediction Accuracy = 51.724000000000004%, Loss = 1.2063506603240968
Epoch: 2210, Batch Gradient Norm: 39.57057614832731
Epoch: 2210, Batch Gradient Norm after: 22.36068012545223
Epoch 2211/10000, Prediction Accuracy = 51.702%, Loss = 1.198758840560913
Epoch: 2211, Batch Gradient Norm: 42.132975013356486
Epoch: 2211, Batch Gradient Norm after: 22.360677791377142
Epoch 2212/10000, Prediction Accuracy = 51.726%, Loss = 1.2058868169784547
Epoch: 2212, Batch Gradient Norm: 39.56652597189832
Epoch: 2212, Batch Gradient Norm after: 22.360680092336406
Epoch 2213/10000, Prediction Accuracy = 51.7%, Loss = 1.1982938528060914
Epoch: 2213, Batch Gradient Norm: 42.12218300527264
Epoch: 2213, Batch Gradient Norm after: 22.36067775555245
Epoch 2214/10000, Prediction Accuracy = 51.734%, Loss = 1.2054070472717284
Epoch: 2214, Batch Gradient Norm: 39.5637309701605
Epoch: 2214, Batch Gradient Norm after: 22.360680094830474
Epoch 2215/10000, Prediction Accuracy = 51.705999999999996%, Loss = 1.1978323459625244
Epoch: 2215, Batch Gradient Norm: 42.11511287487185
Epoch: 2215, Batch Gradient Norm after: 22.360677176134505
Epoch 2216/10000, Prediction Accuracy = 51.748000000000005%, Loss = 1.2049339532852172
Epoch: 2216, Batch Gradient Norm: 39.5593086143397
Epoch: 2216, Batch Gradient Norm after: 22.360680637556865
Epoch 2217/10000, Prediction Accuracy = 51.71999999999999%, Loss = 1.197368335723877
Epoch: 2217, Batch Gradient Norm: 42.109849405663766
Epoch: 2217, Batch Gradient Norm after: 22.360677911036714
Epoch 2218/10000, Prediction Accuracy = 51.760000000000005%, Loss = 1.2044669389724731
Epoch: 2218, Batch Gradient Norm: 39.55382724022006
Epoch: 2218, Batch Gradient Norm after: 22.360680671573288
Epoch 2219/10000, Prediction Accuracy = 51.726%, Loss = 1.196909761428833
Epoch: 2219, Batch Gradient Norm: 42.11019369729842
Epoch: 2219, Batch Gradient Norm after: 22.360678296580573
Epoch 2220/10000, Prediction Accuracy = 51.772000000000006%, Loss = 1.2040162324905395
Epoch: 2220, Batch Gradient Norm: 39.54631127801448
Epoch: 2220, Batch Gradient Norm after: 22.360678617970603
Epoch 2221/10000, Prediction Accuracy = 51.717999999999996%, Loss = 1.1964452743530274
Epoch: 2221, Batch Gradient Norm: 42.11599736823779
Epoch: 2221, Batch Gradient Norm after: 22.36067939327352
Epoch 2222/10000, Prediction Accuracy = 51.782%, Loss = 1.203584861755371
Epoch: 2222, Batch Gradient Norm: 39.538574621251314
Epoch: 2222, Batch Gradient Norm after: 22.360679804678718
Epoch 2223/10000, Prediction Accuracy = 51.73%, Loss = 1.1959746599197387
Epoch: 2223, Batch Gradient Norm: 42.12063238981042
Epoch: 2223, Batch Gradient Norm after: 22.360677690095837
Epoch 2224/10000, Prediction Accuracy = 51.784000000000006%, Loss = 1.203140091896057
Epoch: 2224, Batch Gradient Norm: 39.53370489313626
Epoch: 2224, Batch Gradient Norm after: 22.360678355641287
Epoch 2225/10000, Prediction Accuracy = 51.736000000000004%, Loss = 1.195505952835083
Epoch: 2225, Batch Gradient Norm: 42.122791712583016
Epoch: 2225, Batch Gradient Norm after: 22.36067757841827
Epoch 2226/10000, Prediction Accuracy = 51.794%, Loss = 1.202703833580017
Epoch: 2226, Batch Gradient Norm: 39.52532492470847
Epoch: 2226, Batch Gradient Norm after: 22.36067735500606
Epoch 2227/10000, Prediction Accuracy = 51.75599999999999%, Loss = 1.1950354814529418
Epoch: 2227, Batch Gradient Norm: 42.121235907852096
Epoch: 2227, Batch Gradient Norm after: 22.360677669143698
Epoch 2228/10000, Prediction Accuracy = 51.798%, Loss = 1.2022632598876952
Epoch: 2228, Batch Gradient Norm: 39.518942011391324
Epoch: 2228, Batch Gradient Norm after: 22.360677810726973
Epoch 2229/10000, Prediction Accuracy = 51.77199999999999%, Loss = 1.194571566581726
Epoch: 2229, Batch Gradient Norm: 42.11931978271788
Epoch: 2229, Batch Gradient Norm after: 22.36067638586061
Epoch 2230/10000, Prediction Accuracy = 51.8%, Loss = 1.2018089771270752
Epoch: 2230, Batch Gradient Norm: 39.51374152472764
Epoch: 2230, Batch Gradient Norm after: 22.360677686387923
Epoch 2231/10000, Prediction Accuracy = 51.784000000000006%, Loss = 1.194102692604065
Epoch: 2231, Batch Gradient Norm: 42.115324232851904
Epoch: 2231, Batch Gradient Norm after: 22.36067571406536
Epoch 2232/10000, Prediction Accuracy = 51.803999999999995%, Loss = 1.201351237297058
Epoch: 2232, Batch Gradient Norm: 39.508001270357944
Epoch: 2232, Batch Gradient Norm after: 22.360678309007497
Epoch 2233/10000, Prediction Accuracy = 51.786%, Loss = 1.1936400175094604
Epoch: 2233, Batch Gradient Norm: 42.10830101586399
Epoch: 2233, Batch Gradient Norm after: 22.36067798301427
Epoch 2234/10000, Prediction Accuracy = 51.80799999999999%, Loss = 1.20088050365448
Epoch: 2234, Batch Gradient Norm: 39.504668317198316
Epoch: 2234, Batch Gradient Norm after: 22.360678979111444
Epoch 2235/10000, Prediction Accuracy = 51.79%, Loss = 1.1931838274002076
Epoch: 2235, Batch Gradient Norm: 42.099541520528916
Epoch: 2235, Batch Gradient Norm after: 22.360679111947462
Epoch 2236/10000, Prediction Accuracy = 51.818%, Loss = 1.200398325920105
Epoch: 2236, Batch Gradient Norm: 39.501014742263706
Epoch: 2236, Batch Gradient Norm after: 22.360678308643013
Epoch 2237/10000, Prediction Accuracy = 51.794000000000004%, Loss = 1.1927189350128173
Epoch: 2237, Batch Gradient Norm: 42.092952910603884
Epoch: 2237, Batch Gradient Norm after: 22.360676969145842
Epoch 2238/10000, Prediction Accuracy = 51.831999999999994%, Loss = 1.1999215841293336
Epoch: 2238, Batch Gradient Norm: 39.49390394846283
Epoch: 2238, Batch Gradient Norm after: 22.360676536717634
Epoch 2239/10000, Prediction Accuracy = 51.812%, Loss = 1.1922629594802856
Epoch: 2239, Batch Gradient Norm: 42.079277071146336
Epoch: 2239, Batch Gradient Norm after: 22.360678632836823
Epoch 2240/10000, Prediction Accuracy = 51.864%, Loss = 1.1994371652603149
Epoch: 2240, Batch Gradient Norm: 39.490673604546494
Epoch: 2240, Batch Gradient Norm after: 22.360677758126194
Epoch 2241/10000, Prediction Accuracy = 51.822%, Loss = 1.1918020009994508
Epoch: 2241, Batch Gradient Norm: 42.06399432418806
Epoch: 2241, Batch Gradient Norm after: 22.360678320671088
Epoch 2242/10000, Prediction Accuracy = 51.85600000000001%, Loss = 1.198944354057312
Epoch: 2242, Batch Gradient Norm: 39.489005911739575
Epoch: 2242, Batch Gradient Norm after: 22.36067908873935
Epoch 2243/10000, Prediction Accuracy = 51.834%, Loss = 1.1913420677185058
Epoch: 2243, Batch Gradient Norm: 42.05011858719117
Epoch: 2243, Batch Gradient Norm after: 22.360677671980827
Epoch 2244/10000, Prediction Accuracy = 51.866%, Loss = 1.1984578371047974
Epoch: 2244, Batch Gradient Norm: 39.48385318585507
Epoch: 2244, Batch Gradient Norm after: 22.36067627381923
Epoch 2245/10000, Prediction Accuracy = 51.836%, Loss = 1.190880012512207
Epoch: 2245, Batch Gradient Norm: 42.043709376644074
Epoch: 2245, Batch Gradient Norm after: 22.36067690752403
Epoch 2246/10000, Prediction Accuracy = 51.872%, Loss = 1.197982120513916
Epoch: 2246, Batch Gradient Norm: 39.47566635486091
Epoch: 2246, Batch Gradient Norm after: 22.360678372540836
Epoch 2247/10000, Prediction Accuracy = 51.843999999999994%, Loss = 1.1904221057891846
Epoch: 2247, Batch Gradient Norm: 42.04239519838082
Epoch: 2247, Batch Gradient Norm after: 22.360678732110227
Epoch 2248/10000, Prediction Accuracy = 51.89%, Loss = 1.197540545463562
Epoch: 2248, Batch Gradient Norm: 39.46885134481865
Epoch: 2248, Batch Gradient Norm after: 22.360675522185772
Epoch 2249/10000, Prediction Accuracy = 51.86%, Loss = 1.189957618713379
Epoch: 2249, Batch Gradient Norm: 42.04491029082409
Epoch: 2249, Batch Gradient Norm after: 22.360678033658463
Epoch 2250/10000, Prediction Accuracy = 51.895999999999994%, Loss = 1.1970956802368165
Epoch: 2250, Batch Gradient Norm: 39.461002312686304
Epoch: 2250, Batch Gradient Norm after: 22.36067787761331
Epoch 2251/10000, Prediction Accuracy = 51.884%, Loss = 1.1894953012466432
Epoch: 2251, Batch Gradient Norm: 42.05153774664247
Epoch: 2251, Batch Gradient Norm after: 22.36067615505333
Epoch 2252/10000, Prediction Accuracy = 51.908%, Loss = 1.1966668605804442
Epoch: 2252, Batch Gradient Norm: 39.45146944125371
Epoch: 2252, Batch Gradient Norm after: 22.360679798657927
Epoch 2253/10000, Prediction Accuracy = 51.903999999999996%, Loss = 1.189029574394226
Epoch: 2253, Batch Gradient Norm: 42.056236225591604
Epoch: 2253, Batch Gradient Norm after: 22.360677463539993
Epoch 2254/10000, Prediction Accuracy = 51.92%, Loss = 1.1962430000305175
Epoch: 2254, Batch Gradient Norm: 39.44298141593105
Epoch: 2254, Batch Gradient Norm after: 22.3606767309815
Epoch 2255/10000, Prediction Accuracy = 51.928%, Loss = 1.1885641813278198
Epoch: 2255, Batch Gradient Norm: 42.05891014685655
Epoch: 2255, Batch Gradient Norm after: 22.360677128660733
Epoch 2256/10000, Prediction Accuracy = 51.928%, Loss = 1.1958139896392823
Epoch: 2256, Batch Gradient Norm: 39.4309159736526
Epoch: 2256, Batch Gradient Norm after: 22.360678019369796
Epoch 2257/10000, Prediction Accuracy = 51.92999999999999%, Loss = 1.1881045818328857
Epoch: 2257, Batch Gradient Norm: 42.062835762622655
Epoch: 2257, Batch Gradient Norm after: 22.360677131389238
Epoch 2258/10000, Prediction Accuracy = 51.931999999999995%, Loss = 1.1953885793685912
Epoch: 2258, Batch Gradient Norm: 39.42448137805093
Epoch: 2258, Batch Gradient Norm after: 22.36067880488673
Epoch 2259/10000, Prediction Accuracy = 51.92999999999999%, Loss = 1.1876381158828735
Epoch: 2259, Batch Gradient Norm: 42.05918836482164
Epoch: 2259, Batch Gradient Norm after: 22.360677019080775
Epoch 2260/10000, Prediction Accuracy = 51.948%, Loss = 1.1949438095092773
Epoch: 2260, Batch Gradient Norm: 39.41626717201758
Epoch: 2260, Batch Gradient Norm after: 22.360681505336313
Epoch 2261/10000, Prediction Accuracy = 51.95%, Loss = 1.1871803998947144
Epoch: 2261, Batch Gradient Norm: 42.05982097305967
Epoch: 2261, Batch Gradient Norm after: 22.36067986971327
Epoch 2262/10000, Prediction Accuracy = 51.952%, Loss = 1.1944967031478881
Epoch: 2262, Batch Gradient Norm: 39.40981629011278
Epoch: 2262, Batch Gradient Norm after: 22.36067750894551
Epoch 2263/10000, Prediction Accuracy = 51.959999999999994%, Loss = 1.1867231845855712
Epoch: 2263, Batch Gradient Norm: 42.058130696223444
Epoch: 2263, Batch Gradient Norm after: 22.36067749069636
Epoch 2264/10000, Prediction Accuracy = 51.95399999999999%, Loss = 1.1940633296966552
Epoch: 2264, Batch Gradient Norm: 39.40105715314264
Epoch: 2264, Batch Gradient Norm after: 22.360677087981735
Epoch 2265/10000, Prediction Accuracy = 51.976%, Loss = 1.1862593650817872
Epoch: 2265, Batch Gradient Norm: 42.060022601490374
Epoch: 2265, Batch Gradient Norm after: 22.360676818414998
Epoch 2266/10000, Prediction Accuracy = 51.964%, Loss = 1.19363694190979
Epoch: 2266, Batch Gradient Norm: 39.393090701920435
Epoch: 2266, Batch Gradient Norm after: 22.360678518035822
Epoch 2267/10000, Prediction Accuracy = 51.967999999999996%, Loss = 1.1857981443405152
Epoch: 2267, Batch Gradient Norm: 42.06719451217783
Epoch: 2267, Batch Gradient Norm after: 22.360676286048516
Epoch 2268/10000, Prediction Accuracy = 51.956%, Loss = 1.1932189464569092
Epoch: 2268, Batch Gradient Norm: 39.384313407375416
Epoch: 2268, Batch Gradient Norm after: 22.360680683844095
Epoch 2269/10000, Prediction Accuracy = 51.98%, Loss = 1.1853355646133423
Epoch: 2269, Batch Gradient Norm: 42.070001109394916
Epoch: 2269, Batch Gradient Norm after: 22.360676912417766
Epoch 2270/10000, Prediction Accuracy = 51.971999999999994%, Loss = 1.1927874088287354
Epoch: 2270, Batch Gradient Norm: 39.37648452717629
Epoch: 2270, Batch Gradient Norm after: 22.360680165573342
Epoch 2271/10000, Prediction Accuracy = 51.992000000000004%, Loss = 1.1848737478256226
Epoch: 2271, Batch Gradient Norm: 42.06505732932014
Epoch: 2271, Batch Gradient Norm after: 22.36067819170459
Epoch 2272/10000, Prediction Accuracy = 51.977999999999994%, Loss = 1.192339038848877
Epoch: 2272, Batch Gradient Norm: 39.36967733911145
Epoch: 2272, Batch Gradient Norm after: 22.36067978352286
Epoch 2273/10000, Prediction Accuracy = 51.99799999999999%, Loss = 1.184413528442383
Epoch: 2273, Batch Gradient Norm: 42.068373894765664
Epoch: 2273, Batch Gradient Norm after: 22.360675298700066
Epoch 2274/10000, Prediction Accuracy = 51.989999999999995%, Loss = 1.1918925762176513
Epoch: 2274, Batch Gradient Norm: 39.36365068749207
Epoch: 2274, Batch Gradient Norm after: 22.36067744965566
Epoch 2275/10000, Prediction Accuracy = 52.025999999999996%, Loss = 1.1839607954025269
Epoch: 2275, Batch Gradient Norm: 42.073299964602064
Epoch: 2275, Batch Gradient Norm after: 22.360676056773375
Epoch 2276/10000, Prediction Accuracy = 51.989999999999995%, Loss = 1.1914701223373414
Epoch: 2276, Batch Gradient Norm: 39.353881829960365
Epoch: 2276, Batch Gradient Norm after: 22.360679998917753
Epoch 2277/10000, Prediction Accuracy = 52.04200000000001%, Loss = 1.1835076808929443
Epoch: 2277, Batch Gradient Norm: 42.07187436355038
Epoch: 2277, Batch Gradient Norm after: 22.360676847166015
Epoch 2278/10000, Prediction Accuracy = 51.986000000000004%, Loss = 1.1910350322723389
Epoch: 2278, Batch Gradient Norm: 39.34487984844301
Epoch: 2278, Batch Gradient Norm after: 22.360678574586213
Epoch 2279/10000, Prediction Accuracy = 52.05999999999999%, Loss = 1.1830533742904663
Epoch: 2279, Batch Gradient Norm: 42.06989037395753
Epoch: 2279, Batch Gradient Norm after: 22.36067646517024
Epoch 2280/10000, Prediction Accuracy = 52.004%, Loss = 1.190593409538269
Epoch: 2280, Batch Gradient Norm: 39.339131795803596
Epoch: 2280, Batch Gradient Norm after: 22.360679786419073
Epoch 2281/10000, Prediction Accuracy = 52.05799999999999%, Loss = 1.1826043128967285
Epoch: 2281, Batch Gradient Norm: 42.0676991506084
Epoch: 2281, Batch Gradient Norm after: 22.360678968930383
Epoch 2282/10000, Prediction Accuracy = 52.022000000000006%, Loss = 1.1901529550552368
Epoch: 2282, Batch Gradient Norm: 39.327974747741195
Epoch: 2282, Batch Gradient Norm after: 22.36067807152308
Epoch 2283/10000, Prediction Accuracy = 52.072%, Loss = 1.1821496963500977
Epoch: 2283, Batch Gradient Norm: 42.074797594251685
Epoch: 2283, Batch Gradient Norm after: 22.36067969336605
Epoch 2284/10000, Prediction Accuracy = 52.032000000000004%, Loss = 1.1897394895553588
Epoch: 2284, Batch Gradient Norm: 39.31814970780487
Epoch: 2284, Batch Gradient Norm after: 22.360678658193674
Epoch 2285/10000, Prediction Accuracy = 52.088%, Loss = 1.1816902160644531
Epoch: 2285, Batch Gradient Norm: 42.08742613510372
Epoch: 2285, Batch Gradient Norm after: 22.36067670478265
Epoch 2286/10000, Prediction Accuracy = 52.038%, Loss = 1.1893388032913208
Epoch: 2286, Batch Gradient Norm: 39.304854653403275
Epoch: 2286, Batch Gradient Norm after: 22.360678841638922
Epoch 2287/10000, Prediction Accuracy = 52.08800000000001%, Loss = 1.1812308788299561
Epoch: 2287, Batch Gradient Norm: 42.101487037192825
Epoch: 2287, Batch Gradient Norm after: 22.36067815227818
Epoch 2288/10000, Prediction Accuracy = 52.038%, Loss = 1.1889442682266236
Epoch: 2288, Batch Gradient Norm: 39.29318874515384
Epoch: 2288, Batch Gradient Norm after: 22.36067825601079
Epoch 2289/10000, Prediction Accuracy = 52.09599999999999%, Loss = 1.1807701587677002
Epoch: 2289, Batch Gradient Norm: 42.120033086106424
Epoch: 2289, Batch Gradient Norm after: 22.360679241928047
Epoch 2290/10000, Prediction Accuracy = 52.062%, Loss = 1.1885671138763427
Epoch: 2290, Batch Gradient Norm: 39.278981120205145
Epoch: 2290, Batch Gradient Norm after: 22.3606774894763
Epoch 2291/10000, Prediction Accuracy = 52.104%, Loss = 1.1803100824356079
Epoch: 2291, Batch Gradient Norm: 42.13424555519644
Epoch: 2291, Batch Gradient Norm after: 22.360676366579785
Epoch 2292/10000, Prediction Accuracy = 52.068%, Loss = 1.1881716966629028
Epoch: 2292, Batch Gradient Norm: 39.26656719151404
Epoch: 2292, Batch Gradient Norm after: 22.36067780649862
Epoch 2293/10000, Prediction Accuracy = 52.116%, Loss = 1.17984881401062
Epoch: 2293, Batch Gradient Norm: 42.14715010726767
Epoch: 2293, Batch Gradient Norm after: 22.360678697430227
Epoch 2294/10000, Prediction Accuracy = 52.074%, Loss = 1.1877744436264037
Epoch: 2294, Batch Gradient Norm: 39.253172133583455
Epoch: 2294, Batch Gradient Norm after: 22.360677114216063
Epoch 2295/10000, Prediction Accuracy = 52.129999999999995%, Loss = 1.1793922185897827
Epoch: 2295, Batch Gradient Norm: 42.16255411629222
Epoch: 2295, Batch Gradient Norm after: 22.36067579136481
Epoch 2296/10000, Prediction Accuracy = 52.086%, Loss = 1.1873790979385377
Epoch: 2296, Batch Gradient Norm: 39.24372966915196
Epoch: 2296, Batch Gradient Norm after: 22.360679440268736
Epoch 2297/10000, Prediction Accuracy = 52.153999999999996%, Loss = 1.1789338350296021
Epoch: 2297, Batch Gradient Norm: 42.17185067693067
Epoch: 2297, Batch Gradient Norm after: 22.360677324731594
Epoch 2298/10000, Prediction Accuracy = 52.086%, Loss = 1.1869741201400756
Epoch: 2298, Batch Gradient Norm: 39.23684372263371
Epoch: 2298, Batch Gradient Norm after: 22.360677673708956
Epoch 2299/10000, Prediction Accuracy = 52.16799999999999%, Loss = 1.1784672260284423
Epoch: 2299, Batch Gradient Norm: 42.18708078725151
Epoch: 2299, Batch Gradient Norm after: 22.360675521674008
Epoch 2300/10000, Prediction Accuracy = 52.108000000000004%, Loss = 1.186589741706848
Epoch: 2300, Batch Gradient Norm: 39.22807696374728
Epoch: 2300, Batch Gradient Norm after: 22.36067654246632
Epoch 2301/10000, Prediction Accuracy = 52.19200000000001%, Loss = 1.1779979944229126
Epoch: 2301, Batch Gradient Norm: 42.20566994110208
Epoch: 2301, Batch Gradient Norm after: 22.360675398531217
Epoch 2302/10000, Prediction Accuracy = 52.117999999999995%, Loss = 1.18620126247406
Epoch: 2302, Batch Gradient Norm: 39.21960219956991
Epoch: 2302, Batch Gradient Norm after: 22.36067780924176
Epoch 2303/10000, Prediction Accuracy = 52.215999999999994%, Loss = 1.177532172203064
Epoch: 2303, Batch Gradient Norm: 42.21003639468504
Epoch: 2303, Batch Gradient Norm after: 22.36067800836086
Epoch 2304/10000, Prediction Accuracy = 52.112%, Loss = 1.1857931852340697
Epoch: 2304, Batch Gradient Norm: 39.212034556106694
Epoch: 2304, Batch Gradient Norm after: 22.360675283348826
Epoch 2305/10000, Prediction Accuracy = 52.226%, Loss = 1.177075743675232
Epoch: 2305, Batch Gradient Norm: 42.21513728542442
Epoch: 2305, Batch Gradient Norm after: 22.360678722756248
Epoch 2306/10000, Prediction Accuracy = 52.11800000000001%, Loss = 1.1853716611862182
Epoch: 2306, Batch Gradient Norm: 39.202398277859615
Epoch: 2306, Batch Gradient Norm after: 22.360676015356052
Epoch 2307/10000, Prediction Accuracy = 52.239999999999995%, Loss = 1.1766210794448853
Epoch: 2307, Batch Gradient Norm: 42.223931856792284
Epoch: 2307, Batch Gradient Norm after: 22.36067898003966
Epoch 2308/10000, Prediction Accuracy = 52.14%, Loss = 1.1849621772766112
Epoch: 2308, Batch Gradient Norm: 39.19383610697461
Epoch: 2308, Batch Gradient Norm after: 22.360677162487924
Epoch 2309/10000, Prediction Accuracy = 52.266%, Loss = 1.1761563062667846
Epoch: 2309, Batch Gradient Norm: 42.22441040887123
Epoch: 2309, Batch Gradient Norm after: 22.36067840165984
Epoch 2310/10000, Prediction Accuracy = 52.144000000000005%, Loss = 1.1845258712768554
Epoch: 2310, Batch Gradient Norm: 39.18750292082382
Epoch: 2310, Batch Gradient Norm after: 22.36067636972982
Epoch 2311/10000, Prediction Accuracy = 52.27%, Loss = 1.1756958961486816
Epoch: 2311, Batch Gradient Norm: 42.228272649425264
Epoch: 2311, Batch Gradient Norm after: 22.360676410846963
Epoch 2312/10000, Prediction Accuracy = 52.153999999999996%, Loss = 1.1840938806533814
Epoch: 2312, Batch Gradient Norm: 39.179541181213395
Epoch: 2312, Batch Gradient Norm after: 22.360679138952392
Epoch 2313/10000, Prediction Accuracy = 52.278%, Loss = 1.1752408027648926
Epoch: 2313, Batch Gradient Norm: 42.23773543691045
Epoch: 2313, Batch Gradient Norm after: 22.360678708101652
Epoch 2314/10000, Prediction Accuracy = 52.174%, Loss = 1.1836639881134032
Epoch: 2314, Batch Gradient Norm: 39.16941661591667
Epoch: 2314, Batch Gradient Norm after: 22.360678328835625
Epoch 2315/10000, Prediction Accuracy = 52.3%, Loss = 1.1747811555862426
Epoch: 2315, Batch Gradient Norm: 42.24299736106679
Epoch: 2315, Batch Gradient Norm after: 22.360676220161626
Epoch 2316/10000, Prediction Accuracy = 52.18399999999999%, Loss = 1.1832473039627076
Epoch: 2316, Batch Gradient Norm: 39.16098250462098
Epoch: 2316, Batch Gradient Norm after: 22.36067594320999
Epoch 2317/10000, Prediction Accuracy = 52.312%, Loss = 1.1743223667144775
Epoch: 2317, Batch Gradient Norm: 42.2528908935905
Epoch: 2317, Batch Gradient Norm after: 22.360678512069917
Epoch 2318/10000, Prediction Accuracy = 52.202%, Loss = 1.1828327894210815
Epoch: 2318, Batch Gradient Norm: 39.15396123714135
Epoch: 2318, Batch Gradient Norm after: 22.360678015687295
Epoch 2319/10000, Prediction Accuracy = 52.322%, Loss = 1.1738585948944091
Epoch: 2319, Batch Gradient Norm: 42.25551109585269
Epoch: 2319, Batch Gradient Norm after: 22.360677277391265
Epoch 2320/10000, Prediction Accuracy = 52.24399999999999%, Loss = 1.1823992252349853
Epoch: 2320, Batch Gradient Norm: 39.14738992056386
Epoch: 2320, Batch Gradient Norm after: 22.360675000487785
Epoch 2321/10000, Prediction Accuracy = 52.322%, Loss = 1.1734038829803466
Epoch: 2321, Batch Gradient Norm: 42.25650392595258
Epoch: 2321, Batch Gradient Norm after: 22.36067869645723
Epoch 2322/10000, Prediction Accuracy = 52.248000000000005%, Loss = 1.1819679737091064
Epoch: 2322, Batch Gradient Norm: 39.141886526111705
Epoch: 2322, Batch Gradient Norm after: 22.360675947574777
Epoch 2323/10000, Prediction Accuracy = 52.339999999999996%, Loss = 1.1729466676712037
Epoch: 2323, Batch Gradient Norm: 42.25973156752645
Epoch: 2323, Batch Gradient Norm after: 22.360678001676607
Epoch 2324/10000, Prediction Accuracy = 52.266000000000005%, Loss = 1.1815359830856322
Epoch: 2324, Batch Gradient Norm: 39.13655075587365
Epoch: 2324, Batch Gradient Norm after: 22.36067781102661
Epoch 2325/10000, Prediction Accuracy = 52.338%, Loss = 1.1724911451339721
Epoch: 2325, Batch Gradient Norm: 42.25700292146356
Epoch: 2325, Batch Gradient Norm after: 22.360677369074594
Epoch 2326/10000, Prediction Accuracy = 52.3%, Loss = 1.1810986995697021
Epoch: 2326, Batch Gradient Norm: 39.129393326226214
Epoch: 2326, Batch Gradient Norm after: 22.36067684735508
Epoch 2327/10000, Prediction Accuracy = 52.342%, Loss = 1.1720396280288696
Epoch: 2327, Batch Gradient Norm: 42.2605207069583
Epoch: 2327, Batch Gradient Norm after: 22.360675562660504
Epoch 2328/10000, Prediction Accuracy = 52.306%, Loss = 1.1806809663772584
Epoch: 2328, Batch Gradient Norm: 39.120207809836565
Epoch: 2328, Batch Gradient Norm after: 22.36067871032633
Epoch 2329/10000, Prediction Accuracy = 52.355999999999995%, Loss = 1.171587347984314
Epoch: 2329, Batch Gradient Norm: 42.26314356929151
Epoch: 2329, Batch Gradient Norm after: 22.36067631170921
Epoch 2330/10000, Prediction Accuracy = 52.327999999999996%, Loss = 1.180270791053772
Epoch: 2330, Batch Gradient Norm: 39.112449471145304
Epoch: 2330, Batch Gradient Norm after: 22.360679203832486
Epoch 2331/10000, Prediction Accuracy = 52.36800000000001%, Loss = 1.1711354017257691
Epoch: 2331, Batch Gradient Norm: 42.26240557071163
Epoch: 2331, Batch Gradient Norm after: 22.360677207336643
Epoch 2332/10000, Prediction Accuracy = 52.33399999999999%, Loss = 1.1798345565795898
Epoch: 2332, Batch Gradient Norm: 39.104005881076155
Epoch: 2332, Batch Gradient Norm after: 22.3606772864491
Epoch 2333/10000, Prediction Accuracy = 52.378%, Loss = 1.1706838607788086
Epoch: 2333, Batch Gradient Norm: 42.25321186345409
Epoch: 2333, Batch Gradient Norm after: 22.360678541243267
Epoch 2334/10000, Prediction Accuracy = 52.36%, Loss = 1.1793760061264038
Epoch: 2334, Batch Gradient Norm: 39.09587997669253
Epoch: 2334, Batch Gradient Norm after: 22.360676201128744
Epoch 2335/10000, Prediction Accuracy = 52.378%, Loss = 1.1702358961105346
Epoch: 2335, Batch Gradient Norm: 42.24011379371398
Epoch: 2335, Batch Gradient Norm after: 22.360677599725964
Epoch 2336/10000, Prediction Accuracy = 52.364%, Loss = 1.178915309906006
Epoch: 2336, Batch Gradient Norm: 39.09079524955908
Epoch: 2336, Batch Gradient Norm after: 22.360676984747037
Epoch 2337/10000, Prediction Accuracy = 52.395999999999994%, Loss = 1.16978600025177
Epoch: 2337, Batch Gradient Norm: 42.217128852440794
Epoch: 2337, Batch Gradient Norm after: 22.360677746629435
Epoch 2338/10000, Prediction Accuracy = 52.374%, Loss = 1.178410243988037
Epoch: 2338, Batch Gradient Norm: 39.08994998741038
Epoch: 2338, Batch Gradient Norm after: 22.3606783259025
Epoch 2339/10000, Prediction Accuracy = 52.398%, Loss = 1.1693413734436036
Epoch: 2339, Batch Gradient Norm: 42.19182895041543
Epoch: 2339, Batch Gradient Norm after: 22.360675627442728
Epoch 2340/10000, Prediction Accuracy = 52.384%, Loss = 1.1778946399688721
Epoch: 2340, Batch Gradient Norm: 39.08572485185651
Epoch: 2340, Batch Gradient Norm after: 22.36067522220402
Epoch 2341/10000, Prediction Accuracy = 52.418000000000006%, Loss = 1.168890357017517
Epoch: 2341, Batch Gradient Norm: 42.177107904661206
Epoch: 2341, Batch Gradient Norm after: 22.36067822939439
Epoch 2342/10000, Prediction Accuracy = 52.40400000000001%, Loss = 1.1774187326431274
Epoch: 2342, Batch Gradient Norm: 39.084739136097916
Epoch: 2342, Batch Gradient Norm after: 22.36067765424299
Epoch 2343/10000, Prediction Accuracy = 52.426%, Loss = 1.1684422254562379
Epoch: 2343, Batch Gradient Norm: 42.15863793659244
Epoch: 2343, Batch Gradient Norm after: 22.360678088264585
Epoch 2344/10000, Prediction Accuracy = 52.422000000000004%, Loss = 1.1769282341003418
Epoch: 2344, Batch Gradient Norm: 39.080946350850674
Epoch: 2344, Batch Gradient Norm after: 22.36067595576081
Epoch 2345/10000, Prediction Accuracy = 52.44%, Loss = 1.167996096611023
Epoch: 2345, Batch Gradient Norm: 42.148014502018334
Epoch: 2345, Batch Gradient Norm after: 22.360679692947933
Epoch 2346/10000, Prediction Accuracy = 52.434000000000005%, Loss = 1.1764671564102174
Epoch: 2346, Batch Gradient Norm: 39.07696824049518
Epoch: 2346, Batch Gradient Norm after: 22.3606757359183
Epoch 2347/10000, Prediction Accuracy = 52.438%, Loss = 1.1675493955612182
Epoch: 2347, Batch Gradient Norm: 42.13866820396833
Epoch: 2347, Batch Gradient Norm after: 22.360674919084385
Epoch 2348/10000, Prediction Accuracy = 52.432%, Loss = 1.1760008811950684
Epoch: 2348, Batch Gradient Norm: 39.072739166731424
Epoch: 2348, Batch Gradient Norm after: 22.360676786174018
Epoch 2349/10000, Prediction Accuracy = 52.448%, Loss = 1.1670993328094483
Epoch: 2349, Batch Gradient Norm: 42.133370576554455
Epoch: 2349, Batch Gradient Norm after: 22.360675499829995
Epoch 2350/10000, Prediction Accuracy = 52.436%, Loss = 1.1755497932434082
Epoch: 2350, Batch Gradient Norm: 39.06554952770827
Epoch: 2350, Batch Gradient Norm after: 22.360676960558703
Epoch 2351/10000, Prediction Accuracy = 52.46400000000001%, Loss = 1.166654920578003
Epoch: 2351, Batch Gradient Norm: 42.13103660277548
Epoch: 2351, Batch Gradient Norm after: 22.36067694748981
Epoch 2352/10000, Prediction Accuracy = 52.443999999999996%, Loss = 1.1751148462295533
Epoch: 2352, Batch Gradient Norm: 39.059713938831
Epoch: 2352, Batch Gradient Norm after: 22.360678124107555
Epoch 2353/10000, Prediction Accuracy = 52.477999999999994%, Loss = 1.1662063360214234
Epoch: 2353, Batch Gradient Norm: 42.12424688888126
Epoch: 2353, Batch Gradient Norm after: 22.360676026755904
Epoch 2354/10000, Prediction Accuracy = 52.45399999999999%, Loss = 1.174660086631775
Epoch: 2354, Batch Gradient Norm: 39.05428859142865
Epoch: 2354, Batch Gradient Norm after: 22.36067693643734
Epoch 2355/10000, Prediction Accuracy = 52.484%, Loss = 1.1657588005065918
Epoch: 2355, Batch Gradient Norm: 42.11332805008395
Epoch: 2355, Batch Gradient Norm after: 22.36067567451672
Epoch 2356/10000, Prediction Accuracy = 52.464%, Loss = 1.1742018461227417
Epoch: 2356, Batch Gradient Norm: 39.04775353991275
Epoch: 2356, Batch Gradient Norm after: 22.36067476282221
Epoch 2357/10000, Prediction Accuracy = 52.498000000000005%, Loss = 1.1653096199035644
Epoch: 2357, Batch Gradient Norm: 42.10310758960179
Epoch: 2357, Batch Gradient Norm after: 22.360677430292277
Epoch 2358/10000, Prediction Accuracy = 52.484%, Loss = 1.1737341165542603
Epoch: 2358, Batch Gradient Norm: 39.041237527046654
Epoch: 2358, Batch Gradient Norm after: 22.360676084079728
Epoch 2359/10000, Prediction Accuracy = 52.512%, Loss = 1.1648714303970338
Epoch: 2359, Batch Gradient Norm: 42.09406663957695
Epoch: 2359, Batch Gradient Norm after: 22.360678726090228
Epoch 2360/10000, Prediction Accuracy = 52.492000000000004%, Loss = 1.1732742071151734
Epoch: 2360, Batch Gradient Norm: 39.03612309986679
Epoch: 2360, Batch Gradient Norm after: 22.360678095176088
Epoch 2361/10000, Prediction Accuracy = 52.517999999999994%, Loss = 1.1644268035888672
Epoch: 2361, Batch Gradient Norm: 42.088653174595734
Epoch: 2361, Batch Gradient Norm after: 22.36068009856139
Epoch 2362/10000, Prediction Accuracy = 52.507999999999996%, Loss = 1.172823143005371
Epoch: 2362, Batch Gradient Norm: 39.02958961989736
Epoch: 2362, Batch Gradient Norm after: 22.360678784013658
Epoch 2363/10000, Prediction Accuracy = 52.534000000000006%, Loss = 1.1639815807342528
Epoch: 2363, Batch Gradient Norm: 42.08703420695788
Epoch: 2363, Batch Gradient Norm after: 22.360678804925353
Epoch 2364/10000, Prediction Accuracy = 52.513999999999996%, Loss = 1.1723914861679077
Epoch: 2364, Batch Gradient Norm: 39.02112313824859
Epoch: 2364, Batch Gradient Norm after: 22.360676894154018
Epoch 2365/10000, Prediction Accuracy = 52.553999999999995%, Loss = 1.163535761833191
Epoch: 2365, Batch Gradient Norm: 42.08195286093138
Epoch: 2365, Batch Gradient Norm after: 22.360677949677676
Epoch 2366/10000, Prediction Accuracy = 52.51800000000001%, Loss = 1.1719618320465088
Epoch: 2366, Batch Gradient Norm: 39.01461710277057
Epoch: 2366, Batch Gradient Norm after: 22.360677076997877
Epoch 2367/10000, Prediction Accuracy = 52.572%, Loss = 1.1631037950515748
Epoch: 2367, Batch Gradient Norm: 42.08047717771904
Epoch: 2367, Batch Gradient Norm after: 22.36067746975479
Epoch 2368/10000, Prediction Accuracy = 52.536%, Loss = 1.171530270576477
Epoch: 2368, Batch Gradient Norm: 39.00750620158121
Epoch: 2368, Batch Gradient Norm after: 22.36067672309776
Epoch 2369/10000, Prediction Accuracy = 52.574%, Loss = 1.1626585483551026
Epoch: 2369, Batch Gradient Norm: 42.07599694474968
Epoch: 2369, Batch Gradient Norm after: 22.360678071797636
Epoch 2370/10000, Prediction Accuracy = 52.54600000000001%, Loss = 1.1710917711257935
Epoch: 2370, Batch Gradient Norm: 39.00119481474067
Epoch: 2370, Batch Gradient Norm after: 22.360679144337006
Epoch 2371/10000, Prediction Accuracy = 52.577999999999996%, Loss = 1.1622207403182983
Epoch: 2371, Batch Gradient Norm: 42.06639473892117
Epoch: 2371, Batch Gradient Norm after: 22.360677029861204
Epoch 2372/10000, Prediction Accuracy = 52.54200000000001%, Loss = 1.1706335306167603
Epoch: 2372, Batch Gradient Norm: 38.99373078881002
Epoch: 2372, Batch Gradient Norm after: 22.36067768084516
Epoch 2373/10000, Prediction Accuracy = 52.588%, Loss = 1.1617741346359254
Epoch: 2373, Batch Gradient Norm: 42.06307266794935
Epoch: 2373, Batch Gradient Norm after: 22.360677092319452
Epoch 2374/10000, Prediction Accuracy = 52.55800000000001%, Loss = 1.1701877117156982
Epoch: 2374, Batch Gradient Norm: 38.987781025701366
Epoch: 2374, Batch Gradient Norm after: 22.360677346442337
Epoch 2375/10000, Prediction Accuracy = 52.605999999999995%, Loss = 1.1613341093063354
Epoch: 2375, Batch Gradient Norm: 42.05314505047103
Epoch: 2375, Batch Gradient Norm after: 22.360676511395262
Epoch 2376/10000, Prediction Accuracy = 52.565999999999995%, Loss = 1.1697328567504883
Epoch: 2376, Batch Gradient Norm: 38.98200771970426
Epoch: 2376, Batch Gradient Norm after: 22.360679345664167
Epoch 2377/10000, Prediction Accuracy = 52.612%, Loss = 1.1608911037445069
Epoch: 2377, Batch Gradient Norm: 42.044848788600106
Epoch: 2377, Batch Gradient Norm after: 22.360676452430113
Epoch 2378/10000, Prediction Accuracy = 52.58200000000001%, Loss = 1.1692825078964233
Epoch: 2378, Batch Gradient Norm: 38.97830701537117
Epoch: 2378, Batch Gradient Norm after: 22.360677742632284
Epoch 2379/10000, Prediction Accuracy = 52.638%, Loss = 1.1604522943496705
Epoch: 2379, Batch Gradient Norm: 42.02945641456335
Epoch: 2379, Batch Gradient Norm after: 22.360676609079675
Epoch 2380/10000, Prediction Accuracy = 52.592%, Loss = 1.1688207387924194
Epoch: 2380, Batch Gradient Norm: 38.97419668609833
Epoch: 2380, Batch Gradient Norm after: 22.36067776666961
Epoch 2381/10000, Prediction Accuracy = 52.64399999999999%, Loss = 1.1600100994110107
Epoch: 2381, Batch Gradient Norm: 42.01626729639277
Epoch: 2381, Batch Gradient Norm after: 22.36067896138805
Epoch 2382/10000, Prediction Accuracy = 52.604000000000006%, Loss = 1.1683464765548706
Epoch: 2382, Batch Gradient Norm: 38.969597671278265
Epoch: 2382, Batch Gradient Norm after: 22.36067713119832
Epoch 2383/10000, Prediction Accuracy = 52.658%, Loss = 1.1595733404159545
Epoch: 2383, Batch Gradient Norm: 41.999172547756096
Epoch: 2383, Batch Gradient Norm after: 22.360679033568072
Epoch 2384/10000, Prediction Accuracy = 52.614%, Loss = 1.1678658485412599
Epoch: 2384, Batch Gradient Norm: 38.964320693622824
Epoch: 2384, Batch Gradient Norm after: 22.36067692868577
Epoch 2385/10000, Prediction Accuracy = 52.668000000000006%, Loss = 1.1591361999511718
Epoch: 2385, Batch Gradient Norm: 41.978855503294916
Epoch: 2385, Batch Gradient Norm after: 22.360678469652925
Epoch 2386/10000, Prediction Accuracy = 52.624%, Loss = 1.1673920392990111
Epoch: 2386, Batch Gradient Norm: 38.958035891845626
Epoch: 2386, Batch Gradient Norm after: 22.360678029331424
Epoch 2387/10000, Prediction Accuracy = 52.684000000000005%, Loss = 1.1587027311325073
Epoch: 2387, Batch Gradient Norm: 41.96128373137388
Epoch: 2387, Batch Gradient Norm after: 22.360677856262328
Epoch 2388/10000, Prediction Accuracy = 52.636%, Loss = 1.1669188022613526
Epoch: 2388, Batch Gradient Norm: 38.95651977988101
Epoch: 2388, Batch Gradient Norm after: 22.360677765195717
Epoch 2389/10000, Prediction Accuracy = 52.694%, Loss = 1.1582642316818237
Epoch: 2389, Batch Gradient Norm: 41.94582427947118
Epoch: 2389, Batch Gradient Norm after: 22.3606778778541
Epoch 2390/10000, Prediction Accuracy = 52.65599999999999%, Loss = 1.166447067260742
Epoch: 2390, Batch Gradient Norm: 38.9546104832697
Epoch: 2390, Batch Gradient Norm after: 22.36067760878286
Epoch 2391/10000, Prediction Accuracy = 52.702%, Loss = 1.1578216791152953
Epoch: 2391, Batch Gradient Norm: 41.936801400148134
Epoch: 2391, Batch Gradient Norm after: 22.36067722463522
Epoch 2392/10000, Prediction Accuracy = 52.660000000000004%, Loss = 1.1659991025924683
Epoch: 2392, Batch Gradient Norm: 38.947261373226944
Epoch: 2392, Batch Gradient Norm after: 22.360676163591975
Epoch 2393/10000, Prediction Accuracy = 52.708000000000006%, Loss = 1.1573899507522583
Epoch: 2393, Batch Gradient Norm: 41.92949675949261
Epoch: 2393, Batch Gradient Norm after: 22.360675862495448
Epoch 2394/10000, Prediction Accuracy = 52.67%, Loss = 1.165557312965393
Epoch: 2394, Batch Gradient Norm: 38.94093028065072
Epoch: 2394, Batch Gradient Norm after: 22.36067688933928
Epoch 2395/10000, Prediction Accuracy = 52.718%, Loss = 1.1569492816925049
Epoch: 2395, Batch Gradient Norm: 41.92282555699528
Epoch: 2395, Batch Gradient Norm after: 22.360677496657367
Epoch 2396/10000, Prediction Accuracy = 52.69%, Loss = 1.1651316642761231
Epoch: 2396, Batch Gradient Norm: 38.93333260623041
Epoch: 2396, Batch Gradient Norm after: 22.36067697884273
Epoch 2397/10000, Prediction Accuracy = 52.73%, Loss = 1.1565136194229126
Epoch: 2397, Batch Gradient Norm: 41.92266958609482
Epoch: 2397, Batch Gradient Norm after: 22.360679054888305
Epoch 2398/10000, Prediction Accuracy = 52.694%, Loss = 1.1647061347961425
Epoch: 2398, Batch Gradient Norm: 38.92791706751978
Epoch: 2398, Batch Gradient Norm after: 22.360679525641558
Epoch 2399/10000, Prediction Accuracy = 52.736000000000004%, Loss = 1.1560672998428345
Epoch: 2399, Batch Gradient Norm: 41.92033856879947
Epoch: 2399, Batch Gradient Norm after: 22.36067701066175
Epoch 2400/10000, Prediction Accuracy = 52.714%, Loss = 1.1642815351486206
Epoch: 2400, Batch Gradient Norm: 38.921841856858585
Epoch: 2400, Batch Gradient Norm after: 22.36067755389536
Epoch 2401/10000, Prediction Accuracy = 52.742%, Loss = 1.1556302070617677
Epoch: 2401, Batch Gradient Norm: 41.914670149276084
Epoch: 2401, Batch Gradient Norm after: 22.360678250956298
Epoch 2402/10000, Prediction Accuracy = 52.71%, Loss = 1.1638463020324707
Epoch: 2402, Batch Gradient Norm: 38.91420305569311
Epoch: 2402, Batch Gradient Norm after: 22.36067850115865
Epoch 2403/10000, Prediction Accuracy = 52.748000000000005%, Loss = 1.155203628540039
Epoch: 2403, Batch Gradient Norm: 41.900907127001986
Epoch: 2403, Batch Gradient Norm after: 22.36067587084433
Epoch 2404/10000, Prediction Accuracy = 52.71%, Loss = 1.1633958578109742
Epoch: 2404, Batch Gradient Norm: 38.90505325700609
Epoch: 2404, Batch Gradient Norm after: 22.360676868774547
Epoch 2405/10000, Prediction Accuracy = 52.738%, Loss = 1.1547762870788574
Epoch: 2405, Batch Gradient Norm: 41.893315393654
Epoch: 2405, Batch Gradient Norm after: 22.360677530658233
Epoch 2406/10000, Prediction Accuracy = 52.727999999999994%, Loss = 1.162950873374939
Epoch: 2406, Batch Gradient Norm: 38.89734622909991
Epoch: 2406, Batch Gradient Norm after: 22.360675942174243
Epoch 2407/10000, Prediction Accuracy = 52.736000000000004%, Loss = 1.1543508529663087
Epoch: 2407, Batch Gradient Norm: 41.88736747653916
Epoch: 2407, Batch Gradient Norm after: 22.36067601238144
Epoch 2408/10000, Prediction Accuracy = 52.75%, Loss = 1.1625199556350707
Epoch: 2408, Batch Gradient Norm: 38.89204501008984
Epoch: 2408, Batch Gradient Norm after: 22.360675172472693
Epoch 2409/10000, Prediction Accuracy = 52.751999999999995%, Loss = 1.1539148807525634
Epoch: 2409, Batch Gradient Norm: 41.88117155513917
Epoch: 2409, Batch Gradient Norm after: 22.3606775114491
Epoch 2410/10000, Prediction Accuracy = 52.751999999999995%, Loss = 1.1620949983596802
Epoch: 2410, Batch Gradient Norm: 38.88355485108191
Epoch: 2410, Batch Gradient Norm after: 22.360676627246047
Epoch 2411/10000, Prediction Accuracy = 52.763999999999996%, Loss = 1.15348961353302
Epoch: 2411, Batch Gradient Norm: 41.87108710821388
Epoch: 2411, Batch Gradient Norm after: 22.360677032674644
Epoch 2412/10000, Prediction Accuracy = 52.74799999999999%, Loss = 1.1616504192352295
Epoch: 2412, Batch Gradient Norm: 38.874283802750256
Epoch: 2412, Batch Gradient Norm after: 22.360679008416515
Epoch 2413/10000, Prediction Accuracy = 52.774%, Loss = 1.153059983253479
Epoch: 2413, Batch Gradient Norm: 41.86843256563824
Epoch: 2413, Batch Gradient Norm after: 22.360677981889946
Epoch 2414/10000, Prediction Accuracy = 52.74399999999999%, Loss = 1.1612265348434447
Epoch: 2414, Batch Gradient Norm: 38.867169260326484
Epoch: 2414, Batch Gradient Norm after: 22.36067697902725
Epoch 2415/10000, Prediction Accuracy = 52.782000000000004%, Loss = 1.1526247262954712
Epoch: 2415, Batch Gradient Norm: 41.85947111549947
Epoch: 2415, Batch Gradient Norm after: 22.360674882858262
Epoch 2416/10000, Prediction Accuracy = 52.762%, Loss = 1.1607872009277345
Epoch: 2416, Batch Gradient Norm: 38.86256502523678
Epoch: 2416, Batch Gradient Norm after: 22.360678144520534
Epoch 2417/10000, Prediction Accuracy = 52.80200000000001%, Loss = 1.1521962881088257
Epoch: 2417, Batch Gradient Norm: 41.85332308929122
Epoch: 2417, Batch Gradient Norm after: 22.360676799036344
Epoch 2418/10000, Prediction Accuracy = 52.772000000000006%, Loss = 1.1603515863418579
Epoch: 2418, Batch Gradient Norm: 38.855055505543575
Epoch: 2418, Batch Gradient Norm after: 22.360678953454833
Epoch 2419/10000, Prediction Accuracy = 52.812%, Loss = 1.1517705202102662
Epoch: 2419, Batch Gradient Norm: 41.84572655474959
Epoch: 2419, Batch Gradient Norm after: 22.3606770020956
Epoch 2420/10000, Prediction Accuracy = 52.784000000000006%, Loss = 1.1599201202392577
Epoch: 2420, Batch Gradient Norm: 38.84976461137935
Epoch: 2420, Batch Gradient Norm after: 22.36067940837804
Epoch 2421/10000, Prediction Accuracy = 52.82000000000001%, Loss = 1.1513381242752074
Epoch: 2421, Batch Gradient Norm: 41.83838142064421
Epoch: 2421, Batch Gradient Norm after: 22.360676455713513
Epoch 2422/10000, Prediction Accuracy = 52.798%, Loss = 1.1594871282577515
Epoch: 2422, Batch Gradient Norm: 38.84291461001695
Epoch: 2422, Batch Gradient Norm after: 22.36068010555923
Epoch 2423/10000, Prediction Accuracy = 52.81600000000001%, Loss = 1.1509130716323852
Epoch: 2423, Batch Gradient Norm: 41.83661119172726
Epoch: 2423, Batch Gradient Norm after: 22.3606764215505
Epoch 2424/10000, Prediction Accuracy = 52.80799999999999%, Loss = 1.1590635299682617
Epoch: 2424, Batch Gradient Norm: 38.83699111574003
Epoch: 2424, Batch Gradient Norm after: 22.36067827229195
Epoch 2425/10000, Prediction Accuracy = 52.81999999999999%, Loss = 1.1504793882369995
Epoch: 2425, Batch Gradient Norm: 41.82600114492905
Epoch: 2425, Batch Gradient Norm after: 22.36067664808792
Epoch 2426/10000, Prediction Accuracy = 52.827999999999996%, Loss = 1.1586318492889405
Epoch: 2426, Batch Gradient Norm: 38.83009578999858
Epoch: 2426, Batch Gradient Norm after: 22.360678082636365
Epoch 2427/10000, Prediction Accuracy = 52.836%, Loss = 1.150054359436035
Epoch: 2427, Batch Gradient Norm: 41.81899575607455
Epoch: 2427, Batch Gradient Norm after: 22.360674895358844
Epoch 2428/10000, Prediction Accuracy = 52.831999999999994%, Loss = 1.1581973552703857
Epoch: 2428, Batch Gradient Norm: 38.82404158449767
Epoch: 2428, Batch Gradient Norm after: 22.360679122653284
Epoch 2429/10000, Prediction Accuracy = 52.85799999999999%, Loss = 1.1496232748031616
Epoch: 2429, Batch Gradient Norm: 41.81108906151966
Epoch: 2429, Batch Gradient Norm after: 22.360677153877827
Epoch 2430/10000, Prediction Accuracy = 52.848%, Loss = 1.1577733039855957
Epoch: 2430, Batch Gradient Norm: 38.8172572436672
Epoch: 2430, Batch Gradient Norm after: 22.360678124105245
Epoch 2431/10000, Prediction Accuracy = 52.882000000000005%, Loss = 1.1491941452026366
Epoch: 2431, Batch Gradient Norm: 41.80169085642656
Epoch: 2431, Batch Gradient Norm after: 22.360677730481157
Epoch 2432/10000, Prediction Accuracy = 52.843999999999994%, Loss = 1.1573370695114136
Epoch: 2432, Batch Gradient Norm: 38.808500344842805
Epoch: 2432, Batch Gradient Norm after: 22.360678816018076
Epoch 2433/10000, Prediction Accuracy = 52.898%, Loss = 1.1487673759460448
Epoch: 2433, Batch Gradient Norm: 41.798525992291545
Epoch: 2433, Batch Gradient Norm after: 22.36067614786218
Epoch 2434/10000, Prediction Accuracy = 52.846000000000004%, Loss = 1.156915283203125
Epoch: 2434, Batch Gradient Norm: 38.80238968620699
Epoch: 2434, Batch Gradient Norm after: 22.36067820789993
Epoch 2435/10000, Prediction Accuracy = 52.91200000000001%, Loss = 1.1483429670333862
Epoch: 2435, Batch Gradient Norm: 41.78843533936495
Epoch: 2435, Batch Gradient Norm after: 22.36067556352284
Epoch 2436/10000, Prediction Accuracy = 52.854%, Loss = 1.1564682483673097
Epoch: 2436, Batch Gradient Norm: 38.79704551667606
Epoch: 2436, Batch Gradient Norm after: 22.36067875468585
Epoch 2437/10000, Prediction Accuracy = 52.924%, Loss = 1.147929048538208
Epoch: 2437, Batch Gradient Norm: 41.77709818626309
Epoch: 2437, Batch Gradient Norm after: 22.360675788870374
Epoch 2438/10000, Prediction Accuracy = 52.86800000000001%, Loss = 1.1560227394104003
Epoch: 2438, Batch Gradient Norm: 38.78990100371147
Epoch: 2438, Batch Gradient Norm after: 22.36067940315892
Epoch 2439/10000, Prediction Accuracy = 52.938%, Loss = 1.1475032806396483
Epoch: 2439, Batch Gradient Norm: 41.77346194655928
Epoch: 2439, Batch Gradient Norm after: 22.360675852435914
Epoch 2440/10000, Prediction Accuracy = 52.88599999999999%, Loss = 1.1556043863296508
Epoch: 2440, Batch Gradient Norm: 38.781741307866085
Epoch: 2440, Batch Gradient Norm after: 22.36067772967034
Epoch 2441/10000, Prediction Accuracy = 52.95399999999999%, Loss = 1.1470800161361694
Epoch: 2441, Batch Gradient Norm: 41.76602977559379
Epoch: 2441, Batch Gradient Norm after: 22.36067675090134
Epoch 2442/10000, Prediction Accuracy = 52.902%, Loss = 1.1551883459091186
Epoch: 2442, Batch Gradient Norm: 38.77141653385888
Epoch: 2442, Batch Gradient Norm after: 22.36067878586547
Epoch 2443/10000, Prediction Accuracy = 52.958000000000006%, Loss = 1.1466511964797974
Epoch: 2443, Batch Gradient Norm: 41.75728419340957
Epoch: 2443, Batch Gradient Norm after: 22.36067627309099
Epoch 2444/10000, Prediction Accuracy = 52.92%, Loss = 1.1547687768936157
Epoch: 2444, Batch Gradient Norm: 38.76344424857826
Epoch: 2444, Batch Gradient Norm after: 22.36067748602499
Epoch 2445/10000, Prediction Accuracy = 52.97600000000001%, Loss = 1.14623122215271
Epoch: 2445, Batch Gradient Norm: 41.7443707072865
Epoch: 2445, Batch Gradient Norm after: 22.36067666965687
Epoch 2446/10000, Prediction Accuracy = 52.93800000000001%, Loss = 1.1543323516845703
Epoch: 2446, Batch Gradient Norm: 38.75349728036728
Epoch: 2446, Batch Gradient Norm after: 22.36067621999205
Epoch 2447/10000, Prediction Accuracy = 52.98599999999999%, Loss = 1.145811629295349
Epoch: 2447, Batch Gradient Norm: 41.73475087039876
Epoch: 2447, Batch Gradient Norm after: 22.360674870448705
Epoch 2448/10000, Prediction Accuracy = 52.952%, Loss = 1.1538985967636108
Epoch: 2448, Batch Gradient Norm: 38.74643662561426
Epoch: 2448, Batch Gradient Norm after: 22.36067740632418
Epoch 2449/10000, Prediction Accuracy = 52.988%, Loss = 1.1453926801681518
Epoch: 2449, Batch Gradient Norm: 41.72559291927271
Epoch: 2449, Batch Gradient Norm after: 22.360677254747802
Epoch 2450/10000, Prediction Accuracy = 52.96%, Loss = 1.1534691095352172
Epoch: 2450, Batch Gradient Norm: 38.735619303522874
Epoch: 2450, Batch Gradient Norm after: 22.360678583389404
Epoch 2451/10000, Prediction Accuracy = 53.008%, Loss = 1.1449729919433593
Epoch: 2451, Batch Gradient Norm: 41.713672785956526
Epoch: 2451, Batch Gradient Norm after: 22.36067732500342
Epoch 2452/10000, Prediction Accuracy = 52.96999999999999%, Loss = 1.1530365943908691
Epoch: 2452, Batch Gradient Norm: 38.728261269984735
Epoch: 2452, Batch Gradient Norm after: 22.360677630189898
Epoch 2453/10000, Prediction Accuracy = 53.016%, Loss = 1.144553303718567
Epoch: 2453, Batch Gradient Norm: 41.706108586559786
Epoch: 2453, Batch Gradient Norm after: 22.360676369406495
Epoch 2454/10000, Prediction Accuracy = 52.976%, Loss = 1.1526152849197389
Epoch: 2454, Batch Gradient Norm: 38.718042935465846
Epoch: 2454, Batch Gradient Norm after: 22.36067777781538
Epoch 2455/10000, Prediction Accuracy = 53.022000000000006%, Loss = 1.1441340446472168
Epoch: 2455, Batch Gradient Norm: 41.69755235263207
Epoch: 2455, Batch Gradient Norm after: 22.36067883406665
Epoch 2456/10000, Prediction Accuracy = 52.98199999999999%, Loss = 1.1521880388259889
Epoch: 2456, Batch Gradient Norm: 38.712639583317426
Epoch: 2456, Batch Gradient Norm after: 22.360679818876378
Epoch 2457/10000, Prediction Accuracy = 53.036%, Loss = 1.1437029361724853
Epoch: 2457, Batch Gradient Norm: 41.68606020936603
Epoch: 2457, Batch Gradient Norm after: 22.360678331474194
Epoch 2458/10000, Prediction Accuracy = 53.00599999999999%, Loss = 1.1517539024353027
Epoch: 2458, Batch Gradient Norm: 38.707088972338475
Epoch: 2458, Batch Gradient Norm after: 22.36067893227305
Epoch 2459/10000, Prediction Accuracy = 53.048%, Loss = 1.1432807207107545
Epoch: 2459, Batch Gradient Norm: 41.67300158672978
Epoch: 2459, Batch Gradient Norm after: 22.36067855168867
Epoch 2460/10000, Prediction Accuracy = 53.012%, Loss = 1.1513154983520508
Epoch: 2460, Batch Gradient Norm: 38.70006444562847
Epoch: 2460, Batch Gradient Norm after: 22.360676723394118
Epoch 2461/10000, Prediction Accuracy = 53.05%, Loss = 1.1428611278533936
Epoch: 2461, Batch Gradient Norm: 41.66083724961661
Epoch: 2461, Batch Gradient Norm after: 22.360677376342434
Epoch 2462/10000, Prediction Accuracy = 53.008%, Loss = 1.1508896589279174
Epoch: 2462, Batch Gradient Norm: 38.69198350933693
Epoch: 2462, Batch Gradient Norm after: 22.36067525002487
Epoch 2463/10000, Prediction Accuracy = 53.05%, Loss = 1.1424413442611694
Epoch: 2463, Batch Gradient Norm: 41.651346420812445
Epoch: 2463, Batch Gradient Norm after: 22.360675445537986
Epoch 2464/10000, Prediction Accuracy = 53.022000000000006%, Loss = 1.1504600286483764
Epoch: 2464, Batch Gradient Norm: 38.68577254572177
Epoch: 2464, Batch Gradient Norm after: 22.36067567120326
Epoch 2465/10000, Prediction Accuracy = 53.052%, Loss = 1.142024827003479
Epoch: 2465, Batch Gradient Norm: 41.638293515842996
Epoch: 2465, Batch Gradient Norm after: 22.360676215378138
Epoch 2466/10000, Prediction Accuracy = 53.032000000000004%, Loss = 1.150019145011902
Epoch: 2466, Batch Gradient Norm: 38.68195958508073
Epoch: 2466, Batch Gradient Norm after: 22.36067677605437
Epoch 2467/10000, Prediction Accuracy = 53.065999999999995%, Loss = 1.141610312461853
Epoch: 2467, Batch Gradient Norm: 41.61628323422862
Epoch: 2467, Batch Gradient Norm after: 22.36067837737455
Epoch 2468/10000, Prediction Accuracy = 53.04%, Loss = 1.1495661973953246
Epoch: 2468, Batch Gradient Norm: 38.67491514149939
Epoch: 2468, Batch Gradient Norm after: 22.360677572642484
Epoch 2469/10000, Prediction Accuracy = 53.08200000000001%, Loss = 1.1412007093429566
Epoch: 2469, Batch Gradient Norm: 41.600844368578336
Epoch: 2469, Batch Gradient Norm after: 22.360677179699035
Epoch 2470/10000, Prediction Accuracy = 53.052%, Loss = 1.149125909805298
Epoch: 2470, Batch Gradient Norm: 38.66763183152488
Epoch: 2470, Batch Gradient Norm after: 22.36067662174572
Epoch 2471/10000, Prediction Accuracy = 53.092000000000006%, Loss = 1.1407859563827514
Epoch: 2471, Batch Gradient Norm: 41.58818114713034
Epoch: 2471, Batch Gradient Norm after: 22.360676750189384
Epoch 2472/10000, Prediction Accuracy = 53.065999999999995%, Loss = 1.148697566986084
Epoch: 2472, Batch Gradient Norm: 38.65974290367793
Epoch: 2472, Batch Gradient Norm after: 22.36067599673875
Epoch 2473/10000, Prediction Accuracy = 53.092%, Loss = 1.1403752326965333
Epoch: 2473, Batch Gradient Norm: 41.565175548814224
Epoch: 2473, Batch Gradient Norm after: 22.360676975449994
Epoch 2474/10000, Prediction Accuracy = 53.074%, Loss = 1.1482478141784669
Epoch: 2474, Batch Gradient Norm: 38.65283414042779
Epoch: 2474, Batch Gradient Norm after: 22.36067445592849
Epoch 2475/10000, Prediction Accuracy = 53.1%, Loss = 1.1399633884429932
Epoch: 2475, Batch Gradient Norm: 41.54294704209588
Epoch: 2475, Batch Gradient Norm after: 22.360677916235307
Epoch 2476/10000, Prediction Accuracy = 53.086%, Loss = 1.1477941036224366
Epoch: 2476, Batch Gradient Norm: 38.6468910846208
Epoch: 2476, Batch Gradient Norm after: 22.360675620262143
Epoch 2477/10000, Prediction Accuracy = 53.11800000000001%, Loss = 1.1395580053329468
Epoch: 2477, Batch Gradient Norm: 41.52407621516212
Epoch: 2477, Batch Gradient Norm after: 22.36067694657029
Epoch 2478/10000, Prediction Accuracy = 53.089999999999996%, Loss = 1.147342562675476
Epoch: 2478, Batch Gradient Norm: 38.64073230932272
Epoch: 2478, Batch Gradient Norm after: 22.360676489095805
Epoch 2479/10000, Prediction Accuracy = 53.124%, Loss = 1.1391455411911011
Epoch: 2479, Batch Gradient Norm: 41.50925384374913
Epoch: 2479, Batch Gradient Norm after: 22.36067748758642
Epoch 2480/10000, Prediction Accuracy = 53.105999999999995%, Loss = 1.1469043016433715
Epoch: 2480, Batch Gradient Norm: 38.63068417602403
Epoch: 2480, Batch Gradient Norm after: 22.360677717860874
Epoch 2481/10000, Prediction Accuracy = 53.12800000000001%, Loss = 1.1387349367141724
Epoch: 2481, Batch Gradient Norm: 41.489126832463
Epoch: 2481, Batch Gradient Norm after: 22.36067824853672
Epoch 2482/10000, Prediction Accuracy = 53.116%, Loss = 1.1464590787887574
Epoch: 2482, Batch Gradient Norm: 38.625577279081426
Epoch: 2482, Batch Gradient Norm after: 22.36067805250777
Epoch 2483/10000, Prediction Accuracy = 53.122%, Loss = 1.1383203983306884
Epoch: 2483, Batch Gradient Norm: 41.46832620347835
Epoch: 2483, Batch Gradient Norm after: 22.360678585643
Epoch 2484/10000, Prediction Accuracy = 53.12800000000001%, Loss = 1.146007204055786
Epoch: 2484, Batch Gradient Norm: 38.61842326644811
Epoch: 2484, Batch Gradient Norm after: 22.360677296337666
Epoch 2485/10000, Prediction Accuracy = 53.114%, Loss = 1.1379098176956177
Epoch: 2485, Batch Gradient Norm: 41.449984593962704
Epoch: 2485, Batch Gradient Norm after: 22.360677078467347
Epoch 2486/10000, Prediction Accuracy = 53.136%, Loss = 1.1455605268478393
Epoch: 2486, Batch Gradient Norm: 38.61288951000834
Epoch: 2486, Batch Gradient Norm after: 22.360676097479175
Epoch 2487/10000, Prediction Accuracy = 53.128%, Loss = 1.1374994277954102
Epoch: 2487, Batch Gradient Norm: 41.421427379279045
Epoch: 2487, Batch Gradient Norm after: 22.360677187162633
Epoch 2488/10000, Prediction Accuracy = 53.141999999999996%, Loss = 1.1450984477996826
Epoch: 2488, Batch Gradient Norm: 38.60750071080664
Epoch: 2488, Batch Gradient Norm after: 22.360674645345565
Epoch 2489/10000, Prediction Accuracy = 53.136%, Loss = 1.1370959758758545
Epoch: 2489, Batch Gradient Norm: 41.39690812058186
Epoch: 2489, Batch Gradient Norm after: 22.360676862542803
Epoch 2490/10000, Prediction Accuracy = 53.148%, Loss = 1.1446354150772096
Epoch: 2490, Batch Gradient Norm: 38.60421758666076
Epoch: 2490, Batch Gradient Norm after: 22.360676841216993
Epoch 2491/10000, Prediction Accuracy = 53.15999999999999%, Loss = 1.1366842985153198
Epoch: 2491, Batch Gradient Norm: 41.36970515958315
Epoch: 2491, Batch Gradient Norm after: 22.36067768666765
Epoch 2492/10000, Prediction Accuracy = 53.144000000000005%, Loss = 1.1441732883453368
Epoch: 2492, Batch Gradient Norm: 38.60147881018205
Epoch: 2492, Batch Gradient Norm after: 22.360676212885096
Epoch 2493/10000, Prediction Accuracy = 53.16799999999999%, Loss = 1.1362741947174073
Epoch: 2493, Batch Gradient Norm: 41.33912657105214
Epoch: 2493, Batch Gradient Norm after: 22.360674796728613
Epoch 2494/10000, Prediction Accuracy = 53.144000000000005%, Loss = 1.1436900615692138
Epoch: 2494, Batch Gradient Norm: 38.594991528004975
Epoch: 2494, Batch Gradient Norm after: 22.360678372884315
Epoch 2495/10000, Prediction Accuracy = 53.184000000000005%, Loss = 1.1358696937561035
Epoch: 2495, Batch Gradient Norm: 41.31134821587206
Epoch: 2495, Batch Gradient Norm after: 22.360677057344432
Epoch 2496/10000, Prediction Accuracy = 53.15400000000001%, Loss = 1.1432144403457642
Epoch: 2496, Batch Gradient Norm: 38.592442092897926
Epoch: 2496, Batch Gradient Norm after: 22.360676517516666
Epoch 2497/10000, Prediction Accuracy = 53.19%, Loss = 1.135467267036438
Epoch: 2497, Batch Gradient Norm: 41.27886427427066
Epoch: 2497, Batch Gradient Norm after: 22.360677222986
Epoch 2498/10000, Prediction Accuracy = 53.176%, Loss = 1.1427465200424194
Epoch: 2498, Batch Gradient Norm: 38.58534036844149
Epoch: 2498, Batch Gradient Norm after: 22.360675739406002
Epoch 2499/10000, Prediction Accuracy = 53.194%, Loss = 1.1350675344467163
Epoch: 2499, Batch Gradient Norm: 41.252785931930056
Epoch: 2499, Batch Gradient Norm after: 22.360676974316355
Epoch 2500/10000, Prediction Accuracy = 53.18000000000001%, Loss = 1.1422860860824584
Epoch: 2500, Batch Gradient Norm: 38.58214915972958
Epoch: 2500, Batch Gradient Norm after: 22.360676845857945
Epoch 2501/10000, Prediction Accuracy = 53.19199999999999%, Loss = 1.1346651554107665
Epoch: 2501, Batch Gradient Norm: 41.23117527762983
Epoch: 2501, Batch Gradient Norm after: 22.360676196754856
Epoch 2502/10000, Prediction Accuracy = 53.19%, Loss = 1.1418286085128784
Epoch: 2502, Batch Gradient Norm: 38.57419214226115
Epoch: 2502, Batch Gradient Norm after: 22.360675822121944
Epoch 2503/10000, Prediction Accuracy = 53.214%, Loss = 1.134261465072632
Epoch: 2503, Batch Gradient Norm: 41.20672946763842
Epoch: 2503, Batch Gradient Norm after: 22.3606733585855
Epoch 2504/10000, Prediction Accuracy = 53.188%, Loss = 1.141387367248535
Epoch: 2504, Batch Gradient Norm: 38.569419662719774
Epoch: 2504, Batch Gradient Norm after: 22.360675914037632
Epoch 2505/10000, Prediction Accuracy = 53.218%, Loss = 1.1338587999343872
Epoch: 2505, Batch Gradient Norm: 41.18083263056465
Epoch: 2505, Batch Gradient Norm after: 22.360677164761256
Epoch 2506/10000, Prediction Accuracy = 53.202%, Loss = 1.1409263610839844
Epoch: 2506, Batch Gradient Norm: 38.56435925676604
Epoch: 2506, Batch Gradient Norm after: 22.36067519079275
Epoch 2507/10000, Prediction Accuracy = 53.217999999999996%, Loss = 1.1334577798843384
Epoch: 2507, Batch Gradient Norm: 41.15118401933519
Epoch: 2507, Batch Gradient Norm after: 22.360675318959846
Epoch 2508/10000, Prediction Accuracy = 53.23%, Loss = 1.1404637336730956
Epoch: 2508, Batch Gradient Norm: 38.55970905459566
Epoch: 2508, Batch Gradient Norm after: 22.360677361001297
Epoch 2509/10000, Prediction Accuracy = 53.220000000000006%, Loss = 1.1330592393875123
Epoch: 2509, Batch Gradient Norm: 41.127979993644274
Epoch: 2509, Batch Gradient Norm after: 22.36067563444419
Epoch 2510/10000, Prediction Accuracy = 53.232000000000006%, Loss = 1.140003776550293
Epoch: 2510, Batch Gradient Norm: 38.55607738356766
Epoch: 2510, Batch Gradient Norm after: 22.360675208454218
Epoch 2511/10000, Prediction Accuracy = 53.23%, Loss = 1.1326639175415039
Epoch: 2511, Batch Gradient Norm: 41.10177096351538
Epoch: 2511, Batch Gradient Norm after: 22.360674837696422
Epoch 2512/10000, Prediction Accuracy = 53.234%, Loss = 1.1395504951477051
Epoch: 2512, Batch Gradient Norm: 38.55010902224282
Epoch: 2512, Batch Gradient Norm after: 22.36067502013579
Epoch 2513/10000, Prediction Accuracy = 53.236000000000004%, Loss = 1.1322685956954956
Epoch: 2513, Batch Gradient Norm: 41.07391259464693
Epoch: 2513, Batch Gradient Norm after: 22.360676378315237
Epoch 2514/10000, Prediction Accuracy = 53.24399999999999%, Loss = 1.1390884876251222
Epoch: 2514, Batch Gradient Norm: 38.544541401978336
Epoch: 2514, Batch Gradient Norm after: 22.360675893871917
Epoch 2515/10000, Prediction Accuracy = 53.251999999999995%, Loss = 1.131874394416809
Epoch: 2515, Batch Gradient Norm: 41.042455966112016
Epoch: 2515, Batch Gradient Norm after: 22.36067529425699
Epoch 2516/10000, Prediction Accuracy = 53.238%, Loss = 1.1386227369308473
Epoch: 2516, Batch Gradient Norm: 38.542975745118454
Epoch: 2516, Batch Gradient Norm after: 22.36067528583854
Epoch 2517/10000, Prediction Accuracy = 53.262%, Loss = 1.1314791440963745
Epoch: 2517, Batch Gradient Norm: 41.015084870506
Epoch: 2517, Batch Gradient Norm after: 22.36067682628017
Epoch 2518/10000, Prediction Accuracy = 53.239999999999995%, Loss = 1.1381589889526367
Epoch: 2518, Batch Gradient Norm: 38.53596670522397
Epoch: 2518, Batch Gradient Norm after: 22.360677495877216
Epoch 2519/10000, Prediction Accuracy = 53.279999999999994%, Loss = 1.1310843229293823
Epoch: 2519, Batch Gradient Norm: 40.991452355387786
Epoch: 2519, Batch Gradient Norm after: 22.36067767504556
Epoch 2520/10000, Prediction Accuracy = 53.25%, Loss = 1.1377124547958375
Epoch: 2520, Batch Gradient Norm: 38.53148088375779
Epoch: 2520, Batch Gradient Norm after: 22.360677443209127
Epoch 2521/10000, Prediction Accuracy = 53.29%, Loss = 1.1306876420974732
Epoch: 2521, Batch Gradient Norm: 40.97366309598218
Epoch: 2521, Batch Gradient Norm after: 22.360675274703468
Epoch 2522/10000, Prediction Accuracy = 53.274%, Loss = 1.1372817754745483
Epoch: 2522, Batch Gradient Norm: 38.52688845146668
Epoch: 2522, Batch Gradient Norm after: 22.36067698354194
Epoch 2523/10000, Prediction Accuracy = 53.303999999999995%, Loss = 1.1302810668945313
Epoch: 2523, Batch Gradient Norm: 40.959488787526396
Epoch: 2523, Batch Gradient Norm after: 22.36067688617284
Epoch 2524/10000, Prediction Accuracy = 53.284000000000006%, Loss = 1.136860179901123
Epoch: 2524, Batch Gradient Norm: 38.52015822937477
Epoch: 2524, Batch Gradient Norm after: 22.360675033571777
Epoch 2525/10000, Prediction Accuracy = 53.30800000000001%, Loss = 1.1298785924911499
Epoch: 2525, Batch Gradient Norm: 40.94060249441872
Epoch: 2525, Batch Gradient Norm after: 22.360675074028894
Epoch 2526/10000, Prediction Accuracy = 53.286%, Loss = 1.136442494392395
Epoch: 2526, Batch Gradient Norm: 38.515574257934816
Epoch: 2526, Batch Gradient Norm after: 22.360677192709343
Epoch 2527/10000, Prediction Accuracy = 53.314%, Loss = 1.1294801473617553
Epoch: 2527, Batch Gradient Norm: 40.92517184694328
Epoch: 2527, Batch Gradient Norm after: 22.360679892079894
Epoch 2528/10000, Prediction Accuracy = 53.286%, Loss = 1.1360157489776612
Epoch: 2528, Batch Gradient Norm: 38.509870946454576
Epoch: 2528, Batch Gradient Norm after: 22.360678064160833
Epoch 2529/10000, Prediction Accuracy = 53.32000000000001%, Loss = 1.1290793895721436
Epoch: 2529, Batch Gradient Norm: 40.91006095279768
Epoch: 2529, Batch Gradient Norm after: 22.360678034009798
Epoch 2530/10000, Prediction Accuracy = 53.303999999999995%, Loss = 1.1355877161026
Epoch: 2530, Batch Gradient Norm: 38.50264867197173
Epoch: 2530, Batch Gradient Norm after: 22.360676965779945
Epoch 2531/10000, Prediction Accuracy = 53.354%, Loss = 1.1286783933639526
Epoch: 2531, Batch Gradient Norm: 40.895536358468455
Epoch: 2531, Batch Gradient Norm after: 22.360675093405302
Epoch 2532/10000, Prediction Accuracy = 53.322%, Loss = 1.1351599216461181
Epoch: 2532, Batch Gradient Norm: 38.49841097428471
Epoch: 2532, Batch Gradient Norm after: 22.36067639083194
Epoch 2533/10000, Prediction Accuracy = 53.35799999999999%, Loss = 1.1282745599746704
Epoch: 2533, Batch Gradient Norm: 40.8785169160735
Epoch: 2533, Batch Gradient Norm after: 22.360676984368013
Epoch 2534/10000, Prediction Accuracy = 53.334%, Loss = 1.1347296237945557
Epoch: 2534, Batch Gradient Norm: 38.492174152251856
Epoch: 2534, Batch Gradient Norm after: 22.360677196395258
Epoch 2535/10000, Prediction Accuracy = 53.362%, Loss = 1.1278773307800294
Epoch: 2535, Batch Gradient Norm: 40.86507217851176
Epoch: 2535, Batch Gradient Norm after: 22.36067831082496
Epoch 2536/10000, Prediction Accuracy = 53.34000000000001%, Loss = 1.1343125820159912
Epoch: 2536, Batch Gradient Norm: 38.48542392121883
Epoch: 2536, Batch Gradient Norm after: 22.36067586773664
Epoch 2537/10000, Prediction Accuracy = 53.364%, Loss = 1.127479648590088
Epoch: 2537, Batch Gradient Norm: 40.8498419963535
Epoch: 2537, Batch Gradient Norm after: 22.360677748399684
Epoch 2538/10000, Prediction Accuracy = 53.35%, Loss = 1.1338950872421265
Epoch: 2538, Batch Gradient Norm: 38.47747615140417
Epoch: 2538, Batch Gradient Norm after: 22.360677513090796
Epoch 2539/10000, Prediction Accuracy = 53.362%, Loss = 1.127081608772278
Epoch: 2539, Batch Gradient Norm: 40.839142737199815
Epoch: 2539, Batch Gradient Norm after: 22.360676209292446
Epoch 2540/10000, Prediction Accuracy = 53.352%, Loss = 1.1334880590438843
Epoch: 2540, Batch Gradient Norm: 38.469043005895
Epoch: 2540, Batch Gradient Norm after: 22.360678800162674
Epoch 2541/10000, Prediction Accuracy = 53.36400000000001%, Loss = 1.1266862869262695
Epoch: 2541, Batch Gradient Norm: 40.82788876117577
Epoch: 2541, Batch Gradient Norm after: 22.36067691460912
Epoch 2542/10000, Prediction Accuracy = 53.36%, Loss = 1.1330836057662963
Epoch: 2542, Batch Gradient Norm: 38.4588265353838
Epoch: 2542, Batch Gradient Norm after: 22.360675079991125
Epoch 2543/10000, Prediction Accuracy = 53.37399999999999%, Loss = 1.1262860298156738
Epoch: 2543, Batch Gradient Norm: 40.82391254431017
Epoch: 2543, Batch Gradient Norm after: 22.360675566418244
Epoch 2544/10000, Prediction Accuracy = 53.376%, Loss = 1.1326911687850951
Epoch: 2544, Batch Gradient Norm: 38.44978442770741
Epoch: 2544, Batch Gradient Norm after: 22.360677786456836
Epoch 2545/10000, Prediction Accuracy = 53.38199999999999%, Loss = 1.1258813858032226
Epoch: 2545, Batch Gradient Norm: 40.8171122159677
Epoch: 2545, Batch Gradient Norm after: 22.360675723342904
Epoch 2546/10000, Prediction Accuracy = 53.372%, Loss = 1.1322998762130738
Epoch: 2546, Batch Gradient Norm: 38.439760603651976
Epoch: 2546, Batch Gradient Norm after: 22.360677941444763
Epoch 2547/10000, Prediction Accuracy = 53.38800000000001%, Loss = 1.1254860401153564
Epoch: 2547, Batch Gradient Norm: 40.81041815157798
Epoch: 2547, Batch Gradient Norm after: 22.360675803509505
Epoch 2548/10000, Prediction Accuracy = 53.367999999999995%, Loss = 1.1319130182266235
Epoch: 2548, Batch Gradient Norm: 38.42334477612882
Epoch: 2548, Batch Gradient Norm after: 22.36067689552772
Epoch 2549/10000, Prediction Accuracy = 53.394000000000005%, Loss = 1.1250873804092407
Epoch: 2549, Batch Gradient Norm: 40.812635011713546
Epoch: 2549, Batch Gradient Norm after: 22.360675672163776
Epoch 2550/10000, Prediction Accuracy = 53.384%, Loss = 1.1315406799316405
Epoch: 2550, Batch Gradient Norm: 38.41069051148298
Epoch: 2550, Batch Gradient Norm after: 22.360679492773453
Epoch 2551/10000, Prediction Accuracy = 53.403999999999996%, Loss = 1.1246810674667358
Epoch: 2551, Batch Gradient Norm: 40.82060299803715
Epoch: 2551, Batch Gradient Norm after: 22.36067721871508
Epoch 2552/10000, Prediction Accuracy = 53.394000000000005%, Loss = 1.1311818599700927
Epoch: 2552, Batch Gradient Norm: 38.397408409398125
Epoch: 2552, Batch Gradient Norm after: 22.36067815733487
Epoch 2553/10000, Prediction Accuracy = 53.412%, Loss = 1.124274444580078
Epoch: 2553, Batch Gradient Norm: 40.82349515717452
Epoch: 2553, Batch Gradient Norm after: 22.360677836202168
Epoch 2554/10000, Prediction Accuracy = 53.414%, Loss = 1.1308180809020996
Epoch: 2554, Batch Gradient Norm: 38.384484881538526
Epoch: 2554, Batch Gradient Norm after: 22.360676907858043
Epoch 2555/10000, Prediction Accuracy = 53.412%, Loss = 1.1238755464553833
Epoch: 2555, Batch Gradient Norm: 40.82562013434144
Epoch: 2555, Batch Gradient Norm after: 22.360675728817352
Epoch 2556/10000, Prediction Accuracy = 53.416%, Loss = 1.1304507732391358
Epoch: 2556, Batch Gradient Norm: 38.37144182628381
Epoch: 2556, Batch Gradient Norm after: 22.36067852657157
Epoch 2557/10000, Prediction Accuracy = 53.414%, Loss = 1.1234751224517823
Epoch: 2557, Batch Gradient Norm: 40.837058276187335
Epoch: 2557, Batch Gradient Norm after: 22.36067824352868
Epoch 2558/10000, Prediction Accuracy = 53.434000000000005%, Loss = 1.130105972290039
Epoch: 2558, Batch Gradient Norm: 38.35816717175156
Epoch: 2558, Batch Gradient Norm after: 22.360677565019987
Epoch 2559/10000, Prediction Accuracy = 53.416%, Loss = 1.1230676889419555
Epoch: 2559, Batch Gradient Norm: 40.845587484962586
Epoch: 2559, Batch Gradient Norm after: 22.36067725587721
Epoch 2560/10000, Prediction Accuracy = 53.446000000000005%, Loss = 1.129765248298645
Epoch: 2560, Batch Gradient Norm: 38.34306185476464
Epoch: 2560, Batch Gradient Norm after: 22.36067809869492
Epoch 2561/10000, Prediction Accuracy = 53.436%, Loss = 1.1226623058319092
Epoch: 2561, Batch Gradient Norm: 40.85431066975948
Epoch: 2561, Batch Gradient Norm after: 22.360677494821946
Epoch 2562/10000, Prediction Accuracy = 53.452%, Loss = 1.1294169187545777
Epoch: 2562, Batch Gradient Norm: 38.32799758519163
Epoch: 2562, Batch Gradient Norm after: 22.36067591513143
Epoch 2563/10000, Prediction Accuracy = 53.436%, Loss = 1.1222618579864503
Epoch: 2563, Batch Gradient Norm: 40.861639022936416
Epoch: 2563, Batch Gradient Norm after: 22.360677218436262
Epoch 2564/10000, Prediction Accuracy = 53.464%, Loss = 1.1290761947631835
Epoch: 2564, Batch Gradient Norm: 38.313973220367316
Epoch: 2564, Batch Gradient Norm after: 22.360677516266012
Epoch 2565/10000, Prediction Accuracy = 53.42999999999999%, Loss = 1.1218627452850343
Epoch: 2565, Batch Gradient Norm: 40.8706523702961
Epoch: 2565, Batch Gradient Norm after: 22.360677052110955
Epoch 2566/10000, Prediction Accuracy = 53.48199999999999%, Loss = 1.1287320613861085
Epoch: 2566, Batch Gradient Norm: 38.30265500137778
Epoch: 2566, Batch Gradient Norm after: 22.36067801005026
Epoch 2567/10000, Prediction Accuracy = 53.446000000000005%, Loss = 1.1214548349380493
Epoch: 2567, Batch Gradient Norm: 40.88357412478233
Epoch: 2567, Batch Gradient Norm after: 22.360677256159256
Epoch 2568/10000, Prediction Accuracy = 53.48199999999999%, Loss = 1.1283914566040039
Epoch: 2568, Batch Gradient Norm: 38.290632014363
Epoch: 2568, Batch Gradient Norm after: 22.360676261983723
Epoch 2569/10000, Prediction Accuracy = 53.45399999999999%, Loss = 1.1210514783859253
Epoch: 2569, Batch Gradient Norm: 40.89120083648799
Epoch: 2569, Batch Gradient Norm after: 22.36067661784569
Epoch 2570/10000, Prediction Accuracy = 53.483999999999995%, Loss = 1.1280423879623414
Epoch: 2570, Batch Gradient Norm: 38.27821398500683
Epoch: 2570, Batch Gradient Norm after: 22.36067597915423
Epoch 2571/10000, Prediction Accuracy = 53.465999999999994%, Loss = 1.1206559419631958
Epoch: 2571, Batch Gradient Norm: 40.894671731779546
Epoch: 2571, Batch Gradient Norm after: 22.360676897512352
Epoch 2572/10000, Prediction Accuracy = 53.488%, Loss = 1.1276795148849488
Epoch: 2572, Batch Gradient Norm: 38.264934304175576
Epoch: 2572, Batch Gradient Norm after: 22.360677012896293
Epoch 2573/10000, Prediction Accuracy = 53.48199999999999%, Loss = 1.1202568054199218
Epoch: 2573, Batch Gradient Norm: 40.89607560382839
Epoch: 2573, Batch Gradient Norm after: 22.36067993175736
Epoch 2574/10000, Prediction Accuracy = 53.495999999999995%, Loss = 1.127321481704712
Epoch: 2574, Batch Gradient Norm: 38.25568172144565
Epoch: 2574, Batch Gradient Norm after: 22.36067560250177
Epoch 2575/10000, Prediction Accuracy = 53.476%, Loss = 1.1198596239089966
Epoch: 2575, Batch Gradient Norm: 40.89955790392832
Epoch: 2575, Batch Gradient Norm after: 22.36067890195968
Epoch 2576/10000, Prediction Accuracy = 53.512%, Loss = 1.1269606113433839
Epoch: 2576, Batch Gradient Norm: 38.24008250245452
Epoch: 2576, Batch Gradient Norm after: 22.360677401147584
Epoch 2577/10000, Prediction Accuracy = 53.48199999999999%, Loss = 1.1194650888442994
Epoch: 2577, Batch Gradient Norm: 40.91034342436214
Epoch: 2577, Batch Gradient Norm after: 22.360676000296504
Epoch 2578/10000, Prediction Accuracy = 53.516%, Loss = 1.1266133069992066
Epoch: 2578, Batch Gradient Norm: 38.227059670552286
Epoch: 2578, Batch Gradient Norm after: 22.36067695186059
Epoch 2579/10000, Prediction Accuracy = 53.48%, Loss = 1.119064950942993
Epoch: 2579, Batch Gradient Norm: 40.917110637627076
Epoch: 2579, Batch Gradient Norm after: 22.36067773257204
Epoch 2580/10000, Prediction Accuracy = 53.528%, Loss = 1.1262561082839966
Epoch: 2580, Batch Gradient Norm: 38.21328295484936
Epoch: 2580, Batch Gradient Norm after: 22.360676892206072
Epoch 2581/10000, Prediction Accuracy = 53.496%, Loss = 1.118667221069336
Epoch: 2581, Batch Gradient Norm: 40.92662990452824
Epoch: 2581, Batch Gradient Norm after: 22.360679025044274
Epoch 2582/10000, Prediction Accuracy = 53.529999999999994%, Loss = 1.1259105205535889
Epoch: 2582, Batch Gradient Norm: 38.20058683891808
Epoch: 2582, Batch Gradient Norm after: 22.36067456641355
Epoch 2583/10000, Prediction Accuracy = 53.495999999999995%, Loss = 1.1182671785354614
Epoch: 2583, Batch Gradient Norm: 40.937043017126484
Epoch: 2583, Batch Gradient Norm after: 22.360677647431313
Epoch 2584/10000, Prediction Accuracy = 53.536%, Loss = 1.125573706626892
Epoch: 2584, Batch Gradient Norm: 38.187952669612606
Epoch: 2584, Batch Gradient Norm after: 22.360676226484625
Epoch 2585/10000, Prediction Accuracy = 53.501999999999995%, Loss = 1.1178632497787475
Epoch: 2585, Batch Gradient Norm: 40.94537785917686
Epoch: 2585, Batch Gradient Norm after: 22.36067841768452
Epoch 2586/10000, Prediction Accuracy = 53.552%, Loss = 1.1252270936965942
Epoch: 2586, Batch Gradient Norm: 38.17672523132054
Epoch: 2586, Batch Gradient Norm after: 22.36067678529117
Epoch 2587/10000, Prediction Accuracy = 53.52%, Loss = 1.1174647808074951
Epoch: 2587, Batch Gradient Norm: 40.94904359034431
Epoch: 2587, Batch Gradient Norm after: 22.360677779559147
Epoch 2588/10000, Prediction Accuracy = 53.553999999999995%, Loss = 1.1248727321624756
Epoch: 2588, Batch Gradient Norm: 38.16529350966392
Epoch: 2588, Batch Gradient Norm after: 22.360678655874075
Epoch 2589/10000, Prediction Accuracy = 53.538%, Loss = 1.1170697927474975
Epoch: 2589, Batch Gradient Norm: 40.946261911745424
Epoch: 2589, Batch Gradient Norm after: 22.360675674422968
Epoch 2590/10000, Prediction Accuracy = 53.565999999999995%, Loss = 1.1245141506195069
Epoch: 2590, Batch Gradient Norm: 38.15558844814101
Epoch: 2590, Batch Gradient Norm after: 22.360678786890773
Epoch 2591/10000, Prediction Accuracy = 53.534000000000006%, Loss = 1.1166764974594117
Epoch: 2591, Batch Gradient Norm: 40.947338190350536
Epoch: 2591, Batch Gradient Norm after: 22.360677484283542
Epoch 2592/10000, Prediction Accuracy = 53.574%, Loss = 1.1241431474685668
Epoch: 2592, Batch Gradient Norm: 38.14662082252502
Epoch: 2592, Batch Gradient Norm after: 22.360678337410555
Epoch 2593/10000, Prediction Accuracy = 53.552%, Loss = 1.116284155845642
Epoch: 2593, Batch Gradient Norm: 40.94998870224375
Epoch: 2593, Batch Gradient Norm after: 22.360676532098715
Epoch 2594/10000, Prediction Accuracy = 53.57000000000001%, Loss = 1.1237908601760864
Epoch: 2594, Batch Gradient Norm: 38.133362919833644
Epoch: 2594, Batch Gradient Norm after: 22.360676334011824
Epoch 2595/10000, Prediction Accuracy = 53.572%, Loss = 1.1158844470977782
Epoch: 2595, Batch Gradient Norm: 40.95458781329796
Epoch: 2595, Batch Gradient Norm after: 22.360675718252704
Epoch 2596/10000, Prediction Accuracy = 53.565999999999995%, Loss = 1.1234376192092896
Epoch: 2596, Batch Gradient Norm: 38.12523161744121
Epoch: 2596, Batch Gradient Norm after: 22.360676271527776
Epoch 2597/10000, Prediction Accuracy = 53.588%, Loss = 1.115486478805542
Epoch: 2597, Batch Gradient Norm: 40.95777006427744
Epoch: 2597, Batch Gradient Norm after: 22.360679814520992
Epoch 2598/10000, Prediction Accuracy = 53.57199999999999%, Loss = 1.1230802536010742
Epoch: 2598, Batch Gradient Norm: 38.11790593530014
Epoch: 2598, Batch Gradient Norm after: 22.36067813750089
Epoch 2599/10000, Prediction Accuracy = 53.602%, Loss = 1.1150872707366943
Epoch: 2599, Batch Gradient Norm: 40.95085317606624
Epoch: 2599, Batch Gradient Norm after: 22.36067758835147
Epoch 2600/10000, Prediction Accuracy = 53.584%, Loss = 1.1226959228515625
Epoch: 2600, Batch Gradient Norm: 38.11077945168914
Epoch: 2600, Batch Gradient Norm after: 22.360678391634263
Epoch 2601/10000, Prediction Accuracy = 53.61%, Loss = 1.1146908283233643
Epoch: 2601, Batch Gradient Norm: 40.944014985627824
Epoch: 2601, Batch Gradient Norm after: 22.36067668536487
Epoch 2602/10000, Prediction Accuracy = 53.586%, Loss = 1.1223046064376831
Epoch: 2602, Batch Gradient Norm: 38.10289715967387
Epoch: 2602, Batch Gradient Norm after: 22.360677945549984
Epoch 2603/10000, Prediction Accuracy = 53.61600000000001%, Loss = 1.1143011808395387
Epoch: 2603, Batch Gradient Norm: 40.9330641132053
Epoch: 2603, Batch Gradient Norm after: 22.36067692853275
Epoch 2604/10000, Prediction Accuracy = 53.577999999999996%, Loss = 1.1219072341918945
Epoch: 2604, Batch Gradient Norm: 38.092527216566594
Epoch: 2604, Batch Gradient Norm after: 22.3606757089392
Epoch 2605/10000, Prediction Accuracy = 53.61400000000001%, Loss = 1.1139126539230346
Epoch: 2605, Batch Gradient Norm: 40.92527459776318
Epoch: 2605, Batch Gradient Norm after: 22.360674628328628
Epoch 2606/10000, Prediction Accuracy = 53.593999999999994%, Loss = 1.1215213775634765
Epoch: 2606, Batch Gradient Norm: 38.08427902220462
Epoch: 2606, Batch Gradient Norm after: 22.360677456070114
Epoch 2607/10000, Prediction Accuracy = 53.63000000000001%, Loss = 1.1135179281234742
Epoch: 2607, Batch Gradient Norm: 40.92328564901025
Epoch: 2607, Batch Gradient Norm after: 22.36067744210708
Epoch 2608/10000, Prediction Accuracy = 53.604%, Loss = 1.121137523651123
Epoch: 2608, Batch Gradient Norm: 38.07429667910696
Epoch: 2608, Batch Gradient Norm after: 22.360677545312
Epoch 2609/10000, Prediction Accuracy = 53.638%, Loss = 1.1131291151046754
Epoch: 2609, Batch Gradient Norm: 40.919811722934995
Epoch: 2609, Batch Gradient Norm after: 22.360677021615437
Epoch 2610/10000, Prediction Accuracy = 53.61800000000001%, Loss = 1.1207580089569091
Epoch: 2610, Batch Gradient Norm: 38.064926415237395
Epoch: 2610, Batch Gradient Norm after: 22.360677947330455
Epoch 2611/10000, Prediction Accuracy = 53.646%, Loss = 1.1127364873886108
Epoch: 2611, Batch Gradient Norm: 40.914878048486685
Epoch: 2611, Batch Gradient Norm after: 22.36067860787024
Epoch 2612/10000, Prediction Accuracy = 53.629999999999995%, Loss = 1.1203724145889282
Epoch: 2612, Batch Gradient Norm: 38.05661323712805
Epoch: 2612, Batch Gradient Norm after: 22.36067720774787
Epoch 2613/10000, Prediction Accuracy = 53.656000000000006%, Loss = 1.1123454570770264
Epoch: 2613, Batch Gradient Norm: 40.91346462998572
Epoch: 2613, Batch Gradient Norm after: 22.360675353785325
Epoch 2614/10000, Prediction Accuracy = 53.64%, Loss = 1.1200040340423585
Epoch: 2614, Batch Gradient Norm: 38.04540552903361
Epoch: 2614, Batch Gradient Norm after: 22.360679713757982
Epoch 2615/10000, Prediction Accuracy = 53.67%, Loss = 1.1119559764862061
Epoch: 2615, Batch Gradient Norm: 40.91884272813201
Epoch: 2615, Batch Gradient Norm after: 22.360677126751877
Epoch 2616/10000, Prediction Accuracy = 53.648%, Loss = 1.1196558952331543
Epoch: 2616, Batch Gradient Norm: 38.03766167848969
Epoch: 2616, Batch Gradient Norm after: 22.360678737680637
Epoch 2617/10000, Prediction Accuracy = 53.676%, Loss = 1.1115602016448975
Epoch: 2617, Batch Gradient Norm: 40.9210695018827
Epoch: 2617, Batch Gradient Norm after: 22.360678325740334
Epoch 2618/10000, Prediction Accuracy = 53.664%, Loss = 1.1192988634109498
Epoch: 2618, Batch Gradient Norm: 38.026789777571665
Epoch: 2618, Batch Gradient Norm after: 22.360677307306393
Epoch 2619/10000, Prediction Accuracy = 53.68000000000001%, Loss = 1.1111687183380128
Epoch: 2619, Batch Gradient Norm: 40.92331310586379
Epoch: 2619, Batch Gradient Norm after: 22.360677779811322
Epoch 2620/10000, Prediction Accuracy = 53.672000000000004%, Loss = 1.1189497470855714
Epoch: 2620, Batch Gradient Norm: 38.0178034457127
Epoch: 2620, Batch Gradient Norm after: 22.36067963678675
Epoch 2621/10000, Prediction Accuracy = 53.698%, Loss = 1.1107783794403077
Epoch: 2621, Batch Gradient Norm: 40.91946842916222
Epoch: 2621, Batch Gradient Norm after: 22.360676863166525
Epoch 2622/10000, Prediction Accuracy = 53.684000000000005%, Loss = 1.1185835361480714
Epoch: 2622, Batch Gradient Norm: 38.0086118118677
Epoch: 2622, Batch Gradient Norm after: 22.36067832413852
Epoch 2623/10000, Prediction Accuracy = 53.70399999999999%, Loss = 1.1103907346725463
Epoch: 2623, Batch Gradient Norm: 40.90658476312517
Epoch: 2623, Batch Gradient Norm after: 22.360678158120326
Epoch 2624/10000, Prediction Accuracy = 53.69199999999999%, Loss = 1.1181824445724486
Epoch: 2624, Batch Gradient Norm: 38.00166729328574
Epoch: 2624, Batch Gradient Norm after: 22.360677994484117
Epoch 2625/10000, Prediction Accuracy = 53.712%, Loss = 1.1100083589553833
Epoch: 2625, Batch Gradient Norm: 40.894170754892926
Epoch: 2625, Batch Gradient Norm after: 22.36067851443556
Epoch 2626/10000, Prediction Accuracy = 53.712%, Loss = 1.117777442932129
Epoch: 2626, Batch Gradient Norm: 37.995384102251066
Epoch: 2626, Batch Gradient Norm after: 22.36067863025103
Epoch 2627/10000, Prediction Accuracy = 53.715999999999994%, Loss = 1.1096229076385498
Epoch: 2627, Batch Gradient Norm: 40.88176991301737
Epoch: 2627, Batch Gradient Norm after: 22.36067804587524
Epoch 2628/10000, Prediction Accuracy = 53.722%, Loss = 1.1173850536346435
Epoch: 2628, Batch Gradient Norm: 37.98755139231959
Epoch: 2628, Batch Gradient Norm after: 22.360678961841256
Epoch 2629/10000, Prediction Accuracy = 53.726%, Loss = 1.1092408657073975
Epoch: 2629, Batch Gradient Norm: 40.87301619659717
Epoch: 2629, Batch Gradient Norm after: 22.360679222408965
Epoch 2630/10000, Prediction Accuracy = 53.71999999999999%, Loss = 1.116990852355957
Epoch: 2630, Batch Gradient Norm: 37.97826697660347
Epoch: 2630, Batch Gradient Norm after: 22.360679769846225
Epoch 2631/10000, Prediction Accuracy = 53.739999999999995%, Loss = 1.1088582277297974
Epoch: 2631, Batch Gradient Norm: 40.86700085556193
Epoch: 2631, Batch Gradient Norm after: 22.360677018721386
Epoch 2632/10000, Prediction Accuracy = 53.73%, Loss = 1.1166095972061156
Epoch: 2632, Batch Gradient Norm: 37.97055900876828
Epoch: 2632, Batch Gradient Norm after: 22.360678510467583
Epoch 2633/10000, Prediction Accuracy = 53.748000000000005%, Loss = 1.1084731340408325
Epoch: 2633, Batch Gradient Norm: 40.86208379729816
Epoch: 2633, Batch Gradient Norm after: 22.36067919419941
Epoch 2634/10000, Prediction Accuracy = 53.74400000000001%, Loss = 1.1162327766418456
Epoch: 2634, Batch Gradient Norm: 37.960961258228835
Epoch: 2634, Batch Gradient Norm after: 22.36067936602308
Epoch 2635/10000, Prediction Accuracy = 53.760000000000005%, Loss = 1.1080914735794067
Epoch: 2635, Batch Gradient Norm: 40.85461054466552
Epoch: 2635, Batch Gradient Norm after: 22.36067647227048
Epoch 2636/10000, Prediction Accuracy = 53.739999999999995%, Loss = 1.1158565282821655
Epoch: 2636, Batch Gradient Norm: 37.94997123174974
Epoch: 2636, Batch Gradient Norm after: 22.360679317387888
Epoch 2637/10000, Prediction Accuracy = 53.767999999999994%, Loss = 1.107710313796997
Epoch: 2637, Batch Gradient Norm: 40.84744840600343
Epoch: 2637, Batch Gradient Norm after: 22.36067844925867
Epoch 2638/10000, Prediction Accuracy = 53.742%, Loss = 1.115475583076477
Epoch: 2638, Batch Gradient Norm: 37.94268781261797
Epoch: 2638, Batch Gradient Norm after: 22.360679454838735
Epoch 2639/10000, Prediction Accuracy = 53.774%, Loss = 1.1073302268981933
Epoch: 2639, Batch Gradient Norm: 40.83789072184765
Epoch: 2639, Batch Gradient Norm after: 22.360677445853856
Epoch 2640/10000, Prediction Accuracy = 53.748000000000005%, Loss = 1.1150844812393188
Epoch: 2640, Batch Gradient Norm: 37.93366760490943
Epoch: 2640, Batch Gradient Norm after: 22.360677776504033
Epoch 2641/10000, Prediction Accuracy = 53.778%, Loss = 1.1069456338882446
Epoch: 2641, Batch Gradient Norm: 40.82753215077572
Epoch: 2641, Batch Gradient Norm after: 22.360678627897045
Epoch 2642/10000, Prediction Accuracy = 53.762%, Loss = 1.1146958589553833
Epoch: 2642, Batch Gradient Norm: 37.92470750062268
Epoch: 2642, Batch Gradient Norm after: 22.360678759235803
Epoch 2643/10000, Prediction Accuracy = 53.775999999999996%, Loss = 1.1065637826919557
Epoch: 2643, Batch Gradient Norm: 40.81755770088178
Epoch: 2643, Batch Gradient Norm after: 22.360677056994295
Epoch 2644/10000, Prediction Accuracy = 53.782%, Loss = 1.1143037796020507
Epoch: 2644, Batch Gradient Norm: 37.915531851809575
Epoch: 2644, Batch Gradient Norm after: 22.360678931871618
Epoch 2645/10000, Prediction Accuracy = 53.786%, Loss = 1.1061846494674683
Epoch: 2645, Batch Gradient Norm: 40.79938109752683
Epoch: 2645, Batch Gradient Norm after: 22.360677899553465
Epoch 2646/10000, Prediction Accuracy = 53.79600000000001%, Loss = 1.113898253440857
Epoch: 2646, Batch Gradient Norm: 37.90840807911431
Epoch: 2646, Batch Gradient Norm after: 22.360676094230023
Epoch 2647/10000, Prediction Accuracy = 53.79%, Loss = 1.1058005094528198
Epoch: 2647, Batch Gradient Norm: 40.78380997394761
Epoch: 2647, Batch Gradient Norm after: 22.360677852763303
Epoch 2648/10000, Prediction Accuracy = 53.806%, Loss = 1.1134984493255615
Epoch: 2648, Batch Gradient Norm: 37.900716714045444
Epoch: 2648, Batch Gradient Norm after: 22.360677049888814
Epoch 2649/10000, Prediction Accuracy = 53.79%, Loss = 1.105419135093689
Epoch: 2649, Batch Gradient Norm: 40.76979860236058
Epoch: 2649, Batch Gradient Norm after: 22.36067879686154
Epoch 2650/10000, Prediction Accuracy = 53.814%, Loss = 1.1131015062332152
Epoch: 2650, Batch Gradient Norm: 37.893155759993206
Epoch: 2650, Batch Gradient Norm after: 22.360677703757432
Epoch 2651/10000, Prediction Accuracy = 53.803999999999995%, Loss = 1.1050397157669067
Epoch: 2651, Batch Gradient Norm: 40.75679226703334
Epoch: 2651, Batch Gradient Norm after: 22.360678321169875
Epoch 2652/10000, Prediction Accuracy = 53.82000000000001%, Loss = 1.1127058267593384
Epoch: 2652, Batch Gradient Norm: 37.88409729222663
Epoch: 2652, Batch Gradient Norm after: 22.36067602416095
Epoch 2653/10000, Prediction Accuracy = 53.818%, Loss = 1.104664397239685
Epoch: 2653, Batch Gradient Norm: 40.735198533009175
Epoch: 2653, Batch Gradient Norm after: 22.36067801277351
Epoch 2654/10000, Prediction Accuracy = 53.831999999999994%, Loss = 1.112288498878479
Epoch: 2654, Batch Gradient Norm: 37.874219212951864
Epoch: 2654, Batch Gradient Norm after: 22.360678518819277
Epoch 2655/10000, Prediction Accuracy = 53.834%, Loss = 1.1042871952056885
Epoch: 2655, Batch Gradient Norm: 40.71520654992752
Epoch: 2655, Batch Gradient Norm after: 22.360676796435502
Epoch 2656/10000, Prediction Accuracy = 53.827999999999996%, Loss = 1.1118804931640625
Epoch: 2656, Batch Gradient Norm: 37.86683698247782
Epoch: 2656, Batch Gradient Norm after: 22.36067802656016
Epoch 2657/10000, Prediction Accuracy = 53.838%, Loss = 1.1039101123809814
Epoch: 2657, Batch Gradient Norm: 40.68879299416558
Epoch: 2657, Batch Gradient Norm after: 22.36067893502588
Epoch 2658/10000, Prediction Accuracy = 53.84400000000001%, Loss = 1.1114677906036377
Epoch: 2658, Batch Gradient Norm: 37.8595069117462
Epoch: 2658, Batch Gradient Norm after: 22.36067759525486
Epoch 2659/10000, Prediction Accuracy = 53.852%, Loss = 1.1035300970077515
Epoch: 2659, Batch Gradient Norm: 40.67863754775609
Epoch: 2659, Batch Gradient Norm after: 22.36067809332131
Epoch 2660/10000, Prediction Accuracy = 53.85799999999999%, Loss = 1.1110743045806886
Epoch: 2660, Batch Gradient Norm: 37.85197638452133
Epoch: 2660, Batch Gradient Norm after: 22.360678765276152
Epoch 2661/10000, Prediction Accuracy = 53.864%, Loss = 1.1031513690948487
Epoch: 2661, Batch Gradient Norm: 40.66599081677288
Epoch: 2661, Batch Gradient Norm after: 22.360678511702822
Epoch 2662/10000, Prediction Accuracy = 53.86999999999999%, Loss = 1.1106965303421021
Epoch: 2662, Batch Gradient Norm: 37.844057828913265
Epoch: 2662, Batch Gradient Norm after: 22.360678395451803
Epoch 2663/10000, Prediction Accuracy = 53.872%, Loss = 1.1027678966522216
Epoch: 2663, Batch Gradient Norm: 40.65149569477776
Epoch: 2663, Batch Gradient Norm after: 22.360681086354777
Epoch 2664/10000, Prediction Accuracy = 53.879999999999995%, Loss = 1.1102956771850585
Epoch: 2664, Batch Gradient Norm: 37.83713594378642
Epoch: 2664, Batch Gradient Norm after: 22.36067777778383
Epoch 2665/10000, Prediction Accuracy = 53.86999999999999%, Loss = 1.1023996353149415
Epoch: 2665, Batch Gradient Norm: 40.639703767725386
Epoch: 2665, Batch Gradient Norm after: 22.3606783677859
Epoch 2666/10000, Prediction Accuracy = 53.88000000000001%, Loss = 1.109912395477295
Epoch: 2666, Batch Gradient Norm: 37.82783036448891
Epoch: 2666, Batch Gradient Norm after: 22.360676141278716
Epoch 2667/10000, Prediction Accuracy = 53.89399999999999%, Loss = 1.1020219326019287
Epoch: 2667, Batch Gradient Norm: 40.636669784819404
Epoch: 2667, Batch Gradient Norm after: 22.3606784172503
Epoch 2668/10000, Prediction Accuracy = 53.886%, Loss = 1.1095579385757446
Epoch: 2668, Batch Gradient Norm: 37.8167359729761
Epoch: 2668, Batch Gradient Norm after: 22.360678234094987
Epoch 2669/10000, Prediction Accuracy = 53.903999999999996%, Loss = 1.1016446113586427
Epoch: 2669, Batch Gradient Norm: 40.63471121845273
Epoch: 2669, Batch Gradient Norm after: 22.360676623253838
Epoch 2670/10000, Prediction Accuracy = 53.891999999999996%, Loss = 1.1091919183731078
Epoch: 2670, Batch Gradient Norm: 37.807042998922675
Epoch: 2670, Batch Gradient Norm after: 22.36067541380236
Epoch 2671/10000, Prediction Accuracy = 53.90599999999999%, Loss = 1.101268720626831
Epoch: 2671, Batch Gradient Norm: 40.63241448936534
Epoch: 2671, Batch Gradient Norm after: 22.360676981239568
Epoch 2672/10000, Prediction Accuracy = 53.89%, Loss = 1.1088264465332032
Epoch: 2672, Batch Gradient Norm: 37.79729188750049
Epoch: 2672, Batch Gradient Norm after: 22.360677525557932
Epoch 2673/10000, Prediction Accuracy = 53.90599999999999%, Loss = 1.100891613960266
Epoch: 2673, Batch Gradient Norm: 40.6272301438869
Epoch: 2673, Batch Gradient Norm after: 22.360676197390884
Epoch 2674/10000, Prediction Accuracy = 53.894000000000005%, Loss = 1.1084579706192017
Epoch: 2674, Batch Gradient Norm: 37.791033286631205
Epoch: 2674, Batch Gradient Norm after: 22.36067693748726
Epoch 2675/10000, Prediction Accuracy = 53.903999999999996%, Loss = 1.1005171298980714
Epoch: 2675, Batch Gradient Norm: 40.61883412886765
Epoch: 2675, Batch Gradient Norm after: 22.3606793655134
Epoch 2676/10000, Prediction Accuracy = 53.898%, Loss = 1.1080874443054198
Epoch: 2676, Batch Gradient Norm: 37.78015020061811
Epoch: 2676, Batch Gradient Norm after: 22.360676969814314
Epoch 2677/10000, Prediction Accuracy = 53.907999999999994%, Loss = 1.100144338607788
Epoch: 2677, Batch Gradient Norm: 40.611740999716474
Epoch: 2677, Batch Gradient Norm after: 22.360678127700293
Epoch 2678/10000, Prediction Accuracy = 53.914%, Loss = 1.107727861404419
Epoch: 2678, Batch Gradient Norm: 37.76666274491451
Epoch: 2678, Batch Gradient Norm after: 22.360677986620445
Epoch 2679/10000, Prediction Accuracy = 53.912%, Loss = 1.0997701644897462
Epoch: 2679, Batch Gradient Norm: 40.60411576228718
Epoch: 2679, Batch Gradient Norm after: 22.360676815978774
Epoch 2680/10000, Prediction Accuracy = 53.926%, Loss = 1.1073558330535889
Epoch: 2680, Batch Gradient Norm: 37.75900239926764
Epoch: 2680, Batch Gradient Norm after: 22.36067845236417
Epoch 2681/10000, Prediction Accuracy = 53.914%, Loss = 1.0993908643722534
Epoch: 2681, Batch Gradient Norm: 40.596885550059035
Epoch: 2681, Batch Gradient Norm after: 22.360677253182548
Epoch 2682/10000, Prediction Accuracy = 53.92999999999999%, Loss = 1.1069877862930297
Epoch: 2682, Batch Gradient Norm: 37.7508817047737
Epoch: 2682, Batch Gradient Norm after: 22.360678199438116
Epoch 2683/10000, Prediction Accuracy = 53.936%, Loss = 1.099016284942627
Epoch: 2683, Batch Gradient Norm: 40.588382754907684
Epoch: 2683, Batch Gradient Norm after: 22.360675742350768
Epoch 2684/10000, Prediction Accuracy = 53.94%, Loss = 1.1066130161285401
Epoch: 2684, Batch Gradient Norm: 37.74187914060018
Epoch: 2684, Batch Gradient Norm after: 22.36067746353931
Epoch 2685/10000, Prediction Accuracy = 53.94200000000001%, Loss = 1.0986420869827271
Epoch: 2685, Batch Gradient Norm: 40.58376298668674
Epoch: 2685, Batch Gradient Norm after: 22.360675859527426
Epoch 2686/10000, Prediction Accuracy = 53.956%, Loss = 1.1062462329864502
Epoch: 2686, Batch Gradient Norm: 37.73202049517268
Epoch: 2686, Batch Gradient Norm after: 22.360680107015728
Epoch 2687/10000, Prediction Accuracy = 53.936%, Loss = 1.0982666969299317
Epoch: 2687, Batch Gradient Norm: 40.580428306978625
Epoch: 2687, Batch Gradient Norm after: 22.360676576392127
Epoch 2688/10000, Prediction Accuracy = 53.962%, Loss = 1.1058955430984496
Epoch: 2688, Batch Gradient Norm: 37.7230954474916
Epoch: 2688, Batch Gradient Norm after: 22.360677690726426
Epoch 2689/10000, Prediction Accuracy = 53.95399999999999%, Loss = 1.0978917598724365
Epoch: 2689, Batch Gradient Norm: 40.579580140812844
Epoch: 2689, Batch Gradient Norm after: 22.36067659364195
Epoch 2690/10000, Prediction Accuracy = 53.962%, Loss = 1.10553982257843
Epoch: 2690, Batch Gradient Norm: 37.71010749263519
Epoch: 2690, Batch Gradient Norm after: 22.360677812141436
Epoch 2691/10000, Prediction Accuracy = 53.96600000000001%, Loss = 1.0975163221359252
Epoch: 2691, Batch Gradient Norm: 40.56858848512113
Epoch: 2691, Batch Gradient Norm after: 22.360675834374923
Epoch 2692/10000, Prediction Accuracy = 53.962%, Loss = 1.1051822185516358
Epoch: 2692, Batch Gradient Norm: 37.70196330693479
Epoch: 2692, Batch Gradient Norm after: 22.360679611885462
Epoch 2693/10000, Prediction Accuracy = 53.98%, Loss = 1.097149634361267
Epoch: 2693, Batch Gradient Norm: 40.56035358491591
Epoch: 2693, Batch Gradient Norm after: 22.36067795123217
Epoch 2694/10000, Prediction Accuracy = 53.96600000000001%, Loss = 1.1048059463500977
Epoch: 2694, Batch Gradient Norm: 37.69320244813124
Epoch: 2694, Batch Gradient Norm after: 22.360678520837396
Epoch 2695/10000, Prediction Accuracy = 53.986000000000004%, Loss = 1.0967763185501098
Epoch: 2695, Batch Gradient Norm: 40.54264941365201
Epoch: 2695, Batch Gradient Norm after: 22.360675487449143
Epoch 2696/10000, Prediction Accuracy = 53.976%, Loss = 1.104411506652832
Epoch: 2696, Batch Gradient Norm: 37.68710035258062
Epoch: 2696, Batch Gradient Norm after: 22.360678250499316
Epoch 2697/10000, Prediction Accuracy = 53.998000000000005%, Loss = 1.096401858329773
Epoch: 2697, Batch Gradient Norm: 40.527265831688574
Epoch: 2697, Batch Gradient Norm after: 22.360675516111204
Epoch 2698/10000, Prediction Accuracy = 53.986000000000004%, Loss = 1.1040201425552367
Epoch: 2698, Batch Gradient Norm: 37.679137801890974
Epoch: 2698, Batch Gradient Norm after: 22.36067639878089
Epoch 2699/10000, Prediction Accuracy = 54.013999999999996%, Loss = 1.0960200071334838
Epoch: 2699, Batch Gradient Norm: 40.51970029165193
Epoch: 2699, Batch Gradient Norm after: 22.360674969132113
Epoch 2700/10000, Prediction Accuracy = 54.00599999999999%, Loss = 1.103647518157959
Epoch: 2700, Batch Gradient Norm: 37.671695478888594
Epoch: 2700, Batch Gradient Norm after: 22.360676892495764
Epoch 2701/10000, Prediction Accuracy = 54.032%, Loss = 1.0956466913223266
Epoch: 2701, Batch Gradient Norm: 40.51000079399659
Epoch: 2701, Batch Gradient Norm after: 22.360675142281117
Epoch 2702/10000, Prediction Accuracy = 54.01800000000001%, Loss = 1.103271746635437
Epoch: 2702, Batch Gradient Norm: 37.65959686397042
Epoch: 2702, Batch Gradient Norm after: 22.36067679652596
Epoch 2703/10000, Prediction Accuracy = 54.041999999999994%, Loss = 1.0952787160873414
Epoch: 2703, Batch Gradient Norm: 40.5041235169499
Epoch: 2703, Batch Gradient Norm after: 22.360677142147665
Epoch 2704/10000, Prediction Accuracy = 54.028%, Loss = 1.1029226541519166
Epoch: 2704, Batch Gradient Norm: 37.64943582059732
Epoch: 2704, Batch Gradient Norm after: 22.3606795218291
Epoch 2705/10000, Prediction Accuracy = 54.041999999999994%, Loss = 1.09491024017334
Epoch: 2705, Batch Gradient Norm: 40.50298500766259
Epoch: 2705, Batch Gradient Norm after: 22.360675553482668
Epoch 2706/10000, Prediction Accuracy = 54.041999999999994%, Loss = 1.102581787109375
Epoch: 2706, Batch Gradient Norm: 37.638188065051885
Epoch: 2706, Batch Gradient Norm after: 22.360677824710514
Epoch 2707/10000, Prediction Accuracy = 54.05%, Loss = 1.0945389032363892
Epoch: 2707, Batch Gradient Norm: 40.498186647913464
Epoch: 2707, Batch Gradient Norm after: 22.36067588098656
Epoch 2708/10000, Prediction Accuracy = 54.05%, Loss = 1.1022156476974487
Epoch: 2708, Batch Gradient Norm: 37.62704824088786
Epoch: 2708, Batch Gradient Norm after: 22.360677135969585
Epoch 2709/10000, Prediction Accuracy = 54.052%, Loss = 1.0941700220108033
Epoch: 2709, Batch Gradient Norm: 40.49327063570353
Epoch: 2709, Batch Gradient Norm after: 22.360676076992135
Epoch 2710/10000, Prediction Accuracy = 54.05%, Loss = 1.101853370666504
Epoch: 2710, Batch Gradient Norm: 37.615314028504656
Epoch: 2710, Batch Gradient Norm after: 22.360677500024018
Epoch 2711/10000, Prediction Accuracy = 54.065999999999995%, Loss = 1.0938055992126465
Epoch: 2711, Batch Gradient Norm: 40.476885780774154
Epoch: 2711, Batch Gradient Norm after: 22.360677450312313
Epoch 2712/10000, Prediction Accuracy = 54.064%, Loss = 1.101471471786499
Epoch: 2712, Batch Gradient Norm: 37.605321103137925
Epoch: 2712, Batch Gradient Norm after: 22.36067660381477
Epoch 2713/10000, Prediction Accuracy = 54.07000000000001%, Loss = 1.0934379816055297
Epoch: 2713, Batch Gradient Norm: 40.467208575525085
Epoch: 2713, Batch Gradient Norm after: 22.360674856386133
Epoch 2714/10000, Prediction Accuracy = 54.074%, Loss = 1.1011003017425538
Epoch: 2714, Batch Gradient Norm: 37.59469641932411
Epoch: 2714, Batch Gradient Norm after: 22.360677093007098
Epoch 2715/10000, Prediction Accuracy = 54.08%, Loss = 1.0930708408355714
Epoch: 2715, Batch Gradient Norm: 40.44806839672863
Epoch: 2715, Batch Gradient Norm after: 22.360678140670466
Epoch 2716/10000, Prediction Accuracy = 54.088%, Loss = 1.1007104396820069
Epoch: 2716, Batch Gradient Norm: 37.585943334241804
Epoch: 2716, Batch Gradient Norm after: 22.360678282466516
Epoch 2717/10000, Prediction Accuracy = 54.089999999999996%, Loss = 1.0927054643630982
Epoch: 2717, Batch Gradient Norm: 40.427861605554
Epoch: 2717, Batch Gradient Norm after: 22.360676237942556
Epoch 2718/10000, Prediction Accuracy = 54.104%, Loss = 1.1003139972686768
Epoch: 2718, Batch Gradient Norm: 37.57661760200195
Epoch: 2718, Batch Gradient Norm after: 22.360679291854513
Epoch 2719/10000, Prediction Accuracy = 54.11%, Loss = 1.0923405408859252
Epoch: 2719, Batch Gradient Norm: 40.41116874914015
Epoch: 2719, Batch Gradient Norm after: 22.36067789746194
Epoch 2720/10000, Prediction Accuracy = 54.114%, Loss = 1.0999334573745727
Epoch: 2720, Batch Gradient Norm: 37.56897958054614
Epoch: 2720, Batch Gradient Norm after: 22.36067809311086
Epoch 2721/10000, Prediction Accuracy = 54.134%, Loss = 1.091975164413452
Epoch: 2721, Batch Gradient Norm: 40.390402576395864
Epoch: 2721, Batch Gradient Norm after: 22.36067663669794
Epoch 2722/10000, Prediction Accuracy = 54.124%, Loss = 1.0995325088500976
Epoch: 2722, Batch Gradient Norm: 37.56020983790395
Epoch: 2722, Batch Gradient Norm after: 22.36067778951077
Epoch 2723/10000, Prediction Accuracy = 54.134%, Loss = 1.0916068792343139
Epoch: 2723, Batch Gradient Norm: 40.37154153872651
Epoch: 2723, Batch Gradient Norm after: 22.36067548768385
Epoch 2724/10000, Prediction Accuracy = 54.134%, Loss = 1.0991422653198242
Epoch: 2724, Batch Gradient Norm: 37.55428555963359
Epoch: 2724, Batch Gradient Norm after: 22.360677599994293
Epoch 2725/10000, Prediction Accuracy = 54.148%, Loss = 1.0912433862686157
Epoch: 2725, Batch Gradient Norm: 40.36396226897243
Epoch: 2725, Batch Gradient Norm after: 22.360677647257308
Epoch 2726/10000, Prediction Accuracy = 54.14399999999999%, Loss = 1.0987650632858277
Epoch: 2726, Batch Gradient Norm: 37.54637688967878
Epoch: 2726, Batch Gradient Norm after: 22.36067676412083
Epoch 2727/10000, Prediction Accuracy = 54.152%, Loss = 1.0908715963363647
Epoch: 2727, Batch Gradient Norm: 40.356591721909794
Epoch: 2727, Batch Gradient Norm after: 22.360676764580575
Epoch 2728/10000, Prediction Accuracy = 54.162%, Loss = 1.098405408859253
Epoch: 2728, Batch Gradient Norm: 37.53726113199465
Epoch: 2728, Batch Gradient Norm after: 22.360676948987045
Epoch 2729/10000, Prediction Accuracy = 54.166%, Loss = 1.0905015468597412
Epoch: 2729, Batch Gradient Norm: 40.34264880462015
Epoch: 2729, Batch Gradient Norm after: 22.360677578739136
Epoch 2730/10000, Prediction Accuracy = 54.194%, Loss = 1.0980304718017577
Epoch: 2730, Batch Gradient Norm: 37.52724958449977
Epoch: 2730, Batch Gradient Norm after: 22.360678276824302
Epoch 2731/10000, Prediction Accuracy = 54.169999999999995%, Loss = 1.0901381969451904
Epoch: 2731, Batch Gradient Norm: 40.328118529915244
Epoch: 2731, Batch Gradient Norm after: 22.36067679181756
Epoch 2732/10000, Prediction Accuracy = 54.19199999999999%, Loss = 1.097661304473877
Epoch: 2732, Batch Gradient Norm: 37.51845902903938
Epoch: 2732, Batch Gradient Norm after: 22.36067980393631
Epoch 2733/10000, Prediction Accuracy = 54.182%, Loss = 1.0897810459136963
Epoch: 2733, Batch Gradient Norm: 40.31937733237096
Epoch: 2733, Batch Gradient Norm after: 22.360675828190715
Epoch 2734/10000, Prediction Accuracy = 54.198%, Loss = 1.097309684753418
Epoch: 2734, Batch Gradient Norm: 37.506388251506394
Epoch: 2734, Batch Gradient Norm after: 22.36067961523373
Epoch 2735/10000, Prediction Accuracy = 54.19200000000001%, Loss = 1.0894147872924804
Epoch: 2735, Batch Gradient Norm: 40.30867227471485
Epoch: 2735, Batch Gradient Norm after: 22.36067485919383
Epoch 2736/10000, Prediction Accuracy = 54.20799999999999%, Loss = 1.0969418287277222
Epoch: 2736, Batch Gradient Norm: 37.497978982056146
Epoch: 2736, Batch Gradient Norm after: 22.360676985901563
Epoch 2737/10000, Prediction Accuracy = 54.205999999999996%, Loss = 1.0890541791915893
Epoch: 2737, Batch Gradient Norm: 40.29899593755855
Epoch: 2737, Batch Gradient Norm after: 22.360676356380075
Epoch 2738/10000, Prediction Accuracy = 54.212%, Loss = 1.096574854850769
Epoch: 2738, Batch Gradient Norm: 37.48751876938983
Epoch: 2738, Batch Gradient Norm after: 22.36067834087964
Epoch 2739/10000, Prediction Accuracy = 54.214%, Loss = 1.0886876344680787
Epoch: 2739, Batch Gradient Norm: 40.29326807692071
Epoch: 2739, Batch Gradient Norm after: 22.360675062160585
Epoch 2740/10000, Prediction Accuracy = 54.20799999999999%, Loss = 1.0962187051773071
Epoch: 2740, Batch Gradient Norm: 37.47565381527909
Epoch: 2740, Batch Gradient Norm after: 22.36067693860872
Epoch 2741/10000, Prediction Accuracy = 54.226%, Loss = 1.0883232355117798
Epoch: 2741, Batch Gradient Norm: 40.29035744541053
Epoch: 2741, Batch Gradient Norm after: 22.360677388240717
Epoch 2742/10000, Prediction Accuracy = 54.21600000000001%, Loss = 1.095881414413452
Epoch: 2742, Batch Gradient Norm: 37.462540831901734
Epoch: 2742, Batch Gradient Norm after: 22.36067758007557
Epoch 2743/10000, Prediction Accuracy = 54.226%, Loss = 1.0879525184631347
Epoch: 2743, Batch Gradient Norm: 40.293079097560806
Epoch: 2743, Batch Gradient Norm after: 22.360675519840886
Epoch 2744/10000, Prediction Accuracy = 54.226%, Loss = 1.095549726486206
Epoch: 2744, Batch Gradient Norm: 37.451575085694174
Epoch: 2744, Batch Gradient Norm after: 22.360678413494817
Epoch 2745/10000, Prediction Accuracy = 54.248000000000005%, Loss = 1.0875816822052002
Epoch: 2745, Batch Gradient Norm: 40.28437224125301
Epoch: 2745, Batch Gradient Norm after: 22.360675259429275
Epoch 2746/10000, Prediction Accuracy = 54.242%, Loss = 1.0951855897903442
Epoch: 2746, Batch Gradient Norm: 37.443776317569906
Epoch: 2746, Batch Gradient Norm after: 22.36067832007237
Epoch 2747/10000, Prediction Accuracy = 54.263999999999996%, Loss = 1.087226963043213
Epoch: 2747, Batch Gradient Norm: 40.27283034652181
Epoch: 2747, Batch Gradient Norm after: 22.360676846432824
Epoch 2748/10000, Prediction Accuracy = 54.254%, Loss = 1.094799017906189
Epoch: 2748, Batch Gradient Norm: 37.43712235951712
Epoch: 2748, Batch Gradient Norm after: 22.360676820309866
Epoch 2749/10000, Prediction Accuracy = 54.28000000000001%, Loss = 1.0868644714355469
Epoch: 2749, Batch Gradient Norm: 40.26066020874112
Epoch: 2749, Batch Gradient Norm after: 22.360676748669984
Epoch 2750/10000, Prediction Accuracy = 54.27%, Loss = 1.0944278478622436
Epoch: 2750, Batch Gradient Norm: 37.4265117179167
Epoch: 2750, Batch Gradient Norm after: 22.36067956289016
Epoch 2751/10000, Prediction Accuracy = 54.288%, Loss = 1.0865021228790284
Epoch: 2751, Batch Gradient Norm: 40.25317324443439
Epoch: 2751, Batch Gradient Norm after: 22.360677528681755
Epoch 2752/10000, Prediction Accuracy = 54.278%, Loss = 1.0940707683563233
Epoch: 2752, Batch Gradient Norm: 37.41393606492014
Epoch: 2752, Batch Gradient Norm after: 22.36067654107821
Epoch 2753/10000, Prediction Accuracy = 54.29600000000001%, Loss = 1.086135244369507
Epoch: 2753, Batch Gradient Norm: 40.244525169631636
Epoch: 2753, Batch Gradient Norm after: 22.36067676796654
Epoch 2754/10000, Prediction Accuracy = 54.282%, Loss = 1.0937090873718263
Epoch: 2754, Batch Gradient Norm: 37.40640872153192
Epoch: 2754, Batch Gradient Norm after: 22.360679345911294
Epoch 2755/10000, Prediction Accuracy = 54.298%, Loss = 1.0857738256454468
Epoch: 2755, Batch Gradient Norm: 40.226012344909144
Epoch: 2755, Batch Gradient Norm after: 22.360675767643286
Epoch 2756/10000, Prediction Accuracy = 54.286%, Loss = 1.093329954147339
Epoch: 2756, Batch Gradient Norm: 37.39725696320113
Epoch: 2756, Batch Gradient Norm after: 22.36067825957186
Epoch 2757/10000, Prediction Accuracy = 54.30400000000001%, Loss = 1.0854177713394164
Epoch: 2757, Batch Gradient Norm: 40.20746403647942
Epoch: 2757, Batch Gradient Norm after: 22.360675544794944
Epoch 2758/10000, Prediction Accuracy = 54.294000000000004%, Loss = 1.0929401636123657
Epoch: 2758, Batch Gradient Norm: 37.38884303494806
Epoch: 2758, Batch Gradient Norm after: 22.360675970992162
Epoch 2759/10000, Prediction Accuracy = 54.318%, Loss = 1.085061240196228
Epoch: 2759, Batch Gradient Norm: 40.19024712391617
Epoch: 2759, Batch Gradient Norm after: 22.36067535064104
Epoch 2760/10000, Prediction Accuracy = 54.29200000000001%, Loss = 1.0925508260726928
Epoch: 2760, Batch Gradient Norm: 37.38216662949706
Epoch: 2760, Batch Gradient Norm after: 22.360677713266792
Epoch 2761/10000, Prediction Accuracy = 54.324%, Loss = 1.0847016334533692
Epoch: 2761, Batch Gradient Norm: 40.17632294722638
Epoch: 2761, Batch Gradient Norm after: 22.360677156682566
Epoch 2762/10000, Prediction Accuracy = 54.31%, Loss = 1.092178964614868
Epoch: 2762, Batch Gradient Norm: 37.371293736284166
Epoch: 2762, Batch Gradient Norm after: 22.36067845194084
Epoch 2763/10000, Prediction Accuracy = 54.33%, Loss = 1.0843443870544434
Epoch: 2763, Batch Gradient Norm: 40.16517087354734
Epoch: 2763, Batch Gradient Norm after: 22.36067480605422
Epoch 2764/10000, Prediction Accuracy = 54.327999999999996%, Loss = 1.091810393333435
Epoch: 2764, Batch Gradient Norm: 37.36396076902112
Epoch: 2764, Batch Gradient Norm after: 22.360677494697786
Epoch 2765/10000, Prediction Accuracy = 54.339999999999996%, Loss = 1.0839830398559571
Epoch: 2765, Batch Gradient Norm: 40.153366386562496
Epoch: 2765, Batch Gradient Norm after: 22.360676247297384
Epoch 2766/10000, Prediction Accuracy = 54.336%, Loss = 1.091436743736267
Epoch: 2766, Batch Gradient Norm: 37.35558575386165
Epoch: 2766, Batch Gradient Norm after: 22.360678594900488
Epoch 2767/10000, Prediction Accuracy = 54.346000000000004%, Loss = 1.083623743057251
Epoch: 2767, Batch Gradient Norm: 40.1381649367766
Epoch: 2767, Batch Gradient Norm after: 22.36067656365753
Epoch 2768/10000, Prediction Accuracy = 54.348%, Loss = 1.0910611391067504
Epoch: 2768, Batch Gradient Norm: 37.34858028494496
Epoch: 2768, Batch Gradient Norm after: 22.360678510000447
Epoch 2769/10000, Prediction Accuracy = 54.352%, Loss = 1.0832698822021485
Epoch: 2769, Batch Gradient Norm: 40.12115882308549
Epoch: 2769, Batch Gradient Norm after: 22.36067391148892
Epoch 2770/10000, Prediction Accuracy = 54.362%, Loss = 1.0906835556030274
Epoch: 2770, Batch Gradient Norm: 37.339139147069986
Epoch: 2770, Batch Gradient Norm after: 22.36067818617217
Epoch 2771/10000, Prediction Accuracy = 54.372%, Loss = 1.0829144954681396
Epoch: 2771, Batch Gradient Norm: 40.10554466736521
Epoch: 2771, Batch Gradient Norm after: 22.36067588435397
Epoch 2772/10000, Prediction Accuracy = 54.370000000000005%, Loss = 1.0903079986572266
Epoch: 2772, Batch Gradient Norm: 37.33010019291497
Epoch: 2772, Batch Gradient Norm after: 22.360677525655348
Epoch 2773/10000, Prediction Accuracy = 54.378%, Loss = 1.0825562715530395
Epoch: 2773, Batch Gradient Norm: 40.09047044014888
Epoch: 2773, Batch Gradient Norm after: 22.360677486931493
Epoch 2774/10000, Prediction Accuracy = 54.364%, Loss = 1.089934515953064
Epoch: 2774, Batch Gradient Norm: 37.32143670081261
Epoch: 2774, Batch Gradient Norm after: 22.360676895586895
Epoch 2775/10000, Prediction Accuracy = 54.379999999999995%, Loss = 1.0822022676467895
Epoch: 2775, Batch Gradient Norm: 40.07391152172616
Epoch: 2775, Batch Gradient Norm after: 22.360678706360428
Epoch 2776/10000, Prediction Accuracy = 54.376%, Loss = 1.0895670175552368
Epoch: 2776, Batch Gradient Norm: 37.31065286334107
Epoch: 2776, Batch Gradient Norm after: 22.360678256150834
Epoch 2777/10000, Prediction Accuracy = 54.39%, Loss = 1.0818522930145265
Epoch: 2777, Batch Gradient Norm: 40.05883267143481
Epoch: 2777, Batch Gradient Norm after: 22.360676569459738
Epoch 2778/10000, Prediction Accuracy = 54.382000000000005%, Loss = 1.0891942024230956
Epoch: 2778, Batch Gradient Norm: 37.30222266193416
Epoch: 2778, Batch Gradient Norm after: 22.360677915050072
Epoch 2779/10000, Prediction Accuracy = 54.396%, Loss = 1.081499457359314
Epoch: 2779, Batch Gradient Norm: 40.04542257232807
Epoch: 2779, Batch Gradient Norm after: 22.36067525519161
Epoch 2780/10000, Prediction Accuracy = 54.38399999999999%, Loss = 1.0888312101364135
Epoch: 2780, Batch Gradient Norm: 37.29265234394041
Epoch: 2780, Batch Gradient Norm after: 22.36067852785247
Epoch 2781/10000, Prediction Accuracy = 54.408%, Loss = 1.0811429738998413
Epoch: 2781, Batch Gradient Norm: 40.03178976084565
Epoch: 2781, Batch Gradient Norm after: 22.360676501049337
Epoch 2782/10000, Prediction Accuracy = 54.398%, Loss = 1.0884639739990234
Epoch: 2782, Batch Gradient Norm: 37.28308079477776
Epoch: 2782, Batch Gradient Norm after: 22.360676563474915
Epoch 2783/10000, Prediction Accuracy = 54.41799999999999%, Loss = 1.0807857751846313
Epoch: 2783, Batch Gradient Norm: 40.02144900190777
Epoch: 2783, Batch Gradient Norm after: 22.360675466386677
Epoch 2784/10000, Prediction Accuracy = 54.398%, Loss = 1.0881032705307008
Epoch: 2784, Batch Gradient Norm: 37.27272269458938
Epoch: 2784, Batch Gradient Norm after: 22.3606785610764
Epoch 2785/10000, Prediction Accuracy = 54.424%, Loss = 1.0804376602172852
Epoch: 2785, Batch Gradient Norm: 40.006203145868085
Epoch: 2785, Batch Gradient Norm after: 22.360675119739334
Epoch 2786/10000, Prediction Accuracy = 54.416%, Loss = 1.0877331495285034
Epoch: 2786, Batch Gradient Norm: 37.264062219698864
Epoch: 2786, Batch Gradient Norm after: 22.36067757159056
Epoch 2787/10000, Prediction Accuracy = 54.44200000000001%, Loss = 1.0800787210464478
Epoch: 2787, Batch Gradient Norm: 39.99814764639329
Epoch: 2787, Batch Gradient Norm after: 22.360677456412017
Epoch 2788/10000, Prediction Accuracy = 54.42999999999999%, Loss = 1.0873741388320923
Epoch: 2788, Batch Gradient Norm: 37.25356689313891
Epoch: 2788, Batch Gradient Norm after: 22.3606764211379
Epoch 2789/10000, Prediction Accuracy = 54.44200000000001%, Loss = 1.0797265291213989
Epoch: 2789, Batch Gradient Norm: 39.99645170559169
Epoch: 2789, Batch Gradient Norm after: 22.360677385354162
Epoch 2790/10000, Prediction Accuracy = 54.464%, Loss = 1.0870313882827758
Epoch: 2790, Batch Gradient Norm: 37.24150934307478
Epoch: 2790, Batch Gradient Norm after: 22.360676930875037
Epoch 2791/10000, Prediction Accuracy = 54.444%, Loss = 1.079374361038208
Epoch: 2791, Batch Gradient Norm: 39.9884538693544
Epoch: 2791, Batch Gradient Norm after: 22.360675176485827
Epoch 2792/10000, Prediction Accuracy = 54.472%, Loss = 1.0866790294647217
Epoch: 2792, Batch Gradient Norm: 37.23052885370115
Epoch: 2792, Batch Gradient Norm after: 22.36067722315697
Epoch 2793/10000, Prediction Accuracy = 54.456%, Loss = 1.0790213346481323
Epoch: 2793, Batch Gradient Norm: 39.97870391376765
Epoch: 2793, Batch Gradient Norm after: 22.36067590207749
Epoch 2794/10000, Prediction Accuracy = 54.488%, Loss = 1.0863240242004395
Epoch: 2794, Batch Gradient Norm: 37.2232399249079
Epoch: 2794, Batch Gradient Norm after: 22.360677111112388
Epoch 2795/10000, Prediction Accuracy = 54.462%, Loss = 1.0786680698394775
Epoch: 2795, Batch Gradient Norm: 39.9619714955863
Epoch: 2795, Batch Gradient Norm after: 22.360676924749765
Epoch 2796/10000, Prediction Accuracy = 54.512%, Loss = 1.0859568119049072
Epoch: 2796, Batch Gradient Norm: 37.213537390221845
Epoch: 2796, Batch Gradient Norm after: 22.36067795675486
Epoch 2797/10000, Prediction Accuracy = 54.470000000000006%, Loss = 1.0783217906951905
Epoch: 2797, Batch Gradient Norm: 39.952212241837664
Epoch: 2797, Batch Gradient Norm after: 22.36067960375817
Epoch 2798/10000, Prediction Accuracy = 54.524%, Loss = 1.0855927228927613
Epoch: 2798, Batch Gradient Norm: 37.2047803319346
Epoch: 2798, Batch Gradient Norm after: 22.3606787060348
Epoch 2799/10000, Prediction Accuracy = 54.468%, Loss = 1.0779690265655517
Epoch: 2799, Batch Gradient Norm: 39.9384461117163
Epoch: 2799, Batch Gradient Norm after: 22.36067784061167
Epoch 2800/10000, Prediction Accuracy = 54.522000000000006%, Loss = 1.085231113433838
Epoch: 2800, Batch Gradient Norm: 37.19538844721141
Epoch: 2800, Batch Gradient Norm after: 22.360679265203192
Epoch 2801/10000, Prediction Accuracy = 54.484%, Loss = 1.0776152849197387
Epoch: 2801, Batch Gradient Norm: 39.93152064778698
Epoch: 2801, Batch Gradient Norm after: 22.36067692997915
Epoch 2802/10000, Prediction Accuracy = 54.532%, Loss = 1.084887433052063
Epoch: 2802, Batch Gradient Norm: 37.182438489922426
Epoch: 2802, Batch Gradient Norm after: 22.36067849842801
Epoch 2803/10000, Prediction Accuracy = 54.501999999999995%, Loss = 1.0772600650787354
Epoch: 2803, Batch Gradient Norm: 39.92799974390983
Epoch: 2803, Batch Gradient Norm after: 22.360677357193588
Epoch 2804/10000, Prediction Accuracy = 54.552%, Loss = 1.0845479726791383
Epoch: 2804, Batch Gradient Norm: 37.17069907355714
Epoch: 2804, Batch Gradient Norm after: 22.360677001626506
Epoch 2805/10000, Prediction Accuracy = 54.501999999999995%, Loss = 1.0769054412841796
Epoch: 2805, Batch Gradient Norm: 39.9237664660482
Epoch: 2805, Batch Gradient Norm after: 22.360679064115054
Epoch 2806/10000, Prediction Accuracy = 54.568%, Loss = 1.0842122793197633
Epoch: 2806, Batch Gradient Norm: 37.16051360450547
Epoch: 2806, Batch Gradient Norm after: 22.360678552551153
Epoch 2807/10000, Prediction Accuracy = 54.524%, Loss = 1.0765536546707153
Epoch: 2807, Batch Gradient Norm: 39.922217231223705
Epoch: 2807, Batch Gradient Norm after: 22.360676198396373
Epoch 2808/10000, Prediction Accuracy = 54.576%, Loss = 1.0838825941085815
Epoch: 2808, Batch Gradient Norm: 37.14826428693531
Epoch: 2808, Batch Gradient Norm after: 22.360677700962384
Epoch 2809/10000, Prediction Accuracy = 54.529999999999994%, Loss = 1.0761986017227172
Epoch: 2809, Batch Gradient Norm: 39.91914998250666
Epoch: 2809, Batch Gradient Norm after: 22.360678975273924
Epoch 2810/10000, Prediction Accuracy = 54.576%, Loss = 1.0835466384887695
Epoch: 2810, Batch Gradient Norm: 37.13876192725875
Epoch: 2810, Batch Gradient Norm after: 22.360676934460937
Epoch 2811/10000, Prediction Accuracy = 54.534000000000006%, Loss = 1.0758464336395264
Epoch: 2811, Batch Gradient Norm: 39.90673645120899
Epoch: 2811, Batch Gradient Norm after: 22.36067719312517
Epoch 2812/10000, Prediction Accuracy = 54.592000000000006%, Loss = 1.0831822872161865
Epoch: 2812, Batch Gradient Norm: 37.12878122838828
Epoch: 2812, Batch Gradient Norm after: 22.360678202750012
Epoch 2813/10000, Prediction Accuracy = 54.54200000000001%, Loss = 1.0754923343658447
Epoch: 2813, Batch Gradient Norm: 39.89799309376752
Epoch: 2813, Batch Gradient Norm after: 22.360677016462468
Epoch 2814/10000, Prediction Accuracy = 54.592%, Loss = 1.0828300952911376
Epoch: 2814, Batch Gradient Norm: 37.118782636005136
Epoch: 2814, Batch Gradient Norm after: 22.360677913500147
Epoch 2815/10000, Prediction Accuracy = 54.568000000000005%, Loss = 1.0751492977142334
Epoch: 2815, Batch Gradient Norm: 39.888676298317506
Epoch: 2815, Batch Gradient Norm after: 22.360677098287518
Epoch 2816/10000, Prediction Accuracy = 54.58799999999999%, Loss = 1.0824823141098023
Epoch: 2816, Batch Gradient Norm: 37.10988679400843
Epoch: 2816, Batch Gradient Norm after: 22.360675855423136
Epoch 2817/10000, Prediction Accuracy = 54.58%, Loss = 1.074796772003174
Epoch: 2817, Batch Gradient Norm: 39.87801625896078
Epoch: 2817, Batch Gradient Norm after: 22.360675102832786
Epoch 2818/10000, Prediction Accuracy = 54.608000000000004%, Loss = 1.0821304082870484
Epoch: 2818, Batch Gradient Norm: 37.100456548918395
Epoch: 2818, Batch Gradient Norm after: 22.36067752474477
Epoch 2819/10000, Prediction Accuracy = 54.598%, Loss = 1.0744512796401977
Epoch: 2819, Batch Gradient Norm: 39.857975897379994
Epoch: 2819, Batch Gradient Norm after: 22.36067626914488
Epoch 2820/10000, Prediction Accuracy = 54.608000000000004%, Loss = 1.0817485570907592
Epoch: 2820, Batch Gradient Norm: 37.091822034980424
Epoch: 2820, Batch Gradient Norm after: 22.360677299466932
Epoch 2821/10000, Prediction Accuracy = 54.602%, Loss = 1.0741041898727417
Epoch: 2821, Batch Gradient Norm: 39.844403269202516
Epoch: 2821, Batch Gradient Norm after: 22.36067700273915
Epoch 2822/10000, Prediction Accuracy = 54.63199999999999%, Loss = 1.081386637687683
Epoch: 2822, Batch Gradient Norm: 37.080259371346784
Epoch: 2822, Batch Gradient Norm after: 22.360675476987566
Epoch 2823/10000, Prediction Accuracy = 54.589999999999996%, Loss = 1.0737596988677978
Epoch: 2823, Batch Gradient Norm: 39.83369988532037
Epoch: 2823, Batch Gradient Norm after: 22.360678234281664
Epoch 2824/10000, Prediction Accuracy = 54.63199999999999%, Loss = 1.0810398817062379
Epoch: 2824, Batch Gradient Norm: 37.07198452716005
Epoch: 2824, Batch Gradient Norm after: 22.360676598535385
Epoch 2825/10000, Prediction Accuracy = 54.586%, Loss = 1.0734042644500732
Epoch: 2825, Batch Gradient Norm: 39.827585065248435
Epoch: 2825, Batch Gradient Norm after: 22.360676463699093
Epoch 2826/10000, Prediction Accuracy = 54.634%, Loss = 1.0806992292404174
Epoch: 2826, Batch Gradient Norm: 37.06165840766335
Epoch: 2826, Batch Gradient Norm after: 22.36067799975577
Epoch 2827/10000, Prediction Accuracy = 54.604%, Loss = 1.0730549097061157
Epoch: 2827, Batch Gradient Norm: 39.8228782792273
Epoch: 2827, Batch Gradient Norm after: 22.36067724940276
Epoch 2828/10000, Prediction Accuracy = 54.644000000000005%, Loss = 1.0803706169128418
Epoch: 2828, Batch Gradient Norm: 37.0520350383029
Epoch: 2828, Batch Gradient Norm after: 22.360676623121236
Epoch 2829/10000, Prediction Accuracy = 54.614%, Loss = 1.072702121734619
Epoch: 2829, Batch Gradient Norm: 39.82286574956092
Epoch: 2829, Batch Gradient Norm after: 22.360677332552665
Epoch 2830/10000, Prediction Accuracy = 54.648%, Loss = 1.080051851272583
Epoch: 2830, Batch Gradient Norm: 37.041665035877024
Epoch: 2830, Batch Gradient Norm after: 22.360678990318664
Epoch 2831/10000, Prediction Accuracy = 54.620000000000005%, Loss = 1.0723466157913208
Epoch: 2831, Batch Gradient Norm: 39.82400267726063
Epoch: 2831, Batch Gradient Norm after: 22.360677756214372
Epoch 2832/10000, Prediction Accuracy = 54.657999999999994%, Loss = 1.0797327280044555
Epoch: 2832, Batch Gradient Norm: 37.02798564502109
Epoch: 2832, Batch Gradient Norm after: 22.36067741925899
Epoch 2833/10000, Prediction Accuracy = 54.634%, Loss = 1.0719942331314087
Epoch: 2833, Batch Gradient Norm: 39.826476629237376
Epoch: 2833, Batch Gradient Norm after: 22.36067825465751
Epoch 2834/10000, Prediction Accuracy = 54.666%, Loss = 1.0794244766235352
Epoch: 2834, Batch Gradient Norm: 37.015942937019204
Epoch: 2834, Batch Gradient Norm after: 22.36067740108949
Epoch 2835/10000, Prediction Accuracy = 54.65400000000001%, Loss = 1.0716482639312743
Epoch: 2835, Batch Gradient Norm: 39.82627801316266
Epoch: 2835, Batch Gradient Norm after: 22.36067867846093
Epoch 2836/10000, Prediction Accuracy = 54.67%, Loss = 1.0791035652160645
Epoch: 2836, Batch Gradient Norm: 37.00631455957444
Epoch: 2836, Batch Gradient Norm after: 22.360675063576206
Epoch 2837/10000, Prediction Accuracy = 54.669999999999995%, Loss = 1.0713010787963868
Epoch: 2837, Batch Gradient Norm: 39.82048882549734
Epoch: 2837, Batch Gradient Norm after: 22.360676443689663
Epoch 2838/10000, Prediction Accuracy = 54.682%, Loss = 1.0787691354751587
Epoch: 2838, Batch Gradient Norm: 36.99673733048667
Epoch: 2838, Batch Gradient Norm after: 22.360675423894385
Epoch 2839/10000, Prediction Accuracy = 54.69199999999999%, Loss = 1.0709530591964722
Epoch: 2839, Batch Gradient Norm: 39.814558116220226
Epoch: 2839, Batch Gradient Norm after: 22.360674694412538
Epoch 2840/10000, Prediction Accuracy = 54.69199999999999%, Loss = 1.0784303903579713
Epoch: 2840, Batch Gradient Norm: 36.98510474869127
Epoch: 2840, Batch Gradient Norm after: 22.360675643968058
Epoch 2841/10000, Prediction Accuracy = 54.694%, Loss = 1.0706068992614746
Epoch: 2841, Batch Gradient Norm: 39.806080773872516
Epoch: 2841, Batch Gradient Norm after: 22.36067690147806
Epoch 2842/10000, Prediction Accuracy = 54.696000000000005%, Loss = 1.0780723571777344
Epoch: 2842, Batch Gradient Norm: 36.97592994899658
Epoch: 2842, Batch Gradient Norm after: 22.360676108834838
Epoch 2843/10000, Prediction Accuracy = 54.694%, Loss = 1.07026264667511
Epoch: 2843, Batch Gradient Norm: 39.797572969534954
Epoch: 2843, Batch Gradient Norm after: 22.360675859963003
Epoch 2844/10000, Prediction Accuracy = 54.708000000000006%, Loss = 1.0777215242385865
Epoch: 2844, Batch Gradient Norm: 36.96656128864019
Epoch: 2844, Batch Gradient Norm after: 22.360675230025063
Epoch 2845/10000, Prediction Accuracy = 54.7%, Loss = 1.0699145078659058
Epoch: 2845, Batch Gradient Norm: 39.78482111553814
Epoch: 2845, Batch Gradient Norm after: 22.360676190169006
Epoch 2846/10000, Prediction Accuracy = 54.717999999999996%, Loss = 1.0773728847503663
Epoch: 2846, Batch Gradient Norm: 36.95550886541964
Epoch: 2846, Batch Gradient Norm after: 22.360677024708885
Epoch 2847/10000, Prediction Accuracy = 54.709999999999994%, Loss = 1.0695691108703613
Epoch: 2847, Batch Gradient Norm: 39.77588032371708
Epoch: 2847, Batch Gradient Norm after: 22.360675787605626
Epoch 2848/10000, Prediction Accuracy = 54.71999999999999%, Loss = 1.0770363569259644
Epoch: 2848, Batch Gradient Norm: 36.94179036979807
Epoch: 2848, Batch Gradient Norm after: 22.360677168406056
Epoch 2849/10000, Prediction Accuracy = 54.717999999999996%, Loss = 1.0692222833633422
Epoch: 2849, Batch Gradient Norm: 39.768343312426
Epoch: 2849, Batch Gradient Norm after: 22.360677061866117
Epoch 2850/10000, Prediction Accuracy = 54.727999999999994%, Loss = 1.0767012119293213
Epoch: 2850, Batch Gradient Norm: 36.92920160773394
Epoch: 2850, Batch Gradient Norm after: 22.360678627452323
Epoch 2851/10000, Prediction Accuracy = 54.73599999999999%, Loss = 1.0688726902008057
Epoch: 2851, Batch Gradient Norm: 39.75877082504333
Epoch: 2851, Batch Gradient Norm after: 22.36067562359347
Epoch 2852/10000, Prediction Accuracy = 54.73%, Loss = 1.0763706922531129
Epoch: 2852, Batch Gradient Norm: 36.91888880929233
Epoch: 2852, Batch Gradient Norm after: 22.360679213534542
Epoch 2853/10000, Prediction Accuracy = 54.75%, Loss = 1.0685267686843871
Epoch: 2853, Batch Gradient Norm: 39.75296520694717
Epoch: 2853, Batch Gradient Norm after: 22.360675283394578
Epoch 2854/10000, Prediction Accuracy = 54.739999999999995%, Loss = 1.0760364294052125
Epoch: 2854, Batch Gradient Norm: 36.90787027643662
Epoch: 2854, Batch Gradient Norm after: 22.3606779588216
Epoch 2855/10000, Prediction Accuracy = 54.75%, Loss = 1.0681780576705933
Epoch: 2855, Batch Gradient Norm: 39.74286599382933
Epoch: 2855, Batch Gradient Norm after: 22.36067677648653
Epoch 2856/10000, Prediction Accuracy = 54.748000000000005%, Loss = 1.075695776939392
Epoch: 2856, Batch Gradient Norm: 36.89659155860209
Epoch: 2856, Batch Gradient Norm after: 22.360678953959678
Epoch 2857/10000, Prediction Accuracy = 54.762%, Loss = 1.0678345680236816
Epoch: 2857, Batch Gradient Norm: 39.731053116769864
Epoch: 2857, Batch Gradient Norm after: 22.360676350527577
Epoch 2858/10000, Prediction Accuracy = 54.762000000000015%, Loss = 1.075343918800354
Epoch: 2858, Batch Gradient Norm: 36.88629089937768
Epoch: 2858, Batch Gradient Norm after: 22.36067769049569
Epoch 2859/10000, Prediction Accuracy = 54.77%, Loss = 1.0674877166748047
Epoch: 2859, Batch Gradient Norm: 39.722539721495764
Epoch: 2859, Batch Gradient Norm after: 22.360677596561526
Epoch 2860/10000, Prediction Accuracy = 54.75600000000001%, Loss = 1.0750047445297242
Epoch: 2860, Batch Gradient Norm: 36.87722696466864
Epoch: 2860, Batch Gradient Norm after: 22.360676583114348
Epoch 2861/10000, Prediction Accuracy = 54.778%, Loss = 1.0671456575393676
Epoch: 2861, Batch Gradient Norm: 39.716727159070224
Epoch: 2861, Batch Gradient Norm after: 22.36067699461225
Epoch 2862/10000, Prediction Accuracy = 54.772000000000006%, Loss = 1.0746695280075074
Epoch: 2862, Batch Gradient Norm: 36.86691853195585
Epoch: 2862, Batch Gradient Norm after: 22.360676508020614
Epoch 2863/10000, Prediction Accuracy = 54.779999999999994%, Loss = 1.066801166534424
Epoch: 2863, Batch Gradient Norm: 39.7056830914065
Epoch: 2863, Batch Gradient Norm after: 22.360676188398468
Epoch 2864/10000, Prediction Accuracy = 54.77%, Loss = 1.0743293046951294
Epoch: 2864, Batch Gradient Norm: 36.857594053738644
Epoch: 2864, Batch Gradient Norm after: 22.360678428962945
Epoch 2865/10000, Prediction Accuracy = 54.790000000000006%, Loss = 1.0664566040039063
Epoch: 2865, Batch Gradient Norm: 39.690734841996836
Epoch: 2865, Batch Gradient Norm after: 22.360677308574765
Epoch 2866/10000, Prediction Accuracy = 54.775999999999996%, Loss = 1.0739733934402467
Epoch: 2866, Batch Gradient Norm: 36.851351458831466
Epoch: 2866, Batch Gradient Norm after: 22.360677512667383
Epoch 2867/10000, Prediction Accuracy = 54.79599999999999%, Loss = 1.0661162614822388
Epoch: 2867, Batch Gradient Norm: 39.6786803241334
Epoch: 2867, Batch Gradient Norm after: 22.360678587891382
Epoch 2868/10000, Prediction Accuracy = 54.79%, Loss = 1.0736316680908202
Epoch: 2868, Batch Gradient Norm: 36.83886259242586
Epoch: 2868, Batch Gradient Norm after: 22.360676346005615
Epoch 2869/10000, Prediction Accuracy = 54.806%, Loss = 1.0657636642456054
Epoch: 2869, Batch Gradient Norm: 39.675466731757275
Epoch: 2869, Batch Gradient Norm after: 22.36067768489873
Epoch 2870/10000, Prediction Accuracy = 54.794000000000004%, Loss = 1.0733012914657594
Epoch: 2870, Batch Gradient Norm: 36.8280170330878
Epoch: 2870, Batch Gradient Norm after: 22.360678703098472
Epoch 2871/10000, Prediction Accuracy = 54.806%, Loss = 1.0654201269149781
Epoch: 2871, Batch Gradient Norm: 39.66667320735407
Epoch: 2871, Batch Gradient Norm after: 22.360675522752352
Epoch 2872/10000, Prediction Accuracy = 54.8%, Loss = 1.0729592800140382
Epoch: 2872, Batch Gradient Norm: 36.81798434948979
Epoch: 2872, Batch Gradient Norm after: 22.360677141907654
Epoch 2873/10000, Prediction Accuracy = 54.81999999999999%, Loss = 1.065080451965332
Epoch: 2873, Batch Gradient Norm: 39.65516435644415
Epoch: 2873, Batch Gradient Norm after: 22.360678044042785
Epoch 2874/10000, Prediction Accuracy = 54.79600000000001%, Loss = 1.0726011514663696
Epoch: 2874, Batch Gradient Norm: 36.80998976981977
Epoch: 2874, Batch Gradient Norm after: 22.360676162266667
Epoch 2875/10000, Prediction Accuracy = 54.826%, Loss = 1.064736819267273
Epoch: 2875, Batch Gradient Norm: 39.63808692963746
Epoch: 2875, Batch Gradient Norm after: 22.360678444291352
Epoch 2876/10000, Prediction Accuracy = 54.814%, Loss = 1.0722363948822022
Epoch: 2876, Batch Gradient Norm: 36.79954055509607
Epoch: 2876, Batch Gradient Norm after: 22.36067600108674
Epoch 2877/10000, Prediction Accuracy = 54.839999999999996%, Loss = 1.064396286010742
Epoch: 2877, Batch Gradient Norm: 39.62546370794001
Epoch: 2877, Batch Gradient Norm after: 22.360678625237718
Epoch 2878/10000, Prediction Accuracy = 54.826%, Loss = 1.0718803644180297
Epoch: 2878, Batch Gradient Norm: 36.789288609340794
Epoch: 2878, Batch Gradient Norm after: 22.36067456776548
Epoch 2879/10000, Prediction Accuracy = 54.858000000000004%, Loss = 1.064052438735962
Epoch: 2879, Batch Gradient Norm: 39.615126244013496
Epoch: 2879, Batch Gradient Norm after: 22.360677913631264
Epoch 2880/10000, Prediction Accuracy = 54.834%, Loss = 1.0715284585952758
Epoch: 2880, Batch Gradient Norm: 36.780072328828915
Epoch: 2880, Batch Gradient Norm after: 22.360674965602257
Epoch 2881/10000, Prediction Accuracy = 54.870000000000005%, Loss = 1.0637086629867554
Epoch: 2881, Batch Gradient Norm: 39.60651945058909
Epoch: 2881, Batch Gradient Norm after: 22.36067577124312
Epoch 2882/10000, Prediction Accuracy = 54.838%, Loss = 1.0711879968643188
Epoch: 2882, Batch Gradient Norm: 36.77116979665099
Epoch: 2882, Batch Gradient Norm after: 22.360677923482875
Epoch 2883/10000, Prediction Accuracy = 54.89000000000001%, Loss = 1.063367247581482
Epoch: 2883, Batch Gradient Norm: 39.58838761311573
Epoch: 2883, Batch Gradient Norm after: 22.36067522242366
Epoch 2884/10000, Prediction Accuracy = 54.854%, Loss = 1.0708241224288941
Epoch: 2884, Batch Gradient Norm: 36.765226066249326
Epoch: 2884, Batch Gradient Norm after: 22.36067894143294
Epoch 2885/10000, Prediction Accuracy = 54.9%, Loss = 1.0630291938781737
Epoch: 2885, Batch Gradient Norm: 39.57133251905704
Epoch: 2885, Batch Gradient Norm after: 22.360676543939434
Epoch 2886/10000, Prediction Accuracy = 54.855999999999995%, Loss = 1.0704514265060425
Epoch: 2886, Batch Gradient Norm: 36.75969993123337
Epoch: 2886, Batch Gradient Norm after: 22.360676804747616
Epoch 2887/10000, Prediction Accuracy = 54.916%, Loss = 1.062692356109619
Epoch: 2887, Batch Gradient Norm: 39.55468606848904
Epoch: 2887, Batch Gradient Norm after: 22.360677861922674
Epoch 2888/10000, Prediction Accuracy = 54.870000000000005%, Loss = 1.0700889348983764
Epoch: 2888, Batch Gradient Norm: 36.75389426406981
Epoch: 2888, Batch Gradient Norm after: 22.360678525219804
Epoch 2889/10000, Prediction Accuracy = 54.922000000000004%, Loss = 1.0623536586761475
Epoch: 2889, Batch Gradient Norm: 39.529498965654135
Epoch: 2889, Batch Gradient Norm after: 22.3606754970873
Epoch 2890/10000, Prediction Accuracy = 54.879999999999995%, Loss = 1.069712233543396
Epoch: 2890, Batch Gradient Norm: 36.74396533464651
Epoch: 2890, Batch Gradient Norm after: 22.36067874004635
Epoch 2891/10000, Prediction Accuracy = 54.932%, Loss = 1.0620187759399413
Epoch: 2891, Batch Gradient Norm: 39.51078044032257
Epoch: 2891, Batch Gradient Norm after: 22.360675837182786
Epoch 2892/10000, Prediction Accuracy = 54.88599999999999%, Loss = 1.069338822364807
Epoch: 2892, Batch Gradient Norm: 36.737271641419625
Epoch: 2892, Batch Gradient Norm after: 22.360679211074025
Epoch 2893/10000, Prediction Accuracy = 54.94%, Loss = 1.0616797924041748
Epoch: 2893, Batch Gradient Norm: 39.49423509068936
Epoch: 2893, Batch Gradient Norm after: 22.360676198793623
Epoch 2894/10000, Prediction Accuracy = 54.89000000000001%, Loss = 1.06897931098938
Epoch: 2894, Batch Gradient Norm: 36.728342758038835
Epoch: 2894, Batch Gradient Norm after: 22.36067838653118
Epoch 2895/10000, Prediction Accuracy = 54.946000000000005%, Loss = 1.061343002319336
Epoch: 2895, Batch Gradient Norm: 39.479041290290866
Epoch: 2895, Batch Gradient Norm after: 22.36067688892065
Epoch 2896/10000, Prediction Accuracy = 54.896%, Loss = 1.068609642982483
Epoch: 2896, Batch Gradient Norm: 36.72061641546527
Epoch: 2896, Batch Gradient Norm after: 22.36067685537267
Epoch 2897/10000, Prediction Accuracy = 54.952%, Loss = 1.0610057353973388
Epoch: 2897, Batch Gradient Norm: 39.46143132550505
Epoch: 2897, Batch Gradient Norm after: 22.360678003475694
Epoch 2898/10000, Prediction Accuracy = 54.914%, Loss = 1.0682419538497925
Epoch: 2898, Batch Gradient Norm: 36.70942007528426
Epoch: 2898, Batch Gradient Norm after: 22.360675265735722
Epoch 2899/10000, Prediction Accuracy = 54.974000000000004%, Loss = 1.0606726169586183
Epoch: 2899, Batch Gradient Norm: 39.445552868324675
Epoch: 2899, Batch Gradient Norm after: 22.360677023217942
Epoch 2900/10000, Prediction Accuracy = 54.912%, Loss = 1.0678858995437621
Epoch: 2900, Batch Gradient Norm: 36.700983167526594
Epoch: 2900, Batch Gradient Norm after: 22.36067896867494
Epoch 2901/10000, Prediction Accuracy = 54.967999999999996%, Loss = 1.0603362560272216
Epoch: 2901, Batch Gradient Norm: 39.427661750342736
Epoch: 2901, Batch Gradient Norm after: 22.360677269125546
Epoch 2902/10000, Prediction Accuracy = 54.910000000000004%, Loss = 1.0675199508666993
Epoch: 2902, Batch Gradient Norm: 36.69183014894404
Epoch: 2902, Batch Gradient Norm after: 22.360676768511308
Epoch 2903/10000, Prediction Accuracy = 54.970000000000006%, Loss = 1.0600003242492675
Epoch: 2903, Batch Gradient Norm: 39.41063250807424
Epoch: 2903, Batch Gradient Norm after: 22.360676697458505
Epoch 2904/10000, Prediction Accuracy = 54.91600000000001%, Loss = 1.067160415649414
Epoch: 2904, Batch Gradient Norm: 36.682359553688514
Epoch: 2904, Batch Gradient Norm after: 22.360676398345863
Epoch 2905/10000, Prediction Accuracy = 54.992%, Loss = 1.0596676349639893
Epoch: 2905, Batch Gradient Norm: 39.39859176149626
Epoch: 2905, Batch Gradient Norm after: 22.36067902602334
Epoch 2906/10000, Prediction Accuracy = 54.934000000000005%, Loss = 1.0668138027191163
Epoch: 2906, Batch Gradient Norm: 36.672609531385916
Epoch: 2906, Batch Gradient Norm after: 22.36067785511223
Epoch 2907/10000, Prediction Accuracy = 54.996%, Loss = 1.059326958656311
Epoch: 2907, Batch Gradient Norm: 39.38396347303347
Epoch: 2907, Batch Gradient Norm after: 22.360676131960652
Epoch 2908/10000, Prediction Accuracy = 54.943999999999996%, Loss = 1.0664543628692627
Epoch: 2908, Batch Gradient Norm: 36.66541892454132
Epoch: 2908, Batch Gradient Norm after: 22.360679916515604
Epoch 2909/10000, Prediction Accuracy = 55.001999999999995%, Loss = 1.0589916467666627
Epoch: 2909, Batch Gradient Norm: 39.367541296091105
Epoch: 2909, Batch Gradient Norm after: 22.36067798140449
Epoch 2910/10000, Prediction Accuracy = 54.94%, Loss = 1.0660900592803955
Epoch: 2910, Batch Gradient Norm: 36.659256572272525
Epoch: 2910, Batch Gradient Norm after: 22.360678925994026
Epoch 2911/10000, Prediction Accuracy = 55.016%, Loss = 1.0586616277694703
Epoch: 2911, Batch Gradient Norm: 39.35600196951951
Epoch: 2911, Batch Gradient Norm after: 22.360678242094156
Epoch 2912/10000, Prediction Accuracy = 54.952%, Loss = 1.0657519817352294
Epoch: 2912, Batch Gradient Norm: 36.65028096957212
Epoch: 2912, Batch Gradient Norm after: 22.360679622398713
Epoch 2913/10000, Prediction Accuracy = 55.024%, Loss = 1.0583271026611327
Epoch: 2913, Batch Gradient Norm: 39.349873973644684
Epoch: 2913, Batch Gradient Norm after: 22.36067974328068
Epoch 2914/10000, Prediction Accuracy = 54.95799999999999%, Loss = 1.0654165029525757
Epoch: 2914, Batch Gradient Norm: 36.64013932715738
Epoch: 2914, Batch Gradient Norm after: 22.36067796828155
Epoch 2915/10000, Prediction Accuracy = 55.036%, Loss = 1.0579910039901734
Epoch: 2915, Batch Gradient Norm: 39.34946155511476
Epoch: 2915, Batch Gradient Norm after: 22.360678570963238
Epoch 2916/10000, Prediction Accuracy = 54.965999999999994%, Loss = 1.065098190307617
Epoch: 2916, Batch Gradient Norm: 36.6336238912917
Epoch: 2916, Batch Gradient Norm after: 22.360677236820056
Epoch 2917/10000, Prediction Accuracy = 55.041999999999994%, Loss = 1.0576520204544066
Epoch: 2917, Batch Gradient Norm: 39.35065015420056
Epoch: 2917, Batch Gradient Norm after: 22.360676094918702
Epoch 2918/10000, Prediction Accuracy = 54.976%, Loss = 1.0647786617279054
Epoch: 2918, Batch Gradient Norm: 36.62072580356841
Epoch: 2918, Batch Gradient Norm after: 22.360676786968096
Epoch 2919/10000, Prediction Accuracy = 55.044%, Loss = 1.0573155879974365
Epoch: 2919, Batch Gradient Norm: 39.347373105006035
Epoch: 2919, Batch Gradient Norm after: 22.36067712260417
Epoch 2920/10000, Prediction Accuracy = 54.988%, Loss = 1.0644583702087402
Epoch: 2920, Batch Gradient Norm: 36.611586950071704
Epoch: 2920, Batch Gradient Norm after: 22.360676737381585
Epoch 2921/10000, Prediction Accuracy = 55.048%, Loss = 1.0569815635681152
Epoch: 2921, Batch Gradient Norm: 39.3410608506683
Epoch: 2921, Batch Gradient Norm after: 22.360677554007832
Epoch 2922/10000, Prediction Accuracy = 54.988%, Loss = 1.06413836479187
Epoch: 2922, Batch Gradient Norm: 36.60248517887664
Epoch: 2922, Batch Gradient Norm after: 22.36067627296662
Epoch 2923/10000, Prediction Accuracy = 55.068%, Loss = 1.0566462516784667
Epoch: 2923, Batch Gradient Norm: 39.340825144007034
Epoch: 2923, Batch Gradient Norm after: 22.360677299800304
Epoch 2924/10000, Prediction Accuracy = 54.99399999999999%, Loss = 1.0638262510299683
Epoch: 2924, Batch Gradient Norm: 36.59148752997725
Epoch: 2924, Batch Gradient Norm after: 22.360678132386145
Epoch 2925/10000, Prediction Accuracy = 55.08%, Loss = 1.056310248374939
Epoch: 2925, Batch Gradient Norm: 39.34071368864748
Epoch: 2925, Batch Gradient Norm after: 22.36067770527697
Epoch 2926/10000, Prediction Accuracy = 55.01800000000001%, Loss = 1.063512134552002
Epoch: 2926, Batch Gradient Norm: 36.58020294363597
Epoch: 2926, Batch Gradient Norm after: 22.360675628639978
Epoch 2927/10000, Prediction Accuracy = 55.088%, Loss = 1.0559741020202638
Epoch: 2927, Batch Gradient Norm: 39.340439846169495
Epoch: 2927, Batch Gradient Norm after: 22.360677365753347
Epoch 2928/10000, Prediction Accuracy = 55.032%, Loss = 1.0631962776184083
Epoch: 2928, Batch Gradient Norm: 36.56914965132919
Epoch: 2928, Batch Gradient Norm after: 22.360676060940104
Epoch 2929/10000, Prediction Accuracy = 55.09400000000001%, Loss = 1.0556311130523681
Epoch: 2929, Batch Gradient Norm: 39.337945969536264
Epoch: 2929, Batch Gradient Norm after: 22.360676037100472
Epoch 2930/10000, Prediction Accuracy = 55.028%, Loss = 1.0628713369369507
Epoch: 2930, Batch Gradient Norm: 36.556682144450434
Epoch: 2930, Batch Gradient Norm after: 22.36067639630425
Epoch 2931/10000, Prediction Accuracy = 55.086%, Loss = 1.055298399925232
Epoch: 2931, Batch Gradient Norm: 39.32705201473972
Epoch: 2931, Batch Gradient Norm after: 22.3606775964368
Epoch 2932/10000, Prediction Accuracy = 55.03800000000001%, Loss = 1.0625293493270873
Epoch: 2932, Batch Gradient Norm: 36.54798010418041
Epoch: 2932, Batch Gradient Norm after: 22.36067710928683
Epoch 2933/10000, Prediction Accuracy = 55.088%, Loss = 1.0549593210220336
Epoch: 2933, Batch Gradient Norm: 39.31319775093485
Epoch: 2933, Batch Gradient Norm after: 22.36067656293253
Epoch 2934/10000, Prediction Accuracy = 55.038%, Loss = 1.0621873140335083
Epoch: 2934, Batch Gradient Norm: 36.53729543688634
Epoch: 2934, Batch Gradient Norm after: 22.3606775479782
Epoch 2935/10000, Prediction Accuracy = 55.102%, Loss = 1.0546239137649536
Epoch: 2935, Batch Gradient Norm: 39.302336802398436
Epoch: 2935, Batch Gradient Norm after: 22.360677244349233
Epoch 2936/10000, Prediction Accuracy = 55.053999999999995%, Loss = 1.061845588684082
Epoch: 2936, Batch Gradient Norm: 36.52837658624821
Epoch: 2936, Batch Gradient Norm after: 22.360678539137616
Epoch 2937/10000, Prediction Accuracy = 55.102%, Loss = 1.0542856216430665
Epoch: 2937, Batch Gradient Norm: 39.296162213441875
Epoch: 2937, Batch Gradient Norm after: 22.360679000713734
Epoch 2938/10000, Prediction Accuracy = 55.06999999999999%, Loss = 1.0615251064300537
Epoch: 2938, Batch Gradient Norm: 36.51867357595333
Epoch: 2938, Batch Gradient Norm after: 22.36067752714459
Epoch 2939/10000, Prediction Accuracy = 55.108000000000004%, Loss = 1.0539527654647827
Epoch: 2939, Batch Gradient Norm: 39.28298132552402
Epoch: 2939, Batch Gradient Norm after: 22.36067578574339
Epoch 2940/10000, Prediction Accuracy = 55.08%, Loss = 1.0611851692199707
Epoch: 2940, Batch Gradient Norm: 36.510502674010525
Epoch: 2940, Batch Gradient Norm after: 22.360678341749047
Epoch 2941/10000, Prediction Accuracy = 55.124%, Loss = 1.0536193370819091
Epoch: 2941, Batch Gradient Norm: 39.26818174286833
Epoch: 2941, Batch Gradient Norm after: 22.360678311265307
Epoch 2942/10000, Prediction Accuracy = 55.09400000000001%, Loss = 1.060826849937439
Epoch: 2942, Batch Gradient Norm: 36.50110659589425
Epoch: 2942, Batch Gradient Norm after: 22.36067643824252
Epoch 2943/10000, Prediction Accuracy = 55.116%, Loss = 1.05328311920166
Epoch: 2943, Batch Gradient Norm: 39.26252813106532
Epoch: 2943, Batch Gradient Norm after: 22.360677133044565
Epoch 2944/10000, Prediction Accuracy = 55.124%, Loss = 1.0605042695999145
Epoch: 2944, Batch Gradient Norm: 36.48882845800176
Epoch: 2944, Batch Gradient Norm after: 22.360677499510118
Epoch 2945/10000, Prediction Accuracy = 55.124%, Loss = 1.0529476404190063
Epoch: 2945, Batch Gradient Norm: 39.253238274400296
Epoch: 2945, Batch Gradient Norm after: 22.360678192760293
Epoch 2946/10000, Prediction Accuracy = 55.13199999999999%, Loss = 1.060174822807312
Epoch: 2946, Batch Gradient Norm: 36.4789160098842
Epoch: 2946, Batch Gradient Norm after: 22.36067600619028
Epoch 2947/10000, Prediction Accuracy = 55.128%, Loss = 1.0526164293289184
Epoch: 2947, Batch Gradient Norm: 39.24275106657414
Epoch: 2947, Batch Gradient Norm after: 22.36067744446031
Epoch 2948/10000, Prediction Accuracy = 55.15%, Loss = 1.059845542907715
Epoch: 2948, Batch Gradient Norm: 36.468500428489335
Epoch: 2948, Batch Gradient Norm after: 22.360678337487997
Epoch 2949/10000, Prediction Accuracy = 55.126%, Loss = 1.0522810459136962
Epoch: 2949, Batch Gradient Norm: 39.234061364499944
Epoch: 2949, Batch Gradient Norm after: 22.3606782300805
Epoch 2950/10000, Prediction Accuracy = 55.158%, Loss = 1.0595120906829834
Epoch: 2950, Batch Gradient Norm: 36.459145190170055
Epoch: 2950, Batch Gradient Norm after: 22.360676204764694
Epoch 2951/10000, Prediction Accuracy = 55.13800000000001%, Loss = 1.0519473552703857
Epoch: 2951, Batch Gradient Norm: 39.2214479376159
Epoch: 2951, Batch Gradient Norm after: 22.360678185569718
Epoch 2952/10000, Prediction Accuracy = 55.181999999999995%, Loss = 1.0591641187667846
Epoch: 2952, Batch Gradient Norm: 36.44989244945485
Epoch: 2952, Batch Gradient Norm after: 22.360677637869674
Epoch 2953/10000, Prediction Accuracy = 55.14200000000001%, Loss = 1.0516212463378907
Epoch: 2953, Batch Gradient Norm: 39.212768645660454
Epoch: 2953, Batch Gradient Norm after: 22.36067869683919
Epoch 2954/10000, Prediction Accuracy = 55.19200000000001%, Loss = 1.058829402923584
Epoch: 2954, Batch Gradient Norm: 36.43737351915329
Epoch: 2954, Batch Gradient Norm after: 22.360676849011433
Epoch 2955/10000, Prediction Accuracy = 55.166%, Loss = 1.051284098625183
Epoch: 2955, Batch Gradient Norm: 39.210326333998765
Epoch: 2955, Batch Gradient Norm after: 22.3606784666626
Epoch 2956/10000, Prediction Accuracy = 55.196000000000005%, Loss = 1.0585162162780761
Epoch: 2956, Batch Gradient Norm: 36.42440782208223
Epoch: 2956, Batch Gradient Norm after: 22.360677051494225
Epoch 2957/10000, Prediction Accuracy = 55.17%, Loss = 1.0509496927261353
Epoch: 2957, Batch Gradient Norm: 39.20619345788102
Epoch: 2957, Batch Gradient Norm after: 22.36067651101468
Epoch 2958/10000, Prediction Accuracy = 55.20400000000001%, Loss = 1.058207893371582
Epoch: 2958, Batch Gradient Norm: 36.41434852148662
Epoch: 2958, Batch Gradient Norm after: 22.360677765309994
Epoch 2959/10000, Prediction Accuracy = 55.169999999999995%, Loss = 1.0506225109100342
Epoch: 2959, Batch Gradient Norm: 39.19943516402322
Epoch: 2959, Batch Gradient Norm after: 22.360678841264587
Epoch 2960/10000, Prediction Accuracy = 55.208000000000006%, Loss = 1.0578820943832397
Epoch: 2960, Batch Gradient Norm: 36.404478601724
Epoch: 2960, Batch Gradient Norm after: 22.3606767471277
Epoch 2961/10000, Prediction Accuracy = 55.178%, Loss = 1.0502873659133911
Epoch: 2961, Batch Gradient Norm: 39.18928491637461
Epoch: 2961, Batch Gradient Norm after: 22.360677132929286
Epoch 2962/10000, Prediction Accuracy = 55.215999999999994%, Loss = 1.0575562000274659
Epoch: 2962, Batch Gradient Norm: 36.39456014158001
Epoch: 2962, Batch Gradient Norm after: 22.360677957386052
Epoch 2963/10000, Prediction Accuracy = 55.186%, Loss = 1.049955153465271
Epoch: 2963, Batch Gradient Norm: 39.18596752974478
Epoch: 2963, Batch Gradient Norm after: 22.36067707607521
Epoch 2964/10000, Prediction Accuracy = 55.215999999999994%, Loss = 1.057243275642395
Epoch: 2964, Batch Gradient Norm: 36.385269144987106
Epoch: 2964, Batch Gradient Norm after: 22.360677309402405
Epoch 2965/10000, Prediction Accuracy = 55.20400000000001%, Loss = 1.0496208190917968
Epoch: 2965, Batch Gradient Norm: 39.18498867309524
Epoch: 2965, Batch Gradient Norm after: 22.36067818206073
Epoch 2966/10000, Prediction Accuracy = 55.236000000000004%, Loss = 1.0569398880004883
Epoch: 2966, Batch Gradient Norm: 36.371489602924086
Epoch: 2966, Batch Gradient Norm after: 22.36067769219838
Epoch 2967/10000, Prediction Accuracy = 55.208000000000006%, Loss = 1.0492865800857545
Epoch: 2967, Batch Gradient Norm: 39.176231955834744
Epoch: 2967, Batch Gradient Norm after: 22.360679235411563
Epoch 2968/10000, Prediction Accuracy = 55.242%, Loss = 1.0566134452819824
Epoch: 2968, Batch Gradient Norm: 36.36190047614929
Epoch: 2968, Batch Gradient Norm after: 22.3606785319618
Epoch 2969/10000, Prediction Accuracy = 55.21%, Loss = 1.0489535808563233
Epoch: 2969, Batch Gradient Norm: 39.16126897310571
Epoch: 2969, Batch Gradient Norm after: 22.360675649009906
Epoch 2970/10000, Prediction Accuracy = 55.254%, Loss = 1.0562641620635986
Epoch: 2970, Batch Gradient Norm: 36.35322581724557
Epoch: 2970, Batch Gradient Norm after: 22.36067835555548
Epoch 2971/10000, Prediction Accuracy = 55.208000000000006%, Loss = 1.048624539375305
Epoch: 2971, Batch Gradient Norm: 39.14428473939858
Epoch: 2971, Batch Gradient Norm after: 22.36067845114504
Epoch 2972/10000, Prediction Accuracy = 55.267999999999994%, Loss = 1.0559085130691528
Epoch: 2972, Batch Gradient Norm: 36.34287080879699
Epoch: 2972, Batch Gradient Norm after: 22.360676522180714
Epoch 2973/10000, Prediction Accuracy = 55.215999999999994%, Loss = 1.048295521736145
Epoch: 2973, Batch Gradient Norm: 39.12907460051765
Epoch: 2973, Batch Gradient Norm after: 22.360675965763935
Epoch 2974/10000, Prediction Accuracy = 55.274%, Loss = 1.055561327934265
Epoch: 2974, Batch Gradient Norm: 36.334071494298485
Epoch: 2974, Batch Gradient Norm after: 22.36067805030511
Epoch 2975/10000, Prediction Accuracy = 55.224000000000004%, Loss = 1.0479665994644165
Epoch: 2975, Batch Gradient Norm: 39.10644922182435
Epoch: 2975, Batch Gradient Norm after: 22.36067523054448
Epoch 2976/10000, Prediction Accuracy = 55.267999999999994%, Loss = 1.0551966428756714
Epoch: 2976, Batch Gradient Norm: 36.325810911955706
Epoch: 2976, Batch Gradient Norm after: 22.360677669470096
Epoch 2977/10000, Prediction Accuracy = 55.226%, Loss = 1.0476431608200074
Epoch: 2977, Batch Gradient Norm: 39.08320267307053
Epoch: 2977, Batch Gradient Norm after: 22.360674412395316
Epoch 2978/10000, Prediction Accuracy = 55.272000000000006%, Loss = 1.0548202753067017
Epoch: 2978, Batch Gradient Norm: 36.31712645355597
Epoch: 2978, Batch Gradient Norm after: 22.360676351674392
Epoch 2979/10000, Prediction Accuracy = 55.236000000000004%, Loss = 1.0473182439804076
Epoch: 2979, Batch Gradient Norm: 39.0564444150088
Epoch: 2979, Batch Gradient Norm after: 22.360676443072993
Epoch 2980/10000, Prediction Accuracy = 55.27%, Loss = 1.0544470071792602
Epoch: 2980, Batch Gradient Norm: 36.30796684712111
Epoch: 2980, Batch Gradient Norm after: 22.360678668143226
Epoch 2981/10000, Prediction Accuracy = 55.254%, Loss = 1.0469943761825562
Epoch: 2981, Batch Gradient Norm: 39.02973095895152
Epoch: 2981, Batch Gradient Norm after: 22.360675844710663
Epoch 2982/10000, Prediction Accuracy = 55.279999999999994%, Loss = 1.0540724277496338
Epoch: 2982, Batch Gradient Norm: 36.301395023586
Epoch: 2982, Batch Gradient Norm after: 22.360676502889948
Epoch 2983/10000, Prediction Accuracy = 55.263999999999996%, Loss = 1.0466714382171631
Epoch: 2983, Batch Gradient Norm: 39.0108333540671
Epoch: 2983, Batch Gradient Norm after: 22.360675626932924
Epoch 2984/10000, Prediction Accuracy = 55.29%, Loss = 1.0537218570709228
Epoch: 2984, Batch Gradient Norm: 36.29313563876817
Epoch: 2984, Batch Gradient Norm after: 22.360678222595702
Epoch 2985/10000, Prediction Accuracy = 55.282%, Loss = 1.0463491678237915
Epoch: 2985, Batch Gradient Norm: 38.99184324133506
Epoch: 2985, Batch Gradient Norm after: 22.360676426797657
Epoch 2986/10000, Prediction Accuracy = 55.298%, Loss = 1.053376269340515
Epoch: 2986, Batch Gradient Norm: 36.284172753662816
Epoch: 2986, Batch Gradient Norm after: 22.360678755373236
Epoch 2987/10000, Prediction Accuracy = 55.282%, Loss = 1.046021819114685
Epoch: 2987, Batch Gradient Norm: 38.97635914795551
Epoch: 2987, Batch Gradient Norm after: 22.360675927095105
Epoch 2988/10000, Prediction Accuracy = 55.30800000000001%, Loss = 1.0530359745025635
Epoch: 2988, Batch Gradient Norm: 36.27210761658802
Epoch: 2988, Batch Gradient Norm after: 22.36067676872816
Epoch 2989/10000, Prediction Accuracy = 55.3%, Loss = 1.045701265335083
Epoch: 2989, Batch Gradient Norm: 38.95631952901444
Epoch: 2989, Batch Gradient Norm after: 22.360677564226037
Epoch 2990/10000, Prediction Accuracy = 55.31199999999999%, Loss = 1.0526850700378418
Epoch: 2990, Batch Gradient Norm: 36.26309046326944
Epoch: 2990, Batch Gradient Norm after: 22.360677764386228
Epoch 2991/10000, Prediction Accuracy = 55.30800000000001%, Loss = 1.0453765869140625
Epoch: 2991, Batch Gradient Norm: 38.94373492171941
Epoch: 2991, Batch Gradient Norm after: 22.360675219015768
Epoch 2992/10000, Prediction Accuracy = 55.30999999999999%, Loss = 1.0523459911346436
Epoch: 2992, Batch Gradient Norm: 36.25280083803857
Epoch: 2992, Batch Gradient Norm after: 22.360678056866234
Epoch 2993/10000, Prediction Accuracy = 55.318%, Loss = 1.0450494050979615
Epoch: 2993, Batch Gradient Norm: 38.930416404150144
Epoch: 2993, Batch Gradient Norm after: 22.36067453783444
Epoch 2994/10000, Prediction Accuracy = 55.322%, Loss = 1.0520127296447754
Epoch: 2994, Batch Gradient Norm: 36.24236718129342
Epoch: 2994, Batch Gradient Norm after: 22.360677988099596
Epoch 2995/10000, Prediction Accuracy = 55.324%, Loss = 1.0447197914123536
Epoch: 2995, Batch Gradient Norm: 38.90941972130806
Epoch: 2995, Batch Gradient Norm after: 22.36067791968516
Epoch 2996/10000, Prediction Accuracy = 55.32000000000001%, Loss = 1.0516760110855103
Epoch: 2996, Batch Gradient Norm: 36.233098695795135
Epoch: 2996, Batch Gradient Norm after: 22.360678919967643
Epoch 2997/10000, Prediction Accuracy = 55.334%, Loss = 1.044395875930786
Epoch: 2997, Batch Gradient Norm: 38.89656706099299
Epoch: 2997, Batch Gradient Norm after: 22.360676713415344
Epoch 2998/10000, Prediction Accuracy = 55.324%, Loss = 1.051335620880127
Epoch: 2998, Batch Gradient Norm: 36.22363817008586
Epoch: 2998, Batch Gradient Norm after: 22.36067835632921
Epoch 2999/10000, Prediction Accuracy = 55.336%, Loss = 1.0440728187561035
Epoch: 2999, Batch Gradient Norm: 38.87725760746063
Epoch: 2999, Batch Gradient Norm after: 22.36067617849138
Epoch 3000/10000, Prediction Accuracy = 55.327999999999996%, Loss = 1.0509889841079711
Epoch: 3000, Batch Gradient Norm: 36.213130518140574
Epoch: 3000, Batch Gradient Norm after: 22.36067729793119
Epoch 3001/10000, Prediction Accuracy = 55.331999999999994%, Loss = 1.043751049041748
Epoch: 3001, Batch Gradient Norm: 38.8613226792615
Epoch: 3001, Batch Gradient Norm after: 22.360678688994803
Epoch 3002/10000, Prediction Accuracy = 55.326%, Loss = 1.0506560564041139
Epoch: 3002, Batch Gradient Norm: 36.20294777438644
Epoch: 3002, Batch Gradient Norm after: 22.360678095060557
Epoch 3003/10000, Prediction Accuracy = 55.33200000000001%, Loss = 1.0434277772903442
Epoch: 3003, Batch Gradient Norm: 38.85072978602915
Epoch: 3003, Batch Gradient Norm after: 22.36067931267006
Epoch 3004/10000, Prediction Accuracy = 55.336%, Loss = 1.0503224372863769
Epoch: 3004, Batch Gradient Norm: 36.19549146231964
Epoch: 3004, Batch Gradient Norm after: 22.360677278641383
Epoch 3005/10000, Prediction Accuracy = 55.33%, Loss = 1.043099594116211
Epoch: 3005, Batch Gradient Norm: 38.84014520543666
Epoch: 3005, Batch Gradient Norm after: 22.360679279930878
Epoch 3006/10000, Prediction Accuracy = 55.342000000000006%, Loss = 1.0499892473220824
Epoch: 3006, Batch Gradient Norm: 36.18609351342024
Epoch: 3006, Batch Gradient Norm after: 22.360678877614575
Epoch 3007/10000, Prediction Accuracy = 55.346000000000004%, Loss = 1.0427714347839356
Epoch: 3007, Batch Gradient Norm: 38.829745020238654
Epoch: 3007, Batch Gradient Norm after: 22.360677048913836
Epoch 3008/10000, Prediction Accuracy = 55.35%, Loss = 1.0496547222137451
Epoch: 3008, Batch Gradient Norm: 36.1762060500508
Epoch: 3008, Batch Gradient Norm after: 22.360678646994433
Epoch 3009/10000, Prediction Accuracy = 55.346000000000004%, Loss = 1.0424516201019287
Epoch: 3009, Batch Gradient Norm: 38.8186560681133
Epoch: 3009, Batch Gradient Norm after: 22.36067574572683
Epoch 3010/10000, Prediction Accuracy = 55.352%, Loss = 1.0493236303329467
Epoch: 3010, Batch Gradient Norm: 36.168515622464525
Epoch: 3010, Batch Gradient Norm after: 22.360679507533607
Epoch 3011/10000, Prediction Accuracy = 55.354%, Loss = 1.042130184173584
Epoch: 3011, Batch Gradient Norm: 38.805357427258905
Epoch: 3011, Batch Gradient Norm after: 22.360677457392324
Epoch 3012/10000, Prediction Accuracy = 55.366%, Loss = 1.0489938259124756
Epoch: 3012, Batch Gradient Norm: 36.1601566015321
Epoch: 3012, Batch Gradient Norm after: 22.360679983343946
Epoch 3013/10000, Prediction Accuracy = 55.366%, Loss = 1.0418079376220704
Epoch: 3013, Batch Gradient Norm: 38.78795544973589
Epoch: 3013, Batch Gradient Norm after: 22.36067796457297
Epoch 3014/10000, Prediction Accuracy = 55.378%, Loss = 1.0486625671386718
Epoch: 3014, Batch Gradient Norm: 36.148284537671955
Epoch: 3014, Batch Gradient Norm after: 22.360676372108717
Epoch 3015/10000, Prediction Accuracy = 55.370000000000005%, Loss = 1.0414903163909912
Epoch: 3015, Batch Gradient Norm: 38.774619007383066
Epoch: 3015, Batch Gradient Norm after: 22.36067663796387
Epoch 3016/10000, Prediction Accuracy = 55.39%, Loss = 1.0483314514160156
Epoch: 3016, Batch Gradient Norm: 36.14117906740853
Epoch: 3016, Batch Gradient Norm after: 22.360677711150565
Epoch 3017/10000, Prediction Accuracy = 55.38000000000001%, Loss = 1.0411696672439574
Epoch: 3017, Batch Gradient Norm: 38.76313007340135
Epoch: 3017, Batch Gradient Norm after: 22.360676379665627
Epoch 3018/10000, Prediction Accuracy = 55.4%, Loss = 1.048012137413025
Epoch: 3018, Batch Gradient Norm: 36.1308272772918
Epoch: 3018, Batch Gradient Norm after: 22.360678040310052
Epoch 3019/10000, Prediction Accuracy = 55.386%, Loss = 1.0408477306365966
Epoch: 3019, Batch Gradient Norm: 38.749421104450235
Epoch: 3019, Batch Gradient Norm after: 22.360677974521394
Epoch 3020/10000, Prediction Accuracy = 55.407999999999994%, Loss = 1.0476911544799805
Epoch: 3020, Batch Gradient Norm: 36.121268454666485
Epoch: 3020, Batch Gradient Norm after: 22.360676746903398
Epoch 3021/10000, Prediction Accuracy = 55.39%, Loss = 1.0405230283737184
Epoch: 3021, Batch Gradient Norm: 38.73393717397959
Epoch: 3021, Batch Gradient Norm after: 22.360676813214948
Epoch 3022/10000, Prediction Accuracy = 55.410000000000004%, Loss = 1.0473574638366698
Epoch: 3022, Batch Gradient Norm: 36.11360614933918
Epoch: 3022, Batch Gradient Norm after: 22.360675338222745
Epoch 3023/10000, Prediction Accuracy = 55.403999999999996%, Loss = 1.0402071714401244
Epoch: 3023, Batch Gradient Norm: 38.71230974192528
Epoch: 3023, Batch Gradient Norm after: 22.36067963028865
Epoch 3024/10000, Prediction Accuracy = 55.42%, Loss = 1.047015404701233
Epoch: 3024, Batch Gradient Norm: 36.106900691147956
Epoch: 3024, Batch Gradient Norm after: 22.360678087177313
Epoch 3025/10000, Prediction Accuracy = 55.410000000000004%, Loss = 1.0398845911026
Epoch: 3025, Batch Gradient Norm: 38.696592816666886
Epoch: 3025, Batch Gradient Norm after: 22.360678488004677
Epoch 3026/10000, Prediction Accuracy = 55.42999999999999%, Loss = 1.0466866731643676
Epoch: 3026, Batch Gradient Norm: 36.096366425570224
Epoch: 3026, Batch Gradient Norm after: 22.360677560428122
Epoch 3027/10000, Prediction Accuracy = 55.412%, Loss = 1.0395688533782959
Epoch: 3027, Batch Gradient Norm: 38.682838064080556
Epoch: 3027, Batch Gradient Norm after: 22.360677488890715
Epoch 3028/10000, Prediction Accuracy = 55.448%, Loss = 1.0463544368743896
Epoch: 3028, Batch Gradient Norm: 36.08994821793166
Epoch: 3028, Batch Gradient Norm after: 22.36067876721434
Epoch 3029/10000, Prediction Accuracy = 55.41600000000001%, Loss = 1.0392456769943237
Epoch: 3029, Batch Gradient Norm: 38.669007484798875
Epoch: 3029, Batch Gradient Norm after: 22.36067771513066
Epoch 3030/10000, Prediction Accuracy = 55.465999999999994%, Loss = 1.0460287809371949
Epoch: 3030, Batch Gradient Norm: 36.08146792529973
Epoch: 3030, Batch Gradient Norm after: 22.360678784067673
Epoch 3031/10000, Prediction Accuracy = 55.428%, Loss = 1.0389271020889281
Epoch: 3031, Batch Gradient Norm: 38.65585969085453
Epoch: 3031, Batch Gradient Norm after: 22.360676874088256
Epoch 3032/10000, Prediction Accuracy = 55.480000000000004%, Loss = 1.045699119567871
Epoch: 3032, Batch Gradient Norm: 36.071389413516435
Epoch: 3032, Batch Gradient Norm after: 22.360677784691617
Epoch 3033/10000, Prediction Accuracy = 55.443999999999996%, Loss = 1.0386160373687745
Epoch: 3033, Batch Gradient Norm: 38.64313762612757
Epoch: 3033, Batch Gradient Norm after: 22.360676784743553
Epoch 3034/10000, Prediction Accuracy = 55.486000000000004%, Loss = 1.0453748464584351
Epoch: 3034, Batch Gradient Norm: 36.06090810497989
Epoch: 3034, Batch Gradient Norm after: 22.36067828261047
Epoch 3035/10000, Prediction Accuracy = 55.45%, Loss = 1.0382984161376954
Epoch: 3035, Batch Gradient Norm: 38.62840986379302
Epoch: 3035, Batch Gradient Norm after: 22.360674735510326
Epoch 3036/10000, Prediction Accuracy = 55.492000000000004%, Loss = 1.045045518875122
Epoch: 3036, Batch Gradient Norm: 36.05368849690432
Epoch: 3036, Batch Gradient Norm after: 22.36067793667031
Epoch 3037/10000, Prediction Accuracy = 55.467999999999996%, Loss = 1.0379777431488038
Epoch: 3037, Batch Gradient Norm: 38.61325104032338
Epoch: 3037, Batch Gradient Norm after: 22.360676827393636
Epoch 3038/10000, Prediction Accuracy = 55.5%, Loss = 1.0447070121765136
Epoch: 3038, Batch Gradient Norm: 36.04551160144095
Epoch: 3038, Batch Gradient Norm after: 22.36067731556334
Epoch 3039/10000, Prediction Accuracy = 55.484%, Loss = 1.0376639366149902
Epoch: 3039, Batch Gradient Norm: 38.59676695599512
Epoch: 3039, Batch Gradient Norm after: 22.360676580664993
Epoch 3040/10000, Prediction Accuracy = 55.50599999999999%, Loss = 1.0443682670593262
Epoch: 3040, Batch Gradient Norm: 36.03442778884938
Epoch: 3040, Batch Gradient Norm after: 22.36067877430242
Epoch 3041/10000, Prediction Accuracy = 55.49400000000001%, Loss = 1.0373527765274049
Epoch: 3041, Batch Gradient Norm: 38.57645071421842
Epoch: 3041, Batch Gradient Norm after: 22.360677193963525
Epoch 3042/10000, Prediction Accuracy = 55.51400000000001%, Loss = 1.044025456905365
Epoch: 3042, Batch Gradient Norm: 36.024238391672114
Epoch: 3042, Batch Gradient Norm after: 22.36067777869359
Epoch 3043/10000, Prediction Accuracy = 55.504%, Loss = 1.0370434284210206
Epoch: 3043, Batch Gradient Norm: 38.55985031046565
Epoch: 3043, Batch Gradient Norm after: 22.36067679656151
Epoch 3044/10000, Prediction Accuracy = 55.528%, Loss = 1.0436843276023864
Epoch: 3044, Batch Gradient Norm: 36.0167165013129
Epoch: 3044, Batch Gradient Norm after: 22.36067663546805
Epoch 3045/10000, Prediction Accuracy = 55.512%, Loss = 1.036729884147644
Epoch: 3045, Batch Gradient Norm: 38.5378192817285
Epoch: 3045, Batch Gradient Norm after: 22.360677244032804
Epoch 3046/10000, Prediction Accuracy = 55.528000000000006%, Loss = 1.0433319091796875
Epoch: 3046, Batch Gradient Norm: 36.00843175837414
Epoch: 3046, Batch Gradient Norm after: 22.360677807153674
Epoch 3047/10000, Prediction Accuracy = 55.516000000000005%, Loss = 1.0364225149154662
Epoch: 3047, Batch Gradient Norm: 38.52340113077433
Epoch: 3047, Batch Gradient Norm after: 22.360675452533798
Epoch 3048/10000, Prediction Accuracy = 55.54%, Loss = 1.0429976344108582
Epoch: 3048, Batch Gradient Norm: 35.99773184617423
Epoch: 3048, Batch Gradient Norm after: 22.360679092096536
Epoch 3049/10000, Prediction Accuracy = 55.532000000000004%, Loss = 1.0361169576644897
Epoch: 3049, Batch Gradient Norm: 38.50404118472325
Epoch: 3049, Batch Gradient Norm after: 22.360677731298257
Epoch 3050/10000, Prediction Accuracy = 55.552%, Loss = 1.0426462054252625
Epoch: 3050, Batch Gradient Norm: 35.99048979088887
Epoch: 3050, Batch Gradient Norm after: 22.36067813174514
Epoch 3051/10000, Prediction Accuracy = 55.553999999999995%, Loss = 1.035803985595703
Epoch: 3051, Batch Gradient Norm: 38.48780812397804
Epoch: 3051, Batch Gradient Norm after: 22.36067779258422
Epoch 3052/10000, Prediction Accuracy = 55.55800000000001%, Loss = 1.042310607433319
Epoch: 3052, Batch Gradient Norm: 35.982621797091326
Epoch: 3052, Batch Gradient Norm after: 22.360679110013507
Epoch 3053/10000, Prediction Accuracy = 55.565999999999995%, Loss = 1.0354947566986084
Epoch: 3053, Batch Gradient Norm: 38.47066964343116
Epoch: 3053, Batch Gradient Norm after: 22.36067678757937
Epoch 3054/10000, Prediction Accuracy = 55.564%, Loss = 1.0419784545898438
Epoch: 3054, Batch Gradient Norm: 35.97456336767744
Epoch: 3054, Batch Gradient Norm after: 22.36067729269435
Epoch 3055/10000, Prediction Accuracy = 55.565999999999995%, Loss = 1.035179328918457
Epoch: 3055, Batch Gradient Norm: 38.453014672701435
Epoch: 3055, Batch Gradient Norm after: 22.36067820539106
Epoch 3056/10000, Prediction Accuracy = 55.58%, Loss = 1.0416459798812867
Epoch: 3056, Batch Gradient Norm: 35.96673157845019
Epoch: 3056, Batch Gradient Norm after: 22.360678557872415
Epoch 3057/10000, Prediction Accuracy = 55.572%, Loss = 1.0348650217056274
Epoch: 3057, Batch Gradient Norm: 38.43946905709093
Epoch: 3057, Batch Gradient Norm after: 22.360674910857796
Epoch 3058/10000, Prediction Accuracy = 55.604%, Loss = 1.0413216352462769
Epoch: 3058, Batch Gradient Norm: 35.95562718570136
Epoch: 3058, Batch Gradient Norm after: 22.360677853975144
Epoch 3059/10000, Prediction Accuracy = 55.586%, Loss = 1.0345548391342163
Epoch: 3059, Batch Gradient Norm: 38.43138540411678
Epoch: 3059, Batch Gradient Norm after: 22.3606761214063
Epoch 3060/10000, Prediction Accuracy = 55.614%, Loss = 1.0410074830055236
Epoch: 3060, Batch Gradient Norm: 35.9461555821723
Epoch: 3060, Batch Gradient Norm after: 22.360678553291304
Epoch 3061/10000, Prediction Accuracy = 55.596000000000004%, Loss = 1.0342390060424804
Epoch: 3061, Batch Gradient Norm: 38.421843791624106
Epoch: 3061, Batch Gradient Norm after: 22.360678234539186
Epoch 3062/10000, Prediction Accuracy = 55.612%, Loss = 1.0406911969184875
Epoch: 3062, Batch Gradient Norm: 35.93714637861808
Epoch: 3062, Batch Gradient Norm after: 22.360677305630016
Epoch 3063/10000, Prediction Accuracy = 55.604%, Loss = 1.0339215993881226
Epoch: 3063, Batch Gradient Norm: 38.41161135642768
Epoch: 3063, Batch Gradient Norm after: 22.360679282062723
Epoch 3064/10000, Prediction Accuracy = 55.61800000000001%, Loss = 1.0403774857521058
Epoch: 3064, Batch Gradient Norm: 35.92633189289848
Epoch: 3064, Batch Gradient Norm after: 22.3606793290838
Epoch 3065/10000, Prediction Accuracy = 55.598%, Loss = 1.0336090326309204
Epoch: 3065, Batch Gradient Norm: 38.4043449736541
Epoch: 3065, Batch Gradient Norm after: 22.360679197337394
Epoch 3066/10000, Prediction Accuracy = 55.641999999999996%, Loss = 1.0400766372680663
Epoch: 3066, Batch Gradient Norm: 35.91656619846942
Epoch: 3066, Batch Gradient Norm after: 22.360678167295678
Epoch 3067/10000, Prediction Accuracy = 55.6%, Loss = 1.033293604850769
Epoch: 3067, Batch Gradient Norm: 38.391159686965494
Epoch: 3067, Batch Gradient Norm after: 22.36068008104557
Epoch 3068/10000, Prediction Accuracy = 55.64399999999999%, Loss = 1.0397549390792846
Epoch: 3068, Batch Gradient Norm: 35.906023611987194
Epoch: 3068, Batch Gradient Norm after: 22.360676334258244
Epoch 3069/10000, Prediction Accuracy = 55.605999999999995%, Loss = 1.0329806089401246
Epoch: 3069, Batch Gradient Norm: 38.38959763886498
Epoch: 3069, Batch Gradient Norm after: 22.36067792805887
Epoch 3070/10000, Prediction Accuracy = 55.653999999999996%, Loss = 1.0394659757614135
Epoch: 3070, Batch Gradient Norm: 35.89250492219758
Epoch: 3070, Batch Gradient Norm after: 22.360679146757693
Epoch 3071/10000, Prediction Accuracy = 55.61%, Loss = 1.0326664686203002
Epoch: 3071, Batch Gradient Norm: 38.38785963293918
Epoch: 3071, Batch Gradient Norm after: 22.360677718504117
Epoch 3072/10000, Prediction Accuracy = 55.660000000000004%, Loss = 1.0391729950904847
Epoch: 3072, Batch Gradient Norm: 35.88100114711493
Epoch: 3072, Batch Gradient Norm after: 22.360680534460837
Epoch 3073/10000, Prediction Accuracy = 55.620000000000005%, Loss = 1.0323518991470337
Epoch: 3073, Batch Gradient Norm: 38.38084564934497
Epoch: 3073, Batch Gradient Norm after: 22.360677064651593
Epoch 3074/10000, Prediction Accuracy = 55.660000000000004%, Loss = 1.0388782978057862
Epoch: 3074, Batch Gradient Norm: 35.871509773502794
Epoch: 3074, Batch Gradient Norm after: 22.360680752906006
Epoch 3075/10000, Prediction Accuracy = 55.620000000000005%, Loss = 1.0320414543151855
Epoch: 3075, Batch Gradient Norm: 38.37485230835661
Epoch: 3075, Batch Gradient Norm after: 22.360676367288924
Epoch 3076/10000, Prediction Accuracy = 55.664%, Loss = 1.0385759949684144
Epoch: 3076, Batch Gradient Norm: 35.85890467230552
Epoch: 3076, Batch Gradient Norm after: 22.360677636797014
Epoch 3077/10000, Prediction Accuracy = 55.63000000000001%, Loss = 1.0317306041717529
Epoch: 3077, Batch Gradient Norm: 38.36561872306911
Epoch: 3077, Batch Gradient Norm after: 22.36067694834905
Epoch 3078/10000, Prediction Accuracy = 55.682%, Loss = 1.0382608652114869
Epoch: 3078, Batch Gradient Norm: 35.84745484985975
Epoch: 3078, Batch Gradient Norm after: 22.360675829775694
Epoch 3079/10000, Prediction Accuracy = 55.628%, Loss = 1.0314240217208863
Epoch: 3079, Batch Gradient Norm: 38.352861344488
Epoch: 3079, Batch Gradient Norm after: 22.360676549602307
Epoch 3080/10000, Prediction Accuracy = 55.686%, Loss = 1.03794322013855
Epoch: 3080, Batch Gradient Norm: 35.83747135995179
Epoch: 3080, Batch Gradient Norm after: 22.36067707569776
Epoch 3081/10000, Prediction Accuracy = 55.64%, Loss = 1.0311173439025878
Epoch: 3081, Batch Gradient Norm: 38.34596202618143
Epoch: 3081, Batch Gradient Norm after: 22.36067766134692
Epoch 3082/10000, Prediction Accuracy = 55.696000000000005%, Loss = 1.0376351833343507
Epoch: 3082, Batch Gradient Norm: 35.829180476310725
Epoch: 3082, Batch Gradient Norm after: 22.360678421421213
Epoch 3083/10000, Prediction Accuracy = 55.648%, Loss = 1.03080415725708
Epoch: 3083, Batch Gradient Norm: 38.33958571487658
Epoch: 3083, Batch Gradient Norm after: 22.360677090042113
Epoch 3084/10000, Prediction Accuracy = 55.714%, Loss = 1.0373373985290528
Epoch: 3084, Batch Gradient Norm: 35.81758404750403
Epoch: 3084, Batch Gradient Norm after: 22.360677760043508
Epoch 3085/10000, Prediction Accuracy = 55.664%, Loss = 1.0304916381835938
Epoch: 3085, Batch Gradient Norm: 38.34104583899371
Epoch: 3085, Batch Gradient Norm after: 22.360678073427437
Epoch 3086/10000, Prediction Accuracy = 55.73%, Loss = 1.0370415806770326
Epoch: 3086, Batch Gradient Norm: 35.804023271572916
Epoch: 3086, Batch Gradient Norm after: 22.360675557998956
Epoch 3087/10000, Prediction Accuracy = 55.657999999999994%, Loss = 1.0301847457885742
Epoch: 3087, Batch Gradient Norm: 38.34040862793682
Epoch: 3087, Batch Gradient Norm after: 22.360678749538508
Epoch 3088/10000, Prediction Accuracy = 55.733999999999995%, Loss = 1.0367577433586121
Epoch: 3088, Batch Gradient Norm: 35.79203705884554
Epoch: 3088, Batch Gradient Norm after: 22.360676800796327
Epoch 3089/10000, Prediction Accuracy = 55.67%, Loss = 1.029871153831482
Epoch: 3089, Batch Gradient Norm: 38.33934657715073
Epoch: 3089, Batch Gradient Norm after: 22.360676431857968
Epoch 3090/10000, Prediction Accuracy = 55.742000000000004%, Loss = 1.0364765405654908
Epoch: 3090, Batch Gradient Norm: 35.778456526427924
Epoch: 3090, Batch Gradient Norm after: 22.36067740860967
Epoch 3091/10000, Prediction Accuracy = 55.676%, Loss = 1.0295550584793092
Epoch: 3091, Batch Gradient Norm: 38.33736444628135
Epoch: 3091, Batch Gradient Norm after: 22.36067685837384
Epoch 3092/10000, Prediction Accuracy = 55.738%, Loss = 1.036187756061554
Epoch: 3092, Batch Gradient Norm: 35.76850290858164
Epoch: 3092, Batch Gradient Norm after: 22.36067753174632
Epoch 3093/10000, Prediction Accuracy = 55.688%, Loss = 1.0292416095733643
Epoch: 3093, Batch Gradient Norm: 38.33078855174821
Epoch: 3093, Batch Gradient Norm after: 22.360677409835848
Epoch 3094/10000, Prediction Accuracy = 55.75600000000001%, Loss = 1.0358930587768556
Epoch: 3094, Batch Gradient Norm: 35.758917233832655
Epoch: 3094, Batch Gradient Norm after: 22.360678000862656
Epoch 3095/10000, Prediction Accuracy = 55.69%, Loss = 1.0289355754852294
Epoch: 3095, Batch Gradient Norm: 38.325266065596445
Epoch: 3095, Batch Gradient Norm after: 22.36067688957376
Epoch 3096/10000, Prediction Accuracy = 55.76800000000001%, Loss = 1.0355948686599732
Epoch: 3096, Batch Gradient Norm: 35.74678625269312
Epoch: 3096, Batch Gradient Norm after: 22.360679258777395
Epoch 3097/10000, Prediction Accuracy = 55.696000000000005%, Loss = 1.028619933128357
Epoch: 3097, Batch Gradient Norm: 38.315675125104015
Epoch: 3097, Batch Gradient Norm after: 22.3606769402533
Epoch 3098/10000, Prediction Accuracy = 55.778%, Loss = 1.0352839589118958
Epoch: 3098, Batch Gradient Norm: 35.73979588302871
Epoch: 3098, Batch Gradient Norm after: 22.36067876643267
Epoch 3099/10000, Prediction Accuracy = 55.71200000000001%, Loss = 1.0283143758773803
Epoch: 3099, Batch Gradient Norm: 38.310870631333245
Epoch: 3099, Batch Gradient Norm after: 22.360677877412442
Epoch 3100/10000, Prediction Accuracy = 55.778%, Loss = 1.034988570213318
Epoch: 3100, Batch Gradient Norm: 35.73081558313027
Epoch: 3100, Batch Gradient Norm after: 22.360677819637644
Epoch 3101/10000, Prediction Accuracy = 55.720000000000006%, Loss = 1.0280008792877198
Epoch: 3101, Batch Gradient Norm: 38.30177534213351
Epoch: 3101, Batch Gradient Norm after: 22.3606777195634
Epoch 3102/10000, Prediction Accuracy = 55.763999999999996%, Loss = 1.0346829891204834
Epoch: 3102, Batch Gradient Norm: 35.72034485277047
Epoch: 3102, Batch Gradient Norm after: 22.36067678595265
Epoch 3103/10000, Prediction Accuracy = 55.73199999999999%, Loss = 1.0276965379714966
Epoch: 3103, Batch Gradient Norm: 38.29533481287513
Epoch: 3103, Batch Gradient Norm after: 22.360676120020553
Epoch 3104/10000, Prediction Accuracy = 55.772000000000006%, Loss = 1.0343843340873717
Epoch: 3104, Batch Gradient Norm: 35.710701762198546
Epoch: 3104, Batch Gradient Norm after: 22.360677408203145
Epoch 3105/10000, Prediction Accuracy = 55.738%, Loss = 1.0273786544799806
Epoch: 3105, Batch Gradient Norm: 38.290131381655065
Epoch: 3105, Batch Gradient Norm after: 22.360675315288958
Epoch 3106/10000, Prediction Accuracy = 55.79200000000001%, Loss = 1.0340927720069886
Epoch: 3106, Batch Gradient Norm: 35.69871284530726
Epoch: 3106, Batch Gradient Norm after: 22.36067651391322
Epoch 3107/10000, Prediction Accuracy = 55.748000000000005%, Loss = 1.0270702600479127
Epoch: 3107, Batch Gradient Norm: 38.28343550504456
Epoch: 3107, Batch Gradient Norm after: 22.36067801987452
Epoch 3108/10000, Prediction Accuracy = 55.798%, Loss = 1.033796775341034
Epoch: 3108, Batch Gradient Norm: 35.689083644411994
Epoch: 3108, Batch Gradient Norm after: 22.36067722228719
Epoch 3109/10000, Prediction Accuracy = 55.762%, Loss = 1.0267674446105957
Epoch: 3109, Batch Gradient Norm: 38.28147380868765
Epoch: 3109, Batch Gradient Norm after: 22.360676487497436
Epoch 3110/10000, Prediction Accuracy = 55.8%, Loss = 1.0335136294364928
Epoch: 3110, Batch Gradient Norm: 35.680404285270136
Epoch: 3110, Batch Gradient Norm after: 22.36067705468433
Epoch 3111/10000, Prediction Accuracy = 55.758%, Loss = 1.0264562368392944
Epoch: 3111, Batch Gradient Norm: 38.28040769874105
Epoch: 3111, Batch Gradient Norm after: 22.360675511467655
Epoch 3112/10000, Prediction Accuracy = 55.803999999999995%, Loss = 1.0332310438156127
Epoch: 3112, Batch Gradient Norm: 35.66863264157955
Epoch: 3112, Batch Gradient Norm after: 22.360677354613813
Epoch 3113/10000, Prediction Accuracy = 55.766000000000005%, Loss = 1.0261502981185913
Epoch: 3113, Batch Gradient Norm: 38.28645432092321
Epoch: 3113, Batch Gradient Norm after: 22.360674866341185
Epoch 3114/10000, Prediction Accuracy = 55.81%, Loss = 1.0329665303230287
Epoch: 3114, Batch Gradient Norm: 35.65434703286982
Epoch: 3114, Batch Gradient Norm after: 22.36067792202344
Epoch 3115/10000, Prediction Accuracy = 55.786%, Loss = 1.0258402347564697
Epoch: 3115, Batch Gradient Norm: 38.290324419768886
Epoch: 3115, Batch Gradient Norm after: 22.360674139331195
Epoch 3116/10000, Prediction Accuracy = 55.826%, Loss = 1.0327011108398438
Epoch: 3116, Batch Gradient Norm: 35.643321669149934
Epoch: 3116, Batch Gradient Norm after: 22.36067697312813
Epoch 3117/10000, Prediction Accuracy = 55.794000000000004%, Loss = 1.0255303144454957
Epoch: 3117, Batch Gradient Norm: 38.29330146494737
Epoch: 3117, Batch Gradient Norm after: 22.360675346444815
Epoch 3118/10000, Prediction Accuracy = 55.82199999999999%, Loss = 1.0324243187904358
Epoch: 3118, Batch Gradient Norm: 35.632007083656106
Epoch: 3118, Batch Gradient Norm after: 22.360675573615705
Epoch 3119/10000, Prediction Accuracy = 55.803999999999995%, Loss = 1.0252196788787842
Epoch: 3119, Batch Gradient Norm: 38.29347526505347
Epoch: 3119, Batch Gradient Norm after: 22.36067669914018
Epoch 3120/10000, Prediction Accuracy = 55.826%, Loss = 1.0321455240249633
Epoch: 3120, Batch Gradient Norm: 35.621714444223706
Epoch: 3120, Batch Gradient Norm after: 22.360676535532757
Epoch 3121/10000, Prediction Accuracy = 55.81%, Loss = 1.0249075889587402
Epoch: 3121, Batch Gradient Norm: 38.292419698557495
Epoch: 3121, Batch Gradient Norm after: 22.360675964041835
Epoch 3122/10000, Prediction Accuracy = 55.834%, Loss = 1.0318600535392761
Epoch: 3122, Batch Gradient Norm: 35.6106835353409
Epoch: 3122, Batch Gradient Norm after: 22.360677555932885
Epoch 3123/10000, Prediction Accuracy = 55.80800000000001%, Loss = 1.0245935201644898
Epoch: 3123, Batch Gradient Norm: 38.28736531740471
Epoch: 3123, Batch Gradient Norm after: 22.360676395214753
Epoch 3124/10000, Prediction Accuracy = 55.842%, Loss = 1.0315609931945802
Epoch: 3124, Batch Gradient Norm: 35.602507498939765
Epoch: 3124, Batch Gradient Norm after: 22.36068014013588
Epoch 3125/10000, Prediction Accuracy = 55.80799999999999%, Loss = 1.0242886304855348
Epoch: 3125, Batch Gradient Norm: 38.27962413271695
Epoch: 3125, Batch Gradient Norm after: 22.360677274938787
Epoch 3126/10000, Prediction Accuracy = 55.843999999999994%, Loss = 1.031261646747589
Epoch: 3126, Batch Gradient Norm: 35.59422362081536
Epoch: 3126, Batch Gradient Norm after: 22.360678906794927
Epoch 3127/10000, Prediction Accuracy = 55.812%, Loss = 1.0239839553833008
Epoch: 3127, Batch Gradient Norm: 38.27769768132255
Epoch: 3127, Batch Gradient Norm after: 22.360677511773886
Epoch 3128/10000, Prediction Accuracy = 55.85%, Loss = 1.0309759855270386
Epoch: 3128, Batch Gradient Norm: 35.583519472038596
Epoch: 3128, Batch Gradient Norm after: 22.360679466595357
Epoch 3129/10000, Prediction Accuracy = 55.824%, Loss = 1.0236740827560424
Epoch: 3129, Batch Gradient Norm: 38.27565754974155
Epoch: 3129, Batch Gradient Norm after: 22.36067784010279
Epoch 3130/10000, Prediction Accuracy = 55.862%, Loss = 1.0306912183761596
Epoch: 3130, Batch Gradient Norm: 35.570581468229946
Epoch: 3130, Batch Gradient Norm after: 22.36068137908139
Epoch 3131/10000, Prediction Accuracy = 55.83200000000001%, Loss = 1.023366355895996
Epoch: 3131, Batch Gradient Norm: 38.278510294347264
Epoch: 3131, Batch Gradient Norm after: 22.36067934551422
Epoch 3132/10000, Prediction Accuracy = 55.864%, Loss = 1.030431091785431
Epoch: 3132, Batch Gradient Norm: 35.5608533366608
Epoch: 3132, Batch Gradient Norm after: 22.36067737658158
Epoch 3133/10000, Prediction Accuracy = 55.854000000000006%, Loss = 1.0230568170547485
Epoch: 3133, Batch Gradient Norm: 38.277804547974085
Epoch: 3133, Batch Gradient Norm after: 22.360676921445155
Epoch 3134/10000, Prediction Accuracy = 55.872%, Loss = 1.0301434874534607
Epoch: 3134, Batch Gradient Norm: 35.550811873253444
Epoch: 3134, Batch Gradient Norm after: 22.360678123159648
Epoch 3135/10000, Prediction Accuracy = 55.864%, Loss = 1.0227530956268311
Epoch: 3135, Batch Gradient Norm: 38.27154773105125
Epoch: 3135, Batch Gradient Norm after: 22.36067634935116
Epoch 3136/10000, Prediction Accuracy = 55.878%, Loss = 1.0298599362373353
Epoch: 3136, Batch Gradient Norm: 35.54146047090291
Epoch: 3136, Batch Gradient Norm after: 22.360679891668852
Epoch 3137/10000, Prediction Accuracy = 55.864%, Loss = 1.0224350452423097
Epoch: 3137, Batch Gradient Norm: 38.27224043000785
Epoch: 3137, Batch Gradient Norm after: 22.360676379036978
Epoch 3138/10000, Prediction Accuracy = 55.879999999999995%, Loss = 1.02957763671875
Epoch: 3138, Batch Gradient Norm: 35.53272428099382
Epoch: 3138, Batch Gradient Norm after: 22.360678328582168
Epoch 3139/10000, Prediction Accuracy = 55.867999999999995%, Loss = 1.0221267461776733
Epoch: 3139, Batch Gradient Norm: 38.27052144486222
Epoch: 3139, Batch Gradient Norm after: 22.360677803986004
Epoch 3140/10000, Prediction Accuracy = 55.886%, Loss = 1.029286229610443
Epoch: 3140, Batch Gradient Norm: 35.52004308596184
Epoch: 3140, Batch Gradient Norm after: 22.360680128828445
Epoch 3141/10000, Prediction Accuracy = 55.878%, Loss = 1.0218223571777343
Epoch: 3141, Batch Gradient Norm: 38.26337697072632
Epoch: 3141, Batch Gradient Norm after: 22.36067725840408
Epoch 3142/10000, Prediction Accuracy = 55.9%, Loss = 1.0289871573448182
Epoch: 3142, Batch Gradient Norm: 35.51377536512852
Epoch: 3142, Batch Gradient Norm after: 22.360678958415395
Epoch 3143/10000, Prediction Accuracy = 55.89%, Loss = 1.0215163230895996
Epoch: 3143, Batch Gradient Norm: 38.245212792079464
Epoch: 3143, Batch Gradient Norm after: 22.360676011593636
Epoch 3144/10000, Prediction Accuracy = 55.924%, Loss = 1.0286638021469117
Epoch: 3144, Batch Gradient Norm: 35.50517656071085
Epoch: 3144, Batch Gradient Norm after: 22.360676701075374
Epoch 3145/10000, Prediction Accuracy = 55.895999999999994%, Loss = 1.0212148666381835
Epoch: 3145, Batch Gradient Norm: 38.23069462006786
Epoch: 3145, Batch Gradient Norm after: 22.360677115207388
Epoch 3146/10000, Prediction Accuracy = 55.92199999999999%, Loss = 1.028343880176544
Epoch: 3146, Batch Gradient Norm: 35.49575376781308
Epoch: 3146, Batch Gradient Norm after: 22.360678218237638
Epoch 3147/10000, Prediction Accuracy = 55.903999999999996%, Loss = 1.0209123373031617
Epoch: 3147, Batch Gradient Norm: 38.210396008693095
Epoch: 3147, Batch Gradient Norm after: 22.360677935659766
Epoch 3148/10000, Prediction Accuracy = 55.922000000000004%, Loss = 1.0280096650123596
Epoch: 3148, Batch Gradient Norm: 35.486381108666684
Epoch: 3148, Batch Gradient Norm after: 22.3606796450426
Epoch 3149/10000, Prediction Accuracy = 55.91199999999999%, Loss = 1.0206052541732789
Epoch: 3149, Batch Gradient Norm: 38.19268434707762
Epoch: 3149, Batch Gradient Norm after: 22.360677860324156
Epoch 3150/10000, Prediction Accuracy = 55.916%, Loss = 1.0276772856712342
Epoch: 3150, Batch Gradient Norm: 35.4798293420861
Epoch: 3150, Batch Gradient Norm after: 22.360680142196234
Epoch 3151/10000, Prediction Accuracy = 55.912%, Loss = 1.020299530029297
Epoch: 3151, Batch Gradient Norm: 38.1647519776205
Epoch: 3151, Batch Gradient Norm after: 22.36067823782942
Epoch 3152/10000, Prediction Accuracy = 55.926%, Loss = 1.0273226141929626
Epoch: 3152, Batch Gradient Norm: 35.47238269555749
Epoch: 3152, Batch Gradient Norm after: 22.36067991338131
Epoch 3153/10000, Prediction Accuracy = 55.92%, Loss = 1.020003366470337
Epoch: 3153, Batch Gradient Norm: 38.1417146882294
Epoch: 3153, Batch Gradient Norm after: 22.360678374486067
Epoch 3154/10000, Prediction Accuracy = 55.94%, Loss = 1.026975893974304
Epoch: 3154, Batch Gradient Norm: 35.46496579272009
Epoch: 3154, Batch Gradient Norm after: 22.360679738082652
Epoch 3155/10000, Prediction Accuracy = 55.922000000000004%, Loss = 1.0197008848190308
Epoch: 3155, Batch Gradient Norm: 38.11825704444865
Epoch: 3155, Batch Gradient Norm after: 22.360678974306104
Epoch 3156/10000, Prediction Accuracy = 55.94%, Loss = 1.0266321063041688
Epoch: 3156, Batch Gradient Norm: 35.454850140146974
Epoch: 3156, Batch Gradient Norm after: 22.360680252192868
Epoch 3157/10000, Prediction Accuracy = 55.938%, Loss = 1.019404673576355
Epoch: 3157, Batch Gradient Norm: 38.092401187307516
Epoch: 3157, Batch Gradient Norm after: 22.360677948854192
Epoch 3158/10000, Prediction Accuracy = 55.943999999999996%, Loss = 1.0262907505035401
Epoch: 3158, Batch Gradient Norm: 35.444324498485315
Epoch: 3158, Batch Gradient Norm after: 22.360676790560134
Epoch 3159/10000, Prediction Accuracy = 55.95%, Loss = 1.019106936454773
Epoch: 3159, Batch Gradient Norm: 38.065482431057454
Epoch: 3159, Batch Gradient Norm after: 22.36067762660807
Epoch 3160/10000, Prediction Accuracy = 55.94199999999999%, Loss = 1.025942051410675
Epoch: 3160, Batch Gradient Norm: 35.43601338222695
Epoch: 3160, Batch Gradient Norm after: 22.360676310255776
Epoch 3161/10000, Prediction Accuracy = 55.964%, Loss = 1.0188145399093629
Epoch: 3161, Batch Gradient Norm: 38.04294952311568
Epoch: 3161, Batch Gradient Norm after: 22.36067814261995
Epoch 3162/10000, Prediction Accuracy = 55.944%, Loss = 1.0256060719490052
Epoch: 3162, Batch Gradient Norm: 35.42806214040812
Epoch: 3162, Batch Gradient Norm after: 22.36067776894035
Epoch 3163/10000, Prediction Accuracy = 55.96600000000001%, Loss = 1.018515658378601
Epoch: 3163, Batch Gradient Norm: 38.02579533073995
Epoch: 3163, Batch Gradient Norm after: 22.360679553026337
Epoch 3164/10000, Prediction Accuracy = 55.936%, Loss = 1.025280809402466
Epoch: 3164, Batch Gradient Norm: 35.41814843553971
Epoch: 3164, Batch Gradient Norm after: 22.36067796458014
Epoch 3165/10000, Prediction Accuracy = 55.967999999999996%, Loss = 1.0182167053222657
Epoch: 3165, Batch Gradient Norm: 38.01156134218354
Epoch: 3165, Batch Gradient Norm after: 22.360678466387125
Epoch 3166/10000, Prediction Accuracy = 55.94199999999999%, Loss = 1.024971330165863
Epoch: 3166, Batch Gradient Norm: 35.40939117427765
Epoch: 3166, Batch Gradient Norm after: 22.36067799230093
Epoch 3167/10000, Prediction Accuracy = 55.976%, Loss = 1.017911958694458
Epoch: 3167, Batch Gradient Norm: 37.99567589855215
Epoch: 3167, Batch Gradient Norm after: 22.3606782776869
Epoch 3168/10000, Prediction Accuracy = 55.956%, Loss = 1.0246617197990417
Epoch: 3168, Batch Gradient Norm: 35.399300477750195
Epoch: 3168, Batch Gradient Norm after: 22.360678264991556
Epoch 3169/10000, Prediction Accuracy = 55.977999999999994%, Loss = 1.0176162242889404
Epoch: 3169, Batch Gradient Norm: 37.98250117885364
Epoch: 3169, Batch Gradient Norm after: 22.360677556226904
Epoch 3170/10000, Prediction Accuracy = 55.962%, Loss = 1.0243515253067017
Epoch: 3170, Batch Gradient Norm: 35.38809311062704
Epoch: 3170, Batch Gradient Norm after: 22.36067962556974
Epoch 3171/10000, Prediction Accuracy = 55.977999999999994%, Loss = 1.0173139095306396
Epoch: 3171, Batch Gradient Norm: 37.967101229928424
Epoch: 3171, Batch Gradient Norm after: 22.360676058244714
Epoch 3172/10000, Prediction Accuracy = 55.972%, Loss = 1.0240257024765014
Epoch: 3172, Batch Gradient Norm: 35.37939079809243
Epoch: 3172, Batch Gradient Norm after: 22.360679023508904
Epoch 3173/10000, Prediction Accuracy = 55.983999999999995%, Loss = 1.0170140504837035
Epoch: 3173, Batch Gradient Norm: 37.9467479432653
Epoch: 3173, Batch Gradient Norm after: 22.360674870256542
Epoch 3174/10000, Prediction Accuracy = 55.984%, Loss = 1.02370228767395
Epoch: 3174, Batch Gradient Norm: 35.37285850167938
Epoch: 3174, Batch Gradient Norm after: 22.360677882931416
Epoch 3175/10000, Prediction Accuracy = 55.998000000000005%, Loss = 1.016717553138733
Epoch: 3175, Batch Gradient Norm: 37.92649906786116
Epoch: 3175, Batch Gradient Norm after: 22.360676122669446
Epoch 3176/10000, Prediction Accuracy = 55.989999999999995%, Loss = 1.0233765959739685
Epoch: 3176, Batch Gradient Norm: 35.36278055055954
Epoch: 3176, Batch Gradient Norm after: 22.360678090179906
Epoch 3177/10000, Prediction Accuracy = 56.004%, Loss = 1.0164152383804321
Epoch: 3177, Batch Gradient Norm: 37.910536835282706
Epoch: 3177, Batch Gradient Norm after: 22.36067654184445
Epoch 3178/10000, Prediction Accuracy = 55.986000000000004%, Loss = 1.0230595350265503
Epoch: 3178, Batch Gradient Norm: 35.35217529916107
Epoch: 3178, Batch Gradient Norm after: 22.360677752006303
Epoch 3179/10000, Prediction Accuracy = 56.01399999999999%, Loss = 1.016115951538086
Epoch: 3179, Batch Gradient Norm: 37.895912011246246
Epoch: 3179, Batch Gradient Norm after: 22.360676599683416
Epoch 3180/10000, Prediction Accuracy = 55.986000000000004%, Loss = 1.0227509021759034
Epoch: 3180, Batch Gradient Norm: 35.34120523773881
Epoch: 3180, Batch Gradient Norm after: 22.360677725290223
Epoch 3181/10000, Prediction Accuracy = 56.025999999999996%, Loss = 1.01581871509552
Epoch: 3181, Batch Gradient Norm: 37.878341568748255
Epoch: 3181, Batch Gradient Norm after: 22.360677132538292
Epoch 3182/10000, Prediction Accuracy = 55.984%, Loss = 1.0224377155303954
Epoch: 3182, Batch Gradient Norm: 35.33226768820663
Epoch: 3182, Batch Gradient Norm after: 22.36067696038215
Epoch 3183/10000, Prediction Accuracy = 56.01800000000001%, Loss = 1.0155228137969972
Epoch: 3183, Batch Gradient Norm: 37.86031042729533
Epoch: 3183, Batch Gradient Norm after: 22.360678136692115
Epoch 3184/10000, Prediction Accuracy = 55.976%, Loss = 1.022112202644348
Epoch: 3184, Batch Gradient Norm: 35.324265134089664
Epoch: 3184, Batch Gradient Norm after: 22.36067886622505
Epoch 3185/10000, Prediction Accuracy = 56.013999999999996%, Loss = 1.0152243852615357
Epoch: 3185, Batch Gradient Norm: 37.84306240222216
Epoch: 3185, Batch Gradient Norm after: 22.360675699170553
Epoch 3186/10000, Prediction Accuracy = 55.984%, Loss = 1.0217835545539855
Epoch: 3186, Batch Gradient Norm: 35.312833066361115
Epoch: 3186, Batch Gradient Norm after: 22.36067653937427
Epoch 3187/10000, Prediction Accuracy = 56.013999999999996%, Loss = 1.0149249315261841
Epoch: 3187, Batch Gradient Norm: 37.82887009304138
Epoch: 3187, Batch Gradient Norm after: 22.360676540202796
Epoch 3188/10000, Prediction Accuracy = 55.99399999999999%, Loss = 1.0214844584465026
Epoch: 3188, Batch Gradient Norm: 35.302999289238116
Epoch: 3188, Batch Gradient Norm after: 22.36067551340803
Epoch 3189/10000, Prediction Accuracy = 56.013999999999996%, Loss = 1.0146248817443848
Epoch: 3189, Batch Gradient Norm: 37.815285842201796
Epoch: 3189, Batch Gradient Norm after: 22.360675300514306
Epoch 3190/10000, Prediction Accuracy = 56.013999999999996%, Loss = 1.0211791276931763
Epoch: 3190, Batch Gradient Norm: 35.29345848501084
Epoch: 3190, Batch Gradient Norm after: 22.360679102786815
Epoch 3191/10000, Prediction Accuracy = 56.022000000000006%, Loss = 1.014327096939087
Epoch: 3191, Batch Gradient Norm: 37.80832051419951
Epoch: 3191, Batch Gradient Norm after: 22.360676790223188
Epoch 3192/10000, Prediction Accuracy = 56.01800000000001%, Loss = 1.0208854913711547
Epoch: 3192, Batch Gradient Norm: 35.28083925458109
Epoch: 3192, Batch Gradient Norm after: 22.360676146291308
Epoch 3193/10000, Prediction Accuracy = 56.022000000000006%, Loss = 1.014024519920349
Epoch: 3193, Batch Gradient Norm: 37.7999108129622
Epoch: 3193, Batch Gradient Norm after: 22.36067641242205
Epoch 3194/10000, Prediction Accuracy = 56.029999999999994%, Loss = 1.0205957412719726
Epoch: 3194, Batch Gradient Norm: 35.26691369772163
Epoch: 3194, Batch Gradient Norm after: 22.360676403607297
Epoch 3195/10000, Prediction Accuracy = 56.019999999999996%, Loss = 1.0137258291244506
Epoch: 3195, Batch Gradient Norm: 37.79605714460561
Epoch: 3195, Batch Gradient Norm after: 22.360676510237756
Epoch 3196/10000, Prediction Accuracy = 56.038%, Loss = 1.0203198313713073
Epoch: 3196, Batch Gradient Norm: 35.25463087906979
Epoch: 3196, Batch Gradient Norm after: 22.360679090963547
Epoch 3197/10000, Prediction Accuracy = 56.02600000000001%, Loss = 1.0134227514266967
Epoch: 3197, Batch Gradient Norm: 37.78810849284015
Epoch: 3197, Batch Gradient Norm after: 22.360677244265347
Epoch 3198/10000, Prediction Accuracy = 56.04200000000001%, Loss = 1.0200295209884644
Epoch: 3198, Batch Gradient Norm: 35.2433123905773
Epoch: 3198, Batch Gradient Norm after: 22.360675047425367
Epoch 3199/10000, Prediction Accuracy = 56.028000000000006%, Loss = 1.013120436668396
Epoch: 3199, Batch Gradient Norm: 37.78082625125564
Epoch: 3199, Batch Gradient Norm after: 22.360677003412167
Epoch 3200/10000, Prediction Accuracy = 56.05%, Loss = 1.0197478175163268
Epoch: 3200, Batch Gradient Norm: 35.2294707202358
Epoch: 3200, Batch Gradient Norm after: 22.36067480751398
Epoch 3201/10000, Prediction Accuracy = 56.032%, Loss = 1.012817931175232
Epoch: 3201, Batch Gradient Norm: 37.7788049235997
Epoch: 3201, Batch Gradient Norm after: 22.36067703280405
Epoch 3202/10000, Prediction Accuracy = 56.05%, Loss = 1.019465410709381
Epoch: 3202, Batch Gradient Norm: 35.219770851678135
Epoch: 3202, Batch Gradient Norm after: 22.36067714817496
Epoch 3203/10000, Prediction Accuracy = 56.038%, Loss = 1.0125189304351807
Epoch: 3203, Batch Gradient Norm: 37.77361372590698
Epoch: 3203, Batch Gradient Norm after: 22.36067734840356
Epoch 3204/10000, Prediction Accuracy = 56.052%, Loss = 1.0191827178001405
Epoch: 3204, Batch Gradient Norm: 35.20719642485581
Epoch: 3204, Batch Gradient Norm after: 22.360678565348373
Epoch 3205/10000, Prediction Accuracy = 56.028%, Loss = 1.0122187614440918
Epoch: 3205, Batch Gradient Norm: 37.76893080101326
Epoch: 3205, Batch Gradient Norm after: 22.36067813076311
Epoch 3206/10000, Prediction Accuracy = 56.062%, Loss = 1.0189021587371827
Epoch: 3206, Batch Gradient Norm: 35.19566912660642
Epoch: 3206, Batch Gradient Norm after: 22.36067721197457
Epoch 3207/10000, Prediction Accuracy = 56.028%, Loss = 1.0119258403778075
Epoch: 3207, Batch Gradient Norm: 37.76293573910232
Epoch: 3207, Batch Gradient Norm after: 22.360676770003348
Epoch 3208/10000, Prediction Accuracy = 56.065999999999995%, Loss = 1.018615448474884
Epoch: 3208, Batch Gradient Norm: 35.18534538521411
Epoch: 3208, Batch Gradient Norm after: 22.36067743927206
Epoch 3209/10000, Prediction Accuracy = 56.032%, Loss = 1.011626124382019
Epoch: 3209, Batch Gradient Norm: 37.75403009866516
Epoch: 3209, Batch Gradient Norm after: 22.360674634286074
Epoch 3210/10000, Prediction Accuracy = 56.062%, Loss = 1.0183233380317689
Epoch: 3210, Batch Gradient Norm: 35.174625874390344
Epoch: 3210, Batch Gradient Norm after: 22.360679594018414
Epoch 3211/10000, Prediction Accuracy = 56.040000000000006%, Loss = 1.011327052116394
Epoch: 3211, Batch Gradient Norm: 37.74003986967438
Epoch: 3211, Batch Gradient Norm after: 22.360677844238012
Epoch 3212/10000, Prediction Accuracy = 56.084%, Loss = 1.018014395236969
Epoch: 3212, Batch Gradient Norm: 35.16258771550796
Epoch: 3212, Batch Gradient Norm after: 22.36067667905642
Epoch 3213/10000, Prediction Accuracy = 56.05%, Loss = 1.0110307455062866
Epoch: 3213, Batch Gradient Norm: 37.727148984324785
Epoch: 3213, Batch Gradient Norm after: 22.360677711518797
Epoch 3214/10000, Prediction Accuracy = 56.096000000000004%, Loss = 1.0177095890045167
Epoch: 3214, Batch Gradient Norm: 35.15148298730358
Epoch: 3214, Batch Gradient Norm after: 22.3606782908321
Epoch 3215/10000, Prediction Accuracy = 56.072%, Loss = 1.0107309341430664
Epoch: 3215, Batch Gradient Norm: 37.714176714802186
Epoch: 3215, Batch Gradient Norm after: 22.360678783444904
Epoch 3216/10000, Prediction Accuracy = 56.1%, Loss = 1.0174088954925538
Epoch: 3216, Batch Gradient Norm: 35.13876572211574
Epoch: 3216, Batch Gradient Norm after: 22.360676627435414
Epoch 3217/10000, Prediction Accuracy = 56.084%, Loss = 1.010430908203125
Epoch: 3217, Batch Gradient Norm: 37.702565022649374
Epoch: 3217, Batch Gradient Norm after: 22.360678271069734
Epoch 3218/10000, Prediction Accuracy = 56.117999999999995%, Loss = 1.0171112298965455
Epoch: 3218, Batch Gradient Norm: 35.12884352026826
Epoch: 3218, Batch Gradient Norm after: 22.36067766522187
Epoch 3219/10000, Prediction Accuracy = 56.08399999999999%, Loss = 1.0101310014724731
Epoch: 3219, Batch Gradient Norm: 37.692259912622916
Epoch: 3219, Batch Gradient Norm after: 22.36067694615141
Epoch 3220/10000, Prediction Accuracy = 56.11600000000001%, Loss = 1.0168182969093322
Epoch: 3220, Batch Gradient Norm: 35.119303674881074
Epoch: 3220, Batch Gradient Norm after: 22.360678120990734
Epoch 3221/10000, Prediction Accuracy = 56.09400000000001%, Loss = 1.0098360538482667
Epoch: 3221, Batch Gradient Norm: 37.68089406067546
Epoch: 3221, Batch Gradient Norm after: 22.360675660741183
Epoch 3222/10000, Prediction Accuracy = 56.122%, Loss = 1.0165222764015198
Epoch: 3222, Batch Gradient Norm: 35.10736564962649
Epoch: 3222, Batch Gradient Norm after: 22.360678907372694
Epoch 3223/10000, Prediction Accuracy = 56.09599999999999%, Loss = 1.0095423936843873
Epoch: 3223, Batch Gradient Norm: 37.678775717959304
Epoch: 3223, Batch Gradient Norm after: 22.360677077021837
Epoch 3224/10000, Prediction Accuracy = 56.122%, Loss = 1.0162371039390563
Epoch: 3224, Batch Gradient Norm: 35.09523749038943
Epoch: 3224, Batch Gradient Norm after: 22.36067687031435
Epoch 3225/10000, Prediction Accuracy = 56.10600000000001%, Loss = 1.009246826171875
Epoch: 3225, Batch Gradient Norm: 37.67450262137888
Epoch: 3225, Batch Gradient Norm after: 22.360676430316467
Epoch 3226/10000, Prediction Accuracy = 56.126%, Loss = 1.0159579753875732
Epoch: 3226, Batch Gradient Norm: 35.08498286833939
Epoch: 3226, Batch Gradient Norm after: 22.36067626909551
Epoch 3227/10000, Prediction Accuracy = 56.116%, Loss = 1.008946681022644
Epoch: 3227, Batch Gradient Norm: 37.673798029149744
Epoch: 3227, Batch Gradient Norm after: 22.360677827221824
Epoch 3228/10000, Prediction Accuracy = 56.128%, Loss = 1.015685784816742
Epoch: 3228, Batch Gradient Norm: 35.07317649329747
Epoch: 3228, Batch Gradient Norm after: 22.360676576895855
Epoch 3229/10000, Prediction Accuracy = 56.120000000000005%, Loss = 1.0086510896682739
Epoch: 3229, Batch Gradient Norm: 37.671211855096104
Epoch: 3229, Batch Gradient Norm after: 22.360676940473272
Epoch 3230/10000, Prediction Accuracy = 56.138%, Loss = 1.015410876274109
Epoch: 3230, Batch Gradient Norm: 35.06067123964864
Epoch: 3230, Batch Gradient Norm after: 22.360677068363774
Epoch 3231/10000, Prediction Accuracy = 56.134%, Loss = 1.0083545684814452
Epoch: 3231, Batch Gradient Norm: 37.668761748902455
Epoch: 3231, Batch Gradient Norm after: 22.36067714319579
Epoch 3232/10000, Prediction Accuracy = 56.15400000000001%, Loss = 1.0151353120803832
Epoch: 3232, Batch Gradient Norm: 35.04975748571128
Epoch: 3232, Batch Gradient Norm after: 22.360676646019797
Epoch 3233/10000, Prediction Accuracy = 56.152%, Loss = 1.0080549240112304
Epoch: 3233, Batch Gradient Norm: 37.66228137200801
Epoch: 3233, Batch Gradient Norm after: 22.360677000758162
Epoch 3234/10000, Prediction Accuracy = 56.158%, Loss = 1.0148480534553528
Epoch: 3234, Batch Gradient Norm: 35.03925377695701
Epoch: 3234, Batch Gradient Norm after: 22.360678144790434
Epoch 3235/10000, Prediction Accuracy = 56.153999999999996%, Loss = 1.007757592201233
Epoch: 3235, Batch Gradient Norm: 37.65413626449184
Epoch: 3235, Batch Gradient Norm after: 22.360676918834407
Epoch 3236/10000, Prediction Accuracy = 56.169999999999995%, Loss = 1.0145671963691711
Epoch: 3236, Batch Gradient Norm: 35.026762792689006
Epoch: 3236, Batch Gradient Norm after: 22.36067645861141
Epoch 3237/10000, Prediction Accuracy = 56.16199999999999%, Loss = 1.0074645757675171
Epoch: 3237, Batch Gradient Norm: 37.65035423649639
Epoch: 3237, Batch Gradient Norm after: 22.36067778802235
Epoch 3238/10000, Prediction Accuracy = 56.176%, Loss = 1.0142857313156128
Epoch: 3238, Batch Gradient Norm: 35.01627998530025
Epoch: 3238, Batch Gradient Norm after: 22.36067626124319
Epoch 3239/10000, Prediction Accuracy = 56.166%, Loss = 1.00716552734375
Epoch: 3239, Batch Gradient Norm: 37.64699618871628
Epoch: 3239, Batch Gradient Norm after: 22.36067463136866
Epoch 3240/10000, Prediction Accuracy = 56.18599999999999%, Loss = 1.014009916782379
Epoch: 3240, Batch Gradient Norm: 35.007830753837766
Epoch: 3240, Batch Gradient Norm after: 22.36067624684209
Epoch 3241/10000, Prediction Accuracy = 56.184000000000005%, Loss = 1.006866979598999
Epoch: 3241, Batch Gradient Norm: 37.639394217103586
Epoch: 3241, Batch Gradient Norm after: 22.360674968325135
Epoch 3242/10000, Prediction Accuracy = 56.196000000000005%, Loss = 1.0137185096740722
Epoch: 3242, Batch Gradient Norm: 34.99644872976267
Epoch: 3242, Batch Gradient Norm after: 22.360678238275064
Epoch 3243/10000, Prediction Accuracy = 56.193999999999996%, Loss = 1.0065716981887818
Epoch: 3243, Batch Gradient Norm: 37.6293796992515
Epoch: 3243, Batch Gradient Norm after: 22.36067870956018
Epoch 3244/10000, Prediction Accuracy = 56.19199999999999%, Loss = 1.0134276032447815
Epoch: 3244, Batch Gradient Norm: 34.98720133430082
Epoch: 3244, Batch Gradient Norm after: 22.360676492375287
Epoch 3245/10000, Prediction Accuracy = 56.205999999999996%, Loss = 1.0062810897827148
Epoch: 3245, Batch Gradient Norm: 37.61564643996329
Epoch: 3245, Batch Gradient Norm after: 22.36067768987442
Epoch 3246/10000, Prediction Accuracy = 56.21400000000001%, Loss = 1.0131218433380127
Epoch: 3246, Batch Gradient Norm: 34.97488586204195
Epoch: 3246, Batch Gradient Norm after: 22.36067882213505
Epoch 3247/10000, Prediction Accuracy = 56.215999999999994%, Loss = 1.0059889078140258
Epoch: 3247, Batch Gradient Norm: 37.59967071859365
Epoch: 3247, Batch Gradient Norm after: 22.36067956023223
Epoch 3248/10000, Prediction Accuracy = 56.222%, Loss = 1.0128137588500976
Epoch: 3248, Batch Gradient Norm: 34.9643548680188
Epoch: 3248, Batch Gradient Norm after: 22.360676970048043
Epoch 3249/10000, Prediction Accuracy = 56.220000000000006%, Loss = 1.0056987285614014
Epoch: 3249, Batch Gradient Norm: 37.58471890279778
Epoch: 3249, Batch Gradient Norm after: 22.360676873069558
Epoch 3250/10000, Prediction Accuracy = 56.226%, Loss = 1.0125067234039307
Epoch: 3250, Batch Gradient Norm: 34.95559850863223
Epoch: 3250, Batch Gradient Norm after: 22.360677314890438
Epoch 3251/10000, Prediction Accuracy = 56.226%, Loss = 1.0054073452949523
Epoch: 3251, Batch Gradient Norm: 37.56879520046615
Epoch: 3251, Batch Gradient Norm after: 22.36068025017534
Epoch 3252/10000, Prediction Accuracy = 56.239999999999995%, Loss = 1.0121962785720826
Epoch: 3252, Batch Gradient Norm: 34.946731981055265
Epoch: 3252, Batch Gradient Norm after: 22.360674734606434
Epoch 3253/10000, Prediction Accuracy = 56.234%, Loss = 1.005118978023529
Epoch: 3253, Batch Gradient Norm: 37.55186678501217
Epoch: 3253, Batch Gradient Norm after: 22.360677449360555
Epoch 3254/10000, Prediction Accuracy = 56.254%, Loss = 1.0118844866752625
Epoch: 3254, Batch Gradient Norm: 34.93702072483724
Epoch: 3254, Batch Gradient Norm after: 22.36067635367075
Epoch 3255/10000, Prediction Accuracy = 56.237999999999985%, Loss = 1.004826295375824
Epoch: 3255, Batch Gradient Norm: 37.53760288457545
Epoch: 3255, Batch Gradient Norm after: 22.360676345798762
Epoch 3256/10000, Prediction Accuracy = 56.263999999999996%, Loss = 1.011580753326416
Epoch: 3256, Batch Gradient Norm: 34.92644777185985
Epoch: 3256, Batch Gradient Norm after: 22.360675901369657
Epoch 3257/10000, Prediction Accuracy = 56.24400000000001%, Loss = 1.0045393705368042
Epoch: 3257, Batch Gradient Norm: 37.520394337993565
Epoch: 3257, Batch Gradient Norm after: 22.36067942119449
Epoch 3258/10000, Prediction Accuracy = 56.274%, Loss = 1.0112681627273559
Epoch: 3258, Batch Gradient Norm: 34.918084934944375
Epoch: 3258, Batch Gradient Norm after: 22.36067545011328
Epoch 3259/10000, Prediction Accuracy = 56.254%, Loss = 1.0042477130889893
Epoch: 3259, Batch Gradient Norm: 37.511059309810086
Epoch: 3259, Batch Gradient Norm after: 22.36067892236699
Epoch 3260/10000, Prediction Accuracy = 56.267999999999994%, Loss = 1.0109810471534728
Epoch: 3260, Batch Gradient Norm: 34.90972810992996
Epoch: 3260, Batch Gradient Norm after: 22.360677538408456
Epoch 3261/10000, Prediction Accuracy = 56.260000000000005%, Loss = 1.0039551258087158
Epoch: 3261, Batch Gradient Norm: 37.49691334869828
Epoch: 3261, Batch Gradient Norm after: 22.36067734930555
Epoch 3262/10000, Prediction Accuracy = 56.284000000000006%, Loss = 1.0106797814369202
Epoch: 3262, Batch Gradient Norm: 34.900512915356536
Epoch: 3262, Batch Gradient Norm after: 22.36067576446539
Epoch 3263/10000, Prediction Accuracy = 56.260000000000005%, Loss = 1.0036608576774597
Epoch: 3263, Batch Gradient Norm: 37.47754438044971
Epoch: 3263, Batch Gradient Norm after: 22.36067509700493
Epoch 3264/10000, Prediction Accuracy = 56.286%, Loss = 1.0103628754615783
Epoch: 3264, Batch Gradient Norm: 34.89322283901902
Epoch: 3264, Batch Gradient Norm after: 22.360676989358492
Epoch 3265/10000, Prediction Accuracy = 56.272000000000006%, Loss = 1.003373146057129
Epoch: 3265, Batch Gradient Norm: 37.46163269313283
Epoch: 3265, Batch Gradient Norm after: 22.360677072512395
Epoch 3266/10000, Prediction Accuracy = 56.29%, Loss = 1.0100445747375488
Epoch: 3266, Batch Gradient Norm: 34.88524452122372
Epoch: 3266, Batch Gradient Norm after: 22.360677348620516
Epoch 3267/10000, Prediction Accuracy = 56.284000000000006%, Loss = 1.0030855178833007
Epoch: 3267, Batch Gradient Norm: 37.44247598057778
Epoch: 3267, Batch Gradient Norm after: 22.360678706289566
Epoch 3268/10000, Prediction Accuracy = 56.286%, Loss = 1.0097281813621521
Epoch: 3268, Batch Gradient Norm: 34.87731849566149
Epoch: 3268, Batch Gradient Norm after: 22.360676093733822
Epoch 3269/10000, Prediction Accuracy = 56.288%, Loss = 1.0027956008911132
Epoch: 3269, Batch Gradient Norm: 37.427173729629146
Epoch: 3269, Batch Gradient Norm after: 22.360677115985254
Epoch 3270/10000, Prediction Accuracy = 56.288%, Loss = 1.0094282746315002
Epoch: 3270, Batch Gradient Norm: 34.86940086850667
Epoch: 3270, Batch Gradient Norm after: 22.360676894133693
Epoch 3271/10000, Prediction Accuracy = 56.29600000000001%, Loss = 1.002504861354828
Epoch: 3271, Batch Gradient Norm: 37.41293210720873
Epoch: 3271, Batch Gradient Norm after: 22.360679260709134
Epoch 3272/10000, Prediction Accuracy = 56.294%, Loss = 1.0091249465942382
Epoch: 3272, Batch Gradient Norm: 34.86066680933084
Epoch: 3272, Batch Gradient Norm after: 22.3606741676982
Epoch 3273/10000, Prediction Accuracy = 56.30799999999999%, Loss = 1.0022145748138427
Epoch: 3273, Batch Gradient Norm: 37.40241442191935
Epoch: 3273, Batch Gradient Norm after: 22.36067725642094
Epoch 3274/10000, Prediction Accuracy = 56.291999999999994%, Loss = 1.008826732635498
Epoch: 3274, Batch Gradient Norm: 34.85095726716468
Epoch: 3274, Batch Gradient Norm after: 22.360677841396935
Epoch 3275/10000, Prediction Accuracy = 56.324%, Loss = 1.0019267320632934
Epoch: 3275, Batch Gradient Norm: 37.387262852894565
Epoch: 3275, Batch Gradient Norm after: 22.36067631358973
Epoch 3276/10000, Prediction Accuracy = 56.298%, Loss = 1.0085237503051758
Epoch: 3276, Batch Gradient Norm: 34.84044312005922
Epoch: 3276, Batch Gradient Norm after: 22.360676076508124
Epoch 3277/10000, Prediction Accuracy = 56.331999999999994%, Loss = 1.0016451358795166
Epoch: 3277, Batch Gradient Norm: 37.367360129095665
Epoch: 3277, Batch Gradient Norm after: 22.360677604573137
Epoch 3278/10000, Prediction Accuracy = 56.312%, Loss = 1.0082016825675963
Epoch: 3278, Batch Gradient Norm: 34.8307381951593
Epoch: 3278, Batch Gradient Norm after: 22.360678129744826
Epoch 3279/10000, Prediction Accuracy = 56.33%, Loss = 1.0013582468032838
Epoch: 3279, Batch Gradient Norm: 37.355373776715695
Epoch: 3279, Batch Gradient Norm after: 22.360675616897563
Epoch 3280/10000, Prediction Accuracy = 56.30999999999999%, Loss = 1.007896614074707
Epoch: 3280, Batch Gradient Norm: 34.82086353897366
Epoch: 3280, Batch Gradient Norm after: 22.36067894848281
Epoch 3281/10000, Prediction Accuracy = 56.33800000000001%, Loss = 1.0010719895362854
Epoch: 3281, Batch Gradient Norm: 37.34413336058793
Epoch: 3281, Batch Gradient Norm after: 22.36067751873462
Epoch 3282/10000, Prediction Accuracy = 56.306000000000004%, Loss = 1.0076000690460205
Epoch: 3282, Batch Gradient Norm: 34.812849846308595
Epoch: 3282, Batch Gradient Norm after: 22.36067782376091
Epoch 3283/10000, Prediction Accuracy = 56.342000000000006%, Loss = 1.0007818460464477
Epoch: 3283, Batch Gradient Norm: 37.327642911289274
Epoch: 3283, Batch Gradient Norm after: 22.36067887109665
Epoch 3284/10000, Prediction Accuracy = 56.312%, Loss = 1.0072884917259217
Epoch: 3284, Batch Gradient Norm: 34.80396477722555
Epoch: 3284, Batch Gradient Norm after: 22.360677525444025
Epoch 3285/10000, Prediction Accuracy = 56.352%, Loss = 1.000494623184204
Epoch: 3285, Batch Gradient Norm: 37.315405446630294
Epoch: 3285, Batch Gradient Norm after: 22.36067765913234
Epoch 3286/10000, Prediction Accuracy = 56.318%, Loss = 1.0069846749305724
Epoch: 3286, Batch Gradient Norm: 34.794841291461516
Epoch: 3286, Batch Gradient Norm after: 22.36067672929195
Epoch 3287/10000, Prediction Accuracy = 56.354000000000006%, Loss = 1.0002034068107606
Epoch: 3287, Batch Gradient Norm: 37.300334626488045
Epoch: 3287, Batch Gradient Norm after: 22.360680247613324
Epoch 3288/10000, Prediction Accuracy = 56.315999999999995%, Loss = 1.0066929578781127
Epoch: 3288, Batch Gradient Norm: 34.787432257833146
Epoch: 3288, Batch Gradient Norm after: 22.3606776849876
Epoch 3289/10000, Prediction Accuracy = 56.355999999999995%, Loss = 0.999915611743927
Epoch: 3289, Batch Gradient Norm: 37.28163060168412
Epoch: 3289, Batch Gradient Norm after: 22.360676525195757
Epoch 3290/10000, Prediction Accuracy = 56.32000000000001%, Loss = 1.0063864231109618
Epoch: 3290, Batch Gradient Norm: 34.77761720079654
Epoch: 3290, Batch Gradient Norm after: 22.360674696194536
Epoch 3291/10000, Prediction Accuracy = 56.358000000000004%, Loss = 0.9996291279792786
Epoch: 3291, Batch Gradient Norm: 37.26580802384993
Epoch: 3291, Batch Gradient Norm after: 22.36067835503529
Epoch 3292/10000, Prediction Accuracy = 56.31999999999999%, Loss = 1.0060699939727784
Epoch: 3292, Batch Gradient Norm: 34.76928167892122
Epoch: 3292, Batch Gradient Norm after: 22.36067874777089
Epoch 3293/10000, Prediction Accuracy = 56.372%, Loss = 0.9993447065353394
Epoch: 3293, Batch Gradient Norm: 37.24649538758338
Epoch: 3293, Batch Gradient Norm after: 22.360678641590507
Epoch 3294/10000, Prediction Accuracy = 56.342%, Loss = 1.0057475328445435
Epoch: 3294, Batch Gradient Norm: 34.76394323598906
Epoch: 3294, Batch Gradient Norm after: 22.360676050358244
Epoch 3295/10000, Prediction Accuracy = 56.379999999999995%, Loss = 0.9990550994873046
Epoch: 3295, Batch Gradient Norm: 37.22286400805108
Epoch: 3295, Batch Gradient Norm after: 22.360677270397495
Epoch 3296/10000, Prediction Accuracy = 56.352%, Loss = 1.0054108023643493
Epoch: 3296, Batch Gradient Norm: 34.75752845748651
Epoch: 3296, Batch Gradient Norm after: 22.360677937657286
Epoch 3297/10000, Prediction Accuracy = 56.398%, Loss = 0.9987717032432556
Epoch: 3297, Batch Gradient Norm: 37.19081230951521
Epoch: 3297, Batch Gradient Norm after: 22.360676842920704
Epoch 3298/10000, Prediction Accuracy = 56.358000000000004%, Loss = 1.0050610661506654
Epoch: 3298, Batch Gradient Norm: 34.752047629903714
Epoch: 3298, Batch Gradient Norm after: 22.360677672315607
Epoch 3299/10000, Prediction Accuracy = 56.408%, Loss = 0.9984912037849426
Epoch: 3299, Batch Gradient Norm: 37.16178612009001
Epoch: 3299, Batch Gradient Norm after: 22.360680494245088
Epoch 3300/10000, Prediction Accuracy = 56.366%, Loss = 1.0047180294990539
Epoch: 3300, Batch Gradient Norm: 34.7469834749623
Epoch: 3300, Batch Gradient Norm after: 22.360678886735755
Epoch 3301/10000, Prediction Accuracy = 56.426%, Loss = 0.9982083320617676
Epoch: 3301, Batch Gradient Norm: 37.13458784881512
Epoch: 3301, Batch Gradient Norm after: 22.36067705915049
Epoch 3302/10000, Prediction Accuracy = 56.364%, Loss = 1.0043795466423036
Epoch: 3302, Batch Gradient Norm: 34.738175109276085
Epoch: 3302, Batch Gradient Norm after: 22.360680520064793
Epoch 3303/10000, Prediction Accuracy = 56.431999999999995%, Loss = 0.9979269742965698
Epoch: 3303, Batch Gradient Norm: 37.10629742352782
Epoch: 3303, Batch Gradient Norm after: 22.360680497203862
Epoch 3304/10000, Prediction Accuracy = 56.379999999999995%, Loss = 1.0040409445762635
Epoch: 3304, Batch Gradient Norm: 34.73221248044101
Epoch: 3304, Batch Gradient Norm after: 22.360679454824428
Epoch 3305/10000, Prediction Accuracy = 56.44200000000001%, Loss = 0.9976503133773804
Epoch: 3305, Batch Gradient Norm: 37.073633346429624
Epoch: 3305, Batch Gradient Norm after: 22.36067517435961
Epoch 3306/10000, Prediction Accuracy = 56.395999999999994%, Loss = 1.003687310218811
Epoch: 3306, Batch Gradient Norm: 34.72563399154789
Epoch: 3306, Batch Gradient Norm after: 22.360678382229587
Epoch 3307/10000, Prediction Accuracy = 56.45%, Loss = 0.9973716735839844
Epoch: 3307, Batch Gradient Norm: 37.04027743939585
Epoch: 3307, Batch Gradient Norm after: 22.360674999416847
Epoch 3308/10000, Prediction Accuracy = 56.410000000000004%, Loss = 1.0033370733261109
Epoch: 3308, Batch Gradient Norm: 34.71795319717345
Epoch: 3308, Batch Gradient Norm after: 22.360677983518997
Epoch 3309/10000, Prediction Accuracy = 56.462%, Loss = 0.9970927238464355
Epoch: 3309, Batch Gradient Norm: 37.01208698652112
Epoch: 3309, Batch Gradient Norm after: 22.36067587557162
Epoch 3310/10000, Prediction Accuracy = 56.41799999999999%, Loss = 1.0029949784278869
Epoch: 3310, Batch Gradient Norm: 34.71165134893407
Epoch: 3310, Batch Gradient Norm after: 22.360678781392526
Epoch 3311/10000, Prediction Accuracy = 56.46%, Loss = 0.9968172669410705
Epoch: 3311, Batch Gradient Norm: 36.9818926089877
Epoch: 3311, Batch Gradient Norm after: 22.36067801711525
Epoch 3312/10000, Prediction Accuracy = 56.42%, Loss = 1.0026509165763855
Epoch: 3312, Batch Gradient Norm: 34.707693834147015
Epoch: 3312, Batch Gradient Norm after: 22.360678062717405
Epoch 3313/10000, Prediction Accuracy = 56.46600000000001%, Loss = 0.9965403199195861
Epoch: 3313, Batch Gradient Norm: 36.952514038389324
Epoch: 3313, Batch Gradient Norm after: 22.360677870631754
Epoch 3314/10000, Prediction Accuracy = 56.418000000000006%, Loss = 1.002317225933075
Epoch: 3314, Batch Gradient Norm: 34.699716520329225
Epoch: 3314, Batch Gradient Norm after: 22.360675481573058
Epoch 3315/10000, Prediction Accuracy = 56.470000000000006%, Loss = 0.9962613224983216
Epoch: 3315, Batch Gradient Norm: 36.92570540733662
Epoch: 3315, Batch Gradient Norm after: 22.36067772831293
Epoch 3316/10000, Prediction Accuracy = 56.42999999999999%, Loss = 1.0019813776016235
Epoch: 3316, Batch Gradient Norm: 34.69305910413707
Epoch: 3316, Batch Gradient Norm after: 22.36067656158051
Epoch 3317/10000, Prediction Accuracy = 56.465999999999994%, Loss = 0.9959864139556884
Epoch: 3317, Batch Gradient Norm: 36.894797683540226
Epoch: 3317, Batch Gradient Norm after: 22.36067759406841
Epoch 3318/10000, Prediction Accuracy = 56.44199999999999%, Loss = 1.001642632484436
Epoch: 3318, Batch Gradient Norm: 34.686211103956666
Epoch: 3318, Batch Gradient Norm after: 22.360675911029897
Epoch 3319/10000, Prediction Accuracy = 56.472%, Loss = 0.9957139492034912
Epoch: 3319, Batch Gradient Norm: 36.863973460386994
Epoch: 3319, Batch Gradient Norm after: 22.360679871739332
Epoch 3320/10000, Prediction Accuracy = 56.462%, Loss = 1.0013046383857727
Epoch: 3320, Batch Gradient Norm: 34.67900324024996
Epoch: 3320, Batch Gradient Norm after: 22.360675129100493
Epoch 3321/10000, Prediction Accuracy = 56.477999999999994%, Loss = 0.9954410552978515
Epoch: 3321, Batch Gradient Norm: 36.83476408398834
Epoch: 3321, Batch Gradient Norm after: 22.360678084343835
Epoch 3322/10000, Prediction Accuracy = 56.472%, Loss = 1.000962507724762
Epoch: 3322, Batch Gradient Norm: 34.67182882878649
Epoch: 3322, Batch Gradient Norm after: 22.36067777775228
Epoch 3323/10000, Prediction Accuracy = 56.484%, Loss = 0.995162284374237
Epoch: 3323, Batch Gradient Norm: 36.806735390717236
Epoch: 3323, Batch Gradient Norm after: 22.36067678792595
Epoch 3324/10000, Prediction Accuracy = 56.476%, Loss = 1.0006319403648376
Epoch: 3324, Batch Gradient Norm: 34.66518559039583
Epoch: 3324, Batch Gradient Norm after: 22.360676049542686
Epoch 3325/10000, Prediction Accuracy = 56.488%, Loss = 0.994887363910675
Epoch: 3325, Batch Gradient Norm: 36.784005880276155
Epoch: 3325, Batch Gradient Norm after: 22.360677314157375
Epoch 3326/10000, Prediction Accuracy = 56.49399999999999%, Loss = 1.0003034114837646
Epoch: 3326, Batch Gradient Norm: 34.65744420103821
Epoch: 3326, Batch Gradient Norm after: 22.360676449639122
Epoch 3327/10000, Prediction Accuracy = 56.49399999999999%, Loss = 0.9946090936660766
Epoch: 3327, Batch Gradient Norm: 36.759880889662774
Epoch: 3327, Batch Gradient Norm after: 22.360677335036698
Epoch 3328/10000, Prediction Accuracy = 56.492%, Loss = 0.9999842047691345
Epoch: 3328, Batch Gradient Norm: 34.65096082149134
Epoch: 3328, Batch Gradient Norm after: 22.36067445900321
Epoch 3329/10000, Prediction Accuracy = 56.488%, Loss = 0.994335412979126
Epoch: 3329, Batch Gradient Norm: 36.73592085787963
Epoch: 3329, Batch Gradient Norm after: 22.360676667578165
Epoch 3330/10000, Prediction Accuracy = 56.5%, Loss = 0.99966459274292
Epoch: 3330, Batch Gradient Norm: 34.64697708428757
Epoch: 3330, Batch Gradient Norm after: 22.36067667734895
Epoch 3331/10000, Prediction Accuracy = 56.48%, Loss = 0.9940592169761657
Epoch: 3331, Batch Gradient Norm: 36.71580423040765
Epoch: 3331, Batch Gradient Norm after: 22.360678718419297
Epoch 3332/10000, Prediction Accuracy = 56.510000000000005%, Loss = 0.9993428945541382
Epoch: 3332, Batch Gradient Norm: 34.63813293040092
Epoch: 3332, Batch Gradient Norm after: 22.360675697790043
Epoch 3333/10000, Prediction Accuracy = 56.489999999999995%, Loss = 0.9937785029411316
Epoch: 3333, Batch Gradient Norm: 36.69273643711078
Epoch: 3333, Batch Gradient Norm after: 22.360681900386435
Epoch 3334/10000, Prediction Accuracy = 56.52%, Loss = 0.999029004573822
Epoch: 3334, Batch Gradient Norm: 34.63019450112459
Epoch: 3334, Batch Gradient Norm after: 22.360676803066788
Epoch 3335/10000, Prediction Accuracy = 56.50600000000001%, Loss = 0.9935091733932495
Epoch: 3335, Batch Gradient Norm: 36.672361119712306
Epoch: 3335, Batch Gradient Norm after: 22.360679346919984
Epoch 3336/10000, Prediction Accuracy = 56.534000000000006%, Loss = 0.9987219333648681
Epoch: 3336, Batch Gradient Norm: 34.62274194888329
Epoch: 3336, Batch Gradient Norm after: 22.36067718657315
Epoch 3337/10000, Prediction Accuracy = 56.516%, Loss = 0.993229615688324
Epoch: 3337, Batch Gradient Norm: 36.65572138047517
Epoch: 3337, Batch Gradient Norm after: 22.360680622325336
Epoch 3338/10000, Prediction Accuracy = 56.53399999999999%, Loss = 0.9984209775924683
Epoch: 3338, Batch Gradient Norm: 34.61304498576415
Epoch: 3338, Batch Gradient Norm after: 22.360677869402107
Epoch 3339/10000, Prediction Accuracy = 56.517999999999994%, Loss = 0.9929501295089722
Epoch: 3339, Batch Gradient Norm: 36.63702170630973
Epoch: 3339, Batch Gradient Norm after: 22.360681219719698
Epoch 3340/10000, Prediction Accuracy = 56.54%, Loss = 0.9981229662895202
Epoch: 3340, Batch Gradient Norm: 34.604452192817014
Epoch: 3340, Batch Gradient Norm after: 22.360675817049636
Epoch 3341/10000, Prediction Accuracy = 56.522000000000006%, Loss = 0.9926822900772094
Epoch: 3341, Batch Gradient Norm: 36.61955774179844
Epoch: 3341, Batch Gradient Norm after: 22.360680613929237
Epoch 3342/10000, Prediction Accuracy = 56.54600000000001%, Loss = 0.9978161454200745
Epoch: 3342, Batch Gradient Norm: 34.594427865849845
Epoch: 3342, Batch Gradient Norm after: 22.36067567862825
Epoch 3343/10000, Prediction Accuracy = 56.517999999999994%, Loss = 0.9924035429954529
Epoch: 3343, Batch Gradient Norm: 36.60381019251038
Epoch: 3343, Batch Gradient Norm after: 22.360677469507962
Epoch 3344/10000, Prediction Accuracy = 56.544000000000004%, Loss = 0.9975207805633545
Epoch: 3344, Batch Gradient Norm: 34.588058274568766
Epoch: 3344, Batch Gradient Norm after: 22.36067732727761
Epoch 3345/10000, Prediction Accuracy = 56.528%, Loss = 0.992123031616211
Epoch: 3345, Batch Gradient Norm: 36.5867331069792
Epoch: 3345, Batch Gradient Norm after: 22.360678064392527
Epoch 3346/10000, Prediction Accuracy = 56.553999999999995%, Loss = 0.9972268342971802
Epoch: 3346, Batch Gradient Norm: 34.57880150580031
Epoch: 3346, Batch Gradient Norm after: 22.360678363754715
Epoch 3347/10000, Prediction Accuracy = 56.541999999999994%, Loss = 0.9918523073196411
Epoch: 3347, Batch Gradient Norm: 36.5722362946827
Epoch: 3347, Batch Gradient Norm after: 22.36067737698317
Epoch 3348/10000, Prediction Accuracy = 56.556%, Loss = 0.996944260597229
Epoch: 3348, Batch Gradient Norm: 34.566966736923206
Epoch: 3348, Batch Gradient Norm after: 22.36067910926965
Epoch 3349/10000, Prediction Accuracy = 56.541999999999994%, Loss = 0.9915743470191956
Epoch: 3349, Batch Gradient Norm: 36.55940856023837
Epoch: 3349, Batch Gradient Norm after: 22.360679074460467
Epoch 3350/10000, Prediction Accuracy = 56.55799999999999%, Loss = 0.9966555953025817
Epoch: 3350, Batch Gradient Norm: 34.557823749334034
Epoch: 3350, Batch Gradient Norm after: 22.360675367949632
Epoch 3351/10000, Prediction Accuracy = 56.54600000000001%, Loss = 0.9912970304489136
Epoch: 3351, Batch Gradient Norm: 36.54606584134624
Epoch: 3351, Batch Gradient Norm after: 22.36067937596153
Epoch 3352/10000, Prediction Accuracy = 56.556%, Loss = 0.9963664054870606
Epoch: 3352, Batch Gradient Norm: 34.54947354963048
Epoch: 3352, Batch Gradient Norm after: 22.3606771808253
Epoch 3353/10000, Prediction Accuracy = 56.552%, Loss = 0.9910170078277588
Epoch: 3353, Batch Gradient Norm: 36.53263092987044
Epoch: 3353, Batch Gradient Norm after: 22.360679752161374
Epoch 3354/10000, Prediction Accuracy = 56.568%, Loss = 0.9960767745971679
Epoch: 3354, Batch Gradient Norm: 34.538196089711995
Epoch: 3354, Batch Gradient Norm after: 22.360675901043916
Epoch 3355/10000, Prediction Accuracy = 56.56400000000001%, Loss = 0.9907418131828308
Epoch: 3355, Batch Gradient Norm: 36.52049204578914
Epoch: 3355, Batch Gradient Norm after: 22.360679638086708
Epoch 3356/10000, Prediction Accuracy = 56.576%, Loss = 0.9957890391349793
Epoch: 3356, Batch Gradient Norm: 34.53000050005499
Epoch: 3356, Batch Gradient Norm after: 22.360676278868937
Epoch 3357/10000, Prediction Accuracy = 56.553999999999995%, Loss = 0.990463125705719
Epoch: 3357, Batch Gradient Norm: 36.502835737741435
Epoch: 3357, Batch Gradient Norm after: 22.360676447281254
Epoch 3358/10000, Prediction Accuracy = 56.577999999999996%, Loss = 0.9954933524131775
Epoch: 3358, Batch Gradient Norm: 34.52084322020333
Epoch: 3358, Batch Gradient Norm after: 22.360675403543272
Epoch 3359/10000, Prediction Accuracy = 56.56%, Loss = 0.9901912093162537
Epoch: 3359, Batch Gradient Norm: 36.48581189405976
Epoch: 3359, Batch Gradient Norm after: 22.36067721950862
Epoch 3360/10000, Prediction Accuracy = 56.59000000000001%, Loss = 0.9951928973197937
Epoch: 3360, Batch Gradient Norm: 34.5142420169059
Epoch: 3360, Batch Gradient Norm after: 22.360677411125753
Epoch 3361/10000, Prediction Accuracy = 56.572%, Loss = 0.9899207592010498
Epoch: 3361, Batch Gradient Norm: 36.46479353623617
Epoch: 3361, Batch Gradient Norm after: 22.360677198733356
Epoch 3362/10000, Prediction Accuracy = 56.59400000000001%, Loss = 0.9948952794075012
Epoch: 3362, Batch Gradient Norm: 34.50719487258508
Epoch: 3362, Batch Gradient Norm after: 22.36067763640155
Epoch 3363/10000, Prediction Accuracy = 56.576%, Loss = 0.989647102355957
Epoch: 3363, Batch Gradient Norm: 36.452365373881364
Epoch: 3363, Batch Gradient Norm after: 22.360676921471462
Epoch 3364/10000, Prediction Accuracy = 56.598%, Loss = 0.9946100950241089
Epoch: 3364, Batch Gradient Norm: 34.50057989966893
Epoch: 3364, Batch Gradient Norm after: 22.360675137047014
Epoch 3365/10000, Prediction Accuracy = 56.586%, Loss = 0.9893685460090638
Epoch: 3365, Batch Gradient Norm: 36.44339467243769
Epoch: 3365, Batch Gradient Norm after: 22.360677960168132
Epoch 3366/10000, Prediction Accuracy = 56.6%, Loss = 0.9943352580070496
Epoch: 3366, Batch Gradient Norm: 34.490624468981274
Epoch: 3366, Batch Gradient Norm after: 22.36067761229985
Epoch 3367/10000, Prediction Accuracy = 56.59400000000001%, Loss = 0.9890956521034241
Epoch: 3367, Batch Gradient Norm: 36.43710138786925
Epoch: 3367, Batch Gradient Norm after: 22.360678290548307
Epoch 3368/10000, Prediction Accuracy = 56.592%, Loss = 0.994064474105835
Epoch: 3368, Batch Gradient Norm: 34.47770146540858
Epoch: 3368, Batch Gradient Norm after: 22.360677157932027
Epoch 3369/10000, Prediction Accuracy = 56.59400000000001%, Loss = 0.9888163685798645
Epoch: 3369, Batch Gradient Norm: 36.43664978101534
Epoch: 3369, Batch Gradient Norm after: 22.360678578092912
Epoch 3370/10000, Prediction Accuracy = 56.59000000000001%, Loss = 0.9938133597373963
Epoch: 3370, Batch Gradient Norm: 34.46385816495507
Epoch: 3370, Batch Gradient Norm after: 22.360676208559987
Epoch 3371/10000, Prediction Accuracy = 56.593999999999994%, Loss = 0.9885356783866882
Epoch: 3371, Batch Gradient Norm: 36.43606629359626
Epoch: 3371, Batch Gradient Norm after: 22.360676206019498
Epoch 3372/10000, Prediction Accuracy = 56.598%, Loss = 0.9935491681098938
Epoch: 3372, Batch Gradient Norm: 34.45310891492967
Epoch: 3372, Batch Gradient Norm after: 22.360677240292734
Epoch 3373/10000, Prediction Accuracy = 56.598%, Loss = 0.9882547736167908
Epoch: 3373, Batch Gradient Norm: 36.43126017489661
Epoch: 3373, Batch Gradient Norm after: 22.360677068484012
Epoch 3374/10000, Prediction Accuracy = 56.605999999999995%, Loss = 0.9932876944541931
Epoch: 3374, Batch Gradient Norm: 34.43958857803742
Epoch: 3374, Batch Gradient Norm after: 22.360677407928826
Epoch 3375/10000, Prediction Accuracy = 56.60600000000001%, Loss = 0.9879723429679871
Epoch: 3375, Batch Gradient Norm: 36.42575370845308
Epoch: 3375, Batch Gradient Norm after: 22.3606755751691
Epoch 3376/10000, Prediction Accuracy = 56.61400000000001%, Loss = 0.9930374622344971
Epoch: 3376, Batch Gradient Norm: 34.4285794843669
Epoch: 3376, Batch Gradient Norm after: 22.360677654447485
Epoch 3377/10000, Prediction Accuracy = 56.622%, Loss = 0.9876956105232239
Epoch: 3377, Batch Gradient Norm: 36.422878862392615
Epoch: 3377, Batch Gradient Norm after: 22.360678419077885
Epoch 3378/10000, Prediction Accuracy = 56.628%, Loss = 0.992776882648468
Epoch: 3378, Batch Gradient Norm: 34.42000954583008
Epoch: 3378, Batch Gradient Norm after: 22.36067727165683
Epoch 3379/10000, Prediction Accuracy = 56.63599999999999%, Loss = 0.9874160408973693
Epoch: 3379, Batch Gradient Norm: 36.414741144803024
Epoch: 3379, Batch Gradient Norm after: 22.360676340874065
Epoch 3380/10000, Prediction Accuracy = 56.65%, Loss = 0.9925069212913513
Epoch: 3380, Batch Gradient Norm: 34.40997223528454
Epoch: 3380, Batch Gradient Norm after: 22.36067712441678
Epoch 3381/10000, Prediction Accuracy = 56.646%, Loss = 0.987138819694519
Epoch: 3381, Batch Gradient Norm: 36.40761036091426
Epoch: 3381, Batch Gradient Norm after: 22.36067743305742
Epoch 3382/10000, Prediction Accuracy = 56.660000000000004%, Loss = 0.9922368884086609
Epoch: 3382, Batch Gradient Norm: 34.399529320087126
Epoch: 3382, Batch Gradient Norm after: 22.36067718389205
Epoch 3383/10000, Prediction Accuracy = 56.660000000000004%, Loss = 0.9868599891662597
Epoch: 3383, Batch Gradient Norm: 36.400702363930016
Epoch: 3383, Batch Gradient Norm after: 22.3606779848191
Epoch 3384/10000, Prediction Accuracy = 56.66600000000001%, Loss = 0.9919757723808289
Epoch: 3384, Batch Gradient Norm: 34.38655911695243
Epoch: 3384, Batch Gradient Norm after: 22.360677821743817
Epoch 3385/10000, Prediction Accuracy = 56.66400000000001%, Loss = 0.9865848660469055
Epoch: 3385, Batch Gradient Norm: 36.39337652981705
Epoch: 3385, Batch Gradient Norm after: 22.36067836458651
Epoch 3386/10000, Prediction Accuracy = 56.662%, Loss = 0.9917107462882996
Epoch: 3386, Batch Gradient Norm: 34.37379094188273
Epoch: 3386, Batch Gradient Norm after: 22.360677363151797
Epoch 3387/10000, Prediction Accuracy = 56.672000000000004%, Loss = 0.9863087296485901
Epoch: 3387, Batch Gradient Norm: 36.380316686401464
Epoch: 3387, Batch Gradient Norm after: 22.360677415893743
Epoch 3388/10000, Prediction Accuracy = 56.67%, Loss = 0.9914323091506958
Epoch: 3388, Batch Gradient Norm: 34.363333483883935
Epoch: 3388, Batch Gradient Norm after: 22.360676981687444
Epoch 3389/10000, Prediction Accuracy = 56.672000000000004%, Loss = 0.9860370278358459
Epoch: 3389, Batch Gradient Norm: 36.36448602023272
Epoch: 3389, Batch Gradient Norm after: 22.360678710091655
Epoch 3390/10000, Prediction Accuracy = 56.666%, Loss = 0.9911442279815674
Epoch: 3390, Batch Gradient Norm: 34.355172977333474
Epoch: 3390, Batch Gradient Norm after: 22.360677438377213
Epoch 3391/10000, Prediction Accuracy = 56.676%, Loss = 0.9857635498046875
Epoch: 3391, Batch Gradient Norm: 36.35059883901996
Epoch: 3391, Batch Gradient Norm after: 22.36067722578861
Epoch 3392/10000, Prediction Accuracy = 56.674%, Loss = 0.9908581852912903
Epoch: 3392, Batch Gradient Norm: 34.34707381786468
Epoch: 3392, Batch Gradient Norm after: 22.360679447563463
Epoch 3393/10000, Prediction Accuracy = 56.686%, Loss = 0.9854898571968078
Epoch: 3393, Batch Gradient Norm: 36.339893167716525
Epoch: 3393, Batch Gradient Norm after: 22.36067575578856
Epoch 3394/10000, Prediction Accuracy = 56.67999999999999%, Loss = 0.9905734062194824
Epoch: 3394, Batch Gradient Norm: 34.33698386154582
Epoch: 3394, Batch Gradient Norm after: 22.360678727584443
Epoch 3395/10000, Prediction Accuracy = 56.686%, Loss = 0.9852191567420959
Epoch: 3395, Batch Gradient Norm: 36.330973642946006
Epoch: 3395, Batch Gradient Norm after: 22.360677530681638
Epoch 3396/10000, Prediction Accuracy = 56.68399999999999%, Loss = 0.9902981996536255
Epoch: 3396, Batch Gradient Norm: 34.32858392488858
Epoch: 3396, Batch Gradient Norm after: 22.360677338263912
Epoch 3397/10000, Prediction Accuracy = 56.698%, Loss = 0.984941554069519
Epoch: 3397, Batch Gradient Norm: 36.320198681684694
Epoch: 3397, Batch Gradient Norm after: 22.36067578166378
Epoch 3398/10000, Prediction Accuracy = 56.694%, Loss = 0.990026330947876
Epoch: 3398, Batch Gradient Norm: 34.319694774363256
Epoch: 3398, Batch Gradient Norm after: 22.360677084071835
Epoch 3399/10000, Prediction Accuracy = 56.70799999999999%, Loss = 0.9846688508987427
Epoch: 3399, Batch Gradient Norm: 36.307946276303085
Epoch: 3399, Batch Gradient Norm after: 22.36067710142754
Epoch 3400/10000, Prediction Accuracy = 56.698%, Loss = 0.9897464752197266
Epoch: 3400, Batch Gradient Norm: 34.310655007866785
Epoch: 3400, Batch Gradient Norm after: 22.360677804232775
Epoch 3401/10000, Prediction Accuracy = 56.7%, Loss = 0.9843958854675293
Epoch: 3401, Batch Gradient Norm: 36.297132018988606
Epoch: 3401, Batch Gradient Norm after: 22.360677339954723
Epoch 3402/10000, Prediction Accuracy = 56.709999999999994%, Loss = 0.9894716024398804
Epoch: 3402, Batch Gradient Norm: 34.30149545232391
Epoch: 3402, Batch Gradient Norm after: 22.360677004113178
Epoch 3403/10000, Prediction Accuracy = 56.7%, Loss = 0.9841216444969177
Epoch: 3403, Batch Gradient Norm: 36.287125240044
Epoch: 3403, Batch Gradient Norm after: 22.360675582411602
Epoch 3404/10000, Prediction Accuracy = 56.705999999999996%, Loss = 0.9891920804977417
Epoch: 3404, Batch Gradient Norm: 34.294859912243446
Epoch: 3404, Batch Gradient Norm after: 22.36067809750166
Epoch 3405/10000, Prediction Accuracy = 56.702%, Loss = 0.9838486552238465
Epoch: 3405, Batch Gradient Norm: 36.273208873661964
Epoch: 3405, Batch Gradient Norm after: 22.36067751027299
Epoch 3406/10000, Prediction Accuracy = 56.70400000000001%, Loss = 0.9889181733131409
Epoch: 3406, Batch Gradient Norm: 34.28544399147514
Epoch: 3406, Batch Gradient Norm after: 22.36067838441676
Epoch 3407/10000, Prediction Accuracy = 56.702%, Loss = 0.9835777878761292
Epoch: 3407, Batch Gradient Norm: 36.26163658812671
Epoch: 3407, Batch Gradient Norm after: 22.360679175677895
Epoch 3408/10000, Prediction Accuracy = 56.714%, Loss = 0.988635241985321
Epoch: 3408, Batch Gradient Norm: 34.276300390701124
Epoch: 3408, Batch Gradient Norm after: 22.36067768219744
Epoch 3409/10000, Prediction Accuracy = 56.714%, Loss = 0.983310878276825
Epoch: 3409, Batch Gradient Norm: 36.251214133797575
Epoch: 3409, Batch Gradient Norm after: 22.360677509114957
Epoch 3410/10000, Prediction Accuracy = 56.71999999999999%, Loss = 0.9883589148521423
Epoch: 3410, Batch Gradient Norm: 34.26871383603349
Epoch: 3410, Batch Gradient Norm after: 22.360677433080905
Epoch 3411/10000, Prediction Accuracy = 56.726%, Loss = 0.9830370903015136
Epoch: 3411, Batch Gradient Norm: 36.23884167964423
Epoch: 3411, Batch Gradient Norm after: 22.360676136134146
Epoch 3412/10000, Prediction Accuracy = 56.726%, Loss = 0.9880764245986938
Epoch: 3412, Batch Gradient Norm: 34.26161462559872
Epoch: 3412, Batch Gradient Norm after: 22.36067692151514
Epoch 3413/10000, Prediction Accuracy = 56.73199999999999%, Loss = 0.9827664613723754
Epoch: 3413, Batch Gradient Norm: 36.223458930071985
Epoch: 3413, Batch Gradient Norm after: 22.36067750334433
Epoch 3414/10000, Prediction Accuracy = 56.739999999999995%, Loss = 0.9877876758575439
Epoch: 3414, Batch Gradient Norm: 34.255543498007626
Epoch: 3414, Batch Gradient Norm after: 22.36067766632634
Epoch 3415/10000, Prediction Accuracy = 56.739999999999995%, Loss = 0.9824990391731262
Epoch: 3415, Batch Gradient Norm: 36.20793027050506
Epoch: 3415, Batch Gradient Norm after: 22.360679081994434
Epoch 3416/10000, Prediction Accuracy = 56.746%, Loss = 0.9874966144561768
Epoch: 3416, Batch Gradient Norm: 34.24762285784169
Epoch: 3416, Batch Gradient Norm after: 22.3606751876864
Epoch 3417/10000, Prediction Accuracy = 56.74399999999999%, Loss = 0.9822304725646973
Epoch: 3417, Batch Gradient Norm: 36.19216319637825
Epoch: 3417, Batch Gradient Norm after: 22.36067722771577
Epoch 3418/10000, Prediction Accuracy = 56.75%, Loss = 0.9872079014778137
Epoch: 3418, Batch Gradient Norm: 34.238608264196735
Epoch: 3418, Batch Gradient Norm after: 22.360672418329703
Epoch 3419/10000, Prediction Accuracy = 56.738%, Loss = 0.9819661617279053
Epoch: 3419, Batch Gradient Norm: 36.17676664797381
Epoch: 3419, Batch Gradient Norm after: 22.360676403829313
Epoch 3420/10000, Prediction Accuracy = 56.75599999999999%, Loss = 0.9869186401367187
Epoch: 3420, Batch Gradient Norm: 34.23220401416285
Epoch: 3420, Batch Gradient Norm after: 22.360674055350582
Epoch 3421/10000, Prediction Accuracy = 56.746%, Loss = 0.9817025423049927
Epoch: 3421, Batch Gradient Norm: 36.15877803913471
Epoch: 3421, Batch Gradient Norm after: 22.360675219142237
Epoch 3422/10000, Prediction Accuracy = 56.760000000000005%, Loss = 0.9866188168525696
Epoch: 3422, Batch Gradient Norm: 34.225178198521405
Epoch: 3422, Batch Gradient Norm after: 22.360676613464932
Epoch 3423/10000, Prediction Accuracy = 56.758%, Loss = 0.9814366221427917
Epoch: 3423, Batch Gradient Norm: 36.142114169150844
Epoch: 3423, Batch Gradient Norm after: 22.36067775569849
Epoch 3424/10000, Prediction Accuracy = 56.763999999999996%, Loss = 0.9863349556922912
Epoch: 3424, Batch Gradient Norm: 34.216267362410576
Epoch: 3424, Batch Gradient Norm after: 22.360676902834182
Epoch 3425/10000, Prediction Accuracy = 56.774%, Loss = 0.9811690807342529
Epoch: 3425, Batch Gradient Norm: 36.12264429788327
Epoch: 3425, Batch Gradient Norm after: 22.36067783573518
Epoch 3426/10000, Prediction Accuracy = 56.779999999999994%, Loss = 0.9860382318496704
Epoch: 3426, Batch Gradient Norm: 34.21035453563543
Epoch: 3426, Batch Gradient Norm after: 22.36067580273107
Epoch 3427/10000, Prediction Accuracy = 56.767999999999994%, Loss = 0.9809075355529785
Epoch: 3427, Batch Gradient Norm: 36.10304426684642
Epoch: 3427, Batch Gradient Norm after: 22.360678072453904
Epoch 3428/10000, Prediction Accuracy = 56.784000000000006%, Loss = 0.985745120048523
Epoch: 3428, Batch Gradient Norm: 34.203628772401025
Epoch: 3428, Batch Gradient Norm after: 22.360674926391308
Epoch 3429/10000, Prediction Accuracy = 56.778%, Loss = 0.9806410312652588
Epoch: 3429, Batch Gradient Norm: 36.082689293519756
Epoch: 3429, Batch Gradient Norm after: 22.36067717164929
Epoch 3430/10000, Prediction Accuracy = 56.794%, Loss = 0.985450804233551
Epoch: 3430, Batch Gradient Norm: 34.198987870791484
Epoch: 3430, Batch Gradient Norm after: 22.360677648697326
Epoch 3431/10000, Prediction Accuracy = 56.786%, Loss = 0.9803754925727844
Epoch: 3431, Batch Gradient Norm: 36.06358175490188
Epoch: 3431, Batch Gradient Norm after: 22.36067739632759
Epoch 3432/10000, Prediction Accuracy = 56.80400000000001%, Loss = 0.985149335861206
Epoch: 3432, Batch Gradient Norm: 34.19421125611347
Epoch: 3432, Batch Gradient Norm after: 22.36067576155364
Epoch 3433/10000, Prediction Accuracy = 56.8%, Loss = 0.9801154851913452
Epoch: 3433, Batch Gradient Norm: 36.046919456604115
Epoch: 3433, Batch Gradient Norm after: 22.360676098125012
Epoch 3434/10000, Prediction Accuracy = 56.80800000000001%, Loss = 0.9848544359207153
Epoch: 3434, Batch Gradient Norm: 34.186942917157175
Epoch: 3434, Batch Gradient Norm after: 22.360677280423676
Epoch 3435/10000, Prediction Accuracy = 56.79600000000001%, Loss = 0.9798535227775573
Epoch: 3435, Batch Gradient Norm: 36.02982526161641
Epoch: 3435, Batch Gradient Norm after: 22.360677629001188
Epoch 3436/10000, Prediction Accuracy = 56.824%, Loss = 0.984554934501648
Epoch: 3436, Batch Gradient Norm: 34.17985915280411
Epoch: 3436, Batch Gradient Norm after: 22.360677184839236
Epoch 3437/10000, Prediction Accuracy = 56.79%, Loss = 0.9795897603034973
Epoch: 3437, Batch Gradient Norm: 36.010672559905785
Epoch: 3437, Batch Gradient Norm after: 22.360679035807895
Epoch 3438/10000, Prediction Accuracy = 56.83399999999999%, Loss = 0.9842614769935608
Epoch: 3438, Batch Gradient Norm: 34.17172338478183
Epoch: 3438, Batch Gradient Norm after: 22.36067625355371
Epoch 3439/10000, Prediction Accuracy = 56.79600000000001%, Loss = 0.9793296575546264
Epoch: 3439, Batch Gradient Norm: 35.995579646233395
Epoch: 3439, Batch Gradient Norm after: 22.360677339167715
Epoch 3440/10000, Prediction Accuracy = 56.842000000000006%, Loss = 0.9839755654335022
Epoch: 3440, Batch Gradient Norm: 34.16476071513867
Epoch: 3440, Batch Gradient Norm after: 22.360676386907873
Epoch 3441/10000, Prediction Accuracy = 56.812%, Loss = 0.979064691066742
Epoch: 3441, Batch Gradient Norm: 35.980063420877855
Epoch: 3441, Batch Gradient Norm after: 22.360678200544392
Epoch 3442/10000, Prediction Accuracy = 56.852%, Loss = 0.9836920499801636
Epoch: 3442, Batch Gradient Norm: 34.158202612209045
Epoch: 3442, Batch Gradient Norm after: 22.360681088694363
Epoch 3443/10000, Prediction Accuracy = 56.815999999999995%, Loss = 0.9788054943084716
Epoch: 3443, Batch Gradient Norm: 35.96195113093759
Epoch: 3443, Batch Gradient Norm after: 22.360678626501457
Epoch 3444/10000, Prediction Accuracy = 56.858000000000004%, Loss = 0.9834021210670472
Epoch: 3444, Batch Gradient Norm: 34.14902967180478
Epoch: 3444, Batch Gradient Norm after: 22.36067766325945
Epoch 3445/10000, Prediction Accuracy = 56.822%, Loss = 0.9785439729690552
Epoch: 3445, Batch Gradient Norm: 35.94622309099298
Epoch: 3445, Batch Gradient Norm after: 22.36067735088135
Epoch 3446/10000, Prediction Accuracy = 56.866%, Loss = 0.983111834526062
Epoch: 3446, Batch Gradient Norm: 34.14210565963972
Epoch: 3446, Batch Gradient Norm after: 22.360677773424293
Epoch 3447/10000, Prediction Accuracy = 56.818000000000005%, Loss = 0.9782838940620422
Epoch: 3447, Batch Gradient Norm: 35.93056512267608
Epoch: 3447, Batch Gradient Norm after: 22.360677484070063
Epoch 3448/10000, Prediction Accuracy = 56.874%, Loss = 0.9828325867652893
Epoch: 3448, Batch Gradient Norm: 34.132764190193036
Epoch: 3448, Batch Gradient Norm after: 22.36067776904023
Epoch 3449/10000, Prediction Accuracy = 56.822%, Loss = 0.9780214071273804
Epoch: 3449, Batch Gradient Norm: 35.91826889001439
Epoch: 3449, Batch Gradient Norm after: 22.360678110745884
Epoch 3450/10000, Prediction Accuracy = 56.888%, Loss = 0.9825597286224366
Epoch: 3450, Batch Gradient Norm: 34.120880236110914
Epoch: 3450, Batch Gradient Norm after: 22.360677582861655
Epoch 3451/10000, Prediction Accuracy = 56.83%, Loss = 0.9777555704116822
Epoch: 3451, Batch Gradient Norm: 35.911809531781145
Epoch: 3451, Batch Gradient Norm after: 22.360677594872097
Epoch 3452/10000, Prediction Accuracy = 56.894000000000005%, Loss = 0.9822984337806702
Epoch: 3452, Batch Gradient Norm: 34.10949776753296
Epoch: 3452, Batch Gradient Norm after: 22.360677661638235
Epoch 3453/10000, Prediction Accuracy = 56.838%, Loss = 0.9774873495101929
Epoch: 3453, Batch Gradient Norm: 35.90565406280257
Epoch: 3453, Batch Gradient Norm after: 22.36067853127398
Epoch 3454/10000, Prediction Accuracy = 56.89000000000001%, Loss = 0.9820449829101563
Epoch: 3454, Batch Gradient Norm: 34.09760206450921
Epoch: 3454, Batch Gradient Norm after: 22.36067814432314
Epoch 3455/10000, Prediction Accuracy = 56.848%, Loss = 0.97721928358078
Epoch: 3455, Batch Gradient Norm: 35.904967842459016
Epoch: 3455, Batch Gradient Norm after: 22.36067762482395
Epoch 3456/10000, Prediction Accuracy = 56.891999999999996%, Loss = 0.9818009853363037
Epoch: 3456, Batch Gradient Norm: 34.08505282826441
Epoch: 3456, Batch Gradient Norm after: 22.360676770715813
Epoch 3457/10000, Prediction Accuracy = 56.855999999999995%, Loss = 0.9769510626792908
Epoch: 3457, Batch Gradient Norm: 35.899333071713734
Epoch: 3457, Batch Gradient Norm after: 22.360677638615993
Epoch 3458/10000, Prediction Accuracy = 56.9%, Loss = 0.9815491318702698
Epoch: 3458, Batch Gradient Norm: 34.07580884629456
Epoch: 3458, Batch Gradient Norm after: 22.36067690638518
Epoch 3459/10000, Prediction Accuracy = 56.86600000000001%, Loss = 0.9766837954521179
Epoch: 3459, Batch Gradient Norm: 35.88961293353629
Epoch: 3459, Batch Gradient Norm after: 22.360677693247755
Epoch 3460/10000, Prediction Accuracy = 56.902%, Loss = 0.9812884211540223
Epoch: 3460, Batch Gradient Norm: 34.0648368620132
Epoch: 3460, Batch Gradient Norm after: 22.360678017228075
Epoch 3461/10000, Prediction Accuracy = 56.86800000000001%, Loss = 0.9764201521873475
Epoch: 3461, Batch Gradient Norm: 35.883162507360375
Epoch: 3461, Batch Gradient Norm after: 22.360679031603535
Epoch 3462/10000, Prediction Accuracy = 56.924%, Loss = 0.9810314655303956
Epoch: 3462, Batch Gradient Norm: 34.054472649829506
Epoch: 3462, Batch Gradient Norm after: 22.360675676864016
Epoch 3463/10000, Prediction Accuracy = 56.879999999999995%, Loss = 0.9761549711227417
Epoch: 3463, Batch Gradient Norm: 35.8764082505819
Epoch: 3463, Batch Gradient Norm after: 22.360678060756022
Epoch 3464/10000, Prediction Accuracy = 56.912%, Loss = 0.9807717084884644
Epoch: 3464, Batch Gradient Norm: 34.04544820919175
Epoch: 3464, Batch Gradient Norm after: 22.360675726295035
Epoch 3465/10000, Prediction Accuracy = 56.89%, Loss = 0.9758872985839844
Epoch: 3465, Batch Gradient Norm: 35.868837014265374
Epoch: 3465, Batch Gradient Norm after: 22.360675384851696
Epoch 3466/10000, Prediction Accuracy = 56.938%, Loss = 0.9805111527442932
Epoch: 3466, Batch Gradient Norm: 34.03351088640551
Epoch: 3466, Batch Gradient Norm after: 22.36067686061092
Epoch 3467/10000, Prediction Accuracy = 56.902%, Loss = 0.9756213545799255
Epoch: 3467, Batch Gradient Norm: 35.86210673856221
Epoch: 3467, Batch Gradient Norm after: 22.360676186241637
Epoch 3468/10000, Prediction Accuracy = 56.946000000000005%, Loss = 0.9802593111991882
Epoch: 3468, Batch Gradient Norm: 34.024144804983784
Epoch: 3468, Batch Gradient Norm after: 22.360676628680295
Epoch 3469/10000, Prediction Accuracy = 56.907999999999994%, Loss = 0.975354015827179
Epoch: 3469, Batch Gradient Norm: 35.856580628320835
Epoch: 3469, Batch Gradient Norm after: 22.360677105230042
Epoch 3470/10000, Prediction Accuracy = 56.970000000000006%, Loss = 0.9800037384033203
Epoch: 3470, Batch Gradient Norm: 34.01285384800316
Epoch: 3470, Batch Gradient Norm after: 22.36067816503544
Epoch 3471/10000, Prediction Accuracy = 56.898%, Loss = 0.9750886201858521
Epoch: 3471, Batch Gradient Norm: 35.8505454600701
Epoch: 3471, Batch Gradient Norm after: 22.360677038254575
Epoch 3472/10000, Prediction Accuracy = 56.962%, Loss = 0.9797544717788697
Epoch: 3472, Batch Gradient Norm: 34.00537770688161
Epoch: 3472, Batch Gradient Norm after: 22.3606783577969
Epoch 3473/10000, Prediction Accuracy = 56.912%, Loss = 0.9748217940330506
Epoch: 3473, Batch Gradient Norm: 35.83873036144476
Epoch: 3473, Batch Gradient Norm after: 22.360677638596144
Epoch 3474/10000, Prediction Accuracy = 56.964%, Loss = 0.9794845342636108
Epoch: 3474, Batch Gradient Norm: 33.99599948872221
Epoch: 3474, Batch Gradient Norm after: 22.360675870684666
Epoch 3475/10000, Prediction Accuracy = 56.932%, Loss = 0.9745597958564758
Epoch: 3475, Batch Gradient Norm: 35.828484120722074
Epoch: 3475, Batch Gradient Norm after: 22.36067710466991
Epoch 3476/10000, Prediction Accuracy = 56.965999999999994%, Loss = 0.9792224764823914
Epoch: 3476, Batch Gradient Norm: 33.98522976393915
Epoch: 3476, Batch Gradient Norm after: 22.36067763837546
Epoch 3477/10000, Prediction Accuracy = 56.94200000000001%, Loss = 0.9743029594421386
Epoch: 3477, Batch Gradient Norm: 35.81970551662497
Epoch: 3477, Batch Gradient Norm after: 22.360677940878038
Epoch 3478/10000, Prediction Accuracy = 56.970000000000006%, Loss = 0.9789604306221008
Epoch: 3478, Batch Gradient Norm: 33.976422962389904
Epoch: 3478, Batch Gradient Norm after: 22.360677541944565
Epoch 3479/10000, Prediction Accuracy = 56.946000000000005%, Loss = 0.9740444540977478
Epoch: 3479, Batch Gradient Norm: 35.8082352938796
Epoch: 3479, Batch Gradient Norm after: 22.360678065967342
Epoch 3480/10000, Prediction Accuracy = 56.971999999999994%, Loss = 0.9787019848823547
Epoch: 3480, Batch Gradient Norm: 33.96781490596981
Epoch: 3480, Batch Gradient Norm after: 22.360677003351185
Epoch 3481/10000, Prediction Accuracy = 56.956%, Loss = 0.9737870931625366
Epoch: 3481, Batch Gradient Norm: 35.801198628628946
Epoch: 3481, Batch Gradient Norm after: 22.36067650553951
Epoch 3482/10000, Prediction Accuracy = 56.974000000000004%, Loss = 0.9784374713897706
Epoch: 3482, Batch Gradient Norm: 33.95768731554187
Epoch: 3482, Batch Gradient Norm after: 22.360677036355003
Epoch 3483/10000, Prediction Accuracy = 56.959999999999994%, Loss = 0.9735265970230103
Epoch: 3483, Batch Gradient Norm: 35.78888482575343
Epoch: 3483, Batch Gradient Norm after: 22.360677970211338
Epoch 3484/10000, Prediction Accuracy = 56.983999999999995%, Loss = 0.9781725525856018
Epoch: 3484, Batch Gradient Norm: 33.94853128874787
Epoch: 3484, Batch Gradient Norm after: 22.36067932714368
Epoch 3485/10000, Prediction Accuracy = 56.968%, Loss = 0.9732637405395508
Epoch: 3485, Batch Gradient Norm: 35.77939543377863
Epoch: 3485, Batch Gradient Norm after: 22.360677960648736
Epoch 3486/10000, Prediction Accuracy = 56.98599999999999%, Loss = 0.9779187798500061
Epoch: 3486, Batch Gradient Norm: 33.935832889706965
Epoch: 3486, Batch Gradient Norm after: 22.360678692032437
Epoch 3487/10000, Prediction Accuracy = 56.976%, Loss = 0.9730018973350525
Epoch: 3487, Batch Gradient Norm: 35.7738353666469
Epoch: 3487, Batch Gradient Norm after: 22.360679244762373
Epoch 3488/10000, Prediction Accuracy = 56.98199999999999%, Loss = 0.9776688694953919
Epoch: 3488, Batch Gradient Norm: 33.9251938935634
Epoch: 3488, Batch Gradient Norm after: 22.360677092623305
Epoch 3489/10000, Prediction Accuracy = 56.972%, Loss = 0.9727359533309936
Epoch: 3489, Batch Gradient Norm: 35.76950419236223
Epoch: 3489, Batch Gradient Norm after: 22.36067701873618
Epoch 3490/10000, Prediction Accuracy = 56.982000000000006%, Loss = 0.9774223804473877
Epoch: 3490, Batch Gradient Norm: 33.9169422781309
Epoch: 3490, Batch Gradient Norm after: 22.360677549502068
Epoch 3491/10000, Prediction Accuracy = 56.974000000000004%, Loss = 0.9724743127822876
Epoch: 3491, Batch Gradient Norm: 35.76373420928132
Epoch: 3491, Batch Gradient Norm after: 22.360677940025848
Epoch 3492/10000, Prediction Accuracy = 56.989999999999995%, Loss = 0.9771705746650696
Epoch: 3492, Batch Gradient Norm: 33.90663972773959
Epoch: 3492, Batch Gradient Norm after: 22.36067642843006
Epoch 3493/10000, Prediction Accuracy = 56.98%, Loss = 0.9722076654434204
Epoch: 3493, Batch Gradient Norm: 35.76158132695932
Epoch: 3493, Batch Gradient Norm after: 22.36067716427055
Epoch 3494/10000, Prediction Accuracy = 56.998000000000005%, Loss = 0.9769243240356446
Epoch: 3494, Batch Gradient Norm: 33.89663997289487
Epoch: 3494, Batch Gradient Norm after: 22.360676201979334
Epoch 3495/10000, Prediction Accuracy = 56.99000000000001%, Loss = 0.971941065788269
Epoch: 3495, Batch Gradient Norm: 35.75815529106913
Epoch: 3495, Batch Gradient Norm after: 22.360678116405225
Epoch 3496/10000, Prediction Accuracy = 57.012%, Loss = 0.9766835331916809
Epoch: 3496, Batch Gradient Norm: 33.88552486893976
Epoch: 3496, Batch Gradient Norm after: 22.360678228980976
Epoch 3497/10000, Prediction Accuracy = 56.998000000000005%, Loss = 0.9716764211654663
Epoch: 3497, Batch Gradient Norm: 35.75719650713717
Epoch: 3497, Batch Gradient Norm after: 22.360677098665867
Epoch 3498/10000, Prediction Accuracy = 57.02%, Loss = 0.9764457464218139
Epoch: 3498, Batch Gradient Norm: 33.87355822052895
Epoch: 3498, Batch Gradient Norm after: 22.36067931174031
Epoch 3499/10000, Prediction Accuracy = 57.007999999999996%, Loss = 0.9714144825935364
Epoch: 3499, Batch Gradient Norm: 35.75667758760035
Epoch: 3499, Batch Gradient Norm after: 22.360679284364657
Epoch 3500/10000, Prediction Accuracy = 57.032%, Loss = 0.9762057900428772
Epoch: 3500, Batch Gradient Norm: 33.86432412900539
Epoch: 3500, Batch Gradient Norm after: 22.36067995959999
Epoch 3501/10000, Prediction Accuracy = 57.017999999999994%, Loss = 0.9711444139480591
Epoch: 3501, Batch Gradient Norm: 35.76106924509322
Epoch: 3501, Batch Gradient Norm after: 22.36067849653264
Epoch 3502/10000, Prediction Accuracy = 57.038%, Loss = 0.97597416639328
Epoch: 3502, Batch Gradient Norm: 33.852386972619534
Epoch: 3502, Batch Gradient Norm after: 22.36067873382852
Epoch 3503/10000, Prediction Accuracy = 57.016%, Loss = 0.9708778858184814
Epoch: 3503, Batch Gradient Norm: 35.75858965133146
Epoch: 3503, Batch Gradient Norm after: 22.360678142697967
Epoch 3504/10000, Prediction Accuracy = 57.041999999999994%, Loss = 0.9757361769676208
Epoch: 3504, Batch Gradient Norm: 33.83970331540656
Epoch: 3504, Batch Gradient Norm after: 22.360676251832114
Epoch 3505/10000, Prediction Accuracy = 57.025999999999996%, Loss = 0.9706164002418518
Epoch: 3505, Batch Gradient Norm: 35.75871081147916
Epoch: 3505, Batch Gradient Norm after: 22.360677053600543
Epoch 3506/10000, Prediction Accuracy = 57.05400000000001%, Loss = 0.975497591495514
Epoch: 3506, Batch Gradient Norm: 33.82477770697504
Epoch: 3506, Batch Gradient Norm after: 22.360678544474734
Epoch 3507/10000, Prediction Accuracy = 57.036%, Loss = 0.9703521132469177
Epoch: 3507, Batch Gradient Norm: 35.76028295424747
Epoch: 3507, Batch Gradient Norm after: 22.36067646395841
Epoch 3508/10000, Prediction Accuracy = 57.06%, Loss = 0.9752710700035095
Epoch: 3508, Batch Gradient Norm: 33.8111281636466
Epoch: 3508, Batch Gradient Norm after: 22.3606777069494
Epoch 3509/10000, Prediction Accuracy = 57.036%, Loss = 0.9700886845588684
Epoch: 3509, Batch Gradient Norm: 35.7546657450359
Epoch: 3509, Batch Gradient Norm after: 22.36067913702232
Epoch 3510/10000, Prediction Accuracy = 57.056000000000004%, Loss = 0.9750318646430969
Epoch: 3510, Batch Gradient Norm: 33.80125885205
Epoch: 3510, Batch Gradient Norm after: 22.360677417688972
Epoch 3511/10000, Prediction Accuracy = 57.052%, Loss = 0.9698283910751343
Epoch: 3511, Batch Gradient Norm: 35.749297577442945
Epoch: 3511, Batch Gradient Norm after: 22.360677840200477
Epoch 3512/10000, Prediction Accuracy = 57.077999999999996%, Loss = 0.974784255027771
Epoch: 3512, Batch Gradient Norm: 33.79236697468377
Epoch: 3512, Batch Gradient Norm after: 22.360677564761584
Epoch 3513/10000, Prediction Accuracy = 57.053999999999995%, Loss = 0.9695686340332031
Epoch: 3513, Batch Gradient Norm: 35.74215247819394
Epoch: 3513, Batch Gradient Norm after: 22.36067754107103
Epoch 3514/10000, Prediction Accuracy = 57.08200000000001%, Loss = 0.9745272994041443
Epoch: 3514, Batch Gradient Norm: 33.78465555402023
Epoch: 3514, Batch Gradient Norm after: 22.36067713230533
Epoch 3515/10000, Prediction Accuracy = 57.068%, Loss = 0.9693050265312195
Epoch: 3515, Batch Gradient Norm: 35.731663540907604
Epoch: 3515, Batch Gradient Norm after: 22.360677275308987
Epoch 3516/10000, Prediction Accuracy = 57.084%, Loss = 0.974265968799591
Epoch: 3516, Batch Gradient Norm: 33.7767350430881
Epoch: 3516, Batch Gradient Norm after: 22.36067735600536
Epoch 3517/10000, Prediction Accuracy = 57.065999999999995%, Loss = 0.9690451860427857
Epoch: 3517, Batch Gradient Norm: 35.72074005879123
Epoch: 3517, Batch Gradient Norm after: 22.36067622901012
Epoch 3518/10000, Prediction Accuracy = 57.088%, Loss = 0.9739978432655334
Epoch: 3518, Batch Gradient Norm: 33.7675904116014
Epoch: 3518, Batch Gradient Norm after: 22.360676966556866
Epoch 3519/10000, Prediction Accuracy = 57.076%, Loss = 0.9687929391860962
Epoch: 3519, Batch Gradient Norm: 35.71607021664189
Epoch: 3519, Batch Gradient Norm after: 22.36067540581658
Epoch 3520/10000, Prediction Accuracy = 57.08200000000001%, Loss = 0.9737475991249085
Epoch: 3520, Batch Gradient Norm: 33.759558990474574
Epoch: 3520, Batch Gradient Norm after: 22.360678910679233
Epoch 3521/10000, Prediction Accuracy = 57.081999999999994%, Loss = 0.9685282707214355
Epoch: 3521, Batch Gradient Norm: 35.7043181485248
Epoch: 3521, Batch Gradient Norm after: 22.360678078598728
Epoch 3522/10000, Prediction Accuracy = 57.086%, Loss = 0.9734820365905762
Epoch: 3522, Batch Gradient Norm: 33.749859669852924
Epoch: 3522, Batch Gradient Norm after: 22.360677929470846
Epoch 3523/10000, Prediction Accuracy = 57.084%, Loss = 0.9682748079299927
Epoch: 3523, Batch Gradient Norm: 35.69746787313542
Epoch: 3523, Batch Gradient Norm after: 22.36067719358068
Epoch 3524/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.9732351779937745
Epoch: 3524, Batch Gradient Norm: 33.737369962577574
Epoch: 3524, Batch Gradient Norm after: 22.360677950168277
Epoch 3525/10000, Prediction Accuracy = 57.086%, Loss = 0.9680176019668579
Epoch: 3525, Batch Gradient Norm: 35.69310681777069
Epoch: 3525, Batch Gradient Norm after: 22.36067823854461
Epoch 3526/10000, Prediction Accuracy = 57.092%, Loss = 0.9729909062385559
Epoch: 3526, Batch Gradient Norm: 33.72886553941779
Epoch: 3526, Batch Gradient Norm after: 22.3606777291172
Epoch 3527/10000, Prediction Accuracy = 57.092%, Loss = 0.9677577972412109
Epoch: 3527, Batch Gradient Norm: 35.686632277382
Epoch: 3527, Batch Gradient Norm after: 22.360678496749852
Epoch 3528/10000, Prediction Accuracy = 57.105999999999995%, Loss = 0.9727317214012146
Epoch: 3528, Batch Gradient Norm: 33.72157871703643
Epoch: 3528, Batch Gradient Norm after: 22.360675767932985
Epoch 3529/10000, Prediction Accuracy = 57.096000000000004%, Loss = 0.9674981594085693
Epoch: 3529, Batch Gradient Norm: 35.67944643558534
Epoch: 3529, Batch Gradient Norm after: 22.360676565343887
Epoch 3530/10000, Prediction Accuracy = 57.108000000000004%, Loss = 0.9724770069122315
Epoch: 3530, Batch Gradient Norm: 33.712912664462785
Epoch: 3530, Batch Gradient Norm after: 22.360677401858144
Epoch 3531/10000, Prediction Accuracy = 57.089999999999996%, Loss = 0.9672459602355957
Epoch: 3531, Batch Gradient Norm: 35.67530867656473
Epoch: 3531, Batch Gradient Norm after: 22.360679746679388
Epoch 3532/10000, Prediction Accuracy = 57.11400000000001%, Loss = 0.9722317934036255
Epoch: 3532, Batch Gradient Norm: 33.70238023517089
Epoch: 3532, Batch Gradient Norm after: 22.360678408992083
Epoch 3533/10000, Prediction Accuracy = 57.1%, Loss = 0.9669875860214233
Epoch: 3533, Batch Gradient Norm: 35.66796115802327
Epoch: 3533, Batch Gradient Norm after: 22.360677579072462
Epoch 3534/10000, Prediction Accuracy = 57.114%, Loss = 0.9719787120819092
Epoch: 3534, Batch Gradient Norm: 33.690680806000486
Epoch: 3534, Batch Gradient Norm after: 22.360677527828084
Epoch 3535/10000, Prediction Accuracy = 57.088%, Loss = 0.9667330622673035
Epoch: 3535, Batch Gradient Norm: 35.65752124040433
Epoch: 3535, Batch Gradient Norm after: 22.360678787924705
Epoch 3536/10000, Prediction Accuracy = 57.11600000000001%, Loss = 0.9717217922210694
Epoch: 3536, Batch Gradient Norm: 33.68336718555128
Epoch: 3536, Batch Gradient Norm after: 22.360673183939674
Epoch 3537/10000, Prediction Accuracy = 57.102%, Loss = 0.966477918624878
Epoch: 3537, Batch Gradient Norm: 35.640492721849704
Epoch: 3537, Batch Gradient Norm after: 22.360677989457987
Epoch 3538/10000, Prediction Accuracy = 57.117999999999995%, Loss = 0.9714463114738464
Epoch: 3538, Batch Gradient Norm: 33.67780954349951
Epoch: 3538, Batch Gradient Norm after: 22.36067566264272
Epoch 3539/10000, Prediction Accuracy = 57.102%, Loss = 0.9662240147590637
Epoch: 3539, Batch Gradient Norm: 35.62158245794692
Epoch: 3539, Batch Gradient Norm after: 22.360676647320783
Epoch 3540/10000, Prediction Accuracy = 57.114%, Loss = 0.9711615800857544
Epoch: 3540, Batch Gradient Norm: 33.67082731856703
Epoch: 3540, Batch Gradient Norm after: 22.360675494736935
Epoch 3541/10000, Prediction Accuracy = 57.11%, Loss = 0.9659778356552124
Epoch: 3541, Batch Gradient Norm: 35.60298498218666
Epoch: 3541, Batch Gradient Norm after: 22.360677906237697
Epoch 3542/10000, Prediction Accuracy = 57.11800000000001%, Loss = 0.9708783149719238
Epoch: 3542, Batch Gradient Norm: 33.66445180349091
Epoch: 3542, Batch Gradient Norm after: 22.360675609250464
Epoch 3543/10000, Prediction Accuracy = 57.11400000000001%, Loss = 0.9657236456871032
Epoch: 3543, Batch Gradient Norm: 35.58806158001843
Epoch: 3543, Batch Gradient Norm after: 22.3606761917245
Epoch 3544/10000, Prediction Accuracy = 57.134%, Loss = 0.9705985069274903
Epoch: 3544, Batch Gradient Norm: 33.659558628415795
Epoch: 3544, Batch Gradient Norm after: 22.360675711077857
Epoch 3545/10000, Prediction Accuracy = 57.128%, Loss = 0.9654787063598633
Epoch: 3545, Batch Gradient Norm: 35.5656132992343
Epoch: 3545, Batch Gradient Norm after: 22.360676890438175
Epoch 3546/10000, Prediction Accuracy = 57.14000000000001%, Loss = 0.9703027129173278
Epoch: 3546, Batch Gradient Norm: 33.65465029096008
Epoch: 3546, Batch Gradient Norm after: 22.360677180716284
Epoch 3547/10000, Prediction Accuracy = 57.146%, Loss = 0.9652343392372131
Epoch: 3547, Batch Gradient Norm: 35.5426983937274
Epoch: 3547, Batch Gradient Norm after: 22.36067866918182
Epoch 3548/10000, Prediction Accuracy = 57.15400000000001%, Loss = 0.970007348060608
Epoch: 3548, Batch Gradient Norm: 33.650904733578855
Epoch: 3548, Batch Gradient Norm after: 22.360675737267822
Epoch 3549/10000, Prediction Accuracy = 57.15%, Loss = 0.9649903655052186
Epoch: 3549, Batch Gradient Norm: 35.51720442098419
Epoch: 3549, Batch Gradient Norm after: 22.3606758094585
Epoch 3550/10000, Prediction Accuracy = 57.156000000000006%, Loss = 0.9697014808654785
Epoch: 3550, Batch Gradient Norm: 33.65087926926698
Epoch: 3550, Batch Gradient Norm after: 22.36067618079002
Epoch 3551/10000, Prediction Accuracy = 57.162%, Loss = 0.9647450566291809
Epoch: 3551, Batch Gradient Norm: 35.49182789728195
Epoch: 3551, Batch Gradient Norm after: 22.360676790704172
Epoch 3552/10000, Prediction Accuracy = 57.160000000000004%, Loss = 0.969400691986084
Epoch: 3552, Batch Gradient Norm: 33.64861461017478
Epoch: 3552, Batch Gradient Norm after: 22.360675341154042
Epoch 3553/10000, Prediction Accuracy = 57.172000000000004%, Loss = 0.9645053505897522
Epoch: 3553, Batch Gradient Norm: 35.464815584311495
Epoch: 3553, Batch Gradient Norm after: 22.360676658750627
Epoch 3554/10000, Prediction Accuracy = 57.158%, Loss = 0.9690941333770752
Epoch: 3554, Batch Gradient Norm: 33.643886125531026
Epoch: 3554, Batch Gradient Norm after: 22.36067637798848
Epoch 3555/10000, Prediction Accuracy = 57.17%, Loss = 0.9642634749412536
Epoch: 3555, Batch Gradient Norm: 35.442530044943894
Epoch: 3555, Batch Gradient Norm after: 22.360678097141673
Epoch 3556/10000, Prediction Accuracy = 57.172000000000004%, Loss = 0.9687981963157654
Epoch: 3556, Batch Gradient Norm: 33.64310237207142
Epoch: 3556, Batch Gradient Norm after: 22.36067511260127
Epoch 3557/10000, Prediction Accuracy = 57.17%, Loss = 0.964016318321228
Epoch: 3557, Batch Gradient Norm: 35.419181214053474
Epoch: 3557, Batch Gradient Norm after: 22.36067636000881
Epoch 3558/10000, Prediction Accuracy = 57.17999999999999%, Loss = 0.9685059666633606
Epoch: 3558, Batch Gradient Norm: 33.63732801220172
Epoch: 3558, Batch Gradient Norm after: 22.360675849228095
Epoch 3559/10000, Prediction Accuracy = 57.168000000000006%, Loss = 0.9637746214866638
Epoch: 3559, Batch Gradient Norm: 35.39715076949945
Epoch: 3559, Batch Gradient Norm after: 22.360677039732547
Epoch 3560/10000, Prediction Accuracy = 57.184000000000005%, Loss = 0.9682204842567443
Epoch: 3560, Batch Gradient Norm: 33.632295188869186
Epoch: 3560, Batch Gradient Norm after: 22.360677412340912
Epoch 3561/10000, Prediction Accuracy = 57.17%, Loss = 0.9635318994522095
Epoch: 3561, Batch Gradient Norm: 35.379801246736584
Epoch: 3561, Batch Gradient Norm after: 22.36067625445817
Epoch 3562/10000, Prediction Accuracy = 57.186%, Loss = 0.9679437398910522
Epoch: 3562, Batch Gradient Norm: 33.62740346328703
Epoch: 3562, Batch Gradient Norm after: 22.36067705191085
Epoch 3563/10000, Prediction Accuracy = 57.18000000000001%, Loss = 0.9632898330688476
Epoch: 3563, Batch Gradient Norm: 35.3566033057996
Epoch: 3563, Batch Gradient Norm after: 22.360675509039318
Epoch 3564/10000, Prediction Accuracy = 57.19599999999999%, Loss = 0.967655336856842
Epoch: 3564, Batch Gradient Norm: 33.62008857494807
Epoch: 3564, Batch Gradient Norm after: 22.360676166548853
Epoch 3565/10000, Prediction Accuracy = 57.18000000000001%, Loss = 0.9630568861961365
Epoch: 3565, Batch Gradient Norm: 35.339292726402874
Epoch: 3565, Batch Gradient Norm after: 22.36067729861088
Epoch 3566/10000, Prediction Accuracy = 57.2%, Loss = 0.9673801779747009
Epoch: 3566, Batch Gradient Norm: 33.61581967150156
Epoch: 3566, Batch Gradient Norm after: 22.360678916556445
Epoch 3567/10000, Prediction Accuracy = 57.18800000000001%, Loss = 0.9628104209899903
Epoch: 3567, Batch Gradient Norm: 35.319412242644766
Epoch: 3567, Batch Gradient Norm after: 22.360676333370947
Epoch 3568/10000, Prediction Accuracy = 57.202%, Loss = 0.9670973658561707
Epoch: 3568, Batch Gradient Norm: 33.610486475097304
Epoch: 3568, Batch Gradient Norm after: 22.36067649522303
Epoch 3569/10000, Prediction Accuracy = 57.198%, Loss = 0.9625659227371216
Epoch: 3569, Batch Gradient Norm: 35.30522022212866
Epoch: 3569, Batch Gradient Norm after: 22.36067555335201
Epoch 3570/10000, Prediction Accuracy = 57.20399999999999%, Loss = 0.9668324828147888
Epoch: 3570, Batch Gradient Norm: 33.60553814993113
Epoch: 3570, Batch Gradient Norm after: 22.36067779048311
Epoch 3571/10000, Prediction Accuracy = 57.20400000000001%, Loss = 0.9623182773590088
Epoch: 3571, Batch Gradient Norm: 35.289274463400034
Epoch: 3571, Batch Gradient Norm after: 22.360674955430984
Epoch 3572/10000, Prediction Accuracy = 57.202%, Loss = 0.9665621161460877
Epoch: 3572, Batch Gradient Norm: 33.59654945506247
Epoch: 3572, Batch Gradient Norm after: 22.36067600048166
Epoch 3573/10000, Prediction Accuracy = 57.202%, Loss = 0.9620703101158142
Epoch: 3573, Batch Gradient Norm: 35.27538102123349
Epoch: 3573, Batch Gradient Norm after: 22.360676467470388
Epoch 3574/10000, Prediction Accuracy = 57.214%, Loss = 0.9663007378578186
Epoch: 3574, Batch Gradient Norm: 33.588288136499365
Epoch: 3574, Batch Gradient Norm after: 22.360677219792755
Epoch 3575/10000, Prediction Accuracy = 57.214%, Loss = 0.9618216753005981
Epoch: 3575, Batch Gradient Norm: 35.26457763561691
Epoch: 3575, Batch Gradient Norm after: 22.36067705380243
Epoch 3576/10000, Prediction Accuracy = 57.21%, Loss = 0.9660466313362122
Epoch: 3576, Batch Gradient Norm: 33.581550812718504
Epoch: 3576, Batch Gradient Norm after: 22.36067591496518
Epoch 3577/10000, Prediction Accuracy = 57.205999999999996%, Loss = 0.9615755915641785
Epoch: 3577, Batch Gradient Norm: 35.254344332678684
Epoch: 3577, Batch Gradient Norm after: 22.36067616296535
Epoch 3578/10000, Prediction Accuracy = 57.21%, Loss = 0.9657881736755372
Epoch: 3578, Batch Gradient Norm: 33.57647619335936
Epoch: 3578, Batch Gradient Norm after: 22.360677442670905
Epoch 3579/10000, Prediction Accuracy = 57.208000000000006%, Loss = 0.9613275647163391
Epoch: 3579, Batch Gradient Norm: 35.24619254387263
Epoch: 3579, Batch Gradient Norm after: 22.360676167638694
Epoch 3580/10000, Prediction Accuracy = 57.220000000000006%, Loss = 0.9655330300331115
Epoch: 3580, Batch Gradient Norm: 33.56898008773334
Epoch: 3580, Batch Gradient Norm after: 22.36067718531295
Epoch 3581/10000, Prediction Accuracy = 57.227999999999994%, Loss = 0.961077356338501
Epoch: 3581, Batch Gradient Norm: 35.23876410712071
Epoch: 3581, Batch Gradient Norm after: 22.3606753216117
Epoch 3582/10000, Prediction Accuracy = 57.227999999999994%, Loss = 0.9652816772460937
Epoch: 3582, Batch Gradient Norm: 33.55830024592814
Epoch: 3582, Batch Gradient Norm after: 22.360677540725188
Epoch 3583/10000, Prediction Accuracy = 57.234%, Loss = 0.9608324527740478
Epoch: 3583, Batch Gradient Norm: 35.22989705286996
Epoch: 3583, Batch Gradient Norm after: 22.360678216301412
Epoch 3584/10000, Prediction Accuracy = 57.242%, Loss = 0.9650360584259033
Epoch: 3584, Batch Gradient Norm: 33.552104690712525
Epoch: 3584, Batch Gradient Norm after: 22.36067830662077
Epoch 3585/10000, Prediction Accuracy = 57.242%, Loss = 0.9605798959732056
Epoch: 3585, Batch Gradient Norm: 35.21707051826931
Epoch: 3585, Batch Gradient Norm after: 22.360676643601604
Epoch 3586/10000, Prediction Accuracy = 57.24400000000001%, Loss = 0.9647828102111816
Epoch: 3586, Batch Gradient Norm: 33.544420822324575
Epoch: 3586, Batch Gradient Norm after: 22.36067785409098
Epoch 3587/10000, Prediction Accuracy = 57.24400000000001%, Loss = 0.9603334307670593
Epoch: 3587, Batch Gradient Norm: 35.20421281517635
Epoch: 3587, Batch Gradient Norm after: 22.360677471932934
Epoch 3588/10000, Prediction Accuracy = 57.23199999999999%, Loss = 0.9645203232765198
Epoch: 3588, Batch Gradient Norm: 33.538803996511724
Epoch: 3588, Batch Gradient Norm after: 22.3606760248115
Epoch 3589/10000, Prediction Accuracy = 57.248000000000005%, Loss = 0.9600883603096009
Epoch: 3589, Batch Gradient Norm: 35.192909744078285
Epoch: 3589, Batch Gradient Norm after: 22.3606741685995
Epoch 3590/10000, Prediction Accuracy = 57.24000000000001%, Loss = 0.9642659068107605
Epoch: 3590, Batch Gradient Norm: 33.53128111258761
Epoch: 3590, Batch Gradient Norm after: 22.360678028958663
Epoch 3591/10000, Prediction Accuracy = 57.246%, Loss = 0.9598414182662964
Epoch: 3591, Batch Gradient Norm: 35.17980359194145
Epoch: 3591, Batch Gradient Norm after: 22.3606747452347
Epoch 3592/10000, Prediction Accuracy = 57.239999999999995%, Loss = 0.9640054821968078
Epoch: 3592, Batch Gradient Norm: 33.52534914746419
Epoch: 3592, Batch Gradient Norm after: 22.360676556207753
Epoch 3593/10000, Prediction Accuracy = 57.239999999999995%, Loss = 0.9596019148826599
Epoch: 3593, Batch Gradient Norm: 35.173924466154006
Epoch: 3593, Batch Gradient Norm after: 22.360674483460297
Epoch 3594/10000, Prediction Accuracy = 57.242%, Loss = 0.963765287399292
Epoch: 3594, Batch Gradient Norm: 33.51688172733126
Epoch: 3594, Batch Gradient Norm after: 22.360677490609095
Epoch 3595/10000, Prediction Accuracy = 57.248000000000005%, Loss = 0.9593491196632385
Epoch: 3595, Batch Gradient Norm: 35.16841791462108
Epoch: 3595, Batch Gradient Norm after: 22.36067739282767
Epoch 3596/10000, Prediction Accuracy = 57.258%, Loss = 0.9635260343551636
Epoch: 3596, Batch Gradient Norm: 33.50703213166233
Epoch: 3596, Batch Gradient Norm after: 22.36067753712793
Epoch 3597/10000, Prediction Accuracy = 57.251999999999995%, Loss = 0.9591043710708618
Epoch: 3597, Batch Gradient Norm: 35.15671656447773
Epoch: 3597, Batch Gradient Norm after: 22.360675161043396
Epoch 3598/10000, Prediction Accuracy = 57.263999999999996%, Loss = 0.9632720828056336
Epoch: 3598, Batch Gradient Norm: 33.50265265691326
Epoch: 3598, Batch Gradient Norm after: 22.36067693217577
Epoch 3599/10000, Prediction Accuracy = 57.258%, Loss = 0.9588594675064087
Epoch: 3599, Batch Gradient Norm: 35.14245520432448
Epoch: 3599, Batch Gradient Norm after: 22.360676492212402
Epoch 3600/10000, Prediction Accuracy = 57.267999999999994%, Loss = 0.9630044221878051
Epoch: 3600, Batch Gradient Norm: 33.495639885887186
Epoch: 3600, Batch Gradient Norm after: 22.36067596053421
Epoch 3601/10000, Prediction Accuracy = 57.266000000000005%, Loss = 0.9586226344108582
Epoch: 3601, Batch Gradient Norm: 35.12567986715818
Epoch: 3601, Batch Gradient Norm after: 22.3606761296629
Epoch 3602/10000, Prediction Accuracy = 57.278%, Loss = 0.9627344131469726
Epoch: 3602, Batch Gradient Norm: 33.48943892592769
Epoch: 3602, Batch Gradient Norm after: 22.36067716461751
Epoch 3603/10000, Prediction Accuracy = 57.26800000000001%, Loss = 0.9583805084228516
Epoch: 3603, Batch Gradient Norm: 35.11495799109404
Epoch: 3603, Batch Gradient Norm after: 22.360676722562978
Epoch 3604/10000, Prediction Accuracy = 57.29%, Loss = 0.9624776482582093
Epoch: 3604, Batch Gradient Norm: 33.48005735233586
Epoch: 3604, Batch Gradient Norm after: 22.36067688983665
Epoch 3605/10000, Prediction Accuracy = 57.275999999999996%, Loss = 0.9581359028816223
Epoch: 3605, Batch Gradient Norm: 35.106797271356896
Epoch: 3605, Batch Gradient Norm after: 22.36067576710468
Epoch 3606/10000, Prediction Accuracy = 57.29600000000001%, Loss = 0.9622402548789978
Epoch: 3606, Batch Gradient Norm: 33.47011420544617
Epoch: 3606, Batch Gradient Norm after: 22.36067847882088
Epoch 3607/10000, Prediction Accuracy = 57.286%, Loss = 0.9578889966011047
Epoch: 3607, Batch Gradient Norm: 35.100604173535615
Epoch: 3607, Batch Gradient Norm after: 22.360676435851452
Epoch 3608/10000, Prediction Accuracy = 57.3%, Loss = 0.9619977831840515
Epoch: 3608, Batch Gradient Norm: 33.46326267398278
Epoch: 3608, Batch Gradient Norm after: 22.360678044484597
Epoch 3609/10000, Prediction Accuracy = 57.3%, Loss = 0.9576433062553406
Epoch: 3609, Batch Gradient Norm: 35.0937477693429
Epoch: 3609, Batch Gradient Norm after: 22.36067771092675
Epoch 3610/10000, Prediction Accuracy = 57.29600000000001%, Loss = 0.9617604494094849
Epoch: 3610, Batch Gradient Norm: 33.45420050408681
Epoch: 3610, Batch Gradient Norm after: 22.360677926822625
Epoch 3611/10000, Prediction Accuracy = 57.30800000000001%, Loss = 0.9573952794075012
Epoch: 3611, Batch Gradient Norm: 35.08373237768655
Epoch: 3611, Batch Gradient Norm after: 22.360678065142572
Epoch 3612/10000, Prediction Accuracy = 57.291999999999994%, Loss = 0.9615235447883606
Epoch: 3612, Batch Gradient Norm: 33.44307055276656
Epoch: 3612, Batch Gradient Norm after: 22.36067645549173
Epoch 3613/10000, Prediction Accuracy = 57.324%, Loss = 0.9571478724479675
Epoch: 3613, Batch Gradient Norm: 35.073034866092094
Epoch: 3613, Batch Gradient Norm after: 22.360676701246874
Epoch 3614/10000, Prediction Accuracy = 57.303999999999995%, Loss = 0.9612782597541809
Epoch: 3614, Batch Gradient Norm: 33.43345254537988
Epoch: 3614, Batch Gradient Norm after: 22.360677643346563
Epoch 3615/10000, Prediction Accuracy = 57.336%, Loss = 0.9569056034088135
Epoch: 3615, Batch Gradient Norm: 35.06373323884459
Epoch: 3615, Batch Gradient Norm after: 22.360676482777514
Epoch 3616/10000, Prediction Accuracy = 57.322%, Loss = 0.9610288381576538
Epoch: 3616, Batch Gradient Norm: 33.4250673912233
Epoch: 3616, Batch Gradient Norm after: 22.360677576039848
Epoch 3617/10000, Prediction Accuracy = 57.343999999999994%, Loss = 0.9566585659980774
Epoch: 3617, Batch Gradient Norm: 35.05361512246164
Epoch: 3617, Batch Gradient Norm after: 22.360677506430438
Epoch 3618/10000, Prediction Accuracy = 57.326%, Loss = 0.9607813835144043
Epoch: 3618, Batch Gradient Norm: 33.41432087338522
Epoch: 3618, Batch Gradient Norm after: 22.360678426436692
Epoch 3619/10000, Prediction Accuracy = 57.354%, Loss = 0.9564172983169555
Epoch: 3619, Batch Gradient Norm: 35.04498686497739
Epoch: 3619, Batch Gradient Norm after: 22.36067545338586
Epoch 3620/10000, Prediction Accuracy = 57.318%, Loss = 0.9605486035346985
Epoch: 3620, Batch Gradient Norm: 33.40419003784424
Epoch: 3620, Batch Gradient Norm after: 22.36067742025963
Epoch 3621/10000, Prediction Accuracy = 57.362%, Loss = 0.9561720252037048
Epoch: 3621, Batch Gradient Norm: 35.03757343910266
Epoch: 3621, Batch Gradient Norm after: 22.36067918962517
Epoch 3622/10000, Prediction Accuracy = 57.326%, Loss = 0.9603080987930298
Epoch: 3622, Batch Gradient Norm: 33.39392911071513
Epoch: 3622, Batch Gradient Norm after: 22.36067648276353
Epoch 3623/10000, Prediction Accuracy = 57.374%, Loss = 0.9559226274490357
Epoch: 3623, Batch Gradient Norm: 35.03811709674742
Epoch: 3623, Batch Gradient Norm after: 22.360676933340773
Epoch 3624/10000, Prediction Accuracy = 57.34000000000001%, Loss = 0.9600823044776916
Epoch: 3624, Batch Gradient Norm: 33.38279816776563
Epoch: 3624, Batch Gradient Norm after: 22.360675668425696
Epoch 3625/10000, Prediction Accuracy = 57.374%, Loss = 0.9556683540344239
Epoch: 3625, Batch Gradient Norm: 35.038408760588446
Epoch: 3625, Batch Gradient Norm after: 22.360675756981806
Epoch 3626/10000, Prediction Accuracy = 57.35%, Loss = 0.9598552703857421
Epoch: 3626, Batch Gradient Norm: 33.37104443478564
Epoch: 3626, Batch Gradient Norm after: 22.36067929601355
Epoch 3627/10000, Prediction Accuracy = 57.38000000000001%, Loss = 0.9554215431213379
Epoch: 3627, Batch Gradient Norm: 35.03462416672789
Epoch: 3627, Batch Gradient Norm after: 22.360677751436242
Epoch 3628/10000, Prediction Accuracy = 57.352%, Loss = 0.9596269369125366
Epoch: 3628, Batch Gradient Norm: 33.36204257326104
Epoch: 3628, Batch Gradient Norm after: 22.360679956425525
Epoch 3629/10000, Prediction Accuracy = 57.370000000000005%, Loss = 0.9551693081855774
Epoch: 3629, Batch Gradient Norm: 35.02749912992388
Epoch: 3629, Batch Gradient Norm after: 22.360677392037573
Epoch 3630/10000, Prediction Accuracy = 57.352%, Loss = 0.9593928217887878
Epoch: 3630, Batch Gradient Norm: 33.35193637670466
Epoch: 3630, Batch Gradient Norm after: 22.360678641592436
Epoch 3631/10000, Prediction Accuracy = 57.372%, Loss = 0.954922878742218
Epoch: 3631, Batch Gradient Norm: 35.02022018412003
Epoch: 3631, Batch Gradient Norm after: 22.36067833065424
Epoch 3632/10000, Prediction Accuracy = 57.370000000000005%, Loss = 0.9591578722000123
Epoch: 3632, Batch Gradient Norm: 33.34002516297912
Epoch: 3632, Batch Gradient Norm after: 22.360678046388355
Epoch 3633/10000, Prediction Accuracy = 57.374%, Loss = 0.9546746253967285
Epoch: 3633, Batch Gradient Norm: 35.01567754761858
Epoch: 3633, Batch Gradient Norm after: 22.36067784111908
Epoch 3634/10000, Prediction Accuracy = 57.379999999999995%, Loss = 0.9589290976524353
Epoch: 3634, Batch Gradient Norm: 33.330481577983505
Epoch: 3634, Batch Gradient Norm after: 22.360677336893826
Epoch 3635/10000, Prediction Accuracy = 57.38199999999999%, Loss = 0.9544314742088318
Epoch: 3635, Batch Gradient Norm: 35.01292544950851
Epoch: 3635, Batch Gradient Norm after: 22.36067553021815
Epoch 3636/10000, Prediction Accuracy = 57.39200000000001%, Loss = 0.9586954593658448
Epoch: 3636, Batch Gradient Norm: 33.3194366693178
Epoch: 3636, Batch Gradient Norm after: 22.360677933630868
Epoch 3637/10000, Prediction Accuracy = 57.391999999999996%, Loss = 0.9541879773139954
Epoch: 3637, Batch Gradient Norm: 35.006313806104224
Epoch: 3637, Batch Gradient Norm after: 22.360677568831974
Epoch 3638/10000, Prediction Accuracy = 57.39399999999999%, Loss = 0.9584577560424805
Epoch: 3638, Batch Gradient Norm: 33.30986773053187
Epoch: 3638, Batch Gradient Norm after: 22.360678179821353
Epoch 3639/10000, Prediction Accuracy = 57.394000000000005%, Loss = 0.9539385557174682
Epoch: 3639, Batch Gradient Norm: 35.00330443907779
Epoch: 3639, Batch Gradient Norm after: 22.36067663271726
Epoch 3640/10000, Prediction Accuracy = 57.395999999999994%, Loss = 0.958222758769989
Epoch: 3640, Batch Gradient Norm: 33.30202379220695
Epoch: 3640, Batch Gradient Norm after: 22.36067749766583
Epoch 3641/10000, Prediction Accuracy = 57.407999999999994%, Loss = 0.9536901235580444
Epoch: 3641, Batch Gradient Norm: 35.00168855672931
Epoch: 3641, Batch Gradient Norm after: 22.36067718068333
Epoch 3642/10000, Prediction Accuracy = 57.403999999999996%, Loss = 0.9579949378967285
Epoch: 3642, Batch Gradient Norm: 33.290966008883665
Epoch: 3642, Batch Gradient Norm after: 22.360677997596316
Epoch 3643/10000, Prediction Accuracy = 57.403999999999996%, Loss = 0.9534459710121155
Epoch: 3643, Batch Gradient Norm: 34.99994772660225
Epoch: 3643, Batch Gradient Norm after: 22.36067609981295
Epoch 3644/10000, Prediction Accuracy = 57.40599999999999%, Loss = 0.9577727317810059
Epoch: 3644, Batch Gradient Norm: 33.27862826520508
Epoch: 3644, Batch Gradient Norm after: 22.3606780887264
Epoch 3645/10000, Prediction Accuracy = 57.40599999999999%, Loss = 0.9532024621963501
Epoch: 3645, Batch Gradient Norm: 35.003810288753414
Epoch: 3645, Batch Gradient Norm after: 22.360676691492124
Epoch 3646/10000, Prediction Accuracy = 57.414%, Loss = 0.9575530767440796
Epoch: 3646, Batch Gradient Norm: 33.2672845836706
Epoch: 3646, Batch Gradient Norm after: 22.360675514925124
Epoch 3647/10000, Prediction Accuracy = 57.416%, Loss = 0.9529575705528259
Epoch: 3647, Batch Gradient Norm: 35.00478922728188
Epoch: 3647, Batch Gradient Norm after: 22.36067567653662
Epoch 3648/10000, Prediction Accuracy = 57.41799999999999%, Loss = 0.9573342561721802
Epoch: 3648, Batch Gradient Norm: 33.25638607137055
Epoch: 3648, Batch Gradient Norm after: 22.360676584467843
Epoch 3649/10000, Prediction Accuracy = 57.410000000000004%, Loss = 0.952702796459198
Epoch: 3649, Batch Gradient Norm: 35.0012366497463
Epoch: 3649, Batch Gradient Norm after: 22.360676620390755
Epoch 3650/10000, Prediction Accuracy = 57.418000000000006%, Loss = 0.9571059465408325
Epoch: 3650, Batch Gradient Norm: 33.248249698456604
Epoch: 3650, Batch Gradient Norm after: 22.36067509645158
Epoch 3651/10000, Prediction Accuracy = 57.424%, Loss = 0.9524576425552368
Epoch: 3651, Batch Gradient Norm: 34.99652260447713
Epoch: 3651, Batch Gradient Norm after: 22.36067702069067
Epoch 3652/10000, Prediction Accuracy = 57.419999999999995%, Loss = 0.9568747878074646
Epoch: 3652, Batch Gradient Norm: 33.23834631903315
Epoch: 3652, Batch Gradient Norm after: 22.360677351357836
Epoch 3653/10000, Prediction Accuracy = 57.432%, Loss = 0.9522104144096375
Epoch: 3653, Batch Gradient Norm: 34.993660005214174
Epoch: 3653, Batch Gradient Norm after: 22.36067973068722
Epoch 3654/10000, Prediction Accuracy = 57.422000000000004%, Loss = 0.9566469430923462
Epoch: 3654, Batch Gradient Norm: 33.22843434697013
Epoch: 3654, Batch Gradient Norm after: 22.360677681403
Epoch 3655/10000, Prediction Accuracy = 57.436%, Loss = 0.9519671678543091
Epoch: 3655, Batch Gradient Norm: 34.98913745477768
Epoch: 3655, Batch Gradient Norm after: 22.360675660634893
Epoch 3656/10000, Prediction Accuracy = 57.426%, Loss = 0.9564186692237854
Epoch: 3656, Batch Gradient Norm: 33.21793827238965
Epoch: 3656, Batch Gradient Norm after: 22.36067965266205
Epoch 3657/10000, Prediction Accuracy = 57.444%, Loss = 0.9517216563224793
Epoch: 3657, Batch Gradient Norm: 34.98330713959033
Epoch: 3657, Batch Gradient Norm after: 22.360675686290026
Epoch 3658/10000, Prediction Accuracy = 57.432%, Loss = 0.9561853647232056
Epoch: 3658, Batch Gradient Norm: 33.20837150712678
Epoch: 3658, Batch Gradient Norm after: 22.360677557311387
Epoch 3659/10000, Prediction Accuracy = 57.444%, Loss = 0.9514811277389527
Epoch: 3659, Batch Gradient Norm: 34.979079927401855
Epoch: 3659, Batch Gradient Norm after: 22.360675167422077
Epoch 3660/10000, Prediction Accuracy = 57.426%, Loss = 0.9559554815292358
Epoch: 3660, Batch Gradient Norm: 33.19959622520059
Epoch: 3660, Batch Gradient Norm after: 22.360677621866593
Epoch 3661/10000, Prediction Accuracy = 57.455999999999996%, Loss = 0.9512392401695251
Epoch: 3661, Batch Gradient Norm: 34.96877050771878
Epoch: 3661, Batch Gradient Norm after: 22.360676568416835
Epoch 3662/10000, Prediction Accuracy = 57.42999999999999%, Loss = 0.955712616443634
Epoch: 3662, Batch Gradient Norm: 33.192779740655865
Epoch: 3662, Batch Gradient Norm after: 22.360676927917986
Epoch 3663/10000, Prediction Accuracy = 57.458000000000006%, Loss = 0.9510018110275269
Epoch: 3663, Batch Gradient Norm: 34.962549248920716
Epoch: 3663, Batch Gradient Norm after: 22.360677828480007
Epoch 3664/10000, Prediction Accuracy = 57.436%, Loss = 0.9554714441299439
Epoch: 3664, Batch Gradient Norm: 33.18706162643103
Epoch: 3664, Batch Gradient Norm after: 22.36067746925154
Epoch 3665/10000, Prediction Accuracy = 57.46%, Loss = 0.9507523059844971
Epoch: 3665, Batch Gradient Norm: 34.95605607596854
Epoch: 3665, Batch Gradient Norm after: 22.360677254778558
Epoch 3666/10000, Prediction Accuracy = 57.431999999999995%, Loss = 0.955234682559967
Epoch: 3666, Batch Gradient Norm: 33.18147098016906
Epoch: 3666, Batch Gradient Norm after: 22.36067707347642
Epoch 3667/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.9505107402801514
Epoch: 3667, Batch Gradient Norm: 34.95101148515072
Epoch: 3667, Batch Gradient Norm after: 22.36067838494138
Epoch 3668/10000, Prediction Accuracy = 57.44%, Loss = 0.954998779296875
Epoch: 3668, Batch Gradient Norm: 33.17056269957799
Epoch: 3668, Batch Gradient Norm after: 22.36067774717386
Epoch 3669/10000, Prediction Accuracy = 57.45799999999999%, Loss = 0.950272262096405
Epoch: 3669, Batch Gradient Norm: 34.945634844179914
Epoch: 3669, Batch Gradient Norm after: 22.3606786846408
Epoch 3670/10000, Prediction Accuracy = 57.438%, Loss = 0.9547610282897949
Epoch: 3670, Batch Gradient Norm: 33.162925080176684
Epoch: 3670, Batch Gradient Norm after: 22.36067821450484
Epoch 3671/10000, Prediction Accuracy = 57.462%, Loss = 0.9500283002853394
Epoch: 3671, Batch Gradient Norm: 34.937976796168044
Epoch: 3671, Batch Gradient Norm after: 22.360678111238073
Epoch 3672/10000, Prediction Accuracy = 57.44200000000001%, Loss = 0.9545204401016235
Epoch: 3672, Batch Gradient Norm: 33.15331861565861
Epoch: 3672, Batch Gradient Norm after: 22.360676699418143
Epoch 3673/10000, Prediction Accuracy = 57.462%, Loss = 0.9497908234596253
Epoch: 3673, Batch Gradient Norm: 34.931911960664344
Epoch: 3673, Batch Gradient Norm after: 22.360676998298228
Epoch 3674/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.9542860388755798
Epoch: 3674, Batch Gradient Norm: 33.14409633696591
Epoch: 3674, Batch Gradient Norm after: 22.360676579879726
Epoch 3675/10000, Prediction Accuracy = 57.470000000000006%, Loss = 0.9495539784431457
Epoch: 3675, Batch Gradient Norm: 34.926065970119396
Epoch: 3675, Batch Gradient Norm after: 22.36067698404181
Epoch 3676/10000, Prediction Accuracy = 57.45399999999999%, Loss = 0.9540522217750549
Epoch: 3676, Batch Gradient Norm: 33.13726057666463
Epoch: 3676, Batch Gradient Norm after: 22.36067726164984
Epoch 3677/10000, Prediction Accuracy = 57.476%, Loss = 0.949309766292572
Epoch: 3677, Batch Gradient Norm: 34.918728591934816
Epoch: 3677, Batch Gradient Norm after: 22.360676806048755
Epoch 3678/10000, Prediction Accuracy = 57.465999999999994%, Loss = 0.9538182616233826
Epoch: 3678, Batch Gradient Norm: 33.129390229575364
Epoch: 3678, Batch Gradient Norm after: 22.36067898832079
Epoch 3679/10000, Prediction Accuracy = 57.484%, Loss = 0.9490660548210144
Epoch: 3679, Batch Gradient Norm: 34.91437715749802
Epoch: 3679, Batch Gradient Norm after: 22.36067590040163
Epoch 3680/10000, Prediction Accuracy = 57.472%, Loss = 0.9535853743553162
Epoch: 3680, Batch Gradient Norm: 33.11943290478605
Epoch: 3680, Batch Gradient Norm after: 22.36067688293615
Epoch 3681/10000, Prediction Accuracy = 57.488%, Loss = 0.9488243341445923
Epoch: 3681, Batch Gradient Norm: 34.9051454980668
Epoch: 3681, Batch Gradient Norm after: 22.3606752892294
Epoch 3682/10000, Prediction Accuracy = 57.48%, Loss = 0.9533427357673645
Epoch: 3682, Batch Gradient Norm: 33.11014371433864
Epoch: 3682, Batch Gradient Norm after: 22.360678429509594
Epoch 3683/10000, Prediction Accuracy = 57.489999999999995%, Loss = 0.9485849142074585
Epoch: 3683, Batch Gradient Norm: 34.895349905345526
Epoch: 3683, Batch Gradient Norm after: 22.360677641516027
Epoch 3684/10000, Prediction Accuracy = 57.488%, Loss = 0.9530991077423095
Epoch: 3684, Batch Gradient Norm: 33.101871096592724
Epoch: 3684, Batch Gradient Norm after: 22.36067702366647
Epoch 3685/10000, Prediction Accuracy = 57.488%, Loss = 0.9483477473258972
Epoch: 3685, Batch Gradient Norm: 34.88744084872956
Epoch: 3685, Batch Gradient Norm after: 22.36067672299436
Epoch 3686/10000, Prediction Accuracy = 57.508%, Loss = 0.9528578996658326
Epoch: 3686, Batch Gradient Norm: 33.09487921067071
Epoch: 3686, Batch Gradient Norm after: 22.360675767956014
Epoch 3687/10000, Prediction Accuracy = 57.501999999999995%, Loss = 0.9481062412261962
Epoch: 3687, Batch Gradient Norm: 34.87293614625143
Epoch: 3687, Batch Gradient Norm after: 22.36067912756351
Epoch 3688/10000, Prediction Accuracy = 57.507999999999996%, Loss = 0.9526107549667359
Epoch: 3688, Batch Gradient Norm: 33.08882329852549
Epoch: 3688, Batch Gradient Norm after: 22.36067721756248
Epoch 3689/10000, Prediction Accuracy = 57.510000000000005%, Loss = 0.9478717088699341
Epoch: 3689, Batch Gradient Norm: 34.86605914479977
Epoch: 3689, Batch Gradient Norm after: 22.36067927107075
Epoch 3690/10000, Prediction Accuracy = 57.52%, Loss = 0.9523679137229919
Epoch: 3690, Batch Gradient Norm: 33.08174324483345
Epoch: 3690, Batch Gradient Norm after: 22.360677410773118
Epoch 3691/10000, Prediction Accuracy = 57.516000000000005%, Loss = 0.9476349592208863
Epoch: 3691, Batch Gradient Norm: 34.85709787305993
Epoch: 3691, Batch Gradient Norm after: 22.360679388588615
Epoch 3692/10000, Prediction Accuracy = 57.522000000000006%, Loss = 0.9521288633346557
Epoch: 3692, Batch Gradient Norm: 33.07363461515838
Epoch: 3692, Batch Gradient Norm after: 22.36067701459461
Epoch 3693/10000, Prediction Accuracy = 57.513999999999996%, Loss = 0.9473954796791076
Epoch: 3693, Batch Gradient Norm: 34.84499028566728
Epoch: 3693, Batch Gradient Norm after: 22.360678056057196
Epoch 3694/10000, Prediction Accuracy = 57.524%, Loss = 0.9518759965896606
Epoch: 3694, Batch Gradient Norm: 33.065653823826594
Epoch: 3694, Batch Gradient Norm after: 22.360676932465218
Epoch 3695/10000, Prediction Accuracy = 57.512%, Loss = 0.9471631050109863
Epoch: 3695, Batch Gradient Norm: 34.837594038482095
Epoch: 3695, Batch Gradient Norm after: 22.36067863798331
Epoch 3696/10000, Prediction Accuracy = 57.525999999999996%, Loss = 0.9516289353370666
Epoch: 3696, Batch Gradient Norm: 33.06033638195375
Epoch: 3696, Batch Gradient Norm after: 22.36067652164934
Epoch 3697/10000, Prediction Accuracy = 57.51800000000001%, Loss = 0.9469274878501892
Epoch: 3697, Batch Gradient Norm: 34.82908024368134
Epoch: 3697, Batch Gradient Norm after: 22.36067881823811
Epoch 3698/10000, Prediction Accuracy = 57.532000000000004%, Loss = 0.9513846635818481
Epoch: 3698, Batch Gradient Norm: 33.05199575943741
Epoch: 3698, Batch Gradient Norm after: 22.36067763526768
Epoch 3699/10000, Prediction Accuracy = 57.513999999999996%, Loss = 0.9466927886009217
Epoch: 3699, Batch Gradient Norm: 34.81806873749572
Epoch: 3699, Batch Gradient Norm after: 22.360678445690183
Epoch 3700/10000, Prediction Accuracy = 57.532000000000004%, Loss = 0.9511308193206787
Epoch: 3700, Batch Gradient Norm: 33.046423125839404
Epoch: 3700, Batch Gradient Norm after: 22.360676604594065
Epoch 3701/10000, Prediction Accuracy = 57.516000000000005%, Loss = 0.9464618563652039
Epoch: 3701, Batch Gradient Norm: 34.81149489418956
Epoch: 3701, Batch Gradient Norm after: 22.360677445389623
Epoch 3702/10000, Prediction Accuracy = 57.544000000000004%, Loss = 0.9508833765983582
Epoch: 3702, Batch Gradient Norm: 33.040774141645905
Epoch: 3702, Batch Gradient Norm after: 22.36067638111378
Epoch 3703/10000, Prediction Accuracy = 57.528%, Loss = 0.9462242364883423
Epoch: 3703, Batch Gradient Norm: 34.799459255011264
Epoch: 3703, Batch Gradient Norm after: 22.36067740083312
Epoch 3704/10000, Prediction Accuracy = 57.54%, Loss = 0.9506401896476746
Epoch: 3704, Batch Gradient Norm: 33.03255790824419
Epoch: 3704, Batch Gradient Norm after: 22.360678591320674
Epoch 3705/10000, Prediction Accuracy = 57.541999999999994%, Loss = 0.945992648601532
Epoch: 3705, Batch Gradient Norm: 34.79190023805527
Epoch: 3705, Batch Gradient Norm after: 22.360677001257564
Epoch 3706/10000, Prediction Accuracy = 57.55%, Loss = 0.9504063963890076
Epoch: 3706, Batch Gradient Norm: 33.020228268439965
Epoch: 3706, Batch Gradient Norm after: 22.360675511447134
Epoch 3707/10000, Prediction Accuracy = 57.544000000000004%, Loss = 0.9457569718360901
Epoch: 3707, Batch Gradient Norm: 34.78086386148293
Epoch: 3707, Batch Gradient Norm after: 22.360677559991746
Epoch 3708/10000, Prediction Accuracy = 57.55800000000001%, Loss = 0.9501664519309998
Epoch: 3708, Batch Gradient Norm: 33.01171892699753
Epoch: 3708, Batch Gradient Norm after: 22.36067633682699
Epoch 3709/10000, Prediction Accuracy = 57.556%, Loss = 0.945525074005127
Epoch: 3709, Batch Gradient Norm: 34.77104303133026
Epoch: 3709, Batch Gradient Norm after: 22.360677117727143
Epoch 3710/10000, Prediction Accuracy = 57.565999999999995%, Loss = 0.9499256014823914
Epoch: 3710, Batch Gradient Norm: 33.00252415779231
Epoch: 3710, Batch Gradient Norm after: 22.36067881035507
Epoch 3711/10000, Prediction Accuracy = 57.565999999999995%, Loss = 0.9452872276306152
Epoch: 3711, Batch Gradient Norm: 34.75426772746281
Epoch: 3711, Batch Gradient Norm after: 22.3606769850638
Epoch 3712/10000, Prediction Accuracy = 57.564%, Loss = 0.9496730446815491
Epoch: 3712, Batch Gradient Norm: 32.99894494104142
Epoch: 3712, Batch Gradient Norm after: 22.36067781485261
Epoch 3713/10000, Prediction Accuracy = 57.568000000000005%, Loss = 0.9450539588928223
Epoch: 3713, Batch Gradient Norm: 34.73964627141397
Epoch: 3713, Batch Gradient Norm after: 22.360675876351763
Epoch 3714/10000, Prediction Accuracy = 57.574%, Loss = 0.9494152545928956
Epoch: 3714, Batch Gradient Norm: 32.99261978093875
Epoch: 3714, Batch Gradient Norm after: 22.360680621770545
Epoch 3715/10000, Prediction Accuracy = 57.556%, Loss = 0.9448257565498352
Epoch: 3715, Batch Gradient Norm: 34.72298120108394
Epoch: 3715, Batch Gradient Norm after: 22.360679288166512
Epoch 3716/10000, Prediction Accuracy = 57.565999999999995%, Loss = 0.9491640448570251
Epoch: 3716, Batch Gradient Norm: 32.98818965615188
Epoch: 3716, Batch Gradient Norm after: 22.36067820927825
Epoch 3717/10000, Prediction Accuracy = 57.564%, Loss = 0.9445901036262512
Epoch: 3717, Batch Gradient Norm: 34.70416640871527
Epoch: 3717, Batch Gradient Norm after: 22.360677379259226
Epoch 3718/10000, Prediction Accuracy = 57.576%, Loss = 0.9488954782485962
Epoch: 3718, Batch Gradient Norm: 32.98405758016693
Epoch: 3718, Batch Gradient Norm after: 22.36067812939561
Epoch 3719/10000, Prediction Accuracy = 57.56%, Loss = 0.9443602085113525
Epoch: 3719, Batch Gradient Norm: 34.688714047498216
Epoch: 3719, Batch Gradient Norm after: 22.360677520375773
Epoch 3720/10000, Prediction Accuracy = 57.58%, Loss = 0.9486346006393432
Epoch: 3720, Batch Gradient Norm: 32.978199663575495
Epoch: 3720, Batch Gradient Norm after: 22.360679372200543
Epoch 3721/10000, Prediction Accuracy = 57.568000000000005%, Loss = 0.9441285014152527
Epoch: 3721, Batch Gradient Norm: 34.669497211516514
Epoch: 3721, Batch Gradient Norm after: 22.360675022537205
Epoch 3722/10000, Prediction Accuracy = 57.58399999999999%, Loss = 0.9483744859695434
Epoch: 3722, Batch Gradient Norm: 32.97254680445694
Epoch: 3722, Batch Gradient Norm after: 22.360676976243038
Epoch 3723/10000, Prediction Accuracy = 57.57000000000001%, Loss = 0.9438980340957641
Epoch: 3723, Batch Gradient Norm: 34.6521800225933
Epoch: 3723, Batch Gradient Norm after: 22.360675136506806
Epoch 3724/10000, Prediction Accuracy = 57.588%, Loss = 0.9481146812438965
Epoch: 3724, Batch Gradient Norm: 32.96848770210725
Epoch: 3724, Batch Gradient Norm after: 22.360677644015524
Epoch 3725/10000, Prediction Accuracy = 57.568000000000005%, Loss = 0.9436698675155639
Epoch: 3725, Batch Gradient Norm: 34.63721370335795
Epoch: 3725, Batch Gradient Norm after: 22.36067574283639
Epoch 3726/10000, Prediction Accuracy = 57.586%, Loss = 0.9478545188903809
Epoch: 3726, Batch Gradient Norm: 32.96267172052591
Epoch: 3726, Batch Gradient Norm after: 22.360678043296552
Epoch 3727/10000, Prediction Accuracy = 57.577999999999996%, Loss = 0.9434391379356384
Epoch: 3727, Batch Gradient Norm: 34.62461517631595
Epoch: 3727, Batch Gradient Norm after: 22.360675597147097
Epoch 3728/10000, Prediction Accuracy = 57.592%, Loss = 0.9476140260696411
Epoch: 3728, Batch Gradient Norm: 32.95517830171312
Epoch: 3728, Batch Gradient Norm after: 22.36067725634486
Epoch 3729/10000, Prediction Accuracy = 57.58%, Loss = 0.9432046175003052
Epoch: 3729, Batch Gradient Norm: 34.61076259697879
Epoch: 3729, Batch Gradient Norm after: 22.36067744564929
Epoch 3730/10000, Prediction Accuracy = 57.612%, Loss = 0.9473577022552491
Epoch: 3730, Batch Gradient Norm: 32.949789067096106
Epoch: 3730, Batch Gradient Norm after: 22.360677611937348
Epoch 3731/10000, Prediction Accuracy = 57.58%, Loss = 0.9429789662361145
Epoch: 3731, Batch Gradient Norm: 34.593439127489454
Epoch: 3731, Batch Gradient Norm after: 22.360676466975935
Epoch 3732/10000, Prediction Accuracy = 57.622%, Loss = 0.9471019983291626
Epoch: 3732, Batch Gradient Norm: 32.94481056270101
Epoch: 3732, Batch Gradient Norm after: 22.360675963347823
Epoch 3733/10000, Prediction Accuracy = 57.592000000000006%, Loss = 0.9427445888519287
Epoch: 3733, Batch Gradient Norm: 34.57684121761541
Epoch: 3733, Batch Gradient Norm after: 22.3606787669666
Epoch 3734/10000, Prediction Accuracy = 57.638%, Loss = 0.946844756603241
Epoch: 3734, Batch Gradient Norm: 32.94084426911196
Epoch: 3734, Batch Gradient Norm after: 22.360678318649704
Epoch 3735/10000, Prediction Accuracy = 57.59000000000001%, Loss = 0.9425187587738038
Epoch: 3735, Batch Gradient Norm: 34.557468574880495
Epoch: 3735, Batch Gradient Norm after: 22.360676929229108
Epoch 3736/10000, Prediction Accuracy = 57.63000000000001%, Loss = 0.94658442735672
Epoch: 3736, Batch Gradient Norm: 32.93593742031147
Epoch: 3736, Batch Gradient Norm after: 22.360680330108977
Epoch 3737/10000, Prediction Accuracy = 57.593999999999994%, Loss = 0.9422857880592346
Epoch: 3737, Batch Gradient Norm: 34.540310508141935
Epoch: 3737, Batch Gradient Norm after: 22.360679668505185
Epoch 3738/10000, Prediction Accuracy = 57.63399999999999%, Loss = 0.9463248968124389
Epoch: 3738, Batch Gradient Norm: 32.931647033245184
Epoch: 3738, Batch Gradient Norm after: 22.36067725248165
Epoch 3739/10000, Prediction Accuracy = 57.598%, Loss = 0.942054557800293
Epoch: 3739, Batch Gradient Norm: 34.5234134449673
Epoch: 3739, Batch Gradient Norm after: 22.3606781466216
Epoch 3740/10000, Prediction Accuracy = 57.646%, Loss = 0.9460613608360291
Epoch: 3740, Batch Gradient Norm: 32.92581293268386
Epoch: 3740, Batch Gradient Norm after: 22.360680467225617
Epoch 3741/10000, Prediction Accuracy = 57.598%, Loss = 0.9418266296386719
Epoch: 3741, Batch Gradient Norm: 34.50936770421203
Epoch: 3741, Batch Gradient Norm after: 22.360678682537813
Epoch 3742/10000, Prediction Accuracy = 57.646%, Loss = 0.9458066940307617
Epoch: 3742, Batch Gradient Norm: 32.918597949364376
Epoch: 3742, Batch Gradient Norm after: 22.360679420827044
Epoch 3743/10000, Prediction Accuracy = 57.598%, Loss = 0.9416054248809814
Epoch: 3743, Batch Gradient Norm: 34.49131048618177
Epoch: 3743, Batch Gradient Norm after: 22.36067739983037
Epoch 3744/10000, Prediction Accuracy = 57.653999999999996%, Loss = 0.9455419778823853
Epoch: 3744, Batch Gradient Norm: 32.91238974051218
Epoch: 3744, Batch Gradient Norm after: 22.3606774031604
Epoch 3745/10000, Prediction Accuracy = 57.598%, Loss = 0.941382908821106
Epoch: 3745, Batch Gradient Norm: 34.4781028201587
Epoch: 3745, Batch Gradient Norm after: 22.360677179110642
Epoch 3746/10000, Prediction Accuracy = 57.652%, Loss = 0.9452919244766236
Epoch: 3746, Batch Gradient Norm: 32.9046562156661
Epoch: 3746, Batch Gradient Norm after: 22.36067846079344
Epoch 3747/10000, Prediction Accuracy = 57.604%, Loss = 0.941148555278778
Epoch: 3747, Batch Gradient Norm: 34.46850098301895
Epoch: 3747, Batch Gradient Norm after: 22.360676359752436
Epoch 3748/10000, Prediction Accuracy = 57.656000000000006%, Loss = 0.9450557827949524
Epoch: 3748, Batch Gradient Norm: 32.892554849406054
Epoch: 3748, Batch Gradient Norm after: 22.360679016239988
Epoch 3749/10000, Prediction Accuracy = 57.605999999999995%, Loss = 0.9409155011177063
Epoch: 3749, Batch Gradient Norm: 34.4642254605074
Epoch: 3749, Batch Gradient Norm after: 22.360676190090416
Epoch 3750/10000, Prediction Accuracy = 57.65599999999999%, Loss = 0.9448291540145874
Epoch: 3750, Batch Gradient Norm: 32.884827184643534
Epoch: 3750, Batch Gradient Norm after: 22.360677489588593
Epoch 3751/10000, Prediction Accuracy = 57.598%, Loss = 0.9406770706176758
Epoch: 3751, Batch Gradient Norm: 34.45765401031706
Epoch: 3751, Batch Gradient Norm after: 22.360676403236155
Epoch 3752/10000, Prediction Accuracy = 57.65599999999999%, Loss = 0.9445972442626953
Epoch: 3752, Batch Gradient Norm: 32.8749305320269
Epoch: 3752, Batch Gradient Norm after: 22.360678771017028
Epoch 3753/10000, Prediction Accuracy = 57.60999999999999%, Loss = 0.9404426693916321
Epoch: 3753, Batch Gradient Norm: 34.45506281527774
Epoch: 3753, Batch Gradient Norm after: 22.360675940244647
Epoch 3754/10000, Prediction Accuracy = 57.658%, Loss = 0.9443743586540222
Epoch: 3754, Batch Gradient Norm: 32.86376888226459
Epoch: 3754, Batch Gradient Norm after: 22.36067832154256
Epoch 3755/10000, Prediction Accuracy = 57.61%, Loss = 0.9402058482170105
Epoch: 3755, Batch Gradient Norm: 34.454859951255706
Epoch: 3755, Batch Gradient Norm after: 22.360677578640768
Epoch 3756/10000, Prediction Accuracy = 57.664%, Loss = 0.9441612243652344
Epoch: 3756, Batch Gradient Norm: 32.85535170978667
Epoch: 3756, Batch Gradient Norm after: 22.360677154450183
Epoch 3757/10000, Prediction Accuracy = 57.63000000000001%, Loss = 0.9399717688560486
Epoch: 3757, Batch Gradient Norm: 34.44725753844463
Epoch: 3757, Batch Gradient Norm after: 22.360678179408737
Epoch 3758/10000, Prediction Accuracy = 57.662%, Loss = 0.9439333319664002
Epoch: 3758, Batch Gradient Norm: 32.84650116789919
Epoch: 3758, Batch Gradient Norm after: 22.360678505975336
Epoch 3759/10000, Prediction Accuracy = 57.626%, Loss = 0.9397358417510986
Epoch: 3759, Batch Gradient Norm: 34.44414698597387
Epoch: 3759, Batch Gradient Norm after: 22.36067803544723
Epoch 3760/10000, Prediction Accuracy = 57.67%, Loss = 0.9437206387519836
Epoch: 3760, Batch Gradient Norm: 32.83629252929012
Epoch: 3760, Batch Gradient Norm after: 22.36067727849171
Epoch 3761/10000, Prediction Accuracy = 57.632000000000005%, Loss = 0.9395002365112305
Epoch: 3761, Batch Gradient Norm: 34.43759497368264
Epoch: 3761, Batch Gradient Norm after: 22.360678921427855
Epoch 3762/10000, Prediction Accuracy = 57.652%, Loss = 0.9435023903846741
Epoch: 3762, Batch Gradient Norm: 32.828642368019864
Epoch: 3762, Batch Gradient Norm after: 22.360677246220984
Epoch 3763/10000, Prediction Accuracy = 57.634%, Loss = 0.9392643928527832
Epoch: 3763, Batch Gradient Norm: 34.434550227934835
Epoch: 3763, Batch Gradient Norm after: 22.360678048309047
Epoch 3764/10000, Prediction Accuracy = 57.664%, Loss = 0.9432799100875855
Epoch: 3764, Batch Gradient Norm: 32.82136584293647
Epoch: 3764, Batch Gradient Norm after: 22.36067940311134
Epoch 3765/10000, Prediction Accuracy = 57.646%, Loss = 0.9390289068222046
Epoch: 3765, Batch Gradient Norm: 34.43394232196146
Epoch: 3765, Batch Gradient Norm after: 22.360676220470246
Epoch 3766/10000, Prediction Accuracy = 57.674%, Loss = 0.9430652618408203
Epoch: 3766, Batch Gradient Norm: 32.81149662308519
Epoch: 3766, Batch Gradient Norm after: 22.360679045951304
Epoch 3767/10000, Prediction Accuracy = 57.65%, Loss = 0.938795018196106
Epoch: 3767, Batch Gradient Norm: 34.42631663441358
Epoch: 3767, Batch Gradient Norm after: 22.36067761079927
Epoch 3768/10000, Prediction Accuracy = 57.66799999999999%, Loss = 0.9428397178649902
Epoch: 3768, Batch Gradient Norm: 32.80175290742843
Epoch: 3768, Batch Gradient Norm after: 22.36067765745546
Epoch 3769/10000, Prediction Accuracy = 57.648%, Loss = 0.9385595202445984
Epoch: 3769, Batch Gradient Norm: 34.42003132491033
Epoch: 3769, Batch Gradient Norm after: 22.360675543395047
Epoch 3770/10000, Prediction Accuracy = 57.676%, Loss = 0.9426127552986145
Epoch: 3770, Batch Gradient Norm: 32.79233982325292
Epoch: 3770, Batch Gradient Norm after: 22.360678490940032
Epoch 3771/10000, Prediction Accuracy = 57.652%, Loss = 0.9383339762687684
Epoch: 3771, Batch Gradient Norm: 34.415034062144315
Epoch: 3771, Batch Gradient Norm after: 22.360678584109447
Epoch 3772/10000, Prediction Accuracy = 57.672000000000004%, Loss = 0.9423839092254639
Epoch: 3772, Batch Gradient Norm: 32.786527281175054
Epoch: 3772, Batch Gradient Norm after: 22.360680274289052
Epoch 3773/10000, Prediction Accuracy = 57.653999999999996%, Loss = 0.9380994081497193
Epoch: 3773, Batch Gradient Norm: 34.41123667587824
Epoch: 3773, Batch Gradient Norm after: 22.36067812989693
Epoch 3774/10000, Prediction Accuracy = 57.66600000000001%, Loss = 0.942162549495697
Epoch: 3774, Batch Gradient Norm: 32.77696339648788
Epoch: 3774, Batch Gradient Norm after: 22.360679902689462
Epoch 3775/10000, Prediction Accuracy = 57.65%, Loss = 0.9378638505935669
Epoch: 3775, Batch Gradient Norm: 34.410058900060704
Epoch: 3775, Batch Gradient Norm after: 22.3606782010252
Epoch 3776/10000, Prediction Accuracy = 57.666%, Loss = 0.9419538259506226
Epoch: 3776, Batch Gradient Norm: 32.765723164271044
Epoch: 3776, Batch Gradient Norm after: 22.360679850139235
Epoch 3777/10000, Prediction Accuracy = 57.652%, Loss = 0.9376297116279602
Epoch: 3777, Batch Gradient Norm: 34.41461751307901
Epoch: 3777, Batch Gradient Norm after: 22.36067705080143
Epoch 3778/10000, Prediction Accuracy = 57.67%, Loss = 0.9417500495910645
Epoch: 3778, Batch Gradient Norm: 32.750443007438705
Epoch: 3778, Batch Gradient Norm after: 22.360677286130663
Epoch 3779/10000, Prediction Accuracy = 57.662%, Loss = 0.9373910546302795
Epoch: 3779, Batch Gradient Norm: 34.416686783062524
Epoch: 3779, Batch Gradient Norm after: 22.36067895752846
Epoch 3780/10000, Prediction Accuracy = 57.674%, Loss = 0.9415482401847839
Epoch: 3780, Batch Gradient Norm: 32.73985995540431
Epoch: 3780, Batch Gradient Norm after: 22.360676152213692
Epoch 3781/10000, Prediction Accuracy = 57.664%, Loss = 0.9371564030647278
Epoch: 3781, Batch Gradient Norm: 34.41869398018864
Epoch: 3781, Batch Gradient Norm after: 22.360677399223864
Epoch 3782/10000, Prediction Accuracy = 57.674%, Loss = 0.9413414716720581
Epoch: 3782, Batch Gradient Norm: 32.73024819948365
Epoch: 3782, Batch Gradient Norm after: 22.36067876521363
Epoch 3783/10000, Prediction Accuracy = 57.66799999999999%, Loss = 0.9369201421737671
Epoch: 3783, Batch Gradient Norm: 34.41710873408919
Epoch: 3783, Batch Gradient Norm after: 22.36067650982557
Epoch 3784/10000, Prediction Accuracy = 57.674%, Loss = 0.941125237941742
Epoch: 3784, Batch Gradient Norm: 32.71968053205842
Epoch: 3784, Batch Gradient Norm after: 22.360677867843513
Epoch 3785/10000, Prediction Accuracy = 57.668000000000006%, Loss = 0.9366820096969605
Epoch: 3785, Batch Gradient Norm: 34.41608859738284
Epoch: 3785, Batch Gradient Norm after: 22.360676283887386
Epoch 3786/10000, Prediction Accuracy = 57.681999999999995%, Loss = 0.9409132480621338
Epoch: 3786, Batch Gradient Norm: 32.711599906913825
Epoch: 3786, Batch Gradient Norm after: 22.36067923119547
Epoch 3787/10000, Prediction Accuracy = 57.676%, Loss = 0.9364484190940857
Epoch: 3787, Batch Gradient Norm: 34.41529734678645
Epoch: 3787, Batch Gradient Norm after: 22.360677016227978
Epoch 3788/10000, Prediction Accuracy = 57.674%, Loss = 0.9407022595405579
Epoch: 3788, Batch Gradient Norm: 32.70242605047763
Epoch: 3788, Batch Gradient Norm after: 22.360677640166053
Epoch 3789/10000, Prediction Accuracy = 57.681999999999995%, Loss = 0.9362191081047058
Epoch: 3789, Batch Gradient Norm: 34.41056689124679
Epoch: 3789, Batch Gradient Norm after: 22.360678455273526
Epoch 3790/10000, Prediction Accuracy = 57.678%, Loss = 0.9404836893081665
Epoch: 3790, Batch Gradient Norm: 32.69565838832343
Epoch: 3790, Batch Gradient Norm after: 22.360677951805158
Epoch 3791/10000, Prediction Accuracy = 57.676%, Loss = 0.9359859585762024
Epoch: 3791, Batch Gradient Norm: 34.406579129329245
Epoch: 3791, Batch Gradient Norm after: 22.360676145456406
Epoch 3792/10000, Prediction Accuracy = 57.682%, Loss = 0.9402655005455017
Epoch: 3792, Batch Gradient Norm: 32.68958380675958
Epoch: 3792, Batch Gradient Norm after: 22.360676708576936
Epoch 3793/10000, Prediction Accuracy = 57.678%, Loss = 0.9357598185539245
Epoch: 3793, Batch Gradient Norm: 34.39596711173585
Epoch: 3793, Batch Gradient Norm after: 22.360675518499505
Epoch 3794/10000, Prediction Accuracy = 57.678%, Loss = 0.9400274395942688
Epoch: 3794, Batch Gradient Norm: 32.67933168478105
Epoch: 3794, Batch Gradient Norm after: 22.36067898833239
Epoch 3795/10000, Prediction Accuracy = 57.678%, Loss = 0.9355345010757447
Epoch: 3795, Batch Gradient Norm: 34.38523026612382
Epoch: 3795, Batch Gradient Norm after: 22.360678417125985
Epoch 3796/10000, Prediction Accuracy = 57.678%, Loss = 0.9397983431816102
Epoch: 3796, Batch Gradient Norm: 32.671327909869134
Epoch: 3796, Batch Gradient Norm after: 22.360680018015703
Epoch 3797/10000, Prediction Accuracy = 57.676%, Loss = 0.935302996635437
Epoch: 3797, Batch Gradient Norm: 34.3807225655472
Epoch: 3797, Batch Gradient Norm after: 22.36067688855388
Epoch 3798/10000, Prediction Accuracy = 57.684000000000005%, Loss = 0.9395733118057251
Epoch: 3798, Batch Gradient Norm: 32.662722075104185
Epoch: 3798, Batch Gradient Norm after: 22.360679085813345
Epoch 3799/10000, Prediction Accuracy = 57.682%, Loss = 0.9350754499435425
Epoch: 3799, Batch Gradient Norm: 34.373428250366274
Epoch: 3799, Batch Gradient Norm after: 22.360676149408746
Epoch 3800/10000, Prediction Accuracy = 57.686%, Loss = 0.9393477201461792
Epoch: 3800, Batch Gradient Norm: 32.656266297486354
Epoch: 3800, Batch Gradient Norm after: 22.3606768126999
Epoch 3801/10000, Prediction Accuracy = 57.69199999999999%, Loss = 0.9348420858383178
Epoch: 3801, Batch Gradient Norm: 34.36693409411534
Epoch: 3801, Batch Gradient Norm after: 22.36067695000973
Epoch 3802/10000, Prediction Accuracy = 57.696000000000005%, Loss = 0.9391202330589294
Epoch: 3802, Batch Gradient Norm: 32.64752691306217
Epoch: 3802, Batch Gradient Norm after: 22.36067789454341
Epoch 3803/10000, Prediction Accuracy = 57.698%, Loss = 0.9346135377883911
Epoch: 3803, Batch Gradient Norm: 34.357472378738684
Epoch: 3803, Batch Gradient Norm after: 22.36067927572586
Epoch 3804/10000, Prediction Accuracy = 57.71%, Loss = 0.9388803958892822
Epoch: 3804, Batch Gradient Norm: 32.64326840317779
Epoch: 3804, Batch Gradient Norm after: 22.36067852526954
Epoch 3805/10000, Prediction Accuracy = 57.7%, Loss = 0.9343879580497741
Epoch: 3805, Batch Gradient Norm: 34.347281097027654
Epoch: 3805, Batch Gradient Norm after: 22.36067885680607
Epoch 3806/10000, Prediction Accuracy = 57.720000000000006%, Loss = 0.9386401057243348
Epoch: 3806, Batch Gradient Norm: 32.63570778554473
Epoch: 3806, Batch Gradient Norm after: 22.360679954179762
Epoch 3807/10000, Prediction Accuracy = 57.702%, Loss = 0.9341642498970032
Epoch: 3807, Batch Gradient Norm: 34.33737212468266
Epoch: 3807, Batch Gradient Norm after: 22.36067865589947
Epoch 3808/10000, Prediction Accuracy = 57.721999999999994%, Loss = 0.9384087204933167
Epoch: 3808, Batch Gradient Norm: 32.62756197386415
Epoch: 3808, Batch Gradient Norm after: 22.36067873328422
Epoch 3809/10000, Prediction Accuracy = 57.71999999999999%, Loss = 0.9339361906051635
Epoch: 3809, Batch Gradient Norm: 34.331725751254005
Epoch: 3809, Batch Gradient Norm after: 22.360678321629557
Epoch 3810/10000, Prediction Accuracy = 57.736000000000004%, Loss = 0.9381837725639344
Epoch: 3810, Batch Gradient Norm: 32.62049339984681
Epoch: 3810, Batch Gradient Norm after: 22.360679343332112
Epoch 3811/10000, Prediction Accuracy = 57.726%, Loss = 0.9337058663368225
Epoch: 3811, Batch Gradient Norm: 34.327858414439284
Epoch: 3811, Batch Gradient Norm after: 22.360678005358665
Epoch 3812/10000, Prediction Accuracy = 57.748000000000005%, Loss = 0.9379651546478271
Epoch: 3812, Batch Gradient Norm: 32.610075918644384
Epoch: 3812, Batch Gradient Norm after: 22.360677248880314
Epoch 3813/10000, Prediction Accuracy = 57.724000000000004%, Loss = 0.9334765791893005
Epoch: 3813, Batch Gradient Norm: 34.323273973562046
Epoch: 3813, Batch Gradient Norm after: 22.36067836320388
Epoch 3814/10000, Prediction Accuracy = 57.75599999999999%, Loss = 0.9377423286437988
Epoch: 3814, Batch Gradient Norm: 32.60263540510286
Epoch: 3814, Batch Gradient Norm after: 22.36067794715117
Epoch 3815/10000, Prediction Accuracy = 57.722%, Loss = 0.9332476377487182
Epoch: 3815, Batch Gradient Norm: 34.316914646320626
Epoch: 3815, Batch Gradient Norm after: 22.360677862426087
Epoch 3816/10000, Prediction Accuracy = 57.757999999999996%, Loss = 0.9375164866447449
Epoch: 3816, Batch Gradient Norm: 32.59463696769704
Epoch: 3816, Batch Gradient Norm after: 22.36067734655232
Epoch 3817/10000, Prediction Accuracy = 57.734%, Loss = 0.9330238580703736
Epoch: 3817, Batch Gradient Norm: 34.30648437403924
Epoch: 3817, Batch Gradient Norm after: 22.36067891621038
Epoch 3818/10000, Prediction Accuracy = 57.75599999999999%, Loss = 0.9372869968414307
Epoch: 3818, Batch Gradient Norm: 32.58661223172616
Epoch: 3818, Batch Gradient Norm after: 22.360675720842877
Epoch 3819/10000, Prediction Accuracy = 57.738%, Loss = 0.932797086238861
Epoch: 3819, Batch Gradient Norm: 34.30109260596079
Epoch: 3819, Batch Gradient Norm after: 22.36067791116828
Epoch 3820/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.9370610833168029
Epoch: 3820, Batch Gradient Norm: 32.579026776653244
Epoch: 3820, Batch Gradient Norm after: 22.36067721142129
Epoch 3821/10000, Prediction Accuracy = 57.742%, Loss = 0.9325744152069092
Epoch: 3821, Batch Gradient Norm: 34.298183251493775
Epoch: 3821, Batch Gradient Norm after: 22.360677172472364
Epoch 3822/10000, Prediction Accuracy = 57.779999999999994%, Loss = 0.9368482947349548
Epoch: 3822, Batch Gradient Norm: 32.56921140616028
Epoch: 3822, Batch Gradient Norm after: 22.360677890768226
Epoch 3823/10000, Prediction Accuracy = 57.748000000000005%, Loss = 0.9323430538177491
Epoch: 3823, Batch Gradient Norm: 34.2924741743915
Epoch: 3823, Batch Gradient Norm after: 22.360677220776154
Epoch 3824/10000, Prediction Accuracy = 57.774%, Loss = 0.9366222381591797
Epoch: 3824, Batch Gradient Norm: 32.56154468243861
Epoch: 3824, Batch Gradient Norm after: 22.360675796024132
Epoch 3825/10000, Prediction Accuracy = 57.762%, Loss = 0.9321170330047608
Epoch: 3825, Batch Gradient Norm: 34.2861857088698
Epoch: 3825, Batch Gradient Norm after: 22.36067774057007
Epoch 3826/10000, Prediction Accuracy = 57.788%, Loss = 0.9363929510116578
Epoch: 3826, Batch Gradient Norm: 32.55374208646087
Epoch: 3826, Batch Gradient Norm after: 22.360676889591705
Epoch 3827/10000, Prediction Accuracy = 57.772000000000006%, Loss = 0.9318920969963074
Epoch: 3827, Batch Gradient Norm: 34.281250607845415
Epoch: 3827, Batch Gradient Norm after: 22.360677482365105
Epoch 3828/10000, Prediction Accuracy = 57.794000000000004%, Loss = 0.9361809849739074
Epoch: 3828, Batch Gradient Norm: 32.544650421726296
Epoch: 3828, Batch Gradient Norm after: 22.360678216020467
Epoch 3829/10000, Prediction Accuracy = 57.77199999999999%, Loss = 0.9316642999649047
Epoch: 3829, Batch Gradient Norm: 34.278521569573854
Epoch: 3829, Batch Gradient Norm after: 22.360675931532978
Epoch 3830/10000, Prediction Accuracy = 57.788%, Loss = 0.9359753012657166
Epoch: 3830, Batch Gradient Norm: 32.535821964653806
Epoch: 3830, Batch Gradient Norm after: 22.36067978360784
Epoch 3831/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.931436812877655
Epoch: 3831, Batch Gradient Norm: 34.27597322636043
Epoch: 3831, Batch Gradient Norm after: 22.360677483264823
Epoch 3832/10000, Prediction Accuracy = 57.788%, Loss = 0.93576021194458
Epoch: 3832, Batch Gradient Norm: 32.524201325268535
Epoch: 3832, Batch Gradient Norm after: 22.360677403374734
Epoch 3833/10000, Prediction Accuracy = 57.784000000000006%, Loss = 0.9312017560005188
Epoch: 3833, Batch Gradient Norm: 34.27769814800119
Epoch: 3833, Batch Gradient Norm after: 22.36067577032048
Epoch 3834/10000, Prediction Accuracy = 57.791999999999994%, Loss = 0.9355677008628845
Epoch: 3834, Batch Gradient Norm: 32.51346803488855
Epoch: 3834, Batch Gradient Norm after: 22.36067741681304
Epoch 3835/10000, Prediction Accuracy = 57.794%, Loss = 0.9309745669364929
Epoch: 3835, Batch Gradient Norm: 34.27954098554448
Epoch: 3835, Batch Gradient Norm after: 22.360678011435276
Epoch 3836/10000, Prediction Accuracy = 57.802%, Loss = 0.9353732705116272
Epoch: 3836, Batch Gradient Norm: 32.50555165949928
Epoch: 3836, Batch Gradient Norm after: 22.360678124222044
Epoch 3837/10000, Prediction Accuracy = 57.79600000000001%, Loss = 0.9307453751564025
Epoch: 3837, Batch Gradient Norm: 34.28072981477952
Epoch: 3837, Batch Gradient Norm after: 22.360676945101492
Epoch 3838/10000, Prediction Accuracy = 57.79200000000001%, Loss = 0.9351678848266601
Epoch: 3838, Batch Gradient Norm: 32.49497173686994
Epoch: 3838, Batch Gradient Norm after: 22.360678119212892
Epoch 3839/10000, Prediction Accuracy = 57.79600000000001%, Loss = 0.9305172324180603
Epoch: 3839, Batch Gradient Norm: 34.281552367936875
Epoch: 3839, Batch Gradient Norm after: 22.360676919619095
Epoch 3840/10000, Prediction Accuracy = 57.79799999999999%, Loss = 0.9349591135978699
Epoch: 3840, Batch Gradient Norm: 32.48688180056255
Epoch: 3840, Batch Gradient Norm after: 22.3606761468305
Epoch 3841/10000, Prediction Accuracy = 57.803999999999995%, Loss = 0.9302879691123962
Epoch: 3841, Batch Gradient Norm: 34.277618890809016
Epoch: 3841, Batch Gradient Norm after: 22.360678129496076
Epoch 3842/10000, Prediction Accuracy = 57.8%, Loss = 0.934749448299408
Epoch: 3842, Batch Gradient Norm: 32.477688619763086
Epoch: 3842, Batch Gradient Norm after: 22.360678226461946
Epoch 3843/10000, Prediction Accuracy = 57.80800000000001%, Loss = 0.9300638794898987
Epoch: 3843, Batch Gradient Norm: 34.27306432397078
Epoch: 3843, Batch Gradient Norm after: 22.360679005447004
Epoch 3844/10000, Prediction Accuracy = 57.802%, Loss = 0.9345370769500733
Epoch: 3844, Batch Gradient Norm: 32.471017090002285
Epoch: 3844, Batch Gradient Norm after: 22.360676762176233
Epoch 3845/10000, Prediction Accuracy = 57.818000000000005%, Loss = 0.9298362851142883
Epoch: 3845, Batch Gradient Norm: 34.26892694696419
Epoch: 3845, Batch Gradient Norm after: 22.360677006925723
Epoch 3846/10000, Prediction Accuracy = 57.80400000000001%, Loss = 0.9343126654624939
Epoch: 3846, Batch Gradient Norm: 32.46396448842786
Epoch: 3846, Batch Gradient Norm after: 22.360676094097656
Epoch 3847/10000, Prediction Accuracy = 57.82000000000001%, Loss = 0.9296110153198243
Epoch: 3847, Batch Gradient Norm: 34.26310670943144
Epoch: 3847, Batch Gradient Norm after: 22.360678558234362
Epoch 3848/10000, Prediction Accuracy = 57.824%, Loss = 0.9340931534767151
Epoch: 3848, Batch Gradient Norm: 32.45577778287206
Epoch: 3848, Batch Gradient Norm after: 22.360679348619787
Epoch 3849/10000, Prediction Accuracy = 57.818%, Loss = 0.9293879985809326
Epoch: 3849, Batch Gradient Norm: 34.258078981116846
Epoch: 3849, Batch Gradient Norm after: 22.360677985187877
Epoch 3850/10000, Prediction Accuracy = 57.826%, Loss = 0.9338722348213195
Epoch: 3850, Batch Gradient Norm: 32.44955825549326
Epoch: 3850, Batch Gradient Norm after: 22.360679273845868
Epoch 3851/10000, Prediction Accuracy = 57.822%, Loss = 0.929166603088379
Epoch: 3851, Batch Gradient Norm: 34.244511014810804
Epoch: 3851, Batch Gradient Norm after: 22.36067627585092
Epoch 3852/10000, Prediction Accuracy = 57.834%, Loss = 0.9336348533630371
Epoch: 3852, Batch Gradient Norm: 32.446674187965804
Epoch: 3852, Batch Gradient Norm after: 22.360677746055995
Epoch 3853/10000, Prediction Accuracy = 57.824%, Loss = 0.928946030139923
Epoch: 3853, Batch Gradient Norm: 34.23236360082738
Epoch: 3853, Batch Gradient Norm after: 22.360676399177482
Epoch 3854/10000, Prediction Accuracy = 57.838%, Loss = 0.9333941459655761
Epoch: 3854, Batch Gradient Norm: 32.440729105486234
Epoch: 3854, Batch Gradient Norm after: 22.360679357591327
Epoch 3855/10000, Prediction Accuracy = 57.827999999999996%, Loss = 0.9287269830703735
Epoch: 3855, Batch Gradient Norm: 34.21716088533427
Epoch: 3855, Batch Gradient Norm after: 22.360679233545298
Epoch 3856/10000, Prediction Accuracy = 57.843999999999994%, Loss = 0.9331474661827087
Epoch: 3856, Batch Gradient Norm: 32.43571244459996
Epoch: 3856, Batch Gradient Norm after: 22.36067980344098
Epoch 3857/10000, Prediction Accuracy = 57.834%, Loss = 0.9285106182098388
Epoch: 3857, Batch Gradient Norm: 34.20042842127203
Epoch: 3857, Batch Gradient Norm after: 22.360679680614915
Epoch 3858/10000, Prediction Accuracy = 57.855999999999995%, Loss = 0.9329048752784729
Epoch: 3858, Batch Gradient Norm: 32.4331727479589
Epoch: 3858, Batch Gradient Norm after: 22.360680282636096
Epoch 3859/10000, Prediction Accuracy = 57.83%, Loss = 0.9282967686653137
Epoch: 3859, Batch Gradient Norm: 34.188307704570306
Epoch: 3859, Batch Gradient Norm after: 22.360678999006232
Epoch 3860/10000, Prediction Accuracy = 57.85999999999999%, Loss = 0.9326658964157104
Epoch: 3860, Batch Gradient Norm: 32.4277816286122
Epoch: 3860, Batch Gradient Norm after: 22.360678741154
Epoch 3861/10000, Prediction Accuracy = 57.826%, Loss = 0.9280764579772949
Epoch: 3861, Batch Gradient Norm: 34.18063694120722
Epoch: 3861, Batch Gradient Norm after: 22.36068047177661
Epoch 3862/10000, Prediction Accuracy = 57.862%, Loss = 0.9324418783187867
Epoch: 3862, Batch Gradient Norm: 32.421170391067164
Epoch: 3862, Batch Gradient Norm after: 22.360680593132773
Epoch 3863/10000, Prediction Accuracy = 57.82800000000001%, Loss = 0.9278585910797119
Epoch: 3863, Batch Gradient Norm: 34.17645910285963
Epoch: 3863, Batch Gradient Norm after: 22.3606777757087
Epoch 3864/10000, Prediction Accuracy = 57.86800000000001%, Loss = 0.9322291851043701
Epoch: 3864, Batch Gradient Norm: 32.41453170130852
Epoch: 3864, Batch Gradient Norm after: 22.360677604578413
Epoch 3865/10000, Prediction Accuracy = 57.83399999999999%, Loss = 0.9276349782943726
Epoch: 3865, Batch Gradient Norm: 34.16835005980474
Epoch: 3865, Batch Gradient Norm after: 22.36067855611876
Epoch 3866/10000, Prediction Accuracy = 57.86%, Loss = 0.9320087552070617
Epoch: 3866, Batch Gradient Norm: 32.40754835955898
Epoch: 3866, Batch Gradient Norm after: 22.360678885707422
Epoch 3867/10000, Prediction Accuracy = 57.83200000000001%, Loss = 0.927416718006134
Epoch: 3867, Batch Gradient Norm: 34.158346982396296
Epoch: 3867, Batch Gradient Norm after: 22.360679639241724
Epoch 3868/10000, Prediction Accuracy = 57.862%, Loss = 0.9317862391471863
Epoch: 3868, Batch Gradient Norm: 32.40125115186901
Epoch: 3868, Batch Gradient Norm after: 22.360681744159248
Epoch 3869/10000, Prediction Accuracy = 57.838%, Loss = 0.92719886302948
Epoch: 3869, Batch Gradient Norm: 34.14624446904062
Epoch: 3869, Batch Gradient Norm after: 22.360678174997457
Epoch 3870/10000, Prediction Accuracy = 57.876%, Loss = 0.9315522193908692
Epoch: 3870, Batch Gradient Norm: 32.39473936790294
Epoch: 3870, Batch Gradient Norm after: 22.360680161523774
Epoch 3871/10000, Prediction Accuracy = 57.836%, Loss = 0.9269837021827698
Epoch: 3871, Batch Gradient Norm: 34.13194677227656
Epoch: 3871, Batch Gradient Norm after: 22.36067924706078
Epoch 3872/10000, Prediction Accuracy = 57.882000000000005%, Loss = 0.9313220262527466
Epoch: 3872, Batch Gradient Norm: 32.38746098912988
Epoch: 3872, Batch Gradient Norm after: 22.36067823645563
Epoch 3873/10000, Prediction Accuracy = 57.84599999999999%, Loss = 0.9267660260200501
Epoch: 3873, Batch Gradient Norm: 34.1191551439655
Epoch: 3873, Batch Gradient Norm after: 22.360676131924766
Epoch 3874/10000, Prediction Accuracy = 57.891999999999996%, Loss = 0.9310812473297119
Epoch: 3874, Batch Gradient Norm: 32.38364537133923
Epoch: 3874, Batch Gradient Norm after: 22.36067892454216
Epoch 3875/10000, Prediction Accuracy = 57.855999999999995%, Loss = 0.9265520930290222
Epoch: 3875, Batch Gradient Norm: 34.10316007263118
Epoch: 3875, Batch Gradient Norm after: 22.360677116691
Epoch 3876/10000, Prediction Accuracy = 57.896%, Loss = 0.93083735704422
Epoch: 3876, Batch Gradient Norm: 32.377987579279655
Epoch: 3876, Batch Gradient Norm after: 22.360678692073442
Epoch 3877/10000, Prediction Accuracy = 57.854%, Loss = 0.9263403058052063
Epoch: 3877, Batch Gradient Norm: 34.0913596725478
Epoch: 3877, Batch Gradient Norm after: 22.360676531270183
Epoch 3878/10000, Prediction Accuracy = 57.903999999999996%, Loss = 0.9305978775024414
Epoch: 3878, Batch Gradient Norm: 32.37318874764109
Epoch: 3878, Batch Gradient Norm after: 22.36067753126585
Epoch 3879/10000, Prediction Accuracy = 57.86600000000001%, Loss = 0.9261265516281127
Epoch: 3879, Batch Gradient Norm: 34.075974660605254
Epoch: 3879, Batch Gradient Norm after: 22.360675406687122
Epoch 3880/10000, Prediction Accuracy = 57.912%, Loss = 0.9303592324256897
Epoch: 3880, Batch Gradient Norm: 32.36761134940166
Epoch: 3880, Batch Gradient Norm after: 22.3606776908558
Epoch 3881/10000, Prediction Accuracy = 57.862%, Loss = 0.9259145855903625
Epoch: 3881, Batch Gradient Norm: 34.05973447371606
Epoch: 3881, Batch Gradient Norm after: 22.36067846837525
Epoch 3882/10000, Prediction Accuracy = 57.912%, Loss = 0.930114483833313
Epoch: 3882, Batch Gradient Norm: 32.36268577319655
Epoch: 3882, Batch Gradient Norm after: 22.360679567346462
Epoch 3883/10000, Prediction Accuracy = 57.86%, Loss = 0.9257009744644165
Epoch: 3883, Batch Gradient Norm: 34.042961633545026
Epoch: 3883, Batch Gradient Norm after: 22.36067752875988
Epoch 3884/10000, Prediction Accuracy = 57.91600000000001%, Loss = 0.929872477054596
Epoch: 3884, Batch Gradient Norm: 32.358832649174374
Epoch: 3884, Batch Gradient Norm after: 22.360680343681445
Epoch 3885/10000, Prediction Accuracy = 57.862%, Loss = 0.925484013557434
Epoch: 3885, Batch Gradient Norm: 34.02737775634451
Epoch: 3885, Batch Gradient Norm after: 22.36067802408888
Epoch 3886/10000, Prediction Accuracy = 57.922000000000004%, Loss = 0.9296292781829834
Epoch: 3886, Batch Gradient Norm: 32.352095926287205
Epoch: 3886, Batch Gradient Norm after: 22.360678824606218
Epoch 3887/10000, Prediction Accuracy = 57.867999999999995%, Loss = 0.9252725601196289
Epoch: 3887, Batch Gradient Norm: 34.014993267067496
Epoch: 3887, Batch Gradient Norm after: 22.36067947295322
Epoch 3888/10000, Prediction Accuracy = 57.932%, Loss = 0.9293987989425659
Epoch: 3888, Batch Gradient Norm: 32.34797493914559
Epoch: 3888, Batch Gradient Norm after: 22.360679209960338
Epoch 3889/10000, Prediction Accuracy = 57.876%, Loss = 0.9250598311424255
Epoch: 3889, Batch Gradient Norm: 33.9991085786133
Epoch: 3889, Batch Gradient Norm after: 22.360677448125564
Epoch 3890/10000, Prediction Accuracy = 57.934000000000005%, Loss = 0.9291638135910034
Epoch: 3890, Batch Gradient Norm: 32.34283220457268
Epoch: 3890, Batch Gradient Norm after: 22.360678941851624
Epoch 3891/10000, Prediction Accuracy = 57.874%, Loss = 0.9248448371887207
Epoch: 3891, Batch Gradient Norm: 33.98478575454228
Epoch: 3891, Batch Gradient Norm after: 22.36067956935275
Epoch 3892/10000, Prediction Accuracy = 57.94200000000001%, Loss = 0.9289275169372558
Epoch: 3892, Batch Gradient Norm: 32.33848918220975
Epoch: 3892, Batch Gradient Norm after: 22.36067741838581
Epoch 3893/10000, Prediction Accuracy = 57.891999999999996%, Loss = 0.9246310830116272
Epoch: 3893, Batch Gradient Norm: 33.9745448300502
Epoch: 3893, Batch Gradient Norm after: 22.36068166891178
Epoch 3894/10000, Prediction Accuracy = 57.943999999999996%, Loss = 0.9286929249763489
Epoch: 3894, Batch Gradient Norm: 32.33594153811197
Epoch: 3894, Batch Gradient Norm after: 22.36067958072991
Epoch 3895/10000, Prediction Accuracy = 57.896%, Loss = 0.9244199872016907
Epoch: 3895, Batch Gradient Norm: 33.95847739739078
Epoch: 3895, Batch Gradient Norm after: 22.360679290035456
Epoch 3896/10000, Prediction Accuracy = 57.956%, Loss = 0.928454577922821
Epoch: 3896, Batch Gradient Norm: 32.32804414438944
Epoch: 3896, Batch Gradient Norm after: 22.36068012766227
Epoch 3897/10000, Prediction Accuracy = 57.906000000000006%, Loss = 0.9242078304290772
Epoch: 3897, Batch Gradient Norm: 33.94886541040902
Epoch: 3897, Batch Gradient Norm after: 22.36067753450433
Epoch 3898/10000, Prediction Accuracy = 57.95399999999999%, Loss = 0.9282271862030029
Epoch: 3898, Batch Gradient Norm: 32.32549593667435
Epoch: 3898, Batch Gradient Norm after: 22.360678237935545
Epoch 3899/10000, Prediction Accuracy = 57.916%, Loss = 0.9239932537078858
Epoch: 3899, Batch Gradient Norm: 33.93372470778761
Epoch: 3899, Batch Gradient Norm after: 22.360677343826787
Epoch 3900/10000, Prediction Accuracy = 57.956%, Loss = 0.9279943585395813
Epoch: 3900, Batch Gradient Norm: 32.31847032143877
Epoch: 3900, Batch Gradient Norm after: 22.360678040382595
Epoch 3901/10000, Prediction Accuracy = 57.922000000000004%, Loss = 0.9237755298614502
Epoch: 3901, Batch Gradient Norm: 33.92533460135004
Epoch: 3901, Batch Gradient Norm after: 22.360679028238632
Epoch 3902/10000, Prediction Accuracy = 57.962%, Loss = 0.9277756571769714
Epoch: 3902, Batch Gradient Norm: 32.31278465251221
Epoch: 3902, Batch Gradient Norm after: 22.36067845402313
Epoch 3903/10000, Prediction Accuracy = 57.922000000000004%, Loss = 0.9235639333724975
Epoch: 3903, Batch Gradient Norm: 33.913898780909406
Epoch: 3903, Batch Gradient Norm after: 22.360679611017094
Epoch 3904/10000, Prediction Accuracy = 57.972%, Loss = 0.9275501728057861
Epoch: 3904, Batch Gradient Norm: 32.30667024494979
Epoch: 3904, Batch Gradient Norm after: 22.36067828673744
Epoch 3905/10000, Prediction Accuracy = 57.93599999999999%, Loss = 0.9233532667160034
Epoch: 3905, Batch Gradient Norm: 33.89797113103564
Epoch: 3905, Batch Gradient Norm after: 22.36067901404486
Epoch 3906/10000, Prediction Accuracy = 57.98199999999999%, Loss = 0.9273164987564086
Epoch: 3906, Batch Gradient Norm: 32.30179022538187
Epoch: 3906, Batch Gradient Norm after: 22.36068003959092
Epoch 3907/10000, Prediction Accuracy = 57.93399999999999%, Loss = 0.9231449842453003
Epoch: 3907, Batch Gradient Norm: 33.883880608729186
Epoch: 3907, Batch Gradient Norm after: 22.360679176268427
Epoch 3908/10000, Prediction Accuracy = 57.976%, Loss = 0.9270783185958862
Epoch: 3908, Batch Gradient Norm: 32.29707110101341
Epoch: 3908, Batch Gradient Norm after: 22.360680064325013
Epoch 3909/10000, Prediction Accuracy = 57.946000000000005%, Loss = 0.9229357481002808
Epoch: 3909, Batch Gradient Norm: 33.87013931684194
Epoch: 3909, Batch Gradient Norm after: 22.36068068447296
Epoch 3910/10000, Prediction Accuracy = 57.984%, Loss = 0.9268398284912109
Epoch: 3910, Batch Gradient Norm: 32.29247783822197
Epoch: 3910, Batch Gradient Norm after: 22.360680101574314
Epoch 3911/10000, Prediction Accuracy = 57.952%, Loss = 0.9227259516716003
Epoch: 3911, Batch Gradient Norm: 33.85811869370847
Epoch: 3911, Batch Gradient Norm after: 22.360679457599872
Epoch 3912/10000, Prediction Accuracy = 57.988%, Loss = 0.9266122102737426
Epoch: 3912, Batch Gradient Norm: 32.28573512761273
Epoch: 3912, Batch Gradient Norm after: 22.36068074153243
Epoch 3913/10000, Prediction Accuracy = 57.95%, Loss = 0.9225122928619385
Epoch: 3913, Batch Gradient Norm: 33.845353968110075
Epoch: 3913, Batch Gradient Norm after: 22.360681064237234
Epoch 3914/10000, Prediction Accuracy = 57.988%, Loss = 0.9263824582099914
Epoch: 3914, Batch Gradient Norm: 32.281513039770886
Epoch: 3914, Batch Gradient Norm after: 22.36067758958929
Epoch 3915/10000, Prediction Accuracy = 57.952%, Loss = 0.9223031520843505
Epoch: 3915, Batch Gradient Norm: 33.83190700252166
Epoch: 3915, Batch Gradient Norm after: 22.36067841142386
Epoch 3916/10000, Prediction Accuracy = 57.989999999999995%, Loss = 0.926148521900177
Epoch: 3916, Batch Gradient Norm: 32.27664403366097
Epoch: 3916, Batch Gradient Norm after: 22.36067648257729
Epoch 3917/10000, Prediction Accuracy = 57.96%, Loss = 0.9220950961112976
Epoch: 3917, Batch Gradient Norm: 33.816742982013075
Epoch: 3917, Batch Gradient Norm after: 22.360679380829062
Epoch 3918/10000, Prediction Accuracy = 58.001999999999995%, Loss = 0.9259000897407532
Epoch: 3918, Batch Gradient Norm: 32.27554768373657
Epoch: 3918, Batch Gradient Norm after: 22.360677534760594
Epoch 3919/10000, Prediction Accuracy = 57.95799999999999%, Loss = 0.9218829989433288
Epoch: 3919, Batch Gradient Norm: 33.8003957640276
Epoch: 3919, Batch Gradient Norm after: 22.36067740326982
Epoch 3920/10000, Prediction Accuracy = 58.004%, Loss = 0.9256585359573364
Epoch: 3920, Batch Gradient Norm: 32.27093437430457
Epoch: 3920, Batch Gradient Norm after: 22.360678059370866
Epoch 3921/10000, Prediction Accuracy = 57.970000000000006%, Loss = 0.9216817498207093
Epoch: 3921, Batch Gradient Norm: 33.78248021656557
Epoch: 3921, Batch Gradient Norm after: 22.360677278075165
Epoch 3922/10000, Prediction Accuracy = 58.010000000000005%, Loss = 0.9254136323928833
Epoch: 3922, Batch Gradient Norm: 32.26674577246625
Epoch: 3922, Batch Gradient Norm after: 22.360679371413244
Epoch 3923/10000, Prediction Accuracy = 57.974000000000004%, Loss = 0.9214769959449768
Epoch: 3923, Batch Gradient Norm: 33.764148873491614
Epoch: 3923, Batch Gradient Norm after: 22.360675891450775
Epoch 3924/10000, Prediction Accuracy = 58.008%, Loss = 0.925165855884552
Epoch: 3924, Batch Gradient Norm: 32.26502773362409
Epoch: 3924, Batch Gradient Norm after: 22.36067818072534
Epoch 3925/10000, Prediction Accuracy = 57.976%, Loss = 0.9212695240974427
Epoch: 3925, Batch Gradient Norm: 33.74917753930867
Epoch: 3925, Batch Gradient Norm after: 22.36067870233705
Epoch 3926/10000, Prediction Accuracy = 58.00599999999999%, Loss = 0.9249286651611328
Epoch: 3926, Batch Gradient Norm: 32.26206441796252
Epoch: 3926, Batch Gradient Norm after: 22.36067868781598
Epoch 3927/10000, Prediction Accuracy = 57.977999999999994%, Loss = 0.9210628986358642
Epoch: 3927, Batch Gradient Norm: 33.7381579877846
Epoch: 3927, Batch Gradient Norm after: 22.36067779767446
Epoch 3928/10000, Prediction Accuracy = 58.007999999999996%, Loss = 0.9246972680091858
Epoch: 3928, Batch Gradient Norm: 32.254687616511525
Epoch: 3928, Batch Gradient Norm after: 22.360678364153998
Epoch 3929/10000, Prediction Accuracy = 57.983999999999995%, Loss = 0.9208564162254333
Epoch: 3929, Batch Gradient Norm: 33.72118586916408
Epoch: 3929, Batch Gradient Norm after: 22.36067788055266
Epoch 3930/10000, Prediction Accuracy = 58.008%, Loss = 0.9244622588157654
Epoch: 3930, Batch Gradient Norm: 32.2492288761579
Epoch: 3930, Batch Gradient Norm after: 22.360678110819315
Epoch 3931/10000, Prediction Accuracy = 57.982000000000006%, Loss = 0.9206506848335266
Epoch: 3931, Batch Gradient Norm: 33.70397355653297
Epoch: 3931, Batch Gradient Norm after: 22.36067905234194
Epoch 3932/10000, Prediction Accuracy = 58.013999999999996%, Loss = 0.9242252230644226
Epoch: 3932, Batch Gradient Norm: 32.24651470895493
Epoch: 3932, Batch Gradient Norm after: 22.3606775318988
Epoch 3933/10000, Prediction Accuracy = 57.982000000000006%, Loss = 0.9204476714134217
Epoch: 3933, Batch Gradient Norm: 33.6902204841686
Epoch: 3933, Batch Gradient Norm after: 22.360678973251254
Epoch 3934/10000, Prediction Accuracy = 58.01800000000001%, Loss = 0.9239933490753174
Epoch: 3934, Batch Gradient Norm: 32.240643603064484
Epoch: 3934, Batch Gradient Norm after: 22.36067609019754
Epoch 3935/10000, Prediction Accuracy = 57.98%, Loss = 0.9202440023422241
Epoch: 3935, Batch Gradient Norm: 33.673316525666465
Epoch: 3935, Batch Gradient Norm after: 22.36067935613633
Epoch 3936/10000, Prediction Accuracy = 58.022000000000006%, Loss = 0.9237659931182861
Epoch: 3936, Batch Gradient Norm: 32.2348362481673
Epoch: 3936, Batch Gradient Norm after: 22.360676431801107
Epoch 3937/10000, Prediction Accuracy = 57.98199999999999%, Loss = 0.9200342059135437
Epoch: 3937, Batch Gradient Norm: 33.65658446358112
Epoch: 3937, Batch Gradient Norm after: 22.360680478165538
Epoch 3938/10000, Prediction Accuracy = 58.032%, Loss = 0.9235298871994019
Epoch: 3938, Batch Gradient Norm: 32.233494951231414
Epoch: 3938, Batch Gradient Norm after: 22.360678788394075
Epoch 3939/10000, Prediction Accuracy = 57.98%, Loss = 0.9198242902755738
Epoch: 3939, Batch Gradient Norm: 33.64111502257577
Epoch: 3939, Batch Gradient Norm after: 22.360676704555026
Epoch 3940/10000, Prediction Accuracy = 58.028%, Loss = 0.9232940316200257
Epoch: 3940, Batch Gradient Norm: 32.22874275852783
Epoch: 3940, Batch Gradient Norm after: 22.360679143888945
Epoch 3941/10000, Prediction Accuracy = 57.98599999999999%, Loss = 0.919618546962738
Epoch: 3941, Batch Gradient Norm: 33.63036948287219
Epoch: 3941, Batch Gradient Norm after: 22.36068002809267
Epoch 3942/10000, Prediction Accuracy = 58.022000000000006%, Loss = 0.9230710744857789
Epoch: 3942, Batch Gradient Norm: 32.220715455549126
Epoch: 3942, Batch Gradient Norm after: 22.360678432684974
Epoch 3943/10000, Prediction Accuracy = 57.992%, Loss = 0.9194057941436767
Epoch: 3943, Batch Gradient Norm: 33.62348659484691
Epoch: 3943, Batch Gradient Norm after: 22.3606788252912
Epoch 3944/10000, Prediction Accuracy = 58.024%, Loss = 0.9228608846664429
Epoch: 3944, Batch Gradient Norm: 32.212780109100414
Epoch: 3944, Batch Gradient Norm after: 22.360679591086903
Epoch 3945/10000, Prediction Accuracy = 57.99400000000001%, Loss = 0.9191971898078919
Epoch: 3945, Batch Gradient Norm: 33.61515895796189
Epoch: 3945, Batch Gradient Norm after: 22.360677809350115
Epoch 3946/10000, Prediction Accuracy = 58.01800000000001%, Loss = 0.922646963596344
Epoch: 3946, Batch Gradient Norm: 32.20835697506957
Epoch: 3946, Batch Gradient Norm after: 22.360678939523762
Epoch 3947/10000, Prediction Accuracy = 58.0%, Loss = 0.9189883232116699
Epoch: 3947, Batch Gradient Norm: 33.601657508526564
Epoch: 3947, Batch Gradient Norm after: 22.36067807813031
Epoch 3948/10000, Prediction Accuracy = 58.025999999999996%, Loss = 0.9224193334579468
Epoch: 3948, Batch Gradient Norm: 32.202950127576706
Epoch: 3948, Batch Gradient Norm after: 22.36067822979847
Epoch 3949/10000, Prediction Accuracy = 57.99399999999999%, Loss = 0.9187786817550659
Epoch: 3949, Batch Gradient Norm: 33.59106609383251
Epoch: 3949, Batch Gradient Norm after: 22.360677394550514
Epoch 3950/10000, Prediction Accuracy = 58.025999999999996%, Loss = 0.9222054243087768
Epoch: 3950, Batch Gradient Norm: 32.195443261441035
Epoch: 3950, Batch Gradient Norm after: 22.3606765342725
Epoch 3951/10000, Prediction Accuracy = 58.0%, Loss = 0.9185694098472595
Epoch: 3951, Batch Gradient Norm: 33.58253362876262
Epoch: 3951, Batch Gradient Norm after: 22.360677880501438
Epoch 3952/10000, Prediction Accuracy = 58.024%, Loss = 0.9219941854476928
Epoch: 3952, Batch Gradient Norm: 32.18718199700132
Epoch: 3952, Batch Gradient Norm after: 22.360678260904947
Epoch 3953/10000, Prediction Accuracy = 58.004000000000005%, Loss = 0.9183604478836059
Epoch: 3953, Batch Gradient Norm: 33.57392140164919
Epoch: 3953, Batch Gradient Norm after: 22.36067983760534
Epoch 3954/10000, Prediction Accuracy = 58.036%, Loss = 0.921773362159729
Epoch: 3954, Batch Gradient Norm: 32.180506254102156
Epoch: 3954, Batch Gradient Norm after: 22.360676207689252
Epoch 3955/10000, Prediction Accuracy = 58.012%, Loss = 0.9181522488594055
Epoch: 3955, Batch Gradient Norm: 33.56764497999398
Epoch: 3955, Batch Gradient Norm after: 22.360679407881758
Epoch 3956/10000, Prediction Accuracy = 58.036%, Loss = 0.9215662121772766
Epoch: 3956, Batch Gradient Norm: 32.1725670658564
Epoch: 3956, Batch Gradient Norm after: 22.360677398604718
Epoch 3957/10000, Prediction Accuracy = 58.01800000000001%, Loss = 0.9179408431053162
Epoch: 3957, Batch Gradient Norm: 33.558245535994025
Epoch: 3957, Batch Gradient Norm after: 22.36068017998451
Epoch 3958/10000, Prediction Accuracy = 58.04%, Loss = 0.921358847618103
Epoch: 3958, Batch Gradient Norm: 32.16435587628411
Epoch: 3958, Batch Gradient Norm after: 22.3606769675019
Epoch 3959/10000, Prediction Accuracy = 58.028%, Loss = 0.917733633518219
Epoch: 3959, Batch Gradient Norm: 33.550674631852644
Epoch: 3959, Batch Gradient Norm after: 22.360679820372017
Epoch 3960/10000, Prediction Accuracy = 58.032%, Loss = 0.9211469650268554
Epoch: 3960, Batch Gradient Norm: 32.157619099877934
Epoch: 3960, Batch Gradient Norm after: 22.36067873394157
Epoch 3961/10000, Prediction Accuracy = 58.036%, Loss = 0.9175304889678955
Epoch: 3961, Batch Gradient Norm: 33.537787253903275
Epoch: 3961, Batch Gradient Norm after: 22.360677740317136
Epoch 3962/10000, Prediction Accuracy = 58.038%, Loss = 0.9209227323532104
Epoch: 3962, Batch Gradient Norm: 32.152873291586076
Epoch: 3962, Batch Gradient Norm after: 22.36067807815476
Epoch 3963/10000, Prediction Accuracy = 58.04%, Loss = 0.917318832874298
Epoch: 3963, Batch Gradient Norm: 33.52525517591412
Epoch: 3963, Batch Gradient Norm after: 22.360677431636645
Epoch 3964/10000, Prediction Accuracy = 58.044%, Loss = 0.9207006931304932
Epoch: 3964, Batch Gradient Norm: 32.14535004150214
Epoch: 3964, Batch Gradient Norm after: 22.36067830924794
Epoch 3965/10000, Prediction Accuracy = 58.044%, Loss = 0.9171104907989502
Epoch: 3965, Batch Gradient Norm: 33.51522257259612
Epoch: 3965, Batch Gradient Norm after: 22.360679157147533
Epoch 3966/10000, Prediction Accuracy = 58.048%, Loss = 0.9204866051673889
Epoch: 3966, Batch Gradient Norm: 32.140381937789066
Epoch: 3966, Batch Gradient Norm after: 22.360679562497996
Epoch 3967/10000, Prediction Accuracy = 58.044%, Loss = 0.9169020891189575
Epoch: 3967, Batch Gradient Norm: 33.50520363743284
Epoch: 3967, Batch Gradient Norm after: 22.36067872898286
Epoch 3968/10000, Prediction Accuracy = 58.05%, Loss = 0.9202724933624268
Epoch: 3968, Batch Gradient Norm: 32.13187700879028
Epoch: 3968, Batch Gradient Norm after: 22.360676799603294
Epoch 3969/10000, Prediction Accuracy = 58.05%, Loss = 0.9166976332664489
Epoch: 3969, Batch Gradient Norm: 33.496793244749256
Epoch: 3969, Batch Gradient Norm after: 22.360677396911395
Epoch 3970/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.920052993297577
Epoch: 3970, Batch Gradient Norm: 32.123928366497694
Epoch: 3970, Batch Gradient Norm after: 22.360678331969215
Epoch 3971/10000, Prediction Accuracy = 58.05400000000001%, Loss = 0.916490089893341
Epoch: 3971, Batch Gradient Norm: 33.48783169040893
Epoch: 3971, Batch Gradient Norm after: 22.360679195599992
Epoch 3972/10000, Prediction Accuracy = 58.062%, Loss = 0.9198431134223938
Epoch: 3972, Batch Gradient Norm: 32.11555885640652
Epoch: 3972, Batch Gradient Norm after: 22.36067836076605
Epoch 3973/10000, Prediction Accuracy = 58.052%, Loss = 0.9162798166275025
Epoch: 3973, Batch Gradient Norm: 33.480693851759085
Epoch: 3973, Batch Gradient Norm after: 22.36067722037771
Epoch 3974/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.9196393609046936
Epoch: 3974, Batch Gradient Norm: 32.10783780890822
Epoch: 3974, Batch Gradient Norm after: 22.360678982544083
Epoch 3975/10000, Prediction Accuracy = 58.04599999999999%, Loss = 0.9160737991333008
Epoch: 3975, Batch Gradient Norm: 33.47475687328708
Epoch: 3975, Batch Gradient Norm after: 22.360677412751784
Epoch 3976/10000, Prediction Accuracy = 58.072%, Loss = 0.9194279313087463
Epoch: 3976, Batch Gradient Norm: 32.09868849688438
Epoch: 3976, Batch Gradient Norm after: 22.360677982992005
Epoch 3977/10000, Prediction Accuracy = 58.052%, Loss = 0.9158597469329834
Epoch: 3977, Batch Gradient Norm: 33.47309110855763
Epoch: 3977, Batch Gradient Norm after: 22.360676383965306
Epoch 3978/10000, Prediction Accuracy = 58.074%, Loss = 0.9192329287528992
Epoch: 3978, Batch Gradient Norm: 32.08933027899079
Epoch: 3978, Batch Gradient Norm after: 22.3606778019378
Epoch 3979/10000, Prediction Accuracy = 58.052%, Loss = 0.9156472325325012
Epoch: 3979, Batch Gradient Norm: 33.473149035213396
Epoch: 3979, Batch Gradient Norm after: 22.360676689793042
Epoch 3980/10000, Prediction Accuracy = 58.072%, Loss = 0.9190451502799988
Epoch: 3980, Batch Gradient Norm: 32.08131995300206
Epoch: 3980, Batch Gradient Norm after: 22.36067670751406
Epoch 3981/10000, Prediction Accuracy = 58.052%, Loss = 0.9154355764389038
Epoch: 3981, Batch Gradient Norm: 33.46865019105431
Epoch: 3981, Batch Gradient Norm after: 22.360678349755077
Epoch 3982/10000, Prediction Accuracy = 58.076%, Loss = 0.9188442587852478
Epoch: 3982, Batch Gradient Norm: 32.06969557916936
Epoch: 3982, Batch Gradient Norm after: 22.36067880217404
Epoch 3983/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.9152282834053039
Epoch: 3983, Batch Gradient Norm: 33.461787751486234
Epoch: 3983, Batch Gradient Norm after: 22.360673725073703
Epoch 3984/10000, Prediction Accuracy = 58.077999999999996%, Loss = 0.9186388611793518
Epoch: 3984, Batch Gradient Norm: 32.06398814118533
Epoch: 3984, Batch Gradient Norm after: 22.360676775565672
Epoch 3985/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.9150183320045471
Epoch: 3985, Batch Gradient Norm: 33.450300812140654
Epoch: 3985, Batch Gradient Norm after: 22.360674677251982
Epoch 3986/10000, Prediction Accuracy = 58.096000000000004%, Loss = 0.9184252500534058
Epoch: 3986, Batch Gradient Norm: 32.0562621181136
Epoch: 3986, Batch Gradient Norm after: 22.360677231642068
Epoch 3987/10000, Prediction Accuracy = 58.056000000000004%, Loss = 0.9148112654685974
Epoch: 3987, Batch Gradient Norm: 33.439291227814
Epoch: 3987, Batch Gradient Norm after: 22.360676657618832
Epoch 3988/10000, Prediction Accuracy = 58.10600000000001%, Loss = 0.918207848072052
Epoch: 3988, Batch Gradient Norm: 32.051745632690356
Epoch: 3988, Batch Gradient Norm after: 22.36067714900249
Epoch 3989/10000, Prediction Accuracy = 58.05%, Loss = 0.9146036982536316
Epoch: 3989, Batch Gradient Norm: 33.43044710489884
Epoch: 3989, Batch Gradient Norm after: 22.360676462108156
Epoch 3990/10000, Prediction Accuracy = 58.108000000000004%, Loss = 0.9179877042770386
Epoch: 3990, Batch Gradient Norm: 32.04764990915659
Epoch: 3990, Batch Gradient Norm after: 22.360678548898886
Epoch 3991/10000, Prediction Accuracy = 58.05800000000001%, Loss = 0.9144054412841797
Epoch: 3991, Batch Gradient Norm: 33.41765012333427
Epoch: 3991, Batch Gradient Norm after: 22.36067516191323
Epoch 3992/10000, Prediction Accuracy = 58.108000000000004%, Loss = 0.9177652359008789
Epoch: 3992, Batch Gradient Norm: 32.04269607205429
Epoch: 3992, Batch Gradient Norm after: 22.360676647878158
Epoch 3993/10000, Prediction Accuracy = 58.062%, Loss = 0.9141996502876282
Epoch: 3993, Batch Gradient Norm: 33.40757297442793
Epoch: 3993, Batch Gradient Norm after: 22.360677001625596
Epoch 3994/10000, Prediction Accuracy = 58.120000000000005%, Loss = 0.9175525069236755
Epoch: 3994, Batch Gradient Norm: 32.03432268213734
Epoch: 3994, Batch Gradient Norm after: 22.36067612723251
Epoch 3995/10000, Prediction Accuracy = 58.072%, Loss = 0.9139981746673584
Epoch: 3995, Batch Gradient Norm: 33.39713249091012
Epoch: 3995, Batch Gradient Norm after: 22.36067609960688
Epoch 3996/10000, Prediction Accuracy = 58.11%, Loss = 0.9173339366912842
Epoch: 3996, Batch Gradient Norm: 32.03090550776464
Epoch: 3996, Batch Gradient Norm after: 22.360677714554612
Epoch 3997/10000, Prediction Accuracy = 58.074%, Loss = 0.9137897253036499
Epoch: 3997, Batch Gradient Norm: 33.387076657210926
Epoch: 3997, Batch Gradient Norm after: 22.360676172623986
Epoch 3998/10000, Prediction Accuracy = 58.112%, Loss = 0.917123305797577
Epoch: 3998, Batch Gradient Norm: 32.02437198230582
Epoch: 3998, Batch Gradient Norm after: 22.36067841776783
Epoch 3999/10000, Prediction Accuracy = 58.076%, Loss = 0.9135915994644165
Epoch: 3999, Batch Gradient Norm: 33.38094707648085
Epoch: 3999, Batch Gradient Norm after: 22.360675760123762
Epoch 4000/10000, Prediction Accuracy = 58.117999999999995%, Loss = 0.9169152617454529
Epoch: 4000, Batch Gradient Norm: 32.01638986635716
Epoch: 4000, Batch Gradient Norm after: 22.3606789227035
Epoch 4001/10000, Prediction Accuracy = 58.08200000000001%, Loss = 0.913381814956665
Epoch: 4001, Batch Gradient Norm: 33.374366268937116
Epoch: 4001, Batch Gradient Norm after: 22.360676868594766
Epoch 4002/10000, Prediction Accuracy = 58.11800000000001%, Loss = 0.9167113661766052
Epoch: 4002, Batch Gradient Norm: 32.00913911153065
Epoch: 4002, Batch Gradient Norm after: 22.36067663870281
Epoch 4003/10000, Prediction Accuracy = 58.081999999999994%, Loss = 0.913180923461914
Epoch: 4003, Batch Gradient Norm: 33.365057968443416
Epoch: 4003, Batch Gradient Norm after: 22.3606766974106
Epoch 4004/10000, Prediction Accuracy = 58.116%, Loss = 0.9165079236030579
Epoch: 4004, Batch Gradient Norm: 32.00143500594069
Epoch: 4004, Batch Gradient Norm after: 22.360676588486896
Epoch 4005/10000, Prediction Accuracy = 58.08200000000001%, Loss = 0.912978184223175
Epoch: 4005, Batch Gradient Norm: 33.3542934205209
Epoch: 4005, Batch Gradient Norm after: 22.360675387859036
Epoch 4006/10000, Prediction Accuracy = 58.114%, Loss = 0.9162952184677124
Epoch: 4006, Batch Gradient Norm: 31.994651431951407
Epoch: 4006, Batch Gradient Norm after: 22.360679883890423
Epoch 4007/10000, Prediction Accuracy = 58.089999999999996%, Loss = 0.9127729058265686
Epoch: 4007, Batch Gradient Norm: 33.33923759228366
Epoch: 4007, Batch Gradient Norm after: 22.36067776362141
Epoch 4008/10000, Prediction Accuracy = 58.112%, Loss = 0.916071093082428
Epoch: 4008, Batch Gradient Norm: 31.990894783128713
Epoch: 4008, Batch Gradient Norm after: 22.360677197624735
Epoch 4009/10000, Prediction Accuracy = 58.102%, Loss = 0.9125741600990296
Epoch: 4009, Batch Gradient Norm: 33.330998314451534
Epoch: 4009, Batch Gradient Norm after: 22.360674917170318
Epoch 4010/10000, Prediction Accuracy = 58.120000000000005%, Loss = 0.915857481956482
Epoch: 4010, Batch Gradient Norm: 31.984703291458757
Epoch: 4010, Batch Gradient Norm after: 22.360676518583343
Epoch 4011/10000, Prediction Accuracy = 58.102%, Loss = 0.9123728632926941
Epoch: 4011, Batch Gradient Norm: 33.325146143435134
Epoch: 4011, Batch Gradient Norm after: 22.360677322224667
Epoch 4012/10000, Prediction Accuracy = 58.117999999999995%, Loss = 0.9156553268432617
Epoch: 4012, Batch Gradient Norm: 31.976064143102068
Epoch: 4012, Batch Gradient Norm after: 22.360677945432933
Epoch 4013/10000, Prediction Accuracy = 58.1%, Loss = 0.91216481924057
Epoch: 4013, Batch Gradient Norm: 33.32401489544326
Epoch: 4013, Batch Gradient Norm after: 22.360676552526993
Epoch 4014/10000, Prediction Accuracy = 58.12199999999999%, Loss = 0.9154731154441833
Epoch: 4014, Batch Gradient Norm: 31.965453524543417
Epoch: 4014, Batch Gradient Norm after: 22.360676468983325
Epoch 4015/10000, Prediction Accuracy = 58.1%, Loss = 0.9119578719139099
Epoch: 4015, Batch Gradient Norm: 33.318700755081274
Epoch: 4015, Batch Gradient Norm after: 22.36067728912269
Epoch 4016/10000, Prediction Accuracy = 58.11800000000001%, Loss = 0.9152729749679566
Epoch: 4016, Batch Gradient Norm: 31.956351877031548
Epoch: 4016, Batch Gradient Norm after: 22.360677729905643
Epoch 4017/10000, Prediction Accuracy = 58.104%, Loss = 0.91175217628479
Epoch: 4017, Batch Gradient Norm: 33.318829781116015
Epoch: 4017, Batch Gradient Norm after: 22.360674488610716
Epoch 4018/10000, Prediction Accuracy = 58.124%, Loss = 0.9150835633277893
Epoch: 4018, Batch Gradient Norm: 31.94513780059238
Epoch: 4018, Batch Gradient Norm after: 22.36068017289691
Epoch 4019/10000, Prediction Accuracy = 58.11%, Loss = 0.9115395903587341
Epoch: 4019, Batch Gradient Norm: 33.32125724412583
Epoch: 4019, Batch Gradient Norm after: 22.360675427761123
Epoch 4020/10000, Prediction Accuracy = 58.13000000000001%, Loss = 0.9149064421653748
Epoch: 4020, Batch Gradient Norm: 31.935791375090993
Epoch: 4020, Batch Gradient Norm after: 22.36067898332339
Epoch 4021/10000, Prediction Accuracy = 58.10999999999999%, Loss = 0.911331582069397
Epoch: 4021, Batch Gradient Norm: 33.32021153440997
Epoch: 4021, Batch Gradient Norm after: 22.36067596919328
Epoch 4022/10000, Prediction Accuracy = 58.122%, Loss = 0.9147114992141724
Epoch: 4022, Batch Gradient Norm: 31.92815924507018
Epoch: 4022, Batch Gradient Norm after: 22.360676362234067
Epoch 4023/10000, Prediction Accuracy = 58.112%, Loss = 0.9111224412918091
Epoch: 4023, Batch Gradient Norm: 33.3151673657054
Epoch: 4023, Batch Gradient Norm after: 22.36067506761344
Epoch 4024/10000, Prediction Accuracy = 58.124%, Loss = 0.9145149230957031
Epoch: 4024, Batch Gradient Norm: 31.920823203940383
Epoch: 4024, Batch Gradient Norm after: 22.360676216427212
Epoch 4025/10000, Prediction Accuracy = 58.11999999999999%, Loss = 0.9109178423881531
Epoch: 4025, Batch Gradient Norm: 33.31165744239577
Epoch: 4025, Batch Gradient Norm after: 22.36067724904897
Epoch 4026/10000, Prediction Accuracy = 58.122%, Loss = 0.9143168330192566
Epoch: 4026, Batch Gradient Norm: 31.9109094424277
Epoch: 4026, Batch Gradient Norm after: 22.360676568074936
Epoch 4027/10000, Prediction Accuracy = 58.122%, Loss = 0.9107077836990356
Epoch: 4027, Batch Gradient Norm: 33.319881460900405
Epoch: 4027, Batch Gradient Norm after: 22.360676064823455
Epoch 4028/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9141471743583679
Epoch: 4028, Batch Gradient Norm: 31.902263336997336
Epoch: 4028, Batch Gradient Norm after: 22.360678604545278
Epoch 4029/10000, Prediction Accuracy = 58.128%, Loss = 0.9104960560798645
Epoch: 4029, Batch Gradient Norm: 33.32051000062373
Epoch: 4029, Batch Gradient Norm after: 22.360678884364127
Epoch 4030/10000, Prediction Accuracy = 58.136%, Loss = 0.9139718651771546
Epoch: 4030, Batch Gradient Norm: 31.891760837937642
Epoch: 4030, Batch Gradient Norm after: 22.36067964949561
Epoch 4031/10000, Prediction Accuracy = 58.120000000000005%, Loss = 0.9102912306785583
Epoch: 4031, Batch Gradient Norm: 33.32464977097139
Epoch: 4031, Batch Gradient Norm after: 22.3606779797784
Epoch 4032/10000, Prediction Accuracy = 58.138%, Loss = 0.9137996077537537
Epoch: 4032, Batch Gradient Norm: 31.88007794486806
Epoch: 4032, Batch Gradient Norm after: 22.360678645780183
Epoch 4033/10000, Prediction Accuracy = 58.11800000000001%, Loss = 0.9100814461708069
Epoch: 4033, Batch Gradient Norm: 33.3268888464161
Epoch: 4033, Batch Gradient Norm after: 22.36067553009824
Epoch 4034/10000, Prediction Accuracy = 58.14%, Loss = 0.9136229634284974
Epoch: 4034, Batch Gradient Norm: 31.86778656423807
Epoch: 4034, Batch Gradient Norm after: 22.360679548365788
Epoch 4035/10000, Prediction Accuracy = 58.128%, Loss = 0.9098722696304321
Epoch: 4035, Batch Gradient Norm: 33.33249758597864
Epoch: 4035, Batch Gradient Norm after: 22.360674959906223
Epoch 4036/10000, Prediction Accuracy = 58.144000000000005%, Loss = 0.9134689331054687
Epoch: 4036, Batch Gradient Norm: 31.85364340146211
Epoch: 4036, Batch Gradient Norm after: 22.360677342045935
Epoch 4037/10000, Prediction Accuracy = 58.14%, Loss = 0.9096614480018616
Epoch: 4037, Batch Gradient Norm: 33.34014150814171
Epoch: 4037, Batch Gradient Norm after: 22.360678511153985
Epoch 4038/10000, Prediction Accuracy = 58.148%, Loss = 0.9132996797561646
Epoch: 4038, Batch Gradient Norm: 31.84236552641162
Epoch: 4038, Batch Gradient Norm after: 22.36067807092029
Epoch 4039/10000, Prediction Accuracy = 58.132000000000005%, Loss = 0.9094480156898499
Epoch: 4039, Batch Gradient Norm: 33.34292029561573
Epoch: 4039, Batch Gradient Norm after: 22.360674658905918
Epoch 4040/10000, Prediction Accuracy = 58.169999999999995%, Loss = 0.913131058216095
Epoch: 4040, Batch Gradient Norm: 31.830809001219137
Epoch: 4040, Batch Gradient Norm after: 22.36067733227121
Epoch 4041/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9092442512512207
Epoch: 4041, Batch Gradient Norm: 33.345261835258164
Epoch: 4041, Batch Gradient Norm after: 22.360674927073937
Epoch 4042/10000, Prediction Accuracy = 58.174%, Loss = 0.9129482507705688
Epoch: 4042, Batch Gradient Norm: 31.820442575652333
Epoch: 4042, Batch Gradient Norm after: 22.36067849932415
Epoch 4043/10000, Prediction Accuracy = 58.14%, Loss = 0.9090324282646179
Epoch: 4043, Batch Gradient Norm: 33.34458195280506
Epoch: 4043, Batch Gradient Norm after: 22.360675017804326
Epoch 4044/10000, Prediction Accuracy = 58.168000000000006%, Loss = 0.9127636432647706
Epoch: 4044, Batch Gradient Norm: 31.81029178760567
Epoch: 4044, Batch Gradient Norm after: 22.360677696786148
Epoch 4045/10000, Prediction Accuracy = 58.13199999999999%, Loss = 0.9088291168212891
Epoch: 4045, Batch Gradient Norm: 33.34444159586503
Epoch: 4045, Batch Gradient Norm after: 22.36067571357865
Epoch 4046/10000, Prediction Accuracy = 58.166%, Loss = 0.9125823497772216
Epoch: 4046, Batch Gradient Norm: 31.80091513697502
Epoch: 4046, Batch Gradient Norm after: 22.360679124003326
Epoch 4047/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9086228370666504
Epoch: 4047, Batch Gradient Norm: 33.346801372446386
Epoch: 4047, Batch Gradient Norm after: 22.3606761347113
Epoch 4048/10000, Prediction Accuracy = 58.162%, Loss = 0.9124045729637146
Epoch: 4048, Batch Gradient Norm: 31.79091244937103
Epoch: 4048, Batch Gradient Norm after: 22.360680758723603
Epoch 4049/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9084181547164917
Epoch: 4049, Batch Gradient Norm: 33.34295779820108
Epoch: 4049, Batch Gradient Norm after: 22.3606768446661
Epoch 4050/10000, Prediction Accuracy = 58.17%, Loss = 0.9122191429138183
Epoch: 4050, Batch Gradient Norm: 31.781418349954524
Epoch: 4050, Batch Gradient Norm after: 22.360678329669227
Epoch 4051/10000, Prediction Accuracy = 58.146%, Loss = 0.9082145094871521
Epoch: 4051, Batch Gradient Norm: 33.342506319767615
Epoch: 4051, Batch Gradient Norm after: 22.360675777571565
Epoch 4052/10000, Prediction Accuracy = 58.178%, Loss = 0.9120321869850159
Epoch: 4052, Batch Gradient Norm: 31.771254759694695
Epoch: 4052, Batch Gradient Norm after: 22.36067924249648
Epoch 4053/10000, Prediction Accuracy = 58.14200000000001%, Loss = 0.9080130577087402
Epoch: 4053, Batch Gradient Norm: 33.338345562889295
Epoch: 4053, Batch Gradient Norm after: 22.360676905076943
Epoch 4054/10000, Prediction Accuracy = 58.176%, Loss = 0.9118474125862122
Epoch: 4054, Batch Gradient Norm: 31.76259238713356
Epoch: 4054, Batch Gradient Norm after: 22.360677269539604
Epoch 4055/10000, Prediction Accuracy = 58.144000000000005%, Loss = 0.9078068494796753
Epoch: 4055, Batch Gradient Norm: 33.33427089024274
Epoch: 4055, Batch Gradient Norm after: 22.360678615361017
Epoch 4056/10000, Prediction Accuracy = 58.178%, Loss = 0.9116621255874634
Epoch: 4056, Batch Gradient Norm: 31.75392413664611
Epoch: 4056, Batch Gradient Norm after: 22.36067907870734
Epoch 4057/10000, Prediction Accuracy = 58.148%, Loss = 0.9076028108596802
Epoch: 4057, Batch Gradient Norm: 33.331706187363906
Epoch: 4057, Batch Gradient Norm after: 22.360677783954536
Epoch 4058/10000, Prediction Accuracy = 58.178%, Loss = 0.9114764332771301
Epoch: 4058, Batch Gradient Norm: 31.747174883836138
Epoch: 4058, Batch Gradient Norm after: 22.36067649721646
Epoch 4059/10000, Prediction Accuracy = 58.153999999999996%, Loss = 0.907401728630066
Epoch: 4059, Batch Gradient Norm: 33.32560986408066
Epoch: 4059, Batch Gradient Norm after: 22.36067804386991
Epoch 4060/10000, Prediction Accuracy = 58.18000000000001%, Loss = 0.9112813830375671
Epoch: 4060, Batch Gradient Norm: 31.74123887494508
Epoch: 4060, Batch Gradient Norm after: 22.360676682978816
Epoch 4061/10000, Prediction Accuracy = 58.168000000000006%, Loss = 0.907200837135315
Epoch: 4061, Batch Gradient Norm: 33.31902946232608
Epoch: 4061, Batch Gradient Norm after: 22.3606789070362
Epoch 4062/10000, Prediction Accuracy = 58.19199999999999%, Loss = 0.9110862374305725
Epoch: 4062, Batch Gradient Norm: 31.73438118327706
Epoch: 4062, Batch Gradient Norm after: 22.360676958058253
Epoch 4063/10000, Prediction Accuracy = 58.17%, Loss = 0.9069972157478332
Epoch: 4063, Batch Gradient Norm: 33.310677447163954
Epoch: 4063, Batch Gradient Norm after: 22.360676103246195
Epoch 4064/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.910879623889923
Epoch: 4064, Batch Gradient Norm: 31.72786749005509
Epoch: 4064, Batch Gradient Norm after: 22.360676548598043
Epoch 4065/10000, Prediction Accuracy = 58.176%, Loss = 0.9068036079406738
Epoch: 4065, Batch Gradient Norm: 33.29849088049549
Epoch: 4065, Batch Gradient Norm after: 22.360677108328233
Epoch 4066/10000, Prediction Accuracy = 58.19199999999999%, Loss = 0.910664963722229
Epoch: 4066, Batch Gradient Norm: 31.724031442278115
Epoch: 4066, Batch Gradient Norm after: 22.360677769686347
Epoch 4067/10000, Prediction Accuracy = 58.182%, Loss = 0.9066080927848816
Epoch: 4067, Batch Gradient Norm: 33.28655365183106
Epoch: 4067, Batch Gradient Norm after: 22.36067638033558
Epoch 4068/10000, Prediction Accuracy = 58.196000000000005%, Loss = 0.9104521989822387
Epoch: 4068, Batch Gradient Norm: 31.718751361188623
Epoch: 4068, Batch Gradient Norm after: 22.360675415021912
Epoch 4069/10000, Prediction Accuracy = 58.176%, Loss = 0.9064078330993652
Epoch: 4069, Batch Gradient Norm: 33.27556498305998
Epoch: 4069, Batch Gradient Norm after: 22.360675907963554
Epoch 4070/10000, Prediction Accuracy = 58.19599999999999%, Loss = 0.9102376461029053
Epoch: 4070, Batch Gradient Norm: 31.71379378602746
Epoch: 4070, Batch Gradient Norm after: 22.360674918069765
Epoch 4071/10000, Prediction Accuracy = 58.181999999999995%, Loss = 0.9062096953392029
Epoch: 4071, Batch Gradient Norm: 33.26752805807514
Epoch: 4071, Batch Gradient Norm after: 22.360675919326315
Epoch 4072/10000, Prediction Accuracy = 58.19199999999999%, Loss = 0.9100280642509461
Epoch: 4072, Batch Gradient Norm: 31.70619004132032
Epoch: 4072, Batch Gradient Norm after: 22.36067759040461
Epoch 4073/10000, Prediction Accuracy = 58.176%, Loss = 0.9060144066810608
Epoch: 4073, Batch Gradient Norm: 33.25650087228592
Epoch: 4073, Batch Gradient Norm after: 22.360677076380828
Epoch 4074/10000, Prediction Accuracy = 58.19%, Loss = 0.9098236203193665
Epoch: 4074, Batch Gradient Norm: 31.69863470019532
Epoch: 4074, Batch Gradient Norm after: 22.36067674196091
Epoch 4075/10000, Prediction Accuracy = 58.182%, Loss = 0.9058194994926453
Epoch: 4075, Batch Gradient Norm: 33.24249790477483
Epoch: 4075, Batch Gradient Norm after: 22.3606788256188
Epoch 4076/10000, Prediction Accuracy = 58.19599999999999%, Loss = 0.9096012353897095
Epoch: 4076, Batch Gradient Norm: 31.695226481474364
Epoch: 4076, Batch Gradient Norm after: 22.360677916433986
Epoch 4077/10000, Prediction Accuracy = 58.184000000000005%, Loss = 0.9056151866912842
Epoch: 4077, Batch Gradient Norm: 33.224200460617695
Epoch: 4077, Batch Gradient Norm after: 22.360677341283964
Epoch 4078/10000, Prediction Accuracy = 58.215999999999994%, Loss = 0.9093778252601623
Epoch: 4078, Batch Gradient Norm: 31.690033487984895
Epoch: 4078, Batch Gradient Norm after: 22.36067674935977
Epoch 4079/10000, Prediction Accuracy = 58.18000000000001%, Loss = 0.9054247617721558
Epoch: 4079, Batch Gradient Norm: 33.206895410281014
Epoch: 4079, Batch Gradient Norm after: 22.360675595502165
Epoch 4080/10000, Prediction Accuracy = 58.214%, Loss = 0.9091498136520386
Epoch: 4080, Batch Gradient Norm: 31.688941493604833
Epoch: 4080, Batch Gradient Norm after: 22.360678549648288
Epoch 4081/10000, Prediction Accuracy = 58.182%, Loss = 0.9052301168441772
Epoch: 4081, Batch Gradient Norm: 33.19072671355368
Epoch: 4081, Batch Gradient Norm after: 22.360676326541082
Epoch 4082/10000, Prediction Accuracy = 58.215999999999994%, Loss = 0.9089237570762634
Epoch: 4082, Batch Gradient Norm: 31.68357357119992
Epoch: 4082, Batch Gradient Norm after: 22.360680035136937
Epoch 4083/10000, Prediction Accuracy = 58.194%, Loss = 0.9050442457199097
Epoch: 4083, Batch Gradient Norm: 33.17352842718735
Epoch: 4083, Batch Gradient Norm after: 22.360676842429285
Epoch 4084/10000, Prediction Accuracy = 58.224000000000004%, Loss = 0.9086965799331665
Epoch: 4084, Batch Gradient Norm: 31.681022525182517
Epoch: 4084, Batch Gradient Norm after: 22.36067670287664
Epoch 4085/10000, Prediction Accuracy = 58.198%, Loss = 0.9048469424247741
Epoch: 4085, Batch Gradient Norm: 33.15478450074404
Epoch: 4085, Batch Gradient Norm after: 22.360680298981542
Epoch 4086/10000, Prediction Accuracy = 58.224000000000004%, Loss = 0.9084688782691955
Epoch: 4086, Batch Gradient Norm: 31.676513041574
Epoch: 4086, Batch Gradient Norm after: 22.36067934339476
Epoch 4087/10000, Prediction Accuracy = 58.20399999999999%, Loss = 0.9046595811843872
Epoch: 4087, Batch Gradient Norm: 33.13437770355141
Epoch: 4087, Batch Gradient Norm after: 22.360677996044455
Epoch 4088/10000, Prediction Accuracy = 58.222%, Loss = 0.9082429647445679
Epoch: 4088, Batch Gradient Norm: 31.674122449721075
Epoch: 4088, Batch Gradient Norm after: 22.360679404871895
Epoch 4089/10000, Prediction Accuracy = 58.198%, Loss = 0.9044673323631287
Epoch: 4089, Batch Gradient Norm: 33.11981335790868
Epoch: 4089, Batch Gradient Norm after: 22.36067649777955
Epoch 4090/10000, Prediction Accuracy = 58.232000000000006%, Loss = 0.9080175757408142
Epoch: 4090, Batch Gradient Norm: 31.6677402555871
Epoch: 4090, Batch Gradient Norm after: 22.360678297526093
Epoch 4091/10000, Prediction Accuracy = 58.208000000000006%, Loss = 0.9042790412902832
Epoch: 4091, Batch Gradient Norm: 33.10233891286404
Epoch: 4091, Batch Gradient Norm after: 22.360678439788327
Epoch 4092/10000, Prediction Accuracy = 58.246%, Loss = 0.9078006148338318
Epoch: 4092, Batch Gradient Norm: 31.66284448548399
Epoch: 4092, Batch Gradient Norm after: 22.36067859820113
Epoch 4093/10000, Prediction Accuracy = 58.21600000000001%, Loss = 0.9040886878967285
Epoch: 4093, Batch Gradient Norm: 33.08531472848275
Epoch: 4093, Batch Gradient Norm after: 22.36067687236215
Epoch 4094/10000, Prediction Accuracy = 58.258%, Loss = 0.907575261592865
Epoch: 4094, Batch Gradient Norm: 31.66126930585998
Epoch: 4094, Batch Gradient Norm after: 22.36067895580857
Epoch 4095/10000, Prediction Accuracy = 58.218%, Loss = 0.9038944482803345
Epoch: 4095, Batch Gradient Norm: 33.067294582545806
Epoch: 4095, Batch Gradient Norm after: 22.360677160063574
Epoch 4096/10000, Prediction Accuracy = 58.25%, Loss = 0.9073457598686219
Epoch: 4096, Batch Gradient Norm: 31.65867831601252
Epoch: 4096, Batch Gradient Norm after: 22.360678319563856
Epoch 4097/10000, Prediction Accuracy = 58.217999999999996%, Loss = 0.9037026166915894
Epoch: 4097, Batch Gradient Norm: 33.05099225385971
Epoch: 4097, Batch Gradient Norm after: 22.360676363346265
Epoch 4098/10000, Prediction Accuracy = 58.262%, Loss = 0.9071274995803833
Epoch: 4098, Batch Gradient Norm: 31.653853097272208
Epoch: 4098, Batch Gradient Norm after: 22.360677513138494
Epoch 4099/10000, Prediction Accuracy = 58.214%, Loss = 0.9035141110420227
Epoch: 4099, Batch Gradient Norm: 33.03398006993609
Epoch: 4099, Batch Gradient Norm after: 22.36067802988681
Epoch 4100/10000, Prediction Accuracy = 58.262%, Loss = 0.9069040894508362
Epoch: 4100, Batch Gradient Norm: 31.649445890503575
Epoch: 4100, Batch Gradient Norm after: 22.360676216530848
Epoch 4101/10000, Prediction Accuracy = 58.217999999999996%, Loss = 0.9033227801322937
Epoch: 4101, Batch Gradient Norm: 33.01667601347833
Epoch: 4101, Batch Gradient Norm after: 22.36067626129689
Epoch 4102/10000, Prediction Accuracy = 58.246%, Loss = 0.9066858649253845
Epoch: 4102, Batch Gradient Norm: 31.644770453908134
Epoch: 4102, Batch Gradient Norm after: 22.360677098345253
Epoch 4103/10000, Prediction Accuracy = 58.220000000000006%, Loss = 0.9031354784965515
Epoch: 4103, Batch Gradient Norm: 33.0006505742896
Epoch: 4103, Batch Gradient Norm after: 22.36067698020582
Epoch 4104/10000, Prediction Accuracy = 58.251999999999995%, Loss = 0.9064605951309204
Epoch: 4104, Batch Gradient Norm: 31.63944104653818
Epoch: 4104, Batch Gradient Norm after: 22.36067763988665
Epoch 4105/10000, Prediction Accuracy = 58.22600000000001%, Loss = 0.9029412388801574
Epoch: 4105, Batch Gradient Norm: 32.98491075806
Epoch: 4105, Batch Gradient Norm after: 22.360675264378393
Epoch 4106/10000, Prediction Accuracy = 58.246%, Loss = 0.90624520778656
Epoch: 4106, Batch Gradient Norm: 31.63416831862017
Epoch: 4106, Batch Gradient Norm after: 22.360677584245444
Epoch 4107/10000, Prediction Accuracy = 58.230000000000004%, Loss = 0.9027464389801025
Epoch: 4107, Batch Gradient Norm: 32.971727896505556
Epoch: 4107, Batch Gradient Norm after: 22.360677754539243
Epoch 4108/10000, Prediction Accuracy = 58.24399999999999%, Loss = 0.9060296893119812
Epoch: 4108, Batch Gradient Norm: 31.627804064499202
Epoch: 4108, Batch Gradient Norm after: 22.36067709210573
Epoch 4109/10000, Prediction Accuracy = 58.232000000000006%, Loss = 0.9025597929954529
Epoch: 4109, Batch Gradient Norm: 32.95871064453583
Epoch: 4109, Batch Gradient Norm after: 22.360677177513807
Epoch 4110/10000, Prediction Accuracy = 58.25%, Loss = 0.9058236122131348
Epoch: 4110, Batch Gradient Norm: 31.622236001052432
Epoch: 4110, Batch Gradient Norm after: 22.360677555453194
Epoch 4111/10000, Prediction Accuracy = 58.239999999999995%, Loss = 0.902363383769989
Epoch: 4111, Batch Gradient Norm: 32.945956985791845
Epoch: 4111, Batch Gradient Norm after: 22.360677805325487
Epoch 4112/10000, Prediction Accuracy = 58.25999999999999%, Loss = 0.9056169629096985
Epoch: 4112, Batch Gradient Norm: 31.61681585040062
Epoch: 4112, Batch Gradient Norm after: 22.360679150820122
Epoch 4113/10000, Prediction Accuracy = 58.24400000000001%, Loss = 0.9021749019622802
Epoch: 4113, Batch Gradient Norm: 32.93486894942128
Epoch: 4113, Batch Gradient Norm after: 22.360676611545838
Epoch 4114/10000, Prediction Accuracy = 58.257999999999996%, Loss = 0.905404806137085
Epoch: 4114, Batch Gradient Norm: 31.609805123476537
Epoch: 4114, Batch Gradient Norm after: 22.360679651231514
Epoch 4115/10000, Prediction Accuracy = 58.254%, Loss = 0.9019793510437012
Epoch: 4115, Batch Gradient Norm: 32.92789703532392
Epoch: 4115, Batch Gradient Norm after: 22.360677130386787
Epoch 4116/10000, Prediction Accuracy = 58.266%, Loss = 0.9052041888236999
Epoch: 4116, Batch Gradient Norm: 31.602347302802254
Epoch: 4116, Batch Gradient Norm after: 22.360679803193268
Epoch 4117/10000, Prediction Accuracy = 58.261999999999986%, Loss = 0.9017848491668701
Epoch: 4117, Batch Gradient Norm: 32.91613184793078
Epoch: 4117, Batch Gradient Norm after: 22.360675651026547
Epoch 4118/10000, Prediction Accuracy = 58.258%, Loss = 0.904999029636383
Epoch: 4118, Batch Gradient Norm: 31.597546429824796
Epoch: 4118, Batch Gradient Norm after: 22.360678852343536
Epoch 4119/10000, Prediction Accuracy = 58.257999999999996%, Loss = 0.9015928149223328
Epoch: 4119, Batch Gradient Norm: 32.902636253798384
Epoch: 4119, Batch Gradient Norm after: 22.36067831267022
Epoch 4120/10000, Prediction Accuracy = 58.254%, Loss = 0.9047853708267212
Epoch: 4120, Batch Gradient Norm: 31.590578009066228
Epoch: 4120, Batch Gradient Norm after: 22.360677508137698
Epoch 4121/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9014038681983948
Epoch: 4121, Batch Gradient Norm: 32.890921310613926
Epoch: 4121, Batch Gradient Norm after: 22.360678513248896
Epoch 4122/10000, Prediction Accuracy = 58.258%, Loss = 0.9045716762542725
Epoch: 4122, Batch Gradient Norm: 31.587137221352968
Epoch: 4122, Batch Gradient Norm after: 22.360679108479093
Epoch 4123/10000, Prediction Accuracy = 58.27%, Loss = 0.9012107133865357
Epoch: 4123, Batch Gradient Norm: 32.87887369028684
Epoch: 4123, Batch Gradient Norm after: 22.360678217708468
Epoch 4124/10000, Prediction Accuracy = 58.266000000000005%, Loss = 0.9043665289878845
Epoch: 4124, Batch Gradient Norm: 31.58167680376405
Epoch: 4124, Batch Gradient Norm after: 22.360677821827853
Epoch 4125/10000, Prediction Accuracy = 58.275999999999996%, Loss = 0.9010194420814515
Epoch: 4125, Batch Gradient Norm: 32.86658045053183
Epoch: 4125, Batch Gradient Norm after: 22.36067821256391
Epoch 4126/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9041600942611694
Epoch: 4126, Batch Gradient Norm: 31.57910518312204
Epoch: 4126, Batch Gradient Norm after: 22.360678366097638
Epoch 4127/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9008289575576782
Epoch: 4127, Batch Gradient Norm: 32.85536513681256
Epoch: 4127, Batch Gradient Norm after: 22.360679110058623
Epoch 4128/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9039548635482788
Epoch: 4128, Batch Gradient Norm: 31.57364888732298
Epoch: 4128, Batch Gradient Norm after: 22.360678233833042
Epoch 4129/10000, Prediction Accuracy = 58.272000000000006%, Loss = 0.9006359338760376
Epoch: 4129, Batch Gradient Norm: 32.84655730686211
Epoch: 4129, Batch Gradient Norm after: 22.36067798636086
Epoch 4130/10000, Prediction Accuracy = 58.262%, Loss = 0.903753399848938
Epoch: 4130, Batch Gradient Norm: 31.567930290450608
Epoch: 4130, Batch Gradient Norm after: 22.360677459664004
Epoch 4131/10000, Prediction Accuracy = 58.282000000000004%, Loss = 0.9004409551620484
Epoch: 4131, Batch Gradient Norm: 32.8370755475491
Epoch: 4131, Batch Gradient Norm after: 22.360677304351352
Epoch 4132/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.9035503983497619
Epoch: 4132, Batch Gradient Norm: 31.558465840688516
Epoch: 4132, Batch Gradient Norm after: 22.360678070064058
Epoch 4133/10000, Prediction Accuracy = 58.279999999999994%, Loss = 0.9002483725547791
Epoch: 4133, Batch Gradient Norm: 32.83086059490257
Epoch: 4133, Batch Gradient Norm after: 22.36067851330374
Epoch 4134/10000, Prediction Accuracy = 58.251999999999995%, Loss = 0.90335693359375
Epoch: 4134, Batch Gradient Norm: 31.554400035723912
Epoch: 4134, Batch Gradient Norm after: 22.36067837055481
Epoch 4135/10000, Prediction Accuracy = 58.284000000000006%, Loss = 0.9000549793243409
Epoch: 4135, Batch Gradient Norm: 32.82017226731982
Epoch: 4135, Batch Gradient Norm after: 22.360678127269487
Epoch 4136/10000, Prediction Accuracy = 58.25600000000001%, Loss = 0.9031535267829895
Epoch: 4136, Batch Gradient Norm: 31.550402933110764
Epoch: 4136, Batch Gradient Norm after: 22.36067781429824
Epoch 4137/10000, Prediction Accuracy = 58.286%, Loss = 0.899865198135376
Epoch: 4137, Batch Gradient Norm: 32.814902810061895
Epoch: 4137, Batch Gradient Norm after: 22.360678249981195
Epoch 4138/10000, Prediction Accuracy = 58.262000000000015%, Loss = 0.902960193157196
Epoch: 4138, Batch Gradient Norm: 31.54131024259386
Epoch: 4138, Batch Gradient Norm after: 22.36067752277988
Epoch 4139/10000, Prediction Accuracy = 58.29%, Loss = 0.8996665596961975
Epoch: 4139, Batch Gradient Norm: 32.81107539779418
Epoch: 4139, Batch Gradient Norm after: 22.360677572396497
Epoch 4140/10000, Prediction Accuracy = 58.274%, Loss = 0.9027771711349487
Epoch: 4140, Batch Gradient Norm: 31.53189520648337
Epoch: 4140, Batch Gradient Norm after: 22.360678590557832
Epoch 4141/10000, Prediction Accuracy = 58.298%, Loss = 0.8994739174842834
Epoch: 4141, Batch Gradient Norm: 32.80551682495838
Epoch: 4141, Batch Gradient Norm after: 22.360677126491133
Epoch 4142/10000, Prediction Accuracy = 58.274%, Loss = 0.9025892853736878
Epoch: 4142, Batch Gradient Norm: 31.529246041751378
Epoch: 4142, Batch Gradient Norm after: 22.360678423346428
Epoch 4143/10000, Prediction Accuracy = 58.29600000000001%, Loss = 0.8992795825004578
Epoch: 4143, Batch Gradient Norm: 32.79836649631539
Epoch: 4143, Batch Gradient Norm after: 22.360677876209987
Epoch 4144/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.9023999333381653
Epoch: 4144, Batch Gradient Norm: 31.520783811451885
Epoch: 4144, Batch Gradient Norm after: 22.360677462167473
Epoch 4145/10000, Prediction Accuracy = 58.29599999999999%, Loss = 0.8990872979164124
Epoch: 4145, Batch Gradient Norm: 32.79133811737738
Epoch: 4145, Batch Gradient Norm after: 22.36067781523341
Epoch 4146/10000, Prediction Accuracy = 58.25999999999999%, Loss = 0.9022017002105713
Epoch: 4146, Batch Gradient Norm: 31.515287807047514
Epoch: 4146, Batch Gradient Norm after: 22.360676873156017
Epoch 4147/10000, Prediction Accuracy = 58.298%, Loss = 0.8988945960998536
Epoch: 4147, Batch Gradient Norm: 32.782148896222736
Epoch: 4147, Batch Gradient Norm after: 22.360676555600755
Epoch 4148/10000, Prediction Accuracy = 58.251999999999995%, Loss = 0.9020124554634095
Epoch: 4148, Batch Gradient Norm: 31.508889789390008
Epoch: 4148, Batch Gradient Norm after: 22.360677198108135
Epoch 4149/10000, Prediction Accuracy = 58.29200000000001%, Loss = 0.8987077593803405
Epoch: 4149, Batch Gradient Norm: 32.77430055311745
Epoch: 4149, Batch Gradient Norm after: 22.36067783859541
Epoch 4150/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.9018153667449951
Epoch: 4150, Batch Gradient Norm: 31.50023440260959
Epoch: 4150, Batch Gradient Norm after: 22.360677710358306
Epoch 4151/10000, Prediction Accuracy = 58.284000000000006%, Loss = 0.8985159635543823
Epoch: 4151, Batch Gradient Norm: 32.771790653731756
Epoch: 4151, Batch Gradient Norm after: 22.360676431591855
Epoch 4152/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.9016323924064636
Epoch: 4152, Batch Gradient Norm: 31.491184442707482
Epoch: 4152, Batch Gradient Norm after: 22.360675840649204
Epoch 4153/10000, Prediction Accuracy = 58.303999999999995%, Loss = 0.8983225584030151
Epoch: 4153, Batch Gradient Norm: 32.766688463963696
Epoch: 4153, Batch Gradient Norm after: 22.360677563789537
Epoch 4154/10000, Prediction Accuracy = 58.26800000000001%, Loss = 0.9014471650123597
Epoch: 4154, Batch Gradient Norm: 31.482377677380004
Epoch: 4154, Batch Gradient Norm after: 22.36067546882862
Epoch 4155/10000, Prediction Accuracy = 58.312%, Loss = 0.8981316566467286
Epoch: 4155, Batch Gradient Norm: 32.76096562805012
Epoch: 4155, Batch Gradient Norm after: 22.360676174301584
Epoch 4156/10000, Prediction Accuracy = 58.278%, Loss = 0.901258647441864
Epoch: 4156, Batch Gradient Norm: 31.477618296838354
Epoch: 4156, Batch Gradient Norm after: 22.360678830729245
Epoch 4157/10000, Prediction Accuracy = 58.302%, Loss = 0.8979363799095154
Epoch: 4157, Batch Gradient Norm: 32.750907920492686
Epoch: 4157, Batch Gradient Norm after: 22.360675717791956
Epoch 4158/10000, Prediction Accuracy = 58.278%, Loss = 0.9010515809059143
Epoch: 4158, Batch Gradient Norm: 31.470456212793316
Epoch: 4158, Batch Gradient Norm after: 22.360678062645523
Epoch 4159/10000, Prediction Accuracy = 58.302%, Loss = 0.8977493524551392
Epoch: 4159, Batch Gradient Norm: 32.752012453947266
Epoch: 4159, Batch Gradient Norm after: 22.360677236349794
Epoch 4160/10000, Prediction Accuracy = 58.275999999999996%, Loss = 0.900879979133606
Epoch: 4160, Batch Gradient Norm: 31.463076574754044
Epoch: 4160, Batch Gradient Norm after: 22.360676606628036
Epoch 4161/10000, Prediction Accuracy = 58.312%, Loss = 0.8975551009178162
Epoch: 4161, Batch Gradient Norm: 32.75246001505532
Epoch: 4161, Batch Gradient Norm after: 22.360677463834563
Epoch 4162/10000, Prediction Accuracy = 58.278%, Loss = 0.9007055163383484
Epoch: 4162, Batch Gradient Norm: 31.452729807322275
Epoch: 4162, Batch Gradient Norm after: 22.360678257449315
Epoch 4163/10000, Prediction Accuracy = 58.32199999999999%, Loss = 0.8973600149154664
Epoch: 4163, Batch Gradient Norm: 32.75014008975039
Epoch: 4163, Batch Gradient Norm after: 22.360678335531848
Epoch 4164/10000, Prediction Accuracy = 58.288%, Loss = 0.9005345702171326
Epoch: 4164, Batch Gradient Norm: 31.443607014538777
Epoch: 4164, Batch Gradient Norm after: 22.3606789588957
Epoch 4165/10000, Prediction Accuracy = 58.322%, Loss = 0.897169828414917
Epoch: 4165, Batch Gradient Norm: 32.74829297736076
Epoch: 4165, Batch Gradient Norm after: 22.360677700360146
Epoch 4166/10000, Prediction Accuracy = 58.284000000000006%, Loss = 0.9003554821014405
Epoch: 4166, Batch Gradient Norm: 31.433682231116848
Epoch: 4166, Batch Gradient Norm after: 22.360676428586235
Epoch 4167/10000, Prediction Accuracy = 58.331999999999994%, Loss = 0.8969778180122375
Epoch: 4167, Batch Gradient Norm: 32.747036274908766
Epoch: 4167, Batch Gradient Norm after: 22.360677106302145
Epoch 4168/10000, Prediction Accuracy = 58.29%, Loss = 0.900178861618042
Epoch: 4168, Batch Gradient Norm: 31.424448528796937
Epoch: 4168, Batch Gradient Norm after: 22.360676035116196
Epoch 4169/10000, Prediction Accuracy = 58.342%, Loss = 0.8967880129814148
Epoch: 4169, Batch Gradient Norm: 32.742116053382325
Epoch: 4169, Batch Gradient Norm after: 22.3606793185078
Epoch 4170/10000, Prediction Accuracy = 58.29600000000001%, Loss = 0.8999961733818054
Epoch: 4170, Batch Gradient Norm: 31.416735131877054
Epoch: 4170, Batch Gradient Norm after: 22.360677556802283
Epoch 4171/10000, Prediction Accuracy = 58.348%, Loss = 0.8965938806533813
Epoch: 4171, Batch Gradient Norm: 32.733326639902664
Epoch: 4171, Batch Gradient Norm after: 22.36068029157748
Epoch 4172/10000, Prediction Accuracy = 58.303999999999995%, Loss = 0.8998031854629517
Epoch: 4172, Batch Gradient Norm: 31.4104265234223
Epoch: 4172, Batch Gradient Norm after: 22.360677721188107
Epoch 4173/10000, Prediction Accuracy = 58.354%, Loss = 0.8964093327522278
Epoch: 4173, Batch Gradient Norm: 32.72516232048127
Epoch: 4173, Batch Gradient Norm after: 22.360677419900668
Epoch 4174/10000, Prediction Accuracy = 58.306000000000004%, Loss = 0.8996087431907653
Epoch: 4174, Batch Gradient Norm: 31.402584823327185
Epoch: 4174, Batch Gradient Norm after: 22.360677177709793
Epoch 4175/10000, Prediction Accuracy = 58.358000000000004%, Loss = 0.896222448348999
Epoch: 4175, Batch Gradient Norm: 32.720554464852995
Epoch: 4175, Batch Gradient Norm after: 22.36067812787313
Epoch 4176/10000, Prediction Accuracy = 58.324%, Loss = 0.8994252204895019
Epoch: 4176, Batch Gradient Norm: 31.392835909024008
Epoch: 4176, Batch Gradient Norm after: 22.36067689621205
Epoch 4177/10000, Prediction Accuracy = 58.36600000000001%, Loss = 0.8960334897041321
Epoch: 4177, Batch Gradient Norm: 32.71574940816701
Epoch: 4177, Batch Gradient Norm after: 22.360679519275955
Epoch 4178/10000, Prediction Accuracy = 58.32800000000001%, Loss = 0.8992428302764892
Epoch: 4178, Batch Gradient Norm: 31.38405164516367
Epoch: 4178, Batch Gradient Norm after: 22.360677421561906
Epoch 4179/10000, Prediction Accuracy = 58.36600000000001%, Loss = 0.8958421111106872
Epoch: 4179, Batch Gradient Norm: 32.709808254980864
Epoch: 4179, Batch Gradient Norm after: 22.360677757866018
Epoch 4180/10000, Prediction Accuracy = 58.327999999999996%, Loss = 0.8990495443344116
Epoch: 4180, Batch Gradient Norm: 31.376849290679203
Epoch: 4180, Batch Gradient Norm after: 22.360676702350915
Epoch 4181/10000, Prediction Accuracy = 58.35799999999999%, Loss = 0.8956489443778992
Epoch: 4181, Batch Gradient Norm: 32.70008081501375
Epoch: 4181, Batch Gradient Norm after: 22.360678319381794
Epoch 4182/10000, Prediction Accuracy = 58.326%, Loss = 0.8988528490066529
Epoch: 4182, Batch Gradient Norm: 31.37088561182656
Epoch: 4182, Batch Gradient Norm after: 22.360675821293047
Epoch 4183/10000, Prediction Accuracy = 58.364%, Loss = 0.8954605221748352
Epoch: 4183, Batch Gradient Norm: 32.69546056280792
Epoch: 4183, Batch Gradient Norm after: 22.36067791005245
Epoch 4184/10000, Prediction Accuracy = 58.326%, Loss = 0.8986675620079041
Epoch: 4184, Batch Gradient Norm: 31.362589411829983
Epoch: 4184, Batch Gradient Norm after: 22.360678228502522
Epoch 4185/10000, Prediction Accuracy = 58.378%, Loss = 0.8952659726142883
Epoch: 4185, Batch Gradient Norm: 32.689387340895316
Epoch: 4185, Batch Gradient Norm after: 22.36067722589319
Epoch 4186/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.8984819650650024
Epoch: 4186, Batch Gradient Norm: 31.355636193321583
Epoch: 4186, Batch Gradient Norm after: 22.36067692876242
Epoch 4187/10000, Prediction Accuracy = 58.372%, Loss = 0.8950746297836304
Epoch: 4187, Batch Gradient Norm: 32.68642734596521
Epoch: 4187, Batch Gradient Norm after: 22.36067615268675
Epoch 4188/10000, Prediction Accuracy = 58.346000000000004%, Loss = 0.8983000636100769
Epoch: 4188, Batch Gradient Norm: 31.34739471622779
Epoch: 4188, Batch Gradient Norm after: 22.360677542800484
Epoch 4189/10000, Prediction Accuracy = 58.378%, Loss = 0.8948851943016052
Epoch: 4189, Batch Gradient Norm: 32.6779858028803
Epoch: 4189, Batch Gradient Norm after: 22.360677533105143
Epoch 4190/10000, Prediction Accuracy = 58.36%, Loss = 0.8981114149093627
Epoch: 4190, Batch Gradient Norm: 31.339134122308987
Epoch: 4190, Batch Gradient Norm after: 22.36067748748649
Epoch 4191/10000, Prediction Accuracy = 58.379999999999995%, Loss = 0.8946975231170654
Epoch: 4191, Batch Gradient Norm: 32.673333951543434
Epoch: 4191, Batch Gradient Norm after: 22.360679896683443
Epoch 4192/10000, Prediction Accuracy = 58.362%, Loss = 0.8979285001754761
Epoch: 4192, Batch Gradient Norm: 31.33133981956983
Epoch: 4192, Batch Gradient Norm after: 22.36067728263102
Epoch 4193/10000, Prediction Accuracy = 58.379999999999995%, Loss = 0.8945118427276612
Epoch: 4193, Batch Gradient Norm: 32.66500786558518
Epoch: 4193, Batch Gradient Norm after: 22.36067857955807
Epoch 4194/10000, Prediction Accuracy = 58.378%, Loss = 0.8977423191070557
Epoch: 4194, Batch Gradient Norm: 31.323603404766303
Epoch: 4194, Batch Gradient Norm after: 22.360676258867606
Epoch 4195/10000, Prediction Accuracy = 58.378%, Loss = 0.8943216562271118
Epoch: 4195, Batch Gradient Norm: 32.65820170804568
Epoch: 4195, Batch Gradient Norm after: 22.360677509057254
Epoch 4196/10000, Prediction Accuracy = 58.382000000000005%, Loss = 0.8975607633590699
Epoch: 4196, Batch Gradient Norm: 31.318679365760637
Epoch: 4196, Batch Gradient Norm after: 22.36067615874911
Epoch 4197/10000, Prediction Accuracy = 58.376%, Loss = 0.8941351890563964
Epoch: 4197, Batch Gradient Norm: 32.649311500540875
Epoch: 4197, Batch Gradient Norm after: 22.360677419358122
Epoch 4198/10000, Prediction Accuracy = 58.394000000000005%, Loss = 0.897367537021637
Epoch: 4198, Batch Gradient Norm: 31.310315405536148
Epoch: 4198, Batch Gradient Norm after: 22.360675863075087
Epoch 4199/10000, Prediction Accuracy = 58.38199999999999%, Loss = 0.8939432024955749
Epoch: 4199, Batch Gradient Norm: 32.645705353176076
Epoch: 4199, Batch Gradient Norm after: 22.360677229939686
Epoch 4200/10000, Prediction Accuracy = 58.388%, Loss = 0.8971881628036499
Epoch: 4200, Batch Gradient Norm: 31.303336598365767
Epoch: 4200, Batch Gradient Norm after: 22.360675163009535
Epoch 4201/10000, Prediction Accuracy = 58.384%, Loss = 0.8937467575073242
Epoch: 4201, Batch Gradient Norm: 32.64187380203185
Epoch: 4201, Batch Gradient Norm after: 22.360680799515688
Epoch 4202/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.8970025062561036
Epoch: 4202, Batch Gradient Norm: 31.292949368938025
Epoch: 4202, Batch Gradient Norm after: 22.360675821338962
Epoch 4203/10000, Prediction Accuracy = 58.386%, Loss = 0.8935593724250793
Epoch: 4203, Batch Gradient Norm: 32.63782515517557
Epoch: 4203, Batch Gradient Norm after: 22.36067841342399
Epoch 4204/10000, Prediction Accuracy = 58.40599999999999%, Loss = 0.8968266010284424
Epoch: 4204, Batch Gradient Norm: 31.283799773000677
Epoch: 4204, Batch Gradient Norm after: 22.36067519257394
Epoch 4205/10000, Prediction Accuracy = 58.374%, Loss = 0.8933642745018006
Epoch: 4205, Batch Gradient Norm: 32.636827110796375
Epoch: 4205, Batch Gradient Norm after: 22.360677809421315
Epoch 4206/10000, Prediction Accuracy = 58.398%, Loss = 0.8966538071632385
Epoch: 4206, Batch Gradient Norm: 31.27603358168284
Epoch: 4206, Batch Gradient Norm after: 22.360676514403146
Epoch 4207/10000, Prediction Accuracy = 58.378%, Loss = 0.8931717276573181
Epoch: 4207, Batch Gradient Norm: 32.631644240056836
Epoch: 4207, Batch Gradient Norm after: 22.360677283018966
Epoch 4208/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.8964767336845398
Epoch: 4208, Batch Gradient Norm: 31.26997679139372
Epoch: 4208, Batch Gradient Norm after: 22.36067497466171
Epoch 4209/10000, Prediction Accuracy = 58.386%, Loss = 0.8929808855056762
Epoch: 4209, Batch Gradient Norm: 32.623814832886275
Epoch: 4209, Batch Gradient Norm after: 22.360677641939304
Epoch 4210/10000, Prediction Accuracy = 58.408%, Loss = 0.8962855219841004
Epoch: 4210, Batch Gradient Norm: 31.2612803748276
Epoch: 4210, Batch Gradient Norm after: 22.36067542742119
Epoch 4211/10000, Prediction Accuracy = 58.391999999999996%, Loss = 0.8927913069725036
Epoch: 4211, Batch Gradient Norm: 32.617402266233015
Epoch: 4211, Batch Gradient Norm after: 22.36067875143366
Epoch 4212/10000, Prediction Accuracy = 58.42%, Loss = 0.8960942149162292
Epoch: 4212, Batch Gradient Norm: 31.25691793421236
Epoch: 4212, Batch Gradient Norm after: 22.360676378736922
Epoch 4213/10000, Prediction Accuracy = 58.4%, Loss = 0.8926031827926636
Epoch: 4213, Batch Gradient Norm: 32.6107790476237
Epoch: 4213, Batch Gradient Norm after: 22.360678718706726
Epoch 4214/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8959008574485778
Epoch: 4214, Batch Gradient Norm: 31.25024316258897
Epoch: 4214, Batch Gradient Norm after: 22.36067613658993
Epoch 4215/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.8924109220504761
Epoch: 4215, Batch Gradient Norm: 32.607570458874314
Epoch: 4215, Batch Gradient Norm after: 22.3606792343027
Epoch 4216/10000, Prediction Accuracy = 58.424%, Loss = 0.8957242608070374
Epoch: 4216, Batch Gradient Norm: 31.24179824355638
Epoch: 4216, Batch Gradient Norm after: 22.360676846088143
Epoch 4217/10000, Prediction Accuracy = 58.406000000000006%, Loss = 0.8922208428382874
Epoch: 4217, Batch Gradient Norm: 32.60765250026664
Epoch: 4217, Batch Gradient Norm after: 22.360676383993617
Epoch 4218/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.895541763305664
Epoch: 4218, Batch Gradient Norm: 31.23254569107409
Epoch: 4218, Batch Gradient Norm after: 22.36067547387535
Epoch 4219/10000, Prediction Accuracy = 58.40999999999999%, Loss = 0.892029345035553
Epoch: 4219, Batch Gradient Norm: 32.61049908862739
Epoch: 4219, Batch Gradient Norm after: 22.36067627349126
Epoch 4220/10000, Prediction Accuracy = 58.424%, Loss = 0.8953784704208374
Epoch: 4220, Batch Gradient Norm: 31.221200539020625
Epoch: 4220, Batch Gradient Norm after: 22.36067737363828
Epoch 4221/10000, Prediction Accuracy = 58.416%, Loss = 0.8918370366096496
Epoch: 4221, Batch Gradient Norm: 32.61202753984086
Epoch: 4221, Batch Gradient Norm after: 22.360679059742775
Epoch 4222/10000, Prediction Accuracy = 58.414%, Loss = 0.8952108383178711
Epoch: 4222, Batch Gradient Norm: 31.21357079508249
Epoch: 4222, Batch Gradient Norm after: 22.360678583490657
Epoch 4223/10000, Prediction Accuracy = 58.424%, Loss = 0.8916468739509582
Epoch: 4223, Batch Gradient Norm: 32.61070423922955
Epoch: 4223, Batch Gradient Norm after: 22.360675411037935
Epoch 4224/10000, Prediction Accuracy = 58.416%, Loss = 0.8950425744056701
Epoch: 4224, Batch Gradient Norm: 31.204100374172505
Epoch: 4224, Batch Gradient Norm after: 22.360677295044635
Epoch 4225/10000, Prediction Accuracy = 58.426%, Loss = 0.8914535522460938
Epoch: 4225, Batch Gradient Norm: 32.60949548570819
Epoch: 4225, Batch Gradient Norm after: 22.360678143519213
Epoch 4226/10000, Prediction Accuracy = 58.419999999999995%, Loss = 0.8948781371116639
Epoch: 4226, Batch Gradient Norm: 31.195651896677063
Epoch: 4226, Batch Gradient Norm after: 22.360679657278645
Epoch 4227/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8912653803825379
Epoch: 4227, Batch Gradient Norm: 32.60681857761808
Epoch: 4227, Batch Gradient Norm after: 22.360678531530123
Epoch 4228/10000, Prediction Accuracy = 58.416%, Loss = 0.8947051644325257
Epoch: 4228, Batch Gradient Norm: 31.18573915181454
Epoch: 4228, Batch Gradient Norm after: 22.360677012181515
Epoch 4229/10000, Prediction Accuracy = 58.42199999999999%, Loss = 0.8910775661468506
Epoch: 4229, Batch Gradient Norm: 32.606013836980644
Epoch: 4229, Batch Gradient Norm after: 22.360678008822013
Epoch 4230/10000, Prediction Accuracy = 58.414%, Loss = 0.8945351243019104
Epoch: 4230, Batch Gradient Norm: 31.178159049999305
Epoch: 4230, Batch Gradient Norm after: 22.360677073652827
Epoch 4231/10000, Prediction Accuracy = 58.43599999999999%, Loss = 0.8908864021301269
Epoch: 4231, Batch Gradient Norm: 32.60346094813561
Epoch: 4231, Batch Gradient Norm after: 22.3606759413408
Epoch 4232/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.8943576693534852
Epoch: 4232, Batch Gradient Norm: 31.170863360729
Epoch: 4232, Batch Gradient Norm after: 22.3606772442035
Epoch 4233/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.8906975507736206
Epoch: 4233, Batch Gradient Norm: 32.5957669613808
Epoch: 4233, Batch Gradient Norm after: 22.360676900697676
Epoch 4234/10000, Prediction Accuracy = 58.426%, Loss = 0.8941707730293273
Epoch: 4234, Batch Gradient Norm: 31.164274833829957
Epoch: 4234, Batch Gradient Norm after: 22.360677672477298
Epoch 4235/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.8905115365982056
Epoch: 4235, Batch Gradient Norm: 32.58312724345396
Epoch: 4235, Batch Gradient Norm after: 22.36067890607983
Epoch 4236/10000, Prediction Accuracy = 58.428%, Loss = 0.893969714641571
Epoch: 4236, Batch Gradient Norm: 31.161388318895487
Epoch: 4236, Batch Gradient Norm after: 22.360677265898897
Epoch 4237/10000, Prediction Accuracy = 58.436%, Loss = 0.8903251886367798
Epoch: 4237, Batch Gradient Norm: 32.56915498415099
Epoch: 4237, Batch Gradient Norm after: 22.360677307185302
Epoch 4238/10000, Prediction Accuracy = 58.42999999999999%, Loss = 0.8937669515609741
Epoch: 4238, Batch Gradient Norm: 31.155426284484648
Epoch: 4238, Batch Gradient Norm after: 22.36067847235428
Epoch 4239/10000, Prediction Accuracy = 58.452%, Loss = 0.8901425242424011
Epoch: 4239, Batch Gradient Norm: 32.56117481265765
Epoch: 4239, Batch Gradient Norm after: 22.36067716433963
Epoch 4240/10000, Prediction Accuracy = 58.436%, Loss = 0.893576443195343
Epoch: 4240, Batch Gradient Norm: 31.150903349286768
Epoch: 4240, Batch Gradient Norm after: 22.360678879218817
Epoch 4241/10000, Prediction Accuracy = 58.444%, Loss = 0.8899580717086792
Epoch: 4241, Batch Gradient Norm: 32.55166246958695
Epoch: 4241, Batch Gradient Norm after: 22.360677744820755
Epoch 4242/10000, Prediction Accuracy = 58.446000000000005%, Loss = 0.8933825731277466
Epoch: 4242, Batch Gradient Norm: 31.143684059854493
Epoch: 4242, Batch Gradient Norm after: 22.360676383639678
Epoch 4243/10000, Prediction Accuracy = 58.448%, Loss = 0.8897697687149048
Epoch: 4243, Batch Gradient Norm: 32.5442688991056
Epoch: 4243, Batch Gradient Norm after: 22.3606780399048
Epoch 4244/10000, Prediction Accuracy = 58.44200000000001%, Loss = 0.8931919455528259
Epoch: 4244, Batch Gradient Norm: 31.137481485900622
Epoch: 4244, Batch Gradient Norm after: 22.360677576680523
Epoch 4245/10000, Prediction Accuracy = 58.452%, Loss = 0.8895840764045715
Epoch: 4245, Batch Gradient Norm: 32.53241028179959
Epoch: 4245, Batch Gradient Norm after: 22.360679132480435
Epoch 4246/10000, Prediction Accuracy = 58.44199999999999%, Loss = 0.8929928421974183
Epoch: 4246, Batch Gradient Norm: 31.132819926432866
Epoch: 4246, Batch Gradient Norm after: 22.360675839173027
Epoch 4247/10000, Prediction Accuracy = 58.456%, Loss = 0.8894028306007385
Epoch: 4247, Batch Gradient Norm: 32.52549879810915
Epoch: 4247, Batch Gradient Norm after: 22.36067820634056
Epoch 4248/10000, Prediction Accuracy = 58.448%, Loss = 0.8928059220314026
Epoch: 4248, Batch Gradient Norm: 31.12431272463998
Epoch: 4248, Batch Gradient Norm after: 22.360677092630258
Epoch 4249/10000, Prediction Accuracy = 58.448%, Loss = 0.8892134189605713
Epoch: 4249, Batch Gradient Norm: 32.52097654719856
Epoch: 4249, Batch Gradient Norm after: 22.360677166875053
Epoch 4250/10000, Prediction Accuracy = 58.44%, Loss = 0.892625916004181
Epoch: 4250, Batch Gradient Norm: 31.11744896476354
Epoch: 4250, Batch Gradient Norm after: 22.360677101673964
Epoch 4251/10000, Prediction Accuracy = 58.452%, Loss = 0.8890251874923706
Epoch: 4251, Batch Gradient Norm: 32.51666722322778
Epoch: 4251, Batch Gradient Norm after: 22.360680536150742
Epoch 4252/10000, Prediction Accuracy = 58.44%, Loss = 0.892442810535431
Epoch: 4252, Batch Gradient Norm: 31.109525691621645
Epoch: 4252, Batch Gradient Norm after: 22.360677792765213
Epoch 4253/10000, Prediction Accuracy = 58.448%, Loss = 0.888841700553894
Epoch: 4253, Batch Gradient Norm: 32.51355083801031
Epoch: 4253, Batch Gradient Norm after: 22.360679499298985
Epoch 4254/10000, Prediction Accuracy = 58.44200000000001%, Loss = 0.8922653794288635
Epoch: 4254, Batch Gradient Norm: 31.102071933571157
Epoch: 4254, Batch Gradient Norm after: 22.360677095682547
Epoch 4255/10000, Prediction Accuracy = 58.448%, Loss = 0.8886525988578796
Epoch: 4255, Batch Gradient Norm: 32.506393053388244
Epoch: 4255, Batch Gradient Norm after: 22.360680117436203
Epoch 4256/10000, Prediction Accuracy = 58.452%, Loss = 0.8920806407928467
Epoch: 4256, Batch Gradient Norm: 31.09679038178993
Epoch: 4256, Batch Gradient Norm after: 22.360676891796302
Epoch 4257/10000, Prediction Accuracy = 58.452%, Loss = 0.8884647727012634
Epoch: 4257, Batch Gradient Norm: 32.50100052790864
Epoch: 4257, Batch Gradient Norm after: 22.360678933049726
Epoch 4258/10000, Prediction Accuracy = 58.45799999999999%, Loss = 0.8918985724449158
Epoch: 4258, Batch Gradient Norm: 31.08829338392105
Epoch: 4258, Batch Gradient Norm after: 22.360680183224247
Epoch 4259/10000, Prediction Accuracy = 58.456%, Loss = 0.8882756948471069
Epoch: 4259, Batch Gradient Norm: 32.49813652124991
Epoch: 4259, Batch Gradient Norm after: 22.36067717005036
Epoch 4260/10000, Prediction Accuracy = 58.452%, Loss = 0.891724705696106
Epoch: 4260, Batch Gradient Norm: 31.08135104194884
Epoch: 4260, Batch Gradient Norm after: 22.360678514933475
Epoch 4261/10000, Prediction Accuracy = 58.446000000000005%, Loss = 0.8880921602249146
Epoch: 4261, Batch Gradient Norm: 32.49541966960445
Epoch: 4261, Batch Gradient Norm after: 22.360678957005774
Epoch 4262/10000, Prediction Accuracy = 58.46%, Loss = 0.8915512084960937
Epoch: 4262, Batch Gradient Norm: 31.072632030733
Epoch: 4262, Batch Gradient Norm after: 22.360678732396835
Epoch 4263/10000, Prediction Accuracy = 58.446000000000005%, Loss = 0.8879070043563843
Epoch: 4263, Batch Gradient Norm: 32.48671688610598
Epoch: 4263, Batch Gradient Norm after: 22.360677440286903
Epoch 4264/10000, Prediction Accuracy = 58.465999999999994%, Loss = 0.8913627862930298
Epoch: 4264, Batch Gradient Norm: 31.067096223739526
Epoch: 4264, Batch Gradient Norm after: 22.360676565091794
Epoch 4265/10000, Prediction Accuracy = 58.446000000000005%, Loss = 0.8877238035202026
Epoch: 4265, Batch Gradient Norm: 32.478768549170816
Epoch: 4265, Batch Gradient Norm after: 22.360676816461798
Epoch 4266/10000, Prediction Accuracy = 58.472%, Loss = 0.8911803007125855
Epoch: 4266, Batch Gradient Norm: 31.059378646638535
Epoch: 4266, Batch Gradient Norm after: 22.36067789223509
Epoch 4267/10000, Prediction Accuracy = 58.45%, Loss = 0.8875382423400879
Epoch: 4267, Batch Gradient Norm: 32.47093390622545
Epoch: 4267, Batch Gradient Norm after: 22.360678551267917
Epoch 4268/10000, Prediction Accuracy = 58.48%, Loss = 0.8909947514533997
Epoch: 4268, Batch Gradient Norm: 31.051787715566224
Epoch: 4268, Batch Gradient Norm after: 22.360676602566
Epoch 4269/10000, Prediction Accuracy = 58.45400000000001%, Loss = 0.8873543024063111
Epoch: 4269, Batch Gradient Norm: 32.460842127813585
Epoch: 4269, Batch Gradient Norm after: 22.360677147142514
Epoch 4270/10000, Prediction Accuracy = 58.50599999999999%, Loss = 0.8908119082450867
Epoch: 4270, Batch Gradient Norm: 31.04519248131305
Epoch: 4270, Batch Gradient Norm after: 22.360677550340462
Epoch 4271/10000, Prediction Accuracy = 58.458000000000006%, Loss = 0.8871707081794739
Epoch: 4271, Batch Gradient Norm: 32.457958414876934
Epoch: 4271, Batch Gradient Norm after: 22.360677874952927
Epoch 4272/10000, Prediction Accuracy = 58.504000000000005%, Loss = 0.890625822544098
Epoch: 4272, Batch Gradient Norm: 31.03854176759135
Epoch: 4272, Batch Gradient Norm after: 22.36067712689715
Epoch 4273/10000, Prediction Accuracy = 58.465999999999994%, Loss = 0.8869876146316529
Epoch: 4273, Batch Gradient Norm: 32.45166756454559
Epoch: 4273, Batch Gradient Norm after: 22.360678845634173
Epoch 4274/10000, Prediction Accuracy = 58.507999999999996%, Loss = 0.890443754196167
Epoch: 4274, Batch Gradient Norm: 31.0292909393389
Epoch: 4274, Batch Gradient Norm after: 22.360678945886416
Epoch 4275/10000, Prediction Accuracy = 58.474000000000004%, Loss = 0.8868099093437195
Epoch: 4275, Batch Gradient Norm: 32.444519620908714
Epoch: 4275, Batch Gradient Norm after: 22.360679371721297
Epoch 4276/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.8902633905410766
Epoch: 4276, Batch Gradient Norm: 31.022113140977023
Epoch: 4276, Batch Gradient Norm after: 22.360676461566072
Epoch 4277/10000, Prediction Accuracy = 58.486000000000004%, Loss = 0.8866280436515808
Epoch: 4277, Batch Gradient Norm: 32.44025847735823
Epoch: 4277, Batch Gradient Norm after: 22.36067959649799
Epoch 4278/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.8900817036628723
Epoch: 4278, Batch Gradient Norm: 31.014543323739996
Epoch: 4278, Batch Gradient Norm after: 22.3606785057547
Epoch 4279/10000, Prediction Accuracy = 58.492%, Loss = 0.8864449739456177
Epoch: 4279, Batch Gradient Norm: 32.43580136249491
Epoch: 4279, Batch Gradient Norm after: 22.360679762549307
Epoch 4280/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8899012565612793
Epoch: 4280, Batch Gradient Norm: 31.008200001512463
Epoch: 4280, Batch Gradient Norm after: 22.36067805678587
Epoch 4281/10000, Prediction Accuracy = 58.49399999999999%, Loss = 0.8862574338912964
Epoch: 4281, Batch Gradient Norm: 32.42629529815413
Epoch: 4281, Batch Gradient Norm after: 22.3606785913456
Epoch 4282/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8897114157676697
Epoch: 4282, Batch Gradient Norm: 31.000554997381272
Epoch: 4282, Batch Gradient Norm after: 22.360677126876457
Epoch 4283/10000, Prediction Accuracy = 58.49399999999999%, Loss = 0.886075222492218
Epoch: 4283, Batch Gradient Norm: 32.42824770567397
Epoch: 4283, Batch Gradient Norm after: 22.360678768378683
Epoch 4284/10000, Prediction Accuracy = 58.51800000000001%, Loss = 0.889548373222351
Epoch: 4284, Batch Gradient Norm: 30.99184947612245
Epoch: 4284, Batch Gradient Norm after: 22.36067835690439
Epoch 4285/10000, Prediction Accuracy = 58.49399999999999%, Loss = 0.8858906507492066
Epoch: 4285, Batch Gradient Norm: 32.42127772453299
Epoch: 4285, Batch Gradient Norm after: 22.360677128071956
Epoch 4286/10000, Prediction Accuracy = 58.519999999999996%, Loss = 0.8893714308738708
Epoch: 4286, Batch Gradient Norm: 30.987311235050147
Epoch: 4286, Batch Gradient Norm after: 22.360678645985324
Epoch 4287/10000, Prediction Accuracy = 58.49399999999999%, Loss = 0.8857090473175049
Epoch: 4287, Batch Gradient Norm: 32.41337611507635
Epoch: 4287, Batch Gradient Norm after: 22.36067785609396
Epoch 4288/10000, Prediction Accuracy = 58.50600000000001%, Loss = 0.8891788721084595
Epoch: 4288, Batch Gradient Norm: 30.981648963752292
Epoch: 4288, Batch Gradient Norm after: 22.360677348212132
Epoch 4289/10000, Prediction Accuracy = 58.507999999999996%, Loss = 0.8855231761932373
Epoch: 4289, Batch Gradient Norm: 32.40414360756574
Epoch: 4289, Batch Gradient Norm after: 22.360678184763362
Epoch 4290/10000, Prediction Accuracy = 58.52%, Loss = 0.8889947056770324
Epoch: 4290, Batch Gradient Norm: 30.97519422282534
Epoch: 4290, Batch Gradient Norm after: 22.36067616896111
Epoch 4291/10000, Prediction Accuracy = 58.516%, Loss = 0.8853448748588562
Epoch: 4291, Batch Gradient Norm: 32.39841152711853
Epoch: 4291, Batch Gradient Norm after: 22.36067822382787
Epoch 4292/10000, Prediction Accuracy = 58.50599999999999%, Loss = 0.8888146638870239
Epoch: 4292, Batch Gradient Norm: 30.967261201957662
Epoch: 4292, Batch Gradient Norm after: 22.36067675753732
Epoch 4293/10000, Prediction Accuracy = 58.517999999999994%, Loss = 0.8851640462875366
Epoch: 4293, Batch Gradient Norm: 32.390695286558056
Epoch: 4293, Batch Gradient Norm after: 22.360677321354014
Epoch 4294/10000, Prediction Accuracy = 58.50600000000001%, Loss = 0.8886340141296387
Epoch: 4294, Batch Gradient Norm: 30.960056102481147
Epoch: 4294, Batch Gradient Norm after: 22.36067629081797
Epoch 4295/10000, Prediction Accuracy = 58.52%, Loss = 0.8849815607070923
Epoch: 4295, Batch Gradient Norm: 32.38023762990868
Epoch: 4295, Batch Gradient Norm after: 22.360677487212087
Epoch 4296/10000, Prediction Accuracy = 58.50999999999999%, Loss = 0.888450813293457
Epoch: 4296, Batch Gradient Norm: 30.952583470989406
Epoch: 4296, Batch Gradient Norm after: 22.36067797216325
Epoch 4297/10000, Prediction Accuracy = 58.519999999999996%, Loss = 0.8848052024841309
Epoch: 4297, Batch Gradient Norm: 32.37406364105534
Epoch: 4297, Batch Gradient Norm after: 22.36067848087536
Epoch 4298/10000, Prediction Accuracy = 58.516000000000005%, Loss = 0.8882686614990234
Epoch: 4298, Batch Gradient Norm: 30.945545433656637
Epoch: 4298, Batch Gradient Norm after: 22.360678075689457
Epoch 4299/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8846197724342346
Epoch: 4299, Batch Gradient Norm: 32.36236871609955
Epoch: 4299, Batch Gradient Norm after: 22.3606798701913
Epoch 4300/10000, Prediction Accuracy = 58.51800000000001%, Loss = 0.8880770683288575
Epoch: 4300, Batch Gradient Norm: 30.939981033914485
Epoch: 4300, Batch Gradient Norm after: 22.360676088726194
Epoch 4301/10000, Prediction Accuracy = 58.528%, Loss = 0.8844436049461365
Epoch: 4301, Batch Gradient Norm: 32.352265527210314
Epoch: 4301, Batch Gradient Norm after: 22.36067758805783
Epoch 4302/10000, Prediction Accuracy = 58.510000000000005%, Loss = 0.8878867626190186
Epoch: 4302, Batch Gradient Norm: 30.93446241536967
Epoch: 4302, Batch Gradient Norm after: 22.36067754707999
Epoch 4303/10000, Prediction Accuracy = 58.541999999999994%, Loss = 0.8842653036117554
Epoch: 4303, Batch Gradient Norm: 32.33950157808041
Epoch: 4303, Batch Gradient Norm after: 22.360677670260497
Epoch 4304/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.8876888513565063
Epoch: 4304, Batch Gradient Norm: 30.93063311832132
Epoch: 4304, Batch Gradient Norm after: 22.360677252307124
Epoch 4305/10000, Prediction Accuracy = 58.54600000000001%, Loss = 0.8840888381004334
Epoch: 4305, Batch Gradient Norm: 32.32464318469329
Epoch: 4305, Batch Gradient Norm after: 22.360679764280825
Epoch 4306/10000, Prediction Accuracy = 58.498000000000005%, Loss = 0.8874877333641052
Epoch: 4306, Batch Gradient Norm: 30.924441838447898
Epoch: 4306, Batch Gradient Norm after: 22.360678301074593
Epoch 4307/10000, Prediction Accuracy = 58.54%, Loss = 0.8839122653007507
Epoch: 4307, Batch Gradient Norm: 32.3107539091997
Epoch: 4307, Batch Gradient Norm after: 22.360680036710487
Epoch 4308/10000, Prediction Accuracy = 58.5%, Loss = 0.8872878313064575
Epoch: 4308, Batch Gradient Norm: 30.919430089719118
Epoch: 4308, Batch Gradient Norm after: 22.36067820986624
Epoch 4309/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8837377190589905
Epoch: 4309, Batch Gradient Norm: 32.29758435638268
Epoch: 4309, Batch Gradient Norm after: 22.360676568935727
Epoch 4310/10000, Prediction Accuracy = 58.507999999999996%, Loss = 0.8870906472206116
Epoch: 4310, Batch Gradient Norm: 30.913422828983386
Epoch: 4310, Batch Gradient Norm after: 22.36067939278958
Epoch 4311/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8835599303245545
Epoch: 4311, Batch Gradient Norm: 32.288968736723604
Epoch: 4311, Batch Gradient Norm after: 22.360678191699407
Epoch 4312/10000, Prediction Accuracy = 58.508%, Loss = 0.8869021654129028
Epoch: 4312, Batch Gradient Norm: 30.9076754548529
Epoch: 4312, Batch Gradient Norm after: 22.36067789396951
Epoch 4313/10000, Prediction Accuracy = 58.538%, Loss = 0.8833827018737793
Epoch: 4313, Batch Gradient Norm: 32.27719292892725
Epoch: 4313, Batch Gradient Norm after: 22.360679582881122
Epoch 4314/10000, Prediction Accuracy = 58.51400000000001%, Loss = 0.8867154598236084
Epoch: 4314, Batch Gradient Norm: 30.900518064373415
Epoch: 4314, Batch Gradient Norm after: 22.36067713036794
Epoch 4315/10000, Prediction Accuracy = 58.532%, Loss = 0.8832042455673218
Epoch: 4315, Batch Gradient Norm: 32.269022873242704
Epoch: 4315, Batch Gradient Norm after: 22.36067797386116
Epoch 4316/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.8865279078483581
Epoch: 4316, Batch Gradient Norm: 30.892843887110068
Epoch: 4316, Batch Gradient Norm after: 22.360677662241095
Epoch 4317/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.8830246567726135
Epoch: 4317, Batch Gradient Norm: 32.26111837562203
Epoch: 4317, Batch Gradient Norm after: 22.36067821415619
Epoch 4318/10000, Prediction Accuracy = 58.51800000000001%, Loss = 0.8863471269607544
Epoch: 4318, Batch Gradient Norm: 30.88734137280129
Epoch: 4318, Batch Gradient Norm after: 22.36067919663787
Epoch 4319/10000, Prediction Accuracy = 58.538%, Loss = 0.8828473329544068
Epoch: 4319, Batch Gradient Norm: 32.253132890057756
Epoch: 4319, Batch Gradient Norm after: 22.360677470617357
Epoch 4320/10000, Prediction Accuracy = 58.532%, Loss = 0.8861750483512878
Epoch: 4320, Batch Gradient Norm: 30.87830454136177
Epoch: 4320, Batch Gradient Norm after: 22.360678297156344
Epoch 4321/10000, Prediction Accuracy = 58.54%, Loss = 0.8826692223548889
Epoch: 4321, Batch Gradient Norm: 32.25079293884376
Epoch: 4321, Batch Gradient Norm after: 22.360678982552635
Epoch 4322/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.88599933385849
Epoch: 4322, Batch Gradient Norm: 30.869363535311347
Epoch: 4322, Batch Gradient Norm after: 22.36067867162446
Epoch 4323/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8824878215789795
Epoch: 4323, Batch Gradient Norm: 32.24693594024308
Epoch: 4323, Batch Gradient Norm after: 22.36067852667546
Epoch 4324/10000, Prediction Accuracy = 58.53000000000001%, Loss = 0.8858219146728515
Epoch: 4324, Batch Gradient Norm: 30.86248837773084
Epoch: 4324, Batch Gradient Norm after: 22.36067882501926
Epoch 4325/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8823035836219788
Epoch: 4325, Batch Gradient Norm: 32.246283353658356
Epoch: 4325, Batch Gradient Norm after: 22.36067931337192
Epoch 4326/10000, Prediction Accuracy = 58.529999999999994%, Loss = 0.885658323764801
Epoch: 4326, Batch Gradient Norm: 30.855225963964617
Epoch: 4326, Batch Gradient Norm after: 22.36067868167845
Epoch 4327/10000, Prediction Accuracy = 58.544000000000004%, Loss = 0.8821213603019714
Epoch: 4327, Batch Gradient Norm: 32.243622650287044
Epoch: 4327, Batch Gradient Norm after: 22.360678786465748
Epoch 4328/10000, Prediction Accuracy = 58.54%, Loss = 0.8854990720748901
Epoch: 4328, Batch Gradient Norm: 30.846317799084076
Epoch: 4328, Batch Gradient Norm after: 22.360678794424796
Epoch 4329/10000, Prediction Accuracy = 58.54%, Loss = 0.8819397568702698
Epoch: 4329, Batch Gradient Norm: 32.24210726846414
Epoch: 4329, Batch Gradient Norm after: 22.360679546938353
Epoch 4330/10000, Prediction Accuracy = 58.52600000000001%, Loss = 0.8853323101997376
Epoch: 4330, Batch Gradient Norm: 30.83832996087331
Epoch: 4330, Batch Gradient Norm after: 22.36067679986486
Epoch 4331/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.8817574381828308
Epoch: 4331, Batch Gradient Norm: 32.23931218946183
Epoch: 4331, Batch Gradient Norm after: 22.360677340315366
Epoch 4332/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.885164737701416
Epoch: 4332, Batch Gradient Norm: 30.832496727412533
Epoch: 4332, Batch Gradient Norm after: 22.360679090018042
Epoch 4333/10000, Prediction Accuracy = 58.54%, Loss = 0.8815744400024415
Epoch: 4333, Batch Gradient Norm: 32.236718994273076
Epoch: 4333, Batch Gradient Norm after: 22.36067834228364
Epoch 4334/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.884999692440033
Epoch: 4334, Batch Gradient Norm: 30.825744383052722
Epoch: 4334, Batch Gradient Norm after: 22.3606774735154
Epoch 4335/10000, Prediction Accuracy = 58.541999999999994%, Loss = 0.8813953280448914
Epoch: 4335, Batch Gradient Norm: 32.22892246587714
Epoch: 4335, Batch Gradient Norm after: 22.360679512017505
Epoch 4336/10000, Prediction Accuracy = 58.528000000000006%, Loss = 0.8848163485527039
Epoch: 4336, Batch Gradient Norm: 30.81976687800746
Epoch: 4336, Batch Gradient Norm after: 22.36067931176305
Epoch 4337/10000, Prediction Accuracy = 58.536%, Loss = 0.881216847896576
Epoch: 4337, Batch Gradient Norm: 32.22515003449804
Epoch: 4337, Batch Gradient Norm after: 22.360678382340055
Epoch 4338/10000, Prediction Accuracy = 58.528%, Loss = 0.8846407055854797
Epoch: 4338, Batch Gradient Norm: 30.810377235895313
Epoch: 4338, Batch Gradient Norm after: 22.360678694063466
Epoch 4339/10000, Prediction Accuracy = 58.536%, Loss = 0.8810385465621948
Epoch: 4339, Batch Gradient Norm: 32.220800543560884
Epoch: 4339, Batch Gradient Norm after: 22.360680208995078
Epoch 4340/10000, Prediction Accuracy = 58.528%, Loss = 0.8844656109809875
Epoch: 4340, Batch Gradient Norm: 30.803797397075282
Epoch: 4340, Batch Gradient Norm after: 22.360679207388266
Epoch 4341/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8808579325675965
Epoch: 4341, Batch Gradient Norm: 32.213695324414765
Epoch: 4341, Batch Gradient Norm after: 22.36067860090928
Epoch 4342/10000, Prediction Accuracy = 58.52%, Loss = 0.8842925906181336
Epoch: 4342, Batch Gradient Norm: 30.79525149838486
Epoch: 4342, Batch Gradient Norm after: 22.360676081352057
Epoch 4343/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.8806820273399353
Epoch: 4343, Batch Gradient Norm: 32.20814242868205
Epoch: 4343, Batch Gradient Norm after: 22.36067957317056
Epoch 4344/10000, Prediction Accuracy = 58.52199999999999%, Loss = 0.8841211080551148
Epoch: 4344, Batch Gradient Norm: 30.788885620887182
Epoch: 4344, Batch Gradient Norm after: 22.360676438840446
Epoch 4345/10000, Prediction Accuracy = 58.528%, Loss = 0.880501103401184
Epoch: 4345, Batch Gradient Norm: 32.205043599599776
Epoch: 4345, Batch Gradient Norm after: 22.360680036589166
Epoch 4346/10000, Prediction Accuracy = 58.524%, Loss = 0.8839504003524781
Epoch: 4346, Batch Gradient Norm: 30.781342870711242
Epoch: 4346, Batch Gradient Norm after: 22.360679250121535
Epoch 4347/10000, Prediction Accuracy = 58.53399999999999%, Loss = 0.8803191781044006
Epoch: 4347, Batch Gradient Norm: 32.19990400136988
Epoch: 4347, Batch Gradient Norm after: 22.360677182774346
Epoch 4348/10000, Prediction Accuracy = 58.524%, Loss = 0.883780586719513
Epoch: 4348, Batch Gradient Norm: 30.773304819189246
Epoch: 4348, Batch Gradient Norm after: 22.36067825681169
Epoch 4349/10000, Prediction Accuracy = 58.53599999999999%, Loss = 0.8801437735557556
Epoch: 4349, Batch Gradient Norm: 32.191979088506926
Epoch: 4349, Batch Gradient Norm after: 22.360677424460338
Epoch 4350/10000, Prediction Accuracy = 58.524%, Loss = 0.8836073637008667
Epoch: 4350, Batch Gradient Norm: 30.767130892503953
Epoch: 4350, Batch Gradient Norm after: 22.360677027315745
Epoch 4351/10000, Prediction Accuracy = 58.54%, Loss = 0.8799686551094055
Epoch: 4351, Batch Gradient Norm: 32.186304913101026
Epoch: 4351, Batch Gradient Norm after: 22.360679109877058
Epoch 4352/10000, Prediction Accuracy = 58.51399999999999%, Loss = 0.8834259867668152
Epoch: 4352, Batch Gradient Norm: 30.760813842707353
Epoch: 4352, Batch Gradient Norm after: 22.36067952316724
Epoch 4353/10000, Prediction Accuracy = 58.54200000000001%, Loss = 0.8797905206680298
Epoch: 4353, Batch Gradient Norm: 32.17771204326046
Epoch: 4353, Batch Gradient Norm after: 22.36067864502305
Epoch 4354/10000, Prediction Accuracy = 58.510000000000005%, Loss = 0.8832505941390991
Epoch: 4354, Batch Gradient Norm: 30.751979656030198
Epoch: 4354, Batch Gradient Norm after: 22.360675641721468
Epoch 4355/10000, Prediction Accuracy = 58.54%, Loss = 0.8796144247055053
Epoch: 4355, Batch Gradient Norm: 32.17944302079434
Epoch: 4355, Batch Gradient Norm after: 22.360677788811852
Epoch 4356/10000, Prediction Accuracy = 58.516000000000005%, Loss = 0.8830886244773865
Epoch: 4356, Batch Gradient Norm: 30.74396265581203
Epoch: 4356, Batch Gradient Norm after: 22.360676612683996
Epoch 4357/10000, Prediction Accuracy = 58.54600000000001%, Loss = 0.8794354915618896
Epoch: 4357, Batch Gradient Norm: 32.17537959702624
Epoch: 4357, Batch Gradient Norm after: 22.36067768446639
Epoch 4358/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.8829196214675903
Epoch: 4358, Batch Gradient Norm: 30.73611562073799
Epoch: 4358, Batch Gradient Norm after: 22.36067508314541
Epoch 4359/10000, Prediction Accuracy = 58.548%, Loss = 0.8792534589767456
Epoch: 4359, Batch Gradient Norm: 32.17093494682305
Epoch: 4359, Batch Gradient Norm after: 22.360676710033516
Epoch 4360/10000, Prediction Accuracy = 58.525999999999996%, Loss = 0.8827475786209107
Epoch: 4360, Batch Gradient Norm: 30.72783186900844
Epoch: 4360, Batch Gradient Norm after: 22.360677173971887
Epoch 4361/10000, Prediction Accuracy = 58.54600000000001%, Loss = 0.8790765643119812
Epoch: 4361, Batch Gradient Norm: 32.16196561109584
Epoch: 4361, Batch Gradient Norm after: 22.360676181020985
Epoch 4362/10000, Prediction Accuracy = 58.532000000000004%, Loss = 0.8825682282447815
Epoch: 4362, Batch Gradient Norm: 30.719597693551524
Epoch: 4362, Batch Gradient Norm after: 22.360677827866567
Epoch 4363/10000, Prediction Accuracy = 58.544000000000004%, Loss = 0.8789022922515869
Epoch: 4363, Batch Gradient Norm: 32.152283692672
Epoch: 4363, Batch Gradient Norm after: 22.360675453989217
Epoch 4364/10000, Prediction Accuracy = 58.528%, Loss = 0.8823815584182739
Epoch: 4364, Batch Gradient Norm: 30.71280855319914
Epoch: 4364, Batch Gradient Norm after: 22.360676379337487
Epoch 4365/10000, Prediction Accuracy = 58.54600000000001%, Loss = 0.8787289023399353
Epoch: 4365, Batch Gradient Norm: 32.145396205214176
Epoch: 4365, Batch Gradient Norm after: 22.360677196859903
Epoch 4366/10000, Prediction Accuracy = 58.53599999999999%, Loss = 0.8822024822235107
Epoch: 4366, Batch Gradient Norm: 30.707502011105554
Epoch: 4366, Batch Gradient Norm after: 22.36067901983974
Epoch 4367/10000, Prediction Accuracy = 58.553999999999995%, Loss = 0.8785502910614014
Epoch: 4367, Batch Gradient Norm: 32.134441194182706
Epoch: 4367, Batch Gradient Norm after: 22.36067686367565
Epoch 4368/10000, Prediction Accuracy = 58.55%, Loss = 0.8820154070854187
Epoch: 4368, Batch Gradient Norm: 30.700990994319156
Epoch: 4368, Batch Gradient Norm after: 22.36067737479205
Epoch 4369/10000, Prediction Accuracy = 58.55000000000001%, Loss = 0.8783722758293152
Epoch: 4369, Batch Gradient Norm: 32.12389581256333
Epoch: 4369, Batch Gradient Norm after: 22.360676862324414
Epoch 4370/10000, Prediction Accuracy = 58.548%, Loss = 0.8818273782730103
Epoch: 4370, Batch Gradient Norm: 30.694369124328592
Epoch: 4370, Batch Gradient Norm after: 22.360679487062757
Epoch 4371/10000, Prediction Accuracy = 58.544%, Loss = 0.8781998872756958
Epoch: 4371, Batch Gradient Norm: 32.11433206970809
Epoch: 4371, Batch Gradient Norm after: 22.36067678696713
Epoch 4372/10000, Prediction Accuracy = 58.55%, Loss = 0.8816423058509827
Epoch: 4372, Batch Gradient Norm: 30.68832179955075
Epoch: 4372, Batch Gradient Norm after: 22.360677947373002
Epoch 4373/10000, Prediction Accuracy = 58.56600000000001%, Loss = 0.8780265688896179
Epoch: 4373, Batch Gradient Norm: 32.106869673241924
Epoch: 4373, Batch Gradient Norm after: 22.3606783996245
Epoch 4374/10000, Prediction Accuracy = 58.57000000000001%, Loss = 0.8814653038978577
Epoch: 4374, Batch Gradient Norm: 30.68033245391915
Epoch: 4374, Batch Gradient Norm after: 22.36067659330012
Epoch 4375/10000, Prediction Accuracy = 58.572%, Loss = 0.8778527855873108
Epoch: 4375, Batch Gradient Norm: 32.099225671300026
Epoch: 4375, Batch Gradient Norm after: 22.360679874289957
Epoch 4376/10000, Prediction Accuracy = 58.565999999999995%, Loss = 0.8812920928001404
Epoch: 4376, Batch Gradient Norm: 30.6740130801523
Epoch: 4376, Batch Gradient Norm after: 22.360676812148284
Epoch 4377/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.8776770114898682
Epoch: 4377, Batch Gradient Norm: 32.09025273902545
Epoch: 4377, Batch Gradient Norm after: 22.360679119350117
Epoch 4378/10000, Prediction Accuracy = 58.577999999999996%, Loss = 0.8811183452606202
Epoch: 4378, Batch Gradient Norm: 30.66833805438814
Epoch: 4378, Batch Gradient Norm after: 22.360680558732717
Epoch 4379/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.8774992108345032
Epoch: 4379, Batch Gradient Norm: 32.08376518423131
Epoch: 4379, Batch Gradient Norm after: 22.360676920396454
Epoch 4380/10000, Prediction Accuracy = 58.568%, Loss = 0.8809417605400085
Epoch: 4380, Batch Gradient Norm: 30.662319403126507
Epoch: 4380, Batch Gradient Norm after: 22.360677739462783
Epoch 4381/10000, Prediction Accuracy = 58.59599999999999%, Loss = 0.8773223042488099
Epoch: 4381, Batch Gradient Norm: 32.08306624826036
Epoch: 4381, Batch Gradient Norm after: 22.360678048236306
Epoch 4382/10000, Prediction Accuracy = 58.581999999999994%, Loss = 0.8807786583900452
Epoch: 4382, Batch Gradient Norm: 30.653402383930743
Epoch: 4382, Batch Gradient Norm after: 22.360678718148193
Epoch 4383/10000, Prediction Accuracy = 58.592%, Loss = 0.8771549463272095
Epoch: 4383, Batch Gradient Norm: 32.07763375499712
Epoch: 4383, Batch Gradient Norm after: 22.36067641110536
Epoch 4384/10000, Prediction Accuracy = 58.58%, Loss = 0.8806152939796448
Epoch: 4384, Batch Gradient Norm: 30.646971640077584
Epoch: 4384, Batch Gradient Norm after: 22.36067909458486
Epoch 4385/10000, Prediction Accuracy = 58.592%, Loss = 0.8769826889038086
Epoch: 4385, Batch Gradient Norm: 32.06999045420966
Epoch: 4385, Batch Gradient Norm after: 22.360677692965695
Epoch 4386/10000, Prediction Accuracy = 58.584%, Loss = 0.8804354548454285
Epoch: 4386, Batch Gradient Norm: 30.643020197157284
Epoch: 4386, Batch Gradient Norm after: 22.360677464541173
Epoch 4387/10000, Prediction Accuracy = 58.602%, Loss = 0.8768054246902466
Epoch: 4387, Batch Gradient Norm: 32.0658235283377
Epoch: 4387, Batch Gradient Norm after: 22.36067582214419
Epoch 4388/10000, Prediction Accuracy = 58.584%, Loss = 0.8802591443061829
Epoch: 4388, Batch Gradient Norm: 30.634796947432733
Epoch: 4388, Batch Gradient Norm after: 22.360678537820906
Epoch 4389/10000, Prediction Accuracy = 58.592000000000006%, Loss = 0.876627254486084
Epoch: 4389, Batch Gradient Norm: 32.05518327510866
Epoch: 4389, Batch Gradient Norm after: 22.360675987387758
Epoch 4390/10000, Prediction Accuracy = 58.598%, Loss = 0.8800846338272095
Epoch: 4390, Batch Gradient Norm: 30.630206389901335
Epoch: 4390, Batch Gradient Norm after: 22.360678426731344
Epoch 4391/10000, Prediction Accuracy = 58.605999999999995%, Loss = 0.8764580011367797
Epoch: 4391, Batch Gradient Norm: 32.045022392751115
Epoch: 4391, Batch Gradient Norm after: 22.360676099007186
Epoch 4392/10000, Prediction Accuracy = 58.588%, Loss = 0.8798923015594482
Epoch: 4392, Batch Gradient Norm: 30.62250686713072
Epoch: 4392, Batch Gradient Norm after: 22.360679450257503
Epoch 4393/10000, Prediction Accuracy = 58.61%, Loss = 0.8762930512428284
Epoch: 4393, Batch Gradient Norm: 32.0338383592436
Epoch: 4393, Batch Gradient Norm after: 22.36067522130406
Epoch 4394/10000, Prediction Accuracy = 58.588%, Loss = 0.8797096848487854
Epoch: 4394, Batch Gradient Norm: 30.61650043507223
Epoch: 4394, Batch Gradient Norm after: 22.360679373212967
Epoch 4395/10000, Prediction Accuracy = 58.60799999999999%, Loss = 0.8761207818984985
Epoch: 4395, Batch Gradient Norm: 32.028922505781566
Epoch: 4395, Batch Gradient Norm after: 22.360675978035154
Epoch 4396/10000, Prediction Accuracy = 58.59400000000001%, Loss = 0.8795393347740174
Epoch: 4396, Batch Gradient Norm: 30.609293334015504
Epoch: 4396, Batch Gradient Norm after: 22.360679973865047
Epoch 4397/10000, Prediction Accuracy = 58.605999999999995%, Loss = 0.8759435296058655
Epoch: 4397, Batch Gradient Norm: 32.02436329153437
Epoch: 4397, Batch Gradient Norm after: 22.360676112184965
Epoch 4398/10000, Prediction Accuracy = 58.6%, Loss = 0.8793743968009948
Epoch: 4398, Batch Gradient Norm: 30.6003312381586
Epoch: 4398, Batch Gradient Norm after: 22.36067955985005
Epoch 4399/10000, Prediction Accuracy = 58.61999999999999%, Loss = 0.8757727384567261
Epoch: 4399, Batch Gradient Norm: 32.02251486713251
Epoch: 4399, Batch Gradient Norm after: 22.360677169340075
Epoch 4400/10000, Prediction Accuracy = 58.593999999999994%, Loss = 0.8792163133621216
Epoch: 4400, Batch Gradient Norm: 30.59089488914634
Epoch: 4400, Batch Gradient Norm after: 22.360677264269267
Epoch 4401/10000, Prediction Accuracy = 58.622%, Loss = 0.8755930900573731
Epoch: 4401, Batch Gradient Norm: 32.018878685915844
Epoch: 4401, Batch Gradient Norm after: 22.360678310339672
Epoch 4402/10000, Prediction Accuracy = 58.602%, Loss = 0.8790588140487671
Epoch: 4402, Batch Gradient Norm: 30.583662850847602
Epoch: 4402, Batch Gradient Norm after: 22.360677577485728
Epoch 4403/10000, Prediction Accuracy = 58.624%, Loss = 0.8754173398017884
Epoch: 4403, Batch Gradient Norm: 32.01650853827706
Epoch: 4403, Batch Gradient Norm after: 22.360676353007314
Epoch 4404/10000, Prediction Accuracy = 58.608000000000004%, Loss = 0.8789035558700562
Epoch: 4404, Batch Gradient Norm: 30.57630392733894
Epoch: 4404, Batch Gradient Norm after: 22.360680198664493
Epoch 4405/10000, Prediction Accuracy = 58.632000000000005%, Loss = 0.8752365946769715
Epoch: 4405, Batch Gradient Norm: 32.012370334702474
Epoch: 4405, Batch Gradient Norm after: 22.360675993190604
Epoch 4406/10000, Prediction Accuracy = 58.612%, Loss = 0.8787387371063232
Epoch: 4406, Batch Gradient Norm: 30.56803257593221
Epoch: 4406, Batch Gradient Norm after: 22.360680571519964
Epoch 4407/10000, Prediction Accuracy = 58.63000000000001%, Loss = 0.8750638723373413
Epoch: 4407, Batch Gradient Norm: 32.002510323265035
Epoch: 4407, Batch Gradient Norm after: 22.360678184719546
Epoch 4408/10000, Prediction Accuracy = 58.61%, Loss = 0.8785600662231445
Epoch: 4408, Batch Gradient Norm: 30.56099977317555
Epoch: 4408, Batch Gradient Norm after: 22.360680570235537
Epoch 4409/10000, Prediction Accuracy = 58.632000000000005%, Loss = 0.8748912572860718
Epoch: 4409, Batch Gradient Norm: 31.99429489442183
Epoch: 4409, Batch Gradient Norm after: 22.360677846168905
Epoch 4410/10000, Prediction Accuracy = 58.61%, Loss = 0.8783945083618164
Epoch: 4410, Batch Gradient Norm: 30.55475724454551
Epoch: 4410, Batch Gradient Norm after: 22.360679338209486
Epoch 4411/10000, Prediction Accuracy = 58.626%, Loss = 0.8747233510017395
Epoch: 4411, Batch Gradient Norm: 31.981376758695554
Epoch: 4411, Batch Gradient Norm after: 22.360675386724257
Epoch 4412/10000, Prediction Accuracy = 58.612%, Loss = 0.8782071471214294
Epoch: 4412, Batch Gradient Norm: 30.548328883560288
Epoch: 4412, Batch Gradient Norm after: 22.360679467362612
Epoch 4413/10000, Prediction Accuracy = 58.634%, Loss = 0.8745537996292114
Epoch: 4413, Batch Gradient Norm: 31.97005583667534
Epoch: 4413, Batch Gradient Norm after: 22.36067593906593
Epoch 4414/10000, Prediction Accuracy = 58.626%, Loss = 0.8780231833457947
Epoch: 4414, Batch Gradient Norm: 30.5396896777368
Epoch: 4414, Batch Gradient Norm after: 22.360679599203408
Epoch 4415/10000, Prediction Accuracy = 58.636%, Loss = 0.8743802070617676
Epoch: 4415, Batch Gradient Norm: 31.956850810848135
Epoch: 4415, Batch Gradient Norm after: 22.360677494983413
Epoch 4416/10000, Prediction Accuracy = 58.61800000000001%, Loss = 0.8778403043746948
Epoch: 4416, Batch Gradient Norm: 30.53637032403914
Epoch: 4416, Batch Gradient Norm after: 22.360679836590982
Epoch 4417/10000, Prediction Accuracy = 58.64%, Loss = 0.8742090940475464
Epoch: 4417, Batch Gradient Norm: 31.946727778449237
Epoch: 4417, Batch Gradient Norm after: 22.36067631989231
Epoch 4418/10000, Prediction Accuracy = 58.61800000000001%, Loss = 0.8776602029800415
Epoch: 4418, Batch Gradient Norm: 30.531107267233836
Epoch: 4418, Batch Gradient Norm after: 22.360676689771765
Epoch 4419/10000, Prediction Accuracy = 58.64399999999999%, Loss = 0.8740401864051819
Epoch: 4419, Batch Gradient Norm: 31.933397316335007
Epoch: 4419, Batch Gradient Norm after: 22.360677983285775
Epoch 4420/10000, Prediction Accuracy = 58.617999999999995%, Loss = 0.8774765133857727
Epoch: 4420, Batch Gradient Norm: 30.526537124013597
Epoch: 4420, Batch Gradient Norm after: 22.360677840698163
Epoch 4421/10000, Prediction Accuracy = 58.64200000000001%, Loss = 0.8738738894462585
Epoch: 4421, Batch Gradient Norm: 31.920765032300956
Epoch: 4421, Batch Gradient Norm after: 22.360676797480913
Epoch 4422/10000, Prediction Accuracy = 58.63199999999999%, Loss = 0.8772955536842346
Epoch: 4422, Batch Gradient Norm: 30.520311364962726
Epoch: 4422, Batch Gradient Norm after: 22.360678486638108
Epoch 4423/10000, Prediction Accuracy = 58.64%, Loss = 0.8737030029296875
Epoch: 4423, Batch Gradient Norm: 31.91477206278001
Epoch: 4423, Batch Gradient Norm after: 22.36067792973675
Epoch 4424/10000, Prediction Accuracy = 58.64200000000001%, Loss = 0.8771192550659179
Epoch: 4424, Batch Gradient Norm: 30.51237021056407
Epoch: 4424, Batch Gradient Norm after: 22.360681505102608
Epoch 4425/10000, Prediction Accuracy = 58.638%, Loss = 0.8735316753387451
Epoch: 4425, Batch Gradient Norm: 31.90952231476696
Epoch: 4425, Batch Gradient Norm after: 22.3606768901107
Epoch 4426/10000, Prediction Accuracy = 58.629999999999995%, Loss = 0.8769485950469971
Epoch: 4426, Batch Gradient Norm: 30.503061371158594
Epoch: 4426, Batch Gradient Norm after: 22.3606794590777
Epoch 4427/10000, Prediction Accuracy = 58.64200000000001%, Loss = 0.873360550403595
Epoch: 4427, Batch Gradient Norm: 31.901088492915658
Epoch: 4427, Batch Gradient Norm after: 22.360677095160355
Epoch 4428/10000, Prediction Accuracy = 58.629999999999995%, Loss = 0.8767763257026673
Epoch: 4428, Batch Gradient Norm: 30.49557229064154
Epoch: 4428, Batch Gradient Norm after: 22.3606777250783
Epoch 4429/10000, Prediction Accuracy = 58.638%, Loss = 0.8731936931610107
Epoch: 4429, Batch Gradient Norm: 31.89943319269999
Epoch: 4429, Batch Gradient Norm after: 22.36067766099885
Epoch 4430/10000, Prediction Accuracy = 58.644000000000005%, Loss = 0.8766153216361999
Epoch: 4430, Batch Gradient Norm: 30.48781298413595
Epoch: 4430, Batch Gradient Norm after: 22.36067899990539
Epoch 4431/10000, Prediction Accuracy = 58.64%, Loss = 0.8730162382125854
Epoch: 4431, Batch Gradient Norm: 31.891890396760566
Epoch: 4431, Batch Gradient Norm after: 22.360677332300778
Epoch 4432/10000, Prediction Accuracy = 58.648%, Loss = 0.876444959640503
Epoch: 4432, Batch Gradient Norm: 30.479942771063854
Epoch: 4432, Batch Gradient Norm after: 22.360679219450116
Epoch 4433/10000, Prediction Accuracy = 58.634%, Loss = 0.872844398021698
Epoch: 4433, Batch Gradient Norm: 31.882345040131565
Epoch: 4433, Batch Gradient Norm after: 22.360677989396955
Epoch 4434/10000, Prediction Accuracy = 58.64399999999999%, Loss = 0.8762566685676575
Epoch: 4434, Batch Gradient Norm: 30.4747335572341
Epoch: 4434, Batch Gradient Norm after: 22.36067892549013
Epoch 4435/10000, Prediction Accuracy = 58.632000000000005%, Loss = 0.8726702690124511
Epoch: 4435, Batch Gradient Norm: 31.871508984554506
Epoch: 4435, Batch Gradient Norm after: 22.360677826126036
Epoch 4436/10000, Prediction Accuracy = 58.634%, Loss = 0.8760815024375915
Epoch: 4436, Batch Gradient Norm: 30.470022260307093
Epoch: 4436, Batch Gradient Norm after: 22.36067793433939
Epoch 4437/10000, Prediction Accuracy = 58.64%, Loss = 0.8725000739097595
Epoch: 4437, Batch Gradient Norm: 31.859469411940694
Epoch: 4437, Batch Gradient Norm after: 22.360677543519998
Epoch 4438/10000, Prediction Accuracy = 58.644000000000005%, Loss = 0.8758926510810852
Epoch: 4438, Batch Gradient Norm: 30.46629157461777
Epoch: 4438, Batch Gradient Norm after: 22.360679552817903
Epoch 4439/10000, Prediction Accuracy = 58.636%, Loss = 0.8723405241966248
Epoch: 4439, Batch Gradient Norm: 31.841350402782098
Epoch: 4439, Batch Gradient Norm after: 22.360678204388684
Epoch 4440/10000, Prediction Accuracy = 58.67800000000001%, Loss = 0.8757025361061096
Epoch: 4440, Batch Gradient Norm: 30.463210828002435
Epoch: 4440, Batch Gradient Norm after: 22.36067878996456
Epoch 4441/10000, Prediction Accuracy = 58.64%, Loss = 0.8721800565719604
Epoch: 4441, Batch Gradient Norm: 31.826735812236713
Epoch: 4441, Batch Gradient Norm after: 22.360679152155882
Epoch 4442/10000, Prediction Accuracy = 58.66600000000001%, Loss = 0.8755081295967102
Epoch: 4442, Batch Gradient Norm: 30.458862598856378
Epoch: 4442, Batch Gradient Norm after: 22.360677388010235
Epoch 4443/10000, Prediction Accuracy = 58.63199999999999%, Loss = 0.8720123529434204
Epoch: 4443, Batch Gradient Norm: 31.81019020336228
Epoch: 4443, Batch Gradient Norm after: 22.360676249806982
Epoch 4444/10000, Prediction Accuracy = 58.666%, Loss = 0.8753145694732666
Epoch: 4444, Batch Gradient Norm: 30.45179384791122
Epoch: 4444, Batch Gradient Norm after: 22.36067838352969
Epoch 4445/10000, Prediction Accuracy = 58.646%, Loss = 0.8718454957008361
Epoch: 4445, Batch Gradient Norm: 31.79735781324395
Epoch: 4445, Batch Gradient Norm after: 22.360679367936477
Epoch 4446/10000, Prediction Accuracy = 58.64200000000001%, Loss = 0.8751224637031555
Epoch: 4446, Batch Gradient Norm: 30.44669341053855
Epoch: 4446, Batch Gradient Norm after: 22.360679049447192
Epoch 4447/10000, Prediction Accuracy = 58.648%, Loss = 0.8716816425323486
Epoch: 4447, Batch Gradient Norm: 31.788048843412547
Epoch: 4447, Batch Gradient Norm after: 22.360678096064532
Epoch 4448/10000, Prediction Accuracy = 58.65200000000001%, Loss = 0.8749486565589905
Epoch: 4448, Batch Gradient Norm: 30.441845844452075
Epoch: 4448, Batch Gradient Norm after: 22.360679818450766
Epoch 4449/10000, Prediction Accuracy = 58.624%, Loss = 0.8715315699577332
Epoch: 4449, Batch Gradient Norm: 31.77103037297772
Epoch: 4449, Batch Gradient Norm after: 22.360680462770894
Epoch 4450/10000, Prediction Accuracy = 58.684000000000005%, Loss = 0.8747689366340637
Epoch: 4450, Batch Gradient Norm: 30.436466099065527
Epoch: 4450, Batch Gradient Norm after: 22.360678640571198
Epoch 4451/10000, Prediction Accuracy = 58.632000000000005%, Loss = 0.8713716030120849
Epoch: 4451, Batch Gradient Norm: 31.75756690395944
Epoch: 4451, Batch Gradient Norm after: 22.36067764844446
Epoch 4452/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.8745675086975098
Epoch: 4452, Batch Gradient Norm: 30.430644201200668
Epoch: 4452, Batch Gradient Norm after: 22.36067856879037
Epoch 4453/10000, Prediction Accuracy = 58.65%, Loss = 0.8711879968643188
Epoch: 4453, Batch Gradient Norm: 31.74261724050835
Epoch: 4453, Batch Gradient Norm after: 22.360679902952942
Epoch 4454/10000, Prediction Accuracy = 58.65%, Loss = 0.8743674755096436
Epoch: 4454, Batch Gradient Norm: 30.427601628247913
Epoch: 4454, Batch Gradient Norm after: 22.360679563578685
Epoch 4455/10000, Prediction Accuracy = 58.656000000000006%, Loss = 0.8710225701332093
Epoch: 4455, Batch Gradient Norm: 31.7222025821678
Epoch: 4455, Batch Gradient Norm after: 22.36067748819541
Epoch 4456/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.8741641998291015
Epoch: 4456, Batch Gradient Norm: 30.42475544892873
Epoch: 4456, Batch Gradient Norm after: 22.360676710666045
Epoch 4457/10000, Prediction Accuracy = 58.645999999999994%, Loss = 0.8708679556846619
Epoch: 4457, Batch Gradient Norm: 31.704846301139373
Epoch: 4457, Batch Gradient Norm after: 22.360680167037515
Epoch 4458/10000, Prediction Accuracy = 58.674%, Loss = 0.8739719867706299
Epoch: 4458, Batch Gradient Norm: 30.424998083654547
Epoch: 4458, Batch Gradient Norm after: 22.360677837961845
Epoch 4459/10000, Prediction Accuracy = 58.614%, Loss = 0.8707252264022827
Epoch: 4459, Batch Gradient Norm: 31.683899324463244
Epoch: 4459, Batch Gradient Norm after: 22.36067885563804
Epoch 4460/10000, Prediction Accuracy = 58.674%, Loss = 0.8737750291824341
Epoch: 4460, Batch Gradient Norm: 30.422624877994128
Epoch: 4460, Batch Gradient Norm after: 22.360678917392132
Epoch 4461/10000, Prediction Accuracy = 58.628%, Loss = 0.8705525875091553
Epoch: 4461, Batch Gradient Norm: 31.670314262233802
Epoch: 4461, Batch Gradient Norm after: 22.360678486211448
Epoch 4462/10000, Prediction Accuracy = 58.68000000000001%, Loss = 0.8735733389854431
Epoch: 4462, Batch Gradient Norm: 30.4165725168444
Epoch: 4462, Batch Gradient Norm after: 22.360678598913648
Epoch 4463/10000, Prediction Accuracy = 58.64799999999999%, Loss = 0.8703790664672851
Epoch: 4463, Batch Gradient Norm: 31.656574483934502
Epoch: 4463, Batch Gradient Norm after: 22.360679504308013
Epoch 4464/10000, Prediction Accuracy = 58.652%, Loss = 0.873376727104187
Epoch: 4464, Batch Gradient Norm: 30.412192822991862
Epoch: 4464, Batch Gradient Norm after: 22.36067700225992
Epoch 4465/10000, Prediction Accuracy = 58.644000000000005%, Loss = 0.8702177762985229
Epoch: 4465, Batch Gradient Norm: 31.6445493001259
Epoch: 4465, Batch Gradient Norm after: 22.360680589794725
Epoch 4466/10000, Prediction Accuracy = 58.666%, Loss = 0.8731941699981689
Epoch: 4466, Batch Gradient Norm: 30.40692513887975
Epoch: 4466, Batch Gradient Norm after: 22.360679109688395
Epoch 4467/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.8700539827346802
Epoch: 4467, Batch Gradient Norm: 31.63531165655772
Epoch: 4467, Batch Gradient Norm after: 22.36067685382606
Epoch 4468/10000, Prediction Accuracy = 58.66799999999999%, Loss = 0.8730381250381469
Epoch: 4468, Batch Gradient Norm: 30.400487969886633
Epoch: 4468, Batch Gradient Norm after: 22.360678885809097
Epoch 4469/10000, Prediction Accuracy = 58.629999999999995%, Loss = 0.8698986530303955
Epoch: 4469, Batch Gradient Norm: 31.62538673032001
Epoch: 4469, Batch Gradient Norm after: 22.360678404813584
Epoch 4470/10000, Prediction Accuracy = 58.676%, Loss = 0.8728608846664428
Epoch: 4470, Batch Gradient Norm: 30.39224158588995
Epoch: 4470, Batch Gradient Norm after: 22.360679986355798
Epoch 4471/10000, Prediction Accuracy = 58.646%, Loss = 0.8697271585464478
Epoch: 4471, Batch Gradient Norm: 31.614949418325367
Epoch: 4471, Batch Gradient Norm after: 22.360677694723943
Epoch 4472/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.872676146030426
Epoch: 4472, Batch Gradient Norm: 30.38705653021195
Epoch: 4472, Batch Gradient Norm after: 22.360677005667885
Epoch 4473/10000, Prediction Accuracy = 58.674%, Loss = 0.8695529699325562
Epoch: 4473, Batch Gradient Norm: 31.60706523498625
Epoch: 4473, Batch Gradient Norm after: 22.360678842246703
Epoch 4474/10000, Prediction Accuracy = 58.674%, Loss = 0.8725084066390991
Epoch: 4474, Batch Gradient Norm: 30.37922399550844
Epoch: 4474, Batch Gradient Norm after: 22.360676686231486
Epoch 4475/10000, Prediction Accuracy = 58.67%, Loss = 0.8693854928016662
Epoch: 4475, Batch Gradient Norm: 31.59853887794589
Epoch: 4475, Batch Gradient Norm after: 22.360678700932738
Epoch 4476/10000, Prediction Accuracy = 58.664%, Loss = 0.8723449110984802
Epoch: 4476, Batch Gradient Norm: 30.37280322618835
Epoch: 4476, Batch Gradient Norm after: 22.36068030577676
Epoch 4477/10000, Prediction Accuracy = 58.658%, Loss = 0.8692262172698975
Epoch: 4477, Batch Gradient Norm: 31.587788197495467
Epoch: 4477, Batch Gradient Norm after: 22.360679952969654
Epoch 4478/10000, Prediction Accuracy = 58.67999999999999%, Loss = 0.872183084487915
Epoch: 4478, Batch Gradient Norm: 30.36598507568316
Epoch: 4478, Batch Gradient Norm after: 22.360679737264668
Epoch 4479/10000, Prediction Accuracy = 58.624%, Loss = 0.8690648555755616
Epoch: 4479, Batch Gradient Norm: 31.58015375683997
Epoch: 4479, Batch Gradient Norm after: 22.360678531566204
Epoch 4480/10000, Prediction Accuracy = 58.693999999999996%, Loss = 0.8720098257064819
Epoch: 4480, Batch Gradient Norm: 30.360856575196525
Epoch: 4480, Batch Gradient Norm after: 22.3606774530344
Epoch 4481/10000, Prediction Accuracy = 58.657999999999994%, Loss = 0.8688937067985535
Epoch: 4481, Batch Gradient Norm: 31.573950657229577
Epoch: 4481, Batch Gradient Norm after: 22.360679478412468
Epoch 4482/10000, Prediction Accuracy = 58.69199999999999%, Loss = 0.8718356847763061
Epoch: 4482, Batch Gradient Norm: 30.352242185360442
Epoch: 4482, Batch Gradient Norm after: 22.360677388227494
Epoch 4483/10000, Prediction Accuracy = 58.669999999999995%, Loss = 0.8687285661697388
Epoch: 4483, Batch Gradient Norm: 31.567231374118624
Epoch: 4483, Batch Gradient Norm after: 22.36067579453806
Epoch 4484/10000, Prediction Accuracy = 58.69%, Loss = 0.8716657400131226
Epoch: 4484, Batch Gradient Norm: 30.346812513082263
Epoch: 4484, Batch Gradient Norm after: 22.360677575780777
Epoch 4485/10000, Prediction Accuracy = 58.65400000000001%, Loss = 0.8685597062110901
Epoch: 4485, Batch Gradient Norm: 31.55954444663183
Epoch: 4485, Batch Gradient Norm after: 22.36067871561953
Epoch 4486/10000, Prediction Accuracy = 58.69200000000001%, Loss = 0.8715056777000427
Epoch: 4486, Batch Gradient Norm: 30.33721304957928
Epoch: 4486, Batch Gradient Norm after: 22.360678369339304
Epoch 4487/10000, Prediction Accuracy = 58.658%, Loss = 0.8683983683586121
Epoch: 4487, Batch Gradient Norm: 31.556405814147826
Epoch: 4487, Batch Gradient Norm after: 22.360677357443606
Epoch 4488/10000, Prediction Accuracy = 58.698%, Loss = 0.8713467121124268
Epoch: 4488, Batch Gradient Norm: 30.326683949627665
Epoch: 4488, Batch Gradient Norm after: 22.360680004776302
Epoch 4489/10000, Prediction Accuracy = 58.66600000000001%, Loss = 0.8682311415672302
Epoch: 4489, Batch Gradient Norm: 31.55284250157179
Epoch: 4489, Batch Gradient Norm after: 22.36067752766289
Epoch 4490/10000, Prediction Accuracy = 58.705999999999996%, Loss = 0.8711930036544799
Epoch: 4490, Batch Gradient Norm: 30.31708232759527
Epoch: 4490, Batch Gradient Norm after: 22.360678782975384
Epoch 4491/10000, Prediction Accuracy = 58.664%, Loss = 0.8680540204048157
Epoch: 4491, Batch Gradient Norm: 31.549983989556786
Epoch: 4491, Batch Gradient Norm after: 22.360677407361553
Epoch 4492/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8710328817367554
Epoch: 4492, Batch Gradient Norm: 30.309460605636332
Epoch: 4492, Batch Gradient Norm after: 22.360679241694285
Epoch 4493/10000, Prediction Accuracy = 58.67%, Loss = 0.8678872108459472
Epoch: 4493, Batch Gradient Norm: 31.548567178331798
Epoch: 4493, Batch Gradient Norm after: 22.36067841218522
Epoch 4494/10000, Prediction Accuracy = 58.702%, Loss = 0.8708847284317016
Epoch: 4494, Batch Gradient Norm: 30.29966624070549
Epoch: 4494, Batch Gradient Norm after: 22.360681388409134
Epoch 4495/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.8677216529846191
Epoch: 4495, Batch Gradient Norm: 31.544641071076082
Epoch: 4495, Batch Gradient Norm after: 22.36067812783684
Epoch 4496/10000, Prediction Accuracy = 58.70799999999999%, Loss = 0.8707287788391114
Epoch: 4496, Batch Gradient Norm: 30.292546704569745
Epoch: 4496, Batch Gradient Norm after: 22.360679607441377
Epoch 4497/10000, Prediction Accuracy = 58.662%, Loss = 0.8675545930862427
Epoch: 4497, Batch Gradient Norm: 31.537097358450996
Epoch: 4497, Batch Gradient Norm after: 22.360678158892622
Epoch 4498/10000, Prediction Accuracy = 58.698%, Loss = 0.8705631136894226
Epoch: 4498, Batch Gradient Norm: 30.286565452902245
Epoch: 4498, Batch Gradient Norm after: 22.360678133660446
Epoch 4499/10000, Prediction Accuracy = 58.686%, Loss = 0.8673865556716919
Epoch: 4499, Batch Gradient Norm: 31.533136744742926
Epoch: 4499, Batch Gradient Norm after: 22.36067758491028
Epoch 4500/10000, Prediction Accuracy = 58.714%, Loss = 0.8703927278518677
Epoch: 4500, Batch Gradient Norm: 30.278258468070266
Epoch: 4500, Batch Gradient Norm after: 22.3606788404994
Epoch 4501/10000, Prediction Accuracy = 58.68599999999999%, Loss = 0.8672175288200379
Epoch: 4501, Batch Gradient Norm: 31.526250082653295
Epoch: 4501, Batch Gradient Norm after: 22.36068107469852
Epoch 4502/10000, Prediction Accuracy = 58.7%, Loss = 0.8702381372451782
Epoch: 4502, Batch Gradient Norm: 30.270972762506474
Epoch: 4502, Batch Gradient Norm after: 22.36067918364629
Epoch 4503/10000, Prediction Accuracy = 58.688%, Loss = 0.8670502066612243
Epoch: 4503, Batch Gradient Norm: 31.52115660602094
Epoch: 4503, Batch Gradient Norm after: 22.360677959684068
Epoch 4504/10000, Prediction Accuracy = 58.708000000000006%, Loss = 0.8700866222381591
Epoch: 4504, Batch Gradient Norm: 30.26095713097879
Epoch: 4504, Batch Gradient Norm after: 22.360678398099413
Epoch 4505/10000, Prediction Accuracy = 58.674%, Loss = 0.8668835163116455
Epoch: 4505, Batch Gradient Norm: 31.520454632505476
Epoch: 4505, Batch Gradient Norm after: 22.36067870658986
Epoch 4506/10000, Prediction Accuracy = 58.705999999999996%, Loss = 0.8699381232261658
Epoch: 4506, Batch Gradient Norm: 30.250705726496204
Epoch: 4506, Batch Gradient Norm after: 22.3606786564552
Epoch 4507/10000, Prediction Accuracy = 58.68399999999999%, Loss = 0.8667197108268738
Epoch: 4507, Batch Gradient Norm: 31.51964181429349
Epoch: 4507, Batch Gradient Norm after: 22.360678544500608
Epoch 4508/10000, Prediction Accuracy = 58.698%, Loss = 0.8697985649108887
Epoch: 4508, Batch Gradient Norm: 30.242658571953125
Epoch: 4508, Batch Gradient Norm after: 22.36067593166805
Epoch 4509/10000, Prediction Accuracy = 58.706%, Loss = 0.8665461540222168
Epoch: 4509, Batch Gradient Norm: 31.52104900015059
Epoch: 4509, Batch Gradient Norm after: 22.360680214285537
Epoch 4510/10000, Prediction Accuracy = 58.717999999999996%, Loss = 0.869648790359497
Epoch: 4510, Batch Gradient Norm: 30.231177257525495
Epoch: 4510, Batch Gradient Norm after: 22.360679195800802
Epoch 4511/10000, Prediction Accuracy = 58.718%, Loss = 0.8663780689239502
Epoch: 4511, Batch Gradient Norm: 31.51846413456142
Epoch: 4511, Batch Gradient Norm after: 22.360678420103213
Epoch 4512/10000, Prediction Accuracy = 58.7%, Loss = 0.8695037364959717
Epoch: 4512, Batch Gradient Norm: 30.223919436801733
Epoch: 4512, Batch Gradient Norm after: 22.360678411217553
Epoch 4513/10000, Prediction Accuracy = 58.698%, Loss = 0.8662156701087952
Epoch: 4513, Batch Gradient Norm: 31.514658455510137
Epoch: 4513, Batch Gradient Norm after: 22.360678540124855
Epoch 4514/10000, Prediction Accuracy = 58.720000000000006%, Loss = 0.869351077079773
Epoch: 4514, Batch Gradient Norm: 30.215907848128573
Epoch: 4514, Batch Gradient Norm after: 22.360678131105466
Epoch 4515/10000, Prediction Accuracy = 58.698%, Loss = 0.8660622358322143
Epoch: 4515, Batch Gradient Norm: 31.508428755688346
Epoch: 4515, Batch Gradient Norm after: 22.360680118019324
Epoch 4516/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8692001342773438
Epoch: 4516, Batch Gradient Norm: 30.208577723939282
Epoch: 4516, Batch Gradient Norm after: 22.360679041534702
Epoch 4517/10000, Prediction Accuracy = 58.694%, Loss = 0.8658875226974487
Epoch: 4517, Batch Gradient Norm: 31.50584765948012
Epoch: 4517, Batch Gradient Norm after: 22.360678263311666
Epoch 4518/10000, Prediction Accuracy = 58.712%, Loss = 0.8690353751182556
Epoch: 4518, Batch Gradient Norm: 30.20258523214965
Epoch: 4518, Batch Gradient Norm after: 22.36067882612602
Epoch 4519/10000, Prediction Accuracy = 58.726%, Loss = 0.8657112002372742
Epoch: 4519, Batch Gradient Norm: 31.497682951425297
Epoch: 4519, Batch Gradient Norm after: 22.360679579706815
Epoch 4520/10000, Prediction Accuracy = 58.702%, Loss = 0.8688691020011902
Epoch: 4520, Batch Gradient Norm: 30.19360541654763
Epoch: 4520, Batch Gradient Norm after: 22.36067653031321
Epoch 4521/10000, Prediction Accuracy = 58.738%, Loss = 0.8655480027198792
Epoch: 4521, Batch Gradient Norm: 31.490722625862816
Epoch: 4521, Batch Gradient Norm after: 22.36067735846435
Epoch 4522/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8687111616134644
Epoch: 4522, Batch Gradient Norm: 30.185723709782792
Epoch: 4522, Batch Gradient Norm after: 22.36067859450338
Epoch 4523/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.8653949856758117
Epoch: 4523, Batch Gradient Norm: 31.480691990321272
Epoch: 4523, Batch Gradient Norm after: 22.360680388401832
Epoch 4524/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8685439348220825
Epoch: 4524, Batch Gradient Norm: 30.17740746885726
Epoch: 4524, Batch Gradient Norm after: 22.360678356849526
Epoch 4525/10000, Prediction Accuracy = 58.722%, Loss = 0.8652404308319092
Epoch: 4525, Batch Gradient Norm: 31.473675393966285
Epoch: 4525, Batch Gradient Norm after: 22.36067530513716
Epoch 4526/10000, Prediction Accuracy = 58.75600000000001%, Loss = 0.8683816075325013
Epoch: 4526, Batch Gradient Norm: 30.171192570510723
Epoch: 4526, Batch Gradient Norm after: 22.360680616385455
Epoch 4527/10000, Prediction Accuracy = 58.71%, Loss = 0.8650719285011291
Epoch: 4527, Batch Gradient Norm: 31.462391932254363
Epoch: 4527, Batch Gradient Norm after: 22.36067814554524
Epoch 4528/10000, Prediction Accuracy = 58.726%, Loss = 0.8681988000869751
Epoch: 4528, Batch Gradient Norm: 30.163739634275604
Epoch: 4528, Batch Gradient Norm after: 22.360679549476977
Epoch 4529/10000, Prediction Accuracy = 58.73%, Loss = 0.8649039387702941
Epoch: 4529, Batch Gradient Norm: 31.45091167236126
Epoch: 4529, Batch Gradient Norm after: 22.360679264116108
Epoch 4530/10000, Prediction Accuracy = 58.709999999999994%, Loss = 0.8680237293243408
Epoch: 4530, Batch Gradient Norm: 30.157153162957467
Epoch: 4530, Batch Gradient Norm after: 22.36067835243131
Epoch 4531/10000, Prediction Accuracy = 58.722%, Loss = 0.8647444725036622
Epoch: 4531, Batch Gradient Norm: 31.43596941500695
Epoch: 4531, Batch Gradient Norm after: 22.36067873867677
Epoch 4532/10000, Prediction Accuracy = 58.746%, Loss = 0.8678491830825805
Epoch: 4532, Batch Gradient Norm: 30.151766688219677
Epoch: 4532, Batch Gradient Norm after: 22.360678516912596
Epoch 4533/10000, Prediction Accuracy = 58.712%, Loss = 0.8645920038223267
Epoch: 4533, Batch Gradient Norm: 31.42149623237026
Epoch: 4533, Batch Gradient Norm after: 22.36067871684217
Epoch 4534/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8676709771156311
Epoch: 4534, Batch Gradient Norm: 30.147611649822643
Epoch: 4534, Batch Gradient Norm after: 22.36068008144088
Epoch 4535/10000, Prediction Accuracy = 58.71%, Loss = 0.8644370198249817
Epoch: 4535, Batch Gradient Norm: 31.40475425304842
Epoch: 4535, Batch Gradient Norm after: 22.360677311609322
Epoch 4536/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.8674761533737183
Epoch: 4536, Batch Gradient Norm: 30.14426253719539
Epoch: 4536, Batch Gradient Norm after: 22.36067798138178
Epoch 4537/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.8642730832099914
Epoch: 4537, Batch Gradient Norm: 31.386948141797937
Epoch: 4537, Batch Gradient Norm after: 22.36067999533673
Epoch 4538/10000, Prediction Accuracy = 58.734%, Loss = 0.8672802090644837
Epoch: 4538, Batch Gradient Norm: 30.140670836167153
Epoch: 4538, Batch Gradient Norm after: 22.36067932653954
Epoch 4539/10000, Prediction Accuracy = 58.726%, Loss = 0.8641136884689331
Epoch: 4539, Batch Gradient Norm: 31.370492741674912
Epoch: 4539, Batch Gradient Norm after: 22.360677734068737
Epoch 4540/10000, Prediction Accuracy = 58.738%, Loss = 0.8670938730239868
Epoch: 4540, Batch Gradient Norm: 30.138436301534398
Epoch: 4540, Batch Gradient Norm after: 22.360676819047598
Epoch 4541/10000, Prediction Accuracy = 58.732000000000006%, Loss = 0.8639596223831176
Epoch: 4541, Batch Gradient Norm: 31.352357625675143
Epoch: 4541, Batch Gradient Norm after: 22.36067859485931
Epoch 4542/10000, Prediction Accuracy = 58.757999999999996%, Loss = 0.8669021606445313
Epoch: 4542, Batch Gradient Norm: 30.136151100732558
Epoch: 4542, Batch Gradient Norm after: 22.360682507177714
Epoch 4543/10000, Prediction Accuracy = 58.73%, Loss = 0.8638140320777893
Epoch: 4543, Batch Gradient Norm: 31.334984555051296
Epoch: 4543, Batch Gradient Norm after: 22.36067534483662
Epoch 4544/10000, Prediction Accuracy = 58.76400000000001%, Loss = 0.8667155504226685
Epoch: 4544, Batch Gradient Norm: 30.132189967727378
Epoch: 4544, Batch Gradient Norm after: 22.360677822191327
Epoch 4545/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.863655424118042
Epoch: 4545, Batch Gradient Norm: 31.320790118261208
Epoch: 4545, Batch Gradient Norm after: 22.360679729597166
Epoch 4546/10000, Prediction Accuracy = 58.75%, Loss = 0.8665305256843567
Epoch: 4546, Batch Gradient Norm: 30.12763364809515
Epoch: 4546, Batch Gradient Norm after: 22.360678508923495
Epoch 4547/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8634923577308655
Epoch: 4547, Batch Gradient Norm: 31.30308819573907
Epoch: 4547, Batch Gradient Norm after: 22.36067943320741
Epoch 4548/10000, Prediction Accuracy = 58.746%, Loss = 0.8663384318351746
Epoch: 4548, Batch Gradient Norm: 30.125528433245442
Epoch: 4548, Batch Gradient Norm after: 22.360678952308465
Epoch 4549/10000, Prediction Accuracy = 58.724000000000004%, Loss = 0.8633362174034118
Epoch: 4549, Batch Gradient Norm: 31.290660311184197
Epoch: 4549, Batch Gradient Norm after: 22.360678743099175
Epoch 4550/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8661638855934143
Epoch: 4550, Batch Gradient Norm: 30.119988643395516
Epoch: 4550, Batch Gradient Norm after: 22.360677676519085
Epoch 4551/10000, Prediction Accuracy = 58.726%, Loss = 0.8631791949272156
Epoch: 4551, Batch Gradient Norm: 31.279410229570978
Epoch: 4551, Batch Gradient Norm after: 22.36067741078896
Epoch 4552/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8659946322441101
Epoch: 4552, Batch Gradient Norm: 30.111841040630654
Epoch: 4552, Batch Gradient Norm after: 22.36068091284708
Epoch 4553/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.8630253911018372
Epoch: 4553, Batch Gradient Norm: 31.268814867539
Epoch: 4553, Batch Gradient Norm after: 22.360679743304843
Epoch 4554/10000, Prediction Accuracy = 58.774%, Loss = 0.8658248662948609
Epoch: 4554, Batch Gradient Norm: 30.107010049976367
Epoch: 4554, Batch Gradient Norm after: 22.36067983400137
Epoch 4555/10000, Prediction Accuracy = 58.714%, Loss = 0.8628625392913818
Epoch: 4555, Batch Gradient Norm: 31.255346399683354
Epoch: 4555, Batch Gradient Norm after: 22.360680917591083
Epoch 4556/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.865642511844635
Epoch: 4556, Batch Gradient Norm: 30.102838812786292
Epoch: 4556, Batch Gradient Norm after: 22.360677094594028
Epoch 4557/10000, Prediction Accuracy = 58.714%, Loss = 0.8627040266990662
Epoch: 4557, Batch Gradient Norm: 31.24332047203924
Epoch: 4557, Batch Gradient Norm after: 22.360679219994722
Epoch 4558/10000, Prediction Accuracy = 58.742%, Loss = 0.8654645919799805
Epoch: 4558, Batch Gradient Norm: 30.09710774716261
Epoch: 4558, Batch Gradient Norm after: 22.360677485571568
Epoch 4559/10000, Prediction Accuracy = 58.717999999999996%, Loss = 0.8625484943389893
Epoch: 4559, Batch Gradient Norm: 31.2314217790474
Epoch: 4559, Batch Gradient Norm after: 22.360676556143066
Epoch 4560/10000, Prediction Accuracy = 58.766%, Loss = 0.8652997255325318
Epoch: 4560, Batch Gradient Norm: 30.09237431648361
Epoch: 4560, Batch Gradient Norm after: 22.360677924627392
Epoch 4561/10000, Prediction Accuracy = 58.738%, Loss = 0.8623936176300049
Epoch: 4561, Batch Gradient Norm: 31.21880576614377
Epoch: 4561, Batch Gradient Norm after: 22.360678872810418
Epoch 4562/10000, Prediction Accuracy = 58.766%, Loss = 0.8651325106620789
Epoch: 4562, Batch Gradient Norm: 30.08476151210765
Epoch: 4562, Batch Gradient Norm after: 22.360679351177655
Epoch 4563/10000, Prediction Accuracy = 58.71999999999999%, Loss = 0.8622390985488891
Epoch: 4563, Batch Gradient Norm: 31.20800986058172
Epoch: 4563, Batch Gradient Norm after: 22.360678824619423
Epoch 4564/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8649542331695557
Epoch: 4564, Batch Gradient Norm: 30.079382555642965
Epoch: 4564, Batch Gradient Norm after: 22.360680225100573
Epoch 4565/10000, Prediction Accuracy = 58.73%, Loss = 0.8620746970176697
Epoch: 4565, Batch Gradient Norm: 31.194232013496805
Epoch: 4565, Batch Gradient Norm after: 22.36067909066451
Epoch 4566/10000, Prediction Accuracy = 58.74000000000001%, Loss = 0.8647773027420044
Epoch: 4566, Batch Gradient Norm: 30.072898281111375
Epoch: 4566, Batch Gradient Norm after: 22.360680027640495
Epoch 4567/10000, Prediction Accuracy = 58.726%, Loss = 0.8619143247604371
Epoch: 4567, Batch Gradient Norm: 31.184100632024972
Epoch: 4567, Batch Gradient Norm after: 22.36067977995276
Epoch 4568/10000, Prediction Accuracy = 58.746%, Loss = 0.8646065831184387
Epoch: 4568, Batch Gradient Norm: 30.065274213730927
Epoch: 4568, Batch Gradient Norm after: 22.36068125066104
Epoch 4569/10000, Prediction Accuracy = 58.714%, Loss = 0.8617689490318299
Epoch: 4569, Batch Gradient Norm: 31.17145453680588
Epoch: 4569, Batch Gradient Norm after: 22.360678399772958
Epoch 4570/10000, Prediction Accuracy = 58.757999999999996%, Loss = 0.8644367694854737
Epoch: 4570, Batch Gradient Norm: 30.062381163819627
Epoch: 4570, Batch Gradient Norm after: 22.360681126419745
Epoch 4571/10000, Prediction Accuracy = 58.71%, Loss = 0.8616163849830627
Epoch: 4571, Batch Gradient Norm: 31.15641545000591
Epoch: 4571, Batch Gradient Norm after: 22.360678769541504
Epoch 4572/10000, Prediction Accuracy = 58.77%, Loss = 0.8642577171325684
Epoch: 4572, Batch Gradient Norm: 30.055517248900557
Epoch: 4572, Batch Gradient Norm after: 22.36068230113656
Epoch 4573/10000, Prediction Accuracy = 58.722%, Loss = 0.8614554524421691
Epoch: 4573, Batch Gradient Norm: 31.14417583441735
Epoch: 4573, Batch Gradient Norm after: 22.36067840704943
Epoch 4574/10000, Prediction Accuracy = 58.75%, Loss = 0.864077377319336
Epoch: 4574, Batch Gradient Norm: 30.04945588043698
Epoch: 4574, Batch Gradient Norm after: 22.360678781487202
Epoch 4575/10000, Prediction Accuracy = 58.74400000000001%, Loss = 0.8612892270088196
Epoch: 4575, Batch Gradient Norm: 31.133957435162575
Epoch: 4575, Batch Gradient Norm after: 22.360679680240104
Epoch 4576/10000, Prediction Accuracy = 58.74399999999999%, Loss = 0.8639015913009643
Epoch: 4576, Batch Gradient Norm: 30.04127805532769
Epoch: 4576, Batch Gradient Norm after: 22.360679373854325
Epoch 4577/10000, Prediction Accuracy = 58.73199999999999%, Loss = 0.8611377239227295
Epoch: 4577, Batch Gradient Norm: 31.12859760208605
Epoch: 4577, Batch Gradient Norm after: 22.360678419692643
Epoch 4578/10000, Prediction Accuracy = 58.766%, Loss = 0.8637443542480469
Epoch: 4578, Batch Gradient Norm: 30.036401912932796
Epoch: 4578, Batch Gradient Norm after: 22.360678785729608
Epoch 4579/10000, Prediction Accuracy = 58.71999999999999%, Loss = 0.8609811186790466
Epoch: 4579, Batch Gradient Norm: 31.118186710715708
Epoch: 4579, Batch Gradient Norm after: 22.36067902479387
Epoch 4580/10000, Prediction Accuracy = 58.766%, Loss = 0.8635849714279175
Epoch: 4580, Batch Gradient Norm: 30.028574041768557
Epoch: 4580, Batch Gradient Norm after: 22.360679216015523
Epoch 4581/10000, Prediction Accuracy = 58.715999999999994%, Loss = 0.8608265876770019
Epoch: 4581, Batch Gradient Norm: 31.10995129207116
Epoch: 4581, Batch Gradient Norm after: 22.360677807514172
Epoch 4582/10000, Prediction Accuracy = 58.774%, Loss = 0.8634218811988831
Epoch: 4582, Batch Gradient Norm: 30.021898768929248
Epoch: 4582, Batch Gradient Norm after: 22.360680595019055
Epoch 4583/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8606564044952393
Epoch: 4583, Batch Gradient Norm: 31.099965619069867
Epoch: 4583, Batch Gradient Norm after: 22.360678792131655
Epoch 4584/10000, Prediction Accuracy = 58.767999999999994%, Loss = 0.8632455348968506
Epoch: 4584, Batch Gradient Norm: 30.015901975663304
Epoch: 4584, Batch Gradient Norm after: 22.360678916845945
Epoch 4585/10000, Prediction Accuracy = 58.75%, Loss = 0.86049485206604
Epoch: 4585, Batch Gradient Norm: 31.08787889258323
Epoch: 4585, Batch Gradient Norm after: 22.360677341596404
Epoch 4586/10000, Prediction Accuracy = 58.757999999999996%, Loss = 0.8630746722221374
Epoch: 4586, Batch Gradient Norm: 30.011648534819507
Epoch: 4586, Batch Gradient Norm after: 22.36068058224988
Epoch 4587/10000, Prediction Accuracy = 58.726%, Loss = 0.8603430390357971
Epoch: 4587, Batch Gradient Norm: 31.077328069977444
Epoch: 4587, Batch Gradient Norm after: 22.36067724370814
Epoch 4588/10000, Prediction Accuracy = 58.762%, Loss = 0.8629125475883483
Epoch: 4588, Batch Gradient Norm: 30.003242341467008
Epoch: 4588, Batch Gradient Norm after: 22.360680013936403
Epoch 4589/10000, Prediction Accuracy = 58.75%, Loss = 0.8601991176605225
Epoch: 4589, Batch Gradient Norm: 31.06808672032582
Epoch: 4589, Batch Gradient Norm after: 22.360676502505825
Epoch 4590/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8627490878105164
Epoch: 4590, Batch Gradient Norm: 29.999082540788194
Epoch: 4590, Batch Gradient Norm after: 22.360680564774327
Epoch 4591/10000, Prediction Accuracy = 58.734%, Loss = 0.86003178358078
Epoch: 4591, Batch Gradient Norm: 31.053205680277625
Epoch: 4591, Batch Gradient Norm after: 22.360677634835238
Epoch 4592/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.8625641465187073
Epoch: 4592, Batch Gradient Norm: 29.993914318144903
Epoch: 4592, Batch Gradient Norm after: 22.360678116243506
Epoch 4593/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8598690032958984
Epoch: 4593, Batch Gradient Norm: 31.04215323888815
Epoch: 4593, Batch Gradient Norm after: 22.36067795155725
Epoch 4594/10000, Prediction Accuracy = 58.762%, Loss = 0.8623907327651977
Epoch: 4594, Batch Gradient Norm: 29.98622453059643
Epoch: 4594, Batch Gradient Norm after: 22.360680018316717
Epoch 4595/10000, Prediction Accuracy = 58.75%, Loss = 0.8597149968147277
Epoch: 4595, Batch Gradient Norm: 31.033028224703344
Epoch: 4595, Batch Gradient Norm after: 22.360678458004784
Epoch 4596/10000, Prediction Accuracy = 58.778%, Loss = 0.862222945690155
Epoch: 4596, Batch Gradient Norm: 29.981190437782075
Epoch: 4596, Batch Gradient Norm after: 22.360679338219352
Epoch 4597/10000, Prediction Accuracy = 58.74399999999999%, Loss = 0.8595659255981445
Epoch: 4597, Batch Gradient Norm: 31.021218741068413
Epoch: 4597, Batch Gradient Norm after: 22.360678037888228
Epoch 4598/10000, Prediction Accuracy = 58.766%, Loss = 0.862061870098114
Epoch: 4598, Batch Gradient Norm: 29.97361465484936
Epoch: 4598, Batch Gradient Norm after: 22.36068047081778
Epoch 4599/10000, Prediction Accuracy = 58.766%, Loss = 0.8594204664230347
Epoch: 4599, Batch Gradient Norm: 31.00655226094962
Epoch: 4599, Batch Gradient Norm after: 22.36067593358852
Epoch 4600/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8618778824806214
Epoch: 4600, Batch Gradient Norm: 29.968710179907426
Epoch: 4600, Batch Gradient Norm after: 22.36068176010614
Epoch 4601/10000, Prediction Accuracy = 58.75%, Loss = 0.8592618465423584
Epoch: 4601, Batch Gradient Norm: 30.992175206076094
Epoch: 4601, Batch Gradient Norm after: 22.360676950598865
Epoch 4602/10000, Prediction Accuracy = 58.774%, Loss = 0.8616966962814331
Epoch: 4602, Batch Gradient Norm: 29.96560041270006
Epoch: 4602, Batch Gradient Norm after: 22.36067937893194
Epoch 4603/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8590968728065491
Epoch: 4603, Batch Gradient Norm: 30.980716254367913
Epoch: 4603, Batch Gradient Norm after: 22.36067901112724
Epoch 4604/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.8615214824676514
Epoch: 4604, Batch Gradient Norm: 29.960103983658595
Epoch: 4604, Batch Gradient Norm after: 22.360679873894185
Epoch 4605/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.8589547157287598
Epoch: 4605, Batch Gradient Norm: 30.969639187929193
Epoch: 4605, Batch Gradient Norm after: 22.36067968456866
Epoch 4606/10000, Prediction Accuracy = 58.774%, Loss = 0.8613578677177429
Epoch: 4606, Batch Gradient Norm: 29.953616311580944
Epoch: 4606, Batch Gradient Norm after: 22.360678704019723
Epoch 4607/10000, Prediction Accuracy = 58.75%, Loss = 0.8588061690330505
Epoch: 4607, Batch Gradient Norm: 30.959913180112803
Epoch: 4607, Batch Gradient Norm after: 22.36067951575363
Epoch 4608/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8611939191818238
Epoch: 4608, Batch Gradient Norm: 29.949509479521776
Epoch: 4608, Batch Gradient Norm after: 22.360680914488555
Epoch 4609/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.8586494445800781
Epoch: 4609, Batch Gradient Norm: 30.949084381391625
Epoch: 4609, Batch Gradient Norm after: 22.360677748587534
Epoch 4610/10000, Prediction Accuracy = 58.760000000000005%, Loss = 0.8610235810279846
Epoch: 4610, Batch Gradient Norm: 29.941933308999175
Epoch: 4610, Batch Gradient Norm after: 22.360678237824008
Epoch 4611/10000, Prediction Accuracy = 58.742000000000004%, Loss = 0.8584873795509338
Epoch: 4611, Batch Gradient Norm: 30.943301703451784
Epoch: 4611, Batch Gradient Norm after: 22.360678886875306
Epoch 4612/10000, Prediction Accuracy = 58.769999999999996%, Loss = 0.8608560681343078
Epoch: 4612, Batch Gradient Norm: 29.936220742960117
Epoch: 4612, Batch Gradient Norm after: 22.360676981598335
Epoch 4613/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8583268404006958
Epoch: 4613, Batch Gradient Norm: 30.93255710124627
Epoch: 4613, Batch Gradient Norm after: 22.360679952514054
Epoch 4614/10000, Prediction Accuracy = 58.766%, Loss = 0.8606976628303528
Epoch: 4614, Batch Gradient Norm: 29.926765451807707
Epoch: 4614, Batch Gradient Norm after: 22.36067890367486
Epoch 4615/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8581714510917664
Epoch: 4615, Batch Gradient Norm: 30.929347473746223
Epoch: 4615, Batch Gradient Norm after: 22.360678284716354
Epoch 4616/10000, Prediction Accuracy = 58.757999999999996%, Loss = 0.8605493545532227
Epoch: 4616, Batch Gradient Norm: 29.92107802232701
Epoch: 4616, Batch Gradient Norm after: 22.360679719485432
Epoch 4617/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8580241203308105
Epoch: 4617, Batch Gradient Norm: 30.91870810332147
Epoch: 4617, Batch Gradient Norm after: 22.36067827289418
Epoch 4618/10000, Prediction Accuracy = 58.75%, Loss = 0.860396432876587
Epoch: 4618, Batch Gradient Norm: 29.912847609143157
Epoch: 4618, Batch Gradient Norm after: 22.360678963373672
Epoch 4619/10000, Prediction Accuracy = 58.75999999999999%, Loss = 0.8578667044639587
Epoch: 4619, Batch Gradient Norm: 30.909969440983296
Epoch: 4619, Batch Gradient Norm after: 22.360681279168055
Epoch 4620/10000, Prediction Accuracy = 58.767999999999994%, Loss = 0.8602361083030701
Epoch: 4620, Batch Gradient Norm: 29.905859172977426
Epoch: 4620, Batch Gradient Norm after: 22.36067945410966
Epoch 4621/10000, Prediction Accuracy = 58.762%, Loss = 0.8576997280120849
Epoch: 4621, Batch Gradient Norm: 30.90625769809657
Epoch: 4621, Batch Gradient Norm after: 22.36067950308503
Epoch 4622/10000, Prediction Accuracy = 58.782%, Loss = 0.8600809216499329
Epoch: 4622, Batch Gradient Norm: 29.90087697521038
Epoch: 4622, Batch Gradient Norm after: 22.36067955442776
Epoch 4623/10000, Prediction Accuracy = 58.762%, Loss = 0.8575383067131043
Epoch: 4623, Batch Gradient Norm: 30.896348910648666
Epoch: 4623, Batch Gradient Norm after: 22.360677300648224
Epoch 4624/10000, Prediction Accuracy = 58.774%, Loss = 0.8599179029464722
Epoch: 4624, Batch Gradient Norm: 29.895084223990487
Epoch: 4624, Batch Gradient Norm after: 22.36067807532548
Epoch 4625/10000, Prediction Accuracy = 58.766%, Loss = 0.8573891758918762
Epoch: 4625, Batch Gradient Norm: 30.890453538586286
Epoch: 4625, Batch Gradient Norm after: 22.36067775535913
Epoch 4626/10000, Prediction Accuracy = 58.762%, Loss = 0.8597735524177551
Epoch: 4626, Batch Gradient Norm: 29.88956254329353
Epoch: 4626, Batch Gradient Norm after: 22.360679932178975
Epoch 4627/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.8572423219680786
Epoch: 4627, Batch Gradient Norm: 30.886492668262058
Epoch: 4627, Batch Gradient Norm after: 22.360677907800884
Epoch 4628/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8596244692802429
Epoch: 4628, Batch Gradient Norm: 29.880489699059602
Epoch: 4628, Batch Gradient Norm after: 22.36067822915345
Epoch 4629/10000, Prediction Accuracy = 58.758%, Loss = 0.8570744752883911
Epoch: 4629, Batch Gradient Norm: 30.881714971596647
Epoch: 4629, Batch Gradient Norm after: 22.360676876685467
Epoch 4630/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8594738841056824
Epoch: 4630, Batch Gradient Norm: 29.87354092669928
Epoch: 4630, Batch Gradient Norm after: 22.360677974529274
Epoch 4631/10000, Prediction Accuracy = 58.798%, Loss = 0.8569017648696899
Epoch: 4631, Batch Gradient Norm: 30.880097804042766
Epoch: 4631, Batch Gradient Norm after: 22.36067792524276
Epoch 4632/10000, Prediction Accuracy = 58.788%, Loss = 0.8593274354934692
Epoch: 4632, Batch Gradient Norm: 29.866331910950095
Epoch: 4632, Batch Gradient Norm after: 22.36068074426455
Epoch 4633/10000, Prediction Accuracy = 58.802%, Loss = 0.8567424774169922
Epoch: 4633, Batch Gradient Norm: 30.875547809558775
Epoch: 4633, Batch Gradient Norm after: 22.360677972144877
Epoch 4634/10000, Prediction Accuracy = 58.75599999999999%, Loss = 0.8591785907745362
Epoch: 4634, Batch Gradient Norm: 29.861424585419705
Epoch: 4634, Batch Gradient Norm after: 22.360678758199
Epoch 4635/10000, Prediction Accuracy = 58.778%, Loss = 0.8565990447998046
Epoch: 4635, Batch Gradient Norm: 30.872720526617467
Epoch: 4635, Batch Gradient Norm after: 22.360677279053945
Epoch 4636/10000, Prediction Accuracy = 58.76800000000001%, Loss = 0.8590371966361999
Epoch: 4636, Batch Gradient Norm: 29.854474556269786
Epoch: 4636, Batch Gradient Norm after: 22.360676584280046
Epoch 4637/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.856439733505249
Epoch: 4637, Batch Gradient Norm: 30.868064974980474
Epoch: 4637, Batch Gradient Norm after: 22.360676154091106
Epoch 4638/10000, Prediction Accuracy = 58.757999999999996%, Loss = 0.8588808178901672
Epoch: 4638, Batch Gradient Norm: 29.845336011285294
Epoch: 4638, Batch Gradient Norm after: 22.360677664726694
Epoch 4639/10000, Prediction Accuracy = 58.794%, Loss = 0.8562710046768188
Epoch: 4639, Batch Gradient Norm: 30.862091235002058
Epoch: 4639, Batch Gradient Norm after: 22.36068049801037
Epoch 4640/10000, Prediction Accuracy = 58.778000000000006%, Loss = 0.8587263703346253
Epoch: 4640, Batch Gradient Norm: 29.838026734166007
Epoch: 4640, Batch Gradient Norm after: 22.36067760791782
Epoch 4641/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8561119914054871
Epoch: 4641, Batch Gradient Norm: 30.85342131073148
Epoch: 4641, Batch Gradient Norm after: 22.36067574430513
Epoch 4642/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8585660576820373
Epoch: 4642, Batch Gradient Norm: 29.834839235008022
Epoch: 4642, Batch Gradient Norm after: 22.36067743011639
Epoch 4643/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.8559630632400512
Epoch: 4643, Batch Gradient Norm: 30.845221662219707
Epoch: 4643, Batch Gradient Norm after: 22.360678870425787
Epoch 4644/10000, Prediction Accuracy = 58.774%, Loss = 0.8584150552749634
Epoch: 4644, Batch Gradient Norm: 29.830401060858346
Epoch: 4644, Batch Gradient Norm after: 22.360676888472998
Epoch 4645/10000, Prediction Accuracy = 58.782%, Loss = 0.8558078050613404
Epoch: 4645, Batch Gradient Norm: 30.839324269679967
Epoch: 4645, Batch Gradient Norm after: 22.360677751096468
Epoch 4646/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8582611203193664
Epoch: 4646, Batch Gradient Norm: 29.82325852755349
Epoch: 4646, Batch Gradient Norm after: 22.360680002604678
Epoch 4647/10000, Prediction Accuracy = 58.786%, Loss = 0.855655312538147
Epoch: 4647, Batch Gradient Norm: 30.827166173841068
Epoch: 4647, Batch Gradient Norm after: 22.360677975109926
Epoch 4648/10000, Prediction Accuracy = 58.782000000000004%, Loss = 0.8581009387969971
Epoch: 4648, Batch Gradient Norm: 29.82066534785233
Epoch: 4648, Batch Gradient Norm after: 22.360679770273443
Epoch 4649/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.855499529838562
Epoch: 4649, Batch Gradient Norm: 30.818292719406564
Epoch: 4649, Batch Gradient Norm after: 22.360676459658315
Epoch 4650/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8579266428947449
Epoch: 4650, Batch Gradient Norm: 29.812387256955216
Epoch: 4650, Batch Gradient Norm after: 22.36067942993754
Epoch 4651/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.8553437471389771
Epoch: 4651, Batch Gradient Norm: 30.809251533849224
Epoch: 4651, Batch Gradient Norm after: 22.36067644328787
Epoch 4652/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8577638626098633
Epoch: 4652, Batch Gradient Norm: 29.810016553920793
Epoch: 4652, Batch Gradient Norm after: 22.360678339866585
Epoch 4653/10000, Prediction Accuracy = 58.794%, Loss = 0.8551895260810852
Epoch: 4653, Batch Gradient Norm: 30.801431215649274
Epoch: 4653, Batch Gradient Norm after: 22.360680557483082
Epoch 4654/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8576128125190735
Epoch: 4654, Batch Gradient Norm: 29.80211195790922
Epoch: 4654, Batch Gradient Norm after: 22.360678696173064
Epoch 4655/10000, Prediction Accuracy = 58.778%, Loss = 0.8550502300262451
Epoch: 4655, Batch Gradient Norm: 30.79570513826459
Epoch: 4655, Batch Gradient Norm after: 22.360677309147288
Epoch 4656/10000, Prediction Accuracy = 58.788%, Loss = 0.8574581146240234
Epoch: 4656, Batch Gradient Norm: 29.79752594822908
Epoch: 4656, Batch Gradient Norm after: 22.360678072360628
Epoch 4657/10000, Prediction Accuracy = 58.774%, Loss = 0.8548869609832763
Epoch: 4657, Batch Gradient Norm: 30.790572159694136
Epoch: 4657, Batch Gradient Norm after: 22.360678464497134
Epoch 4658/10000, Prediction Accuracy = 58.782%, Loss = 0.8573063254356384
Epoch: 4658, Batch Gradient Norm: 29.788767185888464
Epoch: 4658, Batch Gradient Norm after: 22.360680815392527
Epoch 4659/10000, Prediction Accuracy = 58.8%, Loss = 0.8547261238098145
Epoch: 4659, Batch Gradient Norm: 30.781467757747276
Epoch: 4659, Batch Gradient Norm after: 22.360676049707163
Epoch 4660/10000, Prediction Accuracy = 58.78399999999999%, Loss = 0.8571489930152894
Epoch: 4660, Batch Gradient Norm: 29.785950880259104
Epoch: 4660, Batch Gradient Norm after: 22.36067913105852
Epoch 4661/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.8545672178268433
Epoch: 4661, Batch Gradient Norm: 30.774471803770485
Epoch: 4661, Batch Gradient Norm after: 22.360673894496276
Epoch 4662/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8569912791252137
Epoch: 4662, Batch Gradient Norm: 29.77765494200891
Epoch: 4662, Batch Gradient Norm after: 22.360679027700233
Epoch 4663/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8544263482093811
Epoch: 4663, Batch Gradient Norm: 30.767761366957366
Epoch: 4663, Batch Gradient Norm after: 22.36067601491
Epoch 4664/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.856840705871582
Epoch: 4664, Batch Gradient Norm: 29.771049507252304
Epoch: 4664, Batch Gradient Norm after: 22.360679397885637
Epoch 4665/10000, Prediction Accuracy = 58.77%, Loss = 0.8542820572853088
Epoch: 4665, Batch Gradient Norm: 30.761974405852346
Epoch: 4665, Batch Gradient Norm after: 22.360677710291178
Epoch 4666/10000, Prediction Accuracy = 58.798%, Loss = 0.8566972255706787
Epoch: 4666, Batch Gradient Norm: 29.761667433353942
Epoch: 4666, Batch Gradient Norm after: 22.360679228413762
Epoch 4667/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.854121744632721
Epoch: 4667, Batch Gradient Norm: 30.76043729415922
Epoch: 4667, Batch Gradient Norm after: 22.360676889213977
Epoch 4668/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8565443396568299
Epoch: 4668, Batch Gradient Norm: 29.753032203238718
Epoch: 4668, Batch Gradient Norm after: 22.360677097644295
Epoch 4669/10000, Prediction Accuracy = 58.814%, Loss = 0.8539560079574585
Epoch: 4669, Batch Gradient Norm: 30.759565609813237
Epoch: 4669, Batch Gradient Norm after: 22.360678805383017
Epoch 4670/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8564133644104004
Epoch: 4670, Batch Gradient Norm: 29.743210550179967
Epoch: 4670, Batch Gradient Norm after: 22.360679346289608
Epoch 4671/10000, Prediction Accuracy = 58.8%, Loss = 0.8537989377975463
Epoch: 4671, Batch Gradient Norm: 30.758283733634357
Epoch: 4671, Batch Gradient Norm after: 22.360676970099966
Epoch 4672/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.8562779784202575
Epoch: 4672, Batch Gradient Norm: 29.734668487963418
Epoch: 4672, Batch Gradient Norm after: 22.360680606941894
Epoch 4673/10000, Prediction Accuracy = 58.788%, Loss = 0.8536491632461548
Epoch: 4673, Batch Gradient Norm: 30.757681166396395
Epoch: 4673, Batch Gradient Norm after: 22.360676737723914
Epoch 4674/10000, Prediction Accuracy = 58.806000000000004%, Loss = 0.8561506152153016
Epoch: 4674, Batch Gradient Norm: 29.726085569857172
Epoch: 4674, Batch Gradient Norm after: 22.360680955517545
Epoch 4675/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8535000205039978
Epoch: 4675, Batch Gradient Norm: 30.750120655739767
Epoch: 4675, Batch Gradient Norm after: 22.36067768001975
Epoch 4676/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8559919834136963
Epoch: 4676, Batch Gradient Norm: 29.71822034015466
Epoch: 4676, Batch Gradient Norm after: 22.360679881768583
Epoch 4677/10000, Prediction Accuracy = 58.791999999999994%, Loss = 0.853344738483429
Epoch: 4677, Batch Gradient Norm: 30.74350816437113
Epoch: 4677, Batch Gradient Norm after: 22.360678042764754
Epoch 4678/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.8558314442634583
Epoch: 4678, Batch Gradient Norm: 29.713889234047812
Epoch: 4678, Batch Gradient Norm after: 22.36067980547189
Epoch 4679/10000, Prediction Accuracy = 58.814%, Loss = 0.8531830549240113
Epoch: 4679, Batch Gradient Norm: 30.736192825270898
Epoch: 4679, Batch Gradient Norm after: 22.360677476574043
Epoch 4680/10000, Prediction Accuracy = 58.766%, Loss = 0.8556730389595032
Epoch: 4680, Batch Gradient Norm: 29.708248813507726
Epoch: 4680, Batch Gradient Norm after: 22.36068094164415
Epoch 4681/10000, Prediction Accuracy = 58.822%, Loss = 0.8530301690101624
Epoch: 4681, Batch Gradient Norm: 30.731535006053896
Epoch: 4681, Batch Gradient Norm after: 22.360675472799805
Epoch 4682/10000, Prediction Accuracy = 58.790000000000006%, Loss = 0.8555317640304565
Epoch: 4682, Batch Gradient Norm: 29.70260141719521
Epoch: 4682, Batch Gradient Norm after: 22.360679698583766
Epoch 4683/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.8528897047042847
Epoch: 4683, Batch Gradient Norm: 30.722333131123207
Epoch: 4683, Batch Gradient Norm after: 22.360675711524646
Epoch 4684/10000, Prediction Accuracy = 58.80799999999999%, Loss = 0.8553844332695008
Epoch: 4684, Batch Gradient Norm: 29.696293752346442
Epoch: 4684, Batch Gradient Norm after: 22.360681352972065
Epoch 4685/10000, Prediction Accuracy = 58.79200000000001%, Loss = 0.8527338027954101
Epoch: 4685, Batch Gradient Norm: 30.71921707290462
Epoch: 4685, Batch Gradient Norm after: 22.36067831987927
Epoch 4686/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8552284479141236
Epoch: 4686, Batch Gradient Norm: 29.68626298063418
Epoch: 4686, Batch Gradient Norm after: 22.360680319250132
Epoch 4687/10000, Prediction Accuracy = 58.822%, Loss = 0.8525747537612915
Epoch: 4687, Batch Gradient Norm: 30.712387659792903
Epoch: 4687, Batch Gradient Norm after: 22.360678362462068
Epoch 4688/10000, Prediction Accuracy = 58.76400000000001%, Loss = 0.8550719022750854
Epoch: 4688, Batch Gradient Norm: 29.68166268391756
Epoch: 4688, Batch Gradient Norm after: 22.360678056337424
Epoch 4689/10000, Prediction Accuracy = 58.814%, Loss = 0.8524197936058044
Epoch: 4689, Batch Gradient Norm: 30.70653552299882
Epoch: 4689, Batch Gradient Norm after: 22.360675685861448
Epoch 4690/10000, Prediction Accuracy = 58.766%, Loss = 0.854913854598999
Epoch: 4690, Batch Gradient Norm: 29.67179766895958
Epoch: 4690, Batch Gradient Norm after: 22.360680976937836
Epoch 4691/10000, Prediction Accuracy = 58.822%, Loss = 0.852275288105011
Epoch: 4691, Batch Gradient Norm: 30.70433006056149
Epoch: 4691, Batch Gradient Norm after: 22.360675046671545
Epoch 4692/10000, Prediction Accuracy = 58.798%, Loss = 0.8547776103019714
Epoch: 4692, Batch Gradient Norm: 29.665233322374583
Epoch: 4692, Batch Gradient Norm after: 22.36068138665051
Epoch 4693/10000, Prediction Accuracy = 58.782000000000004%, Loss = 0.8521302461624145
Epoch: 4693, Batch Gradient Norm: 30.69928713710081
Epoch: 4693, Batch Gradient Norm after: 22.36067621509634
Epoch 4694/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8546350955963135
Epoch: 4694, Batch Gradient Norm: 29.658429873163932
Epoch: 4694, Batch Gradient Norm after: 22.360681181502912
Epoch 4695/10000, Prediction Accuracy = 58.81%, Loss = 0.8519710540771485
Epoch: 4695, Batch Gradient Norm: 30.69956333067056
Epoch: 4695, Batch Gradient Norm after: 22.360674671825155
Epoch 4696/10000, Prediction Accuracy = 58.778%, Loss = 0.8544908165931702
Epoch: 4696, Batch Gradient Norm: 29.647958135920938
Epoch: 4696, Batch Gradient Norm after: 22.360679509763493
Epoch 4697/10000, Prediction Accuracy = 58.814%, Loss = 0.8518050670623779
Epoch: 4697, Batch Gradient Norm: 30.69741087267369
Epoch: 4697, Batch Gradient Norm after: 22.360679519940497
Epoch 4698/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.8543501615524292
Epoch: 4698, Batch Gradient Norm: 29.641210317711728
Epoch: 4698, Batch Gradient Norm after: 22.360679171511244
Epoch 4699/10000, Prediction Accuracy = 58.80799999999999%, Loss = 0.8516496181488037
Epoch: 4699, Batch Gradient Norm: 30.695493946713235
Epoch: 4699, Batch Gradient Norm after: 22.360676690374262
Epoch 4700/10000, Prediction Accuracy = 58.782%, Loss = 0.8542128562927246
Epoch: 4700, Batch Gradient Norm: 29.633780543217412
Epoch: 4700, Batch Gradient Norm after: 22.360678328188204
Epoch 4701/10000, Prediction Accuracy = 58.81%, Loss = 0.8515044093132019
Epoch: 4701, Batch Gradient Norm: 30.69052980595197
Epoch: 4701, Batch Gradient Norm after: 22.360677477104034
Epoch 4702/10000, Prediction Accuracy = 58.814%, Loss = 0.8540745139122009
Epoch: 4702, Batch Gradient Norm: 29.624815257900945
Epoch: 4702, Batch Gradient Norm after: 22.36067717213321
Epoch 4703/10000, Prediction Accuracy = 58.79%, Loss = 0.8513569474220276
Epoch: 4703, Batch Gradient Norm: 30.688756362311842
Epoch: 4703, Batch Gradient Norm after: 22.36067809184885
Epoch 4704/10000, Prediction Accuracy = 58.80800000000001%, Loss = 0.8539307594299317
Epoch: 4704, Batch Gradient Norm: 29.616480432089887
Epoch: 4704, Batch Gradient Norm after: 22.360678806640518
Epoch 4705/10000, Prediction Accuracy = 58.822%, Loss = 0.8511971354484558
Epoch: 4705, Batch Gradient Norm: 30.682841341274873
Epoch: 4705, Batch Gradient Norm after: 22.36067858020196
Epoch 4706/10000, Prediction Accuracy = 58.79%, Loss = 0.8537715673446655
Epoch: 4706, Batch Gradient Norm: 29.60716233138182
Epoch: 4706, Batch Gradient Norm after: 22.360679600991055
Epoch 4707/10000, Prediction Accuracy = 58.81600000000001%, Loss = 0.8510432720184327
Epoch: 4707, Batch Gradient Norm: 30.676142037785315
Epoch: 4707, Batch Gradient Norm after: 22.360678118921548
Epoch 4708/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8536232709884644
Epoch: 4708, Batch Gradient Norm: 29.601771017500695
Epoch: 4708, Batch Gradient Norm after: 22.360679754019465
Epoch 4709/10000, Prediction Accuracy = 58.80999999999999%, Loss = 0.8508862733840943
Epoch: 4709, Batch Gradient Norm: 30.673603435994707
Epoch: 4709, Batch Gradient Norm after: 22.36067753759513
Epoch 4710/10000, Prediction Accuracy = 58.802%, Loss = 0.853483259677887
Epoch: 4710, Batch Gradient Norm: 29.592023586016527
Epoch: 4710, Batch Gradient Norm after: 22.36067865401159
Epoch 4711/10000, Prediction Accuracy = 58.826%, Loss = 0.8507402896881103
Epoch: 4711, Batch Gradient Norm: 30.667744522554642
Epoch: 4711, Batch Gradient Norm after: 22.360677510421624
Epoch 4712/10000, Prediction Accuracy = 58.812%, Loss = 0.8533437013626098
Epoch: 4712, Batch Gradient Norm: 29.588805821833827
Epoch: 4712, Batch Gradient Norm after: 22.360678405326567
Epoch 4713/10000, Prediction Accuracy = 58.812%, Loss = 0.8505889773368835
Epoch: 4713, Batch Gradient Norm: 30.66336627072277
Epoch: 4713, Batch Gradient Norm after: 22.36067993720341
Epoch 4714/10000, Prediction Accuracy = 58.815999999999995%, Loss = 0.8531949996948243
Epoch: 4714, Batch Gradient Norm: 29.578039901923628
Epoch: 4714, Batch Gradient Norm after: 22.36067820983628
Epoch 4715/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.8504298806190491
Epoch: 4715, Batch Gradient Norm: 30.664351458896903
Epoch: 4715, Batch Gradient Norm after: 22.360676742453304
Epoch 4716/10000, Prediction Accuracy = 58.79%, Loss = 0.8530585289001464
Epoch: 4716, Batch Gradient Norm: 29.570744790976896
Epoch: 4716, Batch Gradient Norm after: 22.360676841887383
Epoch 4717/10000, Prediction Accuracy = 58.82000000000001%, Loss = 0.8502680659294128
Epoch: 4717, Batch Gradient Norm: 30.66156694083624
Epoch: 4717, Batch Gradient Norm after: 22.36067919793102
Epoch 4718/10000, Prediction Accuracy = 58.78399999999999%, Loss = 0.8529248118400574
Epoch: 4718, Batch Gradient Norm: 29.559960540777116
Epoch: 4718, Batch Gradient Norm after: 22.360677747848534
Epoch 4719/10000, Prediction Accuracy = 58.824%, Loss = 0.8501179337501525
Epoch: 4719, Batch Gradient Norm: 30.66147460738918
Epoch: 4719, Batch Gradient Norm after: 22.36068006042404
Epoch 4720/10000, Prediction Accuracy = 58.786%, Loss = 0.8528002738952637
Epoch: 4720, Batch Gradient Norm: 29.549194556676344
Epoch: 4720, Batch Gradient Norm after: 22.36068019425362
Epoch 4721/10000, Prediction Accuracy = 58.814%, Loss = 0.8499692201614379
Epoch: 4721, Batch Gradient Norm: 30.657912147219733
Epoch: 4721, Batch Gradient Norm after: 22.360677392558657
Epoch 4722/10000, Prediction Accuracy = 58.82000000000001%, Loss = 0.8526708602905273
Epoch: 4722, Batch Gradient Norm: 29.54103201933294
Epoch: 4722, Batch Gradient Norm after: 22.360679543660982
Epoch 4723/10000, Prediction Accuracy = 58.822%, Loss = 0.8498180747032166
Epoch: 4723, Batch Gradient Norm: 30.65557688490162
Epoch: 4723, Batch Gradient Norm after: 22.360677703873115
Epoch 4724/10000, Prediction Accuracy = 58.79600000000001%, Loss = 0.8525315046310424
Epoch: 4724, Batch Gradient Norm: 29.532898278767696
Epoch: 4724, Batch Gradient Norm after: 22.36067998968933
Epoch 4725/10000, Prediction Accuracy = 58.836%, Loss = 0.8496554613113403
Epoch: 4725, Batch Gradient Norm: 30.652893611519527
Epoch: 4725, Batch Gradient Norm after: 22.36067722758368
Epoch 4726/10000, Prediction Accuracy = 58.767999999999994%, Loss = 0.8523885011672974
Epoch: 4726, Batch Gradient Norm: 29.525402369898828
Epoch: 4726, Batch Gradient Norm after: 22.36067754090445
Epoch 4727/10000, Prediction Accuracy = 58.824%, Loss = 0.8495003700256347
Epoch: 4727, Batch Gradient Norm: 30.65045047451716
Epoch: 4727, Batch Gradient Norm after: 22.360675464078124
Epoch 4728/10000, Prediction Accuracy = 58.782000000000004%, Loss = 0.8522467255592346
Epoch: 4728, Batch Gradient Norm: 29.51958916319268
Epoch: 4728, Batch Gradient Norm after: 22.360678363599284
Epoch 4729/10000, Prediction Accuracy = 58.824%, Loss = 0.8493491888046265
Epoch: 4729, Batch Gradient Norm: 30.64998523298512
Epoch: 4729, Batch Gradient Norm after: 22.36067834478923
Epoch 4730/10000, Prediction Accuracy = 58.782000000000004%, Loss = 0.8521110892295838
Epoch: 4730, Batch Gradient Norm: 29.512594119072176
Epoch: 4730, Batch Gradient Norm after: 22.360679434564695
Epoch 4731/10000, Prediction Accuracy = 58.834%, Loss = 0.8492082238197327
Epoch: 4731, Batch Gradient Norm: 30.64551549895273
Epoch: 4731, Batch Gradient Norm after: 22.36067701344546
Epoch 4732/10000, Prediction Accuracy = 58.802%, Loss = 0.8519775748252869
Epoch: 4732, Batch Gradient Norm: 29.504225684390637
Epoch: 4732, Batch Gradient Norm after: 22.360677782606526
Epoch 4733/10000, Prediction Accuracy = 58.834%, Loss = 0.8490594983100891
Epoch: 4733, Batch Gradient Norm: 30.640864569764094
Epoch: 4733, Batch Gradient Norm after: 22.36067822242502
Epoch 4734/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.8518335223197937
Epoch: 4734, Batch Gradient Norm: 29.497772296702102
Epoch: 4734, Batch Gradient Norm after: 22.360678572279962
Epoch 4735/10000, Prediction Accuracy = 58.834%, Loss = 0.8488877773284912
Epoch: 4735, Batch Gradient Norm: 30.642810496541095
Epoch: 4735, Batch Gradient Norm after: 22.360680023507346
Epoch 4736/10000, Prediction Accuracy = 58.782%, Loss = 0.8517006039619446
Epoch: 4736, Batch Gradient Norm: 29.488852484394112
Epoch: 4736, Batch Gradient Norm after: 22.360677907115853
Epoch 4737/10000, Prediction Accuracy = 58.834%, Loss = 0.8487305402755737
Epoch: 4737, Batch Gradient Norm: 30.642457251149853
Epoch: 4737, Batch Gradient Norm after: 22.360678087666358
Epoch 4738/10000, Prediction Accuracy = 58.782%, Loss = 0.851575744152069
Epoch: 4738, Batch Gradient Norm: 29.480333047098817
Epoch: 4738, Batch Gradient Norm after: 22.360679065322515
Epoch 4739/10000, Prediction Accuracy = 58.824%, Loss = 0.8485825300216675
Epoch: 4739, Batch Gradient Norm: 30.641529198812464
Epoch: 4739, Batch Gradient Norm after: 22.360677288942938
Epoch 4740/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.8514437437057495
Epoch: 4740, Batch Gradient Norm: 29.472378298944054
Epoch: 4740, Batch Gradient Norm after: 22.36067746073708
Epoch 4741/10000, Prediction Accuracy = 58.81400000000001%, Loss = 0.8484395623207093
Epoch: 4741, Batch Gradient Norm: 30.636977873438024
Epoch: 4741, Batch Gradient Norm after: 22.360676288198693
Epoch 4742/10000, Prediction Accuracy = 58.806000000000004%, Loss = 0.8513095617294312
Epoch: 4742, Batch Gradient Norm: 29.465820286945902
Epoch: 4742, Batch Gradient Norm after: 22.36067892202872
Epoch 4743/10000, Prediction Accuracy = 58.822%, Loss = 0.8482823252677918
Epoch: 4743, Batch Gradient Norm: 30.6327681213296
Epoch: 4743, Batch Gradient Norm after: 22.360678291894267
Epoch 4744/10000, Prediction Accuracy = 58.788%, Loss = 0.8511592745780945
Epoch: 4744, Batch Gradient Norm: 29.458166918715637
Epoch: 4744, Batch Gradient Norm after: 22.360677487570246
Epoch 4745/10000, Prediction Accuracy = 58.84799999999999%, Loss = 0.8481236219406127
Epoch: 4745, Batch Gradient Norm: 30.630220560487114
Epoch: 4745, Batch Gradient Norm after: 22.360678955178688
Epoch 4746/10000, Prediction Accuracy = 58.794%, Loss = 0.8510167121887207
Epoch: 4746, Batch Gradient Norm: 29.45082784567598
Epoch: 4746, Batch Gradient Norm after: 22.360678040963936
Epoch 4747/10000, Prediction Accuracy = 58.842%, Loss = 0.8479713439941406
Epoch: 4747, Batch Gradient Norm: 30.624766571803953
Epoch: 4747, Batch Gradient Norm after: 22.360676966074003
Epoch 4748/10000, Prediction Accuracy = 58.784000000000006%, Loss = 0.8508713841438293
Epoch: 4748, Batch Gradient Norm: 29.44286370449206
Epoch: 4748, Batch Gradient Norm after: 22.3606772341188
Epoch 4749/10000, Prediction Accuracy = 58.82000000000001%, Loss = 0.8478330373764038
Epoch: 4749, Batch Gradient Norm: 30.618679178279695
Epoch: 4749, Batch Gradient Norm after: 22.360677478157005
Epoch 4750/10000, Prediction Accuracy = 58.8%, Loss = 0.8507338523864746
Epoch: 4750, Batch Gradient Norm: 29.43481077497332
Epoch: 4750, Batch Gradient Norm after: 22.360676803871122
Epoch 4751/10000, Prediction Accuracy = 58.826%, Loss = 0.8476830720901489
Epoch: 4751, Batch Gradient Norm: 30.61565646895502
Epoch: 4751, Batch Gradient Norm after: 22.3606758962403
Epoch 4752/10000, Prediction Accuracy = 58.802%, Loss = 0.8505940556526184
Epoch: 4752, Batch Gradient Norm: 29.424742048996546
Epoch: 4752, Batch Gradient Norm after: 22.36067773831226
Epoch 4753/10000, Prediction Accuracy = 58.831999999999994%, Loss = 0.8475317120552063
Epoch: 4753, Batch Gradient Norm: 30.612097316606413
Epoch: 4753, Batch Gradient Norm after: 22.360677595945166
Epoch 4754/10000, Prediction Accuracy = 58.80800000000001%, Loss = 0.8504463791847229
Epoch: 4754, Batch Gradient Norm: 29.419581566138053
Epoch: 4754, Batch Gradient Norm after: 22.360677059716593
Epoch 4755/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8473741292953492
Epoch: 4755, Batch Gradient Norm: 30.6065518151976
Epoch: 4755, Batch Gradient Norm after: 22.36067723572896
Epoch 4756/10000, Prediction Accuracy = 58.798%, Loss = 0.8502984881401062
Epoch: 4756, Batch Gradient Norm: 29.412403920187586
Epoch: 4756, Batch Gradient Norm after: 22.360677206083697
Epoch 4757/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.8472255110740662
Epoch: 4757, Batch Gradient Norm: 30.5993918877721
Epoch: 4757, Batch Gradient Norm after: 22.360679009375765
Epoch 4758/10000, Prediction Accuracy = 58.806%, Loss = 0.850156307220459
Epoch: 4758, Batch Gradient Norm: 29.40701497218585
Epoch: 4758, Batch Gradient Norm after: 22.360679219379303
Epoch 4759/10000, Prediction Accuracy = 58.83%, Loss = 0.8470836639404297
Epoch: 4759, Batch Gradient Norm: 30.592988877259547
Epoch: 4759, Batch Gradient Norm after: 22.360680110305623
Epoch 4760/10000, Prediction Accuracy = 58.815999999999995%, Loss = 0.8500075817108155
Epoch: 4760, Batch Gradient Norm: 29.401999328303727
Epoch: 4760, Batch Gradient Norm after: 22.360679036724264
Epoch 4761/10000, Prediction Accuracy = 58.824%, Loss = 0.8469318151473999
Epoch: 4761, Batch Gradient Norm: 30.58576990961799
Epoch: 4761, Batch Gradient Norm after: 22.360678728379437
Epoch 4762/10000, Prediction Accuracy = 58.812%, Loss = 0.8498486161231995
Epoch: 4762, Batch Gradient Norm: 29.397725210206307
Epoch: 4762, Batch Gradient Norm after: 22.360678247742644
Epoch 4763/10000, Prediction Accuracy = 58.836%, Loss = 0.8467766046524048
Epoch: 4763, Batch Gradient Norm: 30.578098148227273
Epoch: 4763, Batch Gradient Norm after: 22.36067936682364
Epoch 4764/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.8496947765350342
Epoch: 4764, Batch Gradient Norm: 29.390703305636396
Epoch: 4764, Batch Gradient Norm after: 22.360679845008317
Epoch 4765/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.8466254353523255
Epoch: 4765, Batch Gradient Norm: 30.573178908103547
Epoch: 4765, Batch Gradient Norm after: 22.3606792148826
Epoch 4766/10000, Prediction Accuracy = 58.836%, Loss = 0.8495522856712341
Epoch: 4766, Batch Gradient Norm: 29.38405840585938
Epoch: 4766, Batch Gradient Norm after: 22.360677948604398
Epoch 4767/10000, Prediction Accuracy = 58.836%, Loss = 0.8464794635772706
Epoch: 4767, Batch Gradient Norm: 30.569824534021595
Epoch: 4767, Batch Gradient Norm after: 22.360678337574928
Epoch 4768/10000, Prediction Accuracy = 58.824%, Loss = 0.849413800239563
Epoch: 4768, Batch Gradient Norm: 29.37821942943502
Epoch: 4768, Batch Gradient Norm after: 22.360678947940904
Epoch 4769/10000, Prediction Accuracy = 58.834%, Loss = 0.8463338255882263
Epoch: 4769, Batch Gradient Norm: 30.563491657385345
Epoch: 4769, Batch Gradient Norm after: 22.360677186569966
Epoch 4770/10000, Prediction Accuracy = 58.826%, Loss = 0.8492675423622131
Epoch: 4770, Batch Gradient Norm: 29.371251077076053
Epoch: 4770, Batch Gradient Norm after: 22.36067828122652
Epoch 4771/10000, Prediction Accuracy = 58.834%, Loss = 0.8461827158927917
Epoch: 4771, Batch Gradient Norm: 30.556758777564674
Epoch: 4771, Batch Gradient Norm after: 22.360680454967227
Epoch 4772/10000, Prediction Accuracy = 58.824%, Loss = 0.849110770225525
Epoch: 4772, Batch Gradient Norm: 29.36376777972005
Epoch: 4772, Batch Gradient Norm after: 22.36067880863459
Epoch 4773/10000, Prediction Accuracy = 58.834%, Loss = 0.84603031873703
Epoch: 4773, Batch Gradient Norm: 30.55044328171892
Epoch: 4773, Batch Gradient Norm after: 22.360678651851686
Epoch 4774/10000, Prediction Accuracy = 58.815999999999995%, Loss = 0.8489636421203614
Epoch: 4774, Batch Gradient Norm: 29.359157904148773
Epoch: 4774, Batch Gradient Norm after: 22.360679655330387
Epoch 4775/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.8458810925483704
Epoch: 4775, Batch Gradient Norm: 30.54270945759891
Epoch: 4775, Batch Gradient Norm after: 22.36067951806935
Epoch 4776/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.8488139152526856
Epoch: 4776, Batch Gradient Norm: 29.353792333814702
Epoch: 4776, Batch Gradient Norm after: 22.36067740927827
Epoch 4777/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.8457389831542969
Epoch: 4777, Batch Gradient Norm: 30.535983003422412
Epoch: 4777, Batch Gradient Norm after: 22.360679026359865
Epoch 4778/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8486776947975159
Epoch: 4778, Batch Gradient Norm: 29.34878918858816
Epoch: 4778, Batch Gradient Norm after: 22.36067880977015
Epoch 4779/10000, Prediction Accuracy = 58.838%, Loss = 0.8456032872200012
Epoch: 4779, Batch Gradient Norm: 30.532701590943198
Epoch: 4779, Batch Gradient Norm after: 22.360680294407953
Epoch 4780/10000, Prediction Accuracy = 58.854%, Loss = 0.8485355615615845
Epoch: 4780, Batch Gradient Norm: 29.34035039256046
Epoch: 4780, Batch Gradient Norm after: 22.360679417168136
Epoch 4781/10000, Prediction Accuracy = 58.831999999999994%, Loss = 0.8454497933387757
Epoch: 4781, Batch Gradient Norm: 30.52976074191876
Epoch: 4781, Batch Gradient Norm after: 22.360678547383202
Epoch 4782/10000, Prediction Accuracy = 58.848%, Loss = 0.848389732837677
Epoch: 4782, Batch Gradient Norm: 29.33285041349938
Epoch: 4782, Batch Gradient Norm after: 22.36067836475432
Epoch 4783/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.8452908396720886
Epoch: 4783, Batch Gradient Norm: 30.52546116840902
Epoch: 4783, Batch Gradient Norm after: 22.36067714301249
Epoch 4784/10000, Prediction Accuracy = 58.822%, Loss = 0.8482486724853515
Epoch: 4784, Batch Gradient Norm: 29.32732354088849
Epoch: 4784, Batch Gradient Norm after: 22.36067632572067
Epoch 4785/10000, Prediction Accuracy = 58.83%, Loss = 0.8451449632644653
Epoch: 4785, Batch Gradient Norm: 30.517153275601242
Epoch: 4785, Batch Gradient Norm after: 22.36067847918525
Epoch 4786/10000, Prediction Accuracy = 58.84400000000001%, Loss = 0.8481007575988769
Epoch: 4786, Batch Gradient Norm: 29.323280915337396
Epoch: 4786, Batch Gradient Norm after: 22.360678563568595
Epoch 4787/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8450114130973816
Epoch: 4787, Batch Gradient Norm: 30.508632751413543
Epoch: 4787, Batch Gradient Norm after: 22.360680597216557
Epoch 4788/10000, Prediction Accuracy = 58.867999999999995%, Loss = 0.8479567527770996
Epoch: 4788, Batch Gradient Norm: 29.31616908750547
Epoch: 4788, Batch Gradient Norm after: 22.360677987603648
Epoch 4789/10000, Prediction Accuracy = 58.843999999999994%, Loss = 0.8448734283447266
Epoch: 4789, Batch Gradient Norm: 30.500481650535644
Epoch: 4789, Batch Gradient Norm after: 22.36067988366307
Epoch 4790/10000, Prediction Accuracy = 58.852%, Loss = 0.8478014945983887
Epoch: 4790, Batch Gradient Norm: 29.311531532498616
Epoch: 4790, Batch Gradient Norm after: 22.360679382264372
Epoch 4791/10000, Prediction Accuracy = 58.822%, Loss = 0.8447174906730652
Epoch: 4791, Batch Gradient Norm: 30.49211649073821
Epoch: 4791, Batch Gradient Norm after: 22.360679087497232
Epoch 4792/10000, Prediction Accuracy = 58.842%, Loss = 0.8476474404335022
Epoch: 4792, Batch Gradient Norm: 29.30569182725297
Epoch: 4792, Batch Gradient Norm after: 22.36068030122706
Epoch 4793/10000, Prediction Accuracy = 58.83%, Loss = 0.8445641160011291
Epoch: 4793, Batch Gradient Norm: 30.484808561006616
Epoch: 4793, Batch Gradient Norm after: 22.360681145035667
Epoch 4794/10000, Prediction Accuracy = 58.83%, Loss = 0.8474948167800903
Epoch: 4794, Batch Gradient Norm: 29.302085017656402
Epoch: 4794, Batch Gradient Norm after: 22.360678549460758
Epoch 4795/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.844423794746399
Epoch: 4795, Batch Gradient Norm: 30.473788978812482
Epoch: 4795, Batch Gradient Norm after: 22.360680260748644
Epoch 4796/10000, Prediction Accuracy = 58.86400000000001%, Loss = 0.847346842288971
Epoch: 4796, Batch Gradient Norm: 29.29610149255081
Epoch: 4796, Batch Gradient Norm after: 22.36067950295054
Epoch 4797/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.8442956805229187
Epoch: 4797, Batch Gradient Norm: 30.46933425733883
Epoch: 4797, Batch Gradient Norm after: 22.36067835440055
Epoch 4798/10000, Prediction Accuracy = 58.89%, Loss = 0.8472086429595947
Epoch: 4798, Batch Gradient Norm: 29.287605179260478
Epoch: 4798, Batch Gradient Norm after: 22.36067695035178
Epoch 4799/10000, Prediction Accuracy = 58.864%, Loss = 0.8441470503807068
Epoch: 4799, Batch Gradient Norm: 30.46020456259012
Epoch: 4799, Batch Gradient Norm after: 22.36067924023353
Epoch 4800/10000, Prediction Accuracy = 58.848%, Loss = 0.8470523595809937
Epoch: 4800, Batch Gradient Norm: 29.28002421370688
Epoch: 4800, Batch Gradient Norm after: 22.360679400679004
Epoch 4801/10000, Prediction Accuracy = 58.842%, Loss = 0.8439928650856018
Epoch: 4801, Batch Gradient Norm: 30.45016220583296
Epoch: 4801, Batch Gradient Norm after: 22.3606777014016
Epoch 4802/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8468927264213562
Epoch: 4802, Batch Gradient Norm: 29.275671409906906
Epoch: 4802, Batch Gradient Norm after: 22.36067784152136
Epoch 4803/10000, Prediction Accuracy = 58.874%, Loss = 0.8438393950462342
Epoch: 4803, Batch Gradient Norm: 30.441501677747972
Epoch: 4803, Batch Gradient Norm after: 22.36067848681836
Epoch 4804/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.8467338919639588
Epoch: 4804, Batch Gradient Norm: 29.270049742247647
Epoch: 4804, Batch Gradient Norm after: 22.36067851156701
Epoch 4805/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.8437068343162537
Epoch: 4805, Batch Gradient Norm: 30.431616831037807
Epoch: 4805, Batch Gradient Norm after: 22.360678005611277
Epoch 4806/10000, Prediction Accuracy = 58.886%, Loss = 0.84658864736557
Epoch: 4806, Batch Gradient Norm: 29.26519345806118
Epoch: 4806, Batch Gradient Norm after: 22.360678597049233
Epoch 4807/10000, Prediction Accuracy = 58.876%, Loss = 0.8435765743255615
Epoch: 4807, Batch Gradient Norm: 30.422887787330875
Epoch: 4807, Batch Gradient Norm after: 22.360677983469518
Epoch 4808/10000, Prediction Accuracy = 58.884%, Loss = 0.8464391827583313
Epoch: 4808, Batch Gradient Norm: 29.259301650118843
Epoch: 4808, Batch Gradient Norm after: 22.36067917822342
Epoch 4809/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.8434260487556458
Epoch: 4809, Batch Gradient Norm: 30.413064420487864
Epoch: 4809, Batch Gradient Norm after: 22.36067968290741
Epoch 4810/10000, Prediction Accuracy = 58.87199999999999%, Loss = 0.8462743759155273
Epoch: 4810, Batch Gradient Norm: 29.252560437655294
Epoch: 4810, Batch Gradient Norm after: 22.360678680334292
Epoch 4811/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.84327232837677
Epoch: 4811, Batch Gradient Norm: 30.4080646430367
Epoch: 4811, Batch Gradient Norm after: 22.36067717015783
Epoch 4812/10000, Prediction Accuracy = 58.842000000000006%, Loss = 0.8461264252662659
Epoch: 4812, Batch Gradient Norm: 29.245475719243693
Epoch: 4812, Batch Gradient Norm after: 22.360679061760713
Epoch 4813/10000, Prediction Accuracy = 58.870000000000005%, Loss = 0.8431341648101807
Epoch: 4813, Batch Gradient Norm: 30.40124159438531
Epoch: 4813, Batch Gradient Norm after: 22.360678783968147
Epoch 4814/10000, Prediction Accuracy = 58.839999999999996%, Loss = 0.8459770202636718
Epoch: 4814, Batch Gradient Norm: 29.24293765906724
Epoch: 4814, Batch Gradient Norm after: 22.360680202864824
Epoch 4815/10000, Prediction Accuracy = 58.86800000000001%, Loss = 0.8429969787597656
Epoch: 4815, Batch Gradient Norm: 30.394834704429382
Epoch: 4815, Batch Gradient Norm after: 22.360677272849127
Epoch 4816/10000, Prediction Accuracy = 58.89%, Loss = 0.8458357691764832
Epoch: 4816, Batch Gradient Norm: 29.23761867952991
Epoch: 4816, Batch Gradient Norm after: 22.360677293120972
Epoch 4817/10000, Prediction Accuracy = 58.89%, Loss = 0.8428653120994568
Epoch: 4817, Batch Gradient Norm: 30.38548791512473
Epoch: 4817, Batch Gradient Norm after: 22.360678722392695
Epoch 4818/10000, Prediction Accuracy = 58.89200000000001%, Loss = 0.8456910610198974
Epoch: 4818, Batch Gradient Norm: 29.2323332107152
Epoch: 4818, Batch Gradient Norm after: 22.360678712228378
Epoch 4819/10000, Prediction Accuracy = 58.89200000000001%, Loss = 0.8427153944969177
Epoch: 4819, Batch Gradient Norm: 30.37357214126276
Epoch: 4819, Batch Gradient Norm after: 22.360678650834547
Epoch 4820/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.8455230474472046
Epoch: 4820, Batch Gradient Norm: 29.22736121167239
Epoch: 4820, Batch Gradient Norm after: 22.360678364287534
Epoch 4821/10000, Prediction Accuracy = 58.86600000000001%, Loss = 0.8425647377967834
Epoch: 4821, Batch Gradient Norm: 30.366178460434142
Epoch: 4821, Batch Gradient Norm after: 22.360677478592716
Epoch 4822/10000, Prediction Accuracy = 58.846000000000004%, Loss = 0.8453658819198608
Epoch: 4822, Batch Gradient Norm: 29.220631531586818
Epoch: 4822, Batch Gradient Norm after: 22.360678597498303
Epoch 4823/10000, Prediction Accuracy = 58.874%, Loss = 0.8424186825752258
Epoch: 4823, Batch Gradient Norm: 30.35643414599899
Epoch: 4823, Batch Gradient Norm after: 22.360678475784088
Epoch 4824/10000, Prediction Accuracy = 58.86%, Loss = 0.8452187895774841
Epoch: 4824, Batch Gradient Norm: 29.216693151774805
Epoch: 4824, Batch Gradient Norm after: 22.36067929889423
Epoch 4825/10000, Prediction Accuracy = 58.894000000000005%, Loss = 0.8422865986824035
Epoch: 4825, Batch Gradient Norm: 30.343618763229102
Epoch: 4825, Batch Gradient Norm after: 22.360679519087764
Epoch 4826/10000, Prediction Accuracy = 58.89399999999999%, Loss = 0.845068359375
Epoch: 4826, Batch Gradient Norm: 29.212495001324744
Epoch: 4826, Batch Gradient Norm after: 22.360679362585195
Epoch 4827/10000, Prediction Accuracy = 58.903999999999996%, Loss = 0.8421558022499085
Epoch: 4827, Batch Gradient Norm: 30.335386178892954
Epoch: 4827, Batch Gradient Norm after: 22.36067755218008
Epoch 4828/10000, Prediction Accuracy = 58.898%, Loss = 0.8449194073677063
Epoch: 4828, Batch Gradient Norm: 29.206350826722876
Epoch: 4828, Batch Gradient Norm after: 22.360678003548845
Epoch 4829/10000, Prediction Accuracy = 58.903999999999996%, Loss = 0.8420059561729432
Epoch: 4829, Batch Gradient Norm: 30.321024940305936
Epoch: 4829, Batch Gradient Norm after: 22.360680018409425
Epoch 4830/10000, Prediction Accuracy = 58.882000000000005%, Loss = 0.8447454333305359
Epoch: 4830, Batch Gradient Norm: 29.203206830117804
Epoch: 4830, Batch Gradient Norm after: 22.360678370729126
Epoch 4831/10000, Prediction Accuracy = 58.86600000000001%, Loss = 0.8418608784675599
Epoch: 4831, Batch Gradient Norm: 30.304913234937118
Epoch: 4831, Batch Gradient Norm after: 22.360679270821123
Epoch 4832/10000, Prediction Accuracy = 58.854%, Loss = 0.8445692420005798
Epoch: 4832, Batch Gradient Norm: 29.20129527938068
Epoch: 4832, Batch Gradient Norm after: 22.360679632100084
Epoch 4833/10000, Prediction Accuracy = 58.872%, Loss = 0.8417167663574219
Epoch: 4833, Batch Gradient Norm: 30.29236009808075
Epoch: 4833, Batch Gradient Norm after: 22.360679380160658
Epoch 4834/10000, Prediction Accuracy = 58.870000000000005%, Loss = 0.8444086074829101
Epoch: 4834, Batch Gradient Norm: 29.198017158578736
Epoch: 4834, Batch Gradient Norm after: 22.36068015585556
Epoch 4835/10000, Prediction Accuracy = 58.907999999999994%, Loss = 0.8415938019752502
Epoch: 4835, Batch Gradient Norm: 30.276949964814886
Epoch: 4835, Batch Gradient Norm after: 22.360678579429113
Epoch 4836/10000, Prediction Accuracy = 58.928%, Loss = 0.8442577838897705
Epoch: 4836, Batch Gradient Norm: 29.191692425320948
Epoch: 4836, Batch Gradient Norm after: 22.36067992653032
Epoch 4837/10000, Prediction Accuracy = 58.89%, Loss = 0.8414656758308411
Epoch: 4837, Batch Gradient Norm: 30.263688927782948
Epoch: 4837, Batch Gradient Norm after: 22.360674882491523
Epoch 4838/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.8440934419631958
Epoch: 4838, Batch Gradient Norm: 29.186256036774427
Epoch: 4838, Batch Gradient Norm after: 22.360678822836242
Epoch 4839/10000, Prediction Accuracy = 58.914%, Loss = 0.8413116931915283
Epoch: 4839, Batch Gradient Norm: 30.25376059437089
Epoch: 4839, Batch Gradient Norm after: 22.360677426177
Epoch 4840/10000, Prediction Accuracy = 58.862%, Loss = 0.8439233303070068
Epoch: 4840, Batch Gradient Norm: 29.179283730059296
Epoch: 4840, Batch Gradient Norm after: 22.360678623056774
Epoch 4841/10000, Prediction Accuracy = 58.886%, Loss = 0.8411636352539062
Epoch: 4841, Batch Gradient Norm: 30.24701274245405
Epoch: 4841, Batch Gradient Norm after: 22.360676807032245
Epoch 4842/10000, Prediction Accuracy = 58.862%, Loss = 0.8437777519226074
Epoch: 4842, Batch Gradient Norm: 29.17423217084756
Epoch: 4842, Batch Gradient Norm after: 22.36067594675023
Epoch 4843/10000, Prediction Accuracy = 58.891999999999996%, Loss = 0.8410224795341492
Epoch: 4843, Batch Gradient Norm: 30.2403302299331
Epoch: 4843, Batch Gradient Norm after: 22.36067939442778
Epoch 4844/10000, Prediction Accuracy = 58.88399999999999%, Loss = 0.8436331510543823
Epoch: 4844, Batch Gradient Norm: 29.168900901370154
Epoch: 4844, Batch Gradient Norm after: 22.360678038630383
Epoch 4845/10000, Prediction Accuracy = 58.906000000000006%, Loss = 0.8408972978591919
Epoch: 4845, Batch Gradient Norm: 30.235201223747847
Epoch: 4845, Batch Gradient Norm after: 22.36067856509603
Epoch 4846/10000, Prediction Accuracy = 58.926%, Loss = 0.843510365486145
Epoch: 4846, Batch Gradient Norm: 29.159398815047567
Epoch: 4846, Batch Gradient Norm after: 22.36067984089748
Epoch 4847/10000, Prediction Accuracy = 58.902%, Loss = 0.8407597661018371
Epoch: 4847, Batch Gradient Norm: 30.230070033036743
Epoch: 4847, Batch Gradient Norm after: 22.36067982357708
Epoch 4848/10000, Prediction Accuracy = 58.894000000000005%, Loss = 0.8433600068092346
Epoch: 4848, Batch Gradient Norm: 29.151591271965465
Epoch: 4848, Batch Gradient Norm after: 22.360678098640058
Epoch 4849/10000, Prediction Accuracy = 58.910000000000004%, Loss = 0.8405976533889771
Epoch: 4849, Batch Gradient Norm: 30.222596509532803
Epoch: 4849, Batch Gradient Norm after: 22.360677762938767
Epoch 4850/10000, Prediction Accuracy = 58.86800000000001%, Loss = 0.8432102680206299
Epoch: 4850, Batch Gradient Norm: 29.14742715180457
Epoch: 4850, Batch Gradient Norm after: 22.360675657732507
Epoch 4851/10000, Prediction Accuracy = 58.89399999999999%, Loss = 0.8404461860656738
Epoch: 4851, Batch Gradient Norm: 30.211512090788858
Epoch: 4851, Batch Gradient Norm after: 22.360679386195272
Epoch 4852/10000, Prediction Accuracy = 58.87600000000001%, Loss = 0.8430590867996216
Epoch: 4852, Batch Gradient Norm: 29.13984962887213
Epoch: 4852, Batch Gradient Norm after: 22.3606770106662
Epoch 4853/10000, Prediction Accuracy = 58.906000000000006%, Loss = 0.8403108954429627
Epoch: 4853, Batch Gradient Norm: 30.204243398348485
Epoch: 4853, Batch Gradient Norm after: 22.36067735190893
Epoch 4854/10000, Prediction Accuracy = 58.894000000000005%, Loss = 0.8429211378097534
Epoch: 4854, Batch Gradient Norm: 29.136890887942606
Epoch: 4854, Batch Gradient Norm after: 22.36067656310816
Epoch 4855/10000, Prediction Accuracy = 58.902%, Loss = 0.8401844143867493
Epoch: 4855, Batch Gradient Norm: 30.19269178145411
Epoch: 4855, Batch Gradient Norm after: 22.36067924577703
Epoch 4856/10000, Prediction Accuracy = 58.918000000000006%, Loss = 0.8427698969841003
Epoch: 4856, Batch Gradient Norm: 29.131558315337358
Epoch: 4856, Batch Gradient Norm after: 22.360678184034658
Epoch 4857/10000, Prediction Accuracy = 58.903999999999996%, Loss = 0.8400465726852417
Epoch: 4857, Batch Gradient Norm: 30.183703905061112
Epoch: 4857, Batch Gradient Norm after: 22.36067811512048
Epoch 4858/10000, Prediction Accuracy = 58.90400000000001%, Loss = 0.8426091313362122
Epoch: 4858, Batch Gradient Norm: 29.12672413388321
Epoch: 4858, Batch Gradient Norm after: 22.3606780326905
Epoch 4859/10000, Prediction Accuracy = 58.898%, Loss = 0.8398948907852173
Epoch: 4859, Batch Gradient Norm: 30.17168029836751
Epoch: 4859, Batch Gradient Norm after: 22.360680166227166
Epoch 4860/10000, Prediction Accuracy = 58.884%, Loss = 0.842454469203949
Epoch: 4860, Batch Gradient Norm: 29.121461924744956
Epoch: 4860, Batch Gradient Norm after: 22.360676815477383
Epoch 4861/10000, Prediction Accuracy = 58.89%, Loss = 0.839749538898468
Epoch: 4861, Batch Gradient Norm: 30.1639447949701
Epoch: 4861, Batch Gradient Norm after: 22.360677400299146
Epoch 4862/10000, Prediction Accuracy = 58.891999999999996%, Loss = 0.8423074960708619
Epoch: 4862, Batch Gradient Norm: 29.115873160438692
Epoch: 4862, Batch Gradient Norm after: 22.36067613168525
Epoch 4863/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.8396133422851563
Epoch: 4863, Batch Gradient Norm: 30.152453949146697
Epoch: 4863, Batch Gradient Norm after: 22.36068003285405
Epoch 4864/10000, Prediction Accuracy = 58.9%, Loss = 0.8421589374542237
Epoch: 4864, Batch Gradient Norm: 29.113380937253222
Epoch: 4864, Batch Gradient Norm after: 22.36067831699515
Epoch 4865/10000, Prediction Accuracy = 58.91199999999999%, Loss = 0.8394863605499268
Epoch: 4865, Batch Gradient Norm: 30.143336646825727
Epoch: 4865, Batch Gradient Norm after: 22.360679216302543
Epoch 4866/10000, Prediction Accuracy = 58.918000000000006%, Loss = 0.8420130133628845
Epoch: 4866, Batch Gradient Norm: 29.10625316013974
Epoch: 4866, Batch Gradient Norm after: 22.360678338100392
Epoch 4867/10000, Prediction Accuracy = 58.898%, Loss = 0.839344072341919
Epoch: 4867, Batch Gradient Norm: 30.133290867616303
Epoch: 4867, Batch Gradient Norm after: 22.36067869999216
Epoch 4868/10000, Prediction Accuracy = 58.91799999999999%, Loss = 0.8418554782867431
Epoch: 4868, Batch Gradient Norm: 29.10000337654948
Epoch: 4868, Batch Gradient Norm after: 22.360677875597045
Epoch 4869/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.8391955733299256
Epoch: 4869, Batch Gradient Norm: 30.1269578510404
Epoch: 4869, Batch Gradient Norm after: 22.3606771465466
Epoch 4870/10000, Prediction Accuracy = 58.902%, Loss = 0.8417084813117981
Epoch: 4870, Batch Gradient Norm: 29.095553055535724
Epoch: 4870, Batch Gradient Norm after: 22.360675038006015
Epoch 4871/10000, Prediction Accuracy = 58.898%, Loss = 0.8390488624572754
Epoch: 4871, Batch Gradient Norm: 30.115665277804723
Epoch: 4871, Batch Gradient Norm after: 22.36067870719616
Epoch 4872/10000, Prediction Accuracy = 58.910000000000004%, Loss = 0.8415532827377319
Epoch: 4872, Batch Gradient Norm: 29.09258982386896
Epoch: 4872, Batch Gradient Norm after: 22.360677088320337
Epoch 4873/10000, Prediction Accuracy = 58.914%, Loss = 0.8389134168624878
Epoch: 4873, Batch Gradient Norm: 30.103553374464443
Epoch: 4873, Batch Gradient Norm after: 22.36068024078599
Epoch 4874/10000, Prediction Accuracy = 58.92%, Loss = 0.8414137244224549
Epoch: 4874, Batch Gradient Norm: 29.087485937762597
Epoch: 4874, Batch Gradient Norm after: 22.360679344391507
Epoch 4875/10000, Prediction Accuracy = 58.919999999999995%, Loss = 0.8387905597686768
Epoch: 4875, Batch Gradient Norm: 30.092803634756876
Epoch: 4875, Batch Gradient Norm after: 22.360677130072848
Epoch 4876/10000, Prediction Accuracy = 58.926%, Loss = 0.8412610650062561
Epoch: 4876, Batch Gradient Norm: 29.082263252260212
Epoch: 4876, Batch Gradient Norm after: 22.360679148819152
Epoch 4877/10000, Prediction Accuracy = 58.906000000000006%, Loss = 0.8386476159095764
Epoch: 4877, Batch Gradient Norm: 30.079948536859284
Epoch: 4877, Batch Gradient Norm after: 22.360679538644224
Epoch 4878/10000, Prediction Accuracy = 58.92%, Loss = 0.8410900235176086
Epoch: 4878, Batch Gradient Norm: 29.077484086511873
Epoch: 4878, Batch Gradient Norm after: 22.360679055081288
Epoch 4879/10000, Prediction Accuracy = 58.896%, Loss = 0.8384994149208069
Epoch: 4879, Batch Gradient Norm: 30.07096433055745
Epoch: 4879, Batch Gradient Norm after: 22.36067726922031
Epoch 4880/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.8409379124641418
Epoch: 4880, Batch Gradient Norm: 29.072619840647167
Epoch: 4880, Batch Gradient Norm after: 22.36068022779604
Epoch 4881/10000, Prediction Accuracy = 58.903999999999996%, Loss = 0.8383573532104492
Epoch: 4881, Batch Gradient Norm: 30.06374697673963
Epoch: 4881, Batch Gradient Norm after: 22.360678442254866
Epoch 4882/10000, Prediction Accuracy = 58.914%, Loss = 0.8407928228378296
Epoch: 4882, Batch Gradient Norm: 29.06381238747032
Epoch: 4882, Batch Gradient Norm after: 22.360679432780994
Epoch 4883/10000, Prediction Accuracy = 58.92%, Loss = 0.838227379322052
Epoch: 4883, Batch Gradient Norm: 30.058755400465895
Epoch: 4883, Batch Gradient Norm after: 22.360678116643676
Epoch 4884/10000, Prediction Accuracy = 58.928%, Loss = 0.8406657814979553
Epoch: 4884, Batch Gradient Norm: 29.05632821992904
Epoch: 4884, Batch Gradient Norm after: 22.360677197642655
Epoch 4885/10000, Prediction Accuracy = 58.926%, Loss = 0.8380992650985718
Epoch: 4885, Batch Gradient Norm: 30.051449048009946
Epoch: 4885, Batch Gradient Norm after: 22.360679955758282
Epoch 4886/10000, Prediction Accuracy = 58.934000000000005%, Loss = 0.8405206441879273
Epoch: 4886, Batch Gradient Norm: 29.0492693058409
Epoch: 4886, Batch Gradient Norm after: 22.360677509045043
Epoch 4887/10000, Prediction Accuracy = 58.908%, Loss = 0.837940788269043
Epoch: 4887, Batch Gradient Norm: 30.049562903079163
Epoch: 4887, Batch Gradient Norm after: 22.360678285868264
Epoch 4888/10000, Prediction Accuracy = 58.918000000000006%, Loss = 0.840385890007019
Epoch: 4888, Batch Gradient Norm: 29.04042960322502
Epoch: 4888, Batch Gradient Norm after: 22.360678670323328
Epoch 4889/10000, Prediction Accuracy = 58.912%, Loss = 0.8377927303314209
Epoch: 4889, Batch Gradient Norm: 30.04498246500107
Epoch: 4889, Batch Gradient Norm after: 22.360678588648202
Epoch 4890/10000, Prediction Accuracy = 58.92%, Loss = 0.8402537822723388
Epoch: 4890, Batch Gradient Norm: 29.03668605700992
Epoch: 4890, Batch Gradient Norm after: 22.360678545353473
Epoch 4891/10000, Prediction Accuracy = 58.92%, Loss = 0.8376488566398621
Epoch: 4891, Batch Gradient Norm: 30.040131874895554
Epoch: 4891, Batch Gradient Norm after: 22.360679735359223
Epoch 4892/10000, Prediction Accuracy = 58.93599999999999%, Loss = 0.8401211142539978
Epoch: 4892, Batch Gradient Norm: 29.025711471574812
Epoch: 4892, Batch Gradient Norm after: 22.360678353105932
Epoch 4893/10000, Prediction Accuracy = 58.916%, Loss = 0.8375175952911377
Epoch: 4893, Batch Gradient Norm: 30.03545450691453
Epoch: 4893, Batch Gradient Norm after: 22.360679202190564
Epoch 4894/10000, Prediction Accuracy = 58.93000000000001%, Loss = 0.8399949789047241
Epoch: 4894, Batch Gradient Norm: 29.018715426393232
Epoch: 4894, Batch Gradient Norm after: 22.360678711639302
Epoch 4895/10000, Prediction Accuracy = 58.922000000000004%, Loss = 0.8373739361763001
Epoch: 4895, Batch Gradient Norm: 30.028202968175027
Epoch: 4895, Batch Gradient Norm after: 22.36067843944191
Epoch 4896/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8398480772972107
Epoch: 4896, Batch Gradient Norm: 29.010669014519255
Epoch: 4896, Batch Gradient Norm after: 22.36067893685073
Epoch 4897/10000, Prediction Accuracy = 58.924%, Loss = 0.8372325539588928
Epoch: 4897, Batch Gradient Norm: 30.023301478570467
Epoch: 4897, Batch Gradient Norm after: 22.360679473514974
Epoch 4898/10000, Prediction Accuracy = 58.936%, Loss = 0.8397051334381104
Epoch: 4898, Batch Gradient Norm: 29.004953105553042
Epoch: 4898, Batch Gradient Norm after: 22.360678849038965
Epoch 4899/10000, Prediction Accuracy = 58.92%, Loss = 0.8370895504951477
Epoch: 4899, Batch Gradient Norm: 30.017547060686283
Epoch: 4899, Batch Gradient Norm after: 22.36067819001116
Epoch 4900/10000, Prediction Accuracy = 58.932%, Loss = 0.839567232131958
Epoch: 4900, Batch Gradient Norm: 28.99634866893915
Epoch: 4900, Batch Gradient Norm after: 22.360678117234674
Epoch 4901/10000, Prediction Accuracy = 58.926%, Loss = 0.8369515180587769
Epoch: 4901, Batch Gradient Norm: 30.016035600191397
Epoch: 4901, Batch Gradient Norm after: 22.360677399637698
Epoch 4902/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8394460201263427
Epoch: 4902, Batch Gradient Norm: 28.990079935696834
Epoch: 4902, Batch Gradient Norm after: 22.360678968081736
Epoch 4903/10000, Prediction Accuracy = 58.918000000000006%, Loss = 0.8368142485618592
Epoch: 4903, Batch Gradient Norm: 30.01160187853151
Epoch: 4903, Batch Gradient Norm after: 22.36067592762862
Epoch 4904/10000, Prediction Accuracy = 58.94200000000001%, Loss = 0.8393131971359253
Epoch: 4904, Batch Gradient Norm: 28.981795386370113
Epoch: 4904, Batch Gradient Norm after: 22.360678485648926
Epoch 4905/10000, Prediction Accuracy = 58.922000000000004%, Loss = 0.8366692781448364
Epoch: 4905, Batch Gradient Norm: 30.006242820336382
Epoch: 4905, Batch Gradient Norm after: 22.360678259365383
Epoch 4906/10000, Prediction Accuracy = 58.926%, Loss = 0.8391763210296631
Epoch: 4906, Batch Gradient Norm: 28.97608639451945
Epoch: 4906, Batch Gradient Norm after: 22.360678289717182
Epoch 4907/10000, Prediction Accuracy = 58.938%, Loss = 0.8365249395370483
Epoch: 4907, Batch Gradient Norm: 29.999154150756468
Epoch: 4907, Batch Gradient Norm after: 22.360678972389636
Epoch 4908/10000, Prediction Accuracy = 58.94000000000001%, Loss = 0.8390362501144409
Epoch: 4908, Batch Gradient Norm: 28.969237517306443
Epoch: 4908, Batch Gradient Norm after: 22.360680768694394
Epoch 4909/10000, Prediction Accuracy = 58.94200000000001%, Loss = 0.836382019519806
Epoch: 4909, Batch Gradient Norm: 29.994524641835472
Epoch: 4909, Batch Gradient Norm after: 22.3606758154521
Epoch 4910/10000, Prediction Accuracy = 58.94%, Loss = 0.8388970732688904
Epoch: 4910, Batch Gradient Norm: 28.963652519922547
Epoch: 4910, Batch Gradient Norm after: 22.360677114407082
Epoch 4911/10000, Prediction Accuracy = 58.94%, Loss = 0.8362437963485718
Epoch: 4911, Batch Gradient Norm: 29.986214926539663
Epoch: 4911, Batch Gradient Norm after: 22.360677720713397
Epoch 4912/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8387498259544373
Epoch: 4912, Batch Gradient Norm: 28.956889786654074
Epoch: 4912, Batch Gradient Norm after: 22.360677127587614
Epoch 4913/10000, Prediction Accuracy = 58.95%, Loss = 0.836106812953949
Epoch: 4913, Batch Gradient Norm: 29.9799618875227
Epoch: 4913, Batch Gradient Norm after: 22.360677413495306
Epoch 4914/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8386066555976868
Epoch: 4914, Batch Gradient Norm: 28.952121479508612
Epoch: 4914, Batch Gradient Norm after: 22.360678995644008
Epoch 4915/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.8359649658203125
Epoch: 4915, Batch Gradient Norm: 29.97006946478799
Epoch: 4915, Batch Gradient Norm after: 22.360679266146768
Epoch 4916/10000, Prediction Accuracy = 58.946000000000005%, Loss = 0.8384616613388062
Epoch: 4916, Batch Gradient Norm: 28.94544451660808
Epoch: 4916, Batch Gradient Norm after: 22.360680406481038
Epoch 4917/10000, Prediction Accuracy = 58.952%, Loss = 0.8358274459838867
Epoch: 4917, Batch Gradient Norm: 29.961976580099215
Epoch: 4917, Batch Gradient Norm after: 22.36067942671018
Epoch 4918/10000, Prediction Accuracy = 58.94200000000001%, Loss = 0.8383165240287781
Epoch: 4918, Batch Gradient Norm: 28.937977013131807
Epoch: 4918, Batch Gradient Norm after: 22.360680411427833
Epoch 4919/10000, Prediction Accuracy = 58.952%, Loss = 0.8356936812400818
Epoch: 4919, Batch Gradient Norm: 29.95439612888314
Epoch: 4919, Batch Gradient Norm after: 22.360677061866486
Epoch 4920/10000, Prediction Accuracy = 58.95399999999999%, Loss = 0.838176941871643
Epoch: 4920, Batch Gradient Norm: 28.93358972823376
Epoch: 4920, Batch Gradient Norm after: 22.360677534294684
Epoch 4921/10000, Prediction Accuracy = 58.952%, Loss = 0.8355639338493347
Epoch: 4921, Batch Gradient Norm: 29.946223895089304
Epoch: 4921, Batch Gradient Norm after: 22.360677270646374
Epoch 4922/10000, Prediction Accuracy = 58.967999999999996%, Loss = 0.8380314350128174
Epoch: 4922, Batch Gradient Norm: 28.92703061702374
Epoch: 4922, Batch Gradient Norm after: 22.360679514937182
Epoch 4923/10000, Prediction Accuracy = 58.95799999999999%, Loss = 0.8354311227798462
Epoch: 4923, Batch Gradient Norm: 29.93950922727254
Epoch: 4923, Batch Gradient Norm after: 22.36067815626705
Epoch 4924/10000, Prediction Accuracy = 58.952%, Loss = 0.8378833055496215
Epoch: 4924, Batch Gradient Norm: 28.920849835210085
Epoch: 4924, Batch Gradient Norm after: 22.36067849892797
Epoch 4925/10000, Prediction Accuracy = 58.972%, Loss = 0.8352832555770874
Epoch: 4925, Batch Gradient Norm: 29.931218633943352
Epoch: 4925, Batch Gradient Norm after: 22.360678053338912
Epoch 4926/10000, Prediction Accuracy = 58.952%, Loss = 0.8377377986907959
Epoch: 4926, Batch Gradient Norm: 28.915361822678797
Epoch: 4926, Batch Gradient Norm after: 22.360679167612997
Epoch 4927/10000, Prediction Accuracy = 58.976%, Loss = 0.835140836238861
Epoch: 4927, Batch Gradient Norm: 29.923273685510267
Epoch: 4927, Batch Gradient Norm after: 22.36067892139287
Epoch 4928/10000, Prediction Accuracy = 58.952%, Loss = 0.8375962495803833
Epoch: 4928, Batch Gradient Norm: 28.90949892415561
Epoch: 4928, Batch Gradient Norm after: 22.36068149214648
Epoch 4929/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8350130081176758
Epoch: 4929, Batch Gradient Norm: 29.91856994180625
Epoch: 4929, Batch Gradient Norm after: 22.36067922364036
Epoch 4930/10000, Prediction Accuracy = 58.971999999999994%, Loss = 0.8374701499938965
Epoch: 4930, Batch Gradient Norm: 28.901068364166026
Epoch: 4930, Batch Gradient Norm after: 22.360679101687197
Epoch 4931/10000, Prediction Accuracy = 58.958000000000006%, Loss = 0.8348889350891113
Epoch: 4931, Batch Gradient Norm: 29.909853247666554
Epoch: 4931, Batch Gradient Norm after: 22.360678307821242
Epoch 4932/10000, Prediction Accuracy = 58.964%, Loss = 0.8373257637023925
Epoch: 4932, Batch Gradient Norm: 28.896034294433225
Epoch: 4932, Batch Gradient Norm after: 22.36068078914006
Epoch 4933/10000, Prediction Accuracy = 58.976%, Loss = 0.8347380995750427
Epoch: 4933, Batch Gradient Norm: 29.90393934832583
Epoch: 4933, Batch Gradient Norm after: 22.360678420478973
Epoch 4934/10000, Prediction Accuracy = 58.962%, Loss = 0.8371792793273926
Epoch: 4934, Batch Gradient Norm: 28.890688970614246
Epoch: 4934, Batch Gradient Norm after: 22.360678299925652
Epoch 4935/10000, Prediction Accuracy = 59.00999999999999%, Loss = 0.8345860600471496
Epoch: 4935, Batch Gradient Norm: 29.894832055604937
Epoch: 4935, Batch Gradient Norm after: 22.360678700561593
Epoch 4936/10000, Prediction Accuracy = 58.965999999999994%, Loss = 0.8370347857475281
Epoch: 4936, Batch Gradient Norm: 28.886789552341142
Epoch: 4936, Batch Gradient Norm after: 22.36067689287883
Epoch 4937/10000, Prediction Accuracy = 59.016%, Loss = 0.8344476580619812
Epoch: 4937, Batch Gradient Norm: 29.889269244059168
Epoch: 4937, Batch Gradient Norm after: 22.36067985782538
Epoch 4938/10000, Prediction Accuracy = 58.962%, Loss = 0.8368988871574402
Epoch: 4938, Batch Gradient Norm: 28.878085730732003
Epoch: 4938, Batch Gradient Norm after: 22.36067804306458
Epoch 4939/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8343295454978943
Epoch: 4939, Batch Gradient Norm: 29.883691147621793
Epoch: 4939, Batch Gradient Norm after: 22.360678816460403
Epoch 4940/10000, Prediction Accuracy = 58.992%, Loss = 0.836783480644226
Epoch: 4940, Batch Gradient Norm: 28.873728031229025
Epoch: 4940, Batch Gradient Norm after: 22.360679060021138
Epoch 4941/10000, Prediction Accuracy = 58.989999999999995%, Loss = 0.8342089653015137
Epoch: 4941, Batch Gradient Norm: 29.876677822735996
Epoch: 4941, Batch Gradient Norm after: 22.360678264714622
Epoch 4942/10000, Prediction Accuracy = 58.992%, Loss = 0.8366345882415771
Epoch: 4942, Batch Gradient Norm: 28.867343542745708
Epoch: 4942, Batch Gradient Norm after: 22.360679915370966
Epoch 4943/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.8340424180030823
Epoch: 4943, Batch Gradient Norm: 29.8707397902281
Epoch: 4943, Batch Gradient Norm after: 22.360680687745155
Epoch 4944/10000, Prediction Accuracy = 58.965999999999994%, Loss = 0.8364830017089844
Epoch: 4944, Batch Gradient Norm: 28.861942172180186
Epoch: 4944, Batch Gradient Norm after: 22.360679859316626
Epoch 4945/10000, Prediction Accuracy = 59.028%, Loss = 0.8338939189910889
Epoch: 4945, Batch Gradient Norm: 29.864100649628625
Epoch: 4945, Batch Gradient Norm after: 22.36067984150697
Epoch 4946/10000, Prediction Accuracy = 58.989999999999995%, Loss = 0.8363451361656189
Epoch: 4946, Batch Gradient Norm: 28.857739799664728
Epoch: 4946, Batch Gradient Norm after: 22.360678351915382
Epoch 4947/10000, Prediction Accuracy = 59.02%, Loss = 0.8337557315826416
Epoch: 4947, Batch Gradient Norm: 29.861061387413372
Epoch: 4947, Batch Gradient Norm after: 22.360677473086394
Epoch 4948/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8362165331840515
Epoch: 4948, Batch Gradient Norm: 28.851616530863943
Epoch: 4948, Batch Gradient Norm after: 22.360677590635152
Epoch 4949/10000, Prediction Accuracy = 59.010000000000005%, Loss = 0.8336426973342895
Epoch: 4949, Batch Gradient Norm: 29.855373786423318
Epoch: 4949, Batch Gradient Norm after: 22.360682484511873
Epoch 4950/10000, Prediction Accuracy = 59.012%, Loss = 0.8360971927642822
Epoch: 4950, Batch Gradient Norm: 28.842985975057967
Epoch: 4950, Batch Gradient Norm after: 22.360675999414283
Epoch 4951/10000, Prediction Accuracy = 59.001999999999995%, Loss = 0.8335094690322876
Epoch: 4951, Batch Gradient Norm: 29.84657909160837
Epoch: 4951, Batch Gradient Norm after: 22.360676359060395
Epoch 4952/10000, Prediction Accuracy = 58.976%, Loss = 0.8359411239624024
Epoch: 4952, Batch Gradient Norm: 28.83866363578042
Epoch: 4952, Batch Gradient Norm after: 22.360678733919485
Epoch 4953/10000, Prediction Accuracy = 59.029999999999994%, Loss = 0.833346426486969
Epoch: 4953, Batch Gradient Norm: 29.841693549544512
Epoch: 4953, Batch Gradient Norm after: 22.360678371396684
Epoch 4954/10000, Prediction Accuracy = 58.96%, Loss = 0.8358063220977783
Epoch: 4954, Batch Gradient Norm: 28.831815663229214
Epoch: 4954, Batch Gradient Norm after: 22.360679665755075
Epoch 4955/10000, Prediction Accuracy = 59.03399999999999%, Loss = 0.8331985712051392
Epoch: 4955, Batch Gradient Norm: 29.841801096765693
Epoch: 4955, Batch Gradient Norm after: 22.360677518353267
Epoch 4956/10000, Prediction Accuracy = 58.972%, Loss = 0.8356777310371399
Epoch: 4956, Batch Gradient Norm: 28.82253993377177
Epoch: 4956, Batch Gradient Norm after: 22.360680715246364
Epoch 4957/10000, Prediction Accuracy = 59.038%, Loss = 0.833069670200348
Epoch: 4957, Batch Gradient Norm: 29.832013762999694
Epoch: 4957, Batch Gradient Norm after: 22.360679406288803
Epoch 4958/10000, Prediction Accuracy = 58.986000000000004%, Loss = 0.8355367302894592
Epoch: 4958, Batch Gradient Norm: 28.81836147001264
Epoch: 4958, Batch Gradient Norm after: 22.36067895342461
Epoch 4959/10000, Prediction Accuracy = 59.024%, Loss = 0.8329473614692688
Epoch: 4959, Batch Gradient Norm: 29.82476228197738
Epoch: 4959, Batch Gradient Norm after: 22.360677592743926
Epoch 4960/10000, Prediction Accuracy = 58.992%, Loss = 0.8354000329971314
Epoch: 4960, Batch Gradient Norm: 28.81110001202376
Epoch: 4960, Batch Gradient Norm after: 22.360678601363585
Epoch 4961/10000, Prediction Accuracy = 59.038%, Loss = 0.8328067898750305
Epoch: 4961, Batch Gradient Norm: 29.82124558322982
Epoch: 4961, Batch Gradient Norm after: 22.360678045090904
Epoch 4962/10000, Prediction Accuracy = 58.977999999999994%, Loss = 0.8352688193321228
Epoch: 4962, Batch Gradient Norm: 28.803058569268686
Epoch: 4962, Batch Gradient Norm after: 22.36067914653815
Epoch 4963/10000, Prediction Accuracy = 59.05%, Loss = 0.8326539874076844
Epoch: 4963, Batch Gradient Norm: 29.814313099835097
Epoch: 4963, Batch Gradient Norm after: 22.360678363119867
Epoch 4964/10000, Prediction Accuracy = 58.983999999999995%, Loss = 0.8351291537284851
Epoch: 4964, Batch Gradient Norm: 28.796509950241177
Epoch: 4964, Batch Gradient Norm after: 22.36068187352355
Epoch 4965/10000, Prediction Accuracy = 59.062%, Loss = 0.8325229763984681
Epoch: 4965, Batch Gradient Norm: 29.813644655015352
Epoch: 4965, Batch Gradient Norm after: 22.36067914806475
Epoch 4966/10000, Prediction Accuracy = 58.976%, Loss = 0.8349969983100891
Epoch: 4966, Batch Gradient Norm: 28.792027115778737
Epoch: 4966, Batch Gradient Norm after: 22.360680435856622
Epoch 4967/10000, Prediction Accuracy = 59.048%, Loss = 0.8323861122131347
Epoch: 4967, Batch Gradient Norm: 29.804319681268137
Epoch: 4967, Batch Gradient Norm after: 22.360679240813322
Epoch 4968/10000, Prediction Accuracy = 59.013999999999996%, Loss = 0.8348706364631653
Epoch: 4968, Batch Gradient Norm: 28.788046997304814
Epoch: 4968, Batch Gradient Norm after: 22.36068042750336
Epoch 4969/10000, Prediction Accuracy = 59.024%, Loss = 0.8322605013847351
Epoch: 4969, Batch Gradient Norm: 29.797361319536734
Epoch: 4969, Batch Gradient Norm after: 22.360679552590494
Epoch 4970/10000, Prediction Accuracy = 59.01399999999999%, Loss = 0.8347287774085999
Epoch: 4970, Batch Gradient Norm: 28.779285390296067
Epoch: 4970, Batch Gradient Norm after: 22.360681294559367
Epoch 4971/10000, Prediction Accuracy = 59.048%, Loss = 0.8321205496788024
Epoch: 4971, Batch Gradient Norm: 29.792767556950828
Epoch: 4971, Batch Gradient Norm after: 22.360679872568983
Epoch 4972/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.8345846176147461
Epoch: 4972, Batch Gradient Norm: 28.7730352077171
Epoch: 4972, Batch Gradient Norm after: 22.36068107601893
Epoch 4973/10000, Prediction Accuracy = 59.062%, Loss = 0.8319729924201965
Epoch: 4973, Batch Gradient Norm: 29.783287454818687
Epoch: 4973, Batch Gradient Norm after: 22.360677275295288
Epoch 4974/10000, Prediction Accuracy = 58.988%, Loss = 0.8344465255737304
Epoch: 4974, Batch Gradient Norm: 28.768042458036778
Epoch: 4974, Batch Gradient Norm after: 22.36067880826265
Epoch 4975/10000, Prediction Accuracy = 59.068%, Loss = 0.8318379521369934
Epoch: 4975, Batch Gradient Norm: 29.779794795251895
Epoch: 4975, Batch Gradient Norm after: 22.36067755211247
Epoch 4976/10000, Prediction Accuracy = 59.004%, Loss = 0.8343128681182861
Epoch: 4976, Batch Gradient Norm: 28.763242049316283
Epoch: 4976, Batch Gradient Norm after: 22.36067798069118
Epoch 4977/10000, Prediction Accuracy = 59.064%, Loss = 0.8317113041877746
Epoch: 4977, Batch Gradient Norm: 29.771864591388486
Epoch: 4977, Batch Gradient Norm after: 22.360679733421094
Epoch 4978/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8341854572296142
Epoch: 4978, Batch Gradient Norm: 28.75481980980549
Epoch: 4978, Batch Gradient Norm after: 22.360679513083376
Epoch 4979/10000, Prediction Accuracy = 59.05%, Loss = 0.8315931558609009
Epoch: 4979, Batch Gradient Norm: 29.766844038784743
Epoch: 4979, Batch Gradient Norm after: 22.360677934065727
Epoch 4980/10000, Prediction Accuracy = 59.017999999999994%, Loss = 0.8340474724769592
Epoch: 4980, Batch Gradient Norm: 28.74813541081981
Epoch: 4980, Batch Gradient Norm after: 22.36067929174632
Epoch 4981/10000, Prediction Accuracy = 59.06%, Loss = 0.8314366936683655
Epoch: 4981, Batch Gradient Norm: 29.762192643190627
Epoch: 4981, Batch Gradient Norm after: 22.360679956883338
Epoch 4982/10000, Prediction Accuracy = 59.00599999999999%, Loss = 0.8339052796363831
Epoch: 4982, Batch Gradient Norm: 28.74197662719628
Epoch: 4982, Batch Gradient Norm after: 22.36067904195648
Epoch 4983/10000, Prediction Accuracy = 59.072%, Loss = 0.8312937498092652
Epoch: 4983, Batch Gradient Norm: 29.759348530617704
Epoch: 4983, Batch Gradient Norm after: 22.360677768287616
Epoch 4984/10000, Prediction Accuracy = 59.013999999999996%, Loss = 0.8337776064872742
Epoch: 4984, Batch Gradient Norm: 28.73482039454819
Epoch: 4984, Batch Gradient Norm after: 22.360677377857517
Epoch 4985/10000, Prediction Accuracy = 59.072%, Loss = 0.8311612606048584
Epoch: 4985, Batch Gradient Norm: 29.75508210439356
Epoch: 4985, Batch Gradient Norm after: 22.3606777120459
Epoch 4986/10000, Prediction Accuracy = 59.016%, Loss = 0.8336445450782776
Epoch: 4986, Batch Gradient Norm: 28.730473115367335
Epoch: 4986, Batch Gradient Norm after: 22.360679605646848
Epoch 4987/10000, Prediction Accuracy = 59.06600000000001%, Loss = 0.8310408592224121
Epoch: 4987, Batch Gradient Norm: 29.747733808618644
Epoch: 4987, Batch Gradient Norm after: 22.360677119275373
Epoch 4988/10000, Prediction Accuracy = 59.04%, Loss = 0.8335220813751221
Epoch: 4988, Batch Gradient Norm: 28.723533976395977
Epoch: 4988, Batch Gradient Norm after: 22.360678950328545
Epoch 4989/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8309160709381104
Epoch: 4989, Batch Gradient Norm: 29.736883602866545
Epoch: 4989, Batch Gradient Norm after: 22.360679065064012
Epoch 4990/10000, Prediction Accuracy = 59.02%, Loss = 0.8333752751350403
Epoch: 4990, Batch Gradient Norm: 28.716003065634375
Epoch: 4990, Batch Gradient Norm after: 22.360680165403867
Epoch 4991/10000, Prediction Accuracy = 59.076%, Loss = 0.8307652235031128
Epoch: 4991, Batch Gradient Norm: 29.73366375586012
Epoch: 4991, Batch Gradient Norm after: 22.360676347684834
Epoch 4992/10000, Prediction Accuracy = 59.010000000000005%, Loss = 0.8332420110702514
Epoch: 4992, Batch Gradient Norm: 28.707454667386926
Epoch: 4992, Batch Gradient Norm after: 22.36068216360553
Epoch 4993/10000, Prediction Accuracy = 59.074%, Loss = 0.8306199789047242
Epoch: 4993, Batch Gradient Norm: 29.732987500609994
Epoch: 4993, Batch Gradient Norm after: 22.360679040277503
Epoch 4994/10000, Prediction Accuracy = 59.025999999999996%, Loss = 0.8331176161766052
Epoch: 4994, Batch Gradient Norm: 28.700557793311837
Epoch: 4994, Batch Gradient Norm after: 22.36068212959575
Epoch 4995/10000, Prediction Accuracy = 59.093999999999994%, Loss = 0.8304856538772583
Epoch: 4995, Batch Gradient Norm: 29.73143417141968
Epoch: 4995, Batch Gradient Norm after: 22.360678892204184
Epoch 4996/10000, Prediction Accuracy = 59.013999999999996%, Loss = 0.8329999685287476
Epoch: 4996, Batch Gradient Norm: 28.693656281727193
Epoch: 4996, Batch Gradient Norm after: 22.36067918785188
Epoch 4997/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.8303665041923523
Epoch: 4997, Batch Gradient Norm: 29.72569696594057
Epoch: 4997, Batch Gradient Norm after: 22.360678913315706
Epoch 4998/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8328789949417115
Epoch: 4998, Batch Gradient Norm: 28.68382299341546
Epoch: 4998, Batch Gradient Norm after: 22.360679459252776
Epoch 4999/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8302368998527527
Epoch: 4999, Batch Gradient Norm: 29.722443057745572
Epoch: 4999, Batch Gradient Norm after: 22.360679955061233
Epoch 5000/10000, Prediction Accuracy = 59.01800000000001%, Loss = 0.832743763923645
Epoch: 5000, Batch Gradient Norm: 28.67779183444467
Epoch: 5000, Batch Gradient Norm after: 22.360677702648022
Epoch 5001/10000, Prediction Accuracy = 59.09400000000001%, Loss = 0.8300809144973755
Epoch: 5001, Batch Gradient Norm: 29.715705627637092
Epoch: 5001, Batch Gradient Norm after: 22.360679182373232
Epoch 5002/10000, Prediction Accuracy = 59.028%, Loss = 0.8326167225837707
Epoch: 5002, Batch Gradient Norm: 28.67119517773727
Epoch: 5002, Batch Gradient Norm after: 22.360678621467187
Epoch 5003/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8299362182617187
Epoch: 5003, Batch Gradient Norm: 29.711743658631228
Epoch: 5003, Batch Gradient Norm after: 22.360678483964982
Epoch 5004/10000, Prediction Accuracy = 59.036%, Loss = 0.832494056224823
Epoch: 5004, Batch Gradient Norm: 28.66341001260352
Epoch: 5004, Batch Gradient Norm after: 22.360678197297005
Epoch 5005/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.8297984838485718
Epoch: 5005, Batch Gradient Norm: 29.711438393486826
Epoch: 5005, Batch Gradient Norm after: 22.360677425719995
Epoch 5006/10000, Prediction Accuracy = 59.019999999999996%, Loss = 0.8323757529258728
Epoch: 5006, Batch Gradient Norm: 28.655534329756843
Epoch: 5006, Batch Gradient Norm after: 22.360678694882612
Epoch 5007/10000, Prediction Accuracy = 59.09400000000001%, Loss = 0.829689335823059
Epoch: 5007, Batch Gradient Norm: 29.707544941752893
Epoch: 5007, Batch Gradient Norm after: 22.360678340402007
Epoch 5008/10000, Prediction Accuracy = 59.062%, Loss = 0.8322545409202575
Epoch: 5008, Batch Gradient Norm: 28.65076978010313
Epoch: 5008, Batch Gradient Norm after: 22.360680471301812
Epoch 5009/10000, Prediction Accuracy = 59.08200000000001%, Loss = 0.8295445680618286
Epoch: 5009, Batch Gradient Norm: 29.695892800272038
Epoch: 5009, Batch Gradient Norm after: 22.360679856608087
Epoch 5010/10000, Prediction Accuracy = 59.017999999999994%, Loss = 0.83209707736969
Epoch: 5010, Batch Gradient Norm: 28.643658093766902
Epoch: 5010, Batch Gradient Norm after: 22.360678795014376
Epoch 5011/10000, Prediction Accuracy = 59.072%, Loss = 0.8293998956680297
Epoch: 5011, Batch Gradient Norm: 29.696348154359498
Epoch: 5011, Batch Gradient Norm after: 22.360679584219977
Epoch 5012/10000, Prediction Accuracy = 59.036%, Loss = 0.8319743752479554
Epoch: 5012, Batch Gradient Norm: 28.63459180036281
Epoch: 5012, Batch Gradient Norm after: 22.360679331618456
Epoch 5013/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.8292536258697509
Epoch: 5013, Batch Gradient Norm: 29.696593857297916
Epoch: 5013, Batch Gradient Norm after: 22.360679728353364
Epoch 5014/10000, Prediction Accuracy = 59.05800000000001%, Loss = 0.8318593740463257
Epoch: 5014, Batch Gradient Norm: 28.62680437347714
Epoch: 5014, Batch Gradient Norm after: 22.36067883075581
Epoch 5015/10000, Prediction Accuracy = 59.098%, Loss = 0.8291216492652893
Epoch: 5015, Batch Gradient Norm: 29.6921640464603
Epoch: 5015, Batch Gradient Norm after: 22.360679699777652
Epoch 5016/10000, Prediction Accuracy = 59.052%, Loss = 0.831741988658905
Epoch: 5016, Batch Gradient Norm: 28.620075491793862
Epoch: 5016, Batch Gradient Norm after: 22.360680421991894
Epoch 5017/10000, Prediction Accuracy = 59.08%, Loss = 0.8290072083473206
Epoch: 5017, Batch Gradient Norm: 29.68787622220922
Epoch: 5017, Batch Gradient Norm after: 22.360680460784643
Epoch 5018/10000, Prediction Accuracy = 59.06400000000001%, Loss = 0.831617021560669
Epoch: 5018, Batch Gradient Norm: 28.61101671826121
Epoch: 5018, Batch Gradient Norm after: 22.360681114489118
Epoch 5019/10000, Prediction Accuracy = 59.088%, Loss = 0.8288706421852112
Epoch: 5019, Batch Gradient Norm: 29.681673322493115
Epoch: 5019, Batch Gradient Norm after: 22.36067829272265
Epoch 5020/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.831471049785614
Epoch: 5020, Batch Gradient Norm: 28.606293307805604
Epoch: 5020, Batch Gradient Norm after: 22.36067834922306
Epoch 5021/10000, Prediction Accuracy = 59.088%, Loss = 0.8287183403968811
Epoch: 5021, Batch Gradient Norm: 29.67452906903123
Epoch: 5021, Batch Gradient Norm after: 22.360679104451314
Epoch 5022/10000, Prediction Accuracy = 59.04600000000001%, Loss = 0.8313370943069458
Epoch: 5022, Batch Gradient Norm: 28.601257661600776
Epoch: 5022, Batch Gradient Norm after: 22.36067746235771
Epoch 5023/10000, Prediction Accuracy = 59.084%, Loss = 0.8285848736763001
Epoch: 5023, Batch Gradient Norm: 29.67333220224142
Epoch: 5023, Batch Gradient Norm after: 22.36067969819613
Epoch 5024/10000, Prediction Accuracy = 59.05%, Loss = 0.831217086315155
Epoch: 5024, Batch Gradient Norm: 28.59255943759147
Epoch: 5024, Batch Gradient Norm after: 22.360681026156744
Epoch 5025/10000, Prediction Accuracy = 59.086%, Loss = 0.8284590601921081
Epoch: 5025, Batch Gradient Norm: 29.67360580870756
Epoch: 5025, Batch Gradient Norm after: 22.360680587870153
Epoch 5026/10000, Prediction Accuracy = 59.08%, Loss = 0.831111752986908
Epoch: 5026, Batch Gradient Norm: 28.583489894756813
Epoch: 5026, Batch Gradient Norm after: 22.36067899919177
Epoch 5027/10000, Prediction Accuracy = 59.077999999999996%, Loss = 0.8283391475677491
Epoch: 5027, Batch Gradient Norm: 29.666638176666705
Epoch: 5027, Batch Gradient Norm after: 22.360678957261555
Epoch 5028/10000, Prediction Accuracy = 59.072%, Loss = 0.8309825181961059
Epoch: 5028, Batch Gradient Norm: 28.578210995965314
Epoch: 5028, Batch Gradient Norm after: 22.360678575144693
Epoch 5029/10000, Prediction Accuracy = 59.086%, Loss = 0.8281876921653748
Epoch: 5029, Batch Gradient Norm: 29.662028096263548
Epoch: 5029, Batch Gradient Norm after: 22.36067840793767
Epoch 5030/10000, Prediction Accuracy = 59.044%, Loss = 0.8308383107185364
Epoch: 5030, Batch Gradient Norm: 28.57074921665043
Epoch: 5030, Batch Gradient Norm after: 22.360678320128315
Epoch 5031/10000, Prediction Accuracy = 59.072%, Loss = 0.8280426979064941
Epoch: 5031, Batch Gradient Norm: 29.65687155185732
Epoch: 5031, Batch Gradient Norm after: 22.36067871925971
Epoch 5032/10000, Prediction Accuracy = 59.062%, Loss = 0.8307050704956055
Epoch: 5032, Batch Gradient Norm: 28.564425092236718
Epoch: 5032, Batch Gradient Norm after: 22.36067692355543
Epoch 5033/10000, Prediction Accuracy = 59.074%, Loss = 0.8279086828231812
Epoch: 5033, Batch Gradient Norm: 29.65112324824517
Epoch: 5033, Batch Gradient Norm after: 22.360677160375477
Epoch 5034/10000, Prediction Accuracy = 59.07000000000001%, Loss = 0.8305800795555115
Epoch: 5034, Batch Gradient Norm: 28.560174149458845
Epoch: 5034, Batch Gradient Norm after: 22.36067865925868
Epoch 5035/10000, Prediction Accuracy = 59.074%, Loss = 0.8277935266494751
Epoch: 5035, Batch Gradient Norm: 29.644519862391252
Epoch: 5035, Batch Gradient Norm after: 22.360679223199742
Epoch 5036/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8304554700851441
Epoch: 5036, Batch Gradient Norm: 28.55322196091606
Epoch: 5036, Batch Gradient Norm after: 22.3606785537467
Epoch 5037/10000, Prediction Accuracy = 59.088%, Loss = 0.8276747822761535
Epoch: 5037, Batch Gradient Norm: 29.637345131597762
Epoch: 5037, Batch Gradient Norm after: 22.360679248968633
Epoch 5038/10000, Prediction Accuracy = 59.088%, Loss = 0.8303141832351685
Epoch: 5038, Batch Gradient Norm: 28.546614790654044
Epoch: 5038, Batch Gradient Norm after: 22.360677885619424
Epoch 5039/10000, Prediction Accuracy = 59.086%, Loss = 0.8275202512741089
Epoch: 5039, Batch Gradient Norm: 29.6301473293624
Epoch: 5039, Batch Gradient Norm after: 22.36067854135016
Epoch 5040/10000, Prediction Accuracy = 59.05800000000001%, Loss = 0.8301711916923523
Epoch: 5040, Batch Gradient Norm: 28.54094305055378
Epoch: 5040, Batch Gradient Norm after: 22.36067890737382
Epoch 5041/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.8273798823356628
Epoch: 5041, Batch Gradient Norm: 29.621579677846885
Epoch: 5041, Batch Gradient Norm after: 22.360678265934624
Epoch 5042/10000, Prediction Accuracy = 59.076%, Loss = 0.8300363421440125
Epoch: 5042, Batch Gradient Norm: 28.537789756854544
Epoch: 5042, Batch Gradient Norm after: 22.360681301025366
Epoch 5043/10000, Prediction Accuracy = 59.086%, Loss = 0.8272438526153565
Epoch: 5043, Batch Gradient Norm: 29.610776827655663
Epoch: 5043, Batch Gradient Norm after: 22.360678419311487
Epoch 5044/10000, Prediction Accuracy = 59.098%, Loss = 0.8298911571502685
Epoch: 5044, Batch Gradient Norm: 28.531596112426516
Epoch: 5044, Batch Gradient Norm after: 22.36068019521293
Epoch 5045/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8271408677101135
Epoch: 5045, Batch Gradient Norm: 29.60297698301209
Epoch: 5045, Batch Gradient Norm after: 22.36068058801323
Epoch 5046/10000, Prediction Accuracy = 59.124%, Loss = 0.8297697186470032
Epoch: 5046, Batch Gradient Norm: 28.526110640917487
Epoch: 5046, Batch Gradient Norm after: 22.360676945280822
Epoch 5047/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8270095586776733
Epoch: 5047, Batch Gradient Norm: 29.59443117456637
Epoch: 5047, Batch Gradient Norm after: 22.360678833681778
Epoch 5048/10000, Prediction Accuracy = 59.11199999999999%, Loss = 0.8296164274215698
Epoch: 5048, Batch Gradient Norm: 28.517475799637257
Epoch: 5048, Batch Gradient Norm after: 22.36067897892129
Epoch 5049/10000, Prediction Accuracy = 59.08399999999999%, Loss = 0.8268576264381409
Epoch: 5049, Batch Gradient Norm: 29.58928771460198
Epoch: 5049, Batch Gradient Norm after: 22.360680428256476
Epoch 5050/10000, Prediction Accuracy = 59.06999999999999%, Loss = 0.8294794201850891
Epoch: 5050, Batch Gradient Norm: 28.513142838713243
Epoch: 5050, Batch Gradient Norm after: 22.360677855072243
Epoch 5051/10000, Prediction Accuracy = 59.092%, Loss = 0.8267205238342286
Epoch: 5051, Batch Gradient Norm: 29.57877968355442
Epoch: 5051, Batch Gradient Norm after: 22.36068014601072
Epoch 5052/10000, Prediction Accuracy = 59.074%, Loss = 0.8293397188186645
Epoch: 5052, Batch Gradient Norm: 28.508049805952787
Epoch: 5052, Batch Gradient Norm after: 22.360677838580195
Epoch 5053/10000, Prediction Accuracy = 59.1%, Loss = 0.8265940189361572
Epoch: 5053, Batch Gradient Norm: 29.57060790438097
Epoch: 5053, Batch Gradient Norm after: 22.360678500575716
Epoch 5054/10000, Prediction Accuracy = 59.11800000000001%, Loss = 0.8292016863822937
Epoch: 5054, Batch Gradient Norm: 28.504593099881742
Epoch: 5054, Batch Gradient Norm after: 22.36067781647098
Epoch 5055/10000, Prediction Accuracy = 59.1%, Loss = 0.8264890909194946
Epoch: 5055, Batch Gradient Norm: 29.558844986051604
Epoch: 5055, Batch Gradient Norm after: 22.360678762909263
Epoch 5056/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8290666580200196
Epoch: 5056, Batch Gradient Norm: 28.49894016787898
Epoch: 5056, Batch Gradient Norm after: 22.36067766009643
Epoch 5057/10000, Prediction Accuracy = 59.098%, Loss = 0.8263594031333923
Epoch: 5057, Batch Gradient Norm: 29.54628654151387
Epoch: 5057, Batch Gradient Norm after: 22.360679589459703
Epoch 5058/10000, Prediction Accuracy = 59.092%, Loss = 0.828897213935852
Epoch: 5058, Batch Gradient Norm: 28.49589510478193
Epoch: 5058, Batch Gradient Norm after: 22.360679288258805
Epoch 5059/10000, Prediction Accuracy = 59.092%, Loss = 0.8262151956558228
Epoch: 5059, Batch Gradient Norm: 29.535590529633428
Epoch: 5059, Batch Gradient Norm after: 22.360680283433076
Epoch 5060/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.8287458419799805
Epoch: 5060, Batch Gradient Norm: 28.490985748617597
Epoch: 5060, Batch Gradient Norm after: 22.360678446091843
Epoch 5061/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.8260836839675904
Epoch: 5061, Batch Gradient Norm: 29.520302666552247
Epoch: 5061, Batch Gradient Norm after: 22.36067842601613
Epoch 5062/10000, Prediction Accuracy = 59.09400000000001%, Loss = 0.8285851359367371
Epoch: 5062, Batch Gradient Norm: 28.489119838922676
Epoch: 5062, Batch Gradient Norm after: 22.360676344141336
Epoch 5063/10000, Prediction Accuracy = 59.1%, Loss = 0.8259650111198426
Epoch: 5063, Batch Gradient Norm: 29.506722464658207
Epoch: 5063, Batch Gradient Norm after: 22.3606810864063
Epoch 5064/10000, Prediction Accuracy = 59.114%, Loss = 0.82845219373703
Epoch: 5064, Batch Gradient Norm: 28.485757114797526
Epoch: 5064, Batch Gradient Norm after: 22.360677137727425
Epoch 5065/10000, Prediction Accuracy = 59.098%, Loss = 0.8258633613586426
Epoch: 5065, Batch Gradient Norm: 29.492675743818523
Epoch: 5065, Batch Gradient Norm after: 22.36068013797214
Epoch 5066/10000, Prediction Accuracy = 59.128%, Loss = 0.8283038258552551
Epoch: 5066, Batch Gradient Norm: 28.481677676505285
Epoch: 5066, Batch Gradient Norm after: 22.360677436783618
Epoch 5067/10000, Prediction Accuracy = 59.114%, Loss = 0.8257266163825989
Epoch: 5067, Batch Gradient Norm: 29.479929171822086
Epoch: 5067, Batch Gradient Norm after: 22.360680709699988
Epoch 5068/10000, Prediction Accuracy = 59.114%, Loss = 0.8281497359275818
Epoch: 5068, Batch Gradient Norm: 28.479572320463014
Epoch: 5068, Batch Gradient Norm after: 22.360679100315004
Epoch 5069/10000, Prediction Accuracy = 59.10200000000001%, Loss = 0.8255820512771607
Epoch: 5069, Batch Gradient Norm: 29.466980746191748
Epoch: 5069, Batch Gradient Norm after: 22.360679895786888
Epoch 5070/10000, Prediction Accuracy = 59.116%, Loss = 0.8279893636703491
Epoch: 5070, Batch Gradient Norm: 28.47672396278422
Epoch: 5070, Batch Gradient Norm after: 22.360676954244084
Epoch 5071/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.825453007221222
Epoch: 5071, Batch Gradient Norm: 29.456400393965755
Epoch: 5071, Batch Gradient Norm after: 22.360678722279754
Epoch 5072/10000, Prediction Accuracy = 59.11800000000001%, Loss = 0.8278474569320678
Epoch: 5072, Batch Gradient Norm: 28.47079056071535
Epoch: 5072, Batch Gradient Norm after: 22.360677271381967
Epoch 5073/10000, Prediction Accuracy = 59.102%, Loss = 0.8253361582756042
Epoch: 5073, Batch Gradient Norm: 29.444691636128947
Epoch: 5073, Batch Gradient Norm after: 22.360679637309516
Epoch 5074/10000, Prediction Accuracy = 59.138%, Loss = 0.8277165412902832
Epoch: 5074, Batch Gradient Norm: 28.46596275735076
Epoch: 5074, Batch Gradient Norm after: 22.360679087588775
Epoch 5075/10000, Prediction Accuracy = 59.116%, Loss = 0.8252293705940247
Epoch: 5075, Batch Gradient Norm: 29.433787823434717
Epoch: 5075, Batch Gradient Norm after: 22.360678532901513
Epoch 5076/10000, Prediction Accuracy = 59.132000000000005%, Loss = 0.8275788068771363
Epoch: 5076, Batch Gradient Norm: 28.459224103328708
Epoch: 5076, Batch Gradient Norm after: 22.360677399553595
Epoch 5077/10000, Prediction Accuracy = 59.11%, Loss = 0.8250930905342102
Epoch: 5077, Batch Gradient Norm: 29.42442392473405
Epoch: 5077, Batch Gradient Norm after: 22.360678916838125
Epoch 5078/10000, Prediction Accuracy = 59.132000000000005%, Loss = 0.8274276733398438
Epoch: 5078, Batch Gradient Norm: 28.45643497310957
Epoch: 5078, Batch Gradient Norm after: 22.360677657984798
Epoch 5079/10000, Prediction Accuracy = 59.1%, Loss = 0.8249475240707398
Epoch: 5079, Batch Gradient Norm: 29.415808328586536
Epoch: 5079, Batch Gradient Norm after: 22.360679549286825
Epoch 5080/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.8272841095924377
Epoch: 5080, Batch Gradient Norm: 28.449335691830733
Epoch: 5080, Batch Gradient Norm after: 22.36067807311513
Epoch 5081/10000, Prediction Accuracy = 59.104%, Loss = 0.8248159766197205
Epoch: 5081, Batch Gradient Norm: 29.406783897020052
Epoch: 5081, Batch Gradient Norm after: 22.36067740354536
Epoch 5082/10000, Prediction Accuracy = 59.13199999999999%, Loss = 0.8271467447280884
Epoch: 5082, Batch Gradient Norm: 28.443830000791078
Epoch: 5082, Batch Gradient Norm after: 22.360677736776815
Epoch 5083/10000, Prediction Accuracy = 59.129999999999995%, Loss = 0.8247051477432251
Epoch: 5083, Batch Gradient Norm: 29.398091035553644
Epoch: 5083, Batch Gradient Norm after: 22.360679640151066
Epoch 5084/10000, Prediction Accuracy = 59.14%, Loss = 0.8270180463790894
Epoch: 5084, Batch Gradient Norm: 28.43886157447291
Epoch: 5084, Batch Gradient Norm after: 22.360681625489473
Epoch 5085/10000, Prediction Accuracy = 59.112%, Loss = 0.8245974183082581
Epoch: 5085, Batch Gradient Norm: 29.386135665408187
Epoch: 5085, Batch Gradient Norm after: 22.360678214932854
Epoch 5086/10000, Prediction Accuracy = 59.14399999999999%, Loss = 0.8268854856491089
Epoch: 5086, Batch Gradient Norm: 28.432747194972922
Epoch: 5086, Batch Gradient Norm after: 22.36068041656846
Epoch 5087/10000, Prediction Accuracy = 59.11%, Loss = 0.8244590163230896
Epoch: 5087, Batch Gradient Norm: 29.375947957486915
Epoch: 5087, Batch Gradient Norm after: 22.360676839449166
Epoch 5088/10000, Prediction Accuracy = 59.146%, Loss = 0.8267266750335693
Epoch: 5088, Batch Gradient Norm: 28.429735675936563
Epoch: 5088, Batch Gradient Norm after: 22.360677633252184
Epoch 5089/10000, Prediction Accuracy = 59.102%, Loss = 0.8243159294128418
Epoch: 5089, Batch Gradient Norm: 29.363548212345776
Epoch: 5089, Batch Gradient Norm after: 22.360677637422352
Epoch 5090/10000, Prediction Accuracy = 59.134%, Loss = 0.8265801429748535
Epoch: 5090, Batch Gradient Norm: 28.425224602437005
Epoch: 5090, Batch Gradient Norm after: 22.36067587008753
Epoch 5091/10000, Prediction Accuracy = 59.108000000000004%, Loss = 0.8241833209991455
Epoch: 5091, Batch Gradient Norm: 29.355459497267187
Epoch: 5091, Batch Gradient Norm after: 22.360677729950716
Epoch 5092/10000, Prediction Accuracy = 59.14399999999999%, Loss = 0.8264387488365174
Epoch: 5092, Batch Gradient Norm: 28.420508294324605
Epoch: 5092, Batch Gradient Norm after: 22.36067694789002
Epoch 5093/10000, Prediction Accuracy = 59.128%, Loss = 0.8240700125694275
Epoch: 5093, Batch Gradient Norm: 29.34484593591866
Epoch: 5093, Batch Gradient Norm after: 22.360679235982644
Epoch 5094/10000, Prediction Accuracy = 59.14200000000001%, Loss = 0.826303493976593
Epoch: 5094, Batch Gradient Norm: 28.417771873034535
Epoch: 5094, Batch Gradient Norm after: 22.360678940446785
Epoch 5095/10000, Prediction Accuracy = 59.11800000000001%, Loss = 0.8239554643630982
Epoch: 5095, Batch Gradient Norm: 29.329476603043492
Epoch: 5095, Batch Gradient Norm after: 22.36067818782992
Epoch 5096/10000, Prediction Accuracy = 59.158%, Loss = 0.8261538147926331
Epoch: 5096, Batch Gradient Norm: 28.41466705343366
Epoch: 5096, Batch Gradient Norm after: 22.360675665612245
Epoch 5097/10000, Prediction Accuracy = 59.124%, Loss = 0.8238240718841553
Epoch: 5097, Batch Gradient Norm: 29.319926682670722
Epoch: 5097, Batch Gradient Norm after: 22.360678142338486
Epoch 5098/10000, Prediction Accuracy = 59.15599999999999%, Loss = 0.825999653339386
Epoch: 5098, Batch Gradient Norm: 28.410118482694646
Epoch: 5098, Batch Gradient Norm after: 22.360678886407747
Epoch 5099/10000, Prediction Accuracy = 59.117999999999995%, Loss = 0.8236835718154907
Epoch: 5099, Batch Gradient Norm: 29.312730453334698
Epoch: 5099, Batch Gradient Norm after: 22.36067791399649
Epoch 5100/10000, Prediction Accuracy = 59.134%, Loss = 0.825864064693451
Epoch: 5100, Batch Gradient Norm: 28.40509005156361
Epoch: 5100, Batch Gradient Norm after: 22.36067645838749
Epoch 5101/10000, Prediction Accuracy = 59.13199999999999%, Loss = 0.8235546469688415
Epoch: 5101, Batch Gradient Norm: 29.308391338928903
Epoch: 5101, Batch Gradient Norm after: 22.36067796543088
Epoch 5102/10000, Prediction Accuracy = 59.178%, Loss = 0.8257344961166382
Epoch: 5102, Batch Gradient Norm: 28.399498331855483
Epoch: 5102, Batch Gradient Norm after: 22.360677032941208
Epoch 5103/10000, Prediction Accuracy = 59.117999999999995%, Loss = 0.8234376192092896
Epoch: 5103, Batch Gradient Norm: 29.30052024073579
Epoch: 5103, Batch Gradient Norm after: 22.360677690880486
Epoch 5104/10000, Prediction Accuracy = 59.152%, Loss = 0.8256105065345765
Epoch: 5104, Batch Gradient Norm: 28.395055132417426
Epoch: 5104, Batch Gradient Norm after: 22.360679633747374
Epoch 5105/10000, Prediction Accuracy = 59.132000000000005%, Loss = 0.8233168125152588
Epoch: 5105, Batch Gradient Norm: 29.29099444970543
Epoch: 5105, Batch Gradient Norm after: 22.36067929331046
Epoch 5106/10000, Prediction Accuracy = 59.16600000000001%, Loss = 0.8254770159721374
Epoch: 5106, Batch Gradient Norm: 28.388808062860424
Epoch: 5106, Batch Gradient Norm after: 22.360678224537843
Epoch 5107/10000, Prediction Accuracy = 59.116%, Loss = 0.8231813192367554
Epoch: 5107, Batch Gradient Norm: 29.28440786612374
Epoch: 5107, Batch Gradient Norm after: 22.36067881522274
Epoch 5108/10000, Prediction Accuracy = 59.141999999999996%, Loss = 0.8253316044807434
Epoch: 5108, Batch Gradient Norm: 28.38179360085007
Epoch: 5108, Batch Gradient Norm after: 22.360678274890805
Epoch 5109/10000, Prediction Accuracy = 59.138%, Loss = 0.8230371236801147
Epoch: 5109, Batch Gradient Norm: 29.275853257088254
Epoch: 5109, Batch Gradient Norm after: 22.360676999120987
Epoch 5110/10000, Prediction Accuracy = 59.146%, Loss = 0.8252050757408143
Epoch: 5110, Batch Gradient Norm: 28.37496753533036
Epoch: 5110, Batch Gradient Norm after: 22.360678799336675
Epoch 5111/10000, Prediction Accuracy = 59.14%, Loss = 0.822905433177948
Epoch: 5111, Batch Gradient Norm: 29.271974802559633
Epoch: 5111, Batch Gradient Norm after: 22.360678092576794
Epoch 5112/10000, Prediction Accuracy = 59.164%, Loss = 0.8250880360603332
Epoch: 5112, Batch Gradient Norm: 28.370294229072506
Epoch: 5112, Batch Gradient Norm after: 22.3606774071585
Epoch 5113/10000, Prediction Accuracy = 59.156000000000006%, Loss = 0.8227980732917786
Epoch: 5113, Batch Gradient Norm: 29.2666410165359
Epoch: 5113, Batch Gradient Norm after: 22.360678516292257
Epoch 5114/10000, Prediction Accuracy = 59.17%, Loss = 0.824985659122467
Epoch: 5114, Batch Gradient Norm: 28.36135800389609
Epoch: 5114, Batch Gradient Norm after: 22.36067991717157
Epoch 5115/10000, Prediction Accuracy = 59.128%, Loss = 0.822681999206543
Epoch: 5115, Batch Gradient Norm: 29.26109670362554
Epoch: 5115, Batch Gradient Norm after: 22.36067646265895
Epoch 5116/10000, Prediction Accuracy = 59.17%, Loss = 0.8248557448387146
Epoch: 5116, Batch Gradient Norm: 28.35446233427291
Epoch: 5116, Batch Gradient Norm after: 22.360677925613544
Epoch 5117/10000, Prediction Accuracy = 59.13399999999999%, Loss = 0.8225239396095276
Epoch: 5117, Batch Gradient Norm: 29.259281168153176
Epoch: 5117, Batch Gradient Norm after: 22.360677840447252
Epoch 5118/10000, Prediction Accuracy = 59.148%, Loss = 0.8247172117233277
Epoch: 5118, Batch Gradient Norm: 28.34786765963176
Epoch: 5118, Batch Gradient Norm after: 22.36067827689554
Epoch 5119/10000, Prediction Accuracy = 59.14399999999999%, Loss = 0.822382116317749
Epoch: 5119, Batch Gradient Norm: 29.253286785908248
Epoch: 5119, Batch Gradient Norm after: 22.36067958327481
Epoch 5120/10000, Prediction Accuracy = 59.14799999999999%, Loss = 0.8245977401733399
Epoch: 5120, Batch Gradient Norm: 28.33961500119917
Epoch: 5120, Batch Gradient Norm after: 22.360678960038953
Epoch 5121/10000, Prediction Accuracy = 59.164%, Loss = 0.8222517609596253
Epoch: 5121, Batch Gradient Norm: 29.248091157067144
Epoch: 5121, Batch Gradient Norm after: 22.360678259221025
Epoch 5122/10000, Prediction Accuracy = 59.160000000000004%, Loss = 0.824480938911438
Epoch: 5122, Batch Gradient Norm: 28.334318159567566
Epoch: 5122, Batch Gradient Norm after: 22.360679818865904
Epoch 5123/10000, Prediction Accuracy = 59.13000000000001%, Loss = 0.8221471548080445
Epoch: 5123, Batch Gradient Norm: 29.2399616432066
Epoch: 5123, Batch Gradient Norm after: 22.360680408782102
Epoch 5124/10000, Prediction Accuracy = 59.176%, Loss = 0.8243646025657654
Epoch: 5124, Batch Gradient Norm: 28.32641076166713
Epoch: 5124, Batch Gradient Norm after: 22.36067896220873
Epoch 5125/10000, Prediction Accuracy = 59.148%, Loss = 0.8220343232154846
Epoch: 5125, Batch Gradient Norm: 29.229546536692702
Epoch: 5125, Batch Gradient Norm after: 22.360678602499153
Epoch 5126/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8242156028747558
Epoch: 5126, Batch Gradient Norm: 28.321097448964906
Epoch: 5126, Batch Gradient Norm after: 22.36067814538153
Epoch 5127/10000, Prediction Accuracy = 59.14399999999999%, Loss = 0.8218777298927307
Epoch: 5127, Batch Gradient Norm: 29.227605239215528
Epoch: 5127, Batch Gradient Norm after: 22.36067801389179
Epoch 5128/10000, Prediction Accuracy = 59.157999999999994%, Loss = 0.8240886569023133
Epoch: 5128, Batch Gradient Norm: 28.31346270550507
Epoch: 5128, Batch Gradient Norm after: 22.360679451225092
Epoch 5129/10000, Prediction Accuracy = 59.166%, Loss = 0.8217339158058167
Epoch: 5129, Batch Gradient Norm: 29.22671784997446
Epoch: 5129, Batch Gradient Norm after: 22.360676256937882
Epoch 5130/10000, Prediction Accuracy = 59.160000000000004%, Loss = 0.8239743828773498
Epoch: 5130, Batch Gradient Norm: 28.305544280452956
Epoch: 5130, Batch Gradient Norm after: 22.36067678981516
Epoch 5131/10000, Prediction Accuracy = 59.148%, Loss = 0.8216034173965454
Epoch: 5131, Batch Gradient Norm: 29.223268544086483
Epoch: 5131, Batch Gradient Norm after: 22.360680120558577
Epoch 5132/10000, Prediction Accuracy = 59.15599999999999%, Loss = 0.8238627433776855
Epoch: 5132, Batch Gradient Norm: 28.298718785328717
Epoch: 5132, Batch Gradient Norm after: 22.360678998956857
Epoch 5133/10000, Prediction Accuracy = 59.156000000000006%, Loss = 0.8215014934539795
Epoch: 5133, Batch Gradient Norm: 29.220809448888147
Epoch: 5133, Batch Gradient Norm after: 22.36067985885318
Epoch 5134/10000, Prediction Accuracy = 59.186%, Loss = 0.8237629771232605
Epoch: 5134, Batch Gradient Norm: 28.29073656629646
Epoch: 5134, Batch Gradient Norm after: 22.36067932821766
Epoch 5135/10000, Prediction Accuracy = 59.158%, Loss = 0.8213709354400635
Epoch: 5135, Batch Gradient Norm: 29.219136429022758
Epoch: 5135, Batch Gradient Norm after: 22.360678953885397
Epoch 5136/10000, Prediction Accuracy = 59.15999999999999%, Loss = 0.8236318349838256
Epoch: 5136, Batch Gradient Norm: 28.283138671534534
Epoch: 5136, Batch Gradient Norm after: 22.360678926512108
Epoch 5137/10000, Prediction Accuracy = 59.16799999999999%, Loss = 0.8212131381034851
Epoch: 5137, Batch Gradient Norm: 29.21705158831427
Epoch: 5137, Batch Gradient Norm after: 22.360677015993804
Epoch 5138/10000, Prediction Accuracy = 59.17%, Loss = 0.8235110402107239
Epoch: 5138, Batch Gradient Norm: 28.275124964781675
Epoch: 5138, Batch Gradient Norm after: 22.360677840424085
Epoch 5139/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8210723280906678
Epoch: 5139, Batch Gradient Norm: 29.214410531072726
Epoch: 5139, Batch Gradient Norm after: 22.36067870923397
Epoch 5140/10000, Prediction Accuracy = 59.16400000000001%, Loss = 0.8233871221542358
Epoch: 5140, Batch Gradient Norm: 28.269002848895852
Epoch: 5140, Batch Gradient Norm after: 22.360678387588926
Epoch 5141/10000, Prediction Accuracy = 59.153999999999996%, Loss = 0.8209512114524842
Epoch: 5141, Batch Gradient Norm: 29.2076873901732
Epoch: 5141, Batch Gradient Norm after: 22.360679882842312
Epoch 5142/10000, Prediction Accuracy = 59.146%, Loss = 0.8232748985290528
Epoch: 5142, Batch Gradient Norm: 28.263097970884196
Epoch: 5142, Batch Gradient Norm after: 22.36067714403816
Epoch 5143/10000, Prediction Accuracy = 59.174%, Loss = 0.8208458423614502
Epoch: 5143, Batch Gradient Norm: 29.202171074136693
Epoch: 5143, Batch Gradient Norm after: 22.360681451414052
Epoch 5144/10000, Prediction Accuracy = 59.176%, Loss = 0.8231618762016296
Epoch: 5144, Batch Gradient Norm: 28.25753795385227
Epoch: 5144, Batch Gradient Norm after: 22.36067802849846
Epoch 5145/10000, Prediction Accuracy = 59.16799999999999%, Loss = 0.8207151532173157
Epoch: 5145, Batch Gradient Norm: 29.196264592999583
Epoch: 5145, Batch Gradient Norm after: 22.360678294363044
Epoch 5146/10000, Prediction Accuracy = 59.16600000000001%, Loss = 0.8230249762535096
Epoch: 5146, Batch Gradient Norm: 28.248740700802443
Epoch: 5146, Batch Gradient Norm after: 22.360678875953095
Epoch 5147/10000, Prediction Accuracy = 59.17%, Loss = 0.8205667614936829
Epoch: 5147, Batch Gradient Norm: 29.1936020881527
Epoch: 5147, Batch Gradient Norm after: 22.360679136150413
Epoch 5148/10000, Prediction Accuracy = 59.176%, Loss = 0.8228967547416687
Epoch: 5148, Batch Gradient Norm: 28.244545103779462
Epoch: 5148, Batch Gradient Norm after: 22.360678426643332
Epoch 5149/10000, Prediction Accuracy = 59.184000000000005%, Loss = 0.820431923866272
Epoch: 5149, Batch Gradient Norm: 29.189893138254632
Epoch: 5149, Batch Gradient Norm after: 22.360677284251192
Epoch 5150/10000, Prediction Accuracy = 59.16799999999999%, Loss = 0.8227765083312988
Epoch: 5150, Batch Gradient Norm: 28.239243933713464
Epoch: 5150, Batch Gradient Norm after: 22.36067666429185
Epoch 5151/10000, Prediction Accuracy = 59.172000000000004%, Loss = 0.8203078269958496
Epoch: 5151, Batch Gradient Norm: 29.182758339172977
Epoch: 5151, Batch Gradient Norm after: 22.360680407487347
Epoch 5152/10000, Prediction Accuracy = 59.166%, Loss = 0.8226563096046448
Epoch: 5152, Batch Gradient Norm: 28.232615049219817
Epoch: 5152, Batch Gradient Norm after: 22.36067771163046
Epoch 5153/10000, Prediction Accuracy = 59.19199999999999%, Loss = 0.8202082514762878
Epoch: 5153, Batch Gradient Norm: 29.17686288155134
Epoch: 5153, Batch Gradient Norm after: 22.3606805447002
Epoch 5154/10000, Prediction Accuracy = 59.178%, Loss = 0.8225463390350342
Epoch: 5154, Batch Gradient Norm: 28.225475719313398
Epoch: 5154, Batch Gradient Norm after: 22.36067964714683
Epoch 5155/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8200777888298034
Epoch: 5155, Batch Gradient Norm: 29.16967229346033
Epoch: 5155, Batch Gradient Norm after: 22.360676624451845
Epoch 5156/10000, Prediction Accuracy = 59.162%, Loss = 0.8224005818367004
Epoch: 5156, Batch Gradient Norm: 28.219789042105997
Epoch: 5156, Batch Gradient Norm after: 22.360676307064562
Epoch 5157/10000, Prediction Accuracy = 59.188%, Loss = 0.8199306726455688
Epoch: 5157, Batch Gradient Norm: 29.163770328988992
Epoch: 5157, Batch Gradient Norm after: 22.360676433324322
Epoch 5158/10000, Prediction Accuracy = 59.174%, Loss = 0.8222730398178101
Epoch: 5158, Batch Gradient Norm: 28.212874638728646
Epoch: 5158, Batch Gradient Norm after: 22.360674861338037
Epoch 5159/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8197924733161926
Epoch: 5159, Batch Gradient Norm: 29.16070980408317
Epoch: 5159, Batch Gradient Norm after: 22.36067712341767
Epoch 5160/10000, Prediction Accuracy = 59.166%, Loss = 0.8221529841423034
Epoch: 5160, Batch Gradient Norm: 28.206715192061303
Epoch: 5160, Batch Gradient Norm after: 22.36067701232223
Epoch 5161/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8196682214736939
Epoch: 5161, Batch Gradient Norm: 29.157649934355174
Epoch: 5161, Batch Gradient Norm after: 22.36067998652412
Epoch 5162/10000, Prediction Accuracy = 59.160000000000004%, Loss = 0.8220419406890869
Epoch: 5162, Batch Gradient Norm: 28.199622893090698
Epoch: 5162, Batch Gradient Norm after: 22.3606769494143
Epoch 5163/10000, Prediction Accuracy = 59.194%, Loss = 0.8195704340934753
Epoch: 5163, Batch Gradient Norm: 29.151776747285297
Epoch: 5163, Batch Gradient Norm after: 22.36067887340092
Epoch 5164/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8219304919242859
Epoch: 5164, Batch Gradient Norm: 28.19048494268484
Epoch: 5164, Batch Gradient Norm after: 22.36067910349384
Epoch 5165/10000, Prediction Accuracy = 59.184000000000005%, Loss = 0.8194324374198914
Epoch: 5165, Batch Gradient Norm: 29.146918964472604
Epoch: 5165, Batch Gradient Norm after: 22.36067919276411
Epoch 5166/10000, Prediction Accuracy = 59.17%, Loss = 0.8217836380004883
Epoch: 5166, Batch Gradient Norm: 28.18536219997521
Epoch: 5166, Batch Gradient Norm after: 22.360677229526136
Epoch 5167/10000, Prediction Accuracy = 59.193999999999996%, Loss = 0.8192801117897034
Epoch: 5167, Batch Gradient Norm: 29.146795354145535
Epoch: 5167, Batch Gradient Norm after: 22.360676166161912
Epoch 5168/10000, Prediction Accuracy = 59.162%, Loss = 0.8216626405715942
Epoch: 5168, Batch Gradient Norm: 28.178063769901268
Epoch: 5168, Batch Gradient Norm after: 22.360676727502042
Epoch 5169/10000, Prediction Accuracy = 59.20399999999999%, Loss = 0.8191464304924011
Epoch: 5169, Batch Gradient Norm: 29.14547793532707
Epoch: 5169, Batch Gradient Norm after: 22.360675506388347
Epoch 5170/10000, Prediction Accuracy = 59.17%, Loss = 0.8215466856956481
Epoch: 5170, Batch Gradient Norm: 28.17210387955754
Epoch: 5170, Batch Gradient Norm after: 22.36067894504835
Epoch 5171/10000, Prediction Accuracy = 59.19%, Loss = 0.8190278291702271
Epoch: 5171, Batch Gradient Norm: 29.141028946364543
Epoch: 5171, Batch Gradient Norm after: 22.36067874973819
Epoch 5172/10000, Prediction Accuracy = 59.17%, Loss = 0.8214354872703552
Epoch: 5172, Batch Gradient Norm: 28.1672416291888
Epoch: 5172, Batch Gradient Norm after: 22.360680364426408
Epoch 5173/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8189287543296814
Epoch: 5173, Batch Gradient Norm: 29.131595721866695
Epoch: 5173, Batch Gradient Norm after: 22.360681272249533
Epoch 5174/10000, Prediction Accuracy = 59.19200000000001%, Loss = 0.821312940120697
Epoch: 5174, Batch Gradient Norm: 28.162630773396593
Epoch: 5174, Batch Gradient Norm after: 22.360677264420723
Epoch 5175/10000, Prediction Accuracy = 59.198%, Loss = 0.818792724609375
Epoch: 5175, Batch Gradient Norm: 29.1219230285155
Epoch: 5175, Batch Gradient Norm after: 22.360677022591734
Epoch 5176/10000, Prediction Accuracy = 59.17%, Loss = 0.8211523413658142
Epoch: 5176, Batch Gradient Norm: 28.15907641511861
Epoch: 5176, Batch Gradient Norm after: 22.36067977574379
Epoch 5177/10000, Prediction Accuracy = 59.20400000000001%, Loss = 0.8186527848243713
Epoch: 5177, Batch Gradient Norm: 29.11377737702719
Epoch: 5177, Batch Gradient Norm after: 22.36067802993664
Epoch 5178/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8210179328918457
Epoch: 5178, Batch Gradient Norm: 28.154598071602596
Epoch: 5178, Batch Gradient Norm after: 22.36067728921516
Epoch 5179/10000, Prediction Accuracy = 59.218%, Loss = 0.8185261249542236
Epoch: 5179, Batch Gradient Norm: 29.1039300780493
Epoch: 5179, Batch Gradient Norm after: 22.36067945391517
Epoch 5180/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.8208748817443847
Epoch: 5180, Batch Gradient Norm: 28.150312189626767
Epoch: 5180, Batch Gradient Norm after: 22.360679604402392
Epoch 5181/10000, Prediction Accuracy = 59.19199999999999%, Loss = 0.8184168219566346
Epoch: 5181, Batch Gradient Norm: 29.095750150995276
Epoch: 5181, Batch Gradient Norm after: 22.360678491103343
Epoch 5182/10000, Prediction Accuracy = 59.17%, Loss = 0.8207518935203553
Epoch: 5182, Batch Gradient Norm: 28.14721022869836
Epoch: 5182, Batch Gradient Norm after: 22.360678560210715
Epoch 5183/10000, Prediction Accuracy = 59.2%, Loss = 0.8183188319206238
Epoch: 5183, Batch Gradient Norm: 29.0871346528703
Epoch: 5183, Batch Gradient Norm after: 22.360678503627565
Epoch 5184/10000, Prediction Accuracy = 59.202%, Loss = 0.8206231713294982
Epoch: 5184, Batch Gradient Norm: 28.140705535617773
Epoch: 5184, Batch Gradient Norm after: 22.360679105773087
Epoch 5185/10000, Prediction Accuracy = 59.206%, Loss = 0.8181861162185669
Epoch: 5185, Batch Gradient Norm: 29.07692633772272
Epoch: 5185, Batch Gradient Norm after: 22.360679593009067
Epoch 5186/10000, Prediction Accuracy = 59.186%, Loss = 0.8204703330993652
Epoch: 5186, Batch Gradient Norm: 28.13754350497207
Epoch: 5186, Batch Gradient Norm after: 22.36067891225108
Epoch 5187/10000, Prediction Accuracy = 59.209999999999994%, Loss = 0.818044102191925
Epoch: 5187, Batch Gradient Norm: 29.071146268922462
Epoch: 5187, Batch Gradient Norm after: 22.360676202532815
Epoch 5188/10000, Prediction Accuracy = 59.176%, Loss = 0.8203458905220031
Epoch: 5188, Batch Gradient Norm: 28.13152074428102
Epoch: 5188, Batch Gradient Norm after: 22.360677577452865
Epoch 5189/10000, Prediction Accuracy = 59.20799999999999%, Loss = 0.8179137110710144
Epoch: 5189, Batch Gradient Norm: 29.06626095673895
Epoch: 5189, Batch Gradient Norm after: 22.360679213793386
Epoch 5190/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8202207803726196
Epoch: 5190, Batch Gradient Norm: 28.127906075401487
Epoch: 5190, Batch Gradient Norm after: 22.36067880405311
Epoch 5191/10000, Prediction Accuracy = 59.212%, Loss = 0.8177980184555054
Epoch: 5191, Batch Gradient Norm: 29.061265405098833
Epoch: 5191, Batch Gradient Norm after: 22.360678736736364
Epoch 5192/10000, Prediction Accuracy = 59.17999999999999%, Loss = 0.8201104998588562
Epoch: 5192, Batch Gradient Norm: 28.120783183965617
Epoch: 5192, Batch Gradient Norm after: 22.36067773263339
Epoch 5193/10000, Prediction Accuracy = 59.20799999999999%, Loss = 0.8176944255828857
Epoch: 5193, Batch Gradient Norm: 29.053160581573415
Epoch: 5193, Batch Gradient Norm after: 22.360678397408314
Epoch 5194/10000, Prediction Accuracy = 59.20799999999999%, Loss = 0.8199872970581055
Epoch: 5194, Batch Gradient Norm: 28.114004944434672
Epoch: 5194, Batch Gradient Norm after: 22.360678379308283
Epoch 5195/10000, Prediction Accuracy = 59.205999999999996%, Loss = 0.8175566911697387
Epoch: 5195, Batch Gradient Norm: 29.048035977269777
Epoch: 5195, Batch Gradient Norm after: 22.360677404756046
Epoch 5196/10000, Prediction Accuracy = 59.212%, Loss = 0.8198512673377991
Epoch: 5196, Batch Gradient Norm: 28.107997386695608
Epoch: 5196, Batch Gradient Norm after: 22.36067913239766
Epoch 5197/10000, Prediction Accuracy = 59.21%, Loss = 0.8174177050590515
Epoch: 5197, Batch Gradient Norm: 29.044786192614378
Epoch: 5197, Batch Gradient Norm after: 22.36067725989229
Epoch 5198/10000, Prediction Accuracy = 59.19599999999999%, Loss = 0.8197316884994507
Epoch: 5198, Batch Gradient Norm: 28.10045879078549
Epoch: 5198, Batch Gradient Norm after: 22.360679350984032
Epoch 5199/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8172897815704345
Epoch: 5199, Batch Gradient Norm: 29.03926360769687
Epoch: 5199, Batch Gradient Norm after: 22.36067693533176
Epoch 5200/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.8196130394935608
Epoch: 5200, Batch Gradient Norm: 28.094126563793367
Epoch: 5200, Batch Gradient Norm after: 22.360678645682132
Epoch 5201/10000, Prediction Accuracy = 59.19200000000001%, Loss = 0.8171774625778199
Epoch: 5201, Batch Gradient Norm: 29.03784183238419
Epoch: 5201, Batch Gradient Norm after: 22.360678589528792
Epoch 5202/10000, Prediction Accuracy = 59.198%, Loss = 0.8195113897323608
Epoch: 5202, Batch Gradient Norm: 28.086864053377354
Epoch: 5202, Batch Gradient Norm after: 22.36067755510068
Epoch 5203/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.8170710802078247
Epoch: 5203, Batch Gradient Norm: 29.03462594902774
Epoch: 5203, Batch Gradient Norm after: 22.36067804058952
Epoch 5204/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8193932294845581
Epoch: 5204, Batch Gradient Norm: 28.07977120505813
Epoch: 5204, Batch Gradient Norm after: 22.360677804312374
Epoch 5205/10000, Prediction Accuracy = 59.20399999999999%, Loss = 0.8169288516044617
Epoch: 5205, Batch Gradient Norm: 29.0312766725861
Epoch: 5205, Batch Gradient Norm after: 22.360679128724527
Epoch 5206/10000, Prediction Accuracy = 59.23%, Loss = 0.8192670464515686
Epoch: 5206, Batch Gradient Norm: 28.07233081919314
Epoch: 5206, Batch Gradient Norm after: 22.36067898727951
Epoch 5207/10000, Prediction Accuracy = 59.217999999999996%, Loss = 0.816787040233612
Epoch: 5207, Batch Gradient Norm: 29.02938489368099
Epoch: 5207, Batch Gradient Norm after: 22.360678439255295
Epoch 5208/10000, Prediction Accuracy = 59.226%, Loss = 0.8191612243652344
Epoch: 5208, Batch Gradient Norm: 28.063551289173336
Epoch: 5208, Batch Gradient Norm after: 22.360677639957316
Epoch 5209/10000, Prediction Accuracy = 59.21%, Loss = 0.8166534066200256
Epoch: 5209, Batch Gradient Norm: 29.030046092589505
Epoch: 5209, Batch Gradient Norm after: 22.360680463563916
Epoch 5210/10000, Prediction Accuracy = 59.226%, Loss = 0.8190505385398865
Epoch: 5210, Batch Gradient Norm: 28.05826392526314
Epoch: 5210, Batch Gradient Norm after: 22.360681378176356
Epoch 5211/10000, Prediction Accuracy = 59.212%, Loss = 0.8165524959564209
Epoch: 5211, Batch Gradient Norm: 29.030442706484962
Epoch: 5211, Batch Gradient Norm after: 22.360679030189463
Epoch 5212/10000, Prediction Accuracy = 59.226%, Loss = 0.8189650535583496
Epoch: 5212, Batch Gradient Norm: 28.048453169640467
Epoch: 5212, Batch Gradient Norm after: 22.36067904825283
Epoch 5213/10000, Prediction Accuracy = 59.245999999999995%, Loss = 0.8164427518844605
Epoch: 5213, Batch Gradient Norm: 29.02713017290126
Epoch: 5213, Batch Gradient Norm after: 22.360680263963562
Epoch 5214/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.8188335418701171
Epoch: 5214, Batch Gradient Norm: 28.041115538093905
Epoch: 5214, Batch Gradient Norm after: 22.360676218178973
Epoch 5215/10000, Prediction Accuracy = 59.217999999999996%, Loss = 0.816286587715149
Epoch: 5215, Batch Gradient Norm: 29.025618655616977
Epoch: 5215, Batch Gradient Norm after: 22.360677706260855
Epoch 5216/10000, Prediction Accuracy = 59.246%, Loss = 0.8187116622924805
Epoch: 5216, Batch Gradient Norm: 28.03503879576829
Epoch: 5216, Batch Gradient Norm after: 22.360679601864206
Epoch 5217/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.8161502242088318
Epoch: 5217, Batch Gradient Norm: 29.023986172802864
Epoch: 5217, Batch Gradient Norm after: 22.360680048663742
Epoch 5218/10000, Prediction Accuracy = 59.257999999999996%, Loss = 0.8185978293418884
Epoch: 5218, Batch Gradient Norm: 28.028144490811783
Epoch: 5218, Batch Gradient Norm after: 22.36067702309468
Epoch 5219/10000, Prediction Accuracy = 59.214%, Loss = 0.8160202503204346
Epoch: 5219, Batch Gradient Norm: 29.02216400406108
Epoch: 5219, Batch Gradient Norm after: 22.36067755821257
Epoch 5220/10000, Prediction Accuracy = 59.242000000000004%, Loss = 0.8184948563575745
Epoch: 5220, Batch Gradient Norm: 28.0216254986417
Epoch: 5220, Batch Gradient Norm after: 22.360681457328674
Epoch 5221/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.815920352935791
Epoch: 5221, Batch Gradient Norm: 29.023334002972263
Epoch: 5221, Batch Gradient Norm after: 22.360678856718643
Epoch 5222/10000, Prediction Accuracy = 59.25%, Loss = 0.8184042334556579
Epoch: 5222, Batch Gradient Norm: 28.012901350902077
Epoch: 5222, Batch Gradient Norm after: 22.360678704993166
Epoch 5223/10000, Prediction Accuracy = 59.232000000000006%, Loss = 0.8158018231391907
Epoch: 5223, Batch Gradient Norm: 29.019657579743285
Epoch: 5223, Batch Gradient Norm after: 22.3606779330277
Epoch 5224/10000, Prediction Accuracy = 59.237999999999985%, Loss = 0.8182758927345276
Epoch: 5224, Batch Gradient Norm: 28.006349154620406
Epoch: 5224, Batch Gradient Norm after: 22.360677090836177
Epoch 5225/10000, Prediction Accuracy = 59.214%, Loss = 0.8156480312347412
Epoch: 5225, Batch Gradient Norm: 29.017759417832522
Epoch: 5225, Batch Gradient Norm after: 22.360676442394645
Epoch 5226/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8181536674499512
Epoch: 5226, Batch Gradient Norm: 27.99806692341787
Epoch: 5226, Batch Gradient Norm after: 22.360678222813235
Epoch 5227/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.8155098915100097
Epoch: 5227, Batch Gradient Norm: 29.015798841107515
Epoch: 5227, Batch Gradient Norm after: 22.36067716315807
Epoch 5228/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.8180422067642212
Epoch: 5228, Batch Gradient Norm: 27.99370231042757
Epoch: 5228, Batch Gradient Norm after: 22.36067787148794
Epoch 5229/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8153877258300781
Epoch: 5229, Batch Gradient Norm: 29.012395373697952
Epoch: 5229, Batch Gradient Norm after: 22.360679553842076
Epoch 5230/10000, Prediction Accuracy = 59.25600000000001%, Loss = 0.8179256796836853
Epoch: 5230, Batch Gradient Norm: 27.98746137884037
Epoch: 5230, Batch Gradient Norm after: 22.360678867920367
Epoch 5231/10000, Prediction Accuracy = 59.234%, Loss = 0.815290904045105
Epoch: 5231, Batch Gradient Norm: 29.005247256423875
Epoch: 5231, Batch Gradient Norm after: 22.36067848097302
Epoch 5232/10000, Prediction Accuracy = 59.27%, Loss = 0.8178160667419434
Epoch: 5232, Batch Gradient Norm: 27.97970793494628
Epoch: 5232, Batch Gradient Norm after: 22.36067863227387
Epoch 5233/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8151738524436951
Epoch: 5233, Batch Gradient Norm: 29.00026360969701
Epoch: 5233, Batch Gradient Norm after: 22.360676809907957
Epoch 5234/10000, Prediction Accuracy = 59.25600000000001%, Loss = 0.817681884765625
Epoch: 5234, Batch Gradient Norm: 27.974187577769197
Epoch: 5234, Batch Gradient Norm after: 22.360678333973535
Epoch 5235/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.8150202751159668
Epoch: 5235, Batch Gradient Norm: 28.99511242529669
Epoch: 5235, Batch Gradient Norm after: 22.36067954535666
Epoch 5236/10000, Prediction Accuracy = 59.262%, Loss = 0.8175552368164063
Epoch: 5236, Batch Gradient Norm: 27.969109022163646
Epoch: 5236, Batch Gradient Norm after: 22.360679351607484
Epoch 5237/10000, Prediction Accuracy = 59.24000000000001%, Loss = 0.8148869633674621
Epoch: 5237, Batch Gradient Norm: 28.98677382765183
Epoch: 5237, Batch Gradient Norm after: 22.360677841685753
Epoch 5238/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.8174275994300843
Epoch: 5238, Batch Gradient Norm: 27.96683982140876
Epoch: 5238, Batch Gradient Norm after: 22.360677582371828
Epoch 5239/10000, Prediction Accuracy = 59.238%, Loss = 0.8147679686546325
Epoch: 5239, Batch Gradient Norm: 28.97468975446092
Epoch: 5239, Batch Gradient Norm after: 22.36067732358407
Epoch 5240/10000, Prediction Accuracy = 59.284000000000006%, Loss = 0.8172973036766052
Epoch: 5240, Batch Gradient Norm: 27.96305637211461
Epoch: 5240, Batch Gradient Norm after: 22.360680168223677
Epoch 5241/10000, Prediction Accuracy = 59.23%, Loss = 0.8146737217903137
Epoch: 5241, Batch Gradient Norm: 28.966529852530073
Epoch: 5241, Batch Gradient Norm after: 22.360676641084382
Epoch 5242/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.8171770930290222
Epoch: 5242, Batch Gradient Norm: 27.9582113847528
Epoch: 5242, Batch Gradient Norm after: 22.36067753714215
Epoch 5243/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.8145573973655701
Epoch: 5243, Batch Gradient Norm: 28.9569858625365
Epoch: 5243, Batch Gradient Norm after: 22.360677181954888
Epoch 5244/10000, Prediction Accuracy = 59.26800000000001%, Loss = 0.8170242071151733
Epoch: 5244, Batch Gradient Norm: 27.953454868949645
Epoch: 5244, Batch Gradient Norm after: 22.36067812489944
Epoch 5245/10000, Prediction Accuracy = 59.24399999999999%, Loss = 0.8144132375717164
Epoch: 5245, Batch Gradient Norm: 28.945181919615308
Epoch: 5245, Batch Gradient Norm after: 22.36067771427841
Epoch 5246/10000, Prediction Accuracy = 59.28599999999999%, Loss = 0.816884446144104
Epoch: 5246, Batch Gradient Norm: 27.948798182936592
Epoch: 5246, Batch Gradient Norm after: 22.36067644186992
Epoch 5247/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.8142849683761597
Epoch: 5247, Batch Gradient Norm: 28.937052352825134
Epoch: 5247, Batch Gradient Norm after: 22.36067852473188
Epoch 5248/10000, Prediction Accuracy = 59.25999999999999%, Loss = 0.8167522788047791
Epoch: 5248, Batch Gradient Norm: 27.943281082272094
Epoch: 5248, Batch Gradient Norm after: 22.36067553487034
Epoch 5249/10000, Prediction Accuracy = 59.23199999999999%, Loss = 0.8141655206680298
Epoch: 5249, Batch Gradient Norm: 28.929801464975277
Epoch: 5249, Batch Gradient Norm after: 22.360677743253063
Epoch 5250/10000, Prediction Accuracy = 59.290000000000006%, Loss = 0.8166339993476868
Epoch: 5250, Batch Gradient Norm: 27.937939612400715
Epoch: 5250, Batch Gradient Norm after: 22.36067864573846
Epoch 5251/10000, Prediction Accuracy = 59.238%, Loss = 0.8140759587287902
Epoch: 5251, Batch Gradient Norm: 28.91758741357928
Epoch: 5251, Batch Gradient Norm after: 22.360680211731662
Epoch 5252/10000, Prediction Accuracy = 59.318%, Loss = 0.8165066123008728
Epoch: 5252, Batch Gradient Norm: 27.930584263219092
Epoch: 5252, Batch Gradient Norm after: 22.360679402348723
Epoch 5253/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8139537930488586
Epoch: 5253, Batch Gradient Norm: 28.909271132231734
Epoch: 5253, Batch Gradient Norm after: 22.36068068875995
Epoch 5254/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.8163580894470215
Epoch: 5254, Batch Gradient Norm: 27.927081337619917
Epoch: 5254, Batch Gradient Norm after: 22.3606785084348
Epoch 5255/10000, Prediction Accuracy = 59.238%, Loss = 0.8138082027435303
Epoch: 5255, Batch Gradient Norm: 28.901918147247283
Epoch: 5255, Batch Gradient Norm after: 22.360677146038455
Epoch 5256/10000, Prediction Accuracy = 59.274%, Loss = 0.816228449344635
Epoch: 5256, Batch Gradient Norm: 27.92144455866899
Epoch: 5256, Batch Gradient Norm after: 22.360676039380863
Epoch 5257/10000, Prediction Accuracy = 59.254%, Loss = 0.8136832356452942
Epoch: 5257, Batch Gradient Norm: 28.892228160598815
Epoch: 5257, Batch Gradient Norm after: 22.36067888415497
Epoch 5258/10000, Prediction Accuracy = 59.267999999999994%, Loss = 0.8160918831825257
Epoch: 5258, Batch Gradient Norm: 27.920258984372115
Epoch: 5258, Batch Gradient Norm after: 22.36067951462777
Epoch 5259/10000, Prediction Accuracy = 59.23%, Loss = 0.8135694265365601
Epoch: 5259, Batch Gradient Norm: 28.880850665106795
Epoch: 5259, Batch Gradient Norm after: 22.360679623754297
Epoch 5260/10000, Prediction Accuracy = 59.30799999999999%, Loss = 0.815966784954071
Epoch: 5260, Batch Gradient Norm: 27.916423625849845
Epoch: 5260, Batch Gradient Norm after: 22.360677333345617
Epoch 5261/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8134796380996704
Epoch: 5261, Batch Gradient Norm: 28.870202684662175
Epoch: 5261, Batch Gradient Norm after: 22.360679551147086
Epoch 5262/10000, Prediction Accuracy = 59.33%, Loss = 0.8158351898193359
Epoch: 5262, Batch Gradient Norm: 27.909721925965105
Epoch: 5262, Batch Gradient Norm after: 22.36067855936954
Epoch 5263/10000, Prediction Accuracy = 59.23%, Loss = 0.8133451700210571
Epoch: 5263, Batch Gradient Norm: 28.861277538292168
Epoch: 5263, Batch Gradient Norm after: 22.360680585818148
Epoch 5264/10000, Prediction Accuracy = 59.288%, Loss = 0.8156855702400208
Epoch: 5264, Batch Gradient Norm: 27.90456303209185
Epoch: 5264, Batch Gradient Norm after: 22.360679251421768
Epoch 5265/10000, Prediction Accuracy = 59.246%, Loss = 0.8132036447525024
Epoch: 5265, Batch Gradient Norm: 28.849825638816164
Epoch: 5265, Batch Gradient Norm after: 22.36067744207819
Epoch 5266/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.8155531406402587
Epoch: 5266, Batch Gradient Norm: 27.900603264281525
Epoch: 5266, Batch Gradient Norm after: 22.360678974209954
Epoch 5267/10000, Prediction Accuracy = 59.272000000000006%, Loss = 0.8130774855613708
Epoch: 5267, Batch Gradient Norm: 28.84173111490734
Epoch: 5267, Batch Gradient Norm after: 22.360676679423527
Epoch 5268/10000, Prediction Accuracy = 59.278%, Loss = 0.8154160976409912
Epoch: 5268, Batch Gradient Norm: 27.89759653244515
Epoch: 5268, Batch Gradient Norm after: 22.360678709805146
Epoch 5269/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.812968909740448
Epoch: 5269, Batch Gradient Norm: 28.8305714139369
Epoch: 5269, Batch Gradient Norm after: 22.36068020059609
Epoch 5270/10000, Prediction Accuracy = 59.318%, Loss = 0.8152884006500244
Epoch: 5270, Batch Gradient Norm: 27.896159349664376
Epoch: 5270, Batch Gradient Norm after: 22.360681017600896
Epoch 5271/10000, Prediction Accuracy = 59.274%, Loss = 0.8128847479820251
Epoch: 5271, Batch Gradient Norm: 28.8150259689406
Epoch: 5271, Batch Gradient Norm after: 22.360677639363548
Epoch 5272/10000, Prediction Accuracy = 59.33200000000001%, Loss = 0.8151434898376465
Epoch: 5272, Batch Gradient Norm: 27.891443208721665
Epoch: 5272, Batch Gradient Norm after: 22.360678329023727
Epoch 5273/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.8127503395080566
Epoch: 5273, Batch Gradient Norm: 28.80092739196738
Epoch: 5273, Batch Gradient Norm after: 22.360678370361494
Epoch 5274/10000, Prediction Accuracy = 59.31%, Loss = 0.8149917244911193
Epoch: 5274, Batch Gradient Norm: 27.88705567289785
Epoch: 5274, Batch Gradient Norm after: 22.360678449890326
Epoch 5275/10000, Prediction Accuracy = 59.260000000000005%, Loss = 0.8126052498817444
Epoch: 5275, Batch Gradient Norm: 28.794581735938333
Epoch: 5275, Batch Gradient Norm after: 22.360678305161358
Epoch 5276/10000, Prediction Accuracy = 59.27%, Loss = 0.8148671150207519
Epoch: 5276, Batch Gradient Norm: 27.88338432973387
Epoch: 5276, Batch Gradient Norm after: 22.360680689708996
Epoch 5277/10000, Prediction Accuracy = 59.278%, Loss = 0.8124846577644348
Epoch: 5277, Batch Gradient Norm: 28.782196026968045
Epoch: 5277, Batch Gradient Norm after: 22.360678064707205
Epoch 5278/10000, Prediction Accuracy = 59.3%, Loss = 0.8147266983985901
Epoch: 5278, Batch Gradient Norm: 27.877834323420664
Epoch: 5278, Batch Gradient Norm after: 22.360678229010833
Epoch 5279/10000, Prediction Accuracy = 59.25599999999999%, Loss = 0.8123797535896301
Epoch: 5279, Batch Gradient Norm: 28.773050522977893
Epoch: 5279, Batch Gradient Norm after: 22.36067897770756
Epoch 5280/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8146058797836304
Epoch: 5280, Batch Gradient Norm: 27.874525154242132
Epoch: 5280, Batch Gradient Norm after: 22.3606788008309
Epoch 5281/10000, Prediction Accuracy = 59.266%, Loss = 0.8122834801673889
Epoch: 5281, Batch Gradient Norm: 28.7630832002387
Epoch: 5281, Batch Gradient Norm after: 22.36067915315732
Epoch 5282/10000, Prediction Accuracy = 59.326%, Loss = 0.8144728422164917
Epoch: 5282, Batch Gradient Norm: 27.86913105333725
Epoch: 5282, Batch Gradient Norm after: 22.36067829229217
Epoch 5283/10000, Prediction Accuracy = 59.258%, Loss = 0.8121463656425476
Epoch: 5283, Batch Gradient Norm: 28.754498112691746
Epoch: 5283, Batch Gradient Norm after: 22.360681097758803
Epoch 5284/10000, Prediction Accuracy = 59.322%, Loss = 0.8143303871154786
Epoch: 5284, Batch Gradient Norm: 27.864902770002878
Epoch: 5284, Batch Gradient Norm after: 22.36067855943514
Epoch 5285/10000, Prediction Accuracy = 59.267999999999994%, Loss = 0.8120087862014771
Epoch: 5285, Batch Gradient Norm: 28.74803739900928
Epoch: 5285, Batch Gradient Norm after: 22.3606787177509
Epoch 5286/10000, Prediction Accuracy = 59.3%, Loss = 0.8142051815986633
Epoch: 5286, Batch Gradient Norm: 27.85955636888892
Epoch: 5286, Batch Gradient Norm after: 22.36067547951835
Epoch 5287/10000, Prediction Accuracy = 59.266000000000005%, Loss = 0.8118886351585388
Epoch: 5287, Batch Gradient Norm: 28.74041116333133
Epoch: 5287, Batch Gradient Norm after: 22.36067950768417
Epoch 5288/10000, Prediction Accuracy = 59.314%, Loss = 0.8140736818313599
Epoch: 5288, Batch Gradient Norm: 27.857096631815423
Epoch: 5288, Batch Gradient Norm after: 22.36067845519734
Epoch 5289/10000, Prediction Accuracy = 59.272000000000006%, Loss = 0.8117817640304565
Epoch: 5289, Batch Gradient Norm: 28.73157483370525
Epoch: 5289, Batch Gradient Norm after: 22.360678242557434
Epoch 5290/10000, Prediction Accuracy = 59.334%, Loss = 0.8139554977416992
Epoch: 5290, Batch Gradient Norm: 27.85184963599097
Epoch: 5290, Batch Gradient Norm after: 22.360680764022543
Epoch 5291/10000, Prediction Accuracy = 59.263999999999996%, Loss = 0.8116831421852112
Epoch: 5291, Batch Gradient Norm: 28.72411368191802
Epoch: 5291, Batch Gradient Norm after: 22.360680002170326
Epoch 5292/10000, Prediction Accuracy = 59.33200000000001%, Loss = 0.8138359427452088
Epoch: 5292, Batch Gradient Norm: 27.844593996278384
Epoch: 5292, Batch Gradient Norm after: 22.36067949541461
Epoch 5293/10000, Prediction Accuracy = 59.262%, Loss = 0.8115426182746888
Epoch: 5293, Batch Gradient Norm: 28.722353550747464
Epoch: 5293, Batch Gradient Norm after: 22.360679447336054
Epoch 5294/10000, Prediction Accuracy = 59.342%, Loss = 0.8137071251869201
Epoch: 5294, Batch Gradient Norm: 27.839475977988034
Epoch: 5294, Batch Gradient Norm after: 22.360678562176926
Epoch 5295/10000, Prediction Accuracy = 59.266%, Loss = 0.8114053010940552
Epoch: 5295, Batch Gradient Norm: 28.716647824582022
Epoch: 5295, Batch Gradient Norm after: 22.360678393791208
Epoch 5296/10000, Prediction Accuracy = 59.302%, Loss = 0.8135929822921752
Epoch: 5296, Batch Gradient Norm: 27.8311172728246
Epoch: 5296, Batch Gradient Norm after: 22.36067780574245
Epoch 5297/10000, Prediction Accuracy = 59.25600000000001%, Loss = 0.811279547214508
Epoch: 5297, Batch Gradient Norm: 28.71147375905586
Epoch: 5297, Batch Gradient Norm after: 22.360679311016273
Epoch 5298/10000, Prediction Accuracy = 59.324%, Loss = 0.8134678125381469
Epoch: 5298, Batch Gradient Norm: 27.828947777878422
Epoch: 5298, Batch Gradient Norm after: 22.36068021333048
Epoch 5299/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.8111794471740723
Epoch: 5299, Batch Gradient Norm: 28.702933092445946
Epoch: 5299, Batch Gradient Norm after: 22.36067923875534
Epoch 5300/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8133536219596863
Epoch: 5300, Batch Gradient Norm: 27.822191833903354
Epoch: 5300, Batch Gradient Norm after: 22.36067903014718
Epoch 5301/10000, Prediction Accuracy = 59.274%, Loss = 0.811080539226532
Epoch: 5301, Batch Gradient Norm: 28.69228020766244
Epoch: 5301, Batch Gradient Norm after: 22.360680236430564
Epoch 5302/10000, Prediction Accuracy = 59.34000000000001%, Loss = 0.8132196545600892
Epoch: 5302, Batch Gradient Norm: 27.81802157245174
Epoch: 5302, Batch Gradient Norm after: 22.360679560830285
Epoch 5303/10000, Prediction Accuracy = 59.274%, Loss = 0.810945188999176
Epoch: 5303, Batch Gradient Norm: 28.684315585271225
Epoch: 5303, Batch Gradient Norm after: 22.360678988435346
Epoch 5304/10000, Prediction Accuracy = 59.336%, Loss = 0.8130805850028991
Epoch: 5304, Batch Gradient Norm: 27.81115763885201
Epoch: 5304, Batch Gradient Norm after: 22.360675442970518
Epoch 5305/10000, Prediction Accuracy = 59.27%, Loss = 0.8108078002929687
Epoch: 5305, Batch Gradient Norm: 28.676211165679593
Epoch: 5305, Batch Gradient Norm after: 22.360678510618232
Epoch 5306/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8129429221153259
Epoch: 5306, Batch Gradient Norm: 27.80802596517768
Epoch: 5306, Batch Gradient Norm after: 22.360676803890293
Epoch 5307/10000, Prediction Accuracy = 59.260000000000005%, Loss = 0.8106908321380615
Epoch: 5307, Batch Gradient Norm: 28.666590383964472
Epoch: 5307, Batch Gradient Norm after: 22.36067930278699
Epoch 5308/10000, Prediction Accuracy = 59.342%, Loss = 0.8128145098686218
Epoch: 5308, Batch Gradient Norm: 27.80651980265373
Epoch: 5308, Batch Gradient Norm after: 22.360678740713624
Epoch 5309/10000, Prediction Accuracy = 59.3%, Loss = 0.810587739944458
Epoch: 5309, Batch Gradient Norm: 28.65685989453725
Epoch: 5309, Batch Gradient Norm after: 22.360678323316794
Epoch 5310/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8126980423927307
Epoch: 5310, Batch Gradient Norm: 27.799963060363275
Epoch: 5310, Batch Gradient Norm after: 22.360678396161884
Epoch 5311/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.8104896068572998
Epoch: 5311, Batch Gradient Norm: 28.646693743437524
Epoch: 5311, Batch Gradient Norm after: 22.36067878591708
Epoch 5312/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.8125614404678345
Epoch: 5312, Batch Gradient Norm: 27.794793694353498
Epoch: 5312, Batch Gradient Norm after: 22.360678449693765
Epoch 5313/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.8103461503982544
Epoch: 5313, Batch Gradient Norm: 28.641525340286517
Epoch: 5313, Batch Gradient Norm after: 22.360678718066726
Epoch 5314/10000, Prediction Accuracy = 59.338%, Loss = 0.8124270796775818
Epoch: 5314, Batch Gradient Norm: 27.787998655474656
Epoch: 5314, Batch Gradient Norm after: 22.360676641100824
Epoch 5315/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.8102112412452698
Epoch: 5315, Batch Gradient Norm: 28.6407347668992
Epoch: 5315, Batch Gradient Norm after: 22.36067749566548
Epoch 5316/10000, Prediction Accuracy = 59.33%, Loss = 0.812318217754364
Epoch: 5316, Batch Gradient Norm: 27.780948461218237
Epoch: 5316, Batch Gradient Norm after: 22.360678264798548
Epoch 5317/10000, Prediction Accuracy = 59.269999999999996%, Loss = 0.8100902199745178
Epoch: 5317, Batch Gradient Norm: 28.638873992020795
Epoch: 5317, Batch Gradient Norm after: 22.360678067168436
Epoch 5318/10000, Prediction Accuracy = 59.35%, Loss = 0.8122039437294006
Epoch: 5318, Batch Gradient Norm: 27.773928026806143
Epoch: 5318, Batch Gradient Norm after: 22.36067779636619
Epoch 5319/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.8099826812744141
Epoch: 5319, Batch Gradient Norm: 28.6366536598966
Epoch: 5319, Batch Gradient Norm after: 22.360678173609084
Epoch 5320/10000, Prediction Accuracy = 59.342000000000006%, Loss = 0.8121047973632812
Epoch: 5320, Batch Gradient Norm: 27.766130257430422
Epoch: 5320, Batch Gradient Norm after: 22.360680446111736
Epoch 5321/10000, Prediction Accuracy = 59.302%, Loss = 0.8098781943321228
Epoch: 5321, Batch Gradient Norm: 28.634596379650134
Epoch: 5321, Batch Gradient Norm after: 22.360677275129287
Epoch 5322/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8119935154914856
Epoch: 5322, Batch Gradient Norm: 27.758139204970814
Epoch: 5322, Batch Gradient Norm after: 22.3606787772038
Epoch 5323/10000, Prediction Accuracy = 59.284000000000006%, Loss = 0.8097308516502381
Epoch: 5323, Batch Gradient Norm: 28.638673882221237
Epoch: 5323, Batch Gradient Norm after: 22.360678282693254
Epoch 5324/10000, Prediction Accuracy = 59.342%, Loss = 0.8118825197219849
Epoch: 5324, Batch Gradient Norm: 27.748349569526077
Epoch: 5324, Batch Gradient Norm after: 22.36067668177856
Epoch 5325/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.8095903277397156
Epoch: 5325, Batch Gradient Norm: 28.641769768758625
Epoch: 5325, Batch Gradient Norm after: 22.36067941503247
Epoch 5326/10000, Prediction Accuracy = 59.33200000000001%, Loss = 0.8117907285690308
Epoch: 5326, Batch Gradient Norm: 27.7401713635757
Epoch: 5326, Batch Gradient Norm after: 22.36067878855674
Epoch 5327/10000, Prediction Accuracy = 59.282%, Loss = 0.8094612121582031
Epoch: 5327, Batch Gradient Norm: 28.643923529553135
Epoch: 5327, Batch Gradient Norm after: 22.36067812416128
Epoch 5328/10000, Prediction Accuracy = 59.366%, Loss = 0.8116865634918213
Epoch: 5328, Batch Gradient Norm: 27.73369526955731
Epoch: 5328, Batch Gradient Norm after: 22.360679483512122
Epoch 5329/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8093604922294617
Epoch: 5329, Batch Gradient Norm: 28.64369617180221
Epoch: 5329, Batch Gradient Norm after: 22.360678330325623
Epoch 5330/10000, Prediction Accuracy = 59.338%, Loss = 0.8116059541702271
Epoch: 5330, Batch Gradient Norm: 27.72274785649051
Epoch: 5330, Batch Gradient Norm after: 22.360679833086795
Epoch 5331/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.8092495203018188
Epoch: 5331, Batch Gradient Norm: 28.64316324230185
Epoch: 5331, Batch Gradient Norm after: 22.360677202543254
Epoch 5332/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.8114877820014954
Epoch: 5332, Batch Gradient Norm: 27.715790295110505
Epoch: 5332, Batch Gradient Norm after: 22.360678743297683
Epoch 5333/10000, Prediction Accuracy = 59.306000000000004%, Loss = 0.8090939879417419
Epoch: 5333, Batch Gradient Norm: 28.643123328199426
Epoch: 5333, Batch Gradient Norm after: 22.360679085430995
Epoch 5334/10000, Prediction Accuracy = 59.352%, Loss = 0.8113684296607971
Epoch: 5334, Batch Gradient Norm: 27.709000888283942
Epoch: 5334, Batch Gradient Norm after: 22.360678650421434
Epoch 5335/10000, Prediction Accuracy = 59.306%, Loss = 0.8089611053466796
Epoch: 5335, Batch Gradient Norm: 28.639963445150737
Epoch: 5335, Batch Gradient Norm after: 22.360677979805295
Epoch 5336/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8112573742866516
Epoch: 5336, Batch Gradient Norm: 27.702419468697343
Epoch: 5336, Batch Gradient Norm after: 22.36067834433688
Epoch 5337/10000, Prediction Accuracy = 59.302%, Loss = 0.8088382124900818
Epoch: 5337, Batch Gradient Norm: 28.638937332026803
Epoch: 5337, Batch Gradient Norm after: 22.36067824534768
Epoch 5338/10000, Prediction Accuracy = 59.36800000000001%, Loss = 0.8111536502838135
Epoch: 5338, Batch Gradient Norm: 27.695161598112787
Epoch: 5338, Batch Gradient Norm after: 22.36067937923339
Epoch 5339/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.8087388396263122
Epoch: 5339, Batch Gradient Norm: 28.63590926341427
Epoch: 5339, Batch Gradient Norm after: 22.360679813515727
Epoch 5340/10000, Prediction Accuracy = 59.342%, Loss = 0.8110590696334838
Epoch: 5340, Batch Gradient Norm: 27.685072192356543
Epoch: 5340, Batch Gradient Norm after: 22.360676522238208
Epoch 5341/10000, Prediction Accuracy = 59.324%, Loss = 0.8086274147033692
Epoch: 5341, Batch Gradient Norm: 28.63430451921828
Epoch: 5341, Batch Gradient Norm after: 22.360681164660217
Epoch 5342/10000, Prediction Accuracy = 59.374%, Loss = 0.8109380960464477
Epoch: 5342, Batch Gradient Norm: 27.677231249312637
Epoch: 5342, Batch Gradient Norm after: 22.3606772943018
Epoch 5343/10000, Prediction Accuracy = 59.324%, Loss = 0.8084735035896301
Epoch: 5343, Batch Gradient Norm: 28.63753533688694
Epoch: 5343, Batch Gradient Norm after: 22.360677887581364
Epoch 5344/10000, Prediction Accuracy = 59.348%, Loss = 0.8108300685882568
Epoch: 5344, Batch Gradient Norm: 27.670926723409046
Epoch: 5344, Batch Gradient Norm after: 22.360678674658644
Epoch 5345/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.8083465337753296
Epoch: 5345, Batch Gradient Norm: 28.636716675484323
Epoch: 5345, Batch Gradient Norm after: 22.36067700159943
Epoch 5346/10000, Prediction Accuracy = 59.35%, Loss = 0.8107277870178222
Epoch: 5346, Batch Gradient Norm: 27.663868592453554
Epoch: 5346, Batch Gradient Norm after: 22.360678854544926
Epoch 5347/10000, Prediction Accuracy = 59.30800000000001%, Loss = 0.8082207202911377
Epoch: 5347, Batch Gradient Norm: 28.633578857121712
Epoch: 5347, Batch Gradient Norm after: 22.36067906317238
Epoch 5348/10000, Prediction Accuracy = 59.36%, Loss = 0.8106236815452575
Epoch: 5348, Batch Gradient Norm: 27.658590205155342
Epoch: 5348, Batch Gradient Norm after: 22.360678292002838
Epoch 5349/10000, Prediction Accuracy = 59.33399999999999%, Loss = 0.8081219434738159
Epoch: 5349, Batch Gradient Norm: 28.628007826698827
Epoch: 5349, Batch Gradient Norm after: 22.360676069417337
Epoch 5350/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8105294466018677
Epoch: 5350, Batch Gradient Norm: 27.65048902377065
Epoch: 5350, Batch Gradient Norm after: 22.360678485241507
Epoch 5351/10000, Prediction Accuracy = 59.324%, Loss = 0.8080044507980346
Epoch: 5351, Batch Gradient Norm: 28.624468236116456
Epoch: 5351, Batch Gradient Norm after: 22.360678203627426
Epoch 5352/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.810397207736969
Epoch: 5352, Batch Gradient Norm: 27.643230697776435
Epoch: 5352, Batch Gradient Norm after: 22.36067879370964
Epoch 5353/10000, Prediction Accuracy = 59.348%, Loss = 0.8078531861305237
Epoch: 5353, Batch Gradient Norm: 28.62202288642019
Epoch: 5353, Batch Gradient Norm after: 22.360679543150965
Epoch 5354/10000, Prediction Accuracy = 59.36%, Loss = 0.8102782487869262
Epoch: 5354, Batch Gradient Norm: 27.638799568302712
Epoch: 5354, Batch Gradient Norm after: 22.36067882368878
Epoch 5355/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.807729172706604
Epoch: 5355, Batch Gradient Norm: 28.61742695895965
Epoch: 5355, Batch Gradient Norm after: 22.36067679397235
Epoch 5356/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8101633429527283
Epoch: 5356, Batch Gradient Norm: 27.631933285894185
Epoch: 5356, Batch Gradient Norm after: 22.360678172166406
Epoch 5357/10000, Prediction Accuracy = 59.33399999999999%, Loss = 0.8076085686683655
Epoch: 5357, Batch Gradient Norm: 28.61539541869044
Epoch: 5357, Batch Gradient Norm after: 22.360679826719373
Epoch 5358/10000, Prediction Accuracy = 59.36400000000001%, Loss = 0.8100523948669434
Epoch: 5358, Batch Gradient Norm: 27.62693493352695
Epoch: 5358, Batch Gradient Norm after: 22.360676666542542
Epoch 5359/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.8075136661529541
Epoch: 5359, Batch Gradient Norm: 28.610831088638083
Epoch: 5359, Batch Gradient Norm after: 22.360681348660112
Epoch 5360/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.8099571943283081
Epoch: 5360, Batch Gradient Norm: 27.61816571062543
Epoch: 5360, Batch Gradient Norm after: 22.360676346509358
Epoch 5361/10000, Prediction Accuracy = 59.331999999999994%, Loss = 0.8073929905891418
Epoch: 5361, Batch Gradient Norm: 28.60995561127578
Epoch: 5361, Batch Gradient Norm after: 22.36068002702283
Epoch 5362/10000, Prediction Accuracy = 59.384%, Loss = 0.8098227620124817
Epoch: 5362, Batch Gradient Norm: 27.612747527136904
Epoch: 5362, Batch Gradient Norm after: 22.360678869154643
Epoch 5363/10000, Prediction Accuracy = 59.36%, Loss = 0.8072494387626648
Epoch: 5363, Batch Gradient Norm: 28.605111122868255
Epoch: 5363, Batch Gradient Norm after: 22.360678391015373
Epoch 5364/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.8097025513648987
Epoch: 5364, Batch Gradient Norm: 27.60635876873955
Epoch: 5364, Batch Gradient Norm after: 22.360678554352308
Epoch 5365/10000, Prediction Accuracy = 59.326%, Loss = 0.8071155548095703
Epoch: 5365, Batch Gradient Norm: 28.60387362653577
Epoch: 5365, Batch Gradient Norm after: 22.360679257586416
Epoch 5366/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8095866680145264
Epoch: 5366, Batch Gradient Norm: 27.60306995924227
Epoch: 5366, Batch Gradient Norm after: 22.360677944193515
Epoch 5367/10000, Prediction Accuracy = 59.342%, Loss = 0.8070010900497436
Epoch: 5367, Batch Gradient Norm: 28.59792217823913
Epoch: 5367, Batch Gradient Norm after: 22.36067862063588
Epoch 5368/10000, Prediction Accuracy = 59.374%, Loss = 0.8094768404960633
Epoch: 5368, Batch Gradient Norm: 27.597552145866178
Epoch: 5368, Batch Gradient Norm after: 22.360677852051083
Epoch 5369/10000, Prediction Accuracy = 59.352%, Loss = 0.8069074153900146
Epoch: 5369, Batch Gradient Norm: 28.59173583107916
Epoch: 5369, Batch Gradient Norm after: 22.360679029198977
Epoch 5370/10000, Prediction Accuracy = 59.362%, Loss = 0.8093741774559021
Epoch: 5370, Batch Gradient Norm: 27.58903598139472
Epoch: 5370, Batch Gradient Norm after: 22.36067871176875
Epoch 5371/10000, Prediction Accuracy = 59.338%, Loss = 0.8067895531654358
Epoch: 5371, Batch Gradient Norm: 28.588630396590528
Epoch: 5371, Batch Gradient Norm after: 22.360679276390297
Epoch 5372/10000, Prediction Accuracy = 59.38199999999999%, Loss = 0.809241247177124
Epoch: 5372, Batch Gradient Norm: 27.581754591259468
Epoch: 5372, Batch Gradient Norm after: 22.360676568473615
Epoch 5373/10000, Prediction Accuracy = 59.342%, Loss = 0.8066402316093445
Epoch: 5373, Batch Gradient Norm: 28.588245145498465
Epoch: 5373, Batch Gradient Norm after: 22.360677095681545
Epoch 5374/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8091296911239624
Epoch: 5374, Batch Gradient Norm: 27.57421254064317
Epoch: 5374, Batch Gradient Norm after: 22.360678512125737
Epoch 5375/10000, Prediction Accuracy = 59.33800000000001%, Loss = 0.8065110325813294
Epoch: 5375, Batch Gradient Norm: 28.58595567218984
Epoch: 5375, Batch Gradient Norm after: 22.360678189643842
Epoch 5376/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8090178489685058
Epoch: 5376, Batch Gradient Norm: 27.568067174130164
Epoch: 5376, Batch Gradient Norm after: 22.360676843648612
Epoch 5377/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.8064003944396972
Epoch: 5377, Batch Gradient Norm: 28.583688228880874
Epoch: 5377, Batch Gradient Norm after: 22.36067859238468
Epoch 5378/10000, Prediction Accuracy = 59.374%, Loss = 0.8089147448539734
Epoch: 5378, Batch Gradient Norm: 27.564854839173297
Epoch: 5378, Batch Gradient Norm after: 22.360678069792655
Epoch 5379/10000, Prediction Accuracy = 59.339999999999996%, Loss = 0.8062996149063111
Epoch: 5379, Batch Gradient Norm: 28.577765973158495
Epoch: 5379, Batch Gradient Norm after: 22.36068095940746
Epoch 5380/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8088043212890625
Epoch: 5380, Batch Gradient Norm: 27.556274890795407
Epoch: 5380, Batch Gradient Norm after: 22.36068026186195
Epoch 5381/10000, Prediction Accuracy = 59.352%, Loss = 0.8061816096305847
Epoch: 5381, Batch Gradient Norm: 28.57443202401657
Epoch: 5381, Batch Gradient Norm after: 22.360680234282277
Epoch 5382/10000, Prediction Accuracy = 59.382000000000005%, Loss = 0.8086832642555237
Epoch: 5382, Batch Gradient Norm: 27.54817514032566
Epoch: 5382, Batch Gradient Norm after: 22.360677295109724
Epoch 5383/10000, Prediction Accuracy = 59.35%, Loss = 0.8060421347618103
Epoch: 5383, Batch Gradient Norm: 28.56253359691093
Epoch: 5383, Batch Gradient Norm after: 22.357774598092444
Epoch 5384/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8085420727729797
Epoch: 5384, Batch Gradient Norm: 27.545795229664076
Epoch: 5384, Batch Gradient Norm after: 22.36067681482981
Epoch 5385/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8059268832206726
Epoch: 5385, Batch Gradient Norm: 28.556304627325023
Epoch: 5385, Batch Gradient Norm after: 22.36067835990922
Epoch 5386/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8084255576133728
Epoch: 5386, Batch Gradient Norm: 27.540638438211822
Epoch: 5386, Batch Gradient Norm after: 22.36067708442011
Epoch 5387/10000, Prediction Accuracy = 59.36%, Loss = 0.805810010433197
Epoch: 5387, Batch Gradient Norm: 28.548207116074863
Epoch: 5387, Batch Gradient Norm after: 22.358915415488124
Epoch 5388/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8083109378814697
Epoch: 5388, Batch Gradient Norm: 27.53530659692965
Epoch: 5388, Batch Gradient Norm after: 22.360679597347104
Epoch 5389/10000, Prediction Accuracy = 59.34599999999999%, Loss = 0.8057146787643432
Epoch: 5389, Batch Gradient Norm: 28.5425342090674
Epoch: 5389, Batch Gradient Norm after: 22.360336644321638
Epoch 5390/10000, Prediction Accuracy = 59.372%, Loss = 0.8082001328468322
Epoch: 5390, Batch Gradient Norm: 27.52902590514672
Epoch: 5390, Batch Gradient Norm after: 22.36067810682813
Epoch 5391/10000, Prediction Accuracy = 59.34799999999999%, Loss = 0.80558842420578
Epoch: 5391, Batch Gradient Norm: 28.529651496219422
Epoch: 5391, Batch Gradient Norm after: 22.357103177665678
Epoch 5392/10000, Prediction Accuracy = 59.36%, Loss = 0.8080492496490479
Epoch: 5392, Batch Gradient Norm: 27.52756859778036
Epoch: 5392, Batch Gradient Norm after: 22.360675894170363
Epoch 5393/10000, Prediction Accuracy = 59.36999999999999%, Loss = 0.8054590225219727
Epoch: 5393, Batch Gradient Norm: 28.52360118852948
Epoch: 5393, Batch Gradient Norm after: 22.36067785965005
Epoch 5394/10000, Prediction Accuracy = 59.366%, Loss = 0.8079211115837097
Epoch: 5394, Batch Gradient Norm: 27.519230982367503
Epoch: 5394, Batch Gradient Norm after: 22.360677835454602
Epoch 5395/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.8053334474563598
Epoch: 5395, Batch Gradient Norm: 28.518527209420014
Epoch: 5395, Batch Gradient Norm after: 22.35965625090871
Epoch 5396/10000, Prediction Accuracy = 59.366%, Loss = 0.8078000903129577
Epoch: 5396, Batch Gradient Norm: 27.514226327736488
Epoch: 5396, Batch Gradient Norm after: 22.360676601594857
Epoch 5397/10000, Prediction Accuracy = 59.362%, Loss = 0.8052208423614502
Epoch: 5397, Batch Gradient Norm: 28.510228893765866
Epoch: 5397, Batch Gradient Norm after: 22.358951546689166
Epoch 5398/10000, Prediction Accuracy = 59.35%, Loss = 0.8076920509338379
Epoch: 5398, Batch Gradient Norm: 27.51068055123486
Epoch: 5398, Batch Gradient Norm after: 22.3606804348843
Epoch 5399/10000, Prediction Accuracy = 59.336%, Loss = 0.8051352262496948
Epoch: 5399, Batch Gradient Norm: 28.504567210815694
Epoch: 5399, Batch Gradient Norm after: 22.359682655411017
Epoch 5400/10000, Prediction Accuracy = 59.372%, Loss = 0.8075778365135193
Epoch: 5400, Batch Gradient Norm: 27.504905343564456
Epoch: 5400, Batch Gradient Norm after: 22.360676544479524
Epoch 5401/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.8050065517425538
Epoch: 5401, Batch Gradient Norm: 28.492228079048626
Epoch: 5401, Batch Gradient Norm after: 22.358591538401043
Epoch 5402/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.8074224352836609
Epoch: 5402, Batch Gradient Norm: 27.49997577218821
Epoch: 5402, Batch Gradient Norm after: 22.360678842063237
Epoch 5403/10000, Prediction Accuracy = 59.391999999999996%, Loss = 0.8048674941062928
Epoch: 5403, Batch Gradient Norm: 28.486603107804708
Epoch: 5403, Batch Gradient Norm after: 22.360677836848733
Epoch 5404/10000, Prediction Accuracy = 59.384%, Loss = 0.807303786277771
Epoch: 5404, Batch Gradient Norm: 27.493023696350466
Epoch: 5404, Batch Gradient Norm after: 22.360679125308636
Epoch 5405/10000, Prediction Accuracy = 59.396%, Loss = 0.8047392845153809
Epoch: 5405, Batch Gradient Norm: 28.469672864001804
Epoch: 5405, Batch Gradient Norm after: 22.355675944715426
Epoch 5406/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8071581959724426
Epoch: 5406, Batch Gradient Norm: 27.495567462692208
Epoch: 5406, Batch Gradient Norm after: 22.360679224757718
Epoch 5407/10000, Prediction Accuracy = 59.364%, Loss = 0.804651939868927
Epoch: 5407, Batch Gradient Norm: 28.458807310975345
Epoch: 5407, Batch Gradient Norm after: 22.36067624798718
Epoch 5408/10000, Prediction Accuracy = 59.362%, Loss = 0.807043194770813
Epoch: 5408, Batch Gradient Norm: 27.488194104510704
Epoch: 5408, Batch Gradient Norm after: 22.360678479393684
Epoch 5409/10000, Prediction Accuracy = 59.354%, Loss = 0.80456383228302
Epoch: 5409, Batch Gradient Norm: 28.455118121624395
Epoch: 5409, Batch Gradient Norm after: 22.360677348659507
Epoch 5410/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.8069293975830079
Epoch: 5410, Batch Gradient Norm: 27.47922395420836
Epoch: 5410, Batch Gradient Norm after: 22.360678430428415
Epoch 5411/10000, Prediction Accuracy = 59.362%, Loss = 0.804420781135559
Epoch: 5411, Batch Gradient Norm: 28.451136444060882
Epoch: 5411, Batch Gradient Norm after: 22.360680547472484
Epoch 5412/10000, Prediction Accuracy = 59.37199999999999%, Loss = 0.806798803806305
Epoch: 5412, Batch Gradient Norm: 27.470861537799372
Epoch: 5412, Batch Gradient Norm after: 22.360679945776262
Epoch 5413/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8042822003364563
Epoch: 5413, Batch Gradient Norm: 28.440477625949548
Epoch: 5413, Batch Gradient Norm after: 22.35709807938425
Epoch 5414/10000, Prediction Accuracy = 59.38599999999999%, Loss = 0.8066668629646301
Epoch: 5414, Batch Gradient Norm: 27.4707219620636
Epoch: 5414, Batch Gradient Norm after: 22.360678686263544
Epoch 5415/10000, Prediction Accuracy = 59.39000000000001%, Loss = 0.8041704177856446
Epoch: 5415, Batch Gradient Norm: 28.433477880997813
Epoch: 5415, Batch Gradient Norm after: 22.360679351905613
Epoch 5416/10000, Prediction Accuracy = 59.342000000000006%, Loss = 0.8065489888191223
Epoch: 5416, Batch Gradient Norm: 27.461148542702478
Epoch: 5416, Batch Gradient Norm after: 22.360679879003584
Epoch 5417/10000, Prediction Accuracy = 59.362%, Loss = 0.8040689349174499
Epoch: 5417, Batch Gradient Norm: 28.431325167526623
Epoch: 5417, Batch Gradient Norm after: 22.360680302002393
Epoch 5418/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.8064512372016907
Epoch: 5418, Batch Gradient Norm: 27.457204721780137
Epoch: 5418, Batch Gradient Norm after: 22.360678312077244
Epoch 5419/10000, Prediction Accuracy = 59.354%, Loss = 0.8039639949798584
Epoch: 5419, Batch Gradient Norm: 28.418881581607565
Epoch: 5419, Batch Gradient Norm after: 22.35770951844657
Epoch 5420/10000, Prediction Accuracy = 59.366%, Loss = 0.8063065767288208
Epoch: 5420, Batch Gradient Norm: 27.453982644655543
Epoch: 5420, Batch Gradient Norm after: 22.36067669696044
Epoch 5421/10000, Prediction Accuracy = 59.376%, Loss = 0.8038242101669312
Epoch: 5421, Batch Gradient Norm: 28.414005137485862
Epoch: 5421, Batch Gradient Norm after: 22.360680061173152
Epoch 5422/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.8061801075935364
Epoch: 5422, Batch Gradient Norm: 27.4456278937821
Epoch: 5422, Batch Gradient Norm after: 22.36067743372625
Epoch 5423/10000, Prediction Accuracy = 59.386%, Loss = 0.8036925554275512
Epoch: 5423, Batch Gradient Norm: 28.402682287686275
Epoch: 5423, Batch Gradient Norm after: 22.356108598034222
Epoch 5424/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.8060418128967285
Epoch: 5424, Batch Gradient Norm: 27.44728958665367
Epoch: 5424, Batch Gradient Norm after: 22.360679852172183
Epoch 5425/10000, Prediction Accuracy = 59.362%, Loss = 0.8035808444023133
Epoch: 5425, Batch Gradient Norm: 28.39812691412285
Epoch: 5425, Batch Gradient Norm after: 22.360678531950256
Epoch 5426/10000, Prediction Accuracy = 59.354000000000006%, Loss = 0.8059269666671753
Epoch: 5426, Batch Gradient Norm: 27.440921229347996
Epoch: 5426, Batch Gradient Norm after: 22.360678560676348
Epoch 5427/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8034847497940063
Epoch: 5427, Batch Gradient Norm: 28.393943603700286
Epoch: 5427, Batch Gradient Norm after: 22.360680221977237
Epoch 5428/10000, Prediction Accuracy = 59.348%, Loss = 0.80583815574646
Epoch: 5428, Batch Gradient Norm: 27.435753700746385
Epoch: 5428, Batch Gradient Norm after: 22.36067856517671
Epoch 5429/10000, Prediction Accuracy = 59.364%, Loss = 0.8033838033676147
Epoch: 5429, Batch Gradient Norm: 28.388793586246436
Epoch: 5429, Batch Gradient Norm after: 22.36067858924994
Epoch 5430/10000, Prediction Accuracy = 59.35799999999999%, Loss = 0.8057058691978455
Epoch: 5430, Batch Gradient Norm: 27.4286424490943
Epoch: 5430, Batch Gradient Norm after: 22.360679865018895
Epoch 5431/10000, Prediction Accuracy = 59.374%, Loss = 0.8032363057136536
Epoch: 5431, Batch Gradient Norm: 28.37613559108819
Epoch: 5431, Batch Gradient Norm after: 22.35701050643928
Epoch 5432/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8055551409721374
Epoch: 5432, Batch Gradient Norm: 27.427582646344998
Epoch: 5432, Batch Gradient Norm after: 22.36067749879522
Epoch 5433/10000, Prediction Accuracy = 59.396%, Loss = 0.8031145572662354
Epoch: 5433, Batch Gradient Norm: 28.371860622993886
Epoch: 5433, Batch Gradient Norm after: 22.36067965338309
Epoch 5434/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.8054409384727478
Epoch: 5434, Batch Gradient Norm: 27.419475593048634
Epoch: 5434, Batch Gradient Norm after: 22.360677697087972
Epoch 5435/10000, Prediction Accuracy = 59.374%, Loss = 0.8029976487159729
Epoch: 5435, Batch Gradient Norm: 28.368179273876702
Epoch: 5435, Batch Gradient Norm after: 22.36067821781634
Epoch 5436/10000, Prediction Accuracy = 59.36%, Loss = 0.8053258299827576
Epoch: 5436, Batch Gradient Norm: 27.415152807512086
Epoch: 5436, Batch Gradient Norm after: 22.360678543964063
Epoch 5437/10000, Prediction Accuracy = 59.366%, Loss = 0.8029114603996277
Epoch: 5437, Batch Gradient Norm: 28.35845482378196
Epoch: 5437, Batch Gradient Norm after: 22.360679167490783
Epoch 5438/10000, Prediction Accuracy = 59.352%, Loss = 0.8052238345146179
Epoch: 5438, Batch Gradient Norm: 27.40877159168254
Epoch: 5438, Batch Gradient Norm after: 22.360676755126818
Epoch 5439/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8028094053268433
Epoch: 5439, Batch Gradient Norm: 28.35042790023413
Epoch: 5439, Batch Gradient Norm after: 22.360677186368545
Epoch 5440/10000, Prediction Accuracy = 59.376%, Loss = 0.8050825834274292
Epoch: 5440, Batch Gradient Norm: 27.404808392987697
Epoch: 5440, Batch Gradient Norm after: 22.36067793086961
Epoch 5441/10000, Prediction Accuracy = 59.378%, Loss = 0.8026619911193847
Epoch: 5441, Batch Gradient Norm: 28.344905251248413
Epoch: 5441, Batch Gradient Norm after: 22.36067800811239
Epoch 5442/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.8049513459205627
Epoch: 5442, Batch Gradient Norm: 27.400945917462955
Epoch: 5442, Batch Gradient Norm after: 22.36067786545544
Epoch 5443/10000, Prediction Accuracy = 59.398%, Loss = 0.8025368690490723
Epoch: 5443, Batch Gradient Norm: 28.337650285629987
Epoch: 5443, Batch Gradient Norm after: 22.360675924015098
Epoch 5444/10000, Prediction Accuracy = 59.4%, Loss = 0.8048381447792053
Epoch: 5444, Batch Gradient Norm: 27.39517916193849
Epoch: 5444, Batch Gradient Norm after: 22.36067731386614
Epoch 5445/10000, Prediction Accuracy = 59.364%, Loss = 0.8024201273918152
Epoch: 5445, Batch Gradient Norm: 28.32717824579936
Epoch: 5445, Batch Gradient Norm after: 22.360678859029377
Epoch 5446/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.8047107815742492
Epoch: 5446, Batch Gradient Norm: 27.392172027190714
Epoch: 5446, Batch Gradient Norm after: 22.36067779654394
Epoch 5447/10000, Prediction Accuracy = 59.354%, Loss = 0.8023367404937745
Epoch: 5447, Batch Gradient Norm: 28.317978107160517
Epoch: 5447, Batch Gradient Norm after: 22.360678870120044
Epoch 5448/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.8045992851257324
Epoch: 5448, Batch Gradient Norm: 27.38669211559831
Epoch: 5448, Batch Gradient Norm after: 22.360677236941747
Epoch 5449/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.8022209405899048
Epoch: 5449, Batch Gradient Norm: 28.30410065142044
Epoch: 5449, Batch Gradient Norm after: 22.360679078271342
Epoch 5450/10000, Prediction Accuracy = 59.384%, Loss = 0.8044471383094788
Epoch: 5450, Batch Gradient Norm: 27.38366737286353
Epoch: 5450, Batch Gradient Norm after: 22.360678005646424
Epoch 5451/10000, Prediction Accuracy = 59.378%, Loss = 0.8020867228507995
Epoch: 5451, Batch Gradient Norm: 28.294972753690914
Epoch: 5451, Batch Gradient Norm after: 22.360679343589105
Epoch 5452/10000, Prediction Accuracy = 59.402%, Loss = 0.8043178915977478
Epoch: 5452, Batch Gradient Norm: 27.378704024659932
Epoch: 5452, Batch Gradient Norm after: 22.360679770924087
Epoch 5453/10000, Prediction Accuracy = 59.412%, Loss = 0.801967442035675
Epoch: 5453, Batch Gradient Norm: 28.28861872006616
Epoch: 5453, Batch Gradient Norm after: 22.36067828649116
Epoch 5454/10000, Prediction Accuracy = 59.4%, Loss = 0.8041974425315856
Epoch: 5454, Batch Gradient Norm: 27.37393994889403
Epoch: 5454, Batch Gradient Norm after: 22.360676471545585
Epoch 5455/10000, Prediction Accuracy = 59.38199999999999%, Loss = 0.8018506288528442
Epoch: 5455, Batch Gradient Norm: 28.281582951406318
Epoch: 5455, Batch Gradient Norm after: 22.36067904676836
Epoch 5456/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.8040882110595703
Epoch: 5456, Batch Gradient Norm: 27.370163242447944
Epoch: 5456, Batch Gradient Norm after: 22.360677848087256
Epoch 5457/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8017637133598328
Epoch: 5457, Batch Gradient Norm: 28.272759444728507
Epoch: 5457, Batch Gradient Norm after: 22.360679708423365
Epoch 5458/10000, Prediction Accuracy = 59.36%, Loss = 0.8039789915084838
Epoch: 5458, Batch Gradient Norm: 27.366018453212803
Epoch: 5458, Batch Gradient Norm after: 22.360675734078388
Epoch 5459/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.8016539096832276
Epoch: 5459, Batch Gradient Norm: 28.26248616991469
Epoch: 5459, Batch Gradient Norm after: 22.360680372159273
Epoch 5460/10000, Prediction Accuracy = 59.4%, Loss = 0.8038323640823364
Epoch: 5460, Batch Gradient Norm: 27.35868365908254
Epoch: 5460, Batch Gradient Norm after: 22.360677774904435
Epoch 5461/10000, Prediction Accuracy = 59.376%, Loss = 0.8015110969543457
Epoch: 5461, Batch Gradient Norm: 28.260410891031107
Epoch: 5461, Batch Gradient Norm after: 22.3606782397095
Epoch 5462/10000, Prediction Accuracy = 59.398%, Loss = 0.803716242313385
Epoch: 5462, Batch Gradient Norm: 27.358193992286477
Epoch: 5462, Batch Gradient Norm after: 22.360678670223443
Epoch 5463/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.8013849258422852
Epoch: 5463, Batch Gradient Norm: 28.246974563823617
Epoch: 5463, Batch Gradient Norm after: 22.360680061430685
Epoch 5464/10000, Prediction Accuracy = 59.396%, Loss = 0.8035835266113281
Epoch: 5464, Batch Gradient Norm: 27.35461041302309
Epoch: 5464, Batch Gradient Norm after: 22.360676028445162
Epoch 5465/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.801282525062561
Epoch: 5465, Batch Gradient Norm: 28.236789785047158
Epoch: 5465, Batch Gradient Norm after: 22.360680529390265
Epoch 5466/10000, Prediction Accuracy = 59.376%, Loss = 0.8034541130065918
Epoch: 5466, Batch Gradient Norm: 27.351757070212233
Epoch: 5466, Batch Gradient Norm after: 22.360676873184197
Epoch 5467/10000, Prediction Accuracy = 59.36%, Loss = 0.8011949539184571
Epoch: 5467, Batch Gradient Norm: 28.222649275477938
Epoch: 5467, Batch Gradient Norm after: 22.360679220291466
Epoch 5468/10000, Prediction Accuracy = 59.374%, Loss = 0.8033335089683533
Epoch: 5468, Batch Gradient Norm: 27.34727397977467
Epoch: 5468, Batch Gradient Norm after: 22.360679348000524
Epoch 5469/10000, Prediction Accuracy = 59.372%, Loss = 0.8010773062705994
Epoch: 5469, Batch Gradient Norm: 28.209600548380706
Epoch: 5469, Batch Gradient Norm after: 22.360678420286256
Epoch 5470/10000, Prediction Accuracy = 59.398%, Loss = 0.8031834244728089
Epoch: 5470, Batch Gradient Norm: 27.342563072683713
Epoch: 5470, Batch Gradient Norm after: 22.360677759333562
Epoch 5471/10000, Prediction Accuracy = 59.406000000000006%, Loss = 0.8009428381919861
Epoch: 5471, Batch Gradient Norm: 28.199418260615378
Epoch: 5471, Batch Gradient Norm after: 22.360679648013875
Epoch 5472/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.8030491709709168
Epoch: 5472, Batch Gradient Norm: 27.33920928545286
Epoch: 5472, Batch Gradient Norm after: 22.360678312638587
Epoch 5473/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.8008319497108459
Epoch: 5473, Batch Gradient Norm: 28.18733665082535
Epoch: 5473, Batch Gradient Norm after: 22.360676774828356
Epoch 5474/10000, Prediction Accuracy = 59.4%, Loss = 0.8029119372367859
Epoch: 5474, Batch Gradient Norm: 27.336111855621656
Epoch: 5474, Batch Gradient Norm after: 22.36067770846899
Epoch 5475/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.8007308125495911
Epoch: 5475, Batch Gradient Norm: 28.175845804155063
Epoch: 5475, Batch Gradient Norm after: 22.360679196748265
Epoch 5476/10000, Prediction Accuracy = 59.384%, Loss = 0.8027915477752685
Epoch: 5476, Batch Gradient Norm: 27.33110278146938
Epoch: 5476, Batch Gradient Norm after: 22.360676470498056
Epoch 5477/10000, Prediction Accuracy = 59.384%, Loss = 0.8006430387496948
Epoch: 5477, Batch Gradient Norm: 28.16392253784573
Epoch: 5477, Batch Gradient Norm after: 22.36067803926648
Epoch 5478/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.802669358253479
Epoch: 5478, Batch Gradient Norm: 27.326520082630875
Epoch: 5478, Batch Gradient Norm after: 22.360676738717576
Epoch 5479/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.8005258321762085
Epoch: 5479, Batch Gradient Norm: 28.152797699595325
Epoch: 5479, Batch Gradient Norm after: 22.36067898946894
Epoch 5480/10000, Prediction Accuracy = 59.4%, Loss = 0.8025199174880981
Epoch: 5480, Batch Gradient Norm: 27.324126561954582
Epoch: 5480, Batch Gradient Norm after: 22.360678395825595
Epoch 5481/10000, Prediction Accuracy = 59.402%, Loss = 0.8003899335861206
Epoch: 5481, Batch Gradient Norm: 28.14837324994497
Epoch: 5481, Batch Gradient Norm after: 22.360678022408738
Epoch 5482/10000, Prediction Accuracy = 59.396%, Loss = 0.8024053812026978
Epoch: 5482, Batch Gradient Norm: 27.31680363728048
Epoch: 5482, Batch Gradient Norm after: 22.36067932752141
Epoch 5483/10000, Prediction Accuracy = 59.424%, Loss = 0.8002716660499573
Epoch: 5483, Batch Gradient Norm: 28.14276711705262
Epoch: 5483, Batch Gradient Norm after: 22.36067749355309
Epoch 5484/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.8022842526435852
Epoch: 5484, Batch Gradient Norm: 27.31256088524747
Epoch: 5484, Batch Gradient Norm after: 22.360678339282256
Epoch 5485/10000, Prediction Accuracy = 59.396%, Loss = 0.8001676678657532
Epoch: 5485, Batch Gradient Norm: 28.136008947976705
Epoch: 5485, Batch Gradient Norm after: 22.360677912850026
Epoch 5486/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8021796584129334
Epoch: 5486, Batch Gradient Norm: 27.306785801873414
Epoch: 5486, Batch Gradient Norm after: 22.36067635331092
Epoch 5487/10000, Prediction Accuracy = 59.395999999999994%, Loss = 0.8000717520713806
Epoch: 5487, Batch Gradient Norm: 28.130661001005276
Epoch: 5487, Batch Gradient Norm after: 22.36067821136526
Epoch 5488/10000, Prediction Accuracy = 59.384%, Loss = 0.8020811319351197
Epoch: 5488, Batch Gradient Norm: 27.298253205708427
Epoch: 5488, Batch Gradient Norm after: 22.360678216141345
Epoch 5489/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.7999550223350524
Epoch: 5489, Batch Gradient Norm: 28.126978406962387
Epoch: 5489, Batch Gradient Norm after: 22.360677050647222
Epoch 5490/10000, Prediction Accuracy = 59.402%, Loss = 0.8019531726837158
Epoch: 5490, Batch Gradient Norm: 27.292761517363058
Epoch: 5490, Batch Gradient Norm after: 22.360677991454022
Epoch 5491/10000, Prediction Accuracy = 59.4%, Loss = 0.7998142719268799
Epoch: 5491, Batch Gradient Norm: 28.122424164461123
Epoch: 5491, Batch Gradient Norm after: 22.36067839379855
Epoch 5492/10000, Prediction Accuracy = 59.396%, Loss = 0.8018436670303345
Epoch: 5492, Batch Gradient Norm: 27.285197374997363
Epoch: 5492, Batch Gradient Norm after: 22.360678178722917
Epoch 5493/10000, Prediction Accuracy = 59.436%, Loss = 0.7996937036514282
Epoch: 5493, Batch Gradient Norm: 28.121130240725286
Epoch: 5493, Batch Gradient Norm after: 22.36067777664913
Epoch 5494/10000, Prediction Accuracy = 59.4%, Loss = 0.8017295837402344
Epoch: 5494, Batch Gradient Norm: 27.280650904666004
Epoch: 5494, Batch Gradient Norm after: 22.360677376339503
Epoch 5495/10000, Prediction Accuracy = 59.408%, Loss = 0.7995891332626343
Epoch: 5495, Batch Gradient Norm: 28.11554235221587
Epoch: 5495, Batch Gradient Norm after: 22.360678975612206
Epoch 5496/10000, Prediction Accuracy = 59.41799999999999%, Loss = 0.8016302704811096
Epoch: 5496, Batch Gradient Norm: 27.272918305795745
Epoch: 5496, Batch Gradient Norm after: 22.360678466024048
Epoch 5497/10000, Prediction Accuracy = 59.39399999999999%, Loss = 0.7995027065277099
Epoch: 5497, Batch Gradient Norm: 28.110267724633307
Epoch: 5497, Batch Gradient Norm after: 22.360678294756557
Epoch 5498/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.8015329122543335
Epoch: 5498, Batch Gradient Norm: 27.267341847078583
Epoch: 5498, Batch Gradient Norm after: 22.360679228604933
Epoch 5499/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7993806958198547
Epoch: 5499, Batch Gradient Norm: 28.10589596779151
Epoch: 5499, Batch Gradient Norm after: 22.36068043316158
Epoch 5500/10000, Prediction Accuracy = 59.412%, Loss = 0.8013984441757203
Epoch: 5500, Batch Gradient Norm: 27.261233129452695
Epoch: 5500, Batch Gradient Norm after: 22.36068125832994
Epoch 5501/10000, Prediction Accuracy = 59.414%, Loss = 0.7992354750633239
Epoch: 5501, Batch Gradient Norm: 28.10400187939195
Epoch: 5501, Batch Gradient Norm after: 22.360677051
Epoch 5502/10000, Prediction Accuracy = 59.39%, Loss = 0.8012954473495484
Epoch: 5502, Batch Gradient Norm: 27.2537000482372
Epoch: 5502, Batch Gradient Norm after: 22.36067977588362
Epoch 5503/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.799110472202301
Epoch: 5503, Batch Gradient Norm: 28.09970100815126
Epoch: 5503, Batch Gradient Norm after: 22.360676935100457
Epoch 5504/10000, Prediction Accuracy = 59.4%, Loss = 0.8011814713478088
Epoch: 5504, Batch Gradient Norm: 27.2493131016822
Epoch: 5504, Batch Gradient Norm after: 22.360679526192346
Epoch 5505/10000, Prediction Accuracy = 59.412%, Loss = 0.7990077853202819
Epoch: 5505, Batch Gradient Norm: 28.094875454364157
Epoch: 5505, Batch Gradient Norm after: 22.36067712640924
Epoch 5506/10000, Prediction Accuracy = 59.42%, Loss = 0.8010814905166626
Epoch: 5506, Batch Gradient Norm: 27.244081128682073
Epoch: 5506, Batch Gradient Norm after: 22.360677162826263
Epoch 5507/10000, Prediction Accuracy = 59.4%, Loss = 0.7989216089248657
Epoch: 5507, Batch Gradient Norm: 28.087292161428017
Epoch: 5507, Batch Gradient Norm after: 22.36067732992471
Epoch 5508/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.8009791731834411
Epoch: 5508, Batch Gradient Norm: 27.23561651207856
Epoch: 5508, Batch Gradient Norm after: 22.360679153300516
Epoch 5509/10000, Prediction Accuracy = 59.4%, Loss = 0.7988049626350403
Epoch: 5509, Batch Gradient Norm: 28.083529200768897
Epoch: 5509, Batch Gradient Norm after: 22.360679501712603
Epoch 5510/10000, Prediction Accuracy = 59.42%, Loss = 0.8008458614349365
Epoch: 5510, Batch Gradient Norm: 27.231170135850054
Epoch: 5510, Batch Gradient Norm after: 22.36067883583982
Epoch 5511/10000, Prediction Accuracy = 59.416%, Loss = 0.79865962266922
Epoch: 5511, Batch Gradient Norm: 28.078554656601238
Epoch: 5511, Batch Gradient Norm after: 22.36067813713097
Epoch 5512/10000, Prediction Accuracy = 59.406000000000006%, Loss = 0.80073162317276
Epoch: 5512, Batch Gradient Norm: 27.222300266486652
Epoch: 5512, Batch Gradient Norm after: 22.36067881452601
Epoch 5513/10000, Prediction Accuracy = 59.45%, Loss = 0.7985404014587403
Epoch: 5513, Batch Gradient Norm: 28.07111440980316
Epoch: 5513, Batch Gradient Norm after: 22.358276810837168
Epoch 5514/10000, Prediction Accuracy = 59.416%, Loss = 0.800607705116272
Epoch: 5514, Batch Gradient Norm: 27.22023838494852
Epoch: 5514, Batch Gradient Norm after: 22.360676206212275
Epoch 5515/10000, Prediction Accuracy = 59.424%, Loss = 0.7984321117401123
Epoch: 5515, Batch Gradient Norm: 28.06613445683945
Epoch: 5515, Batch Gradient Norm after: 22.360678886822807
Epoch 5516/10000, Prediction Accuracy = 59.412%, Loss = 0.8005054831504822
Epoch: 5516, Batch Gradient Norm: 27.21375737132309
Epoch: 5516, Batch Gradient Norm after: 22.360675686835876
Epoch 5517/10000, Prediction Accuracy = 59.402%, Loss = 0.7983467102050781
Epoch: 5517, Batch Gradient Norm: 28.05665912942907
Epoch: 5517, Batch Gradient Norm after: 22.359281471446586
Epoch 5518/10000, Prediction Accuracy = 59.398%, Loss = 0.8003972291946411
Epoch: 5518, Batch Gradient Norm: 27.210162552170296
Epoch: 5518, Batch Gradient Norm after: 22.36067747556253
Epoch 5519/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.7982298374176026
Epoch: 5519, Batch Gradient Norm: 28.047896164370574
Epoch: 5519, Batch Gradient Norm after: 22.360678582637917
Epoch 5520/10000, Prediction Accuracy = 59.418000000000006%, Loss = 0.8002484798431396
Epoch: 5520, Batch Gradient Norm: 27.204158455646187
Epoch: 5520, Batch Gradient Norm after: 22.360678597013635
Epoch 5521/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.798092532157898
Epoch: 5521, Batch Gradient Norm: 28.04281295530087
Epoch: 5521, Batch Gradient Norm after: 22.359869275273834
Epoch 5522/10000, Prediction Accuracy = 59.412%, Loss = 0.8001367092132569
Epoch: 5522, Batch Gradient Norm: 27.196889988657308
Epoch: 5522, Batch Gradient Norm after: 22.360676915628098
Epoch 5523/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.797974419593811
Epoch: 5523, Batch Gradient Norm: 28.03779747709707
Epoch: 5523, Batch Gradient Norm after: 22.360457324194595
Epoch 5524/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.8000189661979675
Epoch: 5524, Batch Gradient Norm: 27.193711338598217
Epoch: 5524, Batch Gradient Norm after: 22.360678108483857
Epoch 5525/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7978664994239807
Epoch: 5525, Batch Gradient Norm: 28.026843816628226
Epoch: 5525, Batch Gradient Norm after: 22.35925122701715
Epoch 5526/10000, Prediction Accuracy = 59.416%, Loss = 0.7999071598052978
Epoch: 5526, Batch Gradient Norm: 27.188941182113325
Epoch: 5526, Batch Gradient Norm after: 22.360678937765613
Epoch 5527/10000, Prediction Accuracy = 59.42%, Loss = 0.7977864146232605
Epoch: 5527, Batch Gradient Norm: 28.019158964278297
Epoch: 5527, Batch Gradient Norm after: 22.359494360045936
Epoch 5528/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.7998059749603271
Epoch: 5528, Batch Gradient Norm: 27.183494138526292
Epoch: 5528, Batch Gradient Norm after: 22.360676612041292
Epoch 5529/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.7976686954498291
Epoch: 5529, Batch Gradient Norm: 28.00591397849895
Epoch: 5529, Batch Gradient Norm after: 22.35794781460025
Epoch 5530/10000, Prediction Accuracy = 59.416%, Loss = 0.7996494174003601
Epoch: 5530, Batch Gradient Norm: 27.180018951922893
Epoch: 5530, Batch Gradient Norm after: 22.360678041203123
Epoch 5531/10000, Prediction Accuracy = 59.428%, Loss = 0.7975300073623657
Epoch: 5531, Batch Gradient Norm: 27.998342032357325
Epoch: 5531, Batch Gradient Norm after: 22.35853775853172
Epoch 5532/10000, Prediction Accuracy = 59.414%, Loss = 0.7995305061340332
Epoch: 5532, Batch Gradient Norm: 27.173324899145186
Epoch: 5532, Batch Gradient Norm after: 22.360676448598984
Epoch 5533/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7974169731140137
Epoch: 5533, Batch Gradient Norm: 27.990324579243907
Epoch: 5533, Batch Gradient Norm after: 22.35649963465112
Epoch 5534/10000, Prediction Accuracy = 59.412%, Loss = 0.7993976593017578
Epoch: 5534, Batch Gradient Norm: 27.172510266967286
Epoch: 5534, Batch Gradient Norm after: 22.36067673925189
Epoch 5535/10000, Prediction Accuracy = 59.424%, Loss = 0.7973106384277344
Epoch: 5535, Batch Gradient Norm: 27.990478522150685
Epoch: 5535, Batch Gradient Norm after: 22.360679127406105
Epoch 5536/10000, Prediction Accuracy = 59.416%, Loss = 0.7993105888366699
Epoch: 5536, Batch Gradient Norm: 27.16359189819222
Epoch: 5536, Batch Gradient Norm after: 22.360678123435534
Epoch 5537/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.7972247719764709
Epoch: 5537, Batch Gradient Norm: 27.973294625762556
Epoch: 5537, Batch Gradient Norm after: 22.354782963539506
Epoch 5538/10000, Prediction Accuracy = 59.416%, Loss = 0.7991813540458679
Epoch: 5538, Batch Gradient Norm: 27.15995711873614
Epoch: 5538, Batch Gradient Norm after: 22.36067964032964
Epoch 5539/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.7971086382865906
Epoch: 5539, Batch Gradient Norm: 27.971293846249704
Epoch: 5539, Batch Gradient Norm after: 22.360678863989964
Epoch 5540/10000, Prediction Accuracy = 59.438%, Loss = 0.7990558028221131
Epoch: 5540, Batch Gradient Norm: 27.153266054918692
Epoch: 5540, Batch Gradient Norm after: 22.360678248679534
Epoch 5541/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7969661951065063
Epoch: 5541, Batch Gradient Norm: 27.95930134462231
Epoch: 5541, Batch Gradient Norm after: 22.35609316133392
Epoch 5542/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.798927116394043
Epoch: 5542, Batch Gradient Norm: 27.152420338969563
Epoch: 5542, Batch Gradient Norm after: 22.360677401120522
Epoch 5543/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.7968565702438355
Epoch: 5543, Batch Gradient Norm: 27.958406216918
Epoch: 5543, Batch Gradient Norm after: 22.360558213530922
Epoch 5544/10000, Prediction Accuracy = 59.432%, Loss = 0.7988237738609314
Epoch: 5544, Batch Gradient Norm: 27.14171522494371
Epoch: 5544, Batch Gradient Norm after: 22.360677857399242
Epoch 5545/10000, Prediction Accuracy = 59.44%, Loss = 0.7967433214187623
Epoch: 5545, Batch Gradient Norm: 27.936681733742958
Epoch: 5545, Batch Gradient Norm after: 22.3504449864703
Epoch 5546/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7986809968948364
Epoch: 5546, Batch Gradient Norm: 27.146962504523728
Epoch: 5546, Batch Gradient Norm after: 22.36067823079746
Epoch 5547/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7966733694076538
Epoch: 5547, Batch Gradient Norm: 27.929981531958838
Epoch: 5547, Batch Gradient Norm after: 22.360678705290816
Epoch 5548/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.7985763788223267
Epoch: 5548, Batch Gradient Norm: 27.138917632585613
Epoch: 5548, Batch Gradient Norm after: 22.360677578564026
Epoch 5549/10000, Prediction Accuracy = 59.45%, Loss = 0.796546459197998
Epoch: 5549, Batch Gradient Norm: 27.931721260656378
Epoch: 5549, Batch Gradient Norm after: 22.360678064316428
Epoch 5550/10000, Prediction Accuracy = 59.43399999999999%, Loss = 0.798459780216217
Epoch: 5550, Batch Gradient Norm: 27.13048615645908
Epoch: 5550, Batch Gradient Norm after: 22.36068014727017
Epoch 5551/10000, Prediction Accuracy = 59.42%, Loss = 0.7964045405387878
Epoch: 5551, Batch Gradient Norm: 27.925226684817744
Epoch: 5551, Batch Gradient Norm after: 22.3581641848638
Epoch 5552/10000, Prediction Accuracy = 59.428%, Loss = 0.7983447194099427
Epoch: 5552, Batch Gradient Norm: 27.125991817522706
Epoch: 5552, Batch Gradient Norm after: 22.36067852073947
Epoch 5553/10000, Prediction Accuracy = 59.428%, Loss = 0.7962911128997803
Epoch: 5553, Batch Gradient Norm: 27.91781759422303
Epoch: 5553, Batch Gradient Norm after: 22.357030661122945
Epoch 5554/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7982234358787537
Epoch: 5554, Batch Gradient Norm: 27.12133426269121
Epoch: 5554, Batch Gradient Norm after: 22.360679434884887
Epoch 5555/10000, Prediction Accuracy = 59.443999999999996%, Loss = 0.7961869716644288
Epoch: 5555, Batch Gradient Norm: 27.910175879912543
Epoch: 5555, Batch Gradient Norm after: 22.356894273514456
Epoch 5556/10000, Prediction Accuracy = 59.407999999999994%, Loss = 0.798120665550232
Epoch: 5556, Batch Gradient Norm: 27.11654736069404
Epoch: 5556, Batch Gradient Norm after: 22.36067772820597
Epoch 5557/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.7961065173149109
Epoch: 5557, Batch Gradient Norm: 27.903280155456354
Epoch: 5557, Batch Gradient Norm after: 22.356755055511226
Epoch 5558/10000, Prediction Accuracy = 59.402%, Loss = 0.7980120658874512
Epoch: 5558, Batch Gradient Norm: 27.108030501984345
Epoch: 5558, Batch Gradient Norm after: 22.36067741252706
Epoch 5559/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7959796905517578
Epoch: 5559, Batch Gradient Norm: 27.89174900413477
Epoch: 5559, Batch Gradient Norm after: 22.356230533347567
Epoch 5560/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7978651642799377
Epoch: 5560, Batch Gradient Norm: 27.107767998256808
Epoch: 5560, Batch Gradient Norm after: 22.36067877020773
Epoch 5561/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7958498597145081
Epoch: 5561, Batch Gradient Norm: 27.8843954921344
Epoch: 5561, Batch Gradient Norm after: 22.3565849496851
Epoch 5562/10000, Prediction Accuracy = 59.43599999999999%, Loss = 0.797749149799347
Epoch: 5562, Batch Gradient Norm: 27.104871036136394
Epoch: 5562, Batch Gradient Norm after: 22.360677639477707
Epoch 5563/10000, Prediction Accuracy = 59.444%, Loss = 0.7957338571548462
Epoch: 5563, Batch Gradient Norm: 27.876926242590734
Epoch: 5563, Batch Gradient Norm after: 22.35674276700759
Epoch 5564/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7976333022117614
Epoch: 5564, Batch Gradient Norm: 27.09937315810841
Epoch: 5564, Batch Gradient Norm after: 22.360680787297866
Epoch 5565/10000, Prediction Accuracy = 59.48%, Loss = 0.7956331253051758
Epoch: 5565, Batch Gradient Norm: 27.87244145170234
Epoch: 5565, Batch Gradient Norm after: 22.35798091847038
Epoch 5566/10000, Prediction Accuracy = 59.416%, Loss = 0.7975345849990845
Epoch: 5566, Batch Gradient Norm: 27.092400005125967
Epoch: 5566, Batch Gradient Norm after: 22.36067705311917
Epoch 5567/10000, Prediction Accuracy = 59.428%, Loss = 0.7955451607704163
Epoch: 5567, Batch Gradient Norm: 27.862300615166774
Epoch: 5567, Batch Gradient Norm after: 22.35629452605231
Epoch 5568/10000, Prediction Accuracy = 59.39200000000001%, Loss = 0.7974226951599122
Epoch: 5568, Batch Gradient Norm: 27.08514535998157
Epoch: 5568, Batch Gradient Norm after: 22.360676143685467
Epoch 5569/10000, Prediction Accuracy = 59.452%, Loss = 0.795430064201355
Epoch: 5569, Batch Gradient Norm: 27.849817948428445
Epoch: 5569, Batch Gradient Norm after: 22.353215727166283
Epoch 5570/10000, Prediction Accuracy = 59.456%, Loss = 0.7972638845443726
Epoch: 5570, Batch Gradient Norm: 27.086506908589854
Epoch: 5570, Batch Gradient Norm after: 22.36067679687487
Epoch 5571/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7953025341033936
Epoch: 5571, Batch Gradient Norm: 27.85148052890268
Epoch: 5571, Batch Gradient Norm after: 22.36018766027183
Epoch 5572/10000, Prediction Accuracy = 59.452%, Loss = 0.797173273563385
Epoch: 5572, Batch Gradient Norm: 27.072753754198512
Epoch: 5572, Batch Gradient Norm after: 22.360677658823615
Epoch 5573/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7951783299446106
Epoch: 5573, Batch Gradient Norm: 27.829127757759956
Epoch: 5573, Batch Gradient Norm after: 22.348714791049744
Epoch 5574/10000, Prediction Accuracy = 59.45%, Loss = 0.7970084547996521
Epoch: 5574, Batch Gradient Norm: 27.079341279126425
Epoch: 5574, Batch Gradient Norm after: 22.36067727346422
Epoch 5575/10000, Prediction Accuracy = 59.496%, Loss = 0.7950935363769531
Epoch: 5575, Batch Gradient Norm: 27.83323996612043
Epoch: 5575, Batch Gradient Norm after: 22.360677570880785
Epoch 5576/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7969366550445557
Epoch: 5576, Batch Gradient Norm: 27.066864394264865
Epoch: 5576, Batch Gradient Norm after: 22.36067722412993
Epoch 5577/10000, Prediction Accuracy = 59.428%, Loss = 0.794997763633728
Epoch: 5577, Batch Gradient Norm: 27.82010682024125
Epoch: 5577, Batch Gradient Norm after: 22.352936944156927
Epoch 5578/10000, Prediction Accuracy = 59.396%, Loss = 0.7968197345733643
Epoch: 5578, Batch Gradient Norm: 27.065807006704812
Epoch: 5578, Batch Gradient Norm after: 22.360677504444837
Epoch 5579/10000, Prediction Accuracy = 59.448%, Loss = 0.7948825120925903
Epoch: 5579, Batch Gradient Norm: 27.82315591605499
Epoch: 5579, Batch Gradient Norm after: 22.360355418703335
Epoch 5580/10000, Prediction Accuracy = 59.464%, Loss = 0.7967071533203125
Epoch: 5580, Batch Gradient Norm: 27.05390506779741
Epoch: 5580, Batch Gradient Norm after: 22.360677108455345
Epoch 5581/10000, Prediction Accuracy = 59.436%, Loss = 0.7947370529174804
Epoch: 5581, Batch Gradient Norm: 27.795436870388713
Epoch: 5581, Batch Gradient Norm after: 22.346918839459512
Epoch 5582/10000, Prediction Accuracy = 59.462%, Loss = 0.7965365409851074
Epoch: 5582, Batch Gradient Norm: 27.06093144296368
Epoch: 5582, Batch Gradient Norm after: 22.360678880154406
Epoch 5583/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.7946534752845764
Epoch: 5583, Batch Gradient Norm: 27.79692962415872
Epoch: 5583, Batch Gradient Norm after: 22.360678593266254
Epoch 5584/10000, Prediction Accuracy = 59.448%, Loss = 0.7964408159255981
Epoch: 5584, Batch Gradient Norm: 27.04676796605831
Epoch: 5584, Batch Gradient Norm after: 22.360678517391673
Epoch 5585/10000, Prediction Accuracy = 59.48%, Loss = 0.7945313453674316
Epoch: 5585, Batch Gradient Norm: 27.795768462538778
Epoch: 5585, Batch Gradient Norm after: 22.355070884486405
Epoch 5586/10000, Prediction Accuracy = 59.448%, Loss = 0.7963465452194214
Epoch: 5586, Batch Gradient Norm: 27.04465003517797
Epoch: 5586, Batch Gradient Norm after: 22.36067669179139
Epoch 5587/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.7944477796554565
Epoch: 5587, Batch Gradient Norm: 27.79119998648822
Epoch: 5587, Batch Gradient Norm after: 22.357420225457435
Epoch 5588/10000, Prediction Accuracy = 59.407999999999994%, Loss = 0.7962519884109497
Epoch: 5588, Batch Gradient Norm: 27.039772701138126
Epoch: 5588, Batch Gradient Norm after: 22.360675675259785
Epoch 5589/10000, Prediction Accuracy = 59.428%, Loss = 0.7943296790122986
Epoch: 5589, Batch Gradient Norm: 27.78446493941029
Epoch: 5589, Batch Gradient Norm after: 22.35698315051475
Epoch 5590/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7961163282394409
Epoch: 5590, Batch Gradient Norm: 27.036095296050597
Epoch: 5590, Batch Gradient Norm after: 22.360678118699592
Epoch 5591/10000, Prediction Accuracy = 59.428%, Loss = 0.7942020416259765
Epoch: 5591, Batch Gradient Norm: 27.7716955098547
Epoch: 5591, Batch Gradient Norm after: 22.354733557754194
Epoch 5592/10000, Prediction Accuracy = 59.45400000000001%, Loss = 0.7959801077842712
Epoch: 5592, Batch Gradient Norm: 27.034653459262014
Epoch: 5592, Batch Gradient Norm after: 22.360676450668898
Epoch 5593/10000, Prediction Accuracy = 59.436%, Loss = 0.7940956711769104
Epoch: 5593, Batch Gradient Norm: 27.77186837271075
Epoch: 5593, Batch Gradient Norm after: 22.35907127309857
Epoch 5594/10000, Prediction Accuracy = 59.45%, Loss = 0.7958816528320313
Epoch: 5594, Batch Gradient Norm: 27.02792502815283
Epoch: 5594, Batch Gradient Norm after: 22.360678073741045
Epoch 5595/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7939836263656617
Epoch: 5595, Batch Gradient Norm: 27.760878607411186
Epoch: 5595, Batch Gradient Norm after: 22.35475657116109
Epoch 5596/10000, Prediction Accuracy = 59.44%, Loss = 0.7957574844360351
Epoch: 5596, Batch Gradient Norm: 27.027043406398896
Epoch: 5596, Batch Gradient Norm after: 22.360677080756297
Epoch 5597/10000, Prediction Accuracy = 59.428%, Loss = 0.7939040064811707
Epoch: 5597, Batch Gradient Norm: 27.762675998270442
Epoch: 5597, Batch Gradient Norm after: 22.360679560245515
Epoch 5598/10000, Prediction Accuracy = 59.412%, Loss = 0.7956789970397949
Epoch: 5598, Batch Gradient Norm: 27.017244382043845
Epoch: 5598, Batch Gradient Norm after: 22.360678519036014
Epoch 5599/10000, Prediction Accuracy = 59.424%, Loss = 0.7937842726707458
Epoch: 5599, Batch Gradient Norm: 27.74222943409209
Epoch: 5599, Batch Gradient Norm after: 22.35273970062582
Epoch 5600/10000, Prediction Accuracy = 59.462%, Loss = 0.7955115437507629
Epoch: 5600, Batch Gradient Norm: 27.021092865904883
Epoch: 5600, Batch Gradient Norm after: 22.360677401953197
Epoch 5601/10000, Prediction Accuracy = 59.432%, Loss = 0.7936646103858948
Epoch: 5601, Batch Gradient Norm: 27.743106760911708
Epoch: 5601, Batch Gradient Norm after: 22.36067839275048
Epoch 5602/10000, Prediction Accuracy = 59.448%, Loss = 0.7954132437705994
Epoch: 5602, Batch Gradient Norm: 27.011949622878937
Epoch: 5602, Batch Gradient Norm after: 22.360676528044685
Epoch 5603/10000, Prediction Accuracy = 59.432%, Loss = 0.7935448288917542
Epoch: 5603, Batch Gradient Norm: 27.729217514025617
Epoch: 5603, Batch Gradient Norm after: 22.353912953046997
Epoch 5604/10000, Prediction Accuracy = 59.45%, Loss = 0.7952852368354797
Epoch: 5604, Batch Gradient Norm: 27.01263157857989
Epoch: 5604, Batch Gradient Norm after: 22.36067916663527
Epoch 5605/10000, Prediction Accuracy = 59.462%, Loss = 0.7934439659118653
Epoch: 5605, Batch Gradient Norm: 27.730852709942024
Epoch: 5605, Batch Gradient Norm after: 22.36067708370931
Epoch 5606/10000, Prediction Accuracy = 59.436%, Loss = 0.7951946020126343
Epoch: 5606, Batch Gradient Norm: 27.000270975766924
Epoch: 5606, Batch Gradient Norm after: 22.360678939279875
Epoch 5607/10000, Prediction Accuracy = 59.444%, Loss = 0.7933547854423523
Epoch: 5607, Batch Gradient Norm: 27.720642735223397
Epoch: 5607, Batch Gradient Norm after: 22.355352360843845
Epoch 5608/10000, Prediction Accuracy = 59.42199999999999%, Loss = 0.795091199874878
Epoch: 5608, Batch Gradient Norm: 26.999157395569792
Epoch: 5608, Batch Gradient Norm after: 22.360679079678974
Epoch 5609/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7932620882987976
Epoch: 5609, Batch Gradient Norm: 27.711424851408527
Epoch: 5609, Batch Gradient Norm after: 22.358589215085814
Epoch 5610/10000, Prediction Accuracy = 59.432%, Loss = 0.7949613213539124
Epoch: 5610, Batch Gradient Norm: 26.99321851235366
Epoch: 5610, Batch Gradient Norm after: 22.360679608066473
Epoch 5611/10000, Prediction Accuracy = 59.456%, Loss = 0.7931219935417175
Epoch: 5611, Batch Gradient Norm: 27.70187275208264
Epoch: 5611, Batch Gradient Norm after: 22.35477539436962
Epoch 5612/10000, Prediction Accuracy = 59.44599999999999%, Loss = 0.7948222994804383
Epoch: 5612, Batch Gradient Norm: 26.99350459791108
Epoch: 5612, Batch Gradient Norm after: 22.360680669624923
Epoch 5613/10000, Prediction Accuracy = 59.44000000000001%, Loss = 0.793017303943634
Epoch: 5613, Batch Gradient Norm: 27.700636891792346
Epoch: 5613, Batch Gradient Norm after: 22.36067901154262
Epoch 5614/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7947229623794556
Epoch: 5614, Batch Gradient Norm: 26.98420187216668
Epoch: 5614, Batch Gradient Norm after: 22.360679137088656
Epoch 5615/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7928974986076355
Epoch: 5615, Batch Gradient Norm: 27.687277884566093
Epoch: 5615, Batch Gradient Norm after: 22.35449391420315
Epoch 5616/10000, Prediction Accuracy = 59.43399999999999%, Loss = 0.7945972561836243
Epoch: 5616, Batch Gradient Norm: 26.986722827181037
Epoch: 5616, Batch Gradient Norm after: 22.36067856492752
Epoch 5617/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.792826235294342
Epoch: 5617, Batch Gradient Norm: 27.68709381353573
Epoch: 5617, Batch Gradient Norm after: 22.36067840480522
Epoch 5618/10000, Prediction Accuracy = 59.43599999999999%, Loss = 0.7945084691047668
Epoch: 5618, Batch Gradient Norm: 26.977454067562867
Epoch: 5618, Batch Gradient Norm after: 22.36067803003398
Epoch 5619/10000, Prediction Accuracy = 59.426%, Loss = 0.792720901966095
Epoch: 5619, Batch Gradient Norm: 27.68277769883123
Epoch: 5619, Batch Gradient Norm after: 22.359124991810326
Epoch 5620/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7943927645683289
Epoch: 5620, Batch Gradient Norm: 26.971546694480097
Epoch: 5620, Batch Gradient Norm after: 22.360679468829964
Epoch 5621/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7925836205482483
Epoch: 5621, Batch Gradient Norm: 27.671334536816726
Epoch: 5621, Batch Gradient Norm after: 22.355852781445698
Epoch 5622/10000, Prediction Accuracy = 59.462%, Loss = 0.7942538022994995
Epoch: 5622, Batch Gradient Norm: 26.969818839419798
Epoch: 5622, Batch Gradient Norm after: 22.36067931607063
Epoch 5623/10000, Prediction Accuracy = 59.45%, Loss = 0.7924795746803284
Epoch: 5623, Batch Gradient Norm: 27.668342055057728
Epoch: 5623, Batch Gradient Norm after: 22.359756459193772
Epoch 5624/10000, Prediction Accuracy = 59.456%, Loss = 0.7941552758216858
Epoch: 5624, Batch Gradient Norm: 26.963222104767326
Epoch: 5624, Batch Gradient Norm after: 22.36067779492493
Epoch 5625/10000, Prediction Accuracy = 59.482000000000006%, Loss = 0.7923643946647644
Epoch: 5625, Batch Gradient Norm: 27.660700865011414
Epoch: 5625, Batch Gradient Norm after: 22.35650155225112
Epoch 5626/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7940346956253052
Epoch: 5626, Batch Gradient Norm: 26.961460337739364
Epoch: 5626, Batch Gradient Norm after: 22.36067750098189
Epoch 5627/10000, Prediction Accuracy = 59.438%, Loss = 0.7922899007797242
Epoch: 5627, Batch Gradient Norm: 27.658343738437548
Epoch: 5627, Batch Gradient Norm after: 22.360678646177693
Epoch 5628/10000, Prediction Accuracy = 59.43599999999999%, Loss = 0.793954336643219
Epoch: 5628, Batch Gradient Norm: 26.952129267854
Epoch: 5628, Batch Gradient Norm after: 22.36067652138606
Epoch 5629/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7921872735023499
Epoch: 5629, Batch Gradient Norm: 27.643940373132175
Epoch: 5629, Batch Gradient Norm after: 22.356659336383636
Epoch 5630/10000, Prediction Accuracy = 59.43800000000001%, Loss = 0.7938060760498047
Epoch: 5630, Batch Gradient Norm: 26.95150295641427
Epoch: 5630, Batch Gradient Norm after: 22.360679662416228
Epoch 5631/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.7920539379119873
Epoch: 5631, Batch Gradient Norm: 27.639662019835885
Epoch: 5631, Batch Gradient Norm after: 22.360125958301317
Epoch 5632/10000, Prediction Accuracy = 59.464%, Loss = 0.7936995148658752
Epoch: 5632, Batch Gradient Norm: 26.944231607109174
Epoch: 5632, Batch Gradient Norm after: 22.36067921138318
Epoch 5633/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7919400691986084
Epoch: 5633, Batch Gradient Norm: 27.625312170642857
Epoch: 5633, Batch Gradient Norm after: 22.355139869928976
Epoch 5634/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7935651183128357
Epoch: 5634, Batch Gradient Norm: 26.94555873231062
Epoch: 5634, Batch Gradient Norm after: 22.360675999477305
Epoch 5635/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.7918378591537476
Epoch: 5635, Batch Gradient Norm: 27.62104648588661
Epoch: 5635, Batch Gradient Norm after: 22.360679361519882
Epoch 5636/10000, Prediction Accuracy = 59.444%, Loss = 0.7934700608253479
Epoch: 5636, Batch Gradient Norm: 26.93929624368919
Epoch: 5636, Batch Gradient Norm after: 22.360677939191174
Epoch 5637/10000, Prediction Accuracy = 59.42999999999999%, Loss = 0.7917512774467468
Epoch: 5637, Batch Gradient Norm: 27.617714453074374
Epoch: 5637, Batch Gradient Norm after: 22.359849193584544
Epoch 5638/10000, Prediction Accuracy = 59.44%, Loss = 0.7933874726295471
Epoch: 5638, Batch Gradient Norm: 26.932583355163047
Epoch: 5638, Batch Gradient Norm after: 22.36067946171979
Epoch 5639/10000, Prediction Accuracy = 59.448%, Loss = 0.7916551947593689
Epoch: 5639, Batch Gradient Norm: 27.603818327650455
Epoch: 5639, Batch Gradient Norm after: 22.356746325348084
Epoch 5640/10000, Prediction Accuracy = 59.44000000000001%, Loss = 0.7932355999946594
Epoch: 5640, Batch Gradient Norm: 26.931955450206868
Epoch: 5640, Batch Gradient Norm after: 22.360680018954284
Epoch 5641/10000, Prediction Accuracy = 59.462%, Loss = 0.7915201663970948
Epoch: 5641, Batch Gradient Norm: 27.60237072995173
Epoch: 5641, Batch Gradient Norm after: 22.36067776835333
Epoch 5642/10000, Prediction Accuracy = 59.488%, Loss = 0.7931249737739563
Epoch: 5642, Batch Gradient Norm: 26.923589123863167
Epoch: 5642, Batch Gradient Norm after: 22.360676809007746
Epoch 5643/10000, Prediction Accuracy = 59.452%, Loss = 0.791402792930603
Epoch: 5643, Batch Gradient Norm: 27.585694730203286
Epoch: 5643, Batch Gradient Norm after: 22.354219173806477
Epoch 5644/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7929878115653992
Epoch: 5644, Batch Gradient Norm: 26.92625697970143
Epoch: 5644, Batch Gradient Norm after: 22.360679556411984
Epoch 5645/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.7913014531135559
Epoch: 5645, Batch Gradient Norm: 27.582100989414627
Epoch: 5645, Batch Gradient Norm after: 22.360676430829447
Epoch 5646/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.7928860664367676
Epoch: 5646, Batch Gradient Norm: 26.919861533011677
Epoch: 5646, Batch Gradient Norm after: 22.360677601319022
Epoch 5647/10000, Prediction Accuracy = 59.452%, Loss = 0.7912086129188538
Epoch: 5647, Batch Gradient Norm: 27.57738917373006
Epoch: 5647, Batch Gradient Norm after: 22.35834454161604
Epoch 5648/10000, Prediction Accuracy = 59.428%, Loss = 0.7927982807159424
Epoch: 5648, Batch Gradient Norm: 26.912885586652752
Epoch: 5648, Batch Gradient Norm after: 22.360677098078288
Epoch 5649/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.7911259055137634
Epoch: 5649, Batch Gradient Norm: 27.571884042531472
Epoch: 5649, Batch Gradient Norm after: 22.360670281841454
Epoch 5650/10000, Prediction Accuracy = 59.436%, Loss = 0.7926745176315307
Epoch: 5650, Batch Gradient Norm: 26.905551859450608
Epoch: 5650, Batch Gradient Norm after: 22.360679150316678
Epoch 5651/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.7909839391708374
Epoch: 5651, Batch Gradient Norm: 27.560355519305485
Epoch: 5651, Batch Gradient Norm after: 22.355993622406608
Epoch 5652/10000, Prediction Accuracy = 59.484%, Loss = 0.7925318360328675
Epoch: 5652, Batch Gradient Norm: 26.907620001927103
Epoch: 5652, Batch Gradient Norm after: 22.360677859353693
Epoch 5653/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.7908715724945068
Epoch: 5653, Batch Gradient Norm: 27.55799224220632
Epoch: 5653, Batch Gradient Norm after: 22.360677903569314
Epoch 5654/10000, Prediction Accuracy = 59.484%, Loss = 0.7924328804016113
Epoch: 5654, Batch Gradient Norm: 26.898839821287535
Epoch: 5654, Batch Gradient Norm after: 22.36067821075654
Epoch 5655/10000, Prediction Accuracy = 59.476%, Loss = 0.7907544851303101
Epoch: 5655, Batch Gradient Norm: 27.54850076550268
Epoch: 5655, Batch Gradient Norm after: 22.35576178814695
Epoch 5656/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7923065543174743
Epoch: 5656, Batch Gradient Norm: 26.898758861023197
Epoch: 5656, Batch Gradient Norm after: 22.360678229300092
Epoch 5657/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.7906712889671326
Epoch: 5657, Batch Gradient Norm: 27.547030468555736
Epoch: 5657, Batch Gradient Norm after: 22.360678090733035
Epoch 5658/10000, Prediction Accuracy = 59.448%, Loss = 0.7922292351722717
Epoch: 5658, Batch Gradient Norm: 26.88926421920881
Epoch: 5658, Batch Gradient Norm after: 22.360676769035713
Epoch 5659/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7905794501304626
Epoch: 5659, Batch Gradient Norm: 27.53384018067135
Epoch: 5659, Batch Gradient Norm after: 22.356390257074658
Epoch 5660/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.792099392414093
Epoch: 5660, Batch Gradient Norm: 26.889862349383044
Epoch: 5660, Batch Gradient Norm after: 22.360679600262614
Epoch 5661/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7904534459114074
Epoch: 5661, Batch Gradient Norm: 27.530155455271967
Epoch: 5661, Batch Gradient Norm after: 22.36067740580877
Epoch 5662/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7919812560081482
Epoch: 5662, Batch Gradient Norm: 26.88138755229324
Epoch: 5662, Batch Gradient Norm after: 22.3606773397223
Epoch 5663/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7903319358825683
Epoch: 5663, Batch Gradient Norm: 27.516063507895073
Epoch: 5663, Batch Gradient Norm after: 22.355981103468356
Epoch 5664/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7918573141098022
Epoch: 5664, Batch Gradient Norm: 26.882972011020208
Epoch: 5664, Batch Gradient Norm after: 22.360678039795065
Epoch 5665/10000, Prediction Accuracy = 59.462%, Loss = 0.7902312874794006
Epoch: 5665, Batch Gradient Norm: 27.51294683659735
Epoch: 5665, Batch Gradient Norm after: 22.36067732180355
Epoch 5666/10000, Prediction Accuracy = 59.468%, Loss = 0.7917504906654358
Epoch: 5666, Batch Gradient Norm: 26.87619150414094
Epoch: 5666, Batch Gradient Norm after: 22.36067904081523
Epoch 5667/10000, Prediction Accuracy = 59.452%, Loss = 0.7901283621788024
Epoch: 5667, Batch Gradient Norm: 27.51478185179386
Epoch: 5667, Batch Gradient Norm after: 22.360677743855213
Epoch 5668/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.7916714549064636
Epoch: 5668, Batch Gradient Norm: 26.868509953752
Epoch: 5668, Batch Gradient Norm after: 22.360679339437084
Epoch 5669/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7900528311729431
Epoch: 5669, Batch Gradient Norm: 27.49610217010508
Epoch: 5669, Batch Gradient Norm after: 22.354966415761396
Epoch 5670/10000, Prediction Accuracy = 59.428%, Loss = 0.7915374875068665
Epoch: 5670, Batch Gradient Norm: 26.86728393352371
Epoch: 5670, Batch Gradient Norm after: 22.360680061415554
Epoch 5671/10000, Prediction Accuracy = 59.46%, Loss = 0.7899351477622986
Epoch: 5671, Batch Gradient Norm: 27.49806495863793
Epoch: 5671, Batch Gradient Norm after: 22.36067719020894
Epoch 5672/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.7914191365242005
Epoch: 5672, Batch Gradient Norm: 26.859186591972886
Epoch: 5672, Batch Gradient Norm after: 22.36067493193509
Epoch 5673/10000, Prediction Accuracy = 59.448%, Loss = 0.7897978067398072
Epoch: 5673, Batch Gradient Norm: 27.480220420921814
Epoch: 5673, Batch Gradient Norm after: 22.352054559942122
Epoch 5674/10000, Prediction Accuracy = 59.47800000000001%, Loss = 0.7912836074829102
Epoch: 5674, Batch Gradient Norm: 26.864419637150004
Epoch: 5674, Batch Gradient Norm after: 22.360678145685615
Epoch 5675/10000, Prediction Accuracy = 59.456%, Loss = 0.7896968483924866
Epoch: 5675, Batch Gradient Norm: 27.481594808891604
Epoch: 5675, Batch Gradient Norm after: 22.360674698086317
Epoch 5676/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7911790251731873
Epoch: 5676, Batch Gradient Norm: 26.854076086258758
Epoch: 5676, Batch Gradient Norm after: 22.360678135440835
Epoch 5677/10000, Prediction Accuracy = 59.476%, Loss = 0.7895951986312866
Epoch: 5677, Batch Gradient Norm: 27.4765326695408
Epoch: 5677, Batch Gradient Norm after: 22.355980433879793
Epoch 5678/10000, Prediction Accuracy = 59.44%, Loss = 0.7910828709602356
Epoch: 5678, Batch Gradient Norm: 26.852337029022586
Epoch: 5678, Batch Gradient Norm after: 22.360676980992483
Epoch 5679/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7895245432853699
Epoch: 5679, Batch Gradient Norm: 27.47390504012941
Epoch: 5679, Batch Gradient Norm after: 22.360676876485115
Epoch 5680/10000, Prediction Accuracy = 59.426%, Loss = 0.7909965515136719
Epoch: 5680, Batch Gradient Norm: 26.842677538751822
Epoch: 5680, Batch Gradient Norm after: 22.360677474557086
Epoch 5681/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7893978357315063
Epoch: 5681, Batch Gradient Norm: 27.458725709362835
Epoch: 5681, Batch Gradient Norm after: 22.35326036583805
Epoch 5682/10000, Prediction Accuracy = 59.46%, Loss = 0.7908397078514099
Epoch: 5682, Batch Gradient Norm: 26.845455665165
Epoch: 5682, Batch Gradient Norm after: 22.360679846776435
Epoch 5683/10000, Prediction Accuracy = 59.448%, Loss = 0.7892794251441956
Epoch: 5683, Batch Gradient Norm: 27.4560395152188
Epoch: 5683, Batch Gradient Norm after: 22.360677121697808
Epoch 5684/10000, Prediction Accuracy = 59.484%, Loss = 0.7907490491867065
Epoch: 5684, Batch Gradient Norm: 26.834325088100332
Epoch: 5684, Batch Gradient Norm after: 22.360679280319694
Epoch 5685/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7891565680503845
Epoch: 5685, Batch Gradient Norm: 27.443637811814188
Epoch: 5685, Batch Gradient Norm after: 22.35312745844681
Epoch 5686/10000, Prediction Accuracy = 59.456%, Loss = 0.7906163930892944
Epoch: 5686, Batch Gradient Norm: 26.837323966232564
Epoch: 5686, Batch Gradient Norm after: 22.36067904386362
Epoch 5687/10000, Prediction Accuracy = 59.476%, Loss = 0.7890665650367736
Epoch: 5687, Batch Gradient Norm: 27.445723055258018
Epoch: 5687, Batch Gradient Norm after: 22.360678105806713
Epoch 5688/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7905373811721802
Epoch: 5688, Batch Gradient Norm: 26.825113741705803
Epoch: 5688, Batch Gradient Norm after: 22.36067648409309
Epoch 5689/10000, Prediction Accuracy = 59.48599999999999%, Loss = 0.7889871001243591
Epoch: 5689, Batch Gradient Norm: 27.435375487494163
Epoch: 5689, Batch Gradient Norm after: 22.35438997188977
Epoch 5690/10000, Prediction Accuracy = 59.42%, Loss = 0.7904304385185241
Epoch: 5690, Batch Gradient Norm: 26.823218622976363
Epoch: 5690, Batch Gradient Norm after: 22.360676947319483
Epoch 5691/10000, Prediction Accuracy = 59.484%, Loss = 0.7888792395591736
Epoch: 5691, Batch Gradient Norm: 27.434461726569427
Epoch: 5691, Batch Gradient Norm after: 22.359694852042086
Epoch 5692/10000, Prediction Accuracy = 59.46%, Loss = 0.7903119206428528
Epoch: 5692, Batch Gradient Norm: 26.812835886763356
Epoch: 5692, Batch Gradient Norm after: 22.36067610230173
Epoch 5693/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.7887325644493103
Epoch: 5693, Batch Gradient Norm: 27.409957349320763
Epoch: 5693, Batch Gradient Norm after: 22.349225508115023
Epoch 5694/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7901703357696533
Epoch: 5694, Batch Gradient Norm: 26.819236641222737
Epoch: 5694, Batch Gradient Norm after: 22.360677909035676
Epoch 5695/10000, Prediction Accuracy = 59.444%, Loss = 0.7886452436447143
Epoch: 5695, Batch Gradient Norm: 27.410267093432314
Epoch: 5695, Batch Gradient Norm after: 22.36067619952898
Epoch 5696/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.7900648474693298
Epoch: 5696, Batch Gradient Norm: 26.80919967757216
Epoch: 5696, Batch Gradient Norm after: 22.360676806420102
Epoch 5697/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7885354280471801
Epoch: 5697, Batch Gradient Norm: 27.412864057270628
Epoch: 5697, Batch Gradient Norm after: 22.359473064867494
Epoch 5698/10000, Prediction Accuracy = 59.44599999999999%, Loss = 0.789993155002594
Epoch: 5698, Batch Gradient Norm: 26.80009159479095
Epoch: 5698, Batch Gradient Norm after: 22.360677003436706
Epoch 5699/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7884633183479309
Epoch: 5699, Batch Gradient Norm: 27.392285302428007
Epoch: 5699, Batch Gradient Norm after: 22.35114282910325
Epoch 5700/10000, Prediction Accuracy = 59.416%, Loss = 0.7898550629615784
Epoch: 5700, Batch Gradient Norm: 26.803615101745926
Epoch: 5700, Batch Gradient Norm after: 22.36067830999356
Epoch 5701/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.7883520722389221
Epoch: 5701, Batch Gradient Norm: 27.391356425448024
Epoch: 5701, Batch Gradient Norm after: 22.3606745827355
Epoch 5702/10000, Prediction Accuracy = 59.468%, Loss = 0.7897406816482544
Epoch: 5702, Batch Gradient Norm: 26.791694538141872
Epoch: 5702, Batch Gradient Norm after: 22.360676510639404
Epoch 5703/10000, Prediction Accuracy = 59.44%, Loss = 0.7882080078125
Epoch: 5703, Batch Gradient Norm: 27.375097090323624
Epoch: 5703, Batch Gradient Norm after: 22.350948473976104
Epoch 5704/10000, Prediction Accuracy = 59.462%, Loss = 0.7896144390106201
Epoch: 5704, Batch Gradient Norm: 26.79461221866109
Epoch: 5704, Batch Gradient Norm after: 22.360677309876305
Epoch 5705/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.7881088852882385
Epoch: 5705, Batch Gradient Norm: 27.38009088870124
Epoch: 5705, Batch Gradient Norm after: 22.360677577014485
Epoch 5706/10000, Prediction Accuracy = 59.46%, Loss = 0.7895248413085938
Epoch: 5706, Batch Gradient Norm: 26.78023500522872
Epoch: 5706, Batch Gradient Norm after: 22.360679346717067
Epoch 5707/10000, Prediction Accuracy = 59.476%, Loss = 0.7880017518997192
Epoch: 5707, Batch Gradient Norm: 27.360017294718737
Epoch: 5707, Batch Gradient Norm after: 22.348190683954243
Epoch 5708/10000, Prediction Accuracy = 59.45%, Loss = 0.7893853068351746
Epoch: 5708, Batch Gradient Norm: 26.786367356406902
Epoch: 5708, Batch Gradient Norm after: 22.360676747394354
Epoch 5709/10000, Prediction Accuracy = 59.496%, Loss = 0.787950086593628
Epoch: 5709, Batch Gradient Norm: 27.360202662692785
Epoch: 5709, Batch Gradient Norm after: 22.360677794257693
Epoch 5710/10000, Prediction Accuracy = 59.426%, Loss = 0.7893055319786072
Epoch: 5710, Batch Gradient Norm: 26.772529166183816
Epoch: 5710, Batch Gradient Norm after: 22.36067842775298
Epoch 5711/10000, Prediction Accuracy = 59.476%, Loss = 0.7878051996231079
Epoch: 5711, Batch Gradient Norm: 27.347382306786628
Epoch: 5711, Batch Gradient Norm after: 22.351954867985814
Epoch 5712/10000, Prediction Accuracy = 59.46600000000001%, Loss = 0.7891641139984131
Epoch: 5712, Batch Gradient Norm: 26.775015203529865
Epoch: 5712, Batch Gradient Norm after: 22.36067581946186
Epoch 5713/10000, Prediction Accuracy = 59.448%, Loss = 0.7876827597618103
Epoch: 5713, Batch Gradient Norm: 27.354223691008297
Epoch: 5713, Batch Gradient Norm after: 22.360677368374635
Epoch 5714/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.7890974760055542
Epoch: 5714, Batch Gradient Norm: 26.762244405472043
Epoch: 5714, Batch Gradient Norm after: 22.36068093435589
Epoch 5715/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7875539302825928
Epoch: 5715, Batch Gradient Norm: 27.32301983141363
Epoch: 5715, Batch Gradient Norm after: 22.345630760711508
Epoch 5716/10000, Prediction Accuracy = 59.459999999999994%, Loss = 0.7889165163040162
Epoch: 5716, Batch Gradient Norm: 26.774903142752773
Epoch: 5716, Batch Gradient Norm after: 22.360677098779416
Epoch 5717/10000, Prediction Accuracy = 59.484%, Loss = 0.7874933004379272
Epoch: 5717, Batch Gradient Norm: 27.32447588546194
Epoch: 5717, Batch Gradient Norm after: 22.360675877562688
Epoch 5718/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7888339757919312
Epoch: 5718, Batch Gradient Norm: 26.761464654170783
Epoch: 5718, Batch Gradient Norm after: 22.36067818883577
Epoch 5719/10000, Prediction Accuracy = 59.486000000000004%, Loss = 0.7874122858047485
Epoch: 5719, Batch Gradient Norm: 27.32636714993714
Epoch: 5719, Batch Gradient Norm after: 22.35809344248561
Epoch 5720/10000, Prediction Accuracy = 59.452%, Loss = 0.7887609004974365
Epoch: 5720, Batch Gradient Norm: 26.752214904371744
Epoch: 5720, Batch Gradient Norm after: 22.360676928769436
Epoch 5721/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.787275207042694
Epoch: 5721, Batch Gradient Norm: 27.31620635932857
Epoch: 5721, Batch Gradient Norm after: 22.35433137048955
Epoch 5722/10000, Prediction Accuracy = 59.472%, Loss = 0.7886107921600342
Epoch: 5722, Batch Gradient Norm: 26.748897052958185
Epoch: 5722, Batch Gradient Norm after: 22.360681247850092
Epoch 5723/10000, Prediction Accuracy = 59.46%, Loss = 0.7871546626091004
Epoch: 5723, Batch Gradient Norm: 27.312932521267463
Epoch: 5723, Batch Gradient Norm after: 22.358210974536615
Epoch 5724/10000, Prediction Accuracy = 59.45%, Loss = 0.7885197877883912
Epoch: 5724, Batch Gradient Norm: 26.743085137221563
Epoch: 5724, Batch Gradient Norm after: 22.360680360961066
Epoch 5725/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7870406627655029
Epoch: 5725, Batch Gradient Norm: 27.296454850095536
Epoch: 5725, Batch Gradient Norm after: 22.35205881865546
Epoch 5726/10000, Prediction Accuracy = 59.448%, Loss = 0.7883715629577637
Epoch: 5726, Batch Gradient Norm: 26.745692735455904
Epoch: 5726, Batch Gradient Norm after: 22.360676976244534
Epoch 5727/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7869524598121643
Epoch: 5727, Batch Gradient Norm: 27.303319741754947
Epoch: 5727, Batch Gradient Norm after: 22.359738836592868
Epoch 5728/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7883047580718994
Epoch: 5728, Batch Gradient Norm: 26.734441768395886
Epoch: 5728, Batch Gradient Norm after: 22.360678989148965
Epoch 5729/10000, Prediction Accuracy = 59.492%, Loss = 0.7868708610534668
Epoch: 5729, Batch Gradient Norm: 27.28262371996565
Epoch: 5729, Batch Gradient Norm after: 22.35041093945617
Epoch 5730/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.7881633281707764
Epoch: 5730, Batch Gradient Norm: 26.738233526922148
Epoch: 5730, Batch Gradient Norm after: 22.360677478078014
Epoch 5731/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.786764121055603
Epoch: 5731, Batch Gradient Norm: 27.283096975926668
Epoch: 5731, Batch Gradient Norm after: 22.360676233205783
Epoch 5732/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.7880509495735168
Epoch: 5732, Batch Gradient Norm: 26.72481941693771
Epoch: 5732, Batch Gradient Norm after: 22.36068040272751
Epoch 5733/10000, Prediction Accuracy = 59.484%, Loss = 0.7866249442100525
Epoch: 5733, Batch Gradient Norm: 27.26837974579409
Epoch: 5733, Batch Gradient Norm after: 22.352607510243843
Epoch 5734/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7879333138465882
Epoch: 5734, Batch Gradient Norm: 26.72537700253103
Epoch: 5734, Batch Gradient Norm after: 22.36067833356077
Epoch 5735/10000, Prediction Accuracy = 59.465999999999994%, Loss = 0.7865311026573181
Epoch: 5735, Batch Gradient Norm: 27.272189645147208
Epoch: 5735, Batch Gradient Norm after: 22.360678190531125
Epoch 5736/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.7878437042236328
Epoch: 5736, Batch Gradient Norm: 26.716072949115862
Epoch: 5736, Batch Gradient Norm after: 22.36067858999224
Epoch 5737/10000, Prediction Accuracy = 59.476%, Loss = 0.78641836643219
Epoch: 5737, Batch Gradient Norm: 27.25755896157299
Epoch: 5737, Batch Gradient Norm after: 22.35170984594222
Epoch 5738/10000, Prediction Accuracy = 59.462%, Loss = 0.7877131581306458
Epoch: 5738, Batch Gradient Norm: 26.71800124969519
Epoch: 5738, Batch Gradient Norm after: 22.360676803032046
Epoch 5739/10000, Prediction Accuracy = 59.492%, Loss = 0.7863685011863708
Epoch: 5739, Batch Gradient Norm: 27.25927349046725
Epoch: 5739, Batch Gradient Norm after: 22.360676896791496
Epoch 5740/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.7876350283622742
Epoch: 5740, Batch Gradient Norm: 26.704776047495614
Epoch: 5740, Batch Gradient Norm after: 22.360677735171077
Epoch 5741/10000, Prediction Accuracy = 59.484%, Loss = 0.7862269639968872
Epoch: 5741, Batch Gradient Norm: 27.239723636625236
Epoch: 5741, Batch Gradient Norm after: 22.35127276604614
Epoch 5742/10000, Prediction Accuracy = 59.462%, Loss = 0.7874769687652587
Epoch: 5742, Batch Gradient Norm: 26.710837057482603
Epoch: 5742, Batch Gradient Norm after: 22.36067922456972
Epoch 5743/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7861156702041626
Epoch: 5743, Batch Gradient Norm: 27.23945349732899
Epoch: 5743, Batch Gradient Norm after: 22.360676774103098
Epoch 5744/10000, Prediction Accuracy = 59.448%, Loss = 0.7873939156532288
Epoch: 5744, Batch Gradient Norm: 26.698578439021432
Epoch: 5744, Batch Gradient Norm after: 22.360679121554035
Epoch 5745/10000, Prediction Accuracy = 59.464%, Loss = 0.7859960317611694
Epoch: 5745, Batch Gradient Norm: 27.23027545540799
Epoch: 5745, Batch Gradient Norm after: 22.354577430042053
Epoch 5746/10000, Prediction Accuracy = 59.43399999999999%, Loss = 0.7872715830802918
Epoch: 5746, Batch Gradient Norm: 26.699231169767344
Epoch: 5746, Batch Gradient Norm after: 22.360676998127197
Epoch 5747/10000, Prediction Accuracy = 59.468%, Loss = 0.7859018445014954
Epoch: 5747, Batch Gradient Norm: 27.232962451764802
Epoch: 5747, Batch Gradient Norm after: 22.360676384588935
Epoch 5748/10000, Prediction Accuracy = 59.46%, Loss = 0.7871964931488037
Epoch: 5748, Batch Gradient Norm: 26.686146896574012
Epoch: 5748, Batch Gradient Norm after: 22.360678900950578
Epoch 5749/10000, Prediction Accuracy = 59.5%, Loss = 0.7858283996582032
Epoch: 5749, Batch Gradient Norm: 27.209549669687025
Epoch: 5749, Batch Gradient Norm after: 22.348896072088092
Epoch 5750/10000, Prediction Accuracy = 59.46%, Loss = 0.7870548844337464
Epoch: 5750, Batch Gradient Norm: 26.69183342713701
Epoch: 5750, Batch Gradient Norm after: 22.360677439117925
Epoch 5751/10000, Prediction Accuracy = 59.48%, Loss = 0.7857286810874939
Epoch: 5751, Batch Gradient Norm: 27.20915128435387
Epoch: 5751, Batch Gradient Norm after: 22.360675223355198
Epoch 5752/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.7869468688964844
Epoch: 5752, Batch Gradient Norm: 26.68072633695169
Epoch: 5752, Batch Gradient Norm after: 22.360678273115372
Epoch 5753/10000, Prediction Accuracy = 59.496%, Loss = 0.7855834364891052
Epoch: 5753, Batch Gradient Norm: 27.19920877819038
Epoch: 5753, Batch Gradient Norm after: 22.353644863525858
Epoch 5754/10000, Prediction Accuracy = 59.44%, Loss = 0.7868325114250183
Epoch: 5754, Batch Gradient Norm: 26.680085437470215
Epoch: 5754, Batch Gradient Norm after: 22.36068035482603
Epoch 5755/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.7854853868484497
Epoch: 5755, Batch Gradient Norm: 27.202110599659527
Epoch: 5755, Batch Gradient Norm after: 22.360676304562947
Epoch 5756/10000, Prediction Accuracy = 59.424%, Loss = 0.7867355704307556
Epoch: 5756, Batch Gradient Norm: 26.670718575973407
Epoch: 5756, Batch Gradient Norm after: 22.36067827658414
Epoch 5757/10000, Prediction Accuracy = 59.476%, Loss = 0.7853697180747986
Epoch: 5757, Batch Gradient Norm: 27.17902428958924
Epoch: 5757, Batch Gradient Norm after: 22.348515108133274
Epoch 5758/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7865935444831849
Epoch: 5758, Batch Gradient Norm: 26.681071717554
Epoch: 5758, Batch Gradient Norm after: 22.360675544898278
Epoch 5759/10000, Prediction Accuracy = 59.524%, Loss = 0.7853242516517639
Epoch: 5759, Batch Gradient Norm: 27.178773641500285
Epoch: 5759, Batch Gradient Norm after: 22.360677434148737
Epoch 5760/10000, Prediction Accuracy = 59.46%, Loss = 0.7865055084228516
Epoch: 5760, Batch Gradient Norm: 26.666081852708928
Epoch: 5760, Batch Gradient Norm after: 22.360678595730175
Epoch 5761/10000, Prediction Accuracy = 59.516000000000005%, Loss = 0.7852007269859314
Epoch: 5761, Batch Gradient Norm: 27.174686330121144
Epoch: 5761, Batch Gradient Norm after: 22.357439832288506
Epoch 5762/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7863967657089234
Epoch: 5762, Batch Gradient Norm: 26.65879422691556
Epoch: 5762, Batch Gradient Norm after: 22.360680359360614
Epoch 5763/10000, Prediction Accuracy = 59.512%, Loss = 0.7850656986236573
Epoch: 5763, Batch Gradient Norm: 27.164935883438236
Epoch: 5763, Batch Gradient Norm after: 22.355788345988845
Epoch 5764/10000, Prediction Accuracy = 59.436%, Loss = 0.7862892031669617
Epoch: 5764, Batch Gradient Norm: 26.655189969349248
Epoch: 5764, Batch Gradient Norm after: 22.360679794813215
Epoch 5765/10000, Prediction Accuracy = 59.45%, Loss = 0.7849698662757874
Epoch: 5765, Batch Gradient Norm: 27.166301480254344
Epoch: 5765, Batch Gradient Norm after: 22.35939816789476
Epoch 5766/10000, Prediction Accuracy = 59.42%, Loss = 0.7861875891685486
Epoch: 5766, Batch Gradient Norm: 26.64799178255227
Epoch: 5766, Batch Gradient Norm after: 22.360677435435075
Epoch 5767/10000, Prediction Accuracy = 59.48%, Loss = 0.7848603367805481
Epoch: 5767, Batch Gradient Norm: 27.155313939028147
Epoch: 5767, Batch Gradient Norm after: 22.35428696180579
Epoch 5768/10000, Prediction Accuracy = 59.495999999999995%, Loss = 0.7860661387443543
Epoch: 5768, Batch Gradient Norm: 26.648213159463463
Epoch: 5768, Batch Gradient Norm after: 22.36067794940537
Epoch 5769/10000, Prediction Accuracy = 59.532%, Loss = 0.7848020553588867
Epoch: 5769, Batch Gradient Norm: 27.157984602004216
Epoch: 5769, Batch Gradient Norm after: 22.36062701927031
Epoch 5770/10000, Prediction Accuracy = 59.488%, Loss = 0.7860018730163574
Epoch: 5770, Batch Gradient Norm: 26.63709451601352
Epoch: 5770, Batch Gradient Norm after: 22.360680886884904
Epoch 5771/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.784679913520813
Epoch: 5771, Batch Gradient Norm: 27.13433447658634
Epoch: 5771, Batch Gradient Norm after: 22.350112902790187
Epoch 5772/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7858260273933411
Epoch: 5772, Batch Gradient Norm: 26.64349033002502
Epoch: 5772, Batch Gradient Norm after: 22.360679401470396
Epoch 5773/10000, Prediction Accuracy = 59.516%, Loss = 0.7845668911933898
Epoch: 5773, Batch Gradient Norm: 27.13230109877972
Epoch: 5773, Batch Gradient Norm after: 22.360676922363577
Epoch 5774/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7857356667518616
Epoch: 5774, Batch Gradient Norm: 26.631136456676554
Epoch: 5774, Batch Gradient Norm after: 22.36067864262555
Epoch 5775/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.7844544649124146
Epoch: 5775, Batch Gradient Norm: 27.12580868152378
Epoch: 5775, Batch Gradient Norm after: 22.354983036391502
Epoch 5776/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7856245994567871
Epoch: 5776, Batch Gradient Norm: 26.63024950146637
Epoch: 5776, Batch Gradient Norm after: 22.360678228038385
Epoch 5777/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.7843528866767884
Epoch: 5777, Batch Gradient Norm: 27.127393135088973
Epoch: 5777, Batch Gradient Norm after: 22.36067648083952
Epoch 5778/10000, Prediction Accuracy = 59.49399999999999%, Loss = 0.7855396628379822
Epoch: 5778, Batch Gradient Norm: 26.622948850242185
Epoch: 5778, Batch Gradient Norm after: 22.36068036852968
Epoch 5779/10000, Prediction Accuracy = 59.53000000000001%, Loss = 0.7842786431312561
Epoch: 5779, Batch Gradient Norm: 27.104073816115058
Epoch: 5779, Batch Gradient Norm after: 22.35033539819105
Epoch 5780/10000, Prediction Accuracy = 59.492%, Loss = 0.7854098558425904
Epoch: 5780, Batch Gradient Norm: 26.626469991780876
Epoch: 5780, Batch Gradient Norm after: 22.360678477793595
Epoch 5781/10000, Prediction Accuracy = 59.516000000000005%, Loss = 0.7841939687728882
Epoch: 5781, Batch Gradient Norm: 27.10536611213998
Epoch: 5781, Batch Gradient Norm after: 22.360675651656344
Epoch 5782/10000, Prediction Accuracy = 59.495999999999995%, Loss = 0.7852971076965332
Epoch: 5782, Batch Gradient Norm: 26.61358561623691
Epoch: 5782, Batch Gradient Norm after: 22.360678112832545
Epoch 5783/10000, Prediction Accuracy = 59.528%, Loss = 0.7840428590774536
Epoch: 5783, Batch Gradient Norm: 27.092805364957748
Epoch: 5783, Batch Gradient Norm after: 22.353455773155506
Epoch 5784/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7851848483085633
Epoch: 5784, Batch Gradient Norm: 26.613579110010704
Epoch: 5784, Batch Gradient Norm after: 22.36068044716254
Epoch 5785/10000, Prediction Accuracy = 59.476%, Loss = 0.7839473605155944
Epoch: 5785, Batch Gradient Norm: 27.097245819786455
Epoch: 5785, Batch Gradient Norm after: 22.359871281021274
Epoch 5786/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.7850971817970276
Epoch: 5786, Batch Gradient Norm: 26.6012614330335
Epoch: 5786, Batch Gradient Norm after: 22.360678851215397
Epoch 5787/10000, Prediction Accuracy = 59.488%, Loss = 0.7838231563568115
Epoch: 5787, Batch Gradient Norm: 27.07680985538357
Epoch: 5787, Batch Gradient Norm after: 22.34776464748892
Epoch 5788/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.7849445223808289
Epoch: 5788, Batch Gradient Norm: 26.611448004497067
Epoch: 5788, Batch Gradient Norm after: 22.36067750240375
Epoch 5789/10000, Prediction Accuracy = 59.53000000000001%, Loss = 0.7837801456451416
Epoch: 5789, Batch Gradient Norm: 27.077539977899114
Epoch: 5789, Batch Gradient Norm after: 22.360677037376785
Epoch 5790/10000, Prediction Accuracy = 59.5%, Loss = 0.7848823428153991
Epoch: 5790, Batch Gradient Norm: 26.596283226336418
Epoch: 5790, Batch Gradient Norm after: 22.360677619956864
Epoch 5791/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7836700558662415
Epoch: 5791, Batch Gradient Norm: 27.062617819748883
Epoch: 5791, Batch Gradient Norm after: 22.351338012539212
Epoch 5792/10000, Prediction Accuracy = 59.516%, Loss = 0.7847324490547181
Epoch: 5792, Batch Gradient Norm: 26.598753199115492
Epoch: 5792, Batch Gradient Norm after: 22.360677773523484
Epoch 5793/10000, Prediction Accuracy = 59.536%, Loss = 0.7835403323173523
Epoch: 5793, Batch Gradient Norm: 27.066861727504808
Epoch: 5793, Batch Gradient Norm after: 22.359806574004317
Epoch 5794/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.7846645712852478
Epoch: 5794, Batch Gradient Norm: 26.5853836423152
Epoch: 5794, Batch Gradient Norm after: 22.36067868965613
Epoch 5795/10000, Prediction Accuracy = 59.504%, Loss = 0.7834221482276916
Epoch: 5795, Batch Gradient Norm: 27.04919231462582
Epoch: 5795, Batch Gradient Norm after: 22.349085051937674
Epoch 5796/10000, Prediction Accuracy = 59.444%, Loss = 0.7845181941986084
Epoch: 5796, Batch Gradient Norm: 26.593903774437152
Epoch: 5796, Batch Gradient Norm after: 22.36067795704859
Epoch 5797/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.783332371711731
Epoch: 5797, Batch Gradient Norm: 27.05372688216166
Epoch: 5797, Batch Gradient Norm after: 22.360677810974423
Epoch 5798/10000, Prediction Accuracy = 59.504%, Loss = 0.7844286799430847
Epoch: 5798, Batch Gradient Norm: 26.58066322525638
Epoch: 5798, Batch Gradient Norm after: 22.360675492831376
Epoch 5799/10000, Prediction Accuracy = 59.552%, Loss = 0.783244526386261
Epoch: 5799, Batch Gradient Norm: 27.044059081619995
Epoch: 5799, Batch Gradient Norm after: 22.3520384263584
Epoch 5800/10000, Prediction Accuracy = 59.524%, Loss = 0.7843234896659851
Epoch: 5800, Batch Gradient Norm: 26.58106692924052
Epoch: 5800, Batch Gradient Norm after: 22.360676122654425
Epoch 5801/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7831660032272338
Epoch: 5801, Batch Gradient Norm: 27.04482390659641
Epoch: 5801, Batch Gradient Norm after: 22.360677793724513
Epoch 5802/10000, Prediction Accuracy = 59.528%, Loss = 0.7842235207557678
Epoch: 5802, Batch Gradient Norm: 26.56928689235624
Epoch: 5802, Batch Gradient Norm after: 22.360676080079006
Epoch 5803/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7830205082893371
Epoch: 5803, Batch Gradient Norm: 27.029724955278176
Epoch: 5803, Batch Gradient Norm after: 22.352179042150663
Epoch 5804/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.784091055393219
Epoch: 5804, Batch Gradient Norm: 26.57348114428931
Epoch: 5804, Batch Gradient Norm after: 22.36067858967508
Epoch 5805/10000, Prediction Accuracy = 59.50999999999999%, Loss = 0.7829291701316834
Epoch: 5805, Batch Gradient Norm: 27.02990355922176
Epoch: 5805, Batch Gradient Norm after: 22.360676483218004
Epoch 5806/10000, Prediction Accuracy = 59.468%, Loss = 0.784002935886383
Epoch: 5806, Batch Gradient Norm: 26.564639416816775
Epoch: 5806, Batch Gradient Norm after: 22.36067926552355
Epoch 5807/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.7828101873397827
Epoch: 5807, Batch Gradient Norm: 27.024010408106033
Epoch: 5807, Batch Gradient Norm after: 22.35569483150658
Epoch 5808/10000, Prediction Accuracy = 59.50599999999999%, Loss = 0.7838893055915832
Epoch: 5808, Batch Gradient Norm: 26.563735737825315
Epoch: 5808, Batch Gradient Norm after: 22.36067920664844
Epoch 5809/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.7827407717704773
Epoch: 5809, Batch Gradient Norm: 27.01633529105407
Epoch: 5809, Batch Gradient Norm after: 22.356119502988687
Epoch 5810/10000, Prediction Accuracy = 59.538%, Loss = 0.7838045954704285
Epoch: 5810, Batch Gradient Norm: 26.557398228695206
Epoch: 5810, Batch Gradient Norm after: 22.3606780147637
Epoch 5811/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.7826526284217834
Epoch: 5811, Batch Gradient Norm: 27.012478239500204
Epoch: 5811, Batch Gradient Norm after: 22.357709231120648
Epoch 5812/10000, Prediction Accuracy = 59.528%, Loss = 0.7836851954460144
Epoch: 5812, Batch Gradient Norm: 26.547451024775047
Epoch: 5812, Batch Gradient Norm after: 22.36067928108549
Epoch 5813/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7825124144554139
Epoch: 5813, Batch Gradient Norm: 26.99319001227714
Epoch: 5813, Batch Gradient Norm after: 22.351477297398894
Epoch 5814/10000, Prediction Accuracy = 59.468%, Loss = 0.7835484027862549
Epoch: 5814, Batch Gradient Norm: 26.552411309540226
Epoch: 5814, Batch Gradient Norm after: 22.36067843551045
Epoch 5815/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7824251651763916
Epoch: 5815, Batch Gradient Norm: 26.99718053827015
Epoch: 5815, Batch Gradient Norm after: 22.360674879345975
Epoch 5816/10000, Prediction Accuracy = 59.495999999999995%, Loss = 0.7834604740142822
Epoch: 5816, Batch Gradient Norm: 26.539355535965598
Epoch: 5816, Batch Gradient Norm after: 22.360677639084244
Epoch 5817/10000, Prediction Accuracy = 59.53599999999999%, Loss = 0.7823050737380981
Epoch: 5817, Batch Gradient Norm: 26.981801728239933
Epoch: 5817, Batch Gradient Norm after: 22.351102201132218
Epoch 5818/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.7833267450332642
Epoch: 5818, Batch Gradient Norm: 26.543301809092075
Epoch: 5818, Batch Gradient Norm after: 22.360677457026014
Epoch 5819/10000, Prediction Accuracy = 59.552%, Loss = 0.782242476940155
Epoch: 5819, Batch Gradient Norm: 26.988213590142134
Epoch: 5819, Batch Gradient Norm after: 22.360678276346082
Epoch 5820/10000, Prediction Accuracy = 59.532%, Loss = 0.7832765102386474
Epoch: 5820, Batch Gradient Norm: 26.529061660577902
Epoch: 5820, Batch Gradient Norm after: 22.360679617034773
Epoch 5821/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7821497559547425
Epoch: 5821, Batch Gradient Norm: 26.972410380369762
Epoch: 5821, Batch Gradient Norm after: 22.35197113440521
Epoch 5822/10000, Prediction Accuracy = 59.528%, Loss = 0.7831231474876403
Epoch: 5822, Batch Gradient Norm: 26.532160357368927
Epoch: 5822, Batch Gradient Norm after: 22.36067932662964
Epoch 5823/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.7820196151733398
Epoch: 5823, Batch Gradient Norm: 26.973115306435517
Epoch: 5823, Batch Gradient Norm after: 22.360678466583785
Epoch 5824/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.7830380082130433
Epoch: 5824, Batch Gradient Norm: 26.520582345563007
Epoch: 5824, Batch Gradient Norm after: 22.360676691491285
Epoch 5825/10000, Prediction Accuracy = 59.528%, Loss = 0.7818944931030274
Epoch: 5825, Batch Gradient Norm: 26.95284858356507
Epoch: 5825, Batch Gradient Norm after: 22.348942126795478
Epoch 5826/10000, Prediction Accuracy = 59.484%, Loss = 0.7829012513160706
Epoch: 5826, Batch Gradient Norm: 26.527888920409136
Epoch: 5826, Batch Gradient Norm after: 22.360678516385825
Epoch 5827/10000, Prediction Accuracy = 59.540000000000006%, Loss = 0.7818071603775024
Epoch: 5827, Batch Gradient Norm: 26.95558854522934
Epoch: 5827, Batch Gradient Norm after: 22.360677336115348
Epoch 5828/10000, Prediction Accuracy = 59.488%, Loss = 0.7828051447868347
Epoch: 5828, Batch Gradient Norm: 26.513966729820233
Epoch: 5828, Batch Gradient Norm after: 22.36067816387818
Epoch 5829/10000, Prediction Accuracy = 59.552%, Loss = 0.7817141771316528
Epoch: 5829, Batch Gradient Norm: 26.944662711099475
Epoch: 5829, Batch Gradient Norm after: 22.350390198600774
Epoch 5830/10000, Prediction Accuracy = 59.53399999999999%, Loss = 0.782709264755249
Epoch: 5830, Batch Gradient Norm: 26.516990042370672
Epoch: 5830, Batch Gradient Norm after: 22.360677908426318
Epoch 5831/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7816499471664429
Epoch: 5831, Batch Gradient Norm: 26.94866986004126
Epoch: 5831, Batch Gradient Norm after: 22.3606767620221
Epoch 5832/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7826156497001648
Epoch: 5832, Batch Gradient Norm: 26.501284405089926
Epoch: 5832, Batch Gradient Norm after: 22.36067631125652
Epoch 5833/10000, Prediction Accuracy = 59.572%, Loss = 0.7814925789833069
Epoch: 5833, Batch Gradient Norm: 26.92477266591619
Epoch: 5833, Batch Gradient Norm after: 22.346796261785805
Epoch 5834/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.7824601054191589
Epoch: 5834, Batch Gradient Norm: 26.514597385607587
Epoch: 5834, Batch Gradient Norm after: 22.360678109909315
Epoch 5835/10000, Prediction Accuracy = 59.538%, Loss = 0.7814083099365234
Epoch: 5835, Batch Gradient Norm: 26.925968207262915
Epoch: 5835, Batch Gradient Norm after: 22.360679338985193
Epoch 5836/10000, Prediction Accuracy = 59.492%, Loss = 0.7823812961578369
Epoch: 5836, Batch Gradient Norm: 26.49917071500143
Epoch: 5836, Batch Gradient Norm after: 22.360675233975904
Epoch 5837/10000, Prediction Accuracy = 59.540000000000006%, Loss = 0.7812876462936401
Epoch: 5837, Batch Gradient Norm: 26.920233834806314
Epoch: 5837, Batch Gradient Norm after: 22.351900861505268
Epoch 5838/10000, Prediction Accuracy = 59.488%, Loss = 0.782258665561676
Epoch: 5838, Batch Gradient Norm: 26.49853799058522
Epoch: 5838, Batch Gradient Norm after: 22.36067663312259
Epoch 5839/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.7812075138092041
Epoch: 5839, Batch Gradient Norm: 26.923095787523774
Epoch: 5839, Batch Gradient Norm after: 22.358967729350375
Epoch 5840/10000, Prediction Accuracy = 59.52%, Loss = 0.7821973562240601
Epoch: 5840, Batch Gradient Norm: 26.487007959735433
Epoch: 5840, Batch Gradient Norm after: 22.360678235956627
Epoch 5841/10000, Prediction Accuracy = 59.592%, Loss = 0.78112952709198
Epoch: 5841, Batch Gradient Norm: 26.900964877950805
Epoch: 5841, Batch Gradient Norm after: 22.348914441479856
Epoch 5842/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7820504069328308
Epoch: 5842, Batch Gradient Norm: 26.490960411337323
Epoch: 5842, Batch Gradient Norm after: 22.360677801752338
Epoch 5843/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7810163855552673
Epoch: 5843, Batch Gradient Norm: 26.90642669971085
Epoch: 5843, Batch Gradient Norm after: 22.360675341894368
Epoch 5844/10000, Prediction Accuracy = 59.5%, Loss = 0.7819532990455628
Epoch: 5844, Batch Gradient Norm: 26.477747180306256
Epoch: 5844, Batch Gradient Norm after: 22.360678210934466
Epoch 5845/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7808832645416259
Epoch: 5845, Batch Gradient Norm: 26.879998715020744
Epoch: 5845, Batch Gradient Norm after: 22.34796961873279
Epoch 5846/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.7818279027938843
Epoch: 5846, Batch Gradient Norm: 26.48666818263151
Epoch: 5846, Batch Gradient Norm after: 22.360678179254204
Epoch 5847/10000, Prediction Accuracy = 59.517999999999994%, Loss = 0.7808016657829284
Epoch: 5847, Batch Gradient Norm: 26.88798568800131
Epoch: 5847, Batch Gradient Norm after: 22.36067695060962
Epoch 5848/10000, Prediction Accuracy = 59.492%, Loss = 0.7817208528518677
Epoch: 5848, Batch Gradient Norm: 26.47384500242763
Epoch: 5848, Batch Gradient Norm after: 22.360678708500423
Epoch 5849/10000, Prediction Accuracy = 59.54599999999999%, Loss = 0.7806897401809693
Epoch: 5849, Batch Gradient Norm: 26.880783540803108
Epoch: 5849, Batch Gradient Norm after: 22.353174462018288
Epoch 5850/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7816350221633911
Epoch: 5850, Batch Gradient Norm: 26.470808203267712
Epoch: 5850, Batch Gradient Norm after: 22.360677862286366
Epoch 5851/10000, Prediction Accuracy = 59.592%, Loss = 0.7806386113166809
Epoch: 5851, Batch Gradient Norm: 26.881556761419642
Epoch: 5851, Batch Gradient Norm after: 22.357430949150643
Epoch 5852/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7815508246421814
Epoch: 5852, Batch Gradient Norm: 26.45917181852537
Epoch: 5852, Batch Gradient Norm after: 22.36067769336306
Epoch 5853/10000, Prediction Accuracy = 59.562%, Loss = 0.7804985761642456
Epoch: 5853, Batch Gradient Norm: 26.85924829445603
Epoch: 5853, Batch Gradient Norm after: 22.349433959814828
Epoch 5854/10000, Prediction Accuracy = 59.50599999999999%, Loss = 0.7813820004463196
Epoch: 5854, Batch Gradient Norm: 26.46674764950807
Epoch: 5854, Batch Gradient Norm after: 22.360677259357793
Epoch 5855/10000, Prediction Accuracy = 59.562%, Loss = 0.780392837524414
Epoch: 5855, Batch Gradient Norm: 26.868155708195175
Epoch: 5855, Batch Gradient Norm after: 22.36067872900103
Epoch 5856/10000, Prediction Accuracy = 59.512%, Loss = 0.7813271403312683
Epoch: 5856, Batch Gradient Norm: 26.451542650080366
Epoch: 5856, Batch Gradient Norm after: 22.360676974222034
Epoch 5857/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7802697896957398
Epoch: 5857, Batch Gradient Norm: 26.844756154626797
Epoch: 5857, Batch Gradient Norm after: 22.34568012836779
Epoch 5858/10000, Prediction Accuracy = 59.489999999999995%, Loss = 0.7811600089073181
Epoch: 5858, Batch Gradient Norm: 26.463734055461547
Epoch: 5858, Batch Gradient Norm after: 22.360677770604234
Epoch 5859/10000, Prediction Accuracy = 59.564%, Loss = 0.780195951461792
Epoch: 5859, Batch Gradient Norm: 26.847366085684868
Epoch: 5859, Batch Gradient Norm after: 22.360680135413265
Epoch 5860/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7810812354087829
Epoch: 5860, Batch Gradient Norm: 26.44960467817045
Epoch: 5860, Batch Gradient Norm after: 22.360679345018347
Epoch 5861/10000, Prediction Accuracy = 59.584%, Loss = 0.7801190376281738
Epoch: 5861, Batch Gradient Norm: 26.84276540403445
Epoch: 5861, Batch Gradient Norm after: 22.354111795848524
Epoch 5862/10000, Prediction Accuracy = 59.538%, Loss = 0.7810046195983886
Epoch: 5862, Batch Gradient Norm: 26.447694541895068
Epoch: 5862, Batch Gradient Norm after: 22.360678040776097
Epoch 5863/10000, Prediction Accuracy = 59.574%, Loss = 0.78000727891922
Epoch: 5863, Batch Gradient Norm: 26.841500670307784
Epoch: 5863, Batch Gradient Norm after: 22.357410264147344
Epoch 5864/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7808779835700989
Epoch: 5864, Batch Gradient Norm: 26.438830600744893
Epoch: 5864, Batch Gradient Norm after: 22.360678332524277
Epoch 5865/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7798702716827393
Epoch: 5865, Batch Gradient Norm: 26.829013736863594
Epoch: 5865, Batch Gradient Norm after: 22.352543865711798
Epoch 5866/10000, Prediction Accuracy = 59.512%, Loss = 0.7807719469070434
Epoch: 5866, Batch Gradient Norm: 26.440124340407056
Epoch: 5866, Batch Gradient Norm after: 22.360675171901132
Epoch 5867/10000, Prediction Accuracy = 59.536%, Loss = 0.7797769546508789
Epoch: 5867, Batch Gradient Norm: 26.828315778294364
Epoch: 5867, Batch Gradient Norm after: 22.357980699693616
Epoch 5868/10000, Prediction Accuracy = 59.5%, Loss = 0.780674409866333
Epoch: 5868, Batch Gradient Norm: 26.433302912033554
Epoch: 5868, Batch Gradient Norm after: 22.36067756956161
Epoch 5869/10000, Prediction Accuracy = 59.58399999999999%, Loss = 0.7796645522117615
Epoch: 5869, Batch Gradient Norm: 26.81785038122266
Epoch: 5869, Batch Gradient Norm after: 22.352431815122326
Epoch 5870/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.7805537223815918
Epoch: 5870, Batch Gradient Norm: 26.435286527258558
Epoch: 5870, Batch Gradient Norm after: 22.360679650061428
Epoch 5871/10000, Prediction Accuracy = 59.584%, Loss = 0.7796043992042542
Epoch: 5871, Batch Gradient Norm: 26.820408573187404
Epoch: 5871, Batch Gradient Norm after: 22.358360793843914
Epoch 5872/10000, Prediction Accuracy = 59.538%, Loss = 0.7804906606674195
Epoch: 5872, Batch Gradient Norm: 26.421673376336962
Epoch: 5872, Batch Gradient Norm after: 22.36067992142761
Epoch 5873/10000, Prediction Accuracy = 59.576%, Loss = 0.7794972419738769
Epoch: 5873, Batch Gradient Norm: 26.79327741848942
Epoch: 5873, Batch Gradient Norm after: 22.345354421257664
Epoch 5874/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7803083539009095
Epoch: 5874, Batch Gradient Norm: 26.433268625392913
Epoch: 5874, Batch Gradient Norm after: 22.36067746404446
Epoch 5875/10000, Prediction Accuracy = 59.556%, Loss = 0.7793904304504394
Epoch: 5875, Batch Gradient Norm: 26.798254917793784
Epoch: 5875, Batch Gradient Norm after: 22.360678267001912
Epoch 5876/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7802347779273987
Epoch: 5876, Batch Gradient Norm: 26.416326636106337
Epoch: 5876, Batch Gradient Norm after: 22.360678626949838
Epoch 5877/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7792630910873413
Epoch: 5877, Batch Gradient Norm: 26.78632206237963
Epoch: 5877, Batch Gradient Norm after: 22.350715616250955
Epoch 5878/10000, Prediction Accuracy = 59.516%, Loss = 0.7801169514656067
Epoch: 5878, Batch Gradient Norm: 26.421115556436625
Epoch: 5878, Batch Gradient Norm after: 22.360676847619597
Epoch 5879/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.7791736245155334
Epoch: 5879, Batch Gradient Norm: 26.79219375308891
Epoch: 5879, Batch Gradient Norm after: 22.360679046421343
Epoch 5880/10000, Prediction Accuracy = 59.52%, Loss = 0.7800382137298584
Epoch: 5880, Batch Gradient Norm: 26.40967258387458
Epoch: 5880, Batch Gradient Norm after: 22.36067726224854
Epoch 5881/10000, Prediction Accuracy = 59.576%, Loss = 0.7790882587432861
Epoch: 5881, Batch Gradient Norm: 26.775062358173574
Epoch: 5881, Batch Gradient Norm after: 22.34923244594117
Epoch 5882/10000, Prediction Accuracy = 59.519999999999996%, Loss = 0.7799259781837463
Epoch: 5882, Batch Gradient Norm: 26.41231164155427
Epoch: 5882, Batch Gradient Norm after: 22.360678897478124
Epoch 5883/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7790289282798767
Epoch: 5883, Batch Gradient Norm: 26.779562747526803
Epoch: 5883, Batch Gradient Norm after: 22.360678646225132
Epoch 5884/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.7798281669616699
Epoch: 5884, Batch Gradient Norm: 26.397868704524527
Epoch: 5884, Batch Gradient Norm after: 22.360678714932998
Epoch 5885/10000, Prediction Accuracy = 59.552%, Loss = 0.7788678050041199
Epoch: 5885, Batch Gradient Norm: 26.761855545362106
Epoch: 5885, Batch Gradient Norm after: 22.348512516309928
Epoch 5886/10000, Prediction Accuracy = 59.516%, Loss = 0.7796891331672668
Epoch: 5886, Batch Gradient Norm: 26.403830412458323
Epoch: 5886, Batch Gradient Norm after: 22.36067665379521
Epoch 5887/10000, Prediction Accuracy = 59.556%, Loss = 0.7787853956222535
Epoch: 5887, Batch Gradient Norm: 26.76874415379339
Epoch: 5887, Batch Gradient Norm after: 22.360679405608323
Epoch 5888/10000, Prediction Accuracy = 59.516000000000005%, Loss = 0.7796263217926025
Epoch: 5888, Batch Gradient Norm: 26.38974781493225
Epoch: 5888, Batch Gradient Norm after: 22.36067728695438
Epoch 5889/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7786541700363159
Epoch: 5889, Batch Gradient Norm: 26.747705631565264
Epoch: 5889, Batch Gradient Norm after: 22.347484493547956
Epoch 5890/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.7794677495956421
Epoch: 5890, Batch Gradient Norm: 26.399278168185997
Epoch: 5890, Batch Gradient Norm after: 22.360677037654074
Epoch 5891/10000, Prediction Accuracy = 59.552%, Loss = 0.7786061048507691
Epoch: 5891, Batch Gradient Norm: 26.754819646369832
Epoch: 5891, Batch Gradient Norm after: 22.360678347869367
Epoch 5892/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.7794223904609681
Epoch: 5892, Batch Gradient Norm: 26.380244210640083
Epoch: 5892, Batch Gradient Norm after: 22.360677738310958
Epoch 5893/10000, Prediction Accuracy = 59.584%, Loss = 0.7785147666931153
Epoch: 5893, Batch Gradient Norm: 26.741952223124414
Epoch: 5893, Batch Gradient Norm after: 22.350003600435507
Epoch 5894/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.7792837619781494
Epoch: 5894, Batch Gradient Norm: 26.384351741997083
Epoch: 5894, Batch Gradient Norm after: 22.360679191256207
Epoch 5895/10000, Prediction Accuracy = 59.54%, Loss = 0.7783875584602356
Epoch: 5895, Batch Gradient Norm: 26.747715678360066
Epoch: 5895, Batch Gradient Norm after: 22.360679477254326
Epoch 5896/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.779205596446991
Epoch: 5896, Batch Gradient Norm: 26.36927508850547
Epoch: 5896, Batch Gradient Norm after: 22.360676087257115
Epoch 5897/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.7782535195350647
Epoch: 5897, Batch Gradient Norm: 26.718713292094463
Epoch: 5897, Batch Gradient Norm after: 22.343683355308585
Epoch 5898/10000, Prediction Accuracy = 59.528%, Loss = 0.7790540218353271
Epoch: 5898, Batch Gradient Norm: 26.38349636251056
Epoch: 5898, Batch Gradient Norm after: 22.360678009845284
Epoch 5899/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.778185498714447
Epoch: 5899, Batch Gradient Norm: 26.72288061906891
Epoch: 5899, Batch Gradient Norm after: 22.36067839601161
Epoch 5900/10000, Prediction Accuracy = 59.528%, Loss = 0.7789589643478394
Epoch: 5900, Batch Gradient Norm: 26.36753321438513
Epoch: 5900, Batch Gradient Norm after: 22.36067840485282
Epoch 5901/10000, Prediction Accuracy = 59.572%, Loss = 0.7780765175819397
Epoch: 5901, Batch Gradient Norm: 26.72324288019045
Epoch: 5901, Batch Gradient Norm after: 22.354113100399875
Epoch 5902/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7788898944854736
Epoch: 5902, Batch Gradient Norm: 26.360981534721436
Epoch: 5902, Batch Gradient Norm after: 22.360677438565347
Epoch 5903/10000, Prediction Accuracy = 59.588%, Loss = 0.7780224323272705
Epoch: 5903, Batch Gradient Norm: 26.714891361098974
Epoch: 5903, Batch Gradient Norm after: 22.354408817749558
Epoch 5904/10000, Prediction Accuracy = 59.55400000000001%, Loss = 0.7787814497947693
Epoch: 5904, Batch Gradient Norm: 26.356244633377546
Epoch: 5904, Batch Gradient Norm after: 22.360679697266853
Epoch 5905/10000, Prediction Accuracy = 59.56%, Loss = 0.7778847813606262
Epoch: 5905, Batch Gradient Norm: 26.709342121840766
Epoch: 5905, Batch Gradient Norm after: 22.353421516081752
Epoch 5906/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.7786487817764283
Epoch: 5906, Batch Gradient Norm: 26.355055219171575
Epoch: 5906, Batch Gradient Norm after: 22.360677488772566
Epoch 5907/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.777765440940857
Epoch: 5907, Batch Gradient Norm: 26.70290447955018
Epoch: 5907, Batch Gradient Norm after: 22.35509595340848
Epoch 5908/10000, Prediction Accuracy = 59.53000000000001%, Loss = 0.7785567998886108
Epoch: 5908, Batch Gradient Norm: 26.34872622932808
Epoch: 5908, Batch Gradient Norm after: 22.360678455728035
Epoch 5909/10000, Prediction Accuracy = 59.58%, Loss = 0.7776604652404785
Epoch: 5909, Batch Gradient Norm: 26.693386552019334
Epoch: 5909, Batch Gradient Norm after: 22.351587439194102
Epoch 5910/10000, Prediction Accuracy = 59.52%, Loss = 0.7784216284751893
Epoch: 5910, Batch Gradient Norm: 26.35012370512567
Epoch: 5910, Batch Gradient Norm after: 22.360677950880945
Epoch 5911/10000, Prediction Accuracy = 59.58%, Loss = 0.7775824546813965
Epoch: 5911, Batch Gradient Norm: 26.697353838609533
Epoch: 5911, Batch Gradient Norm after: 22.35906531330861
Epoch 5912/10000, Prediction Accuracy = 59.556%, Loss = 0.778359854221344
Epoch: 5912, Batch Gradient Norm: 26.336334104321903
Epoch: 5912, Batch Gradient Norm after: 22.36067798400225
Epoch 5913/10000, Prediction Accuracy = 59.6%, Loss = 0.777504813671112
Epoch: 5913, Batch Gradient Norm: 26.67092197946433
Epoch: 5913, Batch Gradient Norm after: 22.345355444621177
Epoch 5914/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7782109975814819
Epoch: 5914, Batch Gradient Norm: 26.34516061812243
Epoch: 5914, Batch Gradient Norm after: 22.3606800441942
Epoch 5915/10000, Prediction Accuracy = 59.568000000000005%, Loss = 0.7774157881736755
Epoch: 5915, Batch Gradient Norm: 26.677760975045356
Epoch: 5915, Batch Gradient Norm after: 22.36067806270932
Epoch 5916/10000, Prediction Accuracy = 59.572%, Loss = 0.7781176805496216
Epoch: 5916, Batch Gradient Norm: 26.327438802576953
Epoch: 5916, Batch Gradient Norm after: 22.360677947281268
Epoch 5917/10000, Prediction Accuracy = 59.586%, Loss = 0.7772584199905396
Epoch: 5917, Batch Gradient Norm: 26.659927666680286
Epoch: 5917, Batch Gradient Norm after: 22.347470545525653
Epoch 5918/10000, Prediction Accuracy = 59.544%, Loss = 0.7779966950416565
Epoch: 5918, Batch Gradient Norm: 26.33563146970738
Epoch: 5918, Batch Gradient Norm after: 22.36067752851945
Epoch 5919/10000, Prediction Accuracy = 59.568%, Loss = 0.7771840333938599
Epoch: 5919, Batch Gradient Norm: 26.665504992475466
Epoch: 5919, Batch Gradient Norm after: 22.36067941979405
Epoch 5920/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7779098153114319
Epoch: 5920, Batch Gradient Norm: 26.31893936741397
Epoch: 5920, Batch Gradient Norm after: 22.360677606406803
Epoch 5921/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7770625233650208
Epoch: 5921, Batch Gradient Norm: 26.644328705242096
Epoch: 5921, Batch Gradient Norm after: 22.34636678145309
Epoch 5922/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.7777739405632019
Epoch: 5922, Batch Gradient Norm: 26.32979145071061
Epoch: 5922, Batch Gradient Norm after: 22.360680002833348
Epoch 5923/10000, Prediction Accuracy = 59.614%, Loss = 0.7770534753799438
Epoch: 5923, Batch Gradient Norm: 26.6478566458645
Epoch: 5923, Batch Gradient Norm after: 22.360678851868354
Epoch 5924/10000, Prediction Accuracy = 59.524%, Loss = 0.7777122616767883
Epoch: 5924, Batch Gradient Norm: 26.31112510512354
Epoch: 5924, Batch Gradient Norm after: 22.360679021121317
Epoch 5925/10000, Prediction Accuracy = 59.58399999999999%, Loss = 0.7769133925437928
Epoch: 5925, Batch Gradient Norm: 26.63687751046878
Epoch: 5925, Batch Gradient Norm after: 22.35108832897623
Epoch 5926/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7775652885437012
Epoch: 5926, Batch Gradient Norm: 26.315286252888686
Epoch: 5926, Batch Gradient Norm after: 22.36067867891786
Epoch 5927/10000, Prediction Accuracy = 59.58%, Loss = 0.776784086227417
Epoch: 5927, Batch Gradient Norm: 26.63854272315427
Epoch: 5927, Batch Gradient Norm after: 22.35805287068239
Epoch 5928/10000, Prediction Accuracy = 59.54%, Loss = 0.7775040149688721
Epoch: 5928, Batch Gradient Norm: 26.305645078468864
Epoch: 5928, Batch Gradient Norm after: 22.36067832396979
Epoch 5929/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.7766729474067688
Epoch: 5929, Batch Gradient Norm: 26.621900850057713
Epoch: 5929, Batch Gradient Norm after: 22.35006446035916
Epoch 5930/10000, Prediction Accuracy = 59.536%, Loss = 0.7773574948310852
Epoch: 5930, Batch Gradient Norm: 26.30722803559799
Epoch: 5930, Batch Gradient Norm after: 22.360678372325186
Epoch 5931/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7765873193740844
Epoch: 5931, Batch Gradient Norm: 26.62912863408859
Epoch: 5931, Batch Gradient Norm after: 22.358188156185133
Epoch 5932/10000, Prediction Accuracy = 59.54%, Loss = 0.777288281917572
Epoch: 5932, Batch Gradient Norm: 26.292826436418714
Epoch: 5932, Batch Gradient Norm after: 22.36067985833171
Epoch 5933/10000, Prediction Accuracy = 59.612%, Loss = 0.7765245914459229
Epoch: 5933, Batch Gradient Norm: 26.611212553625716
Epoch: 5933, Batch Gradient Norm after: 22.346667502435512
Epoch 5934/10000, Prediction Accuracy = 59.52%, Loss = 0.7771670103073121
Epoch: 5934, Batch Gradient Norm: 26.298604480164155
Epoch: 5934, Batch Gradient Norm after: 22.360678097621943
Epoch 5935/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.7764323353767395
Epoch: 5935, Batch Gradient Norm: 26.619781578312725
Epoch: 5935, Batch Gradient Norm after: 22.360678706893456
Epoch 5936/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.7770758390426635
Epoch: 5936, Batch Gradient Norm: 26.28281062519639
Epoch: 5936, Batch Gradient Norm after: 22.36067713071757
Epoch 5937/10000, Prediction Accuracy = 59.584%, Loss = 0.7762682676315308
Epoch: 5937, Batch Gradient Norm: 26.59193510185915
Epoch: 5937, Batch Gradient Norm after: 22.34420045743922
Epoch 5938/10000, Prediction Accuracy = 59.548%, Loss = 0.7769274234771728
Epoch: 5938, Batch Gradient Norm: 26.29805985110903
Epoch: 5938, Batch Gradient Norm after: 22.36067869444378
Epoch 5939/10000, Prediction Accuracy = 59.574%, Loss = 0.7761999726295471
Epoch: 5939, Batch Gradient Norm: 26.59902187956486
Epoch: 5939, Batch Gradient Norm after: 22.36067923507173
Epoch 5940/10000, Prediction Accuracy = 59.532%, Loss = 0.7768479585647583
Epoch: 5940, Batch Gradient Norm: 26.27852384317876
Epoch: 5940, Batch Gradient Norm after: 22.360679670902517
Epoch 5941/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.7760722398757934
Epoch: 5941, Batch Gradient Norm: 26.590343425951083
Epoch: 5941, Batch Gradient Norm after: 22.35014402722853
Epoch 5942/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.7767282247543335
Epoch: 5942, Batch Gradient Norm: 26.28015732696357
Epoch: 5942, Batch Gradient Norm after: 22.360679297294254
Epoch 5943/10000, Prediction Accuracy = 59.612%, Loss = 0.7760340929031372
Epoch: 5943, Batch Gradient Norm: 26.599376202981436
Epoch: 5943, Batch Gradient Norm after: 22.359683454717764
Epoch 5944/10000, Prediction Accuracy = 59.52199999999999%, Loss = 0.7766861915588379
Epoch: 5944, Batch Gradient Norm: 26.263666097653456
Epoch: 5944, Batch Gradient Norm after: 22.360678434690403
Epoch 5945/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7759201049804687
Epoch: 5945, Batch Gradient Norm: 26.57040161303045
Epoch: 5945, Batch Gradient Norm after: 22.344072612307833
Epoch 5946/10000, Prediction Accuracy = 59.528%, Loss = 0.7765022277832031
Epoch: 5946, Batch Gradient Norm: 26.27940290044682
Epoch: 5946, Batch Gradient Norm after: 22.360678057955464
Epoch 5947/10000, Prediction Accuracy = 59.576%, Loss = 0.775813615322113
Epoch: 5947, Batch Gradient Norm: 26.570745228138083
Epoch: 5947, Batch Gradient Norm after: 22.36067873751379
Epoch 5948/10000, Prediction Accuracy = 59.568%, Loss = 0.7764246463775635
Epoch: 5948, Batch Gradient Norm: 26.264875147950892
Epoch: 5948, Batch Gradient Norm after: 22.360679459843475
Epoch 5949/10000, Prediction Accuracy = 59.58%, Loss = 0.7756861686706543
Epoch: 5949, Batch Gradient Norm: 26.5672214147381
Epoch: 5949, Batch Gradient Norm after: 22.353295932796208
Epoch 5950/10000, Prediction Accuracy = 59.562%, Loss = 0.7763336300849915
Epoch: 5950, Batch Gradient Norm: 26.260668065305552
Epoch: 5950, Batch Gradient Norm after: 22.360677760494166
Epoch 5951/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7755805253982544
Epoch: 5951, Batch Gradient Norm: 26.561585243167023
Epoch: 5951, Batch Gradient Norm after: 22.3533223400201
Epoch 5952/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7762176156044006
Epoch: 5952, Batch Gradient Norm: 26.254330626149134
Epoch: 5952, Batch Gradient Norm after: 22.360681011002217
Epoch 5953/10000, Prediction Accuracy = 59.602%, Loss = 0.775517213344574
Epoch: 5953, Batch Gradient Norm: 26.555084429403248
Epoch: 5953, Batch Gradient Norm after: 22.35228341535056
Epoch 5954/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7761413097381592
Epoch: 5954, Batch Gradient Norm: 26.24881669606439
Epoch: 5954, Batch Gradient Norm after: 22.36067791623476
Epoch 5955/10000, Prediction Accuracy = 59.634%, Loss = 0.7754583120346069
Epoch: 5955, Batch Gradient Norm: 26.550060244472387
Epoch: 5955, Batch Gradient Norm after: 22.354603938468703
Epoch 5956/10000, Prediction Accuracy = 59.538%, Loss = 0.7760258555412293
Epoch: 5956, Batch Gradient Norm: 26.2457882306775
Epoch: 5956, Batch Gradient Norm after: 22.36067919044125
Epoch 5957/10000, Prediction Accuracy = 59.574%, Loss = 0.7753064751625061
Epoch: 5957, Batch Gradient Norm: 26.54079683413788
Epoch: 5957, Batch Gradient Norm after: 22.35354061607454
Epoch 5958/10000, Prediction Accuracy = 59.562%, Loss = 0.7759066581726074
Epoch: 5958, Batch Gradient Norm: 26.24380476010775
Epoch: 5958, Batch Gradient Norm after: 22.360678977476752
Epoch 5959/10000, Prediction Accuracy = 59.564%, Loss = 0.7751960158348083
Epoch: 5959, Batch Gradient Norm: 26.53323628098565
Epoch: 5959, Batch Gradient Norm after: 22.35467816810001
Epoch 5960/10000, Prediction Accuracy = 59.556%, Loss = 0.7758185148239136
Epoch: 5960, Batch Gradient Norm: 26.237847636694667
Epoch: 5960, Batch Gradient Norm after: 22.360677926208034
Epoch 5961/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7750872015953064
Epoch: 5961, Batch Gradient Norm: 26.527374835833733
Epoch: 5961, Batch Gradient Norm after: 22.35263437093122
Epoch 5962/10000, Prediction Accuracy = 59.548%, Loss = 0.7756893873214722
Epoch: 5962, Batch Gradient Norm: 26.234829599363565
Epoch: 5962, Batch Gradient Norm after: 22.360678509591708
Epoch 5963/10000, Prediction Accuracy = 59.612%, Loss = 0.7750103712081909
Epoch: 5963, Batch Gradient Norm: 26.5266189731429
Epoch: 5963, Batch Gradient Norm after: 22.353583076919694
Epoch 5964/10000, Prediction Accuracy = 59.536%, Loss = 0.7756165862083435
Epoch: 5964, Batch Gradient Norm: 26.228154450491676
Epoch: 5964, Batch Gradient Norm after: 22.36067730882741
Epoch 5965/10000, Prediction Accuracy = 59.628%, Loss = 0.77495938539505
Epoch: 5965, Batch Gradient Norm: 26.506358305586836
Epoch: 5965, Batch Gradient Norm after: 22.34903538459769
Epoch 5966/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.7754892230033874
Epoch: 5966, Batch Gradient Norm: 26.230173494354815
Epoch: 5966, Batch Gradient Norm after: 22.360677190160768
Epoch 5967/10000, Prediction Accuracy = 59.604%, Loss = 0.7748283743858337
Epoch: 5967, Batch Gradient Norm: 26.5143992561979
Epoch: 5967, Batch Gradient Norm after: 22.35917265667445
Epoch 5968/10000, Prediction Accuracy = 59.56%, Loss = 0.7753981947898865
Epoch: 5968, Batch Gradient Norm: 26.2153563604103
Epoch: 5968, Batch Gradient Norm after: 22.360679021939713
Epoch 5969/10000, Prediction Accuracy = 59.568%, Loss = 0.7746863007545471
Epoch: 5969, Batch Gradient Norm: 26.483587175362512
Epoch: 5969, Batch Gradient Norm after: 22.34266925088711
Epoch 5970/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7752553462982178
Epoch: 5970, Batch Gradient Norm: 26.228780020315
Epoch: 5970, Batch Gradient Norm after: 22.360677972579122
Epoch 5971/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7746196031570435
Epoch: 5971, Batch Gradient Norm: 26.49230704010629
Epoch: 5971, Batch Gradient Norm after: 22.36067707495289
Epoch 5972/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7751679658889771
Epoch: 5972, Batch Gradient Norm: 26.209946836544674
Epoch: 5972, Batch Gradient Norm after: 22.360677794793375
Epoch 5973/10000, Prediction Accuracy = 59.61199999999999%, Loss = 0.7745025753974915
Epoch: 5973, Batch Gradient Norm: 26.480098812797653
Epoch: 5973, Batch Gradient Norm after: 22.347351546939358
Epoch 5974/10000, Prediction Accuracy = 59.517999999999994%, Loss = 0.7750600576400757
Epoch: 5974, Batch Gradient Norm: 26.21383887319204
Epoch: 5974, Batch Gradient Norm after: 22.36067736303748
Epoch 5975/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7744796872138977
Epoch: 5975, Batch Gradient Norm: 26.48841143010826
Epoch: 5975, Batch Gradient Norm after: 22.36067857540572
Epoch 5976/10000, Prediction Accuracy = 59.52%, Loss = 0.7750066876411438
Epoch: 5976, Batch Gradient Norm: 26.1939159906298
Epoch: 5976, Batch Gradient Norm after: 22.360677271585477
Epoch 5977/10000, Prediction Accuracy = 59.626%, Loss = 0.7743202447891235
Epoch: 5977, Batch Gradient Norm: 26.459285032306745
Epoch: 5977, Batch Gradient Norm after: 22.342627058585496
Epoch 5978/10000, Prediction Accuracy = 59.548%, Loss = 0.7748152613639832
Epoch: 5978, Batch Gradient Norm: 26.21452210141351
Epoch: 5978, Batch Gradient Norm after: 22.360677545658667
Epoch 5979/10000, Prediction Accuracy = 59.574%, Loss = 0.7742345929145813
Epoch: 5979, Batch Gradient Norm: 26.460166433050688
Epoch: 5979, Batch Gradient Norm after: 22.360677213702637
Epoch 5980/10000, Prediction Accuracy = 59.568%, Loss = 0.7747527956962585
Epoch: 5980, Batch Gradient Norm: 26.195992156209225
Epoch: 5980, Batch Gradient Norm after: 22.360678906373813
Epoch 5981/10000, Prediction Accuracy = 59.576%, Loss = 0.7741104960441589
Epoch: 5981, Batch Gradient Norm: 26.457659289989916
Epoch: 5981, Batch Gradient Norm after: 22.353143316330762
Epoch 5982/10000, Prediction Accuracy = 59.556%, Loss = 0.7746472716331482
Epoch: 5982, Batch Gradient Norm: 26.193794035714866
Epoch: 5982, Batch Gradient Norm after: 22.360678083758323
Epoch 5983/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7740122318267822
Epoch: 5983, Batch Gradient Norm: 26.45779312998556
Epoch: 5983, Batch Gradient Norm after: 22.354770930407426
Epoch 5984/10000, Prediction Accuracy = 59.536%, Loss = 0.7745591044425965
Epoch: 5984, Batch Gradient Norm: 26.187243130334092
Epoch: 5984, Batch Gradient Norm after: 22.360678827283387
Epoch 5985/10000, Prediction Accuracy = 59.64%, Loss = 0.7739727854728699
Epoch: 5985, Batch Gradient Norm: 26.450850356792472
Epoch: 5985, Batch Gradient Norm after: 22.35491023342919
Epoch 5986/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.7744750499725341
Epoch: 5986, Batch Gradient Norm: 26.18120485059413
Epoch: 5986, Batch Gradient Norm after: 22.36067632081176
Epoch 5987/10000, Prediction Accuracy = 59.63799999999999%, Loss = 0.7738553404808044
Epoch: 5987, Batch Gradient Norm: 26.443443190197044
Epoch: 5987, Batch Gradient Norm after: 22.354838561359184
Epoch 5988/10000, Prediction Accuracy = 59.532%, Loss = 0.7743400454521179
Epoch: 5988, Batch Gradient Norm: 26.180070763452587
Epoch: 5988, Batch Gradient Norm after: 22.360678931243694
Epoch 5989/10000, Prediction Accuracy = 59.584%, Loss = 0.7737172603607178
Epoch: 5989, Batch Gradient Norm: 26.436580803122094
Epoch: 5989, Batch Gradient Norm after: 22.354172529783114
Epoch 5990/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.7742502331733704
Epoch: 5990, Batch Gradient Norm: 26.176047412757292
Epoch: 5990, Batch Gradient Norm after: 22.360680051103255
Epoch 5991/10000, Prediction Accuracy = 59.568%, Loss = 0.7736183404922485
Epoch: 5991, Batch Gradient Norm: 26.427037003943532
Epoch: 5991, Batch Gradient Norm after: 22.353250790476345
Epoch 5992/10000, Prediction Accuracy = 59.574%, Loss = 0.7741352796554566
Epoch: 5992, Batch Gradient Norm: 26.171614661928945
Epoch: 5992, Batch Gradient Norm after: 22.36067943588405
Epoch 5993/10000, Prediction Accuracy = 59.598%, Loss = 0.7735231161117554
Epoch: 5993, Batch Gradient Norm: 26.423200891505516
Epoch: 5993, Batch Gradient Norm after: 22.353859936265735
Epoch 5994/10000, Prediction Accuracy = 59.55799999999999%, Loss = 0.7740371227264404
Epoch: 5994, Batch Gradient Norm: 26.167788862144807
Epoch: 5994, Batch Gradient Norm after: 22.360678101752537
Epoch 5995/10000, Prediction Accuracy = 59.64%, Loss = 0.7734795570373535
Epoch: 5995, Batch Gradient Norm: 26.415827889088447
Epoch: 5995, Batch Gradient Norm after: 22.355066566939342
Epoch 5996/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7739672064781189
Epoch: 5996, Batch Gradient Norm: 26.161014653300356
Epoch: 5996, Batch Gradient Norm after: 22.360676619422335
Epoch 5997/10000, Prediction Accuracy = 59.644000000000005%, Loss = 0.7733719110488891
Epoch: 5997, Batch Gradient Norm: 26.409468505101504
Epoch: 5997, Batch Gradient Norm after: 22.354417485587657
Epoch 5998/10000, Prediction Accuracy = 59.548%, Loss = 0.7738304138183594
Epoch: 5998, Batch Gradient Norm: 26.158238411325765
Epoch: 5998, Batch Gradient Norm after: 22.360678704152157
Epoch 5999/10000, Prediction Accuracy = 59.612%, Loss = 0.7732351660728455
Epoch: 5999, Batch Gradient Norm: 26.40267608854208
Epoch: 5999, Batch Gradient Norm after: 22.353303938038582
Epoch 6000/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7737336158752441
Epoch: 6000, Batch Gradient Norm: 26.154269890203267
Epoch: 6000, Batch Gradient Norm after: 22.360676485759743
Epoch 6001/10000, Prediction Accuracy = 59.56%, Loss = 0.7731398820877076
Epoch: 6001, Batch Gradient Norm: 26.398198424125088
Epoch: 6001, Batch Gradient Norm after: 22.3560640186814
Epoch 6002/10000, Prediction Accuracy = 59.572%, Loss = 0.7736393570899963
Epoch: 6002, Batch Gradient Norm: 26.145668671732285
Epoch: 6002, Batch Gradient Norm after: 22.360676657070325
Epoch 6003/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.77302485704422
Epoch: 6003, Batch Gradient Norm: 26.386944544327786
Epoch: 6003, Batch Gradient Norm after: 22.349142730824774
Epoch 6004/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7735116124153137
Epoch: 6004, Batch Gradient Norm: 26.15007850062867
Epoch: 6004, Batch Gradient Norm after: 22.360677492203557
Epoch 6005/10000, Prediction Accuracy = 59.64399999999999%, Loss = 0.7729930281639099
Epoch: 6005, Batch Gradient Norm: 26.392302011979975
Epoch: 6005, Batch Gradient Norm after: 22.359716972842353
Epoch 6006/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7734790205955505
Epoch: 6006, Batch Gradient Norm: 26.130488366715944
Epoch: 6006, Batch Gradient Norm after: 22.360676781032986
Epoch 6007/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7728902578353882
Epoch: 6007, Batch Gradient Norm: 26.36348956263953
Epoch: 6007, Batch Gradient Norm after: 22.34404680152693
Epoch 6008/10000, Prediction Accuracy = 59.538%, Loss = 0.7732889771461486
Epoch: 6008, Batch Gradient Norm: 26.146655351547796
Epoch: 6008, Batch Gradient Norm after: 22.360677214712993
Epoch 6009/10000, Prediction Accuracy = 59.620000000000005%, Loss = 0.7727814197540284
Epoch: 6009, Batch Gradient Norm: 26.368835174238136
Epoch: 6009, Batch Gradient Norm after: 22.360677280872537
Epoch 6010/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7732237458229065
Epoch: 6010, Batch Gradient Norm: 26.12818224382992
Epoch: 6010, Batch Gradient Norm after: 22.360676574000642
Epoch 6011/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7726558923721314
Epoch: 6011, Batch Gradient Norm: 26.351534983948916
Epoch: 6011, Batch Gradient Norm after: 22.348050745059503
Epoch 6012/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.7731066584587097
Epoch: 6012, Batch Gradient Norm: 26.1344915445784
Epoch: 6012, Batch Gradient Norm after: 22.360677844571082
Epoch 6013/10000, Prediction Accuracy = 59.592%, Loss = 0.7725666880607605
Epoch: 6013, Batch Gradient Norm: 26.36295788639801
Epoch: 6013, Batch Gradient Norm after: 22.360678464785522
Epoch 6014/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7730366230010987
Epoch: 6014, Batch Gradient Norm: 26.11709593944889
Epoch: 6014, Batch Gradient Norm after: 22.360679266517284
Epoch 6015/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.7724724650382996
Epoch: 6015, Batch Gradient Norm: 26.336849694646272
Epoch: 6015, Batch Gradient Norm after: 22.343110679046962
Epoch 6016/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.7729082942008972
Epoch: 6016, Batch Gradient Norm: 26.13243478568083
Epoch: 6016, Batch Gradient Norm after: 22.360678134543402
Epoch 6017/10000, Prediction Accuracy = 59.624%, Loss = 0.7724501967430115
Epoch: 6017, Batch Gradient Norm: 26.33828560941513
Epoch: 6017, Batch Gradient Norm after: 22.360678435952362
Epoch 6018/10000, Prediction Accuracy = 59.548%, Loss = 0.7728107571601868
Epoch: 6018, Batch Gradient Norm: 26.114628154280666
Epoch: 6018, Batch Gradient Norm after: 22.360679806349534
Epoch 6019/10000, Prediction Accuracy = 59.61%, Loss = 0.7722809076309204
Epoch: 6019, Batch Gradient Norm: 26.338223973118566
Epoch: 6019, Batch Gradient Norm after: 22.35459798862644
Epoch 6020/10000, Prediction Accuracy = 59.56199999999999%, Loss = 0.772707486152649
Epoch: 6020, Batch Gradient Norm: 26.108759281183907
Epoch: 6020, Batch Gradient Norm after: 22.36067885271252
Epoch 6021/10000, Prediction Accuracy = 59.55%, Loss = 0.7721663475036621
Epoch: 6021, Batch Gradient Norm: 26.328321662046626
Epoch: 6021, Batch Gradient Norm after: 22.353742784653623
Epoch 6022/10000, Prediction Accuracy = 59.6%, Loss = 0.7726174235343933
Epoch: 6022, Batch Gradient Norm: 26.105591973711665
Epoch: 6022, Batch Gradient Norm after: 22.360677697151772
Epoch 6023/10000, Prediction Accuracy = 59.572%, Loss = 0.7720697164535523
Epoch: 6023, Batch Gradient Norm: 26.327107440508996
Epoch: 6023, Batch Gradient Norm after: 22.355141659902667
Epoch 6024/10000, Prediction Accuracy = 59.552%, Loss = 0.7724989175796508
Epoch: 6024, Batch Gradient Norm: 26.102813323714624
Epoch: 6024, Batch Gradient Norm after: 22.360678808448522
Epoch 6025/10000, Prediction Accuracy = 59.666%, Loss = 0.7719837665557862
Epoch: 6025, Batch Gradient Norm: 26.32076039265933
Epoch: 6025, Batch Gradient Norm after: 22.35400812529015
Epoch 6026/10000, Prediction Accuracy = 59.562%, Loss = 0.7724185228347779
Epoch: 6026, Batch Gradient Norm: 26.09951700201173
Epoch: 6026, Batch Gradient Norm after: 22.360677654860538
Epoch 6027/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7719422340393066
Epoch: 6027, Batch Gradient Norm: 26.311810329871587
Epoch: 6027, Batch Gradient Norm after: 22.354683862561266
Epoch 6028/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7723143100738525
Epoch: 6028, Batch Gradient Norm: 26.09476631489936
Epoch: 6028, Batch Gradient Norm after: 22.360678468115697
Epoch 6029/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7718185186386108
Epoch: 6029, Batch Gradient Norm: 26.309803186562227
Epoch: 6029, Batch Gradient Norm after: 22.356114435350317
Epoch 6030/10000, Prediction Accuracy = 59.56199999999999%, Loss = 0.7721875905990601
Epoch: 6030, Batch Gradient Norm: 26.08759355484639
Epoch: 6030, Batch Gradient Norm after: 22.360679494562337
Epoch 6031/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.7716835021972657
Epoch: 6031, Batch Gradient Norm: 26.294669163959
Epoch: 6031, Batch Gradient Norm after: 22.351500003673994
Epoch 6032/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7720890879631043
Epoch: 6032, Batch Gradient Norm: 26.087622962376287
Epoch: 6032, Batch Gradient Norm after: 22.36067757772558
Epoch 6033/10000, Prediction Accuracy = 59.56199999999999%, Loss = 0.7716026663780212
Epoch: 6033, Batch Gradient Norm: 26.30032604412423
Epoch: 6033, Batch Gradient Norm after: 22.358578390075913
Epoch 6034/10000, Prediction Accuracy = 59.58%, Loss = 0.7719925999641418
Epoch: 6034, Batch Gradient Norm: 26.079074668183516
Epoch: 6034, Batch Gradient Norm after: 22.360679249208104
Epoch 6035/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.7714872837066651
Epoch: 6035, Batch Gradient Norm: 26.28243205883994
Epoch: 6035, Batch Gradient Norm after: 22.34714063925953
Epoch 6036/10000, Prediction Accuracy = 59.564%, Loss = 0.771871018409729
Epoch: 6036, Batch Gradient Norm: 26.08709459187005
Epoch: 6036, Batch Gradient Norm after: 22.360678893521662
Epoch 6037/10000, Prediction Accuracy = 59.65599999999999%, Loss = 0.7714807510375976
Epoch: 6037, Batch Gradient Norm: 26.28549039700393
Epoch: 6037, Batch Gradient Norm after: 22.360679777734102
Epoch 6038/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.7718176484107971
Epoch: 6038, Batch Gradient Norm: 26.070267106971492
Epoch: 6038, Batch Gradient Norm after: 22.360679403431277
Epoch 6039/10000, Prediction Accuracy = 59.648%, Loss = 0.7713409543037415
Epoch: 6039, Batch Gradient Norm: 26.27519127285614
Epoch: 6039, Batch Gradient Norm after: 22.351445627080256
Epoch 6040/10000, Prediction Accuracy = 59.576%, Loss = 0.7716650247573853
Epoch: 6040, Batch Gradient Norm: 26.069156497865215
Epoch: 6040, Batch Gradient Norm after: 22.36067891642136
Epoch 6041/10000, Prediction Accuracy = 59.60600000000001%, Loss = 0.7712140440940857
Epoch: 6041, Batch Gradient Norm: 26.274592114290037
Epoch: 6041, Batch Gradient Norm after: 22.357247347983026
Epoch 6042/10000, Prediction Accuracy = 59.586%, Loss = 0.7715920329093933
Epoch: 6042, Batch Gradient Norm: 26.060438412839794
Epoch: 6042, Batch Gradient Norm after: 22.36067762049313
Epoch 6043/10000, Prediction Accuracy = 59.574%, Loss = 0.7711120247840881
Epoch: 6043, Batch Gradient Norm: 26.260913576836202
Epoch: 6043, Batch Gradient Norm after: 22.351087212803787
Epoch 6044/10000, Prediction Accuracy = 59.598%, Loss = 0.7714641571044922
Epoch: 6044, Batch Gradient Norm: 26.06091079674826
Epoch: 6044, Batch Gradient Norm after: 22.36067843562261
Epoch 6045/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7710197806358338
Epoch: 6045, Batch Gradient Norm: 26.264902275759034
Epoch: 6045, Batch Gradient Norm after: 22.356316559781
Epoch 6046/10000, Prediction Accuracy = 59.598%, Loss = 0.7713868021965027
Epoch: 6046, Batch Gradient Norm: 26.05500376143352
Epoch: 6046, Batch Gradient Norm after: 22.360679758480718
Epoch 6047/10000, Prediction Accuracy = 59.648%, Loss = 0.7709622621536255
Epoch: 6047, Batch Gradient Norm: 26.24752408435732
Epoch: 6047, Batch Gradient Norm after: 22.349403395622957
Epoch 6048/10000, Prediction Accuracy = 59.538%, Loss = 0.7712811827659607
Epoch: 6048, Batch Gradient Norm: 26.057664854099762
Epoch: 6048, Batch Gradient Norm after: 22.360677474043936
Epoch 6049/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.7708898544311523
Epoch: 6049, Batch Gradient Norm: 26.25247714629602
Epoch: 6049, Batch Gradient Norm after: 22.360680813394257
Epoch 6050/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7711833000183106
Epoch: 6050, Batch Gradient Norm: 26.043754957762765
Epoch: 6050, Batch Gradient Norm after: 22.360679093808987
Epoch 6051/10000, Prediction Accuracy = 59.612%, Loss = 0.7707263350486755
Epoch: 6051, Batch Gradient Norm: 26.236485785517488
Epoch: 6051, Batch Gradient Norm after: 22.349696449488885
Epoch 6052/10000, Prediction Accuracy = 59.602%, Loss = 0.7710537672042846
Epoch: 6052, Batch Gradient Norm: 26.049829206586683
Epoch: 6052, Batch Gradient Norm after: 22.360677562179298
Epoch 6053/10000, Prediction Accuracy = 59.588%, Loss = 0.7706512689590455
Epoch: 6053, Batch Gradient Norm: 26.244107799173424
Epoch: 6053, Batch Gradient Norm after: 22.360677974067954
Epoch 6054/10000, Prediction Accuracy = 59.612%, Loss = 0.770989966392517
Epoch: 6054, Batch Gradient Norm: 26.033854400207673
Epoch: 6054, Batch Gradient Norm after: 22.360679349884244
Epoch 6055/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.7705190300941467
Epoch: 6055, Batch Gradient Norm: 26.21962088747532
Epoch: 6055, Batch Gradient Norm after: 22.34432291715465
Epoch 6056/10000, Prediction Accuracy = 59.58200000000001%, Loss = 0.7708277940750122
Epoch: 6056, Batch Gradient Norm: 26.05080896778406
Epoch: 6056, Batch Gradient Norm after: 22.36067894150172
Epoch 6057/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.770508325099945
Epoch: 6057, Batch Gradient Norm: 26.224246041933135
Epoch: 6057, Batch Gradient Norm after: 22.360678092027328
Epoch 6058/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7707941174507141
Epoch: 6058, Batch Gradient Norm: 26.031273786715822
Epoch: 6058, Batch Gradient Norm after: 22.36067827904807
Epoch 6059/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7704104423522949
Epoch: 6059, Batch Gradient Norm: 26.213164337230435
Epoch: 6059, Batch Gradient Norm after: 22.35284056476456
Epoch 6060/10000, Prediction Accuracy = 59.553999999999995%, Loss = 0.7706679105758667
Epoch: 6060, Batch Gradient Norm: 26.03065266490276
Epoch: 6060, Batch Gradient Norm after: 22.360677068227407
Epoch 6061/10000, Prediction Accuracy = 59.636%, Loss = 0.7702731251716614
Epoch: 6061, Batch Gradient Norm: 26.218301066133854
Epoch: 6061, Batch Gradient Norm after: 22.35731023971332
Epoch 6062/10000, Prediction Accuracy = 59.596000000000004%, Loss = 0.7705732107162475
Epoch: 6062, Batch Gradient Norm: 26.020833595015965
Epoch: 6062, Batch Gradient Norm after: 22.360679558012503
Epoch 6063/10000, Prediction Accuracy = 59.6%, Loss = 0.7701541185379028
Epoch: 6063, Batch Gradient Norm: 26.198276756674094
Epoch: 6063, Batch Gradient Norm after: 22.348957677679916
Epoch 6064/10000, Prediction Accuracy = 59.638%, Loss = 0.7704570770263672
Epoch: 6064, Batch Gradient Norm: 26.027619244285898
Epoch: 6064, Batch Gradient Norm after: 22.360679534588463
Epoch 6065/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.7700752139091491
Epoch: 6065, Batch Gradient Norm: 26.208345558129306
Epoch: 6065, Batch Gradient Norm after: 22.360677279406566
Epoch 6066/10000, Prediction Accuracy = 59.584%, Loss = 0.7703689455986023
Epoch: 6066, Batch Gradient Norm: 26.012192385316315
Epoch: 6066, Batch Gradient Norm after: 22.360680265888025
Epoch 6067/10000, Prediction Accuracy = 59.67%, Loss = 0.7699739456176757
Epoch: 6067, Batch Gradient Norm: 26.179307904547613
Epoch: 6067, Batch Gradient Norm after: 22.342248987611963
Epoch 6068/10000, Prediction Accuracy = 59.552%, Loss = 0.7702450156211853
Epoch: 6068, Batch Gradient Norm: 26.028931515279915
Epoch: 6068, Batch Gradient Norm after: 22.360679505434735
Epoch 6069/10000, Prediction Accuracy = 59.652%, Loss = 0.7699667692184449
Epoch: 6069, Batch Gradient Norm: 26.183169594585998
Epoch: 6069, Batch Gradient Norm after: 22.36067964894999
Epoch 6070/10000, Prediction Accuracy = 59.568%, Loss = 0.7701632499694824
Epoch: 6070, Batch Gradient Norm: 26.010723388723903
Epoch: 6070, Batch Gradient Norm after: 22.360680008715256
Epoch 6071/10000, Prediction Accuracy = 59.668000000000006%, Loss = 0.7698100447654724
Epoch: 6071, Batch Gradient Norm: 26.18312815911388
Epoch: 6071, Batch Gradient Norm after: 22.354963676722704
Epoch 6072/10000, Prediction Accuracy = 59.592000000000006%, Loss = 0.7700526595115662
Epoch: 6072, Batch Gradient Norm: 26.00542883241972
Epoch: 6072, Batch Gradient Norm after: 22.360677334755312
Epoch 6073/10000, Prediction Accuracy = 59.6%, Loss = 0.769683575630188
Epoch: 6073, Batch Gradient Norm: 26.173601954608117
Epoch: 6073, Batch Gradient Norm after: 22.352032657859404
Epoch 6074/10000, Prediction Accuracy = 59.644000000000005%, Loss = 0.7699649333953857
Epoch: 6074, Batch Gradient Norm: 26.002153000142883
Epoch: 6074, Batch Gradient Norm after: 22.360678543949646
Epoch 6075/10000, Prediction Accuracy = 59.604%, Loss = 0.7695956349372863
Epoch: 6075, Batch Gradient Norm: 26.164610020827695
Epoch: 6075, Batch Gradient Norm after: 22.353248873224256
Epoch 6076/10000, Prediction Accuracy = 59.60799999999999%, Loss = 0.769845986366272
Epoch: 6076, Batch Gradient Norm: 26.000663646720238
Epoch: 6076, Batch Gradient Norm after: 22.36067689454844
Epoch 6077/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.7695069551467896
Epoch: 6077, Batch Gradient Norm: 26.163315181895744
Epoch: 6077, Batch Gradient Norm after: 22.354027809287274
Epoch 6078/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.7697689414024353
Epoch: 6078, Batch Gradient Norm: 25.994081359109916
Epoch: 6078, Batch Gradient Norm after: 22.360676790830233
Epoch 6079/10000, Prediction Accuracy = 59.678%, Loss = 0.7694638967514038
Epoch: 6079, Batch Gradient Norm: 26.15440846866991
Epoch: 6079, Batch Gradient Norm after: 22.350807093729934
Epoch 6080/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.7696696400642395
Epoch: 6080, Batch Gradient Norm: 25.99242894908526
Epoch: 6080, Batch Gradient Norm after: 22.360679772151528
Epoch 6081/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7693593144416809
Epoch: 6081, Batch Gradient Norm: 26.151004126749733
Epoch: 6081, Batch Gradient Norm after: 22.355767603975565
Epoch 6082/10000, Prediction Accuracy = 59.6%, Loss = 0.7695548892021179
Epoch: 6082, Batch Gradient Norm: 25.981365699102454
Epoch: 6082, Batch Gradient Norm after: 22.360678188529707
Epoch 6083/10000, Prediction Accuracy = 59.626%, Loss = 0.7692114114761353
Epoch: 6083, Batch Gradient Norm: 26.138711886572548
Epoch: 6083, Batch Gradient Norm after: 22.348696773135032
Epoch 6084/10000, Prediction Accuracy = 59.636%, Loss = 0.7694543361663818
Epoch: 6084, Batch Gradient Norm: 25.98592021145732
Epoch: 6084, Batch Gradient Norm after: 22.360677797684662
Epoch 6085/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.7691370487213135
Epoch: 6085, Batch Gradient Norm: 26.13798124236644
Epoch: 6085, Batch Gradient Norm after: 22.355240589098482
Epoch 6086/10000, Prediction Accuracy = 59.64%, Loss = 0.7693598866462708
Epoch: 6086, Batch Gradient Norm: 25.975283161266795
Epoch: 6086, Batch Gradient Norm after: 22.360678983911484
Epoch 6087/10000, Prediction Accuracy = 59.64%, Loss = 0.7690158605575561
Epoch: 6087, Batch Gradient Norm: 26.12302578226941
Epoch: 6087, Batch Gradient Norm after: 22.3453182189481
Epoch 6088/10000, Prediction Accuracy = 59.58599999999999%, Loss = 0.7692265391349793
Epoch: 6088, Batch Gradient Norm: 25.983757746983994
Epoch: 6088, Batch Gradient Norm after: 22.36067754667973
Epoch 6089/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.7690058708190918
Epoch: 6089, Batch Gradient Norm: 26.133365659148115
Epoch: 6089, Batch Gradient Norm after: 22.36067852999692
Epoch 6090/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.7692036390304565
Epoch: 6090, Batch Gradient Norm: 25.9601255458841
Epoch: 6090, Batch Gradient Norm after: 22.360679246781388
Epoch 6091/10000, Prediction Accuracy = 59.69%, Loss = 0.7688818454742432
Epoch: 6091, Batch Gradient Norm: 26.110573589613132
Epoch: 6091, Batch Gradient Norm after: 22.34467146980816
Epoch 6092/10000, Prediction Accuracy = 59.56600000000001%, Loss = 0.7690270900726318
Epoch: 6092, Batch Gradient Norm: 25.973665319405548
Epoch: 6092, Batch Gradient Norm after: 22.360676859974642
Epoch 6093/10000, Prediction Accuracy = 59.64399999999999%, Loss = 0.7687648892402649
Epoch: 6093, Batch Gradient Norm: 26.119594682894107
Epoch: 6093, Batch Gradient Norm after: 22.36068018611668
Epoch 6094/10000, Prediction Accuracy = 59.634%, Loss = 0.7689719676971436
Epoch: 6094, Batch Gradient Norm: 25.953266648272717
Epoch: 6094, Batch Gradient Norm after: 22.360680615934402
Epoch 6095/10000, Prediction Accuracy = 59.652%, Loss = 0.7686356663703918
Epoch: 6095, Batch Gradient Norm: 26.095765495618657
Epoch: 6095, Batch Gradient Norm after: 22.344028437744804
Epoch 6096/10000, Prediction Accuracy = 59.646%, Loss = 0.7688343763351441
Epoch: 6096, Batch Gradient Norm: 25.96840834840288
Epoch: 6096, Batch Gradient Norm after: 22.36067623504737
Epoch 6097/10000, Prediction Accuracy = 59.638%, Loss = 0.7685679197311401
Epoch: 6097, Batch Gradient Norm: 26.105480441591304
Epoch: 6097, Batch Gradient Norm after: 22.360680691620654
Epoch 6098/10000, Prediction Accuracy = 59.598%, Loss = 0.7687641739845276
Epoch: 6098, Batch Gradient Norm: 25.949503092646744
Epoch: 6098, Batch Gradient Norm after: 22.360679970783757
Epoch 6099/10000, Prediction Accuracy = 59.698%, Loss = 0.7684786558151245
Epoch: 6099, Batch Gradient Norm: 26.089657103383423
Epoch: 6099, Batch Gradient Norm after: 22.345542383146263
Epoch 6100/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.7686675548553467
Epoch: 6100, Batch Gradient Norm: 25.954232892418243
Epoch: 6100, Batch Gradient Norm after: 22.360677633901037
Epoch 6101/10000, Prediction Accuracy = 59.67800000000001%, Loss = 0.7684421896934509
Epoch: 6101, Batch Gradient Norm: 26.09876017001036
Epoch: 6101, Batch Gradient Norm after: 22.360677255800972
Epoch 6102/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.7685909628868103
Epoch: 6102, Batch Gradient Norm: 25.936860895016796
Epoch: 6102, Batch Gradient Norm after: 22.360680911266098
Epoch 6103/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.7682613253593444
Epoch: 6103, Batch Gradient Norm: 26.066767274097586
Epoch: 6103, Batch Gradient Norm after: 22.339420388846392
Epoch 6104/10000, Prediction Accuracy = 59.626%, Loss = 0.7684230446815491
Epoch: 6104, Batch Gradient Norm: 25.95493003544315
Epoch: 6104, Batch Gradient Norm after: 22.360678787618312
Epoch 6105/10000, Prediction Accuracy = 59.638%, Loss = 0.7682053089141846
Epoch: 6105, Batch Gradient Norm: 26.076277196780893
Epoch: 6105, Batch Gradient Norm after: 22.360679357169893
Epoch 6106/10000, Prediction Accuracy = 59.65%, Loss = 0.7683733224868774
Epoch: 6106, Batch Gradient Norm: 25.931923191468115
Epoch: 6106, Batch Gradient Norm after: 22.360677862811045
Epoch 6107/10000, Prediction Accuracy = 59.646%, Loss = 0.7680683016777039
Epoch: 6107, Batch Gradient Norm: 26.06208754988707
Epoch: 6107, Batch Gradient Norm after: 22.345088637148468
Epoch 6108/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7682368516921997
Epoch: 6108, Batch Gradient Norm: 25.94199661127141
Epoch: 6108, Batch Gradient Norm after: 22.360677946397
Epoch 6109/10000, Prediction Accuracy = 59.688%, Loss = 0.7680274605751037
Epoch: 6109, Batch Gradient Norm: 26.07300471268866
Epoch: 6109, Batch Gradient Norm after: 22.359302374710044
Epoch 6110/10000, Prediction Accuracy = 59.564%, Loss = 0.7682088971138
Epoch: 6110, Batch Gradient Norm: 25.920265655078783
Epoch: 6110, Batch Gradient Norm after: 22.360676682449483
Epoch 6111/10000, Prediction Accuracy = 59.684000000000005%, Loss = 0.7679450631141662
Epoch: 6111, Batch Gradient Norm: 26.04166899482008
Epoch: 6111, Batch Gradient Norm after: 22.339551454036254
Epoch 6112/10000, Prediction Accuracy = 59.55%, Loss = 0.7680341124534606
Epoch: 6112, Batch Gradient Norm: 25.936877698602366
Epoch: 6112, Batch Gradient Norm after: 22.360680067271083
Epoch 6113/10000, Prediction Accuracy = 59.666%, Loss = 0.7678516507148743
Epoch: 6113, Batch Gradient Norm: 26.055199698331936
Epoch: 6113, Batch Gradient Norm after: 22.3606777855279
Epoch 6114/10000, Prediction Accuracy = 59.626%, Loss = 0.7679689526557922
Epoch: 6114, Batch Gradient Norm: 25.914890628029816
Epoch: 6114, Batch Gradient Norm after: 22.36067882871439
Epoch 6115/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.767696213722229
Epoch: 6115, Batch Gradient Norm: 26.026229543034898
Epoch: 6115, Batch Gradient Norm after: 22.339072298552644
Epoch 6116/10000, Prediction Accuracy = 59.644000000000005%, Loss = 0.7678327441215516
Epoch: 6116, Batch Gradient Norm: 25.93146138643846
Epoch: 6116, Batch Gradient Norm after: 22.360679467137626
Epoch 6117/10000, Prediction Accuracy = 59.658%, Loss = 0.7676430106163025
Epoch: 6117, Batch Gradient Norm: 26.039954645497627
Epoch: 6117, Batch Gradient Norm after: 22.36067993178219
Epoch 6118/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7677586555480957
Epoch: 6118, Batch Gradient Norm: 25.911676984236397
Epoch: 6118, Batch Gradient Norm after: 22.360677526275413
Epoch 6119/10000, Prediction Accuracy = 59.67999999999999%, Loss = 0.7675216317176818
Epoch: 6119, Batch Gradient Norm: 26.024347268678866
Epoch: 6119, Batch Gradient Norm after: 22.344091514534238
Epoch 6120/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.7676539182662964
Epoch: 6120, Batch Gradient Norm: 25.918466117600627
Epoch: 6120, Batch Gradient Norm after: 22.360676731474342
Epoch 6121/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.7675103187561035
Epoch: 6121, Batch Gradient Norm: 26.039310225745417
Epoch: 6121, Batch Gradient Norm after: 22.36067737284527
Epoch 6122/10000, Prediction Accuracy = 59.556%, Loss = 0.7676186323165893
Epoch: 6122, Batch Gradient Norm: 25.894399376727552
Epoch: 6122, Batch Gradient Norm after: 22.36067769849087
Epoch 6123/10000, Prediction Accuracy = 59.69%, Loss = 0.7673491358757019
Epoch: 6123, Batch Gradient Norm: 25.999910232465545
Epoch: 6123, Batch Gradient Norm after: 22.33526610902612
Epoch 6124/10000, Prediction Accuracy = 59.64200000000001%, Loss = 0.7674062013626098
Epoch: 6124, Batch Gradient Norm: 25.92005723690974
Epoch: 6124, Batch Gradient Norm after: 22.360678517488743
Epoch 6125/10000, Prediction Accuracy = 59.678%, Loss = 0.7672781705856323
Epoch: 6125, Batch Gradient Norm: 26.0110743597699
Epoch: 6125, Batch Gradient Norm after: 22.360679396192065
Epoch 6126/10000, Prediction Accuracy = 59.63399999999999%, Loss = 0.7673632502555847
Epoch: 6126, Batch Gradient Norm: 25.895626701329604
Epoch: 6126, Batch Gradient Norm after: 22.360677668405383
Epoch 6127/10000, Prediction Accuracy = 59.652%, Loss = 0.7671509861946106
Epoch: 6127, Batch Gradient Norm: 26.00082621458533
Epoch: 6127, Batch Gradient Norm after: 22.346911210552424
Epoch 6128/10000, Prediction Accuracy = 59.632000000000005%, Loss = 0.7672457575798035
Epoch: 6128, Batch Gradient Norm: 25.900137994359877
Epoch: 6128, Batch Gradient Norm after: 22.360679571686354
Epoch 6129/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.7670725226402283
Epoch: 6129, Batch Gradient Norm: 26.006931131737616
Epoch: 6129, Batch Gradient Norm after: 22.35664344342622
Epoch 6130/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.7671934485435485
Epoch: 6130, Batch Gradient Norm: 25.886695676234886
Epoch: 6130, Batch Gradient Norm after: 22.36067839794334
Epoch 6131/10000, Prediction Accuracy = 59.688%, Loss = 0.7670050859451294
Epoch: 6131, Batch Gradient Norm: 25.98516889497827
Epoch: 6131, Batch Gradient Norm after: 22.34385403615357
Epoch 6132/10000, Prediction Accuracy = 59.552%, Loss = 0.7670696973800659
Epoch: 6132, Batch Gradient Norm: 25.895895086117417
Epoch: 6132, Batch Gradient Norm after: 22.360677544868476
Epoch 6133/10000, Prediction Accuracy = 59.69%, Loss = 0.7669392466545105
Epoch: 6133, Batch Gradient Norm: 25.99874103633063
Epoch: 6133, Batch Gradient Norm after: 22.360677898476023
Epoch 6134/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.7669949650764465
Epoch: 6134, Batch Gradient Norm: 25.87348880074562
Epoch: 6134, Batch Gradient Norm after: 22.360676740620207
Epoch 6135/10000, Prediction Accuracy = 59.674%, Loss = 0.7667614936828613
Epoch: 6135, Batch Gradient Norm: 25.966644551662668
Epoch: 6135, Batch Gradient Norm after: 22.33854158847598
Epoch 6136/10000, Prediction Accuracy = 59.624%, Loss = 0.7668357014656066
Epoch: 6136, Batch Gradient Norm: 25.893742181109133
Epoch: 6136, Batch Gradient Norm after: 22.360676551040605
Epoch 6137/10000, Prediction Accuracy = 59.65%, Loss = 0.7667224049568176
Epoch: 6137, Batch Gradient Norm: 25.971831302174532
Epoch: 6137, Batch Gradient Norm after: 22.360676306749255
Epoch 6138/10000, Prediction Accuracy = 59.652%, Loss = 0.7667723417282104
Epoch: 6138, Batch Gradient Norm: 25.87216043993354
Epoch: 6138, Batch Gradient Norm after: 22.360677393679495
Epoch 6139/10000, Prediction Accuracy = 59.674%, Loss = 0.7665866851806641
Epoch: 6139, Batch Gradient Norm: 25.96856182537568
Epoch: 6139, Batch Gradient Norm after: 22.349831441709743
Epoch 6140/10000, Prediction Accuracy = 59.62600000000001%, Loss = 0.7666737198829651
Epoch: 6140, Batch Gradient Norm: 25.873588039594424
Epoch: 6140, Batch Gradient Norm after: 22.360676898135747
Epoch 6141/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.7665507435798645
Epoch: 6141, Batch Gradient Norm: 25.96837495659343
Epoch: 6141, Batch Gradient Norm after: 22.354141442955427
Epoch 6142/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.7666289687156678
Epoch: 6142, Batch Gradient Norm: 25.859810552256
Epoch: 6142, Batch Gradient Norm after: 22.3606783845825
Epoch 6143/10000, Prediction Accuracy = 59.684000000000005%, Loss = 0.7664645910263062
Epoch: 6143, Batch Gradient Norm: 25.952401921633623
Epoch: 6143, Batch Gradient Norm after: 22.347632505330868
Epoch 6144/10000, Prediction Accuracy = 59.614%, Loss = 0.7664745330810547
Epoch: 6144, Batch Gradient Norm: 25.862958733747416
Epoch: 6144, Batch Gradient Norm after: 22.36067882917318
Epoch 6145/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.7663268566131591
Epoch: 6145, Batch Gradient Norm: 25.958652096027738
Epoch: 6145, Batch Gradient Norm after: 22.357369945313938
Epoch 6146/10000, Prediction Accuracy = 59.634%, Loss = 0.7664125204086304
Epoch: 6146, Batch Gradient Norm: 25.848259880658983
Epoch: 6146, Batch Gradient Norm after: 22.360675806730704
Epoch 6147/10000, Prediction Accuracy = 59.652%, Loss = 0.7662119269371033
Epoch: 6147, Batch Gradient Norm: 25.932106610274825
Epoch: 6147, Batch Gradient Norm after: 22.345142151837482
Epoch 6148/10000, Prediction Accuracy = 59.64200000000001%, Loss = 0.7662716984748841
Epoch: 6148, Batch Gradient Norm: 25.862705811846258
Epoch: 6148, Batch Gradient Norm after: 22.36067746747532
Epoch 6149/10000, Prediction Accuracy = 59.67999999999999%, Loss = 0.7661475658416748
Epoch: 6149, Batch Gradient Norm: 25.940192776519556
Epoch: 6149, Batch Gradient Norm after: 22.360678199033515
Epoch 6150/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.7661908745765686
Epoch: 6150, Batch Gradient Norm: 25.844204206220518
Epoch: 6150, Batch Gradient Norm after: 22.36067688297327
Epoch 6151/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7660587430000305
Epoch: 6151, Batch Gradient Norm: 25.92468744363647
Epoch: 6151, Batch Gradient Norm after: 22.3472687617064
Epoch 6152/10000, Prediction Accuracy = 59.572%, Loss = 0.766103196144104
Epoch: 6152, Batch Gradient Norm: 25.848401204407597
Epoch: 6152, Batch Gradient Norm after: 22.360678916539058
Epoch 6153/10000, Prediction Accuracy = 59.7%, Loss = 0.7660184025764465
Epoch: 6153, Batch Gradient Norm: 25.932167003579405
Epoch: 6153, Batch Gradient Norm after: 22.360676365560277
Epoch 6154/10000, Prediction Accuracy = 59.61%, Loss = 0.7660224914550782
Epoch: 6154, Batch Gradient Norm: 25.82988234949081
Epoch: 6154, Batch Gradient Norm after: 22.36067943869214
Epoch 6155/10000, Prediction Accuracy = 59.676%, Loss = 0.7658506274223328
Epoch: 6155, Batch Gradient Norm: 25.908770373581845
Epoch: 6155, Batch Gradient Norm after: 22.34524958629953
Epoch 6156/10000, Prediction Accuracy = 59.638%, Loss = 0.7658701062202453
Epoch: 6156, Batch Gradient Norm: 25.844342781854902
Epoch: 6156, Batch Gradient Norm after: 22.360677976291413
Epoch 6157/10000, Prediction Accuracy = 59.678%, Loss = 0.7657862424850463
Epoch: 6157, Batch Gradient Norm: 25.912000967757095
Epoch: 6157, Batch Gradient Norm after: 22.36067944415979
Epoch 6158/10000, Prediction Accuracy = 59.65%, Loss = 0.7658151865005494
Epoch: 6158, Batch Gradient Norm: 25.823179876318363
Epoch: 6158, Batch Gradient Norm after: 22.3606792039794
Epoch 6159/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7656664967536926
Epoch: 6159, Batch Gradient Norm: 25.900600620869433
Epoch: 6159, Batch Gradient Norm after: 22.349382716385026
Epoch 6160/10000, Prediction Accuracy = 59.624%, Loss = 0.7656806826591491
Epoch: 6160, Batch Gradient Norm: 25.832297293434006
Epoch: 6160, Batch Gradient Norm after: 22.360677478927425
Epoch 6161/10000, Prediction Accuracy = 59.688%, Loss = 0.7656104087829589
Epoch: 6161, Batch Gradient Norm: 25.907060265803487
Epoch: 6161, Batch Gradient Norm after: 22.360678614485966
Epoch 6162/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.7656459927558898
Epoch: 6162, Batch Gradient Norm: 25.811945769604893
Epoch: 6162, Batch Gradient Norm after: 22.360678509578264
Epoch 6163/10000, Prediction Accuracy = 59.717999999999996%, Loss = 0.7655461430549622
Epoch: 6163, Batch Gradient Norm: 25.885260339789223
Epoch: 6163, Batch Gradient Norm after: 22.34785316692511
Epoch 6164/10000, Prediction Accuracy = 59.59599999999999%, Loss = 0.7655133485794068
Epoch: 6164, Batch Gradient Norm: 25.822670679197987
Epoch: 6164, Batch Gradient Norm after: 22.360676056484582
Epoch 6165/10000, Prediction Accuracy = 59.702%, Loss = 0.7654351472854615
Epoch: 6165, Batch Gradient Norm: 25.888071286289392
Epoch: 6165, Batch Gradient Norm after: 22.360679600443113
Epoch 6166/10000, Prediction Accuracy = 59.64%, Loss = 0.7654114484786987
Epoch: 6166, Batch Gradient Norm: 25.809630243206335
Epoch: 6166, Batch Gradient Norm after: 22.36067789836576
Epoch 6167/10000, Prediction Accuracy = 59.65999999999999%, Loss = 0.7653037786483765
Epoch: 6167, Batch Gradient Norm: 25.879855474029355
Epoch: 6167, Batch Gradient Norm after: 22.352415957848194
Epoch 6168/10000, Prediction Accuracy = 59.666%, Loss = 0.7653337240219116
Epoch: 6168, Batch Gradient Norm: 25.809626994623535
Epoch: 6168, Batch Gradient Norm after: 22.360677244219765
Epoch 6169/10000, Prediction Accuracy = 59.669999999999995%, Loss = 0.7652162194252015
Epoch: 6169, Batch Gradient Norm: 25.873806592178997
Epoch: 6169, Batch Gradient Norm after: 22.35566019105095
Epoch 6170/10000, Prediction Accuracy = 59.622%, Loss = 0.7652113676071167
Epoch: 6170, Batch Gradient Norm: 25.80582787836555
Epoch: 6170, Batch Gradient Norm after: 22.360678172265597
Epoch 6171/10000, Prediction Accuracy = 59.678%, Loss = 0.7651281476020813
Epoch: 6171, Batch Gradient Norm: 25.869359143906934
Epoch: 6171, Batch Gradient Norm after: 22.353728609007405
Epoch 6172/10000, Prediction Accuracy = 59.614%, Loss = 0.7651307702064514
Epoch: 6172, Batch Gradient Norm: 25.800340677526947
Epoch: 6172, Batch Gradient Norm after: 22.36068018415767
Epoch 6173/10000, Prediction Accuracy = 59.712%, Loss = 0.7650882959365845
Epoch: 6173, Batch Gradient Norm: 25.86422621925464
Epoch: 6173, Batch Gradient Norm after: 22.355941540372566
Epoch 6174/10000, Prediction Accuracy = 59.604%, Loss = 0.76505366563797
Epoch: 6174, Batch Gradient Norm: 25.79211783663179
Epoch: 6174, Batch Gradient Norm after: 22.360679073295174
Epoch 6175/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.7649706602096558
Epoch: 6175, Batch Gradient Norm: 25.85146076366921
Epoch: 6175, Batch Gradient Norm after: 22.351013264066495
Epoch 6176/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.764910089969635
Epoch: 6176, Batch Gradient Norm: 25.79585229954441
Epoch: 6176, Batch Gradient Norm after: 22.360677688418036
Epoch 6177/10000, Prediction Accuracy = 59.674%, Loss = 0.7648558735847473
Epoch: 6177, Batch Gradient Norm: 25.851716253742318
Epoch: 6177, Batch Gradient Norm after: 22.35841071097229
Epoch 6178/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7648476958274841
Epoch: 6178, Batch Gradient Norm: 25.782510717523255
Epoch: 6178, Batch Gradient Norm after: 22.360680256943322
Epoch 6179/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.7647531509399415
Epoch: 6179, Batch Gradient Norm: 25.837548829288643
Epoch: 6179, Batch Gradient Norm after: 22.350643382181115
Epoch 6180/10000, Prediction Accuracy = 59.646%, Loss = 0.7647153973579407
Epoch: 6180, Batch Gradient Norm: 25.788213018221118
Epoch: 6180, Batch Gradient Norm after: 22.360677458580355
Epoch 6181/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.7646723985671997
Epoch: 6181, Batch Gradient Norm: 25.84546187919133
Epoch: 6181, Batch Gradient Norm after: 22.360457810680714
Epoch 6182/10000, Prediction Accuracy = 59.62800000000001%, Loss = 0.7646586060523987
Epoch: 6182, Batch Gradient Norm: 25.77162525010405
Epoch: 6182, Batch Gradient Norm after: 22.360676104790407
Epoch 6183/10000, Prediction Accuracy = 59.709999999999994%, Loss = 0.7646071076393127
Epoch: 6183, Batch Gradient Norm: 25.82089380965869
Epoch: 6183, Batch Gradient Norm after: 22.345659655563153
Epoch 6184/10000, Prediction Accuracy = 59.596000000000004%, Loss = 0.7645385265350342
Epoch: 6184, Batch Gradient Norm: 25.78204307422696
Epoch: 6184, Batch Gradient Norm after: 22.360679309277007
Epoch 6185/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.7645559430122375
Epoch: 6185, Batch Gradient Norm: 25.822291927494604
Epoch: 6185, Batch Gradient Norm after: 22.3606776469672
Epoch 6186/10000, Prediction Accuracy = 59.61999999999999%, Loss = 0.7644424676895142
Epoch: 6186, Batch Gradient Norm: 25.767953356773855
Epoch: 6186, Batch Gradient Norm after: 22.36067654585117
Epoch 6187/10000, Prediction Accuracy = 59.682%, Loss = 0.7643889665603638
Epoch: 6187, Batch Gradient Norm: 25.810075148088135
Epoch: 6187, Batch Gradient Norm after: 22.350025730566887
Epoch 6188/10000, Prediction Accuracy = 59.64200000000001%, Loss = 0.7643300294876099
Epoch: 6188, Batch Gradient Norm: 25.77273932524351
Epoch: 6188, Batch Gradient Norm after: 22.360677716840865
Epoch 6189/10000, Prediction Accuracy = 59.674%, Loss = 0.764319658279419
Epoch: 6189, Batch Gradient Norm: 25.81418941007524
Epoch: 6189, Batch Gradient Norm after: 22.359355691705947
Epoch 6190/10000, Prediction Accuracy = 59.662%, Loss = 0.7642563939094543
Epoch: 6190, Batch Gradient Norm: 25.755892869392667
Epoch: 6190, Batch Gradient Norm after: 22.36068024504529
Epoch 6191/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7641929268836976
Epoch: 6191, Batch Gradient Norm: 25.793753632138714
Epoch: 6191, Batch Gradient Norm after: 22.34530070242594
Epoch 6192/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.764109444618225
Epoch: 6192, Batch Gradient Norm: 25.77131090006635
Epoch: 6192, Batch Gradient Norm after: 22.360676662681847
Epoch 6193/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7641788721084595
Epoch: 6193, Batch Gradient Norm: 25.79919387746412
Epoch: 6193, Batch Gradient Norm after: 22.360677754083596
Epoch 6194/10000, Prediction Accuracy = 59.60799999999999%, Loss = 0.7640820980072022
Epoch: 6194, Batch Gradient Norm: 25.750057888944287
Epoch: 6194, Batch Gradient Norm after: 22.360678820283205
Epoch 6195/10000, Prediction Accuracy = 59.70400000000001%, Loss = 0.7640828371047974
Epoch: 6195, Batch Gradient Norm: 25.78240916005672
Epoch: 6195, Batch Gradient Norm after: 22.347684166387978
Epoch 6196/10000, Prediction Accuracy = 59.624%, Loss = 0.7639401078224182
Epoch: 6196, Batch Gradient Norm: 25.75691360913563
Epoch: 6196, Batch Gradient Norm after: 22.360679267274996
Epoch 6197/10000, Prediction Accuracy = 59.7%, Loss = 0.7639618039131164
Epoch: 6197, Batch Gradient Norm: 25.789343520776875
Epoch: 6197, Batch Gradient Norm after: 22.359747362732417
Epoch 6198/10000, Prediction Accuracy = 59.64%, Loss = 0.7638737797737122
Epoch: 6198, Batch Gradient Norm: 25.740921725964473
Epoch: 6198, Batch Gradient Norm after: 22.360680061567503
Epoch 6199/10000, Prediction Accuracy = 59.66400000000001%, Loss = 0.7638363599777221
Epoch: 6199, Batch Gradient Norm: 25.75721531642856
Epoch: 6199, Batch Gradient Norm after: 22.341493972793074
Epoch 6200/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.763726818561554
Epoch: 6200, Batch Gradient Norm: 25.756951824809462
Epoch: 6200, Batch Gradient Norm after: 22.360678738131895
Epoch 6201/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.7637822985649109
Epoch: 6201, Batch Gradient Norm: 25.770362508700067
Epoch: 6201, Batch Gradient Norm after: 22.36067634326587
Epoch 6202/10000, Prediction Accuracy = 59.628%, Loss = 0.7636501908302307
Epoch: 6202, Batch Gradient Norm: 25.73571981332905
Epoch: 6202, Batch Gradient Norm after: 22.36067848897335
Epoch 6203/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7636796116828919
Epoch: 6203, Batch Gradient Norm: 25.757962288035593
Epoch: 6203, Batch Gradient Norm after: 22.346341012542755
Epoch 6204/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7635761260986328
Epoch: 6204, Batch Gradient Norm: 25.741654480276456
Epoch: 6204, Batch Gradient Norm after: 22.36067798752401
Epoch 6205/10000, Prediction Accuracy = 59.71400000000001%, Loss = 0.7636590003967285
Epoch: 6205, Batch Gradient Norm: 25.76439992235992
Epoch: 6205, Batch Gradient Norm after: 22.35940586374751
Epoch 6206/10000, Prediction Accuracy = 59.622%, Loss = 0.7635091662406921
Epoch: 6206, Batch Gradient Norm: 25.721464207985488
Epoch: 6206, Batch Gradient Norm after: 22.360680984362695
Epoch 6207/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7634907603263855
Epoch: 6207, Batch Gradient Norm: 25.73919602211649
Epoch: 6207, Batch Gradient Norm after: 22.34202653898829
Epoch 6208/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.7633382439613342
Epoch: 6208, Batch Gradient Norm: 25.74096646653521
Epoch: 6208, Batch Gradient Norm after: 22.360676863623286
Epoch 6209/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.7634219169616699
Epoch: 6209, Batch Gradient Norm: 25.747070377854463
Epoch: 6209, Batch Gradient Norm after: 22.360679185367157
Epoch 6210/10000, Prediction Accuracy = 59.676%, Loss = 0.7632980585098267
Epoch: 6210, Batch Gradient Norm: 25.717636625498564
Epoch: 6210, Batch Gradient Norm after: 22.36068044082459
Epoch 6211/10000, Prediction Accuracy = 59.688%, Loss = 0.7632923126220703
Epoch: 6211, Batch Gradient Norm: 25.730713430084357
Epoch: 6211, Batch Gradient Norm after: 22.346311497036254
Epoch 6212/10000, Prediction Accuracy = 59.644000000000005%, Loss = 0.7631570100784302
Epoch: 6212, Batch Gradient Norm: 25.728493595785853
Epoch: 6212, Batch Gradient Norm after: 22.36067946641016
Epoch 6213/10000, Prediction Accuracy = 59.702%, Loss = 0.763233232498169
Epoch: 6213, Batch Gradient Norm: 25.742667889447524
Epoch: 6213, Batch Gradient Norm after: 22.36067805015763
Epoch 6214/10000, Prediction Accuracy = 59.646%, Loss = 0.763126015663147
Epoch: 6214, Batch Gradient Norm: 25.709010252359562
Epoch: 6214, Batch Gradient Norm after: 22.360675465681645
Epoch 6215/10000, Prediction Accuracy = 59.71%, Loss = 0.76316397190094
Epoch: 6215, Batch Gradient Norm: 25.719921234598427
Epoch: 6215, Batch Gradient Norm after: 22.34489087473943
Epoch 6216/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7630011200904846
Epoch: 6216, Batch Gradient Norm: 25.72037899700952
Epoch: 6216, Batch Gradient Norm after: 22.360678825899132
Epoch 6217/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7630890250205994
Epoch: 6217, Batch Gradient Norm: 25.72300871991117
Epoch: 6217, Batch Gradient Norm after: 22.36067654278286
Epoch 6218/10000, Prediction Accuracy = 59.63000000000001%, Loss = 0.7628984570503234
Epoch: 6218, Batch Gradient Norm: 25.704452665021787
Epoch: 6218, Batch Gradient Norm after: 22.36067945976737
Epoch 6219/10000, Prediction Accuracy = 59.681999999999995%, Loss = 0.7629334688186645
Epoch: 6219, Batch Gradient Norm: 25.710055827110864
Epoch: 6219, Batch Gradient Norm after: 22.348923562757633
Epoch 6220/10000, Prediction Accuracy = 59.662%, Loss = 0.7627980828285217
Epoch: 6220, Batch Gradient Norm: 25.710246233869352
Epoch: 6220, Batch Gradient Norm after: 22.360677289085356
Epoch 6221/10000, Prediction Accuracy = 59.678%, Loss = 0.7628580570220947
Epoch: 6221, Batch Gradient Norm: 25.71833223854548
Epoch: 6221, Batch Gradient Norm after: 22.359335171193305
Epoch 6222/10000, Prediction Accuracy = 59.664%, Loss = 0.7627224802970887
Epoch: 6222, Batch Gradient Norm: 25.69213088538784
Epoch: 6222, Batch Gradient Norm after: 22.360676643045284
Epoch 6223/10000, Prediction Accuracy = 59.694%, Loss = 0.7627344846725463
Epoch: 6223, Batch Gradient Norm: 25.696225232333948
Epoch: 6223, Batch Gradient Norm after: 22.344419847728688
Epoch 6224/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.762589693069458
Epoch: 6224, Batch Gradient Norm: 25.70864965751303
Epoch: 6224, Batch Gradient Norm after: 22.36067948101707
Epoch 6225/10000, Prediction Accuracy = 59.73199999999999%, Loss = 0.7627322912216187
Epoch: 6225, Batch Gradient Norm: 25.699414668346783
Epoch: 6225, Batch Gradient Norm after: 22.36067931651225
Epoch 6226/10000, Prediction Accuracy = 59.626%, Loss = 0.7625523447990418
Epoch: 6226, Batch Gradient Norm: 25.688558425474028
Epoch: 6226, Batch Gradient Norm after: 22.360678376031863
Epoch 6227/10000, Prediction Accuracy = 59.738%, Loss = 0.7626147270202637
Epoch: 6227, Batch Gradient Norm: 25.690264519579017
Epoch: 6227, Batch Gradient Norm after: 22.351890421770452
Epoch 6228/10000, Prediction Accuracy = 59.634%, Loss = 0.7624204158782959
Epoch: 6228, Batch Gradient Norm: 25.690793523686036
Epoch: 6228, Batch Gradient Norm after: 22.36067779234566
Epoch 6229/10000, Prediction Accuracy = 59.69%, Loss = 0.762485122680664
Epoch: 6229, Batch Gradient Norm: 25.685658749619808
Epoch: 6229, Batch Gradient Norm after: 22.35500018589962
Epoch 6230/10000, Prediction Accuracy = 59.666%, Loss = 0.7623351454734802
Epoch: 6230, Batch Gradient Norm: 25.687028409377536
Epoch: 6230, Batch Gradient Norm after: 22.360679867271656
Epoch 6231/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.7623947501182556
Epoch: 6231, Batch Gradient Norm: 25.677593253618852
Epoch: 6231, Batch Gradient Norm after: 22.352536794612295
Epoch 6232/10000, Prediction Accuracy = 59.674%, Loss = 0.7622311234474182
Epoch: 6232, Batch Gradient Norm: 25.681630891160747
Epoch: 6232, Batch Gradient Norm after: 22.36068108100467
Epoch 6233/10000, Prediction Accuracy = 59.702%, Loss = 0.762299633026123
Epoch: 6233, Batch Gradient Norm: 25.67212860894902
Epoch: 6233, Batch Gradient Norm after: 22.3537308009867
Epoch 6234/10000, Prediction Accuracy = 59.626%, Loss = 0.7621311783790589
Epoch: 6234, Batch Gradient Norm: 25.678881838235174
Epoch: 6234, Batch Gradient Norm after: 22.360676782855634
Epoch 6235/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7622415900230408
Epoch: 6235, Batch Gradient Norm: 25.667824863110493
Epoch: 6235, Batch Gradient Norm after: 22.353530339535062
Epoch 6236/10000, Prediction Accuracy = 59.612%, Loss = 0.7620784997940063
Epoch: 6236, Batch Gradient Norm: 25.67253659559916
Epoch: 6236, Batch Gradient Norm after: 22.36067820543224
Epoch 6237/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7621944427490235
Epoch: 6237, Batch Gradient Norm: 25.66048156920017
Epoch: 6237, Batch Gradient Norm after: 22.354302288813564
Epoch 6238/10000, Prediction Accuracy = 59.648%, Loss = 0.7619677186012268
Epoch: 6238, Batch Gradient Norm: 25.66717931019217
Epoch: 6238, Batch Gradient Norm after: 22.36067868866877
Epoch 6239/10000, Prediction Accuracy = 59.734%, Loss = 0.7620486259460449
Epoch: 6239, Batch Gradient Norm: 25.65508875840616
Epoch: 6239, Batch Gradient Norm after: 22.35389925235918
Epoch 6240/10000, Prediction Accuracy = 59.676%, Loss = 0.7618491768836975
Epoch: 6240, Batch Gradient Norm: 25.667081552111878
Epoch: 6240, Batch Gradient Norm after: 22.360678042398757
Epoch 6241/10000, Prediction Accuracy = 59.688%, Loss = 0.7619536638259887
Epoch: 6241, Batch Gradient Norm: 25.648579069643986
Epoch: 6241, Batch Gradient Norm after: 22.355538336852174
Epoch 6242/10000, Prediction Accuracy = 59.67399999999999%, Loss = 0.7617720484733581
Epoch: 6242, Batch Gradient Norm: 25.656940835738695
Epoch: 6242, Batch Gradient Norm after: 22.360681138346397
Epoch 6243/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.761849308013916
Epoch: 6243, Batch Gradient Norm: 25.638318339872168
Epoch: 6243, Batch Gradient Norm after: 22.350588507072022
Epoch 6244/10000, Prediction Accuracy = 59.646%, Loss = 0.7616379499435425
Epoch: 6244, Batch Gradient Norm: 25.663137266020932
Epoch: 6244, Batch Gradient Norm after: 22.360677482085833
Epoch 6245/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7617988586425781
Epoch: 6245, Batch Gradient Norm: 25.64684936022095
Epoch: 6245, Batch Gradient Norm after: 22.36067805643385
Epoch 6246/10000, Prediction Accuracy = 59.646%, Loss = 0.7616049170494079
Epoch: 6246, Batch Gradient Norm: 25.645449726093116
Epoch: 6246, Batch Gradient Norm after: 22.360677548319142
Epoch 6247/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7617265224456787
Epoch: 6247, Batch Gradient Norm: 25.620432529161924
Epoch: 6247, Batch Gradient Norm after: 22.344555726599747
Epoch 6248/10000, Prediction Accuracy = 59.626%, Loss = 0.7614676833152771
Epoch: 6248, Batch Gradient Norm: 25.65651212042868
Epoch: 6248, Batch Gradient Norm after: 22.360677363927643
Epoch 6249/10000, Prediction Accuracy = 59.758%, Loss = 0.7616395354270935
Epoch: 6249, Batch Gradient Norm: 25.628023558150474
Epoch: 6249, Batch Gradient Norm after: 22.36067750160019
Epoch 6250/10000, Prediction Accuracy = 59.674%, Loss = 0.7613772034645081
Epoch: 6250, Batch Gradient Norm: 25.64014076416687
Epoch: 6250, Batch Gradient Norm after: 22.36067901883267
Epoch 6251/10000, Prediction Accuracy = 59.698%, Loss = 0.7614913702011108
Epoch: 6251, Batch Gradient Norm: 25.610696975953097
Epoch: 6251, Batch Gradient Norm after: 22.346422824753922
Epoch 6252/10000, Prediction Accuracy = 59.666%, Loss = 0.761262059211731
Epoch: 6252, Batch Gradient Norm: 25.64910596270084
Epoch: 6252, Batch Gradient Norm after: 22.360679041324353
Epoch 6253/10000, Prediction Accuracy = 59.698%, Loss = 0.7614299416542053
Epoch: 6253, Batch Gradient Norm: 25.6191551558285
Epoch: 6253, Batch Gradient Norm after: 22.360522335692067
Epoch 6254/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.7611923933029174
Epoch: 6254, Batch Gradient Norm: 25.632974898048452
Epoch: 6254, Batch Gradient Norm after: 22.360679597463136
Epoch 6255/10000, Prediction Accuracy = 59.734%, Loss = 0.761309015750885
Epoch: 6255, Batch Gradient Norm: 25.593062190095438
Epoch: 6255, Batch Gradient Norm after: 22.341010685200033
Epoch 6256/10000, Prediction Accuracy = 59.65%, Loss = 0.7610457897186279
Epoch: 6256, Batch Gradient Norm: 25.652502123337886
Epoch: 6256, Batch Gradient Norm after: 22.360677848422313
Epoch 6257/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.761316454410553
Epoch: 6257, Batch Gradient Norm: 25.599756493038573
Epoch: 6257, Batch Gradient Norm after: 22.360677890045167
Epoch 6258/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.7610167741775513
Epoch: 6258, Batch Gradient Norm: 25.629586861644967
Epoch: 6258, Batch Gradient Norm after: 22.36067818424698
Epoch 6259/10000, Prediction Accuracy = 59.733999999999995%, Loss = 0.7611892700195313
Epoch: 6259, Batch Gradient Norm: 25.59176797099989
Epoch: 6259, Batch Gradient Norm after: 22.34932541672822
Epoch 6260/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.7608787655830384
Epoch: 6260, Batch Gradient Norm: 25.632054464132153
Epoch: 6260, Batch Gradient Norm after: 22.36067852576774
Epoch 6261/10000, Prediction Accuracy = 59.686%, Loss = 0.7610662102699279
Epoch: 6261, Batch Gradient Norm: 25.591102483976833
Epoch: 6261, Batch Gradient Norm after: 22.354619527185353
Epoch 6262/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.7608073949813843
Epoch: 6262, Batch Gradient Norm: 25.624317770922524
Epoch: 6262, Batch Gradient Norm after: 22.36068010123215
Epoch 6263/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7609691619873047
Epoch: 6263, Batch Gradient Norm: 25.580681637606915
Epoch: 6263, Batch Gradient Norm after: 22.350319355606555
Epoch 6264/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.7606890320777893
Epoch: 6264, Batch Gradient Norm: 25.62576056081042
Epoch: 6264, Batch Gradient Norm after: 22.360679393248287
Epoch 6265/10000, Prediction Accuracy = 59.71%, Loss = 0.7608813047409058
Epoch: 6265, Batch Gradient Norm: 25.582958998460956
Epoch: 6265, Batch Gradient Norm after: 22.355704411901957
Epoch 6266/10000, Prediction Accuracy = 59.65599999999999%, Loss = 0.760602593421936
Epoch: 6266, Batch Gradient Norm: 25.61798942532423
Epoch: 6266, Batch Gradient Norm after: 22.360679026891816
Epoch 6267/10000, Prediction Accuracy = 59.746%, Loss = 0.7608194947242737
Epoch: 6267, Batch Gradient Norm: 25.570339870343062
Epoch: 6267, Batch Gradient Norm after: 22.348707130843607
Epoch 6268/10000, Prediction Accuracy = 59.622%, Loss = 0.7605379939079284
Epoch: 6268, Batch Gradient Norm: 25.6189834647407
Epoch: 6268, Batch Gradient Norm after: 22.3606786603639
Epoch 6269/10000, Prediction Accuracy = 59.72800000000001%, Loss = 0.760780131816864
Epoch: 6269, Batch Gradient Norm: 25.572045733680476
Epoch: 6269, Batch Gradient Norm after: 22.35808504903407
Epoch 6270/10000, Prediction Accuracy = 59.66199999999999%, Loss = 0.760438060760498
Epoch: 6270, Batch Gradient Norm: 25.604005066886536
Epoch: 6270, Batch Gradient Norm after: 22.360676998921573
Epoch 6271/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.7606091380119324
Epoch: 6271, Batch Gradient Norm: 25.55602047714086
Epoch: 6271, Batch Gradient Norm after: 22.345897101245146
Epoch 6272/10000, Prediction Accuracy = 59.682%, Loss = 0.7602959156036377
Epoch: 6272, Batch Gradient Norm: 25.6153118211201
Epoch: 6272, Batch Gradient Norm after: 22.360679546452378
Epoch 6273/10000, Prediction Accuracy = 59.7%, Loss = 0.7605418562889099
Epoch: 6273, Batch Gradient Norm: 25.561770022766297
Epoch: 6273, Batch Gradient Norm after: 22.36027192047087
Epoch 6274/10000, Prediction Accuracy = 59.66600000000001%, Loss = 0.7602527379989624
Epoch: 6274, Batch Gradient Norm: 25.596368030304824
Epoch: 6274, Batch Gradient Norm after: 22.36067953175827
Epoch 6275/10000, Prediction Accuracy = 59.73%, Loss = 0.7604114055633545
Epoch: 6275, Batch Gradient Norm: 25.539037967737798
Epoch: 6275, Batch Gradient Norm after: 22.343415316573502
Epoch 6276/10000, Prediction Accuracy = 59.676%, Loss = 0.760084044933319
Epoch: 6276, Batch Gradient Norm: 25.614407062048258
Epoch: 6276, Batch Gradient Norm after: 22.360678235535136
Epoch 6277/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.7603853821754456
Epoch: 6277, Batch Gradient Norm: 25.547495677684175
Epoch: 6277, Batch Gradient Norm after: 22.360677822297628
Epoch 6278/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.7600528359413147
Epoch: 6278, Batch Gradient Norm: 25.59289075347324
Epoch: 6278, Batch Gradient Norm after: 22.36067926608193
Epoch 6279/10000, Prediction Accuracy = 59.726%, Loss = 0.7603145241737366
Epoch: 6279, Batch Gradient Norm: 25.533996356645854
Epoch: 6279, Batch Gradient Norm after: 22.349275148264606
Epoch 6280/10000, Prediction Accuracy = 59.652%, Loss = 0.7599517345428467
Epoch: 6280, Batch Gradient Norm: 25.59312750483422
Epoch: 6280, Batch Gradient Norm after: 22.360678539617055
Epoch 6281/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7601996541023255
Epoch: 6281, Batch Gradient Norm: 25.537359782227842
Epoch: 6281, Batch Gradient Norm after: 22.357666436447587
Epoch 6282/10000, Prediction Accuracy = 59.702%, Loss = 0.7598560929298401
Epoch: 6282, Batch Gradient Norm: 25.583156579483326
Epoch: 6282, Batch Gradient Norm after: 22.360679852676427
Epoch 6283/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7600658178329468
Epoch: 6283, Batch Gradient Norm: 25.51801038015067
Epoch: 6283, Batch Gradient Norm after: 22.34607461579975
Epoch 6284/10000, Prediction Accuracy = 59.67%, Loss = 0.7597370266914367
Epoch: 6284, Batch Gradient Norm: 25.591171616834902
Epoch: 6284, Batch Gradient Norm after: 22.360679243207215
Epoch 6285/10000, Prediction Accuracy = 59.73%, Loss = 0.7600046515464782
Epoch: 6285, Batch Gradient Norm: 25.524982643892237
Epoch: 6285, Batch Gradient Norm after: 22.35941250085035
Epoch 6286/10000, Prediction Accuracy = 59.698%, Loss = 0.7596617817878724
Epoch: 6286, Batch Gradient Norm: 25.575345577925994
Epoch: 6286, Batch Gradient Norm after: 22.360677368782778
Epoch 6287/10000, Prediction Accuracy = 59.746%, Loss = 0.7598857879638672
Epoch: 6287, Batch Gradient Norm: 25.508914857360793
Epoch: 6287, Batch Gradient Norm after: 22.34502537399706
Epoch 6288/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7595424890518189
Epoch: 6288, Batch Gradient Norm: 25.584444259571764
Epoch: 6288, Batch Gradient Norm after: 22.360677899549504
Epoch 6289/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.759885048866272
Epoch: 6289, Batch Gradient Norm: 25.51480005208539
Epoch: 6289, Batch Gradient Norm after: 22.360679296787247
Epoch 6290/10000, Prediction Accuracy = 59.64%, Loss = 0.7595160961151123
Epoch: 6290, Batch Gradient Norm: 25.562679534163962
Epoch: 6290, Batch Gradient Norm after: 22.360678832747148
Epoch 6291/10000, Prediction Accuracy = 59.746%, Loss = 0.7597519397735596
Epoch: 6291, Batch Gradient Norm: 25.490125505970052
Epoch: 6291, Batch Gradient Norm after: 22.34248522089362
Epoch 6292/10000, Prediction Accuracy = 59.702%, Loss = 0.7593290209770203
Epoch: 6292, Batch Gradient Norm: 25.580088080059014
Epoch: 6292, Batch Gradient Norm after: 22.360678008675947
Epoch 6293/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7596581816673279
Epoch: 6293, Batch Gradient Norm: 25.498986155906657
Epoch: 6293, Batch Gradient Norm after: 22.360674935613208
Epoch 6294/10000, Prediction Accuracy = 59.67600000000001%, Loss = 0.7592908620834351
Epoch: 6294, Batch Gradient Norm: 25.558904350392737
Epoch: 6294, Batch Gradient Norm after: 22.360679971568484
Epoch 6295/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7595392942428589
Epoch: 6295, Batch Gradient Norm: 25.47574340787992
Epoch: 6295, Batch Gradient Norm after: 22.343924206082264
Epoch 6296/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7591395139694214
Epoch: 6296, Batch Gradient Norm: 25.573304043988806
Epoch: 6296, Batch Gradient Norm after: 22.360679013582484
Epoch 6297/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.759475314617157
Epoch: 6297, Batch Gradient Norm: 25.488733645174438
Epoch: 6297, Batch Gradient Norm after: 22.36067948550713
Epoch 6298/10000, Prediction Accuracy = 59.698%, Loss = 0.7590900897979737
Epoch: 6298, Batch Gradient Norm: 25.55406868255883
Epoch: 6298, Batch Gradient Norm after: 22.360678637848228
Epoch 6299/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.759405767917633
Epoch: 6299, Batch Gradient Norm: 25.470008504893702
Epoch: 6299, Batch Gradient Norm after: 22.345200854883828
Epoch 6300/10000, Prediction Accuracy = 59.636%, Loss = 0.759001898765564
Epoch: 6300, Batch Gradient Norm: 25.562137295699447
Epoch: 6300, Batch Gradient Norm after: 22.360681882074182
Epoch 6301/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7593612432479858
Epoch: 6301, Batch Gradient Norm: 25.48009154837177
Epoch: 6301, Batch Gradient Norm after: 22.360271673829555
Epoch 6302/10000, Prediction Accuracy = 59.694%, Loss = 0.7589195370674133
Epoch: 6302, Batch Gradient Norm: 25.542244170679442
Epoch: 6302, Batch Gradient Norm after: 22.360680526481882
Epoch 6303/10000, Prediction Accuracy = 59.754%, Loss = 0.7591726422309876
Epoch: 6303, Batch Gradient Norm: 25.44984441257798
Epoch: 6303, Batch Gradient Norm after: 22.34056409590722
Epoch 6304/10000, Prediction Accuracy = 59.708000000000006%, Loss = 0.7587572693824768
Epoch: 6304, Batch Gradient Norm: 25.566432350522852
Epoch: 6304, Batch Gradient Norm after: 22.360679793865035
Epoch 6305/10000, Prediction Accuracy = 59.71199999999999%, Loss = 0.7591394543647766
Epoch: 6305, Batch Gradient Norm: 25.456018610222102
Epoch: 6305, Batch Gradient Norm after: 22.360678030306563
Epoch 6306/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7587024092674255
Epoch: 6306, Batch Gradient Norm: 25.543995505906192
Epoch: 6306, Batch Gradient Norm after: 22.36067907387731
Epoch 6307/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7590007424354553
Epoch: 6307, Batch Gradient Norm: 25.45093692873365
Epoch: 6307, Batch Gradient Norm after: 22.35127572448247
Epoch 6308/10000, Prediction Accuracy = 59.7%, Loss = 0.7585880041122437
Epoch: 6308, Batch Gradient Norm: 25.545884940810755
Epoch: 6308, Batch Gradient Norm after: 22.3606809305363
Epoch 6309/10000, Prediction Accuracy = 59.75%, Loss = 0.7589433312416076
Epoch: 6309, Batch Gradient Norm: 25.458414279090768
Epoch: 6309, Batch Gradient Norm after: 22.35743840125203
Epoch 6310/10000, Prediction Accuracy = 59.652%, Loss = 0.7585582137107849
Epoch: 6310, Batch Gradient Norm: 25.529246970347536
Epoch: 6310, Batch Gradient Norm after: 22.36067975519683
Epoch 6311/10000, Prediction Accuracy = 59.742%, Loss = 0.7588953495025634
Epoch: 6311, Batch Gradient Norm: 25.431801582760606
Epoch: 6311, Batch Gradient Norm after: 22.34479009069961
Epoch 6312/10000, Prediction Accuracy = 59.652%, Loss = 0.7584156632423401
Epoch: 6312, Batch Gradient Norm: 25.544546149111945
Epoch: 6312, Batch Gradient Norm after: 22.360680155089977
Epoch 6313/10000, Prediction Accuracy = 59.769999999999996%, Loss = 0.7587944865226746
Epoch: 6313, Batch Gradient Norm: 25.440981499738733
Epoch: 6313, Batch Gradient Norm after: 22.36067900915755
Epoch 6314/10000, Prediction Accuracy = 59.714%, Loss = 0.7583249926567077
Epoch: 6314, Batch Gradient Norm: 25.52829198277518
Epoch: 6314, Batch Gradient Norm after: 22.360680873716245
Epoch 6315/10000, Prediction Accuracy = 59.742%, Loss = 0.7586475729942321
Epoch: 6315, Batch Gradient Norm: 25.420336758191347
Epoch: 6315, Batch Gradient Norm after: 22.34716578413718
Epoch 6316/10000, Prediction Accuracy = 59.69%, Loss = 0.7582200288772583
Epoch: 6316, Batch Gradient Norm: 25.536717377869056
Epoch: 6316, Batch Gradient Norm after: 22.360678615600662
Epoch 6317/10000, Prediction Accuracy = 59.734%, Loss = 0.758581817150116
Epoch: 6317, Batch Gradient Norm: 25.430573403102738
Epoch: 6317, Batch Gradient Norm after: 22.36067630249308
Epoch 6318/10000, Prediction Accuracy = 59.71%, Loss = 0.7581400275230408
Epoch: 6318, Batch Gradient Norm: 25.519041997833554
Epoch: 6318, Batch Gradient Norm after: 22.360676106485577
Epoch 6319/10000, Prediction Accuracy = 59.766000000000005%, Loss = 0.7584678411483765
Epoch: 6319, Batch Gradient Norm: 25.415817169654655
Epoch: 6319, Batch Gradient Norm after: 22.347740875401346
Epoch 6320/10000, Prediction Accuracy = 59.694%, Loss = 0.7580392837524415
Epoch: 6320, Batch Gradient Norm: 25.526281808076362
Epoch: 6320, Batch Gradient Norm after: 22.36067855112214
Epoch 6321/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7584683060646057
Epoch: 6321, Batch Gradient Norm: 25.422035445926266
Epoch: 6321, Batch Gradient Norm after: 22.360679247676913
Epoch 6322/10000, Prediction Accuracy = 59.656000000000006%, Loss = 0.7579969882965087
Epoch: 6322, Batch Gradient Norm: 25.50538408849719
Epoch: 6322, Batch Gradient Norm after: 22.360680140615948
Epoch 6323/10000, Prediction Accuracy = 59.746%, Loss = 0.7583314180374146
Epoch: 6323, Batch Gradient Norm: 25.396251283198907
Epoch: 6323, Batch Gradient Norm after: 22.34419063492109
Epoch 6324/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7578222274780273
Epoch: 6324, Batch Gradient Norm: 25.524436060019127
Epoch: 6324, Batch Gradient Norm after: 22.36067824628991
Epoch 6325/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7582362055778503
Epoch: 6325, Batch Gradient Norm: 25.40300921506854
Epoch: 6325, Batch Gradient Norm after: 22.36067994916756
Epoch 6326/10000, Prediction Accuracy = 59.7%, Loss = 0.757772696018219
Epoch: 6326, Batch Gradient Norm: 25.503035592232855
Epoch: 6326, Batch Gradient Norm after: 22.36067827563629
Epoch 6327/10000, Prediction Accuracy = 59.73199999999999%, Loss = 0.758116602897644
Epoch: 6327, Batch Gradient Norm: 25.385013077861373
Epoch: 6327, Batch Gradient Norm after: 22.346249568264316
Epoch 6328/10000, Prediction Accuracy = 59.712%, Loss = 0.7576405882835389
Epoch: 6328, Batch Gradient Norm: 25.51511074240462
Epoch: 6328, Batch Gradient Norm after: 22.3606794289226
Epoch 6329/10000, Prediction Accuracy = 59.738%, Loss = 0.7580520510673523
Epoch: 6329, Batch Gradient Norm: 25.396881052140984
Epoch: 6329, Batch Gradient Norm after: 22.360678041264304
Epoch 6330/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.757577633857727
Epoch: 6330, Batch Gradient Norm: 25.495265493069173
Epoch: 6330, Batch Gradient Norm after: 22.360679892384276
Epoch 6331/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7579766988754273
Epoch: 6331, Batch Gradient Norm: 25.378428159584544
Epoch: 6331, Batch Gradient Norm after: 22.346075658625878
Epoch 6332/10000, Prediction Accuracy = 59.664%, Loss = 0.7575011014938354
Epoch: 6332, Batch Gradient Norm: 25.506060449130267
Epoch: 6332, Batch Gradient Norm after: 22.36068026512984
Epoch 6333/10000, Prediction Accuracy = 59.75%, Loss = 0.7579435706138611
Epoch: 6333, Batch Gradient Norm: 25.385268168339078
Epoch: 6333, Batch Gradient Norm after: 22.360680967475222
Epoch 6334/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7574079990386963
Epoch: 6334, Batch Gradient Norm: 25.482469918058342
Epoch: 6334, Batch Gradient Norm after: 22.36067782420001
Epoch 6335/10000, Prediction Accuracy = 59.73%, Loss = 0.7577556729316711
Epoch: 6335, Batch Gradient Norm: 25.36461689643913
Epoch: 6335, Batch Gradient Norm after: 22.34419633578237
Epoch 6336/10000, Prediction Accuracy = 59.70200000000001%, Loss = 0.7572735548019409
Epoch: 6336, Batch Gradient Norm: 25.498768095130185
Epoch: 6336, Batch Gradient Norm after: 22.360677028716218
Epoch 6337/10000, Prediction Accuracy = 59.738%, Loss = 0.7577050805091858
Epoch: 6337, Batch Gradient Norm: 25.37475138539611
Epoch: 6337, Batch Gradient Norm after: 22.360677445274916
Epoch 6338/10000, Prediction Accuracy = 59.706%, Loss = 0.7572316646575927
Epoch: 6338, Batch Gradient Norm: 25.476486305615982
Epoch: 6338, Batch Gradient Norm after: 22.360678733933163
Epoch 6339/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7575780153274536
Epoch: 6339, Batch Gradient Norm: 25.36175961829069
Epoch: 6339, Batch Gradient Norm after: 22.34835709102919
Epoch 6340/10000, Prediction Accuracy = 59.717999999999996%, Loss = 0.7570913791656494
Epoch: 6340, Batch Gradient Norm: 25.48734230483201
Epoch: 6340, Batch Gradient Norm after: 22.360674674490266
Epoch 6341/10000, Prediction Accuracy = 59.742%, Loss = 0.7575423836708068
Epoch: 6341, Batch Gradient Norm: 25.37209862313823
Epoch: 6341, Batch Gradient Norm after: 22.359829071801645
Epoch 6342/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7570846676826477
Epoch: 6342, Batch Gradient Norm: 25.468300828485052
Epoch: 6342, Batch Gradient Norm after: 22.360677654832664
Epoch 6343/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7574765682220459
Epoch: 6343, Batch Gradient Norm: 25.346755924275573
Epoch: 6343, Batch Gradient Norm after: 22.345484320329
Epoch 6344/10000, Prediction Accuracy = 59.681999999999995%, Loss = 0.7569269418716431
Epoch: 6344, Batch Gradient Norm: 25.480989489928355
Epoch: 6344, Batch Gradient Norm after: 22.360678885864065
Epoch 6345/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7573718070983887
Epoch: 6345, Batch Gradient Norm: 25.35388099370021
Epoch: 6345, Batch Gradient Norm after: 22.36067721268139
Epoch 6346/10000, Prediction Accuracy = 59.71600000000001%, Loss = 0.7568431258201599
Epoch: 6346, Batch Gradient Norm: 25.463910272814086
Epoch: 6346, Batch Gradient Norm after: 22.360677050869366
Epoch 6347/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7572293043136596
Epoch: 6347, Batch Gradient Norm: 25.338242776380344
Epoch: 6347, Batch Gradient Norm after: 22.347657764408883
Epoch 6348/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7567421555519104
Epoch: 6348, Batch Gradient Norm: 25.474188013952773
Epoch: 6348, Batch Gradient Norm after: 22.360677994705807
Epoch 6349/10000, Prediction Accuracy = 59.74000000000001%, Loss = 0.757166075706482
Epoch: 6349, Batch Gradient Norm: 25.34620318773113
Epoch: 6349, Batch Gradient Norm after: 22.360677490582678
Epoch 6350/10000, Prediction Accuracy = 59.712%, Loss = 0.7566551566123962
Epoch: 6350, Batch Gradient Norm: 25.458661678748573
Epoch: 6350, Batch Gradient Norm after: 22.360678773686228
Epoch 6351/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.7570585131645202
Epoch: 6351, Batch Gradient Norm: 25.333886334823312
Epoch: 6351, Batch Gradient Norm after: 22.349653486671976
Epoch 6352/10000, Prediction Accuracy = 59.698%, Loss = 0.7565767884254455
Epoch: 6352, Batch Gradient Norm: 25.46229336572639
Epoch: 6352, Batch Gradient Norm after: 22.360677345882596
Epoch 6353/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7570520162582397
Epoch: 6353, Batch Gradient Norm: 25.339972066425027
Epoch: 6353, Batch Gradient Norm after: 22.36068088535446
Epoch 6354/10000, Prediction Accuracy = 59.688%, Loss = 0.7565238952636719
Epoch: 6354, Batch Gradient Norm: 25.44592739918871
Epoch: 6354, Batch Gradient Norm after: 22.360678997856333
Epoch 6355/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7568998098373413
Epoch: 6355, Batch Gradient Norm: 25.319808161526453
Epoch: 6355, Batch Gradient Norm after: 22.347107649027294
Epoch 6356/10000, Prediction Accuracy = 59.717999999999996%, Loss = 0.7563602566719055
Epoch: 6356, Batch Gradient Norm: 25.45835705551638
Epoch: 6356, Batch Gradient Norm after: 22.36067816434309
Epoch 6357/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7568108439445496
Epoch: 6357, Batch Gradient Norm: 25.323787984311807
Epoch: 6357, Batch Gradient Norm after: 22.360678027958553
Epoch 6358/10000, Prediction Accuracy = 59.718%, Loss = 0.756312370300293
Epoch: 6358, Batch Gradient Norm: 25.44156980766379
Epoch: 6358, Batch Gradient Norm after: 22.36068044679713
Epoch 6359/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.756705641746521
Epoch: 6359, Batch Gradient Norm: 25.307483681294702
Epoch: 6359, Batch Gradient Norm after: 22.350436372425623
Epoch 6360/10000, Prediction Accuracy = 59.688%, Loss = 0.7561845183372498
Epoch: 6360, Batch Gradient Norm: 25.44845769955486
Epoch: 6360, Batch Gradient Norm after: 22.36067628101953
Epoch 6361/10000, Prediction Accuracy = 59.762%, Loss = 0.756627106666565
Epoch: 6361, Batch Gradient Norm: 25.31747616515618
Epoch: 6361, Batch Gradient Norm after: 22.36067845863236
Epoch 6362/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7561374425888061
Epoch: 6362, Batch Gradient Norm: 25.433661924814253
Epoch: 6362, Batch Gradient Norm after: 22.360678024479178
Epoch 6363/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7565839529037476
Epoch: 6363, Batch Gradient Norm: 25.293023149511658
Epoch: 6363, Batch Gradient Norm after: 22.34688518333991
Epoch 6364/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.7560459494590759
Epoch: 6364, Batch Gradient Norm: 25.440836431976326
Epoch: 6364, Batch Gradient Norm after: 22.360679856841422
Epoch 6365/10000, Prediction Accuracy = 59.75%, Loss = 0.7565214395523071
Epoch: 6365, Batch Gradient Norm: 25.302831259636086
Epoch: 6365, Batch Gradient Norm after: 22.360677861596105
Epoch 6366/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.755933678150177
Epoch: 6366, Batch Gradient Norm: 25.424450466103
Epoch: 6366, Batch Gradient Norm after: 22.360675668079857
Epoch 6367/10000, Prediction Accuracy = 59.766%, Loss = 0.7563485145568848
Epoch: 6367, Batch Gradient Norm: 25.285938587154046
Epoch: 6367, Batch Gradient Norm after: 22.34915797278831
Epoch 6368/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7558292746543884
Epoch: 6368, Batch Gradient Norm: 25.432507478184856
Epoch: 6368, Batch Gradient Norm after: 22.36067977964477
Epoch 6369/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7562975764274598
Epoch: 6369, Batch Gradient Norm: 25.289704497130508
Epoch: 6369, Batch Gradient Norm after: 22.360678829218344
Epoch 6370/10000, Prediction Accuracy = 59.746%, Loss = 0.7557669520378113
Epoch: 6370, Batch Gradient Norm: 25.416450866774277
Epoch: 6370, Batch Gradient Norm after: 22.360679233699372
Epoch 6371/10000, Prediction Accuracy = 59.769999999999996%, Loss = 0.7561607956886292
Epoch: 6371, Batch Gradient Norm: 25.273025001742937
Epoch: 6371, Batch Gradient Norm after: 22.347470815088474
Epoch 6372/10000, Prediction Accuracy = 59.717999999999996%, Loss = 0.7556268692016601
Epoch: 6372, Batch Gradient Norm: 25.43080613252647
Epoch: 6372, Batch Gradient Norm after: 22.360677604892565
Epoch 6373/10000, Prediction Accuracy = 59.75200000000001%, Loss = 0.7561630249023438
Epoch: 6373, Batch Gradient Norm: 25.276083892321303
Epoch: 6373, Batch Gradient Norm after: 22.360679974600643
Epoch 6374/10000, Prediction Accuracy = 59.682%, Loss = 0.7556024670600892
Epoch: 6374, Batch Gradient Norm: 25.41299446096965
Epoch: 6374, Batch Gradient Norm after: 22.36067892504061
Epoch 6375/10000, Prediction Accuracy = 59.742%, Loss = 0.7560797691345215
Epoch: 6375, Batch Gradient Norm: 25.269108814650572
Epoch: 6375, Batch Gradient Norm after: 22.353319278232075
Epoch 6376/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7554776787757873
Epoch: 6376, Batch Gradient Norm: 25.413168406243333
Epoch: 6376, Batch Gradient Norm after: 22.36067806675254
Epoch 6377/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7559357166290284
Epoch: 6377, Batch Gradient Norm: 25.26939946612419
Epoch: 6377, Batch Gradient Norm after: 22.357464851740012
Epoch 6378/10000, Prediction Accuracy = 59.722%, Loss = 0.7553869962692261
Epoch: 6378, Batch Gradient Norm: 25.4042319376371
Epoch: 6378, Batch Gradient Norm after: 22.360679246262873
Epoch 6379/10000, Prediction Accuracy = 59.766%, Loss = 0.7558271765708924
Epoch: 6379, Batch Gradient Norm: 25.25109330851774
Epoch: 6379, Batch Gradient Norm after: 22.35033180913321
Epoch 6380/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7552803754806519
Epoch: 6380, Batch Gradient Norm: 25.409844498155458
Epoch: 6380, Batch Gradient Norm after: 22.360678581469372
Epoch 6381/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.755759310722351
Epoch: 6381, Batch Gradient Norm: 25.257012126776388
Epoch: 6381, Batch Gradient Norm after: 22.35908012016799
Epoch 6382/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.7551961898803711
Epoch: 6382, Batch Gradient Norm: 25.39743958348897
Epoch: 6382, Batch Gradient Norm after: 22.360679849983786
Epoch 6383/10000, Prediction Accuracy = 59.754%, Loss = 0.7556664228439331
Epoch: 6383, Batch Gradient Norm: 25.242731334777844
Epoch: 6383, Batch Gradient Norm after: 22.34957147898712
Epoch 6384/10000, Prediction Accuracy = 59.718%, Loss = 0.755113708972931
Epoch: 6384, Batch Gradient Norm: 25.403240492678428
Epoch: 6384, Batch Gradient Norm after: 22.360678535223887
Epoch 6385/10000, Prediction Accuracy = 59.734%, Loss = 0.7556638956069947
Epoch: 6385, Batch Gradient Norm: 25.244439239561153
Epoch: 6385, Batch Gradient Norm after: 22.36067978276107
Epoch 6386/10000, Prediction Accuracy = 59.702%, Loss = 0.7550463795661926
Epoch: 6386, Batch Gradient Norm: 25.385022491314363
Epoch: 6386, Batch Gradient Norm after: 22.36067963802289
Epoch 6387/10000, Prediction Accuracy = 59.74399999999999%, Loss = 0.7555054664611817
Epoch: 6387, Batch Gradient Norm: 25.228594577661614
Epoch: 6387, Batch Gradient Norm after: 22.34906918679047
Epoch 6388/10000, Prediction Accuracy = 59.718%, Loss = 0.7548979878425598
Epoch: 6388, Batch Gradient Norm: 25.39610769183432
Epoch: 6388, Batch Gradient Norm after: 22.360677499346927
Epoch 6389/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7554086685180664
Epoch: 6389, Batch Gradient Norm: 25.236844356074588
Epoch: 6389, Batch Gradient Norm after: 22.360676473769992
Epoch 6390/10000, Prediction Accuracy = 59.714%, Loss = 0.7548597574234008
Epoch: 6390, Batch Gradient Norm: 25.377745629201875
Epoch: 6390, Batch Gradient Norm after: 22.36068044574043
Epoch 6391/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7553006172180176
Epoch: 6391, Batch Gradient Norm: 25.2148397690778
Epoch: 6391, Batch Gradient Norm after: 22.345714683935526
Epoch 6392/10000, Prediction Accuracy = 59.71199999999999%, Loss = 0.7547129392623901
Epoch: 6392, Batch Gradient Norm: 25.393994745651444
Epoch: 6392, Batch Gradient Norm after: 22.36067789937374
Epoch 6393/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7552446365356446
Epoch: 6393, Batch Gradient Norm: 25.221289887079347
Epoch: 6393, Batch Gradient Norm after: 22.36067889476496
Epoch 6394/10000, Prediction Accuracy = 59.733999999999995%, Loss = 0.7546556234359741
Epoch: 6394, Batch Gradient Norm: 25.376043631605743
Epoch: 6394, Batch Gradient Norm after: 22.360677929955152
Epoch 6395/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.755204439163208
Epoch: 6395, Batch Gradient Norm: 25.212529707040524
Epoch: 6395, Batch Gradient Norm after: 22.352872299007547
Epoch 6396/10000, Prediction Accuracy = 59.676%, Loss = 0.7545975685119629
Epoch: 6396, Batch Gradient Norm: 25.37343193933839
Epoch: 6396, Batch Gradient Norm after: 22.36067570332513
Epoch 6397/10000, Prediction Accuracy = 59.734%, Loss = 0.7551194548606872
Epoch: 6397, Batch Gradient Norm: 25.210453811870035
Epoch: 6397, Batch Gradient Norm after: 22.357004849359818
Epoch 6398/10000, Prediction Accuracy = 59.698%, Loss = 0.754474425315857
Epoch: 6398, Batch Gradient Norm: 25.367067080513866
Epoch: 6398, Batch Gradient Norm after: 22.360678039595435
Epoch 6399/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7549611449241638
Epoch: 6399, Batch Gradient Norm: 25.20660470359156
Epoch: 6399, Batch Gradient Norm after: 22.35359865613422
Epoch 6400/10000, Prediction Accuracy = 59.702%, Loss = 0.7543871760368347
Epoch: 6400, Batch Gradient Norm: 25.363808752994757
Epoch: 6400, Batch Gradient Norm after: 22.360680995610355
Epoch 6401/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7548803329467774
Epoch: 6401, Batch Gradient Norm: 25.19323910422589
Epoch: 6401, Batch Gradient Norm after: 22.352033966902788
Epoch 6402/10000, Prediction Accuracy = 59.726%, Loss = 0.7542866349220276
Epoch: 6402, Batch Gradient Norm: 25.36736136247984
Epoch: 6402, Batch Gradient Norm after: 22.36067740799116
Epoch 6403/10000, Prediction Accuracy = 59.774%, Loss = 0.7547966361045837
Epoch: 6403, Batch Gradient Norm: 25.200495774962576
Epoch: 6403, Batch Gradient Norm after: 22.359264864891117
Epoch 6404/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.754201078414917
Epoch: 6404, Batch Gradient Norm: 25.35229852674844
Epoch: 6404, Batch Gradient Norm after: 22.36067854195233
Epoch 6405/10000, Prediction Accuracy = 59.73%, Loss = 0.7547278642654419
Epoch: 6405, Batch Gradient Norm: 25.17827103925538
Epoch: 6405, Batch Gradient Norm after: 22.345522853906797
Epoch 6406/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7541095614433289
Epoch: 6406, Batch Gradient Norm: 25.366287545651986
Epoch: 6406, Batch Gradient Norm after: 22.36067659664192
Epoch 6407/10000, Prediction Accuracy = 59.734%, Loss = 0.7547211527824402
Epoch: 6407, Batch Gradient Norm: 25.18092189022116
Epoch: 6407, Batch Gradient Norm after: 22.36067959798356
Epoch 6408/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.7540199637413025
Epoch: 6408, Batch Gradient Norm: 25.345861089527347
Epoch: 6408, Batch Gradient Norm after: 22.360678417285108
Epoch 6409/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7545399308204651
Epoch: 6409, Batch Gradient Norm: 25.173845308974414
Epoch: 6409, Batch Gradient Norm after: 22.350645572479134
Epoch 6410/10000, Prediction Accuracy = 59.758%, Loss = 0.7539051294326782
Epoch: 6410, Batch Gradient Norm: 25.352266673748346
Epoch: 6410, Batch Gradient Norm after: 22.360677471272155
Epoch 6411/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7544585227966308
Epoch: 6411, Batch Gradient Norm: 25.175579913583665
Epoch: 6411, Batch Gradient Norm after: 22.357777033035237
Epoch 6412/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7538477540016174
Epoch: 6412, Batch Gradient Norm: 25.340718106907758
Epoch: 6412, Batch Gradient Norm after: 22.36067793739958
Epoch 6413/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7543551802635193
Epoch: 6413, Batch Gradient Norm: 25.159713821469452
Epoch: 6413, Batch Gradient Norm after: 22.34868250908034
Epoch 6414/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.753704023361206
Epoch: 6414, Batch Gradient Norm: 25.349605080779497
Epoch: 6414, Batch Gradient Norm after: 22.360678244349018
Epoch 6415/10000, Prediction Accuracy = 59.77%, Loss = 0.7542986273765564
Epoch: 6415, Batch Gradient Norm: 25.16943782863357
Epoch: 6415, Batch Gradient Norm after: 22.360679201750127
Epoch 6416/10000, Prediction Accuracy = 59.73199999999999%, Loss = 0.7536831498146057
Epoch: 6416, Batch Gradient Norm: 25.33034752076002
Epoch: 6416, Batch Gradient Norm after: 22.36067542345007
Epoch 6417/10000, Prediction Accuracy = 59.742%, Loss = 0.7542468070983886
Epoch: 6417, Batch Gradient Norm: 25.146464631745978
Epoch: 6417, Batch Gradient Norm after: 22.34657259645299
Epoch 6418/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.7535698175430298
Epoch: 6418, Batch Gradient Norm: 25.342599905887354
Epoch: 6418, Batch Gradient Norm after: 22.360677371612955
Epoch 6419/10000, Prediction Accuracy = 59.754%, Loss = 0.7541623473167419
Epoch: 6419, Batch Gradient Norm: 25.15423333284595
Epoch: 6419, Batch Gradient Norm after: 22.360678418822474
Epoch 6420/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7534639477729798
Epoch: 6420, Batch Gradient Norm: 25.323175627095893
Epoch: 6420, Batch Gradient Norm after: 22.360677453410535
Epoch 6421/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7539965391159058
Epoch: 6421, Batch Gradient Norm: 25.132815393306533
Epoch: 6421, Batch Gradient Norm after: 22.344645178921752
Epoch 6422/10000, Prediction Accuracy = 59.758%, Loss = 0.7533580780029296
Epoch: 6422, Batch Gradient Norm: 25.3377970647788
Epoch: 6422, Batch Gradient Norm after: 22.36068087330856
Epoch 6423/10000, Prediction Accuracy = 59.766%, Loss = 0.7539591670036316
Epoch: 6423, Batch Gradient Norm: 25.145403106843325
Epoch: 6423, Batch Gradient Norm after: 22.360677613668507
Epoch 6424/10000, Prediction Accuracy = 59.714%, Loss = 0.7532995939254761
Epoch: 6424, Batch Gradient Norm: 25.315424310223516
Epoch: 6424, Batch Gradient Norm after: 22.36067783248314
Epoch 6425/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7538118124008178
Epoch: 6425, Batch Gradient Norm: 25.124485347794714
Epoch: 6425, Batch Gradient Norm after: 22.342900853841613
Epoch 6426/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7531665682792663
Epoch: 6426, Batch Gradient Norm: 25.332321481685685
Epoch: 6426, Batch Gradient Norm after: 22.360676928299718
Epoch 6427/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7538374781608581
Epoch: 6427, Batch Gradient Norm: 25.131365067812837
Epoch: 6427, Batch Gradient Norm after: 22.36067869662802
Epoch 6428/10000, Prediction Accuracy = 59.712%, Loss = 0.7531444668769837
Epoch: 6428, Batch Gradient Norm: 25.30891026894031
Epoch: 6428, Batch Gradient Norm after: 22.360677846596527
Epoch 6429/10000, Prediction Accuracy = 59.754%, Loss = 0.7537262558937072
Epoch: 6429, Batch Gradient Norm: 25.11602049594168
Epoch: 6429, Batch Gradient Norm after: 22.34842551969512
Epoch 6430/10000, Prediction Accuracy = 59.721999999999994%, Loss = 0.7530012607574463
Epoch: 6430, Batch Gradient Norm: 25.314586057329638
Epoch: 6430, Batch Gradient Norm after: 22.360677398475097
Epoch 6431/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7536046028137207
Epoch: 6431, Batch Gradient Norm: 25.121486695885775
Epoch: 6431, Batch Gradient Norm after: 22.35692897038576
Epoch 6432/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.75293048620224
Epoch: 6432, Batch Gradient Norm: 25.301258837263536
Epoch: 6432, Batch Gradient Norm after: 22.360677252055464
Epoch 6433/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7534919738769531
Epoch: 6433, Batch Gradient Norm: 25.100426825808352
Epoch: 6433, Batch Gradient Norm after: 22.345329578447334
Epoch 6434/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7528167366981506
Epoch: 6434, Batch Gradient Norm: 25.31174158416543
Epoch: 6434, Batch Gradient Norm after: 22.36067865036011
Epoch 6435/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.753429901599884
Epoch: 6435, Batch Gradient Norm: 25.112544991008175
Epoch: 6435, Batch Gradient Norm after: 22.36067645875094
Epoch 6436/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7527484774589539
Epoch: 6436, Batch Gradient Norm: 25.293032412572025
Epoch: 6436, Batch Gradient Norm after: 22.360677795856468
Epoch 6437/10000, Prediction Accuracy = 59.758%, Loss = 0.7533343195915222
Epoch: 6437, Batch Gradient Norm: 25.087276836561422
Epoch: 6437, Batch Gradient Norm after: 22.34203369717214
Epoch 6438/10000, Prediction Accuracy = 59.73%, Loss = 0.75265052318573
Epoch: 6438, Batch Gradient Norm: 25.312064790740877
Epoch: 6438, Batch Gradient Norm after: 22.36067630226626
Epoch 6439/10000, Prediction Accuracy = 59.738%, Loss = 0.7533564925193786
Epoch: 6439, Batch Gradient Norm: 25.09335378105141
Epoch: 6439, Batch Gradient Norm after: 22.360679327144027
Epoch 6440/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7525765895843506
Epoch: 6440, Batch Gradient Norm: 25.290713958337687
Epoch: 6440, Batch Gradient Norm after: 22.36067681398564
Epoch 6441/10000, Prediction Accuracy = 59.758%, Loss = 0.7531725406646729
Epoch: 6441, Batch Gradient Norm: 25.0861291181525
Epoch: 6441, Batch Gradient Norm after: 22.350627428390776
Epoch 6442/10000, Prediction Accuracy = 59.758%, Loss = 0.7524571299552918
Epoch: 6442, Batch Gradient Norm: 25.29290556203965
Epoch: 6442, Batch Gradient Norm after: 22.36067859807665
Epoch 6443/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7530771851539612
Epoch: 6443, Batch Gradient Norm: 25.081995504933047
Epoch: 6443, Batch Gradient Norm after: 22.354752211597468
Epoch 6444/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7524013757705689
Epoch: 6444, Batch Gradient Norm: 25.284658579316375
Epoch: 6444, Batch Gradient Norm after: 22.3606775060971
Epoch 6445/10000, Prediction Accuracy = 59.766%, Loss = 0.7529875874519348
Epoch: 6445, Batch Gradient Norm: 25.073832665478953
Epoch: 6445, Batch Gradient Norm after: 22.349509747874595
Epoch 6446/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7522719502449036
Epoch: 6446, Batch Gradient Norm: 25.290133765734186
Epoch: 6446, Batch Gradient Norm after: 22.360677385895144
Epoch 6447/10000, Prediction Accuracy = 59.802%, Loss = 0.7529162764549255
Epoch: 6447, Batch Gradient Norm: 25.07908378937163
Epoch: 6447, Batch Gradient Norm after: 22.35758300764082
Epoch 6448/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7522413849830627
Epoch: 6448, Batch Gradient Norm: 25.275467721068843
Epoch: 6448, Batch Gradient Norm after: 22.360673369974776
Epoch 6449/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7528917551040649
Epoch: 6449, Batch Gradient Norm: 25.058107613765763
Epoch: 6449, Batch Gradient Norm after: 22.346771322237217
Epoch 6450/10000, Prediction Accuracy = 59.722%, Loss = 0.7521327376365662
Epoch: 6450, Batch Gradient Norm: 25.28262686001004
Epoch: 6450, Batch Gradient Norm after: 22.36067654956179
Epoch 6451/10000, Prediction Accuracy = 59.746%, Loss = 0.7527929306030273
Epoch: 6451, Batch Gradient Norm: 25.067300243112285
Epoch: 6451, Batch Gradient Norm after: 22.360678588565346
Epoch 6452/10000, Prediction Accuracy = 59.734%, Loss = 0.7520357847213746
Epoch: 6452, Batch Gradient Norm: 25.26310546591469
Epoch: 6452, Batch Gradient Norm after: 22.360680924570616
Epoch 6453/10000, Prediction Accuracy = 59.802%, Loss = 0.7526221871376038
Epoch: 6453, Batch Gradient Norm: 25.04961036919351
Epoch: 6453, Batch Gradient Norm after: 22.346905220501803
Epoch 6454/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7519319176673889
Epoch: 6454, Batch Gradient Norm: 25.273764541696295
Epoch: 6454, Batch Gradient Norm after: 22.360676257217357
Epoch 6455/10000, Prediction Accuracy = 59.762%, Loss = 0.7525774240493774
Epoch: 6455, Batch Gradient Norm: 25.056406128004813
Epoch: 6455, Batch Gradient Norm after: 22.360679771980635
Epoch 6456/10000, Prediction Accuracy = 59.74399999999999%, Loss = 0.7518656253814697
Epoch: 6456, Batch Gradient Norm: 25.25276997631547
Epoch: 6456, Batch Gradient Norm after: 22.36067906783042
Epoch 6457/10000, Prediction Accuracy = 59.786%, Loss = 0.7524438500404358
Epoch: 6457, Batch Gradient Norm: 25.036731901511715
Epoch: 6457, Batch Gradient Norm after: 22.34555466725793
Epoch 6458/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7517348408699036
Epoch: 6458, Batch Gradient Norm: 25.271707912065498
Epoch: 6458, Batch Gradient Norm after: 22.360678993642672
Epoch 6459/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7524616479873657
Epoch: 6459, Batch Gradient Norm: 25.038970940128934
Epoch: 6459, Batch Gradient Norm after: 22.36067990027173
Epoch 6460/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7517102479934692
Epoch: 6460, Batch Gradient Norm: 25.251590899757513
Epoch: 6460, Batch Gradient Norm after: 22.360676853468053
Epoch 6461/10000, Prediction Accuracy = 59.746%, Loss = 0.752357828617096
Epoch: 6461, Batch Gradient Norm: 25.027070032809565
Epoch: 6461, Batch Gradient Norm after: 22.3511936383275
Epoch 6462/10000, Prediction Accuracy = 59.698%, Loss = 0.7515772700309753
Epoch: 6462, Batch Gradient Norm: 25.2562662975624
Epoch: 6462, Batch Gradient Norm after: 22.36067443459428
Epoch 6463/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7522336959838867
Epoch: 6463, Batch Gradient Norm: 25.0360488800342
Epoch: 6463, Batch Gradient Norm after: 22.360677581531938
Epoch 6464/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7515098571777343
Epoch: 6464, Batch Gradient Norm: 25.239421785641134
Epoch: 6464, Batch Gradient Norm after: 22.36067886684898
Epoch 6465/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7521167755126953
Epoch: 6465, Batch Gradient Norm: 25.009094148592254
Epoch: 6465, Batch Gradient Norm after: 22.344365336625973
Epoch 6466/10000, Prediction Accuracy = 59.74399999999999%, Loss = 0.7513814330101013
Epoch: 6466, Batch Gradient Norm: 25.253982673548524
Epoch: 6466, Batch Gradient Norm after: 22.360679255530858
Epoch 6467/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7520705699920655
Epoch: 6467, Batch Gradient Norm: 25.016183137277572
Epoch: 6467, Batch Gradient Norm after: 22.360677509599064
Epoch 6468/10000, Prediction Accuracy = 59.75999999999999%, Loss = 0.7512971997261048
Epoch: 6468, Batch Gradient Norm: 25.238844456270737
Epoch: 6468, Batch Gradient Norm after: 22.36067891857019
Epoch 6469/10000, Prediction Accuracy = 59.754%, Loss = 0.7519807577133178
Epoch: 6469, Batch Gradient Norm: 25.005665481122712
Epoch: 6469, Batch Gradient Norm after: 22.350449308444656
Epoch 6470/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7512478709220887
Epoch: 6470, Batch Gradient Norm: 25.239097920418114
Epoch: 6470, Batch Gradient Norm after: 22.36067843055848
Epoch 6471/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7519729971885681
Epoch: 6471, Batch Gradient Norm: 25.008699248188307
Epoch: 6471, Batch Gradient Norm after: 22.35900243394955
Epoch 6472/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7511776804924011
Epoch: 6472, Batch Gradient Norm: 25.222731618083234
Epoch: 6472, Batch Gradient Norm after: 22.360677770864033
Epoch 6473/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7518016695976257
Epoch: 6473, Batch Gradient Norm: 24.98952369049085
Epoch: 6473, Batch Gradient Norm after: 22.345359906883857
Epoch 6474/10000, Prediction Accuracy = 59.754%, Loss = 0.7510082483291626
Epoch: 6474, Batch Gradient Norm: 25.23806246962096
Epoch: 6474, Batch Gradient Norm after: 22.36067928454925
Epoch 6475/10000, Prediction Accuracy = 59.762%, Loss = 0.7517272233963013
Epoch: 6475, Batch Gradient Norm: 25.001047585760666
Epoch: 6475, Batch Gradient Norm after: 22.360679952517934
Epoch 6476/10000, Prediction Accuracy = 59.726%, Loss = 0.750988757610321
Epoch: 6476, Batch Gradient Norm: 25.215852389310463
Epoch: 6476, Batch Gradient Norm after: 22.360677982756556
Epoch 6477/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7516027808189392
Epoch: 6477, Batch Gradient Norm: 24.97727773195505
Epoch: 6477, Batch Gradient Norm after: 22.34387310401841
Epoch 6478/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7508249998092651
Epoch: 6478, Batch Gradient Norm: 25.234276629438305
Epoch: 6478, Batch Gradient Norm after: 22.360677464380327
Epoch 6479/10000, Prediction Accuracy = 59.806%, Loss = 0.7515696883201599
Epoch: 6479, Batch Gradient Norm: 24.988358596445433
Epoch: 6479, Batch Gradient Norm after: 22.36067842291871
Epoch 6480/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.7508002161979676
Epoch: 6480, Batch Gradient Norm: 25.211784031831783
Epoch: 6480, Batch Gradient Norm after: 22.36068003959764
Epoch 6481/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7515191793441772
Epoch: 6481, Batch Gradient Norm: 24.96672729169963
Epoch: 6481, Batch Gradient Norm after: 22.346046868019467
Epoch 6482/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.7506993889808655
Epoch: 6482, Batch Gradient Norm: 25.220348515429357
Epoch: 6482, Batch Gradient Norm after: 22.36067798180569
Epoch 6483/10000, Prediction Accuracy = 59.77%, Loss = 0.7514292478561402
Epoch: 6483, Batch Gradient Norm: 24.9796991081948
Epoch: 6483, Batch Gradient Norm after: 22.360678391478757
Epoch 6484/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7506137609481811
Epoch: 6484, Batch Gradient Norm: 25.199076715423445
Epoch: 6484, Batch Gradient Norm after: 22.360680708396373
Epoch 6485/10000, Prediction Accuracy = 59.802%, Loss = 0.7512591719627381
Epoch: 6485, Batch Gradient Norm: 24.95334790097707
Epoch: 6485, Batch Gradient Norm after: 22.339010869739628
Epoch 6486/10000, Prediction Accuracy = 59.738000000000014%, Loss = 0.7504714727401733
Epoch: 6486, Batch Gradient Norm: 25.223151702619575
Epoch: 6486, Batch Gradient Norm after: 22.36067816777902
Epoch 6487/10000, Prediction Accuracy = 59.766%, Loss = 0.7512452960014343
Epoch: 6487, Batch Gradient Norm: 24.958045896564773
Epoch: 6487, Batch Gradient Norm after: 22.360679777223837
Epoch 6488/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.750409734249115
Epoch: 6488, Batch Gradient Norm: 25.19916091727795
Epoch: 6488, Batch Gradient Norm after: 22.36067836207233
Epoch 6489/10000, Prediction Accuracy = 59.786%, Loss = 0.751102340221405
Epoch: 6489, Batch Gradient Norm: 24.948845646152716
Epoch: 6489, Batch Gradient Norm after: 22.347127108998873
Epoch 6490/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7503065943717957
Epoch: 6490, Batch Gradient Norm: 25.20712841677192
Epoch: 6490, Batch Gradient Norm after: 22.360678970091328
Epoch 6491/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7510963439941406
Epoch: 6491, Batch Gradient Norm: 24.953291574465837
Epoch: 6491, Batch Gradient Norm after: 22.358137925715774
Epoch 6492/10000, Prediction Accuracy = 59.738%, Loss = 0.7503013610839844
Epoch: 6492, Batch Gradient Norm: 25.186299129969594
Epoch: 6492, Batch Gradient Norm after: 22.36067851465129
Epoch 6493/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.751002311706543
Epoch: 6493, Batch Gradient Norm: 24.931032692435867
Epoch: 6493, Batch Gradient Norm after: 22.343947771898264
Epoch 6494/10000, Prediction Accuracy = 59.714%, Loss = 0.750130295753479
Epoch: 6494, Batch Gradient Norm: 25.200203991069408
Epoch: 6494, Batch Gradient Norm after: 22.360679769356434
Epoch 6495/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.7508867621421814
Epoch: 6495, Batch Gradient Norm: 24.94528299134604
Epoch: 6495, Batch Gradient Norm after: 22.36067971268807
Epoch 6496/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7500826835632324
Epoch: 6496, Batch Gradient Norm: 25.178321084771902
Epoch: 6496, Batch Gradient Norm after: 22.36067794829202
Epoch 6497/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.750758147239685
Epoch: 6497, Batch Gradient Norm: 24.91880579236887
Epoch: 6497, Batch Gradient Norm after: 22.340276537080104
Epoch 6498/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7499497413635254
Epoch: 6498, Batch Gradient Norm: 25.198737117057078
Epoch: 6498, Batch Gradient Norm after: 22.36067853531624
Epoch 6499/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7507199645042419
Epoch: 6499, Batch Gradient Norm: 24.92603611411091
Epoch: 6499, Batch Gradient Norm after: 22.36067776898735
Epoch 6500/10000, Prediction Accuracy = 59.75%, Loss = 0.7498725771903991
Epoch: 6500, Batch Gradient Norm: 25.176073136196834
Epoch: 6500, Batch Gradient Norm after: 22.36067962267646
Epoch 6501/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7506257891654968
Epoch: 6501, Batch Gradient Norm: 24.913913561136994
Epoch: 6501, Batch Gradient Norm after: 22.346344513232374
Epoch 6502/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7498096585273742
Epoch: 6502, Batch Gradient Norm: 25.183029041249274
Epoch: 6502, Batch Gradient Norm after: 22.360678463220957
Epoch 6503/10000, Prediction Accuracy = 59.73%, Loss = 0.7506321549415589
Epoch: 6503, Batch Gradient Norm: 24.919428074260363
Epoch: 6503, Batch Gradient Norm after: 22.359410178415047
Epoch 6504/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.7497438549995422
Epoch: 6504, Batch Gradient Norm: 25.162647670466434
Epoch: 6504, Batch Gradient Norm after: 22.36067733877473
Epoch 6505/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7504402279853821
Epoch: 6505, Batch Gradient Norm: 24.89942666914134
Epoch: 6505, Batch Gradient Norm after: 22.343524399430603
Epoch 6506/10000, Prediction Accuracy = 59.732000000000006%, Loss = 0.7495793104171753
Epoch: 6506, Batch Gradient Norm: 25.182677356398703
Epoch: 6506, Batch Gradient Norm after: 22.36067834139456
Epoch 6507/10000, Prediction Accuracy = 59.766%, Loss = 0.7503808498382568
Epoch: 6507, Batch Gradient Norm: 24.908441322048702
Epoch: 6507, Batch Gradient Norm after: 22.36067804292656
Epoch 6508/10000, Prediction Accuracy = 59.742%, Loss = 0.7495493650436401
Epoch: 6508, Batch Gradient Norm: 25.15765794649142
Epoch: 6508, Batch Gradient Norm after: 22.360680222633835
Epoch 6509/10000, Prediction Accuracy = 59.762%, Loss = 0.7502609014511108
Epoch: 6509, Batch Gradient Norm: 24.8854249969345
Epoch: 6509, Batch Gradient Norm after: 22.34407537847534
Epoch 6510/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7493968963623047
Epoch: 6510, Batch Gradient Norm: 25.175943978175898
Epoch: 6510, Batch Gradient Norm after: 22.360677668517575
Epoch 6511/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7502139449119568
Epoch: 6511, Batch Gradient Norm: 24.897478520764206
Epoch: 6511, Batch Gradient Norm after: 22.36068013898779
Epoch 6512/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7493720293045044
Epoch: 6512, Batch Gradient Norm: 25.150776822196917
Epoch: 6512, Batch Gradient Norm after: 22.360677906374498
Epoch 6513/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7501598834991455
Epoch: 6513, Batch Gradient Norm: 24.876716929403177
Epoch: 6513, Batch Gradient Norm after: 22.344350818643512
Epoch 6514/10000, Prediction Accuracy = 59.742%, Loss = 0.7492760062217713
Epoch: 6514, Batch Gradient Norm: 25.164156174070182
Epoch: 6514, Batch Gradient Norm after: 22.360676243988777
Epoch 6515/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7500989198684692
Epoch: 6515, Batch Gradient Norm: 24.886524168648318
Epoch: 6515, Batch Gradient Norm after: 22.36067832913041
Epoch 6516/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7491854429244995
Epoch: 6516, Batch Gradient Norm: 25.14143916317183
Epoch: 6516, Batch Gradient Norm after: 22.360677986260818
Epoch 6517/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7499107480049133
Epoch: 6517, Batch Gradient Norm: 24.865063376729005
Epoch: 6517, Batch Gradient Norm after: 22.341774825919806
Epoch 6518/10000, Prediction Accuracy = 59.745999999999995%, Loss = 0.7490594625473023
Epoch: 6518, Batch Gradient Norm: 25.160235331322173
Epoch: 6518, Batch Gradient Norm after: 22.36067764880742
Epoch 6519/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7498933792114257
Epoch: 6519, Batch Gradient Norm: 24.871715234110347
Epoch: 6519, Batch Gradient Norm after: 22.36067911619087
Epoch 6520/10000, Prediction Accuracy = 59.733999999999995%, Loss = 0.7490113615989685
Epoch: 6520, Batch Gradient Norm: 25.13571830672022
Epoch: 6520, Batch Gradient Norm after: 22.360675798188414
Epoch 6521/10000, Prediction Accuracy = 59.79%, Loss = 0.7497484564781189
Epoch: 6521, Batch Gradient Norm: 24.85567545272903
Epoch: 6521, Batch Gradient Norm after: 22.34407005848393
Epoch 6522/10000, Prediction Accuracy = 59.754%, Loss = 0.7488737225532531
Epoch: 6522, Batch Gradient Norm: 25.151793949330564
Epoch: 6522, Batch Gradient Norm after: 22.360678813626198
Epoch 6523/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7497500777244568
Epoch: 6523, Batch Gradient Norm: 24.86507782040277
Epoch: 6523, Batch Gradient Norm after: 22.36067891912523
Epoch 6524/10000, Prediction Accuracy = 59.75%, Loss = 0.7488778948783874
Epoch: 6524, Batch Gradient Norm: 25.12632822495379
Epoch: 6524, Batch Gradient Norm after: 22.36067655040263
Epoch 6525/10000, Prediction Accuracy = 59.746%, Loss = 0.7496607303619385
Epoch: 6525, Batch Gradient Norm: 24.84247042848832
Epoch: 6525, Batch Gradient Norm after: 22.34345992613091
Epoch 6526/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.7487123370170593
Epoch: 6526, Batch Gradient Norm: 25.14061961974885
Epoch: 6526, Batch Gradient Norm after: 22.36067862151707
Epoch 6527/10000, Prediction Accuracy = 59.8%, Loss = 0.7495452284812927
Epoch: 6527, Batch Gradient Norm: 24.857700426397273
Epoch: 6527, Batch Gradient Norm after: 22.36067797815267
Epoch 6528/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7486608266830445
Epoch: 6528, Batch Gradient Norm: 25.11738075970152
Epoch: 6528, Batch Gradient Norm after: 22.360679345784582
Epoch 6529/10000, Prediction Accuracy = 59.77%, Loss = 0.7494175434112549
Epoch: 6529, Batch Gradient Norm: 24.832832093609376
Epoch: 6529, Batch Gradient Norm after: 22.34078639204536
Epoch 6530/10000, Prediction Accuracy = 59.722%, Loss = 0.7485379099845886
Epoch: 6530, Batch Gradient Norm: 25.135324897128985
Epoch: 6530, Batch Gradient Norm after: 22.360678982606792
Epoch 6531/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.7493780493736267
Epoch: 6531, Batch Gradient Norm: 24.84386240871073
Epoch: 6531, Batch Gradient Norm after: 22.360677781854665
Epoch 6532/10000, Prediction Accuracy = 59.754%, Loss = 0.7484624505043029
Epoch: 6532, Batch Gradient Norm: 25.113162671395536
Epoch: 6532, Batch Gradient Norm after: 22.360676436495044
Epoch 6533/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7492648124694824
Epoch: 6533, Batch Gradient Norm: 24.82925898185729
Epoch: 6533, Batch Gradient Norm after: 22.34651031913476
Epoch 6534/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.7483893632888794
Epoch: 6534, Batch Gradient Norm: 25.121369327774506
Epoch: 6534, Batch Gradient Norm after: 22.36067709295551
Epoch 6535/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7492730140686035
Epoch: 6535, Batch Gradient Norm: 24.83704412434045
Epoch: 6535, Batch Gradient Norm after: 22.360416756447062
Epoch 6536/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.7483396291732788
Epoch: 6536, Batch Gradient Norm: 25.09900870350626
Epoch: 6536, Batch Gradient Norm after: 22.36067779038998
Epoch 6537/10000, Prediction Accuracy = 59.778000000000006%, Loss = 0.7491000175476075
Epoch: 6537, Batch Gradient Norm: 24.805554117971738
Epoch: 6537, Batch Gradient Norm after: 22.33931279134882
Epoch 6538/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.7481566667556763
Epoch: 6538, Batch Gradient Norm: 25.126899968532403
Epoch: 6538, Batch Gradient Norm after: 22.36067443092273
Epoch 6539/10000, Prediction Accuracy = 59.81400000000001%, Loss = 0.7490565299987793
Epoch: 6539, Batch Gradient Norm: 24.815441370643587
Epoch: 6539, Batch Gradient Norm after: 22.360678922040858
Epoch 6540/10000, Prediction Accuracy = 59.742%, Loss = 0.748119568824768
Epoch: 6540, Batch Gradient Norm: 25.102615391889284
Epoch: 6540, Batch Gradient Norm after: 22.360677096674717
Epoch 6541/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.7489311575889588
Epoch: 6541, Batch Gradient Norm: 24.809499315786724
Epoch: 6541, Batch Gradient Norm after: 22.349438434022897
Epoch 6542/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.7480149269104004
Epoch: 6542, Batch Gradient Norm: 25.101729286476708
Epoch: 6542, Batch Gradient Norm after: 22.360679775765842
Epoch 6543/10000, Prediction Accuracy = 59.802%, Loss = 0.7488495230674743
Epoch: 6543, Batch Gradient Norm: 24.806894010726467
Epoch: 6543, Batch Gradient Norm after: 22.35215141875102
Epoch 6544/10000, Prediction Accuracy = 59.766%, Loss = 0.7479475259780883
Epoch: 6544, Batch Gradient Norm: 25.09839728585747
Epoch: 6544, Batch Gradient Norm after: 22.36067735012435
Epoch 6545/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7488300323486328
Epoch: 6545, Batch Gradient Norm: 24.798256432074457
Epoch: 6545, Batch Gradient Norm after: 22.35057599914906
Epoch 6546/10000, Prediction Accuracy = 59.75%, Loss = 0.7478899478912353
Epoch: 6546, Batch Gradient Norm: 25.092333820719194
Epoch: 6546, Batch Gradient Norm after: 22.360680467717426
Epoch 6547/10000, Prediction Accuracy = 59.75999999999999%, Loss = 0.7487401843070984
Epoch: 6547, Batch Gradient Norm: 24.797139031403642
Epoch: 6547, Batch Gradient Norm after: 22.35165178637545
Epoch 6548/10000, Prediction Accuracy = 59.726%, Loss = 0.7477643609046936
Epoch: 6548, Batch Gradient Norm: 25.086292625941542
Epoch: 6548, Batch Gradient Norm after: 22.36067607684576
Epoch 6549/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.7485906839370727
Epoch: 6549, Batch Gradient Norm: 24.788819137912263
Epoch: 6549, Batch Gradient Norm after: 22.346337747763098
Epoch 6550/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.7476686000823974
Epoch: 6550, Batch Gradient Norm: 25.09142765342777
Epoch: 6550, Batch Gradient Norm after: 22.3606759074467
Epoch 6551/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7485320210456848
Epoch: 6551, Batch Gradient Norm: 24.785947883544015
Epoch: 6551, Batch Gradient Norm after: 22.353443741164348
Epoch 6552/10000, Prediction Accuracy = 59.738%, Loss = 0.7476033329963684
Epoch: 6552, Batch Gradient Norm: 25.080243896506307
Epoch: 6552, Batch Gradient Norm after: 22.360679202320625
Epoch 6553/10000, Prediction Accuracy = 59.782%, Loss = 0.7484192490577698
Epoch: 6553, Batch Gradient Norm: 24.772722041547496
Epoch: 6553, Batch Gradient Norm after: 22.345733470064044
Epoch 6554/10000, Prediction Accuracy = 59.766000000000005%, Loss = 0.7474725246429443
Epoch: 6554, Batch Gradient Norm: 25.088752506391796
Epoch: 6554, Batch Gradient Norm after: 22.36067727943181
Epoch 6555/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7483975052833557
Epoch: 6555, Batch Gradient Norm: 24.783016500678716
Epoch: 6555, Batch Gradient Norm after: 22.35792933987501
Epoch 6556/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7474686145782471
Epoch: 6556, Batch Gradient Norm: 25.067686255275426
Epoch: 6556, Batch Gradient Norm after: 22.360677718115276
Epoch 6557/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7483254790306091
Epoch: 6557, Batch Gradient Norm: 24.753224681665863
Epoch: 6557, Batch Gradient Norm after: 22.340313329263964
Epoch 6558/10000, Prediction Accuracy = 59.738%, Loss = 0.7473040342330932
Epoch: 6558, Batch Gradient Norm: 25.087235843511266
Epoch: 6558, Batch Gradient Norm after: 22.360677944170046
Epoch 6559/10000, Prediction Accuracy = 59.798%, Loss = 0.7482284188270569
Epoch: 6559, Batch Gradient Norm: 24.770918953670797
Epoch: 6559, Batch Gradient Norm after: 22.36067789932118
Epoch 6560/10000, Prediction Accuracy = 59.766000000000005%, Loss = 0.7472494602203369
Epoch: 6560, Batch Gradient Norm: 25.060620211504986
Epoch: 6560, Batch Gradient Norm after: 22.36067699752408
Epoch 6561/10000, Prediction Accuracy = 59.762%, Loss = 0.748079800605774
Epoch: 6561, Batch Gradient Norm: 24.742546413857884
Epoch: 6561, Batch Gradient Norm after: 22.338316962834636
Epoch 6562/10000, Prediction Accuracy = 59.71600000000001%, Loss = 0.7471274495124817
Epoch: 6562, Batch Gradient Norm: 25.083195143326467
Epoch: 6562, Batch Gradient Norm after: 22.36067907416461
Epoch 6563/10000, Prediction Accuracy = 59.758%, Loss = 0.7480611085891724
Epoch: 6563, Batch Gradient Norm: 24.750724098150954
Epoch: 6563, Batch Gradient Norm after: 22.360680861415602
Epoch 6564/10000, Prediction Accuracy = 59.762%, Loss = 0.7470417857170105
Epoch: 6564, Batch Gradient Norm: 25.059591734053537
Epoch: 6564, Batch Gradient Norm after: 22.360680304523537
Epoch 6565/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7479427456855774
Epoch: 6565, Batch Gradient Norm: 24.743604315276365
Epoch: 6565, Batch Gradient Norm after: 22.34792628309026
Epoch 6566/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7469866037368774
Epoch: 6566, Batch Gradient Norm: 25.063154243937973
Epoch: 6566, Batch Gradient Norm after: 22.360677415814443
Epoch 6567/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.7479533553123474
Epoch: 6567, Batch Gradient Norm: 24.74468338089113
Epoch: 6567, Batch Gradient Norm after: 22.357408996191957
Epoch 6568/10000, Prediction Accuracy = 59.762%, Loss = 0.7469408512115479
Epoch: 6568, Batch Gradient Norm: 25.04485105336688
Epoch: 6568, Batch Gradient Norm after: 22.360678811721733
Epoch 6569/10000, Prediction Accuracy = 59.767999999999994%, Loss = 0.7477949261665344
Epoch: 6569, Batch Gradient Norm: 24.725634578973985
Epoch: 6569, Batch Gradient Norm after: 22.343483479204156
Epoch 6570/10000, Prediction Accuracy = 59.754%, Loss = 0.7467633008956909
Epoch: 6570, Batch Gradient Norm: 25.06112387685252
Epoch: 6570, Batch Gradient Norm after: 22.36067762084448
Epoch 6571/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.747711718082428
Epoch: 6571, Batch Gradient Norm: 24.73980204973573
Epoch: 6571, Batch Gradient Norm after: 22.360678618957
Epoch 6572/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7467456579208374
Epoch: 6572, Batch Gradient Norm: 25.03669726441022
Epoch: 6572, Batch Gradient Norm after: 22.36067740510608
Epoch 6573/10000, Prediction Accuracy = 59.746%, Loss = 0.7475880146026611
Epoch: 6573, Batch Gradient Norm: 24.71527665450072
Epoch: 6573, Batch Gradient Norm after: 22.34313057603494
Epoch 6574/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7465937733650208
Epoch: 6574, Batch Gradient Norm: 25.055893831927953
Epoch: 6574, Batch Gradient Norm after: 22.360676679250417
Epoch 6575/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.747549307346344
Epoch: 6575, Batch Gradient Norm: 24.72452474834007
Epoch: 6575, Batch Gradient Norm after: 22.36067872189124
Epoch 6576/10000, Prediction Accuracy = 59.774%, Loss = 0.7465479969978333
Epoch: 6576, Batch Gradient Norm: 25.032987901652767
Epoch: 6576, Batch Gradient Norm after: 22.36067843295584
Epoch 6577/10000, Prediction Accuracy = 59.788%, Loss = 0.7474826574325562
Epoch: 6577, Batch Gradient Norm: 24.702403746476946
Epoch: 6577, Batch Gradient Norm after: 22.344101277401766
Epoch 6578/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7464586734771729
Epoch: 6578, Batch Gradient Norm: 25.0463950964712
Epoch: 6578, Batch Gradient Norm after: 22.36067748194662
Epoch 6579/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7474365353584289
Epoch: 6579, Batch Gradient Norm: 24.708226508183465
Epoch: 6579, Batch Gradient Norm after: 22.36067760975876
Epoch 6580/10000, Prediction Accuracy = 59.742000000000004%, Loss = 0.7463709592819214
Epoch: 6580, Batch Gradient Norm: 25.025472506769937
Epoch: 6580, Batch Gradient Norm after: 22.36067888438318
Epoch 6581/10000, Prediction Accuracy = 59.826%, Loss = 0.747262442111969
Epoch: 6581, Batch Gradient Norm: 24.692599478304434
Epoch: 6581, Batch Gradient Norm after: 22.344987203470453
Epoch 6582/10000, Prediction Accuracy = 59.766%, Loss = 0.7462547540664672
Epoch: 6582, Batch Gradient Norm: 25.039314979940052
Epoch: 6582, Batch Gradient Norm after: 22.360679565368766
Epoch 6583/10000, Prediction Accuracy = 59.77%, Loss = 0.7472229838371277
Epoch: 6583, Batch Gradient Norm: 24.702999918619735
Epoch: 6583, Batch Gradient Norm after: 22.360678803770533
Epoch 6584/10000, Prediction Accuracy = 59.75%, Loss = 0.7462167620658875
Epoch: 6584, Batch Gradient Norm: 25.014436190457076
Epoch: 6584, Batch Gradient Norm after: 22.360677309965617
Epoch 6585/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7470880508422851
Epoch: 6585, Batch Gradient Norm: 24.67760551725996
Epoch: 6585, Batch Gradient Norm after: 22.341209942063717
Epoch 6586/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7460615992546081
Epoch: 6586, Batch Gradient Norm: 25.037211027819332
Epoch: 6586, Batch Gradient Norm after: 22.360677229629225
Epoch 6587/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7470969557762146
Epoch: 6587, Batch Gradient Norm: 24.68654527867829
Epoch: 6587, Batch Gradient Norm after: 22.36067902567747
Epoch 6588/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7460564732551574
Epoch: 6588, Batch Gradient Norm: 25.010159189159854
Epoch: 6588, Batch Gradient Norm after: 22.360679205290936
Epoch 6589/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7470377087593079
Epoch: 6589, Batch Gradient Norm: 24.67455022510556
Epoch: 6589, Batch Gradient Norm after: 22.349313654540644
Epoch 6590/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.7459430694580078
Epoch: 6590, Batch Gradient Norm: 25.014767653567745
Epoch: 6590, Batch Gradient Norm after: 22.36067767648087
Epoch 6591/10000, Prediction Accuracy = 59.812%, Loss = 0.746891975402832
Epoch: 6591, Batch Gradient Norm: 24.676782616266394
Epoch: 6591, Batch Gradient Norm after: 22.356826047483935
Epoch 6592/10000, Prediction Accuracy = 59.762%, Loss = 0.7458454012870789
Epoch: 6592, Batch Gradient Norm: 25.005550072528685
Epoch: 6592, Batch Gradient Norm after: 22.360679074187626
Epoch 6593/10000, Prediction Accuracy = 59.798%, Loss = 0.7467780232429504
Epoch: 6593, Batch Gradient Norm: 24.6583785486808
Epoch: 6593, Batch Gradient Norm after: 22.346862521430705
Epoch 6594/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.745757806301117
Epoch: 6594, Batch Gradient Norm: 25.01296151367813
Epoch: 6594, Batch Gradient Norm after: 22.36067616855547
Epoch 6595/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7467221975326538
Epoch: 6595, Batch Gradient Norm: 24.669199687681918
Epoch: 6595, Batch Gradient Norm after: 22.3606766632692
Epoch 6596/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7456720471382141
Epoch: 6596, Batch Gradient Norm: 24.99529766985103
Epoch: 6596, Batch Gradient Norm after: 22.360678118177105
Epoch 6597/10000, Prediction Accuracy = 59.798%, Loss = 0.7466081500053405
Epoch: 6597, Batch Gradient Norm: 24.644530664892674
Epoch: 6597, Batch Gradient Norm after: 22.342086911496963
Epoch 6598/10000, Prediction Accuracy = 59.778%, Loss = 0.7455655813217164
Epoch: 6598, Batch Gradient Norm: 25.015192089296523
Epoch: 6598, Batch Gradient Norm after: 22.36067759336296
Epoch 6599/10000, Prediction Accuracy = 59.786%, Loss = 0.7466543197631836
Epoch: 6599, Batch Gradient Norm: 24.650400962430194
Epoch: 6599, Batch Gradient Norm after: 22.360678663256735
Epoch 6600/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7455213308334351
Epoch: 6600, Batch Gradient Norm: 24.990164810343476
Epoch: 6600, Batch Gradient Norm after: 22.36067936809888
Epoch 6601/10000, Prediction Accuracy = 59.766%, Loss = 0.7464856505393982
Epoch: 6601, Batch Gradient Norm: 24.639612262752113
Epoch: 6601, Batch Gradient Norm after: 22.349333344612383
Epoch 6602/10000, Prediction Accuracy = 59.75999999999999%, Loss = 0.7453797698020935
Epoch: 6602, Batch Gradient Norm: 24.997570738699412
Epoch: 6602, Batch Gradient Norm after: 22.360678781061647
Epoch 6603/10000, Prediction Accuracy = 59.826%, Loss = 0.7463877320289611
Epoch: 6603, Batch Gradient Norm: 24.642574973744107
Epoch: 6603, Batch Gradient Norm after: 22.357804154373664
Epoch 6604/10000, Prediction Accuracy = 59.77%, Loss = 0.7453324913978576
Epoch: 6604, Batch Gradient Norm: 24.979835562853562
Epoch: 6604, Batch Gradient Norm after: 22.360676503530748
Epoch 6605/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.746284031867981
Epoch: 6605, Batch Gradient Norm: 24.62193115013784
Epoch: 6605, Batch Gradient Norm after: 22.3447519748245
Epoch 6606/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7451980948448181
Epoch: 6606, Batch Gradient Norm: 24.99545462896289
Epoch: 6606, Batch Gradient Norm after: 22.36067980948835
Epoch 6607/10000, Prediction Accuracy = 59.834%, Loss = 0.7462357401847839
Epoch: 6607, Batch Gradient Norm: 24.632813061423324
Epoch: 6607, Batch Gradient Norm after: 22.360678835763487
Epoch 6608/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.745148491859436
Epoch: 6608, Batch Gradient Norm: 24.973085058860075
Epoch: 6608, Batch Gradient Norm after: 22.36067901336339
Epoch 6609/10000, Prediction Accuracy = 59.79%, Loss = 0.7461715936660767
Epoch: 6609, Batch Gradient Norm: 24.61366819288031
Epoch: 6609, Batch Gradient Norm after: 22.3459761321979
Epoch 6610/10000, Prediction Accuracy = 59.762%, Loss = 0.7450770616531373
Epoch: 6610, Batch Gradient Norm: 24.97898214979714
Epoch: 6610, Batch Gradient Norm after: 22.36067923523422
Epoch 6611/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7461338758468627
Epoch: 6611, Batch Gradient Norm: 24.622559141516682
Epoch: 6611, Batch Gradient Norm after: 22.3606092116116
Epoch 6612/10000, Prediction Accuracy = 59.762%, Loss = 0.7449937224388122
Epoch: 6612, Batch Gradient Norm: 24.962238050948613
Epoch: 6612, Batch Gradient Norm after: 22.360676623562007
Epoch 6613/10000, Prediction Accuracy = 59.838%, Loss = 0.745952033996582
Epoch: 6613, Batch Gradient Norm: 24.598986845195142
Epoch: 6613, Batch Gradient Norm after: 22.342502859158984
Epoch 6614/10000, Prediction Accuracy = 59.774%, Loss = 0.7448546767234803
Epoch: 6614, Batch Gradient Norm: 24.980842547649676
Epoch: 6614, Batch Gradient Norm after: 22.360678565658098
Epoch 6615/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7459283590316772
Epoch: 6615, Batch Gradient Norm: 24.60837409102739
Epoch: 6615, Batch Gradient Norm after: 22.360678501049303
Epoch 6616/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.7448200464248658
Epoch: 6616, Batch Gradient Norm: 24.956033840402004
Epoch: 6616, Batch Gradient Norm after: 22.360679977416208
Epoch 6617/10000, Prediction Accuracy = 59.838%, Loss = 0.7457852005958557
Epoch: 6617, Batch Gradient Norm: 24.587785766380907
Epoch: 6617, Batch Gradient Norm after: 22.342415695305224
Epoch 6618/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7446624398231506
Epoch: 6618, Batch Gradient Norm: 24.976088009425453
Epoch: 6618, Batch Gradient Norm after: 22.360679230789138
Epoch 6619/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7457932472229004
Epoch: 6619, Batch Gradient Norm: 24.598205453767758
Epoch: 6619, Batch Gradient Norm after: 22.36067976913129
Epoch 6620/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.7446666121482849
Epoch: 6620, Batch Gradient Norm: 24.949689868306898
Epoch: 6620, Batch Gradient Norm after: 22.360680609835175
Epoch 6621/10000, Prediction Accuracy = 59.778%, Loss = 0.7457295417785644
Epoch: 6621, Batch Gradient Norm: 24.576015317336317
Epoch: 6621, Batch Gradient Norm after: 22.3462512446292
Epoch 6622/10000, Prediction Accuracy = 59.762%, Loss = 0.7445282459259033
Epoch: 6622, Batch Gradient Norm: 24.96376413023014
Epoch: 6622, Batch Gradient Norm after: 22.360678340839204
Epoch 6623/10000, Prediction Accuracy = 59.806%, Loss = 0.7456040143966675
Epoch: 6623, Batch Gradient Norm: 24.587242217952454
Epoch: 6623, Batch Gradient Norm after: 22.360677987233956
Epoch 6624/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.7444537162780762
Epoch: 6624, Batch Gradient Norm: 24.94524545916612
Epoch: 6624, Batch Gradient Norm after: 22.360679362965318
Epoch 6625/10000, Prediction Accuracy = 59.838%, Loss = 0.7454737305641175
Epoch: 6625, Batch Gradient Norm: 24.564012505275176
Epoch: 6625, Batch Gradient Norm after: 22.34419373086822
Epoch 6626/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.7443474650382995
Epoch: 6626, Batch Gradient Norm: 24.959793731069198
Epoch: 6626, Batch Gradient Norm after: 22.360680353169492
Epoch 6627/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.745433247089386
Epoch: 6627, Batch Gradient Norm: 24.571205516893166
Epoch: 6627, Batch Gradient Norm after: 22.360678753932852
Epoch 6628/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7442698240280151
Epoch: 6628, Batch Gradient Norm: 24.939242900957744
Epoch: 6628, Batch Gradient Norm after: 22.360678117205058
Epoch 6629/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7453204870224
Epoch: 6629, Batch Gradient Norm: 24.559594465575866
Epoch: 6629, Batch Gradient Norm after: 22.348131735936782
Epoch 6630/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.744189465045929
Epoch: 6630, Batch Gradient Norm: 24.945479047707572
Epoch: 6630, Batch Gradient Norm after: 22.360678434956725
Epoch 6631/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7453308582305909
Epoch: 6631, Batch Gradient Norm: 24.56241483345103
Epoch: 6631, Batch Gradient Norm after: 22.35901696794226
Epoch 6632/10000, Prediction Accuracy = 59.772000000000006%, Loss = 0.7441473603248596
Epoch: 6632, Batch Gradient Norm: 24.924941240491638
Epoch: 6632, Batch Gradient Norm after: 22.360678163322024
Epoch 6633/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7451819062232972
Epoch: 6633, Batch Gradient Norm: 24.543543138450623
Epoch: 6633, Batch Gradient Norm after: 22.344475272728094
Epoch 6634/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7439830064773559
Epoch: 6634, Batch Gradient Norm: 24.94285035299684
Epoch: 6634, Batch Gradient Norm after: 22.360678464742154
Epoch 6635/10000, Prediction Accuracy = 59.83%, Loss = 0.7451002359390259
Epoch: 6635, Batch Gradient Norm: 24.55823352249871
Epoch: 6635, Batch Gradient Norm after: 22.36068151626462
Epoch 6636/10000, Prediction Accuracy = 59.778%, Loss = 0.743954336643219
Epoch: 6636, Batch Gradient Norm: 24.919657038725077
Epoch: 6636, Batch Gradient Norm after: 22.36067955434541
Epoch 6637/10000, Prediction Accuracy = 59.838%, Loss = 0.7449886322021484
Epoch: 6637, Batch Gradient Norm: 24.53639252594937
Epoch: 6637, Batch Gradient Norm after: 22.343898117102732
Epoch 6638/10000, Prediction Accuracy = 59.788%, Loss = 0.7438192486763
Epoch: 6638, Batch Gradient Norm: 24.937312459178127
Epoch: 6638, Batch Gradient Norm after: 22.360677942091186
Epoch 6639/10000, Prediction Accuracy = 59.834%, Loss = 0.7449371814727783
Epoch: 6639, Batch Gradient Norm: 24.54432115048466
Epoch: 6639, Batch Gradient Norm after: 22.360679224881622
Epoch 6640/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7437655806541443
Epoch: 6640, Batch Gradient Norm: 24.913215207338677
Epoch: 6640, Batch Gradient Norm after: 22.36067652324944
Epoch 6641/10000, Prediction Accuracy = 59.778%, Loss = 0.7448806881904602
Epoch: 6641, Batch Gradient Norm: 24.524708330177212
Epoch: 6641, Batch Gradient Norm after: 22.344188101471445
Epoch 6642/10000, Prediction Accuracy = 59.778%, Loss = 0.7436953186988831
Epoch: 6642, Batch Gradient Norm: 24.92549435563374
Epoch: 6642, Batch Gradient Norm after: 22.36067994417771
Epoch 6643/10000, Prediction Accuracy = 59.798%, Loss = 0.7448494791984558
Epoch: 6643, Batch Gradient Norm: 24.53469389179234
Epoch: 6643, Batch Gradient Norm after: 22.360677640632588
Epoch 6644/10000, Prediction Accuracy = 59.778%, Loss = 0.7436061859130859
Epoch: 6644, Batch Gradient Norm: 24.8996306665764
Epoch: 6644, Batch Gradient Norm after: 22.360679131936966
Epoch 6645/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7446486473083496
Epoch: 6645, Batch Gradient Norm: 24.507958618993936
Epoch: 6645, Batch Gradient Norm after: 22.337605987479066
Epoch 6646/10000, Prediction Accuracy = 59.78000000000001%, Loss = 0.7434608221054078
Epoch: 6646, Batch Gradient Norm: 24.926063002284806
Epoch: 6646, Batch Gradient Norm after: 22.360677861701937
Epoch 6647/10000, Prediction Accuracy = 59.836%, Loss = 0.7446513652801514
Epoch: 6647, Batch Gradient Norm: 24.516725405260946
Epoch: 6647, Batch Gradient Norm after: 22.360679669801147
Epoch 6648/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7434256196022033
Epoch: 6648, Batch Gradient Norm: 24.90017411529884
Epoch: 6648, Batch Gradient Norm after: 22.36068036996809
Epoch 6649/10000, Prediction Accuracy = 59.836%, Loss = 0.7444972276687623
Epoch: 6649, Batch Gradient Norm: 24.503816355344917
Epoch: 6649, Batch Gradient Norm after: 22.34535933266412
Epoch 6650/10000, Prediction Accuracy = 59.778%, Loss = 0.7432933568954467
Epoch: 6650, Batch Gradient Norm: 24.91086765865528
Epoch: 6650, Batch Gradient Norm after: 22.360677728315444
Epoch 6651/10000, Prediction Accuracy = 59.802%, Loss = 0.7444836974143982
Epoch: 6651, Batch Gradient Norm: 24.512192776460463
Epoch: 6651, Batch Gradient Norm after: 22.356806710085195
Epoch 6652/10000, Prediction Accuracy = 59.786%, Loss = 0.7432894349098206
Epoch: 6652, Batch Gradient Norm: 24.88839651822349
Epoch: 6652, Batch Gradient Norm after: 22.36067785374874
Epoch 6653/10000, Prediction Accuracy = 59.798%, Loss = 0.7444192171096802
Epoch: 6653, Batch Gradient Norm: 24.49100443691312
Epoch: 6653, Batch Gradient Norm after: 22.344195972578966
Epoch 6654/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7431529402732849
Epoch: 6654, Batch Gradient Norm: 24.90385487416181
Epoch: 6654, Batch Gradient Norm after: 22.36067836254988
Epoch 6655/10000, Prediction Accuracy = 59.8%, Loss = 0.7443152070045471
Epoch: 6655, Batch Gradient Norm: 24.503146119438796
Epoch: 6655, Batch Gradient Norm after: 22.359497894529063
Epoch 6656/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.7430771350860595
Epoch: 6656, Batch Gradient Norm: 24.880580081796754
Epoch: 6656, Batch Gradient Norm after: 22.36067851034298
Epoch 6657/10000, Prediction Accuracy = 59.842000000000006%, Loss = 0.7441753506660461
Epoch: 6657, Batch Gradient Norm: 24.47916620178409
Epoch: 6657, Batch Gradient Norm after: 22.342325589227297
Epoch 6658/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.7429735898971558
Epoch: 6658, Batch Gradient Norm: 24.90056083201569
Epoch: 6658, Batch Gradient Norm after: 22.360678352410684
Epoch 6659/10000, Prediction Accuracy = 59.842000000000006%, Loss = 0.744150698184967
Epoch: 6659, Batch Gradient Norm: 24.489134688592987
Epoch: 6659, Batch Gradient Norm after: 22.360676552279394
Epoch 6660/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7428950428962707
Epoch: 6660, Batch Gradient Norm: 24.879293310614834
Epoch: 6660, Batch Gradient Norm after: 22.36068007766286
Epoch 6661/10000, Prediction Accuracy = 59.818%, Loss = 0.7440228343009949
Epoch: 6661, Batch Gradient Norm: 24.47561361204972
Epoch: 6661, Batch Gradient Norm after: 22.34601532322681
Epoch 6662/10000, Prediction Accuracy = 59.798%, Loss = 0.7428070425987243
Epoch: 6662, Batch Gradient Norm: 24.8897283735974
Epoch: 6662, Batch Gradient Norm after: 22.360677518907536
Epoch 6663/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7440525531768799
Epoch: 6663, Batch Gradient Norm: 24.481453451570548
Epoch: 6663, Batch Gradient Norm after: 22.360679128431517
Epoch 6664/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.742792546749115
Epoch: 6664, Batch Gradient Norm: 24.864978729280665
Epoch: 6664, Batch Gradient Norm after: 22.360679325720433
Epoch 6665/10000, Prediction Accuracy = 59.798%, Loss = 0.7439041852951049
Epoch: 6665, Batch Gradient Norm: 24.46076585347836
Epoch: 6665, Batch Gradient Norm after: 22.344893036483864
Epoch 6666/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.7426120519638062
Epoch: 6666, Batch Gradient Norm: 24.88558060393072
Epoch: 6666, Batch Gradient Norm after: 22.360678347678448
Epoch 6667/10000, Prediction Accuracy = 59.836%, Loss = 0.7438259482383728
Epoch: 6667, Batch Gradient Norm: 24.469187578831686
Epoch: 6667, Batch Gradient Norm after: 22.36067891365632
Epoch 6668/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7425752878189087
Epoch: 6668, Batch Gradient Norm: 24.862722132749102
Epoch: 6668, Batch Gradient Norm after: 22.360676966304876
Epoch 6669/10000, Prediction Accuracy = 59.83%, Loss = 0.7437263011932373
Epoch: 6669, Batch Gradient Norm: 24.450460049574453
Epoch: 6669, Batch Gradient Norm after: 22.346798792228547
Epoch 6670/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7424575924873352
Epoch: 6670, Batch Gradient Norm: 24.872295204881592
Epoch: 6670, Batch Gradient Norm after: 22.36067644167009
Epoch 6671/10000, Prediction Accuracy = 59.852%, Loss = 0.7436528086662293
Epoch: 6671, Batch Gradient Norm: 24.463767226614067
Epoch: 6671, Batch Gradient Norm after: 22.360677699183025
Epoch 6672/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7423973917961121
Epoch: 6672, Batch Gradient Norm: 24.852342872946675
Epoch: 6672, Batch Gradient Norm after: 22.36067738183755
Epoch 6673/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.7435728430747985
Epoch: 6673, Batch Gradient Norm: 24.44153769457692
Epoch: 6673, Batch Gradient Norm after: 22.343246144416057
Epoch 6674/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7423161864280701
Epoch: 6674, Batch Gradient Norm: 24.86579549596288
Epoch: 6674, Batch Gradient Norm after: 22.36067920195949
Epoch 6675/10000, Prediction Accuracy = 59.81%, Loss = 0.7435746192932129
Epoch: 6675, Batch Gradient Norm: 24.447637822104962
Epoch: 6675, Batch Gradient Norm after: 22.360679005898245
Epoch 6676/10000, Prediction Accuracy = 59.794%, Loss = 0.7422375917434693
Epoch: 6676, Batch Gradient Norm: 24.843999795648674
Epoch: 6676, Batch Gradient Norm after: 22.360677210812735
Epoch 6677/10000, Prediction Accuracy = 59.834%, Loss = 0.7433915495872497
Epoch: 6677, Batch Gradient Norm: 24.430200948824517
Epoch: 6677, Batch Gradient Norm after: 22.343994723033244
Epoch 6678/10000, Prediction Accuracy = 59.778%, Loss = 0.7421003341674804
Epoch: 6678, Batch Gradient Norm: 24.864136841462315
Epoch: 6678, Batch Gradient Norm after: 22.360676384872225
Epoch 6679/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7433543682098389
Epoch: 6679, Batch Gradient Norm: 24.437415469377093
Epoch: 6679, Batch Gradient Norm after: 22.360678885487857
Epoch 6680/10000, Prediction Accuracy = 59.778000000000006%, Loss = 0.7420728087425232
Epoch: 6680, Batch Gradient Norm: 24.83762923042425
Epoch: 6680, Batch Gradient Norm after: 22.360679525574184
Epoch 6681/10000, Prediction Accuracy = 59.858000000000004%, Loss = 0.7432204365730286
Epoch: 6681, Batch Gradient Norm: 24.41753123451439
Epoch: 6681, Batch Gradient Norm after: 22.343566690637527
Epoch 6682/10000, Prediction Accuracy = 59.78399999999999%, Loss = 0.7419182181358337
Epoch: 6682, Batch Gradient Norm: 24.858550787831543
Epoch: 6682, Batch Gradient Norm after: 22.360678694638032
Epoch 6683/10000, Prediction Accuracy = 59.826%, Loss = 0.7432053804397583
Epoch: 6683, Batch Gradient Norm: 24.429340244440716
Epoch: 6683, Batch Gradient Norm after: 22.360680242350252
Epoch 6684/10000, Prediction Accuracy = 59.822%, Loss = 0.7419156789779663
Epoch: 6684, Batch Gradient Norm: 24.83379357912758
Epoch: 6684, Batch Gradient Norm after: 22.360677993453702
Epoch 6685/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7431463718414306
Epoch: 6685, Batch Gradient Norm: 24.408491359205282
Epoch: 6685, Batch Gradient Norm after: 22.346797654277463
Epoch 6686/10000, Prediction Accuracy = 59.788%, Loss = 0.7418036699295044
Epoch: 6686, Batch Gradient Norm: 24.843916122994788
Epoch: 6686, Batch Gradient Norm after: 22.360678740598537
Epoch 6687/10000, Prediction Accuracy = 59.824%, Loss = 0.743044090270996
Epoch: 6687, Batch Gradient Norm: 24.419253219114964
Epoch: 6687, Batch Gradient Norm after: 22.35985825278164
Epoch 6688/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7417135000228882
Epoch: 6688, Batch Gradient Norm: 24.825410431179975
Epoch: 6688, Batch Gradient Norm after: 22.360679096429106
Epoch 6689/10000, Prediction Accuracy = 59.866%, Loss = 0.7428938150405884
Epoch: 6689, Batch Gradient Norm: 24.395188948345865
Epoch: 6689, Batch Gradient Norm after: 22.342708163718722
Epoch 6690/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.7416060328483581
Epoch: 6690, Batch Gradient Norm: 24.845625388631074
Epoch: 6690, Batch Gradient Norm after: 22.360679707761335
Epoch 6691/10000, Prediction Accuracy = 59.862%, Loss = 0.7428764224052429
Epoch: 6691, Batch Gradient Norm: 24.402057786597545
Epoch: 6691, Batch Gradient Norm after: 22.360679415325755
Epoch 6692/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7415368437767029
Epoch: 6692, Batch Gradient Norm: 24.822657668001593
Epoch: 6692, Batch Gradient Norm after: 22.360677760575875
Epoch 6693/10000, Prediction Accuracy = 59.85999999999999%, Loss = 0.7427345871925354
Epoch: 6693, Batch Gradient Norm: 24.38855216471498
Epoch: 6693, Batch Gradient Norm after: 22.34551103798493
Epoch 6694/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7414370894432067
Epoch: 6694, Batch Gradient Norm: 24.835091377561287
Epoch: 6694, Batch Gradient Norm after: 22.360679532539137
Epoch 6695/10000, Prediction Accuracy = 59.786%, Loss = 0.7427618503570557
Epoch: 6695, Batch Gradient Norm: 24.393850576715735
Epoch: 6695, Batch Gradient Norm after: 22.36067857457067
Epoch 6696/10000, Prediction Accuracy = 59.824%, Loss = 0.7414322376251221
Epoch: 6696, Batch Gradient Norm: 24.810386626332203
Epoch: 6696, Batch Gradient Norm after: 22.360677197571764
Epoch 6697/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.7426340222358704
Epoch: 6697, Batch Gradient Norm: 24.367486412997586
Epoch: 6697, Batch Gradient Norm after: 22.342412362808656
Epoch 6698/10000, Prediction Accuracy = 59.83%, Loss = 0.7412512898445129
Epoch: 6698, Batch Gradient Norm: 24.83306151474162
Epoch: 6698, Batch Gradient Norm after: 22.36067846443255
Epoch 6699/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.7425546169281005
Epoch: 6699, Batch Gradient Norm: 24.380103522414338
Epoch: 6699, Batch Gradient Norm after: 22.36067935874526
Epoch 6700/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.7412096619606018
Epoch: 6700, Batch Gradient Norm: 24.81125705534647
Epoch: 6700, Batch Gradient Norm after: 22.360676341279554
Epoch 6701/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.7424416542053223
Epoch: 6701, Batch Gradient Norm: 24.362493751677444
Epoch: 6701, Batch Gradient Norm after: 22.347691712091006
Epoch 6702/10000, Prediction Accuracy = 59.767999999999994%, Loss = 0.7411102771759033
Epoch: 6702, Batch Gradient Norm: 24.81708591500936
Epoch: 6702, Batch Gradient Norm after: 22.36067976898021
Epoch 6703/10000, Prediction Accuracy = 59.886%, Loss = 0.7423704624176025
Epoch: 6703, Batch Gradient Norm: 24.370957671574942
Epoch: 6703, Batch Gradient Norm after: 22.358251777524284
Epoch 6704/10000, Prediction Accuracy = 59.786%, Loss = 0.7410324811935425
Epoch: 6704, Batch Gradient Norm: 24.8029937211413
Epoch: 6704, Batch Gradient Norm after: 22.36067969652511
Epoch 6705/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.7422990560531616
Epoch: 6705, Batch Gradient Norm: 24.353062894822845
Epoch: 6705, Batch Gradient Norm after: 22.34684306079963
Epoch 6706/10000, Prediction Accuracy = 59.824%, Loss = 0.7409747123718262
Epoch: 6706, Batch Gradient Norm: 24.808052353856425
Epoch: 6706, Batch Gradient Norm after: 22.360678780989264
Epoch 6707/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7422971367835999
Epoch: 6707, Batch Gradient Norm: 24.35800414854148
Epoch: 6707, Batch Gradient Norm after: 22.35895044670922
Epoch 6708/10000, Prediction Accuracy = 59.812%, Loss = 0.7408978939056396
Epoch: 6708, Batch Gradient Norm: 24.791664456399978
Epoch: 6708, Batch Gradient Norm after: 22.36067689989041
Epoch 6709/10000, Prediction Accuracy = 59.81999999999999%, Loss = 0.7421254754066468
Epoch: 6709, Batch Gradient Norm: 24.33969778654991
Epoch: 6709, Batch Gradient Norm after: 22.345092114023586
Epoch 6710/10000, Prediction Accuracy = 59.80200000000001%, Loss = 0.7407417535781861
Epoch: 6710, Batch Gradient Norm: 24.807396573349482
Epoch: 6710, Batch Gradient Norm after: 22.360678765909082
Epoch 6711/10000, Prediction Accuracy = 59.878%, Loss = 0.7420713186264039
Epoch: 6711, Batch Gradient Norm: 24.348985910534637
Epoch: 6711, Batch Gradient Norm after: 22.36067999089653
Epoch 6712/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7407168745994568
Epoch: 6712, Batch Gradient Norm: 24.784007569890097
Epoch: 6712, Batch Gradient Norm after: 22.360679820881177
Epoch 6713/10000, Prediction Accuracy = 59.862%, Loss = 0.741954255104065
Epoch: 6713, Batch Gradient Norm: 24.324697485087107
Epoch: 6713, Batch Gradient Norm after: 22.341905111338637
Epoch 6714/10000, Prediction Accuracy = 59.786%, Loss = 0.7405579686164856
Epoch: 6714, Batch Gradient Norm: 24.806926699077035
Epoch: 6714, Batch Gradient Norm after: 22.360678832997937
Epoch 6715/10000, Prediction Accuracy = 59.836%, Loss = 0.7419247627258301
Epoch: 6715, Batch Gradient Norm: 24.336617156508844
Epoch: 6715, Batch Gradient Norm after: 22.360677436194624
Epoch 6716/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.7405437231063843
Epoch: 6716, Batch Gradient Norm: 24.780138529344523
Epoch: 6716, Batch Gradient Norm after: 22.360676852505428
Epoch 6717/10000, Prediction Accuracy = 59.838%, Loss = 0.7418628573417664
Epoch: 6717, Batch Gradient Norm: 24.317237190336094
Epoch: 6717, Batch Gradient Norm after: 22.34397055077423
Epoch 6718/10000, Prediction Accuracy = 59.85%, Loss = 0.7404534697532654
Epoch: 6718, Batch Gradient Norm: 24.79464167728642
Epoch: 6718, Batch Gradient Norm after: 22.36067793312394
Epoch 6719/10000, Prediction Accuracy = 59.822%, Loss = 0.7418002247810364
Epoch: 6719, Batch Gradient Norm: 24.329481639630455
Epoch: 6719, Batch Gradient Norm after: 22.360677819484508
Epoch 6720/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.7403636932373047
Epoch: 6720, Batch Gradient Norm: 24.771293526148757
Epoch: 6720, Batch Gradient Norm after: 22.36067947448984
Epoch 6721/10000, Prediction Accuracy = 59.872%, Loss = 0.741630470752716
Epoch: 6721, Batch Gradient Norm: 24.30751109045824
Epoch: 6721, Batch Gradient Norm after: 22.342868435049798
Epoch 6722/10000, Prediction Accuracy = 59.784000000000006%, Loss = 0.7402474641799927
Epoch: 6722, Batch Gradient Norm: 24.78893760165462
Epoch: 6722, Batch Gradient Norm after: 22.360679416779796
Epoch 6723/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.74161376953125
Epoch: 6723, Batch Gradient Norm: 24.3177134954133
Epoch: 6723, Batch Gradient Norm after: 22.360677882238704
Epoch 6724/10000, Prediction Accuracy = 59.798%, Loss = 0.740204393863678
Epoch: 6724, Batch Gradient Norm: 24.76213232028428
Epoch: 6724, Batch Gradient Norm after: 22.360677098110685
Epoch 6725/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7414624094963074
Epoch: 6725, Batch Gradient Norm: 24.290480989502136
Epoch: 6725, Batch Gradient Norm after: 22.33839363467051
Epoch 6726/10000, Prediction Accuracy = 59.8%, Loss = 0.7400505065917968
Epoch: 6726, Batch Gradient Norm: 24.788920472559955
Epoch: 6726, Batch Gradient Norm after: 22.360677049066467
Epoch 6727/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.7415058970451355
Epoch: 6727, Batch Gradient Norm: 24.302067122582383
Epoch: 6727, Batch Gradient Norm after: 22.36067753668858
Epoch 6728/10000, Prediction Accuracy = 59.86%, Loss = 0.7400697350502015
Epoch: 6728, Batch Gradient Norm: 24.76060459495319
Epoch: 6728, Batch Gradient Norm after: 22.36067625805308
Epoch 6729/10000, Prediction Accuracy = 59.830000000000005%, Loss = 0.7413968920707703
Epoch: 6729, Batch Gradient Norm: 24.289124238391945
Epoch: 6729, Batch Gradient Norm after: 22.34737585547837
Epoch 6730/10000, Prediction Accuracy = 59.822%, Loss = 0.7399211287498474
Epoch: 6730, Batch Gradient Norm: 24.77175273167689
Epoch: 6730, Batch Gradient Norm after: 22.360679428697985
Epoch 6731/10000, Prediction Accuracy = 59.85%, Loss = 0.741287636756897
Epoch: 6731, Batch Gradient Norm: 24.29554221073721
Epoch: 6731, Batch Gradient Norm after: 22.359056765349127
Epoch 6732/10000, Prediction Accuracy = 59.790000000000006%, Loss = 0.7398526072502136
Epoch: 6732, Batch Gradient Norm: 24.75540178792764
Epoch: 6732, Batch Gradient Norm after: 22.360676457814648
Epoch 6733/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7411834597587585
Epoch: 6733, Batch Gradient Norm: 24.273800985278225
Epoch: 6733, Batch Gradient Norm after: 22.345174840255495
Epoch 6734/10000, Prediction Accuracy = 59.79%, Loss = 0.7397524952888489
Epoch: 6734, Batch Gradient Norm: 24.765478474177797
Epoch: 6734, Batch Gradient Norm after: 22.360681025123153
Epoch 6735/10000, Prediction Accuracy = 59.88000000000001%, Loss = 0.7411323308944702
Epoch: 6735, Batch Gradient Norm: 24.285318900879247
Epoch: 6735, Batch Gradient Norm after: 22.360582375701224
Epoch 6736/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.7396814942359924
Epoch: 6736, Batch Gradient Norm: 24.743519707216592
Epoch: 6736, Batch Gradient Norm after: 22.3606770392816
Epoch 6737/10000, Prediction Accuracy = 59.834%, Loss = 0.7410220384597779
Epoch: 6737, Batch Gradient Norm: 24.255751442293622
Epoch: 6737, Batch Gradient Norm after: 22.338778281885162
Epoch 6738/10000, Prediction Accuracy = 59.834%, Loss = 0.7395876765251159
Epoch: 6738, Batch Gradient Norm: 24.768194720835012
Epoch: 6738, Batch Gradient Norm after: 22.36067654809128
Epoch 6739/10000, Prediction Accuracy = 59.83%, Loss = 0.7410701036453247
Epoch: 6739, Batch Gradient Norm: 24.262161564799868
Epoch: 6739, Batch Gradient Norm after: 22.360679582982375
Epoch 6740/10000, Prediction Accuracy = 59.85%, Loss = 0.7395319819450379
Epoch: 6740, Batch Gradient Norm: 24.743894627653972
Epoch: 6740, Batch Gradient Norm after: 22.36067878588151
Epoch 6741/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.7408934712409974
Epoch: 6741, Batch Gradient Norm: 24.25325694838922
Epoch: 6741, Batch Gradient Norm after: 22.34851516169436
Epoch 6742/10000, Prediction Accuracy = 59.802%, Loss = 0.7393986821174622
Epoch: 6742, Batch Gradient Norm: 24.751044336730374
Epoch: 6742, Batch Gradient Norm after: 22.360680628716423
Epoch 6743/10000, Prediction Accuracy = 59.898%, Loss = 0.7408074021339417
Epoch: 6743, Batch Gradient Norm: 24.25786532247526
Epoch: 6743, Batch Gradient Norm after: 22.357406175711418
Epoch 6744/10000, Prediction Accuracy = 59.786%, Loss = 0.7393644571304321
Epoch: 6744, Batch Gradient Norm: 24.733531011143217
Epoch: 6744, Batch Gradient Norm after: 22.360679709509427
Epoch 6745/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7407066226005554
Epoch: 6745, Batch Gradient Norm: 24.2353871710885
Epoch: 6745, Batch Gradient Norm after: 22.342816515212036
Epoch 6746/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7392198204994201
Epoch: 6746, Batch Gradient Norm: 24.75093888692912
Epoch: 6746, Batch Gradient Norm after: 22.360679101646365
Epoch 6747/10000, Prediction Accuracy = 59.876%, Loss = 0.7406633257865906
Epoch: 6747, Batch Gradient Norm: 24.25069991803105
Epoch: 6747, Batch Gradient Norm after: 22.360678656001163
Epoch 6748/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.7391998410224915
Epoch: 6748, Batch Gradient Norm: 24.72392969970526
Epoch: 6748, Batch Gradient Norm after: 22.36067665825007
Epoch 6749/10000, Prediction Accuracy = 59.838%, Loss = 0.7405934929847717
Epoch: 6749, Batch Gradient Norm: 24.219793722336213
Epoch: 6749, Batch Gradient Norm after: 22.337593921030273
Epoch 6750/10000, Prediction Accuracy = 59.85600000000001%, Loss = 0.7391027212142944
Epoch: 6750, Batch Gradient Norm: 24.746503898730577
Epoch: 6750, Batch Gradient Norm after: 22.3606784945843
Epoch 6751/10000, Prediction Accuracy = 59.84599999999999%, Loss = 0.7405777096748352
Epoch: 6751, Batch Gradient Norm: 24.22957170432838
Epoch: 6751, Batch Gradient Norm after: 22.360677093396674
Epoch 6752/10000, Prediction Accuracy = 59.824%, Loss = 0.7390109419822692
Epoch: 6752, Batch Gradient Norm: 24.722088899064502
Epoch: 6752, Batch Gradient Norm after: 22.360676788058377
Epoch 6753/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.7403958082199097
Epoch: 6753, Batch Gradient Norm: 24.217135864807897
Epoch: 6753, Batch Gradient Norm after: 22.345120299414422
Epoch 6754/10000, Prediction Accuracy = 59.786%, Loss = 0.7389074921607971
Epoch: 6754, Batch Gradient Norm: 24.732954392404622
Epoch: 6754, Batch Gradient Norm after: 22.36067903520236
Epoch 6755/10000, Prediction Accuracy = 59.878%, Loss = 0.7403564214706421
Epoch: 6755, Batch Gradient Norm: 24.223628161868024
Epoch: 6755, Batch Gradient Norm after: 22.357278093371296
Epoch 6756/10000, Prediction Accuracy = 59.8%, Loss = 0.7388612151145935
Epoch: 6756, Batch Gradient Norm: 24.709187922527565
Epoch: 6756, Batch Gradient Norm after: 22.360678646626123
Epoch 6757/10000, Prediction Accuracy = 59.898%, Loss = 0.7402206540107727
Epoch: 6757, Batch Gradient Norm: 24.19809844901534
Epoch: 6757, Batch Gradient Norm after: 22.33692767038238
Epoch 6758/10000, Prediction Accuracy = 59.812%, Loss = 0.7387048602104187
Epoch: 6758, Batch Gradient Norm: 24.738659734933112
Epoch: 6758, Batch Gradient Norm after: 22.360677225151402
Epoch 6759/10000, Prediction Accuracy = 59.826%, Loss = 0.7402394652366638
Epoch: 6759, Batch Gradient Norm: 24.213011332793624
Epoch: 6759, Batch Gradient Norm after: 22.360679433164286
Epoch 6760/10000, Prediction Accuracy = 59.852%, Loss = 0.7387293696403503
Epoch: 6760, Batch Gradient Norm: 24.708173719062138
Epoch: 6760, Batch Gradient Norm after: 22.360679755134566
Epoch 6761/10000, Prediction Accuracy = 59.838%, Loss = 0.7401578783988952
Epoch: 6761, Batch Gradient Norm: 24.19514979171201
Epoch: 6761, Batch Gradient Norm after: 22.34435419350482
Epoch 6762/10000, Prediction Accuracy = 59.834%, Loss = 0.7385941743850708
Epoch: 6762, Batch Gradient Norm: 24.719239351377237
Epoch: 6762, Batch Gradient Norm after: 22.360678622229905
Epoch 6763/10000, Prediction Accuracy = 59.826%, Loss = 0.7400422811508178
Epoch: 6763, Batch Gradient Norm: 24.205371686987142
Epoch: 6763, Batch Gradient Norm after: 22.358271168581773
Epoch 6764/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.7385202646255493
Epoch: 6764, Batch Gradient Norm: 24.69906051360068
Epoch: 6764, Batch Gradient Norm after: 22.360675979484004
Epoch 6765/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7399181723594666
Epoch: 6765, Batch Gradient Norm: 24.17785545947875
Epoch: 6765, Batch Gradient Norm after: 22.33875085783448
Epoch 6766/10000, Prediction Accuracy = 59.8%, Loss = 0.738406240940094
Epoch: 6766, Batch Gradient Norm: 24.721828823481324
Epoch: 6766, Batch Gradient Norm after: 22.36068060453068
Epoch 6767/10000, Prediction Accuracy = 59.876%, Loss = 0.7398961186408997
Epoch: 6767, Batch Gradient Norm: 24.192277631581888
Epoch: 6767, Batch Gradient Norm after: 22.360677655242405
Epoch 6768/10000, Prediction Accuracy = 59.788%, Loss = 0.7383413314819336
Epoch: 6768, Batch Gradient Norm: 24.698322734416994
Epoch: 6768, Batch Gradient Norm after: 22.360679157352777
Epoch 6769/10000, Prediction Accuracy = 59.864%, Loss = 0.7397614598274231
Epoch: 6769, Batch Gradient Norm: 24.174852707144705
Epoch: 6769, Batch Gradient Norm after: 22.34255046978578
Epoch 6770/10000, Prediction Accuracy = 59.826%, Loss = 0.7382581472396851
Epoch: 6770, Batch Gradient Norm: 24.71252892369122
Epoch: 6770, Batch Gradient Norm after: 22.360676890440544
Epoch 6771/10000, Prediction Accuracy = 59.84799999999999%, Loss = 0.7398030519485473
Epoch: 6771, Batch Gradient Norm: 24.184818615229712
Epoch: 6771, Batch Gradient Norm after: 22.360681288797085
Epoch 6772/10000, Prediction Accuracy = 59.842000000000006%, Loss = 0.7382373809814453
Epoch: 6772, Batch Gradient Norm: 24.68330058253106
Epoch: 6772, Batch Gradient Norm after: 22.360679525739037
Epoch 6773/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.739628791809082
Epoch: 6773, Batch Gradient Norm: 24.158501203672678
Epoch: 6773, Batch Gradient Norm after: 22.338319545402232
Epoch 6774/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.7380517363548279
Epoch: 6774, Batch Gradient Norm: 24.708522333229684
Epoch: 6774, Batch Gradient Norm after: 22.36067895535787
Epoch 6775/10000, Prediction Accuracy = 59.884%, Loss = 0.7395853161811828
Epoch: 6775, Batch Gradient Norm: 24.174171356357103
Epoch: 6775, Batch Gradient Norm after: 22.36067878337341
Epoch 6776/10000, Prediction Accuracy = 59.818000000000005%, Loss = 0.7380342483520508
Epoch: 6776, Batch Gradient Norm: 24.677951794451534
Epoch: 6776, Batch Gradient Norm after: 22.360676946310395
Epoch 6777/10000, Prediction Accuracy = 59.89999999999999%, Loss = 0.7394498944282532
Epoch: 6777, Batch Gradient Norm: 24.142946403362426
Epoch: 6777, Batch Gradient Norm after: 22.335215496713396
Epoch 6778/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.7378774523735047
Epoch: 6778, Batch Gradient Norm: 24.707670258728694
Epoch: 6778, Batch Gradient Norm after: 22.360677538208513
Epoch 6779/10000, Prediction Accuracy = 59.88399999999999%, Loss = 0.7394352555274963
Epoch: 6779, Batch Gradient Norm: 24.157963040152687
Epoch: 6779, Batch Gradient Norm after: 22.360677242755557
Epoch 6780/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.7378442406654357
Epoch: 6780, Batch Gradient Norm: 24.680738445594457
Epoch: 6780, Batch Gradient Norm after: 22.360679717407006
Epoch 6781/10000, Prediction Accuracy = 59.854%, Loss = 0.7393554210662842
Epoch: 6781, Batch Gradient Norm: 24.145145895755675
Epoch: 6781, Batch Gradient Norm after: 22.34537075970285
Epoch 6782/10000, Prediction Accuracy = 59.866%, Loss = 0.737806522846222
Epoch: 6782, Batch Gradient Norm: 24.68481837313987
Epoch: 6782, Batch Gradient Norm after: 22.360677248152264
Epoch 6783/10000, Prediction Accuracy = 59.855999999999995%, Loss = 0.7393114566802979
Epoch: 6783, Batch Gradient Norm: 24.14828206792201
Epoch: 6783, Batch Gradient Norm after: 22.355986079826693
Epoch 6784/10000, Prediction Accuracy = 59.836%, Loss = 0.737700092792511
Epoch: 6784, Batch Gradient Norm: 24.668210920496566
Epoch: 6784, Batch Gradient Norm after: 22.36067879359624
Epoch 6785/10000, Prediction Accuracy = 59.870000000000005%, Loss = 0.7391429781913758
Epoch: 6785, Batch Gradient Norm: 24.13162626579965
Epoch: 6785, Batch Gradient Norm after: 22.343236897381864
Epoch 6786/10000, Prediction Accuracy = 59.814%, Loss = 0.7375744938850403
Epoch: 6786, Batch Gradient Norm: 24.682816655520234
Epoch: 6786, Batch Gradient Norm after: 22.36067900875688
Epoch 6787/10000, Prediction Accuracy = 59.888%, Loss = 0.7391143798828125
Epoch: 6787, Batch Gradient Norm: 24.140415083005934
Epoch: 6787, Batch Gradient Norm after: 22.357598398567585
Epoch 6788/10000, Prediction Accuracy = 59.81999999999999%, Loss = 0.7375464677810669
Epoch: 6788, Batch Gradient Norm: 24.656854152484037
Epoch: 6788, Batch Gradient Norm after: 22.360679543987874
Epoch 6789/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7389728426933289
Epoch: 6789, Batch Gradient Norm: 24.1179286178893
Epoch: 6789, Batch Gradient Norm after: 22.338592405143217
Epoch 6790/10000, Prediction Accuracy = 59.818%, Loss = 0.737384843826294
Epoch: 6790, Batch Gradient Norm: 24.680804450885113
Epoch: 6790, Batch Gradient Norm after: 22.36067877953522
Epoch 6791/10000, Prediction Accuracy = 59.843999999999994%, Loss = 0.7389747738838196
Epoch: 6791, Batch Gradient Norm: 24.133465925394308
Epoch: 6791, Batch Gradient Norm after: 22.359608431188818
Epoch 6792/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.737416672706604
Epoch: 6792, Batch Gradient Norm: 24.651298259335462
Epoch: 6792, Batch Gradient Norm after: 22.36067909229454
Epoch 6793/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7389025568962098
Epoch: 6793, Batch Gradient Norm: 24.10169351679256
Epoch: 6793, Batch Gradient Norm after: 22.336481444271765
Epoch 6794/10000, Prediction Accuracy = 59.838%, Loss = 0.7372547149658203
Epoch: 6794, Batch Gradient Norm: 24.677466128247925
Epoch: 6794, Batch Gradient Norm after: 22.360678150375314
Epoch 6795/10000, Prediction Accuracy = 59.862%, Loss = 0.7388356924057007
Epoch: 6795, Batch Gradient Norm: 24.11801859292577
Epoch: 6795, Batch Gradient Norm after: 22.360678879478513
Epoch 6796/10000, Prediction Accuracy = 59.80800000000001%, Loss = 0.7371862769126892
Epoch: 6796, Batch Gradient Norm: 24.649461147330733
Epoch: 6796, Batch Gradient Norm after: 22.36067740083304
Epoch 6797/10000, Prediction Accuracy = 59.884%, Loss = 0.7386712431907654
Epoch: 6797, Batch Gradient Norm: 24.096229046060426
Epoch: 6797, Batch Gradient Norm after: 22.338151319770787
Epoch 6798/10000, Prediction Accuracy = 59.834%, Loss = 0.7370808959007263
Epoch: 6798, Batch Gradient Norm: 24.66720828839393
Epoch: 6798, Batch Gradient Norm after: 22.360678440184504
Epoch 6799/10000, Prediction Accuracy = 59.898%, Loss = 0.7386617183685302
Epoch: 6799, Batch Gradient Norm: 24.116490159481152
Epoch: 6799, Batch Gradient Norm after: 22.360679416240956
Epoch 6800/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7370330333709717
Epoch: 6800, Batch Gradient Norm: 24.634467239740484
Epoch: 6800, Batch Gradient Norm after: 22.360676300644414
Epoch 6801/10000, Prediction Accuracy = 59.86%, Loss = 0.7384960293769837
Epoch: 6801, Batch Gradient Norm: 24.081003332766393
Epoch: 6801, Batch Gradient Norm after: 22.330080932104686
Epoch 6802/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.736893379688263
Epoch: 6802, Batch Gradient Norm: 24.673460662636355
Epoch: 6802, Batch Gradient Norm after: 22.36067689527616
Epoch 6803/10000, Prediction Accuracy = 59.85%, Loss = 0.7385856509208679
Epoch: 6803, Batch Gradient Norm: 24.09578718768808
Epoch: 6803, Batch Gradient Norm after: 22.36067833825952
Epoch 6804/10000, Prediction Accuracy = 59.85%, Loss = 0.7369026660919189
Epoch: 6804, Batch Gradient Norm: 24.63763107183475
Epoch: 6804, Batch Gradient Norm after: 22.36067703128719
Epoch 6805/10000, Prediction Accuracy = 59.862%, Loss = 0.7384238362312316
Epoch: 6805, Batch Gradient Norm: 24.08131852359433
Epoch: 6805, Batch Gradient Norm after: 22.342468436714555
Epoch 6806/10000, Prediction Accuracy = 59.798%, Loss = 0.736749529838562
Epoch: 6806, Batch Gradient Norm: 24.650874077299484
Epoch: 6806, Batch Gradient Norm after: 22.36067874272241
Epoch 6807/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.73832848072052
Epoch: 6807, Batch Gradient Norm: 24.092682622873447
Epoch: 6807, Batch Gradient Norm after: 22.356076928161265
Epoch 6808/10000, Prediction Accuracy = 59.822%, Loss = 0.7367140889167786
Epoch: 6808, Batch Gradient Norm: 24.627737263520075
Epoch: 6808, Batch Gradient Norm after: 22.360680309463344
Epoch 6809/10000, Prediction Accuracy = 59.876%, Loss = 0.7382211446762085
Epoch: 6809, Batch Gradient Norm: 24.06664671430832
Epoch: 6809, Batch Gradient Norm after: 22.335878828184136
Epoch 6810/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.7365801215171814
Epoch: 6810, Batch Gradient Norm: 24.650304807396704
Epoch: 6810, Batch Gradient Norm after: 22.36067844622755
Epoch 6811/10000, Prediction Accuracy = 59.872%, Loss = 0.7381877064704895
Epoch: 6811, Batch Gradient Norm: 24.086506847030776
Epoch: 6811, Batch Gradient Norm after: 22.36067692849382
Epoch 6812/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.73654545545578
Epoch: 6812, Batch Gradient Norm: 24.617626642221932
Epoch: 6812, Batch Gradient Norm after: 22.360678236930145
Epoch 6813/10000, Prediction Accuracy = 59.858000000000004%, Loss = 0.7380832552909851
Epoch: 6813, Batch Gradient Norm: 24.053408519128453
Epoch: 6813, Batch Gradient Norm after: 22.33260695950053
Epoch 6814/10000, Prediction Accuracy = 59.862%, Loss = 0.7364566802978516
Epoch: 6814, Batch Gradient Norm: 24.650082719208214
Epoch: 6814, Batch Gradient Norm after: 22.360677014150518
Epoch 6815/10000, Prediction Accuracy = 59.85999999999999%, Loss = 0.7381218194961547
Epoch: 6815, Batch Gradient Norm: 24.06507947213707
Epoch: 6815, Batch Gradient Norm after: 22.36067855774692
Epoch 6816/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.7363840341567993
Epoch: 6816, Batch Gradient Norm: 24.619295801680554
Epoch: 6816, Batch Gradient Norm after: 22.360676445546822
Epoch 6817/10000, Prediction Accuracy = 59.862%, Loss = 0.7379203915596009
Epoch: 6817, Batch Gradient Norm: 24.05165740195074
Epoch: 6817, Batch Gradient Norm after: 22.342766899959052
Epoch 6818/10000, Prediction Accuracy = 59.838%, Loss = 0.7362633347511292
Epoch: 6818, Batch Gradient Norm: 24.632244575049363
Epoch: 6818, Batch Gradient Norm after: 22.360678324389816
Epoch 6819/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.7378725528717041
Epoch: 6819, Batch Gradient Norm: 24.059986319729745
Epoch: 6819, Batch Gradient Norm after: 22.3547148718199
Epoch 6820/10000, Prediction Accuracy = 59.86999999999999%, Loss = 0.7362311720848084
Epoch: 6820, Batch Gradient Norm: 24.61002394401786
Epoch: 6820, Batch Gradient Norm after: 22.36067906448622
Epoch 6821/10000, Prediction Accuracy = 59.898%, Loss = 0.7377514958381652
Epoch: 6821, Batch Gradient Norm: 24.03644756270737
Epoch: 6821, Batch Gradient Norm after: 22.33696194504869
Epoch 6822/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.7360708355903626
Epoch: 6822, Batch Gradient Norm: 24.63583435623075
Epoch: 6822, Batch Gradient Norm after: 22.360677998922665
Epoch 6823/10000, Prediction Accuracy = 59.864%, Loss = 0.7377370476722718
Epoch: 6823, Batch Gradient Norm: 24.055712931738274
Epoch: 6823, Batch Gradient Norm after: 22.36068069111216
Epoch 6824/10000, Prediction Accuracy = 59.824%, Loss = 0.7360961198806762
Epoch: 6824, Batch Gradient Norm: 24.601228217807197
Epoch: 6824, Batch Gradient Norm after: 22.360677670072462
Epoch 6825/10000, Prediction Accuracy = 59.852%, Loss = 0.737665855884552
Epoch: 6825, Batch Gradient Norm: 24.02436375508541
Epoch: 6825, Batch Gradient Norm after: 22.3360418272935
Epoch 6826/10000, Prediction Accuracy = 59.872%, Loss = 0.735957932472229
Epoch: 6826, Batch Gradient Norm: 24.628966013448995
Epoch: 6826, Batch Gradient Norm after: 22.36068028339993
Epoch 6827/10000, Prediction Accuracy = 59.89%, Loss = 0.7376108884811401
Epoch: 6827, Batch Gradient Norm: 24.041194387794487
Epoch: 6827, Batch Gradient Norm after: 22.360678302962263
Epoch 6828/10000, Prediction Accuracy = 59.85799999999999%, Loss = 0.7358800888061523
Epoch: 6828, Batch Gradient Norm: 24.60163868545411
Epoch: 6828, Batch Gradient Norm after: 22.36067700742859
Epoch 6829/10000, Prediction Accuracy = 59.872%, Loss = 0.7374474048614502
Epoch: 6829, Batch Gradient Norm: 24.022292553478987
Epoch: 6829, Batch Gradient Norm after: 22.340512582610483
Epoch 6830/10000, Prediction Accuracy = 59.862%, Loss = 0.7357821106910706
Epoch: 6830, Batch Gradient Norm: 24.618907967419045
Epoch: 6830, Batch Gradient Norm after: 22.360676514594257
Epoch 6831/10000, Prediction Accuracy = 59.896%, Loss = 0.73742835521698
Epoch: 6831, Batch Gradient Norm: 24.034363584490546
Epoch: 6831, Batch Gradient Norm after: 22.359558850343628
Epoch 6832/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.7357337713241577
Epoch: 6832, Batch Gradient Norm: 24.590677703422553
Epoch: 6832, Batch Gradient Norm after: 22.360676370881478
Epoch 6833/10000, Prediction Accuracy = 59.858000000000004%, Loss = 0.7372718930244446
Epoch: 6833, Batch Gradient Norm: 24.005736040902043
Epoch: 6833, Batch Gradient Norm after: 22.335729531459304
Epoch 6834/10000, Prediction Accuracy = 59.882000000000005%, Loss = 0.7356014251708984
Epoch: 6834, Batch Gradient Norm: 24.620328598874895
Epoch: 6834, Batch Gradient Norm after: 22.360678982376175
Epoch 6835/10000, Prediction Accuracy = 59.854%, Loss = 0.7373379707336426
Epoch: 6835, Batch Gradient Norm: 24.016698997314062
Epoch: 6835, Batch Gradient Norm after: 22.36067893238232
Epoch 6836/10000, Prediction Accuracy = 59.884%, Loss = 0.735611081123352
Epoch: 6836, Batch Gradient Norm: 24.590890195004175
Epoch: 6836, Batch Gradient Norm after: 22.3606818769608
Epoch 6837/10000, Prediction Accuracy = 59.870000000000005%, Loss = 0.7372055172920227
Epoch: 6837, Batch Gradient Norm: 24.00178586638101
Epoch: 6837, Batch Gradient Norm after: 22.34547747544102
Epoch 6838/10000, Prediction Accuracy = 59.831999999999994%, Loss = 0.7354529500007629
Epoch: 6838, Batch Gradient Norm: 24.59976938826129
Epoch: 6838, Batch Gradient Norm after: 22.36067699959256
Epoch 6839/10000, Prediction Accuracy = 59.902%, Loss = 0.7370915293693543
Epoch: 6839, Batch Gradient Norm: 24.010154679981735
Epoch: 6839, Batch Gradient Norm after: 22.35633082987316
Epoch 6840/10000, Prediction Accuracy = 59.854%, Loss = 0.7354099750518799
Epoch: 6840, Batch Gradient Norm: 24.581903926090213
Epoch: 6840, Batch Gradient Norm after: 22.360676299351805
Epoch 6841/10000, Prediction Accuracy = 59.898%, Loss = 0.7369985461235047
Epoch: 6841, Batch Gradient Norm: 23.987269765066415
Epoch: 6841, Batch Gradient Norm after: 22.340949736515288
Epoch 6842/10000, Prediction Accuracy = 59.874%, Loss = 0.7352896451950073
Epoch: 6842, Batch Gradient Norm: 24.60074317051388
Epoch: 6842, Batch Gradient Norm after: 22.360676895239312
Epoch 6843/10000, Prediction Accuracy = 59.907999999999994%, Loss = 0.736944055557251
Epoch: 6843, Batch Gradient Norm: 24.004772546923085
Epoch: 6843, Batch Gradient Norm after: 22.360679283897127
Epoch 6844/10000, Prediction Accuracy = 59.864%, Loss = 0.735243284702301
Epoch: 6844, Batch Gradient Norm: 24.57329738854553
Epoch: 6844, Batch Gradient Norm after: 22.360680093680973
Epoch 6845/10000, Prediction Accuracy = 59.866%, Loss = 0.7368541717529297
Epoch: 6845, Batch Gradient Norm: 23.976364836547543
Epoch: 6845, Batch Gradient Norm after: 22.337197162665976
Epoch 6846/10000, Prediction Accuracy = 59.874%, Loss = 0.735166871547699
Epoch: 6846, Batch Gradient Norm: 24.59731897099148
Epoch: 6846, Batch Gradient Norm after: 22.36067809194919
Epoch 6847/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.7368937730789185
Epoch: 6847, Batch Gradient Norm: 23.98329371333556
Epoch: 6847, Batch Gradient Norm after: 22.360677748032813
Epoch 6848/10000, Prediction Accuracy = 59.86%, Loss = 0.7350895524024963
Epoch: 6848, Batch Gradient Norm: 24.57075963453916
Epoch: 6848, Batch Gradient Norm after: 22.360677702586184
Epoch 6849/10000, Prediction Accuracy = 59.898%, Loss = 0.7366884589195252
Epoch: 6849, Batch Gradient Norm: 23.972409746089944
Epoch: 6849, Batch Gradient Norm after: 22.345454791100646
Epoch 6850/10000, Prediction Accuracy = 59.872%, Loss = 0.7349665760993958
Epoch: 6850, Batch Gradient Norm: 24.58316132721262
Epoch: 6850, Batch Gradient Norm after: 22.360677985102498
Epoch 6851/10000, Prediction Accuracy = 59.876%, Loss = 0.7366398215293884
Epoch: 6851, Batch Gradient Norm: 23.98201602028307
Epoch: 6851, Batch Gradient Norm after: 22.357320069142563
Epoch 6852/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7349440097808838
Epoch: 6852, Batch Gradient Norm: 24.560844227744482
Epoch: 6852, Batch Gradient Norm after: 22.360677783584475
Epoch 6853/10000, Prediction Accuracy = 59.9%, Loss = 0.736512553691864
Epoch: 6853, Batch Gradient Norm: 23.953698141657615
Epoch: 6853, Batch Gradient Norm after: 22.337844108392
Epoch 6854/10000, Prediction Accuracy = 59.874%, Loss = 0.734777855873108
Epoch: 6854, Batch Gradient Norm: 24.58935574509873
Epoch: 6854, Batch Gradient Norm after: 22.360677608850633
Epoch 6855/10000, Prediction Accuracy = 59.86999999999999%, Loss = 0.7365134000778198
Epoch: 6855, Batch Gradient Norm: 23.969800890523175
Epoch: 6855, Batch Gradient Norm after: 22.360677871220272
Epoch 6856/10000, Prediction Accuracy = 59.884%, Loss = 0.7347924947738648
Epoch: 6856, Batch Gradient Norm: 24.556480370634553
Epoch: 6856, Batch Gradient Norm after: 22.360677135243318
Epoch 6857/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.7364483594894409
Epoch: 6857, Batch Gradient Norm: 23.94555050402665
Epoch: 6857, Batch Gradient Norm after: 22.340495418245332
Epoch 6858/10000, Prediction Accuracy = 59.888%, Loss = 0.7346739292144775
Epoch: 6858, Batch Gradient Norm: 24.574345718825473
Epoch: 6858, Batch Gradient Norm after: 22.360679088634527
Epoch 6859/10000, Prediction Accuracy = 59.88800000000001%, Loss = 0.7363785743713379
Epoch: 6859, Batch Gradient Norm: 23.961068263901204
Epoch: 6859, Batch Gradient Norm after: 22.360678452902953
Epoch 6860/10000, Prediction Accuracy = 59.862%, Loss = 0.7345996737480164
Epoch: 6860, Batch Gradient Norm: 24.54880712121871
Epoch: 6860, Batch Gradient Norm after: 22.360678665482226
Epoch 6861/10000, Prediction Accuracy = 59.878%, Loss = 0.7361990213394165
Epoch: 6861, Batch Gradient Norm: 23.93591744041095
Epoch: 6861, Batch Gradient Norm after: 22.33698555132689
Epoch 6862/10000, Prediction Accuracy = 59.85%, Loss = 0.7344822406768798
Epoch: 6862, Batch Gradient Norm: 24.57205811827248
Epoch: 6862, Batch Gradient Norm after: 22.360679847956806
Epoch 6863/10000, Prediction Accuracy = 59.884%, Loss = 0.7362028360366821
Epoch: 6863, Batch Gradient Norm: 23.949996409495782
Epoch: 6863, Batch Gradient Norm after: 22.36067894587888
Epoch 6864/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7344441175460815
Epoch: 6864, Batch Gradient Norm: 24.543789311621534
Epoch: 6864, Batch Gradient Norm after: 22.360677094336033
Epoch 6865/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7360419631004333
Epoch: 6865, Batch Gradient Norm: 23.92858584363418
Epoch: 6865, Batch Gradient Norm after: 22.337468509162264
Epoch 6866/10000, Prediction Accuracy = 59.912%, Loss = 0.7343189358711243
Epoch: 6866, Batch Gradient Norm: 24.567749512987913
Epoch: 6866, Batch Gradient Norm after: 22.360678236048113
Epoch 6867/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7361114263534546
Epoch: 6867, Batch Gradient Norm: 23.940684461295433
Epoch: 6867, Batch Gradient Norm after: 22.36067994760755
Epoch 6868/10000, Prediction Accuracy = 59.9%, Loss = 0.7343451023101807
Epoch: 6868, Batch Gradient Norm: 24.5338890360537
Epoch: 6868, Batch Gradient Norm after: 22.360676839865246
Epoch 6869/10000, Prediction Accuracy = 59.864%, Loss = 0.7359744668006897
Epoch: 6869, Batch Gradient Norm: 23.91839767744903
Epoch: 6869, Batch Gradient Norm after: 22.339511078220344
Epoch 6870/10000, Prediction Accuracy = 59.89399999999999%, Loss = 0.7341615438461304
Epoch: 6870, Batch Gradient Norm: 24.55380508335389
Epoch: 6870, Batch Gradient Norm after: 22.360676938319045
Epoch 6871/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7358811855316162
Epoch: 6871, Batch Gradient Norm: 23.93673364860734
Epoch: 6871, Batch Gradient Norm after: 22.360678123546503
Epoch 6872/10000, Prediction Accuracy = 59.872%, Loss = 0.7341354131698609
Epoch: 6872, Batch Gradient Norm: 24.52338437831475
Epoch: 6872, Batch Gradient Norm after: 22.360679352589642
Epoch 6873/10000, Prediction Accuracy = 59.9%, Loss = 0.7357468366622925
Epoch: 6873, Batch Gradient Norm: 23.908907979497418
Epoch: 6873, Batch Gradient Norm after: 22.336657505201636
Epoch 6874/10000, Prediction Accuracy = 59.88000000000001%, Loss = 0.7340062260627747
Epoch: 6874, Batch Gradient Norm: 24.550155306498468
Epoch: 6874, Batch Gradient Norm after: 22.360676910714986
Epoch 6875/10000, Prediction Accuracy = 59.936%, Loss = 0.735727322101593
Epoch: 6875, Batch Gradient Norm: 23.924723883056835
Epoch: 6875, Batch Gradient Norm after: 22.36067891569638
Epoch 6876/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7339503288269043
Epoch: 6876, Batch Gradient Norm: 24.523344986428192
Epoch: 6876, Batch Gradient Norm after: 22.360675872142526
Epoch 6877/10000, Prediction Accuracy = 59.878%, Loss = 0.7356284499168396
Epoch: 6877, Batch Gradient Norm: 23.90426963081019
Epoch: 6877, Batch Gradient Norm after: 22.339417955127104
Epoch 6878/10000, Prediction Accuracy = 59.888%, Loss = 0.7338931441307068
Epoch: 6878, Batch Gradient Norm: 24.542924187194938
Epoch: 6878, Batch Gradient Norm after: 22.3606780129134
Epoch 6879/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.7356756806373597
Epoch: 6879, Batch Gradient Norm: 23.911676717655403
Epoch: 6879, Batch Gradient Norm after: 22.360677910242558
Epoch 6880/10000, Prediction Accuracy = 59.89%, Loss = 0.7338237166404724
Epoch: 6880, Batch Gradient Norm: 24.513294034295207
Epoch: 6880, Batch Gradient Norm after: 22.36067919150629
Epoch 6881/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7354716897010803
Epoch: 6881, Batch Gradient Norm: 23.895809720099013
Epoch: 6881, Batch Gradient Norm after: 22.343027470594073
Epoch 6882/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.7336813569068908
Epoch: 6882, Batch Gradient Norm: 24.528564460061514
Epoch: 6882, Batch Gradient Norm after: 22.36067592176655
Epoch 6883/10000, Prediction Accuracy = 59.908%, Loss = 0.7354135155677796
Epoch: 6883, Batch Gradient Norm: 23.905200425870166
Epoch: 6883, Batch Gradient Norm after: 22.358317438004462
Epoch 6884/10000, Prediction Accuracy = 59.85999999999999%, Loss = 0.733660364151001
Epoch: 6884, Batch Gradient Norm: 24.503517040125303
Epoch: 6884, Batch Gradient Norm after: 22.36067745126635
Epoch 6885/10000, Prediction Accuracy = 59.914%, Loss = 0.7352957248687744
Epoch: 6885, Batch Gradient Norm: 23.87626975822985
Epoch: 6885, Batch Gradient Norm after: 22.335737539239375
Epoch 6886/10000, Prediction Accuracy = 59.894000000000005%, Loss = 0.733493161201477
Epoch: 6886, Batch Gradient Norm: 24.535449562621576
Epoch: 6886, Batch Gradient Norm after: 22.360678189503954
Epoch 6887/10000, Prediction Accuracy = 59.884%, Loss = 0.7352952837944031
Epoch: 6887, Batch Gradient Norm: 23.89037105543153
Epoch: 6887, Batch Gradient Norm after: 22.360678892073757
Epoch 6888/10000, Prediction Accuracy = 59.898%, Loss = 0.7334914565086365
Epoch: 6888, Batch Gradient Norm: 24.506871791898803
Epoch: 6888, Batch Gradient Norm after: 22.360677046577713
Epoch 6889/10000, Prediction Accuracy = 59.876%, Loss = 0.7352383971214295
Epoch: 6889, Batch Gradient Norm: 23.877335940484592
Epoch: 6889, Batch Gradient Norm after: 22.346559680218807
Epoch 6890/10000, Prediction Accuracy = 59.89%, Loss = 0.7334247589111328
Epoch: 6890, Batch Gradient Norm: 24.50838950807552
Epoch: 6890, Batch Gradient Norm after: 22.360678348853074
Epoch 6891/10000, Prediction Accuracy = 59.872%, Loss = 0.7351534008979798
Epoch: 6891, Batch Gradient Norm: 23.87885231353397
Epoch: 6891, Batch Gradient Norm after: 22.354680997488646
Epoch 6892/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.7333153486251831
Epoch: 6892, Batch Gradient Norm: 24.495344467044927
Epoch: 6892, Batch Gradient Norm after: 22.360678042155694
Epoch 6893/10000, Prediction Accuracy = 59.888%, Loss = 0.7350024819374085
Epoch: 6893, Batch Gradient Norm: 23.863158546311237
Epoch: 6893, Batch Gradient Norm after: 22.341397028310066
Epoch 6894/10000, Prediction Accuracy = 59.864%, Loss = 0.7332129836082458
Epoch: 6894, Batch Gradient Norm: 24.507431947859796
Epoch: 6894, Batch Gradient Norm after: 22.36067865197523
Epoch 6895/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.7349843978881836
Epoch: 6895, Batch Gradient Norm: 23.872057584695817
Epoch: 6895, Batch Gradient Norm after: 22.358105002024857
Epoch 6896/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.7331782221794129
Epoch: 6896, Batch Gradient Norm: 24.48442018982669
Epoch: 6896, Batch Gradient Norm after: 22.36067742448393
Epoch 6897/10000, Prediction Accuracy = 59.924%, Loss = 0.7348351359367371
Epoch: 6897, Batch Gradient Norm: 23.84859102544393
Epoch: 6897, Batch Gradient Norm after: 22.337791264099526
Epoch 6898/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7330349802970886
Epoch: 6898, Batch Gradient Norm: 24.511936689901255
Epoch: 6898, Batch Gradient Norm after: 22.36067835996866
Epoch 6899/10000, Prediction Accuracy = 59.876%, Loss = 0.7348933577537536
Epoch: 6899, Batch Gradient Norm: 23.86063781749678
Epoch: 6899, Batch Gradient Norm after: 22.360678636904538
Epoch 6900/10000, Prediction Accuracy = 59.888%, Loss = 0.7330650448799133
Epoch: 6900, Batch Gradient Norm: 24.47963361951288
Epoch: 6900, Batch Gradient Norm after: 22.360678211555328
Epoch 6901/10000, Prediction Accuracy = 59.886%, Loss = 0.7347815871238709
Epoch: 6901, Batch Gradient Norm: 23.841788912775225
Epoch: 6901, Batch Gradient Norm after: 22.343105283538033
Epoch 6902/10000, Prediction Accuracy = 59.896%, Loss = 0.732908058166504
Epoch: 6902, Batch Gradient Norm: 24.494121577858866
Epoch: 6902, Batch Gradient Norm after: 22.36067952812977
Epoch 6903/10000, Prediction Accuracy = 59.91799999999999%, Loss = 0.7346709489822387
Epoch: 6903, Batch Gradient Norm: 23.854998496732147
Epoch: 6903, Batch Gradient Norm after: 22.359321953207978
Epoch 6904/10000, Prediction Accuracy = 59.912%, Loss = 0.732854163646698
Epoch: 6904, Batch Gradient Norm: 24.469636559839532
Epoch: 6904, Batch Gradient Norm after: 22.36067786088519
Epoch 6905/10000, Prediction Accuracy = 59.908%, Loss = 0.7345414757728577
Epoch: 6905, Batch Gradient Norm: 23.826341114912537
Epoch: 6905, Batch Gradient Norm after: 22.335848215927513
Epoch 6906/10000, Prediction Accuracy = 59.876%, Loss = 0.7327336192131042
Epoch: 6906, Batch Gradient Norm: 24.495129684802055
Epoch: 6906, Batch Gradient Norm after: 22.3606769371401
Epoch 6907/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7345337748527527
Epoch: 6907, Batch Gradient Norm: 23.84180008535049
Epoch: 6907, Batch Gradient Norm after: 22.360677375901016
Epoch 6908/10000, Prediction Accuracy = 59.907999999999994%, Loss = 0.7326742768287658
Epoch: 6908, Batch Gradient Norm: 24.46533200534422
Epoch: 6908, Batch Gradient Norm after: 22.360676131808624
Epoch 6909/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7344084739685058
Epoch: 6909, Batch Gradient Norm: 23.822369611598376
Epoch: 6909, Batch Gradient Norm after: 22.33967970795106
Epoch 6910/10000, Prediction Accuracy = 59.89399999999999%, Loss = 0.7326080441474915
Epoch: 6910, Batch Gradient Norm: 24.484300491629643
Epoch: 6910, Batch Gradient Norm after: 22.36067873343281
Epoch 6911/10000, Prediction Accuracy = 59.87800000000001%, Loss = 0.7344565987586975
Epoch: 6911, Batch Gradient Norm: 23.835088038090145
Epoch: 6911, Batch Gradient Norm after: 22.36067935308979
Epoch 6912/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7325718879699707
Epoch: 6912, Batch Gradient Norm: 24.450571980369617
Epoch: 6912, Batch Gradient Norm after: 22.360677616151598
Epoch 6913/10000, Prediction Accuracy = 59.886%, Loss = 0.7342541694641114
Epoch: 6913, Batch Gradient Norm: 23.808311660668124
Epoch: 6913, Batch Gradient Norm after: 22.336362896111517
Epoch 6914/10000, Prediction Accuracy = 59.876%, Loss = 0.7323925256729126
Epoch: 6914, Batch Gradient Norm: 24.479886797569232
Epoch: 6914, Batch Gradient Norm after: 22.360678890737574
Epoch 6915/10000, Prediction Accuracy = 59.932%, Loss = 0.7342259526252747
Epoch: 6915, Batch Gradient Norm: 23.82534146579467
Epoch: 6915, Batch Gradient Norm after: 22.360677502361902
Epoch 6916/10000, Prediction Accuracy = 59.878%, Loss = 0.7323949098587036
Epoch: 6916, Batch Gradient Norm: 24.444695314778002
Epoch: 6916, Batch Gradient Norm after: 22.36067786805832
Epoch 6917/10000, Prediction Accuracy = 59.928%, Loss = 0.734080457687378
Epoch: 6917, Batch Gradient Norm: 23.794257850385964
Epoch: 6917, Batch Gradient Norm after: 22.3338963660887
Epoch 6918/10000, Prediction Accuracy = 59.878%, Loss = 0.7322303533554078
Epoch: 6918, Batch Gradient Norm: 24.480100157799384
Epoch: 6918, Batch Gradient Norm after: 22.360679314172128
Epoch 6919/10000, Prediction Accuracy = 59.896%, Loss = 0.734085488319397
Epoch: 6919, Batch Gradient Norm: 23.80812432088451
Epoch: 6919, Batch Gradient Norm after: 22.360681277708427
Epoch 6920/10000, Prediction Accuracy = 59.906000000000006%, Loss = 0.7322197675704956
Epoch: 6920, Batch Gradient Norm: 24.4489323492224
Epoch: 6920, Batch Gradient Norm after: 22.360677011964775
Epoch 6921/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.7340153098106384
Epoch: 6921, Batch Gradient Norm: 23.79326874720513
Epoch: 6921, Batch Gradient Norm after: 22.3427160181713
Epoch 6922/10000, Prediction Accuracy = 59.884%, Loss = 0.73214350938797
Epoch: 6922, Batch Gradient Norm: 24.45785105291159
Epoch: 6922, Batch Gradient Norm after: 22.360677539216212
Epoch 6923/10000, Prediction Accuracy = 59.876%, Loss = 0.7339578032493591
Epoch: 6923, Batch Gradient Norm: 23.79913645998869
Epoch: 6923, Batch Gradient Norm after: 22.355982691085394
Epoch 6924/10000, Prediction Accuracy = 59.902%, Loss = 0.7320499777793884
Epoch: 6924, Batch Gradient Norm: 24.438056499713205
Epoch: 6924, Batch Gradient Norm after: 22.360677787206132
Epoch 6925/10000, Prediction Accuracy = 59.908%, Loss = 0.733795702457428
Epoch: 6925, Batch Gradient Norm: 23.780073839382137
Epoch: 6925, Batch Gradient Norm after: 22.338451143970694
Epoch 6926/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.7319301843643189
Epoch: 6926, Batch Gradient Norm: 24.456948454897436
Epoch: 6926, Batch Gradient Norm after: 22.36067810800197
Epoch 6927/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7337867498397828
Epoch: 6927, Batch Gradient Norm: 23.792592498578273
Epoch: 6927, Batch Gradient Norm after: 22.359260053982023
Epoch 6928/10000, Prediction Accuracy = 59.878%, Loss = 0.7319161653518677
Epoch: 6928, Batch Gradient Norm: 24.4247610257839
Epoch: 6928, Batch Gradient Norm after: 22.36067749641349
Epoch 6929/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7336288928985596
Epoch: 6929, Batch Gradient Norm: 23.76254927153564
Epoch: 6929, Batch Gradient Norm after: 22.333863637861825
Epoch 6930/10000, Prediction Accuracy = 59.938%, Loss = 0.7317476630210876
Epoch: 6930, Batch Gradient Norm: 24.464544202617166
Epoch: 6930, Batch Gradient Norm after: 22.36067699973888
Epoch 6931/10000, Prediction Accuracy = 59.902%, Loss = 0.7336931705474854
Epoch: 6931, Batch Gradient Norm: 23.777768518562272
Epoch: 6931, Batch Gradient Norm after: 22.36067928552859
Epoch 6932/10000, Prediction Accuracy = 59.912%, Loss = 0.7317752242088318
Epoch: 6932, Batch Gradient Norm: 24.426662107288873
Epoch: 6932, Batch Gradient Norm after: 22.360678141658877
Epoch 6933/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.733600652217865
Epoch: 6933, Batch Gradient Norm: 23.7602028278386
Epoch: 6933, Batch Gradient Norm after: 22.343722173055166
Epoch 6934/10000, Prediction Accuracy = 59.92%, Loss = 0.7316421031951904
Epoch: 6934, Batch Gradient Norm: 24.43730454193472
Epoch: 6934, Batch Gradient Norm after: 22.360677325559855
Epoch 6935/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7334771037101746
Epoch: 6935, Batch Gradient Norm: 23.7700410856182
Epoch: 6935, Batch Gradient Norm after: 22.355435360538575
Epoch 6936/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.7315628290176391
Epoch: 6936, Batch Gradient Norm: 24.419114545977628
Epoch: 6936, Batch Gradient Norm after: 22.360679010525004
Epoch 6937/10000, Prediction Accuracy = 59.932%, Loss = 0.7333612203598022
Epoch: 6937, Batch Gradient Norm: 23.749321082726603
Epoch: 6937, Batch Gradient Norm after: 22.33969697383116
Epoch 6938/10000, Prediction Accuracy = 59.884%, Loss = 0.7314728140830994
Epoch: 6938, Batch Gradient Norm: 24.43644519474616
Epoch: 6938, Batch Gradient Norm after: 22.360677763753845
Epoch 6939/10000, Prediction Accuracy = 59.92199999999999%, Loss = 0.7333290219306946
Epoch: 6939, Batch Gradient Norm: 23.763724186874484
Epoch: 6939, Batch Gradient Norm after: 22.358974161989998
Epoch 6940/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7314061284065246
Epoch: 6940, Batch Gradient Norm: 24.412059160963477
Epoch: 6940, Batch Gradient Norm after: 22.360675296557957
Epoch 6941/10000, Prediction Accuracy = 59.902%, Loss = 0.7332042217254638
Epoch: 6941, Batch Gradient Norm: 23.739543586989836
Epoch: 6941, Batch Gradient Norm after: 22.33897455153927
Epoch 6942/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7313281059265136
Epoch: 6942, Batch Gradient Norm: 24.436474403857716
Epoch: 6942, Batch Gradient Norm after: 22.36067794040202
Epoch 6943/10000, Prediction Accuracy = 59.888%, Loss = 0.7332703351974488
Epoch: 6943, Batch Gradient Norm: 23.747194301974947
Epoch: 6943, Batch Gradient Norm after: 22.360680276713367
Epoch 6944/10000, Prediction Accuracy = 59.912%, Loss = 0.7312902927398681
Epoch: 6944, Batch Gradient Norm: 24.40849831952226
Epoch: 6944, Batch Gradient Norm after: 22.360678972572874
Epoch 6945/10000, Prediction Accuracy = 59.898%, Loss = 0.7330780029296875
Epoch: 6945, Batch Gradient Norm: 23.731659101111195
Epoch: 6945, Batch Gradient Norm after: 22.342696810672685
Epoch 6946/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.7311380267143249
Epoch: 6946, Batch Gradient Norm: 24.42196288085299
Epoch: 6946, Batch Gradient Norm after: 22.360675588138395
Epoch 6947/10000, Prediction Accuracy = 59.924%, Loss = 0.7330160737037659
Epoch: 6947, Batch Gradient Norm: 23.743158247454687
Epoch: 6947, Batch Gradient Norm after: 22.35651986413293
Epoch 6948/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.731110680103302
Epoch: 6948, Batch Gradient Norm: 24.397586868173438
Epoch: 6948, Batch Gradient Norm after: 22.360678831539076
Epoch 6949/10000, Prediction Accuracy = 59.93000000000001%, Loss = 0.7329007148742676
Epoch: 6949, Batch Gradient Norm: 23.718762369250506
Epoch: 6949, Batch Gradient Norm after: 22.336310035569397
Epoch 6950/10000, Prediction Accuracy = 59.866%, Loss = 0.7309632182121277
Epoch: 6950, Batch Gradient Norm: 24.426359230811723
Epoch: 6950, Batch Gradient Norm after: 22.360676675101757
Epoch 6951/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.732879900932312
Epoch: 6951, Batch Gradient Norm: 23.73369739022393
Epoch: 6951, Batch Gradient Norm after: 22.36068030270766
Epoch 6952/10000, Prediction Accuracy = 59.928%, Loss = 0.7309486150741578
Epoch: 6952, Batch Gradient Norm: 24.394819251625577
Epoch: 6952, Batch Gradient Norm after: 22.36067760445203
Epoch 6953/10000, Prediction Accuracy = 59.902%, Loss = 0.7328006744384765
Epoch: 6953, Batch Gradient Norm: 23.7091190725382
Epoch: 6953, Batch Gradient Norm after: 22.338175457028676
Epoch 6954/10000, Prediction Accuracy = 59.908%, Loss = 0.730873191356659
Epoch: 6954, Batch Gradient Norm: 24.416287756075985
Epoch: 6954, Batch Gradient Norm after: 22.360676496388262
Epoch 6955/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.7328145265579223
Epoch: 6955, Batch Gradient Norm: 23.720276203838118
Epoch: 6955, Batch Gradient Norm after: 22.360679070211514
Epoch 6956/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7307904839515686
Epoch: 6956, Batch Gradient Norm: 24.388245806314885
Epoch: 6956, Batch Gradient Norm after: 22.360676103428585
Epoch 6957/10000, Prediction Accuracy = 59.898%, Loss = 0.7326008319854737
Epoch: 6957, Batch Gradient Norm: 23.703227800171433
Epoch: 6957, Batch Gradient Norm after: 22.340144503155457
Epoch 6958/10000, Prediction Accuracy = 59.886%, Loss = 0.730675733089447
Epoch: 6958, Batch Gradient Norm: 24.40561172097683
Epoch: 6958, Batch Gradient Norm after: 22.360675979611493
Epoch 6959/10000, Prediction Accuracy = 59.902%, Loss = 0.7325904726982116
Epoch: 6959, Batch Gradient Norm: 23.713642144108107
Epoch: 6959, Batch Gradient Norm after: 22.357360460469828
Epoch 6960/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.7306566715240479
Epoch: 6960, Batch Gradient Norm: 24.37959461576491
Epoch: 6960, Batch Gradient Norm after: 22.360676206007355
Epoch 6961/10000, Prediction Accuracy = 59.936%, Loss = 0.7324403643608093
Epoch: 6961, Batch Gradient Norm: 23.686275142486313
Epoch: 6961, Batch Gradient Norm after: 22.33493442701268
Epoch 6962/10000, Prediction Accuracy = 59.92%, Loss = 0.7304778099060059
Epoch: 6962, Batch Gradient Norm: 24.411813560399878
Epoch: 6962, Batch Gradient Norm after: 22.36067968220018
Epoch 6963/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7324828386306763
Epoch: 6963, Batch Gradient Norm: 23.700527956863752
Epoch: 6963, Batch Gradient Norm after: 22.36067918010649
Epoch 6964/10000, Prediction Accuracy = 59.92%, Loss = 0.7305207014083862
Epoch: 6964, Batch Gradient Norm: 24.378506441871814
Epoch: 6964, Batch Gradient Norm after: 22.36067847804588
Epoch 6965/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7324155449867249
Epoch: 6965, Batch Gradient Norm: 23.6822765112358
Epoch: 6965, Batch Gradient Norm after: 22.344180013002475
Epoch 6966/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7303861856460572
Epoch: 6966, Batch Gradient Norm: 24.38953792817802
Epoch: 6966, Batch Gradient Norm after: 22.36067980686571
Epoch 6967/10000, Prediction Accuracy = 59.90999999999999%, Loss = 0.7322845458984375
Epoch: 6967, Batch Gradient Norm: 23.691050955053086
Epoch: 6967, Batch Gradient Norm after: 22.354970898772645
Epoch 6968/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.7303026437759399
Epoch: 6968, Batch Gradient Norm: 24.37044357811516
Epoch: 6968, Batch Gradient Norm after: 22.36067443672642
Epoch 6969/10000, Prediction Accuracy = 59.95799999999999%, Loss = 0.7321640253067017
Epoch: 6969, Batch Gradient Norm: 23.6718729546304
Epoch: 6969, Batch Gradient Norm after: 22.338818426079154
Epoch 6970/10000, Prediction Accuracy = 59.872%, Loss = 0.7301984548568725
Epoch: 6970, Batch Gradient Norm: 24.389313680396093
Epoch: 6970, Batch Gradient Norm after: 22.360674495717475
Epoch 6971/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7321381330490112
Epoch: 6971, Batch Gradient Norm: 23.684679068470984
Epoch: 6971, Batch Gradient Norm after: 22.357170541188932
Epoch 6972/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.7301443815231323
Epoch: 6972, Batch Gradient Norm: 24.362377112681514
Epoch: 6972, Batch Gradient Norm after: 22.36067674791796
Epoch 6973/10000, Prediction Accuracy = 59.93000000000001%, Loss = 0.7320013403892517
Epoch: 6973, Batch Gradient Norm: 23.65997020838977
Epoch: 6973, Batch Gradient Norm after: 22.33728625640788
Epoch 6974/10000, Prediction Accuracy = 59.924%, Loss = 0.7300403237342834
Epoch: 6974, Batch Gradient Norm: 24.388545291090892
Epoch: 6974, Batch Gradient Norm after: 22.36067858760829
Epoch 6975/10000, Prediction Accuracy = 59.92%, Loss = 0.732062304019928
Epoch: 6975, Batch Gradient Norm: 23.674024456253473
Epoch: 6975, Batch Gradient Norm after: 22.360679273992982
Epoch 6976/10000, Prediction Accuracy = 59.908%, Loss = 0.7300459027290345
Epoch: 6976, Batch Gradient Norm: 24.351431866032318
Epoch: 6976, Batch Gradient Norm after: 22.360678643062865
Epoch 6977/10000, Prediction Accuracy = 59.924%, Loss = 0.7319059491157531
Epoch: 6977, Batch Gradient Norm: 23.648346610232657
Epoch: 6977, Batch Gradient Norm after: 22.33733795701643
Epoch 6978/10000, Prediction Accuracy = 59.919999999999995%, Loss = 0.7298651337623596
Epoch: 6978, Batch Gradient Norm: 24.379381466466928
Epoch: 6978, Batch Gradient Norm after: 22.36067747874718
Epoch 6979/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7318470120429993
Epoch: 6979, Batch Gradient Norm: 23.670681194733874
Epoch: 6979, Batch Gradient Norm after: 22.360677018258624
Epoch 6980/10000, Prediction Accuracy = 59.878%, Loss = 0.7298608660697937
Epoch: 6980, Batch Gradient Norm: 24.341498790927997
Epoch: 6980, Batch Gradient Norm after: 22.36067732051365
Epoch 6981/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7317071199417114
Epoch: 6981, Batch Gradient Norm: 23.63489718289366
Epoch: 6981, Batch Gradient Norm after: 22.33022867158844
Epoch 6982/10000, Prediction Accuracy = 59.866%, Loss = 0.7296948313713074
Epoch: 6982, Batch Gradient Norm: 24.380105222498017
Epoch: 6982, Batch Gradient Norm after: 22.360675981847553
Epoch 6983/10000, Prediction Accuracy = 59.924%, Loss = 0.7317087173461914
Epoch: 6983, Batch Gradient Norm: 23.65515150950818
Epoch: 6983, Batch Gradient Norm after: 22.360676404614615
Epoch 6984/10000, Prediction Accuracy = 59.948%, Loss = 0.7296764135360718
Epoch: 6984, Batch Gradient Norm: 24.341768499002754
Epoch: 6984, Batch Gradient Norm after: 22.36067885562521
Epoch 6985/10000, Prediction Accuracy = 59.936%, Loss = 0.7316098690032959
Epoch: 6985, Batch Gradient Norm: 23.63454508855423
Epoch: 6985, Batch Gradient Norm after: 22.337882008815868
Epoch 6986/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.7296241879463196
Epoch: 6986, Batch Gradient Norm: 24.358468978919053
Epoch: 6986, Batch Gradient Norm after: 22.360676742411684
Epoch 6987/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7316286206245423
Epoch: 6987, Batch Gradient Norm: 23.644616134675267
Epoch: 6987, Batch Gradient Norm after: 22.359327944221533
Epoch 6988/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7295473098754883
Epoch: 6988, Batch Gradient Norm: 24.330239244364606
Epoch: 6988, Batch Gradient Norm after: 22.36067779609677
Epoch 6989/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.7314031958580017
Epoch: 6989, Batch Gradient Norm: 23.61948786712999
Epoch: 6989, Batch Gradient Norm after: 22.333771150594202
Epoch 6990/10000, Prediction Accuracy = 59.89%, Loss = 0.7293903708457947
Epoch: 6990, Batch Gradient Norm: 24.359189191712893
Epoch: 6990, Batch Gradient Norm after: 22.360677162488873
Epoch 6991/10000, Prediction Accuracy = 59.936%, Loss = 0.7314134240150452
Epoch: 6991, Batch Gradient Norm: 23.63858394958049
Epoch: 6991, Batch Gradient Norm after: 22.360677698702247
Epoch 6992/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.7293943881988525
Epoch: 6992, Batch Gradient Norm: 24.31948891474876
Epoch: 6992, Batch Gradient Norm after: 22.360677944523978
Epoch 6993/10000, Prediction Accuracy = 59.94%, Loss = 0.7312420248985291
Epoch: 6993, Batch Gradient Norm: 23.60752532967283
Epoch: 6993, Batch Gradient Norm after: 22.33205253902248
Epoch 6994/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7292190432548523
Epoch: 6994, Batch Gradient Norm: 24.35669325554596
Epoch: 6994, Batch Gradient Norm after: 22.360677296671657
Epoch 6995/10000, Prediction Accuracy = 59.94000000000001%, Loss = 0.7312836647033691
Epoch: 6995, Batch Gradient Norm: 23.62714497264146
Epoch: 6995, Batch Gradient Norm after: 22.360680474304488
Epoch 6996/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.7292529940605164
Epoch: 6996, Batch Gradient Norm: 24.320805327374003
Epoch: 6996, Batch Gradient Norm after: 22.36067851872596
Epoch 6997/10000, Prediction Accuracy = 59.936%, Loss = 0.7312013387680054
Epoch: 6997, Batch Gradient Norm: 23.607253306327742
Epoch: 6997, Batch Gradient Norm after: 22.34237272162213
Epoch 6998/10000, Prediction Accuracy = 59.924%, Loss = 0.7291379928588867
Epoch: 6998, Batch Gradient Norm: 24.333831802859088
Epoch: 6998, Batch Gradient Norm after: 22.360678071413833
Epoch 6999/10000, Prediction Accuracy = 59.919999999999995%, Loss = 0.7311017274856567
Epoch: 6999, Batch Gradient Norm: 23.618353449119798
Epoch: 6999, Batch Gradient Norm after: 22.356898620700886
Epoch 7000/10000, Prediction Accuracy = 59.92%, Loss = 0.7290567994117737
Epoch: 7000, Batch Gradient Norm: 24.315188027816696
Epoch: 7000, Batch Gradient Norm after: 22.360675961461027
Epoch 7001/10000, Prediction Accuracy = 59.956%, Loss = 0.7309661626815795
Epoch: 7001, Batch Gradient Norm: 23.594914089042653
Epoch: 7001, Batch Gradient Norm after: 22.33970988597687
Epoch 7002/10000, Prediction Accuracy = 59.902%, Loss = 0.7289638042449951
Epoch: 7002, Batch Gradient Norm: 24.33029260179077
Epoch: 7002, Batch Gradient Norm after: 22.360675838065024
Epoch 7003/10000, Prediction Accuracy = 59.92%, Loss = 0.7309518098831177
Epoch: 7003, Batch Gradient Norm: 23.608410964873066
Epoch: 7003, Batch Gradient Norm after: 22.359150101314608
Epoch 7004/10000, Prediction Accuracy = 59.936%, Loss = 0.7288987278938294
Epoch: 7004, Batch Gradient Norm: 24.305044371631045
Epoch: 7004, Batch Gradient Norm after: 22.36068006504108
Epoch 7005/10000, Prediction Accuracy = 59.926%, Loss = 0.730802035331726
Epoch: 7005, Batch Gradient Norm: 23.57871492829773
Epoch: 7005, Batch Gradient Norm after: 22.333351401454554
Epoch 7006/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.728782844543457
Epoch: 7006, Batch Gradient Norm: 24.33905197458902
Epoch: 7006, Batch Gradient Norm after: 22.36067860639509
Epoch 7007/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7309110403060913
Epoch: 7007, Batch Gradient Norm: 23.590603454663633
Epoch: 7007, Batch Gradient Norm after: 22.360678917086663
Epoch 7008/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7288058757781982
Epoch: 7008, Batch Gradient Norm: 24.302345695405204
Epoch: 7008, Batch Gradient Norm after: 22.360679351799792
Epoch 7009/10000, Prediction Accuracy = 59.948%, Loss = 0.7307498455047607
Epoch: 7009, Batch Gradient Norm: 23.57230518025087
Epoch: 7009, Batch Gradient Norm after: 22.34246653079691
Epoch 7010/10000, Prediction Accuracy = 59.952%, Loss = 0.7286287903785705
Epoch: 7010, Batch Gradient Norm: 24.319640064544874
Epoch: 7010, Batch Gradient Norm after: 22.360677847713003
Epoch 7011/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7306457161903381
Epoch: 7011, Batch Gradient Norm: 23.58648793593673
Epoch: 7011, Batch Gradient Norm after: 22.35759124188545
Epoch 7012/10000, Prediction Accuracy = 59.908%, Loss = 0.7286053776741028
Epoch: 7012, Batch Gradient Norm: 24.292359820867013
Epoch: 7012, Batch Gradient Norm after: 22.360676902976554
Epoch 7013/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.7305346131324768
Epoch: 7013, Batch Gradient Norm: 23.560394506154367
Epoch: 7013, Batch Gradient Norm after: 22.336167074959565
Epoch 7014/10000, Prediction Accuracy = 59.898%, Loss = 0.7284581542015076
Epoch: 7014, Batch Gradient Norm: 24.319885423656007
Epoch: 7014, Batch Gradient Norm after: 22.3606778590644
Epoch 7015/10000, Prediction Accuracy = 59.928%, Loss = 0.7305048108100891
Epoch: 7015, Batch Gradient Norm: 23.577496035493606
Epoch: 7015, Batch Gradient Norm after: 22.36067662872642
Epoch 7016/10000, Prediction Accuracy = 59.964%, Loss = 0.7284295916557312
Epoch: 7016, Batch Gradient Norm: 24.286670609741897
Epoch: 7016, Batch Gradient Norm after: 22.360677933200957
Epoch 7017/10000, Prediction Accuracy = 59.95%, Loss = 0.7304076313972473
Epoch: 7017, Batch Gradient Norm: 23.55023616715455
Epoch: 7017, Batch Gradient Norm after: 22.33489372930352
Epoch 7018/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7283467173576355
Epoch: 7018, Batch Gradient Norm: 24.31550249407499
Epoch: 7018, Batch Gradient Norm after: 22.3606772187049
Epoch 7019/10000, Prediction Accuracy = 59.968%, Loss = 0.7304565906524658
Epoch: 7019, Batch Gradient Norm: 23.56448537817784
Epoch: 7019, Batch Gradient Norm after: 22.360678877483142
Epoch 7020/10000, Prediction Accuracy = 59.962%, Loss = 0.7282844305038452
Epoch: 7020, Batch Gradient Norm: 24.282919962145623
Epoch: 7020, Batch Gradient Norm after: 22.360680884908884
Epoch 7021/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.7302364110946655
Epoch: 7021, Batch Gradient Norm: 23.544505294603855
Epoch: 7021, Batch Gradient Norm after: 22.338143824582453
Epoch 7022/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7281396150588989
Epoch: 7022, Batch Gradient Norm: 24.304623510107355
Epoch: 7022, Batch Gradient Norm after: 22.36067810802089
Epoch 7023/10000, Prediction Accuracy = 59.948%, Loss = 0.7302212476730346
Epoch: 7023, Batch Gradient Norm: 23.556312369875073
Epoch: 7023, Batch Gradient Norm after: 22.3586441763864
Epoch 7024/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.7281322240829468
Epoch: 7024, Batch Gradient Norm: 24.27259739180172
Epoch: 7024, Batch Gradient Norm after: 22.360678334757434
Epoch 7025/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.730081069469452
Epoch: 7025, Batch Gradient Norm: 23.530056032346852
Epoch: 7025, Batch Gradient Norm after: 22.335562252898196
Epoch 7026/10000, Prediction Accuracy = 59.964%, Loss = 0.7279725909233093
Epoch: 7026, Batch Gradient Norm: 24.30254610372113
Epoch: 7026, Batch Gradient Norm after: 22.360679823583123
Epoch 7027/10000, Prediction Accuracy = 59.952%, Loss = 0.7300802826881408
Epoch: 7027, Batch Gradient Norm: 23.55034954532528
Epoch: 7027, Batch Gradient Norm after: 22.360679907922155
Epoch 7028/10000, Prediction Accuracy = 59.95%, Loss = 0.7279988646507263
Epoch: 7028, Batch Gradient Norm: 24.267213819221535
Epoch: 7028, Batch Gradient Norm after: 22.360677150841717
Epoch 7029/10000, Prediction Accuracy = 59.952%, Loss = 0.730032742023468
Epoch: 7029, Batch Gradient Norm: 23.518834823899592
Epoch: 7029, Batch Gradient Norm after: 22.336427343833016
Epoch 7030/10000, Prediction Accuracy = 59.95%, Loss = 0.7278972029685974
Epoch: 7030, Batch Gradient Norm: 24.29069215503724
Epoch: 7030, Batch Gradient Norm after: 22.360677196043124
Epoch 7031/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.7299854159355164
Epoch: 7031, Batch Gradient Norm: 23.536586922237106
Epoch: 7031, Batch Gradient Norm after: 22.360677893879846
Epoch 7032/10000, Prediction Accuracy = 59.962%, Loss = 0.7278084039688111
Epoch: 7032, Batch Gradient Norm: 24.26180822813804
Epoch: 7032, Batch Gradient Norm after: 22.360676525317892
Epoch 7033/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.729789674282074
Epoch: 7033, Batch Gradient Norm: 23.512877755422014
Epoch: 7033, Batch Gradient Norm after: 22.336662571296102
Epoch 7034/10000, Prediction Accuracy = 59.907999999999994%, Loss = 0.7277063250541687
Epoch: 7034, Batch Gradient Norm: 24.284192777811445
Epoch: 7034, Batch Gradient Norm after: 22.36067785739329
Epoch 7035/10000, Prediction Accuracy = 59.902%, Loss = 0.7298070192337036
Epoch: 7035, Batch Gradient Norm: 23.524107791343383
Epoch: 7035, Batch Gradient Norm after: 22.35918944229713
Epoch 7036/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7276524305343628
Epoch: 7036, Batch Gradient Norm: 24.25489282181705
Epoch: 7036, Batch Gradient Norm after: 22.360677405525074
Epoch 7037/10000, Prediction Accuracy = 59.972%, Loss = 0.729634141921997
Epoch: 7037, Batch Gradient Norm: 23.503527660404565
Epoch: 7037, Batch Gradient Norm after: 22.33708791522631
Epoch 7038/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7275255799293519
Epoch: 7038, Batch Gradient Norm: 24.28407424219423
Epoch: 7038, Batch Gradient Norm after: 22.360677234870614
Epoch 7039/10000, Prediction Accuracy = 59.932%, Loss = 0.7297055840492248
Epoch: 7039, Batch Gradient Norm: 23.516112524812005
Epoch: 7039, Batch Gradient Norm after: 22.360679232888856
Epoch 7040/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7275453686714173
Epoch: 7040, Batch Gradient Norm: 24.250455570678948
Epoch: 7040, Batch Gradient Norm after: 22.360677045210743
Epoch 7041/10000, Prediction Accuracy = 59.956%, Loss = 0.7295904517173767
Epoch: 7041, Batch Gradient Norm: 23.495444718660934
Epoch: 7041, Batch Gradient Norm after: 22.342956226315508
Epoch 7042/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7273916006088257
Epoch: 7042, Batch Gradient Norm: 24.267112654687654
Epoch: 7042, Batch Gradient Norm after: 22.360680154899352
Epoch 7043/10000, Prediction Accuracy = 59.93399999999999%, Loss = 0.7294857263565063
Epoch: 7043, Batch Gradient Norm: 23.506763030647544
Epoch: 7043, Batch Gradient Norm after: 22.357991556747532
Epoch 7044/10000, Prediction Accuracy = 59.926%, Loss = 0.7273450016975402
Epoch: 7044, Batch Gradient Norm: 24.241636335993494
Epoch: 7044, Batch Gradient Norm after: 22.36067802913755
Epoch 7045/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7293621659278869
Epoch: 7045, Batch Gradient Norm: 23.479744354517496
Epoch: 7045, Batch Gradient Norm after: 22.33564486744315
Epoch 7046/10000, Prediction Accuracy = 59.926%, Loss = 0.7272161960601806
Epoch: 7046, Batch Gradient Norm: 24.266182750492003
Epoch: 7046, Batch Gradient Norm after: 22.360676993360425
Epoch 7047/10000, Prediction Accuracy = 59.948%, Loss = 0.7293514847755432
Epoch: 7047, Batch Gradient Norm: 23.49870201909517
Epoch: 7047, Batch Gradient Norm after: 22.36067817765128
Epoch 7048/10000, Prediction Accuracy = 59.988%, Loss = 0.7271786570549011
Epoch: 7048, Batch Gradient Norm: 24.234004513502413
Epoch: 7048, Batch Gradient Norm after: 22.360678844860434
Epoch 7049/10000, Prediction Accuracy = 59.944%, Loss = 0.7292255163192749
Epoch: 7049, Batch Gradient Norm: 23.476685760905852
Epoch: 7049, Batch Gradient Norm after: 22.33734847875725
Epoch 7050/10000, Prediction Accuracy = 59.96%, Loss = 0.7271022200584412
Epoch: 7050, Batch Gradient Norm: 24.258533732493646
Epoch: 7050, Batch Gradient Norm after: 22.36067790172828
Epoch 7051/10000, Prediction Accuracy = 59.988000000000014%, Loss = 0.7293045878410339
Epoch: 7051, Batch Gradient Norm: 23.487465499689783
Epoch: 7051, Batch Gradient Norm after: 22.360678165295944
Epoch 7052/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7270871877670289
Epoch: 7052, Batch Gradient Norm: 24.223947668497757
Epoch: 7052, Batch Gradient Norm after: 22.36068039170712
Epoch 7053/10000, Prediction Accuracy = 59.926%, Loss = 0.7290922403335571
Epoch: 7053, Batch Gradient Norm: 23.461458089592277
Epoch: 7053, Batch Gradient Norm after: 22.33597749746507
Epoch 7054/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7268978953361511
Epoch: 7054, Batch Gradient Norm: 24.252218848925043
Epoch: 7054, Batch Gradient Norm after: 22.360678197265084
Epoch 7055/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.7290654897689819
Epoch: 7055, Batch Gradient Norm: 23.481303533126937
Epoch: 7055, Batch Gradient Norm after: 22.36067662867335
Epoch 7056/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7269202828407287
Epoch: 7056, Batch Gradient Norm: 24.211538599119137
Epoch: 7056, Batch Gradient Norm after: 22.360678128289926
Epoch 7057/10000, Prediction Accuracy = 59.9%, Loss = 0.7289198994636535
Epoch: 7057, Batch Gradient Norm: 23.44614713679998
Epoch: 7057, Batch Gradient Norm after: 22.33078919242226
Epoch 7058/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.7267261981964112
Epoch: 7058, Batch Gradient Norm: 24.254930289257658
Epoch: 7058, Batch Gradient Norm after: 22.3606802421769
Epoch 7059/10000, Prediction Accuracy = 59.968%, Loss = 0.7289227366447448
Epoch: 7059, Batch Gradient Norm: 23.46729574912087
Epoch: 7059, Batch Gradient Norm after: 22.36067798919412
Epoch 7060/10000, Prediction Accuracy = 59.964%, Loss = 0.7267324805259705
Epoch: 7060, Batch Gradient Norm: 24.221859932034253
Epoch: 7060, Batch Gradient Norm after: 22.360679293453927
Epoch 7061/10000, Prediction Accuracy = 59.964%, Loss = 0.728864312171936
Epoch: 7061, Batch Gradient Norm: 23.447754257990216
Epoch: 7061, Batch Gradient Norm after: 22.34191174198224
Epoch 7062/10000, Prediction Accuracy = 59.96%, Loss = 0.7266748547554016
Epoch: 7062, Batch Gradient Norm: 24.231516278879024
Epoch: 7062, Batch Gradient Norm after: 22.360675633712084
Epoch 7063/10000, Prediction Accuracy = 59.96%, Loss = 0.728814160823822
Epoch: 7063, Batch Gradient Norm: 23.457159230145994
Epoch: 7063, Batch Gradient Norm after: 22.35748518242047
Epoch 7064/10000, Prediction Accuracy = 59.982000000000006%, Loss = 0.7265773177146911
Epoch: 7064, Batch Gradient Norm: 24.207619439607836
Epoch: 7064, Batch Gradient Norm after: 22.360678792619183
Epoch 7065/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7286192893981933
Epoch: 7065, Batch Gradient Norm: 23.43695630532673
Epoch: 7065, Batch Gradient Norm after: 22.33843304435672
Epoch 7066/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7264570355415344
Epoch: 7066, Batch Gradient Norm: 24.22968267053758
Epoch: 7066, Batch Gradient Norm after: 22.360678636071107
Epoch 7067/10000, Prediction Accuracy = 59.932%, Loss = 0.7286247253417969
Epoch: 7067, Batch Gradient Norm: 23.451358052704183
Epoch: 7067, Batch Gradient Norm after: 22.360563944873128
Epoch 7068/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.7264356970787048
Epoch: 7068, Batch Gradient Norm: 24.19592135905276
Epoch: 7068, Batch Gradient Norm after: 22.36068038156776
Epoch 7069/10000, Prediction Accuracy = 59.968%, Loss = 0.7284590601921082
Epoch: 7069, Batch Gradient Norm: 23.4233271346123
Epoch: 7069, Batch Gradient Norm after: 22.32823312748401
Epoch 7070/10000, Prediction Accuracy = 60.00200000000001%, Loss = 0.7262728214263916
Epoch: 7070, Batch Gradient Norm: 24.223812682452294
Epoch: 7070, Batch Gradient Norm after: 22.360677708060734
Epoch 7071/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7284857153892517
Epoch: 7071, Batch Gradient Norm: 23.447476841196814
Epoch: 7071, Batch Gradient Norm after: 22.359774255910484
Epoch 7072/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7263270020484924
Epoch: 7072, Batch Gradient Norm: 24.192845832646924
Epoch: 7072, Batch Gradient Norm after: 22.36067901375982
Epoch 7073/10000, Prediction Accuracy = 59.992%, Loss = 0.7284179925918579
Epoch: 7073, Batch Gradient Norm: 23.416660347274075
Epoch: 7073, Batch Gradient Norm after: 22.335905586114794
Epoch 7074/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7261749148368836
Epoch: 7074, Batch Gradient Norm: 24.211776922236847
Epoch: 7074, Batch Gradient Norm after: 22.36067867774991
Epoch 7075/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.7283217906951904
Epoch: 7075, Batch Gradient Norm: 23.432149667242864
Epoch: 7075, Batch Gradient Norm after: 22.35797574405068
Epoch 7076/10000, Prediction Accuracy = 59.96%, Loss = 0.7261127591133117
Epoch: 7076, Batch Gradient Norm: 24.187461700891564
Epoch: 7076, Batch Gradient Norm after: 22.360676129438698
Epoch 7077/10000, Prediction Accuracy = 59.964%, Loss = 0.7281866073608398
Epoch: 7077, Batch Gradient Norm: 23.408261013432238
Epoch: 7077, Batch Gradient Norm after: 22.328861286913327
Epoch 7078/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.7260072469711304
Epoch: 7078, Batch Gradient Norm: 24.191249783802064
Epoch: 7078, Batch Gradient Norm after: 22.360679560030444
Epoch 7079/10000, Prediction Accuracy = 59.93000000000001%, Loss = 0.7281349420547485
Epoch: 7079, Batch Gradient Norm: 23.412278535937283
Epoch: 7079, Batch Gradient Norm after: 22.343241890889733
Epoch 7080/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7259174585342407
Epoch: 7080, Batch Gradient Norm: 24.203914565271788
Epoch: 7080, Batch Gradient Norm after: 22.360679022542207
Epoch 7081/10000, Prediction Accuracy = 59.988%, Loss = 0.728089439868927
Epoch: 7081, Batch Gradient Norm: 23.421167352961728
Epoch: 7081, Batch Gradient Norm after: 22.355890347787405
Epoch 7082/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7259168982505798
Epoch: 7082, Batch Gradient Norm: 24.182351815138134
Epoch: 7082, Batch Gradient Norm after: 22.36067890437291
Epoch 7083/10000, Prediction Accuracy = 59.988%, Loss = 0.7280738353729248
Epoch: 7083, Batch Gradient Norm: 23.394224767226635
Epoch: 7083, Batch Gradient Norm after: 22.329868383130304
Epoch 7084/10000, Prediction Accuracy = 59.964%, Loss = 0.7258252024650573
Epoch: 7084, Batch Gradient Norm: 24.182453448762512
Epoch: 7084, Batch Gradient Norm after: 22.36067834653859
Epoch 7085/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.727968180179596
Epoch: 7085, Batch Gradient Norm: 23.39740123504459
Epoch: 7085, Batch Gradient Norm after: 22.343496406875786
Epoch 7086/10000, Prediction Accuracy = 59.962%, Loss = 0.7256974697113037
Epoch: 7086, Batch Gradient Norm: 24.18749939846456
Epoch: 7086, Batch Gradient Norm after: 22.360677411031162
Epoch 7087/10000, Prediction Accuracy = 59.996%, Loss = 0.727851140499115
Epoch: 7087, Batch Gradient Norm: 23.39729045736587
Epoch: 7087, Batch Gradient Norm after: 22.346365228460396
Epoch 7088/10000, Prediction Accuracy = 59.936%, Loss = 0.7256553530693054
Epoch: 7088, Batch Gradient Norm: 24.180814989822483
Epoch: 7088, Batch Gradient Norm after: 22.36067820088737
Epoch 7089/10000, Prediction Accuracy = 59.926%, Loss = 0.7277919888496399
Epoch: 7089, Batch Gradient Norm: 23.390444797734297
Epoch: 7089, Batch Gradient Norm after: 22.34233495576828
Epoch 7090/10000, Prediction Accuracy = 59.93800000000001%, Loss = 0.7255503535270691
Epoch: 7090, Batch Gradient Norm: 24.17312513337319
Epoch: 7090, Batch Gradient Norm after: 22.360677862724906
Epoch 7091/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.7276848912239074
Epoch: 7091, Batch Gradient Norm: 23.382631681307146
Epoch: 7091, Batch Gradient Norm after: 22.331952629401695
Epoch 7092/10000, Prediction Accuracy = 59.984%, Loss = 0.7254659414291382
Epoch: 7092, Batch Gradient Norm: 24.16770891538423
Epoch: 7092, Batch Gradient Norm after: 22.360678987840046
Epoch 7093/10000, Prediction Accuracy = 59.976%, Loss = 0.7276705026626586
Epoch: 7093, Batch Gradient Norm: 23.375162687985494
Epoch: 7093, Batch Gradient Norm after: 22.326814350101767
Epoch 7094/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7254420995712281
Epoch: 7094, Batch Gradient Norm: 24.15352994898907
Epoch: 7094, Batch Gradient Norm after: 22.36067733249954
Epoch 7095/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7275979161262512
Epoch: 7095, Batch Gradient Norm: 23.362043127982773
Epoch: 7095, Batch Gradient Norm after: 22.30827617501881
Epoch 7096/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7252978563308716
Epoch: 7096, Batch Gradient Norm: 24.13879272582391
Epoch: 7096, Batch Gradient Norm after: 22.36067697525365
Epoch 7097/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.7274213075637818
Epoch: 7097, Batch Gradient Norm: 23.35245513373853
Epoch: 7097, Batch Gradient Norm after: 22.28418971096313
Epoch 7098/10000, Prediction Accuracy = 59.94799999999999%, Loss = 0.7251935839653015
Epoch: 7098, Batch Gradient Norm: 24.12090143175782
Epoch: 7098, Batch Gradient Norm after: 22.360676751551708
Epoch 7099/10000, Prediction Accuracy = 59.967999999999996%, Loss = 0.7273187875747681
Epoch: 7099, Batch Gradient Norm: 23.328791215411123
Epoch: 7099, Batch Gradient Norm after: 22.23926140676237
Epoch 7100/10000, Prediction Accuracy = 59.924%, Loss = 0.7250868439674377
Epoch: 7100, Batch Gradient Norm: 24.091445267174525
Epoch: 7100, Batch Gradient Norm after: 22.3606769802482
Epoch 7101/10000, Prediction Accuracy = 59.964%, Loss = 0.7271720409393311
Epoch: 7101, Batch Gradient Norm: 23.311880200834988
Epoch: 7101, Batch Gradient Norm after: 22.191642405998973
Epoch 7102/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7249464273452759
Epoch: 7102, Batch Gradient Norm: 24.066463323569135
Epoch: 7102, Batch Gradient Norm after: 22.360678939585252
Epoch 7103/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.7270744442939758
Epoch: 7103, Batch Gradient Norm: 23.29412434982704
Epoch: 7103, Batch Gradient Norm after: 22.142707438797746
Epoch 7104/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7248970866203308
Epoch: 7104, Batch Gradient Norm: 24.044137256961918
Epoch: 7104, Batch Gradient Norm after: 22.360678939476706
Epoch 7105/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.7270424604415894
Epoch: 7105, Batch Gradient Norm: 23.275287703438273
Epoch: 7105, Batch Gradient Norm after: 22.09514295830988
Epoch 7106/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.7247940301895142
Epoch: 7106, Batch Gradient Norm: 24.027155868150285
Epoch: 7106, Batch Gradient Norm after: 22.337180586899702
Epoch 7107/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.726845645904541
Epoch: 7107, Batch Gradient Norm: 23.3039675385806
Epoch: 7107, Batch Gradient Norm after: 22.148945353030218
Epoch 7108/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7247479319572449
Epoch: 7108, Batch Gradient Norm: 24.062989382791393
Epoch: 7108, Batch Gradient Norm after: 22.360680325213178
Epoch 7109/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7268561482429504
Epoch: 7109, Batch Gradient Norm: 23.29260671841933
Epoch: 7109, Batch Gradient Norm after: 22.114100311068228
Epoch 7110/10000, Prediction Accuracy = 59.938%, Loss = 0.7246864914894104
Epoch: 7110, Batch Gradient Norm: 24.04490991090306
Epoch: 7110, Batch Gradient Norm after: 22.355732813148645
Epoch 7111/10000, Prediction Accuracy = 59.952%, Loss = 0.7267630219459533
Epoch: 7111, Batch Gradient Norm: 23.28799495530905
Epoch: 7111, Batch Gradient Norm after: 22.096837200254146
Epoch 7112/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.7245738625526428
Epoch: 7112, Batch Gradient Norm: 24.047611675805793
Epoch: 7112, Batch Gradient Norm after: 22.349362339378132
Epoch 7113/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7266684889793396
Epoch: 7113, Batch Gradient Norm: 23.30760646755878
Epoch: 7113, Batch Gradient Norm after: 22.12840327523593
Epoch 7114/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.7245856761932373
Epoch: 7114, Batch Gradient Norm: 24.070153025413454
Epoch: 7114, Batch Gradient Norm after: 22.36067950251771
Epoch 7115/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7267419219017028
Epoch: 7115, Batch Gradient Norm: 23.304281523505445
Epoch: 7115, Batch Gradient Norm after: 22.119555937217616
Epoch 7116/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7245607733726501
Epoch: 7116, Batch Gradient Norm: 24.06182180553912
Epoch: 7116, Batch Gradient Norm after: 22.360678658107783
Epoch 7117/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7266672015190124
Epoch: 7117, Batch Gradient Norm: 23.29854418568259
Epoch: 7117, Batch Gradient Norm after: 22.11242806076012
Epoch 7118/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7244303703308106
Epoch: 7118, Batch Gradient Norm: 24.059707806365097
Epoch: 7118, Batch Gradient Norm after: 22.360678296051873
Epoch 7119/10000, Prediction Accuracy = 59.952%, Loss = 0.7265232563018799
Epoch: 7119, Batch Gradient Norm: 23.297468019091436
Epoch: 7119, Batch Gradient Norm after: 22.09969062394348
Epoch 7120/10000, Prediction Accuracy = 59.93000000000001%, Loss = 0.724365746974945
Epoch: 7120, Batch Gradient Norm: 24.057633884285682
Epoch: 7120, Batch Gradient Norm after: 22.358521461256043
Epoch 7121/10000, Prediction Accuracy = 59.976%, Loss = 0.7264700412750245
Epoch: 7121, Batch Gradient Norm: 23.293930837075692
Epoch: 7121, Batch Gradient Norm after: 22.094978676997403
Epoch 7122/10000, Prediction Accuracy = 59.92%, Loss = 0.7242960691452026
Epoch: 7122, Batch Gradient Norm: 24.056273510851003
Epoch: 7122, Batch Gradient Norm after: 22.355519348477483
Epoch 7123/10000, Prediction Accuracy = 59.968%, Loss = 0.7263794898986816
Epoch: 7123, Batch Gradient Norm: 23.304690663919914
Epoch: 7123, Batch Gradient Norm after: 22.10941544014431
Epoch 7124/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.7242297887802124
Epoch: 7124, Batch Gradient Norm: 24.065659712184516
Epoch: 7124, Batch Gradient Norm after: 22.360679578046977
Epoch 7125/10000, Prediction Accuracy = 59.964%, Loss = 0.7263838052749634
Epoch: 7125, Batch Gradient Norm: 23.30761384052336
Epoch: 7125, Batch Gradient Norm after: 22.11422423119984
Epoch 7126/10000, Prediction Accuracy = 60.008%, Loss = 0.7242393016815185
Epoch: 7126, Batch Gradient Norm: 24.06905970517777
Epoch: 7126, Batch Gradient Norm after: 22.360677036765768
Epoch 7127/10000, Prediction Accuracy = 59.988%, Loss = 0.7263868570327758
Epoch: 7127, Batch Gradient Norm: 23.30662692262336
Epoch: 7127, Batch Gradient Norm after: 22.122857501201743
Epoch 7128/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7241525888442993
Epoch: 7128, Batch Gradient Norm: 24.066432227355968
Epoch: 7128, Batch Gradient Norm after: 22.360675723310738
Epoch 7129/10000, Prediction Accuracy = 59.964%, Loss = 0.7262461304664611
Epoch: 7129, Batch Gradient Norm: 23.30628394590555
Epoch: 7129, Batch Gradient Norm after: 22.124638151003044
Epoch 7130/10000, Prediction Accuracy = 59.952%, Loss = 0.7240563035011292
Epoch: 7130, Batch Gradient Norm: 24.06790011718959
Epoch: 7130, Batch Gradient Norm after: 22.360676956532508
Epoch 7131/10000, Prediction Accuracy = 59.972%, Loss = 0.7261824607849121
Epoch: 7131, Batch Gradient Norm: 23.304304368149413
Epoch: 7131, Batch Gradient Norm after: 22.125126266720514
Epoch 7132/10000, Prediction Accuracy = 59.932%, Loss = 0.7240151762962341
Epoch: 7132, Batch Gradient Norm: 24.059323716348494
Epoch: 7132, Batch Gradient Norm after: 22.360678726853617
Epoch 7133/10000, Prediction Accuracy = 59.976%, Loss = 0.7261043787002563
Epoch: 7133, Batch Gradient Norm: 23.299269161161806
Epoch: 7133, Batch Gradient Norm after: 22.116757325602272
Epoch 7134/10000, Prediction Accuracy = 59.964%, Loss = 0.7239024639129639
Epoch: 7134, Batch Gradient Norm: 24.056165268050712
Epoch: 7134, Batch Gradient Norm after: 22.360680739742495
Epoch 7135/10000, Prediction Accuracy = 59.984%, Loss = 0.7260245561599732
Epoch: 7135, Batch Gradient Norm: 23.294316348583095
Epoch: 7135, Batch Gradient Norm after: 22.110063859381818
Epoch 7136/10000, Prediction Accuracy = 59.988%, Loss = 0.7238671779632568
Epoch: 7136, Batch Gradient Norm: 24.051556438208355
Epoch: 7136, Batch Gradient Norm after: 22.360677817014377
Epoch 7137/10000, Prediction Accuracy = 59.988%, Loss = 0.7260289907455444
Epoch: 7137, Batch Gradient Norm: 23.285377431931245
Epoch: 7137, Batch Gradient Norm after: 22.101921287634237
Epoch 7138/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7238203525543213
Epoch: 7138, Batch Gradient Norm: 24.03826141711653
Epoch: 7138, Batch Gradient Norm after: 22.357780464999244
Epoch 7139/10000, Prediction Accuracy = 59.984%, Loss = 0.7259068965911866
Epoch: 7139, Batch Gradient Norm: 23.285751522156794
Epoch: 7139, Batch Gradient Norm after: 22.100164862475026
Epoch 7140/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.7236849904060364
Epoch: 7140, Batch Gradient Norm: 24.03468378205751
Epoch: 7140, Batch Gradient Norm after: 22.35299570299711
Epoch 7141/10000, Prediction Accuracy = 59.976%, Loss = 0.7257825136184692
Epoch: 7141, Batch Gradient Norm: 23.286451713131687
Epoch: 7141, Batch Gradient Norm after: 22.1049130001734
Epoch 7142/10000, Prediction Accuracy = 59.94%, Loss = 0.7236516833305359
Epoch: 7142, Batch Gradient Norm: 24.032418077229522
Epoch: 7142, Batch Gradient Norm after: 22.354722470838098
Epoch 7143/10000, Prediction Accuracy = 59.976%, Loss = 0.7257413387298584
Epoch: 7143, Batch Gradient Norm: 23.279368597792516
Epoch: 7143, Batch Gradient Norm after: 22.095652846917115
Epoch 7144/10000, Prediction Accuracy = 59.944%, Loss = 0.72355055809021
Epoch: 7144, Batch Gradient Norm: 24.03058637917614
Epoch: 7144, Batch Gradient Norm after: 22.349048534492983
Epoch 7145/10000, Prediction Accuracy = 59.968%, Loss = 0.7256273031234741
Epoch: 7145, Batch Gradient Norm: 23.288264756625136
Epoch: 7145, Batch Gradient Norm after: 22.118742291244583
Epoch 7146/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7235112071037293
Epoch: 7146, Batch Gradient Norm: 24.041076748157483
Epoch: 7146, Batch Gradient Norm after: 22.360679593522377
Epoch 7147/10000, Prediction Accuracy = 59.964%, Loss = 0.7256618976593018
Epoch: 7147, Batch Gradient Norm: 23.27411771992868
Epoch: 7147, Batch Gradient Norm after: 22.09422404644692
Epoch 7148/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.7234755516052246
Epoch: 7148, Batch Gradient Norm: 24.021784755558926
Epoch: 7148, Batch Gradient Norm after: 22.34968634151476
Epoch 7149/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7255824208259583
Epoch: 7149, Batch Gradient Norm: 23.27658980311415
Epoch: 7149, Batch Gradient Norm after: 22.110104792873162
Epoch 7150/10000, Prediction Accuracy = 60.0%, Loss = 0.7233710646629333
Epoch: 7150, Batch Gradient Norm: 24.023282300582267
Epoch: 7150, Batch Gradient Norm after: 22.354196669318743
Epoch 7151/10000, Prediction Accuracy = 59.94599999999999%, Loss = 0.725439441204071
Epoch: 7151, Batch Gradient Norm: 23.271842417935808
Epoch: 7151, Batch Gradient Norm after: 22.098207970534496
Epoch 7152/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7232771635055542
Epoch: 7152, Batch Gradient Norm: 24.017693043060422
Epoch: 7152, Batch Gradient Norm after: 22.345216856627534
Epoch 7153/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7253674507141114
Epoch: 7153, Batch Gradient Norm: 23.27430506201145
Epoch: 7153, Batch Gradient Norm after: 22.119815993174104
Epoch 7154/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.7232477545738221
Epoch: 7154, Batch Gradient Norm: 24.018045470682164
Epoch: 7154, Batch Gradient Norm after: 22.355489955226396
Epoch 7155/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7253052115440368
Epoch: 7155, Batch Gradient Norm: 23.260181555673903
Epoch: 7155, Batch Gradient Norm after: 22.08825955728215
Epoch 7156/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7231104135513305
Epoch: 7156, Batch Gradient Norm: 24.009935349726497
Epoch: 7156, Batch Gradient Norm after: 22.338100702937346
Epoch 7157/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7252097725868225
Epoch: 7157, Batch Gradient Norm: 23.280855244116452
Epoch: 7157, Batch Gradient Norm after: 22.14097143200941
Epoch 7158/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.7231612205505371
Epoch: 7158, Batch Gradient Norm: 24.028406869758754
Epoch: 7158, Batch Gradient Norm after: 22.3606760775667
Epoch 7159/10000, Prediction Accuracy = 60.012%, Loss = 0.7252915620803833
Epoch: 7159, Batch Gradient Norm: 23.251985803022826
Epoch: 7159, Batch Gradient Norm after: 22.09363284002322
Epoch 7160/10000, Prediction Accuracy = 59.988%, Loss = 0.7230229616165161
Epoch: 7160, Batch Gradient Norm: 23.998276630048785
Epoch: 7160, Batch Gradient Norm after: 22.335453849889372
Epoch 7161/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.7250691890716553
Epoch: 7161, Batch Gradient Norm: 23.272083847904863
Epoch: 7161, Batch Gradient Norm after: 22.142382581700822
Epoch 7162/10000, Prediction Accuracy = 59.968%, Loss = 0.7229585289955139
Epoch: 7162, Batch Gradient Norm: 24.01658778338559
Epoch: 7162, Batch Gradient Norm after: 22.360678215729454
Epoch 7163/10000, Prediction Accuracy = 59.992000000000004%, Loss = 0.725046443939209
Epoch: 7163, Batch Gradient Norm: 23.242439646955646
Epoch: 7163, Batch Gradient Norm after: 22.07789882838357
Epoch 7164/10000, Prediction Accuracy = 59.944%, Loss = 0.7228473663330078
Epoch: 7164, Batch Gradient Norm: 23.981548748551475
Epoch: 7164, Batch Gradient Norm after: 22.319994928893625
Epoch 7165/10000, Prediction Accuracy = 59.984%, Loss = 0.7248865962028503
Epoch: 7165, Batch Gradient Norm: 23.282570087924956
Epoch: 7165, Batch Gradient Norm after: 22.1750538146757
Epoch 7166/10000, Prediction Accuracy = 59.972%, Loss = 0.722853147983551
Epoch: 7166, Batch Gradient Norm: 24.026962754451425
Epoch: 7166, Batch Gradient Norm after: 22.360680513187884
Epoch 7167/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7249257087707519
Epoch: 7167, Batch Gradient Norm: 23.24986545607635
Epoch: 7167, Batch Gradient Norm after: 22.108608530908135
Epoch 7168/10000, Prediction Accuracy = 59.988%, Loss = 0.7227253317832947
Epoch: 7168, Batch Gradient Norm: 23.98821023030764
Epoch: 7168, Batch Gradient Norm after: 22.336396786087978
Epoch 7169/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.724839985370636
Epoch: 7169, Batch Gradient Norm: 23.253653470086707
Epoch: 7169, Batch Gradient Norm after: 22.13028033263655
Epoch 7170/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7227303862571717
Epoch: 7170, Batch Gradient Norm: 23.988801476432634
Epoch: 7170, Batch Gradient Norm after: 22.346736321459918
Epoch 7171/10000, Prediction Accuracy = 60.024%, Loss = 0.7247820973396302
Epoch: 7171, Batch Gradient Norm: 23.236874882458764
Epoch: 7171, Batch Gradient Norm after: 22.101721741114122
Epoch 7172/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7225589275360107
Epoch: 7172, Batch Gradient Norm: 23.974597001680234
Epoch: 7172, Batch Gradient Norm after: 22.326010635702698
Epoch 7173/10000, Prediction Accuracy = 59.952%, Loss = 0.7245965003967285
Epoch: 7173, Batch Gradient Norm: 23.261142479962515
Epoch: 7173, Batch Gradient Norm after: 22.159610493468833
Epoch 7174/10000, Prediction Accuracy = 59.944%, Loss = 0.7225630164146424
Epoch: 7174, Batch Gradient Norm: 23.99511968279235
Epoch: 7174, Batch Gradient Norm after: 22.36067827984886
Epoch 7175/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7246288537979126
Epoch: 7175, Batch Gradient Norm: 23.209966050079338
Epoch: 7175, Batch Gradient Norm after: 22.059125069597794
Epoch 7176/10000, Prediction Accuracy = 59.95%, Loss = 0.7223768830299377
Epoch: 7176, Batch Gradient Norm: 23.947052987470204
Epoch: 7176, Batch Gradient Norm after: 22.294540326028123
Epoch 7177/10000, Prediction Accuracy = 59.988%, Loss = 0.7243859767913818
Epoch: 7177, Batch Gradient Norm: 23.286997670475106
Epoch: 7177, Batch Gradient Norm after: 22.23226866250492
Epoch 7178/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7224934816360473
Epoch: 7178, Batch Gradient Norm: 24.026503316172324
Epoch: 7178, Batch Gradient Norm after: 22.360675925526948
Epoch 7179/10000, Prediction Accuracy = 59.98%, Loss = 0.7245947241783142
Epoch: 7179, Batch Gradient Norm: 23.233556146742195
Epoch: 7179, Batch Gradient Norm after: 22.135812976872504
Epoch 7180/10000, Prediction Accuracy = 59.996%, Loss = 0.7223597049713135
Epoch: 7180, Batch Gradient Norm: 23.96191442754734
Epoch: 7180, Batch Gradient Norm after: 22.33649102724125
Epoch 7181/10000, Prediction Accuracy = 60.02%, Loss = 0.724428129196167
Epoch: 7181, Batch Gradient Norm: 23.21923147115129
Epoch: 7181, Batch Gradient Norm after: 22.116741492858132
Epoch 7182/10000, Prediction Accuracy = 60.008%, Loss = 0.7222429990768433
Epoch: 7182, Batch Gradient Norm: 23.948250650407907
Epoch: 7182, Batch Gradient Norm after: 22.321134206216076
Epoch 7183/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.7242407321929931
Epoch: 7183, Batch Gradient Norm: 23.23804554823621
Epoch: 7183, Batch Gradient Norm after: 22.158401361077367
Epoch 7184/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7221813797950745
Epoch: 7184, Batch Gradient Norm: 23.964090399133536
Epoch: 7184, Batch Gradient Norm after: 22.34572611026773
Epoch 7185/10000, Prediction Accuracy = 60.004%, Loss = 0.7242255330085754
Epoch: 7185, Batch Gradient Norm: 23.20161836215062
Epoch: 7185, Batch Gradient Norm after: 22.086183406284363
Epoch 7186/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7220638871192933
Epoch: 7186, Batch Gradient Norm: 23.92821917719624
Epoch: 7186, Batch Gradient Norm after: 22.296351170326965
Epoch 7187/10000, Prediction Accuracy = 59.992%, Loss = 0.7240519046783447
Epoch: 7187, Batch Gradient Norm: 23.25789168924564
Epoch: 7187, Batch Gradient Norm after: 22.21768553555233
Epoch 7188/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7221029162406921
Epoch: 7188, Batch Gradient Norm: 23.98638559591058
Epoch: 7188, Batch Gradient Norm after: 22.360678538927875
Epoch 7189/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7241536021232605
Epoch: 7189, Batch Gradient Norm: 23.195805051891146
Epoch: 7189, Batch Gradient Norm after: 22.0874486451603
Epoch 7190/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7219175577163697
Epoch: 7190, Batch Gradient Norm: 23.919457372131713
Epoch: 7190, Batch Gradient Norm after: 22.29656881193621
Epoch 7191/10000, Prediction Accuracy = 60.02%, Loss = 0.7239968299865722
Epoch: 7191, Batch Gradient Norm: 23.24669302418581
Epoch: 7191, Batch Gradient Norm after: 22.21583222969255
Epoch 7192/10000, Prediction Accuracy = 59.996%, Loss = 0.722023093700409
Epoch: 7192, Batch Gradient Norm: 23.972152104882042
Epoch: 7192, Batch Gradient Norm after: 22.36067767942437
Epoch 7193/10000, Prediction Accuracy = 60.0%, Loss = 0.7240342020988464
Epoch: 7193, Batch Gradient Norm: 23.17842391283883
Epoch: 7193, Batch Gradient Norm after: 22.07509668301301
Epoch 7194/10000, Prediction Accuracy = 59.982000000000006%, Loss = 0.7217150807380677
Epoch: 7194, Batch Gradient Norm: 23.906320799071757
Epoch: 7194, Batch Gradient Norm after: 22.28287700665738
Epoch 7195/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.723720395565033
Epoch: 7195, Batch Gradient Norm: 23.253711708079425
Epoch: 7195, Batch Gradient Norm after: 22.245094551268647
Epoch 7196/10000, Prediction Accuracy = 59.946000000000005%, Loss = 0.7218648791313171
Epoch: 7196, Batch Gradient Norm: 23.979264256820763
Epoch: 7196, Batch Gradient Norm after: 22.360677085591572
Epoch 7197/10000, Prediction Accuracy = 60.0%, Loss = 0.7238979697227478
Epoch: 7197, Batch Gradient Norm: 23.175585832341195
Epoch: 7197, Batch Gradient Norm after: 22.090328633498405
Epoch 7198/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.7215938568115234
Epoch: 7198, Batch Gradient Norm: 23.899036866274603
Epoch: 7198, Batch Gradient Norm after: 22.28270818308188
Epoch 7199/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7235756039619445
Epoch: 7199, Batch Gradient Norm: 23.246324935530364
Epoch: 7199, Batch Gradient Norm after: 22.245399553095965
Epoch 7200/10000, Prediction Accuracy = 60.008%, Loss = 0.721698796749115
Epoch: 7200, Batch Gradient Norm: 23.967104199183964
Epoch: 7200, Batch Gradient Norm after: 22.36067822631224
Epoch 7201/10000, Prediction Accuracy = 59.984%, Loss = 0.7237746238708496
Epoch: 7201, Batch Gradient Norm: 23.1659404296494
Epoch: 7201, Batch Gradient Norm after: 22.083339796286385
Epoch 7202/10000, Prediction Accuracy = 60.004%, Loss = 0.7214936256408692
Epoch: 7202, Batch Gradient Norm: 23.885221674298393
Epoch: 7202, Batch Gradient Norm after: 22.27807128706849
Epoch 7203/10000, Prediction Accuracy = 60.038%, Loss = 0.7235060930252075
Epoch: 7203, Batch Gradient Norm: 23.234611611255964
Epoch: 7203, Batch Gradient Norm after: 22.250492664231967
Epoch 7204/10000, Prediction Accuracy = 60.0%, Loss = 0.7215749859809876
Epoch: 7204, Batch Gradient Norm: 23.954114044948817
Epoch: 7204, Batch Gradient Norm after: 22.360678328338963
Epoch 7205/10000, Prediction Accuracy = 59.968%, Loss = 0.7235766172409057
Epoch: 7205, Batch Gradient Norm: 23.15124725848366
Epoch: 7205, Batch Gradient Norm after: 22.070083129562317
Epoch 7206/10000, Prediction Accuracy = 59.971999999999994%, Loss = 0.721271300315857
Epoch: 7206, Batch Gradient Norm: 23.87488736142887
Epoch: 7206, Batch Gradient Norm after: 22.264901461347023
Epoch 7207/10000, Prediction Accuracy = 59.99799999999999%, Loss = 0.7232725620269775
Epoch: 7207, Batch Gradient Norm: 23.24006496187546
Epoch: 7207, Batch Gradient Norm after: 22.276991147874668
Epoch 7208/10000, Prediction Accuracy = 59.976%, Loss = 0.7214720249176025
Epoch: 7208, Batch Gradient Norm: 23.95761385435284
Epoch: 7208, Batch Gradient Norm after: 22.3606799297501
Epoch 7209/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7234640836715698
Epoch: 7209, Batch Gradient Norm: 23.14625849552317
Epoch: 7209, Batch Gradient Norm after: 22.08332927289643
Epoch 7210/10000, Prediction Accuracy = 59.988%, Loss = 0.7211261868476868
Epoch: 7210, Batch Gradient Norm: 23.86791464584024
Epoch: 7210, Batch Gradient Norm after: 22.268527865097912
Epoch 7211/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.723128867149353
Epoch: 7211, Batch Gradient Norm: 23.22534622007973
Epoch: 7211, Batch Gradient Norm after: 22.264118922140103
Epoch 7212/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7213267803192138
Epoch: 7212, Batch Gradient Norm: 23.93991301165879
Epoch: 7212, Batch Gradient Norm after: 22.360678901187026
Epoch 7213/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7233842134475708
Epoch: 7213, Batch Gradient Norm: 23.128176692899167
Epoch: 7213, Batch Gradient Norm after: 22.065489317541648
Epoch 7214/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7210211753845215
Epoch: 7214, Batch Gradient Norm: 23.84755450431397
Epoch: 7214, Batch Gradient Norm after: 22.250919195991393
Epoch 7215/10000, Prediction Accuracy = 59.996%, Loss = 0.7229734659194946
Epoch: 7215, Batch Gradient Norm: 23.234010327600227
Epoch: 7215, Batch Gradient Norm after: 22.308856451333423
Epoch 7216/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7211822152137757
Epoch: 7216, Batch Gradient Norm: 23.955835027599587
Epoch: 7216, Batch Gradient Norm after: 22.360679045628437
Epoch 7217/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7232011079788208
Epoch: 7217, Batch Gradient Norm: 23.136343463475793
Epoch: 7217, Batch Gradient Norm after: 22.102813210307723
Epoch 7218/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7208924293518066
Epoch: 7218, Batch Gradient Norm: 23.851038096577128
Epoch: 7218, Batch Gradient Norm after: 22.268033171404888
Epoch 7219/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7228506326675415
Epoch: 7219, Batch Gradient Norm: 23.20253981554067
Epoch: 7219, Batch Gradient Norm after: 22.26181273123889
Epoch 7220/10000, Prediction Accuracy = 59.956%, Loss = 0.7209768056869507
Epoch: 7220, Batch Gradient Norm: 23.9109115939601
Epoch: 7220, Batch Gradient Norm after: 22.36067607791704
Epoch 7221/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7229531884193421
Epoch: 7221, Batch Gradient Norm: 23.095896509695148
Epoch: 7221, Batch Gradient Norm after: 22.025749722226635
Epoch 7222/10000, Prediction Accuracy = 60.016%, Loss = 0.7206485748291016
Epoch: 7222, Batch Gradient Norm: 23.816194799180327
Epoch: 7222, Batch Gradient Norm after: 22.22149579617738
Epoch 7223/10000, Prediction Accuracy = 60.024%, Loss = 0.7226719379425048
Epoch: 7223, Batch Gradient Norm: 23.20028313432094
Epoch: 7223, Batch Gradient Norm after: 22.300567378583594
Epoch 7224/10000, Prediction Accuracy = 60.00999999999999%, Loss = 0.7209240078926087
Epoch: 7224, Batch Gradient Norm: 23.914016602549186
Epoch: 7224, Batch Gradient Norm after: 22.360679092214607
Epoch 7225/10000, Prediction Accuracy = 60.016%, Loss = 0.7229185342788697
Epoch: 7225, Batch Gradient Norm: 23.090525620232267
Epoch: 7225, Batch Gradient Norm after: 22.041640959600716
Epoch 7226/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7205130457878113
Epoch: 7226, Batch Gradient Norm: 23.811794336453033
Epoch: 7226, Batch Gradient Norm after: 22.224782167789048
Epoch 7227/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.7224614977836609
Epoch: 7227, Batch Gradient Norm: 23.201128991204758
Epoch: 7227, Batch Gradient Norm after: 22.304840884428643
Epoch 7228/10000, Prediction Accuracy = 59.948%, Loss = 0.7207381844520568
Epoch: 7228, Batch Gradient Norm: 23.90933307774864
Epoch: 7228, Batch Gradient Norm after: 22.36067773760526
Epoch 7229/10000, Prediction Accuracy = 59.996%, Loss = 0.7227331519126892
Epoch: 7229, Batch Gradient Norm: 23.08599200440994
Epoch: 7229, Batch Gradient Norm after: 22.03653196145703
Epoch 7230/10000, Prediction Accuracy = 59.98%, Loss = 0.7203800916671753
Epoch: 7230, Batch Gradient Norm: 23.787468963952218
Epoch: 7230, Batch Gradient Norm after: 22.2104050445682
Epoch 7231/10000, Prediction Accuracy = 59.988%, Loss = 0.7222591161727905
Epoch: 7231, Batch Gradient Norm: 23.201038722811095
Epoch: 7231, Batch Gradient Norm after: 22.308915488107203
Epoch 7232/10000, Prediction Accuracy = 60.012%, Loss = 0.7205944895744324
Epoch: 7232, Batch Gradient Norm: 23.90996638159908
Epoch: 7232, Batch Gradient Norm after: 22.360677341162127
Epoch 7233/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7226124048233032
Epoch: 7233, Batch Gradient Norm: 23.081419145573715
Epoch: 7233, Batch Gradient Norm after: 22.04431344654411
Epoch 7234/10000, Prediction Accuracy = 60.012%, Loss = 0.7202881693840026
Epoch: 7234, Batch Gradient Norm: 23.787782046215447
Epoch: 7234, Batch Gradient Norm after: 22.21824694965954
Epoch 7235/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.7222705125808716
Epoch: 7235, Batch Gradient Norm: 23.189126260662437
Epoch: 7235, Batch Gradient Norm after: 22.306204142022217
Epoch 7236/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7205006241798401
Epoch: 7236, Batch Gradient Norm: 23.896328609969533
Epoch: 7236, Batch Gradient Norm after: 22.36067904078805
Epoch 7237/10000, Prediction Accuracy = 60.0%, Loss = 0.7224438548088074
Epoch: 7237, Batch Gradient Norm: 23.072214109177928
Epoch: 7237, Batch Gradient Norm after: 22.03542983143126
Epoch 7238/10000, Prediction Accuracy = 59.972%, Loss = 0.7200759410858154
Epoch: 7238, Batch Gradient Norm: 23.748264834432696
Epoch: 7238, Batch Gradient Norm after: 22.19246926858087
Epoch 7239/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7219186305999756
Epoch: 7239, Batch Gradient Norm: 23.188258749561864
Epoch: 7239, Batch Gradient Norm after: 22.299980313883797
Epoch 7240/10000, Prediction Accuracy = 60.008%, Loss = 0.7203696250915528
Epoch: 7240, Batch Gradient Norm: 23.890441650835893
Epoch: 7240, Batch Gradient Norm after: 22.36067841654661
Epoch 7241/10000, Prediction Accuracy = 60.004%, Loss = 0.7223141551017761
Epoch: 7241, Batch Gradient Norm: 23.061344240574016
Epoch: 7241, Batch Gradient Norm after: 22.022425614969347
Epoch 7242/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.7199190855026245
Epoch: 7242, Batch Gradient Norm: 23.702621875216494
Epoch: 7242, Batch Gradient Norm after: 22.164733491977852
Epoch 7243/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7216523051261902
Epoch: 7243, Batch Gradient Norm: 23.16923118346782
Epoch: 7243, Batch Gradient Norm after: 22.260500057293033
Epoch 7244/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.720183265209198
Epoch: 7244, Batch Gradient Norm: 23.87086037570794
Epoch: 7244, Batch Gradient Norm after: 22.34975919989038
Epoch 7245/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.722198486328125
Epoch: 7245, Batch Gradient Norm: 23.06265181345068
Epoch: 7245, Batch Gradient Norm after: 22.03286844265083
Epoch 7246/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7198679447174072
Epoch: 7246, Batch Gradient Norm: 23.721380751251406
Epoch: 7246, Batch Gradient Norm after: 22.18287506834082
Epoch 7247/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7216495156288147
Epoch: 7247, Batch Gradient Norm: 23.167229451357887
Epoch: 7247, Batch Gradient Norm after: 22.274784370120067
Epoch 7248/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7200249910354615
Epoch: 7248, Batch Gradient Norm: 23.87203908666148
Epoch: 7248, Batch Gradient Norm after: 22.35313143353729
Epoch 7249/10000, Prediction Accuracy = 60.016%, Loss = 0.7219838857650757
Epoch: 7249, Batch Gradient Norm: 23.05616143274982
Epoch: 7249, Batch Gradient Norm after: 22.024499338190193
Epoch 7250/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.719686222076416
Epoch: 7250, Batch Gradient Norm: 23.693760235492075
Epoch: 7250, Batch Gradient Norm after: 22.164545907891554
Epoch 7251/10000, Prediction Accuracy = 59.992%, Loss = 0.7214052319526673
Epoch: 7251, Batch Gradient Norm: 23.152607399335626
Epoch: 7251, Batch Gradient Norm after: 22.24927790097675
Epoch 7252/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7198673963546753
Epoch: 7252, Batch Gradient Norm: 23.857098254074067
Epoch: 7252, Batch Gradient Norm after: 22.33650637245292
Epoch 7253/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7218133807182312
Epoch: 7253, Batch Gradient Norm: 23.066304531874046
Epoch: 7253, Batch Gradient Norm after: 22.06002283383034
Epoch 7254/10000, Prediction Accuracy = 59.996%, Loss = 0.7195677399635315
Epoch: 7254, Batch Gradient Norm: 23.759621901193746
Epoch: 7254, Batch Gradient Norm after: 22.210804453884535
Epoch 7255/10000, Prediction Accuracy = 59.996%, Loss = 0.7215011835098266
Epoch: 7255, Batch Gradient Norm: 23.172164131784122
Epoch: 7255, Batch Gradient Norm after: 22.30635947091814
Epoch 7256/10000, Prediction Accuracy = 60.044%, Loss = 0.7198676586151123
Epoch: 7256, Batch Gradient Norm: 23.868372506076586
Epoch: 7256, Batch Gradient Norm after: 22.36067683779892
Epoch 7257/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7218292355537415
Epoch: 7257, Batch Gradient Norm: 23.028673342037475
Epoch: 7257, Batch Gradient Norm after: 21.99997252017905
Epoch 7258/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7193780541419983
Epoch: 7258, Batch Gradient Norm: 23.60641409477005
Epoch: 7258, Batch Gradient Norm after: 22.11389828498269
Epoch 7259/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7209075093269348
Epoch: 7259, Batch Gradient Norm: 23.11536761560129
Epoch: 7259, Batch Gradient Norm after: 22.17455479961854
Epoch 7260/10000, Prediction Accuracy = 59.983999999999995%, Loss = 0.7195187330245971
Epoch: 7260, Batch Gradient Norm: 23.814991660534375
Epoch: 7260, Batch Gradient Norm after: 22.287449904184104
Epoch 7261/10000, Prediction Accuracy = 59.98%, Loss = 0.7214755773544311
Epoch: 7261, Batch Gradient Norm: 23.10553403194931
Epoch: 7261, Batch Gradient Norm after: 22.17259346336736
Epoch 7262/10000, Prediction Accuracy = 60.01800000000001%, Loss = 0.7194679141044616
Epoch: 7262, Batch Gradient Norm: 23.804941196256106
Epoch: 7262, Batch Gradient Norm after: 22.280975846729046
Epoch 7263/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.721374249458313
Epoch: 7263, Batch Gradient Norm: 23.107329663176834
Epoch: 7263, Batch Gradient Norm after: 22.18991472821309
Epoch 7264/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7193589210510254
Epoch: 7264, Batch Gradient Norm: 23.808874218301774
Epoch: 7264, Batch Gradient Norm after: 22.291470020438023
Epoch 7265/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.7213152527809144
Epoch: 7265, Batch Gradient Norm: 23.09369853551599
Epoch: 7265, Batch Gradient Norm after: 22.164132468042258
Epoch 7266/10000, Prediction Accuracy = 60.052%, Loss = 0.7193210005760193
Epoch: 7266, Batch Gradient Norm: 23.792192946443745
Epoch: 7266, Batch Gradient Norm after: 22.273770687591515
Epoch 7267/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7213120579719543
Epoch: 7267, Batch Gradient Norm: 23.09689202112776
Epoch: 7267, Batch Gradient Norm after: 22.200109617430492
Epoch 7268/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.719291603565216
Epoch: 7268, Batch Gradient Norm: 23.79261163377753
Epoch: 7268, Batch Gradient Norm after: 22.28868680792609
Epoch 7269/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.7211863040924072
Epoch: 7269, Batch Gradient Norm: 23.076948401870826
Epoch: 7269, Batch Gradient Norm after: 22.15857221622345
Epoch 7270/10000, Prediction Accuracy = 59.996%, Loss = 0.7191039443016052
Epoch: 7270, Batch Gradient Norm: 23.777087957202134
Epoch: 7270, Batch Gradient Norm after: 22.262099165658476
Epoch 7271/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7210445523262023
Epoch: 7271, Batch Gradient Norm: 23.107251436708957
Epoch: 7271, Batch Gradient Norm after: 22.234857553013686
Epoch 7272/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7191555500030518
Epoch: 7272, Batch Gradient Norm: 23.794284132767554
Epoch: 7272, Batch Gradient Norm after: 22.30185944909307
Epoch 7273/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7210665702819824
Epoch: 7273, Batch Gradient Norm: 23.050122138031078
Epoch: 7273, Batch Gradient Norm after: 22.122871934869668
Epoch 7274/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7189134240150452
Epoch: 7274, Batch Gradient Norm: 23.722391600203668
Epoch: 7274, Batch Gradient Norm after: 22.21935370666297
Epoch 7275/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.7207430720329284
Epoch: 7275, Batch Gradient Norm: 23.106600412796517
Epoch: 7275, Batch Gradient Norm after: 22.24552562260652
Epoch 7276/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7190050721168518
Epoch: 7276, Batch Gradient Norm: 23.794735197273866
Epoch: 7276, Batch Gradient Norm after: 22.3093244082439
Epoch 7277/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7209858655929565
Epoch: 7277, Batch Gradient Norm: 23.03383221743288
Epoch: 7277, Batch Gradient Norm after: 22.104785397113986
Epoch 7278/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.7188325524330139
Epoch: 7278, Batch Gradient Norm: 23.6685076722379
Epoch: 7278, Batch Gradient Norm after: 22.191848391342294
Epoch 7279/10000, Prediction Accuracy = 60.024%, Loss = 0.7205676794052124
Epoch: 7279, Batch Gradient Norm: 23.074362770413046
Epoch: 7279, Batch Gradient Norm after: 22.195659653027963
Epoch 7280/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7188098192214966
Epoch: 7280, Batch Gradient Norm: 23.76837093997712
Epoch: 7280, Batch Gradient Norm after: 22.274700511740445
Epoch 7281/10000, Prediction Accuracy = 60.00999999999999%, Loss = 0.7207042455673218
Epoch: 7281, Batch Gradient Norm: 23.074102862554167
Epoch: 7281, Batch Gradient Norm after: 22.195417327241582
Epoch 7282/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.7187392115592957
Epoch: 7282, Batch Gradient Norm: 23.761522406599685
Epoch: 7282, Batch Gradient Norm after: 22.27175856512413
Epoch 7283/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7206626892089844
Epoch: 7283, Batch Gradient Norm: 23.066120948206265
Epoch: 7283, Batch Gradient Norm after: 22.197978628010144
Epoch 7284/10000, Prediction Accuracy = 60.008%, Loss = 0.7186813473701477
Epoch: 7284, Batch Gradient Norm: 23.75464809563677
Epoch: 7284, Batch Gradient Norm after: 22.267995900337773
Epoch 7285/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.720548939704895
Epoch: 7285, Batch Gradient Norm: 23.069996557992614
Epoch: 7285, Batch Gradient Norm after: 22.20859151259302
Epoch 7286/10000, Prediction Accuracy = 60.016%, Loss = 0.7185832858085632
Epoch: 7286, Batch Gradient Norm: 23.75772090680581
Epoch: 7286, Batch Gradient Norm after: 22.276210272215064
Epoch 7287/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.7205140709877014
Epoch: 7287, Batch Gradient Norm: 23.05164672038785
Epoch: 7287, Batch Gradient Norm after: 22.183076114265077
Epoch 7288/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7185475111007691
Epoch: 7288, Batch Gradient Norm: 23.739645027836346
Epoch: 7288, Batch Gradient Norm after: 22.25842099455599
Epoch 7289/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7204939484596252
Epoch: 7289, Batch Gradient Norm: 23.064768534778153
Epoch: 7289, Batch Gradient Norm after: 22.228307138838105
Epoch 7290/10000, Prediction Accuracy = 60.064%, Loss = 0.7185098767280579
Epoch: 7290, Batch Gradient Norm: 23.74848723501274
Epoch: 7290, Batch Gradient Norm after: 22.280890769760823
Epoch 7291/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.720367705821991
Epoch: 7291, Batch Gradient Norm: 23.03421752894296
Epoch: 7291, Batch Gradient Norm after: 22.16752288197583
Epoch 7292/10000, Prediction Accuracy = 60.0%, Loss = 0.7183203220367431
Epoch: 7292, Batch Gradient Norm: 23.681386994402406
Epoch: 7292, Batch Gradient Norm after: 22.220307299875778
Epoch 7293/10000, Prediction Accuracy = 59.98599999999999%, Loss = 0.7201067090034485
Epoch: 7293, Batch Gradient Norm: 23.04403848021016
Epoch: 7293, Batch Gradient Norm after: 22.188621677883283
Epoch 7294/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7183142900466919
Epoch: 7294, Batch Gradient Norm: 23.713119071873187
Epoch: 7294, Batch Gradient Norm after: 22.24363540326235
Epoch 7295/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7201383948326111
Epoch: 7295, Batch Gradient Norm: 23.04917483261639
Epoch: 7295, Batch Gradient Norm after: 22.211508540998768
Epoch 7296/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.718233871459961
Epoch: 7296, Batch Gradient Norm: 23.731143774670155
Epoch: 7296, Batch Gradient Norm after: 22.260278228380272
Epoch 7297/10000, Prediction Accuracy = 60.004%, Loss = 0.7201132655143738
Epoch: 7297, Batch Gradient Norm: 23.050455162522308
Epoch: 7297, Batch Gradient Norm after: 22.221534681092063
Epoch 7298/10000, Prediction Accuracy = 60.036%, Loss = 0.7182037472724915
Epoch: 7298, Batch Gradient Norm: 23.731732958486592
Epoch: 7298, Batch Gradient Norm after: 22.268175789273254
Epoch 7299/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7201393127441407
Epoch: 7299, Batch Gradient Norm: 23.02967887295773
Epoch: 7299, Batch Gradient Norm after: 22.195845969418894
Epoch 7300/10000, Prediction Accuracy = 60.056%, Loss = 0.7181429982185363
Epoch: 7300, Batch Gradient Norm: 23.679651048596494
Epoch: 7300, Batch Gradient Norm after: 22.234952823192806
Epoch 7301/10000, Prediction Accuracy = 60.028%, Loss = 0.71992928981781
Epoch: 7301, Batch Gradient Norm: 23.02122039370056
Epoch: 7301, Batch Gradient Norm after: 22.181466680747228
Epoch 7302/10000, Prediction Accuracy = 60.024%, Loss = 0.7179986715316773
Epoch: 7302, Batch Gradient Norm: 23.64168238258581
Epoch: 7302, Batch Gradient Norm after: 22.207968028982624
Epoch 7303/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7196872591972351
Epoch: 7303, Batch Gradient Norm: 23.00906417957808
Epoch: 7303, Batch Gradient Norm after: 22.14755465106436
Epoch 7304/10000, Prediction Accuracy = 60.0%, Loss = 0.7179163217544555
Epoch: 7304, Batch Gradient Norm: 23.576496195312767
Epoch: 7304, Batch Gradient Norm after: 22.163416128064586
Epoch 7305/10000, Prediction Accuracy = 60.028%, Loss = 0.7194888830184937
Epoch: 7305, Batch Gradient Norm: 22.984719464260024
Epoch: 7305, Batch Gradient Norm after: 22.08740843343201
Epoch 7306/10000, Prediction Accuracy = 60.008%, Loss = 0.7178086757659912
Epoch: 7306, Batch Gradient Norm: 23.47305688164787
Epoch: 7306, Batch Gradient Norm after: 22.08842941335114
Epoch 7307/10000, Prediction Accuracy = 59.99000000000001%, Loss = 0.7191087365150451
Epoch: 7307, Batch Gradient Norm: 22.971671113869334
Epoch: 7307, Batch Gradient Norm after: 22.020445393694686
Epoch 7308/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.7176753044128418
Epoch: 7308, Batch Gradient Norm: 23.418827119983035
Epoch: 7308, Batch Gradient Norm after: 22.03860348940043
Epoch 7309/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.7189353466033935
Epoch: 7309, Batch Gradient Norm: 22.970954380687722
Epoch: 7309, Batch Gradient Norm after: 21.988741976169745
Epoch 7310/10000, Prediction Accuracy = 60.068%, Loss = 0.7177108645439148
Epoch: 7310, Batch Gradient Norm: 23.41157054206519
Epoch: 7310, Batch Gradient Norm after: 22.028630964501122
Epoch 7311/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.71892249584198
Epoch: 7311, Batch Gradient Norm: 22.986681296865463
Epoch: 7311, Batch Gradient Norm after: 21.996265920357498
Epoch 7312/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.7176331877708435
Epoch: 7312, Batch Gradient Norm: 23.472500572143616
Epoch: 7312, Batch Gradient Norm after: 22.060359588715514
Epoch 7313/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.7189211130142212
Epoch: 7313, Batch Gradient Norm: 23.022465323288234
Epoch: 7313, Batch Gradient Norm after: 22.05758025521185
Epoch 7314/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.7176406860351563
Epoch: 7314, Batch Gradient Norm: 23.61191618054947
Epoch: 7314, Batch Gradient Norm after: 22.14992046239853
Epoch 7315/10000, Prediction Accuracy = 60.0%, Loss = 0.7192557096481323
Epoch: 7315, Batch Gradient Norm: 23.06958681269696
Epoch: 7315, Batch Gradient Norm after: 22.172343103580943
Epoch 7316/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.717727530002594
Epoch: 7316, Batch Gradient Norm: 23.7566152704649
Epoch: 7316, Batch Gradient Norm after: 22.264465586492257
Epoch 7317/10000, Prediction Accuracy = 59.98599999999999%, Loss = 0.719607698917389
Epoch: 7317, Batch Gradient Norm: 23.08585141242298
Epoch: 7317, Batch Gradient Norm after: 22.23218738788179
Epoch 7318/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7176540613174438
Epoch: 7318, Batch Gradient Norm: 23.765880947789764
Epoch: 7318, Batch Gradient Norm after: 22.294522884858655
Epoch 7319/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.719579529762268
Epoch: 7319, Batch Gradient Norm: 23.0364617414106
Epoch: 7319, Batch Gradient Norm after: 22.15199958012946
Epoch 7320/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7175238847732544
Epoch: 7320, Batch Gradient Norm: 23.691479020347536
Epoch: 7320, Batch Gradient Norm after: 22.228106172864848
Epoch 7321/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.7193809390068054
Epoch: 7321, Batch Gradient Norm: 23.054926168963817
Epoch: 7321, Batch Gradient Norm after: 22.21737116521805
Epoch 7322/10000, Prediction Accuracy = 60.074%, Loss = 0.7175222039222717
Epoch: 7322, Batch Gradient Norm: 23.731486512555158
Epoch: 7322, Batch Gradient Norm after: 22.272363880204118
Epoch 7323/10000, Prediction Accuracy = 60.036%, Loss = 0.7193960666656494
Epoch: 7323, Batch Gradient Norm: 23.033740735395526
Epoch: 7323, Batch Gradient Norm after: 22.19686144205342
Epoch 7324/10000, Prediction Accuracy = 60.032%, Loss = 0.7173643469810486
Epoch: 7324, Batch Gradient Norm: 23.69096972107418
Epoch: 7324, Batch Gradient Norm after: 22.241508168583703
Epoch 7325/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7191788673400878
Epoch: 7325, Batch Gradient Norm: 23.02583621259775
Epoch: 7325, Batch Gradient Norm after: 22.197976900181192
Epoch 7326/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.7173110246658325
Epoch: 7326, Batch Gradient Norm: 23.6680306830476
Epoch: 7326, Batch Gradient Norm after: 22.230948315553096
Epoch 7327/10000, Prediction Accuracy = 60.01800000000001%, Loss = 0.7190786123275756
Epoch: 7327, Batch Gradient Norm: 23.007059212178607
Epoch: 7327, Batch Gradient Norm after: 22.171401087439015
Epoch 7328/10000, Prediction Accuracy = 59.989999999999995%, Loss = 0.7171927452087402
Epoch: 7328, Batch Gradient Norm: 23.600360771871873
Epoch: 7328, Batch Gradient Norm after: 22.187186213253973
Epoch 7329/10000, Prediction Accuracy = 60.032%, Loss = 0.7187949776649475
Epoch: 7329, Batch Gradient Norm: 22.98421470022494
Epoch: 7329, Batch Gradient Norm after: 22.117579997355737
Epoch 7330/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.7170585036277771
Epoch: 7330, Batch Gradient Norm: 23.512277268111227
Epoch: 7330, Batch Gradient Norm after: 22.12588944210741
Epoch 7331/10000, Prediction Accuracy = 60.044%, Loss = 0.7185533046722412
Epoch: 7331, Batch Gradient Norm: 22.958971047613353
Epoch: 7331, Batch Gradient Norm after: 22.048900145869712
Epoch 7332/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7170310139656066
Epoch: 7332, Batch Gradient Norm: 23.4076470611777
Epoch: 7332, Batch Gradient Norm after: 22.054324840917644
Epoch 7333/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7182623505592346
Epoch: 7333, Batch Gradient Norm: 22.885842235753596
Epoch: 7333, Batch Gradient Norm after: 21.94378901613317
Epoch 7334/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.71671382188797
Epoch: 7334, Batch Gradient Norm: 23.44398797429356
Epoch: 7334, Batch Gradient Norm after: 22.06503137246212
Epoch 7335/10000, Prediction Accuracy = 60.02%, Loss = 0.7181724786758423
Epoch: 7335, Batch Gradient Norm: 22.968539520959354
Epoch: 7335, Batch Gradient Norm after: 22.012505194885023
Epoch 7336/10000, Prediction Accuracy = 60.0%, Loss = 0.7168411374092102
Epoch: 7336, Batch Gradient Norm: 23.426181040954788
Epoch: 7336, Batch Gradient Norm after: 22.046463807627912
Epoch 7337/10000, Prediction Accuracy = 60.024%, Loss = 0.7180915117263794
Epoch: 7337, Batch Gradient Norm: 22.96943274753364
Epoch: 7337, Batch Gradient Norm after: 21.998329460216755
Epoch 7338/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7168236255645752
Epoch: 7338, Batch Gradient Norm: 23.418602480129135
Epoch: 7338, Batch Gradient Norm after: 22.036136061301423
Epoch 7339/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.7179945826530456
Epoch: 7339, Batch Gradient Norm: 22.98489539689102
Epoch: 7339, Batch Gradient Norm after: 22.004464764285338
Epoch 7340/10000, Prediction Accuracy = 60.062%, Loss = 0.7167385935783386
Epoch: 7340, Batch Gradient Norm: 23.478594105095773
Epoch: 7340, Batch Gradient Norm after: 22.068706015822222
Epoch 7341/10000, Prediction Accuracy = 60.004000000000005%, Loss = 0.7180942058563232
Epoch: 7341, Batch Gradient Norm: 23.01107102140151
Epoch: 7341, Batch Gradient Norm after: 22.063908849633435
Epoch 7342/10000, Prediction Accuracy = 60.064%, Loss = 0.7168115019798279
Epoch: 7342, Batch Gradient Norm: 23.59741968047331
Epoch: 7342, Batch Gradient Norm after: 22.153889180190184
Epoch 7343/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.718456506729126
Epoch: 7343, Batch Gradient Norm: 23.039982146210527
Epoch: 7343, Batch Gradient Norm after: 22.157711662240633
Epoch 7344/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7168275833129882
Epoch: 7344, Batch Gradient Norm: 23.727067499140773
Epoch: 7344, Batch Gradient Norm after: 22.24985717274969
Epoch 7345/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7187055468559265
Epoch: 7345, Batch Gradient Norm: 23.072457495838766
Epoch: 7345, Batch Gradient Norm after: 22.260279699622178
Epoch 7346/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.716801643371582
Epoch: 7346, Batch Gradient Norm: 23.749043039914504
Epoch: 7346, Batch Gradient Norm after: 22.30109111171929
Epoch 7347/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7186985611915588
Epoch: 7347, Batch Gradient Norm: 22.999545570396833
Epoch: 7347, Batch Gradient Norm after: 22.127939253697146
Epoch 7348/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.716575562953949
Epoch: 7348, Batch Gradient Norm: 23.55730470520675
Epoch: 7348, Batch Gradient Norm after: 22.154028015712473
Epoch 7349/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7181076169013977
Epoch: 7349, Batch Gradient Norm: 22.982077528150985
Epoch: 7349, Batch Gradient Norm after: 22.094258147436268
Epoch 7350/10000, Prediction Accuracy = 60.016%, Loss = 0.7164440870285034
Epoch: 7350, Batch Gradient Norm: 23.49543693363656
Epoch: 7350, Batch Gradient Norm after: 22.11096743782765
Epoch 7351/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.7178438663482666
Epoch: 7351, Batch Gradient Norm: 22.965671716233018
Epoch: 7351, Batch Gradient Norm after: 22.05023046458826
Epoch 7352/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.7163619518280029
Epoch: 7352, Batch Gradient Norm: 23.441828676806796
Epoch: 7352, Batch Gradient Norm after: 22.071097609215904
Epoch 7353/10000, Prediction Accuracy = 60.036%, Loss = 0.7177120327949524
Epoch: 7353, Batch Gradient Norm: 22.952882653144542
Epoch: 7353, Batch Gradient Norm after: 22.010450120377122
Epoch 7354/10000, Prediction Accuracy = 60.05%, Loss = 0.7163337588310241
Epoch: 7354, Batch Gradient Norm: 23.382352501937092
Epoch: 7354, Batch Gradient Norm after: 22.029446653659498
Epoch 7355/10000, Prediction Accuracy = 60.016%, Loss = 0.7174967646598815
Epoch: 7355, Batch Gradient Norm: 22.91251138718319
Epoch: 7355, Batch Gradient Norm after: 21.95057948064118
Epoch 7356/10000, Prediction Accuracy = 60.012000000000015%, Loss = 0.7161003232002259
Epoch: 7356, Batch Gradient Norm: 23.42278490916878
Epoch: 7356, Batch Gradient Norm after: 22.045498418721444
Epoch 7357/10000, Prediction Accuracy = 60.011999999999986%, Loss = 0.7174527049064636
Epoch: 7357, Batch Gradient Norm: 22.973173750260802
Epoch: 7357, Batch Gradient Norm after: 22.011694913877186
Epoch 7358/10000, Prediction Accuracy = 59.996%, Loss = 0.7161956548690795
Epoch: 7358, Batch Gradient Norm: 23.442939197736678
Epoch: 7358, Batch Gradient Norm after: 22.05584080292998
Epoch 7359/10000, Prediction Accuracy = 60.032%, Loss = 0.7174775004386902
Epoch: 7359, Batch Gradient Norm: 22.981357380751973
Epoch: 7359, Batch Gradient Norm after: 22.03078348431401
Epoch 7360/10000, Prediction Accuracy = 60.04%, Loss = 0.716179621219635
Epoch: 7360, Batch Gradient Norm: 23.481655035569855
Epoch: 7360, Batch Gradient Norm after: 22.083148615199697
Epoch 7361/10000, Prediction Accuracy = 60.016%, Loss = 0.7175111532211303
Epoch: 7361, Batch Gradient Norm: 22.996689784534702
Epoch: 7361, Batch Gradient Norm after: 22.066396915939606
Epoch 7362/10000, Prediction Accuracy = 60.04599999999999%, Loss = 0.7161120533943176
Epoch: 7362, Batch Gradient Norm: 23.553837356322116
Epoch: 7362, Batch Gradient Norm after: 22.13177459943099
Epoch 7363/10000, Prediction Accuracy = 60.02%, Loss = 0.7176843881607056
Epoch: 7363, Batch Gradient Norm: 23.014288672134253
Epoch: 7363, Batch Gradient Norm after: 22.126750983885714
Epoch 7364/10000, Prediction Accuracy = 60.05799999999999%, Loss = 0.7161798715591431
Epoch: 7364, Batch Gradient Norm: 23.641847950541308
Epoch: 7364, Batch Gradient Norm after: 22.20177511489166
Epoch 7365/10000, Prediction Accuracy = 60.036%, Loss = 0.7179477691650391
Epoch: 7365, Batch Gradient Norm: 23.025451638282686
Epoch: 7365, Batch Gradient Norm after: 22.189637251271517
Epoch 7366/10000, Prediction Accuracy = 60.05%, Loss = 0.7161235690116883
Epoch: 7366, Batch Gradient Norm: 23.703873886428596
Epoch: 7366, Batch Gradient Norm after: 22.25389117854945
Epoch 7367/10000, Prediction Accuracy = 60.06199999999999%, Loss = 0.7179893136024476
Epoch: 7367, Batch Gradient Norm: 23.023923412935567
Epoch: 7367, Batch Gradient Norm after: 22.22367861489628
Epoch 7368/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.716025459766388
Epoch: 7368, Batch Gradient Norm: 23.70174479537771
Epoch: 7368, Batch Gradient Norm after: 22.264010752575988
Epoch 7369/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7179214239120484
Epoch: 7369, Batch Gradient Norm: 23.002772466051812
Epoch: 7369, Batch Gradient Norm after: 22.208750782839925
Epoch 7370/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.7159388899803162
Epoch: 7370, Batch Gradient Norm: 23.608820092707955
Epoch: 7370, Batch Gradient Norm after: 22.21196250327386
Epoch 7371/10000, Prediction Accuracy = 60.024%, Loss = 0.7176071524620056
Epoch: 7371, Batch Gradient Norm: 22.952307131443813
Epoch: 7371, Batch Gradient Norm after: 22.11865023486663
Epoch 7372/10000, Prediction Accuracy = 60.048%, Loss = 0.7157048583030701
Epoch: 7372, Batch Gradient Norm: 23.428075578179964
Epoch: 7372, Batch Gradient Norm after: 22.090653260563904
Epoch 7373/10000, Prediction Accuracy = 60.028%, Loss = 0.7170223474502564
Epoch: 7373, Batch Gradient Norm: 22.827286079485457
Epoch: 7373, Batch Gradient Norm after: 21.940030929714496
Epoch 7374/10000, Prediction Accuracy = 60.04%, Loss = 0.71536203622818
Epoch: 7374, Batch Gradient Norm: 23.42581491947568
Epoch: 7374, Batch Gradient Norm after: 22.082997016306006
Epoch 7375/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.7170498847961426
Epoch: 7375, Batch Gradient Norm: 22.85070468921824
Epoch: 7375, Batch Gradient Norm after: 21.95397525764895
Epoch 7376/10000, Prediction Accuracy = 60.048%, Loss = 0.7153901100158692
Epoch: 7376, Batch Gradient Norm: 23.40583654653175
Epoch: 7376, Batch Gradient Norm after: 22.06663976000885
Epoch 7377/10000, Prediction Accuracy = 60.03800000000001%, Loss = 0.7168859958648681
Epoch: 7377, Batch Gradient Norm: 22.86087868299705
Epoch: 7377, Batch Gradient Norm after: 21.948086524615693
Epoch 7378/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7152852892875672
Epoch: 7378, Batch Gradient Norm: 23.41476166452382
Epoch: 7378, Batch Gradient Norm after: 22.06437203507454
Epoch 7379/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7167961120605468
Epoch: 7379, Batch Gradient Norm: 22.914245299177267
Epoch: 7379, Batch Gradient Norm after: 21.981012297085357
Epoch 7380/10000, Prediction Accuracy = 60.004%, Loss = 0.7153860569000244
Epoch: 7380, Batch Gradient Norm: 23.35711573837133
Epoch: 7380, Batch Gradient Norm after: 22.021521706724485
Epoch 7381/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7165960907936096
Epoch: 7381, Batch Gradient Norm: 22.8423328539537
Epoch: 7381, Batch Gradient Norm after: 21.904976732264352
Epoch 7382/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.7151323556900024
Epoch: 7382, Batch Gradient Norm: 23.467673745571073
Epoch: 7382, Batch Gradient Norm after: 22.090005201854563
Epoch 7383/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7168049812316895
Epoch: 7383, Batch Gradient Norm: 22.96321883304104
Epoch: 7383, Batch Gradient Norm after: 22.043591784880377
Epoch 7384/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7153650641441345
Epoch: 7384, Batch Gradient Norm: 23.45017891286913
Epoch: 7384, Batch Gradient Norm after: 22.079021720972158
Epoch 7385/10000, Prediction Accuracy = 60.038%, Loss = 0.7167551994323731
Epoch: 7385, Batch Gradient Norm: 22.956321948933923
Epoch: 7385, Batch Gradient Norm after: 22.029325616864874
Epoch 7386/10000, Prediction Accuracy = 60.044%, Loss = 0.7153637051582337
Epoch: 7386, Batch Gradient Norm: 23.421733350701807
Epoch: 7386, Batch Gradient Norm after: 22.064588333800263
Epoch 7387/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7166704416275025
Epoch: 7387, Batch Gradient Norm: 22.94337084343769
Epoch: 7387, Batch Gradient Norm after: 22.00227646407872
Epoch 7388/10000, Prediction Accuracy = 60.048%, Loss = 0.7152248501777649
Epoch: 7388, Batch Gradient Norm: 23.364090020521292
Epoch: 7388, Batch Gradient Norm after: 22.021585953969904
Epoch 7389/10000, Prediction Accuracy = 60.048%, Loss = 0.7163591623306275
Epoch: 7389, Batch Gradient Norm: 22.892333209333607
Epoch: 7389, Batch Gradient Norm after: 21.93803341476196
Epoch 7390/10000, Prediction Accuracy = 60.03399999999999%, Loss = 0.7150102257728577
Epoch: 7390, Batch Gradient Norm: 23.422817416623992
Epoch: 7390, Batch Gradient Norm after: 22.057158094757153
Epoch 7391/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7164644598960876
Epoch: 7391, Batch Gradient Norm: 22.957946918732194
Epoch: 7391, Batch Gradient Norm after: 22.011264959356907
Epoch 7392/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7151514887809753
Epoch: 7392, Batch Gradient Norm: 23.397454102975566
Epoch: 7392, Batch Gradient Norm after: 22.04164869105768
Epoch 7393/10000, Prediction Accuracy = 60.03000000000001%, Loss = 0.7163403391838074
Epoch: 7393, Batch Gradient Norm: 22.947203061631665
Epoch: 7393, Batch Gradient Norm after: 21.988081060860747
Epoch 7394/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7150235533714294
Epoch: 7394, Batch Gradient Norm: 23.374706542394478
Epoch: 7394, Batch Gradient Norm after: 22.022381275919855
Epoch 7395/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.716204571723938
Epoch: 7395, Batch Gradient Norm: 22.935995213607868
Epoch: 7395, Batch Gradient Norm after: 21.967222596281427
Epoch 7396/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7149912476539612
Epoch: 7396, Batch Gradient Norm: 23.38436366441337
Epoch: 7396, Batch Gradient Norm after: 22.02996471486176
Epoch 7397/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7162814021110535
Epoch: 7397, Batch Gradient Norm: 22.947819885724808
Epoch: 7397, Batch Gradient Norm after: 21.985007669741282
Epoch 7398/10000, Prediction Accuracy = 60.044%, Loss = 0.7149748563766479
Epoch: 7398, Batch Gradient Norm: 23.379785934726513
Epoch: 7398, Batch Gradient Norm after: 22.02686985682357
Epoch 7399/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.7161340475082397
Epoch: 7399, Batch Gradient Norm: 22.94739396255204
Epoch: 7399, Batch Gradient Norm after: 21.97986955844159
Epoch 7400/10000, Prediction Accuracy = 60.036%, Loss = 0.7148576259613038
Epoch: 7400, Batch Gradient Norm: 23.37256025522687
Epoch: 7400, Batch Gradient Norm after: 22.018589911586773
Epoch 7401/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.7160148024559021
Epoch: 7401, Batch Gradient Norm: 22.951252319694163
Epoch: 7401, Batch Gradient Norm after: 21.975683883274
Epoch 7402/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.714829170703888
Epoch: 7402, Batch Gradient Norm: 23.364770677638063
Epoch: 7402, Batch Gradient Norm after: 22.014226291005137
Epoch 7403/10000, Prediction Accuracy = 60.048%, Loss = 0.7159615516662597
Epoch: 7403, Batch Gradient Norm: 22.93177099892767
Epoch: 7403, Batch Gradient Norm after: 21.95786030380619
Epoch 7404/10000, Prediction Accuracy = 60.012%, Loss = 0.7147003531455993
Epoch: 7404, Batch Gradient Norm: 23.39414424311139
Epoch: 7404, Batch Gradient Norm after: 22.030821311711755
Epoch 7405/10000, Prediction Accuracy = 60.048%, Loss = 0.715946638584137
Epoch: 7405, Batch Gradient Norm: 22.960173743551334
Epoch: 7405, Batch Gradient Norm after: 21.996455137058398
Epoch 7406/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7147120118141175
Epoch: 7406, Batch Gradient Norm: 23.42210847111929
Epoch: 7406, Batch Gradient Norm after: 22.05165119562925
Epoch 7407/10000, Prediction Accuracy = 60.052%, Loss = 0.7160457849502564
Epoch: 7407, Batch Gradient Norm: 22.963623473132994
Epoch: 7407, Batch Gradient Norm after: 22.017569564073813
Epoch 7408/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.7147364854812622
Epoch: 7408, Batch Gradient Norm: 23.441575631435192
Epoch: 7408, Batch Gradient Norm after: 22.072222090748078
Epoch 7409/10000, Prediction Accuracy = 60.044%, Loss = 0.7160708665847778
Epoch: 7409, Batch Gradient Norm: 22.96272864246834
Epoch: 7409, Batch Gradient Norm after: 22.033046369694727
Epoch 7410/10000, Prediction Accuracy = 60.06%, Loss = 0.7146069765090942
Epoch: 7410, Batch Gradient Norm: 23.45115443355768
Epoch: 7410, Batch Gradient Norm after: 22.080903434132313
Epoch 7411/10000, Prediction Accuracy = 60.040000000000006%, Loss = 0.7159451007843017
Epoch: 7411, Batch Gradient Norm: 22.959403632960488
Epoch: 7411, Batch Gradient Norm after: 22.035452655139544
Epoch 7412/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7145322918891907
Epoch: 7412, Batch Gradient Norm: 23.43033705584403
Epoch: 7412, Batch Gradient Norm after: 22.07118784952572
Epoch 7413/10000, Prediction Accuracy = 60.048%, Loss = 0.7158533930778503
Epoch: 7413, Batch Gradient Norm: 22.941761181330666
Epoch: 7413, Batch Gradient Norm after: 22.00592776058137
Epoch 7414/10000, Prediction Accuracy = 60.03599999999999%, Loss = 0.7144554853439331
Epoch: 7414, Batch Gradient Norm: 23.357557047892378
Epoch: 7414, Batch Gradient Norm after: 22.021755000818143
Epoch 7415/10000, Prediction Accuracy = 60.048%, Loss = 0.7155539035797119
Epoch: 7415, Batch Gradient Norm: 22.858914110541825
Epoch: 7415, Batch Gradient Norm after: 21.917594005373516
Epoch 7416/10000, Prediction Accuracy = 60.06400000000001%, Loss = 0.7141293883323669
Epoch: 7416, Batch Gradient Norm: 23.44625356148792
Epoch: 7416, Batch Gradient Norm after: 22.07847519953441
Epoch 7417/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7157667875289917
Epoch: 7417, Batch Gradient Norm: 22.94816890651851
Epoch: 7417, Batch Gradient Norm after: 22.026420060457724
Epoch 7418/10000, Prediction Accuracy = 60.044%, Loss = 0.7143751502037048
Epoch: 7418, Batch Gradient Norm: 23.408723756184727
Epoch: 7418, Batch Gradient Norm after: 22.05941062972118
Epoch 7419/10000, Prediction Accuracy = 60.052%, Loss = 0.715680468082428
Epoch: 7419, Batch Gradient Norm: 22.914712003457815
Epoch: 7419, Batch Gradient Norm after: 21.9834252595488
Epoch 7420/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.7142150282859803
Epoch: 7420, Batch Gradient Norm: 23.359795785254928
Epoch: 7420, Batch Gradient Norm after: 22.027156329490925
Epoch 7421/10000, Prediction Accuracy = 60.064%, Loss = 0.7154104828834533
Epoch: 7421, Batch Gradient Norm: 22.857116069744755
Epoch: 7421, Batch Gradient Norm after: 21.92297056328699
Epoch 7422/10000, Prediction Accuracy = 60.056%, Loss = 0.7139513254165649
Epoch: 7422, Batch Gradient Norm: 23.440276500499742
Epoch: 7422, Batch Gradient Norm after: 22.079069489870445
Epoch 7423/10000, Prediction Accuracy = 60.064%, Loss = 0.7155531764030456
Epoch: 7423, Batch Gradient Norm: 22.937974550525166
Epoch: 7423, Batch Gradient Norm after: 22.014142468082326
Epoch 7424/10000, Prediction Accuracy = 60.036%, Loss = 0.7141442775726319
Epoch: 7424, Batch Gradient Norm: 23.359805987234072
Epoch: 7424, Batch Gradient Norm after: 22.02876837190905
Epoch 7425/10000, Prediction Accuracy = 60.052%, Loss = 0.7152859687805175
Epoch: 7425, Batch Gradient Norm: 22.843964190897225
Epoch: 7425, Batch Gradient Norm after: 21.9165997765932
Epoch 7426/10000, Prediction Accuracy = 60.048%, Loss = 0.7137926101684571
Epoch: 7426, Batch Gradient Norm: 23.442583782045674
Epoch: 7426, Batch Gradient Norm after: 22.083276095368873
Epoch 7427/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.7154322862625122
Epoch: 7427, Batch Gradient Norm: 22.938205991203517
Epoch: 7427, Batch Gradient Norm after: 22.021855191128136
Epoch 7428/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7140134811401367
Epoch: 7428, Batch Gradient Norm: 23.377215033975865
Epoch: 7428, Batch Gradient Norm after: 22.04297307251294
Epoch 7429/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.715288233757019
Epoch: 7429, Batch Gradient Norm: 22.853353188545046
Epoch: 7429, Batch Gradient Norm after: 21.936229706620267
Epoch 7430/10000, Prediction Accuracy = 60.036%, Loss = 0.7137844800949097
Epoch: 7430, Batch Gradient Norm: 23.404398903437492
Epoch: 7430, Batch Gradient Norm after: 22.066081159478866
Epoch 7431/10000, Prediction Accuracy = 60.052%, Loss = 0.7152939677238465
Epoch: 7431, Batch Gradient Norm: 22.88994346216812
Epoch: 7431, Batch Gradient Norm after: 21.97336119233611
Epoch 7432/10000, Prediction Accuracy = 60.08600000000001%, Loss = 0.7137534499168396
Epoch: 7432, Batch Gradient Norm: 23.366643324130127
Epoch: 7432, Batch Gradient Norm after: 22.039069436789287
Epoch 7433/10000, Prediction Accuracy = 60.05400000000001%, Loss = 0.7150450825691224
Epoch: 7433, Batch Gradient Norm: 22.823395177611978
Epoch: 7433, Batch Gradient Norm after: 21.908302432887307
Epoch 7434/10000, Prediction Accuracy = 60.040000000000006%, Loss = 0.7135267972946167
Epoch: 7434, Batch Gradient Norm: 23.44073308443319
Epoch: 7434, Batch Gradient Norm after: 22.08831568628406
Epoch 7435/10000, Prediction Accuracy = 60.05200000000001%, Loss = 0.7152315258979798
Epoch: 7435, Batch Gradient Norm: 22.922034902646
Epoch: 7435, Batch Gradient Norm after: 22.0065092950862
Epoch 7436/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.7137406349182129
Epoch: 7436, Batch Gradient Norm: 23.303473745994847
Epoch: 7436, Batch Gradient Norm after: 21.998717934084784
Epoch 7437/10000, Prediction Accuracy = 60.036%, Loss = 0.7147560358047486
Epoch: 7437, Batch Gradient Norm: 22.724559085258438
Epoch: 7437, Batch Gradient Norm after: 21.81868416516213
Epoch 7438/10000, Prediction Accuracy = 60.05%, Loss = 0.7131237030029297
Epoch: 7438, Batch Gradient Norm: 23.568132568984044
Epoch: 7438, Batch Gradient Norm after: 22.175515016658963
Epoch 7439/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.7155054092407227
Epoch: 7439, Batch Gradient Norm: 22.960361036830715
Epoch: 7439, Batch Gradient Norm after: 22.11870039412006
Epoch 7440/10000, Prediction Accuracy = 60.044%, Loss = 0.7137789845466613
Epoch: 7440, Batch Gradient Norm: 23.485874180889788
Epoch: 7440, Batch Gradient Norm after: 22.135725766512774
Epoch 7441/10000, Prediction Accuracy = 60.05999999999999%, Loss = 0.7152663826942444
Epoch: 7441, Batch Gradient Norm: 22.911117871061208
Epoch: 7441, Batch Gradient Norm after: 22.037568875342476
Epoch 7442/10000, Prediction Accuracy = 60.06%, Loss = 0.7135404825210572
Epoch: 7442, Batch Gradient Norm: 23.29574953648309
Epoch: 7442, Batch Gradient Norm after: 22.01038966735145
Epoch 7443/10000, Prediction Accuracy = 60.062%, Loss = 0.7145743608474732
Epoch: 7443, Batch Gradient Norm: 22.656627342726505
Epoch: 7443, Batch Gradient Norm after: 21.784202030129027
Epoch 7444/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7127681255340577
Epoch: 7444, Batch Gradient Norm: 23.604338756496297
Epoch: 7444, Batch Gradient Norm after: 22.215256555234422
Epoch 7445/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7154038786888123
Epoch: 7445, Batch Gradient Norm: 22.946329247694607
Epoch: 7445, Batch Gradient Norm after: 22.130069138426258
Epoch 7446/10000, Prediction Accuracy = 60.036%, Loss = 0.7135137438774108
Epoch: 7446, Batch Gradient Norm: 23.425074754659022
Epoch: 7446, Batch Gradient Norm after: 22.10756659315932
Epoch 7447/10000, Prediction Accuracy = 60.056000000000004%, Loss = 0.7148370265960693
Epoch: 7447, Batch Gradient Norm: 22.778487167126475
Epoch: 7447, Batch Gradient Norm after: 21.927782236724262
Epoch 7448/10000, Prediction Accuracy = 60.084%, Loss = 0.7129663109779358
Epoch: 7448, Batch Gradient Norm: 23.40449090138446
Epoch: 7448, Batch Gradient Norm after: 22.092098932582942
Epoch 7449/10000, Prediction Accuracy = 60.036%, Loss = 0.7147100687026977
Epoch: 7449, Batch Gradient Norm: 22.761435483122888
Epoch: 7449, Batch Gradient Norm after: 21.90772935558525
Epoch 7450/10000, Prediction Accuracy = 60.06199999999999%, Loss = 0.7129107594490052
Epoch: 7450, Batch Gradient Norm: 23.428042048790513
Epoch: 7450, Batch Gradient Norm after: 22.11013314533032
Epoch 7451/10000, Prediction Accuracy = 60.05799999999999%, Loss = 0.7148161172866822
Epoch: 7451, Batch Gradient Norm: 22.77292985685547
Epoch: 7451, Batch Gradient Norm after: 21.92946992805942
Epoch 7452/10000, Prediction Accuracy = 60.03399999999999%, Loss = 0.7129055261611938
Epoch: 7452, Batch Gradient Norm: 23.401378311565992
Epoch: 7452, Batch Gradient Norm after: 22.09647692453125
Epoch 7453/10000, Prediction Accuracy = 60.062%, Loss = 0.7146252989768982
Epoch: 7453, Batch Gradient Norm: 22.746444363355796
Epoch: 7453, Batch Gradient Norm after: 21.900346704359887
Epoch 7454/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.712707793712616
Epoch: 7454, Batch Gradient Norm: 23.434998411925754
Epoch: 7454, Batch Gradient Norm after: 22.117800046601076
Epoch 7455/10000, Prediction Accuracy = 60.062%, Loss = 0.7146193146705627
Epoch: 7455, Batch Gradient Norm: 22.789593801095435
Epoch: 7455, Batch Gradient Norm after: 21.943709936960328
Epoch 7456/10000, Prediction Accuracy = 60.056000000000004%, Loss = 0.7127974629402161
Epoch: 7456, Batch Gradient Norm: 23.366483874040302
Epoch: 7456, Batch Gradient Norm after: 22.073819604582205
Epoch 7457/10000, Prediction Accuracy = 60.062%, Loss = 0.7143968343734741
Epoch: 7457, Batch Gradient Norm: 22.677237168004638
Epoch: 7457, Batch Gradient Norm after: 21.84233708683235
Epoch 7458/10000, Prediction Accuracy = 60.062%, Loss = 0.712417995929718
Epoch: 7458, Batch Gradient Norm: 23.519387319400053
Epoch: 7458, Batch Gradient Norm after: 22.176514711181298
Epoch 7459/10000, Prediction Accuracy = 60.05799999999999%, Loss = 0.7147231936454773
Epoch: 7459, Batch Gradient Norm: 22.89543821669316
Epoch: 7459, Batch Gradient Norm after: 22.05562213412819
Epoch 7460/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7129347681999206
Epoch: 7460, Batch Gradient Norm: 23.259680137036128
Epoch: 7460, Batch Gradient Norm after: 22.005780745987863
Epoch 7461/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7140246510505677
Epoch: 7461, Batch Gradient Norm: 22.527306083824556
Epoch: 7461, Batch Gradient Norm after: 21.70504780614507
Epoch 7462/10000, Prediction Accuracy = 60.024%, Loss = 0.7119844913482666
Epoch: 7462, Batch Gradient Norm: 23.67005418066616
Epoch: 7462, Batch Gradient Norm after: 22.292101514749447
Epoch 7463/10000, Prediction Accuracy = 60.072%, Loss = 0.7151691436767578
Epoch: 7463, Batch Gradient Norm: 22.913827032983928
Epoch: 7463, Batch Gradient Norm after: 22.131396782348894
Epoch 7464/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7128791451454163
Epoch: 7464, Batch Gradient Norm: 23.351080192341513
Epoch: 7464, Batch Gradient Norm after: 22.080715375515606
Epoch 7465/10000, Prediction Accuracy = 60.076%, Loss = 0.714104688167572
Epoch: 7465, Batch Gradient Norm: 22.618674567515203
Epoch: 7465, Batch Gradient Norm after: 21.814836436740546
Epoch 7466/10000, Prediction Accuracy = 60.076%, Loss = 0.7120269894599914
Epoch: 7466, Batch Gradient Norm: 23.547267514864338
Epoch: 7466, Batch Gradient Norm after: 22.209719831494965
Epoch 7467/10000, Prediction Accuracy = 60.06400000000001%, Loss = 0.7146111488342285
Epoch: 7467, Batch Gradient Norm: 22.874186542838668
Epoch: 7467, Batch Gradient Norm after: 22.062585354602216
Epoch 7468/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.7126674056053162
Epoch: 7468, Batch Gradient Norm: 23.20409012966616
Epoch: 7468, Batch Gradient Norm after: 21.983895553331106
Epoch 7469/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7135725021362305
Epoch: 7469, Batch Gradient Norm: 22.410652952794287
Epoch: 7469, Batch Gradient Norm after: 21.613276925960484
Epoch 7470/10000, Prediction Accuracy = 60.092%, Loss = 0.7113254785537719
Epoch: 7470, Batch Gradient Norm: 23.693869740515854
Epoch: 7470, Batch Gradient Norm after: 22.33433169622118
Epoch 7471/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7149266362190246
Epoch: 7471, Batch Gradient Norm: 22.838418356075255
Epoch: 7471, Batch Gradient Norm after: 22.02609129559834
Epoch 7472/10000, Prediction Accuracy = 60.064%, Loss = 0.7124752044677735
Epoch: 7472, Batch Gradient Norm: 23.257807290159167
Epoch: 7472, Batch Gradient Norm after: 22.0228881914343
Epoch 7473/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7137023687362671
Epoch: 7473, Batch Gradient Norm: 22.46433661654467
Epoch: 7473, Batch Gradient Norm after: 21.675808835617502
Epoch 7474/10000, Prediction Accuracy = 60.052%, Loss = 0.7114103317260743
Epoch: 7474, Batch Gradient Norm: 23.67000220570781
Epoch: 7474, Batch Gradient Norm after: 22.31249183692646
Epoch 7475/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7147496104240417
Epoch: 7475, Batch Gradient Norm: 22.878595514413842
Epoch: 7475, Batch Gradient Norm after: 22.082416051429842
Epoch 7476/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.712419331073761
Epoch: 7476, Batch Gradient Norm: 23.207532012802645
Epoch: 7476, Batch Gradient Norm after: 21.990276802543725
Epoch 7477/10000, Prediction Accuracy = 60.068%, Loss = 0.7133477687835693
Epoch: 7477, Batch Gradient Norm: 22.390594151065333
Epoch: 7477, Batch Gradient Norm after: 21.60647547223282
Epoch 7478/10000, Prediction Accuracy = 60.052%, Loss = 0.7110806703567505
Epoch: 7478, Batch Gradient Norm: 23.687384000291356
Epoch: 7478, Batch Gradient Norm after: 22.337333723172332
Epoch 7479/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7146886467933655
Epoch: 7479, Batch Gradient Norm: 22.791739749357777
Epoch: 7479, Batch Gradient Norm after: 21.996925392209622
Epoch 7480/10000, Prediction Accuracy = 60.092%, Loss = 0.7120603084564209
Epoch: 7480, Batch Gradient Norm: 23.29037809088695
Epoch: 7480, Batch Gradient Norm after: 22.047941720125824
Epoch 7481/10000, Prediction Accuracy = 60.052%, Loss = 0.7134446978569031
Epoch: 7481, Batch Gradient Norm: 22.50741332389771
Epoch: 7481, Batch Gradient Norm after: 21.7236578224205
Epoch 7482/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7112743020057678
Epoch: 7482, Batch Gradient Norm: 23.650639498035662
Epoch: 7482, Batch Gradient Norm after: 22.292321646762826
Epoch 7483/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.714526891708374
Epoch: 7483, Batch Gradient Norm: 22.88164401743332
Epoch: 7483, Batch Gradient Norm after: 22.13113403937582
Epoch 7484/10000, Prediction Accuracy = 60.048%, Loss = 0.7122760653495789
Epoch: 7484, Batch Gradient Norm: 23.253851610154182
Epoch: 7484, Batch Gradient Norm after: 22.037732869060157
Epoch 7485/10000, Prediction Accuracy = 60.088%, Loss = 0.7133133292198182
Epoch: 7485, Batch Gradient Norm: 22.40343535103914
Epoch: 7485, Batch Gradient Norm after: 21.6469524393957
Epoch 7486/10000, Prediction Accuracy = 60.102%, Loss = 0.7108548402786254
Epoch: 7486, Batch Gradient Norm: 23.668968521004967
Epoch: 7486, Batch Gradient Norm after: 22.329658581715957
Epoch 7487/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7143612504005432
Epoch: 7487, Batch Gradient Norm: 22.776413383160676
Epoch: 7487, Batch Gradient Norm after: 22.010677992190754
Epoch 7488/10000, Prediction Accuracy = 60.064%, Loss = 0.7118088126182556
Epoch: 7488, Batch Gradient Norm: 23.26331574094119
Epoch: 7488, Batch Gradient Norm after: 22.044739191125988
Epoch 7489/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7131780862808228
Epoch: 7489, Batch Gradient Norm: 22.38973299021624
Epoch: 7489, Batch Gradient Norm after: 21.641846974889113
Epoch 7490/10000, Prediction Accuracy = 60.068%, Loss = 0.7107163667678833
Epoch: 7490, Batch Gradient Norm: 23.665858932138978
Epoch: 7490, Batch Gradient Norm after: 22.331016896346018
Epoch 7491/10000, Prediction Accuracy = 60.044%, Loss = 0.7142467141151428
Epoch: 7491, Batch Gradient Norm: 22.765746983560643
Epoch: 7491, Batch Gradient Norm after: 22.00522285736133
Epoch 7492/10000, Prediction Accuracy = 60.08600000000001%, Loss = 0.7116353869438171
Epoch: 7492, Batch Gradient Norm: 23.266013332674152
Epoch: 7492, Batch Gradient Norm after: 22.050708132831453
Epoch 7493/10000, Prediction Accuracy = 60.102%, Loss = 0.7130892872810364
Epoch: 7493, Batch Gradient Norm: 22.388757241297775
Epoch: 7493, Batch Gradient Norm after: 21.64423896740859
Epoch 7494/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7106617569923401
Epoch: 7494, Batch Gradient Norm: 23.660001467266316
Epoch: 7494, Batch Gradient Norm after: 22.33352558250502
Epoch 7495/10000, Prediction Accuracy = 60.1%, Loss = 0.714250898361206
Epoch: 7495, Batch Gradient Norm: 22.729678663564794
Epoch: 7495, Batch Gradient Norm after: 21.9860573331302
Epoch 7496/10000, Prediction Accuracy = 60.04599999999999%, Loss = 0.7114689230918885
Epoch: 7496, Batch Gradient Norm: 23.292082624230773
Epoch: 7496, Batch Gradient Norm after: 22.074762116893588
Epoch 7497/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7130225419998169
Epoch: 7497, Batch Gradient Norm: 22.40461074580897
Epoch: 7497, Batch Gradient Norm after: 21.669551366046477
Epoch 7498/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.710503339767456
Epoch: 7498, Batch Gradient Norm: 23.649240668355862
Epoch: 7498, Batch Gradient Norm after: 22.32236145455592
Epoch 7499/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7139806985855103
Epoch: 7499, Batch Gradient Norm: 22.747895634326618
Epoch: 7499, Batch Gradient Norm after: 22.010922436112896
Epoch 7500/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.711408257484436
Epoch: 7500, Batch Gradient Norm: 23.250165723533467
Epoch: 7500, Batch Gradient Norm after: 22.05000002332708
Epoch 7501/10000, Prediction Accuracy = 60.08%, Loss = 0.7127868056297302
Epoch: 7501, Batch Gradient Norm: 22.321907830855398
Epoch: 7501, Batch Gradient Norm after: 21.599695378877456
Epoch 7502/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.710161030292511
Epoch: 7502, Batch Gradient Norm: 23.669746622096103
Epoch: 7502, Batch Gradient Norm after: 22.353661377751877
Epoch 7503/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7139169216156006
Epoch: 7503, Batch Gradient Norm: 22.61771164081237
Epoch: 7503, Batch Gradient Norm after: 21.89087976676926
Epoch 7504/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7109436631202698
Epoch: 7504, Batch Gradient Norm: 23.40677190530368
Epoch: 7504, Batch Gradient Norm after: 22.15975801199418
Epoch 7505/10000, Prediction Accuracy = 60.092%, Loss = 0.7132072687149048
Epoch: 7505, Batch Gradient Norm: 22.50732164100231
Epoch: 7505, Batch Gradient Norm after: 21.793821925975067
Epoch 7506/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.7106048822402954
Epoch: 7506, Batch Gradient Norm: 23.53120786254997
Epoch: 7506, Batch Gradient Norm after: 22.246548476847106
Epoch 7507/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7134449005126953
Epoch: 7507, Batch Gradient Norm: 22.65877139224164
Epoch: 7507, Batch Gradient Norm after: 21.945378859149514
Epoch 7508/10000, Prediction Accuracy = 60.11199999999999%, Loss = 0.7108958125114441
Epoch: 7508, Batch Gradient Norm: 23.332741529360796
Epoch: 7508, Batch Gradient Norm after: 22.11621361841942
Epoch 7509/10000, Prediction Accuracy = 60.065999999999995%, Loss = 0.7127742767333984
Epoch: 7509, Batch Gradient Norm: 22.394048637877884
Epoch: 7509, Batch Gradient Norm after: 21.6898120223611
Epoch 7510/10000, Prediction Accuracy = 60.074%, Loss = 0.7101436257362366
Epoch: 7510, Batch Gradient Norm: 23.626190301003273
Epoch: 7510, Batch Gradient Norm after: 22.318567177475106
Epoch 7511/10000, Prediction Accuracy = 60.088%, Loss = 0.7135781407356262
Epoch: 7511, Batch Gradient Norm: 22.69068177775902
Epoch: 7511, Batch Gradient Norm after: 21.98813795784059
Epoch 7512/10000, Prediction Accuracy = 60.072%, Loss = 0.7108790397644043
Epoch: 7512, Batch Gradient Norm: 23.26854295253121
Epoch: 7512, Batch Gradient Norm after: 22.078426725934282
Epoch 7513/10000, Prediction Accuracy = 60.074%, Loss = 0.7124787449836731
Epoch: 7513, Batch Gradient Norm: 22.289417343009692
Epoch: 7513, Batch Gradient Norm after: 21.595834697832597
Epoch 7514/10000, Prediction Accuracy = 60.074%, Loss = 0.7097312092781067
Epoch: 7514, Batch Gradient Norm: 23.6559062444879
Epoch: 7514, Batch Gradient Norm after: 22.36067759797575
Epoch 7515/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7135975360870361
Epoch: 7515, Batch Gradient Norm: 22.527197730170865
Epoch: 7515, Batch Gradient Norm after: 21.838301541791875
Epoch 7516/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.7104057669639587
Epoch: 7516, Batch Gradient Norm: 23.466406179825526
Epoch: 7516, Batch Gradient Norm after: 22.21747040420041
Epoch 7517/10000, Prediction Accuracy = 60.112%, Loss = 0.7130475163459777
Epoch: 7517, Batch Gradient Norm: 22.5152001883368
Epoch: 7517, Batch Gradient Norm after: 21.8355339206701
Epoch 7518/10000, Prediction Accuracy = 60.08%, Loss = 0.7102534770965576
Epoch: 7518, Batch Gradient Norm: 23.470783049044176
Epoch: 7518, Batch Gradient Norm after: 22.222368117978753
Epoch 7519/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7128965020179748
Epoch: 7519, Batch Gradient Norm: 22.504885146309622
Epoch: 7519, Batch Gradient Norm after: 21.831808510328834
Epoch 7520/10000, Prediction Accuracy = 60.09000000000001%, Loss = 0.7101494550704956
Epoch: 7520, Batch Gradient Norm: 23.470555072385864
Epoch: 7520, Batch Gradient Norm after: 22.224701857872297
Epoch 7521/10000, Prediction Accuracy = 60.088%, Loss = 0.7128536581993103
Epoch: 7521, Batch Gradient Norm: 22.475883734373777
Epoch: 7521, Batch Gradient Norm after: 21.8122715136785
Epoch 7522/10000, Prediction Accuracy = 60.062%, Loss = 0.7100432991981507
Epoch: 7522, Batch Gradient Norm: 23.486063817654667
Epoch: 7522, Batch Gradient Norm after: 22.238696449175862
Epoch 7523/10000, Prediction Accuracy = 60.068%, Loss = 0.7128323078155517
Epoch: 7523, Batch Gradient Norm: 22.486911504207907
Epoch: 7523, Batch Gradient Norm after: 21.828312926551003
Epoch 7524/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7099636435508728
Epoch: 7524, Batch Gradient Norm: 23.476423489690223
Epoch: 7524, Batch Gradient Norm after: 22.233864775361468
Epoch 7525/10000, Prediction Accuracy = 60.102%, Loss = 0.7127382278442382
Epoch: 7525, Batch Gradient Norm: 22.458757083922972
Epoch: 7525, Batch Gradient Norm after: 21.807429833013188
Epoch 7526/10000, Prediction Accuracy = 60.098%, Loss = 0.7099111318588257
Epoch: 7526, Batch Gradient Norm: 23.49385317476477
Epoch: 7526, Batch Gradient Norm after: 22.252608620143008
Epoch 7527/10000, Prediction Accuracy = 60.104%, Loss = 0.7128372073173523
Epoch: 7527, Batch Gradient Norm: 22.457199907182176
Epoch: 7527, Batch Gradient Norm after: 21.819637934871462
Epoch 7528/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.7098350048065185
Epoch: 7528, Batch Gradient Norm: 23.48314774391314
Epoch: 7528, Batch Gradient Norm after: 22.24994352369511
Epoch 7529/10000, Prediction Accuracy = 60.088%, Loss = 0.7126653671264649
Epoch: 7529, Batch Gradient Norm: 22.43282304016072
Epoch: 7529, Batch Gradient Norm after: 21.80021387201073
Epoch 7530/10000, Prediction Accuracy = 60.102%, Loss = 0.7096561193466187
Epoch: 7530, Batch Gradient Norm: 23.508010820081452
Epoch: 7530, Batch Gradient Norm after: 22.26587473271386
Epoch 7531/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7126357555389404
Epoch: 7531, Batch Gradient Norm: 22.454549623874815
Epoch: 7531, Batch Gradient Norm after: 21.82463823983739
Epoch 7532/10000, Prediction Accuracy = 60.076%, Loss = 0.709672749042511
Epoch: 7532, Batch Gradient Norm: 23.459745592218674
Epoch: 7532, Batch Gradient Norm after: 22.23898931537376
Epoch 7533/10000, Prediction Accuracy = 60.088%, Loss = 0.7124683499336243
Epoch: 7533, Batch Gradient Norm: 22.358240843554196
Epoch: 7533, Batch Gradient Norm after: 21.74186806752468
Epoch 7534/10000, Prediction Accuracy = 60.114%, Loss = 0.7093433976173401
Epoch: 7534, Batch Gradient Norm: 23.580250838869528
Epoch: 7534, Batch Gradient Norm after: 22.31936022190575
Epoch 7535/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7127187728881836
Epoch: 7535, Batch Gradient Norm: 22.520489939113308
Epoch: 7535, Batch Gradient Norm after: 21.90212220603968
Epoch 7536/10000, Prediction Accuracy = 60.076%, Loss = 0.7097240924835205
Epoch: 7536, Batch Gradient Norm: 23.358191938712178
Epoch: 7536, Batch Gradient Norm after: 22.177538136140726
Epoch 7537/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7121238231658935
Epoch: 7537, Batch Gradient Norm: 22.207483268337953
Epoch: 7537, Batch Gradient Norm after: 21.6088902178986
Epoch 7538/10000, Prediction Accuracy = 60.092%, Loss = 0.708914577960968
Epoch: 7538, Batch Gradient Norm: 23.629993163190626
Epoch: 7538, Batch Gradient Norm after: 22.36067667039003
Epoch 7539/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7128646016120911
Epoch: 7539, Batch Gradient Norm: 22.44253684059984
Epoch: 7539, Batch Gradient Norm after: 21.84370172206437
Epoch 7540/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.7094168424606323
Epoch: 7540, Batch Gradient Norm: 23.43886815865357
Epoch: 7540, Batch Gradient Norm after: 22.235703043792814
Epoch 7541/10000, Prediction Accuracy = 60.086%, Loss = 0.7121732473373413
Epoch: 7541, Batch Gradient Norm: 22.288844456064677
Epoch: 7541, Batch Gradient Norm after: 21.695670900670496
Epoch 7542/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7089439868927002
Epoch: 7542, Batch Gradient Norm: 23.587905571791485
Epoch: 7542, Batch Gradient Norm after: 22.347160669773046
Epoch 7543/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.712575364112854
Epoch: 7543, Batch Gradient Norm: 22.37957719626048
Epoch: 7543, Batch Gradient Norm after: 21.793335473560454
Epoch 7544/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.709151542186737
Epoch: 7544, Batch Gradient Norm: 23.48501691979711
Epoch: 7544, Batch Gradient Norm after: 22.27406357317694
Epoch 7545/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.7121987581253052
Epoch: 7545, Batch Gradient Norm: 22.311710788977585
Epoch: 7545, Batch Gradient Norm after: 21.73304655311496
Epoch 7546/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7088666796684265
Epoch: 7546, Batch Gradient Norm: 23.571748437000828
Epoch: 7546, Batch Gradient Norm after: 22.333952401160722
Epoch 7547/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7124297738075256
Epoch: 7547, Batch Gradient Norm: 22.416221140452684
Epoch: 7547, Batch Gradient Norm after: 21.842372070608256
Epoch 7548/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7091791510581971
Epoch: 7548, Batch Gradient Norm: 23.412727342107086
Epoch: 7548, Batch Gradient Norm after: 22.23633674069567
Epoch 7549/10000, Prediction Accuracy = 60.104%, Loss = 0.7120079517364502
Epoch: 7549, Batch Gradient Norm: 22.182871648559964
Epoch: 7549, Batch Gradient Norm after: 21.62910025524651
Epoch 7550/10000, Prediction Accuracy = 60.074%, Loss = 0.7084710240364075
Epoch: 7550, Batch Gradient Norm: 23.610161566956453
Epoch: 7550, Batch Gradient Norm after: 22.36067719276453
Epoch 7551/10000, Prediction Accuracy = 60.084%, Loss = 0.7124042272567749
Epoch: 7551, Batch Gradient Norm: 22.37367262056898
Epoch: 7551, Batch Gradient Norm after: 21.814505365021343
Epoch 7552/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7088752031326294
Epoch: 7552, Batch Gradient Norm: 23.456932361471818
Epoch: 7552, Batch Gradient Norm after: 22.267066483115595
Epoch 7553/10000, Prediction Accuracy = 60.08%, Loss = 0.7119069218635559
Epoch: 7553, Batch Gradient Norm: 22.228615965993804
Epoch: 7553, Batch Gradient Norm after: 21.676626591090915
Epoch 7554/10000, Prediction Accuracy = 60.092%, Loss = 0.7084583520889283
Epoch: 7554, Batch Gradient Norm: 23.583388015050154
Epoch: 7554, Batch Gradient Norm after: 22.36067713741205
Epoch 7555/10000, Prediction Accuracy = 60.08599999999999%, Loss = 0.712200927734375
Epoch: 7555, Batch Gradient Norm: 22.28442719878888
Epoch: 7555, Batch Gradient Norm after: 21.741256153581798
Epoch 7556/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7085211396217346
Epoch: 7556, Batch Gradient Norm: 23.554198569517045
Epoch: 7556, Batch Gradient Norm after: 22.3377143777029
Epoch 7557/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7120639204978942
Epoch: 7557, Batch Gradient Norm: 22.326184014515952
Epoch: 7557, Batch Gradient Norm after: 21.78408282992251
Epoch 7558/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7086117029190063
Epoch: 7558, Batch Gradient Norm: 23.4810358400073
Epoch: 7558, Batch Gradient Norm after: 22.294138263724104
Epoch 7559/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7119001269340515
Epoch: 7559, Batch Gradient Norm: 22.199324537675118
Epoch: 7559, Batch Gradient Norm after: 21.677490884781808
Epoch 7560/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7082639098167419
Epoch: 7560, Batch Gradient Norm: 23.578338413971736
Epoch: 7560, Batch Gradient Norm after: 22.36067816623642
Epoch 7561/10000, Prediction Accuracy = 60.112%, Loss = 0.7120794653892517
Epoch: 7561, Batch Gradient Norm: 22.283946859256936
Epoch: 7561, Batch Gradient Norm after: 21.76156174354051
Epoch 7562/10000, Prediction Accuracy = 60.102%, Loss = 0.7083589673042298
Epoch: 7562, Batch Gradient Norm: 23.517425976461784
Epoch: 7562, Batch Gradient Norm after: 22.3220135898741
Epoch 7563/10000, Prediction Accuracy = 60.1%, Loss = 0.7117870211601257
Epoch: 7563, Batch Gradient Norm: 22.228917935572365
Epoch: 7563, Batch Gradient Norm after: 21.714466131694778
Epoch 7564/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7081831693649292
Epoch: 7564, Batch Gradient Norm: 23.557721465770346
Epoch: 7564, Batch Gradient Norm after: 22.357114202871102
Epoch 7565/10000, Prediction Accuracy = 60.064%, Loss = 0.7118890404701232
Epoch: 7565, Batch Gradient Norm: 22.22405339040993
Epoch: 7565, Batch Gradient Norm after: 21.720430245146424
Epoch 7566/10000, Prediction Accuracy = 60.112%, Loss = 0.7081140875816345
Epoch: 7566, Batch Gradient Norm: 23.5512747530714
Epoch: 7566, Batch Gradient Norm after: 22.355946626485803
Epoch 7567/10000, Prediction Accuracy = 60.072%, Loss = 0.7117711186408997
Epoch: 7567, Batch Gradient Norm: 22.219673835803587
Epoch: 7567, Batch Gradient Norm after: 21.715941536096107
Epoch 7568/10000, Prediction Accuracy = 60.088%, Loss = 0.708019471168518
Epoch: 7568, Batch Gradient Norm: 23.55100728728574
Epoch: 7568, Batch Gradient Norm after: 22.35695782493623
Epoch 7569/10000, Prediction Accuracy = 60.104%, Loss = 0.7117948412895203
Epoch: 7569, Batch Gradient Norm: 22.21778262674075
Epoch: 7569, Batch Gradient Norm after: 21.724805686188517
Epoch 7570/10000, Prediction Accuracy = 60.092%, Loss = 0.7080550789833069
Epoch: 7570, Batch Gradient Norm: 23.543655456400998
Epoch: 7570, Batch Gradient Norm after: 22.360373879851988
Epoch 7571/10000, Prediction Accuracy = 60.11%, Loss = 0.7117688775062561
Epoch: 7571, Batch Gradient Norm: 22.17557097072015
Epoch: 7571, Batch Gradient Norm after: 21.696102025692806
Epoch 7572/10000, Prediction Accuracy = 60.10999999999999%, Loss = 0.707818615436554
Epoch: 7572, Batch Gradient Norm: 23.540870188935028
Epoch: 7572, Batch Gradient Norm after: 22.35167238176592
Epoch 7573/10000, Prediction Accuracy = 60.102%, Loss = 0.7115788340568543
Epoch: 7573, Batch Gradient Norm: 22.164767268391994
Epoch: 7573, Batch Gradient Norm after: 21.68383533263764
Epoch 7574/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.7077089190483093
Epoch: 7574, Batch Gradient Norm: 23.538842781449738
Epoch: 7574, Batch Gradient Norm after: 22.349936126835612
Epoch 7575/10000, Prediction Accuracy = 60.07800000000001%, Loss = 0.711526381969452
Epoch: 7575, Batch Gradient Norm: 22.135253048283687
Epoch: 7575, Batch Gradient Norm after: 21.66257089510539
Epoch 7576/10000, Prediction Accuracy = 60.086%, Loss = 0.7076072931289673
Epoch: 7576, Batch Gradient Norm: 23.527380905800623
Epoch: 7576, Batch Gradient Norm after: 22.340323558732504
Epoch 7577/10000, Prediction Accuracy = 60.08%, Loss = 0.7114212989807129
Epoch: 7577, Batch Gradient Norm: 22.10972246798099
Epoch: 7577, Batch Gradient Norm after: 21.636265080483884
Epoch 7578/10000, Prediction Accuracy = 60.116%, Loss = 0.7074356436729431
Epoch: 7578, Batch Gradient Norm: 23.52320086145423
Epoch: 7578, Batch Gradient Norm after: 22.33173640202083
Epoch 7579/10000, Prediction Accuracy = 60.116%, Loss = 0.7113479137420654
Epoch: 7579, Batch Gradient Norm: 22.104982566441663
Epoch: 7579, Batch Gradient Norm after: 21.624492912915073
Epoch 7580/10000, Prediction Accuracy = 60.1%, Loss = 0.7074185729026794
Epoch: 7580, Batch Gradient Norm: 23.512236185909053
Epoch: 7580, Batch Gradient Norm after: 22.325992267903658
Epoch 7581/10000, Prediction Accuracy = 60.084%, Loss = 0.7113688230514527
Epoch: 7581, Batch Gradient Norm: 22.081586358128757
Epoch: 7581, Batch Gradient Norm after: 21.604433488431848
Epoch 7582/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7073160290718079
Epoch: 7582, Batch Gradient Norm: 23.504263519064693
Epoch: 7582, Batch Gradient Norm after: 22.31752722967316
Epoch 7583/10000, Prediction Accuracy = 60.11%, Loss = 0.7112187266349792
Epoch: 7583, Batch Gradient Norm: 22.06051252158321
Epoch: 7583, Batch Gradient Norm after: 21.576496604729446
Epoch 7584/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7071406245231628
Epoch: 7584, Batch Gradient Norm: 23.505561412338245
Epoch: 7584, Batch Gradient Norm after: 22.309995789042553
Epoch 7585/10000, Prediction Accuracy = 60.09799999999999%, Loss = 0.7111193895339966
Epoch: 7585, Batch Gradient Norm: 22.05014098894173
Epoch: 7585, Batch Gradient Norm after: 21.55945698160567
Epoch 7586/10000, Prediction Accuracy = 60.06999999999999%, Loss = 0.7070803284645081
Epoch: 7586, Batch Gradient Norm: 23.50531253762942
Epoch: 7586, Batch Gradient Norm after: 22.307894667210604
Epoch 7587/10000, Prediction Accuracy = 60.072%, Loss = 0.7110846638679504
Epoch: 7587, Batch Gradient Norm: 22.052700732034964
Epoch: 7587, Batch Gradient Norm after: 21.555417214374536
Epoch 7588/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.707008421421051
Epoch: 7588, Batch Gradient Norm: 23.51938179661966
Epoch: 7588, Batch Gradient Norm after: 22.31294808086521
Epoch 7589/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.71102933883667
Epoch: 7589, Batch Gradient Norm: 22.111309248215754
Epoch: 7589, Batch Gradient Norm after: 21.601354593728864
Epoch 7590/10000, Prediction Accuracy = 60.11%, Loss = 0.7071145176887512
Epoch: 7590, Batch Gradient Norm: 23.536788516717408
Epoch: 7590, Batch Gradient Norm after: 22.33056227451705
Epoch 7591/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7111142396926879
Epoch: 7591, Batch Gradient Norm: 22.17447620867086
Epoch: 7591, Batch Gradient Norm after: 21.666201947866693
Epoch 7592/10000, Prediction Accuracy = 60.102%, Loss = 0.7072933793067933
Epoch: 7592, Batch Gradient Norm: 23.546288163318476
Epoch: 7592, Batch Gradient Norm after: 22.35389299875485
Epoch 7593/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7111289978027344
Epoch: 7593, Batch Gradient Norm: 22.209325838708068
Epoch: 7593, Batch Gradient Norm after: 21.717574593259346
Epoch 7594/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7072698473930359
Epoch: 7594, Batch Gradient Norm: 23.54101098797631
Epoch: 7594, Batch Gradient Norm after: 22.357775110078702
Epoch 7595/10000, Prediction Accuracy = 60.102%, Loss = 0.7109540462493896
Epoch: 7595, Batch Gradient Norm: 22.200795820375188
Epoch: 7595, Batch Gradient Norm after: 21.719259271484745
Epoch 7596/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.707182776927948
Epoch: 7596, Batch Gradient Norm: 23.536261783867822
Epoch: 7596, Batch Gradient Norm after: 22.36007880577476
Epoch 7597/10000, Prediction Accuracy = 60.076%, Loss = 0.7109060049057007
Epoch: 7597, Batch Gradient Norm: 22.158439709256456
Epoch: 7597, Batch Gradient Norm after: 21.694435282023765
Epoch 7598/10000, Prediction Accuracy = 60.08%, Loss = 0.7070415019989014
Epoch: 7598, Batch Gradient Norm: 23.51691644459967
Epoch: 7598, Batch Gradient Norm after: 22.34764620989976
Epoch 7599/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.7107672810554504
Epoch: 7599, Batch Gradient Norm: 22.095789420663536
Epoch: 7599, Batch Gradient Norm after: 21.639635909846866
Epoch 7600/10000, Prediction Accuracy = 60.1%, Loss = 0.7067740082740783
Epoch: 7600, Batch Gradient Norm: 23.48799136695919
Epoch: 7600, Batch Gradient Norm after: 22.320434349814768
Epoch 7601/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7106415271759033
Epoch: 7601, Batch Gradient Norm: 22.021591999855975
Epoch: 7601, Batch Gradient Norm after: 21.557931430655714
Epoch 7602/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7065899491310119
Epoch: 7602, Batch Gradient Norm: 23.459133342351087
Epoch: 7602, Batch Gradient Norm after: 22.29062493913914
Epoch 7603/10000, Prediction Accuracy = 60.102%, Loss = 0.7105854749679565
Epoch: 7603, Batch Gradient Norm: 21.912492934876422
Epoch: 7603, Batch Gradient Norm after: 21.454841821148893
Epoch 7604/10000, Prediction Accuracy = 60.098%, Loss = 0.7062468647956848
Epoch: 7604, Batch Gradient Norm: 23.43478262782228
Epoch: 7604, Batch Gradient Norm after: 22.25186586785882
Epoch 7605/10000, Prediction Accuracy = 60.124%, Loss = 0.7103609561920166
Epoch: 7605, Batch Gradient Norm: 21.83173019699994
Epoch: 7605, Batch Gradient Norm after: 21.332463134933185
Epoch 7606/10000, Prediction Accuracy = 60.114%, Loss = 0.7059089064598083
Epoch: 7606, Batch Gradient Norm: 23.464975796960626
Epoch: 7606, Batch Gradient Norm after: 22.256633389716665
Epoch 7607/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7103626370429993
Epoch: 7607, Batch Gradient Norm: 21.92014037099263
Epoch: 7607, Batch Gradient Norm after: 21.40788950630773
Epoch 7608/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7061156988143921
Epoch: 7608, Batch Gradient Norm: 23.479345886135356
Epoch: 7608, Batch Gradient Norm after: 22.263197665575554
Epoch 7609/10000, Prediction Accuracy = 60.086%, Loss = 0.7103686213493348
Epoch: 7609, Batch Gradient Norm: 21.960196886322787
Epoch: 7609, Batch Gradient Norm after: 21.43826245717167
Epoch 7610/10000, Prediction Accuracy = 60.114%, Loss = 0.7061306238174438
Epoch: 7610, Batch Gradient Norm: 23.50944036483038
Epoch: 7610, Batch Gradient Norm after: 22.282744218444044
Epoch 7611/10000, Prediction Accuracy = 60.088%, Loss = 0.7103766322135925
Epoch: 7611, Batch Gradient Norm: 22.08827204215667
Epoch: 7611, Batch Gradient Norm after: 21.54645882088843
Epoch 7612/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.706434965133667
Epoch: 7612, Batch Gradient Norm: 23.545888248796995
Epoch: 7612, Batch Gradient Norm after: 22.32535975881231
Epoch 7613/10000, Prediction Accuracy = 60.102%, Loss = 0.7105365037918091
Epoch: 7613, Batch Gradient Norm: 22.21419936821559
Epoch: 7613, Batch Gradient Norm after: 21.683532887837725
Epoch 7614/10000, Prediction Accuracy = 60.112%, Loss = 0.7067746639251709
Epoch: 7614, Batch Gradient Norm: 23.55008914268156
Epoch: 7614, Batch Gradient Norm after: 22.360679025602195
Epoch 7615/10000, Prediction Accuracy = 60.112%, Loss = 0.7104957938194275
Epoch: 7615, Batch Gradient Norm: 22.232312591081065
Epoch: 7615, Batch Gradient Norm after: 21.726803964433817
Epoch 7616/10000, Prediction Accuracy = 60.12199999999999%, Loss = 0.7066935062408447
Epoch: 7616, Batch Gradient Norm: 23.52404574484839
Epoch: 7616, Batch Gradient Norm after: 22.34602254057835
Epoch 7617/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7102901816368103
Epoch: 7617, Batch Gradient Norm: 22.206474184191404
Epoch: 7617, Batch Gradient Norm after: 21.717423137322193
Epoch 7618/10000, Prediction Accuracy = 60.062%, Loss = 0.7065746307373046
Epoch: 7618, Batch Gradient Norm: 23.52622223473993
Epoch: 7618, Batch Gradient Norm after: 22.35595577799698
Epoch 7619/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7102659344673157
Epoch: 7619, Batch Gradient Norm: 22.153390939059737
Epoch: 7619, Batch Gradient Norm after: 21.688671560149835
Epoch 7620/10000, Prediction Accuracy = 60.098%, Loss = 0.7063946008682251
Epoch: 7620, Batch Gradient Norm: 23.505954595552247
Epoch: 7620, Batch Gradient Norm after: 22.34591824750848
Epoch 7621/10000, Prediction Accuracy = 60.072%, Loss = 0.7101216077804565
Epoch: 7621, Batch Gradient Norm: 22.08454508468946
Epoch: 7621, Batch Gradient Norm after: 21.629965656319392
Epoch 7622/10000, Prediction Accuracy = 60.11%, Loss = 0.7061167478561401
Epoch: 7622, Batch Gradient Norm: 23.472904441941612
Epoch: 7622, Batch Gradient Norm after: 22.314663932400734
Epoch 7623/10000, Prediction Accuracy = 60.136%, Loss = 0.7099992275238037
Epoch: 7623, Batch Gradient Norm: 21.970261174680346
Epoch: 7623, Batch Gradient Norm after: 21.522005908654307
Epoch 7624/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.705850887298584
Epoch: 7624, Batch Gradient Norm: 23.426137206103174
Epoch: 7624, Batch Gradient Norm after: 22.27089984103198
Epoch 7625/10000, Prediction Accuracy = 60.134%, Loss = 0.7098979949951172
Epoch: 7625, Batch Gradient Norm: 21.82104325865904
Epoch: 7625, Batch Gradient Norm after: 21.348034219039786
Epoch 7626/10000, Prediction Accuracy = 60.112%, Loss = 0.7053714871406556
Epoch: 7626, Batch Gradient Norm: 23.43870740614586
Epoch: 7626, Batch Gradient Norm after: 22.266287875066784
Epoch 7627/10000, Prediction Accuracy = 60.114%, Loss = 0.7097580194473266
Epoch: 7627, Batch Gradient Norm: 21.85411629881885
Epoch: 7627, Batch Gradient Norm after: 21.37460939699569
Epoch 7628/10000, Prediction Accuracy = 60.104%, Loss = 0.7053499460220337
Epoch: 7628, Batch Gradient Norm: 23.439786180412977
Epoch: 7628, Batch Gradient Norm after: 22.24807015477829
Epoch 7629/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7096875786781311
Epoch: 7629, Batch Gradient Norm: 21.84020992162084
Epoch: 7629, Batch Gradient Norm after: 21.33731449749399
Epoch 7630/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7052884697914124
Epoch: 7630, Batch Gradient Norm: 23.455389131570364
Epoch: 7630, Batch Gradient Norm after: 22.250902041588297
Epoch 7631/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7096735835075378
Epoch: 7631, Batch Gradient Norm: 21.90149092563878
Epoch: 7631, Batch Gradient Norm after: 21.386110286538315
Epoch 7632/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7053391695022583
Epoch: 7632, Batch Gradient Norm: 23.476577206437614
Epoch: 7632, Batch Gradient Norm after: 22.258728998340743
Epoch 7633/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7096544504165649
Epoch: 7633, Batch Gradient Norm: 21.973811550246833
Epoch: 7633, Batch Gradient Norm after: 21.446203589089443
Epoch 7634/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7055266261100769
Epoch: 7634, Batch Gradient Norm: 23.498112017090307
Epoch: 7634, Batch Gradient Norm after: 22.282081171023826
Epoch 7635/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7097725033760071
Epoch: 7635, Batch Gradient Norm: 22.05452404630414
Epoch: 7635, Batch Gradient Norm after: 21.529113683072097
Epoch 7636/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7057133316993713
Epoch: 7636, Batch Gradient Norm: 23.51663225819727
Epoch: 7636, Batch Gradient Norm after: 22.312259210730637
Epoch 7637/10000, Prediction Accuracy = 60.122%, Loss = 0.7097478151321411
Epoch: 7637, Batch Gradient Norm: 22.13556776741643
Epoch: 7637, Batch Gradient Norm after: 21.617666787954576
Epoch 7638/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7058035969734192
Epoch: 7638, Batch Gradient Norm: 23.54086333717712
Epoch: 7638, Batch Gradient Norm after: 22.34543484169905
Epoch 7639/10000, Prediction Accuracy = 60.116%, Loss = 0.7097124576568603
Epoch: 7639, Batch Gradient Norm: 22.21112042028536
Epoch: 7639, Batch Gradient Norm after: 21.70523012984613
Epoch 7640/10000, Prediction Accuracy = 60.06999999999999%, Loss = 0.7059667229652404
Epoch: 7640, Batch Gradient Norm: 23.51697944580872
Epoch: 7640, Batch Gradient Norm after: 22.35148096576722
Epoch 7641/10000, Prediction Accuracy = 60.122%, Loss = 0.7096345067024231
Epoch: 7641, Batch Gradient Norm: 22.17319159500558
Epoch: 7641, Batch Gradient Norm after: 21.69691494543667
Epoch 7642/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7058065056800842
Epoch: 7642, Batch Gradient Norm: 23.51595336306734
Epoch: 7642, Batch Gradient Norm after: 22.35749651905488
Epoch 7643/10000, Prediction Accuracy = 60.072%, Loss = 0.7095288634300232
Epoch: 7643, Batch Gradient Norm: 22.128135746022025
Epoch: 7643, Batch Gradient Norm after: 21.672901190096752
Epoch 7644/10000, Prediction Accuracy = 60.124%, Loss = 0.7056146502494812
Epoch: 7644, Batch Gradient Norm: 23.47682766940618
Epoch: 7644, Batch Gradient Norm after: 22.3342440878487
Epoch 7645/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7094293713569642
Epoch: 7645, Batch Gradient Norm: 22.009284396505908
Epoch: 7645, Batch Gradient Norm after: 21.572567411743925
Epoch 7646/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.705342423915863
Epoch: 7646, Batch Gradient Norm: 23.41887687451032
Epoch: 7646, Batch Gradient Norm after: 22.286205859052636
Epoch 7647/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7092614889144897
Epoch: 7647, Batch Gradient Norm: 21.8153143036275
Epoch: 7647, Batch Gradient Norm after: 21.361186580430193
Epoch 7648/10000, Prediction Accuracy = 60.126%, Loss = 0.7047083735466003
Epoch: 7648, Batch Gradient Norm: 23.41157986177589
Epoch: 7648, Batch Gradient Norm after: 22.265607125135976
Epoch 7649/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.7090636968612671
Epoch: 7649, Batch Gradient Norm: 21.80380377124093
Epoch: 7649, Batch Gradient Norm after: 21.32285429241361
Epoch 7650/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7045883178710938
Epoch: 7650, Batch Gradient Norm: 23.43126428186764
Epoch: 7650, Batch Gradient Norm after: 22.26832748215528
Epoch 7651/10000, Prediction Accuracy = 60.126%, Loss = 0.7090804576873779
Epoch: 7651, Batch Gradient Norm: 21.852902507353587
Epoch: 7651, Batch Gradient Norm after: 21.376582094117538
Epoch 7652/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7046863436698914
Epoch: 7652, Batch Gradient Norm: 23.41710116212836
Epoch: 7652, Batch Gradient Norm after: 22.24773444655525
Epoch 7653/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7089609861373901
Epoch: 7653, Batch Gradient Norm: 21.815723631667264
Epoch: 7653, Batch Gradient Norm after: 21.311773074955376
Epoch 7654/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7044842600822449
Epoch: 7654, Batch Gradient Norm: 23.444606070785944
Epoch: 7654, Batch Gradient Norm after: 22.25941764362401
Epoch 7655/10000, Prediction Accuracy = 60.122%, Loss = 0.7089845776557923
Epoch: 7655, Batch Gradient Norm: 21.899494761523304
Epoch: 7655, Batch Gradient Norm after: 21.398911528508172
Epoch 7656/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7047188997268676
Epoch: 7656, Batch Gradient Norm: 23.439138171545917
Epoch: 7656, Batch Gradient Norm after: 22.254048334606185
Epoch 7657/10000, Prediction Accuracy = 60.148%, Loss = 0.7090055108070373
Epoch: 7657, Batch Gradient Norm: 21.885778687502533
Epoch: 7657, Batch Gradient Norm after: 21.38449895118024
Epoch 7658/10000, Prediction Accuracy = 60.14%, Loss = 0.7046273112297058
Epoch: 7658, Batch Gradient Norm: 23.438219309732794
Epoch: 7658, Batch Gradient Norm after: 22.249377947300854
Epoch 7659/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.708875036239624
Epoch: 7659, Batch Gradient Norm: 21.89020180841458
Epoch: 7659, Batch Gradient Norm after: 21.379637523492732
Epoch 7660/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.7045205235481262
Epoch: 7660, Batch Gradient Norm: 23.449122165442063
Epoch: 7660, Batch Gradient Norm after: 22.25368542268218
Epoch 7661/10000, Prediction Accuracy = 60.126%, Loss = 0.708815062046051
Epoch: 7661, Batch Gradient Norm: 21.918740454521785
Epoch: 7661, Batch Gradient Norm after: 21.400917716088948
Epoch 7662/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.704564917087555
Epoch: 7662, Batch Gradient Norm: 23.45816317399405
Epoch: 7662, Batch Gradient Norm after: 22.265033698132783
Epoch 7663/10000, Prediction Accuracy = 60.124%, Loss = 0.7088199496269226
Epoch: 7663, Batch Gradient Norm: 21.954749217366256
Epoch: 7663, Batch Gradient Norm after: 21.4377241141915
Epoch 7664/10000, Prediction Accuracy = 60.086%, Loss = 0.7045869588851928
Epoch: 7664, Batch Gradient Norm: 23.477064954064055
Epoch: 7664, Batch Gradient Norm after: 22.281195143823762
Epoch 7665/10000, Prediction Accuracy = 60.086%, Loss = 0.70878084897995
Epoch: 7665, Batch Gradient Norm: 22.024690332375233
Epoch: 7665, Batch Gradient Norm after: 21.507119674393916
Epoch 7666/10000, Prediction Accuracy = 60.12199999999999%, Loss = 0.7047250509262085
Epoch: 7666, Batch Gradient Norm: 23.488174616059702
Epoch: 7666, Batch Gradient Norm after: 22.303319100180477
Epoch 7667/10000, Prediction Accuracy = 60.128%, Loss = 0.7088531374931335
Epoch: 7667, Batch Gradient Norm: 22.05532246758942
Epoch: 7667, Batch Gradient Norm after: 21.556048680382197
Epoch 7668/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.704833984375
Epoch: 7668, Batch Gradient Norm: 23.47902735646163
Epoch: 7668, Batch Gradient Norm after: 22.311605640526103
Epoch 7669/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7088039636611938
Epoch: 7669, Batch Gradient Norm: 22.032319891266244
Epoch: 7669, Batch Gradient Norm after: 21.5555206414782
Epoch 7670/10000, Prediction Accuracy = 60.104%, Loss = 0.704649293422699
Epoch: 7670, Batch Gradient Norm: 23.463016399509105
Epoch: 7670, Batch Gradient Norm after: 22.302576786521996
Epoch 7671/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7085869908332825
Epoch: 7671, Batch Gradient Norm: 21.96858166785104
Epoch: 7671, Batch Gradient Norm after: 21.50016543734652
Epoch 7672/10000, Prediction Accuracy = 60.08%, Loss = 0.7044235110282898
Epoch: 7672, Batch Gradient Norm: 23.43522682252941
Epoch: 7672, Batch Gradient Norm after: 22.281879286818985
Epoch 7673/10000, Prediction Accuracy = 60.128%, Loss = 0.7085095882415772
Epoch: 7673, Batch Gradient Norm: 21.86811640648556
Epoch: 7673, Batch Gradient Norm after: 21.409739807067808
Epoch 7674/10000, Prediction Accuracy = 60.112%, Loss = 0.7041240930557251
Epoch: 7674, Batch Gradient Norm: 23.403263699600316
Epoch: 7674, Batch Gradient Norm after: 22.247050628087976
Epoch 7675/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.708300256729126
Epoch: 7675, Batch Gradient Norm: 21.77366467464738
Epoch: 7675, Batch Gradient Norm after: 21.269571494560104
Epoch 7676/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.7037658810615539
Epoch: 7676, Batch Gradient Norm: 23.44812364677455
Epoch: 7676, Batch Gradient Norm after: 22.27855415888415
Epoch 7677/10000, Prediction Accuracy = 60.126%, Loss = 0.7084082722663879
Epoch: 7677, Batch Gradient Norm: 21.93281470876028
Epoch: 7677, Batch Gradient Norm after: 21.449708737731918
Epoch 7678/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7042132377624511
Epoch: 7678, Batch Gradient Norm: 23.42648024123034
Epoch: 7678, Batch Gradient Norm after: 22.265850941771255
Epoch 7679/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.708375096321106
Epoch: 7679, Batch Gradient Norm: 21.85418713260774
Epoch: 7679, Batch Gradient Norm after: 21.378030512216217
Epoch 7680/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7039160966873169
Epoch: 7680, Batch Gradient Norm: 23.406025887627024
Epoch: 7680, Batch Gradient Norm after: 22.245785683714704
Epoch 7681/10000, Prediction Accuracy = 60.102%, Loss = 0.7081647753715515
Epoch: 7681, Batch Gradient Norm: 21.794903079043213
Epoch: 7681, Batch Gradient Norm after: 21.289714780738834
Epoch 7682/10000, Prediction Accuracy = 60.092%, Loss = 0.7036548376083374
Epoch: 7682, Batch Gradient Norm: 23.44004666380012
Epoch: 7682, Batch Gradient Norm after: 22.268523886673787
Epoch 7683/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7081874251365662
Epoch: 7683, Batch Gradient Norm: 21.900614703762315
Epoch: 7683, Batch Gradient Norm after: 21.41103938300884
Epoch 7684/10000, Prediction Accuracy = 60.06600000000001%, Loss = 0.7039025306701661
Epoch: 7684, Batch Gradient Norm: 23.424898307033633
Epoch: 7684, Batch Gradient Norm after: 22.259032772803515
Epoch 7685/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7081178069114685
Epoch: 7685, Batch Gradient Norm: 21.847949469827704
Epoch: 7685, Batch Gradient Norm after: 21.360110724712808
Epoch 7686/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7036827802658081
Epoch: 7686, Batch Gradient Norm: 23.42014812994906
Epoch: 7686, Batch Gradient Norm after: 22.247733340316827
Epoch 7687/10000, Prediction Accuracy = 60.088%, Loss = 0.7080145478248596
Epoch: 7687, Batch Gradient Norm: 21.84595000405659
Epoch: 7687, Batch Gradient Norm after: 21.345584721700394
Epoch 7688/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7036467432975769
Epoch: 7688, Batch Gradient Norm: 23.419910161043337
Epoch: 7688, Batch Gradient Norm after: 22.247266612579224
Epoch 7689/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7080463528633117
Epoch: 7689, Batch Gradient Norm: 21.842078509523382
Epoch: 7689, Batch Gradient Norm after: 21.342704716893202
Epoch 7690/10000, Prediction Accuracy = 60.11%, Loss = 0.703638243675232
Epoch: 7690, Batch Gradient Norm: 23.418647937664367
Epoch: 7690, Batch Gradient Norm after: 22.250322832082244
Epoch 7691/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7079885959625244
Epoch: 7691, Batch Gradient Norm: 21.844796668634036
Epoch: 7691, Batch Gradient Norm after: 21.35037933593852
Epoch 7692/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7035297751426697
Epoch: 7692, Batch Gradient Norm: 23.421230799980332
Epoch: 7692, Batch Gradient Norm after: 22.248898300325976
Epoch 7693/10000, Prediction Accuracy = 60.11%, Loss = 0.7078622579574585
Epoch: 7693, Batch Gradient Norm: 21.86010431929062
Epoch: 7693, Batch Gradient Norm after: 21.363228684200028
Epoch 7694/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.7035222053527832
Epoch: 7694, Batch Gradient Norm: 23.40976155210745
Epoch: 7694, Batch Gradient Norm after: 22.241851127622702
Epoch 7695/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7078390955924988
Epoch: 7695, Batch Gradient Norm: 21.803441680814984
Epoch: 7695, Batch Gradient Norm after: 21.295470742566764
Epoch 7696/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7033430814743042
Epoch: 7696, Batch Gradient Norm: 23.439750625555384
Epoch: 7696, Batch Gradient Norm after: 22.26674713517939
Epoch 7697/10000, Prediction Accuracy = 60.116%, Loss = 0.7078079104423523
Epoch: 7697, Batch Gradient Norm: 21.928183098567537
Epoch: 7697, Batch Gradient Norm after: 21.42668190124801
Epoch 7698/10000, Prediction Accuracy = 60.116%, Loss = 0.7035552501678467
Epoch: 7698, Batch Gradient Norm: 23.440938551768912
Epoch: 7698, Batch Gradient Norm after: 22.271049476235923
Epoch 7699/10000, Prediction Accuracy = 60.134%, Loss = 0.707813274860382
Epoch: 7699, Batch Gradient Norm: 21.922992954696518
Epoch: 7699, Batch Gradient Norm after: 21.434439931902485
Epoch 7700/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7035917997360229
Epoch: 7700, Batch Gradient Norm: 23.418673179161786
Epoch: 7700, Batch Gradient Norm after: 22.263622880783387
Epoch 7701/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7077679395675659
Epoch: 7701, Batch Gradient Norm: 21.85671649856922
Epoch: 7701, Batch Gradient Norm after: 21.380761172797516
Epoch 7702/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7033026337623596
Epoch: 7702, Batch Gradient Norm: 23.401913998532496
Epoch: 7702, Batch Gradient Norm after: 22.246615545036995
Epoch 7703/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7075481295585633
Epoch: 7703, Batch Gradient Norm: 21.806568506617076
Epoch: 7703, Batch Gradient Norm after: 21.306174547397063
Epoch 7704/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7030691504478455
Epoch: 7704, Batch Gradient Norm: 23.422668418656976
Epoch: 7704, Batch Gradient Norm after: 22.26422041314468
Epoch 7705/10000, Prediction Accuracy = 60.148%, Loss = 0.707548177242279
Epoch: 7705, Batch Gradient Norm: 21.87042793451186
Epoch: 7705, Batch Gradient Norm after: 21.390003098886922
Epoch 7706/10000, Prediction Accuracy = 60.07800000000001%, Loss = 0.7032096982002258
Epoch: 7706, Batch Gradient Norm: 23.399507075392297
Epoch: 7706, Batch Gradient Norm after: 22.247536848579728
Epoch 7707/10000, Prediction Accuracy = 60.134%, Loss = 0.7074385046958923
Epoch: 7707, Batch Gradient Norm: 21.793934456792176
Epoch: 7707, Batch Gradient Norm after: 21.293846956901362
Epoch 7708/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.702923309803009
Epoch: 7708, Batch Gradient Norm: 23.42757916470707
Epoch: 7708, Batch Gradient Norm after: 22.269834062490965
Epoch 7709/10000, Prediction Accuracy = 60.1%, Loss = 0.7074458956718445
Epoch: 7709, Batch Gradient Norm: 21.902172022787795
Epoch: 7709, Batch Gradient Norm after: 21.42023660786253
Epoch 7710/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7031851053237915
Epoch: 7710, Batch Gradient Norm: 23.40510515636923
Epoch: 7710, Batch Gradient Norm after: 22.256222788567555
Epoch 7711/10000, Prediction Accuracy = 60.148%, Loss = 0.70741947889328
Epoch: 7711, Batch Gradient Norm: 21.816555168737686
Epoch: 7711, Batch Gradient Norm after: 21.329409931482605
Epoch 7712/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7029410481452942
Epoch: 7712, Batch Gradient Norm: 23.40761944251647
Epoch: 7712, Batch Gradient Norm after: 22.263390947255683
Epoch 7713/10000, Prediction Accuracy = 60.124%, Loss = 0.7073437213897705
Epoch: 7713, Batch Gradient Norm: 21.848418746911204
Epoch: 7713, Batch Gradient Norm after: 21.373547059521208
Epoch 7714/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.7029067277908325
Epoch: 7714, Batch Gradient Norm: 23.396930924523566
Epoch: 7714, Batch Gradient Norm after: 22.25184674491691
Epoch 7715/10000, Prediction Accuracy = 60.136%, Loss = 0.7071900963783264
Epoch: 7715, Batch Gradient Norm: 21.798044033556252
Epoch: 7715, Batch Gradient Norm after: 21.307957290989926
Epoch 7716/10000, Prediction Accuracy = 60.074%, Loss = 0.702743399143219
Epoch: 7716, Batch Gradient Norm: 23.412778128534693
Epoch: 7716, Batch Gradient Norm after: 22.269446542013245
Epoch 7717/10000, Prediction Accuracy = 60.126%, Loss = 0.7072405934333801
Epoch: 7717, Batch Gradient Norm: 21.846806860319827
Epoch: 7717, Batch Gradient Norm after: 21.38080720516534
Epoch 7718/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.7028280973434449
Epoch: 7718, Batch Gradient Norm: 23.389704124421765
Epoch: 7718, Batch Gradient Norm after: 22.249484524404888
Epoch 7719/10000, Prediction Accuracy = 60.124%, Loss = 0.7070534110069275
Epoch: 7719, Batch Gradient Norm: 21.774582918520075
Epoch: 7719, Batch Gradient Norm after: 21.27760522107828
Epoch 7720/10000, Prediction Accuracy = 60.126%, Loss = 0.7025466084480285
Epoch: 7720, Batch Gradient Norm: 23.421363231765863
Epoch: 7720, Batch Gradient Norm after: 22.279246302453302
Epoch 7721/10000, Prediction Accuracy = 60.134%, Loss = 0.7071778893470764
Epoch: 7721, Batch Gradient Norm: 21.876654435413368
Epoch: 7721, Batch Gradient Norm after: 21.420643983499243
Epoch 7722/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.7028681993484497
Epoch: 7722, Batch Gradient Norm: 23.370339417149797
Epoch: 7722, Batch Gradient Norm after: 22.243568342185224
Epoch 7723/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7070356130599975
Epoch: 7723, Batch Gradient Norm: 21.70534403848516
Epoch: 7723, Batch Gradient Norm after: 21.197747903523034
Epoch 7724/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7022953629493713
Epoch: 7724, Batch Gradient Norm: 23.44098907294282
Epoch: 7724, Batch Gradient Norm after: 22.308323505554963
Epoch 7725/10000, Prediction Accuracy = 60.102%, Loss = 0.7070692181587219
Epoch: 7725, Batch Gradient Norm: 21.967712109180948
Epoch: 7725, Batch Gradient Norm after: 21.51396989400688
Epoch 7726/10000, Prediction Accuracy = 60.088%, Loss = 0.7028937697410583
Epoch: 7726, Batch Gradient Norm: 23.397625461841212
Epoch: 7726, Batch Gradient Norm after: 22.279342872737256
Epoch 7727/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7069090843200684
Epoch: 7727, Batch Gradient Norm: 21.80566025923022
Epoch: 7727, Batch Gradient Norm after: 21.35014882126502
Epoch 7728/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.7024463653564453
Epoch: 7728, Batch Gradient Norm: 23.380078369718422
Epoch: 7728, Batch Gradient Norm after: 22.268578275063295
Epoch 7729/10000, Prediction Accuracy = 60.114%, Loss = 0.7068016171455384
Epoch: 7729, Batch Gradient Norm: 21.74730114366548
Epoch: 7729, Batch Gradient Norm after: 21.27494265930813
Epoch 7730/10000, Prediction Accuracy = 60.104%, Loss = 0.7021952033042907
Epoch: 7730, Batch Gradient Norm: 23.405847386063485
Epoch: 7730, Batch Gradient Norm after: 22.290829483547952
Epoch 7731/10000, Prediction Accuracy = 60.13599999999999%, Loss = 0.706803297996521
Epoch: 7731, Batch Gradient Norm: 21.839998205624674
Epoch: 7731, Batch Gradient Norm after: 21.399159692428118
Epoch 7732/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7024397492408753
Epoch: 7732, Batch Gradient Norm: 23.359043534929757
Epoch: 7732, Batch Gradient Norm after: 22.25442598335492
Epoch 7733/10000, Prediction Accuracy = 60.136%, Loss = 0.706703495979309
Epoch: 7733, Batch Gradient Norm: 21.678689493641482
Epoch: 7733, Batch Gradient Norm after: 21.18613834029182
Epoch 7734/10000, Prediction Accuracy = 60.128%, Loss = 0.7019764542579651
Epoch: 7734, Batch Gradient Norm: 23.431748038919384
Epoch: 7734, Batch Gradient Norm after: 22.322555948981407
Epoch 7735/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7068122863769531
Epoch: 7735, Batch Gradient Norm: 21.95074032762252
Epoch: 7735, Batch Gradient Norm after: 21.525384068933302
Epoch 7736/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.702569580078125
Epoch: 7736, Batch Gradient Norm: 23.3613558920687
Epoch: 7736, Batch Gradient Norm after: 22.26800970115609
Epoch 7737/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7065088629722596
Epoch: 7737, Batch Gradient Norm: 21.708576461729425
Epoch: 7737, Batch Gradient Norm after: 21.235972449784185
Epoch 7738/10000, Prediction Accuracy = 60.098%, Loss = 0.7018966436386108
Epoch: 7738, Batch Gradient Norm: 23.40447386504501
Epoch: 7738, Batch Gradient Norm after: 22.308984935064196
Epoch 7739/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7066266655921936
Epoch: 7739, Batch Gradient Norm: 21.843775866742256
Epoch: 7739, Batch Gradient Norm after: 21.42809672809799
Epoch 7740/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7021992206573486
Epoch: 7740, Batch Gradient Norm: 23.342465333731294
Epoch: 7740, Batch Gradient Norm after: 22.254408312461624
Epoch 7741/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7063310384750366
Epoch: 7741, Batch Gradient Norm: 21.643526462842843
Epoch: 7741, Batch Gradient Norm after: 21.151443819653757
Epoch 7742/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7016043424606323
Epoch: 7742, Batch Gradient Norm: 23.43062368024994
Epoch: 7742, Batch Gradient Norm after: 22.334207541398698
Epoch 7743/10000, Prediction Accuracy = 60.134%, Loss = 0.7066226363182068
Epoch: 7743, Batch Gradient Norm: 21.921244381400456
Epoch: 7743, Batch Gradient Norm after: 21.52532970351331
Epoch 7744/10000, Prediction Accuracy = 60.10999999999999%, Loss = 0.7023774743080139
Epoch: 7744, Batch Gradient Norm: 23.324890256298698
Epoch: 7744, Batch Gradient Norm after: 22.25312853712441
Epoch 7745/10000, Prediction Accuracy = 60.15%, Loss = 0.706304132938385
Epoch: 7745, Batch Gradient Norm: 21.59388466917433
Epoch: 7745, Batch Gradient Norm after: 21.098640619654816
Epoch 7746/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.7013874173164367
Epoch: 7746, Batch Gradient Norm: 23.373388750262183
Epoch: 7746, Batch Gradient Norm after: 22.303687780116196
Epoch 7747/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7062809109687805
Epoch: 7747, Batch Gradient Norm: 21.872707750871605
Epoch: 7747, Batch Gradient Norm after: 21.470762041719382
Epoch 7748/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.7020418405532837
Epoch: 7748, Batch Gradient Norm: 23.318385115063524
Epoch: 7748, Batch Gradient Norm after: 22.24094368097068
Epoch 7749/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.7060858249664307
Epoch: 7749, Batch Gradient Norm: 21.564965132958974
Epoch: 7749, Batch Gradient Norm after: 21.055600559811886
Epoch 7750/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.7012115001678467
Epoch: 7750, Batch Gradient Norm: 23.338560521887967
Epoch: 7750, Batch Gradient Norm after: 22.279578596604804
Epoch 7751/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7060946106910706
Epoch: 7751, Batch Gradient Norm: 21.83053999331651
Epoch: 7751, Batch Gradient Norm after: 21.41184221479771
Epoch 7752/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.7017997145652771
Epoch: 7752, Batch Gradient Norm: 23.33951858798457
Epoch: 7752, Batch Gradient Norm after: 22.25661963573523
Epoch 7753/10000, Prediction Accuracy = 60.124%, Loss = 0.7060057997703553
Epoch: 7753, Batch Gradient Norm: 21.634823684448282
Epoch: 7753, Batch Gradient Norm after: 21.146333590055793
Epoch 7754/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7013112306594849
Epoch: 7754, Batch Gradient Norm: 23.42120947376642
Epoch: 7754, Batch Gradient Norm after: 22.335243722948128
Epoch 7755/10000, Prediction Accuracy = 60.134%, Loss = 0.7063211798667908
Epoch: 7755, Batch Gradient Norm: 21.90174401248331
Epoch: 7755, Batch Gradient Norm after: 21.51730207894068
Epoch 7756/10000, Prediction Accuracy = 60.128%, Loss = 0.7019636034965515
Epoch: 7756, Batch Gradient Norm: 23.306457412578233
Epoch: 7756, Batch Gradient Norm after: 22.24227688537643
Epoch 7757/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7058412194252014
Epoch: 7757, Batch Gradient Norm: 21.54560093333778
Epoch: 7757, Batch Gradient Norm after: 21.035597346141405
Epoch 7758/10000, Prediction Accuracy = 60.126%, Loss = 0.7008956432342529
Epoch: 7758, Batch Gradient Norm: 23.30747973571398
Epoch: 7758, Batch Gradient Norm after: 22.25911710522706
Epoch 7759/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7057549715042114
Epoch: 7759, Batch Gradient Norm: 21.790043122052378
Epoch: 7759, Batch Gradient Norm after: 21.35917112850712
Epoch 7760/10000, Prediction Accuracy = 60.104%, Loss = 0.7015066266059875
Epoch: 7760, Batch Gradient Norm: 23.350282419222232
Epoch: 7760, Batch Gradient Norm after: 22.27500246583239
Epoch 7761/10000, Prediction Accuracy = 60.122%, Loss = 0.7058551907539368
Epoch: 7761, Batch Gradient Norm: 21.677513605192505
Epoch: 7761, Batch Gradient Norm after: 21.21548056598437
Epoch 7762/10000, Prediction Accuracy = 60.112%, Loss = 0.7011355876922607
Epoch: 7762, Batch Gradient Norm: 23.399512350372326
Epoch: 7762, Batch Gradient Norm after: 22.319574772155455
Epoch 7763/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.7058983325958252
Epoch: 7763, Batch Gradient Norm: 21.836975192492858
Epoch: 7763, Batch Gradient Norm after: 21.43515210355488
Epoch 7764/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7015074014663696
Epoch: 7764, Batch Gradient Norm: 23.322886360616668
Epoch: 7764, Batch Gradient Norm after: 22.258143275651086
Epoch 7765/10000, Prediction Accuracy = 60.138%, Loss = 0.7056956171989441
Epoch: 7765, Batch Gradient Norm: 21.58357776557079
Epoch: 7765, Batch Gradient Norm after: 21.09748809723219
Epoch 7766/10000, Prediction Accuracy = 60.124%, Loss = 0.7008650898933411
Epoch: 7766, Batch Gradient Norm: 23.327293342239322
Epoch: 7766, Batch Gradient Norm after: 22.28085233776424
Epoch 7767/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7056978464126586
Epoch: 7767, Batch Gradient Norm: 21.777982054086873
Epoch: 7767, Batch Gradient Norm after: 21.361603363630383
Epoch 7768/10000, Prediction Accuracy = 60.134%, Loss = 0.701266610622406
Epoch: 7768, Batch Gradient Norm: 23.339216688024752
Epoch: 7768, Batch Gradient Norm after: 22.279972409906527
Epoch 7769/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7055779099464417
Epoch: 7769, Batch Gradient Norm: 21.666672844164733
Epoch: 7769, Batch Gradient Norm after: 21.209681464517438
Epoch 7770/10000, Prediction Accuracy = 60.08399999999999%, Loss = 0.7008919477462768
Epoch: 7770, Batch Gradient Norm: 23.383977092162105
Epoch: 7770, Batch Gradient Norm after: 22.32350946873456
Epoch 7771/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.7057126522064209
Epoch: 7771, Batch Gradient Norm: 21.80998874661821
Epoch: 7771, Batch Gradient Norm after: 21.413840278677064
Epoch 7772/10000, Prediction Accuracy = 60.122%, Loss = 0.7012518048286438
Epoch: 7772, Batch Gradient Norm: 23.31129981606039
Epoch: 7772, Batch Gradient Norm after: 22.267541006186317
Epoch 7773/10000, Prediction Accuracy = 60.112%, Loss = 0.7054194927215576
Epoch: 7773, Batch Gradient Norm: 21.580410754329282
Epoch: 7773, Batch Gradient Norm after: 21.105649859853994
Epoch 7774/10000, Prediction Accuracy = 60.112%, Loss = 0.7005338549613953
Epoch: 7774, Batch Gradient Norm: 23.30087858954747
Epoch: 7774, Batch Gradient Norm after: 22.268070983233716
Epoch 7775/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7053218960762024
Epoch: 7775, Batch Gradient Norm: 21.738341500410503
Epoch: 7775, Batch Gradient Norm after: 21.309744600791525
Epoch 7776/10000, Prediction Accuracy = 60.136%, Loss = 0.7009750604629517
Epoch: 7776, Batch Gradient Norm: 23.351538938957916
Epoch: 7776, Batch Gradient Norm after: 22.299631807835482
Epoch 7777/10000, Prediction Accuracy = 60.14000000000001%, Loss = 0.7055441856384277
Epoch: 7777, Batch Gradient Norm: 21.689371580136886
Epoch: 7777, Batch Gradient Norm after: 21.263833076819765
Epoch 7778/10000, Prediction Accuracy = 60.128%, Loss = 0.7008065581321716
Epoch: 7778, Batch Gradient Norm: 23.366254094866804
Epoch: 7778, Batch Gradient Norm after: 22.32138450624917
Epoch 7779/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7054298639297485
Epoch: 7779, Batch Gradient Norm: 21.763059572915623
Epoch: 7779, Batch Gradient Norm after: 21.36476344990547
Epoch 7780/10000, Prediction Accuracy = 60.098%, Loss = 0.7008553624153138
Epoch: 7780, Batch Gradient Norm: 23.325158973347182
Epoch: 7780, Batch Gradient Norm after: 22.28885863274793
Epoch 7781/10000, Prediction Accuracy = 60.136%, Loss = 0.7052098631858825
Epoch: 7781, Batch Gradient Norm: 21.619208069090575
Epoch: 7781, Batch Gradient Norm after: 21.17571737058353
Epoch 7782/10000, Prediction Accuracy = 60.1%, Loss = 0.7004535555839538
Epoch: 7782, Batch Gradient Norm: 23.287660100363595
Epoch: 7782, Batch Gradient Norm after: 22.274919025258612
Epoch 7783/10000, Prediction Accuracy = 60.116%, Loss = 0.7051056027412415
Epoch: 7783, Batch Gradient Norm: 21.65841701891024
Epoch: 7783, Batch Gradient Norm after: 21.22848033795383
Epoch 7784/10000, Prediction Accuracy = 60.1%, Loss = 0.7004794955253602
Epoch: 7784, Batch Gradient Norm: 23.35197659675838
Epoch: 7784, Batch Gradient Norm after: 22.31641365935617
Epoch 7785/10000, Prediction Accuracy = 60.15%, Loss = 0.7051767587661744
Epoch: 7785, Batch Gradient Norm: 21.74265319812702
Epoch: 7785, Batch Gradient Norm after: 21.343451842371383
Epoch 7786/10000, Prediction Accuracy = 60.148%, Loss = 0.7006515145301819
Epoch: 7786, Batch Gradient Norm: 23.330386766228155
Epoch: 7786, Batch Gradient Norm after: 22.299331809752882
Epoch 7787/10000, Prediction Accuracy = 60.14%, Loss = 0.7051337242126465
Epoch: 7787, Batch Gradient Norm: 21.62554770215081
Epoch: 7787, Batch Gradient Norm after: 21.1959193462004
Epoch 7788/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7003678321838379
Epoch: 7788, Batch Gradient Norm: 23.256693248314
Epoch: 7788, Batch Gradient Norm after: 22.26037581156977
Epoch 7789/10000, Prediction Accuracy = 60.116%, Loss = 0.7049121737480164
Epoch: 7789, Batch Gradient Norm: 21.59924567098415
Epoch: 7789, Batch Gradient Norm after: 21.156445744532174
Epoch 7790/10000, Prediction Accuracy = 60.134%, Loss = 0.7001826643943787
Epoch: 7790, Batch Gradient Norm: 23.247753663847543
Epoch: 7790, Batch Gradient Norm after: 22.249078746207392
Epoch 7791/10000, Prediction Accuracy = 60.174%, Loss = 0.7047279238700866
Epoch: 7791, Batch Gradient Norm: 21.621405799964396
Epoch: 7791, Batch Gradient Norm after: 21.17437161824286
Epoch 7792/10000, Prediction Accuracy = 60.1%, Loss = 0.7001658320426941
Epoch: 7792, Batch Gradient Norm: 23.29601175950581
Epoch: 7792, Batch Gradient Norm after: 22.28063954866109
Epoch 7793/10000, Prediction Accuracy = 60.146%, Loss = 0.7048590779304504
Epoch: 7793, Batch Gradient Norm: 21.678907894196353
Epoch: 7793, Batch Gradient Norm after: 21.254326052525492
Epoch 7794/10000, Prediction Accuracy = 60.104%, Loss = 0.7002888798713685
Epoch: 7794, Batch Gradient Norm: 23.347895947006414
Epoch: 7794, Batch Gradient Norm after: 22.321376909060913
Epoch 7795/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.7049442648887634
Epoch: 7795, Batch Gradient Norm: 21.709903698799458
Epoch: 7795, Batch Gradient Norm after: 21.307699257180484
Epoch 7796/10000, Prediction Accuracy = 60.098%, Loss = 0.7002729892730712
Epoch: 7796, Batch Gradient Norm: 23.33328966193687
Epoch: 7796, Batch Gradient Norm after: 22.309831775051006
Epoch 7797/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7048279762268066
Epoch: 7797, Batch Gradient Norm: 21.642581806533354
Epoch: 7797, Batch Gradient Norm after: 21.223841402622682
Epoch 7798/10000, Prediction Accuracy = 60.138%, Loss = 0.7001292705535889
Epoch: 7798, Batch Gradient Norm: 23.26815735818865
Epoch: 7798, Batch Gradient Norm after: 22.272542956896615
Epoch 7799/10000, Prediction Accuracy = 60.126%, Loss = 0.7047171473503113
Epoch: 7799, Batch Gradient Norm: 21.57380873868242
Epoch: 7799, Batch Gradient Norm after: 21.136774818780008
Epoch 7800/10000, Prediction Accuracy = 60.122%, Loss = 0.6999003648757934
Epoch: 7800, Batch Gradient Norm: 23.17957023914019
Epoch: 7800, Batch Gradient Norm after: 22.211561229373356
Epoch 7801/10000, Prediction Accuracy = 60.14%, Loss = 0.7043161988258362
Epoch: 7801, Batch Gradient Norm: 21.512176118975354
Epoch: 7801, Batch Gradient Norm after: 21.0334346346853
Epoch 7802/10000, Prediction Accuracy = 60.11%, Loss = 0.6996012091636657
Epoch: 7802, Batch Gradient Norm: 23.157953015479936
Epoch: 7802, Batch Gradient Norm after: 22.184557967688345
Epoch 7803/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.7041546940803528
Epoch: 7803, Batch Gradient Norm: 21.53392561697575
Epoch: 7803, Batch Gradient Norm after: 21.043248093966543
Epoch 7804/10000, Prediction Accuracy = 60.09400000000001%, Loss = 0.6996406197547913
Epoch: 7804, Batch Gradient Norm: 23.21536162807048
Epoch: 7804, Batch Gradient Norm after: 22.21850605287786
Epoch 7805/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7043191432952881
Epoch: 7805, Batch Gradient Norm: 21.62543627840084
Epoch: 7805, Batch Gradient Norm after: 21.161616486373173
Epoch 7806/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.6997958540916442
Epoch: 7806, Batch Gradient Norm: 23.34199554731833
Epoch: 7806, Batch Gradient Norm after: 22.301722212003106
Epoch 7807/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7045584440231323
Epoch: 7807, Batch Gradient Norm: 21.771456758331194
Epoch: 7807, Batch Gradient Norm after: 21.36646424531319
Epoch 7808/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7001360297203064
Epoch: 7808, Batch Gradient Norm: 23.315309867660307
Epoch: 7808, Batch Gradient Norm after: 22.283040011117386
Epoch 7809/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.7045195698738098
Epoch: 7809, Batch Gradient Norm: 21.592522125204685
Epoch: 7809, Batch Gradient Norm after: 21.13958218391332
Epoch 7810/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6996880531311035
Epoch: 7810, Batch Gradient Norm: 23.234100504441745
Epoch: 7810, Batch Gradient Norm after: 22.241755248696183
Epoch 7811/10000, Prediction Accuracy = 60.102%, Loss = 0.704252552986145
Epoch: 7811, Batch Gradient Norm: 21.59707519312088
Epoch: 7811, Batch Gradient Norm after: 21.14287280985421
Epoch 7812/10000, Prediction Accuracy = 60.126%, Loss = 0.6995814442634583
Epoch: 7812, Batch Gradient Norm: 23.249531763646342
Epoch: 7812, Batch Gradient Norm after: 22.250819790345663
Epoch 7813/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.704154121875763
Epoch: 7813, Batch Gradient Norm: 21.626401948787056
Epoch: 7813, Batch Gradient Norm after: 21.179869501282113
Epoch 7814/10000, Prediction Accuracy = 60.116%, Loss = 0.6995888352394104
Epoch: 7814, Batch Gradient Norm: 23.28285821751884
Epoch: 7814, Batch Gradient Norm after: 22.274752293913952
Epoch 7815/10000, Prediction Accuracy = 60.15400000000001%, Loss = 0.7042388558387757
Epoch: 7815, Batch Gradient Norm: 21.654362241440055
Epoch: 7815, Batch Gradient Norm after: 21.22443208432475
Epoch 7816/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.69963458776474
Epoch: 7816, Batch Gradient Norm: 23.297414547480116
Epoch: 7816, Batch Gradient Norm after: 22.29063058533902
Epoch 7817/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.704216206073761
Epoch: 7817, Batch Gradient Norm: 21.651043312280706
Epoch: 7817, Batch Gradient Norm after: 21.228573741860707
Epoch 7818/10000, Prediction Accuracy = 60.116%, Loss = 0.6995319128036499
Epoch: 7818, Batch Gradient Norm: 23.284654017474157
Epoch: 7818, Batch Gradient Norm after: 22.283328244473168
Epoch 7819/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.7041121244430542
Epoch: 7819, Batch Gradient Norm: 21.625027007910298
Epoch: 7819, Batch Gradient Norm after: 21.19458058608255
Epoch 7820/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.699475884437561
Epoch: 7820, Batch Gradient Norm: 23.228179592066407
Epoch: 7820, Batch Gradient Norm after: 22.25083480391069
Epoch 7821/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7040203094482422
Epoch: 7821, Batch Gradient Norm: 21.53965922604382
Epoch: 7821, Batch Gradient Norm after: 21.085021171506995
Epoch 7822/10000, Prediction Accuracy = 60.146%, Loss = 0.6992135643959045
Epoch: 7822, Batch Gradient Norm: 23.124808469674676
Epoch: 7822, Batch Gradient Norm after: 22.17699871300761
Epoch 7823/10000, Prediction Accuracy = 60.136%, Loss = 0.7035812616348267
Epoch: 7823, Batch Gradient Norm: 21.452411420824586
Epoch: 7823, Batch Gradient Norm after: 20.947830459715217
Epoch 7824/10000, Prediction Accuracy = 60.126%, Loss = 0.6988571405410766
Epoch: 7824, Batch Gradient Norm: 23.07362074767088
Epoch: 7824, Batch Gradient Norm after: 22.12744942593869
Epoch 7825/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.7033299326896667
Epoch: 7825, Batch Gradient Norm: 21.450521950859745
Epoch: 7825, Batch Gradient Norm after: 20.917989876128996
Epoch 7826/10000, Prediction Accuracy = 60.104%, Loss = 0.6988298058509826
Epoch: 7826, Batch Gradient Norm: 23.127057560121585
Epoch: 7826, Batch Gradient Norm after: 22.1539228983606
Epoch 7827/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7034782528877258
Epoch: 7827, Batch Gradient Norm: 21.55750676113136
Epoch: 7827, Batch Gradient Norm after: 21.04878231607204
Epoch 7828/10000, Prediction Accuracy = 60.122%, Loss = 0.6990306496620178
Epoch: 7828, Batch Gradient Norm: 23.285701057840054
Epoch: 7828, Batch Gradient Norm after: 22.258636111894685
Epoch 7829/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7038140416145324
Epoch: 7829, Batch Gradient Norm: 21.760536031999447
Epoch: 7829, Batch Gradient Norm after: 21.32776862785239
Epoch 7830/10000, Prediction Accuracy = 60.148%, Loss = 0.6995107769966126
Epoch: 7830, Batch Gradient Norm: 23.330990335151462
Epoch: 7830, Batch Gradient Norm after: 22.28744573704234
Epoch 7831/10000, Prediction Accuracy = 60.128%, Loss = 0.7039786815643311
Epoch: 7831, Batch Gradient Norm: 21.656559614772664
Epoch: 7831, Batch Gradient Norm after: 21.21257230632062
Epoch 7832/10000, Prediction Accuracy = 60.162%, Loss = 0.6992726683616638
Epoch: 7832, Batch Gradient Norm: 23.30984032842896
Epoch: 7832, Batch Gradient Norm after: 22.295753055422114
Epoch 7833/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.7039116501808167
Epoch: 7833, Batch Gradient Norm: 21.689542108823872
Epoch: 7833, Batch Gradient Norm after: 21.273870063670838
Epoch 7834/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.6992390155792236
Epoch: 7834, Batch Gradient Norm: 23.31422076809523
Epoch: 7834, Batch Gradient Norm after: 22.307647026700042
Epoch 7835/10000, Prediction Accuracy = 60.14%, Loss = 0.7037737011909485
Epoch: 7835, Batch Gradient Norm: 21.65751011928473
Epoch: 7835, Batch Gradient Norm after: 21.24447172065257
Epoch 7836/10000, Prediction Accuracy = 60.138%, Loss = 0.6990885019302369
Epoch: 7836, Batch Gradient Norm: 23.246452386422682
Epoch: 7836, Batch Gradient Norm after: 22.26725718843702
Epoch 7837/10000, Prediction Accuracy = 60.164%, Loss = 0.7035604834556579
Epoch: 7837, Batch Gradient Norm: 21.537741124546027
Epoch: 7837, Batch Gradient Norm after: 21.094107640126158
Epoch 7838/10000, Prediction Accuracy = 60.116%, Loss = 0.6987506151199341
Epoch: 7838, Batch Gradient Norm: 23.099486423591333
Epoch: 7838, Batch Gradient Norm after: 22.166254677429286
Epoch 7839/10000, Prediction Accuracy = 60.162%, Loss = 0.7030672073364258
Epoch: 7839, Batch Gradient Norm: 21.40418110151262
Epoch: 7839, Batch Gradient Norm after: 20.888568858285467
Epoch 7840/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.6982924938201904
Epoch: 7840, Batch Gradient Norm: 22.993327219291242
Epoch: 7840, Batch Gradient Norm after: 22.077117434785496
Epoch 7841/10000, Prediction Accuracy = 60.136%, Loss = 0.7026789665222168
Epoch: 7841, Batch Gradient Norm: 21.366212807616478
Epoch: 7841, Batch Gradient Norm after: 20.79251309172977
Epoch 7842/10000, Prediction Accuracy = 60.158%, Loss = 0.6981920957565307
Epoch: 7842, Batch Gradient Norm: 23.010716607234926
Epoch: 7842, Batch Gradient Norm after: 22.0745518882993
Epoch 7843/10000, Prediction Accuracy = 60.134%, Loss = 0.7027887821197509
Epoch: 7843, Batch Gradient Norm: 21.449054269400392
Epoch: 7843, Batch Gradient Norm after: 20.88346886274235
Epoch 7844/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6983745455741882
Epoch: 7844, Batch Gradient Norm: 23.15809600759654
Epoch: 7844, Batch Gradient Norm after: 22.166668173451104
Epoch 7845/10000, Prediction Accuracy = 60.124%, Loss = 0.7030805706977844
Epoch: 7845, Batch Gradient Norm: 21.666408734968133
Epoch: 7845, Batch Gradient Norm after: 21.172926043861487
Epoch 7846/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6988137483596801
Epoch: 7846, Batch Gradient Norm: 23.389062573992238
Epoch: 7846, Batch Gradient Norm after: 22.32592981275571
Epoch 7847/10000, Prediction Accuracy = 60.178%, Loss = 0.7036648154258728
Epoch: 7847, Batch Gradient Norm: 21.869419576470207
Epoch: 7847, Batch Gradient Norm after: 21.480415176424305
Epoch 7848/10000, Prediction Accuracy = 60.114%, Loss = 0.6993542075157165
Epoch: 7848, Batch Gradient Norm: 23.27535021817801
Epoch: 7848, Batch Gradient Norm after: 22.249760020572644
Epoch 7849/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.70332852602005
Epoch: 7849, Batch Gradient Norm: 21.484831333402134
Epoch: 7849, Batch Gradient Norm after: 20.98760774406348
Epoch 7850/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.6982590675354003
Epoch: 7850, Batch Gradient Norm: 23.107752624914003
Epoch: 7850, Batch Gradient Norm after: 22.15406210849431
Epoch 7851/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.7027279615402222
Epoch: 7851, Batch Gradient Norm: 21.516364133896246
Epoch: 7851, Batch Gradient Norm after: 21.00317537097688
Epoch 7852/10000, Prediction Accuracy = 60.152%, Loss = 0.6982808589935303
Epoch: 7852, Batch Gradient Norm: 23.174359297552336
Epoch: 7852, Batch Gradient Norm after: 22.194690481339432
Epoch 7853/10000, Prediction Accuracy = 60.122%, Loss = 0.7029563188552856
Epoch: 7853, Batch Gradient Norm: 21.61265611942882
Epoch: 7853, Batch Gradient Norm after: 21.13558486517314
Epoch 7854/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6985495090484619
Epoch: 7854, Batch Gradient Norm: 23.27260153839073
Epoch: 7854, Batch Gradient Norm after: 22.269918200019195
Epoch 7855/10000, Prediction Accuracy = 60.104%, Loss = 0.7032067179679871
Epoch: 7855, Batch Gradient Norm: 21.680557120517726
Epoch: 7855, Batch Gradient Norm after: 21.250196752785502
Epoch 7856/10000, Prediction Accuracy = 60.15%, Loss = 0.6986118674278259
Epoch: 7856, Batch Gradient Norm: 23.314825876906898
Epoch: 7856, Batch Gradient Norm after: 22.307347315642797
Epoch 7857/10000, Prediction Accuracy = 60.136%, Loss = 0.7031826496124267
Epoch: 7857, Batch Gradient Norm: 21.684231958434513
Epoch: 7857, Batch Gradient Norm after: 21.27456747217056
Epoch 7858/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6985663533210754
Epoch: 7858, Batch Gradient Norm: 23.270957081293773
Epoch: 7858, Batch Gradient Norm after: 22.287928762322945
Epoch 7859/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.7030568838119506
Epoch: 7859, Batch Gradient Norm: 21.576831831756063
Epoch: 7859, Batch Gradient Norm after: 21.147638955055513
Epoch 7860/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.6982573747634888
Epoch: 7860, Batch Gradient Norm: 23.103331675065718
Epoch: 7860, Batch Gradient Norm after: 22.177159054432813
Epoch 7861/10000, Prediction Accuracy = 60.138%, Loss = 0.7024857878684998
Epoch: 7861, Batch Gradient Norm: 21.39144626735683
Epoch: 7861, Batch Gradient Norm after: 20.882811562581765
Epoch 7862/10000, Prediction Accuracy = 60.114%, Loss = 0.6976662158966065
Epoch: 7862, Batch Gradient Norm: 22.939735490689046
Epoch: 7862, Batch Gradient Norm after: 22.04814608276631
Epoch 7863/10000, Prediction Accuracy = 60.128%, Loss = 0.7019468426704407
Epoch: 7863, Batch Gradient Norm: 21.286490528333957
Epoch: 7863, Batch Gradient Norm after: 20.691701175476144
Epoch 7864/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6974100232124328
Epoch: 7864, Batch Gradient Norm: 22.891143581519195
Epoch: 7864, Batch Gradient Norm after: 21.996253644158646
Epoch 7865/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.7018610954284668
Epoch: 7865, Batch Gradient Norm: 21.321210400520293
Epoch: 7865, Batch Gradient Norm after: 20.70028842241336
Epoch 7866/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6974357962608337
Epoch: 7866, Batch Gradient Norm: 23.009528543923086
Epoch: 7866, Batch Gradient Norm after: 22.061648921957055
Epoch 7867/10000, Prediction Accuracy = 60.128%, Loss = 0.7020384073257446
Epoch: 7867, Batch Gradient Norm: 21.538086189137832
Epoch: 7867, Batch Gradient Norm after: 20.97595263836041
Epoch 7868/10000, Prediction Accuracy = 60.102%, Loss = 0.6978940725326538
Epoch: 7868, Batch Gradient Norm: 23.318919568934238
Epoch: 7868, Batch Gradient Norm after: 22.27160911964167
Epoch 7869/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.7028780579566956
Epoch: 7869, Batch Gradient Norm: 21.87949248775772
Epoch: 7869, Batch Gradient Norm after: 21.466714375257176
Epoch 7870/10000, Prediction Accuracy = 60.11400000000001%, Loss = 0.6987904071807861
Epoch: 7870, Batch Gradient Norm: 23.281521799212165
Epoch: 7870, Batch Gradient Norm after: 22.24303893379901
Epoch 7871/10000, Prediction Accuracy = 60.15%, Loss = 0.7027645945549011
Epoch: 7871, Batch Gradient Norm: 21.541929812361996
Epoch: 7871, Batch Gradient Norm after: 21.040488053566044
Epoch 7872/10000, Prediction Accuracy = 60.126%, Loss = 0.6978074550628662
Epoch: 7872, Batch Gradient Norm: 23.19965635918644
Epoch: 7872, Batch Gradient Norm after: 22.214304924216773
Epoch 7873/10000, Prediction Accuracy = 60.15%, Loss = 0.7024142980575562
Epoch: 7873, Batch Gradient Norm: 21.640701468556706
Epoch: 7873, Batch Gradient Norm after: 21.175120131343764
Epoch 7874/10000, Prediction Accuracy = 60.164%, Loss = 0.6980207920074463
Epoch: 7874, Batch Gradient Norm: 23.30156697976905
Epoch: 7874, Batch Gradient Norm after: 22.291295047419
Epoch 7875/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.7027438402175903
Epoch: 7875, Batch Gradient Norm: 21.709911285027044
Epoch: 7875, Batch Gradient Norm after: 21.296532860724255
Epoch 7876/10000, Prediction Accuracy = 60.17%, Loss = 0.6982147574424744
Epoch: 7876, Batch Gradient Norm: 23.31097873900627
Epoch: 7876, Batch Gradient Norm after: 22.31521176302157
Epoch 7877/10000, Prediction Accuracy = 60.098%, Loss = 0.7027312159538269
Epoch: 7877, Batch Gradient Norm: 21.645610029277698
Epoch: 7877, Batch Gradient Norm after: 21.24106926464538
Epoch 7878/10000, Prediction Accuracy = 60.15%, Loss = 0.6979379177093505
Epoch: 7878, Batch Gradient Norm: 23.167973650607824
Epoch: 7878, Batch Gradient Norm after: 22.226502419079914
Epoch 7879/10000, Prediction Accuracy = 60.138%, Loss = 0.7021873116493225
Epoch: 7879, Batch Gradient Norm: 21.457077963625466
Epoch: 7879, Batch Gradient Norm after: 20.986676025263094
Epoch 7880/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.6973823785781861
Epoch: 7880, Batch Gradient Norm: 22.964880103236215
Epoch: 7880, Batch Gradient Norm after: 22.08080589414313
Epoch 7881/10000, Prediction Accuracy = 60.164%, Loss = 0.7015732288360595
Epoch: 7881, Batch Gradient Norm: 21.252479514471737
Epoch: 7881, Batch Gradient Norm after: 20.677970471935286
Epoch 7882/10000, Prediction Accuracy = 60.112%, Loss = 0.6968034386634827
Epoch: 7882, Batch Gradient Norm: 22.802275638291004
Epoch: 7882, Batch Gradient Norm after: 21.936811374956594
Epoch 7883/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.7010053396224976
Epoch: 7883, Batch Gradient Norm: 21.254170456144863
Epoch: 7883, Batch Gradient Norm after: 20.618540281380152
Epoch 7884/10000, Prediction Accuracy = 60.136%, Loss = 0.6967079162597656
Epoch: 7884, Batch Gradient Norm: 22.90800044609878
Epoch: 7884, Batch Gradient Norm after: 21.99554225250307
Epoch 7885/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.7012763381004333
Epoch: 7885, Batch Gradient Norm: 21.421598713623304
Epoch: 7885, Batch Gradient Norm after: 20.810097180461955
Epoch 7886/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6971798181533814
Epoch: 7886, Batch Gradient Norm: 23.164940883786564
Epoch: 7886, Batch Gradient Norm after: 22.16649236172442
Epoch 7887/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7020794987678528
Epoch: 7887, Batch Gradient Norm: 21.730005779325392
Epoch: 7887, Batch Gradient Norm after: 21.249704775103336
Epoch 7888/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.697928500175476
Epoch: 7888, Batch Gradient Norm: 23.354625169143297
Epoch: 7888, Batch Gradient Norm after: 22.302093142806303
Epoch 7889/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7024654507637024
Epoch: 7889, Batch Gradient Norm: 21.80723828441678
Epoch: 7889, Batch Gradient Norm after: 21.39154950358094
Epoch 7890/10000, Prediction Accuracy = 60.092%, Loss = 0.6980272889137268
Epoch: 7890, Batch Gradient Norm: 23.291744338372343
Epoch: 7890, Batch Gradient Norm after: 22.274607690081325
Epoch 7891/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7022603631019593
Epoch: 7891, Batch Gradient Norm: 21.59916787228016
Epoch: 7891, Batch Gradient Norm after: 21.14292770949066
Epoch 7892/10000, Prediction Accuracy = 60.104%, Loss = 0.6974799752235412
Epoch: 7892, Batch Gradient Norm: 23.174877851150082
Epoch: 7892, Batch Gradient Norm after: 22.220459826934977
Epoch 7893/10000, Prediction Accuracy = 60.128%, Loss = 0.7018887519836425
Epoch: 7893, Batch Gradient Norm: 21.534387589084343
Epoch: 7893, Batch Gradient Norm after: 21.067509144774796
Epoch 7894/10000, Prediction Accuracy = 60.146%, Loss = 0.6971928477287292
Epoch: 7894, Batch Gradient Norm: 23.090263643937107
Epoch: 7894, Batch Gradient Norm after: 22.162391406537868
Epoch 7895/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7015373945236206
Epoch: 7895, Batch Gradient Norm: 21.450101140577353
Epoch: 7895, Batch Gradient Norm after: 20.94137724625905
Epoch 7896/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6969434499740601
Epoch: 7896, Batch Gradient Norm: 23.007745193187663
Epoch: 7896, Batch Gradient Norm after: 22.099957147401465
Epoch 7897/10000, Prediction Accuracy = 60.122%, Loss = 0.7013321876525879
Epoch: 7897, Batch Gradient Norm: 21.373207101733033
Epoch: 7897, Batch Gradient Norm after: 20.824689487897572
Epoch 7898/10000, Prediction Accuracy = 60.152%, Loss = 0.6967401146888733
Epoch: 7898, Batch Gradient Norm: 22.944329735532865
Epoch: 7898, Batch Gradient Norm after: 22.049252619829375
Epoch 7899/10000, Prediction Accuracy = 60.10999999999999%, Loss = 0.7010663151741028
Epoch: 7899, Batch Gradient Norm: 21.362299530843977
Epoch: 7899, Batch Gradient Norm after: 20.783267636612376
Epoch 7900/10000, Prediction Accuracy = 60.15%, Loss = 0.696584963798523
Epoch: 7900, Batch Gradient Norm: 22.988404282959944
Epoch: 7900, Batch Gradient Norm after: 22.065793528419775
Epoch 7901/10000, Prediction Accuracy = 60.16199999999999%, Loss = 0.7010699510574341
Epoch: 7901, Batch Gradient Norm: 21.457066941594654
Epoch: 7901, Batch Gradient Norm after: 20.895971573138446
Epoch 7902/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.6967848539352417
Epoch: 7902, Batch Gradient Norm: 23.14398548707457
Epoch: 7902, Batch Gradient Norm after: 22.16833017988395
Epoch 7903/10000, Prediction Accuracy = 60.134%, Loss = 0.7015018582344055
Epoch: 7903, Batch Gradient Norm: 21.64637818401174
Epoch: 7903, Batch Gradient Norm after: 21.162194940258168
Epoch 7904/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.6972451090812684
Epoch: 7904, Batch Gradient Norm: 23.335195139205677
Epoch: 7904, Batch Gradient Norm after: 22.311363674371588
Epoch 7905/10000, Prediction Accuracy = 60.152%, Loss = 0.7019914150238037
Epoch: 7905, Batch Gradient Norm: 21.79848614604111
Epoch: 7905, Batch Gradient Norm after: 21.40672612073469
Epoch 7906/10000, Prediction Accuracy = 60.136%, Loss = 0.6975693464279175
Epoch: 7906, Batch Gradient Norm: 23.2793285326596
Epoch: 7906, Batch Gradient Norm after: 22.282006968175573
Epoch 7907/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.7017900347709656
Epoch: 7907, Batch Gradient Norm: 21.55764317133717
Epoch: 7907, Batch Gradient Norm after: 21.111283784543502
Epoch 7908/10000, Prediction Accuracy = 60.176%, Loss = 0.6969568729400635
Epoch: 7908, Batch Gradient Norm: 23.077943082065797
Epoch: 7908, Batch Gradient Norm after: 22.16643502366592
Epoch 7909/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7012704730033874
Epoch: 7909, Batch Gradient Norm: 21.378419004235706
Epoch: 7909, Batch Gradient Norm after: 20.870477246216662
Epoch 7910/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6964236974716187
Epoch: 7910, Batch Gradient Norm: 22.876945234370186
Epoch: 7910, Batch Gradient Norm after: 22.02087013383878
Epoch 7911/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.7005237817764283
Epoch: 7911, Batch Gradient Norm: 21.213971902719905
Epoch: 7911, Batch Gradient Norm after: 20.604904261017676
Epoch 7912/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.6958749890327454
Epoch: 7912, Batch Gradient Norm: 22.774779460849288
Epoch: 7912, Batch Gradient Norm after: 21.9043356426963
Epoch 7913/10000, Prediction Accuracy = 60.136%, Loss = 0.7001597642898559
Epoch: 7913, Batch Gradient Norm: 21.304780026536633
Epoch: 7913, Batch Gradient Norm after: 20.680858947222145
Epoch 7914/10000, Prediction Accuracy = 60.1%, Loss = 0.6961114525794982
Epoch: 7914, Batch Gradient Norm: 22.963939776048804
Epoch: 7914, Batch Gradient Norm after: 22.04035272717078
Epoch 7915/10000, Prediction Accuracy = 60.138%, Loss = 0.7006772279739379
Epoch: 7915, Batch Gradient Norm: 21.48007071295605
Epoch: 7915, Batch Gradient Norm after: 20.905101664037684
Epoch 7916/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6964552521705627
Epoch: 7916, Batch Gradient Norm: 23.207539503734502
Epoch: 7916, Batch Gradient Norm after: 22.206357888492825
Epoch 7917/10000, Prediction Accuracy = 60.146%, Loss = 0.7012841939926148
Epoch: 7917, Batch Gradient Norm: 21.772084029325754
Epoch: 7917, Batch Gradient Norm after: 21.3216292146094
Epoch 7918/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6972131729125977
Epoch: 7918, Batch Gradient Norm: 23.31352099032231
Epoch: 7918, Batch Gradient Norm after: 22.28538570203793
Epoch 7919/10000, Prediction Accuracy = 60.108000000000004%, Loss = 0.7016350030899048
Epoch: 7919, Batch Gradient Norm: 21.666242765392546
Epoch: 7919, Batch Gradient Norm after: 21.230933050331384
Epoch 7920/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6969450116157532
Epoch: 7920, Batch Gradient Norm: 23.265443134272083
Epoch: 7920, Batch Gradient Norm after: 22.285969672448534
Epoch 7921/10000, Prediction Accuracy = 60.122%, Loss = 0.7014418244361877
Epoch: 7921, Batch Gradient Norm: 21.643815408508775
Epoch: 7921, Batch Gradient Norm after: 21.23017643299231
Epoch 7922/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6967607140541077
Epoch: 7922, Batch Gradient Norm: 23.16834373837089
Epoch: 7922, Batch Gradient Norm after: 22.232297784366782
Epoch 7923/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7010389804840088
Epoch: 7923, Batch Gradient Norm: 21.474259421567332
Epoch: 7923, Batch Gradient Norm after: 21.016247990917986
Epoch 7924/10000, Prediction Accuracy = 60.114%, Loss = 0.6962610840797424
Epoch: 7924, Batch Gradient Norm: 22.94451418308526
Epoch: 7924, Batch Gradient Norm after: 22.078139247117743
Epoch 7925/10000, Prediction Accuracy = 60.148%, Loss = 0.7003682613372803
Epoch: 7925, Batch Gradient Norm: 21.217087386848867
Epoch: 7925, Batch Gradient Norm after: 20.64437902258756
Epoch 7926/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6955438733100892
Epoch: 7926, Batch Gradient Norm: 22.7070526510282
Epoch: 7926, Batch Gradient Norm after: 21.84360750167494
Epoch 7927/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6995827913284302
Epoch: 7927, Batch Gradient Norm: 21.360791035909195
Epoch: 7927, Batch Gradient Norm after: 20.786132253524258
Epoch 7928/10000, Prediction Accuracy = 60.15%, Loss = 0.6958175778388977
Epoch: 7928, Batch Gradient Norm: 22.972494009795366
Epoch: 7928, Batch Gradient Norm after: 22.061413531818598
Epoch 7929/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7003132581710816
Epoch: 7929, Batch Gradient Norm: 21.425241056689153
Epoch: 7929, Batch Gradient Norm after: 20.86265390992088
Epoch 7930/10000, Prediction Accuracy = 60.19%, Loss = 0.6960237145423889
Epoch: 7930, Batch Gradient Norm: 23.068821774096342
Epoch: 7930, Batch Gradient Norm after: 22.128530503485834
Epoch 7931/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.7006635904312134
Epoch: 7931, Batch Gradient Norm: 21.53475581782767
Epoch: 7931, Batch Gradient Norm after: 21.022574352811784
Epoch 7932/10000, Prediction Accuracy = 60.178%, Loss = 0.6962463736534119
Epoch: 7932, Batch Gradient Norm: 23.17053624207699
Epoch: 7932, Batch Gradient Norm after: 22.20564900776959
Epoch 7933/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.7007941603660583
Epoch: 7933, Batch Gradient Norm: 21.627933125671507
Epoch: 7933, Batch Gradient Norm after: 21.167939905833464
Epoch 7934/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6963921785354614
Epoch: 7934, Batch Gradient Norm: 23.233598962082343
Epoch: 7934, Batch Gradient Norm after: 22.26070241190283
Epoch 7935/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.7009569764137268
Epoch: 7935, Batch Gradient Norm: 21.61107177957864
Epoch: 7935, Batch Gradient Norm after: 21.182399988256677
Epoch 7936/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.6963639497756958
Epoch: 7936, Batch Gradient Norm: 23.131427832253653
Epoch: 7936, Batch Gradient Norm after: 22.207579232378777
Epoch 7937/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.700617504119873
Epoch: 7937, Batch Gradient Norm: 21.438648951351386
Epoch: 7937, Batch Gradient Norm after: 20.964193654248515
Epoch 7938/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6957846283912659
Epoch: 7938, Batch Gradient Norm: 22.923782913327702
Epoch: 7938, Batch Gradient Norm after: 22.06149293564598
Epoch 7939/10000, Prediction Accuracy = 60.146%, Loss = 0.6999110698699951
Epoch: 7939, Batch Gradient Norm: 21.228222048976377
Epoch: 7939, Batch Gradient Norm after: 20.647486915988583
Epoch 7940/10000, Prediction Accuracy = 60.202%, Loss = 0.6952144622802734
Epoch: 7940, Batch Gradient Norm: 22.747571763861885
Epoch: 7940, Batch Gradient Norm after: 21.890636879702598
Epoch 7941/10000, Prediction Accuracy = 60.122%, Loss = 0.6994329571723938
Epoch: 7941, Batch Gradient Norm: 21.292763907882065
Epoch: 7941, Batch Gradient Norm after: 20.696158565910224
Epoch 7942/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6953608632087708
Epoch: 7942, Batch Gradient Norm: 22.884361958087368
Epoch: 7942, Batch Gradient Norm after: 22.003274732849604
Epoch 7943/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6997330188751221
Epoch: 7943, Batch Gradient Norm: 21.33871988848665
Epoch: 7943, Batch Gradient Norm after: 20.7371502960322
Epoch 7944/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6953604340553283
Epoch: 7944, Batch Gradient Norm: 22.99474474087679
Epoch: 7944, Batch Gradient Norm after: 22.068606347926718
Epoch 7945/10000, Prediction Accuracy = 60.162%, Loss = 0.6999332904815674
Epoch: 7945, Batch Gradient Norm: 21.49680362936546
Epoch: 7945, Batch Gradient Norm after: 20.946478573845493
Epoch 7946/10000, Prediction Accuracy = 60.138%, Loss = 0.6957322955131531
Epoch: 7946, Batch Gradient Norm: 23.180788950861157
Epoch: 7946, Batch Gradient Norm after: 22.203344717800256
Epoch 7947/10000, Prediction Accuracy = 60.17%, Loss = 0.7004793167114258
Epoch: 7947, Batch Gradient Norm: 21.677774042855845
Epoch: 7947, Batch Gradient Norm after: 21.221988103275244
Epoch 7948/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.6961750149726867
Epoch: 7948, Batch Gradient Norm: 23.325598072997423
Epoch: 7948, Batch Gradient Norm after: 22.322480823715612
Epoch 7949/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.7008272767066955
Epoch: 7949, Batch Gradient Norm: 21.742002062930943
Epoch: 7949, Batch Gradient Norm after: 21.35846103328224
Epoch 7950/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6962702631950378
Epoch: 7950, Batch Gradient Norm: 23.273987840668067
Epoch: 7950, Batch Gradient Norm after: 22.306814461751426
Epoch 7951/10000, Prediction Accuracy = 60.138%, Loss = 0.7006649494171142
Epoch: 7951, Batch Gradient Norm: 21.546510686793965
Epoch: 7951, Batch Gradient Norm after: 21.13694534562074
Epoch 7952/10000, Prediction Accuracy = 60.178%, Loss = 0.6957866191864014
Epoch: 7952, Batch Gradient Norm: 22.987644206327094
Epoch: 7952, Batch Gradient Norm after: 22.126390189586882
Epoch 7953/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.6998627185821533
Epoch: 7953, Batch Gradient Norm: 21.201681489776124
Epoch: 7953, Batch Gradient Norm after: 20.66459271079822
Epoch 7954/10000, Prediction Accuracy = 60.182%, Loss = 0.6947890281677246
Epoch: 7954, Batch Gradient Norm: 22.63348217162635
Epoch: 7954, Batch Gradient Norm after: 21.7808380509355
Epoch 7955/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6986469626426697
Epoch: 7955, Batch Gradient Norm: 21.405060861925133
Epoch: 7955, Batch Gradient Norm after: 20.88614274340974
Epoch 7956/10000, Prediction Accuracy = 60.112%, Loss = 0.6952132701873779
Epoch: 7956, Batch Gradient Norm: 22.94522211956839
Epoch: 7956, Batch Gradient Norm after: 22.06609516137668
Epoch 7957/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6995208978652954
Epoch: 7957, Batch Gradient Norm: 21.314986792997498
Epoch: 7957, Batch Gradient Norm after: 20.752170327970806
Epoch 7958/10000, Prediction Accuracy = 60.12199999999999%, Loss = 0.6949772357940673
Epoch: 7958, Batch Gradient Norm: 22.8532010638327
Epoch: 7958, Batch Gradient Norm after: 21.997572417962303
Epoch 7959/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.699226725101471
Epoch: 7959, Batch Gradient Norm: 21.243163276443525
Epoch: 7959, Batch Gradient Norm after: 20.629976683156357
Epoch 7960/10000, Prediction Accuracy = 60.152%, Loss = 0.6946807742118836
Epoch: 7960, Batch Gradient Norm: 22.83544721179268
Epoch: 7960, Batch Gradient Norm after: 21.968334818720614
Epoch 7961/10000, Prediction Accuracy = 60.138%, Loss = 0.6990530014038085
Epoch: 7961, Batch Gradient Norm: 21.300775306871845
Epoch: 7961, Batch Gradient Norm after: 20.67634968065473
Epoch 7962/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.694810402393341
Epoch: 7962, Batch Gradient Norm: 22.961998068331035
Epoch: 7962, Batch Gradient Norm after: 22.047381449883055
Epoch 7963/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6994879841804504
Epoch: 7963, Batch Gradient Norm: 21.468472390128063
Epoch: 7963, Batch Gradient Norm after: 20.907948224727928
Epoch 7964/10000, Prediction Accuracy = 60.178%, Loss = 0.6952546954154968
Epoch: 7964, Batch Gradient Norm: 23.150116446282283
Epoch: 7964, Batch Gradient Norm after: 22.184383105476762
Epoch 7965/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6999588727951049
Epoch: 7965, Batch Gradient Norm: 21.656181001985363
Epoch: 7965, Batch Gradient Norm after: 21.19296645109226
Epoch 7966/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6956305265426636
Epoch: 7966, Batch Gradient Norm: 23.311553360175758
Epoch: 7966, Batch Gradient Norm after: 22.313622197150472
Epoch 7967/10000, Prediction Accuracy = 60.158%, Loss = 0.7003168940544129
Epoch: 7967, Batch Gradient Norm: 21.743599882025244
Epoch: 7967, Batch Gradient Norm after: 21.357667154784206
Epoch 7968/10000, Prediction Accuracy = 60.124%, Loss = 0.6958335041999817
Epoch: 7968, Batch Gradient Norm: 23.260036655984255
Epoch: 7968, Batch Gradient Norm after: 22.307520144373477
Epoch 7969/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.7001829743385315
Epoch: 7969, Batch Gradient Norm: 21.53694735096806
Epoch: 7969, Batch Gradient Norm after: 21.127701606199636
Epoch 7970/10000, Prediction Accuracy = 60.124%, Loss = 0.6952300071716309
Epoch: 7970, Batch Gradient Norm: 22.958562817092012
Epoch: 7970, Batch Gradient Norm after: 22.111896397791657
Epoch 7971/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6991906762123108
Epoch: 7971, Batch Gradient Norm: 21.192820264406986
Epoch: 7971, Batch Gradient Norm after: 20.646596218596514
Epoch 7972/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6942390203475952
Epoch: 7972, Batch Gradient Norm: 22.61901451102252
Epoch: 7972, Batch Gradient Norm after: 21.77037745348892
Epoch 7973/10000, Prediction Accuracy = 60.136%, Loss = 0.6981795191764831
Epoch: 7973, Batch Gradient Norm: 21.41249455468452
Epoch: 7973, Batch Gradient Norm after: 20.90478221078065
Epoch 7974/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6948500514030457
Epoch: 7974, Batch Gradient Norm: 22.928836872674673
Epoch: 7974, Batch Gradient Norm after: 22.066808521618306
Epoch 7975/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6991100907325745
Epoch: 7975, Batch Gradient Norm: 21.2650116966629
Epoch: 7975, Batch Gradient Norm after: 20.70307804705987
Epoch 7976/10000, Prediction Accuracy = 60.17%, Loss = 0.694362199306488
Epoch: 7976, Batch Gradient Norm: 22.78110946862137
Epoch: 7976, Batch Gradient Norm after: 21.940921704835297
Epoch 7977/10000, Prediction Accuracy = 60.134%, Loss = 0.6985033392906189
Epoch: 7977, Batch Gradient Norm: 21.235275859572383
Epoch: 7977, Batch Gradient Norm after: 20.626909120890293
Epoch 7978/10000, Prediction Accuracy = 60.126%, Loss = 0.6941960096359253
Epoch: 7978, Batch Gradient Norm: 22.807668196900433
Epoch: 7978, Batch Gradient Norm after: 21.956827813411067
Epoch 7979/10000, Prediction Accuracy = 60.198%, Loss = 0.6985563158988952
Epoch: 7979, Batch Gradient Norm: 21.241732898644287
Epoch: 7979, Batch Gradient Norm after: 20.60599502551778
Epoch 7980/10000, Prediction Accuracy = 60.122%, Loss = 0.6942053437232971
Epoch: 7980, Batch Gradient Norm: 22.859637345008057
Epoch: 7980, Batch Gradient Norm after: 21.98205863597814
Epoch 7981/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.6986502885818482
Epoch: 7981, Batch Gradient Norm: 21.356060133607087
Epoch: 7981, Batch Gradient Norm after: 20.748285363637272
Epoch 7982/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6944014430046082
Epoch: 7982, Batch Gradient Norm: 23.042454973189045
Epoch: 7982, Batch Gradient Norm after: 22.103831306646175
Epoch 7983/10000, Prediction Accuracy = 60.136%, Loss = 0.6990918755531311
Epoch: 7983, Batch Gradient Norm: 21.575686762524224
Epoch: 7983, Batch Gradient Norm after: 21.059416135861657
Epoch 7984/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6949769139289856
Epoch: 7984, Batch Gradient Norm: 23.26407720148812
Epoch: 7984, Batch Gradient Norm after: 22.27394338030589
Epoch 7985/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.6998295068740845
Epoch: 7985, Batch Gradient Norm: 21.73106101316853
Epoch: 7985, Batch Gradient Norm after: 21.327555732949296
Epoch 7986/10000, Prediction Accuracy = 60.178%, Loss = 0.6953787326812744
Epoch: 7986, Batch Gradient Norm: 23.277167732024488
Epoch: 7986, Batch Gradient Norm after: 22.311495928636294
Epoch 7987/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6997649908065796
Epoch: 7987, Batch Gradient Norm: 21.588732626152968
Epoch: 7987, Batch Gradient Norm after: 21.188903359802314
Epoch 7988/10000, Prediction Accuracy = 60.164%, Loss = 0.6948838114738465
Epoch: 7988, Batch Gradient Norm: 23.044839114831685
Epoch: 7988, Batch Gradient Norm after: 22.16882849419902
Epoch 7989/10000, Prediction Accuracy = 60.18800000000001%, Loss = 0.6989819884300232
Epoch: 7989, Batch Gradient Norm: 21.28979513980021
Epoch: 7989, Batch Gradient Norm after: 20.788087294016126
Epoch 7990/10000, Prediction Accuracy = 60.124%, Loss = 0.69406179189682
Epoch: 7990, Batch Gradient Norm: 22.698558879759464
Epoch: 7990, Batch Gradient Norm after: 21.868065760177668
Epoch 7991/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6979541063308716
Epoch: 7991, Batch Gradient Norm: 21.27364959407341
Epoch: 7991, Batch Gradient Norm after: 20.735126879784655
Epoch 7992/10000, Prediction Accuracy = 60.134%, Loss = 0.6939486265182495
Epoch: 7992, Batch Gradient Norm: 22.74561764339368
Epoch: 7992, Batch Gradient Norm after: 21.909988496348422
Epoch 7993/10000, Prediction Accuracy = 60.148%, Loss = 0.697976005077362
Epoch: 7993, Batch Gradient Norm: 21.24650522820069
Epoch: 7993, Batch Gradient Norm after: 20.667885255347894
Epoch 7994/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6938077330589294
Epoch: 7994, Batch Gradient Norm: 22.783494565650177
Epoch: 7994, Batch Gradient Norm after: 21.9418730965942
Epoch 7995/10000, Prediction Accuracy = 60.148%, Loss = 0.6980992317199707
Epoch: 7995, Batch Gradient Norm: 21.22571491090363
Epoch: 7995, Batch Gradient Norm after: 20.613558056451538
Epoch 7996/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6937856674194336
Epoch: 7996, Batch Gradient Norm: 22.79039670718913
Epoch: 7996, Batch Gradient Norm after: 21.94404524982866
Epoch 7997/10000, Prediction Accuracy = 60.116%, Loss = 0.6981205224990845
Epoch: 7997, Batch Gradient Norm: 21.24381271805585
Epoch: 7997, Batch Gradient Norm after: 20.61906899253012
Epoch 7998/10000, Prediction Accuracy = 60.178%, Loss = 0.6937241911888122
Epoch: 7998, Batch Gradient Norm: 22.87379988540989
Epoch: 7998, Batch Gradient Norm after: 21.99534760735204
Epoch 7999/10000, Prediction Accuracy = 60.128%, Loss = 0.6981957077980041
Epoch: 7999, Batch Gradient Norm: 21.367322037013032
Epoch: 7999, Batch Gradient Norm after: 20.773294919233866
Epoch 8000/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.693967080116272
Epoch: 8000, Batch Gradient Norm: 23.04113388285547
Epoch: 8000, Batch Gradient Norm after: 22.11158572387163
Epoch 8001/10000, Prediction Accuracy = 60.2%, Loss = 0.6986732482910156
Epoch: 8001, Batch Gradient Norm: 21.54174788993288
Epoch: 8001, Batch Gradient Norm after: 21.03033118488921
Epoch 8002/10000, Prediction Accuracy = 60.138%, Loss = 0.694420063495636
Epoch: 8002, Batch Gradient Norm: 23.19504533997055
Epoch: 8002, Batch Gradient Norm after: 22.234918426814136
Epoch 8003/10000, Prediction Accuracy = 60.17%, Loss = 0.6990714550018311
Epoch: 8003, Batch Gradient Norm: 21.63402773303851
Epoch: 8003, Batch Gradient Norm after: 21.199796358535654
Epoch 8004/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.694568133354187
Epoch: 8004, Batch Gradient Norm: 23.207134940001154
Epoch: 8004, Batch Gradient Norm after: 22.263671920856048
Epoch 8005/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6990454077720643
Epoch: 8005, Batch Gradient Norm: 21.548637723236006
Epoch: 8005, Batch Gradient Norm after: 21.125417734865852
Epoch 8006/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6943468809127807
Epoch: 8006, Batch Gradient Norm: 23.025745197494505
Epoch: 8006, Batch Gradient Norm after: 22.158230164112652
Epoch 8007/10000, Prediction Accuracy = 60.126%, Loss = 0.6985880255699157
Epoch: 8007, Batch Gradient Norm: 21.26111644702663
Epoch: 8007, Batch Gradient Norm after: 20.755426659947922
Epoch 8008/10000, Prediction Accuracy = 60.17%, Loss = 0.6935626983642578
Epoch: 8008, Batch Gradient Norm: 22.66894562238255
Epoch: 8008, Batch Gradient Norm after: 21.840146657356872
Epoch 8009/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.6974165201187134
Epoch: 8009, Batch Gradient Norm: 21.307172120766886
Epoch: 8009, Batch Gradient Norm after: 20.784143775362736
Epoch 8010/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6935521006584168
Epoch: 8010, Batch Gradient Norm: 22.7952430946304
Epoch: 8010, Batch Gradient Norm after: 21.970286827860985
Epoch 8011/10000, Prediction Accuracy = 60.188%, Loss = 0.6976600646972656
Epoch: 8011, Batch Gradient Norm: 21.16278489062406
Epoch: 8011, Batch Gradient Norm after: 20.558257725980525
Epoch 8012/10000, Prediction Accuracy = 60.134%, Loss = 0.6931384205818176
Epoch: 8012, Batch Gradient Norm: 22.6751741037207
Epoch: 8012, Batch Gradient Norm after: 21.824969929211708
Epoch 8013/10000, Prediction Accuracy = 60.176%, Loss = 0.6973071455955505
Epoch: 8013, Batch Gradient Norm: 21.365503448512364
Epoch: 8013, Batch Gradient Norm after: 20.811829492825556
Epoch 8014/10000, Prediction Accuracy = 60.146%, Loss = 0.6936111807823181
Epoch: 8014, Batch Gradient Norm: 22.964317244342208
Epoch: 8014, Batch Gradient Norm after: 22.078254612343972
Epoch 8015/10000, Prediction Accuracy = 60.138%, Loss = 0.6980425477027893
Epoch: 8015, Batch Gradient Norm: 21.38089440022231
Epoch: 8015, Batch Gradient Norm after: 20.839163827611706
Epoch 8016/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.6935806155204773
Epoch: 8016, Batch Gradient Norm: 22.97491624101406
Epoch: 8016, Batch Gradient Norm after: 22.088901240939222
Epoch 8017/10000, Prediction Accuracy = 60.138%, Loss = 0.6980841040611268
Epoch: 8017, Batch Gradient Norm: 21.371126075225973
Epoch: 8017, Batch Gradient Norm after: 20.83573950677011
Epoch 8018/10000, Prediction Accuracy = 60.174%, Loss = 0.6935834527015686
Epoch: 8018, Batch Gradient Norm: 22.92460940590199
Epoch: 8018, Batch Gradient Norm after: 22.061142013390914
Epoch 8019/10000, Prediction Accuracy = 60.13599999999999%, Loss = 0.6979483604431153
Epoch: 8019, Batch Gradient Norm: 21.290045215320983
Epoch: 8019, Batch Gradient Norm after: 20.730876246726023
Epoch 8020/10000, Prediction Accuracy = 60.174%, Loss = 0.6932783722877502
Epoch: 8020, Batch Gradient Norm: 22.82385146183537
Epoch: 8020, Batch Gradient Norm after: 21.986875596965724
Epoch 8021/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6975056052207946
Epoch: 8021, Batch Gradient Norm: 21.199536634816788
Epoch: 8021, Batch Gradient Norm after: 20.58610276167082
Epoch 8022/10000, Prediction Accuracy = 60.114%, Loss = 0.6929631352424621
Epoch: 8022, Batch Gradient Norm: 22.77184777691361
Epoch: 8022, Batch Gradient Norm after: 21.926664870416282
Epoch 8023/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6973151803016663
Epoch: 8023, Batch Gradient Norm: 21.264949364207858
Epoch: 8023, Batch Gradient Norm after: 20.652662369174124
Epoch 8024/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6931238889694213
Epoch: 8024, Batch Gradient Norm: 22.877246158870886
Epoch: 8024, Batch Gradient Norm after: 22.008341850897637
Epoch 8025/10000, Prediction Accuracy = 60.166%, Loss = 0.6975804805755615
Epoch: 8025, Batch Gradient Norm: 21.33760231624483
Epoch: 8025, Batch Gradient Norm after: 20.751058328735677
Epoch 8026/10000, Prediction Accuracy = 60.162%, Loss = 0.69321368932724
Epoch: 8026, Batch Gradient Norm: 22.97365981235862
Epoch: 8026, Batch Gradient Norm after: 22.073071791670905
Epoch 8027/10000, Prediction Accuracy = 60.146%, Loss = 0.6977698564529419
Epoch: 8027, Batch Gradient Norm: 21.45357735639786
Epoch: 8027, Batch Gradient Norm after: 20.91578659800982
Epoch 8028/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6935157060623169
Epoch: 8028, Batch Gradient Norm: 23.078991369171554
Epoch: 8028, Batch Gradient Norm after: 22.159001179112845
Epoch 8029/10000, Prediction Accuracy = 60.126%, Loss = 0.6981721162796021
Epoch: 8029, Batch Gradient Norm: 21.50123328883838
Epoch: 8029, Batch Gradient Norm after: 21.02115971796491
Epoch 8030/10000, Prediction Accuracy = 60.178%, Loss = 0.6936358571052551
Epoch: 8030, Batch Gradient Norm: 23.053481698382388
Epoch: 8030, Batch Gradient Norm after: 22.16033847005445
Epoch 8031/10000, Prediction Accuracy = 60.122%, Loss = 0.6979878783226013
Epoch: 8031, Batch Gradient Norm: 21.39989832279726
Epoch: 8031, Batch Gradient Norm after: 20.91083422459659
Epoch 8032/10000, Prediction Accuracy = 60.174%, Loss = 0.6932421684265136
Epoch: 8032, Batch Gradient Norm: 22.900168953863865
Epoch: 8032, Batch Gradient Norm after: 22.058404033801065
Epoch 8033/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6974208354949951
Epoch: 8033, Batch Gradient Norm: 21.213097725340234
Epoch: 8033, Batch Gradient Norm after: 20.64721273388799
Epoch 8034/10000, Prediction Accuracy = 60.13799999999999%, Loss = 0.692719578742981
Epoch: 8034, Batch Gradient Norm: 22.677633409129008
Epoch: 8034, Batch Gradient Norm after: 21.84027147949835
Epoch 8035/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6967819571495056
Epoch: 8035, Batch Gradient Norm: 21.34264392234262
Epoch: 8035, Batch Gradient Norm after: 20.803554249105122
Epoch 8036/10000, Prediction Accuracy = 60.138%, Loss = 0.6929922342300415
Epoch: 8036, Batch Gradient Norm: 22.876122586813846
Epoch: 8036, Batch Gradient Norm after: 22.0304756640128
Epoch 8037/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6972349762916565
Epoch: 8037, Batch Gradient Norm: 21.239180675290154
Epoch: 8037, Batch Gradient Norm after: 20.65844119985441
Epoch 8038/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.6926568627357483
Epoch: 8038, Batch Gradient Norm: 22.75849445478827
Epoch: 8038, Batch Gradient Norm after: 21.9223014315148
Epoch 8039/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.6969143271446228
Epoch: 8039, Batch Gradient Norm: 21.249172833773255
Epoch: 8039, Batch Gradient Norm after: 20.65574929047896
Epoch 8040/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6927066087722779
Epoch: 8040, Batch Gradient Norm: 22.799979557642715
Epoch: 8040, Batch Gradient Norm after: 21.968392750465643
Epoch 8041/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6970230460166931
Epoch: 8041, Batch Gradient Norm: 21.19684379223931
Epoch: 8041, Batch Gradient Norm after: 20.57575313712815
Epoch 8042/10000, Prediction Accuracy = 60.166%, Loss = 0.692466425895691
Epoch: 8042, Batch Gradient Norm: 22.760324950248535
Epoch: 8042, Batch Gradient Norm after: 21.91584475383466
Epoch 8043/10000, Prediction Accuracy = 60.176%, Loss = 0.6967552542686463
Epoch: 8043, Batch Gradient Norm: 21.288741394687587
Epoch: 8043, Batch Gradient Norm after: 20.680003876202413
Epoch 8044/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6926236510276794
Epoch: 8044, Batch Gradient Norm: 22.919556133016016
Epoch: 8044, Batch Gradient Norm after: 22.03790582206482
Epoch 8045/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6971934914588929
Epoch: 8045, Batch Gradient Norm: 21.37889312367127
Epoch: 8045, Batch Gradient Norm after: 20.816011480786347
Epoch 8046/10000, Prediction Accuracy = 60.162%, Loss = 0.6928571462631226
Epoch: 8046, Batch Gradient Norm: 22.99345022263266
Epoch: 8046, Batch Gradient Norm after: 22.09927864053788
Epoch 8047/10000, Prediction Accuracy = 60.164%, Loss = 0.6973683595657348
Epoch: 8047, Batch Gradient Norm: 21.438237995477284
Epoch: 8047, Batch Gradient Norm after: 20.916056825151443
Epoch 8048/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6929137229919433
Epoch: 8048, Batch Gradient Norm: 23.022347953154306
Epoch: 8048, Batch Gradient Norm after: 22.127329574887465
Epoch 8049/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.697368609905243
Epoch: 8049, Batch Gradient Norm: 21.42036296268707
Epoch: 8049, Batch Gradient Norm after: 20.9128122047739
Epoch 8050/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6928712487220764
Epoch: 8050, Batch Gradient Norm: 22.941039612081227
Epoch: 8050, Batch Gradient Norm after: 22.08338659114566
Epoch 8051/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.6972127556800842
Epoch: 8051, Batch Gradient Norm: 21.265538536955027
Epoch: 8051, Batch Gradient Norm after: 20.7199753611904
Epoch 8052/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.692442512512207
Epoch: 8052, Batch Gradient Norm: 22.72448629053909
Epoch: 8052, Batch Gradient Norm after: 21.898182807180028
Epoch 8053/10000, Prediction Accuracy = 60.12199999999999%, Loss = 0.6964669823646545
Epoch: 8053, Batch Gradient Norm: 21.266061847124252
Epoch: 8053, Batch Gradient Norm after: 20.704123137352763
Epoch 8054/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6923105716705322
Epoch: 8054, Batch Gradient Norm: 22.783925187773118
Epoch: 8054, Batch Gradient Norm after: 21.957077454469317
Epoch 8055/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6965133905410766
Epoch: 8055, Batch Gradient Norm: 21.197824479007913
Epoch: 8055, Batch Gradient Norm after: 20.592696267928336
Epoch 8056/10000, Prediction Accuracy = 60.13800000000001%, Loss = 0.6921023607254029
Epoch: 8056, Batch Gradient Norm: 22.71037354887535
Epoch: 8056, Batch Gradient Norm after: 21.869115337183683
Epoch 8057/10000, Prediction Accuracy = 60.166%, Loss = 0.6963191032409668
Epoch: 8057, Batch Gradient Norm: 21.32892781792371
Epoch: 8057, Batch Gradient Norm after: 20.7623586036629
Epoch 8058/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6923923492431641
Epoch: 8058, Batch Gradient Norm: 22.903530578900288
Epoch: 8058, Batch Gradient Norm after: 22.042369379663324
Epoch 8059/10000, Prediction Accuracy = 60.124%, Loss = 0.6967608213424683
Epoch: 8059, Batch Gradient Norm: 21.32129147979909
Epoch: 8059, Batch Gradient Norm after: 20.754801170699153
Epoch 8060/10000, Prediction Accuracy = 60.18800000000001%, Loss = 0.6922937750816345
Epoch: 8060, Batch Gradient Norm: 22.878705403700497
Epoch: 8060, Batch Gradient Norm after: 22.02709007020287
Epoch 8061/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6966898441314697
Epoch: 8061, Batch Gradient Norm: 21.26294447911939
Epoch: 8061, Batch Gradient Norm after: 20.682644591480816
Epoch 8062/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.692172360420227
Epoch: 8062, Batch Gradient Norm: 22.78417741123838
Epoch: 8062, Batch Gradient Norm after: 21.956150042398278
Epoch 8063/10000, Prediction Accuracy = 60.134%, Loss = 0.6964312791824341
Epoch: 8063, Batch Gradient Norm: 21.215941361176395
Epoch: 8063, Batch Gradient Norm after: 20.61231501310053
Epoch 8064/10000, Prediction Accuracy = 60.15%, Loss = 0.6919482707977295
Epoch: 8064, Batch Gradient Norm: 22.761614438270488
Epoch: 8064, Batch Gradient Norm after: 21.92733183438899
Epoch 8065/10000, Prediction Accuracy = 60.19%, Loss = 0.6962076187133789
Epoch: 8065, Batch Gradient Norm: 21.26288691345245
Epoch: 8065, Batch Gradient Norm after: 20.661464658938467
Epoch 8066/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.6919961214065552
Epoch: 8066, Batch Gradient Norm: 22.8414261454843
Epoch: 8066, Batch Gradient Norm after: 21.99469483123539
Epoch 8067/10000, Prediction Accuracy = 60.164%, Loss = 0.6964329719543457
Epoch: 8067, Batch Gradient Norm: 21.26710557127337
Epoch: 8067, Batch Gradient Norm after: 20.667170873001584
Epoch 8068/10000, Prediction Accuracy = 60.152%, Loss = 0.6919965744018555
Epoch: 8068, Batch Gradient Norm: 22.831667647697472
Epoch: 8068, Batch Gradient Norm after: 21.989925760331506
Epoch 8069/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6963465452194214
Epoch: 8069, Batch Gradient Norm: 21.261828427185065
Epoch: 8069, Batch Gradient Norm after: 20.65643953923957
Epoch 8070/10000, Prediction Accuracy = 60.17%, Loss = 0.6918716907501221
Epoch: 8070, Batch Gradient Norm: 22.846835407184233
Epoch: 8070, Batch Gradient Norm after: 21.996681231820936
Epoch 8071/10000, Prediction Accuracy = 60.178%, Loss = 0.6963015079498291
Epoch: 8071, Batch Gradient Norm: 21.289646782108687
Epoch: 8071, Batch Gradient Norm after: 20.693592230480323
Epoch 8072/10000, Prediction Accuracy = 60.176%, Loss = 0.6919602394104004
Epoch: 8072, Batch Gradient Norm: 22.870835727996507
Epoch: 8072, Batch Gradient Norm after: 22.01708474025075
Epoch 8073/10000, Prediction Accuracy = 60.134%, Loss = 0.6964463591575623
Epoch: 8073, Batch Gradient Norm: 21.291458468416607
Epoch: 8073, Batch Gradient Norm after: 20.71044333003591
Epoch 8074/10000, Prediction Accuracy = 60.178%, Loss = 0.6919384598731995
Epoch: 8074, Batch Gradient Norm: 22.83697074274019
Epoch: 8074, Batch Gradient Norm after: 21.998728980014693
Epoch 8075/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.6962279677391052
Epoch: 8075, Batch Gradient Norm: 21.237535460811316
Epoch: 8075, Batch Gradient Norm after: 20.63697586515028
Epoch 8076/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6916762471199036
Epoch: 8076, Batch Gradient Norm: 22.79045350473082
Epoch: 8076, Batch Gradient Norm after: 21.95861209619555
Epoch 8077/10000, Prediction Accuracy = 60.162%, Loss = 0.6959960579872131
Epoch: 8077, Batch Gradient Norm: 21.228073660341042
Epoch: 8077, Batch Gradient Norm after: 20.611951511174965
Epoch 8078/10000, Prediction Accuracy = 60.152%, Loss = 0.6916256070137023
Epoch: 8078, Batch Gradient Norm: 22.787990277215275
Epoch: 8078, Batch Gradient Norm after: 21.95461287967569
Epoch 8079/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6959922432899475
Epoch: 8079, Batch Gradient Norm: 21.24093951404077
Epoch: 8079, Batch Gradient Norm after: 20.626486489904476
Epoch 8080/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6915895223617554
Epoch: 8080, Batch Gradient Norm: 22.812209718457915
Epoch: 8080, Batch Gradient Norm after: 21.97400833934773
Epoch 8081/10000, Prediction Accuracy = 60.114%, Loss = 0.695950710773468
Epoch: 8081, Batch Gradient Norm: 21.26270580496031
Epoch: 8081, Batch Gradient Norm after: 20.64865231416082
Epoch 8082/10000, Prediction Accuracy = 60.178%, Loss = 0.6915762662887573
Epoch: 8082, Batch Gradient Norm: 22.853127488119394
Epoch: 8082, Batch Gradient Norm after: 22.002170664555752
Epoch 8083/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6960707306861877
Epoch: 8083, Batch Gradient Norm: 21.29523613492375
Epoch: 8083, Batch Gradient Norm after: 20.704689587972517
Epoch 8084/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6916870713233948
Epoch: 8084, Batch Gradient Norm: 22.858028608271542
Epoch: 8084, Batch Gradient Norm after: 22.012257965552458
Epoch 8085/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6960862874984741
Epoch: 8085, Batch Gradient Norm: 21.274752646804387
Epoch: 8085, Batch Gradient Norm after: 20.687882100198642
Epoch 8086/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6915345072746277
Epoch: 8086, Batch Gradient Norm: 22.826872256115767
Epoch: 8086, Batch Gradient Norm after: 21.99207338622079
Epoch 8087/10000, Prediction Accuracy = 60.176%, Loss = 0.6958544373512268
Epoch: 8087, Batch Gradient Norm: 21.245395994915725
Epoch: 8087, Batch Gradient Norm after: 20.641840574470226
Epoch 8088/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6913847327232361
Epoch: 8088, Batch Gradient Norm: 22.789942171666826
Epoch: 8088, Batch Gradient Norm after: 21.96081324402883
Epoch 8089/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6957359194755555
Epoch: 8089, Batch Gradient Norm: 21.22664317632325
Epoch: 8089, Batch Gradient Norm after: 20.614821198037966
Epoch 8090/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6913135647773743
Epoch: 8090, Batch Gradient Norm: 22.7676502682423
Epoch: 8090, Batch Gradient Norm after: 21.936326972594035
Epoch 8091/10000, Prediction Accuracy = 60.15%, Loss = 0.6956089854240417
Epoch: 8091, Batch Gradient Norm: 21.269121210918094
Epoch: 8091, Batch Gradient Norm after: 20.66889747172819
Epoch 8092/10000, Prediction Accuracy = 60.166%, Loss = 0.6913252353668213
Epoch: 8092, Batch Gradient Norm: 22.837760828293323
Epoch: 8092, Batch Gradient Norm after: 21.995539492754343
Epoch 8093/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6957284450531006
Epoch: 8093, Batch Gradient Norm: 21.27022178489415
Epoch: 8093, Batch Gradient Norm after: 20.674715890241593
Epoch 8094/10000, Prediction Accuracy = 60.174%, Loss = 0.6913419127464294
Epoch: 8094, Batch Gradient Norm: 22.820219254016795
Epoch: 8094, Batch Gradient Norm after: 21.987635356251864
Epoch 8095/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.695758330821991
Epoch: 8095, Batch Gradient Norm: 21.22471456184272
Epoch: 8095, Batch Gradient Norm after: 20.620728500295378
Epoch 8096/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6911961674690247
Epoch: 8096, Batch Gradient Norm: 22.75240305176962
Epoch: 8096, Batch Gradient Norm after: 21.921471821238917
Epoch 8097/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.6954216241836548
Epoch: 8097, Batch Gradient Norm: 21.288439598324565
Epoch: 8097, Batch Gradient Norm after: 20.704193107702192
Epoch 8098/10000, Prediction Accuracy = 60.164%, Loss = 0.6912383198738098
Epoch: 8098, Batch Gradient Norm: 22.83351220861989
Epoch: 8098, Batch Gradient Norm after: 21.998433612992223
Epoch 8099/10000, Prediction Accuracy = 60.15400000000001%, Loss = 0.6955828070640564
Epoch: 8099, Batch Gradient Norm: 21.245162521854105
Epoch: 8099, Batch Gradient Norm after: 20.647903292036826
Epoch 8100/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6911110281944275
Epoch: 8100, Batch Gradient Norm: 22.770109715576524
Epoch: 8100, Batch Gradient Norm after: 21.943792677018084
Epoch 8101/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.6953938007354736
Epoch: 8101, Batch Gradient Norm: 21.254235242079258
Epoch: 8101, Batch Gradient Norm after: 20.659009134562062
Epoch 8102/10000, Prediction Accuracy = 60.186%, Loss = 0.6910481810569763
Epoch: 8102, Batch Gradient Norm: 22.770966539628667
Epoch: 8102, Batch Gradient Norm after: 21.944563746241066
Epoch 8103/10000, Prediction Accuracy = 60.124%, Loss = 0.6952848911285401
Epoch: 8103, Batch Gradient Norm: 21.269337725787555
Epoch: 8103, Batch Gradient Norm after: 20.67465195201705
Epoch 8104/10000, Prediction Accuracy = 60.17%, Loss = 0.6910277962684631
Epoch: 8104, Batch Gradient Norm: 22.806620886279692
Epoch: 8104, Batch Gradient Norm after: 21.98258526438684
Epoch 8105/10000, Prediction Accuracy = 60.14%, Loss = 0.695395541191101
Epoch: 8105, Batch Gradient Norm: 21.211771240773864
Epoch: 8105, Batch Gradient Norm after: 20.603116425118234
Epoch 8106/10000, Prediction Accuracy = 60.15200000000001%, Loss = 0.6909013032913208
Epoch: 8106, Batch Gradient Norm: 22.715190485168556
Epoch: 8106, Batch Gradient Norm after: 21.884717691610156
Epoch 8107/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6951114177703858
Epoch: 8107, Batch Gradient Norm: 21.338942340632936
Epoch: 8107, Batch Gradient Norm after: 20.777782410941875
Epoch 8108/10000, Prediction Accuracy = 60.148%, Loss = 0.6911355257034302
Epoch: 8108, Batch Gradient Norm: 22.86997671349635
Epoch: 8108, Batch Gradient Norm after: 22.032466361807302
Epoch 8109/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6954245686531066
Epoch: 8109, Batch Gradient Norm: 21.257241552852268
Epoch: 8109, Batch Gradient Norm after: 20.681555804670392
Epoch 8110/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6908584713935852
Epoch: 8110, Batch Gradient Norm: 22.74067682410392
Epoch: 8110, Batch Gradient Norm after: 21.916779533375507
Epoch 8111/10000, Prediction Accuracy = 60.138%, Loss = 0.6950440645217896
Epoch: 8111, Batch Gradient Norm: 21.282788362769242
Epoch: 8111, Batch Gradient Norm after: 20.715138333033682
Epoch 8112/10000, Prediction Accuracy = 60.186%, Loss = 0.6908944249153137
Epoch: 8112, Batch Gradient Norm: 22.76801942163494
Epoch: 8112, Batch Gradient Norm after: 21.951158340928224
Epoch 8113/10000, Prediction Accuracy = 60.15%, Loss = 0.6950509190559387
Epoch: 8113, Batch Gradient Norm: 21.23322426026535
Epoch: 8113, Batch Gradient Norm after: 20.650291080333513
Epoch 8114/10000, Prediction Accuracy = 60.146%, Loss = 0.6906700134277344
Epoch: 8114, Batch Gradient Norm: 22.71519378353161
Epoch: 8114, Batch Gradient Norm after: 21.890178575187914
Epoch 8115/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6948262453079224
Epoch: 8115, Batch Gradient Norm: 21.322103022769888
Epoch: 8115, Batch Gradient Norm after: 20.766901359763292
Epoch 8116/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6909146189689637
Epoch: 8116, Batch Gradient Norm: 22.806879530005652
Epoch: 8116, Batch Gradient Norm after: 21.994961013110483
Epoch 8117/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6951733827590942
Epoch: 8117, Batch Gradient Norm: 21.159151709859703
Epoch: 8117, Batch Gradient Norm after: 20.561491920018028
Epoch 8118/10000, Prediction Accuracy = 60.15%, Loss = 0.6904818415641785
Epoch: 8118, Batch Gradient Norm: 22.60814836648999
Epoch: 8118, Batch Gradient Norm after: 21.77416226930682
Epoch 8119/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6944591879844666
Epoch: 8119, Batch Gradient Norm: 21.464005222360537
Epoch: 8119, Batch Gradient Norm after: 20.96808685864684
Epoch 8120/10000, Prediction Accuracy = 60.176%, Loss = 0.6911474943161011
Epoch: 8120, Batch Gradient Norm: 22.952377343569587
Epoch: 8120, Batch Gradient Norm after: 22.103745476363702
Epoch 8121/10000, Prediction Accuracy = 60.152%, Loss = 0.6953880071640015
Epoch: 8121, Batch Gradient Norm: 21.27959724721579
Epoch: 8121, Batch Gradient Norm after: 20.749948465073132
Epoch 8122/10000, Prediction Accuracy = 60.158%, Loss = 0.6906574249267579
Epoch: 8122, Batch Gradient Norm: 22.682748840865333
Epoch: 8122, Batch Gradient Norm after: 21.8704831526079
Epoch 8123/10000, Prediction Accuracy = 60.14%, Loss = 0.6946092963218689
Epoch: 8123, Batch Gradient Norm: 21.298221427964794
Epoch: 8123, Batch Gradient Norm after: 20.77734894753078
Epoch 8124/10000, Prediction Accuracy = 60.182%, Loss = 0.6906184673309326
Epoch: 8124, Batch Gradient Norm: 22.697143724129972
Epoch: 8124, Batch Gradient Norm after: 21.888773633540232
Epoch 8125/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6945295929908752
Epoch: 8125, Batch Gradient Norm: 21.28840761943037
Epoch: 8125, Batch Gradient Norm after: 20.759386563092892
Epoch 8126/10000, Prediction Accuracy = 60.198%, Loss = 0.6905353426933288
Epoch: 8126, Batch Gradient Norm: 22.69794478612351
Epoch: 8126, Batch Gradient Norm after: 21.888815165789683
Epoch 8127/10000, Prediction Accuracy = 60.14000000000001%, Loss = 0.6945323944091797
Epoch: 8127, Batch Gradient Norm: 21.2753485866324
Epoch: 8127, Batch Gradient Norm after: 20.74894277408462
Epoch 8128/10000, Prediction Accuracy = 60.136%, Loss = 0.6905151605606079
Epoch: 8128, Batch Gradient Norm: 22.663192399713036
Epoch: 8128, Batch Gradient Norm after: 21.853274618673517
Epoch 8129/10000, Prediction Accuracy = 60.162%, Loss = 0.6944087147712708
Epoch: 8129, Batch Gradient Norm: 21.318518750804312
Epoch: 8129, Batch Gradient Norm after: 20.814636651209337
Epoch 8130/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6905351161956788
Epoch: 8130, Batch Gradient Norm: 22.70713919820948
Epoch: 8130, Batch Gradient Norm after: 21.90493213794587
Epoch 8131/10000, Prediction Accuracy = 60.166%, Loss = 0.6944149374961853
Epoch: 8131, Batch Gradient Norm: 21.255178313180338
Epoch: 8131, Batch Gradient Norm after: 20.727761011524326
Epoch 8132/10000, Prediction Accuracy = 60.148%, Loss = 0.6902981400489807
Epoch: 8132, Batch Gradient Norm: 22.62996305245691
Epoch: 8132, Batch Gradient Norm after: 21.81815451026181
Epoch 8133/10000, Prediction Accuracy = 60.14%, Loss = 0.694183599948883
Epoch: 8133, Batch Gradient Norm: 21.361480653984312
Epoch: 8133, Batch Gradient Norm after: 20.875701701619903
Epoch 8134/10000, Prediction Accuracy = 60.174%, Loss = 0.6905526399612427
Epoch: 8134, Batch Gradient Norm: 22.737678205851218
Epoch: 8134, Batch Gradient Norm after: 21.94332492515049
Epoch 8135/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6944235324859619
Epoch: 8135, Batch Gradient Norm: 21.191135951638966
Epoch: 8135, Batch Gradient Norm after: 20.651968162452736
Epoch 8136/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6900084137916564
Epoch: 8136, Batch Gradient Norm: 22.5370071748804
Epoch: 8136, Batch Gradient Norm after: 21.714831390851508
Epoch 8137/10000, Prediction Accuracy = 60.176%, Loss = 0.6937792062759399
Epoch: 8137, Batch Gradient Norm: 21.50700451929657
Epoch: 8137, Batch Gradient Norm after: 21.07205090251385
Epoch 8138/10000, Prediction Accuracy = 60.15%, Loss = 0.6908596515655517
Epoch: 8138, Batch Gradient Norm: 22.888946779584373
Epoch: 8138, Batch Gradient Norm after: 22.091690247818487
Epoch 8139/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6948687434196472
Epoch: 8139, Batch Gradient Norm: 21.091936402964887
Epoch: 8139, Batch Gradient Norm after: 20.543456241182277
Epoch 8140/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6897423505783081
Epoch: 8140, Batch Gradient Norm: 22.3781548262928
Epoch: 8140, Batch Gradient Norm after: 21.54684012558736
Epoch 8141/10000, Prediction Accuracy = 60.148%, Loss = 0.693260669708252
Epoch: 8141, Batch Gradient Norm: 21.69401539971863
Epoch: 8141, Batch Gradient Norm after: 21.343978119127744
Epoch 8142/10000, Prediction Accuracy = 60.176%, Loss = 0.6912249565124512
Epoch: 8142, Batch Gradient Norm: 23.054101976954723
Epoch: 8142, Batch Gradient Norm after: 22.22046725768152
Epoch 8143/10000, Prediction Accuracy = 60.13599999999999%, Loss = 0.6951754689216614
Epoch: 8143, Batch Gradient Norm: 21.193623230847667
Epoch: 8143, Batch Gradient Norm after: 20.728274547633855
Epoch 8144/10000, Prediction Accuracy = 60.166%, Loss = 0.6898845314979554
Epoch: 8144, Batch Gradient Norm: 22.41030345181512
Epoch: 8144, Batch Gradient Norm after: 21.598839665590994
Epoch 8145/10000, Prediction Accuracy = 60.152%, Loss = 0.6932834029197693
Epoch: 8145, Batch Gradient Norm: 21.565070360248097
Epoch: 8145, Batch Gradient Norm after: 21.217681735295056
Epoch 8146/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6907820343971253
Epoch: 8146, Batch Gradient Norm: 22.81764776253771
Epoch: 8146, Batch Gradient Norm after: 22.061044452423086
Epoch 8147/10000, Prediction Accuracy = 60.116%, Loss = 0.6943550109863281
Epoch: 8147, Batch Gradient Norm: 20.976457453673373
Epoch: 8147, Batch Gradient Norm after: 20.430493473511905
Epoch 8148/10000, Prediction Accuracy = 60.188%, Loss = 0.689165985584259
Epoch: 8148, Batch Gradient Norm: 22.2909284084177
Epoch: 8148, Batch Gradient Norm after: 21.558370243572394
Epoch 8149/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.6928375005722046
Epoch: 8149, Batch Gradient Norm: 21.372667530802534
Epoch: 8149, Batch Gradient Norm after: 20.939669106282405
Epoch 8150/10000, Prediction Accuracy = 60.14%, Loss = 0.6902404665946961
Epoch: 8150, Batch Gradient Norm: 22.64219702038032
Epoch: 8150, Batch Gradient Norm after: 21.859473253087554
Epoch 8151/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6938211917877197
Epoch: 8151, Batch Gradient Norm: 21.246323275410507
Epoch: 8151, Batch Gradient Norm after: 20.781121381196662
Epoch 8152/10000, Prediction Accuracy = 60.152%, Loss = 0.6897932291030884
Epoch: 8152, Batch Gradient Norm: 22.49221799658747
Epoch: 8152, Batch Gradient Norm after: 21.689101971199538
Epoch 8153/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6932532787322998
Epoch: 8153, Batch Gradient Norm: 21.466363784895254
Epoch: 8153, Batch Gradient Norm after: 21.07245488837403
Epoch 8154/10000, Prediction Accuracy = 60.146%, Loss = 0.6903091430664062
Epoch: 8154, Batch Gradient Norm: 22.739236743412214
Epoch: 8154, Batch Gradient Norm after: 21.970638881000216
Epoch 8155/10000, Prediction Accuracy = 60.146%, Loss = 0.6939682483673095
Epoch: 8155, Batch Gradient Norm: 21.08330543362334
Epoch: 8155, Batch Gradient Norm after: 20.569204919651337
Epoch 8156/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6892573237419128
Epoch: 8156, Batch Gradient Norm: 22.343773563820843
Epoch: 8156, Batch Gradient Norm after: 21.554517915867493
Epoch 8157/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6927419304847717
Epoch: 8157, Batch Gradient Norm: 21.55993434835892
Epoch: 8157, Batch Gradient Norm after: 21.192252301983636
Epoch 8158/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6904365420341492
Epoch: 8158, Batch Gradient Norm: 22.840285713726196
Epoch: 8158, Batch Gradient Norm after: 22.083175139343172
Epoch 8159/10000, Prediction Accuracy = 60.188%, Loss = 0.6941284656524658
Epoch: 8159, Batch Gradient Norm: 20.95779585725315
Epoch: 8159, Batch Gradient Norm after: 20.393437840172766
Epoch 8160/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6888389706611633
Epoch: 8160, Batch Gradient Norm: 22.289514303347286
Epoch: 8160, Batch Gradient Norm after: 21.56099840589741
Epoch 8161/10000, Prediction Accuracy = 60.152%, Loss = 0.6925768494606018
Epoch: 8161, Batch Gradient Norm: 21.360624091618636
Epoch: 8161, Batch Gradient Norm after: 20.921060905203962
Epoch 8162/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6899064779281616
Epoch: 8162, Batch Gradient Norm: 22.64267117880638
Epoch: 8162, Batch Gradient Norm after: 21.859432932175274
Epoch 8163/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6934816837310791
Epoch: 8163, Batch Gradient Norm: 21.248562378626055
Epoch: 8163, Batch Gradient Norm after: 20.779385764062784
Epoch 8164/10000, Prediction Accuracy = 60.162%, Loss = 0.689479649066925
Epoch: 8164, Batch Gradient Norm: 22.516581679381623
Epoch: 8164, Batch Gradient Norm after: 21.714720182729838
Epoch 8165/10000, Prediction Accuracy = 60.148%, Loss = 0.6930338144302368
Epoch: 8165, Batch Gradient Norm: 21.428693692676635
Epoch: 8165, Batch Gradient Norm after: 21.02308726038959
Epoch 8166/10000, Prediction Accuracy = 60.15%, Loss = 0.6899498105049133
Epoch: 8166, Batch Gradient Norm: 22.703050034448594
Epoch: 8166, Batch Gradient Norm after: 21.930891274901683
Epoch 8167/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6935893535614014
Epoch: 8167, Batch Gradient Norm: 21.128161155915105
Epoch: 8167, Batch Gradient Norm after: 20.634923269777524
Epoch 8168/10000, Prediction Accuracy = 60.194%, Loss = 0.6890646696090699
Epoch: 8168, Batch Gradient Norm: 22.359663307182846
Epoch: 8168, Batch Gradient Norm after: 21.549900497773724
Epoch 8169/10000, Prediction Accuracy = 60.124%, Loss = 0.6924647569656373
Epoch: 8169, Batch Gradient Norm: 21.625979936193385
Epoch: 8169, Batch Gradient Norm after: 21.28802236365276
Epoch 8170/10000, Prediction Accuracy = 60.188%, Loss = 0.690353274345398
Epoch: 8170, Batch Gradient Norm: 22.888012367209022
Epoch: 8170, Batch Gradient Norm after: 22.12330289476354
Epoch 8171/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6940354108810425
Epoch: 8171, Batch Gradient Norm: 20.9610863507734
Epoch: 8171, Batch Gradient Norm after: 20.425297733093963
Epoch 8172/10000, Prediction Accuracy = 60.15%, Loss = 0.6885914921760559
Epoch: 8172, Batch Gradient Norm: 22.270333379717375
Epoch: 8172, Batch Gradient Norm after: 21.56477587562134
Epoch 8173/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6921996951103211
Epoch: 8173, Batch Gradient Norm: 21.300983020947513
Epoch: 8173, Batch Gradient Norm after: 20.85983191102117
Epoch 8174/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6893907427787781
Epoch: 8174, Batch Gradient Norm: 22.54975664972396
Epoch: 8174, Batch Gradient Norm after: 21.75994282135407
Epoch 8175/10000, Prediction Accuracy = 60.162%, Loss = 0.6928723573684692
Epoch: 8175, Batch Gradient Norm: 21.359581717852002
Epoch: 8175, Batch Gradient Norm after: 20.943764869410508
Epoch 8176/10000, Prediction Accuracy = 60.15%, Loss = 0.6894874215126038
Epoch: 8176, Batch Gradient Norm: 22.599009440164235
Epoch: 8176, Batch Gradient Norm after: 21.819313069518785
Epoch 8177/10000, Prediction Accuracy = 60.164%, Loss = 0.6930224180221558
Epoch: 8177, Batch Gradient Norm: 21.273867036381077
Epoch: 8177, Batch Gradient Norm after: 20.837756813224388
Epoch 8178/10000, Prediction Accuracy = 60.152%, Loss = 0.6892202138900757
Epoch: 8178, Batch Gradient Norm: 22.476068820517774
Epoch: 8178, Batch Gradient Norm after: 21.683351421503147
Epoch 8179/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6925915122032166
Epoch: 8179, Batch Gradient Norm: 21.465553783226873
Epoch: 8179, Batch Gradient Norm after: 21.09398825039967
Epoch 8180/10000, Prediction Accuracy = 60.166%, Loss = 0.6896407008171082
Epoch: 8180, Batch Gradient Norm: 22.679651055626746
Epoch: 8180, Batch Gradient Norm after: 21.91641175107579
Epoch 8181/10000, Prediction Accuracy = 60.17%, Loss = 0.6931241154670715
Epoch: 8181, Batch Gradient Norm: 21.133977719772247
Epoch: 8181, Batch Gradient Norm after: 20.663468758556537
Epoch 8182/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6887767076492309
Epoch: 8182, Batch Gradient Norm: 22.32928395643491
Epoch: 8182, Batch Gradient Norm after: 21.54931434265062
Epoch 8183/10000, Prediction Accuracy = 60.15400000000001%, Loss = 0.6921566486358642
Epoch: 8183, Batch Gradient Norm: 21.54133734577261
Epoch: 8183, Batch Gradient Norm after: 21.208695939714854
Epoch 8184/10000, Prediction Accuracy = 60.152%, Loss = 0.6898489832878113
Epoch: 8184, Batch Gradient Norm: 22.72244422794191
Epoch: 8184, Batch Gradient Norm after: 21.9721428170011
Epoch 8185/10000, Prediction Accuracy = 60.178%, Loss = 0.6931838393211365
Epoch: 8185, Batch Gradient Norm: 21.049475116826304
Epoch: 8185, Batch Gradient Norm after: 20.567687387680557
Epoch 8186/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6884052276611328
Epoch: 8186, Batch Gradient Norm: 22.290402136456237
Epoch: 8186, Batch Gradient Norm after: 21.564173600051394
Epoch 8187/10000, Prediction Accuracy = 60.148%, Loss = 0.6918697118759155
Epoch: 8187, Batch Gradient Norm: 21.362381145523685
Epoch: 8187, Batch Gradient Norm after: 20.966903826233775
Epoch 8188/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6892312884330749
Epoch: 8188, Batch Gradient Norm: 22.554226244595053
Epoch: 8188, Batch Gradient Norm after: 21.779618497173367
Epoch 8189/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6926130294799805
Epoch: 8189, Batch Gradient Norm: 21.299017054458524
Epoch: 8189, Batch Gradient Norm after: 20.89611686213698
Epoch 8190/10000, Prediction Accuracy = 60.178%, Loss = 0.6889680027961731
Epoch: 8190, Batch Gradient Norm: 22.455385745944294
Epoch: 8190, Batch Gradient Norm after: 21.66971608899415
Epoch 8191/10000, Prediction Accuracy = 60.11999999999999%, Loss = 0.6922176241874695
Epoch: 8191, Batch Gradient Norm: 21.455809649130583
Epoch: 8191, Batch Gradient Norm after: 21.1033187862701
Epoch 8192/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6893516421318054
Epoch: 8192, Batch Gradient Norm: 22.6156283822247
Epoch: 8192, Batch Gradient Norm after: 21.856157472826798
Epoch 8193/10000, Prediction Accuracy = 60.176%, Loss = 0.6927125453948975
Epoch: 8193, Batch Gradient Norm: 21.17526408259911
Epoch: 8193, Batch Gradient Norm after: 20.747588555353136
Epoch 8194/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6886194348335266
Epoch: 8194, Batch Gradient Norm: 22.31994679936264
Epoch: 8194, Batch Gradient Norm after: 21.55515362500562
Epoch 8195/10000, Prediction Accuracy = 60.174%, Loss = 0.6918011546134949
Epoch: 8195, Batch Gradient Norm: 21.498666440701776
Epoch: 8195, Batch Gradient Norm after: 21.17642650978459
Epoch 8196/10000, Prediction Accuracy = 60.148%, Loss = 0.6893749356269836
Epoch: 8196, Batch Gradient Norm: 22.636380417713863
Epoch: 8196, Batch Gradient Norm after: 21.88283762242543
Epoch 8197/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6925988674163819
Epoch: 8197, Batch Gradient Norm: 21.125805367755063
Epoch: 8197, Batch Gradient Norm after: 20.69367485635478
Epoch 8198/10000, Prediction Accuracy = 60.152%, Loss = 0.6883214712142944
Epoch: 8198, Batch Gradient Norm: 22.29506351782023
Epoch: 8198, Batch Gradient Norm after: 21.558810633542524
Epoch 8199/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6916119456291199
Epoch: 8199, Batch Gradient Norm: 21.39300647444792
Epoch: 8199, Batch Gradient Norm after: 21.04068679259155
Epoch 8200/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6889980792999267
Epoch: 8200, Batch Gradient Norm: 22.518220778740112
Epoch: 8200, Batch Gradient Norm after: 21.75133113780112
Epoch 8201/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6921606540679932
Epoch: 8201, Batch Gradient Norm: 21.305331987689627
Epoch: 8201, Batch Gradient Norm after: 20.932580520696035
Epoch 8202/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6886761546134949
Epoch: 8202, Batch Gradient Norm: 22.413928603488067
Epoch: 8202, Batch Gradient Norm after: 21.634283774922537
Epoch 8203/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.6918041586875916
Epoch: 8203, Batch Gradient Norm: 21.45434087864712
Epoch: 8203, Batch Gradient Norm after: 21.13293085518351
Epoch 8204/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6890952467918396
Epoch: 8204, Batch Gradient Norm: 22.552064802331877
Epoch: 8204, Batch Gradient Norm after: 21.798345738797106
Epoch 8205/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.692248809337616
Epoch: 8205, Batch Gradient Norm: 21.207429877595565
Epoch: 8205, Batch Gradient Norm after: 20.825689633430787
Epoch 8206/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6883989691734314
Epoch: 8206, Batch Gradient Norm: 22.303676637812305
Epoch: 8206, Batch Gradient Norm after: 21.55491464317977
Epoch 8207/10000, Prediction Accuracy = 60.186%, Loss = 0.6914145112037658
Epoch: 8207, Batch Gradient Norm: 21.444580699761072
Epoch: 8207, Batch Gradient Norm after: 21.132410052802864
Epoch 8208/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.688918662071228
Epoch: 8208, Batch Gradient Norm: 22.528719444088292
Epoch: 8208, Batch Gradient Norm after: 21.771725781123518
Epoch 8209/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.6920045018196106
Epoch: 8209, Batch Gradient Norm: 21.240967123576844
Epoch: 8209, Batch Gradient Norm after: 20.874249940718553
Epoch 8210/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.688375449180603
Epoch: 8210, Batch Gradient Norm: 22.311768371771407
Epoch: 8210, Batch Gradient Norm after: 21.555033839917538
Epoch 8211/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6913824081420898
Epoch: 8211, Batch Gradient Norm: 21.462573418151088
Epoch: 8211, Batch Gradient Norm after: 21.171090968609562
Epoch 8212/10000, Prediction Accuracy = 60.19%, Loss = 0.6888744950294494
Epoch: 8212, Batch Gradient Norm: 22.518773507133922
Epoch: 8212, Batch Gradient Norm after: 21.76676345748428
Epoch 8213/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.6918479084968567
Epoch: 8213, Batch Gradient Norm: 21.245204039605387
Epoch: 8213, Batch Gradient Norm after: 20.887160216070008
Epoch 8214/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6882525205612182
Epoch: 8214, Batch Gradient Norm: 22.30553000349877
Epoch: 8214, Batch Gradient Norm after: 21.55041268628706
Epoch 8215/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6912691831588745
Epoch: 8215, Batch Gradient Norm: 21.449327223208318
Epoch: 8215, Batch Gradient Norm after: 21.162065844533466
Epoch 8216/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6888343691825867
Epoch: 8216, Batch Gradient Norm: 22.47659897466133
Epoch: 8216, Batch Gradient Norm after: 21.727151195847892
Epoch 8217/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.6917099595069885
Epoch: 8217, Batch Gradient Norm: 21.264982236820273
Epoch: 8217, Batch Gradient Norm after: 20.931657900785723
Epoch 8218/10000, Prediction Accuracy = 60.146%, Loss = 0.6882070422172546
Epoch: 8218, Batch Gradient Norm: 22.30255332490197
Epoch: 8218, Batch Gradient Norm after: 21.560417528153497
Epoch 8219/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6910879611968994
Epoch: 8219, Batch Gradient Norm: 21.4032179879648
Epoch: 8219, Batch Gradient Norm after: 21.11089628649361
Epoch 8220/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6885302424430847
Epoch: 8220, Batch Gradient Norm: 22.420182771842658
Epoch: 8220, Batch Gradient Norm after: 21.66526321286172
Epoch 8221/10000, Prediction Accuracy = 60.178%, Loss = 0.6914207100868225
Epoch: 8221, Batch Gradient Norm: 21.338047966334937
Epoch: 8221, Batch Gradient Norm after: 21.036621513371344
Epoch 8222/10000, Prediction Accuracy = 60.188%, Loss = 0.6883105158805847
Epoch: 8222, Batch Gradient Norm: 22.32924237126529
Epoch: 8222, Batch Gradient Norm after: 21.563283199185364
Epoch 8223/10000, Prediction Accuracy = 60.136%, Loss = 0.6910754323005677
Epoch: 8223, Batch Gradient Norm: 21.48236216442675
Epoch: 8223, Batch Gradient Norm after: 21.229899531668085
Epoch 8224/10000, Prediction Accuracy = 60.15%, Loss = 0.6886147379875183
Epoch: 8224, Batch Gradient Norm: 22.46606143034857
Epoch: 8224, Batch Gradient Norm after: 21.722498389131772
Epoch 8225/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6914229989051819
Epoch: 8225, Batch Gradient Norm: 21.257157661609494
Epoch: 8225, Batch Gradient Norm after: 20.939561450783273
Epoch 8226/10000, Prediction Accuracy = 60.162%, Loss = 0.6880227327346802
Epoch: 8226, Batch Gradient Norm: 22.26818896907085
Epoch: 8226, Batch Gradient Norm after: 21.55617319048521
Epoch 8227/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6908947944641113
Epoch: 8227, Batch Gradient Norm: 21.31669691115749
Epoch: 8227, Batch Gradient Norm after: 21.017666271718117
Epoch 8228/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.6881492972373963
Epoch: 8228, Batch Gradient Norm: 22.295192225478335
Epoch: 8228, Batch Gradient Norm after: 21.55263265337773
Epoch 8229/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6908488750457764
Epoch: 8229, Batch Gradient Norm: 21.429687601837895
Epoch: 8229, Batch Gradient Norm after: 21.167301198244157
Epoch 8230/10000, Prediction Accuracy = 60.188%, Loss = 0.6883350849151612
Epoch: 8230, Batch Gradient Norm: 22.389111574826075
Epoch: 8230, Batch Gradient Norm after: 21.6385979259799
Epoch 8231/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6910680770874024
Epoch: 8231, Batch Gradient Norm: 21.357040382834874
Epoch: 8231, Batch Gradient Norm after: 21.083297561649964
Epoch 8232/10000, Prediction Accuracy = 60.2%, Loss = 0.6881473541259766
Epoch: 8232, Batch Gradient Norm: 22.294546857256737
Epoch: 8232, Batch Gradient Norm after: 21.55095965951279
Epoch 8233/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6908096551895142
Epoch: 8233, Batch Gradient Norm: 21.438315185570374
Epoch: 8233, Batch Gradient Norm after: 21.195269745353386
Epoch 8234/10000, Prediction Accuracy = 60.198%, Loss = 0.6882596254348755
Epoch: 8234, Batch Gradient Norm: 22.360292153710738
Epoch: 8234, Batch Gradient Norm after: 21.61373444969332
Epoch 8235/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6908645749092102
Epoch: 8235, Batch Gradient Norm: 21.378337261531005
Epoch: 8235, Batch Gradient Norm after: 21.12232165896545
Epoch 8236/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6880733728408813
Epoch: 8236, Batch Gradient Norm: 22.291319296209547
Epoch: 8236, Batch Gradient Norm after: 21.547394505976367
Epoch 8237/10000, Prediction Accuracy = 60.174%, Loss = 0.6907043218612671
Epoch: 8237, Batch Gradient Norm: 21.42857299906939
Epoch: 8237, Batch Gradient Norm after: 21.19746177368671
Epoch 8238/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.688229787349701
Epoch: 8238, Batch Gradient Norm: 22.309221343842896
Epoch: 8238, Batch Gradient Norm after: 21.566193578689635
Epoch 8239/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6906960725784301
Epoch: 8239, Batch Gradient Norm: 21.418529571592764
Epoch: 8239, Batch Gradient Norm after: 21.195368476032485
Epoch 8240/10000, Prediction Accuracy = 60.148%, Loss = 0.688078510761261
Epoch: 8240, Batch Gradient Norm: 22.28630115082605
Epoch: 8240, Batch Gradient Norm after: 21.55156111608323
Epoch 8241/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.690524959564209
Epoch: 8241, Batch Gradient Norm: 21.401795315264806
Epoch: 8241, Batch Gradient Norm after: 21.178211184759203
Epoch 8242/10000, Prediction Accuracy = 60.152%, Loss = 0.6879890441894532
Epoch: 8242, Batch Gradient Norm: 22.2728510442331
Epoch: 8242, Batch Gradient Norm after: 21.55083971995327
Epoch 8243/10000, Prediction Accuracy = 60.158%, Loss = 0.6904917597770691
Epoch: 8243, Batch Gradient Norm: 21.363606055916428
Epoch: 8243, Batch Gradient Norm after: 21.132058982826727
Epoch 8244/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6878451704978943
Epoch: 8244, Batch Gradient Norm: 22.25226962200969
Epoch: 8244, Batch Gradient Norm after: 21.554039263084277
Epoch 8245/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6903395652770996
Epoch: 8245, Batch Gradient Norm: 21.29086549644125
Epoch: 8245, Batch Gradient Norm after: 21.032808921548856
Epoch 8246/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6875625133514405
Epoch: 8246, Batch Gradient Norm: 22.231164335402948
Epoch: 8246, Batch Gradient Norm after: 21.556556248965347
Epoch 8247/10000, Prediction Accuracy = 60.17199999999999%, Loss = 0.6902336001396179
Epoch: 8247, Batch Gradient Norm: 21.208564124909554
Epoch: 8247, Batch Gradient Norm after: 20.911252740039274
Epoch 8248/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.687369167804718
Epoch: 8248, Batch Gradient Norm: 22.209546201883242
Epoch: 8248, Batch Gradient Norm after: 21.562111018921257
Epoch 8249/10000, Prediction Accuracy = 60.162%, Loss = 0.6902069568634033
Epoch: 8249, Batch Gradient Norm: 21.12373779803025
Epoch: 8249, Batch Gradient Norm after: 20.78896668171274
Epoch 8250/10000, Prediction Accuracy = 60.146%, Loss = 0.6870929837226868
Epoch: 8250, Batch Gradient Norm: 22.191777709872103
Epoch: 8250, Batch Gradient Norm after: 21.56799073538083
Epoch 8251/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6900239229202271
Epoch: 8251, Batch Gradient Norm: 21.04973546361479
Epoch: 8251, Batch Gradient Norm after: 20.665214072372773
Epoch 8252/10000, Prediction Accuracy = 60.176%, Loss = 0.6867883086204529
Epoch: 8252, Batch Gradient Norm: 22.193765977415612
Epoch: 8252, Batch Gradient Norm after: 21.566553698175294
Epoch 8253/10000, Prediction Accuracy = 60.174%, Loss = 0.6899790406227112
Epoch: 8253, Batch Gradient Norm: 21.07003167959308
Epoch: 8253, Batch Gradient Norm after: 20.662058799513314
Epoch 8254/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6868321895599365
Epoch: 8254, Batch Gradient Norm: 22.220847564847197
Epoch: 8254, Batch Gradient Norm after: 21.56105982199894
Epoch 8255/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6900503516197205
Epoch: 8255, Batch Gradient Norm: 21.189285619459085
Epoch: 8255, Batch Gradient Norm after: 20.808204630089314
Epoch 8256/10000, Prediction Accuracy = 60.2%, Loss = 0.6870596766471863
Epoch: 8256, Batch Gradient Norm: 22.28550423082116
Epoch: 8256, Batch Gradient Norm after: 21.553254333889047
Epoch 8257/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6901074767112731
Epoch: 8257, Batch Gradient Norm: 21.40859102670358
Epoch: 8257, Batch Gradient Norm after: 21.099639789210563
Epoch 8258/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6876238226890564
Epoch: 8258, Batch Gradient Norm: 22.44289324923661
Epoch: 8258, Batch Gradient Norm after: 21.689535375462743
Epoch 8259/10000, Prediction Accuracy = 60.176%, Loss = 0.69061359167099
Epoch: 8259, Batch Gradient Norm: 21.327132804182597
Epoch: 8259, Batch Gradient Norm after: 21.018063294335708
Epoch 8260/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6874305486679078
Epoch: 8260, Batch Gradient Norm: 22.30507832740619
Epoch: 8260, Batch Gradient Norm after: 21.547342972668847
Epoch 8261/10000, Prediction Accuracy = 60.182%, Loss = 0.6901502013206482
Epoch: 8261, Batch Gradient Norm: 21.488559634349844
Epoch: 8261, Batch Gradient Norm after: 21.246680319136654
Epoch 8262/10000, Prediction Accuracy = 60.152%, Loss = 0.6877464890480042
Epoch: 8262, Batch Gradient Norm: 22.431253068827854
Epoch: 8262, Batch Gradient Norm after: 21.691145542766773
Epoch 8263/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6904119491577149
Epoch: 8263, Batch Gradient Norm: 21.289965526066545
Epoch: 8263, Batch Gradient Norm after: 21.003306284428596
Epoch 8264/10000, Prediction Accuracy = 60.162%, Loss = 0.6871630311012268
Epoch: 8264, Batch Gradient Norm: 22.257310859515844
Epoch: 8264, Batch Gradient Norm after: 21.552260185155074
Epoch 8265/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6899265050888062
Epoch: 8265, Batch Gradient Norm: 21.30566939059566
Epoch: 8265, Batch Gradient Norm after: 21.0306908163079
Epoch 8266/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6871858716011048
Epoch: 8266, Batch Gradient Norm: 22.258178549311044
Epoch: 8266, Batch Gradient Norm after: 21.551751155640616
Epoch 8267/10000, Prediction Accuracy = 60.138%, Loss = 0.6898422241210938
Epoch: 8267, Batch Gradient Norm: 21.332787824566996
Epoch: 8267, Batch Gradient Norm after: 21.063471561739437
Epoch 8268/10000, Prediction Accuracy = 60.152%, Loss = 0.6871539950370789
Epoch: 8268, Batch Gradient Norm: 22.269616947697898
Epoch: 8268, Batch Gradient Norm after: 21.55478424516071
Epoch 8269/10000, Prediction Accuracy = 60.176%, Loss = 0.6898167252540588
Epoch: 8269, Batch Gradient Norm: 21.342534032443336
Epoch: 8269, Batch Gradient Norm after: 21.081007685753665
Epoch 8270/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6872168064117432
Epoch: 8270, Batch Gradient Norm: 22.264007939651478
Epoch: 8270, Batch Gradient Norm after: 21.55220094232245
Epoch 8271/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6898436546325684
Epoch: 8271, Batch Gradient Norm: 21.323752684549856
Epoch: 8271, Batch Gradient Norm after: 21.070072862607933
Epoch 8272/10000, Prediction Accuracy = 60.126%, Loss = 0.6871076464653015
Epoch: 8272, Batch Gradient Norm: 22.245169595566324
Epoch: 8272, Batch Gradient Norm after: 21.555043603681053
Epoch 8273/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6896562099456787
Epoch: 8273, Batch Gradient Norm: 21.278943799662315
Epoch: 8273, Batch Gradient Norm after: 21.008269243752068
Epoch 8274/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.686875331401825
Epoch: 8274, Batch Gradient Norm: 22.233870188253675
Epoch: 8274, Batch Gradient Norm after: 21.55943554450863
Epoch 8275/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6895761728286743
Epoch: 8275, Batch Gradient Norm: 21.20741277994292
Epoch: 8275, Batch Gradient Norm after: 20.91427807207709
Epoch 8276/10000, Prediction Accuracy = 60.182%, Loss = 0.6866860270500184
Epoch: 8276, Batch Gradient Norm: 22.205562907390025
Epoch: 8276, Batch Gradient Norm after: 21.559119280015874
Epoch 8277/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.689491081237793
Epoch: 8277, Batch Gradient Norm: 21.123778255581172
Epoch: 8277, Batch Gradient Norm after: 20.795187529285762
Epoch 8278/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.6863752603530884
Epoch: 8278, Batch Gradient Norm: 22.19020419635067
Epoch: 8278, Batch Gradient Norm after: 21.564425291626424
Epoch 8279/10000, Prediction Accuracy = 60.152%, Loss = 0.6893271446228028
Epoch: 8279, Batch Gradient Norm: 21.07625689668634
Epoch: 8279, Batch Gradient Norm after: 20.702716908266506
Epoch 8280/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6862001180648803
Epoch: 8280, Batch Gradient Norm: 22.187940494607364
Epoch: 8280, Batch Gradient Norm after: 21.563668693047912
Epoch 8281/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6893676280975342
Epoch: 8281, Batch Gradient Norm: 21.067578580975322
Epoch: 8281, Batch Gradient Norm after: 20.676706259906922
Epoch 8282/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6862092256546021
Epoch: 8282, Batch Gradient Norm: 22.2005061285609
Epoch: 8282, Batch Gradient Norm after: 21.56526196570874
Epoch 8283/10000, Prediction Accuracy = 60.186%, Loss = 0.689322555065155
Epoch: 8283, Batch Gradient Norm: 21.106172793318954
Epoch: 8283, Batch Gradient Norm after: 20.71243518570604
Epoch 8284/10000, Prediction Accuracy = 60.14%, Loss = 0.6861779570579529
Epoch: 8284, Batch Gradient Norm: 22.232187848760038
Epoch: 8284, Batch Gradient Norm after: 21.559774449241996
Epoch 8285/10000, Prediction Accuracy = 60.15%, Loss = 0.6892931342124939
Epoch: 8285, Batch Gradient Norm: 21.226912196361933
Epoch: 8285, Batch Gradient Norm after: 20.864214986847745
Epoch 8286/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6864441633224487
Epoch: 8286, Batch Gradient Norm: 22.284564449166712
Epoch: 8286, Batch Gradient Norm after: 21.545064542232755
Epoch 8287/10000, Prediction Accuracy = 60.178%, Loss = 0.6894521951675415
Epoch: 8287, Batch Gradient Norm: 21.44361133727792
Epoch: 8287, Batch Gradient Norm after: 21.17153703211887
Epoch 8288/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.687024188041687
Epoch: 8288, Batch Gradient Norm: 22.421286881014986
Epoch: 8288, Batch Gradient Norm after: 21.676924459061546
Epoch 8289/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6897855877876282
Epoch: 8289, Batch Gradient Norm: 21.338408277035402
Epoch: 8289, Batch Gradient Norm after: 21.057077329858217
Epoch 8290/10000, Prediction Accuracy = 60.162%, Loss = 0.6866366744041443
Epoch: 8290, Batch Gradient Norm: 22.286904827879194
Epoch: 8290, Batch Gradient Norm after: 21.553682435814235
Epoch 8291/10000, Prediction Accuracy = 60.164%, Loss = 0.6893405437469482
Epoch: 8291, Batch Gradient Norm: 21.414898177936806
Epoch: 8291, Batch Gradient Norm after: 21.16867723529171
Epoch 8292/10000, Prediction Accuracy = 60.148%, Loss = 0.68687504529953
Epoch: 8292, Batch Gradient Norm: 22.306907826220854
Epoch: 8292, Batch Gradient Norm after: 21.564442443784483
Epoch 8293/10000, Prediction Accuracy = 60.186%, Loss = 0.6894420742988586
Epoch: 8293, Batch Gradient Norm: 21.4176883362371
Epoch: 8293, Batch Gradient Norm after: 21.202011706869076
Epoch 8294/10000, Prediction Accuracy = 60.136%, Loss = 0.6868387460708618
Epoch: 8294, Batch Gradient Norm: 22.27508847326628
Epoch: 8294, Batch Gradient Norm after: 21.55410440759411
Epoch 8295/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.6892201662063598
Epoch: 8295, Batch Gradient Norm: 21.369384015017108
Epoch: 8295, Batch Gradient Norm after: 21.15307641439791
Epoch 8296/10000, Prediction Accuracy = 60.178%, Loss = 0.686597216129303
Epoch: 8296, Batch Gradient Norm: 22.25060475484843
Epoch: 8296, Batch Gradient Norm after: 21.559101525245868
Epoch 8297/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.6890811800956727
Epoch: 8297, Batch Gradient Norm: 21.25852752371326
Epoch: 8297, Batch Gradient Norm after: 21.012597527241965
Epoch 8298/10000, Prediction Accuracy = 60.19%, Loss = 0.6862879157066345
Epoch: 8298, Batch Gradient Norm: 22.192995995093153
Epoch: 8298, Batch Gradient Norm after: 21.557139122438397
Epoch 8299/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6889338374137879
Epoch: 8299, Batch Gradient Norm: 21.100728387872614
Epoch: 8299, Batch Gradient Norm after: 20.79724237840969
Epoch 8300/10000, Prediction Accuracy = 60.202%, Loss = 0.6857775330543519
Epoch: 8300, Batch Gradient Norm: 22.145081064748744
Epoch: 8300, Batch Gradient Norm after: 21.56995003944654
Epoch 8301/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6886759161949157
Epoch: 8301, Batch Gradient Norm: 20.921116329888417
Epoch: 8301, Batch Gradient Norm after: 20.522173107090403
Epoch 8302/10000, Prediction Accuracy = 60.176%, Loss = 0.6852412700653077
Epoch: 8302, Batch Gradient Norm: 22.108835965353684
Epoch: 8302, Batch Gradient Norm after: 21.579255207776466
Epoch 8303/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.6886022686958313
Epoch: 8303, Batch Gradient Norm: 20.78663533304163
Epoch: 8303, Batch Gradient Norm after: 20.299042024964464
Epoch 8304/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6849170446395874
Epoch: 8304, Batch Gradient Norm: 22.09622777789864
Epoch: 8304, Batch Gradient Norm after: 21.562671493819128
Epoch 8305/10000, Prediction Accuracy = 60.198%, Loss = 0.6885135293006897
Epoch: 8305, Batch Gradient Norm: 20.88679183941828
Epoch: 8305, Batch Gradient Norm after: 20.399893667908355
Epoch 8306/10000, Prediction Accuracy = 60.152%, Loss = 0.6850650548934937
Epoch: 8306, Batch Gradient Norm: 22.16804446414113
Epoch: 8306, Batch Gradient Norm after: 21.57548128040036
Epoch 8307/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6885692596435546
Epoch: 8307, Batch Gradient Norm: 20.984783699605824
Epoch: 8307, Batch Gradient Norm after: 20.50258911413693
Epoch 8308/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6852553844451904
Epoch: 8308, Batch Gradient Norm: 22.228667649303045
Epoch: 8308, Batch Gradient Norm after: 21.557431499496165
Epoch 8309/10000, Prediction Accuracy = 60.182%, Loss = 0.6887346625328064
Epoch: 8309, Batch Gradient Norm: 21.220621674873495
Epoch: 8309, Batch Gradient Norm after: 20.820367851181416
Epoch 8310/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6858789801597596
Epoch: 8310, Batch Gradient Norm: 22.31909879268814
Epoch: 8310, Batch Gradient Norm after: 21.54402769182732
Epoch 8311/10000, Prediction Accuracy = 60.158%, Loss = 0.6889467477798462
Epoch: 8311, Batch Gradient Norm: 21.55591100590818
Epoch: 8311, Batch Gradient Norm after: 21.29414984502634
Epoch 8312/10000, Prediction Accuracy = 60.164%, Loss = 0.6867032647132874
Epoch: 8312, Batch Gradient Norm: 22.608876176493485
Epoch: 8312, Batch Gradient Norm after: 21.878684422536953
Epoch 8313/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.689722990989685
Epoch: 8313, Batch Gradient Norm: 21.08217131997596
Epoch: 8313, Batch Gradient Norm after: 20.698880427837874
Epoch 8314/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6854423403739929
Epoch: 8314, Batch Gradient Norm: 22.201185566257834
Epoch: 8314, Batch Gradient Norm after: 21.557749192979273
Epoch 8315/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6886005640029907
Epoch: 8315, Batch Gradient Norm: 21.140093256031953
Epoch: 8315, Batch Gradient Norm after: 20.77226591616733
Epoch 8316/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6855631589889526
Epoch: 8316, Batch Gradient Norm: 22.225228732355816
Epoch: 8316, Batch Gradient Norm after: 21.55534177116246
Epoch 8317/10000, Prediction Accuracy = 60.176%, Loss = 0.688541305065155
Epoch: 8317, Batch Gradient Norm: 21.227714771328266
Epoch: 8317, Batch Gradient Norm after: 20.8918536231763
Epoch 8318/10000, Prediction Accuracy = 60.15%, Loss = 0.685681426525116
Epoch: 8318, Batch Gradient Norm: 22.261517536765012
Epoch: 8318, Batch Gradient Norm after: 21.550039509488414
Epoch 8319/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6885743975639343
Epoch: 8319, Batch Gradient Norm: 21.35270943056689
Epoch: 8319, Batch Gradient Norm after: 21.068360060117016
Epoch 8320/10000, Prediction Accuracy = 60.176%, Loss = 0.6859997987747193
Epoch: 8320, Batch Gradient Norm: 22.31073756877597
Epoch: 8320, Batch Gradient Norm after: 21.563034501472075
Epoch 8321/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6887197732925415
Epoch: 8321, Batch Gradient Norm: 21.44415142426766
Epoch: 8321, Batch Gradient Norm after: 21.220555169429574
Epoch 8322/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6861955642700195
Epoch: 8322, Batch Gradient Norm: 22.329642405405664
Epoch: 8322, Batch Gradient Norm after: 21.595514366446942
Epoch 8323/10000, Prediction Accuracy = 60.152%, Loss = 0.6886794447898865
Epoch: 8323, Batch Gradient Norm: 21.380972073210426
Epoch: 8323, Batch Gradient Norm after: 21.158096264120303
Epoch 8324/10000, Prediction Accuracy = 60.148%, Loss = 0.6859545469284057
Epoch: 8324, Batch Gradient Norm: 22.252623003056854
Epoch: 8324, Batch Gradient Norm after: 21.549771492740906
Epoch 8325/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6884723424911499
Epoch: 8325, Batch Gradient Norm: 21.313712792779935
Epoch: 8325, Batch Gradient Norm after: 21.088894637003197
Epoch 8326/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.6858351945877075
Epoch: 8326, Batch Gradient Norm: 22.202538145024242
Epoch: 8326, Batch Gradient Norm after: 21.555779622393548
Epoch 8327/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6883314967155456
Epoch: 8327, Batch Gradient Norm: 21.14506530146283
Epoch: 8327, Batch Gradient Norm after: 20.87304436306921
Epoch 8328/10000, Prediction Accuracy = 60.12800000000001%, Loss = 0.6852543711662292
Epoch: 8328, Batch Gradient Norm: 22.142469356936346
Epoch: 8328, Batch Gradient Norm after: 21.56969655194713
Epoch 8329/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6880093693733216
Epoch: 8329, Batch Gradient Norm: 20.924522707606787
Epoch: 8329, Batch Gradient Norm after: 20.55239998105457
Epoch 8330/10000, Prediction Accuracy = 60.146%, Loss = 0.6845807790756225
Epoch: 8330, Batch Gradient Norm: 22.08641828261582
Epoch: 8330, Batch Gradient Norm after: 21.580393909985347
Epoch 8331/10000, Prediction Accuracy = 60.174%, Loss = 0.687852680683136
Epoch: 8331, Batch Gradient Norm: 20.721262361529703
Epoch: 8331, Batch Gradient Norm after: 20.236685994196048
Epoch 8332/10000, Prediction Accuracy = 60.23%, Loss = 0.6840290427207947
Epoch: 8332, Batch Gradient Norm: 22.0490943571112
Epoch: 8332, Batch Gradient Norm after: 21.53330034036373
Epoch 8333/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6876818776130676
Epoch: 8333, Batch Gradient Norm: 20.941098470715794
Epoch: 8333, Batch Gradient Norm after: 20.502072571110695
Epoch 8334/10000, Prediction Accuracy = 60.174%, Loss = 0.6844920635223388
Epoch: 8334, Batch Gradient Norm: 22.1599574143296
Epoch: 8334, Batch Gradient Norm after: 21.56993369698836
Epoch 8335/10000, Prediction Accuracy = 60.176%, Loss = 0.6879004716873169
Epoch: 8335, Batch Gradient Norm: 21.003787911287436
Epoch: 8335, Batch Gradient Norm after: 20.562321264917486
Epoch 8336/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6846801042556763
Epoch: 8336, Batch Gradient Norm: 22.193321265431262
Epoch: 8336, Batch Gradient Norm after: 21.557969013036864
Epoch 8337/10000, Prediction Accuracy = 60.194%, Loss = 0.6880414843559265
Epoch: 8337, Batch Gradient Norm: 21.13590804769224
Epoch: 8337, Batch Gradient Norm after: 20.74234252039266
Epoch 8338/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6850143194198608
Epoch: 8338, Batch Gradient Norm: 22.24699877419959
Epoch: 8338, Batch Gradient Norm after: 21.551328906816828
Epoch 8339/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6880792021751404
Epoch: 8339, Batch Gradient Norm: 21.322725150169806
Epoch: 8339, Batch Gradient Norm after: 21.007895727238246
Epoch 8340/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6854107737541199
Epoch: 8340, Batch Gradient Norm: 22.316548288852825
Epoch: 8340, Batch Gradient Norm after: 21.562911012162925
Epoch 8341/10000, Prediction Accuracy = 60.134%, Loss = 0.6882121920585632
Epoch: 8341, Batch Gradient Norm: 21.4760631523165
Epoch: 8341, Batch Gradient Norm after: 21.24030547485622
Epoch 8342/10000, Prediction Accuracy = 60.178%, Loss = 0.6858108878135681
Epoch: 8342, Batch Gradient Norm: 22.409692602611162
Epoch: 8342, Batch Gradient Norm after: 21.685446883735317
Epoch 8343/10000, Prediction Accuracy = 60.174%, Loss = 0.6885077834129334
Epoch: 8343, Batch Gradient Norm: 21.263529991117892
Epoch: 8343, Batch Gradient Norm after: 20.999468233958893
Epoch 8344/10000, Prediction Accuracy = 60.202%, Loss = 0.6851774096488953
Epoch: 8344, Batch Gradient Norm: 22.21821748329795
Epoch: 8344, Batch Gradient Norm after: 21.562563374294037
Epoch 8345/10000, Prediction Accuracy = 60.148%, Loss = 0.6878587245941162
Epoch: 8345, Batch Gradient Norm: 21.196993411778315
Epoch: 8345, Batch Gradient Norm after: 20.911879528166118
Epoch 8346/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6849257230758667
Epoch: 8346, Batch Gradient Norm: 22.18729670240706
Epoch: 8346, Batch Gradient Norm after: 21.566441801245258
Epoch 8347/10000, Prediction Accuracy = 60.162%, Loss = 0.687774395942688
Epoch: 8347, Batch Gradient Norm: 21.075231607834287
Epoch: 8347, Batch Gradient Norm after: 20.748670795127417
Epoch 8348/10000, Prediction Accuracy = 60.124%, Loss = 0.6846565485000611
Epoch: 8348, Batch Gradient Norm: 22.14019945598075
Epoch: 8348, Batch Gradient Norm after: 21.575855571498078
Epoch 8349/10000, Prediction Accuracy = 60.212%, Loss = 0.6876514792442322
Epoch: 8349, Batch Gradient Norm: 20.908889213296206
Epoch: 8349, Batch Gradient Norm after: 20.514616640868702
Epoch 8350/10000, Prediction Accuracy = 60.126%, Loss = 0.6841001510620117
Epoch: 8350, Batch Gradient Norm: 22.09809567902858
Epoch: 8350, Batch Gradient Norm after: 21.58795502512227
Epoch 8351/10000, Prediction Accuracy = 60.164%, Loss = 0.6873731136322021
Epoch: 8351, Batch Gradient Norm: 20.76152772626283
Epoch: 8351, Batch Gradient Norm after: 20.27190817322648
Epoch 8352/10000, Prediction Accuracy = 60.15%, Loss = 0.6836141228675843
Epoch: 8352, Batch Gradient Norm: 22.080953722541047
Epoch: 8352, Batch Gradient Norm after: 21.55923462687971
Epoch 8353/10000, Prediction Accuracy = 60.186%, Loss = 0.6873206973075867
Epoch: 8353, Batch Gradient Norm: 20.894763854655164
Epoch: 8353, Batch Gradient Norm after: 20.42716569043388
Epoch 8354/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6839650869369507
Epoch: 8354, Batch Gradient Norm: 22.144342578459323
Epoch: 8354, Batch Gradient Norm after: 21.57596125778073
Epoch 8355/10000, Prediction Accuracy = 60.164%, Loss = 0.6874374508857727
Epoch: 8355, Batch Gradient Norm: 20.94468010563077
Epoch: 8355, Batch Gradient Norm after: 20.48034157922436
Epoch 8356/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.683994722366333
Epoch: 8356, Batch Gradient Norm: 22.185017637105307
Epoch: 8356, Batch Gradient Norm after: 21.573105600944135
Epoch 8357/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6874562859535217
Epoch: 8357, Batch Gradient Norm: 21.08102785070368
Epoch: 8357, Batch Gradient Norm after: 20.651140674220297
Epoch 8358/10000, Prediction Accuracy = 60.17%, Loss = 0.6843678832054139
Epoch: 8358, Batch Gradient Norm: 22.23626957309326
Epoch: 8358, Batch Gradient Norm after: 21.557878072068746
Epoch 8359/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.6876541256904602
Epoch: 8359, Batch Gradient Norm: 21.27216549159861
Epoch: 8359, Batch Gradient Norm after: 20.930586234489876
Epoch 8360/10000, Prediction Accuracy = 60.138%, Loss = 0.6848731637001038
Epoch: 8360, Batch Gradient Norm: 22.297518083020023
Epoch: 8360, Batch Gradient Norm after: 21.550745633901013
Epoch 8361/10000, Prediction Accuracy = 60.16400000000001%, Loss = 0.6877214074134826
Epoch: 8361, Batch Gradient Norm: 21.496805772100032
Epoch: 8361, Batch Gradient Norm after: 21.261521367565702
Epoch 8362/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6853751301765442
Epoch: 8362, Batch Gradient Norm: 22.44410700065679
Epoch: 8362, Batch Gradient Norm after: 21.725551495389585
Epoch 8363/10000, Prediction Accuracy = 60.134%, Loss = 0.6880853295326232
Epoch: 8363, Batch Gradient Norm: 21.230962513585027
Epoch: 8363, Batch Gradient Norm after: 20.94887842748128
Epoch 8364/10000, Prediction Accuracy = 60.19%, Loss = 0.6846302509307861
Epoch: 8364, Batch Gradient Norm: 22.20818745965743
Epoch: 8364, Batch Gradient Norm after: 21.568606369734148
Epoch 8365/10000, Prediction Accuracy = 60.178%, Loss = 0.6874314188957215
Epoch: 8365, Batch Gradient Norm: 21.144808031027075
Epoch: 8365, Batch Gradient Norm after: 20.847370525374455
Epoch 8366/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.684343421459198
Epoch: 8366, Batch Gradient Norm: 22.165072582718608
Epoch: 8366, Batch Gradient Norm after: 21.57422293152345
Epoch 8367/10000, Prediction Accuracy = 60.148%, Loss = 0.6871970534324646
Epoch: 8367, Batch Gradient Norm: 21.01301871908371
Epoch: 8367, Batch Gradient Norm after: 20.660418413250362
Epoch 8368/10000, Prediction Accuracy = 60.164%, Loss = 0.6839032530784607
Epoch: 8368, Batch Gradient Norm: 22.121295330581365
Epoch: 8368, Batch Gradient Norm after: 21.574589586550655
Epoch 8369/10000, Prediction Accuracy = 60.164%, Loss = 0.6870631694793701
Epoch: 8369, Batch Gradient Norm: 20.876039334832985
Epoch: 8369, Batch Gradient Norm after: 20.459552232573603
Epoch 8370/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6835943460464478
Epoch: 8370, Batch Gradient Norm: 22.084034576167053
Epoch: 8370, Batch Gradient Norm after: 21.585330703836018
Epoch 8371/10000, Prediction Accuracy = 60.198%, Loss = 0.6869735956192017
Epoch: 8371, Batch Gradient Norm: 20.740825986324722
Epoch: 8371, Batch Gradient Norm after: 20.247745243920853
Epoch 8372/10000, Prediction Accuracy = 60.138%, Loss = 0.6831273794174194
Epoch: 8372, Batch Gradient Norm: 22.067780436294967
Epoch: 8372, Batch Gradient Norm after: 21.551226054388703
Epoch 8373/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6867673754692077
Epoch: 8373, Batch Gradient Norm: 20.922762477098047
Epoch: 8373, Batch Gradient Norm after: 20.469102829396046
Epoch 8374/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.683522641658783
Epoch: 8374, Batch Gradient Norm: 22.155010693953727
Epoch: 8374, Batch Gradient Norm after: 21.573515254535007
Epoch 8375/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6870026350021362
Epoch: 8375, Batch Gradient Norm: 20.99403633878564
Epoch: 8375, Batch Gradient Norm after: 20.554668487796786
Epoch 8376/10000, Prediction Accuracy = 60.221999999999994%, Loss = 0.6837088108062744
Epoch: 8376, Batch Gradient Norm: 22.185511283831055
Epoch: 8376, Batch Gradient Norm after: 21.564334758309652
Epoch 8377/10000, Prediction Accuracy = 60.176%, Loss = 0.687033498287201
Epoch: 8377, Batch Gradient Norm: 21.1205501317235
Epoch: 8377, Batch Gradient Norm after: 20.730124136160466
Epoch 8378/10000, Prediction Accuracy = 60.194%, Loss = 0.6839526891708374
Epoch: 8378, Batch Gradient Norm: 22.23049597687245
Epoch: 8378, Batch Gradient Norm after: 21.556558941582775
Epoch 8379/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6870751023292542
Epoch: 8379, Batch Gradient Norm: 21.288687714209683
Epoch: 8379, Batch Gradient Norm after: 20.966439568471944
Epoch 8380/10000, Prediction Accuracy = 60.178%, Loss = 0.6844181895256043
Epoch: 8380, Batch Gradient Norm: 22.27286420212247
Epoch: 8380, Batch Gradient Norm after: 21.547040157811196
Epoch 8381/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.6872582077980042
Epoch: 8381, Batch Gradient Norm: 21.43886224708237
Epoch: 8381, Batch Gradient Norm after: 21.208361473633705
Epoch 8382/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6848265647888183
Epoch: 8382, Batch Gradient Norm: 22.312644349566902
Epoch: 8382, Batch Gradient Norm after: 21.593382754018755
Epoch 8383/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6872787833213806
Epoch: 8383, Batch Gradient Norm: 21.383348289026102
Epoch: 8383, Batch Gradient Norm after: 21.177125427705523
Epoch 8384/10000, Prediction Accuracy = 60.176%, Loss = 0.6845566034317017
Epoch: 8384, Batch Gradient Norm: 22.231858511905042
Epoch: 8384, Batch Gradient Norm after: 21.556708237674297
Epoch 8385/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.6869685053825378
Epoch: 8385, Batch Gradient Norm: 21.270078810020895
Epoch: 8385, Batch Gradient Norm after: 21.0525045006255
Epoch 8386/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6842327833175659
Epoch: 8386, Batch Gradient Norm: 22.16222217168516
Epoch: 8386, Batch Gradient Norm after: 21.559645992242405
Epoch 8387/10000, Prediction Accuracy = 60.18800000000001%, Loss = 0.6867953658103942
Epoch: 8387, Batch Gradient Norm: 21.045464933140458
Epoch: 8387, Batch Gradient Norm after: 20.768438086203506
Epoch 8388/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6835678458213806
Epoch: 8388, Batch Gradient Norm: 22.076367609862977
Epoch: 8388, Batch Gradient Norm after: 21.577365396015818
Epoch 8389/10000, Prediction Accuracy = 60.126%, Loss = 0.6864405035972595
Epoch: 8389, Batch Gradient Norm: 20.75147857042058
Epoch: 8389, Batch Gradient Norm after: 20.34001526773485
Epoch 8390/10000, Prediction Accuracy = 60.174%, Loss = 0.6826927185058593
Epoch: 8390, Batch Gradient Norm: 22.002638746621304
Epoch: 8390, Batch Gradient Norm after: 21.532395454358774
Epoch 8391/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6862065196037292
Epoch: 8391, Batch Gradient Norm: 20.83445348464736
Epoch: 8391, Batch Gradient Norm after: 20.423637000568963
Epoch 8392/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6829522252082825
Epoch: 8392, Batch Gradient Norm: 22.050220734684263
Epoch: 8392, Batch Gradient Norm after: 21.572446913304482
Epoch 8393/10000, Prediction Accuracy = 60.202%, Loss = 0.6863517999649048
Epoch: 8393, Batch Gradient Norm: 20.727683056579195
Epoch: 8393, Batch Gradient Norm after: 20.25298979763603
Epoch 8394/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6825801372528076
Epoch: 8394, Batch Gradient Norm: 22.040901941626718
Epoch: 8394, Batch Gradient Norm after: 21.54172055594245
Epoch 8395/10000, Prediction Accuracy = 60.166%, Loss = 0.6861805319786072
Epoch: 8395, Batch Gradient Norm: 20.898026545712547
Epoch: 8395, Batch Gradient Norm after: 20.461545401044358
Epoch 8396/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6829479098320007
Epoch: 8396, Batch Gradient Norm: 22.118762204040845
Epoch: 8396, Batch Gradient Norm after: 21.567620463458656
Epoch 8397/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6863776326179505
Epoch: 8397, Batch Gradient Norm: 20.90778970410638
Epoch: 8397, Batch Gradient Norm after: 20.464013607756552
Epoch 8398/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6829784393310547
Epoch: 8398, Batch Gradient Norm: 22.125841395262082
Epoch: 8398, Batch Gradient Norm after: 21.566622644419965
Epoch 8399/10000, Prediction Accuracy = 60.152%, Loss = 0.6863694190979004
Epoch: 8399, Batch Gradient Norm: 20.94862322344693
Epoch: 8399, Batch Gradient Norm after: 20.51196872881598
Epoch 8400/10000, Prediction Accuracy = 60.214%, Loss = 0.6829843282699585
Epoch: 8400, Batch Gradient Norm: 22.15173335747939
Epoch: 8400, Batch Gradient Norm after: 21.563606286515526
Epoch 8401/10000, Prediction Accuracy = 60.152%, Loss = 0.6863288402557373
Epoch: 8401, Batch Gradient Norm: 21.038772533347927
Epoch: 8401, Batch Gradient Norm after: 20.623374543597762
Epoch 8402/10000, Prediction Accuracy = 60.178%, Loss = 0.6832218170166016
Epoch: 8402, Batch Gradient Norm: 22.18077465067103
Epoch: 8402, Batch Gradient Norm after: 21.550420611904105
Epoch 8403/10000, Prediction Accuracy = 60.17%, Loss = 0.6864871501922607
Epoch: 8403, Batch Gradient Norm: 21.16296279566989
Epoch: 8403, Batch Gradient Norm after: 20.8119492662007
Epoch 8404/10000, Prediction Accuracy = 60.15%, Loss = 0.68357253074646
Epoch: 8404, Batch Gradient Norm: 22.209145981855244
Epoch: 8404, Batch Gradient Norm after: 21.547787290091975
Epoch 8405/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.686473834514618
Epoch: 8405, Batch Gradient Norm: 21.262248589218224
Epoch: 8405, Batch Gradient Norm after: 20.973795968201202
Epoch 8406/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6837202072143554
Epoch: 8406, Batch Gradient Norm: 22.229073734722732
Epoch: 8406, Batch Gradient Norm after: 21.54702714300366
Epoch 8407/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6864435076713562
Epoch: 8407, Batch Gradient Norm: 21.31409949494193
Epoch: 8407, Batch Gradient Norm after: 21.0710709954623
Epoch 8408/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6838306427001953
Epoch: 8408, Batch Gradient Norm: 22.223169258582114
Epoch: 8408, Batch Gradient Norm after: 21.551446313979422
Epoch 8409/10000, Prediction Accuracy = 60.17%, Loss = 0.6864431262016296
Epoch: 8409, Batch Gradient Norm: 21.26201586613536
Epoch: 8409, Batch Gradient Norm after: 21.03576002238291
Epoch 8410/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6836512804031372
Epoch: 8410, Batch Gradient Norm: 22.1700702802856
Epoch: 8410, Batch Gradient Norm after: 21.557814772569913
Epoch 8411/10000, Prediction Accuracy = 60.12800000000001%, Loss = 0.6861973404884338
Epoch: 8411, Batch Gradient Norm: 21.092246753391898
Epoch: 8411, Batch Gradient Norm after: 20.821744305913867
Epoch 8412/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6831124424934387
Epoch: 8412, Batch Gradient Norm: 22.096804280877382
Epoch: 8412, Batch Gradient Norm after: 21.56445737877997
Epoch 8413/10000, Prediction Accuracy = 60.19%, Loss = 0.6859618425369263
Epoch: 8413, Batch Gradient Norm: 20.837340410392
Epoch: 8413, Batch Gradient Norm after: 20.468755657650892
Epoch 8414/10000, Prediction Accuracy = 60.17%, Loss = 0.6824629783630372
Epoch: 8414, Batch Gradient Norm: 22.014688692048473
Epoch: 8414, Batch Gradient Norm after: 21.562414203568544
Epoch 8415/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6857678771018982
Epoch: 8415, Batch Gradient Norm: 20.68987381447883
Epoch: 8415, Batch Gradient Norm after: 20.24423345630126
Epoch 8416/10000, Prediction Accuracy = 60.138%, Loss = 0.6819845557212829
Epoch: 8416, Batch Gradient Norm: 21.993191161219343
Epoch: 8416, Batch Gradient Norm after: 21.519136525186003
Epoch 8417/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6855538010597229
Epoch: 8417, Batch Gradient Norm: 20.90284669597736
Epoch: 8417, Batch Gradient Norm after: 20.5134166225897
Epoch 8418/10000, Prediction Accuracy = 60.178%, Loss = 0.6824615001678467
Epoch: 8418, Batch Gradient Norm: 22.079928186006605
Epoch: 8418, Batch Gradient Norm after: 21.565802828572046
Epoch 8419/10000, Prediction Accuracy = 60.148%, Loss = 0.6857775688171387
Epoch: 8419, Batch Gradient Norm: 20.81403938434185
Epoch: 8419, Batch Gradient Norm after: 20.374636470640237
Epoch 8420/10000, Prediction Accuracy = 60.226%, Loss = 0.6822185635566711
Epoch: 8420, Batch Gradient Norm: 22.05985088378625
Epoch: 8420, Batch Gradient Norm after: 21.574505477028797
Epoch 8421/10000, Prediction Accuracy = 60.14%, Loss = 0.6856694221496582
Epoch: 8421, Batch Gradient Norm: 20.737925903077944
Epoch: 8421, Batch Gradient Norm after: 20.2482348085989
Epoch 8422/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6819034695625306
Epoch: 8422, Batch Gradient Norm: 22.054156192891693
Epoch: 8422, Batch Gradient Norm after: 21.55298530397015
Epoch 8423/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.6855534076690674
Epoch: 8423, Batch Gradient Norm: 20.862394607067106
Epoch: 8423, Batch Gradient Norm after: 20.39010503161056
Epoch 8424/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6822332382202149
Epoch: 8424, Batch Gradient Norm: 22.109645353450315
Epoch: 8424, Batch Gradient Norm after: 21.55891719668694
Epoch 8425/10000, Prediction Accuracy = 60.174%, Loss = 0.6857753992080688
Epoch: 8425, Batch Gradient Norm: 20.940018339030402
Epoch: 8425, Batch Gradient Norm after: 20.499878922878867
Epoch 8426/10000, Prediction Accuracy = 60.136%, Loss = 0.6824499487876892
Epoch: 8426, Batch Gradient Norm: 22.13811167188899
Epoch: 8426, Batch Gradient Norm after: 21.55627431004284
Epoch 8427/10000, Prediction Accuracy = 60.176%, Loss = 0.68575359582901
Epoch: 8427, Batch Gradient Norm: 21.04647708562264
Epoch: 8427, Batch Gradient Norm after: 20.649989930831655
Epoch 8428/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.6826090335845947
Epoch: 8428, Batch Gradient Norm: 22.175774289194663
Epoch: 8428, Batch Gradient Norm after: 21.548648976056803
Epoch 8429/10000, Prediction Accuracy = 60.146%, Loss = 0.685772967338562
Epoch: 8429, Batch Gradient Norm: 21.16739255626935
Epoch: 8429, Batch Gradient Norm after: 20.82966928639921
Epoch 8430/10000, Prediction Accuracy = 60.226%, Loss = 0.6829174399375916
Epoch: 8430, Batch Gradient Norm: 22.202330328553824
Epoch: 8430, Batch Gradient Norm after: 21.5417268859436
Epoch 8431/10000, Prediction Accuracy = 60.162%, Loss = 0.6858643174171448
Epoch: 8431, Batch Gradient Norm: 21.266675750459427
Epoch: 8431, Batch Gradient Norm after: 20.99398893530024
Epoch 8432/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.683142364025116
Epoch: 8432, Batch Gradient Norm: 22.20918429775929
Epoch: 8432, Batch Gradient Norm after: 21.541546585648614
Epoch 8433/10000, Prediction Accuracy = 60.148%, Loss = 0.685787045955658
Epoch: 8433, Batch Gradient Norm: 21.29837441956603
Epoch: 8433, Batch Gradient Norm after: 21.06556349745825
Epoch 8434/10000, Prediction Accuracy = 60.176%, Loss = 0.6831575274467468
Epoch: 8434, Batch Gradient Norm: 22.185870351974554
Epoch: 8434, Batch Gradient Norm after: 21.53886643870487
Epoch 8435/10000, Prediction Accuracy = 60.164%, Loss = 0.6857216477394104
Epoch: 8435, Batch Gradient Norm: 21.213433832500236
Epoch: 8435, Batch Gradient Norm after: 20.9860478684173
Epoch 8436/10000, Prediction Accuracy = 60.148%, Loss = 0.682987380027771
Epoch: 8436, Batch Gradient Norm: 22.123359012672836
Epoch: 8436, Batch Gradient Norm after: 21.552930215963
Epoch 8437/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.6855558276176452
Epoch: 8437, Batch Gradient Norm: 20.96884686768074
Epoch: 8437, Batch Gradient Norm after: 20.679960596300848
Epoch 8438/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6822295546531677
Epoch: 8438, Batch Gradient Norm: 22.033078313439027
Epoch: 8438, Batch Gradient Norm after: 21.57689207267793
Epoch 8439/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6851649165153504
Epoch: 8439, Batch Gradient Norm: 20.63560349952583
Epoch: 8439, Batch Gradient Norm after: 20.200295437459012
Epoch 8440/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6812475085258484
Epoch: 8440, Batch Gradient Norm: 21.957903828484156
Epoch: 8440, Batch Gradient Norm after: 21.49882094441324
Epoch 8441/10000, Prediction Accuracy = 60.15%, Loss = 0.6849414110183716
Epoch: 8441, Batch Gradient Norm: 20.920961618859867
Epoch: 8441, Batch Gradient Norm after: 20.574444487108224
Epoch 8442/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6820086121559144
Epoch: 8442, Batch Gradient Norm: 22.047600590963512
Epoch: 8442, Batch Gradient Norm after: 21.56469890700957
Epoch 8443/10000, Prediction Accuracy = 60.134%, Loss = 0.6851396679878234
Epoch: 8443, Batch Gradient Norm: 20.73579302141564
Epoch: 8443, Batch Gradient Norm after: 20.303919302816446
Epoch 8444/10000, Prediction Accuracy = 60.23%, Loss = 0.6813979506492615
Epoch: 8444, Batch Gradient Norm: 22.007672157018554
Epoch: 8444, Batch Gradient Norm after: 21.539951672545886
Epoch 8445/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6849311590194702
Epoch: 8445, Batch Gradient Norm: 20.817034505089463
Epoch: 8445, Batch Gradient Norm after: 20.38873650282078
Epoch 8446/10000, Prediction Accuracy = 60.178%, Loss = 0.6816208720207214
Epoch: 8446, Batch Gradient Norm: 22.046117936667148
Epoch: 8446, Batch Gradient Norm after: 21.565853644435023
Epoch 8447/10000, Prediction Accuracy = 60.178%, Loss = 0.6850924611091613
Epoch: 8447, Batch Gradient Norm: 20.727477396797575
Epoch: 8447, Batch Gradient Norm after: 20.256217355805244
Epoch 8448/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.6813854455947876
Epoch: 8448, Batch Gradient Norm: 22.0303678612213
Epoch: 8448, Batch Gradient Norm after: 21.547752424341773
Epoch 8449/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6849378824234009
Epoch: 8449, Batch Gradient Norm: 20.831727345827108
Epoch: 8449, Batch Gradient Norm after: 20.38315923083918
Epoch 8450/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6815230369567871
Epoch: 8450, Batch Gradient Norm: 22.07839047406085
Epoch: 8450, Batch Gradient Norm after: 21.557593015217304
Epoch 8451/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6849801421165467
Epoch: 8451, Batch Gradient Norm: 20.8527791744563
Epoch: 8451, Batch Gradient Norm after: 20.405376617724155
Epoch 8452/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6815701603889466
Epoch: 8452, Batch Gradient Norm: 22.08846139977248
Epoch: 8452, Batch Gradient Norm after: 21.55284614766851
Epoch 8453/10000, Prediction Accuracy = 60.178%, Loss = 0.6850242733955383
Epoch: 8453, Batch Gradient Norm: 20.90595784381727
Epoch: 8453, Batch Gradient Norm after: 20.47608102579343
Epoch 8454/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6816504597663879
Epoch: 8454, Batch Gradient Norm: 22.11047845944752
Epoch: 8454, Batch Gradient Norm after: 21.547158754899613
Epoch 8455/10000, Prediction Accuracy = 60.15%, Loss = 0.6849807739257813
Epoch: 8455, Batch Gradient Norm: 21.01241375428922
Epoch: 8455, Batch Gradient Norm after: 20.61528069763898
Epoch 8456/10000, Prediction Accuracy = 60.178%, Loss = 0.6818582892417908
Epoch: 8456, Batch Gradient Norm: 22.14159717981364
Epoch: 8456, Batch Gradient Norm after: 21.537881307606863
Epoch 8457/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6850605607032776
Epoch: 8457, Batch Gradient Norm: 21.109636378482502
Epoch: 8457, Batch Gradient Norm after: 20.77169936994693
Epoch 8458/10000, Prediction Accuracy = 60.14%, Loss = 0.6821763634681701
Epoch: 8458, Batch Gradient Norm: 22.154844549628983
Epoch: 8458, Batch Gradient Norm after: 21.53652564604491
Epoch 8459/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6851145029067993
Epoch: 8459, Batch Gradient Norm: 21.148375218999863
Epoch: 8459, Batch Gradient Norm after: 20.85448080384496
Epoch 8460/10000, Prediction Accuracy = 60.15400000000001%, Loss = 0.6822017669677735
Epoch: 8460, Batch Gradient Norm: 22.147924031918702
Epoch: 8460, Batch Gradient Norm after: 21.54002831915452
Epoch 8461/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6849745750427246
Epoch: 8461, Batch Gradient Norm: 21.1217205206047
Epoch: 8461, Batch Gradient Norm after: 20.83974116758444
Epoch 8462/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6820446491241455
Epoch: 8462, Batch Gradient Norm: 22.12005357139805
Epoch: 8462, Batch Gradient Norm after: 21.540888356281723
Epoch 8463/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6848819255828857
Epoch: 8463, Batch Gradient Norm: 21.014374801108772
Epoch: 8463, Batch Gradient Norm after: 20.714403768330534
Epoch 8464/10000, Prediction Accuracy = 60.24000000000001%, Loss = 0.6817616820335388
Epoch: 8464, Batch Gradient Norm: 22.05906710747783
Epoch: 8464, Batch Gradient Norm after: 21.550968350579893
Epoch 8465/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6846928477287293
Epoch: 8465, Batch Gradient Norm: 20.82173094281183
Epoch: 8465, Batch Gradient Norm after: 20.453720454950567
Epoch 8466/10000, Prediction Accuracy = 60.23%, Loss = 0.6811330318450928
Epoch: 8466, Batch Gradient Norm: 22.003336954002265
Epoch: 8466, Batch Gradient Norm after: 21.559734483250768
Epoch 8467/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6844204902648926
Epoch: 8467, Batch Gradient Norm: 20.6495188731368
Epoch: 8467, Batch Gradient Norm after: 20.191007289001764
Epoch 8468/10000, Prediction Accuracy = 60.162%, Loss = 0.6806821942329406
Epoch: 8468, Batch Gradient Norm: 21.972151442818117
Epoch: 8468, Batch Gradient Norm after: 21.510647704333678
Epoch 8469/10000, Prediction Accuracy = 60.198%, Loss = 0.6843965768814086
Epoch: 8469, Batch Gradient Norm: 20.88300518978902
Epoch: 8469, Batch Gradient Norm after: 20.503005598961956
Epoch 8470/10000, Prediction Accuracy = 60.138%, Loss = 0.681307566165924
Epoch: 8470, Batch Gradient Norm: 22.04356704335908
Epoch: 8470, Batch Gradient Norm after: 21.55358362799519
Epoch 8471/10000, Prediction Accuracy = 60.194%, Loss = 0.6844941973686218
Epoch: 8471, Batch Gradient Norm: 20.778799956862926
Epoch: 8471, Batch Gradient Norm after: 20.353501078186714
Epoch 8472/10000, Prediction Accuracy = 60.148%, Loss = 0.6808898210525512
Epoch: 8472, Batch Gradient Norm: 22.017346156934977
Epoch: 8472, Batch Gradient Norm after: 21.554637129679538
Epoch 8473/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.684328043460846
Epoch: 8473, Batch Gradient Norm: 20.71306444085846
Epoch: 8473, Batch Gradient Norm after: 20.247756010127528
Epoch 8474/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6807010293006897
Epoch: 8474, Batch Gradient Norm: 22.007181965000754
Epoch: 8474, Batch Gradient Norm after: 21.537156747947925
Epoch 8475/10000, Prediction Accuracy = 60.178%, Loss = 0.6843247413635254
Epoch: 8475, Batch Gradient Norm: 20.787385807481964
Epoch: 8475, Batch Gradient Norm after: 20.340697692469014
Epoch 8476/10000, Prediction Accuracy = 60.24799999999999%, Loss = 0.6808428049087525
Epoch: 8476, Batch Gradient Norm: 22.0402400386363
Epoch: 8476, Batch Gradient Norm after: 21.555657917037777
Epoch 8477/10000, Prediction Accuracy = 60.13800000000001%, Loss = 0.6842973828315735
Epoch: 8477, Batch Gradient Norm: 20.772018652119407
Epoch: 8477, Batch Gradient Norm after: 20.2997545699092
Epoch 8478/10000, Prediction Accuracy = 60.206%, Loss = 0.6807160258293152
Epoch: 8478, Batch Gradient Norm: 22.046341146329954
Epoch: 8478, Batch Gradient Norm after: 21.55570317106077
Epoch 8479/10000, Prediction Accuracy = 60.162%, Loss = 0.68430415391922
Epoch: 8479, Batch Gradient Norm: 20.778694912588104
Epoch: 8479, Batch Gradient Norm after: 20.30214006298397
Epoch 8480/10000, Prediction Accuracy = 60.138%, Loss = 0.6807772755622864
Epoch: 8480, Batch Gradient Norm: 22.054581714281888
Epoch: 8480, Batch Gradient Norm after: 21.556982579361666
Epoch 8481/10000, Prediction Accuracy = 60.198%, Loss = 0.6843269824981689
Epoch: 8481, Batch Gradient Norm: 20.798381576238754
Epoch: 8481, Batch Gradient Norm after: 20.328306480504235
Epoch 8482/10000, Prediction Accuracy = 60.15%, Loss = 0.6807487368583679
Epoch: 8482, Batch Gradient Norm: 22.061525859239605
Epoch: 8482, Batch Gradient Norm after: 21.55332312003379
Epoch 8483/10000, Prediction Accuracy = 60.146%, Loss = 0.6842369914054871
Epoch: 8483, Batch Gradient Norm: 20.853041953062416
Epoch: 8483, Batch Gradient Norm after: 20.397087560370416
Epoch 8484/10000, Prediction Accuracy = 60.188%, Loss = 0.6808086037635803
Epoch: 8484, Batch Gradient Norm: 22.084780972723667
Epoch: 8484, Batch Gradient Norm after: 21.541599341704632
Epoch 8485/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.6842700123786927
Epoch: 8485, Batch Gradient Norm: 20.95403809131292
Epoch: 8485, Batch Gradient Norm after: 20.542378328472957
Epoch 8486/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6810802578926086
Epoch: 8486, Batch Gradient Norm: 22.110492067622296
Epoch: 8486, Batch Gradient Norm after: 21.533485497168897
Epoch 8487/10000, Prediction Accuracy = 60.146%, Loss = 0.6843106627464295
Epoch: 8487, Batch Gradient Norm: 21.061695430149438
Epoch: 8487, Batch Gradient Norm after: 20.70828245454301
Epoch 8488/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.6812798380851746
Epoch: 8488, Batch Gradient Norm: 22.13502673497584
Epoch: 8488, Batch Gradient Norm after: 21.527958485187927
Epoch 8489/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6842950820922852
Epoch: 8489, Batch Gradient Norm: 21.16484284110644
Epoch: 8489, Batch Gradient Norm after: 20.867252347260717
Epoch 8490/10000, Prediction Accuracy = 60.162%, Loss = 0.6815626740455627
Epoch: 8490, Batch Gradient Norm: 22.143457084822685
Epoch: 8490, Batch Gradient Norm after: 21.528666452379337
Epoch 8491/10000, Prediction Accuracy = 60.188%, Loss = 0.6843786001205444
Epoch: 8491, Batch Gradient Norm: 21.156996275474583
Epoch: 8491, Batch Gradient Norm after: 20.899820507170027
Epoch 8492/10000, Prediction Accuracy = 60.134%, Loss = 0.6815659284591675
Epoch: 8492, Batch Gradient Norm: 22.10397673334001
Epoch: 8492, Batch Gradient Norm after: 21.538662549821435
Epoch 8493/10000, Prediction Accuracy = 60.202%, Loss = 0.6841809511184692
Epoch: 8493, Batch Gradient Norm: 21.003439909588916
Epoch: 8493, Batch Gradient Norm after: 20.720040225570447
Epoch 8494/10000, Prediction Accuracy = 60.15%, Loss = 0.6810093283653259
Epoch: 8494, Batch Gradient Norm: 22.039086090127533
Epoch: 8494, Batch Gradient Norm after: 21.546767073696248
Epoch 8495/10000, Prediction Accuracy = 60.164%, Loss = 0.6838914036750794
Epoch: 8495, Batch Gradient Norm: 20.778727167440447
Epoch: 8495, Batch Gradient Norm after: 20.412708622170832
Epoch 8496/10000, Prediction Accuracy = 60.234%, Loss = 0.6803756713867187
Epoch: 8496, Batch Gradient Norm: 21.96716508854098
Epoch: 8496, Batch Gradient Norm after: 21.541619413961186
Epoch 8497/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.6837374210357666
Epoch: 8497, Batch Gradient Norm: 20.651861173988024
Epoch: 8497, Batch Gradient Norm after: 20.229732548720346
Epoch 8498/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6799893140792846
Epoch: 8498, Batch Gradient Norm: 21.941192843177483
Epoch: 8498, Batch Gradient Norm after: 21.502866771085266
Epoch 8499/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6835310459136963
Epoch: 8499, Batch Gradient Norm: 20.84568935042799
Epoch: 8499, Batch Gradient Norm after: 20.47553370434963
Epoch 8500/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6804137110710144
Epoch: 8500, Batch Gradient Norm: 22.00880296575853
Epoch: 8500, Batch Gradient Norm after: 21.548968745055866
Epoch 8501/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6836936593055725
Epoch: 8501, Batch Gradient Norm: 20.699130872160183
Epoch: 8501, Batch Gradient Norm after: 20.26427498832819
Epoch 8502/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6800700068473816
Epoch: 8502, Batch Gradient Norm: 21.96916494401023
Epoch: 8502, Batch Gradient Norm after: 21.524134108618565
Epoch 8503/10000, Prediction Accuracy = 60.186%, Loss = 0.6835989236831665
Epoch: 8503, Batch Gradient Norm: 20.771581704018296
Epoch: 8503, Batch Gradient Norm after: 20.361490578245103
Epoch 8504/10000, Prediction Accuracy = 60.146%, Loss = 0.6801883220672608
Epoch: 8504, Batch Gradient Norm: 21.997848808080363
Epoch: 8504, Batch Gradient Norm after: 21.551953215297516
Epoch 8505/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6835396647453308
Epoch: 8505, Batch Gradient Norm: 20.671562284017945
Epoch: 8505, Batch Gradient Norm after: 20.206600754124647
Epoch 8506/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6798145294189453
Epoch: 8506, Batch Gradient Norm: 21.980668474439586
Epoch: 8506, Batch Gradient Norm after: 21.521776839677784
Epoch 8507/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6834696888923645
Epoch: 8507, Batch Gradient Norm: 20.816791741195917
Epoch: 8507, Batch Gradient Norm after: 20.39996993549911
Epoch 8508/10000, Prediction Accuracy = 60.25%, Loss = 0.6802051305770874
Epoch: 8508, Batch Gradient Norm: 22.02974590760665
Epoch: 8508, Batch Gradient Norm after: 21.54626838900295
Epoch 8509/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6835805773735046
Epoch: 8509, Batch Gradient Norm: 20.778868974040204
Epoch: 8509, Batch Gradient Norm after: 20.34622301889824
Epoch 8510/10000, Prediction Accuracy = 60.234%, Loss = 0.6800156950950622
Epoch: 8510, Batch Gradient Norm: 22.02087613470095
Epoch: 8510, Batch Gradient Norm after: 21.55407659889713
Epoch 8511/10000, Prediction Accuracy = 60.138%, Loss = 0.6834592938423156
Epoch: 8511, Batch Gradient Norm: 20.734487685456035
Epoch: 8511, Batch Gradient Norm after: 20.27068298080961
Epoch 8512/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6798737406730652
Epoch: 8512, Batch Gradient Norm: 22.015335457978388
Epoch: 8512, Batch Gradient Norm after: 21.552362969001788
Epoch 8513/10000, Prediction Accuracy = 60.174%, Loss = 0.6834833860397339
Epoch: 8513, Batch Gradient Norm: 20.733839315016397
Epoch: 8513, Batch Gradient Norm after: 20.269268504692185
Epoch 8514/10000, Prediction Accuracy = 60.136%, Loss = 0.6799018502235412
Epoch: 8514, Batch Gradient Norm: 22.014464000718103
Epoch: 8514, Batch Gradient Norm after: 21.55109726376791
Epoch 8515/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6834223747253418
Epoch: 8515, Batch Gradient Norm: 20.750161265723786
Epoch: 8515, Batch Gradient Norm after: 20.287509253468674
Epoch 8516/10000, Prediction Accuracy = 60.162%, Loss = 0.6798128128051758
Epoch: 8516, Batch Gradient Norm: 22.024857109209442
Epoch: 8516, Batch Gradient Norm after: 21.55243182326959
Epoch 8517/10000, Prediction Accuracy = 60.176%, Loss = 0.6833367347717285
Epoch: 8517, Batch Gradient Norm: 20.744501071890557
Epoch: 8517, Batch Gradient Norm after: 20.273586631196572
Epoch 8518/10000, Prediction Accuracy = 60.238%, Loss = 0.6797643661499023
Epoch: 8518, Batch Gradient Norm: 22.02937561411778
Epoch: 8518, Batch Gradient Norm after: 21.555542045709156
Epoch 8519/10000, Prediction Accuracy = 60.178%, Loss = 0.6833845257759095
Epoch: 8519, Batch Gradient Norm: 20.742466683192262
Epoch: 8519, Batch Gradient Norm after: 20.27241595261448
Epoch 8520/10000, Prediction Accuracy = 60.234%, Loss = 0.6797301888465881
Epoch: 8520, Batch Gradient Norm: 22.029950949509324
Epoch: 8520, Batch Gradient Norm after: 21.554424873104438
Epoch 8521/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6832794904708862
Epoch: 8521, Batch Gradient Norm: 20.771007249467246
Epoch: 8521, Batch Gradient Norm after: 20.29921274076579
Epoch 8522/10000, Prediction Accuracy = 60.202%, Loss = 0.6797060251235962
Epoch: 8522, Batch Gradient Norm: 22.044028415230972
Epoch: 8522, Batch Gradient Norm after: 21.5468180039585
Epoch 8523/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.683270525932312
Epoch: 8523, Batch Gradient Norm: 20.827146703906568
Epoch: 8523, Batch Gradient Norm after: 20.376098689238816
Epoch 8524/10000, Prediction Accuracy = 60.152%, Loss = 0.6799016952514648
Epoch: 8524, Batch Gradient Norm: 22.05937909453495
Epoch: 8524, Batch Gradient Norm after: 21.547137400957606
Epoch 8525/10000, Prediction Accuracy = 60.186%, Loss = 0.6833646893501282
Epoch: 8525, Batch Gradient Norm: 20.872632255889663
Epoch: 8525, Batch Gradient Norm after: 20.457830341860035
Epoch 8526/10000, Prediction Accuracy = 60.128%, Loss = 0.679973590373993
Epoch: 8526, Batch Gradient Norm: 22.064827518222266
Epoch: 8526, Batch Gradient Norm after: 21.54473086594503
Epoch 8527/10000, Prediction Accuracy = 60.16000000000001%, Loss = 0.6832459568977356
Epoch: 8527, Batch Gradient Norm: 20.896549209930374
Epoch: 8527, Batch Gradient Norm after: 20.50281227385483
Epoch 8528/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.6799227356910705
Epoch: 8528, Batch Gradient Norm: 22.067201588200614
Epoch: 8528, Batch Gradient Norm after: 21.54067300455768
Epoch 8529/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.6832167267799377
Epoch: 8529, Batch Gradient Norm: 20.909454178107055
Epoch: 8529, Batch Gradient Norm after: 20.535145548856512
Epoch 8530/10000, Prediction Accuracy = 60.262%, Loss = 0.6799640774726867
Epoch: 8530, Batch Gradient Norm: 22.052399066587785
Epoch: 8530, Batch Gradient Norm after: 21.540431241571113
Epoch 8531/10000, Prediction Accuracy = 60.17199999999999%, Loss = 0.6831746578216553
Epoch: 8531, Batch Gradient Norm: 20.864745219100254
Epoch: 8531, Batch Gradient Norm after: 20.491924040271787
Epoch 8532/10000, Prediction Accuracy = 60.226%, Loss = 0.6797576546669006
Epoch: 8532, Batch Gradient Norm: 22.02927037392756
Epoch: 8532, Batch Gradient Norm after: 21.547182973222174
Epoch 8533/10000, Prediction Accuracy = 60.13199999999999%, Loss = 0.6829922795295715
Epoch: 8533, Batch Gradient Norm: 20.783923630325074
Epoch: 8533, Batch Gradient Norm after: 20.374091889396794
Epoch 8534/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6795040726661682
Epoch: 8534, Batch Gradient Norm: 22.002603726380038
Epoch: 8534, Batch Gradient Norm after: 21.56144653304067
Epoch 8535/10000, Prediction Accuracy = 60.19%, Loss = 0.6829561352729797
Epoch: 8535, Batch Gradient Norm: 20.64422828180805
Epoch: 8535, Batch Gradient Norm after: 20.18177597323421
Epoch 8536/10000, Prediction Accuracy = 60.13399999999999%, Loss = 0.6791593432426453
Epoch: 8536, Batch Gradient Norm: 21.95817213252135
Epoch: 8536, Batch Gradient Norm after: 21.51240326159428
Epoch 8537/10000, Prediction Accuracy = 60.21%, Loss = 0.682780122756958
Epoch: 8537, Batch Gradient Norm: 20.857774976201156
Epoch: 8537, Batch Gradient Norm after: 20.477356218091884
Epoch 8538/10000, Prediction Accuracy = 60.178%, Loss = 0.6796080946922303
Epoch: 8538, Batch Gradient Norm: 22.028350207106676
Epoch: 8538, Batch Gradient Norm after: 21.54335569269215
Epoch 8539/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6828599214553833
Epoch: 8539, Batch Gradient Norm: 20.794151517273775
Epoch: 8539, Batch Gradient Norm after: 20.394527659838964
Epoch 8540/10000, Prediction Accuracy = 60.221999999999994%, Loss = 0.6794035315513611
Epoch: 8540, Batch Gradient Norm: 22.00067837202483
Epoch: 8540, Batch Gradient Norm after: 21.553152874062043
Epoch 8541/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6828155875205993
Epoch: 8541, Batch Gradient Norm: 20.667429207193972
Epoch: 8541, Batch Gradient Norm after: 20.22504475272623
Epoch 8542/10000, Prediction Accuracy = 60.238%, Loss = 0.6790324449539185
Epoch: 8542, Batch Gradient Norm: 21.95656082616768
Epoch: 8542, Batch Gradient Norm after: 21.517627157251553
Epoch 8543/10000, Prediction Accuracy = 60.152%, Loss = 0.6825990319252014
Epoch: 8543, Batch Gradient Norm: 20.81457949708456
Epoch: 8543, Batch Gradient Norm after: 20.413682088402823
Epoch 8544/10000, Prediction Accuracy = 60.214%, Loss = 0.6793251395225525
Epoch: 8544, Batch Gradient Norm: 22.014519732521382
Epoch: 8544, Batch Gradient Norm after: 21.548617841110733
Epoch 8545/10000, Prediction Accuracy = 60.162%, Loss = 0.6826982021331787
Epoch: 8545, Batch Gradient Norm: 20.742853232022757
Epoch: 8545, Batch Gradient Norm after: 20.314460558042008
Epoch 8546/10000, Prediction Accuracy = 60.17%, Loss = 0.6791733503341675
Epoch: 8546, Batch Gradient Norm: 21.985833824157012
Epoch: 8546, Batch Gradient Norm after: 21.549268366048896
Epoch 8547/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6826748251914978
Epoch: 8547, Batch Gradient Norm: 20.671351322558415
Epoch: 8547, Batch Gradient Norm after: 20.2222021232367
Epoch 8548/10000, Prediction Accuracy = 60.146%, Loss = 0.6789399743080139
Epoch: 8548, Batch Gradient Norm: 21.965799245539763
Epoch: 8548, Batch Gradient Norm after: 21.52327715037002
Epoch 8549/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6824922561645508
Epoch: 8549, Batch Gradient Norm: 20.806559974928994
Epoch: 8549, Batch Gradient Norm after: 20.406633596943433
Epoch 8550/10000, Prediction Accuracy = 60.162%, Loss = 0.6791802406311035
Epoch: 8550, Batch Gradient Norm: 22.00758683105444
Epoch: 8550, Batch Gradient Norm after: 21.54331767533941
Epoch 8551/10000, Prediction Accuracy = 60.17%, Loss = 0.6825698614120483
Epoch: 8551, Batch Gradient Norm: 20.744629395756185
Epoch: 8551, Batch Gradient Norm after: 20.321778933703442
Epoch 8552/10000, Prediction Accuracy = 60.266%, Loss = 0.6790156960487366
Epoch: 8552, Batch Gradient Norm: 21.979275779551642
Epoch: 8552, Batch Gradient Norm after: 21.54634845074374
Epoch 8553/10000, Prediction Accuracy = 60.176%, Loss = 0.6824816107749939
Epoch: 8553, Batch Gradient Norm: 20.667301855096746
Epoch: 8553, Batch Gradient Norm after: 20.21977440763941
Epoch 8554/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6787230849266053
Epoch: 8554, Batch Gradient Norm: 21.962273079497706
Epoch: 8554, Batch Gradient Norm after: 21.522067132948052
Epoch 8555/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.6823161602020263
Epoch: 8555, Batch Gradient Norm: 20.803115268622278
Epoch: 8555, Batch Gradient Norm after: 20.395492625385863
Epoch 8556/10000, Prediction Accuracy = 60.186%, Loss = 0.679052734375
Epoch: 8556, Batch Gradient Norm: 22.004878891287834
Epoch: 8556, Batch Gradient Norm after: 21.54261640164314
Epoch 8557/10000, Prediction Accuracy = 60.2%, Loss = 0.6824637055397034
Epoch: 8557, Batch Gradient Norm: 20.743460447253018
Epoch: 8557, Batch Gradient Norm after: 20.321463047571353
Epoch 8558/10000, Prediction Accuracy = 60.148%, Loss = 0.678920567035675
Epoch: 8558, Batch Gradient Norm: 21.976863946913085
Epoch: 8558, Batch Gradient Norm after: 21.545524403372337
Epoch 8559/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6823391795158387
Epoch: 8559, Batch Gradient Norm: 20.667775036477074
Epoch: 8559, Batch Gradient Norm after: 20.223686953992193
Epoch 8560/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6786086201667786
Epoch: 8560, Batch Gradient Norm: 21.958503418331166
Epoch: 8560, Batch Gradient Norm after: 21.52202963925743
Epoch 8561/10000, Prediction Accuracy = 60.162%, Loss = 0.6821831226348877
Epoch: 8561, Batch Gradient Norm: 20.788471043124936
Epoch: 8561, Batch Gradient Norm after: 20.385842521100013
Epoch 8562/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6788923978805542
Epoch: 8562, Batch Gradient Norm: 21.989292418409452
Epoch: 8562, Batch Gradient Norm after: 21.541459722542232
Epoch 8563/10000, Prediction Accuracy = 60.182%, Loss = 0.682287847995758
Epoch: 8563, Batch Gradient Norm: 20.705293868731975
Epoch: 8563, Batch Gradient Norm after: 20.277465374010394
Epoch 8564/10000, Prediction Accuracy = 60.25%, Loss = 0.6786396145820618
Epoch: 8564, Batch Gradient Norm: 21.959323823649026
Epoch: 8564, Batch Gradient Norm after: 21.531008576831073
Epoch 8565/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6821264624595642
Epoch: 8565, Batch Gradient Norm: 20.735363588306797
Epoch: 8565, Batch Gradient Norm after: 20.317347060952905
Epoch 8566/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6786276459693908
Epoch: 8566, Batch Gradient Norm: 21.976231910448597
Epoch: 8566, Batch Gradient Norm after: 21.545359728857996
Epoch 8567/10000, Prediction Accuracy = 60.174%, Loss = 0.6821027994155884
Epoch: 8567, Batch Gradient Norm: 20.675067026877716
Epoch: 8567, Batch Gradient Norm after: 20.228661069968798
Epoch 8568/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.6785009145736695
Epoch: 8568, Batch Gradient Norm: 21.957749564291042
Epoch: 8568, Batch Gradient Norm after: 21.52587951872604
Epoch 8569/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6821192145347595
Epoch: 8569, Batch Gradient Norm: 20.74922384328537
Epoch: 8569, Batch Gradient Norm after: 20.340198914712804
Epoch 8570/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.6786642074584961
Epoch: 8570, Batch Gradient Norm: 21.97268015897433
Epoch: 8570, Batch Gradient Norm after: 21.549006029765696
Epoch 8571/10000, Prediction Accuracy = 60.194%, Loss = 0.6820193290710449
Epoch: 8571, Batch Gradient Norm: 20.653354091037038
Epoch: 8571, Batch Gradient Norm after: 20.206278045139538
Epoch 8572/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6782839298248291
Epoch: 8572, Batch Gradient Norm: 21.945819878577062
Epoch: 8572, Batch Gradient Norm after: 21.514979749758968
Epoch 8573/10000, Prediction Accuracy = 60.174%, Loss = 0.6819108009338379
Epoch: 8573, Batch Gradient Norm: 20.786382371496902
Epoch: 8573, Batch Gradient Norm after: 20.392213394973414
Epoch 8574/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6786447882652282
Epoch: 8574, Batch Gradient Norm: 21.977679972960757
Epoch: 8574, Batch Gradient Norm after: 21.543127368351733
Epoch 8575/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6820016384124756
Epoch: 8575, Batch Gradient Norm: 20.686543477484047
Epoch: 8575, Batch Gradient Norm after: 20.262319891017405
Epoch 8576/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6782903909683228
Epoch: 8576, Batch Gradient Norm: 21.950705739000476
Epoch: 8576, Batch Gradient Norm after: 21.52685660690544
Epoch 8577/10000, Prediction Accuracy = 60.152%, Loss = 0.6818034410476684
Epoch: 8577, Batch Gradient Norm: 20.744956110313783
Epoch: 8577, Batch Gradient Norm after: 20.33692172472963
Epoch 8578/10000, Prediction Accuracy = 60.19799999999999%, Loss = 0.6784037351608276
Epoch: 8578, Batch Gradient Norm: 21.9699047904923
Epoch: 8578, Batch Gradient Norm after: 21.547959964056055
Epoch 8579/10000, Prediction Accuracy = 60.19599999999999%, Loss = 0.6818671226501465
Epoch: 8579, Batch Gradient Norm: 20.626653905501826
Epoch: 8579, Batch Gradient Norm after: 20.178677140790064
Epoch 8580/10000, Prediction Accuracy = 60.152%, Loss = 0.6781229496002197
Epoch: 8580, Batch Gradient Norm: 21.93031319967376
Epoch: 8580, Batch Gradient Norm after: 21.5066557991026
Epoch 8581/10000, Prediction Accuracy = 60.21%, Loss = 0.6817461609840393
Epoch: 8581, Batch Gradient Norm: 20.82356732418203
Epoch: 8581, Batch Gradient Norm after: 20.4562404806029
Epoch 8582/10000, Prediction Accuracy = 60.186%, Loss = 0.6785415530204773
Epoch: 8582, Batch Gradient Norm: 21.97963788442032
Epoch: 8582, Batch Gradient Norm after: 21.533534659092563
Epoch 8583/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.6817443609237671
Epoch: 8583, Batch Gradient Norm: 20.718273784308856
Epoch: 8583, Batch Gradient Norm after: 20.319363526676074
Epoch 8584/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6782116174697876
Epoch: 8584, Batch Gradient Norm: 21.940374350673743
Epoch: 8584, Batch Gradient Norm after: 21.530154770770945
Epoch 8585/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6816745877265931
Epoch: 8585, Batch Gradient Norm: 20.6614422490892
Epoch: 8585, Batch Gradient Norm after: 20.244742889814948
Epoch 8586/10000, Prediction Accuracy = 60.262%, Loss = 0.678028655052185
Epoch: 8586, Batch Gradient Norm: 21.921644716248416
Epoch: 8586, Batch Gradient Norm after: 21.510532976262592
Epoch 8587/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6815313816070556
Epoch: 8587, Batch Gradient Norm: 20.75732841489025
Epoch: 8587, Batch Gradient Norm after: 20.37580324047404
Epoch 8588/10000, Prediction Accuracy = 60.226%, Loss = 0.6781931757926941
Epoch: 8588, Batch Gradient Norm: 21.95228278022551
Epoch: 8588, Batch Gradient Norm after: 21.544678525279537
Epoch 8589/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6815614581108094
Epoch: 8589, Batch Gradient Norm: 20.61812457790425
Epoch: 8589, Batch Gradient Norm after: 20.179553698896644
Epoch 8590/10000, Prediction Accuracy = 60.182%, Loss = 0.6778436541557312
Epoch: 8590, Batch Gradient Norm: 21.913273899631424
Epoch: 8590, Batch Gradient Norm after: 21.498825022536256
Epoch 8591/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.6815023064613343
Epoch: 8591, Batch Gradient Norm: 20.814998246711287
Epoch: 8591, Batch Gradient Norm after: 20.458788703369088
Epoch 8592/10000, Prediction Accuracy = 60.148%, Loss = 0.6783361196517944
Epoch: 8592, Batch Gradient Norm: 21.96007169216388
Epoch: 8592, Batch Gradient Norm after: 21.543182024242466
Epoch 8593/10000, Prediction Accuracy = 60.2%, Loss = 0.6814980745315552
Epoch: 8593, Batch Gradient Norm: 20.633205543612466
Epoch: 8593, Batch Gradient Norm after: 20.221290528446175
Epoch 8594/10000, Prediction Accuracy = 60.176%, Loss = 0.6777397394180298
Epoch: 8594, Batch Gradient Norm: 21.904874377145006
Epoch: 8594, Batch Gradient Norm after: 21.498628233540234
Epoch 8595/10000, Prediction Accuracy = 60.18000000000001%, Loss = 0.681309700012207
Epoch: 8595, Batch Gradient Norm: 20.768785306173353
Epoch: 8595, Batch Gradient Norm after: 20.407035796777127
Epoch 8596/10000, Prediction Accuracy = 60.274%, Loss = 0.6781061172485352
Epoch: 8596, Batch Gradient Norm: 21.93190551403609
Epoch: 8596, Batch Gradient Norm after: 21.540317505201116
Epoch 8597/10000, Prediction Accuracy = 60.178%, Loss = 0.6813890337944031
Epoch: 8597, Batch Gradient Norm: 20.568791717473253
Epoch: 8597, Batch Gradient Norm after: 20.133885482942198
Epoch 8598/10000, Prediction Accuracy = 60.258%, Loss = 0.6774739742279052
Epoch: 8598, Batch Gradient Norm: 21.88142342443534
Epoch: 8598, Batch Gradient Norm after: 21.476116345449434
Epoch 8599/10000, Prediction Accuracy = 60.148%, Loss = 0.681137228012085
Epoch: 8599, Batch Gradient Norm: 20.888686966659844
Epoch: 8599, Batch Gradient Norm after: 20.567679644042084
Epoch 8600/10000, Prediction Accuracy = 60.194%, Loss = 0.6782842159271241
Epoch: 8600, Batch Gradient Norm: 21.968921031929696
Epoch: 8600, Batch Gradient Norm after: 21.523949300506835
Epoch 8601/10000, Prediction Accuracy = 60.178%, Loss = 0.6813764095306396
Epoch: 8601, Batch Gradient Norm: 20.72750726569451
Epoch: 8601, Batch Gradient Norm after: 20.365716016412218
Epoch 8602/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.6778962850570679
Epoch: 8602, Batch Gradient Norm: 21.902025254985574
Epoch: 8602, Batch Gradient Norm after: 21.521205578770857
Epoch 8603/10000, Prediction Accuracy = 60.214%, Loss = 0.6812021374702454
Epoch: 8603, Batch Gradient Norm: 20.628188105539312
Epoch: 8603, Batch Gradient Norm after: 20.238156053680935
Epoch 8604/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6775169372558594
Epoch: 8604, Batch Gradient Norm: 21.879627654604374
Epoch: 8604, Batch Gradient Norm after: 21.48927663097519
Epoch 8605/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.6809985041618347
Epoch: 8605, Batch Gradient Norm: 20.77351386000888
Epoch: 8605, Batch Gradient Norm after: 20.434608024445964
Epoch 8606/10000, Prediction Accuracy = 60.246%, Loss = 0.6778437972068787
Epoch: 8606, Batch Gradient Norm: 21.912512806852302
Epoch: 8606, Batch Gradient Norm after: 21.535064340713966
Epoch 8607/10000, Prediction Accuracy = 60.166%, Loss = 0.6811004400253295
Epoch: 8607, Batch Gradient Norm: 20.53539119956479
Epoch: 8607, Batch Gradient Norm after: 20.109958263858754
Epoch 8608/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6771915912628174
Epoch: 8608, Batch Gradient Norm: 21.814285038376223
Epoch: 8608, Batch Gradient Norm after: 21.44048445083322
Epoch 8609/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.6807670831680298
Epoch: 8609, Batch Gradient Norm: 20.810459996296384
Epoch: 8609, Batch Gradient Norm after: 20.485809917711546
Epoch 8610/10000, Prediction Accuracy = 60.222%, Loss = 0.6778374671936035
Epoch: 8610, Batch Gradient Norm: 21.92568191480127
Epoch: 8610, Batch Gradient Norm after: 21.531126913274573
Epoch 8611/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6809913277626037
Epoch: 8611, Batch Gradient Norm: 20.594388409967994
Epoch: 8611, Batch Gradient Norm after: 20.185648416629693
Epoch 8612/10000, Prediction Accuracy = 60.186%, Loss = 0.6772704362869263
Epoch: 8612, Batch Gradient Norm: 21.86915394451592
Epoch: 8612, Batch Gradient Norm after: 21.47987889664448
Epoch 8613/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6808935403823853
Epoch: 8613, Batch Gradient Norm: 20.81334061830488
Epoch: 8613, Batch Gradient Norm after: 20.49689216474111
Epoch 8614/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.677839195728302
Epoch: 8614, Batch Gradient Norm: 21.914476509722885
Epoch: 8614, Batch Gradient Norm after: 21.53650203900853
Epoch 8615/10000, Prediction Accuracy = 60.222%, Loss = 0.6809106469154358
Epoch: 8615, Batch Gradient Norm: 20.543539943571425
Epoch: 8615, Batch Gradient Norm after: 20.13156135278157
Epoch 8616/10000, Prediction Accuracy = 60.194%, Loss = 0.6769930362701416
Epoch: 8616, Batch Gradient Norm: 21.818253052884057
Epoch: 8616, Batch Gradient Norm after: 21.44594381116288
Epoch 8617/10000, Prediction Accuracy = 60.194%, Loss = 0.6805788755416871
Epoch: 8617, Batch Gradient Norm: 20.790327708941625
Epoch: 8617, Batch Gradient Norm after: 20.4698888399835
Epoch 8618/10000, Prediction Accuracy = 60.258%, Loss = 0.6776587128639221
Epoch: 8618, Batch Gradient Norm: 21.903886092686875
Epoch: 8618, Batch Gradient Norm after: 21.53168435260841
Epoch 8619/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6808342695236206
Epoch: 8619, Batch Gradient Norm: 20.53522019806287
Epoch: 8619, Batch Gradient Norm after: 20.124544916994644
Epoch 8620/10000, Prediction Accuracy = 60.246%, Loss = 0.6769007205963135
Epoch: 8620, Batch Gradient Norm: 21.80268596601222
Epoch: 8620, Batch Gradient Norm after: 21.438536159623055
Epoch 8621/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.680432665348053
Epoch: 8621, Batch Gradient Norm: 20.783928054156846
Epoch: 8621, Batch Gradient Norm after: 20.457391133836186
Epoch 8622/10000, Prediction Accuracy = 60.226%, Loss = 0.6775034308433533
Epoch: 8622, Batch Gradient Norm: 21.910406085436115
Epoch: 8622, Batch Gradient Norm after: 21.537258714231424
Epoch 8623/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.680727231502533
Epoch: 8623, Batch Gradient Norm: 20.533221319841907
Epoch: 8623, Batch Gradient Norm after: 20.11311849425803
Epoch 8624/10000, Prediction Accuracy = 60.18399999999999%, Loss = 0.6768734455108643
Epoch: 8624, Batch Gradient Norm: 21.80040817025665
Epoch: 8624, Batch Gradient Norm after: 21.439062791748885
Epoch 8625/10000, Prediction Accuracy = 60.21%, Loss = 0.6804409980773926
Epoch: 8625, Batch Gradient Norm: 20.76629499815441
Epoch: 8625, Batch Gradient Norm after: 20.441413674874177
Epoch 8626/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6774065256118774
Epoch: 8626, Batch Gradient Norm: 21.89471846240292
Epoch: 8626, Batch Gradient Norm after: 21.529641937000456
Epoch 8627/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6805606365203858
Epoch: 8627, Batch Gradient Norm: 20.534666757317737
Epoch: 8627, Batch Gradient Norm after: 20.119407595147873
Epoch 8628/10000, Prediction Accuracy = 60.23%, Loss = 0.6767108678817749
Epoch: 8628, Batch Gradient Norm: 21.809265894329783
Epoch: 8628, Batch Gradient Norm after: 21.442870855264562
Epoch 8629/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6803342342376709
Epoch: 8629, Batch Gradient Norm: 20.783548485899132
Epoch: 8629, Batch Gradient Norm after: 20.46442107789196
Epoch 8630/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6773799657821655
Epoch: 8630, Batch Gradient Norm: 21.893094807260876
Epoch: 8630, Batch Gradient Norm after: 21.53082694062768
Epoch 8631/10000, Prediction Accuracy = 60.148%, Loss = 0.6805163979530334
Epoch: 8631, Batch Gradient Norm: 20.52194237511225
Epoch: 8631, Batch Gradient Norm after: 20.108221731613593
Epoch 8632/10000, Prediction Accuracy = 60.23%, Loss = 0.6765695810317993
Epoch: 8632, Batch Gradient Norm: 21.78119798519663
Epoch: 8632, Batch Gradient Norm after: 21.4268786584405
Epoch 8633/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.6801171064376831
Epoch: 8633, Batch Gradient Norm: 20.762350150518856
Epoch: 8633, Batch Gradient Norm after: 20.42853726306667
Epoch 8634/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6772153496742248
Epoch: 8634, Batch Gradient Norm: 21.89368210178184
Epoch: 8634, Batch Gradient Norm after: 21.530557073247678
Epoch 8635/10000, Prediction Accuracy = 60.198%, Loss = 0.6804691195487976
Epoch: 8635, Batch Gradient Norm: 20.51477461432304
Epoch: 8635, Batch Gradient Norm after: 20.097294668283276
Epoch 8636/10000, Prediction Accuracy = 60.188%, Loss = 0.6765490651130677
Epoch: 8636, Batch Gradient Norm: 21.76551643724355
Epoch: 8636, Batch Gradient Norm after: 21.420367815506246
Epoch 8637/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.68002849817276
Epoch: 8637, Batch Gradient Norm: 20.728086638965333
Epoch: 8637, Batch Gradient Norm after: 20.388914356261008
Epoch 8638/10000, Prediction Accuracy = 60.194%, Loss = 0.676996898651123
Epoch: 8638, Batch Gradient Norm: 21.883358377443706
Epoch: 8638, Batch Gradient Norm after: 21.517699111800383
Epoch 8639/10000, Prediction Accuracy = 60.202%, Loss = 0.6802643775939942
Epoch: 8639, Batch Gradient Norm: 20.57047808834823
Epoch: 8639, Batch Gradient Norm after: 20.172212474067976
Epoch 8640/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6765593647956848
Epoch: 8640, Batch Gradient Norm: 21.833910247957288
Epoch: 8640, Batch Gradient Norm after: 21.463590208518163
Epoch 8641/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.680157458782196
Epoch: 8641, Batch Gradient Norm: 20.78370685368465
Epoch: 8641, Batch Gradient Norm after: 20.475445729543434
Epoch 8642/10000, Prediction Accuracy = 60.237999999999985%, Loss = 0.6770764827728272
Epoch: 8642, Batch Gradient Norm: 21.881909871661048
Epoch: 8642, Batch Gradient Norm after: 21.524692702951832
Epoch 8643/10000, Prediction Accuracy = 60.178%, Loss = 0.680168867111206
Epoch: 8643, Batch Gradient Norm: 20.514918522660565
Epoch: 8643, Batch Gradient Norm after: 20.10373924054647
Epoch 8644/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.676285982131958
Epoch: 8644, Batch Gradient Norm: 21.759185346978487
Epoch: 8644, Batch Gradient Norm after: 21.41871778811121
Epoch 8645/10000, Prediction Accuracy = 60.198%, Loss = 0.6798364400863648
Epoch: 8645, Batch Gradient Norm: 20.70987513866757
Epoch: 8645, Batch Gradient Norm after: 20.368672618049704
Epoch 8646/10000, Prediction Accuracy = 60.194%, Loss = 0.6768516540527344
Epoch: 8646, Batch Gradient Norm: 21.863679265234744
Epoch: 8646, Batch Gradient Norm after: 21.508802305585785
Epoch 8647/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6801315188407898
Epoch: 8647, Batch Gradient Norm: 20.574826503682544
Epoch: 8647, Batch Gradient Norm after: 20.19504523993005
Epoch 8648/10000, Prediction Accuracy = 60.186%, Loss = 0.6764117121696472
Epoch: 8648, Batch Gradient Norm: 21.821352427582138
Epoch: 8648, Batch Gradient Norm after: 21.460865925546297
Epoch 8649/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6798786997795105
Epoch: 8649, Batch Gradient Norm: 20.75297904200149
Epoch: 8649, Batch Gradient Norm after: 20.442330232430095
Epoch 8650/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6768038153648377
Epoch: 8650, Batch Gradient Norm: 21.869991402239695
Epoch: 8650, Batch Gradient Norm after: 21.52053173206723
Epoch 8651/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.6800062894821167
Epoch: 8651, Batch Gradient Norm: 20.495882773816827
Epoch: 8651, Batch Gradient Norm after: 20.094075956005685
Epoch 8652/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.676120936870575
Epoch: 8652, Batch Gradient Norm: 21.699833480908296
Epoch: 8652, Batch Gradient Norm after: 21.389176793817647
Epoch 8653/10000, Prediction Accuracy = 60.138%, Loss = 0.6795023560523987
Epoch: 8653, Batch Gradient Norm: 20.642509367006344
Epoch: 8653, Batch Gradient Norm after: 20.282730366738516
Epoch 8654/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6764052867889404
Epoch: 8654, Batch Gradient Norm: 21.843280862698986
Epoch: 8654, Batch Gradient Norm after: 21.485955416500012
Epoch 8655/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6797884821891784
Epoch: 8655, Batch Gradient Norm: 20.652696526908397
Epoch: 8655, Batch Gradient Norm after: 20.295931181384873
Epoch 8656/10000, Prediction Accuracy = 60.214%, Loss = 0.6764304399490356
Epoch: 8656, Batch Gradient Norm: 21.84119280337139
Epoch: 8656, Batch Gradient Norm after: 21.48880739926432
Epoch 8657/10000, Prediction Accuracy = 60.212%, Loss = 0.6798576831817627
Epoch: 8657, Batch Gradient Norm: 20.64105962018872
Epoch: 8657, Batch Gradient Norm after: 20.291667933903213
Epoch 8658/10000, Prediction Accuracy = 60.21%, Loss = 0.6764078378677368
Epoch: 8658, Batch Gradient Norm: 21.827499186812183
Epoch: 8658, Batch Gradient Norm after: 21.480820207054666
Epoch 8659/10000, Prediction Accuracy = 60.21999999999999%, Loss = 0.6797324538230896
Epoch: 8659, Batch Gradient Norm: 20.658511743603118
Epoch: 8659, Batch Gradient Norm after: 20.32590223746413
Epoch 8660/10000, Prediction Accuracy = 60.2%, Loss = 0.6763273477554321
Epoch: 8660, Batch Gradient Norm: 21.83062089203576
Epoch: 8660, Batch Gradient Norm after: 21.4849173894652
Epoch 8661/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.6796320080757141
Epoch: 8661, Batch Gradient Norm: 20.62518343154116
Epoch: 8661, Batch Gradient Norm after: 20.279493986333037
Epoch 8662/10000, Prediction Accuracy = 60.25%, Loss = 0.6762222528457642
Epoch: 8662, Batch Gradient Norm: 21.817929192205856
Epoch: 8662, Batch Gradient Norm after: 21.474736989611863
Epoch 8663/10000, Prediction Accuracy = 60.146%, Loss = 0.6796307444572449
Epoch: 8663, Batch Gradient Norm: 20.661699192941327
Epoch: 8663, Batch Gradient Norm after: 20.343168213064786
Epoch 8664/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6762734055519104
Epoch: 8664, Batch Gradient Norm: 21.821637054126334
Epoch: 8664, Batch Gradient Norm after: 21.484646125313983
Epoch 8665/10000, Prediction Accuracy = 60.188%, Loss = 0.6795333862304688
Epoch: 8665, Batch Gradient Norm: 20.633984003313213
Epoch: 8665, Batch Gradient Norm after: 20.3034039817374
Epoch 8666/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6761148571968079
Epoch: 8666, Batch Gradient Norm: 21.810697768379292
Epoch: 8666, Batch Gradient Norm after: 21.474320603190698
Epoch 8667/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.6794853925704956
Epoch: 8667, Batch Gradient Norm: 20.648949594938603
Epoch: 8667, Batch Gradient Norm after: 20.327772220021817
Epoch 8668/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.6762010812759399
Epoch: 8668, Batch Gradient Norm: 21.806820159477624
Epoch: 8668, Batch Gradient Norm after: 21.477325587258733
Epoch 8669/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.6794981241226197
Epoch: 8669, Batch Gradient Norm: 20.613124822667075
Epoch: 8669, Batch Gradient Norm after: 20.293270723722323
Epoch 8670/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6760299444198609
Epoch: 8670, Batch Gradient Norm: 21.77011591411412
Epoch: 8670, Batch Gradient Norm after: 21.451918719261975
Epoch 8671/10000, Prediction Accuracy = 60.238%, Loss = 0.6792575359344483
Epoch: 8671, Batch Gradient Norm: 20.61732332230455
Epoch: 8671, Batch Gradient Norm after: 20.295072748607453
Epoch 8672/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6759473443031311
Epoch: 8672, Batch Gradient Norm: 21.78469173348951
Epoch: 8672, Batch Gradient Norm after: 21.461112114207605
Epoch 8673/10000, Prediction Accuracy = 60.176%, Loss = 0.6792803406715393
Epoch: 8673, Batch Gradient Norm: 20.61915613362303
Epoch: 8673, Batch Gradient Norm after: 20.303181230091532
Epoch 8674/10000, Prediction Accuracy = 60.262%, Loss = 0.6759573221206665
Epoch: 8674, Batch Gradient Norm: 21.770259613547623
Epoch: 8674, Batch Gradient Norm after: 21.455729737269376
Epoch 8675/10000, Prediction Accuracy = 60.15%, Loss = 0.679210078716278
Epoch: 8675, Batch Gradient Norm: 20.59567936263476
Epoch: 8675, Batch Gradient Norm after: 20.274239880208615
Epoch 8676/10000, Prediction Accuracy = 60.234%, Loss = 0.6757911920547486
Epoch: 8676, Batch Gradient Norm: 21.751528264298617
Epoch: 8676, Batch Gradient Norm after: 21.441340957645046
Epoch 8677/10000, Prediction Accuracy = 60.214%, Loss = 0.679058039188385
Epoch: 8677, Batch Gradient Norm: 20.590254767403074
Epoch: 8677, Batch Gradient Norm after: 20.260216056814656
Epoch 8678/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.6757720947265625
Epoch: 8678, Batch Gradient Norm: 21.751991444255257
Epoch: 8678, Batch Gradient Norm after: 21.441952952960637
Epoch 8679/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.6791028022766114
Epoch: 8679, Batch Gradient Norm: 20.581443051529675
Epoch: 8679, Batch Gradient Norm after: 20.250424159714928
Epoch 8680/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.6757557153701782
Epoch: 8680, Batch Gradient Norm: 21.72442884699379
Epoch: 8680, Batch Gradient Norm after: 21.426643415724776
Epoch 8681/10000, Prediction Accuracy = 60.206%, Loss = 0.6789598345756531
Epoch: 8681, Batch Gradient Norm: 20.559387495929318
Epoch: 8681, Batch Gradient Norm after: 20.21989291610399
Epoch 8682/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.6755800366401672
Epoch: 8682, Batch Gradient Norm: 21.726995868601644
Epoch: 8682, Batch Gradient Norm after: 21.423304766367618
Epoch 8683/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6788538932800293
Epoch: 8683, Batch Gradient Norm: 20.57679556808582
Epoch: 8683, Batch Gradient Norm after: 20.23680060545892
Epoch 8684/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6756014108657837
Epoch: 8684, Batch Gradient Norm: 21.75485870966322
Epoch: 8684, Batch Gradient Norm after: 21.440920688303578
Epoch 8685/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.6789506793022155
Epoch: 8685, Batch Gradient Norm: 20.592301021999663
Epoch: 8685, Batch Gradient Norm after: 20.263639165568403
Epoch 8686/10000, Prediction Accuracy = 60.262%, Loss = 0.6756051778793335
Epoch: 8686, Batch Gradient Norm: 21.762018896403532
Epoch: 8686, Batch Gradient Norm after: 21.447307853563274
Epoch 8687/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6788725137710572
Epoch: 8687, Batch Gradient Norm: 20.61094919483356
Epoch: 8687, Batch Gradient Norm after: 20.28744705475907
Epoch 8688/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6755732417106628
Epoch: 8688, Batch Gradient Norm: 21.78920560485449
Epoch: 8688, Batch Gradient Norm after: 21.46400917250927
Epoch 8689/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.6789267063140869
Epoch: 8689, Batch Gradient Norm: 20.631438356607806
Epoch: 8689, Batch Gradient Norm after: 20.31993613334714
Epoch 8690/10000, Prediction Accuracy = 60.206%, Loss = 0.6756720423698426
Epoch: 8690, Batch Gradient Norm: 21.78880154894807
Epoch: 8690, Batch Gradient Norm after: 21.469984078936484
Epoch 8691/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.6789628624916076
Epoch: 8691, Batch Gradient Norm: 20.584647392814144
Epoch: 8691, Batch Gradient Norm after: 20.270364534277626
Epoch 8692/10000, Prediction Accuracy = 60.188%, Loss = 0.6754857897758484
Epoch: 8692, Batch Gradient Norm: 21.709265708328672
Epoch: 8692, Batch Gradient Norm after: 21.422506152913492
Epoch 8693/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6785976886749268
Epoch: 8693, Batch Gradient Norm: 20.526417896287995
Epoch: 8693, Batch Gradient Norm after: 20.1811237143722
Epoch 8694/10000, Prediction Accuracy = 60.21%, Loss = 0.6752260327339172
Epoch: 8694, Batch Gradient Norm: 21.67028923392347
Epoch: 8694, Batch Gradient Norm after: 21.390927085514225
Epoch 8695/10000, Prediction Accuracy = 60.176%, Loss = 0.6784630656242371
Epoch: 8695, Batch Gradient Norm: 20.51442211450484
Epoch: 8695, Batch Gradient Norm after: 20.152356012433756
Epoch 8696/10000, Prediction Accuracy = 60.262%, Loss = 0.6751943826675415
Epoch: 8696, Batch Gradient Norm: 21.667004652417354
Epoch: 8696, Batch Gradient Norm after: 21.387393927473617
Epoch 8697/10000, Prediction Accuracy = 60.15%, Loss = 0.6784258842468261
Epoch: 8697, Batch Gradient Norm: 20.5269924109655
Epoch: 8697, Batch Gradient Norm after: 20.163255899266108
Epoch 8698/10000, Prediction Accuracy = 60.214%, Loss = 0.6751337170600891
Epoch: 8698, Batch Gradient Norm: 21.7093822155503
Epoch: 8698, Batch Gradient Norm after: 21.40964477948263
Epoch 8699/10000, Prediction Accuracy = 60.226%, Loss = 0.6784362077713013
Epoch: 8699, Batch Gradient Norm: 20.59440231717435
Epoch: 8699, Batch Gradient Norm after: 20.245383063893563
Epoch 8700/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.6752945423126221
Epoch: 8700, Batch Gradient Norm: 21.795899175831398
Epoch: 8700, Batch Gradient Norm after: 21.46492721208737
Epoch 8701/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.67872474193573
Epoch: 8701, Batch Gradient Norm: 20.65255238526291
Epoch: 8701, Batch Gradient Norm after: 20.341408045727185
Epoch 8702/10000, Prediction Accuracy = 60.212%, Loss = 0.6754822969436646
Epoch: 8702, Batch Gradient Norm: 21.79679213925105
Epoch: 8702, Batch Gradient Norm after: 21.47975716163953
Epoch 8703/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6786755204200745
Epoch: 8703, Batch Gradient Norm: 20.564527777147003
Epoch: 8703, Batch Gradient Norm after: 20.237550870682806
Epoch 8704/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.6751279950141906
Epoch: 8704, Batch Gradient Norm: 21.710827209561742
Epoch: 8704, Batch Gradient Norm after: 21.419495755861764
Epoch 8705/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6783278107643127
Epoch: 8705, Batch Gradient Norm: 20.54586326249938
Epoch: 8705, Batch Gradient Norm after: 20.20470407900683
Epoch 8706/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6750455498695374
Epoch: 8706, Batch Gradient Norm: 21.700492266011047
Epoch: 8706, Batch Gradient Norm after: 21.411726961824684
Epoch 8707/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.678311562538147
Epoch: 8707, Batch Gradient Norm: 20.535061842066277
Epoch: 8707, Batch Gradient Norm after: 20.190420574300994
Epoch 8708/10000, Prediction Accuracy = 60.254%, Loss = 0.6749750018119812
Epoch: 8708, Batch Gradient Norm: 21.686226284346827
Epoch: 8708, Batch Gradient Norm after: 21.404165905364597
Epoch 8709/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6781718134880066
Epoch: 8709, Batch Gradient Norm: 20.54135710826236
Epoch: 8709, Batch Gradient Norm after: 20.187579179766402
Epoch 8710/10000, Prediction Accuracy = 60.25%, Loss = 0.6749062538146973
Epoch: 8710, Batch Gradient Norm: 21.714086345394332
Epoch: 8710, Batch Gradient Norm after: 21.41687505152414
Epoch 8711/10000, Prediction Accuracy = 60.212%, Loss = 0.6782214045524597
Epoch: 8711, Batch Gradient Norm: 20.57790681199777
Epoch: 8711, Batch Gradient Norm after: 20.23469681926808
Epoch 8712/10000, Prediction Accuracy = 60.218%, Loss = 0.6750407934188842
Epoch: 8712, Batch Gradient Norm: 21.749530076258765
Epoch: 8712, Batch Gradient Norm after: 21.44377346029083
Epoch 8713/10000, Prediction Accuracy = 60.23%, Loss = 0.6783597469329834
Epoch: 8713, Batch Gradient Norm: 20.581125705652862
Epoch: 8713, Batch Gradient Norm after: 20.25485409484205
Epoch 8714/10000, Prediction Accuracy = 60.202%, Loss = 0.6749997019767762
Epoch: 8714, Batch Gradient Norm: 21.730550997093342
Epoch: 8714, Batch Gradient Norm after: 21.434932634674848
Epoch 8715/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6781712412834168
Epoch: 8715, Batch Gradient Norm: 20.56377381405445
Epoch: 8715, Batch Gradient Norm after: 20.23244323264095
Epoch 8716/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.6748506426811218
Epoch: 8716, Batch Gradient Norm: 21.716874740635536
Epoch: 8716, Batch Gradient Norm after: 21.424091488480723
Epoch 8717/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.6781175851821899
Epoch: 8717, Batch Gradient Norm: 20.543700458260652
Epoch: 8717, Batch Gradient Norm after: 20.209225890335947
Epoch 8718/10000, Prediction Accuracy = 60.266%, Loss = 0.6748113870620728
Epoch: 8718, Batch Gradient Norm: 21.68006502575455
Epoch: 8718, Batch Gradient Norm after: 21.406395379587632
Epoch 8719/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.6779935598373413
Epoch: 8719, Batch Gradient Norm: 20.511361792545273
Epoch: 8719, Batch Gradient Norm after: 20.159252095807755
Epoch 8720/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6746118664741516
Epoch: 8720, Batch Gradient Norm: 21.663300722509266
Epoch: 8720, Batch Gradient Norm after: 21.390061410646872
Epoch 8721/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6778252363204956
Epoch: 8721, Batch Gradient Norm: 20.5276092919719
Epoch: 8721, Batch Gradient Norm after: 20.16793114153618
Epoch 8722/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.674634051322937
Epoch: 8722, Batch Gradient Norm: 21.698437355796887
Epoch: 8722, Batch Gradient Norm after: 21.41090851920218
Epoch 8723/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6779658436775208
Epoch: 8723, Batch Gradient Norm: 20.552770050626386
Epoch: 8723, Batch Gradient Norm after: 20.206392307493907
Epoch 8724/10000, Prediction Accuracy = 60.19599999999999%, Loss = 0.6747229933738709
Epoch: 8724, Batch Gradient Norm: 21.709124776066954
Epoch: 8724, Batch Gradient Norm after: 21.421695053571508
Epoch 8725/10000, Prediction Accuracy = 60.214%, Loss = 0.6779423117637634
Epoch: 8725, Batch Gradient Norm: 20.54672711143965
Epoch: 8725, Batch Gradient Norm after: 20.207928931934443
Epoch 8726/10000, Prediction Accuracy = 60.22600000000001%, Loss = 0.6745988845825195
Epoch: 8726, Batch Gradient Norm: 21.70187553588359
Epoch: 8726, Batch Gradient Norm after: 21.416885926527595
Epoch 8727/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6778193831443786
Epoch: 8727, Batch Gradient Norm: 20.54635788566438
Epoch: 8727, Batch Gradient Norm after: 20.204573207848217
Epoch 8728/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.674560296535492
Epoch: 8728, Batch Gradient Norm: 21.70394991710431
Epoch: 8728, Batch Gradient Norm after: 21.417476848150415
Epoch 8729/10000, Prediction Accuracy = 60.176%, Loss = 0.6778347969055176
Epoch: 8729, Batch Gradient Norm: 20.545325461356825
Epoch: 8729, Batch Gradient Norm after: 20.208292889937766
Epoch 8730/10000, Prediction Accuracy = 60.246%, Loss = 0.6745304942131043
Epoch: 8730, Batch Gradient Norm: 21.68256305096108
Epoch: 8730, Batch Gradient Norm after: 21.408304416940425
Epoch 8731/10000, Prediction Accuracy = 60.186%, Loss = 0.6776990413665771
Epoch: 8731, Batch Gradient Norm: 20.522270238715112
Epoch: 8731, Batch Gradient Norm after: 20.172809891967532
Epoch 8732/10000, Prediction Accuracy = 60.254%, Loss = 0.674374270439148
Epoch: 8732, Batch Gradient Norm: 21.671346787829382
Epoch: 8732, Batch Gradient Norm after: 21.397511370442906
Epoch 8733/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6776198148727417
Epoch: 8733, Batch Gradient Norm: 20.520534805966708
Epoch: 8733, Batch Gradient Norm after: 20.164944219470627
Epoch 8734/10000, Prediction Accuracy = 60.222%, Loss = 0.6744072675704956
Epoch: 8734, Batch Gradient Norm: 21.67772646774644
Epoch: 8734, Batch Gradient Norm after: 21.402986777456174
Epoch 8735/10000, Prediction Accuracy = 60.222%, Loss = 0.6776910185813904
Epoch: 8735, Batch Gradient Norm: 20.520439438199222
Epoch: 8735, Batch Gradient Norm after: 20.172926702160833
Epoch 8736/10000, Prediction Accuracy = 60.186%, Loss = 0.6743661761283875
Epoch: 8736, Batch Gradient Norm: 21.664605309232844
Epoch: 8736, Batch Gradient Norm after: 21.396026896225056
Epoch 8737/10000, Prediction Accuracy = 60.266%, Loss = 0.6775118470191955
Epoch: 8737, Batch Gradient Norm: 20.510852818993175
Epoch: 8737, Batch Gradient Norm after: 20.15631732344902
Epoch 8738/10000, Prediction Accuracy = 60.21199999999999%, Loss = 0.6742260575294494
Epoch: 8738, Batch Gradient Norm: 21.66889344432659
Epoch: 8738, Batch Gradient Norm after: 21.395506599732336
Epoch 8739/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6775036573410034
Epoch: 8739, Batch Gradient Norm: 20.52865508577415
Epoch: 8739, Batch Gradient Norm after: 20.178999442328948
Epoch 8740/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.6742881417274476
Epoch: 8740, Batch Gradient Norm: 21.680446021993443
Epoch: 8740, Batch Gradient Norm after: 21.40816334305498
Epoch 8741/10000, Prediction Accuracy = 60.182%, Loss = 0.6775166988372803
Epoch: 8741, Batch Gradient Norm: 20.51786264395641
Epoch: 8741, Batch Gradient Norm after: 20.169490944261003
Epoch 8742/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.67415691614151
Epoch: 8742, Batch Gradient Norm: 21.67221698648195
Epoch: 8742, Batch Gradient Norm after: 21.40002272205726
Epoch 8743/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6773787617683411
Epoch: 8743, Batch Gradient Norm: 20.534731413257955
Epoch: 8743, Batch Gradient Norm after: 20.18483928674916
Epoch 8744/10000, Prediction Accuracy = 60.226%, Loss = 0.674174177646637
Epoch: 8744, Batch Gradient Norm: 21.699836034021914
Epoch: 8744, Batch Gradient Norm after: 21.417197888852478
Epoch 8745/10000, Prediction Accuracy = 60.202%, Loss = 0.6774856090545655
Epoch: 8745, Batch Gradient Norm: 20.53984391290515
Epoch: 8745, Batch Gradient Norm after: 20.201876348270186
Epoch 8746/10000, Prediction Accuracy = 60.206%, Loss = 0.6742142558097839
Epoch: 8746, Batch Gradient Norm: 21.68003442288016
Epoch: 8746, Batch Gradient Norm after: 21.411376432244044
Epoch 8747/10000, Prediction Accuracy = 60.212%, Loss = 0.6773837327957153
Epoch: 8747, Batch Gradient Norm: 20.50248878152713
Epoch: 8747, Batch Gradient Norm after: 20.15845423355834
Epoch 8748/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6740027785301208
Epoch: 8748, Batch Gradient Norm: 21.63405091476452
Epoch: 8748, Batch Gradient Norm after: 21.380412122245446
Epoch 8749/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6771466135978699
Epoch: 8749, Batch Gradient Norm: 20.479794141155082
Epoch: 8749, Batch Gradient Norm after: 20.116728238913744
Epoch 8750/10000, Prediction Accuracy = 60.23%, Loss = 0.6738983869552613
Epoch: 8750, Batch Gradient Norm: 21.617430067431354
Epoch: 8750, Batch Gradient Norm after: 21.369286472613176
Epoch 8751/10000, Prediction Accuracy = 60.186%, Loss = 0.6771312117576599
Epoch: 8751, Batch Gradient Norm: 20.473404079765082
Epoch: 8751, Batch Gradient Norm after: 20.104995841563092
Epoch 8752/10000, Prediction Accuracy = 60.254%, Loss = 0.6738555192947387
Epoch: 8752, Batch Gradient Norm: 21.620433534580116
Epoch: 8752, Batch Gradient Norm after: 21.369044632644375
Epoch 8753/10000, Prediction Accuracy = 60.202%, Loss = 0.6770489931106567
Epoch: 8753, Batch Gradient Norm: 20.494327922207802
Epoch: 8753, Batch Gradient Norm after: 20.12481458421191
Epoch 8754/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6738099932670594
Epoch: 8754, Batch Gradient Norm: 21.66714145460385
Epoch: 8754, Batch Gradient Norm after: 21.394424927350595
Epoch 8755/10000, Prediction Accuracy = 60.214%, Loss = 0.6771157503128051
Epoch: 8755, Batch Gradient Norm: 20.544849539245682
Epoch: 8755, Batch Gradient Norm after: 20.193488682904174
Epoch 8756/10000, Prediction Accuracy = 60.21%, Loss = 0.673984169960022
Epoch: 8756, Batch Gradient Norm: 21.707858111087305
Epoch: 8756, Batch Gradient Norm after: 21.42690356262477
Epoch 8757/10000, Prediction Accuracy = 60.21999999999999%, Loss = 0.6773149251937867
Epoch: 8757, Batch Gradient Norm: 20.540512656121365
Epoch: 8757, Batch Gradient Norm after: 20.210843703399703
Epoch 8758/10000, Prediction Accuracy = 60.186%, Loss = 0.6739690423011779
Epoch: 8758, Batch Gradient Norm: 21.67218590199855
Epoch: 8758, Batch Gradient Norm after: 21.409428450108084
Epoch 8759/10000, Prediction Accuracy = 60.242%, Loss = 0.6770776748657227
Epoch: 8759, Batch Gradient Norm: 20.504955413915955
Epoch: 8759, Batch Gradient Norm after: 20.16183045315697
Epoch 8760/10000, Prediction Accuracy = 60.21%, Loss = 0.673738420009613
Epoch: 8760, Batch Gradient Norm: 21.632037612784384
Epoch: 8760, Batch Gradient Norm after: 21.38372071880354
Epoch 8761/10000, Prediction Accuracy = 60.226%, Loss = 0.6769292712211609
Epoch: 8761, Batch Gradient Norm: 20.46432783146759
Epoch: 8761, Batch Gradient Norm after: 20.10365992528419
Epoch 8762/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6736414551734924
Epoch: 8762, Batch Gradient Norm: 21.5879986115499
Epoch: 8762, Batch Gradient Norm after: 21.355298931673495
Epoch 8763/10000, Prediction Accuracy = 60.19%, Loss = 0.6768072962760925
Epoch: 8763, Batch Gradient Norm: 20.43990489035391
Epoch: 8763, Batch Gradient Norm after: 20.063215152261492
Epoch 8764/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.6734771013259888
Epoch: 8764, Batch Gradient Norm: 21.575157428945566
Epoch: 8764, Batch Gradient Norm after: 21.343178742638504
Epoch 8765/10000, Prediction Accuracy = 60.23%, Loss = 0.6766368865966796
Epoch: 8765, Batch Gradient Norm: 20.459228263976193
Epoch: 8765, Batch Gradient Norm after: 20.070492543958846
Epoch 8766/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6734747648239136
Epoch: 8766, Batch Gradient Norm: 21.623156416789183
Epoch: 8766, Batch Gradient Norm after: 21.368888474495698
Epoch 8767/10000, Prediction Accuracy = 60.212%, Loss = 0.6767877221107483
Epoch: 8767, Batch Gradient Norm: 20.51056152999005
Epoch: 8767, Batch Gradient Norm after: 20.143686261750364
Epoch 8768/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.6736512303352356
Epoch: 8768, Batch Gradient Norm: 21.674244371046104
Epoch: 8768, Batch Gradient Norm after: 21.404121386611436
Epoch 8769/10000, Prediction Accuracy = 60.214%, Loss = 0.6769061923027039
Epoch: 8769, Batch Gradient Norm: 20.53449775986312
Epoch: 8769, Batch Gradient Norm after: 20.194148621090978
Epoch 8770/10000, Prediction Accuracy = 60.246%, Loss = 0.673628032207489
Epoch: 8770, Batch Gradient Norm: 21.68153384339994
Epoch: 8770, Batch Gradient Norm after: 21.41423341559216
Epoch 8771/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6768119812011719
Epoch: 8771, Batch Gradient Norm: 20.540595671567004
Epoch: 8771, Batch Gradient Norm after: 20.20222634247141
Epoch 8772/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.6735745668411255
Epoch: 8772, Batch Gradient Norm: 21.681528689790788
Epoch: 8772, Batch Gradient Norm after: 21.418157348820834
Epoch 8773/10000, Prediction Accuracy = 60.198%, Loss = 0.67682945728302
Epoch: 8773, Batch Gradient Norm: 20.507942841208965
Epoch: 8773, Batch Gradient Norm after: 20.17171384906588
Epoch 8774/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6734791278839112
Epoch: 8774, Batch Gradient Norm: 21.61483100370137
Epoch: 8774, Batch Gradient Norm after: 21.379232207194
Epoch 8775/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.6765813708305359
Epoch: 8775, Batch Gradient Norm: 20.450333284799186
Epoch: 8775, Batch Gradient Norm after: 20.087214124851634
Epoch 8776/10000, Prediction Accuracy = 60.262%, Loss = 0.6732220888137818
Epoch: 8776, Batch Gradient Norm: 21.578548431545485
Epoch: 8776, Batch Gradient Norm after: 21.348542515408127
Epoch 8777/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6763736367225647
Epoch: 8777, Batch Gradient Norm: 20.44502471703893
Epoch: 8777, Batch Gradient Norm after: 20.067334936692305
Epoch 8778/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6732225537300109
Epoch: 8778, Batch Gradient Norm: 21.57973753458757
Epoch: 8778, Batch Gradient Norm after: 21.349590005248956
Epoch 8779/10000, Prediction Accuracy = 60.212%, Loss = 0.6764425992965698
Epoch: 8779, Batch Gradient Norm: 20.43895224894139
Epoch: 8779, Batch Gradient Norm after: 20.059627157476136
Epoch 8780/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6732105016708374
Epoch: 8780, Batch Gradient Norm: 21.572284702444996
Epoch: 8780, Batch Gradient Norm after: 21.345051008108772
Epoch 8781/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6763300180435181
Epoch: 8781, Batch Gradient Norm: 20.442108870179307
Epoch: 8781, Batch Gradient Norm after: 20.060527670251854
Epoch 8782/10000, Prediction Accuracy = 60.254%, Loss = 0.6730890035629272
Epoch: 8782, Batch Gradient Norm: 21.605442415443218
Epoch: 8782, Batch Gradient Norm after: 21.362135378838612
Epoch 8783/10000, Prediction Accuracy = 60.246%, Loss = 0.6763507127761841
Epoch: 8783, Batch Gradient Norm: 20.499340128003563
Epoch: 8783, Batch Gradient Norm after: 20.133997038291085
Epoch 8784/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6732306599617004
Epoch: 8784, Batch Gradient Norm: 21.66470173194239
Epoch: 8784, Batch Gradient Norm after: 21.403172734646752
Epoch 8785/10000, Prediction Accuracy = 60.202%, Loss = 0.6765381455421448
Epoch: 8785, Batch Gradient Norm: 20.53984727411924
Epoch: 8785, Batch Gradient Norm after: 20.201669881255977
Epoch 8786/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6732760667800903
Epoch: 8786, Batch Gradient Norm: 21.693639396852877
Epoch: 8786, Batch Gradient Norm after: 21.425456436222664
Epoch 8787/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.676488995552063
Epoch: 8787, Batch Gradient Norm: 20.538694468801573
Epoch: 8787, Batch Gradient Norm after: 20.211633201191848
Epoch 8788/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6732110261917115
Epoch: 8788, Batch Gradient Norm: 21.66696241659444
Epoch: 8788, Batch Gradient Norm after: 21.413981473317506
Epoch 8789/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6764269232749939
Epoch: 8789, Batch Gradient Norm: 20.488539446958537
Epoch: 8789, Batch Gradient Norm after: 20.149743406868453
Epoch 8790/10000, Prediction Accuracy = 60.21%, Loss = 0.6731319427490234
Epoch: 8790, Batch Gradient Norm: 21.573298261563515
Epoch: 8790, Batch Gradient Norm after: 21.35739405552619
Epoch 8791/10000, Prediction Accuracy = 60.222%, Loss = 0.6761810779571533
Epoch: 8791, Batch Gradient Norm: 20.395902592260498
Epoch: 8791, Batch Gradient Norm after: 20.026738963934655
Epoch 8792/10000, Prediction Accuracy = 60.226%, Loss = 0.6727904438972473
Epoch: 8792, Batch Gradient Norm: 21.488184199918923
Epoch: 8792, Batch Gradient Norm after: 21.270333061599487
Epoch 8793/10000, Prediction Accuracy = 60.27%, Loss = 0.6758053660392761
Epoch: 8793, Batch Gradient Norm: 20.514508289623976
Epoch: 8793, Batch Gradient Norm after: 20.176394799214552
Epoch 8794/10000, Prediction Accuracy = 60.234%, Loss = 0.6730276703834533
Epoch: 8794, Batch Gradient Norm: 21.64112605705534
Epoch: 8794, Batch Gradient Norm after: 21.397362367385142
Epoch 8795/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.6762381553649902
Epoch: 8795, Batch Gradient Norm: 20.465056349436995
Epoch: 8795, Batch Gradient Norm after: 20.119192048107905
Epoch 8796/10000, Prediction Accuracy = 60.274%, Loss = 0.6728945136070251
Epoch: 8796, Batch Gradient Norm: 21.55889349239704
Epoch: 8796, Batch Gradient Norm after: 21.344890878219562
Epoch 8797/10000, Prediction Accuracy = 60.188%, Loss = 0.6759610176086426
Epoch: 8797, Batch Gradient Norm: 20.42010657891874
Epoch: 8797, Batch Gradient Norm after: 20.05063858383892
Epoch 8798/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6726686358451843
Epoch: 8798, Batch Gradient Norm: 21.54056531400565
Epoch: 8798, Batch Gradient Norm after: 21.32045677070067
Epoch 8799/10000, Prediction Accuracy = 60.226%, Loss = 0.675794267654419
Epoch: 8799, Batch Gradient Norm: 20.46172927777896
Epoch: 8799, Batch Gradient Norm after: 20.092888196679528
Epoch 8800/10000, Prediction Accuracy = 60.230000000000004%, Loss = 0.6727759122848511
Epoch: 8800, Batch Gradient Norm: 21.589654250881445
Epoch: 8800, Batch Gradient Norm after: 21.364683584226754
Epoch 8801/10000, Prediction Accuracy = 60.21%, Loss = 0.6759942889213562
Epoch: 8801, Batch Gradient Norm: 20.416959965002313
Epoch: 8801, Batch Gradient Norm after: 20.042015167929268
Epoch 8802/10000, Prediction Accuracy = 60.206%, Loss = 0.6726711630821228
Epoch: 8802, Batch Gradient Norm: 21.527215154816943
Epoch: 8802, Batch Gradient Norm after: 21.308301705120947
Epoch 8803/10000, Prediction Accuracy = 60.226%, Loss = 0.675744092464447
Epoch: 8803, Batch Gradient Norm: 20.479899536752377
Epoch: 8803, Batch Gradient Norm after: 20.123759259364103
Epoch 8804/10000, Prediction Accuracy = 60.254%, Loss = 0.6727138876914978
Epoch: 8804, Batch Gradient Norm: 21.618191053421054
Epoch: 8804, Batch Gradient Norm after: 21.38303753129599
Epoch 8805/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6759105563163758
Epoch: 8805, Batch Gradient Norm: 20.46533123042511
Epoch: 8805, Batch Gradient Norm after: 20.10792949993116
Epoch 8806/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6726573467254638
Epoch: 8806, Batch Gradient Norm: 21.589430290910652
Epoch: 8806, Batch Gradient Norm after: 21.367136113477805
Epoch 8807/10000, Prediction Accuracy = 60.202%, Loss = 0.6758653998374939
Epoch: 8807, Batch Gradient Norm: 20.417941608784623
Epoch: 8807, Batch Gradient Norm after: 20.048422988679814
Epoch 8808/10000, Prediction Accuracy = 60.266%, Loss = 0.6724887132644654
Epoch: 8808, Batch Gradient Norm: 21.52657852248888
Epoch: 8808, Batch Gradient Norm after: 21.310439151299846
Epoch 8809/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6755784392356873
Epoch: 8809, Batch Gradient Norm: 20.47809378941416
Epoch: 8809, Batch Gradient Norm after: 20.120094172223485
Epoch 8810/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6725595355033874
Epoch: 8810, Batch Gradient Norm: 21.60841837971804
Epoch: 8810, Batch Gradient Norm after: 21.378402218297662
Epoch 8811/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6757880806922912
Epoch: 8811, Batch Gradient Norm: 20.43610040501667
Epoch: 8811, Batch Gradient Norm after: 20.07043436902647
Epoch 8812/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6725149273872375
Epoch: 8812, Batch Gradient Norm: 21.53815327829026
Epoch: 8812, Batch Gradient Norm after: 21.321196854010942
Epoch 8813/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6756317853927613
Epoch: 8813, Batch Gradient Norm: 20.45887220059087
Epoch: 8813, Batch Gradient Norm after: 20.108963228079944
Epoch 8814/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.6725006341934204
Epoch: 8814, Batch Gradient Norm: 21.564007640379923
Epoch: 8814, Batch Gradient Norm after: 21.347591573120685
Epoch 8815/10000, Prediction Accuracy = 60.278%, Loss = 0.6755487203598023
Epoch: 8815, Batch Gradient Norm: 20.418314727901937
Epoch: 8815, Batch Gradient Norm after: 20.055320653072027
Epoch 8816/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6723052859306335
Epoch: 8816, Batch Gradient Norm: 21.528491695543902
Epoch: 8816, Batch Gradient Norm after: 21.31358474037041
Epoch 8817/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6754530310630799
Epoch: 8817, Batch Gradient Norm: 20.446136681072673
Epoch: 8817, Batch Gradient Norm after: 20.091238936080746
Epoch 8818/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6723748564720153
Epoch: 8818, Batch Gradient Norm: 21.54659234859027
Epoch: 8818, Batch Gradient Norm after: 21.333903615220922
Epoch 8819/10000, Prediction Accuracy = 60.2%, Loss = 0.67546226978302
Epoch: 8819, Batch Gradient Norm: 20.42033643042934
Epoch: 8819, Batch Gradient Norm after: 20.05854723394494
Epoch 8820/10000, Prediction Accuracy = 60.262%, Loss = 0.6722100496292114
Epoch: 8820, Batch Gradient Norm: 21.52930646349899
Epoch: 8820, Batch Gradient Norm after: 21.312037483024866
Epoch 8821/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6753043532371521
Epoch: 8821, Batch Gradient Norm: 20.476356734231754
Epoch: 8821, Batch Gradient Norm after: 20.121198573057075
Epoch 8822/10000, Prediction Accuracy = 60.222%, Loss = 0.6723416209220886
Epoch: 8822, Batch Gradient Norm: 21.588755078657766
Epoch: 8822, Batch Gradient Norm after: 21.370098264801353
Epoch 8823/10000, Prediction Accuracy = 60.202%, Loss = 0.6755284786224365
Epoch: 8823, Batch Gradient Norm: 20.39265652502679
Epoch: 8823, Batch Gradient Norm after: 20.023468065927585
Epoch 8824/10000, Prediction Accuracy = 60.214%, Loss = 0.6721451640129089
Epoch: 8824, Batch Gradient Norm: 21.478974015332348
Epoch: 8824, Batch Gradient Norm after: 21.26432821261408
Epoch 8825/10000, Prediction Accuracy = 60.218%, Loss = 0.6751358151435852
Epoch: 8825, Batch Gradient Norm: 20.519889137480636
Epoch: 8825, Batch Gradient Norm after: 20.1934914895473
Epoch 8826/10000, Prediction Accuracy = 60.274%, Loss = 0.6723562598228454
Epoch: 8826, Batch Gradient Norm: 21.642211711160968
Epoch: 8826, Batch Gradient Norm after: 21.405342143375872
Epoch 8827/10000, Prediction Accuracy = 60.238%, Loss = 0.675498366355896
Epoch: 8827, Batch Gradient Norm: 20.437287372132538
Epoch: 8827, Batch Gradient Norm after: 20.094762826318117
Epoch 8828/10000, Prediction Accuracy = 60.23%, Loss = 0.6721197009086609
Epoch: 8828, Batch Gradient Norm: 21.51325706946928
Epoch: 8828, Batch Gradient Norm after: 21.30438355249228
Epoch 8829/10000, Prediction Accuracy = 60.2%, Loss = 0.6751803040504456
Epoch: 8829, Batch Gradient Norm: 20.437783303324995
Epoch: 8829, Batch Gradient Norm after: 20.09889711911466
Epoch 8830/10000, Prediction Accuracy = 60.246%, Loss = 0.6720821499824524
Epoch: 8830, Batch Gradient Norm: 21.50457155944162
Epoch: 8830, Batch Gradient Norm after: 21.29597435207611
Epoch 8831/10000, Prediction Accuracy = 60.23599999999999%, Loss = 0.6750484585762024
Epoch: 8831, Batch Gradient Norm: 20.47213706672145
Epoch: 8831, Batch Gradient Norm after: 20.137794073397284
Epoch 8832/10000, Prediction Accuracy = 60.282%, Loss = 0.6720800638198853
Epoch: 8832, Batch Gradient Norm: 21.56002154789336
Epoch: 8832, Batch Gradient Norm after: 21.34935294688723
Epoch 8833/10000, Prediction Accuracy = 60.212%, Loss = 0.6751734256744385
Epoch: 8833, Batch Gradient Norm: 20.406249896484873
Epoch: 8833, Batch Gradient Norm after: 20.04725864752297
Epoch 8834/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6719615697860718
Epoch: 8834, Batch Gradient Norm: 21.471195128389887
Epoch: 8834, Batch Gradient Norm after: 21.262737245314813
Epoch 8835/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6749700307846069
Epoch: 8835, Batch Gradient Norm: 20.506188966428844
Epoch: 8835, Batch Gradient Norm after: 20.19303378914521
Epoch 8836/10000, Prediction Accuracy = 60.21999999999999%, Loss = 0.6721598982810975
Epoch: 8836, Batch Gradient Norm: 21.571159598573015
Epoch: 8836, Batch Gradient Norm after: 21.36450500346784
Epoch 8837/10000, Prediction Accuracy = 60.27%, Loss = 0.6751044631004334
Epoch: 8837, Batch Gradient Norm: 20.37903797574333
Epoch: 8837, Batch Gradient Norm after: 20.024243189278952
Epoch 8838/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.671726393699646
Epoch: 8838, Batch Gradient Norm: 21.421247528802372
Epoch: 8838, Batch Gradient Norm after: 21.21476935026924
Epoch 8839/10000, Prediction Accuracy = 60.214%, Loss = 0.674698007106781
Epoch: 8839, Batch Gradient Norm: 20.556425602246154
Epoch: 8839, Batch Gradient Norm after: 20.26392423521393
Epoch 8840/10000, Prediction Accuracy = 60.266%, Loss = 0.672208034992218
Epoch: 8840, Batch Gradient Norm: 21.613744740991667
Epoch: 8840, Batch Gradient Norm after: 21.402683230957365
Epoch 8841/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6751819491386414
Epoch: 8841, Batch Gradient Norm: 20.359045015397964
Epoch: 8841, Batch Gradient Norm after: 20.010025397415635
Epoch 8842/10000, Prediction Accuracy = 60.288%, Loss = 0.6715689778327942
Epoch: 8842, Batch Gradient Norm: 21.379329667415693
Epoch: 8842, Batch Gradient Norm after: 21.174336093019992
Epoch 8843/10000, Prediction Accuracy = 60.23%, Loss = 0.6744281411170959
Epoch: 8843, Batch Gradient Norm: 20.598949262108224
Epoch: 8843, Batch Gradient Norm after: 20.325728647472374
Epoch 8844/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6722170352935791
Epoch: 8844, Batch Gradient Norm: 21.65762330676138
Epoch: 8844, Batch Gradient Norm after: 21.43135131658693
Epoch 8845/10000, Prediction Accuracy = 60.2%, Loss = 0.6752442479133606
Epoch: 8845, Batch Gradient Norm: 20.37456527910008
Epoch: 8845, Batch Gradient Norm after: 20.046062662626245
Epoch 8846/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6716200232505798
Epoch: 8846, Batch Gradient Norm: 21.356449676755755
Epoch: 8846, Batch Gradient Norm after: 21.15731103522555
Epoch 8847/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6743342518806458
Epoch: 8847, Batch Gradient Norm: 20.596252136340897
Epoch: 8847, Batch Gradient Norm after: 20.345967738229305
Epoch 8848/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6720921277999878
Epoch: 8848, Batch Gradient Norm: 21.613178362193846
Epoch: 8848, Batch Gradient Norm after: 21.4122379874116
Epoch 8849/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6749639987945557
Epoch: 8849, Batch Gradient Norm: 20.30987133343734
Epoch: 8849, Batch Gradient Norm after: 19.966194722881227
Epoch 8850/10000, Prediction Accuracy = 60.23599999999999%, Loss = 0.6713011145591736
Epoch: 8850, Batch Gradient Norm: 21.281202804258307
Epoch: 8850, Batch Gradient Norm after: 21.08333154119082
Epoch 8851/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.6740592360496521
Epoch: 8851, Batch Gradient Norm: 20.67773628497418
Epoch: 8851, Batch Gradient Norm after: 20.459908151738098
Epoch 8852/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6722483515739441
Epoch: 8852, Batch Gradient Norm: 21.673900405847874
Epoch: 8852, Batch Gradient Norm after: 21.458812876562405
Epoch 8853/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6750491499900818
Epoch: 8853, Batch Gradient Norm: 20.319523317469628
Epoch: 8853, Batch Gradient Norm after: 19.99127459175193
Epoch 8854/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.6712051391601562
Epoch: 8854, Batch Gradient Norm: 21.27524829821373
Epoch: 8854, Batch Gradient Norm after: 21.08011204586541
Epoch 8855/10000, Prediction Accuracy = 60.21%, Loss = 0.6739141821861268
Epoch: 8855, Batch Gradient Norm: 20.66210589911802
Epoch: 8855, Batch Gradient Norm after: 20.44740086723207
Epoch 8856/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6721867561340332
Epoch: 8856, Batch Gradient Norm: 21.635951010690032
Epoch: 8856, Batch Gradient Norm after: 21.43844325161217
Epoch 8857/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6749586939811707
Epoch: 8857, Batch Gradient Norm: 20.26926131762388
Epoch: 8857, Batch Gradient Norm after: 19.937223526996473
Epoch 8858/10000, Prediction Accuracy = 60.214%, Loss = 0.671046257019043
Epoch: 8858, Batch Gradient Norm: 21.19278446851557
Epoch: 8858, Batch Gradient Norm after: 20.999807920833693
Epoch 8859/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6735851287841796
Epoch: 8859, Batch Gradient Norm: 20.762601641116525
Epoch: 8859, Batch Gradient Norm after: 20.589405899318155
Epoch 8860/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.6722874164581298
Epoch: 8860, Batch Gradient Norm: 21.682133841156745
Epoch: 8860, Batch Gradient Norm after: 21.47339328584764
Epoch 8861/10000, Prediction Accuracy = 60.238%, Loss = 0.6749373197555542
Epoch: 8861, Batch Gradient Norm: 20.239646174767877
Epoch: 8861, Batch Gradient Norm after: 19.906443369096557
Epoch 8862/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6708720803260804
Epoch: 8862, Batch Gradient Norm: 21.13637119934163
Epoch: 8862, Batch Gradient Norm after: 20.96186157741337
Epoch 8863/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.6733859419822693
Epoch: 8863, Batch Gradient Norm: 20.722114783271877
Epoch: 8863, Batch Gradient Norm after: 20.545590065895585
Epoch 8864/10000, Prediction Accuracy = 60.29%, Loss = 0.672086501121521
Epoch: 8864, Batch Gradient Norm: 21.66333791257005
Epoch: 8864, Batch Gradient Norm after: 21.462672365258086
Epoch 8865/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.674749743938446
Epoch: 8865, Batch Gradient Norm: 20.26208676910612
Epoch: 8865, Batch Gradient Norm after: 19.93786048093191
Epoch 8866/10000, Prediction Accuracy = 60.238%, Loss = 0.6708290338516235
Epoch: 8866, Batch Gradient Norm: 21.16650710644931
Epoch: 8866, Batch Gradient Norm after: 20.978837756296233
Epoch 8867/10000, Prediction Accuracy = 60.234%, Loss = 0.6734095096588135
Epoch: 8867, Batch Gradient Norm: 20.73375665335013
Epoch: 8867, Batch Gradient Norm after: 20.574837100035953
Epoch 8868/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.672150194644928
Epoch: 8868, Batch Gradient Norm: 21.653438914275227
Epoch: 8868, Batch Gradient Norm after: 21.461066387112705
Epoch 8869/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.6746997117996216
Epoch: 8869, Batch Gradient Norm: 20.236270480658767
Epoch: 8869, Batch Gradient Norm after: 19.919868400491723
Epoch 8870/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6706607341766357
Epoch: 8870, Batch Gradient Norm: 21.11145017591068
Epoch: 8870, Batch Gradient Norm after: 20.950268116608594
Epoch 8871/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6730857014656066
Epoch: 8871, Batch Gradient Norm: 20.66761041662953
Epoch: 8871, Batch Gradient Norm after: 20.482888817940157
Epoch 8872/10000, Prediction Accuracy = 60.262%, Loss = 0.671801221370697
Epoch: 8872, Batch Gradient Norm: 21.592585009787374
Epoch: 8872, Batch Gradient Norm after: 21.413456097409657
Epoch 8873/10000, Prediction Accuracy = 60.214%, Loss = 0.674472165107727
Epoch: 8873, Batch Gradient Norm: 20.232199356184104
Epoch: 8873, Batch Gradient Norm after: 19.909456230357534
Epoch 8874/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6705862522125244
Epoch: 8874, Batch Gradient Norm: 21.108240587493356
Epoch: 8874, Batch Gradient Norm after: 20.94949869246585
Epoch 8875/10000, Prediction Accuracy = 60.242%, Loss = 0.673006010055542
Epoch: 8875, Batch Gradient Norm: 20.67671587188651
Epoch: 8875, Batch Gradient Norm after: 20.489250035777214
Epoch 8876/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6716930985450744
Epoch: 8876, Batch Gradient Norm: 21.621470465267972
Epoch: 8876, Batch Gradient Norm after: 21.435637893403406
Epoch 8877/10000, Prediction Accuracy = 60.212%, Loss = 0.6743936419487
Epoch: 8877, Batch Gradient Norm: 20.230320153942852
Epoch: 8877, Batch Gradient Norm after: 19.897283791816623
Epoch 8878/10000, Prediction Accuracy = 60.222%, Loss = 0.670534634590149
Epoch: 8878, Batch Gradient Norm: 21.117968325752432
Epoch: 8878, Batch Gradient Norm after: 20.951763742399674
Epoch 8879/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6730253100395203
Epoch: 8879, Batch Gradient Norm: 20.67357536839571
Epoch: 8879, Batch Gradient Norm after: 20.49591412111827
Epoch 8880/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6716806530952454
Epoch: 8880, Batch Gradient Norm: 21.59972917491217
Epoch: 8880, Batch Gradient Norm after: 21.421416890578005
Epoch 8881/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6742509722709655
Epoch: 8881, Batch Gradient Norm: 20.233825380075654
Epoch: 8881, Batch Gradient Norm after: 19.905952001051872
Epoch 8882/10000, Prediction Accuracy = 60.274%, Loss = 0.6703784465789795
Epoch: 8882, Batch Gradient Norm: 21.125621832364732
Epoch: 8882, Batch Gradient Norm after: 20.956647110684827
Epoch 8883/10000, Prediction Accuracy = 60.25%, Loss = 0.6728866815567016
Epoch: 8883, Batch Gradient Norm: 20.697718049747866
Epoch: 8883, Batch Gradient Norm after: 20.523563480543245
Epoch 8884/10000, Prediction Accuracy = 60.274%, Loss = 0.6716444611549377
Epoch: 8884, Batch Gradient Norm: 21.621767702379188
Epoch: 8884, Batch Gradient Norm after: 21.44392957819197
Epoch 8885/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6742778658866883
Epoch: 8885, Batch Gradient Norm: 20.21034181428707
Epoch: 8885, Batch Gradient Norm after: 19.880686231838162
Epoch 8886/10000, Prediction Accuracy = 60.302%, Loss = 0.6702316164970398
Epoch: 8886, Batch Gradient Norm: 21.084115464762377
Epoch: 8886, Batch Gradient Norm after: 20.934456992946412
Epoch 8887/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6726529359817505
Epoch: 8887, Batch Gradient Norm: 20.640977690647503
Epoch: 8887, Batch Gradient Norm after: 20.443113514473094
Epoch 8888/10000, Prediction Accuracy = 60.238%, Loss = 0.6713717341423034
Epoch: 8888, Batch Gradient Norm: 21.574963194109007
Epoch: 8888, Batch Gradient Norm after: 21.39670930770917
Epoch 8889/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6740643382072449
Epoch: 8889, Batch Gradient Norm: 20.25343517118762
Epoch: 8889, Batch Gradient Norm after: 19.931929792558613
Epoch 8890/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6703436255455018
Epoch: 8890, Batch Gradient Norm: 21.1220405071121
Epoch: 8890, Batch Gradient Norm after: 20.961209674887296
Epoch 8891/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6727562308311462
Epoch: 8891, Batch Gradient Norm: 20.663723104365648
Epoch: 8891, Batch Gradient Norm after: 20.486944393754506
Epoch 8892/10000, Prediction Accuracy = 60.27%, Loss = 0.671363353729248
Epoch: 8892, Batch Gradient Norm: 21.57933947553991
Epoch: 8892, Batch Gradient Norm after: 21.403986202458412
Epoch 8893/10000, Prediction Accuracy = 60.28399999999999%, Loss = 0.6739330291748047
Epoch: 8893, Batch Gradient Norm: 20.260878006374465
Epoch: 8893, Batch Gradient Norm after: 19.943788127378006
Epoch 8894/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6702058553695679
Epoch: 8894, Batch Gradient Norm: 21.138728707292223
Epoch: 8894, Batch Gradient Norm after: 20.97374432922373
Epoch 8895/10000, Prediction Accuracy = 60.21199999999999%, Loss = 0.6727294802665711
Epoch: 8895, Batch Gradient Norm: 20.668571628530305
Epoch: 8895, Batch Gradient Norm after: 20.49770366159705
Epoch 8896/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6713265419006348
Epoch: 8896, Batch Gradient Norm: 21.560995583089905
Epoch: 8896, Batch Gradient Norm after: 21.391578612227118
Epoch 8897/10000, Prediction Accuracy = 60.238%, Loss = 0.6738369226455688
Epoch: 8897, Batch Gradient Norm: 20.25952981265741
Epoch: 8897, Batch Gradient Norm after: 19.952087109209227
Epoch 8898/10000, Prediction Accuracy = 60.294%, Loss = 0.6700877666473388
Epoch: 8898, Batch Gradient Norm: 21.118776227041817
Epoch: 8898, Batch Gradient Norm after: 20.963050724276123
Epoch 8899/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.67252596616745
Epoch: 8899, Batch Gradient Norm: 20.639225383391487
Epoch: 8899, Batch Gradient Norm after: 20.456236924081406
Epoch 8900/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.6711783647537232
Epoch: 8900, Batch Gradient Norm: 21.528880816588504
Epoch: 8900, Batch Gradient Norm after: 21.360470011410445
Epoch 8901/10000, Prediction Accuracy = 60.262%, Loss = 0.673740029335022
Epoch: 8901, Batch Gradient Norm: 20.276864160179006
Epoch: 8901, Batch Gradient Norm after: 19.984783302602484
Epoch 8902/10000, Prediction Accuracy = 60.202%, Loss = 0.6701375365257263
Epoch: 8902, Batch Gradient Norm: 21.11432041548914
Epoch: 8902, Batch Gradient Norm after: 20.966598589046594
Epoch 8903/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6724318861961365
Epoch: 8903, Batch Gradient Norm: 20.620808728207038
Epoch: 8903, Batch Gradient Norm after: 20.439210747657683
Epoch 8904/10000, Prediction Accuracy = 60.286%, Loss = 0.6709564685821533
Epoch: 8904, Batch Gradient Norm: 21.50552139013027
Epoch: 8904, Batch Gradient Norm after: 21.336158022989135
Epoch 8905/10000, Prediction Accuracy = 60.222%, Loss = 0.6734888315200805
Epoch: 8905, Batch Gradient Norm: 20.312309123598332
Epoch: 8905, Batch Gradient Norm after: 20.03202629168081
Epoch 8906/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6701075792312622
Epoch: 8906, Batch Gradient Norm: 21.15270644918632
Epoch: 8906, Batch Gradient Norm after: 20.99406475781889
Epoch 8907/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.6725204348564148
Epoch: 8907, Batch Gradient Norm: 20.644852994443625
Epoch: 8907, Batch Gradient Norm after: 20.48451867402311
Epoch 8908/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6709653615951539
Epoch: 8908, Batch Gradient Norm: 21.505700699862544
Epoch: 8908, Batch Gradient Norm after: 21.342829635094503
Epoch 8909/10000, Prediction Accuracy = 60.246%, Loss = 0.6733878374099731
Epoch: 8909, Batch Gradient Norm: 20.29657034620269
Epoch: 8909, Batch Gradient Norm after: 20.01933653230744
Epoch 8910/10000, Prediction Accuracy = 60.278%, Loss = 0.6699522376060486
Epoch: 8910, Batch Gradient Norm: 21.12283439850728
Epoch: 8910, Batch Gradient Norm after: 20.976726182583583
Epoch 8911/10000, Prediction Accuracy = 60.25%, Loss = 0.6723187208175659
Epoch: 8911, Batch Gradient Norm: 20.597129649635082
Epoch: 8911, Batch Gradient Norm after: 20.420321180875614
Epoch 8912/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.6708135366439819
Epoch: 8912, Batch Gradient Norm: 21.436477702964684
Epoch: 8912, Batch Gradient Norm after: 21.274939879473088
Epoch 8913/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6731975436210632
Epoch: 8913, Batch Gradient Norm: 20.357614388376096
Epoch: 8913, Batch Gradient Norm after: 20.116394928184192
Epoch 8914/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6700681447982788
Epoch: 8914, Batch Gradient Norm: 21.163500114905794
Epoch: 8914, Batch Gradient Norm after: 21.008453191485977
Epoch 8915/10000, Prediction Accuracy = 60.294%, Loss = 0.6723088145256042
Epoch: 8915, Batch Gradient Norm: 20.624912645595145
Epoch: 8915, Batch Gradient Norm after: 20.47144196122637
Epoch 8916/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.670730710029602
Epoch: 8916, Batch Gradient Norm: 21.44986498838764
Epoch: 8916, Batch Gradient Norm after: 21.291935442781615
Epoch 8917/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6731276392936707
Epoch: 8917, Batch Gradient Norm: 20.322915144611695
Epoch: 8917, Batch Gradient Norm after: 20.0795854187719
Epoch 8918/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6699030637741089
Epoch: 8918, Batch Gradient Norm: 21.098730295157146
Epoch: 8918, Batch Gradient Norm after: 20.975130724546755
Epoch 8919/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6720916748046875
Epoch: 8919, Batch Gradient Norm: 20.527858285719496
Epoch: 8919, Batch Gradient Norm after: 20.344891137186725
Epoch 8920/10000, Prediction Accuracy = 60.306%, Loss = 0.6703589677810669
Epoch: 8920, Batch Gradient Norm: 21.351273995437825
Epoch: 8920, Batch Gradient Norm after: 21.19102053425557
Epoch 8921/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6726962089538574
Epoch: 8921, Batch Gradient Norm: 20.438467457968
Epoch: 8921, Batch Gradient Norm after: 20.229081298832906
Epoch 8922/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.670129942893982
Epoch: 8922, Batch Gradient Norm: 21.230726701207136
Epoch: 8922, Batch Gradient Norm after: 21.07178386495648
Epoch 8923/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.6724318504333496
Epoch: 8923, Batch Gradient Norm: 20.563320625543298
Epoch: 8923, Batch Gradient Norm after: 20.410697320816695
Epoch 8924/10000, Prediction Accuracy = 60.194%, Loss = 0.6704760193824768
Epoch: 8924, Batch Gradient Norm: 21.336023095742842
Epoch: 8924, Batch Gradient Norm after: 21.182562954702675
Epoch 8925/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6726179480552673
Epoch: 8925, Batch Gradient Norm: 20.424177455323175
Epoch: 8925, Batch Gradient Norm after: 20.233686349970615
Epoch 8926/10000, Prediction Accuracy = 60.278%, Loss = 0.6699647665023803
Epoch: 8926, Batch Gradient Norm: 21.1808991157388
Epoch: 8926, Batch Gradient Norm after: 21.03307363218722
Epoch 8927/10000, Prediction Accuracy = 60.226%, Loss = 0.6721246004104614
Epoch: 8927, Batch Gradient Norm: 20.5666585996695
Epoch: 8927, Batch Gradient Norm after: 20.423684718966904
Epoch 8928/10000, Prediction Accuracy = 60.278%, Loss = 0.6703453421592712
Epoch: 8928, Batch Gradient Norm: 21.326766560558738
Epoch: 8928, Batch Gradient Norm after: 21.178345857361844
Epoch 8929/10000, Prediction Accuracy = 60.194%, Loss = 0.672555947303772
Epoch: 8929, Batch Gradient Norm: 20.405334720199594
Epoch: 8929, Batch Gradient Norm after: 20.222928649891163
Epoch 8930/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.6698616027832032
Epoch: 8930, Batch Gradient Norm: 21.135093723340752
Epoch: 8930, Batch Gradient Norm after: 21.01121444351734
Epoch 8931/10000, Prediction Accuracy = 60.245999999999995%, Loss = 0.6719005346298218
Epoch: 8931, Batch Gradient Norm: 20.496720616141648
Epoch: 8931, Batch Gradient Norm after: 20.331344034650723
Epoch 8932/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6700263738632202
Epoch: 8932, Batch Gradient Norm: 21.258247500625433
Epoch: 8932, Batch Gradient Norm after: 21.10739737238699
Epoch 8933/10000, Prediction Accuracy = 60.246%, Loss = 0.6722486734390258
Epoch: 8933, Batch Gradient Norm: 20.491148317969866
Epoch: 8933, Batch Gradient Norm after: 20.333315893760687
Epoch 8934/10000, Prediction Accuracy = 60.2%, Loss = 0.6700762629508972
Epoch: 8934, Batch Gradient Norm: 21.218019971080203
Epoch: 8934, Batch Gradient Norm after: 21.070525086116636
Epoch 8935/10000, Prediction Accuracy = 60.274%, Loss = 0.6721496343612671
Epoch: 8935, Batch Gradient Norm: 20.528132055845212
Epoch: 8935, Batch Gradient Norm after: 20.396576130489308
Epoch 8936/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6700820088386535
Epoch: 8936, Batch Gradient Norm: 21.243939650042567
Epoch: 8936, Batch Gradient Norm after: 21.099419443168618
Epoch 8937/10000, Prediction Accuracy = 60.286%, Loss = 0.672079610824585
Epoch: 8937, Batch Gradient Norm: 20.489851541040665
Epoch: 8937, Batch Gradient Norm after: 20.348729289649867
Epoch 8938/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.669897198677063
Epoch: 8938, Batch Gradient Norm: 21.20003965466943
Epoch: 8938, Batch Gradient Norm after: 21.05945167261271
Epoch 8939/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6719619035720825
Epoch: 8939, Batch Gradient Norm: 20.50683096982929
Epoch: 8939, Batch Gradient Norm after: 20.378979926590468
Epoch 8940/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6699502825736999
Epoch: 8940, Batch Gradient Norm: 21.19350803388392
Epoch: 8940, Batch Gradient Norm after: 21.061771841252167
Epoch 8941/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.6719211578369141
Epoch: 8941, Batch Gradient Norm: 20.488476483188176
Epoch: 8941, Batch Gradient Norm after: 20.357994792953775
Epoch 8942/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6697975158691406
Epoch: 8942, Batch Gradient Norm: 21.179654706348472
Epoch: 8942, Batch Gradient Norm after: 21.052703120799084
Epoch 8943/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6717670559883118
Epoch: 8943, Batch Gradient Norm: 20.464738259760487
Epoch: 8943, Batch Gradient Norm after: 20.32248972143816
Epoch 8944/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6697423815727234
Epoch: 8944, Batch Gradient Norm: 21.150371360875834
Epoch: 8944, Batch Gradient Norm after: 21.03422691734234
Epoch 8945/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.6717345595359803
Epoch: 8945, Batch Gradient Norm: 20.408902315498228
Epoch: 8945, Batch Gradient Norm after: 20.253833322174586
Epoch 8946/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6696022152900696
Epoch: 8946, Batch Gradient Norm: 21.082787668049384
Epoch: 8946, Batch Gradient Norm after: 20.990532435059063
Epoch 8947/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6714495539665222
Epoch: 8947, Batch Gradient Norm: 20.358987685534736
Epoch: 8947, Batch Gradient Norm after: 20.172483843128827
Epoch 8948/10000, Prediction Accuracy = 60.286%, Loss = 0.6693422079086304
Epoch: 8948, Batch Gradient Norm: 21.066031680376213
Epoch: 8948, Batch Gradient Norm after: 20.972034914307972
Epoch 8949/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.6713174939155578
Epoch: 8949, Batch Gradient Norm: 20.393006335085705
Epoch: 8949, Batch Gradient Norm after: 20.199684907035174
Epoch 8950/10000, Prediction Accuracy = 60.26399999999999%, Loss = 0.6694032788276673
Epoch: 8950, Batch Gradient Norm: 21.13615389906165
Epoch: 8950, Batch Gradient Norm after: 21.00960935163343
Epoch 8951/10000, Prediction Accuracy = 60.212%, Loss = 0.6715452313423157
Epoch: 8951, Batch Gradient Norm: 20.49569290292002
Epoch: 8951, Batch Gradient Norm after: 20.3397806409849
Epoch 8952/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6696566462516784
Epoch: 8952, Batch Gradient Norm: 21.233339450791725
Epoch: 8952, Batch Gradient Norm after: 21.087906770989783
Epoch 8953/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6717424035072327
Epoch: 8953, Batch Gradient Norm: 20.51204811139112
Epoch: 8953, Batch Gradient Norm after: 20.37118366698095
Epoch 8954/10000, Prediction Accuracy = 60.29%, Loss = 0.6696128368377685
Epoch: 8954, Batch Gradient Norm: 21.23727549351289
Epoch: 8954, Batch Gradient Norm after: 21.092614105218768
Epoch 8955/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6717210054397583
Epoch: 8955, Batch Gradient Norm: 20.492710288783545
Epoch: 8955, Batch Gradient Norm after: 20.35375334852322
Epoch 8956/10000, Prediction Accuracy = 60.19%, Loss = 0.6696230649948121
Epoch: 8956, Batch Gradient Norm: 21.167448550878095
Epoch: 8956, Batch Gradient Norm after: 21.046481773720615
Epoch 8957/10000, Prediction Accuracy = 60.278%, Loss = 0.67156423330307
Epoch: 8957, Batch Gradient Norm: 20.437083524680816
Epoch: 8957, Batch Gradient Norm after: 20.291024089415846
Epoch 8958/10000, Prediction Accuracy = 60.20400000000001%, Loss = 0.6694083213806152
Epoch: 8958, Batch Gradient Norm: 21.101239931821993
Epoch: 8958, Batch Gradient Norm after: 21.00513109144941
Epoch 8959/10000, Prediction Accuracy = 60.302%, Loss = 0.6712356805801392
Epoch: 8959, Batch Gradient Norm: 20.374334048038758
Epoch: 8959, Batch Gradient Norm after: 20.19668707366733
Epoch 8960/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6691261291503906
Epoch: 8960, Batch Gradient Norm: 21.069500195797165
Epoch: 8960, Batch Gradient Norm after: 20.978774888351722
Epoch 8961/10000, Prediction Accuracy = 60.262%, Loss = 0.6711321949958802
Epoch: 8961, Batch Gradient Norm: 20.37481134003842
Epoch: 8961, Batch Gradient Norm after: 20.18368258915627
Epoch 8962/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.669141161441803
Epoch: 8962, Batch Gradient Norm: 21.09248751844185
Epoch: 8962, Batch Gradient Norm after: 20.992229493044796
Epoch 8963/10000, Prediction Accuracy = 60.222%, Loss = 0.6711884617805481
Epoch: 8963, Batch Gradient Norm: 20.42405094984414
Epoch: 8963, Batch Gradient Norm after: 20.24305020147501
Epoch 8964/10000, Prediction Accuracy = 60.314%, Loss = 0.669171142578125
Epoch: 8964, Batch Gradient Norm: 21.158442135009906
Epoch: 8964, Batch Gradient Norm after: 21.029889566765476
Epoch 8965/10000, Prediction Accuracy = 60.237999999999985%, Loss = 0.6712536692619324
Epoch: 8965, Batch Gradient Norm: 20.51158141921819
Epoch: 8965, Batch Gradient Norm after: 20.35918791880352
Epoch 8966/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6693965792655945
Epoch: 8966, Batch Gradient Norm: 21.23006418074019
Epoch: 8966, Batch Gradient Norm after: 21.08686787517315
Epoch 8967/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.67152259349823
Epoch: 8967, Batch Gradient Norm: 20.500241329732845
Epoch: 8967, Batch Gradient Norm after: 20.36693727129938
Epoch 8968/10000, Prediction Accuracy = 60.202%, Loss = 0.6694059610366822
Epoch: 8968, Batch Gradient Norm: 21.174367096744778
Epoch: 8968, Batch Gradient Norm after: 21.05231502137725
Epoch 8969/10000, Prediction Accuracy = 60.27%, Loss = 0.6712886214256286
Epoch: 8969, Batch Gradient Norm: 20.450126247261213
Epoch: 8969, Batch Gradient Norm after: 20.309735244610295
Epoch 8970/10000, Prediction Accuracy = 60.278000000000006%, Loss = 0.6691370010375977
Epoch: 8970, Batch Gradient Norm: 21.12121704490189
Epoch: 8970, Batch Gradient Norm after: 21.021987147274814
Epoch 8971/10000, Prediction Accuracy = 60.274%, Loss = 0.671040678024292
Epoch: 8971, Batch Gradient Norm: 20.383359550492195
Epoch: 8971, Batch Gradient Norm after: 20.21471177683472
Epoch 8972/10000, Prediction Accuracy = 60.254%, Loss = 0.6689146757125854
Epoch: 8972, Batch Gradient Norm: 21.068046297095044
Epoch: 8972, Batch Gradient Norm after: 20.98539046054477
Epoch 8973/10000, Prediction Accuracy = 60.218%, Loss = 0.6709061741828919
Epoch: 8973, Batch Gradient Norm: 20.354082152257575
Epoch: 8973, Batch Gradient Norm after: 20.163435550762973
Epoch 8974/10000, Prediction Accuracy = 60.312%, Loss = 0.66881422996521
Epoch: 8974, Batch Gradient Norm: 21.049963811107126
Epoch: 8974, Batch Gradient Norm after: 20.972353744344534
Epoch 8975/10000, Prediction Accuracy = 60.234%, Loss = 0.6707736849784851
Epoch: 8975, Batch Gradient Norm: 20.35369675652456
Epoch: 8975, Batch Gradient Norm after: 20.14831847157783
Epoch 8976/10000, Prediction Accuracy = 60.314%, Loss = 0.668714416027069
Epoch: 8976, Batch Gradient Norm: 21.082632626563193
Epoch: 8976, Batch Gradient Norm after: 20.98346341792928
Epoch 8977/10000, Prediction Accuracy = 60.288%, Loss = 0.6708134293556214
Epoch: 8977, Batch Gradient Norm: 20.417634892284763
Epoch: 8977, Batch Gradient Norm after: 20.2247137013486
Epoch 8978/10000, Prediction Accuracy = 60.222%, Loss = 0.6689284324645997
Epoch: 8978, Batch Gradient Norm: 21.150289788745326
Epoch: 8978, Batch Gradient Norm after: 21.024238512752017
Epoch 8979/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6710460424423218
Epoch: 8979, Batch Gradient Norm: 20.492016476624354
Epoch: 8979, Batch Gradient Norm after: 20.34319941732753
Epoch 8980/10000, Prediction Accuracy = 60.212%, Loss = 0.6690991997718811
Epoch: 8980, Batch Gradient Norm: 21.194811948264785
Epoch: 8980, Batch Gradient Norm after: 21.060093728190978
Epoch 8981/10000, Prediction Accuracy = 60.289999999999985%, Loss = 0.67107093334198
Epoch: 8981, Batch Gradient Norm: 20.514151901081075
Epoch: 8981, Batch Gradient Norm after: 20.383601512225447
Epoch 8982/10000, Prediction Accuracy = 60.288%, Loss = 0.6690420269966125
Epoch: 8982, Batch Gradient Norm: 21.207428706781535
Epoch: 8982, Batch Gradient Norm after: 21.074331561258113
Epoch 8983/10000, Prediction Accuracy = 60.274%, Loss = 0.6710816502571106
Epoch: 8983, Batch Gradient Norm: 20.49736284694505
Epoch: 8983, Batch Gradient Norm after: 20.374487319315882
Epoch 8984/10000, Prediction Accuracy = 60.326%, Loss = 0.6690093636512756
Epoch: 8984, Batch Gradient Norm: 21.15290641054721
Epoch: 8984, Batch Gradient Norm after: 21.05062575545522
Epoch 8985/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6709359169006348
Epoch: 8985, Batch Gradient Norm: 20.393154480541813
Epoch: 8985, Batch Gradient Norm after: 20.24564987880273
Epoch 8986/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6686426162719726
Epoch: 8986, Batch Gradient Norm: 21.045036552583134
Epoch: 8986, Batch Gradient Norm after: 20.98217232402287
Epoch 8987/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6704829573631287
Epoch: 8987, Batch Gradient Norm: 20.28301003731294
Epoch: 8987, Batch Gradient Norm after: 20.075401143261693
Epoch 8988/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6683024525642395
Epoch: 8988, Batch Gradient Norm: 20.966160035867894
Epoch: 8988, Batch Gradient Norm after: 20.92345223841925
Epoch 8989/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6703013896942138
Epoch: 8989, Batch Gradient Norm: 20.224351792510816
Epoch: 8989, Batch Gradient Norm after: 19.97610247371889
Epoch 8990/10000, Prediction Accuracy = 60.20799999999999%, Loss = 0.668205738067627
Epoch: 8990, Batch Gradient Norm: 20.936131488729952
Epoch: 8990, Batch Gradient Norm after: 20.897183854168876
Epoch 8991/10000, Prediction Accuracy = 60.298%, Loss = 0.670165741443634
Epoch: 8991, Batch Gradient Norm: 20.246476500008647
Epoch: 8991, Batch Gradient Norm after: 19.982840699869513
Epoch 8992/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.668129277229309
Epoch: 8992, Batch Gradient Norm: 21.02444217596183
Epoch: 8992, Batch Gradient Norm after: 20.937603468583436
Epoch 8993/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6702991485595703
Epoch: 8993, Batch Gradient Norm: 20.428187499164434
Epoch: 8993, Batch Gradient Norm after: 20.210783901002344
Epoch 8994/10000, Prediction Accuracy = 60.266000000000005%, Loss = 0.6685533881187439
Epoch: 8994, Batch Gradient Norm: 21.239097160338428
Epoch: 8994, Batch Gradient Norm after: 21.091135610345724
Epoch 8995/10000, Prediction Accuracy = 60.226%, Loss = 0.6709385871887207
Epoch: 8995, Batch Gradient Norm: 20.54443607574723
Epoch: 8995, Batch Gradient Norm after: 20.391636669414336
Epoch 8996/10000, Prediction Accuracy = 60.306%, Loss = 0.6688730478286743
Epoch: 8996, Batch Gradient Norm: 21.296063644516327
Epoch: 8996, Batch Gradient Norm after: 21.15782263194704
Epoch 8997/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6710449695587158
Epoch: 8997, Batch Gradient Norm: 20.44597299025549
Epoch: 8997, Batch Gradient Norm after: 20.283916973749115
Epoch 8998/10000, Prediction Accuracy = 60.30400000000001%, Loss = 0.6685120224952698
Epoch: 8998, Batch Gradient Norm: 21.162662987880367
Epoch: 8998, Batch Gradient Norm after: 21.046546127025216
Epoch 8999/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6705957651138306
Epoch: 8999, Batch Gradient Norm: 20.46343628565196
Epoch: 8999, Batch Gradient Norm after: 20.31131527364766
Epoch 9000/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.6685811161994935
Epoch: 9000, Batch Gradient Norm: 21.149293245017688
Epoch: 9000, Batch Gradient Norm after: 21.04364270225012
Epoch 9001/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6706000924110412
Epoch: 9001, Batch Gradient Norm: 20.3967963675181
Epoch: 9001, Batch Gradient Norm after: 20.240146639614128
Epoch 9002/10000, Prediction Accuracy = 60.222%, Loss = 0.6683886408805847
Epoch: 9002, Batch Gradient Norm: 21.056256655497133
Epoch: 9002, Batch Gradient Norm after: 20.98792174427788
Epoch 9003/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6702241659164428
Epoch: 9003, Batch Gradient Norm: 20.306338273455612
Epoch: 9003, Batch Gradient Norm after: 20.107490368107158
Epoch 9004/10000, Prediction Accuracy = 60.3%, Loss = 0.6680176138877869
Epoch: 9004, Batch Gradient Norm: 20.992557378970453
Epoch: 9004, Batch Gradient Norm after: 20.942432229767487
Epoch 9005/10000, Prediction Accuracy = 60.266%, Loss = 0.6699912548065186
Epoch: 9005, Batch Gradient Norm: 20.26947681502143
Epoch: 9005, Batch Gradient Norm after: 20.03924893847566
Epoch 9006/10000, Prediction Accuracy = 60.31199999999999%, Loss = 0.6679145216941833
Epoch: 9006, Batch Gradient Norm: 20.994929393482728
Epoch: 9006, Batch Gradient Norm after: 20.935724857105054
Epoch 9007/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6700156807899476
Epoch: 9007, Batch Gradient Norm: 20.307977580040514
Epoch: 9007, Batch Gradient Norm after: 20.081096181083694
Epoch 9008/10000, Prediction Accuracy = 60.32399999999999%, Loss = 0.6679676055908204
Epoch: 9008, Batch Gradient Norm: 21.057123939820986
Epoch: 9008, Batch Gradient Norm after: 20.969219684788822
Epoch 9009/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.670075786113739
Epoch: 9009, Batch Gradient Norm: 20.41120972200379
Epoch: 9009, Batch Gradient Norm after: 20.209129249217014
Epoch 9010/10000, Prediction Accuracy = 60.29%, Loss = 0.6681753277778626
Epoch: 9010, Batch Gradient Norm: 21.175399505245316
Epoch: 9010, Batch Gradient Norm after: 21.042692701244977
Epoch 9011/10000, Prediction Accuracy = 60.266000000000005%, Loss = 0.6704399824142456
Epoch: 9011, Batch Gradient Norm: 20.524567626118817
Epoch: 9011, Batch Gradient Norm after: 20.377805460712143
Epoch 9012/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6685630083084106
Epoch: 9012, Batch Gradient Norm: 21.233454015275765
Epoch: 9012, Batch Gradient Norm after: 21.097462827646197
Epoch 9013/10000, Prediction Accuracy = 60.29%, Loss = 0.6705978631973266
Epoch: 9013, Batch Gradient Norm: 20.493084313227534
Epoch: 9013, Batch Gradient Norm after: 20.368405624268924
Epoch 9014/10000, Prediction Accuracy = 60.254%, Loss = 0.6683708906173706
Epoch: 9014, Batch Gradient Norm: 21.15201377885361
Epoch: 9014, Batch Gradient Norm after: 21.05340987153231
Epoch 9015/10000, Prediction Accuracy = 60.312%, Loss = 0.6702373623847961
Epoch: 9015, Batch Gradient Norm: 20.38271460674152
Epoch: 9015, Batch Gradient Norm after: 20.227741524943973
Epoch 9016/10000, Prediction Accuracy = 60.258%, Loss = 0.6679858326911926
Epoch: 9016, Batch Gradient Norm: 21.036629116344013
Epoch: 9016, Batch Gradient Norm after: 20.97961106977063
Epoch 9017/10000, Prediction Accuracy = 60.242%, Loss = 0.6699230670928955
Epoch: 9017, Batch Gradient Norm: 20.259013045179838
Epoch: 9017, Batch Gradient Norm after: 20.05553672968682
Epoch 9018/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.667661440372467
Epoch: 9018, Batch Gradient Norm: 20.91687166995528
Epoch: 9018, Batch Gradient Norm after: 20.902532419430315
Epoch 9019/10000, Prediction Accuracy = 60.23%, Loss = 0.6695221900939942
Epoch: 9019, Batch Gradient Norm: 20.159850396462044
Epoch: 9019, Batch Gradient Norm after: 19.888428690821616
Epoch 9020/10000, Prediction Accuracy = 60.32199999999999%, Loss = 0.6672757506370545
Epoch: 9020, Batch Gradient Norm: 20.880452081133004
Epoch: 9020, Batch Gradient Norm after: 20.86410609758983
Epoch 9021/10000, Prediction Accuracy = 60.278%, Loss = 0.6693126559257507
Epoch: 9021, Batch Gradient Norm: 20.19973445959594
Epoch: 9021, Batch Gradient Norm after: 19.90426473403064
Epoch 9022/10000, Prediction Accuracy = 60.262%, Loss = 0.6673876047134399
Epoch: 9022, Batch Gradient Norm: 20.97748255084678
Epoch: 9022, Batch Gradient Norm after: 20.911791265985446
Epoch 9023/10000, Prediction Accuracy = 60.261999999999986%, Loss = 0.6696354269981384
Epoch: 9023, Batch Gradient Norm: 20.361499486475076
Epoch: 9023, Batch Gradient Norm after: 20.114795978360025
Epoch 9024/10000, Prediction Accuracy = 60.218%, Loss = 0.6678256511688232
Epoch: 9024, Batch Gradient Norm: 21.15962879567497
Epoch: 9024, Batch Gradient Norm after: 21.024778948240485
Epoch 9025/10000, Prediction Accuracy = 60.29599999999999%, Loss = 0.6700785160064697
Epoch: 9025, Batch Gradient Norm: 20.568811947093554
Epoch: 9025, Batch Gradient Norm after: 20.41651228553704
Epoch 9026/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6682812094688415
Epoch: 9026, Batch Gradient Norm: 21.33568063088035
Epoch: 9026, Batch Gradient Norm after: 21.19665446065869
Epoch 9027/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6705156803131104
Epoch: 9027, Batch Gradient Norm: 20.398360948449163
Epoch: 9027, Batch Gradient Norm after: 20.221034100871478
Epoch 9028/10000, Prediction Accuracy = 60.274%, Loss = 0.6678059577941895
Epoch: 9028, Batch Gradient Norm: 21.10221150550918
Epoch: 9028, Batch Gradient Norm after: 21.010887428100215
Epoch 9029/10000, Prediction Accuracy = 60.227999999999994%, Loss = 0.6698742866516113
Epoch: 9029, Batch Gradient Norm: 20.38683539061933
Epoch: 9029, Batch Gradient Norm after: 20.21563359761042
Epoch 9030/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.667739737033844
Epoch: 9030, Batch Gradient Norm: 21.06665421579396
Epoch: 9030, Batch Gradient Norm after: 20.993724908406367
Epoch 9031/10000, Prediction Accuracy = 60.26399999999999%, Loss = 0.6696725487709045
Epoch: 9031, Batch Gradient Norm: 20.331176829691618
Epoch: 9031, Batch Gradient Norm after: 20.13672666395897
Epoch 9032/10000, Prediction Accuracy = 60.306%, Loss = 0.6675015330314636
Epoch: 9032, Batch Gradient Norm: 21.02746060017482
Epoch: 9032, Batch Gradient Norm after: 20.96468828335537
Epoch 9033/10000, Prediction Accuracy = 60.28399999999999%, Loss = 0.6695360541343689
Epoch: 9033, Batch Gradient Norm: 20.301565415354126
Epoch: 9033, Batch Gradient Norm after: 20.087181006611832
Epoch 9034/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6674901366233825
Epoch: 9034, Batch Gradient Norm: 20.999751282051836
Epoch: 9034, Batch Gradient Norm after: 20.94536614484041
Epoch 9035/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6694797515869141
Epoch: 9035, Batch Gradient Norm: 20.276500269091
Epoch: 9035, Batch Gradient Norm after: 20.049649166944388
Epoch 9036/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.6673423171043396
Epoch: 9036, Batch Gradient Norm: 20.986136832352074
Epoch: 9036, Batch Gradient Norm after: 20.934075982062588
Epoch 9037/10000, Prediction Accuracy = 60.318%, Loss = 0.6693033814430237
Epoch: 9037, Batch Gradient Norm: 20.290090597162767
Epoch: 9037, Batch Gradient Norm after: 20.055127625627055
Epoch 9038/10000, Prediction Accuracy = 60.278%, Loss = 0.667276918888092
Epoch: 9038, Batch Gradient Norm: 21.029018033885837
Epoch: 9038, Batch Gradient Norm after: 20.95526076610664
Epoch 9039/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.669436252117157
Epoch: 9039, Batch Gradient Norm: 20.367923808305793
Epoch: 9039, Batch Gradient Norm after: 20.15766766045863
Epoch 9040/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6675002455711365
Epoch: 9040, Batch Gradient Norm: 21.102069207186954
Epoch: 9040, Batch Gradient Norm after: 21.003824824853062
Epoch 9041/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.6696138381958008
Epoch: 9041, Batch Gradient Norm: 20.434240372412823
Epoch: 9041, Batch Gradient Norm after: 20.259436982201702
Epoch 9042/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6675735592842102
Epoch: 9042, Batch Gradient Norm: 21.160275474712446
Epoch: 9042, Batch Gradient Norm after: 21.044795536888216
Epoch 9043/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.6696774244308472
Epoch: 9043, Batch Gradient Norm: 20.469368735111917
Epoch: 9043, Batch Gradient Norm after: 20.324283732559678
Epoch 9044/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6676712155342102
Epoch: 9044, Batch Gradient Norm: 21.14978322489306
Epoch: 9044, Batch Gradient Norm after: 21.04955211010128
Epoch 9045/10000, Prediction Accuracy = 60.258%, Loss = 0.6696996569633484
Epoch: 9045, Batch Gradient Norm: 20.383662372664553
Epoch: 9045, Batch Gradient Norm after: 20.228398434861035
Epoch 9046/10000, Prediction Accuracy = 60.226%, Loss = 0.6674530982971192
Epoch: 9046, Batch Gradient Norm: 21.018937777467894
Epoch: 9046, Batch Gradient Norm after: 20.974149772401603
Epoch 9047/10000, Prediction Accuracy = 60.28800000000001%, Loss = 0.6692477583885192
Epoch: 9047, Batch Gradient Norm: 20.230738050773578
Epoch: 9047, Batch Gradient Norm after: 20.018158876477916
Epoch 9048/10000, Prediction Accuracy = 60.29%, Loss = 0.6669193983078003
Epoch: 9048, Batch Gradient Norm: 20.886300942244752
Epoch: 9048, Batch Gradient Norm after: 20.884987560276315
Epoch 9049/10000, Prediction Accuracy = 60.274%, Loss = 0.6687797427177429
Epoch: 9049, Batch Gradient Norm: 20.134457773174333
Epoch: 9049, Batch Gradient Norm after: 19.853766311790356
Epoch 9050/10000, Prediction Accuracy = 60.29%, Loss = 0.6666295528411865
Epoch: 9050, Batch Gradient Norm: 20.837233904184302
Epoch: 9050, Batch Gradient Norm after: 20.837233904184302
Epoch 9051/10000, Prediction Accuracy = 60.25%, Loss = 0.668653130531311
Epoch: 9051, Batch Gradient Norm: 20.162600030754128
Epoch: 9051, Batch Gradient Norm after: 19.8630300127562
Epoch 9052/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6666734099388123
Epoch: 9052, Batch Gradient Norm: 20.93103753323635
Epoch: 9052, Batch Gradient Norm after: 20.888091948031363
Epoch 9053/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.6688174247741699
Epoch: 9053, Batch Gradient Norm: 20.31299744886043
Epoch: 9053, Batch Gradient Norm after: 20.049151854647874
Epoch 9054/10000, Prediction Accuracy = 60.306%, Loss = 0.6669824719429016
Epoch: 9054, Batch Gradient Norm: 21.12140238872695
Epoch: 9054, Batch Gradient Norm after: 21.0023965975573
Epoch 9055/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6693455457687378
Epoch: 9055, Batch Gradient Norm: 20.531481043017784
Epoch: 9055, Batch Gradient Norm after: 20.36277179299525
Epoch 9056/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6676491856575012
Epoch: 9056, Batch Gradient Norm: 21.291820125437802
Epoch: 9056, Batch Gradient Norm after: 21.157567582792673
Epoch 9057/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6698777556419373
Epoch: 9057, Batch Gradient Norm: 20.42894999836098
Epoch: 9057, Batch Gradient Norm after: 20.27190282643847
Epoch 9058/10000, Prediction Accuracy = 60.238%, Loss = 0.6673182249069214
Epoch: 9058, Batch Gradient Norm: 21.091165377411432
Epoch: 9058, Batch Gradient Norm after: 21.01463043716522
Epoch 9059/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6692054748535157
Epoch: 9059, Batch Gradient Norm: 20.35673263452779
Epoch: 9059, Batch Gradient Norm after: 20.180930320278662
Epoch 9060/10000, Prediction Accuracy = 60.298%, Loss = 0.6670122265815734
Epoch: 9060, Batch Gradient Norm: 21.026456769717182
Epoch: 9060, Batch Gradient Norm after: 20.974361408039407
Epoch 9061/10000, Prediction Accuracy = 60.254%, Loss = 0.6689809322357178
Epoch: 9061, Batch Gradient Norm: 20.274786943205235
Epoch: 9061, Batch Gradient Norm after: 20.068970958492315
Epoch 9062/10000, Prediction Accuracy = 60.314%, Loss = 0.6668046712875366
Epoch: 9062, Batch Gradient Norm: 20.923423933347745
Epoch: 9062, Batch Gradient Norm after: 20.9104710818495
Epoch 9063/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6686915040016175
Epoch: 9063, Batch Gradient Norm: 20.173751559594294
Epoch: 9063, Batch Gradient Norm after: 19.91828892169108
Epoch 9064/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.6664422035217286
Epoch: 9064, Batch Gradient Norm: 20.855154513286987
Epoch: 9064, Batch Gradient Norm after: 20.855154513286987
Epoch 9065/10000, Prediction Accuracy = 60.262%, Loss = 0.6683596730232239
Epoch: 9065, Batch Gradient Norm: 20.170935961947812
Epoch: 9065, Batch Gradient Norm after: 19.878774827950902
Epoch 9066/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.666399335861206
Epoch: 9066, Batch Gradient Norm: 20.923060679445616
Epoch: 9066, Batch Gradient Norm after: 20.888397126855384
Epoch 9067/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6685859084129333
Epoch: 9067, Batch Gradient Norm: 20.27516590656037
Epoch: 9067, Batch Gradient Norm after: 20.01022326166452
Epoch 9068/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6667214035987854
Epoch: 9068, Batch Gradient Norm: 21.028947929151087
Epoch: 9068, Batch Gradient Norm after: 20.95441109759804
Epoch 9069/10000, Prediction Accuracy = 60.282%, Loss = 0.6688570499420166
Epoch: 9069, Batch Gradient Norm: 20.398307861075917
Epoch: 9069, Batch Gradient Norm after: 20.189127097837474
Epoch 9070/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6669395923614502
Epoch: 9070, Batch Gradient Norm: 21.14568464647111
Epoch: 9070, Batch Gradient Norm after: 21.031823078848394
Epoch 9071/10000, Prediction Accuracy = 60.288%, Loss = 0.6690986037254334
Epoch: 9071, Batch Gradient Norm: 20.49547934807373
Epoch: 9071, Batch Gradient Norm after: 20.34350486237365
Epoch 9072/10000, Prediction Accuracy = 60.298%, Loss = 0.6671684265136719
Epoch: 9072, Batch Gradient Norm: 21.199733458408822
Epoch: 9072, Batch Gradient Norm after: 21.080366965332967
Epoch 9073/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6692744135856629
Epoch: 9073, Batch Gradient Norm: 20.48247658056843
Epoch: 9073, Batch Gradient Norm after: 20.361082731524963
Epoch 9074/10000, Prediction Accuracy = 60.314%, Loss = 0.6671106100082398
Epoch: 9074, Batch Gradient Norm: 21.10271320270158
Epoch: 9074, Batch Gradient Norm after: 21.034301569229882
Epoch 9075/10000, Prediction Accuracy = 60.262%, Loss = 0.6689299702644348
Epoch: 9075, Batch Gradient Norm: 20.302542271522302
Epoch: 9075, Batch Gradient Norm after: 20.136456604234994
Epoch 9076/10000, Prediction Accuracy = 60.326%, Loss = 0.6665401220321655
Epoch: 9076, Batch Gradient Norm: 20.914880987099544
Epoch: 9076, Batch Gradient Norm after: 20.914880987099544
Epoch 9077/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.668310797214508
Epoch: 9077, Batch Gradient Norm: 20.108007514141594
Epoch: 9077, Batch Gradient Norm after: 19.847031109242838
Epoch 9078/10000, Prediction Accuracy = 60.254%, Loss = 0.6660418629646301
Epoch: 9078, Batch Gradient Norm: 20.652274846474572
Epoch: 9078, Batch Gradient Norm after: 20.652274846474572
Epoch 9079/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6675938487052917
Epoch: 9079, Batch Gradient Norm: 20.290238961774644
Epoch: 9079, Batch Gradient Norm after: 20.063305208898782
Epoch 9080/10000, Prediction Accuracy = 60.254%, Loss = 0.6665007829666137
Epoch: 9080, Batch Gradient Norm: 20.994971622356115
Epoch: 9080, Batch Gradient Norm after: 20.945138180978535
Epoch 9081/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6684623599052429
Epoch: 9081, Batch Gradient Norm: 20.287385047168172
Epoch: 9081, Batch Gradient Norm after: 20.064596880086086
Epoch 9082/10000, Prediction Accuracy = 60.314%, Loss = 0.6663804888725281
Epoch: 9082, Batch Gradient Norm: 21.0052083579117
Epoch: 9082, Batch Gradient Norm after: 20.95207597584822
Epoch 9083/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6684529781341553
Epoch: 9083, Batch Gradient Norm: 20.295509335525978
Epoch: 9083, Batch Gradient Norm after: 20.075846788859756
Epoch 9084/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6664185404777527
Epoch: 9084, Batch Gradient Norm: 20.996300625541117
Epoch: 9084, Batch Gradient Norm after: 20.950380271121222
Epoch 9085/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.668473744392395
Epoch: 9085, Batch Gradient Norm: 20.272338184370216
Epoch: 9085, Batch Gradient Norm after: 20.050386649974303
Epoch 9086/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.666291356086731
Epoch: 9086, Batch Gradient Norm: 20.969907703036235
Epoch: 9086, Batch Gradient Norm after: 20.933769858462245
Epoch 9087/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6682537794113159
Epoch: 9087, Batch Gradient Norm: 20.255090409596512
Epoch: 9087, Batch Gradient Norm after: 20.016510315168755
Epoch 9088/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.666178286075592
Epoch: 9088, Batch Gradient Norm: 20.969269803046526
Epoch: 9088, Batch Gradient Norm after: 20.92932058327561
Epoch 9089/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6682708024978637
Epoch: 9089, Batch Gradient Norm: 20.25396601683196
Epoch: 9089, Batch Gradient Norm after: 20.013676816046186
Epoch 9090/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6662320613861084
Epoch: 9090, Batch Gradient Norm: 20.966602940181186
Epoch: 9090, Batch Gradient Norm after: 20.929285206033068
Epoch 9091/10000, Prediction Accuracy = 60.3%, Loss = 0.6682407021522522
Epoch: 9091, Batch Gradient Norm: 20.246817056897626
Epoch: 9091, Batch Gradient Norm after: 20.006653368878535
Epoch 9092/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6661070108413696
Epoch: 9092, Batch Gradient Norm: 20.958381390615845
Epoch: 9092, Batch Gradient Norm after: 20.923230677892896
Epoch 9093/10000, Prediction Accuracy = 60.336%, Loss = 0.668104612827301
Epoch: 9093, Batch Gradient Norm: 20.252976795984537
Epoch: 9093, Batch Gradient Norm after: 20.008022656784167
Epoch 9094/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6660638809204101
Epoch: 9094, Batch Gradient Norm: 20.976656587506316
Epoch: 9094, Batch Gradient Norm after: 20.932375186115983
Epoch 9095/10000, Prediction Accuracy = 60.246%, Loss = 0.6681700468063354
Epoch: 9095, Batch Gradient Norm: 20.274905634134164
Epoch: 9095, Batch Gradient Norm after: 20.042166817438282
Epoch 9096/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.66612389087677
Epoch: 9096, Batch Gradient Norm: 20.985292652654785
Epoch: 9096, Batch Gradient Norm after: 20.9428558842117
Epoch 9097/10000, Prediction Accuracy = 60.226%, Loss = 0.6681545495986938
Epoch: 9097, Batch Gradient Norm: 20.275727542357313
Epoch: 9097, Batch Gradient Norm after: 20.044281164917784
Epoch 9098/10000, Prediction Accuracy = 60.322%, Loss = 0.6660289764404297
Epoch: 9098, Batch Gradient Norm: 21.00934692644394
Epoch: 9098, Batch Gradient Norm after: 20.95335929002268
Epoch 9099/10000, Prediction Accuracy = 60.314%, Loss = 0.6681172728538514
Epoch: 9099, Batch Gradient Norm: 20.317138775255433
Epoch: 9099, Batch Gradient Norm after: 20.098163073230026
Epoch 9100/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6661418080329895
Epoch: 9100, Batch Gradient Norm: 21.035816092198807
Epoch: 9100, Batch Gradient Norm after: 20.974479766123928
Epoch 9101/10000, Prediction Accuracy = 60.298%, Loss = 0.6682510852813721
Epoch: 9101, Batch Gradient Norm: 20.317979031618354
Epoch: 9101, Batch Gradient Norm after: 20.113628500579637
Epoch 9102/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.666153883934021
Epoch: 9102, Batch Gradient Norm: 20.983426471340074
Epoch: 9102, Batch Gradient Norm after: 20.950666402582744
Epoch 9103/10000, Prediction Accuracy = 60.302%, Loss = 0.6680282831192017
Epoch: 9103, Batch Gradient Norm: 20.217401785620122
Epoch: 9103, Batch Gradient Norm after: 19.984746743722724
Epoch 9104/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6657642602920533
Epoch: 9104, Batch Gradient Norm: 20.89579589750365
Epoch: 9104, Batch Gradient Norm after: 20.893133184932186
Epoch 9105/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.6677031755447388
Epoch: 9105, Batch Gradient Norm: 20.140706893056695
Epoch: 9105, Batch Gradient Norm after: 19.8635760444277
Epoch 9106/10000, Prediction Accuracy = 60.306000000000004%, Loss = 0.6655515551567077
Epoch: 9106, Batch Gradient Norm: 20.830011449851074
Epoch: 9106, Batch Gradient Norm after: 20.830011449851074
Epoch 9107/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6675375699996948
Epoch: 9107, Batch Gradient Norm: 20.173326361238942
Epoch: 9107, Batch Gradient Norm after: 19.895735692346445
Epoch 9108/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.6655949950218201
Epoch: 9108, Batch Gradient Norm: 20.913569281811412
Epoch: 9108, Batch Gradient Norm after: 20.891561544254678
Epoch 9109/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6676449775695801
Epoch: 9109, Batch Gradient Norm: 20.23837082833338
Epoch: 9109, Batch Gradient Norm after: 19.96610143693924
Epoch 9110/10000, Prediction Accuracy = 60.294%, Loss = 0.665675950050354
Epoch: 9110, Batch Gradient Norm: 21.016643773516353
Epoch: 9110, Batch Gradient Norm after: 20.949603417298594
Epoch 9111/10000, Prediction Accuracy = 60.298%, Loss = 0.6679431200027466
Epoch: 9111, Batch Gradient Norm: 20.360390251652724
Epoch: 9111, Batch Gradient Norm after: 20.142561000483642
Epoch 9112/10000, Prediction Accuracy = 60.27%, Loss = 0.6660652995109558
Epoch: 9112, Batch Gradient Norm: 21.097736781683494
Epoch: 9112, Batch Gradient Norm after: 21.01119015393311
Epoch 9113/10000, Prediction Accuracy = 60.324%, Loss = 0.6681982636451721
Epoch: 9113, Batch Gradient Norm: 20.395070021140157
Epoch: 9113, Batch Gradient Norm after: 20.222757742160233
Epoch 9114/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.6660883665084839
Epoch: 9114, Batch Gradient Norm: 21.075984849493228
Epoch: 9114, Batch Gradient Norm after: 21.010493318290802
Epoch 9115/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6680243492126465
Epoch: 9115, Batch Gradient Norm: 20.313217356435768
Epoch: 9115, Batch Gradient Norm after: 20.128714505415505
Epoch 9116/10000, Prediction Accuracy = 60.288%, Loss = 0.6657905340194702
Epoch: 9116, Batch Gradient Norm: 20.97436275761681
Epoch: 9116, Batch Gradient Norm after: 20.95143596401464
Epoch 9117/10000, Prediction Accuracy = 60.242%, Loss = 0.6677202105522155
Epoch: 9117, Batch Gradient Norm: 20.182839050486873
Epoch: 9117, Batch Gradient Norm after: 19.953837197181482
Epoch 9118/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6654348731040954
Epoch: 9118, Batch Gradient Norm: 20.752864709050044
Epoch: 9118, Batch Gradient Norm after: 20.752864709050044
Epoch 9119/10000, Prediction Accuracy = 60.232000000000006%, Loss = 0.6670608878135681
Epoch: 9119, Batch Gradient Norm: 20.23911024968688
Epoch: 9119, Batch Gradient Norm after: 20.011287285690084
Epoch 9120/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6654940843582153
Epoch: 9120, Batch Gradient Norm: 20.93110997148677
Epoch: 9120, Batch Gradient Norm after: 20.917490192118898
Epoch 9121/10000, Prediction Accuracy = 60.288%, Loss = 0.6674598813056946
Epoch: 9121, Batch Gradient Norm: 20.182219098211466
Epoch: 9121, Batch Gradient Norm after: 19.919504479464358
Epoch 9122/10000, Prediction Accuracy = 60.27%, Loss = 0.6653239369392395
Epoch: 9122, Batch Gradient Norm: 20.888676363254735
Epoch: 9122, Batch Gradient Norm after: 20.88542842693264
Epoch 9123/10000, Prediction Accuracy = 60.31199999999999%, Loss = 0.6673757791519165
Epoch: 9123, Batch Gradient Norm: 20.15255915943384
Epoch: 9123, Batch Gradient Norm after: 19.876252272191934
Epoch 9124/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.6652876496315002
Epoch: 9124, Batch Gradient Norm: 20.84883729506132
Epoch: 9124, Batch Gradient Norm after: 20.84883729506132
Epoch 9125/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6672183394432067
Epoch: 9125, Batch Gradient Norm: 20.182288749329384
Epoch: 9125, Batch Gradient Norm after: 19.905521524077862
Epoch 9126/10000, Prediction Accuracy = 60.29%, Loss = 0.6652397990226746
Epoch: 9126, Batch Gradient Norm: 20.923881100764913
Epoch: 9126, Batch Gradient Norm after: 20.900193391064924
Epoch 9127/10000, Prediction Accuracy = 60.278%, Loss = 0.6673303604125976
Epoch: 9127, Batch Gradient Norm: 20.24039694581464
Epoch: 9127, Batch Gradient Norm after: 19.97996116230891
Epoch 9128/10000, Prediction Accuracy = 60.306%, Loss = 0.6653758525848389
Epoch: 9128, Batch Gradient Norm: 20.9858697082874
Epoch: 9128, Batch Gradient Norm after: 20.939386175364326
Epoch 9129/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6675642251968383
Epoch: 9129, Batch Gradient Norm: 20.30917731799313
Epoch: 9129, Batch Gradient Norm after: 20.087895744939434
Epoch 9130/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6655414700508118
Epoch: 9130, Batch Gradient Norm: 21.02360944659114
Epoch: 9130, Batch Gradient Norm after: 20.973407268774828
Epoch 9131/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6675654053688049
Epoch: 9131, Batch Gradient Norm: 20.314270541052313
Epoch: 9131, Batch Gradient Norm after: 20.105302624187566
Epoch 9132/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6654579162597656
Epoch: 9132, Batch Gradient Norm: 21.01474708268088
Epoch: 9132, Batch Gradient Norm after: 20.969183554243113
Epoch 9133/10000, Prediction Accuracy = 60.314%, Loss = 0.6674930095672608
Epoch: 9133, Batch Gradient Norm: 20.27894281561442
Epoch: 9133, Batch Gradient Norm after: 20.06662962586191
Epoch 9134/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.665395176410675
Epoch: 9134, Batch Gradient Norm: 20.950827671970487
Epoch: 9134, Batch Gradient Norm after: 20.935408483366214
Epoch 9135/10000, Prediction Accuracy = 60.33%, Loss = 0.6673442244529724
Epoch: 9135, Batch Gradient Norm: 20.17788289646211
Epoch: 9135, Batch Gradient Norm after: 19.93864891274117
Epoch 9136/10000, Prediction Accuracy = 60.233999999999995%, Loss = 0.6650830626487731
Epoch: 9136, Batch Gradient Norm: 20.76191298407198
Epoch: 9136, Batch Gradient Norm after: 20.76191298407198
Epoch 9137/10000, Prediction Accuracy = 60.302%, Loss = 0.6667030811309814
Epoch: 9137, Batch Gradient Norm: 20.252042939826953
Epoch: 9137, Batch Gradient Norm after: 20.02599170352734
Epoch 9138/10000, Prediction Accuracy = 60.3%, Loss = 0.6651791334152222
Epoch: 9138, Batch Gradient Norm: 20.942080167655167
Epoch: 9138, Batch Gradient Norm after: 20.92540373952704
Epoch 9139/10000, Prediction Accuracy = 60.25%, Loss = 0.6671878337860108
Epoch: 9139, Batch Gradient Norm: 20.20259594234147
Epoch: 9139, Batch Gradient Norm after: 19.961624223807554
Epoch 9140/10000, Prediction Accuracy = 60.298%, Loss = 0.6650498867034912
Epoch: 9140, Batch Gradient Norm: 20.85993423262228
Epoch: 9140, Batch Gradient Norm after: 20.85993423262228
Epoch 9141/10000, Prediction Accuracy = 60.25200000000001%, Loss = 0.6669492840766906
Epoch: 9141, Batch Gradient Norm: 20.187239418761315
Epoch: 9141, Batch Gradient Norm after: 19.938597880168253
Epoch 9142/10000, Prediction Accuracy = 60.33%, Loss = 0.6649332523345948
Epoch: 9142, Batch Gradient Norm: 20.86705713877165
Epoch: 9142, Batch Gradient Norm after: 20.86705713877165
Epoch 9143/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.666847562789917
Epoch: 9143, Batch Gradient Norm: 20.19032429982228
Epoch: 9143, Batch Gradient Norm after: 19.926113524054397
Epoch 9144/10000, Prediction Accuracy = 60.266%, Loss = 0.6648975014686584
Epoch: 9144, Batch Gradient Norm: 20.906112992777317
Epoch: 9144, Batch Gradient Norm after: 20.896860537953298
Epoch 9145/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.666978657245636
Epoch: 9145, Batch Gradient Norm: 20.189534001193206
Epoch: 9145, Batch Gradient Norm after: 19.92453531833688
Epoch 9146/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.664944076538086
Epoch: 9146, Batch Gradient Norm: 20.886466140856275
Epoch: 9146, Batch Gradient Norm after: 20.886466140856275
Epoch 9147/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6669198989868164
Epoch: 9147, Batch Gradient Norm: 20.163875579243143
Epoch: 9147, Batch Gradient Norm after: 19.891659879263447
Epoch 9148/10000, Prediction Accuracy = 60.274%, Loss = 0.6647701025009155
Epoch: 9148, Batch Gradient Norm: 20.877023992966038
Epoch: 9148, Batch Gradient Norm after: 20.877023992966038
Epoch 9149/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6667659282684326
Epoch: 9149, Batch Gradient Norm: 20.18394491594294
Epoch: 9149, Batch Gradient Norm after: 19.912141228273207
Epoch 9150/10000, Prediction Accuracy = 60.31%, Loss = 0.6647741794586182
Epoch: 9150, Batch Gradient Norm: 20.91827673271669
Epoch: 9150, Batch Gradient Norm after: 20.900961808892546
Epoch 9151/10000, Prediction Accuracy = 60.27%, Loss = 0.6669134974479676
Epoch: 9151, Batch Gradient Norm: 20.235724763928147
Epoch: 9151, Batch Gradient Norm after: 19.98778325443177
Epoch 9152/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.66490079164505
Epoch: 9152, Batch Gradient Norm: 20.962824599160168
Epoch: 9152, Batch Gradient Norm after: 20.932626735712933
Epoch 9153/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.6669482588768005
Epoch: 9153, Batch Gradient Norm: 20.26637549432008
Epoch: 9153, Batch Gradient Norm after: 20.038249144014085
Epoch 9154/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6648889541625976
Epoch: 9154, Batch Gradient Norm: 20.977843387785192
Epoch: 9154, Batch Gradient Norm after: 20.945707743755133
Epoch 9155/10000, Prediction Accuracy = 60.314%, Loss = 0.6669426441192627
Epoch: 9155, Batch Gradient Norm: 20.25650709540855
Epoch: 9155, Batch Gradient Norm after: 20.032524607743625
Epoch 9156/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6648891925811767
Epoch: 9156, Batch Gradient Norm: 20.946583713517263
Epoch: 9156, Batch Gradient Norm after: 20.932563811288404
Epoch 9157/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6669036388397217
Epoch: 9157, Batch Gradient Norm: 20.183624216938576
Epoch: 9157, Batch Gradient Norm after: 19.94602604956853
Epoch 9158/10000, Prediction Accuracy = 60.262%, Loss = 0.6646865963935852
Epoch: 9158, Batch Gradient Norm: 20.77349471868076
Epoch: 9158, Batch Gradient Norm after: 20.77349471868076
Epoch 9159/10000, Prediction Accuracy = 60.322%, Loss = 0.6663201451301575
Epoch: 9159, Batch Gradient Norm: 20.253262333322713
Epoch: 9159, Batch Gradient Norm after: 20.032747781638808
Epoch 9160/10000, Prediction Accuracy = 60.31%, Loss = 0.6647462964057922
Epoch: 9160, Batch Gradient Norm: 20.93946221814616
Epoch: 9160, Batch Gradient Norm after: 20.928790000301987
Epoch 9161/10000, Prediction Accuracy = 60.238000000000014%, Loss = 0.6667403101921081
Epoch: 9161, Batch Gradient Norm: 20.180197311514817
Epoch: 9161, Batch Gradient Norm after: 19.941820789847856
Epoch 9162/10000, Prediction Accuracy = 60.314%, Loss = 0.6645729899406433
Epoch: 9162, Batch Gradient Norm: 20.782523370881133
Epoch: 9162, Batch Gradient Norm after: 20.782523370881133
Epoch 9163/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.6663437128067017
Epoch: 9163, Batch Gradient Norm: 20.256029859653587
Epoch: 9163, Batch Gradient Norm after: 20.03833145424643
Epoch 9164/10000, Prediction Accuracy = 60.336%, Loss = 0.6646981358528137
Epoch: 9164, Batch Gradient Norm: 20.940640495029108
Epoch: 9164, Batch Gradient Norm after: 20.931540700783298
Epoch 9165/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.6666294693946838
Epoch: 9165, Batch Gradient Norm: 20.18735843779056
Epoch: 9165, Batch Gradient Norm after: 19.947052385694338
Epoch 9166/10000, Prediction Accuracy = 60.278%, Loss = 0.6644476532936097
Epoch: 9166, Batch Gradient Norm: 20.825144054060583
Epoch: 9166, Batch Gradient Norm after: 20.825144054060583
Epoch 9167/10000, Prediction Accuracy = 60.31%, Loss = 0.6663153767585754
Epoch: 9167, Batch Gradient Norm: 20.2149987481945
Epoch: 9167, Batch Gradient Norm after: 19.979615165759764
Epoch 9168/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6645767688751221
Epoch: 9168, Batch Gradient Norm: 20.857114015662848
Epoch: 9168, Batch Gradient Norm after: 20.857114015662848
Epoch 9169/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6664056301116943
Epoch: 9169, Batch Gradient Norm: 20.18702841253365
Epoch: 9169, Batch Gradient Norm after: 19.952889141985757
Epoch 9170/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6644166588783265
Epoch: 9170, Batch Gradient Norm: 20.789652637875413
Epoch: 9170, Batch Gradient Norm after: 20.789652637875413
Epoch 9171/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.666099739074707
Epoch: 9171, Batch Gradient Norm: 20.240933904014963
Epoch: 9171, Batch Gradient Norm after: 20.020477034166536
Epoch 9172/10000, Prediction Accuracy = 60.3%, Loss = 0.6644893169403077
Epoch: 9172, Batch Gradient Norm: 20.92865073162228
Epoch: 9172, Batch Gradient Norm after: 20.923081809217713
Epoch 9173/10000, Prediction Accuracy = 60.258%, Loss = 0.6664903998374939
Epoch: 9173, Batch Gradient Norm: 20.15684024788535
Epoch: 9173, Batch Gradient Norm after: 19.9160315938017
Epoch 9174/10000, Prediction Accuracy = 60.338%, Loss = 0.6642569422721862
Epoch: 9174, Batch Gradient Norm: 20.711471607723656
Epoch: 9174, Batch Gradient Norm after: 20.711471607723656
Epoch 9175/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6658488035202026
Epoch: 9175, Batch Gradient Norm: 20.316546871162128
Epoch: 9175, Batch Gradient Norm after: 20.121012317991184
Epoch 9176/10000, Prediction Accuracy = 60.354%, Loss = 0.6645935535430908
Epoch: 9176, Batch Gradient Norm: 21.008849897824533
Epoch: 9176, Batch Gradient Norm after: 20.973609665956758
Epoch 9177/10000, Prediction Accuracy = 60.294%, Loss = 0.6665785431861877
Epoch: 9177, Batch Gradient Norm: 20.24951226513731
Epoch: 9177, Batch Gradient Norm after: 20.047020206313924
Epoch 9178/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6644135594367981
Epoch: 9178, Batch Gradient Norm: 20.856634466949902
Epoch: 9178, Batch Gradient Norm after: 20.856634466949902
Epoch 9179/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6662138819694519
Epoch: 9179, Batch Gradient Norm: 20.188164018172728
Epoch: 9179, Batch Gradient Norm after: 19.968672241671868
Epoch 9180/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6642718434333801
Epoch: 9180, Batch Gradient Norm: 20.68538091133626
Epoch: 9180, Batch Gradient Norm after: 20.68538091133626
Epoch 9181/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6656605839729309
Epoch: 9181, Batch Gradient Norm: 20.31542792021383
Epoch: 9181, Batch Gradient Norm after: 20.139624999438308
Epoch 9182/10000, Prediction Accuracy = 60.298%, Loss = 0.6644903063774109
Epoch: 9182, Batch Gradient Norm: 20.964366103838454
Epoch: 9182, Batch Gradient Norm after: 20.957099770458104
Epoch 9183/10000, Prediction Accuracy = 60.286%, Loss = 0.6663630843162537
Epoch: 9183, Batch Gradient Norm: 20.148518689871665
Epoch: 9183, Batch Gradient Norm after: 19.924915400330192
Epoch 9184/10000, Prediction Accuracy = 60.324%, Loss = 0.6640301704406738
Epoch: 9184, Batch Gradient Norm: 20.605683117490983
Epoch: 9184, Batch Gradient Norm after: 20.605683117490983
Epoch 9185/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6653943181037902
Epoch: 9185, Batch Gradient Norm: 20.374174605511577
Epoch: 9185, Batch Gradient Norm after: 20.22188713124927
Epoch 9186/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6646065473556518
Epoch: 9186, Batch Gradient Norm: 21.009803344380423
Epoch: 9186, Batch Gradient Norm after: 20.989967112819556
Epoch 9187/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6664166331291199
Epoch: 9187, Batch Gradient Norm: 20.182919639796722
Epoch: 9187, Batch Gradient Norm after: 19.983314804840298
Epoch 9188/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.663995110988617
Epoch: 9188, Batch Gradient Norm: 20.64594277301785
Epoch: 9188, Batch Gradient Norm after: 20.64594277301785
Epoch 9189/10000, Prediction Accuracy = 60.312%, Loss = 0.6653590798377991
Epoch: 9189, Batch Gradient Norm: 20.331543502411336
Epoch: 9189, Batch Gradient Norm after: 20.172939314126726
Epoch 9190/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6644593358039856
Epoch: 9190, Batch Gradient Norm: 20.92950846379134
Epoch: 9190, Batch Gradient Norm after: 20.92950846379134
Epoch 9191/10000, Prediction Accuracy = 60.34000000000001%, Loss = 0.6661920070648193
Epoch: 9191, Batch Gradient Norm: 20.11681043909522
Epoch: 9191, Batch Gradient Norm after: 19.90405599118246
Epoch 9192/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.6638201117515564
Epoch: 9192, Batch Gradient Norm: 20.461878772418956
Epoch: 9192, Batch Gradient Norm after: 20.461878772418956
Epoch 9193/10000, Prediction Accuracy = 60.33%, Loss = 0.6647606730461121
Epoch: 9193, Batch Gradient Norm: 20.464309801397704
Epoch: 9193, Batch Gradient Norm after: 20.351726653143707
Epoch 9194/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6646622538566589
Epoch: 9194, Batch Gradient Norm: 21.088061378347238
Epoch: 9194, Batch Gradient Norm after: 21.043485889475058
Epoch 9195/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.6665203452110291
Epoch: 9195, Batch Gradient Norm: 20.234205049525404
Epoch: 9195, Batch Gradient Norm after: 20.077484225755445
Epoch 9196/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.664045512676239
Epoch: 9196, Batch Gradient Norm: 20.578003424283516
Epoch: 9196, Batch Gradient Norm after: 20.578003424283516
Epoch 9197/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.6650673866271972
Epoch: 9197, Batch Gradient Norm: 20.383974603761377
Epoch: 9197, Batch Gradient Norm after: 20.273643833105297
Epoch 9198/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.66436607837677
Epoch: 9198, Batch Gradient Norm: 20.89675672032861
Epoch: 9198, Batch Gradient Norm after: 20.89675672032861
Epoch 9199/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6658403396606445
Epoch: 9199, Batch Gradient Norm: 20.15030584538359
Epoch: 9199, Batch Gradient Norm after: 19.967947112700102
Epoch 9200/10000, Prediction Accuracy = 60.302%, Loss = 0.6636972665786743
Epoch: 9200, Batch Gradient Norm: 20.460084001603267
Epoch: 9200, Batch Gradient Norm after: 20.460084001603267
Epoch 9201/10000, Prediction Accuracy = 60.36%, Loss = 0.6646262884140015
Epoch: 9201, Batch Gradient Norm: 20.440133587401135
Epoch: 9201, Batch Gradient Norm after: 20.342314844024273
Epoch 9202/10000, Prediction Accuracy = 60.286%, Loss = 0.6645380616188049
Epoch: 9202, Batch Gradient Norm: 20.997686699069796
Epoch: 9202, Batch Gradient Norm after: 20.997686699069796
Epoch 9203/10000, Prediction Accuracy = 60.378%, Loss = 0.6661187052726746
Epoch: 9203, Batch Gradient Norm: 20.073942703016677
Epoch: 9203, Batch Gradient Norm after: 19.87703232329823
Epoch 9204/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6634148120880127
Epoch: 9204, Batch Gradient Norm: 20.282699509900766
Epoch: 9204, Batch Gradient Norm after: 20.282699509900766
Epoch 9205/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6639968633651734
Epoch: 9205, Batch Gradient Norm: 20.575959418394927
Epoch: 9205, Batch Gradient Norm after: 20.517747822208428
Epoch 9206/10000, Prediction Accuracy = 60.33%, Loss = 0.6647498369216919
Epoch: 9206, Batch Gradient Norm: 21.137697156600545
Epoch: 9206, Batch Gradient Norm after: 21.090785608635407
Epoch 9207/10000, Prediction Accuracy = 60.3%, Loss = 0.6664645314216614
Epoch: 9207, Batch Gradient Norm: 20.19677804932521
Epoch: 9207, Batch Gradient Norm after: 20.064620026228884
Epoch 9208/10000, Prediction Accuracy = 60.326%, Loss = 0.6636958956718445
Epoch: 9208, Batch Gradient Norm: 20.39386093232754
Epoch: 9208, Batch Gradient Norm after: 20.39386093232754
Epoch 9209/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6642406105995178
Epoch: 9209, Batch Gradient Norm: 20.493457775977564
Epoch: 9209, Batch Gradient Norm after: 20.43776557640107
Epoch 9210/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6644105076789856
Epoch: 9210, Batch Gradient Norm: 20.997364529668392
Epoch: 9210, Batch Gradient Norm after: 20.997364529668392
Epoch 9211/10000, Prediction Accuracy = 60.326%, Loss = 0.6659082174301147
Epoch: 9211, Batch Gradient Norm: 20.0681266840742
Epoch: 9211, Batch Gradient Norm after: 19.89224425014632
Epoch 9212/10000, Prediction Accuracy = 60.302%, Loss = 0.6632811307907105
Epoch: 9212, Batch Gradient Norm: 20.172823099021933
Epoch: 9212, Batch Gradient Norm after: 20.172823099021933
Epoch 9213/10000, Prediction Accuracy = 60.352%, Loss = 0.6636210680007935
Epoch: 9213, Batch Gradient Norm: 20.623047562328857
Epoch: 9213, Batch Gradient Norm after: 20.607637322751422
Epoch 9214/10000, Prediction Accuracy = 60.29%, Loss = 0.6648064613342285
Epoch: 9214, Batch Gradient Norm: 21.130249761045775
Epoch: 9214, Batch Gradient Norm after: 21.097724910323965
Epoch 9215/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6662415504455567
Epoch: 9215, Batch Gradient Norm: 20.138111607359146
Epoch: 9215, Batch Gradient Norm after: 20.003322076230955
Epoch 9216/10000, Prediction Accuracy = 60.334%, Loss = 0.6633311748504639
Epoch: 9216, Batch Gradient Norm: 20.228998761504815
Epoch: 9216, Batch Gradient Norm after: 20.228998761504815
Epoch 9217/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6636278986930847
Epoch: 9217, Batch Gradient Norm: 20.585024652285075
Epoch: 9217, Batch Gradient Norm after: 20.573579823649265
Epoch 9218/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6645808577537536
Epoch: 9218, Batch Gradient Norm: 21.02916203890227
Epoch: 9218, Batch Gradient Norm after: 21.02916203890227
Epoch 9219/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.6659569859504699
Epoch: 9219, Batch Gradient Norm: 20.042709729669124
Epoch: 9219, Batch Gradient Norm after: 19.887749108579083
Epoch 9220/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6630239009857177
Epoch: 9220, Batch Gradient Norm: 20.027139840386152
Epoch: 9220, Batch Gradient Norm after: 20.027139840386152
Epoch 9221/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.662954843044281
Epoch: 9221, Batch Gradient Norm: 20.536798036969376
Epoch: 9221, Batch Gradient Norm after: 20.536798036969376
Epoch 9222/10000, Prediction Accuracy = 60.318%, Loss = 0.6643124341964721
Epoch: 9222, Batch Gradient Norm: 21.112003015836596
Epoch: 9222, Batch Gradient Norm after: 21.08465832660564
Epoch 9223/10000, Prediction Accuracy = 60.338%, Loss = 0.6660533547401428
Epoch: 9223, Batch Gradient Norm: 20.11151907924385
Epoch: 9223, Batch Gradient Norm after: 19.966252913648486
Epoch 9224/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.6631951689720154
Epoch: 9224, Batch Gradient Norm: 20.176735318377883
Epoch: 9224, Batch Gradient Norm after: 20.176735318377883
Epoch 9225/10000, Prediction Accuracy = 60.36%, Loss = 0.663377332687378
Epoch: 9225, Batch Gradient Norm: 20.60002061973275
Epoch: 9225, Batch Gradient Norm after: 20.60002061973275
Epoch 9226/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6644526481628418
Epoch: 9226, Batch Gradient Norm: 21.069781123218515
Epoch: 9226, Batch Gradient Norm after: 21.066570082824455
Epoch 9227/10000, Prediction Accuracy = 60.314%, Loss = 0.6658072471618652
Epoch: 9227, Batch Gradient Norm: 20.0363100565753
Epoch: 9227, Batch Gradient Norm after: 19.873930368650292
Epoch 9228/10000, Prediction Accuracy = 60.33%, Loss = 0.6628282785415649
Epoch: 9228, Batch Gradient Norm: 20.02875938621376
Epoch: 9228, Batch Gradient Norm after: 20.02875938621376
Epoch 9229/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.6628488063812256
Epoch: 9229, Batch Gradient Norm: 20.492784459858505
Epoch: 9229, Batch Gradient Norm after: 20.492784459858505
Epoch 9230/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6640803217887878
Epoch: 9230, Batch Gradient Norm: 21.039972670464667
Epoch: 9230, Batch Gradient Norm after: 21.039972670464667
Epoch 9231/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.6656818747520447
Epoch: 9231, Batch Gradient Norm: 20.04518870578868
Epoch: 9231, Batch Gradient Norm after: 19.873559003816503
Epoch 9232/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6627512454986573
Epoch: 9232, Batch Gradient Norm: 20.098736670742863
Epoch: 9232, Batch Gradient Norm after: 20.098736670742863
Epoch 9233/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6629239320755005
Epoch: 9233, Batch Gradient Norm: 20.64922332687622
Epoch: 9233, Batch Gradient Norm after: 20.64922332687622
Epoch 9234/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6644450783729553
Epoch: 9234, Batch Gradient Norm: 21.149765220396766
Epoch: 9234, Batch Gradient Norm after: 21.115152938441067
Epoch 9235/10000, Prediction Accuracy = 60.342%, Loss = 0.6659845352172852
Epoch: 9235, Batch Gradient Norm: 20.11784759684351
Epoch: 9235, Batch Gradient Norm after: 19.993128102815255
Epoch 9236/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.6629727602005004
Epoch: 9236, Batch Gradient Norm: 20.120359917328496
Epoch: 9236, Batch Gradient Norm after: 20.120359917328496
Epoch 9237/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6629441380500793
Epoch: 9237, Batch Gradient Norm: 20.48553794999033
Epoch: 9237, Batch Gradient Norm after: 20.48553794999033
Epoch 9238/10000, Prediction Accuracy = 60.342%, Loss = 0.6638580441474915
Epoch: 9238, Batch Gradient Norm: 20.966157200667425
Epoch: 9238, Batch Gradient Norm after: 20.966157200667425
Epoch 9239/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.6652760624885559
Epoch: 9239, Batch Gradient Norm: 20.08106036152096
Epoch: 9239, Batch Gradient Norm after: 19.929889465454
Epoch 9240/10000, Prediction Accuracy = 60.338%, Loss = 0.6627350449562073
Epoch: 9240, Batch Gradient Norm: 20.101549587518907
Epoch: 9240, Batch Gradient Norm after: 20.101549587518907
Epoch 9241/10000, Prediction Accuracy = 60.29%, Loss = 0.6628551125526428
Epoch: 9241, Batch Gradient Norm: 20.518473732412954
Epoch: 9241, Batch Gradient Norm after: 20.518473732412954
Epoch 9242/10000, Prediction Accuracy = 60.324%, Loss = 0.6639137387275695
Epoch: 9242, Batch Gradient Norm: 21.014200909122096
Epoch: 9242, Batch Gradient Norm after: 21.014200909122096
Epoch 9243/10000, Prediction Accuracy = 60.288%, Loss = 0.6653263568878174
Epoch: 9243, Batch Gradient Norm: 20.05951436716936
Epoch: 9243, Batch Gradient Norm after: 19.895180698266014
Epoch 9244/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6625498056411743
Epoch: 9244, Batch Gradient Norm: 20.10620468620536
Epoch: 9244, Batch Gradient Norm after: 20.10620468620536
Epoch 9245/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6627328276634217
Epoch: 9245, Batch Gradient Norm: 20.59129752876319
Epoch: 9245, Batch Gradient Norm after: 20.59129752876319
Epoch 9246/10000, Prediction Accuracy = 60.294%, Loss = 0.6641028881072998
Epoch: 9246, Batch Gradient Norm: 21.08074250278119
Epoch: 9246, Batch Gradient Norm after: 21.074946719147352
Epoch 9247/10000, Prediction Accuracy = 60.35%, Loss = 0.6655483603477478
Epoch: 9247, Batch Gradient Norm: 20.030270351644507
Epoch: 9247, Batch Gradient Norm after: 19.873222510446194
Epoch 9248/10000, Prediction Accuracy = 60.282%, Loss = 0.6624531865119934
Epoch: 9248, Batch Gradient Norm: 19.98220285441266
Epoch: 9248, Batch Gradient Norm after: 19.98220285441266
Epoch 9249/10000, Prediction Accuracy = 60.324%, Loss = 0.6622996091842651
Epoch: 9249, Batch Gradient Norm: 20.430545206659346
Epoch: 9249, Batch Gradient Norm after: 20.430545206659346
Epoch 9250/10000, Prediction Accuracy = 60.324%, Loss = 0.6634670376777649
Epoch: 9250, Batch Gradient Norm: 21.019787645345158
Epoch: 9250, Batch Gradient Norm after: 21.019787645345158
Epoch 9251/10000, Prediction Accuracy = 60.3%, Loss = 0.6652293562889099
Epoch: 9251, Batch Gradient Norm: 20.04250498739847
Epoch: 9251, Batch Gradient Norm after: 19.86660318689973
Epoch 9252/10000, Prediction Accuracy = 60.314%, Loss = 0.6624000906944275
Epoch: 9252, Batch Gradient Norm: 20.07371524914384
Epoch: 9252, Batch Gradient Norm after: 20.07371524914384
Epoch 9253/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6624949932098388
Epoch: 9253, Batch Gradient Norm: 20.58799204428404
Epoch: 9253, Batch Gradient Norm after: 20.58799204428404
Epoch 9254/10000, Prediction Accuracy = 60.384%, Loss = 0.6638118505477906
Epoch: 9254, Batch Gradient Norm: 21.121530199576412
Epoch: 9254, Batch Gradient Norm after: 21.096132272698352
Epoch 9255/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6654041886329651
Epoch: 9255, Batch Gradient Norm: 20.095270089961534
Epoch: 9255, Batch Gradient Norm after: 19.94987145357978
Epoch 9256/10000, Prediction Accuracy = 60.35%, Loss = 0.6624497056007386
Epoch: 9256, Batch Gradient Norm: 20.152589637913078
Epoch: 9256, Batch Gradient Norm after: 20.152589637913078
Epoch 9257/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.6626593708992005
Epoch: 9257, Batch Gradient Norm: 20.544879667130353
Epoch: 9257, Batch Gradient Norm after: 20.544879667130353
Epoch 9258/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6637271404266357
Epoch: 9258, Batch Gradient Norm: 20.974978825461765
Epoch: 9258, Batch Gradient Norm after: 20.974978825461765
Epoch 9259/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6649508833885193
Epoch: 9259, Batch Gradient Norm: 20.068056839771646
Epoch: 9259, Batch Gradient Norm after: 19.925753749756755
Epoch 9260/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6622776746749878
Epoch: 9260, Batch Gradient Norm: 20.050247368877915
Epoch: 9260, Batch Gradient Norm after: 20.050247368877915
Epoch 9261/10000, Prediction Accuracy = 60.3%, Loss = 0.6622468709945679
Epoch: 9261, Batch Gradient Norm: 20.450923978445477
Epoch: 9261, Batch Gradient Norm after: 20.450923978445477
Epoch 9262/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6633199453353882
Epoch: 9262, Batch Gradient Norm: 20.943870863254354
Epoch: 9262, Batch Gradient Norm after: 20.943870863254354
Epoch 9263/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.6648330926895142
Epoch: 9263, Batch Gradient Norm: 20.09195583300828
Epoch: 9263, Batch Gradient Norm after: 19.94870074827816
Epoch 9264/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6622978091239929
Epoch: 9264, Batch Gradient Norm: 20.100824703181043
Epoch: 9264, Batch Gradient Norm after: 20.100824703181043
Epoch 9265/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6623103141784668
Epoch: 9265, Batch Gradient Norm: 20.524236403046455
Epoch: 9265, Batch Gradient Norm after: 20.524236403046455
Epoch 9266/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6633934736251831
Epoch: 9266, Batch Gradient Norm: 21.050222976328953
Epoch: 9266, Batch Gradient Norm after: 21.050222976328953
Epoch 9267/10000, Prediction Accuracy = 60.334%, Loss = 0.6649904489517212
Epoch: 9267, Batch Gradient Norm: 20.018705005979776
Epoch: 9267, Batch Gradient Norm after: 19.84728126949166
Epoch 9268/10000, Prediction Accuracy = 60.312%, Loss = 0.6620536565780639
Epoch: 9268, Batch Gradient Norm: 19.990856746903955
Epoch: 9268, Batch Gradient Norm after: 19.990856746903955
Epoch 9269/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6619931221008301
Epoch: 9269, Batch Gradient Norm: 20.421875579982363
Epoch: 9269, Batch Gradient Norm after: 20.421875579982363
Epoch 9270/10000, Prediction Accuracy = 60.312%, Loss = 0.6630973219871521
Epoch: 9270, Batch Gradient Norm: 20.975228420309417
Epoch: 9270, Batch Gradient Norm after: 20.975228420309417
Epoch 9271/10000, Prediction Accuracy = 60.338%, Loss = 0.6646754741668701
Epoch: 9271, Batch Gradient Norm: 20.067938918537262
Epoch: 9271, Batch Gradient Norm after: 19.904614915902858
Epoch 9272/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6620240330696106
Epoch: 9272, Batch Gradient Norm: 20.142170838170223
Epoch: 9272, Batch Gradient Norm after: 20.142170838170223
Epoch 9273/10000, Prediction Accuracy = 60.3%, Loss = 0.6622883319854737
Epoch: 9273, Batch Gradient Norm: 20.590089922087767
Epoch: 9273, Batch Gradient Norm after: 20.590089922087767
Epoch 9274/10000, Prediction Accuracy = 60.336%, Loss = 0.6635013222694397
Epoch: 9274, Batch Gradient Norm: 21.050578302859538
Epoch: 9274, Batch Gradient Norm after: 21.050578302859538
Epoch 9275/10000, Prediction Accuracy = 60.28800000000001%, Loss = 0.6648787260055542
Epoch: 9275, Batch Gradient Norm: 20.023527489590954
Epoch: 9275, Batch Gradient Norm after: 19.86609042538048
Epoch 9276/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6618250727653503
Epoch: 9276, Batch Gradient Norm: 19.980014591537397
Epoch: 9276, Batch Gradient Norm after: 19.980014591537397
Epoch 9277/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.6617172718048095
Epoch: 9277, Batch Gradient Norm: 20.412224008657805
Epoch: 9277, Batch Gradient Norm after: 20.412224008657805
Epoch 9278/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6628716349601745
Epoch: 9278, Batch Gradient Norm: 21.002873458854936
Epoch: 9278, Batch Gradient Norm after: 21.002873458854936
Epoch 9279/10000, Prediction Accuracy = 60.338%, Loss = 0.6646574974060059
Epoch: 9279, Batch Gradient Norm: 20.04469037435494
Epoch: 9279, Batch Gradient Norm after: 19.878311711456703
Epoch 9280/10000, Prediction Accuracy = 60.302%, Loss = 0.6618938446044922
Epoch: 9280, Batch Gradient Norm: 20.06573405562555
Epoch: 9280, Batch Gradient Norm after: 20.06573405562555
Epoch 9281/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6619303584098816
Epoch: 9281, Batch Gradient Norm: 20.49280403979754
Epoch: 9281, Batch Gradient Norm after: 20.49280403979754
Epoch 9282/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6630237936973572
Epoch: 9282, Batch Gradient Norm: 21.015449908624586
Epoch: 9282, Batch Gradient Norm after: 21.015449908624586
Epoch 9283/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6645581126213074
Epoch: 9283, Batch Gradient Norm: 20.0522622097523
Epoch: 9283, Batch Gradient Norm after: 19.891028401058385
Epoch 9284/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.661759889125824
Epoch: 9284, Batch Gradient Norm: 20.061784408151464
Epoch: 9284, Batch Gradient Norm after: 20.061784408151464
Epoch 9285/10000, Prediction Accuracy = 60.288%, Loss = 0.6618644714355468
Epoch: 9285, Batch Gradient Norm: 20.45035293755241
Epoch: 9285, Batch Gradient Norm after: 20.45035293755241
Epoch 9286/10000, Prediction Accuracy = 60.338%, Loss = 0.6628587722778321
Epoch: 9286, Batch Gradient Norm: 20.94231709705389
Epoch: 9286, Batch Gradient Norm after: 20.94231709705389
Epoch 9287/10000, Prediction Accuracy = 60.294%, Loss = 0.664272940158844
Epoch: 9287, Batch Gradient Norm: 20.104401555397303
Epoch: 9287, Batch Gradient Norm after: 19.96148575175126
Epoch 9288/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6617893576622009
Epoch: 9288, Batch Gradient Norm: 20.157026223166106
Epoch: 9288, Batch Gradient Norm after: 20.157026223166106
Epoch 9289/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6619977712631225
Epoch: 9289, Batch Gradient Norm: 20.526825343389174
Epoch: 9289, Batch Gradient Norm after: 20.526825343389174
Epoch 9290/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6630265355110169
Epoch: 9290, Batch Gradient Norm: 20.952892454399603
Epoch: 9290, Batch Gradient Norm after: 20.952892454399603
Epoch 9291/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6643178224563598
Epoch: 9291, Batch Gradient Norm: 20.08839085831964
Epoch: 9291, Batch Gradient Norm after: 19.96128184978161
Epoch 9292/10000, Prediction Accuracy = 60.326%, Loss = 0.6617576599121093
Epoch: 9292, Batch Gradient Norm: 20.008001735256364
Epoch: 9292, Batch Gradient Norm after: 20.008001735256364
Epoch 9293/10000, Prediction Accuracy = 60.352%, Loss = 0.661527407169342
Epoch: 9293, Batch Gradient Norm: 20.300305658426165
Epoch: 9293, Batch Gradient Norm after: 20.300305658426165
Epoch 9294/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6622155904769897
Epoch: 9294, Batch Gradient Norm: 20.80979584883268
Epoch: 9294, Batch Gradient Norm after: 20.80979584883268
Epoch 9295/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6637328267097473
Epoch: 9295, Batch Gradient Norm: 20.169311426791744
Epoch: 9295, Batch Gradient Norm after: 20.047760985701796
Epoch 9296/10000, Prediction Accuracy = 60.338%, Loss = 0.6618799805641175
Epoch: 9296, Batch Gradient Norm: 20.26238506462923
Epoch: 9296, Batch Gradient Norm after: 20.26238506462923
Epoch 9297/10000, Prediction Accuracy = 60.288%, Loss = 0.6621926069259644
Epoch: 9297, Batch Gradient Norm: 20.53711294582991
Epoch: 9297, Batch Gradient Norm after: 20.53711294582991
Epoch 9298/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.662829327583313
Epoch: 9298, Batch Gradient Norm: 20.88709721743359
Epoch: 9298, Batch Gradient Norm after: 20.88709721743359
Epoch 9299/10000, Prediction Accuracy = 60.3%, Loss = 0.6638571619987488
Epoch: 9299, Batch Gradient Norm: 20.12815123998298
Epoch: 9299, Batch Gradient Norm after: 20.016217527426328
Epoch 9300/10000, Prediction Accuracy = 60.366%, Loss = 0.6616415858268738
Epoch: 9300, Batch Gradient Norm: 20.104281565639276
Epoch: 9300, Batch Gradient Norm after: 20.104281565639276
Epoch 9301/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6616515874862671
Epoch: 9301, Batch Gradient Norm: 20.335602051307283
Epoch: 9301, Batch Gradient Norm after: 20.335602051307283
Epoch 9302/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6622813940048218
Epoch: 9302, Batch Gradient Norm: 20.691857222396184
Epoch: 9302, Batch Gradient Norm after: 20.691857222396184
Epoch 9303/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6633058428764343
Epoch: 9303, Batch Gradient Norm: 20.248726347021677
Epoch: 9303, Batch Gradient Norm after: 20.17681046255135
Epoch 9304/10000, Prediction Accuracy = 60.334%, Loss = 0.6619392037391663
Epoch: 9304, Batch Gradient Norm: 20.316730705490905
Epoch: 9304, Batch Gradient Norm after: 20.316730705490905
Epoch 9305/10000, Prediction Accuracy = 60.336%, Loss = 0.6621246814727784
Epoch: 9305, Batch Gradient Norm: 20.475968541590554
Epoch: 9305, Batch Gradient Norm after: 20.475968541590554
Epoch 9306/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6625020980834961
Epoch: 9306, Batch Gradient Norm: 20.721645136405673
Epoch: 9306, Batch Gradient Norm after: 20.721645136405673
Epoch 9307/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.663309121131897
Epoch: 9307, Batch Gradient Norm: 20.217133561293384
Epoch: 9307, Batch Gradient Norm after: 20.155294382185915
Epoch 9308/10000, Prediction Accuracy = 60.336%, Loss = 0.6617947459220886
Epoch: 9308, Batch Gradient Norm: 20.136531262036765
Epoch: 9308, Batch Gradient Norm after: 20.136531262036765
Epoch 9309/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.661593222618103
Epoch: 9309, Batch Gradient Norm: 20.198309168064533
Epoch: 9309, Batch Gradient Norm after: 20.198309168064533
Epoch 9310/10000, Prediction Accuracy = 60.386%, Loss = 0.6616055250167847
Epoch: 9310, Batch Gradient Norm: 20.52857696697851
Epoch: 9310, Batch Gradient Norm after: 20.52857696697851
Epoch 9311/10000, Prediction Accuracy = 60.29%, Loss = 0.6625889778137207
Epoch: 9311, Batch Gradient Norm: 20.357248402694566
Epoch: 9311, Batch Gradient Norm after: 20.311652476396812
Epoch 9312/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6620906829833985
Epoch: 9312, Batch Gradient Norm: 20.504327685132605
Epoch: 9312, Batch Gradient Norm after: 20.504327685132605
Epoch 9313/10000, Prediction Accuracy = 60.36199999999999%, Loss = 0.662573516368866
Epoch: 9313, Batch Gradient Norm: 20.364530429153856
Epoch: 9313, Batch Gradient Norm after: 20.34268700517946
Epoch 9314/10000, Prediction Accuracy = 60.32199999999999%, Loss = 0.6621098041534423
Epoch: 9314, Batch Gradient Norm: 20.419562527834326
Epoch: 9314, Batch Gradient Norm after: 20.419562527834326
Epoch 9315/10000, Prediction Accuracy = 60.35%, Loss = 0.6622525811195373
Epoch: 9315, Batch Gradient Norm: 20.377207913740293
Epoch: 9315, Batch Gradient Norm after: 20.377207913740293
Epoch 9316/10000, Prediction Accuracy = 60.376%, Loss = 0.6620179772377014
Epoch: 9316, Batch Gradient Norm: 20.4547221038369
Epoch: 9316, Batch Gradient Norm after: 20.4547221038369
Epoch 9317/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6623051404953003
Epoch: 9317, Batch Gradient Norm: 20.37617233330114
Epoch: 9317, Batch Gradient Norm after: 20.37617233330114
Epoch 9318/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6620190382003784
Epoch: 9318, Batch Gradient Norm: 20.375998724654057
Epoch: 9318, Batch Gradient Norm after: 20.375998724654057
Epoch 9319/10000, Prediction Accuracy = 60.312%, Loss = 0.6620880126953125
Epoch: 9319, Batch Gradient Norm: 20.231001327529338
Epoch: 9319, Batch Gradient Norm after: 20.231001327529338
Epoch 9320/10000, Prediction Accuracy = 60.348%, Loss = 0.6615454196929932
Epoch: 9320, Batch Gradient Norm: 20.249989478287187
Epoch: 9320, Batch Gradient Norm after: 20.249989478287187
Epoch 9321/10000, Prediction Accuracy = 60.322%, Loss = 0.6616289377212524
Epoch: 9321, Batch Gradient Norm: 20.24917440557498
Epoch: 9321, Batch Gradient Norm after: 20.24917440557498
Epoch 9322/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6615283012390136
Epoch: 9322, Batch Gradient Norm: 20.45319107541359
Epoch: 9322, Batch Gradient Norm after: 20.45319107541359
Epoch 9323/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6621679425239563
Epoch: 9323, Batch Gradient Norm: 20.38609768025755
Epoch: 9323, Batch Gradient Norm after: 20.37370083038631
Epoch 9324/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6619843244552612
Epoch: 9324, Batch Gradient Norm: 20.459120272997737
Epoch: 9324, Batch Gradient Norm after: 20.459120272997737
Epoch 9325/10000, Prediction Accuracy = 60.352%, Loss = 0.6622116684913635
Epoch: 9325, Batch Gradient Norm: 20.331669080599582
Epoch: 9325, Batch Gradient Norm after: 20.331669080599582
Epoch 9326/10000, Prediction Accuracy = 60.352%, Loss = 0.6617483019828796
Epoch: 9326, Batch Gradient Norm: 20.305456939036443
Epoch: 9326, Batch Gradient Norm after: 20.305456939036443
Epoch 9327/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6616784453392028
Epoch: 9327, Batch Gradient Norm: 20.222808733691725
Epoch: 9327, Batch Gradient Norm after: 20.222808733691725
Epoch 9328/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6613533735275269
Epoch: 9328, Batch Gradient Norm: 20.325897691776323
Epoch: 9328, Batch Gradient Norm after: 20.325897691776323
Epoch 9329/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6617441534996032
Epoch: 9329, Batch Gradient Norm: 20.313251724567724
Epoch: 9329, Batch Gradient Norm after: 20.313251724567724
Epoch 9330/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6616356015205384
Epoch: 9330, Batch Gradient Norm: 20.41363515160746
Epoch: 9330, Batch Gradient Norm after: 20.41363515160746
Epoch 9331/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6619418740272522
Epoch: 9331, Batch Gradient Norm: 20.348393173132152
Epoch: 9331, Batch Gradient Norm after: 20.348393173132152
Epoch 9332/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6616147041320801
Epoch: 9332, Batch Gradient Norm: 20.428728554906467
Epoch: 9332, Batch Gradient Norm after: 20.428728554906467
Epoch 9333/10000, Prediction Accuracy = 60.298%, Loss = 0.661880350112915
Epoch: 9333, Batch Gradient Norm: 20.356690067454206
Epoch: 9333, Batch Gradient Norm after: 20.356690067454206
Epoch 9334/10000, Prediction Accuracy = 60.352%, Loss = 0.6616622328758239
Epoch: 9334, Batch Gradient Norm: 20.382782271491042
Epoch: 9334, Batch Gradient Norm after: 20.382782271491042
Epoch 9335/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6618183016777038
Epoch: 9335, Batch Gradient Norm: 20.231854916121502
Epoch: 9335, Batch Gradient Norm after: 20.231854916121502
Epoch 9336/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6613208532333374
Epoch: 9336, Batch Gradient Norm: 20.216074573578215
Epoch: 9336, Batch Gradient Norm after: 20.216074573578215
Epoch 9337/10000, Prediction Accuracy = 60.362%, Loss = 0.6612533092498779
Epoch: 9337, Batch Gradient Norm: 20.163218219443387
Epoch: 9337, Batch Gradient Norm after: 20.163218219443387
Epoch 9338/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6609861016273498
Epoch: 9338, Batch Gradient Norm: 20.317832749554725
Epoch: 9338, Batch Gradient Norm after: 20.317832749554725
Epoch 9339/10000, Prediction Accuracy = 60.326%, Loss = 0.6614660739898681
Epoch: 9339, Batch Gradient Norm: 20.39449557961111
Epoch: 9339, Batch Gradient Norm after: 20.39449557961111
Epoch 9340/10000, Prediction Accuracy = 60.336%, Loss = 0.6616269111633301
Epoch: 9340, Batch Gradient Norm: 20.54259615993363
Epoch: 9340, Batch Gradient Norm after: 20.54259615993363
Epoch 9341/10000, Prediction Accuracy = 60.3%, Loss = 0.6621240496635437
Epoch: 9341, Batch Gradient Norm: 20.318274508473124
Epoch: 9341, Batch Gradient Norm after: 20.302923493446812
Epoch 9342/10000, Prediction Accuracy = 60.338%, Loss = 0.6613714575767518
Epoch: 9342, Batch Gradient Norm: 20.275928516211643
Epoch: 9342, Batch Gradient Norm after: 20.275928516211643
Epoch 9343/10000, Prediction Accuracy = 60.342%, Loss = 0.6612674951553345
Epoch: 9343, Batch Gradient Norm: 20.21975453817797
Epoch: 9343, Batch Gradient Norm after: 20.21975453817797
Epoch 9344/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6610147356987
Epoch: 9344, Batch Gradient Norm: 20.360295675160373
Epoch: 9344, Batch Gradient Norm after: 20.360295675160373
Epoch 9345/10000, Prediction Accuracy = 60.342%, Loss = 0.6614772081375122
Epoch: 9345, Batch Gradient Norm: 20.359953217880115
Epoch: 9345, Batch Gradient Norm after: 20.359953217880115
Epoch 9346/10000, Prediction Accuracy = 60.352%, Loss = 0.6614749550819397
Epoch: 9346, Batch Gradient Norm: 20.43622822825289
Epoch: 9346, Batch Gradient Norm after: 20.43622822825289
Epoch 9347/10000, Prediction Accuracy = 60.352%, Loss = 0.6617154002189636
Epoch: 9347, Batch Gradient Norm: 20.29147047200865
Epoch: 9347, Batch Gradient Norm after: 20.29147047200865
Epoch 9348/10000, Prediction Accuracy = 60.338%, Loss = 0.6612002968788147
Epoch: 9348, Batch Gradient Norm: 20.262088574235076
Epoch: 9348, Batch Gradient Norm after: 20.262088574235076
Epoch 9349/10000, Prediction Accuracy = 60.322%, Loss = 0.6611256718635559
Epoch: 9349, Batch Gradient Norm: 20.183185248468963
Epoch: 9349, Batch Gradient Norm after: 20.183185248468963
Epoch 9350/10000, Prediction Accuracy = 60.35%, Loss = 0.6608084201812744
Epoch: 9350, Batch Gradient Norm: 20.295331655879377
Epoch: 9350, Batch Gradient Norm after: 20.295331655879377
Epoch 9351/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6612326264381408
Epoch: 9351, Batch Gradient Norm: 20.287668295503277
Epoch: 9351, Batch Gradient Norm after: 20.287668295503277
Epoch 9352/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6611233592033386
Epoch: 9352, Batch Gradient Norm: 20.410518017953848
Epoch: 9352, Batch Gradient Norm after: 20.410518017953848
Epoch 9353/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.661498200893402
Epoch: 9353, Batch Gradient Norm: 20.34451216438252
Epoch: 9353, Batch Gradient Norm after: 20.34451216438252
Epoch 9354/10000, Prediction Accuracy = 60.372%, Loss = 0.661176884174347
Epoch: 9354, Batch Gradient Norm: 20.407526596240835
Epoch: 9354, Batch Gradient Norm after: 20.407526596240835
Epoch 9355/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6613917469978332
Epoch: 9355, Batch Gradient Norm: 20.327911833218625
Epoch: 9355, Batch Gradient Norm after: 20.327911833218625
Epoch 9356/10000, Prediction Accuracy = 60.342%, Loss = 0.6611445188522339
Epoch: 9356, Batch Gradient Norm: 20.345843075822803
Epoch: 9356, Batch Gradient Norm after: 20.345843075822803
Epoch 9357/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6612877249717712
Epoch: 9357, Batch Gradient Norm: 20.186189201863822
Epoch: 9357, Batch Gradient Norm after: 20.186189201863822
Epoch 9358/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6607670664787293
Epoch: 9358, Batch Gradient Norm: 20.171828386891622
Epoch: 9358, Batch Gradient Norm after: 20.171828386891622
Epoch 9359/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.660714840888977
Epoch: 9359, Batch Gradient Norm: 20.118668733322615
Epoch: 9359, Batch Gradient Norm after: 20.118668733322615
Epoch 9360/10000, Prediction Accuracy = 60.352%, Loss = 0.6604320883750916
Epoch: 9360, Batch Gradient Norm: 20.30451281753776
Epoch: 9360, Batch Gradient Norm after: 20.30451281753776
Epoch 9361/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6609965205192566
Epoch: 9361, Batch Gradient Norm: 20.39250315710948
Epoch: 9361, Batch Gradient Norm after: 20.39250315710948
Epoch 9362/10000, Prediction Accuracy = 60.35%, Loss = 0.6611989259719848
Epoch: 9362, Batch Gradient Norm: 20.549938052456024
Epoch: 9362, Batch Gradient Norm after: 20.549938052456024
Epoch 9363/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6617458701133728
Epoch: 9363, Batch Gradient Norm: 20.308334341797583
Epoch: 9363, Batch Gradient Norm after: 20.291651636444453
Epoch 9364/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6609207272529602
Epoch: 9364, Batch Gradient Norm: 20.229932910307273
Epoch: 9364, Batch Gradient Norm after: 20.229932910307273
Epoch 9365/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6607289791107178
Epoch: 9365, Batch Gradient Norm: 20.13710624476088
Epoch: 9365, Batch Gradient Norm after: 20.13710624476088
Epoch 9366/10000, Prediction Accuracy = 60.386%, Loss = 0.6603525161743165
Epoch: 9366, Batch Gradient Norm: 20.282235158396816
Epoch: 9366, Batch Gradient Norm after: 20.282235158396816
Epoch 9367/10000, Prediction Accuracy = 60.33%, Loss = 0.6608301043510437
Epoch: 9367, Batch Gradient Norm: 20.32356927329465
Epoch: 9367, Batch Gradient Norm after: 20.32356927329465
Epoch 9368/10000, Prediction Accuracy = 60.352%, Loss = 0.6609264254570008
Epoch: 9368, Batch Gradient Norm: 20.453749294491498
Epoch: 9368, Batch Gradient Norm after: 20.453749294491498
Epoch 9369/10000, Prediction Accuracy = 60.36%, Loss = 0.661340880393982
Epoch: 9369, Batch Gradient Norm: 20.341531840031728
Epoch: 9369, Batch Gradient Norm after: 20.341531840031728
Epoch 9370/10000, Prediction Accuracy = 60.354%, Loss = 0.6609141111373902
Epoch: 9370, Batch Gradient Norm: 20.295280931645333
Epoch: 9370, Batch Gradient Norm after: 20.295280931645333
Epoch 9371/10000, Prediction Accuracy = 60.336%, Loss = 0.6608026385307312
Epoch: 9371, Batch Gradient Norm: 20.15982772278165
Epoch: 9371, Batch Gradient Norm after: 20.15982772278165
Epoch 9372/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6603110432624817
Epoch: 9372, Batch Gradient Norm: 20.21334603163084
Epoch: 9372, Batch Gradient Norm after: 20.21334603163084
Epoch 9373/10000, Prediction Accuracy = 60.324%, Loss = 0.6605748414993287
Epoch: 9373, Batch Gradient Norm: 20.17380150473073
Epoch: 9373, Batch Gradient Norm after: 20.17380150473073
Epoch 9374/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6603732466697693
Epoch: 9374, Batch Gradient Norm: 20.291113511305685
Epoch: 9374, Batch Gradient Norm after: 20.291113511305685
Epoch 9375/10000, Prediction Accuracy = 60.302%, Loss = 0.6607576251029968
Epoch: 9375, Batch Gradient Norm: 20.29481599447984
Epoch: 9375, Batch Gradient Norm after: 20.29481599447984
Epoch 9376/10000, Prediction Accuracy = 60.364%, Loss = 0.6606014966964722
Epoch: 9376, Batch Gradient Norm: 20.458795541105523
Epoch: 9376, Batch Gradient Norm after: 20.458795541105523
Epoch 9377/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6610954523086547
Epoch: 9377, Batch Gradient Norm: 20.38071649561785
Epoch: 9377, Batch Gradient Norm after: 20.37277792935611
Epoch 9378/10000, Prediction Accuracy = 60.362%, Loss = 0.6608516335487366
Epoch: 9378, Batch Gradient Norm: 20.39525490297727
Epoch: 9378, Batch Gradient Norm after: 20.39525490297727
Epoch 9379/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6609919786453247
Epoch: 9379, Batch Gradient Norm: 20.21593983486208
Epoch: 9379, Batch Gradient Norm after: 20.21593983486208
Epoch 9380/10000, Prediction Accuracy = 60.33%, Loss = 0.6604260325431823
Epoch: 9380, Batch Gradient Norm: 20.14634860041171
Epoch: 9380, Batch Gradient Norm after: 20.14634860041171
Epoch 9381/10000, Prediction Accuracy = 60.366%, Loss = 0.6602222681045532
Epoch: 9381, Batch Gradient Norm: 20.0305172600002
Epoch: 9381, Batch Gradient Norm after: 20.0305172600002
Epoch 9382/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6597551226615905
Epoch: 9382, Batch Gradient Norm: 20.180588260777643
Epoch: 9382, Batch Gradient Norm after: 20.180588260777643
Epoch 9383/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6602234959602356
Epoch: 9383, Batch Gradient Norm: 20.289431941010577
Epoch: 9383, Batch Gradient Norm after: 20.289431941010577
Epoch 9384/10000, Prediction Accuracy = 60.342%, Loss = 0.6604694366455078
Epoch: 9384, Batch Gradient Norm: 20.514657068716232
Epoch: 9384, Batch Gradient Norm after: 20.514657068716232
Epoch 9385/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.6612058877944946
Epoch: 9385, Batch Gradient Norm: 20.333769329847346
Epoch: 9385, Batch Gradient Norm after: 20.312830493749583
Epoch 9386/10000, Prediction Accuracy = 60.36999999999999%, Loss = 0.6605671763420105
Epoch: 9386, Batch Gradient Norm: 20.30259214290676
Epoch: 9386, Batch Gradient Norm after: 20.30259214290676
Epoch 9387/10000, Prediction Accuracy = 60.306%, Loss = 0.6605169892311096
Epoch: 9387, Batch Gradient Norm: 20.21142155043794
Epoch: 9387, Batch Gradient Norm after: 20.21142155043794
Epoch 9388/10000, Prediction Accuracy = 60.408%, Loss = 0.6601307034492493
Epoch: 9388, Batch Gradient Norm: 20.313336881229453
Epoch: 9388, Batch Gradient Norm after: 20.313336881229453
Epoch 9389/10000, Prediction Accuracy = 60.338%, Loss = 0.6604891896247864
Epoch: 9389, Batch Gradient Norm: 20.269436432981674
Epoch: 9389, Batch Gradient Norm after: 20.269436432981674
Epoch 9390/10000, Prediction Accuracy = 60.352%, Loss = 0.6603547096252441
Epoch: 9390, Batch Gradient Norm: 20.309556757183223
Epoch: 9390, Batch Gradient Norm after: 20.309556757183223
Epoch 9391/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6605345845222473
Epoch: 9391, Batch Gradient Norm: 20.18439812455588
Epoch: 9391, Batch Gradient Norm after: 20.18439812455588
Epoch 9392/10000, Prediction Accuracy = 60.35%, Loss = 0.6600635647773743
Epoch: 9392, Batch Gradient Norm: 20.211872205673327
Epoch: 9392, Batch Gradient Norm after: 20.211872205673327
Epoch 9393/10000, Prediction Accuracy = 60.338%, Loss = 0.6601340174674988
Epoch: 9393, Batch Gradient Norm: 20.153182692630214
Epoch: 9393, Batch Gradient Norm after: 20.153182692630214
Epoch 9394/10000, Prediction Accuracy = 60.364%, Loss = 0.6598698019981384
Epoch: 9394, Batch Gradient Norm: 20.288299935999706
Epoch: 9394, Batch Gradient Norm after: 20.288299935999706
Epoch 9395/10000, Prediction Accuracy = 60.336%, Loss = 0.6603333950042725
Epoch: 9395, Batch Gradient Norm: 20.297224631353338
Epoch: 9395, Batch Gradient Norm after: 20.297224631353338
Epoch 9396/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6602900981903076
Epoch: 9396, Batch Gradient Norm: 20.400310261046894
Epoch: 9396, Batch Gradient Norm after: 20.400310261046894
Epoch 9397/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6606535673141479
Epoch: 9397, Batch Gradient Norm: 20.27653677099182
Epoch: 9397, Batch Gradient Norm after: 20.27653677099182
Epoch 9398/10000, Prediction Accuracy = 60.342%, Loss = 0.660147750377655
Epoch: 9398, Batch Gradient Norm: 20.28909994434971
Epoch: 9398, Batch Gradient Norm after: 20.28909994434971
Epoch 9399/10000, Prediction Accuracy = 60.318000000000005%, Loss = 0.6602099061012268
Epoch: 9399, Batch Gradient Norm: 20.184674768006946
Epoch: 9399, Batch Gradient Norm after: 20.184674768006946
Epoch 9400/10000, Prediction Accuracy = 60.388%, Loss = 0.6598578453063965
Epoch: 9400, Batch Gradient Norm: 20.266268989956597
Epoch: 9400, Batch Gradient Norm after: 20.266268989956597
Epoch 9401/10000, Prediction Accuracy = 60.338%, Loss = 0.6601727962493896
Epoch: 9401, Batch Gradient Norm: 20.191464208338875
Epoch: 9401, Batch Gradient Norm after: 20.191464208338875
Epoch 9402/10000, Prediction Accuracy = 60.336%, Loss = 0.6599287509918212
Epoch: 9402, Batch Gradient Norm: 20.247011106487
Epoch: 9402, Batch Gradient Norm after: 20.247011106487
Epoch 9403/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6600875854492188
Epoch: 9403, Batch Gradient Norm: 20.164630902052483
Epoch: 9403, Batch Gradient Norm after: 20.164630902052483
Epoch 9404/10000, Prediction Accuracy = 60.354%, Loss = 0.6597258090972901
Epoch: 9404, Batch Gradient Norm: 20.268685602513493
Epoch: 9404, Batch Gradient Norm after: 20.268685602513493
Epoch 9405/10000, Prediction Accuracy = 60.324%, Loss = 0.6600515365600585
Epoch: 9405, Batch Gradient Norm: 20.256719489496835
Epoch: 9405, Batch Gradient Norm after: 20.256719489496835
Epoch 9406/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.6599562048912049
Epoch: 9406, Batch Gradient Norm: 20.367586748841596
Epoch: 9406, Batch Gradient Norm after: 20.367586748841596
Epoch 9407/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.660370934009552
Epoch: 9407, Batch Gradient Norm: 20.253389895634655
Epoch: 9407, Batch Gradient Norm after: 20.253389895634655
Epoch 9408/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6599387407302857
Epoch: 9408, Batch Gradient Norm: 20.24294357151715
Epoch: 9408, Batch Gradient Norm after: 20.24294357151715
Epoch 9409/10000, Prediction Accuracy = 60.338%, Loss = 0.659926426410675
Epoch: 9409, Batch Gradient Norm: 20.129070159853143
Epoch: 9409, Batch Gradient Norm after: 20.129070159853143
Epoch 9410/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6594790101051331
Epoch: 9410, Batch Gradient Norm: 20.257372473757407
Epoch: 9410, Batch Gradient Norm after: 20.257372473757407
Epoch 9411/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.6598910450935364
Epoch: 9411, Batch Gradient Norm: 20.252347715171442
Epoch: 9411, Batch Gradient Norm after: 20.252347715171442
Epoch 9412/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6598780512809753
Epoch: 9412, Batch Gradient Norm: 20.367653052960836
Epoch: 9412, Batch Gradient Norm after: 20.367653052960836
Epoch 9413/10000, Prediction Accuracy = 60.35%, Loss = 0.6602587103843689
Epoch: 9413, Batch Gradient Norm: 20.2574909159519
Epoch: 9413, Batch Gradient Norm after: 20.2574909159519
Epoch 9414/10000, Prediction Accuracy = 60.354%, Loss = 0.6598604798316956
Epoch: 9414, Batch Gradient Norm: 20.239927730589418
Epoch: 9414, Batch Gradient Norm after: 20.239927730589418
Epoch 9415/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6597941875457763
Epoch: 9415, Batch Gradient Norm: 20.10185264442813
Epoch: 9415, Batch Gradient Norm after: 20.10185264442813
Epoch 9416/10000, Prediction Accuracy = 60.364%, Loss = 0.6592973589897155
Epoch: 9416, Batch Gradient Norm: 20.177732415591766
Epoch: 9416, Batch Gradient Norm after: 20.177732415591766
Epoch 9417/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6595925688743591
Epoch: 9417, Batch Gradient Norm: 20.162242350783774
Epoch: 9417, Batch Gradient Norm after: 20.162242350783774
Epoch 9418/10000, Prediction Accuracy = 60.38800000000001%, Loss = 0.6594974637031555
Epoch: 9418, Batch Gradient Norm: 20.299184763190446
Epoch: 9418, Batch Gradient Norm after: 20.299184763190446
Epoch 9419/10000, Prediction Accuracy = 60.298%, Loss = 0.6599517583847045
Epoch: 9419, Batch Gradient Norm: 20.267680262916855
Epoch: 9419, Batch Gradient Norm after: 20.267680262916855
Epoch 9420/10000, Prediction Accuracy = 60.374%, Loss = 0.6597015619277954
Epoch: 9420, Batch Gradient Norm: 20.363393038999146
Epoch: 9420, Batch Gradient Norm after: 20.363393038999146
Epoch 9421/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6599950909614563
Epoch: 9421, Batch Gradient Norm: 20.300244212025905
Epoch: 9421, Batch Gradient Norm after: 20.300244212025905
Epoch 9422/10000, Prediction Accuracy = 60.402%, Loss = 0.6597488760948181
Epoch: 9422, Batch Gradient Norm: 20.34326983842103
Epoch: 9422, Batch Gradient Norm after: 20.34326983842103
Epoch 9423/10000, Prediction Accuracy = 60.33%, Loss = 0.6599600076675415
Epoch: 9423, Batch Gradient Norm: 20.164726201503175
Epoch: 9423, Batch Gradient Norm after: 20.164726201503175
Epoch 9424/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6594316601753235
Epoch: 9424, Batch Gradient Norm: 20.10016646587079
Epoch: 9424, Batch Gradient Norm after: 20.10016646587079
Epoch 9425/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6592726826667785
Epoch: 9425, Batch Gradient Norm: 19.959331103763056
Epoch: 9425, Batch Gradient Norm after: 19.959331103763056
Epoch 9426/10000, Prediction Accuracy = 60.348%, Loss = 0.6587351560592651
Epoch: 9426, Batch Gradient Norm: 20.097633052110698
Epoch: 9426, Batch Gradient Norm after: 20.097633052110698
Epoch 9427/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6591474533081054
Epoch: 9427, Batch Gradient Norm: 20.208951731877445
Epoch: 9427, Batch Gradient Norm after: 20.208951731877445
Epoch 9428/10000, Prediction Accuracy = 60.366%, Loss = 0.6593870162963867
Epoch: 9428, Batch Gradient Norm: 20.460746737947453
Epoch: 9428, Batch Gradient Norm after: 20.460746737947453
Epoch 9429/10000, Prediction Accuracy = 60.322%, Loss = 0.660196053981781
Epoch: 9429, Batch Gradient Norm: 20.367021022756575
Epoch: 9429, Batch Gradient Norm after: 20.35501745817212
Epoch 9430/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6598355770111084
Epoch: 9430, Batch Gradient Norm: 20.360250002325397
Epoch: 9430, Batch Gradient Norm after: 20.360250002325397
Epoch 9431/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6598450660705566
Epoch: 9431, Batch Gradient Norm: 20.187125037213296
Epoch: 9431, Batch Gradient Norm after: 20.187125037213296
Epoch 9432/10000, Prediction Accuracy = 60.40400000000001%, Loss = 0.6592232942581177
Epoch: 9432, Batch Gradient Norm: 20.179342576443954
Epoch: 9432, Batch Gradient Norm after: 20.179342576443954
Epoch 9433/10000, Prediction Accuracy = 60.326%, Loss = 0.6592584252357483
Epoch: 9433, Batch Gradient Norm: 20.078063559987235
Epoch: 9433, Batch Gradient Norm after: 20.078063559987235
Epoch 9434/10000, Prediction Accuracy = 60.378%, Loss = 0.6589470386505127
Epoch: 9434, Batch Gradient Norm: 20.157684932111543
Epoch: 9434, Batch Gradient Norm after: 20.157684932111543
Epoch 9435/10000, Prediction Accuracy = 60.342%, Loss = 0.6592465519905091
Epoch: 9435, Batch Gradient Norm: 20.126165133542784
Epoch: 9435, Batch Gradient Norm after: 20.126165133542784
Epoch 9436/10000, Prediction Accuracy = 60.35%, Loss = 0.659080958366394
Epoch: 9436, Batch Gradient Norm: 20.232331762253395
Epoch: 9436, Batch Gradient Norm after: 20.232331762253395
Epoch 9437/10000, Prediction Accuracy = 60.35%, Loss = 0.6593701958656311
Epoch: 9437, Batch Gradient Norm: 20.230735043545522
Epoch: 9437, Batch Gradient Norm after: 20.230735043545522
Epoch 9438/10000, Prediction Accuracy = 60.374%, Loss = 0.6592427968978882
Epoch: 9438, Batch Gradient Norm: 20.356097266609403
Epoch: 9438, Batch Gradient Norm after: 20.356097266609403
Epoch 9439/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.659679114818573
Epoch: 9439, Batch Gradient Norm: 20.278779718902673
Epoch: 9439, Batch Gradient Norm after: 20.278779718902673
Epoch 9440/10000, Prediction Accuracy = 60.39%, Loss = 0.6594208598136901
Epoch: 9440, Batch Gradient Norm: 20.24298490466363
Epoch: 9440, Batch Gradient Norm after: 20.24298490466363
Epoch 9441/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.6594252228736878
Epoch: 9441, Batch Gradient Norm: 20.0415823484435
Epoch: 9441, Batch Gradient Norm after: 20.0415823484435
Epoch 9442/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6586742758750915
Epoch: 9442, Batch Gradient Norm: 20.050708429129767
Epoch: 9442, Batch Gradient Norm after: 20.050708429129767
Epoch 9443/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6587192058563233
Epoch: 9443, Batch Gradient Norm: 20.060268495098057
Epoch: 9443, Batch Gradient Norm after: 20.060268495098057
Epoch 9444/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.658643925189972
Epoch: 9444, Batch Gradient Norm: 20.325505307072333
Epoch: 9444, Batch Gradient Norm after: 20.325505307072333
Epoch 9445/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.659478235244751
Epoch: 9445, Batch Gradient Norm: 20.396391857869656
Epoch: 9445, Batch Gradient Norm after: 20.396391857869656
Epoch 9446/10000, Prediction Accuracy = 60.37199999999999%, Loss = 0.6596818208694458
Epoch: 9446, Batch Gradient Norm: 20.432827325039298
Epoch: 9446, Batch Gradient Norm after: 20.432827325039298
Epoch 9447/10000, Prediction Accuracy = 60.36%, Loss = 0.6598149299621582
Epoch: 9447, Batch Gradient Norm: 20.20879434961577
Epoch: 9447, Batch Gradient Norm after: 20.20879434961577
Epoch 9448/10000, Prediction Accuracy = 60.352%, Loss = 0.6590389013290405
Epoch: 9448, Batch Gradient Norm: 20.073708404940543
Epoch: 9448, Batch Gradient Norm after: 20.073708404940543
Epoch 9449/10000, Prediction Accuracy = 60.326%, Loss = 0.6586838483810424
Epoch: 9449, Batch Gradient Norm: 19.910602979952902
Epoch: 9449, Batch Gradient Norm after: 19.910602979952902
Epoch 9450/10000, Prediction Accuracy = 60.374%, Loss = 0.6581334471702576
Epoch: 9450, Batch Gradient Norm: 20.010320706156875
Epoch: 9450, Batch Gradient Norm after: 20.010320706156875
Epoch 9451/10000, Prediction Accuracy = 60.33%, Loss = 0.6585265040397644
Epoch: 9451, Batch Gradient Norm: 20.08582195987346
Epoch: 9451, Batch Gradient Norm after: 20.08582195987346
Epoch 9452/10000, Prediction Accuracy = 60.386%, Loss = 0.6586164236068726
Epoch: 9452, Batch Gradient Norm: 20.357590351599256
Epoch: 9452, Batch Gradient Norm after: 20.357590351599256
Epoch 9453/10000, Prediction Accuracy = 60.324%, Loss = 0.6594233393669129
Epoch: 9453, Batch Gradient Norm: 20.410587093939775
Epoch: 9453, Batch Gradient Norm after: 20.410587093939775
Epoch 9454/10000, Prediction Accuracy = 60.412%, Loss = 0.6594472527503967
Epoch: 9454, Batch Gradient Norm: 20.51292899631786
Epoch: 9454, Batch Gradient Norm after: 20.51292899631786
Epoch 9455/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.659795093536377
Epoch: 9455, Batch Gradient Norm: 20.315844588361962
Epoch: 9455, Batch Gradient Norm after: 20.315844588361962
Epoch 9456/10000, Prediction Accuracy = 60.376%, Loss = 0.6592069029808044
Epoch: 9456, Batch Gradient Norm: 20.143615846721975
Epoch: 9456, Batch Gradient Norm after: 20.143615846721975
Epoch 9457/10000, Prediction Accuracy = 60.342%, Loss = 0.6588141202926636
Epoch: 9457, Batch Gradient Norm: 19.850322908574753
Epoch: 9457, Batch Gradient Norm after: 19.850322908574753
Epoch 9458/10000, Prediction Accuracy = 60.362%, Loss = 0.6578973412513733
Epoch: 9458, Batch Gradient Norm: 19.810113050934962
Epoch: 9458, Batch Gradient Norm after: 19.810113050934962
Epoch 9459/10000, Prediction Accuracy = 60.33399999999999%, Loss = 0.6577946782112122
Epoch: 9459, Batch Gradient Norm: 19.862779224106458
Epoch: 9459, Batch Gradient Norm after: 19.862779224106458
Epoch 9460/10000, Prediction Accuracy = 60.39%, Loss = 0.6577977418899537
Epoch: 9460, Batch Gradient Norm: 20.248381543945765
Epoch: 9460, Batch Gradient Norm after: 20.248381543945765
Epoch 9461/10000, Prediction Accuracy = 60.32800000000001%, Loss = 0.6589357614517212
Epoch: 9461, Batch Gradient Norm: 20.482011970155156
Epoch: 9461, Batch Gradient Norm after: 20.482011970155156
Epoch 9462/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6595690011978149
Epoch: 9462, Batch Gradient Norm: 20.68660932747981
Epoch: 9462, Batch Gradient Norm after: 20.68660932747981
Epoch 9463/10000, Prediction Accuracy = 60.312%, Loss = 0.6602715015411377
Epoch: 9463, Batch Gradient Norm: 20.219340206929566
Epoch: 9463, Batch Gradient Norm after: 20.187819223376735
Epoch 9464/10000, Prediction Accuracy = 60.38800000000001%, Loss = 0.6587786316871643
Epoch: 9464, Batch Gradient Norm: 20.009638496761188
Epoch: 9464, Batch Gradient Norm after: 20.009638496761188
Epoch 9465/10000, Prediction Accuracy = 60.33%, Loss = 0.6581985116004944
Epoch: 9465, Batch Gradient Norm: 19.847396650473716
Epoch: 9465, Batch Gradient Norm after: 19.847396650473716
Epoch 9466/10000, Prediction Accuracy = 60.412%, Loss = 0.657637095451355
Epoch: 9466, Batch Gradient Norm: 20.01549232463491
Epoch: 9466, Batch Gradient Norm after: 20.01549232463491
Epoch 9467/10000, Prediction Accuracy = 60.352%, Loss = 0.6582099914550781
Epoch: 9467, Batch Gradient Norm: 20.147380749087926
Epoch: 9467, Batch Gradient Norm after: 20.147380749087926
Epoch 9468/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6585659503936767
Epoch: 9468, Batch Gradient Norm: 20.399216722896927
Epoch: 9468, Batch Gradient Norm after: 20.399216722896927
Epoch 9469/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6593162775039673
Epoch: 9469, Batch Gradient Norm: 20.37571974263292
Epoch: 9469, Batch Gradient Norm after: 20.37571974263292
Epoch 9470/10000, Prediction Accuracy = 60.388%, Loss = 0.6591076374053955
Epoch: 9470, Batch Gradient Norm: 20.36870251071561
Epoch: 9470, Batch Gradient Norm after: 20.36870251071561
Epoch 9471/10000, Prediction Accuracy = 60.35%, Loss = 0.6591107845306396
Epoch: 9471, Batch Gradient Norm: 20.116696473040477
Epoch: 9471, Batch Gradient Norm after: 20.116696473040477
Epoch 9472/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.6583156228065491
Epoch: 9472, Batch Gradient Norm: 20.03300524404334
Epoch: 9472, Batch Gradient Norm after: 20.03300524404334
Epoch 9473/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6582009553909302
Epoch: 9473, Batch Gradient Norm: 19.88142419583845
Epoch: 9473, Batch Gradient Norm after: 19.88142419583845
Epoch 9474/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.657638156414032
Epoch: 9474, Batch Gradient Norm: 19.99138016674992
Epoch: 9474, Batch Gradient Norm after: 19.99138016674992
Epoch 9475/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6579990029335022
Epoch: 9475, Batch Gradient Norm: 20.069811937299647
Epoch: 9475, Batch Gradient Norm after: 20.069811937299647
Epoch 9476/10000, Prediction Accuracy = 60.432%, Loss = 0.6580577850341797
Epoch: 9476, Batch Gradient Norm: 20.398584839609686
Epoch: 9476, Batch Gradient Norm after: 20.398584839609686
Epoch 9477/10000, Prediction Accuracy = 60.342%, Loss = 0.6590440988540649
Epoch: 9477, Batch Gradient Norm: 20.407353369530934
Epoch: 9477, Batch Gradient Norm after: 20.396543453073757
Epoch 9478/10000, Prediction Accuracy = 60.398%, Loss = 0.659042239189148
Epoch: 9478, Batch Gradient Norm: 20.44792105756901
Epoch: 9478, Batch Gradient Norm after: 20.44792105756901
Epoch 9479/10000, Prediction Accuracy = 60.334%, Loss = 0.6592547178268433
Epoch: 9479, Batch Gradient Norm: 20.205363931893977
Epoch: 9479, Batch Gradient Norm after: 20.205363931893977
Epoch 9480/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6584851384162903
Epoch: 9480, Batch Gradient Norm: 20.02275449312074
Epoch: 9480, Batch Gradient Norm after: 20.02275449312074
Epoch 9481/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.6579865574836731
Epoch: 9481, Batch Gradient Norm: 19.78552303696293
Epoch: 9481, Batch Gradient Norm after: 19.78552303696293
Epoch 9482/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6571735739707947
Epoch: 9482, Batch Gradient Norm: 19.89342412971037
Epoch: 9482, Batch Gradient Norm after: 19.89342412971037
Epoch 9483/10000, Prediction Accuracy = 60.35%, Loss = 0.6575352072715759
Epoch: 9483, Batch Gradient Norm: 20.018824114297058
Epoch: 9483, Batch Gradient Norm after: 20.018824114297058
Epoch 9484/10000, Prediction Accuracy = 60.402%, Loss = 0.6578118920326232
Epoch: 9484, Batch Gradient Norm: 20.37467350034999
Epoch: 9484, Batch Gradient Norm after: 20.37467350034999
Epoch 9485/10000, Prediction Accuracy = 60.314%, Loss = 0.6589319944381714
Epoch: 9485, Batch Gradient Norm: 20.42586897323443
Epoch: 9485, Batch Gradient Norm after: 20.423357032868594
Epoch 9486/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6589369058609009
Epoch: 9486, Batch Gradient Norm: 20.50929275803712
Epoch: 9486, Batch Gradient Norm after: 20.50929275803712
Epoch 9487/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6592000484466553
Epoch: 9487, Batch Gradient Norm: 20.266998927545682
Epoch: 9487, Batch Gradient Norm after: 20.266998927545682
Epoch 9488/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6584128499031067
Epoch: 9488, Batch Gradient Norm: 20.107081174174084
Epoch: 9488, Batch Gradient Norm after: 20.107081174174084
Epoch 9489/10000, Prediction Accuracy = 60.30799999999999%, Loss = 0.6580731868743896
Epoch: 9489, Batch Gradient Norm: 19.826739030367786
Epoch: 9489, Batch Gradient Norm after: 19.826739030367786
Epoch 9490/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6572433471679687
Epoch: 9490, Batch Gradient Norm: 19.825657405326712
Epoch: 9490, Batch Gradient Norm after: 19.825657405326712
Epoch 9491/10000, Prediction Accuracy = 60.36%, Loss = 0.6572880148887634
Epoch: 9491, Batch Gradient Norm: 19.833668444199915
Epoch: 9491, Batch Gradient Norm after: 19.833668444199915
Epoch 9492/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6571640729904175
Epoch: 9492, Batch Gradient Norm: 20.135999314524234
Epoch: 9492, Batch Gradient Norm after: 20.135999314524234
Epoch 9493/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6580235004425049
Epoch: 9493, Batch Gradient Norm: 20.34317285009595
Epoch: 9493, Batch Gradient Norm after: 20.34317285009595
Epoch 9494/10000, Prediction Accuracy = 60.398%, Loss = 0.6585156798362732
Epoch: 9494, Batch Gradient Norm: 20.622187493689847
Epoch: 9494, Batch Gradient Norm after: 20.622187493689847
Epoch 9495/10000, Prediction Accuracy = 60.35%, Loss = 0.6594364762306213
Epoch: 9495, Batch Gradient Norm: 20.259744455314323
Epoch: 9495, Batch Gradient Norm after: 20.228970162212526
Epoch 9496/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6583123207092285
Epoch: 9496, Batch Gradient Norm: 20.085015568161253
Epoch: 9496, Batch Gradient Norm after: 20.085015568161253
Epoch 9497/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6578765511512756
Epoch: 9497, Batch Gradient Norm: 19.847554127217478
Epoch: 9497, Batch Gradient Norm after: 19.847554127217478
Epoch 9498/10000, Prediction Accuracy = 60.412%, Loss = 0.6570372819900513
Epoch: 9498, Batch Gradient Norm: 19.930327090018345
Epoch: 9498, Batch Gradient Norm after: 19.930327090018345
Epoch 9499/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.6573259711265564
Epoch: 9499, Batch Gradient Norm: 20.01047599773816
Epoch: 9499, Batch Gradient Norm after: 20.01047599773816
Epoch 9500/10000, Prediction Accuracy = 60.396%, Loss = 0.6574724316596985
Epoch: 9500, Batch Gradient Norm: 20.33541394615133
Epoch: 9500, Batch Gradient Norm after: 20.33541394615133
Epoch 9501/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.65850430727005
Epoch: 9501, Batch Gradient Norm: 20.38916015466781
Epoch: 9501, Batch Gradient Norm after: 20.38916015466781
Epoch 9502/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6586151480674743
Epoch: 9502, Batch Gradient Norm: 20.40535166556813
Epoch: 9502, Batch Gradient Norm after: 20.40535166556813
Epoch 9503/10000, Prediction Accuracy = 60.35%, Loss = 0.6586717247962952
Epoch: 9503, Batch Gradient Norm: 20.142345097724455
Epoch: 9503, Batch Gradient Norm after: 20.142345097724455
Epoch 9504/10000, Prediction Accuracy = 60.398%, Loss = 0.6577778697013855
Epoch: 9504, Batch Gradient Norm: 20.00216950397499
Epoch: 9504, Batch Gradient Norm after: 20.00216950397499
Epoch 9505/10000, Prediction Accuracy = 60.338%, Loss = 0.6574472784996033
Epoch: 9505, Batch Gradient Norm: 19.79532987149782
Epoch: 9505, Batch Gradient Norm after: 19.79532987149782
Epoch 9506/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6567764043807983
Epoch: 9506, Batch Gradient Norm: 19.907643368801935
Epoch: 9506, Batch Gradient Norm after: 19.907643368801935
Epoch 9507/10000, Prediction Accuracy = 60.314%, Loss = 0.6572170853614807
Epoch: 9507, Batch Gradient Norm: 19.962090145705563
Epoch: 9507, Batch Gradient Norm after: 19.962090145705563
Epoch 9508/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6572169661521912
Epoch: 9508, Batch Gradient Norm: 20.284799194007626
Epoch: 9508, Batch Gradient Norm after: 20.284799194007626
Epoch 9509/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6581430792808532
Epoch: 9509, Batch Gradient Norm: 20.398123853113063
Epoch: 9509, Batch Gradient Norm after: 20.398123853113063
Epoch 9510/10000, Prediction Accuracy = 60.414%, Loss = 0.6583564639091491
Epoch: 9510, Batch Gradient Norm: 20.53884360508543
Epoch: 9510, Batch Gradient Norm after: 20.53884360508543
Epoch 9511/10000, Prediction Accuracy = 60.30800000000001%, Loss = 0.6588684439659118
Epoch: 9511, Batch Gradient Norm: 20.296133031016087
Epoch: 9511, Batch Gradient Norm after: 20.293355903410895
Epoch 9512/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6581490874290467
Epoch: 9512, Batch Gradient Norm: 20.065664482334988
Epoch: 9512, Batch Gradient Norm after: 20.065664482334988
Epoch 9513/10000, Prediction Accuracy = 60.348%, Loss = 0.657574450969696
Epoch: 9513, Batch Gradient Norm: 19.695697528580293
Epoch: 9513, Batch Gradient Norm after: 19.695697528580293
Epoch 9514/10000, Prediction Accuracy = 60.398%, Loss = 0.6563957929611206
Epoch: 9514, Batch Gradient Norm: 19.665811890485596
Epoch: 9514, Batch Gradient Norm after: 19.665811890485596
Epoch 9515/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6563207268714905
Epoch: 9515, Batch Gradient Norm: 19.74192716566784
Epoch: 9515, Batch Gradient Norm after: 19.74192716566784
Epoch 9516/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6564015507698059
Epoch: 9516, Batch Gradient Norm: 20.21091765777217
Epoch: 9516, Batch Gradient Norm after: 20.21091765777217
Epoch 9517/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6578119039535523
Epoch: 9517, Batch Gradient Norm: 20.48206539652968
Epoch: 9517, Batch Gradient Norm after: 20.48206539652968
Epoch 9518/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6585291028022766
Epoch: 9518, Batch Gradient Norm: 20.71676337817535
Epoch: 9518, Batch Gradient Norm after: 20.71676337817535
Epoch 9519/10000, Prediction Accuracy = 60.34000000000001%, Loss = 0.6592769384384155
Epoch: 9519, Batch Gradient Norm: 20.19508871875283
Epoch: 9519, Batch Gradient Norm after: 20.1566225237614
Epoch 9520/10000, Prediction Accuracy = 60.42%, Loss = 0.6576297998428344
Epoch: 9520, Batch Gradient Norm: 19.943746675290274
Epoch: 9520, Batch Gradient Norm after: 19.943746675290274
Epoch 9521/10000, Prediction Accuracy = 60.334%, Loss = 0.656975507736206
Epoch: 9521, Batch Gradient Norm: 19.739858924340528
Epoch: 9521, Batch Gradient Norm after: 19.739858924340528
Epoch 9522/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6563075661659241
Epoch: 9522, Batch Gradient Norm: 19.89780710566253
Epoch: 9522, Batch Gradient Norm after: 19.89780710566253
Epoch 9523/10000, Prediction Accuracy = 60.338%, Loss = 0.6568421483039856
Epoch: 9523, Batch Gradient Norm: 20.016922211244804
Epoch: 9523, Batch Gradient Norm after: 20.016922211244804
Epoch 9524/10000, Prediction Accuracy = 60.362%, Loss = 0.6571274280548096
Epoch: 9524, Batch Gradient Norm: 20.30927098077443
Epoch: 9524, Batch Gradient Norm after: 20.30927098077443
Epoch 9525/10000, Prediction Accuracy = 60.364%, Loss = 0.657976222038269
Epoch: 9525, Batch Gradient Norm: 20.353138456861352
Epoch: 9525, Batch Gradient Norm after: 20.353138456861352
Epoch 9526/10000, Prediction Accuracy = 60.372%, Loss = 0.6579738616943359
Epoch: 9526, Batch Gradient Norm: 20.40986034124909
Epoch: 9526, Batch Gradient Norm after: 20.40986034124909
Epoch 9527/10000, Prediction Accuracy = 60.348%, Loss = 0.6581860780715942
Epoch: 9527, Batch Gradient Norm: 20.17455007875492
Epoch: 9527, Batch Gradient Norm after: 20.17455007875492
Epoch 9528/10000, Prediction Accuracy = 60.4%, Loss = 0.6574431419372558
Epoch: 9528, Batch Gradient Norm: 20.024192491598136
Epoch: 9528, Batch Gradient Norm after: 20.024192491598136
Epoch 9529/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.657155168056488
Epoch: 9529, Batch Gradient Norm: 19.721982817601354
Epoch: 9529, Batch Gradient Norm after: 19.721982817601354
Epoch 9530/10000, Prediction Accuracy = 60.398%, Loss = 0.656156861782074
Epoch: 9530, Batch Gradient Norm: 19.750975232805974
Epoch: 9530, Batch Gradient Norm after: 19.750975232805974
Epoch 9531/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6562747836112977
Epoch: 9531, Batch Gradient Norm: 19.839430158457457
Epoch: 9531, Batch Gradient Norm after: 19.839430158457457
Epoch 9532/10000, Prediction Accuracy = 60.444%, Loss = 0.6563631296157837
Epoch: 9532, Batch Gradient Norm: 20.29079050435233
Epoch: 9532, Batch Gradient Norm after: 20.29079050435233
Epoch 9533/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6577046275138855
Epoch: 9533, Batch Gradient Norm: 20.480185879157563
Epoch: 9533, Batch Gradient Norm after: 20.47478991809709
Epoch 9534/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6582435488700866
Epoch: 9534, Batch Gradient Norm: 20.609995094961345
Epoch: 9534, Batch Gradient Norm after: 20.609995094961345
Epoch 9535/10000, Prediction Accuracy = 60.36%, Loss = 0.6587060928344727
Epoch: 9535, Batch Gradient Norm: 20.24534122603598
Epoch: 9535, Batch Gradient Norm after: 20.23845446606189
Epoch 9536/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6575703859329224
Epoch: 9536, Batch Gradient Norm: 19.931411017678652
Epoch: 9536, Batch Gradient Norm after: 19.931411017678652
Epoch 9537/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6566777944564819
Epoch: 9537, Batch Gradient Norm: 19.58385483817983
Epoch: 9537, Batch Gradient Norm after: 19.58385483817983
Epoch 9538/10000, Prediction Accuracy = 60.4%, Loss = 0.6555721044540406
Epoch: 9538, Batch Gradient Norm: 19.649108110293664
Epoch: 9538, Batch Gradient Norm after: 19.649108110293664
Epoch 9539/10000, Prediction Accuracy = 60.33%, Loss = 0.655834686756134
Epoch: 9539, Batch Gradient Norm: 19.806769821268016
Epoch: 9539, Batch Gradient Norm after: 19.806769821268016
Epoch 9540/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6561992287635803
Epoch: 9540, Batch Gradient Norm: 20.26417560019047
Epoch: 9540, Batch Gradient Norm after: 20.26417560019047
Epoch 9541/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6575778841972351
Epoch: 9541, Batch Gradient Norm: 20.470128830184635
Epoch: 9541, Batch Gradient Norm after: 20.470128830184635
Epoch 9542/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6580144762992859
Epoch: 9542, Batch Gradient Norm: 20.66176947886512
Epoch: 9542, Batch Gradient Norm after: 20.66176947886512
Epoch 9543/10000, Prediction Accuracy = 60.348%, Loss = 0.6586115956306458
Epoch: 9543, Batch Gradient Norm: 20.23114781096285
Epoch: 9543, Batch Gradient Norm after: 20.204337073940913
Epoch 9544/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6572861671447754
Epoch: 9544, Batch Gradient Norm: 19.9908477617019
Epoch: 9544, Batch Gradient Norm after: 19.9908477617019
Epoch 9545/10000, Prediction Accuracy = 60.324%, Loss = 0.6567019343376159
Epoch: 9545, Batch Gradient Norm: 19.709736774967602
Epoch: 9545, Batch Gradient Norm after: 19.709736774967602
Epoch 9546/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6558704614639282
Epoch: 9546, Batch Gradient Norm: 19.717635528736484
Epoch: 9546, Batch Gradient Norm after: 19.717635528736484
Epoch 9547/10000, Prediction Accuracy = 60.372%, Loss = 0.6559213995933533
Epoch: 9547, Batch Gradient Norm: 19.762740834691964
Epoch: 9547, Batch Gradient Norm after: 19.762740834691964
Epoch 9548/10000, Prediction Accuracy = 60.402%, Loss = 0.6559046268463135
Epoch: 9548, Batch Gradient Norm: 20.15596988883606
Epoch: 9548, Batch Gradient Norm after: 20.15596988883606
Epoch 9549/10000, Prediction Accuracy = 60.35%, Loss = 0.6570291757583618
Epoch: 9549, Batch Gradient Norm: 20.384007391793393
Epoch: 9549, Batch Gradient Norm after: 20.384007391793393
Epoch 9550/10000, Prediction Accuracy = 60.408%, Loss = 0.6576041460037232
Epoch: 9550, Batch Gradient Norm: 20.625301439291427
Epoch: 9550, Batch Gradient Norm after: 20.625301439291427
Epoch 9551/10000, Prediction Accuracy = 60.35%, Loss = 0.6584298372268677
Epoch: 9551, Batch Gradient Norm: 20.24707844039756
Epoch: 9551, Batch Gradient Norm after: 20.229143468567745
Epoch 9552/10000, Prediction Accuracy = 60.388%, Loss = 0.6572354197502136
Epoch: 9552, Batch Gradient Norm: 19.98700935187873
Epoch: 9552, Batch Gradient Norm after: 19.98700935187873
Epoch 9553/10000, Prediction Accuracy = 60.35%, Loss = 0.6565387725830079
Epoch: 9553, Batch Gradient Norm: 19.6629010430109
Epoch: 9553, Batch Gradient Norm after: 19.6629010430109
Epoch 9554/10000, Prediction Accuracy = 60.444%, Loss = 0.655486238002777
Epoch: 9554, Batch Gradient Norm: 19.711946825426537
Epoch: 9554, Batch Gradient Norm after: 19.711946825426537
Epoch 9555/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.6556733012199402
Epoch: 9555, Batch Gradient Norm: 19.841587534545337
Epoch: 9555, Batch Gradient Norm after: 19.841587534545337
Epoch 9556/10000, Prediction Accuracy = 60.388%, Loss = 0.6559819102287292
Epoch: 9556, Batch Gradient Norm: 20.279568268689513
Epoch: 9556, Batch Gradient Norm after: 20.279568268689513
Epoch 9557/10000, Prediction Accuracy = 60.37199999999999%, Loss = 0.6573072791099548
Epoch: 9557, Batch Gradient Norm: 20.42496983190115
Epoch: 9557, Batch Gradient Norm after: 20.42496983190115
Epoch 9558/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6576855540275574
Epoch: 9558, Batch Gradient Norm: 20.50268156653113
Epoch: 9558, Batch Gradient Norm after: 20.50268156653113
Epoch 9559/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6579077124595643
Epoch: 9559, Batch Gradient Norm: 20.20229016498488
Epoch: 9559, Batch Gradient Norm after: 20.20229016498488
Epoch 9560/10000, Prediction Accuracy = 60.39%, Loss = 0.6569082498550415
Epoch: 9560, Batch Gradient Norm: 19.97897589965896
Epoch: 9560, Batch Gradient Norm after: 19.97897589965896
Epoch 9561/10000, Prediction Accuracy = 60.362%, Loss = 0.6563603281974792
Epoch: 9561, Batch Gradient Norm: 19.640559865716984
Epoch: 9561, Batch Gradient Norm after: 19.640559865716984
Epoch 9562/10000, Prediction Accuracy = 60.398%, Loss = 0.6553338408470154
Epoch: 9562, Batch Gradient Norm: 19.67735317180383
Epoch: 9562, Batch Gradient Norm after: 19.67735317180383
Epoch 9563/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6555456995964051
Epoch: 9563, Batch Gradient Norm: 19.714849167426394
Epoch: 9563, Batch Gradient Norm after: 19.714849167426394
Epoch 9564/10000, Prediction Accuracy = 60.395999999999994%, Loss = 0.6554859042167663
Epoch: 9564, Batch Gradient Norm: 20.121972584967992
Epoch: 9564, Batch Gradient Norm after: 20.121972584967992
Epoch 9565/10000, Prediction Accuracy = 60.366%, Loss = 0.6566455364227295
Epoch: 9565, Batch Gradient Norm: 20.37726401090426
Epoch: 9565, Batch Gradient Norm after: 20.37726401090426
Epoch 9566/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.6572682380676269
Epoch: 9566, Batch Gradient Norm: 20.672851065435374
Epoch: 9566, Batch Gradient Norm after: 20.672851065435374
Epoch 9567/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6582271099090576
Epoch: 9567, Batch Gradient Norm: 20.199958884647938
Epoch: 9567, Batch Gradient Norm after: 20.162215152941965
Epoch 9568/10000, Prediction Accuracy = 60.366%, Loss = 0.6568534016609192
Epoch: 9568, Batch Gradient Norm: 19.952962694013117
Epoch: 9568, Batch Gradient Norm after: 19.952962694013117
Epoch 9569/10000, Prediction Accuracy = 60.374%, Loss = 0.6561959862709046
Epoch: 9569, Batch Gradient Norm: 19.635170117980188
Epoch: 9569, Batch Gradient Norm after: 19.635170117980188
Epoch 9570/10000, Prediction Accuracy = 60.388%, Loss = 0.6551763772964477
Epoch: 9570, Batch Gradient Norm: 19.692699796798205
Epoch: 9570, Batch Gradient Norm after: 19.692699796798205
Epoch 9571/10000, Prediction Accuracy = 60.352%, Loss = 0.6553381323814392
Epoch: 9571, Batch Gradient Norm: 19.81192207721543
Epoch: 9571, Batch Gradient Norm after: 19.81192207721543
Epoch 9572/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.655555522441864
Epoch: 9572, Batch Gradient Norm: 20.244609401782892
Epoch: 9572, Batch Gradient Norm after: 20.244609401782892
Epoch 9573/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6568963289260864
Epoch: 9573, Batch Gradient Norm: 20.405117918880634
Epoch: 9573, Batch Gradient Norm after: 20.405117918880634
Epoch 9574/10000, Prediction Accuracy = 60.408%, Loss = 0.6572644114494324
Epoch: 9574, Batch Gradient Norm: 20.56267411506281
Epoch: 9574, Batch Gradient Norm after: 20.56267411506281
Epoch 9575/10000, Prediction Accuracy = 60.354%, Loss = 0.6577739238739013
Epoch: 9575, Batch Gradient Norm: 20.255124725205242
Epoch: 9575, Batch Gradient Norm after: 20.255124725205242
Epoch 9576/10000, Prediction Accuracy = 60.426%, Loss = 0.6567476034164429
Epoch: 9576, Batch Gradient Norm: 20.013253912602103
Epoch: 9576, Batch Gradient Norm after: 20.013253912602103
Epoch 9577/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6561291217803955
Epoch: 9577, Batch Gradient Norm: 19.63267354992209
Epoch: 9577, Batch Gradient Norm after: 19.63267354992209
Epoch 9578/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6549802899360657
Epoch: 9578, Batch Gradient Norm: 19.640104651992793
Epoch: 9578, Batch Gradient Norm after: 19.640104651992793
Epoch 9579/10000, Prediction Accuracy = 60.324%, Loss = 0.6550902009010315
Epoch: 9579, Batch Gradient Norm: 19.67179390872177
Epoch: 9579, Batch Gradient Norm after: 19.67179390872177
Epoch 9580/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6551004648208618
Epoch: 9580, Batch Gradient Norm: 20.071256924502386
Epoch: 9580, Batch Gradient Norm after: 20.071256924502386
Epoch 9581/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6562398910522461
Epoch: 9581, Batch Gradient Norm: 20.295438299682274
Epoch: 9581, Batch Gradient Norm after: 20.295438299682274
Epoch 9582/10000, Prediction Accuracy = 60.378%, Loss = 0.6567705988883972
Epoch: 9582, Batch Gradient Norm: 20.565518505922086
Epoch: 9582, Batch Gradient Norm after: 20.565518505922086
Epoch 9583/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6575991153717041
Epoch: 9583, Batch Gradient Norm: 20.287633242193703
Epoch: 9583, Batch Gradient Norm after: 20.271002727637086
Epoch 9584/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6567335486412048
Epoch: 9584, Batch Gradient Norm: 20.102543860202612
Epoch: 9584, Batch Gradient Norm after: 20.102543860202612
Epoch 9585/10000, Prediction Accuracy = 60.372%, Loss = 0.6563283920288085
Epoch: 9585, Batch Gradient Norm: 19.696281143092794
Epoch: 9585, Batch Gradient Norm after: 19.696281143092794
Epoch 9586/10000, Prediction Accuracy = 60.412%, Loss = 0.6550526857376099
Epoch: 9586, Batch Gradient Norm: 19.629098679038847
Epoch: 9586, Batch Gradient Norm after: 19.629098679038847
Epoch 9587/10000, Prediction Accuracy = 60.364%, Loss = 0.6549009680747986
Epoch: 9587, Batch Gradient Norm: 19.60349373755554
Epoch: 9587, Batch Gradient Norm after: 19.60349373755554
Epoch 9588/10000, Prediction Accuracy = 60.444%, Loss = 0.654662573337555
Epoch: 9588, Batch Gradient Norm: 20.03158396547323
Epoch: 9588, Batch Gradient Norm after: 20.03158396547323
Epoch 9589/10000, Prediction Accuracy = 60.303999999999995%, Loss = 0.6559518456459046
Epoch: 9589, Batch Gradient Norm: 20.321029097470287
Epoch: 9589, Batch Gradient Norm after: 20.321029097470287
Epoch 9590/10000, Prediction Accuracy = 60.398%, Loss = 0.6567653059959412
Epoch: 9590, Batch Gradient Norm: 20.630475433701626
Epoch: 9590, Batch Gradient Norm after: 20.630475433701626
Epoch 9591/10000, Prediction Accuracy = 60.352%, Loss = 0.6577637195587158
Epoch: 9591, Batch Gradient Norm: 20.224216408635776
Epoch: 9591, Batch Gradient Norm after: 20.19818914526966
Epoch 9592/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6564914584159851
Epoch: 9592, Batch Gradient Norm: 19.96417880528444
Epoch: 9592, Batch Gradient Norm after: 19.96417880528444
Epoch 9593/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.6557213425636291
Epoch: 9593, Batch Gradient Norm: 19.636999623810908
Epoch: 9593, Batch Gradient Norm after: 19.636999623810908
Epoch 9594/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6546698212623596
Epoch: 9594, Batch Gradient Norm: 19.70923623418181
Epoch: 9594, Batch Gradient Norm after: 19.70923623418181
Epoch 9595/10000, Prediction Accuracy = 60.372%, Loss = 0.65497225522995
Epoch: 9595, Batch Gradient Norm: 19.765481110376005
Epoch: 9595, Batch Gradient Norm after: 19.765481110376005
Epoch 9596/10000, Prediction Accuracy = 60.414%, Loss = 0.6550523042678833
Epoch: 9596, Batch Gradient Norm: 20.14336701437721
Epoch: 9596, Batch Gradient Norm after: 20.14336701437721
Epoch 9597/10000, Prediction Accuracy = 60.35%, Loss = 0.6562002420425415
Epoch: 9597, Batch Gradient Norm: 20.26975698798773
Epoch: 9597, Batch Gradient Norm after: 20.26975698798773
Epoch 9598/10000, Prediction Accuracy = 60.428%, Loss = 0.6563997864723206
Epoch: 9598, Batch Gradient Norm: 20.472599778120227
Epoch: 9598, Batch Gradient Norm after: 20.472599778120227
Epoch 9599/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.6570326685905457
Epoch: 9599, Batch Gradient Norm: 20.27098710089296
Epoch: 9599, Batch Gradient Norm after: 20.27098710089296
Epoch 9600/10000, Prediction Accuracy = 60.414%, Loss = 0.6563693404197692
Epoch: 9600, Batch Gradient Norm: 20.103241215195624
Epoch: 9600, Batch Gradient Norm after: 20.103241215195624
Epoch 9601/10000, Prediction Accuracy = 60.336%, Loss = 0.6559924721717835
Epoch: 9601, Batch Gradient Norm: 19.71233323098502
Epoch: 9601, Batch Gradient Norm after: 19.71233323098502
Epoch 9602/10000, Prediction Accuracy = 60.372%, Loss = 0.6548272252082825
Epoch: 9602, Batch Gradient Norm: 19.62608298651586
Epoch: 9602, Batch Gradient Norm after: 19.62608298651586
Epoch 9603/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6546286940574646
Epoch: 9603, Batch Gradient Norm: 19.548438448374228
Epoch: 9603, Batch Gradient Norm after: 19.548438448374228
Epoch 9604/10000, Prediction Accuracy = 60.40400000000001%, Loss = 0.6542718172073364
Epoch: 9604, Batch Gradient Norm: 19.897541330183348
Epoch: 9604, Batch Gradient Norm after: 19.897541330183348
Epoch 9605/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6552782654762268
Epoch: 9605, Batch Gradient Norm: 20.16960862694892
Epoch: 9605, Batch Gradient Norm after: 20.16960862694892
Epoch 9606/10000, Prediction Accuracy = 60.41400000000001%, Loss = 0.6559499025344848
Epoch: 9606, Batch Gradient Norm: 20.564442891377794
Epoch: 9606, Batch Gradient Norm after: 20.564442891377794
Epoch 9607/10000, Prediction Accuracy = 60.35%, Loss = 0.657218337059021
Epoch: 9607, Batch Gradient Norm: 20.300451004850125
Epoch: 9607, Batch Gradient Norm after: 20.276549604753097
Epoch 9608/10000, Prediction Accuracy = 60.419999999999995%, Loss = 0.656348991394043
Epoch: 9608, Batch Gradient Norm: 20.143614754379833
Epoch: 9608, Batch Gradient Norm after: 20.143614754379833
Epoch 9609/10000, Prediction Accuracy = 60.366%, Loss = 0.6559616446495056
Epoch: 9609, Batch Gradient Norm: 19.77404629454452
Epoch: 9609, Batch Gradient Norm after: 19.77404629454452
Epoch 9610/10000, Prediction Accuracy = 60.45%, Loss = 0.6547598123550415
Epoch: 9610, Batch Gradient Norm: 19.72413276390656
Epoch: 9610, Batch Gradient Norm after: 19.72413276390656
Epoch 9611/10000, Prediction Accuracy = 60.314%, Loss = 0.6546998262405396
Epoch: 9611, Batch Gradient Norm: 19.668409106182065
Epoch: 9611, Batch Gradient Norm after: 19.668409106182065
Epoch 9612/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6544554710388184
Epoch: 9612, Batch Gradient Norm: 20.00175194650117
Epoch: 9612, Batch Gradient Norm after: 20.00175194650117
Epoch 9613/10000, Prediction Accuracy = 60.376%, Loss = 0.6555044770240783
Epoch: 9613, Batch Gradient Norm: 20.148589326484398
Epoch: 9613, Batch Gradient Norm after: 20.148589326484398
Epoch 9614/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.6558655381202698
Epoch: 9614, Batch Gradient Norm: 20.361107251873083
Epoch: 9614, Batch Gradient Norm after: 20.361107251873083
Epoch 9615/10000, Prediction Accuracy = 60.378%, Loss = 0.656483256816864
Epoch: 9615, Batch Gradient Norm: 20.199592786882842
Epoch: 9615, Batch Gradient Norm after: 20.199592786882842
Epoch 9616/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6558787584304809
Epoch: 9616, Batch Gradient Norm: 20.105448658320547
Epoch: 9616, Batch Gradient Norm after: 20.105448658320547
Epoch 9617/10000, Prediction Accuracy = 60.352%, Loss = 0.6556934595108033
Epoch: 9617, Batch Gradient Norm: 19.77782666794693
Epoch: 9617, Batch Gradient Norm after: 19.77782666794693
Epoch 9618/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6546943783760071
Epoch: 9618, Batch Gradient Norm: 19.744547913684713
Epoch: 9618, Batch Gradient Norm after: 19.744547913684713
Epoch 9619/10000, Prediction Accuracy = 60.362%, Loss = 0.6547415852546692
Epoch: 9619, Batch Gradient Norm: 19.61170306755491
Epoch: 9619, Batch Gradient Norm after: 19.61170306755491
Epoch 9620/10000, Prediction Accuracy = 60.426%, Loss = 0.6541863203048706
Epoch: 9620, Batch Gradient Norm: 19.88715157165406
Epoch: 9620, Batch Gradient Norm after: 19.88715157165406
Epoch 9621/10000, Prediction Accuracy = 60.372%, Loss = 0.6549771308898926
Epoch: 9621, Batch Gradient Norm: 20.053788130742973
Epoch: 9621, Batch Gradient Norm after: 20.053788130742973
Epoch 9622/10000, Prediction Accuracy = 60.452%, Loss = 0.6553145766258239
Epoch: 9622, Batch Gradient Norm: 20.43581297401239
Epoch: 9622, Batch Gradient Norm after: 20.43581297401239
Epoch 9623/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.656504225730896
Epoch: 9623, Batch Gradient Norm: 20.368482156613286
Epoch: 9623, Batch Gradient Norm after: 20.364136901352243
Epoch 9624/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6562952399253845
Epoch: 9624, Batch Gradient Norm: 20.26693999329915
Epoch: 9624, Batch Gradient Norm after: 20.26693999329915
Epoch 9625/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6560842633247376
Epoch: 9625, Batch Gradient Norm: 19.806852041658153
Epoch: 9625, Batch Gradient Norm after: 19.806852041658153
Epoch 9626/10000, Prediction Accuracy = 60.376%, Loss = 0.6546289443969726
Epoch: 9626, Batch Gradient Norm: 19.619593517828296
Epoch: 9626, Batch Gradient Norm after: 19.619593517828296
Epoch 9627/10000, Prediction Accuracy = 60.36%, Loss = 0.6541189193725586
Epoch: 9627, Batch Gradient Norm: 19.453283433676496
Epoch: 9627, Batch Gradient Norm after: 19.453283433676496
Epoch 9628/10000, Prediction Accuracy = 60.428%, Loss = 0.6535311341285706
Epoch: 9628, Batch Gradient Norm: 19.776824886022663
Epoch: 9628, Batch Gradient Norm after: 19.776824886022663
Epoch 9629/10000, Prediction Accuracy = 60.372%, Loss = 0.6545161604881287
Epoch: 9629, Batch Gradient Norm: 20.029990560630548
Epoch: 9629, Batch Gradient Norm after: 20.029990560630548
Epoch 9630/10000, Prediction Accuracy = 60.42%, Loss = 0.6551441311836242
Epoch: 9630, Batch Gradient Norm: 20.462280617356065
Epoch: 9630, Batch Gradient Norm after: 20.462280617356065
Epoch 9631/10000, Prediction Accuracy = 60.352%, Loss = 0.6564796209335327
Epoch: 9631, Batch Gradient Norm: 20.378918641808443
Epoch: 9631, Batch Gradient Norm after: 20.371489278948015
Epoch 9632/10000, Prediction Accuracy = 60.42%, Loss = 0.6560924887657166
Epoch: 9632, Batch Gradient Norm: 20.327707083338925
Epoch: 9632, Batch Gradient Norm after: 20.327707083338925
Epoch 9633/10000, Prediction Accuracy = 60.352%, Loss = 0.6560203075408936
Epoch: 9633, Batch Gradient Norm: 19.92350164397515
Epoch: 9633, Batch Gradient Norm after: 19.92350164397515
Epoch 9634/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6547734618186951
Epoch: 9634, Batch Gradient Norm: 19.745299170211464
Epoch: 9634, Batch Gradient Norm after: 19.745299170211464
Epoch 9635/10000, Prediction Accuracy = 60.342000000000006%, Loss = 0.6543785214424134
Epoch: 9635, Batch Gradient Norm: 19.480123418767654
Epoch: 9635, Batch Gradient Norm after: 19.480123418767654
Epoch 9636/10000, Prediction Accuracy = 60.366%, Loss = 0.6535736680030823
Epoch: 9636, Batch Gradient Norm: 19.62556557847047
Epoch: 9636, Batch Gradient Norm after: 19.62556557847047
Epoch 9637/10000, Prediction Accuracy = 60.386%, Loss = 0.6539992332458496
Epoch: 9637, Batch Gradient Norm: 19.775849927368593
Epoch: 9637, Batch Gradient Norm after: 19.775849927368593
Epoch 9638/10000, Prediction Accuracy = 60.412%, Loss = 0.6542681694030762
Epoch: 9638, Batch Gradient Norm: 20.251568460689416
Epoch: 9638, Batch Gradient Norm after: 20.251568460689416
Epoch 9639/10000, Prediction Accuracy = 60.35%, Loss = 0.6556733012199402
Epoch: 9639, Batch Gradient Norm: 20.405751496729284
Epoch: 9639, Batch Gradient Norm after: 20.405751496729284
Epoch 9640/10000, Prediction Accuracy = 60.408%, Loss = 0.6560546636581421
Epoch: 9640, Batch Gradient Norm: 20.53492200715447
Epoch: 9640, Batch Gradient Norm after: 20.53492200715447
Epoch 9641/10000, Prediction Accuracy = 60.364%, Loss = 0.6565770149230957
Epoch: 9641, Batch Gradient Norm: 20.098625920552905
Epoch: 9641, Batch Gradient Norm after: 20.098625920552905
Epoch 9642/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.655186903476715
Epoch: 9642, Batch Gradient Norm: 19.786795238222393
Epoch: 9642, Batch Gradient Norm after: 19.786795238222393
Epoch 9643/10000, Prediction Accuracy = 60.352%, Loss = 0.6543473720550537
Epoch: 9643, Batch Gradient Norm: 19.364411893124583
Epoch: 9643, Batch Gradient Norm after: 19.364411893124583
Epoch 9644/10000, Prediction Accuracy = 60.464%, Loss = 0.6529898405075073
Epoch: 9644, Batch Gradient Norm: 19.48784882641622
Epoch: 9644, Batch Gradient Norm after: 19.48784882641622
Epoch 9645/10000, Prediction Accuracy = 60.326%, Loss = 0.6534157991409302
Epoch: 9645, Batch Gradient Norm: 19.690958402191836
Epoch: 9645, Batch Gradient Norm after: 19.690958402191836
Epoch 9646/10000, Prediction Accuracy = 60.39%, Loss = 0.65392906665802
Epoch: 9646, Batch Gradient Norm: 20.268916705330327
Epoch: 9646, Batch Gradient Norm after: 20.268916705330327
Epoch 9647/10000, Prediction Accuracy = 60.388%, Loss = 0.6556672692298889
Epoch: 9647, Batch Gradient Norm: 20.448129034695814
Epoch: 9647, Batch Gradient Norm after: 20.448129034695814
Epoch 9648/10000, Prediction Accuracy = 60.378%, Loss = 0.6561090469360351
Epoch: 9648, Batch Gradient Norm: 20.54081272610971
Epoch: 9648, Batch Gradient Norm after: 20.54081272610971
Epoch 9649/10000, Prediction Accuracy = 60.366%, Loss = 0.656369936466217
Epoch: 9649, Batch Gradient Norm: 20.125799857817146
Epoch: 9649, Batch Gradient Norm after: 20.125799857817146
Epoch 9650/10000, Prediction Accuracy = 60.416%, Loss = 0.655049204826355
Epoch: 9650, Batch Gradient Norm: 19.829904438303963
Epoch: 9650, Batch Gradient Norm after: 19.829904438303963
Epoch 9651/10000, Prediction Accuracy = 60.362%, Loss = 0.6543289303779602
Epoch: 9651, Batch Gradient Norm: 19.400893000492804
Epoch: 9651, Batch Gradient Norm after: 19.400893000492804
Epoch 9652/10000, Prediction Accuracy = 60.45%, Loss = 0.6530218601226807
Epoch: 9652, Batch Gradient Norm: 19.467080238491963
Epoch: 9652, Batch Gradient Norm after: 19.467080238491963
Epoch 9653/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.653280746936798
Epoch: 9653, Batch Gradient Norm: 19.54062007887072
Epoch: 9653, Batch Gradient Norm after: 19.54062007887072
Epoch 9654/10000, Prediction Accuracy = 60.432%, Loss = 0.6533111333847046
Epoch: 9654, Batch Gradient Norm: 20.089494122986316
Epoch: 9654, Batch Gradient Norm after: 20.089494122986316
Epoch 9655/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6549018859863281
Epoch: 9655, Batch Gradient Norm: 20.37477849767078
Epoch: 9655, Batch Gradient Norm after: 20.37477849767078
Epoch 9656/10000, Prediction Accuracy = 60.462%, Loss = 0.6556416749954224
Epoch: 9656, Batch Gradient Norm: 20.676336264364572
Epoch: 9656, Batch Gradient Norm after: 20.676336264364572
Epoch 9657/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.6566370368003845
Epoch: 9657, Batch Gradient Norm: 20.217388358612865
Epoch: 9657, Batch Gradient Norm after: 20.194540081590496
Epoch 9658/10000, Prediction Accuracy = 60.364%, Loss = 0.6552574872970581
Epoch: 9658, Batch Gradient Norm: 19.87856506510769
Epoch: 9658, Batch Gradient Norm after: 19.87856506510769
Epoch 9659/10000, Prediction Accuracy = 60.384%, Loss = 0.6543405890464783
Epoch: 9659, Batch Gradient Norm: 19.412649323879037
Epoch: 9659, Batch Gradient Norm after: 19.412649323879037
Epoch 9660/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6528791904449462
Epoch: 9660, Batch Gradient Norm: 19.42621362760879
Epoch: 9660, Batch Gradient Norm after: 19.42621362760879
Epoch 9661/10000, Prediction Accuracy = 60.384%, Loss = 0.6529553890228271
Epoch: 9661, Batch Gradient Norm: 19.507323260742407
Epoch: 9661, Batch Gradient Norm after: 19.507323260742407
Epoch 9662/10000, Prediction Accuracy = 60.424%, Loss = 0.6530660510063171
Epoch: 9662, Batch Gradient Norm: 20.07290107073522
Epoch: 9662, Batch Gradient Norm after: 20.07290107073522
Epoch 9663/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.6547820806503296
Epoch: 9663, Batch Gradient Norm: 20.3353173173232
Epoch: 9663, Batch Gradient Norm after: 20.3353173173232
Epoch 9664/10000, Prediction Accuracy = 60.432%, Loss = 0.6554394602775574
Epoch: 9664, Batch Gradient Norm: 20.62996386865561
Epoch: 9664, Batch Gradient Norm after: 20.62996386865561
Epoch 9665/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6563621759414673
Epoch: 9665, Batch Gradient Norm: 20.26633098058484
Epoch: 9665, Batch Gradient Norm after: 20.259207589154013
Epoch 9666/10000, Prediction Accuracy = 60.462%, Loss = 0.6551567435264587
Epoch: 9666, Batch Gradient Norm: 20.002607087870974
Epoch: 9666, Batch Gradient Norm after: 20.002607087870974
Epoch 9667/10000, Prediction Accuracy = 60.348%, Loss = 0.6544754505157471
Epoch: 9667, Batch Gradient Norm: 19.53011751008395
Epoch: 9667, Batch Gradient Norm after: 19.53011751008395
Epoch 9668/10000, Prediction Accuracy = 60.402%, Loss = 0.653073501586914
Epoch: 9668, Batch Gradient Norm: 19.46379128208511
Epoch: 9668, Batch Gradient Norm after: 19.46379128208511
Epoch 9669/10000, Prediction Accuracy = 60.386%, Loss = 0.6530089855194092
Epoch: 9669, Batch Gradient Norm: 19.409794212188437
Epoch: 9669, Batch Gradient Norm after: 19.409794212188437
Epoch 9670/10000, Prediction Accuracy = 60.36%, Loss = 0.6527563810348511
Epoch: 9670, Batch Gradient Norm: 19.833653246937544
Epoch: 9670, Batch Gradient Norm after: 19.833653246937544
Epoch 9671/10000, Prediction Accuracy = 60.372%, Loss = 0.6539446353912354
Epoch: 9671, Batch Gradient Norm: 20.095157360234236
Epoch: 9671, Batch Gradient Norm after: 20.095157360234236
Epoch 9672/10000, Prediction Accuracy = 60.428%, Loss = 0.6545516371726989
Epoch: 9672, Batch Gradient Norm: 20.529793542480157
Epoch: 9672, Batch Gradient Norm after: 20.529793542480157
Epoch 9673/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.655891478061676
Epoch: 9673, Batch Gradient Norm: 20.330356151396185
Epoch: 9673, Batch Gradient Norm after: 20.316516274956562
Epoch 9674/10000, Prediction Accuracy = 60.428%, Loss = 0.6552595973014832
Epoch: 9674, Batch Gradient Norm: 20.172622361199057
Epoch: 9674, Batch Gradient Norm after: 20.172622361199057
Epoch 9675/10000, Prediction Accuracy = 60.372%, Loss = 0.654950475692749
Epoch: 9675, Batch Gradient Norm: 19.65065002647781
Epoch: 9675, Batch Gradient Norm after: 19.65065002647781
Epoch 9676/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6532842755317688
Epoch: 9676, Batch Gradient Norm: 19.51419324296255
Epoch: 9676, Batch Gradient Norm after: 19.51419324296255
Epoch 9677/10000, Prediction Accuracy = 60.35%, Loss = 0.6529412746429444
Epoch: 9677, Batch Gradient Norm: 19.36945240167273
Epoch: 9677, Batch Gradient Norm after: 19.36945240167273
Epoch 9678/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6523708939552307
Epoch: 9678, Batch Gradient Norm: 19.80263737980186
Epoch: 9678, Batch Gradient Norm after: 19.80263737980186
Epoch 9679/10000, Prediction Accuracy = 60.336%, Loss = 0.6536870956420898
Epoch: 9679, Batch Gradient Norm: 20.097966850587305
Epoch: 9679, Batch Gradient Norm after: 20.097966850587305
Epoch 9680/10000, Prediction Accuracy = 60.366%, Loss = 0.6544883131980896
Epoch: 9680, Batch Gradient Norm: 20.51689950599125
Epoch: 9680, Batch Gradient Norm after: 20.51689950599125
Epoch 9681/10000, Prediction Accuracy = 60.396%, Loss = 0.6557933926582337
Epoch: 9681, Batch Gradient Norm: 20.341905285647837
Epoch: 9681, Batch Gradient Norm after: 20.33862852521043
Epoch 9682/10000, Prediction Accuracy = 60.36%, Loss = 0.6551632404327392
Epoch: 9682, Batch Gradient Norm: 20.162314260620896
Epoch: 9682, Batch Gradient Norm after: 20.162314260620896
Epoch 9683/10000, Prediction Accuracy = 60.378%, Loss = 0.6546621203422547
Epoch: 9683, Batch Gradient Norm: 19.634287953422014
Epoch: 9683, Batch Gradient Norm after: 19.634287953422014
Epoch 9684/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6530342698097229
Epoch: 9684, Batch Gradient Norm: 19.50647247562502
Epoch: 9684, Batch Gradient Norm after: 19.50647247562502
Epoch 9685/10000, Prediction Accuracy = 60.35%, Loss = 0.652805507183075
Epoch: 9685, Batch Gradient Norm: 19.333991018222207
Epoch: 9685, Batch Gradient Norm after: 19.333991018222207
Epoch 9686/10000, Prediction Accuracy = 60.444%, Loss = 0.652204954624176
Epoch: 9686, Batch Gradient Norm: 19.699426615654488
Epoch: 9686, Batch Gradient Norm after: 19.699426615654488
Epoch 9687/10000, Prediction Accuracy = 60.378%, Loss = 0.653305172920227
Epoch: 9687, Batch Gradient Norm: 19.942051537376422
Epoch: 9687, Batch Gradient Norm after: 19.942051537376422
Epoch 9688/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6538149356842041
Epoch: 9688, Batch Gradient Norm: 20.47012546278183
Epoch: 9688, Batch Gradient Norm after: 20.47012546278183
Epoch 9689/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.6554049968719482
Epoch: 9689, Batch Gradient Norm: 20.404614189564136
Epoch: 9689, Batch Gradient Norm after: 20.393768215565
Epoch 9690/10000, Prediction Accuracy = 60.438%, Loss = 0.6551320195198059
Epoch: 9690, Batch Gradient Norm: 20.36100524678291
Epoch: 9690, Batch Gradient Norm after: 20.36100524678291
Epoch 9691/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6551384568214417
Epoch: 9691, Batch Gradient Norm: 19.844118810691644
Epoch: 9691, Batch Gradient Norm after: 19.844118810691644
Epoch 9692/10000, Prediction Accuracy = 60.338%, Loss = 0.6535882472991943
Epoch: 9692, Batch Gradient Norm: 19.564849739826887
Epoch: 9692, Batch Gradient Norm after: 19.564849739826887
Epoch 9693/10000, Prediction Accuracy = 60.396%, Loss = 0.6528386235237121
Epoch: 9693, Batch Gradient Norm: 19.199670583071132
Epoch: 9693, Batch Gradient Norm after: 19.199670583071132
Epoch 9694/10000, Prediction Accuracy = 60.396%, Loss = 0.6516636133193969
Epoch: 9694, Batch Gradient Norm: 19.434300443559138
Epoch: 9694, Batch Gradient Norm after: 19.434300443559138
Epoch 9695/10000, Prediction Accuracy = 60.35%, Loss = 0.6523699402809143
Epoch: 9695, Batch Gradient Norm: 19.71547512203757
Epoch: 9695, Batch Gradient Norm after: 19.71547512203757
Epoch 9696/10000, Prediction Accuracy = 60.42%, Loss = 0.6530370235443115
Epoch: 9696, Batch Gradient Norm: 20.395544586299476
Epoch: 9696, Batch Gradient Norm after: 20.395544586299476
Epoch 9697/10000, Prediction Accuracy = 60.366%, Loss = 0.6551437735557556
Epoch: 9697, Batch Gradient Norm: 20.455836703868677
Epoch: 9697, Batch Gradient Norm after: 20.444161150907835
Epoch 9698/10000, Prediction Accuracy = 60.45%, Loss = 0.6552034497261048
Epoch: 9698, Batch Gradient Norm: 20.526599256018315
Epoch: 9698, Batch Gradient Norm after: 20.526599256018315
Epoch 9699/10000, Prediction Accuracy = 60.362%, Loss = 0.6554563403129577
Epoch: 9699, Batch Gradient Norm: 20.038547466449295
Epoch: 9699, Batch Gradient Norm after: 20.038547466449295
Epoch 9700/10000, Prediction Accuracy = 60.448%, Loss = 0.6538793087005615
Epoch: 9700, Batch Gradient Norm: 19.746364452046834
Epoch: 9700, Batch Gradient Norm after: 19.746364452046834
Epoch 9701/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.6531418323516845
Epoch: 9701, Batch Gradient Norm: 19.290554739613977
Epoch: 9701, Batch Gradient Norm after: 19.290554739613977
Epoch 9702/10000, Prediction Accuracy = 60.384%, Loss = 0.6518141984939575
Epoch: 9702, Batch Gradient Norm: 19.38179785136117
Epoch: 9702, Batch Gradient Norm after: 19.38179785136117
Epoch 9703/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6521708846092225
Epoch: 9703, Batch Gradient Norm: 19.469374811838623
Epoch: 9703, Batch Gradient Norm after: 19.469374811838623
Epoch 9704/10000, Prediction Accuracy = 60.372%, Loss = 0.6522848010063171
Epoch: 9704, Batch Gradient Norm: 20.04297600547672
Epoch: 9704, Batch Gradient Norm after: 20.04297600547672
Epoch 9705/10000, Prediction Accuracy = 60.354%, Loss = 0.6539053440093994
Epoch: 9705, Batch Gradient Norm: 20.302591713511195
Epoch: 9705, Batch Gradient Norm after: 20.302591713511195
Epoch 9706/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6545323729515076
Epoch: 9706, Batch Gradient Norm: 20.63695754388982
Epoch: 9706, Batch Gradient Norm after: 20.63695754388982
Epoch 9707/10000, Prediction Accuracy = 60.348%, Loss = 0.6556355595588684
Epoch: 9707, Batch Gradient Norm: 20.28856341463526
Epoch: 9707, Batch Gradient Norm after: 20.288266488208645
Epoch 9708/10000, Prediction Accuracy = 60.436%, Loss = 0.6545384407043457
Epoch: 9708, Batch Gradient Norm: 19.962808615942887
Epoch: 9708, Batch Gradient Norm after: 19.962808615942887
Epoch 9709/10000, Prediction Accuracy = 60.39%, Loss = 0.6537102103233338
Epoch: 9709, Batch Gradient Norm: 19.30252253532172
Epoch: 9709, Batch Gradient Norm after: 19.30252253532172
Epoch 9710/10000, Prediction Accuracy = 60.45%, Loss = 0.6516665339469909
Epoch: 9710, Batch Gradient Norm: 19.196936675321428
Epoch: 9710, Batch Gradient Norm after: 19.196936675321428
Epoch 9711/10000, Prediction Accuracy = 60.318%, Loss = 0.6514400720596314
Epoch: 9711, Batch Gradient Norm: 19.216439594824767
Epoch: 9711, Batch Gradient Norm after: 19.216439594824767
Epoch 9712/10000, Prediction Accuracy = 60.444%, Loss = 0.6513327121734619
Epoch: 9712, Batch Gradient Norm: 19.901605973172238
Epoch: 9712, Batch Gradient Norm after: 19.901605973172238
Epoch 9713/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.653350567817688
Epoch: 9713, Batch Gradient Norm: 20.347607264302507
Epoch: 9713, Batch Gradient Norm after: 20.347607264302507
Epoch 9714/10000, Prediction Accuracy = 60.352%, Loss = 0.6546135544776917
Epoch: 9714, Batch Gradient Norm: 20.77901877864638
Epoch: 9714, Batch Gradient Norm after: 20.77901877864638
Epoch 9715/10000, Prediction Accuracy = 60.412%, Loss = 0.6559524774551392
Epoch: 9715, Batch Gradient Norm: 20.204740405396326
Epoch: 9715, Batch Gradient Norm after: 20.16882211602025
Epoch 9716/10000, Prediction Accuracy = 60.372%, Loss = 0.6541335463523865
Epoch: 9716, Batch Gradient Norm: 19.85471071785773
Epoch: 9716, Batch Gradient Norm after: 19.85471071785773
Epoch 9717/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6531699895858765
Epoch: 9717, Batch Gradient Norm: 19.359746027989253
Epoch: 9717, Batch Gradient Norm after: 19.359746027989253
Epoch 9718/10000, Prediction Accuracy = 60.432%, Loss = 0.6516586780548096
Epoch: 9718, Batch Gradient Norm: 19.39072625300373
Epoch: 9718, Batch Gradient Norm after: 19.39072625300373
Epoch 9719/10000, Prediction Accuracy = 60.364%, Loss = 0.6518941760063172
Epoch: 9719, Batch Gradient Norm: 19.397013879559307
Epoch: 9719, Batch Gradient Norm after: 19.397013879559307
Epoch 9720/10000, Prediction Accuracy = 60.465999999999994%, Loss = 0.6517785310745239
Epoch: 9720, Batch Gradient Norm: 19.935303769341562
Epoch: 9720, Batch Gradient Norm after: 19.935303769341562
Epoch 9721/10000, Prediction Accuracy = 60.388%, Loss = 0.653350007534027
Epoch: 9721, Batch Gradient Norm: 20.192997697447215
Epoch: 9721, Batch Gradient Norm after: 20.192997697447215
Epoch 9722/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.6539170742034912
Epoch: 9722, Batch Gradient Norm: 20.605413512079448
Epoch: 9722, Batch Gradient Norm after: 20.605413512079448
Epoch 9723/10000, Prediction Accuracy = 60.364%, Loss = 0.6552009344100952
Epoch: 9723, Batch Gradient Norm: 20.321231979815728
Epoch: 9723, Batch Gradient Norm after: 20.31468885753683
Epoch 9724/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6543212175369263
Epoch: 9724, Batch Gradient Norm: 20.07232153164935
Epoch: 9724, Batch Gradient Norm after: 20.07232153164935
Epoch 9725/10000, Prediction Accuracy = 60.376%, Loss = 0.6537370800971984
Epoch: 9725, Batch Gradient Norm: 19.437161813632883
Epoch: 9725, Batch Gradient Norm after: 19.437161813632883
Epoch 9726/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6518427848815918
Epoch: 9726, Batch Gradient Norm: 19.227709287983924
Epoch: 9726, Batch Gradient Norm after: 19.227709287983924
Epoch 9727/10000, Prediction Accuracy = 60.376%, Loss = 0.6512845516204834
Epoch: 9727, Batch Gradient Norm: 19.087617471634275
Epoch: 9727, Batch Gradient Norm after: 19.087617471634275
Epoch 9728/10000, Prediction Accuracy = 60.4%, Loss = 0.6507121562957764
Epoch: 9728, Batch Gradient Norm: 19.650442975548728
Epoch: 9728, Batch Gradient Norm after: 19.650442975548728
Epoch 9729/10000, Prediction Accuracy = 60.36999999999999%, Loss = 0.6523632526397705
Epoch: 9729, Batch Gradient Norm: 20.095515524750557
Epoch: 9729, Batch Gradient Norm after: 20.095515524750557
Epoch 9730/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.653532886505127
Epoch: 9730, Batch Gradient Norm: 20.741844021158666
Epoch: 9730, Batch Gradient Norm after: 20.741844021158666
Epoch 9731/10000, Prediction Accuracy = 60.39%, Loss = 0.6555517315864563
Epoch: 9731, Batch Gradient Norm: 20.237042795245547
Epoch: 9731, Batch Gradient Norm after: 20.182128328677994
Epoch 9732/10000, Prediction Accuracy = 60.452%, Loss = 0.653938102722168
Epoch: 9732, Batch Gradient Norm: 20.034856211581747
Epoch: 9732, Batch Gradient Norm after: 20.034856211581747
Epoch 9733/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.6534162402153015
Epoch: 9733, Batch Gradient Norm: 19.58771554699266
Epoch: 9733, Batch Gradient Norm after: 19.58771554699266
Epoch 9734/10000, Prediction Accuracy = 60.462%, Loss = 0.6519932866096496
Epoch: 9734, Batch Gradient Norm: 19.58750889193749
Epoch: 9734, Batch Gradient Norm after: 19.58750889193749
Epoch 9735/10000, Prediction Accuracy = 60.352%, Loss = 0.6521033406257629
Epoch: 9735, Batch Gradient Norm: 19.451481245579423
Epoch: 9735, Batch Gradient Norm after: 19.451481245579423
Epoch 9736/10000, Prediction Accuracy = 60.364%, Loss = 0.651663851737976
Epoch: 9736, Batch Gradient Norm: 19.800277074930303
Epoch: 9736, Batch Gradient Norm after: 19.800277074930303
Epoch 9737/10000, Prediction Accuracy = 60.402%, Loss = 0.6527313828468323
Epoch: 9737, Batch Gradient Norm: 19.896050697623124
Epoch: 9737, Batch Gradient Norm after: 19.896050697623124
Epoch 9738/10000, Prediction Accuracy = 60.366%, Loss = 0.6528813719749451
Epoch: 9738, Batch Gradient Norm: 20.242229051126753
Epoch: 9738, Batch Gradient Norm after: 20.242229051126753
Epoch 9739/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6538771986961365
Epoch: 9739, Batch Gradient Norm: 20.111130938936473
Epoch: 9739, Batch Gradient Norm after: 20.111130938936473
Epoch 9740/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6533916831016541
Epoch: 9740, Batch Gradient Norm: 20.143533998363086
Epoch: 9740, Batch Gradient Norm after: 20.143533998363086
Epoch 9741/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6536366701126098
Epoch: 9741, Batch Gradient Norm: 19.726530301047337
Epoch: 9741, Batch Gradient Norm after: 19.726530301047337
Epoch 9742/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6523423075675965
Epoch: 9742, Batch Gradient Norm: 19.610343036612214
Epoch: 9742, Batch Gradient Norm after: 19.610343036612214
Epoch 9743/10000, Prediction Accuracy = 60.39%, Loss = 0.6520979642868042
Epoch: 9743, Batch Gradient Norm: 19.32646800355069
Epoch: 9743, Batch Gradient Norm after: 19.32646800355069
Epoch 9744/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6510969758033752
Epoch: 9744, Batch Gradient Norm: 19.617930996753167
Epoch: 9744, Batch Gradient Norm after: 19.617930996753167
Epoch 9745/10000, Prediction Accuracy = 60.342%, Loss = 0.651971185207367
Epoch: 9745, Batch Gradient Norm: 19.80150752600441
Epoch: 9745, Batch Gradient Norm after: 19.80150752600441
Epoch 9746/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6523847937583923
Epoch: 9746, Batch Gradient Norm: 20.344010419923332
Epoch: 9746, Batch Gradient Norm after: 20.344010419923332
Epoch 9747/10000, Prediction Accuracy = 60.384%, Loss = 0.6540734648704529
Epoch: 9747, Batch Gradient Norm: 20.31045401697715
Epoch: 9747, Batch Gradient Norm after: 20.31045401697715
Epoch 9748/10000, Prediction Accuracy = 60.362%, Loss = 0.6539441823959351
Epoch: 9748, Batch Gradient Norm: 20.259270426562995
Epoch: 9748, Batch Gradient Norm after: 20.259270426562995
Epoch 9749/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.6538250327110291
Epoch: 9749, Batch Gradient Norm: 19.701364060509665
Epoch: 9749, Batch Gradient Norm after: 19.701364060509665
Epoch 9750/10000, Prediction Accuracy = 60.362%, Loss = 0.6520657539367676
Epoch: 9750, Batch Gradient Norm: 19.504911635667337
Epoch: 9750, Batch Gradient Norm after: 19.504911635667337
Epoch 9751/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.6515716552734375
Epoch: 9751, Batch Gradient Norm: 19.217821521978152
Epoch: 9751, Batch Gradient Norm after: 19.217821521978152
Epoch 9752/10000, Prediction Accuracy = 60.438%, Loss = 0.6506508708000183
Epoch: 9752, Batch Gradient Norm: 19.557081744340465
Epoch: 9752, Batch Gradient Norm after: 19.557081744340465
Epoch 9753/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6517325639724731
Epoch: 9753, Batch Gradient Norm: 19.747305750612618
Epoch: 9753, Batch Gradient Norm after: 19.747305750612618
Epoch 9754/10000, Prediction Accuracy = 60.462%, Loss = 0.6521295309066772
Epoch: 9754, Batch Gradient Norm: 20.306343040931758
Epoch: 9754, Batch Gradient Norm after: 20.306343040931758
Epoch 9755/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.653790009021759
Epoch: 9755, Batch Gradient Norm: 20.32508164308002
Epoch: 9755, Batch Gradient Norm after: 20.32508164308002
Epoch 9756/10000, Prediction Accuracy = 60.459999999999994%, Loss = 0.6537097215652465
Epoch: 9756, Batch Gradient Norm: 20.395138457175
Epoch: 9756, Batch Gradient Norm after: 20.395138457175
Epoch 9757/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.6540164589881897
Epoch: 9757, Batch Gradient Norm: 19.87261827754809
Epoch: 9757, Batch Gradient Norm after: 19.87261827754809
Epoch 9758/10000, Prediction Accuracy = 60.4%, Loss = 0.6524513244628907
Epoch: 9758, Batch Gradient Norm: 19.59531451309317
Epoch: 9758, Batch Gradient Norm after: 19.59531451309317
Epoch 9759/10000, Prediction Accuracy = 60.386%, Loss = 0.651765775680542
Epoch: 9759, Batch Gradient Norm: 19.14493063461088
Epoch: 9759, Batch Gradient Norm after: 19.14493063461088
Epoch 9760/10000, Prediction Accuracy = 60.378%, Loss = 0.650382137298584
Epoch: 9760, Batch Gradient Norm: 19.30573895277096
Epoch: 9760, Batch Gradient Norm after: 19.30573895277096
Epoch 9761/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.650849187374115
Epoch: 9761, Batch Gradient Norm: 19.4617962029568
Epoch: 9761, Batch Gradient Norm after: 19.4617962029568
Epoch 9762/10000, Prediction Accuracy = 60.414%, Loss = 0.6511201739311219
Epoch: 9762, Batch Gradient Norm: 20.17962859824436
Epoch: 9762, Batch Gradient Norm after: 20.17962859824436
Epoch 9763/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6532535672187805
Epoch: 9763, Batch Gradient Norm: 20.43234497484112
Epoch: 9763, Batch Gradient Norm after: 20.43234497484112
Epoch 9764/10000, Prediction Accuracy = 60.443999999999996%, Loss = 0.6539405703544616
Epoch: 9764, Batch Gradient Norm: 20.63766367020289
Epoch: 9764, Batch Gradient Norm after: 20.63766367020289
Epoch 9765/10000, Prediction Accuracy = 60.388%, Loss = 0.6546720147132874
Epoch: 9765, Batch Gradient Norm: 20.059739270064764
Epoch: 9765, Batch Gradient Norm after: 20.059739270064764
Epoch 9766/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6528254151344299
Epoch: 9766, Batch Gradient Norm: 19.646929757472357
Epoch: 9766, Batch Gradient Norm after: 19.646929757472357
Epoch 9767/10000, Prediction Accuracy = 60.33%, Loss = 0.6517020106315613
Epoch: 9767, Batch Gradient Norm: 19.06669110181523
Epoch: 9767, Batch Gradient Norm after: 19.06669110181523
Epoch 9768/10000, Prediction Accuracy = 60.444%, Loss = 0.6499287486076355
Epoch: 9768, Batch Gradient Norm: 19.196767563033546
Epoch: 9768, Batch Gradient Norm after: 19.196767563033546
Epoch 9769/10000, Prediction Accuracy = 60.384%, Loss = 0.650404942035675
Epoch: 9769, Batch Gradient Norm: 19.364107706051563
Epoch: 9769, Batch Gradient Norm after: 19.364107706051563
Epoch 9770/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.650830602645874
Epoch: 9770, Batch Gradient Norm: 20.06638406972449
Epoch: 9770, Batch Gradient Norm after: 20.06638406972449
Epoch 9771/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.652872097492218
Epoch: 9771, Batch Gradient Norm: 20.31532056139428
Epoch: 9771, Batch Gradient Norm after: 20.31532056139428
Epoch 9772/10000, Prediction Accuracy = 60.36%, Loss = 0.6534681200981141
Epoch: 9772, Batch Gradient Norm: 20.598449699223924
Epoch: 9772, Batch Gradient Norm after: 20.598449699223924
Epoch 9773/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.654328453540802
Epoch: 9773, Batch Gradient Norm: 20.16538636079608
Epoch: 9773, Batch Gradient Norm after: 20.16538636079608
Epoch 9774/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.6529678583145142
Epoch: 9774, Batch Gradient Norm: 19.861016352265352
Epoch: 9774, Batch Gradient Norm after: 19.861016352265352
Epoch 9775/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6522496342658997
Epoch: 9775, Batch Gradient Norm: 19.208426065884264
Epoch: 9775, Batch Gradient Norm after: 19.208426065884264
Epoch 9776/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.6502676963806152
Epoch: 9776, Batch Gradient Norm: 19.142638813430455
Epoch: 9776, Batch Gradient Norm after: 19.142638813430455
Epoch 9777/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6501522898674011
Epoch: 9777, Batch Gradient Norm: 19.118797438563607
Epoch: 9777, Batch Gradient Norm after: 19.118797438563607
Epoch 9778/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.649878466129303
Epoch: 9778, Batch Gradient Norm: 19.807219056591624
Epoch: 9778, Batch Gradient Norm after: 19.807219056591624
Epoch 9779/10000, Prediction Accuracy = 60.35%, Loss = 0.6518699169158936
Epoch: 9779, Batch Gradient Norm: 20.24243611224705
Epoch: 9779, Batch Gradient Norm after: 20.24243611224705
Epoch 9780/10000, Prediction Accuracy = 60.4%, Loss = 0.6530684351921081
Epoch: 9780, Batch Gradient Norm: 20.776146869663215
Epoch: 9780, Batch Gradient Norm after: 20.776146869663215
Epoch 9781/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.65476313829422
Epoch: 9781, Batch Gradient Norm: 20.216245885414246
Epoch: 9781, Batch Gradient Norm after: 20.185115418913895
Epoch 9782/10000, Prediction Accuracy = 60.374%, Loss = 0.6530553460121155
Epoch: 9782, Batch Gradient Norm: 19.84625088400835
Epoch: 9782, Batch Gradient Norm after: 19.84625088400835
Epoch 9783/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6520150661468506
Epoch: 9783, Batch Gradient Norm: 19.234279177295548
Epoch: 9783, Batch Gradient Norm after: 19.234279177295548
Epoch 9784/10000, Prediction Accuracy = 60.414%, Loss = 0.6501274108886719
Epoch: 9784, Batch Gradient Norm: 19.233841372711
Epoch: 9784, Batch Gradient Norm after: 19.233841372711
Epoch 9785/10000, Prediction Accuracy = 60.372%, Loss = 0.650207269191742
Epoch: 9785, Batch Gradient Norm: 19.238647491054593
Epoch: 9785, Batch Gradient Norm after: 19.238647491054593
Epoch 9786/10000, Prediction Accuracy = 60.46%, Loss = 0.6501075029373169
Epoch: 9786, Batch Gradient Norm: 19.85414449082228
Epoch: 9786, Batch Gradient Norm after: 19.85414449082228
Epoch 9787/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6519561171531677
Epoch: 9787, Batch Gradient Norm: 20.142583835340172
Epoch: 9787, Batch Gradient Norm after: 20.142583835340172
Epoch 9788/10000, Prediction Accuracy = 60.468%, Loss = 0.6526421070098877
Epoch: 9788, Batch Gradient Norm: 20.59884674540811
Epoch: 9788, Batch Gradient Norm after: 20.59884674540811
Epoch 9789/10000, Prediction Accuracy = 60.326%, Loss = 0.6540349960327149
Epoch: 9789, Batch Gradient Norm: 20.295261014563547
Epoch: 9789, Batch Gradient Norm after: 20.295261014563547
Epoch 9790/10000, Prediction Accuracy = 60.45400000000001%, Loss = 0.6530364632606507
Epoch: 9790, Batch Gradient Norm: 20.047363888017568
Epoch: 9790, Batch Gradient Norm after: 20.047363888017568
Epoch 9791/10000, Prediction Accuracy = 60.386%, Loss = 0.6524394512176513
Epoch: 9791, Batch Gradient Norm: 19.359645625547813
Epoch: 9791, Batch Gradient Norm after: 19.359645625547813
Epoch 9792/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6504308819770813
Epoch: 9792, Batch Gradient Norm: 19.176708254862252
Epoch: 9792, Batch Gradient Norm after: 19.176708254862252
Epoch 9793/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.6499975919723511
Epoch: 9793, Batch Gradient Norm: 18.979638298992516
Epoch: 9793, Batch Gradient Norm after: 18.979638298992516
Epoch 9794/10000, Prediction Accuracy = 60.39%, Loss = 0.6493067979812622
Epoch: 9794, Batch Gradient Norm: 19.495548120151316
Epoch: 9794, Batch Gradient Norm after: 19.495548120151316
Epoch 9795/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.6507496118545533
Epoch: 9795, Batch Gradient Norm: 19.905434503484848
Epoch: 9795, Batch Gradient Norm after: 19.905434503484848
Epoch 9796/10000, Prediction Accuracy = 60.462%, Loss = 0.6517759799957276
Epoch: 9796, Batch Gradient Norm: 20.656476791811734
Epoch: 9796, Batch Gradient Norm after: 20.656476791811734
Epoch 9797/10000, Prediction Accuracy = 60.362%, Loss = 0.6541108965873719
Epoch: 9797, Batch Gradient Norm: 20.310119067784726
Epoch: 9797, Batch Gradient Norm after: 20.270413415504855
Epoch 9798/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6530466318130493
Epoch: 9798, Batch Gradient Norm: 20.13615620169295
Epoch: 9798, Batch Gradient Norm after: 20.13615620169295
Epoch 9799/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.652634859085083
Epoch: 9799, Batch Gradient Norm: 19.52327730106873
Epoch: 9799, Batch Gradient Norm after: 19.52327730106873
Epoch 9800/10000, Prediction Accuracy = 60.464%, Loss = 0.6506560564041137
Epoch: 9800, Batch Gradient Norm: 19.385600760194958
Epoch: 9800, Batch Gradient Norm after: 19.385600760194958
Epoch 9801/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.6503487586975097
Epoch: 9801, Batch Gradient Norm: 19.161549012102085
Epoch: 9801, Batch Gradient Norm after: 19.161549012102085
Epoch 9802/10000, Prediction Accuracy = 60.412%, Loss = 0.6496081709861755
Epoch: 9802, Batch Gradient Norm: 19.604925044964453
Epoch: 9802, Batch Gradient Norm after: 19.604925044964453
Epoch 9803/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6509613156318664
Epoch: 9803, Batch Gradient Norm: 19.851378659854095
Epoch: 9803, Batch Gradient Norm after: 19.851378659854095
Epoch 9804/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6516029000282287
Epoch: 9804, Batch Gradient Norm: 20.375596403758298
Epoch: 9804, Batch Gradient Norm after: 20.375596403758298
Epoch 9805/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6531427502632141
Epoch: 9805, Batch Gradient Norm: 20.232722338548143
Epoch: 9805, Batch Gradient Norm after: 20.232722338548143
Epoch 9806/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6526015996932983
Epoch: 9806, Batch Gradient Norm: 20.155405417214887
Epoch: 9806, Batch Gradient Norm after: 20.155405417214887
Epoch 9807/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.6524522304534912
Epoch: 9807, Batch Gradient Norm: 19.57769037255465
Epoch: 9807, Batch Gradient Norm after: 19.57769037255465
Epoch 9808/10000, Prediction Accuracy = 60.46%, Loss = 0.6506899952888489
Epoch: 9808, Batch Gradient Norm: 19.401075116958623
Epoch: 9808, Batch Gradient Norm after: 19.401075116958623
Epoch 9809/10000, Prediction Accuracy = 60.374%, Loss = 0.650362777709961
Epoch: 9809, Batch Gradient Norm: 19.083709627556885
Epoch: 9809, Batch Gradient Norm after: 19.083709627556885
Epoch 9810/10000, Prediction Accuracy = 60.476%, Loss = 0.6492980360984802
Epoch: 9810, Batch Gradient Norm: 19.437453069500993
Epoch: 9810, Batch Gradient Norm after: 19.437453069500993
Epoch 9811/10000, Prediction Accuracy = 60.364%, Loss = 0.6503522753715515
Epoch: 9811, Batch Gradient Norm: 19.675288057430393
Epoch: 9811, Batch Gradient Norm after: 19.675288057430393
Epoch 9812/10000, Prediction Accuracy = 60.46%, Loss = 0.6508261799812317
Epoch: 9812, Batch Gradient Norm: 20.377475000486363
Epoch: 9812, Batch Gradient Norm after: 20.377475000486363
Epoch 9813/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.6529291152954102
Epoch: 9813, Batch Gradient Norm: 20.43250204464256
Epoch: 9813, Batch Gradient Norm after: 20.43250204464256
Epoch 9814/10000, Prediction Accuracy = 60.396%, Loss = 0.6530847907066345
Epoch: 9814, Batch Gradient Norm: 20.43181116544343
Epoch: 9814, Batch Gradient Norm after: 20.43181116544343
Epoch 9815/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.6531942367553711
Epoch: 9815, Batch Gradient Norm: 19.7185030200917
Epoch: 9815, Batch Gradient Norm after: 19.7185030200917
Epoch 9816/10000, Prediction Accuracy = 60.398%, Loss = 0.6510247588157654
Epoch: 9816, Batch Gradient Norm: 19.309532596134147
Epoch: 9816, Batch Gradient Norm after: 19.309532596134147
Epoch 9817/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6499043941497803
Epoch: 9817, Batch Gradient Norm: 18.837415928242127
Epoch: 9817, Batch Gradient Norm after: 18.837415928242127
Epoch 9818/10000, Prediction Accuracy = 60.398%, Loss = 0.6484177589416504
Epoch: 9818, Batch Gradient Norm: 19.176638076361105
Epoch: 9818, Batch Gradient Norm after: 19.176638076361105
Epoch 9819/10000, Prediction Accuracy = 60.378%, Loss = 0.6494544863700866
Epoch: 9819, Batch Gradient Norm: 19.493782626717287
Epoch: 9819, Batch Gradient Norm after: 19.493782626717287
Epoch 9820/10000, Prediction Accuracy = 60.464%, Loss = 0.6502407908439636
Epoch: 9820, Batch Gradient Norm: 20.3262318852737
Epoch: 9820, Batch Gradient Norm after: 20.3262318852737
Epoch 9821/10000, Prediction Accuracy = 60.384%, Loss = 0.6527567148208618
Epoch: 9821, Batch Gradient Norm: 20.514595396055
Epoch: 9821, Batch Gradient Norm after: 20.514595396055
Epoch 9822/10000, Prediction Accuracy = 60.488%, Loss = 0.6531634211540223
Epoch: 9822, Batch Gradient Norm: 20.64133290268306
Epoch: 9822, Batch Gradient Norm after: 20.64133290268306
Epoch 9823/10000, Prediction Accuracy = 60.342%, Loss = 0.6535871624946594
Epoch: 9823, Batch Gradient Norm: 19.97101618381105
Epoch: 9823, Batch Gradient Norm after: 19.97101618381105
Epoch 9824/10000, Prediction Accuracy = 60.462%, Loss = 0.651521635055542
Epoch: 9824, Batch Gradient Norm: 19.5298984939978
Epoch: 9824, Batch Gradient Norm after: 19.5298984939978
Epoch 9825/10000, Prediction Accuracy = 60.39%, Loss = 0.6503780961036683
Epoch: 9825, Batch Gradient Norm: 18.905691971030954
Epoch: 9825, Batch Gradient Norm after: 18.905691971030954
Epoch 9826/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6485744595527649
Epoch: 9826, Batch Gradient Norm: 18.97525782138073
Epoch: 9826, Batch Gradient Norm after: 18.97525782138073
Epoch 9827/10000, Prediction Accuracy = 60.42%, Loss = 0.6488307476043701
Epoch: 9827, Batch Gradient Norm: 19.110477661097637
Epoch: 9827, Batch Gradient Norm after: 19.110477661097637
Epoch 9828/10000, Prediction Accuracy = 60.366%, Loss = 0.6490259647369385
Epoch: 9828, Batch Gradient Norm: 19.91587678599989
Epoch: 9828, Batch Gradient Norm after: 19.91587678599989
Epoch 9829/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6513457536697388
Epoch: 9829, Batch Gradient Norm: 20.35701576161734
Epoch: 9829, Batch Gradient Norm after: 20.35701576161734
Epoch 9830/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.6525241494178772
Epoch: 9830, Batch Gradient Norm: 20.829819541460882
Epoch: 9830, Batch Gradient Norm after: 20.829819541460882
Epoch 9831/10000, Prediction Accuracy = 60.382000000000005%, Loss = 0.6540832757949829
Epoch: 9831, Batch Gradient Norm: 20.203363770658992
Epoch: 9831, Batch Gradient Norm after: 20.18054151737793
Epoch 9832/10000, Prediction Accuracy = 60.486000000000004%, Loss = 0.6521330118179322
Epoch: 9832, Batch Gradient Norm: 19.712331307190848
Epoch: 9832, Batch Gradient Norm after: 19.712331307190848
Epoch 9833/10000, Prediction Accuracy = 60.372%, Loss = 0.6508103728294372
Epoch: 9833, Batch Gradient Norm: 18.959288541323236
Epoch: 9833, Batch Gradient Norm after: 18.959288541323236
Epoch 9834/10000, Prediction Accuracy = 60.464%, Loss = 0.6484821319580079
Epoch: 9834, Batch Gradient Norm: 18.97476497543211
Epoch: 9834, Batch Gradient Norm after: 18.97476497543211
Epoch 9835/10000, Prediction Accuracy = 60.366%, Loss = 0.6485957264900207
Epoch: 9835, Batch Gradient Norm: 19.08607198419931
Epoch: 9835, Batch Gradient Norm after: 19.08607198419931
Epoch 9836/10000, Prediction Accuracy = 60.396%, Loss = 0.6487764239311218
Epoch: 9836, Batch Gradient Norm: 19.920558380282326
Epoch: 9836, Batch Gradient Norm after: 19.920558380282326
Epoch 9837/10000, Prediction Accuracy = 60.419999999999995%, Loss = 0.6512575626373291
Epoch: 9837, Batch Gradient Norm: 20.326419916248376
Epoch: 9837, Batch Gradient Norm after: 20.326419916248376
Epoch 9838/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.6524075150489808
Epoch: 9838, Batch Gradient Norm: 20.70961892116097
Epoch: 9838, Batch Gradient Norm after: 20.70961892116097
Epoch 9839/10000, Prediction Accuracy = 60.4%, Loss = 0.6535841941833496
Epoch: 9839, Batch Gradient Norm: 20.211293557870142
Epoch: 9839, Batch Gradient Norm after: 20.211293557870142
Epoch 9840/10000, Prediction Accuracy = 60.398%, Loss = 0.6519531726837158
Epoch: 9840, Batch Gradient Norm: 19.807057019690976
Epoch: 9840, Batch Gradient Norm after: 19.807057019690976
Epoch 9841/10000, Prediction Accuracy = 60.395999999999994%, Loss = 0.6508827924728393
Epoch: 9841, Batch Gradient Norm: 19.061297653114032
Epoch: 9841, Batch Gradient Norm after: 19.061297653114032
Epoch 9842/10000, Prediction Accuracy = 60.48%, Loss = 0.648659598827362
Epoch: 9842, Batch Gradient Norm: 18.99517603526395
Epoch: 9842, Batch Gradient Norm after: 18.99517603526395
Epoch 9843/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6486285567283631
Epoch: 9843, Batch Gradient Norm: 18.933673388212632
Epoch: 9843, Batch Gradient Norm after: 18.933673388212632
Epoch 9844/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.6482678890228272
Epoch: 9844, Batch Gradient Norm: 19.639095763902837
Epoch: 9844, Batch Gradient Norm after: 19.639095763902837
Epoch 9845/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.6502831459045411
Epoch: 9845, Batch Gradient Norm: 20.093029863955074
Epoch: 9845, Batch Gradient Norm after: 20.093029863955074
Epoch 9846/10000, Prediction Accuracy = 60.45%, Loss = 0.6514453053474426
Epoch: 9846, Batch Gradient Norm: 20.752502760211417
Epoch: 9846, Batch Gradient Norm after: 20.752502760211417
Epoch 9847/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.653514814376831
Epoch: 9847, Batch Gradient Norm: 20.270790080790693
Epoch: 9847, Batch Gradient Norm after: 20.237405405456265
Epoch 9848/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6520591020584107
Epoch: 9848, Batch Gradient Norm: 19.96570475502277
Epoch: 9848, Batch Gradient Norm after: 19.96570475502277
Epoch 9849/10000, Prediction Accuracy = 60.438%, Loss = 0.6512698769569397
Epoch: 9849, Batch Gradient Norm: 19.25895091053882
Epoch: 9849, Batch Gradient Norm after: 19.25895091053882
Epoch 9850/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.649112093448639
Epoch: 9850, Batch Gradient Norm: 19.101119696925412
Epoch: 9850, Batch Gradient Norm after: 19.101119696925412
Epoch 9851/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.6487149715423584
Epoch: 9851, Batch Gradient Norm: 18.948446353510917
Epoch: 9851, Batch Gradient Norm after: 18.948446353510917
Epoch 9852/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6481223583221436
Epoch: 9852, Batch Gradient Norm: 19.579977075781017
Epoch: 9852, Batch Gradient Norm after: 19.579977075781017
Epoch 9853/10000, Prediction Accuracy = 60.37800000000001%, Loss = 0.649998414516449
Epoch: 9853, Batch Gradient Norm: 19.954094025118888
Epoch: 9853, Batch Gradient Norm after: 19.954094025118888
Epoch 9854/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.6509938478469849
Epoch: 9854, Batch Gradient Norm: 20.54473269346525
Epoch: 9854, Batch Gradient Norm after: 20.54473269346525
Epoch 9855/10000, Prediction Accuracy = 60.366%, Loss = 0.6528385162353516
Epoch: 9855, Batch Gradient Norm: 20.324978791315193
Epoch: 9855, Batch Gradient Norm after: 20.324978791315193
Epoch 9856/10000, Prediction Accuracy = 60.480000000000004%, Loss = 0.6520035743713379
Epoch: 9856, Batch Gradient Norm: 20.12409712283235
Epoch: 9856, Batch Gradient Norm after: 20.12409712283235
Epoch 9857/10000, Prediction Accuracy = 60.35%, Loss = 0.6514971375465393
Epoch: 9857, Batch Gradient Norm: 19.41114155381457
Epoch: 9857, Batch Gradient Norm after: 19.41114155381457
Epoch 9858/10000, Prediction Accuracy = 60.4%, Loss = 0.6493375778198243
Epoch: 9858, Batch Gradient Norm: 19.214984786872826
Epoch: 9858, Batch Gradient Norm after: 19.214984786872826
Epoch 9859/10000, Prediction Accuracy = 60.42%, Loss = 0.6489180326461792
Epoch: 9859, Batch Gradient Norm: 18.928642455020903
Epoch: 9859, Batch Gradient Norm after: 18.928642455020903
Epoch 9860/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.648049533367157
Epoch: 9860, Batch Gradient Norm: 19.35665521419983
Epoch: 9860, Batch Gradient Norm after: 19.35665521419983
Epoch 9861/10000, Prediction Accuracy = 60.426%, Loss = 0.6492707133293152
Epoch: 9861, Batch Gradient Norm: 19.63343683807625
Epoch: 9861, Batch Gradient Norm after: 19.63343683807625
Epoch 9862/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6498912930488586
Epoch: 9862, Batch Gradient Norm: 20.32341290422168
Epoch: 9862, Batch Gradient Norm after: 20.32341290422168
Epoch 9863/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.651948606967926
Epoch: 9863, Batch Gradient Norm: 20.369035288722213
Epoch: 9863, Batch Gradient Norm after: 20.369035288722213
Epoch 9864/10000, Prediction Accuracy = 60.484%, Loss = 0.6520045161247253
Epoch: 9864, Batch Gradient Norm: 20.37309169806773
Epoch: 9864, Batch Gradient Norm after: 20.37309169806773
Epoch 9865/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6521945476531983
Epoch: 9865, Batch Gradient Norm: 19.65984604729228
Epoch: 9865, Batch Gradient Norm after: 19.65984604729228
Epoch 9866/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.6499690413475037
Epoch: 9866, Batch Gradient Norm: 19.31434868926834
Epoch: 9866, Batch Gradient Norm after: 19.31434868926834
Epoch 9867/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6490596055984497
Epoch: 9867, Batch Gradient Norm: 18.818646749790986
Epoch: 9867, Batch Gradient Norm after: 18.818646749790986
Epoch 9868/10000, Prediction Accuracy = 60.45200000000001%, Loss = 0.6474902987480163
Epoch: 9868, Batch Gradient Norm: 19.151097272068963
Epoch: 9868, Batch Gradient Norm after: 19.151097272068963
Epoch 9869/10000, Prediction Accuracy = 60.372%, Loss = 0.648484742641449
Epoch: 9869, Batch Gradient Norm: 19.459857199026754
Epoch: 9869, Batch Gradient Norm after: 19.459857199026754
Epoch 9870/10000, Prediction Accuracy = 60.416%, Loss = 0.6492657899856568
Epoch: 9870, Batch Gradient Norm: 20.282748797032934
Epoch: 9870, Batch Gradient Norm after: 20.282748797032934
Epoch 9871/10000, Prediction Accuracy = 60.436%, Loss = 0.6517713308334351
Epoch: 9871, Batch Gradient Norm: 20.418415841266246
Epoch: 9871, Batch Gradient Norm after: 20.418415841266246
Epoch 9872/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.6521037817001343
Epoch: 9872, Batch Gradient Norm: 20.497460540852117
Epoch: 9872, Batch Gradient Norm after: 20.497460540852117
Epoch 9873/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.6523440480232239
Epoch: 9873, Batch Gradient Norm: 19.792114024234984
Epoch: 9873, Batch Gradient Norm after: 19.792114024234984
Epoch 9874/10000, Prediction Accuracy = 60.402%, Loss = 0.6501471877098084
Epoch: 9874, Batch Gradient Norm: 19.39982402918386
Epoch: 9874, Batch Gradient Norm after: 19.39982402918386
Epoch 9875/10000, Prediction Accuracy = 60.4%, Loss = 0.6491462349891662
Epoch: 9875, Batch Gradient Norm: 18.823915805382537
Epoch: 9875, Batch Gradient Norm after: 18.823915805382537
Epoch 9876/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.6474315166473389
Epoch: 9876, Batch Gradient Norm: 19.025128801623516
Epoch: 9876, Batch Gradient Norm after: 19.025128801623516
Epoch 9877/10000, Prediction Accuracy = 60.398%, Loss = 0.6481353759765625
Epoch: 9877, Batch Gradient Norm: 19.187445109652884
Epoch: 9877, Batch Gradient Norm after: 19.187445109652884
Epoch 9878/10000, Prediction Accuracy = 60.488%, Loss = 0.6483771324157714
Epoch: 9878, Batch Gradient Norm: 20.0193318524122
Epoch: 9878, Batch Gradient Norm after: 20.0193318524122
Epoch 9879/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6507822632789612
Epoch: 9879, Batch Gradient Norm: 20.36293615035104
Epoch: 9879, Batch Gradient Norm after: 20.36293615035104
Epoch 9880/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.6516718626022339
Epoch: 9880, Batch Gradient Norm: 20.736182982079516
Epoch: 9880, Batch Gradient Norm after: 20.736182982079516
Epoch 9881/10000, Prediction Accuracy = 60.426%, Loss = 0.6529115319252015
Epoch: 9881, Batch Gradient Norm: 20.16258555142603
Epoch: 9881, Batch Gradient Norm after: 20.16258555142603
Epoch 9882/10000, Prediction Accuracy = 60.412%, Loss = 0.6511943936347961
Epoch: 9882, Batch Gradient Norm: 19.661018310662747
Epoch: 9882, Batch Gradient Norm after: 19.661018310662747
Epoch 9883/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6498043775558472
Epoch: 9883, Batch Gradient Norm: 18.82406860077929
Epoch: 9883, Batch Gradient Norm after: 18.82406860077929
Epoch 9884/10000, Prediction Accuracy = 60.374%, Loss = 0.6472985029220581
Epoch: 9884, Batch Gradient Norm: 18.734148127127643
Epoch: 9884, Batch Gradient Norm after: 18.734148127127643
Epoch 9885/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.647119653224945
Epoch: 9885, Batch Gradient Norm: 18.78562539302183
Epoch: 9885, Batch Gradient Norm after: 18.78562539302183
Epoch 9886/10000, Prediction Accuracy = 60.46200000000001%, Loss = 0.6470886588096618
Epoch: 9886, Batch Gradient Norm: 19.685807207730786
Epoch: 9886, Batch Gradient Norm after: 19.685807207730786
Epoch 9887/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.6497433900833129
Epoch: 9887, Batch Gradient Norm: 20.240475533197746
Epoch: 9887, Batch Gradient Norm after: 20.240475533197746
Epoch 9888/10000, Prediction Accuracy = 60.486000000000004%, Loss = 0.6512595295906067
Epoch: 9888, Batch Gradient Norm: 20.88022207970499
Epoch: 9888, Batch Gradient Norm after: 20.88022207970499
Epoch 9889/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.6532453536987305
Epoch: 9889, Batch Gradient Norm: 20.185899942996794
Epoch: 9889, Batch Gradient Norm after: 20.142213065327788
Epoch 9890/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.6510321974754334
Epoch: 9890, Batch Gradient Norm: 19.795979793821402
Epoch: 9890, Batch Gradient Norm after: 19.795979793821402
Epoch 9891/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.6499563574790954
Epoch: 9891, Batch Gradient Norm: 19.124976133215068
Epoch: 9891, Batch Gradient Norm after: 19.124976133215068
Epoch 9892/10000, Prediction Accuracy = 60.414%, Loss = 0.6479702591896057
Epoch: 9892, Batch Gradient Norm: 19.07868806231623
Epoch: 9892, Batch Gradient Norm after: 19.07868806231623
Epoch 9893/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6479713201522828
Epoch: 9893, Batch Gradient Norm: 18.985420836034567
Epoch: 9893, Batch Gradient Norm after: 18.985420836034567
Epoch 9894/10000, Prediction Accuracy = 60.414%, Loss = 0.6476199150085449
Epoch: 9894, Batch Gradient Norm: 19.585082539974362
Epoch: 9894, Batch Gradient Norm after: 19.585082539974362
Epoch 9895/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6493235349655151
Epoch: 9895, Batch Gradient Norm: 19.892795379637032
Epoch: 9895, Batch Gradient Norm after: 19.892795379637032
Epoch 9896/10000, Prediction Accuracy = 60.40200000000001%, Loss = 0.6500495553016663
Epoch: 9896, Batch Gradient Norm: 20.450029314923366
Epoch: 9896, Batch Gradient Norm after: 20.450029314923366
Epoch 9897/10000, Prediction Accuracy = 60.406000000000006%, Loss = 0.6517692327499389
Epoch: 9897, Batch Gradient Norm: 20.24342290088312
Epoch: 9897, Batch Gradient Norm after: 20.24342290088312
Epoch 9898/10000, Prediction Accuracy = 60.488%, Loss = 0.6510867953300477
Epoch: 9898, Batch Gradient Norm: 20.064600344108023
Epoch: 9898, Batch Gradient Norm after: 20.064600344108023
Epoch 9899/10000, Prediction Accuracy = 60.412%, Loss = 0.6507401466369629
Epoch: 9899, Batch Gradient Norm: 19.298724759399796
Epoch: 9899, Batch Gradient Norm after: 19.298724759399796
Epoch 9900/10000, Prediction Accuracy = 60.483999999999995%, Loss = 0.6483652949333191
Epoch: 9900, Batch Gradient Norm: 19.06351927834574
Epoch: 9900, Batch Gradient Norm after: 19.06351927834574
Epoch 9901/10000, Prediction Accuracy = 60.39%, Loss = 0.64776109457016
Epoch: 9901, Batch Gradient Norm: 18.801826677613928
Epoch: 9901, Batch Gradient Norm after: 18.801826677613928
Epoch 9902/10000, Prediction Accuracy = 60.46%, Loss = 0.6468435764312744
Epoch: 9902, Batch Gradient Norm: 19.358955959419443
Epoch: 9902, Batch Gradient Norm after: 19.358955959419443
Epoch 9903/10000, Prediction Accuracy = 60.416%, Loss = 0.6485024213790893
Epoch: 9903, Batch Gradient Norm: 19.770051726357824
Epoch: 9903, Batch Gradient Norm after: 19.770051726357824
Epoch 9904/10000, Prediction Accuracy = 60.408%, Loss = 0.6496068358421325
Epoch: 9904, Batch Gradient Norm: 20.498325091999064
Epoch: 9904, Batch Gradient Norm after: 20.498325091999064
Epoch 9905/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6518517732620239
Epoch: 9905, Batch Gradient Norm: 20.390719209380133
Epoch: 9905, Batch Gradient Norm after: 20.390719209380133
Epoch 9906/10000, Prediction Accuracy = 60.398%, Loss = 0.6514325737953186
Epoch: 9906, Batch Gradient Norm: 20.192941505924722
Epoch: 9906, Batch Gradient Norm after: 20.192941505924722
Epoch 9907/10000, Prediction Accuracy = 60.406000000000006%, Loss = 0.6508815169334412
Epoch: 9907, Batch Gradient Norm: 19.395067920432684
Epoch: 9907, Batch Gradient Norm after: 19.395067920432684
Epoch 9908/10000, Prediction Accuracy = 60.448%, Loss = 0.6484260082244873
Epoch: 9908, Batch Gradient Norm: 19.088009153751
Epoch: 9908, Batch Gradient Norm after: 19.088009153751
Epoch 9909/10000, Prediction Accuracy = 60.38399999999999%, Loss = 0.6477113366127014
Epoch: 9909, Batch Gradient Norm: 18.737276393131538
Epoch: 9909, Batch Gradient Norm after: 18.737276393131538
Epoch 9910/10000, Prediction Accuracy = 60.492%, Loss = 0.646610951423645
Epoch: 9910, Batch Gradient Norm: 19.180929754684264
Epoch: 9910, Batch Gradient Norm after: 19.180929754684264
Epoch 9911/10000, Prediction Accuracy = 60.398%, Loss = 0.6479789614677429
Epoch: 9911, Batch Gradient Norm: 19.50236662734539
Epoch: 9911, Batch Gradient Norm after: 19.50236662734539
Epoch 9912/10000, Prediction Accuracy = 60.519999999999996%, Loss = 0.6486687898635864
Epoch: 9912, Batch Gradient Norm: 20.312780215664965
Epoch: 9912, Batch Gradient Norm after: 20.312780215664965
Epoch 9913/10000, Prediction Accuracy = 60.376%, Loss = 0.6510755181312561
Epoch: 9913, Batch Gradient Norm: 20.447201487616383
Epoch: 9913, Batch Gradient Norm after: 20.447201487616383
Epoch 9914/10000, Prediction Accuracy = 60.424%, Loss = 0.6513738870620728
Epoch: 9914, Batch Gradient Norm: 20.50203216365272
Epoch: 9914, Batch Gradient Norm after: 20.50203216365272
Epoch 9915/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6516851902008056
Epoch: 9915, Batch Gradient Norm: 19.74952477740785
Epoch: 9915, Batch Gradient Norm after: 19.74952477740785
Epoch 9916/10000, Prediction Accuracy = 60.42%, Loss = 0.6494221210479736
Epoch: 9916, Batch Gradient Norm: 19.251357273827992
Epoch: 9916, Batch Gradient Norm after: 19.251357273827992
Epoch 9917/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6480727434158325
Epoch: 9917, Batch Gradient Norm: 18.61640178843625
Epoch: 9917, Batch Gradient Norm after: 18.61640178843625
Epoch 9918/10000, Prediction Accuracy = 60.366%, Loss = 0.6461248755455017
Epoch: 9918, Batch Gradient Norm: 18.82072636567188
Epoch: 9918, Batch Gradient Norm after: 18.82072636567188
Epoch 9919/10000, Prediction Accuracy = 60.402%, Loss = 0.6467513322830201
Epoch: 9919, Batch Gradient Norm: 19.11613543075625
Epoch: 9919, Batch Gradient Norm after: 19.11613543075625
Epoch 9920/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.6474140286445618
Epoch: 9920, Batch Gradient Norm: 20.084621594302753
Epoch: 9920, Batch Gradient Norm after: 20.084621594302753
Epoch 9921/10000, Prediction Accuracy = 60.386%, Loss = 0.6503286600112915
Epoch: 9921, Batch Gradient Norm: 20.47329732059606
Epoch: 9921, Batch Gradient Norm after: 20.47329732059606
Epoch 9922/10000, Prediction Accuracy = 60.483999999999995%, Loss = 0.6513830423355103
Epoch: 9922, Batch Gradient Norm: 20.738543444060028
Epoch: 9922, Batch Gradient Norm after: 20.738543444060028
Epoch 9923/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6522780060768127
Epoch: 9923, Batch Gradient Norm: 20.051068489717238
Epoch: 9923, Batch Gradient Norm after: 20.051068489717238
Epoch 9924/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.6500535845756531
Epoch: 9924, Batch Gradient Norm: 19.52131666049301
Epoch: 9924, Batch Gradient Norm after: 19.52131666049301
Epoch 9925/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.6486202001571655
Epoch: 9925, Batch Gradient Norm: 18.735321165060462
Epoch: 9925, Batch Gradient Norm after: 18.735321165060462
Epoch 9926/10000, Prediction Accuracy = 60.426%, Loss = 0.6463114023208618
Epoch: 9926, Batch Gradient Norm: 18.75138606811491
Epoch: 9926, Batch Gradient Norm after: 18.75138606811491
Epoch 9927/10000, Prediction Accuracy = 60.408%, Loss = 0.6464896559715271
Epoch: 9927, Batch Gradient Norm: 18.848382342392206
Epoch: 9927, Batch Gradient Norm after: 18.848382342392206
Epoch 9928/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6466422557830811
Epoch: 9928, Batch Gradient Norm: 19.71068967348118
Epoch: 9928, Batch Gradient Norm after: 19.71068967348118
Epoch 9929/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6490981459617615
Epoch: 9929, Batch Gradient Norm: 20.1948253101806
Epoch: 9929, Batch Gradient Norm after: 20.1948253101806
Epoch 9930/10000, Prediction Accuracy = 60.414%, Loss = 0.6503557562828064
Epoch: 9930, Batch Gradient Norm: 20.753828984130323
Epoch: 9930, Batch Gradient Norm after: 20.753828984130323
Epoch 9931/10000, Prediction Accuracy = 60.426%, Loss = 0.6521247148513794
Epoch: 9931, Batch Gradient Norm: 20.259094442915387
Epoch: 9931, Batch Gradient Norm after: 20.247553919341225
Epoch 9932/10000, Prediction Accuracy = 60.482000000000006%, Loss = 0.6505814313888549
Epoch: 9932, Batch Gradient Norm: 19.808091431130162
Epoch: 9932, Batch Gradient Norm after: 19.808091431130162
Epoch 9933/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.6494423151016235
Epoch: 9933, Batch Gradient Norm: 18.929493260340266
Epoch: 9933, Batch Gradient Norm after: 18.929493260340266
Epoch 9934/10000, Prediction Accuracy = 60.5%, Loss = 0.6467459917068481
Epoch: 9934, Batch Gradient Norm: 18.765810833631267
Epoch: 9934, Batch Gradient Norm after: 18.765810833631267
Epoch 9935/10000, Prediction Accuracy = 60.4%, Loss = 0.6463538646697998
Epoch: 9935, Batch Gradient Norm: 18.674876426730417
Epoch: 9935, Batch Gradient Norm after: 18.674876426730417
Epoch 9936/10000, Prediction Accuracy = 60.480000000000004%, Loss = 0.6459087252616882
Epoch: 9936, Batch Gradient Norm: 19.473788735158312
Epoch: 9936, Batch Gradient Norm after: 19.473788735158312
Epoch 9937/10000, Prediction Accuracy = 60.43399999999999%, Loss = 0.6482395052909851
Epoch: 9937, Batch Gradient Norm: 20.035575155839386
Epoch: 9937, Batch Gradient Norm after: 20.035575155839386
Epoch 9938/10000, Prediction Accuracy = 60.4%, Loss = 0.64981130361557
Epoch: 9938, Batch Gradient Norm: 20.7380414952009
Epoch: 9938, Batch Gradient Norm after: 20.7380414952009
Epoch 9939/10000, Prediction Accuracy = 60.426%, Loss = 0.6520322680473327
Epoch: 9939, Batch Gradient Norm: 20.262255323001913
Epoch: 9939, Batch Gradient Norm after: 20.237731101910054
Epoch 9940/10000, Prediction Accuracy = 60.40400000000001%, Loss = 0.650492250919342
Epoch: 9940, Batch Gradient Norm: 19.889887891847202
Epoch: 9940, Batch Gradient Norm after: 19.889887891847202
Epoch 9941/10000, Prediction Accuracy = 60.41399999999999%, Loss = 0.6494220495223999
Epoch: 9941, Batch Gradient Norm: 19.104019959392385
Epoch: 9941, Batch Gradient Norm after: 19.104019959392385
Epoch 9942/10000, Prediction Accuracy = 60.443999999999996%, Loss = 0.647031021118164
Epoch: 9942, Batch Gradient Norm: 18.966510233842456
Epoch: 9942, Batch Gradient Norm after: 18.966510233842456
Epoch 9943/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6467756628990173
Epoch: 9943, Batch Gradient Norm: 18.79695952545406
Epoch: 9943, Batch Gradient Norm after: 18.79695952545406
Epoch 9944/10000, Prediction Accuracy = 60.504%, Loss = 0.6461816191673279
Epoch: 9944, Batch Gradient Norm: 19.415456501072722
Epoch: 9944, Batch Gradient Norm after: 19.415456501072722
Epoch 9945/10000, Prediction Accuracy = 60.398%, Loss = 0.6480389356613159
Epoch: 9945, Batch Gradient Norm: 19.74944565747806
Epoch: 9945, Batch Gradient Norm after: 19.74944565747806
Epoch 9946/10000, Prediction Accuracy = 60.513999999999996%, Loss = 0.6488040804862976
Epoch: 9946, Batch Gradient Norm: 20.41415919672025
Epoch: 9946, Batch Gradient Norm after: 20.41415919672025
Epoch 9947/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6508155465126038
Epoch: 9947, Batch Gradient Norm: 20.285215985257658
Epoch: 9947, Batch Gradient Norm after: 20.285215985257658
Epoch 9948/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.6503441333770752
Epoch: 9948, Batch Gradient Norm: 20.17223775335087
Epoch: 9948, Batch Gradient Norm after: 20.17223775335087
Epoch 9949/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.6501510977745056
Epoch: 9949, Batch Gradient Norm: 19.394244930717033
Epoch: 9949, Batch Gradient Norm after: 19.394244930717033
Epoch 9950/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.6478395104408264
Epoch: 9950, Batch Gradient Norm: 19.062982464266018
Epoch: 9950, Batch Gradient Norm after: 19.062982464266018
Epoch 9951/10000, Prediction Accuracy = 60.424%, Loss = 0.6469486474990844
Epoch: 9951, Batch Gradient Norm: 18.641858832558295
Epoch: 9951, Batch Gradient Norm after: 18.641858832558295
Epoch 9952/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.6456072330474854
Epoch: 9952, Batch Gradient Norm: 19.0723645324235
Epoch: 9952, Batch Gradient Norm after: 19.0723645324235
Epoch 9953/10000, Prediction Accuracy = 60.398%, Loss = 0.6468446850776672
Epoch: 9953, Batch Gradient Norm: 19.425285416886428
Epoch: 9953, Batch Gradient Norm after: 19.425285416886428
Epoch 9954/10000, Prediction Accuracy = 60.512%, Loss = 0.6477230429649353
Epoch: 9954, Batch Gradient Norm: 20.29897731001966
Epoch: 9954, Batch Gradient Norm after: 20.29897731001966
Epoch 9955/10000, Prediction Accuracy = 60.4%, Loss = 0.650418472290039
Epoch: 9955, Batch Gradient Norm: 20.402662758682443
Epoch: 9955, Batch Gradient Norm after: 20.402662758682443
Epoch 9956/10000, Prediction Accuracy = 60.5%, Loss = 0.6506072878837585
Epoch: 9956, Batch Gradient Norm: 20.420839329056218
Epoch: 9956, Batch Gradient Norm after: 20.420839329056218
Epoch 9957/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6507463455200195
Epoch: 9957, Batch Gradient Norm: 19.613682700775808
Epoch: 9957, Batch Gradient Norm after: 19.613682700775808
Epoch 9958/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.6482117533683777
Epoch: 9958, Batch Gradient Norm: 19.218164427183876
Epoch: 9958, Batch Gradient Norm after: 19.218164427183876
Epoch 9959/10000, Prediction Accuracy = 60.407999999999994%, Loss = 0.647170889377594
Epoch: 9959, Batch Gradient Norm: 18.67202427377817
Epoch: 9959, Batch Gradient Norm after: 18.67202427377817
Epoch 9960/10000, Prediction Accuracy = 60.428%, Loss = 0.6455662965774536
Epoch: 9960, Batch Gradient Norm: 18.96948245329888
Epoch: 9960, Batch Gradient Norm after: 18.96948245329888
Epoch 9961/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6464881777763367
Epoch: 9961, Batch Gradient Norm: 19.19289449653064
Epoch: 9961, Batch Gradient Norm after: 19.19289449653064
Epoch 9962/10000, Prediction Accuracy = 60.43000000000001%, Loss = 0.6470307588577271
Epoch: 9962, Batch Gradient Norm: 20.009306930620184
Epoch: 9962, Batch Gradient Norm after: 20.009306930620184
Epoch 9963/10000, Prediction Accuracy = 60.398%, Loss = 0.6493861317634583
Epoch: 9963, Batch Gradient Norm: 20.25136994233163
Epoch: 9963, Batch Gradient Norm after: 20.25136994233163
Epoch 9964/10000, Prediction Accuracy = 60.4%, Loss = 0.6499632000923157
Epoch: 9964, Batch Gradient Norm: 20.50831307120002
Epoch: 9964, Batch Gradient Norm after: 20.50831307120002
Epoch 9965/10000, Prediction Accuracy = 60.40400000000001%, Loss = 0.6508241891860962
Epoch: 9965, Batch Gradient Norm: 19.8944906093659
Epoch: 9965, Batch Gradient Norm after: 19.8944906093659
Epoch 9966/10000, Prediction Accuracy = 60.508%, Loss = 0.6489556074142456
Epoch: 9966, Batch Gradient Norm: 19.484764555940036
Epoch: 9966, Batch Gradient Norm after: 19.484764555940036
Epoch 9967/10000, Prediction Accuracy = 60.412%, Loss = 0.6479318022727967
Epoch: 9967, Batch Gradient Norm: 18.73656146635746
Epoch: 9967, Batch Gradient Norm after: 18.73656146635746
Epoch 9968/10000, Prediction Accuracy = 60.512%, Loss = 0.6456273794174194
Epoch: 9968, Batch Gradient Norm: 18.79932560950436
Epoch: 9968, Batch Gradient Norm after: 18.79932560950436
Epoch 9969/10000, Prediction Accuracy = 60.410000000000004%, Loss = 0.645848023891449
Epoch: 9969, Batch Gradient Norm: 18.879314757978776
Epoch: 9969, Batch Gradient Norm after: 18.879314757978776
Epoch 9970/10000, Prediction Accuracy = 60.452%, Loss = 0.6458928942680359
Epoch: 9970, Batch Gradient Norm: 19.772941001702883
Epoch: 9970, Batch Gradient Norm after: 19.772941001702883
Epoch 9971/10000, Prediction Accuracy = 60.44200000000001%, Loss = 0.6485304713249207
Epoch: 9971, Batch Gradient Norm: 20.21626254976588
Epoch: 9971, Batch Gradient Norm after: 20.21626254976588
Epoch 9972/10000, Prediction Accuracy = 60.42%, Loss = 0.6498053073883057
Epoch: 9972, Batch Gradient Norm: 20.676604026113328
Epoch: 9972, Batch Gradient Norm after: 20.676604026113328
Epoch 9973/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.6512755155563354
Epoch: 9973, Batch Gradient Norm: 20.123063229897184
Epoch: 9973, Batch Gradient Norm after: 20.123063229897184
Epoch 9974/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6495035648345947
Epoch: 9974, Batch Gradient Norm: 19.625522442406616
Epoch: 9974, Batch Gradient Norm after: 19.625522442406616
Epoch 9975/10000, Prediction Accuracy = 60.416%, Loss = 0.6480826258659362
Epoch: 9975, Batch Gradient Norm: 18.774830933810318
Epoch: 9975, Batch Gradient Norm after: 18.774830933810318
Epoch 9976/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.6455393075942993
Epoch: 9976, Batch Gradient Norm: 18.7527689110344
Epoch: 9976, Batch Gradient Norm after: 18.7527689110344
Epoch 9977/10000, Prediction Accuracy = 60.406000000000006%, Loss = 0.6456018209457397
Epoch: 9977, Batch Gradient Norm: 18.738508554643502
Epoch: 9977, Batch Gradient Norm after: 18.738508554643502
Epoch 9978/10000, Prediction Accuracy = 60.5%, Loss = 0.6454420447349548
Epoch: 9978, Batch Gradient Norm: 19.550274168264654
Epoch: 9978, Batch Gradient Norm after: 19.550274168264654
Epoch 9979/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.6478192806243896
Epoch: 9979, Batch Gradient Norm: 19.978762038659593
Epoch: 9979, Batch Gradient Norm after: 19.978762038659593
Epoch 9980/10000, Prediction Accuracy = 60.528000000000006%, Loss = 0.6488844990730286
Epoch: 9980, Batch Gradient Norm: 20.620214831613445
Epoch: 9980, Batch Gradient Norm after: 20.620214831613445
Epoch 9981/10000, Prediction Accuracy = 60.402%, Loss = 0.6508528470993042
Epoch: 9981, Batch Gradient Norm: 20.31863661557331
Epoch: 9981, Batch Gradient Norm after: 20.31863661557331
Epoch 9982/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.6498980164527893
Epoch: 9982, Batch Gradient Norm: 19.98113919089631
Epoch: 9982, Batch Gradient Norm after: 19.98113919089631
Epoch 9983/10000, Prediction Accuracy = 60.398%, Loss = 0.6490298390388489
Epoch: 9983, Batch Gradient Norm: 19.04793323884539
Epoch: 9983, Batch Gradient Norm after: 19.04793323884539
Epoch 9984/10000, Prediction Accuracy = 60.44%, Loss = 0.6462884545326233
Epoch: 9984, Batch Gradient Norm: 18.745170039816077
Epoch: 9984, Batch Gradient Norm after: 18.745170039816077
Epoch 9985/10000, Prediction Accuracy = 60.402%, Loss = 0.6454729318618775
Epoch: 9985, Batch Gradient Norm: 18.453865042739295
Epoch: 9985, Batch Gradient Norm after: 18.453865042739295
Epoch 9986/10000, Prediction Accuracy = 60.402%, Loss = 0.6444974422454834
Epoch: 9986, Batch Gradient Norm: 19.139216834971336
Epoch: 9986, Batch Gradient Norm after: 19.139216834971336
Epoch 9987/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6464344024658203
Epoch: 9987, Batch Gradient Norm: 19.675260512973985
Epoch: 9987, Batch Gradient Norm after: 19.675260512973985
Epoch 9988/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.647865891456604
Epoch: 9988, Batch Gradient Norm: 20.599679261025635
Epoch: 9988, Batch Gradient Norm after: 20.599679261025635
Epoch 9989/10000, Prediction Accuracy = 60.386%, Loss = 0.6507469296455384
Epoch: 9989, Batch Gradient Norm: 20.363425468839033
Epoch: 9989, Batch Gradient Norm after: 20.338981821322356
Epoch 9990/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.6499334454536438
Epoch: 9990, Batch Gradient Norm: 20.150568386010892
Epoch: 9990, Batch Gradient Norm after: 20.150568386010892
Epoch 9991/10000, Prediction Accuracy = 60.40599999999999%, Loss = 0.649372148513794
Epoch: 9991, Batch Gradient Norm: 19.305178605997558
Epoch: 9991, Batch Gradient Norm after: 19.305178605997558
Epoch 9992/10000, Prediction Accuracy = 60.468%, Loss = 0.6467494964599609
Epoch: 9992, Batch Gradient Norm: 19.054015112473323
Epoch: 9992, Batch Gradient Norm after: 19.054015112473323
Epoch 9993/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.6461233139038086
Epoch: 9993, Batch Gradient Norm: 18.670591829155644
Epoch: 9993, Batch Gradient Norm after: 18.670591829155644
Epoch 9994/10000, Prediction Accuracy = 60.408%, Loss = 0.6449931263923645
Epoch: 9994, Batch Gradient Norm: 19.158080745907853
Epoch: 9994, Batch Gradient Norm after: 19.158080745907853
Epoch 9995/10000, Prediction Accuracy = 60.426%, Loss = 0.6464429497718811
Epoch: 9995, Batch Gradient Norm: 19.43414278668858
Epoch: 9995, Batch Gradient Norm after: 19.43414278668858
Epoch 9996/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.6471376657485962
Epoch: 9996, Batch Gradient Norm: 20.17431345086118
Epoch: 9996, Batch Gradient Norm after: 20.17431345086118
Epoch 9997/10000, Prediction Accuracy = 60.416%, Loss = 0.6492897391319274
Epoch: 9997, Batch Gradient Norm: 20.187149269607808
Epoch: 9997, Batch Gradient Norm after: 20.187149269607808
Epoch 9998/10000, Prediction Accuracy = 60.428%, Loss = 0.6492011189460755
Epoch: 9998, Batch Gradient Norm: 20.251485079227876
Epoch: 9998, Batch Gradient Norm after: 20.251485079227876
Epoch 9999/10000, Prediction Accuracy = 60.398%, Loss = 0.6495226383209228
Epoch: 9999, Batch Gradient Norm: 19.54174639542019
Epoch: 9999, Batch Gradient Norm after: 19.54174639542019
Epoch 10000/10000, Prediction Accuracy = 60.517999999999994%, Loss = 0.647370207309723
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode20-dqn/classify_rsmProp2.py", line 215, in <module>
    'Norm after clipping' : total_norm_after,
    ^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/pyplot.py", line 607, in show
    return _get_backend_mod().show(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 3567, in show
    cls.mainloop()
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backends/backend_macosx.py", line 178, in start_main_loop
    with _allow_interrupt_macos():
  File "/Users/athmajanvivekananthan/miniconda3/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 1686, in _allow_interrupt
    old_sigint_handler(*handler_args)
KeyboardInterrupt