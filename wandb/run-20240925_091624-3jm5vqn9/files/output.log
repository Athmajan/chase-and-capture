Epoch: 0, Batch Gradient Norm: 82.44588402812344
Epoch: 0, Batch Gradient Norm after: 22.360678788194686
Epoch 1/10000, Prediction Accuracy = 21.851999999999997%, Loss = 60.03524551391602
Epoch: 1, Batch Gradient Norm: 83.21640906997673
Epoch: 1, Batch Gradient Norm after: 22.360679010273902
Epoch 2/10000, Prediction Accuracy = 25.824%, Loss = 54.371786499023436
Epoch: 2, Batch Gradient Norm: 84.43237595738387
Epoch: 2, Batch Gradient Norm after: 22.36067982972619
Epoch 3/10000, Prediction Accuracy = 26.953999999999997%, Loss = 51.24567413330078
Epoch: 3, Batch Gradient Norm: 85.06986874020711
Epoch: 3, Batch Gradient Norm after: 22.360678627680954
Epoch 4/10000, Prediction Accuracy = 27.372000000000003%, Loss = 48.91347961425781
Epoch: 4, Batch Gradient Norm: 85.23165645733249
Epoch: 4, Batch Gradient Norm after: 22.360677819801193
Epoch 5/10000, Prediction Accuracy = 27.546%, Loss = 47.01821594238281
Epoch: 5, Batch Gradient Norm: 85.05224961746504
Epoch: 5, Batch Gradient Norm after: 22.360678834733385
Epoch 6/10000, Prediction Accuracy = 27.29%, Loss = 45.40322265625
Epoch: 6, Batch Gradient Norm: 84.73956289956668
Epoch: 6, Batch Gradient Norm after: 22.36068056004703
Epoch 7/10000, Prediction Accuracy = 27.333999999999996%, Loss = 43.97728576660156
Epoch: 7, Batch Gradient Norm: 84.37044326328628
Epoch: 7, Batch Gradient Norm after: 22.360678278860085
Epoch 8/10000, Prediction Accuracy = 28.016%, Loss = 42.68446426391601
Epoch: 8, Batch Gradient Norm: 83.95088311788676
Epoch: 8, Batch Gradient Norm after: 22.360679562042712
Epoch 9/10000, Prediction Accuracy = 28.558%, Loss = 41.48708572387695
Epoch: 9, Batch Gradient Norm: 83.43895019969476
Epoch: 9, Batch Gradient Norm after: 22.360679646922918
Epoch 10/10000, Prediction Accuracy = 28.68%, Loss = 40.370478820800784
Epoch: 10, Batch Gradient Norm: 82.83913725236296
Epoch: 10, Batch Gradient Norm after: 22.360679369523083
Epoch 11/10000, Prediction Accuracy = 28.701999999999998%, Loss = 39.31968536376953
Epoch: 11, Batch Gradient Norm: 82.20569541226396
Epoch: 11, Batch Gradient Norm after: 22.360680158759592
Epoch 12/10000, Prediction Accuracy = 28.744%, Loss = 38.323858642578124
Epoch: 12, Batch Gradient Norm: 81.56813391434602
Epoch: 12, Batch Gradient Norm after: 22.360679164353833
Epoch 13/10000, Prediction Accuracy = 28.740000000000002%, Loss = 37.37000503540039
Epoch: 13, Batch Gradient Norm: 80.89889158065093
Epoch: 13, Batch Gradient Norm after: 22.360679384163817
Epoch 14/10000, Prediction Accuracy = 28.674%, Loss = 36.456616973876955
Epoch: 14, Batch Gradient Norm: 80.19226134061817
Epoch: 14, Batch Gradient Norm after: 22.36067921642041
Epoch 15/10000, Prediction Accuracy = 28.477999999999998%, Loss = 35.57712936401367
Epoch: 15, Batch Gradient Norm: 79.4496759328266
Epoch: 15, Batch Gradient Norm after: 22.360679552091458
Epoch 16/10000, Prediction Accuracy = 28.358000000000004%, Loss = 34.73377456665039
Epoch: 16, Batch Gradient Norm: 78.69532440021581
Epoch: 16, Batch Gradient Norm after: 22.360680142698595
Epoch 17/10000, Prediction Accuracy = 28.228%, Loss = 33.91889572143555
Epoch: 17, Batch Gradient Norm: 77.93536895196235
Epoch: 17, Batch Gradient Norm after: 22.360678796906527
Epoch 18/10000, Prediction Accuracy = 28.182%, Loss = 33.12924575805664
Epoch: 18, Batch Gradient Norm: 77.18003779740268
Epoch: 18, Batch Gradient Norm after: 22.360680036459215
Epoch 19/10000, Prediction Accuracy = 28.177999999999997%, Loss = 32.362266540527344
Epoch: 19, Batch Gradient Norm: 76.4499491932486
Epoch: 19, Batch Gradient Norm after: 22.36067894747661
Epoch 20/10000, Prediction Accuracy = 28.264%, Loss = 31.61056022644043
Epoch: 20, Batch Gradient Norm: 75.70763627059169
Epoch: 20, Batch Gradient Norm after: 22.360679696229504
Epoch 21/10000, Prediction Accuracy = 28.440000000000005%, Loss = 30.88236885070801
Epoch: 21, Batch Gradient Norm: 74.95174731415858
Epoch: 21, Batch Gradient Norm after: 22.360676949873305
Epoch 22/10000, Prediction Accuracy = 28.498%, Loss = 30.16645736694336
Epoch: 22, Batch Gradient Norm: 74.18424740176825
Epoch: 22, Batch Gradient Norm after: 22.360679133579104
Epoch 23/10000, Prediction Accuracy = 28.601999999999997%, Loss = 29.466263198852538
Epoch: 23, Batch Gradient Norm: 73.4282870331706
Epoch: 23, Batch Gradient Norm after: 22.360680538777398
Epoch 24/10000, Prediction Accuracy = 28.518%, Loss = 28.777127075195313
Epoch: 24, Batch Gradient Norm: 72.65611314086358
Epoch: 24, Batch Gradient Norm after: 22.360679357772714
Epoch 25/10000, Prediction Accuracy = 28.488%, Loss = 28.112113571166994
Epoch: 25, Batch Gradient Norm: 71.8833102788063
Epoch: 25, Batch Gradient Norm after: 22.360680218448714
Epoch 26/10000, Prediction Accuracy = 28.409999999999997%, Loss = 27.4408935546875
Epoch: 26, Batch Gradient Norm: 71.0828833251342
Epoch: 26, Batch Gradient Norm after: 22.360678442561962
Epoch 27/10000, Prediction Accuracy = 28.407999999999998%, Loss = 26.791239166259764
Epoch: 27, Batch Gradient Norm: 70.26675027616504
Epoch: 27, Batch Gradient Norm after: 22.360677754162793
Epoch 28/10000, Prediction Accuracy = 28.360000000000003%, Loss = 26.15364875793457
Epoch: 28, Batch Gradient Norm: 69.4237442289853
Epoch: 28, Batch Gradient Norm after: 22.360676410701082
Epoch 29/10000, Prediction Accuracy = 28.459999999999997%, Loss = 25.525707626342772
Epoch: 29, Batch Gradient Norm: 68.55844675074955
Epoch: 29, Batch Gradient Norm after: 22.36067902920158
Epoch 30/10000, Prediction Accuracy = 28.458%, Loss = 24.91735382080078
Epoch: 30, Batch Gradient Norm: 67.70146054540727
Epoch: 30, Batch Gradient Norm after: 22.36067834135649
Epoch 31/10000, Prediction Accuracy = 28.356%, Loss = 24.31921730041504
Epoch: 31, Batch Gradient Norm: 66.84367544954394
Epoch: 31, Batch Gradient Norm after: 22.360677777642618
Epoch 32/10000, Prediction Accuracy = 28.306%, Loss = 23.730224227905275
Epoch: 32, Batch Gradient Norm: 65.98972660138708
Epoch: 32, Batch Gradient Norm after: 22.36067902507571
Epoch 33/10000, Prediction Accuracy = 28.074%, Loss = 23.144923782348634
Epoch: 33, Batch Gradient Norm: 65.14996305284983
Epoch: 33, Batch Gradient Norm after: 22.360678327738686
Epoch 34/10000, Prediction Accuracy = 27.966%, Loss = 22.582339477539062
Epoch: 34, Batch Gradient Norm: 64.26768627104939
Epoch: 34, Batch Gradient Norm after: 22.360678510324668
Epoch 35/10000, Prediction Accuracy = 27.938%, Loss = 22.01775016784668
Epoch: 35, Batch Gradient Norm: 63.34622569501689
Epoch: 35, Batch Gradient Norm after: 22.360679220185993
Epoch 36/10000, Prediction Accuracy = 27.918%, Loss = 21.47163314819336
Epoch: 36, Batch Gradient Norm: 62.45817634309445
Epoch: 36, Batch Gradient Norm after: 22.360678184509602
Epoch 37/10000, Prediction Accuracy = 27.848000000000003%, Loss = 20.93219566345215
Epoch: 37, Batch Gradient Norm: 61.54786388753783
Epoch: 37, Batch Gradient Norm after: 22.36067877329529
Epoch 38/10000, Prediction Accuracy = 27.816000000000003%, Loss = 20.39809455871582
Epoch: 38, Batch Gradient Norm: 60.6352093368679
Epoch: 38, Batch Gradient Norm after: 22.360678488519635
Epoch 39/10000, Prediction Accuracy = 27.906%, Loss = 19.88058547973633
Epoch: 39, Batch Gradient Norm: 59.73116769512276
Epoch: 39, Batch Gradient Norm after: 22.36067952887869
Epoch 40/10000, Prediction Accuracy = 27.860000000000003%, Loss = 19.368480682373047
Epoch: 40, Batch Gradient Norm: 58.86103752703504
Epoch: 40, Batch Gradient Norm after: 22.360678110742548
Epoch 41/10000, Prediction Accuracy = 27.880000000000003%, Loss = 18.877971649169922
Epoch: 41, Batch Gradient Norm: 57.92338988722067
Epoch: 41, Batch Gradient Norm after: 22.360677120577403
Epoch 42/10000, Prediction Accuracy = 28.028000000000002%, Loss = 18.371788024902344
Epoch: 42, Batch Gradient Norm: 57.0650789890922
Epoch: 42, Batch Gradient Norm after: 22.360679158213518
Epoch 43/10000, Prediction Accuracy = 28.142000000000003%, Loss = 17.88767738342285
Epoch: 43, Batch Gradient Norm: 56.19533556435105
Epoch: 43, Batch Gradient Norm after: 22.36067732147046
Epoch 44/10000, Prediction Accuracy = 28.292%, Loss = 17.413633346557617
Epoch: 44, Batch Gradient Norm: 55.29575565029464
Epoch: 44, Batch Gradient Norm after: 22.3606770676953
Epoch 45/10000, Prediction Accuracy = 28.381999999999998%, Loss = 16.939616012573243
Epoch: 45, Batch Gradient Norm: 54.40113525017285
Epoch: 45, Batch Gradient Norm after: 22.360678752425244
Epoch 46/10000, Prediction Accuracy = 28.660000000000004%, Loss = 16.468970489501952
Epoch: 46, Batch Gradient Norm: 53.52316604035827
Epoch: 46, Batch Gradient Norm after: 22.360676747127698
Epoch 47/10000, Prediction Accuracy = 28.74%, Loss = 16.014496994018554
Epoch: 47, Batch Gradient Norm: 52.57760676602789
Epoch: 47, Batch Gradient Norm after: 22.360677019854442
Epoch 48/10000, Prediction Accuracy = 28.887999999999998%, Loss = 15.55854377746582
Epoch: 48, Batch Gradient Norm: 51.70621258547585
Epoch: 48, Batch Gradient Norm after: 22.360677818626854
Epoch 49/10000, Prediction Accuracy = 28.880000000000003%, Loss = 15.121191787719727
Epoch: 49, Batch Gradient Norm: 50.73263996684405
Epoch: 49, Batch Gradient Norm after: 22.360677011093234
Epoch 50/10000, Prediction Accuracy = 28.896000000000004%, Loss = 14.683049774169922
Epoch: 50, Batch Gradient Norm: 49.85091358840526
Epoch: 50, Batch Gradient Norm after: 22.360679238107387
Epoch 51/10000, Prediction Accuracy = 28.866000000000003%, Loss = 14.263479423522949
Epoch: 51, Batch Gradient Norm: 48.85906516771691
Epoch: 51, Batch Gradient Norm after: 22.360678092899818
Epoch 52/10000, Prediction Accuracy = 28.844%, Loss = 13.847883796691894
Epoch: 52, Batch Gradient Norm: 47.967656155443976
Epoch: 52, Batch Gradient Norm after: 22.360677231990632
Epoch 53/10000, Prediction Accuracy = 28.748%, Loss = 13.453353881835938
Epoch: 53, Batch Gradient Norm: 47.08492099550812
Epoch: 53, Batch Gradient Norm after: 22.360678678076262
Epoch 54/10000, Prediction Accuracy = 28.78%, Loss = 13.062131309509278
Epoch: 54, Batch Gradient Norm: 46.055597557696565
Epoch: 54, Batch Gradient Norm after: 22.36067738531713
Epoch 55/10000, Prediction Accuracy = 28.764%, Loss = 12.67395896911621
Epoch: 55, Batch Gradient Norm: 45.04696830280148
Epoch: 55, Batch Gradient Norm after: 22.360677331464384
Epoch 56/10000, Prediction Accuracy = 28.692%, Loss = 12.291155433654785
Epoch: 56, Batch Gradient Norm: 44.083184443686136
Epoch: 56, Batch Gradient Norm after: 22.36067816873981
Epoch 57/10000, Prediction Accuracy = 28.637999999999998%, Loss = 11.919439697265625
Epoch: 57, Batch Gradient Norm: 43.29762317309499
Epoch: 57, Batch Gradient Norm after: 22.36067804738004
Epoch 58/10000, Prediction Accuracy = 28.588%, Loss = 11.566899871826172
Epoch: 58, Batch Gradient Norm: 42.33744025765376
Epoch: 58, Batch Gradient Norm after: 22.360675943440256
Epoch 59/10000, Prediction Accuracy = 28.52%, Loss = 11.211148834228515
Epoch: 59, Batch Gradient Norm: 41.33034232018429
Epoch: 59, Batch Gradient Norm after: 22.36067635885601
Epoch 60/10000, Prediction Accuracy = 28.544%, Loss = 10.858909225463867
Epoch: 60, Batch Gradient Norm: 40.50562077849853
Epoch: 60, Batch Gradient Norm after: 22.3606791265277
Epoch 61/10000, Prediction Accuracy = 28.268%, Loss = 10.527608108520507
Epoch: 61, Batch Gradient Norm: 39.83664758939637
Epoch: 61, Batch Gradient Norm after: 22.36067654615301
Epoch 62/10000, Prediction Accuracy = 27.546%, Loss = 10.209404563903808
Epoch: 62, Batch Gradient Norm: 38.77722403532506
Epoch: 62, Batch Gradient Norm after: 22.360676819761103
Epoch 63/10000, Prediction Accuracy = 27.714%, Loss = 9.877896690368653
Epoch: 63, Batch Gradient Norm: 37.78724421742379
Epoch: 63, Batch Gradient Norm after: 22.360676589239016
Epoch 64/10000, Prediction Accuracy = 27.915999999999997%, Loss = 9.57269058227539
Epoch: 64, Batch Gradient Norm: 37.122526593778524
Epoch: 64, Batch Gradient Norm after: 22.360676985072914
Epoch 65/10000, Prediction Accuracy = 28.046%, Loss = 9.277371215820313
Epoch: 65, Batch Gradient Norm: 36.20456229747383
Epoch: 65, Batch Gradient Norm after: 22.360678478875723
Epoch 66/10000, Prediction Accuracy = 28.074%, Loss = 8.973234558105469
Epoch: 66, Batch Gradient Norm: 35.20140272668426
Epoch: 66, Batch Gradient Norm after: 22.360678244796254
Epoch 67/10000, Prediction Accuracy = 28.214%, Loss = 8.671067237854004
Epoch: 67, Batch Gradient Norm: 34.4289769588247
Epoch: 67, Batch Gradient Norm after: 22.360678440508075
Epoch 68/10000, Prediction Accuracy = 28.520000000000003%, Loss = 8.382116889953613
Epoch: 68, Batch Gradient Norm: 33.64677006765791
Epoch: 68, Batch Gradient Norm after: 22.360679077353186
Epoch 69/10000, Prediction Accuracy = 28.840000000000003%, Loss = 8.10708236694336
Epoch: 69, Batch Gradient Norm: 32.81404411147998
Epoch: 69, Batch Gradient Norm after: 22.36067828369669
Epoch 70/10000, Prediction Accuracy = 29.15%, Loss = 7.8344141960144045
Epoch: 70, Batch Gradient Norm: 32.096774686868486
Epoch: 70, Batch Gradient Norm after: 22.360677153331746
Epoch 71/10000, Prediction Accuracy = 29.316000000000003%, Loss = 7.582256603240967
Epoch: 71, Batch Gradient Norm: 31.244865328676863
Epoch: 71, Batch Gradient Norm after: 22.36067815926172
Epoch 72/10000, Prediction Accuracy = 29.590000000000003%, Loss = 7.331717777252197
Epoch: 72, Batch Gradient Norm: 30.701333155517226
Epoch: 72, Batch Gradient Norm after: 22.360677209779237
Epoch 73/10000, Prediction Accuracy = 29.538%, Loss = 7.101116752624511
Epoch: 73, Batch Gradient Norm: 30.19578058951088
Epoch: 73, Batch Gradient Norm after: 22.36067778476964
Epoch 74/10000, Prediction Accuracy = 29.7%, Loss = 6.873977375030518
Epoch: 74, Batch Gradient Norm: 29.090078474767317
Epoch: 74, Batch Gradient Norm after: 22.360679191194745
Epoch 75/10000, Prediction Accuracy = 29.736%, Loss = 6.632192993164063
Epoch: 75, Batch Gradient Norm: 28.28895050699638
Epoch: 75, Batch Gradient Norm after: 22.360677050740968
Epoch 76/10000, Prediction Accuracy = 30.042%, Loss = 6.40498456954956
Epoch: 76, Batch Gradient Norm: 28.146861441524113
Epoch: 76, Batch Gradient Norm after: 22.36067685299895
Epoch 77/10000, Prediction Accuracy = 30.262%, Loss = 6.214517402648926
Epoch: 77, Batch Gradient Norm: 27.04513719481042
Epoch: 77, Batch Gradient Norm after: 22.36067735155802
Epoch 78/10000, Prediction Accuracy = 30.464000000000006%, Loss = 5.9892580032348635
Epoch: 78, Batch Gradient Norm: 26.463322360040582
Epoch: 78, Batch Gradient Norm after: 22.360677499533033
Epoch 79/10000, Prediction Accuracy = 30.65%, Loss = 5.787647724151611
Epoch: 79, Batch Gradient Norm: 26.007483676164757
Epoch: 79, Batch Gradient Norm after: 22.360677581437752
Epoch 80/10000, Prediction Accuracy = 31.014%, Loss = 5.60947904586792
Epoch: 80, Batch Gradient Norm: 24.934906521690774
Epoch: 80, Batch Gradient Norm after: 22.360677266186872
Epoch 81/10000, Prediction Accuracy = 31.345999999999997%, Loss = 5.42162675857544
Epoch: 81, Batch Gradient Norm: 24.224756700224567
Epoch: 81, Batch Gradient Norm after: 22.360678804056512
Epoch 82/10000, Prediction Accuracy = 31.72%, Loss = 5.250434303283692
Epoch: 82, Batch Gradient Norm: 23.64489442242513
Epoch: 82, Batch Gradient Norm after: 22.360678546696672
Epoch 83/10000, Prediction Accuracy = 32.00600000000001%, Loss = 5.0786717414855955
Epoch: 83, Batch Gradient Norm: 23.75456725806792
Epoch: 83, Batch Gradient Norm after: 22.360678682810402
Epoch 84/10000, Prediction Accuracy = 32.37%, Loss = 4.941044425964355
Epoch: 84, Batch Gradient Norm: 22.52214742615822
Epoch: 84, Batch Gradient Norm after: 22.191681199139886
Epoch 85/10000, Prediction Accuracy = 32.82600000000001%, Loss = 4.759117603302002
Epoch: 85, Batch Gradient Norm: 22.19512939883121
Epoch: 85, Batch Gradient Norm after: 22.11611756017484
Epoch 86/10000, Prediction Accuracy = 33.262%, Loss = 4.617603969573975
Epoch: 86, Batch Gradient Norm: 21.84822810913762
Epoch: 86, Batch Gradient Norm after: 21.84822810913762
Epoch 87/10000, Prediction Accuracy = 33.63%, Loss = 4.488998794555664
Epoch: 87, Batch Gradient Norm: 20.99639647870025
Epoch: 87, Batch Gradient Norm after: 20.99639647870025
Epoch 88/10000, Prediction Accuracy = 34.11%, Loss = 4.350462341308594
Epoch: 88, Batch Gradient Norm: 20.40187888418072
Epoch: 88, Batch Gradient Norm after: 20.40187888418072
Epoch 89/10000, Prediction Accuracy = 34.510000000000005%, Loss = 4.226203346252442
Epoch: 89, Batch Gradient Norm: 19.6021700720877
Epoch: 89, Batch Gradient Norm after: 19.6021700720877
Epoch 90/10000, Prediction Accuracy = 34.834%, Loss = 4.10166072845459
Epoch: 90, Batch Gradient Norm: 19.6470797901865
Epoch: 90, Batch Gradient Norm after: 19.6470797901865
Epoch 91/10000, Prediction Accuracy = 35.19199999999999%, Loss = 4.011176824569702
Epoch: 91, Batch Gradient Norm: 19.06237571841315
Epoch: 91, Batch Gradient Norm after: 19.06237571841315
Epoch 92/10000, Prediction Accuracy = 35.474000000000004%, Loss = 3.8952757358551025
Epoch: 92, Batch Gradient Norm: 18.415272462871734
Epoch: 92, Batch Gradient Norm after: 18.415272462871734
Epoch 93/10000, Prediction Accuracy = 35.744%, Loss = 3.7908761978149412
Epoch: 93, Batch Gradient Norm: 18.02750957220498
Epoch: 93, Batch Gradient Norm after: 18.02750957220498
Epoch 94/10000, Prediction Accuracy = 36.007999999999996%, Loss = 3.7038378715515137
Epoch: 94, Batch Gradient Norm: 17.758968085073892
Epoch: 94, Batch Gradient Norm after: 17.758968085073892
Epoch 95/10000, Prediction Accuracy = 36.316%, Loss = 3.6144947052001952
Epoch: 95, Batch Gradient Norm: 18.08742760800337
Epoch: 95, Batch Gradient Norm after: 18.08742760800337
Epoch 96/10000, Prediction Accuracy = 36.577999999999996%, Loss = 3.549296569824219
Epoch: 96, Batch Gradient Norm: 17.39616872068654
Epoch: 96, Batch Gradient Norm after: 17.39616872068654
Epoch 97/10000, Prediction Accuracy = 36.79%, Loss = 3.4630425930023194
Epoch: 97, Batch Gradient Norm: 17.798094457229755
Epoch: 97, Batch Gradient Norm after: 17.798094457229755
Epoch 98/10000, Prediction Accuracy = 37.029999999999994%, Loss = 3.4023948669433595
Epoch: 98, Batch Gradient Norm: 16.969264669273837
Epoch: 98, Batch Gradient Norm after: 16.969264669273837
Epoch 99/10000, Prediction Accuracy = 37.344%, Loss = 3.3300789833068847
Epoch: 99, Batch Gradient Norm: 16.719741667801735
Epoch: 99, Batch Gradient Norm after: 16.719741667801735
Epoch 100/10000, Prediction Accuracy = 37.513999999999996%, Loss = 3.2524682521820067
Epoch: 100, Batch Gradient Norm: 16.287484126745646
Epoch: 100, Batch Gradient Norm after: 16.287484126745646
Epoch 101/10000, Prediction Accuracy = 37.682%, Loss = 3.1803681373596193
Epoch: 101, Batch Gradient Norm: 16.084941208151506
Epoch: 101, Batch Gradient Norm after: 16.084941208151506
Epoch 102/10000, Prediction Accuracy = 37.926%, Loss = 3.127784824371338
Epoch: 102, Batch Gradient Norm: 15.880066913419926
Epoch: 102, Batch Gradient Norm after: 15.880066913419926
Epoch 103/10000, Prediction Accuracy = 38.013999999999996%, Loss = 3.0586838722229004
Epoch: 103, Batch Gradient Norm: 15.368301223443046
Epoch: 103, Batch Gradient Norm after: 15.368301223443046
Epoch 104/10000, Prediction Accuracy = 38.208000000000006%, Loss = 2.9932769298553468
Epoch: 104, Batch Gradient Norm: 17.16903428409109
Epoch: 104, Batch Gradient Norm after: 17.16903428409109
Epoch 105/10000, Prediction Accuracy = 38.339999999999996%, Loss = 2.9736307144165037
Epoch: 105, Batch Gradient Norm: 15.754641818209624
Epoch: 105, Batch Gradient Norm after: 15.754641818209624
Epoch 106/10000, Prediction Accuracy = 38.489999999999995%, Loss = 2.8872442722320555
Epoch: 106, Batch Gradient Norm: 15.674408043140932
Epoch: 106, Batch Gradient Norm after: 15.674408043140932
Epoch 107/10000, Prediction Accuracy = 38.739999999999995%, Loss = 2.8183053493499757
Epoch: 107, Batch Gradient Norm: 15.632418502149832
Epoch: 107, Batch Gradient Norm after: 15.632418502149832
Epoch 108/10000, Prediction Accuracy = 38.924%, Loss = 2.767946243286133
Epoch: 108, Batch Gradient Norm: 15.93604877643802
Epoch: 108, Batch Gradient Norm after: 15.93604877643802
Epoch 109/10000, Prediction Accuracy = 39.012%, Loss = 2.7311688899993896
Epoch: 109, Batch Gradient Norm: 15.255477831186624
Epoch: 109, Batch Gradient Norm after: 15.255477831186624
Epoch 110/10000, Prediction Accuracy = 39.14200000000001%, Loss = 2.670899534225464
Epoch: 110, Batch Gradient Norm: 14.079244283514036
Epoch: 110, Batch Gradient Norm after: 14.079244283514036
Epoch 111/10000, Prediction Accuracy = 39.19%, Loss = 2.5751750469207764
Epoch: 111, Batch Gradient Norm: 14.267647921258952
Epoch: 111, Batch Gradient Norm after: 14.267647921258952
Epoch 112/10000, Prediction Accuracy = 39.248000000000005%, Loss = 2.5157572269439696
Epoch: 112, Batch Gradient Norm: 14.183642981182135
Epoch: 112, Batch Gradient Norm after: 14.183642981182135
Epoch 113/10000, Prediction Accuracy = 39.38199999999999%, Loss = 2.464563274383545
Epoch: 113, Batch Gradient Norm: 14.175475731471316
Epoch: 113, Batch Gradient Norm after: 14.175475731471316
Epoch 114/10000, Prediction Accuracy = 39.4%, Loss = 2.4115318775177004
Epoch: 114, Batch Gradient Norm: 14.00117959916879
Epoch: 114, Batch Gradient Norm after: 14.00117959916879
Epoch 115/10000, Prediction Accuracy = 39.504%, Loss = 2.3146258354187013
Epoch: 115, Batch Gradient Norm: 12.492848600755929
Epoch: 115, Batch Gradient Norm after: 12.492848600755929
Epoch 116/10000, Prediction Accuracy = 39.608%, Loss = 2.1896296977996825
Epoch: 116, Batch Gradient Norm: 14.185777025461645
Epoch: 116, Batch Gradient Norm after: 14.185777025461645
Epoch 117/10000, Prediction Accuracy = 39.762%, Loss = 2.1657494068145753
Epoch: 117, Batch Gradient Norm: 14.764312269906025
Epoch: 117, Batch Gradient Norm after: 14.764312269906025
Epoch 118/10000, Prediction Accuracy = 39.922000000000004%, Loss = 2.1199758529663084
Epoch: 118, Batch Gradient Norm: 13.912411629233247
Epoch: 118, Batch Gradient Norm after: 13.912411629233247
Epoch 119/10000, Prediction Accuracy = 39.892%, Loss = 2.0593804836273195
Epoch: 119, Batch Gradient Norm: 12.762932575990636
Epoch: 119, Batch Gradient Norm after: 12.762932575990636
Epoch 120/10000, Prediction Accuracy = 39.866%, Loss = 2.0032972574234007
Epoch: 120, Batch Gradient Norm: 10.970785602946913
Epoch: 120, Batch Gradient Norm after: 10.970785602946913
Epoch 121/10000, Prediction Accuracy = 39.952%, Loss = 1.9328625917434692
Epoch: 121, Batch Gradient Norm: 11.3838119678644
Epoch: 121, Batch Gradient Norm after: 11.3838119678644
Epoch 122/10000, Prediction Accuracy = 39.948%, Loss = 1.9057675123214721
Epoch: 122, Batch Gradient Norm: 12.615658114685525
Epoch: 122, Batch Gradient Norm after: 12.615658114685525
Epoch 123/10000, Prediction Accuracy = 40.352%, Loss = 1.8611204147338867
Epoch: 123, Batch Gradient Norm: 12.578189964979837
Epoch: 123, Batch Gradient Norm after: 12.578189964979837
Epoch 124/10000, Prediction Accuracy = 40.322%, Loss = 1.7917389631271363
Epoch: 124, Batch Gradient Norm: 14.289794467711646
Epoch: 124, Batch Gradient Norm after: 14.289794467711646
Epoch 125/10000, Prediction Accuracy = 40.416%, Loss = 1.769048285484314
Epoch: 125, Batch Gradient Norm: 10.976358221482698
Epoch: 125, Batch Gradient Norm after: 10.976358221482698
Epoch 126/10000, Prediction Accuracy = 40.36%, Loss = 1.6990670919418336
Epoch: 126, Batch Gradient Norm: 11.151968618160492
Epoch: 126, Batch Gradient Norm after: 11.151968618160492
Epoch 127/10000, Prediction Accuracy = 40.30800000000001%, Loss = 1.6550538063049316
Epoch: 127, Batch Gradient Norm: 10.149416775502926
Epoch: 127, Batch Gradient Norm after: 10.149416775502926
Epoch 128/10000, Prediction Accuracy = 40.11%, Loss = 1.5989620685577393
Epoch: 128, Batch Gradient Norm: 10.826306358084215
Epoch: 128, Batch Gradient Norm after: 10.826306358084215
Epoch 129/10000, Prediction Accuracy = 40.104%, Loss = 1.5742445707321167
Epoch: 129, Batch Gradient Norm: 10.973713445120383
Epoch: 129, Batch Gradient Norm after: 10.973713445120383
Epoch 130/10000, Prediction Accuracy = 40.14%, Loss = 1.5422903299331665
Epoch: 130, Batch Gradient Norm: 11.617038666901516
Epoch: 130, Batch Gradient Norm after: 11.617038666901516
Epoch 131/10000, Prediction Accuracy = 40.214%, Loss = 1.5336600542068481
Epoch: 131, Batch Gradient Norm: 14.785706265397664
Epoch: 131, Batch Gradient Norm after: 14.785706265397664
Epoch 132/10000, Prediction Accuracy = 40.22%, Loss = 1.5419021844863892
Epoch: 132, Batch Gradient Norm: 9.900302934206344
Epoch: 132, Batch Gradient Norm after: 9.900302934206344
Epoch 133/10000, Prediction Accuracy = 40.315999999999995%, Loss = 1.472171926498413
Epoch: 133, Batch Gradient Norm: 8.71644720382945
Epoch: 133, Batch Gradient Norm after: 8.71644720382945
Epoch 134/10000, Prediction Accuracy = 40.589999999999996%, Loss = 1.4045270681381226
Epoch: 134, Batch Gradient Norm: 8.064949402608855
Epoch: 134, Batch Gradient Norm after: 8.064949402608855
Epoch 135/10000, Prediction Accuracy = 40.476%, Loss = 1.3568036317825318
Epoch: 135, Batch Gradient Norm: 8.602469838098125
Epoch: 135, Batch Gradient Norm after: 8.602469838098125
Epoch 136/10000, Prediction Accuracy = 41.326%, Loss = 1.3007029294967651
Epoch: 136, Batch Gradient Norm: 13.202019730736303
Epoch: 136, Batch Gradient Norm after: 13.202019730736303
Epoch 137/10000, Prediction Accuracy = 41.678%, Loss = 1.3142577409744263
Epoch: 137, Batch Gradient Norm: 9.356418773568924
Epoch: 137, Batch Gradient Norm after: 9.356418773568924
Epoch 138/10000, Prediction Accuracy = 41.815999999999995%, Loss = 1.2646669626235962
Epoch: 138, Batch Gradient Norm: 12.8869650950372
Epoch: 138, Batch Gradient Norm after: 12.8869650950372
Epoch 139/10000, Prediction Accuracy = 41.984%, Loss = 1.2732112407684326
Epoch: 139, Batch Gradient Norm: 11.193853764324256
Epoch: 139, Batch Gradient Norm after: 11.193853764324256
Epoch 140/10000, Prediction Accuracy = 42.074%, Loss = 1.2491071462631225
Epoch: 140, Batch Gradient Norm: 7.872945263593142
Epoch: 140, Batch Gradient Norm after: 7.872945263593142
Epoch 141/10000, Prediction Accuracy = 42.304%, Loss = 1.2048438787460327
Epoch: 141, Batch Gradient Norm: 6.643628103464922
Epoch: 141, Batch Gradient Norm after: 6.643628103464922
Epoch 142/10000, Prediction Accuracy = 42.714000000000006%, Loss = 1.1866885662078857
Epoch: 142, Batch Gradient Norm: 16.85359939131177
Epoch: 142, Batch Gradient Norm after: 16.85359939131177
Epoch 143/10000, Prediction Accuracy = 42.854%, Loss = 1.2499249458312989
Epoch: 143, Batch Gradient Norm: 11.01787592591377
Epoch: 143, Batch Gradient Norm after: 11.01787592591377
Epoch 144/10000, Prediction Accuracy = 42.958000000000006%, Loss = 1.191249632835388
Epoch: 144, Batch Gradient Norm: 8.96282356076691
Epoch: 144, Batch Gradient Norm after: 8.96282356076691
Epoch 145/10000, Prediction Accuracy = 43.269999999999996%, Loss = 1.1617037534713746
Epoch: 145, Batch Gradient Norm: 8.429898350135598
Epoch: 145, Batch Gradient Norm after: 8.429898350135598
Epoch 146/10000, Prediction Accuracy = 42.946000000000005%, Loss = 1.1528719902038573
Epoch: 146, Batch Gradient Norm: 8.392682938988541
Epoch: 146, Batch Gradient Norm after: 8.392682938988541
Epoch 147/10000, Prediction Accuracy = 43.076%, Loss = 1.1364604473114013
Epoch: 147, Batch Gradient Norm: 8.17650243882718
Epoch: 147, Batch Gradient Norm after: 8.17650243882718
Epoch 148/10000, Prediction Accuracy = 43.198%, Loss = 1.1362908601760864
Epoch: 148, Batch Gradient Norm: 13.01401789433027
Epoch: 148, Batch Gradient Norm after: 13.01401789433027
Epoch 149/10000, Prediction Accuracy = 43.245999999999995%, Loss = 1.1501431465148926
Epoch: 149, Batch Gradient Norm: 11.926276637018251
Epoch: 149, Batch Gradient Norm after: 11.926276637018251
Epoch 150/10000, Prediction Accuracy = 43.315999999999995%, Loss = 1.1328027725219727
Epoch: 150, Batch Gradient Norm: 6.598507495344733
Epoch: 150, Batch Gradient Norm after: 6.598507495344733
Epoch 151/10000, Prediction Accuracy = 43.606%, Loss = 1.0985710620880127
Epoch: 151, Batch Gradient Norm: 9.996254332580607
Epoch: 151, Batch Gradient Norm after: 9.996254332580607
Epoch 152/10000, Prediction Accuracy = 43.772%, Loss = 1.1047609329223633
Epoch: 152, Batch Gradient Norm: 12.274093612391278
Epoch: 152, Batch Gradient Norm after: 12.274093612391278
Epoch 153/10000, Prediction Accuracy = 43.956%, Loss = 1.12011775970459
Epoch: 153, Batch Gradient Norm: 10.992455324309944
Epoch: 153, Batch Gradient Norm after: 10.992455324309944
Epoch 154/10000, Prediction Accuracy = 44.212%, Loss = 1.1007818937301637
Epoch: 154, Batch Gradient Norm: 11.029202200026036
Epoch: 154, Batch Gradient Norm after: 11.029202200026036
Epoch 155/10000, Prediction Accuracy = 44.577999999999996%, Loss = 1.0965078115463256
Epoch: 155, Batch Gradient Norm: 6.860791399863172
Epoch: 155, Batch Gradient Norm after: 6.860791399863172
Epoch 156/10000, Prediction Accuracy = 44.836%, Loss = 1.0676784276962281
Epoch: 156, Batch Gradient Norm: 6.97960161574865
Epoch: 156, Batch Gradient Norm after: 6.97960161574865
Epoch 157/10000, Prediction Accuracy = 45.212%, Loss = 1.0633687496185302
Epoch: 157, Batch Gradient Norm: 6.8066268000262715
Epoch: 157, Batch Gradient Norm after: 6.8066268000262715
Epoch 158/10000, Prediction Accuracy = 45.952%, Loss = 1.0571774959564209
Epoch: 158, Batch Gradient Norm: 5.8252024529478525
Epoch: 158, Batch Gradient Norm after: 5.8252024529478525
Epoch 159/10000, Prediction Accuracy = 46.702000000000005%, Loss = 1.0401070594787598
Epoch: 159, Batch Gradient Norm: 11.860356112706798
Epoch: 159, Batch Gradient Norm after: 11.860356112706798
Epoch 160/10000, Prediction Accuracy = 47.17%, Loss = 1.0677857637405395
Epoch: 160, Batch Gradient Norm: 19.072894615369016
Epoch: 160, Batch Gradient Norm after: 19.072894615369016
Epoch 161/10000, Prediction Accuracy = 47.541999999999994%, Loss = 1.1195928812026978
Epoch: 161, Batch Gradient Norm: 12.66500119016745
Epoch: 161, Batch Gradient Norm after: 12.66500119016745
Epoch 162/10000, Prediction Accuracy = 47.67199999999999%, Loss = 1.065529727935791
Epoch: 162, Batch Gradient Norm: 10.10660721085887
Epoch: 162, Batch Gradient Norm after: 10.10660721085887
Epoch 163/10000, Prediction Accuracy = 48.206%, Loss = 1.0508032321929932
Epoch: 163, Batch Gradient Norm: 9.08582533093203
Epoch: 163, Batch Gradient Norm after: 9.08582533093203
Epoch 164/10000, Prediction Accuracy = 48.412%, Loss = 1.0444841384887695
Epoch: 164, Batch Gradient Norm: 10.755224668907445
Epoch: 164, Batch Gradient Norm after: 10.755224668907445
Epoch 165/10000, Prediction Accuracy = 48.767999999999994%, Loss = 1.0405377864837646
Epoch: 165, Batch Gradient Norm: 7.335233828840984
Epoch: 165, Batch Gradient Norm after: 7.335233828840984
Epoch 166/10000, Prediction Accuracy = 48.974000000000004%, Loss = 1.023430347442627
Epoch: 166, Batch Gradient Norm: 4.983386818683567
Epoch: 166, Batch Gradient Norm after: 4.983386818683567
Epoch 167/10000, Prediction Accuracy = 49.22200000000001%, Loss = 1.0142669439315797
Epoch: 167, Batch Gradient Norm: 7.238373995879219
Epoch: 167, Batch Gradient Norm after: 7.238373995879219
Epoch 168/10000, Prediction Accuracy = 49.408%, Loss = 1.0161467671394349
Epoch: 168, Batch Gradient Norm: 11.580599509875546
Epoch: 168, Batch Gradient Norm after: 11.580599509875546
Epoch 169/10000, Prediction Accuracy = 49.654%, Loss = 1.0332847714424134
Epoch: 169, Batch Gradient Norm: 18.70708718000886
Epoch: 169, Batch Gradient Norm after: 18.70708718000886
Epoch 170/10000, Prediction Accuracy = 49.622%, Loss = 1.0815678358078002
Epoch: 170, Batch Gradient Norm: 8.18533359383757
Epoch: 170, Batch Gradient Norm after: 8.18533359383757
Epoch 171/10000, Prediction Accuracy = 49.862%, Loss = 1.013431429862976
Epoch: 171, Batch Gradient Norm: 13.575653868842867
Epoch: 171, Batch Gradient Norm after: 13.575653868842867
Epoch 172/10000, Prediction Accuracy = 50.036%, Loss = 1.0365877389907836
Epoch: 172, Batch Gradient Norm: 9.442096627586558
Epoch: 172, Batch Gradient Norm after: 9.442096627586558
Epoch 173/10000, Prediction Accuracy = 50.086%, Loss = 1.0127346754074096
Epoch: 173, Batch Gradient Norm: 8.897950182431716
Epoch: 173, Batch Gradient Norm after: 8.897950182431716
Epoch 174/10000, Prediction Accuracy = 50.198%, Loss = 1.0130762815475465
Epoch: 174, Batch Gradient Norm: 7.812532887547553
Epoch: 174, Batch Gradient Norm after: 7.812532887547553
Epoch 175/10000, Prediction Accuracy = 50.218%, Loss = 1.0086828827857972
Epoch: 175, Batch Gradient Norm: 6.197877422158425
Epoch: 175, Batch Gradient Norm after: 6.197877422158425
Epoch 176/10000, Prediction Accuracy = 50.322%, Loss = 0.9921016812324523
Epoch: 176, Batch Gradient Norm: 6.771938141565198
Epoch: 176, Batch Gradient Norm after: 6.771938141565198
Epoch 177/10000, Prediction Accuracy = 50.428%, Loss = 0.9908576488494873
Epoch: 177, Batch Gradient Norm: 14.14397739488677
Epoch: 177, Batch Gradient Norm after: 14.14397739488677
Epoch 178/10000, Prediction Accuracy = 50.40599999999999%, Loss = 1.027009904384613
Epoch: 178, Batch Gradient Norm: 14.989771278353441
Epoch: 178, Batch Gradient Norm after: 14.989771278353441
Epoch 179/10000, Prediction Accuracy = 50.564%, Loss = 1.026119554042816
Epoch: 179, Batch Gradient Norm: 9.775091975828643
Epoch: 179, Batch Gradient Norm after: 9.775091975828643
Epoch 180/10000, Prediction Accuracy = 50.638%, Loss = 0.9948088526725769
Epoch: 180, Batch Gradient Norm: 6.9736375657789855
Epoch: 180, Batch Gradient Norm after: 6.9736375657789855
Epoch 181/10000, Prediction Accuracy = 50.593999999999994%, Loss = 0.9790226101875306
Epoch: 181, Batch Gradient Norm: 8.91846710219682
Epoch: 181, Batch Gradient Norm after: 8.91846710219682
Epoch 182/10000, Prediction Accuracy = 50.678%, Loss = 0.9917804479599
Epoch: 182, Batch Gradient Norm: 11.180499484254305
Epoch: 182, Batch Gradient Norm after: 11.180499484254305
Epoch 183/10000, Prediction Accuracy = 50.854%, Loss = 0.9953080296516419
Epoch: 183, Batch Gradient Norm: 14.220822326225543
Epoch: 183, Batch Gradient Norm after: 14.220822326225543
Epoch 184/10000, Prediction Accuracy = 50.760000000000005%, Loss = 1.0114858984947204
Epoch: 184, Batch Gradient Norm: 11.905593035080493
Epoch: 184, Batch Gradient Norm after: 11.905593035080493
Epoch 185/10000, Prediction Accuracy = 50.858%, Loss = 0.9972791552543641
Epoch: 185, Batch Gradient Norm: 9.55461276876568
Epoch: 185, Batch Gradient Norm after: 9.55461276876568
Epoch 186/10000, Prediction Accuracy = 50.891999999999996%, Loss = 0.9773176908493042
Epoch: 186, Batch Gradient Norm: 10.278019595522258
Epoch: 186, Batch Gradient Norm after: 10.278019595522258
Epoch 187/10000, Prediction Accuracy = 50.926%, Loss = 0.9953643798828125
Epoch: 187, Batch Gradient Norm: 17.575444026283428
Epoch: 187, Batch Gradient Norm after: 17.575444026283428
Epoch 188/10000, Prediction Accuracy = 50.98%, Loss = 1.0535377502441405
Epoch: 188, Batch Gradient Norm: 11.49138818080564
Epoch: 188, Batch Gradient Norm after: 11.49138818080564
Epoch 189/10000, Prediction Accuracy = 50.96%, Loss = 0.9883765816688538
Epoch: 189, Batch Gradient Norm: 7.444728778916013
Epoch: 189, Batch Gradient Norm after: 7.444728778916013
Epoch 190/10000, Prediction Accuracy = 51.064%, Loss = 0.9637412190437317
Epoch: 190, Batch Gradient Norm: 7.5957649105748155
Epoch: 190, Batch Gradient Norm after: 7.5957649105748155
Epoch 191/10000, Prediction Accuracy = 51.116%, Loss = 0.9710751891136169
Epoch: 191, Batch Gradient Norm: 9.792754529034918
Epoch: 191, Batch Gradient Norm after: 9.792754529034918
Epoch 192/10000, Prediction Accuracy = 51.166000000000004%, Loss = 0.9771224021911621
Epoch: 192, Batch Gradient Norm: 8.447168047399437
Epoch: 192, Batch Gradient Norm after: 8.447168047399437
Epoch 193/10000, Prediction Accuracy = 51.245999999999995%, Loss = 0.9669103860855103
Epoch: 193, Batch Gradient Norm: 11.128985388216952
Epoch: 193, Batch Gradient Norm after: 11.128985388216952
Epoch 194/10000, Prediction Accuracy = 51.278000000000006%, Loss = 0.9743061304092407
Epoch: 194, Batch Gradient Norm: 17.30827214587583
Epoch: 194, Batch Gradient Norm after: 17.30827214587583
Epoch 195/10000, Prediction Accuracy = 51.275999999999996%, Loss = 1.008508825302124
Epoch: 195, Batch Gradient Norm: 15.375517878540155
Epoch: 195, Batch Gradient Norm after: 15.375517878540155
Epoch 196/10000, Prediction Accuracy = 51.288%, Loss = 0.9929104089736939
Epoch: 196, Batch Gradient Norm: 12.779993701783123
Epoch: 196, Batch Gradient Norm after: 12.779993701783123
Epoch 197/10000, Prediction Accuracy = 51.394000000000005%, Loss = 0.9740826725959778
Epoch: 197, Batch Gradient Norm: 9.739664622143957
Epoch: 197, Batch Gradient Norm after: 9.739664622143957
Epoch 198/10000, Prediction Accuracy = 51.30799999999999%, Loss = 0.9632090926170349
Epoch: 198, Batch Gradient Norm: 12.743675835605897
Epoch: 198, Batch Gradient Norm after: 12.743675835605897
Epoch 199/10000, Prediction Accuracy = 51.552%, Loss = 0.9866517901420593
Epoch: 199, Batch Gradient Norm: 9.216131199478292
Epoch: 199, Batch Gradient Norm after: 9.216131199478292
Epoch 200/10000, Prediction Accuracy = 51.522000000000006%, Loss = 0.9517863035202027
Epoch: 200, Batch Gradient Norm: 5.533841351808318
Epoch: 200, Batch Gradient Norm after: 5.533841351808318
Epoch 201/10000, Prediction Accuracy = 51.612%, Loss = 0.9372684478759765
Epoch: 201, Batch Gradient Norm: 6.470262956515763
Epoch: 201, Batch Gradient Norm after: 6.470262956515763
Epoch 202/10000, Prediction Accuracy = 51.586%, Loss = 0.9414056420326233
Epoch: 202, Batch Gradient Norm: 7.14451068877348
Epoch: 202, Batch Gradient Norm after: 7.14451068877348
Epoch 203/10000, Prediction Accuracy = 51.648%, Loss = 0.9413089156150818
Epoch: 203, Batch Gradient Norm: 9.544889511131432
Epoch: 203, Batch Gradient Norm after: 9.544889511131432
Epoch 204/10000, Prediction Accuracy = 51.74399999999999%, Loss = 0.9539703488349914
Epoch: 204, Batch Gradient Norm: 9.268541130663984
Epoch: 204, Batch Gradient Norm after: 9.268541130663984
Epoch 205/10000, Prediction Accuracy = 51.778%, Loss = 0.9556047558784485
Epoch: 205, Batch Gradient Norm: 9.275247221719587
Epoch: 205, Batch Gradient Norm after: 9.275247221719587
Epoch 206/10000, Prediction Accuracy = 51.827999999999996%, Loss = 0.9519888997077942
Epoch: 206, Batch Gradient Norm: 16.478110530025113
Epoch: 206, Batch Gradient Norm after: 16.001196126047677
Epoch 207/10000, Prediction Accuracy = 51.762%, Loss = 0.982248067855835
Epoch: 207, Batch Gradient Norm: 19.25848299371251
Epoch: 207, Batch Gradient Norm after: 19.25848299371251
Epoch 208/10000, Prediction Accuracy = 51.812%, Loss = 1.001586365699768
Epoch: 208, Batch Gradient Norm: 12.676789860641074
Epoch: 208, Batch Gradient Norm after: 12.676789860641074
Epoch 209/10000, Prediction Accuracy = 51.772000000000006%, Loss = 0.9558318495750427
Epoch: 209, Batch Gradient Norm: 10.225663264171542
Epoch: 209, Batch Gradient Norm after: 10.225663264171542
Epoch 210/10000, Prediction Accuracy = 51.757999999999996%, Loss = 0.9526620745658875
Epoch: 210, Batch Gradient Norm: 9.039757773940734
Epoch: 210, Batch Gradient Norm after: 9.039757773940734
Epoch 211/10000, Prediction Accuracy = 51.932%, Loss = 0.9341199517250061
Epoch: 211, Batch Gradient Norm: 8.267109674464576
Epoch: 211, Batch Gradient Norm after: 8.267109674464576
Epoch 212/10000, Prediction Accuracy = 51.891999999999996%, Loss = 0.9261548280715942
Epoch: 212, Batch Gradient Norm: 11.211889326958579
Epoch: 212, Batch Gradient Norm after: 11.211889326958579
Epoch 213/10000, Prediction Accuracy = 52.032%, Loss = 0.9371715903282165
Epoch: 213, Batch Gradient Norm: 11.760796101346376
Epoch: 213, Batch Gradient Norm after: 11.760796101346376
Epoch 214/10000, Prediction Accuracy = 51.90599999999999%, Loss = 0.9462822079658508
Epoch: 214, Batch Gradient Norm: 12.506343539345712
Epoch: 214, Batch Gradient Norm after: 12.506343539345712
Epoch 215/10000, Prediction Accuracy = 52.092000000000006%, Loss = 0.9478054642677307
Epoch: 215, Batch Gradient Norm: 12.367481997995101
Epoch: 215, Batch Gradient Norm after: 12.367481997995101
Epoch 216/10000, Prediction Accuracy = 52.036000000000016%, Loss = 0.9446641087532044
Epoch: 216, Batch Gradient Norm: 13.763342826003882
Epoch: 216, Batch Gradient Norm after: 13.763342826003882
Epoch 217/10000, Prediction Accuracy = 52.077999999999996%, Loss = 0.9495571255683899
Epoch: 217, Batch Gradient Norm: 12.311413868939072
Epoch: 217, Batch Gradient Norm after: 12.311413868939072
Epoch 218/10000, Prediction Accuracy = 52.116%, Loss = 0.9441361904144288
Epoch: 218, Batch Gradient Norm: 7.671533099159559
Epoch: 218, Batch Gradient Norm after: 7.671533099159559
Epoch 219/10000, Prediction Accuracy = 52.20399999999999%, Loss = 0.9165124297142029
Epoch: 219, Batch Gradient Norm: 7.323901065750904
Epoch: 219, Batch Gradient Norm after: 7.323901065750904
Epoch 220/10000, Prediction Accuracy = 52.25999999999999%, Loss = 0.9125643014907837
Epoch: 220, Batch Gradient Norm: 17.455030053257804
Epoch: 220, Batch Gradient Norm after: 17.455030053257804
Epoch 221/10000, Prediction Accuracy = 52.384%, Loss = 0.9665508508682251
Epoch: 221, Batch Gradient Norm: 14.963112404786663
Epoch: 221, Batch Gradient Norm after: 14.963112404786663
Epoch 222/10000, Prediction Accuracy = 52.112%, Loss = 0.9626007318496704
Epoch: 222, Batch Gradient Norm: 6.361366565775457
Epoch: 222, Batch Gradient Norm after: 6.361366565775457
Epoch 223/10000, Prediction Accuracy = 52.33%, Loss = 0.9075650453567505
Epoch: 223, Batch Gradient Norm: 6.3558383528026585
Epoch: 223, Batch Gradient Norm after: 6.3558383528026585
Epoch 224/10000, Prediction Accuracy = 52.239999999999995%, Loss = 0.9083635568618774
Epoch: 224, Batch Gradient Norm: 10.944039021066608
Epoch: 224, Batch Gradient Norm after: 10.944039021066608
Epoch 225/10000, Prediction Accuracy = 52.394000000000005%, Loss = 0.9231281042098999
Epoch: 225, Batch Gradient Norm: 6.990530241742209
Epoch: 225, Batch Gradient Norm after: 6.990530241742209
Epoch 226/10000, Prediction Accuracy = 52.31200000000001%, Loss = 0.9025224685668946
Epoch: 226, Batch Gradient Norm: 10.779310390759493
Epoch: 226, Batch Gradient Norm after: 10.779310390759493
Epoch 227/10000, Prediction Accuracy = 52.398%, Loss = 0.9252331614494324
Epoch: 227, Batch Gradient Norm: 13.134804974403217
Epoch: 227, Batch Gradient Norm after: 13.134804974403217
Epoch 228/10000, Prediction Accuracy = 52.35600000000001%, Loss = 0.9365978479385376
Epoch: 228, Batch Gradient Norm: 14.2176368580586
Epoch: 228, Batch Gradient Norm after: 14.2176368580586
Epoch 229/10000, Prediction Accuracy = 52.513999999999996%, Loss = 0.944332754611969
Epoch: 229, Batch Gradient Norm: 10.947518655338818
Epoch: 229, Batch Gradient Norm after: 10.947518655338818
Epoch 230/10000, Prediction Accuracy = 52.38000000000001%, Loss = 0.9174285888671875
Epoch: 230, Batch Gradient Norm: 8.68063874671492
Epoch: 230, Batch Gradient Norm after: 8.68063874671492
Epoch 231/10000, Prediction Accuracy = 52.474000000000004%, Loss = 0.9021127104759217
Epoch: 231, Batch Gradient Norm: 16.152944086000918
Epoch: 231, Batch Gradient Norm after: 16.152944086000918
Epoch 232/10000, Prediction Accuracy = 52.548%, Loss = 0.949832808971405
Epoch: 232, Batch Gradient Norm: 11.555855914965912
Epoch: 232, Batch Gradient Norm after: 11.555855914965912
Epoch 233/10000, Prediction Accuracy = 52.553999999999995%, Loss = 0.9205366969108582
Epoch: 233, Batch Gradient Norm: 9.085134598155529
Epoch: 233, Batch Gradient Norm after: 9.085134598155529
Epoch 234/10000, Prediction Accuracy = 52.577999999999996%, Loss = 0.9118244528770447
Epoch: 234, Batch Gradient Norm: 9.935265226188191
Epoch: 234, Batch Gradient Norm after: 9.935265226188191
Epoch 235/10000, Prediction Accuracy = 52.632000000000005%, Loss = 0.9130082249641418
Epoch: 235, Batch Gradient Norm: 18.792779440485262
Epoch: 235, Batch Gradient Norm after: 18.792779440485262
Epoch 236/10000, Prediction Accuracy = 52.605999999999995%, Loss = 0.957307493686676
Epoch: 236, Batch Gradient Norm: 12.996709874714975
Epoch: 236, Batch Gradient Norm after: 12.996709874714975
Epoch 237/10000, Prediction Accuracy = 52.705999999999996%, Loss = 0.9205371022224427
Epoch: 237, Batch Gradient Norm: 9.190129366902367
Epoch: 237, Batch Gradient Norm after: 9.190129366902367
Epoch 238/10000, Prediction Accuracy = 52.746%, Loss = 0.9073568940162658
Epoch: 238, Batch Gradient Norm: 8.606021001957783
Epoch: 238, Batch Gradient Norm after: 8.606021001957783
Epoch 239/10000, Prediction Accuracy = 52.69200000000001%, Loss = 0.8976832628250122
Epoch: 239, Batch Gradient Norm: 6.533815191433736
Epoch: 239, Batch Gradient Norm after: 6.533815191433736
Epoch 240/10000, Prediction Accuracy = 52.688%, Loss = 0.884720504283905
Epoch: 240, Batch Gradient Norm: 8.492327733854244
Epoch: 240, Batch Gradient Norm after: 8.492327733854244
Epoch 241/10000, Prediction Accuracy = 52.69%, Loss = 0.8932470679283142
Epoch: 241, Batch Gradient Norm: 10.392609042934836
Epoch: 241, Batch Gradient Norm after: 10.392609042934836
Epoch 242/10000, Prediction Accuracy = 52.803999999999995%, Loss = 0.9041353940963746
Epoch: 242, Batch Gradient Norm: 16.264223690294227
Epoch: 242, Batch Gradient Norm after: 16.264223690294227
Epoch 243/10000, Prediction Accuracy = 52.898%, Loss = 0.9298659682273864
Epoch: 243, Batch Gradient Norm: 16.09656985384713
Epoch: 243, Batch Gradient Norm after: 16.09656985384713
Epoch 244/10000, Prediction Accuracy = 53.04200000000001%, Loss = 0.9279891371726989
Epoch: 244, Batch Gradient Norm: 10.316233911790013
Epoch: 244, Batch Gradient Norm after: 10.316233911790013
Epoch 245/10000, Prediction Accuracy = 52.91199999999999%, Loss = 0.8957732200622559
Epoch: 245, Batch Gradient Norm: 10.085343384744093
Epoch: 245, Batch Gradient Norm after: 10.085343384744093
Epoch 246/10000, Prediction Accuracy = 52.95%, Loss = 0.9024365186691284
Epoch: 246, Batch Gradient Norm: 9.668591603821143
Epoch: 246, Batch Gradient Norm after: 9.668591603821143
Epoch 247/10000, Prediction Accuracy = 53.012%, Loss = 0.8969125866889953
Epoch: 247, Batch Gradient Norm: 9.383737846132542
Epoch: 247, Batch Gradient Norm after: 9.383737846132542
Epoch 248/10000, Prediction Accuracy = 53.056%, Loss = 0.8937781095504761
Epoch: 248, Batch Gradient Norm: 8.57152724936762
Epoch: 248, Batch Gradient Norm after: 8.57152724936762
Epoch 249/10000, Prediction Accuracy = 53.153999999999996%, Loss = 0.8858309626579285
Epoch: 249, Batch Gradient Norm: 12.358131915975319
Epoch: 249, Batch Gradient Norm after: 12.358131915975319
Epoch 250/10000, Prediction Accuracy = 53.108000000000004%, Loss = 0.8994131565093995
Epoch: 250, Batch Gradient Norm: 12.105602925019948
Epoch: 250, Batch Gradient Norm after: 12.105602925019948
Epoch 251/10000, Prediction Accuracy = 52.967999999999996%, Loss = 0.9028428435325623
Epoch: 251, Batch Gradient Norm: 13.662310528454555
Epoch: 251, Batch Gradient Norm after: 13.662310528454555
Epoch 252/10000, Prediction Accuracy = 53.081999999999994%, Loss = 0.9086147308349609
Epoch: 252, Batch Gradient Norm: 11.929628232616968
Epoch: 252, Batch Gradient Norm after: 11.929628232616968
Epoch 253/10000, Prediction Accuracy = 53.222%, Loss = 0.89514240026474
Epoch: 253, Batch Gradient Norm: 8.77176249875077
Epoch: 253, Batch Gradient Norm after: 8.77176249875077
Epoch 254/10000, Prediction Accuracy = 53.324%, Loss = 0.877612853050232
Epoch: 254, Batch Gradient Norm: 10.67080813957984
Epoch: 254, Batch Gradient Norm after: 10.67080813957984
Epoch 255/10000, Prediction Accuracy = 53.11600000000001%, Loss = 0.8892732977867126
Epoch: 255, Batch Gradient Norm: 11.59985417160018
Epoch: 255, Batch Gradient Norm after: 11.59985417160018
Epoch 256/10000, Prediction Accuracy = 53.31%, Loss = 0.9019687533378601
Epoch: 256, Batch Gradient Norm: 10.662215111735549
Epoch: 256, Batch Gradient Norm after: 10.662215111735549
Epoch 257/10000, Prediction Accuracy = 53.326%, Loss = 0.8885828137397767
Epoch: 257, Batch Gradient Norm: 14.436011856990937
Epoch: 257, Batch Gradient Norm after: 14.436011856990937
Epoch 258/10000, Prediction Accuracy = 53.348%, Loss = 0.899058198928833
Epoch: 258, Batch Gradient Norm: 14.897245818543281
Epoch: 258, Batch Gradient Norm after: 14.897245818543281
Epoch 259/10000, Prediction Accuracy = 53.326%, Loss = 0.9023131847381591
Epoch: 259, Batch Gradient Norm: 9.970090446968536
Epoch: 259, Batch Gradient Norm after: 9.970090446968536
Epoch 260/10000, Prediction Accuracy = 53.410000000000004%, Loss = 0.879018235206604
Epoch: 260, Batch Gradient Norm: 11.188548707275618
Epoch: 260, Batch Gradient Norm after: 11.188548707275618
Epoch 261/10000, Prediction Accuracy = 53.324%, Loss = 0.8809189558029175
Epoch: 261, Batch Gradient Norm: 12.219677383639688
Epoch: 261, Batch Gradient Norm after: 12.219677383639688
Epoch 262/10000, Prediction Accuracy = 53.446000000000005%, Loss = 0.8976132869720459
Epoch: 262, Batch Gradient Norm: 15.543739790113449
Epoch: 262, Batch Gradient Norm after: 15.543739790113449
Epoch 263/10000, Prediction Accuracy = 53.536%, Loss = 0.9114663243293762
Epoch: 263, Batch Gradient Norm: 11.103194214854692
Epoch: 263, Batch Gradient Norm after: 11.103194214854692
Epoch 264/10000, Prediction Accuracy = 53.592%, Loss = 0.8837124466896057
Epoch: 264, Batch Gradient Norm: 11.30053429407198
Epoch: 264, Batch Gradient Norm after: 11.30053429407198
Epoch 265/10000, Prediction Accuracy = 53.55400000000001%, Loss = 0.8795172572135925
Epoch: 265, Batch Gradient Norm: 14.856194686262349
Epoch: 265, Batch Gradient Norm after: 14.856194686262349
Epoch 266/10000, Prediction Accuracy = 53.541999999999994%, Loss = 0.8973329305648804
Epoch: 266, Batch Gradient Norm: 9.990915462861727
Epoch: 266, Batch Gradient Norm after: 9.990915462861727
Epoch 267/10000, Prediction Accuracy = 53.65999999999999%, Loss = 0.8717382788658142
Epoch: 267, Batch Gradient Norm: 12.05357079986092
Epoch: 267, Batch Gradient Norm after: 12.05357079986092
Epoch 268/10000, Prediction Accuracy = 53.604%, Loss = 0.8797012686729431
Epoch: 268, Batch Gradient Norm: 11.929111533304106
Epoch: 268, Batch Gradient Norm after: 11.929111533304106
Epoch 269/10000, Prediction Accuracy = 53.629999999999995%, Loss = 0.8911300301551819
Epoch: 269, Batch Gradient Norm: 7.387972257092878
Epoch: 269, Batch Gradient Norm after: 7.387972257092878
Epoch 270/10000, Prediction Accuracy = 53.67999999999999%, Loss = 0.85992830991745
Epoch: 270, Batch Gradient Norm: 12.744713410038438
Epoch: 270, Batch Gradient Norm after: 12.744713410038438
Epoch 271/10000, Prediction Accuracy = 53.644000000000005%, Loss = 0.8966546654701233
Epoch: 271, Batch Gradient Norm: 14.340387987716092
Epoch: 271, Batch Gradient Norm after: 14.340387987716092
Epoch 272/10000, Prediction Accuracy = 53.681999999999995%, Loss = 0.8850461721420289
Epoch: 272, Batch Gradient Norm: 14.495760361132762
Epoch: 272, Batch Gradient Norm after: 14.495760361132762
Epoch 273/10000, Prediction Accuracy = 53.79%, Loss = 0.8947198033332825
Epoch: 273, Batch Gradient Norm: 11.509572650424769
Epoch: 273, Batch Gradient Norm after: 11.509572650424769
Epoch 274/10000, Prediction Accuracy = 53.831999999999994%, Loss = 0.8737143874168396
Epoch: 274, Batch Gradient Norm: 13.357383952897788
Epoch: 274, Batch Gradient Norm after: 13.357383952897788
Epoch 275/10000, Prediction Accuracy = 53.775999999999996%, Loss = 0.8854138493537903
Epoch: 275, Batch Gradient Norm: 10.55378607515993
Epoch: 275, Batch Gradient Norm after: 10.55378607515993
Epoch 276/10000, Prediction Accuracy = 53.884%, Loss = 0.8640777826309204
Epoch: 276, Batch Gradient Norm: 10.902774645164934
Epoch: 276, Batch Gradient Norm after: 10.902774645164934
Epoch 277/10000, Prediction Accuracy = 53.83399999999999%, Loss = 0.8681752800941467
Epoch: 277, Batch Gradient Norm: 15.080478063160298
Epoch: 277, Batch Gradient Norm after: 15.080478063160298
Epoch 278/10000, Prediction Accuracy = 53.814%, Loss = 0.891003155708313
Epoch: 278, Batch Gradient Norm: 14.902247848108377
Epoch: 278, Batch Gradient Norm after: 14.902247848108377
Epoch 279/10000, Prediction Accuracy = 53.854%, Loss = 0.8907228589057923
Epoch: 279, Batch Gradient Norm: 10.39305166261743
Epoch: 279, Batch Gradient Norm after: 10.39305166261743
Epoch 280/10000, Prediction Accuracy = 53.96600000000001%, Loss = 0.8628968834877014
Epoch: 280, Batch Gradient Norm: 8.333318831404965
Epoch: 280, Batch Gradient Norm after: 8.333318831404965
Epoch 281/10000, Prediction Accuracy = 53.879999999999995%, Loss = 0.8562030076980591
Epoch: 281, Batch Gradient Norm: 10.93029012846476
Epoch: 281, Batch Gradient Norm after: 10.93029012846476
Epoch 282/10000, Prediction Accuracy = 54.044%, Loss = 0.8562254548072815
Epoch: 282, Batch Gradient Norm: 14.740077988416306
Epoch: 282, Batch Gradient Norm after: 14.740077988416306
Epoch 283/10000, Prediction Accuracy = 54.036%, Loss = 0.8790441274642944
Epoch: 283, Batch Gradient Norm: 15.930726598597476
Epoch: 283, Batch Gradient Norm after: 15.930726598597476
Epoch 284/10000, Prediction Accuracy = 54.096000000000004%, Loss = 0.8886887550354003
Epoch: 284, Batch Gradient Norm: 9.430351769181671
Epoch: 284, Batch Gradient Norm after: 9.430351769181671
Epoch 285/10000, Prediction Accuracy = 54.086%, Loss = 0.8624478936195373
Epoch: 285, Batch Gradient Norm: 8.056322369052781
Epoch: 285, Batch Gradient Norm after: 8.056322369052781
Epoch 286/10000, Prediction Accuracy = 54.102%, Loss = 0.85076984167099
Epoch: 286, Batch Gradient Norm: 8.745757872058734
Epoch: 286, Batch Gradient Norm after: 8.745757872058734
Epoch 287/10000, Prediction Accuracy = 54.162%, Loss = 0.8541488289833069
Epoch: 287, Batch Gradient Norm: 10.639188092452468
Epoch: 287, Batch Gradient Norm after: 10.639188092452468
Epoch 288/10000, Prediction Accuracy = 54.13199999999999%, Loss = 0.8550934791564941
Epoch: 288, Batch Gradient Norm: 17.81696640775671
Epoch: 288, Batch Gradient Norm after: 17.81696640775671
Epoch 289/10000, Prediction Accuracy = 54.236000000000004%, Loss = 0.906402838230133
Epoch: 289, Batch Gradient Norm: 14.163506402261557
Epoch: 289, Batch Gradient Norm after: 14.163506402261557
Epoch 290/10000, Prediction Accuracy = 54.178%, Loss = 0.8815194249153138
Epoch: 290, Batch Gradient Norm: 7.694326739807266
Epoch: 290, Batch Gradient Norm after: 7.694326739807266
Epoch 291/10000, Prediction Accuracy = 54.251999999999995%, Loss = 0.8434126019477844
Epoch: 291, Batch Gradient Norm: 7.7503846661340265
Epoch: 291, Batch Gradient Norm after: 7.7503846661340265
Epoch 292/10000, Prediction Accuracy = 54.294000000000004%, Loss = 0.8386795401573182
Epoch: 292, Batch Gradient Norm: 9.397135579876613
Epoch: 292, Batch Gradient Norm after: 9.397135579876613
Epoch 293/10000, Prediction Accuracy = 54.29%, Loss = 0.84809889793396
Epoch: 293, Batch Gradient Norm: 10.831254155053545
Epoch: 293, Batch Gradient Norm after: 10.831254155053545
Epoch 294/10000, Prediction Accuracy = 54.303999999999995%, Loss = 0.8583979368209839
Epoch: 294, Batch Gradient Norm: 11.537719760291408
Epoch: 294, Batch Gradient Norm after: 11.537719760291408
Epoch 295/10000, Prediction Accuracy = 54.32000000000001%, Loss = 0.8514062762260437
Epoch: 295, Batch Gradient Norm: 20.335896725026206
Epoch: 295, Batch Gradient Norm after: 19.884213093149334
Epoch 296/10000, Prediction Accuracy = 54.29600000000001%, Loss = 0.9076854586601257
Epoch: 296, Batch Gradient Norm: 14.061170131224769
Epoch: 296, Batch Gradient Norm after: 14.061170131224769
Epoch 297/10000, Prediction Accuracy = 54.41799999999999%, Loss = 0.8674101114273072
Epoch: 297, Batch Gradient Norm: 7.965702090743298
Epoch: 297, Batch Gradient Norm after: 7.965702090743298
Epoch 298/10000, Prediction Accuracy = 54.272000000000006%, Loss = 0.836353600025177
Epoch: 298, Batch Gradient Norm: 11.319206247161373
Epoch: 298, Batch Gradient Norm after: 11.319206247161373
Epoch 299/10000, Prediction Accuracy = 54.41600000000001%, Loss = 0.8637295603752136
Epoch: 299, Batch Gradient Norm: 10.498738001695031
Epoch: 299, Batch Gradient Norm after: 10.498738001695031
Epoch 300/10000, Prediction Accuracy = 54.446000000000005%, Loss = 0.8512553095817565
Epoch: 300, Batch Gradient Norm: 11.874617817938393
Epoch: 300, Batch Gradient Norm after: 11.874617817938393
Epoch 301/10000, Prediction Accuracy = 54.548%, Loss = 0.847571074962616
Epoch: 301, Batch Gradient Norm: 14.626290822149826
Epoch: 301, Batch Gradient Norm after: 14.626290822149826
Epoch 302/10000, Prediction Accuracy = 54.489999999999995%, Loss = 0.8643652319908142
Epoch: 302, Batch Gradient Norm: 9.192162294076944
Epoch: 302, Batch Gradient Norm after: 9.192162294076944
Epoch 303/10000, Prediction Accuracy = 54.59599999999999%, Loss = 0.8443340539932251
Epoch: 303, Batch Gradient Norm: 8.378698204106671
Epoch: 303, Batch Gradient Norm after: 8.378698204106671
Epoch 304/10000, Prediction Accuracy = 54.622%, Loss = 0.835862910747528
Epoch: 304, Batch Gradient Norm: 9.325003412297187
Epoch: 304, Batch Gradient Norm after: 9.325003412297187
Epoch 305/10000, Prediction Accuracy = 54.58%, Loss = 0.8381633639335633
Epoch: 305, Batch Gradient Norm: 10.722908169612813
Epoch: 305, Batch Gradient Norm after: 10.722908169612813
Epoch 306/10000, Prediction Accuracy = 54.69%, Loss = 0.8471199035644531
Epoch: 306, Batch Gradient Norm: 18.927309208263335
Epoch: 306, Batch Gradient Norm after: 18.927309208263335
Epoch 307/10000, Prediction Accuracy = 54.553999999999995%, Loss = 0.8920205712318421
Epoch: 307, Batch Gradient Norm: 16.092359847440427
Epoch: 307, Batch Gradient Norm after: 16.092359847440427
Epoch 308/10000, Prediction Accuracy = 54.60999999999999%, Loss = 0.8694499850273132
Epoch: 308, Batch Gradient Norm: 8.710104459382642
Epoch: 308, Batch Gradient Norm after: 8.710104459382642
Epoch 309/10000, Prediction Accuracy = 54.686%, Loss = 0.8303735613822937
Epoch: 309, Batch Gradient Norm: 12.227035095082933
Epoch: 309, Batch Gradient Norm after: 12.227035095082933
Epoch 310/10000, Prediction Accuracy = 54.74400000000001%, Loss = 0.8487243056297302
Epoch: 310, Batch Gradient Norm: 11.477920265875968
Epoch: 310, Batch Gradient Norm after: 11.477920265875968
Epoch 311/10000, Prediction Accuracy = 54.86800000000001%, Loss = 0.8570022702217102
Epoch: 311, Batch Gradient Norm: 10.88265517342541
Epoch: 311, Batch Gradient Norm after: 10.88265517342541
Epoch 312/10000, Prediction Accuracy = 54.682%, Loss = 0.8434941530227661
Epoch: 312, Batch Gradient Norm: 14.836037016700075
Epoch: 312, Batch Gradient Norm after: 14.836037016700075
Epoch 313/10000, Prediction Accuracy = 54.60600000000001%, Loss = 0.8671186566352844
Epoch: 313, Batch Gradient Norm: 13.885131292662141
Epoch: 313, Batch Gradient Norm after: 13.885131292662141
Epoch 314/10000, Prediction Accuracy = 54.662%, Loss = 0.8538392186164856
Epoch: 314, Batch Gradient Norm: 9.620572116630454
Epoch: 314, Batch Gradient Norm after: 9.620572116630454
Epoch 315/10000, Prediction Accuracy = 54.896%, Loss = 0.8304245948791504
Epoch: 315, Batch Gradient Norm: 10.317256826428217
Epoch: 315, Batch Gradient Norm after: 10.317256826428217
Epoch 316/10000, Prediction Accuracy = 54.788%, Loss = 0.826160478591919
Epoch: 316, Batch Gradient Norm: 10.388593659665293
Epoch: 316, Batch Gradient Norm after: 10.388593659665293
Epoch 317/10000, Prediction Accuracy = 54.798%, Loss = 0.8321613550186158
Epoch: 317, Batch Gradient Norm: 11.757422812303343
Epoch: 317, Batch Gradient Norm after: 11.757422812303343
Epoch 318/10000, Prediction Accuracy = 54.854%, Loss = 0.8391382932662964
Epoch: 318, Batch Gradient Norm: 12.366292695809767
Epoch: 318, Batch Gradient Norm after: 12.366292695809767
Epoch 319/10000, Prediction Accuracy = 54.879999999999995%, Loss = 0.8448499798774719
Epoch: 319, Batch Gradient Norm: 14.66406277150947
Epoch: 319, Batch Gradient Norm after: 14.66406277150947
Epoch 320/10000, Prediction Accuracy = 54.976%, Loss = 0.8594570517539978
Epoch: 320, Batch Gradient Norm: 17.918450169247283
Epoch: 320, Batch Gradient Norm after: 17.918450169247283
Epoch 321/10000, Prediction Accuracy = 54.908%, Loss = 0.8778757095336914
Epoch: 321, Batch Gradient Norm: 9.125629595491867
Epoch: 321, Batch Gradient Norm after: 9.125629595491867
Epoch 322/10000, Prediction Accuracy = 54.95399999999999%, Loss = 0.818486213684082
Epoch: 322, Batch Gradient Norm: 6.547667523145591
Epoch: 322, Batch Gradient Norm after: 6.547667523145591
Epoch 323/10000, Prediction Accuracy = 55.00599999999999%, Loss = 0.8113220691680908
Epoch: 323, Batch Gradient Norm: 7.854686359320235
Epoch: 323, Batch Gradient Norm after: 7.854686359320235
Epoch 324/10000, Prediction Accuracy = 54.898%, Loss = 0.8196505427360534
Epoch: 324, Batch Gradient Norm: 9.871983544528739
Epoch: 324, Batch Gradient Norm after: 9.871983544528739
Epoch 325/10000, Prediction Accuracy = 54.95%, Loss = 0.8226384162902832
Epoch: 325, Batch Gradient Norm: 14.456862623758424
Epoch: 325, Batch Gradient Norm after: 14.456862623758424
Epoch 326/10000, Prediction Accuracy = 55.064%, Loss = 0.8444650530815124
Epoch: 326, Batch Gradient Norm: 12.890595190212489
Epoch: 326, Batch Gradient Norm after: 12.890595190212489
Epoch 327/10000, Prediction Accuracy = 54.95799999999999%, Loss = 0.8326028823852539
Epoch: 327, Batch Gradient Norm: 11.953224092783934
Epoch: 327, Batch Gradient Norm after: 11.953224092783934
Epoch 328/10000, Prediction Accuracy = 55.10600000000001%, Loss = 0.8316810846328735
Epoch: 328, Batch Gradient Norm: 14.404104913016242
Epoch: 328, Batch Gradient Norm after: 14.404104913016242
Epoch 329/10000, Prediction Accuracy = 55.001999999999995%, Loss = 0.8529674530029296
Epoch: 329, Batch Gradient Norm: 9.964502548981782
Epoch: 329, Batch Gradient Norm after: 9.964502548981782
Epoch 330/10000, Prediction Accuracy = 55.096000000000004%, Loss = 0.8196901798248291
Epoch: 330, Batch Gradient Norm: 9.72514797285056
Epoch: 330, Batch Gradient Norm after: 9.72514797285056
Epoch 331/10000, Prediction Accuracy = 55.09400000000001%, Loss = 0.820801067352295
Epoch: 331, Batch Gradient Norm: 9.347877634349649
Epoch: 331, Batch Gradient Norm after: 9.347877634349649
Epoch 332/10000, Prediction Accuracy = 55.11%, Loss = 0.8205852150917053
Epoch: 332, Batch Gradient Norm: 9.308897191840275
Epoch: 332, Batch Gradient Norm after: 9.308897191840275
Epoch 333/10000, Prediction Accuracy = 55.198%, Loss = 0.8207565903663635
Epoch: 333, Batch Gradient Norm: 13.470379803235963
Epoch: 333, Batch Gradient Norm after: 13.470379803235963
Epoch 334/10000, Prediction Accuracy = 55.120000000000005%, Loss = 0.8390490055084229
Epoch: 334, Batch Gradient Norm: 16.262984202760492
Epoch: 334, Batch Gradient Norm after: 16.262984202760492
Epoch 335/10000, Prediction Accuracy = 55.157999999999994%, Loss = 0.855031156539917
Epoch: 335, Batch Gradient Norm: 16.443934616529326
Epoch: 335, Batch Gradient Norm after: 16.443934616529326
Epoch 336/10000, Prediction Accuracy = 55.209999999999994%, Loss = 0.865289318561554
Epoch: 336, Batch Gradient Norm: 13.656824565739548
Epoch: 336, Batch Gradient Norm after: 13.656824565739548
Epoch 337/10000, Prediction Accuracy = 55.128%, Loss = 0.8396668553352356
Epoch: 337, Batch Gradient Norm: 10.546942777853914
Epoch: 337, Batch Gradient Norm after: 10.546942777853914
Epoch 338/10000, Prediction Accuracy = 55.388%, Loss = 0.819543993473053
Epoch: 338, Batch Gradient Norm: 8.521228933746666
Epoch: 338, Batch Gradient Norm after: 8.521228933746666
Epoch 339/10000, Prediction Accuracy = 55.208000000000006%, Loss = 0.807563304901123
Epoch: 339, Batch Gradient Norm: 9.228969543291848
Epoch: 339, Batch Gradient Norm after: 9.228969543291848
Epoch 340/10000, Prediction Accuracy = 55.312%, Loss = 0.8133402228355407
Epoch: 340, Batch Gradient Norm: 12.900936270954375
Epoch: 340, Batch Gradient Norm after: 12.900936270954375
Epoch 341/10000, Prediction Accuracy = 55.354%, Loss = 0.8483656287193299
Epoch: 341, Batch Gradient Norm: 12.711699085576562
Epoch: 341, Batch Gradient Norm after: 12.711699085576562
Epoch 342/10000, Prediction Accuracy = 55.338%, Loss = 0.8431283712387085
Epoch: 342, Batch Gradient Norm: 18.824522834759335
Epoch: 342, Batch Gradient Norm after: 18.824522834759335
Epoch 343/10000, Prediction Accuracy = 55.23199999999999%, Loss = 0.8729967951774598
Epoch: 343, Batch Gradient Norm: 18.52810387423419
Epoch: 343, Batch Gradient Norm after: 18.52810387423419
Epoch 344/10000, Prediction Accuracy = 55.117999999999995%, Loss = 0.8663659572601319
Epoch: 344, Batch Gradient Norm: 9.891302938324326
Epoch: 344, Batch Gradient Norm after: 9.891302938324326
Epoch 345/10000, Prediction Accuracy = 55.376%, Loss = 0.8099239110946655
Epoch: 345, Batch Gradient Norm: 7.71753721092755
Epoch: 345, Batch Gradient Norm after: 7.71753721092755
Epoch 346/10000, Prediction Accuracy = 55.45%, Loss = 0.7992241024971009
Epoch: 346, Batch Gradient Norm: 8.95920537201484
Epoch: 346, Batch Gradient Norm after: 8.95920537201484
Epoch 347/10000, Prediction Accuracy = 55.496%, Loss = 0.8098309993743896
Epoch: 347, Batch Gradient Norm: 7.740190735304264
Epoch: 347, Batch Gradient Norm after: 7.740190735304264
Epoch 348/10000, Prediction Accuracy = 55.44200000000001%, Loss = 0.8042707800865173
Epoch: 348, Batch Gradient Norm: 11.600294440498338
Epoch: 348, Batch Gradient Norm after: 11.600294440498338
Epoch 349/10000, Prediction Accuracy = 55.504%, Loss = 0.8191702723503113
Epoch: 349, Batch Gradient Norm: 15.613902542294479
Epoch: 349, Batch Gradient Norm after: 15.613902542294479
Epoch 350/10000, Prediction Accuracy = 55.388%, Loss = 0.8374593734741211
Epoch: 350, Batch Gradient Norm: 16.205554479815255
Epoch: 350, Batch Gradient Norm after: 16.205554479815255
Epoch 351/10000, Prediction Accuracy = 55.422000000000004%, Loss = 0.8441371440887451
Epoch: 351, Batch Gradient Norm: 12.64489509924586
Epoch: 351, Batch Gradient Norm after: 12.64489509924586
Epoch 352/10000, Prediction Accuracy = 55.428%, Loss = 0.8238934874534607
Epoch: 352, Batch Gradient Norm: 11.006139975368555
Epoch: 352, Batch Gradient Norm after: 11.006139975368555
Epoch 353/10000, Prediction Accuracy = 55.516000000000005%, Loss = 0.8216930627822876
Epoch: 353, Batch Gradient Norm: 8.458342830066744
Epoch: 353, Batch Gradient Norm after: 8.458342830066744
Epoch 354/10000, Prediction Accuracy = 55.612%, Loss = 0.8027825236320496
Epoch: 354, Batch Gradient Norm: 8.277431098256864
Epoch: 354, Batch Gradient Norm after: 8.277431098256864
Epoch 355/10000, Prediction Accuracy = 55.57000000000001%, Loss = 0.8039397120475769
Epoch: 355, Batch Gradient Norm: 11.757082796351748
Epoch: 355, Batch Gradient Norm after: 11.757082796351748
Epoch 356/10000, Prediction Accuracy = 55.501999999999995%, Loss = 0.8163000464439392
Epoch: 356, Batch Gradient Norm: 17.573426945404897
Epoch: 356, Batch Gradient Norm after: 17.573426945404897
Epoch 357/10000, Prediction Accuracy = 55.56%, Loss = 0.8489958882331848
Epoch: 357, Batch Gradient Norm: 16.81503332851096
Epoch: 357, Batch Gradient Norm after: 16.81503332851096
Epoch 358/10000, Prediction Accuracy = 55.576%, Loss = 0.83910071849823
Epoch: 358, Batch Gradient Norm: 11.50199971608362
Epoch: 358, Batch Gradient Norm after: 11.50199971608362
Epoch 359/10000, Prediction Accuracy = 55.68000000000001%, Loss = 0.806760048866272
Epoch: 359, Batch Gradient Norm: 9.384853983380516
Epoch: 359, Batch Gradient Norm after: 9.384853983380516
Epoch 360/10000, Prediction Accuracy = 55.626%, Loss = 0.7995136737823486
Epoch: 360, Batch Gradient Norm: 8.538041210060456
Epoch: 360, Batch Gradient Norm after: 8.538041210060456
Epoch 361/10000, Prediction Accuracy = 55.61%, Loss = 0.7985294222831726
Epoch: 361, Batch Gradient Norm: 10.104974163082083
Epoch: 361, Batch Gradient Norm after: 10.104974163082083
Epoch 362/10000, Prediction Accuracy = 55.693999999999996%, Loss = 0.8007755637168884
Epoch: 362, Batch Gradient Norm: 16.03261963048617
Epoch: 362, Batch Gradient Norm after: 16.03261963048617
Epoch 363/10000, Prediction Accuracy = 55.581999999999994%, Loss = 0.8434065222740174
Epoch: 363, Batch Gradient Norm: 16.208258080195048
Epoch: 363, Batch Gradient Norm after: 16.208258080195048
Epoch 364/10000, Prediction Accuracy = 55.622%, Loss = 0.8329274296760559
Epoch: 364, Batch Gradient Norm: 10.54778544254356
Epoch: 364, Batch Gradient Norm after: 10.54778544254356
Epoch 365/10000, Prediction Accuracy = 55.774%, Loss = 0.8001225113868713
Epoch: 365, Batch Gradient Norm: 10.015314632868952
Epoch: 365, Batch Gradient Norm after: 10.015314632868952
Epoch 366/10000, Prediction Accuracy = 55.75600000000001%, Loss = 0.7996967792510986
Epoch: 366, Batch Gradient Norm: 11.079378358149864
Epoch: 366, Batch Gradient Norm after: 11.079378358149864
Epoch 367/10000, Prediction Accuracy = 55.674%, Loss = 0.8068461418151855
Epoch: 367, Batch Gradient Norm: 13.738414534136652
Epoch: 367, Batch Gradient Norm after: 13.738414534136652
Epoch 368/10000, Prediction Accuracy = 55.80400000000001%, Loss = 0.8285179734230042
Epoch: 368, Batch Gradient Norm: 10.645158344242658
Epoch: 368, Batch Gradient Norm after: 10.645158344242658
Epoch 369/10000, Prediction Accuracy = 55.78399999999999%, Loss = 0.8006543159484864
Epoch: 369, Batch Gradient Norm: 14.018129461060068
Epoch: 369, Batch Gradient Norm after: 14.018129461060068
Epoch 370/10000, Prediction Accuracy = 55.79600000000001%, Loss = 0.8221744537353516
Epoch: 370, Batch Gradient Norm: 16.508939406948834
Epoch: 370, Batch Gradient Norm after: 16.508939406948834
Epoch 371/10000, Prediction Accuracy = 55.834%, Loss = 0.8256122827529907
Epoch: 371, Batch Gradient Norm: 12.066185679752083
Epoch: 371, Batch Gradient Norm after: 12.066185679752083
Epoch 372/10000, Prediction Accuracy = 55.78599999999999%, Loss = 0.8048083662986756
Epoch: 372, Batch Gradient Norm: 14.224246334241545
Epoch: 372, Batch Gradient Norm after: 14.224246334241545
Epoch 373/10000, Prediction Accuracy = 55.827999999999996%, Loss = 0.8203967452049256
Epoch: 373, Batch Gradient Norm: 13.07529867976415
Epoch: 373, Batch Gradient Norm after: 13.07529867976415
Epoch 374/10000, Prediction Accuracy = 55.864%, Loss = 0.8076764702796936
Epoch: 374, Batch Gradient Norm: 11.27371175131146
Epoch: 374, Batch Gradient Norm after: 11.27371175131146
Epoch 375/10000, Prediction Accuracy = 55.786%, Loss = 0.8034725785255432
Epoch: 375, Batch Gradient Norm: 7.9496154329657
Epoch: 375, Batch Gradient Norm after: 7.9496154329657
Epoch 376/10000, Prediction Accuracy = 55.839999999999996%, Loss = 0.7828733921051025
Epoch: 376, Batch Gradient Norm: 11.172247505122968
Epoch: 376, Batch Gradient Norm after: 11.172247505122968
Epoch 377/10000, Prediction Accuracy = 55.914%, Loss = 0.8010346293449402
Epoch: 377, Batch Gradient Norm: 11.003737470837091
Epoch: 377, Batch Gradient Norm after: 11.003737470837091
Epoch 378/10000, Prediction Accuracy = 55.886%, Loss = 0.803650724887848
Epoch: 378, Batch Gradient Norm: 9.0672924548115
Epoch: 378, Batch Gradient Norm after: 9.0672924548115
Epoch 379/10000, Prediction Accuracy = 55.842000000000006%, Loss = 0.79649897813797
Epoch: 379, Batch Gradient Norm: 10.67965251523755
Epoch: 379, Batch Gradient Norm after: 10.67965251523755
Epoch 380/10000, Prediction Accuracy = 55.912%, Loss = 0.8018850564956665
Epoch: 380, Batch Gradient Norm: 16.853063281992895
Epoch: 380, Batch Gradient Norm after: 16.853063281992895
Epoch 381/10000, Prediction Accuracy = 55.977999999999994%, Loss = 0.8448718070983887
Epoch: 381, Batch Gradient Norm: 18.392138043964877
Epoch: 381, Batch Gradient Norm after: 18.392138043964877
Epoch 382/10000, Prediction Accuracy = 55.872%, Loss = 0.8347975015640259
Epoch: 382, Batch Gradient Norm: 17.632356896512288
Epoch: 382, Batch Gradient Norm after: 17.632356896512288
Epoch 383/10000, Prediction Accuracy = 55.95%, Loss = 0.836856460571289
Epoch: 383, Batch Gradient Norm: 10.513334203716152
Epoch: 383, Batch Gradient Norm after: 10.513334203716152
Epoch 384/10000, Prediction Accuracy = 55.888%, Loss = 0.7918428301811218
Epoch: 384, Batch Gradient Norm: 7.092765391879511
Epoch: 384, Batch Gradient Norm after: 7.092765391879511
Epoch 385/10000, Prediction Accuracy = 56.038%, Loss = 0.7739890336990356
Epoch: 385, Batch Gradient Norm: 9.656243350970264
Epoch: 385, Batch Gradient Norm after: 9.656243350970264
Epoch 386/10000, Prediction Accuracy = 55.989999999999995%, Loss = 0.7866898894309997
Epoch: 386, Batch Gradient Norm: 12.340445681062388
Epoch: 386, Batch Gradient Norm after: 12.340445681062388
Epoch 387/10000, Prediction Accuracy = 56.10600000000001%, Loss = 0.7997702479362487
Epoch: 387, Batch Gradient Norm: 14.85956857614191
Epoch: 387, Batch Gradient Norm after: 14.85956857614191
Epoch 388/10000, Prediction Accuracy = 55.972%, Loss = 0.8173338294029235
Epoch: 388, Batch Gradient Norm: 12.000777358747865
Epoch: 388, Batch Gradient Norm after: 12.000777358747865
Epoch 389/10000, Prediction Accuracy = 56.134%, Loss = 0.7964847207069397
Epoch: 389, Batch Gradient Norm: 12.763089449010302
Epoch: 389, Batch Gradient Norm after: 12.763089449010302
Epoch 390/10000, Prediction Accuracy = 55.95799999999999%, Loss = 0.817095148563385
Epoch: 390, Batch Gradient Norm: 9.898571850437953
Epoch: 390, Batch Gradient Norm after: 9.898571850437953
Epoch 391/10000, Prediction Accuracy = 56.032%, Loss = 0.7833471655845642
Epoch: 391, Batch Gradient Norm: 10.3697923761099
Epoch: 391, Batch Gradient Norm after: 10.3697923761099
Epoch 392/10000, Prediction Accuracy = 56.136%, Loss = 0.78606698513031
Epoch: 392, Batch Gradient Norm: 9.878429111230458
Epoch: 392, Batch Gradient Norm after: 9.878429111230458
Epoch 393/10000, Prediction Accuracy = 56.184000000000005%, Loss = 0.7867385149002075
Epoch: 393, Batch Gradient Norm: 11.838814743225475
Epoch: 393, Batch Gradient Norm after: 11.838814743225475
Epoch 394/10000, Prediction Accuracy = 56.01800000000001%, Loss = 0.7977858424186707
Epoch: 394, Batch Gradient Norm: 15.395229991469522
Epoch: 394, Batch Gradient Norm after: 15.395229991469522
Epoch 395/10000, Prediction Accuracy = 56.024%, Loss = 0.817992341518402
Epoch: 395, Batch Gradient Norm: 19.04762561545334
Epoch: 395, Batch Gradient Norm after: 19.04762561545334
Epoch 396/10000, Prediction Accuracy = 56.104%, Loss = 0.8426951885223388
Epoch: 396, Batch Gradient Norm: 11.061846847787416
Epoch: 396, Batch Gradient Norm after: 11.061846847787416
Epoch 397/10000, Prediction Accuracy = 56.132000000000005%, Loss = 0.7834368586540222
Epoch: 397, Batch Gradient Norm: 14.03613283552644
Epoch: 397, Batch Gradient Norm after: 14.03613283552644
Epoch 398/10000, Prediction Accuracy = 56.172000000000004%, Loss = 0.8017171025276184
Epoch: 398, Batch Gradient Norm: 12.290916316941725
Epoch: 398, Batch Gradient Norm after: 12.290916316941725
Epoch 399/10000, Prediction Accuracy = 56.23199999999999%, Loss = 0.7880333065986633
Epoch: 399, Batch Gradient Norm: 9.871403903446952
Epoch: 399, Batch Gradient Norm after: 9.871403903446952
Epoch 400/10000, Prediction Accuracy = 56.206%, Loss = 0.7816706657409668
Epoch: 400, Batch Gradient Norm: 11.57061504901655
Epoch: 400, Batch Gradient Norm after: 11.57061504901655
Epoch 401/10000, Prediction Accuracy = 56.112%, Loss = 0.797035813331604
Epoch: 401, Batch Gradient Norm: 14.547893220965225
Epoch: 401, Batch Gradient Norm after: 14.547893220965225
Epoch 402/10000, Prediction Accuracy = 56.274%, Loss = 0.8079383969306946
Epoch: 402, Batch Gradient Norm: 17.350426375450613
Epoch: 402, Batch Gradient Norm after: 17.350426375450613
Epoch 403/10000, Prediction Accuracy = 56.164%, Loss = 0.8316666960716248
Epoch: 403, Batch Gradient Norm: 12.381663885207427
Epoch: 403, Batch Gradient Norm after: 12.381663885207427
Epoch 404/10000, Prediction Accuracy = 56.17999999999999%, Loss = 0.7951898694038391
Epoch: 404, Batch Gradient Norm: 11.728469906391373
Epoch: 404, Batch Gradient Norm after: 11.728469906391373
Epoch 405/10000, Prediction Accuracy = 56.251999999999995%, Loss = 0.7891386151313782
Epoch: 405, Batch Gradient Norm: 9.724569894909063
Epoch: 405, Batch Gradient Norm after: 9.724569894909063
Epoch 406/10000, Prediction Accuracy = 56.42%, Loss = 0.7728229284286499
Epoch: 406, Batch Gradient Norm: 10.815641440871858
Epoch: 406, Batch Gradient Norm after: 10.815641440871858
Epoch 407/10000, Prediction Accuracy = 56.32199999999999%, Loss = 0.7880921244621277
Epoch: 407, Batch Gradient Norm: 10.358497657785234
Epoch: 407, Batch Gradient Norm after: 10.358497657785234
Epoch 408/10000, Prediction Accuracy = 56.212%, Loss = 0.7773656845092773
Epoch: 408, Batch Gradient Norm: 11.098854775025194
Epoch: 408, Batch Gradient Norm after: 11.098854775025194
Epoch 409/10000, Prediction Accuracy = 56.318%, Loss = 0.7754267573356628
Epoch: 409, Batch Gradient Norm: 12.687119392035658
Epoch: 409, Batch Gradient Norm after: 12.687119392035658
Epoch 410/10000, Prediction Accuracy = 56.25599999999999%, Loss = 0.7873467326164245
Epoch: 410, Batch Gradient Norm: 15.097903176847211
Epoch: 410, Batch Gradient Norm after: 15.097903176847211
Epoch 411/10000, Prediction Accuracy = 56.374%, Loss = 0.8050399899482727
Epoch: 411, Batch Gradient Norm: 14.419472525297017
Epoch: 411, Batch Gradient Norm after: 14.419472525297017
Epoch 412/10000, Prediction Accuracy = 56.251999999999995%, Loss = 0.8188805937767029
Epoch: 412, Batch Gradient Norm: 12.392138367895113
Epoch: 412, Batch Gradient Norm after: 12.392138367895113
Epoch 413/10000, Prediction Accuracy = 56.260000000000005%, Loss = 0.7934904575347901
Epoch: 413, Batch Gradient Norm: 11.301674512605736
Epoch: 413, Batch Gradient Norm after: 11.301674512605736
Epoch 414/10000, Prediction Accuracy = 56.414%, Loss = 0.7877745866775513
Epoch: 414, Batch Gradient Norm: 10.021579518218113
Epoch: 414, Batch Gradient Norm after: 10.021579518218113
Epoch 415/10000, Prediction Accuracy = 56.465999999999994%, Loss = 0.7754526734352112
Epoch: 415, Batch Gradient Norm: 15.292166792238918
Epoch: 415, Batch Gradient Norm after: 15.292166792238918
Epoch 416/10000, Prediction Accuracy = 56.36%, Loss = 0.7976411938667297
Epoch: 416, Batch Gradient Norm: 15.06947516154974
Epoch: 416, Batch Gradient Norm after: 15.06947516154974
Epoch 417/10000, Prediction Accuracy = 56.40200000000001%, Loss = 0.8012741565704345
Epoch: 417, Batch Gradient Norm: 13.605215162588499
Epoch: 417, Batch Gradient Norm after: 13.605215162588499
Epoch 418/10000, Prediction Accuracy = 56.422000000000004%, Loss = 0.7932913303375244
Epoch: 418, Batch Gradient Norm: 10.42768734594339
Epoch: 418, Batch Gradient Norm after: 10.42768734594339
Epoch 419/10000, Prediction Accuracy = 56.398%, Loss = 0.778335976600647
Epoch: 419, Batch Gradient Norm: 8.822118910216458
Epoch: 419, Batch Gradient Norm after: 8.822118910216458
Epoch 420/10000, Prediction Accuracy = 56.517999999999994%, Loss = 0.7647301912307739
Epoch: 420, Batch Gradient Norm: 13.329736604912403
Epoch: 420, Batch Gradient Norm after: 13.329736604912403
Epoch 421/10000, Prediction Accuracy = 56.378%, Loss = 0.7974179863929749
Epoch: 421, Batch Gradient Norm: 17.050804237714406
Epoch: 421, Batch Gradient Norm after: 17.050804237714406
Epoch 422/10000, Prediction Accuracy = 56.465999999999994%, Loss = 0.8128372550010681
Epoch: 422, Batch Gradient Norm: 10.398120489208997
Epoch: 422, Batch Gradient Norm after: 10.398120489208997
Epoch 423/10000, Prediction Accuracy = 56.42999999999999%, Loss = 0.7704532265663147
Epoch: 423, Batch Gradient Norm: 6.9369334049583955
Epoch: 423, Batch Gradient Norm after: 6.9369334049583955
Epoch 424/10000, Prediction Accuracy = 56.576%, Loss = 0.756577467918396
Epoch: 424, Batch Gradient Norm: 9.504912854470867
Epoch: 424, Batch Gradient Norm after: 9.504912854470867
Epoch 425/10000, Prediction Accuracy = 56.60600000000001%, Loss = 0.7651174187660217
Epoch: 425, Batch Gradient Norm: 10.541656597050046
Epoch: 425, Batch Gradient Norm after: 10.541656597050046
Epoch 426/10000, Prediction Accuracy = 56.5%, Loss = 0.7686856031417847
Epoch: 426, Batch Gradient Norm: 8.990130589847974
Epoch: 426, Batch Gradient Norm after: 8.990130589847974
Epoch 427/10000, Prediction Accuracy = 56.644000000000005%, Loss = 0.7629865646362305
Epoch: 427, Batch Gradient Norm: 8.2945064246874
Epoch: 427, Batch Gradient Norm after: 8.2945064246874
Epoch 428/10000, Prediction Accuracy = 56.538%, Loss = 0.7573355317115784
Epoch: 428, Batch Gradient Norm: 13.802136958553191
Epoch: 428, Batch Gradient Norm after: 13.802136958553191
Epoch 429/10000, Prediction Accuracy = 56.602%, Loss = 0.7908215284347534
Epoch: 429, Batch Gradient Norm: 17.704225298541417
Epoch: 429, Batch Gradient Norm after: 17.704225298541417
Epoch 430/10000, Prediction Accuracy = 56.612%, Loss = 0.825756311416626
Epoch: 430, Batch Gradient Norm: 18.839306102994513
Epoch: 430, Batch Gradient Norm after: 18.839306102994513
Epoch 431/10000, Prediction Accuracy = 56.652%, Loss = 0.8214336991310119
Epoch: 431, Batch Gradient Norm: 10.807571631945132
Epoch: 431, Batch Gradient Norm after: 10.807571631945132
Epoch 432/10000, Prediction Accuracy = 56.564%, Loss = 0.7691912174224853
Epoch: 432, Batch Gradient Norm: 11.557473760624339
Epoch: 432, Batch Gradient Norm after: 11.557473760624339
Epoch 433/10000, Prediction Accuracy = 56.686%, Loss = 0.7720130920410156
Epoch: 433, Batch Gradient Norm: 11.568322686310847
Epoch: 433, Batch Gradient Norm after: 11.568322686310847
Epoch 434/10000, Prediction Accuracy = 56.58%, Loss = 0.7704551339149475
Epoch: 434, Batch Gradient Norm: 15.500758778965796
Epoch: 434, Batch Gradient Norm after: 15.500758778965796
Epoch 435/10000, Prediction Accuracy = 56.662%, Loss = 0.79316086769104
Epoch: 435, Batch Gradient Norm: 15.008888730287266
Epoch: 435, Batch Gradient Norm after: 15.008888730287266
Epoch 436/10000, Prediction Accuracy = 56.638%, Loss = 0.7896386981010437
Epoch: 436, Batch Gradient Norm: 8.206226876068586
Epoch: 436, Batch Gradient Norm after: 8.206226876068586
Epoch 437/10000, Prediction Accuracy = 56.709999999999994%, Loss = 0.7537753105163574
Epoch: 437, Batch Gradient Norm: 12.417773958328727
Epoch: 437, Batch Gradient Norm after: 12.417773958328727
Epoch 438/10000, Prediction Accuracy = 56.596000000000004%, Loss = 0.7868592262268066
Epoch: 438, Batch Gradient Norm: 11.234103456210118
Epoch: 438, Batch Gradient Norm after: 11.234103456210118
Epoch 439/10000, Prediction Accuracy = 56.727999999999994%, Loss = 0.7742537021636963
Epoch: 439, Batch Gradient Norm: 11.374330286485144
Epoch: 439, Batch Gradient Norm after: 11.374330286485144
Epoch 440/10000, Prediction Accuracy = 56.722%, Loss = 0.7695137619972229
Epoch: 440, Batch Gradient Norm: 11.086010534378435
Epoch: 440, Batch Gradient Norm after: 11.086010534378435
Epoch 441/10000, Prediction Accuracy = 56.672000000000004%, Loss = 0.7653746128082275
Epoch: 441, Batch Gradient Norm: 13.615877331878137
Epoch: 441, Batch Gradient Norm after: 13.615877331878137
Epoch 442/10000, Prediction Accuracy = 56.762%, Loss = 0.7764496445655823
Epoch: 442, Batch Gradient Norm: 11.702179611119076
Epoch: 442, Batch Gradient Norm after: 11.702179611119076
Epoch 443/10000, Prediction Accuracy = 56.751999999999995%, Loss = 0.7689224720001221
Epoch: 443, Batch Gradient Norm: 11.57786033049262
Epoch: 443, Batch Gradient Norm after: 11.57786033049262
Epoch 444/10000, Prediction Accuracy = 56.722%, Loss = 0.7683933734893799
Epoch: 444, Batch Gradient Norm: 11.487863167903775
Epoch: 444, Batch Gradient Norm after: 11.487863167903775
Epoch 445/10000, Prediction Accuracy = 56.788%, Loss = 0.7651688694953919
Epoch: 445, Batch Gradient Norm: 13.21484285933596
Epoch: 445, Batch Gradient Norm after: 13.21484285933596
Epoch 446/10000, Prediction Accuracy = 56.746%, Loss = 0.7771844863891602
Epoch: 446, Batch Gradient Norm: 11.734074454206006
Epoch: 446, Batch Gradient Norm after: 11.734074454206006
Epoch 447/10000, Prediction Accuracy = 56.784000000000006%, Loss = 0.7683889269828796
Epoch: 447, Batch Gradient Norm: 11.323345608930891
Epoch: 447, Batch Gradient Norm after: 11.323345608930891
Epoch 448/10000, Prediction Accuracy = 56.748000000000005%, Loss = 0.768389105796814
Epoch: 448, Batch Gradient Norm: 10.23872027529457
Epoch: 448, Batch Gradient Norm after: 10.23872027529457
Epoch 449/10000, Prediction Accuracy = 56.766%, Loss = 0.7602294564247132
Epoch: 449, Batch Gradient Norm: 11.966138318586856
Epoch: 449, Batch Gradient Norm after: 11.966138318586856
Epoch 450/10000, Prediction Accuracy = 56.786%, Loss = 0.7713703870773315
Epoch: 450, Batch Gradient Norm: 14.076344959582178
Epoch: 450, Batch Gradient Norm after: 14.076344959582178
Epoch 451/10000, Prediction Accuracy = 56.798%, Loss = 0.7839409589767456
Epoch: 451, Batch Gradient Norm: 19.21806525410517
Epoch: 451, Batch Gradient Norm after: 19.197253311578216
Epoch 452/10000, Prediction Accuracy = 56.779999999999994%, Loss = 0.8138076543807984
Epoch: 452, Batch Gradient Norm: 15.550252463489551
Epoch: 452, Batch Gradient Norm after: 15.550252463489551
Epoch 453/10000, Prediction Accuracy = 56.772000000000006%, Loss = 0.8008117318153382
Epoch: 453, Batch Gradient Norm: 9.983453081583686
Epoch: 453, Batch Gradient Norm after: 9.983453081583686
Epoch 454/10000, Prediction Accuracy = 56.928%, Loss = 0.7633065223693848
Epoch: 454, Batch Gradient Norm: 7.207362009549102
Epoch: 454, Batch Gradient Norm after: 7.207362009549102
Epoch 455/10000, Prediction Accuracy = 56.919999999999995%, Loss = 0.7413677453994751
Epoch: 455, Batch Gradient Norm: 9.157368029124887
Epoch: 455, Batch Gradient Norm after: 9.157368029124887
Epoch 456/10000, Prediction Accuracy = 56.9%, Loss = 0.7505487322807312
Epoch: 456, Batch Gradient Norm: 12.066406414567918
Epoch: 456, Batch Gradient Norm after: 12.066406414567918
Epoch 457/10000, Prediction Accuracy = 56.92199999999999%, Loss = 0.7653228998184204
Epoch: 457, Batch Gradient Norm: 17.467635769011814
Epoch: 457, Batch Gradient Norm after: 17.467635769011814
Epoch 458/10000, Prediction Accuracy = 56.806%, Loss = 0.7905841588973999
Epoch: 458, Batch Gradient Norm: 17.275514946267116
Epoch: 458, Batch Gradient Norm after: 17.275514946267116
Epoch 459/10000, Prediction Accuracy = 56.872%, Loss = 0.8038568615913391
Epoch: 459, Batch Gradient Norm: 8.735506392794644
Epoch: 459, Batch Gradient Norm after: 8.735506392794644
Epoch 460/10000, Prediction Accuracy = 56.934000000000005%, Loss = 0.7499691009521484
Epoch: 460, Batch Gradient Norm: 6.411726369554868
Epoch: 460, Batch Gradient Norm after: 6.411726369554868
Epoch 461/10000, Prediction Accuracy = 56.89%, Loss = 0.7400065183639526
Epoch: 461, Batch Gradient Norm: 9.191836788368981
Epoch: 461, Batch Gradient Norm after: 9.191836788368981
Epoch 462/10000, Prediction Accuracy = 56.891999999999996%, Loss = 0.7501964688301086
Epoch: 462, Batch Gradient Norm: 13.342052901694306
Epoch: 462, Batch Gradient Norm after: 13.342052901694306
Epoch 463/10000, Prediction Accuracy = 56.98199999999999%, Loss = 0.7682675957679749
Epoch: 463, Batch Gradient Norm: 18.2406964251051
Epoch: 463, Batch Gradient Norm after: 18.2406964251051
Epoch 464/10000, Prediction Accuracy = 56.82000000000001%, Loss = 0.8064549803733826
Epoch: 464, Batch Gradient Norm: 13.893801533858682
Epoch: 464, Batch Gradient Norm after: 13.893801533858682
Epoch 465/10000, Prediction Accuracy = 56.858000000000004%, Loss = 0.7802353382110596
Epoch: 465, Batch Gradient Norm: 12.679436458011121
Epoch: 465, Batch Gradient Norm after: 12.679436458011121
Epoch 466/10000, Prediction Accuracy = 56.938%, Loss = 0.7778185129165649
Epoch: 466, Batch Gradient Norm: 10.383532754396125
Epoch: 466, Batch Gradient Norm after: 10.383532754396125
Epoch 467/10000, Prediction Accuracy = 57.008%, Loss = 0.7574113488197327
Epoch: 467, Batch Gradient Norm: 9.319737019463812
Epoch: 467, Batch Gradient Norm after: 9.319737019463812
Epoch 468/10000, Prediction Accuracy = 57.0%, Loss = 0.7499904870986939
Epoch: 468, Batch Gradient Norm: 14.701874556328164
Epoch: 468, Batch Gradient Norm after: 14.701874556328164
Epoch 469/10000, Prediction Accuracy = 56.862%, Loss = 0.7862414598464966
Epoch: 469, Batch Gradient Norm: 12.44179547059497
Epoch: 469, Batch Gradient Norm after: 12.44179547059497
Epoch 470/10000, Prediction Accuracy = 56.946000000000005%, Loss = 0.7828798055648803
Epoch: 470, Batch Gradient Norm: 12.221926625119197
Epoch: 470, Batch Gradient Norm after: 12.221926625119197
Epoch 471/10000, Prediction Accuracy = 56.948%, Loss = 0.7638942837715149
Epoch: 471, Batch Gradient Norm: 17.241278550344173
Epoch: 471, Batch Gradient Norm after: 17.241278550344173
Epoch 472/10000, Prediction Accuracy = 56.964%, Loss = 0.7884302973747254
Epoch: 472, Batch Gradient Norm: 11.921324946617151
Epoch: 472, Batch Gradient Norm after: 11.921324946617151
Epoch 473/10000, Prediction Accuracy = 57.004%, Loss = 0.7618802905082702
Epoch: 473, Batch Gradient Norm: 11.5497688211732
Epoch: 473, Batch Gradient Norm after: 11.5497688211732
Epoch 474/10000, Prediction Accuracy = 57.016%, Loss = 0.7572320938110352
Epoch: 474, Batch Gradient Norm: 13.058101556160779
Epoch: 474, Batch Gradient Norm after: 13.058101556160779
Epoch 475/10000, Prediction Accuracy = 56.972%, Loss = 0.7717784285545349
Epoch: 475, Batch Gradient Norm: 10.58177273612372
Epoch: 475, Batch Gradient Norm after: 10.58177273612372
Epoch 476/10000, Prediction Accuracy = 57.089999999999996%, Loss = 0.7471445441246033
Epoch: 476, Batch Gradient Norm: 12.81477220865859
Epoch: 476, Batch Gradient Norm after: 12.81477220865859
Epoch 477/10000, Prediction Accuracy = 57.1%, Loss = 0.7649693012237548
Epoch: 477, Batch Gradient Norm: 13.45031952063734
Epoch: 477, Batch Gradient Norm after: 13.45031952063734
Epoch 478/10000, Prediction Accuracy = 57.11%, Loss = 0.7737459063529968
Epoch: 478, Batch Gradient Norm: 10.784751240457387
Epoch: 478, Batch Gradient Norm after: 10.784751240457387
Epoch 479/10000, Prediction Accuracy = 57.126%, Loss = 0.7555832505226135
Epoch: 479, Batch Gradient Norm: 12.46470272812592
Epoch: 479, Batch Gradient Norm after: 12.46470272812592
Epoch 480/10000, Prediction Accuracy = 57.068000000000005%, Loss = 0.7564406514167785
Epoch: 480, Batch Gradient Norm: 16.974901668791805
Epoch: 480, Batch Gradient Norm after: 16.974901668791805
Epoch 481/10000, Prediction Accuracy = 57.162%, Loss = 0.790991997718811
Epoch: 481, Batch Gradient Norm: 12.111335551819163
Epoch: 481, Batch Gradient Norm after: 12.111335551819163
Epoch 482/10000, Prediction Accuracy = 56.96400000000001%, Loss = 0.757550060749054
Epoch: 482, Batch Gradient Norm: 12.753809119703375
Epoch: 482, Batch Gradient Norm after: 12.753809119703375
Epoch 483/10000, Prediction Accuracy = 57.08%, Loss = 0.760517132282257
Epoch: 483, Batch Gradient Norm: 11.266033030157972
Epoch: 483, Batch Gradient Norm after: 11.266033030157972
Epoch 484/10000, Prediction Accuracy = 57.068000000000005%, Loss = 0.7574639558792114
Epoch: 484, Batch Gradient Norm: 9.74933050107514
Epoch: 484, Batch Gradient Norm after: 9.74933050107514
Epoch 485/10000, Prediction Accuracy = 57.214%, Loss = 0.7539178252220153
Epoch: 485, Batch Gradient Norm: 12.297347823654615
Epoch: 485, Batch Gradient Norm after: 12.297347823654615
Epoch 486/10000, Prediction Accuracy = 57.126%, Loss = 0.7514835238456726
Epoch: 486, Batch Gradient Norm: 15.233518302281198
Epoch: 486, Batch Gradient Norm after: 15.233518302281198
Epoch 487/10000, Prediction Accuracy = 57.114%, Loss = 0.7796143531799317
Epoch: 487, Batch Gradient Norm: 10.76917490490704
Epoch: 487, Batch Gradient Norm after: 10.76917490490704
Epoch 488/10000, Prediction Accuracy = 57.098%, Loss = 0.744623339176178
Epoch: 488, Batch Gradient Norm: 10.45301245164045
Epoch: 488, Batch Gradient Norm after: 10.45301245164045
Epoch 489/10000, Prediction Accuracy = 57.134%, Loss = 0.7413999795913696
Epoch: 489, Batch Gradient Norm: 8.81506094141244
Epoch: 489, Batch Gradient Norm after: 8.81506094141244
Epoch 490/10000, Prediction Accuracy = 57.239999999999995%, Loss = 0.7359982013702393
Epoch: 490, Batch Gradient Norm: 7.80859280447202
Epoch: 490, Batch Gradient Norm after: 7.80859280447202
Epoch 491/10000, Prediction Accuracy = 57.146%, Loss = 0.7412920236587525
Epoch: 491, Batch Gradient Norm: 12.015862241612366
Epoch: 491, Batch Gradient Norm after: 12.015862241612366
Epoch 492/10000, Prediction Accuracy = 57.19599999999999%, Loss = 0.7600529789924622
Epoch: 492, Batch Gradient Norm: 17.25937044469961
Epoch: 492, Batch Gradient Norm after: 17.25937044469961
Epoch 493/10000, Prediction Accuracy = 57.086%, Loss = 0.8008833527565002
Epoch: 493, Batch Gradient Norm: 18.39204534398177
Epoch: 493, Batch Gradient Norm after: 18.39204534398177
Epoch 494/10000, Prediction Accuracy = 57.016%, Loss = 0.8077975034713745
Epoch: 494, Batch Gradient Norm: 10.826260835013512
Epoch: 494, Batch Gradient Norm after: 10.826260835013512
Epoch 495/10000, Prediction Accuracy = 57.245999999999995%, Loss = 0.7427208185195923
Epoch: 495, Batch Gradient Norm: 12.265916420857423
Epoch: 495, Batch Gradient Norm after: 12.265916420857423
Epoch 496/10000, Prediction Accuracy = 57.260000000000005%, Loss = 0.7492810845375061
Epoch: 496, Batch Gradient Norm: 13.676853837488084
Epoch: 496, Batch Gradient Norm after: 13.676853837488084
Epoch 497/10000, Prediction Accuracy = 57.25599999999999%, Loss = 0.7597134709358215
Epoch: 497, Batch Gradient Norm: 11.789331317328648
Epoch: 497, Batch Gradient Norm after: 11.789331317328648
Epoch 498/10000, Prediction Accuracy = 57.254%, Loss = 0.744724440574646
Epoch: 498, Batch Gradient Norm: 12.311661043333853
Epoch: 498, Batch Gradient Norm after: 12.311661043333853
Epoch 499/10000, Prediction Accuracy = 57.315999999999995%, Loss = 0.7476465940475464
Epoch: 499, Batch Gradient Norm: 14.023931797514642
Epoch: 499, Batch Gradient Norm after: 14.023931797514642
Epoch 500/10000, Prediction Accuracy = 57.202%, Loss = 0.7706544995307922
Epoch: 500, Batch Gradient Norm: 7.907092836551854
Epoch: 500, Batch Gradient Norm after: 7.907092836551854
Epoch 501/10000, Prediction Accuracy = 57.338%, Loss = 0.7291399598121643
Epoch: 501, Batch Gradient Norm: 10.996634851356418
Epoch: 501, Batch Gradient Norm after: 10.996634851356418
Epoch 502/10000, Prediction Accuracy = 57.35%, Loss = 0.7420762181282043
Epoch: 502, Batch Gradient Norm: 15.495422057696896
Epoch: 502, Batch Gradient Norm after: 15.495422057696896
Epoch 503/10000, Prediction Accuracy = 57.267999999999994%, Loss = 0.7710868000984192
Epoch: 503, Batch Gradient Norm: 15.259787593362804
Epoch: 503, Batch Gradient Norm after: 15.259787593362804
Epoch 504/10000, Prediction Accuracy = 57.348%, Loss = 0.7707948803901672
Epoch: 504, Batch Gradient Norm: 13.825493320500378
Epoch: 504, Batch Gradient Norm after: 13.825493320500378
Epoch 505/10000, Prediction Accuracy = 57.322%, Loss = 0.7563594102859497
Epoch: 505, Batch Gradient Norm: 13.31217244416985
Epoch: 505, Batch Gradient Norm after: 13.31217244416985
Epoch 506/10000, Prediction Accuracy = 57.484%, Loss = 0.7529232025146484
Epoch: 506, Batch Gradient Norm: 11.514502407406692
Epoch: 506, Batch Gradient Norm after: 11.514502407406692
Epoch 507/10000, Prediction Accuracy = 57.31999999999999%, Loss = 0.7475235939025879
Epoch: 507, Batch Gradient Norm: 11.584987314699445
Epoch: 507, Batch Gradient Norm after: 11.584987314699445
Epoch 508/10000, Prediction Accuracy = 57.30799999999999%, Loss = 0.7478601455688476
Epoch: 508, Batch Gradient Norm: 11.809917249896197
Epoch: 508, Batch Gradient Norm after: 11.809917249896197
Epoch 509/10000, Prediction Accuracy = 57.367999999999995%, Loss = 0.7524120688438416
Epoch: 509, Batch Gradient Norm: 11.787093556396309
Epoch: 509, Batch Gradient Norm after: 11.787093556396309
Epoch 510/10000, Prediction Accuracy = 57.29599999999999%, Loss = 0.7484305620193481
Epoch: 510, Batch Gradient Norm: 14.843752241714796
Epoch: 510, Batch Gradient Norm after: 14.843752241714796
Epoch 511/10000, Prediction Accuracy = 57.386%, Loss = 0.7568562626838684
Epoch: 511, Batch Gradient Norm: 13.18771943291408
Epoch: 511, Batch Gradient Norm after: 13.18771943291408
Epoch 512/10000, Prediction Accuracy = 57.370000000000005%, Loss = 0.7531525611877441
Epoch: 512, Batch Gradient Norm: 12.80823300731255
Epoch: 512, Batch Gradient Norm after: 12.80823300731255
Epoch 513/10000, Prediction Accuracy = 57.294%, Loss = 0.7583960056304931
Epoch: 513, Batch Gradient Norm: 11.152343517595643
Epoch: 513, Batch Gradient Norm after: 11.152343517595643
Epoch 514/10000, Prediction Accuracy = 57.408%, Loss = 0.7400346159934997
Epoch: 514, Batch Gradient Norm: 10.53589444656665
Epoch: 514, Batch Gradient Norm after: 10.53589444656665
Epoch 515/10000, Prediction Accuracy = 57.394000000000005%, Loss = 0.7387303709983826
Epoch: 515, Batch Gradient Norm: 9.004167352959067
Epoch: 515, Batch Gradient Norm after: 9.004167352959067
Epoch 516/10000, Prediction Accuracy = 57.418000000000006%, Loss = 0.7249945521354675
Epoch: 516, Batch Gradient Norm: 12.245357582161358
Epoch: 516, Batch Gradient Norm after: 12.245357582161358
Epoch 517/10000, Prediction Accuracy = 57.44200000000001%, Loss = 0.7415466547012329
Epoch: 517, Batch Gradient Norm: 11.495127594045096
Epoch: 517, Batch Gradient Norm after: 11.495127594045096
Epoch 518/10000, Prediction Accuracy = 57.379999999999995%, Loss = 0.737103271484375
Epoch: 518, Batch Gradient Norm: 13.053712054443988
Epoch: 518, Batch Gradient Norm after: 13.053712054443988
Epoch 519/10000, Prediction Accuracy = 57.438%, Loss = 0.7470380783081054
Epoch: 519, Batch Gradient Norm: 14.633070433699967
Epoch: 519, Batch Gradient Norm after: 14.633070433699967
Epoch 520/10000, Prediction Accuracy = 57.446000000000005%, Loss = 0.7586225867271423
Epoch: 520, Batch Gradient Norm: 14.20251388559041
Epoch: 520, Batch Gradient Norm after: 14.20251388559041
Epoch 521/10000, Prediction Accuracy = 57.484%, Loss = 0.7632492303848266
Epoch: 521, Batch Gradient Norm: 12.233880284423298
Epoch: 521, Batch Gradient Norm after: 12.233880284423298
Epoch 522/10000, Prediction Accuracy = 57.524%, Loss = 0.7457026720046998
Epoch: 522, Batch Gradient Norm: 8.655475542576097
Epoch: 522, Batch Gradient Norm after: 8.655475542576097
Epoch 523/10000, Prediction Accuracy = 57.472%, Loss = 0.7233679533004761
Epoch: 523, Batch Gradient Norm: 9.449345741901551
Epoch: 523, Batch Gradient Norm after: 9.449345741901551
Epoch 524/10000, Prediction Accuracy = 57.482000000000006%, Loss = 0.7291064381599426
Epoch: 524, Batch Gradient Norm: 13.163261123663863
Epoch: 524, Batch Gradient Norm after: 13.163261123663863
Epoch 525/10000, Prediction Accuracy = 57.513999999999996%, Loss = 0.7479164958000183
Epoch: 525, Batch Gradient Norm: 13.665227204703708
Epoch: 525, Batch Gradient Norm after: 13.665227204703708
Epoch 526/10000, Prediction Accuracy = 57.452%, Loss = 0.7495745420455933
Epoch: 526, Batch Gradient Norm: 17.13140174125209
Epoch: 526, Batch Gradient Norm after: 17.13140174125209
Epoch 527/10000, Prediction Accuracy = 57.465999999999994%, Loss = 0.7697126984596252
Epoch: 527, Batch Gradient Norm: 13.09883299151006
Epoch: 527, Batch Gradient Norm after: 13.09883299151006
Epoch 528/10000, Prediction Accuracy = 57.534000000000006%, Loss = 0.7430773735046386
Epoch: 528, Batch Gradient Norm: 12.110262163982581
Epoch: 528, Batch Gradient Norm after: 12.110262163982581
Epoch 529/10000, Prediction Accuracy = 57.602%, Loss = 0.7502651453018189
Epoch: 529, Batch Gradient Norm: 10.853747730019352
Epoch: 529, Batch Gradient Norm after: 10.853747730019352
Epoch 530/10000, Prediction Accuracy = 57.616%, Loss = 0.7354239344596862
Epoch: 530, Batch Gradient Norm: 7.420193188416344
Epoch: 530, Batch Gradient Norm after: 7.420193188416344
Epoch 531/10000, Prediction Accuracy = 57.470000000000006%, Loss = 0.717548418045044
Epoch: 531, Batch Gradient Norm: 8.433067190383767
Epoch: 531, Batch Gradient Norm after: 8.433067190383767
Epoch 532/10000, Prediction Accuracy = 57.488%, Loss = 0.7243219971656799
Epoch: 532, Batch Gradient Norm: 11.045261373392483
Epoch: 532, Batch Gradient Norm after: 11.045261373392483
Epoch 533/10000, Prediction Accuracy = 57.584%, Loss = 0.7342055439949036
Epoch: 533, Batch Gradient Norm: 16.11503875421447
Epoch: 533, Batch Gradient Norm after: 16.11503875421447
Epoch 534/10000, Prediction Accuracy = 57.528%, Loss = 0.7596009135246277
Epoch: 534, Batch Gradient Norm: 16.551686917407103
Epoch: 534, Batch Gradient Norm after: 16.551686917407103
Epoch 535/10000, Prediction Accuracy = 57.532000000000004%, Loss = 0.764846932888031
Epoch: 535, Batch Gradient Norm: 13.042702663013332
Epoch: 535, Batch Gradient Norm after: 13.042702663013332
Epoch 536/10000, Prediction Accuracy = 57.564%, Loss = 0.7471607804298401
Epoch: 536, Batch Gradient Norm: 10.731099076787736
Epoch: 536, Batch Gradient Norm after: 10.731099076787736
Epoch 537/10000, Prediction Accuracy = 57.676%, Loss = 0.7288981199264526
Epoch: 537, Batch Gradient Norm: 11.328933581516877
Epoch: 537, Batch Gradient Norm after: 11.328933581516877
Epoch 538/10000, Prediction Accuracy = 57.556000000000004%, Loss = 0.7375415802001953
Epoch: 538, Batch Gradient Norm: 9.423431117942412
Epoch: 538, Batch Gradient Norm after: 9.423431117942412
Epoch 539/10000, Prediction Accuracy = 57.63399999999999%, Loss = 0.7228929758071899
Epoch: 539, Batch Gradient Norm: 10.578560765930272
Epoch: 539, Batch Gradient Norm after: 10.578560765930272
Epoch 540/10000, Prediction Accuracy = 57.58399999999999%, Loss = 0.7240281105041504
Epoch: 540, Batch Gradient Norm: 17.781071438732422
Epoch: 540, Batch Gradient Norm after: 17.780426178724838
Epoch 541/10000, Prediction Accuracy = 57.598%, Loss = 0.7721546530723572
Epoch: 541, Batch Gradient Norm: 17.07474967315551
Epoch: 541, Batch Gradient Norm after: 17.07474967315551
Epoch 542/10000, Prediction Accuracy = 57.678%, Loss = 0.777955424785614
Epoch: 542, Batch Gradient Norm: 11.200380657914891
Epoch: 542, Batch Gradient Norm after: 11.200380657914891
Epoch 543/10000, Prediction Accuracy = 57.584%, Loss = 0.7425008654594422
Epoch: 543, Batch Gradient Norm: 10.751699755636503
Epoch: 543, Batch Gradient Norm after: 10.751699755636503
Epoch 544/10000, Prediction Accuracy = 57.74000000000001%, Loss = 0.7240057110786438
Epoch: 544, Batch Gradient Norm: 8.517192097707005
Epoch: 544, Batch Gradient Norm after: 8.517192097707005
Epoch 545/10000, Prediction Accuracy = 57.68599999999999%, Loss = 0.7141888737678528
Epoch: 545, Batch Gradient Norm: 12.322205312398983
Epoch: 545, Batch Gradient Norm after: 12.322205312398983
Epoch 546/10000, Prediction Accuracy = 57.684000000000005%, Loss = 0.7370729446411133
Epoch: 546, Batch Gradient Norm: 16.67639622578039
Epoch: 546, Batch Gradient Norm after: 16.67639622578039
Epoch 547/10000, Prediction Accuracy = 57.562%, Loss = 0.765769076347351
Epoch: 547, Batch Gradient Norm: 14.255779447901398
Epoch: 547, Batch Gradient Norm after: 14.255779447901398
Epoch 548/10000, Prediction Accuracy = 57.78799999999999%, Loss = 0.7476841092109681
Epoch: 548, Batch Gradient Norm: 11.145975960756331
Epoch: 548, Batch Gradient Norm after: 11.145975960756331
Epoch 549/10000, Prediction Accuracy = 57.624%, Loss = 0.7272492289543152
Epoch: 549, Batch Gradient Norm: 8.76629919193567
Epoch: 549, Batch Gradient Norm after: 8.76629919193567
Epoch 550/10000, Prediction Accuracy = 57.698%, Loss = 0.7168158650398254
Epoch: 550, Batch Gradient Norm: 11.354333998859067
Epoch: 550, Batch Gradient Norm after: 11.354333998859067
Epoch 551/10000, Prediction Accuracy = 57.78000000000001%, Loss = 0.7309704780578613
Epoch: 551, Batch Gradient Norm: 11.395249990958725
Epoch: 551, Batch Gradient Norm after: 11.395249990958725
Epoch 552/10000, Prediction Accuracy = 57.696000000000005%, Loss = 0.7384712100028992
Epoch: 552, Batch Gradient Norm: 13.535664881398798
Epoch: 552, Batch Gradient Norm after: 13.535664881398798
Epoch 553/10000, Prediction Accuracy = 57.684000000000005%, Loss = 0.7420400619506836
Epoch: 553, Batch Gradient Norm: 16.75687239663904
Epoch: 553, Batch Gradient Norm after: 16.75687239663904
Epoch 554/10000, Prediction Accuracy = 57.786%, Loss = 0.7624889373779297
Epoch: 554, Batch Gradient Norm: 15.777473643502258
Epoch: 554, Batch Gradient Norm after: 15.777473643502258
Epoch 555/10000, Prediction Accuracy = 57.72800000000001%, Loss = 0.7596353769302369
Epoch: 555, Batch Gradient Norm: 14.591370098209893
Epoch: 555, Batch Gradient Norm after: 14.591370098209893
Epoch 556/10000, Prediction Accuracy = 57.80400000000001%, Loss = 0.7514341473579407
Epoch: 556, Batch Gradient Norm: 10.898094964106088
Epoch: 556, Batch Gradient Norm after: 10.898094964106088
Epoch 557/10000, Prediction Accuracy = 57.66799999999999%, Loss = 0.7309246301651001
Epoch: 557, Batch Gradient Norm: 7.825113040240356
Epoch: 557, Batch Gradient Norm after: 7.825113040240356
Epoch 558/10000, Prediction Accuracy = 57.698%, Loss = 0.7138592362403869
Epoch: 558, Batch Gradient Norm: 7.5930346187160085
Epoch: 558, Batch Gradient Norm after: 7.5930346187160085
Epoch 559/10000, Prediction Accuracy = 57.67%, Loss = 0.7119491815567016
Epoch: 559, Batch Gradient Norm: 10.258432374010525
Epoch: 559, Batch Gradient Norm after: 10.258432374010525
Epoch 560/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.7200456857681274
Epoch: 560, Batch Gradient Norm: 18.565124555352977
Epoch: 560, Batch Gradient Norm after: 18.565124555352977
Epoch 561/10000, Prediction Accuracy = 57.706%, Loss = 0.7875087261199951
Epoch: 561, Batch Gradient Norm: 14.49610900176391
Epoch: 561, Batch Gradient Norm after: 14.49610900176391
Epoch 562/10000, Prediction Accuracy = 57.754%, Loss = 0.7417829155921936
Epoch: 562, Batch Gradient Norm: 9.72938451609637
Epoch: 562, Batch Gradient Norm after: 9.72938451609637
Epoch 563/10000, Prediction Accuracy = 57.85%, Loss = 0.7218825817108154
Epoch: 563, Batch Gradient Norm: 8.558557005839218
Epoch: 563, Batch Gradient Norm after: 8.558557005839218
Epoch 564/10000, Prediction Accuracy = 57.727999999999994%, Loss = 0.7100212693214416
Epoch: 564, Batch Gradient Norm: 7.906749555861373
Epoch: 564, Batch Gradient Norm after: 7.906749555861373
Epoch 565/10000, Prediction Accuracy = 57.902%, Loss = 0.7062774777412415
Epoch: 565, Batch Gradient Norm: 9.179846276787691
Epoch: 565, Batch Gradient Norm after: 9.179846276787691
Epoch 566/10000, Prediction Accuracy = 57.814%, Loss = 0.713918924331665
Epoch: 566, Batch Gradient Norm: 14.895331832666395
Epoch: 566, Batch Gradient Norm after: 14.895331832666395
Epoch 567/10000, Prediction Accuracy = 57.738%, Loss = 0.7448644638061523
Epoch: 567, Batch Gradient Norm: 18.212871148055378
Epoch: 567, Batch Gradient Norm after: 18.212871148055378
Epoch 568/10000, Prediction Accuracy = 57.772000000000006%, Loss = 0.7699158668518067
Epoch: 568, Batch Gradient Norm: 10.872468691338167
Epoch: 568, Batch Gradient Norm after: 10.872468691338167
Epoch 569/10000, Prediction Accuracy = 57.886%, Loss = 0.7219667196273803
Epoch: 569, Batch Gradient Norm: 10.13163679796547
Epoch: 569, Batch Gradient Norm after: 10.13163679796547
Epoch 570/10000, Prediction Accuracy = 57.867999999999995%, Loss = 0.7223114848136902
Epoch: 570, Batch Gradient Norm: 10.384755819355858
Epoch: 570, Batch Gradient Norm after: 10.384755819355858
Epoch 571/10000, Prediction Accuracy = 57.763999999999996%, Loss = 0.726594877243042
Epoch: 571, Batch Gradient Norm: 11.873180012317281
Epoch: 571, Batch Gradient Norm after: 11.873180012317281
Epoch 572/10000, Prediction Accuracy = 57.769999999999996%, Loss = 0.7277267694473266
Epoch: 572, Batch Gradient Norm: 15.917942139758543
Epoch: 572, Batch Gradient Norm after: 15.917942139758543
Epoch 573/10000, Prediction Accuracy = 57.891999999999996%, Loss = 0.7544316411018371
Epoch: 573, Batch Gradient Norm: 12.657562059404492
Epoch: 573, Batch Gradient Norm after: 12.657562059404492
Epoch 574/10000, Prediction Accuracy = 57.95799999999999%, Loss = 0.7312864899635315
Epoch: 574, Batch Gradient Norm: 11.207100473420402
Epoch: 574, Batch Gradient Norm after: 11.207100473420402
Epoch 575/10000, Prediction Accuracy = 57.83%, Loss = 0.7198219418525695
Epoch: 575, Batch Gradient Norm: 10.61571301858824
Epoch: 575, Batch Gradient Norm after: 10.61571301858824
Epoch 576/10000, Prediction Accuracy = 57.959999999999994%, Loss = 0.7189813613891601
Epoch: 576, Batch Gradient Norm: 11.81715312551444
Epoch: 576, Batch Gradient Norm after: 11.81715312551444
Epoch 577/10000, Prediction Accuracy = 57.90599999999999%, Loss = 0.7243671655654907
Epoch: 577, Batch Gradient Norm: 13.71855258405067
Epoch: 577, Batch Gradient Norm after: 13.71855258405067
Epoch 578/10000, Prediction Accuracy = 57.970000000000006%, Loss = 0.7367056608200073
Epoch: 578, Batch Gradient Norm: 14.598756445149045
Epoch: 578, Batch Gradient Norm after: 14.598756445149045
Epoch 579/10000, Prediction Accuracy = 57.912%, Loss = 0.7443304538726807
Epoch: 579, Batch Gradient Norm: 16.30778510988772
Epoch: 579, Batch Gradient Norm after: 16.30778510988772
Epoch 580/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.7483558654785156
Epoch: 580, Batch Gradient Norm: 15.323113645457429
Epoch: 580, Batch Gradient Norm after: 15.323113645457429
Epoch 581/10000, Prediction Accuracy = 57.955999999999996%, Loss = 0.7446880459785461
Epoch: 581, Batch Gradient Norm: 11.580518264424613
Epoch: 581, Batch Gradient Norm after: 11.580518264424613
Epoch 582/10000, Prediction Accuracy = 57.984%, Loss = 0.7217915415763855
Epoch: 582, Batch Gradient Norm: 11.211335039500932
Epoch: 582, Batch Gradient Norm after: 11.211335039500932
Epoch 583/10000, Prediction Accuracy = 58.010000000000005%, Loss = 0.71982501745224
Epoch: 583, Batch Gradient Norm: 13.910805175338512
Epoch: 583, Batch Gradient Norm after: 13.910805175338512
Epoch 584/10000, Prediction Accuracy = 57.989999999999995%, Loss = 0.7394497871398926
Epoch: 584, Batch Gradient Norm: 11.812689911607846
Epoch: 584, Batch Gradient Norm after: 11.812689911607846
Epoch 585/10000, Prediction Accuracy = 57.976%, Loss = 0.7158062100410462
Epoch: 585, Batch Gradient Norm: 12.598216298713194
Epoch: 585, Batch Gradient Norm after: 12.598216298713194
Epoch 586/10000, Prediction Accuracy = 57.938%, Loss = 0.7270588040351867
Epoch: 586, Batch Gradient Norm: 9.117092980266838
Epoch: 586, Batch Gradient Norm after: 9.117092980266838
Epoch 587/10000, Prediction Accuracy = 58.00999999999999%, Loss = 0.7046650648117065
Epoch: 587, Batch Gradient Norm: 10.499591634778584
Epoch: 587, Batch Gradient Norm after: 10.499591634778584
Epoch 588/10000, Prediction Accuracy = 57.918000000000006%, Loss = 0.7138984441757202
Epoch: 588, Batch Gradient Norm: 11.264415074274767
Epoch: 588, Batch Gradient Norm after: 11.264415074274767
Epoch 589/10000, Prediction Accuracy = 57.958000000000006%, Loss = 0.7183070182800293
Epoch: 589, Batch Gradient Norm: 8.468312755011697
Epoch: 589, Batch Gradient Norm after: 8.468312755011697
Epoch 590/10000, Prediction Accuracy = 57.92999999999999%, Loss = 0.703803288936615
Epoch: 590, Batch Gradient Norm: 14.166224140037158
Epoch: 590, Batch Gradient Norm after: 14.166224140037158
Epoch 591/10000, Prediction Accuracy = 57.95399999999999%, Loss = 0.7441854238510132
Epoch: 591, Batch Gradient Norm: 14.039717707520985
Epoch: 591, Batch Gradient Norm after: 14.039717707520985
Epoch 592/10000, Prediction Accuracy = 58.00599999999999%, Loss = 0.7344367623329162
Epoch: 592, Batch Gradient Norm: 14.910880598143304
Epoch: 592, Batch Gradient Norm after: 14.910880598143304
Epoch 593/10000, Prediction Accuracy = 58.04600000000001%, Loss = 0.7396370053291321
Epoch: 593, Batch Gradient Norm: 11.899730151748479
Epoch: 593, Batch Gradient Norm after: 11.899730151748479
Epoch 594/10000, Prediction Accuracy = 58.052%, Loss = 0.718047845363617
Epoch: 594, Batch Gradient Norm: 13.887178228474633
Epoch: 594, Batch Gradient Norm after: 13.887178228474633
Epoch 595/10000, Prediction Accuracy = 57.9%, Loss = 0.7378159642219544
Epoch: 595, Batch Gradient Norm: 13.972897591415954
Epoch: 595, Batch Gradient Norm after: 13.972897591415954
Epoch 596/10000, Prediction Accuracy = 58.089999999999996%, Loss = 0.7300325870513916
Epoch: 596, Batch Gradient Norm: 12.432528208912753
Epoch: 596, Batch Gradient Norm after: 12.432528208912753
Epoch 597/10000, Prediction Accuracy = 57.955999999999996%, Loss = 0.7306081295013428
Epoch: 597, Batch Gradient Norm: 12.255194958579654
Epoch: 597, Batch Gradient Norm after: 12.255194958579654
Epoch 598/10000, Prediction Accuracy = 58.09400000000001%, Loss = 0.7138647198677063
Epoch: 598, Batch Gradient Norm: 15.043454631599984
Epoch: 598, Batch Gradient Norm after: 15.043454631599984
Epoch 599/10000, Prediction Accuracy = 58.029999999999994%, Loss = 0.7410048961639404
Epoch: 599, Batch Gradient Norm: 13.24602338722789
Epoch: 599, Batch Gradient Norm after: 13.24602338722789
Epoch 600/10000, Prediction Accuracy = 58.084%, Loss = 0.725643789768219
Epoch: 600, Batch Gradient Norm: 9.280548073858304
Epoch: 600, Batch Gradient Norm after: 9.280548073858304
Epoch 601/10000, Prediction Accuracy = 58.05%, Loss = 0.7061004281044007
Epoch: 601, Batch Gradient Norm: 10.797629300082212
Epoch: 601, Batch Gradient Norm after: 10.797629300082212
Epoch 602/10000, Prediction Accuracy = 58.07000000000001%, Loss = 0.7065483093261719
Epoch: 602, Batch Gradient Norm: 15.807012036526132
Epoch: 602, Batch Gradient Norm after: 15.807012036526132
Epoch 603/10000, Prediction Accuracy = 58.11%, Loss = 0.7371298193931579
Epoch: 603, Batch Gradient Norm: 11.979509800060805
Epoch: 603, Batch Gradient Norm after: 11.979509800060805
Epoch 604/10000, Prediction Accuracy = 58.022000000000006%, Loss = 0.7178679227828979
Epoch: 604, Batch Gradient Norm: 12.240930701871095
Epoch: 604, Batch Gradient Norm after: 12.240930701871095
Epoch 605/10000, Prediction Accuracy = 58.088%, Loss = 0.7192887306213379
Epoch: 605, Batch Gradient Norm: 13.587218020165295
Epoch: 605, Batch Gradient Norm after: 13.587218020165295
Epoch 606/10000, Prediction Accuracy = 57.964%, Loss = 0.7341472387313843
Epoch: 606, Batch Gradient Norm: 11.275803787070483
Epoch: 606, Batch Gradient Norm after: 11.275803787070483
Epoch 607/10000, Prediction Accuracy = 58.132000000000005%, Loss = 0.7118336915969848
Epoch: 607, Batch Gradient Norm: 11.783196242150689
Epoch: 607, Batch Gradient Norm after: 11.783196242150689
Epoch 608/10000, Prediction Accuracy = 58.028%, Loss = 0.7214059948921203
Epoch: 608, Batch Gradient Norm: 15.64204595081751
Epoch: 608, Batch Gradient Norm after: 15.64204595081751
Epoch 609/10000, Prediction Accuracy = 58.010000000000005%, Loss = 0.7418348073959351
Epoch: 609, Batch Gradient Norm: 14.537204568449999
Epoch: 609, Batch Gradient Norm after: 14.537204568449999
Epoch 610/10000, Prediction Accuracy = 58.052%, Loss = 0.7369967699050903
Epoch: 610, Batch Gradient Norm: 9.518498164440697
Epoch: 610, Batch Gradient Norm after: 9.518498164440697
Epoch 611/10000, Prediction Accuracy = 58.11%, Loss = 0.6985180497169494
Epoch: 611, Batch Gradient Norm: 9.002016099378709
Epoch: 611, Batch Gradient Norm after: 9.002016099378709
Epoch 612/10000, Prediction Accuracy = 58.114%, Loss = 0.700133204460144
Epoch: 612, Batch Gradient Norm: 10.251727242410736
Epoch: 612, Batch Gradient Norm after: 10.251727242410736
Epoch 613/10000, Prediction Accuracy = 58.129999999999995%, Loss = 0.7094763994216919
Epoch: 613, Batch Gradient Norm: 9.163673287157838
Epoch: 613, Batch Gradient Norm after: 9.163673287157838
Epoch 614/10000, Prediction Accuracy = 58.2%, Loss = 0.6983326315879822
Epoch: 614, Batch Gradient Norm: 12.212157127244556
Epoch: 614, Batch Gradient Norm after: 12.212157127244556
Epoch 615/10000, Prediction Accuracy = 58.168000000000006%, Loss = 0.720658254623413
Epoch: 615, Batch Gradient Norm: 16.116290126775624
Epoch: 615, Batch Gradient Norm after: 16.116290126775624
Epoch 616/10000, Prediction Accuracy = 58.072%, Loss = 0.7373486757278442
Epoch: 616, Batch Gradient Norm: 16.171021918046584
Epoch: 616, Batch Gradient Norm after: 16.171021918046584
Epoch 617/10000, Prediction Accuracy = 58.122%, Loss = 0.7427754282951355
Epoch: 617, Batch Gradient Norm: 12.43158690984975
Epoch: 617, Batch Gradient Norm after: 12.43158690984975
Epoch 618/10000, Prediction Accuracy = 58.166%, Loss = 0.7178022384643554
Epoch: 618, Batch Gradient Norm: 12.788930206985794
Epoch: 618, Batch Gradient Norm after: 12.788930206985794
Epoch 619/10000, Prediction Accuracy = 58.114%, Loss = 0.7186198234558105
Epoch: 619, Batch Gradient Norm: 11.1915057174812
Epoch: 619, Batch Gradient Norm after: 11.1915057174812
Epoch 620/10000, Prediction Accuracy = 58.17999999999999%, Loss = 0.7053645372390747
Epoch: 620, Batch Gradient Norm: 9.693241561966108
Epoch: 620, Batch Gradient Norm after: 9.693241561966108
Epoch 621/10000, Prediction Accuracy = 58.205999999999996%, Loss = 0.7005604863166809
Epoch: 621, Batch Gradient Norm: 16.759660165648004
Epoch: 621, Batch Gradient Norm after: 16.759660165648004
Epoch 622/10000, Prediction Accuracy = 58.10799999999999%, Loss = 0.7415651202201843
Epoch: 622, Batch Gradient Norm: 15.408651345274247
Epoch: 622, Batch Gradient Norm after: 15.408651345274247
Epoch 623/10000, Prediction Accuracy = 58.157999999999994%, Loss = 0.7421022295951843
Epoch: 623, Batch Gradient Norm: 13.94416569022917
Epoch: 623, Batch Gradient Norm after: 13.94416569022917
Epoch 624/10000, Prediction Accuracy = 58.20399999999999%, Loss = 0.7241614818572998
Epoch: 624, Batch Gradient Norm: 12.175442432447094
Epoch: 624, Batch Gradient Norm after: 12.175442432447094
Epoch 625/10000, Prediction Accuracy = 58.146%, Loss = 0.7143283128738404
Epoch: 625, Batch Gradient Norm: 12.727575565223235
Epoch: 625, Batch Gradient Norm after: 12.727575565223235
Epoch 626/10000, Prediction Accuracy = 58.212%, Loss = 0.719854199886322
Epoch: 626, Batch Gradient Norm: 9.053354627348702
Epoch: 626, Batch Gradient Norm after: 9.053354627348702
Epoch 627/10000, Prediction Accuracy = 58.234%, Loss = 0.6968890190124511
Epoch: 627, Batch Gradient Norm: 10.511180812435311
Epoch: 627, Batch Gradient Norm after: 10.511180812435311
Epoch 628/10000, Prediction Accuracy = 58.134%, Loss = 0.7090574860572815
Epoch: 628, Batch Gradient Norm: 15.568084386871254
Epoch: 628, Batch Gradient Norm after: 15.568084386871254
Epoch 629/10000, Prediction Accuracy = 58.088%, Loss = 0.7336213707923889
Epoch: 629, Batch Gradient Norm: 12.748719182019435
Epoch: 629, Batch Gradient Norm after: 12.748719182019435
Epoch 630/10000, Prediction Accuracy = 58.258%, Loss = 0.7162119150161743
Epoch: 630, Batch Gradient Norm: 9.846423054403253
Epoch: 630, Batch Gradient Norm after: 9.846423054403253
Epoch 631/10000, Prediction Accuracy = 58.21%, Loss = 0.6945338368415832
Epoch: 631, Batch Gradient Norm: 14.128199900475323
Epoch: 631, Batch Gradient Norm after: 14.128199900475323
Epoch 632/10000, Prediction Accuracy = 58.112%, Loss = 0.727196216583252
Epoch: 632, Batch Gradient Norm: 9.648231718327178
Epoch: 632, Batch Gradient Norm after: 9.648231718327178
Epoch 633/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.6960900545120239
Epoch: 633, Batch Gradient Norm: 12.287679806725036
Epoch: 633, Batch Gradient Norm after: 12.287679806725036
Epoch 634/10000, Prediction Accuracy = 58.3%, Loss = 0.7167046785354614
Epoch: 634, Batch Gradient Norm: 12.344735301078538
Epoch: 634, Batch Gradient Norm after: 12.344735301078538
Epoch 635/10000, Prediction Accuracy = 58.136%, Loss = 0.718155038356781
Epoch: 635, Batch Gradient Norm: 11.483319212566649
Epoch: 635, Batch Gradient Norm after: 11.483319212566649
Epoch 636/10000, Prediction Accuracy = 58.29200000000001%, Loss = 0.7055905818939209
Epoch: 636, Batch Gradient Norm: 12.871126783796498
Epoch: 636, Batch Gradient Norm after: 12.871126783796498
Epoch 637/10000, Prediction Accuracy = 58.26400000000001%, Loss = 0.7124234199523926
Epoch: 637, Batch Gradient Norm: 11.375538704293982
Epoch: 637, Batch Gradient Norm after: 11.375538704293982
Epoch 638/10000, Prediction Accuracy = 58.262%, Loss = 0.7070666074752807
Epoch: 638, Batch Gradient Norm: 13.697924778257086
Epoch: 638, Batch Gradient Norm after: 13.697924778257086
Epoch 639/10000, Prediction Accuracy = 58.220000000000006%, Loss = 0.7196847081184388
Epoch: 639, Batch Gradient Norm: 13.121019606984662
Epoch: 639, Batch Gradient Norm after: 13.121019606984662
Epoch 640/10000, Prediction Accuracy = 58.2%, Loss = 0.7135727882385254
Epoch: 640, Batch Gradient Norm: 14.996793507709247
Epoch: 640, Batch Gradient Norm after: 14.996793507709247
Epoch 641/10000, Prediction Accuracy = 58.238%, Loss = 0.7231933116912842
Epoch: 641, Batch Gradient Norm: 13.24149029482057
Epoch: 641, Batch Gradient Norm after: 13.24149029482057
Epoch 642/10000, Prediction Accuracy = 58.227999999999994%, Loss = 0.7129501938819885
Epoch: 642, Batch Gradient Norm: 9.822758777562772
Epoch: 642, Batch Gradient Norm after: 9.822758777562772
Epoch 643/10000, Prediction Accuracy = 58.248000000000005%, Loss = 0.7018028974533081
Epoch: 643, Batch Gradient Norm: 12.185848646717142
Epoch: 643, Batch Gradient Norm after: 12.185848646717142
Epoch 644/10000, Prediction Accuracy = 58.354%, Loss = 0.7086403250694275
Epoch: 644, Batch Gradient Norm: 14.378241150222442
Epoch: 644, Batch Gradient Norm after: 14.378241150222442
Epoch 645/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.7273296475410461
Epoch: 645, Batch Gradient Norm: 12.197104490453091
Epoch: 645, Batch Gradient Norm after: 12.197104490453091
Epoch 646/10000, Prediction Accuracy = 58.29600000000001%, Loss = 0.7096030950546265
Epoch: 646, Batch Gradient Norm: 12.864834076725803
Epoch: 646, Batch Gradient Norm after: 12.864834076725803
Epoch 647/10000, Prediction Accuracy = 58.194%, Loss = 0.7148722290992737
Epoch: 647, Batch Gradient Norm: 11.206560734582366
Epoch: 647, Batch Gradient Norm after: 11.206560734582366
Epoch 648/10000, Prediction Accuracy = 58.3%, Loss = 0.7040731310844421
Epoch: 648, Batch Gradient Norm: 12.412226150764829
Epoch: 648, Batch Gradient Norm after: 12.412226150764829
Epoch 649/10000, Prediction Accuracy = 58.202%, Loss = 0.7052304983139038
Epoch: 649, Batch Gradient Norm: 14.089089557419769
Epoch: 649, Batch Gradient Norm after: 14.089089557419769
Epoch 650/10000, Prediction Accuracy = 58.312%, Loss = 0.7168788909912109
Epoch: 650, Batch Gradient Norm: 12.377029606630286
Epoch: 650, Batch Gradient Norm after: 12.377029606630286
Epoch 651/10000, Prediction Accuracy = 58.314%, Loss = 0.7073709011077881
Epoch: 651, Batch Gradient Norm: 13.245449824104435
Epoch: 651, Batch Gradient Norm after: 13.245449824104435
Epoch 652/10000, Prediction Accuracy = 58.327999999999996%, Loss = 0.71367267370224
Epoch: 652, Batch Gradient Norm: 14.142356803609614
Epoch: 652, Batch Gradient Norm after: 14.142356803609614
Epoch 653/10000, Prediction Accuracy = 58.464%, Loss = 0.7218307852745056
Epoch: 653, Batch Gradient Norm: 8.699139952248903
Epoch: 653, Batch Gradient Norm after: 8.699139952248903
Epoch 654/10000, Prediction Accuracy = 58.306%, Loss = 0.6864823222160339
Epoch: 654, Batch Gradient Norm: 9.112665537657163
Epoch: 654, Batch Gradient Norm after: 9.112665537657163
Epoch 655/10000, Prediction Accuracy = 58.388%, Loss = 0.6884358644485473
Epoch: 655, Batch Gradient Norm: 12.957536747564763
Epoch: 655, Batch Gradient Norm after: 12.957536747564763
Epoch 656/10000, Prediction Accuracy = 58.326%, Loss = 0.7091048717498779
Epoch: 656, Batch Gradient Norm: 18.669036492797918
Epoch: 656, Batch Gradient Norm after: 18.669036492797918
Epoch 657/10000, Prediction Accuracy = 58.366%, Loss = 0.752678120136261
Epoch: 657, Batch Gradient Norm: 11.430237660783304
Epoch: 657, Batch Gradient Norm after: 11.430237660783304
Epoch 658/10000, Prediction Accuracy = 58.318000000000005%, Loss = 0.6989011526107788
Epoch: 658, Batch Gradient Norm: 12.793458043153484
Epoch: 658, Batch Gradient Norm after: 12.793458043153484
Epoch 659/10000, Prediction Accuracy = 58.31%, Loss = 0.7231804132461548
Epoch: 659, Batch Gradient Norm: 12.157644415359004
Epoch: 659, Batch Gradient Norm after: 12.157644415359004
Epoch 660/10000, Prediction Accuracy = 58.39200000000001%, Loss = 0.7121588349342346
Epoch: 660, Batch Gradient Norm: 12.159721185808793
Epoch: 660, Batch Gradient Norm after: 12.159721185808793
Epoch 661/10000, Prediction Accuracy = 58.43800000000001%, Loss = 0.7034188985824585
Epoch: 661, Batch Gradient Norm: 7.163468123005626
Epoch: 661, Batch Gradient Norm after: 7.163468123005626
Epoch 662/10000, Prediction Accuracy = 58.396%, Loss = 0.6774405837059021
Epoch: 662, Batch Gradient Norm: 14.210420295808012
Epoch: 662, Batch Gradient Norm after: 14.210420295808012
Epoch 663/10000, Prediction Accuracy = 58.32800000000001%, Loss = 0.7154371857643127
Epoch: 663, Batch Gradient Norm: 17.288311499313004
Epoch: 663, Batch Gradient Norm after: 17.288311499313004
Epoch 664/10000, Prediction Accuracy = 58.366%, Loss = 0.7427602052688599
Epoch: 664, Batch Gradient Norm: 11.996582783043445
Epoch: 664, Batch Gradient Norm after: 11.996582783043445
Epoch 665/10000, Prediction Accuracy = 58.35600000000001%, Loss = 0.7020116090774536
Epoch: 665, Batch Gradient Norm: 9.757346993961628
Epoch: 665, Batch Gradient Norm after: 9.757346993961628
Epoch 666/10000, Prediction Accuracy = 58.40599999999999%, Loss = 0.6876113653182984
Epoch: 666, Batch Gradient Norm: 10.343585226310838
Epoch: 666, Batch Gradient Norm after: 10.343585226310838
Epoch 667/10000, Prediction Accuracy = 58.395999999999994%, Loss = 0.6896931767463684
Epoch: 667, Batch Gradient Norm: 11.343559791093446
Epoch: 667, Batch Gradient Norm after: 11.343559791093446
Epoch 668/10000, Prediction Accuracy = 58.434000000000005%, Loss = 0.6964667081832886
Epoch: 668, Batch Gradient Norm: 12.576370474633476
Epoch: 668, Batch Gradient Norm after: 12.576370474633476
Epoch 669/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.7061305522918702
Epoch: 669, Batch Gradient Norm: 13.732531000563673
Epoch: 669, Batch Gradient Norm after: 13.732531000563673
Epoch 670/10000, Prediction Accuracy = 58.477999999999994%, Loss = 0.7164654850959777
Epoch: 670, Batch Gradient Norm: 12.496371821231453
Epoch: 670, Batch Gradient Norm after: 12.496371821231453
Epoch 671/10000, Prediction Accuracy = 58.45%, Loss = 0.7041200518608093
Epoch: 671, Batch Gradient Norm: 9.087798731336685
Epoch: 671, Batch Gradient Norm after: 9.087798731336685
Epoch 672/10000, Prediction Accuracy = 58.438%, Loss = 0.6830573201179504
Epoch: 672, Batch Gradient Norm: 10.300169278147049
Epoch: 672, Batch Gradient Norm after: 10.300169278147049
Epoch 673/10000, Prediction Accuracy = 58.44199999999999%, Loss = 0.6898413300514221
Epoch: 673, Batch Gradient Norm: 14.723711648061656
Epoch: 673, Batch Gradient Norm after: 14.723711648061656
Epoch 674/10000, Prediction Accuracy = 58.414%, Loss = 0.7183144330978394
Epoch: 674, Batch Gradient Norm: 16.623215962082284
Epoch: 674, Batch Gradient Norm after: 16.623215962082284
Epoch 675/10000, Prediction Accuracy = 58.459999999999994%, Loss = 0.7306891202926635
Epoch: 675, Batch Gradient Norm: 12.998522725014285
Epoch: 675, Batch Gradient Norm after: 12.998522725014285
Epoch 676/10000, Prediction Accuracy = 58.378%, Loss = 0.7072876930236817
Epoch: 676, Batch Gradient Norm: 8.549333707774476
Epoch: 676, Batch Gradient Norm after: 8.549333707774476
Epoch 677/10000, Prediction Accuracy = 58.38399999999999%, Loss = 0.6835696816444397
Epoch: 677, Batch Gradient Norm: 8.560271148201071
Epoch: 677, Batch Gradient Norm after: 8.560271148201071
Epoch 678/10000, Prediction Accuracy = 58.480000000000004%, Loss = 0.678069007396698
Epoch: 678, Batch Gradient Norm: 14.654840024643217
Epoch: 678, Batch Gradient Norm after: 14.654840024643217
Epoch 679/10000, Prediction Accuracy = 58.507999999999996%, Loss = 0.7184016704559326
Epoch: 679, Batch Gradient Norm: 13.594140190547488
Epoch: 679, Batch Gradient Norm after: 13.594140190547488
Epoch 680/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.7105319023132324
Epoch: 680, Batch Gradient Norm: 7.874638970051659
Epoch: 680, Batch Gradient Norm after: 7.874638970051659
Epoch 681/10000, Prediction Accuracy = 58.512%, Loss = 0.6768510818481446
Epoch: 681, Batch Gradient Norm: 7.595193434332262
Epoch: 681, Batch Gradient Norm after: 7.595193434332262
Epoch 682/10000, Prediction Accuracy = 58.5%, Loss = 0.6769699931144715
Epoch: 682, Batch Gradient Norm: 14.120932845624491
Epoch: 682, Batch Gradient Norm after: 14.120932845624491
Epoch 683/10000, Prediction Accuracy = 58.462%, Loss = 0.708199429512024
Epoch: 683, Batch Gradient Norm: 16.618671597225138
Epoch: 683, Batch Gradient Norm after: 16.618671597225138
Epoch 684/10000, Prediction Accuracy = 58.448%, Loss = 0.7248441815376282
Epoch: 684, Batch Gradient Norm: 16.989289481813373
Epoch: 684, Batch Gradient Norm after: 16.989289481813373
Epoch 685/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.7353694200515747
Epoch: 685, Batch Gradient Norm: 14.335812827388619
Epoch: 685, Batch Gradient Norm after: 14.335812827388619
Epoch 686/10000, Prediction Accuracy = 58.44%, Loss = 0.716189694404602
Epoch: 686, Batch Gradient Norm: 12.924575913741291
Epoch: 686, Batch Gradient Norm after: 12.924575913741291
Epoch 687/10000, Prediction Accuracy = 58.504%, Loss = 0.7029002785682679
Epoch: 687, Batch Gradient Norm: 10.44770903763543
Epoch: 687, Batch Gradient Norm after: 10.44770903763543
Epoch 688/10000, Prediction Accuracy = 58.4%, Loss = 0.6906691551208496
Epoch: 688, Batch Gradient Norm: 9.061850740881951
Epoch: 688, Batch Gradient Norm after: 9.061850740881951
Epoch 689/10000, Prediction Accuracy = 58.482000000000006%, Loss = 0.6808302521705627
Epoch: 689, Batch Gradient Norm: 11.77790699668075
Epoch: 689, Batch Gradient Norm after: 11.77790699668075
Epoch 690/10000, Prediction Accuracy = 58.538%, Loss = 0.6944456577301026
Epoch: 690, Batch Gradient Norm: 11.291962298281975
Epoch: 690, Batch Gradient Norm after: 11.291962298281975
Epoch 691/10000, Prediction Accuracy = 58.55800000000001%, Loss = 0.6872470378875732
Epoch: 691, Batch Gradient Norm: 15.857875847497226
Epoch: 691, Batch Gradient Norm after: 15.857875847497226
Epoch 692/10000, Prediction Accuracy = 58.498000000000005%, Loss = 0.7223691344261169
Epoch: 692, Batch Gradient Norm: 11.366848702850135
Epoch: 692, Batch Gradient Norm after: 11.366848702850135
Epoch 693/10000, Prediction Accuracy = 58.501999999999995%, Loss = 0.6894867420196533
Epoch: 693, Batch Gradient Norm: 10.409790567214873
Epoch: 693, Batch Gradient Norm after: 10.409790567214873
Epoch 694/10000, Prediction Accuracy = 58.464%, Loss = 0.6856851816177368
Epoch: 694, Batch Gradient Norm: 9.545227837352662
Epoch: 694, Batch Gradient Norm after: 9.545227837352662
Epoch 695/10000, Prediction Accuracy = 58.54599999999999%, Loss = 0.6798206806182862
Epoch: 695, Batch Gradient Norm: 15.270013953536639
Epoch: 695, Batch Gradient Norm after: 15.270013953536639
Epoch 696/10000, Prediction Accuracy = 58.602%, Loss = 0.7161651134490967
Epoch: 696, Batch Gradient Norm: 14.698605629631274
Epoch: 696, Batch Gradient Norm after: 14.698605629631274
Epoch 697/10000, Prediction Accuracy = 58.676%, Loss = 0.7126954197883606
Epoch: 697, Batch Gradient Norm: 13.618422259978017
Epoch: 697, Batch Gradient Norm after: 13.618422259978017
Epoch 698/10000, Prediction Accuracy = 58.568000000000005%, Loss = 0.7076667428016663
Epoch: 698, Batch Gradient Norm: 6.938669879900133
Epoch: 698, Batch Gradient Norm after: 6.938669879900133
Epoch 699/10000, Prediction Accuracy = 58.634%, Loss = 0.668450677394867
Epoch: 699, Batch Gradient Norm: 10.967501766442405
Epoch: 699, Batch Gradient Norm after: 10.967501766442405
Epoch 700/10000, Prediction Accuracy = 58.513999999999996%, Loss = 0.6893331170082092
Epoch: 700, Batch Gradient Norm: 10.572196041042886
Epoch: 700, Batch Gradient Norm after: 10.572196041042886
Epoch 701/10000, Prediction Accuracy = 58.605999999999995%, Loss = 0.6846776723861694
Epoch: 701, Batch Gradient Norm: 12.352792023419576
Epoch: 701, Batch Gradient Norm after: 12.352792023419576
Epoch 702/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.701851201057434
Epoch: 702, Batch Gradient Norm: 16.02956274551496
Epoch: 702, Batch Gradient Norm after: 16.02956274551496
Epoch 703/10000, Prediction Accuracy = 58.596000000000004%, Loss = 0.7179898977279663
Epoch: 703, Batch Gradient Norm: 14.703272026032414
Epoch: 703, Batch Gradient Norm after: 14.703272026032414
Epoch 704/10000, Prediction Accuracy = 58.64%, Loss = 0.7134225249290467
Epoch: 704, Batch Gradient Norm: 13.29010514592065
Epoch: 704, Batch Gradient Norm after: 13.29010514592065
Epoch 705/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.7005928158760071
Epoch: 705, Batch Gradient Norm: 10.053656071465257
Epoch: 705, Batch Gradient Norm after: 10.053656071465257
Epoch 706/10000, Prediction Accuracy = 58.672000000000004%, Loss = 0.6828577637672424
Epoch: 706, Batch Gradient Norm: 9.200493237247857
Epoch: 706, Batch Gradient Norm after: 9.200493237247857
Epoch 707/10000, Prediction Accuracy = 58.58200000000001%, Loss = 0.6753651857376098
Epoch: 707, Batch Gradient Norm: 11.695427075519895
Epoch: 707, Batch Gradient Norm after: 11.695427075519895
Epoch 708/10000, Prediction Accuracy = 58.534000000000006%, Loss = 0.6923457026481629
Epoch: 708, Batch Gradient Norm: 11.284120480246486
Epoch: 708, Batch Gradient Norm after: 11.284120480246486
Epoch 709/10000, Prediction Accuracy = 58.55%, Loss = 0.6916594624519348
Epoch: 709, Batch Gradient Norm: 13.531655734033194
Epoch: 709, Batch Gradient Norm after: 13.531655734033194
Epoch 710/10000, Prediction Accuracy = 58.431999999999995%, Loss = 0.707274878025055
Epoch: 710, Batch Gradient Norm: 14.691140413902204
Epoch: 710, Batch Gradient Norm after: 14.691140413902204
Epoch 711/10000, Prediction Accuracy = 58.70799999999999%, Loss = 0.7099899649620056
Epoch: 711, Batch Gradient Norm: 12.542102281719512
Epoch: 711, Batch Gradient Norm after: 12.542102281719512
Epoch 712/10000, Prediction Accuracy = 58.512%, Loss = 0.6982610583305359
Epoch: 712, Batch Gradient Norm: 11.915174721599682
Epoch: 712, Batch Gradient Norm after: 11.915174721599682
Epoch 713/10000, Prediction Accuracy = 58.588%, Loss = 0.6949135780334472
Epoch: 713, Batch Gradient Norm: 8.649063453801832
Epoch: 713, Batch Gradient Norm after: 8.649063453801832
Epoch 714/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.6749955296516419
Epoch: 714, Batch Gradient Norm: 16.18207635718724
Epoch: 714, Batch Gradient Norm after: 16.18207635718724
Epoch 715/10000, Prediction Accuracy = 58.712%, Loss = 0.7198589205741882
Epoch: 715, Batch Gradient Norm: 15.808843625872623
Epoch: 715, Batch Gradient Norm after: 15.808843625872623
Epoch 716/10000, Prediction Accuracy = 58.708000000000006%, Loss = 0.7201240658760071
Epoch: 716, Batch Gradient Norm: 11.028614495158841
Epoch: 716, Batch Gradient Norm after: 11.028614495158841
Epoch 717/10000, Prediction Accuracy = 58.62800000000001%, Loss = 0.6833109498023987
Epoch: 717, Batch Gradient Norm: 10.515490159379087
Epoch: 717, Batch Gradient Norm after: 10.515490159379087
Epoch 718/10000, Prediction Accuracy = 58.64399999999999%, Loss = 0.6907614946365357
Epoch: 718, Batch Gradient Norm: 13.423407246867638
Epoch: 718, Batch Gradient Norm after: 13.423407246867638
Epoch 719/10000, Prediction Accuracy = 58.58%, Loss = 0.6972902774810791
Epoch: 719, Batch Gradient Norm: 14.34538727818333
Epoch: 719, Batch Gradient Norm after: 14.34538727818333
Epoch 720/10000, Prediction Accuracy = 58.652%, Loss = 0.7060773968696594
Epoch: 720, Batch Gradient Norm: 15.156857039762574
Epoch: 720, Batch Gradient Norm after: 15.156857039762574
Epoch 721/10000, Prediction Accuracy = 58.714%, Loss = 0.711707603931427
Epoch: 721, Batch Gradient Norm: 11.642222251764103
Epoch: 721, Batch Gradient Norm after: 11.642222251764103
Epoch 722/10000, Prediction Accuracy = 58.69%, Loss = 0.6896011114120484
Epoch: 722, Batch Gradient Norm: 7.52002735201341
Epoch: 722, Batch Gradient Norm after: 7.52002735201341
Epoch 723/10000, Prediction Accuracy = 58.69200000000001%, Loss = 0.6675849676132202
Epoch: 723, Batch Gradient Norm: 8.27069907691362
Epoch: 723, Batch Gradient Norm after: 8.27069907691362
Epoch 724/10000, Prediction Accuracy = 58.644000000000005%, Loss = 0.6683327317237854
Epoch: 724, Batch Gradient Norm: 15.580721933297381
Epoch: 724, Batch Gradient Norm after: 15.580721933297381
Epoch 725/10000, Prediction Accuracy = 58.562%, Loss = 0.7140321135520935
Epoch: 725, Batch Gradient Norm: 15.639070518670993
Epoch: 725, Batch Gradient Norm after: 15.639070518670993
Epoch 726/10000, Prediction Accuracy = 58.598%, Loss = 0.715075695514679
Epoch: 726, Batch Gradient Norm: 9.334092173954566
Epoch: 726, Batch Gradient Norm after: 9.334092173954566
Epoch 727/10000, Prediction Accuracy = 58.674%, Loss = 0.6702531814575196
Epoch: 727, Batch Gradient Norm: 11.691622930725885
Epoch: 727, Batch Gradient Norm after: 11.691622930725885
Epoch 728/10000, Prediction Accuracy = 58.7%, Loss = 0.6806667685508728
Epoch: 728, Batch Gradient Norm: 13.796218753927532
Epoch: 728, Batch Gradient Norm after: 13.796218753927532
Epoch 729/10000, Prediction Accuracy = 58.69%, Loss = 0.6988400101661683
Epoch: 729, Batch Gradient Norm: 13.442702773288257
Epoch: 729, Batch Gradient Norm after: 13.442702773288257
Epoch 730/10000, Prediction Accuracy = 58.536%, Loss = 0.7094177603721619
Epoch: 730, Batch Gradient Norm: 8.426186035669657
Epoch: 730, Batch Gradient Norm after: 8.426186035669657
Epoch 731/10000, Prediction Accuracy = 58.67%, Loss = 0.6681426405906677
Epoch: 731, Batch Gradient Norm: 7.2645492235622475
Epoch: 731, Batch Gradient Norm after: 7.2645492235622475
Epoch 732/10000, Prediction Accuracy = 58.662%, Loss = 0.6619814276695252
Epoch: 732, Batch Gradient Norm: 12.86607501104326
Epoch: 732, Batch Gradient Norm after: 12.86607501104326
Epoch 733/10000, Prediction Accuracy = 58.636%, Loss = 0.6872379660606385
Epoch: 733, Batch Gradient Norm: 16.97960984009484
Epoch: 733, Batch Gradient Norm after: 16.97960984009484
Epoch 734/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.7225380539894104
Epoch: 734, Batch Gradient Norm: 12.339470792704793
Epoch: 734, Batch Gradient Norm after: 12.339470792704793
Epoch 735/10000, Prediction Accuracy = 58.666%, Loss = 0.6924225687980652
Epoch: 735, Batch Gradient Norm: 8.308368427943444
Epoch: 735, Batch Gradient Norm after: 8.308368427943444
Epoch 736/10000, Prediction Accuracy = 58.734%, Loss = 0.6706407308578491
Epoch: 736, Batch Gradient Norm: 10.15184512611417
Epoch: 736, Batch Gradient Norm after: 10.15184512611417
Epoch 737/10000, Prediction Accuracy = 58.664%, Loss = 0.6836033582687377
Epoch: 737, Batch Gradient Norm: 10.375658755798952
Epoch: 737, Batch Gradient Norm after: 10.375658755798952
Epoch 738/10000, Prediction Accuracy = 58.69799999999999%, Loss = 0.6760136723518372
Epoch: 738, Batch Gradient Norm: 11.070845216985633
Epoch: 738, Batch Gradient Norm after: 11.070845216985633
Epoch 739/10000, Prediction Accuracy = 58.686%, Loss = 0.6817404270172119
Epoch: 739, Batch Gradient Norm: 14.27193238805998
Epoch: 739, Batch Gradient Norm after: 14.27193238805998
Epoch 740/10000, Prediction Accuracy = 58.720000000000006%, Loss = 0.6941993236541748
Epoch: 740, Batch Gradient Norm: 17.59249235747681
Epoch: 740, Batch Gradient Norm after: 17.59249235747681
Epoch 741/10000, Prediction Accuracy = 58.588%, Loss = 0.7247244477272033
Epoch: 741, Batch Gradient Norm: 16.352705596186745
Epoch: 741, Batch Gradient Norm after: 16.352705596186745
Epoch 742/10000, Prediction Accuracy = 58.684000000000005%, Loss = 0.7145642161369323
Epoch: 742, Batch Gradient Norm: 13.34512845068459
Epoch: 742, Batch Gradient Norm after: 13.34512845068459
Epoch 743/10000, Prediction Accuracy = 58.694%, Loss = 0.695263147354126
Epoch: 743, Batch Gradient Norm: 11.811066545685703
Epoch: 743, Batch Gradient Norm after: 11.811066545685703
Epoch 744/10000, Prediction Accuracy = 58.720000000000006%, Loss = 0.6830814838409424
Epoch: 744, Batch Gradient Norm: 8.488021814863075
Epoch: 744, Batch Gradient Norm after: 8.488021814863075
Epoch 745/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.6663685798645019
Epoch: 745, Batch Gradient Norm: 8.176398411660776
Epoch: 745, Batch Gradient Norm after: 8.176398411660776
Epoch 746/10000, Prediction Accuracy = 58.74799999999999%, Loss = 0.665459144115448
Epoch: 746, Batch Gradient Norm: 7.007859609459894
Epoch: 746, Batch Gradient Norm after: 7.007859609459894
Epoch 747/10000, Prediction Accuracy = 58.74400000000001%, Loss = 0.6597282767295838
Epoch: 747, Batch Gradient Norm: 5.945103139093492
Epoch: 747, Batch Gradient Norm after: 5.945103139093492
Epoch 748/10000, Prediction Accuracy = 58.745999999999995%, Loss = 0.6518379926681519
Epoch: 748, Batch Gradient Norm: 17.182690459293372
Epoch: 748, Batch Gradient Norm after: 16.842410241013035
Epoch 749/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.710318410396576
Epoch: 749, Batch Gradient Norm: 20.058143650182803
Epoch: 749, Batch Gradient Norm after: 19.694291123607087
Epoch 750/10000, Prediction Accuracy = 58.71%, Loss = 0.7541083931922913
Epoch: 750, Batch Gradient Norm: 11.494392279860145
Epoch: 750, Batch Gradient Norm after: 11.494392279860145
Epoch 751/10000, Prediction Accuracy = 58.754%, Loss = 0.6814540266990662
Epoch: 751, Batch Gradient Norm: 10.26239001163405
Epoch: 751, Batch Gradient Norm after: 10.26239001163405
Epoch 752/10000, Prediction Accuracy = 58.78399999999999%, Loss = 0.672345769405365
Epoch: 752, Batch Gradient Norm: 13.970901320216113
Epoch: 752, Batch Gradient Norm after: 13.970901320216113
Epoch 753/10000, Prediction Accuracy = 58.739999999999995%, Loss = 0.7051057100296021
Epoch: 753, Batch Gradient Norm: 9.306213209596203
Epoch: 753, Batch Gradient Norm after: 9.306213209596203
Epoch 754/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.6730511307716369
Epoch: 754, Batch Gradient Norm: 12.404938151895937
Epoch: 754, Batch Gradient Norm after: 12.404938151895937
Epoch 755/10000, Prediction Accuracy = 58.73199999999999%, Loss = 0.6844745755195618
Epoch: 755, Batch Gradient Norm: 14.494415951401693
Epoch: 755, Batch Gradient Norm after: 14.494415951401693
Epoch 756/10000, Prediction Accuracy = 58.73199999999999%, Loss = 0.6972956657409668
Epoch: 756, Batch Gradient Norm: 14.44330491222914
Epoch: 756, Batch Gradient Norm after: 14.44330491222914
Epoch 757/10000, Prediction Accuracy = 58.69199999999999%, Loss = 0.7104720950126648
Epoch: 757, Batch Gradient Norm: 13.210684472473194
Epoch: 757, Batch Gradient Norm after: 13.210684472473194
Epoch 758/10000, Prediction Accuracy = 58.758%, Loss = 0.6952743887901306
Epoch: 758, Batch Gradient Norm: 11.005863001535777
Epoch: 758, Batch Gradient Norm after: 11.005863001535777
Epoch 759/10000, Prediction Accuracy = 58.803999999999995%, Loss = 0.6773671388626099
Epoch: 759, Batch Gradient Norm: 12.024687200248836
Epoch: 759, Batch Gradient Norm after: 12.024687200248836
Epoch 760/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.6864946722984314
Epoch: 760, Batch Gradient Norm: 9.617322270289083
Epoch: 760, Batch Gradient Norm after: 9.617322270289083
Epoch 761/10000, Prediction Accuracy = 58.693999999999996%, Loss = 0.6673908829689026
Epoch: 761, Batch Gradient Norm: 12.458632666918565
Epoch: 761, Batch Gradient Norm after: 12.458632666918565
Epoch 762/10000, Prediction Accuracy = 58.89%, Loss = 0.6822010397911071
Epoch: 762, Batch Gradient Norm: 13.32689538751559
Epoch: 762, Batch Gradient Norm after: 13.32689538751559
Epoch 763/10000, Prediction Accuracy = 58.766000000000005%, Loss = 0.6945658087730407
Epoch: 763, Batch Gradient Norm: 11.2269299036092
Epoch: 763, Batch Gradient Norm after: 11.2269299036092
Epoch 764/10000, Prediction Accuracy = 58.751999999999995%, Loss = 0.6733582019805908
Epoch: 764, Batch Gradient Norm: 12.244311768875646
Epoch: 764, Batch Gradient Norm after: 12.244311768875646
Epoch 765/10000, Prediction Accuracy = 58.83%, Loss = 0.6820885539054871
Epoch: 765, Batch Gradient Norm: 11.444841274917945
Epoch: 765, Batch Gradient Norm after: 11.444841274917945
Epoch 766/10000, Prediction Accuracy = 58.83200000000001%, Loss = 0.6771510601043701
Epoch: 766, Batch Gradient Norm: 12.037534591483244
Epoch: 766, Batch Gradient Norm after: 12.037534591483244
Epoch 767/10000, Prediction Accuracy = 58.798%, Loss = 0.6784749746322631
Epoch: 767, Batch Gradient Norm: 12.398111360077854
Epoch: 767, Batch Gradient Norm after: 12.398111360077854
Epoch 768/10000, Prediction Accuracy = 58.763999999999996%, Loss = 0.6861612558364868
Epoch: 768, Batch Gradient Norm: 9.105505880042996
Epoch: 768, Batch Gradient Norm after: 9.105505880042996
Epoch 769/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.6625978589057923
Epoch: 769, Batch Gradient Norm: 11.27023246898295
Epoch: 769, Batch Gradient Norm after: 11.27023246898295
Epoch 770/10000, Prediction Accuracy = 58.791999999999994%, Loss = 0.6739074945449829
Epoch: 770, Batch Gradient Norm: 14.288014754751387
Epoch: 770, Batch Gradient Norm after: 14.288014754751387
Epoch 771/10000, Prediction Accuracy = 58.842%, Loss = 0.6998929142951965
Epoch: 771, Batch Gradient Norm: 13.170756530179643
Epoch: 771, Batch Gradient Norm after: 13.170756530179643
Epoch 772/10000, Prediction Accuracy = 58.75600000000001%, Loss = 0.6949472308158875
Epoch: 772, Batch Gradient Norm: 11.474434431635443
Epoch: 772, Batch Gradient Norm after: 11.474434431635443
Epoch 773/10000, Prediction Accuracy = 58.826%, Loss = 0.6753986239433288
Epoch: 773, Batch Gradient Norm: 18.027998519698645
Epoch: 773, Batch Gradient Norm after: 18.027998519698645
Epoch 774/10000, Prediction Accuracy = 58.831999999999994%, Loss = 0.7217490792274475
Epoch: 774, Batch Gradient Norm: 18.259371922621966
Epoch: 774, Batch Gradient Norm after: 18.259371922621966
Epoch 775/10000, Prediction Accuracy = 58.70399999999999%, Loss = 0.725490128993988
Epoch: 775, Batch Gradient Norm: 9.921750000349963
Epoch: 775, Batch Gradient Norm after: 9.921750000349963
Epoch 776/10000, Prediction Accuracy = 58.814%, Loss = 0.6670895934104919
Epoch: 776, Batch Gradient Norm: 10.702713097565953
Epoch: 776, Batch Gradient Norm after: 10.702713097565953
Epoch 777/10000, Prediction Accuracy = 58.733999999999995%, Loss = 0.6751959681510925
Epoch: 777, Batch Gradient Norm: 8.50065496089381
Epoch: 777, Batch Gradient Norm after: 8.50065496089381
Epoch 778/10000, Prediction Accuracy = 58.83%, Loss = 0.6599791884422302
Epoch: 778, Batch Gradient Norm: 11.287308424925278
Epoch: 778, Batch Gradient Norm after: 11.287308424925278
Epoch 779/10000, Prediction Accuracy = 58.862%, Loss = 0.6744884848594666
Epoch: 779, Batch Gradient Norm: 13.917893897402008
Epoch: 779, Batch Gradient Norm after: 13.917893897402008
Epoch 780/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.6954166173934937
Epoch: 780, Batch Gradient Norm: 12.988453598162563
Epoch: 780, Batch Gradient Norm after: 12.988453598162563
Epoch 781/10000, Prediction Accuracy = 58.794%, Loss = 0.683467161655426
Epoch: 781, Batch Gradient Norm: 10.57617190078483
Epoch: 781, Batch Gradient Norm after: 10.57617190078483
Epoch 782/10000, Prediction Accuracy = 58.914%, Loss = 0.6684916138648986
Epoch: 782, Batch Gradient Norm: 12.05407852758703
Epoch: 782, Batch Gradient Norm after: 12.05407852758703
Epoch 783/10000, Prediction Accuracy = 58.826%, Loss = 0.6751220107078553
Epoch: 783, Batch Gradient Norm: 16.197593867730507
Epoch: 783, Batch Gradient Norm after: 16.197593867730507
Epoch 784/10000, Prediction Accuracy = 58.842000000000006%, Loss = 0.7060490369796752
Epoch: 784, Batch Gradient Norm: 15.782966257419103
Epoch: 784, Batch Gradient Norm after: 15.782966257419103
Epoch 785/10000, Prediction Accuracy = 58.92%, Loss = 0.7038704991340637
Epoch: 785, Batch Gradient Norm: 11.224409983987305
Epoch: 785, Batch Gradient Norm after: 11.224409983987305
Epoch 786/10000, Prediction Accuracy = 58.855999999999995%, Loss = 0.6744319081306458
Epoch: 786, Batch Gradient Norm: 7.278362463082402
Epoch: 786, Batch Gradient Norm after: 7.278362463082402
Epoch 787/10000, Prediction Accuracy = 58.932%, Loss = 0.6493446826934814
Epoch: 787, Batch Gradient Norm: 11.498909815696214
Epoch: 787, Batch Gradient Norm after: 11.498909815696214
Epoch 788/10000, Prediction Accuracy = 58.831999999999994%, Loss = 0.6759560465812683
Epoch: 788, Batch Gradient Norm: 10.56291646139379
Epoch: 788, Batch Gradient Norm after: 10.56291646139379
Epoch 789/10000, Prediction Accuracy = 58.77%, Loss = 0.670904004573822
Epoch: 789, Batch Gradient Norm: 10.559977344431433
Epoch: 789, Batch Gradient Norm after: 10.559977344431433
Epoch 790/10000, Prediction Accuracy = 58.85%, Loss = 0.6669911503791809
Epoch: 790, Batch Gradient Norm: 12.998253472958751
Epoch: 790, Batch Gradient Norm after: 12.998253472958751
Epoch 791/10000, Prediction Accuracy = 58.908%, Loss = 0.6791465282440186
Epoch: 791, Batch Gradient Norm: 13.340054146939401
Epoch: 791, Batch Gradient Norm after: 13.340054146939401
Epoch 792/10000, Prediction Accuracy = 58.89200000000001%, Loss = 0.6817594647407532
Epoch: 792, Batch Gradient Norm: 11.889408074607672
Epoch: 792, Batch Gradient Norm after: 11.889408074607672
Epoch 793/10000, Prediction Accuracy = 58.96199999999999%, Loss = 0.6715423464775085
Epoch: 793, Batch Gradient Norm: 12.67723281379662
Epoch: 793, Batch Gradient Norm after: 12.67723281379662
Epoch 794/10000, Prediction Accuracy = 58.866%, Loss = 0.682400906085968
Epoch: 794, Batch Gradient Norm: 11.003260491951448
Epoch: 794, Batch Gradient Norm after: 11.003260491951448
Epoch 795/10000, Prediction Accuracy = 58.912%, Loss = 0.6667149782180786
Epoch: 795, Batch Gradient Norm: 9.865811303332276
Epoch: 795, Batch Gradient Norm after: 9.865811303332276
Epoch 796/10000, Prediction Accuracy = 58.95%, Loss = 0.6685811042785644
Epoch: 796, Batch Gradient Norm: 12.704186388790335
Epoch: 796, Batch Gradient Norm after: 12.704186388790335
Epoch 797/10000, Prediction Accuracy = 58.788%, Loss = 0.6817497372627258
Epoch: 797, Batch Gradient Norm: 17.490584033774553
Epoch: 797, Batch Gradient Norm after: 17.490584033774553
Epoch 798/10000, Prediction Accuracy = 58.95%, Loss = 0.7167044162750245
Epoch: 798, Batch Gradient Norm: 14.246912487265181
Epoch: 798, Batch Gradient Norm after: 14.246912487265181
Epoch 799/10000, Prediction Accuracy = 58.896%, Loss = 0.6922163605690003
Epoch: 799, Batch Gradient Norm: 8.223238232435925
Epoch: 799, Batch Gradient Norm after: 8.223238232435925
Epoch 800/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.6535691738128662
Epoch: 800, Batch Gradient Norm: 11.227152067381452
Epoch: 800, Batch Gradient Norm after: 11.227152067381452
Epoch 801/10000, Prediction Accuracy = 58.90599999999999%, Loss = 0.6722715258598327
Epoch: 801, Batch Gradient Norm: 13.413587290217887
Epoch: 801, Batch Gradient Norm after: 13.413587290217887
Epoch 802/10000, Prediction Accuracy = 58.91799999999999%, Loss = 0.6867316961288452
Epoch: 802, Batch Gradient Norm: 11.72520529743578
Epoch: 802, Batch Gradient Norm after: 11.72520529743578
Epoch 803/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.6698257684707641
Epoch: 803, Batch Gradient Norm: 8.23454929722437
Epoch: 803, Batch Gradient Norm after: 8.23454929722437
Epoch 804/10000, Prediction Accuracy = 58.926%, Loss = 0.6507792234420776
Epoch: 804, Batch Gradient Norm: 10.385552445380311
Epoch: 804, Batch Gradient Norm after: 10.385552445380311
Epoch 805/10000, Prediction Accuracy = 58.910000000000004%, Loss = 0.6648683428764344
Epoch: 805, Batch Gradient Norm: 15.776705791529409
Epoch: 805, Batch Gradient Norm after: 15.776705791529409
Epoch 806/10000, Prediction Accuracy = 58.936%, Loss = 0.7001521468162537
Epoch: 806, Batch Gradient Norm: 13.945404370694526
Epoch: 806, Batch Gradient Norm after: 13.945404370694526
Epoch 807/10000, Prediction Accuracy = 58.876%, Loss = 0.6834018111228943
Epoch: 807, Batch Gradient Norm: 9.863400038825793
Epoch: 807, Batch Gradient Norm after: 9.863400038825793
Epoch 808/10000, Prediction Accuracy = 58.95400000000001%, Loss = 0.661281681060791
Epoch: 808, Batch Gradient Norm: 12.615510642231213
Epoch: 808, Batch Gradient Norm after: 12.615510642231213
Epoch 809/10000, Prediction Accuracy = 58.86%, Loss = 0.6750980377197265
Epoch: 809, Batch Gradient Norm: 14.744798267570184
Epoch: 809, Batch Gradient Norm after: 14.744798267570184
Epoch 810/10000, Prediction Accuracy = 58.879999999999995%, Loss = 0.6936798572540284
Epoch: 810, Batch Gradient Norm: 12.273181047944048
Epoch: 810, Batch Gradient Norm after: 12.273181047944048
Epoch 811/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.6773453235626221
Epoch: 811, Batch Gradient Norm: 12.017543087521007
Epoch: 811, Batch Gradient Norm after: 12.017543087521007
Epoch 812/10000, Prediction Accuracy = 58.916%, Loss = 0.6734004855155945
Epoch: 812, Batch Gradient Norm: 15.046600329945546
Epoch: 812, Batch Gradient Norm after: 15.046600329945546
Epoch 813/10000, Prediction Accuracy = 59.0%, Loss = 0.6954091310501098
Epoch: 813, Batch Gradient Norm: 14.836233560540876
Epoch: 813, Batch Gradient Norm after: 14.836233560540876
Epoch 814/10000, Prediction Accuracy = 59.025999999999996%, Loss = 0.6954041004180909
Epoch: 814, Batch Gradient Norm: 10.402369076113313
Epoch: 814, Batch Gradient Norm after: 10.402369076113313
Epoch 815/10000, Prediction Accuracy = 59.017999999999994%, Loss = 0.6651633501052856
Epoch: 815, Batch Gradient Norm: 8.633370335559613
Epoch: 815, Batch Gradient Norm after: 8.633370335559613
Epoch 816/10000, Prediction Accuracy = 58.95%, Loss = 0.650986111164093
Epoch: 816, Batch Gradient Norm: 9.557530749223453
Epoch: 816, Batch Gradient Norm after: 9.557530749223453
Epoch 817/10000, Prediction Accuracy = 59.025999999999996%, Loss = 0.6573292255401612
Epoch: 817, Batch Gradient Norm: 10.14129782477231
Epoch: 817, Batch Gradient Norm after: 10.14129782477231
Epoch 818/10000, Prediction Accuracy = 58.912%, Loss = 0.658147931098938
Epoch: 818, Batch Gradient Norm: 12.564737804232728
Epoch: 818, Batch Gradient Norm after: 12.564737804232728
Epoch 819/10000, Prediction Accuracy = 58.85999999999999%, Loss = 0.6726624965667725
Epoch: 819, Batch Gradient Norm: 13.400120994828233
Epoch: 819, Batch Gradient Norm after: 13.400120994828233
Epoch 820/10000, Prediction Accuracy = 58.93000000000001%, Loss = 0.6765882492065429
Epoch: 820, Batch Gradient Norm: 12.859001344438164
Epoch: 820, Batch Gradient Norm after: 12.859001344438164
Epoch 821/10000, Prediction Accuracy = 59.017999999999994%, Loss = 0.6748515844345093
Epoch: 821, Batch Gradient Norm: 14.477324150143657
Epoch: 821, Batch Gradient Norm after: 14.477324150143657
Epoch 822/10000, Prediction Accuracy = 58.988%, Loss = 0.689962363243103
Epoch: 822, Batch Gradient Norm: 14.040444404900473
Epoch: 822, Batch Gradient Norm after: 14.040444404900473
Epoch 823/10000, Prediction Accuracy = 58.984%, Loss = 0.6819389820098877
Epoch: 823, Batch Gradient Norm: 11.522009155241246
Epoch: 823, Batch Gradient Norm after: 11.522009155241246
Epoch 824/10000, Prediction Accuracy = 58.998000000000005%, Loss = 0.6645728707313537
Epoch: 824, Batch Gradient Norm: 12.20611136314292
Epoch: 824, Batch Gradient Norm after: 12.20611136314292
Epoch 825/10000, Prediction Accuracy = 58.964%, Loss = 0.6731663584709168
Epoch: 825, Batch Gradient Norm: 11.32808712246179
Epoch: 825, Batch Gradient Norm after: 11.32808712246179
Epoch 826/10000, Prediction Accuracy = 59.048%, Loss = 0.6625914216041565
Epoch: 826, Batch Gradient Norm: 11.105621991014207
Epoch: 826, Batch Gradient Norm after: 11.105621991014207
Epoch 827/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.6643288612365723
Epoch: 827, Batch Gradient Norm: 12.844104126683272
Epoch: 827, Batch Gradient Norm after: 12.844104126683272
Epoch 828/10000, Prediction Accuracy = 58.988%, Loss = 0.6707128643989563
Epoch: 828, Batch Gradient Norm: 12.981526716535013
Epoch: 828, Batch Gradient Norm after: 12.981526716535013
Epoch 829/10000, Prediction Accuracy = 58.9%, Loss = 0.6734537959098816
Epoch: 829, Batch Gradient Norm: 11.304565406434483
Epoch: 829, Batch Gradient Norm after: 11.304565406434483
Epoch 830/10000, Prediction Accuracy = 59.06199999999999%, Loss = 0.6663400411605835
Epoch: 830, Batch Gradient Norm: 14.707041344050541
Epoch: 830, Batch Gradient Norm after: 14.707041344050541
Epoch 831/10000, Prediction Accuracy = 59.048%, Loss = 0.6928916811943054
Epoch: 831, Batch Gradient Norm: 12.558426507113772
Epoch: 831, Batch Gradient Norm after: 12.558426507113772
Epoch 832/10000, Prediction Accuracy = 59.036%, Loss = 0.6734306812286377
Epoch: 832, Batch Gradient Norm: 12.323670484871027
Epoch: 832, Batch Gradient Norm after: 12.323670484871027
Epoch 833/10000, Prediction Accuracy = 58.996%, Loss = 0.6672530174255371
Epoch: 833, Batch Gradient Norm: 10.99432496010391
Epoch: 833, Batch Gradient Norm after: 10.99432496010391
Epoch 834/10000, Prediction Accuracy = 59.144000000000005%, Loss = 0.6602800488471985
Epoch: 834, Batch Gradient Norm: 10.928896302187777
Epoch: 834, Batch Gradient Norm after: 10.928896302187777
Epoch 835/10000, Prediction Accuracy = 59.13399999999999%, Loss = 0.6662294626235962
Epoch: 835, Batch Gradient Norm: 13.088602629605889
Epoch: 835, Batch Gradient Norm after: 13.088602629605889
Epoch 836/10000, Prediction Accuracy = 59.01800000000001%, Loss = 0.676704216003418
Epoch: 836, Batch Gradient Norm: 11.561084482055405
Epoch: 836, Batch Gradient Norm after: 11.561084482055405
Epoch 837/10000, Prediction Accuracy = 59.012%, Loss = 0.6623059391975403
Epoch: 837, Batch Gradient Norm: 13.511998469187551
Epoch: 837, Batch Gradient Norm after: 13.511998469187551
Epoch 838/10000, Prediction Accuracy = 58.988%, Loss = 0.6768417477607727
Epoch: 838, Batch Gradient Norm: 14.199902550921358
Epoch: 838, Batch Gradient Norm after: 14.199902550921358
Epoch 839/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.6833837628364563
Epoch: 839, Batch Gradient Norm: 10.44295006767197
Epoch: 839, Batch Gradient Norm after: 10.44295006767197
Epoch 840/10000, Prediction Accuracy = 59.041999999999994%, Loss = 0.6603548049926757
Epoch: 840, Batch Gradient Norm: 10.922532987082324
Epoch: 840, Batch Gradient Norm after: 10.922532987082324
Epoch 841/10000, Prediction Accuracy = 59.01800000000001%, Loss = 0.6660994529724121
Epoch: 841, Batch Gradient Norm: 10.749123670279701
Epoch: 841, Batch Gradient Norm after: 10.749123670279701
Epoch 842/10000, Prediction Accuracy = 59.076%, Loss = 0.6575793147087097
Epoch: 842, Batch Gradient Norm: 10.836857546616468
Epoch: 842, Batch Gradient Norm after: 10.836857546616468
Epoch 843/10000, Prediction Accuracy = 59.001999999999995%, Loss = 0.6585492491722107
Epoch: 843, Batch Gradient Norm: 12.87980445756344
Epoch: 843, Batch Gradient Norm after: 12.87980445756344
Epoch 844/10000, Prediction Accuracy = 58.965999999999994%, Loss = 0.6717355728149415
Epoch: 844, Batch Gradient Norm: 15.382370891611528
Epoch: 844, Batch Gradient Norm after: 15.382370891611528
Epoch 845/10000, Prediction Accuracy = 59.086%, Loss = 0.6957805633544922
Epoch: 845, Batch Gradient Norm: 13.07756999327132
Epoch: 845, Batch Gradient Norm after: 13.07756999327132
Epoch 846/10000, Prediction Accuracy = 59.001999999999995%, Loss = 0.6703610897064209
Epoch: 846, Batch Gradient Norm: 9.70068134467787
Epoch: 846, Batch Gradient Norm after: 9.70068134467787
Epoch 847/10000, Prediction Accuracy = 59.072%, Loss = 0.6503853559494018
Epoch: 847, Batch Gradient Norm: 11.480552204480738
Epoch: 847, Batch Gradient Norm after: 11.480552204480738
Epoch 848/10000, Prediction Accuracy = 59.086%, Loss = 0.6627084970474243
Epoch: 848, Batch Gradient Norm: 13.613483945023361
Epoch: 848, Batch Gradient Norm after: 13.613483945023361
Epoch 849/10000, Prediction Accuracy = 59.084%, Loss = 0.6785748481750489
Epoch: 849, Batch Gradient Norm: 9.894216562799835
Epoch: 849, Batch Gradient Norm after: 9.894216562799835
Epoch 850/10000, Prediction Accuracy = 59.028%, Loss = 0.6532453179359436
Epoch: 850, Batch Gradient Norm: 13.553498044866267
Epoch: 850, Batch Gradient Norm after: 13.553498044866267
Epoch 851/10000, Prediction Accuracy = 59.036%, Loss = 0.6694034099578857
Epoch: 851, Batch Gradient Norm: 15.54983672394857
Epoch: 851, Batch Gradient Norm after: 15.54983672394857
Epoch 852/10000, Prediction Accuracy = 59.007999999999996%, Loss = 0.6926051616668701
Epoch: 852, Batch Gradient Norm: 10.232473613749736
Epoch: 852, Batch Gradient Norm after: 10.232473613749736
Epoch 853/10000, Prediction Accuracy = 59.048%, Loss = 0.6568169832229614
Epoch: 853, Batch Gradient Norm: 9.244381959677675
Epoch: 853, Batch Gradient Norm after: 9.244381959677675
Epoch 854/10000, Prediction Accuracy = 59.08200000000001%, Loss = 0.6555521368980408
Epoch: 854, Batch Gradient Norm: 12.909922150424093
Epoch: 854, Batch Gradient Norm after: 12.909922150424093
Epoch 855/10000, Prediction Accuracy = 59.025999999999996%, Loss = 0.673006284236908
Epoch: 855, Batch Gradient Norm: 14.054974931286006
Epoch: 855, Batch Gradient Norm after: 14.054974931286006
Epoch 856/10000, Prediction Accuracy = 59.19199999999999%, Loss = 0.678391444683075
Epoch: 856, Batch Gradient Norm: 14.20980287127613
Epoch: 856, Batch Gradient Norm after: 14.20980287127613
Epoch 857/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.6747571229934692
Epoch: 857, Batch Gradient Norm: 15.211846979951217
Epoch: 857, Batch Gradient Norm after: 15.211846979951217
Epoch 858/10000, Prediction Accuracy = 59.08%, Loss = 0.6841136813163757
Epoch: 858, Batch Gradient Norm: 11.46215379115805
Epoch: 858, Batch Gradient Norm after: 11.46215379115805
Epoch 859/10000, Prediction Accuracy = 59.024%, Loss = 0.659519898891449
Epoch: 859, Batch Gradient Norm: 11.104574275566327
Epoch: 859, Batch Gradient Norm after: 11.104574275566327
Epoch 860/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.657063901424408
Epoch: 860, Batch Gradient Norm: 11.361775970938202
Epoch: 860, Batch Gradient Norm after: 11.361775970938202
Epoch 861/10000, Prediction Accuracy = 59.044%, Loss = 0.6592053651809693
Epoch: 861, Batch Gradient Norm: 11.548673764848713
Epoch: 861, Batch Gradient Norm after: 11.548673764848713
Epoch 862/10000, Prediction Accuracy = 59.048%, Loss = 0.6599508166313172
Epoch: 862, Batch Gradient Norm: 9.487816062168873
Epoch: 862, Batch Gradient Norm after: 9.487816062168873
Epoch 863/10000, Prediction Accuracy = 59.148%, Loss = 0.6483665823936462
Epoch: 863, Batch Gradient Norm: 8.64622597868665
Epoch: 863, Batch Gradient Norm after: 8.64622597868665
Epoch 864/10000, Prediction Accuracy = 59.102%, Loss = 0.6428360223770142
Epoch: 864, Batch Gradient Norm: 10.722321973432361
Epoch: 864, Batch Gradient Norm after: 10.722321973432361
Epoch 865/10000, Prediction Accuracy = 59.144000000000005%, Loss = 0.65462566614151
Epoch: 865, Batch Gradient Norm: 16.91200490825414
Epoch: 865, Batch Gradient Norm after: 16.91200490825414
Epoch 866/10000, Prediction Accuracy = 59.114%, Loss = 0.7040071129798889
Epoch: 866, Batch Gradient Norm: 14.147061773360221
Epoch: 866, Batch Gradient Norm after: 14.147061773360221
Epoch 867/10000, Prediction Accuracy = 59.08200000000001%, Loss = 0.680162537097931
Epoch: 867, Batch Gradient Norm: 9.662381854363677
Epoch: 867, Batch Gradient Norm after: 9.662381854363677
Epoch 868/10000, Prediction Accuracy = 59.102%, Loss = 0.6462238550186157
Epoch: 868, Batch Gradient Norm: 9.878191686650228
Epoch: 868, Batch Gradient Norm after: 9.878191686650228
Epoch 869/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.6482168436050415
Epoch: 869, Batch Gradient Norm: 14.976360737497746
Epoch: 869, Batch Gradient Norm after: 14.976360737497746
Epoch 870/10000, Prediction Accuracy = 59.114%, Loss = 0.6793419122695923
Epoch: 870, Batch Gradient Norm: 15.664474266279884
Epoch: 870, Batch Gradient Norm after: 15.664474266279884
Epoch 871/10000, Prediction Accuracy = 59.074%, Loss = 0.693182396888733
Epoch: 871, Batch Gradient Norm: 10.717588090422515
Epoch: 871, Batch Gradient Norm after: 10.717588090422515
Epoch 872/10000, Prediction Accuracy = 59.077999999999996%, Loss = 0.6536052346229553
Epoch: 872, Batch Gradient Norm: 9.332676315455869
Epoch: 872, Batch Gradient Norm after: 9.332676315455869
Epoch 873/10000, Prediction Accuracy = 59.16600000000001%, Loss = 0.6482843875885009
Epoch: 873, Batch Gradient Norm: 12.444380548532283
Epoch: 873, Batch Gradient Norm after: 12.444380548532283
Epoch 874/10000, Prediction Accuracy = 59.15000000000001%, Loss = 0.6674398303031921
Epoch: 874, Batch Gradient Norm: 11.043422948781757
Epoch: 874, Batch Gradient Norm after: 11.043422948781757
Epoch 875/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.6534060359001159
Epoch: 875, Batch Gradient Norm: 14.636543854378075
Epoch: 875, Batch Gradient Norm after: 14.636543854378075
Epoch 876/10000, Prediction Accuracy = 59.18000000000001%, Loss = 0.6798144459724427
Epoch: 876, Batch Gradient Norm: 13.68329516300153
Epoch: 876, Batch Gradient Norm after: 13.68329516300153
Epoch 877/10000, Prediction Accuracy = 59.136%, Loss = 0.6825082659721374
Epoch: 877, Batch Gradient Norm: 11.670712828054207
Epoch: 877, Batch Gradient Norm after: 11.670712828054207
Epoch 878/10000, Prediction Accuracy = 59.16799999999999%, Loss = 0.66641606092453
Epoch: 878, Batch Gradient Norm: 11.42527391803504
Epoch: 878, Batch Gradient Norm after: 11.42527391803504
Epoch 879/10000, Prediction Accuracy = 59.062%, Loss = 0.6670831918716431
Epoch: 879, Batch Gradient Norm: 6.750108791576655
Epoch: 879, Batch Gradient Norm after: 6.750108791576655
Epoch 880/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.6341219663619995
Epoch: 880, Batch Gradient Norm: 14.86155085890938
Epoch: 880, Batch Gradient Norm after: 14.86155085890938
Epoch 881/10000, Prediction Accuracy = 59.128%, Loss = 0.676211702823639
Epoch: 881, Batch Gradient Norm: 17.076963678858
Epoch: 881, Batch Gradient Norm after: 17.076963678858
Epoch 882/10000, Prediction Accuracy = 59.160000000000004%, Loss = 0.6975170612335205
Epoch: 882, Batch Gradient Norm: 15.630367070968889
Epoch: 882, Batch Gradient Norm after: 15.630367070968889
Epoch 883/10000, Prediction Accuracy = 59.148%, Loss = 0.6910526156425476
Epoch: 883, Batch Gradient Norm: 11.140890028744968
Epoch: 883, Batch Gradient Norm after: 11.140890028744968
Epoch 884/10000, Prediction Accuracy = 59.186%, Loss = 0.6548720955848694
Epoch: 884, Batch Gradient Norm: 9.560404008403976
Epoch: 884, Batch Gradient Norm after: 9.560404008403976
Epoch 885/10000, Prediction Accuracy = 59.206%, Loss = 0.6455932259559631
Epoch: 885, Batch Gradient Norm: 8.724702943177899
Epoch: 885, Batch Gradient Norm after: 8.724702943177899
Epoch 886/10000, Prediction Accuracy = 59.17399999999999%, Loss = 0.6413294911384583
Epoch: 886, Batch Gradient Norm: 12.446351265347767
Epoch: 886, Batch Gradient Norm after: 12.446351265347767
Epoch 887/10000, Prediction Accuracy = 59.116%, Loss = 0.6645009160041809
Epoch: 887, Batch Gradient Norm: 13.294580900238739
Epoch: 887, Batch Gradient Norm after: 13.294580900238739
Epoch 888/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.6691436052322388
Epoch: 888, Batch Gradient Norm: 10.849417547597474
Epoch: 888, Batch Gradient Norm after: 10.849417547597474
Epoch 889/10000, Prediction Accuracy = 59.072%, Loss = 0.6556795120239258
Epoch: 889, Batch Gradient Norm: 9.692686861332543
Epoch: 889, Batch Gradient Norm after: 9.692686861332543
Epoch 890/10000, Prediction Accuracy = 59.208000000000006%, Loss = 0.6418148398399353
Epoch: 890, Batch Gradient Norm: 10.712143528566925
Epoch: 890, Batch Gradient Norm after: 10.712143528566925
Epoch 891/10000, Prediction Accuracy = 59.11800000000001%, Loss = 0.6467511773109436
Epoch: 891, Batch Gradient Norm: 14.091113336771969
Epoch: 891, Batch Gradient Norm after: 14.091113336771969
Epoch 892/10000, Prediction Accuracy = 59.182%, Loss = 0.6693462014198304
Epoch: 892, Batch Gradient Norm: 13.109983202798771
Epoch: 892, Batch Gradient Norm after: 13.109983202798771
Epoch 893/10000, Prediction Accuracy = 59.19200000000001%, Loss = 0.6659767866134644
Epoch: 893, Batch Gradient Norm: 14.476512991640778
Epoch: 893, Batch Gradient Norm after: 14.476512991640778
Epoch 894/10000, Prediction Accuracy = 59.278%, Loss = 0.6735137939453125
Epoch: 894, Batch Gradient Norm: 13.045073280986017
Epoch: 894, Batch Gradient Norm after: 13.045073280986017
Epoch 895/10000, Prediction Accuracy = 59.24799999999999%, Loss = 0.6625691413879394
Epoch: 895, Batch Gradient Norm: 9.824947638107783
Epoch: 895, Batch Gradient Norm after: 9.824947638107783
Epoch 896/10000, Prediction Accuracy = 59.242%, Loss = 0.6461955785751343
Epoch: 896, Batch Gradient Norm: 11.230727944846945
Epoch: 896, Batch Gradient Norm after: 11.230727944846945
Epoch 897/10000, Prediction Accuracy = 59.080000000000005%, Loss = 0.6555609107017517
Epoch: 897, Batch Gradient Norm: 12.142418135643181
Epoch: 897, Batch Gradient Norm after: 12.142418135643181
Epoch 898/10000, Prediction Accuracy = 59.169999999999995%, Loss = 0.6569614171981811
Epoch: 898, Batch Gradient Norm: 9.52862083154367
Epoch: 898, Batch Gradient Norm after: 9.52862083154367
Epoch 899/10000, Prediction Accuracy = 59.076%, Loss = 0.6439868211746216
Epoch: 899, Batch Gradient Norm: 12.859577219303308
Epoch: 899, Batch Gradient Norm after: 12.859577219303308
Epoch 900/10000, Prediction Accuracy = 59.214%, Loss = 0.6618896961212158
Epoch: 900, Batch Gradient Norm: 14.103874101498134
Epoch: 900, Batch Gradient Norm after: 14.103874101498134
Epoch 901/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.6707592487335206
Epoch: 901, Batch Gradient Norm: 15.014950508713364
Epoch: 901, Batch Gradient Norm after: 15.014950508713364
Epoch 902/10000, Prediction Accuracy = 59.158%, Loss = 0.6878069162368774
Epoch: 902, Batch Gradient Norm: 11.177873206188579
Epoch: 902, Batch Gradient Norm after: 11.177873206188579
Epoch 903/10000, Prediction Accuracy = 59.25%, Loss = 0.6525593400001526
Epoch: 903, Batch Gradient Norm: 12.56633580455613
Epoch: 903, Batch Gradient Norm after: 12.56633580455613
Epoch 904/10000, Prediction Accuracy = 59.233999999999995%, Loss = 0.661074948310852
Epoch: 904, Batch Gradient Norm: 11.7372296997147
Epoch: 904, Batch Gradient Norm after: 11.7372296997147
Epoch 905/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.6554989576339721
Epoch: 905, Batch Gradient Norm: 14.542056043435599
Epoch: 905, Batch Gradient Norm after: 14.542056043435599
Epoch 906/10000, Prediction Accuracy = 59.2%, Loss = 0.6728482604026794
Epoch: 906, Batch Gradient Norm: 8.815194355221502
Epoch: 906, Batch Gradient Norm after: 8.815194355221502
Epoch 907/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.6380258083343506
Epoch: 907, Batch Gradient Norm: 7.259752389351442
Epoch: 907, Batch Gradient Norm after: 7.259752389351442
Epoch 908/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.6308070182800293
Epoch: 908, Batch Gradient Norm: 12.918913117039676
Epoch: 908, Batch Gradient Norm after: 12.918913117039676
Epoch 909/10000, Prediction Accuracy = 59.164%, Loss = 0.657562255859375
Epoch: 909, Batch Gradient Norm: 12.743841653436023
Epoch: 909, Batch Gradient Norm after: 12.743841653436023
Epoch 910/10000, Prediction Accuracy = 59.262%, Loss = 0.6635523438453674
Epoch: 910, Batch Gradient Norm: 12.331125533581872
Epoch: 910, Batch Gradient Norm after: 12.331125533581872
Epoch 911/10000, Prediction Accuracy = 59.33%, Loss = 0.6570724606513977
Epoch: 911, Batch Gradient Norm: 10.048061315018787
Epoch: 911, Batch Gradient Norm after: 10.048061315018787
Epoch 912/10000, Prediction Accuracy = 59.16799999999999%, Loss = 0.6444845080375672
Epoch: 912, Batch Gradient Norm: 10.699052768309755
Epoch: 912, Batch Gradient Norm after: 10.699052768309755
Epoch 913/10000, Prediction Accuracy = 59.262%, Loss = 0.6448460578918457
Epoch: 913, Batch Gradient Norm: 14.605121256796325
Epoch: 913, Batch Gradient Norm after: 14.605121256796325
Epoch 914/10000, Prediction Accuracy = 59.239999999999995%, Loss = 0.6725433468818665
Epoch: 914, Batch Gradient Norm: 10.915127019693447
Epoch: 914, Batch Gradient Norm after: 10.915127019693447
Epoch 915/10000, Prediction Accuracy = 59.238%, Loss = 0.6508816480636597
Epoch: 915, Batch Gradient Norm: 11.40244244160295
Epoch: 915, Batch Gradient Norm after: 11.40244244160295
Epoch 916/10000, Prediction Accuracy = 59.238%, Loss = 0.6508472442626954
Epoch: 916, Batch Gradient Norm: 13.641719222967241
Epoch: 916, Batch Gradient Norm after: 13.641719222967241
Epoch 917/10000, Prediction Accuracy = 59.26800000000001%, Loss = 0.6664238095283508
Epoch: 917, Batch Gradient Norm: 12.388705901327702
Epoch: 917, Batch Gradient Norm after: 12.388705901327702
Epoch 918/10000, Prediction Accuracy = 59.312%, Loss = 0.6552955389022828
Epoch: 918, Batch Gradient Norm: 14.01001284032666
Epoch: 918, Batch Gradient Norm after: 14.01001284032666
Epoch 919/10000, Prediction Accuracy = 59.258%, Loss = 0.6685965180397033
Epoch: 919, Batch Gradient Norm: 13.560997614686753
Epoch: 919, Batch Gradient Norm after: 13.560997614686753
Epoch 920/10000, Prediction Accuracy = 59.212%, Loss = 0.6637880444526673
Epoch: 920, Batch Gradient Norm: 12.421991555676659
Epoch: 920, Batch Gradient Norm after: 12.421991555676659
Epoch 921/10000, Prediction Accuracy = 59.274%, Loss = 0.6533929228782653
Epoch: 921, Batch Gradient Norm: 9.380187957844582
Epoch: 921, Batch Gradient Norm after: 9.380187957844582
Epoch 922/10000, Prediction Accuracy = 59.218%, Loss = 0.6409503102302552
Epoch: 922, Batch Gradient Norm: 9.431930568226006
Epoch: 922, Batch Gradient Norm after: 9.431930568226006
Epoch 923/10000, Prediction Accuracy = 59.269999999999996%, Loss = 0.6363553166389465
Epoch: 923, Batch Gradient Norm: 10.195402561742098
Epoch: 923, Batch Gradient Norm after: 10.195402561742098
Epoch 924/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.642196249961853
Epoch: 924, Batch Gradient Norm: 10.929849048197342
Epoch: 924, Batch Gradient Norm after: 10.929849048197342
Epoch 925/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.646543538570404
Epoch: 925, Batch Gradient Norm: 16.70806202307383
Epoch: 925, Batch Gradient Norm after: 16.70806202307383
Epoch 926/10000, Prediction Accuracy = 59.238%, Loss = 0.6905491471290588
Epoch: 926, Batch Gradient Norm: 15.886492978757834
Epoch: 926, Batch Gradient Norm after: 15.886492978757834
Epoch 927/10000, Prediction Accuracy = 59.25%, Loss = 0.6875574588775635
Epoch: 927, Batch Gradient Norm: 9.83457995949359
Epoch: 927, Batch Gradient Norm after: 9.83457995949359
Epoch 928/10000, Prediction Accuracy = 59.331999999999994%, Loss = 0.6414531826972961
Epoch: 928, Batch Gradient Norm: 8.466820589346739
Epoch: 928, Batch Gradient Norm after: 8.466820589346739
Epoch 929/10000, Prediction Accuracy = 59.331999999999994%, Loss = 0.6314229846000672
Epoch: 929, Batch Gradient Norm: 10.450093135792041
Epoch: 929, Batch Gradient Norm after: 10.450093135792041
Epoch 930/10000, Prediction Accuracy = 59.226%, Loss = 0.639665710926056
Epoch: 930, Batch Gradient Norm: 14.977981828275182
Epoch: 930, Batch Gradient Norm after: 14.977981828275182
Epoch 931/10000, Prediction Accuracy = 59.251999999999995%, Loss = 0.6707288026809692
Epoch: 931, Batch Gradient Norm: 12.723895829624237
Epoch: 931, Batch Gradient Norm after: 12.723895829624237
Epoch 932/10000, Prediction Accuracy = 59.24000000000001%, Loss = 0.6607961535453797
Epoch: 932, Batch Gradient Norm: 13.553699184206847
Epoch: 932, Batch Gradient Norm after: 13.553699184206847
Epoch 933/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.6616025328636169
Epoch: 933, Batch Gradient Norm: 12.112553943300215
Epoch: 933, Batch Gradient Norm after: 12.112553943300215
Epoch 934/10000, Prediction Accuracy = 59.27%, Loss = 0.6534069895744323
Epoch: 934, Batch Gradient Norm: 10.079578373134485
Epoch: 934, Batch Gradient Norm after: 10.079578373134485
Epoch 935/10000, Prediction Accuracy = 59.36%, Loss = 0.6374658823013306
Epoch: 935, Batch Gradient Norm: 10.306993775127529
Epoch: 935, Batch Gradient Norm after: 10.306993775127529
Epoch 936/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.6397159576416016
Epoch: 936, Batch Gradient Norm: 10.370007432588523
Epoch: 936, Batch Gradient Norm after: 10.370007432588523
Epoch 937/10000, Prediction Accuracy = 59.20399999999999%, Loss = 0.6392838001251221
Epoch: 937, Batch Gradient Norm: 12.652635016268468
Epoch: 937, Batch Gradient Norm after: 12.652635016268468
Epoch 938/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.6540624618530273
Epoch: 938, Batch Gradient Norm: 10.72340004593786
Epoch: 938, Batch Gradient Norm after: 10.72340004593786
Epoch 939/10000, Prediction Accuracy = 59.324%, Loss = 0.6428668022155761
Epoch: 939, Batch Gradient Norm: 13.825300095505861
Epoch: 939, Batch Gradient Norm after: 13.825300095505861
Epoch 940/10000, Prediction Accuracy = 59.367999999999995%, Loss = 0.6626061797142029
Epoch: 940, Batch Gradient Norm: 12.73000971363341
Epoch: 940, Batch Gradient Norm after: 12.73000971363341
Epoch 941/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.6538303613662719
Epoch: 941, Batch Gradient Norm: 12.937646390661186
Epoch: 941, Batch Gradient Norm after: 12.937646390661186
Epoch 942/10000, Prediction Accuracy = 59.30799999999999%, Loss = 0.6651909470558166
Epoch: 942, Batch Gradient Norm: 12.665691657877348
Epoch: 942, Batch Gradient Norm after: 12.665691657877348
Epoch 943/10000, Prediction Accuracy = 59.218%, Loss = 0.6591722130775451
Epoch: 943, Batch Gradient Norm: 15.548116011341252
Epoch: 943, Batch Gradient Norm after: 15.548116011341252
Epoch 944/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.6774991869926452
Epoch: 944, Batch Gradient Norm: 12.941651983157266
Epoch: 944, Batch Gradient Norm after: 12.941651983157266
Epoch 945/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.6579280972480774
Epoch: 945, Batch Gradient Norm: 9.7976522448133
Epoch: 945, Batch Gradient Norm after: 9.7976522448133
Epoch 946/10000, Prediction Accuracy = 59.362%, Loss = 0.6375772476196289
Epoch: 946, Batch Gradient Norm: 12.45200468922143
Epoch: 946, Batch Gradient Norm after: 12.45200468922143
Epoch 947/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.6650971174240112
Epoch: 947, Batch Gradient Norm: 10.480662291329637
Epoch: 947, Batch Gradient Norm after: 10.480662291329637
Epoch 948/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.6463508367538452
Epoch: 948, Batch Gradient Norm: 11.369610282403867
Epoch: 948, Batch Gradient Norm after: 11.369610282403867
Epoch 949/10000, Prediction Accuracy = 59.291999999999994%, Loss = 0.6494140744209289
Epoch: 949, Batch Gradient Norm: 11.120571535764043
Epoch: 949, Batch Gradient Norm after: 11.120571535764043
Epoch 950/10000, Prediction Accuracy = 59.394000000000005%, Loss = 0.6401050567626954
Epoch: 950, Batch Gradient Norm: 13.835342033572658
Epoch: 950, Batch Gradient Norm after: 13.835342033572658
Epoch 951/10000, Prediction Accuracy = 59.26800000000001%, Loss = 0.6643082618713378
Epoch: 951, Batch Gradient Norm: 12.841397454816297
Epoch: 951, Batch Gradient Norm after: 12.841397454816297
Epoch 952/10000, Prediction Accuracy = 59.34000000000001%, Loss = 0.6527148365974427
Epoch: 952, Batch Gradient Norm: 13.048128772056433
Epoch: 952, Batch Gradient Norm after: 13.048128772056433
Epoch 953/10000, Prediction Accuracy = 59.24000000000001%, Loss = 0.6594809770584107
Epoch: 953, Batch Gradient Norm: 12.207805975044652
Epoch: 953, Batch Gradient Norm after: 12.207805975044652
Epoch 954/10000, Prediction Accuracy = 59.342%, Loss = 0.6571903347969055
Epoch: 954, Batch Gradient Norm: 13.556929995591716
Epoch: 954, Batch Gradient Norm after: 13.556929995591716
Epoch 955/10000, Prediction Accuracy = 59.260000000000005%, Loss = 0.6625664949417114
Epoch: 955, Batch Gradient Norm: 14.102957175767092
Epoch: 955, Batch Gradient Norm after: 14.102957175767092
Epoch 956/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.6775957822799683
Epoch: 956, Batch Gradient Norm: 8.632653966244284
Epoch: 956, Batch Gradient Norm after: 8.632653966244284
Epoch 957/10000, Prediction Accuracy = 59.462%, Loss = 0.6295814633369445
Epoch: 957, Batch Gradient Norm: 8.199156916677298
Epoch: 957, Batch Gradient Norm after: 8.199156916677298
Epoch 958/10000, Prediction Accuracy = 59.39000000000001%, Loss = 0.6260697364807128
Epoch: 958, Batch Gradient Norm: 10.689212601327682
Epoch: 958, Batch Gradient Norm after: 10.689212601327682
Epoch 959/10000, Prediction Accuracy = 59.343999999999994%, Loss = 0.6427598237991333
Epoch: 959, Batch Gradient Norm: 15.019164812594749
Epoch: 959, Batch Gradient Norm after: 15.019164812594749
Epoch 960/10000, Prediction Accuracy = 59.34400000000001%, Loss = 0.6730898737907409
Epoch: 960, Batch Gradient Norm: 10.612736768287611
Epoch: 960, Batch Gradient Norm after: 10.612736768287611
Epoch 961/10000, Prediction Accuracy = 59.36%, Loss = 0.6389678120613098
Epoch: 961, Batch Gradient Norm: 11.071836426395627
Epoch: 961, Batch Gradient Norm after: 11.071836426395627
Epoch 962/10000, Prediction Accuracy = 59.426%, Loss = 0.6411605715751648
Epoch: 962, Batch Gradient Norm: 12.217646662217883
Epoch: 962, Batch Gradient Norm after: 12.217646662217883
Epoch 963/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.650952959060669
Epoch: 963, Batch Gradient Norm: 11.436182606008972
Epoch: 963, Batch Gradient Norm after: 11.436182606008972
Epoch 964/10000, Prediction Accuracy = 59.31%, Loss = 0.6451974272727966
Epoch: 964, Batch Gradient Norm: 10.375020539020886
Epoch: 964, Batch Gradient Norm after: 10.375020539020886
Epoch 965/10000, Prediction Accuracy = 59.269999999999996%, Loss = 0.638158917427063
Epoch: 965, Batch Gradient Norm: 12.143035319577743
Epoch: 965, Batch Gradient Norm after: 12.143035319577743
Epoch 966/10000, Prediction Accuracy = 59.327999999999996%, Loss = 0.6440893292427063
Epoch: 966, Batch Gradient Norm: 14.515820269315434
Epoch: 966, Batch Gradient Norm after: 14.515820269315434
Epoch 967/10000, Prediction Accuracy = 59.44799999999999%, Loss = 0.6687459111213684
Epoch: 967, Batch Gradient Norm: 15.213561723172598
Epoch: 967, Batch Gradient Norm after: 15.213561723172598
Epoch 968/10000, Prediction Accuracy = 59.4%, Loss = 0.6824253320693969
Epoch: 968, Batch Gradient Norm: 11.672687933814473
Epoch: 968, Batch Gradient Norm after: 11.672687933814473
Epoch 969/10000, Prediction Accuracy = 59.44%, Loss = 0.6462761521339416
Epoch: 969, Batch Gradient Norm: 13.921156857269384
Epoch: 969, Batch Gradient Norm after: 13.921156857269384
Epoch 970/10000, Prediction Accuracy = 59.306000000000004%, Loss = 0.667175042629242
Epoch: 970, Batch Gradient Norm: 11.599918193544324
Epoch: 970, Batch Gradient Norm after: 11.599918193544324
Epoch 971/10000, Prediction Accuracy = 59.448%, Loss = 0.6430402636528015
Epoch: 971, Batch Gradient Norm: 11.31172773718325
Epoch: 971, Batch Gradient Norm after: 11.31172773718325
Epoch 972/10000, Prediction Accuracy = 59.402%, Loss = 0.6414022803306579
Epoch: 972, Batch Gradient Norm: 8.838503755914273
Epoch: 972, Batch Gradient Norm after: 8.838503755914273
Epoch 973/10000, Prediction Accuracy = 59.403999999999996%, Loss = 0.6250297665596009
Epoch: 973, Batch Gradient Norm: 10.478635247826896
Epoch: 973, Batch Gradient Norm after: 10.478635247826896
Epoch 974/10000, Prediction Accuracy = 59.260000000000005%, Loss = 0.637229609489441
Epoch: 974, Batch Gradient Norm: 13.267559188558213
Epoch: 974, Batch Gradient Norm after: 13.267559188558213
Epoch 975/10000, Prediction Accuracy = 59.362%, Loss = 0.6515515327453614
Epoch: 975, Batch Gradient Norm: 13.448736564069156
Epoch: 975, Batch Gradient Norm after: 13.448736564069156
Epoch 976/10000, Prediction Accuracy = 59.35%, Loss = 0.6553749203681946
Epoch: 976, Batch Gradient Norm: 12.875556494737857
Epoch: 976, Batch Gradient Norm after: 12.875556494737857
Epoch 977/10000, Prediction Accuracy = 59.358000000000004%, Loss = 0.6481886744499207
Epoch: 977, Batch Gradient Norm: 14.062483016445634
Epoch: 977, Batch Gradient Norm after: 14.062483016445634
Epoch 978/10000, Prediction Accuracy = 59.336%, Loss = 0.6603666782379151
Epoch: 978, Batch Gradient Norm: 11.80638738390901
Epoch: 978, Batch Gradient Norm after: 11.80638738390901
Epoch 979/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.6451493501663208
Epoch: 979, Batch Gradient Norm: 7.956951209602369
Epoch: 979, Batch Gradient Norm after: 7.956951209602369
Epoch 980/10000, Prediction Accuracy = 59.438%, Loss = 0.6203569173812866
Epoch: 980, Batch Gradient Norm: 8.867528324910214
Epoch: 980, Batch Gradient Norm after: 8.867528324910214
Epoch 981/10000, Prediction Accuracy = 59.29200000000001%, Loss = 0.6284752011299133
Epoch: 981, Batch Gradient Norm: 14.92580024050433
Epoch: 981, Batch Gradient Norm after: 14.92580024050433
Epoch 982/10000, Prediction Accuracy = 59.36%, Loss = 0.6650675892829895
Epoch: 982, Batch Gradient Norm: 12.933010092486818
Epoch: 982, Batch Gradient Norm after: 12.933010092486818
Epoch 983/10000, Prediction Accuracy = 59.27%, Loss = 0.6627046704292298
Epoch: 983, Batch Gradient Norm: 7.8400465364667316
Epoch: 983, Batch Gradient Norm after: 7.8400465364667316
Epoch 984/10000, Prediction Accuracy = 59.426%, Loss = 0.6230181932449341
Epoch: 984, Batch Gradient Norm: 11.19698455041786
Epoch: 984, Batch Gradient Norm after: 11.19698455041786
Epoch 985/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.6363146424293518
Epoch: 985, Batch Gradient Norm: 14.87637277274525
Epoch: 985, Batch Gradient Norm after: 14.87637277274525
Epoch 986/10000, Prediction Accuracy = 59.504%, Loss = 0.6643480777740478
Epoch: 986, Batch Gradient Norm: 11.529257766319557
Epoch: 986, Batch Gradient Norm after: 11.529257766319557
Epoch 987/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.6408993244171143
Epoch: 987, Batch Gradient Norm: 9.41987440571549
Epoch: 987, Batch Gradient Norm after: 9.41987440571549
Epoch 988/10000, Prediction Accuracy = 59.464%, Loss = 0.626708734035492
Epoch: 988, Batch Gradient Norm: 11.623868382540453
Epoch: 988, Batch Gradient Norm after: 11.623868382540453
Epoch 989/10000, Prediction Accuracy = 59.44199999999999%, Loss = 0.6407414078712463
Epoch: 989, Batch Gradient Norm: 13.193155150753984
Epoch: 989, Batch Gradient Norm after: 13.193155150753984
Epoch 990/10000, Prediction Accuracy = 59.376%, Loss = 0.6537915945053101
Epoch: 990, Batch Gradient Norm: 12.31501869957231
Epoch: 990, Batch Gradient Norm after: 12.31501869957231
Epoch 991/10000, Prediction Accuracy = 59.414%, Loss = 0.6478594064712524
Epoch: 991, Batch Gradient Norm: 13.091365762189696
Epoch: 991, Batch Gradient Norm after: 13.091365762189696
Epoch 992/10000, Prediction Accuracy = 59.25%, Loss = 0.6547622799873352
Epoch: 992, Batch Gradient Norm: 13.316933179240536
Epoch: 992, Batch Gradient Norm after: 13.316933179240536
Epoch 993/10000, Prediction Accuracy = 59.495999999999995%, Loss = 0.6535017728805542
Epoch: 993, Batch Gradient Norm: 13.049305782092688
Epoch: 993, Batch Gradient Norm after: 13.049305782092688
Epoch 994/10000, Prediction Accuracy = 59.48599999999999%, Loss = 0.649712610244751
Epoch: 994, Batch Gradient Norm: 11.219076883374683
Epoch: 994, Batch Gradient Norm after: 11.219076883374683
Epoch 995/10000, Prediction Accuracy = 59.476%, Loss = 0.6410866975784302
Epoch: 995, Batch Gradient Norm: 14.38527531963408
Epoch: 995, Batch Gradient Norm after: 14.38527531963408
Epoch 996/10000, Prediction Accuracy = 59.452%, Loss = 0.6633150696754455
Epoch: 996, Batch Gradient Norm: 13.225647297474893
Epoch: 996, Batch Gradient Norm after: 13.225647297474893
Epoch 997/10000, Prediction Accuracy = 59.443999999999996%, Loss = 0.6538117408752442
Epoch: 997, Batch Gradient Norm: 14.874425335249768
Epoch: 997, Batch Gradient Norm after: 14.874425335249768
Epoch 998/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.6613785028457642
Epoch: 998, Batch Gradient Norm: 10.489614184677068
Epoch: 998, Batch Gradient Norm after: 10.489614184677068
Epoch 999/10000, Prediction Accuracy = 59.366%, Loss = 0.6332968235015869
Epoch: 999, Batch Gradient Norm: 8.998416300169641
Epoch: 999, Batch Gradient Norm after: 8.998416300169641
Epoch 1000/10000, Prediction Accuracy = 59.44%, Loss = 0.6277279257774353
Epoch: 1000, Batch Gradient Norm: 11.595365301394091
Epoch: 1000, Batch Gradient Norm after: 11.595365301394091
Epoch 1001/10000, Prediction Accuracy = 59.476%, Loss = 0.6370548367500305
Epoch: 1001, Batch Gradient Norm: 13.662631835530588
Epoch: 1001, Batch Gradient Norm after: 13.662631835530588
Epoch 1002/10000, Prediction Accuracy = 59.312%, Loss = 0.6545109629631043
Epoch: 1002, Batch Gradient Norm: 11.706737716692961
Epoch: 1002, Batch Gradient Norm after: 11.706737716692961
Epoch 1003/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.6390889406204223
Epoch: 1003, Batch Gradient Norm: 13.139002061987984
Epoch: 1003, Batch Gradient Norm after: 13.139002061987984
Epoch 1004/10000, Prediction Accuracy = 59.418000000000006%, Loss = 0.6508677840232849
Epoch: 1004, Batch Gradient Norm: 13.32254860837898
Epoch: 1004, Batch Gradient Norm after: 13.32254860837898
Epoch 1005/10000, Prediction Accuracy = 59.564%, Loss = 0.6498091340065002
Epoch: 1005, Batch Gradient Norm: 12.407379228830388
Epoch: 1005, Batch Gradient Norm after: 12.407379228830388
Epoch 1006/10000, Prediction Accuracy = 59.452%, Loss = 0.644005537033081
Epoch: 1006, Batch Gradient Norm: 13.874536053196394
Epoch: 1006, Batch Gradient Norm after: 13.874536053196394
Epoch 1007/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.6551084041595459
Epoch: 1007, Batch Gradient Norm: 10.81956285068414
Epoch: 1007, Batch Gradient Norm after: 10.81956285068414
Epoch 1008/10000, Prediction Accuracy = 59.428%, Loss = 0.6334524035453797
Epoch: 1008, Batch Gradient Norm: 12.464798021260734
Epoch: 1008, Batch Gradient Norm after: 12.464798021260734
Epoch 1009/10000, Prediction Accuracy = 59.548%, Loss = 0.644105052947998
Epoch: 1009, Batch Gradient Norm: 10.024621606790998
Epoch: 1009, Batch Gradient Norm after: 10.024621606790998
Epoch 1010/10000, Prediction Accuracy = 59.39000000000001%, Loss = 0.635964035987854
Epoch: 1010, Batch Gradient Norm: 11.733426922881183
Epoch: 1010, Batch Gradient Norm after: 11.733426922881183
Epoch 1011/10000, Prediction Accuracy = 59.48%, Loss = 0.6373506665229798
Epoch: 1011, Batch Gradient Norm: 11.97028940402778
Epoch: 1011, Batch Gradient Norm after: 11.97028940402778
Epoch 1012/10000, Prediction Accuracy = 59.432%, Loss = 0.6411526322364807
Epoch: 1012, Batch Gradient Norm: 13.187268708751418
Epoch: 1012, Batch Gradient Norm after: 13.187268708751418
Epoch 1013/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.6459279417991638
Epoch: 1013, Batch Gradient Norm: 13.268234517957046
Epoch: 1013, Batch Gradient Norm after: 13.268234517957046
Epoch 1014/10000, Prediction Accuracy = 59.42800000000001%, Loss = 0.646642005443573
Epoch: 1014, Batch Gradient Norm: 11.591269659429713
Epoch: 1014, Batch Gradient Norm after: 11.591269659429713
Epoch 1015/10000, Prediction Accuracy = 59.422000000000004%, Loss = 0.6494907021522522
Epoch: 1015, Batch Gradient Norm: 9.941464400166055
Epoch: 1015, Batch Gradient Norm after: 9.941464400166055
Epoch 1016/10000, Prediction Accuracy = 59.424%, Loss = 0.6275603294372558
Epoch: 1016, Batch Gradient Norm: 11.512664809794492
Epoch: 1016, Batch Gradient Norm after: 11.512664809794492
Epoch 1017/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.6353700995445252
Epoch: 1017, Batch Gradient Norm: 8.827780362438364
Epoch: 1017, Batch Gradient Norm after: 8.827780362438364
Epoch 1018/10000, Prediction Accuracy = 59.464%, Loss = 0.6212417840957641
Epoch: 1018, Batch Gradient Norm: 10.934889253197992
Epoch: 1018, Batch Gradient Norm after: 10.934889253197992
Epoch 1019/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.6328665971755981
Epoch: 1019, Batch Gradient Norm: 12.985759288029916
Epoch: 1019, Batch Gradient Norm after: 12.985759288029916
Epoch 1020/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.6502498149871826
Epoch: 1020, Batch Gradient Norm: 11.604366468532458
Epoch: 1020, Batch Gradient Norm after: 11.604366468532458
Epoch 1021/10000, Prediction Accuracy = 59.483999999999995%, Loss = 0.6382051229476928
Epoch: 1021, Batch Gradient Norm: 8.383204593142688
Epoch: 1021, Batch Gradient Norm after: 8.383204593142688
Epoch 1022/10000, Prediction Accuracy = 59.484%, Loss = 0.6168049097061157
Epoch: 1022, Batch Gradient Norm: 15.745250973297455
Epoch: 1022, Batch Gradient Norm after: 15.745250973297455
Epoch 1023/10000, Prediction Accuracy = 59.382000000000005%, Loss = 0.6667659163475037
Epoch: 1023, Batch Gradient Norm: 10.986936883881585
Epoch: 1023, Batch Gradient Norm after: 10.986936883881585
Epoch 1024/10000, Prediction Accuracy = 59.517999999999994%, Loss = 0.6328991770744323
Epoch: 1024, Batch Gradient Norm: 16.24340122993681
Epoch: 1024, Batch Gradient Norm after: 16.24340122993681
Epoch 1025/10000, Prediction Accuracy = 59.483999999999995%, Loss = 0.6691240668296814
Epoch: 1025, Batch Gradient Norm: 13.80957940604406
Epoch: 1025, Batch Gradient Norm after: 13.80957940604406
Epoch 1026/10000, Prediction Accuracy = 59.496%, Loss = 0.652019488811493
Epoch: 1026, Batch Gradient Norm: 9.867634227755108
Epoch: 1026, Batch Gradient Norm after: 9.867634227755108
Epoch 1027/10000, Prediction Accuracy = 59.472%, Loss = 0.6316787600517273
Epoch: 1027, Batch Gradient Norm: 10.805001024130506
Epoch: 1027, Batch Gradient Norm after: 10.805001024130506
Epoch 1028/10000, Prediction Accuracy = 59.472%, Loss = 0.6321104764938354
Epoch: 1028, Batch Gradient Norm: 8.9353733762898
Epoch: 1028, Batch Gradient Norm after: 8.9353733762898
Epoch 1029/10000, Prediction Accuracy = 59.576%, Loss = 0.6245693445205689
Epoch: 1029, Batch Gradient Norm: 9.363117880754434
Epoch: 1029, Batch Gradient Norm after: 9.363117880754434
Epoch 1030/10000, Prediction Accuracy = 59.538%, Loss = 0.6297441840171814
Epoch: 1030, Batch Gradient Norm: 11.830124894556572
Epoch: 1030, Batch Gradient Norm after: 11.830124894556572
Epoch 1031/10000, Prediction Accuracy = 59.517999999999994%, Loss = 0.6422698378562928
Epoch: 1031, Batch Gradient Norm: 15.012497969505036
Epoch: 1031, Batch Gradient Norm after: 15.012497969505036
Epoch 1032/10000, Prediction Accuracy = 59.513999999999996%, Loss = 0.6644705772399903
Epoch: 1032, Batch Gradient Norm: 12.122800893807545
Epoch: 1032, Batch Gradient Norm after: 12.122800893807545
Epoch 1033/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.6423785448074341
Epoch: 1033, Batch Gradient Norm: 12.817649104970732
Epoch: 1033, Batch Gradient Norm after: 12.817649104970732
Epoch 1034/10000, Prediction Accuracy = 59.55400000000001%, Loss = 0.6421923637390137
Epoch: 1034, Batch Gradient Norm: 14.830677195816673
Epoch: 1034, Batch Gradient Norm after: 14.830677195816673
Epoch 1035/10000, Prediction Accuracy = 59.42%, Loss = 0.6556227564811706
Epoch: 1035, Batch Gradient Norm: 13.173633718311416
Epoch: 1035, Batch Gradient Norm after: 13.173633718311416
Epoch 1036/10000, Prediction Accuracy = 59.544%, Loss = 0.6436570167541504
Epoch: 1036, Batch Gradient Norm: 9.799146413478985
Epoch: 1036, Batch Gradient Norm after: 9.799146413478985
Epoch 1037/10000, Prediction Accuracy = 59.484%, Loss = 0.6237855434417725
Epoch: 1037, Batch Gradient Norm: 9.705040140865648
Epoch: 1037, Batch Gradient Norm after: 9.705040140865648
Epoch 1038/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.6253175616264344
Epoch: 1038, Batch Gradient Norm: 9.569376176143486
Epoch: 1038, Batch Gradient Norm after: 9.569376176143486
Epoch 1039/10000, Prediction Accuracy = 59.476%, Loss = 0.6261588096618652
Epoch: 1039, Batch Gradient Norm: 7.629243323802905
Epoch: 1039, Batch Gradient Norm after: 7.629243323802905
Epoch 1040/10000, Prediction Accuracy = 59.480000000000004%, Loss = 0.6150075674057007
Epoch: 1040, Batch Gradient Norm: 9.002802523971422
Epoch: 1040, Batch Gradient Norm after: 9.002802523971422
Epoch 1041/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.6209330677986145
Epoch: 1041, Batch Gradient Norm: 18.587463830390586
Epoch: 1041, Batch Gradient Norm after: 18.587463830390586
Epoch 1042/10000, Prediction Accuracy = 59.544%, Loss = 0.6932218432426452
Epoch: 1042, Batch Gradient Norm: 15.991087684593404
Epoch: 1042, Batch Gradient Norm after: 15.991087684593404
Epoch 1043/10000, Prediction Accuracy = 59.572%, Loss = 0.6746738910675049
Epoch: 1043, Batch Gradient Norm: 9.524258045582908
Epoch: 1043, Batch Gradient Norm after: 9.524258045582908
Epoch 1044/10000, Prediction Accuracy = 59.581999999999994%, Loss = 0.625447690486908
Epoch: 1044, Batch Gradient Norm: 12.196040033600637
Epoch: 1044, Batch Gradient Norm after: 12.196040033600637
Epoch 1045/10000, Prediction Accuracy = 59.492%, Loss = 0.6351130843162537
Epoch: 1045, Batch Gradient Norm: 13.576258211182717
Epoch: 1045, Batch Gradient Norm after: 13.576258211182717
Epoch 1046/10000, Prediction Accuracy = 59.54600000000001%, Loss = 0.6473430633544922
Epoch: 1046, Batch Gradient Norm: 10.567327960994973
Epoch: 1046, Batch Gradient Norm after: 10.567327960994973
Epoch 1047/10000, Prediction Accuracy = 59.48%, Loss = 0.6311000704765319
Epoch: 1047, Batch Gradient Norm: 9.873705517680254
Epoch: 1047, Batch Gradient Norm after: 9.873705517680254
Epoch 1048/10000, Prediction Accuracy = 59.568000000000005%, Loss = 0.629810094833374
Epoch: 1048, Batch Gradient Norm: 8.487407485621752
Epoch: 1048, Batch Gradient Norm after: 8.487407485621752
Epoch 1049/10000, Prediction Accuracy = 59.552%, Loss = 0.6143868803977967
Epoch: 1049, Batch Gradient Norm: 11.125574819813094
Epoch: 1049, Batch Gradient Norm after: 11.125574819813094
Epoch 1050/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.6273859739303589
Epoch: 1050, Batch Gradient Norm: 18.52910629930988
Epoch: 1050, Batch Gradient Norm after: 18.52910629930988
Epoch 1051/10000, Prediction Accuracy = 59.519999999999996%, Loss = 0.6964133739471435
Epoch: 1051, Batch Gradient Norm: 12.033258477282793
Epoch: 1051, Batch Gradient Norm after: 12.033258477282793
Epoch 1052/10000, Prediction Accuracy = 59.638%, Loss = 0.6400030255317688
Epoch: 1052, Batch Gradient Norm: 9.439439700429618
Epoch: 1052, Batch Gradient Norm after: 9.439439700429618
Epoch 1053/10000, Prediction Accuracy = 59.522000000000006%, Loss = 0.6211038589477539
Epoch: 1053, Batch Gradient Norm: 8.715512459152395
Epoch: 1053, Batch Gradient Norm after: 8.715512459152395
Epoch 1054/10000, Prediction Accuracy = 59.54200000000001%, Loss = 0.6199056029319763
Epoch: 1054, Batch Gradient Norm: 8.134343589387617
Epoch: 1054, Batch Gradient Norm after: 8.134343589387617
Epoch 1055/10000, Prediction Accuracy = 59.58%, Loss = 0.6123876929283142
Epoch: 1055, Batch Gradient Norm: 11.615437443610084
Epoch: 1055, Batch Gradient Norm after: 11.615437443610084
Epoch 1056/10000, Prediction Accuracy = 59.524%, Loss = 0.6336005091667175
Epoch: 1056, Batch Gradient Norm: 16.713316806236147
Epoch: 1056, Batch Gradient Norm after: 16.713316806236147
Epoch 1057/10000, Prediction Accuracy = 59.507999999999996%, Loss = 0.6786533236503601
Epoch: 1057, Batch Gradient Norm: 13.782062690126393
Epoch: 1057, Batch Gradient Norm after: 13.782062690126393
Epoch 1058/10000, Prediction Accuracy = 59.512%, Loss = 0.649715006351471
Epoch: 1058, Batch Gradient Norm: 10.206080069243287
Epoch: 1058, Batch Gradient Norm after: 10.206080069243287
Epoch 1059/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.6266196846961976
Epoch: 1059, Batch Gradient Norm: 8.498816180987061
Epoch: 1059, Batch Gradient Norm after: 8.498816180987061
Epoch 1060/10000, Prediction Accuracy = 59.59000000000001%, Loss = 0.6143585562705993
Epoch: 1060, Batch Gradient Norm: 12.811530039121813
Epoch: 1060, Batch Gradient Norm after: 12.811530039121813
Epoch 1061/10000, Prediction Accuracy = 59.476%, Loss = 0.6481653451919556
Epoch: 1061, Batch Gradient Norm: 14.14776106262307
Epoch: 1061, Batch Gradient Norm after: 14.14776106262307
Epoch 1062/10000, Prediction Accuracy = 59.63800000000001%, Loss = 0.6547182083129883
Epoch: 1062, Batch Gradient Norm: 13.09944806648913
Epoch: 1062, Batch Gradient Norm after: 13.09944806648913
Epoch 1063/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.648417341709137
Epoch: 1063, Batch Gradient Norm: 12.860307875818322
Epoch: 1063, Batch Gradient Norm after: 12.860307875818322
Epoch 1064/10000, Prediction Accuracy = 59.714%, Loss = 0.6442058086395264
Epoch: 1064, Batch Gradient Norm: 11.560205094778357
Epoch: 1064, Batch Gradient Norm after: 11.560205094778357
Epoch 1065/10000, Prediction Accuracy = 59.604%, Loss = 0.631840443611145
Epoch: 1065, Batch Gradient Norm: 11.719433401697524
Epoch: 1065, Batch Gradient Norm after: 11.719433401697524
Epoch 1066/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.6341751098632813
Epoch: 1066, Batch Gradient Norm: 14.917565411858202
Epoch: 1066, Batch Gradient Norm after: 14.917565411858202
Epoch 1067/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.6533358097076416
Epoch: 1067, Batch Gradient Norm: 14.962610877309809
Epoch: 1067, Batch Gradient Norm after: 14.962610877309809
Epoch 1068/10000, Prediction Accuracy = 59.528%, Loss = 0.6566650748252869
Epoch: 1068, Batch Gradient Norm: 11.284763544492867
Epoch: 1068, Batch Gradient Norm after: 11.284763544492867
Epoch 1069/10000, Prediction Accuracy = 59.50999999999999%, Loss = 0.6382706642150879
Epoch: 1069, Batch Gradient Norm: 7.923984731351859
Epoch: 1069, Batch Gradient Norm after: 7.923984731351859
Epoch 1070/10000, Prediction Accuracy = 59.572%, Loss = 0.6119420409202576
Epoch: 1070, Batch Gradient Norm: 10.454192707874464
Epoch: 1070, Batch Gradient Norm after: 10.454192707874464
Epoch 1071/10000, Prediction Accuracy = 59.56%, Loss = 0.627822744846344
Epoch: 1071, Batch Gradient Norm: 10.250019892123971
Epoch: 1071, Batch Gradient Norm after: 10.250019892123971
Epoch 1072/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.6260870456695556
Epoch: 1072, Batch Gradient Norm: 8.226702281720454
Epoch: 1072, Batch Gradient Norm after: 8.226702281720454
Epoch 1073/10000, Prediction Accuracy = 59.50600000000001%, Loss = 0.6147180676460267
Epoch: 1073, Batch Gradient Norm: 8.97905675761771
Epoch: 1073, Batch Gradient Norm after: 8.97905675761771
Epoch 1074/10000, Prediction Accuracy = 59.576%, Loss = 0.6172418355941772
Epoch: 1074, Batch Gradient Norm: 10.406845051022447
Epoch: 1074, Batch Gradient Norm after: 10.406845051022447
Epoch 1075/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.6254464626312256
Epoch: 1075, Batch Gradient Norm: 15.12723963315246
Epoch: 1075, Batch Gradient Norm after: 15.12723963315246
Epoch 1076/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.6611708402633667
Epoch: 1076, Batch Gradient Norm: 18.512487757903877
Epoch: 1076, Batch Gradient Norm after: 18.512487757903877
Epoch 1077/10000, Prediction Accuracy = 59.58%, Loss = 0.6872861623764038
Epoch: 1077, Batch Gradient Norm: 10.410376488652686
Epoch: 1077, Batch Gradient Norm after: 10.410376488652686
Epoch 1078/10000, Prediction Accuracy = 59.696000000000005%, Loss = 0.6233721375465393
Epoch: 1078, Batch Gradient Norm: 10.142544141873726
Epoch: 1078, Batch Gradient Norm after: 10.142544141873726
Epoch 1079/10000, Prediction Accuracy = 59.577999999999996%, Loss = 0.6198933959007263
Epoch: 1079, Batch Gradient Norm: 10.830347148194436
Epoch: 1079, Batch Gradient Norm after: 10.830347148194436
Epoch 1080/10000, Prediction Accuracy = 59.64%, Loss = 0.6275278210639954
Epoch: 1080, Batch Gradient Norm: 13.653196943273528
Epoch: 1080, Batch Gradient Norm after: 13.653196943273528
Epoch 1081/10000, Prediction Accuracy = 59.45799999999999%, Loss = 0.6451478481292725
Epoch: 1081, Batch Gradient Norm: 16.51168187294048
Epoch: 1081, Batch Gradient Norm after: 16.51168187294048
Epoch 1082/10000, Prediction Accuracy = 59.676%, Loss = 0.6664722681045532
Epoch: 1082, Batch Gradient Norm: 13.539662523260235
Epoch: 1082, Batch Gradient Norm after: 13.539662523260235
Epoch 1083/10000, Prediction Accuracy = 59.592%, Loss = 0.6425760388374329
Epoch: 1083, Batch Gradient Norm: 9.59187940285977
Epoch: 1083, Batch Gradient Norm after: 9.59187940285977
Epoch 1084/10000, Prediction Accuracy = 59.568000000000005%, Loss = 0.6163161516189575
Epoch: 1084, Batch Gradient Norm: 9.13926443137032
Epoch: 1084, Batch Gradient Norm after: 9.13926443137032
Epoch 1085/10000, Prediction Accuracy = 59.516%, Loss = 0.6152634501457215
Epoch: 1085, Batch Gradient Norm: 10.822745338316505
Epoch: 1085, Batch Gradient Norm after: 10.822745338316505
Epoch 1086/10000, Prediction Accuracy = 59.634%, Loss = 0.6269719719886779
Epoch: 1086, Batch Gradient Norm: 9.877545901819555
Epoch: 1086, Batch Gradient Norm after: 9.877545901819555
Epoch 1087/10000, Prediction Accuracy = 59.596000000000004%, Loss = 0.6186372756958007
Epoch: 1087, Batch Gradient Norm: 9.345674919111506
Epoch: 1087, Batch Gradient Norm after: 9.345674919111506
Epoch 1088/10000, Prediction Accuracy = 59.634%, Loss = 0.6148786067962646
Epoch: 1088, Batch Gradient Norm: 9.763994064913053
Epoch: 1088, Batch Gradient Norm after: 9.763994064913053
Epoch 1089/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.6177690625190735
Epoch: 1089, Batch Gradient Norm: 12.30665345284326
Epoch: 1089, Batch Gradient Norm after: 12.30665345284326
Epoch 1090/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.6317084908485413
Epoch: 1090, Batch Gradient Norm: 11.718809673328032
Epoch: 1090, Batch Gradient Norm after: 11.718809673328032
Epoch 1091/10000, Prediction Accuracy = 59.552%, Loss = 0.6249338626861572
Epoch: 1091, Batch Gradient Norm: 13.715406980880752
Epoch: 1091, Batch Gradient Norm after: 13.715406980880752
Epoch 1092/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.6402845501899719
Epoch: 1092, Batch Gradient Norm: 13.768756387215838
Epoch: 1092, Batch Gradient Norm after: 13.768756387215838
Epoch 1093/10000, Prediction Accuracy = 59.58200000000001%, Loss = 0.6442500829696656
Epoch: 1093, Batch Gradient Norm: 17.395671137647547
Epoch: 1093, Batch Gradient Norm after: 17.395671137647547
Epoch 1094/10000, Prediction Accuracy = 59.588%, Loss = 0.6731382250785828
Epoch: 1094, Batch Gradient Norm: 14.805396360057609
Epoch: 1094, Batch Gradient Norm after: 14.805396360057609
Epoch 1095/10000, Prediction Accuracy = 59.653999999999996%, Loss = 0.648676085472107
Epoch: 1095, Batch Gradient Norm: 7.240110502986483
Epoch: 1095, Batch Gradient Norm after: 7.240110502986483
Epoch 1096/10000, Prediction Accuracy = 59.674%, Loss = 0.6056484580039978
Epoch: 1096, Batch Gradient Norm: 9.32719931825304
Epoch: 1096, Batch Gradient Norm after: 9.32719931825304
Epoch 1097/10000, Prediction Accuracy = 59.50600000000001%, Loss = 0.6167870759963989
Epoch: 1097, Batch Gradient Norm: 10.183430783556577
Epoch: 1097, Batch Gradient Norm after: 10.183430783556577
Epoch 1098/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.6200721859931946
Epoch: 1098, Batch Gradient Norm: 10.034714437647303
Epoch: 1098, Batch Gradient Norm after: 10.034714437647303
Epoch 1099/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.6174440145492553
Epoch: 1099, Batch Gradient Norm: 9.643931594251086
Epoch: 1099, Batch Gradient Norm after: 9.643931594251086
Epoch 1100/10000, Prediction Accuracy = 59.638%, Loss = 0.6134417653083801
Epoch: 1100, Batch Gradient Norm: 16.133360426804476
Epoch: 1100, Batch Gradient Norm after: 16.133360426804476
Epoch 1101/10000, Prediction Accuracy = 59.624%, Loss = 0.6634656429290772
Epoch: 1101, Batch Gradient Norm: 14.08158654300832
Epoch: 1101, Batch Gradient Norm after: 14.08158654300832
Epoch 1102/10000, Prediction Accuracy = 59.589999999999996%, Loss = 0.6463011145591736
Epoch: 1102, Batch Gradient Norm: 10.006738422299508
Epoch: 1102, Batch Gradient Norm after: 10.006738422299508
Epoch 1103/10000, Prediction Accuracy = 59.648%, Loss = 0.620983338356018
Epoch: 1103, Batch Gradient Norm: 11.069664565752134
Epoch: 1103, Batch Gradient Norm after: 11.069664565752134
Epoch 1104/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.6267496466636657
Epoch: 1104, Batch Gradient Norm: 10.585398663409944
Epoch: 1104, Batch Gradient Norm after: 10.585398663409944
Epoch 1105/10000, Prediction Accuracy = 59.622%, Loss = 0.6177256941795349
Epoch: 1105, Batch Gradient Norm: 13.58268271364836
Epoch: 1105, Batch Gradient Norm after: 13.58268271364836
Epoch 1106/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.640561830997467
Epoch: 1106, Batch Gradient Norm: 12.038568805862043
Epoch: 1106, Batch Gradient Norm after: 12.038568805862043
Epoch 1107/10000, Prediction Accuracy = 59.622%, Loss = 0.6277843713760376
Epoch: 1107, Batch Gradient Norm: 15.862549932020551
Epoch: 1107, Batch Gradient Norm after: 15.862549932020551
Epoch 1108/10000, Prediction Accuracy = 59.681999999999995%, Loss = 0.6590208411216736
Epoch: 1108, Batch Gradient Norm: 12.350398109298503
Epoch: 1108, Batch Gradient Norm after: 12.350398109298503
Epoch 1109/10000, Prediction Accuracy = 59.622%, Loss = 0.6305739998817443
Epoch: 1109, Batch Gradient Norm: 10.080999133006586
Epoch: 1109, Batch Gradient Norm after: 10.080999133006586
Epoch 1110/10000, Prediction Accuracy = 59.678%, Loss = 0.6140408515930176
Epoch: 1110, Batch Gradient Norm: 11.523309910287983
Epoch: 1110, Batch Gradient Norm after: 11.523309910287983
Epoch 1111/10000, Prediction Accuracy = 59.532000000000004%, Loss = 0.6283340454101562
Epoch: 1111, Batch Gradient Norm: 11.77859563025302
Epoch: 1111, Batch Gradient Norm after: 11.77859563025302
Epoch 1112/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.6389070987701416
Epoch: 1112, Batch Gradient Norm: 11.504784029118388
Epoch: 1112, Batch Gradient Norm after: 11.504784029118388
Epoch 1113/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.6259655237197876
Epoch: 1113, Batch Gradient Norm: 11.29792136545866
Epoch: 1113, Batch Gradient Norm after: 11.29792136545866
Epoch 1114/10000, Prediction Accuracy = 59.574%, Loss = 0.6237617015838623
Epoch: 1114, Batch Gradient Norm: 12.159985898669937
Epoch: 1114, Batch Gradient Norm after: 12.159985898669937
Epoch 1115/10000, Prediction Accuracy = 59.666%, Loss = 0.6301708102226258
Epoch: 1115, Batch Gradient Norm: 7.763169490174247
Epoch: 1115, Batch Gradient Norm after: 7.763169490174247
Epoch 1116/10000, Prediction Accuracy = 59.682%, Loss = 0.6054654359817505
Epoch: 1116, Batch Gradient Norm: 9.152926615770312
Epoch: 1116, Batch Gradient Norm after: 9.152926615770312
Epoch 1117/10000, Prediction Accuracy = 59.616%, Loss = 0.6138196229934693
Epoch: 1117, Batch Gradient Norm: 14.247880856733188
Epoch: 1117, Batch Gradient Norm after: 14.247880856733188
Epoch 1118/10000, Prediction Accuracy = 59.53399999999999%, Loss = 0.660881519317627
Epoch: 1118, Batch Gradient Norm: 12.962299888924516
Epoch: 1118, Batch Gradient Norm after: 12.962299888924516
Epoch 1119/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.6448646187782288
Epoch: 1119, Batch Gradient Norm: 10.219129109917315
Epoch: 1119, Batch Gradient Norm after: 10.219129109917315
Epoch 1120/10000, Prediction Accuracy = 59.64%, Loss = 0.614837646484375
Epoch: 1120, Batch Gradient Norm: 14.117835357458594
Epoch: 1120, Batch Gradient Norm after: 14.117835357458594
Epoch 1121/10000, Prediction Accuracy = 59.58799999999999%, Loss = 0.6400295257568359
Epoch: 1121, Batch Gradient Norm: 8.39285901854231
Epoch: 1121, Batch Gradient Norm after: 8.39285901854231
Epoch 1122/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.6071779608726502
Epoch: 1122, Batch Gradient Norm: 10.157402938639063
Epoch: 1122, Batch Gradient Norm after: 10.157402938639063
Epoch 1123/10000, Prediction Accuracy = 59.68800000000001%, Loss = 0.6145431518554687
Epoch: 1123, Batch Gradient Norm: 14.736848987666106
Epoch: 1123, Batch Gradient Norm after: 14.736848987666106
Epoch 1124/10000, Prediction Accuracy = 59.68000000000001%, Loss = 0.6501235246658326
Epoch: 1124, Batch Gradient Norm: 13.63417900038541
Epoch: 1124, Batch Gradient Norm after: 13.63417900038541
Epoch 1125/10000, Prediction Accuracy = 59.592%, Loss = 0.6370763063430787
Epoch: 1125, Batch Gradient Norm: 11.05428960138987
Epoch: 1125, Batch Gradient Norm after: 11.05428960138987
Epoch 1126/10000, Prediction Accuracy = 59.564%, Loss = 0.6254899621009826
Epoch: 1126, Batch Gradient Norm: 11.162328845589137
Epoch: 1126, Batch Gradient Norm after: 11.162328845589137
Epoch 1127/10000, Prediction Accuracy = 59.73%, Loss = 0.6198489785194397
Epoch: 1127, Batch Gradient Norm: 13.055106420785561
Epoch: 1127, Batch Gradient Norm after: 13.055106420785561
Epoch 1128/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.6319589376449585
Epoch: 1128, Batch Gradient Norm: 12.44925212250583
Epoch: 1128, Batch Gradient Norm after: 12.44925212250583
Epoch 1129/10000, Prediction Accuracy = 59.684000000000005%, Loss = 0.6322415590286254
Epoch: 1129, Batch Gradient Norm: 13.014337540023318
Epoch: 1129, Batch Gradient Norm after: 13.014337540023318
Epoch 1130/10000, Prediction Accuracy = 59.638%, Loss = 0.6304183006286621
Epoch: 1130, Batch Gradient Norm: 11.21907865064617
Epoch: 1130, Batch Gradient Norm after: 11.21907865064617
Epoch 1131/10000, Prediction Accuracy = 59.614%, Loss = 0.6209363102912903
Epoch: 1131, Batch Gradient Norm: 10.410514250390948
Epoch: 1131, Batch Gradient Norm after: 10.410514250390948
Epoch 1132/10000, Prediction Accuracy = 59.694%, Loss = 0.6159304499626159
Epoch: 1132, Batch Gradient Norm: 12.673879263122426
Epoch: 1132, Batch Gradient Norm after: 12.673879263122426
Epoch 1133/10000, Prediction Accuracy = 59.65%, Loss = 0.636288583278656
Epoch: 1133, Batch Gradient Norm: 11.719268523873316
Epoch: 1133, Batch Gradient Norm after: 11.719268523873316
Epoch 1134/10000, Prediction Accuracy = 59.734%, Loss = 0.6222463607788086
Epoch: 1134, Batch Gradient Norm: 12.78723863017007
Epoch: 1134, Batch Gradient Norm after: 12.78723863017007
Epoch 1135/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.6285458683967591
Epoch: 1135, Batch Gradient Norm: 13.847845036733984
Epoch: 1135, Batch Gradient Norm after: 13.847845036733984
Epoch 1136/10000, Prediction Accuracy = 59.61199999999999%, Loss = 0.6375042557716369
Epoch: 1136, Batch Gradient Norm: 12.834214477962293
Epoch: 1136, Batch Gradient Norm after: 12.834214477962293
Epoch 1137/10000, Prediction Accuracy = 59.668000000000006%, Loss = 0.6328313469886779
Epoch: 1137, Batch Gradient Norm: 9.552739803902627
Epoch: 1137, Batch Gradient Norm after: 9.552739803902627
Epoch 1138/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.6105039834976196
Epoch: 1138, Batch Gradient Norm: 11.352764510346296
Epoch: 1138, Batch Gradient Norm after: 11.352764510346296
Epoch 1139/10000, Prediction Accuracy = 59.60600000000001%, Loss = 0.6217118501663208
Epoch: 1139, Batch Gradient Norm: 11.402484903979866
Epoch: 1139, Batch Gradient Norm after: 11.402484903979866
Epoch 1140/10000, Prediction Accuracy = 59.64200000000001%, Loss = 0.62225341796875
Epoch: 1140, Batch Gradient Norm: 13.194321699052969
Epoch: 1140, Batch Gradient Norm after: 13.194321699052969
Epoch 1141/10000, Prediction Accuracy = 59.73%, Loss = 0.6353362679481507
Epoch: 1141, Batch Gradient Norm: 14.274061760122288
Epoch: 1141, Batch Gradient Norm after: 14.274061760122288
Epoch 1142/10000, Prediction Accuracy = 59.628%, Loss = 0.6457225918769837
Epoch: 1142, Batch Gradient Norm: 10.720130071724709
Epoch: 1142, Batch Gradient Norm after: 10.720130071724709
Epoch 1143/10000, Prediction Accuracy = 59.714%, Loss = 0.61957848072052
Epoch: 1143, Batch Gradient Norm: 11.891321890783614
Epoch: 1143, Batch Gradient Norm after: 11.891321890783614
Epoch 1144/10000, Prediction Accuracy = 59.702%, Loss = 0.6229599833488464
Epoch: 1144, Batch Gradient Norm: 11.401116665382334
Epoch: 1144, Batch Gradient Norm after: 11.401116665382334
Epoch 1145/10000, Prediction Accuracy = 59.798%, Loss = 0.6190106391906738
Epoch: 1145, Batch Gradient Norm: 10.096526412281307
Epoch: 1145, Batch Gradient Norm after: 10.096526412281307
Epoch 1146/10000, Prediction Accuracy = 59.681999999999995%, Loss = 0.6113335371017456
Epoch: 1146, Batch Gradient Norm: 12.618724774036346
Epoch: 1146, Batch Gradient Norm after: 12.618724774036346
Epoch 1147/10000, Prediction Accuracy = 59.686%, Loss = 0.6298842906951905
Epoch: 1147, Batch Gradient Norm: 14.752483280956158
Epoch: 1147, Batch Gradient Norm after: 14.752483280956158
Epoch 1148/10000, Prediction Accuracy = 59.688%, Loss = 0.6471081495285034
Epoch: 1148, Batch Gradient Norm: 11.160430383316744
Epoch: 1148, Batch Gradient Norm after: 11.160430383316744
Epoch 1149/10000, Prediction Accuracy = 59.718%, Loss = 0.6228219985961914
Epoch: 1149, Batch Gradient Norm: 10.20534500199089
Epoch: 1149, Batch Gradient Norm after: 10.20534500199089
Epoch 1150/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.6208776354789733
Epoch: 1150, Batch Gradient Norm: 11.859074359496956
Epoch: 1150, Batch Gradient Norm after: 11.859074359496956
Epoch 1151/10000, Prediction Accuracy = 59.622%, Loss = 0.6353994965553283
Epoch: 1151, Batch Gradient Norm: 8.94026261734279
Epoch: 1151, Batch Gradient Norm after: 8.94026261734279
Epoch 1152/10000, Prediction Accuracy = 59.698%, Loss = 0.6043760299682617
Epoch: 1152, Batch Gradient Norm: 12.57471203088038
Epoch: 1152, Batch Gradient Norm after: 12.57471203088038
Epoch 1153/10000, Prediction Accuracy = 59.724000000000004%, Loss = 0.6269203186035156
Epoch: 1153, Batch Gradient Norm: 12.523727636811424
Epoch: 1153, Batch Gradient Norm after: 12.523727636811424
Epoch 1154/10000, Prediction Accuracy = 59.626%, Loss = 0.631437623500824
Epoch: 1154, Batch Gradient Norm: 12.12557898732105
Epoch: 1154, Batch Gradient Norm after: 12.12557898732105
Epoch 1155/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.6284342288970948
Epoch: 1155, Batch Gradient Norm: 12.659069958863487
Epoch: 1155, Batch Gradient Norm after: 12.659069958863487
Epoch 1156/10000, Prediction Accuracy = 59.738%, Loss = 0.6285856485366821
Epoch: 1156, Batch Gradient Norm: 12.1935646118084
Epoch: 1156, Batch Gradient Norm after: 12.1935646118084
Epoch 1157/10000, Prediction Accuracy = 59.726%, Loss = 0.6267722368240356
Epoch: 1157, Batch Gradient Norm: 14.95055274640664
Epoch: 1157, Batch Gradient Norm after: 14.95055274640664
Epoch 1158/10000, Prediction Accuracy = 59.698%, Loss = 0.6474206686019898
Epoch: 1158, Batch Gradient Norm: 9.972820895734195
Epoch: 1158, Batch Gradient Norm after: 9.972820895734195
Epoch 1159/10000, Prediction Accuracy = 59.69%, Loss = 0.6119583964347839
Epoch: 1159, Batch Gradient Norm: 8.536208590187204
Epoch: 1159, Batch Gradient Norm after: 8.536208590187204
Epoch 1160/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.6101723194122315
Epoch: 1160, Batch Gradient Norm: 11.141784061511283
Epoch: 1160, Batch Gradient Norm after: 11.141784061511283
Epoch 1161/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.6170500040054321
Epoch: 1161, Batch Gradient Norm: 13.6856398576548
Epoch: 1161, Batch Gradient Norm after: 13.6856398576548
Epoch 1162/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.6379510283470153
Epoch: 1162, Batch Gradient Norm: 11.448022889674423
Epoch: 1162, Batch Gradient Norm after: 11.448022889674423
Epoch 1163/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.6186072587966919
Epoch: 1163, Batch Gradient Norm: 13.33568004586141
Epoch: 1163, Batch Gradient Norm after: 13.33568004586141
Epoch 1164/10000, Prediction Accuracy = 59.676%, Loss = 0.6284287571907043
Epoch: 1164, Batch Gradient Norm: 13.712041444702614
Epoch: 1164, Batch Gradient Norm after: 13.712041444702614
Epoch 1165/10000, Prediction Accuracy = 59.686%, Loss = 0.6344485759735108
Epoch: 1165, Batch Gradient Norm: 8.389633390301718
Epoch: 1165, Batch Gradient Norm after: 8.389633390301718
Epoch 1166/10000, Prediction Accuracy = 59.788%, Loss = 0.6012119770050048
Epoch: 1166, Batch Gradient Norm: 9.941142227306045
Epoch: 1166, Batch Gradient Norm after: 9.941142227306045
Epoch 1167/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.6073066353797912
Epoch: 1167, Batch Gradient Norm: 15.797479334568724
Epoch: 1167, Batch Gradient Norm after: 15.797479334568724
Epoch 1168/10000, Prediction Accuracy = 59.598%, Loss = 0.6558172821998596
Epoch: 1168, Batch Gradient Norm: 10.892551870555783
Epoch: 1168, Batch Gradient Norm after: 10.892551870555783
Epoch 1169/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.6172319293022156
Epoch: 1169, Batch Gradient Norm: 10.360883514538214
Epoch: 1169, Batch Gradient Norm after: 10.360883514538214
Epoch 1170/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.6118985652923584
Epoch: 1170, Batch Gradient Norm: 11.678634858344562
Epoch: 1170, Batch Gradient Norm after: 11.678634858344562
Epoch 1171/10000, Prediction Accuracy = 59.7%, Loss = 0.6186527609825134
Epoch: 1171, Batch Gradient Norm: 13.738403455511444
Epoch: 1171, Batch Gradient Norm after: 13.738403455511444
Epoch 1172/10000, Prediction Accuracy = 59.74000000000001%, Loss = 0.638715386390686
Epoch: 1172, Batch Gradient Norm: 9.274492175158136
Epoch: 1172, Batch Gradient Norm after: 9.274492175158136
Epoch 1173/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.6102324485778808
Epoch: 1173, Batch Gradient Norm: 12.753629791863425
Epoch: 1173, Batch Gradient Norm after: 12.753629791863425
Epoch 1174/10000, Prediction Accuracy = 59.706%, Loss = 0.6232020258903503
Epoch: 1174, Batch Gradient Norm: 15.42500670867041
Epoch: 1174, Batch Gradient Norm after: 15.42500670867041
Epoch 1175/10000, Prediction Accuracy = 59.60600000000001%, Loss = 0.6461412906646729
Epoch: 1175, Batch Gradient Norm: 12.131526948575722
Epoch: 1175, Batch Gradient Norm after: 12.131526948575722
Epoch 1176/10000, Prediction Accuracy = 59.778%, Loss = 0.6200898051261902
Epoch: 1176, Batch Gradient Norm: 15.500780008604467
Epoch: 1176, Batch Gradient Norm after: 15.500780008604467
Epoch 1177/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.6506589889526367
Epoch: 1177, Batch Gradient Norm: 8.550506895586084
Epoch: 1177, Batch Gradient Norm after: 8.550506895586084
Epoch 1178/10000, Prediction Accuracy = 59.73%, Loss = 0.6013067245483399
Epoch: 1178, Batch Gradient Norm: 8.736312271540285
Epoch: 1178, Batch Gradient Norm after: 8.736312271540285
Epoch 1179/10000, Prediction Accuracy = 59.720000000000006%, Loss = 0.6000266790390014
Epoch: 1179, Batch Gradient Norm: 10.752884779031502
Epoch: 1179, Batch Gradient Norm after: 10.752884779031502
Epoch 1180/10000, Prediction Accuracy = 59.75%, Loss = 0.6180888772010803
Epoch: 1180, Batch Gradient Norm: 12.759069028635455
Epoch: 1180, Batch Gradient Norm after: 12.759069028635455
Epoch 1181/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.6306337833404541
Epoch: 1181, Batch Gradient Norm: 11.389745716959178
Epoch: 1181, Batch Gradient Norm after: 11.389745716959178
Epoch 1182/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.6157099366188049
Epoch: 1182, Batch Gradient Norm: 11.903750073725652
Epoch: 1182, Batch Gradient Norm after: 11.903750073725652
Epoch 1183/10000, Prediction Accuracy = 59.77%, Loss = 0.6199141502380371
Epoch: 1183, Batch Gradient Norm: 13.323621472757022
Epoch: 1183, Batch Gradient Norm after: 13.323621472757022
Epoch 1184/10000, Prediction Accuracy = 59.754000000000005%, Loss = 0.626406979560852
Epoch: 1184, Batch Gradient Norm: 17.161129578678736
Epoch: 1184, Batch Gradient Norm after: 17.161129578678736
Epoch 1185/10000, Prediction Accuracy = 59.726%, Loss = 0.6628908157348633
Epoch: 1185, Batch Gradient Norm: 10.962086632969603
Epoch: 1185, Batch Gradient Norm after: 10.962086632969603
Epoch 1186/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.6178858280181885
Epoch: 1186, Batch Gradient Norm: 10.918445797118299
Epoch: 1186, Batch Gradient Norm after: 10.918445797118299
Epoch 1187/10000, Prediction Accuracy = 59.73199999999999%, Loss = 0.6224629402160644
Epoch: 1187, Batch Gradient Norm: 10.19466176134369
Epoch: 1187, Batch Gradient Norm after: 10.19466176134369
Epoch 1188/10000, Prediction Accuracy = 59.788%, Loss = 0.6144280195236206
Epoch: 1188, Batch Gradient Norm: 9.264890596264426
Epoch: 1188, Batch Gradient Norm after: 9.264890596264426
Epoch 1189/10000, Prediction Accuracy = 59.8%, Loss = 0.6024094581604004
Epoch: 1189, Batch Gradient Norm: 10.968107954459622
Epoch: 1189, Batch Gradient Norm after: 10.968107954459622
Epoch 1190/10000, Prediction Accuracy = 59.790000000000006%, Loss = 0.6151721715927124
Epoch: 1190, Batch Gradient Norm: 11.04906977239451
Epoch: 1190, Batch Gradient Norm after: 11.04906977239451
Epoch 1191/10000, Prediction Accuracy = 59.83999999999999%, Loss = 0.6133101463317872
Epoch: 1191, Batch Gradient Norm: 11.029458915189517
Epoch: 1191, Batch Gradient Norm after: 11.029458915189517
Epoch 1192/10000, Prediction Accuracy = 59.69200000000001%, Loss = 0.6128464341163635
Epoch: 1192, Batch Gradient Norm: 11.540276518936803
Epoch: 1192, Batch Gradient Norm after: 11.540276518936803
Epoch 1193/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.613051462173462
Epoch: 1193, Batch Gradient Norm: 11.274712899783616
Epoch: 1193, Batch Gradient Norm after: 11.274712899783616
Epoch 1194/10000, Prediction Accuracy = 59.79600000000001%, Loss = 0.6173184871673584
Epoch: 1194, Batch Gradient Norm: 14.074170751694691
Epoch: 1194, Batch Gradient Norm after: 14.074170751694691
Epoch 1195/10000, Prediction Accuracy = 59.758%, Loss = 0.6330670595169068
Epoch: 1195, Batch Gradient Norm: 12.954165086343899
Epoch: 1195, Batch Gradient Norm after: 12.954165086343899
Epoch 1196/10000, Prediction Accuracy = 59.722%, Loss = 0.6233051896095276
Epoch: 1196, Batch Gradient Norm: 13.34734621973214
Epoch: 1196, Batch Gradient Norm after: 13.34734621973214
Epoch 1197/10000, Prediction Accuracy = 59.762%, Loss = 0.6280920147895813
Epoch: 1197, Batch Gradient Norm: 12.15534894696281
Epoch: 1197, Batch Gradient Norm after: 12.15534894696281
Epoch 1198/10000, Prediction Accuracy = 59.646%, Loss = 0.6182962536811829
Epoch: 1198, Batch Gradient Norm: 9.298011861261005
Epoch: 1198, Batch Gradient Norm after: 9.298011861261005
Epoch 1199/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.6018852710723877
Epoch: 1199, Batch Gradient Norm: 12.771201243631763
Epoch: 1199, Batch Gradient Norm after: 12.771201243631763
Epoch 1200/10000, Prediction Accuracy = 59.688%, Loss = 0.6252125144004822
Epoch: 1200, Batch Gradient Norm: 16.315507416427593
Epoch: 1200, Batch Gradient Norm after: 16.315507416427593
Epoch 1201/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.6535736203193665
Epoch: 1201, Batch Gradient Norm: 13.323671946168165
Epoch: 1201, Batch Gradient Norm after: 13.323671946168165
Epoch 1202/10000, Prediction Accuracy = 59.722%, Loss = 0.6429405927658081
Epoch: 1202, Batch Gradient Norm: 7.633550861499047
Epoch: 1202, Batch Gradient Norm after: 7.633550861499047
Epoch 1203/10000, Prediction Accuracy = 59.831999999999994%, Loss = 0.598485279083252
Epoch: 1203, Batch Gradient Norm: 8.757616374704458
Epoch: 1203, Batch Gradient Norm after: 8.757616374704458
Epoch 1204/10000, Prediction Accuracy = 59.77%, Loss = 0.5978628396987915
Epoch: 1204, Batch Gradient Norm: 11.73989702033645
Epoch: 1204, Batch Gradient Norm after: 11.73989702033645
Epoch 1205/10000, Prediction Accuracy = 59.778%, Loss = 0.6167331457138061
Epoch: 1205, Batch Gradient Norm: 10.782360135060868
Epoch: 1205, Batch Gradient Norm after: 10.782360135060868
Epoch 1206/10000, Prediction Accuracy = 59.834%, Loss = 0.6147238135337829
Epoch: 1206, Batch Gradient Norm: 9.581500464387306
Epoch: 1206, Batch Gradient Norm after: 9.581500464387306
Epoch 1207/10000, Prediction Accuracy = 59.68399999999999%, Loss = 0.6046517252922058
Epoch: 1207, Batch Gradient Norm: 14.97965347194674
Epoch: 1207, Batch Gradient Norm after: 14.97965347194674
Epoch 1208/10000, Prediction Accuracy = 59.686%, Loss = 0.6393327593803406
Epoch: 1208, Batch Gradient Norm: 10.46946809039877
Epoch: 1208, Batch Gradient Norm after: 10.46946809039877
Epoch 1209/10000, Prediction Accuracy = 59.876%, Loss = 0.6122019410133361
Epoch: 1209, Batch Gradient Norm: 7.635069843188485
Epoch: 1209, Batch Gradient Norm after: 7.635069843188485
Epoch 1210/10000, Prediction Accuracy = 59.812%, Loss = 0.5943425416946411
Epoch: 1210, Batch Gradient Norm: 10.062276198135772
Epoch: 1210, Batch Gradient Norm after: 10.062276198135772
Epoch 1211/10000, Prediction Accuracy = 59.80999999999999%, Loss = 0.6038876175880432
Epoch: 1211, Batch Gradient Norm: 13.803741906557429
Epoch: 1211, Batch Gradient Norm after: 13.803741906557429
Epoch 1212/10000, Prediction Accuracy = 59.77%, Loss = 0.6255863070487976
Epoch: 1212, Batch Gradient Norm: 14.694637464578713
Epoch: 1212, Batch Gradient Norm after: 14.694637464578713
Epoch 1213/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.6314274311065674
Epoch: 1213, Batch Gradient Norm: 11.41617900601599
Epoch: 1213, Batch Gradient Norm after: 11.41617900601599
Epoch 1214/10000, Prediction Accuracy = 59.77%, Loss = 0.6118254661560059
Epoch: 1214, Batch Gradient Norm: 11.820942060984443
Epoch: 1214, Batch Gradient Norm after: 11.820942060984443
Epoch 1215/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.6210931777954102
Epoch: 1215, Batch Gradient Norm: 14.460812667659996
Epoch: 1215, Batch Gradient Norm after: 14.460812667659996
Epoch 1216/10000, Prediction Accuracy = 59.766%, Loss = 0.6378408670425415
Epoch: 1216, Batch Gradient Norm: 10.746705244215985
Epoch: 1216, Batch Gradient Norm after: 10.746705244215985
Epoch 1217/10000, Prediction Accuracy = 59.762%, Loss = 0.6125153064727783
Epoch: 1217, Batch Gradient Norm: 10.722562106443757
Epoch: 1217, Batch Gradient Norm after: 10.722562106443757
Epoch 1218/10000, Prediction Accuracy = 59.775999999999996%, Loss = 0.6094709396362304
Epoch: 1218, Batch Gradient Norm: 12.042997740257242
Epoch: 1218, Batch Gradient Norm after: 12.042997740257242
Epoch 1219/10000, Prediction Accuracy = 59.727999999999994%, Loss = 0.6167531132698059
Epoch: 1219, Batch Gradient Norm: 11.133065610970563
Epoch: 1219, Batch Gradient Norm after: 11.133065610970563
Epoch 1220/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.612794017791748
Epoch: 1220, Batch Gradient Norm: 8.719663979897366
Epoch: 1220, Batch Gradient Norm after: 8.719663979897366
Epoch 1221/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.596949303150177
Epoch: 1221, Batch Gradient Norm: 14.8062710759404
Epoch: 1221, Batch Gradient Norm after: 14.8062710759404
Epoch 1222/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.6381312131881713
Epoch: 1222, Batch Gradient Norm: 12.674864376963583
Epoch: 1222, Batch Gradient Norm after: 12.674864376963583
Epoch 1223/10000, Prediction Accuracy = 59.872%, Loss = 0.621580183506012
Epoch: 1223, Batch Gradient Norm: 9.19225397030692
Epoch: 1223, Batch Gradient Norm after: 9.19225397030692
Epoch 1224/10000, Prediction Accuracy = 59.876%, Loss = 0.5981309652328491
Epoch: 1224, Batch Gradient Norm: 11.427545346008099
Epoch: 1224, Batch Gradient Norm after: 11.427545346008099
Epoch 1225/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.6098168849945068
Epoch: 1225, Batch Gradient Norm: 11.041422285984694
Epoch: 1225, Batch Gradient Norm after: 11.041422285984694
Epoch 1226/10000, Prediction Accuracy = 59.74000000000001%, Loss = 0.6105313658714294
Epoch: 1226, Batch Gradient Norm: 10.671439151616848
Epoch: 1226, Batch Gradient Norm after: 10.671439151616848
Epoch 1227/10000, Prediction Accuracy = 59.786%, Loss = 0.6088817596435547
Epoch: 1227, Batch Gradient Norm: 13.268202626425255
Epoch: 1227, Batch Gradient Norm after: 13.268202626425255
Epoch 1228/10000, Prediction Accuracy = 59.73%, Loss = 0.6277856945991516
Epoch: 1228, Batch Gradient Norm: 10.214118974830127
Epoch: 1228, Batch Gradient Norm after: 10.214118974830127
Epoch 1229/10000, Prediction Accuracy = 59.80799999999999%, Loss = 0.6077332854270935
Epoch: 1229, Batch Gradient Norm: 10.53810098733212
Epoch: 1229, Batch Gradient Norm after: 10.53810098733212
Epoch 1230/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.6076609492301941
Epoch: 1230, Batch Gradient Norm: 11.656323417688531
Epoch: 1230, Batch Gradient Norm after: 11.656323417688531
Epoch 1231/10000, Prediction Accuracy = 59.714%, Loss = 0.6116808176040649
Epoch: 1231, Batch Gradient Norm: 15.822462894415262
Epoch: 1231, Batch Gradient Norm after: 15.822462894415262
Epoch 1232/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.6471346259117127
Epoch: 1232, Batch Gradient Norm: 13.358572337960757
Epoch: 1232, Batch Gradient Norm after: 13.358572337960757
Epoch 1233/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.6270065784454346
Epoch: 1233, Batch Gradient Norm: 11.599958087269112
Epoch: 1233, Batch Gradient Norm after: 11.599958087269112
Epoch 1234/10000, Prediction Accuracy = 59.938%, Loss = 0.6101874232292175
Epoch: 1234, Batch Gradient Norm: 12.185501092951077
Epoch: 1234, Batch Gradient Norm after: 12.185501092951077
Epoch 1235/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.6126953721046448
Epoch: 1235, Batch Gradient Norm: 13.787204502122242
Epoch: 1235, Batch Gradient Norm after: 13.787204502122242
Epoch 1236/10000, Prediction Accuracy = 59.864%, Loss = 0.6259018063545227
Epoch: 1236, Batch Gradient Norm: 11.956042081495436
Epoch: 1236, Batch Gradient Norm after: 11.956042081495436
Epoch 1237/10000, Prediction Accuracy = 59.806%, Loss = 0.6109169244766235
Epoch: 1237, Batch Gradient Norm: 12.239602482417327
Epoch: 1237, Batch Gradient Norm after: 12.239602482417327
Epoch 1238/10000, Prediction Accuracy = 59.864%, Loss = 0.6135277152061462
Epoch: 1238, Batch Gradient Norm: 11.66378018575718
Epoch: 1238, Batch Gradient Norm after: 11.66378018575718
Epoch 1239/10000, Prediction Accuracy = 59.830000000000005%, Loss = 0.6094889640808105
Epoch: 1239, Batch Gradient Norm: 12.84505763531745
Epoch: 1239, Batch Gradient Norm after: 12.84505763531745
Epoch 1240/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.6309190988540649
Epoch: 1240, Batch Gradient Norm: 7.570936158345547
Epoch: 1240, Batch Gradient Norm after: 7.570936158345547
Epoch 1241/10000, Prediction Accuracy = 59.848%, Loss = 0.5904802083969116
Epoch: 1241, Batch Gradient Norm: 8.894665348476
Epoch: 1241, Batch Gradient Norm after: 8.894665348476
Epoch 1242/10000, Prediction Accuracy = 59.924%, Loss = 0.5965370416641236
Epoch: 1242, Batch Gradient Norm: 13.48917659530775
Epoch: 1242, Batch Gradient Norm after: 13.48917659530775
Epoch 1243/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.6250192999839783
Epoch: 1243, Batch Gradient Norm: 13.710462662919932
Epoch: 1243, Batch Gradient Norm after: 13.710462662919932
Epoch 1244/10000, Prediction Accuracy = 59.77%, Loss = 0.6329092741012573
Epoch: 1244, Batch Gradient Norm: 10.446858415980763
Epoch: 1244, Batch Gradient Norm after: 10.446858415980763
Epoch 1245/10000, Prediction Accuracy = 59.79200000000001%, Loss = 0.6035773396492005
Epoch: 1245, Batch Gradient Norm: 11.368831518567982
Epoch: 1245, Batch Gradient Norm after: 11.368831518567982
Epoch 1246/10000, Prediction Accuracy = 59.83399999999999%, Loss = 0.6118340849876404
Epoch: 1246, Batch Gradient Norm: 12.69990576739414
Epoch: 1246, Batch Gradient Norm after: 12.69990576739414
Epoch 1247/10000, Prediction Accuracy = 59.678%, Loss = 0.6200562238693237
Epoch: 1247, Batch Gradient Norm: 11.834790173154342
Epoch: 1247, Batch Gradient Norm after: 11.834790173154342
Epoch 1248/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.6113832116127014
Epoch: 1248, Batch Gradient Norm: 12.725346655508408
Epoch: 1248, Batch Gradient Norm after: 12.725346655508408
Epoch 1249/10000, Prediction Accuracy = 59.806000000000004%, Loss = 0.6202845811843872
Epoch: 1249, Batch Gradient Norm: 13.98460929636217
Epoch: 1249, Batch Gradient Norm after: 13.98460929636217
Epoch 1250/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6299286723136902
Epoch: 1250, Batch Gradient Norm: 10.597304435601222
Epoch: 1250, Batch Gradient Norm after: 10.597304435601222
Epoch 1251/10000, Prediction Accuracy = 59.81%, Loss = 0.6110714912414551
Epoch: 1251, Batch Gradient Norm: 9.122940116895395
Epoch: 1251, Batch Gradient Norm after: 9.122940116895395
Epoch 1252/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.5968465805053711
Epoch: 1252, Batch Gradient Norm: 9.177932468503219
Epoch: 1252, Batch Gradient Norm after: 9.177932468503219
Epoch 1253/10000, Prediction Accuracy = 59.827999999999996%, Loss = 0.597535514831543
Epoch: 1253, Batch Gradient Norm: 10.133979579756259
Epoch: 1253, Batch Gradient Norm after: 10.133979579756259
Epoch 1254/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.5995155334472656
Epoch: 1254, Batch Gradient Norm: 11.171510813519069
Epoch: 1254, Batch Gradient Norm after: 11.171510813519069
Epoch 1255/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.6077100396156311
Epoch: 1255, Batch Gradient Norm: 14.190810903470055
Epoch: 1255, Batch Gradient Norm after: 14.190810903470055
Epoch 1256/10000, Prediction Accuracy = 59.866%, Loss = 0.6305154919624328
Epoch: 1256, Batch Gradient Norm: 12.064686854136697
Epoch: 1256, Batch Gradient Norm after: 12.064686854136697
Epoch 1257/10000, Prediction Accuracy = 59.908%, Loss = 0.6123392462730408
Epoch: 1257, Batch Gradient Norm: 14.656526051943896
Epoch: 1257, Batch Gradient Norm after: 14.656526051943896
Epoch 1258/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.6288089156150818
Epoch: 1258, Batch Gradient Norm: 12.563877714225933
Epoch: 1258, Batch Gradient Norm after: 12.563877714225933
Epoch 1259/10000, Prediction Accuracy = 59.872%, Loss = 0.6165435671806335
Epoch: 1259, Batch Gradient Norm: 11.030395249555793
Epoch: 1259, Batch Gradient Norm after: 11.030395249555793
Epoch 1260/10000, Prediction Accuracy = 59.92%, Loss = 0.6082581043243408
Epoch: 1260, Batch Gradient Norm: 9.545521648089116
Epoch: 1260, Batch Gradient Norm after: 9.545521648089116
Epoch 1261/10000, Prediction Accuracy = 59.926%, Loss = 0.5993050575256348
Epoch: 1261, Batch Gradient Norm: 13.941417254405806
Epoch: 1261, Batch Gradient Norm after: 13.941417254405806
Epoch 1262/10000, Prediction Accuracy = 59.838%, Loss = 0.6270833611488342
Epoch: 1262, Batch Gradient Norm: 11.513853552574998
Epoch: 1262, Batch Gradient Norm after: 11.513853552574998
Epoch 1263/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.6141674280166626
Epoch: 1263, Batch Gradient Norm: 9.593363576115264
Epoch: 1263, Batch Gradient Norm after: 9.593363576115264
Epoch 1264/10000, Prediction Accuracy = 59.83599999999999%, Loss = 0.5973905920982361
Epoch: 1264, Batch Gradient Norm: 12.795347467014414
Epoch: 1264, Batch Gradient Norm after: 12.795347467014414
Epoch 1265/10000, Prediction Accuracy = 59.884%, Loss = 0.61488276720047
Epoch: 1265, Batch Gradient Norm: 9.798156609834354
Epoch: 1265, Batch Gradient Norm after: 9.798156609834354
Epoch 1266/10000, Prediction Accuracy = 59.85600000000001%, Loss = 0.6018019676208496
Epoch: 1266, Batch Gradient Norm: 9.07219861518445
Epoch: 1266, Batch Gradient Norm after: 9.07219861518445
Epoch 1267/10000, Prediction Accuracy = 59.83399999999999%, Loss = 0.5933974385261536
Epoch: 1267, Batch Gradient Norm: 12.13228341559514
Epoch: 1267, Batch Gradient Norm after: 12.13228341559514
Epoch 1268/10000, Prediction Accuracy = 59.806%, Loss = 0.612766432762146
Epoch: 1268, Batch Gradient Norm: 18.216405828759125
Epoch: 1268, Batch Gradient Norm after: 18.216405828759125
Epoch 1269/10000, Prediction Accuracy = 59.84400000000001%, Loss = 0.6653302788734436
Epoch: 1269, Batch Gradient Norm: 13.185287566068263
Epoch: 1269, Batch Gradient Norm after: 13.185287566068263
Epoch 1270/10000, Prediction Accuracy = 59.826%, Loss = 0.6190202116966248
Epoch: 1270, Batch Gradient Norm: 11.782229656092522
Epoch: 1270, Batch Gradient Norm after: 11.782229656092522
Epoch 1271/10000, Prediction Accuracy = 59.80999999999999%, Loss = 0.6146624207496643
Epoch: 1271, Batch Gradient Norm: 11.596960546063137
Epoch: 1271, Batch Gradient Norm after: 11.596960546063137
Epoch 1272/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.6101102828979492
Epoch: 1272, Batch Gradient Norm: 10.071121974050387
Epoch: 1272, Batch Gradient Norm after: 10.071121974050387
Epoch 1273/10000, Prediction Accuracy = 59.826%, Loss = 0.6012351393699646
Epoch: 1273, Batch Gradient Norm: 8.992627888295132
Epoch: 1273, Batch Gradient Norm after: 8.992627888295132
Epoch 1274/10000, Prediction Accuracy = 59.862%, Loss = 0.5995723009109497
Epoch: 1274, Batch Gradient Norm: 7.810478093136346
Epoch: 1274, Batch Gradient Norm after: 7.810478093136346
Epoch 1275/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.5864974975585937
Epoch: 1275, Batch Gradient Norm: 10.337829689644053
Epoch: 1275, Batch Gradient Norm after: 10.337829689644053
Epoch 1276/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.597309684753418
Epoch: 1276, Batch Gradient Norm: 13.711082868094527
Epoch: 1276, Batch Gradient Norm after: 13.711082868094527
Epoch 1277/10000, Prediction Accuracy = 59.774%, Loss = 0.6175004243850708
Epoch: 1277, Batch Gradient Norm: 15.189333424491316
Epoch: 1277, Batch Gradient Norm after: 15.189333424491316
Epoch 1278/10000, Prediction Accuracy = 59.83%, Loss = 0.6333051323890686
Epoch: 1278, Batch Gradient Norm: 12.014715898341734
Epoch: 1278, Batch Gradient Norm after: 12.014715898341734
Epoch 1279/10000, Prediction Accuracy = 59.88199999999999%, Loss = 0.6086579918861389
Epoch: 1279, Batch Gradient Norm: 11.318729873263994
Epoch: 1279, Batch Gradient Norm after: 11.318729873263994
Epoch 1280/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6066945552825928
Epoch: 1280, Batch Gradient Norm: 10.494835184926526
Epoch: 1280, Batch Gradient Norm after: 10.494835184926526
Epoch 1281/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.605797016620636
Epoch: 1281, Batch Gradient Norm: 7.432482239393881
Epoch: 1281, Batch Gradient Norm after: 7.432482239393881
Epoch 1282/10000, Prediction Accuracy = 59.874%, Loss = 0.5883090853691101
Epoch: 1282, Batch Gradient Norm: 9.30456021449526
Epoch: 1282, Batch Gradient Norm after: 9.30456021449526
Epoch 1283/10000, Prediction Accuracy = 59.848%, Loss = 0.5923842430114746
Epoch: 1283, Batch Gradient Norm: 14.849412692540419
Epoch: 1283, Batch Gradient Norm after: 14.849412692540419
Epoch 1284/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.6307906627655029
Epoch: 1284, Batch Gradient Norm: 15.303960069321498
Epoch: 1284, Batch Gradient Norm after: 15.303960069321498
Epoch 1285/10000, Prediction Accuracy = 59.898%, Loss = 0.6346257567405701
Epoch: 1285, Batch Gradient Norm: 13.274336838423443
Epoch: 1285, Batch Gradient Norm after: 13.274336838423443
Epoch 1286/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.6206930875778198
Epoch: 1286, Batch Gradient Norm: 10.526006792927525
Epoch: 1286, Batch Gradient Norm after: 10.526006792927525
Epoch 1287/10000, Prediction Accuracy = 59.88399999999999%, Loss = 0.6014280080795288
Epoch: 1287, Batch Gradient Norm: 9.737195313240013
Epoch: 1287, Batch Gradient Norm after: 9.737195313240013
Epoch 1288/10000, Prediction Accuracy = 59.862%, Loss = 0.5977463364601135
Epoch: 1288, Batch Gradient Norm: 10.383348485296448
Epoch: 1288, Batch Gradient Norm after: 10.383348485296448
Epoch 1289/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.6032086730003356
Epoch: 1289, Batch Gradient Norm: 12.595500465222212
Epoch: 1289, Batch Gradient Norm after: 12.595500465222212
Epoch 1290/10000, Prediction Accuracy = 59.907999999999994%, Loss = 0.6195342063903808
Epoch: 1290, Batch Gradient Norm: 12.240160228066232
Epoch: 1290, Batch Gradient Norm after: 12.240160228066232
Epoch 1291/10000, Prediction Accuracy = 59.912%, Loss = 0.6125451326370239
Epoch: 1291, Batch Gradient Norm: 11.613505862491715
Epoch: 1291, Batch Gradient Norm after: 11.613505862491715
Epoch 1292/10000, Prediction Accuracy = 59.86%, Loss = 0.6119261860847474
Epoch: 1292, Batch Gradient Norm: 10.215761130397222
Epoch: 1292, Batch Gradient Norm after: 10.215761130397222
Epoch 1293/10000, Prediction Accuracy = 59.94%, Loss = 0.5961201190948486
Epoch: 1293, Batch Gradient Norm: 10.442049789464312
Epoch: 1293, Batch Gradient Norm after: 10.442049789464312
Epoch 1294/10000, Prediction Accuracy = 59.826%, Loss = 0.5999791264533997
Epoch: 1294, Batch Gradient Norm: 10.901079543476916
Epoch: 1294, Batch Gradient Norm after: 10.901079543476916
Epoch 1295/10000, Prediction Accuracy = 59.9%, Loss = 0.5991692781448364
Epoch: 1295, Batch Gradient Norm: 13.597045666611331
Epoch: 1295, Batch Gradient Norm after: 13.597045666611331
Epoch 1296/10000, Prediction Accuracy = 59.806%, Loss = 0.6239811539649963
Epoch: 1296, Batch Gradient Norm: 13.191753344380015
Epoch: 1296, Batch Gradient Norm after: 13.191753344380015
Epoch 1297/10000, Prediction Accuracy = 59.848%, Loss = 0.6183223247528076
Epoch: 1297, Batch Gradient Norm: 12.755790679292632
Epoch: 1297, Batch Gradient Norm after: 12.755790679292632
Epoch 1298/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.6194128036499024
Epoch: 1298, Batch Gradient Norm: 12.25911041177909
Epoch: 1298, Batch Gradient Norm after: 12.25911041177909
Epoch 1299/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.6134547829627991
Epoch: 1299, Batch Gradient Norm: 11.652799759798507
Epoch: 1299, Batch Gradient Norm after: 11.652799759798507
Epoch 1300/10000, Prediction Accuracy = 59.912%, Loss = 0.6083051085472106
Epoch: 1300, Batch Gradient Norm: 13.882353483559587
Epoch: 1300, Batch Gradient Norm after: 13.882353483559587
Epoch 1301/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.620625102519989
Epoch: 1301, Batch Gradient Norm: 12.644696838250361
Epoch: 1301, Batch Gradient Norm after: 12.644696838250361
Epoch 1302/10000, Prediction Accuracy = 59.888%, Loss = 0.6102677464485169
Epoch: 1302, Batch Gradient Norm: 10.723793067924905
Epoch: 1302, Batch Gradient Norm after: 10.723793067924905
Epoch 1303/10000, Prediction Accuracy = 59.836%, Loss = 0.599635910987854
Epoch: 1303, Batch Gradient Norm: 9.427755672167196
Epoch: 1303, Batch Gradient Norm after: 9.427755672167196
Epoch 1304/10000, Prediction Accuracy = 59.866%, Loss = 0.5920289397239685
Epoch: 1304, Batch Gradient Norm: 10.213991350450211
Epoch: 1304, Batch Gradient Norm after: 10.213991350450211
Epoch 1305/10000, Prediction Accuracy = 59.886%, Loss = 0.594576632976532
Epoch: 1305, Batch Gradient Norm: 16.659239427112624
Epoch: 1305, Batch Gradient Norm after: 16.659239427112624
Epoch 1306/10000, Prediction Accuracy = 59.86600000000001%, Loss = 0.6463565945625305
Epoch: 1306, Batch Gradient Norm: 12.439719884533206
Epoch: 1306, Batch Gradient Norm after: 12.439719884533206
Epoch 1307/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.609160840511322
Epoch: 1307, Batch Gradient Norm: 10.763345733229805
Epoch: 1307, Batch Gradient Norm after: 10.763345733229805
Epoch 1308/10000, Prediction Accuracy = 59.924%, Loss = 0.5999534249305725
Epoch: 1308, Batch Gradient Norm: 10.245046308265701
Epoch: 1308, Batch Gradient Norm after: 10.245046308265701
Epoch 1309/10000, Prediction Accuracy = 59.92%, Loss = 0.5969823479652405
Epoch: 1309, Batch Gradient Norm: 13.994481300285765
Epoch: 1309, Batch Gradient Norm after: 13.994481300285765
Epoch 1310/10000, Prediction Accuracy = 59.774%, Loss = 0.6223840355873108
Epoch: 1310, Batch Gradient Norm: 11.32891910752181
Epoch: 1310, Batch Gradient Norm after: 11.32891910752181
Epoch 1311/10000, Prediction Accuracy = 59.924%, Loss = 0.6031758904457092
Epoch: 1311, Batch Gradient Norm: 10.85065458899872
Epoch: 1311, Batch Gradient Norm after: 10.85065458899872
Epoch 1312/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.6015435457229614
Epoch: 1312, Batch Gradient Norm: 11.44114005244352
Epoch: 1312, Batch Gradient Norm after: 11.44114005244352
Epoch 1313/10000, Prediction Accuracy = 59.938%, Loss = 0.6044595599174499
Epoch: 1313, Batch Gradient Norm: 11.533413149854479
Epoch: 1313, Batch Gradient Norm after: 11.533413149854479
Epoch 1314/10000, Prediction Accuracy = 59.864%, Loss = 0.6048398017883301
Epoch: 1314, Batch Gradient Norm: 13.639714767460555
Epoch: 1314, Batch Gradient Norm after: 13.639714767460555
Epoch 1315/10000, Prediction Accuracy = 59.838%, Loss = 0.6331878304481506
Epoch: 1315, Batch Gradient Norm: 9.14410117803925
Epoch: 1315, Batch Gradient Norm after: 9.14410117803925
Epoch 1316/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.5904815316200256
Epoch: 1316, Batch Gradient Norm: 10.062009232820966
Epoch: 1316, Batch Gradient Norm after: 10.062009232820966
Epoch 1317/10000, Prediction Accuracy = 59.84000000000001%, Loss = 0.602577292919159
Epoch: 1317, Batch Gradient Norm: 9.937828310917656
Epoch: 1317, Batch Gradient Norm after: 9.937828310917656
Epoch 1318/10000, Prediction Accuracy = 59.89000000000001%, Loss = 0.5958667278289795
Epoch: 1318, Batch Gradient Norm: 14.335700857365438
Epoch: 1318, Batch Gradient Norm after: 14.335700857365438
Epoch 1319/10000, Prediction Accuracy = 59.864%, Loss = 0.6335512399673462
Epoch: 1319, Batch Gradient Norm: 12.106686564027473
Epoch: 1319, Batch Gradient Norm after: 12.106686564027473
Epoch 1320/10000, Prediction Accuracy = 59.898%, Loss = 0.6145583033561707
Epoch: 1320, Batch Gradient Norm: 10.555424260893796
Epoch: 1320, Batch Gradient Norm after: 10.555424260893796
Epoch 1321/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.6017802238464356
Epoch: 1321, Batch Gradient Norm: 10.799441611286449
Epoch: 1321, Batch Gradient Norm after: 10.799441611286449
Epoch 1322/10000, Prediction Accuracy = 59.938%, Loss = 0.6047693848609924
Epoch: 1322, Batch Gradient Norm: 10.885925284549916
Epoch: 1322, Batch Gradient Norm after: 10.885925284549916
Epoch 1323/10000, Prediction Accuracy = 59.98%, Loss = 0.5987179756164551
Epoch: 1323, Batch Gradient Norm: 12.848527719722819
Epoch: 1323, Batch Gradient Norm after: 12.848527719722819
Epoch 1324/10000, Prediction Accuracy = 59.89399999999999%, Loss = 0.6121941328048706
Epoch: 1324, Batch Gradient Norm: 10.143386653378379
Epoch: 1324, Batch Gradient Norm after: 10.143386653378379
Epoch 1325/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.5946458458900452
Epoch: 1325, Batch Gradient Norm: 11.716572084576052
Epoch: 1325, Batch Gradient Norm after: 11.716572084576052
Epoch 1326/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6051739454269409
Epoch: 1326, Batch Gradient Norm: 9.45878597319697
Epoch: 1326, Batch Gradient Norm after: 9.45878597319697
Epoch 1327/10000, Prediction Accuracy = 59.938%, Loss = 0.5920606017112732
Epoch: 1327, Batch Gradient Norm: 14.795769499454398
Epoch: 1327, Batch Gradient Norm after: 14.795769499454398
Epoch 1328/10000, Prediction Accuracy = 59.74400000000001%, Loss = 0.6277701497077942
Epoch: 1328, Batch Gradient Norm: 13.894329026676905
Epoch: 1328, Batch Gradient Norm after: 13.894329026676905
Epoch 1329/10000, Prediction Accuracy = 59.972%, Loss = 0.6223976016044617
Epoch: 1329, Batch Gradient Norm: 12.676728501878848
Epoch: 1329, Batch Gradient Norm after: 12.676728501878848
Epoch 1330/10000, Prediction Accuracy = 59.948%, Loss = 0.6143834471702576
Epoch: 1330, Batch Gradient Norm: 12.219105193263832
Epoch: 1330, Batch Gradient Norm after: 12.219105193263832
Epoch 1331/10000, Prediction Accuracy = 59.914%, Loss = 0.6056895136833191
Epoch: 1331, Batch Gradient Norm: 10.893994489524667
Epoch: 1331, Batch Gradient Norm after: 10.893994489524667
Epoch 1332/10000, Prediction Accuracy = 59.952%, Loss = 0.5985129475593567
Epoch: 1332, Batch Gradient Norm: 9.431999133405762
Epoch: 1332, Batch Gradient Norm after: 9.431999133405762
Epoch 1333/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.5913928270339965
Epoch: 1333, Batch Gradient Norm: 11.76479771716787
Epoch: 1333, Batch Gradient Norm after: 11.76479771716787
Epoch 1334/10000, Prediction Accuracy = 59.986000000000004%, Loss = 0.6046637296676636
Epoch: 1334, Batch Gradient Norm: 14.432385873357434
Epoch: 1334, Batch Gradient Norm after: 14.432385873357434
Epoch 1335/10000, Prediction Accuracy = 59.924%, Loss = 0.6282420635223389
Epoch: 1335, Batch Gradient Norm: 12.149242209113547
Epoch: 1335, Batch Gradient Norm after: 12.149242209113547
Epoch 1336/10000, Prediction Accuracy = 59.88599999999999%, Loss = 0.6073883533477783
Epoch: 1336, Batch Gradient Norm: 9.514595437575773
Epoch: 1336, Batch Gradient Norm after: 9.514595437575773
Epoch 1337/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.5879784464836121
Epoch: 1337, Batch Gradient Norm: 9.471549277669112
Epoch: 1337, Batch Gradient Norm after: 9.471549277669112
Epoch 1338/10000, Prediction Accuracy = 59.872%, Loss = 0.5948803424835205
Epoch: 1338, Batch Gradient Norm: 13.163152180270922
Epoch: 1338, Batch Gradient Norm after: 13.163152180270922
Epoch 1339/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.6124321699142456
Epoch: 1339, Batch Gradient Norm: 11.768630939704595
Epoch: 1339, Batch Gradient Norm after: 11.768630939704595
Epoch 1340/10000, Prediction Accuracy = 59.831999999999994%, Loss = 0.610837996006012
Epoch: 1340, Batch Gradient Norm: 10.920749891839273
Epoch: 1340, Batch Gradient Norm after: 10.920749891839273
Epoch 1341/10000, Prediction Accuracy = 59.94199999999999%, Loss = 0.6030576825141907
Epoch: 1341, Batch Gradient Norm: 11.874684074634049
Epoch: 1341, Batch Gradient Norm after: 11.874684074634049
Epoch 1342/10000, Prediction Accuracy = 60.068000000000005%, Loss = 0.6054303884506226
Epoch: 1342, Batch Gradient Norm: 11.306373870926759
Epoch: 1342, Batch Gradient Norm after: 11.306373870926759
Epoch 1343/10000, Prediction Accuracy = 59.998000000000005%, Loss = 0.6007792830467225
Epoch: 1343, Batch Gradient Norm: 12.76779339104068
Epoch: 1343, Batch Gradient Norm after: 12.76779339104068
Epoch 1344/10000, Prediction Accuracy = 59.89%, Loss = 0.6085304141044616
Epoch: 1344, Batch Gradient Norm: 12.719350476210792
Epoch: 1344, Batch Gradient Norm after: 12.719350476210792
Epoch 1345/10000, Prediction Accuracy = 59.972%, Loss = 0.6104904532432556
Epoch: 1345, Batch Gradient Norm: 10.960558990861438
Epoch: 1345, Batch Gradient Norm after: 10.960558990861438
Epoch 1346/10000, Prediction Accuracy = 59.95%, Loss = 0.5991899132728576
Epoch: 1346, Batch Gradient Norm: 9.574542567343698
Epoch: 1346, Batch Gradient Norm after: 9.574542567343698
Epoch 1347/10000, Prediction Accuracy = 59.955999999999996%, Loss = 0.5921942472457886
Epoch: 1347, Batch Gradient Norm: 12.043472884947393
Epoch: 1347, Batch Gradient Norm after: 12.043472884947393
Epoch 1348/10000, Prediction Accuracy = 59.93000000000001%, Loss = 0.6044224500656128
Epoch: 1348, Batch Gradient Norm: 10.715077119482798
Epoch: 1348, Batch Gradient Norm after: 10.715077119482798
Epoch 1349/10000, Prediction Accuracy = 59.943999999999996%, Loss = 0.5967367887496948
Epoch: 1349, Batch Gradient Norm: 12.63126219824991
Epoch: 1349, Batch Gradient Norm after: 12.63126219824991
Epoch 1350/10000, Prediction Accuracy = 59.85%, Loss = 0.6103044629096985
Epoch: 1350, Batch Gradient Norm: 11.637845187456447
Epoch: 1350, Batch Gradient Norm after: 11.637845187456447
Epoch 1351/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.6016476392745972
Epoch: 1351, Batch Gradient Norm: 9.538809726713827
Epoch: 1351, Batch Gradient Norm after: 9.538809726713827
Epoch 1352/10000, Prediction Accuracy = 59.924%, Loss = 0.590889286994934
Epoch: 1352, Batch Gradient Norm: 8.799666502362342
Epoch: 1352, Batch Gradient Norm after: 8.799666502362342
Epoch 1353/10000, Prediction Accuracy = 59.924%, Loss = 0.5871256470680237
Epoch: 1353, Batch Gradient Norm: 10.019468437195075
Epoch: 1353, Batch Gradient Norm after: 10.019468437195075
Epoch 1354/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.5884404659271241
Epoch: 1354, Batch Gradient Norm: 13.610777789288848
Epoch: 1354, Batch Gradient Norm after: 13.610777789288848
Epoch 1355/10000, Prediction Accuracy = 59.962%, Loss = 0.6126408457756043
Epoch: 1355, Batch Gradient Norm: 13.427399712535014
Epoch: 1355, Batch Gradient Norm after: 13.427399712535014
Epoch 1356/10000, Prediction Accuracy = 60.072%, Loss = 0.611335837841034
Epoch: 1356, Batch Gradient Norm: 12.803930272147904
Epoch: 1356, Batch Gradient Norm after: 12.803930272147904
Epoch 1357/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.6084261059761047
Epoch: 1357, Batch Gradient Norm: 14.811164404419113
Epoch: 1357, Batch Gradient Norm after: 14.811164404419113
Epoch 1358/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.6276049375534057
Epoch: 1358, Batch Gradient Norm: 14.021123850974146
Epoch: 1358, Batch Gradient Norm after: 14.021123850974146
Epoch 1359/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.6283685803413391
Epoch: 1359, Batch Gradient Norm: 10.316276622188955
Epoch: 1359, Batch Gradient Norm after: 10.316276622188955
Epoch 1360/10000, Prediction Accuracy = 60.122%, Loss = 0.594272255897522
Epoch: 1360, Batch Gradient Norm: 11.990847414960717
Epoch: 1360, Batch Gradient Norm after: 11.990847414960717
Epoch 1361/10000, Prediction Accuracy = 60.092%, Loss = 0.6026794672012329
Epoch: 1361, Batch Gradient Norm: 10.24944385508439
Epoch: 1361, Batch Gradient Norm after: 10.24944385508439
Epoch 1362/10000, Prediction Accuracy = 59.964%, Loss = 0.593025553226471
Epoch: 1362, Batch Gradient Norm: 11.166098299758923
Epoch: 1362, Batch Gradient Norm after: 11.166098299758923
Epoch 1363/10000, Prediction Accuracy = 59.891999999999996%, Loss = 0.5953333497047424
Epoch: 1363, Batch Gradient Norm: 11.844556195646891
Epoch: 1363, Batch Gradient Norm after: 11.844556195646891
Epoch 1364/10000, Prediction Accuracy = 60.01800000000001%, Loss = 0.6072459816932678
Epoch: 1364, Batch Gradient Norm: 11.191363682691671
Epoch: 1364, Batch Gradient Norm after: 11.191363682691671
Epoch 1365/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.5967668294906616
Epoch: 1365, Batch Gradient Norm: 9.521130443596467
Epoch: 1365, Batch Gradient Norm after: 9.521130443596467
Epoch 1366/10000, Prediction Accuracy = 60.062%, Loss = 0.5852631568908692
Epoch: 1366, Batch Gradient Norm: 8.900607725517455
Epoch: 1366, Batch Gradient Norm after: 8.900607725517455
Epoch 1367/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.5817722797393798
Epoch: 1367, Batch Gradient Norm: 12.326708591896375
Epoch: 1367, Batch Gradient Norm after: 12.326708591896375
Epoch 1368/10000, Prediction Accuracy = 59.862%, Loss = 0.606262743473053
Epoch: 1368, Batch Gradient Norm: 12.074394575867641
Epoch: 1368, Batch Gradient Norm after: 12.074394575867641
Epoch 1369/10000, Prediction Accuracy = 60.126%, Loss = 0.6025045990943909
Epoch: 1369, Batch Gradient Norm: 10.39270838149409
Epoch: 1369, Batch Gradient Norm after: 10.39270838149409
Epoch 1370/10000, Prediction Accuracy = 59.976%, Loss = 0.59437255859375
Epoch: 1370, Batch Gradient Norm: 11.59675040825385
Epoch: 1370, Batch Gradient Norm after: 11.59675040825385
Epoch 1371/10000, Prediction Accuracy = 59.956%, Loss = 0.5981781125068665
Epoch: 1371, Batch Gradient Norm: 16.623987301798614
Epoch: 1371, Batch Gradient Norm after: 16.623987301798614
Epoch 1372/10000, Prediction Accuracy = 60.012%, Loss = 0.6444379806518554
Epoch: 1372, Batch Gradient Norm: 12.17895820823619
Epoch: 1372, Batch Gradient Norm after: 12.17895820823619
Epoch 1373/10000, Prediction Accuracy = 60.02%, Loss = 0.6046406984329223
Epoch: 1373, Batch Gradient Norm: 11.390810413706065
Epoch: 1373, Batch Gradient Norm after: 11.390810413706065
Epoch 1374/10000, Prediction Accuracy = 59.878%, Loss = 0.6056630134582519
Epoch: 1374, Batch Gradient Norm: 12.080904818407982
Epoch: 1374, Batch Gradient Norm after: 12.080904818407982
Epoch 1375/10000, Prediction Accuracy = 60.104%, Loss = 0.59963937997818
Epoch: 1375, Batch Gradient Norm: 11.006139051042972
Epoch: 1375, Batch Gradient Norm after: 11.006139051042972
Epoch 1376/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.5989575862884522
Epoch: 1376, Batch Gradient Norm: 9.572844315834379
Epoch: 1376, Batch Gradient Norm after: 9.572844315834379
Epoch 1377/10000, Prediction Accuracy = 60.040000000000006%, Loss = 0.5859472393989563
Epoch: 1377, Batch Gradient Norm: 13.451100409813025
Epoch: 1377, Batch Gradient Norm after: 13.451100409813025
Epoch 1378/10000, Prediction Accuracy = 59.94200000000001%, Loss = 0.6125473022460938
Epoch: 1378, Batch Gradient Norm: 13.85166420856302
Epoch: 1378, Batch Gradient Norm after: 13.85166420856302
Epoch 1379/10000, Prediction Accuracy = 60.056%, Loss = 0.617931604385376
Epoch: 1379, Batch Gradient Norm: 12.383764485433122
Epoch: 1379, Batch Gradient Norm after: 12.383764485433122
Epoch 1380/10000, Prediction Accuracy = 60.11%, Loss = 0.6038221955299378
Epoch: 1380, Batch Gradient Norm: 12.849954829453496
Epoch: 1380, Batch Gradient Norm after: 12.849954829453496
Epoch 1381/10000, Prediction Accuracy = 59.988000000000014%, Loss = 0.6124545693397522
Epoch: 1381, Batch Gradient Norm: 11.73385642040746
Epoch: 1381, Batch Gradient Norm after: 11.73385642040746
Epoch 1382/10000, Prediction Accuracy = 59.952%, Loss = 0.5984963536262512
Epoch: 1382, Batch Gradient Norm: 10.566680345734612
Epoch: 1382, Batch Gradient Norm after: 10.566680345734612
Epoch 1383/10000, Prediction Accuracy = 59.972%, Loss = 0.5943469762802124
Epoch: 1383, Batch Gradient Norm: 10.472763176640111
Epoch: 1383, Batch Gradient Norm after: 10.472763176640111
Epoch 1384/10000, Prediction Accuracy = 60.102%, Loss = 0.5898916244506835
Epoch: 1384, Batch Gradient Norm: 14.889870277654788
Epoch: 1384, Batch Gradient Norm after: 14.889870277654788
Epoch 1385/10000, Prediction Accuracy = 60.028%, Loss = 0.6261744141578675
Epoch: 1385, Batch Gradient Norm: 10.804334567641769
Epoch: 1385, Batch Gradient Norm after: 10.804334567641769
Epoch 1386/10000, Prediction Accuracy = 59.987999999999985%, Loss = 0.5915031433105469
Epoch: 1386, Batch Gradient Norm: 11.745592765757207
Epoch: 1386, Batch Gradient Norm after: 11.745592765757207
Epoch 1387/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.5985960245132447
Epoch: 1387, Batch Gradient Norm: 9.993098246789051
Epoch: 1387, Batch Gradient Norm after: 9.993098246789051
Epoch 1388/10000, Prediction Accuracy = 60.072%, Loss = 0.5898180723190307
Epoch: 1388, Batch Gradient Norm: 11.068087676494207
Epoch: 1388, Batch Gradient Norm after: 11.068087676494207
Epoch 1389/10000, Prediction Accuracy = 59.988%, Loss = 0.594849693775177
Epoch: 1389, Batch Gradient Norm: 12.482251568014293
Epoch: 1389, Batch Gradient Norm after: 12.482251568014293
Epoch 1390/10000, Prediction Accuracy = 60.008%, Loss = 0.603375232219696
Epoch: 1390, Batch Gradient Norm: 11.360430733216884
Epoch: 1390, Batch Gradient Norm after: 11.360430733216884
Epoch 1391/10000, Prediction Accuracy = 59.934000000000005%, Loss = 0.6000818371772766
Epoch: 1391, Batch Gradient Norm: 9.722232446197754
Epoch: 1391, Batch Gradient Norm after: 9.722232446197754
Epoch 1392/10000, Prediction Accuracy = 60.124%, Loss = 0.5852221012115478
Epoch: 1392, Batch Gradient Norm: 10.610554476103015
Epoch: 1392, Batch Gradient Norm after: 10.610554476103015
Epoch 1393/10000, Prediction Accuracy = 60.11%, Loss = 0.5913604378700257
Epoch: 1393, Batch Gradient Norm: 9.862064080497085
Epoch: 1393, Batch Gradient Norm after: 9.862064080497085
Epoch 1394/10000, Prediction Accuracy = 60.092%, Loss = 0.587411892414093
Epoch: 1394, Batch Gradient Norm: 13.141367765022547
Epoch: 1394, Batch Gradient Norm after: 13.141367765022547
Epoch 1395/10000, Prediction Accuracy = 60.124%, Loss = 0.606758964061737
Epoch: 1395, Batch Gradient Norm: 10.801230115133341
Epoch: 1395, Batch Gradient Norm after: 10.801230115133341
Epoch 1396/10000, Prediction Accuracy = 59.916%, Loss = 0.5929546594619751
Epoch: 1396, Batch Gradient Norm: 9.764672417166807
Epoch: 1396, Batch Gradient Norm after: 9.764672417166807
Epoch 1397/10000, Prediction Accuracy = 60.036%, Loss = 0.5845577359199524
Epoch: 1397, Batch Gradient Norm: 14.552387159435945
Epoch: 1397, Batch Gradient Norm after: 14.552387159435945
Epoch 1398/10000, Prediction Accuracy = 60.004%, Loss = 0.6216554045677185
Epoch: 1398, Batch Gradient Norm: 12.297060309601987
Epoch: 1398, Batch Gradient Norm after: 12.297060309601987
Epoch 1399/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.6027098774909974
Epoch: 1399, Batch Gradient Norm: 8.412114063083946
Epoch: 1399, Batch Gradient Norm after: 8.412114063083946
Epoch 1400/10000, Prediction Accuracy = 60.128%, Loss = 0.5785568833351136
Epoch: 1400, Batch Gradient Norm: 12.726508677659295
Epoch: 1400, Batch Gradient Norm after: 12.726508677659295
Epoch 1401/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.605288851261139
Epoch: 1401, Batch Gradient Norm: 12.38139072311286
Epoch: 1401, Batch Gradient Norm after: 12.38139072311286
Epoch 1402/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.6055660128593445
Epoch: 1402, Batch Gradient Norm: 14.59478861326095
Epoch: 1402, Batch Gradient Norm after: 14.59478861326095
Epoch 1403/10000, Prediction Accuracy = 60.008%, Loss = 0.6202616572380066
Epoch: 1403, Batch Gradient Norm: 10.672589650280498
Epoch: 1403, Batch Gradient Norm after: 10.672589650280498
Epoch 1404/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.5914272427558899
Epoch: 1404, Batch Gradient Norm: 11.281298157293278
Epoch: 1404, Batch Gradient Norm after: 11.281298157293278
Epoch 1405/10000, Prediction Accuracy = 60.084%, Loss = 0.5947859168052674
Epoch: 1405, Batch Gradient Norm: 12.889380192833276
Epoch: 1405, Batch Gradient Norm after: 12.889380192833276
Epoch 1406/10000, Prediction Accuracy = 60.112%, Loss = 0.6053146600723267
Epoch: 1406, Batch Gradient Norm: 13.831865530271571
Epoch: 1406, Batch Gradient Norm after: 13.831865530271571
Epoch 1407/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.6137688040733338
Epoch: 1407, Batch Gradient Norm: 12.671384435838018
Epoch: 1407, Batch Gradient Norm after: 12.671384435838018
Epoch 1408/10000, Prediction Accuracy = 60.041999999999994%, Loss = 0.6014661312103271
Epoch: 1408, Batch Gradient Norm: 8.964895254196868
Epoch: 1408, Batch Gradient Norm after: 8.964895254196868
Epoch 1409/10000, Prediction Accuracy = 60.048%, Loss = 0.5799864649772644
Epoch: 1409, Batch Gradient Norm: 13.105156477372457
Epoch: 1409, Batch Gradient Norm after: 13.105156477372457
Epoch 1410/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.6032151818275452
Epoch: 1410, Batch Gradient Norm: 10.498442667402204
Epoch: 1410, Batch Gradient Norm after: 10.498442667402204
Epoch 1411/10000, Prediction Accuracy = 60.098%, Loss = 0.5867157816886902
Epoch: 1411, Batch Gradient Norm: 7.51743940736417
Epoch: 1411, Batch Gradient Norm after: 7.51743940736417
Epoch 1412/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.5729352831840515
Epoch: 1412, Batch Gradient Norm: 10.956068121647075
Epoch: 1412, Batch Gradient Norm after: 10.956068121647075
Epoch 1413/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.5907754063606262
Epoch: 1413, Batch Gradient Norm: 10.794862205659351
Epoch: 1413, Batch Gradient Norm after: 10.794862205659351
Epoch 1414/10000, Prediction Accuracy = 60.044%, Loss = 0.5907542109489441
Epoch: 1414, Batch Gradient Norm: 9.310803256369619
Epoch: 1414, Batch Gradient Norm after: 9.310803256369619
Epoch 1415/10000, Prediction Accuracy = 60.072%, Loss = 0.5807639241218567
Epoch: 1415, Batch Gradient Norm: 14.047228426099704
Epoch: 1415, Batch Gradient Norm after: 14.047228426099704
Epoch 1416/10000, Prediction Accuracy = 60.072%, Loss = 0.6107217073440552
Epoch: 1416, Batch Gradient Norm: 14.562419903793835
Epoch: 1416, Batch Gradient Norm after: 14.562419903793835
Epoch 1417/10000, Prediction Accuracy = 60.062%, Loss = 0.6189289808273315
Epoch: 1417, Batch Gradient Norm: 10.691754128235761
Epoch: 1417, Batch Gradient Norm after: 10.691754128235761
Epoch 1418/10000, Prediction Accuracy = 60.156000000000006%, Loss = 0.5906570076942443
Epoch: 1418, Batch Gradient Norm: 9.317763769271634
Epoch: 1418, Batch Gradient Norm after: 9.317763769271634
Epoch 1419/10000, Prediction Accuracy = 60.14%, Loss = 0.5897956848144531
Epoch: 1419, Batch Gradient Norm: 12.468665661187165
Epoch: 1419, Batch Gradient Norm after: 12.468665661187165
Epoch 1420/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.6037164807319642
Epoch: 1420, Batch Gradient Norm: 12.5570996273364
Epoch: 1420, Batch Gradient Norm after: 12.5570996273364
Epoch 1421/10000, Prediction Accuracy = 60.05%, Loss = 0.6019519448280335
Epoch: 1421, Batch Gradient Norm: 9.94712011097623
Epoch: 1421, Batch Gradient Norm after: 9.94712011097623
Epoch 1422/10000, Prediction Accuracy = 60.11600000000001%, Loss = 0.5859721183776856
Epoch: 1422, Batch Gradient Norm: 9.44873840867691
Epoch: 1422, Batch Gradient Norm after: 9.44873840867691
Epoch 1423/10000, Prediction Accuracy = 60.08%, Loss = 0.5815922379493713
Epoch: 1423, Batch Gradient Norm: 13.303556358404329
Epoch: 1423, Batch Gradient Norm after: 13.303556358404329
Epoch 1424/10000, Prediction Accuracy = 59.888%, Loss = 0.6078871250152588
Epoch: 1424, Batch Gradient Norm: 13.558391219109222
Epoch: 1424, Batch Gradient Norm after: 13.558391219109222
Epoch 1425/10000, Prediction Accuracy = 60.008%, Loss = 0.6099685192108154
Epoch: 1425, Batch Gradient Norm: 11.752511240450517
Epoch: 1425, Batch Gradient Norm after: 11.752511240450517
Epoch 1426/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.5964650630950927
Epoch: 1426, Batch Gradient Norm: 12.743746828235032
Epoch: 1426, Batch Gradient Norm after: 12.743746828235032
Epoch 1427/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.6069347977638244
Epoch: 1427, Batch Gradient Norm: 10.500281896008623
Epoch: 1427, Batch Gradient Norm after: 10.500281896008623
Epoch 1428/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.5862058758735657
Epoch: 1428, Batch Gradient Norm: 10.126876264591226
Epoch: 1428, Batch Gradient Norm after: 10.126876264591226
Epoch 1429/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.5868135929107666
Epoch: 1429, Batch Gradient Norm: 11.679577828314022
Epoch: 1429, Batch Gradient Norm after: 11.679577828314022
Epoch 1430/10000, Prediction Accuracy = 60.03599999999999%, Loss = 0.5932912111282349
Epoch: 1430, Batch Gradient Norm: 13.207071961872149
Epoch: 1430, Batch Gradient Norm after: 13.207071961872149
Epoch 1431/10000, Prediction Accuracy = 60.072%, Loss = 0.611542284488678
Epoch: 1431, Batch Gradient Norm: 8.316282503752499
Epoch: 1431, Batch Gradient Norm after: 8.316282503752499
Epoch 1432/10000, Prediction Accuracy = 60.086%, Loss = 0.5777459621429444
Epoch: 1432, Batch Gradient Norm: 9.940814675055039
Epoch: 1432, Batch Gradient Norm after: 9.940814675055039
Epoch 1433/10000, Prediction Accuracy = 60.076%, Loss = 0.5833507299423217
Epoch: 1433, Batch Gradient Norm: 18.356152184142296
Epoch: 1433, Batch Gradient Norm after: 18.356152184142296
Epoch 1434/10000, Prediction Accuracy = 59.948%, Loss = 0.653796398639679
Epoch: 1434, Batch Gradient Norm: 11.075632419751308
Epoch: 1434, Batch Gradient Norm after: 11.075632419751308
Epoch 1435/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.5939767599105835
Epoch: 1435, Batch Gradient Norm: 9.375471984600146
Epoch: 1435, Batch Gradient Norm after: 9.375471984600146
Epoch 1436/10000, Prediction Accuracy = 60.128%, Loss = 0.5863534927368164
Epoch: 1436, Batch Gradient Norm: 11.536893917660228
Epoch: 1436, Batch Gradient Norm after: 11.536893917660228
Epoch 1437/10000, Prediction Accuracy = 60.044%, Loss = 0.6027648210525512
Epoch: 1437, Batch Gradient Norm: 12.903476400388035
Epoch: 1437, Batch Gradient Norm after: 12.903476400388035
Epoch 1438/10000, Prediction Accuracy = 59.974000000000004%, Loss = 0.6110398888587951
Epoch: 1438, Batch Gradient Norm: 12.786220653402307
Epoch: 1438, Batch Gradient Norm after: 12.786220653402307
Epoch 1439/10000, Prediction Accuracy = 60.178%, Loss = 0.6098355770111084
Epoch: 1439, Batch Gradient Norm: 11.775948298117987
Epoch: 1439, Batch Gradient Norm after: 11.775948298117987
Epoch 1440/10000, Prediction Accuracy = 60.14%, Loss = 0.5992099523544312
Epoch: 1440, Batch Gradient Norm: 11.133456241064213
Epoch: 1440, Batch Gradient Norm after: 11.133456241064213
Epoch 1441/10000, Prediction Accuracy = 60.102%, Loss = 0.5897712945938111
Epoch: 1441, Batch Gradient Norm: 9.094918821449692
Epoch: 1441, Batch Gradient Norm after: 9.094918821449692
Epoch 1442/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.5747462630271911
Epoch: 1442, Batch Gradient Norm: 14.598075186526914
Epoch: 1442, Batch Gradient Norm after: 14.598075186526914
Epoch 1443/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.6118908762931824
Epoch: 1443, Batch Gradient Norm: 11.504968385766604
Epoch: 1443, Batch Gradient Norm after: 11.504968385766604
Epoch 1444/10000, Prediction Accuracy = 60.16600000000001%, Loss = 0.5915769457817077
Epoch: 1444, Batch Gradient Norm: 11.85451017798739
Epoch: 1444, Batch Gradient Norm after: 11.85451017798739
Epoch 1445/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.5934261441230774
Epoch: 1445, Batch Gradient Norm: 10.07526646900765
Epoch: 1445, Batch Gradient Norm after: 10.07526646900765
Epoch 1446/10000, Prediction Accuracy = 60.221999999999994%, Loss = 0.5852444052696228
Epoch: 1446, Batch Gradient Norm: 10.940500375358823
Epoch: 1446, Batch Gradient Norm after: 10.940500375358823
Epoch 1447/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5910578966140747
Epoch: 1447, Batch Gradient Norm: 9.95165649708384
Epoch: 1447, Batch Gradient Norm after: 9.95165649708384
Epoch 1448/10000, Prediction Accuracy = 60.038%, Loss = 0.5823907852172852
Epoch: 1448, Batch Gradient Norm: 10.109230901812197
Epoch: 1448, Batch Gradient Norm after: 10.109230901812197
Epoch 1449/10000, Prediction Accuracy = 60.186%, Loss = 0.5874943137168884
Epoch: 1449, Batch Gradient Norm: 11.862471444566914
Epoch: 1449, Batch Gradient Norm after: 11.862471444566914
Epoch 1450/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.5952347397804261
Epoch: 1450, Batch Gradient Norm: 11.147676913895703
Epoch: 1450, Batch Gradient Norm after: 11.147676913895703
Epoch 1451/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.5895371675491333
Epoch: 1451, Batch Gradient Norm: 10.595895984702269
Epoch: 1451, Batch Gradient Norm after: 10.595895984702269
Epoch 1452/10000, Prediction Accuracy = 60.03000000000001%, Loss = 0.5856464624404907
Epoch: 1452, Batch Gradient Norm: 13.312678362641334
Epoch: 1452, Batch Gradient Norm after: 13.312678362641334
Epoch 1453/10000, Prediction Accuracy = 60.008%, Loss = 0.606921648979187
Epoch: 1453, Batch Gradient Norm: 15.539434165955418
Epoch: 1453, Batch Gradient Norm after: 15.539434165955418
Epoch 1454/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.6256114602088928
Epoch: 1454, Batch Gradient Norm: 9.300262061069079
Epoch: 1454, Batch Gradient Norm after: 9.300262061069079
Epoch 1455/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.5779176115989685
Epoch: 1455, Batch Gradient Norm: 11.942687357404814
Epoch: 1455, Batch Gradient Norm after: 11.942687357404814
Epoch 1456/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.5976485848426819
Epoch: 1456, Batch Gradient Norm: 11.732003726501429
Epoch: 1456, Batch Gradient Norm after: 11.732003726501429
Epoch 1457/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.5913701534271241
Epoch: 1457, Batch Gradient Norm: 10.979550390746729
Epoch: 1457, Batch Gradient Norm after: 10.979550390746729
Epoch 1458/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.5896228432655335
Epoch: 1458, Batch Gradient Norm: 11.198920516490736
Epoch: 1458, Batch Gradient Norm after: 11.198920516490736
Epoch 1459/10000, Prediction Accuracy = 60.176%, Loss = 0.5904712677001953
Epoch: 1459, Batch Gradient Norm: 12.136793794386017
Epoch: 1459, Batch Gradient Norm after: 12.136793794386017
Epoch 1460/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.5933970451354981
Epoch: 1460, Batch Gradient Norm: 15.08153864903933
Epoch: 1460, Batch Gradient Norm after: 15.08153864903933
Epoch 1461/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.6142788410186768
Epoch: 1461, Batch Gradient Norm: 11.361609356127262
Epoch: 1461, Batch Gradient Norm after: 11.361609356127262
Epoch 1462/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5898728847503663
Epoch: 1462, Batch Gradient Norm: 7.107313399777258
Epoch: 1462, Batch Gradient Norm after: 7.107313399777258
Epoch 1463/10000, Prediction Accuracy = 60.096000000000004%, Loss = 0.5662178516387939
Epoch: 1463, Batch Gradient Norm: 8.094796472184086
Epoch: 1463, Batch Gradient Norm after: 8.094796472184086
Epoch 1464/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5761159420013428
Epoch: 1464, Batch Gradient Norm: 8.331275722538273
Epoch: 1464, Batch Gradient Norm after: 8.331275722538273
Epoch 1465/10000, Prediction Accuracy = 60.102%, Loss = 0.5742008090019226
Epoch: 1465, Batch Gradient Norm: 12.559862099110557
Epoch: 1465, Batch Gradient Norm after: 12.559862099110557
Epoch 1466/10000, Prediction Accuracy = 60.052%, Loss = 0.5968998789787292
Epoch: 1466, Batch Gradient Norm: 16.287407214110615
Epoch: 1466, Batch Gradient Norm after: 16.287407214110615
Epoch 1467/10000, Prediction Accuracy = 60.05%, Loss = 0.6351255178451538
Epoch: 1467, Batch Gradient Norm: 12.738268731837511
Epoch: 1467, Batch Gradient Norm after: 12.738268731837511
Epoch 1468/10000, Prediction Accuracy = 60.116%, Loss = 0.6005272626876831
Epoch: 1468, Batch Gradient Norm: 11.486634385924024
Epoch: 1468, Batch Gradient Norm after: 11.486634385924024
Epoch 1469/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.5957343459129334
Epoch: 1469, Batch Gradient Norm: 10.368593944302111
Epoch: 1469, Batch Gradient Norm after: 10.368593944302111
Epoch 1470/10000, Prediction Accuracy = 60.221999999999994%, Loss = 0.5870210886001587
Epoch: 1470, Batch Gradient Norm: 9.81492401640392
Epoch: 1470, Batch Gradient Norm after: 9.81492401640392
Epoch 1471/10000, Prediction Accuracy = 60.076%, Loss = 0.5834652066230774
Epoch: 1471, Batch Gradient Norm: 9.420127730356917
Epoch: 1471, Batch Gradient Norm after: 9.420127730356917
Epoch 1472/10000, Prediction Accuracy = 60.15599999999999%, Loss = 0.5782614111900329
Epoch: 1472, Batch Gradient Norm: 10.311273030559773
Epoch: 1472, Batch Gradient Norm after: 10.311273030559773
Epoch 1473/10000, Prediction Accuracy = 60.134%, Loss = 0.5820684909820557
Epoch: 1473, Batch Gradient Norm: 14.711855945488946
Epoch: 1473, Batch Gradient Norm after: 14.711855945488946
Epoch 1474/10000, Prediction Accuracy = 60.266%, Loss = 0.6152406454086303
Epoch: 1474, Batch Gradient Norm: 10.365070070975136
Epoch: 1474, Batch Gradient Norm after: 10.365070070975136
Epoch 1475/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.5869951605796814
Epoch: 1475, Batch Gradient Norm: 13.38477558643462
Epoch: 1475, Batch Gradient Norm after: 13.38477558643462
Epoch 1476/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.604083251953125
Epoch: 1476, Batch Gradient Norm: 14.848668747579307
Epoch: 1476, Batch Gradient Norm after: 14.848668747579307
Epoch 1477/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.6143692851066589
Epoch: 1477, Batch Gradient Norm: 10.838056599202337
Epoch: 1477, Batch Gradient Norm after: 10.838056599202337
Epoch 1478/10000, Prediction Accuracy = 60.138%, Loss = 0.5899343252182007
Epoch: 1478, Batch Gradient Norm: 12.842744436178993
Epoch: 1478, Batch Gradient Norm after: 12.842744436178993
Epoch 1479/10000, Prediction Accuracy = 60.114%, Loss = 0.6016992092132568
Epoch: 1479, Batch Gradient Norm: 10.35625637973014
Epoch: 1479, Batch Gradient Norm after: 10.35625637973014
Epoch 1480/10000, Prediction Accuracy = 60.117999999999995%, Loss = 0.5806455016136169
Epoch: 1480, Batch Gradient Norm: 10.149783080423944
Epoch: 1480, Batch Gradient Norm after: 10.149783080423944
Epoch 1481/10000, Prediction Accuracy = 60.081999999999994%, Loss = 0.5815412282943726
Epoch: 1481, Batch Gradient Norm: 10.674858223400149
Epoch: 1481, Batch Gradient Norm after: 10.674858223400149
Epoch 1482/10000, Prediction Accuracy = 60.088%, Loss = 0.5850656390190124
Epoch: 1482, Batch Gradient Norm: 12.55739080147338
Epoch: 1482, Batch Gradient Norm after: 12.55739080147338
Epoch 1483/10000, Prediction Accuracy = 60.134%, Loss = 0.5965275883674621
Epoch: 1483, Batch Gradient Norm: 9.79872954938699
Epoch: 1483, Batch Gradient Norm after: 9.79872954938699
Epoch 1484/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.5830269455909729
Epoch: 1484, Batch Gradient Norm: 9.017537432501511
Epoch: 1484, Batch Gradient Norm after: 9.017537432501511
Epoch 1485/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.5762440085411071
Epoch: 1485, Batch Gradient Norm: 13.593595515801299
Epoch: 1485, Batch Gradient Norm after: 13.593595515801299
Epoch 1486/10000, Prediction Accuracy = 60.176%, Loss = 0.6060262084007263
Epoch: 1486, Batch Gradient Norm: 13.608933515010154
Epoch: 1486, Batch Gradient Norm after: 13.608933515010154
Epoch 1487/10000, Prediction Accuracy = 60.092%, Loss = 0.6040043950080871
Epoch: 1487, Batch Gradient Norm: 11.909693797675631
Epoch: 1487, Batch Gradient Norm after: 11.909693797675631
Epoch 1488/10000, Prediction Accuracy = 60.15%, Loss = 0.5902843594551086
Epoch: 1488, Batch Gradient Norm: 14.389251247909943
Epoch: 1488, Batch Gradient Norm after: 14.389251247909943
Epoch 1489/10000, Prediction Accuracy = 60.17800000000001%, Loss = 0.6091359376907348
Epoch: 1489, Batch Gradient Norm: 13.09414160103985
Epoch: 1489, Batch Gradient Norm after: 13.09414160103985
Epoch 1490/10000, Prediction Accuracy = 60.12600000000001%, Loss = 0.5959503650665283
Epoch: 1490, Batch Gradient Norm: 10.352154457465446
Epoch: 1490, Batch Gradient Norm after: 10.352154457465446
Epoch 1491/10000, Prediction Accuracy = 60.166%, Loss = 0.57912837266922
Epoch: 1491, Batch Gradient Norm: 11.747042053572027
Epoch: 1491, Batch Gradient Norm after: 11.747042053572027
Epoch 1492/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.5899100303649902
Epoch: 1492, Batch Gradient Norm: 10.198426629571907
Epoch: 1492, Batch Gradient Norm after: 10.198426629571907
Epoch 1493/10000, Prediction Accuracy = 59.99399999999999%, Loss = 0.5810966730117798
Epoch: 1493, Batch Gradient Norm: 9.546011916915521
Epoch: 1493, Batch Gradient Norm after: 9.546011916915521
Epoch 1494/10000, Prediction Accuracy = 60.122%, Loss = 0.5765618443489074
Epoch: 1494, Batch Gradient Norm: 8.617625685479242
Epoch: 1494, Batch Gradient Norm after: 8.617625685479242
Epoch 1495/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.5711640238761901
Epoch: 1495, Batch Gradient Norm: 9.71422699721158
Epoch: 1495, Batch Gradient Norm after: 9.71422699721158
Epoch 1496/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.5760369777679444
Epoch: 1496, Batch Gradient Norm: 11.345166464574525
Epoch: 1496, Batch Gradient Norm after: 11.345166464574525
Epoch 1497/10000, Prediction Accuracy = 60.162%, Loss = 0.5881873488426208
Epoch: 1497, Batch Gradient Norm: 11.787178954234225
Epoch: 1497, Batch Gradient Norm after: 11.787178954234225
Epoch 1498/10000, Prediction Accuracy = 60.112%, Loss = 0.6006438732147217
Epoch: 1498, Batch Gradient Norm: 11.245960494050205
Epoch: 1498, Batch Gradient Norm after: 11.245960494050205
Epoch 1499/10000, Prediction Accuracy = 60.136%, Loss = 0.587548017501831
Epoch: 1499, Batch Gradient Norm: 13.347997349778653
Epoch: 1499, Batch Gradient Norm after: 13.347997349778653
Epoch 1500/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.6014195442199707
Epoch: 1500, Batch Gradient Norm: 13.776184376975745
Epoch: 1500, Batch Gradient Norm after: 13.776184376975745
Epoch 1501/10000, Prediction Accuracy = 60.198%, Loss = 0.6016517996788024
Epoch: 1501, Batch Gradient Norm: 11.144606202410987
Epoch: 1501, Batch Gradient Norm after: 11.144606202410987
Epoch 1502/10000, Prediction Accuracy = 60.157999999999994%, Loss = 0.584035861492157
Epoch: 1502, Batch Gradient Norm: 10.320109240930607
Epoch: 1502, Batch Gradient Norm after: 10.320109240930607
Epoch 1503/10000, Prediction Accuracy = 60.112%, Loss = 0.5811363697052002
Epoch: 1503, Batch Gradient Norm: 12.706831861402675
Epoch: 1503, Batch Gradient Norm after: 12.706831861402675
Epoch 1504/10000, Prediction Accuracy = 60.124%, Loss = 0.6006551384925842
Epoch: 1504, Batch Gradient Norm: 12.175847778811962
Epoch: 1504, Batch Gradient Norm after: 12.175847778811962
Epoch 1505/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.5925666928291321
Epoch: 1505, Batch Gradient Norm: 8.094396523338192
Epoch: 1505, Batch Gradient Norm after: 8.094396523338192
Epoch 1506/10000, Prediction Accuracy = 60.158%, Loss = 0.5690761089324952
Epoch: 1506, Batch Gradient Norm: 7.838461528400007
Epoch: 1506, Batch Gradient Norm after: 7.838461528400007
Epoch 1507/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.568828010559082
Epoch: 1507, Batch Gradient Norm: 10.886986176077421
Epoch: 1507, Batch Gradient Norm after: 10.886986176077421
Epoch 1508/10000, Prediction Accuracy = 60.176%, Loss = 0.5824780583381652
Epoch: 1508, Batch Gradient Norm: 16.82798246119984
Epoch: 1508, Batch Gradient Norm after: 16.82798246119984
Epoch 1509/10000, Prediction Accuracy = 60.074%, Loss = 0.6316359639167786
Epoch: 1509, Batch Gradient Norm: 12.66985782650028
Epoch: 1509, Batch Gradient Norm after: 12.66985782650028
Epoch 1510/10000, Prediction Accuracy = 60.288%, Loss = 0.6005056619644165
Epoch: 1510, Batch Gradient Norm: 8.317686144013631
Epoch: 1510, Batch Gradient Norm after: 8.317686144013631
Epoch 1511/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.5711477994918823
Epoch: 1511, Batch Gradient Norm: 9.841648068461796
Epoch: 1511, Batch Gradient Norm after: 9.841648068461796
Epoch 1512/10000, Prediction Accuracy = 60.194%, Loss = 0.575965166091919
Epoch: 1512, Batch Gradient Norm: 13.582365530181
Epoch: 1512, Batch Gradient Norm after: 13.582365530181
Epoch 1513/10000, Prediction Accuracy = 60.222%, Loss = 0.6022413849830628
Epoch: 1513, Batch Gradient Norm: 14.081688654472984
Epoch: 1513, Batch Gradient Norm after: 14.081688654472984
Epoch 1514/10000, Prediction Accuracy = 60.126%, Loss = 0.6057523369789124
Epoch: 1514, Batch Gradient Norm: 13.134345686723567
Epoch: 1514, Batch Gradient Norm after: 13.134345686723567
Epoch 1515/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.5952246308326721
Epoch: 1515, Batch Gradient Norm: 12.902585010246844
Epoch: 1515, Batch Gradient Norm after: 12.902585010246844
Epoch 1516/10000, Prediction Accuracy = 60.176%, Loss = 0.5948246598243714
Epoch: 1516, Batch Gradient Norm: 11.959896315883386
Epoch: 1516, Batch Gradient Norm after: 11.959896315883386
Epoch 1517/10000, Prediction Accuracy = 60.246%, Loss = 0.5945007681846619
Epoch: 1517, Batch Gradient Norm: 8.199351315072946
Epoch: 1517, Batch Gradient Norm after: 8.199351315072946
Epoch 1518/10000, Prediction Accuracy = 60.19%, Loss = 0.5686121344566345
Epoch: 1518, Batch Gradient Norm: 13.190733975910018
Epoch: 1518, Batch Gradient Norm after: 13.190733975910018
Epoch 1519/10000, Prediction Accuracy = 60.21%, Loss = 0.6060021758079529
Epoch: 1519, Batch Gradient Norm: 12.677999201670312
Epoch: 1519, Batch Gradient Norm after: 12.677999201670312
Epoch 1520/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.5980615735054016
Epoch: 1520, Batch Gradient Norm: 9.562078424294297
Epoch: 1520, Batch Gradient Norm after: 9.562078424294297
Epoch 1521/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.5736947894096375
Epoch: 1521, Batch Gradient Norm: 9.72961694811424
Epoch: 1521, Batch Gradient Norm after: 9.72961694811424
Epoch 1522/10000, Prediction Accuracy = 60.202%, Loss = 0.5739226579666138
Epoch: 1522, Batch Gradient Norm: 8.574319820590928
Epoch: 1522, Batch Gradient Norm after: 8.574319820590928
Epoch 1523/10000, Prediction Accuracy = 60.14399999999999%, Loss = 0.5731056213378907
Epoch: 1523, Batch Gradient Norm: 7.841179800358433
Epoch: 1523, Batch Gradient Norm after: 7.841179800358433
Epoch 1524/10000, Prediction Accuracy = 60.144000000000005%, Loss = 0.563944399356842
Epoch: 1524, Batch Gradient Norm: 9.159324294915802
Epoch: 1524, Batch Gradient Norm after: 9.159324294915802
Epoch 1525/10000, Prediction Accuracy = 60.134%, Loss = 0.5728777647018433
Epoch: 1525, Batch Gradient Norm: 16.696331104437878
Epoch: 1525, Batch Gradient Norm after: 16.696331104437878
Epoch 1526/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.6302528023719788
Epoch: 1526, Batch Gradient Norm: 15.157036872464081
Epoch: 1526, Batch Gradient Norm after: 15.157036872464081
Epoch 1527/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.6203202366828918
Epoch: 1527, Batch Gradient Norm: 9.26061927120585
Epoch: 1527, Batch Gradient Norm after: 9.26061927120585
Epoch 1528/10000, Prediction Accuracy = 60.242000000000004%, Loss = 0.5724864602088928
Epoch: 1528, Batch Gradient Norm: 11.205997543813018
Epoch: 1528, Batch Gradient Norm after: 11.205997543813018
Epoch 1529/10000, Prediction Accuracy = 60.188%, Loss = 0.5859782457351684
Epoch: 1529, Batch Gradient Norm: 13.108386668524822
Epoch: 1529, Batch Gradient Norm after: 13.108386668524822
Epoch 1530/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.5988820314407348
Epoch: 1530, Batch Gradient Norm: 12.784405931812536
Epoch: 1530, Batch Gradient Norm after: 12.784405931812536
Epoch 1531/10000, Prediction Accuracy = 60.126%, Loss = 0.6028026103973388
Epoch: 1531, Batch Gradient Norm: 8.336710424381842
Epoch: 1531, Batch Gradient Norm after: 8.336710424381842
Epoch 1532/10000, Prediction Accuracy = 60.16199999999999%, Loss = 0.5662692785263062
Epoch: 1532, Batch Gradient Norm: 11.881527083015547
Epoch: 1532, Batch Gradient Norm after: 11.881527083015547
Epoch 1533/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.5887831807136535
Epoch: 1533, Batch Gradient Norm: 12.543458052317925
Epoch: 1533, Batch Gradient Norm after: 12.543458052317925
Epoch 1534/10000, Prediction Accuracy = 60.15%, Loss = 0.5950367689132691
Epoch: 1534, Batch Gradient Norm: 12.224766687618299
Epoch: 1534, Batch Gradient Norm after: 12.224766687618299
Epoch 1535/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.5958613991737366
Epoch: 1535, Batch Gradient Norm: 8.77715175867257
Epoch: 1535, Batch Gradient Norm after: 8.77715175867257
Epoch 1536/10000, Prediction Accuracy = 60.2%, Loss = 0.5701275110244751
Epoch: 1536, Batch Gradient Norm: 9.253821204829444
Epoch: 1536, Batch Gradient Norm after: 9.253821204829444
Epoch 1537/10000, Prediction Accuracy = 60.186%, Loss = 0.5700608491897583
Epoch: 1537, Batch Gradient Norm: 11.089101353016822
Epoch: 1537, Batch Gradient Norm after: 11.089101353016822
Epoch 1538/10000, Prediction Accuracy = 60.19%, Loss = 0.5858518481254578
Epoch: 1538, Batch Gradient Norm: 15.041062693943775
Epoch: 1538, Batch Gradient Norm after: 15.041062693943775
Epoch 1539/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6106958985328674
Epoch: 1539, Batch Gradient Norm: 12.768827757504166
Epoch: 1539, Batch Gradient Norm after: 12.768827757504166
Epoch 1540/10000, Prediction Accuracy = 60.21%, Loss = 0.5949259161949157
Epoch: 1540, Batch Gradient Norm: 7.99568164533539
Epoch: 1540, Batch Gradient Norm after: 7.99568164533539
Epoch 1541/10000, Prediction Accuracy = 60.19000000000001%, Loss = 0.5645243763923645
Epoch: 1541, Batch Gradient Norm: 11.582223436445075
Epoch: 1541, Batch Gradient Norm after: 11.582223436445075
Epoch 1542/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.585945987701416
Epoch: 1542, Batch Gradient Norm: 11.433976336465529
Epoch: 1542, Batch Gradient Norm after: 11.433976336465529
Epoch 1543/10000, Prediction Accuracy = 60.24399999999999%, Loss = 0.5828490376472473
Epoch: 1543, Batch Gradient Norm: 11.38455178542196
Epoch: 1543, Batch Gradient Norm after: 11.38455178542196
Epoch 1544/10000, Prediction Accuracy = 60.212%, Loss = 0.5855552792549134
Epoch: 1544, Batch Gradient Norm: 11.787132947856358
Epoch: 1544, Batch Gradient Norm after: 11.787132947856358
Epoch 1545/10000, Prediction Accuracy = 60.21%, Loss = 0.5870571494102478
Epoch: 1545, Batch Gradient Norm: 11.35630928227749
Epoch: 1545, Batch Gradient Norm after: 11.35630928227749
Epoch 1546/10000, Prediction Accuracy = 60.17199999999999%, Loss = 0.5852035999298095
Epoch: 1546, Batch Gradient Norm: 9.028699212091356
Epoch: 1546, Batch Gradient Norm after: 9.028699212091356
Epoch 1547/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.5693310856819153
Epoch: 1547, Batch Gradient Norm: 9.812834547334012
Epoch: 1547, Batch Gradient Norm after: 9.812834547334012
Epoch 1548/10000, Prediction Accuracy = 60.19200000000001%, Loss = 0.5742504477500916
Epoch: 1548, Batch Gradient Norm: 13.393481361776535
Epoch: 1548, Batch Gradient Norm after: 13.393481361776535
Epoch 1549/10000, Prediction Accuracy = 60.164%, Loss = 0.6069532990455627
Epoch: 1549, Batch Gradient Norm: 11.5887960769238
Epoch: 1549, Batch Gradient Norm after: 11.5887960769238
Epoch 1550/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.5932043194770813
Epoch: 1550, Batch Gradient Norm: 9.589064986809129
Epoch: 1550, Batch Gradient Norm after: 9.589064986809129
Epoch 1551/10000, Prediction Accuracy = 60.176%, Loss = 0.574151074886322
Epoch: 1551, Batch Gradient Norm: 13.179511868711868
Epoch: 1551, Batch Gradient Norm after: 13.179511868711868
Epoch 1552/10000, Prediction Accuracy = 60.25600000000001%, Loss = 0.6000471353530884
Epoch: 1552, Batch Gradient Norm: 13.750458369588934
Epoch: 1552, Batch Gradient Norm after: 13.750458369588934
Epoch 1553/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.5984205603599548
Epoch: 1553, Batch Gradient Norm: 14.986099071231326
Epoch: 1553, Batch Gradient Norm after: 14.986099071231326
Epoch 1554/10000, Prediction Accuracy = 60.370000000000005%, Loss = 0.6083627343177795
Epoch: 1554, Batch Gradient Norm: 12.619600346201144
Epoch: 1554, Batch Gradient Norm after: 12.619600346201144
Epoch 1555/10000, Prediction Accuracy = 60.254%, Loss = 0.5934473276138306
Epoch: 1555, Batch Gradient Norm: 11.184760370991828
Epoch: 1555, Batch Gradient Norm after: 11.184760370991828
Epoch 1556/10000, Prediction Accuracy = 60.21%, Loss = 0.5842530488967895
Epoch: 1556, Batch Gradient Norm: 9.521032962893834
Epoch: 1556, Batch Gradient Norm after: 9.521032962893834
Epoch 1557/10000, Prediction Accuracy = 60.209999999999994%, Loss = 0.5756237864494324
Epoch: 1557, Batch Gradient Norm: 11.312497397708038
Epoch: 1557, Batch Gradient Norm after: 11.312497397708038
Epoch 1558/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5816169381141663
Epoch: 1558, Batch Gradient Norm: 11.701619469962708
Epoch: 1558, Batch Gradient Norm after: 11.701619469962708
Epoch 1559/10000, Prediction Accuracy = 60.164%, Loss = 0.5886391401290894
Epoch: 1559, Batch Gradient Norm: 8.923083489859867
Epoch: 1559, Batch Gradient Norm after: 8.923083489859867
Epoch 1560/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.5701167345046997
Epoch: 1560, Batch Gradient Norm: 13.402569417075679
Epoch: 1560, Batch Gradient Norm after: 13.402569417075679
Epoch 1561/10000, Prediction Accuracy = 60.205999999999996%, Loss = 0.5948071718215943
Epoch: 1561, Batch Gradient Norm: 10.18264225059733
Epoch: 1561, Batch Gradient Norm after: 10.18264225059733
Epoch 1562/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.5726574182510376
Epoch: 1562, Batch Gradient Norm: 11.76387307000957
Epoch: 1562, Batch Gradient Norm after: 11.76387307000957
Epoch 1563/10000, Prediction Accuracy = 60.176%, Loss = 0.5897110819816589
Epoch: 1563, Batch Gradient Norm: 9.338719227361139
Epoch: 1563, Batch Gradient Norm after: 9.338719227361139
Epoch 1564/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5686708927154541
Epoch: 1564, Batch Gradient Norm: 12.128591104913447
Epoch: 1564, Batch Gradient Norm after: 12.128591104913447
Epoch 1565/10000, Prediction Accuracy = 60.395999999999994%, Loss = 0.5890952706336975
Epoch: 1565, Batch Gradient Norm: 13.546219723674024
Epoch: 1565, Batch Gradient Norm after: 13.546219723674024
Epoch 1566/10000, Prediction Accuracy = 60.33%, Loss = 0.5954115629196167
Epoch: 1566, Batch Gradient Norm: 12.244147563563004
Epoch: 1566, Batch Gradient Norm after: 12.244147563563004
Epoch 1567/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.588356614112854
Epoch: 1567, Batch Gradient Norm: 11.351862841604188
Epoch: 1567, Batch Gradient Norm after: 11.351862841604188
Epoch 1568/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.5819982886314392
Epoch: 1568, Batch Gradient Norm: 10.922250786205208
Epoch: 1568, Batch Gradient Norm after: 10.922250786205208
Epoch 1569/10000, Prediction Accuracy = 60.3%, Loss = 0.5809407591819763
Epoch: 1569, Batch Gradient Norm: 12.33020961884539
Epoch: 1569, Batch Gradient Norm after: 12.33020961884539
Epoch 1570/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.5920240879058838
Epoch: 1570, Batch Gradient Norm: 9.984283790451
Epoch: 1570, Batch Gradient Norm after: 9.984283790451
Epoch 1571/10000, Prediction Accuracy = 60.386%, Loss = 0.5738795161247253
Epoch: 1571, Batch Gradient Norm: 11.209572123141477
Epoch: 1571, Batch Gradient Norm after: 11.209572123141477
Epoch 1572/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.5822709918022155
Epoch: 1572, Batch Gradient Norm: 8.82150372588603
Epoch: 1572, Batch Gradient Norm after: 8.82150372588603
Epoch 1573/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.5651889443397522
Epoch: 1573, Batch Gradient Norm: 12.685913317032648
Epoch: 1573, Batch Gradient Norm after: 12.685913317032648
Epoch 1574/10000, Prediction Accuracy = 60.19%, Loss = 0.591624641418457
Epoch: 1574, Batch Gradient Norm: 13.47945649889809
Epoch: 1574, Batch Gradient Norm after: 13.47945649889809
Epoch 1575/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.6126531362533569
Epoch: 1575, Batch Gradient Norm: 10.144767613986417
Epoch: 1575, Batch Gradient Norm after: 10.144767613986417
Epoch 1576/10000, Prediction Accuracy = 60.234%, Loss = 0.5765925765037536
Epoch: 1576, Batch Gradient Norm: 9.813713206684795
Epoch: 1576, Batch Gradient Norm after: 9.813713206684795
Epoch 1577/10000, Prediction Accuracy = 60.3%, Loss = 0.5698858499526978
Epoch: 1577, Batch Gradient Norm: 6.58409283464852
Epoch: 1577, Batch Gradient Norm after: 6.58409283464852
Epoch 1578/10000, Prediction Accuracy = 60.254%, Loss = 0.5553340196609498
Epoch: 1578, Batch Gradient Norm: 9.895905608821009
Epoch: 1578, Batch Gradient Norm after: 9.895905608821009
Epoch 1579/10000, Prediction Accuracy = 60.314%, Loss = 0.5686416745185852
Epoch: 1579, Batch Gradient Norm: 13.693171880282948
Epoch: 1579, Batch Gradient Norm after: 13.693171880282948
Epoch 1580/10000, Prediction Accuracy = 60.15999999999999%, Loss = 0.5957288026809693
Epoch: 1580, Batch Gradient Norm: 15.166288826122234
Epoch: 1580, Batch Gradient Norm after: 15.166288826122234
Epoch 1581/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.6077841520309448
Epoch: 1581, Batch Gradient Norm: 12.205289191632033
Epoch: 1581, Batch Gradient Norm after: 12.205289191632033
Epoch 1582/10000, Prediction Accuracy = 60.388%, Loss = 0.5830503821372985
Epoch: 1582, Batch Gradient Norm: 12.28242329295034
Epoch: 1582, Batch Gradient Norm after: 12.28242329295034
Epoch 1583/10000, Prediction Accuracy = 60.394000000000005%, Loss = 0.5854393482208252
Epoch: 1583, Batch Gradient Norm: 11.239488391507969
Epoch: 1583, Batch Gradient Norm after: 11.239488391507969
Epoch 1584/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.5781387567520142
Epoch: 1584, Batch Gradient Norm: 12.538112132065304
Epoch: 1584, Batch Gradient Norm after: 12.538112132065304
Epoch 1585/10000, Prediction Accuracy = 60.284000000000006%, Loss = 0.593778109550476
Epoch: 1585, Batch Gradient Norm: 12.191441426504047
Epoch: 1585, Batch Gradient Norm after: 12.191441426504047
Epoch 1586/10000, Prediction Accuracy = 60.221999999999994%, Loss = 0.5906105279922486
Epoch: 1586, Batch Gradient Norm: 9.66285453921935
Epoch: 1586, Batch Gradient Norm after: 9.66285453921935
Epoch 1587/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.5735778927803039
Epoch: 1587, Batch Gradient Norm: 12.567334376062771
Epoch: 1587, Batch Gradient Norm after: 12.567334376062771
Epoch 1588/10000, Prediction Accuracy = 60.338%, Loss = 0.5931029200553894
Epoch: 1588, Batch Gradient Norm: 13.67323617084579
Epoch: 1588, Batch Gradient Norm after: 13.67323617084579
Epoch 1589/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.5988722681999207
Epoch: 1589, Batch Gradient Norm: 11.227645212816505
Epoch: 1589, Batch Gradient Norm after: 11.227645212816505
Epoch 1590/10000, Prediction Accuracy = 60.39%, Loss = 0.5816853523254395
Epoch: 1590, Batch Gradient Norm: 9.761942745990511
Epoch: 1590, Batch Gradient Norm after: 9.761942745990511
Epoch 1591/10000, Prediction Accuracy = 60.164%, Loss = 0.5764321327209473
Epoch: 1591, Batch Gradient Norm: 13.364504334962383
Epoch: 1591, Batch Gradient Norm after: 13.364504334962383
Epoch 1592/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.6081807494163514
Epoch: 1592, Batch Gradient Norm: 14.177677793646254
Epoch: 1592, Batch Gradient Norm after: 14.177677793646254
Epoch 1593/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5997087597846985
Epoch: 1593, Batch Gradient Norm: 10.54058808772089
Epoch: 1593, Batch Gradient Norm after: 10.54058808772089
Epoch 1594/10000, Prediction Accuracy = 60.378%, Loss = 0.5760810375213623
Epoch: 1594, Batch Gradient Norm: 9.076883010457362
Epoch: 1594, Batch Gradient Norm after: 9.076883010457362
Epoch 1595/10000, Prediction Accuracy = 60.362%, Loss = 0.5721929550170899
Epoch: 1595, Batch Gradient Norm: 10.165955772017101
Epoch: 1595, Batch Gradient Norm after: 10.165955772017101
Epoch 1596/10000, Prediction Accuracy = 60.217999999999996%, Loss = 0.5796513438224793
Epoch: 1596, Batch Gradient Norm: 13.194835858350485
Epoch: 1596, Batch Gradient Norm after: 13.194835858350485
Epoch 1597/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.5926210522651673
Epoch: 1597, Batch Gradient Norm: 12.414639975764116
Epoch: 1597, Batch Gradient Norm after: 12.414639975764116
Epoch 1598/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.5888095617294311
Epoch: 1598, Batch Gradient Norm: 9.011169888898635
Epoch: 1598, Batch Gradient Norm after: 9.011169888898635
Epoch 1599/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.5654224276542663
Epoch: 1599, Batch Gradient Norm: 10.623149989633296
Epoch: 1599, Batch Gradient Norm after: 10.623149989633296
Epoch 1600/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5756402134895324
Epoch: 1600, Batch Gradient Norm: 12.080366793489885
Epoch: 1600, Batch Gradient Norm after: 12.080366793489885
Epoch 1601/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.5871343016624451
Epoch: 1601, Batch Gradient Norm: 13.57311804489092
Epoch: 1601, Batch Gradient Norm after: 13.57311804489092
Epoch 1602/10000, Prediction Accuracy = 60.258%, Loss = 0.60167156457901
Epoch: 1602, Batch Gradient Norm: 9.65108291609423
Epoch: 1602, Batch Gradient Norm after: 9.65108291609423
Epoch 1603/10000, Prediction Accuracy = 60.314%, Loss = 0.569623339176178
Epoch: 1603, Batch Gradient Norm: 10.369503478052966
Epoch: 1603, Batch Gradient Norm after: 10.369503478052966
Epoch 1604/10000, Prediction Accuracy = 60.28599999999999%, Loss = 0.5728492498397827
Epoch: 1604, Batch Gradient Norm: 9.237908561956438
Epoch: 1604, Batch Gradient Norm after: 9.237908561956438
Epoch 1605/10000, Prediction Accuracy = 60.36%, Loss = 0.5684819459915161
Epoch: 1605, Batch Gradient Norm: 10.652713303147719
Epoch: 1605, Batch Gradient Norm after: 10.652713303147719
Epoch 1606/10000, Prediction Accuracy = 60.288%, Loss = 0.579393744468689
Epoch: 1606, Batch Gradient Norm: 11.200285538839006
Epoch: 1606, Batch Gradient Norm after: 11.200285538839006
Epoch 1607/10000, Prediction Accuracy = 60.27199999999999%, Loss = 0.5802001476287841
Epoch: 1607, Batch Gradient Norm: 13.386914080853836
Epoch: 1607, Batch Gradient Norm after: 13.386914080853836
Epoch 1608/10000, Prediction Accuracy = 60.44000000000001%, Loss = 0.5932344913482666
Epoch: 1608, Batch Gradient Norm: 13.135307202632413
Epoch: 1608, Batch Gradient Norm after: 13.135307202632413
Epoch 1609/10000, Prediction Accuracy = 60.348%, Loss = 0.5965353608131408
Epoch: 1609, Batch Gradient Norm: 10.90965695517361
Epoch: 1609, Batch Gradient Norm after: 10.90965695517361
Epoch 1610/10000, Prediction Accuracy = 60.352%, Loss = 0.572726047039032
Epoch: 1610, Batch Gradient Norm: 13.501865932146758
Epoch: 1610, Batch Gradient Norm after: 13.501865932146758
Epoch 1611/10000, Prediction Accuracy = 60.334%, Loss = 0.5927217125892639
Epoch: 1611, Batch Gradient Norm: 15.366984373994017
Epoch: 1611, Batch Gradient Norm after: 15.366984373994017
Epoch 1612/10000, Prediction Accuracy = 60.29%, Loss = 0.6124869346618652
Epoch: 1612, Batch Gradient Norm: 9.67357124900875
Epoch: 1612, Batch Gradient Norm after: 9.67357124900875
Epoch 1613/10000, Prediction Accuracy = 60.384%, Loss = 0.5708099961280823
Epoch: 1613, Batch Gradient Norm: 8.56542666383239
Epoch: 1613, Batch Gradient Norm after: 8.56542666383239
Epoch 1614/10000, Prediction Accuracy = 60.25599999999999%, Loss = 0.5620071887969971
Epoch: 1614, Batch Gradient Norm: 13.52520899789403
Epoch: 1614, Batch Gradient Norm after: 13.52520899789403
Epoch 1615/10000, Prediction Accuracy = 60.324%, Loss = 0.5965871930122375
Epoch: 1615, Batch Gradient Norm: 10.952560347438206
Epoch: 1615, Batch Gradient Norm after: 10.952560347438206
Epoch 1616/10000, Prediction Accuracy = 60.45%, Loss = 0.5777118086814881
Epoch: 1616, Batch Gradient Norm: 9.939835407875362
Epoch: 1616, Batch Gradient Norm after: 9.939835407875362
Epoch 1617/10000, Prediction Accuracy = 60.384%, Loss = 0.5724419593811035
Epoch: 1617, Batch Gradient Norm: 11.569702107314445
Epoch: 1617, Batch Gradient Norm after: 11.569702107314445
Epoch 1618/10000, Prediction Accuracy = 60.354000000000006%, Loss = 0.5858483791351319
Epoch: 1618, Batch Gradient Norm: 13.73680312786472
Epoch: 1618, Batch Gradient Norm after: 13.73680312786472
Epoch 1619/10000, Prediction Accuracy = 60.452%, Loss = 0.5949079632759094
Epoch: 1619, Batch Gradient Norm: 10.77463109796226
Epoch: 1619, Batch Gradient Norm after: 10.77463109796226
Epoch 1620/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.5740669727325439
Epoch: 1620, Batch Gradient Norm: 10.712105669155529
Epoch: 1620, Batch Gradient Norm after: 10.712105669155529
Epoch 1621/10000, Prediction Accuracy = 60.395999999999994%, Loss = 0.5736842751502991
Epoch: 1621, Batch Gradient Norm: 10.52991479414315
Epoch: 1621, Batch Gradient Norm after: 10.52991479414315
Epoch 1622/10000, Prediction Accuracy = 60.269999999999996%, Loss = 0.5736628532409668
Epoch: 1622, Batch Gradient Norm: 11.315082723270514
Epoch: 1622, Batch Gradient Norm after: 11.315082723270514
Epoch 1623/10000, Prediction Accuracy = 60.364%, Loss = 0.5819414496421814
Epoch: 1623, Batch Gradient Norm: 9.013510856647024
Epoch: 1623, Batch Gradient Norm after: 9.013510856647024
Epoch 1624/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.5640791773796081
Epoch: 1624, Batch Gradient Norm: 9.86138490065887
Epoch: 1624, Batch Gradient Norm after: 9.86138490065887
Epoch 1625/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.5717442750930786
Epoch: 1625, Batch Gradient Norm: 12.861651926028575
Epoch: 1625, Batch Gradient Norm after: 12.861651926028575
Epoch 1626/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.5877173185348511
Epoch: 1626, Batch Gradient Norm: 12.068495287969569
Epoch: 1626, Batch Gradient Norm after: 12.068495287969569
Epoch 1627/10000, Prediction Accuracy = 60.27%, Loss = 0.5824415206909179
Epoch: 1627, Batch Gradient Norm: 11.492466000508598
Epoch: 1627, Batch Gradient Norm after: 11.492466000508598
Epoch 1628/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.5804364085197449
Epoch: 1628, Batch Gradient Norm: 12.377437990514649
Epoch: 1628, Batch Gradient Norm after: 12.377437990514649
Epoch 1629/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.5883652567863464
Epoch: 1629, Batch Gradient Norm: 7.453747487208302
Epoch: 1629, Batch Gradient Norm after: 7.453747487208302
Epoch 1630/10000, Prediction Accuracy = 60.378%, Loss = 0.5578491926193238
Epoch: 1630, Batch Gradient Norm: 12.190719640251764
Epoch: 1630, Batch Gradient Norm after: 12.190719640251764
Epoch 1631/10000, Prediction Accuracy = 60.314%, Loss = 0.5828787088394165
Epoch: 1631, Batch Gradient Norm: 14.504699685592316
Epoch: 1631, Batch Gradient Norm after: 14.504699685592316
Epoch 1632/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.6023823857307434
Epoch: 1632, Batch Gradient Norm: 11.760153877204935
Epoch: 1632, Batch Gradient Norm after: 11.760153877204935
Epoch 1633/10000, Prediction Accuracy = 60.28399999999999%, Loss = 0.5854536294937134
Epoch: 1633, Batch Gradient Norm: 10.411355198537207
Epoch: 1633, Batch Gradient Norm after: 10.411355198537207
Epoch 1634/10000, Prediction Accuracy = 60.395999999999994%, Loss = 0.5748307704925537
Epoch: 1634, Batch Gradient Norm: 11.719818524428293
Epoch: 1634, Batch Gradient Norm after: 11.719818524428293
Epoch 1635/10000, Prediction Accuracy = 60.279999999999994%, Loss = 0.5947903156280517
Epoch: 1635, Batch Gradient Norm: 7.92400574465945
Epoch: 1635, Batch Gradient Norm after: 7.92400574465945
Epoch 1636/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5578001022338868
Epoch: 1636, Batch Gradient Norm: 9.857835113889065
Epoch: 1636, Batch Gradient Norm after: 9.857835113889065
Epoch 1637/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.5689780354499817
Epoch: 1637, Batch Gradient Norm: 11.819142834807568
Epoch: 1637, Batch Gradient Norm after: 11.819142834807568
Epoch 1638/10000, Prediction Accuracy = 60.29200000000001%, Loss = 0.5808985114097596
Epoch: 1638, Batch Gradient Norm: 10.343050267259446
Epoch: 1638, Batch Gradient Norm after: 10.343050267259446
Epoch 1639/10000, Prediction Accuracy = 60.424%, Loss = 0.5697819828987122
Epoch: 1639, Batch Gradient Norm: 9.485320533705845
Epoch: 1639, Batch Gradient Norm after: 9.485320533705845
Epoch 1640/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.564372432231903
Epoch: 1640, Batch Gradient Norm: 12.251918235727267
Epoch: 1640, Batch Gradient Norm after: 12.251918235727267
Epoch 1641/10000, Prediction Accuracy = 60.35%, Loss = 0.5835944652557373
Epoch: 1641, Batch Gradient Norm: 14.749636114931485
Epoch: 1641, Batch Gradient Norm after: 14.749636114931485
Epoch 1642/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.6008238315582275
Epoch: 1642, Batch Gradient Norm: 14.675564517904135
Epoch: 1642, Batch Gradient Norm after: 14.675564517904135
Epoch 1643/10000, Prediction Accuracy = 60.294000000000004%, Loss = 0.5986911058425903
Epoch: 1643, Batch Gradient Norm: 12.231601745475661
Epoch: 1643, Batch Gradient Norm after: 12.231601745475661
Epoch 1644/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.581922459602356
Epoch: 1644, Batch Gradient Norm: 12.019882398550648
Epoch: 1644, Batch Gradient Norm after: 12.019882398550648
Epoch 1645/10000, Prediction Accuracy = 60.315999999999995%, Loss = 0.5779079079627991
Epoch: 1645, Batch Gradient Norm: 13.17469122347069
Epoch: 1645, Batch Gradient Norm after: 13.17469122347069
Epoch 1646/10000, Prediction Accuracy = 60.44000000000001%, Loss = 0.5879090547561645
Epoch: 1646, Batch Gradient Norm: 12.241573522053601
Epoch: 1646, Batch Gradient Norm after: 12.241573522053601
Epoch 1647/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5920539617538452
Epoch: 1647, Batch Gradient Norm: 11.151508045692847
Epoch: 1647, Batch Gradient Norm after: 11.151508045692847
Epoch 1648/10000, Prediction Accuracy = 60.358000000000004%, Loss = 0.5746377348899842
Epoch: 1648, Batch Gradient Norm: 9.690727353287294
Epoch: 1648, Batch Gradient Norm after: 9.690727353287294
Epoch 1649/10000, Prediction Accuracy = 60.31%, Loss = 0.5657386064529419
Epoch: 1649, Batch Gradient Norm: 11.752570048695103
Epoch: 1649, Batch Gradient Norm after: 11.752570048695103
Epoch 1650/10000, Prediction Accuracy = 60.298%, Loss = 0.5798076391220093
Epoch: 1650, Batch Gradient Norm: 13.364204249377059
Epoch: 1650, Batch Gradient Norm after: 13.364204249377059
Epoch 1651/10000, Prediction Accuracy = 60.31%, Loss = 0.5981688022613525
Epoch: 1651, Batch Gradient Norm: 12.274172484374358
Epoch: 1651, Batch Gradient Norm after: 12.274172484374358
Epoch 1652/10000, Prediction Accuracy = 60.298%, Loss = 0.5903098583221436
Epoch: 1652, Batch Gradient Norm: 10.279197822007102
Epoch: 1652, Batch Gradient Norm after: 10.279197822007102
Epoch 1653/10000, Prediction Accuracy = 60.42199999999999%, Loss = 0.5739357948303223
Epoch: 1653, Batch Gradient Norm: 8.138931577999536
Epoch: 1653, Batch Gradient Norm after: 8.138931577999536
Epoch 1654/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5566779613494873
Epoch: 1654, Batch Gradient Norm: 10.548844925181145
Epoch: 1654, Batch Gradient Norm after: 10.548844925181145
Epoch 1655/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.5704250693321228
Epoch: 1655, Batch Gradient Norm: 12.558251234709115
Epoch: 1655, Batch Gradient Norm after: 12.558251234709115
Epoch 1656/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.5830864787101746
Epoch: 1656, Batch Gradient Norm: 9.505641860798512
Epoch: 1656, Batch Gradient Norm after: 9.505641860798512
Epoch 1657/10000, Prediction Accuracy = 60.33%, Loss = 0.5633655548095703
Epoch: 1657, Batch Gradient Norm: 8.678791611462612
Epoch: 1657, Batch Gradient Norm after: 8.678791611462612
Epoch 1658/10000, Prediction Accuracy = 60.44%, Loss = 0.5584607243537902
Epoch: 1658, Batch Gradient Norm: 9.890452287804852
Epoch: 1658, Batch Gradient Norm after: 9.890452287804852
Epoch 1659/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.5669221043586731
Epoch: 1659, Batch Gradient Norm: 13.679359758354062
Epoch: 1659, Batch Gradient Norm after: 13.679359758354062
Epoch 1660/10000, Prediction Accuracy = 60.412%, Loss = 0.5895022749900818
Epoch: 1660, Batch Gradient Norm: 12.732477009930165
Epoch: 1660, Batch Gradient Norm after: 12.732477009930165
Epoch 1661/10000, Prediction Accuracy = 60.314%, Loss = 0.5875331878662109
Epoch: 1661, Batch Gradient Norm: 13.155768909792744
Epoch: 1661, Batch Gradient Norm after: 13.155768909792744
Epoch 1662/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.5943118929862976
Epoch: 1662, Batch Gradient Norm: 9.703614476196833
Epoch: 1662, Batch Gradient Norm after: 9.703614476196833
Epoch 1663/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5638801455497742
Epoch: 1663, Batch Gradient Norm: 11.924519543840674
Epoch: 1663, Batch Gradient Norm after: 11.924519543840674
Epoch 1664/10000, Prediction Accuracy = 60.342%, Loss = 0.5824425697326661
Epoch: 1664, Batch Gradient Norm: 12.536445886459857
Epoch: 1664, Batch Gradient Norm after: 12.536445886459857
Epoch 1665/10000, Prediction Accuracy = 60.426%, Loss = 0.587287986278534
Epoch: 1665, Batch Gradient Norm: 11.58970262354184
Epoch: 1665, Batch Gradient Norm after: 11.58970262354184
Epoch 1666/10000, Prediction Accuracy = 60.386%, Loss = 0.5762778520584106
Epoch: 1666, Batch Gradient Norm: 13.585425629517445
Epoch: 1666, Batch Gradient Norm after: 13.585425629517445
Epoch 1667/10000, Prediction Accuracy = 60.39%, Loss = 0.5922085046768188
Epoch: 1667, Batch Gradient Norm: 10.364968750046755
Epoch: 1667, Batch Gradient Norm after: 10.364968750046755
Epoch 1668/10000, Prediction Accuracy = 60.338%, Loss = 0.5767657041549683
Epoch: 1668, Batch Gradient Norm: 8.965847973457228
Epoch: 1668, Batch Gradient Norm after: 8.965847973457228
Epoch 1669/10000, Prediction Accuracy = 60.338%, Loss = 0.5689139842987061
Epoch: 1669, Batch Gradient Norm: 8.34692206955254
Epoch: 1669, Batch Gradient Norm after: 8.34692206955254
Epoch 1670/10000, Prediction Accuracy = 60.438%, Loss = 0.5571943640708923
Epoch: 1670, Batch Gradient Norm: 11.308315694183962
Epoch: 1670, Batch Gradient Norm after: 11.308315694183962
Epoch 1671/10000, Prediction Accuracy = 60.222%, Loss = 0.577150559425354
Epoch: 1671, Batch Gradient Norm: 12.76870582704286
Epoch: 1671, Batch Gradient Norm after: 12.76870582704286
Epoch 1672/10000, Prediction Accuracy = 60.35%, Loss = 0.5821217179298401
Epoch: 1672, Batch Gradient Norm: 12.779997103188759
Epoch: 1672, Batch Gradient Norm after: 12.779997103188759
Epoch 1673/10000, Prediction Accuracy = 60.372%, Loss = 0.5878534197807312
Epoch: 1673, Batch Gradient Norm: 10.567703330091527
Epoch: 1673, Batch Gradient Norm after: 10.567703330091527
Epoch 1674/10000, Prediction Accuracy = 60.39%, Loss = 0.5679195761680603
Epoch: 1674, Batch Gradient Norm: 14.227821071429114
Epoch: 1674, Batch Gradient Norm after: 14.227821071429114
Epoch 1675/10000, Prediction Accuracy = 60.33200000000001%, Loss = 0.5983754754066467
Epoch: 1675, Batch Gradient Norm: 11.596744246317172
Epoch: 1675, Batch Gradient Norm after: 11.596744246317172
Epoch 1676/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.5781108975410462
Epoch: 1676, Batch Gradient Norm: 14.14624494535971
Epoch: 1676, Batch Gradient Norm after: 14.14624494535971
Epoch 1677/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.5961356043815613
Epoch: 1677, Batch Gradient Norm: 13.424852998151168
Epoch: 1677, Batch Gradient Norm after: 13.424852998151168
Epoch 1678/10000, Prediction Accuracy = 60.282%, Loss = 0.5853741288185119
Epoch: 1678, Batch Gradient Norm: 7.319890690081883
Epoch: 1678, Batch Gradient Norm after: 7.319890690081883
Epoch 1679/10000, Prediction Accuracy = 60.517999999999994%, Loss = 0.5506144285202026
Epoch: 1679, Batch Gradient Norm: 8.893866485941423
Epoch: 1679, Batch Gradient Norm after: 8.893866485941423
Epoch 1680/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.5596529245376587
Epoch: 1680, Batch Gradient Norm: 12.921239011163054
Epoch: 1680, Batch Gradient Norm after: 12.921239011163054
Epoch 1681/10000, Prediction Accuracy = 60.372%, Loss = 0.5926368355751037
Epoch: 1681, Batch Gradient Norm: 8.520424431234716
Epoch: 1681, Batch Gradient Norm after: 8.520424431234716
Epoch 1682/10000, Prediction Accuracy = 60.432%, Loss = 0.5565346956253052
Epoch: 1682, Batch Gradient Norm: 7.556590066080263
Epoch: 1682, Batch Gradient Norm after: 7.556590066080263
Epoch 1683/10000, Prediction Accuracy = 60.448%, Loss = 0.552776300907135
Epoch: 1683, Batch Gradient Norm: 11.091672821363563
Epoch: 1683, Batch Gradient Norm after: 11.091672821363563
Epoch 1684/10000, Prediction Accuracy = 60.432%, Loss = 0.5729718089103699
Epoch: 1684, Batch Gradient Norm: 13.489649577213477
Epoch: 1684, Batch Gradient Norm after: 13.489649577213477
Epoch 1685/10000, Prediction Accuracy = 60.484%, Loss = 0.5908354878425598
Epoch: 1685, Batch Gradient Norm: 10.985721942293946
Epoch: 1685, Batch Gradient Norm after: 10.985721942293946
Epoch 1686/10000, Prediction Accuracy = 60.414%, Loss = 0.5702094435691833
Epoch: 1686, Batch Gradient Norm: 8.44525671323778
Epoch: 1686, Batch Gradient Norm after: 8.44525671323778
Epoch 1687/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.5568374991416931
Epoch: 1687, Batch Gradient Norm: 10.317658912882363
Epoch: 1687, Batch Gradient Norm after: 10.317658912882363
Epoch 1688/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.5663643598556518
Epoch: 1688, Batch Gradient Norm: 15.47621187759453
Epoch: 1688, Batch Gradient Norm after: 15.47621187759453
Epoch 1689/10000, Prediction Accuracy = 60.331999999999994%, Loss = 0.6221883177757264
Epoch: 1689, Batch Gradient Norm: 10.716696855476663
Epoch: 1689, Batch Gradient Norm after: 10.716696855476663
Epoch 1690/10000, Prediction Accuracy = 60.3%, Loss = 0.5749960780143738
Epoch: 1690, Batch Gradient Norm: 12.223051359796616
Epoch: 1690, Batch Gradient Norm after: 12.223051359796616
Epoch 1691/10000, Prediction Accuracy = 60.35600000000001%, Loss = 0.5795550227165223
Epoch: 1691, Batch Gradient Norm: 14.337757717115835
Epoch: 1691, Batch Gradient Norm after: 14.337757717115835
Epoch 1692/10000, Prediction Accuracy = 60.4%, Loss = 0.5950486779212951
Epoch: 1692, Batch Gradient Norm: 10.84115696376822
Epoch: 1692, Batch Gradient Norm after: 10.84115696376822
Epoch 1693/10000, Prediction Accuracy = 60.408%, Loss = 0.5716922640800476
Epoch: 1693, Batch Gradient Norm: 11.776475932873248
Epoch: 1693, Batch Gradient Norm after: 11.776475932873248
Epoch 1694/10000, Prediction Accuracy = 60.379999999999995%, Loss = 0.5760220170021058
Epoch: 1694, Batch Gradient Norm: 13.050797273785763
Epoch: 1694, Batch Gradient Norm after: 13.050797273785763
Epoch 1695/10000, Prediction Accuracy = 60.42%, Loss = 0.5834348440170288
Epoch: 1695, Batch Gradient Norm: 9.179289560380958
Epoch: 1695, Batch Gradient Norm after: 9.179289560380958
Epoch 1696/10000, Prediction Accuracy = 60.414%, Loss = 0.5631719589233398
Epoch: 1696, Batch Gradient Norm: 8.940434538746661
Epoch: 1696, Batch Gradient Norm after: 8.940434538746661
Epoch 1697/10000, Prediction Accuracy = 60.416%, Loss = 0.5580853700637818
Epoch: 1697, Batch Gradient Norm: 12.40885573891982
Epoch: 1697, Batch Gradient Norm after: 12.40885573891982
Epoch 1698/10000, Prediction Accuracy = 60.36199999999999%, Loss = 0.5812339067459107
Epoch: 1698, Batch Gradient Norm: 13.767773960228274
Epoch: 1698, Batch Gradient Norm after: 13.767773960228274
Epoch 1699/10000, Prediction Accuracy = 60.452%, Loss = 0.5926538467407226
Epoch: 1699, Batch Gradient Norm: 12.25441126862991
Epoch: 1699, Batch Gradient Norm after: 12.25441126862991
Epoch 1700/10000, Prediction Accuracy = 60.464%, Loss = 0.5808620095252991
Epoch: 1700, Batch Gradient Norm: 8.60294008676108
Epoch: 1700, Batch Gradient Norm after: 8.60294008676108
Epoch 1701/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.5590508222579956
Epoch: 1701, Batch Gradient Norm: 9.021798228629114
Epoch: 1701, Batch Gradient Norm after: 9.021798228629114
Epoch 1702/10000, Prediction Accuracy = 60.334%, Loss = 0.5578900218009949
Epoch: 1702, Batch Gradient Norm: 10.432142589048297
Epoch: 1702, Batch Gradient Norm after: 10.432142589048297
Epoch 1703/10000, Prediction Accuracy = 60.4%, Loss = 0.5648189425468445
Epoch: 1703, Batch Gradient Norm: 11.682466202858835
Epoch: 1703, Batch Gradient Norm after: 11.682466202858835
Epoch 1704/10000, Prediction Accuracy = 60.374%, Loss = 0.5773708462715149
Epoch: 1704, Batch Gradient Norm: 9.794898809196733
Epoch: 1704, Batch Gradient Norm after: 9.794898809196733
Epoch 1705/10000, Prediction Accuracy = 60.416%, Loss = 0.5614649772644043
Epoch: 1705, Batch Gradient Norm: 7.696440272036027
Epoch: 1705, Batch Gradient Norm after: 7.696440272036027
Epoch 1706/10000, Prediction Accuracy = 60.40999999999999%, Loss = 0.5544030070304871
Epoch: 1706, Batch Gradient Norm: 15.840889231837833
Epoch: 1706, Batch Gradient Norm after: 15.840889231837833
Epoch 1707/10000, Prediction Accuracy = 60.448%, Loss = 0.6094638347625733
Epoch: 1707, Batch Gradient Norm: 12.158228967299948
Epoch: 1707, Batch Gradient Norm after: 12.158228967299948
Epoch 1708/10000, Prediction Accuracy = 60.35%, Loss = 0.5821911931037903
Epoch: 1708, Batch Gradient Norm: 11.195018022755574
Epoch: 1708, Batch Gradient Norm after: 11.195018022755574
Epoch 1709/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.5722672462463378
Epoch: 1709, Batch Gradient Norm: 9.966252828737922
Epoch: 1709, Batch Gradient Norm after: 9.966252828737922
Epoch 1710/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.5644667506217956
Epoch: 1710, Batch Gradient Norm: 10.719639259614384
Epoch: 1710, Batch Gradient Norm after: 10.719639259614384
Epoch 1711/10000, Prediction Accuracy = 60.564%, Loss = 0.5676220417022705
Epoch: 1711, Batch Gradient Norm: 12.136963369947278
Epoch: 1711, Batch Gradient Norm after: 12.136963369947278
Epoch 1712/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.5803620100021363
Epoch: 1712, Batch Gradient Norm: 15.215415263043564
Epoch: 1712, Batch Gradient Norm after: 15.215415263043564
Epoch 1713/10000, Prediction Accuracy = 60.396%, Loss = 0.6020417451858521
Epoch: 1713, Batch Gradient Norm: 7.96690957920354
Epoch: 1713, Batch Gradient Norm after: 7.96690957920354
Epoch 1714/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.5516257643699646
Epoch: 1714, Batch Gradient Norm: 8.566202340278235
Epoch: 1714, Batch Gradient Norm after: 8.566202340278235
Epoch 1715/10000, Prediction Accuracy = 60.402%, Loss = 0.5578698754310608
Epoch: 1715, Batch Gradient Norm: 12.149964711819855
Epoch: 1715, Batch Gradient Norm after: 12.149964711819855
Epoch 1716/10000, Prediction Accuracy = 60.339999999999996%, Loss = 0.5830607175827026
Epoch: 1716, Batch Gradient Norm: 12.645377784901681
Epoch: 1716, Batch Gradient Norm after: 12.645377784901681
Epoch 1717/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.5832753419876099
Epoch: 1717, Batch Gradient Norm: 12.911004939815617
Epoch: 1717, Batch Gradient Norm after: 12.911004939815617
Epoch 1718/10000, Prediction Accuracy = 60.288%, Loss = 0.5908633828163147
Epoch: 1718, Batch Gradient Norm: 8.432600851962148
Epoch: 1718, Batch Gradient Norm after: 8.432600851962148
Epoch 1719/10000, Prediction Accuracy = 60.41600000000001%, Loss = 0.5587902069091797
Epoch: 1719, Batch Gradient Norm: 8.893606539675119
Epoch: 1719, Batch Gradient Norm after: 8.893606539675119
Epoch 1720/10000, Prediction Accuracy = 60.372%, Loss = 0.5566885590553283
Epoch: 1720, Batch Gradient Norm: 12.887886399729224
Epoch: 1720, Batch Gradient Norm after: 12.887886399729224
Epoch 1721/10000, Prediction Accuracy = 60.568%, Loss = 0.5788294434547424
Epoch: 1721, Batch Gradient Norm: 13.446094728568228
Epoch: 1721, Batch Gradient Norm after: 13.446094728568228
Epoch 1722/10000, Prediction Accuracy = 60.472%, Loss = 0.5799655318260193
Epoch: 1722, Batch Gradient Norm: 14.030631340820422
Epoch: 1722, Batch Gradient Norm after: 14.030631340820422
Epoch 1723/10000, Prediction Accuracy = 60.432%, Loss = 0.5878975749015808
Epoch: 1723, Batch Gradient Norm: 10.387524939381146
Epoch: 1723, Batch Gradient Norm after: 10.387524939381146
Epoch 1724/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.5686475396156311
Epoch: 1724, Batch Gradient Norm: 9.067782073239623
Epoch: 1724, Batch Gradient Norm after: 9.067782073239623
Epoch 1725/10000, Prediction Accuracy = 60.529999999999994%, Loss = 0.5577667117118835
Epoch: 1725, Batch Gradient Norm: 13.538027592507884
Epoch: 1725, Batch Gradient Norm after: 13.538027592507884
Epoch 1726/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5986947178840637
Epoch: 1726, Batch Gradient Norm: 9.348646304269826
Epoch: 1726, Batch Gradient Norm after: 9.348646304269826
Epoch 1727/10000, Prediction Accuracy = 60.46%, Loss = 0.5608291387557983
Epoch: 1727, Batch Gradient Norm: 6.827377852974964
Epoch: 1727, Batch Gradient Norm after: 6.827377852974964
Epoch 1728/10000, Prediction Accuracy = 60.501999999999995%, Loss = 0.5457268238067627
Epoch: 1728, Batch Gradient Norm: 11.429325804897308
Epoch: 1728, Batch Gradient Norm after: 11.429325804897308
Epoch 1729/10000, Prediction Accuracy = 60.507999999999996%, Loss = 0.5690671443939209
Epoch: 1729, Batch Gradient Norm: 15.537993683197652
Epoch: 1729, Batch Gradient Norm after: 15.537993683197652
Epoch 1730/10000, Prediction Accuracy = 60.314%, Loss = 0.6024238109588623
Epoch: 1730, Batch Gradient Norm: 15.517415751925054
Epoch: 1730, Batch Gradient Norm after: 15.517415751925054
Epoch 1731/10000, Prediction Accuracy = 60.326%, Loss = 0.6048888325691223
Epoch: 1731, Batch Gradient Norm: 10.773736726932665
Epoch: 1731, Batch Gradient Norm after: 10.773736726932665
Epoch 1732/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5665266513824463
Epoch: 1732, Batch Gradient Norm: 8.580364928407523
Epoch: 1732, Batch Gradient Norm after: 8.580364928407523
Epoch 1733/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.5543274402618408
Epoch: 1733, Batch Gradient Norm: 7.925987907918107
Epoch: 1733, Batch Gradient Norm after: 7.925987907918107
Epoch 1734/10000, Prediction Accuracy = 60.45%, Loss = 0.5541443228721619
Epoch: 1734, Batch Gradient Norm: 13.62823475368663
Epoch: 1734, Batch Gradient Norm after: 13.62823475368663
Epoch 1735/10000, Prediction Accuracy = 60.488%, Loss = 0.5845818281173706
Epoch: 1735, Batch Gradient Norm: 13.699572198816103
Epoch: 1735, Batch Gradient Norm after: 13.699572198816103
Epoch 1736/10000, Prediction Accuracy = 60.443999999999996%, Loss = 0.590521764755249
Epoch: 1736, Batch Gradient Norm: 8.814202234242241
Epoch: 1736, Batch Gradient Norm after: 8.814202234242241
Epoch 1737/10000, Prediction Accuracy = 60.443999999999996%, Loss = 0.555060625076294
Epoch: 1737, Batch Gradient Norm: 10.040637238941637
Epoch: 1737, Batch Gradient Norm after: 10.040637238941637
Epoch 1738/10000, Prediction Accuracy = 60.496%, Loss = 0.5607953071594238
Epoch: 1738, Batch Gradient Norm: 8.949903074988084
Epoch: 1738, Batch Gradient Norm after: 8.949903074988084
Epoch 1739/10000, Prediction Accuracy = 60.472%, Loss = 0.5555626153945923
Epoch: 1739, Batch Gradient Norm: 12.519587443147223
Epoch: 1739, Batch Gradient Norm after: 12.519587443147223
Epoch 1740/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.5753105044364929
Epoch: 1740, Batch Gradient Norm: 14.564104465501423
Epoch: 1740, Batch Gradient Norm after: 14.564104465501423
Epoch 1741/10000, Prediction Accuracy = 60.39%, Loss = 0.5916567921638489
Epoch: 1741, Batch Gradient Norm: 13.024933488930364
Epoch: 1741, Batch Gradient Norm after: 13.024933488930364
Epoch 1742/10000, Prediction Accuracy = 60.376%, Loss = 0.5858605027198791
Epoch: 1742, Batch Gradient Norm: 9.243228051699713
Epoch: 1742, Batch Gradient Norm after: 9.243228051699713
Epoch 1743/10000, Prediction Accuracy = 60.516%, Loss = 0.5563605546951294
Epoch: 1743, Batch Gradient Norm: 9.629228918970695
Epoch: 1743, Batch Gradient Norm after: 9.629228918970695
Epoch 1744/10000, Prediction Accuracy = 60.49400000000001%, Loss = 0.5614083886146546
Epoch: 1744, Batch Gradient Norm: 12.090963821176716
Epoch: 1744, Batch Gradient Norm after: 12.090963821176716
Epoch 1745/10000, Prediction Accuracy = 60.484%, Loss = 0.5840014100074769
Epoch: 1745, Batch Gradient Norm: 10.592968469393677
Epoch: 1745, Batch Gradient Norm after: 10.592968469393677
Epoch 1746/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.5711409568786621
Epoch: 1746, Batch Gradient Norm: 9.893073162393287
Epoch: 1746, Batch Gradient Norm after: 9.893073162393287
Epoch 1747/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.5610047221183777
Epoch: 1747, Batch Gradient Norm: 12.040307250360373
Epoch: 1747, Batch Gradient Norm after: 12.040307250360373
Epoch 1748/10000, Prediction Accuracy = 60.424%, Loss = 0.572244930267334
Epoch: 1748, Batch Gradient Norm: 11.469795004409761
Epoch: 1748, Batch Gradient Norm after: 11.469795004409761
Epoch 1749/10000, Prediction Accuracy = 60.43399999999999%, Loss = 0.569158673286438
Epoch: 1749, Batch Gradient Norm: 10.931235424243031
Epoch: 1749, Batch Gradient Norm after: 10.931235424243031
Epoch 1750/10000, Prediction Accuracy = 60.431999999999995%, Loss = 0.5654037356376648
Epoch: 1750, Batch Gradient Norm: 14.917065133940795
Epoch: 1750, Batch Gradient Norm after: 14.917065133940795
Epoch 1751/10000, Prediction Accuracy = 60.36%, Loss = 0.5967300057411193
Epoch: 1751, Batch Gradient Norm: 15.00440325429363
Epoch: 1751, Batch Gradient Norm after: 15.00440325429363
Epoch 1752/10000, Prediction Accuracy = 60.54200000000001%, Loss = 0.6008912920951843
Epoch: 1752, Batch Gradient Norm: 7.9652690560595545
Epoch: 1752, Batch Gradient Norm after: 7.9652690560595545
Epoch 1753/10000, Prediction Accuracy = 60.584%, Loss = 0.5493050456047058
Epoch: 1753, Batch Gradient Norm: 10.685550186576801
Epoch: 1753, Batch Gradient Norm after: 10.685550186576801
Epoch 1754/10000, Prediction Accuracy = 60.364%, Loss = 0.5646498680114747
Epoch: 1754, Batch Gradient Norm: 12.487265105716663
Epoch: 1754, Batch Gradient Norm after: 12.487265105716663
Epoch 1755/10000, Prediction Accuracy = 60.468%, Loss = 0.576205062866211
Epoch: 1755, Batch Gradient Norm: 14.09363614318413
Epoch: 1755, Batch Gradient Norm after: 14.09363614318413
Epoch 1756/10000, Prediction Accuracy = 60.43399999999999%, Loss = 0.5898892879486084
Epoch: 1756, Batch Gradient Norm: 7.691370144462151
Epoch: 1756, Batch Gradient Norm after: 7.691370144462151
Epoch 1757/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.5480099558830261
Epoch: 1757, Batch Gradient Norm: 7.30032526779425
Epoch: 1757, Batch Gradient Norm after: 7.30032526779425
Epoch 1758/10000, Prediction Accuracy = 60.548%, Loss = 0.5461264491081238
Epoch: 1758, Batch Gradient Norm: 10.636272683271736
Epoch: 1758, Batch Gradient Norm after: 10.636272683271736
Epoch 1759/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.5623444676399231
Epoch: 1759, Batch Gradient Norm: 15.359678317702311
Epoch: 1759, Batch Gradient Norm after: 15.359678317702311
Epoch 1760/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.5948990225791931
Epoch: 1760, Batch Gradient Norm: 11.222391370347369
Epoch: 1760, Batch Gradient Norm after: 11.222391370347369
Epoch 1761/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5692357897758484
Epoch: 1761, Batch Gradient Norm: 8.546512734622501
Epoch: 1761, Batch Gradient Norm after: 8.546512734622501
Epoch 1762/10000, Prediction Accuracy = 60.616%, Loss = 0.5526088118553162
Epoch: 1762, Batch Gradient Norm: 7.8313143303312085
Epoch: 1762, Batch Gradient Norm after: 7.8313143303312085
Epoch 1763/10000, Prediction Accuracy = 60.507999999999996%, Loss = 0.548111081123352
Epoch: 1763, Batch Gradient Norm: 9.28311897520121
Epoch: 1763, Batch Gradient Norm after: 9.28311897520121
Epoch 1764/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.5579142332077026
Epoch: 1764, Batch Gradient Norm: 13.735029829347203
Epoch: 1764, Batch Gradient Norm after: 13.735029829347203
Epoch 1765/10000, Prediction Accuracy = 60.428%, Loss = 0.5920569062232971
Epoch: 1765, Batch Gradient Norm: 10.550342017628326
Epoch: 1765, Batch Gradient Norm after: 10.550342017628326
Epoch 1766/10000, Prediction Accuracy = 60.524%, Loss = 0.5646454811096191
Epoch: 1766, Batch Gradient Norm: 12.586120196075019
Epoch: 1766, Batch Gradient Norm after: 12.586120196075019
Epoch 1767/10000, Prediction Accuracy = 60.452%, Loss = 0.5778512597084046
Epoch: 1767, Batch Gradient Norm: 12.472647094804326
Epoch: 1767, Batch Gradient Norm after: 12.472647094804326
Epoch 1768/10000, Prediction Accuracy = 60.46600000000001%, Loss = 0.5811285614967346
Epoch: 1768, Batch Gradient Norm: 8.785743176595219
Epoch: 1768, Batch Gradient Norm after: 8.785743176595219
Epoch 1769/10000, Prediction Accuracy = 60.541999999999994%, Loss = 0.5571698665618896
Epoch: 1769, Batch Gradient Norm: 10.130653118254683
Epoch: 1769, Batch Gradient Norm after: 10.130653118254683
Epoch 1770/10000, Prediction Accuracy = 60.5%, Loss = 0.5603536367416382
Epoch: 1770, Batch Gradient Norm: 13.198425220337626
Epoch: 1770, Batch Gradient Norm after: 13.198425220337626
Epoch 1771/10000, Prediction Accuracy = 60.476%, Loss = 0.581566572189331
Epoch: 1771, Batch Gradient Norm: 12.360247224261293
Epoch: 1771, Batch Gradient Norm after: 12.360247224261293
Epoch 1772/10000, Prediction Accuracy = 60.398%, Loss = 0.5753542900085449
Epoch: 1772, Batch Gradient Norm: 13.013366473917284
Epoch: 1772, Batch Gradient Norm after: 13.013366473917284
Epoch 1773/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.5885735034942627
Epoch: 1773, Batch Gradient Norm: 10.688194076249609
Epoch: 1773, Batch Gradient Norm after: 10.688194076249609
Epoch 1774/10000, Prediction Accuracy = 60.45%, Loss = 0.5679684638977051
Epoch: 1774, Batch Gradient Norm: 11.19525503212229
Epoch: 1774, Batch Gradient Norm after: 11.19525503212229
Epoch 1775/10000, Prediction Accuracy = 60.544000000000004%, Loss = 0.5679732322692871
Epoch: 1775, Batch Gradient Norm: 10.237724501370318
Epoch: 1775, Batch Gradient Norm after: 10.237724501370318
Epoch 1776/10000, Prediction Accuracy = 60.45%, Loss = 0.5605706930160522
Epoch: 1776, Batch Gradient Norm: 8.434135936508893
Epoch: 1776, Batch Gradient Norm after: 8.434135936508893
Epoch 1777/10000, Prediction Accuracy = 60.584%, Loss = 0.5516609907150268
Epoch: 1777, Batch Gradient Norm: 12.353794323240146
Epoch: 1777, Batch Gradient Norm after: 12.353794323240146
Epoch 1778/10000, Prediction Accuracy = 60.519999999999996%, Loss = 0.5747630000114441
Epoch: 1778, Batch Gradient Norm: 12.869937289202245
Epoch: 1778, Batch Gradient Norm after: 12.869937289202245
Epoch 1779/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.5790780901908874
Epoch: 1779, Batch Gradient Norm: 12.954039674177906
Epoch: 1779, Batch Gradient Norm after: 12.954039674177906
Epoch 1780/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.5804418563842774
Epoch: 1780, Batch Gradient Norm: 10.67806756884085
Epoch: 1780, Batch Gradient Norm after: 10.67806756884085
Epoch 1781/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.5689661264419555
Epoch: 1781, Batch Gradient Norm: 10.133173138523835
Epoch: 1781, Batch Gradient Norm after: 10.133173138523835
Epoch 1782/10000, Prediction Accuracy = 60.562%, Loss = 0.5612393617630005
Epoch: 1782, Batch Gradient Norm: 14.341645153759547
Epoch: 1782, Batch Gradient Norm after: 14.341645153759547
Epoch 1783/10000, Prediction Accuracy = 60.576%, Loss = 0.5889174342155457
Epoch: 1783, Batch Gradient Norm: 12.833809912120067
Epoch: 1783, Batch Gradient Norm after: 12.833809912120067
Epoch 1784/10000, Prediction Accuracy = 60.48%, Loss = 0.5763416528701782
Epoch: 1784, Batch Gradient Norm: 10.141072633525505
Epoch: 1784, Batch Gradient Norm after: 10.141072633525505
Epoch 1785/10000, Prediction Accuracy = 60.508%, Loss = 0.5575429797172546
Epoch: 1785, Batch Gradient Norm: 7.8960487780019335
Epoch: 1785, Batch Gradient Norm after: 7.8960487780019335
Epoch 1786/10000, Prediction Accuracy = 60.574%, Loss = 0.5455447316169739
Epoch: 1786, Batch Gradient Norm: 11.77004262259919
Epoch: 1786, Batch Gradient Norm after: 11.77004262259919
Epoch 1787/10000, Prediction Accuracy = 60.584%, Loss = 0.5686044216156005
Epoch: 1787, Batch Gradient Norm: 11.242294222560584
Epoch: 1787, Batch Gradient Norm after: 11.242294222560584
Epoch 1788/10000, Prediction Accuracy = 60.622%, Loss = 0.5671895980834961
Epoch: 1788, Batch Gradient Norm: 10.715809663676094
Epoch: 1788, Batch Gradient Norm after: 10.715809663676094
Epoch 1789/10000, Prediction Accuracy = 60.638%, Loss = 0.5675296783447266
Epoch: 1789, Batch Gradient Norm: 10.76133381498678
Epoch: 1789, Batch Gradient Norm after: 10.76133381498678
Epoch 1790/10000, Prediction Accuracy = 60.534000000000006%, Loss = 0.5652425646781921
Epoch: 1790, Batch Gradient Norm: 12.037276717537036
Epoch: 1790, Batch Gradient Norm after: 12.037276717537036
Epoch 1791/10000, Prediction Accuracy = 60.664%, Loss = 0.569498872756958
Epoch: 1791, Batch Gradient Norm: 10.95673641324754
Epoch: 1791, Batch Gradient Norm after: 10.95673641324754
Epoch 1792/10000, Prediction Accuracy = 60.424%, Loss = 0.5645466923713685
Epoch: 1792, Batch Gradient Norm: 8.611681526888017
Epoch: 1792, Batch Gradient Norm after: 8.611681526888017
Epoch 1793/10000, Prediction Accuracy = 60.620000000000005%, Loss = 0.5470889687538147
Epoch: 1793, Batch Gradient Norm: 10.40728297003731
Epoch: 1793, Batch Gradient Norm after: 10.40728297003731
Epoch 1794/10000, Prediction Accuracy = 60.529999999999994%, Loss = 0.5635578513145447
Epoch: 1794, Batch Gradient Norm: 16.270161192875552
Epoch: 1794, Batch Gradient Norm after: 16.270161192875552
Epoch 1795/10000, Prediction Accuracy = 60.39000000000001%, Loss = 0.6127322435379028
Epoch: 1795, Batch Gradient Norm: 14.474335871274157
Epoch: 1795, Batch Gradient Norm after: 14.474335871274157
Epoch 1796/10000, Prediction Accuracy = 60.448%, Loss = 0.5929965615272522
Epoch: 1796, Batch Gradient Norm: 9.492097814936056
Epoch: 1796, Batch Gradient Norm after: 9.492097814936056
Epoch 1797/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5547248601913453
Epoch: 1797, Batch Gradient Norm: 10.03529273590188
Epoch: 1797, Batch Gradient Norm after: 10.03529273590188
Epoch 1798/10000, Prediction Accuracy = 60.662%, Loss = 0.5609222292900086
Epoch: 1798, Batch Gradient Norm: 8.09993212239783
Epoch: 1798, Batch Gradient Norm after: 8.09993212239783
Epoch 1799/10000, Prediction Accuracy = 60.612%, Loss = 0.5481018185615539
Epoch: 1799, Batch Gradient Norm: 8.244218577231075
Epoch: 1799, Batch Gradient Norm after: 8.244218577231075
Epoch 1800/10000, Prediction Accuracy = 60.617999999999995%, Loss = 0.548970103263855
Epoch: 1800, Batch Gradient Norm: 11.551643831294983
Epoch: 1800, Batch Gradient Norm after: 11.551643831294983
Epoch 1801/10000, Prediction Accuracy = 60.525999999999996%, Loss = 0.5709923386573792
Epoch: 1801, Batch Gradient Norm: 9.940456450567307
Epoch: 1801, Batch Gradient Norm after: 9.940456450567307
Epoch 1802/10000, Prediction Accuracy = 60.63399999999999%, Loss = 0.5567899942398071
Epoch: 1802, Batch Gradient Norm: 11.973908266425859
Epoch: 1802, Batch Gradient Norm after: 11.973908266425859
Epoch 1803/10000, Prediction Accuracy = 60.544%, Loss = 0.5699914574623108
Epoch: 1803, Batch Gradient Norm: 16.66157967792193
Epoch: 1803, Batch Gradient Norm after: 16.66157967792193
Epoch 1804/10000, Prediction Accuracy = 60.482000000000006%, Loss = 0.6126745700836181
Epoch: 1804, Batch Gradient Norm: 9.827356748579149
Epoch: 1804, Batch Gradient Norm after: 9.827356748579149
Epoch 1805/10000, Prediction Accuracy = 60.622%, Loss = 0.5609528183937073
Epoch: 1805, Batch Gradient Norm: 8.796197610096817
Epoch: 1805, Batch Gradient Norm after: 8.796197610096817
Epoch 1806/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.5524086236953736
Epoch: 1806, Batch Gradient Norm: 12.623896056461579
Epoch: 1806, Batch Gradient Norm after: 12.623896056461579
Epoch 1807/10000, Prediction Accuracy = 60.617999999999995%, Loss = 0.5730948805809021
Epoch: 1807, Batch Gradient Norm: 13.313343030243226
Epoch: 1807, Batch Gradient Norm after: 13.313343030243226
Epoch 1808/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.5779360175132752
Epoch: 1808, Batch Gradient Norm: 11.209546427789551
Epoch: 1808, Batch Gradient Norm after: 11.209546427789551
Epoch 1809/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.5670750260353088
Epoch: 1809, Batch Gradient Norm: 9.72667884358574
Epoch: 1809, Batch Gradient Norm after: 9.72667884358574
Epoch 1810/10000, Prediction Accuracy = 60.652%, Loss = 0.555178439617157
Epoch: 1810, Batch Gradient Norm: 14.184289960299056
Epoch: 1810, Batch Gradient Norm after: 14.184289960299056
Epoch 1811/10000, Prediction Accuracy = 60.492%, Loss = 0.5994884610176087
Epoch: 1811, Batch Gradient Norm: 8.534533337589444
Epoch: 1811, Batch Gradient Norm after: 8.534533337589444
Epoch 1812/10000, Prediction Accuracy = 60.708000000000006%, Loss = 0.5525612235069275
Epoch: 1812, Batch Gradient Norm: 9.99641984708424
Epoch: 1812, Batch Gradient Norm after: 9.99641984708424
Epoch 1813/10000, Prediction Accuracy = 60.681999999999995%, Loss = 0.5552896499633789
Epoch: 1813, Batch Gradient Norm: 13.48576030919448
Epoch: 1813, Batch Gradient Norm after: 13.48576030919448
Epoch 1814/10000, Prediction Accuracy = 60.492%, Loss = 0.5817247986793518
Epoch: 1814, Batch Gradient Norm: 12.582869308933677
Epoch: 1814, Batch Gradient Norm after: 12.582869308933677
Epoch 1815/10000, Prediction Accuracy = 60.455999999999996%, Loss = 0.5750944018363953
Epoch: 1815, Batch Gradient Norm: 7.2872307659350355
Epoch: 1815, Batch Gradient Norm after: 7.2872307659350355
Epoch 1816/10000, Prediction Accuracy = 60.54200000000001%, Loss = 0.5420062065124511
Epoch: 1816, Batch Gradient Norm: 10.764999144213647
Epoch: 1816, Batch Gradient Norm after: 10.764999144213647
Epoch 1817/10000, Prediction Accuracy = 60.54%, Loss = 0.5657845854759216
Epoch: 1817, Batch Gradient Norm: 12.834021463024655
Epoch: 1817, Batch Gradient Norm after: 12.834021463024655
Epoch 1818/10000, Prediction Accuracy = 60.5%, Loss = 0.5778910517692566
Epoch: 1818, Batch Gradient Norm: 10.1742971938524
Epoch: 1818, Batch Gradient Norm after: 10.1742971938524
Epoch 1819/10000, Prediction Accuracy = 60.513999999999996%, Loss = 0.5605387091636658
Epoch: 1819, Batch Gradient Norm: 12.7012227898365
Epoch: 1819, Batch Gradient Norm after: 12.7012227898365
Epoch 1820/10000, Prediction Accuracy = 60.588%, Loss = 0.5766071915626526
Epoch: 1820, Batch Gradient Norm: 11.153409348234351
Epoch: 1820, Batch Gradient Norm after: 11.153409348234351
Epoch 1821/10000, Prediction Accuracy = 60.472%, Loss = 0.5645992636680603
Epoch: 1821, Batch Gradient Norm: 8.995082954090861
Epoch: 1821, Batch Gradient Norm after: 8.995082954090861
Epoch 1822/10000, Prediction Accuracy = 60.489999999999995%, Loss = 0.5511393904685974
Epoch: 1822, Batch Gradient Norm: 8.49413378288204
Epoch: 1822, Batch Gradient Norm after: 8.49413378288204
Epoch 1823/10000, Prediction Accuracy = 60.504%, Loss = 0.5523348808288574
Epoch: 1823, Batch Gradient Norm: 9.936222519075569
Epoch: 1823, Batch Gradient Norm after: 9.936222519075569
Epoch 1824/10000, Prediction Accuracy = 60.58%, Loss = 0.5540842652320862
Epoch: 1824, Batch Gradient Norm: 12.802297639611533
Epoch: 1824, Batch Gradient Norm after: 12.802297639611533
Epoch 1825/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.5752641081809997
Epoch: 1825, Batch Gradient Norm: 10.501703346073816
Epoch: 1825, Batch Gradient Norm after: 10.501703346073816
Epoch 1826/10000, Prediction Accuracy = 60.604%, Loss = 0.5585381627082825
Epoch: 1826, Batch Gradient Norm: 12.283001550121526
Epoch: 1826, Batch Gradient Norm after: 12.283001550121526
Epoch 1827/10000, Prediction Accuracy = 60.61800000000001%, Loss = 0.5705928325653076
Epoch: 1827, Batch Gradient Norm: 11.472876213137358
Epoch: 1827, Batch Gradient Norm after: 11.472876213137358
Epoch 1828/10000, Prediction Accuracy = 60.584%, Loss = 0.5630298852920532
Epoch: 1828, Batch Gradient Norm: 13.587764815235102
Epoch: 1828, Batch Gradient Norm after: 13.587764815235102
Epoch 1829/10000, Prediction Accuracy = 60.512%, Loss = 0.580902349948883
Epoch: 1829, Batch Gradient Norm: 13.237295428869706
Epoch: 1829, Batch Gradient Norm after: 13.237295428869706
Epoch 1830/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.5828812718391418
Epoch: 1830, Batch Gradient Norm: 9.637266782007478
Epoch: 1830, Batch Gradient Norm after: 9.637266782007478
Epoch 1831/10000, Prediction Accuracy = 60.504%, Loss = 0.5604594707489013
Epoch: 1831, Batch Gradient Norm: 9.758867025501244
Epoch: 1831, Batch Gradient Norm after: 9.758867025501244
Epoch 1832/10000, Prediction Accuracy = 60.652%, Loss = 0.5524707198143005
Epoch: 1832, Batch Gradient Norm: 11.34501055099152
Epoch: 1832, Batch Gradient Norm after: 11.34501055099152
Epoch 1833/10000, Prediction Accuracy = 60.709999999999994%, Loss = 0.5651105761528015
Epoch: 1833, Batch Gradient Norm: 7.806551715194713
Epoch: 1833, Batch Gradient Norm after: 7.806551715194713
Epoch 1834/10000, Prediction Accuracy = 60.658%, Loss = 0.5430655717849732
Epoch: 1834, Batch Gradient Norm: 10.689339619222414
Epoch: 1834, Batch Gradient Norm after: 10.689339619222414
Epoch 1835/10000, Prediction Accuracy = 60.576%, Loss = 0.5573980569839477
Epoch: 1835, Batch Gradient Norm: 10.597586212666576
Epoch: 1835, Batch Gradient Norm after: 10.597586212666576
Epoch 1836/10000, Prediction Accuracy = 60.544%, Loss = 0.5573525786399841
Epoch: 1836, Batch Gradient Norm: 13.309471157272737
Epoch: 1836, Batch Gradient Norm after: 13.309471157272737
Epoch 1837/10000, Prediction Accuracy = 60.64%, Loss = 0.5811098456382752
Epoch: 1837, Batch Gradient Norm: 11.72437738292012
Epoch: 1837, Batch Gradient Norm after: 11.72437738292012
Epoch 1838/10000, Prediction Accuracy = 60.586%, Loss = 0.5700390458106994
Epoch: 1838, Batch Gradient Norm: 10.824631133917581
Epoch: 1838, Batch Gradient Norm after: 10.824631133917581
Epoch 1839/10000, Prediction Accuracy = 60.68399999999999%, Loss = 0.5621783971786499
Epoch: 1839, Batch Gradient Norm: 13.825927072384385
Epoch: 1839, Batch Gradient Norm after: 13.825927072384385
Epoch 1840/10000, Prediction Accuracy = 60.602%, Loss = 0.5826419591903687
Epoch: 1840, Batch Gradient Norm: 12.205032946187227
Epoch: 1840, Batch Gradient Norm after: 12.205032946187227
Epoch 1841/10000, Prediction Accuracy = 60.528%, Loss = 0.5760247468948364
Epoch: 1841, Batch Gradient Norm: 9.28178829700189
Epoch: 1841, Batch Gradient Norm after: 9.28178829700189
Epoch 1842/10000, Prediction Accuracy = 60.532000000000004%, Loss = 0.5538004040718079
Epoch: 1842, Batch Gradient Norm: 11.940344745920541
Epoch: 1842, Batch Gradient Norm after: 11.940344745920541
Epoch 1843/10000, Prediction Accuracy = 60.614%, Loss = 0.5697298765182495
Epoch: 1843, Batch Gradient Norm: 10.130863975771877
Epoch: 1843, Batch Gradient Norm after: 10.130863975771877
Epoch 1844/10000, Prediction Accuracy = 60.55800000000001%, Loss = 0.557555103302002
Epoch: 1844, Batch Gradient Norm: 9.618044791239761
Epoch: 1844, Batch Gradient Norm after: 9.618044791239761
Epoch 1845/10000, Prediction Accuracy = 60.653999999999996%, Loss = 0.5519899010658265
Epoch: 1845, Batch Gradient Norm: 12.498759741423646
Epoch: 1845, Batch Gradient Norm after: 12.498759741423646
Epoch 1846/10000, Prediction Accuracy = 60.598%, Loss = 0.5720077991485596
Epoch: 1846, Batch Gradient Norm: 15.97426304850891
Epoch: 1846, Batch Gradient Norm after: 15.97426304850891
Epoch 1847/10000, Prediction Accuracy = 60.605999999999995%, Loss = 0.6016080737113952
Epoch: 1847, Batch Gradient Norm: 11.465821639097516
Epoch: 1847, Batch Gradient Norm after: 11.465821639097516
Epoch 1848/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.5689868211746216
Epoch: 1848, Batch Gradient Norm: 10.14471037418184
Epoch: 1848, Batch Gradient Norm after: 10.14471037418184
Epoch 1849/10000, Prediction Accuracy = 60.5%, Loss = 0.561888313293457
Epoch: 1849, Batch Gradient Norm: 9.877840836563106
Epoch: 1849, Batch Gradient Norm after: 9.877840836563106
Epoch 1850/10000, Prediction Accuracy = 60.562%, Loss = 0.5524216771125794
Epoch: 1850, Batch Gradient Norm: 9.09437141218006
Epoch: 1850, Batch Gradient Norm after: 9.09437141218006
Epoch 1851/10000, Prediction Accuracy = 60.636%, Loss = 0.5487473487854004
Epoch: 1851, Batch Gradient Norm: 7.566057779251952
Epoch: 1851, Batch Gradient Norm after: 7.566057779251952
Epoch 1852/10000, Prediction Accuracy = 60.513999999999996%, Loss = 0.5437047839164734
Epoch: 1852, Batch Gradient Norm: 9.91046528511923
Epoch: 1852, Batch Gradient Norm after: 9.91046528511923
Epoch 1853/10000, Prediction Accuracy = 60.68399999999999%, Loss = 0.5548529982566833
Epoch: 1853, Batch Gradient Norm: 10.525490775000861
Epoch: 1853, Batch Gradient Norm after: 10.525490775000861
Epoch 1854/10000, Prediction Accuracy = 60.646%, Loss = 0.5583821535110474
Epoch: 1854, Batch Gradient Norm: 10.337342738512174
Epoch: 1854, Batch Gradient Norm after: 10.337342738512174
Epoch 1855/10000, Prediction Accuracy = 60.66600000000001%, Loss = 0.5565754652023316
Epoch: 1855, Batch Gradient Norm: 11.024951941080934
Epoch: 1855, Batch Gradient Norm after: 11.024951941080934
Epoch 1856/10000, Prediction Accuracy = 60.65%, Loss = 0.5619761943817139
Epoch: 1856, Batch Gradient Norm: 12.20761011063004
Epoch: 1856, Batch Gradient Norm after: 12.20761011063004
Epoch 1857/10000, Prediction Accuracy = 60.58200000000001%, Loss = 0.568131935596466
Epoch: 1857, Batch Gradient Norm: 13.039092762699413
Epoch: 1857, Batch Gradient Norm after: 13.039092762699413
Epoch 1858/10000, Prediction Accuracy = 60.496%, Loss = 0.5805997490882874
Epoch: 1858, Batch Gradient Norm: 14.185959730690282
Epoch: 1858, Batch Gradient Norm after: 14.185959730690282
Epoch 1859/10000, Prediction Accuracy = 60.60600000000001%, Loss = 0.5856675863265991
Epoch: 1859, Batch Gradient Norm: 13.116041861674882
Epoch: 1859, Batch Gradient Norm after: 13.116041861674882
Epoch 1860/10000, Prediction Accuracy = 60.541999999999994%, Loss = 0.5812129616737366
Epoch: 1860, Batch Gradient Norm: 8.916143524717018
Epoch: 1860, Batch Gradient Norm after: 8.916143524717018
Epoch 1861/10000, Prediction Accuracy = 60.59599999999999%, Loss = 0.5480629444122315
Epoch: 1861, Batch Gradient Norm: 10.5827931789089
Epoch: 1861, Batch Gradient Norm after: 10.5827931789089
Epoch 1862/10000, Prediction Accuracy = 60.678%, Loss = 0.5549497842788697
Epoch: 1862, Batch Gradient Norm: 14.146719413507677
Epoch: 1862, Batch Gradient Norm after: 14.146719413507677
Epoch 1863/10000, Prediction Accuracy = 60.693999999999996%, Loss = 0.5814186811447144
Epoch: 1863, Batch Gradient Norm: 11.524977189494495
Epoch: 1863, Batch Gradient Norm after: 11.524977189494495
Epoch 1864/10000, Prediction Accuracy = 60.528000000000006%, Loss = 0.5645196437835693
Epoch: 1864, Batch Gradient Norm: 12.510603992858362
Epoch: 1864, Batch Gradient Norm after: 12.510603992858362
Epoch 1865/10000, Prediction Accuracy = 60.688%, Loss = 0.5712668538093567
Epoch: 1865, Batch Gradient Norm: 7.732629961681159
Epoch: 1865, Batch Gradient Norm after: 7.732629961681159
Epoch 1866/10000, Prediction Accuracy = 60.657999999999994%, Loss = 0.5421644449234009
Epoch: 1866, Batch Gradient Norm: 8.333671684340928
Epoch: 1866, Batch Gradient Norm after: 8.333671684340928
Epoch 1867/10000, Prediction Accuracy = 60.556000000000004%, Loss = 0.5476727366447449
Epoch: 1867, Batch Gradient Norm: 10.63851426888704
Epoch: 1867, Batch Gradient Norm after: 10.63851426888704
Epoch 1868/10000, Prediction Accuracy = 60.564%, Loss = 0.5556796789169312
Epoch: 1868, Batch Gradient Norm: 15.480423164120369
Epoch: 1868, Batch Gradient Norm after: 15.480423164120369
Epoch 1869/10000, Prediction Accuracy = 60.6%, Loss = 0.6006935238838196
Epoch: 1869, Batch Gradient Norm: 9.139947126776677
Epoch: 1869, Batch Gradient Norm after: 9.139947126776677
Epoch 1870/10000, Prediction Accuracy = 60.572%, Loss = 0.5478659749031067
Epoch: 1870, Batch Gradient Norm: 14.035096618464976
Epoch: 1870, Batch Gradient Norm after: 14.035096618464976
Epoch 1871/10000, Prediction Accuracy = 60.58%, Loss = 0.587480890750885
Epoch: 1871, Batch Gradient Norm: 12.760798353222018
Epoch: 1871, Batch Gradient Norm after: 12.760798353222018
Epoch 1872/10000, Prediction Accuracy = 60.55%, Loss = 0.5805378913879394
Epoch: 1872, Batch Gradient Norm: 7.670762648986184
Epoch: 1872, Batch Gradient Norm after: 7.670762648986184
Epoch 1873/10000, Prediction Accuracy = 60.604%, Loss = 0.5415215492248535
Epoch: 1873, Batch Gradient Norm: 10.580776404837993
Epoch: 1873, Batch Gradient Norm after: 10.580776404837993
Epoch 1874/10000, Prediction Accuracy = 60.564%, Loss = 0.5564921379089356
Epoch: 1874, Batch Gradient Norm: 10.658883231501122
Epoch: 1874, Batch Gradient Norm after: 10.658883231501122
Epoch 1875/10000, Prediction Accuracy = 60.576%, Loss = 0.5628650307655334
Epoch: 1875, Batch Gradient Norm: 7.472542969912031
Epoch: 1875, Batch Gradient Norm after: 7.472542969912031
Epoch 1876/10000, Prediction Accuracy = 60.73%, Loss = 0.5419490933418274
Epoch: 1876, Batch Gradient Norm: 12.037910571590416
Epoch: 1876, Batch Gradient Norm after: 12.037910571590416
Epoch 1877/10000, Prediction Accuracy = 60.63199999999999%, Loss = 0.5752230525016785
Epoch: 1877, Batch Gradient Norm: 8.971940834138728
Epoch: 1877, Batch Gradient Norm after: 8.971940834138728
Epoch 1878/10000, Prediction Accuracy = 60.662%, Loss = 0.5481082916259765
Epoch: 1878, Batch Gradient Norm: 12.470530736271302
Epoch: 1878, Batch Gradient Norm after: 12.470530736271302
Epoch 1879/10000, Prediction Accuracy = 60.61%, Loss = 0.5699005842208862
Epoch: 1879, Batch Gradient Norm: 10.219536394211005
Epoch: 1879, Batch Gradient Norm after: 10.219536394211005
Epoch 1880/10000, Prediction Accuracy = 60.58%, Loss = 0.5578994631767273
Epoch: 1880, Batch Gradient Norm: 10.403075500669399
Epoch: 1880, Batch Gradient Norm after: 10.403075500669399
Epoch 1881/10000, Prediction Accuracy = 60.644000000000005%, Loss = 0.5559383273124695
Epoch: 1881, Batch Gradient Norm: 10.960026021328787
Epoch: 1881, Batch Gradient Norm after: 10.960026021328787
Epoch 1882/10000, Prediction Accuracy = 60.61%, Loss = 0.5607239961624145
Epoch: 1882, Batch Gradient Norm: 10.356124974065091
Epoch: 1882, Batch Gradient Norm after: 10.356124974065091
Epoch 1883/10000, Prediction Accuracy = 60.67%, Loss = 0.5571703672409057
Epoch: 1883, Batch Gradient Norm: 13.99121699625161
Epoch: 1883, Batch Gradient Norm after: 13.99121699625161
Epoch 1884/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.5842133402824402
Epoch: 1884, Batch Gradient Norm: 15.910082131117582
Epoch: 1884, Batch Gradient Norm after: 15.910082131117582
Epoch 1885/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.6019759893417358
Epoch: 1885, Batch Gradient Norm: 8.921811608702738
Epoch: 1885, Batch Gradient Norm after: 8.921811608702738
Epoch 1886/10000, Prediction Accuracy = 60.678%, Loss = 0.5456482529640198
Epoch: 1886, Batch Gradient Norm: 10.257100591126653
Epoch: 1886, Batch Gradient Norm after: 10.257100591126653
Epoch 1887/10000, Prediction Accuracy = 60.678%, Loss = 0.5543033719062805
Epoch: 1887, Batch Gradient Norm: 12.099354674671089
Epoch: 1887, Batch Gradient Norm after: 12.099354674671089
Epoch 1888/10000, Prediction Accuracy = 60.588%, Loss = 0.5648290157318115
Epoch: 1888, Batch Gradient Norm: 12.762413500749187
Epoch: 1888, Batch Gradient Norm after: 12.762413500749187
Epoch 1889/10000, Prediction Accuracy = 60.602%, Loss = 0.5698915839195251
Epoch: 1889, Batch Gradient Norm: 12.089354814216778
Epoch: 1889, Batch Gradient Norm after: 12.089354814216778
Epoch 1890/10000, Prediction Accuracy = 60.617999999999995%, Loss = 0.5634213089942932
Epoch: 1890, Batch Gradient Norm: 11.386338863795519
Epoch: 1890, Batch Gradient Norm after: 11.386338863795519
Epoch 1891/10000, Prediction Accuracy = 60.638%, Loss = 0.5602274775505066
Epoch: 1891, Batch Gradient Norm: 8.362010703946785
Epoch: 1891, Batch Gradient Norm after: 8.362010703946785
Epoch 1892/10000, Prediction Accuracy = 60.646%, Loss = 0.5436150431632996
Epoch: 1892, Batch Gradient Norm: 10.866615040994702
Epoch: 1892, Batch Gradient Norm after: 10.866615040994702
Epoch 1893/10000, Prediction Accuracy = 60.653999999999996%, Loss = 0.5575350284576416
Epoch: 1893, Batch Gradient Norm: 13.94888526650597
Epoch: 1893, Batch Gradient Norm after: 13.94888526650597
Epoch 1894/10000, Prediction Accuracy = 60.718%, Loss = 0.5791078686714173
Epoch: 1894, Batch Gradient Norm: 12.770266733024172
Epoch: 1894, Batch Gradient Norm after: 12.770266733024172
Epoch 1895/10000, Prediction Accuracy = 60.69%, Loss = 0.5709582567214966
Epoch: 1895, Batch Gradient Norm: 9.206910857500151
Epoch: 1895, Batch Gradient Norm after: 9.206910857500151
Epoch 1896/10000, Prediction Accuracy = 60.676%, Loss = 0.5469898104667663
Epoch: 1896, Batch Gradient Norm: 8.10761569165319
Epoch: 1896, Batch Gradient Norm after: 8.10761569165319
Epoch 1897/10000, Prediction Accuracy = 60.598%, Loss = 0.5404886126518249
Epoch: 1897, Batch Gradient Norm: 9.642937275516347
Epoch: 1897, Batch Gradient Norm after: 9.642937275516347
Epoch 1898/10000, Prediction Accuracy = 60.714%, Loss = 0.5477967500686646
Epoch: 1898, Batch Gradient Norm: 12.891558967800513
Epoch: 1898, Batch Gradient Norm after: 12.891558967800513
Epoch 1899/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.5702453017234802
Epoch: 1899, Batch Gradient Norm: 13.754869720452461
Epoch: 1899, Batch Gradient Norm after: 13.754869720452461
Epoch 1900/10000, Prediction Accuracy = 60.67999999999999%, Loss = 0.5775661587715148
Epoch: 1900, Batch Gradient Norm: 11.302516067958868
Epoch: 1900, Batch Gradient Norm after: 11.302516067958868
Epoch 1901/10000, Prediction Accuracy = 60.598%, Loss = 0.5632472157478332
Epoch: 1901, Batch Gradient Norm: 8.694464303268703
Epoch: 1901, Batch Gradient Norm after: 8.694464303268703
Epoch 1902/10000, Prediction Accuracy = 60.69199999999999%, Loss = 0.5466023802757263
Epoch: 1902, Batch Gradient Norm: 11.85256220470201
Epoch: 1902, Batch Gradient Norm after: 11.85256220470201
Epoch 1903/10000, Prediction Accuracy = 60.734%, Loss = 0.5619434952735901
Epoch: 1903, Batch Gradient Norm: 13.483245717274993
Epoch: 1903, Batch Gradient Norm after: 13.483245717274993
Epoch 1904/10000, Prediction Accuracy = 60.624%, Loss = 0.572448992729187
Epoch: 1904, Batch Gradient Norm: 11.154293150937416
Epoch: 1904, Batch Gradient Norm after: 11.154293150937416
Epoch 1905/10000, Prediction Accuracy = 60.61999999999999%, Loss = 0.5571487188339234
Epoch: 1905, Batch Gradient Norm: 9.721798643014253
Epoch: 1905, Batch Gradient Norm after: 9.721798643014253
Epoch 1906/10000, Prediction Accuracy = 60.660000000000004%, Loss = 0.5558966040611267
Epoch: 1906, Batch Gradient Norm: 6.819404669522321
Epoch: 1906, Batch Gradient Norm after: 6.819404669522321
Epoch 1907/10000, Prediction Accuracy = 60.669999999999995%, Loss = 0.5362712979316712
Epoch: 1907, Batch Gradient Norm: 14.771651704684029
Epoch: 1907, Batch Gradient Norm after: 14.771651704684029
Epoch 1908/10000, Prediction Accuracy = 60.580000000000005%, Loss = 0.5899872303009033
Epoch: 1908, Batch Gradient Norm: 11.553049156995387
Epoch: 1908, Batch Gradient Norm after: 11.553049156995387
Epoch 1909/10000, Prediction Accuracy = 60.8%, Loss = 0.5634686470031738
Epoch: 1909, Batch Gradient Norm: 9.9098747407674
Epoch: 1909, Batch Gradient Norm after: 9.9098747407674
Epoch 1910/10000, Prediction Accuracy = 60.652%, Loss = 0.5521828770637512
Epoch: 1910, Batch Gradient Norm: 10.024513494447751
Epoch: 1910, Batch Gradient Norm after: 10.024513494447751
Epoch 1911/10000, Prediction Accuracy = 60.73%, Loss = 0.5543083906173706
Epoch: 1911, Batch Gradient Norm: 9.13200761659573
Epoch: 1911, Batch Gradient Norm after: 9.13200761659573
Epoch 1912/10000, Prediction Accuracy = 60.664%, Loss = 0.5474704146385193
Epoch: 1912, Batch Gradient Norm: 11.418620111662966
Epoch: 1912, Batch Gradient Norm after: 11.418620111662966
Epoch 1913/10000, Prediction Accuracy = 60.612%, Loss = 0.5653877854347229
Epoch: 1913, Batch Gradient Norm: 12.331258638595592
Epoch: 1913, Batch Gradient Norm after: 12.331258638595592
Epoch 1914/10000, Prediction Accuracy = 60.718%, Loss = 0.5658661484718323
Epoch: 1914, Batch Gradient Norm: 13.450133938499755
Epoch: 1914, Batch Gradient Norm after: 13.450133938499755
Epoch 1915/10000, Prediction Accuracy = 60.662%, Loss = 0.5748277425765991
Epoch: 1915, Batch Gradient Norm: 10.512030507105685
Epoch: 1915, Batch Gradient Norm after: 10.512030507105685
Epoch 1916/10000, Prediction Accuracy = 60.602%, Loss = 0.5563608765602112
Epoch: 1916, Batch Gradient Norm: 11.528559007599952
Epoch: 1916, Batch Gradient Norm after: 11.528559007599952
Epoch 1917/10000, Prediction Accuracy = 60.668000000000006%, Loss = 0.5607845425605774
Epoch: 1917, Batch Gradient Norm: 11.461457359217226
Epoch: 1917, Batch Gradient Norm after: 11.461457359217226
Epoch 1918/10000, Prediction Accuracy = 60.678%, Loss = 0.5584256768226623
Epoch: 1918, Batch Gradient Norm: 12.300630494291108
Epoch: 1918, Batch Gradient Norm after: 12.300630494291108
Epoch 1919/10000, Prediction Accuracy = 60.75%, Loss = 0.5666976809501648
Epoch: 1919, Batch Gradient Norm: 10.749204636095884
Epoch: 1919, Batch Gradient Norm after: 10.749204636095884
Epoch 1920/10000, Prediction Accuracy = 60.634%, Loss = 0.5557625412940979
Epoch: 1920, Batch Gradient Norm: 8.845356834372051
Epoch: 1920, Batch Gradient Norm after: 8.845356834372051
Epoch 1921/10000, Prediction Accuracy = 60.584%, Loss = 0.5448793411254883
Epoch: 1921, Batch Gradient Norm: 11.409196862729567
Epoch: 1921, Batch Gradient Norm after: 11.409196862729567
Epoch 1922/10000, Prediction Accuracy = 60.641999999999996%, Loss = 0.569375479221344
Epoch: 1922, Batch Gradient Norm: 13.012669987222514
Epoch: 1922, Batch Gradient Norm after: 13.012669987222514
Epoch 1923/10000, Prediction Accuracy = 60.55400000000001%, Loss = 0.5787374377250671
Epoch: 1923, Batch Gradient Norm: 9.96544400408449
Epoch: 1923, Batch Gradient Norm after: 9.96544400408449
Epoch 1924/10000, Prediction Accuracy = 60.73%, Loss = 0.5546552419662476
Epoch: 1924, Batch Gradient Norm: 9.096392478408774
Epoch: 1924, Batch Gradient Norm after: 9.096392478408774
Epoch 1925/10000, Prediction Accuracy = 60.598%, Loss = 0.5475179553031921
Epoch: 1925, Batch Gradient Norm: 9.102160288563782
Epoch: 1925, Batch Gradient Norm after: 9.102160288563782
Epoch 1926/10000, Prediction Accuracy = 60.61999999999999%, Loss = 0.5460146546363831
Epoch: 1926, Batch Gradient Norm: 14.68300166974809
Epoch: 1926, Batch Gradient Norm after: 14.68300166974809
Epoch 1927/10000, Prediction Accuracy = 60.664%, Loss = 0.5834791779518127
Epoch: 1927, Batch Gradient Norm: 11.487978903735858
Epoch: 1927, Batch Gradient Norm after: 11.487978903735858
Epoch 1928/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.561867094039917
Epoch: 1928, Batch Gradient Norm: 9.87907367475074
Epoch: 1928, Batch Gradient Norm after: 9.87907367475074
Epoch 1929/10000, Prediction Accuracy = 60.61%, Loss = 0.5519363760948182
Epoch: 1929, Batch Gradient Norm: 15.45732270515701
Epoch: 1929, Batch Gradient Norm after: 15.45732270515701
Epoch 1930/10000, Prediction Accuracy = 60.614%, Loss = 0.5948747515678405
Epoch: 1930, Batch Gradient Norm: 10.878951303283905
Epoch: 1930, Batch Gradient Norm after: 10.878951303283905
Epoch 1931/10000, Prediction Accuracy = 60.69200000000001%, Loss = 0.5580488324165345
Epoch: 1931, Batch Gradient Norm: 8.430452125654092
Epoch: 1931, Batch Gradient Norm after: 8.430452125654092
Epoch 1932/10000, Prediction Accuracy = 60.64%, Loss = 0.5433667421340942
Epoch: 1932, Batch Gradient Norm: 11.27373403285033
Epoch: 1932, Batch Gradient Norm after: 11.27373403285033
Epoch 1933/10000, Prediction Accuracy = 60.664%, Loss = 0.5577685236930847
Epoch: 1933, Batch Gradient Norm: 13.079100308877667
Epoch: 1933, Batch Gradient Norm after: 13.079100308877667
Epoch 1934/10000, Prediction Accuracy = 60.696000000000005%, Loss = 0.5688141942024231
Epoch: 1934, Batch Gradient Norm: 10.996522140549416
Epoch: 1934, Batch Gradient Norm after: 10.996522140549416
Epoch 1935/10000, Prediction Accuracy = 60.644000000000005%, Loss = 0.5562607288360596
Epoch: 1935, Batch Gradient Norm: 9.94483212623886
Epoch: 1935, Batch Gradient Norm after: 9.94483212623886
Epoch 1936/10000, Prediction Accuracy = 60.626%, Loss = 0.5524686932563782
Epoch: 1936, Batch Gradient Norm: 9.611733558594187
Epoch: 1936, Batch Gradient Norm after: 9.611733558594187
Epoch 1937/10000, Prediction Accuracy = 60.64%, Loss = 0.5492159008979798
Epoch: 1937, Batch Gradient Norm: 10.405968527865038
Epoch: 1937, Batch Gradient Norm after: 10.405968527865038
Epoch 1938/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5527610182762146
Epoch: 1938, Batch Gradient Norm: 10.05838834581081
Epoch: 1938, Batch Gradient Norm after: 10.05838834581081
Epoch 1939/10000, Prediction Accuracy = 60.636%, Loss = 0.5506994485855102
Epoch: 1939, Batch Gradient Norm: 12.491269085263415
Epoch: 1939, Batch Gradient Norm after: 12.491269085263415
Epoch 1940/10000, Prediction Accuracy = 60.69200000000001%, Loss = 0.5673010945320129
Epoch: 1940, Batch Gradient Norm: 14.820748472856842
Epoch: 1940, Batch Gradient Norm after: 14.820748472856842
Epoch 1941/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5849641680717468
Epoch: 1941, Batch Gradient Norm: 12.851981455467799
Epoch: 1941, Batch Gradient Norm after: 12.851981455467799
Epoch 1942/10000, Prediction Accuracy = 60.676%, Loss = 0.5654711842536926
Epoch: 1942, Batch Gradient Norm: 11.384819884560766
Epoch: 1942, Batch Gradient Norm after: 11.384819884560766
Epoch 1943/10000, Prediction Accuracy = 60.562%, Loss = 0.5562613248825073
Epoch: 1943, Batch Gradient Norm: 9.11946901001112
Epoch: 1943, Batch Gradient Norm after: 9.11946901001112
Epoch 1944/10000, Prediction Accuracy = 60.662%, Loss = 0.545243763923645
Epoch: 1944, Batch Gradient Norm: 8.523457390752338
Epoch: 1944, Batch Gradient Norm after: 8.523457390752338
Epoch 1945/10000, Prediction Accuracy = 60.70400000000001%, Loss = 0.5441954851150512
Epoch: 1945, Batch Gradient Norm: 10.882615962952356
Epoch: 1945, Batch Gradient Norm after: 10.882615962952356
Epoch 1946/10000, Prediction Accuracy = 60.67999999999999%, Loss = 0.5529404997825622
Epoch: 1946, Batch Gradient Norm: 14.834064869068317
Epoch: 1946, Batch Gradient Norm after: 14.834064869068317
Epoch 1947/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5864356517791748
Epoch: 1947, Batch Gradient Norm: 9.476189916955216
Epoch: 1947, Batch Gradient Norm after: 9.476189916955216
Epoch 1948/10000, Prediction Accuracy = 60.564%, Loss = 0.5490013480186462
Epoch: 1948, Batch Gradient Norm: 12.457376106649201
Epoch: 1948, Batch Gradient Norm after: 12.457376106649201
Epoch 1949/10000, Prediction Accuracy = 60.726%, Loss = 0.5699126362800598
Epoch: 1949, Batch Gradient Norm: 11.373977236089408
Epoch: 1949, Batch Gradient Norm after: 11.373977236089408
Epoch 1950/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5609517097473145
Epoch: 1950, Batch Gradient Norm: 12.249532584965436
Epoch: 1950, Batch Gradient Norm after: 12.249532584965436
Epoch 1951/10000, Prediction Accuracy = 60.79600000000001%, Loss = 0.5638259649276733
Epoch: 1951, Batch Gradient Norm: 10.573857609166753
Epoch: 1951, Batch Gradient Norm after: 10.573857609166753
Epoch 1952/10000, Prediction Accuracy = 60.738%, Loss = 0.5536254286766052
Epoch: 1952, Batch Gradient Norm: 9.825540913961472
Epoch: 1952, Batch Gradient Norm after: 9.825540913961472
Epoch 1953/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5489413142204285
Epoch: 1953, Batch Gradient Norm: 10.345871023157228
Epoch: 1953, Batch Gradient Norm after: 10.345871023157228
Epoch 1954/10000, Prediction Accuracy = 60.682%, Loss = 0.553318977355957
Epoch: 1954, Batch Gradient Norm: 10.222326354923672
Epoch: 1954, Batch Gradient Norm after: 10.222326354923672
Epoch 1955/10000, Prediction Accuracy = 60.727999999999994%, Loss = 0.5501812815666198
Epoch: 1955, Batch Gradient Norm: 11.753353824709693
Epoch: 1955, Batch Gradient Norm after: 11.753353824709693
Epoch 1956/10000, Prediction Accuracy = 60.75599999999999%, Loss = 0.5637611865997314
Epoch: 1956, Batch Gradient Norm: 12.829523224772604
Epoch: 1956, Batch Gradient Norm after: 12.829523224772604
Epoch 1957/10000, Prediction Accuracy = 60.666%, Loss = 0.5715898156166077
Epoch: 1957, Batch Gradient Norm: 13.036497601388998
Epoch: 1957, Batch Gradient Norm after: 13.036497601388998
Epoch 1958/10000, Prediction Accuracy = 60.64399999999999%, Loss = 0.5719316482543946
Epoch: 1958, Batch Gradient Norm: 8.463976093536012
Epoch: 1958, Batch Gradient Norm after: 8.463976093536012
Epoch 1959/10000, Prediction Accuracy = 60.7%, Loss = 0.538085663318634
Epoch: 1959, Batch Gradient Norm: 7.9844967917743865
Epoch: 1959, Batch Gradient Norm after: 7.9844967917743865
Epoch 1960/10000, Prediction Accuracy = 60.75%, Loss = 0.5397414565086365
Epoch: 1960, Batch Gradient Norm: 8.57598255339282
Epoch: 1960, Batch Gradient Norm after: 8.57598255339282
Epoch 1961/10000, Prediction Accuracy = 60.774%, Loss = 0.5368774890899658
Epoch: 1961, Batch Gradient Norm: 13.769309772338143
Epoch: 1961, Batch Gradient Norm after: 13.769309772338143
Epoch 1962/10000, Prediction Accuracy = 60.726%, Loss = 0.5748327732086181
Epoch: 1962, Batch Gradient Norm: 12.291906493082712
Epoch: 1962, Batch Gradient Norm after: 12.291906493082712
Epoch 1963/10000, Prediction Accuracy = 60.698%, Loss = 0.5615379095077515
Epoch: 1963, Batch Gradient Norm: 11.673467289650159
Epoch: 1963, Batch Gradient Norm after: 11.673467289650159
Epoch 1964/10000, Prediction Accuracy = 60.73%, Loss = 0.5643022537231446
Epoch: 1964, Batch Gradient Norm: 7.367018486225989
Epoch: 1964, Batch Gradient Norm after: 7.367018486225989
Epoch 1965/10000, Prediction Accuracy = 60.742%, Loss = 0.5372333288192749
Epoch: 1965, Batch Gradient Norm: 9.814958008442082
Epoch: 1965, Batch Gradient Norm after: 9.814958008442082
Epoch 1966/10000, Prediction Accuracy = 60.722%, Loss = 0.5469044327735901
Epoch: 1966, Batch Gradient Norm: 11.446872586161879
Epoch: 1966, Batch Gradient Norm after: 11.446872586161879
Epoch 1967/10000, Prediction Accuracy = 60.702%, Loss = 0.5585399985313415
Epoch: 1967, Batch Gradient Norm: 12.487220642104615
Epoch: 1967, Batch Gradient Norm after: 12.487220642104615
Epoch 1968/10000, Prediction Accuracy = 60.646%, Loss = 0.56974778175354
Epoch: 1968, Batch Gradient Norm: 12.158045425849565
Epoch: 1968, Batch Gradient Norm after: 12.158045425849565
Epoch 1969/10000, Prediction Accuracy = 60.812%, Loss = 0.5633803486824036
Epoch: 1969, Batch Gradient Norm: 14.392403630756471
Epoch: 1969, Batch Gradient Norm after: 14.392403630756471
Epoch 1970/10000, Prediction Accuracy = 60.802%, Loss = 0.5773984313011169
Epoch: 1970, Batch Gradient Norm: 13.153793123401167
Epoch: 1970, Batch Gradient Norm after: 13.153793123401167
Epoch 1971/10000, Prediction Accuracy = 60.632000000000005%, Loss = 0.5725093841552734
Epoch: 1971, Batch Gradient Norm: 8.326501971522648
Epoch: 1971, Batch Gradient Norm after: 8.326501971522648
Epoch 1972/10000, Prediction Accuracy = 60.684000000000005%, Loss = 0.5409073948860168
Epoch: 1972, Batch Gradient Norm: 9.091391141190586
Epoch: 1972, Batch Gradient Norm after: 9.091391141190586
Epoch 1973/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5436793684959411
Epoch: 1973, Batch Gradient Norm: 11.39033072947841
Epoch: 1973, Batch Gradient Norm after: 11.39033072947841
Epoch 1974/10000, Prediction Accuracy = 60.648%, Loss = 0.5562596797943116
Epoch: 1974, Batch Gradient Norm: 12.605572678557492
Epoch: 1974, Batch Gradient Norm after: 12.605572678557492
Epoch 1975/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.5696197390556336
Epoch: 1975, Batch Gradient Norm: 11.383766408597056
Epoch: 1975, Batch Gradient Norm after: 11.383766408597056
Epoch 1976/10000, Prediction Accuracy = 60.730000000000004%, Loss = 0.5562767624855042
Epoch: 1976, Batch Gradient Norm: 12.509776745093504
Epoch: 1976, Batch Gradient Norm after: 12.509776745093504
Epoch 1977/10000, Prediction Accuracy = 60.69%, Loss = 0.5662100791931153
Epoch: 1977, Batch Gradient Norm: 11.499451945480601
Epoch: 1977, Batch Gradient Norm after: 11.499451945480601
Epoch 1978/10000, Prediction Accuracy = 60.678%, Loss = 0.5579640030860901
Epoch: 1978, Batch Gradient Norm: 12.699858731993741
Epoch: 1978, Batch Gradient Norm after: 12.699858731993741
Epoch 1979/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.5701219797134399
Epoch: 1979, Batch Gradient Norm: 8.681788007757122
Epoch: 1979, Batch Gradient Norm after: 8.681788007757122
Epoch 1980/10000, Prediction Accuracy = 60.67199999999999%, Loss = 0.5404143095016479
Epoch: 1980, Batch Gradient Norm: 8.69406447707208
Epoch: 1980, Batch Gradient Norm after: 8.69406447707208
Epoch 1981/10000, Prediction Accuracy = 60.712%, Loss = 0.5410132765769958
Epoch: 1981, Batch Gradient Norm: 9.470914388352025
Epoch: 1981, Batch Gradient Norm after: 9.470914388352025
Epoch 1982/10000, Prediction Accuracy = 60.64200000000001%, Loss = 0.5471696257591248
Epoch: 1982, Batch Gradient Norm: 9.885996311933914
Epoch: 1982, Batch Gradient Norm after: 9.885996311933914
Epoch 1983/10000, Prediction Accuracy = 60.769999999999996%, Loss = 0.5477678775787354
Epoch: 1983, Batch Gradient Norm: 12.106598561112458
Epoch: 1983, Batch Gradient Norm after: 12.106598561112458
Epoch 1984/10000, Prediction Accuracy = 60.708000000000006%, Loss = 0.5640852928161622
Epoch: 1984, Batch Gradient Norm: 10.06360626758357
Epoch: 1984, Batch Gradient Norm after: 10.06360626758357
Epoch 1985/10000, Prediction Accuracy = 60.77%, Loss = 0.5462350368499755
Epoch: 1985, Batch Gradient Norm: 10.151506473717875
Epoch: 1985, Batch Gradient Norm after: 10.151506473717875
Epoch 1986/10000, Prediction Accuracy = 60.617999999999995%, Loss = 0.5485825777053833
Epoch: 1986, Batch Gradient Norm: 10.573345728314894
Epoch: 1986, Batch Gradient Norm after: 10.573345728314894
Epoch 1987/10000, Prediction Accuracy = 60.85%, Loss = 0.5510339617729187
Epoch: 1987, Batch Gradient Norm: 11.350080714787728
Epoch: 1987, Batch Gradient Norm after: 11.350080714787728
Epoch 1988/10000, Prediction Accuracy = 60.702%, Loss = 0.5613936424255371
Epoch: 1988, Batch Gradient Norm: 9.442619959436477
Epoch: 1988, Batch Gradient Norm after: 9.442619959436477
Epoch 1989/10000, Prediction Accuracy = 60.676%, Loss = 0.543644642829895
Epoch: 1989, Batch Gradient Norm: 14.580354602110274
Epoch: 1989, Batch Gradient Norm after: 14.580354602110274
Epoch 1990/10000, Prediction Accuracy = 60.69%, Loss = 0.5766753196716309
Epoch: 1990, Batch Gradient Norm: 11.881788049754496
Epoch: 1990, Batch Gradient Norm after: 11.881788049754496
Epoch 1991/10000, Prediction Accuracy = 60.706%, Loss = 0.5602412939071655
Epoch: 1991, Batch Gradient Norm: 11.47396418012257
Epoch: 1991, Batch Gradient Norm after: 11.47396418012257
Epoch 1992/10000, Prediction Accuracy = 60.71%, Loss = 0.5591339707374573
Epoch: 1992, Batch Gradient Norm: 11.660254510905872
Epoch: 1992, Batch Gradient Norm after: 11.660254510905872
Epoch 1993/10000, Prediction Accuracy = 60.717999999999996%, Loss = 0.5549209117889404
Epoch: 1993, Batch Gradient Norm: 12.34740106222501
Epoch: 1993, Batch Gradient Norm after: 12.34740106222501
Epoch 1994/10000, Prediction Accuracy = 60.712%, Loss = 0.5603788375854493
Epoch: 1994, Batch Gradient Norm: 12.046966815278978
Epoch: 1994, Batch Gradient Norm after: 12.046966815278978
Epoch 1995/10000, Prediction Accuracy = 60.836%, Loss = 0.5578203558921814
Epoch: 1995, Batch Gradient Norm: 10.845118373268262
Epoch: 1995, Batch Gradient Norm after: 10.845118373268262
Epoch 1996/10000, Prediction Accuracy = 60.78000000000001%, Loss = 0.5534831285476685
Epoch: 1996, Batch Gradient Norm: 9.87618208611308
Epoch: 1996, Batch Gradient Norm after: 9.87618208611308
Epoch 1997/10000, Prediction Accuracy = 60.7%, Loss = 0.5474458575248718
Epoch: 1997, Batch Gradient Norm: 10.186112257862225
Epoch: 1997, Batch Gradient Norm after: 10.186112257862225
Epoch 1998/10000, Prediction Accuracy = 60.79%, Loss = 0.547556459903717
Epoch: 1998, Batch Gradient Norm: 9.995127035796344
Epoch: 1998, Batch Gradient Norm after: 9.995127035796344
Epoch 1999/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5459649443626404
Epoch: 1999, Batch Gradient Norm: 12.153043722042694
Epoch: 1999, Batch Gradient Norm after: 12.153043722042694
Epoch 2000/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.5677857160568237
Epoch: 2000, Batch Gradient Norm: 10.73095368441414
Epoch: 2000, Batch Gradient Norm after: 10.73095368441414
Epoch 2001/10000, Prediction Accuracy = 60.732000000000006%, Loss = 0.5508442640304565
Epoch: 2001, Batch Gradient Norm: 11.238745893510428
Epoch: 2001, Batch Gradient Norm after: 11.238745893510428
Epoch 2002/10000, Prediction Accuracy = 60.822%, Loss = 0.5541943430900573
Epoch: 2002, Batch Gradient Norm: 8.06089366287089
Epoch: 2002, Batch Gradient Norm after: 8.06089366287089
Epoch 2003/10000, Prediction Accuracy = 60.666%, Loss = 0.5376371145248413
Epoch: 2003, Batch Gradient Norm: 12.291326236845844
Epoch: 2003, Batch Gradient Norm after: 12.291326236845844
Epoch 2004/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5708478331565857
Epoch: 2004, Batch Gradient Norm: 9.542269974545018
Epoch: 2004, Batch Gradient Norm after: 9.542269974545018
Epoch 2005/10000, Prediction Accuracy = 60.739999999999995%, Loss = 0.5438492298126221
Epoch: 2005, Batch Gradient Norm: 7.52845598978277
Epoch: 2005, Batch Gradient Norm after: 7.52845598978277
Epoch 2006/10000, Prediction Accuracy = 60.722%, Loss = 0.5316756963729858
Epoch: 2006, Batch Gradient Norm: 10.353216446028641
Epoch: 2006, Batch Gradient Norm after: 10.353216446028641
Epoch 2007/10000, Prediction Accuracy = 60.712%, Loss = 0.5518675327301026
Epoch: 2007, Batch Gradient Norm: 13.545422007785028
Epoch: 2007, Batch Gradient Norm after: 13.545422007785028
Epoch 2008/10000, Prediction Accuracy = 60.852%, Loss = 0.5721157312393188
Epoch: 2008, Batch Gradient Norm: 16.08800094132832
Epoch: 2008, Batch Gradient Norm after: 16.08800094132832
Epoch 2009/10000, Prediction Accuracy = 60.626%, Loss = 0.5962382793426514
Epoch: 2009, Batch Gradient Norm: 11.776199100546288
Epoch: 2009, Batch Gradient Norm after: 11.776199100546288
Epoch 2010/10000, Prediction Accuracy = 60.738%, Loss = 0.5569798707962036
Epoch: 2010, Batch Gradient Norm: 11.15313529679456
Epoch: 2010, Batch Gradient Norm after: 11.15313529679456
Epoch 2011/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.559092378616333
Epoch: 2011, Batch Gradient Norm: 10.110039567424808
Epoch: 2011, Batch Gradient Norm after: 10.110039567424808
Epoch 2012/10000, Prediction Accuracy = 60.678%, Loss = 0.5466640949249267
Epoch: 2012, Batch Gradient Norm: 9.694603526142592
Epoch: 2012, Batch Gradient Norm after: 9.694603526142592
Epoch 2013/10000, Prediction Accuracy = 60.812%, Loss = 0.5430172920227051
Epoch: 2013, Batch Gradient Norm: 10.512940410040569
Epoch: 2013, Batch Gradient Norm after: 10.512940410040569
Epoch 2014/10000, Prediction Accuracy = 60.724000000000004%, Loss = 0.5488812327384949
Epoch: 2014, Batch Gradient Norm: 11.547139139662466
Epoch: 2014, Batch Gradient Norm after: 11.547139139662466
Epoch 2015/10000, Prediction Accuracy = 60.61%, Loss = 0.5587882041931153
Epoch: 2015, Batch Gradient Norm: 11.266384954727355
Epoch: 2015, Batch Gradient Norm after: 11.266384954727355
Epoch 2016/10000, Prediction Accuracy = 60.7%, Loss = 0.5542317628860474
Epoch: 2016, Batch Gradient Norm: 11.820882191433174
Epoch: 2016, Batch Gradient Norm after: 11.820882191433174
Epoch 2017/10000, Prediction Accuracy = 60.69000000000001%, Loss = 0.5611421227455139
Epoch: 2017, Batch Gradient Norm: 10.346662157708481
Epoch: 2017, Batch Gradient Norm after: 10.346662157708481
Epoch 2018/10000, Prediction Accuracy = 60.75%, Loss = 0.5483080506324768
Epoch: 2018, Batch Gradient Norm: 10.424674900461307
Epoch: 2018, Batch Gradient Norm after: 10.424674900461307
Epoch 2019/10000, Prediction Accuracy = 60.734%, Loss = 0.5479547858238221
Epoch: 2019, Batch Gradient Norm: 15.785867681888215
Epoch: 2019, Batch Gradient Norm after: 15.785867681888215
Epoch 2020/10000, Prediction Accuracy = 60.65599999999999%, Loss = 0.5910868287086487
Epoch: 2020, Batch Gradient Norm: 12.721534506444373
Epoch: 2020, Batch Gradient Norm after: 12.721534506444373
Epoch 2021/10000, Prediction Accuracy = 60.63399999999999%, Loss = 0.5716125726699829
Epoch: 2021, Batch Gradient Norm: 11.58696211778277
Epoch: 2021, Batch Gradient Norm after: 11.58696211778277
Epoch 2022/10000, Prediction Accuracy = 60.67%, Loss = 0.5547133564949036
Epoch: 2022, Batch Gradient Norm: 11.793443063874053
Epoch: 2022, Batch Gradient Norm after: 11.793443063874053
Epoch 2023/10000, Prediction Accuracy = 60.77%, Loss = 0.5528729081153869
Epoch: 2023, Batch Gradient Norm: 11.3627588018973
Epoch: 2023, Batch Gradient Norm after: 11.3627588018973
Epoch 2024/10000, Prediction Accuracy = 60.854%, Loss = 0.5532081246376037
Epoch: 2024, Batch Gradient Norm: 11.335877841076329
Epoch: 2024, Batch Gradient Norm after: 11.335877841076329
Epoch 2025/10000, Prediction Accuracy = 60.73199999999999%, Loss = 0.5548133373260498
Epoch: 2025, Batch Gradient Norm: 9.286647676505678
Epoch: 2025, Batch Gradient Norm after: 9.286647676505678
Epoch 2026/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.5424934148788452
Epoch: 2026, Batch Gradient Norm: 8.148272174589307
Epoch: 2026, Batch Gradient Norm after: 8.148272174589307
Epoch 2027/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.5359567523002624
Epoch: 2027, Batch Gradient Norm: 10.164400907573986
Epoch: 2027, Batch Gradient Norm after: 10.164400907573986
Epoch 2028/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.5464666843414306
Epoch: 2028, Batch Gradient Norm: 10.077710923570688
Epoch: 2028, Batch Gradient Norm after: 10.077710923570688
Epoch 2029/10000, Prediction Accuracy = 60.732000000000006%, Loss = 0.5486145257949829
Epoch: 2029, Batch Gradient Norm: 8.625716313453049
Epoch: 2029, Batch Gradient Norm after: 8.625716313453049
Epoch 2030/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.5395016551017762
Epoch: 2030, Batch Gradient Norm: 14.377193746775971
Epoch: 2030, Batch Gradient Norm after: 14.377193746775971
Epoch 2031/10000, Prediction Accuracy = 60.760000000000005%, Loss = 0.5751571655273438
Epoch: 2031, Batch Gradient Norm: 10.52260529494781
Epoch: 2031, Batch Gradient Norm after: 10.52260529494781
Epoch 2032/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.5523999333381653
Epoch: 2032, Batch Gradient Norm: 7.898800752707713
Epoch: 2032, Batch Gradient Norm after: 7.898800752707713
Epoch 2033/10000, Prediction Accuracy = 60.77%, Loss = 0.5339962720870972
Epoch: 2033, Batch Gradient Norm: 8.617424849433458
Epoch: 2033, Batch Gradient Norm after: 8.617424849433458
Epoch 2034/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.5389343976974488
Epoch: 2034, Batch Gradient Norm: 9.684297334049406
Epoch: 2034, Batch Gradient Norm after: 9.684297334049406
Epoch 2035/10000, Prediction Accuracy = 60.727999999999994%, Loss = 0.5444445252418518
Epoch: 2035, Batch Gradient Norm: 15.60133950360215
Epoch: 2035, Batch Gradient Norm after: 15.60133950360215
Epoch 2036/10000, Prediction Accuracy = 60.738%, Loss = 0.5910870432853699
Epoch: 2036, Batch Gradient Norm: 14.584639591780135
Epoch: 2036, Batch Gradient Norm after: 14.584639591780135
Epoch 2037/10000, Prediction Accuracy = 60.702%, Loss = 0.5846629977226258
Epoch: 2037, Batch Gradient Norm: 9.715308528785533
Epoch: 2037, Batch Gradient Norm after: 9.715308528785533
Epoch 2038/10000, Prediction Accuracy = 60.678%, Loss = 0.5457642793655395
Epoch: 2038, Batch Gradient Norm: 8.874910633013995
Epoch: 2038, Batch Gradient Norm after: 8.874910633013995
Epoch 2039/10000, Prediction Accuracy = 60.722%, Loss = 0.5398864388465882
Epoch: 2039, Batch Gradient Norm: 14.206204786240557
Epoch: 2039, Batch Gradient Norm after: 14.206204786240557
Epoch 2040/10000, Prediction Accuracy = 60.68000000000001%, Loss = 0.5756727457046509
Epoch: 2040, Batch Gradient Norm: 13.758573294847832
Epoch: 2040, Batch Gradient Norm after: 13.758573294847832
Epoch 2041/10000, Prediction Accuracy = 60.742%, Loss = 0.5738508343696594
Epoch: 2041, Batch Gradient Norm: 10.756033437999541
Epoch: 2041, Batch Gradient Norm after: 10.756033437999541
Epoch 2042/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.5502886533737182
Epoch: 2042, Batch Gradient Norm: 9.655579762045319
Epoch: 2042, Batch Gradient Norm after: 9.655579762045319
Epoch 2043/10000, Prediction Accuracy = 60.74400000000001%, Loss = 0.5425376892089844
Epoch: 2043, Batch Gradient Norm: 11.174756670110702
Epoch: 2043, Batch Gradient Norm after: 11.174756670110702
Epoch 2044/10000, Prediction Accuracy = 60.568%, Loss = 0.5541392087936401
Epoch: 2044, Batch Gradient Norm: 11.336969927956712
Epoch: 2044, Batch Gradient Norm after: 11.336969927956712
Epoch 2045/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.5518914103507996
Epoch: 2045, Batch Gradient Norm: 10.691516689478032
Epoch: 2045, Batch Gradient Norm after: 10.691516689478032
Epoch 2046/10000, Prediction Accuracy = 60.778%, Loss = 0.5508411645889282
Epoch: 2046, Batch Gradient Norm: 8.903158079824202
Epoch: 2046, Batch Gradient Norm after: 8.903158079824202
Epoch 2047/10000, Prediction Accuracy = 60.709999999999994%, Loss = 0.5436731696128845
Epoch: 2047, Batch Gradient Norm: 7.105011004861195
Epoch: 2047, Batch Gradient Norm after: 7.105011004861195
Epoch 2048/10000, Prediction Accuracy = 60.79600000000001%, Loss = 0.5274485945701599
Epoch: 2048, Batch Gradient Norm: 10.842212403120534
Epoch: 2048, Batch Gradient Norm after: 10.842212403120534
Epoch 2049/10000, Prediction Accuracy = 60.74399999999999%, Loss = 0.5456060767173767
Epoch: 2049, Batch Gradient Norm: 14.26333988675791
Epoch: 2049, Batch Gradient Norm after: 14.26333988675791
Epoch 2050/10000, Prediction Accuracy = 60.708000000000006%, Loss = 0.5741043806076049
Epoch: 2050, Batch Gradient Norm: 13.585908865688804
Epoch: 2050, Batch Gradient Norm after: 13.585908865688804
Epoch 2051/10000, Prediction Accuracy = 60.762%, Loss = 0.5734423995018005
Epoch: 2051, Batch Gradient Norm: 9.890749873369295
Epoch: 2051, Batch Gradient Norm after: 9.890749873369295
Epoch 2052/10000, Prediction Accuracy = 60.70799999999999%, Loss = 0.5454854846000672
Epoch: 2052, Batch Gradient Norm: 10.678122276516973
Epoch: 2052, Batch Gradient Norm after: 10.678122276516973
Epoch 2053/10000, Prediction Accuracy = 60.71999999999999%, Loss = 0.5478712916374207
Epoch: 2053, Batch Gradient Norm: 11.351476983952262
Epoch: 2053, Batch Gradient Norm after: 11.351476983952262
Epoch 2054/10000, Prediction Accuracy = 60.733999999999995%, Loss = 0.5522307872772216
Epoch: 2054, Batch Gradient Norm: 10.402840548352996
Epoch: 2054, Batch Gradient Norm after: 10.402840548352996
Epoch 2055/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.5487321138381958
Epoch: 2055, Batch Gradient Norm: 11.924375507492362
Epoch: 2055, Batch Gradient Norm after: 11.924375507492362
Epoch 2056/10000, Prediction Accuracy = 60.778%, Loss = 0.5611695528030396
Epoch: 2056, Batch Gradient Norm: 12.667541183365243
Epoch: 2056, Batch Gradient Norm after: 12.667541183365243
Epoch 2057/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.564584481716156
Epoch: 2057, Batch Gradient Norm: 12.093504015533872
Epoch: 2057, Batch Gradient Norm after: 12.093504015533872
Epoch 2058/10000, Prediction Accuracy = 60.736000000000004%, Loss = 0.5565017580986023
Epoch: 2058, Batch Gradient Norm: 10.881840493114325
Epoch: 2058, Batch Gradient Norm after: 10.881840493114325
Epoch 2059/10000, Prediction Accuracy = 60.866%, Loss = 0.5490166664123535
Epoch: 2059, Batch Gradient Norm: 7.975056333070376
Epoch: 2059, Batch Gradient Norm after: 7.975056333070376
Epoch 2060/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.5316019535064698
Epoch: 2060, Batch Gradient Norm: 9.681149717953762
Epoch: 2060, Batch Gradient Norm after: 9.681149717953762
Epoch 2061/10000, Prediction Accuracy = 60.81600000000001%, Loss = 0.5404279708862305
Epoch: 2061, Batch Gradient Norm: 12.228620689719222
Epoch: 2061, Batch Gradient Norm after: 12.228620689719222
Epoch 2062/10000, Prediction Accuracy = 60.876%, Loss = 0.5593062281608582
Epoch: 2062, Batch Gradient Norm: 11.733058083326558
Epoch: 2062, Batch Gradient Norm after: 11.733058083326558
Epoch 2063/10000, Prediction Accuracy = 60.824%, Loss = 0.5557035684585572
Epoch: 2063, Batch Gradient Norm: 10.816443882947794
Epoch: 2063, Batch Gradient Norm after: 10.816443882947794
Epoch 2064/10000, Prediction Accuracy = 60.818%, Loss = 0.5516081571578979
Epoch: 2064, Batch Gradient Norm: 11.167437034720297
Epoch: 2064, Batch Gradient Norm after: 11.167437034720297
Epoch 2065/10000, Prediction Accuracy = 60.698%, Loss = 0.5482842683792114
Epoch: 2065, Batch Gradient Norm: 12.86661305303516
Epoch: 2065, Batch Gradient Norm after: 12.86661305303516
Epoch 2066/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.5596936821937561
Epoch: 2066, Batch Gradient Norm: 10.626646295601047
Epoch: 2066, Batch Gradient Norm after: 10.626646295601047
Epoch 2067/10000, Prediction Accuracy = 60.666%, Loss = 0.5462287545204163
Epoch: 2067, Batch Gradient Norm: 12.809191216293195
Epoch: 2067, Batch Gradient Norm after: 12.809191216293195
Epoch 2068/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.5651286602020263
Epoch: 2068, Batch Gradient Norm: 11.058316414759872
Epoch: 2068, Batch Gradient Norm after: 11.058316414759872
Epoch 2069/10000, Prediction Accuracy = 60.732000000000006%, Loss = 0.5520513534545899
Epoch: 2069, Batch Gradient Norm: 10.926044345170594
Epoch: 2069, Batch Gradient Norm after: 10.926044345170594
Epoch 2070/10000, Prediction Accuracy = 60.742%, Loss = 0.546009647846222
Epoch: 2070, Batch Gradient Norm: 11.212728582912503
Epoch: 2070, Batch Gradient Norm after: 11.212728582912503
Epoch 2071/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.54694664478302
Epoch: 2071, Batch Gradient Norm: 10.32600426380653
Epoch: 2071, Batch Gradient Norm after: 10.32600426380653
Epoch 2072/10000, Prediction Accuracy = 60.722%, Loss = 0.5446015357971191
Epoch: 2072, Batch Gradient Norm: 8.886476491141735
Epoch: 2072, Batch Gradient Norm after: 8.886476491141735
Epoch 2073/10000, Prediction Accuracy = 60.766000000000005%, Loss = 0.5363186597824097
Epoch: 2073, Batch Gradient Norm: 8.279228619947846
Epoch: 2073, Batch Gradient Norm after: 8.279228619947846
Epoch 2074/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.5320796608924866
Epoch: 2074, Batch Gradient Norm: 14.749811144521669
Epoch: 2074, Batch Gradient Norm after: 14.749811144521669
Epoch 2075/10000, Prediction Accuracy = 60.65%, Loss = 0.5806294798851013
Epoch: 2075, Batch Gradient Norm: 12.228173040872699
Epoch: 2075, Batch Gradient Norm after: 12.228173040872699
Epoch 2076/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.5601274371147156
Epoch: 2076, Batch Gradient Norm: 9.178820754511673
Epoch: 2076, Batch Gradient Norm after: 9.178820754511673
Epoch 2077/10000, Prediction Accuracy = 60.812%, Loss = 0.5402477145195007
Epoch: 2077, Batch Gradient Norm: 12.24565781668792
Epoch: 2077, Batch Gradient Norm after: 12.24565781668792
Epoch 2078/10000, Prediction Accuracy = 60.763999999999996%, Loss = 0.5697417497634888
Epoch: 2078, Batch Gradient Norm: 9.904739818402755
Epoch: 2078, Batch Gradient Norm after: 9.904739818402755
Epoch 2079/10000, Prediction Accuracy = 60.757999999999996%, Loss = 0.5433921933174133
Epoch: 2079, Batch Gradient Norm: 9.139314267794905
Epoch: 2079, Batch Gradient Norm after: 9.139314267794905
Epoch 2080/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.5398492813110352
Epoch: 2080, Batch Gradient Norm: 9.869776882532957
Epoch: 2080, Batch Gradient Norm after: 9.869776882532957
Epoch 2081/10000, Prediction Accuracy = 60.8%, Loss = 0.542136287689209
Epoch: 2081, Batch Gradient Norm: 13.198540686552526
Epoch: 2081, Batch Gradient Norm after: 13.198540686552526
Epoch 2082/10000, Prediction Accuracy = 60.694%, Loss = 0.5660773634910583
Epoch: 2082, Batch Gradient Norm: 13.23385756861442
Epoch: 2082, Batch Gradient Norm after: 13.23385756861442
Epoch 2083/10000, Prediction Accuracy = 60.760000000000005%, Loss = 0.5648543238639832
Epoch: 2083, Batch Gradient Norm: 12.232329250636667
Epoch: 2083, Batch Gradient Norm after: 12.232329250636667
Epoch 2084/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.5651665806770325
Epoch: 2084, Batch Gradient Norm: 9.493290259114262
Epoch: 2084, Batch Gradient Norm after: 9.493290259114262
Epoch 2085/10000, Prediction Accuracy = 60.714%, Loss = 0.5404377460479737
Epoch: 2085, Batch Gradient Norm: 9.271401947455374
Epoch: 2085, Batch Gradient Norm after: 9.271401947455374
Epoch 2086/10000, Prediction Accuracy = 60.77%, Loss = 0.5403241991996766
Epoch: 2086, Batch Gradient Norm: 8.252028906071565
Epoch: 2086, Batch Gradient Norm after: 8.252028906071565
Epoch 2087/10000, Prediction Accuracy = 60.874%, Loss = 0.5340524196624756
Epoch: 2087, Batch Gradient Norm: 12.358244551011197
Epoch: 2087, Batch Gradient Norm after: 12.358244551011197
Epoch 2088/10000, Prediction Accuracy = 60.85%, Loss = 0.563974404335022
Epoch: 2088, Batch Gradient Norm: 12.912855321965415
Epoch: 2088, Batch Gradient Norm after: 12.912855321965415
Epoch 2089/10000, Prediction Accuracy = 60.79%, Loss = 0.5607317924499512
Epoch: 2089, Batch Gradient Norm: 9.035427767884784
Epoch: 2089, Batch Gradient Norm after: 9.035427767884784
Epoch 2090/10000, Prediction Accuracy = 60.79600000000001%, Loss = 0.5349209904670715
Epoch: 2090, Batch Gradient Norm: 11.350451253016363
Epoch: 2090, Batch Gradient Norm after: 11.350451253016363
Epoch 2091/10000, Prediction Accuracy = 60.734%, Loss = 0.5492509365081787
Epoch: 2091, Batch Gradient Norm: 14.166423416078795
Epoch: 2091, Batch Gradient Norm after: 14.166423416078795
Epoch 2092/10000, Prediction Accuracy = 60.846000000000004%, Loss = 0.569050407409668
Epoch: 2092, Batch Gradient Norm: 10.927155112404002
Epoch: 2092, Batch Gradient Norm after: 10.927155112404002
Epoch 2093/10000, Prediction Accuracy = 60.839999999999996%, Loss = 0.5478021979331971
Epoch: 2093, Batch Gradient Norm: 8.733013658957804
Epoch: 2093, Batch Gradient Norm after: 8.733013658957804
Epoch 2094/10000, Prediction Accuracy = 60.818000000000005%, Loss = 0.5366490960121155
Epoch: 2094, Batch Gradient Norm: 10.243976577024782
Epoch: 2094, Batch Gradient Norm after: 10.243976577024782
Epoch 2095/10000, Prediction Accuracy = 60.872%, Loss = 0.5420677781105041
Epoch: 2095, Batch Gradient Norm: 11.614062041068987
Epoch: 2095, Batch Gradient Norm after: 11.614062041068987
Epoch 2096/10000, Prediction Accuracy = 60.74399999999999%, Loss = 0.5533029913902283
Epoch: 2096, Batch Gradient Norm: 9.183228943510843
Epoch: 2096, Batch Gradient Norm after: 9.183228943510843
Epoch 2097/10000, Prediction Accuracy = 60.876%, Loss = 0.5350977659225464
Epoch: 2097, Batch Gradient Norm: 11.529917704134544
Epoch: 2097, Batch Gradient Norm after: 11.529917704134544
Epoch 2098/10000, Prediction Accuracy = 60.769999999999996%, Loss = 0.5585950016975403
Epoch: 2098, Batch Gradient Norm: 10.777247311061208
Epoch: 2098, Batch Gradient Norm after: 10.777247311061208
Epoch 2099/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5484669208526611
Epoch: 2099, Batch Gradient Norm: 15.164862529017515
Epoch: 2099, Batch Gradient Norm after: 15.164862529017515
Epoch 2100/10000, Prediction Accuracy = 60.862%, Loss = 0.579984474182129
Epoch: 2100, Batch Gradient Norm: 12.791028710859559
Epoch: 2100, Batch Gradient Norm after: 12.791028710859559
Epoch 2101/10000, Prediction Accuracy = 60.85%, Loss = 0.556822907924652
Epoch: 2101, Batch Gradient Norm: 12.518755753847536
Epoch: 2101, Batch Gradient Norm after: 12.518755753847536
Epoch 2102/10000, Prediction Accuracy = 60.83599999999999%, Loss = 0.5610352277755737
Epoch: 2102, Batch Gradient Norm: 12.373713686992776
Epoch: 2102, Batch Gradient Norm after: 12.373713686992776
Epoch 2103/10000, Prediction Accuracy = 60.826%, Loss = 0.5601646304130554
Epoch: 2103, Batch Gradient Norm: 9.689498079569036
Epoch: 2103, Batch Gradient Norm after: 9.689498079569036
Epoch 2104/10000, Prediction Accuracy = 60.95%, Loss = 0.5389493346214295
Epoch: 2104, Batch Gradient Norm: 12.085059521056522
Epoch: 2104, Batch Gradient Norm after: 12.085059521056522
Epoch 2105/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5554320931434631
Epoch: 2105, Batch Gradient Norm: 12.831792116904888
Epoch: 2105, Batch Gradient Norm after: 12.831792116904888
Epoch 2106/10000, Prediction Accuracy = 60.75600000000001%, Loss = 0.5624426484107972
Epoch: 2106, Batch Gradient Norm: 7.691896758248992
Epoch: 2106, Batch Gradient Norm after: 7.691896758248992
Epoch 2107/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.530539619922638
Epoch: 2107, Batch Gradient Norm: 6.4106773270650645
Epoch: 2107, Batch Gradient Norm after: 6.4106773270650645
Epoch 2108/10000, Prediction Accuracy = 60.89200000000001%, Loss = 0.5225007176399231
Epoch: 2108, Batch Gradient Norm: 9.871065168069803
Epoch: 2108, Batch Gradient Norm after: 9.871065168069803
Epoch 2109/10000, Prediction Accuracy = 60.778%, Loss = 0.5398392677307129
Epoch: 2109, Batch Gradient Norm: 11.80090779147195
Epoch: 2109, Batch Gradient Norm after: 11.80090779147195
Epoch 2110/10000, Prediction Accuracy = 60.898%, Loss = 0.5544260263442993
Epoch: 2110, Batch Gradient Norm: 8.167130820863237
Epoch: 2110, Batch Gradient Norm after: 8.167130820863237
Epoch 2111/10000, Prediction Accuracy = 60.826%, Loss = 0.5303643226623536
Epoch: 2111, Batch Gradient Norm: 12.439593110270655
Epoch: 2111, Batch Gradient Norm after: 12.439593110270655
Epoch 2112/10000, Prediction Accuracy = 60.898%, Loss = 0.5588047742843628
Epoch: 2112, Batch Gradient Norm: 12.605822479264424
Epoch: 2112, Batch Gradient Norm after: 12.605822479264424
Epoch 2113/10000, Prediction Accuracy = 60.879999999999995%, Loss = 0.5674914240837097
Epoch: 2113, Batch Gradient Norm: 8.944614840736275
Epoch: 2113, Batch Gradient Norm after: 8.944614840736275
Epoch 2114/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.5354185700416565
Epoch: 2114, Batch Gradient Norm: 13.091916015205248
Epoch: 2114, Batch Gradient Norm after: 13.091916015205248
Epoch 2115/10000, Prediction Accuracy = 60.766000000000005%, Loss = 0.5613528728485108
Epoch: 2115, Batch Gradient Norm: 12.449967465981821
Epoch: 2115, Batch Gradient Norm after: 12.449967465981821
Epoch 2116/10000, Prediction Accuracy = 60.818000000000005%, Loss = 0.5555363416671752
Epoch: 2116, Batch Gradient Norm: 10.002106094522086
Epoch: 2116, Batch Gradient Norm after: 10.002106094522086
Epoch 2117/10000, Prediction Accuracy = 60.848%, Loss = 0.5373221039772034
Epoch: 2117, Batch Gradient Norm: 11.704096221979299
Epoch: 2117, Batch Gradient Norm after: 11.704096221979299
Epoch 2118/10000, Prediction Accuracy = 60.846000000000004%, Loss = 0.5479095935821533
Epoch: 2118, Batch Gradient Norm: 11.575521783102499
Epoch: 2118, Batch Gradient Norm after: 11.575521783102499
Epoch 2119/10000, Prediction Accuracy = 60.70799999999999%, Loss = 0.5530949592590332
Epoch: 2119, Batch Gradient Norm: 8.208967580535854
Epoch: 2119, Batch Gradient Norm after: 8.208967580535854
Epoch 2120/10000, Prediction Accuracy = 60.83599999999999%, Loss = 0.5303842902183533
Epoch: 2120, Batch Gradient Norm: 9.265188596086038
Epoch: 2120, Batch Gradient Norm after: 9.265188596086038
Epoch 2121/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.5377436518669129
Epoch: 2121, Batch Gradient Norm: 10.283595646792662
Epoch: 2121, Batch Gradient Norm after: 10.283595646792662
Epoch 2122/10000, Prediction Accuracy = 60.779999999999994%, Loss = 0.5416974186897278
Epoch: 2122, Batch Gradient Norm: 13.724213503778543
Epoch: 2122, Batch Gradient Norm after: 13.724213503778543
Epoch 2123/10000, Prediction Accuracy = 60.8%, Loss = 0.5690167784690857
Epoch: 2123, Batch Gradient Norm: 13.19510780888506
Epoch: 2123, Batch Gradient Norm after: 13.19510780888506
Epoch 2124/10000, Prediction Accuracy = 60.806%, Loss = 0.5702884554862976
Epoch: 2124, Batch Gradient Norm: 9.88919042196042
Epoch: 2124, Batch Gradient Norm after: 9.88919042196042
Epoch 2125/10000, Prediction Accuracy = 60.864%, Loss = 0.5386797070503235
Epoch: 2125, Batch Gradient Norm: 11.51261218260253
Epoch: 2125, Batch Gradient Norm after: 11.51261218260253
Epoch 2126/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5494935870170593
Epoch: 2126, Batch Gradient Norm: 11.235842327413977
Epoch: 2126, Batch Gradient Norm after: 11.235842327413977
Epoch 2127/10000, Prediction Accuracy = 60.8%, Loss = 0.5459782958030701
Epoch: 2127, Batch Gradient Norm: 11.705127945491437
Epoch: 2127, Batch Gradient Norm after: 11.705127945491437
Epoch 2128/10000, Prediction Accuracy = 60.722%, Loss = 0.5507148742675781
Epoch: 2128, Batch Gradient Norm: 9.904143831305106
Epoch: 2128, Batch Gradient Norm after: 9.904143831305106
Epoch 2129/10000, Prediction Accuracy = 60.908%, Loss = 0.5390492081642151
Epoch: 2129, Batch Gradient Norm: 10.56398497535942
Epoch: 2129, Batch Gradient Norm after: 10.56398497535942
Epoch 2130/10000, Prediction Accuracy = 60.838%, Loss = 0.5448695540428161
Epoch: 2130, Batch Gradient Norm: 9.962768801653548
Epoch: 2130, Batch Gradient Norm after: 9.962768801653548
Epoch 2131/10000, Prediction Accuracy = 60.754%, Loss = 0.5422670364379882
Epoch: 2131, Batch Gradient Norm: 9.203649246754463
Epoch: 2131, Batch Gradient Norm after: 9.203649246754463
Epoch 2132/10000, Prediction Accuracy = 60.806%, Loss = 0.5354152321815491
Epoch: 2132, Batch Gradient Norm: 11.805308695257361
Epoch: 2132, Batch Gradient Norm after: 11.805308695257361
Epoch 2133/10000, Prediction Accuracy = 60.876%, Loss = 0.5491716265678406
Epoch: 2133, Batch Gradient Norm: 11.754811259593966
Epoch: 2133, Batch Gradient Norm after: 11.754811259593966
Epoch 2134/10000, Prediction Accuracy = 60.774%, Loss = 0.5506779670715332
Epoch: 2134, Batch Gradient Norm: 11.307676156377402
Epoch: 2134, Batch Gradient Norm after: 11.307676156377402
Epoch 2135/10000, Prediction Accuracy = 60.826%, Loss = 0.5464890122413635
Epoch: 2135, Batch Gradient Norm: 12.55596776636515
Epoch: 2135, Batch Gradient Norm after: 12.55596776636515
Epoch 2136/10000, Prediction Accuracy = 60.855999999999995%, Loss = 0.5516250610351563
Epoch: 2136, Batch Gradient Norm: 12.873509499082521
Epoch: 2136, Batch Gradient Norm after: 12.873509499082521
Epoch 2137/10000, Prediction Accuracy = 60.848%, Loss = 0.5541080594062805
Epoch: 2137, Batch Gradient Norm: 10.95291134032694
Epoch: 2137, Batch Gradient Norm after: 10.95291134032694
Epoch 2138/10000, Prediction Accuracy = 60.854%, Loss = 0.5464970827102661
Epoch: 2138, Batch Gradient Norm: 12.391296005844545
Epoch: 2138, Batch Gradient Norm after: 12.391296005844545
Epoch 2139/10000, Prediction Accuracy = 60.812%, Loss = 0.557211697101593
Epoch: 2139, Batch Gradient Norm: 9.249121292181202
Epoch: 2139, Batch Gradient Norm after: 9.249121292181202
Epoch 2140/10000, Prediction Accuracy = 60.742%, Loss = 0.5359313607215881
Epoch: 2140, Batch Gradient Norm: 7.179558209096946
Epoch: 2140, Batch Gradient Norm after: 7.179558209096946
Epoch 2141/10000, Prediction Accuracy = 60.831999999999994%, Loss = 0.522972309589386
Epoch: 2141, Batch Gradient Norm: 13.743765516964961
Epoch: 2141, Batch Gradient Norm after: 13.743765516964961
Epoch 2142/10000, Prediction Accuracy = 60.91799999999999%, Loss = 0.5721968293190003
Epoch: 2142, Batch Gradient Norm: 9.839010943919694
Epoch: 2142, Batch Gradient Norm after: 9.839010943919694
Epoch 2143/10000, Prediction Accuracy = 60.912%, Loss = 0.5400087714195252
Epoch: 2143, Batch Gradient Norm: 9.387821493348131
Epoch: 2143, Batch Gradient Norm after: 9.387821493348131
Epoch 2144/10000, Prediction Accuracy = 60.862%, Loss = 0.5388892412185669
Epoch: 2144, Batch Gradient Norm: 9.457870308130248
Epoch: 2144, Batch Gradient Norm after: 9.457870308130248
Epoch 2145/10000, Prediction Accuracy = 60.87800000000001%, Loss = 0.534614086151123
Epoch: 2145, Batch Gradient Norm: 10.908190134331269
Epoch: 2145, Batch Gradient Norm after: 10.908190134331269
Epoch 2146/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.5472920179367066
Epoch: 2146, Batch Gradient Norm: 11.958571179244279
Epoch: 2146, Batch Gradient Norm after: 11.958571179244279
Epoch 2147/10000, Prediction Accuracy = 60.944%, Loss = 0.5489640593528747
Epoch: 2147, Batch Gradient Norm: 13.413316146624972
Epoch: 2147, Batch Gradient Norm after: 13.413316146624972
Epoch 2148/10000, Prediction Accuracy = 60.872%, Loss = 0.5668540954589844
Epoch: 2148, Batch Gradient Norm: 7.135937549927818
Epoch: 2148, Batch Gradient Norm after: 7.135937549927818
Epoch 2149/10000, Prediction Accuracy = 60.80799999999999%, Loss = 0.5238746762275696
Epoch: 2149, Batch Gradient Norm: 7.969252700113148
Epoch: 2149, Batch Gradient Norm after: 7.969252700113148
Epoch 2150/10000, Prediction Accuracy = 60.75599999999999%, Loss = 0.53125501871109
Epoch: 2150, Batch Gradient Norm: 7.691174700992955
Epoch: 2150, Batch Gradient Norm after: 7.691174700992955
Epoch 2151/10000, Prediction Accuracy = 60.86600000000001%, Loss = 0.5284177660942078
Epoch: 2151, Batch Gradient Norm: 7.958529241133012
Epoch: 2151, Batch Gradient Norm after: 7.958529241133012
Epoch 2152/10000, Prediction Accuracy = 60.938%, Loss = 0.5270150542259217
Epoch: 2152, Batch Gradient Norm: 14.222591524266779
Epoch: 2152, Batch Gradient Norm after: 14.222591524266779
Epoch 2153/10000, Prediction Accuracy = 60.778%, Loss = 0.5735520482063293
Epoch: 2153, Batch Gradient Norm: 15.180468426745744
Epoch: 2153, Batch Gradient Norm after: 15.180468426745744
Epoch 2154/10000, Prediction Accuracy = 60.79200000000001%, Loss = 0.5816399455070496
Epoch: 2154, Batch Gradient Norm: 9.483479706842202
Epoch: 2154, Batch Gradient Norm after: 9.483479706842202
Epoch 2155/10000, Prediction Accuracy = 60.98%, Loss = 0.5367306232452392
Epoch: 2155, Batch Gradient Norm: 10.974497184590557
Epoch: 2155, Batch Gradient Norm after: 10.974497184590557
Epoch 2156/10000, Prediction Accuracy = 60.827999999999996%, Loss = 0.5459381699562073
Epoch: 2156, Batch Gradient Norm: 12.784706648555149
Epoch: 2156, Batch Gradient Norm after: 12.784706648555149
Epoch 2157/10000, Prediction Accuracy = 60.855999999999995%, Loss = 0.5563516855239868
Epoch: 2157, Batch Gradient Norm: 12.31751958614206
Epoch: 2157, Batch Gradient Norm after: 12.31751958614206
Epoch 2158/10000, Prediction Accuracy = 60.824%, Loss = 0.5510113358497619
Epoch: 2158, Batch Gradient Norm: 12.381596121698067
Epoch: 2158, Batch Gradient Norm after: 12.381596121698067
Epoch 2159/10000, Prediction Accuracy = 60.826%, Loss = 0.5527214884757996
Epoch: 2159, Batch Gradient Norm: 8.801026384316478
Epoch: 2159, Batch Gradient Norm after: 8.801026384316478
Epoch 2160/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.5336040377616882
Epoch: 2160, Batch Gradient Norm: 9.967242928311205
Epoch: 2160, Batch Gradient Norm after: 9.967242928311205
Epoch 2161/10000, Prediction Accuracy = 60.85%, Loss = 0.5366324424743653
Epoch: 2161, Batch Gradient Norm: 11.65284767637031
Epoch: 2161, Batch Gradient Norm after: 11.65284767637031
Epoch 2162/10000, Prediction Accuracy = 60.824%, Loss = 0.5459261178970337
Epoch: 2162, Batch Gradient Norm: 13.77705535025363
Epoch: 2162, Batch Gradient Norm after: 13.77705535025363
Epoch 2163/10000, Prediction Accuracy = 60.802%, Loss = 0.5650789618492127
Epoch: 2163, Batch Gradient Norm: 9.80635235997847
Epoch: 2163, Batch Gradient Norm after: 9.80635235997847
Epoch 2164/10000, Prediction Accuracy = 60.834%, Loss = 0.5368225336074829
Epoch: 2164, Batch Gradient Norm: 11.080311928806667
Epoch: 2164, Batch Gradient Norm after: 11.080311928806667
Epoch 2165/10000, Prediction Accuracy = 60.83399999999999%, Loss = 0.5412592887878418
Epoch: 2165, Batch Gradient Norm: 11.652287925432063
Epoch: 2165, Batch Gradient Norm after: 11.652287925432063
Epoch 2166/10000, Prediction Accuracy = 60.791999999999994%, Loss = 0.5506643295288086
Epoch: 2166, Batch Gradient Norm: 11.23570329792035
Epoch: 2166, Batch Gradient Norm after: 11.23570329792035
Epoch 2167/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.5468607664108276
Epoch: 2167, Batch Gradient Norm: 9.233847216562936
Epoch: 2167, Batch Gradient Norm after: 9.233847216562936
Epoch 2168/10000, Prediction Accuracy = 60.815999999999995%, Loss = 0.5356098771095276
Epoch: 2168, Batch Gradient Norm: 13.235189396830071
Epoch: 2168, Batch Gradient Norm after: 13.235189396830071
Epoch 2169/10000, Prediction Accuracy = 60.838%, Loss = 0.5589993000030518
Epoch: 2169, Batch Gradient Norm: 10.336162473571427
Epoch: 2169, Batch Gradient Norm after: 10.336162473571427
Epoch 2170/10000, Prediction Accuracy = 60.852%, Loss = 0.5425338506698608
Epoch: 2170, Batch Gradient Norm: 7.244831698383036
Epoch: 2170, Batch Gradient Norm after: 7.244831698383036
Epoch 2171/10000, Prediction Accuracy = 60.818%, Loss = 0.5250659823417664
Epoch: 2171, Batch Gradient Norm: 10.942180400727771
Epoch: 2171, Batch Gradient Norm after: 10.942180400727771
Epoch 2172/10000, Prediction Accuracy = 60.848%, Loss = 0.5440285205841064
Epoch: 2172, Batch Gradient Norm: 12.13734838065032
Epoch: 2172, Batch Gradient Norm after: 12.13734838065032
Epoch 2173/10000, Prediction Accuracy = 60.738%, Loss = 0.5541491627693176
Epoch: 2173, Batch Gradient Norm: 14.38061236889488
Epoch: 2173, Batch Gradient Norm after: 14.38061236889488
Epoch 2174/10000, Prediction Accuracy = 60.886%, Loss = 0.5711416363716125
Epoch: 2174, Batch Gradient Norm: 9.09236446494903
Epoch: 2174, Batch Gradient Norm after: 9.09236446494903
Epoch 2175/10000, Prediction Accuracy = 60.85799999999999%, Loss = 0.5313856720924377
Epoch: 2175, Batch Gradient Norm: 8.360294965768253
Epoch: 2175, Batch Gradient Norm after: 8.360294965768253
Epoch 2176/10000, Prediction Accuracy = 60.83399999999999%, Loss = 0.5320161700248718
Epoch: 2176, Batch Gradient Norm: 7.695818816897525
Epoch: 2176, Batch Gradient Norm after: 7.695818816897525
Epoch 2177/10000, Prediction Accuracy = 60.916%, Loss = 0.5240520477294922
Epoch: 2177, Batch Gradient Norm: 10.166059398808162
Epoch: 2177, Batch Gradient Norm after: 10.166059398808162
Epoch 2178/10000, Prediction Accuracy = 60.854000000000006%, Loss = 0.5382141947746277
Epoch: 2178, Batch Gradient Norm: 11.519469192799628
Epoch: 2178, Batch Gradient Norm after: 11.519469192799628
Epoch 2179/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.5477897167205811
Epoch: 2179, Batch Gradient Norm: 10.75718825364572
Epoch: 2179, Batch Gradient Norm after: 10.75718825364572
Epoch 2180/10000, Prediction Accuracy = 60.818000000000005%, Loss = 0.5502178192138671
Epoch: 2180, Batch Gradient Norm: 9.700593658852048
Epoch: 2180, Batch Gradient Norm after: 9.700593658852048
Epoch 2181/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.5357248425483704
Epoch: 2181, Batch Gradient Norm: 14.112097657098959
Epoch: 2181, Batch Gradient Norm after: 14.112097657098959
Epoch 2182/10000, Prediction Accuracy = 60.872%, Loss = 0.5672342658042908
Epoch: 2182, Batch Gradient Norm: 11.221981754467128
Epoch: 2182, Batch Gradient Norm after: 11.221981754467128
Epoch 2183/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.5498953104019165
Epoch: 2183, Batch Gradient Norm: 10.614176281387397
Epoch: 2183, Batch Gradient Norm after: 10.614176281387397
Epoch 2184/10000, Prediction Accuracy = 60.894000000000005%, Loss = 0.5443601369857788
Epoch: 2184, Batch Gradient Norm: 12.21421816470351
Epoch: 2184, Batch Gradient Norm after: 12.21421816470351
Epoch 2185/10000, Prediction Accuracy = 60.754%, Loss = 0.5526073575019836
Epoch: 2185, Batch Gradient Norm: 13.162704387638811
Epoch: 2185, Batch Gradient Norm after: 13.162704387638811
Epoch 2186/10000, Prediction Accuracy = 60.838%, Loss = 0.5565698862075805
Epoch: 2186, Batch Gradient Norm: 12.023175304185322
Epoch: 2186, Batch Gradient Norm after: 12.023175304185322
Epoch 2187/10000, Prediction Accuracy = 60.926%, Loss = 0.5532635807991028
Epoch: 2187, Batch Gradient Norm: 11.799044096361422
Epoch: 2187, Batch Gradient Norm after: 11.799044096361422
Epoch 2188/10000, Prediction Accuracy = 60.943999999999996%, Loss = 0.5478610038757324
Epoch: 2188, Batch Gradient Norm: 10.021233468869085
Epoch: 2188, Batch Gradient Norm after: 10.021233468869085
Epoch 2189/10000, Prediction Accuracy = 60.94%, Loss = 0.5357858896255493
Epoch: 2189, Batch Gradient Norm: 7.332354082122038
Epoch: 2189, Batch Gradient Norm after: 7.332354082122038
Epoch 2190/10000, Prediction Accuracy = 60.96%, Loss = 0.5233680963516235
Epoch: 2190, Batch Gradient Norm: 11.065941982939759
Epoch: 2190, Batch Gradient Norm after: 11.065941982939759
Epoch 2191/10000, Prediction Accuracy = 60.80799999999999%, Loss = 0.5517329454421998
Epoch: 2191, Batch Gradient Norm: 11.315376863002294
Epoch: 2191, Batch Gradient Norm after: 11.315376863002294
Epoch 2192/10000, Prediction Accuracy = 60.86600000000001%, Loss = 0.5441623806953431
Epoch: 2192, Batch Gradient Norm: 13.068803001819534
Epoch: 2192, Batch Gradient Norm after: 13.068803001819534
Epoch 2193/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.56251060962677
Epoch: 2193, Batch Gradient Norm: 8.720865087553225
Epoch: 2193, Batch Gradient Norm after: 8.720865087553225
Epoch 2194/10000, Prediction Accuracy = 60.822%, Loss = 0.5343835711479187
Epoch: 2194, Batch Gradient Norm: 11.39897306112174
Epoch: 2194, Batch Gradient Norm after: 11.39897306112174
Epoch 2195/10000, Prediction Accuracy = 60.9%, Loss = 0.5484297752380372
Epoch: 2195, Batch Gradient Norm: 11.876671392138016
Epoch: 2195, Batch Gradient Norm after: 11.876671392138016
Epoch 2196/10000, Prediction Accuracy = 60.92%, Loss = 0.5589085698127747
Epoch: 2196, Batch Gradient Norm: 7.5166954601573295
Epoch: 2196, Batch Gradient Norm after: 7.5166954601573295
Epoch 2197/10000, Prediction Accuracy = 60.81400000000001%, Loss = 0.5212563872337341
Epoch: 2197, Batch Gradient Norm: 9.095628955750032
Epoch: 2197, Batch Gradient Norm after: 9.095628955750032
Epoch 2198/10000, Prediction Accuracy = 60.852%, Loss = 0.5302849292755127
Epoch: 2198, Batch Gradient Norm: 14.006605511802666
Epoch: 2198, Batch Gradient Norm after: 14.006605511802666
Epoch 2199/10000, Prediction Accuracy = 60.896%, Loss = 0.5613327383995056
Epoch: 2199, Batch Gradient Norm: 15.24705044975188
Epoch: 2199, Batch Gradient Norm after: 15.24705044975188
Epoch 2200/10000, Prediction Accuracy = 60.948%, Loss = 0.5754576325416565
Epoch: 2200, Batch Gradient Norm: 13.265020486708577
Epoch: 2200, Batch Gradient Norm after: 13.265020486708577
Epoch 2201/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.5643559575080872
Epoch: 2201, Batch Gradient Norm: 10.748399957156565
Epoch: 2201, Batch Gradient Norm after: 10.748399957156565
Epoch 2202/10000, Prediction Accuracy = 60.866%, Loss = 0.550380289554596
Epoch: 2202, Batch Gradient Norm: 7.921199491506403
Epoch: 2202, Batch Gradient Norm after: 7.921199491506403
Epoch 2203/10000, Prediction Accuracy = 61.008%, Loss = 0.5289107322692871
Epoch: 2203, Batch Gradient Norm: 13.114124096808107
Epoch: 2203, Batch Gradient Norm after: 13.114124096808107
Epoch 2204/10000, Prediction Accuracy = 61.012%, Loss = 0.555904996395111
Epoch: 2204, Batch Gradient Norm: 15.63064246670912
Epoch: 2204, Batch Gradient Norm after: 15.63064246670912
Epoch 2205/10000, Prediction Accuracy = 60.878%, Loss = 0.5790720343589782
Epoch: 2205, Batch Gradient Norm: 10.009950941961785
Epoch: 2205, Batch Gradient Norm after: 10.009950941961785
Epoch 2206/10000, Prediction Accuracy = 60.891999999999996%, Loss = 0.5358513116836547
Epoch: 2206, Batch Gradient Norm: 10.708685255601358
Epoch: 2206, Batch Gradient Norm after: 10.708685255601358
Epoch 2207/10000, Prediction Accuracy = 60.798%, Loss = 0.5413076162338257
Epoch: 2207, Batch Gradient Norm: 12.919684960333486
Epoch: 2207, Batch Gradient Norm after: 12.919684960333486
Epoch 2208/10000, Prediction Accuracy = 60.912%, Loss = 0.5528065443038941
Epoch: 2208, Batch Gradient Norm: 10.272303810800157
Epoch: 2208, Batch Gradient Norm after: 10.272303810800157
Epoch 2209/10000, Prediction Accuracy = 60.81999999999999%, Loss = 0.5369746565818787
Epoch: 2209, Batch Gradient Norm: 10.520390017230175
Epoch: 2209, Batch Gradient Norm after: 10.520390017230175
Epoch 2210/10000, Prediction Accuracy = 61.019999999999996%, Loss = 0.5410439848899842
Epoch: 2210, Batch Gradient Norm: 9.599872046278556
Epoch: 2210, Batch Gradient Norm after: 9.599872046278556
Epoch 2211/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.5356218338012695
Epoch: 2211, Batch Gradient Norm: 11.504907260762325
Epoch: 2211, Batch Gradient Norm after: 11.504907260762325
Epoch 2212/10000, Prediction Accuracy = 60.79%, Loss = 0.5476091504096985
Epoch: 2212, Batch Gradient Norm: 11.19932665568793
Epoch: 2212, Batch Gradient Norm after: 11.19932665568793
Epoch 2213/10000, Prediction Accuracy = 60.79200000000001%, Loss = 0.543854033946991
Epoch: 2213, Batch Gradient Norm: 9.957722420654127
Epoch: 2213, Batch Gradient Norm after: 9.957722420654127
Epoch 2214/10000, Prediction Accuracy = 60.855999999999995%, Loss = 0.5394064903259277
Epoch: 2214, Batch Gradient Norm: 9.958563399377258
Epoch: 2214, Batch Gradient Norm after: 9.958563399377258
Epoch 2215/10000, Prediction Accuracy = 60.944%, Loss = 0.5383544087409973
Epoch: 2215, Batch Gradient Norm: 9.375650955043845
Epoch: 2215, Batch Gradient Norm after: 9.375650955043845
Epoch 2216/10000, Prediction Accuracy = 60.782000000000004%, Loss = 0.5329394340515137
Epoch: 2216, Batch Gradient Norm: 9.055114101771595
Epoch: 2216, Batch Gradient Norm after: 9.055114101771595
Epoch 2217/10000, Prediction Accuracy = 60.938%, Loss = 0.5311470866203308
Epoch: 2217, Batch Gradient Norm: 10.340828877154317
Epoch: 2217, Batch Gradient Norm after: 10.340828877154317
Epoch 2218/10000, Prediction Accuracy = 60.88399999999999%, Loss = 0.5390035033226013
Epoch: 2218, Batch Gradient Norm: 12.417962152561996
Epoch: 2218, Batch Gradient Norm after: 12.417962152561996
Epoch 2219/10000, Prediction Accuracy = 60.870000000000005%, Loss = 0.5542724370956421
Epoch: 2219, Batch Gradient Norm: 10.981178440453062
Epoch: 2219, Batch Gradient Norm after: 10.981178440453062
Epoch 2220/10000, Prediction Accuracy = 60.94199999999999%, Loss = 0.5430158257484436
Epoch: 2220, Batch Gradient Norm: 10.510386282279695
Epoch: 2220, Batch Gradient Norm after: 10.510386282279695
Epoch 2221/10000, Prediction Accuracy = 60.818000000000005%, Loss = 0.538064706325531
Epoch: 2221, Batch Gradient Norm: 9.977507535269075
Epoch: 2221, Batch Gradient Norm after: 9.977507535269075
Epoch 2222/10000, Prediction Accuracy = 60.896%, Loss = 0.5337197422981262
Epoch: 2222, Batch Gradient Norm: 11.299086357191815
Epoch: 2222, Batch Gradient Norm after: 11.299086357191815
Epoch 2223/10000, Prediction Accuracy = 60.918000000000006%, Loss = 0.5467193484306335
Epoch: 2223, Batch Gradient Norm: 13.781439451166602
Epoch: 2223, Batch Gradient Norm after: 13.781439451166602
Epoch 2224/10000, Prediction Accuracy = 60.884%, Loss = 0.5617898583412171
Epoch: 2224, Batch Gradient Norm: 12.124972723743568
Epoch: 2224, Batch Gradient Norm after: 12.124972723743568
Epoch 2225/10000, Prediction Accuracy = 60.78000000000001%, Loss = 0.5512446403503418
Epoch: 2225, Batch Gradient Norm: 9.555150003071757
Epoch: 2225, Batch Gradient Norm after: 9.555150003071757
Epoch 2226/10000, Prediction Accuracy = 60.886%, Loss = 0.5325341820716858
Epoch: 2226, Batch Gradient Norm: 9.590274024425119
Epoch: 2226, Batch Gradient Norm after: 9.590274024425119
Epoch 2227/10000, Prediction Accuracy = 60.936%, Loss = 0.5298746705055237
Epoch: 2227, Batch Gradient Norm: 10.955873913247228
Epoch: 2227, Batch Gradient Norm after: 10.955873913247228
Epoch 2228/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.5412301421165466
Epoch: 2228, Batch Gradient Norm: 13.098209917110392
Epoch: 2228, Batch Gradient Norm after: 13.098209917110392
Epoch 2229/10000, Prediction Accuracy = 60.85%, Loss = 0.5548029541969299
Epoch: 2229, Batch Gradient Norm: 12.403460137038357
Epoch: 2229, Batch Gradient Norm after: 12.403460137038357
Epoch 2230/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.5485021948814393
Epoch: 2230, Batch Gradient Norm: 12.173282348552224
Epoch: 2230, Batch Gradient Norm after: 12.173282348552224
Epoch 2231/10000, Prediction Accuracy = 60.839999999999996%, Loss = 0.5576012372970581
Epoch: 2231, Batch Gradient Norm: 8.074362160884382
Epoch: 2231, Batch Gradient Norm after: 8.074362160884382
Epoch 2232/10000, Prediction Accuracy = 60.908%, Loss = 0.5274283289909363
Epoch: 2232, Batch Gradient Norm: 9.503716675141417
Epoch: 2232, Batch Gradient Norm after: 9.503716675141417
Epoch 2233/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5341951727867127
Epoch: 2233, Batch Gradient Norm: 9.661223177198817
Epoch: 2233, Batch Gradient Norm after: 9.661223177198817
Epoch 2234/10000, Prediction Accuracy = 60.89%, Loss = 0.5352335333824157
Epoch: 2234, Batch Gradient Norm: 9.78163739024167
Epoch: 2234, Batch Gradient Norm after: 9.78163739024167
Epoch 2235/10000, Prediction Accuracy = 60.96600000000001%, Loss = 0.5379743576049805
Epoch: 2235, Batch Gradient Norm: 11.621800543462921
Epoch: 2235, Batch Gradient Norm after: 11.621800543462921
Epoch 2236/10000, Prediction Accuracy = 60.906000000000006%, Loss = 0.5492178797721863
Epoch: 2236, Batch Gradient Norm: 9.18516870758428
Epoch: 2236, Batch Gradient Norm after: 9.18516870758428
Epoch 2237/10000, Prediction Accuracy = 60.842000000000006%, Loss = 0.5307098269462586
Epoch: 2237, Batch Gradient Norm: 11.147414955472355
Epoch: 2237, Batch Gradient Norm after: 11.147414955472355
Epoch 2238/10000, Prediction Accuracy = 60.983999999999995%, Loss = 0.5392058730125427
Epoch: 2238, Batch Gradient Norm: 14.689884308985697
Epoch: 2238, Batch Gradient Norm after: 14.689884308985697
Epoch 2239/10000, Prediction Accuracy = 60.79%, Loss = 0.5685293912887573
Epoch: 2239, Batch Gradient Norm: 11.555291409556878
Epoch: 2239, Batch Gradient Norm after: 11.555291409556878
Epoch 2240/10000, Prediction Accuracy = 60.866%, Loss = 0.5445974707603455
Epoch: 2240, Batch Gradient Norm: 7.5806439386892395
Epoch: 2240, Batch Gradient Norm after: 7.5806439386892395
Epoch 2241/10000, Prediction Accuracy = 60.879999999999995%, Loss = 0.5197328448295593
Epoch: 2241, Batch Gradient Norm: 9.994028034691336
Epoch: 2241, Batch Gradient Norm after: 9.994028034691336
Epoch 2242/10000, Prediction Accuracy = 60.936%, Loss = 0.533598268032074
Epoch: 2242, Batch Gradient Norm: 13.954217841930172
Epoch: 2242, Batch Gradient Norm after: 13.954217841930172
Epoch 2243/10000, Prediction Accuracy = 60.846000000000004%, Loss = 0.5629598021507263
Epoch: 2243, Batch Gradient Norm: 13.343462220433084
Epoch: 2243, Batch Gradient Norm after: 13.343462220433084
Epoch 2244/10000, Prediction Accuracy = 60.924%, Loss = 0.5597107648849488
Epoch: 2244, Batch Gradient Norm: 10.96354282371759
Epoch: 2244, Batch Gradient Norm after: 10.96354282371759
Epoch 2245/10000, Prediction Accuracy = 60.996%, Loss = 0.5447615623474121
Epoch: 2245, Batch Gradient Norm: 8.509135525384057
Epoch: 2245, Batch Gradient Norm after: 8.509135525384057
Epoch 2246/10000, Prediction Accuracy = 60.94%, Loss = 0.5288812518119812
Epoch: 2246, Batch Gradient Norm: 10.668085663879816
Epoch: 2246, Batch Gradient Norm after: 10.668085663879816
Epoch 2247/10000, Prediction Accuracy = 60.831999999999994%, Loss = 0.5491948366165161
Epoch: 2247, Batch Gradient Norm: 12.093364702022194
Epoch: 2247, Batch Gradient Norm after: 12.093364702022194
Epoch 2248/10000, Prediction Accuracy = 61.0%, Loss = 0.5497023344039917
Epoch: 2248, Batch Gradient Norm: 10.60350136538348
Epoch: 2248, Batch Gradient Norm after: 10.60350136538348
Epoch 2249/10000, Prediction Accuracy = 60.872%, Loss = 0.5357699632644654
Epoch: 2249, Batch Gradient Norm: 11.824312354303665
Epoch: 2249, Batch Gradient Norm after: 11.824312354303665
Epoch 2250/10000, Prediction Accuracy = 60.92%, Loss = 0.5480995178222656
Epoch: 2250, Batch Gradient Norm: 9.60122625193667
Epoch: 2250, Batch Gradient Norm after: 9.60122625193667
Epoch 2251/10000, Prediction Accuracy = 60.834%, Loss = 0.5331135511398315
Epoch: 2251, Batch Gradient Norm: 10.684527196856177
Epoch: 2251, Batch Gradient Norm after: 10.684527196856177
Epoch 2252/10000, Prediction Accuracy = 60.92800000000001%, Loss = 0.5397623658180237
Epoch: 2252, Batch Gradient Norm: 12.110420201647916
Epoch: 2252, Batch Gradient Norm after: 12.110420201647916
Epoch 2253/10000, Prediction Accuracy = 60.884%, Loss = 0.5496778607368469
Epoch: 2253, Batch Gradient Norm: 10.68302185705968
Epoch: 2253, Batch Gradient Norm after: 10.68302185705968
Epoch 2254/10000, Prediction Accuracy = 60.958000000000006%, Loss = 0.5361793041229248
Epoch: 2254, Batch Gradient Norm: 11.740109056752726
Epoch: 2254, Batch Gradient Norm after: 11.740109056752726
Epoch 2255/10000, Prediction Accuracy = 60.838%, Loss = 0.5447688102722168
Epoch: 2255, Batch Gradient Norm: 12.099278473824043
Epoch: 2255, Batch Gradient Norm after: 12.099278473824043
Epoch 2256/10000, Prediction Accuracy = 60.878%, Loss = 0.5549466609954834
Epoch: 2256, Batch Gradient Norm: 11.65185220366885
Epoch: 2256, Batch Gradient Norm after: 11.65185220366885
Epoch 2257/10000, Prediction Accuracy = 60.86%, Loss = 0.5484458327293396
Epoch: 2257, Batch Gradient Norm: 11.803144760338718
Epoch: 2257, Batch Gradient Norm after: 11.803144760338718
Epoch 2258/10000, Prediction Accuracy = 60.914%, Loss = 0.5485936045646668
Epoch: 2258, Batch Gradient Norm: 9.452881882274058
Epoch: 2258, Batch Gradient Norm after: 9.452881882274058
Epoch 2259/10000, Prediction Accuracy = 60.895999999999994%, Loss = 0.5301896095275879
Epoch: 2259, Batch Gradient Norm: 7.3811089787190065
Epoch: 2259, Batch Gradient Norm after: 7.3811089787190065
Epoch 2260/10000, Prediction Accuracy = 60.93000000000001%, Loss = 0.5176334142684936
Epoch: 2260, Batch Gradient Norm: 11.191311965834226
Epoch: 2260, Batch Gradient Norm after: 11.191311965834226
Epoch 2261/10000, Prediction Accuracy = 60.908%, Loss = 0.5376476049423218
Epoch: 2261, Batch Gradient Norm: 13.852971405720744
Epoch: 2261, Batch Gradient Norm after: 13.852971405720744
Epoch 2262/10000, Prediction Accuracy = 60.94000000000001%, Loss = 0.5616154670715332
Epoch: 2262, Batch Gradient Norm: 9.63625606783258
Epoch: 2262, Batch Gradient Norm after: 9.63625606783258
Epoch 2263/10000, Prediction Accuracy = 60.85600000000001%, Loss = 0.530483341217041
Epoch: 2263, Batch Gradient Norm: 10.28738685429246
Epoch: 2263, Batch Gradient Norm after: 10.28738685429246
Epoch 2264/10000, Prediction Accuracy = 60.926%, Loss = 0.5366954565048218
Epoch: 2264, Batch Gradient Norm: 8.016348483494443
Epoch: 2264, Batch Gradient Norm after: 8.016348483494443
Epoch 2265/10000, Prediction Accuracy = 60.928%, Loss = 0.524621844291687
Epoch: 2265, Batch Gradient Norm: 10.365529878194064
Epoch: 2265, Batch Gradient Norm after: 10.365529878194064
Epoch 2266/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.5360364556312561
Epoch: 2266, Batch Gradient Norm: 11.998195437273383
Epoch: 2266, Batch Gradient Norm after: 11.998195437273383
Epoch 2267/10000, Prediction Accuracy = 60.919999999999995%, Loss = 0.5490602612495422
Epoch: 2267, Batch Gradient Norm: 11.988010380208769
Epoch: 2267, Batch Gradient Norm after: 11.988010380208769
Epoch 2268/10000, Prediction Accuracy = 60.984%, Loss = 0.5458070278167725
Epoch: 2268, Batch Gradient Norm: 11.317224125521768
Epoch: 2268, Batch Gradient Norm after: 11.317224125521768
Epoch 2269/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5431837201118469
Epoch: 2269, Batch Gradient Norm: 13.066851508796624
Epoch: 2269, Batch Gradient Norm after: 13.066851508796624
Epoch 2270/10000, Prediction Accuracy = 60.806%, Loss = 0.5603229165077209
Epoch: 2270, Batch Gradient Norm: 11.857985478296381
Epoch: 2270, Batch Gradient Norm after: 11.857985478296381
Epoch 2271/10000, Prediction Accuracy = 60.971999999999994%, Loss = 0.5479398012161255
Epoch: 2271, Batch Gradient Norm: 10.436529100057696
Epoch: 2271, Batch Gradient Norm after: 10.436529100057696
Epoch 2272/10000, Prediction Accuracy = 60.778%, Loss = 0.5367109775543213
Epoch: 2272, Batch Gradient Norm: 9.99887094333929
Epoch: 2272, Batch Gradient Norm after: 9.99887094333929
Epoch 2273/10000, Prediction Accuracy = 60.884%, Loss = 0.5284439086914062
Epoch: 2273, Batch Gradient Norm: 11.840049623630126
Epoch: 2273, Batch Gradient Norm after: 11.840049623630126
Epoch 2274/10000, Prediction Accuracy = 60.938%, Loss = 0.5433035016059875
Epoch: 2274, Batch Gradient Norm: 10.52653066694999
Epoch: 2274, Batch Gradient Norm after: 10.52653066694999
Epoch 2275/10000, Prediction Accuracy = 60.848%, Loss = 0.5379289150238037
Epoch: 2275, Batch Gradient Norm: 8.952666560107655
Epoch: 2275, Batch Gradient Norm after: 8.952666560107655
Epoch 2276/10000, Prediction Accuracy = 60.896%, Loss = 0.5320115566253663
Epoch: 2276, Batch Gradient Norm: 10.097102755482966
Epoch: 2276, Batch Gradient Norm after: 10.097102755482966
Epoch 2277/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.5359451413154602
Epoch: 2277, Batch Gradient Norm: 13.331469435009941
Epoch: 2277, Batch Gradient Norm after: 13.331469435009941
Epoch 2278/10000, Prediction Accuracy = 60.94200000000001%, Loss = 0.5605988025665283
Epoch: 2278, Batch Gradient Norm: 11.158056629897057
Epoch: 2278, Batch Gradient Norm after: 11.158056629897057
Epoch 2279/10000, Prediction Accuracy = 60.85%, Loss = 0.5406137585639954
Epoch: 2279, Batch Gradient Norm: 10.61753179841434
Epoch: 2279, Batch Gradient Norm after: 10.61753179841434
Epoch 2280/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5397281765937805
Epoch: 2280, Batch Gradient Norm: 9.270489021518538
Epoch: 2280, Batch Gradient Norm after: 9.270489021518538
Epoch 2281/10000, Prediction Accuracy = 60.886%, Loss = 0.537498164176941
Epoch: 2281, Batch Gradient Norm: 10.490855607697751
Epoch: 2281, Batch Gradient Norm after: 10.490855607697751
Epoch 2282/10000, Prediction Accuracy = 60.919999999999995%, Loss = 0.539800500869751
Epoch: 2282, Batch Gradient Norm: 10.366074140013232
Epoch: 2282, Batch Gradient Norm after: 10.366074140013232
Epoch 2283/10000, Prediction Accuracy = 60.992%, Loss = 0.5339066863059998
Epoch: 2283, Batch Gradient Norm: 8.535542186580814
Epoch: 2283, Batch Gradient Norm after: 8.535542186580814
Epoch 2284/10000, Prediction Accuracy = 60.89399999999999%, Loss = 0.5229884862899781
Epoch: 2284, Batch Gradient Norm: 9.810060361268876
Epoch: 2284, Batch Gradient Norm after: 9.810060361268876
Epoch 2285/10000, Prediction Accuracy = 60.86600000000001%, Loss = 0.5289506316184998
Epoch: 2285, Batch Gradient Norm: 15.140306199564733
Epoch: 2285, Batch Gradient Norm after: 15.140306199564733
Epoch 2286/10000, Prediction Accuracy = 60.96199999999999%, Loss = 0.5694698810577392
Epoch: 2286, Batch Gradient Norm: 10.520620948653594
Epoch: 2286, Batch Gradient Norm after: 10.520620948653594
Epoch 2287/10000, Prediction Accuracy = 60.926%, Loss = 0.5347373127937317
Epoch: 2287, Batch Gradient Norm: 12.908769674540244
Epoch: 2287, Batch Gradient Norm after: 12.908769674540244
Epoch 2288/10000, Prediction Accuracy = 60.862%, Loss = 0.5490530848503112
Epoch: 2288, Batch Gradient Norm: 13.041946629307349
Epoch: 2288, Batch Gradient Norm after: 13.041946629307349
Epoch 2289/10000, Prediction Accuracy = 60.918000000000006%, Loss = 0.5514723181724548
Epoch: 2289, Batch Gradient Norm: 9.786678962881794
Epoch: 2289, Batch Gradient Norm after: 9.786678962881794
Epoch 2290/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5294787883758545
Epoch: 2290, Batch Gradient Norm: 9.113809825982553
Epoch: 2290, Batch Gradient Norm after: 9.113809825982553
Epoch 2291/10000, Prediction Accuracy = 60.932%, Loss = 0.5248579621315003
Epoch: 2291, Batch Gradient Norm: 10.947136737794057
Epoch: 2291, Batch Gradient Norm after: 10.947136737794057
Epoch 2292/10000, Prediction Accuracy = 61.004%, Loss = 0.5375945806503296
Epoch: 2292, Batch Gradient Norm: 11.199125675522101
Epoch: 2292, Batch Gradient Norm after: 11.199125675522101
Epoch 2293/10000, Prediction Accuracy = 60.852%, Loss = 0.5404309034347534
Epoch: 2293, Batch Gradient Norm: 12.98133435379233
Epoch: 2293, Batch Gradient Norm after: 12.98133435379233
Epoch 2294/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5502677917480469
Epoch: 2294, Batch Gradient Norm: 9.141589322223982
Epoch: 2294, Batch Gradient Norm after: 9.141589322223982
Epoch 2295/10000, Prediction Accuracy = 60.972%, Loss = 0.523620855808258
Epoch: 2295, Batch Gradient Norm: 8.009059439373784
Epoch: 2295, Batch Gradient Norm after: 8.009059439373784
Epoch 2296/10000, Prediction Accuracy = 60.984%, Loss = 0.5190789937973023
Epoch: 2296, Batch Gradient Norm: 6.745710587626487
Epoch: 2296, Batch Gradient Norm after: 6.745710587626487
Epoch 2297/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.5133843541145324
Epoch: 2297, Batch Gradient Norm: 13.088944499136998
Epoch: 2297, Batch Gradient Norm after: 13.088944499136998
Epoch 2298/10000, Prediction Accuracy = 60.914%, Loss = 0.5560876965522766
Epoch: 2298, Batch Gradient Norm: 13.337023799159745
Epoch: 2298, Batch Gradient Norm after: 13.337023799159745
Epoch 2299/10000, Prediction Accuracy = 60.912%, Loss = 0.5576751828193665
Epoch: 2299, Batch Gradient Norm: 11.377728774559742
Epoch: 2299, Batch Gradient Norm after: 11.377728774559742
Epoch 2300/10000, Prediction Accuracy = 60.903999999999996%, Loss = 0.5436458349227905
Epoch: 2300, Batch Gradient Norm: 8.476406607598763
Epoch: 2300, Batch Gradient Norm after: 8.476406607598763
Epoch 2301/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5238821864128113
Epoch: 2301, Batch Gradient Norm: 8.816037148197193
Epoch: 2301, Batch Gradient Norm after: 8.816037148197193
Epoch 2302/10000, Prediction Accuracy = 60.924%, Loss = 0.5271150946617127
Epoch: 2302, Batch Gradient Norm: 8.811435051396868
Epoch: 2302, Batch Gradient Norm after: 8.811435051396868
Epoch 2303/10000, Prediction Accuracy = 60.964%, Loss = 0.5247122645378113
Epoch: 2303, Batch Gradient Norm: 11.877147105022951
Epoch: 2303, Batch Gradient Norm after: 11.877147105022951
Epoch 2304/10000, Prediction Accuracy = 61.036%, Loss = 0.5435117602348327
Epoch: 2304, Batch Gradient Norm: 13.775789245845589
Epoch: 2304, Batch Gradient Norm after: 13.775789245845589
Epoch 2305/10000, Prediction Accuracy = 60.924%, Loss = 0.5613349318504334
Epoch: 2305, Batch Gradient Norm: 10.11775221891043
Epoch: 2305, Batch Gradient Norm after: 10.11775221891043
Epoch 2306/10000, Prediction Accuracy = 60.86%, Loss = 0.5331881999969482
Epoch: 2306, Batch Gradient Norm: 13.049920621902073
Epoch: 2306, Batch Gradient Norm after: 13.049920621902073
Epoch 2307/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.5518110156059265
Epoch: 2307, Batch Gradient Norm: 12.61589723095902
Epoch: 2307, Batch Gradient Norm after: 12.61589723095902
Epoch 2308/10000, Prediction Accuracy = 60.90599999999999%, Loss = 0.5536064743995667
Epoch: 2308, Batch Gradient Norm: 12.601213018313253
Epoch: 2308, Batch Gradient Norm after: 12.601213018313253
Epoch 2309/10000, Prediction Accuracy = 60.988%, Loss = 0.5509193062782287
Epoch: 2309, Batch Gradient Norm: 11.122042202766396
Epoch: 2309, Batch Gradient Norm after: 11.122042202766396
Epoch 2310/10000, Prediction Accuracy = 60.972%, Loss = 0.5383649468421936
Epoch: 2310, Batch Gradient Norm: 10.96008410552041
Epoch: 2310, Batch Gradient Norm after: 10.96008410552041
Epoch 2311/10000, Prediction Accuracy = 61.06600000000001%, Loss = 0.535383677482605
Epoch: 2311, Batch Gradient Norm: 9.807514922510403
Epoch: 2311, Batch Gradient Norm after: 9.807514922510403
Epoch 2312/10000, Prediction Accuracy = 60.898%, Loss = 0.5305381298065186
Epoch: 2312, Batch Gradient Norm: 12.83997041001352
Epoch: 2312, Batch Gradient Norm after: 12.83997041001352
Epoch 2313/10000, Prediction Accuracy = 61.08%, Loss = 0.5533042907714844
Epoch: 2313, Batch Gradient Norm: 8.536344781898771
Epoch: 2313, Batch Gradient Norm after: 8.536344781898771
Epoch 2314/10000, Prediction Accuracy = 60.962%, Loss = 0.5234489917755127
Epoch: 2314, Batch Gradient Norm: 7.481791939004243
Epoch: 2314, Batch Gradient Norm after: 7.481791939004243
Epoch 2315/10000, Prediction Accuracy = 60.903999999999996%, Loss = 0.5163649559020996
Epoch: 2315, Batch Gradient Norm: 8.64192261374531
Epoch: 2315, Batch Gradient Norm after: 8.64192261374531
Epoch 2316/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5198643207550049
Epoch: 2316, Batch Gradient Norm: 10.605448473040227
Epoch: 2316, Batch Gradient Norm after: 10.605448473040227
Epoch 2317/10000, Prediction Accuracy = 60.948%, Loss = 0.5343077301979064
Epoch: 2317, Batch Gradient Norm: 11.803727673754143
Epoch: 2317, Batch Gradient Norm after: 11.803727673754143
Epoch 2318/10000, Prediction Accuracy = 60.926%, Loss = 0.5426260352134704
Epoch: 2318, Batch Gradient Norm: 11.832546511644283
Epoch: 2318, Batch Gradient Norm after: 11.832546511644283
Epoch 2319/10000, Prediction Accuracy = 60.928%, Loss = 0.5446960091590881
Epoch: 2319, Batch Gradient Norm: 10.421335072817563
Epoch: 2319, Batch Gradient Norm after: 10.421335072817563
Epoch 2320/10000, Prediction Accuracy = 60.972%, Loss = 0.5316808938980102
Epoch: 2320, Batch Gradient Norm: 11.919539304241958
Epoch: 2320, Batch Gradient Norm after: 11.919539304241958
Epoch 2321/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5409291625022888
Epoch: 2321, Batch Gradient Norm: 8.747614231762544
Epoch: 2321, Batch Gradient Norm after: 8.747614231762544
Epoch 2322/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.5223438501358032
Epoch: 2322, Batch Gradient Norm: 9.644810030129323
Epoch: 2322, Batch Gradient Norm after: 9.644810030129323
Epoch 2323/10000, Prediction Accuracy = 60.874%, Loss = 0.530367624759674
Epoch: 2323, Batch Gradient Norm: 9.16141605887602
Epoch: 2323, Batch Gradient Norm after: 9.16141605887602
Epoch 2324/10000, Prediction Accuracy = 61.0%, Loss = 0.5246914267539978
Epoch: 2324, Batch Gradient Norm: 11.649951852102992
Epoch: 2324, Batch Gradient Norm after: 11.649951852102992
Epoch 2325/10000, Prediction Accuracy = 60.964%, Loss = 0.5403260469436646
Epoch: 2325, Batch Gradient Norm: 11.879707694678707
Epoch: 2325, Batch Gradient Norm after: 11.879707694678707
Epoch 2326/10000, Prediction Accuracy = 61.012%, Loss = 0.5444120168685913
Epoch: 2326, Batch Gradient Norm: 14.693314859015883
Epoch: 2326, Batch Gradient Norm after: 14.693314859015883
Epoch 2327/10000, Prediction Accuracy = 61.02%, Loss = 0.567947781085968
Epoch: 2327, Batch Gradient Norm: 12.40146740740346
Epoch: 2327, Batch Gradient Norm after: 12.40146740740346
Epoch 2328/10000, Prediction Accuracy = 60.936%, Loss = 0.550550103187561
Epoch: 2328, Batch Gradient Norm: 13.16563707443306
Epoch: 2328, Batch Gradient Norm after: 13.16563707443306
Epoch 2329/10000, Prediction Accuracy = 60.952%, Loss = 0.5541175603866577
Epoch: 2329, Batch Gradient Norm: 12.213942929354943
Epoch: 2329, Batch Gradient Norm after: 12.213942929354943
Epoch 2330/10000, Prediction Accuracy = 60.988%, Loss = 0.5454210638999939
Epoch: 2330, Batch Gradient Norm: 8.577494366233205
Epoch: 2330, Batch Gradient Norm after: 8.577494366233205
Epoch 2331/10000, Prediction Accuracy = 61.038%, Loss = 0.5211180031299592
Epoch: 2331, Batch Gradient Norm: 11.126133652203702
Epoch: 2331, Batch Gradient Norm after: 11.126133652203702
Epoch 2332/10000, Prediction Accuracy = 60.984%, Loss = 0.539761745929718
Epoch: 2332, Batch Gradient Norm: 11.003312457554529
Epoch: 2332, Batch Gradient Norm after: 11.003312457554529
Epoch 2333/10000, Prediction Accuracy = 60.86800000000001%, Loss = 0.5417404532432556
Epoch: 2333, Batch Gradient Norm: 7.187459124468963
Epoch: 2333, Batch Gradient Norm after: 7.187459124468963
Epoch 2334/10000, Prediction Accuracy = 61.012%, Loss = 0.5166713476181031
Epoch: 2334, Batch Gradient Norm: 9.778361434185054
Epoch: 2334, Batch Gradient Norm after: 9.778361434185054
Epoch 2335/10000, Prediction Accuracy = 60.98%, Loss = 0.5299558281898499
Epoch: 2335, Batch Gradient Norm: 9.589594135909747
Epoch: 2335, Batch Gradient Norm after: 9.589594135909747
Epoch 2336/10000, Prediction Accuracy = 61.064%, Loss = 0.5270802855491639
Epoch: 2336, Batch Gradient Norm: 9.982532279664708
Epoch: 2336, Batch Gradient Norm after: 9.982532279664708
Epoch 2337/10000, Prediction Accuracy = 60.928%, Loss = 0.5258688688278198
Epoch: 2337, Batch Gradient Norm: 14.12792173672773
Epoch: 2337, Batch Gradient Norm after: 14.12792173672773
Epoch 2338/10000, Prediction Accuracy = 60.898%, Loss = 0.5586085557937622
Epoch: 2338, Batch Gradient Norm: 11.506043262417606
Epoch: 2338, Batch Gradient Norm after: 11.506043262417606
Epoch 2339/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.5385953903198242
Epoch: 2339, Batch Gradient Norm: 10.111208660748593
Epoch: 2339, Batch Gradient Norm after: 10.111208660748593
Epoch 2340/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.5291360974311828
Epoch: 2340, Batch Gradient Norm: 10.821463341610391
Epoch: 2340, Batch Gradient Norm after: 10.821463341610391
Epoch 2341/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.5338512897491455
Epoch: 2341, Batch Gradient Norm: 9.152189336056765
Epoch: 2341, Batch Gradient Norm after: 9.152189336056765
Epoch 2342/10000, Prediction Accuracy = 60.84400000000001%, Loss = 0.5228418827056884
Epoch: 2342, Batch Gradient Norm: 11.51206827459876
Epoch: 2342, Batch Gradient Norm after: 11.51206827459876
Epoch 2343/10000, Prediction Accuracy = 60.926%, Loss = 0.5405919551849365
Epoch: 2343, Batch Gradient Norm: 11.321979582718107
Epoch: 2343, Batch Gradient Norm after: 11.321979582718107
Epoch 2344/10000, Prediction Accuracy = 60.838%, Loss = 0.5406716585159301
Epoch: 2344, Batch Gradient Norm: 9.742967613205307
Epoch: 2344, Batch Gradient Norm after: 9.742967613205307
Epoch 2345/10000, Prediction Accuracy = 60.995999999999995%, Loss = 0.5327815294265748
Epoch: 2345, Batch Gradient Norm: 11.13528607343134
Epoch: 2345, Batch Gradient Norm after: 11.13528607343134
Epoch 2346/10000, Prediction Accuracy = 60.878%, Loss = 0.5474626779556274
Epoch: 2346, Batch Gradient Norm: 8.633695005596696
Epoch: 2346, Batch Gradient Norm after: 8.633695005596696
Epoch 2347/10000, Prediction Accuracy = 61.11600000000001%, Loss = 0.521664845943451
Epoch: 2347, Batch Gradient Norm: 12.333365319314774
Epoch: 2347, Batch Gradient Norm after: 12.333365319314774
Epoch 2348/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5508154273033142
Epoch: 2348, Batch Gradient Norm: 9.64311770187498
Epoch: 2348, Batch Gradient Norm after: 9.64311770187498
Epoch 2349/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5267273187637329
Epoch: 2349, Batch Gradient Norm: 10.126101513951124
Epoch: 2349, Batch Gradient Norm after: 10.126101513951124
Epoch 2350/10000, Prediction Accuracy = 61.074%, Loss = 0.528083598613739
Epoch: 2350, Batch Gradient Norm: 12.43529663555047
Epoch: 2350, Batch Gradient Norm after: 12.43529663555047
Epoch 2351/10000, Prediction Accuracy = 61.062%, Loss = 0.5433522701263428
Epoch: 2351, Batch Gradient Norm: 9.780418730555759
Epoch: 2351, Batch Gradient Norm after: 9.780418730555759
Epoch 2352/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5243638515472412
Epoch: 2352, Batch Gradient Norm: 12.968542959251794
Epoch: 2352, Batch Gradient Norm after: 12.968542959251794
Epoch 2353/10000, Prediction Accuracy = 60.92%, Loss = 0.5470651865005494
Epoch: 2353, Batch Gradient Norm: 14.896067582893757
Epoch: 2353, Batch Gradient Norm after: 14.896067582893757
Epoch 2354/10000, Prediction Accuracy = 60.98%, Loss = 0.5674799084663391
Epoch: 2354, Batch Gradient Norm: 10.704569731167545
Epoch: 2354, Batch Gradient Norm after: 10.704569731167545
Epoch 2355/10000, Prediction Accuracy = 60.92%, Loss = 0.5332685351371765
Epoch: 2355, Batch Gradient Norm: 8.66704321519324
Epoch: 2355, Batch Gradient Norm after: 8.66704321519324
Epoch 2356/10000, Prediction Accuracy = 61.048%, Loss = 0.5230283737182617
Epoch: 2356, Batch Gradient Norm: 8.074977757295501
Epoch: 2356, Batch Gradient Norm after: 8.074977757295501
Epoch 2357/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.5198437929153442
Epoch: 2357, Batch Gradient Norm: 9.143561052980209
Epoch: 2357, Batch Gradient Norm after: 9.143561052980209
Epoch 2358/10000, Prediction Accuracy = 60.98%, Loss = 0.5235264301300049
Epoch: 2358, Batch Gradient Norm: 9.973143171088754
Epoch: 2358, Batch Gradient Norm after: 9.973143171088754
Epoch 2359/10000, Prediction Accuracy = 61.016%, Loss = 0.5315786838531494
Epoch: 2359, Batch Gradient Norm: 10.792092468684933
Epoch: 2359, Batch Gradient Norm after: 10.792092468684933
Epoch 2360/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.532958698272705
Epoch: 2360, Batch Gradient Norm: 11.219694180333956
Epoch: 2360, Batch Gradient Norm after: 11.219694180333956
Epoch 2361/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5384633183479309
Epoch: 2361, Batch Gradient Norm: 12.069752975919972
Epoch: 2361, Batch Gradient Norm after: 12.069752975919972
Epoch 2362/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.546865439414978
Epoch: 2362, Batch Gradient Norm: 12.741864331040809
Epoch: 2362, Batch Gradient Norm after: 12.741864331040809
Epoch 2363/10000, Prediction Accuracy = 60.96%, Loss = 0.545391583442688
Epoch: 2363, Batch Gradient Norm: 14.808181582088812
Epoch: 2363, Batch Gradient Norm after: 14.808181582088812
Epoch 2364/10000, Prediction Accuracy = 60.96600000000001%, Loss = 0.5677279591560364
Epoch: 2364, Batch Gradient Norm: 10.873298260745305
Epoch: 2364, Batch Gradient Norm after: 10.873298260745305
Epoch 2365/10000, Prediction Accuracy = 61.06%, Loss = 0.5349766731262207
Epoch: 2365, Batch Gradient Norm: 7.961232895399017
Epoch: 2365, Batch Gradient Norm after: 7.961232895399017
Epoch 2366/10000, Prediction Accuracy = 60.948%, Loss = 0.5203924179077148
Epoch: 2366, Batch Gradient Norm: 6.565019189318895
Epoch: 2366, Batch Gradient Norm after: 6.565019189318895
Epoch 2367/10000, Prediction Accuracy = 61.01800000000001%, Loss = 0.5107848286628723
Epoch: 2367, Batch Gradient Norm: 9.472924250482821
Epoch: 2367, Batch Gradient Norm after: 9.472924250482821
Epoch 2368/10000, Prediction Accuracy = 61.108000000000004%, Loss = 0.52793288230896
Epoch: 2368, Batch Gradient Norm: 12.130948468322755
Epoch: 2368, Batch Gradient Norm after: 12.130948468322755
Epoch 2369/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5454850673675538
Epoch: 2369, Batch Gradient Norm: 14.212536864984518
Epoch: 2369, Batch Gradient Norm after: 14.212536864984518
Epoch 2370/10000, Prediction Accuracy = 60.912%, Loss = 0.5656358361244201
Epoch: 2370, Batch Gradient Norm: 12.324867422106818
Epoch: 2370, Batch Gradient Norm after: 12.324867422106818
Epoch 2371/10000, Prediction Accuracy = 60.99400000000001%, Loss = 0.5420557856559753
Epoch: 2371, Batch Gradient Norm: 9.103998794746925
Epoch: 2371, Batch Gradient Norm after: 9.103998794746925
Epoch 2372/10000, Prediction Accuracy = 61.004%, Loss = 0.5241081953048706
Epoch: 2372, Batch Gradient Norm: 10.566184351427845
Epoch: 2372, Batch Gradient Norm after: 10.566184351427845
Epoch 2373/10000, Prediction Accuracy = 60.982000000000006%, Loss = 0.5310968041419983
Epoch: 2373, Batch Gradient Norm: 12.843045908203088
Epoch: 2373, Batch Gradient Norm after: 12.843045908203088
Epoch 2374/10000, Prediction Accuracy = 60.898%, Loss = 0.5462016344070435
Epoch: 2374, Batch Gradient Norm: 10.890514642669006
Epoch: 2374, Batch Gradient Norm after: 10.890514642669006
Epoch 2375/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.531048309803009
Epoch: 2375, Batch Gradient Norm: 9.548252255819062
Epoch: 2375, Batch Gradient Norm after: 9.548252255819062
Epoch 2376/10000, Prediction Accuracy = 61.02%, Loss = 0.5224945902824402
Epoch: 2376, Batch Gradient Norm: 13.20088056926717
Epoch: 2376, Batch Gradient Norm after: 13.20088056926717
Epoch 2377/10000, Prediction Accuracy = 61.036%, Loss = 0.548592472076416
Epoch: 2377, Batch Gradient Norm: 12.763258964893065
Epoch: 2377, Batch Gradient Norm after: 12.763258964893065
Epoch 2378/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5507933497428894
Epoch: 2378, Batch Gradient Norm: 8.880646193908236
Epoch: 2378, Batch Gradient Norm after: 8.880646193908236
Epoch 2379/10000, Prediction Accuracy = 60.976%, Loss = 0.5210101127624511
Epoch: 2379, Batch Gradient Norm: 11.603772165278157
Epoch: 2379, Batch Gradient Norm after: 11.603772165278157
Epoch 2380/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.5381669163703918
Epoch: 2380, Batch Gradient Norm: 12.196557361107285
Epoch: 2380, Batch Gradient Norm after: 12.196557361107285
Epoch 2381/10000, Prediction Accuracy = 60.958000000000006%, Loss = 0.5448876261711121
Epoch: 2381, Batch Gradient Norm: 10.742390655644574
Epoch: 2381, Batch Gradient Norm after: 10.742390655644574
Epoch 2382/10000, Prediction Accuracy = 60.944%, Loss = 0.5340698719024658
Epoch: 2382, Batch Gradient Norm: 10.184166651336376
Epoch: 2382, Batch Gradient Norm after: 10.184166651336376
Epoch 2383/10000, Prediction Accuracy = 60.976%, Loss = 0.5286495089530945
Epoch: 2383, Batch Gradient Norm: 6.828366315656697
Epoch: 2383, Batch Gradient Norm after: 6.828366315656697
Epoch 2384/10000, Prediction Accuracy = 60.962%, Loss = 0.511275839805603
Epoch: 2384, Batch Gradient Norm: 8.459667558964123
Epoch: 2384, Batch Gradient Norm after: 8.459667558964123
Epoch 2385/10000, Prediction Accuracy = 61.016%, Loss = 0.5178955674171448
Epoch: 2385, Batch Gradient Norm: 11.601829930855699
Epoch: 2385, Batch Gradient Norm after: 11.601829930855699
Epoch 2386/10000, Prediction Accuracy = 61.068000000000005%, Loss = 0.5405822038650513
Epoch: 2386, Batch Gradient Norm: 12.355680283081462
Epoch: 2386, Batch Gradient Norm after: 12.355680283081462
Epoch 2387/10000, Prediction Accuracy = 61.036%, Loss = 0.5445584297180176
Epoch: 2387, Batch Gradient Norm: 10.642312641879908
Epoch: 2387, Batch Gradient Norm after: 10.642312641879908
Epoch 2388/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.5298661947250366
Epoch: 2388, Batch Gradient Norm: 6.669938198275374
Epoch: 2388, Batch Gradient Norm after: 6.669938198275374
Epoch 2389/10000, Prediction Accuracy = 61.05999999999999%, Loss = 0.5086774230003357
Epoch: 2389, Batch Gradient Norm: 9.327432830520028
Epoch: 2389, Batch Gradient Norm after: 9.327432830520028
Epoch 2390/10000, Prediction Accuracy = 60.94%, Loss = 0.5262349009513855
Epoch: 2390, Batch Gradient Norm: 11.694624978462903
Epoch: 2390, Batch Gradient Norm after: 11.694624978462903
Epoch 2391/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.538036298751831
Epoch: 2391, Batch Gradient Norm: 14.933746923838441
Epoch: 2391, Batch Gradient Norm after: 14.933746923838441
Epoch 2392/10000, Prediction Accuracy = 61.052%, Loss = 0.5623220324516296
Epoch: 2392, Batch Gradient Norm: 9.576869906288595
Epoch: 2392, Batch Gradient Norm after: 9.576869906288595
Epoch 2393/10000, Prediction Accuracy = 61.048%, Loss = 0.523844051361084
Epoch: 2393, Batch Gradient Norm: 11.338361256912009
Epoch: 2393, Batch Gradient Norm after: 11.338361256912009
Epoch 2394/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.53510080575943
Epoch: 2394, Batch Gradient Norm: 11.60859464843167
Epoch: 2394, Batch Gradient Norm after: 11.60859464843167
Epoch 2395/10000, Prediction Accuracy = 60.948%, Loss = 0.5392073869705201
Epoch: 2395, Batch Gradient Norm: 9.978397719795193
Epoch: 2395, Batch Gradient Norm after: 9.978397719795193
Epoch 2396/10000, Prediction Accuracy = 61.010000000000005%, Loss = 0.5252501964569092
Epoch: 2396, Batch Gradient Norm: 8.929792327607068
Epoch: 2396, Batch Gradient Norm after: 8.929792327607068
Epoch 2397/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5185849905014038
Epoch: 2397, Batch Gradient Norm: 10.144463485306536
Epoch: 2397, Batch Gradient Norm after: 10.144463485306536
Epoch 2398/10000, Prediction Accuracy = 61.016%, Loss = 0.5272313117980957
Epoch: 2398, Batch Gradient Norm: 10.455483442095163
Epoch: 2398, Batch Gradient Norm after: 10.455483442095163
Epoch 2399/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5283957242965698
Epoch: 2399, Batch Gradient Norm: 11.517058244951315
Epoch: 2399, Batch Gradient Norm after: 11.517058244951315
Epoch 2400/10000, Prediction Accuracy = 60.968%, Loss = 0.5349780559539795
Epoch: 2400, Batch Gradient Norm: 9.226922697694798
Epoch: 2400, Batch Gradient Norm after: 9.226922697694798
Epoch 2401/10000, Prediction Accuracy = 61.068%, Loss = 0.5238083481788636
Epoch: 2401, Batch Gradient Norm: 8.23594425017562
Epoch: 2401, Batch Gradient Norm after: 8.23594425017562
Epoch 2402/10000, Prediction Accuracy = 61.074%, Loss = 0.5167434334754943
Epoch: 2402, Batch Gradient Norm: 10.40664281449579
Epoch: 2402, Batch Gradient Norm after: 10.40664281449579
Epoch 2403/10000, Prediction Accuracy = 60.944%, Loss = 0.5370296716690064
Epoch: 2403, Batch Gradient Norm: 8.818324395773661
Epoch: 2403, Batch Gradient Norm after: 8.818324395773661
Epoch 2404/10000, Prediction Accuracy = 61.05799999999999%, Loss = 0.5192712903022766
Epoch: 2404, Batch Gradient Norm: 17.252443535513308
Epoch: 2404, Batch Gradient Norm after: 17.252443535513308
Epoch 2405/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.5852696895599365
Epoch: 2405, Batch Gradient Norm: 13.721285653458873
Epoch: 2405, Batch Gradient Norm after: 13.721285653458873
Epoch 2406/10000, Prediction Accuracy = 61.048%, Loss = 0.5533758401870728
Epoch: 2406, Batch Gradient Norm: 10.584718370408762
Epoch: 2406, Batch Gradient Norm after: 10.584718370408762
Epoch 2407/10000, Prediction Accuracy = 61.052%, Loss = 0.5304242014884949
Epoch: 2407, Batch Gradient Norm: 9.495457290136907
Epoch: 2407, Batch Gradient Norm after: 9.495457290136907
Epoch 2408/10000, Prediction Accuracy = 61.05%, Loss = 0.5267649173736573
Epoch: 2408, Batch Gradient Norm: 9.486133718287984
Epoch: 2408, Batch Gradient Norm after: 9.486133718287984
Epoch 2409/10000, Prediction Accuracy = 61.10799999999999%, Loss = 0.5249241590499878
Epoch: 2409, Batch Gradient Norm: 10.263947361068093
Epoch: 2409, Batch Gradient Norm after: 10.263947361068093
Epoch 2410/10000, Prediction Accuracy = 60.996%, Loss = 0.5317392945289612
Epoch: 2410, Batch Gradient Norm: 7.01251263517784
Epoch: 2410, Batch Gradient Norm after: 7.01251263517784
Epoch 2411/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.5092052578926086
Epoch: 2411, Batch Gradient Norm: 10.127838359091664
Epoch: 2411, Batch Gradient Norm after: 10.127838359091664
Epoch 2412/10000, Prediction Accuracy = 61.048%, Loss = 0.5262045979499816
Epoch: 2412, Batch Gradient Norm: 15.994101935501963
Epoch: 2412, Batch Gradient Norm after: 15.994101935501963
Epoch 2413/10000, Prediction Accuracy = 61.016%, Loss = 0.5759252190589905
Epoch: 2413, Batch Gradient Norm: 9.772646878019259
Epoch: 2413, Batch Gradient Norm after: 9.772646878019259
Epoch 2414/10000, Prediction Accuracy = 60.98%, Loss = 0.5247761964797973
Epoch: 2414, Batch Gradient Norm: 7.531691489062007
Epoch: 2414, Batch Gradient Norm after: 7.531691489062007
Epoch 2415/10000, Prediction Accuracy = 61.048%, Loss = 0.5111123979091644
Epoch: 2415, Batch Gradient Norm: 10.903726128433835
Epoch: 2415, Batch Gradient Norm after: 10.903726128433835
Epoch 2416/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5318063378334046
Epoch: 2416, Batch Gradient Norm: 11.641953794834095
Epoch: 2416, Batch Gradient Norm after: 11.641953794834095
Epoch 2417/10000, Prediction Accuracy = 61.024%, Loss = 0.5321307420730591
Epoch: 2417, Batch Gradient Norm: 13.518609944754719
Epoch: 2417, Batch Gradient Norm after: 13.518609944754719
Epoch 2418/10000, Prediction Accuracy = 61.008%, Loss = 0.5475707650184631
Epoch: 2418, Batch Gradient Norm: 12.113304401439933
Epoch: 2418, Batch Gradient Norm after: 12.113304401439933
Epoch 2419/10000, Prediction Accuracy = 60.93000000000001%, Loss = 0.5380540728569031
Epoch: 2419, Batch Gradient Norm: 7.922698740665954
Epoch: 2419, Batch Gradient Norm after: 7.922698740665954
Epoch 2420/10000, Prediction Accuracy = 61.016%, Loss = 0.5145336151123047
Epoch: 2420, Batch Gradient Norm: 9.947884790253674
Epoch: 2420, Batch Gradient Norm after: 9.947884790253674
Epoch 2421/10000, Prediction Accuracy = 61.016%, Loss = 0.5255685448646545
Epoch: 2421, Batch Gradient Norm: 11.343029187874526
Epoch: 2421, Batch Gradient Norm after: 11.343029187874526
Epoch 2422/10000, Prediction Accuracy = 61.05799999999999%, Loss = 0.5327481150627136
Epoch: 2422, Batch Gradient Norm: 12.954099376311634
Epoch: 2422, Batch Gradient Norm after: 12.954099376311634
Epoch 2423/10000, Prediction Accuracy = 60.984%, Loss = 0.5480804085731507
Epoch: 2423, Batch Gradient Norm: 11.645626579067425
Epoch: 2423, Batch Gradient Norm after: 11.645626579067425
Epoch 2424/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.5363136649131774
Epoch: 2424, Batch Gradient Norm: 8.745143223113017
Epoch: 2424, Batch Gradient Norm after: 8.745143223113017
Epoch 2425/10000, Prediction Accuracy = 61.00600000000001%, Loss = 0.520203173160553
Epoch: 2425, Batch Gradient Norm: 11.82773914204936
Epoch: 2425, Batch Gradient Norm after: 11.82773914204936
Epoch 2426/10000, Prediction Accuracy = 60.94000000000001%, Loss = 0.5435765862464905
Epoch: 2426, Batch Gradient Norm: 8.449950888608756
Epoch: 2426, Batch Gradient Norm after: 8.449950888608756
Epoch 2427/10000, Prediction Accuracy = 60.932%, Loss = 0.5174625277519226
Epoch: 2427, Batch Gradient Norm: 7.599973732050795
Epoch: 2427, Batch Gradient Norm after: 7.599973732050795
Epoch 2428/10000, Prediction Accuracy = 61.11800000000001%, Loss = 0.5114073395729065
Epoch: 2428, Batch Gradient Norm: 13.517423455524144
Epoch: 2428, Batch Gradient Norm after: 13.517423455524144
Epoch 2429/10000, Prediction Accuracy = 61.092%, Loss = 0.5535822749137879
Epoch: 2429, Batch Gradient Norm: 11.541332381645365
Epoch: 2429, Batch Gradient Norm after: 11.541332381645365
Epoch 2430/10000, Prediction Accuracy = 61.114%, Loss = 0.5497819304466247
Epoch: 2430, Batch Gradient Norm: 5.648056953399668
Epoch: 2430, Batch Gradient Norm after: 5.648056953399668
Epoch 2431/10000, Prediction Accuracy = 61.112%, Loss = 0.5026971817016601
Epoch: 2431, Batch Gradient Norm: 8.861036171914462
Epoch: 2431, Batch Gradient Norm after: 8.861036171914462
Epoch 2432/10000, Prediction Accuracy = 61.02%, Loss = 0.5177449107170105
Epoch: 2432, Batch Gradient Norm: 11.155167120861739
Epoch: 2432, Batch Gradient Norm after: 11.155167120861739
Epoch 2433/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.5327925682067871
Epoch: 2433, Batch Gradient Norm: 13.715560642856035
Epoch: 2433, Batch Gradient Norm after: 13.715560642856035
Epoch 2434/10000, Prediction Accuracy = 60.928%, Loss = 0.5569493174552917
Epoch: 2434, Batch Gradient Norm: 10.093565115497716
Epoch: 2434, Batch Gradient Norm after: 10.093565115497716
Epoch 2435/10000, Prediction Accuracy = 61.053999999999995%, Loss = 0.5250954866409302
Epoch: 2435, Batch Gradient Norm: 12.030918910194195
Epoch: 2435, Batch Gradient Norm after: 12.030918910194195
Epoch 2436/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.5386018991470337
Epoch: 2436, Batch Gradient Norm: 11.626262336854152
Epoch: 2436, Batch Gradient Norm after: 11.626262336854152
Epoch 2437/10000, Prediction Accuracy = 61.04799999999999%, Loss = 0.5391700625419616
Epoch: 2437, Batch Gradient Norm: 12.147746720752409
Epoch: 2437, Batch Gradient Norm after: 12.147746720752409
Epoch 2438/10000, Prediction Accuracy = 61.080000000000005%, Loss = 0.5402632832527161
Epoch: 2438, Batch Gradient Norm: 11.444123182567727
Epoch: 2438, Batch Gradient Norm after: 11.444123182567727
Epoch 2439/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.5396932363510132
Epoch: 2439, Batch Gradient Norm: 11.084996824092018
Epoch: 2439, Batch Gradient Norm after: 11.084996824092018
Epoch 2440/10000, Prediction Accuracy = 61.02%, Loss = 0.5385734915733338
Epoch: 2440, Batch Gradient Norm: 12.444484428714748
Epoch: 2440, Batch Gradient Norm after: 12.444484428714748
Epoch 2441/10000, Prediction Accuracy = 60.948%, Loss = 0.5415437936782836
Epoch: 2441, Batch Gradient Norm: 8.824795709568452
Epoch: 2441, Batch Gradient Norm after: 8.824795709568452
Epoch 2442/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.5192224740982055
Epoch: 2442, Batch Gradient Norm: 8.454887009122807
Epoch: 2442, Batch Gradient Norm after: 8.454887009122807
Epoch 2443/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5189504981040954
Epoch: 2443, Batch Gradient Norm: 6.041938271876709
Epoch: 2443, Batch Gradient Norm after: 6.041938271876709
Epoch 2444/10000, Prediction Accuracy = 61.00200000000001%, Loss = 0.5058951318264008
Epoch: 2444, Batch Gradient Norm: 10.720686315320824
Epoch: 2444, Batch Gradient Norm after: 10.720686315320824
Epoch 2445/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.5302820563316345
Epoch: 2445, Batch Gradient Norm: 15.0476015110096
Epoch: 2445, Batch Gradient Norm after: 15.0476015110096
Epoch 2446/10000, Prediction Accuracy = 61.1%, Loss = 0.5669144272804261
Epoch: 2446, Batch Gradient Norm: 11.28818380921808
Epoch: 2446, Batch Gradient Norm after: 11.28818380921808
Epoch 2447/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.534187400341034
Epoch: 2447, Batch Gradient Norm: 11.474163983368477
Epoch: 2447, Batch Gradient Norm after: 11.474163983368477
Epoch 2448/10000, Prediction Accuracy = 60.992%, Loss = 0.5327921271324157
Epoch: 2448, Batch Gradient Norm: 10.397736189026144
Epoch: 2448, Batch Gradient Norm after: 10.397736189026144
Epoch 2449/10000, Prediction Accuracy = 61.188%, Loss = 0.5254161477088928
Epoch: 2449, Batch Gradient Norm: 12.830921211228212
Epoch: 2449, Batch Gradient Norm after: 12.830921211228212
Epoch 2450/10000, Prediction Accuracy = 61.04%, Loss = 0.548540472984314
Epoch: 2450, Batch Gradient Norm: 8.063402183988417
Epoch: 2450, Batch Gradient Norm after: 8.063402183988417
Epoch 2451/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.5124801158905029
Epoch: 2451, Batch Gradient Norm: 13.18322268856493
Epoch: 2451, Batch Gradient Norm after: 13.18322268856493
Epoch 2452/10000, Prediction Accuracy = 60.972%, Loss = 0.5479307889938354
Epoch: 2452, Batch Gradient Norm: 14.102474264816312
Epoch: 2452, Batch Gradient Norm after: 14.102474264816312
Epoch 2453/10000, Prediction Accuracy = 61.028%, Loss = 0.552316677570343
Epoch: 2453, Batch Gradient Norm: 7.670391716285321
Epoch: 2453, Batch Gradient Norm after: 7.670391716285321
Epoch 2454/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.512311863899231
Epoch: 2454, Batch Gradient Norm: 8.064218337099687
Epoch: 2454, Batch Gradient Norm after: 8.064218337099687
Epoch 2455/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.5127958059310913
Epoch: 2455, Batch Gradient Norm: 12.43913128376044
Epoch: 2455, Batch Gradient Norm after: 12.43913128376044
Epoch 2456/10000, Prediction Accuracy = 61.028%, Loss = 0.5388801097869873
Epoch: 2456, Batch Gradient Norm: 12.066938087488206
Epoch: 2456, Batch Gradient Norm after: 12.066938087488206
Epoch 2457/10000, Prediction Accuracy = 61.145999999999994%, Loss = 0.536409068107605
Epoch: 2457, Batch Gradient Norm: 11.04741237696363
Epoch: 2457, Batch Gradient Norm after: 11.04741237696363
Epoch 2458/10000, Prediction Accuracy = 61.068%, Loss = 0.5281084656715394
Epoch: 2458, Batch Gradient Norm: 6.583973796704643
Epoch: 2458, Batch Gradient Norm after: 6.583973796704643
Epoch 2459/10000, Prediction Accuracy = 61.10600000000001%, Loss = 0.5050498366355896
Epoch: 2459, Batch Gradient Norm: 10.62228229274282
Epoch: 2459, Batch Gradient Norm after: 10.62228229274282
Epoch 2460/10000, Prediction Accuracy = 61.074%, Loss = 0.5284693241119385
Epoch: 2460, Batch Gradient Norm: 8.709091448614187
Epoch: 2460, Batch Gradient Norm after: 8.709091448614187
Epoch 2461/10000, Prediction Accuracy = 61.036%, Loss = 0.515334403514862
Epoch: 2461, Batch Gradient Norm: 14.17693767208413
Epoch: 2461, Batch Gradient Norm after: 14.17693767208413
Epoch 2462/10000, Prediction Accuracy = 61.169999999999995%, Loss = 0.5512977242469788
Epoch: 2462, Batch Gradient Norm: 11.372575661051062
Epoch: 2462, Batch Gradient Norm after: 11.372575661051062
Epoch 2463/10000, Prediction Accuracy = 61.02%, Loss = 0.5318333745002747
Epoch: 2463, Batch Gradient Norm: 11.154926290177498
Epoch: 2463, Batch Gradient Norm after: 11.154926290177498
Epoch 2464/10000, Prediction Accuracy = 61.09599999999999%, Loss = 0.5364617347717285
Epoch: 2464, Batch Gradient Norm: 11.316363702111078
Epoch: 2464, Batch Gradient Norm after: 11.316363702111078
Epoch 2465/10000, Prediction Accuracy = 61.048%, Loss = 0.5360693216323853
Epoch: 2465, Batch Gradient Norm: 8.801894314919513
Epoch: 2465, Batch Gradient Norm after: 8.801894314919513
Epoch 2466/10000, Prediction Accuracy = 60.938%, Loss = 0.5186775684356689
Epoch: 2466, Batch Gradient Norm: 8.681467581765672
Epoch: 2466, Batch Gradient Norm after: 8.681467581765672
Epoch 2467/10000, Prediction Accuracy = 61.044000000000004%, Loss = 0.516558051109314
Epoch: 2467, Batch Gradient Norm: 11.692152788750347
Epoch: 2467, Batch Gradient Norm after: 11.692152788750347
Epoch 2468/10000, Prediction Accuracy = 61.086%, Loss = 0.5356498956680298
Epoch: 2468, Batch Gradient Norm: 11.855359251659065
Epoch: 2468, Batch Gradient Norm after: 11.855359251659065
Epoch 2469/10000, Prediction Accuracy = 61.11%, Loss = 0.5430536031723022
Epoch: 2469, Batch Gradient Norm: 7.873750809087112
Epoch: 2469, Batch Gradient Norm after: 7.873750809087112
Epoch 2470/10000, Prediction Accuracy = 61.012%, Loss = 0.514128851890564
Epoch: 2470, Batch Gradient Norm: 10.357131997156046
Epoch: 2470, Batch Gradient Norm after: 10.357131997156046
Epoch 2471/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5246563613414764
Epoch: 2471, Batch Gradient Norm: 12.366875291323211
Epoch: 2471, Batch Gradient Norm after: 12.366875291323211
Epoch 2472/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5415526032447815
Epoch: 2472, Batch Gradient Norm: 11.049138960702386
Epoch: 2472, Batch Gradient Norm after: 11.049138960702386
Epoch 2473/10000, Prediction Accuracy = 61.2%, Loss = 0.5316156387329102
Epoch: 2473, Batch Gradient Norm: 11.387200610100157
Epoch: 2473, Batch Gradient Norm after: 11.387200610100157
Epoch 2474/10000, Prediction Accuracy = 61.074%, Loss = 0.5313431262969971
Epoch: 2474, Batch Gradient Norm: 9.91704888950334
Epoch: 2474, Batch Gradient Norm after: 9.91704888950334
Epoch 2475/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5253779888153076
Epoch: 2475, Batch Gradient Norm: 10.293823602916849
Epoch: 2475, Batch Gradient Norm after: 10.293823602916849
Epoch 2476/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.5271251916885376
Epoch: 2476, Batch Gradient Norm: 12.277985886859387
Epoch: 2476, Batch Gradient Norm after: 12.277985886859387
Epoch 2477/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.5394571900367737
Epoch: 2477, Batch Gradient Norm: 11.257364283308071
Epoch: 2477, Batch Gradient Norm after: 11.257364283308071
Epoch 2478/10000, Prediction Accuracy = 61.14%, Loss = 0.5316715359687805
Epoch: 2478, Batch Gradient Norm: 11.292434346275082
Epoch: 2478, Batch Gradient Norm after: 11.292434346275082
Epoch 2479/10000, Prediction Accuracy = 60.99799999999999%, Loss = 0.5303399562835693
Epoch: 2479, Batch Gradient Norm: 10.88473023272975
Epoch: 2479, Batch Gradient Norm after: 10.88473023272975
Epoch 2480/10000, Prediction Accuracy = 61.134%, Loss = 0.5276901483535766
Epoch: 2480, Batch Gradient Norm: 11.172078764961562
Epoch: 2480, Batch Gradient Norm after: 11.172078764961562
Epoch 2481/10000, Prediction Accuracy = 61.028%, Loss = 0.5269996643066406
Epoch: 2481, Batch Gradient Norm: 13.097368732699104
Epoch: 2481, Batch Gradient Norm after: 13.097368732699104
Epoch 2482/10000, Prediction Accuracy = 61.092%, Loss = 0.5470410346984863
Epoch: 2482, Batch Gradient Norm: 9.400658482638246
Epoch: 2482, Batch Gradient Norm after: 9.400658482638246
Epoch 2483/10000, Prediction Accuracy = 61.048%, Loss = 0.5206697702407836
Epoch: 2483, Batch Gradient Norm: 8.701904945704836
Epoch: 2483, Batch Gradient Norm after: 8.701904945704836
Epoch 2484/10000, Prediction Accuracy = 61.128%, Loss = 0.5172219157218934
Epoch: 2484, Batch Gradient Norm: 8.640299875419002
Epoch: 2484, Batch Gradient Norm after: 8.640299875419002
Epoch 2485/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.517884123325348
Epoch: 2485, Batch Gradient Norm: 8.625312660065031
Epoch: 2485, Batch Gradient Norm after: 8.625312660065031
Epoch 2486/10000, Prediction Accuracy = 61.182%, Loss = 0.5136163115501404
Epoch: 2486, Batch Gradient Norm: 14.920674311243147
Epoch: 2486, Batch Gradient Norm after: 14.920674311243147
Epoch 2487/10000, Prediction Accuracy = 61.108000000000004%, Loss = 0.5662572860717774
Epoch: 2487, Batch Gradient Norm: 10.494674389041936
Epoch: 2487, Batch Gradient Norm after: 10.494674389041936
Epoch 2488/10000, Prediction Accuracy = 61.004%, Loss = 0.5347486793994903
Epoch: 2488, Batch Gradient Norm: 9.726877288260825
Epoch: 2488, Batch Gradient Norm after: 9.726877288260825
Epoch 2489/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.5265005588531494
Epoch: 2489, Batch Gradient Norm: 14.737887709355896
Epoch: 2489, Batch Gradient Norm after: 14.737887709355896
Epoch 2490/10000, Prediction Accuracy = 61.044000000000004%, Loss = 0.5549725294113159
Epoch: 2490, Batch Gradient Norm: 12.052700229175748
Epoch: 2490, Batch Gradient Norm after: 12.052700229175748
Epoch 2491/10000, Prediction Accuracy = 61.072%, Loss = 0.536622428894043
Epoch: 2491, Batch Gradient Norm: 6.976243783352448
Epoch: 2491, Batch Gradient Norm after: 6.976243783352448
Epoch 2492/10000, Prediction Accuracy = 61.048%, Loss = 0.5060519993305206
Epoch: 2492, Batch Gradient Norm: 6.50821469902909
Epoch: 2492, Batch Gradient Norm after: 6.50821469902909
Epoch 2493/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.503541749715805
Epoch: 2493, Batch Gradient Norm: 10.797659045123888
Epoch: 2493, Batch Gradient Norm after: 10.797659045123888
Epoch 2494/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5308891534805298
Epoch: 2494, Batch Gradient Norm: 13.752073268395117
Epoch: 2494, Batch Gradient Norm after: 13.752073268395117
Epoch 2495/10000, Prediction Accuracy = 60.996%, Loss = 0.5606386303901673
Epoch: 2495, Batch Gradient Norm: 12.355942702862729
Epoch: 2495, Batch Gradient Norm after: 12.355942702862729
Epoch 2496/10000, Prediction Accuracy = 61.07800000000001%, Loss = 0.5383267164230346
Epoch: 2496, Batch Gradient Norm: 12.526689919134677
Epoch: 2496, Batch Gradient Norm after: 12.526689919134677
Epoch 2497/10000, Prediction Accuracy = 61.05%, Loss = 0.5368715286254883
Epoch: 2497, Batch Gradient Norm: 7.186475021008693
Epoch: 2497, Batch Gradient Norm after: 7.186475021008693
Epoch 2498/10000, Prediction Accuracy = 61.108000000000004%, Loss = 0.5077252924442291
Epoch: 2498, Batch Gradient Norm: 8.172399568514402
Epoch: 2498, Batch Gradient Norm after: 8.172399568514402
Epoch 2499/10000, Prediction Accuracy = 61.004%, Loss = 0.5127132534980774
Epoch: 2499, Batch Gradient Norm: 9.019670603281664
Epoch: 2499, Batch Gradient Norm after: 9.019670603281664
Epoch 2500/10000, Prediction Accuracy = 61.102%, Loss = 0.5142079710960388
Epoch: 2500, Batch Gradient Norm: 13.56351771092159
Epoch: 2500, Batch Gradient Norm after: 13.56351771092159
Epoch 2501/10000, Prediction Accuracy = 61.092%, Loss = 0.5462281584739686
Epoch: 2501, Batch Gradient Norm: 9.897919233106704
Epoch: 2501, Batch Gradient Norm after: 9.897919233106704
Epoch 2502/10000, Prediction Accuracy = 61.108000000000004%, Loss = 0.5203754663467407
Epoch: 2502, Batch Gradient Norm: 9.061606274085333
Epoch: 2502, Batch Gradient Norm after: 9.061606274085333
Epoch 2503/10000, Prediction Accuracy = 61.152%, Loss = 0.5155610561370849
Epoch: 2503, Batch Gradient Norm: 10.515135884345884
Epoch: 2503, Batch Gradient Norm after: 10.515135884345884
Epoch 2504/10000, Prediction Accuracy = 61.098%, Loss = 0.5252577304840088
Epoch: 2504, Batch Gradient Norm: 11.832032683311807
Epoch: 2504, Batch Gradient Norm after: 11.832032683311807
Epoch 2505/10000, Prediction Accuracy = 61.134%, Loss = 0.5336045861244202
Epoch: 2505, Batch Gradient Norm: 12.22729041008421
Epoch: 2505, Batch Gradient Norm after: 12.22729041008421
Epoch 2506/10000, Prediction Accuracy = 61.041999999999994%, Loss = 0.5384781837463379
Epoch: 2506, Batch Gradient Norm: 13.880407816799424
Epoch: 2506, Batch Gradient Norm after: 13.880407816799424
Epoch 2507/10000, Prediction Accuracy = 61.05200000000001%, Loss = 0.5545191049575806
Epoch: 2507, Batch Gradient Norm: 10.41402215979192
Epoch: 2507, Batch Gradient Norm after: 10.41402215979192
Epoch 2508/10000, Prediction Accuracy = 61.041999999999994%, Loss = 0.5272901773452758
Epoch: 2508, Batch Gradient Norm: 9.494831237685414
Epoch: 2508, Batch Gradient Norm after: 9.494831237685414
Epoch 2509/10000, Prediction Accuracy = 61.15%, Loss = 0.5196410059928894
Epoch: 2509, Batch Gradient Norm: 10.358587976080415
Epoch: 2509, Batch Gradient Norm after: 10.358587976080415
Epoch 2510/10000, Prediction Accuracy = 61.136%, Loss = 0.5258305907249451
Epoch: 2510, Batch Gradient Norm: 10.634968362007546
Epoch: 2510, Batch Gradient Norm after: 10.634968362007546
Epoch 2511/10000, Prediction Accuracy = 61.041999999999994%, Loss = 0.5261326909065247
Epoch: 2511, Batch Gradient Norm: 9.759222113747605
Epoch: 2511, Batch Gradient Norm after: 9.759222113747605
Epoch 2512/10000, Prediction Accuracy = 61.14%, Loss = 0.5203466355800629
Epoch: 2512, Batch Gradient Norm: 9.53664368444502
Epoch: 2512, Batch Gradient Norm after: 9.53664368444502
Epoch 2513/10000, Prediction Accuracy = 61.138%, Loss = 0.5212947487831116
Epoch: 2513, Batch Gradient Norm: 13.741034670235816
Epoch: 2513, Batch Gradient Norm after: 13.741034670235816
Epoch 2514/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.546723461151123
Epoch: 2514, Batch Gradient Norm: 8.883402660294614
Epoch: 2514, Batch Gradient Norm after: 8.883402660294614
Epoch 2515/10000, Prediction Accuracy = 61.11800000000001%, Loss = 0.515033733844757
Epoch: 2515, Batch Gradient Norm: 13.625850156228877
Epoch: 2515, Batch Gradient Norm after: 13.625850156228877
Epoch 2516/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5464024186134339
Epoch: 2516, Batch Gradient Norm: 11.021034557998522
Epoch: 2516, Batch Gradient Norm after: 11.021034557998522
Epoch 2517/10000, Prediction Accuracy = 61.162%, Loss = 0.527526605129242
Epoch: 2517, Batch Gradient Norm: 9.530846671447518
Epoch: 2517, Batch Gradient Norm after: 9.530846671447518
Epoch 2518/10000, Prediction Accuracy = 61.14%, Loss = 0.515540885925293
Epoch: 2518, Batch Gradient Norm: 10.925042704073148
Epoch: 2518, Batch Gradient Norm after: 10.925042704073148
Epoch 2519/10000, Prediction Accuracy = 61.032000000000004%, Loss = 0.5275462627410888
Epoch: 2519, Batch Gradient Norm: 10.524213206773812
Epoch: 2519, Batch Gradient Norm after: 10.524213206773812
Epoch 2520/10000, Prediction Accuracy = 61.044%, Loss = 0.5213141202926636
Epoch: 2520, Batch Gradient Norm: 10.226573498465994
Epoch: 2520, Batch Gradient Norm after: 10.226573498465994
Epoch 2521/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.5215521335601807
Epoch: 2521, Batch Gradient Norm: 11.32013173025637
Epoch: 2521, Batch Gradient Norm after: 11.32013173025637
Epoch 2522/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.5307734370231628
Epoch: 2522, Batch Gradient Norm: 10.933368978604529
Epoch: 2522, Batch Gradient Norm after: 10.933368978604529
Epoch 2523/10000, Prediction Accuracy = 61.04%, Loss = 0.5303147315979004
Epoch: 2523, Batch Gradient Norm: 8.98842136283449
Epoch: 2523, Batch Gradient Norm after: 8.98842136283449
Epoch 2524/10000, Prediction Accuracy = 61.136%, Loss = 0.5154101490974426
Epoch: 2524, Batch Gradient Norm: 8.903857281026507
Epoch: 2524, Batch Gradient Norm after: 8.903857281026507
Epoch 2525/10000, Prediction Accuracy = 61.102%, Loss = 0.5137332320213318
Epoch: 2525, Batch Gradient Norm: 8.245171490821543
Epoch: 2525, Batch Gradient Norm after: 8.245171490821543
Epoch 2526/10000, Prediction Accuracy = 61.048%, Loss = 0.5133963406085968
Epoch: 2526, Batch Gradient Norm: 7.730766729446016
Epoch: 2526, Batch Gradient Norm after: 7.730766729446016
Epoch 2527/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.5103400230407715
Epoch: 2527, Batch Gradient Norm: 10.54657925013511
Epoch: 2527, Batch Gradient Norm after: 10.54657925013511
Epoch 2528/10000, Prediction Accuracy = 60.992000000000004%, Loss = 0.5255836486816406
Epoch: 2528, Batch Gradient Norm: 12.59911782335334
Epoch: 2528, Batch Gradient Norm after: 12.59911782335334
Epoch 2529/10000, Prediction Accuracy = 61.053999999999995%, Loss = 0.5364509463310242
Epoch: 2529, Batch Gradient Norm: 12.338870371134862
Epoch: 2529, Batch Gradient Norm after: 12.338870371134862
Epoch 2530/10000, Prediction Accuracy = 61.182%, Loss = 0.5350277423858643
Epoch: 2530, Batch Gradient Norm: 12.262112378103621
Epoch: 2530, Batch Gradient Norm after: 12.262112378103621
Epoch 2531/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5366873145103455
Epoch: 2531, Batch Gradient Norm: 11.526683511227427
Epoch: 2531, Batch Gradient Norm after: 11.526683511227427
Epoch 2532/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5320076227188111
Epoch: 2532, Batch Gradient Norm: 10.129512434831264
Epoch: 2532, Batch Gradient Norm after: 10.129512434831264
Epoch 2533/10000, Prediction Accuracy = 60.996%, Loss = 0.5257827043533325
Epoch: 2533, Batch Gradient Norm: 9.63728823923127
Epoch: 2533, Batch Gradient Norm after: 9.63728823923127
Epoch 2534/10000, Prediction Accuracy = 61.198%, Loss = 0.5198429584503174
Epoch: 2534, Batch Gradient Norm: 12.84548128053744
Epoch: 2534, Batch Gradient Norm after: 12.84548128053744
Epoch 2535/10000, Prediction Accuracy = 61.112%, Loss = 0.5428835630416871
Epoch: 2535, Batch Gradient Norm: 13.116777001294189
Epoch: 2535, Batch Gradient Norm after: 13.116777001294189
Epoch 2536/10000, Prediction Accuracy = 61.164%, Loss = 0.5419084191322326
Epoch: 2536, Batch Gradient Norm: 13.9196502779034
Epoch: 2536, Batch Gradient Norm after: 13.9196502779034
Epoch 2537/10000, Prediction Accuracy = 61.028%, Loss = 0.5544318675994873
Epoch: 2537, Batch Gradient Norm: 10.431898450785061
Epoch: 2537, Batch Gradient Norm after: 10.431898450785061
Epoch 2538/10000, Prediction Accuracy = 61.226%, Loss = 0.5280534386634826
Epoch: 2538, Batch Gradient Norm: 10.016420105696396
Epoch: 2538, Batch Gradient Norm after: 10.016420105696396
Epoch 2539/10000, Prediction Accuracy = 61.11800000000001%, Loss = 0.5261172652244568
Epoch: 2539, Batch Gradient Norm: 9.867319614693054
Epoch: 2539, Batch Gradient Norm after: 9.867319614693054
Epoch 2540/10000, Prediction Accuracy = 61.008%, Loss = 0.5191595196723938
Epoch: 2540, Batch Gradient Norm: 10.325557581179924
Epoch: 2540, Batch Gradient Norm after: 10.325557581179924
Epoch 2541/10000, Prediction Accuracy = 61.102%, Loss = 0.5231423258781434
Epoch: 2541, Batch Gradient Norm: 9.756390288402011
Epoch: 2541, Batch Gradient Norm after: 9.756390288402011
Epoch 2542/10000, Prediction Accuracy = 61.104%, Loss = 0.5185974836349487
Epoch: 2542, Batch Gradient Norm: 10.275012599697643
Epoch: 2542, Batch Gradient Norm after: 10.275012599697643
Epoch 2543/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.5222141027450562
Epoch: 2543, Batch Gradient Norm: 7.626824346881167
Epoch: 2543, Batch Gradient Norm after: 7.626824346881167
Epoch 2544/10000, Prediction Accuracy = 61.13199999999999%, Loss = 0.5084535598754882
Epoch: 2544, Batch Gradient Norm: 9.067402349706846
Epoch: 2544, Batch Gradient Norm after: 9.067402349706846
Epoch 2545/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.5168707370758057
Epoch: 2545, Batch Gradient Norm: 10.680003971612686
Epoch: 2545, Batch Gradient Norm after: 10.680003971612686
Epoch 2546/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5265043139457702
Epoch: 2546, Batch Gradient Norm: 11.409879975179889
Epoch: 2546, Batch Gradient Norm after: 11.409879975179889
Epoch 2547/10000, Prediction Accuracy = 61.1%, Loss = 0.528865396976471
Epoch: 2547, Batch Gradient Norm: 10.226887614383118
Epoch: 2547, Batch Gradient Norm after: 10.226887614383118
Epoch 2548/10000, Prediction Accuracy = 61.05%, Loss = 0.5208975434303283
Epoch: 2548, Batch Gradient Norm: 12.182019552152164
Epoch: 2548, Batch Gradient Norm after: 12.182019552152164
Epoch 2549/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.5403290152549743
Epoch: 2549, Batch Gradient Norm: 11.73566563235787
Epoch: 2549, Batch Gradient Norm after: 11.73566563235787
Epoch 2550/10000, Prediction Accuracy = 61.048%, Loss = 0.534577465057373
Epoch: 2550, Batch Gradient Norm: 9.41215668013625
Epoch: 2550, Batch Gradient Norm after: 9.41215668013625
Epoch 2551/10000, Prediction Accuracy = 61.1%, Loss = 0.5207022190093994
Epoch: 2551, Batch Gradient Norm: 9.624367817829505
Epoch: 2551, Batch Gradient Norm after: 9.624367817829505
Epoch 2552/10000, Prediction Accuracy = 61.09599999999999%, Loss = 0.5165823817253112
Epoch: 2552, Batch Gradient Norm: 12.758113194524311
Epoch: 2552, Batch Gradient Norm after: 12.758113194524311
Epoch 2553/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.5372139096260071
Epoch: 2553, Batch Gradient Norm: 11.091088429870707
Epoch: 2553, Batch Gradient Norm after: 11.091088429870707
Epoch 2554/10000, Prediction Accuracy = 61.194%, Loss = 0.5272804856300354
Epoch: 2554, Batch Gradient Norm: 9.957924525176367
Epoch: 2554, Batch Gradient Norm after: 9.957924525176367
Epoch 2555/10000, Prediction Accuracy = 60.944%, Loss = 0.5185364007949829
Epoch: 2555, Batch Gradient Norm: 11.948339927009501
Epoch: 2555, Batch Gradient Norm after: 11.948339927009501
Epoch 2556/10000, Prediction Accuracy = 61.222%, Loss = 0.5338657140731812
Epoch: 2556, Batch Gradient Norm: 11.771451754799727
Epoch: 2556, Batch Gradient Norm after: 11.771451754799727
Epoch 2557/10000, Prediction Accuracy = 61.128%, Loss = 0.532391619682312
Epoch: 2557, Batch Gradient Norm: 9.700564207226712
Epoch: 2557, Batch Gradient Norm after: 9.700564207226712
Epoch 2558/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5156226396560669
Epoch: 2558, Batch Gradient Norm: 14.492079219511442
Epoch: 2558, Batch Gradient Norm after: 14.492079219511442
Epoch 2559/10000, Prediction Accuracy = 61.03000000000001%, Loss = 0.5547514200210572
Epoch: 2559, Batch Gradient Norm: 11.252169484743177
Epoch: 2559, Batch Gradient Norm after: 11.252169484743177
Epoch 2560/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5268298745155334
Epoch: 2560, Batch Gradient Norm: 12.239527911449512
Epoch: 2560, Batch Gradient Norm after: 12.239527911449512
Epoch 2561/10000, Prediction Accuracy = 61.08399999999999%, Loss = 0.5394205212593078
Epoch: 2561, Batch Gradient Norm: 10.296692169442236
Epoch: 2561, Batch Gradient Norm after: 10.296692169442236
Epoch 2562/10000, Prediction Accuracy = 61.136%, Loss = 0.5223199248313903
Epoch: 2562, Batch Gradient Norm: 8.70576672147868
Epoch: 2562, Batch Gradient Norm after: 8.70576672147868
Epoch 2563/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.5097985744476319
Epoch: 2563, Batch Gradient Norm: 8.583484189107464
Epoch: 2563, Batch Gradient Norm after: 8.583484189107464
Epoch 2564/10000, Prediction Accuracy = 61.10600000000001%, Loss = 0.5106468260288238
Epoch: 2564, Batch Gradient Norm: 6.35790216588103
Epoch: 2564, Batch Gradient Norm after: 6.35790216588103
Epoch 2565/10000, Prediction Accuracy = 61.126%, Loss = 0.49962432980537413
Epoch: 2565, Batch Gradient Norm: 9.679331586460163
Epoch: 2565, Batch Gradient Norm after: 9.679331586460163
Epoch 2566/10000, Prediction Accuracy = 61.184000000000005%, Loss = 0.5192594408988953
Epoch: 2566, Batch Gradient Norm: 9.702938266757307
Epoch: 2566, Batch Gradient Norm after: 9.702938266757307
Epoch 2567/10000, Prediction Accuracy = 61.086%, Loss = 0.5192646145820617
Epoch: 2567, Batch Gradient Norm: 12.002191338646721
Epoch: 2567, Batch Gradient Norm after: 12.002191338646721
Epoch 2568/10000, Prediction Accuracy = 61.11199999999999%, Loss = 0.5329464077949524
Epoch: 2568, Batch Gradient Norm: 13.28749572775204
Epoch: 2568, Batch Gradient Norm after: 13.28749572775204
Epoch 2569/10000, Prediction Accuracy = 61.122%, Loss = 0.5432883262634277
Epoch: 2569, Batch Gradient Norm: 11.137040264204158
Epoch: 2569, Batch Gradient Norm after: 11.137040264204158
Epoch 2570/10000, Prediction Accuracy = 61.052%, Loss = 0.5281130194664001
Epoch: 2570, Batch Gradient Norm: 8.465066340694484
Epoch: 2570, Batch Gradient Norm after: 8.465066340694484
Epoch 2571/10000, Prediction Accuracy = 61.04%, Loss = 0.5082302689552307
Epoch: 2571, Batch Gradient Norm: 9.859771981757063
Epoch: 2571, Batch Gradient Norm after: 9.859771981757063
Epoch 2572/10000, Prediction Accuracy = 61.128%, Loss = 0.5147143006324768
Epoch: 2572, Batch Gradient Norm: 12.927293536187888
Epoch: 2572, Batch Gradient Norm after: 12.927293536187888
Epoch 2573/10000, Prediction Accuracy = 61.136%, Loss = 0.5395017743110657
Epoch: 2573, Batch Gradient Norm: 13.109710837697264
Epoch: 2573, Batch Gradient Norm after: 13.109710837697264
Epoch 2574/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5394187569618225
Epoch: 2574, Batch Gradient Norm: 9.014356100661347
Epoch: 2574, Batch Gradient Norm after: 9.014356100661347
Epoch 2575/10000, Prediction Accuracy = 61.162%, Loss = 0.5130721449851989
Epoch: 2575, Batch Gradient Norm: 10.172717289366773
Epoch: 2575, Batch Gradient Norm after: 10.172717289366773
Epoch 2576/10000, Prediction Accuracy = 61.132000000000005%, Loss = 0.5190620303153992
Epoch: 2576, Batch Gradient Norm: 13.985117445752831
Epoch: 2576, Batch Gradient Norm after: 13.985117445752831
Epoch 2577/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.549244225025177
Epoch: 2577, Batch Gradient Norm: 9.291263207537403
Epoch: 2577, Batch Gradient Norm after: 9.291263207537403
Epoch 2578/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5179235219955445
Epoch: 2578, Batch Gradient Norm: 9.610002054819457
Epoch: 2578, Batch Gradient Norm after: 9.610002054819457
Epoch 2579/10000, Prediction Accuracy = 61.21600000000001%, Loss = 0.5177757620811463
Epoch: 2579, Batch Gradient Norm: 9.964644293003508
Epoch: 2579, Batch Gradient Norm after: 9.964644293003508
Epoch 2580/10000, Prediction Accuracy = 61.052%, Loss = 0.5175666451454163
Epoch: 2580, Batch Gradient Norm: 9.81455525218114
Epoch: 2580, Batch Gradient Norm after: 9.81455525218114
Epoch 2581/10000, Prediction Accuracy = 61.19200000000001%, Loss = 0.5163211464881897
Epoch: 2581, Batch Gradient Norm: 12.729773778665733
Epoch: 2581, Batch Gradient Norm after: 12.729773778665733
Epoch 2582/10000, Prediction Accuracy = 61.13199999999999%, Loss = 0.5343633174896241
Epoch: 2582, Batch Gradient Norm: 12.429579504890658
Epoch: 2582, Batch Gradient Norm after: 12.429579504890658
Epoch 2583/10000, Prediction Accuracy = 61.11600000000001%, Loss = 0.5341830253601074
Epoch: 2583, Batch Gradient Norm: 12.530982171698566
Epoch: 2583, Batch Gradient Norm after: 12.530982171698566
Epoch 2584/10000, Prediction Accuracy = 61.134%, Loss = 0.5350627541542053
Epoch: 2584, Batch Gradient Norm: 10.452908537752183
Epoch: 2584, Batch Gradient Norm after: 10.452908537752183
Epoch 2585/10000, Prediction Accuracy = 61.186%, Loss = 0.5205264568328858
Epoch: 2585, Batch Gradient Norm: 11.087229458084323
Epoch: 2585, Batch Gradient Norm after: 11.087229458084323
Epoch 2586/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5271026134490967
Epoch: 2586, Batch Gradient Norm: 9.396238540840404
Epoch: 2586, Batch Gradient Norm after: 9.396238540840404
Epoch 2587/10000, Prediction Accuracy = 61.134%, Loss = 0.5154516696929932
Epoch: 2587, Batch Gradient Norm: 5.754524021598218
Epoch: 2587, Batch Gradient Norm after: 5.754524021598218
Epoch 2588/10000, Prediction Accuracy = 61.202%, Loss = 0.4974645435810089
Epoch: 2588, Batch Gradient Norm: 9.118178843538896
Epoch: 2588, Batch Gradient Norm after: 9.118178843538896
Epoch 2589/10000, Prediction Accuracy = 61.17199999999999%, Loss = 0.5120623409748077
Epoch: 2589, Batch Gradient Norm: 10.410557984202539
Epoch: 2589, Batch Gradient Norm after: 10.410557984202539
Epoch 2590/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.521083927154541
Epoch: 2590, Batch Gradient Norm: 10.638314520443295
Epoch: 2590, Batch Gradient Norm after: 10.638314520443295
Epoch 2591/10000, Prediction Accuracy = 61.254%, Loss = 0.5261927247047424
Epoch: 2591, Batch Gradient Norm: 10.7079962414849
Epoch: 2591, Batch Gradient Norm after: 10.7079962414849
Epoch 2592/10000, Prediction Accuracy = 61.093999999999994%, Loss = 0.5292046308517456
Epoch: 2592, Batch Gradient Norm: 10.561650269074642
Epoch: 2592, Batch Gradient Norm after: 10.561650269074642
Epoch 2593/10000, Prediction Accuracy = 61.19000000000001%, Loss = 0.5209259629249573
Epoch: 2593, Batch Gradient Norm: 12.639546417676854
Epoch: 2593, Batch Gradient Norm after: 12.639546417676854
Epoch 2594/10000, Prediction Accuracy = 61.19%, Loss = 0.5363144040107727
Epoch: 2594, Batch Gradient Norm: 8.40244943800394
Epoch: 2594, Batch Gradient Norm after: 8.40244943800394
Epoch 2595/10000, Prediction Accuracy = 61.174%, Loss = 0.5077648758888245
Epoch: 2595, Batch Gradient Norm: 8.788777713482078
Epoch: 2595, Batch Gradient Norm after: 8.788777713482078
Epoch 2596/10000, Prediction Accuracy = 61.13199999999999%, Loss = 0.509720367193222
Epoch: 2596, Batch Gradient Norm: 11.875350863835186
Epoch: 2596, Batch Gradient Norm after: 11.875350863835186
Epoch 2597/10000, Prediction Accuracy = 61.038%, Loss = 0.5308633327484131
Epoch: 2597, Batch Gradient Norm: 11.903934627701217
Epoch: 2597, Batch Gradient Norm after: 11.903934627701217
Epoch 2598/10000, Prediction Accuracy = 61.19799999999999%, Loss = 0.5275366187095643
Epoch: 2598, Batch Gradient Norm: 12.630258095156305
Epoch: 2598, Batch Gradient Norm after: 12.630258095156305
Epoch 2599/10000, Prediction Accuracy = 61.138%, Loss = 0.5344162106513977
Epoch: 2599, Batch Gradient Norm: 12.350270373163738
Epoch: 2599, Batch Gradient Norm after: 12.350270373163738
Epoch 2600/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.5367488741874695
Epoch: 2600, Batch Gradient Norm: 10.022248928350646
Epoch: 2600, Batch Gradient Norm after: 10.022248928350646
Epoch 2601/10000, Prediction Accuracy = 61.1%, Loss = 0.5166519165039063
Epoch: 2601, Batch Gradient Norm: 9.698815467478902
Epoch: 2601, Batch Gradient Norm after: 9.698815467478902
Epoch 2602/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.5183798789978027
Epoch: 2602, Batch Gradient Norm: 11.92128447900969
Epoch: 2602, Batch Gradient Norm after: 11.92128447900969
Epoch 2603/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.532861328125
Epoch: 2603, Batch Gradient Norm: 10.31868224950387
Epoch: 2603, Batch Gradient Norm after: 10.31868224950387
Epoch 2604/10000, Prediction Accuracy = 61.126%, Loss = 0.5188474774360656
Epoch: 2604, Batch Gradient Norm: 11.239663140307094
Epoch: 2604, Batch Gradient Norm after: 11.239663140307094
Epoch 2605/10000, Prediction Accuracy = 61.194%, Loss = 0.5265202045440673
Epoch: 2605, Batch Gradient Norm: 10.94186904709924
Epoch: 2605, Batch Gradient Norm after: 10.94186904709924
Epoch 2606/10000, Prediction Accuracy = 61.17%, Loss = 0.5273651838302612
Epoch: 2606, Batch Gradient Norm: 10.085395032347401
Epoch: 2606, Batch Gradient Norm after: 10.085395032347401
Epoch 2607/10000, Prediction Accuracy = 61.10799999999999%, Loss = 0.5215933442115783
Epoch: 2607, Batch Gradient Norm: 9.79096245423003
Epoch: 2607, Batch Gradient Norm after: 9.79096245423003
Epoch 2608/10000, Prediction Accuracy = 61.13199999999999%, Loss = 0.5151463508605957
Epoch: 2608, Batch Gradient Norm: 13.324294321223839
Epoch: 2608, Batch Gradient Norm after: 13.324294321223839
Epoch 2609/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.541104793548584
Epoch: 2609, Batch Gradient Norm: 5.844638229927997
Epoch: 2609, Batch Gradient Norm after: 5.844638229927997
Epoch 2610/10000, Prediction Accuracy = 61.198%, Loss = 0.496244478225708
Epoch: 2610, Batch Gradient Norm: 6.804109961350141
Epoch: 2610, Batch Gradient Norm after: 6.804109961350141
Epoch 2611/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.49925885796546937
Epoch: 2611, Batch Gradient Norm: 11.175834608743196
Epoch: 2611, Batch Gradient Norm after: 11.175834608743196
Epoch 2612/10000, Prediction Accuracy = 61.245999999999995%, Loss = 0.5285924553871155
Epoch: 2612, Batch Gradient Norm: 12.031260546542129
Epoch: 2612, Batch Gradient Norm after: 12.031260546542129
Epoch 2613/10000, Prediction Accuracy = 61.114%, Loss = 0.5401842713356018
Epoch: 2613, Batch Gradient Norm: 9.810082827769666
Epoch: 2613, Batch Gradient Norm after: 9.810082827769666
Epoch 2614/10000, Prediction Accuracy = 61.27%, Loss = 0.5184477686882019
Epoch: 2614, Batch Gradient Norm: 11.565063803240443
Epoch: 2614, Batch Gradient Norm after: 11.565063803240443
Epoch 2615/10000, Prediction Accuracy = 61.2%, Loss = 0.5337651968002319
Epoch: 2615, Batch Gradient Norm: 10.453193969819758
Epoch: 2615, Batch Gradient Norm after: 10.453193969819758
Epoch 2616/10000, Prediction Accuracy = 61.14200000000001%, Loss = 0.5204111993312835
Epoch: 2616, Batch Gradient Norm: 12.758906121172073
Epoch: 2616, Batch Gradient Norm after: 12.758906121172073
Epoch 2617/10000, Prediction Accuracy = 61.116%, Loss = 0.5361757040023803
Epoch: 2617, Batch Gradient Norm: 11.475295751017555
Epoch: 2617, Batch Gradient Norm after: 11.475295751017555
Epoch 2618/10000, Prediction Accuracy = 60.983999999999995%, Loss = 0.5306730508804322
Epoch: 2618, Batch Gradient Norm: 9.676471861053944
Epoch: 2618, Batch Gradient Norm after: 9.676471861053944
Epoch 2619/10000, Prediction Accuracy = 61.134%, Loss = 0.5180392384529113
Epoch: 2619, Batch Gradient Norm: 10.74263368877734
Epoch: 2619, Batch Gradient Norm after: 10.74263368877734
Epoch 2620/10000, Prediction Accuracy = 61.138%, Loss = 0.5192120313644409
Epoch: 2620, Batch Gradient Norm: 9.946636995053694
Epoch: 2620, Batch Gradient Norm after: 9.946636995053694
Epoch 2621/10000, Prediction Accuracy = 61.1%, Loss = 0.5146193027496337
Epoch: 2621, Batch Gradient Norm: 9.09270751755598
Epoch: 2621, Batch Gradient Norm after: 9.09270751755598
Epoch 2622/10000, Prediction Accuracy = 61.148%, Loss = 0.5094494104385376
Epoch: 2622, Batch Gradient Norm: 11.618218783446002
Epoch: 2622, Batch Gradient Norm after: 11.618218783446002
Epoch 2623/10000, Prediction Accuracy = 61.176%, Loss = 0.5260740518569946
Epoch: 2623, Batch Gradient Norm: 13.124199922777331
Epoch: 2623, Batch Gradient Norm after: 13.124199922777331
Epoch 2624/10000, Prediction Accuracy = 61.152%, Loss = 0.544945764541626
Epoch: 2624, Batch Gradient Norm: 11.141769940511592
Epoch: 2624, Batch Gradient Norm after: 11.141769940511592
Epoch 2625/10000, Prediction Accuracy = 61.122%, Loss = 0.5261224150657654
Epoch: 2625, Batch Gradient Norm: 13.01774298591428
Epoch: 2625, Batch Gradient Norm after: 13.01774298591428
Epoch 2626/10000, Prediction Accuracy = 61.1%, Loss = 0.536721920967102
Epoch: 2626, Batch Gradient Norm: 11.08549382739645
Epoch: 2626, Batch Gradient Norm after: 11.08549382739645
Epoch 2627/10000, Prediction Accuracy = 61.11%, Loss = 0.5254599928855896
Epoch: 2627, Batch Gradient Norm: 10.782669547873029
Epoch: 2627, Batch Gradient Norm after: 10.782669547873029
Epoch 2628/10000, Prediction Accuracy = 61.15%, Loss = 0.521000611782074
Epoch: 2628, Batch Gradient Norm: 8.76858191114197
Epoch: 2628, Batch Gradient Norm after: 8.76858191114197
Epoch 2629/10000, Prediction Accuracy = 61.25%, Loss = 0.5119322538375854
Epoch: 2629, Batch Gradient Norm: 12.168802336129371
Epoch: 2629, Batch Gradient Norm after: 12.168802336129371
Epoch 2630/10000, Prediction Accuracy = 61.174%, Loss = 0.5333414912223816
Epoch: 2630, Batch Gradient Norm: 10.919950772665544
Epoch: 2630, Batch Gradient Norm after: 10.919950772665544
Epoch 2631/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.5229909777641296
Epoch: 2631, Batch Gradient Norm: 7.740619648191801
Epoch: 2631, Batch Gradient Norm after: 7.740619648191801
Epoch 2632/10000, Prediction Accuracy = 61.141999999999996%, Loss = 0.5040707111358642
Epoch: 2632, Batch Gradient Norm: 9.13556738114706
Epoch: 2632, Batch Gradient Norm after: 9.13556738114706
Epoch 2633/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.5116946160793304
Epoch: 2633, Batch Gradient Norm: 12.68137735257067
Epoch: 2633, Batch Gradient Norm after: 12.68137735257067
Epoch 2634/10000, Prediction Accuracy = 61.104000000000006%, Loss = 0.5327815771102905
Epoch: 2634, Batch Gradient Norm: 10.655985442407475
Epoch: 2634, Batch Gradient Norm after: 10.655985442407475
Epoch 2635/10000, Prediction Accuracy = 61.312%, Loss = 0.5194816946983337
Epoch: 2635, Batch Gradient Norm: 11.000142213032658
Epoch: 2635, Batch Gradient Norm after: 11.000142213032658
Epoch 2636/10000, Prediction Accuracy = 61.09599999999999%, Loss = 0.5200287461280823
Epoch: 2636, Batch Gradient Norm: 10.427933759396339
Epoch: 2636, Batch Gradient Norm after: 10.427933759396339
Epoch 2637/10000, Prediction Accuracy = 61.18000000000001%, Loss = 0.5181757569313049
Epoch: 2637, Batch Gradient Norm: 9.797768753880012
Epoch: 2637, Batch Gradient Norm after: 9.797768753880012
Epoch 2638/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.5185194849967957
Epoch: 2638, Batch Gradient Norm: 7.318820787060707
Epoch: 2638, Batch Gradient Norm after: 7.318820787060707
Epoch 2639/10000, Prediction Accuracy = 61.354000000000006%, Loss = 0.5007328510284423
Epoch: 2639, Batch Gradient Norm: 10.980979868292419
Epoch: 2639, Batch Gradient Norm after: 10.980979868292419
Epoch 2640/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5277890801429749
Epoch: 2640, Batch Gradient Norm: 9.652378885275134
Epoch: 2640, Batch Gradient Norm after: 9.652378885275134
Epoch 2641/10000, Prediction Accuracy = 61.222%, Loss = 0.5163171887397766
Epoch: 2641, Batch Gradient Norm: 10.394094653660586
Epoch: 2641, Batch Gradient Norm after: 10.394094653660586
Epoch 2642/10000, Prediction Accuracy = 61.21400000000001%, Loss = 0.5194620966911316
Epoch: 2642, Batch Gradient Norm: 11.900973202832025
Epoch: 2642, Batch Gradient Norm after: 11.900973202832025
Epoch 2643/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.528607714176178
Epoch: 2643, Batch Gradient Norm: 13.666110459763233
Epoch: 2643, Batch Gradient Norm after: 13.666110459763233
Epoch 2644/10000, Prediction Accuracy = 61.08200000000001%, Loss = 0.5459972739219665
Epoch: 2644, Batch Gradient Norm: 11.286076732519613
Epoch: 2644, Batch Gradient Norm after: 11.286076732519613
Epoch 2645/10000, Prediction Accuracy = 61.257999999999996%, Loss = 0.5267052054405212
Epoch: 2645, Batch Gradient Norm: 12.79995122348562
Epoch: 2645, Batch Gradient Norm after: 12.79995122348562
Epoch 2646/10000, Prediction Accuracy = 61.2%, Loss = 0.5341924786567688
Epoch: 2646, Batch Gradient Norm: 13.544850222359356
Epoch: 2646, Batch Gradient Norm after: 13.544850222359356
Epoch 2647/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.5390385866165162
Epoch: 2647, Batch Gradient Norm: 10.128401538224564
Epoch: 2647, Batch Gradient Norm after: 10.128401538224564
Epoch 2648/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.5161309719085694
Epoch: 2648, Batch Gradient Norm: 9.832111148214373
Epoch: 2648, Batch Gradient Norm after: 9.832111148214373
Epoch 2649/10000, Prediction Accuracy = 61.222%, Loss = 0.5130081534385681
Epoch: 2649, Batch Gradient Norm: 8.962101565860948
Epoch: 2649, Batch Gradient Norm after: 8.962101565860948
Epoch 2650/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.5088913321495057
Epoch: 2650, Batch Gradient Norm: 9.87656965890373
Epoch: 2650, Batch Gradient Norm after: 9.87656965890373
Epoch 2651/10000, Prediction Accuracy = 61.122%, Loss = 0.5172325551509858
Epoch: 2651, Batch Gradient Norm: 12.025273376820946
Epoch: 2651, Batch Gradient Norm after: 12.025273376820946
Epoch 2652/10000, Prediction Accuracy = 61.146%, Loss = 0.5319692015647888
Epoch: 2652, Batch Gradient Norm: 11.152075928927651
Epoch: 2652, Batch Gradient Norm after: 11.152075928927651
Epoch 2653/10000, Prediction Accuracy = 61.242%, Loss = 0.531580114364624
Epoch: 2653, Batch Gradient Norm: 6.976536895771302
Epoch: 2653, Batch Gradient Norm after: 6.976536895771302
Epoch 2654/10000, Prediction Accuracy = 61.117999999999995%, Loss = 0.4991798400878906
Epoch: 2654, Batch Gradient Norm: 10.728160717660758
Epoch: 2654, Batch Gradient Norm after: 10.728160717660758
Epoch 2655/10000, Prediction Accuracy = 61.162%, Loss = 0.5212428331375122
Epoch: 2655, Batch Gradient Norm: 12.993315695743304
Epoch: 2655, Batch Gradient Norm after: 12.993315695743304
Epoch 2656/10000, Prediction Accuracy = 61.040000000000006%, Loss = 0.538731598854065
Epoch: 2656, Batch Gradient Norm: 10.525612956390415
Epoch: 2656, Batch Gradient Norm after: 10.525612956390415
Epoch 2657/10000, Prediction Accuracy = 61.176%, Loss = 0.5209550023078918
Epoch: 2657, Batch Gradient Norm: 7.536384468031503
Epoch: 2657, Batch Gradient Norm after: 7.536384468031503
Epoch 2658/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.5024256527423858
Epoch: 2658, Batch Gradient Norm: 9.662114442152497
Epoch: 2658, Batch Gradient Norm after: 9.662114442152497
Epoch 2659/10000, Prediction Accuracy = 61.136%, Loss = 0.5146202325820923
Epoch: 2659, Batch Gradient Norm: 11.832702162837018
Epoch: 2659, Batch Gradient Norm after: 11.832702162837018
Epoch 2660/10000, Prediction Accuracy = 61.064%, Loss = 0.5283752322196961
Epoch: 2660, Batch Gradient Norm: 14.17758533536333
Epoch: 2660, Batch Gradient Norm after: 14.17758533536333
Epoch 2661/10000, Prediction Accuracy = 61.122%, Loss = 0.5401312470436096
Epoch: 2661, Batch Gradient Norm: 10.318149435809657
Epoch: 2661, Batch Gradient Norm after: 10.318149435809657
Epoch 2662/10000, Prediction Accuracy = 61.162%, Loss = 0.5165550470352173
Epoch: 2662, Batch Gradient Norm: 9.460003039632042
Epoch: 2662, Batch Gradient Norm after: 9.460003039632042
Epoch 2663/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.5165616750717164
Epoch: 2663, Batch Gradient Norm: 8.320801451215818
Epoch: 2663, Batch Gradient Norm after: 8.320801451215818
Epoch 2664/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.507862138748169
Epoch: 2664, Batch Gradient Norm: 9.411066027553034
Epoch: 2664, Batch Gradient Norm after: 9.411066027553034
Epoch 2665/10000, Prediction Accuracy = 61.14%, Loss = 0.5106799364089966
Epoch: 2665, Batch Gradient Norm: 9.793826136033106
Epoch: 2665, Batch Gradient Norm after: 9.793826136033106
Epoch 2666/10000, Prediction Accuracy = 61.2%, Loss = 0.5119548439979553
Epoch: 2666, Batch Gradient Norm: 12.232161113116721
Epoch: 2666, Batch Gradient Norm after: 12.232161113116721
Epoch 2667/10000, Prediction Accuracy = 61.202%, Loss = 0.5329449057579041
Epoch: 2667, Batch Gradient Norm: 10.328910602600567
Epoch: 2667, Batch Gradient Norm after: 10.328910602600567
Epoch 2668/10000, Prediction Accuracy = 61.217999999999996%, Loss = 0.518506121635437
Epoch: 2668, Batch Gradient Norm: 11.134092480425894
Epoch: 2668, Batch Gradient Norm after: 11.134092480425894
Epoch 2669/10000, Prediction Accuracy = 61.14%, Loss = 0.5240618228912354
Epoch: 2669, Batch Gradient Norm: 12.089515501851889
Epoch: 2669, Batch Gradient Norm after: 12.089515501851889
Epoch 2670/10000, Prediction Accuracy = 61.274%, Loss = 0.526808500289917
Epoch: 2670, Batch Gradient Norm: 11.8974152751518
Epoch: 2670, Batch Gradient Norm after: 11.8974152751518
Epoch 2671/10000, Prediction Accuracy = 61.193999999999996%, Loss = 0.5287288069725037
Epoch: 2671, Batch Gradient Norm: 9.498411068576623
Epoch: 2671, Batch Gradient Norm after: 9.498411068576623
Epoch 2672/10000, Prediction Accuracy = 61.14%, Loss = 0.5144964814186096
Epoch: 2672, Batch Gradient Norm: 8.04770439876968
Epoch: 2672, Batch Gradient Norm after: 8.04770439876968
Epoch 2673/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.5044432282447815
Epoch: 2673, Batch Gradient Norm: 10.646722869947581
Epoch: 2673, Batch Gradient Norm after: 10.646722869947581
Epoch 2674/10000, Prediction Accuracy = 61.23600000000001%, Loss = 0.5172521829605102
Epoch: 2674, Batch Gradient Norm: 13.016927786248935
Epoch: 2674, Batch Gradient Norm after: 13.016927786248935
Epoch 2675/10000, Prediction Accuracy = 61.162%, Loss = 0.5360223412513733
Epoch: 2675, Batch Gradient Norm: 8.987118356348434
Epoch: 2675, Batch Gradient Norm after: 8.987118356348434
Epoch 2676/10000, Prediction Accuracy = 61.226%, Loss = 0.5090970635414124
Epoch: 2676, Batch Gradient Norm: 11.5203915705521
Epoch: 2676, Batch Gradient Norm after: 11.5203915705521
Epoch 2677/10000, Prediction Accuracy = 61.152%, Loss = 0.5284032821655273
Epoch: 2677, Batch Gradient Norm: 12.654881879243254
Epoch: 2677, Batch Gradient Norm after: 12.654881879243254
Epoch 2678/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5385538339614868
Epoch: 2678, Batch Gradient Norm: 7.110163007830744
Epoch: 2678, Batch Gradient Norm after: 7.110163007830744
Epoch 2679/10000, Prediction Accuracy = 61.134%, Loss = 0.5012839138507843
Epoch: 2679, Batch Gradient Norm: 7.228822469372633
Epoch: 2679, Batch Gradient Norm after: 7.228822469372633
Epoch 2680/10000, Prediction Accuracy = 61.193999999999996%, Loss = 0.49894940853118896
Epoch: 2680, Batch Gradient Norm: 8.67125168326291
Epoch: 2680, Batch Gradient Norm after: 8.67125168326291
Epoch 2681/10000, Prediction Accuracy = 61.27%, Loss = 0.5069197952747345
Epoch: 2681, Batch Gradient Norm: 15.380768385455543
Epoch: 2681, Batch Gradient Norm after: 15.380768385455543
Epoch 2682/10000, Prediction Accuracy = 61.1%, Loss = 0.5627485036849975
Epoch: 2682, Batch Gradient Norm: 13.520053210039745
Epoch: 2682, Batch Gradient Norm after: 13.520053210039745
Epoch 2683/10000, Prediction Accuracy = 61.134%, Loss = 0.5407207846641541
Epoch: 2683, Batch Gradient Norm: 9.091180708887588
Epoch: 2683, Batch Gradient Norm after: 9.091180708887588
Epoch 2684/10000, Prediction Accuracy = 61.15%, Loss = 0.5086031317710876
Epoch: 2684, Batch Gradient Norm: 13.116439617917125
Epoch: 2684, Batch Gradient Norm after: 13.116439617917125
Epoch 2685/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.5360414028167725
Epoch: 2685, Batch Gradient Norm: 12.045786345435687
Epoch: 2685, Batch Gradient Norm after: 12.045786345435687
Epoch 2686/10000, Prediction Accuracy = 61.032000000000004%, Loss = 0.5296159744262695
Epoch: 2686, Batch Gradient Norm: 12.185294032042425
Epoch: 2686, Batch Gradient Norm after: 12.185294032042425
Epoch 2687/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.5400792121887207
Epoch: 2687, Batch Gradient Norm: 8.676576977660094
Epoch: 2687, Batch Gradient Norm after: 8.676576977660094
Epoch 2688/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5053337514400482
Epoch: 2688, Batch Gradient Norm: 8.571966687102321
Epoch: 2688, Batch Gradient Norm after: 8.571966687102321
Epoch 2689/10000, Prediction Accuracy = 61.128%, Loss = 0.504516875743866
Epoch: 2689, Batch Gradient Norm: 7.4651856826138
Epoch: 2689, Batch Gradient Norm after: 7.4651856826138
Epoch 2690/10000, Prediction Accuracy = 61.148%, Loss = 0.5009526193141938
Epoch: 2690, Batch Gradient Norm: 8.66760510107699
Epoch: 2690, Batch Gradient Norm after: 8.66760510107699
Epoch 2691/10000, Prediction Accuracy = 61.302%, Loss = 0.5064946234226226
Epoch: 2691, Batch Gradient Norm: 14.876162791569579
Epoch: 2691, Batch Gradient Norm after: 14.876162791569579
Epoch 2692/10000, Prediction Accuracy = 61.226%, Loss = 0.5540817141532898
Epoch: 2692, Batch Gradient Norm: 9.716011166110965
Epoch: 2692, Batch Gradient Norm after: 9.716011166110965
Epoch 2693/10000, Prediction Accuracy = 61.174%, Loss = 0.5135341823101044
Epoch: 2693, Batch Gradient Norm: 9.916011851388289
Epoch: 2693, Batch Gradient Norm after: 9.916011851388289
Epoch 2694/10000, Prediction Accuracy = 61.19200000000001%, Loss = 0.5106332659721374
Epoch: 2694, Batch Gradient Norm: 9.76872054395773
Epoch: 2694, Batch Gradient Norm after: 9.76872054395773
Epoch 2695/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.5100887417793274
Epoch: 2695, Batch Gradient Norm: 10.3875564692696
Epoch: 2695, Batch Gradient Norm after: 10.3875564692696
Epoch 2696/10000, Prediction Accuracy = 61.226%, Loss = 0.5134055495262146
Epoch: 2696, Batch Gradient Norm: 13.644551264294844
Epoch: 2696, Batch Gradient Norm after: 13.644551264294844
Epoch 2697/10000, Prediction Accuracy = 61.214%, Loss = 0.5394001245498657
Epoch: 2697, Batch Gradient Norm: 10.64161606523113
Epoch: 2697, Batch Gradient Norm after: 10.64161606523113
Epoch 2698/10000, Prediction Accuracy = 61.23%, Loss = 0.5186671018600464
Epoch: 2698, Batch Gradient Norm: 10.70415348735239
Epoch: 2698, Batch Gradient Norm after: 10.70415348735239
Epoch 2699/10000, Prediction Accuracy = 61.324%, Loss = 0.5199331104755401
Epoch: 2699, Batch Gradient Norm: 8.543872242128472
Epoch: 2699, Batch Gradient Norm after: 8.543872242128472
Epoch 2700/10000, Prediction Accuracy = 61.258%, Loss = 0.5048891842365265
Epoch: 2700, Batch Gradient Norm: 10.494832043096077
Epoch: 2700, Batch Gradient Norm after: 10.494832043096077
Epoch 2701/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.5186557173728943
Epoch: 2701, Batch Gradient Norm: 10.547007249331992
Epoch: 2701, Batch Gradient Norm after: 10.547007249331992
Epoch 2702/10000, Prediction Accuracy = 61.162%, Loss = 0.5202461838722229
Epoch: 2702, Batch Gradient Norm: 12.838602028100976
Epoch: 2702, Batch Gradient Norm after: 12.838602028100976
Epoch 2703/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.5356825828552246
Epoch: 2703, Batch Gradient Norm: 9.809905551430983
Epoch: 2703, Batch Gradient Norm after: 9.809905551430983
Epoch 2704/10000, Prediction Accuracy = 61.246%, Loss = 0.5098913967609405
Epoch: 2704, Batch Gradient Norm: 9.029635108826843
Epoch: 2704, Batch Gradient Norm after: 9.029635108826843
Epoch 2705/10000, Prediction Accuracy = 61.25%, Loss = 0.5091763973236084
Epoch: 2705, Batch Gradient Norm: 10.287091155599885
Epoch: 2705, Batch Gradient Norm after: 10.287091155599885
Epoch 2706/10000, Prediction Accuracy = 61.178%, Loss = 0.5120944738388061
Epoch: 2706, Batch Gradient Norm: 11.363503955293925
Epoch: 2706, Batch Gradient Norm after: 11.363503955293925
Epoch 2707/10000, Prediction Accuracy = 61.198%, Loss = 0.5207071304321289
Epoch: 2707, Batch Gradient Norm: 12.063892525320835
Epoch: 2707, Batch Gradient Norm after: 12.063892525320835
Epoch 2708/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.5343411922454834
Epoch: 2708, Batch Gradient Norm: 8.46167627279175
Epoch: 2708, Batch Gradient Norm after: 8.46167627279175
Epoch 2709/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.5038387417793274
Epoch: 2709, Batch Gradient Norm: 13.086687990251418
Epoch: 2709, Batch Gradient Norm after: 13.086687990251418
Epoch 2710/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.5350160241127014
Epoch: 2710, Batch Gradient Norm: 11.558653208909195
Epoch: 2710, Batch Gradient Norm after: 11.558653208909195
Epoch 2711/10000, Prediction Accuracy = 61.088%, Loss = 0.5235419511795044
Epoch: 2711, Batch Gradient Norm: 11.156753794773
Epoch: 2711, Batch Gradient Norm after: 11.156753794773
Epoch 2712/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.52371746301651
Epoch: 2712, Batch Gradient Norm: 10.490014130519032
Epoch: 2712, Batch Gradient Norm after: 10.490014130519032
Epoch 2713/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5190101027488708
Epoch: 2713, Batch Gradient Norm: 9.191812510619359
Epoch: 2713, Batch Gradient Norm after: 9.191812510619359
Epoch 2714/10000, Prediction Accuracy = 61.146%, Loss = 0.5107057929039002
Epoch: 2714, Batch Gradient Norm: 9.279514050060495
Epoch: 2714, Batch Gradient Norm after: 9.279514050060495
Epoch 2715/10000, Prediction Accuracy = 61.262%, Loss = 0.5093983888626099
Epoch: 2715, Batch Gradient Norm: 10.131565472775751
Epoch: 2715, Batch Gradient Norm after: 10.131565472775751
Epoch 2716/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.514246666431427
Epoch: 2716, Batch Gradient Norm: 10.911626056015015
Epoch: 2716, Batch Gradient Norm after: 10.911626056015015
Epoch 2717/10000, Prediction Accuracy = 61.074%, Loss = 0.5201352357864379
Epoch: 2717, Batch Gradient Norm: 8.466405235946235
Epoch: 2717, Batch Gradient Norm after: 8.466405235946235
Epoch 2718/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.5033271431922912
Epoch: 2718, Batch Gradient Norm: 8.86274542319175
Epoch: 2718, Batch Gradient Norm after: 8.86274542319175
Epoch 2719/10000, Prediction Accuracy = 61.222%, Loss = 0.5074585199356079
Epoch: 2719, Batch Gradient Norm: 10.848466294711248
Epoch: 2719, Batch Gradient Norm after: 10.848466294711248
Epoch 2720/10000, Prediction Accuracy = 61.19%, Loss = 0.5241715848445893
Epoch: 2720, Batch Gradient Norm: 11.362053281543146
Epoch: 2720, Batch Gradient Norm after: 11.362053281543146
Epoch 2721/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.5233530879020691
Epoch: 2721, Batch Gradient Norm: 11.19498653451533
Epoch: 2721, Batch Gradient Norm after: 11.19498653451533
Epoch 2722/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.519825029373169
Epoch: 2722, Batch Gradient Norm: 10.60490286285856
Epoch: 2722, Batch Gradient Norm after: 10.60490286285856
Epoch 2723/10000, Prediction Accuracy = 61.036%, Loss = 0.5137663841247558
Epoch: 2723, Batch Gradient Norm: 12.73288217047163
Epoch: 2723, Batch Gradient Norm after: 12.73288217047163
Epoch 2724/10000, Prediction Accuracy = 61.188%, Loss = 0.536231541633606
Epoch: 2724, Batch Gradient Norm: 12.740505508481998
Epoch: 2724, Batch Gradient Norm after: 12.740505508481998
Epoch 2725/10000, Prediction Accuracy = 61.352%, Loss = 0.5346673727035522
Epoch: 2725, Batch Gradient Norm: 10.559558702765157
Epoch: 2725, Batch Gradient Norm after: 10.559558702765157
Epoch 2726/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.5162632703781128
Epoch: 2726, Batch Gradient Norm: 11.066104979747816
Epoch: 2726, Batch Gradient Norm after: 11.066104979747816
Epoch 2727/10000, Prediction Accuracy = 61.168000000000006%, Loss = 0.5220391035079956
Epoch: 2727, Batch Gradient Norm: 11.250354969842867
Epoch: 2727, Batch Gradient Norm after: 11.250354969842867
Epoch 2728/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.5223634719848633
Epoch: 2728, Batch Gradient Norm: 8.020871646283574
Epoch: 2728, Batch Gradient Norm after: 8.020871646283574
Epoch 2729/10000, Prediction Accuracy = 61.23%, Loss = 0.5004069387912751
Epoch: 2729, Batch Gradient Norm: 8.900471824012103
Epoch: 2729, Batch Gradient Norm after: 8.900471824012103
Epoch 2730/10000, Prediction Accuracy = 61.18800000000001%, Loss = 0.5060363054275513
Epoch: 2730, Batch Gradient Norm: 13.816326391908085
Epoch: 2730, Batch Gradient Norm after: 13.816326391908085
Epoch 2731/10000, Prediction Accuracy = 61.21999999999999%, Loss = 0.5393696546554565
Epoch: 2731, Batch Gradient Norm: 11.929919051605266
Epoch: 2731, Batch Gradient Norm after: 11.929919051605266
Epoch 2732/10000, Prediction Accuracy = 61.062%, Loss = 0.5254944086074829
Epoch: 2732, Batch Gradient Norm: 8.79655393548337
Epoch: 2732, Batch Gradient Norm after: 8.79655393548337
Epoch 2733/10000, Prediction Accuracy = 61.238%, Loss = 0.5060961365699768
Epoch: 2733, Batch Gradient Norm: 7.615578768743087
Epoch: 2733, Batch Gradient Norm after: 7.615578768743087
Epoch 2734/10000, Prediction Accuracy = 61.217999999999996%, Loss = 0.501276683807373
Epoch: 2734, Batch Gradient Norm: 9.530070797447301
Epoch: 2734, Batch Gradient Norm after: 9.530070797447301
Epoch 2735/10000, Prediction Accuracy = 61.198%, Loss = 0.508643651008606
Epoch: 2735, Batch Gradient Norm: 12.350684234308236
Epoch: 2735, Batch Gradient Norm after: 12.350684234308236
Epoch 2736/10000, Prediction Accuracy = 61.146%, Loss = 0.5263540625572205
Epoch: 2736, Batch Gradient Norm: 13.217768679269486
Epoch: 2736, Batch Gradient Norm after: 13.217768679269486
Epoch 2737/10000, Prediction Accuracy = 61.238%, Loss = 0.5364428997039795
Epoch: 2737, Batch Gradient Norm: 11.184284685625663
Epoch: 2737, Batch Gradient Norm after: 11.184284685625663
Epoch 2738/10000, Prediction Accuracy = 61.188%, Loss = 0.5199151158332824
Epoch: 2738, Batch Gradient Norm: 9.594293013487269
Epoch: 2738, Batch Gradient Norm after: 9.594293013487269
Epoch 2739/10000, Prediction Accuracy = 61.174%, Loss = 0.5103446304798126
Epoch: 2739, Batch Gradient Norm: 10.815183489265435
Epoch: 2739, Batch Gradient Norm after: 10.815183489265435
Epoch 2740/10000, Prediction Accuracy = 61.3%, Loss = 0.5175772070884704
Epoch: 2740, Batch Gradient Norm: 11.72393281668185
Epoch: 2740, Batch Gradient Norm after: 11.72393281668185
Epoch 2741/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5261289596557617
Epoch: 2741, Batch Gradient Norm: 11.287337126954121
Epoch: 2741, Batch Gradient Norm after: 11.287337126954121
Epoch 2742/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.5261391162872314
Epoch: 2742, Batch Gradient Norm: 7.389627910341415
Epoch: 2742, Batch Gradient Norm after: 7.389627910341415
Epoch 2743/10000, Prediction Accuracy = 61.164%, Loss = 0.49751226902008056
Epoch: 2743, Batch Gradient Norm: 8.312392474829487
Epoch: 2743, Batch Gradient Norm after: 8.312392474829487
Epoch 2744/10000, Prediction Accuracy = 61.19%, Loss = 0.504456627368927
Epoch: 2744, Batch Gradient Norm: 9.771770721453935
Epoch: 2744, Batch Gradient Norm after: 9.771770721453935
Epoch 2745/10000, Prediction Accuracy = 61.202%, Loss = 0.5103848099708557
Epoch: 2745, Batch Gradient Norm: 12.403137898884633
Epoch: 2745, Batch Gradient Norm after: 12.403137898884633
Epoch 2746/10000, Prediction Accuracy = 61.246%, Loss = 0.5276314079761505
Epoch: 2746, Batch Gradient Norm: 11.299009413363205
Epoch: 2746, Batch Gradient Norm after: 11.299009413363205
Epoch 2747/10000, Prediction Accuracy = 61.298%, Loss = 0.5189533472061157
Epoch: 2747, Batch Gradient Norm: 9.513364200147086
Epoch: 2747, Batch Gradient Norm after: 9.513364200147086
Epoch 2748/10000, Prediction Accuracy = 61.242%, Loss = 0.5072562336921692
Epoch: 2748, Batch Gradient Norm: 9.736003985424823
Epoch: 2748, Batch Gradient Norm after: 9.736003985424823
Epoch 2749/10000, Prediction Accuracy = 61.36%, Loss = 0.5092628359794616
Epoch: 2749, Batch Gradient Norm: 12.175412922254958
Epoch: 2749, Batch Gradient Norm after: 12.175412922254958
Epoch 2750/10000, Prediction Accuracy = 61.21%, Loss = 0.5217514038085938
Epoch: 2750, Batch Gradient Norm: 11.651610162601871
Epoch: 2750, Batch Gradient Norm after: 11.651610162601871
Epoch 2751/10000, Prediction Accuracy = 61.364%, Loss = 0.5214388370513916
Epoch: 2751, Batch Gradient Norm: 11.517845990404938
Epoch: 2751, Batch Gradient Norm after: 11.517845990404938
Epoch 2752/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.527271568775177
Epoch: 2752, Batch Gradient Norm: 7.827230666039207
Epoch: 2752, Batch Gradient Norm after: 7.827230666039207
Epoch 2753/10000, Prediction Accuracy = 61.348%, Loss = 0.4993121325969696
Epoch: 2753, Batch Gradient Norm: 9.715716793267287
Epoch: 2753, Batch Gradient Norm after: 9.715716793267287
Epoch 2754/10000, Prediction Accuracy = 61.2%, Loss = 0.5108961462974548
Epoch: 2754, Batch Gradient Norm: 13.127902783361863
Epoch: 2754, Batch Gradient Norm after: 13.127902783361863
Epoch 2755/10000, Prediction Accuracy = 61.226%, Loss = 0.5381296515464783
Epoch: 2755, Batch Gradient Norm: 10.738960457478484
Epoch: 2755, Batch Gradient Norm after: 10.738960457478484
Epoch 2756/10000, Prediction Accuracy = 61.19599999999999%, Loss = 0.5170843839645386
Epoch: 2756, Batch Gradient Norm: 8.674310576856675
Epoch: 2756, Batch Gradient Norm after: 8.674310576856675
Epoch 2757/10000, Prediction Accuracy = 61.254%, Loss = 0.503926831483841
Epoch: 2757, Batch Gradient Norm: 11.075285372065972
Epoch: 2757, Batch Gradient Norm after: 11.075285372065972
Epoch 2758/10000, Prediction Accuracy = 61.136%, Loss = 0.5181739926338196
Epoch: 2758, Batch Gradient Norm: 12.030826361644568
Epoch: 2758, Batch Gradient Norm after: 12.030826361644568
Epoch 2759/10000, Prediction Accuracy = 61.15%, Loss = 0.5274115443229676
Epoch: 2759, Batch Gradient Norm: 9.390834258042975
Epoch: 2759, Batch Gradient Norm after: 9.390834258042975
Epoch 2760/10000, Prediction Accuracy = 61.242%, Loss = 0.5096381068229675
Epoch: 2760, Batch Gradient Norm: 9.169070999344004
Epoch: 2760, Batch Gradient Norm after: 9.169070999344004
Epoch 2761/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.5068145573139191
Epoch: 2761, Batch Gradient Norm: 8.977245557978959
Epoch: 2761, Batch Gradient Norm after: 8.977245557978959
Epoch 2762/10000, Prediction Accuracy = 61.21%, Loss = 0.5057254791259765
Epoch: 2762, Batch Gradient Norm: 11.451875029254188
Epoch: 2762, Batch Gradient Norm after: 11.451875029254188
Epoch 2763/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5220471620559692
Epoch: 2763, Batch Gradient Norm: 11.54817418149617
Epoch: 2763, Batch Gradient Norm after: 11.54817418149617
Epoch 2764/10000, Prediction Accuracy = 61.33%, Loss = 0.5209903836250305
Epoch: 2764, Batch Gradient Norm: 9.78653271827725
Epoch: 2764, Batch Gradient Norm after: 9.78653271827725
Epoch 2765/10000, Prediction Accuracy = 61.21600000000001%, Loss = 0.5115418314933777
Epoch: 2765, Batch Gradient Norm: 9.262219702559005
Epoch: 2765, Batch Gradient Norm after: 9.262219702559005
Epoch 2766/10000, Prediction Accuracy = 61.164%, Loss = 0.5062926054000855
Epoch: 2766, Batch Gradient Norm: 9.76836170101949
Epoch: 2766, Batch Gradient Norm after: 9.76836170101949
Epoch 2767/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.5108779847621918
Epoch: 2767, Batch Gradient Norm: 12.240500178609018
Epoch: 2767, Batch Gradient Norm after: 12.240500178609018
Epoch 2768/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.5331510663032532
Epoch: 2768, Batch Gradient Norm: 14.338418873698275
Epoch: 2768, Batch Gradient Norm after: 14.338418873698275
Epoch 2769/10000, Prediction Accuracy = 61.3%, Loss = 0.5432703495025635
Epoch: 2769, Batch Gradient Norm: 8.841716778594497
Epoch: 2769, Batch Gradient Norm after: 8.841716778594497
Epoch 2770/10000, Prediction Accuracy = 61.322%, Loss = 0.5036231994628906
Epoch: 2770, Batch Gradient Norm: 9.349394463197763
Epoch: 2770, Batch Gradient Norm after: 9.349394463197763
Epoch 2771/10000, Prediction Accuracy = 61.116%, Loss = 0.5069192111492157
Epoch: 2771, Batch Gradient Norm: 7.356976065880433
Epoch: 2771, Batch Gradient Norm after: 7.356976065880433
Epoch 2772/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.4940899670124054
Epoch: 2772, Batch Gradient Norm: 10.376739801183316
Epoch: 2772, Batch Gradient Norm after: 10.376739801183316
Epoch 2773/10000, Prediction Accuracy = 61.218%, Loss = 0.510032570362091
Epoch: 2773, Batch Gradient Norm: 14.824433892468116
Epoch: 2773, Batch Gradient Norm after: 14.824433892468116
Epoch 2774/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.549826443195343
Epoch: 2774, Batch Gradient Norm: 14.545459338618057
Epoch: 2774, Batch Gradient Norm after: 14.545459338618057
Epoch 2775/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5448573350906372
Epoch: 2775, Batch Gradient Norm: 9.533426224392962
Epoch: 2775, Batch Gradient Norm after: 9.533426224392962
Epoch 2776/10000, Prediction Accuracy = 61.278%, Loss = 0.5078880310058593
Epoch: 2776, Batch Gradient Norm: 6.868224181042154
Epoch: 2776, Batch Gradient Norm after: 6.868224181042154
Epoch 2777/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.49394835233688356
Epoch: 2777, Batch Gradient Norm: 8.607679449197615
Epoch: 2777, Batch Gradient Norm after: 8.607679449197615
Epoch 2778/10000, Prediction Accuracy = 61.30800000000001%, Loss = 0.5023766338825226
Epoch: 2778, Batch Gradient Norm: 11.647742345625007
Epoch: 2778, Batch Gradient Norm after: 11.647742345625007
Epoch 2779/10000, Prediction Accuracy = 61.162%, Loss = 0.5233325242996216
Epoch: 2779, Batch Gradient Norm: 9.622078203426916
Epoch: 2779, Batch Gradient Norm after: 9.622078203426916
Epoch 2780/10000, Prediction Accuracy = 61.19000000000001%, Loss = 0.5073030412197113
Epoch: 2780, Batch Gradient Norm: 10.16513963432608
Epoch: 2780, Batch Gradient Norm after: 10.16513963432608
Epoch 2781/10000, Prediction Accuracy = 61.132000000000005%, Loss = 0.5116328001022339
Epoch: 2781, Batch Gradient Norm: 12.54788879100497
Epoch: 2781, Batch Gradient Norm after: 12.54788879100497
Epoch 2782/10000, Prediction Accuracy = 61.218%, Loss = 0.5343477487564087
Epoch: 2782, Batch Gradient Norm: 11.004158812587423
Epoch: 2782, Batch Gradient Norm after: 11.004158812587423
Epoch 2783/10000, Prediction Accuracy = 61.214%, Loss = 0.5180319905281067
Epoch: 2783, Batch Gradient Norm: 11.919426055302429
Epoch: 2783, Batch Gradient Norm after: 11.919426055302429
Epoch 2784/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.5201241850852967
Epoch: 2784, Batch Gradient Norm: 12.383652708646055
Epoch: 2784, Batch Gradient Norm after: 12.383652708646055
Epoch 2785/10000, Prediction Accuracy = 61.29200000000001%, Loss = 0.5346167802810669
Epoch: 2785, Batch Gradient Norm: 9.664264956321354
Epoch: 2785, Batch Gradient Norm after: 9.664264956321354
Epoch 2786/10000, Prediction Accuracy = 61.269999999999996%, Loss = 0.5125882148742675
Epoch: 2786, Batch Gradient Norm: 8.368784571640052
Epoch: 2786, Batch Gradient Norm after: 8.368784571640052
Epoch 2787/10000, Prediction Accuracy = 61.314%, Loss = 0.5026948750019073
Epoch: 2787, Batch Gradient Norm: 12.463799016860323
Epoch: 2787, Batch Gradient Norm after: 12.463799016860323
Epoch 2788/10000, Prediction Accuracy = 61.326%, Loss = 0.5255991220474243
Epoch: 2788, Batch Gradient Norm: 9.575369384310754
Epoch: 2788, Batch Gradient Norm after: 9.575369384310754
Epoch 2789/10000, Prediction Accuracy = 61.30999999999999%, Loss = 0.5080207467079163
Epoch: 2789, Batch Gradient Norm: 10.264760246481188
Epoch: 2789, Batch Gradient Norm after: 10.264760246481188
Epoch 2790/10000, Prediction Accuracy = 61.3%, Loss = 0.5107342004776001
Epoch: 2790, Batch Gradient Norm: 10.552892561084278
Epoch: 2790, Batch Gradient Norm after: 10.552892561084278
Epoch 2791/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5146055102348328
Epoch: 2791, Batch Gradient Norm: 7.596659260993717
Epoch: 2791, Batch Gradient Norm after: 7.596659260993717
Epoch 2792/10000, Prediction Accuracy = 61.226%, Loss = 0.49774840474128723
Epoch: 2792, Batch Gradient Norm: 10.49793850732591
Epoch: 2792, Batch Gradient Norm after: 10.49793850732591
Epoch 2793/10000, Prediction Accuracy = 61.212%, Loss = 0.5135059475898742
Epoch: 2793, Batch Gradient Norm: 12.619744733527373
Epoch: 2793, Batch Gradient Norm after: 12.619744733527373
Epoch 2794/10000, Prediction Accuracy = 61.426%, Loss = 0.5286208868026734
Epoch: 2794, Batch Gradient Norm: 9.99789970702454
Epoch: 2794, Batch Gradient Norm after: 9.99789970702454
Epoch 2795/10000, Prediction Accuracy = 61.25%, Loss = 0.5103135347366333
Epoch: 2795, Batch Gradient Norm: 9.250662825981445
Epoch: 2795, Batch Gradient Norm after: 9.250662825981445
Epoch 2796/10000, Prediction Accuracy = 61.27199999999999%, Loss = 0.5034601509571075
Epoch: 2796, Batch Gradient Norm: 11.44603016207452
Epoch: 2796, Batch Gradient Norm after: 11.44603016207452
Epoch 2797/10000, Prediction Accuracy = 61.198%, Loss = 0.5165462255477905
Epoch: 2797, Batch Gradient Norm: 12.659730980457596
Epoch: 2797, Batch Gradient Norm after: 12.659730980457596
Epoch 2798/10000, Prediction Accuracy = 61.196000000000005%, Loss = 0.5302612781524658
Epoch: 2798, Batch Gradient Norm: 9.842913977671211
Epoch: 2798, Batch Gradient Norm after: 9.842913977671211
Epoch 2799/10000, Prediction Accuracy = 61.2%, Loss = 0.5070960283279419
Epoch: 2799, Batch Gradient Norm: 10.063348982729355
Epoch: 2799, Batch Gradient Norm after: 10.063348982729355
Epoch 2800/10000, Prediction Accuracy = 61.222%, Loss = 0.5075210213661194
Epoch: 2800, Batch Gradient Norm: 13.853792192567136
Epoch: 2800, Batch Gradient Norm after: 13.853792192567136
Epoch 2801/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.534787404537201
Epoch: 2801, Batch Gradient Norm: 11.625921735616197
Epoch: 2801, Batch Gradient Norm after: 11.625921735616197
Epoch 2802/10000, Prediction Accuracy = 61.302%, Loss = 0.5248733043670655
Epoch: 2802, Batch Gradient Norm: 7.813293638175746
Epoch: 2802, Batch Gradient Norm after: 7.813293638175746
Epoch 2803/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.4977558672428131
Epoch: 2803, Batch Gradient Norm: 11.310190759762497
Epoch: 2803, Batch Gradient Norm after: 11.310190759762497
Epoch 2804/10000, Prediction Accuracy = 61.25600000000001%, Loss = 0.5177839756011963
Epoch: 2804, Batch Gradient Norm: 11.506121660893795
Epoch: 2804, Batch Gradient Norm after: 11.506121660893795
Epoch 2805/10000, Prediction Accuracy = 61.196000000000005%, Loss = 0.5195994079113007
Epoch: 2805, Batch Gradient Norm: 12.004725691527112
Epoch: 2805, Batch Gradient Norm after: 12.004725691527112
Epoch 2806/10000, Prediction Accuracy = 61.306%, Loss = 0.5268264651298523
Epoch: 2806, Batch Gradient Norm: 10.787090632245343
Epoch: 2806, Batch Gradient Norm after: 10.787090632245343
Epoch 2807/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.5183966279029846
Epoch: 2807, Batch Gradient Norm: 10.493271061869176
Epoch: 2807, Batch Gradient Norm after: 10.493271061869176
Epoch 2808/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.5137507021427155
Epoch: 2808, Batch Gradient Norm: 10.18319216574139
Epoch: 2808, Batch Gradient Norm after: 10.18319216574139
Epoch 2809/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5126481890678406
Epoch: 2809, Batch Gradient Norm: 11.05571168979576
Epoch: 2809, Batch Gradient Norm after: 11.05571168979576
Epoch 2810/10000, Prediction Accuracy = 61.334%, Loss = 0.5200366973876953
Epoch: 2810, Batch Gradient Norm: 11.109618987103737
Epoch: 2810, Batch Gradient Norm after: 11.109618987103737
Epoch 2811/10000, Prediction Accuracy = 61.186%, Loss = 0.5178547143936157
Epoch: 2811, Batch Gradient Norm: 9.345454475661782
Epoch: 2811, Batch Gradient Norm after: 9.345454475661782
Epoch 2812/10000, Prediction Accuracy = 61.3%, Loss = 0.5031926274299622
Epoch: 2812, Batch Gradient Norm: 9.077777757568304
Epoch: 2812, Batch Gradient Norm after: 9.077777757568304
Epoch 2813/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.5026324391365051
Epoch: 2813, Batch Gradient Norm: 10.437286637795596
Epoch: 2813, Batch Gradient Norm after: 10.437286637795596
Epoch 2814/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.5121468305587769
Epoch: 2814, Batch Gradient Norm: 10.145767974535662
Epoch: 2814, Batch Gradient Norm after: 10.145767974535662
Epoch 2815/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5089419484138489
Epoch: 2815, Batch Gradient Norm: 12.050167691516211
Epoch: 2815, Batch Gradient Norm after: 12.050167691516211
Epoch 2816/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.5241704821586609
Epoch: 2816, Batch Gradient Norm: 11.286551544381359
Epoch: 2816, Batch Gradient Norm after: 11.286551544381359
Epoch 2817/10000, Prediction Accuracy = 61.267999999999994%, Loss = 0.51815265417099
Epoch: 2817, Batch Gradient Norm: 7.434370168655649
Epoch: 2817, Batch Gradient Norm after: 7.434370168655649
Epoch 2818/10000, Prediction Accuracy = 61.226%, Loss = 0.49686444401741026
Epoch: 2818, Batch Gradient Norm: 9.538018987269346
Epoch: 2818, Batch Gradient Norm after: 9.538018987269346
Epoch 2819/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.505837905406952
Epoch: 2819, Batch Gradient Norm: 11.47880193249327
Epoch: 2819, Batch Gradient Norm after: 11.47880193249327
Epoch 2820/10000, Prediction Accuracy = 61.38199999999999%, Loss = 0.5246447205543519
Epoch: 2820, Batch Gradient Norm: 9.111079525917843
Epoch: 2820, Batch Gradient Norm after: 9.111079525917843
Epoch 2821/10000, Prediction Accuracy = 61.23%, Loss = 0.5035394251346588
Epoch: 2821, Batch Gradient Norm: 9.45893454715408
Epoch: 2821, Batch Gradient Norm after: 9.45893454715408
Epoch 2822/10000, Prediction Accuracy = 61.262%, Loss = 0.5076118111610413
Epoch: 2822, Batch Gradient Norm: 10.409734611998326
Epoch: 2822, Batch Gradient Norm after: 10.409734611998326
Epoch 2823/10000, Prediction Accuracy = 61.112%, Loss = 0.5137175261974335
Epoch: 2823, Batch Gradient Norm: 10.947452006013881
Epoch: 2823, Batch Gradient Norm after: 10.947452006013881
Epoch 2824/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5151578783988953
Epoch: 2824, Batch Gradient Norm: 11.992651064719453
Epoch: 2824, Batch Gradient Norm after: 11.992651064719453
Epoch 2825/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5213550925254822
Epoch: 2825, Batch Gradient Norm: 12.412329254066377
Epoch: 2825, Batch Gradient Norm after: 12.412329254066377
Epoch 2826/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.5272545099258423
Epoch: 2826, Batch Gradient Norm: 10.455048598563227
Epoch: 2826, Batch Gradient Norm after: 10.455048598563227
Epoch 2827/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.5169479668140411
Epoch: 2827, Batch Gradient Norm: 10.580424146695904
Epoch: 2827, Batch Gradient Norm after: 10.580424146695904
Epoch 2828/10000, Prediction Accuracy = 61.19599999999999%, Loss = 0.511756283044815
Epoch: 2828, Batch Gradient Norm: 11.430521344571954
Epoch: 2828, Batch Gradient Norm after: 11.430521344571954
Epoch 2829/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.5204314231872559
Epoch: 2829, Batch Gradient Norm: 11.656134481731165
Epoch: 2829, Batch Gradient Norm after: 11.656134481731165
Epoch 2830/10000, Prediction Accuracy = 61.298%, Loss = 0.5173635721206665
Epoch: 2830, Batch Gradient Norm: 11.479169366931991
Epoch: 2830, Batch Gradient Norm after: 11.479169366931991
Epoch 2831/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.5175997972488403
Epoch: 2831, Batch Gradient Norm: 9.315679917868037
Epoch: 2831, Batch Gradient Norm after: 9.315679917868037
Epoch 2832/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5061983585357666
Epoch: 2832, Batch Gradient Norm: 8.008342971525874
Epoch: 2832, Batch Gradient Norm after: 8.008342971525874
Epoch 2833/10000, Prediction Accuracy = 61.217999999999996%, Loss = 0.5007934212684632
Epoch: 2833, Batch Gradient Norm: 8.887871248829113
Epoch: 2833, Batch Gradient Norm after: 8.887871248829113
Epoch 2834/10000, Prediction Accuracy = 61.282%, Loss = 0.5018490135669709
Epoch: 2834, Batch Gradient Norm: 10.389644510566319
Epoch: 2834, Batch Gradient Norm after: 10.389644510566319
Epoch 2835/10000, Prediction Accuracy = 61.378%, Loss = 0.5123477220535279
Epoch: 2835, Batch Gradient Norm: 11.23785214781253
Epoch: 2835, Batch Gradient Norm after: 11.23785214781253
Epoch 2836/10000, Prediction Accuracy = 61.29200000000001%, Loss = 0.5157140135765076
Epoch: 2836, Batch Gradient Norm: 11.736132124230505
Epoch: 2836, Batch Gradient Norm after: 11.736132124230505
Epoch 2837/10000, Prediction Accuracy = 61.25%, Loss = 0.5168103575706482
Epoch: 2837, Batch Gradient Norm: 10.975692995186732
Epoch: 2837, Batch Gradient Norm after: 10.975692995186732
Epoch 2838/10000, Prediction Accuracy = 61.33399999999999%, Loss = 0.5111411094665528
Epoch: 2838, Batch Gradient Norm: 9.191064533369342
Epoch: 2838, Batch Gradient Norm after: 9.191064533369342
Epoch 2839/10000, Prediction Accuracy = 61.352%, Loss = 0.5009550631046296
Epoch: 2839, Batch Gradient Norm: 10.53529550212788
Epoch: 2839, Batch Gradient Norm after: 10.53529550212788
Epoch 2840/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5076990902423859
Epoch: 2840, Batch Gradient Norm: 12.393972042803778
Epoch: 2840, Batch Gradient Norm after: 12.393972042803778
Epoch 2841/10000, Prediction Accuracy = 61.23199999999999%, Loss = 0.5226604342460632
Epoch: 2841, Batch Gradient Norm: 11.03275624124999
Epoch: 2841, Batch Gradient Norm after: 11.03275624124999
Epoch 2842/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.5158231496810913
Epoch: 2842, Batch Gradient Norm: 8.474689975559313
Epoch: 2842, Batch Gradient Norm after: 8.474689975559313
Epoch 2843/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.5004427194595337
Epoch: 2843, Batch Gradient Norm: 7.861812159635062
Epoch: 2843, Batch Gradient Norm after: 7.861812159635062
Epoch 2844/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.49803327918052676
Epoch: 2844, Batch Gradient Norm: 9.930128058192018
Epoch: 2844, Batch Gradient Norm after: 9.930128058192018
Epoch 2845/10000, Prediction Accuracy = 61.31%, Loss = 0.507066011428833
Epoch: 2845, Batch Gradient Norm: 12.200099174789246
Epoch: 2845, Batch Gradient Norm after: 12.200099174789246
Epoch 2846/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.5248906016349792
Epoch: 2846, Batch Gradient Norm: 10.585128876515913
Epoch: 2846, Batch Gradient Norm after: 10.585128876515913
Epoch 2847/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.5186349332332612
Epoch: 2847, Batch Gradient Norm: 8.691828247296547
Epoch: 2847, Batch Gradient Norm after: 8.691828247296547
Epoch 2848/10000, Prediction Accuracy = 61.414%, Loss = 0.49992223978042605
Epoch: 2848, Batch Gradient Norm: 12.288116903389232
Epoch: 2848, Batch Gradient Norm after: 12.288116903389232
Epoch 2849/10000, Prediction Accuracy = 61.198%, Loss = 0.5229118704795838
Epoch: 2849, Batch Gradient Norm: 11.325341410295277
Epoch: 2849, Batch Gradient Norm after: 11.325341410295277
Epoch 2850/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.5149190306663514
Epoch: 2850, Batch Gradient Norm: 5.9373139776113435
Epoch: 2850, Batch Gradient Norm after: 5.9373139776113435
Epoch 2851/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.4870058298110962
Epoch: 2851, Batch Gradient Norm: 8.74593291638354
Epoch: 2851, Batch Gradient Norm after: 8.74593291638354
Epoch 2852/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.49855290055274964
Epoch: 2852, Batch Gradient Norm: 14.81953571838763
Epoch: 2852, Batch Gradient Norm after: 14.81953571838763
Epoch 2853/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.5447806477546692
Epoch: 2853, Batch Gradient Norm: 12.23365926334646
Epoch: 2853, Batch Gradient Norm after: 12.23365926334646
Epoch 2854/10000, Prediction Accuracy = 61.23%, Loss = 0.5246149063110351
Epoch: 2854, Batch Gradient Norm: 14.153941873633649
Epoch: 2854, Batch Gradient Norm after: 14.153941873633649
Epoch 2855/10000, Prediction Accuracy = 61.23%, Loss = 0.5400351405143737
Epoch: 2855, Batch Gradient Norm: 12.988631968577558
Epoch: 2855, Batch Gradient Norm after: 12.988631968577558
Epoch 2856/10000, Prediction Accuracy = 61.314%, Loss = 0.5303625702857971
Epoch: 2856, Batch Gradient Norm: 10.095392165914683
Epoch: 2856, Batch Gradient Norm after: 10.095392165914683
Epoch 2857/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.51334228515625
Epoch: 2857, Batch Gradient Norm: 6.582269438721437
Epoch: 2857, Batch Gradient Norm after: 6.582269438721437
Epoch 2858/10000, Prediction Accuracy = 61.298%, Loss = 0.48940056562423706
Epoch: 2858, Batch Gradient Norm: 10.877331903978716
Epoch: 2858, Batch Gradient Norm after: 10.877331903978716
Epoch 2859/10000, Prediction Accuracy = 61.29200000000001%, Loss = 0.5091574966907502
Epoch: 2859, Batch Gradient Norm: 11.213639426258506
Epoch: 2859, Batch Gradient Norm after: 11.213639426258506
Epoch 2860/10000, Prediction Accuracy = 61.286%, Loss = 0.5148647606372834
Epoch: 2860, Batch Gradient Norm: 9.841034002034672
Epoch: 2860, Batch Gradient Norm after: 9.841034002034672
Epoch 2861/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.5072521984577179
Epoch: 2861, Batch Gradient Norm: 9.385870895731708
Epoch: 2861, Batch Gradient Norm after: 9.385870895731708
Epoch 2862/10000, Prediction Accuracy = 61.18399999999999%, Loss = 0.5046850204467773
Epoch: 2862, Batch Gradient Norm: 9.671304241621266
Epoch: 2862, Batch Gradient Norm after: 9.671304241621266
Epoch 2863/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5066966593265534
Epoch: 2863, Batch Gradient Norm: 9.992344628109757
Epoch: 2863, Batch Gradient Norm after: 9.992344628109757
Epoch 2864/10000, Prediction Accuracy = 61.234%, Loss = 0.5094455242156982
Epoch: 2864, Batch Gradient Norm: 10.447490184506892
Epoch: 2864, Batch Gradient Norm after: 10.447490184506892
Epoch 2865/10000, Prediction Accuracy = 61.282%, Loss = 0.5138743042945861
Epoch: 2865, Batch Gradient Norm: 10.313688441428694
Epoch: 2865, Batch Gradient Norm after: 10.313688441428694
Epoch 2866/10000, Prediction Accuracy = 61.298%, Loss = 0.5083327710628509
Epoch: 2866, Batch Gradient Norm: 9.721846875870927
Epoch: 2866, Batch Gradient Norm after: 9.721846875870927
Epoch 2867/10000, Prediction Accuracy = 61.16600000000001%, Loss = 0.5070860385894775
Epoch: 2867, Batch Gradient Norm: 10.855838896353477
Epoch: 2867, Batch Gradient Norm after: 10.855838896353477
Epoch 2868/10000, Prediction Accuracy = 61.314%, Loss = 0.5214754223823548
Epoch: 2868, Batch Gradient Norm: 11.108783023883163
Epoch: 2868, Batch Gradient Norm after: 11.108783023883163
Epoch 2869/10000, Prediction Accuracy = 61.312%, Loss = 0.5136113882064819
Epoch: 2869, Batch Gradient Norm: 14.04684685393824
Epoch: 2869, Batch Gradient Norm after: 14.04684685393824
Epoch 2870/10000, Prediction Accuracy = 61.2%, Loss = 0.5424858093261719
Epoch: 2870, Batch Gradient Norm: 10.253818671658204
Epoch: 2870, Batch Gradient Norm after: 10.253818671658204
Epoch 2871/10000, Prediction Accuracy = 61.36%, Loss = 0.5065524697303772
Epoch: 2871, Batch Gradient Norm: 12.730799318713657
Epoch: 2871, Batch Gradient Norm after: 12.730799318713657
Epoch 2872/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.529241394996643
Epoch: 2872, Batch Gradient Norm: 8.74288624068703
Epoch: 2872, Batch Gradient Norm after: 8.74288624068703
Epoch 2873/10000, Prediction Accuracy = 61.278%, Loss = 0.5014890968799591
Epoch: 2873, Batch Gradient Norm: 12.138710420497386
Epoch: 2873, Batch Gradient Norm after: 12.138710420497386
Epoch 2874/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.5201595902442933
Epoch: 2874, Batch Gradient Norm: 11.44895549955072
Epoch: 2874, Batch Gradient Norm after: 11.44895549955072
Epoch 2875/10000, Prediction Accuracy = 61.138%, Loss = 0.5173447608947754
Epoch: 2875, Batch Gradient Norm: 10.041243498642713
Epoch: 2875, Batch Gradient Norm after: 10.041243498642713
Epoch 2876/10000, Prediction Accuracy = 61.35%, Loss = 0.5089855849742889
Epoch: 2876, Batch Gradient Norm: 8.410667781342076
Epoch: 2876, Batch Gradient Norm after: 8.410667781342076
Epoch 2877/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.49764578938484194
Epoch: 2877, Batch Gradient Norm: 9.966279653269648
Epoch: 2877, Batch Gradient Norm after: 9.966279653269648
Epoch 2878/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.5062912762165069
Epoch: 2878, Batch Gradient Norm: 10.891627811238125
Epoch: 2878, Batch Gradient Norm after: 10.891627811238125
Epoch 2879/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.5122334241867066
Epoch: 2879, Batch Gradient Norm: 12.266764358945936
Epoch: 2879, Batch Gradient Norm after: 12.266764358945936
Epoch 2880/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.5244515299797058
Epoch: 2880, Batch Gradient Norm: 10.53423957009036
Epoch: 2880, Batch Gradient Norm after: 10.53423957009036
Epoch 2881/10000, Prediction Accuracy = 61.33%, Loss = 0.50944584608078
Epoch: 2881, Batch Gradient Norm: 9.949384984079135
Epoch: 2881, Batch Gradient Norm after: 9.949384984079135
Epoch 2882/10000, Prediction Accuracy = 61.44%, Loss = 0.5060954391956329
Epoch: 2882, Batch Gradient Norm: 9.52181012277152
Epoch: 2882, Batch Gradient Norm after: 9.52181012277152
Epoch 2883/10000, Prediction Accuracy = 61.288%, Loss = 0.5013383507728577
Epoch: 2883, Batch Gradient Norm: 10.35353873646634
Epoch: 2883, Batch Gradient Norm after: 10.35353873646634
Epoch 2884/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.5066196084022522
Epoch: 2884, Batch Gradient Norm: 10.528759346606334
Epoch: 2884, Batch Gradient Norm after: 10.528759346606334
Epoch 2885/10000, Prediction Accuracy = 61.218%, Loss = 0.5077699720859528
Epoch: 2885, Batch Gradient Norm: 9.78256620098682
Epoch: 2885, Batch Gradient Norm after: 9.78256620098682
Epoch 2886/10000, Prediction Accuracy = 61.31%, Loss = 0.5024729609489441
Epoch: 2886, Batch Gradient Norm: 11.926413367771024
Epoch: 2886, Batch Gradient Norm after: 11.926413367771024
Epoch 2887/10000, Prediction Accuracy = 61.354%, Loss = 0.5194222807884217
Epoch: 2887, Batch Gradient Norm: 13.044240847825632
Epoch: 2887, Batch Gradient Norm after: 13.044240847825632
Epoch 2888/10000, Prediction Accuracy = 61.21999999999999%, Loss = 0.530677330493927
Epoch: 2888, Batch Gradient Norm: 8.841651678640835
Epoch: 2888, Batch Gradient Norm after: 8.841651678640835
Epoch 2889/10000, Prediction Accuracy = 61.326%, Loss = 0.5020507752895356
Epoch: 2889, Batch Gradient Norm: 6.784208909729927
Epoch: 2889, Batch Gradient Norm after: 6.784208909729927
Epoch 2890/10000, Prediction Accuracy = 61.31%, Loss = 0.48850916028022767
Epoch: 2890, Batch Gradient Norm: 11.477915185059754
Epoch: 2890, Batch Gradient Norm after: 11.477915185059754
Epoch 2891/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5158114433288574
Epoch: 2891, Batch Gradient Norm: 12.423532861067592
Epoch: 2891, Batch Gradient Norm after: 12.423532861067592
Epoch 2892/10000, Prediction Accuracy = 61.294%, Loss = 0.5219615221023559
Epoch: 2892, Batch Gradient Norm: 13.003677247370344
Epoch: 2892, Batch Gradient Norm after: 13.003677247370344
Epoch 2893/10000, Prediction Accuracy = 61.31999999999999%, Loss = 0.5290676116943359
Epoch: 2893, Batch Gradient Norm: 10.362158732630403
Epoch: 2893, Batch Gradient Norm after: 10.362158732630403
Epoch 2894/10000, Prediction Accuracy = 61.278%, Loss = 0.5079064130783081
Epoch: 2894, Batch Gradient Norm: 8.601021692027338
Epoch: 2894, Batch Gradient Norm after: 8.601021692027338
Epoch 2895/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.4991106867790222
Epoch: 2895, Batch Gradient Norm: 8.857712198015143
Epoch: 2895, Batch Gradient Norm after: 8.857712198015143
Epoch 2896/10000, Prediction Accuracy = 61.372%, Loss = 0.4990595281124115
Epoch: 2896, Batch Gradient Norm: 10.769567824205698
Epoch: 2896, Batch Gradient Norm after: 10.769567824205698
Epoch 2897/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.5129573822021485
Epoch: 2897, Batch Gradient Norm: 10.328829528411292
Epoch: 2897, Batch Gradient Norm after: 10.328829528411292
Epoch 2898/10000, Prediction Accuracy = 61.286%, Loss = 0.5078722894191742
Epoch: 2898, Batch Gradient Norm: 9.120691620413876
Epoch: 2898, Batch Gradient Norm after: 9.120691620413876
Epoch 2899/10000, Prediction Accuracy = 61.152%, Loss = 0.5002490401268005
Epoch: 2899, Batch Gradient Norm: 9.057159548438744
Epoch: 2899, Batch Gradient Norm after: 9.057159548438744
Epoch 2900/10000, Prediction Accuracy = 61.378%, Loss = 0.5002328395843506
Epoch: 2900, Batch Gradient Norm: 8.04214670217841
Epoch: 2900, Batch Gradient Norm after: 8.04214670217841
Epoch 2901/10000, Prediction Accuracy = 61.330000000000005%, Loss = 0.4947705090045929
Epoch: 2901, Batch Gradient Norm: 9.256868795439981
Epoch: 2901, Batch Gradient Norm after: 9.256868795439981
Epoch 2902/10000, Prediction Accuracy = 61.294%, Loss = 0.49969072341918946
Epoch: 2902, Batch Gradient Norm: 9.653371582772207
Epoch: 2902, Batch Gradient Norm after: 9.653371582772207
Epoch 2903/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.5036543905735016
Epoch: 2903, Batch Gradient Norm: 12.17636557463228
Epoch: 2903, Batch Gradient Norm after: 12.17636557463228
Epoch 2904/10000, Prediction Accuracy = 61.324%, Loss = 0.5213509678840638
Epoch: 2904, Batch Gradient Norm: 10.935442124284275
Epoch: 2904, Batch Gradient Norm after: 10.935442124284275
Epoch 2905/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.5099531948566437
Epoch: 2905, Batch Gradient Norm: 11.900162361419287
Epoch: 2905, Batch Gradient Norm after: 11.900162361419287
Epoch 2906/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.5179634988307953
Epoch: 2906, Batch Gradient Norm: 11.88022894013542
Epoch: 2906, Batch Gradient Norm after: 11.88022894013542
Epoch 2907/10000, Prediction Accuracy = 61.302%, Loss = 0.5174880504608155
Epoch: 2907, Batch Gradient Norm: 12.040788115357747
Epoch: 2907, Batch Gradient Norm after: 12.040788115357747
Epoch 2908/10000, Prediction Accuracy = 61.314%, Loss = 0.5161677241325379
Epoch: 2908, Batch Gradient Norm: 11.403406498122317
Epoch: 2908, Batch Gradient Norm after: 11.403406498122317
Epoch 2909/10000, Prediction Accuracy = 61.26800000000001%, Loss = 0.5120542645454407
Epoch: 2909, Batch Gradient Norm: 8.194581467026033
Epoch: 2909, Batch Gradient Norm after: 8.194581467026033
Epoch 2910/10000, Prediction Accuracy = 61.336%, Loss = 0.4956493079662323
Epoch: 2910, Batch Gradient Norm: 8.330124038245243
Epoch: 2910, Batch Gradient Norm after: 8.330124038245243
Epoch 2911/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.49300721287727356
Epoch: 2911, Batch Gradient Norm: 13.775699379725728
Epoch: 2911, Batch Gradient Norm after: 13.775699379725728
Epoch 2912/10000, Prediction Accuracy = 61.16799999999999%, Loss = 0.5356291174888611
Epoch: 2912, Batch Gradient Norm: 13.344609686416701
Epoch: 2912, Batch Gradient Norm after: 13.344609686416701
Epoch 2913/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.5304413557052612
Epoch: 2913, Batch Gradient Norm: 9.322608735722126
Epoch: 2913, Batch Gradient Norm after: 9.322608735722126
Epoch 2914/10000, Prediction Accuracy = 61.326%, Loss = 0.5014071762561798
Epoch: 2914, Batch Gradient Norm: 8.72431155304179
Epoch: 2914, Batch Gradient Norm after: 8.72431155304179
Epoch 2915/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.4997944414615631
Epoch: 2915, Batch Gradient Norm: 11.745241049999237
Epoch: 2915, Batch Gradient Norm after: 11.745241049999237
Epoch 2916/10000, Prediction Accuracy = 61.238%, Loss = 0.5202008843421936
Epoch: 2916, Batch Gradient Norm: 12.496734883201437
Epoch: 2916, Batch Gradient Norm after: 12.496734883201437
Epoch 2917/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.5225420236587525
Epoch: 2917, Batch Gradient Norm: 10.461315802685712
Epoch: 2917, Batch Gradient Norm after: 10.461315802685712
Epoch 2918/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5109175026416779
Epoch: 2918, Batch Gradient Norm: 7.658565706846435
Epoch: 2918, Batch Gradient Norm after: 7.658565706846435
Epoch 2919/10000, Prediction Accuracy = 61.396%, Loss = 0.4929589033126831
Epoch: 2919, Batch Gradient Norm: 8.922512783960686
Epoch: 2919, Batch Gradient Norm after: 8.922512783960686
Epoch 2920/10000, Prediction Accuracy = 61.372%, Loss = 0.49889421463012695
Epoch: 2920, Batch Gradient Norm: 9.199789855837995
Epoch: 2920, Batch Gradient Norm after: 9.199789855837995
Epoch 2921/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.499394291639328
Epoch: 2921, Batch Gradient Norm: 11.896755490812096
Epoch: 2921, Batch Gradient Norm after: 11.896755490812096
Epoch 2922/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.5168240785598754
Epoch: 2922, Batch Gradient Norm: 8.088262535317742
Epoch: 2922, Batch Gradient Norm after: 8.088262535317742
Epoch 2923/10000, Prediction Accuracy = 61.374%, Loss = 0.49537224173545835
Epoch: 2923, Batch Gradient Norm: 9.462395977708603
Epoch: 2923, Batch Gradient Norm after: 9.462395977708603
Epoch 2924/10000, Prediction Accuracy = 61.282%, Loss = 0.49941214323043825
Epoch: 2924, Batch Gradient Norm: 14.57164680169277
Epoch: 2924, Batch Gradient Norm after: 14.57164680169277
Epoch 2925/10000, Prediction Accuracy = 61.128%, Loss = 0.5400619387626648
Epoch: 2925, Batch Gradient Norm: 9.507940619757868
Epoch: 2925, Batch Gradient Norm after: 9.507940619757868
Epoch 2926/10000, Prediction Accuracy = 61.36%, Loss = 0.5011616706848144
Epoch: 2926, Batch Gradient Norm: 11.302875760707867
Epoch: 2926, Batch Gradient Norm after: 11.302875760707867
Epoch 2927/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.5130329549312591
Epoch: 2927, Batch Gradient Norm: 9.3989748445493
Epoch: 2927, Batch Gradient Norm after: 9.3989748445493
Epoch 2928/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5014640688896179
Epoch: 2928, Batch Gradient Norm: 8.515414985051576
Epoch: 2928, Batch Gradient Norm after: 8.515414985051576
Epoch 2929/10000, Prediction Accuracy = 61.324%, Loss = 0.49884390234947207
Epoch: 2929, Batch Gradient Norm: 12.743961342769838
Epoch: 2929, Batch Gradient Norm after: 12.743961342769838
Epoch 2930/10000, Prediction Accuracy = 61.246%, Loss = 0.5228238701820374
Epoch: 2930, Batch Gradient Norm: 10.444531021855376
Epoch: 2930, Batch Gradient Norm after: 10.444531021855376
Epoch 2931/10000, Prediction Accuracy = 61.28399999999999%, Loss = 0.5084139585494996
Epoch: 2931, Batch Gradient Norm: 8.997063825814129
Epoch: 2931, Batch Gradient Norm after: 8.997063825814129
Epoch 2932/10000, Prediction Accuracy = 61.28000000000001%, Loss = 0.49974703788757324
Epoch: 2932, Batch Gradient Norm: 13.735436743954434
Epoch: 2932, Batch Gradient Norm after: 13.735436743954434
Epoch 2933/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.5294625163078308
Epoch: 2933, Batch Gradient Norm: 13.130865103258499
Epoch: 2933, Batch Gradient Norm after: 13.130865103258499
Epoch 2934/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.5271074891090393
Epoch: 2934, Batch Gradient Norm: 11.097537112075042
Epoch: 2934, Batch Gradient Norm after: 11.097537112075042
Epoch 2935/10000, Prediction Accuracy = 61.366%, Loss = 0.5128121733665466
Epoch: 2935, Batch Gradient Norm: 9.317253002113915
Epoch: 2935, Batch Gradient Norm after: 9.317253002113915
Epoch 2936/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.5012044668197632
Epoch: 2936, Batch Gradient Norm: 9.521398622760087
Epoch: 2936, Batch Gradient Norm after: 9.521398622760087
Epoch 2937/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.5049072146415711
Epoch: 2937, Batch Gradient Norm: 6.972620846217837
Epoch: 2937, Batch Gradient Norm after: 6.972620846217837
Epoch 2938/10000, Prediction Accuracy = 61.492%, Loss = 0.4864276349544525
Epoch: 2938, Batch Gradient Norm: 9.577379255407468
Epoch: 2938, Batch Gradient Norm after: 9.577379255407468
Epoch 2939/10000, Prediction Accuracy = 61.42%, Loss = 0.4991957128047943
Epoch: 2939, Batch Gradient Norm: 10.894946670273228
Epoch: 2939, Batch Gradient Norm after: 10.894946670273228
Epoch 2940/10000, Prediction Accuracy = 61.298%, Loss = 0.5089295864105224
Epoch: 2940, Batch Gradient Norm: 11.850403602081338
Epoch: 2940, Batch Gradient Norm after: 11.850403602081338
Epoch 2941/10000, Prediction Accuracy = 61.56%, Loss = 0.5153838396072388
Epoch: 2941, Batch Gradient Norm: 9.972045616893132
Epoch: 2941, Batch Gradient Norm after: 9.972045616893132
Epoch 2942/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.5023239076137542
Epoch: 2942, Batch Gradient Norm: 9.546528766119634
Epoch: 2942, Batch Gradient Norm after: 9.546528766119634
Epoch 2943/10000, Prediction Accuracy = 61.362%, Loss = 0.5011180579662323
Epoch: 2943, Batch Gradient Norm: 12.042553792074532
Epoch: 2943, Batch Gradient Norm after: 12.042553792074532
Epoch 2944/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.5252860724925995
Epoch: 2944, Batch Gradient Norm: 8.513378040938816
Epoch: 2944, Batch Gradient Norm after: 8.513378040938816
Epoch 2945/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.4989402949810028
Epoch: 2945, Batch Gradient Norm: 6.759127970182871
Epoch: 2945, Batch Gradient Norm after: 6.759127970182871
Epoch 2946/10000, Prediction Accuracy = 61.28399999999999%, Loss = 0.4866617560386658
Epoch: 2946, Batch Gradient Norm: 10.577505007050647
Epoch: 2946, Batch Gradient Norm after: 10.577505007050647
Epoch 2947/10000, Prediction Accuracy = 61.348%, Loss = 0.5116302609443665
Epoch: 2947, Batch Gradient Norm: 11.13771138249959
Epoch: 2947, Batch Gradient Norm after: 11.13771138249959
Epoch 2948/10000, Prediction Accuracy = 61.322%, Loss = 0.5138127207756042
Epoch: 2948, Batch Gradient Norm: 11.76005682307861
Epoch: 2948, Batch Gradient Norm after: 11.76005682307861
Epoch 2949/10000, Prediction Accuracy = 61.424%, Loss = 0.5156259536743164
Epoch: 2949, Batch Gradient Norm: 8.620648144978382
Epoch: 2949, Batch Gradient Norm after: 8.620648144978382
Epoch 2950/10000, Prediction Accuracy = 61.3%, Loss = 0.49635679721832277
Epoch: 2950, Batch Gradient Norm: 11.99466077966863
Epoch: 2950, Batch Gradient Norm after: 11.99466077966863
Epoch 2951/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.5160262942314148
Epoch: 2951, Batch Gradient Norm: 12.873706524361552
Epoch: 2951, Batch Gradient Norm after: 12.873706524361552
Epoch 2952/10000, Prediction Accuracy = 61.282%, Loss = 0.524577796459198
Epoch: 2952, Batch Gradient Norm: 11.470754269973602
Epoch: 2952, Batch Gradient Norm after: 11.470754269973602
Epoch 2953/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.5126110076904297
Epoch: 2953, Batch Gradient Norm: 10.341900657119368
Epoch: 2953, Batch Gradient Norm after: 10.341900657119368
Epoch 2954/10000, Prediction Accuracy = 61.314%, Loss = 0.5068751513957978
Epoch: 2954, Batch Gradient Norm: 8.885792280392613
Epoch: 2954, Batch Gradient Norm after: 8.885792280392613
Epoch 2955/10000, Prediction Accuracy = 61.27%, Loss = 0.4965782344341278
Epoch: 2955, Batch Gradient Norm: 11.406368881840955
Epoch: 2955, Batch Gradient Norm after: 11.406368881840955
Epoch 2956/10000, Prediction Accuracy = 61.266%, Loss = 0.513826334476471
Epoch: 2956, Batch Gradient Norm: 11.303861484969264
Epoch: 2956, Batch Gradient Norm after: 11.303861484969264
Epoch 2957/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.5148378014564514
Epoch: 2957, Batch Gradient Norm: 9.93534142119955
Epoch: 2957, Batch Gradient Norm after: 9.93534142119955
Epoch 2958/10000, Prediction Accuracy = 61.372%, Loss = 0.5071357905864715
Epoch: 2958, Batch Gradient Norm: 9.08201634637405
Epoch: 2958, Batch Gradient Norm after: 9.08201634637405
Epoch 2959/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.5000003635883331
Epoch: 2959, Batch Gradient Norm: 14.385227414840411
Epoch: 2959, Batch Gradient Norm after: 14.385227414840411
Epoch 2960/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5504682183265686
Epoch: 2960, Batch Gradient Norm: 10.507050527133243
Epoch: 2960, Batch Gradient Norm after: 10.507050527133243
Epoch 2961/10000, Prediction Accuracy = 61.266%, Loss = 0.5074541389942169
Epoch: 2961, Batch Gradient Norm: 10.47314322755787
Epoch: 2961, Batch Gradient Norm after: 10.47314322755787
Epoch 2962/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.5118496954441071
Epoch: 2962, Batch Gradient Norm: 8.560652709823398
Epoch: 2962, Batch Gradient Norm after: 8.560652709823398
Epoch 2963/10000, Prediction Accuracy = 61.35%, Loss = 0.4949427008628845
Epoch: 2963, Batch Gradient Norm: 10.29749697915202
Epoch: 2963, Batch Gradient Norm after: 10.29749697915202
Epoch 2964/10000, Prediction Accuracy = 61.338%, Loss = 0.5057674050331116
Epoch: 2964, Batch Gradient Norm: 12.581419038237689
Epoch: 2964, Batch Gradient Norm after: 12.581419038237689
Epoch 2965/10000, Prediction Accuracy = 61.336%, Loss = 0.5251726031303405
Epoch: 2965, Batch Gradient Norm: 11.133907756050522
Epoch: 2965, Batch Gradient Norm after: 11.133907756050522
Epoch 2966/10000, Prediction Accuracy = 61.32399999999999%, Loss = 0.5103633761405945
Epoch: 2966, Batch Gradient Norm: 8.183933187177246
Epoch: 2966, Batch Gradient Norm after: 8.183933187177246
Epoch 2967/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.4927727997303009
Epoch: 2967, Batch Gradient Norm: 8.05494583063116
Epoch: 2967, Batch Gradient Norm after: 8.05494583063116
Epoch 2968/10000, Prediction Accuracy = 61.376%, Loss = 0.49121366143226625
Epoch: 2968, Batch Gradient Norm: 10.960666870351018
Epoch: 2968, Batch Gradient Norm after: 10.960666870351018
Epoch 2969/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.5103186011314392
Epoch: 2969, Batch Gradient Norm: 11.135258691043527
Epoch: 2969, Batch Gradient Norm after: 11.135258691043527
Epoch 2970/10000, Prediction Accuracy = 61.384%, Loss = 0.5146216511726379
Epoch: 2970, Batch Gradient Norm: 10.635313340357236
Epoch: 2970, Batch Gradient Norm after: 10.635313340357236
Epoch 2971/10000, Prediction Accuracy = 61.35%, Loss = 0.5099104523658753
Epoch: 2971, Batch Gradient Norm: 10.303710487010317
Epoch: 2971, Batch Gradient Norm after: 10.303710487010317
Epoch 2972/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5084583520889282
Epoch: 2972, Batch Gradient Norm: 11.19093989865472
Epoch: 2972, Batch Gradient Norm after: 11.19093989865472
Epoch 2973/10000, Prediction Accuracy = 61.29%, Loss = 0.5116741538047791
Epoch: 2973, Batch Gradient Norm: 11.251905750820708
Epoch: 2973, Batch Gradient Norm after: 11.251905750820708
Epoch 2974/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.50875284075737
Epoch: 2974, Batch Gradient Norm: 13.650157085507578
Epoch: 2974, Batch Gradient Norm after: 13.650157085507578
Epoch 2975/10000, Prediction Accuracy = 61.222%, Loss = 0.5295277118682862
Epoch: 2975, Batch Gradient Norm: 10.96813485080393
Epoch: 2975, Batch Gradient Norm after: 10.96813485080393
Epoch 2976/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5107709884643554
Epoch: 2976, Batch Gradient Norm: 6.508323159474772
Epoch: 2976, Batch Gradient Norm after: 6.508323159474772
Epoch 2977/10000, Prediction Accuracy = 61.334%, Loss = 0.4856520414352417
Epoch: 2977, Batch Gradient Norm: 8.810838530232846
Epoch: 2977, Batch Gradient Norm after: 8.810838530232846
Epoch 2978/10000, Prediction Accuracy = 61.444%, Loss = 0.4945123612880707
Epoch: 2978, Batch Gradient Norm: 8.97316799021973
Epoch: 2978, Batch Gradient Norm after: 8.97316799021973
Epoch 2979/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.4956352949142456
Epoch: 2979, Batch Gradient Norm: 11.457928988099193
Epoch: 2979, Batch Gradient Norm after: 11.457928988099193
Epoch 2980/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5130281805992126
Epoch: 2980, Batch Gradient Norm: 11.096798687308267
Epoch: 2980, Batch Gradient Norm after: 11.096798687308267
Epoch 2981/10000, Prediction Accuracy = 61.41199999999999%, Loss = 0.5113926291465759
Epoch: 2981, Batch Gradient Norm: 6.480516385227309
Epoch: 2981, Batch Gradient Norm after: 6.480516385227309
Epoch 2982/10000, Prediction Accuracy = 61.314%, Loss = 0.48357529640197755
Epoch: 2982, Batch Gradient Norm: 11.878201192218803
Epoch: 2982, Batch Gradient Norm after: 11.878201192218803
Epoch 2983/10000, Prediction Accuracy = 61.288%, Loss = 0.5199426233768463
Epoch: 2983, Batch Gradient Norm: 11.795702140880774
Epoch: 2983, Batch Gradient Norm after: 11.795702140880774
Epoch 2984/10000, Prediction Accuracy = 61.372%, Loss = 0.5130754053592682
Epoch: 2984, Batch Gradient Norm: 10.447738510943417
Epoch: 2984, Batch Gradient Norm after: 10.447738510943417
Epoch 2985/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.5040315389633179
Epoch: 2985, Batch Gradient Norm: 11.759586587249794
Epoch: 2985, Batch Gradient Norm after: 11.759586587249794
Epoch 2986/10000, Prediction Accuracy = 61.294%, Loss = 0.5120199501514435
Epoch: 2986, Batch Gradient Norm: 8.096727455215413
Epoch: 2986, Batch Gradient Norm after: 8.096727455215413
Epoch 2987/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.4917306900024414
Epoch: 2987, Batch Gradient Norm: 10.21553397891676
Epoch: 2987, Batch Gradient Norm after: 10.21553397891676
Epoch 2988/10000, Prediction Accuracy = 61.372%, Loss = 0.5036288917064666
Epoch: 2988, Batch Gradient Norm: 10.400647060056626
Epoch: 2988, Batch Gradient Norm after: 10.400647060056626
Epoch 2989/10000, Prediction Accuracy = 61.364%, Loss = 0.5059125125408173
Epoch: 2989, Batch Gradient Norm: 10.125594225065521
Epoch: 2989, Batch Gradient Norm after: 10.125594225065521
Epoch 2990/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5028059005737304
Epoch: 2990, Batch Gradient Norm: 9.660217321462675
Epoch: 2990, Batch Gradient Norm after: 9.660217321462675
Epoch 2991/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.49718351364135743
Epoch: 2991, Batch Gradient Norm: 12.746883362465422
Epoch: 2991, Batch Gradient Norm after: 12.746883362465422
Epoch 2992/10000, Prediction Accuracy = 61.42%, Loss = 0.5215808272361755
Epoch: 2992, Batch Gradient Norm: 14.298212708113203
Epoch: 2992, Batch Gradient Norm after: 14.298212708113203
Epoch 2993/10000, Prediction Accuracy = 61.38199999999999%, Loss = 0.5335766434669494
Epoch: 2993, Batch Gradient Norm: 11.844070054675395
Epoch: 2993, Batch Gradient Norm after: 11.844070054675395
Epoch 2994/10000, Prediction Accuracy = 61.23%, Loss = 0.5161027073860168
Epoch: 2994, Batch Gradient Norm: 7.758434071857565
Epoch: 2994, Batch Gradient Norm after: 7.758434071857565
Epoch 2995/10000, Prediction Accuracy = 61.342%, Loss = 0.4897520005702972
Epoch: 2995, Batch Gradient Norm: 6.747224753896456
Epoch: 2995, Batch Gradient Norm after: 6.747224753896456
Epoch 2996/10000, Prediction Accuracy = 61.338%, Loss = 0.4847868502140045
Epoch: 2996, Batch Gradient Norm: 10.116538378584297
Epoch: 2996, Batch Gradient Norm after: 10.116538378584297
Epoch 2997/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.5028411030769349
Epoch: 2997, Batch Gradient Norm: 14.78412493017771
Epoch: 2997, Batch Gradient Norm after: 14.78412493017771
Epoch 2998/10000, Prediction Accuracy = 61.28399999999999%, Loss = 0.5588831663131714
Epoch: 2998, Batch Gradient Norm: 8.189898135666747
Epoch: 2998, Batch Gradient Norm after: 8.189898135666747
Epoch 2999/10000, Prediction Accuracy = 61.338%, Loss = 0.49446517825126646
Epoch: 2999, Batch Gradient Norm: 10.2431957147718
Epoch: 2999, Batch Gradient Norm after: 10.2431957147718
Epoch 3000/10000, Prediction Accuracy = 61.41200000000001%, Loss = 0.5037394464015961
Epoch: 3000, Batch Gradient Norm: 11.233639372549291
Epoch: 3000, Batch Gradient Norm after: 11.233639372549291
Epoch 3001/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5147773087024688
Epoch: 3001, Batch Gradient Norm: 8.486652675281201
Epoch: 3001, Batch Gradient Norm after: 8.486652675281201
Epoch 3002/10000, Prediction Accuracy = 61.34000000000001%, Loss = 0.49097050428390504
Epoch: 3002, Batch Gradient Norm: 8.32192282337644
Epoch: 3002, Batch Gradient Norm after: 8.32192282337644
Epoch 3003/10000, Prediction Accuracy = 61.402%, Loss = 0.4920761466026306
Epoch: 3003, Batch Gradient Norm: 10.048566016090252
Epoch: 3003, Batch Gradient Norm after: 10.048566016090252
Epoch 3004/10000, Prediction Accuracy = 61.416%, Loss = 0.5040283381938935
Epoch: 3004, Batch Gradient Norm: 12.218523088281717
Epoch: 3004, Batch Gradient Norm after: 12.218523088281717
Epoch 3005/10000, Prediction Accuracy = 61.274%, Loss = 0.5226754903793335
Epoch: 3005, Batch Gradient Norm: 8.320440366499747
Epoch: 3005, Batch Gradient Norm after: 8.320440366499747
Epoch 3006/10000, Prediction Accuracy = 61.376%, Loss = 0.49421871900558473
Epoch: 3006, Batch Gradient Norm: 9.20035038114008
Epoch: 3006, Batch Gradient Norm after: 9.20035038114008
Epoch 3007/10000, Prediction Accuracy = 61.352%, Loss = 0.4952359735965729
Epoch: 3007, Batch Gradient Norm: 10.263232730217947
Epoch: 3007, Batch Gradient Norm after: 10.263232730217947
Epoch 3008/10000, Prediction Accuracy = 61.35%, Loss = 0.5029018640518188
Epoch: 3008, Batch Gradient Norm: 9.654375910290982
Epoch: 3008, Batch Gradient Norm after: 9.654375910290982
Epoch 3009/10000, Prediction Accuracy = 61.396%, Loss = 0.4993464171886444
Epoch: 3009, Batch Gradient Norm: 11.141669308778248
Epoch: 3009, Batch Gradient Norm after: 11.141669308778248
Epoch 3010/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.5097101211547852
Epoch: 3010, Batch Gradient Norm: 10.995871769545731
Epoch: 3010, Batch Gradient Norm after: 10.995871769545731
Epoch 3011/10000, Prediction Accuracy = 61.44200000000001%, Loss = 0.5092811822891236
Epoch: 3011, Batch Gradient Norm: 7.708275060558443
Epoch: 3011, Batch Gradient Norm after: 7.708275060558443
Epoch 3012/10000, Prediction Accuracy = 61.33%, Loss = 0.48786080479621885
Epoch: 3012, Batch Gradient Norm: 12.51862443274995
Epoch: 3012, Batch Gradient Norm after: 12.51862443274995
Epoch 3013/10000, Prediction Accuracy = 61.448%, Loss = 0.519416218996048
Epoch: 3013, Batch Gradient Norm: 13.31074104723864
Epoch: 3013, Batch Gradient Norm after: 13.31074104723864
Epoch 3014/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.5280968546867371
Epoch: 3014, Batch Gradient Norm: 10.110466123849838
Epoch: 3014, Batch Gradient Norm after: 10.110466123849838
Epoch 3015/10000, Prediction Accuracy = 61.362%, Loss = 0.5009194672107696
Epoch: 3015, Batch Gradient Norm: 9.49696860800757
Epoch: 3015, Batch Gradient Norm after: 9.49696860800757
Epoch 3016/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.49687933921813965
Epoch: 3016, Batch Gradient Norm: 11.754709493586857
Epoch: 3016, Batch Gradient Norm after: 11.754709493586857
Epoch 3017/10000, Prediction Accuracy = 61.384%, Loss = 0.5172312080860137
Epoch: 3017, Batch Gradient Norm: 12.056462014710332
Epoch: 3017, Batch Gradient Norm after: 12.056462014710332
Epoch 3018/10000, Prediction Accuracy = 61.354%, Loss = 0.5167991518974304
Epoch: 3018, Batch Gradient Norm: 12.301979519371626
Epoch: 3018, Batch Gradient Norm after: 12.301979519371626
Epoch 3019/10000, Prediction Accuracy = 61.428%, Loss = 0.5159817576408386
Epoch: 3019, Batch Gradient Norm: 14.204749481989337
Epoch: 3019, Batch Gradient Norm after: 14.204749481989337
Epoch 3020/10000, Prediction Accuracy = 61.39%, Loss = 0.5311850547790528
Epoch: 3020, Batch Gradient Norm: 10.473105323529047
Epoch: 3020, Batch Gradient Norm after: 10.473105323529047
Epoch 3021/10000, Prediction Accuracy = 61.238%, Loss = 0.5087351441383362
Epoch: 3021, Batch Gradient Norm: 7.782319355368284
Epoch: 3021, Batch Gradient Norm after: 7.782319355368284
Epoch 3022/10000, Prediction Accuracy = 61.263999999999996%, Loss = 0.4912350535392761
Epoch: 3022, Batch Gradient Norm: 9.152970953294874
Epoch: 3022, Batch Gradient Norm after: 9.152970953294874
Epoch 3023/10000, Prediction Accuracy = 61.31999999999999%, Loss = 0.49889069199562075
Epoch: 3023, Batch Gradient Norm: 13.30674708652459
Epoch: 3023, Batch Gradient Norm after: 13.30674708652459
Epoch 3024/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.5240208029747009
Epoch: 3024, Batch Gradient Norm: 11.979247333197653
Epoch: 3024, Batch Gradient Norm after: 11.979247333197653
Epoch 3025/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.5133834302425384
Epoch: 3025, Batch Gradient Norm: 7.515671076267463
Epoch: 3025, Batch Gradient Norm after: 7.515671076267463
Epoch 3026/10000, Prediction Accuracy = 61.424%, Loss = 0.4903151631355286
Epoch: 3026, Batch Gradient Norm: 7.230515943323127
Epoch: 3026, Batch Gradient Norm after: 7.230515943323127
Epoch 3027/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4843281924724579
Epoch: 3027, Batch Gradient Norm: 9.397303485593588
Epoch: 3027, Batch Gradient Norm after: 9.397303485593588
Epoch 3028/10000, Prediction Accuracy = 61.46600000000001%, Loss = 0.49800161719322206
Epoch: 3028, Batch Gradient Norm: 10.258476967834278
Epoch: 3028, Batch Gradient Norm after: 10.258476967834278
Epoch 3029/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.5033396065235138
Epoch: 3029, Batch Gradient Norm: 9.025424416032749
Epoch: 3029, Batch Gradient Norm after: 9.025424416032749
Epoch 3030/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.49599873423576357
Epoch: 3030, Batch Gradient Norm: 10.761082592319006
Epoch: 3030, Batch Gradient Norm after: 10.761082592319006
Epoch 3031/10000, Prediction Accuracy = 61.288%, Loss = 0.5063090443611145
Epoch: 3031, Batch Gradient Norm: 11.064297040119554
Epoch: 3031, Batch Gradient Norm after: 11.064297040119554
Epoch 3032/10000, Prediction Accuracy = 61.395999999999994%, Loss = 0.5081021785736084
Epoch: 3032, Batch Gradient Norm: 12.634801089461586
Epoch: 3032, Batch Gradient Norm after: 12.634801089461586
Epoch 3033/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.5240635097026825
Epoch: 3033, Batch Gradient Norm: 7.978557844808382
Epoch: 3033, Batch Gradient Norm after: 7.978557844808382
Epoch 3034/10000, Prediction Accuracy = 61.46600000000001%, Loss = 0.4896834850311279
Epoch: 3034, Batch Gradient Norm: 9.99031145645507
Epoch: 3034, Batch Gradient Norm after: 9.99031145645507
Epoch 3035/10000, Prediction Accuracy = 61.36%, Loss = 0.49862123727798463
Epoch: 3035, Batch Gradient Norm: 12.640170204922972
Epoch: 3035, Batch Gradient Norm after: 12.640170204922972
Epoch 3036/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.5168140769004822
Epoch: 3036, Batch Gradient Norm: 11.497607125489104
Epoch: 3036, Batch Gradient Norm after: 11.497607125489104
Epoch 3037/10000, Prediction Accuracy = 61.456%, Loss = 0.5114141881465912
Epoch: 3037, Batch Gradient Norm: 9.655187431253024
Epoch: 3037, Batch Gradient Norm after: 9.655187431253024
Epoch 3038/10000, Prediction Accuracy = 61.38000000000001%, Loss = 0.5004462838172913
Epoch: 3038, Batch Gradient Norm: 10.946584199966559
Epoch: 3038, Batch Gradient Norm after: 10.946584199966559
Epoch 3039/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5109221756458282
Epoch: 3039, Batch Gradient Norm: 13.249820968173205
Epoch: 3039, Batch Gradient Norm after: 13.249820968173205
Epoch 3040/10000, Prediction Accuracy = 61.342000000000006%, Loss = 0.5324501752853393
Epoch: 3040, Batch Gradient Norm: 11.258714556327487
Epoch: 3040, Batch Gradient Norm after: 11.258714556327487
Epoch 3041/10000, Prediction Accuracy = 61.331999999999994%, Loss = 0.5122877478599548
Epoch: 3041, Batch Gradient Norm: 9.160243398920548
Epoch: 3041, Batch Gradient Norm after: 9.160243398920548
Epoch 3042/10000, Prediction Accuracy = 61.38399999999999%, Loss = 0.4974637567996979
Epoch: 3042, Batch Gradient Norm: 11.942794206652795
Epoch: 3042, Batch Gradient Norm after: 11.942794206652795
Epoch 3043/10000, Prediction Accuracy = 61.29%, Loss = 0.5211443185806275
Epoch: 3043, Batch Gradient Norm: 6.067430206070849
Epoch: 3043, Batch Gradient Norm after: 6.067430206070849
Epoch 3044/10000, Prediction Accuracy = 61.39%, Loss = 0.48121673464775083
Epoch: 3044, Batch Gradient Norm: 9.193480232429815
Epoch: 3044, Batch Gradient Norm after: 9.193480232429815
Epoch 3045/10000, Prediction Accuracy = 61.36%, Loss = 0.4975438892841339
Epoch: 3045, Batch Gradient Norm: 10.148970830241844
Epoch: 3045, Batch Gradient Norm after: 10.148970830241844
Epoch 3046/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5003680229187012
Epoch: 3046, Batch Gradient Norm: 10.98948066476243
Epoch: 3046, Batch Gradient Norm after: 10.98948066476243
Epoch 3047/10000, Prediction Accuracy = 61.5%, Loss = 0.5066526889801025
Epoch: 3047, Batch Gradient Norm: 9.542273569424687
Epoch: 3047, Batch Gradient Norm after: 9.542273569424687
Epoch 3048/10000, Prediction Accuracy = 61.338%, Loss = 0.4986079573631287
Epoch: 3048, Batch Gradient Norm: 10.924177506771152
Epoch: 3048, Batch Gradient Norm after: 10.924177506771152
Epoch 3049/10000, Prediction Accuracy = 61.414%, Loss = 0.5070546627044678
Epoch: 3049, Batch Gradient Norm: 10.570265414786903
Epoch: 3049, Batch Gradient Norm after: 10.570265414786903
Epoch 3050/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.5061209380626679
Epoch: 3050, Batch Gradient Norm: 9.71565048212579
Epoch: 3050, Batch Gradient Norm after: 9.71565048212579
Epoch 3051/10000, Prediction Accuracy = 61.362%, Loss = 0.4986198604106903
Epoch: 3051, Batch Gradient Norm: 9.34069582042384
Epoch: 3051, Batch Gradient Norm after: 9.34069582042384
Epoch 3052/10000, Prediction Accuracy = 61.484%, Loss = 0.49603421688079835
Epoch: 3052, Batch Gradient Norm: 11.453011605167783
Epoch: 3052, Batch Gradient Norm after: 11.453011605167783
Epoch 3053/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.5081186830997467
Epoch: 3053, Batch Gradient Norm: 13.405611775811847
Epoch: 3053, Batch Gradient Norm after: 13.405611775811847
Epoch 3054/10000, Prediction Accuracy = 61.386%, Loss = 0.5221780657768249
Epoch: 3054, Batch Gradient Norm: 11.948550291387726
Epoch: 3054, Batch Gradient Norm after: 11.948550291387726
Epoch 3055/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.5132467865943908
Epoch: 3055, Batch Gradient Norm: 11.662283148599492
Epoch: 3055, Batch Gradient Norm after: 11.662283148599492
Epoch 3056/10000, Prediction Accuracy = 61.37800000000001%, Loss = 0.5107473731040955
Epoch: 3056, Batch Gradient Norm: 8.610898986725573
Epoch: 3056, Batch Gradient Norm after: 8.610898986725573
Epoch 3057/10000, Prediction Accuracy = 61.467999999999996%, Loss = 0.49224615693092344
Epoch: 3057, Batch Gradient Norm: 11.199401059183698
Epoch: 3057, Batch Gradient Norm after: 11.199401059183698
Epoch 3058/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.514004522562027
Epoch: 3058, Batch Gradient Norm: 11.08091563608466
Epoch: 3058, Batch Gradient Norm after: 11.08091563608466
Epoch 3059/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.5108670353889465
Epoch: 3059, Batch Gradient Norm: 11.478751467577668
Epoch: 3059, Batch Gradient Norm after: 11.478751467577668
Epoch 3060/10000, Prediction Accuracy = 61.23%, Loss = 0.5155339717864991
Epoch: 3060, Batch Gradient Norm: 9.249637880982757
Epoch: 3060, Batch Gradient Norm after: 9.249637880982757
Epoch 3061/10000, Prediction Accuracy = 61.44200000000001%, Loss = 0.4942559540271759
Epoch: 3061, Batch Gradient Norm: 8.068155540466995
Epoch: 3061, Batch Gradient Norm after: 8.068155540466995
Epoch 3062/10000, Prediction Accuracy = 61.46%, Loss = 0.4882175028324127
Epoch: 3062, Batch Gradient Norm: 9.124599483608657
Epoch: 3062, Batch Gradient Norm after: 9.124599483608657
Epoch 3063/10000, Prediction Accuracy = 61.444%, Loss = 0.49526530504226685
Epoch: 3063, Batch Gradient Norm: 9.71885661008194
Epoch: 3063, Batch Gradient Norm after: 9.71885661008194
Epoch 3064/10000, Prediction Accuracy = 61.408%, Loss = 0.4976072072982788
Epoch: 3064, Batch Gradient Norm: 7.02205410104714
Epoch: 3064, Batch Gradient Norm after: 7.02205410104714
Epoch 3065/10000, Prediction Accuracy = 61.464%, Loss = 0.48519458174705504
Epoch: 3065, Batch Gradient Norm: 9.557057717808904
Epoch: 3065, Batch Gradient Norm after: 9.557057717808904
Epoch 3066/10000, Prediction Accuracy = 61.396%, Loss = 0.4947509229183197
Epoch: 3066, Batch Gradient Norm: 10.981119884018568
Epoch: 3066, Batch Gradient Norm after: 10.981119884018568
Epoch 3067/10000, Prediction Accuracy = 61.24400000000001%, Loss = 0.5082532584667205
Epoch: 3067, Batch Gradient Norm: 12.036906302632381
Epoch: 3067, Batch Gradient Norm after: 12.036906302632381
Epoch 3068/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.5173535585403443
Epoch: 3068, Batch Gradient Norm: 12.017725109586953
Epoch: 3068, Batch Gradient Norm after: 12.017725109586953
Epoch 3069/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.5133109092712402
Epoch: 3069, Batch Gradient Norm: 10.116881818079534
Epoch: 3069, Batch Gradient Norm after: 10.116881818079534
Epoch 3070/10000, Prediction Accuracy = 61.362%, Loss = 0.5006123542785644
Epoch: 3070, Batch Gradient Norm: 10.350698535483993
Epoch: 3070, Batch Gradient Norm after: 10.350698535483993
Epoch 3071/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.5027475953102112
Epoch: 3071, Batch Gradient Norm: 11.50607523424378
Epoch: 3071, Batch Gradient Norm after: 11.50607523424378
Epoch 3072/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5096935451030731
Epoch: 3072, Batch Gradient Norm: 11.362191197449338
Epoch: 3072, Batch Gradient Norm after: 11.362191197449338
Epoch 3073/10000, Prediction Accuracy = 61.34799999999999%, Loss = 0.5106489002704621
Epoch: 3073, Batch Gradient Norm: 8.218735329715711
Epoch: 3073, Batch Gradient Norm after: 8.218735329715711
Epoch 3074/10000, Prediction Accuracy = 61.468%, Loss = 0.4876944661140442
Epoch: 3074, Batch Gradient Norm: 11.525707744222386
Epoch: 3074, Batch Gradient Norm after: 11.525707744222386
Epoch 3075/10000, Prediction Accuracy = 61.396%, Loss = 0.5098817586898804
Epoch: 3075, Batch Gradient Norm: 11.370479722315702
Epoch: 3075, Batch Gradient Norm after: 11.370479722315702
Epoch 3076/10000, Prediction Accuracy = 61.33%, Loss = 0.5069581806659699
Epoch: 3076, Batch Gradient Norm: 11.712499248440752
Epoch: 3076, Batch Gradient Norm after: 11.712499248440752
Epoch 3077/10000, Prediction Accuracy = 61.331999999999994%, Loss = 0.509978187084198
Epoch: 3077, Batch Gradient Norm: 10.433383449187842
Epoch: 3077, Batch Gradient Norm after: 10.433383449187842
Epoch 3078/10000, Prediction Accuracy = 61.412%, Loss = 0.5011574625968933
Epoch: 3078, Batch Gradient Norm: 7.226374152324084
Epoch: 3078, Batch Gradient Norm after: 7.226374152324084
Epoch 3079/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.48241122961044314
Epoch: 3079, Batch Gradient Norm: 6.413710110657012
Epoch: 3079, Batch Gradient Norm after: 6.413710110657012
Epoch 3080/10000, Prediction Accuracy = 61.378%, Loss = 0.4803066551685333
Epoch: 3080, Batch Gradient Norm: 7.6293434928543435
Epoch: 3080, Batch Gradient Norm after: 7.6293434928543435
Epoch 3081/10000, Prediction Accuracy = 61.384%, Loss = 0.48588030934333803
Epoch: 3081, Batch Gradient Norm: 11.049206804569883
Epoch: 3081, Batch Gradient Norm after: 11.049206804569883
Epoch 3082/10000, Prediction Accuracy = 61.334%, Loss = 0.509026724100113
Epoch: 3082, Batch Gradient Norm: 11.692616444458313
Epoch: 3082, Batch Gradient Norm after: 11.692616444458313
Epoch 3083/10000, Prediction Accuracy = 61.378%, Loss = 0.5116774082183838
Epoch: 3083, Batch Gradient Norm: 12.64013958392782
Epoch: 3083, Batch Gradient Norm after: 12.64013958392782
Epoch 3084/10000, Prediction Accuracy = 61.43000000000001%, Loss = 0.5190401911735535
Epoch: 3084, Batch Gradient Norm: 12.37235324755955
Epoch: 3084, Batch Gradient Norm after: 12.37235324755955
Epoch 3085/10000, Prediction Accuracy = 61.39%, Loss = 0.517989706993103
Epoch: 3085, Batch Gradient Norm: 9.936303406285809
Epoch: 3085, Batch Gradient Norm after: 9.936303406285809
Epoch 3086/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.49820129871368407
Epoch: 3086, Batch Gradient Norm: 7.505806023217042
Epoch: 3086, Batch Gradient Norm after: 7.505806023217042
Epoch 3087/10000, Prediction Accuracy = 61.476%, Loss = 0.48477147817611693
Epoch: 3087, Batch Gradient Norm: 11.052172778255562
Epoch: 3087, Batch Gradient Norm after: 11.052172778255562
Epoch 3088/10000, Prediction Accuracy = 61.436%, Loss = 0.5048277318477631
Epoch: 3088, Batch Gradient Norm: 14.034132731096097
Epoch: 3088, Batch Gradient Norm after: 14.034132731096097
Epoch 3089/10000, Prediction Accuracy = 61.278%, Loss = 0.5333394885063172
Epoch: 3089, Batch Gradient Norm: 9.356069548527236
Epoch: 3089, Batch Gradient Norm after: 9.356069548527236
Epoch 3090/10000, Prediction Accuracy = 61.414%, Loss = 0.4961519718170166
Epoch: 3090, Batch Gradient Norm: 7.549572345865224
Epoch: 3090, Batch Gradient Norm after: 7.549572345865224
Epoch 3091/10000, Prediction Accuracy = 61.465999999999994%, Loss = 0.4860272824764252
Epoch: 3091, Batch Gradient Norm: 10.611150021219938
Epoch: 3091, Batch Gradient Norm after: 10.611150021219938
Epoch 3092/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.5009857654571533
Epoch: 3092, Batch Gradient Norm: 11.340047374354667
Epoch: 3092, Batch Gradient Norm after: 11.340047374354667
Epoch 3093/10000, Prediction Accuracy = 61.398%, Loss = 0.5080339252948761
Epoch: 3093, Batch Gradient Norm: 10.349066118624442
Epoch: 3093, Batch Gradient Norm after: 10.349066118624442
Epoch 3094/10000, Prediction Accuracy = 61.4%, Loss = 0.5031424045562745
Epoch: 3094, Batch Gradient Norm: 7.679684036002964
Epoch: 3094, Batch Gradient Norm after: 7.679684036002964
Epoch 3095/10000, Prediction Accuracy = 61.372%, Loss = 0.48766844272613524
Epoch: 3095, Batch Gradient Norm: 11.954237196785503
Epoch: 3095, Batch Gradient Norm after: 11.954237196785503
Epoch 3096/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.5132262289524079
Epoch: 3096, Batch Gradient Norm: 15.642800967593008
Epoch: 3096, Batch Gradient Norm after: 15.642800967593008
Epoch 3097/10000, Prediction Accuracy = 61.36999999999999%, Loss = 0.5452306509017945
Epoch: 3097, Batch Gradient Norm: 9.970950382245846
Epoch: 3097, Batch Gradient Norm after: 9.970950382245846
Epoch 3098/10000, Prediction Accuracy = 61.336%, Loss = 0.5006778836250305
Epoch: 3098, Batch Gradient Norm: 9.022583702311799
Epoch: 3098, Batch Gradient Norm after: 9.022583702311799
Epoch 3099/10000, Prediction Accuracy = 61.52%, Loss = 0.49496212005615237
Epoch: 3099, Batch Gradient Norm: 8.850629921930588
Epoch: 3099, Batch Gradient Norm after: 8.850629921930588
Epoch 3100/10000, Prediction Accuracy = 61.4%, Loss = 0.4914024233818054
Epoch: 3100, Batch Gradient Norm: 12.675169579311788
Epoch: 3100, Batch Gradient Norm after: 12.675169579311788
Epoch 3101/10000, Prediction Accuracy = 61.436%, Loss = 0.5156131982803345
Epoch: 3101, Batch Gradient Norm: 11.864222869523724
Epoch: 3101, Batch Gradient Norm after: 11.864222869523724
Epoch 3102/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.5159061789512634
Epoch: 3102, Batch Gradient Norm: 7.863060605084235
Epoch: 3102, Batch Gradient Norm after: 7.863060605084235
Epoch 3103/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.48707118034362795
Epoch: 3103, Batch Gradient Norm: 7.965296415047578
Epoch: 3103, Batch Gradient Norm after: 7.965296415047578
Epoch 3104/10000, Prediction Accuracy = 61.488%, Loss = 0.48523278832435607
Epoch: 3104, Batch Gradient Norm: 9.372668872098041
Epoch: 3104, Batch Gradient Norm after: 9.372668872098041
Epoch 3105/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.4979790210723877
Epoch: 3105, Batch Gradient Norm: 11.364385871109356
Epoch: 3105, Batch Gradient Norm after: 11.364385871109356
Epoch 3106/10000, Prediction Accuracy = 61.40400000000001%, Loss = 0.5080989539623261
Epoch: 3106, Batch Gradient Norm: 11.089820805092144
Epoch: 3106, Batch Gradient Norm after: 11.089820805092144
Epoch 3107/10000, Prediction Accuracy = 61.5%, Loss = 0.5056408524513245
Epoch: 3107, Batch Gradient Norm: 8.38834655368575
Epoch: 3107, Batch Gradient Norm after: 8.38834655368575
Epoch 3108/10000, Prediction Accuracy = 61.376%, Loss = 0.4899555206298828
Epoch: 3108, Batch Gradient Norm: 8.986535015384357
Epoch: 3108, Batch Gradient Norm after: 8.986535015384357
Epoch 3109/10000, Prediction Accuracy = 61.367999999999995%, Loss = 0.48935853242874144
Epoch: 3109, Batch Gradient Norm: 9.851051719536736
Epoch: 3109, Batch Gradient Norm after: 9.851051719536736
Epoch 3110/10000, Prediction Accuracy = 61.338%, Loss = 0.49724552035331726
Epoch: 3110, Batch Gradient Norm: 8.946729074506624
Epoch: 3110, Batch Gradient Norm after: 8.946729074506624
Epoch 3111/10000, Prediction Accuracy = 61.44%, Loss = 0.4910435020923615
Epoch: 3111, Batch Gradient Norm: 11.396575307494517
Epoch: 3111, Batch Gradient Norm after: 11.396575307494517
Epoch 3112/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.5053432762622834
Epoch: 3112, Batch Gradient Norm: 11.077349042757955
Epoch: 3112, Batch Gradient Norm after: 11.077349042757955
Epoch 3113/10000, Prediction Accuracy = 61.376%, Loss = 0.5033740103244781
Epoch: 3113, Batch Gradient Norm: 11.793602996091034
Epoch: 3113, Batch Gradient Norm after: 11.793602996091034
Epoch 3114/10000, Prediction Accuracy = 61.36600000000001%, Loss = 0.513019347190857
Epoch: 3114, Batch Gradient Norm: 12.245834402242638
Epoch: 3114, Batch Gradient Norm after: 12.245834402242638
Epoch 3115/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.5160590767860412
Epoch: 3115, Batch Gradient Norm: 10.780559390756101
Epoch: 3115, Batch Gradient Norm after: 10.780559390756101
Epoch 3116/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.506383192539215
Epoch: 3116, Batch Gradient Norm: 10.899092257282621
Epoch: 3116, Batch Gradient Norm after: 10.899092257282621
Epoch 3117/10000, Prediction Accuracy = 61.45400000000001%, Loss = 0.5052215278148651
Epoch: 3117, Batch Gradient Norm: 7.86271187852417
Epoch: 3117, Batch Gradient Norm after: 7.86271187852417
Epoch 3118/10000, Prediction Accuracy = 61.374%, Loss = 0.48903457522392274
Epoch: 3118, Batch Gradient Norm: 8.416341992063948
Epoch: 3118, Batch Gradient Norm after: 8.416341992063948
Epoch 3119/10000, Prediction Accuracy = 61.42%, Loss = 0.4881979525089264
Epoch: 3119, Batch Gradient Norm: 12.537845227121462
Epoch: 3119, Batch Gradient Norm after: 12.537845227121462
Epoch 3120/10000, Prediction Accuracy = 61.402%, Loss = 0.5204404473304749
Epoch: 3120, Batch Gradient Norm: 14.426290638455496
Epoch: 3120, Batch Gradient Norm after: 14.426290638455496
Epoch 3121/10000, Prediction Accuracy = 61.35600000000001%, Loss = 0.5413511157035827
Epoch: 3121, Batch Gradient Norm: 7.123917198415936
Epoch: 3121, Batch Gradient Norm after: 7.123917198415936
Epoch 3122/10000, Prediction Accuracy = 61.298%, Loss = 0.4853463590145111
Epoch: 3122, Batch Gradient Norm: 8.281248737534376
Epoch: 3122, Batch Gradient Norm after: 8.281248737534376
Epoch 3123/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.48941675424575803
Epoch: 3123, Batch Gradient Norm: 11.598007847621185
Epoch: 3123, Batch Gradient Norm after: 11.598007847621185
Epoch 3124/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.5107012093067169
Epoch: 3124, Batch Gradient Norm: 10.297400668498547
Epoch: 3124, Batch Gradient Norm after: 10.297400668498547
Epoch 3125/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.49987046122550965
Epoch: 3125, Batch Gradient Norm: 14.849747763097286
Epoch: 3125, Batch Gradient Norm after: 14.849747763097286
Epoch 3126/10000, Prediction Accuracy = 61.352%, Loss = 0.5391292810440064
Epoch: 3126, Batch Gradient Norm: 11.87639291316489
Epoch: 3126, Batch Gradient Norm after: 11.87639291316489
Epoch 3127/10000, Prediction Accuracy = 61.44199999999999%, Loss = 0.51586132645607
Epoch: 3127, Batch Gradient Norm: 10.95335776429626
Epoch: 3127, Batch Gradient Norm after: 10.95335776429626
Epoch 3128/10000, Prediction Accuracy = 61.37199999999999%, Loss = 0.5039867758750916
Epoch: 3128, Batch Gradient Norm: 10.213785609041372
Epoch: 3128, Batch Gradient Norm after: 10.213785609041372
Epoch 3129/10000, Prediction Accuracy = 61.524%, Loss = 0.49660497307777407
Epoch: 3129, Batch Gradient Norm: 9.221682525895401
Epoch: 3129, Batch Gradient Norm after: 9.221682525895401
Epoch 3130/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.48945345878601076
Epoch: 3130, Batch Gradient Norm: 8.078439547471215
Epoch: 3130, Batch Gradient Norm after: 8.078439547471215
Epoch 3131/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.4839527368545532
Epoch: 3131, Batch Gradient Norm: 8.743606102088929
Epoch: 3131, Batch Gradient Norm after: 8.743606102088929
Epoch 3132/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.48837408423423767
Epoch: 3132, Batch Gradient Norm: 9.874742934149078
Epoch: 3132, Batch Gradient Norm after: 9.874742934149078
Epoch 3133/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.49615644812583926
Epoch: 3133, Batch Gradient Norm: 12.28313788407817
Epoch: 3133, Batch Gradient Norm after: 12.28313788407817
Epoch 3134/10000, Prediction Accuracy = 61.388%, Loss = 0.5173722028732299
Epoch: 3134, Batch Gradient Norm: 11.051520699459141
Epoch: 3134, Batch Gradient Norm after: 11.051520699459141
Epoch 3135/10000, Prediction Accuracy = 61.464%, Loss = 0.5075181603431702
Epoch: 3135, Batch Gradient Norm: 10.574841576652151
Epoch: 3135, Batch Gradient Norm after: 10.574841576652151
Epoch 3136/10000, Prediction Accuracy = 61.483999999999995%, Loss = 0.5012391924858093
Epoch: 3136, Batch Gradient Norm: 13.472155339650142
Epoch: 3136, Batch Gradient Norm after: 13.472155339650142
Epoch 3137/10000, Prediction Accuracy = 61.29%, Loss = 0.5327274024486541
Epoch: 3137, Batch Gradient Norm: 9.059693642350602
Epoch: 3137, Batch Gradient Norm after: 9.059693642350602
Epoch 3138/10000, Prediction Accuracy = 61.402%, Loss = 0.4924413084983826
Epoch: 3138, Batch Gradient Norm: 8.033546554245017
Epoch: 3138, Batch Gradient Norm after: 8.033546554245017
Epoch 3139/10000, Prediction Accuracy = 61.464%, Loss = 0.48909543752670287
Epoch: 3139, Batch Gradient Norm: 7.397517835250883
Epoch: 3139, Batch Gradient Norm after: 7.397517835250883
Epoch 3140/10000, Prediction Accuracy = 61.524%, Loss = 0.48127172589302064
Epoch: 3140, Batch Gradient Norm: 11.14376519789391
Epoch: 3140, Batch Gradient Norm after: 11.14376519789391
Epoch 3141/10000, Prediction Accuracy = 61.416%, Loss = 0.5048776984214782
Epoch: 3141, Batch Gradient Norm: 12.53395524698742
Epoch: 3141, Batch Gradient Norm after: 12.53395524698742
Epoch 3142/10000, Prediction Accuracy = 61.48%, Loss = 0.5178876757621765
Epoch: 3142, Batch Gradient Norm: 10.645416496025865
Epoch: 3142, Batch Gradient Norm after: 10.645416496025865
Epoch 3143/10000, Prediction Accuracy = 61.352%, Loss = 0.5026208877563476
Epoch: 3143, Batch Gradient Norm: 10.321075476621402
Epoch: 3143, Batch Gradient Norm after: 10.321075476621402
Epoch 3144/10000, Prediction Accuracy = 61.428%, Loss = 0.499594384431839
Epoch: 3144, Batch Gradient Norm: 8.717810728152717
Epoch: 3144, Batch Gradient Norm after: 8.717810728152717
Epoch 3145/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4894953668117523
Epoch: 3145, Batch Gradient Norm: 9.90376802500645
Epoch: 3145, Batch Gradient Norm after: 9.90376802500645
Epoch 3146/10000, Prediction Accuracy = 61.55%, Loss = 0.4967478811740875
Epoch: 3146, Batch Gradient Norm: 10.300411415410956
Epoch: 3146, Batch Gradient Norm after: 10.300411415410956
Epoch 3147/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.4985924601554871
Epoch: 3147, Batch Gradient Norm: 10.140611403815122
Epoch: 3147, Batch Gradient Norm after: 10.140611403815122
Epoch 3148/10000, Prediction Accuracy = 61.286%, Loss = 0.5023818194866181
Epoch: 3148, Batch Gradient Norm: 10.850877209764612
Epoch: 3148, Batch Gradient Norm after: 10.850877209764612
Epoch 3149/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.5001680970191955
Epoch: 3149, Batch Gradient Norm: 12.788459117779524
Epoch: 3149, Batch Gradient Norm after: 12.788459117779524
Epoch 3150/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.5142803192138672
Epoch: 3150, Batch Gradient Norm: 10.914201376962179
Epoch: 3150, Batch Gradient Norm after: 10.914201376962179
Epoch 3151/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5006236791610718
Epoch: 3151, Batch Gradient Norm: 10.835754030770397
Epoch: 3151, Batch Gradient Norm after: 10.835754030770397
Epoch 3152/10000, Prediction Accuracy = 61.452%, Loss = 0.5020966947078704
Epoch: 3152, Batch Gradient Norm: 11.442365927857038
Epoch: 3152, Batch Gradient Norm after: 11.442365927857038
Epoch 3153/10000, Prediction Accuracy = 61.4%, Loss = 0.5070174038410187
Epoch: 3153, Batch Gradient Norm: 11.112291301697187
Epoch: 3153, Batch Gradient Norm after: 11.112291301697187
Epoch 3154/10000, Prediction Accuracy = 61.324%, Loss = 0.5068531513214112
Epoch: 3154, Batch Gradient Norm: 12.088312059508885
Epoch: 3154, Batch Gradient Norm after: 12.088312059508885
Epoch 3155/10000, Prediction Accuracy = 61.46%, Loss = 0.5130402863025665
Epoch: 3155, Batch Gradient Norm: 10.512144532002244
Epoch: 3155, Batch Gradient Norm after: 10.512144532002244
Epoch 3156/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.5004739761352539
Epoch: 3156, Batch Gradient Norm: 6.808023461743595
Epoch: 3156, Batch Gradient Norm after: 6.808023461743595
Epoch 3157/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4800018846988678
Epoch: 3157, Batch Gradient Norm: 8.795499657331444
Epoch: 3157, Batch Gradient Norm after: 8.795499657331444
Epoch 3158/10000, Prediction Accuracy = 61.472%, Loss = 0.49406052827835084
Epoch: 3158, Batch Gradient Norm: 13.81829892812133
Epoch: 3158, Batch Gradient Norm after: 13.81829892812133
Epoch 3159/10000, Prediction Accuracy = 61.33399999999999%, Loss = 0.536796224117279
Epoch: 3159, Batch Gradient Norm: 10.280131219449062
Epoch: 3159, Batch Gradient Norm after: 10.280131219449062
Epoch 3160/10000, Prediction Accuracy = 61.44199999999999%, Loss = 0.4974938452243805
Epoch: 3160, Batch Gradient Norm: 9.26562049239624
Epoch: 3160, Batch Gradient Norm after: 9.26562049239624
Epoch 3161/10000, Prediction Accuracy = 61.438%, Loss = 0.4937326192855835
Epoch: 3161, Batch Gradient Norm: 9.436512435228373
Epoch: 3161, Batch Gradient Norm after: 9.436512435228373
Epoch 3162/10000, Prediction Accuracy = 61.384%, Loss = 0.4941785573959351
Epoch: 3162, Batch Gradient Norm: 8.314248354035623
Epoch: 3162, Batch Gradient Norm after: 8.314248354035623
Epoch 3163/10000, Prediction Accuracy = 61.428%, Loss = 0.48613356947898867
Epoch: 3163, Batch Gradient Norm: 9.914172492658013
Epoch: 3163, Batch Gradient Norm after: 9.914172492658013
Epoch 3164/10000, Prediction Accuracy = 61.552%, Loss = 0.49166104197502136
Epoch: 3164, Batch Gradient Norm: 11.077592339904486
Epoch: 3164, Batch Gradient Norm after: 11.077592339904486
Epoch 3165/10000, Prediction Accuracy = 61.44%, Loss = 0.5014928042888641
Epoch: 3165, Batch Gradient Norm: 10.442288548125186
Epoch: 3165, Batch Gradient Norm after: 10.442288548125186
Epoch 3166/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.49806236028671264
Epoch: 3166, Batch Gradient Norm: 9.450871629148065
Epoch: 3166, Batch Gradient Norm after: 9.450871629148065
Epoch 3167/10000, Prediction Accuracy = 61.436%, Loss = 0.49429147839546206
Epoch: 3167, Batch Gradient Norm: 12.366017095556547
Epoch: 3167, Batch Gradient Norm after: 12.366017095556547
Epoch 3168/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5182394742965698
Epoch: 3168, Batch Gradient Norm: 8.309005063473576
Epoch: 3168, Batch Gradient Norm after: 8.309005063473576
Epoch 3169/10000, Prediction Accuracy = 61.443999999999996%, Loss = 0.48628238439559934
Epoch: 3169, Batch Gradient Norm: 9.062527114733772
Epoch: 3169, Batch Gradient Norm after: 9.062527114733772
Epoch 3170/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.4919241428375244
Epoch: 3170, Batch Gradient Norm: 11.030712301148585
Epoch: 3170, Batch Gradient Norm after: 11.030712301148585
Epoch 3171/10000, Prediction Accuracy = 61.444%, Loss = 0.5072785198688508
Epoch: 3171, Batch Gradient Norm: 11.905607543298672
Epoch: 3171, Batch Gradient Norm after: 11.905607543298672
Epoch 3172/10000, Prediction Accuracy = 61.396%, Loss = 0.5070502698421478
Epoch: 3172, Batch Gradient Norm: 12.321617639727013
Epoch: 3172, Batch Gradient Norm after: 12.321617639727013
Epoch 3173/10000, Prediction Accuracy = 61.462%, Loss = 0.5111385464668274
Epoch: 3173, Batch Gradient Norm: 9.604737043178794
Epoch: 3173, Batch Gradient Norm after: 9.604737043178794
Epoch 3174/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.4911902487277985
Epoch: 3174, Batch Gradient Norm: 10.167236173010087
Epoch: 3174, Batch Gradient Norm after: 10.167236173010087
Epoch 3175/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4940232515335083
Epoch: 3175, Batch Gradient Norm: 13.047664343687858
Epoch: 3175, Batch Gradient Norm after: 13.047664343687858
Epoch 3176/10000, Prediction Accuracy = 61.471999999999994%, Loss = 0.5171765327453614
Epoch: 3176, Batch Gradient Norm: 8.450340379094817
Epoch: 3176, Batch Gradient Norm after: 8.450340379094817
Epoch 3177/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4895790219306946
Epoch: 3177, Batch Gradient Norm: 11.09326370369337
Epoch: 3177, Batch Gradient Norm after: 11.09326370369337
Epoch 3178/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.5077953100204468
Epoch: 3178, Batch Gradient Norm: 11.238863530401392
Epoch: 3178, Batch Gradient Norm after: 11.238863530401392
Epoch 3179/10000, Prediction Accuracy = 61.44%, Loss = 0.5051068902015686
Epoch: 3179, Batch Gradient Norm: 8.528918070105806
Epoch: 3179, Batch Gradient Norm after: 8.528918070105806
Epoch 3180/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.4875741720199585
Epoch: 3180, Batch Gradient Norm: 10.276989791288534
Epoch: 3180, Batch Gradient Norm after: 10.276989791288534
Epoch 3181/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.4950560688972473
Epoch: 3181, Batch Gradient Norm: 12.06930313314277
Epoch: 3181, Batch Gradient Norm after: 12.06930313314277
Epoch 3182/10000, Prediction Accuracy = 61.408%, Loss = 0.5111957788467407
Epoch: 3182, Batch Gradient Norm: 10.594640295810255
Epoch: 3182, Batch Gradient Norm after: 10.594640295810255
Epoch 3183/10000, Prediction Accuracy = 61.40999999999999%, Loss = 0.49915788769721986
Epoch: 3183, Batch Gradient Norm: 10.580670346772134
Epoch: 3183, Batch Gradient Norm after: 10.580670346772134
Epoch 3184/10000, Prediction Accuracy = 61.462%, Loss = 0.5005221724510193
Epoch: 3184, Batch Gradient Norm: 12.65761088320259
Epoch: 3184, Batch Gradient Norm after: 12.65761088320259
Epoch 3185/10000, Prediction Accuracy = 61.372%, Loss = 0.516266006231308
Epoch: 3185, Batch Gradient Norm: 10.469485791793904
Epoch: 3185, Batch Gradient Norm after: 10.469485791793904
Epoch 3186/10000, Prediction Accuracy = 61.553999999999995%, Loss = 0.5008511900901794
Epoch: 3186, Batch Gradient Norm: 8.253316956724056
Epoch: 3186, Batch Gradient Norm after: 8.253316956724056
Epoch 3187/10000, Prediction Accuracy = 61.34000000000001%, Loss = 0.48703858256340027
Epoch: 3187, Batch Gradient Norm: 9.04585859874364
Epoch: 3187, Batch Gradient Norm after: 9.04585859874364
Epoch 3188/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.4885382533073425
Epoch: 3188, Batch Gradient Norm: 12.606194231656602
Epoch: 3188, Batch Gradient Norm after: 12.606194231656602
Epoch 3189/10000, Prediction Accuracy = 61.56%, Loss = 0.5130042433738708
Epoch: 3189, Batch Gradient Norm: 10.322885438539723
Epoch: 3189, Batch Gradient Norm after: 10.322885438539723
Epoch 3190/10000, Prediction Accuracy = 61.398%, Loss = 0.49769986867904664
Epoch: 3190, Batch Gradient Norm: 12.404671626927774
Epoch: 3190, Batch Gradient Norm after: 12.404671626927774
Epoch 3191/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.5134201288223267
Epoch: 3191, Batch Gradient Norm: 10.867018975028083
Epoch: 3191, Batch Gradient Norm after: 10.867018975028083
Epoch 3192/10000, Prediction Accuracy = 61.324%, Loss = 0.5024817943572998
Epoch: 3192, Batch Gradient Norm: 9.505570141415014
Epoch: 3192, Batch Gradient Norm after: 9.505570141415014
Epoch 3193/10000, Prediction Accuracy = 61.525999999999996%, Loss = 0.496747350692749
Epoch: 3193, Batch Gradient Norm: 7.894711295534105
Epoch: 3193, Batch Gradient Norm after: 7.894711295534105
Epoch 3194/10000, Prediction Accuracy = 61.524%, Loss = 0.4827994704246521
Epoch: 3194, Batch Gradient Norm: 6.895018529834296
Epoch: 3194, Batch Gradient Norm after: 6.895018529834296
Epoch 3195/10000, Prediction Accuracy = 61.54%, Loss = 0.4774398744106293
Epoch: 3195, Batch Gradient Norm: 9.976719143681374
Epoch: 3195, Batch Gradient Norm after: 9.976719143681374
Epoch 3196/10000, Prediction Accuracy = 61.517999999999994%, Loss = 0.4953646719455719
Epoch: 3196, Batch Gradient Norm: 10.920409402367142
Epoch: 3196, Batch Gradient Norm after: 10.920409402367142
Epoch 3197/10000, Prediction Accuracy = 61.436%, Loss = 0.5003689408302308
Epoch: 3197, Batch Gradient Norm: 11.249053952092309
Epoch: 3197, Batch Gradient Norm after: 11.249053952092309
Epoch 3198/10000, Prediction Accuracy = 61.538%, Loss = 0.5021891355514526
Epoch: 3198, Batch Gradient Norm: 9.212549403272783
Epoch: 3198, Batch Gradient Norm after: 9.212549403272783
Epoch 3199/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.48907132148742677
Epoch: 3199, Batch Gradient Norm: 9.792906469971125
Epoch: 3199, Batch Gradient Norm after: 9.792906469971125
Epoch 3200/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.4965112328529358
Epoch: 3200, Batch Gradient Norm: 9.419853321831468
Epoch: 3200, Batch Gradient Norm after: 9.419853321831468
Epoch 3201/10000, Prediction Accuracy = 61.496%, Loss = 0.4921013653278351
Epoch: 3201, Batch Gradient Norm: 6.759119809479455
Epoch: 3201, Batch Gradient Norm after: 6.759119809479455
Epoch 3202/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.4778430342674255
Epoch: 3202, Batch Gradient Norm: 9.394151728868396
Epoch: 3202, Batch Gradient Norm after: 9.394151728868396
Epoch 3203/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4946812868118286
Epoch: 3203, Batch Gradient Norm: 15.004398040832031
Epoch: 3203, Batch Gradient Norm after: 15.004398040832031
Epoch 3204/10000, Prediction Accuracy = 61.38199999999999%, Loss = 0.5383346438407898
Epoch: 3204, Batch Gradient Norm: 11.16892097892854
Epoch: 3204, Batch Gradient Norm after: 11.16892097892854
Epoch 3205/10000, Prediction Accuracy = 61.492%, Loss = 0.5034412622451783
Epoch: 3205, Batch Gradient Norm: 9.364127449064123
Epoch: 3205, Batch Gradient Norm after: 9.364127449064123
Epoch 3206/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.4901584625244141
Epoch: 3206, Batch Gradient Norm: 13.798461414942532
Epoch: 3206, Batch Gradient Norm after: 13.798461414942532
Epoch 3207/10000, Prediction Accuracy = 61.432%, Loss = 0.520121943950653
Epoch: 3207, Batch Gradient Norm: 12.936711379385084
Epoch: 3207, Batch Gradient Norm after: 12.936711379385084
Epoch 3208/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.5195151686668396
Epoch: 3208, Batch Gradient Norm: 9.288660903017337
Epoch: 3208, Batch Gradient Norm after: 9.288660903017337
Epoch 3209/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.49248905181884767
Epoch: 3209, Batch Gradient Norm: 9.545504681921486
Epoch: 3209, Batch Gradient Norm after: 9.545504681921486
Epoch 3210/10000, Prediction Accuracy = 61.488%, Loss = 0.49331743121147154
Epoch: 3210, Batch Gradient Norm: 8.745824620065909
Epoch: 3210, Batch Gradient Norm after: 8.745824620065909
Epoch 3211/10000, Prediction Accuracy = 61.44199999999999%, Loss = 0.4882754981517792
Epoch: 3211, Batch Gradient Norm: 9.966032303160185
Epoch: 3211, Batch Gradient Norm after: 9.966032303160185
Epoch 3212/10000, Prediction Accuracy = 61.496%, Loss = 0.49666746854782107
Epoch: 3212, Batch Gradient Norm: 7.754440946943018
Epoch: 3212, Batch Gradient Norm after: 7.754440946943018
Epoch 3213/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.4825414001941681
Epoch: 3213, Batch Gradient Norm: 9.222012460112486
Epoch: 3213, Batch Gradient Norm after: 9.222012460112486
Epoch 3214/10000, Prediction Accuracy = 61.59799999999999%, Loss = 0.4913053631782532
Epoch: 3214, Batch Gradient Norm: 14.468422738293388
Epoch: 3214, Batch Gradient Norm after: 14.468422738293388
Epoch 3215/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.5321433067321777
Epoch: 3215, Batch Gradient Norm: 13.021935863339063
Epoch: 3215, Batch Gradient Norm after: 13.021935863339063
Epoch 3216/10000, Prediction Accuracy = 61.428%, Loss = 0.5247725307941437
Epoch: 3216, Batch Gradient Norm: 7.96805335772739
Epoch: 3216, Batch Gradient Norm after: 7.96805335772739
Epoch 3217/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4832130491733551
Epoch: 3217, Batch Gradient Norm: 9.208706024355424
Epoch: 3217, Batch Gradient Norm after: 9.208706024355424
Epoch 3218/10000, Prediction Accuracy = 61.424%, Loss = 0.49199668765068055
Epoch: 3218, Batch Gradient Norm: 13.370454594972603
Epoch: 3218, Batch Gradient Norm after: 13.370454594972603
Epoch 3219/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.5247617602348328
Epoch: 3219, Batch Gradient Norm: 11.92068972074403
Epoch: 3219, Batch Gradient Norm after: 11.92068972074403
Epoch 3220/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.5112573504447937
Epoch: 3220, Batch Gradient Norm: 10.145338224985998
Epoch: 3220, Batch Gradient Norm after: 10.145338224985998
Epoch 3221/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.5045606613159179
Epoch: 3221, Batch Gradient Norm: 10.499263821196259
Epoch: 3221, Batch Gradient Norm after: 10.499263821196259
Epoch 3222/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.4989859342575073
Epoch: 3222, Batch Gradient Norm: 13.419736935596152
Epoch: 3222, Batch Gradient Norm after: 13.419736935596152
Epoch 3223/10000, Prediction Accuracy = 61.553999999999995%, Loss = 0.517518949508667
Epoch: 3223, Batch Gradient Norm: 10.619231998524473
Epoch: 3223, Batch Gradient Norm after: 10.619231998524473
Epoch 3224/10000, Prediction Accuracy = 61.519999999999996%, Loss = 0.49701091051101687
Epoch: 3224, Batch Gradient Norm: 8.079795637993318
Epoch: 3224, Batch Gradient Norm after: 8.079795637993318
Epoch 3225/10000, Prediction Accuracy = 61.528%, Loss = 0.48270148038864136
Epoch: 3225, Batch Gradient Norm: 9.069903394610371
Epoch: 3225, Batch Gradient Norm after: 9.069903394610371
Epoch 3226/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4871364176273346
Epoch: 3226, Batch Gradient Norm: 9.970988415480505
Epoch: 3226, Batch Gradient Norm after: 9.970988415480505
Epoch 3227/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.49353745579719543
Epoch: 3227, Batch Gradient Norm: 8.352712849661676
Epoch: 3227, Batch Gradient Norm after: 8.352712849661676
Epoch 3228/10000, Prediction Accuracy = 61.536%, Loss = 0.4828596353530884
Epoch: 3228, Batch Gradient Norm: 11.290177690890223
Epoch: 3228, Batch Gradient Norm after: 11.290177690890223
Epoch 3229/10000, Prediction Accuracy = 61.402%, Loss = 0.5004347443580628
Epoch: 3229, Batch Gradient Norm: 11.398696457660915
Epoch: 3229, Batch Gradient Norm after: 11.398696457660915
Epoch 3230/10000, Prediction Accuracy = 61.43000000000001%, Loss = 0.5015679478645325
Epoch: 3230, Batch Gradient Norm: 10.274708577821306
Epoch: 3230, Batch Gradient Norm after: 10.274708577821306
Epoch 3231/10000, Prediction Accuracy = 61.508%, Loss = 0.49853175282478335
Epoch: 3231, Batch Gradient Norm: 7.271157292733514
Epoch: 3231, Batch Gradient Norm after: 7.271157292733514
Epoch 3232/10000, Prediction Accuracy = 61.57600000000001%, Loss = 0.47810479402542116
Epoch: 3232, Batch Gradient Norm: 7.297477470225668
Epoch: 3232, Batch Gradient Norm after: 7.297477470225668
Epoch 3233/10000, Prediction Accuracy = 61.52%, Loss = 0.47712820768356323
Epoch: 3233, Batch Gradient Norm: 10.218833053848064
Epoch: 3233, Batch Gradient Norm after: 10.218833053848064
Epoch 3234/10000, Prediction Accuracy = 61.552%, Loss = 0.49225724339485166
Epoch: 3234, Batch Gradient Norm: 11.962279491758524
Epoch: 3234, Batch Gradient Norm after: 11.962279491758524
Epoch 3235/10000, Prediction Accuracy = 61.395999999999994%, Loss = 0.5071340441703797
Epoch: 3235, Batch Gradient Norm: 12.358197044476677
Epoch: 3235, Batch Gradient Norm after: 12.358197044476677
Epoch 3236/10000, Prediction Accuracy = 61.419999999999995%, Loss = 0.5167582213878632
Epoch: 3236, Batch Gradient Norm: 9.215330258099232
Epoch: 3236, Batch Gradient Norm after: 9.215330258099232
Epoch 3237/10000, Prediction Accuracy = 61.39%, Loss = 0.48888867497444155
Epoch: 3237, Batch Gradient Norm: 12.66937049341746
Epoch: 3237, Batch Gradient Norm after: 12.66937049341746
Epoch 3238/10000, Prediction Accuracy = 61.42%, Loss = 0.5176644206047059
Epoch: 3238, Batch Gradient Norm: 8.189218820048739
Epoch: 3238, Batch Gradient Norm after: 8.189218820048739
Epoch 3239/10000, Prediction Accuracy = 61.58%, Loss = 0.4866655170917511
Epoch: 3239, Batch Gradient Norm: 7.044309245268056
Epoch: 3239, Batch Gradient Norm after: 7.044309245268056
Epoch 3240/10000, Prediction Accuracy = 61.496%, Loss = 0.47768386006355285
Epoch: 3240, Batch Gradient Norm: 9.893490431036915
Epoch: 3240, Batch Gradient Norm after: 9.893490431036915
Epoch 3241/10000, Prediction Accuracy = 61.426%, Loss = 0.4936951756477356
Epoch: 3241, Batch Gradient Norm: 11.096211344008895
Epoch: 3241, Batch Gradient Norm after: 11.096211344008895
Epoch 3242/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.5041227996349334
Epoch: 3242, Batch Gradient Norm: 11.809306972095943
Epoch: 3242, Batch Gradient Norm after: 11.809306972095943
Epoch 3243/10000, Prediction Accuracy = 61.47800000000001%, Loss = 0.5052829265594483
Epoch: 3243, Batch Gradient Norm: 9.790617818599202
Epoch: 3243, Batch Gradient Norm after: 9.790617818599202
Epoch 3244/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.4901777863502502
Epoch: 3244, Batch Gradient Norm: 11.475963337015358
Epoch: 3244, Batch Gradient Norm after: 11.475963337015358
Epoch 3245/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.5003364145755768
Epoch: 3245, Batch Gradient Norm: 12.386709258169672
Epoch: 3245, Batch Gradient Norm after: 12.386709258169672
Epoch 3246/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.5088578462600708
Epoch: 3246, Batch Gradient Norm: 8.999716376221874
Epoch: 3246, Batch Gradient Norm after: 8.999716376221874
Epoch 3247/10000, Prediction Accuracy = 61.448%, Loss = 0.4866785705089569
Epoch: 3247, Batch Gradient Norm: 11.97986709134889
Epoch: 3247, Batch Gradient Norm after: 11.97986709134889
Epoch 3248/10000, Prediction Accuracy = 61.44599999999999%, Loss = 0.5136892795562744
Epoch: 3248, Batch Gradient Norm: 9.196558589943207
Epoch: 3248, Batch Gradient Norm after: 9.196558589943207
Epoch 3249/10000, Prediction Accuracy = 61.516%, Loss = 0.4918162524700165
Epoch: 3249, Batch Gradient Norm: 9.779979419715268
Epoch: 3249, Batch Gradient Norm after: 9.779979419715268
Epoch 3250/10000, Prediction Accuracy = 61.462%, Loss = 0.49382981061935427
Epoch: 3250, Batch Gradient Norm: 9.506533654164592
Epoch: 3250, Batch Gradient Norm after: 9.506533654164592
Epoch 3251/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.4894322156906128
Epoch: 3251, Batch Gradient Norm: 11.173340667635072
Epoch: 3251, Batch Gradient Norm after: 11.173340667635072
Epoch 3252/10000, Prediction Accuracy = 61.44%, Loss = 0.5020521461963654
Epoch: 3252, Batch Gradient Norm: 11.136204105962722
Epoch: 3252, Batch Gradient Norm after: 11.136204105962722
Epoch 3253/10000, Prediction Accuracy = 61.584%, Loss = 0.5008829534053802
Epoch: 3253, Batch Gradient Norm: 11.086806038544042
Epoch: 3253, Batch Gradient Norm after: 11.086806038544042
Epoch 3254/10000, Prediction Accuracy = 61.472%, Loss = 0.5010738909244538
Epoch: 3254, Batch Gradient Norm: 10.953461172599058
Epoch: 3254, Batch Gradient Norm after: 10.953461172599058
Epoch 3255/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.5005247354507446
Epoch: 3255, Batch Gradient Norm: 10.057670969712115
Epoch: 3255, Batch Gradient Norm after: 10.057670969712115
Epoch 3256/10000, Prediction Accuracy = 61.507999999999996%, Loss = 0.4906855165958405
Epoch: 3256, Batch Gradient Norm: 10.796999316584236
Epoch: 3256, Batch Gradient Norm after: 10.796999316584236
Epoch 3257/10000, Prediction Accuracy = 61.53599999999999%, Loss = 0.5000274002552032
Epoch: 3257, Batch Gradient Norm: 11.081925394927074
Epoch: 3257, Batch Gradient Norm after: 11.081925394927074
Epoch 3258/10000, Prediction Accuracy = 61.416%, Loss = 0.5039647340774536
Epoch: 3258, Batch Gradient Norm: 11.57972441237099
Epoch: 3258, Batch Gradient Norm after: 11.57972441237099
Epoch 3259/10000, Prediction Accuracy = 61.532000000000004%, Loss = 0.5051968514919281
Epoch: 3259, Batch Gradient Norm: 10.249750812349744
Epoch: 3259, Batch Gradient Norm after: 10.249750812349744
Epoch 3260/10000, Prediction Accuracy = 61.464%, Loss = 0.49430933594703674
Epoch: 3260, Batch Gradient Norm: 7.816278525387593
Epoch: 3260, Batch Gradient Norm after: 7.816278525387593
Epoch 3261/10000, Prediction Accuracy = 61.51800000000001%, Loss = 0.47904874086380006
Epoch: 3261, Batch Gradient Norm: 7.4477208784081705
Epoch: 3261, Batch Gradient Norm after: 7.4477208784081705
Epoch 3262/10000, Prediction Accuracy = 61.598%, Loss = 0.47673373818397524
Epoch: 3262, Batch Gradient Norm: 10.310244268419233
Epoch: 3262, Batch Gradient Norm after: 10.310244268419233
Epoch 3263/10000, Prediction Accuracy = 61.44200000000001%, Loss = 0.4924998879432678
Epoch: 3263, Batch Gradient Norm: 12.089947977519675
Epoch: 3263, Batch Gradient Norm after: 12.089947977519675
Epoch 3264/10000, Prediction Accuracy = 61.504%, Loss = 0.5038617849349976
Epoch: 3264, Batch Gradient Norm: 12.268313547281654
Epoch: 3264, Batch Gradient Norm after: 12.268313547281654
Epoch 3265/10000, Prediction Accuracy = 61.362%, Loss = 0.5043210744857788
Epoch: 3265, Batch Gradient Norm: 13.4263415303683
Epoch: 3265, Batch Gradient Norm after: 13.4263415303683
Epoch 3266/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.517027747631073
Epoch: 3266, Batch Gradient Norm: 11.206611361037465
Epoch: 3266, Batch Gradient Norm after: 11.206611361037465
Epoch 3267/10000, Prediction Accuracy = 61.476%, Loss = 0.5015236437320709
Epoch: 3267, Batch Gradient Norm: 11.272542257698145
Epoch: 3267, Batch Gradient Norm after: 11.272542257698145
Epoch 3268/10000, Prediction Accuracy = 61.636%, Loss = 0.49974218010902405
Epoch: 3268, Batch Gradient Norm: 11.37156643277066
Epoch: 3268, Batch Gradient Norm after: 11.37156643277066
Epoch 3269/10000, Prediction Accuracy = 61.55400000000001%, Loss = 0.5057625532150268
Epoch: 3269, Batch Gradient Norm: 10.616216929611484
Epoch: 3269, Batch Gradient Norm after: 10.616216929611484
Epoch 3270/10000, Prediction Accuracy = 61.4%, Loss = 0.49792001247406004
Epoch: 3270, Batch Gradient Norm: 9.12417946117928
Epoch: 3270, Batch Gradient Norm after: 9.12417946117928
Epoch 3271/10000, Prediction Accuracy = 61.586%, Loss = 0.486432147026062
Epoch: 3271, Batch Gradient Norm: 10.604429139178967
Epoch: 3271, Batch Gradient Norm after: 10.604429139178967
Epoch 3272/10000, Prediction Accuracy = 61.468%, Loss = 0.49581736922264097
Epoch: 3272, Batch Gradient Norm: 9.422309000135222
Epoch: 3272, Batch Gradient Norm after: 9.422309000135222
Epoch 3273/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.4885998427867889
Epoch: 3273, Batch Gradient Norm: 10.673230278170205
Epoch: 3273, Batch Gradient Norm after: 10.673230278170205
Epoch 3274/10000, Prediction Accuracy = 61.53399999999999%, Loss = 0.4968135952949524
Epoch: 3274, Batch Gradient Norm: 9.032027177002849
Epoch: 3274, Batch Gradient Norm after: 9.032027177002849
Epoch 3275/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.484948867559433
Epoch: 3275, Batch Gradient Norm: 6.619296477962388
Epoch: 3275, Batch Gradient Norm after: 6.619296477962388
Epoch 3276/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4740419745445251
Epoch: 3276, Batch Gradient Norm: 8.31375077490716
Epoch: 3276, Batch Gradient Norm after: 8.31375077490716
Epoch 3277/10000, Prediction Accuracy = 61.42%, Loss = 0.48174052834510805
Epoch: 3277, Batch Gradient Norm: 10.25367754392704
Epoch: 3277, Batch Gradient Norm after: 10.25367754392704
Epoch 3278/10000, Prediction Accuracy = 61.59599999999999%, Loss = 0.49647327661514284
Epoch: 3278, Batch Gradient Norm: 12.406377946026126
Epoch: 3278, Batch Gradient Norm after: 12.406377946026126
Epoch 3279/10000, Prediction Accuracy = 61.532%, Loss = 0.5087652742862702
Epoch: 3279, Batch Gradient Norm: 12.589870372605029
Epoch: 3279, Batch Gradient Norm after: 12.589870372605029
Epoch 3280/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5133814632892608
Epoch: 3280, Batch Gradient Norm: 6.661461100035931
Epoch: 3280, Batch Gradient Norm after: 6.661461100035931
Epoch 3281/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.4752602279186249
Epoch: 3281, Batch Gradient Norm: 8.802251620359627
Epoch: 3281, Batch Gradient Norm after: 8.802251620359627
Epoch 3282/10000, Prediction Accuracy = 61.512%, Loss = 0.4865467190742493
Epoch: 3282, Batch Gradient Norm: 8.818200602374727
Epoch: 3282, Batch Gradient Norm after: 8.818200602374727
Epoch 3283/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.4858534336090088
Epoch: 3283, Batch Gradient Norm: 10.944344399968266
Epoch: 3283, Batch Gradient Norm after: 10.944344399968266
Epoch 3284/10000, Prediction Accuracy = 61.398%, Loss = 0.5042969703674316
Epoch: 3284, Batch Gradient Norm: 10.59172560169409
Epoch: 3284, Batch Gradient Norm after: 10.59172560169409
Epoch 3285/10000, Prediction Accuracy = 61.372%, Loss = 0.49586459398269656
Epoch: 3285, Batch Gradient Norm: 12.050579384979374
Epoch: 3285, Batch Gradient Norm after: 12.050579384979374
Epoch 3286/10000, Prediction Accuracy = 61.46%, Loss = 0.5083541810512543
Epoch: 3286, Batch Gradient Norm: 12.366576203799399
Epoch: 3286, Batch Gradient Norm after: 12.366576203799399
Epoch 3287/10000, Prediction Accuracy = 61.58%, Loss = 0.5077282547950744
Epoch: 3287, Batch Gradient Norm: 9.736697659979642
Epoch: 3287, Batch Gradient Norm after: 9.736697659979642
Epoch 3288/10000, Prediction Accuracy = 61.536%, Loss = 0.48882445096969607
Epoch: 3288, Batch Gradient Norm: 9.727448496067192
Epoch: 3288, Batch Gradient Norm after: 9.727448496067192
Epoch 3289/10000, Prediction Accuracy = 61.556%, Loss = 0.4920714318752289
Epoch: 3289, Batch Gradient Norm: 10.283022444918057
Epoch: 3289, Batch Gradient Norm after: 10.283022444918057
Epoch 3290/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.49435692429542544
Epoch: 3290, Batch Gradient Norm: 10.816073309908719
Epoch: 3290, Batch Gradient Norm after: 10.816073309908719
Epoch 3291/10000, Prediction Accuracy = 61.517999999999994%, Loss = 0.4976546406745911
Epoch: 3291, Batch Gradient Norm: 8.580140912609473
Epoch: 3291, Batch Gradient Norm after: 8.580140912609473
Epoch 3292/10000, Prediction Accuracy = 61.428%, Loss = 0.4826969087123871
Epoch: 3292, Batch Gradient Norm: 13.344885441551838
Epoch: 3292, Batch Gradient Norm after: 13.344885441551838
Epoch 3293/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.5184663474559784
Epoch: 3293, Batch Gradient Norm: 11.448502436353904
Epoch: 3293, Batch Gradient Norm after: 11.448502436353904
Epoch 3294/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.5023654997348785
Epoch: 3294, Batch Gradient Norm: 12.688769759176493
Epoch: 3294, Batch Gradient Norm after: 12.688769759176493
Epoch 3295/10000, Prediction Accuracy = 61.386%, Loss = 0.5149024605751038
Epoch: 3295, Batch Gradient Norm: 10.0877796061105
Epoch: 3295, Batch Gradient Norm after: 10.0877796061105
Epoch 3296/10000, Prediction Accuracy = 61.58800000000001%, Loss = 0.4960017204284668
Epoch: 3296, Batch Gradient Norm: 9.016699711623819
Epoch: 3296, Batch Gradient Norm after: 9.016699711623819
Epoch 3297/10000, Prediction Accuracy = 61.54%, Loss = 0.48980056643486025
Epoch: 3297, Batch Gradient Norm: 10.052783512814734
Epoch: 3297, Batch Gradient Norm after: 10.052783512814734
Epoch 3298/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.49060619473457334
Epoch: 3298, Batch Gradient Norm: 9.456143389451096
Epoch: 3298, Batch Gradient Norm after: 9.456143389451096
Epoch 3299/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.4894735336303711
Epoch: 3299, Batch Gradient Norm: 10.628253737854005
Epoch: 3299, Batch Gradient Norm after: 10.628253737854005
Epoch 3300/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.5015634834766388
Epoch: 3300, Batch Gradient Norm: 7.76199457139861
Epoch: 3300, Batch Gradient Norm after: 7.76199457139861
Epoch 3301/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.47922438383102417
Epoch: 3301, Batch Gradient Norm: 7.668983846651554
Epoch: 3301, Batch Gradient Norm after: 7.668983846651554
Epoch 3302/10000, Prediction Accuracy = 61.612%, Loss = 0.477768737077713
Epoch: 3302, Batch Gradient Norm: 12.22007594798708
Epoch: 3302, Batch Gradient Norm after: 12.22007594798708
Epoch 3303/10000, Prediction Accuracy = 61.592%, Loss = 0.5095055103302002
Epoch: 3303, Batch Gradient Norm: 13.429050002178396
Epoch: 3303, Batch Gradient Norm after: 13.429050002178396
Epoch 3304/10000, Prediction Accuracy = 61.60600000000001%, Loss = 0.5159424126148224
Epoch: 3304, Batch Gradient Norm: 11.414698747805756
Epoch: 3304, Batch Gradient Norm after: 11.414698747805756
Epoch 3305/10000, Prediction Accuracy = 61.39%, Loss = 0.5032388865947723
Epoch: 3305, Batch Gradient Norm: 7.6475422071535855
Epoch: 3305, Batch Gradient Norm after: 7.6475422071535855
Epoch 3306/10000, Prediction Accuracy = 61.548%, Loss = 0.4773576021194458
Epoch: 3306, Batch Gradient Norm: 8.298992585740287
Epoch: 3306, Batch Gradient Norm after: 8.298992585740287
Epoch 3307/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.48102461695671084
Epoch: 3307, Batch Gradient Norm: 9.865444168265457
Epoch: 3307, Batch Gradient Norm after: 9.865444168265457
Epoch 3308/10000, Prediction Accuracy = 61.508%, Loss = 0.4911803722381592
Epoch: 3308, Batch Gradient Norm: 11.046037247371805
Epoch: 3308, Batch Gradient Norm after: 11.046037247371805
Epoch 3309/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.49605469703674315
Epoch: 3309, Batch Gradient Norm: 11.58120516815545
Epoch: 3309, Batch Gradient Norm after: 11.58120516815545
Epoch 3310/10000, Prediction Accuracy = 61.472%, Loss = 0.5012534856796265
Epoch: 3310, Batch Gradient Norm: 7.729881463609534
Epoch: 3310, Batch Gradient Norm after: 7.729881463609534
Epoch 3311/10000, Prediction Accuracy = 61.480000000000004%, Loss = 0.47812422513961794
Epoch: 3311, Batch Gradient Norm: 11.587329945824948
Epoch: 3311, Batch Gradient Norm after: 11.587329945824948
Epoch 3312/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.5034558057785035
Epoch: 3312, Batch Gradient Norm: 9.29355901289992
Epoch: 3312, Batch Gradient Norm after: 9.29355901289992
Epoch 3313/10000, Prediction Accuracy = 61.564%, Loss = 0.4858624517917633
Epoch: 3313, Batch Gradient Norm: 11.849150817972744
Epoch: 3313, Batch Gradient Norm after: 11.849150817972744
Epoch 3314/10000, Prediction Accuracy = 61.432%, Loss = 0.5069153547286988
Epoch: 3314, Batch Gradient Norm: 11.490522597767123
Epoch: 3314, Batch Gradient Norm after: 11.490522597767123
Epoch 3315/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.5000949800014496
Epoch: 3315, Batch Gradient Norm: 10.578736345500865
Epoch: 3315, Batch Gradient Norm after: 10.578736345500865
Epoch 3316/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4955885410308838
Epoch: 3316, Batch Gradient Norm: 12.24578319763818
Epoch: 3316, Batch Gradient Norm after: 12.24578319763818
Epoch 3317/10000, Prediction Accuracy = 61.39%, Loss = 0.5073976039886474
Epoch: 3317, Batch Gradient Norm: 11.261627482275616
Epoch: 3317, Batch Gradient Norm after: 11.261627482275616
Epoch 3318/10000, Prediction Accuracy = 61.538%, Loss = 0.5022269725799561
Epoch: 3318, Batch Gradient Norm: 9.92426899237376
Epoch: 3318, Batch Gradient Norm after: 9.92426899237376
Epoch 3319/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.4938512802124023
Epoch: 3319, Batch Gradient Norm: 8.538612975710045
Epoch: 3319, Batch Gradient Norm after: 8.538612975710045
Epoch 3320/10000, Prediction Accuracy = 61.67%, Loss = 0.48121041655540464
Epoch: 3320, Batch Gradient Norm: 10.75751743898897
Epoch: 3320, Batch Gradient Norm after: 10.75751743898897
Epoch 3321/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.49658300876617434
Epoch: 3321, Batch Gradient Norm: 9.29887601883699
Epoch: 3321, Batch Gradient Norm after: 9.29887601883699
Epoch 3322/10000, Prediction Accuracy = 61.5%, Loss = 0.4895051419734955
Epoch: 3322, Batch Gradient Norm: 11.331005550419869
Epoch: 3322, Batch Gradient Norm after: 11.331005550419869
Epoch 3323/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.5012287676334382
Epoch: 3323, Batch Gradient Norm: 9.72808524764447
Epoch: 3323, Batch Gradient Norm after: 9.72808524764447
Epoch 3324/10000, Prediction Accuracy = 61.462%, Loss = 0.4950703799724579
Epoch: 3324, Batch Gradient Norm: 8.014319981141773
Epoch: 3324, Batch Gradient Norm after: 8.014319981141773
Epoch 3325/10000, Prediction Accuracy = 61.56600000000001%, Loss = 0.4801297128200531
Epoch: 3325, Batch Gradient Norm: 10.928169383902963
Epoch: 3325, Batch Gradient Norm after: 10.928169383902963
Epoch 3326/10000, Prediction Accuracy = 61.524%, Loss = 0.49740121364593504
Epoch: 3326, Batch Gradient Norm: 11.589213696458119
Epoch: 3326, Batch Gradient Norm after: 11.589213696458119
Epoch 3327/10000, Prediction Accuracy = 61.5%, Loss = 0.5024201452732087
Epoch: 3327, Batch Gradient Norm: 10.014016620178745
Epoch: 3327, Batch Gradient Norm after: 10.014016620178745
Epoch 3328/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.48934297561645507
Epoch: 3328, Batch Gradient Norm: 9.482605008830781
Epoch: 3328, Batch Gradient Norm after: 9.482605008830781
Epoch 3329/10000, Prediction Accuracy = 61.556%, Loss = 0.4846381306648254
Epoch: 3329, Batch Gradient Norm: 9.778300292019425
Epoch: 3329, Batch Gradient Norm after: 9.778300292019425
Epoch 3330/10000, Prediction Accuracy = 61.45%, Loss = 0.488375061750412
Epoch: 3330, Batch Gradient Norm: 10.66384992570738
Epoch: 3330, Batch Gradient Norm after: 10.66384992570738
Epoch 3331/10000, Prediction Accuracy = 61.492%, Loss = 0.4967101216316223
Epoch: 3331, Batch Gradient Norm: 11.699595511735327
Epoch: 3331, Batch Gradient Norm after: 11.699595511735327
Epoch 3332/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.5125725269317627
Epoch: 3332, Batch Gradient Norm: 9.693169607281343
Epoch: 3332, Batch Gradient Norm after: 9.693169607281343
Epoch 3333/10000, Prediction Accuracy = 61.525999999999996%, Loss = 0.49025728106498717
Epoch: 3333, Batch Gradient Norm: 10.032278914543484
Epoch: 3333, Batch Gradient Norm after: 10.032278914543484
Epoch 3334/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.49380791187286377
Epoch: 3334, Batch Gradient Norm: 12.796319493509204
Epoch: 3334, Batch Gradient Norm after: 12.796319493509204
Epoch 3335/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.5167092502117157
Epoch: 3335, Batch Gradient Norm: 11.659081985033014
Epoch: 3335, Batch Gradient Norm after: 11.659081985033014
Epoch 3336/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.5033438920974731
Epoch: 3336, Batch Gradient Norm: 11.082982431381774
Epoch: 3336, Batch Gradient Norm after: 11.082982431381774
Epoch 3337/10000, Prediction Accuracy = 61.48%, Loss = 0.4962895691394806
Epoch: 3337, Batch Gradient Norm: 9.34867083606352
Epoch: 3337, Batch Gradient Norm after: 9.34867083606352
Epoch 3338/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.4830120146274567
Epoch: 3338, Batch Gradient Norm: 9.32681512601396
Epoch: 3338, Batch Gradient Norm after: 9.32681512601396
Epoch 3339/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.4843493700027466
Epoch: 3339, Batch Gradient Norm: 10.973484133089018
Epoch: 3339, Batch Gradient Norm after: 10.973484133089018
Epoch 3340/10000, Prediction Accuracy = 61.602%, Loss = 0.4965646743774414
Epoch: 3340, Batch Gradient Norm: 7.315972971370701
Epoch: 3340, Batch Gradient Norm after: 7.315972971370701
Epoch 3341/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.4739485263824463
Epoch: 3341, Batch Gradient Norm: 6.763998840148787
Epoch: 3341, Batch Gradient Norm after: 6.763998840148787
Epoch 3342/10000, Prediction Accuracy = 61.438%, Loss = 0.47203499674797056
Epoch: 3342, Batch Gradient Norm: 11.753945706630926
Epoch: 3342, Batch Gradient Norm after: 11.753945706630926
Epoch 3343/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.5014340996742248
Epoch: 3343, Batch Gradient Norm: 10.458595536373352
Epoch: 3343, Batch Gradient Norm after: 10.458595536373352
Epoch 3344/10000, Prediction Accuracy = 61.462%, Loss = 0.49839540719985964
Epoch: 3344, Batch Gradient Norm: 9.972623527874948
Epoch: 3344, Batch Gradient Norm after: 9.972623527874948
Epoch 3345/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4911032736301422
Epoch: 3345, Batch Gradient Norm: 11.203727444138584
Epoch: 3345, Batch Gradient Norm after: 11.203727444138584
Epoch 3346/10000, Prediction Accuracy = 61.553999999999995%, Loss = 0.49991685152053833
Epoch: 3346, Batch Gradient Norm: 11.606052547165962
Epoch: 3346, Batch Gradient Norm after: 11.606052547165962
Epoch 3347/10000, Prediction Accuracy = 61.66600000000001%, Loss = 0.5023547351360321
Epoch: 3347, Batch Gradient Norm: 8.516354171118332
Epoch: 3347, Batch Gradient Norm after: 8.516354171118332
Epoch 3348/10000, Prediction Accuracy = 61.604%, Loss = 0.4800804197788239
Epoch: 3348, Batch Gradient Norm: 11.995814643562197
Epoch: 3348, Batch Gradient Norm after: 11.995814643562197
Epoch 3349/10000, Prediction Accuracy = 61.472%, Loss = 0.5022248446941375
Epoch: 3349, Batch Gradient Norm: 12.750453348401031
Epoch: 3349, Batch Gradient Norm after: 12.750453348401031
Epoch 3350/10000, Prediction Accuracy = 61.512%, Loss = 0.5082485914230347
Epoch: 3350, Batch Gradient Norm: 9.890566607319709
Epoch: 3350, Batch Gradient Norm after: 9.890566607319709
Epoch 3351/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4897217094898224
Epoch: 3351, Batch Gradient Norm: 13.160253547068164
Epoch: 3351, Batch Gradient Norm after: 13.160253547068164
Epoch 3352/10000, Prediction Accuracy = 61.584%, Loss = 0.511474734544754
Epoch: 3352, Batch Gradient Norm: 9.24158021509522
Epoch: 3352, Batch Gradient Norm after: 9.24158021509522
Epoch 3353/10000, Prediction Accuracy = 61.398%, Loss = 0.49130616784095765
Epoch: 3353, Batch Gradient Norm: 8.92359351645681
Epoch: 3353, Batch Gradient Norm after: 8.92359351645681
Epoch 3354/10000, Prediction Accuracy = 61.50999999999999%, Loss = 0.4889332830905914
Epoch: 3354, Batch Gradient Norm: 7.319500327616349
Epoch: 3354, Batch Gradient Norm after: 7.319500327616349
Epoch 3355/10000, Prediction Accuracy = 61.648%, Loss = 0.47634602785110475
Epoch: 3355, Batch Gradient Norm: 10.490894966931293
Epoch: 3355, Batch Gradient Norm after: 10.490894966931293
Epoch 3356/10000, Prediction Accuracy = 61.674%, Loss = 0.4933955311775208
Epoch: 3356, Batch Gradient Norm: 13.066673810523305
Epoch: 3356, Batch Gradient Norm after: 13.066673810523305
Epoch 3357/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.5100301384925843
Epoch: 3357, Batch Gradient Norm: 10.824084030246595
Epoch: 3357, Batch Gradient Norm after: 10.824084030246595
Epoch 3358/10000, Prediction Accuracy = 61.54200000000001%, Loss = 0.49167691469192504
Epoch: 3358, Batch Gradient Norm: 10.059285152257454
Epoch: 3358, Batch Gradient Norm after: 10.059285152257454
Epoch 3359/10000, Prediction Accuracy = 61.508%, Loss = 0.48696920871734617
Epoch: 3359, Batch Gradient Norm: 8.83316149979003
Epoch: 3359, Batch Gradient Norm after: 8.83316149979003
Epoch 3360/10000, Prediction Accuracy = 61.548%, Loss = 0.4834734916687012
Epoch: 3360, Batch Gradient Norm: 10.360659994252366
Epoch: 3360, Batch Gradient Norm after: 10.360659994252366
Epoch 3361/10000, Prediction Accuracy = 61.496%, Loss = 0.49074667096138
Epoch: 3361, Batch Gradient Norm: 9.445602468562512
Epoch: 3361, Batch Gradient Norm after: 9.445602468562512
Epoch 3362/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.48493104577064516
Epoch: 3362, Batch Gradient Norm: 11.020829026967037
Epoch: 3362, Batch Gradient Norm after: 11.020829026967037
Epoch 3363/10000, Prediction Accuracy = 61.488%, Loss = 0.49525535106658936
Epoch: 3363, Batch Gradient Norm: 12.674770452733345
Epoch: 3363, Batch Gradient Norm after: 12.674770452733345
Epoch 3364/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.511519479751587
Epoch: 3364, Batch Gradient Norm: 9.757953471921521
Epoch: 3364, Batch Gradient Norm after: 9.757953471921521
Epoch 3365/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4909613192081451
Epoch: 3365, Batch Gradient Norm: 7.073991874044356
Epoch: 3365, Batch Gradient Norm after: 7.073991874044356
Epoch 3366/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.4746403515338898
Epoch: 3366, Batch Gradient Norm: 8.630647735381523
Epoch: 3366, Batch Gradient Norm after: 8.630647735381523
Epoch 3367/10000, Prediction Accuracy = 61.398%, Loss = 0.483803802728653
Epoch: 3367, Batch Gradient Norm: 12.631925525525261
Epoch: 3367, Batch Gradient Norm after: 12.631925525525261
Epoch 3368/10000, Prediction Accuracy = 61.662%, Loss = 0.5148284018039704
Epoch: 3368, Batch Gradient Norm: 12.06891379661844
Epoch: 3368, Batch Gradient Norm after: 12.06891379661844
Epoch 3369/10000, Prediction Accuracy = 61.602%, Loss = 0.5031065285205841
Epoch: 3369, Batch Gradient Norm: 12.916360138394923
Epoch: 3369, Batch Gradient Norm after: 12.916360138394923
Epoch 3370/10000, Prediction Accuracy = 61.438%, Loss = 0.5112955927848816
Epoch: 3370, Batch Gradient Norm: 10.132590950757399
Epoch: 3370, Batch Gradient Norm after: 10.132590950757399
Epoch 3371/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.48906130194664
Epoch: 3371, Batch Gradient Norm: 8.718663036352618
Epoch: 3371, Batch Gradient Norm after: 8.718663036352618
Epoch 3372/10000, Prediction Accuracy = 61.553999999999995%, Loss = 0.47988743185997007
Epoch: 3372, Batch Gradient Norm: 8.853162542073651
Epoch: 3372, Batch Gradient Norm after: 8.853162542073651
Epoch 3373/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.4810742735862732
Epoch: 3373, Batch Gradient Norm: 9.369106591550928
Epoch: 3373, Batch Gradient Norm after: 9.369106591550928
Epoch 3374/10000, Prediction Accuracy = 61.50600000000001%, Loss = 0.48263258934020997
Epoch: 3374, Batch Gradient Norm: 12.65962572545396
Epoch: 3374, Batch Gradient Norm after: 12.65962572545396
Epoch 3375/10000, Prediction Accuracy = 61.544%, Loss = 0.5080310344696045
Epoch: 3375, Batch Gradient Norm: 10.410030033856893
Epoch: 3375, Batch Gradient Norm after: 10.410030033856893
Epoch 3376/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.49287788271903993
Epoch: 3376, Batch Gradient Norm: 10.478951465403354
Epoch: 3376, Batch Gradient Norm after: 10.478951465403354
Epoch 3377/10000, Prediction Accuracy = 61.602%, Loss = 0.4937464952468872
Epoch: 3377, Batch Gradient Norm: 12.087348056854287
Epoch: 3377, Batch Gradient Norm after: 12.087348056854287
Epoch 3378/10000, Prediction Accuracy = 61.512%, Loss = 0.5029541850090027
Epoch: 3378, Batch Gradient Norm: 10.397609332346798
Epoch: 3378, Batch Gradient Norm after: 10.397609332346798
Epoch 3379/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.49257475733757017
Epoch: 3379, Batch Gradient Norm: 6.730449318616128
Epoch: 3379, Batch Gradient Norm after: 6.730449318616128
Epoch 3380/10000, Prediction Accuracy = 61.552%, Loss = 0.4748620331287384
Epoch: 3380, Batch Gradient Norm: 7.9754286894066775
Epoch: 3380, Batch Gradient Norm after: 7.9754286894066775
Epoch 3381/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.4773148775100708
Epoch: 3381, Batch Gradient Norm: 11.053081750760207
Epoch: 3381, Batch Gradient Norm after: 11.053081750760207
Epoch 3382/10000, Prediction Accuracy = 61.519999999999996%, Loss = 0.4938768208026886
Epoch: 3382, Batch Gradient Norm: 11.492496738044046
Epoch: 3382, Batch Gradient Norm after: 11.492496738044046
Epoch 3383/10000, Prediction Accuracy = 61.564%, Loss = 0.49858259558677676
Epoch: 3383, Batch Gradient Norm: 11.252783364622813
Epoch: 3383, Batch Gradient Norm after: 11.252783364622813
Epoch 3384/10000, Prediction Accuracy = 61.604%, Loss = 0.5023094832897186
Epoch: 3384, Batch Gradient Norm: 4.499961376961863
Epoch: 3384, Batch Gradient Norm after: 4.499961376961863
Epoch 3385/10000, Prediction Accuracy = 61.562%, Loss = 0.46145880818367
Epoch: 3385, Batch Gradient Norm: 8.450378765195163
Epoch: 3385, Batch Gradient Norm after: 8.450378765195163
Epoch 3386/10000, Prediction Accuracy = 61.604%, Loss = 0.4816746413707733
Epoch: 3386, Batch Gradient Norm: 12.669305345942893
Epoch: 3386, Batch Gradient Norm after: 12.669305345942893
Epoch 3387/10000, Prediction Accuracy = 61.662%, Loss = 0.5095300436019897
Epoch: 3387, Batch Gradient Norm: 13.777958720130528
Epoch: 3387, Batch Gradient Norm after: 13.777958720130528
Epoch 3388/10000, Prediction Accuracy = 61.556%, Loss = 0.5178003549575806
Epoch: 3388, Batch Gradient Norm: 10.39510714314228
Epoch: 3388, Batch Gradient Norm after: 10.39510714314228
Epoch 3389/10000, Prediction Accuracy = 61.436%, Loss = 0.49286327362060545
Epoch: 3389, Batch Gradient Norm: 11.840605806565636
Epoch: 3389, Batch Gradient Norm after: 11.840605806565636
Epoch 3390/10000, Prediction Accuracy = 61.622%, Loss = 0.5041854441165924
Epoch: 3390, Batch Gradient Norm: 10.28584708140962
Epoch: 3390, Batch Gradient Norm after: 10.28584708140962
Epoch 3391/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4925255298614502
Epoch: 3391, Batch Gradient Norm: 8.353594987134171
Epoch: 3391, Batch Gradient Norm after: 8.353594987134171
Epoch 3392/10000, Prediction Accuracy = 61.564%, Loss = 0.47894303798675536
Epoch: 3392, Batch Gradient Norm: 9.982331545442136
Epoch: 3392, Batch Gradient Norm after: 9.982331545442136
Epoch 3393/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.485136616230011
Epoch: 3393, Batch Gradient Norm: 9.946384262441024
Epoch: 3393, Batch Gradient Norm after: 9.946384262441024
Epoch 3394/10000, Prediction Accuracy = 61.58%, Loss = 0.4882961750030518
Epoch: 3394, Batch Gradient Norm: 12.078870255157307
Epoch: 3394, Batch Gradient Norm after: 12.078870255157307
Epoch 3395/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.5062405169010162
Epoch: 3395, Batch Gradient Norm: 10.43296714913504
Epoch: 3395, Batch Gradient Norm after: 10.43296714913504
Epoch 3396/10000, Prediction Accuracy = 61.524%, Loss = 0.49015507102012634
Epoch: 3396, Batch Gradient Norm: 11.49003844230331
Epoch: 3396, Batch Gradient Norm after: 11.49003844230331
Epoch 3397/10000, Prediction Accuracy = 61.462%, Loss = 0.502079302072525
Epoch: 3397, Batch Gradient Norm: 7.48998262684154
Epoch: 3397, Batch Gradient Norm after: 7.48998262684154
Epoch 3398/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.47269927263259887
Epoch: 3398, Batch Gradient Norm: 9.036834145474387
Epoch: 3398, Batch Gradient Norm after: 9.036834145474387
Epoch 3399/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.480876100063324
Epoch: 3399, Batch Gradient Norm: 7.011023117435289
Epoch: 3399, Batch Gradient Norm after: 7.011023117435289
Epoch 3400/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4718838810920715
Epoch: 3400, Batch Gradient Norm: 10.545387883602643
Epoch: 3400, Batch Gradient Norm after: 10.545387883602643
Epoch 3401/10000, Prediction Accuracy = 61.576%, Loss = 0.49608761072158813
Epoch: 3401, Batch Gradient Norm: 10.182609180581858
Epoch: 3401, Batch Gradient Norm after: 10.182609180581858
Epoch 3402/10000, Prediction Accuracy = 61.54%, Loss = 0.48864631056785585
Epoch: 3402, Batch Gradient Norm: 10.671044322195433
Epoch: 3402, Batch Gradient Norm after: 10.671044322195433
Epoch 3403/10000, Prediction Accuracy = 61.51800000000001%, Loss = 0.49170796275138856
Epoch: 3403, Batch Gradient Norm: 10.50003175349178
Epoch: 3403, Batch Gradient Norm after: 10.50003175349178
Epoch 3404/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.49387014508247373
Epoch: 3404, Batch Gradient Norm: 10.072334590993139
Epoch: 3404, Batch Gradient Norm after: 10.072334590993139
Epoch 3405/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.49172220230102537
Epoch: 3405, Batch Gradient Norm: 14.781088619330664
Epoch: 3405, Batch Gradient Norm after: 14.781088619330664
Epoch 3406/10000, Prediction Accuracy = 61.56600000000001%, Loss = 0.5269909560680389
Epoch: 3406, Batch Gradient Norm: 10.783759270991684
Epoch: 3406, Batch Gradient Norm after: 10.783759270991684
Epoch 3407/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.49222165942192075
Epoch: 3407, Batch Gradient Norm: 11.327485434743318
Epoch: 3407, Batch Gradient Norm after: 11.327485434743318
Epoch 3408/10000, Prediction Accuracy = 61.604%, Loss = 0.49837145805358884
Epoch: 3408, Batch Gradient Norm: 8.264864281646272
Epoch: 3408, Batch Gradient Norm after: 8.264864281646272
Epoch 3409/10000, Prediction Accuracy = 61.45799999999999%, Loss = 0.47966012358665466
Epoch: 3409, Batch Gradient Norm: 9.347251861065475
Epoch: 3409, Batch Gradient Norm after: 9.347251861065475
Epoch 3410/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.4834592819213867
Epoch: 3410, Batch Gradient Norm: 10.81933091734829
Epoch: 3410, Batch Gradient Norm after: 10.81933091734829
Epoch 3411/10000, Prediction Accuracy = 61.58%, Loss = 0.4932380378246307
Epoch: 3411, Batch Gradient Norm: 13.022610019623981
Epoch: 3411, Batch Gradient Norm after: 13.022610019623981
Epoch 3412/10000, Prediction Accuracy = 61.648%, Loss = 0.5108754813671113
Epoch: 3412, Batch Gradient Norm: 9.090324770167147
Epoch: 3412, Batch Gradient Norm after: 9.090324770167147
Epoch 3413/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.4846312940120697
Epoch: 3413, Batch Gradient Norm: 8.542110872210555
Epoch: 3413, Batch Gradient Norm after: 8.542110872210555
Epoch 3414/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.4779214322566986
Epoch: 3414, Batch Gradient Norm: 7.737449566716797
Epoch: 3414, Batch Gradient Norm after: 7.737449566716797
Epoch 3415/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4751016616821289
Epoch: 3415, Batch Gradient Norm: 10.946781625364613
Epoch: 3415, Batch Gradient Norm after: 10.946781625364613
Epoch 3416/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.49672051668167116
Epoch: 3416, Batch Gradient Norm: 10.694133565751953
Epoch: 3416, Batch Gradient Norm after: 10.694133565751953
Epoch 3417/10000, Prediction Accuracy = 61.686%, Loss = 0.48996429443359374
Epoch: 3417, Batch Gradient Norm: 10.46534572734409
Epoch: 3417, Batch Gradient Norm after: 10.46534572734409
Epoch 3418/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.48787477016448977
Epoch: 3418, Batch Gradient Norm: 11.003395544651447
Epoch: 3418, Batch Gradient Norm after: 11.003395544651447
Epoch 3419/10000, Prediction Accuracy = 61.602%, Loss = 0.4938942492008209
Epoch: 3419, Batch Gradient Norm: 12.572678312908309
Epoch: 3419, Batch Gradient Norm after: 12.572678312908309
Epoch 3420/10000, Prediction Accuracy = 61.532%, Loss = 0.5072839379310607
Epoch: 3420, Batch Gradient Norm: 10.451578141518818
Epoch: 3420, Batch Gradient Norm after: 10.451578141518818
Epoch 3421/10000, Prediction Accuracy = 61.598%, Loss = 0.49038790464401244
Epoch: 3421, Batch Gradient Norm: 7.373907058922224
Epoch: 3421, Batch Gradient Norm after: 7.373907058922224
Epoch 3422/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4715917110443115
Epoch: 3422, Batch Gradient Norm: 9.581572367166538
Epoch: 3422, Batch Gradient Norm after: 9.581572367166538
Epoch 3423/10000, Prediction Accuracy = 61.726%, Loss = 0.48366538286209104
Epoch: 3423, Batch Gradient Norm: 9.5659476540449
Epoch: 3423, Batch Gradient Norm after: 9.5659476540449
Epoch 3424/10000, Prediction Accuracy = 61.438%, Loss = 0.48523770570755004
Epoch: 3424, Batch Gradient Norm: 10.02092487709391
Epoch: 3424, Batch Gradient Norm after: 10.02092487709391
Epoch 3425/10000, Prediction Accuracy = 61.59799999999999%, Loss = 0.486261785030365
Epoch: 3425, Batch Gradient Norm: 9.965379886191355
Epoch: 3425, Batch Gradient Norm after: 9.965379886191355
Epoch 3426/10000, Prediction Accuracy = 61.548%, Loss = 0.4891778647899628
Epoch: 3426, Batch Gradient Norm: 7.505241532994733
Epoch: 3426, Batch Gradient Norm after: 7.505241532994733
Epoch 3427/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.47410197257995607
Epoch: 3427, Batch Gradient Norm: 8.356143924904856
Epoch: 3427, Batch Gradient Norm after: 8.356143924904856
Epoch 3428/10000, Prediction Accuracy = 61.614%, Loss = 0.4768281102180481
Epoch: 3428, Batch Gradient Norm: 8.524262696138814
Epoch: 3428, Batch Gradient Norm after: 8.524262696138814
Epoch 3429/10000, Prediction Accuracy = 61.6%, Loss = 0.4774034023284912
Epoch: 3429, Batch Gradient Norm: 10.745165045589344
Epoch: 3429, Batch Gradient Norm after: 10.745165045589344
Epoch 3430/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.49231454730033875
Epoch: 3430, Batch Gradient Norm: 11.945989811747845
Epoch: 3430, Batch Gradient Norm after: 11.945989811747845
Epoch 3431/10000, Prediction Accuracy = 61.504%, Loss = 0.5042324721813202
Epoch: 3431, Batch Gradient Norm: 11.91210104556653
Epoch: 3431, Batch Gradient Norm after: 11.91210104556653
Epoch 3432/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.5000012457370758
Epoch: 3432, Batch Gradient Norm: 9.309013602220336
Epoch: 3432, Batch Gradient Norm after: 9.309013602220336
Epoch 3433/10000, Prediction Accuracy = 61.544%, Loss = 0.48479140400886533
Epoch: 3433, Batch Gradient Norm: 11.173448612678607
Epoch: 3433, Batch Gradient Norm after: 11.173448612678607
Epoch 3434/10000, Prediction Accuracy = 61.46%, Loss = 0.5012930810451508
Epoch: 3434, Batch Gradient Norm: 7.977677234442287
Epoch: 3434, Batch Gradient Norm after: 7.977677234442287
Epoch 3435/10000, Prediction Accuracy = 61.61600000000001%, Loss = 0.47459999918937684
Epoch: 3435, Batch Gradient Norm: 8.545934031736504
Epoch: 3435, Batch Gradient Norm after: 8.545934031736504
Epoch 3436/10000, Prediction Accuracy = 61.614%, Loss = 0.4768852233886719
Epoch: 3436, Batch Gradient Norm: 14.750391584294523
Epoch: 3436, Batch Gradient Norm after: 14.750391584294523
Epoch 3437/10000, Prediction Accuracy = 61.586%, Loss = 0.5233443319797516
Epoch: 3437, Batch Gradient Norm: 14.896986512575397
Epoch: 3437, Batch Gradient Norm after: 14.896986512575397
Epoch 3438/10000, Prediction Accuracy = 61.61400000000001%, Loss = 0.5273274421691895
Epoch: 3438, Batch Gradient Norm: 8.236671327821323
Epoch: 3438, Batch Gradient Norm after: 8.236671327821323
Epoch 3439/10000, Prediction Accuracy = 61.628%, Loss = 0.47608981728553773
Epoch: 3439, Batch Gradient Norm: 9.820185935166108
Epoch: 3439, Batch Gradient Norm after: 9.820185935166108
Epoch 3440/10000, Prediction Accuracy = 61.726%, Loss = 0.48578662872314454
Epoch: 3440, Batch Gradient Norm: 10.134800979318051
Epoch: 3440, Batch Gradient Norm after: 10.134800979318051
Epoch 3441/10000, Prediction Accuracy = 61.64999999999999%, Loss = 0.48952463269233704
Epoch: 3441, Batch Gradient Norm: 8.567866520347026
Epoch: 3441, Batch Gradient Norm after: 8.567866520347026
Epoch 3442/10000, Prediction Accuracy = 61.636%, Loss = 0.48089211583137514
Epoch: 3442, Batch Gradient Norm: 8.071558539882625
Epoch: 3442, Batch Gradient Norm after: 8.071558539882625
Epoch 3443/10000, Prediction Accuracy = 61.6%, Loss = 0.475067675113678
Epoch: 3443, Batch Gradient Norm: 11.717636494328325
Epoch: 3443, Batch Gradient Norm after: 11.717636494328325
Epoch 3444/10000, Prediction Accuracy = 61.504%, Loss = 0.49807931780815123
Epoch: 3444, Batch Gradient Norm: 11.587150080217395
Epoch: 3444, Batch Gradient Norm after: 11.587150080217395
Epoch 3445/10000, Prediction Accuracy = 61.598%, Loss = 0.49540539979934695
Epoch: 3445, Batch Gradient Norm: 11.010350659815812
Epoch: 3445, Batch Gradient Norm after: 11.010350659815812
Epoch 3446/10000, Prediction Accuracy = 61.6%, Loss = 0.49246189594268797
Epoch: 3446, Batch Gradient Norm: 13.322875468906133
Epoch: 3446, Batch Gradient Norm after: 13.322875468906133
Epoch 3447/10000, Prediction Accuracy = 61.548%, Loss = 0.5185542523860931
Epoch: 3447, Batch Gradient Norm: 11.132548068617991
Epoch: 3447, Batch Gradient Norm after: 11.132548068617991
Epoch 3448/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4947289526462555
Epoch: 3448, Batch Gradient Norm: 8.962693778798329
Epoch: 3448, Batch Gradient Norm after: 8.962693778798329
Epoch 3449/10000, Prediction Accuracy = 61.516%, Loss = 0.481915545463562
Epoch: 3449, Batch Gradient Norm: 7.245807074307259
Epoch: 3449, Batch Gradient Norm after: 7.245807074307259
Epoch 3450/10000, Prediction Accuracy = 61.574%, Loss = 0.4718755006790161
Epoch: 3450, Batch Gradient Norm: 10.580312368216209
Epoch: 3450, Batch Gradient Norm after: 10.580312368216209
Epoch 3451/10000, Prediction Accuracy = 61.528000000000006%, Loss = 0.4914315164089203
Epoch: 3451, Batch Gradient Norm: 13.57326185221484
Epoch: 3451, Batch Gradient Norm after: 13.57326185221484
Epoch 3452/10000, Prediction Accuracy = 61.56200000000001%, Loss = 0.5167235970497132
Epoch: 3452, Batch Gradient Norm: 9.771920465022736
Epoch: 3452, Batch Gradient Norm after: 9.771920465022736
Epoch 3453/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.48790099620819094
Epoch: 3453, Batch Gradient Norm: 8.61069817068372
Epoch: 3453, Batch Gradient Norm after: 8.61069817068372
Epoch 3454/10000, Prediction Accuracy = 61.480000000000004%, Loss = 0.47700701355934144
Epoch: 3454, Batch Gradient Norm: 9.186431792081757
Epoch: 3454, Batch Gradient Norm after: 9.186431792081757
Epoch 3455/10000, Prediction Accuracy = 61.468%, Loss = 0.4805272579193115
Epoch: 3455, Batch Gradient Norm: 11.459741991587933
Epoch: 3455, Batch Gradient Norm after: 11.459741991587933
Epoch 3456/10000, Prediction Accuracy = 61.538%, Loss = 0.4972874760627747
Epoch: 3456, Batch Gradient Norm: 10.40464363888255
Epoch: 3456, Batch Gradient Norm after: 10.40464363888255
Epoch 3457/10000, Prediction Accuracy = 61.528%, Loss = 0.48836719393730166
Epoch: 3457, Batch Gradient Norm: 9.516836593545644
Epoch: 3457, Batch Gradient Norm after: 9.516836593545644
Epoch 3458/10000, Prediction Accuracy = 61.588%, Loss = 0.48440971970558167
Epoch: 3458, Batch Gradient Norm: 8.888997423643836
Epoch: 3458, Batch Gradient Norm after: 8.888997423643836
Epoch 3459/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.4815566182136536
Epoch: 3459, Batch Gradient Norm: 8.110885975566193
Epoch: 3459, Batch Gradient Norm after: 8.110885975566193
Epoch 3460/10000, Prediction Accuracy = 61.428%, Loss = 0.4782726764678955
Epoch: 3460, Batch Gradient Norm: 10.993433736403748
Epoch: 3460, Batch Gradient Norm after: 10.993433736403748
Epoch 3461/10000, Prediction Accuracy = 61.614%, Loss = 0.496797776222229
Epoch: 3461, Batch Gradient Norm: 13.54392569041441
Epoch: 3461, Batch Gradient Norm after: 13.54392569041441
Epoch 3462/10000, Prediction Accuracy = 61.528%, Loss = 0.515399432182312
Epoch: 3462, Batch Gradient Norm: 9.15948568287798
Epoch: 3462, Batch Gradient Norm after: 9.15948568287798
Epoch 3463/10000, Prediction Accuracy = 61.588%, Loss = 0.4811569511890411
Epoch: 3463, Batch Gradient Norm: 10.094425780548518
Epoch: 3463, Batch Gradient Norm after: 10.094425780548518
Epoch 3464/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.4877181351184845
Epoch: 3464, Batch Gradient Norm: 9.770024885413411
Epoch: 3464, Batch Gradient Norm after: 9.770024885413411
Epoch 3465/10000, Prediction Accuracy = 61.628%, Loss = 0.4867297768592834
Epoch: 3465, Batch Gradient Norm: 9.567890034247492
Epoch: 3465, Batch Gradient Norm after: 9.567890034247492
Epoch 3466/10000, Prediction Accuracy = 61.548%, Loss = 0.4853726923465729
Epoch: 3466, Batch Gradient Norm: 9.554041849190233
Epoch: 3466, Batch Gradient Norm after: 9.554041849190233
Epoch 3467/10000, Prediction Accuracy = 61.586%, Loss = 0.4846145033836365
Epoch: 3467, Batch Gradient Norm: 8.028099735451468
Epoch: 3467, Batch Gradient Norm after: 8.028099735451468
Epoch 3468/10000, Prediction Accuracy = 61.55%, Loss = 0.47324312329292295
Epoch: 3468, Batch Gradient Norm: 13.710326146188407
Epoch: 3468, Batch Gradient Norm after: 13.710326146188407
Epoch 3469/10000, Prediction Accuracy = 61.538%, Loss = 0.5119359612464904
Epoch: 3469, Batch Gradient Norm: 12.099601655241045
Epoch: 3469, Batch Gradient Norm after: 12.099601655241045
Epoch 3470/10000, Prediction Accuracy = 61.42%, Loss = 0.5056225478649139
Epoch: 3470, Batch Gradient Norm: 9.111137475807777
Epoch: 3470, Batch Gradient Norm after: 9.111137475807777
Epoch 3471/10000, Prediction Accuracy = 61.562%, Loss = 0.48593050241470337
Epoch: 3471, Batch Gradient Norm: 10.936462889980486
Epoch: 3471, Batch Gradient Norm after: 10.936462889980486
Epoch 3472/10000, Prediction Accuracy = 61.586%, Loss = 0.49410845041275026
Epoch: 3472, Batch Gradient Norm: 11.993032479065352
Epoch: 3472, Batch Gradient Norm after: 11.993032479065352
Epoch 3473/10000, Prediction Accuracy = 61.628%, Loss = 0.5006556928157806
Epoch: 3473, Batch Gradient Norm: 11.18956364920632
Epoch: 3473, Batch Gradient Norm after: 11.18956364920632
Epoch 3474/10000, Prediction Accuracy = 61.452%, Loss = 0.49145208597183226
Epoch: 3474, Batch Gradient Norm: 9.709226739544766
Epoch: 3474, Batch Gradient Norm after: 9.709226739544766
Epoch 3475/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4822418808937073
Epoch: 3475, Batch Gradient Norm: 9.737757873968707
Epoch: 3475, Batch Gradient Norm after: 9.737757873968707
Epoch 3476/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.48322454690933225
Epoch: 3476, Batch Gradient Norm: 10.075043409341369
Epoch: 3476, Batch Gradient Norm after: 10.075043409341369
Epoch 3477/10000, Prediction Accuracy = 61.476%, Loss = 0.48640176653862
Epoch: 3477, Batch Gradient Norm: 10.698097436345538
Epoch: 3477, Batch Gradient Norm after: 10.698097436345538
Epoch 3478/10000, Prediction Accuracy = 61.516000000000005%, Loss = 0.4920682549476624
Epoch: 3478, Batch Gradient Norm: 8.097566845184504
Epoch: 3478, Batch Gradient Norm after: 8.097566845184504
Epoch 3479/10000, Prediction Accuracy = 61.556%, Loss = 0.47707576751708985
Epoch: 3479, Batch Gradient Norm: 9.178380225490379
Epoch: 3479, Batch Gradient Norm after: 9.178380225490379
Epoch 3480/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.47956000566482543
Epoch: 3480, Batch Gradient Norm: 12.922423404717447
Epoch: 3480, Batch Gradient Norm after: 12.922423404717447
Epoch 3481/10000, Prediction Accuracy = 61.60600000000001%, Loss = 0.5048576653003692
Epoch: 3481, Batch Gradient Norm: 11.57818274070793
Epoch: 3481, Batch Gradient Norm after: 11.57818274070793
Epoch 3482/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.49487541913986205
Epoch: 3482, Batch Gradient Norm: 10.685063034729025
Epoch: 3482, Batch Gradient Norm after: 10.685063034729025
Epoch 3483/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4885150849819183
Epoch: 3483, Batch Gradient Norm: 10.190677148687433
Epoch: 3483, Batch Gradient Norm after: 10.190677148687433
Epoch 3484/10000, Prediction Accuracy = 61.366%, Loss = 0.49451957941055297
Epoch: 3484, Batch Gradient Norm: 8.885515697814796
Epoch: 3484, Batch Gradient Norm after: 8.885515697814796
Epoch 3485/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.47701048851013184
Epoch: 3485, Batch Gradient Norm: 8.969036259474302
Epoch: 3485, Batch Gradient Norm after: 8.969036259474302
Epoch 3486/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4824275553226471
Epoch: 3486, Batch Gradient Norm: 10.13959156403244
Epoch: 3486, Batch Gradient Norm after: 10.13959156403244
Epoch 3487/10000, Prediction Accuracy = 61.657999999999994%, Loss = 0.4893303275108337
Epoch: 3487, Batch Gradient Norm: 8.775857137727021
Epoch: 3487, Batch Gradient Norm after: 8.775857137727021
Epoch 3488/10000, Prediction Accuracy = 61.612%, Loss = 0.47843942046165466
Epoch: 3488, Batch Gradient Norm: 8.996249644832709
Epoch: 3488, Batch Gradient Norm after: 8.996249644832709
Epoch 3489/10000, Prediction Accuracy = 61.538%, Loss = 0.4794167041778564
Epoch: 3489, Batch Gradient Norm: 10.47145981395682
Epoch: 3489, Batch Gradient Norm after: 10.47145981395682
Epoch 3490/10000, Prediction Accuracy = 61.562%, Loss = 0.4898560345172882
Epoch: 3490, Batch Gradient Norm: 10.831893769007927
Epoch: 3490, Batch Gradient Norm after: 10.831893769007927
Epoch 3491/10000, Prediction Accuracy = 61.43599999999999%, Loss = 0.4884271860122681
Epoch: 3491, Batch Gradient Norm: 12.38382804120812
Epoch: 3491, Batch Gradient Norm after: 12.38382804120812
Epoch 3492/10000, Prediction Accuracy = 61.564%, Loss = 0.4987752974033356
Epoch: 3492, Batch Gradient Norm: 12.013099175807996
Epoch: 3492, Batch Gradient Norm after: 12.013099175807996
Epoch 3493/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.49799219369888303
Epoch: 3493, Batch Gradient Norm: 10.078346863624631
Epoch: 3493, Batch Gradient Norm after: 10.078346863624631
Epoch 3494/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4856422126293182
Epoch: 3494, Batch Gradient Norm: 11.448405622115464
Epoch: 3494, Batch Gradient Norm after: 11.448405622115464
Epoch 3495/10000, Prediction Accuracy = 61.488%, Loss = 0.4942568004131317
Epoch: 3495, Batch Gradient Norm: 8.945327306199706
Epoch: 3495, Batch Gradient Norm after: 8.945327306199706
Epoch 3496/10000, Prediction Accuracy = 61.538%, Loss = 0.47957481145858766
Epoch: 3496, Batch Gradient Norm: 7.72724952671778
Epoch: 3496, Batch Gradient Norm after: 7.72724952671778
Epoch 3497/10000, Prediction Accuracy = 61.7%, Loss = 0.4719551980495453
Epoch: 3497, Batch Gradient Norm: 10.593088075723507
Epoch: 3497, Batch Gradient Norm after: 10.593088075723507
Epoch 3498/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4895561456680298
Epoch: 3498, Batch Gradient Norm: 10.181303266549964
Epoch: 3498, Batch Gradient Norm after: 10.181303266549964
Epoch 3499/10000, Prediction Accuracy = 61.66400000000001%, Loss = 0.488157844543457
Epoch: 3499, Batch Gradient Norm: 10.334072687204117
Epoch: 3499, Batch Gradient Norm after: 10.334072687204117
Epoch 3500/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.49076037406921386
Epoch: 3500, Batch Gradient Norm: 10.585281803498518
Epoch: 3500, Batch Gradient Norm after: 10.585281803498518
Epoch 3501/10000, Prediction Accuracy = 61.548%, Loss = 0.4959941029548645
Epoch: 3501, Batch Gradient Norm: 10.951773734057635
Epoch: 3501, Batch Gradient Norm after: 10.951773734057635
Epoch 3502/10000, Prediction Accuracy = 61.634%, Loss = 0.4899873673915863
Epoch: 3502, Batch Gradient Norm: 15.20279237323707
Epoch: 3502, Batch Gradient Norm after: 15.20279237323707
Epoch 3503/10000, Prediction Accuracy = 61.538%, Loss = 0.5236106395721436
Epoch: 3503, Batch Gradient Norm: 9.69665121948012
Epoch: 3503, Batch Gradient Norm after: 9.69665121948012
Epoch 3504/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.48148499727249144
Epoch: 3504, Batch Gradient Norm: 9.050146185779507
Epoch: 3504, Batch Gradient Norm after: 9.050146185779507
Epoch 3505/10000, Prediction Accuracy = 61.538%, Loss = 0.47994195222854613
Epoch: 3505, Batch Gradient Norm: 9.386046235198021
Epoch: 3505, Batch Gradient Norm after: 9.386046235198021
Epoch 3506/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.48032853603363035
Epoch: 3506, Batch Gradient Norm: 9.670353613383543
Epoch: 3506, Batch Gradient Norm after: 9.670353613383543
Epoch 3507/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4838158845901489
Epoch: 3507, Batch Gradient Norm: 7.530922352308084
Epoch: 3507, Batch Gradient Norm after: 7.530922352308084
Epoch 3508/10000, Prediction Accuracy = 61.68599999999999%, Loss = 0.47319849133491515
Epoch: 3508, Batch Gradient Norm: 10.67635022529866
Epoch: 3508, Batch Gradient Norm after: 10.67635022529866
Epoch 3509/10000, Prediction Accuracy = 61.536%, Loss = 0.4910903453826904
Epoch: 3509, Batch Gradient Norm: 11.750761813372819
Epoch: 3509, Batch Gradient Norm after: 11.750761813372819
Epoch 3510/10000, Prediction Accuracy = 61.536%, Loss = 0.4962669491767883
Epoch: 3510, Batch Gradient Norm: 13.236557092382808
Epoch: 3510, Batch Gradient Norm after: 13.236557092382808
Epoch 3511/10000, Prediction Accuracy = 61.516000000000005%, Loss = 0.5116729378700257
Epoch: 3511, Batch Gradient Norm: 9.190849897195596
Epoch: 3511, Batch Gradient Norm after: 9.190849897195596
Epoch 3512/10000, Prediction Accuracy = 61.604%, Loss = 0.4821549654006958
Epoch: 3512, Batch Gradient Norm: 8.44016797530803
Epoch: 3512, Batch Gradient Norm after: 8.44016797530803
Epoch 3513/10000, Prediction Accuracy = 61.60600000000001%, Loss = 0.4765287697315216
Epoch: 3513, Batch Gradient Norm: 10.995303786098356
Epoch: 3513, Batch Gradient Norm after: 10.995303786098356
Epoch 3514/10000, Prediction Accuracy = 61.496%, Loss = 0.490431421995163
Epoch: 3514, Batch Gradient Norm: 7.636197081997816
Epoch: 3514, Batch Gradient Norm after: 7.636197081997816
Epoch 3515/10000, Prediction Accuracy = 61.51400000000001%, Loss = 0.4697312474250793
Epoch: 3515, Batch Gradient Norm: 7.868573637846835
Epoch: 3515, Batch Gradient Norm after: 7.868573637846835
Epoch 3516/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.4711821019649506
Epoch: 3516, Batch Gradient Norm: 10.16928329469706
Epoch: 3516, Batch Gradient Norm after: 10.16928329469706
Epoch 3517/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.487111097574234
Epoch: 3517, Batch Gradient Norm: 10.44551252449947
Epoch: 3517, Batch Gradient Norm after: 10.44551252449947
Epoch 3518/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.4878785789012909
Epoch: 3518, Batch Gradient Norm: 10.002619143799434
Epoch: 3518, Batch Gradient Norm after: 10.002619143799434
Epoch 3519/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4828449010848999
Epoch: 3519, Batch Gradient Norm: 11.167622451069269
Epoch: 3519, Batch Gradient Norm after: 11.167622451069269
Epoch 3520/10000, Prediction Accuracy = 61.6%, Loss = 0.4917061746120453
Epoch: 3520, Batch Gradient Norm: 12.845487416093665
Epoch: 3520, Batch Gradient Norm after: 12.845487416093665
Epoch 3521/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.5127382874488831
Epoch: 3521, Batch Gradient Norm: 9.838379949058668
Epoch: 3521, Batch Gradient Norm after: 9.838379949058668
Epoch 3522/10000, Prediction Accuracy = 61.552%, Loss = 0.48769221305847166
Epoch: 3522, Batch Gradient Norm: 14.402055077172049
Epoch: 3522, Batch Gradient Norm after: 14.402055077172049
Epoch 3523/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.5198333919048309
Epoch: 3523, Batch Gradient Norm: 9.710839627708932
Epoch: 3523, Batch Gradient Norm after: 9.710839627708932
Epoch 3524/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4813570439815521
Epoch: 3524, Batch Gradient Norm: 9.951917070002835
Epoch: 3524, Batch Gradient Norm after: 9.951917070002835
Epoch 3525/10000, Prediction Accuracy = 61.504%, Loss = 0.4852424442768097
Epoch: 3525, Batch Gradient Norm: 8.183063367150075
Epoch: 3525, Batch Gradient Norm after: 8.183063367150075
Epoch 3526/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.47262648344039915
Epoch: 3526, Batch Gradient Norm: 7.8731196555013945
Epoch: 3526, Batch Gradient Norm after: 7.8731196555013945
Epoch 3527/10000, Prediction Accuracy = 61.507999999999996%, Loss = 0.47004948258399964
Epoch: 3527, Batch Gradient Norm: 13.438842663243259
Epoch: 3527, Batch Gradient Norm after: 13.438842663243259
Epoch 3528/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.5101054072380066
Epoch: 3528, Batch Gradient Norm: 8.561651049751926
Epoch: 3528, Batch Gradient Norm after: 8.561651049751926
Epoch 3529/10000, Prediction Accuracy = 61.574%, Loss = 0.47477005124092103
Epoch: 3529, Batch Gradient Norm: 9.651394232727316
Epoch: 3529, Batch Gradient Norm after: 9.651394232727316
Epoch 3530/10000, Prediction Accuracy = 61.624%, Loss = 0.4795516788959503
Epoch: 3530, Batch Gradient Norm: 11.540104652533445
Epoch: 3530, Batch Gradient Norm after: 11.540104652533445
Epoch 3531/10000, Prediction Accuracy = 61.61600000000001%, Loss = 0.4938418447971344
Epoch: 3531, Batch Gradient Norm: 11.7833953313456
Epoch: 3531, Batch Gradient Norm after: 11.7833953313456
Epoch 3532/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.5007561147212982
Epoch: 3532, Batch Gradient Norm: 6.3994599882221745
Epoch: 3532, Batch Gradient Norm after: 6.3994599882221745
Epoch 3533/10000, Prediction Accuracy = 61.628%, Loss = 0.4651004791259766
Epoch: 3533, Batch Gradient Norm: 5.4356792461743595
Epoch: 3533, Batch Gradient Norm after: 5.4356792461743595
Epoch 3534/10000, Prediction Accuracy = 61.6%, Loss = 0.46221628189086916
Epoch: 3534, Batch Gradient Norm: 7.982093824149889
Epoch: 3534, Batch Gradient Norm after: 7.982093824149889
Epoch 3535/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.47170974016189576
Epoch: 3535, Batch Gradient Norm: 9.428392284184197
Epoch: 3535, Batch Gradient Norm after: 9.428392284184197
Epoch 3536/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4776653826236725
Epoch: 3536, Batch Gradient Norm: 10.863644666403733
Epoch: 3536, Batch Gradient Norm after: 10.863644666403733
Epoch 3537/10000, Prediction Accuracy = 61.517999999999994%, Loss = 0.4894477307796478
Epoch: 3537, Batch Gradient Norm: 13.016224016409204
Epoch: 3537, Batch Gradient Norm after: 13.016224016409204
Epoch 3538/10000, Prediction Accuracy = 61.55799999999999%, Loss = 0.5065648138523102
Epoch: 3538, Batch Gradient Norm: 12.136553432490421
Epoch: 3538, Batch Gradient Norm after: 12.136553432490421
Epoch 3539/10000, Prediction Accuracy = 61.54%, Loss = 0.49933106303215025
Epoch: 3539, Batch Gradient Norm: 11.382186280157068
Epoch: 3539, Batch Gradient Norm after: 11.382186280157068
Epoch 3540/10000, Prediction Accuracy = 61.496%, Loss = 0.4894548237323761
Epoch: 3540, Batch Gradient Norm: 12.261412641561124
Epoch: 3540, Batch Gradient Norm after: 12.261412641561124
Epoch 3541/10000, Prediction Accuracy = 61.664%, Loss = 0.49846069812774657
Epoch: 3541, Batch Gradient Norm: 10.44249044606135
Epoch: 3541, Batch Gradient Norm after: 10.44249044606135
Epoch 3542/10000, Prediction Accuracy = 61.576%, Loss = 0.48631711602211
Epoch: 3542, Batch Gradient Norm: 9.762559559202954
Epoch: 3542, Batch Gradient Norm after: 9.762559559202954
Epoch 3543/10000, Prediction Accuracy = 61.624%, Loss = 0.4804571092128754
Epoch: 3543, Batch Gradient Norm: 9.071123345346326
Epoch: 3543, Batch Gradient Norm after: 9.071123345346326
Epoch 3544/10000, Prediction Accuracy = 61.7%, Loss = 0.4788621306419373
Epoch: 3544, Batch Gradient Norm: 7.078080259179439
Epoch: 3544, Batch Gradient Norm after: 7.078080259179439
Epoch 3545/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.46715937852859496
Epoch: 3545, Batch Gradient Norm: 8.241731609891684
Epoch: 3545, Batch Gradient Norm after: 8.241731609891684
Epoch 3546/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.4722164511680603
Epoch: 3546, Batch Gradient Norm: 13.114319544338048
Epoch: 3546, Batch Gradient Norm after: 13.114319544338048
Epoch 3547/10000, Prediction Accuracy = 61.576%, Loss = 0.5079714834690094
Epoch: 3547, Batch Gradient Norm: 10.916317186248012
Epoch: 3547, Batch Gradient Norm after: 10.916317186248012
Epoch 3548/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.4906062364578247
Epoch: 3548, Batch Gradient Norm: 9.31347941570234
Epoch: 3548, Batch Gradient Norm after: 9.31347941570234
Epoch 3549/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4812378525733948
Epoch: 3549, Batch Gradient Norm: 9.408217148494119
Epoch: 3549, Batch Gradient Norm after: 9.408217148494119
Epoch 3550/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.47948590517044065
Epoch: 3550, Batch Gradient Norm: 12.245145140155538
Epoch: 3550, Batch Gradient Norm after: 12.245145140155538
Epoch 3551/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.49860323071479795
Epoch: 3551, Batch Gradient Norm: 9.86056495089676
Epoch: 3551, Batch Gradient Norm after: 9.86056495089676
Epoch 3552/10000, Prediction Accuracy = 61.504000000000005%, Loss = 0.48433887362480166
Epoch: 3552, Batch Gradient Norm: 6.952751673990506
Epoch: 3552, Batch Gradient Norm after: 6.952751673990506
Epoch 3553/10000, Prediction Accuracy = 61.612%, Loss = 0.46601793766021726
Epoch: 3553, Batch Gradient Norm: 11.379014829439747
Epoch: 3553, Batch Gradient Norm after: 11.379014829439747
Epoch 3554/10000, Prediction Accuracy = 61.586%, Loss = 0.4941428482532501
Epoch: 3554, Batch Gradient Norm: 11.542020899512329
Epoch: 3554, Batch Gradient Norm after: 11.542020899512329
Epoch 3555/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.4928307354450226
Epoch: 3555, Batch Gradient Norm: 8.511900846355799
Epoch: 3555, Batch Gradient Norm after: 8.511900846355799
Epoch 3556/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.47326362133026123
Epoch: 3556, Batch Gradient Norm: 10.568505692922512
Epoch: 3556, Batch Gradient Norm after: 10.568505692922512
Epoch 3557/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.4842886090278625
Epoch: 3557, Batch Gradient Norm: 11.534385540253563
Epoch: 3557, Batch Gradient Norm after: 11.534385540253563
Epoch 3558/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.49436781406402586
Epoch: 3558, Batch Gradient Norm: 12.031307352545673
Epoch: 3558, Batch Gradient Norm after: 12.031307352545673
Epoch 3559/10000, Prediction Accuracy = 61.525999999999996%, Loss = 0.5030783712863922
Epoch: 3559, Batch Gradient Norm: 11.217510931693779
Epoch: 3559, Batch Gradient Norm after: 11.217510931693779
Epoch 3560/10000, Prediction Accuracy = 61.722%, Loss = 0.4919623374938965
Epoch: 3560, Batch Gradient Norm: 8.83470714525595
Epoch: 3560, Batch Gradient Norm after: 8.83470714525595
Epoch 3561/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.47921088337898254
Epoch: 3561, Batch Gradient Norm: 10.15574468392116
Epoch: 3561, Batch Gradient Norm after: 10.15574468392116
Epoch 3562/10000, Prediction Accuracy = 61.734%, Loss = 0.4871959209442139
Epoch: 3562, Batch Gradient Norm: 9.91986357749826
Epoch: 3562, Batch Gradient Norm after: 9.91986357749826
Epoch 3563/10000, Prediction Accuracy = 61.580000000000005%, Loss = 0.48606709241867063
Epoch: 3563, Batch Gradient Norm: 10.823881990043684
Epoch: 3563, Batch Gradient Norm after: 10.823881990043684
Epoch 3564/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.48900327682495115
Epoch: 3564, Batch Gradient Norm: 9.507569976316674
Epoch: 3564, Batch Gradient Norm after: 9.507569976316674
Epoch 3565/10000, Prediction Accuracy = 61.602%, Loss = 0.47824515104293824
Epoch: 3565, Batch Gradient Norm: 10.41533039527054
Epoch: 3565, Batch Gradient Norm after: 10.41533039527054
Epoch 3566/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.48378145694732666
Epoch: 3566, Batch Gradient Norm: 8.11252048164259
Epoch: 3566, Batch Gradient Norm after: 8.11252048164259
Epoch 3567/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.4704311490058899
Epoch: 3567, Batch Gradient Norm: 9.125208749900478
Epoch: 3567, Batch Gradient Norm after: 9.125208749900478
Epoch 3568/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.48119096159935
Epoch: 3568, Batch Gradient Norm: 10.089133316777703
Epoch: 3568, Batch Gradient Norm after: 10.089133316777703
Epoch 3569/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.4855843186378479
Epoch: 3569, Batch Gradient Norm: 11.842888979057674
Epoch: 3569, Batch Gradient Norm after: 11.842888979057674
Epoch 3570/10000, Prediction Accuracy = 61.59599999999999%, Loss = 0.4970896065235138
Epoch: 3570, Batch Gradient Norm: 7.673906204763671
Epoch: 3570, Batch Gradient Norm after: 7.673906204763671
Epoch 3571/10000, Prediction Accuracy = 61.6%, Loss = 0.4697374105453491
Epoch: 3571, Batch Gradient Norm: 10.661646426732712
Epoch: 3571, Batch Gradient Norm after: 10.661646426732712
Epoch 3572/10000, Prediction Accuracy = 61.636%, Loss = 0.4889648973941803
Epoch: 3572, Batch Gradient Norm: 12.879300383919462
Epoch: 3572, Batch Gradient Norm after: 12.879300383919462
Epoch 3573/10000, Prediction Accuracy = 61.598%, Loss = 0.5076325297355652
Epoch: 3573, Batch Gradient Norm: 12.750677640178537
Epoch: 3573, Batch Gradient Norm after: 12.750677640178537
Epoch 3574/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.5038678705692291
Epoch: 3574, Batch Gradient Norm: 11.920228423511857
Epoch: 3574, Batch Gradient Norm after: 11.920228423511857
Epoch 3575/10000, Prediction Accuracy = 61.562%, Loss = 0.5020274877548218
Epoch: 3575, Batch Gradient Norm: 7.0890437671870785
Epoch: 3575, Batch Gradient Norm after: 7.0890437671870785
Epoch 3576/10000, Prediction Accuracy = 61.65%, Loss = 0.4681246757507324
Epoch: 3576, Batch Gradient Norm: 9.159077582388326
Epoch: 3576, Batch Gradient Norm after: 9.159077582388326
Epoch 3577/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4795724809169769
Epoch: 3577, Batch Gradient Norm: 9.381784485987767
Epoch: 3577, Batch Gradient Norm after: 9.381784485987767
Epoch 3578/10000, Prediction Accuracy = 61.614%, Loss = 0.48226330280303953
Epoch: 3578, Batch Gradient Norm: 7.45400653412265
Epoch: 3578, Batch Gradient Norm after: 7.45400653412265
Epoch 3579/10000, Prediction Accuracy = 61.694%, Loss = 0.4700559377670288
Epoch: 3579, Batch Gradient Norm: 8.543459181795921
Epoch: 3579, Batch Gradient Norm after: 8.543459181795921
Epoch 3580/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.4773495554924011
Epoch: 3580, Batch Gradient Norm: 11.6990655955406
Epoch: 3580, Batch Gradient Norm after: 11.6990655955406
Epoch 3581/10000, Prediction Accuracy = 61.698%, Loss = 0.4948377013206482
Epoch: 3581, Batch Gradient Norm: 14.388321609910797
Epoch: 3581, Batch Gradient Norm after: 14.388321609910797
Epoch 3582/10000, Prediction Accuracy = 61.666%, Loss = 0.5165146589279175
Epoch: 3582, Batch Gradient Norm: 10.501346372709376
Epoch: 3582, Batch Gradient Norm after: 10.501346372709376
Epoch 3583/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.48447640538215636
Epoch: 3583, Batch Gradient Norm: 7.882242868779244
Epoch: 3583, Batch Gradient Norm after: 7.882242868779244
Epoch 3584/10000, Prediction Accuracy = 61.538%, Loss = 0.47365871667861936
Epoch: 3584, Batch Gradient Norm: 11.54677740068641
Epoch: 3584, Batch Gradient Norm after: 11.54677740068641
Epoch 3585/10000, Prediction Accuracy = 61.645999999999994%, Loss = 0.4945584237575531
Epoch: 3585, Batch Gradient Norm: 13.990823850399364
Epoch: 3585, Batch Gradient Norm after: 13.990823850399364
Epoch 3586/10000, Prediction Accuracy = 61.598%, Loss = 0.5200860857963562
Epoch: 3586, Batch Gradient Norm: 9.66955421740262
Epoch: 3586, Batch Gradient Norm after: 9.66955421740262
Epoch 3587/10000, Prediction Accuracy = 61.65%, Loss = 0.47986815571784974
Epoch: 3587, Batch Gradient Norm: 9.401760843364952
Epoch: 3587, Batch Gradient Norm after: 9.401760843364952
Epoch 3588/10000, Prediction Accuracy = 61.702%, Loss = 0.48156224489212035
Epoch: 3588, Batch Gradient Norm: 5.945524551742402
Epoch: 3588, Batch Gradient Norm after: 5.945524551742402
Epoch 3589/10000, Prediction Accuracy = 61.712%, Loss = 0.4600826859474182
Epoch: 3589, Batch Gradient Norm: 7.507487415910161
Epoch: 3589, Batch Gradient Norm after: 7.507487415910161
Epoch 3590/10000, Prediction Accuracy = 61.657999999999994%, Loss = 0.46584298014640807
Epoch: 3590, Batch Gradient Norm: 9.383598867546395
Epoch: 3590, Batch Gradient Norm after: 9.383598867546395
Epoch 3591/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.47784361243247986
Epoch: 3591, Batch Gradient Norm: 9.837442964467042
Epoch: 3591, Batch Gradient Norm after: 9.837442964467042
Epoch 3592/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4823829472064972
Epoch: 3592, Batch Gradient Norm: 10.11076932202254
Epoch: 3592, Batch Gradient Norm after: 10.11076932202254
Epoch 3593/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.4812816560268402
Epoch: 3593, Batch Gradient Norm: 12.613030132378942
Epoch: 3593, Batch Gradient Norm after: 12.613030132378942
Epoch 3594/10000, Prediction Accuracy = 61.552%, Loss = 0.5039313197135925
Epoch: 3594, Batch Gradient Norm: 12.761665632542615
Epoch: 3594, Batch Gradient Norm after: 12.761665632542615
Epoch 3595/10000, Prediction Accuracy = 61.712%, Loss = 0.5070296049118042
Epoch: 3595, Batch Gradient Norm: 9.681072599108587
Epoch: 3595, Batch Gradient Norm after: 9.681072599108587
Epoch 3596/10000, Prediction Accuracy = 61.6%, Loss = 0.4799662947654724
Epoch: 3596, Batch Gradient Norm: 11.246114670259562
Epoch: 3596, Batch Gradient Norm after: 11.246114670259562
Epoch 3597/10000, Prediction Accuracy = 61.576%, Loss = 0.48861989974975584
Epoch: 3597, Batch Gradient Norm: 9.989124607908233
Epoch: 3597, Batch Gradient Norm after: 9.989124607908233
Epoch 3598/10000, Prediction Accuracy = 61.60799999999999%, Loss = 0.4840627610683441
Epoch: 3598, Batch Gradient Norm: 10.11730000536038
Epoch: 3598, Batch Gradient Norm after: 10.11730000536038
Epoch 3599/10000, Prediction Accuracy = 61.572%, Loss = 0.48395431637763975
Epoch: 3599, Batch Gradient Norm: 13.05743611497838
Epoch: 3599, Batch Gradient Norm after: 13.05743611497838
Epoch 3600/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.5006397008895874
Epoch: 3600, Batch Gradient Norm: 11.462128124141364
Epoch: 3600, Batch Gradient Norm after: 11.462128124141364
Epoch 3601/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4904728651046753
Epoch: 3601, Batch Gradient Norm: 9.309664304782826
Epoch: 3601, Batch Gradient Norm after: 9.309664304782826
Epoch 3602/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4767026901245117
Epoch: 3602, Batch Gradient Norm: 8.625381698080602
Epoch: 3602, Batch Gradient Norm after: 8.625381698080602
Epoch 3603/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.47423615455627444
Epoch: 3603, Batch Gradient Norm: 8.898213690956547
Epoch: 3603, Batch Gradient Norm after: 8.898213690956547
Epoch 3604/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4758902430534363
Epoch: 3604, Batch Gradient Norm: 11.007466931988159
Epoch: 3604, Batch Gradient Norm after: 11.007466931988159
Epoch 3605/10000, Prediction Accuracy = 61.63000000000001%, Loss = 0.49153760075569153
Epoch: 3605, Batch Gradient Norm: 9.805083311046246
Epoch: 3605, Batch Gradient Norm after: 9.805083311046246
Epoch 3606/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.48386719822883606
Epoch: 3606, Batch Gradient Norm: 9.47679462794656
Epoch: 3606, Batch Gradient Norm after: 9.47679462794656
Epoch 3607/10000, Prediction Accuracy = 61.712%, Loss = 0.47617462277412415
Epoch: 3607, Batch Gradient Norm: 12.704932017663888
Epoch: 3607, Batch Gradient Norm after: 12.704932017663888
Epoch 3608/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.49887258410453794
Epoch: 3608, Batch Gradient Norm: 8.006448312137739
Epoch: 3608, Batch Gradient Norm after: 8.006448312137739
Epoch 3609/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.46889281272888184
Epoch: 3609, Batch Gradient Norm: 7.096085482842785
Epoch: 3609, Batch Gradient Norm after: 7.096085482842785
Epoch 3610/10000, Prediction Accuracy = 61.646%, Loss = 0.46478779911994933
Epoch: 3610, Batch Gradient Norm: 11.766362214659441
Epoch: 3610, Batch Gradient Norm after: 11.766362214659441
Epoch 3611/10000, Prediction Accuracy = 61.636%, Loss = 0.4937562167644501
Epoch: 3611, Batch Gradient Norm: 10.348596321181132
Epoch: 3611, Batch Gradient Norm after: 10.348596321181132
Epoch 3612/10000, Prediction Accuracy = 61.6%, Loss = 0.4846948742866516
Epoch: 3612, Batch Gradient Norm: 7.161704635034561
Epoch: 3612, Batch Gradient Norm after: 7.161704635034561
Epoch 3613/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.46662819385528564
Epoch: 3613, Batch Gradient Norm: 8.865916236757494
Epoch: 3613, Batch Gradient Norm after: 8.865916236757494
Epoch 3614/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.47662683129310607
Epoch: 3614, Batch Gradient Norm: 8.358468905554986
Epoch: 3614, Batch Gradient Norm after: 8.358468905554986
Epoch 3615/10000, Prediction Accuracy = 61.53799999999999%, Loss = 0.47410425543785095
Epoch: 3615, Batch Gradient Norm: 12.062762872291911
Epoch: 3615, Batch Gradient Norm after: 12.062762872291911
Epoch 3616/10000, Prediction Accuracy = 61.648%, Loss = 0.497005307674408
Epoch: 3616, Batch Gradient Norm: 14.74392408687849
Epoch: 3616, Batch Gradient Norm after: 14.74392408687849
Epoch 3617/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.5160677969455719
Epoch: 3617, Batch Gradient Norm: 13.463585252038282
Epoch: 3617, Batch Gradient Norm after: 13.463585252038282
Epoch 3618/10000, Prediction Accuracy = 61.592%, Loss = 0.5125768125057221
Epoch: 3618, Batch Gradient Norm: 7.9657448413054475
Epoch: 3618, Batch Gradient Norm after: 7.9657448413054475
Epoch 3619/10000, Prediction Accuracy = 61.60799999999999%, Loss = 0.4698538899421692
Epoch: 3619, Batch Gradient Norm: 8.924025377625846
Epoch: 3619, Batch Gradient Norm after: 8.924025377625846
Epoch 3620/10000, Prediction Accuracy = 61.626%, Loss = 0.4746218502521515
Epoch: 3620, Batch Gradient Norm: 10.724873721851116
Epoch: 3620, Batch Gradient Norm after: 10.724873721851116
Epoch 3621/10000, Prediction Accuracy = 61.6%, Loss = 0.486509644985199
Epoch: 3621, Batch Gradient Norm: 9.555734679700551
Epoch: 3621, Batch Gradient Norm after: 9.555734679700551
Epoch 3622/10000, Prediction Accuracy = 61.628%, Loss = 0.48089932799339297
Epoch: 3622, Batch Gradient Norm: 8.514620297107246
Epoch: 3622, Batch Gradient Norm after: 8.514620297107246
Epoch 3623/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4719637632369995
Epoch: 3623, Batch Gradient Norm: 8.252483148585538
Epoch: 3623, Batch Gradient Norm after: 8.252483148585538
Epoch 3624/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.46939598917961123
Epoch: 3624, Batch Gradient Norm: 9.029083151472346
Epoch: 3624, Batch Gradient Norm after: 9.029083151472346
Epoch 3625/10000, Prediction Accuracy = 61.6%, Loss = 0.4770287096500397
Epoch: 3625, Batch Gradient Norm: 11.905469360514655
Epoch: 3625, Batch Gradient Norm after: 11.905469360514655
Epoch 3626/10000, Prediction Accuracy = 61.688%, Loss = 0.4963411033153534
Epoch: 3626, Batch Gradient Norm: 12.512024427308026
Epoch: 3626, Batch Gradient Norm after: 12.512024427308026
Epoch 3627/10000, Prediction Accuracy = 61.678%, Loss = 0.4993481576442719
Epoch: 3627, Batch Gradient Norm: 9.580928954716228
Epoch: 3627, Batch Gradient Norm after: 9.580928954716228
Epoch 3628/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4804391860961914
Epoch: 3628, Batch Gradient Norm: 10.386316772449813
Epoch: 3628, Batch Gradient Norm after: 10.386316772449813
Epoch 3629/10000, Prediction Accuracy = 61.588%, Loss = 0.4841378152370453
Epoch: 3629, Batch Gradient Norm: 9.689016207647787
Epoch: 3629, Batch Gradient Norm after: 9.689016207647787
Epoch 3630/10000, Prediction Accuracy = 61.75%, Loss = 0.48198431730270386
Epoch: 3630, Batch Gradient Norm: 10.088114792101935
Epoch: 3630, Batch Gradient Norm after: 10.088114792101935
Epoch 3631/10000, Prediction Accuracy = 61.70400000000001%, Loss = 0.4790396451950073
Epoch: 3631, Batch Gradient Norm: 9.362956085234114
Epoch: 3631, Batch Gradient Norm after: 9.362956085234114
Epoch 3632/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.47845534086227415
Epoch: 3632, Batch Gradient Norm: 7.658417673475899
Epoch: 3632, Batch Gradient Norm after: 7.658417673475899
Epoch 3633/10000, Prediction Accuracy = 61.507999999999996%, Loss = 0.46689664721488955
Epoch: 3633, Batch Gradient Norm: 10.896989817647775
Epoch: 3633, Batch Gradient Norm after: 10.896989817647775
Epoch 3634/10000, Prediction Accuracy = 61.6%, Loss = 0.48753868937492373
Epoch: 3634, Batch Gradient Norm: 14.717426871676272
Epoch: 3634, Batch Gradient Norm after: 14.717426871676272
Epoch 3635/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.5178587555885314
Epoch: 3635, Batch Gradient Norm: 9.60608884777398
Epoch: 3635, Batch Gradient Norm after: 9.60608884777398
Epoch 3636/10000, Prediction Accuracy = 61.634%, Loss = 0.4774873197078705
Epoch: 3636, Batch Gradient Norm: 8.710572243914735
Epoch: 3636, Batch Gradient Norm after: 8.710572243914735
Epoch 3637/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.47353782057762145
Epoch: 3637, Batch Gradient Norm: 8.815660917414151
Epoch: 3637, Batch Gradient Norm after: 8.815660917414151
Epoch 3638/10000, Prediction Accuracy = 61.688%, Loss = 0.4740752220153809
Epoch: 3638, Batch Gradient Norm: 7.066680072706654
Epoch: 3638, Batch Gradient Norm after: 7.066680072706654
Epoch 3639/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.46461552381515503
Epoch: 3639, Batch Gradient Norm: 8.258612500469985
Epoch: 3639, Batch Gradient Norm after: 8.258612500469985
Epoch 3640/10000, Prediction Accuracy = 61.653999999999996%, Loss = 0.4687925696372986
Epoch: 3640, Batch Gradient Norm: 10.19030382461206
Epoch: 3640, Batch Gradient Norm after: 10.19030382461206
Epoch 3641/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.4800824761390686
Epoch: 3641, Batch Gradient Norm: 9.569765037357827
Epoch: 3641, Batch Gradient Norm after: 9.569765037357827
Epoch 3642/10000, Prediction Accuracy = 61.666%, Loss = 0.4758734941482544
Epoch: 3642, Batch Gradient Norm: 11.078500551324652
Epoch: 3642, Batch Gradient Norm after: 11.078500551324652
Epoch 3643/10000, Prediction Accuracy = 61.712%, Loss = 0.4841610431671143
Epoch: 3643, Batch Gradient Norm: 12.507082804410363
Epoch: 3643, Batch Gradient Norm after: 12.507082804410363
Epoch 3644/10000, Prediction Accuracy = 61.64000000000001%, Loss = 0.4997409522533417
Epoch: 3644, Batch Gradient Norm: 12.577238599588359
Epoch: 3644, Batch Gradient Norm after: 12.577238599588359
Epoch 3645/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.5014459133148194
Epoch: 3645, Batch Gradient Norm: 13.328117434230169
Epoch: 3645, Batch Gradient Norm after: 13.328117434230169
Epoch 3646/10000, Prediction Accuracy = 61.63599999999999%, Loss = 0.5024699449539185
Epoch: 3646, Batch Gradient Norm: 11.954071508762482
Epoch: 3646, Batch Gradient Norm after: 11.954071508762482
Epoch 3647/10000, Prediction Accuracy = 61.598%, Loss = 0.49611605405807496
Epoch: 3647, Batch Gradient Norm: 8.027577167101276
Epoch: 3647, Batch Gradient Norm after: 8.027577167101276
Epoch 3648/10000, Prediction Accuracy = 61.678%, Loss = 0.46998007893562316
Epoch: 3648, Batch Gradient Norm: 9.220899132869617
Epoch: 3648, Batch Gradient Norm after: 9.220899132869617
Epoch 3649/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.47741190791130067
Epoch: 3649, Batch Gradient Norm: 11.888025893397428
Epoch: 3649, Batch Gradient Norm after: 11.888025893397428
Epoch 3650/10000, Prediction Accuracy = 61.66199999999999%, Loss = 0.4990579724311829
Epoch: 3650, Batch Gradient Norm: 9.447055758289375
Epoch: 3650, Batch Gradient Norm after: 9.447055758289375
Epoch 3651/10000, Prediction Accuracy = 61.65200000000001%, Loss = 0.47785653471946715
Epoch: 3651, Batch Gradient Norm: 8.997524711763644
Epoch: 3651, Batch Gradient Norm after: 8.997524711763644
Epoch 3652/10000, Prediction Accuracy = 61.6%, Loss = 0.48140001893043516
Epoch: 3652, Batch Gradient Norm: 9.193273612573574
Epoch: 3652, Batch Gradient Norm after: 9.193273612573574
Epoch 3653/10000, Prediction Accuracy = 61.64200000000001%, Loss = 0.4750441491603851
Epoch: 3653, Batch Gradient Norm: 7.254667890454703
Epoch: 3653, Batch Gradient Norm after: 7.254667890454703
Epoch 3654/10000, Prediction Accuracy = 61.626%, Loss = 0.4644178509712219
Epoch: 3654, Batch Gradient Norm: 9.540307604698715
Epoch: 3654, Batch Gradient Norm after: 9.540307604698715
Epoch 3655/10000, Prediction Accuracy = 61.548%, Loss = 0.47736331820487976
Epoch: 3655, Batch Gradient Norm: 8.682040264057733
Epoch: 3655, Batch Gradient Norm after: 8.682040264057733
Epoch 3656/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.47215449810028076
Epoch: 3656, Batch Gradient Norm: 7.316612978646818
Epoch: 3656, Batch Gradient Norm after: 7.316612978646818
Epoch 3657/10000, Prediction Accuracy = 61.646%, Loss = 0.46341983079910276
Epoch: 3657, Batch Gradient Norm: 13.93677381465054
Epoch: 3657, Batch Gradient Norm after: 13.93677381465054
Epoch 3658/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.5122987627983093
Epoch: 3658, Batch Gradient Norm: 12.927879915132763
Epoch: 3658, Batch Gradient Norm after: 12.927879915132763
Epoch 3659/10000, Prediction Accuracy = 61.688%, Loss = 0.5047626614570617
Epoch: 3659, Batch Gradient Norm: 9.593429279139013
Epoch: 3659, Batch Gradient Norm after: 9.593429279139013
Epoch 3660/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.47624478340148924
Epoch: 3660, Batch Gradient Norm: 10.16262838672368
Epoch: 3660, Batch Gradient Norm after: 10.16262838672368
Epoch 3661/10000, Prediction Accuracy = 61.536%, Loss = 0.4846509754657745
Epoch: 3661, Batch Gradient Norm: 7.312384528168182
Epoch: 3661, Batch Gradient Norm after: 7.312384528168182
Epoch 3662/10000, Prediction Accuracy = 61.682%, Loss = 0.4643666923046112
Epoch: 3662, Batch Gradient Norm: 11.510889488564667
Epoch: 3662, Batch Gradient Norm after: 11.510889488564667
Epoch 3663/10000, Prediction Accuracy = 61.584%, Loss = 0.4954787611961365
Epoch: 3663, Batch Gradient Norm: 9.011506081118338
Epoch: 3663, Batch Gradient Norm after: 9.011506081118338
Epoch 3664/10000, Prediction Accuracy = 61.638%, Loss = 0.47542819380760193
Epoch: 3664, Batch Gradient Norm: 9.794546981354642
Epoch: 3664, Batch Gradient Norm after: 9.794546981354642
Epoch 3665/10000, Prediction Accuracy = 61.678%, Loss = 0.4799058258533478
Epoch: 3665, Batch Gradient Norm: 9.96540640199684
Epoch: 3665, Batch Gradient Norm after: 9.96540640199684
Epoch 3666/10000, Prediction Accuracy = 61.686%, Loss = 0.47930868268013
Epoch: 3666, Batch Gradient Norm: 11.39885696090265
Epoch: 3666, Batch Gradient Norm after: 11.39885696090265
Epoch 3667/10000, Prediction Accuracy = 61.6%, Loss = 0.48867976665496826
Epoch: 3667, Batch Gradient Norm: 10.870261990017264
Epoch: 3667, Batch Gradient Norm after: 10.870261990017264
Epoch 3668/10000, Prediction Accuracy = 61.652%, Loss = 0.48206421732902527
Epoch: 3668, Batch Gradient Norm: 9.755488111419268
Epoch: 3668, Batch Gradient Norm after: 9.755488111419268
Epoch 3669/10000, Prediction Accuracy = 61.678%, Loss = 0.47810239195823667
Epoch: 3669, Batch Gradient Norm: 10.837465456345072
Epoch: 3669, Batch Gradient Norm after: 10.837465456345072
Epoch 3670/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.48699731230735777
Epoch: 3670, Batch Gradient Norm: 10.590073909026295
Epoch: 3670, Batch Gradient Norm after: 10.590073909026295
Epoch 3671/10000, Prediction Accuracy = 61.77%, Loss = 0.48975165486335753
Epoch: 3671, Batch Gradient Norm: 8.493926321974806
Epoch: 3671, Batch Gradient Norm after: 8.493926321974806
Epoch 3672/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.47127676010131836
Epoch: 3672, Batch Gradient Norm: 12.932890647837056
Epoch: 3672, Batch Gradient Norm after: 12.932890647837056
Epoch 3673/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.5051427602767944
Epoch: 3673, Batch Gradient Norm: 8.222644672060776
Epoch: 3673, Batch Gradient Norm after: 8.222644672060776
Epoch 3674/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.46963684558868407
Epoch: 3674, Batch Gradient Norm: 7.002761575991176
Epoch: 3674, Batch Gradient Norm after: 7.002761575991176
Epoch 3675/10000, Prediction Accuracy = 61.593999999999994%, Loss = 0.4618724822998047
Epoch: 3675, Batch Gradient Norm: 9.961377496087971
Epoch: 3675, Batch Gradient Norm after: 9.961377496087971
Epoch 3676/10000, Prediction Accuracy = 61.646%, Loss = 0.47859286069869994
Epoch: 3676, Batch Gradient Norm: 14.878216593416147
Epoch: 3676, Batch Gradient Norm after: 14.878216593416147
Epoch 3677/10000, Prediction Accuracy = 61.646%, Loss = 0.5226581454277038
Epoch: 3677, Batch Gradient Norm: 11.323724085215277
Epoch: 3677, Batch Gradient Norm after: 11.323724085215277
Epoch 3678/10000, Prediction Accuracy = 61.64%, Loss = 0.4883157193660736
Epoch: 3678, Batch Gradient Norm: 11.775297717855315
Epoch: 3678, Batch Gradient Norm after: 11.775297717855315
Epoch 3679/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.49303005933761596
Epoch: 3679, Batch Gradient Norm: 11.937422362870631
Epoch: 3679, Batch Gradient Norm after: 11.937422362870631
Epoch 3680/10000, Prediction Accuracy = 61.541999999999994%, Loss = 0.49398269653320315
Epoch: 3680, Batch Gradient Norm: 10.167538671195045
Epoch: 3680, Batch Gradient Norm after: 10.167538671195045
Epoch 3681/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4809028685092926
Epoch: 3681, Batch Gradient Norm: 10.258373011603279
Epoch: 3681, Batch Gradient Norm after: 10.258373011603279
Epoch 3682/10000, Prediction Accuracy = 61.69%, Loss = 0.4816580057144165
Epoch: 3682, Batch Gradient Norm: 9.007175460460802
Epoch: 3682, Batch Gradient Norm after: 9.007175460460802
Epoch 3683/10000, Prediction Accuracy = 61.678%, Loss = 0.4721757769584656
Epoch: 3683, Batch Gradient Norm: 8.116617075314322
Epoch: 3683, Batch Gradient Norm after: 8.116617075314322
Epoch 3684/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.4713277220726013
Epoch: 3684, Batch Gradient Norm: 7.855830210411829
Epoch: 3684, Batch Gradient Norm after: 7.855830210411829
Epoch 3685/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46758833527565
Epoch: 3685, Batch Gradient Norm: 9.506172366902462
Epoch: 3685, Batch Gradient Norm after: 9.506172366902462
Epoch 3686/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.4742566287517548
Epoch: 3686, Batch Gradient Norm: 14.432182888827526
Epoch: 3686, Batch Gradient Norm after: 14.432182888827526
Epoch 3687/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.5167526960372925
Epoch: 3687, Batch Gradient Norm: 9.556370409193022
Epoch: 3687, Batch Gradient Norm after: 9.556370409193022
Epoch 3688/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4770133554935455
Epoch: 3688, Batch Gradient Norm: 6.651744411641072
Epoch: 3688, Batch Gradient Norm after: 6.651744411641072
Epoch 3689/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.45977885723114015
Epoch: 3689, Batch Gradient Norm: 10.043500729077321
Epoch: 3689, Batch Gradient Norm after: 10.043500729077321
Epoch 3690/10000, Prediction Accuracy = 61.746%, Loss = 0.4784586548805237
Epoch: 3690, Batch Gradient Norm: 9.18661924623348
Epoch: 3690, Batch Gradient Norm after: 9.18661924623348
Epoch 3691/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4738669753074646
Epoch: 3691, Batch Gradient Norm: 8.867531929039558
Epoch: 3691, Batch Gradient Norm after: 8.867531929039558
Epoch 3692/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.47276862859725954
Epoch: 3692, Batch Gradient Norm: 11.30651876835389
Epoch: 3692, Batch Gradient Norm after: 11.30651876835389
Epoch 3693/10000, Prediction Accuracy = 61.538%, Loss = 0.4935174882411957
Epoch: 3693, Batch Gradient Norm: 9.559756822084177
Epoch: 3693, Batch Gradient Norm after: 9.559756822084177
Epoch 3694/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.47958502173423767
Epoch: 3694, Batch Gradient Norm: 10.38726178045537
Epoch: 3694, Batch Gradient Norm after: 10.38726178045537
Epoch 3695/10000, Prediction Accuracy = 61.612%, Loss = 0.4805879831314087
Epoch: 3695, Batch Gradient Norm: 11.118387569637667
Epoch: 3695, Batch Gradient Norm after: 11.118387569637667
Epoch 3696/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4852830648422241
Epoch: 3696, Batch Gradient Norm: 10.496116828202588
Epoch: 3696, Batch Gradient Norm after: 10.496116828202588
Epoch 3697/10000, Prediction Accuracy = 61.674%, Loss = 0.4810021102428436
Epoch: 3697, Batch Gradient Norm: 10.29101792494217
Epoch: 3697, Batch Gradient Norm after: 10.29101792494217
Epoch 3698/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.48224611282348634
Epoch: 3698, Batch Gradient Norm: 9.055922718887233
Epoch: 3698, Batch Gradient Norm after: 9.055922718887233
Epoch 3699/10000, Prediction Accuracy = 61.564%, Loss = 0.4730009138584137
Epoch: 3699, Batch Gradient Norm: 11.450110689165417
Epoch: 3699, Batch Gradient Norm after: 11.450110689165417
Epoch 3700/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.49327440857887267
Epoch: 3700, Batch Gradient Norm: 11.570228083404858
Epoch: 3700, Batch Gradient Norm after: 11.570228083404858
Epoch 3701/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.49497637152671814
Epoch: 3701, Batch Gradient Norm: 8.716284616854237
Epoch: 3701, Batch Gradient Norm after: 8.716284616854237
Epoch 3702/10000, Prediction Accuracy = 61.814%, Loss = 0.46933296918869016
Epoch: 3702, Batch Gradient Norm: 7.797552710715812
Epoch: 3702, Batch Gradient Norm after: 7.797552710715812
Epoch 3703/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4633440554141998
Epoch: 3703, Batch Gradient Norm: 11.75838704603899
Epoch: 3703, Batch Gradient Norm after: 11.75838704603899
Epoch 3704/10000, Prediction Accuracy = 61.8%, Loss = 0.48806620240211485
Epoch: 3704, Batch Gradient Norm: 11.973209825932791
Epoch: 3704, Batch Gradient Norm after: 11.973209825932791
Epoch 3705/10000, Prediction Accuracy = 61.658%, Loss = 0.501084953546524
Epoch: 3705, Batch Gradient Norm: 7.442762666160253
Epoch: 3705, Batch Gradient Norm after: 7.442762666160253
Epoch 3706/10000, Prediction Accuracy = 61.64%, Loss = 0.46532405018806455
Epoch: 3706, Batch Gradient Norm: 8.845073205140581
Epoch: 3706, Batch Gradient Norm after: 8.845073205140581
Epoch 3707/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.47022852301597595
Epoch: 3707, Batch Gradient Norm: 8.739608121167953
Epoch: 3707, Batch Gradient Norm after: 8.739608121167953
Epoch 3708/10000, Prediction Accuracy = 61.56%, Loss = 0.4702632904052734
Epoch: 3708, Batch Gradient Norm: 11.368116487869981
Epoch: 3708, Batch Gradient Norm after: 11.368116487869981
Epoch 3709/10000, Prediction Accuracy = 61.646%, Loss = 0.48801481127738955
Epoch: 3709, Batch Gradient Norm: 12.92379016612583
Epoch: 3709, Batch Gradient Norm after: 12.92379016612583
Epoch 3710/10000, Prediction Accuracy = 61.55%, Loss = 0.5027014195919037
Epoch: 3710, Batch Gradient Norm: 11.121685195554532
Epoch: 3710, Batch Gradient Norm after: 11.121685195554532
Epoch 3711/10000, Prediction Accuracy = 61.652%, Loss = 0.4878139555454254
Epoch: 3711, Batch Gradient Norm: 9.473112593255498
Epoch: 3711, Batch Gradient Norm after: 9.473112593255498
Epoch 3712/10000, Prediction Accuracy = 61.674%, Loss = 0.47421411275863645
Epoch: 3712, Batch Gradient Norm: 11.192227181961325
Epoch: 3712, Batch Gradient Norm after: 11.192227181961325
Epoch 3713/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.4858254373073578
Epoch: 3713, Batch Gradient Norm: 12.02255027317101
Epoch: 3713, Batch Gradient Norm after: 12.02255027317101
Epoch 3714/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.49366248250007627
Epoch: 3714, Batch Gradient Norm: 8.047344518065199
Epoch: 3714, Batch Gradient Norm after: 8.047344518065199
Epoch 3715/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.4707452654838562
Epoch: 3715, Batch Gradient Norm: 7.431385767560818
Epoch: 3715, Batch Gradient Norm after: 7.431385767560818
Epoch 3716/10000, Prediction Accuracy = 61.726%, Loss = 0.4679480195045471
Epoch: 3716, Batch Gradient Norm: 9.12956814236514
Epoch: 3716, Batch Gradient Norm after: 9.12956814236514
Epoch 3717/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.47102317214012146
Epoch: 3717, Batch Gradient Norm: 10.919204861563507
Epoch: 3717, Batch Gradient Norm after: 10.919204861563507
Epoch 3718/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4866474986076355
Epoch: 3718, Batch Gradient Norm: 11.6418916502195
Epoch: 3718, Batch Gradient Norm after: 11.6418916502195
Epoch 3719/10000, Prediction Accuracy = 61.586%, Loss = 0.49196677207946776
Epoch: 3719, Batch Gradient Norm: 10.930454854241828
Epoch: 3719, Batch Gradient Norm after: 10.930454854241828
Epoch 3720/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.48397462964057925
Epoch: 3720, Batch Gradient Norm: 10.450201122443218
Epoch: 3720, Batch Gradient Norm after: 10.450201122443218
Epoch 3721/10000, Prediction Accuracy = 61.634%, Loss = 0.4818041503429413
Epoch: 3721, Batch Gradient Norm: 9.546684105144646
Epoch: 3721, Batch Gradient Norm after: 9.546684105144646
Epoch 3722/10000, Prediction Accuracy = 61.598%, Loss = 0.48126020431518557
Epoch: 3722, Batch Gradient Norm: 9.198941298679692
Epoch: 3722, Batch Gradient Norm after: 9.198941298679692
Epoch 3723/10000, Prediction Accuracy = 61.676%, Loss = 0.4723894715309143
Epoch: 3723, Batch Gradient Norm: 9.74572569273289
Epoch: 3723, Batch Gradient Norm after: 9.74572569273289
Epoch 3724/10000, Prediction Accuracy = 61.788%, Loss = 0.47552100419998167
Epoch: 3724, Batch Gradient Norm: 12.753136805417862
Epoch: 3724, Batch Gradient Norm after: 12.753136805417862
Epoch 3725/10000, Prediction Accuracy = 61.541999999999994%, Loss = 0.49873136878013613
Epoch: 3725, Batch Gradient Norm: 12.285346003391247
Epoch: 3725, Batch Gradient Norm after: 12.285346003391247
Epoch 3726/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4970630705356598
Epoch: 3726, Batch Gradient Norm: 11.152103532448637
Epoch: 3726, Batch Gradient Norm after: 11.152103532448637
Epoch 3727/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.4872961163520813
Epoch: 3727, Batch Gradient Norm: 11.064343415192534
Epoch: 3727, Batch Gradient Norm after: 11.064343415192534
Epoch 3728/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.4835402548313141
Epoch: 3728, Batch Gradient Norm: 8.175073079529945
Epoch: 3728, Batch Gradient Norm after: 8.175073079529945
Epoch 3729/10000, Prediction Accuracy = 61.626%, Loss = 0.46536403298377993
Epoch: 3729, Batch Gradient Norm: 8.30233284332115
Epoch: 3729, Batch Gradient Norm after: 8.30233284332115
Epoch 3730/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.4658079147338867
Epoch: 3730, Batch Gradient Norm: 11.943746678670472
Epoch: 3730, Batch Gradient Norm after: 11.943746678670472
Epoch 3731/10000, Prediction Accuracy = 61.664%, Loss = 0.49136167764663696
Epoch: 3731, Batch Gradient Norm: 8.705068588215221
Epoch: 3731, Batch Gradient Norm after: 8.705068588215221
Epoch 3732/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.46794024109840393
Epoch: 3732, Batch Gradient Norm: 8.865930126521985
Epoch: 3732, Batch Gradient Norm after: 8.865930126521985
Epoch 3733/10000, Prediction Accuracy = 61.56999999999999%, Loss = 0.46994969844818113
Epoch: 3733, Batch Gradient Norm: 7.322177010640366
Epoch: 3733, Batch Gradient Norm after: 7.322177010640366
Epoch 3734/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.46245277523994444
Epoch: 3734, Batch Gradient Norm: 7.745182162990487
Epoch: 3734, Batch Gradient Norm after: 7.745182162990487
Epoch 3735/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.46471131443977354
Epoch: 3735, Batch Gradient Norm: 10.717802439623942
Epoch: 3735, Batch Gradient Norm after: 10.717802439623942
Epoch 3736/10000, Prediction Accuracy = 61.55999999999999%, Loss = 0.4822995126247406
Epoch: 3736, Batch Gradient Norm: 12.112820804087285
Epoch: 3736, Batch Gradient Norm after: 12.112820804087285
Epoch 3737/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4928993880748749
Epoch: 3737, Batch Gradient Norm: 10.83941523467902
Epoch: 3737, Batch Gradient Norm after: 10.83941523467902
Epoch 3738/10000, Prediction Accuracy = 61.676%, Loss = 0.4801588475704193
Epoch: 3738, Batch Gradient Norm: 12.000350830679055
Epoch: 3738, Batch Gradient Norm after: 12.000350830679055
Epoch 3739/10000, Prediction Accuracy = 61.688%, Loss = 0.4901420533657074
Epoch: 3739, Batch Gradient Norm: 10.146338100706615
Epoch: 3739, Batch Gradient Norm after: 10.146338100706615
Epoch 3740/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4780584514141083
Epoch: 3740, Batch Gradient Norm: 8.88495994423941
Epoch: 3740, Batch Gradient Norm after: 8.88495994423941
Epoch 3741/10000, Prediction Accuracy = 61.734%, Loss = 0.4700543940067291
Epoch: 3741, Batch Gradient Norm: 10.925413914515701
Epoch: 3741, Batch Gradient Norm after: 10.925413914515701
Epoch 3742/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.482166051864624
Epoch: 3742, Batch Gradient Norm: 10.195204834397849
Epoch: 3742, Batch Gradient Norm after: 10.195204834397849
Epoch 3743/10000, Prediction Accuracy = 61.572%, Loss = 0.47846090197563174
Epoch: 3743, Batch Gradient Norm: 10.733953066496538
Epoch: 3743, Batch Gradient Norm after: 10.733953066496538
Epoch 3744/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.48373687267303467
Epoch: 3744, Batch Gradient Norm: 7.569944409166657
Epoch: 3744, Batch Gradient Norm after: 7.569944409166657
Epoch 3745/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.46373692750930784
Epoch: 3745, Batch Gradient Norm: 7.6763011207438785
Epoch: 3745, Batch Gradient Norm after: 7.6763011207438785
Epoch 3746/10000, Prediction Accuracy = 61.686%, Loss = 0.4642463982105255
Epoch: 3746, Batch Gradient Norm: 9.949510955952888
Epoch: 3746, Batch Gradient Norm after: 9.949510955952888
Epoch 3747/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4754116117954254
Epoch: 3747, Batch Gradient Norm: 11.125699249881398
Epoch: 3747, Batch Gradient Norm after: 11.125699249881398
Epoch 3748/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.48238754868507383
Epoch: 3748, Batch Gradient Norm: 14.750496774778881
Epoch: 3748, Batch Gradient Norm after: 14.750496774778881
Epoch 3749/10000, Prediction Accuracy = 61.624%, Loss = 0.5132358312606812
Epoch: 3749, Batch Gradient Norm: 11.60820123138988
Epoch: 3749, Batch Gradient Norm after: 11.60820123138988
Epoch 3750/10000, Prediction Accuracy = 61.63599999999999%, Loss = 0.493421870470047
Epoch: 3750, Batch Gradient Norm: 8.979427319788151
Epoch: 3750, Batch Gradient Norm after: 8.979427319788151
Epoch 3751/10000, Prediction Accuracy = 61.730000000000004%, Loss = 0.4736850142478943
Epoch: 3751, Batch Gradient Norm: 9.079093790367304
Epoch: 3751, Batch Gradient Norm after: 9.079093790367304
Epoch 3752/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4738833367824554
Epoch: 3752, Batch Gradient Norm: 7.27709728255577
Epoch: 3752, Batch Gradient Norm after: 7.27709728255577
Epoch 3753/10000, Prediction Accuracy = 61.622%, Loss = 0.46235243082046507
Epoch: 3753, Batch Gradient Norm: 5.4792482751888
Epoch: 3753, Batch Gradient Norm after: 5.4792482751888
Epoch 3754/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4548488676548004
Epoch: 3754, Batch Gradient Norm: 6.591015753904342
Epoch: 3754, Batch Gradient Norm after: 6.591015753904342
Epoch 3755/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.45792973041534424
Epoch: 3755, Batch Gradient Norm: 11.698998071990731
Epoch: 3755, Batch Gradient Norm after: 11.698998071990731
Epoch 3756/10000, Prediction Accuracy = 61.686%, Loss = 0.4869510352611542
Epoch: 3756, Batch Gradient Norm: 12.0409950393642
Epoch: 3756, Batch Gradient Norm after: 12.0409950393642
Epoch 3757/10000, Prediction Accuracy = 61.762%, Loss = 0.4892771303653717
Epoch: 3757, Batch Gradient Norm: 10.996542221517403
Epoch: 3757, Batch Gradient Norm after: 10.996542221517403
Epoch 3758/10000, Prediction Accuracy = 61.706%, Loss = 0.48244792222976685
Epoch: 3758, Batch Gradient Norm: 11.340271031612033
Epoch: 3758, Batch Gradient Norm after: 11.340271031612033
Epoch 3759/10000, Prediction Accuracy = 61.79%, Loss = 0.4874549448490143
Epoch: 3759, Batch Gradient Norm: 9.427765786132342
Epoch: 3759, Batch Gradient Norm after: 9.427765786132342
Epoch 3760/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4739190340042114
Epoch: 3760, Batch Gradient Norm: 9.381033421621426
Epoch: 3760, Batch Gradient Norm after: 9.381033421621426
Epoch 3761/10000, Prediction Accuracy = 61.69%, Loss = 0.47464008927345275
Epoch: 3761, Batch Gradient Norm: 8.816520849345537
Epoch: 3761, Batch Gradient Norm after: 8.816520849345537
Epoch 3762/10000, Prediction Accuracy = 61.66199999999999%, Loss = 0.46892127990722654
Epoch: 3762, Batch Gradient Norm: 10.720908240952069
Epoch: 3762, Batch Gradient Norm after: 10.720908240952069
Epoch 3763/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.47756083607673644
Epoch: 3763, Batch Gradient Norm: 9.88559995576417
Epoch: 3763, Batch Gradient Norm after: 9.88559995576417
Epoch 3764/10000, Prediction Accuracy = 61.61400000000001%, Loss = 0.47389221787452696
Epoch: 3764, Batch Gradient Norm: 9.609745779313707
Epoch: 3764, Batch Gradient Norm after: 9.609745779313707
Epoch 3765/10000, Prediction Accuracy = 61.712%, Loss = 0.47308295369148257
Epoch: 3765, Batch Gradient Norm: 11.183853696206691
Epoch: 3765, Batch Gradient Norm after: 11.183853696206691
Epoch 3766/10000, Prediction Accuracy = 61.778000000000006%, Loss = 0.48553617000579835
Epoch: 3766, Batch Gradient Norm: 12.95246100440772
Epoch: 3766, Batch Gradient Norm after: 12.95246100440772
Epoch 3767/10000, Prediction Accuracy = 61.646%, Loss = 0.5009752810001373
Epoch: 3767, Batch Gradient Norm: 11.352087645466305
Epoch: 3767, Batch Gradient Norm after: 11.352087645466305
Epoch 3768/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.48646058440208434
Epoch: 3768, Batch Gradient Norm: 9.777444392131343
Epoch: 3768, Batch Gradient Norm after: 9.777444392131343
Epoch 3769/10000, Prediction Accuracy = 61.64%, Loss = 0.47988240122795106
Epoch: 3769, Batch Gradient Norm: 8.383526935273517
Epoch: 3769, Batch Gradient Norm after: 8.383526935273517
Epoch 3770/10000, Prediction Accuracy = 61.65%, Loss = 0.467191743850708
Epoch: 3770, Batch Gradient Norm: 7.302839746710239
Epoch: 3770, Batch Gradient Norm after: 7.302839746710239
Epoch 3771/10000, Prediction Accuracy = 61.628%, Loss = 0.46050001978874205
Epoch: 3771, Batch Gradient Norm: 9.6063016129223
Epoch: 3771, Batch Gradient Norm after: 9.6063016129223
Epoch 3772/10000, Prediction Accuracy = 61.722%, Loss = 0.47136685252189636
Epoch: 3772, Batch Gradient Norm: 10.895701453706431
Epoch: 3772, Batch Gradient Norm after: 10.895701453706431
Epoch 3773/10000, Prediction Accuracy = 61.604%, Loss = 0.4850107908248901
Epoch: 3773, Batch Gradient Norm: 13.480500940962216
Epoch: 3773, Batch Gradient Norm after: 13.480500940962216
Epoch 3774/10000, Prediction Accuracy = 61.62800000000001%, Loss = 0.5123946845531464
Epoch: 3774, Batch Gradient Norm: 12.186335596415958
Epoch: 3774, Batch Gradient Norm after: 12.186335596415958
Epoch 3775/10000, Prediction Accuracy = 61.602%, Loss = 0.49545019268989565
Epoch: 3775, Batch Gradient Norm: 8.874871518700488
Epoch: 3775, Batch Gradient Norm after: 8.874871518700488
Epoch 3776/10000, Prediction Accuracy = 61.843999999999994%, Loss = 0.4687077522277832
Epoch: 3776, Batch Gradient Norm: 7.641745677011563
Epoch: 3776, Batch Gradient Norm after: 7.641745677011563
Epoch 3777/10000, Prediction Accuracy = 61.67%, Loss = 0.4647055208683014
Epoch: 3777, Batch Gradient Norm: 9.410445920979722
Epoch: 3777, Batch Gradient Norm after: 9.410445920979722
Epoch 3778/10000, Prediction Accuracy = 61.556000000000004%, Loss = 0.470759904384613
Epoch: 3778, Batch Gradient Norm: 10.480182756127805
Epoch: 3778, Batch Gradient Norm after: 10.480182756127805
Epoch 3779/10000, Prediction Accuracy = 61.586%, Loss = 0.4794455409049988
Epoch: 3779, Batch Gradient Norm: 10.531102693169046
Epoch: 3779, Batch Gradient Norm after: 10.531102693169046
Epoch 3780/10000, Prediction Accuracy = 61.624%, Loss = 0.47711479663848877
Epoch: 3780, Batch Gradient Norm: 8.163884806758867
Epoch: 3780, Batch Gradient Norm after: 8.163884806758867
Epoch 3781/10000, Prediction Accuracy = 61.678%, Loss = 0.4623174786567688
Epoch: 3781, Batch Gradient Norm: 10.633895423974415
Epoch: 3781, Batch Gradient Norm after: 10.633895423974415
Epoch 3782/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.48061215281486513
Epoch: 3782, Batch Gradient Norm: 10.197442746384072
Epoch: 3782, Batch Gradient Norm after: 10.197442746384072
Epoch 3783/10000, Prediction Accuracy = 61.41200000000001%, Loss = 0.48187205791473386
Epoch: 3783, Batch Gradient Norm: 9.87010525418985
Epoch: 3783, Batch Gradient Norm after: 9.87010525418985
Epoch 3784/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.47568840980529786
Epoch: 3784, Batch Gradient Norm: 11.950643597033263
Epoch: 3784, Batch Gradient Norm after: 11.950643597033263
Epoch 3785/10000, Prediction Accuracy = 61.75%, Loss = 0.4903746902942657
Epoch: 3785, Batch Gradient Norm: 9.730957896386586
Epoch: 3785, Batch Gradient Norm after: 9.730957896386586
Epoch 3786/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.47342199087142944
Epoch: 3786, Batch Gradient Norm: 13.172700772771407
Epoch: 3786, Batch Gradient Norm after: 13.172700772771407
Epoch 3787/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.5006491422653199
Epoch: 3787, Batch Gradient Norm: 9.426280939416804
Epoch: 3787, Batch Gradient Norm after: 9.426280939416804
Epoch 3788/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4723868131637573
Epoch: 3788, Batch Gradient Norm: 9.042289983333983
Epoch: 3788, Batch Gradient Norm after: 9.042289983333983
Epoch 3789/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.47067385315895083
Epoch: 3789, Batch Gradient Norm: 9.447320278532043
Epoch: 3789, Batch Gradient Norm after: 9.447320278532043
Epoch 3790/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.46918954253196715
Epoch: 3790, Batch Gradient Norm: 11.10777240266871
Epoch: 3790, Batch Gradient Norm after: 11.10777240266871
Epoch 3791/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4810335040092468
Epoch: 3791, Batch Gradient Norm: 11.111141955786618
Epoch: 3791, Batch Gradient Norm after: 11.111141955786618
Epoch 3792/10000, Prediction Accuracy = 61.676%, Loss = 0.48619185090065004
Epoch: 3792, Batch Gradient Norm: 7.146065717191913
Epoch: 3792, Batch Gradient Norm after: 7.146065717191913
Epoch 3793/10000, Prediction Accuracy = 61.638%, Loss = 0.4598728835582733
Epoch: 3793, Batch Gradient Norm: 10.035088805649252
Epoch: 3793, Batch Gradient Norm after: 10.035088805649252
Epoch 3794/10000, Prediction Accuracy = 61.69%, Loss = 0.475691145658493
Epoch: 3794, Batch Gradient Norm: 12.465412395230825
Epoch: 3794, Batch Gradient Norm after: 12.465412395230825
Epoch 3795/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.4953725516796112
Epoch: 3795, Batch Gradient Norm: 8.791922931222773
Epoch: 3795, Batch Gradient Norm after: 8.791922931222773
Epoch 3796/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4714473605155945
Epoch: 3796, Batch Gradient Norm: 5.460293085275354
Epoch: 3796, Batch Gradient Norm after: 5.460293085275354
Epoch 3797/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.45212321877479555
Epoch: 3797, Batch Gradient Norm: 7.295335459613907
Epoch: 3797, Batch Gradient Norm after: 7.295335459613907
Epoch 3798/10000, Prediction Accuracy = 61.60999999999999%, Loss = 0.45891115069389343
Epoch: 3798, Batch Gradient Norm: 11.055673800360337
Epoch: 3798, Batch Gradient Norm after: 11.055673800360337
Epoch 3799/10000, Prediction Accuracy = 61.758%, Loss = 0.48031197786331176
Epoch: 3799, Batch Gradient Norm: 10.62470523896949
Epoch: 3799, Batch Gradient Norm after: 10.62470523896949
Epoch 3800/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.4816458821296692
Epoch: 3800, Batch Gradient Norm: 10.35673773120507
Epoch: 3800, Batch Gradient Norm after: 10.35673773120507
Epoch 3801/10000, Prediction Accuracy = 61.574%, Loss = 0.4807519793510437
Epoch: 3801, Batch Gradient Norm: 10.211603968275393
Epoch: 3801, Batch Gradient Norm after: 10.211603968275393
Epoch 3802/10000, Prediction Accuracy = 61.746%, Loss = 0.4782982885837555
Epoch: 3802, Batch Gradient Norm: 10.911262501485952
Epoch: 3802, Batch Gradient Norm after: 10.911262501485952
Epoch 3803/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.48338847756385805
Epoch: 3803, Batch Gradient Norm: 10.206969849481007
Epoch: 3803, Batch Gradient Norm after: 10.206969849481007
Epoch 3804/10000, Prediction Accuracy = 61.676%, Loss = 0.47821226716041565
Epoch: 3804, Batch Gradient Norm: 9.442395980559766
Epoch: 3804, Batch Gradient Norm after: 9.442395980559766
Epoch 3805/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.47152145504951476
Epoch: 3805, Batch Gradient Norm: 8.939598145013028
Epoch: 3805, Batch Gradient Norm after: 8.939598145013028
Epoch 3806/10000, Prediction Accuracy = 61.65%, Loss = 0.4692179083824158
Epoch: 3806, Batch Gradient Norm: 10.761808017817717
Epoch: 3806, Batch Gradient Norm after: 10.761808017817717
Epoch 3807/10000, Prediction Accuracy = 61.726%, Loss = 0.4828860878944397
Epoch: 3807, Batch Gradient Norm: 15.069129460282024
Epoch: 3807, Batch Gradient Norm after: 15.069129460282024
Epoch 3808/10000, Prediction Accuracy = 61.6%, Loss = 0.5144610643386841
Epoch: 3808, Batch Gradient Norm: 11.458459932672254
Epoch: 3808, Batch Gradient Norm after: 11.458459932672254
Epoch 3809/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.48628523349761965
Epoch: 3809, Batch Gradient Norm: 8.807333004006923
Epoch: 3809, Batch Gradient Norm after: 8.807333004006923
Epoch 3810/10000, Prediction Accuracy = 61.624%, Loss = 0.46714155077934266
Epoch: 3810, Batch Gradient Norm: 8.558278204537729
Epoch: 3810, Batch Gradient Norm after: 8.558278204537729
Epoch 3811/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4654815673828125
Epoch: 3811, Batch Gradient Norm: 9.004884182079488
Epoch: 3811, Batch Gradient Norm after: 9.004884182079488
Epoch 3812/10000, Prediction Accuracy = 61.628%, Loss = 0.4692835509777069
Epoch: 3812, Batch Gradient Norm: 10.508549228698042
Epoch: 3812, Batch Gradient Norm after: 10.508549228698042
Epoch 3813/10000, Prediction Accuracy = 61.612%, Loss = 0.47943212985992434
Epoch: 3813, Batch Gradient Norm: 9.32222002451242
Epoch: 3813, Batch Gradient Norm after: 9.32222002451242
Epoch 3814/10000, Prediction Accuracy = 61.624%, Loss = 0.4712133049964905
Epoch: 3814, Batch Gradient Norm: 10.673478604875946
Epoch: 3814, Batch Gradient Norm after: 10.673478604875946
Epoch 3815/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.48038833737373354
Epoch: 3815, Batch Gradient Norm: 11.01708329191203
Epoch: 3815, Batch Gradient Norm after: 11.01708329191203
Epoch 3816/10000, Prediction Accuracy = 61.72800000000001%, Loss = 0.4835191309452057
Epoch: 3816, Batch Gradient Norm: 10.327141703695455
Epoch: 3816, Batch Gradient Norm after: 10.327141703695455
Epoch 3817/10000, Prediction Accuracy = 61.766%, Loss = 0.47861788272857664
Epoch: 3817, Batch Gradient Norm: 9.328788050670585
Epoch: 3817, Batch Gradient Norm after: 9.328788050670585
Epoch 3818/10000, Prediction Accuracy = 61.56%, Loss = 0.47015332579612734
Epoch: 3818, Batch Gradient Norm: 8.744795000610512
Epoch: 3818, Batch Gradient Norm after: 8.744795000610512
Epoch 3819/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4658660709857941
Epoch: 3819, Batch Gradient Norm: 12.16306417924452
Epoch: 3819, Batch Gradient Norm after: 12.16306417924452
Epoch 3820/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4897440612316132
Epoch: 3820, Batch Gradient Norm: 8.908976893812621
Epoch: 3820, Batch Gradient Norm after: 8.908976893812621
Epoch 3821/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4689245045185089
Epoch: 3821, Batch Gradient Norm: 8.829595434639412
Epoch: 3821, Batch Gradient Norm after: 8.829595434639412
Epoch 3822/10000, Prediction Accuracy = 61.638%, Loss = 0.4676032841205597
Epoch: 3822, Batch Gradient Norm: 10.790480330309983
Epoch: 3822, Batch Gradient Norm after: 10.790480330309983
Epoch 3823/10000, Prediction Accuracy = 61.62199999999999%, Loss = 0.4817786872386932
Epoch: 3823, Batch Gradient Norm: 9.46852555455988
Epoch: 3823, Batch Gradient Norm after: 9.46852555455988
Epoch 3824/10000, Prediction Accuracy = 61.524%, Loss = 0.47534377574920655
Epoch: 3824, Batch Gradient Norm: 10.789938888509054
Epoch: 3824, Batch Gradient Norm after: 10.789938888509054
Epoch 3825/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4851537346839905
Epoch: 3825, Batch Gradient Norm: 9.849133554775905
Epoch: 3825, Batch Gradient Norm after: 9.849133554775905
Epoch 3826/10000, Prediction Accuracy = 61.709999999999994%, Loss = 0.47321895360946653
Epoch: 3826, Batch Gradient Norm: 11.604176325057992
Epoch: 3826, Batch Gradient Norm after: 11.604176325057992
Epoch 3827/10000, Prediction Accuracy = 61.688%, Loss = 0.4846338272094727
Epoch: 3827, Batch Gradient Norm: 12.12300698226136
Epoch: 3827, Batch Gradient Norm after: 12.12300698226136
Epoch 3828/10000, Prediction Accuracy = 61.713999999999984%, Loss = 0.49164255857467654
Epoch: 3828, Batch Gradient Norm: 8.355001307213383
Epoch: 3828, Batch Gradient Norm after: 8.355001307213383
Epoch 3829/10000, Prediction Accuracy = 61.65400000000001%, Loss = 0.46603673696517944
Epoch: 3829, Batch Gradient Norm: 9.435979658908021
Epoch: 3829, Batch Gradient Norm after: 9.435979658908021
Epoch 3830/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4717365562915802
Epoch: 3830, Batch Gradient Norm: 8.366878519178867
Epoch: 3830, Batch Gradient Norm after: 8.366878519178867
Epoch 3831/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4638322412967682
Epoch: 3831, Batch Gradient Norm: 10.251935244052172
Epoch: 3831, Batch Gradient Norm after: 10.251935244052172
Epoch 3832/10000, Prediction Accuracy = 61.686%, Loss = 0.4773277759552002
Epoch: 3832, Batch Gradient Norm: 9.258486327842885
Epoch: 3832, Batch Gradient Norm after: 9.258486327842885
Epoch 3833/10000, Prediction Accuracy = 61.588%, Loss = 0.46979616284370423
Epoch: 3833, Batch Gradient Norm: 12.760287318407167
Epoch: 3833, Batch Gradient Norm after: 12.760287318407167
Epoch 3834/10000, Prediction Accuracy = 61.574%, Loss = 0.5015164375305176
Epoch: 3834, Batch Gradient Norm: 12.657686942539145
Epoch: 3834, Batch Gradient Norm after: 12.657686942539145
Epoch 3835/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.49365052580833435
Epoch: 3835, Batch Gradient Norm: 8.771016745241285
Epoch: 3835, Batch Gradient Norm after: 8.771016745241285
Epoch 3836/10000, Prediction Accuracy = 61.782%, Loss = 0.4662656247615814
Epoch: 3836, Batch Gradient Norm: 10.238854691652783
Epoch: 3836, Batch Gradient Norm after: 10.238854691652783
Epoch 3837/10000, Prediction Accuracy = 61.612%, Loss = 0.47706579566001894
Epoch: 3837, Batch Gradient Norm: 9.20870920900397
Epoch: 3837, Batch Gradient Norm after: 9.20870920900397
Epoch 3838/10000, Prediction Accuracy = 61.75%, Loss = 0.4677541494369507
Epoch: 3838, Batch Gradient Norm: 9.462529742913036
Epoch: 3838, Batch Gradient Norm after: 9.462529742913036
Epoch 3839/10000, Prediction Accuracy = 61.626%, Loss = 0.4723477363586426
Epoch: 3839, Batch Gradient Norm: 7.606333243602557
Epoch: 3839, Batch Gradient Norm after: 7.606333243602557
Epoch 3840/10000, Prediction Accuracy = 61.77%, Loss = 0.4613876760005951
Epoch: 3840, Batch Gradient Norm: 7.131931719091248
Epoch: 3840, Batch Gradient Norm after: 7.131931719091248
Epoch 3841/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.4589492082595825
Epoch: 3841, Batch Gradient Norm: 9.29110399581376
Epoch: 3841, Batch Gradient Norm after: 9.29110399581376
Epoch 3842/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4691940724849701
Epoch: 3842, Batch Gradient Norm: 8.550979278887368
Epoch: 3842, Batch Gradient Norm after: 8.550979278887368
Epoch 3843/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.4631273984909058
Epoch: 3843, Batch Gradient Norm: 8.421957620051689
Epoch: 3843, Batch Gradient Norm after: 8.421957620051689
Epoch 3844/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.46533331274986267
Epoch: 3844, Batch Gradient Norm: 11.204025296455823
Epoch: 3844, Batch Gradient Norm after: 11.204025296455823
Epoch 3845/10000, Prediction Accuracy = 61.56999999999999%, Loss = 0.48732407093048097
Epoch: 3845, Batch Gradient Norm: 12.690057722554709
Epoch: 3845, Batch Gradient Norm after: 12.690057722554709
Epoch 3846/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4943871974945068
Epoch: 3846, Batch Gradient Norm: 13.693444830103305
Epoch: 3846, Batch Gradient Norm after: 13.693444830103305
Epoch 3847/10000, Prediction Accuracy = 61.624%, Loss = 0.49902047514915465
Epoch: 3847, Batch Gradient Norm: 10.253473607383517
Epoch: 3847, Batch Gradient Norm after: 10.253473607383517
Epoch 3848/10000, Prediction Accuracy = 61.6%, Loss = 0.47331631779670713
Epoch: 3848, Batch Gradient Norm: 8.683685385087513
Epoch: 3848, Batch Gradient Norm after: 8.683685385087513
Epoch 3849/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.46810616850852965
Epoch: 3849, Batch Gradient Norm: 9.260082313431662
Epoch: 3849, Batch Gradient Norm after: 9.260082313431662
Epoch 3850/10000, Prediction Accuracy = 61.602%, Loss = 0.47099071741104126
Epoch: 3850, Batch Gradient Norm: 10.998856656149844
Epoch: 3850, Batch Gradient Norm after: 10.998856656149844
Epoch 3851/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.4809964656829834
Epoch: 3851, Batch Gradient Norm: 13.069516484179909
Epoch: 3851, Batch Gradient Norm after: 13.069516484179909
Epoch 3852/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.4947623610496521
Epoch: 3852, Batch Gradient Norm: 11.060431686612947
Epoch: 3852, Batch Gradient Norm after: 11.060431686612947
Epoch 3853/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.48204387426376344
Epoch: 3853, Batch Gradient Norm: 11.146677380940762
Epoch: 3853, Batch Gradient Norm after: 11.146677380940762
Epoch 3854/10000, Prediction Accuracy = 61.598%, Loss = 0.48635571599006655
Epoch: 3854, Batch Gradient Norm: 8.854860703668507
Epoch: 3854, Batch Gradient Norm after: 8.854860703668507
Epoch 3855/10000, Prediction Accuracy = 61.726%, Loss = 0.46498237252235414
Epoch: 3855, Batch Gradient Norm: 11.02614514325589
Epoch: 3855, Batch Gradient Norm after: 11.02614514325589
Epoch 3856/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.480221563577652
Epoch: 3856, Batch Gradient Norm: 8.436848779813523
Epoch: 3856, Batch Gradient Norm after: 8.436848779813523
Epoch 3857/10000, Prediction Accuracy = 61.66199999999999%, Loss = 0.46358650326728823
Epoch: 3857, Batch Gradient Norm: 9.856385092502615
Epoch: 3857, Batch Gradient Norm after: 9.856385092502615
Epoch 3858/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.4750335991382599
Epoch: 3858, Batch Gradient Norm: 9.40587202930727
Epoch: 3858, Batch Gradient Norm after: 9.40587202930727
Epoch 3859/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.47021151185035703
Epoch: 3859, Batch Gradient Norm: 6.6011818353419995
Epoch: 3859, Batch Gradient Norm after: 6.6011818353419995
Epoch 3860/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.45609005093574523
Epoch: 3860, Batch Gradient Norm: 9.106856710692272
Epoch: 3860, Batch Gradient Norm after: 9.106856710692272
Epoch 3861/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4680484890937805
Epoch: 3861, Batch Gradient Norm: 12.875019139429815
Epoch: 3861, Batch Gradient Norm after: 12.875019139429815
Epoch 3862/10000, Prediction Accuracy = 61.628%, Loss = 0.49429833292961123
Epoch: 3862, Batch Gradient Norm: 9.549051486946501
Epoch: 3862, Batch Gradient Norm after: 9.549051486946501
Epoch 3863/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4713877499103546
Epoch: 3863, Batch Gradient Norm: 9.217093346555519
Epoch: 3863, Batch Gradient Norm after: 9.217093346555519
Epoch 3864/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.4696930587291718
Epoch: 3864, Batch Gradient Norm: 9.896361424421116
Epoch: 3864, Batch Gradient Norm after: 9.896361424421116
Epoch 3865/10000, Prediction Accuracy = 61.68000000000001%, Loss = 0.4785915553569794
Epoch: 3865, Batch Gradient Norm: 7.4740739560454035
Epoch: 3865, Batch Gradient Norm after: 7.4740739560454035
Epoch 3866/10000, Prediction Accuracy = 61.69%, Loss = 0.4611922919750214
Epoch: 3866, Batch Gradient Norm: 7.755384429236318
Epoch: 3866, Batch Gradient Norm after: 7.755384429236318
Epoch 3867/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.4594873249530792
Epoch: 3867, Batch Gradient Norm: 13.376490368771014
Epoch: 3867, Batch Gradient Norm after: 13.376490368771014
Epoch 3868/10000, Prediction Accuracy = 61.66400000000001%, Loss = 0.5041777074337006
Epoch: 3868, Batch Gradient Norm: 10.664764085578929
Epoch: 3868, Batch Gradient Norm after: 10.664764085578929
Epoch 3869/10000, Prediction Accuracy = 61.74000000000001%, Loss = 0.4805414378643036
Epoch: 3869, Batch Gradient Norm: 9.55483165897176
Epoch: 3869, Batch Gradient Norm after: 9.55483165897176
Epoch 3870/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.47197348475456236
Epoch: 3870, Batch Gradient Norm: 12.011853964915785
Epoch: 3870, Batch Gradient Norm after: 12.011853964915785
Epoch 3871/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4893940925598145
Epoch: 3871, Batch Gradient Norm: 7.8419114264947
Epoch: 3871, Batch Gradient Norm after: 7.8419114264947
Epoch 3872/10000, Prediction Accuracy = 61.648%, Loss = 0.4628995478153229
Epoch: 3872, Batch Gradient Norm: 8.223768589409158
Epoch: 3872, Batch Gradient Norm after: 8.223768589409158
Epoch 3873/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4616921961307526
Epoch: 3873, Batch Gradient Norm: 11.378408386768495
Epoch: 3873, Batch Gradient Norm after: 11.378408386768495
Epoch 3874/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.4832584083080292
Epoch: 3874, Batch Gradient Norm: 12.550522946619457
Epoch: 3874, Batch Gradient Norm after: 12.550522946619457
Epoch 3875/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.4881891429424286
Epoch: 3875, Batch Gradient Norm: 10.58491743940298
Epoch: 3875, Batch Gradient Norm after: 10.58491743940298
Epoch 3876/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4762350857257843
Epoch: 3876, Batch Gradient Norm: 10.325050925717317
Epoch: 3876, Batch Gradient Norm after: 10.325050925717317
Epoch 3877/10000, Prediction Accuracy = 61.592%, Loss = 0.4760712325572968
Epoch: 3877, Batch Gradient Norm: 11.444289608697803
Epoch: 3877, Batch Gradient Norm after: 11.444289608697803
Epoch 3878/10000, Prediction Accuracy = 61.58599999999999%, Loss = 0.48368735909461974
Epoch: 3878, Batch Gradient Norm: 12.354837666431825
Epoch: 3878, Batch Gradient Norm after: 12.354837666431825
Epoch 3879/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4891923785209656
Epoch: 3879, Batch Gradient Norm: 9.587879631488745
Epoch: 3879, Batch Gradient Norm after: 9.587879631488745
Epoch 3880/10000, Prediction Accuracy = 61.588%, Loss = 0.47116068601608274
Epoch: 3880, Batch Gradient Norm: 8.113756473598913
Epoch: 3880, Batch Gradient Norm after: 8.113756473598913
Epoch 3881/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.4645163297653198
Epoch: 3881, Batch Gradient Norm: 7.869854774203886
Epoch: 3881, Batch Gradient Norm after: 7.869854774203886
Epoch 3882/10000, Prediction Accuracy = 61.59000000000001%, Loss = 0.4609620749950409
Epoch: 3882, Batch Gradient Norm: 8.151655130569292
Epoch: 3882, Batch Gradient Norm after: 8.151655130569292
Epoch 3883/10000, Prediction Accuracy = 61.666%, Loss = 0.45996173620224
Epoch: 3883, Batch Gradient Norm: 10.29960894335125
Epoch: 3883, Batch Gradient Norm after: 10.29960894335125
Epoch 3884/10000, Prediction Accuracy = 61.726%, Loss = 0.47533204555511477
Epoch: 3884, Batch Gradient Norm: 10.672415843345624
Epoch: 3884, Batch Gradient Norm after: 10.672415843345624
Epoch 3885/10000, Prediction Accuracy = 61.638%, Loss = 0.4774340033531189
Epoch: 3885, Batch Gradient Norm: 10.914811260364555
Epoch: 3885, Batch Gradient Norm after: 10.914811260364555
Epoch 3886/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4771705329418182
Epoch: 3886, Batch Gradient Norm: 13.343525645131226
Epoch: 3886, Batch Gradient Norm after: 13.343525645131226
Epoch 3887/10000, Prediction Accuracy = 61.688%, Loss = 0.49991342425346375
Epoch: 3887, Batch Gradient Norm: 7.875445486535041
Epoch: 3887, Batch Gradient Norm after: 7.875445486535041
Epoch 3888/10000, Prediction Accuracy = 61.758%, Loss = 0.45882028341293335
Epoch: 3888, Batch Gradient Norm: 8.50763314380871
Epoch: 3888, Batch Gradient Norm after: 8.50763314380871
Epoch 3889/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4648978352546692
Epoch: 3889, Batch Gradient Norm: 9.57837780703996
Epoch: 3889, Batch Gradient Norm after: 9.57837780703996
Epoch 3890/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.47143030166625977
Epoch: 3890, Batch Gradient Norm: 9.064027269639816
Epoch: 3890, Batch Gradient Norm after: 9.064027269639816
Epoch 3891/10000, Prediction Accuracy = 61.702%, Loss = 0.4714669346809387
Epoch: 3891, Batch Gradient Norm: 7.861596619865772
Epoch: 3891, Batch Gradient Norm after: 7.861596619865772
Epoch 3892/10000, Prediction Accuracy = 61.636%, Loss = 0.46073302030563357
Epoch: 3892, Batch Gradient Norm: 8.895523379281984
Epoch: 3892, Batch Gradient Norm after: 8.895523379281984
Epoch 3893/10000, Prediction Accuracy = 61.62800000000001%, Loss = 0.4666444778442383
Epoch: 3893, Batch Gradient Norm: 10.576352564046326
Epoch: 3893, Batch Gradient Norm after: 10.576352564046326
Epoch 3894/10000, Prediction Accuracy = 61.738%, Loss = 0.47860092520713804
Epoch: 3894, Batch Gradient Norm: 13.22710441772521
Epoch: 3894, Batch Gradient Norm after: 13.22710441772521
Epoch 3895/10000, Prediction Accuracy = 61.65%, Loss = 0.498047286272049
Epoch: 3895, Batch Gradient Norm: 10.573032268058643
Epoch: 3895, Batch Gradient Norm after: 10.573032268058643
Epoch 3896/10000, Prediction Accuracy = 61.636%, Loss = 0.47637575268745425
Epoch: 3896, Batch Gradient Norm: 7.877383489422627
Epoch: 3896, Batch Gradient Norm after: 7.877383489422627
Epoch 3897/10000, Prediction Accuracy = 61.742%, Loss = 0.46283444166183474
Epoch: 3897, Batch Gradient Norm: 9.078675919197527
Epoch: 3897, Batch Gradient Norm after: 9.078675919197527
Epoch 3898/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.4661390960216522
Epoch: 3898, Batch Gradient Norm: 11.357437854100242
Epoch: 3898, Batch Gradient Norm after: 11.357437854100242
Epoch 3899/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.4784455060958862
Epoch: 3899, Batch Gradient Norm: 14.10275476316852
Epoch: 3899, Batch Gradient Norm after: 14.10275476316852
Epoch 3900/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.5015967667102814
Epoch: 3900, Batch Gradient Norm: 11.20000362745215
Epoch: 3900, Batch Gradient Norm after: 11.20000362745215
Epoch 3901/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4800765454769135
Epoch: 3901, Batch Gradient Norm: 9.839889609726908
Epoch: 3901, Batch Gradient Norm after: 9.839889609726908
Epoch 3902/10000, Prediction Accuracy = 61.576%, Loss = 0.4726312577724457
Epoch: 3902, Batch Gradient Norm: 10.169206057069548
Epoch: 3902, Batch Gradient Norm after: 10.169206057069548
Epoch 3903/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.47438742518424987
Epoch: 3903, Batch Gradient Norm: 10.143813656226655
Epoch: 3903, Batch Gradient Norm after: 10.143813656226655
Epoch 3904/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4750921368598938
Epoch: 3904, Batch Gradient Norm: 8.135849445278408
Epoch: 3904, Batch Gradient Norm after: 8.135849445278408
Epoch 3905/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.46243147253990174
Epoch: 3905, Batch Gradient Norm: 8.53012776503347
Epoch: 3905, Batch Gradient Norm after: 8.53012776503347
Epoch 3906/10000, Prediction Accuracy = 61.706%, Loss = 0.46129371523857116
Epoch: 3906, Batch Gradient Norm: 11.049155265239637
Epoch: 3906, Batch Gradient Norm after: 11.049155265239637
Epoch 3907/10000, Prediction Accuracy = 61.64%, Loss = 0.4818413555622101
Epoch: 3907, Batch Gradient Norm: 8.486179082892264
Epoch: 3907, Batch Gradient Norm after: 8.486179082892264
Epoch 3908/10000, Prediction Accuracy = 61.694%, Loss = 0.4615758180618286
Epoch: 3908, Batch Gradient Norm: 10.483570399948947
Epoch: 3908, Batch Gradient Norm after: 10.483570399948947
Epoch 3909/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.47724645137786864
Epoch: 3909, Batch Gradient Norm: 10.38444158984635
Epoch: 3909, Batch Gradient Norm after: 10.38444158984635
Epoch 3910/10000, Prediction Accuracy = 61.674%, Loss = 0.47402982115745546
Epoch: 3910, Batch Gradient Norm: 8.961970067429181
Epoch: 3910, Batch Gradient Norm after: 8.961970067429181
Epoch 3911/10000, Prediction Accuracy = 61.61%, Loss = 0.4638547897338867
Epoch: 3911, Batch Gradient Norm: 7.807939517417217
Epoch: 3911, Batch Gradient Norm after: 7.807939517417217
Epoch 3912/10000, Prediction Accuracy = 61.612%, Loss = 0.45977404713630676
Epoch: 3912, Batch Gradient Norm: 10.58521791470995
Epoch: 3912, Batch Gradient Norm after: 10.58521791470995
Epoch 3913/10000, Prediction Accuracy = 61.658%, Loss = 0.4747487008571625
Epoch: 3913, Batch Gradient Norm: 15.035022756400863
Epoch: 3913, Batch Gradient Norm after: 15.035022756400863
Epoch 3914/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.5104572951793671
Epoch: 3914, Batch Gradient Norm: 11.006813557444872
Epoch: 3914, Batch Gradient Norm after: 11.006813557444872
Epoch 3915/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.47762585878372193
Epoch: 3915, Batch Gradient Norm: 9.37090773399866
Epoch: 3915, Batch Gradient Norm after: 9.37090773399866
Epoch 3916/10000, Prediction Accuracy = 61.64%, Loss = 0.47101758122444154
Epoch: 3916, Batch Gradient Norm: 9.228047306161786
Epoch: 3916, Batch Gradient Norm after: 9.228047306161786
Epoch 3917/10000, Prediction Accuracy = 61.73%, Loss = 0.4673182725906372
Epoch: 3917, Batch Gradient Norm: 9.477748531533857
Epoch: 3917, Batch Gradient Norm after: 9.477748531533857
Epoch 3918/10000, Prediction Accuracy = 61.778%, Loss = 0.47034668922424316
Epoch: 3918, Batch Gradient Norm: 12.775594853272356
Epoch: 3918, Batch Gradient Norm after: 12.775594853272356
Epoch 3919/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4923920571804047
Epoch: 3919, Batch Gradient Norm: 10.346275760279141
Epoch: 3919, Batch Gradient Norm after: 10.346275760279141
Epoch 3920/10000, Prediction Accuracy = 61.71400000000001%, Loss = 0.4730996608734131
Epoch: 3920, Batch Gradient Norm: 10.395306195406478
Epoch: 3920, Batch Gradient Norm after: 10.395306195406478
Epoch 3921/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.47377747893333433
Epoch: 3921, Batch Gradient Norm: 9.137163027049139
Epoch: 3921, Batch Gradient Norm after: 9.137163027049139
Epoch 3922/10000, Prediction Accuracy = 61.638%, Loss = 0.4700464069843292
Epoch: 3922, Batch Gradient Norm: 8.680405633375216
Epoch: 3922, Batch Gradient Norm after: 8.680405633375216
Epoch 3923/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4636885464191437
Epoch: 3923, Batch Gradient Norm: 10.020838030941182
Epoch: 3923, Batch Gradient Norm after: 10.020838030941182
Epoch 3924/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.47168208360672
Epoch: 3924, Batch Gradient Norm: 10.123243608585751
Epoch: 3924, Batch Gradient Norm after: 10.123243608585751
Epoch 3925/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4753972291946411
Epoch: 3925, Batch Gradient Norm: 8.214514779047876
Epoch: 3925, Batch Gradient Norm after: 8.214514779047876
Epoch 3926/10000, Prediction Accuracy = 61.64000000000001%, Loss = 0.46215328574180603
Epoch: 3926, Batch Gradient Norm: 8.869794922948259
Epoch: 3926, Batch Gradient Norm after: 8.869794922948259
Epoch 3927/10000, Prediction Accuracy = 61.56%, Loss = 0.46443692445755
Epoch: 3927, Batch Gradient Norm: 9.312309393837525
Epoch: 3927, Batch Gradient Norm after: 9.312309393837525
Epoch 3928/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.46891591548919676
Epoch: 3928, Batch Gradient Norm: 11.990759757273274
Epoch: 3928, Batch Gradient Norm after: 11.990759757273274
Epoch 3929/10000, Prediction Accuracy = 61.718%, Loss = 0.48867077827453614
Epoch: 3929, Batch Gradient Norm: 11.668881355562952
Epoch: 3929, Batch Gradient Norm after: 11.668881355562952
Epoch 3930/10000, Prediction Accuracy = 61.714%, Loss = 0.4839012086391449
Epoch: 3930, Batch Gradient Norm: 11.181570607820852
Epoch: 3930, Batch Gradient Norm after: 11.181570607820852
Epoch 3931/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.48131003975868225
Epoch: 3931, Batch Gradient Norm: 9.010135193322343
Epoch: 3931, Batch Gradient Norm after: 9.010135193322343
Epoch 3932/10000, Prediction Accuracy = 61.70400000000001%, Loss = 0.46644644141197206
Epoch: 3932, Batch Gradient Norm: 7.62015752156034
Epoch: 3932, Batch Gradient Norm after: 7.62015752156034
Epoch 3933/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.45895227789878845
Epoch: 3933, Batch Gradient Norm: 11.735949245752957
Epoch: 3933, Batch Gradient Norm after: 11.735949245752957
Epoch 3934/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.4825595021247864
Epoch: 3934, Batch Gradient Norm: 11.222410608034957
Epoch: 3934, Batch Gradient Norm after: 11.222410608034957
Epoch 3935/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4776585459709167
Epoch: 3935, Batch Gradient Norm: 9.926722932293064
Epoch: 3935, Batch Gradient Norm after: 9.926722932293064
Epoch 3936/10000, Prediction Accuracy = 61.612%, Loss = 0.47180151343345644
Epoch: 3936, Batch Gradient Norm: 9.084581063348635
Epoch: 3936, Batch Gradient Norm after: 9.084581063348635
Epoch 3937/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.4675411403179169
Epoch: 3937, Batch Gradient Norm: 8.549415386600447
Epoch: 3937, Batch Gradient Norm after: 8.549415386600447
Epoch 3938/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.46450076103210447
Epoch: 3938, Batch Gradient Norm: 7.216597046839134
Epoch: 3938, Batch Gradient Norm after: 7.216597046839134
Epoch 3939/10000, Prediction Accuracy = 61.838%, Loss = 0.4561654984951019
Epoch: 3939, Batch Gradient Norm: 10.790764756069528
Epoch: 3939, Batch Gradient Norm after: 10.790764756069528
Epoch 3940/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.47710158824920657
Epoch: 3940, Batch Gradient Norm: 11.577181339638221
Epoch: 3940, Batch Gradient Norm after: 11.577181339638221
Epoch 3941/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.481103903055191
Epoch: 3941, Batch Gradient Norm: 12.319853785867783
Epoch: 3941, Batch Gradient Norm after: 12.319853785867783
Epoch 3942/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.48849881887435914
Epoch: 3942, Batch Gradient Norm: 9.168134180134292
Epoch: 3942, Batch Gradient Norm after: 9.168134180134292
Epoch 3943/10000, Prediction Accuracy = 61.65999999999999%, Loss = 0.4666286170482635
Epoch: 3943, Batch Gradient Norm: 9.243765950628308
Epoch: 3943, Batch Gradient Norm after: 9.243765950628308
Epoch 3944/10000, Prediction Accuracy = 61.788%, Loss = 0.46551265716552737
Epoch: 3944, Batch Gradient Norm: 10.137034695593098
Epoch: 3944, Batch Gradient Norm after: 10.137034695593098
Epoch 3945/10000, Prediction Accuracy = 61.730000000000004%, Loss = 0.4709110915660858
Epoch: 3945, Batch Gradient Norm: 10.375989109387866
Epoch: 3945, Batch Gradient Norm after: 10.375989109387866
Epoch 3946/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.4727011501789093
Epoch: 3946, Batch Gradient Norm: 9.468870192566083
Epoch: 3946, Batch Gradient Norm after: 9.468870192566083
Epoch 3947/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.46708322763442994
Epoch: 3947, Batch Gradient Norm: 9.055420158100805
Epoch: 3947, Batch Gradient Norm after: 9.055420158100805
Epoch 3948/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.46605634689331055
Epoch: 3948, Batch Gradient Norm: 9.207235819411157
Epoch: 3948, Batch Gradient Norm after: 9.207235819411157
Epoch 3949/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4665906488895416
Epoch: 3949, Batch Gradient Norm: 11.352291221211466
Epoch: 3949, Batch Gradient Norm after: 11.352291221211466
Epoch 3950/10000, Prediction Accuracy = 61.658%, Loss = 0.4782304704189301
Epoch: 3950, Batch Gradient Norm: 16.38465917519602
Epoch: 3950, Batch Gradient Norm after: 16.38465917519602
Epoch 3951/10000, Prediction Accuracy = 61.636%, Loss = 0.524689257144928
Epoch: 3951, Batch Gradient Norm: 12.201730387941756
Epoch: 3951, Batch Gradient Norm after: 12.201730387941756
Epoch 3952/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.48935150504112246
Epoch: 3952, Batch Gradient Norm: 8.532516869925796
Epoch: 3952, Batch Gradient Norm after: 8.532516869925796
Epoch 3953/10000, Prediction Accuracy = 61.706%, Loss = 0.46368765234947207
Epoch: 3953, Batch Gradient Norm: 7.858280833067442
Epoch: 3953, Batch Gradient Norm after: 7.858280833067442
Epoch 3954/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4621832609176636
Epoch: 3954, Batch Gradient Norm: 8.695318597683164
Epoch: 3954, Batch Gradient Norm after: 8.695318597683164
Epoch 3955/10000, Prediction Accuracy = 61.58%, Loss = 0.4624038636684418
Epoch: 3955, Batch Gradient Norm: 9.571135831655356
Epoch: 3955, Batch Gradient Norm after: 9.571135831655356
Epoch 3956/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.469935142993927
Epoch: 3956, Batch Gradient Norm: 10.717781250133
Epoch: 3956, Batch Gradient Norm after: 10.717781250133
Epoch 3957/10000, Prediction Accuracy = 61.63000000000001%, Loss = 0.47554941177368165
Epoch: 3957, Batch Gradient Norm: 10.000827465367758
Epoch: 3957, Batch Gradient Norm after: 10.000827465367758
Epoch 3958/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.47084816098213195
Epoch: 3958, Batch Gradient Norm: 11.123206508634862
Epoch: 3958, Batch Gradient Norm after: 11.123206508634862
Epoch 3959/10000, Prediction Accuracy = 61.592%, Loss = 0.47931285500526427
Epoch: 3959, Batch Gradient Norm: 8.619236913479545
Epoch: 3959, Batch Gradient Norm after: 8.619236913479545
Epoch 3960/10000, Prediction Accuracy = 61.67%, Loss = 0.46163876056671144
Epoch: 3960, Batch Gradient Norm: 10.630234797802087
Epoch: 3960, Batch Gradient Norm after: 10.630234797802087
Epoch 3961/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.4796965181827545
Epoch: 3961, Batch Gradient Norm: 9.844383889506334
Epoch: 3961, Batch Gradient Norm after: 9.844383889506334
Epoch 3962/10000, Prediction Accuracy = 61.71%, Loss = 0.46944350004196167
Epoch: 3962, Batch Gradient Norm: 11.24496116064465
Epoch: 3962, Batch Gradient Norm after: 11.24496116064465
Epoch 3963/10000, Prediction Accuracy = 61.754%, Loss = 0.4793867528438568
Epoch: 3963, Batch Gradient Norm: 11.180728439972222
Epoch: 3963, Batch Gradient Norm after: 11.180728439972222
Epoch 3964/10000, Prediction Accuracy = 61.69%, Loss = 0.48175262212753295
Epoch: 3964, Batch Gradient Norm: 9.53684980987716
Epoch: 3964, Batch Gradient Norm after: 9.53684980987716
Epoch 3965/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.46822077631950376
Epoch: 3965, Batch Gradient Norm: 8.95011654521507
Epoch: 3965, Batch Gradient Norm after: 8.95011654521507
Epoch 3966/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.4656355619430542
Epoch: 3966, Batch Gradient Norm: 9.572739280889376
Epoch: 3966, Batch Gradient Norm after: 9.572739280889376
Epoch 3967/10000, Prediction Accuracy = 61.726%, Loss = 0.4676870822906494
Epoch: 3967, Batch Gradient Norm: 7.7431884794689125
Epoch: 3967, Batch Gradient Norm after: 7.7431884794689125
Epoch 3968/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.45933655500411985
Epoch: 3968, Batch Gradient Norm: 8.292139893514442
Epoch: 3968, Batch Gradient Norm after: 8.292139893514442
Epoch 3969/10000, Prediction Accuracy = 61.592000000000006%, Loss = 0.46225849986076356
Epoch: 3969, Batch Gradient Norm: 10.44360790287294
Epoch: 3969, Batch Gradient Norm after: 10.44360790287294
Epoch 3970/10000, Prediction Accuracy = 61.664%, Loss = 0.4748396098613739
Epoch: 3970, Batch Gradient Norm: 10.19048599991032
Epoch: 3970, Batch Gradient Norm after: 10.19048599991032
Epoch 3971/10000, Prediction Accuracy = 61.674%, Loss = 0.47177661657333375
Epoch: 3971, Batch Gradient Norm: 9.025343021145213
Epoch: 3971, Batch Gradient Norm after: 9.025343021145213
Epoch 3972/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.4666322946548462
Epoch: 3972, Batch Gradient Norm: 11.80393318694329
Epoch: 3972, Batch Gradient Norm after: 11.80393318694329
Epoch 3973/10000, Prediction Accuracy = 61.676%, Loss = 0.479610276222229
Epoch: 3973, Batch Gradient Norm: 12.357408840950095
Epoch: 3973, Batch Gradient Norm after: 12.357408840950095
Epoch 3974/10000, Prediction Accuracy = 61.862%, Loss = 0.48549301624298097
Epoch: 3974, Batch Gradient Norm: 11.576611670789877
Epoch: 3974, Batch Gradient Norm after: 11.576611670789877
Epoch 3975/10000, Prediction Accuracy = 61.604%, Loss = 0.48586466908454895
Epoch: 3975, Batch Gradient Norm: 7.950335749276773
Epoch: 3975, Batch Gradient Norm after: 7.950335749276773
Epoch 3976/10000, Prediction Accuracy = 61.826%, Loss = 0.4596548616886139
Epoch: 3976, Batch Gradient Norm: 10.013829876059276
Epoch: 3976, Batch Gradient Norm after: 10.013829876059276
Epoch 3977/10000, Prediction Accuracy = 61.738%, Loss = 0.47334460616111756
Epoch: 3977, Batch Gradient Norm: 10.3275664644787
Epoch: 3977, Batch Gradient Norm after: 10.3275664644787
Epoch 3978/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4735336899757385
Epoch: 3978, Batch Gradient Norm: 4.892685838358777
Epoch: 3978, Batch Gradient Norm after: 4.892685838358777
Epoch 3979/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.4439892590045929
Epoch: 3979, Batch Gradient Norm: 11.150502832748005
Epoch: 3979, Batch Gradient Norm after: 11.150502832748005
Epoch 3980/10000, Prediction Accuracy = 61.746%, Loss = 0.47744224071502683
Epoch: 3980, Batch Gradient Norm: 12.27688292119131
Epoch: 3980, Batch Gradient Norm after: 12.27688292119131
Epoch 3981/10000, Prediction Accuracy = 61.638%, Loss = 0.4900834858417511
Epoch: 3981, Batch Gradient Norm: 10.565931722019299
Epoch: 3981, Batch Gradient Norm after: 10.565931722019299
Epoch 3982/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.47945478558540344
Epoch: 3982, Batch Gradient Norm: 9.748766013439665
Epoch: 3982, Batch Gradient Norm after: 9.748766013439665
Epoch 3983/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.47096306681632993
Epoch: 3983, Batch Gradient Norm: 9.045257291824583
Epoch: 3983, Batch Gradient Norm after: 9.045257291824583
Epoch 3984/10000, Prediction Accuracy = 61.7%, Loss = 0.4645368933677673
Epoch: 3984, Batch Gradient Norm: 10.394526936244025
Epoch: 3984, Batch Gradient Norm after: 10.394526936244025
Epoch 3985/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.47198857069015504
Epoch: 3985, Batch Gradient Norm: 13.3840997614952
Epoch: 3985, Batch Gradient Norm after: 13.3840997614952
Epoch 3986/10000, Prediction Accuracy = 61.686%, Loss = 0.49516710042953493
Epoch: 3986, Batch Gradient Norm: 8.33173708502501
Epoch: 3986, Batch Gradient Norm after: 8.33173708502501
Epoch 3987/10000, Prediction Accuracy = 61.762%, Loss = 0.4620271325111389
Epoch: 3987, Batch Gradient Norm: 8.404603032496434
Epoch: 3987, Batch Gradient Norm after: 8.404603032496434
Epoch 3988/10000, Prediction Accuracy = 61.718%, Loss = 0.46150965094566343
Epoch: 3988, Batch Gradient Norm: 9.219486417134778
Epoch: 3988, Batch Gradient Norm after: 9.219486417134778
Epoch 3989/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.46552855968475343
Epoch: 3989, Batch Gradient Norm: 9.725873786645368
Epoch: 3989, Batch Gradient Norm after: 9.725873786645368
Epoch 3990/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4679310739040375
Epoch: 3990, Batch Gradient Norm: 13.23717661902076
Epoch: 3990, Batch Gradient Norm after: 13.23717661902076
Epoch 3991/10000, Prediction Accuracy = 61.702%, Loss = 0.4913971066474915
Epoch: 3991, Batch Gradient Norm: 10.644377861413908
Epoch: 3991, Batch Gradient Norm after: 10.644377861413908
Epoch 3992/10000, Prediction Accuracy = 61.676%, Loss = 0.47240747809410094
Epoch: 3992, Batch Gradient Norm: 10.149544849323236
Epoch: 3992, Batch Gradient Norm after: 10.149544849323236
Epoch 3993/10000, Prediction Accuracy = 61.60799999999999%, Loss = 0.4719749212265015
Epoch: 3993, Batch Gradient Norm: 11.530384601619609
Epoch: 3993, Batch Gradient Norm after: 11.530384601619609
Epoch 3994/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.48249560594558716
Epoch: 3994, Batch Gradient Norm: 8.940397807274163
Epoch: 3994, Batch Gradient Norm after: 8.940397807274163
Epoch 3995/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4645027041435242
Epoch: 3995, Batch Gradient Norm: 11.39064314119563
Epoch: 3995, Batch Gradient Norm after: 11.39064314119563
Epoch 3996/10000, Prediction Accuracy = 61.562%, Loss = 0.4782457172870636
Epoch: 3996, Batch Gradient Norm: 11.603815427555787
Epoch: 3996, Batch Gradient Norm after: 11.603815427555787
Epoch 3997/10000, Prediction Accuracy = 61.496%, Loss = 0.4812979280948639
Epoch: 3997, Batch Gradient Norm: 8.911569209225966
Epoch: 3997, Batch Gradient Norm after: 8.911569209225966
Epoch 3998/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4619102418422699
Epoch: 3998, Batch Gradient Norm: 7.260962017258159
Epoch: 3998, Batch Gradient Norm after: 7.260962017258159
Epoch 3999/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.45464600920677184
Epoch: 3999, Batch Gradient Norm: 9.109778108092359
Epoch: 3999, Batch Gradient Norm after: 9.109778108092359
Epoch 4000/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4639775812625885
Epoch: 4000, Batch Gradient Norm: 9.45779916908019
Epoch: 4000, Batch Gradient Norm after: 9.45779916908019
Epoch 4001/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.4676676273345947
Epoch: 4001, Batch Gradient Norm: 8.951024786908354
Epoch: 4001, Batch Gradient Norm after: 8.951024786908354
Epoch 4002/10000, Prediction Accuracy = 61.624%, Loss = 0.46552339792251585
Epoch: 4002, Batch Gradient Norm: 8.478075910729638
Epoch: 4002, Batch Gradient Norm after: 8.478075910729638
Epoch 4003/10000, Prediction Accuracy = 61.66600000000001%, Loss = 0.4616118252277374
Epoch: 4003, Batch Gradient Norm: 7.800064823801466
Epoch: 4003, Batch Gradient Norm after: 7.800064823801466
Epoch 4004/10000, Prediction Accuracy = 61.698%, Loss = 0.4549180805683136
Epoch: 4004, Batch Gradient Norm: 11.259244483629066
Epoch: 4004, Batch Gradient Norm after: 11.259244483629066
Epoch 4005/10000, Prediction Accuracy = 61.702%, Loss = 0.4757546901702881
Epoch: 4005, Batch Gradient Norm: 11.608365509892705
Epoch: 4005, Batch Gradient Norm after: 11.608365509892705
Epoch 4006/10000, Prediction Accuracy = 61.674%, Loss = 0.47958675026893616
Epoch: 4006, Batch Gradient Norm: 10.085100714848963
Epoch: 4006, Batch Gradient Norm after: 10.085100714848963
Epoch 4007/10000, Prediction Accuracy = 61.81%, Loss = 0.468405282497406
Epoch: 4007, Batch Gradient Norm: 9.378476470948174
Epoch: 4007, Batch Gradient Norm after: 9.378476470948174
Epoch 4008/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.46674893498420716
Epoch: 4008, Batch Gradient Norm: 9.673433151581554
Epoch: 4008, Batch Gradient Norm after: 9.673433151581554
Epoch 4009/10000, Prediction Accuracy = 61.64%, Loss = 0.468539696931839
Epoch: 4009, Batch Gradient Norm: 10.228898069860518
Epoch: 4009, Batch Gradient Norm after: 10.228898069860518
Epoch 4010/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.4692759871482849
Epoch: 4010, Batch Gradient Norm: 13.209804713191442
Epoch: 4010, Batch Gradient Norm after: 13.209804713191442
Epoch 4011/10000, Prediction Accuracy = 61.564%, Loss = 0.4948790490627289
Epoch: 4011, Batch Gradient Norm: 12.967286959021365
Epoch: 4011, Batch Gradient Norm after: 12.967286959021365
Epoch 4012/10000, Prediction Accuracy = 61.56600000000001%, Loss = 0.4898898422718048
Epoch: 4012, Batch Gradient Norm: 8.523460711693989
Epoch: 4012, Batch Gradient Norm after: 8.523460711693989
Epoch 4013/10000, Prediction Accuracy = 61.584%, Loss = 0.46065950989723203
Epoch: 4013, Batch Gradient Norm: 7.996112625135249
Epoch: 4013, Batch Gradient Norm after: 7.996112625135249
Epoch 4014/10000, Prediction Accuracy = 61.788%, Loss = 0.4590388834476471
Epoch: 4014, Batch Gradient Norm: 10.836241372100288
Epoch: 4014, Batch Gradient Norm after: 10.836241372100288
Epoch 4015/10000, Prediction Accuracy = 61.712%, Loss = 0.4798523008823395
Epoch: 4015, Batch Gradient Norm: 11.389130048362942
Epoch: 4015, Batch Gradient Norm after: 11.389130048362942
Epoch 4016/10000, Prediction Accuracy = 61.622%, Loss = 0.4831929922103882
Epoch: 4016, Batch Gradient Norm: 7.290015324570444
Epoch: 4016, Batch Gradient Norm after: 7.290015324570444
Epoch 4017/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4530709505081177
Epoch: 4017, Batch Gradient Norm: 8.155498901872638
Epoch: 4017, Batch Gradient Norm after: 8.155498901872638
Epoch 4018/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.45671176314353945
Epoch: 4018, Batch Gradient Norm: 11.08038305325208
Epoch: 4018, Batch Gradient Norm after: 11.08038305325208
Epoch 4019/10000, Prediction Accuracy = 61.658%, Loss = 0.4755121946334839
Epoch: 4019, Batch Gradient Norm: 11.185645787949195
Epoch: 4019, Batch Gradient Norm after: 11.185645787949195
Epoch 4020/10000, Prediction Accuracy = 61.612%, Loss = 0.4763647258281708
Epoch: 4020, Batch Gradient Norm: 10.95319521355476
Epoch: 4020, Batch Gradient Norm after: 10.95319521355476
Epoch 4021/10000, Prediction Accuracy = 61.6%, Loss = 0.4750924468040466
Epoch: 4021, Batch Gradient Norm: 9.889532698414213
Epoch: 4021, Batch Gradient Norm after: 9.889532698414213
Epoch 4022/10000, Prediction Accuracy = 61.73%, Loss = 0.46878803968429567
Epoch: 4022, Batch Gradient Norm: 7.794947792492881
Epoch: 4022, Batch Gradient Norm after: 7.794947792492881
Epoch 4023/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.45541866421699523
Epoch: 4023, Batch Gradient Norm: 9.42243615201822
Epoch: 4023, Batch Gradient Norm after: 9.42243615201822
Epoch 4024/10000, Prediction Accuracy = 61.75%, Loss = 0.46529399752616885
Epoch: 4024, Batch Gradient Norm: 10.140390560802063
Epoch: 4024, Batch Gradient Norm after: 10.140390560802063
Epoch 4025/10000, Prediction Accuracy = 61.698%, Loss = 0.46864465475082395
Epoch: 4025, Batch Gradient Norm: 9.753483569450836
Epoch: 4025, Batch Gradient Norm after: 9.753483569450836
Epoch 4026/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.46788595914840697
Epoch: 4026, Batch Gradient Norm: 9.650612281324708
Epoch: 4026, Batch Gradient Norm after: 9.650612281324708
Epoch 4027/10000, Prediction Accuracy = 61.766%, Loss = 0.4679486155509949
Epoch: 4027, Batch Gradient Norm: 10.751359795867753
Epoch: 4027, Batch Gradient Norm after: 10.751359795867753
Epoch 4028/10000, Prediction Accuracy = 61.766000000000005%, Loss = 0.47492942214012146
Epoch: 4028, Batch Gradient Norm: 8.845206885913631
Epoch: 4028, Batch Gradient Norm after: 8.845206885913631
Epoch 4029/10000, Prediction Accuracy = 61.71%, Loss = 0.4617434501647949
Epoch: 4029, Batch Gradient Norm: 11.017402947295055
Epoch: 4029, Batch Gradient Norm after: 11.017402947295055
Epoch 4030/10000, Prediction Accuracy = 61.842%, Loss = 0.47712746262550354
Epoch: 4030, Batch Gradient Norm: 11.846234053461847
Epoch: 4030, Batch Gradient Norm after: 11.846234053461847
Epoch 4031/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.48342406153678896
Epoch: 4031, Batch Gradient Norm: 10.737584540024153
Epoch: 4031, Batch Gradient Norm after: 10.737584540024153
Epoch 4032/10000, Prediction Accuracy = 61.65%, Loss = 0.473203706741333
Epoch: 4032, Batch Gradient Norm: 7.9514014177314465
Epoch: 4032, Batch Gradient Norm after: 7.9514014177314465
Epoch 4033/10000, Prediction Accuracy = 61.598%, Loss = 0.45663581490516664
Epoch: 4033, Batch Gradient Norm: 7.9531060151184585
Epoch: 4033, Batch Gradient Norm after: 7.9531060151184585
Epoch 4034/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.4551720261573792
Epoch: 4034, Batch Gradient Norm: 10.329728581801854
Epoch: 4034, Batch Gradient Norm after: 10.329728581801854
Epoch 4035/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.47116270661354065
Epoch: 4035, Batch Gradient Norm: 10.490871278731596
Epoch: 4035, Batch Gradient Norm after: 10.490871278731596
Epoch 4036/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.47323666214942933
Epoch: 4036, Batch Gradient Norm: 11.85601180295886
Epoch: 4036, Batch Gradient Norm after: 11.85601180295886
Epoch 4037/10000, Prediction Accuracy = 61.766%, Loss = 0.488132643699646
Epoch: 4037, Batch Gradient Norm: 8.756234819950839
Epoch: 4037, Batch Gradient Norm after: 8.756234819950839
Epoch 4038/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.46374515891075135
Epoch: 4038, Batch Gradient Norm: 11.258832785578816
Epoch: 4038, Batch Gradient Norm after: 11.258832785578816
Epoch 4039/10000, Prediction Accuracy = 61.7%, Loss = 0.48433085083961486
Epoch: 4039, Batch Gradient Norm: 11.812734912691429
Epoch: 4039, Batch Gradient Norm after: 11.812734912691429
Epoch 4040/10000, Prediction Accuracy = 61.602%, Loss = 0.4845205545425415
Epoch: 4040, Batch Gradient Norm: 10.721398178802337
Epoch: 4040, Batch Gradient Norm after: 10.721398178802337
Epoch 4041/10000, Prediction Accuracy = 61.722%, Loss = 0.47438918352127074
Epoch: 4041, Batch Gradient Norm: 11.335363335357338
Epoch: 4041, Batch Gradient Norm after: 11.335363335357338
Epoch 4042/10000, Prediction Accuracy = 61.81600000000001%, Loss = 0.47891377210617064
Epoch: 4042, Batch Gradient Norm: 11.308203253857892
Epoch: 4042, Batch Gradient Norm after: 11.308203253857892
Epoch 4043/10000, Prediction Accuracy = 61.8%, Loss = 0.47999147772789
Epoch: 4043, Batch Gradient Norm: 8.279846810931694
Epoch: 4043, Batch Gradient Norm after: 8.279846810931694
Epoch 4044/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4588851511478424
Epoch: 4044, Batch Gradient Norm: 11.251151153143745
Epoch: 4044, Batch Gradient Norm after: 11.251151153143745
Epoch 4045/10000, Prediction Accuracy = 61.85600000000001%, Loss = 0.47650346755981443
Epoch: 4045, Batch Gradient Norm: 9.946380916447836
Epoch: 4045, Batch Gradient Norm after: 9.946380916447836
Epoch 4046/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4659807324409485
Epoch: 4046, Batch Gradient Norm: 10.567965290790074
Epoch: 4046, Batch Gradient Norm after: 10.567965290790074
Epoch 4047/10000, Prediction Accuracy = 61.656000000000006%, Loss = 0.4704829275608063
Epoch: 4047, Batch Gradient Norm: 9.957587865735281
Epoch: 4047, Batch Gradient Norm after: 9.957587865735281
Epoch 4048/10000, Prediction Accuracy = 61.75%, Loss = 0.4683330535888672
Epoch: 4048, Batch Gradient Norm: 9.690265689680436
Epoch: 4048, Batch Gradient Norm after: 9.690265689680436
Epoch 4049/10000, Prediction Accuracy = 61.798%, Loss = 0.4669315755367279
Epoch: 4049, Batch Gradient Norm: 8.120697123496196
Epoch: 4049, Batch Gradient Norm after: 8.120697123496196
Epoch 4050/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.4570315182209015
Epoch: 4050, Batch Gradient Norm: 10.78210478841226
Epoch: 4050, Batch Gradient Norm after: 10.78210478841226
Epoch 4051/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.47353832721710204
Epoch: 4051, Batch Gradient Norm: 11.953055824682878
Epoch: 4051, Batch Gradient Norm after: 11.953055824682878
Epoch 4052/10000, Prediction Accuracy = 61.60999999999999%, Loss = 0.4827626466751099
Epoch: 4052, Batch Gradient Norm: 10.547860204491121
Epoch: 4052, Batch Gradient Norm after: 10.547860204491121
Epoch 4053/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.4730546593666077
Epoch: 4053, Batch Gradient Norm: 10.112403839335222
Epoch: 4053, Batch Gradient Norm after: 10.112403839335222
Epoch 4054/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4691054880619049
Epoch: 4054, Batch Gradient Norm: 9.57745284930975
Epoch: 4054, Batch Gradient Norm after: 9.57745284930975
Epoch 4055/10000, Prediction Accuracy = 61.71600000000001%, Loss = 0.4645503580570221
Epoch: 4055, Batch Gradient Norm: 7.28376319014307
Epoch: 4055, Batch Gradient Norm after: 7.28376319014307
Epoch 4056/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.453792530298233
Epoch: 4056, Batch Gradient Norm: 9.188788761061012
Epoch: 4056, Batch Gradient Norm after: 9.188788761061012
Epoch 4057/10000, Prediction Accuracy = 61.572%, Loss = 0.46616018414497373
Epoch: 4057, Batch Gradient Norm: 10.30766250169117
Epoch: 4057, Batch Gradient Norm after: 10.30766250169117
Epoch 4058/10000, Prediction Accuracy = 61.604%, Loss = 0.47472894191741943
Epoch: 4058, Batch Gradient Norm: 8.390336622554011
Epoch: 4058, Batch Gradient Norm after: 8.390336622554011
Epoch 4059/10000, Prediction Accuracy = 61.60600000000001%, Loss = 0.45863906741142274
Epoch: 4059, Batch Gradient Norm: 9.99168102467904
Epoch: 4059, Batch Gradient Norm after: 9.99168102467904
Epoch 4060/10000, Prediction Accuracy = 61.7%, Loss = 0.4670160174369812
Epoch: 4060, Batch Gradient Norm: 11.77900657870796
Epoch: 4060, Batch Gradient Norm after: 11.77900657870796
Epoch 4061/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4811594605445862
Epoch: 4061, Batch Gradient Norm: 7.554441710741581
Epoch: 4061, Batch Gradient Norm after: 7.554441710741581
Epoch 4062/10000, Prediction Accuracy = 61.838%, Loss = 0.4546452581882477
Epoch: 4062, Batch Gradient Norm: 9.942058928488027
Epoch: 4062, Batch Gradient Norm after: 9.942058928488027
Epoch 4063/10000, Prediction Accuracy = 61.745999999999995%, Loss = 0.4714611530303955
Epoch: 4063, Batch Gradient Norm: 11.961069917883144
Epoch: 4063, Batch Gradient Norm after: 11.961069917883144
Epoch 4064/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4905555784702301
Epoch: 4064, Batch Gradient Norm: 9.295870605152915
Epoch: 4064, Batch Gradient Norm after: 9.295870605152915
Epoch 4065/10000, Prediction Accuracy = 61.666%, Loss = 0.4607542514801025
Epoch: 4065, Batch Gradient Norm: 9.101973526440473
Epoch: 4065, Batch Gradient Norm after: 9.101973526440473
Epoch 4066/10000, Prediction Accuracy = 61.721999999999994%, Loss = 0.46405036449432374
Epoch: 4066, Batch Gradient Norm: 12.467311137396997
Epoch: 4066, Batch Gradient Norm after: 12.467311137396997
Epoch 4067/10000, Prediction Accuracy = 61.622%, Loss = 0.48326764106750486
Epoch: 4067, Batch Gradient Norm: 14.139910991768017
Epoch: 4067, Batch Gradient Norm after: 14.139910991768017
Epoch 4068/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.4989982545375824
Epoch: 4068, Batch Gradient Norm: 9.258794277467453
Epoch: 4068, Batch Gradient Norm after: 9.258794277467453
Epoch 4069/10000, Prediction Accuracy = 61.67%, Loss = 0.46187481880187986
Epoch: 4069, Batch Gradient Norm: 8.38400505937863
Epoch: 4069, Batch Gradient Norm after: 8.38400505937863
Epoch 4070/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.45971894860267637
Epoch: 4070, Batch Gradient Norm: 9.736432839903921
Epoch: 4070, Batch Gradient Norm after: 9.736432839903921
Epoch 4071/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4658073663711548
Epoch: 4071, Batch Gradient Norm: 6.854538121410251
Epoch: 4071, Batch Gradient Norm after: 6.854538121410251
Epoch 4072/10000, Prediction Accuracy = 61.766%, Loss = 0.4492492198944092
Epoch: 4072, Batch Gradient Norm: 10.593421446824387
Epoch: 4072, Batch Gradient Norm after: 10.593421446824387
Epoch 4073/10000, Prediction Accuracy = 61.754000000000005%, Loss = 0.4743625640869141
Epoch: 4073, Batch Gradient Norm: 11.73669162204761
Epoch: 4073, Batch Gradient Norm after: 11.73669162204761
Epoch 4074/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.48179218769073484
Epoch: 4074, Batch Gradient Norm: 9.811485555726941
Epoch: 4074, Batch Gradient Norm after: 9.811485555726941
Epoch 4075/10000, Prediction Accuracy = 61.638%, Loss = 0.46623345017433165
Epoch: 4075, Batch Gradient Norm: 10.40542121941023
Epoch: 4075, Batch Gradient Norm after: 10.40542121941023
Epoch 4076/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4694845497608185
Epoch: 4076, Batch Gradient Norm: 9.234703497422197
Epoch: 4076, Batch Gradient Norm after: 9.234703497422197
Epoch 4077/10000, Prediction Accuracy = 61.646%, Loss = 0.46346006393432615
Epoch: 4077, Batch Gradient Norm: 12.088336955249927
Epoch: 4077, Batch Gradient Norm after: 12.088336955249927
Epoch 4078/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4823116362094879
Epoch: 4078, Batch Gradient Norm: 9.928451191212588
Epoch: 4078, Batch Gradient Norm after: 9.928451191212588
Epoch 4079/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.4656966686248779
Epoch: 4079, Batch Gradient Norm: 8.229947189860576
Epoch: 4079, Batch Gradient Norm after: 8.229947189860576
Epoch 4080/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4554172456264496
Epoch: 4080, Batch Gradient Norm: 9.102215773012132
Epoch: 4080, Batch Gradient Norm after: 9.102215773012132
Epoch 4081/10000, Prediction Accuracy = 61.734%, Loss = 0.46248871088027954
Epoch: 4081, Batch Gradient Norm: 9.269673254100127
Epoch: 4081, Batch Gradient Norm after: 9.269673254100127
Epoch 4082/10000, Prediction Accuracy = 61.698%, Loss = 0.46747098565101625
Epoch: 4082, Batch Gradient Norm: 7.195476587037447
Epoch: 4082, Batch Gradient Norm after: 7.195476587037447
Epoch 4083/10000, Prediction Accuracy = 61.79%, Loss = 0.45106217861175535
Epoch: 4083, Batch Gradient Norm: 10.622513977962578
Epoch: 4083, Batch Gradient Norm after: 10.622513977962578
Epoch 4084/10000, Prediction Accuracy = 61.802%, Loss = 0.47138535380363467
Epoch: 4084, Batch Gradient Norm: 11.230573969633012
Epoch: 4084, Batch Gradient Norm after: 11.230573969633012
Epoch 4085/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.4741960883140564
Epoch: 4085, Batch Gradient Norm: 9.06807820029657
Epoch: 4085, Batch Gradient Norm after: 9.06807820029657
Epoch 4086/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.46113869547843933
Epoch: 4086, Batch Gradient Norm: 11.340257061799816
Epoch: 4086, Batch Gradient Norm after: 11.340257061799816
Epoch 4087/10000, Prediction Accuracy = 61.746%, Loss = 0.4805714964866638
Epoch: 4087, Batch Gradient Norm: 10.610859839936959
Epoch: 4087, Batch Gradient Norm after: 10.610859839936959
Epoch 4088/10000, Prediction Accuracy = 61.624%, Loss = 0.4709475576877594
Epoch: 4088, Batch Gradient Norm: 9.05936300460773
Epoch: 4088, Batch Gradient Norm after: 9.05936300460773
Epoch 4089/10000, Prediction Accuracy = 61.814%, Loss = 0.4580624759197235
Epoch: 4089, Batch Gradient Norm: 12.120253078670137
Epoch: 4089, Batch Gradient Norm after: 12.120253078670137
Epoch 4090/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.4783353567123413
Epoch: 4090, Batch Gradient Norm: 11.5023245549948
Epoch: 4090, Batch Gradient Norm after: 11.5023245549948
Epoch 4091/10000, Prediction Accuracy = 61.766000000000005%, Loss = 0.47498180270195006
Epoch: 4091, Batch Gradient Norm: 9.714448248330598
Epoch: 4091, Batch Gradient Norm after: 9.714448248330598
Epoch 4092/10000, Prediction Accuracy = 61.636%, Loss = 0.4643692851066589
Epoch: 4092, Batch Gradient Norm: 8.804524751666639
Epoch: 4092, Batch Gradient Norm after: 8.804524751666639
Epoch 4093/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.46050150990486144
Epoch: 4093, Batch Gradient Norm: 6.618911612721115
Epoch: 4093, Batch Gradient Norm after: 6.618911612721115
Epoch 4094/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4496238768100739
Epoch: 4094, Batch Gradient Norm: 7.524885202509805
Epoch: 4094, Batch Gradient Norm after: 7.524885202509805
Epoch 4095/10000, Prediction Accuracy = 61.628%, Loss = 0.4546880602836609
Epoch: 4095, Batch Gradient Norm: 8.550787029241802
Epoch: 4095, Batch Gradient Norm after: 8.550787029241802
Epoch 4096/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.4595631241798401
Epoch: 4096, Batch Gradient Norm: 11.352426178126047
Epoch: 4096, Batch Gradient Norm after: 11.352426178126047
Epoch 4097/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.47715981006622316
Epoch: 4097, Batch Gradient Norm: 13.46648745487416
Epoch: 4097, Batch Gradient Norm after: 13.46648745487416
Epoch 4098/10000, Prediction Accuracy = 61.638%, Loss = 0.4947766661643982
Epoch: 4098, Batch Gradient Norm: 13.794949128523719
Epoch: 4098, Batch Gradient Norm after: 13.794949128523719
Epoch 4099/10000, Prediction Accuracy = 61.65%, Loss = 0.4949726998806
Epoch: 4099, Batch Gradient Norm: 11.894555400740872
Epoch: 4099, Batch Gradient Norm after: 11.894555400740872
Epoch 4100/10000, Prediction Accuracy = 61.718%, Loss = 0.4827136754989624
Epoch: 4100, Batch Gradient Norm: 6.981031574531647
Epoch: 4100, Batch Gradient Norm after: 6.981031574531647
Epoch 4101/10000, Prediction Accuracy = 61.656000000000006%, Loss = 0.4508982837200165
Epoch: 4101, Batch Gradient Norm: 6.9486265235998355
Epoch: 4101, Batch Gradient Norm after: 6.9486265235998355
Epoch 4102/10000, Prediction Accuracy = 61.67%, Loss = 0.4486542046070099
Epoch: 4102, Batch Gradient Norm: 11.352972567342956
Epoch: 4102, Batch Gradient Norm after: 11.352972567342956
Epoch 4103/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.47526981234550475
Epoch: 4103, Batch Gradient Norm: 12.78461197673227
Epoch: 4103, Batch Gradient Norm after: 12.78461197673227
Epoch 4104/10000, Prediction Accuracy = 61.65%, Loss = 0.4880265951156616
Epoch: 4104, Batch Gradient Norm: 11.208127369504593
Epoch: 4104, Batch Gradient Norm after: 11.208127369504593
Epoch 4105/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.4783555448055267
Epoch: 4105, Batch Gradient Norm: 8.395465254079015
Epoch: 4105, Batch Gradient Norm after: 8.395465254079015
Epoch 4106/10000, Prediction Accuracy = 61.584%, Loss = 0.46080063581466674
Epoch: 4106, Batch Gradient Norm: 9.529624775406361
Epoch: 4106, Batch Gradient Norm after: 9.529624775406361
Epoch 4107/10000, Prediction Accuracy = 61.852%, Loss = 0.4685078501701355
Epoch: 4107, Batch Gradient Norm: 6.3272835410013535
Epoch: 4107, Batch Gradient Norm after: 6.3272835410013535
Epoch 4108/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.44541594982147215
Epoch: 4108, Batch Gradient Norm: 7.414799793223264
Epoch: 4108, Batch Gradient Norm after: 7.414799793223264
Epoch 4109/10000, Prediction Accuracy = 61.8%, Loss = 0.4513314664363861
Epoch: 4109, Batch Gradient Norm: 8.522988269683992
Epoch: 4109, Batch Gradient Norm after: 8.522988269683992
Epoch 4110/10000, Prediction Accuracy = 61.714%, Loss = 0.4565978229045868
Epoch: 4110, Batch Gradient Norm: 11.046863412974927
Epoch: 4110, Batch Gradient Norm after: 11.046863412974927
Epoch 4111/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.4720518171787262
Epoch: 4111, Batch Gradient Norm: 13.709206289545257
Epoch: 4111, Batch Gradient Norm after: 13.709206289545257
Epoch 4112/10000, Prediction Accuracy = 61.67%, Loss = 0.4983576834201813
Epoch: 4112, Batch Gradient Norm: 11.436048456841535
Epoch: 4112, Batch Gradient Norm after: 11.436048456841535
Epoch 4113/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.4848525583744049
Epoch: 4113, Batch Gradient Norm: 7.621589840019685
Epoch: 4113, Batch Gradient Norm after: 7.621589840019685
Epoch 4114/10000, Prediction Accuracy = 61.77%, Loss = 0.45115063786506654
Epoch: 4114, Batch Gradient Norm: 11.655837406751118
Epoch: 4114, Batch Gradient Norm after: 11.655837406751118
Epoch 4115/10000, Prediction Accuracy = 61.73%, Loss = 0.477221691608429
Epoch: 4115, Batch Gradient Norm: 10.327090662925384
Epoch: 4115, Batch Gradient Norm after: 10.327090662925384
Epoch 4116/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.4682945430278778
Epoch: 4116, Batch Gradient Norm: 10.43606122542332
Epoch: 4116, Batch Gradient Norm after: 10.43606122542332
Epoch 4117/10000, Prediction Accuracy = 61.754%, Loss = 0.47075924277305603
Epoch: 4117, Batch Gradient Norm: 11.066321753098205
Epoch: 4117, Batch Gradient Norm after: 11.066321753098205
Epoch 4118/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4747940540313721
Epoch: 4118, Batch Gradient Norm: 9.15589291719873
Epoch: 4118, Batch Gradient Norm after: 9.15589291719873
Epoch 4119/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4598192393779755
Epoch: 4119, Batch Gradient Norm: 8.475061951876318
Epoch: 4119, Batch Gradient Norm after: 8.475061951876318
Epoch 4120/10000, Prediction Accuracy = 61.66600000000001%, Loss = 0.45814130306243894
Epoch: 4120, Batch Gradient Norm: 7.62566105950841
Epoch: 4120, Batch Gradient Norm after: 7.62566105950841
Epoch 4121/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4536731123924255
Epoch: 4121, Batch Gradient Norm: 10.00284552832886
Epoch: 4121, Batch Gradient Norm after: 10.00284552832886
Epoch 4122/10000, Prediction Accuracy = 61.678%, Loss = 0.4657708525657654
Epoch: 4122, Batch Gradient Norm: 10.62414695884767
Epoch: 4122, Batch Gradient Norm after: 10.62414695884767
Epoch 4123/10000, Prediction Accuracy = 61.71%, Loss = 0.4699764847755432
Epoch: 4123, Batch Gradient Norm: 9.681174170840384
Epoch: 4123, Batch Gradient Norm after: 9.681174170840384
Epoch 4124/10000, Prediction Accuracy = 61.646%, Loss = 0.46273638010025026
Epoch: 4124, Batch Gradient Norm: 8.172906005201357
Epoch: 4124, Batch Gradient Norm after: 8.172906005201357
Epoch 4125/10000, Prediction Accuracy = 61.74000000000001%, Loss = 0.45408875942230226
Epoch: 4125, Batch Gradient Norm: 12.835054709131374
Epoch: 4125, Batch Gradient Norm after: 12.835054709131374
Epoch 4126/10000, Prediction Accuracy = 61.63000000000001%, Loss = 0.5003587663173675
Epoch: 4126, Batch Gradient Norm: 7.858622347636273
Epoch: 4126, Batch Gradient Norm after: 7.858622347636273
Epoch 4127/10000, Prediction Accuracy = 61.694%, Loss = 0.4535102844238281
Epoch: 4127, Batch Gradient Norm: 9.38856015257066
Epoch: 4127, Batch Gradient Norm after: 9.38856015257066
Epoch 4128/10000, Prediction Accuracy = 61.686%, Loss = 0.46192782521247866
Epoch: 4128, Batch Gradient Norm: 10.654252140499768
Epoch: 4128, Batch Gradient Norm after: 10.654252140499768
Epoch 4129/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4729462802410126
Epoch: 4129, Batch Gradient Norm: 10.073056927412036
Epoch: 4129, Batch Gradient Norm after: 10.073056927412036
Epoch 4130/10000, Prediction Accuracy = 61.71400000000001%, Loss = 0.4672121167182922
Epoch: 4130, Batch Gradient Norm: 11.986905896998447
Epoch: 4130, Batch Gradient Norm after: 11.986905896998447
Epoch 4131/10000, Prediction Accuracy = 61.726%, Loss = 0.4818867206573486
Epoch: 4131, Batch Gradient Norm: 13.014312590046057
Epoch: 4131, Batch Gradient Norm after: 13.014312590046057
Epoch 4132/10000, Prediction Accuracy = 61.67%, Loss = 0.4941736400127411
Epoch: 4132, Batch Gradient Norm: 9.204609350145102
Epoch: 4132, Batch Gradient Norm after: 9.204609350145102
Epoch 4133/10000, Prediction Accuracy = 61.886%, Loss = 0.46082407236099243
Epoch: 4133, Batch Gradient Norm: 11.373153842319386
Epoch: 4133, Batch Gradient Norm after: 11.373153842319386
Epoch 4134/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4778576850891113
Epoch: 4134, Batch Gradient Norm: 8.8219503748742
Epoch: 4134, Batch Gradient Norm after: 8.8219503748742
Epoch 4135/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4590006411075592
Epoch: 4135, Batch Gradient Norm: 8.926672268126973
Epoch: 4135, Batch Gradient Norm after: 8.926672268126973
Epoch 4136/10000, Prediction Accuracy = 61.698%, Loss = 0.459576678276062
Epoch: 4136, Batch Gradient Norm: 11.445931375549911
Epoch: 4136, Batch Gradient Norm after: 11.445931375549911
Epoch 4137/10000, Prediction Accuracy = 61.602%, Loss = 0.47252304553985597
Epoch: 4137, Batch Gradient Norm: 12.055662889608552
Epoch: 4137, Batch Gradient Norm after: 12.055662889608552
Epoch 4138/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4773920297622681
Epoch: 4138, Batch Gradient Norm: 8.89912162649989
Epoch: 4138, Batch Gradient Norm after: 8.89912162649989
Epoch 4139/10000, Prediction Accuracy = 61.688%, Loss = 0.4583884537220001
Epoch: 4139, Batch Gradient Norm: 6.520030812484342
Epoch: 4139, Batch Gradient Norm after: 6.520030812484342
Epoch 4140/10000, Prediction Accuracy = 61.674%, Loss = 0.4456532061100006
Epoch: 4140, Batch Gradient Norm: 7.842821029769838
Epoch: 4140, Batch Gradient Norm after: 7.842821029769838
Epoch 4141/10000, Prediction Accuracy = 61.604%, Loss = 0.45403467416763305
Epoch: 4141, Batch Gradient Norm: 8.915783065219314
Epoch: 4141, Batch Gradient Norm after: 8.915783065219314
Epoch 4142/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.46010942459106446
Epoch: 4142, Batch Gradient Norm: 9.592439421314028
Epoch: 4142, Batch Gradient Norm after: 9.592439421314028
Epoch 4143/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4628321886062622
Epoch: 4143, Batch Gradient Norm: 11.169907044750857
Epoch: 4143, Batch Gradient Norm after: 11.169907044750857
Epoch 4144/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.4753799378871918
Epoch: 4144, Batch Gradient Norm: 10.651005941199298
Epoch: 4144, Batch Gradient Norm after: 10.651005941199298
Epoch 4145/10000, Prediction Accuracy = 61.75%, Loss = 0.46916139125823975
Epoch: 4145, Batch Gradient Norm: 11.15601881135221
Epoch: 4145, Batch Gradient Norm after: 11.15601881135221
Epoch 4146/10000, Prediction Accuracy = 61.782000000000004%, Loss = 0.4710646390914917
Epoch: 4146, Batch Gradient Norm: 9.230094364999534
Epoch: 4146, Batch Gradient Norm after: 9.230094364999534
Epoch 4147/10000, Prediction Accuracy = 61.754%, Loss = 0.46011746525764463
Epoch: 4147, Batch Gradient Norm: 9.134952193740357
Epoch: 4147, Batch Gradient Norm after: 9.134952193740357
Epoch 4148/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.4585556864738464
Epoch: 4148, Batch Gradient Norm: 11.276032663709358
Epoch: 4148, Batch Gradient Norm after: 11.276032663709358
Epoch 4149/10000, Prediction Accuracy = 61.622%, Loss = 0.4745196163654327
Epoch: 4149, Batch Gradient Norm: 8.52246581665578
Epoch: 4149, Batch Gradient Norm after: 8.52246581665578
Epoch 4150/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4565927267074585
Epoch: 4150, Batch Gradient Norm: 7.56346327690592
Epoch: 4150, Batch Gradient Norm after: 7.56346327690592
Epoch 4151/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.45017983913421633
Epoch: 4151, Batch Gradient Norm: 11.045986163700173
Epoch: 4151, Batch Gradient Norm after: 11.045986163700173
Epoch 4152/10000, Prediction Accuracy = 61.72399999999999%, Loss = 0.4772182106971741
Epoch: 4152, Batch Gradient Norm: 11.776635543376996
Epoch: 4152, Batch Gradient Norm after: 11.776635543376996
Epoch 4153/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.47875175476074217
Epoch: 4153, Batch Gradient Norm: 12.319124152128074
Epoch: 4153, Batch Gradient Norm after: 12.319124152128074
Epoch 4154/10000, Prediction Accuracy = 61.712%, Loss = 0.48402711749076843
Epoch: 4154, Batch Gradient Norm: 8.294032719729122
Epoch: 4154, Batch Gradient Norm after: 8.294032719729122
Epoch 4155/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4552727580070496
Epoch: 4155, Batch Gradient Norm: 7.0127160850740315
Epoch: 4155, Batch Gradient Norm after: 7.0127160850740315
Epoch 4156/10000, Prediction Accuracy = 61.648%, Loss = 0.44677241444587706
Epoch: 4156, Batch Gradient Norm: 11.947860640482583
Epoch: 4156, Batch Gradient Norm after: 11.947860640482583
Epoch 4157/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.47778910398483276
Epoch: 4157, Batch Gradient Norm: 11.69946951085399
Epoch: 4157, Batch Gradient Norm after: 11.69946951085399
Epoch 4158/10000, Prediction Accuracy = 61.8%, Loss = 0.47487651705741885
Epoch: 4158, Batch Gradient Norm: 8.117092921491281
Epoch: 4158, Batch Gradient Norm after: 8.117092921491281
Epoch 4159/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4529789566993713
Epoch: 4159, Batch Gradient Norm: 7.8633324355092205
Epoch: 4159, Batch Gradient Norm after: 7.8633324355092205
Epoch 4160/10000, Prediction Accuracy = 61.652%, Loss = 0.4528685212135315
Epoch: 4160, Batch Gradient Norm: 8.90907751947423
Epoch: 4160, Batch Gradient Norm after: 8.90907751947423
Epoch 4161/10000, Prediction Accuracy = 61.806%, Loss = 0.4563870966434479
Epoch: 4161, Batch Gradient Norm: 12.220802306106874
Epoch: 4161, Batch Gradient Norm after: 12.220802306106874
Epoch 4162/10000, Prediction Accuracy = 61.69%, Loss = 0.483756023645401
Epoch: 4162, Batch Gradient Norm: 12.065038418690149
Epoch: 4162, Batch Gradient Norm after: 12.065038418690149
Epoch 4163/10000, Prediction Accuracy = 61.598%, Loss = 0.4829943597316742
Epoch: 4163, Batch Gradient Norm: 12.306365063246199
Epoch: 4163, Batch Gradient Norm after: 12.306365063246199
Epoch 4164/10000, Prediction Accuracy = 61.712%, Loss = 0.48937835097312926
Epoch: 4164, Batch Gradient Norm: 7.782217231691091
Epoch: 4164, Batch Gradient Norm after: 7.782217231691091
Epoch 4165/10000, Prediction Accuracy = 61.79%, Loss = 0.4522913873195648
Epoch: 4165, Batch Gradient Norm: 8.058586623943873
Epoch: 4165, Batch Gradient Norm after: 8.058586623943873
Epoch 4166/10000, Prediction Accuracy = 61.794%, Loss = 0.45204458236694334
Epoch: 4166, Batch Gradient Norm: 10.41744358712588
Epoch: 4166, Batch Gradient Norm after: 10.41744358712588
Epoch 4167/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.465756356716156
Epoch: 4167, Batch Gradient Norm: 9.619922276463559
Epoch: 4167, Batch Gradient Norm after: 9.619922276463559
Epoch 4168/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.46060401797294614
Epoch: 4168, Batch Gradient Norm: 10.17057569306416
Epoch: 4168, Batch Gradient Norm after: 10.17057569306416
Epoch 4169/10000, Prediction Accuracy = 61.876%, Loss = 0.4677066087722778
Epoch: 4169, Batch Gradient Norm: 9.949755612508367
Epoch: 4169, Batch Gradient Norm after: 9.949755612508367
Epoch 4170/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4652861952781677
Epoch: 4170, Batch Gradient Norm: 9.779665816420234
Epoch: 4170, Batch Gradient Norm after: 9.779665816420234
Epoch 4171/10000, Prediction Accuracy = 61.67%, Loss = 0.4654375672340393
Epoch: 4171, Batch Gradient Norm: 6.293869130756291
Epoch: 4171, Batch Gradient Norm after: 6.293869130756291
Epoch 4172/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.44572353959083555
Epoch: 4172, Batch Gradient Norm: 12.564522334409927
Epoch: 4172, Batch Gradient Norm after: 12.564522334409927
Epoch 4173/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4856915414333344
Epoch: 4173, Batch Gradient Norm: 12.239603523267851
Epoch: 4173, Batch Gradient Norm after: 12.239603523267851
Epoch 4174/10000, Prediction Accuracy = 61.734%, Loss = 0.48411738872528076
Epoch: 4174, Batch Gradient Norm: 9.770687619884821
Epoch: 4174, Batch Gradient Norm after: 9.770687619884821
Epoch 4175/10000, Prediction Accuracy = 61.836%, Loss = 0.46361332535743716
Epoch: 4175, Batch Gradient Norm: 13.406078995871772
Epoch: 4175, Batch Gradient Norm after: 13.406078995871772
Epoch 4176/10000, Prediction Accuracy = 61.682%, Loss = 0.4912082254886627
Epoch: 4176, Batch Gradient Norm: 9.1534222973591
Epoch: 4176, Batch Gradient Norm after: 9.1534222973591
Epoch 4177/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.45661652088165283
Epoch: 4177, Batch Gradient Norm: 10.389352636807388
Epoch: 4177, Batch Gradient Norm after: 10.389352636807388
Epoch 4178/10000, Prediction Accuracy = 61.766%, Loss = 0.47030128836631774
Epoch: 4178, Batch Gradient Norm: 10.957569304209862
Epoch: 4178, Batch Gradient Norm after: 10.957569304209862
Epoch 4179/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.47609648704528806
Epoch: 4179, Batch Gradient Norm: 7.878824547923562
Epoch: 4179, Batch Gradient Norm after: 7.878824547923562
Epoch 4180/10000, Prediction Accuracy = 61.838%, Loss = 0.45627453923225403
Epoch: 4180, Batch Gradient Norm: 7.68348985734005
Epoch: 4180, Batch Gradient Norm after: 7.68348985734005
Epoch 4181/10000, Prediction Accuracy = 61.64799999999999%, Loss = 0.4523646593093872
Epoch: 4181, Batch Gradient Norm: 9.120232004787605
Epoch: 4181, Batch Gradient Norm after: 9.120232004787605
Epoch 4182/10000, Prediction Accuracy = 61.624%, Loss = 0.4592910885810852
Epoch: 4182, Batch Gradient Norm: 9.30174312567699
Epoch: 4182, Batch Gradient Norm after: 9.30174312567699
Epoch 4183/10000, Prediction Accuracy = 61.794%, Loss = 0.4587851524353027
Epoch: 4183, Batch Gradient Norm: 10.821704497030318
Epoch: 4183, Batch Gradient Norm after: 10.821704497030318
Epoch 4184/10000, Prediction Accuracy = 61.80400000000001%, Loss = 0.469775390625
Epoch: 4184, Batch Gradient Norm: 12.8147663073731
Epoch: 4184, Batch Gradient Norm after: 12.8147663073731
Epoch 4185/10000, Prediction Accuracy = 61.646%, Loss = 0.4904719412326813
Epoch: 4185, Batch Gradient Norm: 10.78933965384206
Epoch: 4185, Batch Gradient Norm after: 10.78933965384206
Epoch 4186/10000, Prediction Accuracy = 61.726%, Loss = 0.4706041574478149
Epoch: 4186, Batch Gradient Norm: 9.26892468068883
Epoch: 4186, Batch Gradient Norm after: 9.26892468068883
Epoch 4187/10000, Prediction Accuracy = 61.794%, Loss = 0.4597882091999054
Epoch: 4187, Batch Gradient Norm: 7.39763143700046
Epoch: 4187, Batch Gradient Norm after: 7.39763143700046
Epoch 4188/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.4491256535053253
Epoch: 4188, Batch Gradient Norm: 9.347155592925366
Epoch: 4188, Batch Gradient Norm after: 9.347155592925366
Epoch 4189/10000, Prediction Accuracy = 61.79%, Loss = 0.45889312624931333
Epoch: 4189, Batch Gradient Norm: 8.45374799173734
Epoch: 4189, Batch Gradient Norm after: 8.45374799173734
Epoch 4190/10000, Prediction Accuracy = 61.762%, Loss = 0.4554026424884796
Epoch: 4190, Batch Gradient Norm: 9.867630571498367
Epoch: 4190, Batch Gradient Norm after: 9.867630571498367
Epoch 4191/10000, Prediction Accuracy = 61.754%, Loss = 0.4635807812213898
Epoch: 4191, Batch Gradient Norm: 11.751929809237506
Epoch: 4191, Batch Gradient Norm after: 11.751929809237506
Epoch 4192/10000, Prediction Accuracy = 61.745999999999995%, Loss = 0.47731035351753237
Epoch: 4192, Batch Gradient Norm: 10.09160846159045
Epoch: 4192, Batch Gradient Norm after: 10.09160846159045
Epoch 4193/10000, Prediction Accuracy = 61.676%, Loss = 0.46687028408050535
Epoch: 4193, Batch Gradient Norm: 10.272752489695456
Epoch: 4193, Batch Gradient Norm after: 10.272752489695456
Epoch 4194/10000, Prediction Accuracy = 61.798%, Loss = 0.46425597071647645
Epoch: 4194, Batch Gradient Norm: 11.842322084325133
Epoch: 4194, Batch Gradient Norm after: 11.842322084325133
Epoch 4195/10000, Prediction Accuracy = 61.638%, Loss = 0.476911199092865
Epoch: 4195, Batch Gradient Norm: 9.01888234820624
Epoch: 4195, Batch Gradient Norm after: 9.01888234820624
Epoch 4196/10000, Prediction Accuracy = 61.79799999999999%, Loss = 0.45674203634262084
Epoch: 4196, Batch Gradient Norm: 10.22616876142459
Epoch: 4196, Batch Gradient Norm after: 10.22616876142459
Epoch 4197/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.4643190264701843
Epoch: 4197, Batch Gradient Norm: 10.722766144713113
Epoch: 4197, Batch Gradient Norm after: 10.722766144713113
Epoch 4198/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4671287894248962
Epoch: 4198, Batch Gradient Norm: 10.708476570057268
Epoch: 4198, Batch Gradient Norm after: 10.708476570057268
Epoch 4199/10000, Prediction Accuracy = 61.852%, Loss = 0.4676536500453949
Epoch: 4199, Batch Gradient Norm: 9.49444951005694
Epoch: 4199, Batch Gradient Norm after: 9.49444951005694
Epoch 4200/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.46197267174720763
Epoch: 4200, Batch Gradient Norm: 7.059887116686228
Epoch: 4200, Batch Gradient Norm after: 7.059887116686228
Epoch 4201/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4470091462135315
Epoch: 4201, Batch Gradient Norm: 9.183955529008738
Epoch: 4201, Batch Gradient Norm after: 9.183955529008738
Epoch 4202/10000, Prediction Accuracy = 61.814%, Loss = 0.4589771211147308
Epoch: 4202, Batch Gradient Norm: 10.69447063982067
Epoch: 4202, Batch Gradient Norm after: 10.69447063982067
Epoch 4203/10000, Prediction Accuracy = 61.56199999999999%, Loss = 0.4705245852470398
Epoch: 4203, Batch Gradient Norm: 11.960416762791203
Epoch: 4203, Batch Gradient Norm after: 11.960416762791203
Epoch 4204/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4787963032722473
Epoch: 4204, Batch Gradient Norm: 13.74828271119495
Epoch: 4204, Batch Gradient Norm after: 13.74828271119495
Epoch 4205/10000, Prediction Accuracy = 61.63199999999999%, Loss = 0.4964802086353302
Epoch: 4205, Batch Gradient Norm: 8.6858283192328
Epoch: 4205, Batch Gradient Norm after: 8.6858283192328
Epoch 4206/10000, Prediction Accuracy = 61.75600000000001%, Loss = 0.45553491711616517
Epoch: 4206, Batch Gradient Norm: 7.784557609634888
Epoch: 4206, Batch Gradient Norm after: 7.784557609634888
Epoch 4207/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.45100306868553164
Epoch: 4207, Batch Gradient Norm: 10.937480485553976
Epoch: 4207, Batch Gradient Norm after: 10.937480485553976
Epoch 4208/10000, Prediction Accuracy = 61.75600000000001%, Loss = 0.4701841473579407
Epoch: 4208, Batch Gradient Norm: 11.301521105696558
Epoch: 4208, Batch Gradient Norm after: 11.301521105696558
Epoch 4209/10000, Prediction Accuracy = 61.714%, Loss = 0.4723699688911438
Epoch: 4209, Batch Gradient Norm: 10.51754177428659
Epoch: 4209, Batch Gradient Norm after: 10.51754177428659
Epoch 4210/10000, Prediction Accuracy = 61.721999999999994%, Loss = 0.4668007969856262
Epoch: 4210, Batch Gradient Norm: 7.809405785864542
Epoch: 4210, Batch Gradient Norm after: 7.809405785864542
Epoch 4211/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.44903741478919984
Epoch: 4211, Batch Gradient Norm: 7.175910261088621
Epoch: 4211, Batch Gradient Norm after: 7.175910261088621
Epoch 4212/10000, Prediction Accuracy = 61.738000000000014%, Loss = 0.44727898836135865
Epoch: 4212, Batch Gradient Norm: 11.179563903141542
Epoch: 4212, Batch Gradient Norm after: 11.179563903141542
Epoch 4213/10000, Prediction Accuracy = 61.802%, Loss = 0.47699538469314573
Epoch: 4213, Batch Gradient Norm: 10.033062009548715
Epoch: 4213, Batch Gradient Norm after: 10.033062009548715
Epoch 4214/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4629099130630493
Epoch: 4214, Batch Gradient Norm: 10.608524730839264
Epoch: 4214, Batch Gradient Norm after: 10.608524730839264
Epoch 4215/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.46697393655776975
Epoch: 4215, Batch Gradient Norm: 8.71531418569271
Epoch: 4215, Batch Gradient Norm after: 8.71531418569271
Epoch 4216/10000, Prediction Accuracy = 61.786%, Loss = 0.4564790606498718
Epoch: 4216, Batch Gradient Norm: 9.800559728300135
Epoch: 4216, Batch Gradient Norm after: 9.800559728300135
Epoch 4217/10000, Prediction Accuracy = 61.65%, Loss = 0.46371166706085204
Epoch: 4217, Batch Gradient Norm: 12.225561800532411
Epoch: 4217, Batch Gradient Norm after: 12.225561800532411
Epoch 4218/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4763262331485748
Epoch: 4218, Batch Gradient Norm: 12.80367200694099
Epoch: 4218, Batch Gradient Norm after: 12.80367200694099
Epoch 4219/10000, Prediction Accuracy = 61.848%, Loss = 0.4892320930957794
Epoch: 4219, Batch Gradient Norm: 8.704465204181547
Epoch: 4219, Batch Gradient Norm after: 8.704465204181547
Epoch 4220/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.4574275553226471
Epoch: 4220, Batch Gradient Norm: 8.43400023450327
Epoch: 4220, Batch Gradient Norm after: 8.43400023450327
Epoch 4221/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4532683491706848
Epoch: 4221, Batch Gradient Norm: 8.322740063479548
Epoch: 4221, Batch Gradient Norm after: 8.322740063479548
Epoch 4222/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4528491199016571
Epoch: 4222, Batch Gradient Norm: 8.348512476051493
Epoch: 4222, Batch Gradient Norm after: 8.348512476051493
Epoch 4223/10000, Prediction Accuracy = 61.669999999999995%, Loss = 0.45262172222137453
Epoch: 4223, Batch Gradient Norm: 11.976451393143302
Epoch: 4223, Batch Gradient Norm after: 11.976451393143302
Epoch 4224/10000, Prediction Accuracy = 61.648%, Loss = 0.48003136515617373
Epoch: 4224, Batch Gradient Norm: 10.789385521408859
Epoch: 4224, Batch Gradient Norm after: 10.789385521408859
Epoch 4225/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.46888079643249514
Epoch: 4225, Batch Gradient Norm: 9.900919887535123
Epoch: 4225, Batch Gradient Norm after: 9.900919887535123
Epoch 4226/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4632942020893097
Epoch: 4226, Batch Gradient Norm: 8.126615770573387
Epoch: 4226, Batch Gradient Norm after: 8.126615770573387
Epoch 4227/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.451752507686615
Epoch: 4227, Batch Gradient Norm: 10.117075592155176
Epoch: 4227, Batch Gradient Norm after: 10.117075592155176
Epoch 4228/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.46187509298324586
Epoch: 4228, Batch Gradient Norm: 10.052748597708119
Epoch: 4228, Batch Gradient Norm after: 10.052748597708119
Epoch 4229/10000, Prediction Accuracy = 61.686%, Loss = 0.4653064370155334
Epoch: 4229, Batch Gradient Norm: 8.352204141583188
Epoch: 4229, Batch Gradient Norm after: 8.352204141583188
Epoch 4230/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.4579294979572296
Epoch: 4230, Batch Gradient Norm: 10.301177235345317
Epoch: 4230, Batch Gradient Norm after: 10.301177235345317
Epoch 4231/10000, Prediction Accuracy = 61.786%, Loss = 0.4666791081428528
Epoch: 4231, Batch Gradient Norm: 9.05674703673797
Epoch: 4231, Batch Gradient Norm after: 9.05674703673797
Epoch 4232/10000, Prediction Accuracy = 61.774%, Loss = 0.45479523539543154
Epoch: 4232, Batch Gradient Norm: 11.63458056280715
Epoch: 4232, Batch Gradient Norm after: 11.63458056280715
Epoch 4233/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.4772822976112366
Epoch: 4233, Batch Gradient Norm: 8.906093468150344
Epoch: 4233, Batch Gradient Norm after: 8.906093468150344
Epoch 4234/10000, Prediction Accuracy = 61.69799999999999%, Loss = 0.45650445818901064
Epoch: 4234, Batch Gradient Norm: 8.95635074348015
Epoch: 4234, Batch Gradient Norm after: 8.95635074348015
Epoch 4235/10000, Prediction Accuracy = 61.686%, Loss = 0.4570420026779175
Epoch: 4235, Batch Gradient Norm: 10.99862002691614
Epoch: 4235, Batch Gradient Norm after: 10.99862002691614
Epoch 4236/10000, Prediction Accuracy = 61.722%, Loss = 0.4691704213619232
Epoch: 4236, Batch Gradient Norm: 11.749205872972489
Epoch: 4236, Batch Gradient Norm after: 11.749205872972489
Epoch 4237/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4764508068561554
Epoch: 4237, Batch Gradient Norm: 8.73773236636557
Epoch: 4237, Batch Gradient Norm after: 8.73773236636557
Epoch 4238/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.45394558310508726
Epoch: 4238, Batch Gradient Norm: 9.260537013631902
Epoch: 4238, Batch Gradient Norm after: 9.260537013631902
Epoch 4239/10000, Prediction Accuracy = 61.722%, Loss = 0.4601230323314667
Epoch: 4239, Batch Gradient Norm: 10.314770845780808
Epoch: 4239, Batch Gradient Norm after: 10.314770845780808
Epoch 4240/10000, Prediction Accuracy = 61.648%, Loss = 0.46759676933288574
Epoch: 4240, Batch Gradient Norm: 8.677940728488553
Epoch: 4240, Batch Gradient Norm after: 8.677940728488553
Epoch 4241/10000, Prediction Accuracy = 61.77%, Loss = 0.45485159754753113
Epoch: 4241, Batch Gradient Norm: 12.404740833328763
Epoch: 4241, Batch Gradient Norm after: 12.404740833328763
Epoch 4242/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.48166604042053224
Epoch: 4242, Batch Gradient Norm: 11.768776917084054
Epoch: 4242, Batch Gradient Norm after: 11.768776917084054
Epoch 4243/10000, Prediction Accuracy = 61.61%, Loss = 0.47573251724243165
Epoch: 4243, Batch Gradient Norm: 10.366623642149758
Epoch: 4243, Batch Gradient Norm after: 10.366623642149758
Epoch 4244/10000, Prediction Accuracy = 61.662%, Loss = 0.4675746560096741
Epoch: 4244, Batch Gradient Norm: 10.759551056071908
Epoch: 4244, Batch Gradient Norm after: 10.759551056071908
Epoch 4245/10000, Prediction Accuracy = 61.778%, Loss = 0.46913973689079286
Epoch: 4245, Batch Gradient Norm: 10.310444326085078
Epoch: 4245, Batch Gradient Norm after: 10.310444326085078
Epoch 4246/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.46849969029426575
Epoch: 4246, Batch Gradient Norm: 8.547201915808818
Epoch: 4246, Batch Gradient Norm after: 8.547201915808818
Epoch 4247/10000, Prediction Accuracy = 61.698%, Loss = 0.4528869569301605
Epoch: 4247, Batch Gradient Norm: 12.326531820477172
Epoch: 4247, Batch Gradient Norm after: 12.326531820477172
Epoch 4248/10000, Prediction Accuracy = 61.812%, Loss = 0.48396262526512146
Epoch: 4248, Batch Gradient Norm: 7.220988695675474
Epoch: 4248, Batch Gradient Norm after: 7.220988695675474
Epoch 4249/10000, Prediction Accuracy = 61.682%, Loss = 0.44781386852264404
Epoch: 4249, Batch Gradient Norm: 9.93316664478565
Epoch: 4249, Batch Gradient Norm after: 9.93316664478565
Epoch 4250/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4620705425739288
Epoch: 4250, Batch Gradient Norm: 15.123939313874809
Epoch: 4250, Batch Gradient Norm after: 15.123939313874809
Epoch 4251/10000, Prediction Accuracy = 61.718%, Loss = 0.5030761063098907
Epoch: 4251, Batch Gradient Norm: 11.185338624195342
Epoch: 4251, Batch Gradient Norm after: 11.185338624195342
Epoch 4252/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.4739052474498749
Epoch: 4252, Batch Gradient Norm: 7.858688207360265
Epoch: 4252, Batch Gradient Norm after: 7.858688207360265
Epoch 4253/10000, Prediction Accuracy = 61.73%, Loss = 0.44979310035705566
Epoch: 4253, Batch Gradient Norm: 9.685805259380574
Epoch: 4253, Batch Gradient Norm after: 9.685805259380574
Epoch 4254/10000, Prediction Accuracy = 61.788%, Loss = 0.4600008189678192
Epoch: 4254, Batch Gradient Norm: 10.627043047943678
Epoch: 4254, Batch Gradient Norm after: 10.627043047943678
Epoch 4255/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.46772477626800535
Epoch: 4255, Batch Gradient Norm: 7.528886292015124
Epoch: 4255, Batch Gradient Norm after: 7.528886292015124
Epoch 4256/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4456833302974701
Epoch: 4256, Batch Gradient Norm: 8.458225368004916
Epoch: 4256, Batch Gradient Norm after: 8.458225368004916
Epoch 4257/10000, Prediction Accuracy = 61.786%, Loss = 0.4539840817451477
Epoch: 4257, Batch Gradient Norm: 8.433154994443003
Epoch: 4257, Batch Gradient Norm after: 8.433154994443003
Epoch 4258/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4512928605079651
Epoch: 4258, Batch Gradient Norm: 12.648296797894048
Epoch: 4258, Batch Gradient Norm after: 12.648296797894048
Epoch 4259/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.4802745819091797
Epoch: 4259, Batch Gradient Norm: 9.363251536230413
Epoch: 4259, Batch Gradient Norm after: 9.363251536230413
Epoch 4260/10000, Prediction Accuracy = 61.766%, Loss = 0.45763748288154604
Epoch: 4260, Batch Gradient Norm: 10.045283077760875
Epoch: 4260, Batch Gradient Norm after: 10.045283077760875
Epoch 4261/10000, Prediction Accuracy = 61.666%, Loss = 0.4663579106330872
Epoch: 4261, Batch Gradient Norm: 10.534823892655298
Epoch: 4261, Batch Gradient Norm after: 10.534823892655298
Epoch 4262/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.46891571283340455
Epoch: 4262, Batch Gradient Norm: 13.48084411602695
Epoch: 4262, Batch Gradient Norm after: 13.48084411602695
Epoch 4263/10000, Prediction Accuracy = 61.709999999999994%, Loss = 0.49865769743919375
Epoch: 4263, Batch Gradient Norm: 6.4616563546279915
Epoch: 4263, Batch Gradient Norm after: 6.4616563546279915
Epoch 4264/10000, Prediction Accuracy = 61.818%, Loss = 0.4437324106693268
Epoch: 4264, Batch Gradient Norm: 7.232691249859291
Epoch: 4264, Batch Gradient Norm after: 7.232691249859291
Epoch 4265/10000, Prediction Accuracy = 61.758%, Loss = 0.4469117224216461
Epoch: 4265, Batch Gradient Norm: 9.003136759350964
Epoch: 4265, Batch Gradient Norm after: 9.003136759350964
Epoch 4266/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.45462003350257874
Epoch: 4266, Batch Gradient Norm: 9.008346549309195
Epoch: 4266, Batch Gradient Norm after: 9.008346549309195
Epoch 4267/10000, Prediction Accuracy = 61.746%, Loss = 0.4552912533283234
Epoch: 4267, Batch Gradient Norm: 11.758281988277904
Epoch: 4267, Batch Gradient Norm after: 11.758281988277904
Epoch 4268/10000, Prediction Accuracy = 61.79599999999999%, Loss = 0.47469463348388674
Epoch: 4268, Batch Gradient Norm: 11.587110432114521
Epoch: 4268, Batch Gradient Norm after: 11.587110432114521
Epoch 4269/10000, Prediction Accuracy = 61.774%, Loss = 0.4784731090068817
Epoch: 4269, Batch Gradient Norm: 7.004361288062128
Epoch: 4269, Batch Gradient Norm after: 7.004361288062128
Epoch 4270/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.44423598051071167
Epoch: 4270, Batch Gradient Norm: 10.032480614662445
Epoch: 4270, Batch Gradient Norm after: 10.032480614662445
Epoch 4271/10000, Prediction Accuracy = 61.878%, Loss = 0.4608554482460022
Epoch: 4271, Batch Gradient Norm: 12.201763423345863
Epoch: 4271, Batch Gradient Norm after: 12.201763423345863
Epoch 4272/10000, Prediction Accuracy = 61.794%, Loss = 0.4746242582798004
Epoch: 4272, Batch Gradient Norm: 11.607809651031996
Epoch: 4272, Batch Gradient Norm after: 11.607809651031996
Epoch 4273/10000, Prediction Accuracy = 61.786%, Loss = 0.4726801574230194
Epoch: 4273, Batch Gradient Norm: 9.939880800859026
Epoch: 4273, Batch Gradient Norm after: 9.939880800859026
Epoch 4274/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46691033244132996
Epoch: 4274, Batch Gradient Norm: 11.984476867649372
Epoch: 4274, Batch Gradient Norm after: 11.984476867649372
Epoch 4275/10000, Prediction Accuracy = 61.722%, Loss = 0.4811475932598114
Epoch: 4275, Batch Gradient Norm: 10.067349870875534
Epoch: 4275, Batch Gradient Norm after: 10.067349870875534
Epoch 4276/10000, Prediction Accuracy = 61.812%, Loss = 0.46318790316581726
Epoch: 4276, Batch Gradient Norm: 7.65403748420684
Epoch: 4276, Batch Gradient Norm after: 7.65403748420684
Epoch 4277/10000, Prediction Accuracy = 61.730000000000004%, Loss = 0.44836164116859434
Epoch: 4277, Batch Gradient Norm: 9.092597489693024
Epoch: 4277, Batch Gradient Norm after: 9.092597489693024
Epoch 4278/10000, Prediction Accuracy = 61.75%, Loss = 0.4576060056686401
Epoch: 4278, Batch Gradient Norm: 9.153677185517054
Epoch: 4278, Batch Gradient Norm after: 9.153677185517054
Epoch 4279/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.45761567950248716
Epoch: 4279, Batch Gradient Norm: 8.633556834256716
Epoch: 4279, Batch Gradient Norm after: 8.633556834256716
Epoch 4280/10000, Prediction Accuracy = 61.834%, Loss = 0.4517795562744141
Epoch: 4280, Batch Gradient Norm: 7.423681499196464
Epoch: 4280, Batch Gradient Norm after: 7.423681499196464
Epoch 4281/10000, Prediction Accuracy = 61.822%, Loss = 0.44479366540908816
Epoch: 4281, Batch Gradient Norm: 10.851669692214365
Epoch: 4281, Batch Gradient Norm after: 10.851669692214365
Epoch 4282/10000, Prediction Accuracy = 61.746%, Loss = 0.4705153346061707
Epoch: 4282, Batch Gradient Norm: 5.989314046381957
Epoch: 4282, Batch Gradient Norm after: 5.989314046381957
Epoch 4283/10000, Prediction Accuracy = 61.766%, Loss = 0.4397265136241913
Epoch: 4283, Batch Gradient Norm: 10.979804809266017
Epoch: 4283, Batch Gradient Norm after: 10.979804809266017
Epoch 4284/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.4753170728683472
Epoch: 4284, Batch Gradient Norm: 11.408916960368515
Epoch: 4284, Batch Gradient Norm after: 11.408916960368515
Epoch 4285/10000, Prediction Accuracy = 61.722%, Loss = 0.47119286060333254
Epoch: 4285, Batch Gradient Norm: 10.759895927339164
Epoch: 4285, Batch Gradient Norm after: 10.759895927339164
Epoch 4286/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.46645429730415344
Epoch: 4286, Batch Gradient Norm: 13.943358284516028
Epoch: 4286, Batch Gradient Norm after: 13.943358284516028
Epoch 4287/10000, Prediction Accuracy = 61.766%, Loss = 0.497113972902298
Epoch: 4287, Batch Gradient Norm: 9.376948181423414
Epoch: 4287, Batch Gradient Norm after: 9.376948181423414
Epoch 4288/10000, Prediction Accuracy = 61.758%, Loss = 0.4577416658401489
Epoch: 4288, Batch Gradient Norm: 9.232214301443788
Epoch: 4288, Batch Gradient Norm after: 9.232214301443788
Epoch 4289/10000, Prediction Accuracy = 61.73%, Loss = 0.4587182343006134
Epoch: 4289, Batch Gradient Norm: 10.220892937286157
Epoch: 4289, Batch Gradient Norm after: 10.220892937286157
Epoch 4290/10000, Prediction Accuracy = 61.666%, Loss = 0.4630084753036499
Epoch: 4290, Batch Gradient Norm: 13.241770036260696
Epoch: 4290, Batch Gradient Norm after: 13.241770036260696
Epoch 4291/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4825903236865997
Epoch: 4291, Batch Gradient Norm: 12.23622155967563
Epoch: 4291, Batch Gradient Norm after: 12.23622155967563
Epoch 4292/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4766120672225952
Epoch: 4292, Batch Gradient Norm: 6.316497591298644
Epoch: 4292, Batch Gradient Norm after: 6.316497591298644
Epoch 4293/10000, Prediction Accuracy = 61.666%, Loss = 0.44107070565223694
Epoch: 4293, Batch Gradient Norm: 5.876051968440281
Epoch: 4293, Batch Gradient Norm after: 5.876051968440281
Epoch 4294/10000, Prediction Accuracy = 61.674%, Loss = 0.44046295881271363
Epoch: 4294, Batch Gradient Norm: 8.642208105562045
Epoch: 4294, Batch Gradient Norm after: 8.642208105562045
Epoch 4295/10000, Prediction Accuracy = 61.83599999999999%, Loss = 0.4513814032077789
Epoch: 4295, Batch Gradient Norm: 10.696638857574264
Epoch: 4295, Batch Gradient Norm after: 10.696638857574264
Epoch 4296/10000, Prediction Accuracy = 61.746%, Loss = 0.46678908467292785
Epoch: 4296, Batch Gradient Norm: 9.650693872590285
Epoch: 4296, Batch Gradient Norm after: 9.650693872590285
Epoch 4297/10000, Prediction Accuracy = 61.634%, Loss = 0.4586864113807678
Epoch: 4297, Batch Gradient Norm: 12.573028044091439
Epoch: 4297, Batch Gradient Norm after: 12.573028044091439
Epoch 4298/10000, Prediction Accuracy = 61.7%, Loss = 0.4784416913986206
Epoch: 4298, Batch Gradient Norm: 11.6527787022733
Epoch: 4298, Batch Gradient Norm after: 11.6527787022733
Epoch 4299/10000, Prediction Accuracy = 61.80999999999999%, Loss = 0.4736063599586487
Epoch: 4299, Batch Gradient Norm: 9.478323528597988
Epoch: 4299, Batch Gradient Norm after: 9.478323528597988
Epoch 4300/10000, Prediction Accuracy = 61.754000000000005%, Loss = 0.4578108012676239
Epoch: 4300, Batch Gradient Norm: 10.618469139078842
Epoch: 4300, Batch Gradient Norm after: 10.618469139078842
Epoch 4301/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4647409498691559
Epoch: 4301, Batch Gradient Norm: 11.793322517942503
Epoch: 4301, Batch Gradient Norm after: 11.793322517942503
Epoch 4302/10000, Prediction Accuracy = 61.59599999999999%, Loss = 0.4794262409210205
Epoch: 4302, Batch Gradient Norm: 7.550320119184483
Epoch: 4302, Batch Gradient Norm after: 7.550320119184483
Epoch 4303/10000, Prediction Accuracy = 61.774%, Loss = 0.44647257328033446
Epoch: 4303, Batch Gradient Norm: 9.67478123251791
Epoch: 4303, Batch Gradient Norm after: 9.67478123251791
Epoch 4304/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4588875651359558
Epoch: 4304, Batch Gradient Norm: 9.79699791957251
Epoch: 4304, Batch Gradient Norm after: 9.79699791957251
Epoch 4305/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.46132286787033083
Epoch: 4305, Batch Gradient Norm: 6.589504628643018
Epoch: 4305, Batch Gradient Norm after: 6.589504628643018
Epoch 4306/10000, Prediction Accuracy = 61.826%, Loss = 0.442515504360199
Epoch: 4306, Batch Gradient Norm: 7.855076575494687
Epoch: 4306, Batch Gradient Norm after: 7.855076575494687
Epoch 4307/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.44895862340927123
Epoch: 4307, Batch Gradient Norm: 11.324684859823702
Epoch: 4307, Batch Gradient Norm after: 11.324684859823702
Epoch 4308/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.4691037893295288
Epoch: 4308, Batch Gradient Norm: 9.979194525210781
Epoch: 4308, Batch Gradient Norm after: 9.979194525210781
Epoch 4309/10000, Prediction Accuracy = 61.838%, Loss = 0.46178696155548093
Epoch: 4309, Batch Gradient Norm: 8.26187374638301
Epoch: 4309, Batch Gradient Norm after: 8.26187374638301
Epoch 4310/10000, Prediction Accuracy = 61.7%, Loss = 0.4494886636734009
Epoch: 4310, Batch Gradient Norm: 12.40122008074727
Epoch: 4310, Batch Gradient Norm after: 12.40122008074727
Epoch 4311/10000, Prediction Accuracy = 61.75%, Loss = 0.4765350878238678
Epoch: 4311, Batch Gradient Norm: 10.676759712150456
Epoch: 4311, Batch Gradient Norm after: 10.676759712150456
Epoch 4312/10000, Prediction Accuracy = 61.73%, Loss = 0.4656587719917297
Epoch: 4312, Batch Gradient Norm: 8.40483451570766
Epoch: 4312, Batch Gradient Norm after: 8.40483451570766
Epoch 4313/10000, Prediction Accuracy = 61.75600000000001%, Loss = 0.45238322019577026
Epoch: 4313, Batch Gradient Norm: 12.008180682369984
Epoch: 4313, Batch Gradient Norm after: 12.008180682369984
Epoch 4314/10000, Prediction Accuracy = 61.75%, Loss = 0.47605790495872496
Epoch: 4314, Batch Gradient Norm: 10.873980107941149
Epoch: 4314, Batch Gradient Norm after: 10.873980107941149
Epoch 4315/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.46689882278442385
Epoch: 4315, Batch Gradient Norm: 11.400940425086356
Epoch: 4315, Batch Gradient Norm after: 11.400940425086356
Epoch 4316/10000, Prediction Accuracy = 61.67%, Loss = 0.4692129611968994
Epoch: 4316, Batch Gradient Norm: 11.942649777501787
Epoch: 4316, Batch Gradient Norm after: 11.942649777501787
Epoch 4317/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.47600541114807127
Epoch: 4317, Batch Gradient Norm: 10.836780628508851
Epoch: 4317, Batch Gradient Norm after: 10.836780628508851
Epoch 4318/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4681559920310974
Epoch: 4318, Batch Gradient Norm: 7.850204816714121
Epoch: 4318, Batch Gradient Norm after: 7.850204816714121
Epoch 4319/10000, Prediction Accuracy = 61.7%, Loss = 0.44830683469772337
Epoch: 4319, Batch Gradient Norm: 7.023656009943419
Epoch: 4319, Batch Gradient Norm after: 7.023656009943419
Epoch 4320/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.44358068108558657
Epoch: 4320, Batch Gradient Norm: 10.027184695320773
Epoch: 4320, Batch Gradient Norm after: 10.027184695320773
Epoch 4321/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.45751150846481325
Epoch: 4321, Batch Gradient Norm: 10.274355659551142
Epoch: 4321, Batch Gradient Norm after: 10.274355659551142
Epoch 4322/10000, Prediction Accuracy = 61.766%, Loss = 0.46111670732498167
Epoch: 4322, Batch Gradient Norm: 8.28039935035868
Epoch: 4322, Batch Gradient Norm after: 8.28039935035868
Epoch 4323/10000, Prediction Accuracy = 61.688%, Loss = 0.45001996755599977
Epoch: 4323, Batch Gradient Norm: 6.673286599239975
Epoch: 4323, Batch Gradient Norm after: 6.673286599239975
Epoch 4324/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.44324753284454343
Epoch: 4324, Batch Gradient Norm: 12.612120994098273
Epoch: 4324, Batch Gradient Norm after: 12.612120994098273
Epoch 4325/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.48715352416038515
Epoch: 4325, Batch Gradient Norm: 11.002145870002536
Epoch: 4325, Batch Gradient Norm after: 11.002145870002536
Epoch 4326/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.46684926748275757
Epoch: 4326, Batch Gradient Norm: 11.78742327892722
Epoch: 4326, Batch Gradient Norm after: 11.78742327892722
Epoch 4327/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.4774422526359558
Epoch: 4327, Batch Gradient Norm: 8.7607829083631
Epoch: 4327, Batch Gradient Norm after: 8.7607829083631
Epoch 4328/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.45609796047210693
Epoch: 4328, Batch Gradient Norm: 7.016706398576478
Epoch: 4328, Batch Gradient Norm after: 7.016706398576478
Epoch 4329/10000, Prediction Accuracy = 61.864%, Loss = 0.4426872253417969
Epoch: 4329, Batch Gradient Norm: 10.017247407457788
Epoch: 4329, Batch Gradient Norm after: 10.017247407457788
Epoch 4330/10000, Prediction Accuracy = 61.86%, Loss = 0.4591168224811554
Epoch: 4330, Batch Gradient Norm: 11.298272086708621
Epoch: 4330, Batch Gradient Norm after: 11.298272086708621
Epoch 4331/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.4683412551879883
Epoch: 4331, Batch Gradient Norm: 8.548915406337894
Epoch: 4331, Batch Gradient Norm after: 8.548915406337894
Epoch 4332/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4492433309555054
Epoch: 4332, Batch Gradient Norm: 11.45093902997531
Epoch: 4332, Batch Gradient Norm after: 11.45093902997531
Epoch 4333/10000, Prediction Accuracy = 61.698%, Loss = 0.47272809147834777
Epoch: 4333, Batch Gradient Norm: 8.919655659677298
Epoch: 4333, Batch Gradient Norm after: 8.919655659677298
Epoch 4334/10000, Prediction Accuracy = 61.774%, Loss = 0.45564127564430235
Epoch: 4334, Batch Gradient Norm: 8.587129208855634
Epoch: 4334, Batch Gradient Norm after: 8.587129208855634
Epoch 4335/10000, Prediction Accuracy = 61.626%, Loss = 0.45109531879425047
Epoch: 4335, Batch Gradient Norm: 11.366857055520343
Epoch: 4335, Batch Gradient Norm after: 11.366857055520343
Epoch 4336/10000, Prediction Accuracy = 61.914%, Loss = 0.4697106659412384
Epoch: 4336, Batch Gradient Norm: 10.428794765463614
Epoch: 4336, Batch Gradient Norm after: 10.428794765463614
Epoch 4337/10000, Prediction Accuracy = 61.80400000000001%, Loss = 0.46229777932167054
Epoch: 4337, Batch Gradient Norm: 10.242772641480034
Epoch: 4337, Batch Gradient Norm after: 10.242772641480034
Epoch 4338/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4658791422843933
Epoch: 4338, Batch Gradient Norm: 7.045853659900587
Epoch: 4338, Batch Gradient Norm after: 7.045853659900587
Epoch 4339/10000, Prediction Accuracy = 61.8%, Loss = 0.44303934574127196
Epoch: 4339, Batch Gradient Norm: 8.841873888491875
Epoch: 4339, Batch Gradient Norm after: 8.841873888491875
Epoch 4340/10000, Prediction Accuracy = 61.77%, Loss = 0.4507133185863495
Epoch: 4340, Batch Gradient Norm: 10.316286487799697
Epoch: 4340, Batch Gradient Norm after: 10.316286487799697
Epoch 4341/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.46250095367431643
Epoch: 4341, Batch Gradient Norm: 8.899727573882215
Epoch: 4341, Batch Gradient Norm after: 8.899727573882215
Epoch 4342/10000, Prediction Accuracy = 61.762%, Loss = 0.4523175835609436
Epoch: 4342, Batch Gradient Norm: 11.05461590846539
Epoch: 4342, Batch Gradient Norm after: 11.05461590846539
Epoch 4343/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4670329749584198
Epoch: 4343, Batch Gradient Norm: 10.837276049667075
Epoch: 4343, Batch Gradient Norm after: 10.837276049667075
Epoch 4344/10000, Prediction Accuracy = 61.686%, Loss = 0.46659440994262696
Epoch: 4344, Batch Gradient Norm: 11.225029075263171
Epoch: 4344, Batch Gradient Norm after: 11.225029075263171
Epoch 4345/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.4689414441585541
Epoch: 4345, Batch Gradient Norm: 12.058343049605147
Epoch: 4345, Batch Gradient Norm after: 12.058343049605147
Epoch 4346/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4765383243560791
Epoch: 4346, Batch Gradient Norm: 10.419162509997827
Epoch: 4346, Batch Gradient Norm after: 10.419162509997827
Epoch 4347/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4609813868999481
Epoch: 4347, Batch Gradient Norm: 9.766235561728665
Epoch: 4347, Batch Gradient Norm after: 9.766235561728665
Epoch 4348/10000, Prediction Accuracy = 61.726%, Loss = 0.4566272497177124
Epoch: 4348, Batch Gradient Norm: 8.84513397507838
Epoch: 4348, Batch Gradient Norm after: 8.84513397507838
Epoch 4349/10000, Prediction Accuracy = 61.73%, Loss = 0.45048094987869264
Epoch: 4349, Batch Gradient Norm: 10.713819648012787
Epoch: 4349, Batch Gradient Norm after: 10.713819648012787
Epoch 4350/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4645762801170349
Epoch: 4350, Batch Gradient Norm: 8.900795047143502
Epoch: 4350, Batch Gradient Norm after: 8.900795047143502
Epoch 4351/10000, Prediction Accuracy = 61.898%, Loss = 0.4538323700428009
Epoch: 4351, Batch Gradient Norm: 7.623771979787315
Epoch: 4351, Batch Gradient Norm after: 7.623771979787315
Epoch 4352/10000, Prediction Accuracy = 61.862%, Loss = 0.44457094073295594
Epoch: 4352, Batch Gradient Norm: 9.120114385539766
Epoch: 4352, Batch Gradient Norm after: 9.120114385539766
Epoch 4353/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4570544421672821
Epoch: 4353, Batch Gradient Norm: 9.857520001553828
Epoch: 4353, Batch Gradient Norm after: 9.857520001553828
Epoch 4354/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4579464256763458
Epoch: 4354, Batch Gradient Norm: 12.429157933678596
Epoch: 4354, Batch Gradient Norm after: 12.429157933678596
Epoch 4355/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.47572062015533445
Epoch: 4355, Batch Gradient Norm: 9.474061754416853
Epoch: 4355, Batch Gradient Norm after: 9.474061754416853
Epoch 4356/10000, Prediction Accuracy = 61.638%, Loss = 0.4586329936981201
Epoch: 4356, Batch Gradient Norm: 6.076134248249331
Epoch: 4356, Batch Gradient Norm after: 6.076134248249331
Epoch 4357/10000, Prediction Accuracy = 61.77%, Loss = 0.43962952494621277
Epoch: 4357, Batch Gradient Norm: 8.859193379209042
Epoch: 4357, Batch Gradient Norm after: 8.859193379209042
Epoch 4358/10000, Prediction Accuracy = 61.78399999999999%, Loss = 0.4561701714992523
Epoch: 4358, Batch Gradient Norm: 11.596431278045737
Epoch: 4358, Batch Gradient Norm after: 11.596431278045737
Epoch 4359/10000, Prediction Accuracy = 61.64%, Loss = 0.47265289425849916
Epoch: 4359, Batch Gradient Norm: 12.264358797709091
Epoch: 4359, Batch Gradient Norm after: 12.264358797709091
Epoch 4360/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4777085900306702
Epoch: 4360, Batch Gradient Norm: 8.539368813740598
Epoch: 4360, Batch Gradient Norm after: 8.539368813740598
Epoch 4361/10000, Prediction Accuracy = 61.698%, Loss = 0.4505297660827637
Epoch: 4361, Batch Gradient Norm: 9.568460170297481
Epoch: 4361, Batch Gradient Norm after: 9.568460170297481
Epoch 4362/10000, Prediction Accuracy = 61.81%, Loss = 0.4600542724132538
Epoch: 4362, Batch Gradient Norm: 7.990069857279709
Epoch: 4362, Batch Gradient Norm after: 7.990069857279709
Epoch 4363/10000, Prediction Accuracy = 61.614%, Loss = 0.44898590445518494
Epoch: 4363, Batch Gradient Norm: 13.680806527080776
Epoch: 4363, Batch Gradient Norm after: 13.680806527080776
Epoch 4364/10000, Prediction Accuracy = 61.794%, Loss = 0.4872403144836426
Epoch: 4364, Batch Gradient Norm: 10.911903126620313
Epoch: 4364, Batch Gradient Norm after: 10.911903126620313
Epoch 4365/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.4664255023002625
Epoch: 4365, Batch Gradient Norm: 9.48063801021767
Epoch: 4365, Batch Gradient Norm after: 9.48063801021767
Epoch 4366/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.45860318541526796
Epoch: 4366, Batch Gradient Norm: 12.455916903309342
Epoch: 4366, Batch Gradient Norm after: 12.455916903309342
Epoch 4367/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.48321275115013124
Epoch: 4367, Batch Gradient Norm: 8.699396246141273
Epoch: 4367, Batch Gradient Norm after: 8.699396246141273
Epoch 4368/10000, Prediction Accuracy = 61.843999999999994%, Loss = 0.4500028908252716
Epoch: 4368, Batch Gradient Norm: 9.843581606329474
Epoch: 4368, Batch Gradient Norm after: 9.843581606329474
Epoch 4369/10000, Prediction Accuracy = 61.612%, Loss = 0.45718529224395754
Epoch: 4369, Batch Gradient Norm: 12.216673378023197
Epoch: 4369, Batch Gradient Norm after: 12.216673378023197
Epoch 4370/10000, Prediction Accuracy = 61.68000000000001%, Loss = 0.4802538096904755
Epoch: 4370, Batch Gradient Norm: 9.258565701471321
Epoch: 4370, Batch Gradient Norm after: 9.258565701471321
Epoch 4371/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.4546417951583862
Epoch: 4371, Batch Gradient Norm: 9.93653180819283
Epoch: 4371, Batch Gradient Norm after: 9.93653180819283
Epoch 4372/10000, Prediction Accuracy = 61.686%, Loss = 0.46057575941085815
Epoch: 4372, Batch Gradient Norm: 9.568858112252805
Epoch: 4372, Batch Gradient Norm after: 9.568858112252805
Epoch 4373/10000, Prediction Accuracy = 61.762%, Loss = 0.4582676887512207
Epoch: 4373, Batch Gradient Norm: 9.718690287552944
Epoch: 4373, Batch Gradient Norm after: 9.718690287552944
Epoch 4374/10000, Prediction Accuracy = 61.758%, Loss = 0.4568286299705505
Epoch: 4374, Batch Gradient Norm: 10.14598873312077
Epoch: 4374, Batch Gradient Norm after: 10.14598873312077
Epoch 4375/10000, Prediction Accuracy = 61.648%, Loss = 0.4603551745414734
Epoch: 4375, Batch Gradient Norm: 8.517381234808528
Epoch: 4375, Batch Gradient Norm after: 8.517381234808528
Epoch 4376/10000, Prediction Accuracy = 61.948%, Loss = 0.4499527931213379
Epoch: 4376, Batch Gradient Norm: 9.149839952273641
Epoch: 4376, Batch Gradient Norm after: 9.149839952273641
Epoch 4377/10000, Prediction Accuracy = 61.91199999999999%, Loss = 0.451404470205307
Epoch: 4377, Batch Gradient Norm: 8.29718739799264
Epoch: 4377, Batch Gradient Norm after: 8.29718739799264
Epoch 4378/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.4469752013683319
Epoch: 4378, Batch Gradient Norm: 12.75081161461692
Epoch: 4378, Batch Gradient Norm after: 12.75081161461692
Epoch 4379/10000, Prediction Accuracy = 61.698%, Loss = 0.47601935267448425
Epoch: 4379, Batch Gradient Norm: 11.611916564207968
Epoch: 4379, Batch Gradient Norm after: 11.611916564207968
Epoch 4380/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4719557881355286
Epoch: 4380, Batch Gradient Norm: 9.282017792596653
Epoch: 4380, Batch Gradient Norm after: 9.282017792596653
Epoch 4381/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.45598735213279723
Epoch: 4381, Batch Gradient Norm: 8.905416646758725
Epoch: 4381, Batch Gradient Norm after: 8.905416646758725
Epoch 4382/10000, Prediction Accuracy = 61.734%, Loss = 0.4496725618839264
Epoch: 4382, Batch Gradient Norm: 7.7355212616754025
Epoch: 4382, Batch Gradient Norm after: 7.7355212616754025
Epoch 4383/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4431928753852844
Epoch: 4383, Batch Gradient Norm: 9.029394192949795
Epoch: 4383, Batch Gradient Norm after: 9.029394192949795
Epoch 4384/10000, Prediction Accuracy = 61.746%, Loss = 0.4517798602581024
Epoch: 4384, Batch Gradient Norm: 10.761242596740024
Epoch: 4384, Batch Gradient Norm after: 10.761242596740024
Epoch 4385/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4655524015426636
Epoch: 4385, Batch Gradient Norm: 8.646390361860451
Epoch: 4385, Batch Gradient Norm after: 8.646390361860451
Epoch 4386/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.45362454652786255
Epoch: 4386, Batch Gradient Norm: 9.113583234275149
Epoch: 4386, Batch Gradient Norm after: 9.113583234275149
Epoch 4387/10000, Prediction Accuracy = 61.826%, Loss = 0.45630080103874204
Epoch: 4387, Batch Gradient Norm: 10.960110359232068
Epoch: 4387, Batch Gradient Norm after: 10.960110359232068
Epoch 4388/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4671466052532196
Epoch: 4388, Batch Gradient Norm: 10.710184621924283
Epoch: 4388, Batch Gradient Norm after: 10.710184621924283
Epoch 4389/10000, Prediction Accuracy = 61.734%, Loss = 0.47229443192481996
Epoch: 4389, Batch Gradient Norm: 8.805471251961805
Epoch: 4389, Batch Gradient Norm after: 8.805471251961805
Epoch 4390/10000, Prediction Accuracy = 61.814%, Loss = 0.4540270805358887
Epoch: 4390, Batch Gradient Norm: 9.151387459759958
Epoch: 4390, Batch Gradient Norm after: 9.151387459759958
Epoch 4391/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4540854156017303
Epoch: 4391, Batch Gradient Norm: 10.183062597392173
Epoch: 4391, Batch Gradient Norm after: 10.183062597392173
Epoch 4392/10000, Prediction Accuracy = 61.74000000000001%, Loss = 0.4610681414604187
Epoch: 4392, Batch Gradient Norm: 7.873310590868514
Epoch: 4392, Batch Gradient Norm after: 7.873310590868514
Epoch 4393/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4455717384815216
Epoch: 4393, Batch Gradient Norm: 12.663308678645302
Epoch: 4393, Batch Gradient Norm after: 12.663308678645302
Epoch 4394/10000, Prediction Accuracy = 61.746%, Loss = 0.47666717171669004
Epoch: 4394, Batch Gradient Norm: 11.886971114779946
Epoch: 4394, Batch Gradient Norm after: 11.886971114779946
Epoch 4395/10000, Prediction Accuracy = 61.628%, Loss = 0.470141738653183
Epoch: 4395, Batch Gradient Norm: 11.91862631937697
Epoch: 4395, Batch Gradient Norm after: 11.91862631937697
Epoch 4396/10000, Prediction Accuracy = 61.694%, Loss = 0.47007848620414733
Epoch: 4396, Batch Gradient Norm: 9.854490421524444
Epoch: 4396, Batch Gradient Norm after: 9.854490421524444
Epoch 4397/10000, Prediction Accuracy = 61.888%, Loss = 0.4572703421115875
Epoch: 4397, Batch Gradient Norm: 8.990431169431771
Epoch: 4397, Batch Gradient Norm after: 8.990431169431771
Epoch 4398/10000, Prediction Accuracy = 61.926%, Loss = 0.4514312148094177
Epoch: 4398, Batch Gradient Norm: 9.948217342454699
Epoch: 4398, Batch Gradient Norm after: 9.948217342454699
Epoch 4399/10000, Prediction Accuracy = 61.85%, Loss = 0.45946674346923827
Epoch: 4399, Batch Gradient Norm: 11.4779912707138
Epoch: 4399, Batch Gradient Norm after: 11.4779912707138
Epoch 4400/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4710163950920105
Epoch: 4400, Batch Gradient Norm: 9.258770063578547
Epoch: 4400, Batch Gradient Norm after: 9.258770063578547
Epoch 4401/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4556339204311371
Epoch: 4401, Batch Gradient Norm: 9.467917404960444
Epoch: 4401, Batch Gradient Norm after: 9.467917404960444
Epoch 4402/10000, Prediction Accuracy = 61.81%, Loss = 0.45608893036842346
Epoch: 4402, Batch Gradient Norm: 9.271816586736582
Epoch: 4402, Batch Gradient Norm after: 9.271816586736582
Epoch 4403/10000, Prediction Accuracy = 61.89%, Loss = 0.45634215474128725
Epoch: 4403, Batch Gradient Norm: 10.406928967667328
Epoch: 4403, Batch Gradient Norm after: 10.406928967667328
Epoch 4404/10000, Prediction Accuracy = 61.814%, Loss = 0.4613323509693146
Epoch: 4404, Batch Gradient Norm: 10.522306718341754
Epoch: 4404, Batch Gradient Norm after: 10.522306718341754
Epoch 4405/10000, Prediction Accuracy = 61.826%, Loss = 0.46387099623680117
Epoch: 4405, Batch Gradient Norm: 8.508865790329544
Epoch: 4405, Batch Gradient Norm after: 8.508865790329544
Epoch 4406/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.45108565092086794
Epoch: 4406, Batch Gradient Norm: 10.468033431489385
Epoch: 4406, Batch Gradient Norm after: 10.468033431489385
Epoch 4407/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4609668731689453
Epoch: 4407, Batch Gradient Norm: 10.616671759209822
Epoch: 4407, Batch Gradient Norm after: 10.616671759209822
Epoch 4408/10000, Prediction Accuracy = 61.7%, Loss = 0.4639001727104187
Epoch: 4408, Batch Gradient Norm: 10.745028079820402
Epoch: 4408, Batch Gradient Norm after: 10.745028079820402
Epoch 4409/10000, Prediction Accuracy = 61.876%, Loss = 0.4640883386135101
Epoch: 4409, Batch Gradient Norm: 10.715366387116623
Epoch: 4409, Batch Gradient Norm after: 10.715366387116623
Epoch 4410/10000, Prediction Accuracy = 61.712%, Loss = 0.46716219186782837
Epoch: 4410, Batch Gradient Norm: 9.043050659098608
Epoch: 4410, Batch Gradient Norm after: 9.043050659098608
Epoch 4411/10000, Prediction Accuracy = 61.812%, Loss = 0.4524907052516937
Epoch: 4411, Batch Gradient Norm: 10.420507456933981
Epoch: 4411, Batch Gradient Norm after: 10.420507456933981
Epoch 4412/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.46582759618759156
Epoch: 4412, Batch Gradient Norm: 6.421696274212631
Epoch: 4412, Batch Gradient Norm after: 6.421696274212631
Epoch 4413/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.43985781669616697
Epoch: 4413, Batch Gradient Norm: 10.689225152373854
Epoch: 4413, Batch Gradient Norm after: 10.689225152373854
Epoch 4414/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.46112478375434873
Epoch: 4414, Batch Gradient Norm: 10.990800223515812
Epoch: 4414, Batch Gradient Norm after: 10.990800223515812
Epoch 4415/10000, Prediction Accuracy = 61.67800000000001%, Loss = 0.4657296895980835
Epoch: 4415, Batch Gradient Norm: 11.833191829332572
Epoch: 4415, Batch Gradient Norm after: 11.833191829332572
Epoch 4416/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4785115718841553
Epoch: 4416, Batch Gradient Norm: 10.778748074854542
Epoch: 4416, Batch Gradient Norm after: 10.778748074854542
Epoch 4417/10000, Prediction Accuracy = 61.836%, Loss = 0.4631096422672272
Epoch: 4417, Batch Gradient Norm: 9.664728839951918
Epoch: 4417, Batch Gradient Norm after: 9.664728839951918
Epoch 4418/10000, Prediction Accuracy = 61.622%, Loss = 0.4585917294025421
Epoch: 4418, Batch Gradient Norm: 9.077536160540651
Epoch: 4418, Batch Gradient Norm after: 9.077536160540651
Epoch 4419/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.45155497789382937
Epoch: 4419, Batch Gradient Norm: 10.559797043136527
Epoch: 4419, Batch Gradient Norm after: 10.559797043136527
Epoch 4420/10000, Prediction Accuracy = 61.762%, Loss = 0.46271662712097167
Epoch: 4420, Batch Gradient Norm: 12.120940073621544
Epoch: 4420, Batch Gradient Norm after: 12.120940073621544
Epoch 4421/10000, Prediction Accuracy = 61.71600000000001%, Loss = 0.47117058634757997
Epoch: 4421, Batch Gradient Norm: 11.291317638551332
Epoch: 4421, Batch Gradient Norm after: 11.291317638551332
Epoch 4422/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4670267939567566
Epoch: 4422, Batch Gradient Norm: 12.320362385174835
Epoch: 4422, Batch Gradient Norm after: 12.320362385174835
Epoch 4423/10000, Prediction Accuracy = 61.602%, Loss = 0.4797159910202026
Epoch: 4423, Batch Gradient Norm: 8.288069105575316
Epoch: 4423, Batch Gradient Norm after: 8.288069105575316
Epoch 4424/10000, Prediction Accuracy = 61.714%, Loss = 0.4490549027919769
Epoch: 4424, Batch Gradient Norm: 6.193670324739363
Epoch: 4424, Batch Gradient Norm after: 6.193670324739363
Epoch 4425/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4360175609588623
Epoch: 4425, Batch Gradient Norm: 9.65364918181865
Epoch: 4425, Batch Gradient Norm after: 9.65364918181865
Epoch 4426/10000, Prediction Accuracy = 61.788%, Loss = 0.4534710705280304
Epoch: 4426, Batch Gradient Norm: 9.43831274869358
Epoch: 4426, Batch Gradient Norm after: 9.43831274869358
Epoch 4427/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.45279586911201475
Epoch: 4427, Batch Gradient Norm: 9.6692869926781
Epoch: 4427, Batch Gradient Norm after: 9.6692869926781
Epoch 4428/10000, Prediction Accuracy = 61.78399999999999%, Loss = 0.4564144253730774
Epoch: 4428, Batch Gradient Norm: 9.59910076638373
Epoch: 4428, Batch Gradient Norm after: 9.59910076638373
Epoch 4429/10000, Prediction Accuracy = 61.626%, Loss = 0.4585641622543335
Epoch: 4429, Batch Gradient Norm: 6.507151490833062
Epoch: 4429, Batch Gradient Norm after: 6.507151490833062
Epoch 4430/10000, Prediction Accuracy = 61.666%, Loss = 0.43867468237876894
Epoch: 4430, Batch Gradient Norm: 13.093051686448147
Epoch: 4430, Batch Gradient Norm after: 13.093051686448147
Epoch 4431/10000, Prediction Accuracy = 61.822%, Loss = 0.48131311535835264
Epoch: 4431, Batch Gradient Norm: 12.092743645337581
Epoch: 4431, Batch Gradient Norm after: 12.092743645337581
Epoch 4432/10000, Prediction Accuracy = 61.702%, Loss = 0.47805179953575133
Epoch: 4432, Batch Gradient Norm: 7.287358983878463
Epoch: 4432, Batch Gradient Norm after: 7.287358983878463
Epoch 4433/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.4449317753314972
Epoch: 4433, Batch Gradient Norm: 7.537755991702394
Epoch: 4433, Batch Gradient Norm after: 7.537755991702394
Epoch 4434/10000, Prediction Accuracy = 61.86%, Loss = 0.441873037815094
Epoch: 4434, Batch Gradient Norm: 11.826079547506572
Epoch: 4434, Batch Gradient Norm after: 11.826079547506572
Epoch 4435/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.4748023092746735
Epoch: 4435, Batch Gradient Norm: 9.179463195502985
Epoch: 4435, Batch Gradient Norm after: 9.179463195502985
Epoch 4436/10000, Prediction Accuracy = 61.878%, Loss = 0.45149248242378237
Epoch: 4436, Batch Gradient Norm: 7.583616684890037
Epoch: 4436, Batch Gradient Norm after: 7.583616684890037
Epoch 4437/10000, Prediction Accuracy = 61.722%, Loss = 0.4418478965759277
Epoch: 4437, Batch Gradient Norm: 7.309167362786889
Epoch: 4437, Batch Gradient Norm after: 7.309167362786889
Epoch 4438/10000, Prediction Accuracy = 61.794%, Loss = 0.4421954810619354
Epoch: 4438, Batch Gradient Norm: 8.518178137582824
Epoch: 4438, Batch Gradient Norm after: 8.518178137582824
Epoch 4439/10000, Prediction Accuracy = 61.754%, Loss = 0.4467620253562927
Epoch: 4439, Batch Gradient Norm: 10.893714184761869
Epoch: 4439, Batch Gradient Norm after: 10.893714184761869
Epoch 4440/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.4609954833984375
Epoch: 4440, Batch Gradient Norm: 12.27686996978872
Epoch: 4440, Batch Gradient Norm after: 12.27686996978872
Epoch 4441/10000, Prediction Accuracy = 61.766000000000005%, Loss = 0.4771028280258179
Epoch: 4441, Batch Gradient Norm: 11.836038176431705
Epoch: 4441, Batch Gradient Norm after: 11.836038176431705
Epoch 4442/10000, Prediction Accuracy = 61.664%, Loss = 0.47447351813316346
Epoch: 4442, Batch Gradient Norm: 11.018994832693458
Epoch: 4442, Batch Gradient Norm after: 11.018994832693458
Epoch 4443/10000, Prediction Accuracy = 61.686%, Loss = 0.46294829845428465
Epoch: 4443, Batch Gradient Norm: 11.47651441918168
Epoch: 4443, Batch Gradient Norm after: 11.47651441918168
Epoch 4444/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.467252379655838
Epoch: 4444, Batch Gradient Norm: 10.022020568714947
Epoch: 4444, Batch Gradient Norm after: 10.022020568714947
Epoch 4445/10000, Prediction Accuracy = 61.786%, Loss = 0.45603397488594055
Epoch: 4445, Batch Gradient Norm: 9.719066893858136
Epoch: 4445, Batch Gradient Norm after: 9.719066893858136
Epoch 4446/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.45182085037231445
Epoch: 4446, Batch Gradient Norm: 9.219998334791118
Epoch: 4446, Batch Gradient Norm after: 9.219998334791118
Epoch 4447/10000, Prediction Accuracy = 61.742%, Loss = 0.45009828805923463
Epoch: 4447, Batch Gradient Norm: 7.667572599726588
Epoch: 4447, Batch Gradient Norm after: 7.667572599726588
Epoch 4448/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.44376165270805357
Epoch: 4448, Batch Gradient Norm: 8.443202192844103
Epoch: 4448, Batch Gradient Norm after: 8.443202192844103
Epoch 4449/10000, Prediction Accuracy = 61.89399999999999%, Loss = 0.4479209423065186
Epoch: 4449, Batch Gradient Norm: 11.008254237691117
Epoch: 4449, Batch Gradient Norm after: 11.008254237691117
Epoch 4450/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.46459686756134033
Epoch: 4450, Batch Gradient Norm: 10.12178125546433
Epoch: 4450, Batch Gradient Norm after: 10.12178125546433
Epoch 4451/10000, Prediction Accuracy = 61.738%, Loss = 0.45679903626441953
Epoch: 4451, Batch Gradient Norm: 10.012466493537614
Epoch: 4451, Batch Gradient Norm after: 10.012466493537614
Epoch 4452/10000, Prediction Accuracy = 61.862%, Loss = 0.45508408546447754
Epoch: 4452, Batch Gradient Norm: 11.118541210312173
Epoch: 4452, Batch Gradient Norm after: 11.118541210312173
Epoch 4453/10000, Prediction Accuracy = 61.77%, Loss = 0.4667640864849091
Epoch: 4453, Batch Gradient Norm: 10.325559017167377
Epoch: 4453, Batch Gradient Norm after: 10.325559017167377
Epoch 4454/10000, Prediction Accuracy = 61.664%, Loss = 0.4640091359615326
Epoch: 4454, Batch Gradient Norm: 9.290615581567495
Epoch: 4454, Batch Gradient Norm after: 9.290615581567495
Epoch 4455/10000, Prediction Accuracy = 61.884%, Loss = 0.45419837832450866
Epoch: 4455, Batch Gradient Norm: 10.994683421335498
Epoch: 4455, Batch Gradient Norm after: 10.994683421335498
Epoch 4456/10000, Prediction Accuracy = 61.86999999999999%, Loss = 0.46253666281700134
Epoch: 4456, Batch Gradient Norm: 9.53304469173598
Epoch: 4456, Batch Gradient Norm after: 9.53304469173598
Epoch 4457/10000, Prediction Accuracy = 61.826%, Loss = 0.45287674069404604
Epoch: 4457, Batch Gradient Norm: 8.756029188424904
Epoch: 4457, Batch Gradient Norm after: 8.756029188424904
Epoch 4458/10000, Prediction Accuracy = 61.826%, Loss = 0.45055097341537476
Epoch: 4458, Batch Gradient Norm: 9.855001949117286
Epoch: 4458, Batch Gradient Norm after: 9.855001949117286
Epoch 4459/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4631422221660614
Epoch: 4459, Batch Gradient Norm: 6.711044108832163
Epoch: 4459, Batch Gradient Norm after: 6.711044108832163
Epoch 4460/10000, Prediction Accuracy = 61.662%, Loss = 0.4426685631275177
Epoch: 4460, Batch Gradient Norm: 10.986523276626261
Epoch: 4460, Batch Gradient Norm after: 10.986523276626261
Epoch 4461/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.46227983832359315
Epoch: 4461, Batch Gradient Norm: 12.032456886818446
Epoch: 4461, Batch Gradient Norm after: 12.032456886818446
Epoch 4462/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.4710512638092041
Epoch: 4462, Batch Gradient Norm: 9.138713167050822
Epoch: 4462, Batch Gradient Norm after: 9.138713167050822
Epoch 4463/10000, Prediction Accuracy = 61.774%, Loss = 0.4491142749786377
Epoch: 4463, Batch Gradient Norm: 9.51328337062828
Epoch: 4463, Batch Gradient Norm after: 9.51328337062828
Epoch 4464/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.45142407417297364
Epoch: 4464, Batch Gradient Norm: 12.37969997550459
Epoch: 4464, Batch Gradient Norm after: 12.37969997550459
Epoch 4465/10000, Prediction Accuracy = 61.794%, Loss = 0.47569126486778257
Epoch: 4465, Batch Gradient Norm: 10.282411329953325
Epoch: 4465, Batch Gradient Norm after: 10.282411329953325
Epoch 4466/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.45864230394363403
Epoch: 4466, Batch Gradient Norm: 8.256702728620345
Epoch: 4466, Batch Gradient Norm after: 8.256702728620345
Epoch 4467/10000, Prediction Accuracy = 61.762%, Loss = 0.4463313281536102
Epoch: 4467, Batch Gradient Norm: 8.952098132254559
Epoch: 4467, Batch Gradient Norm after: 8.952098132254559
Epoch 4468/10000, Prediction Accuracy = 61.862%, Loss = 0.4506674468517303
Epoch: 4468, Batch Gradient Norm: 11.502884858904117
Epoch: 4468, Batch Gradient Norm after: 11.502884858904117
Epoch 4469/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.4673964560031891
Epoch: 4469, Batch Gradient Norm: 9.531876020018787
Epoch: 4469, Batch Gradient Norm after: 9.531876020018787
Epoch 4470/10000, Prediction Accuracy = 61.872%, Loss = 0.4527361989021301
Epoch: 4470, Batch Gradient Norm: 7.497091663437353
Epoch: 4470, Batch Gradient Norm after: 7.497091663437353
Epoch 4471/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.44132099747657777
Epoch: 4471, Batch Gradient Norm: 6.937927643297141
Epoch: 4471, Batch Gradient Norm after: 6.937927643297141
Epoch 4472/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.44144795536994935
Epoch: 4472, Batch Gradient Norm: 8.614001877702282
Epoch: 4472, Batch Gradient Norm after: 8.614001877702282
Epoch 4473/10000, Prediction Accuracy = 61.702%, Loss = 0.446939754486084
Epoch: 4473, Batch Gradient Norm: 11.620570132864438
Epoch: 4473, Batch Gradient Norm after: 11.620570132864438
Epoch 4474/10000, Prediction Accuracy = 61.788%, Loss = 0.4690469026565552
Epoch: 4474, Batch Gradient Norm: 10.25398151888764
Epoch: 4474, Batch Gradient Norm after: 10.25398151888764
Epoch 4475/10000, Prediction Accuracy = 61.866%, Loss = 0.45928871631622314
Epoch: 4475, Batch Gradient Norm: 10.86004085897059
Epoch: 4475, Batch Gradient Norm after: 10.86004085897059
Epoch 4476/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.46282442212104796
Epoch: 4476, Batch Gradient Norm: 10.010164555557855
Epoch: 4476, Batch Gradient Norm after: 10.010164555557855
Epoch 4477/10000, Prediction Accuracy = 61.658%, Loss = 0.45606250762939454
Epoch: 4477, Batch Gradient Norm: 8.043525353920373
Epoch: 4477, Batch Gradient Norm after: 8.043525353920373
Epoch 4478/10000, Prediction Accuracy = 61.786%, Loss = 0.4443713426589966
Epoch: 4478, Batch Gradient Norm: 10.10142198948818
Epoch: 4478, Batch Gradient Norm after: 10.10142198948818
Epoch 4479/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4588066041469574
Epoch: 4479, Batch Gradient Norm: 11.034196960603182
Epoch: 4479, Batch Gradient Norm after: 11.034196960603182
Epoch 4480/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.46953739523887633
Epoch: 4480, Batch Gradient Norm: 10.77527169138184
Epoch: 4480, Batch Gradient Norm after: 10.77527169138184
Epoch 4481/10000, Prediction Accuracy = 61.798%, Loss = 0.46054755449295043
Epoch: 4481, Batch Gradient Norm: 11.076211361413076
Epoch: 4481, Batch Gradient Norm after: 11.076211361413076
Epoch 4482/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.4624080777168274
Epoch: 4482, Batch Gradient Norm: 10.934866317228433
Epoch: 4482, Batch Gradient Norm after: 10.934866317228433
Epoch 4483/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.46236749887466433
Epoch: 4483, Batch Gradient Norm: 10.720514883570502
Epoch: 4483, Batch Gradient Norm after: 10.720514883570502
Epoch 4484/10000, Prediction Accuracy = 61.536%, Loss = 0.46471274495124815
Epoch: 4484, Batch Gradient Norm: 8.109120044731435
Epoch: 4484, Batch Gradient Norm after: 8.109120044731435
Epoch 4485/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4475467920303345
Epoch: 4485, Batch Gradient Norm: 8.955354249225305
Epoch: 4485, Batch Gradient Norm after: 8.955354249225305
Epoch 4486/10000, Prediction Accuracy = 61.848%, Loss = 0.4489175021648407
Epoch: 4486, Batch Gradient Norm: 11.077441664245734
Epoch: 4486, Batch Gradient Norm after: 11.077441664245734
Epoch 4487/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.46320422887802126
Epoch: 4487, Batch Gradient Norm: 11.406617413176965
Epoch: 4487, Batch Gradient Norm after: 11.406617413176965
Epoch 4488/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.46733344793319703
Epoch: 4488, Batch Gradient Norm: 10.480883744415063
Epoch: 4488, Batch Gradient Norm after: 10.480883744415063
Epoch 4489/10000, Prediction Accuracy = 61.876%, Loss = 0.4571306765079498
Epoch: 4489, Batch Gradient Norm: 7.799136941895617
Epoch: 4489, Batch Gradient Norm after: 7.799136941895617
Epoch 4490/10000, Prediction Accuracy = 61.718%, Loss = 0.4423552453517914
Epoch: 4490, Batch Gradient Norm: 6.932539201222593
Epoch: 4490, Batch Gradient Norm after: 6.932539201222593
Epoch 4491/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.4376554250717163
Epoch: 4491, Batch Gradient Norm: 9.068192760498208
Epoch: 4491, Batch Gradient Norm after: 9.068192760498208
Epoch 4492/10000, Prediction Accuracy = 61.77%, Loss = 0.4478462696075439
Epoch: 4492, Batch Gradient Norm: 9.707899368346553
Epoch: 4492, Batch Gradient Norm after: 9.707899368346553
Epoch 4493/10000, Prediction Accuracy = 61.824%, Loss = 0.45251641273498533
Epoch: 4493, Batch Gradient Norm: 11.665739636004155
Epoch: 4493, Batch Gradient Norm after: 11.665739636004155
Epoch 4494/10000, Prediction Accuracy = 61.71600000000001%, Loss = 0.46947337985038756
Epoch: 4494, Batch Gradient Norm: 6.409747554807599
Epoch: 4494, Batch Gradient Norm after: 6.409747554807599
Epoch 4495/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.43706201314926146
Epoch: 4495, Batch Gradient Norm: 9.771022058349821
Epoch: 4495, Batch Gradient Norm after: 9.771022058349821
Epoch 4496/10000, Prediction Accuracy = 61.79%, Loss = 0.4519086480140686
Epoch: 4496, Batch Gradient Norm: 11.507025565822278
Epoch: 4496, Batch Gradient Norm after: 11.507025565822278
Epoch 4497/10000, Prediction Accuracy = 61.788%, Loss = 0.4652619123458862
Epoch: 4497, Batch Gradient Norm: 8.994117842415791
Epoch: 4497, Batch Gradient Norm after: 8.994117842415791
Epoch 4498/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.44986229538917544
Epoch: 4498, Batch Gradient Norm: 11.473366686193538
Epoch: 4498, Batch Gradient Norm after: 11.473366686193538
Epoch 4499/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.4667308211326599
Epoch: 4499, Batch Gradient Norm: 13.060189410245162
Epoch: 4499, Batch Gradient Norm after: 13.060189410245162
Epoch 4500/10000, Prediction Accuracy = 61.656000000000006%, Loss = 0.48154292702674867
Epoch: 4500, Batch Gradient Norm: 10.239058645939691
Epoch: 4500, Batch Gradient Norm after: 10.239058645939691
Epoch 4501/10000, Prediction Accuracy = 61.802%, Loss = 0.458237898349762
Epoch: 4501, Batch Gradient Norm: 9.495994238959511
Epoch: 4501, Batch Gradient Norm after: 9.495994238959511
Epoch 4502/10000, Prediction Accuracy = 61.734%, Loss = 0.4507167637348175
Epoch: 4502, Batch Gradient Norm: 9.403979712979284
Epoch: 4502, Batch Gradient Norm after: 9.403979712979284
Epoch 4503/10000, Prediction Accuracy = 61.944%, Loss = 0.45094709992408755
Epoch: 4503, Batch Gradient Norm: 7.951228939572779
Epoch: 4503, Batch Gradient Norm after: 7.951228939572779
Epoch 4504/10000, Prediction Accuracy = 61.77%, Loss = 0.4428637742996216
Epoch: 4504, Batch Gradient Norm: 10.02953571154014
Epoch: 4504, Batch Gradient Norm after: 10.02953571154014
Epoch 4505/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.45816791653633115
Epoch: 4505, Batch Gradient Norm: 6.8218304685339755
Epoch: 4505, Batch Gradient Norm after: 6.8218304685339755
Epoch 4506/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.43697919249534606
Epoch: 4506, Batch Gradient Norm: 8.161424871525744
Epoch: 4506, Batch Gradient Norm after: 8.161424871525744
Epoch 4507/10000, Prediction Accuracy = 61.864%, Loss = 0.4442590355873108
Epoch: 4507, Batch Gradient Norm: 9.309421011931688
Epoch: 4507, Batch Gradient Norm after: 9.309421011931688
Epoch 4508/10000, Prediction Accuracy = 61.814%, Loss = 0.45233734846115115
Epoch: 4508, Batch Gradient Norm: 11.829583721353249
Epoch: 4508, Batch Gradient Norm after: 11.829583721353249
Epoch 4509/10000, Prediction Accuracy = 61.73%, Loss = 0.469227546453476
Epoch: 4509, Batch Gradient Norm: 11.159042668346713
Epoch: 4509, Batch Gradient Norm after: 11.159042668346713
Epoch 4510/10000, Prediction Accuracy = 61.782000000000004%, Loss = 0.464576256275177
Epoch: 4510, Batch Gradient Norm: 12.128730983937425
Epoch: 4510, Batch Gradient Norm after: 12.128730983937425
Epoch 4511/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.4769384264945984
Epoch: 4511, Batch Gradient Norm: 7.655433495584021
Epoch: 4511, Batch Gradient Norm after: 7.655433495584021
Epoch 4512/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.44171316623687745
Epoch: 4512, Batch Gradient Norm: 6.279591823738833
Epoch: 4512, Batch Gradient Norm after: 6.279591823738833
Epoch 4513/10000, Prediction Accuracy = 61.972%, Loss = 0.4339758038520813
Epoch: 4513, Batch Gradient Norm: 10.037343294140214
Epoch: 4513, Batch Gradient Norm after: 10.037343294140214
Epoch 4514/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4593330264091492
Epoch: 4514, Batch Gradient Norm: 8.100981070117452
Epoch: 4514, Batch Gradient Norm after: 8.100981070117452
Epoch 4515/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4458641052246094
Epoch: 4515, Batch Gradient Norm: 10.129657747481735
Epoch: 4515, Batch Gradient Norm after: 10.129657747481735
Epoch 4516/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.4575619101524353
Epoch: 4516, Batch Gradient Norm: 12.390260043433846
Epoch: 4516, Batch Gradient Norm after: 12.390260043433846
Epoch 4517/10000, Prediction Accuracy = 61.57800000000001%, Loss = 0.4754411280155182
Epoch: 4517, Batch Gradient Norm: 9.734679626990939
Epoch: 4517, Batch Gradient Norm after: 9.734679626990939
Epoch 4518/10000, Prediction Accuracy = 61.812%, Loss = 0.4574665367603302
Epoch: 4518, Batch Gradient Norm: 8.796170207676145
Epoch: 4518, Batch Gradient Norm after: 8.796170207676145
Epoch 4519/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4457017660140991
Epoch: 4519, Batch Gradient Norm: 11.615133198361304
Epoch: 4519, Batch Gradient Norm after: 11.615133198361304
Epoch 4520/10000, Prediction Accuracy = 61.83599999999999%, Loss = 0.4687726438045502
Epoch: 4520, Batch Gradient Norm: 10.583611562548903
Epoch: 4520, Batch Gradient Norm after: 10.583611562548903
Epoch 4521/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4576193392276764
Epoch: 4521, Batch Gradient Norm: 12.556155511484022
Epoch: 4521, Batch Gradient Norm after: 12.556155511484022
Epoch 4522/10000, Prediction Accuracy = 61.726%, Loss = 0.47555274367332456
Epoch: 4522, Batch Gradient Norm: 9.843705193564741
Epoch: 4522, Batch Gradient Norm after: 9.843705193564741
Epoch 4523/10000, Prediction Accuracy = 61.85799999999999%, Loss = 0.456013947725296
Epoch: 4523, Batch Gradient Norm: 10.327294746132825
Epoch: 4523, Batch Gradient Norm after: 10.327294746132825
Epoch 4524/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4612008571624756
Epoch: 4524, Batch Gradient Norm: 8.854702472896117
Epoch: 4524, Batch Gradient Norm after: 8.854702472896117
Epoch 4525/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4517803847789764
Epoch: 4525, Batch Gradient Norm: 10.237435364554793
Epoch: 4525, Batch Gradient Norm after: 10.237435364554793
Epoch 4526/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4605965256690979
Epoch: 4526, Batch Gradient Norm: 10.897985697328192
Epoch: 4526, Batch Gradient Norm after: 10.897985697328192
Epoch 4527/10000, Prediction Accuracy = 61.864%, Loss = 0.46023266911506655
Epoch: 4527, Batch Gradient Norm: 7.624165574797402
Epoch: 4527, Batch Gradient Norm after: 7.624165574797402
Epoch 4528/10000, Prediction Accuracy = 61.81%, Loss = 0.4401009500026703
Epoch: 4528, Batch Gradient Norm: 9.62318493692833
Epoch: 4528, Batch Gradient Norm after: 9.62318493692833
Epoch 4529/10000, Prediction Accuracy = 61.598%, Loss = 0.45716137886047364
Epoch: 4529, Batch Gradient Norm: 9.350878130269937
Epoch: 4529, Batch Gradient Norm after: 9.350878130269937
Epoch 4530/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.45050306916236876
Epoch: 4530, Batch Gradient Norm: 10.626259122928172
Epoch: 4530, Batch Gradient Norm after: 10.626259122928172
Epoch 4531/10000, Prediction Accuracy = 61.802%, Loss = 0.45950071811676024
Epoch: 4531, Batch Gradient Norm: 12.453637673258752
Epoch: 4531, Batch Gradient Norm after: 12.453637673258752
Epoch 4532/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.47348231077194214
Epoch: 4532, Batch Gradient Norm: 9.540216495094441
Epoch: 4532, Batch Gradient Norm after: 9.540216495094441
Epoch 4533/10000, Prediction Accuracy = 61.85%, Loss = 0.45295838713645936
Epoch: 4533, Batch Gradient Norm: 7.588357213174735
Epoch: 4533, Batch Gradient Norm after: 7.588357213174735
Epoch 4534/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.4404953598976135
Epoch: 4534, Batch Gradient Norm: 12.979077130426422
Epoch: 4534, Batch Gradient Norm after: 12.979077130426422
Epoch 4535/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.47672889232635496
Epoch: 4535, Batch Gradient Norm: 14.325509573008638
Epoch: 4535, Batch Gradient Norm after: 14.325509573008638
Epoch 4536/10000, Prediction Accuracy = 61.824%, Loss = 0.49245412945747374
Epoch: 4536, Batch Gradient Norm: 9.270531355393118
Epoch: 4536, Batch Gradient Norm after: 9.270531355393118
Epoch 4537/10000, Prediction Accuracy = 61.86%, Loss = 0.451416552066803
Epoch: 4537, Batch Gradient Norm: 8.117598844597596
Epoch: 4537, Batch Gradient Norm after: 8.117598844597596
Epoch 4538/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.44280014634132386
Epoch: 4538, Batch Gradient Norm: 7.358201885668133
Epoch: 4538, Batch Gradient Norm after: 7.358201885668133
Epoch 4539/10000, Prediction Accuracy = 61.836%, Loss = 0.44003707766532896
Epoch: 4539, Batch Gradient Norm: 8.373002262583485
Epoch: 4539, Batch Gradient Norm after: 8.373002262583485
Epoch 4540/10000, Prediction Accuracy = 61.84599999999999%, Loss = 0.4459455728530884
Epoch: 4540, Batch Gradient Norm: 10.428285715035258
Epoch: 4540, Batch Gradient Norm after: 10.428285715035258
Epoch 4541/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4607948839664459
Epoch: 4541, Batch Gradient Norm: 10.08146800576009
Epoch: 4541, Batch Gradient Norm after: 10.08146800576009
Epoch 4542/10000, Prediction Accuracy = 61.762%, Loss = 0.45528057813644407
Epoch: 4542, Batch Gradient Norm: 10.009793572336765
Epoch: 4542, Batch Gradient Norm after: 10.009793572336765
Epoch 4543/10000, Prediction Accuracy = 61.826%, Loss = 0.4566386938095093
Epoch: 4543, Batch Gradient Norm: 10.581027464432639
Epoch: 4543, Batch Gradient Norm after: 10.581027464432639
Epoch 4544/10000, Prediction Accuracy = 61.65%, Loss = 0.4611789584159851
Epoch: 4544, Batch Gradient Norm: 11.531858980426454
Epoch: 4544, Batch Gradient Norm after: 11.531858980426454
Epoch 4545/10000, Prediction Accuracy = 61.86%, Loss = 0.4696096062660217
Epoch: 4545, Batch Gradient Norm: 10.42145235498283
Epoch: 4545, Batch Gradient Norm after: 10.42145235498283
Epoch 4546/10000, Prediction Accuracy = 61.77%, Loss = 0.46277052760124204
Epoch: 4546, Batch Gradient Norm: 8.994263196903324
Epoch: 4546, Batch Gradient Norm after: 8.994263196903324
Epoch 4547/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.4532084703445435
Epoch: 4547, Batch Gradient Norm: 8.23839295251438
Epoch: 4547, Batch Gradient Norm after: 8.23839295251438
Epoch 4548/10000, Prediction Accuracy = 61.806%, Loss = 0.44450982213020324
Epoch: 4548, Batch Gradient Norm: 10.622250390474077
Epoch: 4548, Batch Gradient Norm after: 10.622250390474077
Epoch 4549/10000, Prediction Accuracy = 61.786%, Loss = 0.4589852035045624
Epoch: 4549, Batch Gradient Norm: 10.820283728143044
Epoch: 4549, Batch Gradient Norm after: 10.820283728143044
Epoch 4550/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.4620280086994171
Epoch: 4550, Batch Gradient Norm: 11.85341244817282
Epoch: 4550, Batch Gradient Norm after: 11.85341244817282
Epoch 4551/10000, Prediction Accuracy = 61.754000000000005%, Loss = 0.4691671907901764
Epoch: 4551, Batch Gradient Norm: 8.17354284153191
Epoch: 4551, Batch Gradient Norm after: 8.17354284153191
Epoch 4552/10000, Prediction Accuracy = 61.838%, Loss = 0.4431028008460999
Epoch: 4552, Batch Gradient Norm: 7.08434820085055
Epoch: 4552, Batch Gradient Norm after: 7.08434820085055
Epoch 4553/10000, Prediction Accuracy = 61.902%, Loss = 0.43542435169219973
Epoch: 4553, Batch Gradient Norm: 11.68361161044681
Epoch: 4553, Batch Gradient Norm after: 11.68361161044681
Epoch 4554/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.46645957231521606
Epoch: 4554, Batch Gradient Norm: 11.861211052692187
Epoch: 4554, Batch Gradient Norm after: 11.861211052692187
Epoch 4555/10000, Prediction Accuracy = 61.85999999999999%, Loss = 0.46860045194625854
Epoch: 4555, Batch Gradient Norm: 10.917078690069186
Epoch: 4555, Batch Gradient Norm after: 10.917078690069186
Epoch 4556/10000, Prediction Accuracy = 61.83%, Loss = 0.4653129458427429
Epoch: 4556, Batch Gradient Norm: 8.909087272891869
Epoch: 4556, Batch Gradient Norm after: 8.909087272891869
Epoch 4557/10000, Prediction Accuracy = 61.628%, Loss = 0.4512126505374908
Epoch: 4557, Batch Gradient Norm: 6.9948525724064705
Epoch: 4557, Batch Gradient Norm after: 6.9948525724064705
Epoch 4558/10000, Prediction Accuracy = 61.88000000000001%, Loss = 0.437014102935791
Epoch: 4558, Batch Gradient Norm: 8.905771344912084
Epoch: 4558, Batch Gradient Norm after: 8.905771344912084
Epoch 4559/10000, Prediction Accuracy = 61.888%, Loss = 0.446722012758255
Epoch: 4559, Batch Gradient Norm: 11.436156192099185
Epoch: 4559, Batch Gradient Norm after: 11.436156192099185
Epoch 4560/10000, Prediction Accuracy = 61.778000000000006%, Loss = 0.46375150680541993
Epoch: 4560, Batch Gradient Norm: 9.827845400246828
Epoch: 4560, Batch Gradient Norm after: 9.827845400246828
Epoch 4561/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.45419800877571104
Epoch: 4561, Batch Gradient Norm: 8.025927694281089
Epoch: 4561, Batch Gradient Norm after: 8.025927694281089
Epoch 4562/10000, Prediction Accuracy = 61.92%, Loss = 0.4429973542690277
Epoch: 4562, Batch Gradient Norm: 10.593965052247007
Epoch: 4562, Batch Gradient Norm after: 10.593965052247007
Epoch 4563/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4560540378093719
Epoch: 4563, Batch Gradient Norm: 9.313164777033812
Epoch: 4563, Batch Gradient Norm after: 9.313164777033812
Epoch 4564/10000, Prediction Accuracy = 61.826%, Loss = 0.4470970928668976
Epoch: 4564, Batch Gradient Norm: 7.375446034708696
Epoch: 4564, Batch Gradient Norm after: 7.375446034708696
Epoch 4565/10000, Prediction Accuracy = 61.86999999999999%, Loss = 0.4391347527503967
Epoch: 4565, Batch Gradient Norm: 8.487534589942026
Epoch: 4565, Batch Gradient Norm after: 8.487534589942026
Epoch 4566/10000, Prediction Accuracy = 61.734%, Loss = 0.44305824041366576
Epoch: 4566, Batch Gradient Norm: 11.887131462436603
Epoch: 4566, Batch Gradient Norm after: 11.887131462436603
Epoch 4567/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4685374557971954
Epoch: 4567, Batch Gradient Norm: 12.019982903406751
Epoch: 4567, Batch Gradient Norm after: 12.019982903406751
Epoch 4568/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.4729557275772095
Epoch: 4568, Batch Gradient Norm: 13.095100383973913
Epoch: 4568, Batch Gradient Norm after: 13.095100383973913
Epoch 4569/10000, Prediction Accuracy = 61.852%, Loss = 0.47810774445533755
Epoch: 4569, Batch Gradient Norm: 9.801797983069847
Epoch: 4569, Batch Gradient Norm after: 9.801797983069847
Epoch 4570/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.45219417810440066
Epoch: 4570, Batch Gradient Norm: 8.252648793831698
Epoch: 4570, Batch Gradient Norm after: 8.252648793831698
Epoch 4571/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.44278534054756163
Epoch: 4571, Batch Gradient Norm: 9.625660810749752
Epoch: 4571, Batch Gradient Norm after: 9.625660810749752
Epoch 4572/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.44997900128364565
Epoch: 4572, Batch Gradient Norm: 12.282024535243869
Epoch: 4572, Batch Gradient Norm after: 12.282024535243869
Epoch 4573/10000, Prediction Accuracy = 61.745999999999995%, Loss = 0.47360324263572695
Epoch: 4573, Batch Gradient Norm: 7.475209464320324
Epoch: 4573, Batch Gradient Norm after: 7.475209464320324
Epoch 4574/10000, Prediction Accuracy = 61.806%, Loss = 0.4400860071182251
Epoch: 4574, Batch Gradient Norm: 12.01916011143342
Epoch: 4574, Batch Gradient Norm after: 12.01916011143342
Epoch 4575/10000, Prediction Accuracy = 61.746%, Loss = 0.4686487913131714
Epoch: 4575, Batch Gradient Norm: 10.597972244871247
Epoch: 4575, Batch Gradient Norm after: 10.597972244871247
Epoch 4576/10000, Prediction Accuracy = 61.79%, Loss = 0.45880612134933474
Epoch: 4576, Batch Gradient Norm: 8.161707187267066
Epoch: 4576, Batch Gradient Norm after: 8.161707187267066
Epoch 4577/10000, Prediction Accuracy = 61.902%, Loss = 0.442287814617157
Epoch: 4577, Batch Gradient Norm: 7.969460859275495
Epoch: 4577, Batch Gradient Norm after: 7.969460859275495
Epoch 4578/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4427875816822052
Epoch: 4578, Batch Gradient Norm: 8.63830405151774
Epoch: 4578, Batch Gradient Norm after: 8.63830405151774
Epoch 4579/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.44807674288749694
Epoch: 4579, Batch Gradient Norm: 10.084857051757979
Epoch: 4579, Batch Gradient Norm after: 10.084857051757979
Epoch 4580/10000, Prediction Accuracy = 61.73599999999999%, Loss = 0.454414165019989
Epoch: 4580, Batch Gradient Norm: 10.186476928157953
Epoch: 4580, Batch Gradient Norm after: 10.186476928157953
Epoch 4581/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.45421348214149476
Epoch: 4581, Batch Gradient Norm: 8.853062878335079
Epoch: 4581, Batch Gradient Norm after: 8.853062878335079
Epoch 4582/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.44582715034484866
Epoch: 4582, Batch Gradient Norm: 10.572854718860665
Epoch: 4582, Batch Gradient Norm after: 10.572854718860665
Epoch 4583/10000, Prediction Accuracy = 61.774%, Loss = 0.45870524644851685
Epoch: 4583, Batch Gradient Norm: 9.052992506958459
Epoch: 4583, Batch Gradient Norm after: 9.052992506958459
Epoch 4584/10000, Prediction Accuracy = 61.836%, Loss = 0.4494697332382202
Epoch: 4584, Batch Gradient Norm: 6.034383833160416
Epoch: 4584, Batch Gradient Norm after: 6.034383833160416
Epoch 4585/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.43147939443588257
Epoch: 4585, Batch Gradient Norm: 9.352725091354074
Epoch: 4585, Batch Gradient Norm after: 9.352725091354074
Epoch 4586/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.448896187543869
Epoch: 4586, Batch Gradient Norm: 14.050279210185897
Epoch: 4586, Batch Gradient Norm after: 14.050279210185897
Epoch 4587/10000, Prediction Accuracy = 61.77600000000001%, Loss = 0.4914409339427948
Epoch: 4587, Batch Gradient Norm: 10.869477998932204
Epoch: 4587, Batch Gradient Norm after: 10.869477998932204
Epoch 4588/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.46127541065216066
Epoch: 4588, Batch Gradient Norm: 9.781047432939053
Epoch: 4588, Batch Gradient Norm after: 9.781047432939053
Epoch 4589/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.45265817642211914
Epoch: 4589, Batch Gradient Norm: 10.49441078610699
Epoch: 4589, Batch Gradient Norm after: 10.49441078610699
Epoch 4590/10000, Prediction Accuracy = 61.848%, Loss = 0.45838738679885865
Epoch: 4590, Batch Gradient Norm: 7.049207873913683
Epoch: 4590, Batch Gradient Norm after: 7.049207873913683
Epoch 4591/10000, Prediction Accuracy = 61.836%, Loss = 0.4360684394836426
Epoch: 4591, Batch Gradient Norm: 8.264800017865674
Epoch: 4591, Batch Gradient Norm after: 8.264800017865674
Epoch 4592/10000, Prediction Accuracy = 61.89%, Loss = 0.4429139792919159
Epoch: 4592, Batch Gradient Norm: 7.987996809905601
Epoch: 4592, Batch Gradient Norm after: 7.987996809905601
Epoch 4593/10000, Prediction Accuracy = 61.912%, Loss = 0.438908189535141
Epoch: 4593, Batch Gradient Norm: 10.912819438127766
Epoch: 4593, Batch Gradient Norm after: 10.912819438127766
Epoch 4594/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.46164419054985045
Epoch: 4594, Batch Gradient Norm: 10.771024570196472
Epoch: 4594, Batch Gradient Norm after: 10.771024570196472
Epoch 4595/10000, Prediction Accuracy = 61.788%, Loss = 0.45894878506660464
Epoch: 4595, Batch Gradient Norm: 12.766582911207896
Epoch: 4595, Batch Gradient Norm after: 12.766582911207896
Epoch 4596/10000, Prediction Accuracy = 61.766%, Loss = 0.47941285371780396
Epoch: 4596, Batch Gradient Norm: 10.326623371407644
Epoch: 4596, Batch Gradient Norm after: 10.326623371407644
Epoch 4597/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.45709521174430845
Epoch: 4597, Batch Gradient Norm: 8.048846021966321
Epoch: 4597, Batch Gradient Norm after: 8.048846021966321
Epoch 4598/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.44292070269584655
Epoch: 4598, Batch Gradient Norm: 8.351450664694887
Epoch: 4598, Batch Gradient Norm after: 8.351450664694887
Epoch 4599/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4438736081123352
Epoch: 4599, Batch Gradient Norm: 9.034103053731751
Epoch: 4599, Batch Gradient Norm after: 9.034103053731751
Epoch 4600/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.44812321066856386
Epoch: 4600, Batch Gradient Norm: 10.984414446693924
Epoch: 4600, Batch Gradient Norm after: 10.984414446693924
Epoch 4601/10000, Prediction Accuracy = 61.75999999999999%, Loss = 0.462845116853714
Epoch: 4601, Batch Gradient Norm: 12.287692819768305
Epoch: 4601, Batch Gradient Norm after: 12.287692819768305
Epoch 4602/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.4707806587219238
Epoch: 4602, Batch Gradient Norm: 10.815103692839902
Epoch: 4602, Batch Gradient Norm after: 10.815103692839902
Epoch 4603/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.4599828541278839
Epoch: 4603, Batch Gradient Norm: 9.947610020033116
Epoch: 4603, Batch Gradient Norm after: 9.947610020033116
Epoch 4604/10000, Prediction Accuracy = 61.944%, Loss = 0.4522103428840637
Epoch: 4604, Batch Gradient Norm: 8.337220292094104
Epoch: 4604, Batch Gradient Norm after: 8.337220292094104
Epoch 4605/10000, Prediction Accuracy = 61.818%, Loss = 0.4447470247745514
Epoch: 4605, Batch Gradient Norm: 8.541830472626849
Epoch: 4605, Batch Gradient Norm after: 8.541830472626849
Epoch 4606/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.44292241930961607
Epoch: 4606, Batch Gradient Norm: 12.141627930474682
Epoch: 4606, Batch Gradient Norm after: 12.141627930474682
Epoch 4607/10000, Prediction Accuracy = 61.842%, Loss = 0.4742908298969269
Epoch: 4607, Batch Gradient Norm: 8.022255338118827
Epoch: 4607, Batch Gradient Norm after: 8.022255338118827
Epoch 4608/10000, Prediction Accuracy = 61.83%, Loss = 0.44234333634376527
Epoch: 4608, Batch Gradient Norm: 10.795142941312752
Epoch: 4608, Batch Gradient Norm after: 10.795142941312752
Epoch 4609/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.46523130536079405
Epoch: 4609, Batch Gradient Norm: 6.440741679350178
Epoch: 4609, Batch Gradient Norm after: 6.440741679350178
Epoch 4610/10000, Prediction Accuracy = 61.83%, Loss = 0.43282469511032107
Epoch: 4610, Batch Gradient Norm: 10.20719991934694
Epoch: 4610, Batch Gradient Norm after: 10.20719991934694
Epoch 4611/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4562303960323334
Epoch: 4611, Batch Gradient Norm: 10.962772196464213
Epoch: 4611, Batch Gradient Norm after: 10.962772196464213
Epoch 4612/10000, Prediction Accuracy = 61.866%, Loss = 0.4610264480113983
Epoch: 4612, Batch Gradient Norm: 10.881633174083943
Epoch: 4612, Batch Gradient Norm after: 10.881633174083943
Epoch 4613/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.45911234617233276
Epoch: 4613, Batch Gradient Norm: 9.889463099037814
Epoch: 4613, Batch Gradient Norm after: 9.889463099037814
Epoch 4614/10000, Prediction Accuracy = 61.778%, Loss = 0.45247651934623717
Epoch: 4614, Batch Gradient Norm: 12.543016931600736
Epoch: 4614, Batch Gradient Norm after: 12.543016931600736
Epoch 4615/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.4734960973262787
Epoch: 4615, Batch Gradient Norm: 10.223833401074613
Epoch: 4615, Batch Gradient Norm after: 10.223833401074613
Epoch 4616/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.45458799600601196
Epoch: 4616, Batch Gradient Norm: 9.708307287121482
Epoch: 4616, Batch Gradient Norm after: 9.708307287121482
Epoch 4617/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4495589077472687
Epoch: 4617, Batch Gradient Norm: 10.980631866057646
Epoch: 4617, Batch Gradient Norm after: 10.980631866057646
Epoch 4618/10000, Prediction Accuracy = 61.83%, Loss = 0.4573439955711365
Epoch: 4618, Batch Gradient Norm: 8.669238610386687
Epoch: 4618, Batch Gradient Norm after: 8.669238610386687
Epoch 4619/10000, Prediction Accuracy = 61.838%, Loss = 0.4464709281921387
Epoch: 4619, Batch Gradient Norm: 8.058940805856501
Epoch: 4619, Batch Gradient Norm after: 8.058940805856501
Epoch 4620/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.43921241760253904
Epoch: 4620, Batch Gradient Norm: 8.559708181212
Epoch: 4620, Batch Gradient Norm after: 8.559708181212
Epoch 4621/10000, Prediction Accuracy = 61.916%, Loss = 0.4450370728969574
Epoch: 4621, Batch Gradient Norm: 8.75610708739049
Epoch: 4621, Batch Gradient Norm after: 8.75610708739049
Epoch 4622/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.44348224401474
Epoch: 4622, Batch Gradient Norm: 9.051598043483647
Epoch: 4622, Batch Gradient Norm after: 9.051598043483647
Epoch 4623/10000, Prediction Accuracy = 61.848%, Loss = 0.44590160846710203
Epoch: 4623, Batch Gradient Norm: 10.83201344381934
Epoch: 4623, Batch Gradient Norm after: 10.83201344381934
Epoch 4624/10000, Prediction Accuracy = 61.8%, Loss = 0.4549229621887207
Epoch: 4624, Batch Gradient Norm: 13.819599217107578
Epoch: 4624, Batch Gradient Norm after: 13.819599217107578
Epoch 4625/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.4823785960674286
Epoch: 4625, Batch Gradient Norm: 12.346440527506122
Epoch: 4625, Batch Gradient Norm after: 12.346440527506122
Epoch 4626/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.47950899600982666
Epoch: 4626, Batch Gradient Norm: 7.265751524087715
Epoch: 4626, Batch Gradient Norm after: 7.265751524087715
Epoch 4627/10000, Prediction Accuracy = 61.734%, Loss = 0.43744189143180845
Epoch: 4627, Batch Gradient Norm: 6.89194696233016
Epoch: 4627, Batch Gradient Norm after: 6.89194696233016
Epoch 4628/10000, Prediction Accuracy = 61.812%, Loss = 0.4352306663990021
Epoch: 4628, Batch Gradient Norm: 9.016736318169906
Epoch: 4628, Batch Gradient Norm after: 9.016736318169906
Epoch 4629/10000, Prediction Accuracy = 61.86%, Loss = 0.44572754502296447
Epoch: 4629, Batch Gradient Norm: 12.24734659289417
Epoch: 4629, Batch Gradient Norm after: 12.24734659289417
Epoch 4630/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4688414752483368
Epoch: 4630, Batch Gradient Norm: 12.090665079888106
Epoch: 4630, Batch Gradient Norm after: 12.090665079888106
Epoch 4631/10000, Prediction Accuracy = 61.742%, Loss = 0.472676545381546
Epoch: 4631, Batch Gradient Norm: 8.360699544899068
Epoch: 4631, Batch Gradient Norm after: 8.360699544899068
Epoch 4632/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4438580334186554
Epoch: 4632, Batch Gradient Norm: 9.09804093238744
Epoch: 4632, Batch Gradient Norm after: 9.09804093238744
Epoch 4633/10000, Prediction Accuracy = 61.914%, Loss = 0.446434885263443
Epoch: 4633, Batch Gradient Norm: 9.82419311502811
Epoch: 4633, Batch Gradient Norm after: 9.82419311502811
Epoch 4634/10000, Prediction Accuracy = 61.93399999999999%, Loss = 0.45345284342765807
Epoch: 4634, Batch Gradient Norm: 12.237887830825867
Epoch: 4634, Batch Gradient Norm after: 12.237887830825867
Epoch 4635/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.477070277929306
Epoch: 4635, Batch Gradient Norm: 8.741783692007036
Epoch: 4635, Batch Gradient Norm after: 8.741783692007036
Epoch 4636/10000, Prediction Accuracy = 61.812%, Loss = 0.4463685154914856
Epoch: 4636, Batch Gradient Norm: 9.183559789954373
Epoch: 4636, Batch Gradient Norm after: 9.183559789954373
Epoch 4637/10000, Prediction Accuracy = 61.802%, Loss = 0.44925443530082704
Epoch: 4637, Batch Gradient Norm: 9.221559359154028
Epoch: 4637, Batch Gradient Norm after: 9.221559359154028
Epoch 4638/10000, Prediction Accuracy = 61.778%, Loss = 0.44977492094039917
Epoch: 4638, Batch Gradient Norm: 8.229689410998365
Epoch: 4638, Batch Gradient Norm after: 8.229689410998365
Epoch 4639/10000, Prediction Accuracy = 61.94%, Loss = 0.4409002661705017
Epoch: 4639, Batch Gradient Norm: 11.764504265800111
Epoch: 4639, Batch Gradient Norm after: 11.764504265800111
Epoch 4640/10000, Prediction Accuracy = 61.80400000000001%, Loss = 0.46109238266944885
Epoch: 4640, Batch Gradient Norm: 12.346888884026804
Epoch: 4640, Batch Gradient Norm after: 12.346888884026804
Epoch 4641/10000, Prediction Accuracy = 61.826%, Loss = 0.4774193108081818
Epoch: 4641, Batch Gradient Norm: 7.430082007197505
Epoch: 4641, Batch Gradient Norm after: 7.430082007197505
Epoch 4642/10000, Prediction Accuracy = 61.806%, Loss = 0.4363990783691406
Epoch: 4642, Batch Gradient Norm: 8.086394773382407
Epoch: 4642, Batch Gradient Norm after: 8.086394773382407
Epoch 4643/10000, Prediction Accuracy = 61.852%, Loss = 0.43857645988464355
Epoch: 4643, Batch Gradient Norm: 10.651225975836429
Epoch: 4643, Batch Gradient Norm after: 10.651225975836429
Epoch 4644/10000, Prediction Accuracy = 61.798%, Loss = 0.4589292466640472
Epoch: 4644, Batch Gradient Norm: 11.521071063844781
Epoch: 4644, Batch Gradient Norm after: 11.521071063844781
Epoch 4645/10000, Prediction Accuracy = 61.76400000000001%, Loss = 0.4708546161651611
Epoch: 4645, Batch Gradient Norm: 8.200029252726555
Epoch: 4645, Batch Gradient Norm after: 8.200029252726555
Epoch 4646/10000, Prediction Accuracy = 61.834%, Loss = 0.4450477361679077
Epoch: 4646, Batch Gradient Norm: 7.329155586744374
Epoch: 4646, Batch Gradient Norm after: 7.329155586744374
Epoch 4647/10000, Prediction Accuracy = 61.876%, Loss = 0.43525790572166445
Epoch: 4647, Batch Gradient Norm: 9.24241346965713
Epoch: 4647, Batch Gradient Norm after: 9.24241346965713
Epoch 4648/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4452019095420837
Epoch: 4648, Batch Gradient Norm: 9.155395119081103
Epoch: 4648, Batch Gradient Norm after: 9.155395119081103
Epoch 4649/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.444513612985611
Epoch: 4649, Batch Gradient Norm: 10.550867472135277
Epoch: 4649, Batch Gradient Norm after: 10.550867472135277
Epoch 4650/10000, Prediction Accuracy = 61.81400000000001%, Loss = 0.4555407166481018
Epoch: 4650, Batch Gradient Norm: 11.254850414453362
Epoch: 4650, Batch Gradient Norm after: 11.254850414453362
Epoch 4651/10000, Prediction Accuracy = 61.89200000000001%, Loss = 0.45856425166130066
Epoch: 4651, Batch Gradient Norm: 11.359088306483416
Epoch: 4651, Batch Gradient Norm after: 11.359088306483416
Epoch 4652/10000, Prediction Accuracy = 61.81%, Loss = 0.4597795844078064
Epoch: 4652, Batch Gradient Norm: 11.018128810417613
Epoch: 4652, Batch Gradient Norm after: 11.018128810417613
Epoch 4653/10000, Prediction Accuracy = 61.864%, Loss = 0.4601108729839325
Epoch: 4653, Batch Gradient Norm: 8.619134546797515
Epoch: 4653, Batch Gradient Norm after: 8.619134546797515
Epoch 4654/10000, Prediction Accuracy = 61.912%, Loss = 0.44475640058517457
Epoch: 4654, Batch Gradient Norm: 10.507649675723611
Epoch: 4654, Batch Gradient Norm after: 10.507649675723611
Epoch 4655/10000, Prediction Accuracy = 61.852%, Loss = 0.45357089042663573
Epoch: 4655, Batch Gradient Norm: 11.468696281759666
Epoch: 4655, Batch Gradient Norm after: 11.468696281759666
Epoch 4656/10000, Prediction Accuracy = 61.806%, Loss = 0.4609930098056793
Epoch: 4656, Batch Gradient Norm: 8.666462389143271
Epoch: 4656, Batch Gradient Norm after: 8.666462389143271
Epoch 4657/10000, Prediction Accuracy = 61.85600000000001%, Loss = 0.4442250669002533
Epoch: 4657, Batch Gradient Norm: 8.27839517280456
Epoch: 4657, Batch Gradient Norm after: 8.27839517280456
Epoch 4658/10000, Prediction Accuracy = 61.8%, Loss = 0.4410643517971039
Epoch: 4658, Batch Gradient Norm: 9.380062711809494
Epoch: 4658, Batch Gradient Norm after: 9.380062711809494
Epoch 4659/10000, Prediction Accuracy = 61.864%, Loss = 0.447988486289978
Epoch: 4659, Batch Gradient Norm: 6.9806071724513
Epoch: 4659, Batch Gradient Norm after: 6.9806071724513
Epoch 4660/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4337916910648346
Epoch: 4660, Batch Gradient Norm: 11.074755148192747
Epoch: 4660, Batch Gradient Norm after: 11.074755148192747
Epoch 4661/10000, Prediction Accuracy = 61.782000000000004%, Loss = 0.464274525642395
Epoch: 4661, Batch Gradient Norm: 13.22112993484288
Epoch: 4661, Batch Gradient Norm after: 13.22112993484288
Epoch 4662/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4784451425075531
Epoch: 4662, Batch Gradient Norm: 11.350698167128709
Epoch: 4662, Batch Gradient Norm after: 11.350698167128709
Epoch 4663/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.46253193616867067
Epoch: 4663, Batch Gradient Norm: 7.756872966862179
Epoch: 4663, Batch Gradient Norm after: 7.756872966862179
Epoch 4664/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.43793673515319825
Epoch: 4664, Batch Gradient Norm: 8.196283387504769
Epoch: 4664, Batch Gradient Norm after: 8.196283387504769
Epoch 4665/10000, Prediction Accuracy = 61.81%, Loss = 0.4413051426410675
Epoch: 4665, Batch Gradient Norm: 10.652666788911493
Epoch: 4665, Batch Gradient Norm after: 10.652666788911493
Epoch 4666/10000, Prediction Accuracy = 61.96%, Loss = 0.4595590651035309
Epoch: 4666, Batch Gradient Norm: 10.773297223294888
Epoch: 4666, Batch Gradient Norm after: 10.773297223294888
Epoch 4667/10000, Prediction Accuracy = 61.85600000000001%, Loss = 0.4587745189666748
Epoch: 4667, Batch Gradient Norm: 9.814629616733088
Epoch: 4667, Batch Gradient Norm after: 9.814629616733088
Epoch 4668/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.4514486491680145
Epoch: 4668, Batch Gradient Norm: 9.702688875825467
Epoch: 4668, Batch Gradient Norm after: 9.702688875825467
Epoch 4669/10000, Prediction Accuracy = 61.854%, Loss = 0.45002651810646055
Epoch: 4669, Batch Gradient Norm: 11.740423239084976
Epoch: 4669, Batch Gradient Norm after: 11.740423239084976
Epoch 4670/10000, Prediction Accuracy = 61.89000000000001%, Loss = 0.46646141409873965
Epoch: 4670, Batch Gradient Norm: 8.67587942685677
Epoch: 4670, Batch Gradient Norm after: 8.67587942685677
Epoch 4671/10000, Prediction Accuracy = 61.878%, Loss = 0.44146804213523866
Epoch: 4671, Batch Gradient Norm: 8.756946643859282
Epoch: 4671, Batch Gradient Norm after: 8.756946643859282
Epoch 4672/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4434361279010773
Epoch: 4672, Batch Gradient Norm: 11.405381183345318
Epoch: 4672, Batch Gradient Norm after: 11.405381183345318
Epoch 4673/10000, Prediction Accuracy = 61.848%, Loss = 0.46397368907928466
Epoch: 4673, Batch Gradient Norm: 10.458528158577602
Epoch: 4673, Batch Gradient Norm after: 10.458528158577602
Epoch 4674/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4557421565055847
Epoch: 4674, Batch Gradient Norm: 10.292946735568956
Epoch: 4674, Batch Gradient Norm after: 10.292946735568956
Epoch 4675/10000, Prediction Accuracy = 61.734%, Loss = 0.45770018696784975
Epoch: 4675, Batch Gradient Norm: 10.341281114404111
Epoch: 4675, Batch Gradient Norm after: 10.341281114404111
Epoch 4676/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4541514039039612
Epoch: 4676, Batch Gradient Norm: 9.309159927144147
Epoch: 4676, Batch Gradient Norm after: 9.309159927144147
Epoch 4677/10000, Prediction Accuracy = 61.88800000000001%, Loss = 0.44849061965942383
Epoch: 4677, Batch Gradient Norm: 9.09023513292714
Epoch: 4677, Batch Gradient Norm after: 9.09023513292714
Epoch 4678/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.4456151247024536
Epoch: 4678, Batch Gradient Norm: 11.222517830179221
Epoch: 4678, Batch Gradient Norm after: 11.222517830179221
Epoch 4679/10000, Prediction Accuracy = 61.89200000000001%, Loss = 0.46006842255592345
Epoch: 4679, Batch Gradient Norm: 9.303955585746971
Epoch: 4679, Batch Gradient Norm after: 9.303955585746971
Epoch 4680/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4459097146987915
Epoch: 4680, Batch Gradient Norm: 6.689922821928075
Epoch: 4680, Batch Gradient Norm after: 6.689922821928075
Epoch 4681/10000, Prediction Accuracy = 61.786%, Loss = 0.43044133186340333
Epoch: 4681, Batch Gradient Norm: 7.347608859007617
Epoch: 4681, Batch Gradient Norm after: 7.347608859007617
Epoch 4682/10000, Prediction Accuracy = 61.926%, Loss = 0.4331977546215057
Epoch: 4682, Batch Gradient Norm: 8.428571709819876
Epoch: 4682, Batch Gradient Norm after: 8.428571709819876
Epoch 4683/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.4420218110084534
Epoch: 4683, Batch Gradient Norm: 13.36091959802371
Epoch: 4683, Batch Gradient Norm after: 13.36091959802371
Epoch 4684/10000, Prediction Accuracy = 61.688%, Loss = 0.47846934795379636
Epoch: 4684, Batch Gradient Norm: 11.447362153759585
Epoch: 4684, Batch Gradient Norm after: 11.447362153759585
Epoch 4685/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4614114224910736
Epoch: 4685, Batch Gradient Norm: 11.081543308719507
Epoch: 4685, Batch Gradient Norm after: 11.081543308719507
Epoch 4686/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.45773398876190186
Epoch: 4686, Batch Gradient Norm: 8.589387292332882
Epoch: 4686, Batch Gradient Norm after: 8.589387292332882
Epoch 4687/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.44179133176803587
Epoch: 4687, Batch Gradient Norm: 8.372607887839136
Epoch: 4687, Batch Gradient Norm after: 8.372607887839136
Epoch 4688/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.44077653288841245
Epoch: 4688, Batch Gradient Norm: 10.947841189800975
Epoch: 4688, Batch Gradient Norm after: 10.947841189800975
Epoch 4689/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4561939060688019
Epoch: 4689, Batch Gradient Norm: 11.932908400820358
Epoch: 4689, Batch Gradient Norm after: 11.932908400820358
Epoch 4690/10000, Prediction Accuracy = 61.874%, Loss = 0.46663036942481995
Epoch: 4690, Batch Gradient Norm: 9.263437931664713
Epoch: 4690, Batch Gradient Norm after: 9.263437931664713
Epoch 4691/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4468929886817932
Epoch: 4691, Batch Gradient Norm: 9.816670677766984
Epoch: 4691, Batch Gradient Norm after: 9.816670677766984
Epoch 4692/10000, Prediction Accuracy = 61.948%, Loss = 0.4479177832603455
Epoch: 4692, Batch Gradient Norm: 10.185631887939115
Epoch: 4692, Batch Gradient Norm after: 10.185631887939115
Epoch 4693/10000, Prediction Accuracy = 61.866%, Loss = 0.452876889705658
Epoch: 4693, Batch Gradient Norm: 10.194099441665928
Epoch: 4693, Batch Gradient Norm after: 10.194099441665928
Epoch 4694/10000, Prediction Accuracy = 61.83%, Loss = 0.4560728073120117
Epoch: 4694, Batch Gradient Norm: 6.951940519580115
Epoch: 4694, Batch Gradient Norm after: 6.951940519580115
Epoch 4695/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.433526611328125
Epoch: 4695, Batch Gradient Norm: 7.558501271665583
Epoch: 4695, Batch Gradient Norm after: 7.558501271665583
Epoch 4696/10000, Prediction Accuracy = 61.9%, Loss = 0.4355282008647919
Epoch: 4696, Batch Gradient Norm: 12.541766544876173
Epoch: 4696, Batch Gradient Norm after: 12.541766544876173
Epoch 4697/10000, Prediction Accuracy = 61.746%, Loss = 0.4731081426143646
Epoch: 4697, Batch Gradient Norm: 11.106423566366699
Epoch: 4697, Batch Gradient Norm after: 11.106423566366699
Epoch 4698/10000, Prediction Accuracy = 61.914%, Loss = 0.4593529045581818
Epoch: 4698, Batch Gradient Norm: 9.76523126841919
Epoch: 4698, Batch Gradient Norm after: 9.76523126841919
Epoch 4699/10000, Prediction Accuracy = 61.898%, Loss = 0.45180413126945496
Epoch: 4699, Batch Gradient Norm: 11.631468547113398
Epoch: 4699, Batch Gradient Norm after: 11.631468547113398
Epoch 4700/10000, Prediction Accuracy = 61.802%, Loss = 0.4695843100547791
Epoch: 4700, Batch Gradient Norm: 12.050498177763005
Epoch: 4700, Batch Gradient Norm after: 12.050498177763005
Epoch 4701/10000, Prediction Accuracy = 61.774%, Loss = 0.47250736951828004
Epoch: 4701, Batch Gradient Norm: 8.135983409303831
Epoch: 4701, Batch Gradient Norm after: 8.135983409303831
Epoch 4702/10000, Prediction Accuracy = 61.848%, Loss = 0.4400349497795105
Epoch: 4702, Batch Gradient Norm: 9.072882912122001
Epoch: 4702, Batch Gradient Norm after: 9.072882912122001
Epoch 4703/10000, Prediction Accuracy = 61.93000000000001%, Loss = 0.44335509538650514
Epoch: 4703, Batch Gradient Norm: 8.96306680752661
Epoch: 4703, Batch Gradient Norm after: 8.96306680752661
Epoch 4704/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.443964546918869
Epoch: 4704, Batch Gradient Norm: 10.57859625978482
Epoch: 4704, Batch Gradient Norm after: 10.57859625978482
Epoch 4705/10000, Prediction Accuracy = 61.886%, Loss = 0.4530312180519104
Epoch: 4705, Batch Gradient Norm: 9.09906578101449
Epoch: 4705, Batch Gradient Norm after: 9.09906578101449
Epoch 4706/10000, Prediction Accuracy = 61.852%, Loss = 0.4436104655265808
Epoch: 4706, Batch Gradient Norm: 8.882558881165373
Epoch: 4706, Batch Gradient Norm after: 8.882558881165373
Epoch 4707/10000, Prediction Accuracy = 61.852%, Loss = 0.44636763334274293
Epoch: 4707, Batch Gradient Norm: 7.732083700469811
Epoch: 4707, Batch Gradient Norm after: 7.732083700469811
Epoch 4708/10000, Prediction Accuracy = 61.862%, Loss = 0.43674578666687014
Epoch: 4708, Batch Gradient Norm: 11.71547295478343
Epoch: 4708, Batch Gradient Norm after: 11.71547295478343
Epoch 4709/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4696817517280579
Epoch: 4709, Batch Gradient Norm: 6.726875740915583
Epoch: 4709, Batch Gradient Norm after: 6.726875740915583
Epoch 4710/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4327601432800293
Epoch: 4710, Batch Gradient Norm: 9.535364422933144
Epoch: 4710, Batch Gradient Norm after: 9.535364422933144
Epoch 4711/10000, Prediction Accuracy = 61.824%, Loss = 0.44580101370811465
Epoch: 4711, Batch Gradient Norm: 11.770598286196753
Epoch: 4711, Batch Gradient Norm after: 11.770598286196753
Epoch 4712/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.46291009783744813
Epoch: 4712, Batch Gradient Norm: 12.392496018174135
Epoch: 4712, Batch Gradient Norm after: 12.392496018174135
Epoch 4713/10000, Prediction Accuracy = 61.874%, Loss = 0.46810545325279235
Epoch: 4713, Batch Gradient Norm: 10.94313203902802
Epoch: 4713, Batch Gradient Norm after: 10.94313203902802
Epoch 4714/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4559949219226837
Epoch: 4714, Batch Gradient Norm: 9.593873369428078
Epoch: 4714, Batch Gradient Norm after: 9.593873369428078
Epoch 4715/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4511589288711548
Epoch: 4715, Batch Gradient Norm: 9.876060799695493
Epoch: 4715, Batch Gradient Norm after: 9.876060799695493
Epoch 4716/10000, Prediction Accuracy = 61.948%, Loss = 0.451304042339325
Epoch: 4716, Batch Gradient Norm: 7.708619117819159
Epoch: 4716, Batch Gradient Norm after: 7.708619117819159
Epoch 4717/10000, Prediction Accuracy = 61.83%, Loss = 0.440584272146225
Epoch: 4717, Batch Gradient Norm: 8.278552784431527
Epoch: 4717, Batch Gradient Norm after: 8.278552784431527
Epoch 4718/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.440373432636261
Epoch: 4718, Batch Gradient Norm: 8.331182935555574
Epoch: 4718, Batch Gradient Norm after: 8.331182935555574
Epoch 4719/10000, Prediction Accuracy = 62.024%, Loss = 0.4413336515426636
Epoch: 4719, Batch Gradient Norm: 7.765191366093081
Epoch: 4719, Batch Gradient Norm after: 7.765191366093081
Epoch 4720/10000, Prediction Accuracy = 62.09000000000001%, Loss = 0.43735966086387634
Epoch: 4720, Batch Gradient Norm: 9.331520603109972
Epoch: 4720, Batch Gradient Norm after: 9.331520603109972
Epoch 4721/10000, Prediction Accuracy = 61.92%, Loss = 0.44566338658332827
Epoch: 4721, Batch Gradient Norm: 12.585211015541054
Epoch: 4721, Batch Gradient Norm after: 12.585211015541054
Epoch 4722/10000, Prediction Accuracy = 61.888%, Loss = 0.4701629638671875
Epoch: 4722, Batch Gradient Norm: 11.2573751496967
Epoch: 4722, Batch Gradient Norm after: 11.2573751496967
Epoch 4723/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4620999157428741
Epoch: 4723, Batch Gradient Norm: 10.33599283456608
Epoch: 4723, Batch Gradient Norm after: 10.33599283456608
Epoch 4724/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.45412911772727965
Epoch: 4724, Batch Gradient Norm: 9.455704474701635
Epoch: 4724, Batch Gradient Norm after: 9.455704474701635
Epoch 4725/10000, Prediction Accuracy = 61.886%, Loss = 0.44415038228034975
Epoch: 4725, Batch Gradient Norm: 7.871303363173562
Epoch: 4725, Batch Gradient Norm after: 7.871303363173562
Epoch 4726/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4346080243587494
Epoch: 4726, Batch Gradient Norm: 10.963555074842427
Epoch: 4726, Batch Gradient Norm after: 10.963555074842427
Epoch 4727/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.4527563273906708
Epoch: 4727, Batch Gradient Norm: 15.106894709240903
Epoch: 4727, Batch Gradient Norm after: 15.106894709240903
Epoch 4728/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.48795866370201113
Epoch: 4728, Batch Gradient Norm: 9.667601034874698
Epoch: 4728, Batch Gradient Norm after: 9.667601034874698
Epoch 4729/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.4477107644081116
Epoch: 4729, Batch Gradient Norm: 9.2813804019726
Epoch: 4729, Batch Gradient Norm after: 9.2813804019726
Epoch 4730/10000, Prediction Accuracy = 61.698%, Loss = 0.44673582911491394
Epoch: 4730, Batch Gradient Norm: 10.43764115463767
Epoch: 4730, Batch Gradient Norm after: 10.43764115463767
Epoch 4731/10000, Prediction Accuracy = 61.98600000000001%, Loss = 0.4545099139213562
Epoch: 4731, Batch Gradient Norm: 7.646982996652021
Epoch: 4731, Batch Gradient Norm after: 7.646982996652021
Epoch 4732/10000, Prediction Accuracy = 61.92%, Loss = 0.4358870148658752
Epoch: 4732, Batch Gradient Norm: 6.748140268220117
Epoch: 4732, Batch Gradient Norm after: 6.748140268220117
Epoch 4733/10000, Prediction Accuracy = 61.944%, Loss = 0.4298364520072937
Epoch: 4733, Batch Gradient Norm: 10.779367598849953
Epoch: 4733, Batch Gradient Norm after: 10.779367598849953
Epoch 4734/10000, Prediction Accuracy = 61.836%, Loss = 0.45269232988357544
Epoch: 4734, Batch Gradient Norm: 10.462421525562311
Epoch: 4734, Batch Gradient Norm after: 10.462421525562311
Epoch 4735/10000, Prediction Accuracy = 61.814%, Loss = 0.4603502810001373
Epoch: 4735, Batch Gradient Norm: 9.302701936890646
Epoch: 4735, Batch Gradient Norm after: 9.302701936890646
Epoch 4736/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4461523950099945
Epoch: 4736, Batch Gradient Norm: 7.006657380869537
Epoch: 4736, Batch Gradient Norm after: 7.006657380869537
Epoch 4737/10000, Prediction Accuracy = 61.948%, Loss = 0.4318240642547607
Epoch: 4737, Batch Gradient Norm: 9.253527742608036
Epoch: 4737, Batch Gradient Norm after: 9.253527742608036
Epoch 4738/10000, Prediction Accuracy = 61.888%, Loss = 0.4443388402462006
Epoch: 4738, Batch Gradient Norm: 10.495212204865977
Epoch: 4738, Batch Gradient Norm after: 10.495212204865977
Epoch 4739/10000, Prediction Accuracy = 61.758%, Loss = 0.4530197024345398
Epoch: 4739, Batch Gradient Norm: 8.775411207390846
Epoch: 4739, Batch Gradient Norm after: 8.775411207390846
Epoch 4740/10000, Prediction Accuracy = 61.90999999999999%, Loss = 0.4426972270011902
Epoch: 4740, Batch Gradient Norm: 11.635336172277613
Epoch: 4740, Batch Gradient Norm after: 11.635336172277613
Epoch 4741/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4636619985103607
Epoch: 4741, Batch Gradient Norm: 11.190359577724779
Epoch: 4741, Batch Gradient Norm after: 11.190359577724779
Epoch 4742/10000, Prediction Accuracy = 61.83%, Loss = 0.45867769718170165
Epoch: 4742, Batch Gradient Norm: 8.26067192106309
Epoch: 4742, Batch Gradient Norm after: 8.26067192106309
Epoch 4743/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.4380351424217224
Epoch: 4743, Batch Gradient Norm: 8.399974197648167
Epoch: 4743, Batch Gradient Norm after: 8.399974197648167
Epoch 4744/10000, Prediction Accuracy = 61.886%, Loss = 0.43657331466674804
Epoch: 4744, Batch Gradient Norm: 10.893538717386466
Epoch: 4744, Batch Gradient Norm after: 10.893538717386466
Epoch 4745/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4531893849372864
Epoch: 4745, Batch Gradient Norm: 12.776126719371103
Epoch: 4745, Batch Gradient Norm after: 12.776126719371103
Epoch 4746/10000, Prediction Accuracy = 61.830000000000005%, Loss = 0.47518421411514283
Epoch: 4746, Batch Gradient Norm: 11.629233323283312
Epoch: 4746, Batch Gradient Norm after: 11.629233323283312
Epoch 4747/10000, Prediction Accuracy = 61.688%, Loss = 0.4700064957141876
Epoch: 4747, Batch Gradient Norm: 6.2845868098259565
Epoch: 4747, Batch Gradient Norm after: 6.2845868098259565
Epoch 4748/10000, Prediction Accuracy = 61.936%, Loss = 0.430514931678772
Epoch: 4748, Batch Gradient Norm: 11.281770483087989
Epoch: 4748, Batch Gradient Norm after: 11.281770483087989
Epoch 4749/10000, Prediction Accuracy = 61.888%, Loss = 0.4611720860004425
Epoch: 4749, Batch Gradient Norm: 9.302070375952342
Epoch: 4749, Batch Gradient Norm after: 9.302070375952342
Epoch 4750/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.44495253562927245
Epoch: 4750, Batch Gradient Norm: 9.026450845743963
Epoch: 4750, Batch Gradient Norm after: 9.026450845743963
Epoch 4751/10000, Prediction Accuracy = 61.9%, Loss = 0.4417889893054962
Epoch: 4751, Batch Gradient Norm: 8.754453836359733
Epoch: 4751, Batch Gradient Norm after: 8.754453836359733
Epoch 4752/10000, Prediction Accuracy = 61.734%, Loss = 0.43984119296073915
Epoch: 4752, Batch Gradient Norm: 10.72353284412291
Epoch: 4752, Batch Gradient Norm after: 10.72353284412291
Epoch 4753/10000, Prediction Accuracy = 61.814%, Loss = 0.4563974916934967
Epoch: 4753, Batch Gradient Norm: 10.194080833293317
Epoch: 4753, Batch Gradient Norm after: 10.194080833293317
Epoch 4754/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4515430867671967
Epoch: 4754, Batch Gradient Norm: 8.311700047783065
Epoch: 4754, Batch Gradient Norm after: 8.311700047783065
Epoch 4755/10000, Prediction Accuracy = 61.824%, Loss = 0.44018346071243286
Epoch: 4755, Batch Gradient Norm: 8.144667016764856
Epoch: 4755, Batch Gradient Norm after: 8.144667016764856
Epoch 4756/10000, Prediction Accuracy = 61.95%, Loss = 0.4378812849521637
Epoch: 4756, Batch Gradient Norm: 10.167782945415036
Epoch: 4756, Batch Gradient Norm after: 10.167782945415036
Epoch 4757/10000, Prediction Accuracy = 61.854%, Loss = 0.45344385504722595
Epoch: 4757, Batch Gradient Norm: 9.42144651074888
Epoch: 4757, Batch Gradient Norm after: 9.42144651074888
Epoch 4758/10000, Prediction Accuracy = 61.79%, Loss = 0.44692654609680177
Epoch: 4758, Batch Gradient Norm: 10.698791493816238
Epoch: 4758, Batch Gradient Norm after: 10.698791493816238
Epoch 4759/10000, Prediction Accuracy = 61.676%, Loss = 0.458242791891098
Epoch: 4759, Batch Gradient Norm: 12.262815647340416
Epoch: 4759, Batch Gradient Norm after: 12.262815647340416
Epoch 4760/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4639891147613525
Epoch: 4760, Batch Gradient Norm: 14.091229459401355
Epoch: 4760, Batch Gradient Norm after: 14.091229459401355
Epoch 4761/10000, Prediction Accuracy = 61.86199999999999%, Loss = 0.4828235685825348
Epoch: 4761, Batch Gradient Norm: 10.459957327444728
Epoch: 4761, Batch Gradient Norm after: 10.459957327444728
Epoch 4762/10000, Prediction Accuracy = 61.762%, Loss = 0.45579888224601744
Epoch: 4762, Batch Gradient Norm: 9.282343167675391
Epoch: 4762, Batch Gradient Norm after: 9.282343167675391
Epoch 4763/10000, Prediction Accuracy = 61.952%, Loss = 0.444635283946991
Epoch: 4763, Batch Gradient Norm: 7.947618147738162
Epoch: 4763, Batch Gradient Norm after: 7.947618147738162
Epoch 4764/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.4369865894317627
Epoch: 4764, Batch Gradient Norm: 7.523542600919088
Epoch: 4764, Batch Gradient Norm after: 7.523542600919088
Epoch 4765/10000, Prediction Accuracy = 61.824%, Loss = 0.43470877408981323
Epoch: 4765, Batch Gradient Norm: 6.843031391030676
Epoch: 4765, Batch Gradient Norm after: 6.843031391030676
Epoch 4766/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.430531257390976
Epoch: 4766, Batch Gradient Norm: 8.667946870779051
Epoch: 4766, Batch Gradient Norm after: 8.667946870779051
Epoch 4767/10000, Prediction Accuracy = 61.838%, Loss = 0.4419360220432281
Epoch: 4767, Batch Gradient Norm: 8.376097571451105
Epoch: 4767, Batch Gradient Norm after: 8.376097571451105
Epoch 4768/10000, Prediction Accuracy = 61.876%, Loss = 0.43940517902374265
Epoch: 4768, Batch Gradient Norm: 10.16651044264687
Epoch: 4768, Batch Gradient Norm after: 10.16651044264687
Epoch 4769/10000, Prediction Accuracy = 61.838%, Loss = 0.44878627061843873
Epoch: 4769, Batch Gradient Norm: 13.909740488419335
Epoch: 4769, Batch Gradient Norm after: 13.909740488419335
Epoch 4770/10000, Prediction Accuracy = 61.94000000000001%, Loss = 0.4805685818195343
Epoch: 4770, Batch Gradient Norm: 11.078877108778613
Epoch: 4770, Batch Gradient Norm after: 11.078877108778613
Epoch 4771/10000, Prediction Accuracy = 61.854%, Loss = 0.45626758337020873
Epoch: 4771, Batch Gradient Norm: 9.849419044395047
Epoch: 4771, Batch Gradient Norm after: 9.849419044395047
Epoch 4772/10000, Prediction Accuracy = 61.758%, Loss = 0.4484477698802948
Epoch: 4772, Batch Gradient Norm: 6.986499160336443
Epoch: 4772, Batch Gradient Norm after: 6.986499160336443
Epoch 4773/10000, Prediction Accuracy = 61.96%, Loss = 0.43186606764793395
Epoch: 4773, Batch Gradient Norm: 8.384427909172608
Epoch: 4773, Batch Gradient Norm after: 8.384427909172608
Epoch 4774/10000, Prediction Accuracy = 61.886%, Loss = 0.43827914595603945
Epoch: 4774, Batch Gradient Norm: 9.110340150751705
Epoch: 4774, Batch Gradient Norm after: 9.110340150751705
Epoch 4775/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4407712757587433
Epoch: 4775, Batch Gradient Norm: 6.541723649831502
Epoch: 4775, Batch Gradient Norm after: 6.541723649831502
Epoch 4776/10000, Prediction Accuracy = 61.908%, Loss = 0.4286412954330444
Epoch: 4776, Batch Gradient Norm: 7.227316285367604
Epoch: 4776, Batch Gradient Norm after: 7.227316285367604
Epoch 4777/10000, Prediction Accuracy = 61.976%, Loss = 0.4320779383182526
Epoch: 4777, Batch Gradient Norm: 12.171202746601407
Epoch: 4777, Batch Gradient Norm after: 12.171202746601407
Epoch 4778/10000, Prediction Accuracy = 61.702%, Loss = 0.4686456322669983
Epoch: 4778, Batch Gradient Norm: 11.709886745078835
Epoch: 4778, Batch Gradient Norm after: 11.709886745078835
Epoch 4779/10000, Prediction Accuracy = 61.77%, Loss = 0.46211334466934206
Epoch: 4779, Batch Gradient Norm: 9.957715057779028
Epoch: 4779, Batch Gradient Norm after: 9.957715057779028
Epoch 4780/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4474481701850891
Epoch: 4780, Batch Gradient Norm: 11.450161903051724
Epoch: 4780, Batch Gradient Norm after: 11.450161903051724
Epoch 4781/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4655587196350098
Epoch: 4781, Batch Gradient Norm: 9.972721811776786
Epoch: 4781, Batch Gradient Norm after: 9.972721811776786
Epoch 4782/10000, Prediction Accuracy = 61.812%, Loss = 0.4510085105895996
Epoch: 4782, Batch Gradient Norm: 9.473344303276646
Epoch: 4782, Batch Gradient Norm after: 9.473344303276646
Epoch 4783/10000, Prediction Accuracy = 61.96%, Loss = 0.4452728807926178
Epoch: 4783, Batch Gradient Norm: 10.013333550692774
Epoch: 4783, Batch Gradient Norm after: 10.013333550692774
Epoch 4784/10000, Prediction Accuracy = 61.968%, Loss = 0.44724156260490416
Epoch: 4784, Batch Gradient Norm: 10.082693063422902
Epoch: 4784, Batch Gradient Norm after: 10.082693063422902
Epoch 4785/10000, Prediction Accuracy = 61.992%, Loss = 0.4492107629776001
Epoch: 4785, Batch Gradient Norm: 8.34254988387832
Epoch: 4785, Batch Gradient Norm after: 8.34254988387832
Epoch 4786/10000, Prediction Accuracy = 61.91400000000001%, Loss = 0.437988817691803
Epoch: 4786, Batch Gradient Norm: 8.891259788590494
Epoch: 4786, Batch Gradient Norm after: 8.891259788590494
Epoch 4787/10000, Prediction Accuracy = 61.938%, Loss = 0.4390495002269745
Epoch: 4787, Batch Gradient Norm: 11.140466075147026
Epoch: 4787, Batch Gradient Norm after: 11.140466075147026
Epoch 4788/10000, Prediction Accuracy = 61.774%, Loss = 0.4542742073535919
Epoch: 4788, Batch Gradient Norm: 12.893646382912358
Epoch: 4788, Batch Gradient Norm after: 12.893646382912358
Epoch 4789/10000, Prediction Accuracy = 61.676%, Loss = 0.4696531355381012
Epoch: 4789, Batch Gradient Norm: 10.229063639835163
Epoch: 4789, Batch Gradient Norm after: 10.229063639835163
Epoch 4790/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4533425152301788
Epoch: 4790, Batch Gradient Norm: 9.61942166716749
Epoch: 4790, Batch Gradient Norm after: 9.61942166716749
Epoch 4791/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.447211891412735
Epoch: 4791, Batch Gradient Norm: 9.12281080183372
Epoch: 4791, Batch Gradient Norm after: 9.12281080183372
Epoch 4792/10000, Prediction Accuracy = 62.016%, Loss = 0.4418513536453247
Epoch: 4792, Batch Gradient Norm: 8.946325913251883
Epoch: 4792, Batch Gradient Norm after: 8.946325913251883
Epoch 4793/10000, Prediction Accuracy = 61.838%, Loss = 0.44100261926651
Epoch: 4793, Batch Gradient Norm: 11.295604735748658
Epoch: 4793, Batch Gradient Norm after: 11.295604735748658
Epoch 4794/10000, Prediction Accuracy = 61.884%, Loss = 0.4559051632881165
Epoch: 4794, Batch Gradient Norm: 12.805667940288794
Epoch: 4794, Batch Gradient Norm after: 12.805667940288794
Epoch 4795/10000, Prediction Accuracy = 61.83%, Loss = 0.47053208351135256
Epoch: 4795, Batch Gradient Norm: 8.303581500365661
Epoch: 4795, Batch Gradient Norm after: 8.303581500365661
Epoch 4796/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.43824291229248047
Epoch: 4796, Batch Gradient Norm: 9.079561609347731
Epoch: 4796, Batch Gradient Norm after: 9.079561609347731
Epoch 4797/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.4417287826538086
Epoch: 4797, Batch Gradient Norm: 10.225754751643887
Epoch: 4797, Batch Gradient Norm after: 10.225754751643887
Epoch 4798/10000, Prediction Accuracy = 61.914%, Loss = 0.45263904333114624
Epoch: 4798, Batch Gradient Norm: 11.01446825301416
Epoch: 4798, Batch Gradient Norm after: 11.01446825301416
Epoch 4799/10000, Prediction Accuracy = 61.85%, Loss = 0.46426761746406553
Epoch: 4799, Batch Gradient Norm: 5.956173295602106
Epoch: 4799, Batch Gradient Norm after: 5.956173295602106
Epoch 4800/10000, Prediction Accuracy = 61.89%, Loss = 0.4267310082912445
Epoch: 4800, Batch Gradient Norm: 6.7177543427821655
Epoch: 4800, Batch Gradient Norm after: 6.7177543427821655
Epoch 4801/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4300025403499603
Epoch: 4801, Batch Gradient Norm: 10.01177545054195
Epoch: 4801, Batch Gradient Norm after: 10.01177545054195
Epoch 4802/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.44890886545181274
Epoch: 4802, Batch Gradient Norm: 12.349477437175716
Epoch: 4802, Batch Gradient Norm after: 12.349477437175716
Epoch 4803/10000, Prediction Accuracy = 61.988%, Loss = 0.46901214122772217
Epoch: 4803, Batch Gradient Norm: 10.575192854650052
Epoch: 4803, Batch Gradient Norm after: 10.575192854650052
Epoch 4804/10000, Prediction Accuracy = 61.778%, Loss = 0.45256741642951964
Epoch: 4804, Batch Gradient Norm: 10.2783412119386
Epoch: 4804, Batch Gradient Norm after: 10.2783412119386
Epoch 4805/10000, Prediction Accuracy = 61.778%, Loss = 0.4499978840351105
Epoch: 4805, Batch Gradient Norm: 10.533795748630801
Epoch: 4805, Batch Gradient Norm after: 10.533795748630801
Epoch 4806/10000, Prediction Accuracy = 61.806%, Loss = 0.4543541967868805
Epoch: 4806, Batch Gradient Norm: 8.341800085939697
Epoch: 4806, Batch Gradient Norm after: 8.341800085939697
Epoch 4807/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.43658374547958373
Epoch: 4807, Batch Gradient Norm: 8.812051654532445
Epoch: 4807, Batch Gradient Norm after: 8.812051654532445
Epoch 4808/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.4392297983169556
Epoch: 4808, Batch Gradient Norm: 10.612235209000424
Epoch: 4808, Batch Gradient Norm after: 10.612235209000424
Epoch 4809/10000, Prediction Accuracy = 61.878%, Loss = 0.45142701268196106
Epoch: 4809, Batch Gradient Norm: 12.020696000800672
Epoch: 4809, Batch Gradient Norm after: 12.020696000800672
Epoch 4810/10000, Prediction Accuracy = 61.688%, Loss = 0.46708364486694337
Epoch: 4810, Batch Gradient Norm: 8.591405105900206
Epoch: 4810, Batch Gradient Norm after: 8.591405105900206
Epoch 4811/10000, Prediction Accuracy = 61.818%, Loss = 0.43960232734680177
Epoch: 4811, Batch Gradient Norm: 7.9929016856929
Epoch: 4811, Batch Gradient Norm after: 7.9929016856929
Epoch 4812/10000, Prediction Accuracy = 61.876%, Loss = 0.43490583896636964
Epoch: 4812, Batch Gradient Norm: 8.614698126414343
Epoch: 4812, Batch Gradient Norm after: 8.614698126414343
Epoch 4813/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.439982271194458
Epoch: 4813, Batch Gradient Norm: 12.93330500715464
Epoch: 4813, Batch Gradient Norm after: 12.93330500715464
Epoch 4814/10000, Prediction Accuracy = 61.838%, Loss = 0.471987372636795
Epoch: 4814, Batch Gradient Norm: 10.169875219129398
Epoch: 4814, Batch Gradient Norm after: 10.169875219129398
Epoch 4815/10000, Prediction Accuracy = 61.862%, Loss = 0.4523374319076538
Epoch: 4815, Batch Gradient Norm: 8.637779016901431
Epoch: 4815, Batch Gradient Norm after: 8.637779016901431
Epoch 4816/10000, Prediction Accuracy = 61.884%, Loss = 0.43790319561958313
Epoch: 4816, Batch Gradient Norm: 12.925878252796627
Epoch: 4816, Batch Gradient Norm after: 12.925878252796627
Epoch 4817/10000, Prediction Accuracy = 61.838%, Loss = 0.46988884806632997
Epoch: 4817, Batch Gradient Norm: 11.247579498997663
Epoch: 4817, Batch Gradient Norm after: 11.247579498997663
Epoch 4818/10000, Prediction Accuracy = 61.898%, Loss = 0.45702369809150695
Epoch: 4818, Batch Gradient Norm: 5.613764644723561
Epoch: 4818, Batch Gradient Norm after: 5.613764644723561
Epoch 4819/10000, Prediction Accuracy = 61.838%, Loss = 0.4228492558002472
Epoch: 4819, Batch Gradient Norm: 7.81819371891665
Epoch: 4819, Batch Gradient Norm after: 7.81819371891665
Epoch 4820/10000, Prediction Accuracy = 61.814%, Loss = 0.43421375155448916
Epoch: 4820, Batch Gradient Norm: 10.267826252624506
Epoch: 4820, Batch Gradient Norm after: 10.267826252624506
Epoch 4821/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.44829931259155276
Epoch: 4821, Batch Gradient Norm: 13.055130985487091
Epoch: 4821, Batch Gradient Norm after: 13.055130985487091
Epoch 4822/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4693888247013092
Epoch: 4822, Batch Gradient Norm: 12.139970231985744
Epoch: 4822, Batch Gradient Norm after: 12.139970231985744
Epoch 4823/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4676421284675598
Epoch: 4823, Batch Gradient Norm: 7.547267069760052
Epoch: 4823, Batch Gradient Norm after: 7.547267069760052
Epoch 4824/10000, Prediction Accuracy = 61.812%, Loss = 0.4358629047870636
Epoch: 4824, Batch Gradient Norm: 8.05505399976154
Epoch: 4824, Batch Gradient Norm after: 8.05505399976154
Epoch 4825/10000, Prediction Accuracy = 61.908%, Loss = 0.4371980011463165
Epoch: 4825, Batch Gradient Norm: 9.552305410951105
Epoch: 4825, Batch Gradient Norm after: 9.552305410951105
Epoch 4826/10000, Prediction Accuracy = 62.048%, Loss = 0.4463954150676727
Epoch: 4826, Batch Gradient Norm: 10.070530306067873
Epoch: 4826, Batch Gradient Norm after: 10.070530306067873
Epoch 4827/10000, Prediction Accuracy = 61.786%, Loss = 0.45461971759796144
Epoch: 4827, Batch Gradient Norm: 7.466615318224374
Epoch: 4827, Batch Gradient Norm after: 7.466615318224374
Epoch 4828/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.43273609280586245
Epoch: 4828, Batch Gradient Norm: 9.600133590088136
Epoch: 4828, Batch Gradient Norm after: 9.600133590088136
Epoch 4829/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.44676060080528257
Epoch: 4829, Batch Gradient Norm: 11.864556487057303
Epoch: 4829, Batch Gradient Norm after: 11.864556487057303
Epoch 4830/10000, Prediction Accuracy = 61.866%, Loss = 0.4636803865432739
Epoch: 4830, Batch Gradient Norm: 10.257926152376907
Epoch: 4830, Batch Gradient Norm after: 10.257926152376907
Epoch 4831/10000, Prediction Accuracy = 61.786%, Loss = 0.4482273876667023
Epoch: 4831, Batch Gradient Norm: 11.277899290562264
Epoch: 4831, Batch Gradient Norm after: 11.277899290562264
Epoch 4832/10000, Prediction Accuracy = 61.842%, Loss = 0.45401889085769653
Epoch: 4832, Batch Gradient Norm: 10.39028531668755
Epoch: 4832, Batch Gradient Norm after: 10.39028531668755
Epoch 4833/10000, Prediction Accuracy = 61.848%, Loss = 0.4515222132205963
Epoch: 4833, Batch Gradient Norm: 8.43402934362044
Epoch: 4833, Batch Gradient Norm after: 8.43402934362044
Epoch 4834/10000, Prediction Accuracy = 61.968%, Loss = 0.43778903484344484
Epoch: 4834, Batch Gradient Norm: 8.275194929943437
Epoch: 4834, Batch Gradient Norm after: 8.275194929943437
Epoch 4835/10000, Prediction Accuracy = 61.96%, Loss = 0.43514713644981384
Epoch: 4835, Batch Gradient Norm: 9.582374481329857
Epoch: 4835, Batch Gradient Norm after: 9.582374481329857
Epoch 4836/10000, Prediction Accuracy = 61.85%, Loss = 0.4457697868347168
Epoch: 4836, Batch Gradient Norm: 10.992766517639488
Epoch: 4836, Batch Gradient Norm after: 10.992766517639488
Epoch 4837/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4539530396461487
Epoch: 4837, Batch Gradient Norm: 7.961241005739425
Epoch: 4837, Batch Gradient Norm after: 7.961241005739425
Epoch 4838/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.435111403465271
Epoch: 4838, Batch Gradient Norm: 8.358705219718608
Epoch: 4838, Batch Gradient Norm after: 8.358705219718608
Epoch 4839/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4372712016105652
Epoch: 4839, Batch Gradient Norm: 7.646980829322152
Epoch: 4839, Batch Gradient Norm after: 7.646980829322152
Epoch 4840/10000, Prediction Accuracy = 61.874%, Loss = 0.43209166526794435
Epoch: 4840, Batch Gradient Norm: 10.961369271157968
Epoch: 4840, Batch Gradient Norm after: 10.961369271157968
Epoch 4841/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.45236998200416567
Epoch: 4841, Batch Gradient Norm: 11.799132310549934
Epoch: 4841, Batch Gradient Norm after: 11.799132310549934
Epoch 4842/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.46211209893226624
Epoch: 4842, Batch Gradient Norm: 10.015291457743247
Epoch: 4842, Batch Gradient Norm after: 10.015291457743247
Epoch 4843/10000, Prediction Accuracy = 61.836%, Loss = 0.450687438249588
Epoch: 4843, Batch Gradient Norm: 10.549606255538148
Epoch: 4843, Batch Gradient Norm after: 10.549606255538148
Epoch 4844/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.45739701986312864
Epoch: 4844, Batch Gradient Norm: 9.655387756766629
Epoch: 4844, Batch Gradient Norm after: 9.655387756766629
Epoch 4845/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4468688368797302
Epoch: 4845, Batch Gradient Norm: 9.187025020329415
Epoch: 4845, Batch Gradient Norm after: 9.187025020329415
Epoch 4846/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.44282073378562925
Epoch: 4846, Batch Gradient Norm: 10.131488801389322
Epoch: 4846, Batch Gradient Norm after: 10.131488801389322
Epoch 4847/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.4505616009235382
Epoch: 4847, Batch Gradient Norm: 6.429013872815945
Epoch: 4847, Batch Gradient Norm after: 6.429013872815945
Epoch 4848/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4280516028404236
Epoch: 4848, Batch Gradient Norm: 8.577605351157676
Epoch: 4848, Batch Gradient Norm after: 8.577605351157676
Epoch 4849/10000, Prediction Accuracy = 61.838%, Loss = 0.439022558927536
Epoch: 4849, Batch Gradient Norm: 10.184498845880555
Epoch: 4849, Batch Gradient Norm after: 10.184498845880555
Epoch 4850/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4484024465084076
Epoch: 4850, Batch Gradient Norm: 11.335953539080474
Epoch: 4850, Batch Gradient Norm after: 11.335953539080474
Epoch 4851/10000, Prediction Accuracy = 61.826%, Loss = 0.45855507254600525
Epoch: 4851, Batch Gradient Norm: 7.3049294054089975
Epoch: 4851, Batch Gradient Norm after: 7.3049294054089975
Epoch 4852/10000, Prediction Accuracy = 61.89000000000001%, Loss = 0.4310917258262634
Epoch: 4852, Batch Gradient Norm: 10.502182062163085
Epoch: 4852, Batch Gradient Norm after: 10.502182062163085
Epoch 4853/10000, Prediction Accuracy = 61.938%, Loss = 0.45283021330833434
Epoch: 4853, Batch Gradient Norm: 11.152063626941985
Epoch: 4853, Batch Gradient Norm after: 11.152063626941985
Epoch 4854/10000, Prediction Accuracy = 61.928%, Loss = 0.45583072304725647
Epoch: 4854, Batch Gradient Norm: 11.685361204090027
Epoch: 4854, Batch Gradient Norm after: 11.685361204090027
Epoch 4855/10000, Prediction Accuracy = 61.878%, Loss = 0.4602642893791199
Epoch: 4855, Batch Gradient Norm: 10.175781476305513
Epoch: 4855, Batch Gradient Norm after: 10.175781476305513
Epoch 4856/10000, Prediction Accuracy = 61.96%, Loss = 0.4479415059089661
Epoch: 4856, Batch Gradient Norm: 9.600304661716851
Epoch: 4856, Batch Gradient Norm after: 9.600304661716851
Epoch 4857/10000, Prediction Accuracy = 61.976%, Loss = 0.4468161940574646
Epoch: 4857, Batch Gradient Norm: 9.340214987922751
Epoch: 4857, Batch Gradient Norm after: 9.340214987922751
Epoch 4858/10000, Prediction Accuracy = 61.908%, Loss = 0.4423741340637207
Epoch: 4858, Batch Gradient Norm: 14.420613776772637
Epoch: 4858, Batch Gradient Norm after: 14.420613776772637
Epoch 4859/10000, Prediction Accuracy = 61.876%, Loss = 0.4827305734157562
Epoch: 4859, Batch Gradient Norm: 10.257919605746093
Epoch: 4859, Batch Gradient Norm after: 10.257919605746093
Epoch 4860/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.44697101712226867
Epoch: 4860, Batch Gradient Norm: 8.284422495799237
Epoch: 4860, Batch Gradient Norm after: 8.284422495799237
Epoch 4861/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4355985105037689
Epoch: 4861, Batch Gradient Norm: 7.130455095436764
Epoch: 4861, Batch Gradient Norm after: 7.130455095436764
Epoch 4862/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.42916661500930786
Epoch: 4862, Batch Gradient Norm: 9.011810162728324
Epoch: 4862, Batch Gradient Norm after: 9.011810162728324
Epoch 4863/10000, Prediction Accuracy = 61.924%, Loss = 0.44201125502586364
Epoch: 4863, Batch Gradient Norm: 7.199589644438844
Epoch: 4863, Batch Gradient Norm after: 7.199589644438844
Epoch 4864/10000, Prediction Accuracy = 61.888%, Loss = 0.43177462816238404
Epoch: 4864, Batch Gradient Norm: 7.863209790259008
Epoch: 4864, Batch Gradient Norm after: 7.863209790259008
Epoch 4865/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.43177264332771303
Epoch: 4865, Batch Gradient Norm: 12.109537216119621
Epoch: 4865, Batch Gradient Norm after: 12.109537216119621
Epoch 4866/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.4657795786857605
Epoch: 4866, Batch Gradient Norm: 8.499454785081522
Epoch: 4866, Batch Gradient Norm after: 8.499454785081522
Epoch 4867/10000, Prediction Accuracy = 61.964%, Loss = 0.4398674190044403
Epoch: 4867, Batch Gradient Norm: 11.61449625441275
Epoch: 4867, Batch Gradient Norm after: 11.61449625441275
Epoch 4868/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.45956498980522154
Epoch: 4868, Batch Gradient Norm: 11.034289566741535
Epoch: 4868, Batch Gradient Norm after: 11.034289566741535
Epoch 4869/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.45588648319244385
Epoch: 4869, Batch Gradient Norm: 8.908793717052978
Epoch: 4869, Batch Gradient Norm after: 8.908793717052978
Epoch 4870/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4449391603469849
Epoch: 4870, Batch Gradient Norm: 7.414281036011859
Epoch: 4870, Batch Gradient Norm after: 7.414281036011859
Epoch 4871/10000, Prediction Accuracy = 61.912%, Loss = 0.42912439107894895
Epoch: 4871, Batch Gradient Norm: 7.095041292308343
Epoch: 4871, Batch Gradient Norm after: 7.095041292308343
Epoch 4872/10000, Prediction Accuracy = 61.938%, Loss = 0.4267087340354919
Epoch: 4872, Batch Gradient Norm: 8.653926658100044
Epoch: 4872, Batch Gradient Norm after: 8.653926658100044
Epoch 4873/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.43834105134010315
Epoch: 4873, Batch Gradient Norm: 12.207800231321235
Epoch: 4873, Batch Gradient Norm after: 12.207800231321235
Epoch 4874/10000, Prediction Accuracy = 61.78000000000001%, Loss = 0.46750532984733584
Epoch: 4874, Batch Gradient Norm: 9.448912116361049
Epoch: 4874, Batch Gradient Norm after: 9.448912116361049
Epoch 4875/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4428026735782623
Epoch: 4875, Batch Gradient Norm: 11.369642288599975
Epoch: 4875, Batch Gradient Norm after: 11.369642288599975
Epoch 4876/10000, Prediction Accuracy = 61.822%, Loss = 0.46020678877830506
Epoch: 4876, Batch Gradient Norm: 11.49257414493773
Epoch: 4876, Batch Gradient Norm after: 11.49257414493773
Epoch 4877/10000, Prediction Accuracy = 61.878%, Loss = 0.4588177680969238
Epoch: 4877, Batch Gradient Norm: 10.668824955536056
Epoch: 4877, Batch Gradient Norm after: 10.668824955536056
Epoch 4878/10000, Prediction Accuracy = 61.8%, Loss = 0.4499387502670288
Epoch: 4878, Batch Gradient Norm: 11.972406595278992
Epoch: 4878, Batch Gradient Norm after: 11.972406595278992
Epoch 4879/10000, Prediction Accuracy = 61.733999999999995%, Loss = 0.4593742549419403
Epoch: 4879, Batch Gradient Norm: 11.385832302344387
Epoch: 4879, Batch Gradient Norm after: 11.385832302344387
Epoch 4880/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4581565082073212
Epoch: 4880, Batch Gradient Norm: 9.659099995448782
Epoch: 4880, Batch Gradient Norm after: 9.659099995448782
Epoch 4881/10000, Prediction Accuracy = 61.924%, Loss = 0.4417617440223694
Epoch: 4881, Batch Gradient Norm: 6.887937397944819
Epoch: 4881, Batch Gradient Norm after: 6.887937397944819
Epoch 4882/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.426596063375473
Epoch: 4882, Batch Gradient Norm: 9.761974889410192
Epoch: 4882, Batch Gradient Norm after: 9.761974889410192
Epoch 4883/10000, Prediction Accuracy = 61.85799999999999%, Loss = 0.44635710716247556
Epoch: 4883, Batch Gradient Norm: 11.766857136125369
Epoch: 4883, Batch Gradient Norm after: 11.766857136125369
Epoch 4884/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.4597287893295288
Epoch: 4884, Batch Gradient Norm: 9.005054739782105
Epoch: 4884, Batch Gradient Norm after: 9.005054739782105
Epoch 4885/10000, Prediction Accuracy = 61.802%, Loss = 0.4403806209564209
Epoch: 4885, Batch Gradient Norm: 7.460428261369745
Epoch: 4885, Batch Gradient Norm after: 7.460428261369745
Epoch 4886/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.429488468170166
Epoch: 4886, Batch Gradient Norm: 8.878432431656742
Epoch: 4886, Batch Gradient Norm after: 8.878432431656742
Epoch 4887/10000, Prediction Accuracy = 61.812%, Loss = 0.437737774848938
Epoch: 4887, Batch Gradient Norm: 12.48883179909462
Epoch: 4887, Batch Gradient Norm after: 12.48883179909462
Epoch 4888/10000, Prediction Accuracy = 61.77%, Loss = 0.46717952489852904
Epoch: 4888, Batch Gradient Norm: 9.885326070664407
Epoch: 4888, Batch Gradient Norm after: 9.885326070664407
Epoch 4889/10000, Prediction Accuracy = 62.004000000000005%, Loss = 0.44511061906814575
Epoch: 4889, Batch Gradient Norm: 9.285806441137938
Epoch: 4889, Batch Gradient Norm after: 9.285806441137938
Epoch 4890/10000, Prediction Accuracy = 61.89%, Loss = 0.439074969291687
Epoch: 4890, Batch Gradient Norm: 7.643965019132892
Epoch: 4890, Batch Gradient Norm after: 7.643965019132892
Epoch 4891/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.43400214314460756
Epoch: 4891, Batch Gradient Norm: 11.189656808032458
Epoch: 4891, Batch Gradient Norm after: 11.189656808032458
Epoch 4892/10000, Prediction Accuracy = 61.876%, Loss = 0.4544462084770203
Epoch: 4892, Batch Gradient Norm: 13.047498676442167
Epoch: 4892, Batch Gradient Norm after: 13.047498676442167
Epoch 4893/10000, Prediction Accuracy = 61.852%, Loss = 0.47424399852752686
Epoch: 4893, Batch Gradient Norm: 10.069520819025778
Epoch: 4893, Batch Gradient Norm after: 10.069520819025778
Epoch 4894/10000, Prediction Accuracy = 61.94200000000001%, Loss = 0.44838252663612366
Epoch: 4894, Batch Gradient Norm: 9.532552334760553
Epoch: 4894, Batch Gradient Norm after: 9.532552334760553
Epoch 4895/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.44677700400352477
Epoch: 4895, Batch Gradient Norm: 6.886261212545901
Epoch: 4895, Batch Gradient Norm after: 6.886261212545901
Epoch 4896/10000, Prediction Accuracy = 62.02%, Loss = 0.42806256413459776
Epoch: 4896, Batch Gradient Norm: 8.511776727799937
Epoch: 4896, Batch Gradient Norm after: 8.511776727799937
Epoch 4897/10000, Prediction Accuracy = 61.9%, Loss = 0.43916794657707214
Epoch: 4897, Batch Gradient Norm: 8.145430334429417
Epoch: 4897, Batch Gradient Norm after: 8.145430334429417
Epoch 4898/10000, Prediction Accuracy = 61.988%, Loss = 0.4361577808856964
Epoch: 4898, Batch Gradient Norm: 8.90142351538859
Epoch: 4898, Batch Gradient Norm after: 8.90142351538859
Epoch 4899/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4388044536113739
Epoch: 4899, Batch Gradient Norm: 11.274624321938088
Epoch: 4899, Batch Gradient Norm after: 11.274624321938088
Epoch 4900/10000, Prediction Accuracy = 62.076%, Loss = 0.4533891797065735
Epoch: 4900, Batch Gradient Norm: 7.532369517146055
Epoch: 4900, Batch Gradient Norm after: 7.532369517146055
Epoch 4901/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.4320279657840729
Epoch: 4901, Batch Gradient Norm: 6.683192844361445
Epoch: 4901, Batch Gradient Norm after: 6.683192844361445
Epoch 4902/10000, Prediction Accuracy = 61.852%, Loss = 0.4268868863582611
Epoch: 4902, Batch Gradient Norm: 10.499614288190882
Epoch: 4902, Batch Gradient Norm after: 10.499614288190882
Epoch 4903/10000, Prediction Accuracy = 61.948%, Loss = 0.44591336250305175
Epoch: 4903, Batch Gradient Norm: 11.359842225157385
Epoch: 4903, Batch Gradient Norm after: 11.359842225157385
Epoch 4904/10000, Prediction Accuracy = 61.812%, Loss = 0.4552789568901062
Epoch: 4904, Batch Gradient Norm: 8.346895224495308
Epoch: 4904, Batch Gradient Norm after: 8.346895224495308
Epoch 4905/10000, Prediction Accuracy = 61.86%, Loss = 0.4360224485397339
Epoch: 4905, Batch Gradient Norm: 8.927788439856755
Epoch: 4905, Batch Gradient Norm after: 8.927788439856755
Epoch 4906/10000, Prediction Accuracy = 61.914%, Loss = 0.4397731482982635
Epoch: 4906, Batch Gradient Norm: 12.00036142320913
Epoch: 4906, Batch Gradient Norm after: 12.00036142320913
Epoch 4907/10000, Prediction Accuracy = 61.902%, Loss = 0.4611803233623505
Epoch: 4907, Batch Gradient Norm: 10.596713877738019
Epoch: 4907, Batch Gradient Norm after: 10.596713877738019
Epoch 4908/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.44903693795204164
Epoch: 4908, Batch Gradient Norm: 9.458296891287933
Epoch: 4908, Batch Gradient Norm after: 9.458296891287933
Epoch 4909/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.44141613841056826
Epoch: 4909, Batch Gradient Norm: 12.291153532420054
Epoch: 4909, Batch Gradient Norm after: 12.291153532420054
Epoch 4910/10000, Prediction Accuracy = 61.798%, Loss = 0.4691831648349762
Epoch: 4910, Batch Gradient Norm: 11.22332314794793
Epoch: 4910, Batch Gradient Norm after: 11.22332314794793
Epoch 4911/10000, Prediction Accuracy = 61.926%, Loss = 0.4578332006931305
Epoch: 4911, Batch Gradient Norm: 10.014212241056276
Epoch: 4911, Batch Gradient Norm after: 10.014212241056276
Epoch 4912/10000, Prediction Accuracy = 62.0%, Loss = 0.4448175013065338
Epoch: 4912, Batch Gradient Norm: 10.134792881852034
Epoch: 4912, Batch Gradient Norm after: 10.134792881852034
Epoch 4913/10000, Prediction Accuracy = 61.872%, Loss = 0.44630420207977295
Epoch: 4913, Batch Gradient Norm: 6.484102842119578
Epoch: 4913, Batch Gradient Norm after: 6.484102842119578
Epoch 4914/10000, Prediction Accuracy = 61.926%, Loss = 0.4253507018089294
Epoch: 4914, Batch Gradient Norm: 9.4429068667825
Epoch: 4914, Batch Gradient Norm after: 9.4429068667825
Epoch 4915/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.43917234539985656
Epoch: 4915, Batch Gradient Norm: 12.487896133897602
Epoch: 4915, Batch Gradient Norm after: 12.487896133897602
Epoch 4916/10000, Prediction Accuracy = 61.706%, Loss = 0.46093079447746277
Epoch: 4916, Batch Gradient Norm: 10.644904382522698
Epoch: 4916, Batch Gradient Norm after: 10.644904382522698
Epoch 4917/10000, Prediction Accuracy = 61.854%, Loss = 0.45028420090675353
Epoch: 4917, Batch Gradient Norm: 10.26408231513101
Epoch: 4917, Batch Gradient Norm after: 10.26408231513101
Epoch 4918/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.4478497266769409
Epoch: 4918, Batch Gradient Norm: 8.471739774258827
Epoch: 4918, Batch Gradient Norm after: 8.471739774258827
Epoch 4919/10000, Prediction Accuracy = 61.952%, Loss = 0.4372602880001068
Epoch: 4919, Batch Gradient Norm: 10.125252316523364
Epoch: 4919, Batch Gradient Norm after: 10.125252316523364
Epoch 4920/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.44599026441574097
Epoch: 4920, Batch Gradient Norm: 9.329790375595268
Epoch: 4920, Batch Gradient Norm after: 9.329790375595268
Epoch 4921/10000, Prediction Accuracy = 61.972%, Loss = 0.4389358162879944
Epoch: 4921, Batch Gradient Norm: 9.0487965880522
Epoch: 4921, Batch Gradient Norm after: 9.0487965880522
Epoch 4922/10000, Prediction Accuracy = 61.92%, Loss = 0.4382359981536865
Epoch: 4922, Batch Gradient Norm: 7.633942629901745
Epoch: 4922, Batch Gradient Norm after: 7.633942629901745
Epoch 4923/10000, Prediction Accuracy = 61.822%, Loss = 0.43034903407096864
Epoch: 4923, Batch Gradient Norm: 10.156327695714232
Epoch: 4923, Batch Gradient Norm after: 10.156327695714232
Epoch 4924/10000, Prediction Accuracy = 61.9%, Loss = 0.44533986449241636
Epoch: 4924, Batch Gradient Norm: 10.435841371738375
Epoch: 4924, Batch Gradient Norm after: 10.435841371738375
Epoch 4925/10000, Prediction Accuracy = 61.83799999999999%, Loss = 0.4514364540576935
Epoch: 4925, Batch Gradient Norm: 7.992226685060454
Epoch: 4925, Batch Gradient Norm after: 7.992226685060454
Epoch 4926/10000, Prediction Accuracy = 61.988%, Loss = 0.43278326392173766
Epoch: 4926, Batch Gradient Norm: 10.744053101476537
Epoch: 4926, Batch Gradient Norm after: 10.744053101476537
Epoch 4927/10000, Prediction Accuracy = 61.94%, Loss = 0.44882374405860903
Epoch: 4927, Batch Gradient Norm: 10.711191284381687
Epoch: 4927, Batch Gradient Norm after: 10.711191284381687
Epoch 4928/10000, Prediction Accuracy = 61.888%, Loss = 0.451773601770401
Epoch: 4928, Batch Gradient Norm: 9.768471950615362
Epoch: 4928, Batch Gradient Norm after: 9.768471950615362
Epoch 4929/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.4470873177051544
Epoch: 4929, Batch Gradient Norm: 10.41053662396631
Epoch: 4929, Batch Gradient Norm after: 10.41053662396631
Epoch 4930/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.448309326171875
Epoch: 4930, Batch Gradient Norm: 10.26898108522622
Epoch: 4930, Batch Gradient Norm after: 10.26898108522622
Epoch 4931/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4465833842754364
Epoch: 4931, Batch Gradient Norm: 10.010859868650721
Epoch: 4931, Batch Gradient Norm after: 10.010859868650721
Epoch 4932/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.44960578680038454
Epoch: 4932, Batch Gradient Norm: 7.94425694156398
Epoch: 4932, Batch Gradient Norm after: 7.94425694156398
Epoch 4933/10000, Prediction Accuracy = 61.85%, Loss = 0.43440635204315187
Epoch: 4933, Batch Gradient Norm: 5.998956632981115
Epoch: 4933, Batch Gradient Norm after: 5.998956632981115
Epoch 4934/10000, Prediction Accuracy = 61.924%, Loss = 0.4222035050392151
Epoch: 4934, Batch Gradient Norm: 11.899578390717572
Epoch: 4934, Batch Gradient Norm after: 11.899578390717572
Epoch 4935/10000, Prediction Accuracy = 61.984%, Loss = 0.45789664387702944
Epoch: 4935, Batch Gradient Norm: 13.921397784434486
Epoch: 4935, Batch Gradient Norm after: 13.921397784434486
Epoch 4936/10000, Prediction Accuracy = 61.914%, Loss = 0.47541088461875913
Epoch: 4936, Batch Gradient Norm: 11.466228960235666
Epoch: 4936, Batch Gradient Norm after: 11.466228960235666
Epoch 4937/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4585862696170807
Epoch: 4937, Batch Gradient Norm: 7.811061052583632
Epoch: 4937, Batch Gradient Norm after: 7.811061052583632
Epoch 4938/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.43393015265464785
Epoch: 4938, Batch Gradient Norm: 10.087783346741599
Epoch: 4938, Batch Gradient Norm after: 10.087783346741599
Epoch 4939/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4490790069103241
Epoch: 4939, Batch Gradient Norm: 9.898767188603822
Epoch: 4939, Batch Gradient Norm after: 9.898767188603822
Epoch 4940/10000, Prediction Accuracy = 61.96%, Loss = 0.4498520016670227
Epoch: 4940, Batch Gradient Norm: 8.656325845212422
Epoch: 4940, Batch Gradient Norm after: 8.656325845212422
Epoch 4941/10000, Prediction Accuracy = 61.85999999999999%, Loss = 0.4379002869129181
Epoch: 4941, Batch Gradient Norm: 8.440771825727026
Epoch: 4941, Batch Gradient Norm after: 8.440771825727026
Epoch 4942/10000, Prediction Accuracy = 62.024%, Loss = 0.43368812203407286
Epoch: 4942, Batch Gradient Norm: 8.401412748333723
Epoch: 4942, Batch Gradient Norm after: 8.401412748333723
Epoch 4943/10000, Prediction Accuracy = 61.992%, Loss = 0.43434142470359804
Epoch: 4943, Batch Gradient Norm: 7.343521458673012
Epoch: 4943, Batch Gradient Norm after: 7.343521458673012
Epoch 4944/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.42969797253608705
Epoch: 4944, Batch Gradient Norm: 9.728667638554542
Epoch: 4944, Batch Gradient Norm after: 9.728667638554542
Epoch 4945/10000, Prediction Accuracy = 61.984%, Loss = 0.44407222867012025
Epoch: 4945, Batch Gradient Norm: 11.763448336969908
Epoch: 4945, Batch Gradient Norm after: 11.763448336969908
Epoch 4946/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.45701879262924194
Epoch: 4946, Batch Gradient Norm: 11.828494385727037
Epoch: 4946, Batch Gradient Norm after: 11.828494385727037
Epoch 4947/10000, Prediction Accuracy = 61.996%, Loss = 0.4601988077163696
Epoch: 4947, Batch Gradient Norm: 10.087956603658673
Epoch: 4947, Batch Gradient Norm after: 10.087956603658673
Epoch 4948/10000, Prediction Accuracy = 61.826%, Loss = 0.44429972767829895
Epoch: 4948, Batch Gradient Norm: 12.150692078398906
Epoch: 4948, Batch Gradient Norm after: 12.150692078398906
Epoch 4949/10000, Prediction Accuracy = 62.05%, Loss = 0.4583648920059204
Epoch: 4949, Batch Gradient Norm: 9.248079470835027
Epoch: 4949, Batch Gradient Norm after: 9.248079470835027
Epoch 4950/10000, Prediction Accuracy = 61.896%, Loss = 0.4391294538974762
Epoch: 4950, Batch Gradient Norm: 9.106726310477539
Epoch: 4950, Batch Gradient Norm after: 9.106726310477539
Epoch 4951/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.44203793406486513
Epoch: 4951, Batch Gradient Norm: 11.126899045313088
Epoch: 4951, Batch Gradient Norm after: 11.126899045313088
Epoch 4952/10000, Prediction Accuracy = 61.85%, Loss = 0.4515268087387085
Epoch: 4952, Batch Gradient Norm: 12.843047837943798
Epoch: 4952, Batch Gradient Norm after: 12.843047837943798
Epoch 4953/10000, Prediction Accuracy = 61.786%, Loss = 0.4678346574306488
Epoch: 4953, Batch Gradient Norm: 9.110688828124912
Epoch: 4953, Batch Gradient Norm after: 9.110688828124912
Epoch 4954/10000, Prediction Accuracy = 61.876%, Loss = 0.4394791781902313
Epoch: 4954, Batch Gradient Norm: 9.819916917642097
Epoch: 4954, Batch Gradient Norm after: 9.819916917642097
Epoch 4955/10000, Prediction Accuracy = 61.824%, Loss = 0.44882585406303405
Epoch: 4955, Batch Gradient Norm: 6.893903030127362
Epoch: 4955, Batch Gradient Norm after: 6.893903030127362
Epoch 4956/10000, Prediction Accuracy = 61.79%, Loss = 0.4270170748233795
Epoch: 4956, Batch Gradient Norm: 6.461576848974799
Epoch: 4956, Batch Gradient Norm after: 6.461576848974799
Epoch 4957/10000, Prediction Accuracy = 61.948%, Loss = 0.4272294044494629
Epoch: 4957, Batch Gradient Norm: 8.331695763833705
Epoch: 4957, Batch Gradient Norm after: 8.331695763833705
Epoch 4958/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.43411905169487
Epoch: 4958, Batch Gradient Norm: 10.325017199034598
Epoch: 4958, Batch Gradient Norm after: 10.325017199034598
Epoch 4959/10000, Prediction Accuracy = 61.878%, Loss = 0.4469428837299347
Epoch: 4959, Batch Gradient Norm: 11.195160200806384
Epoch: 4959, Batch Gradient Norm after: 11.195160200806384
Epoch 4960/10000, Prediction Accuracy = 61.872%, Loss = 0.4515403985977173
Epoch: 4960, Batch Gradient Norm: 8.78095602640856
Epoch: 4960, Batch Gradient Norm after: 8.78095602640856
Epoch 4961/10000, Prediction Accuracy = 61.928%, Loss = 0.4387702941894531
Epoch: 4961, Batch Gradient Norm: 7.890789110014699
Epoch: 4961, Batch Gradient Norm after: 7.890789110014699
Epoch 4962/10000, Prediction Accuracy = 61.908%, Loss = 0.4322690784931183
Epoch: 4962, Batch Gradient Norm: 9.319982437067512
Epoch: 4962, Batch Gradient Norm after: 9.319982437067512
Epoch 4963/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4384640395641327
Epoch: 4963, Batch Gradient Norm: 9.076201992450393
Epoch: 4963, Batch Gradient Norm after: 9.076201992450393
Epoch 4964/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.44045692682266235
Epoch: 4964, Batch Gradient Norm: 7.30539511773928
Epoch: 4964, Batch Gradient Norm after: 7.30539511773928
Epoch 4965/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.42814759016036985
Epoch: 4965, Batch Gradient Norm: 10.223343748352436
Epoch: 4965, Batch Gradient Norm after: 10.223343748352436
Epoch 4966/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.44630958437919616
Epoch: 4966, Batch Gradient Norm: 13.660884803127328
Epoch: 4966, Batch Gradient Norm after: 13.660884803127328
Epoch 4967/10000, Prediction Accuracy = 61.85999999999999%, Loss = 0.47436492443084716
Epoch: 4967, Batch Gradient Norm: 10.217551235357693
Epoch: 4967, Batch Gradient Norm after: 10.217551235357693
Epoch 4968/10000, Prediction Accuracy = 61.916%, Loss = 0.44813750982284545
Epoch: 4968, Batch Gradient Norm: 9.034993289337125
Epoch: 4968, Batch Gradient Norm after: 9.034993289337125
Epoch 4969/10000, Prediction Accuracy = 61.962%, Loss = 0.43690980672836305
Epoch: 4969, Batch Gradient Norm: 10.94556637739945
Epoch: 4969, Batch Gradient Norm after: 10.94556637739945
Epoch 4970/10000, Prediction Accuracy = 61.914%, Loss = 0.4513369381427765
Epoch: 4970, Batch Gradient Norm: 8.103045760794096
Epoch: 4970, Batch Gradient Norm after: 8.103045760794096
Epoch 4971/10000, Prediction Accuracy = 62.05400000000001%, Loss = 0.4322997212409973
Epoch: 4971, Batch Gradient Norm: 9.538814588083778
Epoch: 4971, Batch Gradient Norm after: 9.538814588083778
Epoch 4972/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.44010232090950013
Epoch: 4972, Batch Gradient Norm: 11.627860876934607
Epoch: 4972, Batch Gradient Norm after: 11.627860876934607
Epoch 4973/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4582177221775055
Epoch: 4973, Batch Gradient Norm: 13.918422743963088
Epoch: 4973, Batch Gradient Norm after: 13.918422743963088
Epoch 4974/10000, Prediction Accuracy = 61.878%, Loss = 0.4735256314277649
Epoch: 4974, Batch Gradient Norm: 8.431847018159898
Epoch: 4974, Batch Gradient Norm after: 8.431847018159898
Epoch 4975/10000, Prediction Accuracy = 61.848%, Loss = 0.43383894562721254
Epoch: 4975, Batch Gradient Norm: 5.320861679414405
Epoch: 4975, Batch Gradient Norm after: 5.320861679414405
Epoch 4976/10000, Prediction Accuracy = 61.926%, Loss = 0.41778175830841063
Epoch: 4976, Batch Gradient Norm: 10.2474536538327
Epoch: 4976, Batch Gradient Norm after: 10.2474536538327
Epoch 4977/10000, Prediction Accuracy = 61.914%, Loss = 0.44872687458992006
Epoch: 4977, Batch Gradient Norm: 8.575023469093667
Epoch: 4977, Batch Gradient Norm after: 8.575023469093667
Epoch 4978/10000, Prediction Accuracy = 61.84599999999999%, Loss = 0.4381915688514709
Epoch: 4978, Batch Gradient Norm: 8.586397779908356
Epoch: 4978, Batch Gradient Norm after: 8.586397779908356
Epoch 4979/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.44068865180015565
Epoch: 4979, Batch Gradient Norm: 9.805957662272125
Epoch: 4979, Batch Gradient Norm after: 9.805957662272125
Epoch 4980/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.44243188500404357
Epoch: 4980, Batch Gradient Norm: 10.494907319138274
Epoch: 4980, Batch Gradient Norm after: 10.494907319138274
Epoch 4981/10000, Prediction Accuracy = 61.952%, Loss = 0.446221137046814
Epoch: 4981, Batch Gradient Norm: 11.98031353719161
Epoch: 4981, Batch Gradient Norm after: 11.98031353719161
Epoch 4982/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.45673889517784116
Epoch: 4982, Batch Gradient Norm: 11.162514425865472
Epoch: 4982, Batch Gradient Norm after: 11.162514425865472
Epoch 4983/10000, Prediction Accuracy = 61.874%, Loss = 0.45091562867164614
Epoch: 4983, Batch Gradient Norm: 10.149076836889162
Epoch: 4983, Batch Gradient Norm after: 10.149076836889162
Epoch 4984/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.44517189264297485
Epoch: 4984, Batch Gradient Norm: 9.062825187892448
Epoch: 4984, Batch Gradient Norm after: 9.062825187892448
Epoch 4985/10000, Prediction Accuracy = 61.81%, Loss = 0.43740932941436766
Epoch: 4985, Batch Gradient Norm: 7.447565118135289
Epoch: 4985, Batch Gradient Norm after: 7.447565118135289
Epoch 4986/10000, Prediction Accuracy = 61.968%, Loss = 0.4275355875492096
Epoch: 4986, Batch Gradient Norm: 10.335609834972345
Epoch: 4986, Batch Gradient Norm after: 10.335609834972345
Epoch 4987/10000, Prediction Accuracy = 61.928%, Loss = 0.45155351161956786
Epoch: 4987, Batch Gradient Norm: 10.167465922751294
Epoch: 4987, Batch Gradient Norm after: 10.167465922751294
Epoch 4988/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.44995227456092834
Epoch: 4988, Batch Gradient Norm: 10.590647853530498
Epoch: 4988, Batch Gradient Norm after: 10.590647853530498
Epoch 4989/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4512714445590973
Epoch: 4989, Batch Gradient Norm: 8.868337268445078
Epoch: 4989, Batch Gradient Norm after: 8.868337268445078
Epoch 4990/10000, Prediction Accuracy = 61.89200000000001%, Loss = 0.4354367911815643
Epoch: 4990, Batch Gradient Norm: 10.450284644059868
Epoch: 4990, Batch Gradient Norm after: 10.450284644059868
Epoch 4991/10000, Prediction Accuracy = 61.912%, Loss = 0.4475416004657745
Epoch: 4991, Batch Gradient Norm: 11.033143048298822
Epoch: 4991, Batch Gradient Norm after: 11.033143048298822
Epoch 4992/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4504649519920349
Epoch: 4992, Batch Gradient Norm: 9.317009241548732
Epoch: 4992, Batch Gradient Norm after: 9.317009241548732
Epoch 4993/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.4369651794433594
Epoch: 4993, Batch Gradient Norm: 6.278095448385259
Epoch: 4993, Batch Gradient Norm after: 6.278095448385259
Epoch 4994/10000, Prediction Accuracy = 62.008%, Loss = 0.4217263162136078
Epoch: 4994, Batch Gradient Norm: 7.992404345915636
Epoch: 4994, Batch Gradient Norm after: 7.992404345915636
Epoch 4995/10000, Prediction Accuracy = 61.843999999999994%, Loss = 0.43040440082550047
Epoch: 4995, Batch Gradient Norm: 11.291007935923082
Epoch: 4995, Batch Gradient Norm after: 11.291007935923082
Epoch 4996/10000, Prediction Accuracy = 61.848%, Loss = 0.4558269798755646
Epoch: 4996, Batch Gradient Norm: 10.546564914040932
Epoch: 4996, Batch Gradient Norm after: 10.546564914040932
Epoch 4997/10000, Prediction Accuracy = 61.944%, Loss = 0.4477765500545502
Epoch: 4997, Batch Gradient Norm: 10.039141653952724
Epoch: 4997, Batch Gradient Norm after: 10.039141653952724
Epoch 4998/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.44520328044891355
Epoch: 4998, Batch Gradient Norm: 8.341326638943986
Epoch: 4998, Batch Gradient Norm after: 8.341326638943986
Epoch 4999/10000, Prediction Accuracy = 61.95%, Loss = 0.4342256188392639
Epoch: 4999, Batch Gradient Norm: 9.380449467309912
Epoch: 4999, Batch Gradient Norm after: 9.380449467309912
Epoch 5000/10000, Prediction Accuracy = 61.83%, Loss = 0.43968555331230164
Epoch: 5000, Batch Gradient Norm: 10.06546394428522
Epoch: 5000, Batch Gradient Norm after: 10.06546394428522
Epoch 5001/10000, Prediction Accuracy = 61.852%, Loss = 0.4459376513957977
Epoch: 5001, Batch Gradient Norm: 9.226923292132323
Epoch: 5001, Batch Gradient Norm after: 9.226923292132323
Epoch 5002/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4416057109832764
Epoch: 5002, Batch Gradient Norm: 13.290114959083319
Epoch: 5002, Batch Gradient Norm after: 13.290114959083319
Epoch 5003/10000, Prediction Accuracy = 61.71%, Loss = 0.47153284549713137
Epoch: 5003, Batch Gradient Norm: 10.030750568190049
Epoch: 5003, Batch Gradient Norm after: 10.030750568190049
Epoch 5004/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4451245129108429
Epoch: 5004, Batch Gradient Norm: 7.194858176385365
Epoch: 5004, Batch Gradient Norm after: 7.194858176385365
Epoch 5005/10000, Prediction Accuracy = 61.916%, Loss = 0.4284469723701477
Epoch: 5005, Batch Gradient Norm: 6.148019238311965
Epoch: 5005, Batch Gradient Norm after: 6.148019238311965
Epoch 5006/10000, Prediction Accuracy = 61.96%, Loss = 0.4226584851741791
Epoch: 5006, Batch Gradient Norm: 9.15223377896405
Epoch: 5006, Batch Gradient Norm after: 9.15223377896405
Epoch 5007/10000, Prediction Accuracy = 62.007999999999996%, Loss = 0.4354314267635345
Epoch: 5007, Batch Gradient Norm: 12.108946691522338
Epoch: 5007, Batch Gradient Norm after: 12.108946691522338
Epoch 5008/10000, Prediction Accuracy = 61.912%, Loss = 0.45644875764846804
Epoch: 5008, Batch Gradient Norm: 11.314612769717487
Epoch: 5008, Batch Gradient Norm after: 11.314612769717487
Epoch 5009/10000, Prediction Accuracy = 61.86999999999999%, Loss = 0.455008202791214
Epoch: 5009, Batch Gradient Norm: 10.552753203142643
Epoch: 5009, Batch Gradient Norm after: 10.552753203142643
Epoch 5010/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4500582039356232
Epoch: 5010, Batch Gradient Norm: 9.178694539173065
Epoch: 5010, Batch Gradient Norm after: 9.178694539173065
Epoch 5011/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.43868011236190796
Epoch: 5011, Batch Gradient Norm: 9.115270365513636
Epoch: 5011, Batch Gradient Norm after: 9.115270365513636
Epoch 5012/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.4391919434070587
Epoch: 5012, Batch Gradient Norm: 9.226492343536261
Epoch: 5012, Batch Gradient Norm after: 9.226492343536261
Epoch 5013/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4408245921134949
Epoch: 5013, Batch Gradient Norm: 10.067714209026901
Epoch: 5013, Batch Gradient Norm after: 10.067714209026901
Epoch 5014/10000, Prediction Accuracy = 61.914%, Loss = 0.44484712481498717
Epoch: 5014, Batch Gradient Norm: 8.573335325651962
Epoch: 5014, Batch Gradient Norm after: 8.573335325651962
Epoch 5015/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4370884358882904
Epoch: 5015, Batch Gradient Norm: 6.625521508526667
Epoch: 5015, Batch Gradient Norm after: 6.625521508526667
Epoch 5016/10000, Prediction Accuracy = 61.924%, Loss = 0.42376912832260133
Epoch: 5016, Batch Gradient Norm: 9.984782756326787
Epoch: 5016, Batch Gradient Norm after: 9.984782756326787
Epoch 5017/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.438548618555069
Epoch: 5017, Batch Gradient Norm: 15.792373580794425
Epoch: 5017, Batch Gradient Norm after: 15.792373580794425
Epoch 5018/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.49091888666152955
Epoch: 5018, Batch Gradient Norm: 10.873271529500022
Epoch: 5018, Batch Gradient Norm after: 10.873271529500022
Epoch 5019/10000, Prediction Accuracy = 61.968%, Loss = 0.449019068479538
Epoch: 5019, Batch Gradient Norm: 6.382536065056797
Epoch: 5019, Batch Gradient Norm after: 6.382536065056797
Epoch 5020/10000, Prediction Accuracy = 61.806%, Loss = 0.4221323072910309
Epoch: 5020, Batch Gradient Norm: 7.4369169759093285
Epoch: 5020, Batch Gradient Norm after: 7.4369169759093285
Epoch 5021/10000, Prediction Accuracy = 61.714%, Loss = 0.4281054675579071
Epoch: 5021, Batch Gradient Norm: 9.582422772591311
Epoch: 5021, Batch Gradient Norm after: 9.582422772591311
Epoch 5022/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4411167144775391
Epoch: 5022, Batch Gradient Norm: 10.279109565423731
Epoch: 5022, Batch Gradient Norm after: 10.279109565423731
Epoch 5023/10000, Prediction Accuracy = 61.964%, Loss = 0.44701839685440065
Epoch: 5023, Batch Gradient Norm: 8.953404951965952
Epoch: 5023, Batch Gradient Norm after: 8.953404951965952
Epoch 5024/10000, Prediction Accuracy = 61.886%, Loss = 0.43793305158615115
Epoch: 5024, Batch Gradient Norm: 9.382585522778728
Epoch: 5024, Batch Gradient Norm after: 9.382585522778728
Epoch 5025/10000, Prediction Accuracy = 61.94200000000001%, Loss = 0.43943147659301757
Epoch: 5025, Batch Gradient Norm: 13.904859022953048
Epoch: 5025, Batch Gradient Norm after: 13.904859022953048
Epoch 5026/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.48256402015686034
Epoch: 5026, Batch Gradient Norm: 7.975627080949208
Epoch: 5026, Batch Gradient Norm after: 7.975627080949208
Epoch 5027/10000, Prediction Accuracy = 62.104%, Loss = 0.4337370276451111
Epoch: 5027, Batch Gradient Norm: 10.36718716112604
Epoch: 5027, Batch Gradient Norm after: 10.36718716112604
Epoch 5028/10000, Prediction Accuracy = 62.076%, Loss = 0.44525040984153746
Epoch: 5028, Batch Gradient Norm: 11.633460386610125
Epoch: 5028, Batch Gradient Norm after: 11.633460386610125
Epoch 5029/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.4553873121738434
Epoch: 5029, Batch Gradient Norm: 11.510267004017065
Epoch: 5029, Batch Gradient Norm after: 11.510267004017065
Epoch 5030/10000, Prediction Accuracy = 61.99799999999999%, Loss = 0.4611076295375824
Epoch: 5030, Batch Gradient Norm: 10.933758997475216
Epoch: 5030, Batch Gradient Norm after: 10.933758997475216
Epoch 5031/10000, Prediction Accuracy = 61.94%, Loss = 0.45438682436943056
Epoch: 5031, Batch Gradient Norm: 6.242501644306187
Epoch: 5031, Batch Gradient Norm after: 6.242501644306187
Epoch 5032/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.42277517914772034
Epoch: 5032, Batch Gradient Norm: 6.9389607409638
Epoch: 5032, Batch Gradient Norm after: 6.9389607409638
Epoch 5033/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.4257419049739838
Epoch: 5033, Batch Gradient Norm: 9.39198048475437
Epoch: 5033, Batch Gradient Norm after: 9.39198048475437
Epoch 5034/10000, Prediction Accuracy = 61.872%, Loss = 0.44060021042823794
Epoch: 5034, Batch Gradient Norm: 11.418977829824888
Epoch: 5034, Batch Gradient Norm after: 11.418977829824888
Epoch 5035/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.45960153341293336
Epoch: 5035, Batch Gradient Norm: 11.7036495204099
Epoch: 5035, Batch Gradient Norm after: 11.7036495204099
Epoch 5036/10000, Prediction Accuracy = 61.8%, Loss = 0.45762030482292176
Epoch: 5036, Batch Gradient Norm: 8.786864048859094
Epoch: 5036, Batch Gradient Norm after: 8.786864048859094
Epoch 5037/10000, Prediction Accuracy = 61.952%, Loss = 0.43480350375175475
Epoch: 5037, Batch Gradient Norm: 8.722321610823613
Epoch: 5037, Batch Gradient Norm after: 8.722321610823613
Epoch 5038/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4333261549472809
Epoch: 5038, Batch Gradient Norm: 8.772781757259668
Epoch: 5038, Batch Gradient Norm after: 8.772781757259668
Epoch 5039/10000, Prediction Accuracy = 61.94000000000001%, Loss = 0.43398089408874513
Epoch: 5039, Batch Gradient Norm: 10.419164280182839
Epoch: 5039, Batch Gradient Norm after: 10.419164280182839
Epoch 5040/10000, Prediction Accuracy = 61.96%, Loss = 0.44416252970695497
Epoch: 5040, Batch Gradient Norm: 9.024446330996645
Epoch: 5040, Batch Gradient Norm after: 9.024446330996645
Epoch 5041/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.43681580424308775
Epoch: 5041, Batch Gradient Norm: 7.65650520008284
Epoch: 5041, Batch Gradient Norm after: 7.65650520008284
Epoch 5042/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.42808815836906433
Epoch: 5042, Batch Gradient Norm: 8.903599186698324
Epoch: 5042, Batch Gradient Norm after: 8.903599186698324
Epoch 5043/10000, Prediction Accuracy = 61.814%, Loss = 0.43616677522659303
Epoch: 5043, Batch Gradient Norm: 10.817874773257264
Epoch: 5043, Batch Gradient Norm after: 10.817874773257264
Epoch 5044/10000, Prediction Accuracy = 61.71%, Loss = 0.4543207049369812
Epoch: 5044, Batch Gradient Norm: 9.522575699165996
Epoch: 5044, Batch Gradient Norm after: 9.522575699165996
Epoch 5045/10000, Prediction Accuracy = 61.902%, Loss = 0.43878982663154603
Epoch: 5045, Batch Gradient Norm: 12.701548508753378
Epoch: 5045, Batch Gradient Norm after: 12.701548508753378
Epoch 5046/10000, Prediction Accuracy = 61.902%, Loss = 0.4624427378177643
Epoch: 5046, Batch Gradient Norm: 11.724904694313597
Epoch: 5046, Batch Gradient Norm after: 11.724904694313597
Epoch 5047/10000, Prediction Accuracy = 61.878%, Loss = 0.4538159191608429
Epoch: 5047, Batch Gradient Norm: 9.535874871281784
Epoch: 5047, Batch Gradient Norm after: 9.535874871281784
Epoch 5048/10000, Prediction Accuracy = 61.938%, Loss = 0.43747774362564085
Epoch: 5048, Batch Gradient Norm: 9.131070450749638
Epoch: 5048, Batch Gradient Norm after: 9.131070450749638
Epoch 5049/10000, Prediction Accuracy = 62.024%, Loss = 0.43668296933174133
Epoch: 5049, Batch Gradient Norm: 10.76967723883786
Epoch: 5049, Batch Gradient Norm after: 10.76967723883786
Epoch 5050/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.445723557472229
Epoch: 5050, Batch Gradient Norm: 8.093865364590009
Epoch: 5050, Batch Gradient Norm after: 8.093865364590009
Epoch 5051/10000, Prediction Accuracy = 61.938%, Loss = 0.4307838797569275
Epoch: 5051, Batch Gradient Norm: 7.628030366865163
Epoch: 5051, Batch Gradient Norm after: 7.628030366865163
Epoch 5052/10000, Prediction Accuracy = 61.952%, Loss = 0.42720577120780945
Epoch: 5052, Batch Gradient Norm: 9.985068116652169
Epoch: 5052, Batch Gradient Norm after: 9.985068116652169
Epoch 5053/10000, Prediction Accuracy = 61.886%, Loss = 0.4424674570560455
Epoch: 5053, Batch Gradient Norm: 8.570369576756663
Epoch: 5053, Batch Gradient Norm after: 8.570369576756663
Epoch 5054/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.4313868463039398
Epoch: 5054, Batch Gradient Norm: 10.874764726301091
Epoch: 5054, Batch Gradient Norm after: 10.874764726301091
Epoch 5055/10000, Prediction Accuracy = 61.932%, Loss = 0.4448751747608185
Epoch: 5055, Batch Gradient Norm: 13.30048175778819
Epoch: 5055, Batch Gradient Norm after: 13.30048175778819
Epoch 5056/10000, Prediction Accuracy = 61.912%, Loss = 0.4697630226612091
Epoch: 5056, Batch Gradient Norm: 8.757604382842954
Epoch: 5056, Batch Gradient Norm after: 8.757604382842954
Epoch 5057/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4378579318523407
Epoch: 5057, Batch Gradient Norm: 6.428691652340708
Epoch: 5057, Batch Gradient Norm after: 6.428691652340708
Epoch 5058/10000, Prediction Accuracy = 62.164%, Loss = 0.42193267345428465
Epoch: 5058, Batch Gradient Norm: 12.926230239837562
Epoch: 5058, Batch Gradient Norm after: 12.926230239837562
Epoch 5059/10000, Prediction Accuracy = 61.802%, Loss = 0.465164440870285
Epoch: 5059, Batch Gradient Norm: 12.663914189732129
Epoch: 5059, Batch Gradient Norm after: 12.663914189732129
Epoch 5060/10000, Prediction Accuracy = 61.884%, Loss = 0.46927117705345156
Epoch: 5060, Batch Gradient Norm: 10.358005386428632
Epoch: 5060, Batch Gradient Norm after: 10.358005386428632
Epoch 5061/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4475297689437866
Epoch: 5061, Batch Gradient Norm: 7.11011738889893
Epoch: 5061, Batch Gradient Norm after: 7.11011738889893
Epoch 5062/10000, Prediction Accuracy = 61.968%, Loss = 0.42570526003837583
Epoch: 5062, Batch Gradient Norm: 7.543585959369103
Epoch: 5062, Batch Gradient Norm after: 7.543585959369103
Epoch 5063/10000, Prediction Accuracy = 62.02%, Loss = 0.4277209758758545
Epoch: 5063, Batch Gradient Norm: 10.588608218851284
Epoch: 5063, Batch Gradient Norm after: 10.588608218851284
Epoch 5064/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.44634224772453307
Epoch: 5064, Batch Gradient Norm: 12.249565952571071
Epoch: 5064, Batch Gradient Norm after: 12.249565952571071
Epoch 5065/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.45460876226425173
Epoch: 5065, Batch Gradient Norm: 11.979737734266825
Epoch: 5065, Batch Gradient Norm after: 11.979737734266825
Epoch 5066/10000, Prediction Accuracy = 61.834%, Loss = 0.455829381942749
Epoch: 5066, Batch Gradient Norm: 9.250978635246579
Epoch: 5066, Batch Gradient Norm after: 9.250978635246579
Epoch 5067/10000, Prediction Accuracy = 61.976%, Loss = 0.43863036036491393
Epoch: 5067, Batch Gradient Norm: 8.451531653572436
Epoch: 5067, Batch Gradient Norm after: 8.451531653572436
Epoch 5068/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.43240479230880735
Epoch: 5068, Batch Gradient Norm: 7.821732666894853
Epoch: 5068, Batch Gradient Norm after: 7.821732666894853
Epoch 5069/10000, Prediction Accuracy = 62.028%, Loss = 0.4301283240318298
Epoch: 5069, Batch Gradient Norm: 8.941652753718518
Epoch: 5069, Batch Gradient Norm after: 8.941652753718518
Epoch 5070/10000, Prediction Accuracy = 61.952%, Loss = 0.44003982543945314
Epoch: 5070, Batch Gradient Norm: 8.344911391367905
Epoch: 5070, Batch Gradient Norm after: 8.344911391367905
Epoch 5071/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.4310004472732544
Epoch: 5071, Batch Gradient Norm: 9.838291177842267
Epoch: 5071, Batch Gradient Norm after: 9.838291177842267
Epoch 5072/10000, Prediction Accuracy = 61.854%, Loss = 0.4399080514907837
Epoch: 5072, Batch Gradient Norm: 10.56304213490866
Epoch: 5072, Batch Gradient Norm after: 10.56304213490866
Epoch 5073/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4449195623397827
Epoch: 5073, Batch Gradient Norm: 10.03496282708502
Epoch: 5073, Batch Gradient Norm after: 10.03496282708502
Epoch 5074/10000, Prediction Accuracy = 61.842%, Loss = 0.44133676290512086
Epoch: 5074, Batch Gradient Norm: 12.740102176508518
Epoch: 5074, Batch Gradient Norm after: 12.740102176508518
Epoch 5075/10000, Prediction Accuracy = 61.86%, Loss = 0.4640768229961395
Epoch: 5075, Batch Gradient Norm: 11.516159793049436
Epoch: 5075, Batch Gradient Norm after: 11.516159793049436
Epoch 5076/10000, Prediction Accuracy = 61.89%, Loss = 0.45611810088157656
Epoch: 5076, Batch Gradient Norm: 8.799557779376011
Epoch: 5076, Batch Gradient Norm after: 8.799557779376011
Epoch 5077/10000, Prediction Accuracy = 61.95%, Loss = 0.4390339970588684
Epoch: 5077, Batch Gradient Norm: 8.247439793691164
Epoch: 5077, Batch Gradient Norm after: 8.247439793691164
Epoch 5078/10000, Prediction Accuracy = 62.048%, Loss = 0.433639270067215
Epoch: 5078, Batch Gradient Norm: 9.582058748594353
Epoch: 5078, Batch Gradient Norm after: 9.582058748594353
Epoch 5079/10000, Prediction Accuracy = 61.984%, Loss = 0.441388338804245
Epoch: 5079, Batch Gradient Norm: 9.163722461851764
Epoch: 5079, Batch Gradient Norm after: 9.163722461851764
Epoch 5080/10000, Prediction Accuracy = 61.916%, Loss = 0.4370110988616943
Epoch: 5080, Batch Gradient Norm: 10.522017310106097
Epoch: 5080, Batch Gradient Norm after: 10.522017310106097
Epoch 5081/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4457564055919647
Epoch: 5081, Batch Gradient Norm: 9.961191049977105
Epoch: 5081, Batch Gradient Norm after: 9.961191049977105
Epoch 5082/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.44140794277191164
Epoch: 5082, Batch Gradient Norm: 9.818698845202581
Epoch: 5082, Batch Gradient Norm after: 9.818698845202581
Epoch 5083/10000, Prediction Accuracy = 61.98%, Loss = 0.4420314848423004
Epoch: 5083, Batch Gradient Norm: 10.310129569710947
Epoch: 5083, Batch Gradient Norm after: 10.310129569710947
Epoch 5084/10000, Prediction Accuracy = 62.028%, Loss = 0.44254729747772215
Epoch: 5084, Batch Gradient Norm: 11.525802637995143
Epoch: 5084, Batch Gradient Norm after: 11.525802637995143
Epoch 5085/10000, Prediction Accuracy = 61.91400000000001%, Loss = 0.45778862237930296
Epoch: 5085, Batch Gradient Norm: 8.377398760784319
Epoch: 5085, Batch Gradient Norm after: 8.377398760784319
Epoch 5086/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.43166932463645935
Epoch: 5086, Batch Gradient Norm: 9.698500784851483
Epoch: 5086, Batch Gradient Norm after: 9.698500784851483
Epoch 5087/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.44015839099884035
Epoch: 5087, Batch Gradient Norm: 7.447702303933187
Epoch: 5087, Batch Gradient Norm after: 7.447702303933187
Epoch 5088/10000, Prediction Accuracy = 62.084%, Loss = 0.4268553197383881
Epoch: 5088, Batch Gradient Norm: 8.007599158124954
Epoch: 5088, Batch Gradient Norm after: 8.007599158124954
Epoch 5089/10000, Prediction Accuracy = 61.852%, Loss = 0.42806564569473265
Epoch: 5089, Batch Gradient Norm: 11.724334926844657
Epoch: 5089, Batch Gradient Norm after: 11.724334926844657
Epoch 5090/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4489253580570221
Epoch: 5090, Batch Gradient Norm: 12.715527388030418
Epoch: 5090, Batch Gradient Norm after: 12.715527388030418
Epoch 5091/10000, Prediction Accuracy = 61.94000000000001%, Loss = 0.4630336344242096
Epoch: 5091, Batch Gradient Norm: 8.212553748873015
Epoch: 5091, Batch Gradient Norm after: 8.212553748873015
Epoch 5092/10000, Prediction Accuracy = 61.85%, Loss = 0.43321031928062437
Epoch: 5092, Batch Gradient Norm: 8.151702977291954
Epoch: 5092, Batch Gradient Norm after: 8.151702977291954
Epoch 5093/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.43173774480819704
Epoch: 5093, Batch Gradient Norm: 11.26258022176653
Epoch: 5093, Batch Gradient Norm after: 11.26258022176653
Epoch 5094/10000, Prediction Accuracy = 61.96%, Loss = 0.45365228652954104
Epoch: 5094, Batch Gradient Norm: 10.610293278669939
Epoch: 5094, Batch Gradient Norm after: 10.610293278669939
Epoch 5095/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4509926438331604
Epoch: 5095, Batch Gradient Norm: 8.58265992097176
Epoch: 5095, Batch Gradient Norm after: 8.58265992097176
Epoch 5096/10000, Prediction Accuracy = 61.89399999999999%, Loss = 0.4349565804004669
Epoch: 5096, Batch Gradient Norm: 9.447081443726816
Epoch: 5096, Batch Gradient Norm after: 9.447081443726816
Epoch 5097/10000, Prediction Accuracy = 61.982000000000006%, Loss = 0.43781731128692625
Epoch: 5097, Batch Gradient Norm: 9.332373230745842
Epoch: 5097, Batch Gradient Norm after: 9.332373230745842
Epoch 5098/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4385648012161255
Epoch: 5098, Batch Gradient Norm: 7.8520078434353175
Epoch: 5098, Batch Gradient Norm after: 7.8520078434353175
Epoch 5099/10000, Prediction Accuracy = 61.89399999999999%, Loss = 0.42710731029510496
Epoch: 5099, Batch Gradient Norm: 10.343828253892626
Epoch: 5099, Batch Gradient Norm after: 10.343828253892626
Epoch 5100/10000, Prediction Accuracy = 61.94%, Loss = 0.4450937151908875
Epoch: 5100, Batch Gradient Norm: 12.089176496254698
Epoch: 5100, Batch Gradient Norm after: 12.089176496254698
Epoch 5101/10000, Prediction Accuracy = 62.024%, Loss = 0.4584649085998535
Epoch: 5101, Batch Gradient Norm: 11.071006984061876
Epoch: 5101, Batch Gradient Norm after: 11.071006984061876
Epoch 5102/10000, Prediction Accuracy = 61.96%, Loss = 0.4485379457473755
Epoch: 5102, Batch Gradient Norm: 11.40038326650536
Epoch: 5102, Batch Gradient Norm after: 11.40038326650536
Epoch 5103/10000, Prediction Accuracy = 61.962%, Loss = 0.4519950568675995
Epoch: 5103, Batch Gradient Norm: 8.157487737440372
Epoch: 5103, Batch Gradient Norm after: 8.157487737440372
Epoch 5104/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.4286005437374115
Epoch: 5104, Batch Gradient Norm: 10.409415311283187
Epoch: 5104, Batch Gradient Norm after: 10.409415311283187
Epoch 5105/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.442208343744278
Epoch: 5105, Batch Gradient Norm: 11.296504022299338
Epoch: 5105, Batch Gradient Norm after: 11.296504022299338
Epoch 5106/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.44813758730888364
Epoch: 5106, Batch Gradient Norm: 10.111274953460072
Epoch: 5106, Batch Gradient Norm after: 10.111274953460072
Epoch 5107/10000, Prediction Accuracy = 61.938%, Loss = 0.4477375328540802
Epoch: 5107, Batch Gradient Norm: 8.350676329086454
Epoch: 5107, Batch Gradient Norm after: 8.350676329086454
Epoch 5108/10000, Prediction Accuracy = 61.926%, Loss = 0.433332622051239
Epoch: 5108, Batch Gradient Norm: 8.503236953752337
Epoch: 5108, Batch Gradient Norm after: 8.503236953752337
Epoch 5109/10000, Prediction Accuracy = 61.884%, Loss = 0.43120546340942384
Epoch: 5109, Batch Gradient Norm: 7.211085061086211
Epoch: 5109, Batch Gradient Norm after: 7.211085061086211
Epoch 5110/10000, Prediction Accuracy = 62.056%, Loss = 0.42368640303611754
Epoch: 5110, Batch Gradient Norm: 8.328759508517267
Epoch: 5110, Batch Gradient Norm after: 8.328759508517267
Epoch 5111/10000, Prediction Accuracy = 62.016000000000005%, Loss = 0.42981197834014895
Epoch: 5111, Batch Gradient Norm: 10.973959254541628
Epoch: 5111, Batch Gradient Norm after: 10.973959254541628
Epoch 5112/10000, Prediction Accuracy = 61.834%, Loss = 0.44793154001235963
Epoch: 5112, Batch Gradient Norm: 10.094081359161565
Epoch: 5112, Batch Gradient Norm after: 10.094081359161565
Epoch 5113/10000, Prediction Accuracy = 61.878%, Loss = 0.44181976318359373
Epoch: 5113, Batch Gradient Norm: 7.6204850924317755
Epoch: 5113, Batch Gradient Norm after: 7.6204850924317755
Epoch 5114/10000, Prediction Accuracy = 61.952%, Loss = 0.42641252279281616
Epoch: 5114, Batch Gradient Norm: 9.150776094730395
Epoch: 5114, Batch Gradient Norm after: 9.150776094730395
Epoch 5115/10000, Prediction Accuracy = 62.04%, Loss = 0.4363418400287628
Epoch: 5115, Batch Gradient Norm: 9.453195796609002
Epoch: 5115, Batch Gradient Norm after: 9.453195796609002
Epoch 5116/10000, Prediction Accuracy = 62.016%, Loss = 0.43746410608291625
Epoch: 5116, Batch Gradient Norm: 11.13324616434534
Epoch: 5116, Batch Gradient Norm after: 11.13324616434534
Epoch 5117/10000, Prediction Accuracy = 61.874%, Loss = 0.4484426498413086
Epoch: 5117, Batch Gradient Norm: 8.032843433500153
Epoch: 5117, Batch Gradient Norm after: 8.032843433500153
Epoch 5118/10000, Prediction Accuracy = 62.008%, Loss = 0.4272493779659271
Epoch: 5118, Batch Gradient Norm: 12.910160818444448
Epoch: 5118, Batch Gradient Norm after: 12.910160818444448
Epoch 5119/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4636183798313141
Epoch: 5119, Batch Gradient Norm: 13.096790259211804
Epoch: 5119, Batch Gradient Norm after: 13.096790259211804
Epoch 5120/10000, Prediction Accuracy = 61.878%, Loss = 0.4681138157844543
Epoch: 5120, Batch Gradient Norm: 8.960388025008307
Epoch: 5120, Batch Gradient Norm after: 8.960388025008307
Epoch 5121/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.4333042323589325
Epoch: 5121, Batch Gradient Norm: 12.074451673117606
Epoch: 5121, Batch Gradient Norm after: 12.074451673117606
Epoch 5122/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.47082950472831725
Epoch: 5122, Batch Gradient Norm: 7.065366895678026
Epoch: 5122, Batch Gradient Norm after: 7.065366895678026
Epoch 5123/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4245318710803986
Epoch: 5123, Batch Gradient Norm: 8.700765144020934
Epoch: 5123, Batch Gradient Norm after: 8.700765144020934
Epoch 5124/10000, Prediction Accuracy = 62.004%, Loss = 0.4339082360267639
Epoch: 5124, Batch Gradient Norm: 9.501215129006845
Epoch: 5124, Batch Gradient Norm after: 9.501215129006845
Epoch 5125/10000, Prediction Accuracy = 61.962%, Loss = 0.43980074524879453
Epoch: 5125, Batch Gradient Norm: 7.488909320338443
Epoch: 5125, Batch Gradient Norm after: 7.488909320338443
Epoch 5126/10000, Prediction Accuracy = 62.05%, Loss = 0.4261214375495911
Epoch: 5126, Batch Gradient Norm: 7.673870320356956
Epoch: 5126, Batch Gradient Norm after: 7.673870320356956
Epoch 5127/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.4264775276184082
Epoch: 5127, Batch Gradient Norm: 8.440938271600283
Epoch: 5127, Batch Gradient Norm after: 8.440938271600283
Epoch 5128/10000, Prediction Accuracy = 62.032%, Loss = 0.4317110002040863
Epoch: 5128, Batch Gradient Norm: 10.006548240797995
Epoch: 5128, Batch Gradient Norm after: 10.006548240797995
Epoch 5129/10000, Prediction Accuracy = 61.924%, Loss = 0.43920776844024656
Epoch: 5129, Batch Gradient Norm: 13.25530993832332
Epoch: 5129, Batch Gradient Norm after: 13.25530993832332
Epoch 5130/10000, Prediction Accuracy = 61.988%, Loss = 0.4630686044692993
Epoch: 5130, Batch Gradient Norm: 10.60247544713747
Epoch: 5130, Batch Gradient Norm after: 10.60247544713747
Epoch 5131/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.44374641180038454
Epoch: 5131, Batch Gradient Norm: 9.173967220514195
Epoch: 5131, Batch Gradient Norm after: 9.173967220514195
Epoch 5132/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.436674702167511
Epoch: 5132, Batch Gradient Norm: 10.563142858503264
Epoch: 5132, Batch Gradient Norm after: 10.563142858503264
Epoch 5133/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.443936687707901
Epoch: 5133, Batch Gradient Norm: 12.302253479829773
Epoch: 5133, Batch Gradient Norm after: 12.302253479829773
Epoch 5134/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.4585909128189087
Epoch: 5134, Batch Gradient Norm: 9.711182724170467
Epoch: 5134, Batch Gradient Norm after: 9.711182724170467
Epoch 5135/10000, Prediction Accuracy = 62.06%, Loss = 0.4410476326942444
Epoch: 5135, Batch Gradient Norm: 7.288385515311613
Epoch: 5135, Batch Gradient Norm after: 7.288385515311613
Epoch 5136/10000, Prediction Accuracy = 62.036%, Loss = 0.42633634209632876
Epoch: 5136, Batch Gradient Norm: 6.2267906483991995
Epoch: 5136, Batch Gradient Norm after: 6.2267906483991995
Epoch 5137/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.42023800015449525
Epoch: 5137, Batch Gradient Norm: 8.922313005936157
Epoch: 5137, Batch Gradient Norm after: 8.922313005936157
Epoch 5138/10000, Prediction Accuracy = 62.024%, Loss = 0.43369539380073546
Epoch: 5138, Batch Gradient Norm: 9.817879145418695
Epoch: 5138, Batch Gradient Norm after: 9.817879145418695
Epoch 5139/10000, Prediction Accuracy = 61.852%, Loss = 0.440467768907547
Epoch: 5139, Batch Gradient Norm: 9.595161403161985
Epoch: 5139, Batch Gradient Norm after: 9.595161403161985
Epoch 5140/10000, Prediction Accuracy = 61.936%, Loss = 0.43812793493270874
Epoch: 5140, Batch Gradient Norm: 11.169559127280717
Epoch: 5140, Batch Gradient Norm after: 11.169559127280717
Epoch 5141/10000, Prediction Accuracy = 61.996%, Loss = 0.4447693586349487
Epoch: 5141, Batch Gradient Norm: 10.622961424555985
Epoch: 5141, Batch Gradient Norm after: 10.622961424555985
Epoch 5142/10000, Prediction Accuracy = 61.996%, Loss = 0.4422959744930267
Epoch: 5142, Batch Gradient Norm: 11.203200094828762
Epoch: 5142, Batch Gradient Norm after: 11.203200094828762
Epoch 5143/10000, Prediction Accuracy = 61.96%, Loss = 0.4466949462890625
Epoch: 5143, Batch Gradient Norm: 8.900402339330585
Epoch: 5143, Batch Gradient Norm after: 8.900402339330585
Epoch 5144/10000, Prediction Accuracy = 61.952%, Loss = 0.43561251163482667
Epoch: 5144, Batch Gradient Norm: 8.351514691100894
Epoch: 5144, Batch Gradient Norm after: 8.351514691100894
Epoch 5145/10000, Prediction Accuracy = 61.836%, Loss = 0.43149014115333556
Epoch: 5145, Batch Gradient Norm: 8.82333295208196
Epoch: 5145, Batch Gradient Norm after: 8.82333295208196
Epoch 5146/10000, Prediction Accuracy = 62.088%, Loss = 0.4329409122467041
Epoch: 5146, Batch Gradient Norm: 12.479771920733775
Epoch: 5146, Batch Gradient Norm after: 12.479771920733775
Epoch 5147/10000, Prediction Accuracy = 61.996%, Loss = 0.45989225506782533
Epoch: 5147, Batch Gradient Norm: 10.76231245616461
Epoch: 5147, Batch Gradient Norm after: 10.76231245616461
Epoch 5148/10000, Prediction Accuracy = 61.898%, Loss = 0.444115948677063
Epoch: 5148, Batch Gradient Norm: 8.32940261149464
Epoch: 5148, Batch Gradient Norm after: 8.32940261149464
Epoch 5149/10000, Prediction Accuracy = 61.93399999999999%, Loss = 0.42950775027275084
Epoch: 5149, Batch Gradient Norm: 11.403733916011628
Epoch: 5149, Batch Gradient Norm after: 11.403733916011628
Epoch 5150/10000, Prediction Accuracy = 61.926%, Loss = 0.44951832890510557
Epoch: 5150, Batch Gradient Norm: 11.754678063915811
Epoch: 5150, Batch Gradient Norm after: 11.754678063915811
Epoch 5151/10000, Prediction Accuracy = 62.028%, Loss = 0.455423104763031
Epoch: 5151, Batch Gradient Norm: 9.300542026712492
Epoch: 5151, Batch Gradient Norm after: 9.300542026712492
Epoch 5152/10000, Prediction Accuracy = 61.98%, Loss = 0.43796381950378416
Epoch: 5152, Batch Gradient Norm: 8.640164046525783
Epoch: 5152, Batch Gradient Norm after: 8.640164046525783
Epoch 5153/10000, Prediction Accuracy = 61.996%, Loss = 0.42986935973167417
Epoch: 5153, Batch Gradient Norm: 7.654123246836919
Epoch: 5153, Batch Gradient Norm after: 7.654123246836919
Epoch 5154/10000, Prediction Accuracy = 62.076%, Loss = 0.4259174942970276
Epoch: 5154, Batch Gradient Norm: 8.89934584153791
Epoch: 5154, Batch Gradient Norm after: 8.89934584153791
Epoch 5155/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.4310468196868896
Epoch: 5155, Batch Gradient Norm: 11.133582039312913
Epoch: 5155, Batch Gradient Norm after: 11.133582039312913
Epoch 5156/10000, Prediction Accuracy = 62.072%, Loss = 0.44747923612594603
Epoch: 5156, Batch Gradient Norm: 11.212786274227733
Epoch: 5156, Batch Gradient Norm after: 11.212786274227733
Epoch 5157/10000, Prediction Accuracy = 61.964%, Loss = 0.4511841297149658
Epoch: 5157, Batch Gradient Norm: 9.014822036715694
Epoch: 5157, Batch Gradient Norm after: 9.014822036715694
Epoch 5158/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4352802217006683
Epoch: 5158, Batch Gradient Norm: 11.666581885316102
Epoch: 5158, Batch Gradient Norm after: 11.666581885316102
Epoch 5159/10000, Prediction Accuracy = 61.916%, Loss = 0.4572285532951355
Epoch: 5159, Batch Gradient Norm: 9.476931639609733
Epoch: 5159, Batch Gradient Norm after: 9.476931639609733
Epoch 5160/10000, Prediction Accuracy = 61.98%, Loss = 0.43628392815589906
Epoch: 5160, Batch Gradient Norm: 8.851544523669723
Epoch: 5160, Batch Gradient Norm after: 8.851544523669723
Epoch 5161/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.4346601068973541
Epoch: 5161, Batch Gradient Norm: 8.693905911942236
Epoch: 5161, Batch Gradient Norm after: 8.693905911942236
Epoch 5162/10000, Prediction Accuracy = 61.884%, Loss = 0.4312510251998901
Epoch: 5162, Batch Gradient Norm: 10.656918514679676
Epoch: 5162, Batch Gradient Norm after: 10.656918514679676
Epoch 5163/10000, Prediction Accuracy = 62.012%, Loss = 0.44324970841407774
Epoch: 5163, Batch Gradient Norm: 9.230018867138728
Epoch: 5163, Batch Gradient Norm after: 9.230018867138728
Epoch 5164/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.43315168023109435
Epoch: 5164, Batch Gradient Norm: 9.480850838845633
Epoch: 5164, Batch Gradient Norm after: 9.480850838845633
Epoch 5165/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.43428573608398435
Epoch: 5165, Batch Gradient Norm: 10.475732618502033
Epoch: 5165, Batch Gradient Norm after: 10.475732618502033
Epoch 5166/10000, Prediction Accuracy = 61.914%, Loss = 0.44197251796722414
Epoch: 5166, Batch Gradient Norm: 9.547520823096514
Epoch: 5166, Batch Gradient Norm after: 9.547520823096514
Epoch 5167/10000, Prediction Accuracy = 61.984%, Loss = 0.4370395541191101
Epoch: 5167, Batch Gradient Norm: 8.944595732534532
Epoch: 5167, Batch Gradient Norm after: 8.944595732534532
Epoch 5168/10000, Prediction Accuracy = 61.952%, Loss = 0.4325151860713959
Epoch: 5168, Batch Gradient Norm: 10.032809141347004
Epoch: 5168, Batch Gradient Norm after: 10.032809141347004
Epoch 5169/10000, Prediction Accuracy = 61.936%, Loss = 0.43913530707359316
Epoch: 5169, Batch Gradient Norm: 8.926001093041144
Epoch: 5169, Batch Gradient Norm after: 8.926001093041144
Epoch 5170/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.431663578748703
Epoch: 5170, Batch Gradient Norm: 7.997031634435792
Epoch: 5170, Batch Gradient Norm after: 7.997031634435792
Epoch 5171/10000, Prediction Accuracy = 62.024%, Loss = 0.428385066986084
Epoch: 5171, Batch Gradient Norm: 10.498954502921086
Epoch: 5171, Batch Gradient Norm after: 10.498954502921086
Epoch 5172/10000, Prediction Accuracy = 61.89399999999999%, Loss = 0.442468935251236
Epoch: 5172, Batch Gradient Norm: 11.45665230712085
Epoch: 5172, Batch Gradient Norm after: 11.45665230712085
Epoch 5173/10000, Prediction Accuracy = 61.9%, Loss = 0.4492204308509827
Epoch: 5173, Batch Gradient Norm: 10.661612045782851
Epoch: 5173, Batch Gradient Norm after: 10.661612045782851
Epoch 5174/10000, Prediction Accuracy = 62.05%, Loss = 0.44284805059432986
Epoch: 5174, Batch Gradient Norm: 10.352870241316953
Epoch: 5174, Batch Gradient Norm after: 10.352870241316953
Epoch 5175/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.442502361536026
Epoch: 5175, Batch Gradient Norm: 12.975978015175818
Epoch: 5175, Batch Gradient Norm after: 12.975978015175818
Epoch 5176/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.4686468720436096
Epoch: 5176, Batch Gradient Norm: 9.192489008718024
Epoch: 5176, Batch Gradient Norm after: 9.192489008718024
Epoch 5177/10000, Prediction Accuracy = 61.96999999999999%, Loss = 0.4364116847515106
Epoch: 5177, Batch Gradient Norm: 8.393636233219464
Epoch: 5177, Batch Gradient Norm after: 8.393636233219464
Epoch 5178/10000, Prediction Accuracy = 62.03399999999999%, Loss = 0.43151617646217344
Epoch: 5178, Batch Gradient Norm: 8.377345798702578
Epoch: 5178, Batch Gradient Norm after: 8.377345798702578
Epoch 5179/10000, Prediction Accuracy = 62.14%, Loss = 0.4289062857627869
Epoch: 5179, Batch Gradient Norm: 10.219711197310792
Epoch: 5179, Batch Gradient Norm after: 10.219711197310792
Epoch 5180/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4377449870109558
Epoch: 5180, Batch Gradient Norm: 11.112022222822278
Epoch: 5180, Batch Gradient Norm after: 11.112022222822278
Epoch 5181/10000, Prediction Accuracy = 62.011999999999986%, Loss = 0.4464599132537842
Epoch: 5181, Batch Gradient Norm: 11.507005952106233
Epoch: 5181, Batch Gradient Norm after: 11.507005952106233
Epoch 5182/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.4524188697338104
Epoch: 5182, Batch Gradient Norm: 9.45414650944032
Epoch: 5182, Batch Gradient Norm after: 9.45414650944032
Epoch 5183/10000, Prediction Accuracy = 61.972%, Loss = 0.43565478920936584
Epoch: 5183, Batch Gradient Norm: 6.898419805119705
Epoch: 5183, Batch Gradient Norm after: 6.898419805119705
Epoch 5184/10000, Prediction Accuracy = 61.972%, Loss = 0.4202722907066345
Epoch: 5184, Batch Gradient Norm: 10.132637644221273
Epoch: 5184, Batch Gradient Norm after: 10.132637644221273
Epoch 5185/10000, Prediction Accuracy = 61.902%, Loss = 0.44229971766471865
Epoch: 5185, Batch Gradient Norm: 11.856155511974661
Epoch: 5185, Batch Gradient Norm after: 11.856155511974661
Epoch 5186/10000, Prediction Accuracy = 61.822%, Loss = 0.45404171347618105
Epoch: 5186, Batch Gradient Norm: 9.296823173484512
Epoch: 5186, Batch Gradient Norm after: 9.296823173484512
Epoch 5187/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.4328422248363495
Epoch: 5187, Batch Gradient Norm: 11.436592709543481
Epoch: 5187, Batch Gradient Norm after: 11.436592709543481
Epoch 5188/10000, Prediction Accuracy = 61.952%, Loss = 0.45099427700042727
Epoch: 5188, Batch Gradient Norm: 9.954573120818148
Epoch: 5188, Batch Gradient Norm after: 9.954573120818148
Epoch 5189/10000, Prediction Accuracy = 62.024%, Loss = 0.43934345841407774
Epoch: 5189, Batch Gradient Norm: 11.660815063404963
Epoch: 5189, Batch Gradient Norm after: 11.660815063404963
Epoch 5190/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.45812107920646666
Epoch: 5190, Batch Gradient Norm: 5.890854922740204
Epoch: 5190, Batch Gradient Norm after: 5.890854922740204
Epoch 5191/10000, Prediction Accuracy = 61.976%, Loss = 0.4188319742679596
Epoch: 5191, Batch Gradient Norm: 8.638451754470006
Epoch: 5191, Batch Gradient Norm after: 8.638451754470006
Epoch 5192/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.43125189542770387
Epoch: 5192, Batch Gradient Norm: 9.03292282610663
Epoch: 5192, Batch Gradient Norm after: 9.03292282610663
Epoch 5193/10000, Prediction Accuracy = 61.9%, Loss = 0.43275312185287473
Epoch: 5193, Batch Gradient Norm: 9.929919752935023
Epoch: 5193, Batch Gradient Norm after: 9.929919752935023
Epoch 5194/10000, Prediction Accuracy = 61.972%, Loss = 0.4399367690086365
Epoch: 5194, Batch Gradient Norm: 10.635765263443332
Epoch: 5194, Batch Gradient Norm after: 10.635765263443332
Epoch 5195/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.4460932970046997
Epoch: 5195, Batch Gradient Norm: 8.144984948448245
Epoch: 5195, Batch Gradient Norm after: 8.144984948448245
Epoch 5196/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.42786659598350524
Epoch: 5196, Batch Gradient Norm: 8.582556847924964
Epoch: 5196, Batch Gradient Norm after: 8.582556847924964
Epoch 5197/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.42987709045410155
Epoch: 5197, Batch Gradient Norm: 8.260322636049505
Epoch: 5197, Batch Gradient Norm after: 8.260322636049505
Epoch 5198/10000, Prediction Accuracy = 61.964%, Loss = 0.42664519548416135
Epoch: 5198, Batch Gradient Norm: 10.029336128463187
Epoch: 5198, Batch Gradient Norm after: 10.029336128463187
Epoch 5199/10000, Prediction Accuracy = 62.088%, Loss = 0.43659825921058654
Epoch: 5199, Batch Gradient Norm: 10.805500770883263
Epoch: 5199, Batch Gradient Norm after: 10.805500770883263
Epoch 5200/10000, Prediction Accuracy = 61.972%, Loss = 0.4457390785217285
Epoch: 5200, Batch Gradient Norm: 9.531874468567207
Epoch: 5200, Batch Gradient Norm after: 9.531874468567207
Epoch 5201/10000, Prediction Accuracy = 62.048%, Loss = 0.43951395750045774
Epoch: 5201, Batch Gradient Norm: 9.691063859764036
Epoch: 5201, Batch Gradient Norm after: 9.691063859764036
Epoch 5202/10000, Prediction Accuracy = 61.926%, Loss = 0.4381314396858215
Epoch: 5202, Batch Gradient Norm: 10.958662810725029
Epoch: 5202, Batch Gradient Norm after: 10.958662810725029
Epoch 5203/10000, Prediction Accuracy = 61.886%, Loss = 0.4457439064979553
Epoch: 5203, Batch Gradient Norm: 10.189972537169545
Epoch: 5203, Batch Gradient Norm after: 10.189972537169545
Epoch 5204/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4400600492954254
Epoch: 5204, Batch Gradient Norm: 9.173821993008515
Epoch: 5204, Batch Gradient Norm after: 9.173821993008515
Epoch 5205/10000, Prediction Accuracy = 61.976%, Loss = 0.43120288848876953
Epoch: 5205, Batch Gradient Norm: 11.465471594059142
Epoch: 5205, Batch Gradient Norm after: 11.465471594059142
Epoch 5206/10000, Prediction Accuracy = 62.116%, Loss = 0.44546698331832885
Epoch: 5206, Batch Gradient Norm: 8.060849368440158
Epoch: 5206, Batch Gradient Norm after: 8.060849368440158
Epoch 5207/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.42550642490386964
Epoch: 5207, Batch Gradient Norm: 6.902660448732256
Epoch: 5207, Batch Gradient Norm after: 6.902660448732256
Epoch 5208/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4213561058044434
Epoch: 5208, Batch Gradient Norm: 7.8995275942452166
Epoch: 5208, Batch Gradient Norm after: 7.8995275942452166
Epoch 5209/10000, Prediction Accuracy = 62.076%, Loss = 0.4270534873008728
Epoch: 5209, Batch Gradient Norm: 9.629077997041987
Epoch: 5209, Batch Gradient Norm after: 9.629077997041987
Epoch 5210/10000, Prediction Accuracy = 61.983999999999995%, Loss = 0.4342050313949585
Epoch: 5210, Batch Gradient Norm: 9.490213636204206
Epoch: 5210, Batch Gradient Norm after: 9.490213636204206
Epoch 5211/10000, Prediction Accuracy = 61.962%, Loss = 0.4334297478199005
Epoch: 5211, Batch Gradient Norm: 10.651374835448253
Epoch: 5211, Batch Gradient Norm after: 10.651374835448253
Epoch 5212/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.44386318922042844
Epoch: 5212, Batch Gradient Norm: 10.512276905818188
Epoch: 5212, Batch Gradient Norm after: 10.512276905818188
Epoch 5213/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.4457326114177704
Epoch: 5213, Batch Gradient Norm: 9.473429953021068
Epoch: 5213, Batch Gradient Norm after: 9.473429953021068
Epoch 5214/10000, Prediction Accuracy = 62.004%, Loss = 0.43551616072654725
Epoch: 5214, Batch Gradient Norm: 11.210816165615375
Epoch: 5214, Batch Gradient Norm after: 11.210816165615375
Epoch 5215/10000, Prediction Accuracy = 61.964%, Loss = 0.44649245738983157
Epoch: 5215, Batch Gradient Norm: 10.214566909210701
Epoch: 5215, Batch Gradient Norm after: 10.214566909210701
Epoch 5216/10000, Prediction Accuracy = 61.996%, Loss = 0.4381078541278839
Epoch: 5216, Batch Gradient Norm: 10.09201353666812
Epoch: 5216, Batch Gradient Norm after: 10.09201353666812
Epoch 5217/10000, Prediction Accuracy = 62.012%, Loss = 0.43963085412979125
Epoch: 5217, Batch Gradient Norm: 12.081370158759833
Epoch: 5217, Batch Gradient Norm after: 12.081370158759833
Epoch 5218/10000, Prediction Accuracy = 61.89200000000001%, Loss = 0.46397038698196413
Epoch: 5218, Batch Gradient Norm: 6.421785926965008
Epoch: 5218, Batch Gradient Norm after: 6.421785926965008
Epoch 5219/10000, Prediction Accuracy = 62.056%, Loss = 0.4177186608314514
Epoch: 5219, Batch Gradient Norm: 9.292936487001187
Epoch: 5219, Batch Gradient Norm after: 9.292936487001187
Epoch 5220/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4353575348854065
Epoch: 5220, Batch Gradient Norm: 10.614498108676017
Epoch: 5220, Batch Gradient Norm after: 10.614498108676017
Epoch 5221/10000, Prediction Accuracy = 61.932%, Loss = 0.44494385123252866
Epoch: 5221, Batch Gradient Norm: 7.398672290430768
Epoch: 5221, Batch Gradient Norm after: 7.398672290430768
Epoch 5222/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4222460389137268
Epoch: 5222, Batch Gradient Norm: 8.149997396700762
Epoch: 5222, Batch Gradient Norm after: 8.149997396700762
Epoch 5223/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.4272005558013916
Epoch: 5223, Batch Gradient Norm: 9.300341559130333
Epoch: 5223, Batch Gradient Norm after: 9.300341559130333
Epoch 5224/10000, Prediction Accuracy = 61.938%, Loss = 0.4331160008907318
Epoch: 5224, Batch Gradient Norm: 14.267916018106888
Epoch: 5224, Batch Gradient Norm after: 14.267916018106888
Epoch 5225/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.4758787453174591
Epoch: 5225, Batch Gradient Norm: 12.284338053563777
Epoch: 5225, Batch Gradient Norm after: 12.284338053563777
Epoch 5226/10000, Prediction Accuracy = 61.84599999999999%, Loss = 0.4580127477645874
Epoch: 5226, Batch Gradient Norm: 8.907131970694364
Epoch: 5226, Batch Gradient Norm after: 8.907131970694364
Epoch 5227/10000, Prediction Accuracy = 62.088%, Loss = 0.43159270882606504
Epoch: 5227, Batch Gradient Norm: 8.959410192203235
Epoch: 5227, Batch Gradient Norm after: 8.959410192203235
Epoch 5228/10000, Prediction Accuracy = 62.032000000000004%, Loss = 0.43433953523635865
Epoch: 5228, Batch Gradient Norm: 10.29527877320507
Epoch: 5228, Batch Gradient Norm after: 10.29527877320507
Epoch 5229/10000, Prediction Accuracy = 62.038%, Loss = 0.44496755599975585
Epoch: 5229, Batch Gradient Norm: 11.434706680692607
Epoch: 5229, Batch Gradient Norm after: 11.434706680692607
Epoch 5230/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.45156997442245483
Epoch: 5230, Batch Gradient Norm: 8.072689854316963
Epoch: 5230, Batch Gradient Norm after: 8.072689854316963
Epoch 5231/10000, Prediction Accuracy = 62.1%, Loss = 0.425309944152832
Epoch: 5231, Batch Gradient Norm: 9.396621583924532
Epoch: 5231, Batch Gradient Norm after: 9.396621583924532
Epoch 5232/10000, Prediction Accuracy = 61.948%, Loss = 0.43267700672149656
Epoch: 5232, Batch Gradient Norm: 9.012341439533287
Epoch: 5232, Batch Gradient Norm after: 9.012341439533287
Epoch 5233/10000, Prediction Accuracy = 62.062%, Loss = 0.43082135915756226
Epoch: 5233, Batch Gradient Norm: 8.861559372305942
Epoch: 5233, Batch Gradient Norm after: 8.861559372305942
Epoch 5234/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.429219514131546
Epoch: 5234, Batch Gradient Norm: 9.569925826622931
Epoch: 5234, Batch Gradient Norm after: 9.569925826622931
Epoch 5235/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4374012291431427
Epoch: 5235, Batch Gradient Norm: 9.020878128784933
Epoch: 5235, Batch Gradient Norm after: 9.020878128784933
Epoch 5236/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.43440174460411074
Epoch: 5236, Batch Gradient Norm: 11.049883529465546
Epoch: 5236, Batch Gradient Norm after: 11.049883529465546
Epoch 5237/10000, Prediction Accuracy = 61.932%, Loss = 0.4518423736095428
Epoch: 5237, Batch Gradient Norm: 10.26157917329982
Epoch: 5237, Batch Gradient Norm after: 10.26157917329982
Epoch 5238/10000, Prediction Accuracy = 62.064%, Loss = 0.4441528916358948
Epoch: 5238, Batch Gradient Norm: 6.951670314406011
Epoch: 5238, Batch Gradient Norm after: 6.951670314406011
Epoch 5239/10000, Prediction Accuracy = 62.064%, Loss = 0.419322007894516
Epoch: 5239, Batch Gradient Norm: 9.999527836393813
Epoch: 5239, Batch Gradient Norm after: 9.999527836393813
Epoch 5240/10000, Prediction Accuracy = 62.128%, Loss = 0.43638224005699155
Epoch: 5240, Batch Gradient Norm: 11.039874183340222
Epoch: 5240, Batch Gradient Norm after: 11.039874183340222
Epoch 5241/10000, Prediction Accuracy = 61.83%, Loss = 0.44571671485900877
Epoch: 5241, Batch Gradient Norm: 10.045497979960393
Epoch: 5241, Batch Gradient Norm after: 10.045497979960393
Epoch 5242/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.43780653476715087
Epoch: 5242, Batch Gradient Norm: 9.232502816303793
Epoch: 5242, Batch Gradient Norm after: 9.232502816303793
Epoch 5243/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.43108507990837097
Epoch: 5243, Batch Gradient Norm: 9.703289257943315
Epoch: 5243, Batch Gradient Norm after: 9.703289257943315
Epoch 5244/10000, Prediction Accuracy = 62.126%, Loss = 0.4349792838096619
Epoch: 5244, Batch Gradient Norm: 10.04346254580699
Epoch: 5244, Batch Gradient Norm after: 10.04346254580699
Epoch 5245/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.4366439342498779
Epoch: 5245, Batch Gradient Norm: 9.065646157980265
Epoch: 5245, Batch Gradient Norm after: 9.065646157980265
Epoch 5246/10000, Prediction Accuracy = 62.08%, Loss = 0.429379403591156
Epoch: 5246, Batch Gradient Norm: 10.07952429565874
Epoch: 5246, Batch Gradient Norm after: 10.07952429565874
Epoch 5247/10000, Prediction Accuracy = 61.924%, Loss = 0.438321453332901
Epoch: 5247, Batch Gradient Norm: 11.008231837586543
Epoch: 5247, Batch Gradient Norm after: 11.008231837586543
Epoch 5248/10000, Prediction Accuracy = 62.064%, Loss = 0.4473810732364655
Epoch: 5248, Batch Gradient Norm: 12.683714789637381
Epoch: 5248, Batch Gradient Norm after: 12.683714789637381
Epoch 5249/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.46369199752807616
Epoch: 5249, Batch Gradient Norm: 8.128311555756392
Epoch: 5249, Batch Gradient Norm after: 8.128311555756392
Epoch 5250/10000, Prediction Accuracy = 61.984%, Loss = 0.4260883033275604
Epoch: 5250, Batch Gradient Norm: 9.092913791798273
Epoch: 5250, Batch Gradient Norm after: 9.092913791798273
Epoch 5251/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4333937704563141
Epoch: 5251, Batch Gradient Norm: 9.607211339214254
Epoch: 5251, Batch Gradient Norm after: 9.607211339214254
Epoch 5252/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.434555846452713
Epoch: 5252, Batch Gradient Norm: 10.616628280255734
Epoch: 5252, Batch Gradient Norm after: 10.616628280255734
Epoch 5253/10000, Prediction Accuracy = 61.852%, Loss = 0.44063896536827085
Epoch: 5253, Batch Gradient Norm: 8.607236866175048
Epoch: 5253, Batch Gradient Norm after: 8.607236866175048
Epoch 5254/10000, Prediction Accuracy = 62.11400000000001%, Loss = 0.42644089460372925
Epoch: 5254, Batch Gradient Norm: 8.777770427413625
Epoch: 5254, Batch Gradient Norm after: 8.777770427413625
Epoch 5255/10000, Prediction Accuracy = 62.134%, Loss = 0.4279376924037933
Epoch: 5255, Batch Gradient Norm: 9.231607397195285
Epoch: 5255, Batch Gradient Norm after: 9.231607397195285
Epoch 5256/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.43318256735801697
Epoch: 5256, Batch Gradient Norm: 11.466943310747013
Epoch: 5256, Batch Gradient Norm after: 11.466943310747013
Epoch 5257/10000, Prediction Accuracy = 61.944%, Loss = 0.44961296916008
Epoch: 5257, Batch Gradient Norm: 10.979769427047676
Epoch: 5257, Batch Gradient Norm after: 10.979769427047676
Epoch 5258/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.44239821434021
Epoch: 5258, Batch Gradient Norm: 9.727600956403153
Epoch: 5258, Batch Gradient Norm after: 9.727600956403153
Epoch 5259/10000, Prediction Accuracy = 61.988%, Loss = 0.4330199837684631
Epoch: 5259, Batch Gradient Norm: 7.516781777456471
Epoch: 5259, Batch Gradient Norm after: 7.516781777456471
Epoch 5260/10000, Prediction Accuracy = 62.02%, Loss = 0.4215945839881897
Epoch: 5260, Batch Gradient Norm: 7.8238910405856394
Epoch: 5260, Batch Gradient Norm after: 7.8238910405856394
Epoch 5261/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4246257543563843
Epoch: 5261, Batch Gradient Norm: 9.334233622030652
Epoch: 5261, Batch Gradient Norm after: 9.334233622030652
Epoch 5262/10000, Prediction Accuracy = 61.916%, Loss = 0.4358700931072235
Epoch: 5262, Batch Gradient Norm: 10.69360230178589
Epoch: 5262, Batch Gradient Norm after: 10.69360230178589
Epoch 5263/10000, Prediction Accuracy = 61.972%, Loss = 0.44328404068946836
Epoch: 5263, Batch Gradient Norm: 12.939227342154673
Epoch: 5263, Batch Gradient Norm after: 12.939227342154673
Epoch 5264/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.45897129774093626
Epoch: 5264, Batch Gradient Norm: 10.67167756931812
Epoch: 5264, Batch Gradient Norm after: 10.67167756931812
Epoch 5265/10000, Prediction Accuracy = 62.112%, Loss = 0.44367302656173707
Epoch: 5265, Batch Gradient Norm: 8.92687392552376
Epoch: 5265, Batch Gradient Norm after: 8.92687392552376
Epoch 5266/10000, Prediction Accuracy = 61.996%, Loss = 0.43058589696884153
Epoch: 5266, Batch Gradient Norm: 11.210320800044185
Epoch: 5266, Batch Gradient Norm after: 11.210320800044185
Epoch 5267/10000, Prediction Accuracy = 62.102%, Loss = 0.4481667459011078
Epoch: 5267, Batch Gradient Norm: 12.140389828819066
Epoch: 5267, Batch Gradient Norm after: 12.140389828819066
Epoch 5268/10000, Prediction Accuracy = 61.886%, Loss = 0.45478594303131104
Epoch: 5268, Batch Gradient Norm: 10.876712631751019
Epoch: 5268, Batch Gradient Norm after: 10.876712631751019
Epoch 5269/10000, Prediction Accuracy = 61.962%, Loss = 0.44427173733711245
Epoch: 5269, Batch Gradient Norm: 10.735851725938966
Epoch: 5269, Batch Gradient Norm after: 10.735851725938966
Epoch 5270/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.4498041391372681
Epoch: 5270, Batch Gradient Norm: 9.46499936666006
Epoch: 5270, Batch Gradient Norm after: 9.46499936666006
Epoch 5271/10000, Prediction Accuracy = 61.982000000000006%, Loss = 0.43985424041748045
Epoch: 5271, Batch Gradient Norm: 7.386673920198573
Epoch: 5271, Batch Gradient Norm after: 7.386673920198573
Epoch 5272/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.42120081186294556
Epoch: 5272, Batch Gradient Norm: 8.076289499301849
Epoch: 5272, Batch Gradient Norm after: 8.076289499301849
Epoch 5273/10000, Prediction Accuracy = 61.976%, Loss = 0.42828069925308226
Epoch: 5273, Batch Gradient Norm: 9.659844172818037
Epoch: 5273, Batch Gradient Norm after: 9.659844172818037
Epoch 5274/10000, Prediction Accuracy = 62.05%, Loss = 0.4376919150352478
Epoch: 5274, Batch Gradient Norm: 9.556398305931214
Epoch: 5274, Batch Gradient Norm after: 9.556398305931214
Epoch 5275/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.43743016123771666
Epoch: 5275, Batch Gradient Norm: 9.784483797338993
Epoch: 5275, Batch Gradient Norm after: 9.784483797338993
Epoch 5276/10000, Prediction Accuracy = 62.152%, Loss = 0.43593951463699343
Epoch: 5276, Batch Gradient Norm: 10.167840469711999
Epoch: 5276, Batch Gradient Norm after: 10.167840469711999
Epoch 5277/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.437780499458313
Epoch: 5277, Batch Gradient Norm: 9.713077738110577
Epoch: 5277, Batch Gradient Norm after: 9.713077738110577
Epoch 5278/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4342106103897095
Epoch: 5278, Batch Gradient Norm: 11.473587590965847
Epoch: 5278, Batch Gradient Norm after: 11.473587590965847
Epoch 5279/10000, Prediction Accuracy = 62.1%, Loss = 0.44426036477088926
Epoch: 5279, Batch Gradient Norm: 9.969149373422095
Epoch: 5279, Batch Gradient Norm after: 9.969149373422095
Epoch 5280/10000, Prediction Accuracy = 61.996%, Loss = 0.43444626927375796
Epoch: 5280, Batch Gradient Norm: 12.169997476359356
Epoch: 5280, Batch Gradient Norm after: 12.169997476359356
Epoch 5281/10000, Prediction Accuracy = 61.984%, Loss = 0.45157427787780763
Epoch: 5281, Batch Gradient Norm: 8.026474083524798
Epoch: 5281, Batch Gradient Norm after: 8.026474083524798
Epoch 5282/10000, Prediction Accuracy = 62.019999999999996%, Loss = 0.424881112575531
Epoch: 5282, Batch Gradient Norm: 9.049353008601608
Epoch: 5282, Batch Gradient Norm after: 9.049353008601608
Epoch 5283/10000, Prediction Accuracy = 61.886%, Loss = 0.4310563623905182
Epoch: 5283, Batch Gradient Norm: 8.608042940775814
Epoch: 5283, Batch Gradient Norm after: 8.608042940775814
Epoch 5284/10000, Prediction Accuracy = 62.064%, Loss = 0.4275811374187469
Epoch: 5284, Batch Gradient Norm: 8.42415463278969
Epoch: 5284, Batch Gradient Norm after: 8.42415463278969
Epoch 5285/10000, Prediction Accuracy = 62.188%, Loss = 0.42632336020469663
Epoch: 5285, Batch Gradient Norm: 8.905539074262903
Epoch: 5285, Batch Gradient Norm after: 8.905539074262903
Epoch 5286/10000, Prediction Accuracy = 62.032%, Loss = 0.4298677444458008
Epoch: 5286, Batch Gradient Norm: 10.137295331156285
Epoch: 5286, Batch Gradient Norm after: 10.137295331156285
Epoch 5287/10000, Prediction Accuracy = 62.008%, Loss = 0.4385029494762421
Epoch: 5287, Batch Gradient Norm: 10.968845529073372
Epoch: 5287, Batch Gradient Norm after: 10.968845529073372
Epoch 5288/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.44853886365890505
Epoch: 5288, Batch Gradient Norm: 5.795519432619403
Epoch: 5288, Batch Gradient Norm after: 5.795519432619403
Epoch 5289/10000, Prediction Accuracy = 62.112%, Loss = 0.4137986123561859
Epoch: 5289, Batch Gradient Norm: 7.029484023445521
Epoch: 5289, Batch Gradient Norm after: 7.029484023445521
Epoch 5290/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.41914852857589724
Epoch: 5290, Batch Gradient Norm: 12.304171038742652
Epoch: 5290, Batch Gradient Norm after: 12.304171038742652
Epoch 5291/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4572638154029846
Epoch: 5291, Batch Gradient Norm: 10.795008417990818
Epoch: 5291, Batch Gradient Norm after: 10.795008417990818
Epoch 5292/10000, Prediction Accuracy = 62.104%, Loss = 0.44077792167663576
Epoch: 5292, Batch Gradient Norm: 10.778912919066396
Epoch: 5292, Batch Gradient Norm after: 10.778912919066396
Epoch 5293/10000, Prediction Accuracy = 61.962%, Loss = 0.44270679354667664
Epoch: 5293, Batch Gradient Norm: 10.985408682721959
Epoch: 5293, Batch Gradient Norm after: 10.985408682721959
Epoch 5294/10000, Prediction Accuracy = 61.928%, Loss = 0.4438948750495911
Epoch: 5294, Batch Gradient Norm: 12.143648318295167
Epoch: 5294, Batch Gradient Norm after: 12.143648318295167
Epoch 5295/10000, Prediction Accuracy = 61.996%, Loss = 0.45451390743255615
Epoch: 5295, Batch Gradient Norm: 8.483296866108754
Epoch: 5295, Batch Gradient Norm after: 8.483296866108754
Epoch 5296/10000, Prediction Accuracy = 62.122%, Loss = 0.42751129865646365
Epoch: 5296, Batch Gradient Norm: 10.078825718032634
Epoch: 5296, Batch Gradient Norm after: 10.078825718032634
Epoch 5297/10000, Prediction Accuracy = 61.987999999999985%, Loss = 0.4336584746837616
Epoch: 5297, Batch Gradient Norm: 8.890846458273192
Epoch: 5297, Batch Gradient Norm after: 8.890846458273192
Epoch 5298/10000, Prediction Accuracy = 62.024%, Loss = 0.4279643654823303
Epoch: 5298, Batch Gradient Norm: 8.464423266002585
Epoch: 5298, Batch Gradient Norm after: 8.464423266002585
Epoch 5299/10000, Prediction Accuracy = 61.92%, Loss = 0.42733617424964904
Epoch: 5299, Batch Gradient Norm: 7.852229419286231
Epoch: 5299, Batch Gradient Norm after: 7.852229419286231
Epoch 5300/10000, Prediction Accuracy = 62.098%, Loss = 0.42356978058815004
Epoch: 5300, Batch Gradient Norm: 11.139340181400884
Epoch: 5300, Batch Gradient Norm after: 11.139340181400884
Epoch 5301/10000, Prediction Accuracy = 62.112%, Loss = 0.4451650381088257
Epoch: 5301, Batch Gradient Norm: 12.629085445099586
Epoch: 5301, Batch Gradient Norm after: 12.629085445099586
Epoch 5302/10000, Prediction Accuracy = 62.104%, Loss = 0.4544351875782013
Epoch: 5302, Batch Gradient Norm: 10.25653497938323
Epoch: 5302, Batch Gradient Norm after: 10.25653497938323
Epoch 5303/10000, Prediction Accuracy = 61.91400000000001%, Loss = 0.4367035984992981
Epoch: 5303, Batch Gradient Norm: 10.561358101981865
Epoch: 5303, Batch Gradient Norm after: 10.561358101981865
Epoch 5304/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4439360976219177
Epoch: 5304, Batch Gradient Norm: 10.690555012282987
Epoch: 5304, Batch Gradient Norm after: 10.690555012282987
Epoch 5305/10000, Prediction Accuracy = 62.048%, Loss = 0.444451767206192
Epoch: 5305, Batch Gradient Norm: 8.982066193633594
Epoch: 5305, Batch Gradient Norm after: 8.982066193633594
Epoch 5306/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.4316630721092224
Epoch: 5306, Batch Gradient Norm: 4.907005551622891
Epoch: 5306, Batch Gradient Norm after: 4.907005551622891
Epoch 5307/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.40908633470535277
Epoch: 5307, Batch Gradient Norm: 7.067876326252956
Epoch: 5307, Batch Gradient Norm after: 7.067876326252956
Epoch 5308/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4195203423500061
Epoch: 5308, Batch Gradient Norm: 10.189617110016716
Epoch: 5308, Batch Gradient Norm after: 10.189617110016716
Epoch 5309/10000, Prediction Accuracy = 62.016%, Loss = 0.43773147463798523
Epoch: 5309, Batch Gradient Norm: 10.916420219221779
Epoch: 5309, Batch Gradient Norm after: 10.916420219221779
Epoch 5310/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.44356355667114256
Epoch: 5310, Batch Gradient Norm: 10.000582834549139
Epoch: 5310, Batch Gradient Norm after: 10.000582834549139
Epoch 5311/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.43715024590492246
Epoch: 5311, Batch Gradient Norm: 9.711354425738438
Epoch: 5311, Batch Gradient Norm after: 9.711354425738438
Epoch 5312/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.43643433451652525
Epoch: 5312, Batch Gradient Norm: 7.260986498626202
Epoch: 5312, Batch Gradient Norm after: 7.260986498626202
Epoch 5313/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4205510377883911
Epoch: 5313, Batch Gradient Norm: 10.771997182524611
Epoch: 5313, Batch Gradient Norm after: 10.771997182524611
Epoch 5314/10000, Prediction Accuracy = 62.074%, Loss = 0.4407478868961334
Epoch: 5314, Batch Gradient Norm: 11.767478481905345
Epoch: 5314, Batch Gradient Norm after: 11.767478481905345
Epoch 5315/10000, Prediction Accuracy = 62.086%, Loss = 0.4482677519321442
Epoch: 5315, Batch Gradient Norm: 9.06779338859602
Epoch: 5315, Batch Gradient Norm after: 9.06779338859602
Epoch 5316/10000, Prediction Accuracy = 62.14000000000001%, Loss = 0.4292713403701782
Epoch: 5316, Batch Gradient Norm: 9.9830385798168
Epoch: 5316, Batch Gradient Norm after: 9.9830385798168
Epoch 5317/10000, Prediction Accuracy = 61.943999999999996%, Loss = 0.4360727250576019
Epoch: 5317, Batch Gradient Norm: 9.252932447976665
Epoch: 5317, Batch Gradient Norm after: 9.252932447976665
Epoch 5318/10000, Prediction Accuracy = 61.938%, Loss = 0.43361103534698486
Epoch: 5318, Batch Gradient Norm: 7.739527539081795
Epoch: 5318, Batch Gradient Norm after: 7.739527539081795
Epoch 5319/10000, Prediction Accuracy = 61.984%, Loss = 0.4223466157913208
Epoch: 5319, Batch Gradient Norm: 10.694892301117122
Epoch: 5319, Batch Gradient Norm after: 10.694892301117122
Epoch 5320/10000, Prediction Accuracy = 62.08%, Loss = 0.43924720883369445
Epoch: 5320, Batch Gradient Norm: 11.700153302224974
Epoch: 5320, Batch Gradient Norm after: 11.700153302224974
Epoch 5321/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4480345606803894
Epoch: 5321, Batch Gradient Norm: 12.617603845331884
Epoch: 5321, Batch Gradient Norm after: 12.617603845331884
Epoch 5322/10000, Prediction Accuracy = 62.174%, Loss = 0.4553786635398865
Epoch: 5322, Batch Gradient Norm: 12.647507190788138
Epoch: 5322, Batch Gradient Norm after: 12.647507190788138
Epoch 5323/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.4625299334526062
Epoch: 5323, Batch Gradient Norm: 8.81413030713119
Epoch: 5323, Batch Gradient Norm after: 8.81413030713119
Epoch 5324/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.42727248668670653
Epoch: 5324, Batch Gradient Norm: 6.59414855446291
Epoch: 5324, Batch Gradient Norm after: 6.59414855446291
Epoch 5325/10000, Prediction Accuracy = 62.14%, Loss = 0.4156552851200104
Epoch: 5325, Batch Gradient Norm: 8.523907385413532
Epoch: 5325, Batch Gradient Norm after: 8.523907385413532
Epoch 5326/10000, Prediction Accuracy = 62.056000000000004%, Loss = 0.4263808310031891
Epoch: 5326, Batch Gradient Norm: 8.952789002440714
Epoch: 5326, Batch Gradient Norm after: 8.952789002440714
Epoch 5327/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.429297947883606
Epoch: 5327, Batch Gradient Norm: 9.555199045023205
Epoch: 5327, Batch Gradient Norm after: 9.555199045023205
Epoch 5328/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4335747480392456
Epoch: 5328, Batch Gradient Norm: 11.21209581987575
Epoch: 5328, Batch Gradient Norm after: 11.21209581987575
Epoch 5329/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4450100064277649
Epoch: 5329, Batch Gradient Norm: 10.230738610390567
Epoch: 5329, Batch Gradient Norm after: 10.230738610390567
Epoch 5330/10000, Prediction Accuracy = 62.024%, Loss = 0.44369486570358274
Epoch: 5330, Batch Gradient Norm: 10.611677703452473
Epoch: 5330, Batch Gradient Norm after: 10.611677703452473
Epoch 5331/10000, Prediction Accuracy = 62.036%, Loss = 0.4411798119544983
Epoch: 5331, Batch Gradient Norm: 8.602624559250854
Epoch: 5331, Batch Gradient Norm after: 8.602624559250854
Epoch 5332/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4279575288295746
Epoch: 5332, Batch Gradient Norm: 8.45648815221992
Epoch: 5332, Batch Gradient Norm after: 8.45648815221992
Epoch 5333/10000, Prediction Accuracy = 62.084%, Loss = 0.42555390000343324
Epoch: 5333, Batch Gradient Norm: 9.55189454890412
Epoch: 5333, Batch Gradient Norm after: 9.55189454890412
Epoch 5334/10000, Prediction Accuracy = 62.07399999999999%, Loss = 0.43240549564361574
Epoch: 5334, Batch Gradient Norm: 11.834544273699704
Epoch: 5334, Batch Gradient Norm after: 11.834544273699704
Epoch 5335/10000, Prediction Accuracy = 62.016%, Loss = 0.44547195434570314
Epoch: 5335, Batch Gradient Norm: 9.519255371840085
Epoch: 5335, Batch Gradient Norm after: 9.519255371840085
Epoch 5336/10000, Prediction Accuracy = 62.102%, Loss = 0.4312561392784119
Epoch: 5336, Batch Gradient Norm: 6.7402688394129875
Epoch: 5336, Batch Gradient Norm after: 6.7402688394129875
Epoch 5337/10000, Prediction Accuracy = 62.14200000000001%, Loss = 0.4160302519798279
Epoch: 5337, Batch Gradient Norm: 9.730776518576802
Epoch: 5337, Batch Gradient Norm after: 9.730776518576802
Epoch 5338/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.43469353914260866
Epoch: 5338, Batch Gradient Norm: 10.431693804692394
Epoch: 5338, Batch Gradient Norm after: 10.431693804692394
Epoch 5339/10000, Prediction Accuracy = 62.08200000000001%, Loss = 0.44112794399261473
Epoch: 5339, Batch Gradient Norm: 10.643995454055894
Epoch: 5339, Batch Gradient Norm after: 10.643995454055894
Epoch 5340/10000, Prediction Accuracy = 62.062%, Loss = 0.44081948399543763
Epoch: 5340, Batch Gradient Norm: 9.892992026379055
Epoch: 5340, Batch Gradient Norm after: 9.892992026379055
Epoch 5341/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.4398838818073273
Epoch: 5341, Batch Gradient Norm: 9.435999484863276
Epoch: 5341, Batch Gradient Norm after: 9.435999484863276
Epoch 5342/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4338758409023285
Epoch: 5342, Batch Gradient Norm: 8.59532421188135
Epoch: 5342, Batch Gradient Norm after: 8.59532421188135
Epoch 5343/10000, Prediction Accuracy = 62.176%, Loss = 0.4267868399620056
Epoch: 5343, Batch Gradient Norm: 10.02521981678155
Epoch: 5343, Batch Gradient Norm after: 10.02521981678155
Epoch 5344/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4335769832134247
Epoch: 5344, Batch Gradient Norm: 12.719370286294701
Epoch: 5344, Batch Gradient Norm after: 12.719370286294701
Epoch 5345/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.45620139241218566
Epoch: 5345, Batch Gradient Norm: 11.094848450730982
Epoch: 5345, Batch Gradient Norm after: 11.094848450730982
Epoch 5346/10000, Prediction Accuracy = 61.962%, Loss = 0.44342089295387266
Epoch: 5346, Batch Gradient Norm: 11.083225449273025
Epoch: 5346, Batch Gradient Norm after: 11.083225449273025
Epoch 5347/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.44330331683158875
Epoch: 5347, Batch Gradient Norm: 10.724756390518715
Epoch: 5347, Batch Gradient Norm after: 10.724756390518715
Epoch 5348/10000, Prediction Accuracy = 62.198%, Loss = 0.4385711371898651
Epoch: 5348, Batch Gradient Norm: 7.6411306597053406
Epoch: 5348, Batch Gradient Norm after: 7.6411306597053406
Epoch 5349/10000, Prediction Accuracy = 62.11999999999999%, Loss = 0.42185928821563723
Epoch: 5349, Batch Gradient Norm: 7.5799040887610465
Epoch: 5349, Batch Gradient Norm after: 7.5799040887610465
Epoch 5350/10000, Prediction Accuracy = 62.176%, Loss = 0.41994261741638184
Epoch: 5350, Batch Gradient Norm: 8.888129584937737
Epoch: 5350, Batch Gradient Norm after: 8.888129584937737
Epoch 5351/10000, Prediction Accuracy = 62.122%, Loss = 0.42719902396202086
Epoch: 5351, Batch Gradient Norm: 6.610895575242148
Epoch: 5351, Batch Gradient Norm after: 6.610895575242148
Epoch 5352/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.4153547763824463
Epoch: 5352, Batch Gradient Norm: 8.078550536934635
Epoch: 5352, Batch Gradient Norm after: 8.078550536934635
Epoch 5353/10000, Prediction Accuracy = 61.972%, Loss = 0.4233936905860901
Epoch: 5353, Batch Gradient Norm: 10.599626831064588
Epoch: 5353, Batch Gradient Norm after: 10.599626831064588
Epoch 5354/10000, Prediction Accuracy = 61.912%, Loss = 0.4440488278865814
Epoch: 5354, Batch Gradient Norm: 8.6411776860059
Epoch: 5354, Batch Gradient Norm after: 8.6411776860059
Epoch 5355/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.4301700294017792
Epoch: 5355, Batch Gradient Norm: 10.864602501747067
Epoch: 5355, Batch Gradient Norm after: 10.864602501747067
Epoch 5356/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.44109468460083007
Epoch: 5356, Batch Gradient Norm: 10.867631522548107
Epoch: 5356, Batch Gradient Norm after: 10.867631522548107
Epoch 5357/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4421604752540588
Epoch: 5357, Batch Gradient Norm: 11.678845605006153
Epoch: 5357, Batch Gradient Norm after: 11.678845605006153
Epoch 5358/10000, Prediction Accuracy = 61.958000000000006%, Loss = 0.4472303628921509
Epoch: 5358, Batch Gradient Norm: 9.68773827096669
Epoch: 5358, Batch Gradient Norm after: 9.68773827096669
Epoch 5359/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.43088749051094055
Epoch: 5359, Batch Gradient Norm: 10.179523034814949
Epoch: 5359, Batch Gradient Norm after: 10.179523034814949
Epoch 5360/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.43625280261039734
Epoch: 5360, Batch Gradient Norm: 8.812193014979655
Epoch: 5360, Batch Gradient Norm after: 8.812193014979655
Epoch 5361/10000, Prediction Accuracy = 62.016000000000005%, Loss = 0.42660858631134035
Epoch: 5361, Batch Gradient Norm: 10.975055793336137
Epoch: 5361, Batch Gradient Norm after: 10.975055793336137
Epoch 5362/10000, Prediction Accuracy = 61.902%, Loss = 0.4443168878555298
Epoch: 5362, Batch Gradient Norm: 10.435596848437775
Epoch: 5362, Batch Gradient Norm after: 10.435596848437775
Epoch 5363/10000, Prediction Accuracy = 62.076%, Loss = 0.43805662393569944
Epoch: 5363, Batch Gradient Norm: 9.895805348974205
Epoch: 5363, Batch Gradient Norm after: 9.895805348974205
Epoch 5364/10000, Prediction Accuracy = 62.044%, Loss = 0.43488094210624695
Epoch: 5364, Batch Gradient Norm: 10.352415670912869
Epoch: 5364, Batch Gradient Norm after: 10.352415670912869
Epoch 5365/10000, Prediction Accuracy = 61.988%, Loss = 0.4408818244934082
Epoch: 5365, Batch Gradient Norm: 9.317610226553352
Epoch: 5365, Batch Gradient Norm after: 9.317610226553352
Epoch 5366/10000, Prediction Accuracy = 62.158%, Loss = 0.4315681099891663
Epoch: 5366, Batch Gradient Norm: 9.370563419956383
Epoch: 5366, Batch Gradient Norm after: 9.370563419956383
Epoch 5367/10000, Prediction Accuracy = 62.126%, Loss = 0.4323646068572998
Epoch: 5367, Batch Gradient Norm: 10.982948519114721
Epoch: 5367, Batch Gradient Norm after: 10.982948519114721
Epoch 5368/10000, Prediction Accuracy = 62.038%, Loss = 0.44500797986984253
Epoch: 5368, Batch Gradient Norm: 8.595513369514341
Epoch: 5368, Batch Gradient Norm after: 8.595513369514341
Epoch 5369/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4254299819469452
Epoch: 5369, Batch Gradient Norm: 8.80660928873979
Epoch: 5369, Batch Gradient Norm after: 8.80660928873979
Epoch 5370/10000, Prediction Accuracy = 62.098%, Loss = 0.42882814407348635
Epoch: 5370, Batch Gradient Norm: 10.256917702641058
Epoch: 5370, Batch Gradient Norm after: 10.256917702641058
Epoch 5371/10000, Prediction Accuracy = 62.052%, Loss = 0.4379767060279846
Epoch: 5371, Batch Gradient Norm: 9.151534974986262
Epoch: 5371, Batch Gradient Norm after: 9.151534974986262
Epoch 5372/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.42836822271347047
Epoch: 5372, Batch Gradient Norm: 13.289352778339117
Epoch: 5372, Batch Gradient Norm after: 13.289352778339117
Epoch 5373/10000, Prediction Accuracy = 62.00999999999999%, Loss = 0.46140040159225465
Epoch: 5373, Batch Gradient Norm: 9.837370176261137
Epoch: 5373, Batch Gradient Norm after: 9.837370176261137
Epoch 5374/10000, Prediction Accuracy = 61.972%, Loss = 0.4355787932872772
Epoch: 5374, Batch Gradient Norm: 10.817013617486644
Epoch: 5374, Batch Gradient Norm after: 10.817013617486644
Epoch 5375/10000, Prediction Accuracy = 62.092000000000006%, Loss = 0.4419386386871338
Epoch: 5375, Batch Gradient Norm: 10.020366157635966
Epoch: 5375, Batch Gradient Norm after: 10.020366157635966
Epoch 5376/10000, Prediction Accuracy = 62.072%, Loss = 0.4390433132648468
Epoch: 5376, Batch Gradient Norm: 9.574644814798217
Epoch: 5376, Batch Gradient Norm after: 9.574644814798217
Epoch 5377/10000, Prediction Accuracy = 62.052%, Loss = 0.43069576621055605
Epoch: 5377, Batch Gradient Norm: 11.262137129131917
Epoch: 5377, Batch Gradient Norm after: 11.262137129131917
Epoch 5378/10000, Prediction Accuracy = 62.072%, Loss = 0.4457254111766815
Epoch: 5378, Batch Gradient Norm: 10.215507352382147
Epoch: 5378, Batch Gradient Norm after: 10.215507352382147
Epoch 5379/10000, Prediction Accuracy = 62.11%, Loss = 0.44174760580062866
Epoch: 5379, Batch Gradient Norm: 5.210164744437137
Epoch: 5379, Batch Gradient Norm after: 5.210164744437137
Epoch 5380/10000, Prediction Accuracy = 62.038%, Loss = 0.40862358212471006
Epoch: 5380, Batch Gradient Norm: 10.423592614377236
Epoch: 5380, Batch Gradient Norm after: 10.423592614377236
Epoch 5381/10000, Prediction Accuracy = 62.08%, Loss = 0.44053285717964175
Epoch: 5381, Batch Gradient Norm: 6.959414061580942
Epoch: 5381, Batch Gradient Norm after: 6.959414061580942
Epoch 5382/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.4196258783340454
Epoch: 5382, Batch Gradient Norm: 7.9237073440186165
Epoch: 5382, Batch Gradient Norm after: 7.9237073440186165
Epoch 5383/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4207469403743744
Epoch: 5383, Batch Gradient Norm: 11.202745716162324
Epoch: 5383, Batch Gradient Norm after: 11.202745716162324
Epoch 5384/10000, Prediction Accuracy = 62.052%, Loss = 0.44720777273178103
Epoch: 5384, Batch Gradient Norm: 9.455058321854633
Epoch: 5384, Batch Gradient Norm after: 9.455058321854633
Epoch 5385/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4330601155757904
Epoch: 5385, Batch Gradient Norm: 10.231203393046052
Epoch: 5385, Batch Gradient Norm after: 10.231203393046052
Epoch 5386/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.43561222553253176
Epoch: 5386, Batch Gradient Norm: 10.457542642249502
Epoch: 5386, Batch Gradient Norm after: 10.457542642249502
Epoch 5387/10000, Prediction Accuracy = 62.158%, Loss = 0.43934873342514036
Epoch: 5387, Batch Gradient Norm: 10.267945454227743
Epoch: 5387, Batch Gradient Norm after: 10.267945454227743
Epoch 5388/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4366132438182831
Epoch: 5388, Batch Gradient Norm: 9.01370948975071
Epoch: 5388, Batch Gradient Norm after: 9.01370948975071
Epoch 5389/10000, Prediction Accuracy = 62.11%, Loss = 0.43061932921409607
Epoch: 5389, Batch Gradient Norm: 9.2447663299325
Epoch: 5389, Batch Gradient Norm after: 9.2447663299325
Epoch 5390/10000, Prediction Accuracy = 62.1%, Loss = 0.4288544535636902
Epoch: 5390, Batch Gradient Norm: 11.222850506903082
Epoch: 5390, Batch Gradient Norm after: 11.222850506903082
Epoch 5391/10000, Prediction Accuracy = 62.17%, Loss = 0.4459843039512634
Epoch: 5391, Batch Gradient Norm: 10.536566660268214
Epoch: 5391, Batch Gradient Norm after: 10.536566660268214
Epoch 5392/10000, Prediction Accuracy = 62.1%, Loss = 0.4410270512104034
Epoch: 5392, Batch Gradient Norm: 8.11949476689414
Epoch: 5392, Batch Gradient Norm after: 8.11949476689414
Epoch 5393/10000, Prediction Accuracy = 62.072%, Loss = 0.4236972212791443
Epoch: 5393, Batch Gradient Norm: 7.741533252227942
Epoch: 5393, Batch Gradient Norm after: 7.741533252227942
Epoch 5394/10000, Prediction Accuracy = 62.11%, Loss = 0.42075689435005187
Epoch: 5394, Batch Gradient Norm: 11.778540412905588
Epoch: 5394, Batch Gradient Norm after: 11.778540412905588
Epoch 5395/10000, Prediction Accuracy = 62.019999999999996%, Loss = 0.447616046667099
Epoch: 5395, Batch Gradient Norm: 11.830488293301487
Epoch: 5395, Batch Gradient Norm after: 11.830488293301487
Epoch 5396/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.4478237748146057
Epoch: 5396, Batch Gradient Norm: 9.156161529290642
Epoch: 5396, Batch Gradient Norm after: 9.156161529290642
Epoch 5397/10000, Prediction Accuracy = 62.112%, Loss = 0.4255745649337769
Epoch: 5397, Batch Gradient Norm: 11.608746695959468
Epoch: 5397, Batch Gradient Norm after: 11.608746695959468
Epoch 5398/10000, Prediction Accuracy = 62.05400000000001%, Loss = 0.44646555185317993
Epoch: 5398, Batch Gradient Norm: 11.413569616823963
Epoch: 5398, Batch Gradient Norm after: 11.413569616823963
Epoch 5399/10000, Prediction Accuracy = 62.148%, Loss = 0.44895400404930114
Epoch: 5399, Batch Gradient Norm: 10.33568401767147
Epoch: 5399, Batch Gradient Norm after: 10.33568401767147
Epoch 5400/10000, Prediction Accuracy = 62.112%, Loss = 0.4420653820037842
Epoch: 5400, Batch Gradient Norm: 8.226210037854745
Epoch: 5400, Batch Gradient Norm after: 8.226210037854745
Epoch 5401/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4263452053070068
Epoch: 5401, Batch Gradient Norm: 9.74814030738079
Epoch: 5401, Batch Gradient Norm after: 9.74814030738079
Epoch 5402/10000, Prediction Accuracy = 62.008%, Loss = 0.4334891140460968
Epoch: 5402, Batch Gradient Norm: 7.964823013180505
Epoch: 5402, Batch Gradient Norm after: 7.964823013180505
Epoch 5403/10000, Prediction Accuracy = 61.983999999999995%, Loss = 0.422538959980011
Epoch: 5403, Batch Gradient Norm: 8.167055769660353
Epoch: 5403, Batch Gradient Norm after: 8.167055769660353
Epoch 5404/10000, Prediction Accuracy = 62.04%, Loss = 0.4217948794364929
Epoch: 5404, Batch Gradient Norm: 9.174831056678638
Epoch: 5404, Batch Gradient Norm after: 9.174831056678638
Epoch 5405/10000, Prediction Accuracy = 62.032000000000004%, Loss = 0.4330741107463837
Epoch: 5405, Batch Gradient Norm: 9.50373025442839
Epoch: 5405, Batch Gradient Norm after: 9.50373025442839
Epoch 5406/10000, Prediction Accuracy = 61.964%, Loss = 0.42956048250198364
Epoch: 5406, Batch Gradient Norm: 11.8239981874248
Epoch: 5406, Batch Gradient Norm after: 11.8239981874248
Epoch 5407/10000, Prediction Accuracy = 61.86400000000001%, Loss = 0.4467725992202759
Epoch: 5407, Batch Gradient Norm: 12.001876130750437
Epoch: 5407, Batch Gradient Norm after: 12.001876130750437
Epoch 5408/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.4466128170490265
Epoch: 5408, Batch Gradient Norm: 10.563868733674191
Epoch: 5408, Batch Gradient Norm after: 10.563868733674191
Epoch 5409/10000, Prediction Accuracy = 61.91600000000001%, Loss = 0.4393076479434967
Epoch: 5409, Batch Gradient Norm: 9.563525437297764
Epoch: 5409, Batch Gradient Norm after: 9.563525437297764
Epoch 5410/10000, Prediction Accuracy = 62.148%, Loss = 0.43246798515319823
Epoch: 5410, Batch Gradient Norm: 8.66819410408765
Epoch: 5410, Batch Gradient Norm after: 8.66819410408765
Epoch 5411/10000, Prediction Accuracy = 62.088%, Loss = 0.42647055983543397
Epoch: 5411, Batch Gradient Norm: 8.712757111277078
Epoch: 5411, Batch Gradient Norm after: 8.712757111277078
Epoch 5412/10000, Prediction Accuracy = 62.012%, Loss = 0.4275944888591766
Epoch: 5412, Batch Gradient Norm: 8.134991701634064
Epoch: 5412, Batch Gradient Norm after: 8.134991701634064
Epoch 5413/10000, Prediction Accuracy = 61.94200000000001%, Loss = 0.42410844564437866
Epoch: 5413, Batch Gradient Norm: 9.5998912514449
Epoch: 5413, Batch Gradient Norm after: 9.5998912514449
Epoch 5414/10000, Prediction Accuracy = 62.072%, Loss = 0.43261985182762147
Epoch: 5414, Batch Gradient Norm: 8.92437543627794
Epoch: 5414, Batch Gradient Norm after: 8.92437543627794
Epoch 5415/10000, Prediction Accuracy = 62.076%, Loss = 0.4266580641269684
Epoch: 5415, Batch Gradient Norm: 7.504159355703364
Epoch: 5415, Batch Gradient Norm after: 7.504159355703364
Epoch 5416/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4193058907985687
Epoch: 5416, Batch Gradient Norm: 10.527860515419132
Epoch: 5416, Batch Gradient Norm after: 10.527860515419132
Epoch 5417/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4354631841182709
Epoch: 5417, Batch Gradient Norm: 13.033494310889742
Epoch: 5417, Batch Gradient Norm after: 13.033494310889742
Epoch 5418/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.4561147391796112
Epoch: 5418, Batch Gradient Norm: 11.870059585013053
Epoch: 5418, Batch Gradient Norm after: 11.870059585013053
Epoch 5419/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4494406521320343
Epoch: 5419, Batch Gradient Norm: 9.40431523586078
Epoch: 5419, Batch Gradient Norm after: 9.40431523586078
Epoch 5420/10000, Prediction Accuracy = 62.086%, Loss = 0.4288244068622589
Epoch: 5420, Batch Gradient Norm: 9.35006557532594
Epoch: 5420, Batch Gradient Norm after: 9.35006557532594
Epoch 5421/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.43126962184906004
Epoch: 5421, Batch Gradient Norm: 9.421116670480785
Epoch: 5421, Batch Gradient Norm after: 9.421116670480785
Epoch 5422/10000, Prediction Accuracy = 62.08%, Loss = 0.42819130420684814
Epoch: 5422, Batch Gradient Norm: 10.59472225641
Epoch: 5422, Batch Gradient Norm after: 10.59472225641
Epoch 5423/10000, Prediction Accuracy = 62.028%, Loss = 0.4368925392627716
Epoch: 5423, Batch Gradient Norm: 12.167524533202956
Epoch: 5423, Batch Gradient Norm after: 12.167524533202956
Epoch 5424/10000, Prediction Accuracy = 62.05800000000001%, Loss = 0.44657461643218993
Epoch: 5424, Batch Gradient Norm: 11.035788721692924
Epoch: 5424, Batch Gradient Norm after: 11.035788721692924
Epoch 5425/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.44129887223243713
Epoch: 5425, Batch Gradient Norm: 8.618469553035851
Epoch: 5425, Batch Gradient Norm after: 8.618469553035851
Epoch 5426/10000, Prediction Accuracy = 62.02%, Loss = 0.42904677987098694
Epoch: 5426, Batch Gradient Norm: 8.940273755954797
Epoch: 5426, Batch Gradient Norm after: 8.940273755954797
Epoch 5427/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.42738752961158755
Epoch: 5427, Batch Gradient Norm: 11.082225484528232
Epoch: 5427, Batch Gradient Norm after: 11.082225484528232
Epoch 5428/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.4486266791820526
Epoch: 5428, Batch Gradient Norm: 6.017568230915138
Epoch: 5428, Batch Gradient Norm after: 6.017568230915138
Epoch 5429/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.41269675493240354
Epoch: 5429, Batch Gradient Norm: 10.753914954301996
Epoch: 5429, Batch Gradient Norm after: 10.753914954301996
Epoch 5430/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.44211551547050476
Epoch: 5430, Batch Gradient Norm: 10.705634915553217
Epoch: 5430, Batch Gradient Norm after: 10.705634915553217
Epoch 5431/10000, Prediction Accuracy = 62.032000000000004%, Loss = 0.4403169572353363
Epoch: 5431, Batch Gradient Norm: 9.887982309492681
Epoch: 5431, Batch Gradient Norm after: 9.887982309492681
Epoch 5432/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4344816207885742
Epoch: 5432, Batch Gradient Norm: 7.2593846536828535
Epoch: 5432, Batch Gradient Norm after: 7.2593846536828535
Epoch 5433/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.41695637106895445
Epoch: 5433, Batch Gradient Norm: 9.421656244338045
Epoch: 5433, Batch Gradient Norm after: 9.421656244338045
Epoch 5434/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4293705940246582
Epoch: 5434, Batch Gradient Norm: 10.214458353003158
Epoch: 5434, Batch Gradient Norm after: 10.214458353003158
Epoch 5435/10000, Prediction Accuracy = 61.96%, Loss = 0.43338181376457213
Epoch: 5435, Batch Gradient Norm: 8.709575769384223
Epoch: 5435, Batch Gradient Norm after: 8.709575769384223
Epoch 5436/10000, Prediction Accuracy = 62.138%, Loss = 0.42715063095092776
Epoch: 5436, Batch Gradient Norm: 9.31693388644297
Epoch: 5436, Batch Gradient Norm after: 9.31693388644297
Epoch 5437/10000, Prediction Accuracy = 62.11%, Loss = 0.4319698989391327
Epoch: 5437, Batch Gradient Norm: 10.014353598779993
Epoch: 5437, Batch Gradient Norm after: 10.014353598779993
Epoch 5438/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.4346687853336334
Epoch: 5438, Batch Gradient Norm: 10.750620468245424
Epoch: 5438, Batch Gradient Norm after: 10.750620468245424
Epoch 5439/10000, Prediction Accuracy = 62.164%, Loss = 0.43844794034957885
Epoch: 5439, Batch Gradient Norm: 12.238533800728709
Epoch: 5439, Batch Gradient Norm after: 12.238533800728709
Epoch 5440/10000, Prediction Accuracy = 61.886%, Loss = 0.4518501400947571
Epoch: 5440, Batch Gradient Norm: 11.759231773007727
Epoch: 5440, Batch Gradient Norm after: 11.759231773007727
Epoch 5441/10000, Prediction Accuracy = 61.95%, Loss = 0.44786485433578493
Epoch: 5441, Batch Gradient Norm: 8.274471532833566
Epoch: 5441, Batch Gradient Norm after: 8.274471532833566
Epoch 5442/10000, Prediction Accuracy = 62.008%, Loss = 0.4231108844280243
Epoch: 5442, Batch Gradient Norm: 7.285439557887145
Epoch: 5442, Batch Gradient Norm after: 7.285439557887145
Epoch 5443/10000, Prediction Accuracy = 62.10799999999999%, Loss = 0.4173760533332825
Epoch: 5443, Batch Gradient Norm: 9.017620178349217
Epoch: 5443, Batch Gradient Norm after: 9.017620178349217
Epoch 5444/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.4269626557826996
Epoch: 5444, Batch Gradient Norm: 11.900312316946586
Epoch: 5444, Batch Gradient Norm after: 11.900312316946586
Epoch 5445/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.4475990056991577
Epoch: 5445, Batch Gradient Norm: 11.081569539603976
Epoch: 5445, Batch Gradient Norm after: 11.081569539603976
Epoch 5446/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.4382836103439331
Epoch: 5446, Batch Gradient Norm: 9.8671723032097
Epoch: 5446, Batch Gradient Norm after: 9.8671723032097
Epoch 5447/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.43043392300605776
Epoch: 5447, Batch Gradient Norm: 10.69827549394735
Epoch: 5447, Batch Gradient Norm after: 10.69827549394735
Epoch 5448/10000, Prediction Accuracy = 62.18000000000001%, Loss = 0.4406621277332306
Epoch: 5448, Batch Gradient Norm: 8.193965878589172
Epoch: 5448, Batch Gradient Norm after: 8.193965878589172
Epoch 5449/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.42482748031616213
Epoch: 5449, Batch Gradient Norm: 9.261042028650916
Epoch: 5449, Batch Gradient Norm after: 9.261042028650916
Epoch 5450/10000, Prediction Accuracy = 62.14%, Loss = 0.428978705406189
Epoch: 5450, Batch Gradient Norm: 9.677667931393383
Epoch: 5450, Batch Gradient Norm after: 9.677667931393383
Epoch 5451/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4343651533126831
Epoch: 5451, Batch Gradient Norm: 10.440729333302235
Epoch: 5451, Batch Gradient Norm after: 10.440729333302235
Epoch 5452/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.4359943628311157
Epoch: 5452, Batch Gradient Norm: 11.690719096591682
Epoch: 5452, Batch Gradient Norm after: 11.690719096591682
Epoch 5453/10000, Prediction Accuracy = 62.024%, Loss = 0.4434096932411194
Epoch: 5453, Batch Gradient Norm: 8.79127766221515
Epoch: 5453, Batch Gradient Norm after: 8.79127766221515
Epoch 5454/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4258894920349121
Epoch: 5454, Batch Gradient Norm: 5.4372417235153545
Epoch: 5454, Batch Gradient Norm after: 5.4372417235153545
Epoch 5455/10000, Prediction Accuracy = 62.126%, Loss = 0.4085202574729919
Epoch: 5455, Batch Gradient Norm: 5.890337168770858
Epoch: 5455, Batch Gradient Norm after: 5.890337168770858
Epoch 5456/10000, Prediction Accuracy = 62.152%, Loss = 0.40841750502586366
Epoch: 5456, Batch Gradient Norm: 7.446576488290277
Epoch: 5456, Batch Gradient Norm after: 7.446576488290277
Epoch 5457/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.41717720627784727
Epoch: 5457, Batch Gradient Norm: 13.992011669589301
Epoch: 5457, Batch Gradient Norm after: 13.992011669589301
Epoch 5458/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4692915081977844
Epoch: 5458, Batch Gradient Norm: 9.824766483110686
Epoch: 5458, Batch Gradient Norm after: 9.824766483110686
Epoch 5459/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.43457257747650146
Epoch: 5459, Batch Gradient Norm: 8.823609095716211
Epoch: 5459, Batch Gradient Norm after: 8.823609095716211
Epoch 5460/10000, Prediction Accuracy = 62.112%, Loss = 0.4280399143695831
Epoch: 5460, Batch Gradient Norm: 8.394118608964394
Epoch: 5460, Batch Gradient Norm after: 8.394118608964394
Epoch 5461/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.42157781720161436
Epoch: 5461, Batch Gradient Norm: 12.806868572817994
Epoch: 5461, Batch Gradient Norm after: 12.806868572817994
Epoch 5462/10000, Prediction Accuracy = 61.888%, Loss = 0.45821813941001893
Epoch: 5462, Batch Gradient Norm: 6.992738578132609
Epoch: 5462, Batch Gradient Norm after: 6.992738578132609
Epoch 5463/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.416034996509552
Epoch: 5463, Batch Gradient Norm: 10.054971478422233
Epoch: 5463, Batch Gradient Norm after: 10.054971478422233
Epoch 5464/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.432364422082901
Epoch: 5464, Batch Gradient Norm: 11.295164068673502
Epoch: 5464, Batch Gradient Norm after: 11.295164068673502
Epoch 5465/10000, Prediction Accuracy = 62.024%, Loss = 0.4446033239364624
Epoch: 5465, Batch Gradient Norm: 8.780083633856309
Epoch: 5465, Batch Gradient Norm after: 8.780083633856309
Epoch 5466/10000, Prediction Accuracy = 62.114%, Loss = 0.42556862235069276
Epoch: 5466, Batch Gradient Norm: 11.233471779960349
Epoch: 5466, Batch Gradient Norm after: 11.233471779960349
Epoch 5467/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.43899562358856203
Epoch: 5467, Batch Gradient Norm: 11.292992008668797
Epoch: 5467, Batch Gradient Norm after: 11.292992008668797
Epoch 5468/10000, Prediction Accuracy = 62.138%, Loss = 0.443219256401062
Epoch: 5468, Batch Gradient Norm: 9.01387571293816
Epoch: 5468, Batch Gradient Norm after: 9.01387571293816
Epoch 5469/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.42565845251083373
Epoch: 5469, Batch Gradient Norm: 9.458200627638941
Epoch: 5469, Batch Gradient Norm after: 9.458200627638941
Epoch 5470/10000, Prediction Accuracy = 62.072%, Loss = 0.42757056951522826
Epoch: 5470, Batch Gradient Norm: 9.117409453094828
Epoch: 5470, Batch Gradient Norm after: 9.117409453094828
Epoch 5471/10000, Prediction Accuracy = 62.206%, Loss = 0.4263490378856659
Epoch: 5471, Batch Gradient Norm: 8.245149407353885
Epoch: 5471, Batch Gradient Norm after: 8.245149407353885
Epoch 5472/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4233798861503601
Epoch: 5472, Batch Gradient Norm: 8.938845602714657
Epoch: 5472, Batch Gradient Norm after: 8.938845602714657
Epoch 5473/10000, Prediction Accuracy = 62.098%, Loss = 0.4242480516433716
Epoch: 5473, Batch Gradient Norm: 11.339059950215006
Epoch: 5473, Batch Gradient Norm after: 11.339059950215006
Epoch 5474/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4467074930667877
Epoch: 5474, Batch Gradient Norm: 7.577821639343825
Epoch: 5474, Batch Gradient Norm after: 7.577821639343825
Epoch 5475/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4198959946632385
Epoch: 5475, Batch Gradient Norm: 8.513293062377938
Epoch: 5475, Batch Gradient Norm after: 8.513293062377938
Epoch 5476/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.42354239225387574
Epoch: 5476, Batch Gradient Norm: 12.508301282205814
Epoch: 5476, Batch Gradient Norm after: 12.508301282205814
Epoch 5477/10000, Prediction Accuracy = 61.924%, Loss = 0.44960454702377317
Epoch: 5477, Batch Gradient Norm: 11.463598256636587
Epoch: 5477, Batch Gradient Norm after: 11.463598256636587
Epoch 5478/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.44346401691436765
Epoch: 5478, Batch Gradient Norm: 9.817081448343691
Epoch: 5478, Batch Gradient Norm after: 9.817081448343691
Epoch 5479/10000, Prediction Accuracy = 61.98%, Loss = 0.4312800049781799
Epoch: 5479, Batch Gradient Norm: 9.409331478851527
Epoch: 5479, Batch Gradient Norm after: 9.409331478851527
Epoch 5480/10000, Prediction Accuracy = 62.132000000000005%, Loss = 0.42954484224319456
Epoch: 5480, Batch Gradient Norm: 10.948079269672201
Epoch: 5480, Batch Gradient Norm after: 10.948079269672201
Epoch 5481/10000, Prediction Accuracy = 62.169999999999995%, Loss = 0.4386140048503876
Epoch: 5481, Batch Gradient Norm: 7.932690575737568
Epoch: 5481, Batch Gradient Norm after: 7.932690575737568
Epoch 5482/10000, Prediction Accuracy = 62.102%, Loss = 0.41995699405670167
Epoch: 5482, Batch Gradient Norm: 7.787071921260054
Epoch: 5482, Batch Gradient Norm after: 7.787071921260054
Epoch 5483/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4180067181587219
Epoch: 5483, Batch Gradient Norm: 9.994765597278068
Epoch: 5483, Batch Gradient Norm after: 9.994765597278068
Epoch 5484/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.4295474708080292
Epoch: 5484, Batch Gradient Norm: 14.339958592085368
Epoch: 5484, Batch Gradient Norm after: 14.339958592085368
Epoch 5485/10000, Prediction Accuracy = 61.972%, Loss = 0.4688990771770477
Epoch: 5485, Batch Gradient Norm: 9.184458899018692
Epoch: 5485, Batch Gradient Norm after: 9.184458899018692
Epoch 5486/10000, Prediction Accuracy = 62.124%, Loss = 0.42789584398269653
Epoch: 5486, Batch Gradient Norm: 6.837045528274248
Epoch: 5486, Batch Gradient Norm after: 6.837045528274248
Epoch 5487/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.41718569993972776
Epoch: 5487, Batch Gradient Norm: 6.858698385366207
Epoch: 5487, Batch Gradient Norm after: 6.858698385366207
Epoch 5488/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.4124549627304077
Epoch: 5488, Batch Gradient Norm: 10.280690561774637
Epoch: 5488, Batch Gradient Norm after: 10.280690561774637
Epoch 5489/10000, Prediction Accuracy = 62.138%, Loss = 0.43563724756240846
Epoch: 5489, Batch Gradient Norm: 13.2594596508623
Epoch: 5489, Batch Gradient Norm after: 13.2594596508623
Epoch 5490/10000, Prediction Accuracy = 62.098%, Loss = 0.4587798655033112
Epoch: 5490, Batch Gradient Norm: 8.876648559510398
Epoch: 5490, Batch Gradient Norm after: 8.876648559510398
Epoch 5491/10000, Prediction Accuracy = 62.117999999999995%, Loss = 0.4242204546928406
Epoch: 5491, Batch Gradient Norm: 7.7859695770905635
Epoch: 5491, Batch Gradient Norm after: 7.7859695770905635
Epoch 5492/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.4180645227432251
Epoch: 5492, Batch Gradient Norm: 9.185728606887519
Epoch: 5492, Batch Gradient Norm after: 9.185728606887519
Epoch 5493/10000, Prediction Accuracy = 62.1%, Loss = 0.42441306710243226
Epoch: 5493, Batch Gradient Norm: 9.983540325153317
Epoch: 5493, Batch Gradient Norm after: 9.983540325153317
Epoch 5494/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.429188472032547
Epoch: 5494, Batch Gradient Norm: 11.94859227313133
Epoch: 5494, Batch Gradient Norm after: 11.94859227313133
Epoch 5495/10000, Prediction Accuracy = 62.164%, Loss = 0.4453549087047577
Epoch: 5495, Batch Gradient Norm: 9.405704006331746
Epoch: 5495, Batch Gradient Norm after: 9.405704006331746
Epoch 5496/10000, Prediction Accuracy = 62.06200000000001%, Loss = 0.42845848202705383
Epoch: 5496, Batch Gradient Norm: 7.9688572165259375
Epoch: 5496, Batch Gradient Norm after: 7.9688572165259375
Epoch 5497/10000, Prediction Accuracy = 62.076%, Loss = 0.4191511571407318
Epoch: 5497, Batch Gradient Norm: 10.038473732072468
Epoch: 5497, Batch Gradient Norm after: 10.038473732072468
Epoch 5498/10000, Prediction Accuracy = 62.064%, Loss = 0.43409891724586486
Epoch: 5498, Batch Gradient Norm: 10.014293431454519
Epoch: 5498, Batch Gradient Norm after: 10.014293431454519
Epoch 5499/10000, Prediction Accuracy = 62.13000000000001%, Loss = 0.43453468680381774
Epoch: 5499, Batch Gradient Norm: 8.964779659556294
Epoch: 5499, Batch Gradient Norm after: 8.964779659556294
Epoch 5500/10000, Prediction Accuracy = 61.98%, Loss = 0.4261096239089966
Epoch: 5500, Batch Gradient Norm: 8.817423072243784
Epoch: 5500, Batch Gradient Norm after: 8.817423072243784
Epoch 5501/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.4254670560359955
Epoch: 5501, Batch Gradient Norm: 11.062842103584593
Epoch: 5501, Batch Gradient Norm after: 11.062842103584593
Epoch 5502/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.44025450348854067
Epoch: 5502, Batch Gradient Norm: 11.151369287707219
Epoch: 5502, Batch Gradient Norm after: 11.151369287707219
Epoch 5503/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.44620001912117
Epoch: 5503, Batch Gradient Norm: 11.293389974278472
Epoch: 5503, Batch Gradient Norm after: 11.293389974278472
Epoch 5504/10000, Prediction Accuracy = 62.068%, Loss = 0.44179279208183286
Epoch: 5504, Batch Gradient Norm: 11.955832978385871
Epoch: 5504, Batch Gradient Norm after: 11.955832978385871
Epoch 5505/10000, Prediction Accuracy = 62.07800000000001%, Loss = 0.44809462428092955
Epoch: 5505, Batch Gradient Norm: 11.281382225214369
Epoch: 5505, Batch Gradient Norm after: 11.281382225214369
Epoch 5506/10000, Prediction Accuracy = 62.02199999999999%, Loss = 0.44187207221984864
Epoch: 5506, Batch Gradient Norm: 9.638238813964694
Epoch: 5506, Batch Gradient Norm after: 9.638238813964694
Epoch 5507/10000, Prediction Accuracy = 62.032%, Loss = 0.4358519196510315
Epoch: 5507, Batch Gradient Norm: 8.017387984861063
Epoch: 5507, Batch Gradient Norm after: 8.017387984861063
Epoch 5508/10000, Prediction Accuracy = 62.19%, Loss = 0.4194572627544403
Epoch: 5508, Batch Gradient Norm: 9.249240145466661
Epoch: 5508, Batch Gradient Norm after: 9.249240145466661
Epoch 5509/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.4278590798377991
Epoch: 5509, Batch Gradient Norm: 8.404752603267328
Epoch: 5509, Batch Gradient Norm after: 8.404752603267328
Epoch 5510/10000, Prediction Accuracy = 62.09000000000001%, Loss = 0.42228338718414304
Epoch: 5510, Batch Gradient Norm: 9.558214578897193
Epoch: 5510, Batch Gradient Norm after: 9.558214578897193
Epoch 5511/10000, Prediction Accuracy = 62.17%, Loss = 0.4310245096683502
Epoch: 5511, Batch Gradient Norm: 7.083036553643199
Epoch: 5511, Batch Gradient Norm after: 7.083036553643199
Epoch 5512/10000, Prediction Accuracy = 62.116%, Loss = 0.4127721130847931
Epoch: 5512, Batch Gradient Norm: 9.131025866505809
Epoch: 5512, Batch Gradient Norm after: 9.131025866505809
Epoch 5513/10000, Prediction Accuracy = 62.062%, Loss = 0.42671995162963866
Epoch: 5513, Batch Gradient Norm: 8.785047293130642
Epoch: 5513, Batch Gradient Norm after: 8.785047293130642
Epoch 5514/10000, Prediction Accuracy = 62.138%, Loss = 0.4210347354412079
Epoch: 5514, Batch Gradient Norm: 12.654391447247976
Epoch: 5514, Batch Gradient Norm after: 12.654391447247976
Epoch 5515/10000, Prediction Accuracy = 61.824%, Loss = 0.4581920802593231
Epoch: 5515, Batch Gradient Norm: 8.906636555834833
Epoch: 5515, Batch Gradient Norm after: 8.906636555834833
Epoch 5516/10000, Prediction Accuracy = 62.086%, Loss = 0.425329327583313
Epoch: 5516, Batch Gradient Norm: 8.646059243302911
Epoch: 5516, Batch Gradient Norm after: 8.646059243302911
Epoch 5517/10000, Prediction Accuracy = 62.007999999999996%, Loss = 0.4234002709388733
Epoch: 5517, Batch Gradient Norm: 9.190676017097354
Epoch: 5517, Batch Gradient Norm after: 9.190676017097354
Epoch 5518/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.42638845443725587
Epoch: 5518, Batch Gradient Norm: 8.027569525974014
Epoch: 5518, Batch Gradient Norm after: 8.027569525974014
Epoch 5519/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.41863645911216735
Epoch: 5519, Batch Gradient Norm: 8.61279452923089
Epoch: 5519, Batch Gradient Norm after: 8.61279452923089
Epoch 5520/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.42256423830986023
Epoch: 5520, Batch Gradient Norm: 8.918780343964668
Epoch: 5520, Batch Gradient Norm after: 8.918780343964668
Epoch 5521/10000, Prediction Accuracy = 62.1%, Loss = 0.42564074993133544
Epoch: 5521, Batch Gradient Norm: 8.935640400626179
Epoch: 5521, Batch Gradient Norm after: 8.935640400626179
Epoch 5522/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.42553359270095825
Epoch: 5522, Batch Gradient Norm: 8.180791427039741
Epoch: 5522, Batch Gradient Norm after: 8.180791427039741
Epoch 5523/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4219123363494873
Epoch: 5523, Batch Gradient Norm: 12.307142124011227
Epoch: 5523, Batch Gradient Norm after: 12.307142124011227
Epoch 5524/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.44751436114311216
Epoch: 5524, Batch Gradient Norm: 11.819964037644022
Epoch: 5524, Batch Gradient Norm after: 11.819964037644022
Epoch 5525/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.44618836641311643
Epoch: 5525, Batch Gradient Norm: 8.903486382053737
Epoch: 5525, Batch Gradient Norm after: 8.903486382053737
Epoch 5526/10000, Prediction Accuracy = 62.036%, Loss = 0.42935737371444704
Epoch: 5526, Batch Gradient Norm: 6.5084632661881265
Epoch: 5526, Batch Gradient Norm after: 6.5084632661881265
Epoch 5527/10000, Prediction Accuracy = 62.126%, Loss = 0.4127501368522644
Epoch: 5527, Batch Gradient Norm: 10.197013700590055
Epoch: 5527, Batch Gradient Norm after: 10.197013700590055
Epoch 5528/10000, Prediction Accuracy = 62.263999999999996%, Loss = 0.430942302942276
Epoch: 5528, Batch Gradient Norm: 11.382816378355242
Epoch: 5528, Batch Gradient Norm after: 11.382816378355242
Epoch 5529/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.44034751653671267
Epoch: 5529, Batch Gradient Norm: 10.732992862586174
Epoch: 5529, Batch Gradient Norm after: 10.732992862586174
Epoch 5530/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.4376132428646088
Epoch: 5530, Batch Gradient Norm: 12.318600473389894
Epoch: 5530, Batch Gradient Norm after: 12.318600473389894
Epoch 5531/10000, Prediction Accuracy = 62.0%, Loss = 0.45269727110862734
Epoch: 5531, Batch Gradient Norm: 16.158919763279595
Epoch: 5531, Batch Gradient Norm after: 16.158919763279595
Epoch 5532/10000, Prediction Accuracy = 62.146%, Loss = 0.48795544505119326
Epoch: 5532, Batch Gradient Norm: 8.001651475908528
Epoch: 5532, Batch Gradient Norm after: 8.001651475908528
Epoch 5533/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4185691177845001
Epoch: 5533, Batch Gradient Norm: 8.454420405471067
Epoch: 5533, Batch Gradient Norm after: 8.454420405471067
Epoch 5534/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.42252522706985474
Epoch: 5534, Batch Gradient Norm: 8.977717468719591
Epoch: 5534, Batch Gradient Norm after: 8.977717468719591
Epoch 5535/10000, Prediction Accuracy = 62.12600000000001%, Loss = 0.427857905626297
Epoch: 5535, Batch Gradient Norm: 7.734451348708407
Epoch: 5535, Batch Gradient Norm after: 7.734451348708407
Epoch 5536/10000, Prediction Accuracy = 62.178%, Loss = 0.4169489502906799
Epoch: 5536, Batch Gradient Norm: 10.630956452899476
Epoch: 5536, Batch Gradient Norm after: 10.630956452899476
Epoch 5537/10000, Prediction Accuracy = 61.99000000000001%, Loss = 0.4388909816741943
Epoch: 5537, Batch Gradient Norm: 8.063746599708534
Epoch: 5537, Batch Gradient Norm after: 8.063746599708534
Epoch 5538/10000, Prediction Accuracy = 62.068%, Loss = 0.42318702340126035
Epoch: 5538, Batch Gradient Norm: 7.269586610853163
Epoch: 5538, Batch Gradient Norm after: 7.269586610853163
Epoch 5539/10000, Prediction Accuracy = 62.15%, Loss = 0.4144387185573578
Epoch: 5539, Batch Gradient Norm: 9.769481341473435
Epoch: 5539, Batch Gradient Norm after: 9.769481341473435
Epoch 5540/10000, Prediction Accuracy = 62.128%, Loss = 0.4275722622871399
Epoch: 5540, Batch Gradient Norm: 12.187274731898604
Epoch: 5540, Batch Gradient Norm after: 12.187274731898604
Epoch 5541/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.4452046036720276
Epoch: 5541, Batch Gradient Norm: 9.456068977407865
Epoch: 5541, Batch Gradient Norm after: 9.456068977407865
Epoch 5542/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4269824266433716
Epoch: 5542, Batch Gradient Norm: 8.507754032422646
Epoch: 5542, Batch Gradient Norm after: 8.507754032422646
Epoch 5543/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.4230314314365387
Epoch: 5543, Batch Gradient Norm: 7.372645093519084
Epoch: 5543, Batch Gradient Norm after: 7.372645093519084
Epoch 5544/10000, Prediction Accuracy = 62.214%, Loss = 0.41588094830513
Epoch: 5544, Batch Gradient Norm: 7.524029803185481
Epoch: 5544, Batch Gradient Norm after: 7.524029803185481
Epoch 5545/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.4155917465686798
Epoch: 5545, Batch Gradient Norm: 11.476527008674335
Epoch: 5545, Batch Gradient Norm after: 11.476527008674335
Epoch 5546/10000, Prediction Accuracy = 62.148%, Loss = 0.44193063378334047
Epoch: 5546, Batch Gradient Norm: 12.011336948101967
Epoch: 5546, Batch Gradient Norm after: 12.011336948101967
Epoch 5547/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.4511882483959198
Epoch: 5547, Batch Gradient Norm: 8.598034636837273
Epoch: 5547, Batch Gradient Norm after: 8.598034636837273
Epoch 5548/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.42062073945999146
Epoch: 5548, Batch Gradient Norm: 10.670195226556796
Epoch: 5548, Batch Gradient Norm after: 10.670195226556796
Epoch 5549/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.44195095300674436
Epoch: 5549, Batch Gradient Norm: 8.124232883885846
Epoch: 5549, Batch Gradient Norm after: 8.124232883885846
Epoch 5550/10000, Prediction Accuracy = 62.138%, Loss = 0.419921875
Epoch: 5550, Batch Gradient Norm: 10.609432817667162
Epoch: 5550, Batch Gradient Norm after: 10.609432817667162
Epoch 5551/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.43517674803733825
Epoch: 5551, Batch Gradient Norm: 9.41357132946039
Epoch: 5551, Batch Gradient Norm after: 9.41357132946039
Epoch 5552/10000, Prediction Accuracy = 62.038%, Loss = 0.4301047921180725
Epoch: 5552, Batch Gradient Norm: 9.437257342840903
Epoch: 5552, Batch Gradient Norm after: 9.437257342840903
Epoch 5553/10000, Prediction Accuracy = 62.298%, Loss = 0.4291072905063629
Epoch: 5553, Batch Gradient Norm: 10.293919955599982
Epoch: 5553, Batch Gradient Norm after: 10.293919955599982
Epoch 5554/10000, Prediction Accuracy = 62.15%, Loss = 0.4339538931846619
Epoch: 5554, Batch Gradient Norm: 10.721444882772307
Epoch: 5554, Batch Gradient Norm after: 10.721444882772307
Epoch 5555/10000, Prediction Accuracy = 62.048%, Loss = 0.4349753975868225
Epoch: 5555, Batch Gradient Norm: 11.221850081249618
Epoch: 5555, Batch Gradient Norm after: 11.221850081249618
Epoch 5556/10000, Prediction Accuracy = 62.076%, Loss = 0.43761475682258605
Epoch: 5556, Batch Gradient Norm: 9.038814751162148
Epoch: 5556, Batch Gradient Norm after: 9.038814751162148
Epoch 5557/10000, Prediction Accuracy = 62.27%, Loss = 0.4228415608406067
Epoch: 5557, Batch Gradient Norm: 7.560746769699417
Epoch: 5557, Batch Gradient Norm after: 7.560746769699417
Epoch 5558/10000, Prediction Accuracy = 62.09599999999999%, Loss = 0.41609975695610046
Epoch: 5558, Batch Gradient Norm: 9.212487338642607
Epoch: 5558, Batch Gradient Norm after: 9.212487338642607
Epoch 5559/10000, Prediction Accuracy = 61.948%, Loss = 0.42730073928833007
Epoch: 5559, Batch Gradient Norm: 8.953394800233804
Epoch: 5559, Batch Gradient Norm after: 8.953394800233804
Epoch 5560/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4242672920227051
Epoch: 5560, Batch Gradient Norm: 10.931965509012064
Epoch: 5560, Batch Gradient Norm after: 10.931965509012064
Epoch 5561/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.43884525299072263
Epoch: 5561, Batch Gradient Norm: 10.574973405912448
Epoch: 5561, Batch Gradient Norm after: 10.574973405912448
Epoch 5562/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4324052453041077
Epoch: 5562, Batch Gradient Norm: 11.437634892219164
Epoch: 5562, Batch Gradient Norm after: 11.437634892219164
Epoch 5563/10000, Prediction Accuracy = 62.21%, Loss = 0.4394516944885254
Epoch: 5563, Batch Gradient Norm: 11.332928652467732
Epoch: 5563, Batch Gradient Norm after: 11.332928652467732
Epoch 5564/10000, Prediction Accuracy = 62.126%, Loss = 0.4418833076953888
Epoch: 5564, Batch Gradient Norm: 8.191786639116605
Epoch: 5564, Batch Gradient Norm after: 8.191786639116605
Epoch 5565/10000, Prediction Accuracy = 62.22800000000001%, Loss = 0.4181017577648163
Epoch: 5565, Batch Gradient Norm: 9.051881637433763
Epoch: 5565, Batch Gradient Norm after: 9.051881637433763
Epoch 5566/10000, Prediction Accuracy = 62.290000000000006%, Loss = 0.42262032628059387
Epoch: 5566, Batch Gradient Norm: 7.743046422456407
Epoch: 5566, Batch Gradient Norm after: 7.743046422456407
Epoch 5567/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4163948237895966
Epoch: 5567, Batch Gradient Norm: 10.5310233137316
Epoch: 5567, Batch Gradient Norm after: 10.5310233137316
Epoch 5568/10000, Prediction Accuracy = 62.128%, Loss = 0.4356847405433655
Epoch: 5568, Batch Gradient Norm: 9.408207560139298
Epoch: 5568, Batch Gradient Norm after: 9.408207560139298
Epoch 5569/10000, Prediction Accuracy = 62.052%, Loss = 0.4280249059200287
Epoch: 5569, Batch Gradient Norm: 9.857033334239983
Epoch: 5569, Batch Gradient Norm after: 9.857033334239983
Epoch 5570/10000, Prediction Accuracy = 62.1%, Loss = 0.4266474783420563
Epoch: 5570, Batch Gradient Norm: 8.573124245023001
Epoch: 5570, Batch Gradient Norm after: 8.573124245023001
Epoch 5571/10000, Prediction Accuracy = 62.0%, Loss = 0.42253766655921937
Epoch: 5571, Batch Gradient Norm: 8.801488882555004
Epoch: 5571, Batch Gradient Norm after: 8.801488882555004
Epoch 5572/10000, Prediction Accuracy = 62.21%, Loss = 0.4225430369377136
Epoch: 5572, Batch Gradient Norm: 9.98125225713168
Epoch: 5572, Batch Gradient Norm after: 9.98125225713168
Epoch 5573/10000, Prediction Accuracy = 62.007999999999996%, Loss = 0.43173729777336123
Epoch: 5573, Batch Gradient Norm: 10.222572810507726
Epoch: 5573, Batch Gradient Norm after: 10.222572810507726
Epoch 5574/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4336564362049103
Epoch: 5574, Batch Gradient Norm: 8.176845380549095
Epoch: 5574, Batch Gradient Norm after: 8.176845380549095
Epoch 5575/10000, Prediction Accuracy = 62.212%, Loss = 0.4203791320323944
Epoch: 5575, Batch Gradient Norm: 7.895375136704546
Epoch: 5575, Batch Gradient Norm after: 7.895375136704546
Epoch 5576/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.41807695627212527
Epoch: 5576, Batch Gradient Norm: 11.335966189071328
Epoch: 5576, Batch Gradient Norm after: 11.335966189071328
Epoch 5577/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.4430208206176758
Epoch: 5577, Batch Gradient Norm: 11.260707150492351
Epoch: 5577, Batch Gradient Norm after: 11.260707150492351
Epoch 5578/10000, Prediction Accuracy = 62.092%, Loss = 0.43979870676994326
Epoch: 5578, Batch Gradient Norm: 9.440680805639607
Epoch: 5578, Batch Gradient Norm after: 9.440680805639607
Epoch 5579/10000, Prediction Accuracy = 62.088%, Loss = 0.42561490535736085
Epoch: 5579, Batch Gradient Norm: 10.02414539583903
Epoch: 5579, Batch Gradient Norm after: 10.02414539583903
Epoch 5580/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4288515031337738
Epoch: 5580, Batch Gradient Norm: 11.86834604860077
Epoch: 5580, Batch Gradient Norm after: 11.86834604860077
Epoch 5581/10000, Prediction Accuracy = 62.074%, Loss = 0.4474788248538971
Epoch: 5581, Batch Gradient Norm: 10.5274283467684
Epoch: 5581, Batch Gradient Norm after: 10.5274283467684
Epoch 5582/10000, Prediction Accuracy = 62.238%, Loss = 0.4376679003238678
Epoch: 5582, Batch Gradient Norm: 9.287325187810657
Epoch: 5582, Batch Gradient Norm after: 9.287325187810657
Epoch 5583/10000, Prediction Accuracy = 62.056%, Loss = 0.42548190951347353
Epoch: 5583, Batch Gradient Norm: 10.227309604706925
Epoch: 5583, Batch Gradient Norm after: 10.227309604706925
Epoch 5584/10000, Prediction Accuracy = 62.016%, Loss = 0.4296643376350403
Epoch: 5584, Batch Gradient Norm: 8.1105860179818
Epoch: 5584, Batch Gradient Norm after: 8.1105860179818
Epoch 5585/10000, Prediction Accuracy = 62.15999999999999%, Loss = 0.4166921615600586
Epoch: 5585, Batch Gradient Norm: 7.675126483945117
Epoch: 5585, Batch Gradient Norm after: 7.675126483945117
Epoch 5586/10000, Prediction Accuracy = 62.242%, Loss = 0.41481950879096985
Epoch: 5586, Batch Gradient Norm: 11.38940252441958
Epoch: 5586, Batch Gradient Norm after: 11.38940252441958
Epoch 5587/10000, Prediction Accuracy = 62.0%, Loss = 0.4387360394001007
Epoch: 5587, Batch Gradient Norm: 10.504490563306906
Epoch: 5587, Batch Gradient Norm after: 10.504490563306906
Epoch 5588/10000, Prediction Accuracy = 62.152%, Loss = 0.433812153339386
Epoch: 5588, Batch Gradient Norm: 7.994727021052965
Epoch: 5588, Batch Gradient Norm after: 7.994727021052965
Epoch 5589/10000, Prediction Accuracy = 62.162%, Loss = 0.41724455952644346
Epoch: 5589, Batch Gradient Norm: 13.294701308671941
Epoch: 5589, Batch Gradient Norm after: 13.294701308671941
Epoch 5590/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.46382567286491394
Epoch: 5590, Batch Gradient Norm: 8.737422455899843
Epoch: 5590, Batch Gradient Norm after: 8.737422455899843
Epoch 5591/10000, Prediction Accuracy = 62.136%, Loss = 0.42187689542770385
Epoch: 5591, Batch Gradient Norm: 12.272520520556935
Epoch: 5591, Batch Gradient Norm after: 12.272520520556935
Epoch 5592/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.4544606447219849
Epoch: 5592, Batch Gradient Norm: 9.068119653276598
Epoch: 5592, Batch Gradient Norm after: 9.068119653276598
Epoch 5593/10000, Prediction Accuracy = 62.136%, Loss = 0.4231817364692688
Epoch: 5593, Batch Gradient Norm: 10.292664506868523
Epoch: 5593, Batch Gradient Norm after: 10.292664506868523
Epoch 5594/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.43278048038482664
Epoch: 5594, Batch Gradient Norm: 8.560088849370372
Epoch: 5594, Batch Gradient Norm after: 8.560088849370372
Epoch 5595/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.4202269375324249
Epoch: 5595, Batch Gradient Norm: 11.78505586463306
Epoch: 5595, Batch Gradient Norm after: 11.78505586463306
Epoch 5596/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.44635645747184755
Epoch: 5596, Batch Gradient Norm: 8.18661718651093
Epoch: 5596, Batch Gradient Norm after: 8.18661718651093
Epoch 5597/10000, Prediction Accuracy = 62.11%, Loss = 0.41903448700904844
Epoch: 5597, Batch Gradient Norm: 7.323889749712506
Epoch: 5597, Batch Gradient Norm after: 7.323889749712506
Epoch 5598/10000, Prediction Accuracy = 62.298%, Loss = 0.4146742284297943
Epoch: 5598, Batch Gradient Norm: 9.393428712805143
Epoch: 5598, Batch Gradient Norm after: 9.393428712805143
Epoch 5599/10000, Prediction Accuracy = 62.11%, Loss = 0.4282627284526825
Epoch: 5599, Batch Gradient Norm: 7.711191592337218
Epoch: 5599, Batch Gradient Norm after: 7.711191592337218
Epoch 5600/10000, Prediction Accuracy = 62.166%, Loss = 0.4174416303634644
Epoch: 5600, Batch Gradient Norm: 11.061788034823204
Epoch: 5600, Batch Gradient Norm after: 11.061788034823204
Epoch 5601/10000, Prediction Accuracy = 62.14%, Loss = 0.4378724217414856
Epoch: 5601, Batch Gradient Norm: 8.200277518837844
Epoch: 5601, Batch Gradient Norm after: 8.200277518837844
Epoch 5602/10000, Prediction Accuracy = 62.114%, Loss = 0.419300776720047
Epoch: 5602, Batch Gradient Norm: 10.996845549053049
Epoch: 5602, Batch Gradient Norm after: 10.996845549053049
Epoch 5603/10000, Prediction Accuracy = 62.104%, Loss = 0.4356301188468933
Epoch: 5603, Batch Gradient Norm: 12.077195062438404
Epoch: 5603, Batch Gradient Norm after: 12.077195062438404
Epoch 5604/10000, Prediction Accuracy = 62.036%, Loss = 0.4471604585647583
Epoch: 5604, Batch Gradient Norm: 10.68041439392754
Epoch: 5604, Batch Gradient Norm after: 10.68041439392754
Epoch 5605/10000, Prediction Accuracy = 62.056%, Loss = 0.4372646927833557
Epoch: 5605, Batch Gradient Norm: 8.865471825305141
Epoch: 5605, Batch Gradient Norm after: 8.865471825305141
Epoch 5606/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4255857765674591
Epoch: 5606, Batch Gradient Norm: 11.482723069317212
Epoch: 5606, Batch Gradient Norm after: 11.482723069317212
Epoch 5607/10000, Prediction Accuracy = 61.984%, Loss = 0.4407703518867493
Epoch: 5607, Batch Gradient Norm: 11.13469588190435
Epoch: 5607, Batch Gradient Norm after: 11.13469588190435
Epoch 5608/10000, Prediction Accuracy = 62.019999999999996%, Loss = 0.4371022403240204
Epoch: 5608, Batch Gradient Norm: 10.199186371811802
Epoch: 5608, Batch Gradient Norm after: 10.199186371811802
Epoch 5609/10000, Prediction Accuracy = 62.146%, Loss = 0.4341560363769531
Epoch: 5609, Batch Gradient Norm: 6.710303060745173
Epoch: 5609, Batch Gradient Norm after: 6.710303060745173
Epoch 5610/10000, Prediction Accuracy = 62.174%, Loss = 0.4137227714061737
Epoch: 5610, Batch Gradient Norm: 7.343737551013247
Epoch: 5610, Batch Gradient Norm after: 7.343737551013247
Epoch 5611/10000, Prediction Accuracy = 62.134%, Loss = 0.41788057088851926
Epoch: 5611, Batch Gradient Norm: 6.780327735311718
Epoch: 5611, Batch Gradient Norm after: 6.780327735311718
Epoch 5612/10000, Prediction Accuracy = 62.214%, Loss = 0.41064793467521665
Epoch: 5612, Batch Gradient Norm: 11.209289812383316
Epoch: 5612, Batch Gradient Norm after: 11.209289812383316
Epoch 5613/10000, Prediction Accuracy = 62.148%, Loss = 0.4395348966121674
Epoch: 5613, Batch Gradient Norm: 9.554108174706156
Epoch: 5613, Batch Gradient Norm after: 9.554108174706156
Epoch 5614/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.4250641167163849
Epoch: 5614, Batch Gradient Norm: 10.7730188148431
Epoch: 5614, Batch Gradient Norm after: 10.7730188148431
Epoch 5615/10000, Prediction Accuracy = 62.104%, Loss = 0.4340978980064392
Epoch: 5615, Batch Gradient Norm: 10.939668572530513
Epoch: 5615, Batch Gradient Norm after: 10.939668572530513
Epoch 5616/10000, Prediction Accuracy = 62.062%, Loss = 0.43877425193786623
Epoch: 5616, Batch Gradient Norm: 10.334878345799758
Epoch: 5616, Batch Gradient Norm after: 10.334878345799758
Epoch 5617/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.4350539743900299
Epoch: 5617, Batch Gradient Norm: 9.03735969542151
Epoch: 5617, Batch Gradient Norm after: 9.03735969542151
Epoch 5618/10000, Prediction Accuracy = 62.012%, Loss = 0.4235574543476105
Epoch: 5618, Batch Gradient Norm: 9.193420002212724
Epoch: 5618, Batch Gradient Norm after: 9.193420002212724
Epoch 5619/10000, Prediction Accuracy = 62.05200000000001%, Loss = 0.4251101195812225
Epoch: 5619, Batch Gradient Norm: 8.634303916821233
Epoch: 5619, Batch Gradient Norm after: 8.634303916821233
Epoch 5620/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.4212959349155426
Epoch: 5620, Batch Gradient Norm: 10.08868447788584
Epoch: 5620, Batch Gradient Norm after: 10.08868447788584
Epoch 5621/10000, Prediction Accuracy = 62.202%, Loss = 0.4310824990272522
Epoch: 5621, Batch Gradient Norm: 11.876022856187527
Epoch: 5621, Batch Gradient Norm after: 11.876022856187527
Epoch 5622/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.44286139607429503
Epoch: 5622, Batch Gradient Norm: 10.244416401702203
Epoch: 5622, Batch Gradient Norm after: 10.244416401702203
Epoch 5623/10000, Prediction Accuracy = 62.13399999999999%, Loss = 0.4283589243888855
Epoch: 5623, Batch Gradient Norm: 7.441994045505365
Epoch: 5623, Batch Gradient Norm after: 7.441994045505365
Epoch 5624/10000, Prediction Accuracy = 62.25%, Loss = 0.4130362093448639
Epoch: 5624, Batch Gradient Norm: 8.963809201292213
Epoch: 5624, Batch Gradient Norm after: 8.963809201292213
Epoch 5625/10000, Prediction Accuracy = 62.274%, Loss = 0.42194623947143556
Epoch: 5625, Batch Gradient Norm: 10.174425678334497
Epoch: 5625, Batch Gradient Norm after: 10.174425678334497
Epoch 5626/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.4293089210987091
Epoch: 5626, Batch Gradient Norm: 10.2661429997313
Epoch: 5626, Batch Gradient Norm after: 10.2661429997313
Epoch 5627/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.43050811290740965
Epoch: 5627, Batch Gradient Norm: 7.926881651051386
Epoch: 5627, Batch Gradient Norm after: 7.926881651051386
Epoch 5628/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.41694076657295226
Epoch: 5628, Batch Gradient Norm: 10.590739872357853
Epoch: 5628, Batch Gradient Norm after: 10.590739872357853
Epoch 5629/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.4356036841869354
Epoch: 5629, Batch Gradient Norm: 10.287624375930317
Epoch: 5629, Batch Gradient Norm after: 10.287624375930317
Epoch 5630/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.43090038299560546
Epoch: 5630, Batch Gradient Norm: 11.600213306905957
Epoch: 5630, Batch Gradient Norm after: 11.600213306905957
Epoch 5631/10000, Prediction Accuracy = 62.08399999999999%, Loss = 0.4420508503913879
Epoch: 5631, Batch Gradient Norm: 7.844130515601977
Epoch: 5631, Batch Gradient Norm after: 7.844130515601977
Epoch 5632/10000, Prediction Accuracy = 62.126%, Loss = 0.41685675382614135
Epoch: 5632, Batch Gradient Norm: 9.290463986845994
Epoch: 5632, Batch Gradient Norm after: 9.290463986845994
Epoch 5633/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.42325960397720336
Epoch: 5633, Batch Gradient Norm: 13.13642890902922
Epoch: 5633, Batch Gradient Norm after: 13.13642890902922
Epoch 5634/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.45496176481246947
Epoch: 5634, Batch Gradient Norm: 10.14290616321212
Epoch: 5634, Batch Gradient Norm after: 10.14290616321212
Epoch 5635/10000, Prediction Accuracy = 62.077999999999996%, Loss = 0.4307644903659821
Epoch: 5635, Batch Gradient Norm: 9.81511668092814
Epoch: 5635, Batch Gradient Norm after: 9.81511668092814
Epoch 5636/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.42841999530792235
Epoch: 5636, Batch Gradient Norm: 9.962976063685563
Epoch: 5636, Batch Gradient Norm after: 9.962976063685563
Epoch 5637/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.4293586492538452
Epoch: 5637, Batch Gradient Norm: 9.559457579973184
Epoch: 5637, Batch Gradient Norm after: 9.559457579973184
Epoch 5638/10000, Prediction Accuracy = 62.076%, Loss = 0.427657151222229
Epoch: 5638, Batch Gradient Norm: 11.520524261018082
Epoch: 5638, Batch Gradient Norm after: 11.520524261018082
Epoch 5639/10000, Prediction Accuracy = 62.234%, Loss = 0.4382471442222595
Epoch: 5639, Batch Gradient Norm: 7.179576912433946
Epoch: 5639, Batch Gradient Norm after: 7.179576912433946
Epoch 5640/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.41126925945281984
Epoch: 5640, Batch Gradient Norm: 7.802331825420472
Epoch: 5640, Batch Gradient Norm after: 7.802331825420472
Epoch 5641/10000, Prediction Accuracy = 62.166%, Loss = 0.414146888256073
Epoch: 5641, Batch Gradient Norm: 9.535920949902973
Epoch: 5641, Batch Gradient Norm after: 9.535920949902973
Epoch 5642/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.42512025237083434
Epoch: 5642, Batch Gradient Norm: 9.32648336155435
Epoch: 5642, Batch Gradient Norm after: 9.32648336155435
Epoch 5643/10000, Prediction Accuracy = 61.94%, Loss = 0.42559831142425536
Epoch: 5643, Batch Gradient Norm: 7.227792533397498
Epoch: 5643, Batch Gradient Norm after: 7.227792533397498
Epoch 5644/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.41276058554649353
Epoch: 5644, Batch Gradient Norm: 12.110745604502261
Epoch: 5644, Batch Gradient Norm after: 12.110745604502261
Epoch 5645/10000, Prediction Accuracy = 62.186%, Loss = 0.44120573401451113
Epoch: 5645, Batch Gradient Norm: 12.834377058462605
Epoch: 5645, Batch Gradient Norm after: 12.834377058462605
Epoch 5646/10000, Prediction Accuracy = 62.278%, Loss = 0.4488768696784973
Epoch: 5646, Batch Gradient Norm: 11.176847671105648
Epoch: 5646, Batch Gradient Norm after: 11.176847671105648
Epoch 5647/10000, Prediction Accuracy = 62.21%, Loss = 0.43599075078964233
Epoch: 5647, Batch Gradient Norm: 7.727119239810549
Epoch: 5647, Batch Gradient Norm after: 7.727119239810549
Epoch 5648/10000, Prediction Accuracy = 62.126%, Loss = 0.4139938116073608
Epoch: 5648, Batch Gradient Norm: 8.071337807142605
Epoch: 5648, Batch Gradient Norm after: 8.071337807142605
Epoch 5649/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.4178087830543518
Epoch: 5649, Batch Gradient Norm: 8.73972550637467
Epoch: 5649, Batch Gradient Norm after: 8.73972550637467
Epoch 5650/10000, Prediction Accuracy = 62.146%, Loss = 0.42032169103622435
Epoch: 5650, Batch Gradient Norm: 9.764344605065038
Epoch: 5650, Batch Gradient Norm after: 9.764344605065038
Epoch 5651/10000, Prediction Accuracy = 62.098%, Loss = 0.42986873388290403
Epoch: 5651, Batch Gradient Norm: 7.835883481365187
Epoch: 5651, Batch Gradient Norm after: 7.835883481365187
Epoch 5652/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.4150081634521484
Epoch: 5652, Batch Gradient Norm: 13.013000869541486
Epoch: 5652, Batch Gradient Norm after: 13.013000869541486
Epoch 5653/10000, Prediction Accuracy = 61.972%, Loss = 0.4514919340610504
Epoch: 5653, Batch Gradient Norm: 9.877024408940995
Epoch: 5653, Batch Gradient Norm after: 9.877024408940995
Epoch 5654/10000, Prediction Accuracy = 62.226%, Loss = 0.4287366807460785
Epoch: 5654, Batch Gradient Norm: 7.040588276278862
Epoch: 5654, Batch Gradient Norm after: 7.040588276278862
Epoch 5655/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.40982837677001954
Epoch: 5655, Batch Gradient Norm: 8.201483465106424
Epoch: 5655, Batch Gradient Norm after: 8.201483465106424
Epoch 5656/10000, Prediction Accuracy = 62.18000000000001%, Loss = 0.41508619785308837
Epoch: 5656, Batch Gradient Norm: 9.096609159067935
Epoch: 5656, Batch Gradient Norm after: 9.096609159067935
Epoch 5657/10000, Prediction Accuracy = 62.052%, Loss = 0.42434488534927367
Epoch: 5657, Batch Gradient Norm: 10.712172642239022
Epoch: 5657, Batch Gradient Norm after: 10.712172642239022
Epoch 5658/10000, Prediction Accuracy = 62.05%, Loss = 0.4373339116573334
Epoch: 5658, Batch Gradient Norm: 8.405269566916033
Epoch: 5658, Batch Gradient Norm after: 8.405269566916033
Epoch 5659/10000, Prediction Accuracy = 61.988%, Loss = 0.4215633273124695
Epoch: 5659, Batch Gradient Norm: 7.275508296127968
Epoch: 5659, Batch Gradient Norm after: 7.275508296127968
Epoch 5660/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.41355223655700685
Epoch: 5660, Batch Gradient Norm: 10.202423113088702
Epoch: 5660, Batch Gradient Norm after: 10.202423113088702
Epoch 5661/10000, Prediction Accuracy = 62.3%, Loss = 0.43026604056358336
Epoch: 5661, Batch Gradient Norm: 13.312106474359595
Epoch: 5661, Batch Gradient Norm after: 13.312106474359595
Epoch 5662/10000, Prediction Accuracy = 62.178%, Loss = 0.4542593717575073
Epoch: 5662, Batch Gradient Norm: 10.369297630443837
Epoch: 5662, Batch Gradient Norm after: 10.369297630443837
Epoch 5663/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.43154372572898864
Epoch: 5663, Batch Gradient Norm: 11.244397979949724
Epoch: 5663, Batch Gradient Norm after: 11.244397979949724
Epoch 5664/10000, Prediction Accuracy = 62.024%, Loss = 0.44236997365951536
Epoch: 5664, Batch Gradient Norm: 13.24441802551262
Epoch: 5664, Batch Gradient Norm after: 13.24441802551262
Epoch 5665/10000, Prediction Accuracy = 62.31%, Loss = 0.4555314779281616
Epoch: 5665, Batch Gradient Norm: 8.354678812999541
Epoch: 5665, Batch Gradient Norm after: 8.354678812999541
Epoch 5666/10000, Prediction Accuracy = 62.19799999999999%, Loss = 0.420035308599472
Epoch: 5666, Batch Gradient Norm: 6.1876830355590595
Epoch: 5666, Batch Gradient Norm after: 6.1876830355590595
Epoch 5667/10000, Prediction Accuracy = 62.290000000000006%, Loss = 0.40911847949028013
Epoch: 5667, Batch Gradient Norm: 6.09179485360697
Epoch: 5667, Batch Gradient Norm after: 6.09179485360697
Epoch 5668/10000, Prediction Accuracy = 62.138%, Loss = 0.4058153390884399
Epoch: 5668, Batch Gradient Norm: 8.361345736184798
Epoch: 5668, Batch Gradient Norm after: 8.361345736184798
Epoch 5669/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.4173052430152893
Epoch: 5669, Batch Gradient Norm: 9.291308130644103
Epoch: 5669, Batch Gradient Norm after: 9.291308130644103
Epoch 5670/10000, Prediction Accuracy = 62.1%, Loss = 0.42384570837020874
Epoch: 5670, Batch Gradient Norm: 10.741303211689289
Epoch: 5670, Batch Gradient Norm after: 10.741303211689289
Epoch 5671/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.4350898265838623
Epoch: 5671, Batch Gradient Norm: 11.4321682597569
Epoch: 5671, Batch Gradient Norm after: 11.4321682597569
Epoch 5672/10000, Prediction Accuracy = 62.14%, Loss = 0.43880627155303953
Epoch: 5672, Batch Gradient Norm: 9.220835275041678
Epoch: 5672, Batch Gradient Norm after: 9.220835275041678
Epoch 5673/10000, Prediction Accuracy = 62.194%, Loss = 0.42194772362709043
Epoch: 5673, Batch Gradient Norm: 10.579354796429286
Epoch: 5673, Batch Gradient Norm after: 10.579354796429286
Epoch 5674/10000, Prediction Accuracy = 62.302%, Loss = 0.43048830032348634
Epoch: 5674, Batch Gradient Norm: 11.37991998825274
Epoch: 5674, Batch Gradient Norm after: 11.37991998825274
Epoch 5675/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.4418460249900818
Epoch: 5675, Batch Gradient Norm: 9.666074093851298
Epoch: 5675, Batch Gradient Norm after: 9.666074093851298
Epoch 5676/10000, Prediction Accuracy = 62.302%, Loss = 0.4246286153793335
Epoch: 5676, Batch Gradient Norm: 10.789954126540886
Epoch: 5676, Batch Gradient Norm after: 10.789954126540886
Epoch 5677/10000, Prediction Accuracy = 62.2%, Loss = 0.4322850346565247
Epoch: 5677, Batch Gradient Norm: 8.854680972352023
Epoch: 5677, Batch Gradient Norm after: 8.854680972352023
Epoch 5678/10000, Prediction Accuracy = 62.102%, Loss = 0.42223084568977354
Epoch: 5678, Batch Gradient Norm: 9.255768561669324
Epoch: 5678, Batch Gradient Norm after: 9.255768561669324
Epoch 5679/10000, Prediction Accuracy = 62.18399999999999%, Loss = 0.42306520938873293
Epoch: 5679, Batch Gradient Norm: 9.956658278734658
Epoch: 5679, Batch Gradient Norm after: 9.956658278734658
Epoch 5680/10000, Prediction Accuracy = 62.234%, Loss = 0.4294884502887726
Epoch: 5680, Batch Gradient Norm: 10.490517312834596
Epoch: 5680, Batch Gradient Norm after: 10.490517312834596
Epoch 5681/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.4288028836250305
Epoch: 5681, Batch Gradient Norm: 11.938337107491382
Epoch: 5681, Batch Gradient Norm after: 11.938337107491382
Epoch 5682/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.44118810892105104
Epoch: 5682, Batch Gradient Norm: 7.911719761308852
Epoch: 5682, Batch Gradient Norm after: 7.911719761308852
Epoch 5683/10000, Prediction Accuracy = 62.092%, Loss = 0.41617457270622255
Epoch: 5683, Batch Gradient Norm: 7.229862948696294
Epoch: 5683, Batch Gradient Norm after: 7.229862948696294
Epoch 5684/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.4104862153530121
Epoch: 5684, Batch Gradient Norm: 7.178437998781522
Epoch: 5684, Batch Gradient Norm after: 7.178437998781522
Epoch 5685/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.4101409733295441
Epoch: 5685, Batch Gradient Norm: 9.984879979236144
Epoch: 5685, Batch Gradient Norm after: 9.984879979236144
Epoch 5686/10000, Prediction Accuracy = 62.032000000000004%, Loss = 0.43217040300369264
Epoch: 5686, Batch Gradient Norm: 8.917169628652852
Epoch: 5686, Batch Gradient Norm after: 8.917169628652852
Epoch 5687/10000, Prediction Accuracy = 62.198%, Loss = 0.4248290300369263
Epoch: 5687, Batch Gradient Norm: 8.87321463556715
Epoch: 5687, Batch Gradient Norm after: 8.87321463556715
Epoch 5688/10000, Prediction Accuracy = 62.064%, Loss = 0.42001211643218994
Epoch: 5688, Batch Gradient Norm: 11.53343958204727
Epoch: 5688, Batch Gradient Norm after: 11.53343958204727
Epoch 5689/10000, Prediction Accuracy = 62.20400000000001%, Loss = 0.4362420380115509
Epoch: 5689, Batch Gradient Norm: 10.446733763645408
Epoch: 5689, Batch Gradient Norm after: 10.446733763645408
Epoch 5690/10000, Prediction Accuracy = 62.112%, Loss = 0.4309706509113312
Epoch: 5690, Batch Gradient Norm: 10.14551909326589
Epoch: 5690, Batch Gradient Norm after: 10.14551909326589
Epoch 5691/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.4301509439945221
Epoch: 5691, Batch Gradient Norm: 9.374895926427714
Epoch: 5691, Batch Gradient Norm after: 9.374895926427714
Epoch 5692/10000, Prediction Accuracy = 62.23%, Loss = 0.42488972544670106
Epoch: 5692, Batch Gradient Norm: 10.975268265734604
Epoch: 5692, Batch Gradient Norm after: 10.975268265734604
Epoch 5693/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.43733338117599485
Epoch: 5693, Batch Gradient Norm: 8.807112339033264
Epoch: 5693, Batch Gradient Norm after: 8.807112339033264
Epoch 5694/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4201954483985901
Epoch: 5694, Batch Gradient Norm: 11.231876917304058
Epoch: 5694, Batch Gradient Norm after: 11.231876917304058
Epoch 5695/10000, Prediction Accuracy = 62.188%, Loss = 0.4395821452140808
Epoch: 5695, Batch Gradient Norm: 10.133825173739714
Epoch: 5695, Batch Gradient Norm after: 10.133825173739714
Epoch 5696/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4267482578754425
Epoch: 5696, Batch Gradient Norm: 11.629024807889705
Epoch: 5696, Batch Gradient Norm after: 11.629024807889705
Epoch 5697/10000, Prediction Accuracy = 62.20400000000001%, Loss = 0.43832727074623107
Epoch: 5697, Batch Gradient Norm: 10.240419529227179
Epoch: 5697, Batch Gradient Norm after: 10.240419529227179
Epoch 5698/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4281383454799652
Epoch: 5698, Batch Gradient Norm: 11.59776960166226
Epoch: 5698, Batch Gradient Norm after: 11.59776960166226
Epoch 5699/10000, Prediction Accuracy = 62.152%, Loss = 0.44341328740119934
Epoch: 5699, Batch Gradient Norm: 6.433501495091212
Epoch: 5699, Batch Gradient Norm after: 6.433501495091212
Epoch 5700/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.4078410565853119
Epoch: 5700, Batch Gradient Norm: 10.350491630899613
Epoch: 5700, Batch Gradient Norm after: 10.350491630899613
Epoch 5701/10000, Prediction Accuracy = 62.1%, Loss = 0.43239801526069643
Epoch: 5701, Batch Gradient Norm: 7.61520673699439
Epoch: 5701, Batch Gradient Norm after: 7.61520673699439
Epoch 5702/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4119890630245209
Epoch: 5702, Batch Gradient Norm: 8.712457470198379
Epoch: 5702, Batch Gradient Norm after: 8.712457470198379
Epoch 5703/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4204894721508026
Epoch: 5703, Batch Gradient Norm: 9.51922566160499
Epoch: 5703, Batch Gradient Norm after: 9.51922566160499
Epoch 5704/10000, Prediction Accuracy = 62.11200000000001%, Loss = 0.4248478412628174
Epoch: 5704, Batch Gradient Norm: 7.41412178279098
Epoch: 5704, Batch Gradient Norm after: 7.41412178279098
Epoch 5705/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4110991895198822
Epoch: 5705, Batch Gradient Norm: 8.787922944667466
Epoch: 5705, Batch Gradient Norm after: 8.787922944667466
Epoch 5706/10000, Prediction Accuracy = 62.084%, Loss = 0.4210003614425659
Epoch: 5706, Batch Gradient Norm: 8.222832526432317
Epoch: 5706, Batch Gradient Norm after: 8.222832526432317
Epoch 5707/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.41409223675727846
Epoch: 5707, Batch Gradient Norm: 10.399964965198405
Epoch: 5707, Batch Gradient Norm after: 10.399964965198405
Epoch 5708/10000, Prediction Accuracy = 62.338%, Loss = 0.4284767210483551
Epoch: 5708, Batch Gradient Norm: 14.214044156064062
Epoch: 5708, Batch Gradient Norm after: 14.214044156064062
Epoch 5709/10000, Prediction Accuracy = 62.14%, Loss = 0.4591794967651367
Epoch: 5709, Batch Gradient Norm: 10.458336128462928
Epoch: 5709, Batch Gradient Norm after: 10.458336128462928
Epoch 5710/10000, Prediction Accuracy = 62.17%, Loss = 0.43151840567588806
Epoch: 5710, Batch Gradient Norm: 9.022482098310672
Epoch: 5710, Batch Gradient Norm after: 9.022482098310672
Epoch 5711/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.4232664942741394
Epoch: 5711, Batch Gradient Norm: 9.126037484694107
Epoch: 5711, Batch Gradient Norm after: 9.126037484694107
Epoch 5712/10000, Prediction Accuracy = 62.178%, Loss = 0.4210027575492859
Epoch: 5712, Batch Gradient Norm: 11.182766053190106
Epoch: 5712, Batch Gradient Norm after: 11.182766053190106
Epoch 5713/10000, Prediction Accuracy = 62.186%, Loss = 0.4368454158306122
Epoch: 5713, Batch Gradient Norm: 10.923726928621566
Epoch: 5713, Batch Gradient Norm after: 10.923726928621566
Epoch 5714/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4358748018741608
Epoch: 5714, Batch Gradient Norm: 10.164311660972595
Epoch: 5714, Batch Gradient Norm after: 10.164311660972595
Epoch 5715/10000, Prediction Accuracy = 62.212%, Loss = 0.4313878953456879
Epoch: 5715, Batch Gradient Norm: 7.92265717420205
Epoch: 5715, Batch Gradient Norm after: 7.92265717420205
Epoch 5716/10000, Prediction Accuracy = 62.14000000000001%, Loss = 0.4153859853744507
Epoch: 5716, Batch Gradient Norm: 9.825035943335132
Epoch: 5716, Batch Gradient Norm after: 9.825035943335132
Epoch 5717/10000, Prediction Accuracy = 62.012%, Loss = 0.4285487234592438
Epoch: 5717, Batch Gradient Norm: 7.250995244647431
Epoch: 5717, Batch Gradient Norm after: 7.250995244647431
Epoch 5718/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.41197149753570556
Epoch: 5718, Batch Gradient Norm: 8.311857376841786
Epoch: 5718, Batch Gradient Norm after: 8.311857376841786
Epoch 5719/10000, Prediction Accuracy = 62.164%, Loss = 0.41612479090690613
Epoch: 5719, Batch Gradient Norm: 8.839458302644255
Epoch: 5719, Batch Gradient Norm after: 8.839458302644255
Epoch 5720/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4199811160564423
Epoch: 5720, Batch Gradient Norm: 9.15759928864012
Epoch: 5720, Batch Gradient Norm after: 9.15759928864012
Epoch 5721/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4208972692489624
Epoch: 5721, Batch Gradient Norm: 10.75961179376033
Epoch: 5721, Batch Gradient Norm after: 10.75961179376033
Epoch 5722/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4330091059207916
Epoch: 5722, Batch Gradient Norm: 10.981256071956286
Epoch: 5722, Batch Gradient Norm after: 10.981256071956286
Epoch 5723/10000, Prediction Accuracy = 62.102%, Loss = 0.435601270198822
Epoch: 5723, Batch Gradient Norm: 9.141531509857732
Epoch: 5723, Batch Gradient Norm after: 9.141531509857732
Epoch 5724/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.421112471818924
Epoch: 5724, Batch Gradient Norm: 11.429981988951987
Epoch: 5724, Batch Gradient Norm after: 11.429981988951987
Epoch 5725/10000, Prediction Accuracy = 62.178%, Loss = 0.4395081579685211
Epoch: 5725, Batch Gradient Norm: 7.349697912929924
Epoch: 5725, Batch Gradient Norm after: 7.349697912929924
Epoch 5726/10000, Prediction Accuracy = 62.306%, Loss = 0.4109192728996277
Epoch: 5726, Batch Gradient Norm: 10.667041351910592
Epoch: 5726, Batch Gradient Norm after: 10.667041351910592
Epoch 5727/10000, Prediction Accuracy = 62.258%, Loss = 0.43324939012527464
Epoch: 5727, Batch Gradient Norm: 7.8807203754507436
Epoch: 5727, Batch Gradient Norm after: 7.8807203754507436
Epoch 5728/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.4129743278026581
Epoch: 5728, Batch Gradient Norm: 8.356973917947006
Epoch: 5728, Batch Gradient Norm after: 8.356973917947006
Epoch 5729/10000, Prediction Accuracy = 62.282%, Loss = 0.4146941423416138
Epoch: 5729, Batch Gradient Norm: 9.566001551965911
Epoch: 5729, Batch Gradient Norm after: 9.566001551965911
Epoch 5730/10000, Prediction Accuracy = 62.18799999999999%, Loss = 0.4231778621673584
Epoch: 5730, Batch Gradient Norm: 11.519389443664389
Epoch: 5730, Batch Gradient Norm after: 11.519389443664389
Epoch 5731/10000, Prediction Accuracy = 62.074%, Loss = 0.439656138420105
Epoch: 5731, Batch Gradient Norm: 8.63732707069343
Epoch: 5731, Batch Gradient Norm after: 8.63732707069343
Epoch 5732/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4192683160305023
Epoch: 5732, Batch Gradient Norm: 8.358414234042769
Epoch: 5732, Batch Gradient Norm after: 8.358414234042769
Epoch 5733/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.42061625719070433
Epoch: 5733, Batch Gradient Norm: 7.873197451340126
Epoch: 5733, Batch Gradient Norm after: 7.873197451340126
Epoch 5734/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4125990033149719
Epoch: 5734, Batch Gradient Norm: 9.720415202193706
Epoch: 5734, Batch Gradient Norm after: 9.720415202193706
Epoch 5735/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.4265835523605347
Epoch: 5735, Batch Gradient Norm: 12.58583255158064
Epoch: 5735, Batch Gradient Norm after: 12.58583255158064
Epoch 5736/10000, Prediction Accuracy = 62.112%, Loss = 0.4497079074382782
Epoch: 5736, Batch Gradient Norm: 10.79044226881836
Epoch: 5736, Batch Gradient Norm after: 10.79044226881836
Epoch 5737/10000, Prediction Accuracy = 62.262%, Loss = 0.4350581347942352
Epoch: 5737, Batch Gradient Norm: 10.38523485798308
Epoch: 5737, Batch Gradient Norm after: 10.38523485798308
Epoch 5738/10000, Prediction Accuracy = 62.088%, Loss = 0.43332942128181456
Epoch: 5738, Batch Gradient Norm: 9.415806808246344
Epoch: 5738, Batch Gradient Norm after: 9.415806808246344
Epoch 5739/10000, Prediction Accuracy = 62.10600000000001%, Loss = 0.4272465527057648
Epoch: 5739, Batch Gradient Norm: 7.72596870367595
Epoch: 5739, Batch Gradient Norm after: 7.72596870367595
Epoch 5740/10000, Prediction Accuracy = 62.134%, Loss = 0.413292133808136
Epoch: 5740, Batch Gradient Norm: 10.558278128059957
Epoch: 5740, Batch Gradient Norm after: 10.558278128059957
Epoch 5741/10000, Prediction Accuracy = 62.086%, Loss = 0.42909772992134093
Epoch: 5741, Batch Gradient Norm: 11.734653369190845
Epoch: 5741, Batch Gradient Norm after: 11.734653369190845
Epoch 5742/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.44020100235939025
Epoch: 5742, Batch Gradient Norm: 12.107220834387054
Epoch: 5742, Batch Gradient Norm after: 12.107220834387054
Epoch 5743/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.44858992099761963
Epoch: 5743, Batch Gradient Norm: 6.458035213883004
Epoch: 5743, Batch Gradient Norm after: 6.458035213883004
Epoch 5744/10000, Prediction Accuracy = 62.254%, Loss = 0.40857150554656985
Epoch: 5744, Batch Gradient Norm: 6.361925937100115
Epoch: 5744, Batch Gradient Norm after: 6.361925937100115
Epoch 5745/10000, Prediction Accuracy = 62.14200000000001%, Loss = 0.4073202431201935
Epoch: 5745, Batch Gradient Norm: 7.876845909914174
Epoch: 5745, Batch Gradient Norm after: 7.876845909914174
Epoch 5746/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4131180703639984
Epoch: 5746, Batch Gradient Norm: 11.55340251424071
Epoch: 5746, Batch Gradient Norm after: 11.55340251424071
Epoch 5747/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4359182894229889
Epoch: 5747, Batch Gradient Norm: 10.530274265202642
Epoch: 5747, Batch Gradient Norm after: 10.530274265202642
Epoch 5748/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.43185468912124636
Epoch: 5748, Batch Gradient Norm: 11.193790606811751
Epoch: 5748, Batch Gradient Norm after: 11.193790606811751
Epoch 5749/10000, Prediction Accuracy = 62.086%, Loss = 0.4386013805866241
Epoch: 5749, Batch Gradient Norm: 11.160164229369638
Epoch: 5749, Batch Gradient Norm after: 11.160164229369638
Epoch 5750/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.4330484449863434
Epoch: 5750, Batch Gradient Norm: 8.10933851040902
Epoch: 5750, Batch Gradient Norm after: 8.10933851040902
Epoch 5751/10000, Prediction Accuracy = 62.169999999999995%, Loss = 0.4143452823162079
Epoch: 5751, Batch Gradient Norm: 7.559906213829499
Epoch: 5751, Batch Gradient Norm after: 7.559906213829499
Epoch 5752/10000, Prediction Accuracy = 62.182%, Loss = 0.4118127882480621
Epoch: 5752, Batch Gradient Norm: 8.203366983750852
Epoch: 5752, Batch Gradient Norm after: 8.203366983750852
Epoch 5753/10000, Prediction Accuracy = 62.234%, Loss = 0.41642929911613463
Epoch: 5753, Batch Gradient Norm: 11.023981670740513
Epoch: 5753, Batch Gradient Norm after: 11.023981670740513
Epoch 5754/10000, Prediction Accuracy = 62.202%, Loss = 0.4363285958766937
Epoch: 5754, Batch Gradient Norm: 11.156837629427839
Epoch: 5754, Batch Gradient Norm after: 11.156837629427839
Epoch 5755/10000, Prediction Accuracy = 62.18000000000001%, Loss = 0.4344880521297455
Epoch: 5755, Batch Gradient Norm: 10.896675709478147
Epoch: 5755, Batch Gradient Norm after: 10.896675709478147
Epoch 5756/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.43968344330787656
Epoch: 5756, Batch Gradient Norm: 8.897030905815257
Epoch: 5756, Batch Gradient Norm after: 8.897030905815257
Epoch 5757/10000, Prediction Accuracy = 62.174%, Loss = 0.42167606949806213
Epoch: 5757, Batch Gradient Norm: 7.786191191252311
Epoch: 5757, Batch Gradient Norm after: 7.786191191252311
Epoch 5758/10000, Prediction Accuracy = 62.15%, Loss = 0.41285076141357424
Epoch: 5758, Batch Gradient Norm: 11.508764788448223
Epoch: 5758, Batch Gradient Norm after: 11.508764788448223
Epoch 5759/10000, Prediction Accuracy = 62.164%, Loss = 0.43808541297912595
Epoch: 5759, Batch Gradient Norm: 10.988808163830246
Epoch: 5759, Batch Gradient Norm after: 10.988808163830246
Epoch 5760/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4345962405204773
Epoch: 5760, Batch Gradient Norm: 9.06637047574571
Epoch: 5760, Batch Gradient Norm after: 9.06637047574571
Epoch 5761/10000, Prediction Accuracy = 62.076%, Loss = 0.4218619465827942
Epoch: 5761, Batch Gradient Norm: 12.059957819538141
Epoch: 5761, Batch Gradient Norm after: 12.059957819538141
Epoch 5762/10000, Prediction Accuracy = 62.004%, Loss = 0.4459005296230316
Epoch: 5762, Batch Gradient Norm: 12.82508032068616
Epoch: 5762, Batch Gradient Norm after: 12.82508032068616
Epoch 5763/10000, Prediction Accuracy = 62.324%, Loss = 0.44857377409934995
Epoch: 5763, Batch Gradient Norm: 8.66491026141296
Epoch: 5763, Batch Gradient Norm after: 8.66491026141296
Epoch 5764/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41540567874908446
Epoch: 5764, Batch Gradient Norm: 9.009968828481027
Epoch: 5764, Batch Gradient Norm after: 9.009968828481027
Epoch 5765/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.41719200611114504
Epoch: 5765, Batch Gradient Norm: 9.695670259574587
Epoch: 5765, Batch Gradient Norm after: 9.695670259574587
Epoch 5766/10000, Prediction Accuracy = 62.174%, Loss = 0.42434367537498474
Epoch: 5766, Batch Gradient Norm: 9.127683575822685
Epoch: 5766, Batch Gradient Norm after: 9.127683575822685
Epoch 5767/10000, Prediction Accuracy = 62.17%, Loss = 0.42027865052223207
Epoch: 5767, Batch Gradient Norm: 10.36545345970469
Epoch: 5767, Batch Gradient Norm after: 10.36545345970469
Epoch 5768/10000, Prediction Accuracy = 62.14%, Loss = 0.42852953672409055
Epoch: 5768, Batch Gradient Norm: 9.439601475379023
Epoch: 5768, Batch Gradient Norm after: 9.439601475379023
Epoch 5769/10000, Prediction Accuracy = 62.176%, Loss = 0.4257944881916046
Epoch: 5769, Batch Gradient Norm: 6.596836519866613
Epoch: 5769, Batch Gradient Norm after: 6.596836519866613
Epoch 5770/10000, Prediction Accuracy = 62.206%, Loss = 0.4062195956707001
Epoch: 5770, Batch Gradient Norm: 11.491217326057793
Epoch: 5770, Batch Gradient Norm after: 11.491217326057793
Epoch 5771/10000, Prediction Accuracy = 62.084%, Loss = 0.4442832052707672
Epoch: 5771, Batch Gradient Norm: 7.329907902161819
Epoch: 5771, Batch Gradient Norm after: 7.329907902161819
Epoch 5772/10000, Prediction Accuracy = 62.298%, Loss = 0.4092811048030853
Epoch: 5772, Batch Gradient Norm: 8.628208827581023
Epoch: 5772, Batch Gradient Norm after: 8.628208827581023
Epoch 5773/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.4157841384410858
Epoch: 5773, Batch Gradient Norm: 12.464477769318455
Epoch: 5773, Batch Gradient Norm after: 12.464477769318455
Epoch 5774/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.44757758378982543
Epoch: 5774, Batch Gradient Norm: 9.065395457462566
Epoch: 5774, Batch Gradient Norm after: 9.065395457462566
Epoch 5775/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.4189425528049469
Epoch: 5775, Batch Gradient Norm: 9.180475980528831
Epoch: 5775, Batch Gradient Norm after: 9.180475980528831
Epoch 5776/10000, Prediction Accuracy = 62.19799999999999%, Loss = 0.4191334962844849
Epoch: 5776, Batch Gradient Norm: 9.47568730555232
Epoch: 5776, Batch Gradient Norm after: 9.47568730555232
Epoch 5777/10000, Prediction Accuracy = 62.14399999999999%, Loss = 0.4236918568611145
Epoch: 5777, Batch Gradient Norm: 10.610757905996675
Epoch: 5777, Batch Gradient Norm after: 10.610757905996675
Epoch 5778/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.43256567120552064
Epoch: 5778, Batch Gradient Norm: 8.484267629917692
Epoch: 5778, Batch Gradient Norm after: 8.484267629917692
Epoch 5779/10000, Prediction Accuracy = 62.226%, Loss = 0.4169916570186615
Epoch: 5779, Batch Gradient Norm: 8.013674802106056
Epoch: 5779, Batch Gradient Norm after: 8.013674802106056
Epoch 5780/10000, Prediction Accuracy = 62.214%, Loss = 0.4171712756156921
Epoch: 5780, Batch Gradient Norm: 11.203471104567704
Epoch: 5780, Batch Gradient Norm after: 11.203471104567704
Epoch 5781/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.43566717505455016
Epoch: 5781, Batch Gradient Norm: 13.41434694796126
Epoch: 5781, Batch Gradient Norm after: 13.41434694796126
Epoch 5782/10000, Prediction Accuracy = 62.24000000000001%, Loss = 0.4546131372451782
Epoch: 5782, Batch Gradient Norm: 11.264281898268564
Epoch: 5782, Batch Gradient Norm after: 11.264281898268564
Epoch 5783/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.4366215705871582
Epoch: 5783, Batch Gradient Norm: 12.193537362697567
Epoch: 5783, Batch Gradient Norm after: 12.193537362697567
Epoch 5784/10000, Prediction Accuracy = 62.274%, Loss = 0.444500732421875
Epoch: 5784, Batch Gradient Norm: 7.921115499422252
Epoch: 5784, Batch Gradient Norm after: 7.921115499422252
Epoch 5785/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4130731463432312
Epoch: 5785, Batch Gradient Norm: 6.453785230563535
Epoch: 5785, Batch Gradient Norm after: 6.453785230563535
Epoch 5786/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4053184807300568
Epoch: 5786, Batch Gradient Norm: 7.375982976218913
Epoch: 5786, Batch Gradient Norm after: 7.375982976218913
Epoch 5787/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4103790044784546
Epoch: 5787, Batch Gradient Norm: 9.001469387261615
Epoch: 5787, Batch Gradient Norm after: 9.001469387261615
Epoch 5788/10000, Prediction Accuracy = 62.102%, Loss = 0.4195566654205322
Epoch: 5788, Batch Gradient Norm: 7.478411262081838
Epoch: 5788, Batch Gradient Norm after: 7.478411262081838
Epoch 5789/10000, Prediction Accuracy = 62.19%, Loss = 0.410284823179245
Epoch: 5789, Batch Gradient Norm: 10.280495329651204
Epoch: 5789, Batch Gradient Norm after: 10.280495329651204
Epoch 5790/10000, Prediction Accuracy = 62.122%, Loss = 0.42834978103637694
Epoch: 5790, Batch Gradient Norm: 10.412361595240153
Epoch: 5790, Batch Gradient Norm after: 10.412361595240153
Epoch 5791/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.42792701721191406
Epoch: 5791, Batch Gradient Norm: 9.533614228170377
Epoch: 5791, Batch Gradient Norm after: 9.533614228170377
Epoch 5792/10000, Prediction Accuracy = 62.10600000000001%, Loss = 0.4217206180095673
Epoch: 5792, Batch Gradient Norm: 10.008825820819483
Epoch: 5792, Batch Gradient Norm after: 10.008825820819483
Epoch 5793/10000, Prediction Accuracy = 62.222%, Loss = 0.4242147088050842
Epoch: 5793, Batch Gradient Norm: 10.92394554532889
Epoch: 5793, Batch Gradient Norm after: 10.92394554532889
Epoch 5794/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.43403984904289244
Epoch: 5794, Batch Gradient Norm: 9.22540250819618
Epoch: 5794, Batch Gradient Norm after: 9.22540250819618
Epoch 5795/10000, Prediction Accuracy = 62.162%, Loss = 0.4207990109920502
Epoch: 5795, Batch Gradient Norm: 9.51999052158145
Epoch: 5795, Batch Gradient Norm after: 9.51999052158145
Epoch 5796/10000, Prediction Accuracy = 62.138%, Loss = 0.4225085496902466
Epoch: 5796, Batch Gradient Norm: 8.274396923897243
Epoch: 5796, Batch Gradient Norm after: 8.274396923897243
Epoch 5797/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.4130465567111969
Epoch: 5797, Batch Gradient Norm: 8.869009975976638
Epoch: 5797, Batch Gradient Norm after: 8.869009975976638
Epoch 5798/10000, Prediction Accuracy = 62.162%, Loss = 0.41742944717407227
Epoch: 5798, Batch Gradient Norm: 7.9023897167199415
Epoch: 5798, Batch Gradient Norm after: 7.9023897167199415
Epoch 5799/10000, Prediction Accuracy = 62.324%, Loss = 0.41325558423995973
Epoch: 5799, Batch Gradient Norm: 9.837217319396176
Epoch: 5799, Batch Gradient Norm after: 9.837217319396176
Epoch 5800/10000, Prediction Accuracy = 62.194%, Loss = 0.4256582796573639
Epoch: 5800, Batch Gradient Norm: 11.652870787677577
Epoch: 5800, Batch Gradient Norm after: 11.652870787677577
Epoch 5801/10000, Prediction Accuracy = 62.02199999999999%, Loss = 0.43870940804481506
Epoch: 5801, Batch Gradient Norm: 10.572395781595239
Epoch: 5801, Batch Gradient Norm after: 10.572395781595239
Epoch 5802/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.4297347366809845
Epoch: 5802, Batch Gradient Norm: 12.162510645917388
Epoch: 5802, Batch Gradient Norm after: 12.162510645917388
Epoch 5803/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4395200848579407
Epoch: 5803, Batch Gradient Norm: 11.968866209422275
Epoch: 5803, Batch Gradient Norm after: 11.968866209422275
Epoch 5804/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.4404581308364868
Epoch: 5804, Batch Gradient Norm: 11.30989893583777
Epoch: 5804, Batch Gradient Norm after: 11.30989893583777
Epoch 5805/10000, Prediction Accuracy = 62.374%, Loss = 0.43499099612236025
Epoch: 5805, Batch Gradient Norm: 9.889153735915682
Epoch: 5805, Batch Gradient Norm after: 9.889153735915682
Epoch 5806/10000, Prediction Accuracy = 62.298%, Loss = 0.42602850794792174
Epoch: 5806, Batch Gradient Norm: 8.163695100249173
Epoch: 5806, Batch Gradient Norm after: 8.163695100249173
Epoch 5807/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.4119135200977325
Epoch: 5807, Batch Gradient Norm: 8.98207631287359
Epoch: 5807, Batch Gradient Norm after: 8.98207631287359
Epoch 5808/10000, Prediction Accuracy = 62.14399999999999%, Loss = 0.4159325361251831
Epoch: 5808, Batch Gradient Norm: 11.563176076810983
Epoch: 5808, Batch Gradient Norm after: 11.563176076810983
Epoch 5809/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.4403291165828705
Epoch: 5809, Batch Gradient Norm: 8.515367956930627
Epoch: 5809, Batch Gradient Norm after: 8.515367956930627
Epoch 5810/10000, Prediction Accuracy = 62.226%, Loss = 0.41669992804527284
Epoch: 5810, Batch Gradient Norm: 8.8908011554257
Epoch: 5810, Batch Gradient Norm after: 8.8908011554257
Epoch 5811/10000, Prediction Accuracy = 62.25999999999999%, Loss = 0.41764354705810547
Epoch: 5811, Batch Gradient Norm: 9.647125490919032
Epoch: 5811, Batch Gradient Norm after: 9.647125490919032
Epoch 5812/10000, Prediction Accuracy = 62.306000000000004%, Loss = 0.42274634838104247
Epoch: 5812, Batch Gradient Norm: 7.369858684048995
Epoch: 5812, Batch Gradient Norm after: 7.369858684048995
Epoch 5813/10000, Prediction Accuracy = 62.18399999999999%, Loss = 0.40903329849243164
Epoch: 5813, Batch Gradient Norm: 10.734932929388279
Epoch: 5813, Batch Gradient Norm after: 10.734932929388279
Epoch 5814/10000, Prediction Accuracy = 62.176%, Loss = 0.43252156376838685
Epoch: 5814, Batch Gradient Norm: 10.213767165495698
Epoch: 5814, Batch Gradient Norm after: 10.213767165495698
Epoch 5815/10000, Prediction Accuracy = 62.18599999999999%, Loss = 0.4314228892326355
Epoch: 5815, Batch Gradient Norm: 8.429655132875475
Epoch: 5815, Batch Gradient Norm after: 8.429655132875475
Epoch 5816/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.41686047315597535
Epoch: 5816, Batch Gradient Norm: 7.729669476618498
Epoch: 5816, Batch Gradient Norm after: 7.729669476618498
Epoch 5817/10000, Prediction Accuracy = 62.188%, Loss = 0.4107478022575378
Epoch: 5817, Batch Gradient Norm: 11.611325243660694
Epoch: 5817, Batch Gradient Norm after: 11.611325243660694
Epoch 5818/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.44136176705360414
Epoch: 5818, Batch Gradient Norm: 7.155931019481657
Epoch: 5818, Batch Gradient Norm after: 7.155931019481657
Epoch 5819/10000, Prediction Accuracy = 62.214%, Loss = 0.4108668327331543
Epoch: 5819, Batch Gradient Norm: 7.8486749183107865
Epoch: 5819, Batch Gradient Norm after: 7.8486749183107865
Epoch 5820/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.41159203052520754
Epoch: 5820, Batch Gradient Norm: 9.559355679197328
Epoch: 5820, Batch Gradient Norm after: 9.559355679197328
Epoch 5821/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.4192675054073334
Epoch: 5821, Batch Gradient Norm: 9.86353986444322
Epoch: 5821, Batch Gradient Norm after: 9.86353986444322
Epoch 5822/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.4265137791633606
Epoch: 5822, Batch Gradient Norm: 9.67047959310216
Epoch: 5822, Batch Gradient Norm after: 9.67047959310216
Epoch 5823/10000, Prediction Accuracy = 62.1%, Loss = 0.42243454456329343
Epoch: 5823, Batch Gradient Norm: 13.125054335228333
Epoch: 5823, Batch Gradient Norm after: 13.125054335228333
Epoch 5824/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.44789875745773317
Epoch: 5824, Batch Gradient Norm: 9.177468769238763
Epoch: 5824, Batch Gradient Norm after: 9.177468769238763
Epoch 5825/10000, Prediction Accuracy = 62.194%, Loss = 0.41844711303710935
Epoch: 5825, Batch Gradient Norm: 8.375677988595955
Epoch: 5825, Batch Gradient Norm after: 8.375677988595955
Epoch 5826/10000, Prediction Accuracy = 62.186%, Loss = 0.4172175943851471
Epoch: 5826, Batch Gradient Norm: 9.687934859097139
Epoch: 5826, Batch Gradient Norm after: 9.687934859097139
Epoch 5827/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4237777888774872
Epoch: 5827, Batch Gradient Norm: 10.456677058654613
Epoch: 5827, Batch Gradient Norm after: 10.456677058654613
Epoch 5828/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4290307343006134
Epoch: 5828, Batch Gradient Norm: 10.294930735530833
Epoch: 5828, Batch Gradient Norm after: 10.294930735530833
Epoch 5829/10000, Prediction Accuracy = 62.158%, Loss = 0.4323420703411102
Epoch: 5829, Batch Gradient Norm: 8.90355648719843
Epoch: 5829, Batch Gradient Norm after: 8.90355648719843
Epoch 5830/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.42181879878044126
Epoch: 5830, Batch Gradient Norm: 7.70700852354162
Epoch: 5830, Batch Gradient Norm after: 7.70700852354162
Epoch 5831/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4130432903766632
Epoch: 5831, Batch Gradient Norm: 5.796229366223127
Epoch: 5831, Batch Gradient Norm after: 5.796229366223127
Epoch 5832/10000, Prediction Accuracy = 62.34599999999999%, Loss = 0.4011542975902557
Epoch: 5832, Batch Gradient Norm: 9.008827228533503
Epoch: 5832, Batch Gradient Norm after: 9.008827228533503
Epoch 5833/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.41706290245056155
Epoch: 5833, Batch Gradient Norm: 8.13311921877928
Epoch: 5833, Batch Gradient Norm after: 8.13311921877928
Epoch 5834/10000, Prediction Accuracy = 62.336%, Loss = 0.41110050678253174
Epoch: 5834, Batch Gradient Norm: 12.142104470941376
Epoch: 5834, Batch Gradient Norm after: 12.142104470941376
Epoch 5835/10000, Prediction Accuracy = 62.282%, Loss = 0.44163362979888915
Epoch: 5835, Batch Gradient Norm: 11.401763775534526
Epoch: 5835, Batch Gradient Norm after: 11.401763775534526
Epoch 5836/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.43188588619232177
Epoch: 5836, Batch Gradient Norm: 10.866548966603522
Epoch: 5836, Batch Gradient Norm after: 10.866548966603522
Epoch 5837/10000, Prediction Accuracy = 62.136%, Loss = 0.4290115892887115
Epoch: 5837, Batch Gradient Norm: 10.498492905319543
Epoch: 5837, Batch Gradient Norm after: 10.498492905319543
Epoch 5838/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.42407646775245667
Epoch: 5838, Batch Gradient Norm: 12.8096344812149
Epoch: 5838, Batch Gradient Norm after: 12.8096344812149
Epoch 5839/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.44602821469306947
Epoch: 5839, Batch Gradient Norm: 10.202936069945114
Epoch: 5839, Batch Gradient Norm after: 10.202936069945114
Epoch 5840/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.4255873322486877
Epoch: 5840, Batch Gradient Norm: 9.755355136385807
Epoch: 5840, Batch Gradient Norm after: 9.755355136385807
Epoch 5841/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.42206438183784484
Epoch: 5841, Batch Gradient Norm: 9.667784219719527
Epoch: 5841, Batch Gradient Norm after: 9.667784219719527
Epoch 5842/10000, Prediction Accuracy = 62.06600000000001%, Loss = 0.42959619164466856
Epoch: 5842, Batch Gradient Norm: 6.120598882930949
Epoch: 5842, Batch Gradient Norm after: 6.120598882930949
Epoch 5843/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.40224234461784364
Epoch: 5843, Batch Gradient Norm: 9.142403317732771
Epoch: 5843, Batch Gradient Norm after: 9.142403317732771
Epoch 5844/10000, Prediction Accuracy = 62.162%, Loss = 0.41755003929138185
Epoch: 5844, Batch Gradient Norm: 12.089603287224259
Epoch: 5844, Batch Gradient Norm after: 12.089603287224259
Epoch 5845/10000, Prediction Accuracy = 62.230000000000004%, Loss = 0.44140003323554994
Epoch: 5845, Batch Gradient Norm: 7.497541391573493
Epoch: 5845, Batch Gradient Norm after: 7.497541391573493
Epoch 5846/10000, Prediction Accuracy = 62.426%, Loss = 0.4080872476100922
Epoch: 5846, Batch Gradient Norm: 7.257455386434893
Epoch: 5846, Batch Gradient Norm after: 7.257455386434893
Epoch 5847/10000, Prediction Accuracy = 62.158%, Loss = 0.4087755262851715
Epoch: 5847, Batch Gradient Norm: 12.473411835576472
Epoch: 5847, Batch Gradient Norm after: 12.473411835576472
Epoch 5848/10000, Prediction Accuracy = 62.08%, Loss = 0.44313706159591676
Epoch: 5848, Batch Gradient Norm: 12.09206425830292
Epoch: 5848, Batch Gradient Norm after: 12.09206425830292
Epoch 5849/10000, Prediction Accuracy = 62.254%, Loss = 0.4423631727695465
Epoch: 5849, Batch Gradient Norm: 7.205826129334963
Epoch: 5849, Batch Gradient Norm after: 7.205826129334963
Epoch 5850/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.40747876167297364
Epoch: 5850, Batch Gradient Norm: 10.130983159694441
Epoch: 5850, Batch Gradient Norm after: 10.130983159694441
Epoch 5851/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4283867061138153
Epoch: 5851, Batch Gradient Norm: 7.556286555218107
Epoch: 5851, Batch Gradient Norm after: 7.556286555218107
Epoch 5852/10000, Prediction Accuracy = 62.266%, Loss = 0.40789555907249453
Epoch: 5852, Batch Gradient Norm: 9.418022714822733
Epoch: 5852, Batch Gradient Norm after: 9.418022714822733
Epoch 5853/10000, Prediction Accuracy = 62.174%, Loss = 0.42137869596481325
Epoch: 5853, Batch Gradient Norm: 8.931091080964645
Epoch: 5853, Batch Gradient Norm after: 8.931091080964645
Epoch 5854/10000, Prediction Accuracy = 62.24399999999999%, Loss = 0.4198509931564331
Epoch: 5854, Batch Gradient Norm: 9.493624020680267
Epoch: 5854, Batch Gradient Norm after: 9.493624020680267
Epoch 5855/10000, Prediction Accuracy = 62.234%, Loss = 0.420475047826767
Epoch: 5855, Batch Gradient Norm: 12.935449607037619
Epoch: 5855, Batch Gradient Norm after: 12.935449607037619
Epoch 5856/10000, Prediction Accuracy = 62.164%, Loss = 0.4503591001033783
Epoch: 5856, Batch Gradient Norm: 10.504136694648194
Epoch: 5856, Batch Gradient Norm after: 10.504136694648194
Epoch 5857/10000, Prediction Accuracy = 62.052%, Loss = 0.42989999055862427
Epoch: 5857, Batch Gradient Norm: 7.891928797142734
Epoch: 5857, Batch Gradient Norm after: 7.891928797142734
Epoch 5858/10000, Prediction Accuracy = 62.25%, Loss = 0.41240512728691103
Epoch: 5858, Batch Gradient Norm: 11.620817361706159
Epoch: 5858, Batch Gradient Norm after: 11.620817361706159
Epoch 5859/10000, Prediction Accuracy = 62.35%, Loss = 0.4367865800857544
Epoch: 5859, Batch Gradient Norm: 8.427208655302781
Epoch: 5859, Batch Gradient Norm after: 8.427208655302781
Epoch 5860/10000, Prediction Accuracy = 62.312%, Loss = 0.4132820665836334
Epoch: 5860, Batch Gradient Norm: 10.754319905707312
Epoch: 5860, Batch Gradient Norm after: 10.754319905707312
Epoch 5861/10000, Prediction Accuracy = 62.226%, Loss = 0.4307185113430023
Epoch: 5861, Batch Gradient Norm: 8.00030825843575
Epoch: 5861, Batch Gradient Norm after: 8.00030825843575
Epoch 5862/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.4126801908016205
Epoch: 5862, Batch Gradient Norm: 9.96030440712608
Epoch: 5862, Batch Gradient Norm after: 9.96030440712608
Epoch 5863/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.42546404600143434
Epoch: 5863, Batch Gradient Norm: 9.733019056483503
Epoch: 5863, Batch Gradient Norm after: 9.733019056483503
Epoch 5864/10000, Prediction Accuracy = 62.274%, Loss = 0.42275407910346985
Epoch: 5864, Batch Gradient Norm: 8.85749544705079
Epoch: 5864, Batch Gradient Norm after: 8.85749544705079
Epoch 5865/10000, Prediction Accuracy = 62.152%, Loss = 0.4178297698497772
Epoch: 5865, Batch Gradient Norm: 6.718745146036117
Epoch: 5865, Batch Gradient Norm after: 6.718745146036117
Epoch 5866/10000, Prediction Accuracy = 62.31%, Loss = 0.4057005286216736
Epoch: 5866, Batch Gradient Norm: 10.315397060531387
Epoch: 5866, Batch Gradient Norm after: 10.315397060531387
Epoch 5867/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4252319872379303
Epoch: 5867, Batch Gradient Norm: 9.569075782824335
Epoch: 5867, Batch Gradient Norm after: 9.569075782824335
Epoch 5868/10000, Prediction Accuracy = 62.164%, Loss = 0.4230992555618286
Epoch: 5868, Batch Gradient Norm: 10.02607115851241
Epoch: 5868, Batch Gradient Norm after: 10.02607115851241
Epoch 5869/10000, Prediction Accuracy = 62.262%, Loss = 0.42463778257369994
Epoch: 5869, Batch Gradient Norm: 12.784192246983832
Epoch: 5869, Batch Gradient Norm after: 12.784192246983832
Epoch 5870/10000, Prediction Accuracy = 62.362%, Loss = 0.4429320752620697
Epoch: 5870, Batch Gradient Norm: 10.202430893118173
Epoch: 5870, Batch Gradient Norm after: 10.202430893118173
Epoch 5871/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.42624853253364564
Epoch: 5871, Batch Gradient Norm: 8.207647712702961
Epoch: 5871, Batch Gradient Norm after: 8.207647712702961
Epoch 5872/10000, Prediction Accuracy = 62.302%, Loss = 0.4121867060661316
Epoch: 5872, Batch Gradient Norm: 9.796948482138676
Epoch: 5872, Batch Gradient Norm after: 9.796948482138676
Epoch 5873/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.42423213720321656
Epoch: 5873, Batch Gradient Norm: 6.888315677157586
Epoch: 5873, Batch Gradient Norm after: 6.888315677157586
Epoch 5874/10000, Prediction Accuracy = 62.41400000000001%, Loss = 0.4048092007637024
Epoch: 5874, Batch Gradient Norm: 9.300763145150778
Epoch: 5874, Batch Gradient Norm after: 9.300763145150778
Epoch 5875/10000, Prediction Accuracy = 62.23%, Loss = 0.418242746591568
Epoch: 5875, Batch Gradient Norm: 12.80604373379337
Epoch: 5875, Batch Gradient Norm after: 12.80604373379337
Epoch 5876/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4459704041481018
Epoch: 5876, Batch Gradient Norm: 9.401279239042506
Epoch: 5876, Batch Gradient Norm after: 9.401279239042506
Epoch 5877/10000, Prediction Accuracy = 62.128%, Loss = 0.4183929204940796
Epoch: 5877, Batch Gradient Norm: 10.152188019739546
Epoch: 5877, Batch Gradient Norm after: 10.152188019739546
Epoch 5878/10000, Prediction Accuracy = 62.126%, Loss = 0.4228893995285034
Epoch: 5878, Batch Gradient Norm: 11.567296082774474
Epoch: 5878, Batch Gradient Norm after: 11.567296082774474
Epoch 5879/10000, Prediction Accuracy = 62.126%, Loss = 0.43463895916938783
Epoch: 5879, Batch Gradient Norm: 10.570438136824606
Epoch: 5879, Batch Gradient Norm after: 10.570438136824606
Epoch 5880/10000, Prediction Accuracy = 62.298%, Loss = 0.4277209579944611
Epoch: 5880, Batch Gradient Norm: 10.774296705152949
Epoch: 5880, Batch Gradient Norm after: 10.774296705152949
Epoch 5881/10000, Prediction Accuracy = 62.306000000000004%, Loss = 0.4274941861629486
Epoch: 5881, Batch Gradient Norm: 10.421514256510472
Epoch: 5881, Batch Gradient Norm after: 10.421514256510472
Epoch 5882/10000, Prediction Accuracy = 62.39799999999999%, Loss = 0.42702924013137816
Epoch: 5882, Batch Gradient Norm: 8.995900298181947
Epoch: 5882, Batch Gradient Norm after: 8.995900298181947
Epoch 5883/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.4175565540790558
Epoch: 5883, Batch Gradient Norm: 9.094768885408637
Epoch: 5883, Batch Gradient Norm after: 9.094768885408637
Epoch 5884/10000, Prediction Accuracy = 62.2%, Loss = 0.42251108288764955
Epoch: 5884, Batch Gradient Norm: 8.12377996937713
Epoch: 5884, Batch Gradient Norm after: 8.12377996937713
Epoch 5885/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.41260199546813964
Epoch: 5885, Batch Gradient Norm: 8.244686489835166
Epoch: 5885, Batch Gradient Norm after: 8.244686489835166
Epoch 5886/10000, Prediction Accuracy = 62.126%, Loss = 0.413971072435379
Epoch: 5886, Batch Gradient Norm: 7.361249981045873
Epoch: 5886, Batch Gradient Norm after: 7.361249981045873
Epoch 5887/10000, Prediction Accuracy = 62.152%, Loss = 0.4074113130569458
Epoch: 5887, Batch Gradient Norm: 10.563840423019165
Epoch: 5887, Batch Gradient Norm after: 10.563840423019165
Epoch 5888/10000, Prediction Accuracy = 62.218%, Loss = 0.43018176555633547
Epoch: 5888, Batch Gradient Norm: 13.022093266284953
Epoch: 5888, Batch Gradient Norm after: 13.022093266284953
Epoch 5889/10000, Prediction Accuracy = 62.278%, Loss = 0.4457237899303436
Epoch: 5889, Batch Gradient Norm: 10.694314811400897
Epoch: 5889, Batch Gradient Norm after: 10.694314811400897
Epoch 5890/10000, Prediction Accuracy = 62.138%, Loss = 0.428388512134552
Epoch: 5890, Batch Gradient Norm: 8.878375129125534
Epoch: 5890, Batch Gradient Norm after: 8.878375129125534
Epoch 5891/10000, Prediction Accuracy = 62.282%, Loss = 0.41706153750419617
Epoch: 5891, Batch Gradient Norm: 8.0110025964335
Epoch: 5891, Batch Gradient Norm after: 8.0110025964335
Epoch 5892/10000, Prediction Accuracy = 62.33399999999999%, Loss = 0.411671906709671
Epoch: 5892, Batch Gradient Norm: 8.565708074192987
Epoch: 5892, Batch Gradient Norm after: 8.565708074192987
Epoch 5893/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.4153487622737885
Epoch: 5893, Batch Gradient Norm: 10.054819769221417
Epoch: 5893, Batch Gradient Norm after: 10.054819769221417
Epoch 5894/10000, Prediction Accuracy = 62.098%, Loss = 0.42650356888771057
Epoch: 5894, Batch Gradient Norm: 9.660584016505581
Epoch: 5894, Batch Gradient Norm after: 9.660584016505581
Epoch 5895/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.4205425262451172
Epoch: 5895, Batch Gradient Norm: 9.190416305033516
Epoch: 5895, Batch Gradient Norm after: 9.190416305033516
Epoch 5896/10000, Prediction Accuracy = 62.25%, Loss = 0.4165613651275635
Epoch: 5896, Batch Gradient Norm: 8.802963567866867
Epoch: 5896, Batch Gradient Norm after: 8.802963567866867
Epoch 5897/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.41618031859397886
Epoch: 5897, Batch Gradient Norm: 11.1224515393144
Epoch: 5897, Batch Gradient Norm after: 11.1224515393144
Epoch 5898/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.43294278979301454
Epoch: 5898, Batch Gradient Norm: 9.144462383160992
Epoch: 5898, Batch Gradient Norm after: 9.144462383160992
Epoch 5899/10000, Prediction Accuracy = 62.36%, Loss = 0.41705101132392886
Epoch: 5899, Batch Gradient Norm: 8.027371484340442
Epoch: 5899, Batch Gradient Norm after: 8.027371484340442
Epoch 5900/10000, Prediction Accuracy = 62.374%, Loss = 0.41069525480270386
Epoch: 5900, Batch Gradient Norm: 10.702732308025722
Epoch: 5900, Batch Gradient Norm after: 10.702732308025722
Epoch 5901/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.43020747900009154
Epoch: 5901, Batch Gradient Norm: 8.592118140730674
Epoch: 5901, Batch Gradient Norm after: 8.592118140730674
Epoch 5902/10000, Prediction Accuracy = 62.24399999999999%, Loss = 0.4148934483528137
Epoch: 5902, Batch Gradient Norm: 10.363777587707936
Epoch: 5902, Batch Gradient Norm after: 10.363777587707936
Epoch 5903/10000, Prediction Accuracy = 62.132000000000005%, Loss = 0.4275561273097992
Epoch: 5903, Batch Gradient Norm: 10.844576238725226
Epoch: 5903, Batch Gradient Norm after: 10.844576238725226
Epoch 5904/10000, Prediction Accuracy = 62.212%, Loss = 0.42784335017204284
Epoch: 5904, Batch Gradient Norm: 8.882219347823131
Epoch: 5904, Batch Gradient Norm after: 8.882219347823131
Epoch 5905/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4159677743911743
Epoch: 5905, Batch Gradient Norm: 11.496621868382897
Epoch: 5905, Batch Gradient Norm after: 11.496621868382897
Epoch 5906/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.4319581210613251
Epoch: 5906, Batch Gradient Norm: 8.096058077753476
Epoch: 5906, Batch Gradient Norm after: 8.096058077753476
Epoch 5907/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.41008649468421937
Epoch: 5907, Batch Gradient Norm: 7.990303067131549
Epoch: 5907, Batch Gradient Norm after: 7.990303067131549
Epoch 5908/10000, Prediction Accuracy = 62.198%, Loss = 0.4126199662685394
Epoch: 5908, Batch Gradient Norm: 9.18211830919504
Epoch: 5908, Batch Gradient Norm after: 9.18211830919504
Epoch 5909/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41885368824005126
Epoch: 5909, Batch Gradient Norm: 9.639331756929325
Epoch: 5909, Batch Gradient Norm after: 9.639331756929325
Epoch 5910/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.42324445247650144
Epoch: 5910, Batch Gradient Norm: 7.716301692874085
Epoch: 5910, Batch Gradient Norm after: 7.716301692874085
Epoch 5911/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.41146646738052367
Epoch: 5911, Batch Gradient Norm: 12.20139780994069
Epoch: 5911, Batch Gradient Norm after: 12.20139780994069
Epoch 5912/10000, Prediction Accuracy = 62.14%, Loss = 0.44385274052619933
Epoch: 5912, Batch Gradient Norm: 10.655504270873534
Epoch: 5912, Batch Gradient Norm after: 10.655504270873534
Epoch 5913/10000, Prediction Accuracy = 62.246%, Loss = 0.42995148301124575
Epoch: 5913, Batch Gradient Norm: 10.760455375392137
Epoch: 5913, Batch Gradient Norm after: 10.760455375392137
Epoch 5914/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4302354037761688
Epoch: 5914, Batch Gradient Norm: 9.672303139207296
Epoch: 5914, Batch Gradient Norm after: 9.672303139207296
Epoch 5915/10000, Prediction Accuracy = 62.238%, Loss = 0.4243753433227539
Epoch: 5915, Batch Gradient Norm: 9.44523999871137
Epoch: 5915, Batch Gradient Norm after: 9.44523999871137
Epoch 5916/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4190589964389801
Epoch: 5916, Batch Gradient Norm: 10.979209202951006
Epoch: 5916, Batch Gradient Norm after: 10.979209202951006
Epoch 5917/10000, Prediction Accuracy = 62.352%, Loss = 0.43001197576522826
Epoch: 5917, Batch Gradient Norm: 11.390229559265817
Epoch: 5917, Batch Gradient Norm after: 11.390229559265817
Epoch 5918/10000, Prediction Accuracy = 62.266%, Loss = 0.4343820154666901
Epoch: 5918, Batch Gradient Norm: 9.632331009031105
Epoch: 5918, Batch Gradient Norm after: 9.632331009031105
Epoch 5919/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.42035635113716124
Epoch: 5919, Batch Gradient Norm: 7.2530179757663
Epoch: 5919, Batch Gradient Norm after: 7.2530179757663
Epoch 5920/10000, Prediction Accuracy = 62.306%, Loss = 0.40716790556907656
Epoch: 5920, Batch Gradient Norm: 7.103956112698312
Epoch: 5920, Batch Gradient Norm after: 7.103956112698312
Epoch 5921/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.40688631534576414
Epoch: 5921, Batch Gradient Norm: 9.616271227116007
Epoch: 5921, Batch Gradient Norm after: 9.616271227116007
Epoch 5922/10000, Prediction Accuracy = 62.194%, Loss = 0.4176008880138397
Epoch: 5922, Batch Gradient Norm: 12.128394972241043
Epoch: 5922, Batch Gradient Norm after: 12.128394972241043
Epoch 5923/10000, Prediction Accuracy = 62.088%, Loss = 0.4389386117458344
Epoch: 5923, Batch Gradient Norm: 8.974469303477425
Epoch: 5923, Batch Gradient Norm after: 8.974469303477425
Epoch 5924/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.415605628490448
Epoch: 5924, Batch Gradient Norm: 9.669105538376169
Epoch: 5924, Batch Gradient Norm after: 9.669105538376169
Epoch 5925/10000, Prediction Accuracy = 62.23599999999999%, Loss = 0.42005056142807007
Epoch: 5925, Batch Gradient Norm: 10.952044286805728
Epoch: 5925, Batch Gradient Norm after: 10.952044286805728
Epoch 5926/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4298322796821594
Epoch: 5926, Batch Gradient Norm: 9.976992521863943
Epoch: 5926, Batch Gradient Norm after: 9.976992521863943
Epoch 5927/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.4277492225170135
Epoch: 5927, Batch Gradient Norm: 7.548526662722627
Epoch: 5927, Batch Gradient Norm after: 7.548526662722627
Epoch 5928/10000, Prediction Accuracy = 62.302%, Loss = 0.4102417051792145
Epoch: 5928, Batch Gradient Norm: 9.279033245523278
Epoch: 5928, Batch Gradient Norm after: 9.279033245523278
Epoch 5929/10000, Prediction Accuracy = 62.282%, Loss = 0.41982182264328005
Epoch: 5929, Batch Gradient Norm: 9.278324736218924
Epoch: 5929, Batch Gradient Norm after: 9.278324736218924
Epoch 5930/10000, Prediction Accuracy = 62.31400000000001%, Loss = 0.4177705466747284
Epoch: 5930, Batch Gradient Norm: 9.49806825808523
Epoch: 5930, Batch Gradient Norm after: 9.49806825808523
Epoch 5931/10000, Prediction Accuracy = 62.42%, Loss = 0.4180013477802277
Epoch: 5931, Batch Gradient Norm: 9.44287938909566
Epoch: 5931, Batch Gradient Norm after: 9.44287938909566
Epoch 5932/10000, Prediction Accuracy = 62.226%, Loss = 0.41988768577575686
Epoch: 5932, Batch Gradient Norm: 9.895987965890829
Epoch: 5932, Batch Gradient Norm after: 9.895987965890829
Epoch 5933/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.4208914339542389
Epoch: 5933, Batch Gradient Norm: 11.741059887043079
Epoch: 5933, Batch Gradient Norm after: 11.741059887043079
Epoch 5934/10000, Prediction Accuracy = 62.39%, Loss = 0.4338201344013214
Epoch: 5934, Batch Gradient Norm: 11.653484299907944
Epoch: 5934, Batch Gradient Norm after: 11.653484299907944
Epoch 5935/10000, Prediction Accuracy = 62.162%, Loss = 0.4371090352535248
Epoch: 5935, Batch Gradient Norm: 8.354028238596028
Epoch: 5935, Batch Gradient Norm after: 8.354028238596028
Epoch 5936/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.41251401901245116
Epoch: 5936, Batch Gradient Norm: 7.581517210819677
Epoch: 5936, Batch Gradient Norm after: 7.581517210819677
Epoch 5937/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.40758384466171266
Epoch: 5937, Batch Gradient Norm: 8.925057231988001
Epoch: 5937, Batch Gradient Norm after: 8.925057231988001
Epoch 5938/10000, Prediction Accuracy = 62.254%, Loss = 0.4176069498062134
Epoch: 5938, Batch Gradient Norm: 9.973794172289958
Epoch: 5938, Batch Gradient Norm after: 9.973794172289958
Epoch 5939/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.4249496817588806
Epoch: 5939, Batch Gradient Norm: 9.572331640291496
Epoch: 5939, Batch Gradient Norm after: 9.572331640291496
Epoch 5940/10000, Prediction Accuracy = 62.32199999999999%, Loss = 0.4210789144039154
Epoch: 5940, Batch Gradient Norm: 12.408297773699246
Epoch: 5940, Batch Gradient Norm after: 12.408297773699246
Epoch 5941/10000, Prediction Accuracy = 62.354%, Loss = 0.4442517817020416
Epoch: 5941, Batch Gradient Norm: 9.138264158824281
Epoch: 5941, Batch Gradient Norm after: 9.138264158824281
Epoch 5942/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.41602168679237367
Epoch: 5942, Batch Gradient Norm: 12.050342066359013
Epoch: 5942, Batch Gradient Norm after: 12.050342066359013
Epoch 5943/10000, Prediction Accuracy = 62.15999999999999%, Loss = 0.44203320145606995
Epoch: 5943, Batch Gradient Norm: 8.614753133720004
Epoch: 5943, Batch Gradient Norm after: 8.614753133720004
Epoch 5944/10000, Prediction Accuracy = 62.312%, Loss = 0.41577669978141785
Epoch: 5944, Batch Gradient Norm: 7.719832145843803
Epoch: 5944, Batch Gradient Norm after: 7.719832145843803
Epoch 5945/10000, Prediction Accuracy = 62.374%, Loss = 0.40968782901763917
Epoch: 5945, Batch Gradient Norm: 7.55148000541237
Epoch: 5945, Batch Gradient Norm after: 7.55148000541237
Epoch 5946/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.40557786226272585
Epoch: 5946, Batch Gradient Norm: 11.906631788592994
Epoch: 5946, Batch Gradient Norm after: 11.906631788592994
Epoch 5947/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.433352929353714
Epoch: 5947, Batch Gradient Norm: 13.399308208500576
Epoch: 5947, Batch Gradient Norm after: 13.399308208500576
Epoch 5948/10000, Prediction Accuracy = 62.226%, Loss = 0.449049574136734
Epoch: 5948, Batch Gradient Norm: 9.467723826485328
Epoch: 5948, Batch Gradient Norm after: 9.467723826485328
Epoch 5949/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.4226208984851837
Epoch: 5949, Batch Gradient Norm: 8.173269869943331
Epoch: 5949, Batch Gradient Norm after: 8.173269869943331
Epoch 5950/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.4098500609397888
Epoch: 5950, Batch Gradient Norm: 8.664867017799313
Epoch: 5950, Batch Gradient Norm after: 8.664867017799313
Epoch 5951/10000, Prediction Accuracy = 62.376%, Loss = 0.41545743942260743
Epoch: 5951, Batch Gradient Norm: 7.601074296953793
Epoch: 5951, Batch Gradient Norm after: 7.601074296953793
Epoch 5952/10000, Prediction Accuracy = 62.202%, Loss = 0.4079026758670807
Epoch: 5952, Batch Gradient Norm: 11.749981615220111
Epoch: 5952, Batch Gradient Norm after: 11.749981615220111
Epoch 5953/10000, Prediction Accuracy = 62.076%, Loss = 0.4355422556400299
Epoch: 5953, Batch Gradient Norm: 10.2477474408593
Epoch: 5953, Batch Gradient Norm after: 10.2477474408593
Epoch 5954/10000, Prediction Accuracy = 62.25999999999999%, Loss = 0.4269485890865326
Epoch: 5954, Batch Gradient Norm: 9.233621803203718
Epoch: 5954, Batch Gradient Norm after: 9.233621803203718
Epoch 5955/10000, Prediction Accuracy = 62.4%, Loss = 0.41720361113548277
Epoch: 5955, Batch Gradient Norm: 9.036064159481395
Epoch: 5955, Batch Gradient Norm after: 9.036064159481395
Epoch 5956/10000, Prediction Accuracy = 62.246%, Loss = 0.41683876514434814
Epoch: 5956, Batch Gradient Norm: 8.264525004830965
Epoch: 5956, Batch Gradient Norm after: 8.264525004830965
Epoch 5957/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.41196760535240173
Epoch: 5957, Batch Gradient Norm: 7.9501269583190455
Epoch: 5957, Batch Gradient Norm after: 7.9501269583190455
Epoch 5958/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.4096260130405426
Epoch: 5958, Batch Gradient Norm: 9.292178391184041
Epoch: 5958, Batch Gradient Norm after: 9.292178391184041
Epoch 5959/10000, Prediction Accuracy = 62.254%, Loss = 0.4130701243877411
Epoch: 5959, Batch Gradient Norm: 11.399939474034184
Epoch: 5959, Batch Gradient Norm after: 11.399939474034184
Epoch 5960/10000, Prediction Accuracy = 62.13000000000001%, Loss = 0.43005184531211854
Epoch: 5960, Batch Gradient Norm: 9.535934223436486
Epoch: 5960, Batch Gradient Norm after: 9.535934223436486
Epoch 5961/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.4158248782157898
Epoch: 5961, Batch Gradient Norm: 9.210500211388052
Epoch: 5961, Batch Gradient Norm after: 9.210500211388052
Epoch 5962/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4155028283596039
Epoch: 5962, Batch Gradient Norm: 9.616694533512026
Epoch: 5962, Batch Gradient Norm after: 9.616694533512026
Epoch 5963/10000, Prediction Accuracy = 62.258%, Loss = 0.42456984519958496
Epoch: 5963, Batch Gradient Norm: 10.677885601691651
Epoch: 5963, Batch Gradient Norm after: 10.677885601691651
Epoch 5964/10000, Prediction Accuracy = 62.254%, Loss = 0.42935699224472046
Epoch: 5964, Batch Gradient Norm: 10.923607599992556
Epoch: 5964, Batch Gradient Norm after: 10.923607599992556
Epoch 5965/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.42762097120285036
Epoch: 5965, Batch Gradient Norm: 7.996700945669793
Epoch: 5965, Batch Gradient Norm after: 7.996700945669793
Epoch 5966/10000, Prediction Accuracy = 62.222%, Loss = 0.4088966190814972
Epoch: 5966, Batch Gradient Norm: 7.40534625618265
Epoch: 5966, Batch Gradient Norm after: 7.40534625618265
Epoch 5967/10000, Prediction Accuracy = 62.378%, Loss = 0.40457311272621155
Epoch: 5967, Batch Gradient Norm: 10.787053849945192
Epoch: 5967, Batch Gradient Norm after: 10.787053849945192
Epoch 5968/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.42689529061317444
Epoch: 5968, Batch Gradient Norm: 8.532299072527579
Epoch: 5968, Batch Gradient Norm after: 8.532299072527579
Epoch 5969/10000, Prediction Accuracy = 62.378%, Loss = 0.4138058006763458
Epoch: 5969, Batch Gradient Norm: 9.28707016605611
Epoch: 5969, Batch Gradient Norm after: 9.28707016605611
Epoch 5970/10000, Prediction Accuracy = 62.06%, Loss = 0.42010231614112853
Epoch: 5970, Batch Gradient Norm: 9.782053564860384
Epoch: 5970, Batch Gradient Norm after: 9.782053564860384
Epoch 5971/10000, Prediction Accuracy = 62.288%, Loss = 0.419721394777298
Epoch: 5971, Batch Gradient Norm: 13.313581248763342
Epoch: 5971, Batch Gradient Norm after: 13.313581248763342
Epoch 5972/10000, Prediction Accuracy = 62.25%, Loss = 0.4454562127590179
Epoch: 5972, Batch Gradient Norm: 10.742511100718856
Epoch: 5972, Batch Gradient Norm after: 10.742511100718856
Epoch 5973/10000, Prediction Accuracy = 62.14%, Loss = 0.4304846882820129
Epoch: 5973, Batch Gradient Norm: 10.587681692297872
Epoch: 5973, Batch Gradient Norm after: 10.587681692297872
Epoch 5974/10000, Prediction Accuracy = 62.36%, Loss = 0.4276496350765228
Epoch: 5974, Batch Gradient Norm: 8.393533628818728
Epoch: 5974, Batch Gradient Norm after: 8.393533628818728
Epoch 5975/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.41058614253997805
Epoch: 5975, Batch Gradient Norm: 11.305256735543995
Epoch: 5975, Batch Gradient Norm after: 11.305256735543995
Epoch 5976/10000, Prediction Accuracy = 62.162%, Loss = 0.4342740535736084
Epoch: 5976, Batch Gradient Norm: 11.883558891631015
Epoch: 5976, Batch Gradient Norm after: 11.883558891631015
Epoch 5977/10000, Prediction Accuracy = 62.334%, Loss = 0.43509753942489626
Epoch: 5977, Batch Gradient Norm: 9.915326089006884
Epoch: 5977, Batch Gradient Norm after: 9.915326089006884
Epoch 5978/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4191670775413513
Epoch: 5978, Batch Gradient Norm: 8.792070380675307
Epoch: 5978, Batch Gradient Norm after: 8.792070380675307
Epoch 5979/10000, Prediction Accuracy = 62.27%, Loss = 0.4128083348274231
Epoch: 5979, Batch Gradient Norm: 7.42877231743068
Epoch: 5979, Batch Gradient Norm after: 7.42877231743068
Epoch 5980/10000, Prediction Accuracy = 62.212%, Loss = 0.40836896300315856
Epoch: 5980, Batch Gradient Norm: 8.548900302451031
Epoch: 5980, Batch Gradient Norm after: 8.548900302451031
Epoch 5981/10000, Prediction Accuracy = 62.188%, Loss = 0.4120055794715881
Epoch: 5981, Batch Gradient Norm: 8.700406961080315
Epoch: 5981, Batch Gradient Norm after: 8.700406961080315
Epoch 5982/10000, Prediction Accuracy = 62.302%, Loss = 0.41149780750274656
Epoch: 5982, Batch Gradient Norm: 12.675701388239341
Epoch: 5982, Batch Gradient Norm after: 12.675701388239341
Epoch 5983/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.442470520734787
Epoch: 5983, Batch Gradient Norm: 11.194595921860055
Epoch: 5983, Batch Gradient Norm after: 11.194595921860055
Epoch 5984/10000, Prediction Accuracy = 62.312%, Loss = 0.432387638092041
Epoch: 5984, Batch Gradient Norm: 9.565284565401768
Epoch: 5984, Batch Gradient Norm after: 9.565284565401768
Epoch 5985/10000, Prediction Accuracy = 62.23%, Loss = 0.4218886375427246
Epoch: 5985, Batch Gradient Norm: 10.3085239742938
Epoch: 5985, Batch Gradient Norm after: 10.3085239742938
Epoch 5986/10000, Prediction Accuracy = 62.314%, Loss = 0.42779189348220825
Epoch: 5986, Batch Gradient Norm: 8.198340222090012
Epoch: 5986, Batch Gradient Norm after: 8.198340222090012
Epoch 5987/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.41075505018234254
Epoch: 5987, Batch Gradient Norm: 10.378651321748471
Epoch: 5987, Batch Gradient Norm after: 10.378651321748471
Epoch 5988/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.4261495351791382
Epoch: 5988, Batch Gradient Norm: 7.8912773428585785
Epoch: 5988, Batch Gradient Norm after: 7.8912773428585785
Epoch 5989/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.40977131128311156
Epoch: 5989, Batch Gradient Norm: 8.93747756211356
Epoch: 5989, Batch Gradient Norm after: 8.93747756211356
Epoch 5990/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4149579286575317
Epoch: 5990, Batch Gradient Norm: 11.541861936075142
Epoch: 5990, Batch Gradient Norm after: 11.541861936075142
Epoch 5991/10000, Prediction Accuracy = 62.23%, Loss = 0.43240532875061033
Epoch: 5991, Batch Gradient Norm: 8.538308162350765
Epoch: 5991, Batch Gradient Norm after: 8.538308162350765
Epoch 5992/10000, Prediction Accuracy = 62.326%, Loss = 0.41244677901268006
Epoch: 5992, Batch Gradient Norm: 10.765935070600824
Epoch: 5992, Batch Gradient Norm after: 10.765935070600824
Epoch 5993/10000, Prediction Accuracy = 62.21%, Loss = 0.4283665955066681
Epoch: 5993, Batch Gradient Norm: 11.43954479618356
Epoch: 5993, Batch Gradient Norm after: 11.43954479618356
Epoch 5994/10000, Prediction Accuracy = 62.338%, Loss = 0.4340074360370636
Epoch: 5994, Batch Gradient Norm: 8.748996619176626
Epoch: 5994, Batch Gradient Norm after: 8.748996619176626
Epoch 5995/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.41613959074020385
Epoch: 5995, Batch Gradient Norm: 8.964765030962047
Epoch: 5995, Batch Gradient Norm after: 8.964765030962047
Epoch 5996/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.4144562125205994
Epoch: 5996, Batch Gradient Norm: 9.498435043510952
Epoch: 5996, Batch Gradient Norm after: 9.498435043510952
Epoch 5997/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.41784087419509885
Epoch: 5997, Batch Gradient Norm: 9.674307966197677
Epoch: 5997, Batch Gradient Norm after: 9.674307966197677
Epoch 5998/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.41954119205474855
Epoch: 5998, Batch Gradient Norm: 9.17188963390983
Epoch: 5998, Batch Gradient Norm after: 9.17188963390983
Epoch 5999/10000, Prediction Accuracy = 62.15%, Loss = 0.41728784441947936
Epoch: 5999, Batch Gradient Norm: 10.158854548913867
Epoch: 5999, Batch Gradient Norm after: 10.158854548913867
Epoch 6000/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.42811978459358213
Epoch: 6000, Batch Gradient Norm: 7.7306806903060385
Epoch: 6000, Batch Gradient Norm after: 7.7306806903060385
Epoch 6001/10000, Prediction Accuracy = 62.322%, Loss = 0.4077220618724823
Epoch: 6001, Batch Gradient Norm: 10.173675160291989
Epoch: 6001, Batch Gradient Norm after: 10.173675160291989
Epoch 6002/10000, Prediction Accuracy = 62.36%, Loss = 0.4208142042160034
Epoch: 6002, Batch Gradient Norm: 10.469188754714404
Epoch: 6002, Batch Gradient Norm after: 10.469188754714404
Epoch 6003/10000, Prediction Accuracy = 62.15%, Loss = 0.42284278869628905
Epoch: 6003, Batch Gradient Norm: 9.915321164281591
Epoch: 6003, Batch Gradient Norm after: 9.915321164281591
Epoch 6004/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4209202527999878
Epoch: 6004, Batch Gradient Norm: 10.713004675397846
Epoch: 6004, Batch Gradient Norm after: 10.713004675397846
Epoch 6005/10000, Prediction Accuracy = 62.206%, Loss = 0.42596960067749023
Epoch: 6005, Batch Gradient Norm: 9.388204208925943
Epoch: 6005, Batch Gradient Norm after: 9.388204208925943
Epoch 6006/10000, Prediction Accuracy = 62.138%, Loss = 0.41871013641357424
Epoch: 6006, Batch Gradient Norm: 9.988891214979708
Epoch: 6006, Batch Gradient Norm after: 9.988891214979708
Epoch 6007/10000, Prediction Accuracy = 62.174%, Loss = 0.42328158020973206
Epoch: 6007, Batch Gradient Norm: 12.224694755815714
Epoch: 6007, Batch Gradient Norm after: 12.224694755815714
Epoch 6008/10000, Prediction Accuracy = 62.193999999999996%, Loss = 0.44261080026626587
Epoch: 6008, Batch Gradient Norm: 10.592448117147278
Epoch: 6008, Batch Gradient Norm after: 10.592448117147278
Epoch 6009/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.4238720774650574
Epoch: 6009, Batch Gradient Norm: 12.337387211185046
Epoch: 6009, Batch Gradient Norm after: 12.337387211185046
Epoch 6010/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.43966902494430543
Epoch: 6010, Batch Gradient Norm: 9.488425671469983
Epoch: 6010, Batch Gradient Norm after: 9.488425671469983
Epoch 6011/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.418162477016449
Epoch: 6011, Batch Gradient Norm: 6.217306156385855
Epoch: 6011, Batch Gradient Norm after: 6.217306156385855
Epoch 6012/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.40125688910484314
Epoch: 6012, Batch Gradient Norm: 7.128721159900569
Epoch: 6012, Batch Gradient Norm after: 7.128721159900569
Epoch 6013/10000, Prediction Accuracy = 62.362%, Loss = 0.40286619067192075
Epoch: 6013, Batch Gradient Norm: 10.380965805195931
Epoch: 6013, Batch Gradient Norm after: 10.380965805195931
Epoch 6014/10000, Prediction Accuracy = 62.324%, Loss = 0.42292935252189634
Epoch: 6014, Batch Gradient Norm: 10.370439263912237
Epoch: 6014, Batch Gradient Norm after: 10.370439263912237
Epoch 6015/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.42307049036026
Epoch: 6015, Batch Gradient Norm: 11.76920482219299
Epoch: 6015, Batch Gradient Norm after: 11.76920482219299
Epoch 6016/10000, Prediction Accuracy = 62.262%, Loss = 0.4333033859729767
Epoch: 6016, Batch Gradient Norm: 9.663049220012745
Epoch: 6016, Batch Gradient Norm after: 9.663049220012745
Epoch 6017/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4177713692188263
Epoch: 6017, Batch Gradient Norm: 9.683409544211763
Epoch: 6017, Batch Gradient Norm after: 9.683409544211763
Epoch 6018/10000, Prediction Accuracy = 62.342%, Loss = 0.4223337292671204
Epoch: 6018, Batch Gradient Norm: 9.075106504583086
Epoch: 6018, Batch Gradient Norm after: 9.075106504583086
Epoch 6019/10000, Prediction Accuracy = 62.354%, Loss = 0.4131576597690582
Epoch: 6019, Batch Gradient Norm: 11.282886656550659
Epoch: 6019, Batch Gradient Norm after: 11.282886656550659
Epoch 6020/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.43307392597198485
Epoch: 6020, Batch Gradient Norm: 6.953215185351141
Epoch: 6020, Batch Gradient Norm after: 6.953215185351141
Epoch 6021/10000, Prediction Accuracy = 62.3%, Loss = 0.40502018928527833
Epoch: 6021, Batch Gradient Norm: 9.596668337225092
Epoch: 6021, Batch Gradient Norm after: 9.596668337225092
Epoch 6022/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.41925949454307554
Epoch: 6022, Batch Gradient Norm: 9.783583181337347
Epoch: 6022, Batch Gradient Norm after: 9.783583181337347
Epoch 6023/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.4198395133018494
Epoch: 6023, Batch Gradient Norm: 10.447367674072733
Epoch: 6023, Batch Gradient Norm after: 10.447367674072733
Epoch 6024/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4231746673583984
Epoch: 6024, Batch Gradient Norm: 9.044435848374745
Epoch: 6024, Batch Gradient Norm after: 9.044435848374745
Epoch 6025/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.41794328689575194
Epoch: 6025, Batch Gradient Norm: 9.49116680648899
Epoch: 6025, Batch Gradient Norm after: 9.49116680648899
Epoch 6026/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.41868273019790647
Epoch: 6026, Batch Gradient Norm: 7.889710816825275
Epoch: 6026, Batch Gradient Norm after: 7.889710816825275
Epoch 6027/10000, Prediction Accuracy = 62.488%, Loss = 0.4081160604953766
Epoch: 6027, Batch Gradient Norm: 10.061868995014086
Epoch: 6027, Batch Gradient Norm after: 10.061868995014086
Epoch 6028/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4193179547786713
Epoch: 6028, Batch Gradient Norm: 9.572040422627078
Epoch: 6028, Batch Gradient Norm after: 9.572040422627078
Epoch 6029/10000, Prediction Accuracy = 62.33%, Loss = 0.4183655381202698
Epoch: 6029, Batch Gradient Norm: 8.790622213510165
Epoch: 6029, Batch Gradient Norm after: 8.790622213510165
Epoch 6030/10000, Prediction Accuracy = 62.214%, Loss = 0.4160513401031494
Epoch: 6030, Batch Gradient Norm: 8.713540153372923
Epoch: 6030, Batch Gradient Norm after: 8.713540153372923
Epoch 6031/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.41278901100158694
Epoch: 6031, Batch Gradient Norm: 10.373651357722629
Epoch: 6031, Batch Gradient Norm after: 10.373651357722629
Epoch 6032/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4245596766471863
Epoch: 6032, Batch Gradient Norm: 10.572476166375717
Epoch: 6032, Batch Gradient Norm after: 10.572476166375717
Epoch 6033/10000, Prediction Accuracy = 62.37199999999999%, Loss = 0.4266888201236725
Epoch: 6033, Batch Gradient Norm: 10.36025425811234
Epoch: 6033, Batch Gradient Norm after: 10.36025425811234
Epoch 6034/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.42304221391677854
Epoch: 6034, Batch Gradient Norm: 11.33302352654883
Epoch: 6034, Batch Gradient Norm after: 11.33302352654883
Epoch 6035/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4315672695636749
Epoch: 6035, Batch Gradient Norm: 11.347206528132196
Epoch: 6035, Batch Gradient Norm after: 11.347206528132196
Epoch 6036/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.43055609464645384
Epoch: 6036, Batch Gradient Norm: 9.765745255190225
Epoch: 6036, Batch Gradient Norm after: 9.765745255190225
Epoch 6037/10000, Prediction Accuracy = 62.376%, Loss = 0.41987173557281493
Epoch: 6037, Batch Gradient Norm: 7.759887384969492
Epoch: 6037, Batch Gradient Norm after: 7.759887384969492
Epoch 6038/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.40806769132614135
Epoch: 6038, Batch Gradient Norm: 6.814234942254983
Epoch: 6038, Batch Gradient Norm after: 6.814234942254983
Epoch 6039/10000, Prediction Accuracy = 62.298%, Loss = 0.4018312990665436
Epoch: 6039, Batch Gradient Norm: 7.9171028629629445
Epoch: 6039, Batch Gradient Norm after: 7.9171028629629445
Epoch 6040/10000, Prediction Accuracy = 62.162%, Loss = 0.4050251543521881
Epoch: 6040, Batch Gradient Norm: 9.659726733507908
Epoch: 6040, Batch Gradient Norm after: 9.659726733507908
Epoch 6041/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.41747102737426756
Epoch: 6041, Batch Gradient Norm: 10.639722853173454
Epoch: 6041, Batch Gradient Norm after: 10.639722853173454
Epoch 6042/10000, Prediction Accuracy = 62.33399999999999%, Loss = 0.4265473663806915
Epoch: 6042, Batch Gradient Norm: 11.233070870098423
Epoch: 6042, Batch Gradient Norm after: 11.233070870098423
Epoch 6043/10000, Prediction Accuracy = 62.302%, Loss = 0.4355279326438904
Epoch: 6043, Batch Gradient Norm: 9.481243998747725
Epoch: 6043, Batch Gradient Norm after: 9.481243998747725
Epoch 6044/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.4194159984588623
Epoch: 6044, Batch Gradient Norm: 12.578378402157808
Epoch: 6044, Batch Gradient Norm after: 12.578378402157808
Epoch 6045/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4379887282848358
Epoch: 6045, Batch Gradient Norm: 12.384849574506816
Epoch: 6045, Batch Gradient Norm after: 12.384849574506816
Epoch 6046/10000, Prediction Accuracy = 62.246%, Loss = 0.4359337091445923
Epoch: 6046, Batch Gradient Norm: 8.340773227507862
Epoch: 6046, Batch Gradient Norm after: 8.340773227507862
Epoch 6047/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.40984063148498534
Epoch: 6047, Batch Gradient Norm: 7.9473173651008375
Epoch: 6047, Batch Gradient Norm after: 7.9473173651008375
Epoch 6048/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.4063128769397736
Epoch: 6048, Batch Gradient Norm: 9.115537011394993
Epoch: 6048, Batch Gradient Norm after: 9.115537011394993
Epoch 6049/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4126525342464447
Epoch: 6049, Batch Gradient Norm: 11.48393400208325
Epoch: 6049, Batch Gradient Norm after: 11.48393400208325
Epoch 6050/10000, Prediction Accuracy = 62.43399999999999%, Loss = 0.43404871225357056
Epoch: 6050, Batch Gradient Norm: 9.147119331253617
Epoch: 6050, Batch Gradient Norm after: 9.147119331253617
Epoch 6051/10000, Prediction Accuracy = 62.306%, Loss = 0.4142483532428741
Epoch: 6051, Batch Gradient Norm: 9.830398672441873
Epoch: 6051, Batch Gradient Norm after: 9.830398672441873
Epoch 6052/10000, Prediction Accuracy = 62.212%, Loss = 0.4174996197223663
Epoch: 6052, Batch Gradient Norm: 10.115090577990122
Epoch: 6052, Batch Gradient Norm after: 10.115090577990122
Epoch 6053/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.41996843218803404
Epoch: 6053, Batch Gradient Norm: 8.968958151882788
Epoch: 6053, Batch Gradient Norm after: 8.968958151882788
Epoch 6054/10000, Prediction Accuracy = 62.3%, Loss = 0.4155684471130371
Epoch: 6054, Batch Gradient Norm: 10.49398462215312
Epoch: 6054, Batch Gradient Norm after: 10.49398462215312
Epoch 6055/10000, Prediction Accuracy = 62.17199999999999%, Loss = 0.42372953295707705
Epoch: 6055, Batch Gradient Norm: 9.080739051658286
Epoch: 6055, Batch Gradient Norm after: 9.080739051658286
Epoch 6056/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.414184707403183
Epoch: 6056, Batch Gradient Norm: 9.155799399939218
Epoch: 6056, Batch Gradient Norm after: 9.155799399939218
Epoch 6057/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.41751577854156496
Epoch: 6057, Batch Gradient Norm: 5.54125971229951
Epoch: 6057, Batch Gradient Norm after: 5.54125971229951
Epoch 6058/10000, Prediction Accuracy = 62.29%, Loss = 0.39579045176506045
Epoch: 6058, Batch Gradient Norm: 8.459098484119673
Epoch: 6058, Batch Gradient Norm after: 8.459098484119673
Epoch 6059/10000, Prediction Accuracy = 62.286%, Loss = 0.412925523519516
Epoch: 6059, Batch Gradient Norm: 8.896114032055682
Epoch: 6059, Batch Gradient Norm after: 8.896114032055682
Epoch 6060/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.413533878326416
Epoch: 6060, Batch Gradient Norm: 10.06183416819118
Epoch: 6060, Batch Gradient Norm after: 10.06183416819118
Epoch 6061/10000, Prediction Accuracy = 62.372%, Loss = 0.41859503388404845
Epoch: 6061, Batch Gradient Norm: 12.581951451374332
Epoch: 6061, Batch Gradient Norm after: 12.581951451374332
Epoch 6062/10000, Prediction Accuracy = 62.464%, Loss = 0.435771119594574
Epoch: 6062, Batch Gradient Norm: 11.7390540148534
Epoch: 6062, Batch Gradient Norm after: 11.7390540148534
Epoch 6063/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.43208941221237185
Epoch: 6063, Batch Gradient Norm: 10.480999285701596
Epoch: 6063, Batch Gradient Norm after: 10.480999285701596
Epoch 6064/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.42252119183540343
Epoch: 6064, Batch Gradient Norm: 12.149899538312079
Epoch: 6064, Batch Gradient Norm after: 12.149899538312079
Epoch 6065/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.440957635641098
Epoch: 6065, Batch Gradient Norm: 11.604582486233642
Epoch: 6065, Batch Gradient Norm after: 11.604582486233642
Epoch 6066/10000, Prediction Accuracy = 62.242%, Loss = 0.4335383474826813
Epoch: 6066, Batch Gradient Norm: 8.731434815991594
Epoch: 6066, Batch Gradient Norm after: 8.731434815991594
Epoch 6067/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.4108801782131195
Epoch: 6067, Batch Gradient Norm: 8.603922140026066
Epoch: 6067, Batch Gradient Norm after: 8.603922140026066
Epoch 6068/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.41077669262886046
Epoch: 6068, Batch Gradient Norm: 10.149348706116227
Epoch: 6068, Batch Gradient Norm after: 10.149348706116227
Epoch 6069/10000, Prediction Accuracy = 62.27%, Loss = 0.4210604131221771
Epoch: 6069, Batch Gradient Norm: 8.673664742765517
Epoch: 6069, Batch Gradient Norm after: 8.673664742765517
Epoch 6070/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4090250015258789
Epoch: 6070, Batch Gradient Norm: 7.129484622319618
Epoch: 6070, Batch Gradient Norm after: 7.129484622319618
Epoch 6071/10000, Prediction Accuracy = 62.472%, Loss = 0.4016375243663788
Epoch: 6071, Batch Gradient Norm: 9.464290945506058
Epoch: 6071, Batch Gradient Norm after: 9.464290945506058
Epoch 6072/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.41695706844329833
Epoch: 6072, Batch Gradient Norm: 9.82002689979073
Epoch: 6072, Batch Gradient Norm after: 9.82002689979073
Epoch 6073/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.416372013092041
Epoch: 6073, Batch Gradient Norm: 9.435979297987048
Epoch: 6073, Batch Gradient Norm after: 9.435979297987048
Epoch 6074/10000, Prediction Accuracy = 62.37800000000001%, Loss = 0.41303333044052126
Epoch: 6074, Batch Gradient Norm: 6.76805060162097
Epoch: 6074, Batch Gradient Norm after: 6.76805060162097
Epoch 6075/10000, Prediction Accuracy = 62.342%, Loss = 0.4006095230579376
Epoch: 6075, Batch Gradient Norm: 9.811099883289467
Epoch: 6075, Batch Gradient Norm after: 9.811099883289467
Epoch 6076/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.42014600038528443
Epoch: 6076, Batch Gradient Norm: 10.339978565173357
Epoch: 6076, Batch Gradient Norm after: 10.339978565173357
Epoch 6077/10000, Prediction Accuracy = 62.326%, Loss = 0.4226705729961395
Epoch: 6077, Batch Gradient Norm: 12.242319559723688
Epoch: 6077, Batch Gradient Norm after: 12.242319559723688
Epoch 6078/10000, Prediction Accuracy = 62.386%, Loss = 0.4353817582130432
Epoch: 6078, Batch Gradient Norm: 11.21286665360023
Epoch: 6078, Batch Gradient Norm after: 11.21286665360023
Epoch 6079/10000, Prediction Accuracy = 62.238%, Loss = 0.4332824945449829
Epoch: 6079, Batch Gradient Norm: 8.405501025479941
Epoch: 6079, Batch Gradient Norm after: 8.405501025479941
Epoch 6080/10000, Prediction Accuracy = 62.364%, Loss = 0.41217033863067626
Epoch: 6080, Batch Gradient Norm: 12.575808773943354
Epoch: 6080, Batch Gradient Norm after: 12.575808773943354
Epoch 6081/10000, Prediction Accuracy = 62.25999999999999%, Loss = 0.44068277478218076
Epoch: 6081, Batch Gradient Norm: 11.088897188581527
Epoch: 6081, Batch Gradient Norm after: 11.088897188581527
Epoch 6082/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4272673070430756
Epoch: 6082, Batch Gradient Norm: 8.96683755910937
Epoch: 6082, Batch Gradient Norm after: 8.96683755910937
Epoch 6083/10000, Prediction Accuracy = 62.386%, Loss = 0.4119963824748993
Epoch: 6083, Batch Gradient Norm: 10.097942406064195
Epoch: 6083, Batch Gradient Norm after: 10.097942406064195
Epoch 6084/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.42115058898925783
Epoch: 6084, Batch Gradient Norm: 8.751418176446174
Epoch: 6084, Batch Gradient Norm after: 8.751418176446174
Epoch 6085/10000, Prediction Accuracy = 62.194%, Loss = 0.4126197576522827
Epoch: 6085, Batch Gradient Norm: 8.708843901227619
Epoch: 6085, Batch Gradient Norm after: 8.708843901227619
Epoch 6086/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.411899209022522
Epoch: 6086, Batch Gradient Norm: 9.317298724149474
Epoch: 6086, Batch Gradient Norm after: 9.317298724149474
Epoch 6087/10000, Prediction Accuracy = 62.3%, Loss = 0.4139886498451233
Epoch: 6087, Batch Gradient Norm: 9.813890585257614
Epoch: 6087, Batch Gradient Norm after: 9.813890585257614
Epoch 6088/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.41711935997009275
Epoch: 6088, Batch Gradient Norm: 9.2273214664586
Epoch: 6088, Batch Gradient Norm after: 9.2273214664586
Epoch 6089/10000, Prediction Accuracy = 62.314%, Loss = 0.4152815520763397
Epoch: 6089, Batch Gradient Norm: 9.364760075850947
Epoch: 6089, Batch Gradient Norm after: 9.364760075850947
Epoch 6090/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4158842146396637
Epoch: 6090, Batch Gradient Norm: 8.027886797336462
Epoch: 6090, Batch Gradient Norm after: 8.027886797336462
Epoch 6091/10000, Prediction Accuracy = 62.45799999999999%, Loss = 0.40648099184036257
Epoch: 6091, Batch Gradient Norm: 8.531691109036034
Epoch: 6091, Batch Gradient Norm after: 8.531691109036034
Epoch 6092/10000, Prediction Accuracy = 62.278%, Loss = 0.40873950719833374
Epoch: 6092, Batch Gradient Norm: 11.678219864633931
Epoch: 6092, Batch Gradient Norm after: 11.678219864633931
Epoch 6093/10000, Prediction Accuracy = 62.226%, Loss = 0.4322186529636383
Epoch: 6093, Batch Gradient Norm: 12.374459021123053
Epoch: 6093, Batch Gradient Norm after: 12.374459021123053
Epoch 6094/10000, Prediction Accuracy = 62.30200000000001%, Loss = 0.4362505733966827
Epoch: 6094, Batch Gradient Norm: 10.304503586414572
Epoch: 6094, Batch Gradient Norm after: 10.304503586414572
Epoch 6095/10000, Prediction Accuracy = 62.352%, Loss = 0.4224400818347931
Epoch: 6095, Batch Gradient Norm: 9.696859165602866
Epoch: 6095, Batch Gradient Norm after: 9.696859165602866
Epoch 6096/10000, Prediction Accuracy = 62.426%, Loss = 0.415938538312912
Epoch: 6096, Batch Gradient Norm: 9.756850728523297
Epoch: 6096, Batch Gradient Norm after: 9.756850728523297
Epoch 6097/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4185809850692749
Epoch: 6097, Batch Gradient Norm: 9.599042001856075
Epoch: 6097, Batch Gradient Norm after: 9.599042001856075
Epoch 6098/10000, Prediction Accuracy = 62.334%, Loss = 0.4198583424091339
Epoch: 6098, Batch Gradient Norm: 7.007962947469416
Epoch: 6098, Batch Gradient Norm after: 7.007962947469416
Epoch 6099/10000, Prediction Accuracy = 62.342%, Loss = 0.4027735412120819
Epoch: 6099, Batch Gradient Norm: 9.070069380372203
Epoch: 6099, Batch Gradient Norm after: 9.070069380372203
Epoch 6100/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.41626521944999695
Epoch: 6100, Batch Gradient Norm: 8.931269936786366
Epoch: 6100, Batch Gradient Norm after: 8.931269936786366
Epoch 6101/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.4139819979667664
Epoch: 6101, Batch Gradient Norm: 7.826289219716825
Epoch: 6101, Batch Gradient Norm after: 7.826289219716825
Epoch 6102/10000, Prediction Accuracy = 62.324%, Loss = 0.40405070781707764
Epoch: 6102, Batch Gradient Norm: 10.997967108717114
Epoch: 6102, Batch Gradient Norm after: 10.997967108717114
Epoch 6103/10000, Prediction Accuracy = 62.202%, Loss = 0.4266685724258423
Epoch: 6103, Batch Gradient Norm: 12.99281570360383
Epoch: 6103, Batch Gradient Norm after: 12.99281570360383
Epoch 6104/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.4436064541339874
Epoch: 6104, Batch Gradient Norm: 14.78635475038962
Epoch: 6104, Batch Gradient Norm after: 14.78635475038962
Epoch 6105/10000, Prediction Accuracy = 62.18599999999999%, Loss = 0.4640934407711029
Epoch: 6105, Batch Gradient Norm: 6.3621786588402065
Epoch: 6105, Batch Gradient Norm after: 6.3621786588402065
Epoch 6106/10000, Prediction Accuracy = 62.45%, Loss = 0.39891324043273924
Epoch: 6106, Batch Gradient Norm: 6.123994735166876
Epoch: 6106, Batch Gradient Norm after: 6.123994735166876
Epoch 6107/10000, Prediction Accuracy = 62.212%, Loss = 0.39771853685379027
Epoch: 6107, Batch Gradient Norm: 7.150875674437909
Epoch: 6107, Batch Gradient Norm after: 7.150875674437909
Epoch 6108/10000, Prediction Accuracy = 62.364%, Loss = 0.4039359033107758
Epoch: 6108, Batch Gradient Norm: 7.478399755429979
Epoch: 6108, Batch Gradient Norm after: 7.478399755429979
Epoch 6109/10000, Prediction Accuracy = 62.366%, Loss = 0.4031169652938843
Epoch: 6109, Batch Gradient Norm: 11.90514170939981
Epoch: 6109, Batch Gradient Norm after: 11.90514170939981
Epoch 6110/10000, Prediction Accuracy = 62.402%, Loss = 0.4334944486618042
Epoch: 6110, Batch Gradient Norm: 11.852773636940586
Epoch: 6110, Batch Gradient Norm after: 11.852773636940586
Epoch 6111/10000, Prediction Accuracy = 62.152%, Loss = 0.4337677896022797
Epoch: 6111, Batch Gradient Norm: 9.186914856918136
Epoch: 6111, Batch Gradient Norm after: 9.186914856918136
Epoch 6112/10000, Prediction Accuracy = 62.35%, Loss = 0.4157849371433258
Epoch: 6112, Batch Gradient Norm: 9.332802814084921
Epoch: 6112, Batch Gradient Norm after: 9.332802814084921
Epoch 6113/10000, Prediction Accuracy = 62.374%, Loss = 0.4158082365989685
Epoch: 6113, Batch Gradient Norm: 9.47219764984605
Epoch: 6113, Batch Gradient Norm after: 9.47219764984605
Epoch 6114/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4148437023162842
Epoch: 6114, Batch Gradient Norm: 8.63882411284741
Epoch: 6114, Batch Gradient Norm after: 8.63882411284741
Epoch 6115/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.4121947050094604
Epoch: 6115, Batch Gradient Norm: 8.320197335619847
Epoch: 6115, Batch Gradient Norm after: 8.320197335619847
Epoch 6116/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4086561679840088
Epoch: 6116, Batch Gradient Norm: 9.909565158724133
Epoch: 6116, Batch Gradient Norm after: 9.909565158724133
Epoch 6117/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.41916726231575013
Epoch: 6117, Batch Gradient Norm: 9.401950449691272
Epoch: 6117, Batch Gradient Norm after: 9.401950449691272
Epoch 6118/10000, Prediction Accuracy = 62.298%, Loss = 0.41388254761695864
Epoch: 6118, Batch Gradient Norm: 10.549168258306349
Epoch: 6118, Batch Gradient Norm after: 10.549168258306349
Epoch 6119/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4248837113380432
Epoch: 6119, Batch Gradient Norm: 10.104436579962933
Epoch: 6119, Batch Gradient Norm after: 10.104436579962933
Epoch 6120/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4184325635433197
Epoch: 6120, Batch Gradient Norm: 9.95325655304008
Epoch: 6120, Batch Gradient Norm after: 9.95325655304008
Epoch 6121/10000, Prediction Accuracy = 62.376%, Loss = 0.4171184539794922
Epoch: 6121, Batch Gradient Norm: 8.461053210328673
Epoch: 6121, Batch Gradient Norm after: 8.461053210328673
Epoch 6122/10000, Prediction Accuracy = 62.35799999999999%, Loss = 0.40549612045288086
Epoch: 6122, Batch Gradient Norm: 9.314859185131127
Epoch: 6122, Batch Gradient Norm after: 9.314859185131127
Epoch 6123/10000, Prediction Accuracy = 62.322%, Loss = 0.4151176452636719
Epoch: 6123, Batch Gradient Norm: 13.304702966604879
Epoch: 6123, Batch Gradient Norm after: 13.304702966604879
Epoch 6124/10000, Prediction Accuracy = 62.302%, Loss = 0.4472706913948059
Epoch: 6124, Batch Gradient Norm: 10.432352331435123
Epoch: 6124, Batch Gradient Norm after: 10.432352331435123
Epoch 6125/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.4240870177745819
Epoch: 6125, Batch Gradient Norm: 9.420852611830611
Epoch: 6125, Batch Gradient Norm after: 9.420852611830611
Epoch 6126/10000, Prediction Accuracy = 62.242%, Loss = 0.4175870418548584
Epoch: 6126, Batch Gradient Norm: 10.367695319495397
Epoch: 6126, Batch Gradient Norm after: 10.367695319495397
Epoch 6127/10000, Prediction Accuracy = 62.378%, Loss = 0.41926801204681396
Epoch: 6127, Batch Gradient Norm: 10.309317976174729
Epoch: 6127, Batch Gradient Norm after: 10.309317976174729
Epoch 6128/10000, Prediction Accuracy = 62.39%, Loss = 0.41844976544380186
Epoch: 6128, Batch Gradient Norm: 8.06819108783144
Epoch: 6128, Batch Gradient Norm after: 8.06819108783144
Epoch 6129/10000, Prediction Accuracy = 62.35999999999999%, Loss = 0.4069418430328369
Epoch: 6129, Batch Gradient Norm: 10.49669544564978
Epoch: 6129, Batch Gradient Norm after: 10.49669544564978
Epoch 6130/10000, Prediction Accuracy = 62.21%, Loss = 0.42178521752357484
Epoch: 6130, Batch Gradient Norm: 9.014128549533794
Epoch: 6130, Batch Gradient Norm after: 9.014128549533794
Epoch 6131/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.41163346767425535
Epoch: 6131, Batch Gradient Norm: 8.84135018074319
Epoch: 6131, Batch Gradient Norm after: 8.84135018074319
Epoch 6132/10000, Prediction Accuracy = 62.334%, Loss = 0.4095907688140869
Epoch: 6132, Batch Gradient Norm: 9.126692718294501
Epoch: 6132, Batch Gradient Norm after: 9.126692718294501
Epoch 6133/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.4116460859775543
Epoch: 6133, Batch Gradient Norm: 8.895868117339525
Epoch: 6133, Batch Gradient Norm after: 8.895868117339525
Epoch 6134/10000, Prediction Accuracy = 62.374%, Loss = 0.4094830334186554
Epoch: 6134, Batch Gradient Norm: 9.966986977695308
Epoch: 6134, Batch Gradient Norm after: 9.966986977695308
Epoch 6135/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.41784814596176145
Epoch: 6135, Batch Gradient Norm: 10.369434991034133
Epoch: 6135, Batch Gradient Norm after: 10.369434991034133
Epoch 6136/10000, Prediction Accuracy = 62.364%, Loss = 0.4252305805683136
Epoch: 6136, Batch Gradient Norm: 9.520516062268191
Epoch: 6136, Batch Gradient Norm after: 9.520516062268191
Epoch 6137/10000, Prediction Accuracy = 62.41199999999999%, Loss = 0.41212683320045473
Epoch: 6137, Batch Gradient Norm: 9.103204901043974
Epoch: 6137, Batch Gradient Norm after: 9.103204901043974
Epoch 6138/10000, Prediction Accuracy = 62.286%, Loss = 0.41248666644096377
Epoch: 6138, Batch Gradient Norm: 9.118632442305868
Epoch: 6138, Batch Gradient Norm after: 9.118632442305868
Epoch 6139/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.41164536476135255
Epoch: 6139, Batch Gradient Norm: 10.520320502662647
Epoch: 6139, Batch Gradient Norm after: 10.520320502662647
Epoch 6140/10000, Prediction Accuracy = 62.318%, Loss = 0.41893689036369325
Epoch: 6140, Batch Gradient Norm: 12.102496551464863
Epoch: 6140, Batch Gradient Norm after: 12.102496551464863
Epoch 6141/10000, Prediction Accuracy = 62.226%, Loss = 0.4337498903274536
Epoch: 6141, Batch Gradient Norm: 8.736388200466585
Epoch: 6141, Batch Gradient Norm after: 8.736388200466585
Epoch 6142/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.41065295934677126
Epoch: 6142, Batch Gradient Norm: 9.131705916011978
Epoch: 6142, Batch Gradient Norm after: 9.131705916011978
Epoch 6143/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4146095097064972
Epoch: 6143, Batch Gradient Norm: 9.74109271882835
Epoch: 6143, Batch Gradient Norm after: 9.74109271882835
Epoch 6144/10000, Prediction Accuracy = 62.282%, Loss = 0.4169208288192749
Epoch: 6144, Batch Gradient Norm: 10.322566094459907
Epoch: 6144, Batch Gradient Norm after: 10.322566094459907
Epoch 6145/10000, Prediction Accuracy = 62.263999999999996%, Loss = 0.4194883108139038
Epoch: 6145, Batch Gradient Norm: 10.224341837981545
Epoch: 6145, Batch Gradient Norm after: 10.224341837981545
Epoch 6146/10000, Prediction Accuracy = 62.302%, Loss = 0.4231053411960602
Epoch: 6146, Batch Gradient Norm: 9.827546029422784
Epoch: 6146, Batch Gradient Norm after: 9.827546029422784
Epoch 6147/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.41619272232055665
Epoch: 6147, Batch Gradient Norm: 11.25946512472079
Epoch: 6147, Batch Gradient Norm after: 11.25946512472079
Epoch 6148/10000, Prediction Accuracy = 62.352%, Loss = 0.42805840969085696
Epoch: 6148, Batch Gradient Norm: 9.698416040202929
Epoch: 6148, Batch Gradient Norm after: 9.698416040202929
Epoch 6149/10000, Prediction Accuracy = 62.314%, Loss = 0.41681628823280337
Epoch: 6149, Batch Gradient Norm: 7.18804603019848
Epoch: 6149, Batch Gradient Norm after: 7.18804603019848
Epoch 6150/10000, Prediction Accuracy = 62.468%, Loss = 0.3998288869857788
Epoch: 6150, Batch Gradient Norm: 9.913905479550573
Epoch: 6150, Batch Gradient Norm after: 9.913905479550573
Epoch 6151/10000, Prediction Accuracy = 62.222%, Loss = 0.4176562190055847
Epoch: 6151, Batch Gradient Norm: 10.608379210508474
Epoch: 6151, Batch Gradient Norm after: 10.608379210508474
Epoch 6152/10000, Prediction Accuracy = 62.370000000000005%, Loss = 0.42430933713912966
Epoch: 6152, Batch Gradient Norm: 8.59254072058472
Epoch: 6152, Batch Gradient Norm after: 8.59254072058472
Epoch 6153/10000, Prediction Accuracy = 62.348%, Loss = 0.41221995949745177
Epoch: 6153, Batch Gradient Norm: 7.837104899643862
Epoch: 6153, Batch Gradient Norm after: 7.837104899643862
Epoch 6154/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.40601348876953125
Epoch: 6154, Batch Gradient Norm: 10.932463789551363
Epoch: 6154, Batch Gradient Norm after: 10.932463789551363
Epoch 6155/10000, Prediction Accuracy = 62.32399999999999%, Loss = 0.4245728313922882
Epoch: 6155, Batch Gradient Norm: 9.716481092674039
Epoch: 6155, Batch Gradient Norm after: 9.716481092674039
Epoch 6156/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.41581403017044066
Epoch: 6156, Batch Gradient Norm: 10.578827051217283
Epoch: 6156, Batch Gradient Norm after: 10.578827051217283
Epoch 6157/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.42119635343551637
Epoch: 6157, Batch Gradient Norm: 9.923485544875222
Epoch: 6157, Batch Gradient Norm after: 9.923485544875222
Epoch 6158/10000, Prediction Accuracy = 62.326%, Loss = 0.4186304330825806
Epoch: 6158, Batch Gradient Norm: 9.418120122383243
Epoch: 6158, Batch Gradient Norm after: 9.418120122383243
Epoch 6159/10000, Prediction Accuracy = 62.416%, Loss = 0.413189697265625
Epoch: 6159, Batch Gradient Norm: 10.493578433479973
Epoch: 6159, Batch Gradient Norm after: 10.493578433479973
Epoch 6160/10000, Prediction Accuracy = 62.352%, Loss = 0.41900562047958373
Epoch: 6160, Batch Gradient Norm: 10.877310490977173
Epoch: 6160, Batch Gradient Norm after: 10.877310490977173
Epoch 6161/10000, Prediction Accuracy = 62.38800000000001%, Loss = 0.4239342451095581
Epoch: 6161, Batch Gradient Norm: 7.569861512448511
Epoch: 6161, Batch Gradient Norm after: 7.569861512448511
Epoch 6162/10000, Prediction Accuracy = 62.456%, Loss = 0.40244712829589846
Epoch: 6162, Batch Gradient Norm: 9.888897966235024
Epoch: 6162, Batch Gradient Norm after: 9.888897966235024
Epoch 6163/10000, Prediction Accuracy = 62.374%, Loss = 0.41634270548820496
Epoch: 6163, Batch Gradient Norm: 11.749327089620623
Epoch: 6163, Batch Gradient Norm after: 11.749327089620623
Epoch 6164/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.4299397647380829
Epoch: 6164, Batch Gradient Norm: 8.325588398667005
Epoch: 6164, Batch Gradient Norm after: 8.325588398667005
Epoch 6165/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.4063233494758606
Epoch: 6165, Batch Gradient Norm: 7.644581138376571
Epoch: 6165, Batch Gradient Norm after: 7.644581138376571
Epoch 6166/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.404260528087616
Epoch: 6166, Batch Gradient Norm: 8.826495100073396
Epoch: 6166, Batch Gradient Norm after: 8.826495100073396
Epoch 6167/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4111743688583374
Epoch: 6167, Batch Gradient Norm: 9.012922744241742
Epoch: 6167, Batch Gradient Norm after: 9.012922744241742
Epoch 6168/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4124977707862854
Epoch: 6168, Batch Gradient Norm: 8.494248564005533
Epoch: 6168, Batch Gradient Norm after: 8.494248564005533
Epoch 6169/10000, Prediction Accuracy = 62.402%, Loss = 0.41085275411605837
Epoch: 6169, Batch Gradient Norm: 10.024646919041752
Epoch: 6169, Batch Gradient Norm after: 10.024646919041752
Epoch 6170/10000, Prediction Accuracy = 62.278%, Loss = 0.4179382026195526
Epoch: 6170, Batch Gradient Norm: 11.50440640705097
Epoch: 6170, Batch Gradient Norm after: 11.50440640705097
Epoch 6171/10000, Prediction Accuracy = 62.402%, Loss = 0.42620020508766177
Epoch: 6171, Batch Gradient Norm: 10.472733233348617
Epoch: 6171, Batch Gradient Norm after: 10.472733233348617
Epoch 6172/10000, Prediction Accuracy = 62.29%, Loss = 0.41912091374397276
Epoch: 6172, Batch Gradient Norm: 10.755127599259207
Epoch: 6172, Batch Gradient Norm after: 10.755127599259207
Epoch 6173/10000, Prediction Accuracy = 62.348%, Loss = 0.4248208343982697
Epoch: 6173, Batch Gradient Norm: 9.653403359522637
Epoch: 6173, Batch Gradient Norm after: 9.653403359522637
Epoch 6174/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4178114295005798
Epoch: 6174, Batch Gradient Norm: 9.634071692771931
Epoch: 6174, Batch Gradient Norm after: 9.634071692771931
Epoch 6175/10000, Prediction Accuracy = 62.370000000000005%, Loss = 0.4145493030548096
Epoch: 6175, Batch Gradient Norm: 8.47048612260584
Epoch: 6175, Batch Gradient Norm after: 8.47048612260584
Epoch 6176/10000, Prediction Accuracy = 62.336%, Loss = 0.40795443058013914
Epoch: 6176, Batch Gradient Norm: 7.488392767866699
Epoch: 6176, Batch Gradient Norm after: 7.488392767866699
Epoch 6177/10000, Prediction Accuracy = 62.342%, Loss = 0.4030266642570496
Epoch: 6177, Batch Gradient Norm: 10.327689577935352
Epoch: 6177, Batch Gradient Norm after: 10.327689577935352
Epoch 6178/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.41922028064727784
Epoch: 6178, Batch Gradient Norm: 10.93509572068884
Epoch: 6178, Batch Gradient Norm after: 10.93509572068884
Epoch 6179/10000, Prediction Accuracy = 62.4%, Loss = 0.42248313426971434
Epoch: 6179, Batch Gradient Norm: 12.752265904002334
Epoch: 6179, Batch Gradient Norm after: 12.752265904002334
Epoch 6180/10000, Prediction Accuracy = 62.209999999999994%, Loss = 0.43849933743476865
Epoch: 6180, Batch Gradient Norm: 12.474413367669058
Epoch: 6180, Batch Gradient Norm after: 12.474413367669058
Epoch 6181/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.44204644560813905
Epoch: 6181, Batch Gradient Norm: 6.645128721474802
Epoch: 6181, Batch Gradient Norm after: 6.645128721474802
Epoch 6182/10000, Prediction Accuracy = 62.438%, Loss = 0.3984929919242859
Epoch: 6182, Batch Gradient Norm: 9.52281129630824
Epoch: 6182, Batch Gradient Norm after: 9.52281129630824
Epoch 6183/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.4150648534297943
Epoch: 6183, Batch Gradient Norm: 8.345220316204477
Epoch: 6183, Batch Gradient Norm after: 8.345220316204477
Epoch 6184/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.4067291557788849
Epoch: 6184, Batch Gradient Norm: 9.579259816699997
Epoch: 6184, Batch Gradient Norm after: 9.579259816699997
Epoch 6185/10000, Prediction Accuracy = 62.38599999999999%, Loss = 0.41608878374099734
Epoch: 6185, Batch Gradient Norm: 7.745235797392119
Epoch: 6185, Batch Gradient Norm after: 7.745235797392119
Epoch 6186/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.40184958577156066
Epoch: 6186, Batch Gradient Norm: 10.686797059305437
Epoch: 6186, Batch Gradient Norm after: 10.686797059305437
Epoch 6187/10000, Prediction Accuracy = 62.438%, Loss = 0.4200774967670441
Epoch: 6187, Batch Gradient Norm: 12.078605872746877
Epoch: 6187, Batch Gradient Norm after: 12.078605872746877
Epoch 6188/10000, Prediction Accuracy = 62.254%, Loss = 0.43511050939559937
Epoch: 6188, Batch Gradient Norm: 9.36678546026898
Epoch: 6188, Batch Gradient Norm after: 9.36678546026898
Epoch 6189/10000, Prediction Accuracy = 62.459999999999994%, Loss = 0.41550357937812804
Epoch: 6189, Batch Gradient Norm: 7.22462284434474
Epoch: 6189, Batch Gradient Norm after: 7.22462284434474
Epoch 6190/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4019763469696045
Epoch: 6190, Batch Gradient Norm: 8.02742250712609
Epoch: 6190, Batch Gradient Norm after: 8.02742250712609
Epoch 6191/10000, Prediction Accuracy = 62.484%, Loss = 0.40528624653816225
Epoch: 6191, Batch Gradient Norm: 10.363433765985882
Epoch: 6191, Batch Gradient Norm after: 10.363433765985882
Epoch 6192/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.41996489763259887
Epoch: 6192, Batch Gradient Norm: 9.917491824414535
Epoch: 6192, Batch Gradient Norm after: 9.917491824414535
Epoch 6193/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.41646636724472047
Epoch: 6193, Batch Gradient Norm: 8.95637808359015
Epoch: 6193, Batch Gradient Norm after: 8.95637808359015
Epoch 6194/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.4108882546424866
Epoch: 6194, Batch Gradient Norm: 10.472086375232378
Epoch: 6194, Batch Gradient Norm after: 10.472086375232378
Epoch 6195/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.4186535060405731
Epoch: 6195, Batch Gradient Norm: 9.150361763905485
Epoch: 6195, Batch Gradient Norm after: 9.150361763905485
Epoch 6196/10000, Prediction Accuracy = 62.408%, Loss = 0.4136124670505524
Epoch: 6196, Batch Gradient Norm: 7.2441293985935316
Epoch: 6196, Batch Gradient Norm after: 7.2441293985935316
Epoch 6197/10000, Prediction Accuracy = 62.386%, Loss = 0.40010809898376465
Epoch: 6197, Batch Gradient Norm: 10.73069384398593
Epoch: 6197, Batch Gradient Norm after: 10.73069384398593
Epoch 6198/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.42186545133590697
Epoch: 6198, Batch Gradient Norm: 12.974998369881805
Epoch: 6198, Batch Gradient Norm after: 12.974998369881805
Epoch 6199/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.4379189670085907
Epoch: 6199, Batch Gradient Norm: 10.897538296682514
Epoch: 6199, Batch Gradient Norm after: 10.897538296682514
Epoch 6200/10000, Prediction Accuracy = 62.27%, Loss = 0.42689160704612733
Epoch: 6200, Batch Gradient Norm: 7.95016149979873
Epoch: 6200, Batch Gradient Norm after: 7.95016149979873
Epoch 6201/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.4037966847419739
Epoch: 6201, Batch Gradient Norm: 9.972820592805409
Epoch: 6201, Batch Gradient Norm after: 9.972820592805409
Epoch 6202/10000, Prediction Accuracy = 62.4%, Loss = 0.41939058899879456
Epoch: 6202, Batch Gradient Norm: 8.755760565217306
Epoch: 6202, Batch Gradient Norm after: 8.755760565217306
Epoch 6203/10000, Prediction Accuracy = 62.202%, Loss = 0.4099149167537689
Epoch: 6203, Batch Gradient Norm: 8.026475244751884
Epoch: 6203, Batch Gradient Norm after: 8.026475244751884
Epoch 6204/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.40391883850097654
Epoch: 6204, Batch Gradient Norm: 10.275416991798332
Epoch: 6204, Batch Gradient Norm after: 10.275416991798332
Epoch 6205/10000, Prediction Accuracy = 62.302%, Loss = 0.42129774689674376
Epoch: 6205, Batch Gradient Norm: 8.256448341208817
Epoch: 6205, Batch Gradient Norm after: 8.256448341208817
Epoch 6206/10000, Prediction Accuracy = 62.46%, Loss = 0.4036825180053711
Epoch: 6206, Batch Gradient Norm: 7.847794489964495
Epoch: 6206, Batch Gradient Norm after: 7.847794489964495
Epoch 6207/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4050118625164032
Epoch: 6207, Batch Gradient Norm: 10.918414673343563
Epoch: 6207, Batch Gradient Norm after: 10.918414673343563
Epoch 6208/10000, Prediction Accuracy = 62.306%, Loss = 0.42173115015029905
Epoch: 6208, Batch Gradient Norm: 11.053977049939085
Epoch: 6208, Batch Gradient Norm after: 11.053977049939085
Epoch 6209/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4232453525066376
Epoch: 6209, Batch Gradient Norm: 7.491899700265191
Epoch: 6209, Batch Gradient Norm after: 7.491899700265191
Epoch 6210/10000, Prediction Accuracy = 62.56%, Loss = 0.40213294625282286
Epoch: 6210, Batch Gradient Norm: 10.669801724044145
Epoch: 6210, Batch Gradient Norm after: 10.669801724044145
Epoch 6211/10000, Prediction Accuracy = 62.126%, Loss = 0.41947320103645325
Epoch: 6211, Batch Gradient Norm: 13.963495908289442
Epoch: 6211, Batch Gradient Norm after: 13.963495908289442
Epoch 6212/10000, Prediction Accuracy = 62.221999999999994%, Loss = 0.45574126243591306
Epoch: 6212, Batch Gradient Norm: 8.466876518546174
Epoch: 6212, Batch Gradient Norm after: 8.466876518546174
Epoch 6213/10000, Prediction Accuracy = 62.202%, Loss = 0.4119259715080261
Epoch: 6213, Batch Gradient Norm: 9.213035974703162
Epoch: 6213, Batch Gradient Norm after: 9.213035974703162
Epoch 6214/10000, Prediction Accuracy = 62.428%, Loss = 0.41572734117507937
Epoch: 6214, Batch Gradient Norm: 9.052283930435072
Epoch: 6214, Batch Gradient Norm after: 9.052283930435072
Epoch 6215/10000, Prediction Accuracy = 62.262%, Loss = 0.4109743535518646
Epoch: 6215, Batch Gradient Norm: 9.412763077561772
Epoch: 6215, Batch Gradient Norm after: 9.412763077561772
Epoch 6216/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.41049527525901797
Epoch: 6216, Batch Gradient Norm: 10.174310689495154
Epoch: 6216, Batch Gradient Norm after: 10.174310689495154
Epoch 6217/10000, Prediction Accuracy = 62.164%, Loss = 0.4160678267478943
Epoch: 6217, Batch Gradient Norm: 10.475932443210112
Epoch: 6217, Batch Gradient Norm after: 10.475932443210112
Epoch 6218/10000, Prediction Accuracy = 62.428%, Loss = 0.4193569004535675
Epoch: 6218, Batch Gradient Norm: 10.107322893396562
Epoch: 6218, Batch Gradient Norm after: 10.107322893396562
Epoch 6219/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.41793062090873717
Epoch: 6219, Batch Gradient Norm: 11.337619967336932
Epoch: 6219, Batch Gradient Norm after: 11.337619967336932
Epoch 6220/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4256288528442383
Epoch: 6220, Batch Gradient Norm: 7.035893179874358
Epoch: 6220, Batch Gradient Norm after: 7.035893179874358
Epoch 6221/10000, Prediction Accuracy = 62.42%, Loss = 0.39798877835273744
Epoch: 6221, Batch Gradient Norm: 8.789117171249078
Epoch: 6221, Batch Gradient Norm after: 8.789117171249078
Epoch 6222/10000, Prediction Accuracy = 62.45%, Loss = 0.4114949107170105
Epoch: 6222, Batch Gradient Norm: 11.276496328325507
Epoch: 6222, Batch Gradient Norm after: 11.276496328325507
Epoch 6223/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4301594734191895
Epoch: 6223, Batch Gradient Norm: 10.675407417230604
Epoch: 6223, Batch Gradient Norm after: 10.675407417230604
Epoch 6224/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.4212669491767883
Epoch: 6224, Batch Gradient Norm: 8.879035966885764
Epoch: 6224, Batch Gradient Norm after: 8.879035966885764
Epoch 6225/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.4115185499191284
Epoch: 6225, Batch Gradient Norm: 10.138764033216225
Epoch: 6225, Batch Gradient Norm after: 10.138764033216225
Epoch 6226/10000, Prediction Accuracy = 62.314%, Loss = 0.4202719509601593
Epoch: 6226, Batch Gradient Norm: 7.3060216439702845
Epoch: 6226, Batch Gradient Norm after: 7.3060216439702845
Epoch 6227/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.40052748918533326
Epoch: 6227, Batch Gradient Norm: 9.616424681249242
Epoch: 6227, Batch Gradient Norm after: 9.616424681249242
Epoch 6228/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.4165500342845917
Epoch: 6228, Batch Gradient Norm: 8.01322343065014
Epoch: 6228, Batch Gradient Norm after: 8.01322343065014
Epoch 6229/10000, Prediction Accuracy = 62.456%, Loss = 0.40568642020225526
Epoch: 6229, Batch Gradient Norm: 7.530068926845268
Epoch: 6229, Batch Gradient Norm after: 7.530068926845268
Epoch 6230/10000, Prediction Accuracy = 62.33%, Loss = 0.4006062686443329
Epoch: 6230, Batch Gradient Norm: 6.435883592535979
Epoch: 6230, Batch Gradient Norm after: 6.435883592535979
Epoch 6231/10000, Prediction Accuracy = 62.452%, Loss = 0.3947041451931
Epoch: 6231, Batch Gradient Norm: 9.223722317894628
Epoch: 6231, Batch Gradient Norm after: 9.223722317894628
Epoch 6232/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.4104795277118683
Epoch: 6232, Batch Gradient Norm: 10.40306256276437
Epoch: 6232, Batch Gradient Norm after: 10.40306256276437
Epoch 6233/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.4185346901416779
Epoch: 6233, Batch Gradient Norm: 12.898996238409492
Epoch: 6233, Batch Gradient Norm after: 12.898996238409492
Epoch 6234/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.43705894947052004
Epoch: 6234, Batch Gradient Norm: 12.978797265758095
Epoch: 6234, Batch Gradient Norm after: 12.978797265758095
Epoch 6235/10000, Prediction Accuracy = 62.36%, Loss = 0.43861726522445676
Epoch: 6235, Batch Gradient Norm: 9.743003918246929
Epoch: 6235, Batch Gradient Norm after: 9.743003918246929
Epoch 6236/10000, Prediction Accuracy = 62.352%, Loss = 0.41377493739128113
Epoch: 6236, Batch Gradient Norm: 8.295977171508211
Epoch: 6236, Batch Gradient Norm after: 8.295977171508211
Epoch 6237/10000, Prediction Accuracy = 62.262%, Loss = 0.4065395057201385
Epoch: 6237, Batch Gradient Norm: 11.469823987026057
Epoch: 6237, Batch Gradient Norm after: 11.469823987026057
Epoch 6238/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.42960127592086794
Epoch: 6238, Batch Gradient Norm: 8.791496009481166
Epoch: 6238, Batch Gradient Norm after: 8.791496009481166
Epoch 6239/10000, Prediction Accuracy = 62.436%, Loss = 0.4108007371425629
Epoch: 6239, Batch Gradient Norm: 10.239725921644592
Epoch: 6239, Batch Gradient Norm after: 10.239725921644592
Epoch 6240/10000, Prediction Accuracy = 62.338%, Loss = 0.4189990758895874
Epoch: 6240, Batch Gradient Norm: 10.464069388529438
Epoch: 6240, Batch Gradient Norm after: 10.464069388529438
Epoch 6241/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4243472158908844
Epoch: 6241, Batch Gradient Norm: 8.955060579940143
Epoch: 6241, Batch Gradient Norm after: 8.955060579940143
Epoch 6242/10000, Prediction Accuracy = 62.428%, Loss = 0.4087732136249542
Epoch: 6242, Batch Gradient Norm: 9.928385213269529
Epoch: 6242, Batch Gradient Norm after: 9.928385213269529
Epoch 6243/10000, Prediction Accuracy = 62.348%, Loss = 0.41366114020347594
Epoch: 6243, Batch Gradient Norm: 10.333811675048148
Epoch: 6243, Batch Gradient Norm after: 10.333811675048148
Epoch 6244/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.41694448590278627
Epoch: 6244, Batch Gradient Norm: 8.129225334844492
Epoch: 6244, Batch Gradient Norm after: 8.129225334844492
Epoch 6245/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.4035381555557251
Epoch: 6245, Batch Gradient Norm: 8.591545567407753
Epoch: 6245, Batch Gradient Norm after: 8.591545567407753
Epoch 6246/10000, Prediction Accuracy = 62.212%, Loss = 0.4093798577785492
Epoch: 6246, Batch Gradient Norm: 8.62424059969885
Epoch: 6246, Batch Gradient Norm after: 8.62424059969885
Epoch 6247/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.40743227005004884
Epoch: 6247, Batch Gradient Norm: 11.290414240225651
Epoch: 6247, Batch Gradient Norm after: 11.290414240225651
Epoch 6248/10000, Prediction Accuracy = 62.45%, Loss = 0.4249896168708801
Epoch: 6248, Batch Gradient Norm: 9.910583218251183
Epoch: 6248, Batch Gradient Norm after: 9.910583218251183
Epoch 6249/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.41716373562812803
Epoch: 6249, Batch Gradient Norm: 7.983179924421777
Epoch: 6249, Batch Gradient Norm after: 7.983179924421777
Epoch 6250/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.4034891128540039
Epoch: 6250, Batch Gradient Norm: 8.967010832233209
Epoch: 6250, Batch Gradient Norm after: 8.967010832233209
Epoch 6251/10000, Prediction Accuracy = 62.412%, Loss = 0.40868049263954165
Epoch: 6251, Batch Gradient Norm: 12.523233559213185
Epoch: 6251, Batch Gradient Norm after: 12.523233559213185
Epoch 6252/10000, Prediction Accuracy = 62.45799999999999%, Loss = 0.4348698854446411
Epoch: 6252, Batch Gradient Norm: 11.434109018170833
Epoch: 6252, Batch Gradient Norm after: 11.434109018170833
Epoch 6253/10000, Prediction Accuracy = 62.39000000000001%, Loss = 0.42393786311149595
Epoch: 6253, Batch Gradient Norm: 8.254464386713511
Epoch: 6253, Batch Gradient Norm after: 8.254464386713511
Epoch 6254/10000, Prediction Accuracy = 62.338%, Loss = 0.4055813491344452
Epoch: 6254, Batch Gradient Norm: 7.402001866540024
Epoch: 6254, Batch Gradient Norm after: 7.402001866540024
Epoch 6255/10000, Prediction Accuracy = 62.504%, Loss = 0.3994452655315399
Epoch: 6255, Batch Gradient Norm: 9.001329217049758
Epoch: 6255, Batch Gradient Norm after: 9.001329217049758
Epoch 6256/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.4103113353252411
Epoch: 6256, Batch Gradient Norm: 11.084392884211871
Epoch: 6256, Batch Gradient Norm after: 11.084392884211871
Epoch 6257/10000, Prediction Accuracy = 62.202%, Loss = 0.42441264986991883
Epoch: 6257, Batch Gradient Norm: 10.076081522884683
Epoch: 6257, Batch Gradient Norm after: 10.076081522884683
Epoch 6258/10000, Prediction Accuracy = 62.376%, Loss = 0.4203087091445923
Epoch: 6258, Batch Gradient Norm: 11.06232496996234
Epoch: 6258, Batch Gradient Norm after: 11.06232496996234
Epoch 6259/10000, Prediction Accuracy = 62.444%, Loss = 0.4225083410739899
Epoch: 6259, Batch Gradient Norm: 10.800505167060662
Epoch: 6259, Batch Gradient Norm after: 10.800505167060662
Epoch 6260/10000, Prediction Accuracy = 62.45%, Loss = 0.41826759576797484
Epoch: 6260, Batch Gradient Norm: 9.64813892871382
Epoch: 6260, Batch Gradient Norm after: 9.64813892871382
Epoch 6261/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.41082834005355834
Epoch: 6261, Batch Gradient Norm: 12.244032610110077
Epoch: 6261, Batch Gradient Norm after: 12.244032610110077
Epoch 6262/10000, Prediction Accuracy = 62.246%, Loss = 0.431575620174408
Epoch: 6262, Batch Gradient Norm: 10.144072881938346
Epoch: 6262, Batch Gradient Norm after: 10.144072881938346
Epoch 6263/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.41791491508483886
Epoch: 6263, Batch Gradient Norm: 8.368347443146854
Epoch: 6263, Batch Gradient Norm after: 8.368347443146854
Epoch 6264/10000, Prediction Accuracy = 62.234%, Loss = 0.40648846626281737
Epoch: 6264, Batch Gradient Norm: 6.615596293513
Epoch: 6264, Batch Gradient Norm after: 6.615596293513
Epoch 6265/10000, Prediction Accuracy = 62.254%, Loss = 0.3977863609790802
Epoch: 6265, Batch Gradient Norm: 7.875383354684843
Epoch: 6265, Batch Gradient Norm after: 7.875383354684843
Epoch 6266/10000, Prediction Accuracy = 62.544000000000004%, Loss = 0.4012202322483063
Epoch: 6266, Batch Gradient Norm: 7.67648093646756
Epoch: 6266, Batch Gradient Norm after: 7.67648093646756
Epoch 6267/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.3998621702194214
Epoch: 6267, Batch Gradient Norm: 11.33706277205133
Epoch: 6267, Batch Gradient Norm after: 11.33706277205133
Epoch 6268/10000, Prediction Accuracy = 62.21%, Loss = 0.42789213061332704
Epoch: 6268, Batch Gradient Norm: 11.51997757882492
Epoch: 6268, Batch Gradient Norm after: 11.51997757882492
Epoch 6269/10000, Prediction Accuracy = 62.25%, Loss = 0.4302516579627991
Epoch: 6269, Batch Gradient Norm: 9.242086185070347
Epoch: 6269, Batch Gradient Norm after: 9.242086185070347
Epoch 6270/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.4141898453235626
Epoch: 6270, Batch Gradient Norm: 11.974289336779345
Epoch: 6270, Batch Gradient Norm after: 11.974289336779345
Epoch 6271/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.43149871230125425
Epoch: 6271, Batch Gradient Norm: 10.954755087792613
Epoch: 6271, Batch Gradient Norm after: 10.954755087792613
Epoch 6272/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.4206563293933868
Epoch: 6272, Batch Gradient Norm: 7.508471837133835
Epoch: 6272, Batch Gradient Norm after: 7.508471837133835
Epoch 6273/10000, Prediction Accuracy = 62.35999999999999%, Loss = 0.3981639802455902
Epoch: 6273, Batch Gradient Norm: 7.154277412253611
Epoch: 6273, Batch Gradient Norm after: 7.154277412253611
Epoch 6274/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.3989644765853882
Epoch: 6274, Batch Gradient Norm: 8.369534330939283
Epoch: 6274, Batch Gradient Norm after: 8.369534330939283
Epoch 6275/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.4061818778514862
Epoch: 6275, Batch Gradient Norm: 8.829847502284803
Epoch: 6275, Batch Gradient Norm after: 8.829847502284803
Epoch 6276/10000, Prediction Accuracy = 62.3%, Loss = 0.4080127775669098
Epoch: 6276, Batch Gradient Norm: 11.482344577931652
Epoch: 6276, Batch Gradient Norm after: 11.482344577931652
Epoch 6277/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.43173065185546877
Epoch: 6277, Batch Gradient Norm: 10.28352003840745
Epoch: 6277, Batch Gradient Norm after: 10.28352003840745
Epoch 6278/10000, Prediction Accuracy = 62.338%, Loss = 0.42001636028289796
Epoch: 6278, Batch Gradient Norm: 10.039788273204394
Epoch: 6278, Batch Gradient Norm after: 10.039788273204394
Epoch 6279/10000, Prediction Accuracy = 62.456%, Loss = 0.41550484895706175
Epoch: 6279, Batch Gradient Norm: 11.501969245328214
Epoch: 6279, Batch Gradient Norm after: 11.501969245328214
Epoch 6280/10000, Prediction Accuracy = 62.36400000000001%, Loss = 0.429792708158493
Epoch: 6280, Batch Gradient Norm: 10.58238797999374
Epoch: 6280, Batch Gradient Norm after: 10.58238797999374
Epoch 6281/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.42214747071266173
Epoch: 6281, Batch Gradient Norm: 10.277879978138564
Epoch: 6281, Batch Gradient Norm after: 10.277879978138564
Epoch 6282/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4235660135746002
Epoch: 6282, Batch Gradient Norm: 7.126241752361657
Epoch: 6282, Batch Gradient Norm after: 7.126241752361657
Epoch 6283/10000, Prediction Accuracy = 62.592%, Loss = 0.397891628742218
Epoch: 6283, Batch Gradient Norm: 8.00160513815702
Epoch: 6283, Batch Gradient Norm after: 8.00160513815702
Epoch 6284/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.40160001516342164
Epoch: 6284, Batch Gradient Norm: 9.52443323959746
Epoch: 6284, Batch Gradient Norm after: 9.52443323959746
Epoch 6285/10000, Prediction Accuracy = 62.63799999999999%, Loss = 0.4114320814609528
Epoch: 6285, Batch Gradient Norm: 8.197420440008752
Epoch: 6285, Batch Gradient Norm after: 8.197420440008752
Epoch 6286/10000, Prediction Accuracy = 62.48%, Loss = 0.40207449793815614
Epoch: 6286, Batch Gradient Norm: 10.502945030092105
Epoch: 6286, Batch Gradient Norm after: 10.502945030092105
Epoch 6287/10000, Prediction Accuracy = 62.354%, Loss = 0.41628333926200867
Epoch: 6287, Batch Gradient Norm: 10.44243516462007
Epoch: 6287, Batch Gradient Norm after: 10.44243516462007
Epoch 6288/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.41691531538963317
Epoch: 6288, Batch Gradient Norm: 6.519631443287821
Epoch: 6288, Batch Gradient Norm after: 6.519631443287821
Epoch 6289/10000, Prediction Accuracy = 62.398%, Loss = 0.3942932963371277
Epoch: 6289, Batch Gradient Norm: 8.804783687253355
Epoch: 6289, Batch Gradient Norm after: 8.804783687253355
Epoch 6290/10000, Prediction Accuracy = 62.44%, Loss = 0.40665828585624697
Epoch: 6290, Batch Gradient Norm: 9.208027858847917
Epoch: 6290, Batch Gradient Norm after: 9.208027858847917
Epoch 6291/10000, Prediction Accuracy = 62.302%, Loss = 0.4161487817764282
Epoch: 6291, Batch Gradient Norm: 9.745613320689802
Epoch: 6291, Batch Gradient Norm after: 9.745613320689802
Epoch 6292/10000, Prediction Accuracy = 62.324%, Loss = 0.4140166163444519
Epoch: 6292, Batch Gradient Norm: 11.831963366192092
Epoch: 6292, Batch Gradient Norm after: 11.831963366192092
Epoch 6293/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.43099157214164735
Epoch: 6293, Batch Gradient Norm: 9.731316034394117
Epoch: 6293, Batch Gradient Norm after: 9.731316034394117
Epoch 6294/10000, Prediction Accuracy = 62.44%, Loss = 0.41270208954811094
Epoch: 6294, Batch Gradient Norm: 9.237601009874329
Epoch: 6294, Batch Gradient Norm after: 9.237601009874329
Epoch 6295/10000, Prediction Accuracy = 62.386%, Loss = 0.40837725400924685
Epoch: 6295, Batch Gradient Norm: 11.781448097641551
Epoch: 6295, Batch Gradient Norm after: 11.781448097641551
Epoch 6296/10000, Prediction Accuracy = 62.414%, Loss = 0.42933125495910646
Epoch: 6296, Batch Gradient Norm: 11.463287965858566
Epoch: 6296, Batch Gradient Norm after: 11.463287965858566
Epoch 6297/10000, Prediction Accuracy = 62.492%, Loss = 0.42755837440490724
Epoch: 6297, Batch Gradient Norm: 12.16399580196885
Epoch: 6297, Batch Gradient Norm after: 12.16399580196885
Epoch 6298/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.43028318881988525
Epoch: 6298, Batch Gradient Norm: 9.69105132156214
Epoch: 6298, Batch Gradient Norm after: 9.69105132156214
Epoch 6299/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.41297515034675597
Epoch: 6299, Batch Gradient Norm: 9.945287684446242
Epoch: 6299, Batch Gradient Norm after: 9.945287684446242
Epoch 6300/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.41983055472373965
Epoch: 6300, Batch Gradient Norm: 7.061523999593698
Epoch: 6300, Batch Gradient Norm after: 7.061523999593698
Epoch 6301/10000, Prediction Accuracy = 62.448%, Loss = 0.39646598100662234
Epoch: 6301, Batch Gradient Norm: 7.947167763741086
Epoch: 6301, Batch Gradient Norm after: 7.947167763741086
Epoch 6302/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.40115163922309877
Epoch: 6302, Batch Gradient Norm: 10.83123290463228
Epoch: 6302, Batch Gradient Norm after: 10.83123290463228
Epoch 6303/10000, Prediction Accuracy = 62.294%, Loss = 0.4233739197254181
Epoch: 6303, Batch Gradient Norm: 9.248301745672272
Epoch: 6303, Batch Gradient Norm after: 9.248301745672272
Epoch 6304/10000, Prediction Accuracy = 62.396%, Loss = 0.41109108328819277
Epoch: 6304, Batch Gradient Norm: 8.394250362856559
Epoch: 6304, Batch Gradient Norm after: 8.394250362856559
Epoch 6305/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4050879001617432
Epoch: 6305, Batch Gradient Norm: 10.519156230118647
Epoch: 6305, Batch Gradient Norm after: 10.519156230118647
Epoch 6306/10000, Prediction Accuracy = 62.428%, Loss = 0.41846329569816587
Epoch: 6306, Batch Gradient Norm: 12.247768853697467
Epoch: 6306, Batch Gradient Norm after: 12.247768853697467
Epoch 6307/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.43604262471199035
Epoch: 6307, Batch Gradient Norm: 7.333047188369054
Epoch: 6307, Batch Gradient Norm after: 7.333047188369054
Epoch 6308/10000, Prediction Accuracy = 62.39%, Loss = 0.3990996479988098
Epoch: 6308, Batch Gradient Norm: 7.915490429196925
Epoch: 6308, Batch Gradient Norm after: 7.915490429196925
Epoch 6309/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.405522084236145
Epoch: 6309, Batch Gradient Norm: 10.14447259161309
Epoch: 6309, Batch Gradient Norm after: 10.14447259161309
Epoch 6310/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.41894456148147585
Epoch: 6310, Batch Gradient Norm: 9.206137851120328
Epoch: 6310, Batch Gradient Norm after: 9.206137851120328
Epoch 6311/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.41028207540512085
Epoch: 6311, Batch Gradient Norm: 9.066859609929763
Epoch: 6311, Batch Gradient Norm after: 9.066859609929763
Epoch 6312/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.4086164116859436
Epoch: 6312, Batch Gradient Norm: 10.58313973514476
Epoch: 6312, Batch Gradient Norm after: 10.58313973514476
Epoch 6313/10000, Prediction Accuracy = 62.366%, Loss = 0.4169872462749481
Epoch: 6313, Batch Gradient Norm: 10.882245571225033
Epoch: 6313, Batch Gradient Norm after: 10.882245571225033
Epoch 6314/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.41924756169319155
Epoch: 6314, Batch Gradient Norm: 8.87594968487811
Epoch: 6314, Batch Gradient Norm after: 8.87594968487811
Epoch 6315/10000, Prediction Accuracy = 62.49000000000001%, Loss = 0.40477614998817446
Epoch: 6315, Batch Gradient Norm: 10.61900762320589
Epoch: 6315, Batch Gradient Norm after: 10.61900762320589
Epoch 6316/10000, Prediction Accuracy = 62.438%, Loss = 0.4188620984554291
Epoch: 6316, Batch Gradient Norm: 11.26963479728596
Epoch: 6316, Batch Gradient Norm after: 11.26963479728596
Epoch 6317/10000, Prediction Accuracy = 62.388%, Loss = 0.4218535006046295
Epoch: 6317, Batch Gradient Norm: 9.533872542219097
Epoch: 6317, Batch Gradient Norm after: 9.533872542219097
Epoch 6318/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.41171088218688967
Epoch: 6318, Batch Gradient Norm: 7.802906940688544
Epoch: 6318, Batch Gradient Norm after: 7.802906940688544
Epoch 6319/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.4006601870059967
Epoch: 6319, Batch Gradient Norm: 7.7046826869504175
Epoch: 6319, Batch Gradient Norm after: 7.7046826869504175
Epoch 6320/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.40361644625663756
Epoch: 6320, Batch Gradient Norm: 10.947740731746926
Epoch: 6320, Batch Gradient Norm after: 10.947740731746926
Epoch 6321/10000, Prediction Accuracy = 62.378%, Loss = 0.4192639708518982
Epoch: 6321, Batch Gradient Norm: 9.695252158793426
Epoch: 6321, Batch Gradient Norm after: 9.695252158793426
Epoch 6322/10000, Prediction Accuracy = 62.48%, Loss = 0.4132003903388977
Epoch: 6322, Batch Gradient Norm: 7.83268015905359
Epoch: 6322, Batch Gradient Norm after: 7.83268015905359
Epoch 6323/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.40069172382354734
Epoch: 6323, Batch Gradient Norm: 10.971074287888282
Epoch: 6323, Batch Gradient Norm after: 10.971074287888282
Epoch 6324/10000, Prediction Accuracy = 62.34599999999999%, Loss = 0.4213836133480072
Epoch: 6324, Batch Gradient Norm: 10.580271927350791
Epoch: 6324, Batch Gradient Norm after: 10.580271927350791
Epoch 6325/10000, Prediction Accuracy = 62.50600000000001%, Loss = 0.41933671832084657
Epoch: 6325, Batch Gradient Norm: 8.866481729884072
Epoch: 6325, Batch Gradient Norm after: 8.866481729884072
Epoch 6326/10000, Prediction Accuracy = 62.448%, Loss = 0.4076426088809967
Epoch: 6326, Batch Gradient Norm: 9.431095900128712
Epoch: 6326, Batch Gradient Norm after: 9.431095900128712
Epoch 6327/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.41178159713745116
Epoch: 6327, Batch Gradient Norm: 12.097895664780157
Epoch: 6327, Batch Gradient Norm after: 12.097895664780157
Epoch 6328/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.4350976705551147
Epoch: 6328, Batch Gradient Norm: 10.429567268647496
Epoch: 6328, Batch Gradient Norm after: 10.429567268647496
Epoch 6329/10000, Prediction Accuracy = 62.34000000000001%, Loss = 0.4171979486942291
Epoch: 6329, Batch Gradient Norm: 11.18540811779998
Epoch: 6329, Batch Gradient Norm after: 11.18540811779998
Epoch 6330/10000, Prediction Accuracy = 62.419999999999995%, Loss = 0.4235048770904541
Epoch: 6330, Batch Gradient Norm: 11.902199990597847
Epoch: 6330, Batch Gradient Norm after: 11.902199990597847
Epoch 6331/10000, Prediction Accuracy = 62.47399999999999%, Loss = 0.4295042991638184
Epoch: 6331, Batch Gradient Norm: 8.689175518999226
Epoch: 6331, Batch Gradient Norm after: 8.689175518999226
Epoch 6332/10000, Prediction Accuracy = 62.35999999999999%, Loss = 0.4067119836807251
Epoch: 6332, Batch Gradient Norm: 9.72618988683969
Epoch: 6332, Batch Gradient Norm after: 9.72618988683969
Epoch 6333/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.4150181233882904
Epoch: 6333, Batch Gradient Norm: 7.179333992545421
Epoch: 6333, Batch Gradient Norm after: 7.179333992545421
Epoch 6334/10000, Prediction Accuracy = 62.464%, Loss = 0.39551929831504823
Epoch: 6334, Batch Gradient Norm: 8.563429989561897
Epoch: 6334, Batch Gradient Norm after: 8.563429989561897
Epoch 6335/10000, Prediction Accuracy = 62.398%, Loss = 0.4064111769199371
Epoch: 6335, Batch Gradient Norm: 7.034390255975001
Epoch: 6335, Batch Gradient Norm after: 7.034390255975001
Epoch 6336/10000, Prediction Accuracy = 62.48%, Loss = 0.3943250298500061
Epoch: 6336, Batch Gradient Norm: 9.279650688886674
Epoch: 6336, Batch Gradient Norm after: 9.279650688886674
Epoch 6337/10000, Prediction Accuracy = 62.342%, Loss = 0.41076491475105287
Epoch: 6337, Batch Gradient Norm: 11.889519698501118
Epoch: 6337, Batch Gradient Norm after: 11.889519698501118
Epoch 6338/10000, Prediction Accuracy = 62.174%, Loss = 0.4310949981212616
Epoch: 6338, Batch Gradient Norm: 9.950170270465863
Epoch: 6338, Batch Gradient Norm after: 9.950170270465863
Epoch 6339/10000, Prediction Accuracy = 62.322%, Loss = 0.4151704967021942
Epoch: 6339, Batch Gradient Norm: 11.371791414198078
Epoch: 6339, Batch Gradient Norm after: 11.371791414198078
Epoch 6340/10000, Prediction Accuracy = 62.21%, Loss = 0.42686989307403567
Epoch: 6340, Batch Gradient Norm: 9.402780385102256
Epoch: 6340, Batch Gradient Norm after: 9.402780385102256
Epoch 6341/10000, Prediction Accuracy = 62.504%, Loss = 0.40985018014907837
Epoch: 6341, Batch Gradient Norm: 8.190048281194866
Epoch: 6341, Batch Gradient Norm after: 8.190048281194866
Epoch 6342/10000, Prediction Accuracy = 62.376%, Loss = 0.4022029161453247
Epoch: 6342, Batch Gradient Norm: 10.084297103199482
Epoch: 6342, Batch Gradient Norm after: 10.084297103199482
Epoch 6343/10000, Prediction Accuracy = 62.378%, Loss = 0.4183488368988037
Epoch: 6343, Batch Gradient Norm: 7.809361694259848
Epoch: 6343, Batch Gradient Norm after: 7.809361694259848
Epoch 6344/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.40298250913619993
Epoch: 6344, Batch Gradient Norm: 8.498511521649649
Epoch: 6344, Batch Gradient Norm after: 8.498511521649649
Epoch 6345/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4077493488788605
Epoch: 6345, Batch Gradient Norm: 12.540254254129923
Epoch: 6345, Batch Gradient Norm after: 12.540254254129923
Epoch 6346/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.433909547328949
Epoch: 6346, Batch Gradient Norm: 12.365497871571218
Epoch: 6346, Batch Gradient Norm after: 12.365497871571218
Epoch 6347/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.43084887266159055
Epoch: 6347, Batch Gradient Norm: 10.951988983927377
Epoch: 6347, Batch Gradient Norm after: 10.951988983927377
Epoch 6348/10000, Prediction Accuracy = 62.5%, Loss = 0.41985536813735963
Epoch: 6348, Batch Gradient Norm: 9.063148267918017
Epoch: 6348, Batch Gradient Norm after: 9.063148267918017
Epoch 6349/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4077783107757568
Epoch: 6349, Batch Gradient Norm: 8.7238546978153
Epoch: 6349, Batch Gradient Norm after: 8.7238546978153
Epoch 6350/10000, Prediction Accuracy = 62.484%, Loss = 0.40290295481681826
Epoch: 6350, Batch Gradient Norm: 10.800433110382848
Epoch: 6350, Batch Gradient Norm after: 10.800433110382848
Epoch 6351/10000, Prediction Accuracy = 62.278%, Loss = 0.4182888090610504
Epoch: 6351, Batch Gradient Norm: 10.531228186233967
Epoch: 6351, Batch Gradient Norm after: 10.531228186233967
Epoch 6352/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4198933780193329
Epoch: 6352, Batch Gradient Norm: 10.912195670919402
Epoch: 6352, Batch Gradient Norm after: 10.912195670919402
Epoch 6353/10000, Prediction Accuracy = 62.646%, Loss = 0.42266557812690736
Epoch: 6353, Batch Gradient Norm: 11.342730679543681
Epoch: 6353, Batch Gradient Norm after: 11.342730679543681
Epoch 6354/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.42497901916503905
Epoch: 6354, Batch Gradient Norm: 9.103090127385098
Epoch: 6354, Batch Gradient Norm after: 9.103090127385098
Epoch 6355/10000, Prediction Accuracy = 62.512%, Loss = 0.40694300532341005
Epoch: 6355, Batch Gradient Norm: 8.94129489050697
Epoch: 6355, Batch Gradient Norm after: 8.94129489050697
Epoch 6356/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.4076621174812317
Epoch: 6356, Batch Gradient Norm: 8.583197517412472
Epoch: 6356, Batch Gradient Norm after: 8.583197517412472
Epoch 6357/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.40588918924331663
Epoch: 6357, Batch Gradient Norm: 6.804117015371128
Epoch: 6357, Batch Gradient Norm after: 6.804117015371128
Epoch 6358/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.3942364752292633
Epoch: 6358, Batch Gradient Norm: 7.75240020669174
Epoch: 6358, Batch Gradient Norm after: 7.75240020669174
Epoch 6359/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.39891754388809203
Epoch: 6359, Batch Gradient Norm: 11.097322055867219
Epoch: 6359, Batch Gradient Norm after: 11.097322055867219
Epoch 6360/10000, Prediction Accuracy = 62.212%, Loss = 0.4226806879043579
Epoch: 6360, Batch Gradient Norm: 10.578848119451031
Epoch: 6360, Batch Gradient Norm after: 10.578848119451031
Epoch 6361/10000, Prediction Accuracy = 62.48%, Loss = 0.4184622287750244
Epoch: 6361, Batch Gradient Norm: 7.90930910455482
Epoch: 6361, Batch Gradient Norm after: 7.90930910455482
Epoch 6362/10000, Prediction Accuracy = 62.552%, Loss = 0.4010539293289185
Epoch: 6362, Batch Gradient Norm: 7.198210563623767
Epoch: 6362, Batch Gradient Norm after: 7.198210563623767
Epoch 6363/10000, Prediction Accuracy = 62.54%, Loss = 0.39620585441589357
Epoch: 6363, Batch Gradient Norm: 9.689884923664433
Epoch: 6363, Batch Gradient Norm after: 9.689884923664433
Epoch 6364/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.40849403142929075
Epoch: 6364, Batch Gradient Norm: 12.868185630013224
Epoch: 6364, Batch Gradient Norm after: 12.868185630013224
Epoch 6365/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4355261981487274
Epoch: 6365, Batch Gradient Norm: 8.269367113936791
Epoch: 6365, Batch Gradient Norm after: 8.269367113936791
Epoch 6366/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.40319005250930784
Epoch: 6366, Batch Gradient Norm: 7.699696294081006
Epoch: 6366, Batch Gradient Norm after: 7.699696294081006
Epoch 6367/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.4002417027950287
Epoch: 6367, Batch Gradient Norm: 7.432104087172959
Epoch: 6367, Batch Gradient Norm after: 7.432104087172959
Epoch 6368/10000, Prediction Accuracy = 62.294%, Loss = 0.3963345766067505
Epoch: 6368, Batch Gradient Norm: 11.413836150032136
Epoch: 6368, Batch Gradient Norm after: 11.413836150032136
Epoch 6369/10000, Prediction Accuracy = 62.298%, Loss = 0.4213462591171265
Epoch: 6369, Batch Gradient Norm: 13.153317801606367
Epoch: 6369, Batch Gradient Norm after: 13.153317801606367
Epoch 6370/10000, Prediction Accuracy = 62.376%, Loss = 0.4357001960277557
Epoch: 6370, Batch Gradient Norm: 9.946992511129253
Epoch: 6370, Batch Gradient Norm after: 9.946992511129253
Epoch 6371/10000, Prediction Accuracy = 62.438%, Loss = 0.41253808736801145
Epoch: 6371, Batch Gradient Norm: 10.550801837681968
Epoch: 6371, Batch Gradient Norm after: 10.550801837681968
Epoch 6372/10000, Prediction Accuracy = 62.362%, Loss = 0.41730560064315797
Epoch: 6372, Batch Gradient Norm: 9.466412241149078
Epoch: 6372, Batch Gradient Norm after: 9.466412241149078
Epoch 6373/10000, Prediction Accuracy = 62.428%, Loss = 0.4145653486251831
Epoch: 6373, Batch Gradient Norm: 5.932684481740455
Epoch: 6373, Batch Gradient Norm after: 5.932684481740455
Epoch 6374/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.3890308141708374
Epoch: 6374, Batch Gradient Norm: 8.323177881008313
Epoch: 6374, Batch Gradient Norm after: 8.323177881008313
Epoch 6375/10000, Prediction Accuracy = 62.35%, Loss = 0.4019138038158417
Epoch: 6375, Batch Gradient Norm: 9.007084780519936
Epoch: 6375, Batch Gradient Norm after: 9.007084780519936
Epoch 6376/10000, Prediction Accuracy = 62.362%, Loss = 0.40676161646842957
Epoch: 6376, Batch Gradient Norm: 11.212355957410812
Epoch: 6376, Batch Gradient Norm after: 11.212355957410812
Epoch 6377/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.4263133525848389
Epoch: 6377, Batch Gradient Norm: 10.808691377322596
Epoch: 6377, Batch Gradient Norm after: 10.808691377322596
Epoch 6378/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.42074230313301086
Epoch: 6378, Batch Gradient Norm: 9.925598252535028
Epoch: 6378, Batch Gradient Norm after: 9.925598252535028
Epoch 6379/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4142878592014313
Epoch: 6379, Batch Gradient Norm: 9.708597199310622
Epoch: 6379, Batch Gradient Norm after: 9.708597199310622
Epoch 6380/10000, Prediction Accuracy = 62.38599999999999%, Loss = 0.4099320113658905
Epoch: 6380, Batch Gradient Norm: 9.93871749536724
Epoch: 6380, Batch Gradient Norm after: 9.93871749536724
Epoch 6381/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.41432976722717285
Epoch: 6381, Batch Gradient Norm: 11.757621314054964
Epoch: 6381, Batch Gradient Norm after: 11.757621314054964
Epoch 6382/10000, Prediction Accuracy = 62.544%, Loss = 0.43218269348144533
Epoch: 6382, Batch Gradient Norm: 5.000928935715586
Epoch: 6382, Batch Gradient Norm after: 5.000928935715586
Epoch 6383/10000, Prediction Accuracy = 62.416%, Loss = 0.3861530840396881
Epoch: 6383, Batch Gradient Norm: 6.804918537630937
Epoch: 6383, Batch Gradient Norm after: 6.804918537630937
Epoch 6384/10000, Prediction Accuracy = 62.42%, Loss = 0.3956387460231781
Epoch: 6384, Batch Gradient Norm: 7.8643318249147
Epoch: 6384, Batch Gradient Norm after: 7.8643318249147
Epoch 6385/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.3981255292892456
Epoch: 6385, Batch Gradient Norm: 9.421882562511557
Epoch: 6385, Batch Gradient Norm after: 9.421882562511557
Epoch 6386/10000, Prediction Accuracy = 62.42%, Loss = 0.4071901500225067
Epoch: 6386, Batch Gradient Norm: 10.062752324739193
Epoch: 6386, Batch Gradient Norm after: 10.062752324739193
Epoch 6387/10000, Prediction Accuracy = 62.462%, Loss = 0.4143730640411377
Epoch: 6387, Batch Gradient Norm: 10.610191705313305
Epoch: 6387, Batch Gradient Norm after: 10.610191705313305
Epoch 6388/10000, Prediction Accuracy = 62.448%, Loss = 0.41853408217430116
Epoch: 6388, Batch Gradient Norm: 11.288902508543673
Epoch: 6388, Batch Gradient Norm after: 11.288902508543673
Epoch 6389/10000, Prediction Accuracy = 62.386%, Loss = 0.4262264907360077
Epoch: 6389, Batch Gradient Norm: 10.388365048228964
Epoch: 6389, Batch Gradient Norm after: 10.388365048228964
Epoch 6390/10000, Prediction Accuracy = 62.6%, Loss = 0.41428988575935366
Epoch: 6390, Batch Gradient Norm: 10.196046617672327
Epoch: 6390, Batch Gradient Norm after: 10.196046617672327
Epoch 6391/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4138584196567535
Epoch: 6391, Batch Gradient Norm: 8.60917613602301
Epoch: 6391, Batch Gradient Norm after: 8.60917613602301
Epoch 6392/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.40437920689582824
Epoch: 6392, Batch Gradient Norm: 10.597186702645864
Epoch: 6392, Batch Gradient Norm after: 10.597186702645864
Epoch 6393/10000, Prediction Accuracy = 62.402%, Loss = 0.414410138130188
Epoch: 6393, Batch Gradient Norm: 11.672642338482875
Epoch: 6393, Batch Gradient Norm after: 11.672642338482875
Epoch 6394/10000, Prediction Accuracy = 62.414%, Loss = 0.42487512826919555
Epoch: 6394, Batch Gradient Norm: 8.616771461283328
Epoch: 6394, Batch Gradient Norm after: 8.616771461283328
Epoch 6395/10000, Prediction Accuracy = 62.28599999999999%, Loss = 0.4101939022541046
Epoch: 6395, Batch Gradient Norm: 13.100090236324363
Epoch: 6395, Batch Gradient Norm after: 13.100090236324363
Epoch 6396/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.43992278575897215
Epoch: 6396, Batch Gradient Norm: 12.972711991866676
Epoch: 6396, Batch Gradient Norm after: 12.972711991866676
Epoch 6397/10000, Prediction Accuracy = 62.348%, Loss = 0.4410460710525513
Epoch: 6397, Batch Gradient Norm: 8.997631525874537
Epoch: 6397, Batch Gradient Norm after: 8.997631525874537
Epoch 6398/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.4066742181777954
Epoch: 6398, Batch Gradient Norm: 8.158221037820487
Epoch: 6398, Batch Gradient Norm after: 8.158221037820487
Epoch 6399/10000, Prediction Accuracy = 62.396%, Loss = 0.402812534570694
Epoch: 6399, Batch Gradient Norm: 7.751617976042856
Epoch: 6399, Batch Gradient Norm after: 7.751617976042856
Epoch 6400/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.39942599534988404
Epoch: 6400, Batch Gradient Norm: 8.333694799618826
Epoch: 6400, Batch Gradient Norm after: 8.333694799618826
Epoch 6401/10000, Prediction Accuracy = 62.46200000000001%, Loss = 0.4004110276699066
Epoch: 6401, Batch Gradient Norm: 8.907281131295822
Epoch: 6401, Batch Gradient Norm after: 8.907281131295822
Epoch 6402/10000, Prediction Accuracy = 62.428%, Loss = 0.40805621147155763
Epoch: 6402, Batch Gradient Norm: 7.2783257158078625
Epoch: 6402, Batch Gradient Norm after: 7.2783257158078625
Epoch 6403/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.3960404932498932
Epoch: 6403, Batch Gradient Norm: 9.70219664134063
Epoch: 6403, Batch Gradient Norm after: 9.70219664134063
Epoch 6404/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4122200310230255
Epoch: 6404, Batch Gradient Norm: 9.874933188729027
Epoch: 6404, Batch Gradient Norm after: 9.874933188729027
Epoch 6405/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.41107314825057983
Epoch: 6405, Batch Gradient Norm: 7.978673597282929
Epoch: 6405, Batch Gradient Norm after: 7.978673597282929
Epoch 6406/10000, Prediction Accuracy = 62.55%, Loss = 0.3999046564102173
Epoch: 6406, Batch Gradient Norm: 7.6996156292676705
Epoch: 6406, Batch Gradient Norm after: 7.6996156292676705
Epoch 6407/10000, Prediction Accuracy = 62.4%, Loss = 0.3981059968471527
Epoch: 6407, Batch Gradient Norm: 14.223684994225668
Epoch: 6407, Batch Gradient Norm after: 14.223684994225668
Epoch 6408/10000, Prediction Accuracy = 62.330000000000005%, Loss = 0.4449828267097473
Epoch: 6408, Batch Gradient Norm: 12.191706989230658
Epoch: 6408, Batch Gradient Norm after: 12.191706989230658
Epoch 6409/10000, Prediction Accuracy = 62.492%, Loss = 0.4301843047142029
Epoch: 6409, Batch Gradient Norm: 9.76447197319865
Epoch: 6409, Batch Gradient Norm after: 9.76447197319865
Epoch 6410/10000, Prediction Accuracy = 62.412%, Loss = 0.4109599769115448
Epoch: 6410, Batch Gradient Norm: 6.075730262299421
Epoch: 6410, Batch Gradient Norm after: 6.075730262299421
Epoch 6411/10000, Prediction Accuracy = 62.4%, Loss = 0.3916959404945374
Epoch: 6411, Batch Gradient Norm: 7.657052397987299
Epoch: 6411, Batch Gradient Norm after: 7.657052397987299
Epoch 6412/10000, Prediction Accuracy = 62.416%, Loss = 0.3975337028503418
Epoch: 6412, Batch Gradient Norm: 7.898188220640445
Epoch: 6412, Batch Gradient Norm after: 7.898188220640445
Epoch 6413/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.3982423424720764
Epoch: 6413, Batch Gradient Norm: 11.99623283732302
Epoch: 6413, Batch Gradient Norm after: 11.99623283732302
Epoch 6414/10000, Prediction Accuracy = 62.42%, Loss = 0.4261544108390808
Epoch: 6414, Batch Gradient Norm: 12.121401877283544
Epoch: 6414, Batch Gradient Norm after: 12.121401877283544
Epoch 6415/10000, Prediction Accuracy = 62.419999999999995%, Loss = 0.4285631000995636
Epoch: 6415, Batch Gradient Norm: 11.434502142019532
Epoch: 6415, Batch Gradient Norm after: 11.434502142019532
Epoch 6416/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.42584148049354553
Epoch: 6416, Batch Gradient Norm: 12.094760586438145
Epoch: 6416, Batch Gradient Norm after: 12.094760586438145
Epoch 6417/10000, Prediction Accuracy = 62.342%, Loss = 0.4265326917171478
Epoch: 6417, Batch Gradient Norm: 9.257515749981168
Epoch: 6417, Batch Gradient Norm after: 9.257515749981168
Epoch 6418/10000, Prediction Accuracy = 62.464%, Loss = 0.4081891715526581
Epoch: 6418, Batch Gradient Norm: 7.383409118496311
Epoch: 6418, Batch Gradient Norm after: 7.383409118496311
Epoch 6419/10000, Prediction Accuracy = 62.364%, Loss = 0.3985694408416748
Epoch: 6419, Batch Gradient Norm: 8.847335799410324
Epoch: 6419, Batch Gradient Norm after: 8.847335799410324
Epoch 6420/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.40540221333503723
Epoch: 6420, Batch Gradient Norm: 9.121830084380079
Epoch: 6420, Batch Gradient Norm after: 9.121830084380079
Epoch 6421/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.40851540565490724
Epoch: 6421, Batch Gradient Norm: 8.066734352186208
Epoch: 6421, Batch Gradient Norm after: 8.066734352186208
Epoch 6422/10000, Prediction Accuracy = 62.426%, Loss = 0.39997885227203367
Epoch: 6422, Batch Gradient Norm: 8.472381779719083
Epoch: 6422, Batch Gradient Norm after: 8.472381779719083
Epoch 6423/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.405840003490448
Epoch: 6423, Batch Gradient Norm: 9.071632775285874
Epoch: 6423, Batch Gradient Norm after: 9.071632775285874
Epoch 6424/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.4094169199466705
Epoch: 6424, Batch Gradient Norm: 8.08148208924776
Epoch: 6424, Batch Gradient Norm after: 8.08148208924776
Epoch 6425/10000, Prediction Accuracy = 62.331999999999994%, Loss = 0.39821418523788454
Epoch: 6425, Batch Gradient Norm: 10.535082759077753
Epoch: 6425, Batch Gradient Norm after: 10.535082759077753
Epoch 6426/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.4156085729598999
Epoch: 6426, Batch Gradient Norm: 8.965891480094044
Epoch: 6426, Batch Gradient Norm after: 8.965891480094044
Epoch 6427/10000, Prediction Accuracy = 62.374%, Loss = 0.4087547123432159
Epoch: 6427, Batch Gradient Norm: 10.807727601919758
Epoch: 6427, Batch Gradient Norm after: 10.807727601919758
Epoch 6428/10000, Prediction Accuracy = 62.384%, Loss = 0.41982135772705076
Epoch: 6428, Batch Gradient Norm: 10.43253021416037
Epoch: 6428, Batch Gradient Norm after: 10.43253021416037
Epoch 6429/10000, Prediction Accuracy = 62.42800000000001%, Loss = 0.41908461451530454
Epoch: 6429, Batch Gradient Norm: 9.014232663194814
Epoch: 6429, Batch Gradient Norm after: 9.014232663194814
Epoch 6430/10000, Prediction Accuracy = 62.314%, Loss = 0.4088541090488434
Epoch: 6430, Batch Gradient Norm: 12.1722927317483
Epoch: 6430, Batch Gradient Norm after: 12.1722927317483
Epoch 6431/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.42985605001449584
Epoch: 6431, Batch Gradient Norm: 9.733841291809174
Epoch: 6431, Batch Gradient Norm after: 9.733841291809174
Epoch 6432/10000, Prediction Accuracy = 62.46600000000001%, Loss = 0.409482342004776
Epoch: 6432, Batch Gradient Norm: 11.750128196783658
Epoch: 6432, Batch Gradient Norm after: 11.750128196783658
Epoch 6433/10000, Prediction Accuracy = 62.484%, Loss = 0.4259613692760468
Epoch: 6433, Batch Gradient Norm: 12.58413825183194
Epoch: 6433, Batch Gradient Norm after: 12.58413825183194
Epoch 6434/10000, Prediction Accuracy = 62.436%, Loss = 0.43151889443397523
Epoch: 6434, Batch Gradient Norm: 8.90590287981701
Epoch: 6434, Batch Gradient Norm after: 8.90590287981701
Epoch 6435/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.4043443322181702
Epoch: 6435, Batch Gradient Norm: 8.41962933072432
Epoch: 6435, Batch Gradient Norm after: 8.41962933072432
Epoch 6436/10000, Prediction Accuracy = 62.34000000000001%, Loss = 0.4038690984249115
Epoch: 6436, Batch Gradient Norm: 8.11569551159117
Epoch: 6436, Batch Gradient Norm after: 8.11569551159117
Epoch 6437/10000, Prediction Accuracy = 62.488%, Loss = 0.40087650418281556
Epoch: 6437, Batch Gradient Norm: 9.769212980298283
Epoch: 6437, Batch Gradient Norm after: 9.769212980298283
Epoch 6438/10000, Prediction Accuracy = 62.476%, Loss = 0.40982582569122317
Epoch: 6438, Batch Gradient Norm: 9.237980453810195
Epoch: 6438, Batch Gradient Norm after: 9.237980453810195
Epoch 6439/10000, Prediction Accuracy = 62.572%, Loss = 0.40885016322135925
Epoch: 6439, Batch Gradient Norm: 9.340377978662431
Epoch: 6439, Batch Gradient Norm after: 9.340377978662431
Epoch 6440/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.4114367485046387
Epoch: 6440, Batch Gradient Norm: 8.910282357513566
Epoch: 6440, Batch Gradient Norm after: 8.910282357513566
Epoch 6441/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.4067474901676178
Epoch: 6441, Batch Gradient Norm: 10.318767967904066
Epoch: 6441, Batch Gradient Norm after: 10.318767967904066
Epoch 6442/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.4135388135910034
Epoch: 6442, Batch Gradient Norm: 12.408518548944295
Epoch: 6442, Batch Gradient Norm after: 12.408518548944295
Epoch 6443/10000, Prediction Accuracy = 62.516000000000005%, Loss = 0.43022607564926146
Epoch: 6443, Batch Gradient Norm: 9.153367026035298
Epoch: 6443, Batch Gradient Norm after: 9.153367026035298
Epoch 6444/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.406974059343338
Epoch: 6444, Batch Gradient Norm: 8.025760588935826
Epoch: 6444, Batch Gradient Norm after: 8.025760588935826
Epoch 6445/10000, Prediction Accuracy = 62.376%, Loss = 0.40060927867889407
Epoch: 6445, Batch Gradient Norm: 9.995021625387107
Epoch: 6445, Batch Gradient Norm after: 9.995021625387107
Epoch 6446/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.4119944334030151
Epoch: 6446, Batch Gradient Norm: 8.993560380886173
Epoch: 6446, Batch Gradient Norm after: 8.993560380886173
Epoch 6447/10000, Prediction Accuracy = 62.462%, Loss = 0.4049882233142853
Epoch: 6447, Batch Gradient Norm: 9.838557165089409
Epoch: 6447, Batch Gradient Norm after: 9.838557165089409
Epoch 6448/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.41230406165122985
Epoch: 6448, Batch Gradient Norm: 9.294692118919901
Epoch: 6448, Batch Gradient Norm after: 9.294692118919901
Epoch 6449/10000, Prediction Accuracy = 62.426%, Loss = 0.40950155854225156
Epoch: 6449, Batch Gradient Norm: 10.189141478487175
Epoch: 6449, Batch Gradient Norm after: 10.189141478487175
Epoch 6450/10000, Prediction Accuracy = 62.438%, Loss = 0.41268762946128845
Epoch: 6450, Batch Gradient Norm: 9.645644845147148
Epoch: 6450, Batch Gradient Norm after: 9.645644845147148
Epoch 6451/10000, Prediction Accuracy = 62.436%, Loss = 0.40787733197212217
Epoch: 6451, Batch Gradient Norm: 9.490860749888652
Epoch: 6451, Batch Gradient Norm after: 9.490860749888652
Epoch 6452/10000, Prediction Accuracy = 62.386%, Loss = 0.4079470932483673
Epoch: 6452, Batch Gradient Norm: 9.058834841392274
Epoch: 6452, Batch Gradient Norm after: 9.058834841392274
Epoch 6453/10000, Prediction Accuracy = 62.352%, Loss = 0.40546995401382446
Epoch: 6453, Batch Gradient Norm: 8.558950061692663
Epoch: 6453, Batch Gradient Norm after: 8.558950061692663
Epoch 6454/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.40158985257148744
Epoch: 6454, Batch Gradient Norm: 9.75395415129115
Epoch: 6454, Batch Gradient Norm after: 9.75395415129115
Epoch 6455/10000, Prediction Accuracy = 62.44000000000001%, Loss = 0.41113112568855287
Epoch: 6455, Batch Gradient Norm: 10.495327342716825
Epoch: 6455, Batch Gradient Norm after: 10.495327342716825
Epoch 6456/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.4151554048061371
Epoch: 6456, Batch Gradient Norm: 9.241604534156146
Epoch: 6456, Batch Gradient Norm after: 9.241604534156146
Epoch 6457/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.40626893043518064
Epoch: 6457, Batch Gradient Norm: 12.8019846211121
Epoch: 6457, Batch Gradient Norm after: 12.8019846211121
Epoch 6458/10000, Prediction Accuracy = 62.452%, Loss = 0.4344900965690613
Epoch: 6458, Batch Gradient Norm: 8.929587827175668
Epoch: 6458, Batch Gradient Norm after: 8.929587827175668
Epoch 6459/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.4039354085922241
Epoch: 6459, Batch Gradient Norm: 8.603180317097063
Epoch: 6459, Batch Gradient Norm after: 8.603180317097063
Epoch 6460/10000, Prediction Accuracy = 62.312%, Loss = 0.40415896773338317
Epoch: 6460, Batch Gradient Norm: 9.415876866685116
Epoch: 6460, Batch Gradient Norm after: 9.415876866685116
Epoch 6461/10000, Prediction Accuracy = 62.338%, Loss = 0.41209532618522643
Epoch: 6461, Batch Gradient Norm: 8.835217435076668
Epoch: 6461, Batch Gradient Norm after: 8.835217435076668
Epoch 6462/10000, Prediction Accuracy = 62.378%, Loss = 0.40521517395973206
Epoch: 6462, Batch Gradient Norm: 10.387297008760756
Epoch: 6462, Batch Gradient Norm after: 10.387297008760756
Epoch 6463/10000, Prediction Accuracy = 62.528%, Loss = 0.4151717841625214
Epoch: 6463, Batch Gradient Norm: 10.095947266669683
Epoch: 6463, Batch Gradient Norm after: 10.095947266669683
Epoch 6464/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.41250365376472475
Epoch: 6464, Batch Gradient Norm: 7.585408017945632
Epoch: 6464, Batch Gradient Norm after: 7.585408017945632
Epoch 6465/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.39709017872810365
Epoch: 6465, Batch Gradient Norm: 8.928676363610746
Epoch: 6465, Batch Gradient Norm after: 8.928676363610746
Epoch 6466/10000, Prediction Accuracy = 62.414%, Loss = 0.40445263385772706
Epoch: 6466, Batch Gradient Norm: 7.900242339792102
Epoch: 6466, Batch Gradient Norm after: 7.900242339792102
Epoch 6467/10000, Prediction Accuracy = 62.626%, Loss = 0.3979167103767395
Epoch: 6467, Batch Gradient Norm: 7.938191777743385
Epoch: 6467, Batch Gradient Norm after: 7.938191777743385
Epoch 6468/10000, Prediction Accuracy = 62.572%, Loss = 0.39831326007843015
Epoch: 6468, Batch Gradient Norm: 13.235468302143776
Epoch: 6468, Batch Gradient Norm after: 13.235468302143776
Epoch 6469/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.44013017416000366
Epoch: 6469, Batch Gradient Norm: 9.555313227128417
Epoch: 6469, Batch Gradient Norm after: 9.555313227128417
Epoch 6470/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.4084113597869873
Epoch: 6470, Batch Gradient Norm: 9.96702474354761
Epoch: 6470, Batch Gradient Norm after: 9.96702474354761
Epoch 6471/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.41044580936431885
Epoch: 6471, Batch Gradient Norm: 10.401960601061992
Epoch: 6471, Batch Gradient Norm after: 10.401960601061992
Epoch 6472/10000, Prediction Accuracy = 62.318%, Loss = 0.4148699164390564
Epoch: 6472, Batch Gradient Norm: 11.239106321670576
Epoch: 6472, Batch Gradient Norm after: 11.239106321670576
Epoch 6473/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.41796584129333497
Epoch: 6473, Batch Gradient Norm: 10.959654733044266
Epoch: 6473, Batch Gradient Norm after: 10.959654733044266
Epoch 6474/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.4201375305652618
Epoch: 6474, Batch Gradient Norm: 8.619894020842985
Epoch: 6474, Batch Gradient Norm after: 8.619894020842985
Epoch 6475/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4019473850727081
Epoch: 6475, Batch Gradient Norm: 8.111371553251113
Epoch: 6475, Batch Gradient Norm after: 8.111371553251113
Epoch 6476/10000, Prediction Accuracy = 62.65%, Loss = 0.3997752904891968
Epoch: 6476, Batch Gradient Norm: 8.079461050642337
Epoch: 6476, Batch Gradient Norm after: 8.079461050642337
Epoch 6477/10000, Prediction Accuracy = 62.42%, Loss = 0.400015252828598
Epoch: 6477, Batch Gradient Norm: 7.678952060541063
Epoch: 6477, Batch Gradient Norm after: 7.678952060541063
Epoch 6478/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.39695964455604554
Epoch: 6478, Batch Gradient Norm: 10.180435122986339
Epoch: 6478, Batch Gradient Norm after: 10.180435122986339
Epoch 6479/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.4107528328895569
Epoch: 6479, Batch Gradient Norm: 13.126884432063111
Epoch: 6479, Batch Gradient Norm after: 13.126884432063111
Epoch 6480/10000, Prediction Accuracy = 62.436%, Loss = 0.44221081733703616
Epoch: 6480, Batch Gradient Norm: 10.495684526674344
Epoch: 6480, Batch Gradient Norm after: 10.495684526674344
Epoch 6481/10000, Prediction Accuracy = 62.274%, Loss = 0.4197934627532959
Epoch: 6481, Batch Gradient Norm: 11.510002159202193
Epoch: 6481, Batch Gradient Norm after: 11.510002159202193
Epoch 6482/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4206292390823364
Epoch: 6482, Batch Gradient Norm: 9.43523386294113
Epoch: 6482, Batch Gradient Norm after: 9.43523386294113
Epoch 6483/10000, Prediction Accuracy = 62.54%, Loss = 0.40890930891036986
Epoch: 6483, Batch Gradient Norm: 8.369296249067387
Epoch: 6483, Batch Gradient Norm after: 8.369296249067387
Epoch 6484/10000, Prediction Accuracy = 62.45%, Loss = 0.39997887015342715
Epoch: 6484, Batch Gradient Norm: 8.934503749190913
Epoch: 6484, Batch Gradient Norm after: 8.934503749190913
Epoch 6485/10000, Prediction Accuracy = 62.472%, Loss = 0.40175368189811705
Epoch: 6485, Batch Gradient Norm: 10.94634327918773
Epoch: 6485, Batch Gradient Norm after: 10.94634327918773
Epoch 6486/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.4175560474395752
Epoch: 6486, Batch Gradient Norm: 10.59597907579048
Epoch: 6486, Batch Gradient Norm after: 10.59597907579048
Epoch 6487/10000, Prediction Accuracy = 62.402%, Loss = 0.41507362127304076
Epoch: 6487, Batch Gradient Norm: 8.44924761341312
Epoch: 6487, Batch Gradient Norm after: 8.44924761341312
Epoch 6488/10000, Prediction Accuracy = 62.412%, Loss = 0.4021998405456543
Epoch: 6488, Batch Gradient Norm: 8.196711305750837
Epoch: 6488, Batch Gradient Norm after: 8.196711305750837
Epoch 6489/10000, Prediction Accuracy = 62.428%, Loss = 0.39934268593788147
Epoch: 6489, Batch Gradient Norm: 7.952717073448459
Epoch: 6489, Batch Gradient Norm after: 7.952717073448459
Epoch 6490/10000, Prediction Accuracy = 62.532%, Loss = 0.39909651279449465
Epoch: 6490, Batch Gradient Norm: 9.213217617044192
Epoch: 6490, Batch Gradient Norm after: 9.213217617044192
Epoch 6491/10000, Prediction Accuracy = 62.476%, Loss = 0.40860362648963927
Epoch: 6491, Batch Gradient Norm: 8.374006785972073
Epoch: 6491, Batch Gradient Norm after: 8.374006785972073
Epoch 6492/10000, Prediction Accuracy = 62.428%, Loss = 0.39890897274017334
Epoch: 6492, Batch Gradient Norm: 10.298848343881382
Epoch: 6492, Batch Gradient Norm after: 10.298848343881382
Epoch 6493/10000, Prediction Accuracy = 62.286%, Loss = 0.41367473006248473
Epoch: 6493, Batch Gradient Norm: 12.142761325261874
Epoch: 6493, Batch Gradient Norm after: 12.142761325261874
Epoch 6494/10000, Prediction Accuracy = 62.44%, Loss = 0.4251704752445221
Epoch: 6494, Batch Gradient Norm: 11.593177064965175
Epoch: 6494, Batch Gradient Norm after: 11.593177064965175
Epoch 6495/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.4244340717792511
Epoch: 6495, Batch Gradient Norm: 10.369165075826357
Epoch: 6495, Batch Gradient Norm after: 10.369165075826357
Epoch 6496/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4156591534614563
Epoch: 6496, Batch Gradient Norm: 9.337599886127425
Epoch: 6496, Batch Gradient Norm after: 9.337599886127425
Epoch 6497/10000, Prediction Accuracy = 62.39%, Loss = 0.41039679050445554
Epoch: 6497, Batch Gradient Norm: 9.932192504998271
Epoch: 6497, Batch Gradient Norm after: 9.932192504998271
Epoch 6498/10000, Prediction Accuracy = 62.386%, Loss = 0.4118626952171326
Epoch: 6498, Batch Gradient Norm: 11.34867013564241
Epoch: 6498, Batch Gradient Norm after: 11.34867013564241
Epoch 6499/10000, Prediction Accuracy = 62.362%, Loss = 0.42072449922561644
Epoch: 6499, Batch Gradient Norm: 8.426251018391646
Epoch: 6499, Batch Gradient Norm after: 8.426251018391646
Epoch 6500/10000, Prediction Accuracy = 62.464%, Loss = 0.4019594967365265
Epoch: 6500, Batch Gradient Norm: 6.683545369065674
Epoch: 6500, Batch Gradient Norm after: 6.683545369065674
Epoch 6501/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.3907382428646088
Epoch: 6501, Batch Gradient Norm: 6.526102994993763
Epoch: 6501, Batch Gradient Norm after: 6.526102994993763
Epoch 6502/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3906375765800476
Epoch: 6502, Batch Gradient Norm: 8.295921097428453
Epoch: 6502, Batch Gradient Norm after: 8.295921097428453
Epoch 6503/10000, Prediction Accuracy = 62.432%, Loss = 0.39864528775215147
Epoch: 6503, Batch Gradient Norm: 12.798371290832982
Epoch: 6503, Batch Gradient Norm after: 12.798371290832982
Epoch 6504/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.42791550755500796
Epoch: 6504, Batch Gradient Norm: 11.017105018581624
Epoch: 6504, Batch Gradient Norm after: 11.017105018581624
Epoch 6505/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.4172905206680298
Epoch: 6505, Batch Gradient Norm: 8.556364533340503
Epoch: 6505, Batch Gradient Norm after: 8.556364533340503
Epoch 6506/10000, Prediction Accuracy = 62.448%, Loss = 0.4033979117870331
Epoch: 6506, Batch Gradient Norm: 9.706169145337531
Epoch: 6506, Batch Gradient Norm after: 9.706169145337531
Epoch 6507/10000, Prediction Accuracy = 62.44%, Loss = 0.4086027920246124
Epoch: 6507, Batch Gradient Norm: 11.002142929528503
Epoch: 6507, Batch Gradient Norm after: 11.002142929528503
Epoch 6508/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.4165708839893341
Epoch: 6508, Batch Gradient Norm: 10.696815866551944
Epoch: 6508, Batch Gradient Norm after: 10.696815866551944
Epoch 6509/10000, Prediction Accuracy = 62.58%, Loss = 0.41691837906837464
Epoch: 6509, Batch Gradient Norm: 8.91346913993236
Epoch: 6509, Batch Gradient Norm after: 8.91346913993236
Epoch 6510/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4020848870277405
Epoch: 6510, Batch Gradient Norm: 10.772598198498233
Epoch: 6510, Batch Gradient Norm after: 10.772598198498233
Epoch 6511/10000, Prediction Accuracy = 62.402%, Loss = 0.4153317153453827
Epoch: 6511, Batch Gradient Norm: 11.436077793590488
Epoch: 6511, Batch Gradient Norm after: 11.436077793590488
Epoch 6512/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.42333815097808836
Epoch: 6512, Batch Gradient Norm: 7.727484968486572
Epoch: 6512, Batch Gradient Norm after: 7.727484968486572
Epoch 6513/10000, Prediction Accuracy = 62.45400000000001%, Loss = 0.39713364839553833
Epoch: 6513, Batch Gradient Norm: 8.13595440342349
Epoch: 6513, Batch Gradient Norm after: 8.13595440342349
Epoch 6514/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.39892860054969786
Epoch: 6514, Batch Gradient Norm: 8.787291905504373
Epoch: 6514, Batch Gradient Norm after: 8.787291905504373
Epoch 6515/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4016139507293701
Epoch: 6515, Batch Gradient Norm: 9.61282232317642
Epoch: 6515, Batch Gradient Norm after: 9.61282232317642
Epoch 6516/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.40819759368896485
Epoch: 6516, Batch Gradient Norm: 10.840847323218153
Epoch: 6516, Batch Gradient Norm after: 10.840847323218153
Epoch 6517/10000, Prediction Accuracy = 62.362%, Loss = 0.4157459199428558
Epoch: 6517, Batch Gradient Norm: 9.154706249019894
Epoch: 6517, Batch Gradient Norm after: 9.154706249019894
Epoch 6518/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.4048748791217804
Epoch: 6518, Batch Gradient Norm: 7.9787408959042745
Epoch: 6518, Batch Gradient Norm after: 7.9787408959042745
Epoch 6519/10000, Prediction Accuracy = 62.456%, Loss = 0.3982055366039276
Epoch: 6519, Batch Gradient Norm: 10.534228813849202
Epoch: 6519, Batch Gradient Norm after: 10.534228813849202
Epoch 6520/10000, Prediction Accuracy = 62.35799999999999%, Loss = 0.4168354570865631
Epoch: 6520, Batch Gradient Norm: 11.862833594253557
Epoch: 6520, Batch Gradient Norm after: 11.862833594253557
Epoch 6521/10000, Prediction Accuracy = 62.444%, Loss = 0.4243511736392975
Epoch: 6521, Batch Gradient Norm: 9.261970383956976
Epoch: 6521, Batch Gradient Norm after: 9.261970383956976
Epoch 6522/10000, Prediction Accuracy = 62.372%, Loss = 0.4064092457294464
Epoch: 6522, Batch Gradient Norm: 8.767288085776531
Epoch: 6522, Batch Gradient Norm after: 8.767288085776531
Epoch 6523/10000, Prediction Accuracy = 62.436%, Loss = 0.4016911804676056
Epoch: 6523, Batch Gradient Norm: 10.052479221309984
Epoch: 6523, Batch Gradient Norm after: 10.052479221309984
Epoch 6524/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.40912647247314454
Epoch: 6524, Batch Gradient Norm: 9.7872065934786
Epoch: 6524, Batch Gradient Norm after: 9.7872065934786
Epoch 6525/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.4096961259841919
Epoch: 6525, Batch Gradient Norm: 9.304612013709354
Epoch: 6525, Batch Gradient Norm after: 9.304612013709354
Epoch 6526/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.4064780414104462
Epoch: 6526, Batch Gradient Norm: 10.301695074676037
Epoch: 6526, Batch Gradient Norm after: 10.301695074676037
Epoch 6527/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.41331794261932375
Epoch: 6527, Batch Gradient Norm: 12.350087419794457
Epoch: 6527, Batch Gradient Norm after: 12.350087419794457
Epoch 6528/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.4329055488109589
Epoch: 6528, Batch Gradient Norm: 9.717373096612999
Epoch: 6528, Batch Gradient Norm after: 9.717373096612999
Epoch 6529/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.4105514883995056
Epoch: 6529, Batch Gradient Norm: 6.910415897869779
Epoch: 6529, Batch Gradient Norm after: 6.910415897869779
Epoch 6530/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.3923737406730652
Epoch: 6530, Batch Gradient Norm: 8.553038508910959
Epoch: 6530, Batch Gradient Norm after: 8.553038508910959
Epoch 6531/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.40328397154808043
Epoch: 6531, Batch Gradient Norm: 8.332555476875447
Epoch: 6531, Batch Gradient Norm after: 8.332555476875447
Epoch 6532/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.40054802894592284
Epoch: 6532, Batch Gradient Norm: 10.586928788283746
Epoch: 6532, Batch Gradient Norm after: 10.586928788283746
Epoch 6533/10000, Prediction Accuracy = 62.496%, Loss = 0.41588929295539856
Epoch: 6533, Batch Gradient Norm: 10.43613168893117
Epoch: 6533, Batch Gradient Norm after: 10.43613168893117
Epoch 6534/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.41482911705970765
Epoch: 6534, Batch Gradient Norm: 8.966350318605606
Epoch: 6534, Batch Gradient Norm after: 8.966350318605606
Epoch 6535/10000, Prediction Accuracy = 62.532%, Loss = 0.4037723422050476
Epoch: 6535, Batch Gradient Norm: 10.288052056031395
Epoch: 6535, Batch Gradient Norm after: 10.288052056031395
Epoch 6536/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.41178945302963255
Epoch: 6536, Batch Gradient Norm: 8.376241648325935
Epoch: 6536, Batch Gradient Norm after: 8.376241648325935
Epoch 6537/10000, Prediction Accuracy = 62.444%, Loss = 0.3983471214771271
Epoch: 6537, Batch Gradient Norm: 7.80000253060001
Epoch: 6537, Batch Gradient Norm after: 7.80000253060001
Epoch 6538/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.3959242761135101
Epoch: 6538, Batch Gradient Norm: 11.557438124345516
Epoch: 6538, Batch Gradient Norm after: 11.557438124345516
Epoch 6539/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.4220526397228241
Epoch: 6539, Batch Gradient Norm: 11.268415722742875
Epoch: 6539, Batch Gradient Norm after: 11.268415722742875
Epoch 6540/10000, Prediction Accuracy = 62.388%, Loss = 0.41833474636077883
Epoch: 6540, Batch Gradient Norm: 10.797776732060626
Epoch: 6540, Batch Gradient Norm after: 10.797776732060626
Epoch 6541/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.4170296251773834
Epoch: 6541, Batch Gradient Norm: 8.208950766709465
Epoch: 6541, Batch Gradient Norm after: 8.208950766709465
Epoch 6542/10000, Prediction Accuracy = 62.384%, Loss = 0.4002944827079773
Epoch: 6542, Batch Gradient Norm: 7.774158520100498
Epoch: 6542, Batch Gradient Norm after: 7.774158520100498
Epoch 6543/10000, Prediction Accuracy = 62.386%, Loss = 0.39623541235923765
Epoch: 6543, Batch Gradient Norm: 8.092002730746405
Epoch: 6543, Batch Gradient Norm after: 8.092002730746405
Epoch 6544/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.39836875796318055
Epoch: 6544, Batch Gradient Norm: 11.308903616102686
Epoch: 6544, Batch Gradient Norm after: 11.308903616102686
Epoch 6545/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.41743982434272764
Epoch: 6545, Batch Gradient Norm: 12.340014834125453
Epoch: 6545, Batch Gradient Norm after: 12.340014834125453
Epoch 6546/10000, Prediction Accuracy = 62.512%, Loss = 0.4271373748779297
Epoch: 6546, Batch Gradient Norm: 11.24502841689233
Epoch: 6546, Batch Gradient Norm after: 11.24502841689233
Epoch 6547/10000, Prediction Accuracy = 62.376%, Loss = 0.4180965542793274
Epoch: 6547, Batch Gradient Norm: 8.365116821194695
Epoch: 6547, Batch Gradient Norm after: 8.365116821194695
Epoch 6548/10000, Prediction Accuracy = 62.472%, Loss = 0.3988649070262909
Epoch: 6548, Batch Gradient Norm: 9.47077715265376
Epoch: 6548, Batch Gradient Norm after: 9.47077715265376
Epoch 6549/10000, Prediction Accuracy = 62.424%, Loss = 0.40907199382781984
Epoch: 6549, Batch Gradient Norm: 7.431510420264385
Epoch: 6549, Batch Gradient Norm after: 7.431510420264385
Epoch 6550/10000, Prediction Accuracy = 62.604%, Loss = 0.3946277260780334
Epoch: 6550, Batch Gradient Norm: 6.834161522149211
Epoch: 6550, Batch Gradient Norm after: 6.834161522149211
Epoch 6551/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.39028559923171996
Epoch: 6551, Batch Gradient Norm: 9.197738040439967
Epoch: 6551, Batch Gradient Norm after: 9.197738040439967
Epoch 6552/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4024816393852234
Epoch: 6552, Batch Gradient Norm: 10.145875961585107
Epoch: 6552, Batch Gradient Norm after: 10.145875961585107
Epoch 6553/10000, Prediction Accuracy = 62.42%, Loss = 0.4100189685821533
Epoch: 6553, Batch Gradient Norm: 11.789240595020189
Epoch: 6553, Batch Gradient Norm after: 11.789240595020189
Epoch 6554/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.4246289312839508
Epoch: 6554, Batch Gradient Norm: 8.950950501346592
Epoch: 6554, Batch Gradient Norm after: 8.950950501346592
Epoch 6555/10000, Prediction Accuracy = 62.50999999999999%, Loss = 0.4026838481426239
Epoch: 6555, Batch Gradient Norm: 10.172670747918346
Epoch: 6555, Batch Gradient Norm after: 10.172670747918346
Epoch 6556/10000, Prediction Accuracy = 62.544000000000004%, Loss = 0.4114235579967499
Epoch: 6556, Batch Gradient Norm: 8.347330131477506
Epoch: 6556, Batch Gradient Norm after: 8.347330131477506
Epoch 6557/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.3998625636100769
Epoch: 6557, Batch Gradient Norm: 9.955308111893729
Epoch: 6557, Batch Gradient Norm after: 9.955308111893729
Epoch 6558/10000, Prediction Accuracy = 62.53799999999999%, Loss = 0.4094647765159607
Epoch: 6558, Batch Gradient Norm: 8.99064860505323
Epoch: 6558, Batch Gradient Norm after: 8.99064860505323
Epoch 6559/10000, Prediction Accuracy = 62.206%, Loss = 0.40612333416938784
Epoch: 6559, Batch Gradient Norm: 10.538118074649903
Epoch: 6559, Batch Gradient Norm after: 10.538118074649903
Epoch 6560/10000, Prediction Accuracy = 62.35799999999999%, Loss = 0.4171279311180115
Epoch: 6560, Batch Gradient Norm: 9.763559936871312
Epoch: 6560, Batch Gradient Norm after: 9.763559936871312
Epoch 6561/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.40670626759529116
Epoch: 6561, Batch Gradient Norm: 11.70917114376843
Epoch: 6561, Batch Gradient Norm after: 11.70917114376843
Epoch 6562/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.4213300883769989
Epoch: 6562, Batch Gradient Norm: 9.335102652314818
Epoch: 6562, Batch Gradient Norm after: 9.335102652314818
Epoch 6563/10000, Prediction Accuracy = 62.452%, Loss = 0.40481182336807253
Epoch: 6563, Batch Gradient Norm: 9.93048330648912
Epoch: 6563, Batch Gradient Norm after: 9.93048330648912
Epoch 6564/10000, Prediction Accuracy = 62.396%, Loss = 0.4126535475254059
Epoch: 6564, Batch Gradient Norm: 8.485984935461072
Epoch: 6564, Batch Gradient Norm after: 8.485984935461072
Epoch 6565/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.3981378674507141
Epoch: 6565, Batch Gradient Norm: 9.684543218771715
Epoch: 6565, Batch Gradient Norm after: 9.684543218771715
Epoch 6566/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.40604178309440614
Epoch: 6566, Batch Gradient Norm: 12.106558963250803
Epoch: 6566, Batch Gradient Norm after: 12.106558963250803
Epoch 6567/10000, Prediction Accuracy = 62.33599999999999%, Loss = 0.42807250618934634
Epoch: 6567, Batch Gradient Norm: 10.04072340929892
Epoch: 6567, Batch Gradient Norm after: 10.04072340929892
Epoch 6568/10000, Prediction Accuracy = 62.222%, Loss = 0.4129175662994385
Epoch: 6568, Batch Gradient Norm: 6.466999789906939
Epoch: 6568, Batch Gradient Norm after: 6.466999789906939
Epoch 6569/10000, Prediction Accuracy = 62.66600000000001%, Loss = 0.389782178401947
Epoch: 6569, Batch Gradient Norm: 7.936042252804931
Epoch: 6569, Batch Gradient Norm after: 7.936042252804931
Epoch 6570/10000, Prediction Accuracy = 62.364%, Loss = 0.39598810076713564
Epoch: 6570, Batch Gradient Norm: 10.1166517484124
Epoch: 6570, Batch Gradient Norm after: 10.1166517484124
Epoch 6571/10000, Prediction Accuracy = 62.354%, Loss = 0.4116948664188385
Epoch: 6571, Batch Gradient Norm: 9.005049041622538
Epoch: 6571, Batch Gradient Norm after: 9.005049041622538
Epoch 6572/10000, Prediction Accuracy = 62.254%, Loss = 0.4028391659259796
Epoch: 6572, Batch Gradient Norm: 13.555788902585947
Epoch: 6572, Batch Gradient Norm after: 13.555788902585947
Epoch 6573/10000, Prediction Accuracy = 62.412%, Loss = 0.4439310312271118
Epoch: 6573, Batch Gradient Norm: 12.901509652183737
Epoch: 6573, Batch Gradient Norm after: 12.901509652183737
Epoch 6574/10000, Prediction Accuracy = 62.616%, Loss = 0.43164499998092654
Epoch: 6574, Batch Gradient Norm: 7.16573312629806
Epoch: 6574, Batch Gradient Norm after: 7.16573312629806
Epoch 6575/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.39334715008735655
Epoch: 6575, Batch Gradient Norm: 8.629419549719803
Epoch: 6575, Batch Gradient Norm after: 8.629419549719803
Epoch 6576/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.4010717749595642
Epoch: 6576, Batch Gradient Norm: 10.455522998629565
Epoch: 6576, Batch Gradient Norm after: 10.455522998629565
Epoch 6577/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4137649893760681
Epoch: 6577, Batch Gradient Norm: 9.40090331010672
Epoch: 6577, Batch Gradient Norm after: 9.40090331010672
Epoch 6578/10000, Prediction Accuracy = 62.602%, Loss = 0.40611745715141295
Epoch: 6578, Batch Gradient Norm: 8.915995771761681
Epoch: 6578, Batch Gradient Norm after: 8.915995771761681
Epoch 6579/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.4019130766391754
Epoch: 6579, Batch Gradient Norm: 10.091980209857315
Epoch: 6579, Batch Gradient Norm after: 10.091980209857315
Epoch 6580/10000, Prediction Accuracy = 62.386%, Loss = 0.4119192063808441
Epoch: 6580, Batch Gradient Norm: 7.463772517593825
Epoch: 6580, Batch Gradient Norm after: 7.463772517593825
Epoch 6581/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.39240381121635437
Epoch: 6581, Batch Gradient Norm: 10.02095347109886
Epoch: 6581, Batch Gradient Norm after: 10.02095347109886
Epoch 6582/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.40738390684127807
Epoch: 6582, Batch Gradient Norm: 8.674291231255646
Epoch: 6582, Batch Gradient Norm after: 8.674291231255646
Epoch 6583/10000, Prediction Accuracy = 62.664%, Loss = 0.3983880579471588
Epoch: 6583, Batch Gradient Norm: 9.053821224202816
Epoch: 6583, Batch Gradient Norm after: 9.053821224202816
Epoch 6584/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4029514729976654
Epoch: 6584, Batch Gradient Norm: 7.243211846270725
Epoch: 6584, Batch Gradient Norm after: 7.243211846270725
Epoch 6585/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.3915022909641266
Epoch: 6585, Batch Gradient Norm: 9.086616268144546
Epoch: 6585, Batch Gradient Norm after: 9.086616268144546
Epoch 6586/10000, Prediction Accuracy = 62.35799999999999%, Loss = 0.4027736663818359
Epoch: 6586, Batch Gradient Norm: 10.910033811315113
Epoch: 6586, Batch Gradient Norm after: 10.910033811315113
Epoch 6587/10000, Prediction Accuracy = 62.366%, Loss = 0.4141476809978485
Epoch: 6587, Batch Gradient Norm: 13.310861767566399
Epoch: 6587, Batch Gradient Norm after: 13.310861767566399
Epoch 6588/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4369246900081635
Epoch: 6588, Batch Gradient Norm: 10.177811411603049
Epoch: 6588, Batch Gradient Norm after: 10.177811411603049
Epoch 6589/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.4102688550949097
Epoch: 6589, Batch Gradient Norm: 10.19216073497983
Epoch: 6589, Batch Gradient Norm after: 10.19216073497983
Epoch 6590/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.4099375307559967
Epoch: 6590, Batch Gradient Norm: 6.875365644096233
Epoch: 6590, Batch Gradient Norm after: 6.875365644096233
Epoch 6591/10000, Prediction Accuracy = 62.516%, Loss = 0.3912308394908905
Epoch: 6591, Batch Gradient Norm: 7.228684189390497
Epoch: 6591, Batch Gradient Norm after: 7.228684189390497
Epoch 6592/10000, Prediction Accuracy = 62.426%, Loss = 0.39402603507041933
Epoch: 6592, Batch Gradient Norm: 10.076438895365488
Epoch: 6592, Batch Gradient Norm after: 10.076438895365488
Epoch 6593/10000, Prediction Accuracy = 62.48%, Loss = 0.4091759741306305
Epoch: 6593, Batch Gradient Norm: 8.729097316636317
Epoch: 6593, Batch Gradient Norm after: 8.729097316636317
Epoch 6594/10000, Prediction Accuracy = 62.436%, Loss = 0.40211800336837766
Epoch: 6594, Batch Gradient Norm: 9.731905019243607
Epoch: 6594, Batch Gradient Norm after: 9.731905019243607
Epoch 6595/10000, Prediction Accuracy = 62.43399999999999%, Loss = 0.4071195662021637
Epoch: 6595, Batch Gradient Norm: 11.13315055382418
Epoch: 6595, Batch Gradient Norm after: 11.13315055382418
Epoch 6596/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.418880432844162
Epoch: 6596, Batch Gradient Norm: 8.952866771588392
Epoch: 6596, Batch Gradient Norm after: 8.952866771588392
Epoch 6597/10000, Prediction Accuracy = 62.452%, Loss = 0.4033293604850769
Epoch: 6597, Batch Gradient Norm: 10.53535026686677
Epoch: 6597, Batch Gradient Norm after: 10.53535026686677
Epoch 6598/10000, Prediction Accuracy = 62.61%, Loss = 0.412839013338089
Epoch: 6598, Batch Gradient Norm: 10.807451429427434
Epoch: 6598, Batch Gradient Norm after: 10.807451429427434
Epoch 6599/10000, Prediction Accuracy = 62.432%, Loss = 0.41395599246025083
Epoch: 6599, Batch Gradient Norm: 8.313852017937508
Epoch: 6599, Batch Gradient Norm after: 8.313852017937508
Epoch 6600/10000, Prediction Accuracy = 62.586%, Loss = 0.39859938621520996
Epoch: 6600, Batch Gradient Norm: 9.272673343350093
Epoch: 6600, Batch Gradient Norm after: 9.272673343350093
Epoch 6601/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.4022713124752045
Epoch: 6601, Batch Gradient Norm: 10.385108149461216
Epoch: 6601, Batch Gradient Norm after: 10.385108149461216
Epoch 6602/10000, Prediction Accuracy = 62.374%, Loss = 0.41057658195495605
Epoch: 6602, Batch Gradient Norm: 8.56085375529553
Epoch: 6602, Batch Gradient Norm after: 8.56085375529553
Epoch 6603/10000, Prediction Accuracy = 62.424%, Loss = 0.3998108983039856
Epoch: 6603, Batch Gradient Norm: 11.095495879111978
Epoch: 6603, Batch Gradient Norm after: 11.095495879111978
Epoch 6604/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.41560407280921935
Epoch: 6604, Batch Gradient Norm: 9.71888198053381
Epoch: 6604, Batch Gradient Norm after: 9.71888198053381
Epoch 6605/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.4071094274520874
Epoch: 6605, Batch Gradient Norm: 9.036529491191365
Epoch: 6605, Batch Gradient Norm after: 9.036529491191365
Epoch 6606/10000, Prediction Accuracy = 62.528%, Loss = 0.4002593278884888
Epoch: 6606, Batch Gradient Norm: 8.367527245543796
Epoch: 6606, Batch Gradient Norm after: 8.367527245543796
Epoch 6607/10000, Prediction Accuracy = 62.51800000000001%, Loss = 0.3963826775550842
Epoch: 6607, Batch Gradient Norm: 10.333597639970641
Epoch: 6607, Batch Gradient Norm after: 10.333597639970641
Epoch 6608/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.41049993634223936
Epoch: 6608, Batch Gradient Norm: 12.304098841809944
Epoch: 6608, Batch Gradient Norm after: 12.304098841809944
Epoch 6609/10000, Prediction Accuracy = 62.364%, Loss = 0.4263737380504608
Epoch: 6609, Batch Gradient Norm: 8.779611866855294
Epoch: 6609, Batch Gradient Norm after: 8.779611866855294
Epoch 6610/10000, Prediction Accuracy = 62.574%, Loss = 0.3987645208835602
Epoch: 6610, Batch Gradient Norm: 11.682086148566345
Epoch: 6610, Batch Gradient Norm after: 11.682086148566345
Epoch 6611/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.4206430494785309
Epoch: 6611, Batch Gradient Norm: 12.264858825396002
Epoch: 6611, Batch Gradient Norm after: 12.264858825396002
Epoch 6612/10000, Prediction Accuracy = 62.5%, Loss = 0.4305461585521698
Epoch: 6612, Batch Gradient Norm: 8.802508412987697
Epoch: 6612, Batch Gradient Norm after: 8.802508412987697
Epoch 6613/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.4009637951850891
Epoch: 6613, Batch Gradient Norm: 8.491521130948685
Epoch: 6613, Batch Gradient Norm after: 8.491521130948685
Epoch 6614/10000, Prediction Accuracy = 62.45%, Loss = 0.39763686060905457
Epoch: 6614, Batch Gradient Norm: 10.0605142454257
Epoch: 6614, Batch Gradient Norm after: 10.0605142454257
Epoch 6615/10000, Prediction Accuracy = 62.274%, Loss = 0.4096534252166748
Epoch: 6615, Batch Gradient Norm: 10.754096192689053
Epoch: 6615, Batch Gradient Norm after: 10.754096192689053
Epoch 6616/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.4143714189529419
Epoch: 6616, Batch Gradient Norm: 10.220760631991775
Epoch: 6616, Batch Gradient Norm after: 10.220760631991775
Epoch 6617/10000, Prediction Accuracy = 62.45%, Loss = 0.4124453604221344
Epoch: 6617, Batch Gradient Norm: 7.362982056757866
Epoch: 6617, Batch Gradient Norm after: 7.362982056757866
Epoch 6618/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.39111940264701844
Epoch: 6618, Batch Gradient Norm: 8.06493213857419
Epoch: 6618, Batch Gradient Norm after: 8.06493213857419
Epoch 6619/10000, Prediction Accuracy = 62.53399999999999%, Loss = 0.3960793137550354
Epoch: 6619, Batch Gradient Norm: 7.8923996049840435
Epoch: 6619, Batch Gradient Norm after: 7.8923996049840435
Epoch 6620/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.39458686113357544
Epoch: 6620, Batch Gradient Norm: 10.974213097986363
Epoch: 6620, Batch Gradient Norm after: 10.974213097986363
Epoch 6621/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.4191329002380371
Epoch: 6621, Batch Gradient Norm: 10.19255033319878
Epoch: 6621, Batch Gradient Norm after: 10.19255033319878
Epoch 6622/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.41029482483863833
Epoch: 6622, Batch Gradient Norm: 9.797695738928493
Epoch: 6622, Batch Gradient Norm after: 9.797695738928493
Epoch 6623/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.40830026268959047
Epoch: 6623, Batch Gradient Norm: 10.596166955920653
Epoch: 6623, Batch Gradient Norm after: 10.596166955920653
Epoch 6624/10000, Prediction Accuracy = 62.508%, Loss = 0.41338697671890257
Epoch: 6624, Batch Gradient Norm: 9.23319643680134
Epoch: 6624, Batch Gradient Norm after: 9.23319643680134
Epoch 6625/10000, Prediction Accuracy = 62.602%, Loss = 0.4047651767730713
Epoch: 6625, Batch Gradient Norm: 8.746909556253861
Epoch: 6625, Batch Gradient Norm after: 8.746909556253861
Epoch 6626/10000, Prediction Accuracy = 62.702%, Loss = 0.39901341795921325
Epoch: 6626, Batch Gradient Norm: 9.63048095105247
Epoch: 6626, Batch Gradient Norm after: 9.63048095105247
Epoch 6627/10000, Prediction Accuracy = 62.676%, Loss = 0.40285336375236513
Epoch: 6627, Batch Gradient Norm: 12.287922179598766
Epoch: 6627, Batch Gradient Norm after: 12.287922179598766
Epoch 6628/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.4281268775463104
Epoch: 6628, Batch Gradient Norm: 11.01009157372847
Epoch: 6628, Batch Gradient Norm after: 11.01009157372847
Epoch 6629/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.4188982665538788
Epoch: 6629, Batch Gradient Norm: 6.930865447960562
Epoch: 6629, Batch Gradient Norm after: 6.930865447960562
Epoch 6630/10000, Prediction Accuracy = 62.5%, Loss = 0.38810967803001406
Epoch: 6630, Batch Gradient Norm: 8.244230698636446
Epoch: 6630, Batch Gradient Norm after: 8.244230698636446
Epoch 6631/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.3958007335662842
Epoch: 6631, Batch Gradient Norm: 11.596650856747228
Epoch: 6631, Batch Gradient Norm after: 11.596650856747228
Epoch 6632/10000, Prediction Accuracy = 62.39%, Loss = 0.42460142970085146
Epoch: 6632, Batch Gradient Norm: 7.844128127683787
Epoch: 6632, Batch Gradient Norm after: 7.844128127683787
Epoch 6633/10000, Prediction Accuracy = 62.64%, Loss = 0.3966736733913422
Epoch: 6633, Batch Gradient Norm: 11.524864133699852
Epoch: 6633, Batch Gradient Norm after: 11.524864133699852
Epoch 6634/10000, Prediction Accuracy = 62.459999999999994%, Loss = 0.4204038679599762
Epoch: 6634, Batch Gradient Norm: 9.525137549068905
Epoch: 6634, Batch Gradient Norm after: 9.525137549068905
Epoch 6635/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.4074632465839386
Epoch: 6635, Batch Gradient Norm: 9.645042452726784
Epoch: 6635, Batch Gradient Norm after: 9.645042452726784
Epoch 6636/10000, Prediction Accuracy = 62.424%, Loss = 0.40827290415763856
Epoch: 6636, Batch Gradient Norm: 9.976349771078258
Epoch: 6636, Batch Gradient Norm after: 9.976349771078258
Epoch 6637/10000, Prediction Accuracy = 62.436%, Loss = 0.4116623342037201
Epoch: 6637, Batch Gradient Norm: 9.976038298440132
Epoch: 6637, Batch Gradient Norm after: 9.976038298440132
Epoch 6638/10000, Prediction Accuracy = 62.44%, Loss = 0.4073091745376587
Epoch: 6638, Batch Gradient Norm: 8.757868756699391
Epoch: 6638, Batch Gradient Norm after: 8.757868756699391
Epoch 6639/10000, Prediction Accuracy = 62.472%, Loss = 0.40117996335029604
Epoch: 6639, Batch Gradient Norm: 9.649003016254197
Epoch: 6639, Batch Gradient Norm after: 9.649003016254197
Epoch 6640/10000, Prediction Accuracy = 62.45400000000001%, Loss = 0.4065175592899323
Epoch: 6640, Batch Gradient Norm: 13.262139320013452
Epoch: 6640, Batch Gradient Norm after: 13.262139320013452
Epoch 6641/10000, Prediction Accuracy = 62.492%, Loss = 0.4360246419906616
Epoch: 6641, Batch Gradient Norm: 10.318301702974534
Epoch: 6641, Batch Gradient Norm after: 10.318301702974534
Epoch 6642/10000, Prediction Accuracy = 62.528%, Loss = 0.41324286460876464
Epoch: 6642, Batch Gradient Norm: 7.073272378248454
Epoch: 6642, Batch Gradient Norm after: 7.073272378248454
Epoch 6643/10000, Prediction Accuracy = 62.56600000000001%, Loss = 0.39106919765472414
Epoch: 6643, Batch Gradient Norm: 9.560425539655936
Epoch: 6643, Batch Gradient Norm after: 9.560425539655936
Epoch 6644/10000, Prediction Accuracy = 62.35999999999999%, Loss = 0.4045227706432343
Epoch: 6644, Batch Gradient Norm: 10.1400818145688
Epoch: 6644, Batch Gradient Norm after: 10.1400818145688
Epoch 6645/10000, Prediction Accuracy = 62.476%, Loss = 0.40845551490783694
Epoch: 6645, Batch Gradient Norm: 8.442849478209167
Epoch: 6645, Batch Gradient Norm after: 8.442849478209167
Epoch 6646/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.4008439242839813
Epoch: 6646, Batch Gradient Norm: 8.451985501225831
Epoch: 6646, Batch Gradient Norm after: 8.451985501225831
Epoch 6647/10000, Prediction Accuracy = 62.504%, Loss = 0.3974580466747284
Epoch: 6647, Batch Gradient Norm: 10.85504179263915
Epoch: 6647, Batch Gradient Norm after: 10.85504179263915
Epoch 6648/10000, Prediction Accuracy = 62.46200000000001%, Loss = 0.41270582675933837
Epoch: 6648, Batch Gradient Norm: 12.52494776802688
Epoch: 6648, Batch Gradient Norm after: 12.52494776802688
Epoch 6649/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.42954447865486145
Epoch: 6649, Batch Gradient Norm: 10.138032341163179
Epoch: 6649, Batch Gradient Norm after: 10.138032341163179
Epoch 6650/10000, Prediction Accuracy = 62.602%, Loss = 0.4099676191806793
Epoch: 6650, Batch Gradient Norm: 7.725414618560281
Epoch: 6650, Batch Gradient Norm after: 7.725414618560281
Epoch 6651/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.3938263118267059
Epoch: 6651, Batch Gradient Norm: 9.678737268488488
Epoch: 6651, Batch Gradient Norm after: 9.678737268488488
Epoch 6652/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.40687076449394227
Epoch: 6652, Batch Gradient Norm: 7.08657376196556
Epoch: 6652, Batch Gradient Norm after: 7.08657376196556
Epoch 6653/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.38907621502876283
Epoch: 6653, Batch Gradient Norm: 11.17504192705232
Epoch: 6653, Batch Gradient Norm after: 11.17504192705232
Epoch 6654/10000, Prediction Accuracy = 62.484%, Loss = 0.4218947052955627
Epoch: 6654, Batch Gradient Norm: 10.556128085856795
Epoch: 6654, Batch Gradient Norm after: 10.556128085856795
Epoch 6655/10000, Prediction Accuracy = 62.496%, Loss = 0.4132828593254089
Epoch: 6655, Batch Gradient Norm: 8.603448437187739
Epoch: 6655, Batch Gradient Norm after: 8.603448437187739
Epoch 6656/10000, Prediction Accuracy = 62.374%, Loss = 0.3969200074672699
Epoch: 6656, Batch Gradient Norm: 11.218674946724626
Epoch: 6656, Batch Gradient Norm after: 11.218674946724626
Epoch 6657/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.4171138823032379
Epoch: 6657, Batch Gradient Norm: 12.720255868343532
Epoch: 6657, Batch Gradient Norm after: 12.720255868343532
Epoch 6658/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.4308383882045746
Epoch: 6658, Batch Gradient Norm: 8.575202385887032
Epoch: 6658, Batch Gradient Norm after: 8.575202385887032
Epoch 6659/10000, Prediction Accuracy = 62.552%, Loss = 0.3999305129051208
Epoch: 6659, Batch Gradient Norm: 7.88533556307644
Epoch: 6659, Batch Gradient Norm after: 7.88533556307644
Epoch 6660/10000, Prediction Accuracy = 62.462%, Loss = 0.3951288402080536
Epoch: 6660, Batch Gradient Norm: 6.585548330382779
Epoch: 6660, Batch Gradient Norm after: 6.585548330382779
Epoch 6661/10000, Prediction Accuracy = 62.576%, Loss = 0.38832720518112185
Epoch: 6661, Batch Gradient Norm: 8.508673299888537
Epoch: 6661, Batch Gradient Norm after: 8.508673299888537
Epoch 6662/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.39626373052597047
Epoch: 6662, Batch Gradient Norm: 9.421088974579138
Epoch: 6662, Batch Gradient Norm after: 9.421088974579138
Epoch 6663/10000, Prediction Accuracy = 62.424%, Loss = 0.403053879737854
Epoch: 6663, Batch Gradient Norm: 8.253904646578645
Epoch: 6663, Batch Gradient Norm after: 8.253904646578645
Epoch 6664/10000, Prediction Accuracy = 62.432%, Loss = 0.3957997798919678
Epoch: 6664, Batch Gradient Norm: 9.810912800002507
Epoch: 6664, Batch Gradient Norm after: 9.810912800002507
Epoch 6665/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.4032414615154266
Epoch: 6665, Batch Gradient Norm: 12.28297858201085
Epoch: 6665, Batch Gradient Norm after: 12.28297858201085
Epoch 6666/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.42149139046669004
Epoch: 6666, Batch Gradient Norm: 10.735751988610811
Epoch: 6666, Batch Gradient Norm after: 10.735751988610811
Epoch 6667/10000, Prediction Accuracy = 62.538%, Loss = 0.41233102083206175
Epoch: 6667, Batch Gradient Norm: 11.38205864793286
Epoch: 6667, Batch Gradient Norm after: 11.38205864793286
Epoch 6668/10000, Prediction Accuracy = 62.402%, Loss = 0.41494664549827576
Epoch: 6668, Batch Gradient Norm: 12.527098126726571
Epoch: 6668, Batch Gradient Norm after: 12.527098126726571
Epoch 6669/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.4285828351974487
Epoch: 6669, Batch Gradient Norm: 8.470573734451378
Epoch: 6669, Batch Gradient Norm after: 8.470573734451378
Epoch 6670/10000, Prediction Accuracy = 62.52%, Loss = 0.39972934126853943
Epoch: 6670, Batch Gradient Norm: 6.51846651556315
Epoch: 6670, Batch Gradient Norm after: 6.51846651556315
Epoch 6671/10000, Prediction Accuracy = 62.622%, Loss = 0.3851809561252594
Epoch: 6671, Batch Gradient Norm: 8.462288554396286
Epoch: 6671, Batch Gradient Norm after: 8.462288554396286
Epoch 6672/10000, Prediction Accuracy = 62.408%, Loss = 0.3980616867542267
Epoch: 6672, Batch Gradient Norm: 8.817715930779558
Epoch: 6672, Batch Gradient Norm after: 8.817715930779558
Epoch 6673/10000, Prediction Accuracy = 62.532%, Loss = 0.3994687259197235
Epoch: 6673, Batch Gradient Norm: 7.715858987268454
Epoch: 6673, Batch Gradient Norm after: 7.715858987268454
Epoch 6674/10000, Prediction Accuracy = 62.488%, Loss = 0.3925714910030365
Epoch: 6674, Batch Gradient Norm: 9.759594697442216
Epoch: 6674, Batch Gradient Norm after: 9.759594697442216
Epoch 6675/10000, Prediction Accuracy = 62.476%, Loss = 0.40329253673553467
Epoch: 6675, Batch Gradient Norm: 9.858630363314113
Epoch: 6675, Batch Gradient Norm after: 9.858630363314113
Epoch 6676/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.4068784475326538
Epoch: 6676, Batch Gradient Norm: 10.147992768587883
Epoch: 6676, Batch Gradient Norm after: 10.147992768587883
Epoch 6677/10000, Prediction Accuracy = 62.516%, Loss = 0.4127800464630127
Epoch: 6677, Batch Gradient Norm: 8.872816690533169
Epoch: 6677, Batch Gradient Norm after: 8.872816690533169
Epoch 6678/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4011799693107605
Epoch: 6678, Batch Gradient Norm: 12.151157712146173
Epoch: 6678, Batch Gradient Norm after: 12.151157712146173
Epoch 6679/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4228349030017853
Epoch: 6679, Batch Gradient Norm: 10.338336790082398
Epoch: 6679, Batch Gradient Norm after: 10.338336790082398
Epoch 6680/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.4119868576526642
Epoch: 6680, Batch Gradient Norm: 6.795236878811296
Epoch: 6680, Batch Gradient Norm after: 6.795236878811296
Epoch 6681/10000, Prediction Accuracy = 62.693999999999996%, Loss = 0.3874345898628235
Epoch: 6681, Batch Gradient Norm: 7.762189949216938
Epoch: 6681, Batch Gradient Norm after: 7.762189949216938
Epoch 6682/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.3924012303352356
Epoch: 6682, Batch Gradient Norm: 9.358422936830655
Epoch: 6682, Batch Gradient Norm after: 9.358422936830655
Epoch 6683/10000, Prediction Accuracy = 62.468%, Loss = 0.4053790867328644
Epoch: 6683, Batch Gradient Norm: 11.265403745963718
Epoch: 6683, Batch Gradient Norm after: 11.265403745963718
Epoch 6684/10000, Prediction Accuracy = 62.538%, Loss = 0.41800156235694885
Epoch: 6684, Batch Gradient Norm: 12.962769989117934
Epoch: 6684, Batch Gradient Norm after: 12.962769989117934
Epoch 6685/10000, Prediction Accuracy = 62.524%, Loss = 0.4299630641937256
Epoch: 6685, Batch Gradient Norm: 10.886537067603948
Epoch: 6685, Batch Gradient Norm after: 10.886537067603948
Epoch 6686/10000, Prediction Accuracy = 62.62600000000001%, Loss = 0.41172372102737426
Epoch: 6686, Batch Gradient Norm: 8.254854303055756
Epoch: 6686, Batch Gradient Norm after: 8.254854303055756
Epoch 6687/10000, Prediction Accuracy = 62.538%, Loss = 0.39372090101242063
Epoch: 6687, Batch Gradient Norm: 10.700064108480188
Epoch: 6687, Batch Gradient Norm after: 10.700064108480188
Epoch 6688/10000, Prediction Accuracy = 62.36999999999999%, Loss = 0.4122903525829315
Epoch: 6688, Batch Gradient Norm: 10.551242610902968
Epoch: 6688, Batch Gradient Norm after: 10.551242610902968
Epoch 6689/10000, Prediction Accuracy = 62.48599999999999%, Loss = 0.41183097958564757
Epoch: 6689, Batch Gradient Norm: 10.407984628574086
Epoch: 6689, Batch Gradient Norm after: 10.407984628574086
Epoch 6690/10000, Prediction Accuracy = 62.508%, Loss = 0.4170193374156952
Epoch: 6690, Batch Gradient Norm: 8.001652194097739
Epoch: 6690, Batch Gradient Norm after: 8.001652194097739
Epoch 6691/10000, Prediction Accuracy = 62.61%, Loss = 0.3961780548095703
Epoch: 6691, Batch Gradient Norm: 6.828630046328269
Epoch: 6691, Batch Gradient Norm after: 6.828630046328269
Epoch 6692/10000, Prediction Accuracy = 62.372%, Loss = 0.38840084075927733
Epoch: 6692, Batch Gradient Norm: 7.664447305772522
Epoch: 6692, Batch Gradient Norm after: 7.664447305772522
Epoch 6693/10000, Prediction Accuracy = 62.58800000000001%, Loss = 0.3924851953983307
Epoch: 6693, Batch Gradient Norm: 9.39110214526725
Epoch: 6693, Batch Gradient Norm after: 9.39110214526725
Epoch 6694/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4018872618675232
Epoch: 6694, Batch Gradient Norm: 10.261894252473274
Epoch: 6694, Batch Gradient Norm after: 10.261894252473274
Epoch 6695/10000, Prediction Accuracy = 62.6%, Loss = 0.4066961705684662
Epoch: 6695, Batch Gradient Norm: 9.090789765539238
Epoch: 6695, Batch Gradient Norm after: 9.090789765539238
Epoch 6696/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.4010596752166748
Epoch: 6696, Batch Gradient Norm: 8.822924761633294
Epoch: 6696, Batch Gradient Norm after: 8.822924761633294
Epoch 6697/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.4000202059745789
Epoch: 6697, Batch Gradient Norm: 10.308130810816035
Epoch: 6697, Batch Gradient Norm after: 10.308130810816035
Epoch 6698/10000, Prediction Accuracy = 62.568%, Loss = 0.407315194606781
Epoch: 6698, Batch Gradient Norm: 11.378988814184918
Epoch: 6698, Batch Gradient Norm after: 11.378988814184918
Epoch 6699/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4179512679576874
Epoch: 6699, Batch Gradient Norm: 9.397820035944124
Epoch: 6699, Batch Gradient Norm after: 9.397820035944124
Epoch 6700/10000, Prediction Accuracy = 62.602%, Loss = 0.40177382826805114
Epoch: 6700, Batch Gradient Norm: 8.984950878441362
Epoch: 6700, Batch Gradient Norm after: 8.984950878441362
Epoch 6701/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.4022041320800781
Epoch: 6701, Batch Gradient Norm: 7.828647428260076
Epoch: 6701, Batch Gradient Norm after: 7.828647428260076
Epoch 6702/10000, Prediction Accuracy = 62.532%, Loss = 0.39263962507247924
Epoch: 6702, Batch Gradient Norm: 9.564591788118735
Epoch: 6702, Batch Gradient Norm after: 9.564591788118735
Epoch 6703/10000, Prediction Accuracy = 62.45%, Loss = 0.40420891642570494
Epoch: 6703, Batch Gradient Norm: 10.034876687772872
Epoch: 6703, Batch Gradient Norm after: 10.034876687772872
Epoch 6704/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.4091412782669067
Epoch: 6704, Batch Gradient Norm: 10.478574882739476
Epoch: 6704, Batch Gradient Norm after: 10.478574882739476
Epoch 6705/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.4124058485031128
Epoch: 6705, Batch Gradient Norm: 10.318982781352505
Epoch: 6705, Batch Gradient Norm after: 10.318982781352505
Epoch 6706/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.4108407378196716
Epoch: 6706, Batch Gradient Norm: 9.434133272355266
Epoch: 6706, Batch Gradient Norm after: 9.434133272355266
Epoch 6707/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.40637622475624086
Epoch: 6707, Batch Gradient Norm: 9.496562854748726
Epoch: 6707, Batch Gradient Norm after: 9.496562854748726
Epoch 6708/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.40406949520111085
Epoch: 6708, Batch Gradient Norm: 10.899043325955363
Epoch: 6708, Batch Gradient Norm after: 10.899043325955363
Epoch 6709/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.41450141072273256
Epoch: 6709, Batch Gradient Norm: 11.055302519729794
Epoch: 6709, Batch Gradient Norm after: 11.055302519729794
Epoch 6710/10000, Prediction Accuracy = 62.48%, Loss = 0.4144002377986908
Epoch: 6710, Batch Gradient Norm: 11.842087751627934
Epoch: 6710, Batch Gradient Norm after: 11.842087751627934
Epoch 6711/10000, Prediction Accuracy = 62.492%, Loss = 0.4221134722232819
Epoch: 6711, Batch Gradient Norm: 10.791535148613404
Epoch: 6711, Batch Gradient Norm after: 10.791535148613404
Epoch 6712/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.4119515895843506
Epoch: 6712, Batch Gradient Norm: 10.416307746003728
Epoch: 6712, Batch Gradient Norm after: 10.416307746003728
Epoch 6713/10000, Prediction Accuracy = 62.7%, Loss = 0.409336656332016
Epoch: 6713, Batch Gradient Norm: 9.21155475957798
Epoch: 6713, Batch Gradient Norm after: 9.21155475957798
Epoch 6714/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.3990957081317902
Epoch: 6714, Batch Gradient Norm: 6.4455052118501435
Epoch: 6714, Batch Gradient Norm after: 6.4455052118501435
Epoch 6715/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.3847675085067749
Epoch: 6715, Batch Gradient Norm: 7.981383094284014
Epoch: 6715, Batch Gradient Norm after: 7.981383094284014
Epoch 6716/10000, Prediction Accuracy = 62.586%, Loss = 0.39087842106819154
Epoch: 6716, Batch Gradient Norm: 7.705759451226883
Epoch: 6716, Batch Gradient Norm after: 7.705759451226883
Epoch 6717/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.39097049832344055
Epoch: 6717, Batch Gradient Norm: 11.85034093351668
Epoch: 6717, Batch Gradient Norm after: 11.85034093351668
Epoch 6718/10000, Prediction Accuracy = 62.376%, Loss = 0.4194400370121002
Epoch: 6718, Batch Gradient Norm: 11.521102286818127
Epoch: 6718, Batch Gradient Norm after: 11.521102286818127
Epoch 6719/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.42004311084747314
Epoch: 6719, Batch Gradient Norm: 8.772134587994785
Epoch: 6719, Batch Gradient Norm after: 8.772134587994785
Epoch 6720/10000, Prediction Accuracy = 62.6%, Loss = 0.39764460921287537
Epoch: 6720, Batch Gradient Norm: 9.702088869026946
Epoch: 6720, Batch Gradient Norm after: 9.702088869026946
Epoch 6721/10000, Prediction Accuracy = 62.574%, Loss = 0.4047822058200836
Epoch: 6721, Batch Gradient Norm: 6.486667474183332
Epoch: 6721, Batch Gradient Norm after: 6.486667474183332
Epoch 6722/10000, Prediction Accuracy = 62.436%, Loss = 0.3852028429508209
Epoch: 6722, Batch Gradient Norm: 10.053193581534748
Epoch: 6722, Batch Gradient Norm after: 10.053193581534748
Epoch 6723/10000, Prediction Accuracy = 62.412%, Loss = 0.40979822278022765
Epoch: 6723, Batch Gradient Norm: 10.611598142735803
Epoch: 6723, Batch Gradient Norm after: 10.611598142735803
Epoch 6724/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.4105873882770538
Epoch: 6724, Batch Gradient Norm: 8.984040835546338
Epoch: 6724, Batch Gradient Norm after: 8.984040835546338
Epoch 6725/10000, Prediction Accuracy = 62.548%, Loss = 0.4001130998134613
Epoch: 6725, Batch Gradient Norm: 8.707144136679018
Epoch: 6725, Batch Gradient Norm after: 8.707144136679018
Epoch 6726/10000, Prediction Accuracy = 62.584%, Loss = 0.39783544540405275
Epoch: 6726, Batch Gradient Norm: 9.884121517392435
Epoch: 6726, Batch Gradient Norm after: 9.884121517392435
Epoch 6727/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.40820902585983276
Epoch: 6727, Batch Gradient Norm: 9.593219706940603
Epoch: 6727, Batch Gradient Norm after: 9.593219706940603
Epoch 6728/10000, Prediction Accuracy = 62.58399999999999%, Loss = 0.40608457922935487
Epoch: 6728, Batch Gradient Norm: 10.03578630736649
Epoch: 6728, Batch Gradient Norm after: 10.03578630736649
Epoch 6729/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.408867746591568
Epoch: 6729, Batch Gradient Norm: 8.53652853165731
Epoch: 6729, Batch Gradient Norm after: 8.53652853165731
Epoch 6730/10000, Prediction Accuracy = 62.528000000000006%, Loss = 0.3968722939491272
Epoch: 6730, Batch Gradient Norm: 9.313414319896935
Epoch: 6730, Batch Gradient Norm after: 9.313414319896935
Epoch 6731/10000, Prediction Accuracy = 62.626%, Loss = 0.40160231590270995
Epoch: 6731, Batch Gradient Norm: 10.370091282498718
Epoch: 6731, Batch Gradient Norm after: 10.370091282498718
Epoch 6732/10000, Prediction Accuracy = 62.576%, Loss = 0.41024153828620913
Epoch: 6732, Batch Gradient Norm: 10.93129470080524
Epoch: 6732, Batch Gradient Norm after: 10.93129470080524
Epoch 6733/10000, Prediction Accuracy = 62.598%, Loss = 0.4115860044956207
Epoch: 6733, Batch Gradient Norm: 9.015351122220617
Epoch: 6733, Batch Gradient Norm after: 9.015351122220617
Epoch 6734/10000, Prediction Accuracy = 62.562%, Loss = 0.3990563929080963
Epoch: 6734, Batch Gradient Norm: 8.787403755854672
Epoch: 6734, Batch Gradient Norm after: 8.787403755854672
Epoch 6735/10000, Prediction Accuracy = 62.492000000000004%, Loss = 0.39888447523117065
Epoch: 6735, Batch Gradient Norm: 8.1710549080275
Epoch: 6735, Batch Gradient Norm after: 8.1710549080275
Epoch 6736/10000, Prediction Accuracy = 62.432%, Loss = 0.3961679220199585
Epoch: 6736, Batch Gradient Norm: 10.056321456235871
Epoch: 6736, Batch Gradient Norm after: 10.056321456235871
Epoch 6737/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4049936830997467
Epoch: 6737, Batch Gradient Norm: 11.826542963823242
Epoch: 6737, Batch Gradient Norm after: 11.826542963823242
Epoch 6738/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4206733167171478
Epoch: 6738, Batch Gradient Norm: 9.127969686839421
Epoch: 6738, Batch Gradient Norm after: 9.127969686839421
Epoch 6739/10000, Prediction Accuracy = 62.496%, Loss = 0.4017065346240997
Epoch: 6739, Batch Gradient Norm: 8.59472815322733
Epoch: 6739, Batch Gradient Norm after: 8.59472815322733
Epoch 6740/10000, Prediction Accuracy = 62.598%, Loss = 0.39531506299972535
Epoch: 6740, Batch Gradient Norm: 10.83119029979157
Epoch: 6740, Batch Gradient Norm after: 10.83119029979157
Epoch 6741/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.410122412443161
Epoch: 6741, Batch Gradient Norm: 11.684408385079442
Epoch: 6741, Batch Gradient Norm after: 11.684408385079442
Epoch 6742/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.41752145290374754
Epoch: 6742, Batch Gradient Norm: 10.450563892826288
Epoch: 6742, Batch Gradient Norm after: 10.450563892826288
Epoch 6743/10000, Prediction Accuracy = 62.588%, Loss = 0.41164559721946714
Epoch: 6743, Batch Gradient Norm: 8.924251797634565
Epoch: 6743, Batch Gradient Norm after: 8.924251797634565
Epoch 6744/10000, Prediction Accuracy = 62.54%, Loss = 0.39886645674705506
Epoch: 6744, Batch Gradient Norm: 8.085076794902847
Epoch: 6744, Batch Gradient Norm after: 8.085076794902847
Epoch 6745/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.39350337386131284
Epoch: 6745, Batch Gradient Norm: 9.268185925464747
Epoch: 6745, Batch Gradient Norm after: 9.268185925464747
Epoch 6746/10000, Prediction Accuracy = 62.348%, Loss = 0.402069753408432
Epoch: 6746, Batch Gradient Norm: 12.53354091871212
Epoch: 6746, Batch Gradient Norm after: 12.53354091871212
Epoch 6747/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.42225825786590576
Epoch: 6747, Batch Gradient Norm: 9.209015440586239
Epoch: 6747, Batch Gradient Norm after: 9.209015440586239
Epoch 6748/10000, Prediction Accuracy = 62.64%, Loss = 0.39912461042404174
Epoch: 6748, Batch Gradient Norm: 9.736443180940366
Epoch: 6748, Batch Gradient Norm after: 9.736443180940366
Epoch 6749/10000, Prediction Accuracy = 62.472%, Loss = 0.4098550796508789
Epoch: 6749, Batch Gradient Norm: 6.826262047475007
Epoch: 6749, Batch Gradient Norm after: 6.826262047475007
Epoch 6750/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.3856182873249054
Epoch: 6750, Batch Gradient Norm: 10.008291189621797
Epoch: 6750, Batch Gradient Norm after: 10.008291189621797
Epoch 6751/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.405472993850708
Epoch: 6751, Batch Gradient Norm: 10.080848489689176
Epoch: 6751, Batch Gradient Norm after: 10.080848489689176
Epoch 6752/10000, Prediction Accuracy = 62.598%, Loss = 0.4072989821434021
Epoch: 6752, Batch Gradient Norm: 7.3295981993732635
Epoch: 6752, Batch Gradient Norm after: 7.3295981993732635
Epoch 6753/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.39059197902679443
Epoch: 6753, Batch Gradient Norm: 9.702720208215572
Epoch: 6753, Batch Gradient Norm after: 9.702720208215572
Epoch 6754/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.4045262515544891
Epoch: 6754, Batch Gradient Norm: 13.549552596010646
Epoch: 6754, Batch Gradient Norm after: 13.549552596010646
Epoch 6755/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4409089207649231
Epoch: 6755, Batch Gradient Norm: 9.24563111473483
Epoch: 6755, Batch Gradient Norm after: 9.24563111473483
Epoch 6756/10000, Prediction Accuracy = 62.39%, Loss = 0.4026805520057678
Epoch: 6756, Batch Gradient Norm: 9.494551131054385
Epoch: 6756, Batch Gradient Norm after: 9.494551131054385
Epoch 6757/10000, Prediction Accuracy = 62.342%, Loss = 0.40228060483932493
Epoch: 6757, Batch Gradient Norm: 11.05359119044259
Epoch: 6757, Batch Gradient Norm after: 11.05359119044259
Epoch 6758/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.4155868649482727
Epoch: 6758, Batch Gradient Norm: 10.882676596896246
Epoch: 6758, Batch Gradient Norm after: 10.882676596896246
Epoch 6759/10000, Prediction Accuracy = 62.588%, Loss = 0.4128866732120514
Epoch: 6759, Batch Gradient Norm: 10.097542267902401
Epoch: 6759, Batch Gradient Norm after: 10.097542267902401
Epoch 6760/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.40622934103012087
Epoch: 6760, Batch Gradient Norm: 7.605559592053127
Epoch: 6760, Batch Gradient Norm after: 7.605559592053127
Epoch 6761/10000, Prediction Accuracy = 62.612%, Loss = 0.3908421635627747
Epoch: 6761, Batch Gradient Norm: 9.269819886955505
Epoch: 6761, Batch Gradient Norm after: 9.269819886955505
Epoch 6762/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3999474823474884
Epoch: 6762, Batch Gradient Norm: 11.961363643420865
Epoch: 6762, Batch Gradient Norm after: 11.961363643420865
Epoch 6763/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.42081100344657896
Epoch: 6763, Batch Gradient Norm: 10.98357503149183
Epoch: 6763, Batch Gradient Norm after: 10.98357503149183
Epoch 6764/10000, Prediction Accuracy = 62.628%, Loss = 0.4112318754196167
Epoch: 6764, Batch Gradient Norm: 8.491933332822107
Epoch: 6764, Batch Gradient Norm after: 8.491933332822107
Epoch 6765/10000, Prediction Accuracy = 62.54%, Loss = 0.3953276574611664
Epoch: 6765, Batch Gradient Norm: 8.625020555614899
Epoch: 6765, Batch Gradient Norm after: 8.625020555614899
Epoch 6766/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.3956554114818573
Epoch: 6766, Batch Gradient Norm: 8.682859660792431
Epoch: 6766, Batch Gradient Norm after: 8.682859660792431
Epoch 6767/10000, Prediction Accuracy = 62.54%, Loss = 0.39652379155158995
Epoch: 6767, Batch Gradient Norm: 7.137310094712294
Epoch: 6767, Batch Gradient Norm after: 7.137310094712294
Epoch 6768/10000, Prediction Accuracy = 62.476%, Loss = 0.3908625185489655
Epoch: 6768, Batch Gradient Norm: 10.223604964345157
Epoch: 6768, Batch Gradient Norm after: 10.223604964345157
Epoch 6769/10000, Prediction Accuracy = 62.528%, Loss = 0.4065428614616394
Epoch: 6769, Batch Gradient Norm: 12.688369095286161
Epoch: 6769, Batch Gradient Norm after: 12.688369095286161
Epoch 6770/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.4301189720630646
Epoch: 6770, Batch Gradient Norm: 8.467256760070534
Epoch: 6770, Batch Gradient Norm after: 8.467256760070534
Epoch 6771/10000, Prediction Accuracy = 62.56199999999999%, Loss = 0.39927542209625244
Epoch: 6771, Batch Gradient Norm: 11.126392492175952
Epoch: 6771, Batch Gradient Norm after: 11.126392492175952
Epoch 6772/10000, Prediction Accuracy = 62.674%, Loss = 0.4115682184696198
Epoch: 6772, Batch Gradient Norm: 12.005657719989493
Epoch: 6772, Batch Gradient Norm after: 12.005657719989493
Epoch 6773/10000, Prediction Accuracy = 62.612%, Loss = 0.41978827118873596
Epoch: 6773, Batch Gradient Norm: 9.351036989636814
Epoch: 6773, Batch Gradient Norm after: 9.351036989636814
Epoch 6774/10000, Prediction Accuracy = 62.604%, Loss = 0.40264649987220763
Epoch: 6774, Batch Gradient Norm: 8.898768931385808
Epoch: 6774, Batch Gradient Norm after: 8.898768931385808
Epoch 6775/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.3972655475139618
Epoch: 6775, Batch Gradient Norm: 8.963058167051296
Epoch: 6775, Batch Gradient Norm after: 8.963058167051296
Epoch 6776/10000, Prediction Accuracy = 62.58399999999999%, Loss = 0.3959206521511078
Epoch: 6776, Batch Gradient Norm: 7.536538733628361
Epoch: 6776, Batch Gradient Norm after: 7.536538733628361
Epoch 6777/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.3885940730571747
Epoch: 6777, Batch Gradient Norm: 9.283295667906517
Epoch: 6777, Batch Gradient Norm after: 9.283295667906517
Epoch 6778/10000, Prediction Accuracy = 62.424%, Loss = 0.4016211688518524
Epoch: 6778, Batch Gradient Norm: 8.157130037242078
Epoch: 6778, Batch Gradient Norm after: 8.157130037242078
Epoch 6779/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.39331465363502505
Epoch: 6779, Batch Gradient Norm: 9.878328190754367
Epoch: 6779, Batch Gradient Norm after: 9.878328190754367
Epoch 6780/10000, Prediction Accuracy = 62.662%, Loss = 0.4086125373840332
Epoch: 6780, Batch Gradient Norm: 12.00229403689006
Epoch: 6780, Batch Gradient Norm after: 12.00229403689006
Epoch 6781/10000, Prediction Accuracy = 62.576%, Loss = 0.4274228930473328
Epoch: 6781, Batch Gradient Norm: 10.069831998577456
Epoch: 6781, Batch Gradient Norm after: 10.069831998577456
Epoch 6782/10000, Prediction Accuracy = 62.6%, Loss = 0.40423900485038755
Epoch: 6782, Batch Gradient Norm: 11.998253844605106
Epoch: 6782, Batch Gradient Norm after: 11.998253844605106
Epoch 6783/10000, Prediction Accuracy = 62.48%, Loss = 0.4200598657131195
Epoch: 6783, Batch Gradient Norm: 7.877444862951489
Epoch: 6783, Batch Gradient Norm after: 7.877444862951489
Epoch 6784/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.39325606226921084
Epoch: 6784, Batch Gradient Norm: 7.985174434842732
Epoch: 6784, Batch Gradient Norm after: 7.985174434842732
Epoch 6785/10000, Prediction Accuracy = 62.634%, Loss = 0.3933661699295044
Epoch: 6785, Batch Gradient Norm: 11.196306403002591
Epoch: 6785, Batch Gradient Norm after: 11.196306403002591
Epoch 6786/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.41513620018959047
Epoch: 6786, Batch Gradient Norm: 12.248000282173567
Epoch: 6786, Batch Gradient Norm after: 12.248000282173567
Epoch 6787/10000, Prediction Accuracy = 62.3%, Loss = 0.42296973466873167
Epoch: 6787, Batch Gradient Norm: 11.43921089434868
Epoch: 6787, Batch Gradient Norm after: 11.43921089434868
Epoch 6788/10000, Prediction Accuracy = 62.64%, Loss = 0.41485395431518557
Epoch: 6788, Batch Gradient Norm: 9.039914914827085
Epoch: 6788, Batch Gradient Norm after: 9.039914914827085
Epoch 6789/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.39918742775917054
Epoch: 6789, Batch Gradient Norm: 11.086532018504958
Epoch: 6789, Batch Gradient Norm after: 11.086532018504958
Epoch 6790/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.41244136691093447
Epoch: 6790, Batch Gradient Norm: 10.471169318018925
Epoch: 6790, Batch Gradient Norm after: 10.471169318018925
Epoch 6791/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.4090155363082886
Epoch: 6791, Batch Gradient Norm: 8.473052969350439
Epoch: 6791, Batch Gradient Norm after: 8.473052969350439
Epoch 6792/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.39774619340896605
Epoch: 6792, Batch Gradient Norm: 9.685365898609387
Epoch: 6792, Batch Gradient Norm after: 9.685365898609387
Epoch 6793/10000, Prediction Accuracy = 62.678%, Loss = 0.4029076337814331
Epoch: 6793, Batch Gradient Norm: 10.175816843705471
Epoch: 6793, Batch Gradient Norm after: 10.175816843705471
Epoch 6794/10000, Prediction Accuracy = 62.60999999999999%, Loss = 0.4067664980888367
Epoch: 6794, Batch Gradient Norm: 9.804372759313363
Epoch: 6794, Batch Gradient Norm after: 9.804372759313363
Epoch 6795/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.4031132459640503
Epoch: 6795, Batch Gradient Norm: 8.731690837930852
Epoch: 6795, Batch Gradient Norm after: 8.731690837930852
Epoch 6796/10000, Prediction Accuracy = 62.504%, Loss = 0.3985943078994751
Epoch: 6796, Batch Gradient Norm: 8.038027156178465
Epoch: 6796, Batch Gradient Norm after: 8.038027156178465
Epoch 6797/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.3915671706199646
Epoch: 6797, Batch Gradient Norm: 8.779556944691125
Epoch: 6797, Batch Gradient Norm after: 8.779556944691125
Epoch 6798/10000, Prediction Accuracy = 62.63399999999999%, Loss = 0.39724854230880735
Epoch: 6798, Batch Gradient Norm: 9.639750031038446
Epoch: 6798, Batch Gradient Norm after: 9.639750031038446
Epoch 6799/10000, Prediction Accuracy = 62.386%, Loss = 0.40423306822776794
Epoch: 6799, Batch Gradient Norm: 10.716126644343925
Epoch: 6799, Batch Gradient Norm after: 10.716126644343925
Epoch 6800/10000, Prediction Accuracy = 62.464%, Loss = 0.4138931155204773
Epoch: 6800, Batch Gradient Norm: 6.874725779248755
Epoch: 6800, Batch Gradient Norm after: 6.874725779248755
Epoch 6801/10000, Prediction Accuracy = 62.564%, Loss = 0.3867992639541626
Epoch: 6801, Batch Gradient Norm: 10.514066451117587
Epoch: 6801, Batch Gradient Norm after: 10.514066451117587
Epoch 6802/10000, Prediction Accuracy = 62.544%, Loss = 0.4095850706100464
Epoch: 6802, Batch Gradient Norm: 10.05427930227735
Epoch: 6802, Batch Gradient Norm after: 10.05427930227735
Epoch 6803/10000, Prediction Accuracy = 62.56%, Loss = 0.40633569955825805
Epoch: 6803, Batch Gradient Norm: 7.400261272179259
Epoch: 6803, Batch Gradient Norm after: 7.400261272179259
Epoch 6804/10000, Prediction Accuracy = 62.61%, Loss = 0.38980462551116946
Epoch: 6804, Batch Gradient Norm: 9.124494190676415
Epoch: 6804, Batch Gradient Norm after: 9.124494190676415
Epoch 6805/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.3977310240268707
Epoch: 6805, Batch Gradient Norm: 9.707361712916963
Epoch: 6805, Batch Gradient Norm after: 9.707361712916963
Epoch 6806/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.401795756816864
Epoch: 6806, Batch Gradient Norm: 9.479084624968051
Epoch: 6806, Batch Gradient Norm after: 9.479084624968051
Epoch 6807/10000, Prediction Accuracy = 62.476%, Loss = 0.4048857271671295
Epoch: 6807, Batch Gradient Norm: 10.69323953667874
Epoch: 6807, Batch Gradient Norm after: 10.69323953667874
Epoch 6808/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.414140522480011
Epoch: 6808, Batch Gradient Norm: 12.67658859173649
Epoch: 6808, Batch Gradient Norm after: 12.67658859173649
Epoch 6809/10000, Prediction Accuracy = 62.556%, Loss = 0.4255094587802887
Epoch: 6809, Batch Gradient Norm: 10.95318921318957
Epoch: 6809, Batch Gradient Norm after: 10.95318921318957
Epoch 6810/10000, Prediction Accuracy = 62.714%, Loss = 0.41282652616500853
Epoch: 6810, Batch Gradient Norm: 9.407968742876152
Epoch: 6810, Batch Gradient Norm after: 9.407968742876152
Epoch 6811/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.40067163705825803
Epoch: 6811, Batch Gradient Norm: 8.243295751820641
Epoch: 6811, Batch Gradient Norm after: 8.243295751820641
Epoch 6812/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.39367892146110534
Epoch: 6812, Batch Gradient Norm: 8.230641510062597
Epoch: 6812, Batch Gradient Norm after: 8.230641510062597
Epoch 6813/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.39348838329315183
Epoch: 6813, Batch Gradient Norm: 9.971992537991705
Epoch: 6813, Batch Gradient Norm after: 9.971992537991705
Epoch 6814/10000, Prediction Accuracy = 62.624%, Loss = 0.40623324513435366
Epoch: 6814, Batch Gradient Norm: 9.466080072827436
Epoch: 6814, Batch Gradient Norm after: 9.466080072827436
Epoch 6815/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.401784086227417
Epoch: 6815, Batch Gradient Norm: 8.921112928051182
Epoch: 6815, Batch Gradient Norm after: 8.921112928051182
Epoch 6816/10000, Prediction Accuracy = 62.552%, Loss = 0.3961677849292755
Epoch: 6816, Batch Gradient Norm: 10.202597034960004
Epoch: 6816, Batch Gradient Norm after: 10.202597034960004
Epoch 6817/10000, Prediction Accuracy = 62.496%, Loss = 0.40616915225982664
Epoch: 6817, Batch Gradient Norm: 10.449906770418993
Epoch: 6817, Batch Gradient Norm after: 10.449906770418993
Epoch 6818/10000, Prediction Accuracy = 62.588%, Loss = 0.40896097421646116
Epoch: 6818, Batch Gradient Norm: 8.806208222290634
Epoch: 6818, Batch Gradient Norm after: 8.806208222290634
Epoch 6819/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.3984679996967316
Epoch: 6819, Batch Gradient Norm: 8.604673870952912
Epoch: 6819, Batch Gradient Norm after: 8.604673870952912
Epoch 6820/10000, Prediction Accuracy = 62.63399999999999%, Loss = 0.3949269115924835
Epoch: 6820, Batch Gradient Norm: 10.187126941394498
Epoch: 6820, Batch Gradient Norm after: 10.187126941394498
Epoch 6821/10000, Prediction Accuracy = 62.61%, Loss = 0.40721642374992373
Epoch: 6821, Batch Gradient Norm: 10.03742666416881
Epoch: 6821, Batch Gradient Norm after: 10.03742666416881
Epoch 6822/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.40480183362960814
Epoch: 6822, Batch Gradient Norm: 12.229842502528387
Epoch: 6822, Batch Gradient Norm after: 12.229842502528387
Epoch 6823/10000, Prediction Accuracy = 62.396%, Loss = 0.4231537222862244
Epoch: 6823, Batch Gradient Norm: 10.805839838287973
Epoch: 6823, Batch Gradient Norm after: 10.805839838287973
Epoch 6824/10000, Prediction Accuracy = 62.59000000000001%, Loss = 0.41095252633094786
Epoch: 6824, Batch Gradient Norm: 8.025866631141486
Epoch: 6824, Batch Gradient Norm after: 8.025866631141486
Epoch 6825/10000, Prediction Accuracy = 62.508%, Loss = 0.39339710474014283
Epoch: 6825, Batch Gradient Norm: 6.780566658754358
Epoch: 6825, Batch Gradient Norm after: 6.780566658754358
Epoch 6826/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.38406654000282286
Epoch: 6826, Batch Gradient Norm: 9.449554462187256
Epoch: 6826, Batch Gradient Norm after: 9.449554462187256
Epoch 6827/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.400804215669632
Epoch: 6827, Batch Gradient Norm: 9.608427079094023
Epoch: 6827, Batch Gradient Norm after: 9.608427079094023
Epoch 6828/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.40210424065589906
Epoch: 6828, Batch Gradient Norm: 11.944358570667008
Epoch: 6828, Batch Gradient Norm after: 11.944358570667008
Epoch 6829/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.42168283462524414
Epoch: 6829, Batch Gradient Norm: 11.856780106713355
Epoch: 6829, Batch Gradient Norm after: 11.856780106713355
Epoch 6830/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.4222246825695038
Epoch: 6830, Batch Gradient Norm: 7.321060216440336
Epoch: 6830, Batch Gradient Norm after: 7.321060216440336
Epoch 6831/10000, Prediction Accuracy = 62.586%, Loss = 0.3875911355018616
Epoch: 6831, Batch Gradient Norm: 7.459126335731372
Epoch: 6831, Batch Gradient Norm after: 7.459126335731372
Epoch 6832/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.3877574741840363
Epoch: 6832, Batch Gradient Norm: 11.92773293546573
Epoch: 6832, Batch Gradient Norm after: 11.92773293546573
Epoch 6833/10000, Prediction Accuracy = 62.4%, Loss = 0.4193926930427551
Epoch: 6833, Batch Gradient Norm: 11.303987905984235
Epoch: 6833, Batch Gradient Norm after: 11.303987905984235
Epoch 6834/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.415532249212265
Epoch: 6834, Batch Gradient Norm: 11.308180694658475
Epoch: 6834, Batch Gradient Norm after: 11.308180694658475
Epoch 6835/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.4198739051818848
Epoch: 6835, Batch Gradient Norm: 8.178122461141966
Epoch: 6835, Batch Gradient Norm after: 8.178122461141966
Epoch 6836/10000, Prediction Accuracy = 62.726%, Loss = 0.3927343785762787
Epoch: 6836, Batch Gradient Norm: 10.98218699041898
Epoch: 6836, Batch Gradient Norm after: 10.98218699041898
Epoch 6837/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4153436541557312
Epoch: 6837, Batch Gradient Norm: 9.650889857589654
Epoch: 6837, Batch Gradient Norm after: 9.650889857589654
Epoch 6838/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.4065869331359863
Epoch: 6838, Batch Gradient Norm: 9.275628023400467
Epoch: 6838, Batch Gradient Norm after: 9.275628023400467
Epoch 6839/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.40297932028770445
Epoch: 6839, Batch Gradient Norm: 8.79773779857625
Epoch: 6839, Batch Gradient Norm after: 8.79773779857625
Epoch 6840/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.39713247418403624
Epoch: 6840, Batch Gradient Norm: 9.917385840179106
Epoch: 6840, Batch Gradient Norm after: 9.917385840179106
Epoch 6841/10000, Prediction Accuracy = 62.682%, Loss = 0.4038687884807587
Epoch: 6841, Batch Gradient Norm: 8.851081147800231
Epoch: 6841, Batch Gradient Norm after: 8.851081147800231
Epoch 6842/10000, Prediction Accuracy = 62.536%, Loss = 0.39701594710350036
Epoch: 6842, Batch Gradient Norm: 7.577892900673055
Epoch: 6842, Batch Gradient Norm after: 7.577892900673055
Epoch 6843/10000, Prediction Accuracy = 62.592%, Loss = 0.39088650941848757
Epoch: 6843, Batch Gradient Norm: 8.893404729809363
Epoch: 6843, Batch Gradient Norm after: 8.893404729809363
Epoch 6844/10000, Prediction Accuracy = 62.72800000000001%, Loss = 0.39783520698547364
Epoch: 6844, Batch Gradient Norm: 12.204133744711298
Epoch: 6844, Batch Gradient Norm after: 12.204133744711298
Epoch 6845/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.41890971064567567
Epoch: 6845, Batch Gradient Norm: 12.334184087359757
Epoch: 6845, Batch Gradient Norm after: 12.334184087359757
Epoch 6846/10000, Prediction Accuracy = 62.564%, Loss = 0.4218788266181946
Epoch: 6846, Batch Gradient Norm: 11.577573920503795
Epoch: 6846, Batch Gradient Norm after: 11.577573920503795
Epoch 6847/10000, Prediction Accuracy = 62.472%, Loss = 0.4159677863121033
Epoch: 6847, Batch Gradient Norm: 10.54195445349024
Epoch: 6847, Batch Gradient Norm after: 10.54195445349024
Epoch 6848/10000, Prediction Accuracy = 62.65%, Loss = 0.40878947973251345
Epoch: 6848, Batch Gradient Norm: 8.632348900896469
Epoch: 6848, Batch Gradient Norm after: 8.632348900896469
Epoch 6849/10000, Prediction Accuracy = 62.688%, Loss = 0.3953330934047699
Epoch: 6849, Batch Gradient Norm: 10.347511809756964
Epoch: 6849, Batch Gradient Norm after: 10.347511809756964
Epoch 6850/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.40841482281684877
Epoch: 6850, Batch Gradient Norm: 7.4288232020336675
Epoch: 6850, Batch Gradient Norm after: 7.4288232020336675
Epoch 6851/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.3895843267440796
Epoch: 6851, Batch Gradient Norm: 7.534018974695681
Epoch: 6851, Batch Gradient Norm after: 7.534018974695681
Epoch 6852/10000, Prediction Accuracy = 62.612%, Loss = 0.38657714128494264
Epoch: 6852, Batch Gradient Norm: 9.903846779265034
Epoch: 6852, Batch Gradient Norm after: 9.903846779265034
Epoch 6853/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4031967282295227
Epoch: 6853, Batch Gradient Norm: 8.945307062782305
Epoch: 6853, Batch Gradient Norm after: 8.945307062782305
Epoch 6854/10000, Prediction Accuracy = 62.53000000000001%, Loss = 0.3961777210235596
Epoch: 6854, Batch Gradient Norm: 10.773838181424845
Epoch: 6854, Batch Gradient Norm after: 10.773838181424845
Epoch 6855/10000, Prediction Accuracy = 62.614%, Loss = 0.4109511137008667
Epoch: 6855, Batch Gradient Norm: 8.858238292658713
Epoch: 6855, Batch Gradient Norm after: 8.858238292658713
Epoch 6856/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.3978408634662628
Epoch: 6856, Batch Gradient Norm: 7.187592741284207
Epoch: 6856, Batch Gradient Norm after: 7.187592741284207
Epoch 6857/10000, Prediction Accuracy = 62.476%, Loss = 0.3867232918739319
Epoch: 6857, Batch Gradient Norm: 8.030224323908849
Epoch: 6857, Batch Gradient Norm after: 8.030224323908849
Epoch 6858/10000, Prediction Accuracy = 62.64399999999999%, Loss = 0.3926621198654175
Epoch: 6858, Batch Gradient Norm: 11.216043320372222
Epoch: 6858, Batch Gradient Norm after: 11.216043320372222
Epoch 6859/10000, Prediction Accuracy = 62.48%, Loss = 0.4113535523414612
Epoch: 6859, Batch Gradient Norm: 12.064314250804488
Epoch: 6859, Batch Gradient Norm after: 12.064314250804488
Epoch 6860/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.41705103516578673
Epoch: 6860, Batch Gradient Norm: 8.650108192741701
Epoch: 6860, Batch Gradient Norm after: 8.650108192741701
Epoch 6861/10000, Prediction Accuracy = 62.798%, Loss = 0.3968267023563385
Epoch: 6861, Batch Gradient Norm: 12.046324799459049
Epoch: 6861, Batch Gradient Norm after: 12.046324799459049
Epoch 6862/10000, Prediction Accuracy = 62.428%, Loss = 0.42832891941070556
Epoch: 6862, Batch Gradient Norm: 8.062423652346157
Epoch: 6862, Batch Gradient Norm after: 8.062423652346157
Epoch 6863/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.3907612144947052
Epoch: 6863, Batch Gradient Norm: 10.101859696138645
Epoch: 6863, Batch Gradient Norm after: 10.101859696138645
Epoch 6864/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4038982927799225
Epoch: 6864, Batch Gradient Norm: 10.662614975110829
Epoch: 6864, Batch Gradient Norm after: 10.662614975110829
Epoch 6865/10000, Prediction Accuracy = 62.53800000000001%, Loss = 0.4107499897480011
Epoch: 6865, Batch Gradient Norm: 7.366235059000789
Epoch: 6865, Batch Gradient Norm after: 7.366235059000789
Epoch 6866/10000, Prediction Accuracy = 62.638%, Loss = 0.38872775435447693
Epoch: 6866, Batch Gradient Norm: 8.402547526663666
Epoch: 6866, Batch Gradient Norm after: 8.402547526663666
Epoch 6867/10000, Prediction Accuracy = 62.602%, Loss = 0.393696790933609
Epoch: 6867, Batch Gradient Norm: 10.390034549962168
Epoch: 6867, Batch Gradient Norm after: 10.390034549962168
Epoch 6868/10000, Prediction Accuracy = 62.626%, Loss = 0.4044671535491943
Epoch: 6868, Batch Gradient Norm: 12.382729209814372
Epoch: 6868, Batch Gradient Norm after: 12.382729209814372
Epoch 6869/10000, Prediction Accuracy = 62.568%, Loss = 0.42642695307731626
Epoch: 6869, Batch Gradient Norm: 7.3959557198014
Epoch: 6869, Batch Gradient Norm after: 7.3959557198014
Epoch 6870/10000, Prediction Accuracy = 62.674%, Loss = 0.38954868316650393
Epoch: 6870, Batch Gradient Norm: 6.422773270932551
Epoch: 6870, Batch Gradient Norm after: 6.422773270932551
Epoch 6871/10000, Prediction Accuracy = 62.652%, Loss = 0.38287863731384275
Epoch: 6871, Batch Gradient Norm: 12.535976629349108
Epoch: 6871, Batch Gradient Norm after: 12.535976629349108
Epoch 6872/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.4288241326808929
Epoch: 6872, Batch Gradient Norm: 9.10936645414822
Epoch: 6872, Batch Gradient Norm after: 9.10936645414822
Epoch 6873/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.4017383098602295
Epoch: 6873, Batch Gradient Norm: 11.479270939589737
Epoch: 6873, Batch Gradient Norm after: 11.479270939589737
Epoch 6874/10000, Prediction Accuracy = 62.71%, Loss = 0.41379216909408567
Epoch: 6874, Batch Gradient Norm: 11.24977107018507
Epoch: 6874, Batch Gradient Norm after: 11.24977107018507
Epoch 6875/10000, Prediction Accuracy = 62.448%, Loss = 0.4115186154842377
Epoch: 6875, Batch Gradient Norm: 8.96693010984362
Epoch: 6875, Batch Gradient Norm after: 8.96693010984362
Epoch 6876/10000, Prediction Accuracy = 62.508%, Loss = 0.39849221110343935
Epoch: 6876, Batch Gradient Norm: 9.895231088530442
Epoch: 6876, Batch Gradient Norm after: 9.895231088530442
Epoch 6877/10000, Prediction Accuracy = 62.69%, Loss = 0.40139532685279844
Epoch: 6877, Batch Gradient Norm: 12.734107103339095
Epoch: 6877, Batch Gradient Norm after: 12.734107103339095
Epoch 6878/10000, Prediction Accuracy = 62.592%, Loss = 0.4225317597389221
Epoch: 6878, Batch Gradient Norm: 9.49491763189562
Epoch: 6878, Batch Gradient Norm after: 9.49491763189562
Epoch 6879/10000, Prediction Accuracy = 62.702%, Loss = 0.40054538249969485
Epoch: 6879, Batch Gradient Norm: 10.935000760040129
Epoch: 6879, Batch Gradient Norm after: 10.935000760040129
Epoch 6880/10000, Prediction Accuracy = 62.624%, Loss = 0.4084736704826355
Epoch: 6880, Batch Gradient Norm: 9.274336208483467
Epoch: 6880, Batch Gradient Norm after: 9.274336208483467
Epoch 6881/10000, Prediction Accuracy = 62.726%, Loss = 0.39695810079574584
Epoch: 6881, Batch Gradient Norm: 9.815660558234386
Epoch: 6881, Batch Gradient Norm after: 9.815660558234386
Epoch 6882/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.40058253407478334
Epoch: 6882, Batch Gradient Norm: 8.947594106864509
Epoch: 6882, Batch Gradient Norm after: 8.947594106864509
Epoch 6883/10000, Prediction Accuracy = 62.616%, Loss = 0.3953115165233612
Epoch: 6883, Batch Gradient Norm: 9.278543524048501
Epoch: 6883, Batch Gradient Norm after: 9.278543524048501
Epoch 6884/10000, Prediction Accuracy = 62.548%, Loss = 0.3993906617164612
Epoch: 6884, Batch Gradient Norm: 7.589744629456917
Epoch: 6884, Batch Gradient Norm after: 7.589744629456917
Epoch 6885/10000, Prediction Accuracy = 62.53800000000001%, Loss = 0.38803382515907286
Epoch: 6885, Batch Gradient Norm: 9.909880604191146
Epoch: 6885, Batch Gradient Norm after: 9.909880604191146
Epoch 6886/10000, Prediction Accuracy = 62.568%, Loss = 0.40684614777565004
Epoch: 6886, Batch Gradient Norm: 8.098905708122235
Epoch: 6886, Batch Gradient Norm after: 8.098905708122235
Epoch 6887/10000, Prediction Accuracy = 62.67%, Loss = 0.39166274666786194
Epoch: 6887, Batch Gradient Norm: 7.258155032337778
Epoch: 6887, Batch Gradient Norm after: 7.258155032337778
Epoch 6888/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.38454718589782716
Epoch: 6888, Batch Gradient Norm: 11.209347012560189
Epoch: 6888, Batch Gradient Norm after: 11.209347012560189
Epoch 6889/10000, Prediction Accuracy = 62.652%, Loss = 0.40996593832969663
Epoch: 6889, Batch Gradient Norm: 10.318443208179104
Epoch: 6889, Batch Gradient Norm after: 10.318443208179104
Epoch 6890/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.406239253282547
Epoch: 6890, Batch Gradient Norm: 9.072150011259863
Epoch: 6890, Batch Gradient Norm after: 9.072150011259863
Epoch 6891/10000, Prediction Accuracy = 62.444%, Loss = 0.39647339582443236
Epoch: 6891, Batch Gradient Norm: 11.335954114957367
Epoch: 6891, Batch Gradient Norm after: 11.335954114957367
Epoch 6892/10000, Prediction Accuracy = 62.35%, Loss = 0.41506776213645935
Epoch: 6892, Batch Gradient Norm: 10.528987841665131
Epoch: 6892, Batch Gradient Norm after: 10.528987841665131
Epoch 6893/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.40741046667099
Epoch: 6893, Batch Gradient Norm: 9.252856927921432
Epoch: 6893, Batch Gradient Norm after: 9.252856927921432
Epoch 6894/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.39876514077186587
Epoch: 6894, Batch Gradient Norm: 9.717449186271569
Epoch: 6894, Batch Gradient Norm after: 9.717449186271569
Epoch 6895/10000, Prediction Accuracy = 62.59000000000001%, Loss = 0.40408769249916077
Epoch: 6895, Batch Gradient Norm: 9.078755817476843
Epoch: 6895, Batch Gradient Norm after: 9.078755817476843
Epoch 6896/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.39938005805015564
Epoch: 6896, Batch Gradient Norm: 7.66395560244821
Epoch: 6896, Batch Gradient Norm after: 7.66395560244821
Epoch 6897/10000, Prediction Accuracy = 62.676%, Loss = 0.3891187131404877
Epoch: 6897, Batch Gradient Norm: 9.849337064316744
Epoch: 6897, Batch Gradient Norm after: 9.849337064316744
Epoch 6898/10000, Prediction Accuracy = 62.71%, Loss = 0.40448476672172545
Epoch: 6898, Batch Gradient Norm: 11.747888863553603
Epoch: 6898, Batch Gradient Norm after: 11.747888863553603
Epoch 6899/10000, Prediction Accuracy = 62.538%, Loss = 0.4181725084781647
Epoch: 6899, Batch Gradient Norm: 9.274322455584338
Epoch: 6899, Batch Gradient Norm after: 9.274322455584338
Epoch 6900/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.3977802932262421
Epoch: 6900, Batch Gradient Norm: 13.01323761607889
Epoch: 6900, Batch Gradient Norm after: 13.01323761607889
Epoch 6901/10000, Prediction Accuracy = 62.51800000000001%, Loss = 0.4391776144504547
Epoch: 6901, Batch Gradient Norm: 8.942900707149107
Epoch: 6901, Batch Gradient Norm after: 8.942900707149107
Epoch 6902/10000, Prediction Accuracy = 62.746%, Loss = 0.39705991744995117
Epoch: 6902, Batch Gradient Norm: 10.912414447999042
Epoch: 6902, Batch Gradient Norm after: 10.912414447999042
Epoch 6903/10000, Prediction Accuracy = 62.71%, Loss = 0.4077090919017792
Epoch: 6903, Batch Gradient Norm: 10.805116029279356
Epoch: 6903, Batch Gradient Norm after: 10.805116029279356
Epoch 6904/10000, Prediction Accuracy = 62.548%, Loss = 0.40732510685920714
Epoch: 6904, Batch Gradient Norm: 7.846879656228282
Epoch: 6904, Batch Gradient Norm after: 7.846879656228282
Epoch 6905/10000, Prediction Accuracy = 62.584%, Loss = 0.3900737941265106
Epoch: 6905, Batch Gradient Norm: 6.577365974426828
Epoch: 6905, Batch Gradient Norm after: 6.577365974426828
Epoch 6906/10000, Prediction Accuracy = 62.652%, Loss = 0.3816467642784119
Epoch: 6906, Batch Gradient Norm: 10.180665896732934
Epoch: 6906, Batch Gradient Norm after: 10.180665896732934
Epoch 6907/10000, Prediction Accuracy = 62.604%, Loss = 0.4044801592826843
Epoch: 6907, Batch Gradient Norm: 10.896995422617566
Epoch: 6907, Batch Gradient Norm after: 10.896995422617566
Epoch 6908/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.40805272459983827
Epoch: 6908, Batch Gradient Norm: 14.089740280429888
Epoch: 6908, Batch Gradient Norm after: 14.089740280429888
Epoch 6909/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.43802410364151
Epoch: 6909, Batch Gradient Norm: 8.860681279001872
Epoch: 6909, Batch Gradient Norm after: 8.860681279001872
Epoch 6910/10000, Prediction Accuracy = 62.694%, Loss = 0.39519441723823545
Epoch: 6910, Batch Gradient Norm: 7.340995655889045
Epoch: 6910, Batch Gradient Norm after: 7.340995655889045
Epoch 6911/10000, Prediction Accuracy = 62.602%, Loss = 0.3888237655162811
Epoch: 6911, Batch Gradient Norm: 9.48449279446196
Epoch: 6911, Batch Gradient Norm after: 9.48449279446196
Epoch 6912/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.4011642038822174
Epoch: 6912, Batch Gradient Norm: 10.154897257173438
Epoch: 6912, Batch Gradient Norm after: 10.154897257173438
Epoch 6913/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.40324254631996154
Epoch: 6913, Batch Gradient Norm: 9.168884207971493
Epoch: 6913, Batch Gradient Norm after: 9.168884207971493
Epoch 6914/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3983461081981659
Epoch: 6914, Batch Gradient Norm: 9.187801853914612
Epoch: 6914, Batch Gradient Norm after: 9.187801853914612
Epoch 6915/10000, Prediction Accuracy = 62.678%, Loss = 0.3969001889228821
Epoch: 6915, Batch Gradient Norm: 8.814505267717236
Epoch: 6915, Batch Gradient Norm after: 8.814505267717236
Epoch 6916/10000, Prediction Accuracy = 62.733999999999995%, Loss = 0.3933303713798523
Epoch: 6916, Batch Gradient Norm: 10.118135251430008
Epoch: 6916, Batch Gradient Norm after: 10.118135251430008
Epoch 6917/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.40350338220596316
Epoch: 6917, Batch Gradient Norm: 8.219720250227974
Epoch: 6917, Batch Gradient Norm after: 8.219720250227974
Epoch 6918/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.3897591829299927
Epoch: 6918, Batch Gradient Norm: 9.898338461328569
Epoch: 6918, Batch Gradient Norm after: 9.898338461328569
Epoch 6919/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.40342010259628297
Epoch: 6919, Batch Gradient Norm: 8.617072211195422
Epoch: 6919, Batch Gradient Norm after: 8.617072211195422
Epoch 6920/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.39549278616905215
Epoch: 6920, Batch Gradient Norm: 9.760991176357797
Epoch: 6920, Batch Gradient Norm after: 9.760991176357797
Epoch 6921/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.40314623713493347
Epoch: 6921, Batch Gradient Norm: 9.784979059302016
Epoch: 6921, Batch Gradient Norm after: 9.784979059302016
Epoch 6922/10000, Prediction Accuracy = 62.674%, Loss = 0.40116176605224607
Epoch: 6922, Batch Gradient Norm: 9.284662855471314
Epoch: 6922, Batch Gradient Norm after: 9.284662855471314
Epoch 6923/10000, Prediction Accuracy = 62.676%, Loss = 0.39790996313095095
Epoch: 6923, Batch Gradient Norm: 6.086307076656771
Epoch: 6923, Batch Gradient Norm after: 6.086307076656771
Epoch 6924/10000, Prediction Accuracy = 62.718%, Loss = 0.3797681093215942
Epoch: 6924, Batch Gradient Norm: 7.930940737801434
Epoch: 6924, Batch Gradient Norm after: 7.930940737801434
Epoch 6925/10000, Prediction Accuracy = 62.8%, Loss = 0.38789175152778627
Epoch: 6925, Batch Gradient Norm: 12.720986075375782
Epoch: 6925, Batch Gradient Norm after: 12.720986075375782
Epoch 6926/10000, Prediction Accuracy = 62.318%, Loss = 0.4258584141731262
Epoch: 6926, Batch Gradient Norm: 11.945494625307528
Epoch: 6926, Batch Gradient Norm after: 11.945494625307528
Epoch 6927/10000, Prediction Accuracy = 62.53000000000001%, Loss = 0.41378586888313296
Epoch: 6927, Batch Gradient Norm: 9.34009256554369
Epoch: 6927, Batch Gradient Norm after: 9.34009256554369
Epoch 6928/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.39966397285461425
Epoch: 6928, Batch Gradient Norm: 9.906043007413695
Epoch: 6928, Batch Gradient Norm after: 9.906043007413695
Epoch 6929/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.40291914343833923
Epoch: 6929, Batch Gradient Norm: 10.864402010305186
Epoch: 6929, Batch Gradient Norm after: 10.864402010305186
Epoch 6930/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.40873926877975464
Epoch: 6930, Batch Gradient Norm: 9.103609261391357
Epoch: 6930, Batch Gradient Norm after: 9.103609261391357
Epoch 6931/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.3972390055656433
Epoch: 6931, Batch Gradient Norm: 8.740862014393675
Epoch: 6931, Batch Gradient Norm after: 8.740862014393675
Epoch 6932/10000, Prediction Accuracy = 62.6%, Loss = 0.3939233124256134
Epoch: 6932, Batch Gradient Norm: 9.158892579410956
Epoch: 6932, Batch Gradient Norm after: 9.158892579410956
Epoch 6933/10000, Prediction Accuracy = 62.634%, Loss = 0.3989559292793274
Epoch: 6933, Batch Gradient Norm: 10.700796060767338
Epoch: 6933, Batch Gradient Norm after: 10.700796060767338
Epoch 6934/10000, Prediction Accuracy = 62.624%, Loss = 0.4174945533275604
Epoch: 6934, Batch Gradient Norm: 8.867107445134408
Epoch: 6934, Batch Gradient Norm after: 8.867107445134408
Epoch 6935/10000, Prediction Accuracy = 62.71400000000001%, Loss = 0.3946900963783264
Epoch: 6935, Batch Gradient Norm: 10.31270064056601
Epoch: 6935, Batch Gradient Norm after: 10.31270064056601
Epoch 6936/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.4045415461063385
Epoch: 6936, Batch Gradient Norm: 9.41453865213586
Epoch: 6936, Batch Gradient Norm after: 9.41453865213586
Epoch 6937/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.3988687515258789
Epoch: 6937, Batch Gradient Norm: 8.879508765190204
Epoch: 6937, Batch Gradient Norm after: 8.879508765190204
Epoch 6938/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.394374406337738
Epoch: 6938, Batch Gradient Norm: 10.027888596458192
Epoch: 6938, Batch Gradient Norm after: 10.027888596458192
Epoch 6939/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.40101176500320435
Epoch: 6939, Batch Gradient Norm: 12.961229773273969
Epoch: 6939, Batch Gradient Norm after: 12.961229773273969
Epoch 6940/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.4244100511074066
Epoch: 6940, Batch Gradient Norm: 12.211816413757235
Epoch: 6940, Batch Gradient Norm after: 12.211816413757235
Epoch 6941/10000, Prediction Accuracy = 62.624%, Loss = 0.4181725084781647
Epoch: 6941, Batch Gradient Norm: 7.848001930326065
Epoch: 6941, Batch Gradient Norm after: 7.848001930326065
Epoch 6942/10000, Prediction Accuracy = 62.64399999999999%, Loss = 0.3887475192546844
Epoch: 6942, Batch Gradient Norm: 9.324875938518641
Epoch: 6942, Batch Gradient Norm after: 9.324875938518641
Epoch 6943/10000, Prediction Accuracy = 62.648%, Loss = 0.40191181302070617
Epoch: 6943, Batch Gradient Norm: 6.616966630264704
Epoch: 6943, Batch Gradient Norm after: 6.616966630264704
Epoch 6944/10000, Prediction Accuracy = 62.638%, Loss = 0.38202974796295164
Epoch: 6944, Batch Gradient Norm: 6.888965943350821
Epoch: 6944, Batch Gradient Norm after: 6.888965943350821
Epoch 6945/10000, Prediction Accuracy = 62.658%, Loss = 0.3858536422252655
Epoch: 6945, Batch Gradient Norm: 8.357030076221777
Epoch: 6945, Batch Gradient Norm after: 8.357030076221777
Epoch 6946/10000, Prediction Accuracy = 62.512%, Loss = 0.3916519582271576
Epoch: 6946, Batch Gradient Norm: 12.061476667310329
Epoch: 6946, Batch Gradient Norm after: 12.061476667310329
Epoch 6947/10000, Prediction Accuracy = 62.61200000000001%, Loss = 0.41594685316085817
Epoch: 6947, Batch Gradient Norm: 10.11256208331218
Epoch: 6947, Batch Gradient Norm after: 10.11256208331218
Epoch 6948/10000, Prediction Accuracy = 62.598%, Loss = 0.40220174193382263
Epoch: 6948, Batch Gradient Norm: 10.476131156576221
Epoch: 6948, Batch Gradient Norm after: 10.476131156576221
Epoch 6949/10000, Prediction Accuracy = 62.676%, Loss = 0.4041344881057739
Epoch: 6949, Batch Gradient Norm: 9.880941137456121
Epoch: 6949, Batch Gradient Norm after: 9.880941137456121
Epoch 6950/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.4026352107524872
Epoch: 6950, Batch Gradient Norm: 9.546228730029805
Epoch: 6950, Batch Gradient Norm after: 9.546228730029805
Epoch 6951/10000, Prediction Accuracy = 62.624%, Loss = 0.4015450417995453
Epoch: 6951, Batch Gradient Norm: 9.12722385921896
Epoch: 6951, Batch Gradient Norm after: 9.12722385921896
Epoch 6952/10000, Prediction Accuracy = 62.634%, Loss = 0.39806435704231263
Epoch: 6952, Batch Gradient Norm: 10.613045653890262
Epoch: 6952, Batch Gradient Norm after: 10.613045653890262
Epoch 6953/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4096013903617859
Epoch: 6953, Batch Gradient Norm: 10.988954817786178
Epoch: 6953, Batch Gradient Norm after: 10.988954817786178
Epoch 6954/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.4093723475933075
Epoch: 6954, Batch Gradient Norm: 6.694378582289062
Epoch: 6954, Batch Gradient Norm after: 6.694378582289062
Epoch 6955/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.3819455862045288
Epoch: 6955, Batch Gradient Norm: 9.908163312202468
Epoch: 6955, Batch Gradient Norm after: 9.908163312202468
Epoch 6956/10000, Prediction Accuracy = 62.56%, Loss = 0.40133869647979736
Epoch: 6956, Batch Gradient Norm: 11.172730009277172
Epoch: 6956, Batch Gradient Norm after: 11.172730009277172
Epoch 6957/10000, Prediction Accuracy = 62.322%, Loss = 0.4141165614128113
Epoch: 6957, Batch Gradient Norm: 10.563185233966996
Epoch: 6957, Batch Gradient Norm after: 10.563185233966996
Epoch 6958/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.40560453534126284
Epoch: 6958, Batch Gradient Norm: 10.617094242643681
Epoch: 6958, Batch Gradient Norm after: 10.617094242643681
Epoch 6959/10000, Prediction Accuracy = 62.586%, Loss = 0.4071321070194244
Epoch: 6959, Batch Gradient Norm: 9.53094791941057
Epoch: 6959, Batch Gradient Norm after: 9.53094791941057
Epoch 6960/10000, Prediction Accuracy = 62.602%, Loss = 0.39886811971664426
Epoch: 6960, Batch Gradient Norm: 9.266198546515216
Epoch: 6960, Batch Gradient Norm after: 9.266198546515216
Epoch 6961/10000, Prediction Accuracy = 62.754%, Loss = 0.39778603315353395
Epoch: 6961, Batch Gradient Norm: 7.6701891476984425
Epoch: 6961, Batch Gradient Norm after: 7.6701891476984425
Epoch 6962/10000, Prediction Accuracy = 62.548%, Loss = 0.3867645561695099
Epoch: 6962, Batch Gradient Norm: 11.021243954904007
Epoch: 6962, Batch Gradient Norm after: 11.021243954904007
Epoch 6963/10000, Prediction Accuracy = 62.552%, Loss = 0.4124742686748505
Epoch: 6963, Batch Gradient Norm: 8.607515499748793
Epoch: 6963, Batch Gradient Norm after: 8.607515499748793
Epoch 6964/10000, Prediction Accuracy = 62.806%, Loss = 0.3963761568069458
Epoch: 6964, Batch Gradient Norm: 9.434087287216887
Epoch: 6964, Batch Gradient Norm after: 9.434087287216887
Epoch 6965/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3960890591144562
Epoch: 6965, Batch Gradient Norm: 10.020236086177855
Epoch: 6965, Batch Gradient Norm after: 10.020236086177855
Epoch 6966/10000, Prediction Accuracy = 62.66199999999999%, Loss = 0.4004142701625824
Epoch: 6966, Batch Gradient Norm: 10.97796310930033
Epoch: 6966, Batch Gradient Norm after: 10.97796310930033
Epoch 6967/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.41587154269218446
Epoch: 6967, Batch Gradient Norm: 10.524121048719039
Epoch: 6967, Batch Gradient Norm after: 10.524121048719039
Epoch 6968/10000, Prediction Accuracy = 62.64399999999999%, Loss = 0.40475496649742126
Epoch: 6968, Batch Gradient Norm: 9.897985994160507
Epoch: 6968, Batch Gradient Norm after: 9.897985994160507
Epoch 6969/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3994352400302887
Epoch: 6969, Batch Gradient Norm: 9.137835919854377
Epoch: 6969, Batch Gradient Norm after: 9.137835919854377
Epoch 6970/10000, Prediction Accuracy = 62.39%, Loss = 0.39731873869895934
Epoch: 6970, Batch Gradient Norm: 9.175606989304937
Epoch: 6970, Batch Gradient Norm after: 9.175606989304937
Epoch 6971/10000, Prediction Accuracy = 62.64%, Loss = 0.3944938898086548
Epoch: 6971, Batch Gradient Norm: 9.176547837698466
Epoch: 6971, Batch Gradient Norm after: 9.176547837698466
Epoch 6972/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.39502766728401184
Epoch: 6972, Batch Gradient Norm: 10.203110594554282
Epoch: 6972, Batch Gradient Norm after: 10.203110594554282
Epoch 6973/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.40408207178115846
Epoch: 6973, Batch Gradient Norm: 11.081644337143034
Epoch: 6973, Batch Gradient Norm after: 11.081644337143034
Epoch 6974/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.4157720744609833
Epoch: 6974, Batch Gradient Norm: 10.78054332870794
Epoch: 6974, Batch Gradient Norm after: 10.78054332870794
Epoch 6975/10000, Prediction Accuracy = 62.738%, Loss = 0.41292437314987185
Epoch: 6975, Batch Gradient Norm: 7.211133031858584
Epoch: 6975, Batch Gradient Norm after: 7.211133031858584
Epoch 6976/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.3872714340686798
Epoch: 6976, Batch Gradient Norm: 8.43116113919383
Epoch: 6976, Batch Gradient Norm after: 8.43116113919383
Epoch 6977/10000, Prediction Accuracy = 62.7%, Loss = 0.39077848196029663
Epoch: 6977, Batch Gradient Norm: 11.246713854710201
Epoch: 6977, Batch Gradient Norm after: 11.246713854710201
Epoch 6978/10000, Prediction Accuracy = 62.806000000000004%, Loss = 0.4126792669296265
Epoch: 6978, Batch Gradient Norm: 12.01659828247412
Epoch: 6978, Batch Gradient Norm after: 12.01659828247412
Epoch 6979/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.41758317947387696
Epoch: 6979, Batch Gradient Norm: 8.37588995604479
Epoch: 6979, Batch Gradient Norm after: 8.37588995604479
Epoch 6980/10000, Prediction Accuracy = 62.714%, Loss = 0.3902959167957306
Epoch: 6980, Batch Gradient Norm: 9.654980108269276
Epoch: 6980, Batch Gradient Norm after: 9.654980108269276
Epoch 6981/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.39795772433280946
Epoch: 6981, Batch Gradient Norm: 11.15116330769802
Epoch: 6981, Batch Gradient Norm after: 11.15116330769802
Epoch 6982/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.4094079554080963
Epoch: 6982, Batch Gradient Norm: 9.50939695124055
Epoch: 6982, Batch Gradient Norm after: 9.50939695124055
Epoch 6983/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.39775227904319765
Epoch: 6983, Batch Gradient Norm: 8.2692979679281
Epoch: 6983, Batch Gradient Norm after: 8.2692979679281
Epoch 6984/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.3902370989322662
Epoch: 6984, Batch Gradient Norm: 8.759809491526816
Epoch: 6984, Batch Gradient Norm after: 8.759809491526816
Epoch 6985/10000, Prediction Accuracy = 62.624%, Loss = 0.39166361689567564
Epoch: 6985, Batch Gradient Norm: 10.734467259557787
Epoch: 6985, Batch Gradient Norm after: 10.734467259557787
Epoch 6986/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.40371946096420286
Epoch: 6986, Batch Gradient Norm: 10.259793410756895
Epoch: 6986, Batch Gradient Norm after: 10.259793410756895
Epoch 6987/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.40650254487991333
Epoch: 6987, Batch Gradient Norm: 7.691199845152496
Epoch: 6987, Batch Gradient Norm after: 7.691199845152496
Epoch 6988/10000, Prediction Accuracy = 62.686%, Loss = 0.38831177949905393
Epoch: 6988, Batch Gradient Norm: 10.41596259038264
Epoch: 6988, Batch Gradient Norm after: 10.41596259038264
Epoch 6989/10000, Prediction Accuracy = 62.622%, Loss = 0.4082093834877014
Epoch: 6989, Batch Gradient Norm: 9.31519715109551
Epoch: 6989, Batch Gradient Norm after: 9.31519715109551
Epoch 6990/10000, Prediction Accuracy = 62.648%, Loss = 0.39781704545021057
Epoch: 6990, Batch Gradient Norm: 10.875735421474547
Epoch: 6990, Batch Gradient Norm after: 10.875735421474547
Epoch 6991/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.40880166888237
Epoch: 6991, Batch Gradient Norm: 8.825917376642192
Epoch: 6991, Batch Gradient Norm after: 8.825917376642192
Epoch 6992/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.3960664987564087
Epoch: 6992, Batch Gradient Norm: 10.436440804654097
Epoch: 6992, Batch Gradient Norm after: 10.436440804654097
Epoch 6993/10000, Prediction Accuracy = 62.540000000000006%, Loss = 0.402752548456192
Epoch: 6993, Batch Gradient Norm: 9.734963383128884
Epoch: 6993, Batch Gradient Norm after: 9.734963383128884
Epoch 6994/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.39930888414382937
Epoch: 6994, Batch Gradient Norm: 10.186021990322018
Epoch: 6994, Batch Gradient Norm after: 10.186021990322018
Epoch 6995/10000, Prediction Accuracy = 62.674%, Loss = 0.4017075777053833
Epoch: 6995, Batch Gradient Norm: 8.080001770583696
Epoch: 6995, Batch Gradient Norm after: 8.080001770583696
Epoch 6996/10000, Prediction Accuracy = 62.638%, Loss = 0.3884308457374573
Epoch: 6996, Batch Gradient Norm: 9.930290532351858
Epoch: 6996, Batch Gradient Norm after: 9.930290532351858
Epoch 6997/10000, Prediction Accuracy = 62.6%, Loss = 0.40366613268852236
Epoch: 6997, Batch Gradient Norm: 6.73141924090558
Epoch: 6997, Batch Gradient Norm after: 6.73141924090558
Epoch 6998/10000, Prediction Accuracy = 62.552%, Loss = 0.38141781091690063
Epoch: 6998, Batch Gradient Norm: 9.283825033954795
Epoch: 6998, Batch Gradient Norm after: 9.283825033954795
Epoch 6999/10000, Prediction Accuracy = 62.69%, Loss = 0.3940818428993225
Epoch: 6999, Batch Gradient Norm: 9.059229480769213
Epoch: 6999, Batch Gradient Norm after: 9.059229480769213
Epoch 7000/10000, Prediction Accuracy = 62.77%, Loss = 0.39603652954101565
Epoch: 7000, Batch Gradient Norm: 11.105288144834846
Epoch: 7000, Batch Gradient Norm after: 11.105288144834846
Epoch 7001/10000, Prediction Accuracy = 62.652%, Loss = 0.40824556946754453
Epoch: 7001, Batch Gradient Norm: 9.621708617633088
Epoch: 7001, Batch Gradient Norm after: 9.621708617633088
Epoch 7002/10000, Prediction Accuracy = 62.648%, Loss = 0.39784020781517027
Epoch: 7002, Batch Gradient Norm: 10.372900143236219
Epoch: 7002, Batch Gradient Norm after: 10.372900143236219
Epoch 7003/10000, Prediction Accuracy = 62.754%, Loss = 0.4051803171634674
Epoch: 7003, Batch Gradient Norm: 10.940025529184789
Epoch: 7003, Batch Gradient Norm after: 10.940025529184789
Epoch 7004/10000, Prediction Accuracy = 62.806%, Loss = 0.40623501539230344
Epoch: 7004, Batch Gradient Norm: 10.876339482975617
Epoch: 7004, Batch Gradient Norm after: 10.876339482975617
Epoch 7005/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.4095369815826416
Epoch: 7005, Batch Gradient Norm: 9.534044476861446
Epoch: 7005, Batch Gradient Norm after: 9.534044476861446
Epoch 7006/10000, Prediction Accuracy = 62.604%, Loss = 0.40088270902633666
Epoch: 7006, Batch Gradient Norm: 8.54511997706278
Epoch: 7006, Batch Gradient Norm after: 8.54511997706278
Epoch 7007/10000, Prediction Accuracy = 62.662%, Loss = 0.39115135073661805
Epoch: 7007, Batch Gradient Norm: 9.24594778926148
Epoch: 7007, Batch Gradient Norm after: 9.24594778926148
Epoch 7008/10000, Prediction Accuracy = 62.712%, Loss = 0.39935089349746705
Epoch: 7008, Batch Gradient Norm: 7.8683901158338365
Epoch: 7008, Batch Gradient Norm after: 7.8683901158338365
Epoch 7009/10000, Prediction Accuracy = 62.56199999999999%, Loss = 0.3873395025730133
Epoch: 7009, Batch Gradient Norm: 12.274079546631535
Epoch: 7009, Batch Gradient Norm after: 12.274079546631535
Epoch 7010/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.41556993722915647
Epoch: 7010, Batch Gradient Norm: 11.614972679503296
Epoch: 7010, Batch Gradient Norm after: 11.614972679503296
Epoch 7011/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.41442571878433226
Epoch: 7011, Batch Gradient Norm: 8.427677515848101
Epoch: 7011, Batch Gradient Norm after: 8.427677515848101
Epoch 7012/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.39016234278678896
Epoch: 7012, Batch Gradient Norm: 10.510841393579078
Epoch: 7012, Batch Gradient Norm after: 10.510841393579078
Epoch 7013/10000, Prediction Accuracy = 62.662%, Loss = 0.4036828398704529
Epoch: 7013, Batch Gradient Norm: 8.874500119535014
Epoch: 7013, Batch Gradient Norm after: 8.874500119535014
Epoch 7014/10000, Prediction Accuracy = 62.552%, Loss = 0.39094645977020265
Epoch: 7014, Batch Gradient Norm: 9.408383923731648
Epoch: 7014, Batch Gradient Norm after: 9.408383923731648
Epoch 7015/10000, Prediction Accuracy = 62.419999999999995%, Loss = 0.39817864894866944
Epoch: 7015, Batch Gradient Norm: 10.029075818255686
Epoch: 7015, Batch Gradient Norm after: 10.029075818255686
Epoch 7016/10000, Prediction Accuracy = 62.612%, Loss = 0.400039929151535
Epoch: 7016, Batch Gradient Norm: 7.931431429579717
Epoch: 7016, Batch Gradient Norm after: 7.931431429579717
Epoch 7017/10000, Prediction Accuracy = 62.622%, Loss = 0.3885172247886658
Epoch: 7017, Batch Gradient Norm: 12.76796984544059
Epoch: 7017, Batch Gradient Norm after: 12.76796984544059
Epoch 7018/10000, Prediction Accuracy = 62.638%, Loss = 0.42833123803138734
Epoch: 7018, Batch Gradient Norm: 12.061496406050134
Epoch: 7018, Batch Gradient Norm after: 12.061496406050134
Epoch 7019/10000, Prediction Accuracy = 62.648%, Loss = 0.41663903594017027
Epoch: 7019, Batch Gradient Norm: 8.400965027378971
Epoch: 7019, Batch Gradient Norm after: 8.400965027378971
Epoch 7020/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.39021661281585696
Epoch: 7020, Batch Gradient Norm: 6.208125555896654
Epoch: 7020, Batch Gradient Norm after: 6.208125555896654
Epoch 7021/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.38180922269821166
Epoch: 7021, Batch Gradient Norm: 8.973432982470875
Epoch: 7021, Batch Gradient Norm after: 8.973432982470875
Epoch 7022/10000, Prediction Accuracy = 62.67199999999999%, Loss = 0.39635306000709536
Epoch: 7022, Batch Gradient Norm: 11.025478245812652
Epoch: 7022, Batch Gradient Norm after: 11.025478245812652
Epoch 7023/10000, Prediction Accuracy = 62.39%, Loss = 0.41386250853538514
Epoch: 7023, Batch Gradient Norm: 7.73601257919021
Epoch: 7023, Batch Gradient Norm after: 7.73601257919021
Epoch 7024/10000, Prediction Accuracy = 62.674%, Loss = 0.38965888023376466
Epoch: 7024, Batch Gradient Norm: 6.317877969337759
Epoch: 7024, Batch Gradient Norm after: 6.317877969337759
Epoch 7025/10000, Prediction Accuracy = 62.682%, Loss = 0.3806667387485504
Epoch: 7025, Batch Gradient Norm: 11.367453834403902
Epoch: 7025, Batch Gradient Norm after: 11.367453834403902
Epoch 7026/10000, Prediction Accuracy = 62.53399999999999%, Loss = 0.4103114426136017
Epoch: 7026, Batch Gradient Norm: 13.283614679312162
Epoch: 7026, Batch Gradient Norm after: 13.283614679312162
Epoch 7027/10000, Prediction Accuracy = 62.426%, Loss = 0.4280399978160858
Epoch: 7027, Batch Gradient Norm: 9.438941304448276
Epoch: 7027, Batch Gradient Norm after: 9.438941304448276
Epoch 7028/10000, Prediction Accuracy = 62.614%, Loss = 0.39862515926361086
Epoch: 7028, Batch Gradient Norm: 6.767242480509707
Epoch: 7028, Batch Gradient Norm after: 6.767242480509707
Epoch 7029/10000, Prediction Accuracy = 62.798%, Loss = 0.3809190630912781
Epoch: 7029, Batch Gradient Norm: 9.899822312605636
Epoch: 7029, Batch Gradient Norm after: 9.899822312605636
Epoch 7030/10000, Prediction Accuracy = 62.464%, Loss = 0.39864531755447385
Epoch: 7030, Batch Gradient Norm: 12.146341231071252
Epoch: 7030, Batch Gradient Norm after: 12.146341231071252
Epoch 7031/10000, Prediction Accuracy = 62.614%, Loss = 0.41577420830726625
Epoch: 7031, Batch Gradient Norm: 11.102013840843108
Epoch: 7031, Batch Gradient Norm after: 11.102013840843108
Epoch 7032/10000, Prediction Accuracy = 62.568%, Loss = 0.40972686409950254
Epoch: 7032, Batch Gradient Norm: 8.363138269371388
Epoch: 7032, Batch Gradient Norm after: 8.363138269371388
Epoch 7033/10000, Prediction Accuracy = 62.75%, Loss = 0.3910943269729614
Epoch: 7033, Batch Gradient Norm: 6.575668641357885
Epoch: 7033, Batch Gradient Norm after: 6.575668641357885
Epoch 7034/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.38032063841819763
Epoch: 7034, Batch Gradient Norm: 7.693274076682193
Epoch: 7034, Batch Gradient Norm after: 7.693274076682193
Epoch 7035/10000, Prediction Accuracy = 62.538%, Loss = 0.38727661967277527
Epoch: 7035, Batch Gradient Norm: 11.392670875410642
Epoch: 7035, Batch Gradient Norm after: 11.392670875410642
Epoch 7036/10000, Prediction Accuracy = 62.5%, Loss = 0.4143223166465759
Epoch: 7036, Batch Gradient Norm: 11.43112123398848
Epoch: 7036, Batch Gradient Norm after: 11.43112123398848
Epoch 7037/10000, Prediction Accuracy = 62.574%, Loss = 0.41258049607276914
Epoch: 7037, Batch Gradient Norm: 10.558044403574744
Epoch: 7037, Batch Gradient Norm after: 10.558044403574744
Epoch 7038/10000, Prediction Accuracy = 62.648%, Loss = 0.40345968008041383
Epoch: 7038, Batch Gradient Norm: 8.045073862594373
Epoch: 7038, Batch Gradient Norm after: 8.045073862594373
Epoch 7039/10000, Prediction Accuracy = 62.834%, Loss = 0.3858585774898529
Epoch: 7039, Batch Gradient Norm: 6.385754398953276
Epoch: 7039, Batch Gradient Norm after: 6.385754398953276
Epoch 7040/10000, Prediction Accuracy = 62.806000000000004%, Loss = 0.3785854458808899
Epoch: 7040, Batch Gradient Norm: 10.986590954709806
Epoch: 7040, Batch Gradient Norm after: 10.986590954709806
Epoch 7041/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.40563763976097106
Epoch: 7041, Batch Gradient Norm: 9.986668428827727
Epoch: 7041, Batch Gradient Norm after: 9.986668428827727
Epoch 7042/10000, Prediction Accuracy = 62.678%, Loss = 0.3999775290489197
Epoch: 7042, Batch Gradient Norm: 9.918351605815312
Epoch: 7042, Batch Gradient Norm after: 9.918351605815312
Epoch 7043/10000, Prediction Accuracy = 62.652%, Loss = 0.401841938495636
Epoch: 7043, Batch Gradient Norm: 9.905511981052364
Epoch: 7043, Batch Gradient Norm after: 9.905511981052364
Epoch 7044/10000, Prediction Accuracy = 62.648%, Loss = 0.3992411375045776
Epoch: 7044, Batch Gradient Norm: 10.052552307370878
Epoch: 7044, Batch Gradient Norm after: 10.052552307370878
Epoch 7045/10000, Prediction Accuracy = 62.678%, Loss = 0.40063272714614867
Epoch: 7045, Batch Gradient Norm: 8.979612465243186
Epoch: 7045, Batch Gradient Norm after: 8.979612465243186
Epoch 7046/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.3936959862709045
Epoch: 7046, Batch Gradient Norm: 8.638564642120024
Epoch: 7046, Batch Gradient Norm after: 8.638564642120024
Epoch 7047/10000, Prediction Accuracy = 62.574%, Loss = 0.39442200064659116
Epoch: 7047, Batch Gradient Norm: 10.51536886819203
Epoch: 7047, Batch Gradient Norm after: 10.51536886819203
Epoch 7048/10000, Prediction Accuracy = 62.488%, Loss = 0.40901262760162355
Epoch: 7048, Batch Gradient Norm: 8.564251155211108
Epoch: 7048, Batch Gradient Norm after: 8.564251155211108
Epoch 7049/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.38939828872680665
Epoch: 7049, Batch Gradient Norm: 9.983534044649774
Epoch: 7049, Batch Gradient Norm after: 9.983534044649774
Epoch 7050/10000, Prediction Accuracy = 62.746%, Loss = 0.39871989488601683
Epoch: 7050, Batch Gradient Norm: 13.293463129320216
Epoch: 7050, Batch Gradient Norm after: 13.293463129320216
Epoch 7051/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.42401258945465087
Epoch: 7051, Batch Gradient Norm: 9.415959637527765
Epoch: 7051, Batch Gradient Norm after: 9.415959637527765
Epoch 7052/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.3965793430805206
Epoch: 7052, Batch Gradient Norm: 10.749935533737165
Epoch: 7052, Batch Gradient Norm after: 10.749935533737165
Epoch 7053/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.4049616396427155
Epoch: 7053, Batch Gradient Norm: 11.70695484484533
Epoch: 7053, Batch Gradient Norm after: 11.70695484484533
Epoch 7054/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.41117194294929504
Epoch: 7054, Batch Gradient Norm: 7.140970331574946
Epoch: 7054, Batch Gradient Norm after: 7.140970331574946
Epoch 7055/10000, Prediction Accuracy = 62.77%, Loss = 0.3823449075222015
Epoch: 7055, Batch Gradient Norm: 9.12081178981738
Epoch: 7055, Batch Gradient Norm after: 9.12081178981738
Epoch 7056/10000, Prediction Accuracy = 62.666%, Loss = 0.3959130346775055
Epoch: 7056, Batch Gradient Norm: 9.615643972723039
Epoch: 7056, Batch Gradient Norm after: 9.615643972723039
Epoch 7057/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.3971970558166504
Epoch: 7057, Batch Gradient Norm: 9.973902645571282
Epoch: 7057, Batch Gradient Norm after: 9.973902645571282
Epoch 7058/10000, Prediction Accuracy = 62.694%, Loss = 0.40195450782775877
Epoch: 7058, Batch Gradient Norm: 7.188154570051484
Epoch: 7058, Batch Gradient Norm after: 7.188154570051484
Epoch 7059/10000, Prediction Accuracy = 62.762%, Loss = 0.3845129072666168
Epoch: 7059, Batch Gradient Norm: 8.852420415438925
Epoch: 7059, Batch Gradient Norm after: 8.852420415438925
Epoch 7060/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.39044720530509947
Epoch: 7060, Batch Gradient Norm: 9.832168946000596
Epoch: 7060, Batch Gradient Norm after: 9.832168946000596
Epoch 7061/10000, Prediction Accuracy = 62.664%, Loss = 0.3990206837654114
Epoch: 7061, Batch Gradient Norm: 8.592794312760743
Epoch: 7061, Batch Gradient Norm after: 8.592794312760743
Epoch 7062/10000, Prediction Accuracy = 62.8%, Loss = 0.39011141657829285
Epoch: 7062, Batch Gradient Norm: 10.036017898909982
Epoch: 7062, Batch Gradient Norm after: 10.036017898909982
Epoch 7063/10000, Prediction Accuracy = 62.572%, Loss = 0.4014197111129761
Epoch: 7063, Batch Gradient Norm: 8.371324859880554
Epoch: 7063, Batch Gradient Norm after: 8.371324859880554
Epoch 7064/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.3924160897731781
Epoch: 7064, Batch Gradient Norm: 8.571130665914804
Epoch: 7064, Batch Gradient Norm after: 8.571130665914804
Epoch 7065/10000, Prediction Accuracy = 62.58%, Loss = 0.390166699886322
Epoch: 7065, Batch Gradient Norm: 8.792193755248235
Epoch: 7065, Batch Gradient Norm after: 8.792193755248235
Epoch 7066/10000, Prediction Accuracy = 62.512%, Loss = 0.38976357579231263
Epoch: 7066, Batch Gradient Norm: 11.007960626557292
Epoch: 7066, Batch Gradient Norm after: 11.007960626557292
Epoch 7067/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.40882947444915774
Epoch: 7067, Batch Gradient Norm: 13.110349711657115
Epoch: 7067, Batch Gradient Norm after: 13.110349711657115
Epoch 7068/10000, Prediction Accuracy = 62.592%, Loss = 0.42364022731781004
Epoch: 7068, Batch Gradient Norm: 11.089605707257977
Epoch: 7068, Batch Gradient Norm after: 11.089605707257977
Epoch 7069/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.4060753405094147
Epoch: 7069, Batch Gradient Norm: 10.902826152179289
Epoch: 7069, Batch Gradient Norm after: 10.902826152179289
Epoch 7070/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.4091675043106079
Epoch: 7070, Batch Gradient Norm: 7.7860671387783515
Epoch: 7070, Batch Gradient Norm after: 7.7860671387783515
Epoch 7071/10000, Prediction Accuracy = 62.81%, Loss = 0.3858906149864197
Epoch: 7071, Batch Gradient Norm: 9.024664871120725
Epoch: 7071, Batch Gradient Norm after: 9.024664871120725
Epoch 7072/10000, Prediction Accuracy = 62.598%, Loss = 0.3936218798160553
Epoch: 7072, Batch Gradient Norm: 9.277483156670792
Epoch: 7072, Batch Gradient Norm after: 9.277483156670792
Epoch 7073/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.39781118631362916
Epoch: 7073, Batch Gradient Norm: 8.977943708914337
Epoch: 7073, Batch Gradient Norm after: 8.977943708914337
Epoch 7074/10000, Prediction Accuracy = 62.624%, Loss = 0.395345139503479
Epoch: 7074, Batch Gradient Norm: 10.25562893423542
Epoch: 7074, Batch Gradient Norm after: 10.25562893423542
Epoch 7075/10000, Prediction Accuracy = 62.652%, Loss = 0.4041277527809143
Epoch: 7075, Batch Gradient Norm: 10.371105342096532
Epoch: 7075, Batch Gradient Norm after: 10.371105342096532
Epoch 7076/10000, Prediction Accuracy = 62.714%, Loss = 0.4022416055202484
Epoch: 7076, Batch Gradient Norm: 11.254344293583836
Epoch: 7076, Batch Gradient Norm after: 11.254344293583836
Epoch 7077/10000, Prediction Accuracy = 62.624%, Loss = 0.40663090348243713
Epoch: 7077, Batch Gradient Norm: 9.356528636001439
Epoch: 7077, Batch Gradient Norm after: 9.356528636001439
Epoch 7078/10000, Prediction Accuracy = 62.426%, Loss = 0.395389711856842
Epoch: 7078, Batch Gradient Norm: 9.086567228223657
Epoch: 7078, Batch Gradient Norm after: 9.086567228223657
Epoch 7079/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.3940364599227905
Epoch: 7079, Batch Gradient Norm: 11.497031398949034
Epoch: 7079, Batch Gradient Norm after: 11.497031398949034
Epoch 7080/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.41135255694389344
Epoch: 7080, Batch Gradient Norm: 9.208721773280573
Epoch: 7080, Batch Gradient Norm after: 9.208721773280573
Epoch 7081/10000, Prediction Accuracy = 62.488000000000014%, Loss = 0.39677647948265077
Epoch: 7081, Batch Gradient Norm: 10.826533955456156
Epoch: 7081, Batch Gradient Norm after: 10.826533955456156
Epoch 7082/10000, Prediction Accuracy = 62.538%, Loss = 0.40166108012199403
Epoch: 7082, Batch Gradient Norm: 11.12380399396876
Epoch: 7082, Batch Gradient Norm after: 11.12380399396876
Epoch 7083/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4085634112358093
Epoch: 7083, Batch Gradient Norm: 9.462828137021784
Epoch: 7083, Batch Gradient Norm after: 9.462828137021784
Epoch 7084/10000, Prediction Accuracy = 62.62199999999999%, Loss = 0.3990545034408569
Epoch: 7084, Batch Gradient Norm: 7.637176845592357
Epoch: 7084, Batch Gradient Norm after: 7.637176845592357
Epoch 7085/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.388956755399704
Epoch: 7085, Batch Gradient Norm: 9.0408783400992
Epoch: 7085, Batch Gradient Norm after: 9.0408783400992
Epoch 7086/10000, Prediction Accuracy = 62.762%, Loss = 0.3983712673187256
Epoch: 7086, Batch Gradient Norm: 7.6560387190717005
Epoch: 7086, Batch Gradient Norm after: 7.6560387190717005
Epoch 7087/10000, Prediction Accuracy = 62.52%, Loss = 0.38749725818634034
Epoch: 7087, Batch Gradient Norm: 9.118201194526213
Epoch: 7087, Batch Gradient Norm after: 9.118201194526213
Epoch 7088/10000, Prediction Accuracy = 62.71%, Loss = 0.3926310777664185
Epoch: 7088, Batch Gradient Norm: 12.735169305416655
Epoch: 7088, Batch Gradient Norm after: 12.735169305416655
Epoch 7089/10000, Prediction Accuracy = 62.75%, Loss = 0.4252242922782898
Epoch: 7089, Batch Gradient Norm: 12.200321589359964
Epoch: 7089, Batch Gradient Norm after: 12.200321589359964
Epoch 7090/10000, Prediction Accuracy = 62.572%, Loss = 0.41708874702453613
Epoch: 7090, Batch Gradient Norm: 9.484167577770089
Epoch: 7090, Batch Gradient Norm after: 9.484167577770089
Epoch 7091/10000, Prediction Accuracy = 62.652%, Loss = 0.3948931753635406
Epoch: 7091, Batch Gradient Norm: 7.344442714690919
Epoch: 7091, Batch Gradient Norm after: 7.344442714690919
Epoch 7092/10000, Prediction Accuracy = 62.806%, Loss = 0.3821596324443817
Epoch: 7092, Batch Gradient Norm: 9.004218401058951
Epoch: 7092, Batch Gradient Norm after: 9.004218401058951
Epoch 7093/10000, Prediction Accuracy = 62.678%, Loss = 0.3914494693279266
Epoch: 7093, Batch Gradient Norm: 10.66204853844129
Epoch: 7093, Batch Gradient Norm after: 10.66204853844129
Epoch 7094/10000, Prediction Accuracy = 62.669999999999995%, Loss = 0.40649041533470154
Epoch: 7094, Batch Gradient Norm: 10.249659221633864
Epoch: 7094, Batch Gradient Norm after: 10.249659221633864
Epoch 7095/10000, Prediction Accuracy = 62.674%, Loss = 0.4048212647438049
Epoch: 7095, Batch Gradient Norm: 10.615481633702796
Epoch: 7095, Batch Gradient Norm after: 10.615481633702796
Epoch 7096/10000, Prediction Accuracy = 62.762%, Loss = 0.40469778776168824
Epoch: 7096, Batch Gradient Norm: 8.537385117758054
Epoch: 7096, Batch Gradient Norm after: 8.537385117758054
Epoch 7097/10000, Prediction Accuracy = 62.634%, Loss = 0.3887097656726837
Epoch: 7097, Batch Gradient Norm: 8.698589608136198
Epoch: 7097, Batch Gradient Norm after: 8.698589608136198
Epoch 7098/10000, Prediction Accuracy = 62.58%, Loss = 0.3916510701179504
Epoch: 7098, Batch Gradient Norm: 8.778784693784308
Epoch: 7098, Batch Gradient Norm after: 8.778784693784308
Epoch 7099/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.392275732755661
Epoch: 7099, Batch Gradient Norm: 9.962013808964436
Epoch: 7099, Batch Gradient Norm after: 9.962013808964436
Epoch 7100/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.39918196201324463
Epoch: 7100, Batch Gradient Norm: 9.771230479087132
Epoch: 7100, Batch Gradient Norm after: 9.771230479087132
Epoch 7101/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.39785184264183043
Epoch: 7101, Batch Gradient Norm: 11.04447805030652
Epoch: 7101, Batch Gradient Norm after: 11.04447805030652
Epoch 7102/10000, Prediction Accuracy = 62.65999999999999%, Loss = 0.4062708139419556
Epoch: 7102, Batch Gradient Norm: 10.542084413894397
Epoch: 7102, Batch Gradient Norm after: 10.542084413894397
Epoch 7103/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.40404348373413085
Epoch: 7103, Batch Gradient Norm: 9.469707983014874
Epoch: 7103, Batch Gradient Norm after: 9.469707983014874
Epoch 7104/10000, Prediction Accuracy = 62.778%, Loss = 0.39251271486282346
Epoch: 7104, Batch Gradient Norm: 8.942143227168469
Epoch: 7104, Batch Gradient Norm after: 8.942143227168469
Epoch 7105/10000, Prediction Accuracy = 62.688%, Loss = 0.3923443615436554
Epoch: 7105, Batch Gradient Norm: 6.343435261549777
Epoch: 7105, Batch Gradient Norm after: 6.343435261549777
Epoch 7106/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.3789796233177185
Epoch: 7106, Batch Gradient Norm: 9.087789123663626
Epoch: 7106, Batch Gradient Norm after: 9.087789123663626
Epoch 7107/10000, Prediction Accuracy = 62.706%, Loss = 0.3970924437046051
Epoch: 7107, Batch Gradient Norm: 10.89785659527283
Epoch: 7107, Batch Gradient Norm after: 10.89785659527283
Epoch 7108/10000, Prediction Accuracy = 62.55%, Loss = 0.4059754192829132
Epoch: 7108, Batch Gradient Norm: 9.102013137299465
Epoch: 7108, Batch Gradient Norm after: 9.102013137299465
Epoch 7109/10000, Prediction Accuracy = 62.876%, Loss = 0.3940801203250885
Epoch: 7109, Batch Gradient Norm: 9.324488608391672
Epoch: 7109, Batch Gradient Norm after: 9.324488608391672
Epoch 7110/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.39166547656059264
Epoch: 7110, Batch Gradient Norm: 8.388220004486072
Epoch: 7110, Batch Gradient Norm after: 8.388220004486072
Epoch 7111/10000, Prediction Accuracy = 62.742%, Loss = 0.38589749336242674
Epoch: 7111, Batch Gradient Norm: 12.09281218168823
Epoch: 7111, Batch Gradient Norm after: 12.09281218168823
Epoch 7112/10000, Prediction Accuracy = 62.798%, Loss = 0.41482725739479065
Epoch: 7112, Batch Gradient Norm: 7.811109741949706
Epoch: 7112, Batch Gradient Norm after: 7.811109741949706
Epoch 7113/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.38460217118263246
Epoch: 7113, Batch Gradient Norm: 10.728037826769059
Epoch: 7113, Batch Gradient Norm after: 10.728037826769059
Epoch 7114/10000, Prediction Accuracy = 62.55%, Loss = 0.4062375247478485
Epoch: 7114, Batch Gradient Norm: 7.839494043129242
Epoch: 7114, Batch Gradient Norm after: 7.839494043129242
Epoch 7115/10000, Prediction Accuracy = 62.774%, Loss = 0.38582044243812563
Epoch: 7115, Batch Gradient Norm: 10.516624484967517
Epoch: 7115, Batch Gradient Norm after: 10.516624484967517
Epoch 7116/10000, Prediction Accuracy = 62.798%, Loss = 0.4001745700836182
Epoch: 7116, Batch Gradient Norm: 10.623148947046776
Epoch: 7116, Batch Gradient Norm after: 10.623148947046776
Epoch 7117/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4054131329059601
Epoch: 7117, Batch Gradient Norm: 9.949800675104914
Epoch: 7117, Batch Gradient Norm after: 9.949800675104914
Epoch 7118/10000, Prediction Accuracy = 62.548%, Loss = 0.401998245716095
Epoch: 7118, Batch Gradient Norm: 7.684994525827822
Epoch: 7118, Batch Gradient Norm after: 7.684994525827822
Epoch 7119/10000, Prediction Accuracy = 62.722%, Loss = 0.3849266767501831
Epoch: 7119, Batch Gradient Norm: 8.213522099882146
Epoch: 7119, Batch Gradient Norm after: 8.213522099882146
Epoch 7120/10000, Prediction Accuracy = 62.722%, Loss = 0.3871526837348938
Epoch: 7120, Batch Gradient Norm: 10.31434767194253
Epoch: 7120, Batch Gradient Norm after: 10.31434767194253
Epoch 7121/10000, Prediction Accuracy = 62.67%, Loss = 0.40210548639297483
Epoch: 7121, Batch Gradient Norm: 7.865120578682207
Epoch: 7121, Batch Gradient Norm after: 7.865120578682207
Epoch 7122/10000, Prediction Accuracy = 62.896%, Loss = 0.384669816493988
Epoch: 7122, Batch Gradient Norm: 9.645586863940554
Epoch: 7122, Batch Gradient Norm after: 9.645586863940554
Epoch 7123/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.3946925699710846
Epoch: 7123, Batch Gradient Norm: 12.055749534630811
Epoch: 7123, Batch Gradient Norm after: 12.055749534630811
Epoch 7124/10000, Prediction Accuracy = 62.604%, Loss = 0.41607285737991334
Epoch: 7124, Batch Gradient Norm: 11.375622008804042
Epoch: 7124, Batch Gradient Norm after: 11.375622008804042
Epoch 7125/10000, Prediction Accuracy = 62.65%, Loss = 0.40852208733558654
Epoch: 7125, Batch Gradient Norm: 8.991208542876546
Epoch: 7125, Batch Gradient Norm after: 8.991208542876546
Epoch 7126/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3929857134819031
Epoch: 7126, Batch Gradient Norm: 10.04211706480061
Epoch: 7126, Batch Gradient Norm after: 10.04211706480061
Epoch 7127/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.40265302658081054
Epoch: 7127, Batch Gradient Norm: 9.329000110658939
Epoch: 7127, Batch Gradient Norm after: 9.329000110658939
Epoch 7128/10000, Prediction Accuracy = 62.738000000000014%, Loss = 0.39478726387023927
Epoch: 7128, Batch Gradient Norm: 9.376074536662623
Epoch: 7128, Batch Gradient Norm after: 9.376074536662623
Epoch 7129/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.39500654935836793
Epoch: 7129, Batch Gradient Norm: 6.357456107973518
Epoch: 7129, Batch Gradient Norm after: 6.357456107973518
Epoch 7130/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.3759173035621643
Epoch: 7130, Batch Gradient Norm: 10.00173798547259
Epoch: 7130, Batch Gradient Norm after: 10.00173798547259
Epoch 7131/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3976775884628296
Epoch: 7131, Batch Gradient Norm: 14.601998121077045
Epoch: 7131, Batch Gradient Norm after: 14.601998121077045
Epoch 7132/10000, Prediction Accuracy = 62.714%, Loss = 0.4359898567199707
Epoch: 7132, Batch Gradient Norm: 11.225317342619881
Epoch: 7132, Batch Gradient Norm after: 11.225317342619881
Epoch 7133/10000, Prediction Accuracy = 62.61%, Loss = 0.40724555850028993
Epoch: 7133, Batch Gradient Norm: 9.145872842716123
Epoch: 7133, Batch Gradient Norm after: 9.145872842716123
Epoch 7134/10000, Prediction Accuracy = 62.75%, Loss = 0.3939984977245331
Epoch: 7134, Batch Gradient Norm: 7.556389595113928
Epoch: 7134, Batch Gradient Norm after: 7.556389595113928
Epoch 7135/10000, Prediction Accuracy = 62.702%, Loss = 0.38710678219795225
Epoch: 7135, Batch Gradient Norm: 5.951195298624121
Epoch: 7135, Batch Gradient Norm after: 5.951195298624121
Epoch 7136/10000, Prediction Accuracy = 62.778%, Loss = 0.3759970784187317
Epoch: 7136, Batch Gradient Norm: 10.344426128710085
Epoch: 7136, Batch Gradient Norm after: 10.344426128710085
Epoch 7137/10000, Prediction Accuracy = 62.624%, Loss = 0.40451117753982546
Epoch: 7137, Batch Gradient Norm: 9.046113837922684
Epoch: 7137, Batch Gradient Norm after: 9.046113837922684
Epoch 7138/10000, Prediction Accuracy = 62.664%, Loss = 0.3942586600780487
Epoch: 7138, Batch Gradient Norm: 11.850502895449905
Epoch: 7138, Batch Gradient Norm after: 11.850502895449905
Epoch 7139/10000, Prediction Accuracy = 62.722%, Loss = 0.41642128825187685
Epoch: 7139, Batch Gradient Norm: 11.860376333949514
Epoch: 7139, Batch Gradient Norm after: 11.860376333949514
Epoch 7140/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.4134685814380646
Epoch: 7140, Batch Gradient Norm: 8.527636113084153
Epoch: 7140, Batch Gradient Norm after: 8.527636113084153
Epoch 7141/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.3894091010093689
Epoch: 7141, Batch Gradient Norm: 9.378776121215708
Epoch: 7141, Batch Gradient Norm after: 9.378776121215708
Epoch 7142/10000, Prediction Accuracy = 62.6%, Loss = 0.39544802308082583
Epoch: 7142, Batch Gradient Norm: 8.8998157157801
Epoch: 7142, Batch Gradient Norm after: 8.8998157157801
Epoch 7143/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.39221715927124023
Epoch: 7143, Batch Gradient Norm: 9.4333530483342
Epoch: 7143, Batch Gradient Norm after: 9.4333530483342
Epoch 7144/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.3946682333946228
Epoch: 7144, Batch Gradient Norm: 9.633563192286044
Epoch: 7144, Batch Gradient Norm after: 9.633563192286044
Epoch 7145/10000, Prediction Accuracy = 62.58800000000001%, Loss = 0.39516415596008303
Epoch: 7145, Batch Gradient Norm: 9.685107532603698
Epoch: 7145, Batch Gradient Norm after: 9.685107532603698
Epoch 7146/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.3965153217315674
Epoch: 7146, Batch Gradient Norm: 10.553595358781175
Epoch: 7146, Batch Gradient Norm after: 10.553595358781175
Epoch 7147/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.40204575657844543
Epoch: 7147, Batch Gradient Norm: 11.419608236592296
Epoch: 7147, Batch Gradient Norm after: 11.419608236592296
Epoch 7148/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.4082535684108734
Epoch: 7148, Batch Gradient Norm: 10.194417367464494
Epoch: 7148, Batch Gradient Norm after: 10.194417367464494
Epoch 7149/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.3995257318019867
Epoch: 7149, Batch Gradient Norm: 8.499869709644338
Epoch: 7149, Batch Gradient Norm after: 8.499869709644338
Epoch 7150/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.38785913586616516
Epoch: 7150, Batch Gradient Norm: 8.852327802833084
Epoch: 7150, Batch Gradient Norm after: 8.852327802833084
Epoch 7151/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.3900554060935974
Epoch: 7151, Batch Gradient Norm: 10.375317331696078
Epoch: 7151, Batch Gradient Norm after: 10.375317331696078
Epoch 7152/10000, Prediction Accuracy = 62.614%, Loss = 0.40274863243103026
Epoch: 7152, Batch Gradient Norm: 11.357508430746863
Epoch: 7152, Batch Gradient Norm after: 11.357508430746863
Epoch 7153/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.4102390170097351
Epoch: 7153, Batch Gradient Norm: 10.83998815953784
Epoch: 7153, Batch Gradient Norm after: 10.83998815953784
Epoch 7154/10000, Prediction Accuracy = 62.75%, Loss = 0.4067661285400391
Epoch: 7154, Batch Gradient Norm: 11.753788951986644
Epoch: 7154, Batch Gradient Norm after: 11.753788951986644
Epoch 7155/10000, Prediction Accuracy = 62.75%, Loss = 0.41453683376312256
Epoch: 7155, Batch Gradient Norm: 10.314623475517703
Epoch: 7155, Batch Gradient Norm after: 10.314623475517703
Epoch 7156/10000, Prediction Accuracy = 62.516%, Loss = 0.4018147110939026
Epoch: 7156, Batch Gradient Norm: 9.00390278412671
Epoch: 7156, Batch Gradient Norm after: 9.00390278412671
Epoch 7157/10000, Prediction Accuracy = 62.624%, Loss = 0.3925373911857605
Epoch: 7157, Batch Gradient Norm: 10.63378977196867
Epoch: 7157, Batch Gradient Norm after: 10.63378977196867
Epoch 7158/10000, Prediction Accuracy = 62.81%, Loss = 0.40055586099624635
Epoch: 7158, Batch Gradient Norm: 8.280969092925568
Epoch: 7158, Batch Gradient Norm after: 8.280969092925568
Epoch 7159/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.3900512158870697
Epoch: 7159, Batch Gradient Norm: 11.798644117254081
Epoch: 7159, Batch Gradient Norm after: 11.798644117254081
Epoch 7160/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.4190911054611206
Epoch: 7160, Batch Gradient Norm: 5.7792565550283745
Epoch: 7160, Batch Gradient Norm after: 5.7792565550283745
Epoch 7161/10000, Prediction Accuracy = 62.79%, Loss = 0.3753556251525879
Epoch: 7161, Batch Gradient Norm: 9.672869753565214
Epoch: 7161, Batch Gradient Norm after: 9.672869753565214
Epoch 7162/10000, Prediction Accuracy = 62.842%, Loss = 0.3944423794746399
Epoch: 7162, Batch Gradient Norm: 9.373896530020101
Epoch: 7162, Batch Gradient Norm after: 9.373896530020101
Epoch 7163/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.3913662195205688
Epoch: 7163, Batch Gradient Norm: 10.500532953271108
Epoch: 7163, Batch Gradient Norm after: 10.500532953271108
Epoch 7164/10000, Prediction Accuracy = 62.73%, Loss = 0.3996337831020355
Epoch: 7164, Batch Gradient Norm: 10.692542790323449
Epoch: 7164, Batch Gradient Norm after: 10.692542790323449
Epoch 7165/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.4030996561050415
Epoch: 7165, Batch Gradient Norm: 10.624219636629666
Epoch: 7165, Batch Gradient Norm after: 10.624219636629666
Epoch 7166/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.404655385017395
Epoch: 7166, Batch Gradient Norm: 9.327270338609829
Epoch: 7166, Batch Gradient Norm after: 9.327270338609829
Epoch 7167/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.3940182089805603
Epoch: 7167, Batch Gradient Norm: 7.411278731092852
Epoch: 7167, Batch Gradient Norm after: 7.411278731092852
Epoch 7168/10000, Prediction Accuracy = 62.658%, Loss = 0.38145740032196046
Epoch: 7168, Batch Gradient Norm: 7.3676473642353875
Epoch: 7168, Batch Gradient Norm after: 7.3676473642353875
Epoch 7169/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.38069489002227785
Epoch: 7169, Batch Gradient Norm: 9.900853829930673
Epoch: 7169, Batch Gradient Norm after: 9.900853829930673
Epoch 7170/10000, Prediction Accuracy = 62.488%, Loss = 0.39616334438323975
Epoch: 7170, Batch Gradient Norm: 13.425781176062932
Epoch: 7170, Batch Gradient Norm after: 13.425781176062932
Epoch 7171/10000, Prediction Accuracy = 62.81%, Loss = 0.42248438000679017
Epoch: 7171, Batch Gradient Norm: 12.661543547835878
Epoch: 7171, Batch Gradient Norm after: 12.661543547835878
Epoch 7172/10000, Prediction Accuracy = 62.56600000000001%, Loss = 0.418856018781662
Epoch: 7172, Batch Gradient Norm: 8.77438536176389
Epoch: 7172, Batch Gradient Norm after: 8.77438536176389
Epoch 7173/10000, Prediction Accuracy = 62.762%, Loss = 0.39243119955062866
Epoch: 7173, Batch Gradient Norm: 7.200371674630233
Epoch: 7173, Batch Gradient Norm after: 7.200371674630233
Epoch 7174/10000, Prediction Accuracy = 62.694%, Loss = 0.3805523574352264
Epoch: 7174, Batch Gradient Norm: 10.588204067419175
Epoch: 7174, Batch Gradient Norm after: 10.588204067419175
Epoch 7175/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.4053705155849457
Epoch: 7175, Batch Gradient Norm: 8.031926912920525
Epoch: 7175, Batch Gradient Norm after: 8.031926912920525
Epoch 7176/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.38518944978713987
Epoch: 7176, Batch Gradient Norm: 7.4345136319816545
Epoch: 7176, Batch Gradient Norm after: 7.4345136319816545
Epoch 7177/10000, Prediction Accuracy = 62.702%, Loss = 0.38178168535232543
Epoch: 7177, Batch Gradient Norm: 11.336486591975985
Epoch: 7177, Batch Gradient Norm after: 11.336486591975985
Epoch 7178/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.41143034100532533
Epoch: 7178, Batch Gradient Norm: 10.409451650349737
Epoch: 7178, Batch Gradient Norm after: 10.409451650349737
Epoch 7179/10000, Prediction Accuracy = 62.686%, Loss = 0.40118408799171446
Epoch: 7179, Batch Gradient Norm: 10.76772435818553
Epoch: 7179, Batch Gradient Norm after: 10.76772435818553
Epoch 7180/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.4065946161746979
Epoch: 7180, Batch Gradient Norm: 9.325975799046528
Epoch: 7180, Batch Gradient Norm after: 9.325975799046528
Epoch 7181/10000, Prediction Accuracy = 62.709999999999994%, Loss = 0.3963181138038635
Epoch: 7181, Batch Gradient Norm: 11.109264093798922
Epoch: 7181, Batch Gradient Norm after: 11.109264093798922
Epoch 7182/10000, Prediction Accuracy = 62.688%, Loss = 0.40598254799842837
Epoch: 7182, Batch Gradient Norm: 10.144512152287115
Epoch: 7182, Batch Gradient Norm after: 10.144512152287115
Epoch 7183/10000, Prediction Accuracy = 62.77%, Loss = 0.3988707423210144
Epoch: 7183, Batch Gradient Norm: 8.280500704573102
Epoch: 7183, Batch Gradient Norm after: 8.280500704573102
Epoch 7184/10000, Prediction Accuracy = 62.648%, Loss = 0.3852401256561279
Epoch: 7184, Batch Gradient Norm: 8.366089157054443
Epoch: 7184, Batch Gradient Norm after: 8.366089157054443
Epoch 7185/10000, Prediction Accuracy = 62.574%, Loss = 0.38641977310180664
Epoch: 7185, Batch Gradient Norm: 8.569624896713607
Epoch: 7185, Batch Gradient Norm after: 8.569624896713607
Epoch 7186/10000, Prediction Accuracy = 62.676%, Loss = 0.39299713373184203
Epoch: 7186, Batch Gradient Norm: 9.516618291468008
Epoch: 7186, Batch Gradient Norm after: 9.516618291468008
Epoch 7187/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.3942162334918976
Epoch: 7187, Batch Gradient Norm: 11.618673390166395
Epoch: 7187, Batch Gradient Norm after: 11.618673390166395
Epoch 7188/10000, Prediction Accuracy = 62.646%, Loss = 0.4106287479400635
Epoch: 7188, Batch Gradient Norm: 9.603759997503525
Epoch: 7188, Batch Gradient Norm after: 9.603759997503525
Epoch 7189/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.3960555672645569
Epoch: 7189, Batch Gradient Norm: 7.8525828452048225
Epoch: 7189, Batch Gradient Norm after: 7.8525828452048225
Epoch 7190/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.3824941873550415
Epoch: 7190, Batch Gradient Norm: 9.860504151998581
Epoch: 7190, Batch Gradient Norm after: 9.860504151998581
Epoch 7191/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.4004639804363251
Epoch: 7191, Batch Gradient Norm: 8.67116160071758
Epoch: 7191, Batch Gradient Norm after: 8.67116160071758
Epoch 7192/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.38952853083610534
Epoch: 7192, Batch Gradient Norm: 9.817990730928537
Epoch: 7192, Batch Gradient Norm after: 9.817990730928537
Epoch 7193/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.39384270906448365
Epoch: 7193, Batch Gradient Norm: 10.623134215181802
Epoch: 7193, Batch Gradient Norm after: 10.623134215181802
Epoch 7194/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.4054608106613159
Epoch: 7194, Batch Gradient Norm: 9.724332465856262
Epoch: 7194, Batch Gradient Norm after: 9.724332465856262
Epoch 7195/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.39256447553634644
Epoch: 7195, Batch Gradient Norm: 9.515131055910228
Epoch: 7195, Batch Gradient Norm after: 9.515131055910228
Epoch 7196/10000, Prediction Accuracy = 62.638%, Loss = 0.3978037774562836
Epoch: 7196, Batch Gradient Norm: 10.328981365478626
Epoch: 7196, Batch Gradient Norm after: 10.328981365478626
Epoch 7197/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.40366547703742983
Epoch: 7197, Batch Gradient Norm: 10.807001961436134
Epoch: 7197, Batch Gradient Norm after: 10.807001961436134
Epoch 7198/10000, Prediction Accuracy = 62.766%, Loss = 0.40363398790359495
Epoch: 7198, Batch Gradient Norm: 9.848853595259964
Epoch: 7198, Batch Gradient Norm after: 9.848853595259964
Epoch 7199/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.3950518608093262
Epoch: 7199, Batch Gradient Norm: 9.500490330742537
Epoch: 7199, Batch Gradient Norm after: 9.500490330742537
Epoch 7200/10000, Prediction Accuracy = 62.576%, Loss = 0.3987585186958313
Epoch: 7200, Batch Gradient Norm: 8.418999679669813
Epoch: 7200, Batch Gradient Norm after: 8.418999679669813
Epoch 7201/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.3879288673400879
Epoch: 7201, Batch Gradient Norm: 10.648877134263191
Epoch: 7201, Batch Gradient Norm after: 10.648877134263191
Epoch 7202/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.4054377794265747
Epoch: 7202, Batch Gradient Norm: 8.720957167416096
Epoch: 7202, Batch Gradient Norm after: 8.720957167416096
Epoch 7203/10000, Prediction Accuracy = 62.778%, Loss = 0.3888435363769531
Epoch: 7203, Batch Gradient Norm: 9.258629750913892
Epoch: 7203, Batch Gradient Norm after: 9.258629750913892
Epoch 7204/10000, Prediction Accuracy = 62.68000000000001%, Loss = 0.394292414188385
Epoch: 7204, Batch Gradient Norm: 12.953172664861432
Epoch: 7204, Batch Gradient Norm after: 12.953172664861432
Epoch 7205/10000, Prediction Accuracy = 62.79599999999999%, Loss = 0.42166341543197633
Epoch: 7205, Batch Gradient Norm: 9.004953321786898
Epoch: 7205, Batch Gradient Norm after: 9.004953321786898
Epoch 7206/10000, Prediction Accuracy = 62.682%, Loss = 0.3913413524627686
Epoch: 7206, Batch Gradient Norm: 9.421992427268293
Epoch: 7206, Batch Gradient Norm after: 9.421992427268293
Epoch 7207/10000, Prediction Accuracy = 62.63399999999999%, Loss = 0.395251202583313
Epoch: 7207, Batch Gradient Norm: 10.195230415026902
Epoch: 7207, Batch Gradient Norm after: 10.195230415026902
Epoch 7208/10000, Prediction Accuracy = 62.666%, Loss = 0.3969843327999115
Epoch: 7208, Batch Gradient Norm: 8.782931613574405
Epoch: 7208, Batch Gradient Norm after: 8.782931613574405
Epoch 7209/10000, Prediction Accuracy = 62.524%, Loss = 0.38987950682640077
Epoch: 7209, Batch Gradient Norm: 9.047381434830807
Epoch: 7209, Batch Gradient Norm after: 9.047381434830807
Epoch 7210/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.39615404009819033
Epoch: 7210, Batch Gradient Norm: 7.773850388756097
Epoch: 7210, Batch Gradient Norm after: 7.773850388756097
Epoch 7211/10000, Prediction Accuracy = 62.722%, Loss = 0.38302289247512816
Epoch: 7211, Batch Gradient Norm: 9.291161822296678
Epoch: 7211, Batch Gradient Norm after: 9.291161822296678
Epoch 7212/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.39148293137550355
Epoch: 7212, Batch Gradient Norm: 9.376945892421835
Epoch: 7212, Batch Gradient Norm after: 9.376945892421835
Epoch 7213/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.39351788759231565
Epoch: 7213, Batch Gradient Norm: 12.823452844023155
Epoch: 7213, Batch Gradient Norm after: 12.823452844023155
Epoch 7214/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.41717785596847534
Epoch: 7214, Batch Gradient Norm: 11.693671421380833
Epoch: 7214, Batch Gradient Norm after: 11.693671421380833
Epoch 7215/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4107374966144562
Epoch: 7215, Batch Gradient Norm: 11.11619101552848
Epoch: 7215, Batch Gradient Norm after: 11.11619101552848
Epoch 7216/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.40855193734169004
Epoch: 7216, Batch Gradient Norm: 12.47299017589925
Epoch: 7216, Batch Gradient Norm after: 12.47299017589925
Epoch 7217/10000, Prediction Accuracy = 62.586%, Loss = 0.427765029668808
Epoch: 7217, Batch Gradient Norm: 7.592339683176578
Epoch: 7217, Batch Gradient Norm after: 7.592339683176578
Epoch 7218/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3834316372871399
Epoch: 7218, Batch Gradient Norm: 7.934891044371941
Epoch: 7218, Batch Gradient Norm after: 7.934891044371941
Epoch 7219/10000, Prediction Accuracy = 62.738000000000014%, Loss = 0.38338558077812196
Epoch: 7219, Batch Gradient Norm: 8.340994256153163
Epoch: 7219, Batch Gradient Norm after: 8.340994256153163
Epoch 7220/10000, Prediction Accuracy = 62.730000000000004%, Loss = 0.3843392789363861
Epoch: 7220, Batch Gradient Norm: 7.791965279833436
Epoch: 7220, Batch Gradient Norm after: 7.791965279833436
Epoch 7221/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.38479686975479127
Epoch: 7221, Batch Gradient Norm: 9.782379969235285
Epoch: 7221, Batch Gradient Norm after: 9.782379969235285
Epoch 7222/10000, Prediction Accuracy = 62.746%, Loss = 0.3967909812927246
Epoch: 7222, Batch Gradient Norm: 8.686698466729633
Epoch: 7222, Batch Gradient Norm after: 8.686698466729633
Epoch 7223/10000, Prediction Accuracy = 62.89200000000001%, Loss = 0.3892228901386261
Epoch: 7223, Batch Gradient Norm: 9.870169160444565
Epoch: 7223, Batch Gradient Norm after: 9.870169160444565
Epoch 7224/10000, Prediction Accuracy = 62.694%, Loss = 0.39868679642677307
Epoch: 7224, Batch Gradient Norm: 8.377160727700238
Epoch: 7224, Batch Gradient Norm after: 8.377160727700238
Epoch 7225/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.38653035163879396
Epoch: 7225, Batch Gradient Norm: 7.717185071106278
Epoch: 7225, Batch Gradient Norm after: 7.717185071106278
Epoch 7226/10000, Prediction Accuracy = 62.588%, Loss = 0.3836211085319519
Epoch: 7226, Batch Gradient Norm: 9.318243308968386
Epoch: 7226, Batch Gradient Norm after: 9.318243308968386
Epoch 7227/10000, Prediction Accuracy = 62.66600000000001%, Loss = 0.39187716841697695
Epoch: 7227, Batch Gradient Norm: 12.4161931478624
Epoch: 7227, Batch Gradient Norm after: 12.4161931478624
Epoch 7228/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.41669681668281555
Epoch: 7228, Batch Gradient Norm: 10.818422711669264
Epoch: 7228, Batch Gradient Norm after: 10.818422711669264
Epoch 7229/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.4061442673206329
Epoch: 7229, Batch Gradient Norm: 9.849206848635097
Epoch: 7229, Batch Gradient Norm after: 9.849206848635097
Epoch 7230/10000, Prediction Accuracy = 62.758%, Loss = 0.39634166955947875
Epoch: 7230, Batch Gradient Norm: 9.407475293734205
Epoch: 7230, Batch Gradient Norm after: 9.407475293734205
Epoch 7231/10000, Prediction Accuracy = 62.67%, Loss = 0.3931929230690002
Epoch: 7231, Batch Gradient Norm: 7.715504249084789
Epoch: 7231, Batch Gradient Norm after: 7.715504249084789
Epoch 7232/10000, Prediction Accuracy = 62.936%, Loss = 0.3834823429584503
Epoch: 7232, Batch Gradient Norm: 9.782081133254112
Epoch: 7232, Batch Gradient Norm after: 9.782081133254112
Epoch 7233/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.3979697167873383
Epoch: 7233, Batch Gradient Norm: 8.741662035877038
Epoch: 7233, Batch Gradient Norm after: 8.741662035877038
Epoch 7234/10000, Prediction Accuracy = 62.77%, Loss = 0.38929983973503113
Epoch: 7234, Batch Gradient Norm: 11.344849353964406
Epoch: 7234, Batch Gradient Norm after: 11.344849353964406
Epoch 7235/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.40378324389457704
Epoch: 7235, Batch Gradient Norm: 10.641602567541986
Epoch: 7235, Batch Gradient Norm after: 10.641602567541986
Epoch 7236/10000, Prediction Accuracy = 62.69000000000001%, Loss = 0.4027892589569092
Epoch: 7236, Batch Gradient Norm: 11.904729903210827
Epoch: 7236, Batch Gradient Norm after: 11.904729903210827
Epoch 7237/10000, Prediction Accuracy = 62.612%, Loss = 0.4101355493068695
Epoch: 7237, Batch Gradient Norm: 9.676423973470047
Epoch: 7237, Batch Gradient Norm after: 9.676423973470047
Epoch 7238/10000, Prediction Accuracy = 62.767999999999994%, Loss = 0.3941772639751434
Epoch: 7238, Batch Gradient Norm: 8.286472092640533
Epoch: 7238, Batch Gradient Norm after: 8.286472092640533
Epoch 7239/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.384903746843338
Epoch: 7239, Batch Gradient Norm: 8.972023141614441
Epoch: 7239, Batch Gradient Norm after: 8.972023141614441
Epoch 7240/10000, Prediction Accuracy = 62.738%, Loss = 0.3893046796321869
Epoch: 7240, Batch Gradient Norm: 9.589826724470619
Epoch: 7240, Batch Gradient Norm after: 9.589826724470619
Epoch 7241/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.3949763000011444
Epoch: 7241, Batch Gradient Norm: 8.357124947051211
Epoch: 7241, Batch Gradient Norm after: 8.357124947051211
Epoch 7242/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.38636078834533694
Epoch: 7242, Batch Gradient Norm: 11.081262083481576
Epoch: 7242, Batch Gradient Norm after: 11.081262083481576
Epoch 7243/10000, Prediction Accuracy = 62.698%, Loss = 0.4059638261795044
Epoch: 7243, Batch Gradient Norm: 9.00298673051077
Epoch: 7243, Batch Gradient Norm after: 9.00298673051077
Epoch 7244/10000, Prediction Accuracy = 62.85%, Loss = 0.3897890329360962
Epoch: 7244, Batch Gradient Norm: 10.241667189010041
Epoch: 7244, Batch Gradient Norm after: 10.241667189010041
Epoch 7245/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.396127724647522
Epoch: 7245, Batch Gradient Norm: 11.95173790243742
Epoch: 7245, Batch Gradient Norm after: 11.95173790243742
Epoch 7246/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.40741569399833677
Epoch: 7246, Batch Gradient Norm: 11.245510755810225
Epoch: 7246, Batch Gradient Norm after: 11.245510755810225
Epoch 7247/10000, Prediction Accuracy = 62.61%, Loss = 0.4087564468383789
Epoch: 7247, Batch Gradient Norm: 8.25487127546858
Epoch: 7247, Batch Gradient Norm after: 8.25487127546858
Epoch 7248/10000, Prediction Accuracy = 62.586%, Loss = 0.3862015664577484
Epoch: 7248, Batch Gradient Norm: 9.26024867404612
Epoch: 7248, Batch Gradient Norm after: 9.26024867404612
Epoch 7249/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3926766276359558
Epoch: 7249, Batch Gradient Norm: 10.192756605606013
Epoch: 7249, Batch Gradient Norm after: 10.192756605606013
Epoch 7250/10000, Prediction Accuracy = 62.798%, Loss = 0.4004656136035919
Epoch: 7250, Batch Gradient Norm: 8.810982288786132
Epoch: 7250, Batch Gradient Norm after: 8.810982288786132
Epoch 7251/10000, Prediction Accuracy = 62.886%, Loss = 0.38807912468910216
Epoch: 7251, Batch Gradient Norm: 10.229365672722727
Epoch: 7251, Batch Gradient Norm after: 10.229365672722727
Epoch 7252/10000, Prediction Accuracy = 62.908%, Loss = 0.3971058249473572
Epoch: 7252, Batch Gradient Norm: 10.314512576931902
Epoch: 7252, Batch Gradient Norm after: 10.314512576931902
Epoch 7253/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.4005524218082428
Epoch: 7253, Batch Gradient Norm: 8.28619951143367
Epoch: 7253, Batch Gradient Norm after: 8.28619951143367
Epoch 7254/10000, Prediction Accuracy = 62.698%, Loss = 0.38555343747138976
Epoch: 7254, Batch Gradient Norm: 7.333858331165421
Epoch: 7254, Batch Gradient Norm after: 7.333858331165421
Epoch 7255/10000, Prediction Accuracy = 62.932%, Loss = 0.379965615272522
Epoch: 7255, Batch Gradient Norm: 12.400803579674989
Epoch: 7255, Batch Gradient Norm after: 12.400803579674989
Epoch 7256/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.4123377323150635
Epoch: 7256, Batch Gradient Norm: 11.18889109444406
Epoch: 7256, Batch Gradient Norm after: 11.18889109444406
Epoch 7257/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.40623674988746644
Epoch: 7257, Batch Gradient Norm: 8.649917541715409
Epoch: 7257, Batch Gradient Norm after: 8.649917541715409
Epoch 7258/10000, Prediction Accuracy = 62.788%, Loss = 0.3871920883655548
Epoch: 7258, Batch Gradient Norm: 10.755258963969707
Epoch: 7258, Batch Gradient Norm after: 10.755258963969707
Epoch 7259/10000, Prediction Accuracy = 62.61%, Loss = 0.40283823013305664
Epoch: 7259, Batch Gradient Norm: 12.29003561401785
Epoch: 7259, Batch Gradient Norm after: 12.29003561401785
Epoch 7260/10000, Prediction Accuracy = 62.634%, Loss = 0.41678449511528015
Epoch: 7260, Batch Gradient Norm: 12.538856009511502
Epoch: 7260, Batch Gradient Norm after: 12.538856009511502
Epoch 7261/10000, Prediction Accuracy = 62.726%, Loss = 0.4245711982250214
Epoch: 7261, Batch Gradient Norm: 9.398776690970807
Epoch: 7261, Batch Gradient Norm after: 9.398776690970807
Epoch 7262/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.39442785978317263
Epoch: 7262, Batch Gradient Norm: 8.634595341824994
Epoch: 7262, Batch Gradient Norm after: 8.634595341824994
Epoch 7263/10000, Prediction Accuracy = 62.762%, Loss = 0.38814141154289244
Epoch: 7263, Batch Gradient Norm: 9.694787226393682
Epoch: 7263, Batch Gradient Norm after: 9.694787226393682
Epoch 7264/10000, Prediction Accuracy = 62.674%, Loss = 0.39652066230773925
Epoch: 7264, Batch Gradient Norm: 7.740631578466882
Epoch: 7264, Batch Gradient Norm after: 7.740631578466882
Epoch 7265/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.3817955732345581
Epoch: 7265, Batch Gradient Norm: 9.96406371784177
Epoch: 7265, Batch Gradient Norm after: 9.96406371784177
Epoch 7266/10000, Prediction Accuracy = 62.678%, Loss = 0.39603721499443056
Epoch: 7266, Batch Gradient Norm: 10.158307760835537
Epoch: 7266, Batch Gradient Norm after: 10.158307760835537
Epoch 7267/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.3956686854362488
Epoch: 7267, Batch Gradient Norm: 8.411110145976153
Epoch: 7267, Batch Gradient Norm after: 8.411110145976153
Epoch 7268/10000, Prediction Accuracy = 62.876%, Loss = 0.3859428524971008
Epoch: 7268, Batch Gradient Norm: 8.05888049536136
Epoch: 7268, Batch Gradient Norm after: 8.05888049536136
Epoch 7269/10000, Prediction Accuracy = 62.824%, Loss = 0.3846218943595886
Epoch: 7269, Batch Gradient Norm: 8.36520442949864
Epoch: 7269, Batch Gradient Norm after: 8.36520442949864
Epoch 7270/10000, Prediction Accuracy = 62.774%, Loss = 0.3899531185626984
Epoch: 7270, Batch Gradient Norm: 11.549874788177314
Epoch: 7270, Batch Gradient Norm after: 11.549874788177314
Epoch 7271/10000, Prediction Accuracy = 62.602%, Loss = 0.41071972250938416
Epoch: 7271, Batch Gradient Norm: 9.065304857079207
Epoch: 7271, Batch Gradient Norm after: 9.065304857079207
Epoch 7272/10000, Prediction Accuracy = 62.81%, Loss = 0.3913682341575623
Epoch: 7272, Batch Gradient Norm: 9.557894685862395
Epoch: 7272, Batch Gradient Norm after: 9.557894685862395
Epoch 7273/10000, Prediction Accuracy = 62.944%, Loss = 0.39257403612136843
Epoch: 7273, Batch Gradient Norm: 10.659943704243835
Epoch: 7273, Batch Gradient Norm after: 10.659943704243835
Epoch 7274/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.39863612651824953
Epoch: 7274, Batch Gradient Norm: 9.745635817425695
Epoch: 7274, Batch Gradient Norm after: 9.745635817425695
Epoch 7275/10000, Prediction Accuracy = 62.824%, Loss = 0.3929990649223328
Epoch: 7275, Batch Gradient Norm: 9.071837358752491
Epoch: 7275, Batch Gradient Norm after: 9.071837358752491
Epoch 7276/10000, Prediction Accuracy = 62.854%, Loss = 0.3917321443557739
Epoch: 7276, Batch Gradient Norm: 7.954795384485518
Epoch: 7276, Batch Gradient Norm after: 7.954795384485518
Epoch 7277/10000, Prediction Accuracy = 62.726%, Loss = 0.3849881887435913
Epoch: 7277, Batch Gradient Norm: 11.224945782632213
Epoch: 7277, Batch Gradient Norm after: 11.224945782632213
Epoch 7278/10000, Prediction Accuracy = 62.636%, Loss = 0.4056391894817352
Epoch: 7278, Batch Gradient Norm: 12.163136849228792
Epoch: 7278, Batch Gradient Norm after: 12.163136849228792
Epoch 7279/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.41283863186836245
Epoch: 7279, Batch Gradient Norm: 10.185461079620413
Epoch: 7279, Batch Gradient Norm after: 10.185461079620413
Epoch 7280/10000, Prediction Accuracy = 62.628%, Loss = 0.4018056273460388
Epoch: 7280, Batch Gradient Norm: 8.221435716891792
Epoch: 7280, Batch Gradient Norm after: 8.221435716891792
Epoch 7281/10000, Prediction Accuracy = 62.733999999999995%, Loss = 0.3853622555732727
Epoch: 7281, Batch Gradient Norm: 7.991047229099776
Epoch: 7281, Batch Gradient Norm after: 7.991047229099776
Epoch 7282/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.38346288204193113
Epoch: 7282, Batch Gradient Norm: 6.456549707561775
Epoch: 7282, Batch Gradient Norm after: 6.456549707561775
Epoch 7283/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3741171956062317
Epoch: 7283, Batch Gradient Norm: 10.426414989460394
Epoch: 7283, Batch Gradient Norm after: 10.426414989460394
Epoch 7284/10000, Prediction Accuracy = 62.73%, Loss = 0.39902831315994264
Epoch: 7284, Batch Gradient Norm: 10.413673105181509
Epoch: 7284, Batch Gradient Norm after: 10.413673105181509
Epoch 7285/10000, Prediction Accuracy = 62.726%, Loss = 0.39925384521484375
Epoch: 7285, Batch Gradient Norm: 8.328142899091597
Epoch: 7285, Batch Gradient Norm after: 8.328142899091597
Epoch 7286/10000, Prediction Accuracy = 62.67%, Loss = 0.3857888996601105
Epoch: 7286, Batch Gradient Norm: 9.692231684724538
Epoch: 7286, Batch Gradient Norm after: 9.692231684724538
Epoch 7287/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.39409799575805665
Epoch: 7287, Batch Gradient Norm: 12.623979382483622
Epoch: 7287, Batch Gradient Norm after: 12.623979382483622
Epoch 7288/10000, Prediction Accuracy = 62.86%, Loss = 0.4164933919906616
Epoch: 7288, Batch Gradient Norm: 10.151487593279224
Epoch: 7288, Batch Gradient Norm after: 10.151487593279224
Epoch 7289/10000, Prediction Accuracy = 62.788%, Loss = 0.3996960520744324
Epoch: 7289, Batch Gradient Norm: 7.4935227414572765
Epoch: 7289, Batch Gradient Norm after: 7.4935227414572765
Epoch 7290/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.38100485801696776
Epoch: 7290, Batch Gradient Norm: 12.573753629425902
Epoch: 7290, Batch Gradient Norm after: 12.573753629425902
Epoch 7291/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.41627917289733884
Epoch: 7291, Batch Gradient Norm: 9.449576221938333
Epoch: 7291, Batch Gradient Norm after: 9.449576221938333
Epoch 7292/10000, Prediction Accuracy = 62.8%, Loss = 0.3906747937202454
Epoch: 7292, Batch Gradient Norm: 7.587966164788411
Epoch: 7292, Batch Gradient Norm after: 7.587966164788411
Epoch 7293/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.3804779589176178
Epoch: 7293, Batch Gradient Norm: 8.508944116483256
Epoch: 7293, Batch Gradient Norm after: 8.508944116483256
Epoch 7294/10000, Prediction Accuracy = 62.734%, Loss = 0.3868552505970001
Epoch: 7294, Batch Gradient Norm: 8.143617711242598
Epoch: 7294, Batch Gradient Norm after: 8.143617711242598
Epoch 7295/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.38337684273719785
Epoch: 7295, Batch Gradient Norm: 9.504320808610581
Epoch: 7295, Batch Gradient Norm after: 9.504320808610581
Epoch 7296/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.39309134483337405
Epoch: 7296, Batch Gradient Norm: 9.998475685427394
Epoch: 7296, Batch Gradient Norm after: 9.998475685427394
Epoch 7297/10000, Prediction Accuracy = 62.738%, Loss = 0.39456156492233274
Epoch: 7297, Batch Gradient Norm: 11.196635959485143
Epoch: 7297, Batch Gradient Norm after: 11.196635959485143
Epoch 7298/10000, Prediction Accuracy = 62.812%, Loss = 0.4046731472015381
Epoch: 7298, Batch Gradient Norm: 8.430444605812255
Epoch: 7298, Batch Gradient Norm after: 8.430444605812255
Epoch 7299/10000, Prediction Accuracy = 62.688%, Loss = 0.3862269878387451
Epoch: 7299, Batch Gradient Norm: 10.40784194712554
Epoch: 7299, Batch Gradient Norm after: 10.40784194712554
Epoch 7300/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.40128905773162843
Epoch: 7300, Batch Gradient Norm: 9.749471811735342
Epoch: 7300, Batch Gradient Norm after: 9.749471811735342
Epoch 7301/10000, Prediction Accuracy = 62.674%, Loss = 0.3919304370880127
Epoch: 7301, Batch Gradient Norm: 11.131030678631923
Epoch: 7301, Batch Gradient Norm after: 11.131030678631923
Epoch 7302/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.4017388641834259
Epoch: 7302, Batch Gradient Norm: 9.367481462329097
Epoch: 7302, Batch Gradient Norm after: 9.367481462329097
Epoch 7303/10000, Prediction Accuracy = 62.774%, Loss = 0.39151848554611207
Epoch: 7303, Batch Gradient Norm: 6.87933895007003
Epoch: 7303, Batch Gradient Norm after: 6.87933895007003
Epoch 7304/10000, Prediction Accuracy = 62.646%, Loss = 0.3773283720016479
Epoch: 7304, Batch Gradient Norm: 8.523611492724923
Epoch: 7304, Batch Gradient Norm after: 8.523611492724923
Epoch 7305/10000, Prediction Accuracy = 62.63599999999999%, Loss = 0.38464216589927674
Epoch: 7305, Batch Gradient Norm: 12.970752115653893
Epoch: 7305, Batch Gradient Norm after: 12.970752115653893
Epoch 7306/10000, Prediction Accuracy = 62.71%, Loss = 0.41952069401741027
Epoch: 7306, Batch Gradient Norm: 10.828769838422764
Epoch: 7306, Batch Gradient Norm after: 10.828769838422764
Epoch 7307/10000, Prediction Accuracy = 62.65%, Loss = 0.4010898768901825
Epoch: 7307, Batch Gradient Norm: 9.858010474952085
Epoch: 7307, Batch Gradient Norm after: 9.858010474952085
Epoch 7308/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.39501983523368833
Epoch: 7308, Batch Gradient Norm: 11.625101589720574
Epoch: 7308, Batch Gradient Norm after: 11.625101589720574
Epoch 7309/10000, Prediction Accuracy = 62.58600000000001%, Loss = 0.40856035947799685
Epoch: 7309, Batch Gradient Norm: 10.46604796144017
Epoch: 7309, Batch Gradient Norm after: 10.46604796144017
Epoch 7310/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.39617257118225097
Epoch: 7310, Batch Gradient Norm: 8.376708787810527
Epoch: 7310, Batch Gradient Norm after: 8.376708787810527
Epoch 7311/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.3818432688713074
Epoch: 7311, Batch Gradient Norm: 10.3252943145565
Epoch: 7311, Batch Gradient Norm after: 10.3252943145565
Epoch 7312/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.39775157570838926
Epoch: 7312, Batch Gradient Norm: 10.110136209743311
Epoch: 7312, Batch Gradient Norm after: 10.110136209743311
Epoch 7313/10000, Prediction Accuracy = 62.81%, Loss = 0.39548776745796205
Epoch: 7313, Batch Gradient Norm: 9.00017399276341
Epoch: 7313, Batch Gradient Norm after: 9.00017399276341
Epoch 7314/10000, Prediction Accuracy = 62.822%, Loss = 0.38780555725097654
Epoch: 7314, Batch Gradient Norm: 6.928556475021449
Epoch: 7314, Batch Gradient Norm after: 6.928556475021449
Epoch 7315/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.37892624735832214
Epoch: 7315, Batch Gradient Norm: 8.364970129293976
Epoch: 7315, Batch Gradient Norm after: 8.364970129293976
Epoch 7316/10000, Prediction Accuracy = 62.798%, Loss = 0.38425150513648987
Epoch: 7316, Batch Gradient Norm: 12.803245795987149
Epoch: 7316, Batch Gradient Norm after: 12.803245795987149
Epoch 7317/10000, Prediction Accuracy = 62.592%, Loss = 0.41866127252578733
Epoch: 7317, Batch Gradient Norm: 9.597836897802347
Epoch: 7317, Batch Gradient Norm after: 9.597836897802347
Epoch 7318/10000, Prediction Accuracy = 62.562%, Loss = 0.39967008829116824
Epoch: 7318, Batch Gradient Norm: 7.985734271043365
Epoch: 7318, Batch Gradient Norm after: 7.985734271043365
Epoch 7319/10000, Prediction Accuracy = 62.895999999999994%, Loss = 0.3841018617153168
Epoch: 7319, Batch Gradient Norm: 8.228318372689657
Epoch: 7319, Batch Gradient Norm after: 8.228318372689657
Epoch 7320/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.3843476474285126
Epoch: 7320, Batch Gradient Norm: 6.88436681145444
Epoch: 7320, Batch Gradient Norm after: 6.88436681145444
Epoch 7321/10000, Prediction Accuracy = 62.907999999999994%, Loss = 0.3765342175960541
Epoch: 7321, Batch Gradient Norm: 8.198810738101583
Epoch: 7321, Batch Gradient Norm after: 8.198810738101583
Epoch 7322/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.38334022760391234
Epoch: 7322, Batch Gradient Norm: 12.713200455375992
Epoch: 7322, Batch Gradient Norm after: 12.713200455375992
Epoch 7323/10000, Prediction Accuracy = 62.624%, Loss = 0.42381551861763
Epoch: 7323, Batch Gradient Norm: 10.408069949771882
Epoch: 7323, Batch Gradient Norm after: 10.408069949771882
Epoch 7324/10000, Prediction Accuracy = 62.836%, Loss = 0.4003578782081604
Epoch: 7324, Batch Gradient Norm: 8.454875507397569
Epoch: 7324, Batch Gradient Norm after: 8.454875507397569
Epoch 7325/10000, Prediction Accuracy = 62.888%, Loss = 0.3857796013355255
Epoch: 7325, Batch Gradient Norm: 10.676518264467008
Epoch: 7325, Batch Gradient Norm after: 10.676518264467008
Epoch 7326/10000, Prediction Accuracy = 62.746%, Loss = 0.400774747133255
Epoch: 7326, Batch Gradient Norm: 8.72400200538091
Epoch: 7326, Batch Gradient Norm after: 8.72400200538091
Epoch 7327/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.3853778958320618
Epoch: 7327, Batch Gradient Norm: 10.988337480096972
Epoch: 7327, Batch Gradient Norm after: 10.988337480096972
Epoch 7328/10000, Prediction Accuracy = 62.774%, Loss = 0.40362608432769775
Epoch: 7328, Batch Gradient Norm: 10.641440179047136
Epoch: 7328, Batch Gradient Norm after: 10.641440179047136
Epoch 7329/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.40197615027427674
Epoch: 7329, Batch Gradient Norm: 10.418090419828472
Epoch: 7329, Batch Gradient Norm after: 10.418090419828472
Epoch 7330/10000, Prediction Accuracy = 62.678%, Loss = 0.39675437211990355
Epoch: 7330, Batch Gradient Norm: 10.321572982929721
Epoch: 7330, Batch Gradient Norm after: 10.321572982929721
Epoch 7331/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.39626761674880984
Epoch: 7331, Batch Gradient Norm: 10.05101488636749
Epoch: 7331, Batch Gradient Norm after: 10.05101488636749
Epoch 7332/10000, Prediction Accuracy = 62.83%, Loss = 0.3952127516269684
Epoch: 7332, Batch Gradient Norm: 10.720334986347952
Epoch: 7332, Batch Gradient Norm after: 10.720334986347952
Epoch 7333/10000, Prediction Accuracy = 62.75%, Loss = 0.398517370223999
Epoch: 7333, Batch Gradient Norm: 12.248947352638316
Epoch: 7333, Batch Gradient Norm after: 12.248947352638316
Epoch 7334/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.413484126329422
Epoch: 7334, Batch Gradient Norm: 7.759968209965469
Epoch: 7334, Batch Gradient Norm after: 7.759968209965469
Epoch 7335/10000, Prediction Accuracy = 62.862%, Loss = 0.37999250292778014
Epoch: 7335, Batch Gradient Norm: 7.931314594155694
Epoch: 7335, Batch Gradient Norm after: 7.931314594155694
Epoch 7336/10000, Prediction Accuracy = 62.626%, Loss = 0.37955300211906434
Epoch: 7336, Batch Gradient Norm: 9.98626279064097
Epoch: 7336, Batch Gradient Norm after: 9.98626279064097
Epoch 7337/10000, Prediction Accuracy = 62.824%, Loss = 0.3965133249759674
Epoch: 7337, Batch Gradient Norm: 9.345980617423349
Epoch: 7337, Batch Gradient Norm after: 9.345980617423349
Epoch 7338/10000, Prediction Accuracy = 62.79200000000001%, Loss = 0.3900953412055969
Epoch: 7338, Batch Gradient Norm: 10.65041922385225
Epoch: 7338, Batch Gradient Norm after: 10.65041922385225
Epoch 7339/10000, Prediction Accuracy = 62.79599999999999%, Loss = 0.40239074230194094
Epoch: 7339, Batch Gradient Norm: 9.329545940950013
Epoch: 7339, Batch Gradient Norm after: 9.329545940950013
Epoch 7340/10000, Prediction Accuracy = 62.814%, Loss = 0.39018112421035767
Epoch: 7340, Batch Gradient Norm: 6.6395772823442405
Epoch: 7340, Batch Gradient Norm after: 6.6395772823442405
Epoch 7341/10000, Prediction Accuracy = 62.838%, Loss = 0.37445285320281985
Epoch: 7341, Batch Gradient Norm: 8.80622095333113
Epoch: 7341, Batch Gradient Norm after: 8.80622095333113
Epoch 7342/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38605956435203553
Epoch: 7342, Batch Gradient Norm: 9.536008340421047
Epoch: 7342, Batch Gradient Norm after: 9.536008340421047
Epoch 7343/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.39319596290588377
Epoch: 7343, Batch Gradient Norm: 8.644533477765625
Epoch: 7343, Batch Gradient Norm after: 8.644533477765625
Epoch 7344/10000, Prediction Accuracy = 62.826%, Loss = 0.3859604179859161
Epoch: 7344, Batch Gradient Norm: 9.74642285351433
Epoch: 7344, Batch Gradient Norm after: 9.74642285351433
Epoch 7345/10000, Prediction Accuracy = 62.61200000000001%, Loss = 0.3926757276058197
Epoch: 7345, Batch Gradient Norm: 9.767903648357835
Epoch: 7345, Batch Gradient Norm after: 9.767903648357835
Epoch 7346/10000, Prediction Accuracy = 62.9%, Loss = 0.3930027842521667
Epoch: 7346, Batch Gradient Norm: 10.006146469963747
Epoch: 7346, Batch Gradient Norm after: 10.006146469963747
Epoch 7347/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3948318183422089
Epoch: 7347, Batch Gradient Norm: 11.569416679218604
Epoch: 7347, Batch Gradient Norm after: 11.569416679218604
Epoch 7348/10000, Prediction Accuracy = 62.754%, Loss = 0.40320456624031065
Epoch: 7348, Batch Gradient Norm: 9.761013897939156
Epoch: 7348, Batch Gradient Norm after: 9.761013897939156
Epoch 7349/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.39459078907966616
Epoch: 7349, Batch Gradient Norm: 11.160111821223294
Epoch: 7349, Batch Gradient Norm after: 11.160111821223294
Epoch 7350/10000, Prediction Accuracy = 62.65200000000001%, Loss = 0.40635693073272705
Epoch: 7350, Batch Gradient Norm: 10.603637768024484
Epoch: 7350, Batch Gradient Norm after: 10.603637768024484
Epoch 7351/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.39908666014671323
Epoch: 7351, Batch Gradient Norm: 9.568660745427861
Epoch: 7351, Batch Gradient Norm after: 9.568660745427861
Epoch 7352/10000, Prediction Accuracy = 62.824%, Loss = 0.39363664388656616
Epoch: 7352, Batch Gradient Norm: 7.496609795024219
Epoch: 7352, Batch Gradient Norm after: 7.496609795024219
Epoch 7353/10000, Prediction Accuracy = 62.738%, Loss = 0.37864981293678285
Epoch: 7353, Batch Gradient Norm: 9.863709488939495
Epoch: 7353, Batch Gradient Norm after: 9.863709488939495
Epoch 7354/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.3893915295600891
Epoch: 7354, Batch Gradient Norm: 10.598070825272567
Epoch: 7354, Batch Gradient Norm after: 10.598070825272567
Epoch 7355/10000, Prediction Accuracy = 62.726%, Loss = 0.3995999932289124
Epoch: 7355, Batch Gradient Norm: 10.169802479043607
Epoch: 7355, Batch Gradient Norm after: 10.169802479043607
Epoch 7356/10000, Prediction Accuracy = 62.80400000000001%, Loss = 0.39662593603134155
Epoch: 7356, Batch Gradient Norm: 8.717437413259868
Epoch: 7356, Batch Gradient Norm after: 8.717437413259868
Epoch 7357/10000, Prediction Accuracy = 62.67%, Loss = 0.3868270516395569
Epoch: 7357, Batch Gradient Norm: 8.2542016260419
Epoch: 7357, Batch Gradient Norm after: 8.2542016260419
Epoch 7358/10000, Prediction Accuracy = 62.81199999999999%, Loss = 0.38544521331787107
Epoch: 7358, Batch Gradient Norm: 9.754980793673669
Epoch: 7358, Batch Gradient Norm after: 9.754980793673669
Epoch 7359/10000, Prediction Accuracy = 62.6%, Loss = 0.3956978380680084
Epoch: 7359, Batch Gradient Norm: 10.132282846603601
Epoch: 7359, Batch Gradient Norm after: 10.132282846603601
Epoch 7360/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.39468422532081604
Epoch: 7360, Batch Gradient Norm: 10.481323848132467
Epoch: 7360, Batch Gradient Norm after: 10.481323848132467
Epoch 7361/10000, Prediction Accuracy = 62.746%, Loss = 0.39954193234443663
Epoch: 7361, Batch Gradient Norm: 12.424260562882221
Epoch: 7361, Batch Gradient Norm after: 12.424260562882221
Epoch 7362/10000, Prediction Accuracy = 62.742%, Loss = 0.4181049048900604
Epoch: 7362, Batch Gradient Norm: 9.017577643902749
Epoch: 7362, Batch Gradient Norm after: 9.017577643902749
Epoch 7363/10000, Prediction Accuracy = 62.854%, Loss = 0.390777051448822
Epoch: 7363, Batch Gradient Norm: 9.186671660439202
Epoch: 7363, Batch Gradient Norm after: 9.186671660439202
Epoch 7364/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3911805391311646
Epoch: 7364, Batch Gradient Norm: 11.049273526910813
Epoch: 7364, Batch Gradient Norm after: 11.049273526910813
Epoch 7365/10000, Prediction Accuracy = 62.77%, Loss = 0.40008469820022585
Epoch: 7365, Batch Gradient Norm: 11.933040825600083
Epoch: 7365, Batch Gradient Norm after: 11.933040825600083
Epoch 7366/10000, Prediction Accuracy = 62.852%, Loss = 0.4177722096443176
Epoch: 7366, Batch Gradient Norm: 7.45800713270323
Epoch: 7366, Batch Gradient Norm after: 7.45800713270323
Epoch 7367/10000, Prediction Accuracy = 62.92%, Loss = 0.37735178470611574
Epoch: 7367, Batch Gradient Norm: 9.569064332915252
Epoch: 7367, Batch Gradient Norm after: 9.569064332915252
Epoch 7368/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.39256757497787476
Epoch: 7368, Batch Gradient Norm: 9.300903205333801
Epoch: 7368, Batch Gradient Norm after: 9.300903205333801
Epoch 7369/10000, Prediction Accuracy = 62.722%, Loss = 0.3882237434387207
Epoch: 7369, Batch Gradient Norm: 8.271871957136126
Epoch: 7369, Batch Gradient Norm after: 8.271871957136126
Epoch 7370/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3813769996166229
Epoch: 7370, Batch Gradient Norm: 8.343935140542653
Epoch: 7370, Batch Gradient Norm after: 8.343935140542653
Epoch 7371/10000, Prediction Accuracy = 62.834%, Loss = 0.38332839012145997
Epoch: 7371, Batch Gradient Norm: 9.78772903202639
Epoch: 7371, Batch Gradient Norm after: 9.78772903202639
Epoch 7372/10000, Prediction Accuracy = 62.769999999999996%, Loss = 0.3941739916801453
Epoch: 7372, Batch Gradient Norm: 10.784516538066157
Epoch: 7372, Batch Gradient Norm after: 10.784516538066157
Epoch 7373/10000, Prediction Accuracy = 62.7%, Loss = 0.4061159551143646
Epoch: 7373, Batch Gradient Norm: 9.264638727802836
Epoch: 7373, Batch Gradient Norm after: 9.264638727802836
Epoch 7374/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3905917823314667
Epoch: 7374, Batch Gradient Norm: 10.047727890958916
Epoch: 7374, Batch Gradient Norm after: 10.047727890958916
Epoch 7375/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.3960668683052063
Epoch: 7375, Batch Gradient Norm: 9.269227522337827
Epoch: 7375, Batch Gradient Norm after: 9.269227522337827
Epoch 7376/10000, Prediction Accuracy = 62.824%, Loss = 0.39018588662147524
Epoch: 7376, Batch Gradient Norm: 8.512468721902087
Epoch: 7376, Batch Gradient Norm after: 8.512468721902087
Epoch 7377/10000, Prediction Accuracy = 62.842%, Loss = 0.38348562717437745
Epoch: 7377, Batch Gradient Norm: 11.02339777480589
Epoch: 7377, Batch Gradient Norm after: 11.02339777480589
Epoch 7378/10000, Prediction Accuracy = 62.9%, Loss = 0.4017364263534546
Epoch: 7378, Batch Gradient Norm: 11.061737857533029
Epoch: 7378, Batch Gradient Norm after: 11.061737857533029
Epoch 7379/10000, Prediction Accuracy = 62.812%, Loss = 0.4037936806678772
Epoch: 7379, Batch Gradient Norm: 10.607386528732643
Epoch: 7379, Batch Gradient Norm after: 10.607386528732643
Epoch 7380/10000, Prediction Accuracy = 62.714%, Loss = 0.4011856198310852
Epoch: 7380, Batch Gradient Norm: 9.849094800421097
Epoch: 7380, Batch Gradient Norm after: 9.849094800421097
Epoch 7381/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.39294334650039675
Epoch: 7381, Batch Gradient Norm: 11.208219498718003
Epoch: 7381, Batch Gradient Norm after: 11.208219498718003
Epoch 7382/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.402852064371109
Epoch: 7382, Batch Gradient Norm: 7.925236437785059
Epoch: 7382, Batch Gradient Norm after: 7.925236437785059
Epoch 7383/10000, Prediction Accuracy = 62.834%, Loss = 0.38051080107688906
Epoch: 7383, Batch Gradient Norm: 7.788646645952114
Epoch: 7383, Batch Gradient Norm after: 7.788646645952114
Epoch 7384/10000, Prediction Accuracy = 62.80999999999999%, Loss = 0.3800166666507721
Epoch: 7384, Batch Gradient Norm: 9.44085676437219
Epoch: 7384, Batch Gradient Norm after: 9.44085676437219
Epoch 7385/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.39197590947151184
Epoch: 7385, Batch Gradient Norm: 11.846602897964056
Epoch: 7385, Batch Gradient Norm after: 11.846602897964056
Epoch 7386/10000, Prediction Accuracy = 62.714%, Loss = 0.40850741863250734
Epoch: 7386, Batch Gradient Norm: 10.050107345154913
Epoch: 7386, Batch Gradient Norm after: 10.050107345154913
Epoch 7387/10000, Prediction Accuracy = 62.686%, Loss = 0.393326997756958
Epoch: 7387, Batch Gradient Norm: 8.461793824803722
Epoch: 7387, Batch Gradient Norm after: 8.461793824803722
Epoch 7388/10000, Prediction Accuracy = 62.830000000000005%, Loss = 0.38349500894546507
Epoch: 7388, Batch Gradient Norm: 6.908867347133633
Epoch: 7388, Batch Gradient Norm after: 6.908867347133633
Epoch 7389/10000, Prediction Accuracy = 63.06600000000001%, Loss = 0.37543208003044126
Epoch: 7389, Batch Gradient Norm: 9.302828960136603
Epoch: 7389, Batch Gradient Norm after: 9.302828960136603
Epoch 7390/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.38765132427215576
Epoch: 7390, Batch Gradient Norm: 13.228611814940194
Epoch: 7390, Batch Gradient Norm after: 13.228611814940194
Epoch 7391/10000, Prediction Accuracy = 62.738%, Loss = 0.4179386794567108
Epoch: 7391, Batch Gradient Norm: 11.464639441707149
Epoch: 7391, Batch Gradient Norm after: 11.464639441707149
Epoch 7392/10000, Prediction Accuracy = 62.61%, Loss = 0.4052792191505432
Epoch: 7392, Batch Gradient Norm: 9.766410815825393
Epoch: 7392, Batch Gradient Norm after: 9.766410815825393
Epoch 7393/10000, Prediction Accuracy = 62.876%, Loss = 0.3923828423023224
Epoch: 7393, Batch Gradient Norm: 5.417505601934659
Epoch: 7393, Batch Gradient Norm after: 5.417505601934659
Epoch 7394/10000, Prediction Accuracy = 62.936%, Loss = 0.36904385685920715
Epoch: 7394, Batch Gradient Norm: 7.85879784954611
Epoch: 7394, Batch Gradient Norm after: 7.85879784954611
Epoch 7395/10000, Prediction Accuracy = 62.788%, Loss = 0.37937481999397277
Epoch: 7395, Batch Gradient Norm: 12.393741857925443
Epoch: 7395, Batch Gradient Norm after: 12.393741857925443
Epoch 7396/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.4103905975818634
Epoch: 7396, Batch Gradient Norm: 11.586523580901563
Epoch: 7396, Batch Gradient Norm after: 11.586523580901563
Epoch 7397/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.4080298006534576
Epoch: 7397, Batch Gradient Norm: 8.652200892766704
Epoch: 7397, Batch Gradient Norm after: 8.652200892766704
Epoch 7398/10000, Prediction Accuracy = 62.746%, Loss = 0.38775068521499634
Epoch: 7398, Batch Gradient Norm: 9.120954388162234
Epoch: 7398, Batch Gradient Norm after: 9.120954388162234
Epoch 7399/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.3876621127128601
Epoch: 7399, Batch Gradient Norm: 12.31623683899476
Epoch: 7399, Batch Gradient Norm after: 12.31623683899476
Epoch 7400/10000, Prediction Accuracy = 62.862%, Loss = 0.4151759445667267
Epoch: 7400, Batch Gradient Norm: 10.615515226086092
Epoch: 7400, Batch Gradient Norm after: 10.615515226086092
Epoch 7401/10000, Prediction Accuracy = 62.688%, Loss = 0.4015758097171783
Epoch: 7401, Batch Gradient Norm: 11.118657803336149
Epoch: 7401, Batch Gradient Norm after: 11.118657803336149
Epoch 7402/10000, Prediction Accuracy = 62.646%, Loss = 0.40288946628570554
Epoch: 7402, Batch Gradient Norm: 9.41007994565685
Epoch: 7402, Batch Gradient Norm after: 9.41007994565685
Epoch 7403/10000, Prediction Accuracy = 62.722%, Loss = 0.3917675316333771
Epoch: 7403, Batch Gradient Norm: 9.780650466612133
Epoch: 7403, Batch Gradient Norm after: 9.780650466612133
Epoch 7404/10000, Prediction Accuracy = 62.788%, Loss = 0.39394978880882264
Epoch: 7404, Batch Gradient Norm: 6.974100066082097
Epoch: 7404, Batch Gradient Norm after: 6.974100066082097
Epoch 7405/10000, Prediction Accuracy = 62.902%, Loss = 0.3753663897514343
Epoch: 7405, Batch Gradient Norm: 6.854790299342805
Epoch: 7405, Batch Gradient Norm after: 6.854790299342805
Epoch 7406/10000, Prediction Accuracy = 62.848%, Loss = 0.3753214657306671
Epoch: 7406, Batch Gradient Norm: 7.969852339921953
Epoch: 7406, Batch Gradient Norm after: 7.969852339921953
Epoch 7407/10000, Prediction Accuracy = 62.886%, Loss = 0.3824833512306213
Epoch: 7407, Batch Gradient Norm: 11.304191984208428
Epoch: 7407, Batch Gradient Norm after: 11.304191984208428
Epoch 7408/10000, Prediction Accuracy = 62.758%, Loss = 0.4042656719684601
Epoch: 7408, Batch Gradient Norm: 10.733432516010122
Epoch: 7408, Batch Gradient Norm after: 10.733432516010122
Epoch 7409/10000, Prediction Accuracy = 62.73%, Loss = 0.3990217804908752
Epoch: 7409, Batch Gradient Norm: 9.426248849320086
Epoch: 7409, Batch Gradient Norm after: 9.426248849320086
Epoch 7410/10000, Prediction Accuracy = 62.79%, Loss = 0.3884211003780365
Epoch: 7410, Batch Gradient Norm: 8.799464496812684
Epoch: 7410, Batch Gradient Norm after: 8.799464496812684
Epoch 7411/10000, Prediction Accuracy = 62.862%, Loss = 0.3858449935913086
Epoch: 7411, Batch Gradient Norm: 9.580426438387365
Epoch: 7411, Batch Gradient Norm after: 9.580426438387365
Epoch 7412/10000, Prediction Accuracy = 62.786%, Loss = 0.3905950367450714
Epoch: 7412, Batch Gradient Norm: 10.208964707303007
Epoch: 7412, Batch Gradient Norm after: 10.208964707303007
Epoch 7413/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.39694035053253174
Epoch: 7413, Batch Gradient Norm: 10.423507320156864
Epoch: 7413, Batch Gradient Norm after: 10.423507320156864
Epoch 7414/10000, Prediction Accuracy = 62.798%, Loss = 0.39878285527229307
Epoch: 7414, Batch Gradient Norm: 10.130763839935257
Epoch: 7414, Batch Gradient Norm after: 10.130763839935257
Epoch 7415/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.39540702700614927
Epoch: 7415, Batch Gradient Norm: 9.511171990837758
Epoch: 7415, Batch Gradient Norm after: 9.511171990837758
Epoch 7416/10000, Prediction Accuracy = 62.968%, Loss = 0.38896130919456484
Epoch: 7416, Batch Gradient Norm: 10.090679196835493
Epoch: 7416, Batch Gradient Norm after: 10.090679196835493
Epoch 7417/10000, Prediction Accuracy = 62.786%, Loss = 0.3916132807731628
Epoch: 7417, Batch Gradient Norm: 10.392672877121244
Epoch: 7417, Batch Gradient Norm after: 10.392672877121244
Epoch 7418/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.39381893873214724
Epoch: 7418, Batch Gradient Norm: 11.662626282970587
Epoch: 7418, Batch Gradient Norm after: 11.662626282970587
Epoch 7419/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.403559947013855
Epoch: 7419, Batch Gradient Norm: 9.480087507827234
Epoch: 7419, Batch Gradient Norm after: 9.480087507827234
Epoch 7420/10000, Prediction Accuracy = 62.639999999999986%, Loss = 0.3886190474033356
Epoch: 7420, Batch Gradient Norm: 7.402369896914747
Epoch: 7420, Batch Gradient Norm after: 7.402369896914747
Epoch 7421/10000, Prediction Accuracy = 62.83%, Loss = 0.3758024275302887
Epoch: 7421, Batch Gradient Norm: 10.009580392935932
Epoch: 7421, Batch Gradient Norm after: 10.009580392935932
Epoch 7422/10000, Prediction Accuracy = 62.638%, Loss = 0.39329119920730593
Epoch: 7422, Batch Gradient Norm: 9.891256026085264
Epoch: 7422, Batch Gradient Norm after: 9.891256026085264
Epoch 7423/10000, Prediction Accuracy = 62.836%, Loss = 0.39255987405776976
Epoch: 7423, Batch Gradient Norm: 10.60933385013872
Epoch: 7423, Batch Gradient Norm after: 10.60933385013872
Epoch 7424/10000, Prediction Accuracy = 62.746%, Loss = 0.4025527238845825
Epoch: 7424, Batch Gradient Norm: 9.366881515141696
Epoch: 7424, Batch Gradient Norm after: 9.366881515141696
Epoch 7425/10000, Prediction Accuracy = 62.754000000000005%, Loss = 0.3878975987434387
Epoch: 7425, Batch Gradient Norm: 9.120877277256312
Epoch: 7425, Batch Gradient Norm after: 9.120877277256312
Epoch 7426/10000, Prediction Accuracy = 62.81999999999999%, Loss = 0.38756143450737
Epoch: 7426, Batch Gradient Norm: 8.450934384387839
Epoch: 7426, Batch Gradient Norm after: 8.450934384387839
Epoch 7427/10000, Prediction Accuracy = 62.806%, Loss = 0.3829029262065887
Epoch: 7427, Batch Gradient Norm: 10.175990331084249
Epoch: 7427, Batch Gradient Norm after: 10.175990331084249
Epoch 7428/10000, Prediction Accuracy = 62.738%, Loss = 0.3939107656478882
Epoch: 7428, Batch Gradient Norm: 10.545349001229802
Epoch: 7428, Batch Gradient Norm after: 10.545349001229802
Epoch 7429/10000, Prediction Accuracy = 62.802%, Loss = 0.3956911742687225
Epoch: 7429, Batch Gradient Norm: 7.233113385923165
Epoch: 7429, Batch Gradient Norm after: 7.233113385923165
Epoch 7430/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3747998118400574
Epoch: 7430, Batch Gradient Norm: 6.623479505028553
Epoch: 7430, Batch Gradient Norm after: 6.623479505028553
Epoch 7431/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.37246693968772887
Epoch: 7431, Batch Gradient Norm: 9.092064530214637
Epoch: 7431, Batch Gradient Norm after: 9.092064530214637
Epoch 7432/10000, Prediction Accuracy = 62.888%, Loss = 0.3858722627162933
Epoch: 7432, Batch Gradient Norm: 14.02644880874653
Epoch: 7432, Batch Gradient Norm after: 14.02644880874653
Epoch 7433/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4258733749389648
Epoch: 7433, Batch Gradient Norm: 13.705268289862497
Epoch: 7433, Batch Gradient Norm after: 13.705268289862497
Epoch 7434/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.4328893184661865
Epoch: 7434, Batch Gradient Norm: 8.784011633789007
Epoch: 7434, Batch Gradient Norm after: 8.784011633789007
Epoch 7435/10000, Prediction Accuracy = 62.86%, Loss = 0.3887354075908661
Epoch: 7435, Batch Gradient Norm: 8.318282978961852
Epoch: 7435, Batch Gradient Norm after: 8.318282978961852
Epoch 7436/10000, Prediction Accuracy = 62.626%, Loss = 0.38354894518852234
Epoch: 7436, Batch Gradient Norm: 7.56438122640158
Epoch: 7436, Batch Gradient Norm after: 7.56438122640158
Epoch 7437/10000, Prediction Accuracy = 62.826%, Loss = 0.37852550148963926
Epoch: 7437, Batch Gradient Norm: 9.746648647839134
Epoch: 7437, Batch Gradient Norm after: 9.746648647839134
Epoch 7438/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3936005771160126
Epoch: 7438, Batch Gradient Norm: 9.826329322853628
Epoch: 7438, Batch Gradient Norm after: 9.826329322853628
Epoch 7439/10000, Prediction Accuracy = 62.88599999999999%, Loss = 0.3922258973121643
Epoch: 7439, Batch Gradient Norm: 12.231683046715474
Epoch: 7439, Batch Gradient Norm after: 12.231683046715474
Epoch 7440/10000, Prediction Accuracy = 62.84000000000001%, Loss = 0.4098013699054718
Epoch: 7440, Batch Gradient Norm: 11.731001186251667
Epoch: 7440, Batch Gradient Norm after: 11.731001186251667
Epoch 7441/10000, Prediction Accuracy = 62.912%, Loss = 0.40919776558876036
Epoch: 7441, Batch Gradient Norm: 8.372509377550848
Epoch: 7441, Batch Gradient Norm after: 8.372509377550848
Epoch 7442/10000, Prediction Accuracy = 62.834%, Loss = 0.3894657909870148
Epoch: 7442, Batch Gradient Norm: 10.348421801260328
Epoch: 7442, Batch Gradient Norm after: 10.348421801260328
Epoch 7443/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.40161901116371157
Epoch: 7443, Batch Gradient Norm: 9.752569375848838
Epoch: 7443, Batch Gradient Norm after: 9.752569375848838
Epoch 7444/10000, Prediction Accuracy = 62.812%, Loss = 0.38858962059020996
Epoch: 7444, Batch Gradient Norm: 9.485124942127632
Epoch: 7444, Batch Gradient Norm after: 9.485124942127632
Epoch 7445/10000, Prediction Accuracy = 62.79%, Loss = 0.3903992593288422
Epoch: 7445, Batch Gradient Norm: 10.321611175272018
Epoch: 7445, Batch Gradient Norm after: 10.321611175272018
Epoch 7446/10000, Prediction Accuracy = 62.754%, Loss = 0.3956248342990875
Epoch: 7446, Batch Gradient Norm: 9.101787790860728
Epoch: 7446, Batch Gradient Norm after: 9.101787790860728
Epoch 7447/10000, Prediction Accuracy = 62.812%, Loss = 0.38658319115638734
Epoch: 7447, Batch Gradient Norm: 8.67890398352492
Epoch: 7447, Batch Gradient Norm after: 8.67890398352492
Epoch 7448/10000, Prediction Accuracy = 62.8%, Loss = 0.38467624187469485
Epoch: 7448, Batch Gradient Norm: 10.212691445393792
Epoch: 7448, Batch Gradient Norm after: 10.212691445393792
Epoch 7449/10000, Prediction Accuracy = 62.85%, Loss = 0.39412484169006345
Epoch: 7449, Batch Gradient Norm: 8.926232014750415
Epoch: 7449, Batch Gradient Norm after: 8.926232014750415
Epoch 7450/10000, Prediction Accuracy = 62.76400000000001%, Loss = 0.38449387550354003
Epoch: 7450, Batch Gradient Norm: 8.809860177506604
Epoch: 7450, Batch Gradient Norm after: 8.809860177506604
Epoch 7451/10000, Prediction Accuracy = 62.872%, Loss = 0.3868101418018341
Epoch: 7451, Batch Gradient Norm: 10.614659025426338
Epoch: 7451, Batch Gradient Norm after: 10.614659025426338
Epoch 7452/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.4030936360359192
Epoch: 7452, Batch Gradient Norm: 8.854363819548619
Epoch: 7452, Batch Gradient Norm after: 8.854363819548619
Epoch 7453/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.387355637550354
Epoch: 7453, Batch Gradient Norm: 11.183315629849188
Epoch: 7453, Batch Gradient Norm after: 11.183315629849188
Epoch 7454/10000, Prediction Accuracy = 62.67%, Loss = 0.4037989616394043
Epoch: 7454, Batch Gradient Norm: 9.8741494558536
Epoch: 7454, Batch Gradient Norm after: 9.8741494558536
Epoch 7455/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.39063945412635803
Epoch: 7455, Batch Gradient Norm: 10.52316521796447
Epoch: 7455, Batch Gradient Norm after: 10.52316521796447
Epoch 7456/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.3949236810207367
Epoch: 7456, Batch Gradient Norm: 10.199106365733241
Epoch: 7456, Batch Gradient Norm after: 10.199106365733241
Epoch 7457/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.39271631836891174
Epoch: 7457, Batch Gradient Norm: 8.779041507883585
Epoch: 7457, Batch Gradient Norm after: 8.779041507883585
Epoch 7458/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.38386638164520265
Epoch: 7458, Batch Gradient Norm: 8.819761809330135
Epoch: 7458, Batch Gradient Norm after: 8.819761809330135
Epoch 7459/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3856406927108765
Epoch: 7459, Batch Gradient Norm: 9.581136178214214
Epoch: 7459, Batch Gradient Norm after: 9.581136178214214
Epoch 7460/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3879063665866852
Epoch: 7460, Batch Gradient Norm: 8.660663332267319
Epoch: 7460, Batch Gradient Norm after: 8.660663332267319
Epoch 7461/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.38379071950912474
Epoch: 7461, Batch Gradient Norm: 11.23473014365468
Epoch: 7461, Batch Gradient Norm after: 11.23473014365468
Epoch 7462/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.4024232506752014
Epoch: 7462, Batch Gradient Norm: 11.27306827938459
Epoch: 7462, Batch Gradient Norm after: 11.27306827938459
Epoch 7463/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.40731545686721804
Epoch: 7463, Batch Gradient Norm: 7.2891745660777785
Epoch: 7463, Batch Gradient Norm after: 7.2891745660777785
Epoch 7464/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.3776033163070679
Epoch: 7464, Batch Gradient Norm: 7.592258810808711
Epoch: 7464, Batch Gradient Norm after: 7.592258810808711
Epoch 7465/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.37597814202308655
Epoch: 7465, Batch Gradient Norm: 11.745197647985284
Epoch: 7465, Batch Gradient Norm after: 11.745197647985284
Epoch 7466/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.4070869028568268
Epoch: 7466, Batch Gradient Norm: 12.737631276933488
Epoch: 7466, Batch Gradient Norm after: 12.737631276933488
Epoch 7467/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.41494280099868774
Epoch: 7467, Batch Gradient Norm: 11.436531484696706
Epoch: 7467, Batch Gradient Norm after: 11.436531484696706
Epoch 7468/10000, Prediction Accuracy = 62.938%, Loss = 0.4020467162132263
Epoch: 7468, Batch Gradient Norm: 9.494085154842326
Epoch: 7468, Batch Gradient Norm after: 9.494085154842326
Epoch 7469/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.38876340389251707
Epoch: 7469, Batch Gradient Norm: 7.329626432127868
Epoch: 7469, Batch Gradient Norm after: 7.329626432127868
Epoch 7470/10000, Prediction Accuracy = 62.678%, Loss = 0.3762633800506592
Epoch: 7470, Batch Gradient Norm: 8.357565456447722
Epoch: 7470, Batch Gradient Norm after: 8.357565456447722
Epoch 7471/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.3825744867324829
Epoch: 7471, Batch Gradient Norm: 10.496977882311707
Epoch: 7471, Batch Gradient Norm after: 10.496977882311707
Epoch 7472/10000, Prediction Accuracy = 62.84799999999999%, Loss = 0.39868354201316836
Epoch: 7472, Batch Gradient Norm: 12.477266890747243
Epoch: 7472, Batch Gradient Norm after: 12.477266890747243
Epoch 7473/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.4123009204864502
Epoch: 7473, Batch Gradient Norm: 7.7959311330978585
Epoch: 7473, Batch Gradient Norm after: 7.7959311330978585
Epoch 7474/10000, Prediction Accuracy = 62.88599999999999%, Loss = 0.37938024997711184
Epoch: 7474, Batch Gradient Norm: 9.344358175613708
Epoch: 7474, Batch Gradient Norm after: 9.344358175613708
Epoch 7475/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.38892157673835753
Epoch: 7475, Batch Gradient Norm: 9.976003366003113
Epoch: 7475, Batch Gradient Norm after: 9.976003366003113
Epoch 7476/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.3926441609859467
Epoch: 7476, Batch Gradient Norm: 10.216218179091856
Epoch: 7476, Batch Gradient Norm after: 10.216218179091856
Epoch 7477/10000, Prediction Accuracy = 62.674%, Loss = 0.39613571763038635
Epoch: 7477, Batch Gradient Norm: 8.294209860894316
Epoch: 7477, Batch Gradient Norm after: 8.294209860894316
Epoch 7478/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.382136607170105
Epoch: 7478, Batch Gradient Norm: 7.552398855715883
Epoch: 7478, Batch Gradient Norm after: 7.552398855715883
Epoch 7479/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3778993546962738
Epoch: 7479, Batch Gradient Norm: 9.81296682464086
Epoch: 7479, Batch Gradient Norm after: 9.81296682464086
Epoch 7480/10000, Prediction Accuracy = 62.902%, Loss = 0.39000203609466555
Epoch: 7480, Batch Gradient Norm: 11.24783196175018
Epoch: 7480, Batch Gradient Norm after: 11.24783196175018
Epoch 7481/10000, Prediction Accuracy = 62.77%, Loss = 0.4001206636428833
Epoch: 7481, Batch Gradient Norm: 9.411786116954547
Epoch: 7481, Batch Gradient Norm after: 9.411786116954547
Epoch 7482/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.3888823390007019
Epoch: 7482, Batch Gradient Norm: 11.314334680154213
Epoch: 7482, Batch Gradient Norm after: 11.314334680154213
Epoch 7483/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.40024101734161377
Epoch: 7483, Batch Gradient Norm: 9.265588964538878
Epoch: 7483, Batch Gradient Norm after: 9.265588964538878
Epoch 7484/10000, Prediction Accuracy = 62.721999999999994%, Loss = 0.3877267599105835
Epoch: 7484, Batch Gradient Norm: 9.620981032084407
Epoch: 7484, Batch Gradient Norm after: 9.620981032084407
Epoch 7485/10000, Prediction Accuracy = 62.866%, Loss = 0.39424759745597837
Epoch: 7485, Batch Gradient Norm: 7.6742140027852805
Epoch: 7485, Batch Gradient Norm after: 7.6742140027852805
Epoch 7486/10000, Prediction Accuracy = 62.912%, Loss = 0.3795251131057739
Epoch: 7486, Batch Gradient Norm: 8.26642542112533
Epoch: 7486, Batch Gradient Norm after: 8.26642542112533
Epoch 7487/10000, Prediction Accuracy = 62.896%, Loss = 0.38279021978378297
Epoch: 7487, Batch Gradient Norm: 10.862618947059099
Epoch: 7487, Batch Gradient Norm after: 10.862618947059099
Epoch 7488/10000, Prediction Accuracy = 62.69%, Loss = 0.3971953570842743
Epoch: 7488, Batch Gradient Norm: 11.406596293334944
Epoch: 7488, Batch Gradient Norm after: 11.406596293334944
Epoch 7489/10000, Prediction Accuracy = 62.786%, Loss = 0.40471956729888914
Epoch: 7489, Batch Gradient Norm: 10.261297002555324
Epoch: 7489, Batch Gradient Norm after: 10.261297002555324
Epoch 7490/10000, Prediction Accuracy = 62.721999999999994%, Loss = 0.3965605080127716
Epoch: 7490, Batch Gradient Norm: 7.528150502369371
Epoch: 7490, Batch Gradient Norm after: 7.528150502369371
Epoch 7491/10000, Prediction Accuracy = 62.822%, Loss = 0.37882094383239745
Epoch: 7491, Batch Gradient Norm: 9.463158202517242
Epoch: 7491, Batch Gradient Norm after: 9.463158202517242
Epoch 7492/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3899808585643768
Epoch: 7492, Batch Gradient Norm: 8.602748887296134
Epoch: 7492, Batch Gradient Norm after: 8.602748887296134
Epoch 7493/10000, Prediction Accuracy = 62.86%, Loss = 0.3848275661468506
Epoch: 7493, Batch Gradient Norm: 9.811171080048528
Epoch: 7493, Batch Gradient Norm after: 9.811171080048528
Epoch 7494/10000, Prediction Accuracy = 63.088%, Loss = 0.3906518995761871
Epoch: 7494, Batch Gradient Norm: 9.499758035242353
Epoch: 7494, Batch Gradient Norm after: 9.499758035242353
Epoch 7495/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.38899906873703005
Epoch: 7495, Batch Gradient Norm: 10.204232905672626
Epoch: 7495, Batch Gradient Norm after: 10.204232905672626
Epoch 7496/10000, Prediction Accuracy = 62.70400000000001%, Loss = 0.3921373784542084
Epoch: 7496, Batch Gradient Norm: 8.990632397851257
Epoch: 7496, Batch Gradient Norm after: 8.990632397851257
Epoch 7497/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.383981853723526
Epoch: 7497, Batch Gradient Norm: 10.393473755614538
Epoch: 7497, Batch Gradient Norm after: 10.393473755614538
Epoch 7498/10000, Prediction Accuracy = 62.788%, Loss = 0.39342315793037413
Epoch: 7498, Batch Gradient Norm: 10.315101589946757
Epoch: 7498, Batch Gradient Norm after: 10.315101589946757
Epoch 7499/10000, Prediction Accuracy = 62.758%, Loss = 0.3935402274131775
Epoch: 7499, Batch Gradient Norm: 11.75999178196586
Epoch: 7499, Batch Gradient Norm after: 11.75999178196586
Epoch 7500/10000, Prediction Accuracy = 62.75600000000001%, Loss = 0.4050982415676117
Epoch: 7500, Batch Gradient Norm: 10.11927091774558
Epoch: 7500, Batch Gradient Norm after: 10.11927091774558
Epoch 7501/10000, Prediction Accuracy = 62.98600000000001%, Loss = 0.391448849439621
Epoch: 7501, Batch Gradient Norm: 8.48874451364579
Epoch: 7501, Batch Gradient Norm after: 8.48874451364579
Epoch 7502/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.38431915640830994
Epoch: 7502, Batch Gradient Norm: 8.03499355846374
Epoch: 7502, Batch Gradient Norm after: 8.03499355846374
Epoch 7503/10000, Prediction Accuracy = 62.854%, Loss = 0.380096572637558
Epoch: 7503, Batch Gradient Norm: 9.07898693811694
Epoch: 7503, Batch Gradient Norm after: 9.07898693811694
Epoch 7504/10000, Prediction Accuracy = 62.834%, Loss = 0.3819332838058472
Epoch: 7504, Batch Gradient Norm: 10.035066784939199
Epoch: 7504, Batch Gradient Norm after: 10.035066784939199
Epoch 7505/10000, Prediction Accuracy = 62.864%, Loss = 0.38884900212287904
Epoch: 7505, Batch Gradient Norm: 11.707482289259097
Epoch: 7505, Batch Gradient Norm after: 11.707482289259097
Epoch 7506/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.40528947710990904
Epoch: 7506, Batch Gradient Norm: 9.5629681018381
Epoch: 7506, Batch Gradient Norm after: 9.5629681018381
Epoch 7507/10000, Prediction Accuracy = 62.79%, Loss = 0.3884588420391083
Epoch: 7507, Batch Gradient Norm: 7.890363491051096
Epoch: 7507, Batch Gradient Norm after: 7.890363491051096
Epoch 7508/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3786725878715515
Epoch: 7508, Batch Gradient Norm: 8.623646220077696
Epoch: 7508, Batch Gradient Norm after: 8.623646220077696
Epoch 7509/10000, Prediction Accuracy = 62.814%, Loss = 0.38209015130996704
Epoch: 7509, Batch Gradient Norm: 10.24132497449185
Epoch: 7509, Batch Gradient Norm after: 10.24132497449185
Epoch 7510/10000, Prediction Accuracy = 62.898%, Loss = 0.3954746425151825
Epoch: 7510, Batch Gradient Norm: 10.124955195323539
Epoch: 7510, Batch Gradient Norm after: 10.124955195323539
Epoch 7511/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.39426218867301943
Epoch: 7511, Batch Gradient Norm: 8.57174846301742
Epoch: 7511, Batch Gradient Norm after: 8.57174846301742
Epoch 7512/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.38416218757629395
Epoch: 7512, Batch Gradient Norm: 10.620353077446973
Epoch: 7512, Batch Gradient Norm after: 10.620353077446973
Epoch 7513/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.39249667525291443
Epoch: 7513, Batch Gradient Norm: 10.30323433222579
Epoch: 7513, Batch Gradient Norm after: 10.30323433222579
Epoch 7514/10000, Prediction Accuracy = 62.866%, Loss = 0.3922855257987976
Epoch: 7514, Batch Gradient Norm: 6.778981832073423
Epoch: 7514, Batch Gradient Norm after: 6.778981832073423
Epoch 7515/10000, Prediction Accuracy = 62.992%, Loss = 0.3732099711894989
Epoch: 7515, Batch Gradient Norm: 7.776045855813295
Epoch: 7515, Batch Gradient Norm after: 7.776045855813295
Epoch 7516/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.3764252305030823
Epoch: 7516, Batch Gradient Norm: 11.212808700199954
Epoch: 7516, Batch Gradient Norm after: 11.212808700199954
Epoch 7517/10000, Prediction Accuracy = 62.964%, Loss = 0.39935833811759947
Epoch: 7517, Batch Gradient Norm: 9.83728537953944
Epoch: 7517, Batch Gradient Norm after: 9.83728537953944
Epoch 7518/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.3916846752166748
Epoch: 7518, Batch Gradient Norm: 10.403501213497169
Epoch: 7518, Batch Gradient Norm after: 10.403501213497169
Epoch 7519/10000, Prediction Accuracy = 62.802%, Loss = 0.3942369520664215
Epoch: 7519, Batch Gradient Norm: 9.18025373429968
Epoch: 7519, Batch Gradient Norm after: 9.18025373429968
Epoch 7520/10000, Prediction Accuracy = 62.94%, Loss = 0.38544230461120604
Epoch: 7520, Batch Gradient Norm: 10.946246413296047
Epoch: 7520, Batch Gradient Norm after: 10.946246413296047
Epoch 7521/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.39886597394943235
Epoch: 7521, Batch Gradient Norm: 11.64567813289565
Epoch: 7521, Batch Gradient Norm after: 11.64567813289565
Epoch 7522/10000, Prediction Accuracy = 62.8%, Loss = 0.4046814739704132
Epoch: 7522, Batch Gradient Norm: 11.345970032813446
Epoch: 7522, Batch Gradient Norm after: 11.345970032813446
Epoch 7523/10000, Prediction Accuracy = 62.95%, Loss = 0.401633620262146
Epoch: 7523, Batch Gradient Norm: 12.391981995828614
Epoch: 7523, Batch Gradient Norm after: 12.391981995828614
Epoch 7524/10000, Prediction Accuracy = 62.924%, Loss = 0.40903310775756835
Epoch: 7524, Batch Gradient Norm: 9.89205091103914
Epoch: 7524, Batch Gradient Norm after: 9.89205091103914
Epoch 7525/10000, Prediction Accuracy = 62.838%, Loss = 0.39271730184555054
Epoch: 7525, Batch Gradient Norm: 9.717386918791416
Epoch: 7525, Batch Gradient Norm after: 9.717386918791416
Epoch 7526/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3917106866836548
Epoch: 7526, Batch Gradient Norm: 9.094613095071496
Epoch: 7526, Batch Gradient Norm after: 9.094613095071496
Epoch 7527/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.38996243476867676
Epoch: 7527, Batch Gradient Norm: 6.710828685083514
Epoch: 7527, Batch Gradient Norm after: 6.710828685083514
Epoch 7528/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3750019371509552
Epoch: 7528, Batch Gradient Norm: 8.282457657755357
Epoch: 7528, Batch Gradient Norm after: 8.282457657755357
Epoch 7529/10000, Prediction Accuracy = 62.944%, Loss = 0.3800820767879486
Epoch: 7529, Batch Gradient Norm: 10.89523910282596
Epoch: 7529, Batch Gradient Norm after: 10.89523910282596
Epoch 7530/10000, Prediction Accuracy = 62.898%, Loss = 0.39764934182167055
Epoch: 7530, Batch Gradient Norm: 11.367251087679639
Epoch: 7530, Batch Gradient Norm after: 11.367251087679639
Epoch 7531/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.39887579083442687
Epoch: 7531, Batch Gradient Norm: 9.704763576729691
Epoch: 7531, Batch Gradient Norm after: 9.704763576729691
Epoch 7532/10000, Prediction Accuracy = 62.884%, Loss = 0.3886531591415405
Epoch: 7532, Batch Gradient Norm: 8.773842742049544
Epoch: 7532, Batch Gradient Norm after: 8.773842742049544
Epoch 7533/10000, Prediction Accuracy = 62.94%, Loss = 0.38178170919418336
Epoch: 7533, Batch Gradient Norm: 10.227489776832066
Epoch: 7533, Batch Gradient Norm after: 10.227489776832066
Epoch 7534/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3918477475643158
Epoch: 7534, Batch Gradient Norm: 8.42258810568734
Epoch: 7534, Batch Gradient Norm after: 8.42258810568734
Epoch 7535/10000, Prediction Accuracy = 62.92%, Loss = 0.3819150686264038
Epoch: 7535, Batch Gradient Norm: 9.618000345065552
Epoch: 7535, Batch Gradient Norm after: 9.618000345065552
Epoch 7536/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.3900649607181549
Epoch: 7536, Batch Gradient Norm: 9.509012376682072
Epoch: 7536, Batch Gradient Norm after: 9.509012376682072
Epoch 7537/10000, Prediction Accuracy = 63.032%, Loss = 0.3886869430541992
Epoch: 7537, Batch Gradient Norm: 10.826470116798522
Epoch: 7537, Batch Gradient Norm after: 10.826470116798522
Epoch 7538/10000, Prediction Accuracy = 63.056%, Loss = 0.3992831766605377
Epoch: 7538, Batch Gradient Norm: 9.128795294045783
Epoch: 7538, Batch Gradient Norm after: 9.128795294045783
Epoch 7539/10000, Prediction Accuracy = 62.902%, Loss = 0.3870434880256653
Epoch: 7539, Batch Gradient Norm: 8.785518815311123
Epoch: 7539, Batch Gradient Norm after: 8.785518815311123
Epoch 7540/10000, Prediction Accuracy = 62.932%, Loss = 0.3834672212600708
Epoch: 7540, Batch Gradient Norm: 9.861826937750285
Epoch: 7540, Batch Gradient Norm after: 9.861826937750285
Epoch 7541/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.38810024261474607
Epoch: 7541, Batch Gradient Norm: 9.906409527864648
Epoch: 7541, Batch Gradient Norm after: 9.906409527864648
Epoch 7542/10000, Prediction Accuracy = 62.786%, Loss = 0.387763524055481
Epoch: 7542, Batch Gradient Norm: 10.252326985859542
Epoch: 7542, Batch Gradient Norm after: 10.252326985859542
Epoch 7543/10000, Prediction Accuracy = 62.77%, Loss = 0.39120709896087646
Epoch: 7543, Batch Gradient Norm: 9.494145207096942
Epoch: 7543, Batch Gradient Norm after: 9.494145207096942
Epoch 7544/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.38887731432914735
Epoch: 7544, Batch Gradient Norm: 11.689007744702455
Epoch: 7544, Batch Gradient Norm after: 11.689007744702455
Epoch 7545/10000, Prediction Accuracy = 62.698%, Loss = 0.4068764626979828
Epoch: 7545, Batch Gradient Norm: 11.037482125041352
Epoch: 7545, Batch Gradient Norm after: 11.037482125041352
Epoch 7546/10000, Prediction Accuracy = 62.90400000000001%, Loss = 0.39832037687301636
Epoch: 7546, Batch Gradient Norm: 10.44221592506738
Epoch: 7546, Batch Gradient Norm after: 10.44221592506738
Epoch 7547/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.39532631635665894
Epoch: 7547, Batch Gradient Norm: 8.771701808025343
Epoch: 7547, Batch Gradient Norm after: 8.771701808025343
Epoch 7548/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.38484657406806944
Epoch: 7548, Batch Gradient Norm: 8.010578252732227
Epoch: 7548, Batch Gradient Norm after: 8.010578252732227
Epoch 7549/10000, Prediction Accuracy = 62.622%, Loss = 0.3793801307678223
Epoch: 7549, Batch Gradient Norm: 9.577247593228742
Epoch: 7549, Batch Gradient Norm after: 9.577247593228742
Epoch 7550/10000, Prediction Accuracy = 62.976%, Loss = 0.3872650027275085
Epoch: 7550, Batch Gradient Norm: 11.991985493319103
Epoch: 7550, Batch Gradient Norm after: 11.991985493319103
Epoch 7551/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.4052752792835236
Epoch: 7551, Batch Gradient Norm: 10.329744541146757
Epoch: 7551, Batch Gradient Norm after: 10.329744541146757
Epoch 7552/10000, Prediction Accuracy = 62.96%, Loss = 0.3931346297264099
Epoch: 7552, Batch Gradient Norm: 9.457441559250826
Epoch: 7552, Batch Gradient Norm after: 9.457441559250826
Epoch 7553/10000, Prediction Accuracy = 62.902%, Loss = 0.3887720167636871
Epoch: 7553, Batch Gradient Norm: 11.441047951852687
Epoch: 7553, Batch Gradient Norm after: 11.441047951852687
Epoch 7554/10000, Prediction Accuracy = 62.826%, Loss = 0.4062517166137695
Epoch: 7554, Batch Gradient Norm: 8.482647913788936
Epoch: 7554, Batch Gradient Norm after: 8.482647913788936
Epoch 7555/10000, Prediction Accuracy = 62.83%, Loss = 0.38230804800987245
Epoch: 7555, Batch Gradient Norm: 9.069504034207547
Epoch: 7555, Batch Gradient Norm after: 9.069504034207547
Epoch 7556/10000, Prediction Accuracy = 62.802%, Loss = 0.38221846222877504
Epoch: 7556, Batch Gradient Norm: 11.639289830122978
Epoch: 7556, Batch Gradient Norm after: 11.639289830122978
Epoch 7557/10000, Prediction Accuracy = 62.852%, Loss = 0.40248733162879946
Epoch: 7557, Batch Gradient Norm: 10.716758880250278
Epoch: 7557, Batch Gradient Norm after: 10.716758880250278
Epoch 7558/10000, Prediction Accuracy = 62.898%, Loss = 0.39619542956352233
Epoch: 7558, Batch Gradient Norm: 10.17185453045062
Epoch: 7558, Batch Gradient Norm after: 10.17185453045062
Epoch 7559/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3938891768455505
Epoch: 7559, Batch Gradient Norm: 9.935507834978038
Epoch: 7559, Batch Gradient Norm after: 9.935507834978038
Epoch 7560/10000, Prediction Accuracy = 62.996%, Loss = 0.3937091648578644
Epoch: 7560, Batch Gradient Norm: 8.622236337259144
Epoch: 7560, Batch Gradient Norm after: 8.622236337259144
Epoch 7561/10000, Prediction Accuracy = 62.814%, Loss = 0.38534368872642516
Epoch: 7561, Batch Gradient Norm: 8.874380943143096
Epoch: 7561, Batch Gradient Norm after: 8.874380943143096
Epoch 7562/10000, Prediction Accuracy = 62.89%, Loss = 0.3841849982738495
Epoch: 7562, Batch Gradient Norm: 9.295637557067218
Epoch: 7562, Batch Gradient Norm after: 9.295637557067218
Epoch 7563/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.38603526949882505
Epoch: 7563, Batch Gradient Norm: 9.969618545415628
Epoch: 7563, Batch Gradient Norm after: 9.969618545415628
Epoch 7564/10000, Prediction Accuracy = 62.8%, Loss = 0.38927125930786133
Epoch: 7564, Batch Gradient Norm: 10.447579579728309
Epoch: 7564, Batch Gradient Norm after: 10.447579579728309
Epoch 7565/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3939808368682861
Epoch: 7565, Batch Gradient Norm: 7.751798839890386
Epoch: 7565, Batch Gradient Norm after: 7.751798839890386
Epoch 7566/10000, Prediction Accuracy = 62.822%, Loss = 0.37654211521148684
Epoch: 7566, Batch Gradient Norm: 6.879942185417369
Epoch: 7566, Batch Gradient Norm after: 6.879942185417369
Epoch 7567/10000, Prediction Accuracy = 62.938%, Loss = 0.37198011875152587
Epoch: 7567, Batch Gradient Norm: 8.967534641415826
Epoch: 7567, Batch Gradient Norm after: 8.967534641415826
Epoch 7568/10000, Prediction Accuracy = 62.88000000000001%, Loss = 0.38236876130104064
Epoch: 7568, Batch Gradient Norm: 9.651021025564427
Epoch: 7568, Batch Gradient Norm after: 9.651021025564427
Epoch 7569/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.38860373497009276
Epoch: 7569, Batch Gradient Norm: 8.784161599808774
Epoch: 7569, Batch Gradient Norm after: 8.784161599808774
Epoch 7570/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.3841099917888641
Epoch: 7570, Batch Gradient Norm: 8.632612767975173
Epoch: 7570, Batch Gradient Norm after: 8.632612767975173
Epoch 7571/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.3828117668628693
Epoch: 7571, Batch Gradient Norm: 9.349419226392305
Epoch: 7571, Batch Gradient Norm after: 9.349419226392305
Epoch 7572/10000, Prediction Accuracy = 62.836%, Loss = 0.3886158406734467
Epoch: 7572, Batch Gradient Norm: 10.348714088002527
Epoch: 7572, Batch Gradient Norm after: 10.348714088002527
Epoch 7573/10000, Prediction Accuracy = 62.89799999999999%, Loss = 0.39344587326049807
Epoch: 7573, Batch Gradient Norm: 10.292616480531969
Epoch: 7573, Batch Gradient Norm after: 10.292616480531969
Epoch 7574/10000, Prediction Accuracy = 62.778%, Loss = 0.3966128170490265
Epoch: 7574, Batch Gradient Norm: 10.261042992901658
Epoch: 7574, Batch Gradient Norm after: 10.261042992901658
Epoch 7575/10000, Prediction Accuracy = 62.69%, Loss = 0.3950917422771454
Epoch: 7575, Batch Gradient Norm: 10.771673435252827
Epoch: 7575, Batch Gradient Norm after: 10.771673435252827
Epoch 7576/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.39844855666160583
Epoch: 7576, Batch Gradient Norm: 8.943765425334098
Epoch: 7576, Batch Gradient Norm after: 8.943765425334098
Epoch 7577/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.38429197669029236
Epoch: 7577, Batch Gradient Norm: 7.749194484503507
Epoch: 7577, Batch Gradient Norm after: 7.749194484503507
Epoch 7578/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.37670308351516724
Epoch: 7578, Batch Gradient Norm: 11.165861194905577
Epoch: 7578, Batch Gradient Norm after: 11.165861194905577
Epoch 7579/10000, Prediction Accuracy = 62.698%, Loss = 0.40081220865249634
Epoch: 7579, Batch Gradient Norm: 10.387193296746238
Epoch: 7579, Batch Gradient Norm after: 10.387193296746238
Epoch 7580/10000, Prediction Accuracy = 62.914%, Loss = 0.390391880273819
Epoch: 7580, Batch Gradient Norm: 9.824575206748523
Epoch: 7580, Batch Gradient Norm after: 9.824575206748523
Epoch 7581/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.3890161752700806
Epoch: 7581, Batch Gradient Norm: 8.791496730529905
Epoch: 7581, Batch Gradient Norm after: 8.791496730529905
Epoch 7582/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.38389134407043457
Epoch: 7582, Batch Gradient Norm: 10.474202642604139
Epoch: 7582, Batch Gradient Norm after: 10.474202642604139
Epoch 7583/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.3928439199924469
Epoch: 7583, Batch Gradient Norm: 10.869147524829323
Epoch: 7583, Batch Gradient Norm after: 10.869147524829323
Epoch 7584/10000, Prediction Accuracy = 62.896%, Loss = 0.3957418084144592
Epoch: 7584, Batch Gradient Norm: 9.238393363658748
Epoch: 7584, Batch Gradient Norm after: 9.238393363658748
Epoch 7585/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.38388762474060056
Epoch: 7585, Batch Gradient Norm: 10.494258044107026
Epoch: 7585, Batch Gradient Norm after: 10.494258044107026
Epoch 7586/10000, Prediction Accuracy = 62.94599999999999%, Loss = 0.3979551374912262
Epoch: 7586, Batch Gradient Norm: 11.989324497283935
Epoch: 7586, Batch Gradient Norm after: 11.989324497283935
Epoch 7587/10000, Prediction Accuracy = 62.73%, Loss = 0.41250311136245726
Epoch: 7587, Batch Gradient Norm: 8.528507925334374
Epoch: 7587, Batch Gradient Norm after: 8.528507925334374
Epoch 7588/10000, Prediction Accuracy = 62.738%, Loss = 0.3803004860877991
Epoch: 7588, Batch Gradient Norm: 9.39567600202952
Epoch: 7588, Batch Gradient Norm after: 9.39567600202952
Epoch 7589/10000, Prediction Accuracy = 63.016%, Loss = 0.3868687808513641
Epoch: 7589, Batch Gradient Norm: 7.992301285435773
Epoch: 7589, Batch Gradient Norm after: 7.992301285435773
Epoch 7590/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3784853219985962
Epoch: 7590, Batch Gradient Norm: 9.714333155981919
Epoch: 7590, Batch Gradient Norm after: 9.714333155981919
Epoch 7591/10000, Prediction Accuracy = 62.826%, Loss = 0.3888340175151825
Epoch: 7591, Batch Gradient Norm: 9.310903689218307
Epoch: 7591, Batch Gradient Norm after: 9.310903689218307
Epoch 7592/10000, Prediction Accuracy = 62.790000000000006%, Loss = 0.38506996631622314
Epoch: 7592, Batch Gradient Norm: 10.37394638284931
Epoch: 7592, Batch Gradient Norm after: 10.37394638284931
Epoch 7593/10000, Prediction Accuracy = 62.8%, Loss = 0.3933294713497162
Epoch: 7593, Batch Gradient Norm: 10.042882719769313
Epoch: 7593, Batch Gradient Norm after: 10.042882719769313
Epoch 7594/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.3923849046230316
Epoch: 7594, Batch Gradient Norm: 8.193802129559668
Epoch: 7594, Batch Gradient Norm after: 8.193802129559668
Epoch 7595/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.37950282692909243
Epoch: 7595, Batch Gradient Norm: 9.052171168095468
Epoch: 7595, Batch Gradient Norm after: 9.052171168095468
Epoch 7596/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.38382408022880554
Epoch: 7596, Batch Gradient Norm: 11.153080896093254
Epoch: 7596, Batch Gradient Norm after: 11.153080896093254
Epoch 7597/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.39983153343200684
Epoch: 7597, Batch Gradient Norm: 10.46861991325072
Epoch: 7597, Batch Gradient Norm after: 10.46861991325072
Epoch 7598/10000, Prediction Accuracy = 62.54599999999999%, Loss = 0.3920241594314575
Epoch: 7598, Batch Gradient Norm: 9.47648079911999
Epoch: 7598, Batch Gradient Norm after: 9.47648079911999
Epoch 7599/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3874274671077728
Epoch: 7599, Batch Gradient Norm: 7.673342477236621
Epoch: 7599, Batch Gradient Norm after: 7.673342477236621
Epoch 7600/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.37528403997421267
Epoch: 7600, Batch Gradient Norm: 8.791028289044887
Epoch: 7600, Batch Gradient Norm after: 8.791028289044887
Epoch 7601/10000, Prediction Accuracy = 62.85999999999999%, Loss = 0.3806479096412659
Epoch: 7601, Batch Gradient Norm: 13.302666982810978
Epoch: 7601, Batch Gradient Norm after: 13.302666982810978
Epoch 7602/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.4139291822910309
Epoch: 7602, Batch Gradient Norm: 9.229826263700255
Epoch: 7602, Batch Gradient Norm after: 9.229826263700255
Epoch 7603/10000, Prediction Accuracy = 62.96%, Loss = 0.38410825133323667
Epoch: 7603, Batch Gradient Norm: 7.30368260146914
Epoch: 7603, Batch Gradient Norm after: 7.30368260146914
Epoch 7604/10000, Prediction Accuracy = 63.041999999999994%, Loss = 0.3736709713935852
Epoch: 7604, Batch Gradient Norm: 9.116384346981214
Epoch: 7604, Batch Gradient Norm after: 9.116384346981214
Epoch 7605/10000, Prediction Accuracy = 62.9%, Loss = 0.38565372824668886
Epoch: 7605, Batch Gradient Norm: 11.024037349652229
Epoch: 7605, Batch Gradient Norm after: 11.024037349652229
Epoch 7606/10000, Prediction Accuracy = 62.862%, Loss = 0.39980204701423644
Epoch: 7606, Batch Gradient Norm: 9.159551591953797
Epoch: 7606, Batch Gradient Norm after: 9.159551591953797
Epoch 7607/10000, Prediction Accuracy = 62.948%, Loss = 0.3840532064437866
Epoch: 7607, Batch Gradient Norm: 9.598041943976598
Epoch: 7607, Batch Gradient Norm after: 9.598041943976598
Epoch 7608/10000, Prediction Accuracy = 62.902%, Loss = 0.38567333817481997
Epoch: 7608, Batch Gradient Norm: 9.154989088596405
Epoch: 7608, Batch Gradient Norm after: 9.154989088596405
Epoch 7609/10000, Prediction Accuracy = 62.9%, Loss = 0.3846775770187378
Epoch: 7609, Batch Gradient Norm: 9.494488989933243
Epoch: 7609, Batch Gradient Norm after: 9.494488989933243
Epoch 7610/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.38700384497642515
Epoch: 7610, Batch Gradient Norm: 8.149524230636041
Epoch: 7610, Batch Gradient Norm after: 8.149524230636041
Epoch 7611/10000, Prediction Accuracy = 62.872%, Loss = 0.37687933444976807
Epoch: 7611, Batch Gradient Norm: 8.209116305876876
Epoch: 7611, Batch Gradient Norm after: 8.209116305876876
Epoch 7612/10000, Prediction Accuracy = 62.77%, Loss = 0.377990198135376
Epoch: 7612, Batch Gradient Norm: 9.764820580646186
Epoch: 7612, Batch Gradient Norm after: 9.764820580646186
Epoch 7613/10000, Prediction Accuracy = 62.924%, Loss = 0.3873038589954376
Epoch: 7613, Batch Gradient Norm: 12.48658305944245
Epoch: 7613, Batch Gradient Norm after: 12.48658305944245
Epoch 7614/10000, Prediction Accuracy = 62.866%, Loss = 0.4086101889610291
Epoch: 7614, Batch Gradient Norm: 10.299087501630645
Epoch: 7614, Batch Gradient Norm after: 10.299087501630645
Epoch 7615/10000, Prediction Accuracy = 62.906000000000006%, Loss = 0.39456013441085813
Epoch: 7615, Batch Gradient Norm: 9.013322964773678
Epoch: 7615, Batch Gradient Norm after: 9.013322964773678
Epoch 7616/10000, Prediction Accuracy = 62.834%, Loss = 0.38736177086830137
Epoch: 7616, Batch Gradient Norm: 10.530869608945105
Epoch: 7616, Batch Gradient Norm after: 10.530869608945105
Epoch 7617/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.39311582446098325
Epoch: 7617, Batch Gradient Norm: 13.505892135082922
Epoch: 7617, Batch Gradient Norm after: 13.505892135082922
Epoch 7618/10000, Prediction Accuracy = 62.878%, Loss = 0.4173863887786865
Epoch: 7618, Batch Gradient Norm: 10.831376781920856
Epoch: 7618, Batch Gradient Norm after: 10.831376781920856
Epoch 7619/10000, Prediction Accuracy = 63.012%, Loss = 0.3989780187606812
Epoch: 7619, Batch Gradient Norm: 8.211892143404919
Epoch: 7619, Batch Gradient Norm after: 8.211892143404919
Epoch 7620/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.37804771065711973
Epoch: 7620, Batch Gradient Norm: 9.54114995579912
Epoch: 7620, Batch Gradient Norm after: 9.54114995579912
Epoch 7621/10000, Prediction Accuracy = 62.872%, Loss = 0.38470911979675293
Epoch: 7621, Batch Gradient Norm: 9.037016869954838
Epoch: 7621, Batch Gradient Norm after: 9.037016869954838
Epoch 7622/10000, Prediction Accuracy = 62.836%, Loss = 0.3852694511413574
Epoch: 7622, Batch Gradient Norm: 9.173668142948388
Epoch: 7622, Batch Gradient Norm after: 9.173668142948388
Epoch 7623/10000, Prediction Accuracy = 63.0%, Loss = 0.3851146936416626
Epoch: 7623, Batch Gradient Norm: 10.683568784273195
Epoch: 7623, Batch Gradient Norm after: 10.683568784273195
Epoch 7624/10000, Prediction Accuracy = 62.98%, Loss = 0.39528741836547854
Epoch: 7624, Batch Gradient Norm: 8.692781126564562
Epoch: 7624, Batch Gradient Norm after: 8.692781126564562
Epoch 7625/10000, Prediction Accuracy = 62.866%, Loss = 0.3820797026157379
Epoch: 7625, Batch Gradient Norm: 11.468334890657333
Epoch: 7625, Batch Gradient Norm after: 11.468334890657333
Epoch 7626/10000, Prediction Accuracy = 62.86999999999999%, Loss = 0.40459423065185546
Epoch: 7626, Batch Gradient Norm: 10.04774451816313
Epoch: 7626, Batch Gradient Norm after: 10.04774451816313
Epoch 7627/10000, Prediction Accuracy = 62.884%, Loss = 0.3905162811279297
Epoch: 7627, Batch Gradient Norm: 9.949105726106309
Epoch: 7627, Batch Gradient Norm after: 9.949105726106309
Epoch 7628/10000, Prediction Accuracy = 62.734%, Loss = 0.3904548704624176
Epoch: 7628, Batch Gradient Norm: 8.991034540049087
Epoch: 7628, Batch Gradient Norm after: 8.991034540049087
Epoch 7629/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.382980877161026
Epoch: 7629, Batch Gradient Norm: 8.252242224234525
Epoch: 7629, Batch Gradient Norm after: 8.252242224234525
Epoch 7630/10000, Prediction Accuracy = 62.898%, Loss = 0.3779986798763275
Epoch: 7630, Batch Gradient Norm: 8.870598961486987
Epoch: 7630, Batch Gradient Norm after: 8.870598961486987
Epoch 7631/10000, Prediction Accuracy = 63.056000000000004%, Loss = 0.38057873249053953
Epoch: 7631, Batch Gradient Norm: 10.105449358205178
Epoch: 7631, Batch Gradient Norm after: 10.105449358205178
Epoch 7632/10000, Prediction Accuracy = 62.83%, Loss = 0.38671339154243467
Epoch: 7632, Batch Gradient Norm: 8.771083387521845
Epoch: 7632, Batch Gradient Norm after: 8.771083387521845
Epoch 7633/10000, Prediction Accuracy = 62.862%, Loss = 0.3805524051189423
Epoch: 7633, Batch Gradient Norm: 8.425259990192203
Epoch: 7633, Batch Gradient Norm after: 8.425259990192203
Epoch 7634/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.37965603470802306
Epoch: 7634, Batch Gradient Norm: 12.06660010545742
Epoch: 7634, Batch Gradient Norm after: 12.06660010545742
Epoch 7635/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.40754441618919374
Epoch: 7635, Batch Gradient Norm: 9.551037484756328
Epoch: 7635, Batch Gradient Norm after: 9.551037484756328
Epoch 7636/10000, Prediction Accuracy = 63.004%, Loss = 0.38529425859451294
Epoch: 7636, Batch Gradient Norm: 8.312679008620472
Epoch: 7636, Batch Gradient Norm after: 8.312679008620472
Epoch 7637/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3784561097621918
Epoch: 7637, Batch Gradient Norm: 9.696021451123153
Epoch: 7637, Batch Gradient Norm after: 9.696021451123153
Epoch 7638/10000, Prediction Accuracy = 62.92%, Loss = 0.3887704014778137
Epoch: 7638, Batch Gradient Norm: 11.051353967284124
Epoch: 7638, Batch Gradient Norm after: 11.051353967284124
Epoch 7639/10000, Prediction Accuracy = 63.112%, Loss = 0.403194785118103
Epoch: 7639, Batch Gradient Norm: 9.78752340953018
Epoch: 7639, Batch Gradient Norm after: 9.78752340953018
Epoch 7640/10000, Prediction Accuracy = 62.86600000000001%, Loss = 0.3893056333065033
Epoch: 7640, Batch Gradient Norm: 10.458524759953873
Epoch: 7640, Batch Gradient Norm after: 10.458524759953873
Epoch 7641/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.3942144513130188
Epoch: 7641, Batch Gradient Norm: 11.016885525789576
Epoch: 7641, Batch Gradient Norm after: 11.016885525789576
Epoch 7642/10000, Prediction Accuracy = 62.94%, Loss = 0.3973181188106537
Epoch: 7642, Batch Gradient Norm: 9.522796633071927
Epoch: 7642, Batch Gradient Norm after: 9.522796633071927
Epoch 7643/10000, Prediction Accuracy = 62.886%, Loss = 0.38567750453948973
Epoch: 7643, Batch Gradient Norm: 8.077917029870331
Epoch: 7643, Batch Gradient Norm after: 8.077917029870331
Epoch 7644/10000, Prediction Accuracy = 62.956%, Loss = 0.37709043025970457
Epoch: 7644, Batch Gradient Norm: 10.465848927744949
Epoch: 7644, Batch Gradient Norm after: 10.465848927744949
Epoch 7645/10000, Prediction Accuracy = 62.852%, Loss = 0.39407089352607727
Epoch: 7645, Batch Gradient Norm: 10.148168959701563
Epoch: 7645, Batch Gradient Norm after: 10.148168959701563
Epoch 7646/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.39085546135902405
Epoch: 7646, Batch Gradient Norm: 10.220261006313207
Epoch: 7646, Batch Gradient Norm after: 10.220261006313207
Epoch 7647/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.39253647327423097
Epoch: 7647, Batch Gradient Norm: 9.546498562379435
Epoch: 7647, Batch Gradient Norm after: 9.546498562379435
Epoch 7648/10000, Prediction Accuracy = 62.83399999999999%, Loss = 0.3891768574714661
Epoch: 7648, Batch Gradient Norm: 8.756676946802298
Epoch: 7648, Batch Gradient Norm after: 8.756676946802298
Epoch 7649/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.38225271701812746
Epoch: 7649, Batch Gradient Norm: 8.09516732267725
Epoch: 7649, Batch Gradient Norm after: 8.09516732267725
Epoch 7650/10000, Prediction Accuracy = 62.878%, Loss = 0.3775184154510498
Epoch: 7650, Batch Gradient Norm: 9.170118543065282
Epoch: 7650, Batch Gradient Norm after: 9.170118543065282
Epoch 7651/10000, Prediction Accuracy = 62.812%, Loss = 0.38446393609046936
Epoch: 7651, Batch Gradient Norm: 9.612486503716603
Epoch: 7651, Batch Gradient Norm after: 9.612486503716603
Epoch 7652/10000, Prediction Accuracy = 62.802%, Loss = 0.3872761070728302
Epoch: 7652, Batch Gradient Norm: 10.289425902569647
Epoch: 7652, Batch Gradient Norm after: 10.289425902569647
Epoch 7653/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.39013110995292666
Epoch: 7653, Batch Gradient Norm: 9.835038558958718
Epoch: 7653, Batch Gradient Norm after: 9.835038558958718
Epoch 7654/10000, Prediction Accuracy = 62.858000000000004%, Loss = 0.38888874650001526
Epoch: 7654, Batch Gradient Norm: 11.134836503764987
Epoch: 7654, Batch Gradient Norm after: 11.134836503764987
Epoch 7655/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3969132721424103
Epoch: 7655, Batch Gradient Norm: 12.00358511674055
Epoch: 7655, Batch Gradient Norm after: 12.00358511674055
Epoch 7656/10000, Prediction Accuracy = 62.824%, Loss = 0.40602163076400755
Epoch: 7656, Batch Gradient Norm: 10.04836618508361
Epoch: 7656, Batch Gradient Norm after: 10.04836618508361
Epoch 7657/10000, Prediction Accuracy = 62.79%, Loss = 0.3912192702293396
Epoch: 7657, Batch Gradient Norm: 7.881174094513355
Epoch: 7657, Batch Gradient Norm after: 7.881174094513355
Epoch 7658/10000, Prediction Accuracy = 62.774%, Loss = 0.3748884260654449
Epoch: 7658, Batch Gradient Norm: 9.269302336424325
Epoch: 7658, Batch Gradient Norm after: 9.269302336424325
Epoch 7659/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3831991612911224
Epoch: 7659, Batch Gradient Norm: 9.606733603062585
Epoch: 7659, Batch Gradient Norm after: 9.606733603062585
Epoch 7660/10000, Prediction Accuracy = 62.944%, Loss = 0.38704548478126527
Epoch: 7660, Batch Gradient Norm: 8.955998702331554
Epoch: 7660, Batch Gradient Norm after: 8.955998702331554
Epoch 7661/10000, Prediction Accuracy = 62.996%, Loss = 0.3825158834457397
Epoch: 7661, Batch Gradient Norm: 9.486404603683146
Epoch: 7661, Batch Gradient Norm after: 9.486404603683146
Epoch 7662/10000, Prediction Accuracy = 62.936%, Loss = 0.38595115542411806
Epoch: 7662, Batch Gradient Norm: 10.383110886561642
Epoch: 7662, Batch Gradient Norm after: 10.383110886561642
Epoch 7663/10000, Prediction Accuracy = 63.08%, Loss = 0.39330976009368895
Epoch: 7663, Batch Gradient Norm: 9.412682262523498
Epoch: 7663, Batch Gradient Norm after: 9.412682262523498
Epoch 7664/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.38484215140342715
Epoch: 7664, Batch Gradient Norm: 11.772778796536336
Epoch: 7664, Batch Gradient Norm after: 11.772778796536336
Epoch 7665/10000, Prediction Accuracy = 62.888%, Loss = 0.4024799346923828
Epoch: 7665, Batch Gradient Norm: 10.489733756025675
Epoch: 7665, Batch Gradient Norm after: 10.489733756025675
Epoch 7666/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3923086762428284
Epoch: 7666, Batch Gradient Norm: 11.338074662410053
Epoch: 7666, Batch Gradient Norm after: 11.338074662410053
Epoch 7667/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3999713122844696
Epoch: 7667, Batch Gradient Norm: 10.104759520284817
Epoch: 7667, Batch Gradient Norm after: 10.104759520284817
Epoch 7668/10000, Prediction Accuracy = 62.98%, Loss = 0.3906249463558197
Epoch: 7668, Batch Gradient Norm: 9.490843861299199
Epoch: 7668, Batch Gradient Norm after: 9.490843861299199
Epoch 7669/10000, Prediction Accuracy = 63.032%, Loss = 0.3853942573070526
Epoch: 7669, Batch Gradient Norm: 9.274970095243887
Epoch: 7669, Batch Gradient Norm after: 9.274970095243887
Epoch 7670/10000, Prediction Accuracy = 62.81%, Loss = 0.38684760928153994
Epoch: 7670, Batch Gradient Norm: 9.631448570720385
Epoch: 7670, Batch Gradient Norm after: 9.631448570720385
Epoch 7671/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.3889708161354065
Epoch: 7671, Batch Gradient Norm: 9.222160444724569
Epoch: 7671, Batch Gradient Norm after: 9.222160444724569
Epoch 7672/10000, Prediction Accuracy = 62.876%, Loss = 0.382079541683197
Epoch: 7672, Batch Gradient Norm: 8.951236843162558
Epoch: 7672, Batch Gradient Norm after: 8.951236843162558
Epoch 7673/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3817222476005554
Epoch: 7673, Batch Gradient Norm: 8.799885221057167
Epoch: 7673, Batch Gradient Norm after: 8.799885221057167
Epoch 7674/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3798575222492218
Epoch: 7674, Batch Gradient Norm: 8.285669060728459
Epoch: 7674, Batch Gradient Norm after: 8.285669060728459
Epoch 7675/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.37958256602287294
Epoch: 7675, Batch Gradient Norm: 8.738374426211516
Epoch: 7675, Batch Gradient Norm after: 8.738374426211516
Epoch 7676/10000, Prediction Accuracy = 62.824%, Loss = 0.38068039417266847
Epoch: 7676, Batch Gradient Norm: 10.004133188584904
Epoch: 7676, Batch Gradient Norm after: 10.004133188584904
Epoch 7677/10000, Prediction Accuracy = 62.884%, Loss = 0.38786640763282776
Epoch: 7677, Batch Gradient Norm: 9.483000156734601
Epoch: 7677, Batch Gradient Norm after: 9.483000156734601
Epoch 7678/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.38415980339050293
Epoch: 7678, Batch Gradient Norm: 13.549276076892289
Epoch: 7678, Batch Gradient Norm after: 13.549276076892289
Epoch 7679/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.42005526423454287
Epoch: 7679, Batch Gradient Norm: 12.067624118625526
Epoch: 7679, Batch Gradient Norm after: 12.067624118625526
Epoch 7680/10000, Prediction Accuracy = 62.95%, Loss = 0.4059654474258423
Epoch: 7680, Batch Gradient Norm: 11.099434448629786
Epoch: 7680, Batch Gradient Norm after: 11.099434448629786
Epoch 7681/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.4024228096008301
Epoch: 7681, Batch Gradient Norm: 6.368928955087823
Epoch: 7681, Batch Gradient Norm after: 6.368928955087823
Epoch 7682/10000, Prediction Accuracy = 63.0%, Loss = 0.367740136384964
Epoch: 7682, Batch Gradient Norm: 6.69553845881442
Epoch: 7682, Batch Gradient Norm after: 6.69553845881442
Epoch 7683/10000, Prediction Accuracy = 62.984%, Loss = 0.37012178301811216
Epoch: 7683, Batch Gradient Norm: 6.836877542659846
Epoch: 7683, Batch Gradient Norm after: 6.836877542659846
Epoch 7684/10000, Prediction Accuracy = 63.032000000000004%, Loss = 0.3693172514438629
Epoch: 7684, Batch Gradient Norm: 8.84875084645064
Epoch: 7684, Batch Gradient Norm after: 8.84875084645064
Epoch 7685/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3781519114971161
Epoch: 7685, Batch Gradient Norm: 12.202977246707853
Epoch: 7685, Batch Gradient Norm after: 12.202977246707853
Epoch 7686/10000, Prediction Accuracy = 62.84000000000001%, Loss = 0.4037539422512054
Epoch: 7686, Batch Gradient Norm: 9.480342764015944
Epoch: 7686, Batch Gradient Norm after: 9.480342764015944
Epoch 7687/10000, Prediction Accuracy = 62.782%, Loss = 0.3840051531791687
Epoch: 7687, Batch Gradient Norm: 8.82155826375805
Epoch: 7687, Batch Gradient Norm after: 8.82155826375805
Epoch 7688/10000, Prediction Accuracy = 62.81%, Loss = 0.38178873658180235
Epoch: 7688, Batch Gradient Norm: 9.548163700278298
Epoch: 7688, Batch Gradient Norm after: 9.548163700278298
Epoch 7689/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.3879533290863037
Epoch: 7689, Batch Gradient Norm: 8.64002376963125
Epoch: 7689, Batch Gradient Norm after: 8.64002376963125
Epoch 7690/10000, Prediction Accuracy = 63.038%, Loss = 0.3811285972595215
Epoch: 7690, Batch Gradient Norm: 11.118252764693533
Epoch: 7690, Batch Gradient Norm after: 11.118252764693533
Epoch 7691/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.39601868987083433
Epoch: 7691, Batch Gradient Norm: 9.265433361488412
Epoch: 7691, Batch Gradient Norm after: 9.265433361488412
Epoch 7692/10000, Prediction Accuracy = 62.94599999999999%, Loss = 0.38111911416053773
Epoch: 7692, Batch Gradient Norm: 10.822324192623581
Epoch: 7692, Batch Gradient Norm after: 10.822324192623581
Epoch 7693/10000, Prediction Accuracy = 62.762%, Loss = 0.3936407923698425
Epoch: 7693, Batch Gradient Norm: 10.519639369026855
Epoch: 7693, Batch Gradient Norm after: 10.519639369026855
Epoch 7694/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.3915986716747284
Epoch: 7694, Batch Gradient Norm: 9.317999650776216
Epoch: 7694, Batch Gradient Norm after: 9.317999650776216
Epoch 7695/10000, Prediction Accuracy = 62.86199999999999%, Loss = 0.3858124613761902
Epoch: 7695, Batch Gradient Norm: 9.607771678440681
Epoch: 7695, Batch Gradient Norm after: 9.607771678440681
Epoch 7696/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.3889737784862518
Epoch: 7696, Batch Gradient Norm: 10.097427082496042
Epoch: 7696, Batch Gradient Norm after: 10.097427082496042
Epoch 7697/10000, Prediction Accuracy = 62.86600000000001%, Loss = 0.3899647116661072
Epoch: 7697, Batch Gradient Norm: 8.870204634098085
Epoch: 7697, Batch Gradient Norm after: 8.870204634098085
Epoch 7698/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3812414586544037
Epoch: 7698, Batch Gradient Norm: 8.0142293354144
Epoch: 7698, Batch Gradient Norm after: 8.0142293354144
Epoch 7699/10000, Prediction Accuracy = 63.08%, Loss = 0.3765600025653839
Epoch: 7699, Batch Gradient Norm: 11.217576162900642
Epoch: 7699, Batch Gradient Norm after: 11.217576162900642
Epoch 7700/10000, Prediction Accuracy = 62.93399999999999%, Loss = 0.3984863758087158
Epoch: 7700, Batch Gradient Norm: 12.227424325254374
Epoch: 7700, Batch Gradient Norm after: 12.227424325254374
Epoch 7701/10000, Prediction Accuracy = 62.826%, Loss = 0.40555196404457095
Epoch: 7701, Batch Gradient Norm: 11.49902323638886
Epoch: 7701, Batch Gradient Norm after: 11.49902323638886
Epoch 7702/10000, Prediction Accuracy = 63.104%, Loss = 0.3977727174758911
Epoch: 7702, Batch Gradient Norm: 11.751124653310967
Epoch: 7702, Batch Gradient Norm after: 11.751124653310967
Epoch 7703/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.4035764694213867
Epoch: 7703, Batch Gradient Norm: 7.880240849302813
Epoch: 7703, Batch Gradient Norm after: 7.880240849302813
Epoch 7704/10000, Prediction Accuracy = 62.92%, Loss = 0.375466525554657
Epoch: 7704, Batch Gradient Norm: 6.809554724177163
Epoch: 7704, Batch Gradient Norm after: 6.809554724177163
Epoch 7705/10000, Prediction Accuracy = 62.848%, Loss = 0.36980178356170657
Epoch: 7705, Batch Gradient Norm: 9.780855985295357
Epoch: 7705, Batch Gradient Norm after: 9.780855985295357
Epoch 7706/10000, Prediction Accuracy = 62.968%, Loss = 0.38461349010467527
Epoch: 7706, Batch Gradient Norm: 9.31354031182517
Epoch: 7706, Batch Gradient Norm after: 9.31354031182517
Epoch 7707/10000, Prediction Accuracy = 62.854%, Loss = 0.38388427495956423
Epoch: 7707, Batch Gradient Norm: 10.39390047415926
Epoch: 7707, Batch Gradient Norm after: 10.39390047415926
Epoch 7708/10000, Prediction Accuracy = 62.852%, Loss = 0.3923344433307648
Epoch: 7708, Batch Gradient Norm: 9.595244702993554
Epoch: 7708, Batch Gradient Norm after: 9.595244702993554
Epoch 7709/10000, Prediction Accuracy = 63.004%, Loss = 0.3882656455039978
Epoch: 7709, Batch Gradient Norm: 10.733825382122655
Epoch: 7709, Batch Gradient Norm after: 10.733825382122655
Epoch 7710/10000, Prediction Accuracy = 62.888%, Loss = 0.3961085557937622
Epoch: 7710, Batch Gradient Norm: 11.506083956087656
Epoch: 7710, Batch Gradient Norm after: 11.506083956087656
Epoch 7711/10000, Prediction Accuracy = 62.976%, Loss = 0.3975278675556183
Epoch: 7711, Batch Gradient Norm: 11.193253425642437
Epoch: 7711, Batch Gradient Norm after: 11.193253425642437
Epoch 7712/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.39525766372680665
Epoch: 7712, Batch Gradient Norm: 9.474984336348454
Epoch: 7712, Batch Gradient Norm after: 9.474984336348454
Epoch 7713/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3864006519317627
Epoch: 7713, Batch Gradient Norm: 7.48275841469839
Epoch: 7713, Batch Gradient Norm after: 7.48275841469839
Epoch 7714/10000, Prediction Accuracy = 63.04%, Loss = 0.3747963845729828
Epoch: 7714, Batch Gradient Norm: 9.880846076102111
Epoch: 7714, Batch Gradient Norm after: 9.880846076102111
Epoch 7715/10000, Prediction Accuracy = 62.896%, Loss = 0.3948956549167633
Epoch: 7715, Batch Gradient Norm: 7.698080383051158
Epoch: 7715, Batch Gradient Norm after: 7.698080383051158
Epoch 7716/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3756648600101471
Epoch: 7716, Batch Gradient Norm: 8.23677348194237
Epoch: 7716, Batch Gradient Norm after: 8.23677348194237
Epoch 7717/10000, Prediction Accuracy = 62.898%, Loss = 0.3762017250061035
Epoch: 7717, Batch Gradient Norm: 10.109039626610002
Epoch: 7717, Batch Gradient Norm after: 10.109039626610002
Epoch 7718/10000, Prediction Accuracy = 62.806%, Loss = 0.39161792397499084
Epoch: 7718, Batch Gradient Norm: 9.156428548109481
Epoch: 7718, Batch Gradient Norm after: 9.156428548109481
Epoch 7719/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.38403240442276
Epoch: 7719, Batch Gradient Norm: 10.436837140641947
Epoch: 7719, Batch Gradient Norm after: 10.436837140641947
Epoch 7720/10000, Prediction Accuracy = 62.9%, Loss = 0.3916549026966095
Epoch: 7720, Batch Gradient Norm: 9.177998896194676
Epoch: 7720, Batch Gradient Norm after: 9.177998896194676
Epoch 7721/10000, Prediction Accuracy = 62.926%, Loss = 0.38454360961914064
Epoch: 7721, Batch Gradient Norm: 10.532837943410609
Epoch: 7721, Batch Gradient Norm after: 10.532837943410609
Epoch 7722/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.39480674266815186
Epoch: 7722, Batch Gradient Norm: 11.243859110798969
Epoch: 7722, Batch Gradient Norm after: 11.243859110798969
Epoch 7723/10000, Prediction Accuracy = 62.854%, Loss = 0.3992133498191833
Epoch: 7723, Batch Gradient Norm: 9.084391878379236
Epoch: 7723, Batch Gradient Norm after: 9.084391878379236
Epoch 7724/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.3796448290348053
Epoch: 7724, Batch Gradient Norm: 9.785324034923102
Epoch: 7724, Batch Gradient Norm after: 9.785324034923102
Epoch 7725/10000, Prediction Accuracy = 62.99000000000001%, Loss = 0.38537588715553284
Epoch: 7725, Batch Gradient Norm: 10.89464131495194
Epoch: 7725, Batch Gradient Norm after: 10.89464131495194
Epoch 7726/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.39227476716041565
Epoch: 7726, Batch Gradient Norm: 9.208020818283766
Epoch: 7726, Batch Gradient Norm after: 9.208020818283766
Epoch 7727/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.38297701478004453
Epoch: 7727, Batch Gradient Norm: 11.699037189150198
Epoch: 7727, Batch Gradient Norm after: 11.699037189150198
Epoch 7728/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.40179550647735596
Epoch: 7728, Batch Gradient Norm: 10.634515065251932
Epoch: 7728, Batch Gradient Norm after: 10.634515065251932
Epoch 7729/10000, Prediction Accuracy = 63.1%, Loss = 0.39401776194572447
Epoch: 7729, Batch Gradient Norm: 10.984755911191249
Epoch: 7729, Batch Gradient Norm after: 10.984755911191249
Epoch 7730/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.39785621166229246
Epoch: 7730, Batch Gradient Norm: 7.850161293451565
Epoch: 7730, Batch Gradient Norm after: 7.850161293451565
Epoch 7731/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3767599582672119
Epoch: 7731, Batch Gradient Norm: 9.81743044031634
Epoch: 7731, Batch Gradient Norm after: 9.81743044031634
Epoch 7732/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3883331775665283
Epoch: 7732, Batch Gradient Norm: 12.54312792608646
Epoch: 7732, Batch Gradient Norm after: 12.54312792608646
Epoch 7733/10000, Prediction Accuracy = 62.726%, Loss = 0.4152442395687103
Epoch: 7733, Batch Gradient Norm: 8.982751417726424
Epoch: 7733, Batch Gradient Norm after: 8.982751417726424
Epoch 7734/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.3853640854358673
Epoch: 7734, Batch Gradient Norm: 8.055989520329522
Epoch: 7734, Batch Gradient Norm after: 8.055989520329522
Epoch 7735/10000, Prediction Accuracy = 63.074%, Loss = 0.3778068542480469
Epoch: 7735, Batch Gradient Norm: 8.3936558751013
Epoch: 7735, Batch Gradient Norm after: 8.3936558751013
Epoch 7736/10000, Prediction Accuracy = 62.872%, Loss = 0.37993186712265015
Epoch: 7736, Batch Gradient Norm: 10.817280199644992
Epoch: 7736, Batch Gradient Norm after: 10.817280199644992
Epoch 7737/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.39098039269447327
Epoch: 7737, Batch Gradient Norm: 10.592409649754655
Epoch: 7737, Batch Gradient Norm after: 10.592409649754655
Epoch 7738/10000, Prediction Accuracy = 62.746%, Loss = 0.390770947933197
Epoch: 7738, Batch Gradient Norm: 6.570122414351041
Epoch: 7738, Batch Gradient Norm after: 6.570122414351041
Epoch 7739/10000, Prediction Accuracy = 62.98%, Loss = 0.3678430914878845
Epoch: 7739, Batch Gradient Norm: 8.196767734190768
Epoch: 7739, Batch Gradient Norm after: 8.196767734190768
Epoch 7740/10000, Prediction Accuracy = 62.988%, Loss = 0.37712318897247316
Epoch: 7740, Batch Gradient Norm: 9.703424835445327
Epoch: 7740, Batch Gradient Norm after: 9.703424835445327
Epoch 7741/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.39249828457832336
Epoch: 7741, Batch Gradient Norm: 9.367415824998982
Epoch: 7741, Batch Gradient Norm after: 9.367415824998982
Epoch 7742/10000, Prediction Accuracy = 62.767999999999994%, Loss = 0.38635531067848206
Epoch: 7742, Batch Gradient Norm: 12.589888360603112
Epoch: 7742, Batch Gradient Norm after: 12.589888360603112
Epoch 7743/10000, Prediction Accuracy = 62.94799999999999%, Loss = 0.408251017332077
Epoch: 7743, Batch Gradient Norm: 10.88019964587203
Epoch: 7743, Batch Gradient Norm after: 10.88019964587203
Epoch 7744/10000, Prediction Accuracy = 62.91600000000001%, Loss = 0.39606893658638
Epoch: 7744, Batch Gradient Norm: 8.291281645948073
Epoch: 7744, Batch Gradient Norm after: 8.291281645948073
Epoch 7745/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.37979472875595094
Epoch: 7745, Batch Gradient Norm: 9.281461713642834
Epoch: 7745, Batch Gradient Norm after: 9.281461713642834
Epoch 7746/10000, Prediction Accuracy = 63.064%, Loss = 0.3827490210533142
Epoch: 7746, Batch Gradient Norm: 8.857996667256815
Epoch: 7746, Batch Gradient Norm after: 8.857996667256815
Epoch 7747/10000, Prediction Accuracy = 62.988%, Loss = 0.37847896814346316
Epoch: 7747, Batch Gradient Norm: 11.079510641174403
Epoch: 7747, Batch Gradient Norm after: 11.079510641174403
Epoch 7748/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.39815521240234375
Epoch: 7748, Batch Gradient Norm: 8.50049815731079
Epoch: 7748, Batch Gradient Norm after: 8.50049815731079
Epoch 7749/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.37693297266960146
Epoch: 7749, Batch Gradient Norm: 7.44333814735775
Epoch: 7749, Batch Gradient Norm after: 7.44333814735775
Epoch 7750/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3727240741252899
Epoch: 7750, Batch Gradient Norm: 11.503398488447015
Epoch: 7750, Batch Gradient Norm after: 11.503398488447015
Epoch 7751/10000, Prediction Accuracy = 62.864%, Loss = 0.3974673509597778
Epoch: 7751, Batch Gradient Norm: 9.091294684578912
Epoch: 7751, Batch Gradient Norm after: 9.091294684578912
Epoch 7752/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.38020854592323305
Epoch: 7752, Batch Gradient Norm: 11.591787272938788
Epoch: 7752, Batch Gradient Norm after: 11.591787272938788
Epoch 7753/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.39794310331344607
Epoch: 7753, Batch Gradient Norm: 9.911466532707898
Epoch: 7753, Batch Gradient Norm after: 9.911466532707898
Epoch 7754/10000, Prediction Accuracy = 63.124%, Loss = 0.38628884553909304
Epoch: 7754, Batch Gradient Norm: 10.595589467796785
Epoch: 7754, Batch Gradient Norm after: 10.595589467796785
Epoch 7755/10000, Prediction Accuracy = 62.836%, Loss = 0.3905600905418396
Epoch: 7755, Batch Gradient Norm: 10.953467506223637
Epoch: 7755, Batch Gradient Norm after: 10.953467506223637
Epoch 7756/10000, Prediction Accuracy = 62.92%, Loss = 0.39321957230567933
Epoch: 7756, Batch Gradient Norm: 8.149126529865857
Epoch: 7756, Batch Gradient Norm after: 8.149126529865857
Epoch 7757/10000, Prediction Accuracy = 62.96%, Loss = 0.3750789642333984
Epoch: 7757, Batch Gradient Norm: 8.858202134102772
Epoch: 7757, Batch Gradient Norm after: 8.858202134102772
Epoch 7758/10000, Prediction Accuracy = 63.11%, Loss = 0.37826300859451295
Epoch: 7758, Batch Gradient Norm: 12.024395763344172
Epoch: 7758, Batch Gradient Norm after: 12.024395763344172
Epoch 7759/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.40422245264053347
Epoch: 7759, Batch Gradient Norm: 8.703059028901508
Epoch: 7759, Batch Gradient Norm after: 8.703059028901508
Epoch 7760/10000, Prediction Accuracy = 62.94%, Loss = 0.3819404780864716
Epoch: 7760, Batch Gradient Norm: 7.9610661689655675
Epoch: 7760, Batch Gradient Norm after: 7.9610661689655675
Epoch 7761/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.3758435308933258
Epoch: 7761, Batch Gradient Norm: 9.533682326006046
Epoch: 7761, Batch Gradient Norm after: 9.533682326006046
Epoch 7762/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38486374616622926
Epoch: 7762, Batch Gradient Norm: 7.332413930124053
Epoch: 7762, Batch Gradient Norm after: 7.332413930124053
Epoch 7763/10000, Prediction Accuracy = 62.96600000000001%, Loss = 0.3706323802471161
Epoch: 7763, Batch Gradient Norm: 10.540543970004867
Epoch: 7763, Batch Gradient Norm after: 10.540543970004867
Epoch 7764/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.3927542746067047
Epoch: 7764, Batch Gradient Norm: 14.335140683820343
Epoch: 7764, Batch Gradient Norm after: 14.335140683820343
Epoch 7765/10000, Prediction Accuracy = 62.77%, Loss = 0.4247256875038147
Epoch: 7765, Batch Gradient Norm: 8.727304177610899
Epoch: 7765, Batch Gradient Norm after: 8.727304177610899
Epoch 7766/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.379637748003006
Epoch: 7766, Batch Gradient Norm: 7.731278517981445
Epoch: 7766, Batch Gradient Norm after: 7.731278517981445
Epoch 7767/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3727481484413147
Epoch: 7767, Batch Gradient Norm: 7.543989219599789
Epoch: 7767, Batch Gradient Norm after: 7.543989219599789
Epoch 7768/10000, Prediction Accuracy = 62.93000000000001%, Loss = 0.37204042077064514
Epoch: 7768, Batch Gradient Norm: 9.940143977653252
Epoch: 7768, Batch Gradient Norm after: 9.940143977653252
Epoch 7769/10000, Prediction Accuracy = 63.076%, Loss = 0.3870174646377563
Epoch: 7769, Batch Gradient Norm: 10.41792097512123
Epoch: 7769, Batch Gradient Norm after: 10.41792097512123
Epoch 7770/10000, Prediction Accuracy = 62.952%, Loss = 0.39197722673416135
Epoch: 7770, Batch Gradient Norm: 10.56936494419773
Epoch: 7770, Batch Gradient Norm after: 10.56936494419773
Epoch 7771/10000, Prediction Accuracy = 62.96199999999999%, Loss = 0.39009440541267393
Epoch: 7771, Batch Gradient Norm: 11.419033594481785
Epoch: 7771, Batch Gradient Norm after: 11.419033594481785
Epoch 7772/10000, Prediction Accuracy = 63.222%, Loss = 0.4006722331047058
Epoch: 7772, Batch Gradient Norm: 9.175557357153624
Epoch: 7772, Batch Gradient Norm after: 9.175557357153624
Epoch 7773/10000, Prediction Accuracy = 62.932%, Loss = 0.38359623551368716
Epoch: 7773, Batch Gradient Norm: 11.508420638002121
Epoch: 7773, Batch Gradient Norm after: 11.508420638002121
Epoch 7774/10000, Prediction Accuracy = 62.852%, Loss = 0.3990428686141968
Epoch: 7774, Batch Gradient Norm: 9.762572764171134
Epoch: 7774, Batch Gradient Norm after: 9.762572764171134
Epoch 7775/10000, Prediction Accuracy = 62.932%, Loss = 0.3872120201587677
Epoch: 7775, Batch Gradient Norm: 9.457051953156247
Epoch: 7775, Batch Gradient Norm after: 9.457051953156247
Epoch 7776/10000, Prediction Accuracy = 63.036%, Loss = 0.38329697847366334
Epoch: 7776, Batch Gradient Norm: 7.908314198988764
Epoch: 7776, Batch Gradient Norm after: 7.908314198988764
Epoch 7777/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.3736570656299591
Epoch: 7777, Batch Gradient Norm: 8.789820734416196
Epoch: 7777, Batch Gradient Norm after: 8.789820734416196
Epoch 7778/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.37996660470962523
Epoch: 7778, Batch Gradient Norm: 9.164463717065951
Epoch: 7778, Batch Gradient Norm after: 9.164463717065951
Epoch 7779/10000, Prediction Accuracy = 63.186%, Loss = 0.38094688653945924
Epoch: 7779, Batch Gradient Norm: 10.51174105201208
Epoch: 7779, Batch Gradient Norm after: 10.51174105201208
Epoch 7780/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3884191870689392
Epoch: 7780, Batch Gradient Norm: 10.866177101333568
Epoch: 7780, Batch Gradient Norm after: 10.866177101333568
Epoch 7781/10000, Prediction Accuracy = 62.766%, Loss = 0.395118248462677
Epoch: 7781, Batch Gradient Norm: 10.571015976036001
Epoch: 7781, Batch Gradient Norm after: 10.571015976036001
Epoch 7782/10000, Prediction Accuracy = 62.874%, Loss = 0.38943994641304014
Epoch: 7782, Batch Gradient Norm: 11.626114386865162
Epoch: 7782, Batch Gradient Norm after: 11.626114386865162
Epoch 7783/10000, Prediction Accuracy = 63.086%, Loss = 0.3997905492782593
Epoch: 7783, Batch Gradient Norm: 9.308676587507355
Epoch: 7783, Batch Gradient Norm after: 9.308676587507355
Epoch 7784/10000, Prediction Accuracy = 63.074%, Loss = 0.38277862668037416
Epoch: 7784, Batch Gradient Norm: 8.27961648153262
Epoch: 7784, Batch Gradient Norm after: 8.27961648153262
Epoch 7785/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3783613622188568
Epoch: 7785, Batch Gradient Norm: 9.992420996192768
Epoch: 7785, Batch Gradient Norm after: 9.992420996192768
Epoch 7786/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3854557275772095
Epoch: 7786, Batch Gradient Norm: 12.799038436358426
Epoch: 7786, Batch Gradient Norm after: 12.799038436358426
Epoch 7787/10000, Prediction Accuracy = 62.898%, Loss = 0.4102342128753662
Epoch: 7787, Batch Gradient Norm: 10.548089989851887
Epoch: 7787, Batch Gradient Norm after: 10.548089989851887
Epoch 7788/10000, Prediction Accuracy = 62.872%, Loss = 0.3923460364341736
Epoch: 7788, Batch Gradient Norm: 9.657036900009576
Epoch: 7788, Batch Gradient Norm after: 9.657036900009576
Epoch 7789/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3885723650455475
Epoch: 7789, Batch Gradient Norm: 8.70534509922557
Epoch: 7789, Batch Gradient Norm after: 8.70534509922557
Epoch 7790/10000, Prediction Accuracy = 63.080000000000005%, Loss = 0.3804949879646301
Epoch: 7790, Batch Gradient Norm: 9.216679133983018
Epoch: 7790, Batch Gradient Norm after: 9.216679133983018
Epoch 7791/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.3818157553672791
Epoch: 7791, Batch Gradient Norm: 9.269461666987702
Epoch: 7791, Batch Gradient Norm after: 9.269461666987702
Epoch 7792/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.3820653438568115
Epoch: 7792, Batch Gradient Norm: 7.057404721906588
Epoch: 7792, Batch Gradient Norm after: 7.057404721906588
Epoch 7793/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3714483916759491
Epoch: 7793, Batch Gradient Norm: 7.983221846100526
Epoch: 7793, Batch Gradient Norm after: 7.983221846100526
Epoch 7794/10000, Prediction Accuracy = 62.944%, Loss = 0.3732965290546417
Epoch: 7794, Batch Gradient Norm: 13.634115662089028
Epoch: 7794, Batch Gradient Norm after: 13.634115662089028
Epoch 7795/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.4150235176086426
Epoch: 7795, Batch Gradient Norm: 11.133758372846643
Epoch: 7795, Batch Gradient Norm after: 11.133758372846643
Epoch 7796/10000, Prediction Accuracy = 62.83%, Loss = 0.3943823277950287
Epoch: 7796, Batch Gradient Norm: 8.11287309612068
Epoch: 7796, Batch Gradient Norm after: 8.11287309612068
Epoch 7797/10000, Prediction Accuracy = 62.886%, Loss = 0.376857602596283
Epoch: 7797, Batch Gradient Norm: 9.684000290237133
Epoch: 7797, Batch Gradient Norm after: 9.684000290237133
Epoch 7798/10000, Prediction Accuracy = 62.89%, Loss = 0.3850845158100128
Epoch: 7798, Batch Gradient Norm: 8.126154261919282
Epoch: 7798, Batch Gradient Norm after: 8.126154261919282
Epoch 7799/10000, Prediction Accuracy = 62.968%, Loss = 0.37509074807167053
Epoch: 7799, Batch Gradient Norm: 6.785600309493715
Epoch: 7799, Batch Gradient Norm after: 6.785600309493715
Epoch 7800/10000, Prediction Accuracy = 63.178%, Loss = 0.36661416888237
Epoch: 7800, Batch Gradient Norm: 9.709750482187836
Epoch: 7800, Batch Gradient Norm after: 9.709750482187836
Epoch 7801/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.38609052300453184
Epoch: 7801, Batch Gradient Norm: 10.73618573846209
Epoch: 7801, Batch Gradient Norm after: 10.73618573846209
Epoch 7802/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.3910686433315277
Epoch: 7802, Batch Gradient Norm: 10.017851593262584
Epoch: 7802, Batch Gradient Norm after: 10.017851593262584
Epoch 7803/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.38787112236022947
Epoch: 7803, Batch Gradient Norm: 7.520873224640512
Epoch: 7803, Batch Gradient Norm after: 7.520873224640512
Epoch 7804/10000, Prediction Accuracy = 63.004%, Loss = 0.3711107671260834
Epoch: 7804, Batch Gradient Norm: 8.548313324035089
Epoch: 7804, Batch Gradient Norm after: 8.548313324035089
Epoch 7805/10000, Prediction Accuracy = 62.968%, Loss = 0.3761836588382721
Epoch: 7805, Batch Gradient Norm: 9.035437918234166
Epoch: 7805, Batch Gradient Norm after: 9.035437918234166
Epoch 7806/10000, Prediction Accuracy = 62.888%, Loss = 0.37865283489227297
Epoch: 7806, Batch Gradient Norm: 10.29681470131428
Epoch: 7806, Batch Gradient Norm after: 10.29681470131428
Epoch 7807/10000, Prediction Accuracy = 62.824%, Loss = 0.3887420237064362
Epoch: 7807, Batch Gradient Norm: 10.774465462713518
Epoch: 7807, Batch Gradient Norm after: 10.774465462713518
Epoch 7808/10000, Prediction Accuracy = 62.912%, Loss = 0.3925504744052887
Epoch: 7808, Batch Gradient Norm: 12.810038495031637
Epoch: 7808, Batch Gradient Norm after: 12.810038495031637
Epoch 7809/10000, Prediction Accuracy = 63.004%, Loss = 0.40818406343460084
Epoch: 7809, Batch Gradient Norm: 11.076810923261567
Epoch: 7809, Batch Gradient Norm after: 11.076810923261567
Epoch 7810/10000, Prediction Accuracy = 63.016%, Loss = 0.3964857578277588
Epoch: 7810, Batch Gradient Norm: 10.300544083905535
Epoch: 7810, Batch Gradient Norm after: 10.300544083905535
Epoch 7811/10000, Prediction Accuracy = 62.888%, Loss = 0.3933180272579193
Epoch: 7811, Batch Gradient Norm: 8.225290978361398
Epoch: 7811, Batch Gradient Norm after: 8.225290978361398
Epoch 7812/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.3770204961299896
Epoch: 7812, Batch Gradient Norm: 8.6101592610529
Epoch: 7812, Batch Gradient Norm after: 8.6101592610529
Epoch 7813/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.3773582100868225
Epoch: 7813, Batch Gradient Norm: 8.52843335586015
Epoch: 7813, Batch Gradient Norm after: 8.52843335586015
Epoch 7814/10000, Prediction Accuracy = 62.928%, Loss = 0.37928585410118104
Epoch: 7814, Batch Gradient Norm: 9.227721776999052
Epoch: 7814, Batch Gradient Norm after: 9.227721776999052
Epoch 7815/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.3836071491241455
Epoch: 7815, Batch Gradient Norm: 10.697014168786648
Epoch: 7815, Batch Gradient Norm after: 10.697014168786648
Epoch 7816/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.39267381429672243
Epoch: 7816, Batch Gradient Norm: 10.366400197573077
Epoch: 7816, Batch Gradient Norm after: 10.366400197573077
Epoch 7817/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.3887769401073456
Epoch: 7817, Batch Gradient Norm: 8.7028302346799
Epoch: 7817, Batch Gradient Norm after: 8.7028302346799
Epoch 7818/10000, Prediction Accuracy = 62.938%, Loss = 0.3784528911113739
Epoch: 7818, Batch Gradient Norm: 8.184868968701975
Epoch: 7818, Batch Gradient Norm after: 8.184868968701975
Epoch 7819/10000, Prediction Accuracy = 62.948%, Loss = 0.375122594833374
Epoch: 7819, Batch Gradient Norm: 8.724909885507806
Epoch: 7819, Batch Gradient Norm after: 8.724909885507806
Epoch 7820/10000, Prediction Accuracy = 63.024%, Loss = 0.3791701078414917
Epoch: 7820, Batch Gradient Norm: 10.116780244961626
Epoch: 7820, Batch Gradient Norm after: 10.116780244961626
Epoch 7821/10000, Prediction Accuracy = 62.87800000000001%, Loss = 0.38524687886238096
Epoch: 7821, Batch Gradient Norm: 10.64380833603949
Epoch: 7821, Batch Gradient Norm after: 10.64380833603949
Epoch 7822/10000, Prediction Accuracy = 63.072%, Loss = 0.3874419808387756
Epoch: 7822, Batch Gradient Norm: 9.579916576253153
Epoch: 7822, Batch Gradient Norm after: 9.579916576253153
Epoch 7823/10000, Prediction Accuracy = 62.79799999999999%, Loss = 0.3812819063663483
Epoch: 7823, Batch Gradient Norm: 9.090705102524453
Epoch: 7823, Batch Gradient Norm after: 9.090705102524453
Epoch 7824/10000, Prediction Accuracy = 62.972%, Loss = 0.3806150436401367
Epoch: 7824, Batch Gradient Norm: 11.193407920486889
Epoch: 7824, Batch Gradient Norm after: 11.193407920486889
Epoch 7825/10000, Prediction Accuracy = 62.96%, Loss = 0.3944493234157562
Epoch: 7825, Batch Gradient Norm: 10.179844958337204
Epoch: 7825, Batch Gradient Norm after: 10.179844958337204
Epoch 7826/10000, Prediction Accuracy = 62.968%, Loss = 0.3882722020149231
Epoch: 7826, Batch Gradient Norm: 9.17818811610198
Epoch: 7826, Batch Gradient Norm after: 9.17818811610198
Epoch 7827/10000, Prediction Accuracy = 63.056000000000004%, Loss = 0.3800240993499756
Epoch: 7827, Batch Gradient Norm: 11.912876346257747
Epoch: 7827, Batch Gradient Norm after: 11.912876346257747
Epoch 7828/10000, Prediction Accuracy = 63.038%, Loss = 0.4037883341312408
Epoch: 7828, Batch Gradient Norm: 9.173675341228087
Epoch: 7828, Batch Gradient Norm after: 9.173675341228087
Epoch 7829/10000, Prediction Accuracy = 62.876%, Loss = 0.38317573070526123
Epoch: 7829, Batch Gradient Norm: 7.4467227053399405
Epoch: 7829, Batch Gradient Norm after: 7.4467227053399405
Epoch 7830/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.3706978440284729
Epoch: 7830, Batch Gradient Norm: 7.616209438544566
Epoch: 7830, Batch Gradient Norm after: 7.616209438544566
Epoch 7831/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3706361472606659
Epoch: 7831, Batch Gradient Norm: 9.738869628212054
Epoch: 7831, Batch Gradient Norm after: 9.738869628212054
Epoch 7832/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.38493332266807556
Epoch: 7832, Batch Gradient Norm: 11.500833421402278
Epoch: 7832, Batch Gradient Norm after: 11.500833421402278
Epoch 7833/10000, Prediction Accuracy = 62.888%, Loss = 0.39715710282325745
Epoch: 7833, Batch Gradient Norm: 11.598651053465204
Epoch: 7833, Batch Gradient Norm after: 11.598651053465204
Epoch 7834/10000, Prediction Accuracy = 62.848%, Loss = 0.403288745880127
Epoch: 7834, Batch Gradient Norm: 9.714724985665898
Epoch: 7834, Batch Gradient Norm after: 9.714724985665898
Epoch 7835/10000, Prediction Accuracy = 63.007999999999996%, Loss = 0.38467429876327514
Epoch: 7835, Batch Gradient Norm: 10.136273926322101
Epoch: 7835, Batch Gradient Norm after: 10.136273926322101
Epoch 7836/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.3889558434486389
Epoch: 7836, Batch Gradient Norm: 10.691896731387212
Epoch: 7836, Batch Gradient Norm after: 10.691896731387212
Epoch 7837/10000, Prediction Accuracy = 63.0%, Loss = 0.38972078561782836
Epoch: 7837, Batch Gradient Norm: 6.74483365378788
Epoch: 7837, Batch Gradient Norm after: 6.74483365378788
Epoch 7838/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.36616492867469785
Epoch: 7838, Batch Gradient Norm: 8.408109814645794
Epoch: 7838, Batch Gradient Norm after: 8.408109814645794
Epoch 7839/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3777224123477936
Epoch: 7839, Batch Gradient Norm: 10.055037186326732
Epoch: 7839, Batch Gradient Norm after: 10.055037186326732
Epoch 7840/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.38773428797721865
Epoch: 7840, Batch Gradient Norm: 8.88773723231737
Epoch: 7840, Batch Gradient Norm after: 8.88773723231737
Epoch 7841/10000, Prediction Accuracy = 62.984%, Loss = 0.3778483748435974
Epoch: 7841, Batch Gradient Norm: 10.845625932938944
Epoch: 7841, Batch Gradient Norm after: 10.845625932938944
Epoch 7842/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.3906327962875366
Epoch: 7842, Batch Gradient Norm: 12.016759565238562
Epoch: 7842, Batch Gradient Norm after: 12.016759565238562
Epoch 7843/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.40454707145690916
Epoch: 7843, Batch Gradient Norm: 11.682445953827978
Epoch: 7843, Batch Gradient Norm after: 11.682445953827978
Epoch 7844/10000, Prediction Accuracy = 62.864%, Loss = 0.4068098306655884
Epoch: 7844, Batch Gradient Norm: 9.38052807097033
Epoch: 7844, Batch Gradient Norm after: 9.38052807097033
Epoch 7845/10000, Prediction Accuracy = 63.04%, Loss = 0.38219982385635376
Epoch: 7845, Batch Gradient Norm: 7.883113174343662
Epoch: 7845, Batch Gradient Norm after: 7.883113174343662
Epoch 7846/10000, Prediction Accuracy = 63.062%, Loss = 0.37055519223213196
Epoch: 7846, Batch Gradient Norm: 9.97659600477215
Epoch: 7846, Batch Gradient Norm after: 9.97659600477215
Epoch 7847/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.3833585321903229
Epoch: 7847, Batch Gradient Norm: 10.691365181483102
Epoch: 7847, Batch Gradient Norm after: 10.691365181483102
Epoch 7848/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3900212049484253
Epoch: 7848, Batch Gradient Norm: 9.335004114118755
Epoch: 7848, Batch Gradient Norm after: 9.335004114118755
Epoch 7849/10000, Prediction Accuracy = 63.052%, Loss = 0.38126906752586365
Epoch: 7849, Batch Gradient Norm: 10.765661691128297
Epoch: 7849, Batch Gradient Norm after: 10.765661691128297
Epoch 7850/10000, Prediction Accuracy = 62.912%, Loss = 0.3939662933349609
Epoch: 7850, Batch Gradient Norm: 12.115709381338132
Epoch: 7850, Batch Gradient Norm after: 12.115709381338132
Epoch 7851/10000, Prediction Accuracy = 62.926%, Loss = 0.40433823466300967
Epoch: 7851, Batch Gradient Norm: 9.346078384574598
Epoch: 7851, Batch Gradient Norm after: 9.346078384574598
Epoch 7852/10000, Prediction Accuracy = 63.074%, Loss = 0.37995840311050416
Epoch: 7852, Batch Gradient Norm: 9.48694021964243
Epoch: 7852, Batch Gradient Norm after: 9.48694021964243
Epoch 7853/10000, Prediction Accuracy = 63.084%, Loss = 0.3804147720336914
Epoch: 7853, Batch Gradient Norm: 9.395537653667827
Epoch: 7853, Batch Gradient Norm after: 9.395537653667827
Epoch 7854/10000, Prediction Accuracy = 63.012%, Loss = 0.3791468501091003
Epoch: 7854, Batch Gradient Norm: 10.464246524127615
Epoch: 7854, Batch Gradient Norm after: 10.464246524127615
Epoch 7855/10000, Prediction Accuracy = 62.87800000000001%, Loss = 0.3895577907562256
Epoch: 7855, Batch Gradient Norm: 10.475156368570799
Epoch: 7855, Batch Gradient Norm after: 10.475156368570799
Epoch 7856/10000, Prediction Accuracy = 62.96600000000001%, Loss = 0.3907055199146271
Epoch: 7856, Batch Gradient Norm: 7.066395033630968
Epoch: 7856, Batch Gradient Norm after: 7.066395033630968
Epoch 7857/10000, Prediction Accuracy = 63.084%, Loss = 0.36969527006149294
Epoch: 7857, Batch Gradient Norm: 8.597280662733747
Epoch: 7857, Batch Gradient Norm after: 8.597280662733747
Epoch 7858/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.37818864583969114
Epoch: 7858, Batch Gradient Norm: 6.9190141887096726
Epoch: 7858, Batch Gradient Norm after: 6.9190141887096726
Epoch 7859/10000, Prediction Accuracy = 63.102%, Loss = 0.3674562096595764
Epoch: 7859, Batch Gradient Norm: 10.062614766833892
Epoch: 7859, Batch Gradient Norm after: 10.062614766833892
Epoch 7860/10000, Prediction Accuracy = 62.92%, Loss = 0.38782293200492857
Epoch: 7860, Batch Gradient Norm: 11.06790085138334
Epoch: 7860, Batch Gradient Norm after: 11.06790085138334
Epoch 7861/10000, Prediction Accuracy = 62.928%, Loss = 0.3935230910778046
Epoch: 7861, Batch Gradient Norm: 9.285196486536487
Epoch: 7861, Batch Gradient Norm after: 9.285196486536487
Epoch 7862/10000, Prediction Accuracy = 62.976%, Loss = 0.3801268577575684
Epoch: 7862, Batch Gradient Norm: 9.34965945604225
Epoch: 7862, Batch Gradient Norm after: 9.34965945604225
Epoch 7863/10000, Prediction Accuracy = 63.138%, Loss = 0.3800505220890045
Epoch: 7863, Batch Gradient Norm: 12.176867244852282
Epoch: 7863, Batch Gradient Norm after: 12.176867244852282
Epoch 7864/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.4062884211540222
Epoch: 7864, Batch Gradient Norm: 11.70860610361893
Epoch: 7864, Batch Gradient Norm after: 11.70860610361893
Epoch 7865/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.4013110756874084
Epoch: 7865, Batch Gradient Norm: 9.450530854245288
Epoch: 7865, Batch Gradient Norm after: 9.450530854245288
Epoch 7866/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.37981497049331664
Epoch: 7866, Batch Gradient Norm: 10.40107296658514
Epoch: 7866, Batch Gradient Norm after: 10.40107296658514
Epoch 7867/10000, Prediction Accuracy = 63.05%, Loss = 0.38558931946754454
Epoch: 7867, Batch Gradient Norm: 10.457103988180274
Epoch: 7867, Batch Gradient Norm after: 10.457103988180274
Epoch 7868/10000, Prediction Accuracy = 63.048%, Loss = 0.38968127965927124
Epoch: 7868, Batch Gradient Norm: 8.731935579178133
Epoch: 7868, Batch Gradient Norm after: 8.731935579178133
Epoch 7869/10000, Prediction Accuracy = 63.18000000000001%, Loss = 0.3773163974285126
Epoch: 7869, Batch Gradient Norm: 10.547583149809665
Epoch: 7869, Batch Gradient Norm after: 10.547583149809665
Epoch 7870/10000, Prediction Accuracy = 62.984%, Loss = 0.39296668767929077
Epoch: 7870, Batch Gradient Norm: 10.605050180824154
Epoch: 7870, Batch Gradient Norm after: 10.605050180824154
Epoch 7871/10000, Prediction Accuracy = 63.044000000000004%, Loss = 0.38951694369316103
Epoch: 7871, Batch Gradient Norm: 9.134316476588372
Epoch: 7871, Batch Gradient Norm after: 9.134316476588372
Epoch 7872/10000, Prediction Accuracy = 63.038%, Loss = 0.37902655601501467
Epoch: 7872, Batch Gradient Norm: 9.045360033755125
Epoch: 7872, Batch Gradient Norm after: 9.045360033755125
Epoch 7873/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3819894790649414
Epoch: 7873, Batch Gradient Norm: 9.57762931065566
Epoch: 7873, Batch Gradient Norm after: 9.57762931065566
Epoch 7874/10000, Prediction Accuracy = 63.072%, Loss = 0.38349801301956177
Epoch: 7874, Batch Gradient Norm: 11.488311938731556
Epoch: 7874, Batch Gradient Norm after: 11.488311938731556
Epoch 7875/10000, Prediction Accuracy = 63.074%, Loss = 0.4003786325454712
Epoch: 7875, Batch Gradient Norm: 11.007683121634688
Epoch: 7875, Batch Gradient Norm after: 11.007683121634688
Epoch 7876/10000, Prediction Accuracy = 62.983999999999995%, Loss = 0.39765289425849915
Epoch: 7876, Batch Gradient Norm: 9.356178538090857
Epoch: 7876, Batch Gradient Norm after: 9.356178538090857
Epoch 7877/10000, Prediction Accuracy = 62.916%, Loss = 0.38390177488327026
Epoch: 7877, Batch Gradient Norm: 9.575108428674564
Epoch: 7877, Batch Gradient Norm after: 9.575108428674564
Epoch 7878/10000, Prediction Accuracy = 62.983999999999995%, Loss = 0.3876971542835236
Epoch: 7878, Batch Gradient Norm: 7.241439369689787
Epoch: 7878, Batch Gradient Norm after: 7.241439369689787
Epoch 7879/10000, Prediction Accuracy = 63.124%, Loss = 0.3698325097560883
Epoch: 7879, Batch Gradient Norm: 7.694490677786063
Epoch: 7879, Batch Gradient Norm after: 7.694490677786063
Epoch 7880/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3720817148685455
Epoch: 7880, Batch Gradient Norm: 9.543159415285581
Epoch: 7880, Batch Gradient Norm after: 9.543159415285581
Epoch 7881/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3810952425003052
Epoch: 7881, Batch Gradient Norm: 12.972131185946761
Epoch: 7881, Batch Gradient Norm after: 12.972131185946761
Epoch 7882/10000, Prediction Accuracy = 62.806%, Loss = 0.41094821095466616
Epoch: 7882, Batch Gradient Norm: 10.412065507476111
Epoch: 7882, Batch Gradient Norm after: 10.412065507476111
Epoch 7883/10000, Prediction Accuracy = 62.89000000000001%, Loss = 0.3879329919815063
Epoch: 7883, Batch Gradient Norm: 10.181790945424884
Epoch: 7883, Batch Gradient Norm after: 10.181790945424884
Epoch 7884/10000, Prediction Accuracy = 62.932%, Loss = 0.38822307586669924
Epoch: 7884, Batch Gradient Norm: 8.778669328198538
Epoch: 7884, Batch Gradient Norm after: 8.778669328198538
Epoch 7885/10000, Prediction Accuracy = 63.10999999999999%, Loss = 0.3768681764602661
Epoch: 7885, Batch Gradient Norm: 11.19129874381207
Epoch: 7885, Batch Gradient Norm after: 11.19129874381207
Epoch 7886/10000, Prediction Accuracy = 62.9%, Loss = 0.39742083549499513
Epoch: 7886, Batch Gradient Norm: 10.837307923674455
Epoch: 7886, Batch Gradient Norm after: 10.837307923674455
Epoch 7887/10000, Prediction Accuracy = 62.98%, Loss = 0.39018641114234925
Epoch: 7887, Batch Gradient Norm: 8.71669385291917
Epoch: 7887, Batch Gradient Norm after: 8.71669385291917
Epoch 7888/10000, Prediction Accuracy = 63.04%, Loss = 0.3760522842407227
Epoch: 7888, Batch Gradient Norm: 8.869382054951204
Epoch: 7888, Batch Gradient Norm after: 8.869382054951204
Epoch 7889/10000, Prediction Accuracy = 63.016%, Loss = 0.37510175108909605
Epoch: 7889, Batch Gradient Norm: 8.473669466423775
Epoch: 7889, Batch Gradient Norm after: 8.473669466423775
Epoch 7890/10000, Prediction Accuracy = 63.10200000000001%, Loss = 0.3745059847831726
Epoch: 7890, Batch Gradient Norm: 8.342750475076627
Epoch: 7890, Batch Gradient Norm after: 8.342750475076627
Epoch 7891/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.3747337818145752
Epoch: 7891, Batch Gradient Norm: 10.931496638707445
Epoch: 7891, Batch Gradient Norm after: 10.931496638707445
Epoch 7892/10000, Prediction Accuracy = 62.866%, Loss = 0.39606852531433107
Epoch: 7892, Batch Gradient Norm: 10.546549457509562
Epoch: 7892, Batch Gradient Norm after: 10.546549457509562
Epoch 7893/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.38939828276634214
Epoch: 7893, Batch Gradient Norm: 10.398324264070277
Epoch: 7893, Batch Gradient Norm after: 10.398324264070277
Epoch 7894/10000, Prediction Accuracy = 62.99000000000001%, Loss = 0.3915318131446838
Epoch: 7894, Batch Gradient Norm: 9.988402519137662
Epoch: 7894, Batch Gradient Norm after: 9.988402519137662
Epoch 7895/10000, Prediction Accuracy = 63.00999999999999%, Loss = 0.38576443791389464
Epoch: 7895, Batch Gradient Norm: 10.844377041256118
Epoch: 7895, Batch Gradient Norm after: 10.844377041256118
Epoch 7896/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.38998576402664187
Epoch: 7896, Batch Gradient Norm: 11.094140082898814
Epoch: 7896, Batch Gradient Norm after: 11.094140082898814
Epoch 7897/10000, Prediction Accuracy = 62.988%, Loss = 0.3914622187614441
Epoch: 7897, Batch Gradient Norm: 10.203571301587685
Epoch: 7897, Batch Gradient Norm after: 10.203571301587685
Epoch 7898/10000, Prediction Accuracy = 63.21999999999999%, Loss = 0.38489599227905275
Epoch: 7898, Batch Gradient Norm: 5.975570829970784
Epoch: 7898, Batch Gradient Norm after: 5.975570829970784
Epoch 7899/10000, Prediction Accuracy = 63.068%, Loss = 0.36541174054145814
Epoch: 7899, Batch Gradient Norm: 9.899131398129821
Epoch: 7899, Batch Gradient Norm after: 9.899131398129821
Epoch 7900/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.38561996817588806
Epoch: 7900, Batch Gradient Norm: 9.492543501399842
Epoch: 7900, Batch Gradient Norm after: 9.492543501399842
Epoch 7901/10000, Prediction Accuracy = 63.004000000000005%, Loss = 0.38023959994316103
Epoch: 7901, Batch Gradient Norm: 8.708282722882373
Epoch: 7901, Batch Gradient Norm after: 8.708282722882373
Epoch 7902/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3749530971050262
Epoch: 7902, Batch Gradient Norm: 14.296044211604347
Epoch: 7902, Batch Gradient Norm after: 14.296044211604347
Epoch 7903/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.42894948124885557
Epoch: 7903, Batch Gradient Norm: 9.277743090873384
Epoch: 7903, Batch Gradient Norm after: 9.277743090873384
Epoch 7904/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.38265749216079714
Epoch: 7904, Batch Gradient Norm: 9.17766345506969
Epoch: 7904, Batch Gradient Norm after: 9.17766345506969
Epoch 7905/10000, Prediction Accuracy = 63.13000000000001%, Loss = 0.3802312910556793
Epoch: 7905, Batch Gradient Norm: 11.526673672043556
Epoch: 7905, Batch Gradient Norm after: 11.526673672043556
Epoch 7906/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.39462087154388426
Epoch: 7906, Batch Gradient Norm: 9.40091655849516
Epoch: 7906, Batch Gradient Norm after: 9.40091655849516
Epoch 7907/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.38152816891670227
Epoch: 7907, Batch Gradient Norm: 8.038746794506544
Epoch: 7907, Batch Gradient Norm after: 8.038746794506544
Epoch 7908/10000, Prediction Accuracy = 63.04200000000001%, Loss = 0.37473337054252626
Epoch: 7908, Batch Gradient Norm: 7.104065204280138
Epoch: 7908, Batch Gradient Norm after: 7.104065204280138
Epoch 7909/10000, Prediction Accuracy = 63.04799999999999%, Loss = 0.36951794028282164
Epoch: 7909, Batch Gradient Norm: 8.037280877104736
Epoch: 7909, Batch Gradient Norm after: 8.037280877104736
Epoch 7910/10000, Prediction Accuracy = 63.04%, Loss = 0.37457136511802674
Epoch: 7910, Batch Gradient Norm: 9.239807131306186
Epoch: 7910, Batch Gradient Norm after: 9.239807131306186
Epoch 7911/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.37851831316947937
Epoch: 7911, Batch Gradient Norm: 12.13335027576574
Epoch: 7911, Batch Gradient Norm after: 12.13335027576574
Epoch 7912/10000, Prediction Accuracy = 63.062%, Loss = 0.39915682673454284
Epoch: 7912, Batch Gradient Norm: 8.68503139059852
Epoch: 7912, Batch Gradient Norm after: 8.68503139059852
Epoch 7913/10000, Prediction Accuracy = 63.102%, Loss = 0.3753807723522186
Epoch: 7913, Batch Gradient Norm: 9.606156383474962
Epoch: 7913, Batch Gradient Norm after: 9.606156383474962
Epoch 7914/10000, Prediction Accuracy = 62.964%, Loss = 0.3828587055206299
Epoch: 7914, Batch Gradient Norm: 9.284092556227378
Epoch: 7914, Batch Gradient Norm after: 9.284092556227378
Epoch 7915/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.379595685005188
Epoch: 7915, Batch Gradient Norm: 8.871769904647085
Epoch: 7915, Batch Gradient Norm after: 8.871769904647085
Epoch 7916/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.3759961247444153
Epoch: 7916, Batch Gradient Norm: 11.22847411605778
Epoch: 7916, Batch Gradient Norm after: 11.22847411605778
Epoch 7917/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.392076712846756
Epoch: 7917, Batch Gradient Norm: 11.507413919023374
Epoch: 7917, Batch Gradient Norm after: 11.507413919023374
Epoch 7918/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.3999315559864044
Epoch: 7918, Batch Gradient Norm: 10.17926805642312
Epoch: 7918, Batch Gradient Norm after: 10.17926805642312
Epoch 7919/10000, Prediction Accuracy = 63.1%, Loss = 0.38992109298706057
Epoch: 7919, Batch Gradient Norm: 10.864126146117952
Epoch: 7919, Batch Gradient Norm after: 10.864126146117952
Epoch 7920/10000, Prediction Accuracy = 62.959999999999994%, Loss = 0.3938856780529022
Epoch: 7920, Batch Gradient Norm: 9.05193121624763
Epoch: 7920, Batch Gradient Norm after: 9.05193121624763
Epoch 7921/10000, Prediction Accuracy = 63.07199999999999%, Loss = 0.37944306135177613
Epoch: 7921, Batch Gradient Norm: 8.111715045231003
Epoch: 7921, Batch Gradient Norm after: 8.111715045231003
Epoch 7922/10000, Prediction Accuracy = 63.088%, Loss = 0.37254504561424256
Epoch: 7922, Batch Gradient Norm: 6.996806101022961
Epoch: 7922, Batch Gradient Norm after: 6.996806101022961
Epoch 7923/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.366253536939621
Epoch: 7923, Batch Gradient Norm: 9.972538575670558
Epoch: 7923, Batch Gradient Norm after: 9.972538575670558
Epoch 7924/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.3954802453517914
Epoch: 7924, Batch Gradient Norm: 11.130236035298823
Epoch: 7924, Batch Gradient Norm after: 11.130236035298823
Epoch 7925/10000, Prediction Accuracy = 62.952%, Loss = 0.3964327275753021
Epoch: 7925, Batch Gradient Norm: 8.444162634236118
Epoch: 7925, Batch Gradient Norm after: 8.444162634236118
Epoch 7926/10000, Prediction Accuracy = 63.012%, Loss = 0.3747697651386261
Epoch: 7926, Batch Gradient Norm: 8.891639229269083
Epoch: 7926, Batch Gradient Norm after: 8.891639229269083
Epoch 7927/10000, Prediction Accuracy = 63.07800000000001%, Loss = 0.3775032341480255
Epoch: 7927, Batch Gradient Norm: 10.981589647631369
Epoch: 7927, Batch Gradient Norm after: 10.981589647631369
Epoch 7928/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.39337891936302183
Epoch: 7928, Batch Gradient Norm: 9.290470357960082
Epoch: 7928, Batch Gradient Norm after: 9.290470357960082
Epoch 7929/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.38096550703048704
Epoch: 7929, Batch Gradient Norm: 9.934767721821531
Epoch: 7929, Batch Gradient Norm after: 9.934767721821531
Epoch 7930/10000, Prediction Accuracy = 62.902%, Loss = 0.3857947587966919
Epoch: 7930, Batch Gradient Norm: 9.830037472324289
Epoch: 7930, Batch Gradient Norm after: 9.830037472324289
Epoch 7931/10000, Prediction Accuracy = 62.842%, Loss = 0.3844295561313629
Epoch: 7931, Batch Gradient Norm: 6.7769214229489
Epoch: 7931, Batch Gradient Norm after: 6.7769214229489
Epoch 7932/10000, Prediction Accuracy = 63.032000000000004%, Loss = 0.36682838201522827
Epoch: 7932, Batch Gradient Norm: 10.116496673429683
Epoch: 7932, Batch Gradient Norm after: 10.116496673429683
Epoch 7933/10000, Prediction Accuracy = 62.943999999999996%, Loss = 0.3840986728668213
Epoch: 7933, Batch Gradient Norm: 11.40170266412447
Epoch: 7933, Batch Gradient Norm after: 11.40170266412447
Epoch 7934/10000, Prediction Accuracy = 63.146%, Loss = 0.39525632858276366
Epoch: 7934, Batch Gradient Norm: 10.065523261174864
Epoch: 7934, Batch Gradient Norm after: 10.065523261174864
Epoch 7935/10000, Prediction Accuracy = 63.044%, Loss = 0.3851419031620026
Epoch: 7935, Batch Gradient Norm: 9.895919141225164
Epoch: 7935, Batch Gradient Norm after: 9.895919141225164
Epoch 7936/10000, Prediction Accuracy = 63.146%, Loss = 0.38373836874961853
Epoch: 7936, Batch Gradient Norm: 11.510538118445679
Epoch: 7936, Batch Gradient Norm after: 11.510538118445679
Epoch 7937/10000, Prediction Accuracy = 63.041999999999994%, Loss = 0.39439108967781067
Epoch: 7937, Batch Gradient Norm: 11.657171001325835
Epoch: 7937, Batch Gradient Norm after: 11.657171001325835
Epoch 7938/10000, Prediction Accuracy = 62.824%, Loss = 0.39696048498153685
Epoch: 7938, Batch Gradient Norm: 7.931262794340158
Epoch: 7938, Batch Gradient Norm after: 7.931262794340158
Epoch 7939/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.37139196395874025
Epoch: 7939, Batch Gradient Norm: 8.96474943481838
Epoch: 7939, Batch Gradient Norm after: 8.96474943481838
Epoch 7940/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3783715307712555
Epoch: 7940, Batch Gradient Norm: 9.704544416525293
Epoch: 7940, Batch Gradient Norm after: 9.704544416525293
Epoch 7941/10000, Prediction Accuracy = 63.13799999999999%, Loss = 0.3820067346096039
Epoch: 7941, Batch Gradient Norm: 12.178214740804851
Epoch: 7941, Batch Gradient Norm after: 12.178214740804851
Epoch 7942/10000, Prediction Accuracy = 62.95%, Loss = 0.40324186682701113
Epoch: 7942, Batch Gradient Norm: 13.553340744009619
Epoch: 7942, Batch Gradient Norm after: 13.553340744009619
Epoch 7943/10000, Prediction Accuracy = 63.088%, Loss = 0.4171240389347076
Epoch: 7943, Batch Gradient Norm: 8.165852446191467
Epoch: 7943, Batch Gradient Norm after: 8.165852446191467
Epoch 7944/10000, Prediction Accuracy = 63.298%, Loss = 0.37321120500564575
Epoch: 7944, Batch Gradient Norm: 6.860875590576953
Epoch: 7944, Batch Gradient Norm after: 6.860875590576953
Epoch 7945/10000, Prediction Accuracy = 63.03399999999999%, Loss = 0.3658569574356079
Epoch: 7945, Batch Gradient Norm: 7.691870071193455
Epoch: 7945, Batch Gradient Norm after: 7.691870071193455
Epoch 7946/10000, Prediction Accuracy = 62.99000000000001%, Loss = 0.37043771147727966
Epoch: 7946, Batch Gradient Norm: 7.878710240422605
Epoch: 7946, Batch Gradient Norm after: 7.878710240422605
Epoch 7947/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.36834511160850525
Epoch: 7947, Batch Gradient Norm: 9.46714284212069
Epoch: 7947, Batch Gradient Norm after: 9.46714284212069
Epoch 7948/10000, Prediction Accuracy = 62.81999999999999%, Loss = 0.38001331090927126
Epoch: 7948, Batch Gradient Norm: 11.228088531453652
Epoch: 7948, Batch Gradient Norm after: 11.228088531453652
Epoch 7949/10000, Prediction Accuracy = 62.988%, Loss = 0.3930517554283142
Epoch: 7949, Batch Gradient Norm: 12.346734887776092
Epoch: 7949, Batch Gradient Norm after: 12.346734887776092
Epoch 7950/10000, Prediction Accuracy = 62.848%, Loss = 0.40607314109802245
Epoch: 7950, Batch Gradient Norm: 9.470314099939594
Epoch: 7950, Batch Gradient Norm after: 9.470314099939594
Epoch 7951/10000, Prediction Accuracy = 62.984%, Loss = 0.38523868322372434
Epoch: 7951, Batch Gradient Norm: 5.422041365549129
Epoch: 7951, Batch Gradient Norm after: 5.422041365549129
Epoch 7952/10000, Prediction Accuracy = 63.13000000000001%, Loss = 0.35863061547279357
Epoch: 7952, Batch Gradient Norm: 9.66435658694869
Epoch: 7952, Batch Gradient Norm after: 9.66435658694869
Epoch 7953/10000, Prediction Accuracy = 63.076%, Loss = 0.37956684827804565
Epoch: 7953, Batch Gradient Norm: 12.879219823328565
Epoch: 7953, Batch Gradient Norm after: 12.879219823328565
Epoch 7954/10000, Prediction Accuracy = 62.862%, Loss = 0.40666425228118896
Epoch: 7954, Batch Gradient Norm: 10.583429133213917
Epoch: 7954, Batch Gradient Norm after: 10.583429133213917
Epoch 7955/10000, Prediction Accuracy = 63.152%, Loss = 0.3902333974838257
Epoch: 7955, Batch Gradient Norm: 8.763366705530906
Epoch: 7955, Batch Gradient Norm after: 8.763366705530906
Epoch 7956/10000, Prediction Accuracy = 63.08%, Loss = 0.3754679262638092
Epoch: 7956, Batch Gradient Norm: 11.259301562360426
Epoch: 7956, Batch Gradient Norm after: 11.259301562360426
Epoch 7957/10000, Prediction Accuracy = 62.995999999999995%, Loss = 0.3926084995269775
Epoch: 7957, Batch Gradient Norm: 10.1066095241623
Epoch: 7957, Batch Gradient Norm after: 10.1066095241623
Epoch 7958/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3837952673435211
Epoch: 7958, Batch Gradient Norm: 10.290454537358904
Epoch: 7958, Batch Gradient Norm after: 10.290454537358904
Epoch 7959/10000, Prediction Accuracy = 62.952%, Loss = 0.38536069393157957
Epoch: 7959, Batch Gradient Norm: 10.25431290280416
Epoch: 7959, Batch Gradient Norm after: 10.25431290280416
Epoch 7960/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3872378945350647
Epoch: 7960, Batch Gradient Norm: 7.651338731577316
Epoch: 7960, Batch Gradient Norm after: 7.651338731577316
Epoch 7961/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.36920981407165526
Epoch: 7961, Batch Gradient Norm: 9.71081686404364
Epoch: 7961, Batch Gradient Norm after: 9.71081686404364
Epoch 7962/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3829306423664093
Epoch: 7962, Batch Gradient Norm: 11.547334822453456
Epoch: 7962, Batch Gradient Norm after: 11.547334822453456
Epoch 7963/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.39580352902412413
Epoch: 7963, Batch Gradient Norm: 9.65477755772363
Epoch: 7963, Batch Gradient Norm after: 9.65477755772363
Epoch 7964/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.38035041093826294
Epoch: 7964, Batch Gradient Norm: 8.616745121885018
Epoch: 7964, Batch Gradient Norm after: 8.616745121885018
Epoch 7965/10000, Prediction Accuracy = 62.98%, Loss = 0.3753743529319763
Epoch: 7965, Batch Gradient Norm: 10.765126908914443
Epoch: 7965, Batch Gradient Norm after: 10.765126908914443
Epoch 7966/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.3915747761726379
Epoch: 7966, Batch Gradient Norm: 9.963059301622172
Epoch: 7966, Batch Gradient Norm after: 9.963059301622172
Epoch 7967/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.38434627652168274
Epoch: 7967, Batch Gradient Norm: 8.371765089929038
Epoch: 7967, Batch Gradient Norm after: 8.371765089929038
Epoch 7968/10000, Prediction Accuracy = 63.186%, Loss = 0.37400569319725036
Epoch: 7968, Batch Gradient Norm: 7.337736573870008
Epoch: 7968, Batch Gradient Norm after: 7.337736573870008
Epoch 7969/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3689358115196228
Epoch: 7969, Batch Gradient Norm: 7.296022147307581
Epoch: 7969, Batch Gradient Norm after: 7.296022147307581
Epoch 7970/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3672486126422882
Epoch: 7970, Batch Gradient Norm: 9.755753265141799
Epoch: 7970, Batch Gradient Norm after: 9.755753265141799
Epoch 7971/10000, Prediction Accuracy = 62.914%, Loss = 0.38628255724906924
Epoch: 7971, Batch Gradient Norm: 10.863063173358967
Epoch: 7971, Batch Gradient Norm after: 10.863063173358967
Epoch 7972/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3910937249660492
Epoch: 7972, Batch Gradient Norm: 10.131240852484435
Epoch: 7972, Batch Gradient Norm after: 10.131240852484435
Epoch 7973/10000, Prediction Accuracy = 63.04200000000001%, Loss = 0.3870594441890717
Epoch: 7973, Batch Gradient Norm: 10.299551419730395
Epoch: 7973, Batch Gradient Norm after: 10.299551419730395
Epoch 7974/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.39129945635795593
Epoch: 7974, Batch Gradient Norm: 7.292096456840752
Epoch: 7974, Batch Gradient Norm after: 7.292096456840752
Epoch 7975/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.36813544034957885
Epoch: 7975, Batch Gradient Norm: 9.025006682976073
Epoch: 7975, Batch Gradient Norm after: 9.025006682976073
Epoch 7976/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.37568947672843933
Epoch: 7976, Batch Gradient Norm: 10.861361453950156
Epoch: 7976, Batch Gradient Norm after: 10.861361453950156
Epoch 7977/10000, Prediction Accuracy = 63.117999999999995%, Loss = 0.38734639883041383
Epoch: 7977, Batch Gradient Norm: 11.837252700875236
Epoch: 7977, Batch Gradient Norm after: 11.837252700875236
Epoch 7978/10000, Prediction Accuracy = 63.004000000000005%, Loss = 0.39576004147529603
Epoch: 7978, Batch Gradient Norm: 8.736171490442272
Epoch: 7978, Batch Gradient Norm after: 8.736171490442272
Epoch 7979/10000, Prediction Accuracy = 62.984%, Loss = 0.37406723499298095
Epoch: 7979, Batch Gradient Norm: 9.357194397666374
Epoch: 7979, Batch Gradient Norm after: 9.357194397666374
Epoch 7980/10000, Prediction Accuracy = 63.068%, Loss = 0.3797475755214691
Epoch: 7980, Batch Gradient Norm: 10.345271638272894
Epoch: 7980, Batch Gradient Norm after: 10.345271638272894
Epoch 7981/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3869903028011322
Epoch: 7981, Batch Gradient Norm: 11.266366034866266
Epoch: 7981, Batch Gradient Norm after: 11.266366034866266
Epoch 7982/10000, Prediction Accuracy = 63.116%, Loss = 0.39094769954681396
Epoch: 7982, Batch Gradient Norm: 11.456028498795467
Epoch: 7982, Batch Gradient Norm after: 11.456028498795467
Epoch 7983/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.3925593733787537
Epoch: 7983, Batch Gradient Norm: 9.932364872690156
Epoch: 7983, Batch Gradient Norm after: 9.932364872690156
Epoch 7984/10000, Prediction Accuracy = 63.088%, Loss = 0.38522872924804685
Epoch: 7984, Batch Gradient Norm: 12.430186192719171
Epoch: 7984, Batch Gradient Norm after: 12.430186192719171
Epoch 7985/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.4020659625530243
Epoch: 7985, Batch Gradient Norm: 11.85926457000033
Epoch: 7985, Batch Gradient Norm after: 11.85926457000033
Epoch 7986/10000, Prediction Accuracy = 62.99000000000001%, Loss = 0.40055215954780576
Epoch: 7986, Batch Gradient Norm: 7.060527006230365
Epoch: 7986, Batch Gradient Norm after: 7.060527006230365
Epoch 7987/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.36620640754699707
Epoch: 7987, Batch Gradient Norm: 6.974923999079207
Epoch: 7987, Batch Gradient Norm after: 6.974923999079207
Epoch 7988/10000, Prediction Accuracy = 63.146%, Loss = 0.36638002991676333
Epoch: 7988, Batch Gradient Norm: 7.3864718160255896
Epoch: 7988, Batch Gradient Norm after: 7.3864718160255896
Epoch 7989/10000, Prediction Accuracy = 63.2%, Loss = 0.3679987668991089
Epoch: 7989, Batch Gradient Norm: 9.667967470779338
Epoch: 7989, Batch Gradient Norm after: 9.667967470779338
Epoch 7990/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.38247900605201723
Epoch: 7990, Batch Gradient Norm: 9.693522841746198
Epoch: 7990, Batch Gradient Norm after: 9.693522841746198
Epoch 7991/10000, Prediction Accuracy = 63.092%, Loss = 0.3815109372138977
Epoch: 7991, Batch Gradient Norm: 11.35892503354267
Epoch: 7991, Batch Gradient Norm after: 11.35892503354267
Epoch 7992/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.39866307377815247
Epoch: 7992, Batch Gradient Norm: 8.157010108425752
Epoch: 7992, Batch Gradient Norm after: 8.157010108425752
Epoch 7993/10000, Prediction Accuracy = 63.1%, Loss = 0.37115113735198973
Epoch: 7993, Batch Gradient Norm: 8.582435727682295
Epoch: 7993, Batch Gradient Norm after: 8.582435727682295
Epoch 7994/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.3756497085094452
Epoch: 7994, Batch Gradient Norm: 9.747734903022288
Epoch: 7994, Batch Gradient Norm after: 9.747734903022288
Epoch 7995/10000, Prediction Accuracy = 63.032%, Loss = 0.38650643825531006
Epoch: 7995, Batch Gradient Norm: 8.420416058469927
Epoch: 7995, Batch Gradient Norm after: 8.420416058469927
Epoch 7996/10000, Prediction Accuracy = 63.104%, Loss = 0.3750552713871002
Epoch: 7996, Batch Gradient Norm: 8.473396596493764
Epoch: 7996, Batch Gradient Norm after: 8.473396596493764
Epoch 7997/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.373389858007431
Epoch: 7997, Batch Gradient Norm: 10.700309912344517
Epoch: 7997, Batch Gradient Norm after: 10.700309912344517
Epoch 7998/10000, Prediction Accuracy = 63.064%, Loss = 0.38730854392051695
Epoch: 7998, Batch Gradient Norm: 10.898325893785143
Epoch: 7998, Batch Gradient Norm after: 10.898325893785143
Epoch 7999/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3876439809799194
Epoch: 7999, Batch Gradient Norm: 11.821115816767694
Epoch: 7999, Batch Gradient Norm after: 11.821115816767694
Epoch 8000/10000, Prediction Accuracy = 63.136%, Loss = 0.39533334970474243
Epoch: 8000, Batch Gradient Norm: 9.416681351180134
Epoch: 8000, Batch Gradient Norm after: 9.416681351180134
Epoch 8001/10000, Prediction Accuracy = 62.914%, Loss = 0.37859743237495425
Epoch: 8001, Batch Gradient Norm: 9.189473016048428
Epoch: 8001, Batch Gradient Norm after: 9.189473016048428
Epoch 8002/10000, Prediction Accuracy = 63.122%, Loss = 0.37893486618995664
Epoch: 8002, Batch Gradient Norm: 10.24446550329786
Epoch: 8002, Batch Gradient Norm after: 10.24446550329786
Epoch 8003/10000, Prediction Accuracy = 63.012%, Loss = 0.3835434794425964
Epoch: 8003, Batch Gradient Norm: 10.994259970231669
Epoch: 8003, Batch Gradient Norm after: 10.994259970231669
Epoch 8004/10000, Prediction Accuracy = 63.112%, Loss = 0.39269164204597473
Epoch: 8004, Batch Gradient Norm: 8.714265388129123
Epoch: 8004, Batch Gradient Norm after: 8.714265388129123
Epoch 8005/10000, Prediction Accuracy = 63.074%, Loss = 0.37583022713661196
Epoch: 8005, Batch Gradient Norm: 7.3580702429154785
Epoch: 8005, Batch Gradient Norm after: 7.3580702429154785
Epoch 8006/10000, Prediction Accuracy = 63.21%, Loss = 0.3682974338531494
Epoch: 8006, Batch Gradient Norm: 8.383138730044116
Epoch: 8006, Batch Gradient Norm after: 8.383138730044116
Epoch 8007/10000, Prediction Accuracy = 63.194%, Loss = 0.37323970198631284
Epoch: 8007, Batch Gradient Norm: 7.794168586625999
Epoch: 8007, Batch Gradient Norm after: 7.794168586625999
Epoch 8008/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3679841339588165
Epoch: 8008, Batch Gradient Norm: 11.715608428795925
Epoch: 8008, Batch Gradient Norm after: 11.715608428795925
Epoch 8009/10000, Prediction Accuracy = 62.774%, Loss = 0.3975155293941498
Epoch: 8009, Batch Gradient Norm: 10.900899675332587
Epoch: 8009, Batch Gradient Norm after: 10.900899675332587
Epoch 8010/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.39208163022994996
Epoch: 8010, Batch Gradient Norm: 9.898129001788615
Epoch: 8010, Batch Gradient Norm after: 9.898129001788615
Epoch 8011/10000, Prediction Accuracy = 63.098%, Loss = 0.38334590196609497
Epoch: 8011, Batch Gradient Norm: 9.666542894390373
Epoch: 8011, Batch Gradient Norm after: 9.666542894390373
Epoch 8012/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.37970271706581116
Epoch: 8012, Batch Gradient Norm: 9.247102296129826
Epoch: 8012, Batch Gradient Norm after: 9.247102296129826
Epoch 8013/10000, Prediction Accuracy = 63.193999999999996%, Loss = 0.3796112358570099
Epoch: 8013, Batch Gradient Norm: 8.978667052776418
Epoch: 8013, Batch Gradient Norm after: 8.978667052776418
Epoch 8014/10000, Prediction Accuracy = 62.95%, Loss = 0.3757465124130249
Epoch: 8014, Batch Gradient Norm: 7.7032007609430355
Epoch: 8014, Batch Gradient Norm after: 7.7032007609430355
Epoch 8015/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.3685364484786987
Epoch: 8015, Batch Gradient Norm: 10.307351195480253
Epoch: 8015, Batch Gradient Norm after: 10.307351195480253
Epoch 8016/10000, Prediction Accuracy = 62.872%, Loss = 0.383965528011322
Epoch: 8016, Batch Gradient Norm: 12.471269294185888
Epoch: 8016, Batch Gradient Norm after: 12.471269294185888
Epoch 8017/10000, Prediction Accuracy = 62.944%, Loss = 0.4012773036956787
Epoch: 8017, Batch Gradient Norm: 13.648839526181764
Epoch: 8017, Batch Gradient Norm after: 13.648839526181764
Epoch 8018/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.4205734968185425
Epoch: 8018, Batch Gradient Norm: 8.086013334412195
Epoch: 8018, Batch Gradient Norm after: 8.086013334412195
Epoch 8019/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.3734297096729279
Epoch: 8019, Batch Gradient Norm: 7.762962368871778
Epoch: 8019, Batch Gradient Norm after: 7.762962368871778
Epoch 8020/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.36867918372154235
Epoch: 8020, Batch Gradient Norm: 9.409624260521655
Epoch: 8020, Batch Gradient Norm after: 9.409624260521655
Epoch 8021/10000, Prediction Accuracy = 62.96%, Loss = 0.3788604140281677
Epoch: 8021, Batch Gradient Norm: 9.409602498094749
Epoch: 8021, Batch Gradient Norm after: 9.409602498094749
Epoch 8022/10000, Prediction Accuracy = 63.112%, Loss = 0.3774035215377808
Epoch: 8022, Batch Gradient Norm: 11.90640133643793
Epoch: 8022, Batch Gradient Norm after: 11.90640133643793
Epoch 8023/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.39955516457557677
Epoch: 8023, Batch Gradient Norm: 10.609623982877517
Epoch: 8023, Batch Gradient Norm after: 10.609623982877517
Epoch 8024/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3885473310947418
Epoch: 8024, Batch Gradient Norm: 9.174909503297279
Epoch: 8024, Batch Gradient Norm after: 9.174909503297279
Epoch 8025/10000, Prediction Accuracy = 63.112%, Loss = 0.37767807841300965
Epoch: 8025, Batch Gradient Norm: 8.381657174549364
Epoch: 8025, Batch Gradient Norm after: 8.381657174549364
Epoch 8026/10000, Prediction Accuracy = 63.148%, Loss = 0.3731416165828705
Epoch: 8026, Batch Gradient Norm: 11.462209148623172
Epoch: 8026, Batch Gradient Norm after: 11.462209148623172
Epoch 8027/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.39260756969451904
Epoch: 8027, Batch Gradient Norm: 10.93782677520653
Epoch: 8027, Batch Gradient Norm after: 10.93782677520653
Epoch 8028/10000, Prediction Accuracy = 63.114%, Loss = 0.38987661004066465
Epoch: 8028, Batch Gradient Norm: 7.522092927826535
Epoch: 8028, Batch Gradient Norm after: 7.522092927826535
Epoch 8029/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.3686957061290741
Epoch: 8029, Batch Gradient Norm: 8.31335029285931
Epoch: 8029, Batch Gradient Norm after: 8.31335029285931
Epoch 8030/10000, Prediction Accuracy = 63.152%, Loss = 0.3747642397880554
Epoch: 8030, Batch Gradient Norm: 7.693065160205126
Epoch: 8030, Batch Gradient Norm after: 7.693065160205126
Epoch 8031/10000, Prediction Accuracy = 63.198%, Loss = 0.370247882604599
Epoch: 8031, Batch Gradient Norm: 9.388319343396239
Epoch: 8031, Batch Gradient Norm after: 9.388319343396239
Epoch 8032/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.37524765729904175
Epoch: 8032, Batch Gradient Norm: 10.97857168299259
Epoch: 8032, Batch Gradient Norm after: 10.97857168299259
Epoch 8033/10000, Prediction Accuracy = 62.968%, Loss = 0.39004595279693605
Epoch: 8033, Batch Gradient Norm: 9.045451606103097
Epoch: 8033, Batch Gradient Norm after: 9.045451606103097
Epoch 8034/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3779974102973938
Epoch: 8034, Batch Gradient Norm: 8.404485222854648
Epoch: 8034, Batch Gradient Norm after: 8.404485222854648
Epoch 8035/10000, Prediction Accuracy = 63.23%, Loss = 0.37323097586631776
Epoch: 8035, Batch Gradient Norm: 8.920954813272209
Epoch: 8035, Batch Gradient Norm after: 8.920954813272209
Epoch 8036/10000, Prediction Accuracy = 63.14%, Loss = 0.373688793182373
Epoch: 8036, Batch Gradient Norm: 11.732604161681413
Epoch: 8036, Batch Gradient Norm after: 11.732604161681413
Epoch 8037/10000, Prediction Accuracy = 63.14%, Loss = 0.3951097786426544
Epoch: 8037, Batch Gradient Norm: 9.26607676416363
Epoch: 8037, Batch Gradient Norm after: 9.26607676416363
Epoch 8038/10000, Prediction Accuracy = 63.25%, Loss = 0.37555867433547974
Epoch: 8038, Batch Gradient Norm: 13.677161255527
Epoch: 8038, Batch Gradient Norm after: 13.677161255527
Epoch 8039/10000, Prediction Accuracy = 62.912%, Loss = 0.41500394344329833
Epoch: 8039, Batch Gradient Norm: 10.185968419146823
Epoch: 8039, Batch Gradient Norm after: 10.185968419146823
Epoch 8040/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3855049967765808
Epoch: 8040, Batch Gradient Norm: 9.9319188670582
Epoch: 8040, Batch Gradient Norm after: 9.9319188670582
Epoch 8041/10000, Prediction Accuracy = 63.15%, Loss = 0.38224874138832093
Epoch: 8041, Batch Gradient Norm: 6.779026081460738
Epoch: 8041, Batch Gradient Norm after: 6.779026081460738
Epoch 8042/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.36558775305747987
Epoch: 8042, Batch Gradient Norm: 8.659713414511927
Epoch: 8042, Batch Gradient Norm after: 8.659713414511927
Epoch 8043/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.37174429297447203
Epoch: 8043, Batch Gradient Norm: 10.628532795618181
Epoch: 8043, Batch Gradient Norm after: 10.628532795618181
Epoch 8044/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.38540634512901306
Epoch: 8044, Batch Gradient Norm: 11.816438614424458
Epoch: 8044, Batch Gradient Norm after: 11.816438614424458
Epoch 8045/10000, Prediction Accuracy = 63.10799999999999%, Loss = 0.39474521279335023
Epoch: 8045, Batch Gradient Norm: 9.859938517219826
Epoch: 8045, Batch Gradient Norm after: 9.859938517219826
Epoch 8046/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.38163297176361083
Epoch: 8046, Batch Gradient Norm: 9.796291423696955
Epoch: 8046, Batch Gradient Norm after: 9.796291423696955
Epoch 8047/10000, Prediction Accuracy = 63.06%, Loss = 0.3846434116363525
Epoch: 8047, Batch Gradient Norm: 9.954988951217631
Epoch: 8047, Batch Gradient Norm after: 9.954988951217631
Epoch 8048/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3849175155162811
Epoch: 8048, Batch Gradient Norm: 10.377644741835308
Epoch: 8048, Batch Gradient Norm after: 10.377644741835308
Epoch 8049/10000, Prediction Accuracy = 62.94000000000001%, Loss = 0.38844225406646726
Epoch: 8049, Batch Gradient Norm: 9.27765016570649
Epoch: 8049, Batch Gradient Norm after: 9.27765016570649
Epoch 8050/10000, Prediction Accuracy = 63.032%, Loss = 0.3809404015541077
Epoch: 8050, Batch Gradient Norm: 9.047414857535953
Epoch: 8050, Batch Gradient Norm after: 9.047414857535953
Epoch 8051/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.38018969297409055
Epoch: 8051, Batch Gradient Norm: 9.050630966250983
Epoch: 8051, Batch Gradient Norm after: 9.050630966250983
Epoch 8052/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3758165419101715
Epoch: 8052, Batch Gradient Norm: 8.334161416476475
Epoch: 8052, Batch Gradient Norm after: 8.334161416476475
Epoch 8053/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.3700656950473785
Epoch: 8053, Batch Gradient Norm: 10.881749674160572
Epoch: 8053, Batch Gradient Norm after: 10.881749674160572
Epoch 8054/10000, Prediction Accuracy = 63.17%, Loss = 0.38765119910240176
Epoch: 8054, Batch Gradient Norm: 11.01197983876109
Epoch: 8054, Batch Gradient Norm after: 11.01197983876109
Epoch 8055/10000, Prediction Accuracy = 63.086%, Loss = 0.3881836473941803
Epoch: 8055, Batch Gradient Norm: 10.51207023067181
Epoch: 8055, Batch Gradient Norm after: 10.51207023067181
Epoch 8056/10000, Prediction Accuracy = 62.912%, Loss = 0.38819206357002256
Epoch: 8056, Batch Gradient Norm: 10.965731587907243
Epoch: 8056, Batch Gradient Norm after: 10.965731587907243
Epoch 8057/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3886345624923706
Epoch: 8057, Batch Gradient Norm: 9.011584628728633
Epoch: 8057, Batch Gradient Norm after: 9.011584628728633
Epoch 8058/10000, Prediction Accuracy = 63.14%, Loss = 0.3756420135498047
Epoch: 8058, Batch Gradient Norm: 9.39993634624647
Epoch: 8058, Batch Gradient Norm after: 9.39993634624647
Epoch 8059/10000, Prediction Accuracy = 63.03000000000001%, Loss = 0.3790017306804657
Epoch: 8059, Batch Gradient Norm: 10.045741694212332
Epoch: 8059, Batch Gradient Norm after: 10.045741694212332
Epoch 8060/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.38292406797409057
Epoch: 8060, Batch Gradient Norm: 9.524255511382067
Epoch: 8060, Batch Gradient Norm after: 9.524255511382067
Epoch 8061/10000, Prediction Accuracy = 63.114%, Loss = 0.3797492027282715
Epoch: 8061, Batch Gradient Norm: 8.416047985808378
Epoch: 8061, Batch Gradient Norm after: 8.416047985808378
Epoch 8062/10000, Prediction Accuracy = 63.048%, Loss = 0.3705093502998352
Epoch: 8062, Batch Gradient Norm: 10.826490666229235
Epoch: 8062, Batch Gradient Norm after: 10.826490666229235
Epoch 8063/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.3902587115764618
Epoch: 8063, Batch Gradient Norm: 9.762831898703308
Epoch: 8063, Batch Gradient Norm after: 9.762831898703308
Epoch 8064/10000, Prediction Accuracy = 63.072%, Loss = 0.38492551445961
Epoch: 8064, Batch Gradient Norm: 9.38010460762798
Epoch: 8064, Batch Gradient Norm after: 9.38010460762798
Epoch 8065/10000, Prediction Accuracy = 63.112%, Loss = 0.3809105694293976
Epoch: 8065, Batch Gradient Norm: 9.470807328017816
Epoch: 8065, Batch Gradient Norm after: 9.470807328017816
Epoch 8066/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3794442415237427
Epoch: 8066, Batch Gradient Norm: 11.215939779016086
Epoch: 8066, Batch Gradient Norm after: 11.215939779016086
Epoch 8067/10000, Prediction Accuracy = 62.956%, Loss = 0.3890454351902008
Epoch: 8067, Batch Gradient Norm: 10.265957569116107
Epoch: 8067, Batch Gradient Norm after: 10.265957569116107
Epoch 8068/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.38298566937446593
Epoch: 8068, Batch Gradient Norm: 10.281937641120486
Epoch: 8068, Batch Gradient Norm after: 10.281937641120486
Epoch 8069/10000, Prediction Accuracy = 63.05%, Loss = 0.38238200545310974
Epoch: 8069, Batch Gradient Norm: 9.006024277169344
Epoch: 8069, Batch Gradient Norm after: 9.006024277169344
Epoch 8070/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3740658164024353
Epoch: 8070, Batch Gradient Norm: 9.071180783339319
Epoch: 8070, Batch Gradient Norm after: 9.071180783339319
Epoch 8071/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.37629730701446534
Epoch: 8071, Batch Gradient Norm: 9.032913464252793
Epoch: 8071, Batch Gradient Norm after: 9.032913464252793
Epoch 8072/10000, Prediction Accuracy = 63.086%, Loss = 0.376788341999054
Epoch: 8072, Batch Gradient Norm: 11.71831369267142
Epoch: 8072, Batch Gradient Norm after: 11.71831369267142
Epoch 8073/10000, Prediction Accuracy = 63.112%, Loss = 0.39681469798088076
Epoch: 8073, Batch Gradient Norm: 7.510285821853579
Epoch: 8073, Batch Gradient Norm after: 7.510285821853579
Epoch 8074/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.36658304929733276
Epoch: 8074, Batch Gradient Norm: 8.77006572903179
Epoch: 8074, Batch Gradient Norm after: 8.77006572903179
Epoch 8075/10000, Prediction Accuracy = 63.148%, Loss = 0.37140066623687745
Epoch: 8075, Batch Gradient Norm: 12.211849411036642
Epoch: 8075, Batch Gradient Norm after: 12.211849411036642
Epoch 8076/10000, Prediction Accuracy = 62.878%, Loss = 0.39625072479248047
Epoch: 8076, Batch Gradient Norm: 10.862616952750585
Epoch: 8076, Batch Gradient Norm after: 10.862616952750585
Epoch 8077/10000, Prediction Accuracy = 62.90400000000001%, Loss = 0.39346290230751035
Epoch: 8077, Batch Gradient Norm: 6.909150702115646
Epoch: 8077, Batch Gradient Norm after: 6.909150702115646
Epoch 8078/10000, Prediction Accuracy = 63.138%, Loss = 0.36290769577026366
Epoch: 8078, Batch Gradient Norm: 10.163612777277605
Epoch: 8078, Batch Gradient Norm after: 10.163612777277605
Epoch 8079/10000, Prediction Accuracy = 63.222%, Loss = 0.3857124924659729
Epoch: 8079, Batch Gradient Norm: 8.861428341893195
Epoch: 8079, Batch Gradient Norm after: 8.861428341893195
Epoch 8080/10000, Prediction Accuracy = 63.062%, Loss = 0.3737835943698883
Epoch: 8080, Batch Gradient Norm: 9.274455942111084
Epoch: 8080, Batch Gradient Norm after: 9.274455942111084
Epoch 8081/10000, Prediction Accuracy = 63.19%, Loss = 0.3772319912910461
Epoch: 8081, Batch Gradient Norm: 11.132619820516222
Epoch: 8081, Batch Gradient Norm after: 11.132619820516222
Epoch 8082/10000, Prediction Accuracy = 63.138%, Loss = 0.3887352466583252
Epoch: 8082, Batch Gradient Norm: 10.7162061529574
Epoch: 8082, Batch Gradient Norm after: 10.7162061529574
Epoch 8083/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3879537761211395
Epoch: 8083, Batch Gradient Norm: 10.070762880767179
Epoch: 8083, Batch Gradient Norm after: 10.070762880767179
Epoch 8084/10000, Prediction Accuracy = 63.064%, Loss = 0.3838063180446625
Epoch: 8084, Batch Gradient Norm: 10.807401360048493
Epoch: 8084, Batch Gradient Norm after: 10.807401360048493
Epoch 8085/10000, Prediction Accuracy = 63.072%, Loss = 0.3912698805332184
Epoch: 8085, Batch Gradient Norm: 8.647516405568503
Epoch: 8085, Batch Gradient Norm after: 8.647516405568503
Epoch 8086/10000, Prediction Accuracy = 63.004000000000005%, Loss = 0.373541522026062
Epoch: 8086, Batch Gradient Norm: 8.05216992952376
Epoch: 8086, Batch Gradient Norm after: 8.05216992952376
Epoch 8087/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3697753071784973
Epoch: 8087, Batch Gradient Norm: 7.418981489414041
Epoch: 8087, Batch Gradient Norm after: 7.418981489414041
Epoch 8088/10000, Prediction Accuracy = 63.194%, Loss = 0.3645599901676178
Epoch: 8088, Batch Gradient Norm: 10.065132430604576
Epoch: 8088, Batch Gradient Norm after: 10.065132430604576
Epoch 8089/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.38235538005828856
Epoch: 8089, Batch Gradient Norm: 9.623734734879914
Epoch: 8089, Batch Gradient Norm after: 9.623734734879914
Epoch 8090/10000, Prediction Accuracy = 63.182%, Loss = 0.37975391149520876
Epoch: 8090, Batch Gradient Norm: 10.144522706264684
Epoch: 8090, Batch Gradient Norm after: 10.144522706264684
Epoch 8091/10000, Prediction Accuracy = 63.169999999999995%, Loss = 0.38510997891426085
Epoch: 8091, Batch Gradient Norm: 10.361625205575683
Epoch: 8091, Batch Gradient Norm after: 10.361625205575683
Epoch 8092/10000, Prediction Accuracy = 63.104%, Loss = 0.38677555322647095
Epoch: 8092, Batch Gradient Norm: 9.231335816363949
Epoch: 8092, Batch Gradient Norm after: 9.231335816363949
Epoch 8093/10000, Prediction Accuracy = 63.14%, Loss = 0.3749744653701782
Epoch: 8093, Batch Gradient Norm: 11.773011961795829
Epoch: 8093, Batch Gradient Norm after: 11.773011961795829
Epoch 8094/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.39267303347587584
Epoch: 8094, Batch Gradient Norm: 9.77113101629018
Epoch: 8094, Batch Gradient Norm after: 9.77113101629018
Epoch 8095/10000, Prediction Accuracy = 62.976%, Loss = 0.3790819406509399
Epoch: 8095, Batch Gradient Norm: 12.730837924756514
Epoch: 8095, Batch Gradient Norm after: 12.730837924756514
Epoch 8096/10000, Prediction Accuracy = 63.124%, Loss = 0.4056034028530121
Epoch: 8096, Batch Gradient Norm: 11.16752426234771
Epoch: 8096, Batch Gradient Norm after: 11.16752426234771
Epoch 8097/10000, Prediction Accuracy = 63.012%, Loss = 0.39396576285362245
Epoch: 8097, Batch Gradient Norm: 7.9728327544979125
Epoch: 8097, Batch Gradient Norm after: 7.9728327544979125
Epoch 8098/10000, Prediction Accuracy = 63.254%, Loss = 0.36884410977363585
Epoch: 8098, Batch Gradient Norm: 8.607907811240585
Epoch: 8098, Batch Gradient Norm after: 8.607907811240585
Epoch 8099/10000, Prediction Accuracy = 63.036%, Loss = 0.3756307721138
Epoch: 8099, Batch Gradient Norm: 8.149558297389447
Epoch: 8099, Batch Gradient Norm after: 8.149558297389447
Epoch 8100/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.37119112014770506
Epoch: 8100, Batch Gradient Norm: 10.748966123133554
Epoch: 8100, Batch Gradient Norm after: 10.748966123133554
Epoch 8101/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.38954615592956543
Epoch: 8101, Batch Gradient Norm: 9.27764204883799
Epoch: 8101, Batch Gradient Norm after: 9.27764204883799
Epoch 8102/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3783486247062683
Epoch: 8102, Batch Gradient Norm: 7.867258214706615
Epoch: 8102, Batch Gradient Norm after: 7.867258214706615
Epoch 8103/10000, Prediction Accuracy = 63.1%, Loss = 0.36974849104881286
Epoch: 8103, Batch Gradient Norm: 11.884713967197976
Epoch: 8103, Batch Gradient Norm after: 11.884713967197976
Epoch 8104/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.3972397565841675
Epoch: 8104, Batch Gradient Norm: 8.587098617839088
Epoch: 8104, Batch Gradient Norm after: 8.587098617839088
Epoch 8105/10000, Prediction Accuracy = 63.024%, Loss = 0.37256974577903745
Epoch: 8105, Batch Gradient Norm: 10.078963804479395
Epoch: 8105, Batch Gradient Norm after: 10.078963804479395
Epoch 8106/10000, Prediction Accuracy = 63.152%, Loss = 0.3853124439716339
Epoch: 8106, Batch Gradient Norm: 11.84125412238227
Epoch: 8106, Batch Gradient Norm after: 11.84125412238227
Epoch 8107/10000, Prediction Accuracy = 62.944%, Loss = 0.3976259887218475
Epoch: 8107, Batch Gradient Norm: 9.860354978014403
Epoch: 8107, Batch Gradient Norm after: 9.860354978014403
Epoch 8108/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3806027829647064
Epoch: 8108, Batch Gradient Norm: 8.176274957674584
Epoch: 8108, Batch Gradient Norm after: 8.176274957674584
Epoch 8109/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3728425204753876
Epoch: 8109, Batch Gradient Norm: 9.16170384544218
Epoch: 8109, Batch Gradient Norm after: 9.16170384544218
Epoch 8110/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.37432198524475097
Epoch: 8110, Batch Gradient Norm: 10.300863880487205
Epoch: 8110, Batch Gradient Norm after: 10.300863880487205
Epoch 8111/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.381982696056366
Epoch: 8111, Batch Gradient Norm: 9.172975800363202
Epoch: 8111, Batch Gradient Norm after: 9.172975800363202
Epoch 8112/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.37506027817726134
Epoch: 8112, Batch Gradient Norm: 9.903311888677882
Epoch: 8112, Batch Gradient Norm after: 9.903311888677882
Epoch 8113/10000, Prediction Accuracy = 63.134%, Loss = 0.3804171621799469
Epoch: 8113, Batch Gradient Norm: 7.906184449070754
Epoch: 8113, Batch Gradient Norm after: 7.906184449070754
Epoch 8114/10000, Prediction Accuracy = 63.004%, Loss = 0.36847726702690126
Epoch: 8114, Batch Gradient Norm: 9.042112953755321
Epoch: 8114, Batch Gradient Norm after: 9.042112953755321
Epoch 8115/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.37537527084350586
Epoch: 8115, Batch Gradient Norm: 11.120715028079108
Epoch: 8115, Batch Gradient Norm after: 11.120715028079108
Epoch 8116/10000, Prediction Accuracy = 63.102%, Loss = 0.3939737915992737
Epoch: 8116, Batch Gradient Norm: 7.379998834857338
Epoch: 8116, Batch Gradient Norm after: 7.379998834857338
Epoch 8117/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.36743344068527223
Epoch: 8117, Batch Gradient Norm: 9.208105410355628
Epoch: 8117, Batch Gradient Norm after: 9.208105410355628
Epoch 8118/10000, Prediction Accuracy = 63.086%, Loss = 0.3778367877006531
Epoch: 8118, Batch Gradient Norm: 12.131645836218583
Epoch: 8118, Batch Gradient Norm after: 12.131645836218583
Epoch 8119/10000, Prediction Accuracy = 63.112%, Loss = 0.3971381545066833
Epoch: 8119, Batch Gradient Norm: 10.801590739329235
Epoch: 8119, Batch Gradient Norm after: 10.801590739329235
Epoch 8120/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.38942196369171145
Epoch: 8120, Batch Gradient Norm: 9.311030784789757
Epoch: 8120, Batch Gradient Norm after: 9.311030784789757
Epoch 8121/10000, Prediction Accuracy = 63.124%, Loss = 0.37663801312446593
Epoch: 8121, Batch Gradient Norm: 11.89119585527203
Epoch: 8121, Batch Gradient Norm after: 11.89119585527203
Epoch 8122/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.39734559655189516
Epoch: 8122, Batch Gradient Norm: 10.071499221028798
Epoch: 8122, Batch Gradient Norm after: 10.071499221028798
Epoch 8123/10000, Prediction Accuracy = 63.188%, Loss = 0.38144206404685976
Epoch: 8123, Batch Gradient Norm: 8.630879477892066
Epoch: 8123, Batch Gradient Norm after: 8.630879477892066
Epoch 8124/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.3734869480133057
Epoch: 8124, Batch Gradient Norm: 9.768174568294405
Epoch: 8124, Batch Gradient Norm after: 9.768174568294405
Epoch 8125/10000, Prediction Accuracy = 62.864%, Loss = 0.3782422780990601
Epoch: 8125, Batch Gradient Norm: 10.377286759296716
Epoch: 8125, Batch Gradient Norm after: 10.377286759296716
Epoch 8126/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3835147857666016
Epoch: 8126, Batch Gradient Norm: 11.303611311314427
Epoch: 8126, Batch Gradient Norm after: 11.303611311314427
Epoch 8127/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.39167688488960267
Epoch: 8127, Batch Gradient Norm: 7.486406817257074
Epoch: 8127, Batch Gradient Norm after: 7.486406817257074
Epoch 8128/10000, Prediction Accuracy = 63.092%, Loss = 0.366425096988678
Epoch: 8128, Batch Gradient Norm: 7.38312147173857
Epoch: 8128, Batch Gradient Norm after: 7.38312147173857
Epoch 8129/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.36430469155311584
Epoch: 8129, Batch Gradient Norm: 8.852390444379328
Epoch: 8129, Batch Gradient Norm after: 8.852390444379328
Epoch 8130/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.37154254913330076
Epoch: 8130, Batch Gradient Norm: 11.516817099797324
Epoch: 8130, Batch Gradient Norm after: 11.516817099797324
Epoch 8131/10000, Prediction Accuracy = 62.952%, Loss = 0.390631639957428
Epoch: 8131, Batch Gradient Norm: 10.665303476625569
Epoch: 8131, Batch Gradient Norm after: 10.665303476625569
Epoch 8132/10000, Prediction Accuracy = 62.95%, Loss = 0.3855541169643402
Epoch: 8132, Batch Gradient Norm: 6.344377456116303
Epoch: 8132, Batch Gradient Norm after: 6.344377456116303
Epoch 8133/10000, Prediction Accuracy = 63.269999999999996%, Loss = 0.36172735691070557
Epoch: 8133, Batch Gradient Norm: 11.161415815108361
Epoch: 8133, Batch Gradient Norm after: 11.161415815108361
Epoch 8134/10000, Prediction Accuracy = 63.178%, Loss = 0.39110660552978516
Epoch: 8134, Batch Gradient Norm: 11.517743645128132
Epoch: 8134, Batch Gradient Norm after: 11.517743645128132
Epoch 8135/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.39440528154373167
Epoch: 8135, Batch Gradient Norm: 7.96122631723843
Epoch: 8135, Batch Gradient Norm after: 7.96122631723843
Epoch 8136/10000, Prediction Accuracy = 63.176%, Loss = 0.3705097198486328
Epoch: 8136, Batch Gradient Norm: 9.481604416113047
Epoch: 8136, Batch Gradient Norm after: 9.481604416113047
Epoch 8137/10000, Prediction Accuracy = 63.178%, Loss = 0.38379803895950315
Epoch: 8137, Batch Gradient Norm: 11.02516320614661
Epoch: 8137, Batch Gradient Norm after: 11.02516320614661
Epoch 8138/10000, Prediction Accuracy = 63.212%, Loss = 0.3933510959148407
Epoch: 8138, Batch Gradient Norm: 11.612526110806495
Epoch: 8138, Batch Gradient Norm after: 11.612526110806495
Epoch 8139/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.39139618873596194
Epoch: 8139, Batch Gradient Norm: 8.542768081313113
Epoch: 8139, Batch Gradient Norm after: 8.542768081313113
Epoch 8140/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.3718703031539917
Epoch: 8140, Batch Gradient Norm: 9.447210178842013
Epoch: 8140, Batch Gradient Norm after: 9.447210178842013
Epoch 8141/10000, Prediction Accuracy = 63.176%, Loss = 0.37811585068702697
Epoch: 8141, Batch Gradient Norm: 10.137914318749438
Epoch: 8141, Batch Gradient Norm after: 10.137914318749438
Epoch 8142/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.38139930963516233
Epoch: 8142, Batch Gradient Norm: 9.80562014944718
Epoch: 8142, Batch Gradient Norm after: 9.80562014944718
Epoch 8143/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3839955747127533
Epoch: 8143, Batch Gradient Norm: 7.172065062812698
Epoch: 8143, Batch Gradient Norm after: 7.172065062812698
Epoch 8144/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.3650040149688721
Epoch: 8144, Batch Gradient Norm: 11.339203712656518
Epoch: 8144, Batch Gradient Norm after: 11.339203712656518
Epoch 8145/10000, Prediction Accuracy = 63.194%, Loss = 0.39073488116264343
Epoch: 8145, Batch Gradient Norm: 10.489444084756373
Epoch: 8145, Batch Gradient Norm after: 10.489444084756373
Epoch 8146/10000, Prediction Accuracy = 63.10999999999999%, Loss = 0.38418992757797243
Epoch: 8146, Batch Gradient Norm: 10.957223523003426
Epoch: 8146, Batch Gradient Norm after: 10.957223523003426
Epoch 8147/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3965651154518127
Epoch: 8147, Batch Gradient Norm: 9.898871372506592
Epoch: 8147, Batch Gradient Norm after: 9.898871372506592
Epoch 8148/10000, Prediction Accuracy = 63.152%, Loss = 0.38257455825805664
Epoch: 8148, Batch Gradient Norm: 9.515472855380057
Epoch: 8148, Batch Gradient Norm after: 9.515472855380057
Epoch 8149/10000, Prediction Accuracy = 63.056%, Loss = 0.37832985520362855
Epoch: 8149, Batch Gradient Norm: 6.767943328257983
Epoch: 8149, Batch Gradient Norm after: 6.767943328257983
Epoch 8150/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.3639171361923218
Epoch: 8150, Batch Gradient Norm: 9.854747474550113
Epoch: 8150, Batch Gradient Norm after: 9.854747474550113
Epoch 8151/10000, Prediction Accuracy = 62.988%, Loss = 0.3802450180053711
Epoch: 8151, Batch Gradient Norm: 11.20370621199517
Epoch: 8151, Batch Gradient Norm after: 11.20370621199517
Epoch 8152/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3900802493095398
Epoch: 8152, Batch Gradient Norm: 11.460225899811439
Epoch: 8152, Batch Gradient Norm after: 11.460225899811439
Epoch 8153/10000, Prediction Accuracy = 63.064%, Loss = 0.3930723607540131
Epoch: 8153, Batch Gradient Norm: 8.953839305400747
Epoch: 8153, Batch Gradient Norm after: 8.953839305400747
Epoch 8154/10000, Prediction Accuracy = 63.306%, Loss = 0.37463198900222777
Epoch: 8154, Batch Gradient Norm: 9.667003865992822
Epoch: 8154, Batch Gradient Norm after: 9.667003865992822
Epoch 8155/10000, Prediction Accuracy = 63.016%, Loss = 0.37850295901298525
Epoch: 8155, Batch Gradient Norm: 9.494198125520155
Epoch: 8155, Batch Gradient Norm after: 9.494198125520155
Epoch 8156/10000, Prediction Accuracy = 63.164%, Loss = 0.37647274136543274
Epoch: 8156, Batch Gradient Norm: 11.36282339945525
Epoch: 8156, Batch Gradient Norm after: 11.36282339945525
Epoch 8157/10000, Prediction Accuracy = 63.03399999999999%, Loss = 0.39315614104270935
Epoch: 8157, Batch Gradient Norm: 9.48232679892119
Epoch: 8157, Batch Gradient Norm after: 9.48232679892119
Epoch 8158/10000, Prediction Accuracy = 63.233999999999995%, Loss = 0.37592525482177735
Epoch: 8158, Batch Gradient Norm: 10.554468144978646
Epoch: 8158, Batch Gradient Norm after: 10.554468144978646
Epoch 8159/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.3847900390625
Epoch: 8159, Batch Gradient Norm: 10.57887009418441
Epoch: 8159, Batch Gradient Norm after: 10.57887009418441
Epoch 8160/10000, Prediction Accuracy = 62.822%, Loss = 0.38618598580360414
Epoch: 8160, Batch Gradient Norm: 8.895311740581345
Epoch: 8160, Batch Gradient Norm after: 8.895311740581345
Epoch 8161/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3738600552082062
Epoch: 8161, Batch Gradient Norm: 9.389946988024176
Epoch: 8161, Batch Gradient Norm after: 9.389946988024176
Epoch 8162/10000, Prediction Accuracy = 63.08%, Loss = 0.37993706464767457
Epoch: 8162, Batch Gradient Norm: 9.417897671321432
Epoch: 8162, Batch Gradient Norm after: 9.417897671321432
Epoch 8163/10000, Prediction Accuracy = 63.214%, Loss = 0.37610284686088563
Epoch: 8163, Batch Gradient Norm: 11.693756363898498
Epoch: 8163, Batch Gradient Norm after: 11.693756363898498
Epoch 8164/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.38972286581993104
Epoch: 8164, Batch Gradient Norm: 8.343139291181318
Epoch: 8164, Batch Gradient Norm after: 8.343139291181318
Epoch 8165/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.3712557852268219
Epoch: 8165, Batch Gradient Norm: 7.693653705739916
Epoch: 8165, Batch Gradient Norm after: 7.693653705739916
Epoch 8166/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.36727548241615293
Epoch: 8166, Batch Gradient Norm: 9.252384523499725
Epoch: 8166, Batch Gradient Norm after: 9.252384523499725
Epoch 8167/10000, Prediction Accuracy = 62.98599999999999%, Loss = 0.3761951565742493
Epoch: 8167, Batch Gradient Norm: 11.37209899275859
Epoch: 8167, Batch Gradient Norm after: 11.37209899275859
Epoch 8168/10000, Prediction Accuracy = 62.914%, Loss = 0.3902911007404327
Epoch: 8168, Batch Gradient Norm: 12.262343674738007
Epoch: 8168, Batch Gradient Norm after: 12.262343674738007
Epoch 8169/10000, Prediction Accuracy = 62.806%, Loss = 0.4011471509933472
Epoch: 8169, Batch Gradient Norm: 10.471978301014467
Epoch: 8169, Batch Gradient Norm after: 10.471978301014467
Epoch 8170/10000, Prediction Accuracy = 62.962%, Loss = 0.38406510949134826
Epoch: 8170, Batch Gradient Norm: 9.76775423895588
Epoch: 8170, Batch Gradient Norm after: 9.76775423895588
Epoch 8171/10000, Prediction Accuracy = 63.074%, Loss = 0.3801371991634369
Epoch: 8171, Batch Gradient Norm: 9.217710493001356
Epoch: 8171, Batch Gradient Norm after: 9.217710493001356
Epoch 8172/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.37660115361213686
Epoch: 8172, Batch Gradient Norm: 7.617162050481086
Epoch: 8172, Batch Gradient Norm after: 7.617162050481086
Epoch 8173/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.36623375415802
Epoch: 8173, Batch Gradient Norm: 8.391077139013861
Epoch: 8173, Batch Gradient Norm after: 8.391077139013861
Epoch 8174/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.37203693985939024
Epoch: 8174, Batch Gradient Norm: 12.405828420024678
Epoch: 8174, Batch Gradient Norm after: 12.405828420024678
Epoch 8175/10000, Prediction Accuracy = 62.96%, Loss = 0.4005543410778046
Epoch: 8175, Batch Gradient Norm: 9.647327805442139
Epoch: 8175, Batch Gradient Norm after: 9.647327805442139
Epoch 8176/10000, Prediction Accuracy = 62.987999999999985%, Loss = 0.3791778028011322
Epoch: 8176, Batch Gradient Norm: 9.233661567068237
Epoch: 8176, Batch Gradient Norm after: 9.233661567068237
Epoch 8177/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3781655848026276
Epoch: 8177, Batch Gradient Norm: 7.3054668636254805
Epoch: 8177, Batch Gradient Norm after: 7.3054668636254805
Epoch 8178/10000, Prediction Accuracy = 63.138%, Loss = 0.3640163719654083
Epoch: 8178, Batch Gradient Norm: 8.029221140960326
Epoch: 8178, Batch Gradient Norm after: 8.029221140960326
Epoch 8179/10000, Prediction Accuracy = 63.032000000000004%, Loss = 0.36834738254547117
Epoch: 8179, Batch Gradient Norm: 10.813415638745271
Epoch: 8179, Batch Gradient Norm after: 10.813415638745271
Epoch 8180/10000, Prediction Accuracy = 63.238%, Loss = 0.38686447143554686
Epoch: 8180, Batch Gradient Norm: 11.802079943627886
Epoch: 8180, Batch Gradient Norm after: 11.802079943627886
Epoch 8181/10000, Prediction Accuracy = 63.13599999999999%, Loss = 0.39618977904319763
Epoch: 8181, Batch Gradient Norm: 6.061789882908853
Epoch: 8181, Batch Gradient Norm after: 6.061789882908853
Epoch 8182/10000, Prediction Accuracy = 63.194%, Loss = 0.3595539927482605
Epoch: 8182, Batch Gradient Norm: 9.471600190109395
Epoch: 8182, Batch Gradient Norm after: 9.471600190109395
Epoch 8183/10000, Prediction Accuracy = 63.088%, Loss = 0.3752224385738373
Epoch: 8183, Batch Gradient Norm: 8.99704865218984
Epoch: 8183, Batch Gradient Norm after: 8.99704865218984
Epoch 8184/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.37736108899116516
Epoch: 8184, Batch Gradient Norm: 7.87010602355439
Epoch: 8184, Batch Gradient Norm after: 7.87010602355439
Epoch 8185/10000, Prediction Accuracy = 63.012%, Loss = 0.3662832617759705
Epoch: 8185, Batch Gradient Norm: 12.577524950786565
Epoch: 8185, Batch Gradient Norm after: 12.577524950786565
Epoch 8186/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.4032429397106171
Epoch: 8186, Batch Gradient Norm: 9.776241478323275
Epoch: 8186, Batch Gradient Norm after: 9.776241478323275
Epoch 8187/10000, Prediction Accuracy = 63.286%, Loss = 0.38349097967147827
Epoch: 8187, Batch Gradient Norm: 8.32991655693915
Epoch: 8187, Batch Gradient Norm after: 8.32991655693915
Epoch 8188/10000, Prediction Accuracy = 63.182%, Loss = 0.368101704120636
Epoch: 8188, Batch Gradient Norm: 12.093680234095382
Epoch: 8188, Batch Gradient Norm after: 12.093680234095382
Epoch 8189/10000, Prediction Accuracy = 63.238%, Loss = 0.39496334791183474
Epoch: 8189, Batch Gradient Norm: 10.277307601131847
Epoch: 8189, Batch Gradient Norm after: 10.277307601131847
Epoch 8190/10000, Prediction Accuracy = 63.004%, Loss = 0.3819733202457428
Epoch: 8190, Batch Gradient Norm: 10.783745890333979
Epoch: 8190, Batch Gradient Norm after: 10.783745890333979
Epoch 8191/10000, Prediction Accuracy = 63.048%, Loss = 0.3891949713230133
Epoch: 8191, Batch Gradient Norm: 7.778250214286159
Epoch: 8191, Batch Gradient Norm after: 7.778250214286159
Epoch 8192/10000, Prediction Accuracy = 63.072%, Loss = 0.36678336262702943
Epoch: 8192, Batch Gradient Norm: 8.865526640490474
Epoch: 8192, Batch Gradient Norm after: 8.865526640490474
Epoch 8193/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3759938180446625
Epoch: 8193, Batch Gradient Norm: 11.996616930664922
Epoch: 8193, Batch Gradient Norm after: 11.996616930664922
Epoch 8194/10000, Prediction Accuracy = 63.036%, Loss = 0.40039881467819216
Epoch: 8194, Batch Gradient Norm: 13.277757992701853
Epoch: 8194, Batch Gradient Norm after: 13.277757992701853
Epoch 8195/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.4058650732040405
Epoch: 8195, Batch Gradient Norm: 10.077276405846627
Epoch: 8195, Batch Gradient Norm after: 10.077276405846627
Epoch 8196/10000, Prediction Accuracy = 63.15%, Loss = 0.380131322145462
Epoch: 8196, Batch Gradient Norm: 10.068458070082205
Epoch: 8196, Batch Gradient Norm after: 10.068458070082205
Epoch 8197/10000, Prediction Accuracy = 62.952%, Loss = 0.38500866293907166
Epoch: 8197, Batch Gradient Norm: 9.003574618385676
Epoch: 8197, Batch Gradient Norm after: 9.003574618385676
Epoch 8198/10000, Prediction Accuracy = 63.092000000000006%, Loss = 0.374724680185318
Epoch: 8198, Batch Gradient Norm: 11.839942415637104
Epoch: 8198, Batch Gradient Norm after: 11.839942415637104
Epoch 8199/10000, Prediction Accuracy = 63.08%, Loss = 0.39306824207305907
Epoch: 8199, Batch Gradient Norm: 10.040617622357953
Epoch: 8199, Batch Gradient Norm after: 10.040617622357953
Epoch 8200/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3850466549396515
Epoch: 8200, Batch Gradient Norm: 9.511857599506834
Epoch: 8200, Batch Gradient Norm after: 9.511857599506834
Epoch 8201/10000, Prediction Accuracy = 63.134%, Loss = 0.3772279262542725
Epoch: 8201, Batch Gradient Norm: 7.986619109471328
Epoch: 8201, Batch Gradient Norm after: 7.986619109471328
Epoch 8202/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.3687404990196228
Epoch: 8202, Batch Gradient Norm: 7.340813746363683
Epoch: 8202, Batch Gradient Norm after: 7.340813746363683
Epoch 8203/10000, Prediction Accuracy = 63.088%, Loss = 0.3655804395675659
Epoch: 8203, Batch Gradient Norm: 9.239259865860838
Epoch: 8203, Batch Gradient Norm after: 9.239259865860838
Epoch 8204/10000, Prediction Accuracy = 63.21600000000001%, Loss = 0.3780862748622894
Epoch: 8204, Batch Gradient Norm: 9.097761826296143
Epoch: 8204, Batch Gradient Norm after: 9.097761826296143
Epoch 8205/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.37589330673217775
Epoch: 8205, Batch Gradient Norm: 9.959773829032537
Epoch: 8205, Batch Gradient Norm after: 9.959773829032537
Epoch 8206/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.3777132272720337
Epoch: 8206, Batch Gradient Norm: 10.155273621614642
Epoch: 8206, Batch Gradient Norm after: 10.155273621614642
Epoch 8207/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.38094732761383054
Epoch: 8207, Batch Gradient Norm: 12.297470738480921
Epoch: 8207, Batch Gradient Norm after: 12.297470738480921
Epoch 8208/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.3980628788471222
Epoch: 8208, Batch Gradient Norm: 9.976692742439349
Epoch: 8208, Batch Gradient Norm after: 9.976692742439349
Epoch 8209/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.3800671100616455
Epoch: 8209, Batch Gradient Norm: 9.721276831762967
Epoch: 8209, Batch Gradient Norm after: 9.721276831762967
Epoch 8210/10000, Prediction Accuracy = 63.096000000000004%, Loss = 0.37874733209609984
Epoch: 8210, Batch Gradient Norm: 7.6197366595371685
Epoch: 8210, Batch Gradient Norm after: 7.6197366595371685
Epoch 8211/10000, Prediction Accuracy = 62.928%, Loss = 0.36634031534194944
Epoch: 8211, Batch Gradient Norm: 9.776420169962169
Epoch: 8211, Batch Gradient Norm after: 9.776420169962169
Epoch 8212/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.37982465624809264
Epoch: 8212, Batch Gradient Norm: 9.051416632503678
Epoch: 8212, Batch Gradient Norm after: 9.051416632503678
Epoch 8213/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.37333999276161195
Epoch: 8213, Batch Gradient Norm: 9.695035978219769
Epoch: 8213, Batch Gradient Norm after: 9.695035978219769
Epoch 8214/10000, Prediction Accuracy = 63.198%, Loss = 0.3757541000843048
Epoch: 8214, Batch Gradient Norm: 11.233691756299054
Epoch: 8214, Batch Gradient Norm after: 11.233691756299054
Epoch 8215/10000, Prediction Accuracy = 63.0%, Loss = 0.3909253478050232
Epoch: 8215, Batch Gradient Norm: 8.57167770524705
Epoch: 8215, Batch Gradient Norm after: 8.57167770524705
Epoch 8216/10000, Prediction Accuracy = 63.1%, Loss = 0.3686293840408325
Epoch: 8216, Batch Gradient Norm: 7.065805762606332
Epoch: 8216, Batch Gradient Norm after: 7.065805762606332
Epoch 8217/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.3622598767280579
Epoch: 8217, Batch Gradient Norm: 9.800484502097783
Epoch: 8217, Batch Gradient Norm after: 9.800484502097783
Epoch 8218/10000, Prediction Accuracy = 63.116%, Loss = 0.378616464138031
Epoch: 8218, Batch Gradient Norm: 11.48353767127114
Epoch: 8218, Batch Gradient Norm after: 11.48353767127114
Epoch 8219/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3972900927066803
Epoch: 8219, Batch Gradient Norm: 9.123547180475976
Epoch: 8219, Batch Gradient Norm after: 9.123547180475976
Epoch 8220/10000, Prediction Accuracy = 63.27%, Loss = 0.3741771638393402
Epoch: 8220, Batch Gradient Norm: 12.421430899561607
Epoch: 8220, Batch Gradient Norm after: 12.421430899561607
Epoch 8221/10000, Prediction Accuracy = 63.122%, Loss = 0.3977426767349243
Epoch: 8221, Batch Gradient Norm: 10.646934331098407
Epoch: 8221, Batch Gradient Norm after: 10.646934331098407
Epoch 8222/10000, Prediction Accuracy = 63.0%, Loss = 0.3857854962348938
Epoch: 8222, Batch Gradient Norm: 8.27526856343929
Epoch: 8222, Batch Gradient Norm after: 8.27526856343929
Epoch 8223/10000, Prediction Accuracy = 63.11600000000001%, Loss = 0.36867867708206176
Epoch: 8223, Batch Gradient Norm: 10.433773689914801
Epoch: 8223, Batch Gradient Norm after: 10.433773689914801
Epoch 8224/10000, Prediction Accuracy = 63.174%, Loss = 0.3890017092227936
Epoch: 8224, Batch Gradient Norm: 9.403939817051661
Epoch: 8224, Batch Gradient Norm after: 9.403939817051661
Epoch 8225/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.3777289569377899
Epoch: 8225, Batch Gradient Norm: 8.67527788129258
Epoch: 8225, Batch Gradient Norm after: 8.67527788129258
Epoch 8226/10000, Prediction Accuracy = 63.242%, Loss = 0.37346820831298827
Epoch: 8226, Batch Gradient Norm: 11.394817056016027
Epoch: 8226, Batch Gradient Norm after: 11.394817056016027
Epoch 8227/10000, Prediction Accuracy = 63.114%, Loss = 0.3917879045009613
Epoch: 8227, Batch Gradient Norm: 11.105196266113532
Epoch: 8227, Batch Gradient Norm after: 11.105196266113532
Epoch 8228/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3853165030479431
Epoch: 8228, Batch Gradient Norm: 8.378349006503115
Epoch: 8228, Batch Gradient Norm after: 8.378349006503115
Epoch 8229/10000, Prediction Accuracy = 63.29%, Loss = 0.37048361301422117
Epoch: 8229, Batch Gradient Norm: 9.911193699013946
Epoch: 8229, Batch Gradient Norm after: 9.911193699013946
Epoch 8230/10000, Prediction Accuracy = 63.288%, Loss = 0.38196614384651184
Epoch: 8230, Batch Gradient Norm: 7.020431481981409
Epoch: 8230, Batch Gradient Norm after: 7.020431481981409
Epoch 8231/10000, Prediction Accuracy = 63.222%, Loss = 0.36368821263313295
Epoch: 8231, Batch Gradient Norm: 10.316735911407985
Epoch: 8231, Batch Gradient Norm after: 10.316735911407985
Epoch 8232/10000, Prediction Accuracy = 63.124%, Loss = 0.3805055320262909
Epoch: 8232, Batch Gradient Norm: 9.278168753894844
Epoch: 8232, Batch Gradient Norm after: 9.278168753894844
Epoch 8233/10000, Prediction Accuracy = 63.076%, Loss = 0.3748642444610596
Epoch: 8233, Batch Gradient Norm: 8.882770616307855
Epoch: 8233, Batch Gradient Norm after: 8.882770616307855
Epoch 8234/10000, Prediction Accuracy = 63.136%, Loss = 0.3721238374710083
Epoch: 8234, Batch Gradient Norm: 13.1942854481854
Epoch: 8234, Batch Gradient Norm after: 13.1942854481854
Epoch 8235/10000, Prediction Accuracy = 63.188%, Loss = 0.40719549655914306
Epoch: 8235, Batch Gradient Norm: 11.181587460895289
Epoch: 8235, Batch Gradient Norm after: 11.181587460895289
Epoch 8236/10000, Prediction Accuracy = 63.148%, Loss = 0.38945724368095397
Epoch: 8236, Batch Gradient Norm: 10.663840056576218
Epoch: 8236, Batch Gradient Norm after: 10.663840056576218
Epoch 8237/10000, Prediction Accuracy = 63.282000000000004%, Loss = 0.3825790286064148
Epoch: 8237, Batch Gradient Norm: 7.938353055052175
Epoch: 8237, Batch Gradient Norm after: 7.938353055052175
Epoch 8238/10000, Prediction Accuracy = 63.194%, Loss = 0.36536593437194825
Epoch: 8238, Batch Gradient Norm: 7.8606710426379856
Epoch: 8238, Batch Gradient Norm after: 7.8606710426379856
Epoch 8239/10000, Prediction Accuracy = 63.114%, Loss = 0.36807578802108765
Epoch: 8239, Batch Gradient Norm: 9.569013271129988
Epoch: 8239, Batch Gradient Norm after: 9.569013271129988
Epoch 8240/10000, Prediction Accuracy = 63.188%, Loss = 0.3803484797477722
Epoch: 8240, Batch Gradient Norm: 12.203746012635095
Epoch: 8240, Batch Gradient Norm after: 12.203746012635095
Epoch 8241/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.39433653354644777
Epoch: 8241, Batch Gradient Norm: 12.171364711068861
Epoch: 8241, Batch Gradient Norm after: 12.171364711068861
Epoch 8242/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.3971921384334564
Epoch: 8242, Batch Gradient Norm: 9.924823468790965
Epoch: 8242, Batch Gradient Norm after: 9.924823468790965
Epoch 8243/10000, Prediction Accuracy = 63.098%, Loss = 0.3790067195892334
Epoch: 8243, Batch Gradient Norm: 8.951229405481095
Epoch: 8243, Batch Gradient Norm after: 8.951229405481095
Epoch 8244/10000, Prediction Accuracy = 63.164%, Loss = 0.3722354888916016
Epoch: 8244, Batch Gradient Norm: 8.22549053917852
Epoch: 8244, Batch Gradient Norm after: 8.22549053917852
Epoch 8245/10000, Prediction Accuracy = 63.254%, Loss = 0.36919450759887695
Epoch: 8245, Batch Gradient Norm: 10.299428275047953
Epoch: 8245, Batch Gradient Norm after: 10.299428275047953
Epoch 8246/10000, Prediction Accuracy = 63.062%, Loss = 0.3831920146942139
Epoch: 8246, Batch Gradient Norm: 8.176914942362956
Epoch: 8246, Batch Gradient Norm after: 8.176914942362956
Epoch 8247/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.3679010450839996
Epoch: 8247, Batch Gradient Norm: 9.996637738746534
Epoch: 8247, Batch Gradient Norm after: 9.996637738746534
Epoch 8248/10000, Prediction Accuracy = 63.14%, Loss = 0.37736201882362364
Epoch: 8248, Batch Gradient Norm: 10.385752154114325
Epoch: 8248, Batch Gradient Norm after: 10.385752154114325
Epoch 8249/10000, Prediction Accuracy = 63.02%, Loss = 0.38450455069541933
Epoch: 8249, Batch Gradient Norm: 8.892551925850125
Epoch: 8249, Batch Gradient Norm after: 8.892551925850125
Epoch 8250/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.37081356048583985
Epoch: 8250, Batch Gradient Norm: 11.02805690915448
Epoch: 8250, Batch Gradient Norm after: 11.02805690915448
Epoch 8251/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.38511461615562437
Epoch: 8251, Batch Gradient Norm: 8.884035966908646
Epoch: 8251, Batch Gradient Norm after: 8.884035966908646
Epoch 8252/10000, Prediction Accuracy = 63.274%, Loss = 0.37289788722991946
Epoch: 8252, Batch Gradient Norm: 8.562765594905164
Epoch: 8252, Batch Gradient Norm after: 8.562765594905164
Epoch 8253/10000, Prediction Accuracy = 63.15400000000001%, Loss = 0.3687433898448944
Epoch: 8253, Batch Gradient Norm: 8.659041526420815
Epoch: 8253, Batch Gradient Norm after: 8.659041526420815
Epoch 8254/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.37091625928878785
Epoch: 8254, Batch Gradient Norm: 9.141639552097253
Epoch: 8254, Batch Gradient Norm after: 9.141639552097253
Epoch 8255/10000, Prediction Accuracy = 63.126%, Loss = 0.37533714771270754
Epoch: 8255, Batch Gradient Norm: 11.060546955747233
Epoch: 8255, Batch Gradient Norm after: 11.060546955747233
Epoch 8256/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.38684465885162356
Epoch: 8256, Batch Gradient Norm: 10.229525334956566
Epoch: 8256, Batch Gradient Norm after: 10.229525334956566
Epoch 8257/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.3804431140422821
Epoch: 8257, Batch Gradient Norm: 12.096300844056
Epoch: 8257, Batch Gradient Norm after: 12.096300844056
Epoch 8258/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.3979610025882721
Epoch: 8258, Batch Gradient Norm: 12.447223832792309
Epoch: 8258, Batch Gradient Norm after: 12.447223832792309
Epoch 8259/10000, Prediction Accuracy = 63.02%, Loss = 0.3995051860809326
Epoch: 8259, Batch Gradient Norm: 9.374299425350904
Epoch: 8259, Batch Gradient Norm after: 9.374299425350904
Epoch 8260/10000, Prediction Accuracy = 63.104%, Loss = 0.37671366333961487
Epoch: 8260, Batch Gradient Norm: 8.828297556487662
Epoch: 8260, Batch Gradient Norm after: 8.828297556487662
Epoch 8261/10000, Prediction Accuracy = 63.088%, Loss = 0.3735962867736816
Epoch: 8261, Batch Gradient Norm: 7.852016476230828
Epoch: 8261, Batch Gradient Norm after: 7.852016476230828
Epoch 8262/10000, Prediction Accuracy = 63.15999999999999%, Loss = 0.3672905623912811
Epoch: 8262, Batch Gradient Norm: 7.13955030950402
Epoch: 8262, Batch Gradient Norm after: 7.13955030950402
Epoch 8263/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.36062145233154297
Epoch: 8263, Batch Gradient Norm: 9.332991074631916
Epoch: 8263, Batch Gradient Norm after: 9.332991074631916
Epoch 8264/10000, Prediction Accuracy = 63.148%, Loss = 0.37401680946350097
Epoch: 8264, Batch Gradient Norm: 11.579295168788189
Epoch: 8264, Batch Gradient Norm after: 11.579295168788189
Epoch 8265/10000, Prediction Accuracy = 63.176%, Loss = 0.3913791835308075
Epoch: 8265, Batch Gradient Norm: 10.36891334700662
Epoch: 8265, Batch Gradient Norm after: 10.36891334700662
Epoch 8266/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3834713339805603
Epoch: 8266, Batch Gradient Norm: 9.753269487548955
Epoch: 8266, Batch Gradient Norm after: 9.753269487548955
Epoch 8267/10000, Prediction Accuracy = 63.29%, Loss = 0.3781682252883911
Epoch: 8267, Batch Gradient Norm: 9.697986944503398
Epoch: 8267, Batch Gradient Norm after: 9.697986944503398
Epoch 8268/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.37718775272369387
Epoch: 8268, Batch Gradient Norm: 9.138791422218288
Epoch: 8268, Batch Gradient Norm after: 9.138791422218288
Epoch 8269/10000, Prediction Accuracy = 63.0%, Loss = 0.37217122316360474
Epoch: 8269, Batch Gradient Norm: 9.096760701675091
Epoch: 8269, Batch Gradient Norm after: 9.096760701675091
Epoch 8270/10000, Prediction Accuracy = 63.221999999999994%, Loss = 0.3737055718898773
Epoch: 8270, Batch Gradient Norm: 10.115067982630828
Epoch: 8270, Batch Gradient Norm after: 10.115067982630828
Epoch 8271/10000, Prediction Accuracy = 63.1%, Loss = 0.3849746286869049
Epoch: 8271, Batch Gradient Norm: 10.57664311131585
Epoch: 8271, Batch Gradient Norm after: 10.57664311131585
Epoch 8272/10000, Prediction Accuracy = 63.146%, Loss = 0.380914044380188
Epoch: 8272, Batch Gradient Norm: 11.555584052382999
Epoch: 8272, Batch Gradient Norm after: 11.555584052382999
Epoch 8273/10000, Prediction Accuracy = 63.062%, Loss = 0.3909117579460144
Epoch: 8273, Batch Gradient Norm: 9.553009590119288
Epoch: 8273, Batch Gradient Norm after: 9.553009590119288
Epoch 8274/10000, Prediction Accuracy = 63.334%, Loss = 0.37701348066329954
Epoch: 8274, Batch Gradient Norm: 9.902347770991573
Epoch: 8274, Batch Gradient Norm after: 9.902347770991573
Epoch 8275/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.37728739976882936
Epoch: 8275, Batch Gradient Norm: 10.43185789430928
Epoch: 8275, Batch Gradient Norm after: 10.43185789430928
Epoch 8276/10000, Prediction Accuracy = 63.202%, Loss = 0.38309266567230227
Epoch: 8276, Batch Gradient Norm: 9.075371528952502
Epoch: 8276, Batch Gradient Norm after: 9.075371528952502
Epoch 8277/10000, Prediction Accuracy = 63.262%, Loss = 0.3732309341430664
Epoch: 8277, Batch Gradient Norm: 9.382807612105344
Epoch: 8277, Batch Gradient Norm after: 9.382807612105344
Epoch 8278/10000, Prediction Accuracy = 63.298%, Loss = 0.37777227759361265
Epoch: 8278, Batch Gradient Norm: 7.510889548753392
Epoch: 8278, Batch Gradient Norm after: 7.510889548753392
Epoch 8279/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.3662034869194031
Epoch: 8279, Batch Gradient Norm: 9.177012646331146
Epoch: 8279, Batch Gradient Norm after: 9.177012646331146
Epoch 8280/10000, Prediction Accuracy = 63.386%, Loss = 0.37412993907928466
Epoch: 8280, Batch Gradient Norm: 9.480731687614318
Epoch: 8280, Batch Gradient Norm after: 9.480731687614318
Epoch 8281/10000, Prediction Accuracy = 63.136%, Loss = 0.3764634966850281
Epoch: 8281, Batch Gradient Norm: 7.520506649007546
Epoch: 8281, Batch Gradient Norm after: 7.520506649007546
Epoch 8282/10000, Prediction Accuracy = 63.288%, Loss = 0.3645909011363983
Epoch: 8282, Batch Gradient Norm: 8.508763736058008
Epoch: 8282, Batch Gradient Norm after: 8.508763736058008
Epoch 8283/10000, Prediction Accuracy = 63.186%, Loss = 0.36773608922958373
Epoch: 8283, Batch Gradient Norm: 10.387264629528472
Epoch: 8283, Batch Gradient Norm after: 10.387264629528472
Epoch 8284/10000, Prediction Accuracy = 62.912%, Loss = 0.3823882699012756
Epoch: 8284, Batch Gradient Norm: 9.777521898874829
Epoch: 8284, Batch Gradient Norm after: 9.777521898874829
Epoch 8285/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.37769703269004823
Epoch: 8285, Batch Gradient Norm: 11.632789128641754
Epoch: 8285, Batch Gradient Norm after: 11.632789128641754
Epoch 8286/10000, Prediction Accuracy = 63.136%, Loss = 0.3875689446926117
Epoch: 8286, Batch Gradient Norm: 11.014828543442496
Epoch: 8286, Batch Gradient Norm after: 11.014828543442496
Epoch 8287/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3851711392402649
Epoch: 8287, Batch Gradient Norm: 8.973922895456452
Epoch: 8287, Batch Gradient Norm after: 8.973922895456452
Epoch 8288/10000, Prediction Accuracy = 63.012%, Loss = 0.3704756677150726
Epoch: 8288, Batch Gradient Norm: 9.486061151044083
Epoch: 8288, Batch Gradient Norm after: 9.486061151044083
Epoch 8289/10000, Prediction Accuracy = 63.186%, Loss = 0.375869083404541
Epoch: 8289, Batch Gradient Norm: 12.042021752418952
Epoch: 8289, Batch Gradient Norm after: 12.042021752418952
Epoch 8290/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3931033551692963
Epoch: 8290, Batch Gradient Norm: 8.699529889646737
Epoch: 8290, Batch Gradient Norm after: 8.699529889646737
Epoch 8291/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.3720232129096985
Epoch: 8291, Batch Gradient Norm: 8.62794464731186
Epoch: 8291, Batch Gradient Norm after: 8.62794464731186
Epoch 8292/10000, Prediction Accuracy = 63.221999999999994%, Loss = 0.3719638526439667
Epoch: 8292, Batch Gradient Norm: 9.32738076384397
Epoch: 8292, Batch Gradient Norm after: 9.32738076384397
Epoch 8293/10000, Prediction Accuracy = 63.242%, Loss = 0.37591293454170227
Epoch: 8293, Batch Gradient Norm: 8.984221129627816
Epoch: 8293, Batch Gradient Norm after: 8.984221129627816
Epoch 8294/10000, Prediction Accuracy = 63.174%, Loss = 0.37438985109329226
Epoch: 8294, Batch Gradient Norm: 9.40134698455694
Epoch: 8294, Batch Gradient Norm after: 9.40134698455694
Epoch 8295/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3757950484752655
Epoch: 8295, Batch Gradient Norm: 9.7414509893677
Epoch: 8295, Batch Gradient Norm after: 9.7414509893677
Epoch 8296/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3781761169433594
Epoch: 8296, Batch Gradient Norm: 9.95358914328849
Epoch: 8296, Batch Gradient Norm after: 9.95358914328849
Epoch 8297/10000, Prediction Accuracy = 63.258%, Loss = 0.3789210975170135
Epoch: 8297, Batch Gradient Norm: 13.398855465002995
Epoch: 8297, Batch Gradient Norm after: 13.398855465002995
Epoch 8298/10000, Prediction Accuracy = 62.936%, Loss = 0.41518000364303587
Epoch: 8298, Batch Gradient Norm: 11.304412555390451
Epoch: 8298, Batch Gradient Norm after: 11.304412555390451
Epoch 8299/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3930380940437317
Epoch: 8299, Batch Gradient Norm: 9.889267051036104
Epoch: 8299, Batch Gradient Norm after: 9.889267051036104
Epoch 8300/10000, Prediction Accuracy = 63.134%, Loss = 0.379489403963089
Epoch: 8300, Batch Gradient Norm: 11.54196240646796
Epoch: 8300, Batch Gradient Norm after: 11.54196240646796
Epoch 8301/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3879520118236542
Epoch: 8301, Batch Gradient Norm: 11.946096154530657
Epoch: 8301, Batch Gradient Norm after: 11.946096154530657
Epoch 8302/10000, Prediction Accuracy = 63.386%, Loss = 0.3933586239814758
Epoch: 8302, Batch Gradient Norm: 11.425600383825062
Epoch: 8302, Batch Gradient Norm after: 11.425600383825062
Epoch 8303/10000, Prediction Accuracy = 63.222%, Loss = 0.39415804743766786
Epoch: 8303, Batch Gradient Norm: 8.627370458112853
Epoch: 8303, Batch Gradient Norm after: 8.627370458112853
Epoch 8304/10000, Prediction Accuracy = 63.29599999999999%, Loss = 0.3692521691322327
Epoch: 8304, Batch Gradient Norm: 7.504824075104303
Epoch: 8304, Batch Gradient Norm after: 7.504824075104303
Epoch 8305/10000, Prediction Accuracy = 63.35200000000001%, Loss = 0.36134445667266846
Epoch: 8305, Batch Gradient Norm: 8.04667745314547
Epoch: 8305, Batch Gradient Norm after: 8.04667745314547
Epoch 8306/10000, Prediction Accuracy = 63.28000000000001%, Loss = 0.36624751687049867
Epoch: 8306, Batch Gradient Norm: 10.920003781785024
Epoch: 8306, Batch Gradient Norm after: 10.920003781785024
Epoch 8307/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.3869692325592041
Epoch: 8307, Batch Gradient Norm: 9.042925730200052
Epoch: 8307, Batch Gradient Norm after: 9.042925730200052
Epoch 8308/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3739175915718079
Epoch: 8308, Batch Gradient Norm: 7.911095897444193
Epoch: 8308, Batch Gradient Norm after: 7.911095897444193
Epoch 8309/10000, Prediction Accuracy = 63.076%, Loss = 0.3671649873256683
Epoch: 8309, Batch Gradient Norm: 8.272624910947469
Epoch: 8309, Batch Gradient Norm after: 8.272624910947469
Epoch 8310/10000, Prediction Accuracy = 63.128%, Loss = 0.3697191715240479
Epoch: 8310, Batch Gradient Norm: 9.559802858620927
Epoch: 8310, Batch Gradient Norm after: 9.559802858620927
Epoch 8311/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.37736966609954836
Epoch: 8311, Batch Gradient Norm: 7.420197189738114
Epoch: 8311, Batch Gradient Norm after: 7.420197189738114
Epoch 8312/10000, Prediction Accuracy = 63.212%, Loss = 0.36347482204437254
Epoch: 8312, Batch Gradient Norm: 9.372478236090801
Epoch: 8312, Batch Gradient Norm after: 9.372478236090801
Epoch 8313/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3762245297431946
Epoch: 8313, Batch Gradient Norm: 10.37056648209719
Epoch: 8313, Batch Gradient Norm after: 10.37056648209719
Epoch 8314/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.38193618059158324
Epoch: 8314, Batch Gradient Norm: 12.802339792109342
Epoch: 8314, Batch Gradient Norm after: 12.802339792109342
Epoch 8315/10000, Prediction Accuracy = 62.926%, Loss = 0.40424872636795045
Epoch: 8315, Batch Gradient Norm: 9.123770826653223
Epoch: 8315, Batch Gradient Norm after: 9.123770826653223
Epoch 8316/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3723306775093079
Epoch: 8316, Batch Gradient Norm: 12.63876128681695
Epoch: 8316, Batch Gradient Norm after: 12.63876128681695
Epoch 8317/10000, Prediction Accuracy = 63.18999999999998%, Loss = 0.3941783905029297
Epoch: 8317, Batch Gradient Norm: 10.647759002855633
Epoch: 8317, Batch Gradient Norm after: 10.647759002855633
Epoch 8318/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.380481481552124
Epoch: 8318, Batch Gradient Norm: 8.705120933833783
Epoch: 8318, Batch Gradient Norm after: 8.705120933833783
Epoch 8319/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.36924396753311156
Epoch: 8319, Batch Gradient Norm: 10.031310914079983
Epoch: 8319, Batch Gradient Norm after: 10.031310914079983
Epoch 8320/10000, Prediction Accuracy = 63.08599999999999%, Loss = 0.3787854552268982
Epoch: 8320, Batch Gradient Norm: 6.417361640415536
Epoch: 8320, Batch Gradient Norm after: 6.417361640415536
Epoch 8321/10000, Prediction Accuracy = 63.186%, Loss = 0.3574177920818329
Epoch: 8321, Batch Gradient Norm: 7.795862247943268
Epoch: 8321, Batch Gradient Norm after: 7.795862247943268
Epoch 8322/10000, Prediction Accuracy = 63.266%, Loss = 0.3617882251739502
Epoch: 8322, Batch Gradient Norm: 11.860478693171391
Epoch: 8322, Batch Gradient Norm after: 11.860478693171391
Epoch 8323/10000, Prediction Accuracy = 63.138%, Loss = 0.39385061264038085
Epoch: 8323, Batch Gradient Norm: 10.381235193784573
Epoch: 8323, Batch Gradient Norm after: 10.381235193784573
Epoch 8324/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.38222368359565734
Epoch: 8324, Batch Gradient Norm: 9.001603708466858
Epoch: 8324, Batch Gradient Norm after: 9.001603708466858
Epoch 8325/10000, Prediction Accuracy = 63.19%, Loss = 0.37134782075881956
Epoch: 8325, Batch Gradient Norm: 8.851820193474264
Epoch: 8325, Batch Gradient Norm after: 8.851820193474264
Epoch 8326/10000, Prediction Accuracy = 63.16799999999999%, Loss = 0.37197513580322267
Epoch: 8326, Batch Gradient Norm: 9.173684223930715
Epoch: 8326, Batch Gradient Norm after: 9.173684223930715
Epoch 8327/10000, Prediction Accuracy = 63.25%, Loss = 0.3763414263725281
Epoch: 8327, Batch Gradient Norm: 10.003962395737316
Epoch: 8327, Batch Gradient Norm after: 10.003962395737316
Epoch 8328/10000, Prediction Accuracy = 63.2%, Loss = 0.3783977746963501
Epoch: 8328, Batch Gradient Norm: 12.205852376602452
Epoch: 8328, Batch Gradient Norm after: 12.205852376602452
Epoch 8329/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.3947765350341797
Epoch: 8329, Batch Gradient Norm: 11.570883573746594
Epoch: 8329, Batch Gradient Norm after: 11.570883573746594
Epoch 8330/10000, Prediction Accuracy = 63.278%, Loss = 0.3913114607334137
Epoch: 8330, Batch Gradient Norm: 9.56738469433406
Epoch: 8330, Batch Gradient Norm after: 9.56738469433406
Epoch 8331/10000, Prediction Accuracy = 63.169999999999995%, Loss = 0.3749897122383118
Epoch: 8331, Batch Gradient Norm: 8.95963174947872
Epoch: 8331, Batch Gradient Norm after: 8.95963174947872
Epoch 8332/10000, Prediction Accuracy = 63.266%, Loss = 0.37247341871261597
Epoch: 8332, Batch Gradient Norm: 9.06879664722434
Epoch: 8332, Batch Gradient Norm after: 9.06879664722434
Epoch 8333/10000, Prediction Accuracy = 63.114%, Loss = 0.3746996521949768
Epoch: 8333, Batch Gradient Norm: 8.234481195254391
Epoch: 8333, Batch Gradient Norm after: 8.234481195254391
Epoch 8334/10000, Prediction Accuracy = 63.28000000000001%, Loss = 0.3666693925857544
Epoch: 8334, Batch Gradient Norm: 9.629961893176503
Epoch: 8334, Batch Gradient Norm after: 9.629961893176503
Epoch 8335/10000, Prediction Accuracy = 63.145999999999994%, Loss = 0.380745655298233
Epoch: 8335, Batch Gradient Norm: 9.38339481060984
Epoch: 8335, Batch Gradient Norm after: 9.38339481060984
Epoch 8336/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3731578290462494
Epoch: 8336, Batch Gradient Norm: 11.335112928748078
Epoch: 8336, Batch Gradient Norm after: 11.335112928748078
Epoch 8337/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.38936592936515807
Epoch: 8337, Batch Gradient Norm: 10.056696935387587
Epoch: 8337, Batch Gradient Norm after: 10.056696935387587
Epoch 8338/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.37880677580833433
Epoch: 8338, Batch Gradient Norm: 11.182663340549563
Epoch: 8338, Batch Gradient Norm after: 11.182663340549563
Epoch 8339/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.38975889682769777
Epoch: 8339, Batch Gradient Norm: 9.650046982910373
Epoch: 8339, Batch Gradient Norm after: 9.650046982910373
Epoch 8340/10000, Prediction Accuracy = 63.188%, Loss = 0.37715070843696596
Epoch: 8340, Batch Gradient Norm: 8.072375936398428
Epoch: 8340, Batch Gradient Norm after: 8.072375936398428
Epoch 8341/10000, Prediction Accuracy = 63.246%, Loss = 0.36549248099327086
Epoch: 8341, Batch Gradient Norm: 10.742986083073154
Epoch: 8341, Batch Gradient Norm after: 10.742986083073154
Epoch 8342/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3815671980381012
Epoch: 8342, Batch Gradient Norm: 11.100917430725092
Epoch: 8342, Batch Gradient Norm after: 11.100917430725092
Epoch 8343/10000, Prediction Accuracy = 63.09799999999999%, Loss = 0.386101222038269
Epoch: 8343, Batch Gradient Norm: 10.982437126133574
Epoch: 8343, Batch Gradient Norm after: 10.982437126133574
Epoch 8344/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.38652983903884885
Epoch: 8344, Batch Gradient Norm: 10.880876074409016
Epoch: 8344, Batch Gradient Norm after: 10.880876074409016
Epoch 8345/10000, Prediction Accuracy = 63.278%, Loss = 0.3878468036651611
Epoch: 8345, Batch Gradient Norm: 8.3495700277227
Epoch: 8345, Batch Gradient Norm after: 8.3495700277227
Epoch 8346/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.369157612323761
Epoch: 8346, Batch Gradient Norm: 8.30247635656541
Epoch: 8346, Batch Gradient Norm after: 8.30247635656541
Epoch 8347/10000, Prediction Accuracy = 63.30800000000001%, Loss = 0.3649910569190979
Epoch: 8347, Batch Gradient Norm: 9.148879149297096
Epoch: 8347, Batch Gradient Norm after: 9.148879149297096
Epoch 8348/10000, Prediction Accuracy = 63.17800000000001%, Loss = 0.3714048147201538
Epoch: 8348, Batch Gradient Norm: 8.816360117859197
Epoch: 8348, Batch Gradient Norm after: 8.816360117859197
Epoch 8349/10000, Prediction Accuracy = 63.298%, Loss = 0.37074747681617737
Epoch: 8349, Batch Gradient Norm: 10.12157275410812
Epoch: 8349, Batch Gradient Norm after: 10.12157275410812
Epoch 8350/10000, Prediction Accuracy = 63.038%, Loss = 0.37864273190498354
Epoch: 8350, Batch Gradient Norm: 10.825317795843683
Epoch: 8350, Batch Gradient Norm after: 10.825317795843683
Epoch 8351/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.38576855659484866
Epoch: 8351, Batch Gradient Norm: 9.2912964574676
Epoch: 8351, Batch Gradient Norm after: 9.2912964574676
Epoch 8352/10000, Prediction Accuracy = 63.27%, Loss = 0.3730742514133453
Epoch: 8352, Batch Gradient Norm: 12.356948472414055
Epoch: 8352, Batch Gradient Norm after: 12.356948472414055
Epoch 8353/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.39467466473579405
Epoch: 8353, Batch Gradient Norm: 8.513882041581025
Epoch: 8353, Batch Gradient Norm after: 8.513882041581025
Epoch 8354/10000, Prediction Accuracy = 63.136%, Loss = 0.3704388439655304
Epoch: 8354, Batch Gradient Norm: 8.547705389265614
Epoch: 8354, Batch Gradient Norm after: 8.547705389265614
Epoch 8355/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.36986916065216063
Epoch: 8355, Batch Gradient Norm: 9.821077301747103
Epoch: 8355, Batch Gradient Norm after: 9.821077301747103
Epoch 8356/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.37735071778297424
Epoch: 8356, Batch Gradient Norm: 12.108742181797256
Epoch: 8356, Batch Gradient Norm after: 12.108742181797256
Epoch 8357/10000, Prediction Accuracy = 63.088%, Loss = 0.39426846504211427
Epoch: 8357, Batch Gradient Norm: 10.420977476898466
Epoch: 8357, Batch Gradient Norm after: 10.420977476898466
Epoch 8358/10000, Prediction Accuracy = 63.19%, Loss = 0.38106014728546145
Epoch: 8358, Batch Gradient Norm: 9.12083465557767
Epoch: 8358, Batch Gradient Norm after: 9.12083465557767
Epoch 8359/10000, Prediction Accuracy = 63.226%, Loss = 0.3694916903972626
Epoch: 8359, Batch Gradient Norm: 9.565561711237816
Epoch: 8359, Batch Gradient Norm after: 9.565561711237816
Epoch 8360/10000, Prediction Accuracy = 63.186%, Loss = 0.37500224709510804
Epoch: 8360, Batch Gradient Norm: 11.450711207756973
Epoch: 8360, Batch Gradient Norm after: 11.450711207756973
Epoch 8361/10000, Prediction Accuracy = 63.234%, Loss = 0.3926215171813965
Epoch: 8361, Batch Gradient Norm: 8.576187863682435
Epoch: 8361, Batch Gradient Norm after: 8.576187863682435
Epoch 8362/10000, Prediction Accuracy = 63.164%, Loss = 0.3685552358627319
Epoch: 8362, Batch Gradient Norm: 8.307737360617505
Epoch: 8362, Batch Gradient Norm after: 8.307737360617505
Epoch 8363/10000, Prediction Accuracy = 63.362%, Loss = 0.3664619982242584
Epoch: 8363, Batch Gradient Norm: 7.732462349132578
Epoch: 8363, Batch Gradient Norm after: 7.732462349132578
Epoch 8364/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.36355444192886355
Epoch: 8364, Batch Gradient Norm: 8.553309850289963
Epoch: 8364, Batch Gradient Norm after: 8.553309850289963
Epoch 8365/10000, Prediction Accuracy = 63.134%, Loss = 0.3688302278518677
Epoch: 8365, Batch Gradient Norm: 9.870862520648602
Epoch: 8365, Batch Gradient Norm after: 9.870862520648602
Epoch 8366/10000, Prediction Accuracy = 63.364%, Loss = 0.3808680474758148
Epoch: 8366, Batch Gradient Norm: 10.709419428144045
Epoch: 8366, Batch Gradient Norm after: 10.709419428144045
Epoch 8367/10000, Prediction Accuracy = 63.222%, Loss = 0.3861480951309204
Epoch: 8367, Batch Gradient Norm: 10.346242717389018
Epoch: 8367, Batch Gradient Norm after: 10.346242717389018
Epoch 8368/10000, Prediction Accuracy = 63.182%, Loss = 0.3818995296955109
Epoch: 8368, Batch Gradient Norm: 10.397126122480167
Epoch: 8368, Batch Gradient Norm after: 10.397126122480167
Epoch 8369/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.3834221839904785
Epoch: 8369, Batch Gradient Norm: 10.778671491640461
Epoch: 8369, Batch Gradient Norm after: 10.778671491640461
Epoch 8370/10000, Prediction Accuracy = 63.338%, Loss = 0.38384353518486025
Epoch: 8370, Batch Gradient Norm: 7.364081232178933
Epoch: 8370, Batch Gradient Norm after: 7.364081232178933
Epoch 8371/10000, Prediction Accuracy = 63.378%, Loss = 0.35934093594551086
Epoch: 8371, Batch Gradient Norm: 10.952162876640944
Epoch: 8371, Batch Gradient Norm after: 10.952162876640944
Epoch 8372/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.3833568632602692
Epoch: 8372, Batch Gradient Norm: 11.808061783706421
Epoch: 8372, Batch Gradient Norm after: 11.808061783706421
Epoch 8373/10000, Prediction Accuracy = 63.126%, Loss = 0.3897361993789673
Epoch: 8373, Batch Gradient Norm: 9.828654956154367
Epoch: 8373, Batch Gradient Norm after: 9.828654956154367
Epoch 8374/10000, Prediction Accuracy = 63.122%, Loss = 0.3785612881183624
Epoch: 8374, Batch Gradient Norm: 7.891400062404208
Epoch: 8374, Batch Gradient Norm after: 7.891400062404208
Epoch 8375/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.36350235939025877
Epoch: 8375, Batch Gradient Norm: 11.09342796858728
Epoch: 8375, Batch Gradient Norm after: 11.09342796858728
Epoch 8376/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.38403902649879457
Epoch: 8376, Batch Gradient Norm: 10.287898671570437
Epoch: 8376, Batch Gradient Norm after: 10.287898671570437
Epoch 8377/10000, Prediction Accuracy = 63.282%, Loss = 0.38104565143585206
Epoch: 8377, Batch Gradient Norm: 8.55631852165136
Epoch: 8377, Batch Gradient Norm after: 8.55631852165136
Epoch 8378/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3674354672431946
Epoch: 8378, Batch Gradient Norm: 8.489560119440645
Epoch: 8378, Batch Gradient Norm after: 8.489560119440645
Epoch 8379/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.36464282870292664
Epoch: 8379, Batch Gradient Norm: 9.884456789798506
Epoch: 8379, Batch Gradient Norm after: 9.884456789798506
Epoch 8380/10000, Prediction Accuracy = 63.102%, Loss = 0.374986469745636
Epoch: 8380, Batch Gradient Norm: 9.905718483217617
Epoch: 8380, Batch Gradient Norm after: 9.905718483217617
Epoch 8381/10000, Prediction Accuracy = 63.174%, Loss = 0.3745479345321655
Epoch: 8381, Batch Gradient Norm: 9.034647742896318
Epoch: 8381, Batch Gradient Norm after: 9.034647742896318
Epoch 8382/10000, Prediction Accuracy = 63.148%, Loss = 0.3711215853691101
Epoch: 8382, Batch Gradient Norm: 9.449804802475958
Epoch: 8382, Batch Gradient Norm after: 9.449804802475958
Epoch 8383/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.37450125217437746
Epoch: 8383, Batch Gradient Norm: 9.303463355202856
Epoch: 8383, Batch Gradient Norm after: 9.303463355202856
Epoch 8384/10000, Prediction Accuracy = 63.222%, Loss = 0.3721879482269287
Epoch: 8384, Batch Gradient Norm: 9.863006852911502
Epoch: 8384, Batch Gradient Norm after: 9.863006852911502
Epoch 8385/10000, Prediction Accuracy = 62.992%, Loss = 0.3790150821208954
Epoch: 8385, Batch Gradient Norm: 10.236955209039083
Epoch: 8385, Batch Gradient Norm after: 10.236955209039083
Epoch 8386/10000, Prediction Accuracy = 63.188%, Loss = 0.38049125075340273
Epoch: 8386, Batch Gradient Norm: 10.955002649810854
Epoch: 8386, Batch Gradient Norm after: 10.955002649810854
Epoch 8387/10000, Prediction Accuracy = 63.364%, Loss = 0.38272188901901244
Epoch: 8387, Batch Gradient Norm: 8.823656995145098
Epoch: 8387, Batch Gradient Norm after: 8.823656995145098
Epoch 8388/10000, Prediction Accuracy = 63.274%, Loss = 0.36814923882484435
Epoch: 8388, Batch Gradient Norm: 10.663178649417926
Epoch: 8388, Batch Gradient Norm after: 10.663178649417926
Epoch 8389/10000, Prediction Accuracy = 63.25%, Loss = 0.3814453125
Epoch: 8389, Batch Gradient Norm: 7.489360491646459
Epoch: 8389, Batch Gradient Norm after: 7.489360491646459
Epoch 8390/10000, Prediction Accuracy = 63.414%, Loss = 0.3627241551876068
Epoch: 8390, Batch Gradient Norm: 8.640701646622594
Epoch: 8390, Batch Gradient Norm after: 8.640701646622594
Epoch 8391/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.36884008049964906
Epoch: 8391, Batch Gradient Norm: 11.797115729346928
Epoch: 8391, Batch Gradient Norm after: 11.797115729346928
Epoch 8392/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.38827112317085266
Epoch: 8392, Batch Gradient Norm: 10.960697441007532
Epoch: 8392, Batch Gradient Norm after: 10.960697441007532
Epoch 8393/10000, Prediction Accuracy = 63.138%, Loss = 0.3819380044937134
Epoch: 8393, Batch Gradient Norm: 8.633713480222294
Epoch: 8393, Batch Gradient Norm after: 8.633713480222294
Epoch 8394/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3687205672264099
Epoch: 8394, Batch Gradient Norm: 12.271215874867533
Epoch: 8394, Batch Gradient Norm after: 12.271215874867533
Epoch 8395/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.4012634038925171
Epoch: 8395, Batch Gradient Norm: 7.06000764187732
Epoch: 8395, Batch Gradient Norm after: 7.06000764187732
Epoch 8396/10000, Prediction Accuracy = 63.30999999999999%, Loss = 0.3622073233127594
Epoch: 8396, Batch Gradient Norm: 8.36810056584155
Epoch: 8396, Batch Gradient Norm after: 8.36810056584155
Epoch 8397/10000, Prediction Accuracy = 63.15%, Loss = 0.3691870331764221
Epoch: 8397, Batch Gradient Norm: 10.210494891930516
Epoch: 8397, Batch Gradient Norm after: 10.210494891930516
Epoch 8398/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.3784224450588226
Epoch: 8398, Batch Gradient Norm: 10.866533631905403
Epoch: 8398, Batch Gradient Norm after: 10.866533631905403
Epoch 8399/10000, Prediction Accuracy = 63.18799999999999%, Loss = 0.38431221842765806
Epoch: 8399, Batch Gradient Norm: 8.822833553261328
Epoch: 8399, Batch Gradient Norm after: 8.822833553261328
Epoch 8400/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.3690408289432526
Epoch: 8400, Batch Gradient Norm: 12.121642115354604
Epoch: 8400, Batch Gradient Norm after: 12.121642115354604
Epoch 8401/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3940459847450256
Epoch: 8401, Batch Gradient Norm: 8.005538910550726
Epoch: 8401, Batch Gradient Norm after: 8.005538910550726
Epoch 8402/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.36386649012565614
Epoch: 8402, Batch Gradient Norm: 8.486622587917692
Epoch: 8402, Batch Gradient Norm after: 8.486622587917692
Epoch 8403/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.36851674914360044
Epoch: 8403, Batch Gradient Norm: 9.810847166327132
Epoch: 8403, Batch Gradient Norm after: 9.810847166327132
Epoch 8404/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3775431513786316
Epoch: 8404, Batch Gradient Norm: 10.298957786923413
Epoch: 8404, Batch Gradient Norm after: 10.298957786923413
Epoch 8405/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.38104320764541627
Epoch: 8405, Batch Gradient Norm: 9.421092245785342
Epoch: 8405, Batch Gradient Norm after: 9.421092245785342
Epoch 8406/10000, Prediction Accuracy = 63.194%, Loss = 0.3757722795009613
Epoch: 8406, Batch Gradient Norm: 7.851182635564628
Epoch: 8406, Batch Gradient Norm after: 7.851182635564628
Epoch 8407/10000, Prediction Accuracy = 63.324%, Loss = 0.36670886874198916
Epoch: 8407, Batch Gradient Norm: 10.075078333779352
Epoch: 8407, Batch Gradient Norm after: 10.075078333779352
Epoch 8408/10000, Prediction Accuracy = 63.194%, Loss = 0.37624377608299253
Epoch: 8408, Batch Gradient Norm: 13.058035512811484
Epoch: 8408, Batch Gradient Norm after: 13.058035512811484
Epoch 8409/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.40084247589111327
Epoch: 8409, Batch Gradient Norm: 12.518007374880952
Epoch: 8409, Batch Gradient Norm after: 12.518007374880952
Epoch 8410/10000, Prediction Accuracy = 63.226%, Loss = 0.39526037573814393
Epoch: 8410, Batch Gradient Norm: 10.035713124568922
Epoch: 8410, Batch Gradient Norm after: 10.035713124568922
Epoch 8411/10000, Prediction Accuracy = 63.23%, Loss = 0.37572675943374634
Epoch: 8411, Batch Gradient Norm: 9.998625158222325
Epoch: 8411, Batch Gradient Norm after: 9.998625158222325
Epoch 8412/10000, Prediction Accuracy = 63.18399999999999%, Loss = 0.3740329027175903
Epoch: 8412, Batch Gradient Norm: 11.18900195999772
Epoch: 8412, Batch Gradient Norm after: 11.18900195999772
Epoch 8413/10000, Prediction Accuracy = 63.15%, Loss = 0.3887892603874207
Epoch: 8413, Batch Gradient Norm: 8.570785898830078
Epoch: 8413, Batch Gradient Norm after: 8.570785898830078
Epoch 8414/10000, Prediction Accuracy = 63.17%, Loss = 0.3667719066143036
Epoch: 8414, Batch Gradient Norm: 9.12360317033836
Epoch: 8414, Batch Gradient Norm after: 9.12360317033836
Epoch 8415/10000, Prediction Accuracy = 63.23%, Loss = 0.37322962284088135
Epoch: 8415, Batch Gradient Norm: 8.735628688144045
Epoch: 8415, Batch Gradient Norm after: 8.735628688144045
Epoch 8416/10000, Prediction Accuracy = 63.158%, Loss = 0.3673325479030609
Epoch: 8416, Batch Gradient Norm: 8.414385191011876
Epoch: 8416, Batch Gradient Norm after: 8.414385191011876
Epoch 8417/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.36686124801635744
Epoch: 8417, Batch Gradient Norm: 9.765984401660267
Epoch: 8417, Batch Gradient Norm after: 9.765984401660267
Epoch 8418/10000, Prediction Accuracy = 63.21%, Loss = 0.37976604104042055
Epoch: 8418, Batch Gradient Norm: 8.020472449740971
Epoch: 8418, Batch Gradient Norm after: 8.020472449740971
Epoch 8419/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3652507781982422
Epoch: 8419, Batch Gradient Norm: 9.357318475749208
Epoch: 8419, Batch Gradient Norm after: 9.357318475749208
Epoch 8420/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3724497616291046
Epoch: 8420, Batch Gradient Norm: 10.248552350131618
Epoch: 8420, Batch Gradient Norm after: 10.248552350131618
Epoch 8421/10000, Prediction Accuracy = 63.31%, Loss = 0.3747081756591797
Epoch: 8421, Batch Gradient Norm: 9.25195553704984
Epoch: 8421, Batch Gradient Norm after: 9.25195553704984
Epoch 8422/10000, Prediction Accuracy = 63.306%, Loss = 0.37159318923950196
Epoch: 8422, Batch Gradient Norm: 7.070109469231833
Epoch: 8422, Batch Gradient Norm after: 7.070109469231833
Epoch 8423/10000, Prediction Accuracy = 63.266000000000005%, Loss = 0.3602116107940674
Epoch: 8423, Batch Gradient Norm: 9.982779179583405
Epoch: 8423, Batch Gradient Norm after: 9.982779179583405
Epoch 8424/10000, Prediction Accuracy = 63.064%, Loss = 0.3780380189418793
Epoch: 8424, Batch Gradient Norm: 12.25653183424823
Epoch: 8424, Batch Gradient Norm after: 12.25653183424823
Epoch 8425/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.39483829140663146
Epoch: 8425, Batch Gradient Norm: 11.431202175241241
Epoch: 8425, Batch Gradient Norm after: 11.431202175241241
Epoch 8426/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3869573771953583
Epoch: 8426, Batch Gradient Norm: 10.78252678986171
Epoch: 8426, Batch Gradient Norm after: 10.78252678986171
Epoch 8427/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.38275944590568545
Epoch: 8427, Batch Gradient Norm: 10.923716792282514
Epoch: 8427, Batch Gradient Norm after: 10.923716792282514
Epoch 8428/10000, Prediction Accuracy = 63.194%, Loss = 0.38223589658737184
Epoch: 8428, Batch Gradient Norm: 11.398807658892396
Epoch: 8428, Batch Gradient Norm after: 11.398807658892396
Epoch 8429/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3895839035511017
Epoch: 8429, Batch Gradient Norm: 9.02843842823519
Epoch: 8429, Batch Gradient Norm after: 9.02843842823519
Epoch 8430/10000, Prediction Accuracy = 63.152%, Loss = 0.3715567350387573
Epoch: 8430, Batch Gradient Norm: 10.077816828195116
Epoch: 8430, Batch Gradient Norm after: 10.077816828195116
Epoch 8431/10000, Prediction Accuracy = 63.212%, Loss = 0.3804879069328308
Epoch: 8431, Batch Gradient Norm: 9.517314347226675
Epoch: 8431, Batch Gradient Norm after: 9.517314347226675
Epoch 8432/10000, Prediction Accuracy = 63.136%, Loss = 0.37415874004364014
Epoch: 8432, Batch Gradient Norm: 7.374083528595034
Epoch: 8432, Batch Gradient Norm after: 7.374083528595034
Epoch 8433/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.36201626658439634
Epoch: 8433, Batch Gradient Norm: 9.68870144922525
Epoch: 8433, Batch Gradient Norm after: 9.68870144922525
Epoch 8434/10000, Prediction Accuracy = 63.348%, Loss = 0.37288287878036497
Epoch: 8434, Batch Gradient Norm: 10.869863566260683
Epoch: 8434, Batch Gradient Norm after: 10.869863566260683
Epoch 8435/10000, Prediction Accuracy = 63.274%, Loss = 0.3845726132392883
Epoch: 8435, Batch Gradient Norm: 10.130212244072963
Epoch: 8435, Batch Gradient Norm after: 10.130212244072963
Epoch 8436/10000, Prediction Accuracy = 63.322%, Loss = 0.3779125511646271
Epoch: 8436, Batch Gradient Norm: 10.164219259304215
Epoch: 8436, Batch Gradient Norm after: 10.164219259304215
Epoch 8437/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.37756300568580625
Epoch: 8437, Batch Gradient Norm: 10.327568476371859
Epoch: 8437, Batch Gradient Norm after: 10.327568476371859
Epoch 8438/10000, Prediction Accuracy = 63.21%, Loss = 0.38017829656600954
Epoch: 8438, Batch Gradient Norm: 10.006454819199794
Epoch: 8438, Batch Gradient Norm after: 10.006454819199794
Epoch 8439/10000, Prediction Accuracy = 63.124%, Loss = 0.3788196384906769
Epoch: 8439, Batch Gradient Norm: 8.272523702017802
Epoch: 8439, Batch Gradient Norm after: 8.272523702017802
Epoch 8440/10000, Prediction Accuracy = 63.254%, Loss = 0.3702540099620819
Epoch: 8440, Batch Gradient Norm: 8.052755822290406
Epoch: 8440, Batch Gradient Norm after: 8.052755822290406
Epoch 8441/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3677564084529877
Epoch: 8441, Batch Gradient Norm: 11.093785501696946
Epoch: 8441, Batch Gradient Norm after: 11.093785501696946
Epoch 8442/10000, Prediction Accuracy = 63.072%, Loss = 0.38641937971115115
Epoch: 8442, Batch Gradient Norm: 10.970961386950021
Epoch: 8442, Batch Gradient Norm after: 10.970961386950021
Epoch 8443/10000, Prediction Accuracy = 63.238%, Loss = 0.38835760951042175
Epoch: 8443, Batch Gradient Norm: 10.579301132587542
Epoch: 8443, Batch Gradient Norm after: 10.579301132587542
Epoch 8444/10000, Prediction Accuracy = 63.218%, Loss = 0.37880922555923463
Epoch: 8444, Batch Gradient Norm: 11.737244157340037
Epoch: 8444, Batch Gradient Norm after: 11.737244157340037
Epoch 8445/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3888025403022766
Epoch: 8445, Batch Gradient Norm: 8.547703170864587
Epoch: 8445, Batch Gradient Norm after: 8.547703170864587
Epoch 8446/10000, Prediction Accuracy = 63.222%, Loss = 0.3692229688167572
Epoch: 8446, Batch Gradient Norm: 7.701913429543524
Epoch: 8446, Batch Gradient Norm after: 7.701913429543524
Epoch 8447/10000, Prediction Accuracy = 63.302%, Loss = 0.3665745496749878
Epoch: 8447, Batch Gradient Norm: 7.417976303844643
Epoch: 8447, Batch Gradient Norm after: 7.417976303844643
Epoch 8448/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.35890421867370603
Epoch: 8448, Batch Gradient Norm: 9.215407488888161
Epoch: 8448, Batch Gradient Norm after: 9.215407488888161
Epoch 8449/10000, Prediction Accuracy = 63.174%, Loss = 0.37058480381965636
Epoch: 8449, Batch Gradient Norm: 10.038105417806955
Epoch: 8449, Batch Gradient Norm after: 10.038105417806955
Epoch 8450/10000, Prediction Accuracy = 63.202%, Loss = 0.37699835300445556
Epoch: 8450, Batch Gradient Norm: 12.0306179961377
Epoch: 8450, Batch Gradient Norm after: 12.0306179961377
Epoch 8451/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.39285518527030944
Epoch: 8451, Batch Gradient Norm: 9.32748898366467
Epoch: 8451, Batch Gradient Norm after: 9.32748898366467
Epoch 8452/10000, Prediction Accuracy = 63.238%, Loss = 0.3696141839027405
Epoch: 8452, Batch Gradient Norm: 11.047938741784751
Epoch: 8452, Batch Gradient Norm after: 11.047938741784751
Epoch 8453/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3816677451133728
Epoch: 8453, Batch Gradient Norm: 10.394699273362031
Epoch: 8453, Batch Gradient Norm after: 10.394699273362031
Epoch 8454/10000, Prediction Accuracy = 63.044%, Loss = 0.37887619733810424
Epoch: 8454, Batch Gradient Norm: 10.151198072870855
Epoch: 8454, Batch Gradient Norm after: 10.151198072870855
Epoch 8455/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3799583733081818
Epoch: 8455, Batch Gradient Norm: 8.370046557129676
Epoch: 8455, Batch Gradient Norm after: 8.370046557129676
Epoch 8456/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.3694568932056427
Epoch: 8456, Batch Gradient Norm: 9.373052285579975
Epoch: 8456, Batch Gradient Norm after: 9.373052285579975
Epoch 8457/10000, Prediction Accuracy = 63.334%, Loss = 0.3749088108539581
Epoch: 8457, Batch Gradient Norm: 9.72710378305463
Epoch: 8457, Batch Gradient Norm after: 9.72710378305463
Epoch 8458/10000, Prediction Accuracy = 63.06%, Loss = 0.3724990785121918
Epoch: 8458, Batch Gradient Norm: 10.701547766546238
Epoch: 8458, Batch Gradient Norm after: 10.701547766546238
Epoch 8459/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.3817522704601288
Epoch: 8459, Batch Gradient Norm: 10.063048219728982
Epoch: 8459, Batch Gradient Norm after: 10.063048219728982
Epoch 8460/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.373404449224472
Epoch: 8460, Batch Gradient Norm: 9.952277313180163
Epoch: 8460, Batch Gradient Norm after: 9.952277313180163
Epoch 8461/10000, Prediction Accuracy = 63.194%, Loss = 0.3749411880970001
Epoch: 8461, Batch Gradient Norm: 10.249905431424283
Epoch: 8461, Batch Gradient Norm after: 10.249905431424283
Epoch 8462/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.3779516875743866
Epoch: 8462, Batch Gradient Norm: 9.720575319742204
Epoch: 8462, Batch Gradient Norm after: 9.720575319742204
Epoch 8463/10000, Prediction Accuracy = 62.988000000000014%, Loss = 0.3730881929397583
Epoch: 8463, Batch Gradient Norm: 9.62091285310373
Epoch: 8463, Batch Gradient Norm after: 9.62091285310373
Epoch 8464/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3758035123348236
Epoch: 8464, Batch Gradient Norm: 10.67694540868341
Epoch: 8464, Batch Gradient Norm after: 10.67694540868341
Epoch 8465/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.3867757558822632
Epoch: 8465, Batch Gradient Norm: 7.391713626970658
Epoch: 8465, Batch Gradient Norm after: 7.391713626970658
Epoch 8466/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.3585629403591156
Epoch: 8466, Batch Gradient Norm: 9.525972723961454
Epoch: 8466, Batch Gradient Norm after: 9.525972723961454
Epoch 8467/10000, Prediction Accuracy = 63.176%, Loss = 0.3714526891708374
Epoch: 8467, Batch Gradient Norm: 13.482178981102784
Epoch: 8467, Batch Gradient Norm after: 13.482178981102784
Epoch 8468/10000, Prediction Accuracy = 63.352%, Loss = 0.4046511650085449
Epoch: 8468, Batch Gradient Norm: 10.145327715282576
Epoch: 8468, Batch Gradient Norm after: 10.145327715282576
Epoch 8469/10000, Prediction Accuracy = 63.048%, Loss = 0.37731393575668337
Epoch: 8469, Batch Gradient Norm: 7.308951365445228
Epoch: 8469, Batch Gradient Norm after: 7.308951365445228
Epoch 8470/10000, Prediction Accuracy = 63.278000000000006%, Loss = 0.35993110537528994
Epoch: 8470, Batch Gradient Norm: 10.52506844830094
Epoch: 8470, Batch Gradient Norm after: 10.52506844830094
Epoch 8471/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.3805883049964905
Epoch: 8471, Batch Gradient Norm: 9.782554328219085
Epoch: 8471, Batch Gradient Norm after: 9.782554328219085
Epoch 8472/10000, Prediction Accuracy = 63.286%, Loss = 0.3758386135101318
Epoch: 8472, Batch Gradient Norm: 9.96791254426782
Epoch: 8472, Batch Gradient Norm after: 9.96791254426782
Epoch 8473/10000, Prediction Accuracy = 63.314%, Loss = 0.37414037585258486
Epoch: 8473, Batch Gradient Norm: 12.35479085761821
Epoch: 8473, Batch Gradient Norm after: 12.35479085761821
Epoch 8474/10000, Prediction Accuracy = 63.14%, Loss = 0.3934818744659424
Epoch: 8474, Batch Gradient Norm: 7.829401653611235
Epoch: 8474, Batch Gradient Norm after: 7.829401653611235
Epoch 8475/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.360057932138443
Epoch: 8475, Batch Gradient Norm: 8.060728250756073
Epoch: 8475, Batch Gradient Norm after: 8.060728250756073
Epoch 8476/10000, Prediction Accuracy = 62.996%, Loss = 0.36493430733680726
Epoch: 8476, Batch Gradient Norm: 8.8067497865618
Epoch: 8476, Batch Gradient Norm after: 8.8067497865618
Epoch 8477/10000, Prediction Accuracy = 63.25%, Loss = 0.37064825296401976
Epoch: 8477, Batch Gradient Norm: 8.369685756063475
Epoch: 8477, Batch Gradient Norm after: 8.369685756063475
Epoch 8478/10000, Prediction Accuracy = 63.298%, Loss = 0.36456788778305055
Epoch: 8478, Batch Gradient Norm: 10.862879141120633
Epoch: 8478, Batch Gradient Norm after: 10.862879141120633
Epoch 8479/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3801579296588898
Epoch: 8479, Batch Gradient Norm: 10.125490264697232
Epoch: 8479, Batch Gradient Norm after: 10.125490264697232
Epoch 8480/10000, Prediction Accuracy = 63.262%, Loss = 0.37595039010047915
Epoch: 8480, Batch Gradient Norm: 9.842875959190271
Epoch: 8480, Batch Gradient Norm after: 9.842875959190271
Epoch 8481/10000, Prediction Accuracy = 63.182%, Loss = 0.3783793091773987
Epoch: 8481, Batch Gradient Norm: 9.799548793587139
Epoch: 8481, Batch Gradient Norm after: 9.799548793587139
Epoch 8482/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3741238653659821
Epoch: 8482, Batch Gradient Norm: 9.333224161794243
Epoch: 8482, Batch Gradient Norm after: 9.333224161794243
Epoch 8483/10000, Prediction Accuracy = 63.238%, Loss = 0.3693351626396179
Epoch: 8483, Batch Gradient Norm: 9.713473614327958
Epoch: 8483, Batch Gradient Norm after: 9.713473614327958
Epoch 8484/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.37604177594184873
Epoch: 8484, Batch Gradient Norm: 10.910382091162585
Epoch: 8484, Batch Gradient Norm after: 10.910382091162585
Epoch 8485/10000, Prediction Accuracy = 63.27%, Loss = 0.3863184034824371
Epoch: 8485, Batch Gradient Norm: 10.932062942986663
Epoch: 8485, Batch Gradient Norm after: 10.932062942986663
Epoch 8486/10000, Prediction Accuracy = 63.288%, Loss = 0.3836901247501373
Epoch: 8486, Batch Gradient Norm: 8.655513574975606
Epoch: 8486, Batch Gradient Norm after: 8.655513574975606
Epoch 8487/10000, Prediction Accuracy = 63.266%, Loss = 0.36644868850708007
Epoch: 8487, Batch Gradient Norm: 10.267346918804073
Epoch: 8487, Batch Gradient Norm after: 10.267346918804073
Epoch 8488/10000, Prediction Accuracy = 63.226%, Loss = 0.37540653347969055
Epoch: 8488, Batch Gradient Norm: 8.659824306465998
Epoch: 8488, Batch Gradient Norm after: 8.659824306465998
Epoch 8489/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.3654863595962524
Epoch: 8489, Batch Gradient Norm: 11.460828256776011
Epoch: 8489, Batch Gradient Norm after: 11.460828256776011
Epoch 8490/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3931397259235382
Epoch: 8490, Batch Gradient Norm: 7.541090574152406
Epoch: 8490, Batch Gradient Norm after: 7.541090574152406
Epoch 8491/10000, Prediction Accuracy = 63.354%, Loss = 0.3605166256427765
Epoch: 8491, Batch Gradient Norm: 8.483056777820908
Epoch: 8491, Batch Gradient Norm after: 8.483056777820908
Epoch 8492/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3669064164161682
Epoch: 8492, Batch Gradient Norm: 11.097001178170423
Epoch: 8492, Batch Gradient Norm after: 11.097001178170423
Epoch 8493/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.38309815526008606
Epoch: 8493, Batch Gradient Norm: 9.303191756383894
Epoch: 8493, Batch Gradient Norm after: 9.303191756383894
Epoch 8494/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.3704065322875977
Epoch: 8494, Batch Gradient Norm: 10.721999698981014
Epoch: 8494, Batch Gradient Norm after: 10.721999698981014
Epoch 8495/10000, Prediction Accuracy = 63.246%, Loss = 0.38056219816207887
Epoch: 8495, Batch Gradient Norm: 9.997141975732655
Epoch: 8495, Batch Gradient Norm after: 9.997141975732655
Epoch 8496/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.37713453769683836
Epoch: 8496, Batch Gradient Norm: 9.769530368151123
Epoch: 8496, Batch Gradient Norm after: 9.769530368151123
Epoch 8497/10000, Prediction Accuracy = 63.112%, Loss = 0.37590755224227906
Epoch: 8497, Batch Gradient Norm: 10.760321793501344
Epoch: 8497, Batch Gradient Norm after: 10.760321793501344
Epoch 8498/10000, Prediction Accuracy = 63.10999999999999%, Loss = 0.3800933361053467
Epoch: 8498, Batch Gradient Norm: 8.999732845853487
Epoch: 8498, Batch Gradient Norm after: 8.999732845853487
Epoch 8499/10000, Prediction Accuracy = 63.198%, Loss = 0.36781143546104433
Epoch: 8499, Batch Gradient Norm: 9.97537294936863
Epoch: 8499, Batch Gradient Norm after: 9.97537294936863
Epoch 8500/10000, Prediction Accuracy = 63.214%, Loss = 0.3745435893535614
Epoch: 8500, Batch Gradient Norm: 11.306799050555204
Epoch: 8500, Batch Gradient Norm after: 11.306799050555204
Epoch 8501/10000, Prediction Accuracy = 63.166%, Loss = 0.38615880608558656
Epoch: 8501, Batch Gradient Norm: 9.139474115787293
Epoch: 8501, Batch Gradient Norm after: 9.139474115787293
Epoch 8502/10000, Prediction Accuracy = 63.202%, Loss = 0.3698116779327393
Epoch: 8502, Batch Gradient Norm: 10.192602123612723
Epoch: 8502, Batch Gradient Norm after: 10.192602123612723
Epoch 8503/10000, Prediction Accuracy = 63.278%, Loss = 0.37840238213539124
Epoch: 8503, Batch Gradient Norm: 10.198589093918482
Epoch: 8503, Batch Gradient Norm after: 10.198589093918482
Epoch 8504/10000, Prediction Accuracy = 63.15400000000001%, Loss = 0.37649723291397097
Epoch: 8504, Batch Gradient Norm: 9.994100236772113
Epoch: 8504, Batch Gradient Norm after: 9.994100236772113
Epoch 8505/10000, Prediction Accuracy = 63.342%, Loss = 0.3735955893993378
Epoch: 8505, Batch Gradient Norm: 10.021544534264757
Epoch: 8505, Batch Gradient Norm after: 10.021544534264757
Epoch 8506/10000, Prediction Accuracy = 63.322%, Loss = 0.376395046710968
Epoch: 8506, Batch Gradient Norm: 13.579697767277468
Epoch: 8506, Batch Gradient Norm after: 13.579697767277468
Epoch 8507/10000, Prediction Accuracy = 63.124%, Loss = 0.40990167260169985
Epoch: 8507, Batch Gradient Norm: 10.274711071265495
Epoch: 8507, Batch Gradient Norm after: 10.274711071265495
Epoch 8508/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3791151106357574
Epoch: 8508, Batch Gradient Norm: 7.431039439365653
Epoch: 8508, Batch Gradient Norm after: 7.431039439365653
Epoch 8509/10000, Prediction Accuracy = 63.348%, Loss = 0.35988926887512207
Epoch: 8509, Batch Gradient Norm: 7.432499731450063
Epoch: 8509, Batch Gradient Norm after: 7.432499731450063
Epoch 8510/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3576624572277069
Epoch: 8510, Batch Gradient Norm: 11.418512369563148
Epoch: 8510, Batch Gradient Norm after: 11.418512369563148
Epoch 8511/10000, Prediction Accuracy = 63.214%, Loss = 0.38643576502799987
Epoch: 8511, Batch Gradient Norm: 9.85446438076505
Epoch: 8511, Batch Gradient Norm after: 9.85446438076505
Epoch 8512/10000, Prediction Accuracy = 63.084%, Loss = 0.3753441572189331
Epoch: 8512, Batch Gradient Norm: 9.853765685474608
Epoch: 8512, Batch Gradient Norm after: 9.853765685474608
Epoch 8513/10000, Prediction Accuracy = 63.126%, Loss = 0.3738000214099884
Epoch: 8513, Batch Gradient Norm: 8.265008270658292
Epoch: 8513, Batch Gradient Norm after: 8.265008270658292
Epoch 8514/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3622405230998993
Epoch: 8514, Batch Gradient Norm: 9.319112058728138
Epoch: 8514, Batch Gradient Norm after: 9.319112058728138
Epoch 8515/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3707765698432922
Epoch: 8515, Batch Gradient Norm: 10.702684774505357
Epoch: 8515, Batch Gradient Norm after: 10.702684774505357
Epoch 8516/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3820765554904938
Epoch: 8516, Batch Gradient Norm: 10.361990615448288
Epoch: 8516, Batch Gradient Norm after: 10.361990615448288
Epoch 8517/10000, Prediction Accuracy = 63.188%, Loss = 0.3787962973117828
Epoch: 8517, Batch Gradient Norm: 9.635327104567914
Epoch: 8517, Batch Gradient Norm after: 9.635327104567914
Epoch 8518/10000, Prediction Accuracy = 63.202%, Loss = 0.37340366244316103
Epoch: 8518, Batch Gradient Norm: 8.736310319240113
Epoch: 8518, Batch Gradient Norm after: 8.736310319240113
Epoch 8519/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.36738321185112
Epoch: 8519, Batch Gradient Norm: 9.894149600846793
Epoch: 8519, Batch Gradient Norm after: 9.894149600846793
Epoch 8520/10000, Prediction Accuracy = 63.330000000000005%, Loss = 0.3744430899620056
Epoch: 8520, Batch Gradient Norm: 10.235942487471547
Epoch: 8520, Batch Gradient Norm after: 10.235942487471547
Epoch 8521/10000, Prediction Accuracy = 63.056%, Loss = 0.3786317706108093
Epoch: 8521, Batch Gradient Norm: 10.447585231024721
Epoch: 8521, Batch Gradient Norm after: 10.447585231024721
Epoch 8522/10000, Prediction Accuracy = 63.384%, Loss = 0.37812135815620423
Epoch: 8522, Batch Gradient Norm: 10.005428138051043
Epoch: 8522, Batch Gradient Norm after: 10.005428138051043
Epoch 8523/10000, Prediction Accuracy = 63.246%, Loss = 0.3754657328128815
Epoch: 8523, Batch Gradient Norm: 9.38105668499763
Epoch: 8523, Batch Gradient Norm after: 9.38105668499763
Epoch 8524/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.3725542962551117
Epoch: 8524, Batch Gradient Norm: 9.807401396247977
Epoch: 8524, Batch Gradient Norm after: 9.807401396247977
Epoch 8525/10000, Prediction Accuracy = 63.178%, Loss = 0.3781369388103485
Epoch: 8525, Batch Gradient Norm: 9.212956964078497
Epoch: 8525, Batch Gradient Norm after: 9.212956964078497
Epoch 8526/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.36897349953651426
Epoch: 8526, Batch Gradient Norm: 12.427930454981556
Epoch: 8526, Batch Gradient Norm after: 12.427930454981556
Epoch 8527/10000, Prediction Accuracy = 63.14%, Loss = 0.3930161356925964
Epoch: 8527, Batch Gradient Norm: 10.873140558885082
Epoch: 8527, Batch Gradient Norm after: 10.873140558885082
Epoch 8528/10000, Prediction Accuracy = 63.25%, Loss = 0.38182474970817565
Epoch: 8528, Batch Gradient Norm: 10.760469063645296
Epoch: 8528, Batch Gradient Norm after: 10.760469063645296
Epoch 8529/10000, Prediction Accuracy = 63.31999999999999%, Loss = 0.38109837770462035
Epoch: 8529, Batch Gradient Norm: 7.52676328613433
Epoch: 8529, Batch Gradient Norm after: 7.52676328613433
Epoch 8530/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.3592513859272003
Epoch: 8530, Batch Gradient Norm: 10.261130084710885
Epoch: 8530, Batch Gradient Norm after: 10.261130084710885
Epoch 8531/10000, Prediction Accuracy = 63.298%, Loss = 0.3773479461669922
Epoch: 8531, Batch Gradient Norm: 9.890179948823382
Epoch: 8531, Batch Gradient Norm after: 9.890179948823382
Epoch 8532/10000, Prediction Accuracy = 63.3%, Loss = 0.37485505938529967
Epoch: 8532, Batch Gradient Norm: 10.907618852834831
Epoch: 8532, Batch Gradient Norm after: 10.907618852834831
Epoch 8533/10000, Prediction Accuracy = 63.282%, Loss = 0.3793723702430725
Epoch: 8533, Batch Gradient Norm: 8.858276314005455
Epoch: 8533, Batch Gradient Norm after: 8.858276314005455
Epoch 8534/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3687523424625397
Epoch: 8534, Batch Gradient Norm: 6.836138788494758
Epoch: 8534, Batch Gradient Norm after: 6.836138788494758
Epoch 8535/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3566691756248474
Epoch: 8535, Batch Gradient Norm: 7.9836165992575
Epoch: 8535, Batch Gradient Norm after: 7.9836165992575
Epoch 8536/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.362074077129364
Epoch: 8536, Batch Gradient Norm: 9.849930618675678
Epoch: 8536, Batch Gradient Norm after: 9.849930618675678
Epoch 8537/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.37566688656806946
Epoch: 8537, Batch Gradient Norm: 10.946237938996523
Epoch: 8537, Batch Gradient Norm after: 10.946237938996523
Epoch 8538/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.38257399797439573
Epoch: 8538, Batch Gradient Norm: 10.382634334575643
Epoch: 8538, Batch Gradient Norm after: 10.382634334575643
Epoch 8539/10000, Prediction Accuracy = 63.314%, Loss = 0.3805472910404205
Epoch: 8539, Batch Gradient Norm: 9.870195958423933
Epoch: 8539, Batch Gradient Norm after: 9.870195958423933
Epoch 8540/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.3727621376514435
Epoch: 8540, Batch Gradient Norm: 6.705474406638488
Epoch: 8540, Batch Gradient Norm after: 6.705474406638488
Epoch 8541/10000, Prediction Accuracy = 63.402%, Loss = 0.35509238839149476
Epoch: 8541, Batch Gradient Norm: 10.772981006118615
Epoch: 8541, Batch Gradient Norm after: 10.772981006118615
Epoch 8542/10000, Prediction Accuracy = 63.20200000000001%, Loss = 0.381003338098526
Epoch: 8542, Batch Gradient Norm: 10.202805696862184
Epoch: 8542, Batch Gradient Norm after: 10.202805696862184
Epoch 8543/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.3786884605884552
Epoch: 8543, Batch Gradient Norm: 8.414760177314564
Epoch: 8543, Batch Gradient Norm after: 8.414760177314564
Epoch 8544/10000, Prediction Accuracy = 63.198%, Loss = 0.36399800777435304
Epoch: 8544, Batch Gradient Norm: 10.786925361967798
Epoch: 8544, Batch Gradient Norm after: 10.786925361967798
Epoch 8545/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.3805634558200836
Epoch: 8545, Batch Gradient Norm: 11.032466874620464
Epoch: 8545, Batch Gradient Norm after: 11.032466874620464
Epoch 8546/10000, Prediction Accuracy = 63.324%, Loss = 0.38432093262672423
Epoch: 8546, Batch Gradient Norm: 9.347955774552862
Epoch: 8546, Batch Gradient Norm after: 9.347955774552862
Epoch 8547/10000, Prediction Accuracy = 63.306%, Loss = 0.3728153586387634
Epoch: 8547, Batch Gradient Norm: 7.87616836126852
Epoch: 8547, Batch Gradient Norm after: 7.87616836126852
Epoch 8548/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.36350632309913633
Epoch: 8548, Batch Gradient Norm: 8.8711830087792
Epoch: 8548, Batch Gradient Norm after: 8.8711830087792
Epoch 8549/10000, Prediction Accuracy = 63.146%, Loss = 0.3677520275115967
Epoch: 8549, Batch Gradient Norm: 10.995952489622416
Epoch: 8549, Batch Gradient Norm after: 10.995952489622416
Epoch 8550/10000, Prediction Accuracy = 63.06199999999999%, Loss = 0.38315216898918153
Epoch: 8550, Batch Gradient Norm: 10.188328864336265
Epoch: 8550, Batch Gradient Norm after: 10.188328864336265
Epoch 8551/10000, Prediction Accuracy = 63.186%, Loss = 0.37777047157287597
Epoch: 8551, Batch Gradient Norm: 6.935423776518252
Epoch: 8551, Batch Gradient Norm after: 6.935423776518252
Epoch 8552/10000, Prediction Accuracy = 63.238%, Loss = 0.3560587763786316
Epoch: 8552, Batch Gradient Norm: 12.893069522586117
Epoch: 8552, Batch Gradient Norm after: 12.893069522586117
Epoch 8553/10000, Prediction Accuracy = 63.318%, Loss = 0.3940508782863617
Epoch: 8553, Batch Gradient Norm: 13.700436532814873
Epoch: 8553, Batch Gradient Norm after: 13.700436532814873
Epoch 8554/10000, Prediction Accuracy = 63.15%, Loss = 0.4056291878223419
Epoch: 8554, Batch Gradient Norm: 8.165190255161923
Epoch: 8554, Batch Gradient Norm after: 8.165190255161923
Epoch 8555/10000, Prediction Accuracy = 63.28399999999999%, Loss = 0.3637570202350616
Epoch: 8555, Batch Gradient Norm: 8.30210149958312
Epoch: 8555, Batch Gradient Norm after: 8.30210149958312
Epoch 8556/10000, Prediction Accuracy = 63.324%, Loss = 0.36033218502998354
Epoch: 8556, Batch Gradient Norm: 9.895652521525443
Epoch: 8556, Batch Gradient Norm after: 9.895652521525443
Epoch 8557/10000, Prediction Accuracy = 63.202%, Loss = 0.3723880350589752
Epoch: 8557, Batch Gradient Norm: 8.418293126035458
Epoch: 8557, Batch Gradient Norm after: 8.418293126035458
Epoch 8558/10000, Prediction Accuracy = 63.262%, Loss = 0.3649768173694611
Epoch: 8558, Batch Gradient Norm: 9.707288372294885
Epoch: 8558, Batch Gradient Norm after: 9.707288372294885
Epoch 8559/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.37100839614868164
Epoch: 8559, Batch Gradient Norm: 9.876737672381621
Epoch: 8559, Batch Gradient Norm after: 9.876737672381621
Epoch 8560/10000, Prediction Accuracy = 63.198%, Loss = 0.3734163284301758
Epoch: 8560, Batch Gradient Norm: 11.847454846829791
Epoch: 8560, Batch Gradient Norm after: 11.847454846829791
Epoch 8561/10000, Prediction Accuracy = 63.226%, Loss = 0.3883323073387146
Epoch: 8561, Batch Gradient Norm: 10.33578965629572
Epoch: 8561, Batch Gradient Norm after: 10.33578965629572
Epoch 8562/10000, Prediction Accuracy = 63.162%, Loss = 0.37738693952560426
Epoch: 8562, Batch Gradient Norm: 10.497788407208512
Epoch: 8562, Batch Gradient Norm after: 10.497788407208512
Epoch 8563/10000, Prediction Accuracy = 63.322%, Loss = 0.3831274151802063
Epoch: 8563, Batch Gradient Norm: 9.887819582791456
Epoch: 8563, Batch Gradient Norm after: 9.887819582791456
Epoch 8564/10000, Prediction Accuracy = 63.302%, Loss = 0.374088442325592
Epoch: 8564, Batch Gradient Norm: 7.857362997534786
Epoch: 8564, Batch Gradient Norm after: 7.857362997534786
Epoch 8565/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.36111305356025697
Epoch: 8565, Batch Gradient Norm: 9.063028292418043
Epoch: 8565, Batch Gradient Norm after: 9.063028292418043
Epoch 8566/10000, Prediction Accuracy = 63.032%, Loss = 0.36978259682655334
Epoch: 8566, Batch Gradient Norm: 9.935904265406153
Epoch: 8566, Batch Gradient Norm after: 9.935904265406153
Epoch 8567/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.3759659886360168
Epoch: 8567, Batch Gradient Norm: 10.311410831270164
Epoch: 8567, Batch Gradient Norm after: 10.311410831270164
Epoch 8568/10000, Prediction Accuracy = 63.212%, Loss = 0.3815032124519348
Epoch: 8568, Batch Gradient Norm: 8.488609778834334
Epoch: 8568, Batch Gradient Norm after: 8.488609778834334
Epoch 8569/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.36420938968658445
Epoch: 8569, Batch Gradient Norm: 9.796540014922504
Epoch: 8569, Batch Gradient Norm after: 9.796540014922504
Epoch 8570/10000, Prediction Accuracy = 63.318%, Loss = 0.37365995049476625
Epoch: 8570, Batch Gradient Norm: 9.588169886640353
Epoch: 8570, Batch Gradient Norm after: 9.588169886640353
Epoch 8571/10000, Prediction Accuracy = 63.326%, Loss = 0.37269617319107057
Epoch: 8571, Batch Gradient Norm: 9.847286190674705
Epoch: 8571, Batch Gradient Norm after: 9.847286190674705
Epoch 8572/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.3737392723560333
Epoch: 8572, Batch Gradient Norm: 9.340374951028139
Epoch: 8572, Batch Gradient Norm after: 9.340374951028139
Epoch 8573/10000, Prediction Accuracy = 63.408%, Loss = 0.37122057676315307
Epoch: 8573, Batch Gradient Norm: 13.427930273945444
Epoch: 8573, Batch Gradient Norm after: 13.427930273945444
Epoch 8574/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.40320258736610415
Epoch: 8574, Batch Gradient Norm: 11.934576406932367
Epoch: 8574, Batch Gradient Norm after: 11.934576406932367
Epoch 8575/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.3932405412197113
Epoch: 8575, Batch Gradient Norm: 7.021047001909163
Epoch: 8575, Batch Gradient Norm after: 7.021047001909163
Epoch 8576/10000, Prediction Accuracy = 63.246%, Loss = 0.3560185432434082
Epoch: 8576, Batch Gradient Norm: 8.563265217943592
Epoch: 8576, Batch Gradient Norm after: 8.563265217943592
Epoch 8577/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3648205935955048
Epoch: 8577, Batch Gradient Norm: 9.701494109463818
Epoch: 8577, Batch Gradient Norm after: 9.701494109463818
Epoch 8578/10000, Prediction Accuracy = 63.217999999999996%, Loss = 0.3741348922252655
Epoch: 8578, Batch Gradient Norm: 9.648675959255558
Epoch: 8578, Batch Gradient Norm after: 9.648675959255558
Epoch 8579/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3715439260005951
Epoch: 8579, Batch Gradient Norm: 10.943183767479809
Epoch: 8579, Batch Gradient Norm after: 10.943183767479809
Epoch 8580/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3863924562931061
Epoch: 8580, Batch Gradient Norm: 8.635597192036856
Epoch: 8580, Batch Gradient Norm after: 8.635597192036856
Epoch 8581/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3676545023918152
Epoch: 8581, Batch Gradient Norm: 8.488790748607473
Epoch: 8581, Batch Gradient Norm after: 8.488790748607473
Epoch 8582/10000, Prediction Accuracy = 63.388%, Loss = 0.3639981210231781
Epoch: 8582, Batch Gradient Norm: 9.015074133531117
Epoch: 8582, Batch Gradient Norm after: 9.015074133531117
Epoch 8583/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.36851292848587036
Epoch: 8583, Batch Gradient Norm: 11.029998816902756
Epoch: 8583, Batch Gradient Norm after: 11.029998816902756
Epoch 8584/10000, Prediction Accuracy = 63.157999999999994%, Loss = 0.3886071503162384
Epoch: 8584, Batch Gradient Norm: 10.371535137091891
Epoch: 8584, Batch Gradient Norm after: 10.371535137091891
Epoch 8585/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3797314941883087
Epoch: 8585, Batch Gradient Norm: 12.390765113605235
Epoch: 8585, Batch Gradient Norm after: 12.390765113605235
Epoch 8586/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.39100505113601686
Epoch: 8586, Batch Gradient Norm: 6.753754186389412
Epoch: 8586, Batch Gradient Norm after: 6.753754186389412
Epoch 8587/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.35416245460510254
Epoch: 8587, Batch Gradient Norm: 8.34914086518601
Epoch: 8587, Batch Gradient Norm after: 8.34914086518601
Epoch 8588/10000, Prediction Accuracy = 63.342%, Loss = 0.3638919770717621
Epoch: 8588, Batch Gradient Norm: 10.162433202961694
Epoch: 8588, Batch Gradient Norm after: 10.162433202961694
Epoch 8589/10000, Prediction Accuracy = 63.222%, Loss = 0.37425283789634706
Epoch: 8589, Batch Gradient Norm: 9.864628481683784
Epoch: 8589, Batch Gradient Norm after: 9.864628481683784
Epoch 8590/10000, Prediction Accuracy = 63.322%, Loss = 0.37240858674049376
Epoch: 8590, Batch Gradient Norm: 9.267107049574468
Epoch: 8590, Batch Gradient Norm after: 9.267107049574468
Epoch 8591/10000, Prediction Accuracy = 63.388%, Loss = 0.3683782577514648
Epoch: 8591, Batch Gradient Norm: 10.563732072255714
Epoch: 8591, Batch Gradient Norm after: 10.563732072255714
Epoch 8592/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.37696237564086915
Epoch: 8592, Batch Gradient Norm: 12.55339863973313
Epoch: 8592, Batch Gradient Norm after: 12.55339863973313
Epoch 8593/10000, Prediction Accuracy = 63.45%, Loss = 0.3942571938037872
Epoch: 8593, Batch Gradient Norm: 11.768051679242992
Epoch: 8593, Batch Gradient Norm after: 11.768051679242992
Epoch 8594/10000, Prediction Accuracy = 63.398%, Loss = 0.38983181715011594
Epoch: 8594, Batch Gradient Norm: 9.28386413578877
Epoch: 8594, Batch Gradient Norm after: 9.28386413578877
Epoch 8595/10000, Prediction Accuracy = 63.338%, Loss = 0.36729036569595336
Epoch: 8595, Batch Gradient Norm: 9.2508045034567
Epoch: 8595, Batch Gradient Norm after: 9.2508045034567
Epoch 8596/10000, Prediction Accuracy = 63.33%, Loss = 0.3701690673828125
Epoch: 8596, Batch Gradient Norm: 8.722752294684595
Epoch: 8596, Batch Gradient Norm after: 8.722752294684595
Epoch 8597/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.36478466391563413
Epoch: 8597, Batch Gradient Norm: 8.805095369676355
Epoch: 8597, Batch Gradient Norm after: 8.805095369676355
Epoch 8598/10000, Prediction Accuracy = 63.274%, Loss = 0.3632201373577118
Epoch: 8598, Batch Gradient Norm: 9.16455911163539
Epoch: 8598, Batch Gradient Norm after: 9.16455911163539
Epoch 8599/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.36645385026931765
Epoch: 8599, Batch Gradient Norm: 9.52581551038682
Epoch: 8599, Batch Gradient Norm after: 9.52581551038682
Epoch 8600/10000, Prediction Accuracy = 63.396%, Loss = 0.3704149663448334
Epoch: 8600, Batch Gradient Norm: 9.31992873120791
Epoch: 8600, Batch Gradient Norm after: 9.31992873120791
Epoch 8601/10000, Prediction Accuracy = 63.298%, Loss = 0.37376219034194946
Epoch: 8601, Batch Gradient Norm: 10.390218435479197
Epoch: 8601, Batch Gradient Norm after: 10.390218435479197
Epoch 8602/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.37901856303215026
Epoch: 8602, Batch Gradient Norm: 9.16458903662811
Epoch: 8602, Batch Gradient Norm after: 9.16458903662811
Epoch 8603/10000, Prediction Accuracy = 63.25600000000001%, Loss = 0.37046807408332827
Epoch: 8603, Batch Gradient Norm: 9.779581543407053
Epoch: 8603, Batch Gradient Norm after: 9.779581543407053
Epoch 8604/10000, Prediction Accuracy = 63.4%, Loss = 0.37186573147773744
Epoch: 8604, Batch Gradient Norm: 12.451752048097081
Epoch: 8604, Batch Gradient Norm after: 12.451752048097081
Epoch 8605/10000, Prediction Accuracy = 63.230000000000004%, Loss = 0.39257424473762514
Epoch: 8605, Batch Gradient Norm: 7.784140252689959
Epoch: 8605, Batch Gradient Norm after: 7.784140252689959
Epoch 8606/10000, Prediction Accuracy = 63.48%, Loss = 0.36159822940826414
Epoch: 8606, Batch Gradient Norm: 9.286159419454494
Epoch: 8606, Batch Gradient Norm after: 9.286159419454494
Epoch 8607/10000, Prediction Accuracy = 63.274%, Loss = 0.3707063317298889
Epoch: 8607, Batch Gradient Norm: 8.702806016879217
Epoch: 8607, Batch Gradient Norm after: 8.702806016879217
Epoch 8608/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.365226548910141
Epoch: 8608, Batch Gradient Norm: 10.328458749999681
Epoch: 8608, Batch Gradient Norm after: 10.328458749999681
Epoch 8609/10000, Prediction Accuracy = 63.17%, Loss = 0.3741248369216919
Epoch: 8609, Batch Gradient Norm: 11.679580660728403
Epoch: 8609, Batch Gradient Norm after: 11.679580660728403
Epoch 8610/10000, Prediction Accuracy = 63.124%, Loss = 0.39122687578201293
Epoch: 8610, Batch Gradient Norm: 8.058464174051892
Epoch: 8610, Batch Gradient Norm after: 8.058464174051892
Epoch 8611/10000, Prediction Accuracy = 63.426%, Loss = 0.36308045983314513
Epoch: 8611, Batch Gradient Norm: 9.663297676419953
Epoch: 8611, Batch Gradient Norm after: 9.663297676419953
Epoch 8612/10000, Prediction Accuracy = 63.174%, Loss = 0.3757428824901581
Epoch: 8612, Batch Gradient Norm: 10.233187821512658
Epoch: 8612, Batch Gradient Norm after: 10.233187821512658
Epoch 8613/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.3756198525428772
Epoch: 8613, Batch Gradient Norm: 10.011585590879982
Epoch: 8613, Batch Gradient Norm after: 10.011585590879982
Epoch 8614/10000, Prediction Accuracy = 63.35%, Loss = 0.371382337808609
Epoch: 8614, Batch Gradient Norm: 11.674555585360432
Epoch: 8614, Batch Gradient Norm after: 11.674555585360432
Epoch 8615/10000, Prediction Accuracy = 63.376%, Loss = 0.384986025094986
Epoch: 8615, Batch Gradient Norm: 10.465913296806004
Epoch: 8615, Batch Gradient Norm after: 10.465913296806004
Epoch 8616/10000, Prediction Accuracy = 63.2%, Loss = 0.3783799707889557
Epoch: 8616, Batch Gradient Norm: 8.770857996724796
Epoch: 8616, Batch Gradient Norm after: 8.770857996724796
Epoch 8617/10000, Prediction Accuracy = 63.128%, Loss = 0.3668270707130432
Epoch: 8617, Batch Gradient Norm: 8.956064918466208
Epoch: 8617, Batch Gradient Norm after: 8.956064918466208
Epoch 8618/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.36791226267814636
Epoch: 8618, Batch Gradient Norm: 9.547196473062003
Epoch: 8618, Batch Gradient Norm after: 9.547196473062003
Epoch 8619/10000, Prediction Accuracy = 63.286%, Loss = 0.37228824496269225
Epoch: 8619, Batch Gradient Norm: 7.735975147488822
Epoch: 8619, Batch Gradient Norm after: 7.735975147488822
Epoch 8620/10000, Prediction Accuracy = 63.362%, Loss = 0.3614372193813324
Epoch: 8620, Batch Gradient Norm: 7.965337067010841
Epoch: 8620, Batch Gradient Norm after: 7.965337067010841
Epoch 8621/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3599878251552582
Epoch: 8621, Batch Gradient Norm: 14.451024597702968
Epoch: 8621, Batch Gradient Norm after: 14.451024597702968
Epoch 8622/10000, Prediction Accuracy = 63.312%, Loss = 0.4075342833995819
Epoch: 8622, Batch Gradient Norm: 10.852307840918606
Epoch: 8622, Batch Gradient Norm after: 10.852307840918606
Epoch 8623/10000, Prediction Accuracy = 63.334%, Loss = 0.3786092817783356
Epoch: 8623, Batch Gradient Norm: 9.834710504292252
Epoch: 8623, Batch Gradient Norm after: 9.834710504292252
Epoch 8624/10000, Prediction Accuracy = 63.448%, Loss = 0.3737749934196472
Epoch: 8624, Batch Gradient Norm: 8.868514795766918
Epoch: 8624, Batch Gradient Norm after: 8.868514795766918
Epoch 8625/10000, Prediction Accuracy = 63.452%, Loss = 0.36883909702301027
Epoch: 8625, Batch Gradient Norm: 9.291688634430079
Epoch: 8625, Batch Gradient Norm after: 9.291688634430079
Epoch 8626/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3668392539024353
Epoch: 8626, Batch Gradient Norm: 9.87525235288658
Epoch: 8626, Batch Gradient Norm after: 9.87525235288658
Epoch 8627/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.37491825222969055
Epoch: 8627, Batch Gradient Norm: 10.014406944516779
Epoch: 8627, Batch Gradient Norm after: 10.014406944516779
Epoch 8628/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.37772042751312257
Epoch: 8628, Batch Gradient Norm: 10.722793508875109
Epoch: 8628, Batch Gradient Norm after: 10.722793508875109
Epoch 8629/10000, Prediction Accuracy = 63.334%, Loss = 0.385031932592392
Epoch: 8629, Batch Gradient Norm: 7.467747955688458
Epoch: 8629, Batch Gradient Norm after: 7.467747955688458
Epoch 8630/10000, Prediction Accuracy = 63.306000000000004%, Loss = 0.35958197712898254
Epoch: 8630, Batch Gradient Norm: 14.43024680017291
Epoch: 8630, Batch Gradient Norm after: 14.43024680017291
Epoch 8631/10000, Prediction Accuracy = 63.17%, Loss = 0.4135387599468231
Epoch: 8631, Batch Gradient Norm: 9.958541892417356
Epoch: 8631, Batch Gradient Norm after: 9.958541892417356
Epoch 8632/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.3738856315612793
Epoch: 8632, Batch Gradient Norm: 7.03532432362737
Epoch: 8632, Batch Gradient Norm after: 7.03532432362737
Epoch 8633/10000, Prediction Accuracy = 63.524%, Loss = 0.3560525059700012
Epoch: 8633, Batch Gradient Norm: 6.487160396350746
Epoch: 8633, Batch Gradient Norm after: 6.487160396350746
Epoch 8634/10000, Prediction Accuracy = 63.27%, Loss = 0.3521795094013214
Epoch: 8634, Batch Gradient Norm: 7.555458304444217
Epoch: 8634, Batch Gradient Norm after: 7.555458304444217
Epoch 8635/10000, Prediction Accuracy = 63.146%, Loss = 0.3567764937877655
Epoch: 8635, Batch Gradient Norm: 10.21579964210717
Epoch: 8635, Batch Gradient Norm after: 10.21579964210717
Epoch 8636/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3727722644805908
Epoch: 8636, Batch Gradient Norm: 11.469692854375609
Epoch: 8636, Batch Gradient Norm after: 11.469692854375609
Epoch 8637/10000, Prediction Accuracy = 63.08%, Loss = 0.384947806596756
Epoch: 8637, Batch Gradient Norm: 8.446931076677775
Epoch: 8637, Batch Gradient Norm after: 8.446931076677775
Epoch 8638/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.3633692920207977
Epoch: 8638, Batch Gradient Norm: 9.00993370276007
Epoch: 8638, Batch Gradient Norm after: 9.00993370276007
Epoch 8639/10000, Prediction Accuracy = 63.17%, Loss = 0.3664280891418457
Epoch: 8639, Batch Gradient Norm: 11.592136710216769
Epoch: 8639, Batch Gradient Norm after: 11.592136710216769
Epoch 8640/10000, Prediction Accuracy = 63.35%, Loss = 0.3817628681659698
Epoch: 8640, Batch Gradient Norm: 11.017865171806989
Epoch: 8640, Batch Gradient Norm after: 11.017865171806989
Epoch 8641/10000, Prediction Accuracy = 63.354000000000006%, Loss = 0.3789842128753662
Epoch: 8641, Batch Gradient Norm: 8.901129818437242
Epoch: 8641, Batch Gradient Norm after: 8.901129818437242
Epoch 8642/10000, Prediction Accuracy = 63.336%, Loss = 0.3702735364437103
Epoch: 8642, Batch Gradient Norm: 7.251627344069286
Epoch: 8642, Batch Gradient Norm after: 7.251627344069286
Epoch 8643/10000, Prediction Accuracy = 63.354000000000006%, Loss = 0.35702823400497435
Epoch: 8643, Batch Gradient Norm: 10.65753803456807
Epoch: 8643, Batch Gradient Norm after: 10.65753803456807
Epoch 8644/10000, Prediction Accuracy = 63.354%, Loss = 0.3826051652431488
Epoch: 8644, Batch Gradient Norm: 12.284313552686886
Epoch: 8644, Batch Gradient Norm after: 12.284313552686886
Epoch 8645/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3888651907444
Epoch: 8645, Batch Gradient Norm: 10.841093389661534
Epoch: 8645, Batch Gradient Norm after: 10.841093389661534
Epoch 8646/10000, Prediction Accuracy = 63.176%, Loss = 0.3800398468971252
Epoch: 8646, Batch Gradient Norm: 9.56088513234215
Epoch: 8646, Batch Gradient Norm after: 9.56088513234215
Epoch 8647/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3694585502147675
Epoch: 8647, Batch Gradient Norm: 10.124717363636256
Epoch: 8647, Batch Gradient Norm after: 10.124717363636256
Epoch 8648/10000, Prediction Accuracy = 63.352%, Loss = 0.37538960576057434
Epoch: 8648, Batch Gradient Norm: 9.785128524085353
Epoch: 8648, Batch Gradient Norm after: 9.785128524085353
Epoch 8649/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.37352163195610044
Epoch: 8649, Batch Gradient Norm: 9.72696279482681
Epoch: 8649, Batch Gradient Norm after: 9.72696279482681
Epoch 8650/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.37272408604621887
Epoch: 8650, Batch Gradient Norm: 11.612297641485105
Epoch: 8650, Batch Gradient Norm after: 11.612297641485105
Epoch 8651/10000, Prediction Accuracy = 63.386%, Loss = 0.3834432899951935
Epoch: 8651, Batch Gradient Norm: 9.701133705722889
Epoch: 8651, Batch Gradient Norm after: 9.701133705722889
Epoch 8652/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3695278763771057
Epoch: 8652, Batch Gradient Norm: 11.824751723469985
Epoch: 8652, Batch Gradient Norm after: 11.824751723469985
Epoch 8653/10000, Prediction Accuracy = 62.85600000000001%, Loss = 0.39013669490814207
Epoch: 8653, Batch Gradient Norm: 8.47537189121609
Epoch: 8653, Batch Gradient Norm after: 8.47537189121609
Epoch 8654/10000, Prediction Accuracy = 63.46400000000001%, Loss = 0.36693733334541323
Epoch: 8654, Batch Gradient Norm: 8.902234358196614
Epoch: 8654, Batch Gradient Norm after: 8.902234358196614
Epoch 8655/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3656304359436035
Epoch: 8655, Batch Gradient Norm: 9.714338695632698
Epoch: 8655, Batch Gradient Norm after: 9.714338695632698
Epoch 8656/10000, Prediction Accuracy = 63.152%, Loss = 0.3733097851276398
Epoch: 8656, Batch Gradient Norm: 10.777836771760912
Epoch: 8656, Batch Gradient Norm after: 10.777836771760912
Epoch 8657/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.3798068404197693
Epoch: 8657, Batch Gradient Norm: 9.969332531920175
Epoch: 8657, Batch Gradient Norm after: 9.969332531920175
Epoch 8658/10000, Prediction Accuracy = 63.388%, Loss = 0.37418435215950013
Epoch: 8658, Batch Gradient Norm: 7.361524984110239
Epoch: 8658, Batch Gradient Norm after: 7.361524984110239
Epoch 8659/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.35760620832443235
Epoch: 8659, Batch Gradient Norm: 8.554672092177984
Epoch: 8659, Batch Gradient Norm after: 8.554672092177984
Epoch 8660/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.36463948488235476
Epoch: 8660, Batch Gradient Norm: 10.032987477759248
Epoch: 8660, Batch Gradient Norm after: 10.032987477759248
Epoch 8661/10000, Prediction Accuracy = 63.27199999999999%, Loss = 0.37262927889823916
Epoch: 8661, Batch Gradient Norm: 11.918761129103267
Epoch: 8661, Batch Gradient Norm after: 11.918761129103267
Epoch 8662/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3862611472606659
Epoch: 8662, Batch Gradient Norm: 11.893849097455432
Epoch: 8662, Batch Gradient Norm after: 11.893849097455432
Epoch 8663/10000, Prediction Accuracy = 63.33%, Loss = 0.385792076587677
Epoch: 8663, Batch Gradient Norm: 10.559681810035686
Epoch: 8663, Batch Gradient Norm after: 10.559681810035686
Epoch 8664/10000, Prediction Accuracy = 63.136%, Loss = 0.37561547160148623
Epoch: 8664, Batch Gradient Norm: 9.58832429320236
Epoch: 8664, Batch Gradient Norm after: 9.58832429320236
Epoch 8665/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3723865509033203
Epoch: 8665, Batch Gradient Norm: 7.586083945625384
Epoch: 8665, Batch Gradient Norm after: 7.586083945625384
Epoch 8666/10000, Prediction Accuracy = 63.282%, Loss = 0.35928648710250854
Epoch: 8666, Batch Gradient Norm: 8.01416684340501
Epoch: 8666, Batch Gradient Norm after: 8.01416684340501
Epoch 8667/10000, Prediction Accuracy = 63.456%, Loss = 0.360686331987381
Epoch: 8667, Batch Gradient Norm: 8.228350127349028
Epoch: 8667, Batch Gradient Norm after: 8.228350127349028
Epoch 8668/10000, Prediction Accuracy = 63.40999999999999%, Loss = 0.3612999200820923
Epoch: 8668, Batch Gradient Norm: 8.53328040282724
Epoch: 8668, Batch Gradient Norm after: 8.53328040282724
Epoch 8669/10000, Prediction Accuracy = 63.334%, Loss = 0.36184370517730713
Epoch: 8669, Batch Gradient Norm: 9.815841535389216
Epoch: 8669, Batch Gradient Norm after: 9.815841535389216
Epoch 8670/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3719653785228729
Epoch: 8670, Batch Gradient Norm: 10.59714586939881
Epoch: 8670, Batch Gradient Norm after: 10.59714586939881
Epoch 8671/10000, Prediction Accuracy = 63.3%, Loss = 0.378192675113678
Epoch: 8671, Batch Gradient Norm: 8.66209544605458
Epoch: 8671, Batch Gradient Norm after: 8.66209544605458
Epoch 8672/10000, Prediction Accuracy = 63.24000000000001%, Loss = 0.36254932880401614
Epoch: 8672, Batch Gradient Norm: 9.861581763079128
Epoch: 8672, Batch Gradient Norm after: 9.861581763079128
Epoch 8673/10000, Prediction Accuracy = 63.378%, Loss = 0.3703679502010345
Epoch: 8673, Batch Gradient Norm: 10.562013326659162
Epoch: 8673, Batch Gradient Norm after: 10.562013326659162
Epoch 8674/10000, Prediction Accuracy = 63.282%, Loss = 0.37824992537498475
Epoch: 8674, Batch Gradient Norm: 11.580410759926089
Epoch: 8674, Batch Gradient Norm after: 11.580410759926089
Epoch 8675/10000, Prediction Accuracy = 63.214%, Loss = 0.38650972247123716
Epoch: 8675, Batch Gradient Norm: 10.826499284472467
Epoch: 8675, Batch Gradient Norm after: 10.826499284472467
Epoch 8676/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.37877451777458193
Epoch: 8676, Batch Gradient Norm: 10.167164362345574
Epoch: 8676, Batch Gradient Norm after: 10.167164362345574
Epoch 8677/10000, Prediction Accuracy = 63.348%, Loss = 0.37263553142547606
Epoch: 8677, Batch Gradient Norm: 10.2408157527842
Epoch: 8677, Batch Gradient Norm after: 10.2408157527842
Epoch 8678/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3767673671245575
Epoch: 8678, Batch Gradient Norm: 10.265911496046385
Epoch: 8678, Batch Gradient Norm after: 10.265911496046385
Epoch 8679/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3752648174762726
Epoch: 8679, Batch Gradient Norm: 10.307064208966032
Epoch: 8679, Batch Gradient Norm after: 10.307064208966032
Epoch 8680/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3752897560596466
Epoch: 8680, Batch Gradient Norm: 10.479396207130039
Epoch: 8680, Batch Gradient Norm after: 10.479396207130039
Epoch 8681/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.3756889045238495
Epoch: 8681, Batch Gradient Norm: 8.71710430630756
Epoch: 8681, Batch Gradient Norm after: 8.71710430630756
Epoch 8682/10000, Prediction Accuracy = 63.15%, Loss = 0.36388972997665403
Epoch: 8682, Batch Gradient Norm: 8.585719193675121
Epoch: 8682, Batch Gradient Norm after: 8.585719193675121
Epoch 8683/10000, Prediction Accuracy = 63.314%, Loss = 0.36059795022010804
Epoch: 8683, Batch Gradient Norm: 10.916979816772951
Epoch: 8683, Batch Gradient Norm after: 10.916979816772951
Epoch 8684/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.380564934015274
Epoch: 8684, Batch Gradient Norm: 9.86283424096194
Epoch: 8684, Batch Gradient Norm after: 9.86283424096194
Epoch 8685/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3717952370643616
Epoch: 8685, Batch Gradient Norm: 9.513371206508243
Epoch: 8685, Batch Gradient Norm after: 9.513371206508243
Epoch 8686/10000, Prediction Accuracy = 63.476%, Loss = 0.3685247957706451
Epoch: 8686, Batch Gradient Norm: 8.167829292593327
Epoch: 8686, Batch Gradient Norm after: 8.167829292593327
Epoch 8687/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.35933768153190615
Epoch: 8687, Batch Gradient Norm: 9.248404765522011
Epoch: 8687, Batch Gradient Norm after: 9.248404765522011
Epoch 8688/10000, Prediction Accuracy = 63.294%, Loss = 0.36664209961891175
Epoch: 8688, Batch Gradient Norm: 12.262228877459648
Epoch: 8688, Batch Gradient Norm after: 12.262228877459648
Epoch 8689/10000, Prediction Accuracy = 63.3%, Loss = 0.38994950652122495
Epoch: 8689, Batch Gradient Norm: 8.96610309004253
Epoch: 8689, Batch Gradient Norm after: 8.96610309004253
Epoch 8690/10000, Prediction Accuracy = 63.42%, Loss = 0.3665861845016479
Epoch: 8690, Batch Gradient Norm: 10.56198571295571
Epoch: 8690, Batch Gradient Norm after: 10.56198571295571
Epoch 8691/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.37469820976257323
Epoch: 8691, Batch Gradient Norm: 9.13951682135433
Epoch: 8691, Batch Gradient Norm after: 9.13951682135433
Epoch 8692/10000, Prediction Accuracy = 63.238%, Loss = 0.3665687799453735
Epoch: 8692, Batch Gradient Norm: 7.198372156855899
Epoch: 8692, Batch Gradient Norm after: 7.198372156855899
Epoch 8693/10000, Prediction Accuracy = 63.336%, Loss = 0.3565831959247589
Epoch: 8693, Batch Gradient Norm: 9.609170021264951
Epoch: 8693, Batch Gradient Norm after: 9.609170021264951
Epoch 8694/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.3693226158618927
Epoch: 8694, Batch Gradient Norm: 12.008858619991818
Epoch: 8694, Batch Gradient Norm after: 12.008858619991818
Epoch 8695/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.38692532777786254
Epoch: 8695, Batch Gradient Norm: 9.354235072419575
Epoch: 8695, Batch Gradient Norm after: 9.354235072419575
Epoch 8696/10000, Prediction Accuracy = 63.352%, Loss = 0.36870973110198973
Epoch: 8696, Batch Gradient Norm: 8.679155119133467
Epoch: 8696, Batch Gradient Norm after: 8.679155119133467
Epoch 8697/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.36294957995414734
Epoch: 8697, Batch Gradient Norm: 8.593934377942132
Epoch: 8697, Batch Gradient Norm after: 8.593934377942132
Epoch 8698/10000, Prediction Accuracy = 63.286%, Loss = 0.36057859659194946
Epoch: 8698, Batch Gradient Norm: 11.889652832086535
Epoch: 8698, Batch Gradient Norm after: 11.889652832086535
Epoch 8699/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.38812272548675536
Epoch: 8699, Batch Gradient Norm: 11.573312073120803
Epoch: 8699, Batch Gradient Norm after: 11.573312073120803
Epoch 8700/10000, Prediction Accuracy = 63.342000000000006%, Loss = 0.38677594661712644
Epoch: 8700, Batch Gradient Norm: 12.062815095746243
Epoch: 8700, Batch Gradient Norm after: 12.062815095746243
Epoch 8701/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3912892520427704
Epoch: 8701, Batch Gradient Norm: 8.752396512100557
Epoch: 8701, Batch Gradient Norm after: 8.752396512100557
Epoch 8702/10000, Prediction Accuracy = 63.28599999999999%, Loss = 0.365016108751297
Epoch: 8702, Batch Gradient Norm: 7.80828856795966
Epoch: 8702, Batch Gradient Norm after: 7.80828856795966
Epoch 8703/10000, Prediction Accuracy = 63.402%, Loss = 0.35822641253471377
Epoch: 8703, Batch Gradient Norm: 9.987672587477373
Epoch: 8703, Batch Gradient Norm after: 9.987672587477373
Epoch 8704/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.372215610742569
Epoch: 8704, Batch Gradient Norm: 10.78991766875394
Epoch: 8704, Batch Gradient Norm after: 10.78991766875394
Epoch 8705/10000, Prediction Accuracy = 63.459999999999994%, Loss = 0.3833330512046814
Epoch: 8705, Batch Gradient Norm: 11.47985538372553
Epoch: 8705, Batch Gradient Norm after: 11.47985538372553
Epoch 8706/10000, Prediction Accuracy = 63.074%, Loss = 0.3861112117767334
Epoch: 8706, Batch Gradient Norm: 11.348932515488745
Epoch: 8706, Batch Gradient Norm after: 11.348932515488745
Epoch 8707/10000, Prediction Accuracy = 63.306000000000004%, Loss = 0.3802056133747101
Epoch: 8707, Batch Gradient Norm: 11.613649209391228
Epoch: 8707, Batch Gradient Norm after: 11.613649209391228
Epoch 8708/10000, Prediction Accuracy = 63.376%, Loss = 0.381487113237381
Epoch: 8708, Batch Gradient Norm: 8.679944877519727
Epoch: 8708, Batch Gradient Norm after: 8.679944877519727
Epoch 8709/10000, Prediction Accuracy = 63.476%, Loss = 0.36594467163085936
Epoch: 8709, Batch Gradient Norm: 7.854970650312304
Epoch: 8709, Batch Gradient Norm after: 7.854970650312304
Epoch 8710/10000, Prediction Accuracy = 63.288%, Loss = 0.35952287912368774
Epoch: 8710, Batch Gradient Norm: 11.02658246152447
Epoch: 8710, Batch Gradient Norm after: 11.02658246152447
Epoch 8711/10000, Prediction Accuracy = 63.088%, Loss = 0.3822633147239685
Epoch: 8711, Batch Gradient Norm: 10.879725826575946
Epoch: 8711, Batch Gradient Norm after: 10.879725826575946
Epoch 8712/10000, Prediction Accuracy = 63.274%, Loss = 0.38559213280677795
Epoch: 8712, Batch Gradient Norm: 7.5954638467371876
Epoch: 8712, Batch Gradient Norm after: 7.5954638467371876
Epoch 8713/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.35907392501831054
Epoch: 8713, Batch Gradient Norm: 9.553414788751374
Epoch: 8713, Batch Gradient Norm after: 9.553414788751374
Epoch 8714/10000, Prediction Accuracy = 63.407999999999994%, Loss = 0.3706803500652313
Epoch: 8714, Batch Gradient Norm: 8.782204978397234
Epoch: 8714, Batch Gradient Norm after: 8.782204978397234
Epoch 8715/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.367511510848999
Epoch: 8715, Batch Gradient Norm: 9.238164803304745
Epoch: 8715, Batch Gradient Norm after: 9.238164803304745
Epoch 8716/10000, Prediction Accuracy = 63.468%, Loss = 0.36647247076034545
Epoch: 8716, Batch Gradient Norm: 10.454857847031814
Epoch: 8716, Batch Gradient Norm after: 10.454857847031814
Epoch 8717/10000, Prediction Accuracy = 63.126%, Loss = 0.3744635820388794
Epoch: 8717, Batch Gradient Norm: 9.779338470036805
Epoch: 8717, Batch Gradient Norm after: 9.779338470036805
Epoch 8718/10000, Prediction Accuracy = 63.234%, Loss = 0.36824617385864256
Epoch: 8718, Batch Gradient Norm: 9.179297150562116
Epoch: 8718, Batch Gradient Norm after: 9.179297150562116
Epoch 8719/10000, Prediction Accuracy = 63.424%, Loss = 0.36513054370880127
Epoch: 8719, Batch Gradient Norm: 11.867375626779731
Epoch: 8719, Batch Gradient Norm after: 11.867375626779731
Epoch 8720/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.38332782983779906
Epoch: 8720, Batch Gradient Norm: 9.322712262933074
Epoch: 8720, Batch Gradient Norm after: 9.322712262933074
Epoch 8721/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3679246485233307
Epoch: 8721, Batch Gradient Norm: 10.146210399434459
Epoch: 8721, Batch Gradient Norm after: 10.146210399434459
Epoch 8722/10000, Prediction Accuracy = 63.226%, Loss = 0.3730860650539398
Epoch: 8722, Batch Gradient Norm: 10.963314548054955
Epoch: 8722, Batch Gradient Norm after: 10.963314548054955
Epoch 8723/10000, Prediction Accuracy = 63.362%, Loss = 0.3807801902294159
Epoch: 8723, Batch Gradient Norm: 11.044617875365237
Epoch: 8723, Batch Gradient Norm after: 11.044617875365237
Epoch 8724/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.38107641935348513
Epoch: 8724, Batch Gradient Norm: 8.411452272943068
Epoch: 8724, Batch Gradient Norm after: 8.411452272943068
Epoch 8725/10000, Prediction Accuracy = 63.4%, Loss = 0.3620842039585114
Epoch: 8725, Batch Gradient Norm: 8.240499574467421
Epoch: 8725, Batch Gradient Norm after: 8.240499574467421
Epoch 8726/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.3594318091869354
Epoch: 8726, Batch Gradient Norm: 8.647300314298972
Epoch: 8726, Batch Gradient Norm after: 8.647300314298972
Epoch 8727/10000, Prediction Accuracy = 63.258%, Loss = 0.363474452495575
Epoch: 8727, Batch Gradient Norm: 8.372711630928693
Epoch: 8727, Batch Gradient Norm after: 8.372711630928693
Epoch 8728/10000, Prediction Accuracy = 63.11600000000001%, Loss = 0.36079899668693544
Epoch: 8728, Batch Gradient Norm: 9.256261851954358
Epoch: 8728, Batch Gradient Norm after: 9.256261851954358
Epoch 8729/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.36526378989219666
Epoch: 8729, Batch Gradient Norm: 13.22621088953091
Epoch: 8729, Batch Gradient Norm after: 13.22621088953091
Epoch 8730/10000, Prediction Accuracy = 63.306000000000004%, Loss = 0.39540749192237856
Epoch: 8730, Batch Gradient Norm: 11.509266853594752
Epoch: 8730, Batch Gradient Norm after: 11.509266853594752
Epoch 8731/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.3828728973865509
Epoch: 8731, Batch Gradient Norm: 9.206343553969644
Epoch: 8731, Batch Gradient Norm after: 9.206343553969644
Epoch 8732/10000, Prediction Accuracy = 63.342%, Loss = 0.3665278494358063
Epoch: 8732, Batch Gradient Norm: 10.505055523511302
Epoch: 8732, Batch Gradient Norm after: 10.505055523511302
Epoch 8733/10000, Prediction Accuracy = 63.298%, Loss = 0.3752823770046234
Epoch: 8733, Batch Gradient Norm: 7.4330653059913985
Epoch: 8733, Batch Gradient Norm after: 7.4330653059913985
Epoch 8734/10000, Prediction Accuracy = 63.318%, Loss = 0.35651773810386655
Epoch: 8734, Batch Gradient Norm: 8.928581833893684
Epoch: 8734, Batch Gradient Norm after: 8.928581833893684
Epoch 8735/10000, Prediction Accuracy = 63.42%, Loss = 0.36547940969467163
Epoch: 8735, Batch Gradient Norm: 9.449387539113237
Epoch: 8735, Batch Gradient Norm after: 9.449387539113237
Epoch 8736/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.36781749725341795
Epoch: 8736, Batch Gradient Norm: 8.25097628801625
Epoch: 8736, Batch Gradient Norm after: 8.25097628801625
Epoch 8737/10000, Prediction Accuracy = 63.378%, Loss = 0.3611250460147858
Epoch: 8737, Batch Gradient Norm: 9.055273856643678
Epoch: 8737, Batch Gradient Norm after: 9.055273856643678
Epoch 8738/10000, Prediction Accuracy = 63.386%, Loss = 0.3679612159729004
Epoch: 8738, Batch Gradient Norm: 11.38243993968375
Epoch: 8738, Batch Gradient Norm after: 11.38243993968375
Epoch 8739/10000, Prediction Accuracy = 63.152%, Loss = 0.38657585382461546
Epoch: 8739, Batch Gradient Norm: 9.44263232871065
Epoch: 8739, Batch Gradient Norm after: 9.44263232871065
Epoch 8740/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.367341947555542
Epoch: 8740, Batch Gradient Norm: 10.917892293047855
Epoch: 8740, Batch Gradient Norm after: 10.917892293047855
Epoch 8741/10000, Prediction Accuracy = 63.352%, Loss = 0.38000066876411437
Epoch: 8741, Batch Gradient Norm: 8.923092303201093
Epoch: 8741, Batch Gradient Norm after: 8.923092303201093
Epoch 8742/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.364528226852417
Epoch: 8742, Batch Gradient Norm: 10.991360207009814
Epoch: 8742, Batch Gradient Norm after: 10.991360207009814
Epoch 8743/10000, Prediction Accuracy = 63.132000000000005%, Loss = 0.3826481938362122
Epoch: 8743, Batch Gradient Norm: 11.977942055361009
Epoch: 8743, Batch Gradient Norm after: 11.977942055361009
Epoch 8744/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.38834580183029177
Epoch: 8744, Batch Gradient Norm: 9.720573640642483
Epoch: 8744, Batch Gradient Norm after: 9.720573640642483
Epoch 8745/10000, Prediction Accuracy = 63.198%, Loss = 0.3738946199417114
Epoch: 8745, Batch Gradient Norm: 6.0702826206002936
Epoch: 8745, Batch Gradient Norm after: 6.0702826206002936
Epoch 8746/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3505697906017303
Epoch: 8746, Batch Gradient Norm: 8.014334604324027
Epoch: 8746, Batch Gradient Norm after: 8.014334604324027
Epoch 8747/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.35998901128768923
Epoch: 8747, Batch Gradient Norm: 9.792097643122087
Epoch: 8747, Batch Gradient Norm after: 9.792097643122087
Epoch 8748/10000, Prediction Accuracy = 63.352%, Loss = 0.3683422446250916
Epoch: 8748, Batch Gradient Norm: 12.162837371683665
Epoch: 8748, Batch Gradient Norm after: 12.162837371683665
Epoch 8749/10000, Prediction Accuracy = 63.336%, Loss = 0.3907602071762085
Epoch: 8749, Batch Gradient Norm: 9.567767947326212
Epoch: 8749, Batch Gradient Norm after: 9.567767947326212
Epoch 8750/10000, Prediction Accuracy = 63.36%, Loss = 0.37048482298851015
Epoch: 8750, Batch Gradient Norm: 8.128117874318624
Epoch: 8750, Batch Gradient Norm after: 8.128117874318624
Epoch 8751/10000, Prediction Accuracy = 63.354%, Loss = 0.3624675452709198
Epoch: 8751, Batch Gradient Norm: 10.052270419689117
Epoch: 8751, Batch Gradient Norm after: 10.052270419689117
Epoch 8752/10000, Prediction Accuracy = 63.376%, Loss = 0.37341395020484924
Epoch: 8752, Batch Gradient Norm: 10.581847782089872
Epoch: 8752, Batch Gradient Norm after: 10.581847782089872
Epoch 8753/10000, Prediction Accuracy = 63.395999999999994%, Loss = 0.37619932889938357
Epoch: 8753, Batch Gradient Norm: 10.165796292756518
Epoch: 8753, Batch Gradient Norm after: 10.165796292756518
Epoch 8754/10000, Prediction Accuracy = 63.352%, Loss = 0.3755613386631012
Epoch: 8754, Batch Gradient Norm: 9.899117631667004
Epoch: 8754, Batch Gradient Norm after: 9.899117631667004
Epoch 8755/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.3715942680835724
Epoch: 8755, Batch Gradient Norm: 8.547941840774863
Epoch: 8755, Batch Gradient Norm after: 8.547941840774863
Epoch 8756/10000, Prediction Accuracy = 63.412%, Loss = 0.35955139994621277
Epoch: 8756, Batch Gradient Norm: 12.553504790820886
Epoch: 8756, Batch Gradient Norm after: 12.553504790820886
Epoch 8757/10000, Prediction Accuracy = 63.146%, Loss = 0.39213958382606506
Epoch: 8757, Batch Gradient Norm: 11.688928026590721
Epoch: 8757, Batch Gradient Norm after: 11.688928026590721
Epoch 8758/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3851433157920837
Epoch: 8758, Batch Gradient Norm: 9.785912239338183
Epoch: 8758, Batch Gradient Norm after: 9.785912239338183
Epoch 8759/10000, Prediction Accuracy = 63.318%, Loss = 0.37163921594619753
Epoch: 8759, Batch Gradient Norm: 8.85676248399083
Epoch: 8759, Batch Gradient Norm after: 8.85676248399083
Epoch 8760/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.3648988425731659
Epoch: 8760, Batch Gradient Norm: 7.997371858352348
Epoch: 8760, Batch Gradient Norm after: 7.997371858352348
Epoch 8761/10000, Prediction Accuracy = 63.468%, Loss = 0.36114410758018495
Epoch: 8761, Batch Gradient Norm: 9.115789972471731
Epoch: 8761, Batch Gradient Norm after: 9.115789972471731
Epoch 8762/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.3657281160354614
Epoch: 8762, Batch Gradient Norm: 11.009691608011597
Epoch: 8762, Batch Gradient Norm after: 11.009691608011597
Epoch 8763/10000, Prediction Accuracy = 63.324%, Loss = 0.378032511472702
Epoch: 8763, Batch Gradient Norm: 11.848441253105403
Epoch: 8763, Batch Gradient Norm after: 11.848441253105403
Epoch 8764/10000, Prediction Accuracy = 63.42%, Loss = 0.3812252521514893
Epoch: 8764, Batch Gradient Norm: 10.249045255750238
Epoch: 8764, Batch Gradient Norm after: 10.249045255750238
Epoch 8765/10000, Prediction Accuracy = 63.233999999999995%, Loss = 0.37052532434463503
Epoch: 8765, Batch Gradient Norm: 7.1450420995077195
Epoch: 8765, Batch Gradient Norm after: 7.1450420995077195
Epoch 8766/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3575291931629181
Epoch: 8766, Batch Gradient Norm: 9.591859199108322
Epoch: 8766, Batch Gradient Norm after: 9.591859199108322
Epoch 8767/10000, Prediction Accuracy = 63.302%, Loss = 0.368532782793045
Epoch: 8767, Batch Gradient Norm: 8.196940834975038
Epoch: 8767, Batch Gradient Norm after: 8.196940834975038
Epoch 8768/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.3577416718006134
Epoch: 8768, Batch Gradient Norm: 10.15565525565874
Epoch: 8768, Batch Gradient Norm after: 10.15565525565874
Epoch 8769/10000, Prediction Accuracy = 63.14200000000001%, Loss = 0.37498053908348083
Epoch: 8769, Batch Gradient Norm: 8.809165658691038
Epoch: 8769, Batch Gradient Norm after: 8.809165658691038
Epoch 8770/10000, Prediction Accuracy = 63.428%, Loss = 0.3669082045555115
Epoch: 8770, Batch Gradient Norm: 11.556941984446055
Epoch: 8770, Batch Gradient Norm after: 11.556941984446055
Epoch 8771/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3853886902332306
Epoch: 8771, Batch Gradient Norm: 11.155140893690582
Epoch: 8771, Batch Gradient Norm after: 11.155140893690582
Epoch 8772/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3774066030979156
Epoch: 8772, Batch Gradient Norm: 8.99851712637663
Epoch: 8772, Batch Gradient Norm after: 8.99851712637663
Epoch 8773/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.36523217558860777
Epoch: 8773, Batch Gradient Norm: 9.546593379651508
Epoch: 8773, Batch Gradient Norm after: 9.546593379651508
Epoch 8774/10000, Prediction Accuracy = 63.476%, Loss = 0.3700474977493286
Epoch: 8774, Batch Gradient Norm: 11.108970395550656
Epoch: 8774, Batch Gradient Norm after: 11.108970395550656
Epoch 8775/10000, Prediction Accuracy = 63.23599999999999%, Loss = 0.3802634060382843
Epoch: 8775, Batch Gradient Norm: 9.83661170029597
Epoch: 8775, Batch Gradient Norm after: 9.83661170029597
Epoch 8776/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3726579785346985
Epoch: 8776, Batch Gradient Norm: 8.069123499833214
Epoch: 8776, Batch Gradient Norm after: 8.069123499833214
Epoch 8777/10000, Prediction Accuracy = 63.254%, Loss = 0.3614322304725647
Epoch: 8777, Batch Gradient Norm: 9.964361107897647
Epoch: 8777, Batch Gradient Norm after: 9.964361107897647
Epoch 8778/10000, Prediction Accuracy = 63.258%, Loss = 0.37003275752067566
Epoch: 8778, Batch Gradient Norm: 11.98556158083438
Epoch: 8778, Batch Gradient Norm after: 11.98556158083438
Epoch 8779/10000, Prediction Accuracy = 63.233999999999995%, Loss = 0.3848578274250031
Epoch: 8779, Batch Gradient Norm: 10.455921175622438
Epoch: 8779, Batch Gradient Norm after: 10.455921175622438
Epoch 8780/10000, Prediction Accuracy = 63.27%, Loss = 0.3756370902061462
Epoch: 8780, Batch Gradient Norm: 9.529163919425548
Epoch: 8780, Batch Gradient Norm after: 9.529163919425548
Epoch 8781/10000, Prediction Accuracy = 63.484%, Loss = 0.3683811366558075
Epoch: 8781, Batch Gradient Norm: 8.513824433377993
Epoch: 8781, Batch Gradient Norm after: 8.513824433377993
Epoch 8782/10000, Prediction Accuracy = 63.43599999999999%, Loss = 0.3632426977157593
Epoch: 8782, Batch Gradient Norm: 9.09838826480692
Epoch: 8782, Batch Gradient Norm after: 9.09838826480692
Epoch 8783/10000, Prediction Accuracy = 63.414%, Loss = 0.363835746049881
Epoch: 8783, Batch Gradient Norm: 10.406539737850796
Epoch: 8783, Batch Gradient Norm after: 10.406539737850796
Epoch 8784/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.37167548537254336
Epoch: 8784, Batch Gradient Norm: 9.815649915496602
Epoch: 8784, Batch Gradient Norm after: 9.815649915496602
Epoch 8785/10000, Prediction Accuracy = 63.302%, Loss = 0.3669162571430206
Epoch: 8785, Batch Gradient Norm: 9.020583881229296
Epoch: 8785, Batch Gradient Norm after: 9.020583881229296
Epoch 8786/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.36559555530548093
Epoch: 8786, Batch Gradient Norm: 8.762950517152241
Epoch: 8786, Batch Gradient Norm after: 8.762950517152241
Epoch 8787/10000, Prediction Accuracy = 63.314%, Loss = 0.36673959493637087
Epoch: 8787, Batch Gradient Norm: 8.957495351232156
Epoch: 8787, Batch Gradient Norm after: 8.957495351232156
Epoch 8788/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.36674129366874697
Epoch: 8788, Batch Gradient Norm: 9.82606867449313
Epoch: 8788, Batch Gradient Norm after: 9.82606867449313
Epoch 8789/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3700011134147644
Epoch: 8789, Batch Gradient Norm: 12.434243088903616
Epoch: 8789, Batch Gradient Norm after: 12.434243088903616
Epoch 8790/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3891332447528839
Epoch: 8790, Batch Gradient Norm: 10.328031859823879
Epoch: 8790, Batch Gradient Norm after: 10.328031859823879
Epoch 8791/10000, Prediction Accuracy = 63.226%, Loss = 0.3760408878326416
Epoch: 8791, Batch Gradient Norm: 11.309964705849445
Epoch: 8791, Batch Gradient Norm after: 11.309964705849445
Epoch 8792/10000, Prediction Accuracy = 63.388%, Loss = 0.38339267373085023
Epoch: 8792, Batch Gradient Norm: 10.385092414377041
Epoch: 8792, Batch Gradient Norm after: 10.385092414377041
Epoch 8793/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.37684375047683716
Epoch: 8793, Batch Gradient Norm: 8.723275772681527
Epoch: 8793, Batch Gradient Norm after: 8.723275772681527
Epoch 8794/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.36145101189613343
Epoch: 8794, Batch Gradient Norm: 6.864052073002346
Epoch: 8794, Batch Gradient Norm after: 6.864052073002346
Epoch 8795/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.35233383178710936
Epoch: 8795, Batch Gradient Norm: 8.698831503061847
Epoch: 8795, Batch Gradient Norm after: 8.698831503061847
Epoch 8796/10000, Prediction Accuracy = 63.52%, Loss = 0.36126766800880433
Epoch: 8796, Batch Gradient Norm: 10.001007575694732
Epoch: 8796, Batch Gradient Norm after: 10.001007575694732
Epoch 8797/10000, Prediction Accuracy = 63.432%, Loss = 0.37004886865615844
Epoch: 8797, Batch Gradient Norm: 9.075086273824576
Epoch: 8797, Batch Gradient Norm after: 9.075086273824576
Epoch 8798/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.36324724555015564
Epoch: 8798, Batch Gradient Norm: 13.088669529452043
Epoch: 8798, Batch Gradient Norm after: 13.088669529452043
Epoch 8799/10000, Prediction Accuracy = 63.35%, Loss = 0.39399400949478147
Epoch: 8799, Batch Gradient Norm: 10.552843478011086
Epoch: 8799, Batch Gradient Norm after: 10.552843478011086
Epoch 8800/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.37671095728874204
Epoch: 8800, Batch Gradient Norm: 9.424344435856552
Epoch: 8800, Batch Gradient Norm after: 9.424344435856552
Epoch 8801/10000, Prediction Accuracy = 63.312%, Loss = 0.3689550578594208
Epoch: 8801, Batch Gradient Norm: 7.164447906693241
Epoch: 8801, Batch Gradient Norm after: 7.164447906693241
Epoch 8802/10000, Prediction Accuracy = 63.222%, Loss = 0.3562679588794708
Epoch: 8802, Batch Gradient Norm: 8.975131623835455
Epoch: 8802, Batch Gradient Norm after: 8.975131623835455
Epoch 8803/10000, Prediction Accuracy = 63.38000000000001%, Loss = 0.3620943307876587
Epoch: 8803, Batch Gradient Norm: 9.038125731123158
Epoch: 8803, Batch Gradient Norm after: 9.038125731123158
Epoch 8804/10000, Prediction Accuracy = 63.36%, Loss = 0.3628674030303955
Epoch: 8804, Batch Gradient Norm: 8.630026763928187
Epoch: 8804, Batch Gradient Norm after: 8.630026763928187
Epoch 8805/10000, Prediction Accuracy = 63.262%, Loss = 0.3655212163925171
Epoch: 8805, Batch Gradient Norm: 9.07581014964412
Epoch: 8805, Batch Gradient Norm after: 9.07581014964412
Epoch 8806/10000, Prediction Accuracy = 63.480000000000004%, Loss = 0.36487880945205686
Epoch: 8806, Batch Gradient Norm: 9.867801441858552
Epoch: 8806, Batch Gradient Norm after: 9.867801441858552
Epoch 8807/10000, Prediction Accuracy = 63.342%, Loss = 0.3671255648136139
Epoch: 8807, Batch Gradient Norm: 12.323403297632767
Epoch: 8807, Batch Gradient Norm after: 12.323403297632767
Epoch 8808/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.389345520734787
Epoch: 8808, Batch Gradient Norm: 10.534390852194283
Epoch: 8808, Batch Gradient Norm after: 10.534390852194283
Epoch 8809/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.376491117477417
Epoch: 8809, Batch Gradient Norm: 10.683158722374802
Epoch: 8809, Batch Gradient Norm after: 10.683158722374802
Epoch 8810/10000, Prediction Accuracy = 63.456%, Loss = 0.37698999643325803
Epoch: 8810, Batch Gradient Norm: 9.013368659778525
Epoch: 8810, Batch Gradient Norm after: 9.013368659778525
Epoch 8811/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.36349642276763916
Epoch: 8811, Batch Gradient Norm: 10.423820998542613
Epoch: 8811, Batch Gradient Norm after: 10.423820998542613
Epoch 8812/10000, Prediction Accuracy = 63.384%, Loss = 0.37423090934753417
Epoch: 8812, Batch Gradient Norm: 12.919540076531325
Epoch: 8812, Batch Gradient Norm after: 12.919540076531325
Epoch 8813/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.39718255400657654
Epoch: 8813, Batch Gradient Norm: 9.485457958365123
Epoch: 8813, Batch Gradient Norm after: 9.485457958365123
Epoch 8814/10000, Prediction Accuracy = 63.532%, Loss = 0.36570967435836793
Epoch: 8814, Batch Gradient Norm: 9.919567639426766
Epoch: 8814, Batch Gradient Norm after: 9.919567639426766
Epoch 8815/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.3694866240024567
Epoch: 8815, Batch Gradient Norm: 12.431981529031392
Epoch: 8815, Batch Gradient Norm after: 12.431981529031392
Epoch 8816/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.39155826568603513
Epoch: 8816, Batch Gradient Norm: 8.715318013435045
Epoch: 8816, Batch Gradient Norm after: 8.715318013435045
Epoch 8817/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.36306009292602537
Epoch: 8817, Batch Gradient Norm: 10.081368939275661
Epoch: 8817, Batch Gradient Norm after: 10.081368939275661
Epoch 8818/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3700517058372498
Epoch: 8818, Batch Gradient Norm: 7.40581151435065
Epoch: 8818, Batch Gradient Norm after: 7.40581151435065
Epoch 8819/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.35486982464790345
Epoch: 8819, Batch Gradient Norm: 10.26885284625706
Epoch: 8819, Batch Gradient Norm after: 10.26885284625706
Epoch 8820/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.37257891297340395
Epoch: 8820, Batch Gradient Norm: 11.938091876243588
Epoch: 8820, Batch Gradient Norm after: 11.938091876243588
Epoch 8821/10000, Prediction Accuracy = 63.262%, Loss = 0.3897964656352997
Epoch: 8821, Batch Gradient Norm: 9.431707177658247
Epoch: 8821, Batch Gradient Norm after: 9.431707177658247
Epoch 8822/10000, Prediction Accuracy = 63.511999999999986%, Loss = 0.3674468994140625
Epoch: 8822, Batch Gradient Norm: 8.545026008315613
Epoch: 8822, Batch Gradient Norm after: 8.545026008315613
Epoch 8823/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.36386123299598694
Epoch: 8823, Batch Gradient Norm: 9.344707460866422
Epoch: 8823, Batch Gradient Norm after: 9.344707460866422
Epoch 8824/10000, Prediction Accuracy = 63.29%, Loss = 0.3689490854740143
Epoch: 8824, Batch Gradient Norm: 10.325705974555536
Epoch: 8824, Batch Gradient Norm after: 10.325705974555536
Epoch 8825/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3776850163936615
Epoch: 8825, Batch Gradient Norm: 8.775416531017633
Epoch: 8825, Batch Gradient Norm after: 8.775416531017633
Epoch 8826/10000, Prediction Accuracy = 63.46600000000001%, Loss = 0.36253854632377625
Epoch: 8826, Batch Gradient Norm: 9.373928807107006
Epoch: 8826, Batch Gradient Norm after: 9.373928807107006
Epoch 8827/10000, Prediction Accuracy = 63.326%, Loss = 0.36721028089523317
Epoch: 8827, Batch Gradient Norm: 9.621406861964143
Epoch: 8827, Batch Gradient Norm after: 9.621406861964143
Epoch 8828/10000, Prediction Accuracy = 63.428%, Loss = 0.36851550340652467
Epoch: 8828, Batch Gradient Norm: 10.123576223778082
Epoch: 8828, Batch Gradient Norm after: 10.123576223778082
Epoch 8829/10000, Prediction Accuracy = 63.288%, Loss = 0.373352175951004
Epoch: 8829, Batch Gradient Norm: 10.467044513014578
Epoch: 8829, Batch Gradient Norm after: 10.467044513014578
Epoch 8830/10000, Prediction Accuracy = 63.348%, Loss = 0.374265456199646
Epoch: 8830, Batch Gradient Norm: 11.13230189774969
Epoch: 8830, Batch Gradient Norm after: 11.13230189774969
Epoch 8831/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3774651825428009
Epoch: 8831, Batch Gradient Norm: 10.371825235577365
Epoch: 8831, Batch Gradient Norm after: 10.371825235577365
Epoch 8832/10000, Prediction Accuracy = 63.396%, Loss = 0.37007908821105956
Epoch: 8832, Batch Gradient Norm: 9.301298601830199
Epoch: 8832, Batch Gradient Norm after: 9.301298601830199
Epoch 8833/10000, Prediction Accuracy = 63.52%, Loss = 0.3685983955860138
Epoch: 8833, Batch Gradient Norm: 8.872135072677825
Epoch: 8833, Batch Gradient Norm after: 8.872135072677825
Epoch 8834/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.36363927125930784
Epoch: 8834, Batch Gradient Norm: 9.573923325162697
Epoch: 8834, Batch Gradient Norm after: 9.573923325162697
Epoch 8835/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3680382966995239
Epoch: 8835, Batch Gradient Norm: 8.273839296840372
Epoch: 8835, Batch Gradient Norm after: 8.273839296840372
Epoch 8836/10000, Prediction Accuracy = 63.348%, Loss = 0.36012904047966005
Epoch: 8836, Batch Gradient Norm: 9.243394864911641
Epoch: 8836, Batch Gradient Norm after: 9.243394864911641
Epoch 8837/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3623208343982697
Epoch: 8837, Batch Gradient Norm: 9.540142394141085
Epoch: 8837, Batch Gradient Norm after: 9.540142394141085
Epoch 8838/10000, Prediction Accuracy = 63.354%, Loss = 0.3665965020656586
Epoch: 8838, Batch Gradient Norm: 12.189348783007134
Epoch: 8838, Batch Gradient Norm after: 12.189348783007134
Epoch 8839/10000, Prediction Accuracy = 63.146%, Loss = 0.38747451901435853
Epoch: 8839, Batch Gradient Norm: 10.973515132132457
Epoch: 8839, Batch Gradient Norm after: 10.973515132132457
Epoch 8840/10000, Prediction Accuracy = 63.245999999999995%, Loss = 0.3766669392585754
Epoch: 8840, Batch Gradient Norm: 8.142818450949
Epoch: 8840, Batch Gradient Norm after: 8.142818450949
Epoch 8841/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.35855830907821656
Epoch: 8841, Batch Gradient Norm: 9.473014915844583
Epoch: 8841, Batch Gradient Norm after: 9.473014915844583
Epoch 8842/10000, Prediction Accuracy = 63.402%, Loss = 0.3659834086894989
Epoch: 8842, Batch Gradient Norm: 8.629253100609752
Epoch: 8842, Batch Gradient Norm after: 8.629253100609752
Epoch 8843/10000, Prediction Accuracy = 63.36%, Loss = 0.36273764371871947
Epoch: 8843, Batch Gradient Norm: 8.480588131641344
Epoch: 8843, Batch Gradient Norm after: 8.480588131641344
Epoch 8844/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3615997850894928
Epoch: 8844, Batch Gradient Norm: 9.12394114317475
Epoch: 8844, Batch Gradient Norm after: 9.12394114317475
Epoch 8845/10000, Prediction Accuracy = 63.376%, Loss = 0.3661090075969696
Epoch: 8845, Batch Gradient Norm: 9.202085097384401
Epoch: 8845, Batch Gradient Norm after: 9.202085097384401
Epoch 8846/10000, Prediction Accuracy = 63.402%, Loss = 0.3666525185108185
Epoch: 8846, Batch Gradient Norm: 10.901732988037674
Epoch: 8846, Batch Gradient Norm after: 10.901732988037674
Epoch 8847/10000, Prediction Accuracy = 63.254%, Loss = 0.375751793384552
Epoch: 8847, Batch Gradient Norm: 13.477217032699526
Epoch: 8847, Batch Gradient Norm after: 13.477217032699526
Epoch 8848/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.39869325160980223
Epoch: 8848, Batch Gradient Norm: 10.87157445119194
Epoch: 8848, Batch Gradient Norm after: 10.87157445119194
Epoch 8849/10000, Prediction Accuracy = 63.278%, Loss = 0.38278829455375674
Epoch: 8849, Batch Gradient Norm: 6.934626464976087
Epoch: 8849, Batch Gradient Norm after: 6.934626464976087
Epoch 8850/10000, Prediction Accuracy = 63.5%, Loss = 0.35357495546340945
Epoch: 8850, Batch Gradient Norm: 6.1366789686493854
Epoch: 8850, Batch Gradient Norm after: 6.1366789686493854
Epoch 8851/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.347680002450943
Epoch: 8851, Batch Gradient Norm: 8.3132655108222
Epoch: 8851, Batch Gradient Norm after: 8.3132655108222
Epoch 8852/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.35769946575164796
Epoch: 8852, Batch Gradient Norm: 12.219717294783809
Epoch: 8852, Batch Gradient Norm after: 12.219717294783809
Epoch 8853/10000, Prediction Accuracy = 63.342%, Loss = 0.38851349949836733
Epoch: 8853, Batch Gradient Norm: 10.661978343366501
Epoch: 8853, Batch Gradient Norm after: 10.661978343366501
Epoch 8854/10000, Prediction Accuracy = 63.33399999999999%, Loss = 0.3750599384307861
Epoch: 8854, Batch Gradient Norm: 9.416127589541778
Epoch: 8854, Batch Gradient Norm after: 9.416127589541778
Epoch 8855/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.36472383737564085
Epoch: 8855, Batch Gradient Norm: 9.822856837981508
Epoch: 8855, Batch Gradient Norm after: 9.822856837981508
Epoch 8856/10000, Prediction Accuracy = 63.408%, Loss = 0.3695243775844574
Epoch: 8856, Batch Gradient Norm: 8.967012905145674
Epoch: 8856, Batch Gradient Norm after: 8.967012905145674
Epoch 8857/10000, Prediction Accuracy = 63.396%, Loss = 0.3630915105342865
Epoch: 8857, Batch Gradient Norm: 9.011720490836579
Epoch: 8857, Batch Gradient Norm after: 9.011720490836579
Epoch 8858/10000, Prediction Accuracy = 63.274%, Loss = 0.36519787311553953
Epoch: 8858, Batch Gradient Norm: 11.476921019902829
Epoch: 8858, Batch Gradient Norm after: 11.476921019902829
Epoch 8859/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.3782167911529541
Epoch: 8859, Batch Gradient Norm: 9.824981372087002
Epoch: 8859, Batch Gradient Norm after: 9.824981372087002
Epoch 8860/10000, Prediction Accuracy = 63.138%, Loss = 0.36727290153503417
Epoch: 8860, Batch Gradient Norm: 9.7138245121028
Epoch: 8860, Batch Gradient Norm after: 9.7138245121028
Epoch 8861/10000, Prediction Accuracy = 63.412%, Loss = 0.37190091609954834
Epoch: 8861, Batch Gradient Norm: 9.68414014842426
Epoch: 8861, Batch Gradient Norm after: 9.68414014842426
Epoch 8862/10000, Prediction Accuracy = 63.372%, Loss = 0.3694451034069061
Epoch: 8862, Batch Gradient Norm: 8.607586332238476
Epoch: 8862, Batch Gradient Norm after: 8.607586332238476
Epoch 8863/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.3624336957931519
Epoch: 8863, Batch Gradient Norm: 10.05641671222843
Epoch: 8863, Batch Gradient Norm after: 10.05641671222843
Epoch 8864/10000, Prediction Accuracy = 63.314%, Loss = 0.36860241293907164
Epoch: 8864, Batch Gradient Norm: 9.267842311852812
Epoch: 8864, Batch Gradient Norm after: 9.267842311852812
Epoch 8865/10000, Prediction Accuracy = 63.32399999999999%, Loss = 0.3620644211769104
Epoch: 8865, Batch Gradient Norm: 12.175452426035978
Epoch: 8865, Batch Gradient Norm after: 12.175452426035978
Epoch 8866/10000, Prediction Accuracy = 63.116%, Loss = 0.3867464542388916
Epoch: 8866, Batch Gradient Norm: 9.884139035848929
Epoch: 8866, Batch Gradient Norm after: 9.884139035848929
Epoch 8867/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.3688037872314453
Epoch: 8867, Batch Gradient Norm: 9.93247056220431
Epoch: 8867, Batch Gradient Norm after: 9.93247056220431
Epoch 8868/10000, Prediction Accuracy = 63.41799999999999%, Loss = 0.3715958297252655
Epoch: 8868, Batch Gradient Norm: 9.17062652840773
Epoch: 8868, Batch Gradient Norm after: 9.17062652840773
Epoch 8869/10000, Prediction Accuracy = 63.416%, Loss = 0.36558838486671447
Epoch: 8869, Batch Gradient Norm: 10.368207570621578
Epoch: 8869, Batch Gradient Norm after: 10.368207570621578
Epoch 8870/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3736275672912598
Epoch: 8870, Batch Gradient Norm: 14.022397232872011
Epoch: 8870, Batch Gradient Norm after: 14.022397232872011
Epoch 8871/10000, Prediction Accuracy = 63.238%, Loss = 0.4005566656589508
Epoch: 8871, Batch Gradient Norm: 12.624902441462119
Epoch: 8871, Batch Gradient Norm after: 12.624902441462119
Epoch 8872/10000, Prediction Accuracy = 63.331999999999994%, Loss = 0.3945904433727264
Epoch: 8872, Batch Gradient Norm: 9.727650111314013
Epoch: 8872, Batch Gradient Norm after: 9.727650111314013
Epoch 8873/10000, Prediction Accuracy = 63.246%, Loss = 0.37383503317832945
Epoch: 8873, Batch Gradient Norm: 8.374343815654065
Epoch: 8873, Batch Gradient Norm after: 8.374343815654065
Epoch 8874/10000, Prediction Accuracy = 63.588%, Loss = 0.3630934119224548
Epoch: 8874, Batch Gradient Norm: 7.345013861798278
Epoch: 8874, Batch Gradient Norm after: 7.345013861798278
Epoch 8875/10000, Prediction Accuracy = 63.562%, Loss = 0.3539290726184845
Epoch: 8875, Batch Gradient Norm: 9.372029838181083
Epoch: 8875, Batch Gradient Norm after: 9.372029838181083
Epoch 8876/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3648097574710846
Epoch: 8876, Batch Gradient Norm: 10.583467459506318
Epoch: 8876, Batch Gradient Norm after: 10.583467459506318
Epoch 8877/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.37523823976516724
Epoch: 8877, Batch Gradient Norm: 8.49846183378041
Epoch: 8877, Batch Gradient Norm after: 8.49846183378041
Epoch 8878/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.3599033057689667
Epoch: 8878, Batch Gradient Norm: 9.36002911661051
Epoch: 8878, Batch Gradient Norm after: 9.36002911661051
Epoch 8879/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.3725248217582703
Epoch: 8879, Batch Gradient Norm: 8.656441456202991
Epoch: 8879, Batch Gradient Norm after: 8.656441456202991
Epoch 8880/10000, Prediction Accuracy = 63.34599999999999%, Loss = 0.36078220009803774
Epoch: 8880, Batch Gradient Norm: 10.04772945379497
Epoch: 8880, Batch Gradient Norm after: 10.04772945379497
Epoch 8881/10000, Prediction Accuracy = 63.378%, Loss = 0.3706532418727875
Epoch: 8881, Batch Gradient Norm: 9.082939592708135
Epoch: 8881, Batch Gradient Norm after: 9.082939592708135
Epoch 8882/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3611651599407196
Epoch: 8882, Batch Gradient Norm: 11.15893566510169
Epoch: 8882, Batch Gradient Norm after: 11.15893566510169
Epoch 8883/10000, Prediction Accuracy = 63.21200000000001%, Loss = 0.37756372690200807
Epoch: 8883, Batch Gradient Norm: 10.929394036910493
Epoch: 8883, Batch Gradient Norm after: 10.929394036910493
Epoch 8884/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.378124988079071
Epoch: 8884, Batch Gradient Norm: 9.684625391865117
Epoch: 8884, Batch Gradient Norm after: 9.684625391865117
Epoch 8885/10000, Prediction Accuracy = 63.382000000000005%, Loss = 0.3704362750053406
Epoch: 8885, Batch Gradient Norm: 8.951664429056024
Epoch: 8885, Batch Gradient Norm after: 8.951664429056024
Epoch 8886/10000, Prediction Accuracy = 63.39%, Loss = 0.3623177230358124
Epoch: 8886, Batch Gradient Norm: 9.46211256796251
Epoch: 8886, Batch Gradient Norm after: 9.46211256796251
Epoch 8887/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3700560748577118
Epoch: 8887, Batch Gradient Norm: 10.359606198171136
Epoch: 8887, Batch Gradient Norm after: 10.359606198171136
Epoch 8888/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.37274917364120486
Epoch: 8888, Batch Gradient Norm: 11.740556550799349
Epoch: 8888, Batch Gradient Norm after: 11.740556550799349
Epoch 8889/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.38731530904769895
Epoch: 8889, Batch Gradient Norm: 10.005693631527496
Epoch: 8889, Batch Gradient Norm after: 10.005693631527496
Epoch 8890/10000, Prediction Accuracy = 63.262%, Loss = 0.368205326795578
Epoch: 8890, Batch Gradient Norm: 12.41312509146566
Epoch: 8890, Batch Gradient Norm after: 12.41312509146566
Epoch 8891/10000, Prediction Accuracy = 63.254%, Loss = 0.3917121410369873
Epoch: 8891, Batch Gradient Norm: 12.145891059383485
Epoch: 8891, Batch Gradient Norm after: 12.145891059383485
Epoch 8892/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.3940364718437195
Epoch: 8892, Batch Gradient Norm: 7.626945489791687
Epoch: 8892, Batch Gradient Norm after: 7.626945489791687
Epoch 8893/10000, Prediction Accuracy = 63.540000000000006%, Loss = 0.354906952381134
Epoch: 8893, Batch Gradient Norm: 10.199635993555544
Epoch: 8893, Batch Gradient Norm after: 10.199635993555544
Epoch 8894/10000, Prediction Accuracy = 63.364%, Loss = 0.36851565837860106
Epoch: 8894, Batch Gradient Norm: 9.637778141316032
Epoch: 8894, Batch Gradient Norm after: 9.637778141316032
Epoch 8895/10000, Prediction Accuracy = 63.338%, Loss = 0.3630580186843872
Epoch: 8895, Batch Gradient Norm: 7.064100061392223
Epoch: 8895, Batch Gradient Norm after: 7.064100061392223
Epoch 8896/10000, Prediction Accuracy = 63.572%, Loss = 0.35097173452377317
Epoch: 8896, Batch Gradient Norm: 9.592783423890612
Epoch: 8896, Batch Gradient Norm after: 9.592783423890612
Epoch 8897/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.37153523564338686
Epoch: 8897, Batch Gradient Norm: 8.133448740714655
Epoch: 8897, Batch Gradient Norm after: 8.133448740714655
Epoch 8898/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.3563404560089111
Epoch: 8898, Batch Gradient Norm: 10.96396378151271
Epoch: 8898, Batch Gradient Norm after: 10.96396378151271
Epoch 8899/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.3776445984840393
Epoch: 8899, Batch Gradient Norm: 11.138690632971706
Epoch: 8899, Batch Gradient Norm after: 11.138690632971706
Epoch 8900/10000, Prediction Accuracy = 63.488%, Loss = 0.3803061366081238
Epoch: 8900, Batch Gradient Norm: 9.32063106578586
Epoch: 8900, Batch Gradient Norm after: 9.32063106578586
Epoch 8901/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3658923149108887
Epoch: 8901, Batch Gradient Norm: 8.864053787736616
Epoch: 8901, Batch Gradient Norm after: 8.864053787736616
Epoch 8902/10000, Prediction Accuracy = 63.336%, Loss = 0.36679997444152834
Epoch: 8902, Batch Gradient Norm: 9.500812236865688
Epoch: 8902, Batch Gradient Norm after: 9.500812236865688
Epoch 8903/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.3654952347278595
Epoch: 8903, Batch Gradient Norm: 11.242341564101972
Epoch: 8903, Batch Gradient Norm after: 11.242341564101972
Epoch 8904/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3775720357894897
Epoch: 8904, Batch Gradient Norm: 10.473966357520725
Epoch: 8904, Batch Gradient Norm after: 10.473966357520725
Epoch 8905/10000, Prediction Accuracy = 63.43599999999999%, Loss = 0.37472317218780515
Epoch: 8905, Batch Gradient Norm: 8.467868221834657
Epoch: 8905, Batch Gradient Norm after: 8.467868221834657
Epoch 8906/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.36173527836799624
Epoch: 8906, Batch Gradient Norm: 7.9979143321198976
Epoch: 8906, Batch Gradient Norm after: 7.9979143321198976
Epoch 8907/10000, Prediction Accuracy = 63.528%, Loss = 0.3574432015419006
Epoch: 8907, Batch Gradient Norm: 10.956117792900068
Epoch: 8907, Batch Gradient Norm after: 10.956117792900068
Epoch 8908/10000, Prediction Accuracy = 63.378%, Loss = 0.3769194781780243
Epoch: 8908, Batch Gradient Norm: 11.977645259662454
Epoch: 8908, Batch Gradient Norm after: 11.977645259662454
Epoch 8909/10000, Prediction Accuracy = 63.49000000000001%, Loss = 0.3827648460865021
Epoch: 8909, Batch Gradient Norm: 10.834093464053055
Epoch: 8909, Batch Gradient Norm after: 10.834093464053055
Epoch 8910/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.373534220457077
Epoch: 8910, Batch Gradient Norm: 8.25389557316238
Epoch: 8910, Batch Gradient Norm after: 8.25389557316238
Epoch 8911/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3579082190990448
Epoch: 8911, Batch Gradient Norm: 9.738482719527042
Epoch: 8911, Batch Gradient Norm after: 9.738482719527042
Epoch 8912/10000, Prediction Accuracy = 63.214%, Loss = 0.36675328612327573
Epoch: 8912, Batch Gradient Norm: 8.053513326459761
Epoch: 8912, Batch Gradient Norm after: 8.053513326459761
Epoch 8913/10000, Prediction Accuracy = 63.492%, Loss = 0.3567783236503601
Epoch: 8913, Batch Gradient Norm: 7.615048949585678
Epoch: 8913, Batch Gradient Norm after: 7.615048949585678
Epoch 8914/10000, Prediction Accuracy = 63.386%, Loss = 0.35476120114326476
Epoch: 8914, Batch Gradient Norm: 11.607293441690267
Epoch: 8914, Batch Gradient Norm after: 11.607293441690267
Epoch 8915/10000, Prediction Accuracy = 63.402%, Loss = 0.38598611354827883
Epoch: 8915, Batch Gradient Norm: 9.954404340468079
Epoch: 8915, Batch Gradient Norm after: 9.954404340468079
Epoch 8916/10000, Prediction Accuracy = 63.288%, Loss = 0.37259954810142515
Epoch: 8916, Batch Gradient Norm: 10.26512596890785
Epoch: 8916, Batch Gradient Norm after: 10.26512596890785
Epoch 8917/10000, Prediction Accuracy = 63.42%, Loss = 0.37155547738075256
Epoch: 8917, Batch Gradient Norm: 10.161287876985135
Epoch: 8917, Batch Gradient Norm after: 10.161287876985135
Epoch 8918/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3731564998626709
Epoch: 8918, Batch Gradient Norm: 10.18815996689601
Epoch: 8918, Batch Gradient Norm after: 10.18815996689601
Epoch 8919/10000, Prediction Accuracy = 63.436%, Loss = 0.37055380940437316
Epoch: 8919, Batch Gradient Norm: 11.29213313336625
Epoch: 8919, Batch Gradient Norm after: 11.29213313336625
Epoch 8920/10000, Prediction Accuracy = 63.124%, Loss = 0.37896065711975097
Epoch: 8920, Batch Gradient Norm: 11.581475214809675
Epoch: 8920, Batch Gradient Norm after: 11.581475214809675
Epoch 8921/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.38071476221084594
Epoch: 8921, Batch Gradient Norm: 11.019309616634178
Epoch: 8921, Batch Gradient Norm after: 11.019309616634178
Epoch 8922/10000, Prediction Accuracy = 63.3%, Loss = 0.3763203680515289
Epoch: 8922, Batch Gradient Norm: 8.906795125061622
Epoch: 8922, Batch Gradient Norm after: 8.906795125061622
Epoch 8923/10000, Prediction Accuracy = 63.472%, Loss = 0.3610083401203156
Epoch: 8923, Batch Gradient Norm: 9.641557678238522
Epoch: 8923, Batch Gradient Norm after: 9.641557678238522
Epoch 8924/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.36795175075531006
Epoch: 8924, Batch Gradient Norm: 10.625097720326755
Epoch: 8924, Batch Gradient Norm after: 10.625097720326755
Epoch 8925/10000, Prediction Accuracy = 63.562%, Loss = 0.37633135318756106
Epoch: 8925, Batch Gradient Norm: 9.316130796062764
Epoch: 8925, Batch Gradient Norm after: 9.316130796062764
Epoch 8926/10000, Prediction Accuracy = 63.166%, Loss = 0.3670665979385376
Epoch: 8926, Batch Gradient Norm: 8.395767576660624
Epoch: 8926, Batch Gradient Norm after: 8.395767576660624
Epoch 8927/10000, Prediction Accuracy = 63.536%, Loss = 0.3589652836322784
Epoch: 8927, Batch Gradient Norm: 9.027083263142643
Epoch: 8927, Batch Gradient Norm after: 9.027083263142643
Epoch 8928/10000, Prediction Accuracy = 63.476%, Loss = 0.3621022880077362
Epoch: 8928, Batch Gradient Norm: 7.193702383108933
Epoch: 8928, Batch Gradient Norm after: 7.193702383108933
Epoch 8929/10000, Prediction Accuracy = 63.4%, Loss = 0.3514809787273407
Epoch: 8929, Batch Gradient Norm: 8.915525842274123
Epoch: 8929, Batch Gradient Norm after: 8.915525842274123
Epoch 8930/10000, Prediction Accuracy = 63.456%, Loss = 0.36030919551849366
Epoch: 8930, Batch Gradient Norm: 11.447327645879225
Epoch: 8930, Batch Gradient Norm after: 11.447327645879225
Epoch 8931/10000, Prediction Accuracy = 63.488%, Loss = 0.3793161630630493
Epoch: 8931, Batch Gradient Norm: 11.46537664883289
Epoch: 8931, Batch Gradient Norm after: 11.46537664883289
Epoch 8932/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3801665723323822
Epoch: 8932, Batch Gradient Norm: 10.334069380473526
Epoch: 8932, Batch Gradient Norm after: 10.334069380473526
Epoch 8933/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.3771126687526703
Epoch: 8933, Batch Gradient Norm: 9.92412295536043
Epoch: 8933, Batch Gradient Norm after: 9.92412295536043
Epoch 8934/10000, Prediction Accuracy = 63.48%, Loss = 0.36764325499534606
Epoch: 8934, Batch Gradient Norm: 8.887823990857468
Epoch: 8934, Batch Gradient Norm after: 8.887823990857468
Epoch 8935/10000, Prediction Accuracy = 63.38000000000001%, Loss = 0.3621769607067108
Epoch: 8935, Batch Gradient Norm: 10.209023170912229
Epoch: 8935, Batch Gradient Norm after: 10.209023170912229
Epoch 8936/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.37100820541381835
Epoch: 8936, Batch Gradient Norm: 9.963525254752646
Epoch: 8936, Batch Gradient Norm after: 9.963525254752646
Epoch 8937/10000, Prediction Accuracy = 63.21400000000001%, Loss = 0.36986812949180603
Epoch: 8937, Batch Gradient Norm: 10.98697164264654
Epoch: 8937, Batch Gradient Norm after: 10.98697164264654
Epoch 8938/10000, Prediction Accuracy = 63.36999999999999%, Loss = 0.3780864953994751
Epoch: 8938, Batch Gradient Norm: 12.63570888501356
Epoch: 8938, Batch Gradient Norm after: 12.63570888501356
Epoch 8939/10000, Prediction Accuracy = 63.322%, Loss = 0.38994069695472716
Epoch: 8939, Batch Gradient Norm: 10.446283105679447
Epoch: 8939, Batch Gradient Norm after: 10.446283105679447
Epoch 8940/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.37296085357666015
Epoch: 8940, Batch Gradient Norm: 8.156694094383122
Epoch: 8940, Batch Gradient Norm after: 8.156694094383122
Epoch 8941/10000, Prediction Accuracy = 63.472%, Loss = 0.35789169669151305
Epoch: 8941, Batch Gradient Norm: 10.907109146437843
Epoch: 8941, Batch Gradient Norm after: 10.907109146437843
Epoch 8942/10000, Prediction Accuracy = 63.588%, Loss = 0.37466915249824523
Epoch: 8942, Batch Gradient Norm: 10.043197886359748
Epoch: 8942, Batch Gradient Norm after: 10.043197886359748
Epoch 8943/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.3731002151966095
Epoch: 8943, Batch Gradient Norm: 8.846149559151494
Epoch: 8943, Batch Gradient Norm after: 8.846149559151494
Epoch 8944/10000, Prediction Accuracy = 63.472%, Loss = 0.3595812737941742
Epoch: 8944, Batch Gradient Norm: 9.054293638643658
Epoch: 8944, Batch Gradient Norm after: 9.054293638643658
Epoch 8945/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.36050996780395506
Epoch: 8945, Batch Gradient Norm: 9.80083956676919
Epoch: 8945, Batch Gradient Norm after: 9.80083956676919
Epoch 8946/10000, Prediction Accuracy = 63.354%, Loss = 0.3655814230442047
Epoch: 8946, Batch Gradient Norm: 9.477669456656189
Epoch: 8946, Batch Gradient Norm after: 9.477669456656189
Epoch 8947/10000, Prediction Accuracy = 63.412%, Loss = 0.3673516631126404
Epoch: 8947, Batch Gradient Norm: 8.115023773612135
Epoch: 8947, Batch Gradient Norm after: 8.115023773612135
Epoch 8948/10000, Prediction Accuracy = 63.49399999999999%, Loss = 0.35883343815803526
Epoch: 8948, Batch Gradient Norm: 9.643105347017658
Epoch: 8948, Batch Gradient Norm after: 9.643105347017658
Epoch 8949/10000, Prediction Accuracy = 63.45%, Loss = 0.3674143970012665
Epoch: 8949, Batch Gradient Norm: 10.682175898476116
Epoch: 8949, Batch Gradient Norm after: 10.682175898476116
Epoch 8950/10000, Prediction Accuracy = 63.30800000000001%, Loss = 0.37562682628631594
Epoch: 8950, Batch Gradient Norm: 8.407134464579078
Epoch: 8950, Batch Gradient Norm after: 8.407134464579078
Epoch 8951/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3579479455947876
Epoch: 8951, Batch Gradient Norm: 11.0052686717657
Epoch: 8951, Batch Gradient Norm after: 11.0052686717657
Epoch 8952/10000, Prediction Accuracy = 63.49399999999999%, Loss = 0.372534441947937
Epoch: 8952, Batch Gradient Norm: 11.168540849190741
Epoch: 8952, Batch Gradient Norm after: 11.168540849190741
Epoch 8953/10000, Prediction Accuracy = 63.41400000000001%, Loss = 0.3781079947948456
Epoch: 8953, Batch Gradient Norm: 10.034919385464294
Epoch: 8953, Batch Gradient Norm after: 10.034919385464294
Epoch 8954/10000, Prediction Accuracy = 63.406000000000006%, Loss = 0.36847180128097534
Epoch: 8954, Batch Gradient Norm: 10.158233443089804
Epoch: 8954, Batch Gradient Norm after: 10.158233443089804
Epoch 8955/10000, Prediction Accuracy = 63.354%, Loss = 0.3686364531517029
Epoch: 8955, Batch Gradient Norm: 8.856814829544001
Epoch: 8955, Batch Gradient Norm after: 8.856814829544001
Epoch 8956/10000, Prediction Accuracy = 63.5%, Loss = 0.36008482575416567
Epoch: 8956, Batch Gradient Norm: 8.448879816221238
Epoch: 8956, Batch Gradient Norm after: 8.448879816221238
Epoch 8957/10000, Prediction Accuracy = 63.322%, Loss = 0.3585404098033905
Epoch: 8957, Batch Gradient Norm: 8.57966771522158
Epoch: 8957, Batch Gradient Norm after: 8.57966771522158
Epoch 8958/10000, Prediction Accuracy = 63.48%, Loss = 0.35881988406181337
Epoch: 8958, Batch Gradient Norm: 11.084547556173098
Epoch: 8958, Batch Gradient Norm after: 11.084547556173098
Epoch 8959/10000, Prediction Accuracy = 63.278%, Loss = 0.3785263419151306
Epoch: 8959, Batch Gradient Norm: 10.193489767757477
Epoch: 8959, Batch Gradient Norm after: 10.193489767757477
Epoch 8960/10000, Prediction Accuracy = 63.452%, Loss = 0.37374935746192933
Epoch: 8960, Batch Gradient Norm: 10.258811243370227
Epoch: 8960, Batch Gradient Norm after: 10.258811243370227
Epoch 8961/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3720539271831512
Epoch: 8961, Batch Gradient Norm: 10.109071960836632
Epoch: 8961, Batch Gradient Norm after: 10.109071960836632
Epoch 8962/10000, Prediction Accuracy = 63.242%, Loss = 0.3669195532798767
Epoch: 8962, Batch Gradient Norm: 10.796929115886279
Epoch: 8962, Batch Gradient Norm after: 10.796929115886279
Epoch 8963/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.3723403215408325
Epoch: 8963, Batch Gradient Norm: 8.309887035848194
Epoch: 8963, Batch Gradient Norm after: 8.309887035848194
Epoch 8964/10000, Prediction Accuracy = 63.463999999999984%, Loss = 0.3589643955230713
Epoch: 8964, Batch Gradient Norm: 10.802118382939428
Epoch: 8964, Batch Gradient Norm after: 10.802118382939428
Epoch 8965/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3726504385471344
Epoch: 8965, Batch Gradient Norm: 9.87465297605429
Epoch: 8965, Batch Gradient Norm after: 9.87465297605429
Epoch 8966/10000, Prediction Accuracy = 63.396%, Loss = 0.3662641704082489
Epoch: 8966, Batch Gradient Norm: 8.566637894046535
Epoch: 8966, Batch Gradient Norm after: 8.566637894046535
Epoch 8967/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.35812893509864807
Epoch: 8967, Batch Gradient Norm: 8.978427662278937
Epoch: 8967, Batch Gradient Norm after: 8.978427662278937
Epoch 8968/10000, Prediction Accuracy = 63.254%, Loss = 0.3660408318042755
Epoch: 8968, Batch Gradient Norm: 10.518521171684258
Epoch: 8968, Batch Gradient Norm after: 10.518521171684258
Epoch 8969/10000, Prediction Accuracy = 63.402%, Loss = 0.37403101921081544
Epoch: 8969, Batch Gradient Norm: 12.377800000595569
Epoch: 8969, Batch Gradient Norm after: 12.377800000595569
Epoch 8970/10000, Prediction Accuracy = 63.246%, Loss = 0.38718523979187014
Epoch: 8970, Batch Gradient Norm: 9.565664439369497
Epoch: 8970, Batch Gradient Norm after: 9.565664439369497
Epoch 8971/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3679269015789032
Epoch: 8971, Batch Gradient Norm: 8.814445262603314
Epoch: 8971, Batch Gradient Norm after: 8.814445262603314
Epoch 8972/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.35958110690116885
Epoch: 8972, Batch Gradient Norm: 12.073433865222736
Epoch: 8972, Batch Gradient Norm after: 12.073433865222736
Epoch 8973/10000, Prediction Accuracy = 63.376%, Loss = 0.38198367357254026
Epoch: 8973, Batch Gradient Norm: 12.1985073157274
Epoch: 8973, Batch Gradient Norm after: 12.1985073157274
Epoch 8974/10000, Prediction Accuracy = 63.343999999999994%, Loss = 0.38491876125335694
Epoch: 8974, Batch Gradient Norm: 8.050067604298755
Epoch: 8974, Batch Gradient Norm after: 8.050067604298755
Epoch 8975/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.3558716416358948
Epoch: 8975, Batch Gradient Norm: 8.264189236312587
Epoch: 8975, Batch Gradient Norm after: 8.264189236312587
Epoch 8976/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3583896040916443
Epoch: 8976, Batch Gradient Norm: 9.287229268110725
Epoch: 8976, Batch Gradient Norm after: 9.287229268110725
Epoch 8977/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3646887481212616
Epoch: 8977, Batch Gradient Norm: 12.570647449794816
Epoch: 8977, Batch Gradient Norm after: 12.570647449794816
Epoch 8978/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.3903330206871033
Epoch: 8978, Batch Gradient Norm: 8.464006027175087
Epoch: 8978, Batch Gradient Norm after: 8.464006027175087
Epoch 8979/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3575251460075378
Epoch: 8979, Batch Gradient Norm: 8.953931511436844
Epoch: 8979, Batch Gradient Norm after: 8.953931511436844
Epoch 8980/10000, Prediction Accuracy = 63.504%, Loss = 0.36287014484405516
Epoch: 8980, Batch Gradient Norm: 10.79757783462588
Epoch: 8980, Batch Gradient Norm after: 10.79757783462588
Epoch 8981/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.37456187009811404
Epoch: 8981, Batch Gradient Norm: 10.675120481036757
Epoch: 8981, Batch Gradient Norm after: 10.675120481036757
Epoch 8982/10000, Prediction Accuracy = 63.48%, Loss = 0.37399918437004087
Epoch: 8982, Batch Gradient Norm: 11.463291498405425
Epoch: 8982, Batch Gradient Norm after: 11.463291498405425
Epoch 8983/10000, Prediction Accuracy = 63.30400000000001%, Loss = 0.37853569984436036
Epoch: 8983, Batch Gradient Norm: 10.811618318854803
Epoch: 8983, Batch Gradient Norm after: 10.811618318854803
Epoch 8984/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3752540051937103
Epoch: 8984, Batch Gradient Norm: 7.398822648974363
Epoch: 8984, Batch Gradient Norm after: 7.398822648974363
Epoch 8985/10000, Prediction Accuracy = 63.602%, Loss = 0.3537653863430023
Epoch: 8985, Batch Gradient Norm: 8.10560115741211
Epoch: 8985, Batch Gradient Norm after: 8.10560115741211
Epoch 8986/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.35537286996841433
Epoch: 8986, Batch Gradient Norm: 10.084310377114484
Epoch: 8986, Batch Gradient Norm after: 10.084310377114484
Epoch 8987/10000, Prediction Accuracy = 63.528%, Loss = 0.3728917419910431
Epoch: 8987, Batch Gradient Norm: 9.134443210033192
Epoch: 8987, Batch Gradient Norm after: 9.134443210033192
Epoch 8988/10000, Prediction Accuracy = 63.396%, Loss = 0.36403981447219846
Epoch: 8988, Batch Gradient Norm: 10.050704989051173
Epoch: 8988, Batch Gradient Norm after: 10.050704989051173
Epoch 8989/10000, Prediction Accuracy = 63.462%, Loss = 0.36969285607337954
Epoch: 8989, Batch Gradient Norm: 10.349882780530507
Epoch: 8989, Batch Gradient Norm after: 10.349882780530507
Epoch 8990/10000, Prediction Accuracy = 63.298%, Loss = 0.37381386160850527
Epoch: 8990, Batch Gradient Norm: 9.458112127989
Epoch: 8990, Batch Gradient Norm after: 9.458112127989
Epoch 8991/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.36294841170310976
Epoch: 8991, Batch Gradient Norm: 11.69246337255333
Epoch: 8991, Batch Gradient Norm after: 11.69246337255333
Epoch 8992/10000, Prediction Accuracy = 63.452%, Loss = 0.37899133563041687
Epoch: 8992, Batch Gradient Norm: 11.653943911840107
Epoch: 8992, Batch Gradient Norm after: 11.653943911840107
Epoch 8993/10000, Prediction Accuracy = 63.552%, Loss = 0.38214916586875913
Epoch: 8993, Batch Gradient Norm: 10.296979144046446
Epoch: 8993, Batch Gradient Norm after: 10.296979144046446
Epoch 8994/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.3704166293144226
Epoch: 8994, Batch Gradient Norm: 7.790846933414735
Epoch: 8994, Batch Gradient Norm after: 7.790846933414735
Epoch 8995/10000, Prediction Accuracy = 63.4%, Loss = 0.35373615026474
Epoch: 8995, Batch Gradient Norm: 9.891578278936706
Epoch: 8995, Batch Gradient Norm after: 9.891578278936706
Epoch 8996/10000, Prediction Accuracy = 63.306000000000004%, Loss = 0.37169221639633176
Epoch: 8996, Batch Gradient Norm: 8.913186594838475
Epoch: 8996, Batch Gradient Norm after: 8.913186594838475
Epoch 8997/10000, Prediction Accuracy = 63.532%, Loss = 0.36526910662651063
Epoch: 8997, Batch Gradient Norm: 6.540022806910973
Epoch: 8997, Batch Gradient Norm after: 6.540022806910973
Epoch 8998/10000, Prediction Accuracy = 63.628%, Loss = 0.35253016352653505
Epoch: 8998, Batch Gradient Norm: 7.743768041161791
Epoch: 8998, Batch Gradient Norm after: 7.743768041161791
Epoch 8999/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3539499223232269
Epoch: 8999, Batch Gradient Norm: 12.123654072900733
Epoch: 8999, Batch Gradient Norm after: 12.123654072900733
Epoch 9000/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.38585922718048093
Epoch: 9000, Batch Gradient Norm: 8.873711888090591
Epoch: 9000, Batch Gradient Norm after: 8.873711888090591
Epoch 9001/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.35946413278579714
Epoch: 9001, Batch Gradient Norm: 8.946728277382544
Epoch: 9001, Batch Gradient Norm after: 8.946728277382544
Epoch 9002/10000, Prediction Accuracy = 63.324%, Loss = 0.3630755007266998
Epoch: 9002, Batch Gradient Norm: 13.534539668613693
Epoch: 9002, Batch Gradient Norm after: 13.534539668613693
Epoch 9003/10000, Prediction Accuracy = 63.412%, Loss = 0.3968497574329376
Epoch: 9003, Batch Gradient Norm: 13.270374284392034
Epoch: 9003, Batch Gradient Norm after: 13.270374284392034
Epoch 9004/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.3940946519374847
Epoch: 9004, Batch Gradient Norm: 7.059235383628078
Epoch: 9004, Batch Gradient Norm after: 7.059235383628078
Epoch 9005/10000, Prediction Accuracy = 63.476%, Loss = 0.3494191884994507
Epoch: 9005, Batch Gradient Norm: 8.280960558568221
Epoch: 9005, Batch Gradient Norm after: 8.280960558568221
Epoch 9006/10000, Prediction Accuracy = 63.484%, Loss = 0.3575122594833374
Epoch: 9006, Batch Gradient Norm: 9.040644664912405
Epoch: 9006, Batch Gradient Norm after: 9.040644664912405
Epoch 9007/10000, Prediction Accuracy = 63.55200000000001%, Loss = 0.3617270886898041
Epoch: 9007, Batch Gradient Norm: 10.515627563521711
Epoch: 9007, Batch Gradient Norm after: 10.515627563521711
Epoch 9008/10000, Prediction Accuracy = 63.45400000000001%, Loss = 0.37023246884346006
Epoch: 9008, Batch Gradient Norm: 8.736352968921977
Epoch: 9008, Batch Gradient Norm after: 8.736352968921977
Epoch 9009/10000, Prediction Accuracy = 63.568%, Loss = 0.3608527719974518
Epoch: 9009, Batch Gradient Norm: 9.299782232246121
Epoch: 9009, Batch Gradient Norm after: 9.299782232246121
Epoch 9010/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3653108060359955
Epoch: 9010, Batch Gradient Norm: 9.651548814516959
Epoch: 9010, Batch Gradient Norm after: 9.651548814516959
Epoch 9011/10000, Prediction Accuracy = 63.476%, Loss = 0.36617681980133054
Epoch: 9011, Batch Gradient Norm: 8.612186587737307
Epoch: 9011, Batch Gradient Norm after: 8.612186587737307
Epoch 9012/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.36070201396942136
Epoch: 9012, Batch Gradient Norm: 9.747859896755251
Epoch: 9012, Batch Gradient Norm after: 9.747859896755251
Epoch 9013/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.36704440116882325
Epoch: 9013, Batch Gradient Norm: 8.919539947783836
Epoch: 9013, Batch Gradient Norm after: 8.919539947783836
Epoch 9014/10000, Prediction Accuracy = 63.269999999999996%, Loss = 0.36168513894081117
Epoch: 9014, Batch Gradient Norm: 9.759090648054222
Epoch: 9014, Batch Gradient Norm after: 9.759090648054222
Epoch 9015/10000, Prediction Accuracy = 63.592%, Loss = 0.36327625513076783
Epoch: 9015, Batch Gradient Norm: 12.603203706956975
Epoch: 9015, Batch Gradient Norm after: 12.603203706956975
Epoch 9016/10000, Prediction Accuracy = 63.46%, Loss = 0.3884317219257355
Epoch: 9016, Batch Gradient Norm: 9.315965741627311
Epoch: 9016, Batch Gradient Norm after: 9.315965741627311
Epoch 9017/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.3630103886127472
Epoch: 9017, Batch Gradient Norm: 12.658386787439655
Epoch: 9017, Batch Gradient Norm after: 12.658386787439655
Epoch 9018/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.38969067931175233
Epoch: 9018, Batch Gradient Norm: 10.416600316325066
Epoch: 9018, Batch Gradient Norm after: 10.416600316325066
Epoch 9019/10000, Prediction Accuracy = 63.226%, Loss = 0.3694552481174469
Epoch: 9019, Batch Gradient Norm: 8.562538502568632
Epoch: 9019, Batch Gradient Norm after: 8.562538502568632
Epoch 9020/10000, Prediction Accuracy = 63.39%, Loss = 0.35961244702339173
Epoch: 9020, Batch Gradient Norm: 8.963176144358414
Epoch: 9020, Batch Gradient Norm after: 8.963176144358414
Epoch 9021/10000, Prediction Accuracy = 63.202%, Loss = 0.36006908416748046
Epoch: 9021, Batch Gradient Norm: 10.995682473538968
Epoch: 9021, Batch Gradient Norm after: 10.995682473538968
Epoch 9022/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.37595653533935547
Epoch: 9022, Batch Gradient Norm: 10.114712285326647
Epoch: 9022, Batch Gradient Norm after: 10.114712285326647
Epoch 9023/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.37225505113601687
Epoch: 9023, Batch Gradient Norm: 10.95536944189937
Epoch: 9023, Batch Gradient Norm after: 10.95536944189937
Epoch 9024/10000, Prediction Accuracy = 63.54%, Loss = 0.3787041187286377
Epoch: 9024, Batch Gradient Norm: 8.968788596275424
Epoch: 9024, Batch Gradient Norm after: 8.968788596275424
Epoch 9025/10000, Prediction Accuracy = 63.46600000000001%, Loss = 0.36538016200065615
Epoch: 9025, Batch Gradient Norm: 5.238721188063298
Epoch: 9025, Batch Gradient Norm after: 5.238721188063298
Epoch 9026/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.34290128350257876
Epoch: 9026, Batch Gradient Norm: 7.703536953248988
Epoch: 9026, Batch Gradient Norm after: 7.703536953248988
Epoch 9027/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.35260292887687683
Epoch: 9027, Batch Gradient Norm: 11.697088509986925
Epoch: 9027, Batch Gradient Norm after: 11.697088509986925
Epoch 9028/10000, Prediction Accuracy = 63.46%, Loss = 0.3782914638519287
Epoch: 9028, Batch Gradient Norm: 12.538163391008094
Epoch: 9028, Batch Gradient Norm after: 12.538163391008094
Epoch 9029/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3846996188163757
Epoch: 9029, Batch Gradient Norm: 12.489342259342104
Epoch: 9029, Batch Gradient Norm after: 12.489342259342104
Epoch 9030/10000, Prediction Accuracy = 63.53599999999999%, Loss = 0.38825302124023436
Epoch: 9030, Batch Gradient Norm: 11.830140582062924
Epoch: 9030, Batch Gradient Norm after: 11.830140582062924
Epoch 9031/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.3789654552936554
Epoch: 9031, Batch Gradient Norm: 9.026930481346565
Epoch: 9031, Batch Gradient Norm after: 9.026930481346565
Epoch 9032/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.361290967464447
Epoch: 9032, Batch Gradient Norm: 7.514628882851017
Epoch: 9032, Batch Gradient Norm after: 7.514628882851017
Epoch 9033/10000, Prediction Accuracy = 63.374%, Loss = 0.3557071387767792
Epoch: 9033, Batch Gradient Norm: 8.345933177459003
Epoch: 9033, Batch Gradient Norm after: 8.345933177459003
Epoch 9034/10000, Prediction Accuracy = 63.31600000000001%, Loss = 0.3576590418815613
Epoch: 9034, Batch Gradient Norm: 8.971447863544409
Epoch: 9034, Batch Gradient Norm after: 8.971447863544409
Epoch 9035/10000, Prediction Accuracy = 63.288%, Loss = 0.36087971925735474
Epoch: 9035, Batch Gradient Norm: 11.226344165162333
Epoch: 9035, Batch Gradient Norm after: 11.226344165162333
Epoch 9036/10000, Prediction Accuracy = 63.36%, Loss = 0.37676326036453245
Epoch: 9036, Batch Gradient Norm: 13.489271815655973
Epoch: 9036, Batch Gradient Norm after: 13.489271815655973
Epoch 9037/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.397541469335556
Epoch: 9037, Batch Gradient Norm: 9.110776760421942
Epoch: 9037, Batch Gradient Norm after: 9.110776760421942
Epoch 9038/10000, Prediction Accuracy = 63.462%, Loss = 0.3639746367931366
Epoch: 9038, Batch Gradient Norm: 6.5136330612302356
Epoch: 9038, Batch Gradient Norm after: 6.5136330612302356
Epoch 9039/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.34744695425033567
Epoch: 9039, Batch Gradient Norm: 10.658317540385307
Epoch: 9039, Batch Gradient Norm after: 10.658317540385307
Epoch 9040/10000, Prediction Accuracy = 63.496%, Loss = 0.3745652437210083
Epoch: 9040, Batch Gradient Norm: 9.280192115466118
Epoch: 9040, Batch Gradient Norm after: 9.280192115466118
Epoch 9041/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.36410401463508607
Epoch: 9041, Batch Gradient Norm: 8.495532140640515
Epoch: 9041, Batch Gradient Norm after: 8.495532140640515
Epoch 9042/10000, Prediction Accuracy = 63.532%, Loss = 0.35884328484535216
Epoch: 9042, Batch Gradient Norm: 10.197620953032132
Epoch: 9042, Batch Gradient Norm after: 10.197620953032132
Epoch 9043/10000, Prediction Accuracy = 63.602%, Loss = 0.3681842267513275
Epoch: 9043, Batch Gradient Norm: 11.10171923035163
Epoch: 9043, Batch Gradient Norm after: 11.10171923035163
Epoch 9044/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.3735307276248932
Epoch: 9044, Batch Gradient Norm: 11.391692415485284
Epoch: 9044, Batch Gradient Norm after: 11.391692415485284
Epoch 9045/10000, Prediction Accuracy = 63.436%, Loss = 0.374096417427063
Epoch: 9045, Batch Gradient Norm: 8.974166715128991
Epoch: 9045, Batch Gradient Norm after: 8.974166715128991
Epoch 9046/10000, Prediction Accuracy = 63.584%, Loss = 0.36063459515571594
Epoch: 9046, Batch Gradient Norm: 9.902143246367322
Epoch: 9046, Batch Gradient Norm after: 9.902143246367322
Epoch 9047/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.36553582549095154
Epoch: 9047, Batch Gradient Norm: 9.525027027555845
Epoch: 9047, Batch Gradient Norm after: 9.525027027555845
Epoch 9048/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.36762702465057373
Epoch: 9048, Batch Gradient Norm: 10.251881432308553
Epoch: 9048, Batch Gradient Norm after: 10.251881432308553
Epoch 9049/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.37188373804092406
Epoch: 9049, Batch Gradient Norm: 8.401782997513545
Epoch: 9049, Batch Gradient Norm after: 8.401782997513545
Epoch 9050/10000, Prediction Accuracy = 63.436%, Loss = 0.35688549280166626
Epoch: 9050, Batch Gradient Norm: 8.519104640869433
Epoch: 9050, Batch Gradient Norm after: 8.519104640869433
Epoch 9051/10000, Prediction Accuracy = 63.482000000000006%, Loss = 0.3570828139781952
Epoch: 9051, Batch Gradient Norm: 10.433842810299529
Epoch: 9051, Batch Gradient Norm after: 10.433842810299529
Epoch 9052/10000, Prediction Accuracy = 63.274%, Loss = 0.372936749458313
Epoch: 9052, Batch Gradient Norm: 8.843909418558978
Epoch: 9052, Batch Gradient Norm after: 8.843909418558978
Epoch 9053/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.3606716215610504
Epoch: 9053, Batch Gradient Norm: 9.355616463089456
Epoch: 9053, Batch Gradient Norm after: 9.355616463089456
Epoch 9054/10000, Prediction Accuracy = 63.18800000000001%, Loss = 0.36204892992973325
Epoch: 9054, Batch Gradient Norm: 11.116016621130381
Epoch: 9054, Batch Gradient Norm after: 11.116016621130381
Epoch 9055/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3750573813915253
Epoch: 9055, Batch Gradient Norm: 9.182741091298638
Epoch: 9055, Batch Gradient Norm after: 9.182741091298638
Epoch 9056/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.3618850827217102
Epoch: 9056, Batch Gradient Norm: 11.616191606343511
Epoch: 9056, Batch Gradient Norm after: 11.616191606343511
Epoch 9057/10000, Prediction Accuracy = 63.338%, Loss = 0.3835215151309967
Epoch: 9057, Batch Gradient Norm: 7.882107923338973
Epoch: 9057, Batch Gradient Norm after: 7.882107923338973
Epoch 9058/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3556131899356842
Epoch: 9058, Batch Gradient Norm: 10.782708782722917
Epoch: 9058, Batch Gradient Norm after: 10.782708782722917
Epoch 9059/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.37205201387405396
Epoch: 9059, Batch Gradient Norm: 11.799697457599319
Epoch: 9059, Batch Gradient Norm after: 11.799697457599319
Epoch 9060/10000, Prediction Accuracy = 63.36999999999999%, Loss = 0.37784305214881897
Epoch: 9060, Batch Gradient Norm: 10.264686217152919
Epoch: 9060, Batch Gradient Norm after: 10.264686217152919
Epoch 9061/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.3677669823169708
Epoch: 9061, Batch Gradient Norm: 10.101888725125146
Epoch: 9061, Batch Gradient Norm after: 10.101888725125146
Epoch 9062/10000, Prediction Accuracy = 63.488%, Loss = 0.3681767642498016
Epoch: 9062, Batch Gradient Norm: 8.735183060773442
Epoch: 9062, Batch Gradient Norm after: 8.735183060773442
Epoch 9063/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3601021945476532
Epoch: 9063, Batch Gradient Norm: 8.178879832350765
Epoch: 9063, Batch Gradient Norm after: 8.178879832350765
Epoch 9064/10000, Prediction Accuracy = 63.536%, Loss = 0.3575261950492859
Epoch: 9064, Batch Gradient Norm: 7.645053909995742
Epoch: 9064, Batch Gradient Norm after: 7.645053909995742
Epoch 9065/10000, Prediction Accuracy = 63.43999999999998%, Loss = 0.3541400611400604
Epoch: 9065, Batch Gradient Norm: 11.065662062643016
Epoch: 9065, Batch Gradient Norm after: 11.065662062643016
Epoch 9066/10000, Prediction Accuracy = 63.516%, Loss = 0.37301307916641235
Epoch: 9066, Batch Gradient Norm: 11.566617393477731
Epoch: 9066, Batch Gradient Norm after: 11.566617393477731
Epoch 9067/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.37609254121780394
Epoch: 9067, Batch Gradient Norm: 7.321471478871886
Epoch: 9067, Batch Gradient Norm after: 7.321471478871886
Epoch 9068/10000, Prediction Accuracy = 63.58399999999999%, Loss = 0.35097846388816833
Epoch: 9068, Batch Gradient Norm: 9.625719977499665
Epoch: 9068, Batch Gradient Norm after: 9.625719977499665
Epoch 9069/10000, Prediction Accuracy = 63.39200000000001%, Loss = 0.36430363059043885
Epoch: 9069, Batch Gradient Norm: 9.015593532973316
Epoch: 9069, Batch Gradient Norm after: 9.015593532973316
Epoch 9070/10000, Prediction Accuracy = 63.464%, Loss = 0.3616215169429779
Epoch: 9070, Batch Gradient Norm: 10.458897090888842
Epoch: 9070, Batch Gradient Norm after: 10.458897090888842
Epoch 9071/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.3735915422439575
Epoch: 9071, Batch Gradient Norm: 12.279394222375823
Epoch: 9071, Batch Gradient Norm after: 12.279394222375823
Epoch 9072/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.38685542345046997
Epoch: 9072, Batch Gradient Norm: 9.110640971151199
Epoch: 9072, Batch Gradient Norm after: 9.110640971151199
Epoch 9073/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3615549385547638
Epoch: 9073, Batch Gradient Norm: 9.307275310367459
Epoch: 9073, Batch Gradient Norm after: 9.307275310367459
Epoch 9074/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3616214096546173
Epoch: 9074, Batch Gradient Norm: 9.142932340057298
Epoch: 9074, Batch Gradient Norm after: 9.142932340057298
Epoch 9075/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.3605578601360321
Epoch: 9075, Batch Gradient Norm: 9.719257655827088
Epoch: 9075, Batch Gradient Norm after: 9.719257655827088
Epoch 9076/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.365488201379776
Epoch: 9076, Batch Gradient Norm: 11.227277795075965
Epoch: 9076, Batch Gradient Norm after: 11.227277795075965
Epoch 9077/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.37362719178199766
Epoch: 9077, Batch Gradient Norm: 9.312218490999134
Epoch: 9077, Batch Gradient Norm after: 9.312218490999134
Epoch 9078/10000, Prediction Accuracy = 63.641999999999996%, Loss = 0.36084066033363343
Epoch: 9078, Batch Gradient Norm: 11.621617399739863
Epoch: 9078, Batch Gradient Norm after: 11.621617399739863
Epoch 9079/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.37989898920059206
Epoch: 9079, Batch Gradient Norm: 10.616580282384595
Epoch: 9079, Batch Gradient Norm after: 10.616580282384595
Epoch 9080/10000, Prediction Accuracy = 63.398%, Loss = 0.36829313039779665
Epoch: 9080, Batch Gradient Norm: 6.236393433264627
Epoch: 9080, Batch Gradient Norm after: 6.236393433264627
Epoch 9081/10000, Prediction Accuracy = 63.574%, Loss = 0.34425501227378846
Epoch: 9081, Batch Gradient Norm: 10.42928585575869
Epoch: 9081, Batch Gradient Norm after: 10.42928585575869
Epoch 9082/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3733344256877899
Epoch: 9082, Batch Gradient Norm: 9.005141285797109
Epoch: 9082, Batch Gradient Norm after: 9.005141285797109
Epoch 9083/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3628646969795227
Epoch: 9083, Batch Gradient Norm: 11.504194775286976
Epoch: 9083, Batch Gradient Norm after: 11.504194775286976
Epoch 9084/10000, Prediction Accuracy = 63.338%, Loss = 0.38189334869384767
Epoch: 9084, Batch Gradient Norm: 9.648360759208465
Epoch: 9084, Batch Gradient Norm after: 9.648360759208465
Epoch 9085/10000, Prediction Accuracy = 63.471999999999994%, Loss = 0.3687265753746033
Epoch: 9085, Batch Gradient Norm: 8.032363045868749
Epoch: 9085, Batch Gradient Norm after: 8.032363045868749
Epoch 9086/10000, Prediction Accuracy = 63.44%, Loss = 0.3540307283401489
Epoch: 9086, Batch Gradient Norm: 10.958964042908436
Epoch: 9086, Batch Gradient Norm after: 10.958964042908436
Epoch 9087/10000, Prediction Accuracy = 63.488%, Loss = 0.3717722773551941
Epoch: 9087, Batch Gradient Norm: 11.82208761389949
Epoch: 9087, Batch Gradient Norm after: 11.82208761389949
Epoch 9088/10000, Prediction Accuracy = 63.426%, Loss = 0.3827285051345825
Epoch: 9088, Batch Gradient Norm: 9.187165036022144
Epoch: 9088, Batch Gradient Norm after: 9.187165036022144
Epoch 9089/10000, Prediction Accuracy = 63.471999999999994%, Loss = 0.3616002321243286
Epoch: 9089, Batch Gradient Norm: 11.600658513637967
Epoch: 9089, Batch Gradient Norm after: 11.600658513637967
Epoch 9090/10000, Prediction Accuracy = 63.286%, Loss = 0.37668190598487855
Epoch: 9090, Batch Gradient Norm: 9.740368332518953
Epoch: 9090, Batch Gradient Norm after: 9.740368332518953
Epoch 9091/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.36573862433433535
Epoch: 9091, Batch Gradient Norm: 9.986832387823775
Epoch: 9091, Batch Gradient Norm after: 9.986832387823775
Epoch 9092/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.36925439834594725
Epoch: 9092, Batch Gradient Norm: 8.92096362386861
Epoch: 9092, Batch Gradient Norm after: 8.92096362386861
Epoch 9093/10000, Prediction Accuracy = 63.562%, Loss = 0.3594156980514526
Epoch: 9093, Batch Gradient Norm: 8.264119963583859
Epoch: 9093, Batch Gradient Norm after: 8.264119963583859
Epoch 9094/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.3572657942771912
Epoch: 9094, Batch Gradient Norm: 9.611504778679766
Epoch: 9094, Batch Gradient Norm after: 9.611504778679766
Epoch 9095/10000, Prediction Accuracy = 63.45%, Loss = 0.3646138072013855
Epoch: 9095, Batch Gradient Norm: 11.053935752066861
Epoch: 9095, Batch Gradient Norm after: 11.053935752066861
Epoch 9096/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3739593803882599
Epoch: 9096, Batch Gradient Norm: 11.139567767623383
Epoch: 9096, Batch Gradient Norm after: 11.139567767623383
Epoch 9097/10000, Prediction Accuracy = 63.484%, Loss = 0.3792384147644043
Epoch: 9097, Batch Gradient Norm: 8.876765664948195
Epoch: 9097, Batch Gradient Norm after: 8.876765664948195
Epoch 9098/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.36116876602172854
Epoch: 9098, Batch Gradient Norm: 10.229861079952592
Epoch: 9098, Batch Gradient Norm after: 10.229861079952592
Epoch 9099/10000, Prediction Accuracy = 63.574%, Loss = 0.3690391182899475
Epoch: 9099, Batch Gradient Norm: 10.8288813108733
Epoch: 9099, Batch Gradient Norm after: 10.8288813108733
Epoch 9100/10000, Prediction Accuracy = 63.5%, Loss = 0.37273945212364196
Epoch: 9100, Batch Gradient Norm: 9.95390653497366
Epoch: 9100, Batch Gradient Norm after: 9.95390653497366
Epoch 9101/10000, Prediction Accuracy = 63.41799999999999%, Loss = 0.3663378894329071
Epoch: 9101, Batch Gradient Norm: 8.515420960704903
Epoch: 9101, Batch Gradient Norm after: 8.515420960704903
Epoch 9102/10000, Prediction Accuracy = 63.56999999999999%, Loss = 0.35737942457199096
Epoch: 9102, Batch Gradient Norm: 9.644147533704936
Epoch: 9102, Batch Gradient Norm after: 9.644147533704936
Epoch 9103/10000, Prediction Accuracy = 63.45%, Loss = 0.3659159243106842
Epoch: 9103, Batch Gradient Norm: 11.459915948379042
Epoch: 9103, Batch Gradient Norm after: 11.459915948379042
Epoch 9104/10000, Prediction Accuracy = 63.269999999999996%, Loss = 0.3771225333213806
Epoch: 9104, Batch Gradient Norm: 9.220679531079908
Epoch: 9104, Batch Gradient Norm after: 9.220679531079908
Epoch 9105/10000, Prediction Accuracy = 63.49399999999999%, Loss = 0.36636834740638735
Epoch: 9105, Batch Gradient Norm: 8.27386772337212
Epoch: 9105, Batch Gradient Norm after: 8.27386772337212
Epoch 9106/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3581682324409485
Epoch: 9106, Batch Gradient Norm: 11.149104127640564
Epoch: 9106, Batch Gradient Norm after: 11.149104127640564
Epoch 9107/10000, Prediction Accuracy = 63.438%, Loss = 0.3745585560798645
Epoch: 9107, Batch Gradient Norm: 12.106463822523008
Epoch: 9107, Batch Gradient Norm after: 12.106463822523008
Epoch 9108/10000, Prediction Accuracy = 63.52%, Loss = 0.384441202878952
Epoch: 9108, Batch Gradient Norm: 8.872222517746785
Epoch: 9108, Batch Gradient Norm after: 8.872222517746785
Epoch 9109/10000, Prediction Accuracy = 63.604%, Loss = 0.3574464023113251
Epoch: 9109, Batch Gradient Norm: 8.936953597525996
Epoch: 9109, Batch Gradient Norm after: 8.936953597525996
Epoch 9110/10000, Prediction Accuracy = 63.178%, Loss = 0.36145123839378357
Epoch: 9110, Batch Gradient Norm: 8.921560693517417
Epoch: 9110, Batch Gradient Norm after: 8.921560693517417
Epoch 9111/10000, Prediction Accuracy = 63.626%, Loss = 0.36078030467033384
Epoch: 9111, Batch Gradient Norm: 10.809673785367673
Epoch: 9111, Batch Gradient Norm after: 10.809673785367673
Epoch 9112/10000, Prediction Accuracy = 63.428%, Loss = 0.3763249099254608
Epoch: 9112, Batch Gradient Norm: 9.576430964736872
Epoch: 9112, Batch Gradient Norm after: 9.576430964736872
Epoch 9113/10000, Prediction Accuracy = 63.552%, Loss = 0.36722363233566285
Epoch: 9113, Batch Gradient Norm: 10.50128821904948
Epoch: 9113, Batch Gradient Norm after: 10.50128821904948
Epoch 9114/10000, Prediction Accuracy = 63.438%, Loss = 0.3732903242111206
Epoch: 9114, Batch Gradient Norm: 8.427364303683763
Epoch: 9114, Batch Gradient Norm after: 8.427364303683763
Epoch 9115/10000, Prediction Accuracy = 63.56999999999999%, Loss = 0.3559307336807251
Epoch: 9115, Batch Gradient Norm: 11.830995598743948
Epoch: 9115, Batch Gradient Norm after: 11.830995598743948
Epoch 9116/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3787959635257721
Epoch: 9116, Batch Gradient Norm: 11.46438657437285
Epoch: 9116, Batch Gradient Norm after: 11.46438657437285
Epoch 9117/10000, Prediction Accuracy = 63.330000000000005%, Loss = 0.37715325355529783
Epoch: 9117, Batch Gradient Norm: 10.193629718771415
Epoch: 9117, Batch Gradient Norm after: 10.193629718771415
Epoch 9118/10000, Prediction Accuracy = 63.456%, Loss = 0.36747303009033205
Epoch: 9118, Batch Gradient Norm: 8.057947590719273
Epoch: 9118, Batch Gradient Norm after: 8.057947590719273
Epoch 9119/10000, Prediction Accuracy = 63.389999999999986%, Loss = 0.3556752860546112
Epoch: 9119, Batch Gradient Norm: 9.732089686318263
Epoch: 9119, Batch Gradient Norm after: 9.732089686318263
Epoch 9120/10000, Prediction Accuracy = 63.38399999999999%, Loss = 0.36412957310676575
Epoch: 9120, Batch Gradient Norm: 11.049588716990169
Epoch: 9120, Batch Gradient Norm after: 11.049588716990169
Epoch 9121/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.36994064450263975
Epoch: 9121, Batch Gradient Norm: 10.770510329568296
Epoch: 9121, Batch Gradient Norm after: 10.770510329568296
Epoch 9122/10000, Prediction Accuracy = 63.476%, Loss = 0.3717494308948517
Epoch: 9122, Batch Gradient Norm: 10.113937847245657
Epoch: 9122, Batch Gradient Norm after: 10.113937847245657
Epoch 9123/10000, Prediction Accuracy = 63.538%, Loss = 0.3695605397224426
Epoch: 9123, Batch Gradient Norm: 8.328412633048695
Epoch: 9123, Batch Gradient Norm after: 8.328412633048695
Epoch 9124/10000, Prediction Accuracy = 63.552%, Loss = 0.35606160163879397
Epoch: 9124, Batch Gradient Norm: 8.946586665706958
Epoch: 9124, Batch Gradient Norm after: 8.946586665706958
Epoch 9125/10000, Prediction Accuracy = 63.504%, Loss = 0.36215354800224303
Epoch: 9125, Batch Gradient Norm: 8.881799896167479
Epoch: 9125, Batch Gradient Norm after: 8.881799896167479
Epoch 9126/10000, Prediction Accuracy = 63.45799999999999%, Loss = 0.3575169384479523
Epoch: 9126, Batch Gradient Norm: 9.729626613205886
Epoch: 9126, Batch Gradient Norm after: 9.729626613205886
Epoch 9127/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3653570532798767
Epoch: 9127, Batch Gradient Norm: 9.574385982775624
Epoch: 9127, Batch Gradient Norm after: 9.574385982775624
Epoch 9128/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3642806768417358
Epoch: 9128, Batch Gradient Norm: 12.023330713071212
Epoch: 9128, Batch Gradient Norm after: 12.023330713071212
Epoch 9129/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.38007274866104124
Epoch: 9129, Batch Gradient Norm: 10.565903491889815
Epoch: 9129, Batch Gradient Norm after: 10.565903491889815
Epoch 9130/10000, Prediction Accuracy = 63.431999999999995%, Loss = 0.37234293222427367
Epoch: 9130, Batch Gradient Norm: 10.322330550785518
Epoch: 9130, Batch Gradient Norm after: 10.322330550785518
Epoch 9131/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3683407187461853
Epoch: 9131, Batch Gradient Norm: 11.434672151493544
Epoch: 9131, Batch Gradient Norm after: 11.434672151493544
Epoch 9132/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.37571326494216917
Epoch: 9132, Batch Gradient Norm: 11.383685313065348
Epoch: 9132, Batch Gradient Norm after: 11.383685313065348
Epoch 9133/10000, Prediction Accuracy = 63.266000000000005%, Loss = 0.37502001523971557
Epoch: 9133, Batch Gradient Norm: 10.198959075812777
Epoch: 9133, Batch Gradient Norm after: 10.198959075812777
Epoch 9134/10000, Prediction Accuracy = 63.50999999999999%, Loss = 0.3709641695022583
Epoch: 9134, Batch Gradient Norm: 9.523596265808367
Epoch: 9134, Batch Gradient Norm after: 9.523596265808367
Epoch 9135/10000, Prediction Accuracy = 63.398%, Loss = 0.3648711979389191
Epoch: 9135, Batch Gradient Norm: 8.561441235524702
Epoch: 9135, Batch Gradient Norm after: 8.561441235524702
Epoch 9136/10000, Prediction Accuracy = 63.536%, Loss = 0.35572009086608886
Epoch: 9136, Batch Gradient Norm: 7.514716479746463
Epoch: 9136, Batch Gradient Norm after: 7.514716479746463
Epoch 9137/10000, Prediction Accuracy = 63.552%, Loss = 0.35012375116348265
Epoch: 9137, Batch Gradient Norm: 10.945328402284861
Epoch: 9137, Batch Gradient Norm after: 10.945328402284861
Epoch 9138/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3747684895992279
Epoch: 9138, Batch Gradient Norm: 11.32928347167038
Epoch: 9138, Batch Gradient Norm after: 11.32928347167038
Epoch 9139/10000, Prediction Accuracy = 63.568%, Loss = 0.3818717360496521
Epoch: 9139, Batch Gradient Norm: 6.506090159638607
Epoch: 9139, Batch Gradient Norm after: 6.506090159638607
Epoch 9140/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.34676689505577085
Epoch: 9140, Batch Gradient Norm: 8.262070574988009
Epoch: 9140, Batch Gradient Norm after: 8.262070574988009
Epoch 9141/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.35326740741729734
Epoch: 9141, Batch Gradient Norm: 11.236406047839688
Epoch: 9141, Batch Gradient Norm after: 11.236406047839688
Epoch 9142/10000, Prediction Accuracy = 63.35799999999999%, Loss = 0.3747677683830261
Epoch: 9142, Batch Gradient Norm: 9.556644807933235
Epoch: 9142, Batch Gradient Norm after: 9.556644807933235
Epoch 9143/10000, Prediction Accuracy = 63.484%, Loss = 0.3628577649593353
Epoch: 9143, Batch Gradient Norm: 9.655659891651693
Epoch: 9143, Batch Gradient Norm after: 9.655659891651693
Epoch 9144/10000, Prediction Accuracy = 63.378%, Loss = 0.3622799515724182
Epoch: 9144, Batch Gradient Norm: 10.291294717149588
Epoch: 9144, Batch Gradient Norm after: 10.291294717149588
Epoch 9145/10000, Prediction Accuracy = 63.402%, Loss = 0.36827322840690613
Epoch: 9145, Batch Gradient Norm: 9.54098290511056
Epoch: 9145, Batch Gradient Norm after: 9.54098290511056
Epoch 9146/10000, Prediction Accuracy = 63.334%, Loss = 0.36222519874572756
Epoch: 9146, Batch Gradient Norm: 10.203792560221647
Epoch: 9146, Batch Gradient Norm after: 10.203792560221647
Epoch 9147/10000, Prediction Accuracy = 63.414%, Loss = 0.36821040511131287
Epoch: 9147, Batch Gradient Norm: 8.212345226798327
Epoch: 9147, Batch Gradient Norm after: 8.212345226798327
Epoch 9148/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.35343087315559385
Epoch: 9148, Batch Gradient Norm: 8.718468223586092
Epoch: 9148, Batch Gradient Norm after: 8.718468223586092
Epoch 9149/10000, Prediction Accuracy = 63.58%, Loss = 0.35669143199920655
Epoch: 9149, Batch Gradient Norm: 11.026582643449755
Epoch: 9149, Batch Gradient Norm after: 11.026582643449755
Epoch 9150/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.37065866589546204
Epoch: 9150, Batch Gradient Norm: 12.779169093041084
Epoch: 9150, Batch Gradient Norm after: 12.779169093041084
Epoch 9151/10000, Prediction Accuracy = 63.408%, Loss = 0.38968291878700256
Epoch: 9151, Batch Gradient Norm: 12.047221242633862
Epoch: 9151, Batch Gradient Norm after: 12.047221242633862
Epoch 9152/10000, Prediction Accuracy = 63.51399999999999%, Loss = 0.38658313155174256
Epoch: 9152, Batch Gradient Norm: 10.74394982342792
Epoch: 9152, Batch Gradient Norm after: 10.74394982342792
Epoch 9153/10000, Prediction Accuracy = 63.488%, Loss = 0.37544416785240176
Epoch: 9153, Batch Gradient Norm: 10.120998120335452
Epoch: 9153, Batch Gradient Norm after: 10.120998120335452
Epoch 9154/10000, Prediction Accuracy = 63.602%, Loss = 0.3665513157844543
Epoch: 9154, Batch Gradient Norm: 7.825768723505451
Epoch: 9154, Batch Gradient Norm after: 7.825768723505451
Epoch 9155/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.3522277235984802
Epoch: 9155, Batch Gradient Norm: 8.475969528042766
Epoch: 9155, Batch Gradient Norm after: 8.475969528042766
Epoch 9156/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3547666370868683
Epoch: 9156, Batch Gradient Norm: 8.691162458399278
Epoch: 9156, Batch Gradient Norm after: 8.691162458399278
Epoch 9157/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.35328906774520874
Epoch: 9157, Batch Gradient Norm: 11.205032629309715
Epoch: 9157, Batch Gradient Norm after: 11.205032629309715
Epoch 9158/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.37181596755981444
Epoch: 9158, Batch Gradient Norm: 12.989397271857323
Epoch: 9158, Batch Gradient Norm after: 12.989397271857323
Epoch 9159/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3940404236316681
Epoch: 9159, Batch Gradient Norm: 8.812122397532134
Epoch: 9159, Batch Gradient Norm after: 8.812122397532134
Epoch 9160/10000, Prediction Accuracy = 63.624%, Loss = 0.3601254403591156
Epoch: 9160, Batch Gradient Norm: 8.040420912331873
Epoch: 9160, Batch Gradient Norm after: 8.040420912331873
Epoch 9161/10000, Prediction Accuracy = 63.584%, Loss = 0.3555584192276001
Epoch: 9161, Batch Gradient Norm: 7.8784201263516636
Epoch: 9161, Batch Gradient Norm after: 7.8784201263516636
Epoch 9162/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.3541338622570038
Epoch: 9162, Batch Gradient Norm: 9.299274506328347
Epoch: 9162, Batch Gradient Norm after: 9.299274506328347
Epoch 9163/10000, Prediction Accuracy = 63.556%, Loss = 0.36133025884628295
Epoch: 9163, Batch Gradient Norm: 9.818994325721171
Epoch: 9163, Batch Gradient Norm after: 9.818994325721171
Epoch 9164/10000, Prediction Accuracy = 63.536%, Loss = 0.36324538588523864
Epoch: 9164, Batch Gradient Norm: 12.018181664912948
Epoch: 9164, Batch Gradient Norm after: 12.018181664912948
Epoch 9165/10000, Prediction Accuracy = 63.462%, Loss = 0.3783198297023773
Epoch: 9165, Batch Gradient Norm: 11.615679822236379
Epoch: 9165, Batch Gradient Norm after: 11.615679822236379
Epoch 9166/10000, Prediction Accuracy = 63.444%, Loss = 0.3798787176609039
Epoch: 9166, Batch Gradient Norm: 9.210680478863212
Epoch: 9166, Batch Gradient Norm after: 9.210680478863212
Epoch 9167/10000, Prediction Accuracy = 63.574%, Loss = 0.36404775977134707
Epoch: 9167, Batch Gradient Norm: 7.791943422747758
Epoch: 9167, Batch Gradient Norm after: 7.791943422747758
Epoch 9168/10000, Prediction Accuracy = 63.55400000000001%, Loss = 0.3540840744972229
Epoch: 9168, Batch Gradient Norm: 9.352663461633567
Epoch: 9168, Batch Gradient Norm after: 9.352663461633567
Epoch 9169/10000, Prediction Accuracy = 63.676%, Loss = 0.36050750613212584
Epoch: 9169, Batch Gradient Norm: 9.746405566143983
Epoch: 9169, Batch Gradient Norm after: 9.746405566143983
Epoch 9170/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.36196271181106565
Epoch: 9170, Batch Gradient Norm: 9.269781533045943
Epoch: 9170, Batch Gradient Norm after: 9.269781533045943
Epoch 9171/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.359876948595047
Epoch: 9171, Batch Gradient Norm: 9.144667467342598
Epoch: 9171, Batch Gradient Norm after: 9.144667467342598
Epoch 9172/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3601708233356476
Epoch: 9172, Batch Gradient Norm: 9.331428824900897
Epoch: 9172, Batch Gradient Norm after: 9.331428824900897
Epoch 9173/10000, Prediction Accuracy = 63.438%, Loss = 0.36327633261680603
Epoch: 9173, Batch Gradient Norm: 10.55332236939511
Epoch: 9173, Batch Gradient Norm after: 10.55332236939511
Epoch 9174/10000, Prediction Accuracy = 63.614%, Loss = 0.3733614504337311
Epoch: 9174, Batch Gradient Norm: 10.51855609841198
Epoch: 9174, Batch Gradient Norm after: 10.51855609841198
Epoch 9175/10000, Prediction Accuracy = 63.27%, Loss = 0.36834958791732786
Epoch: 9175, Batch Gradient Norm: 11.193312579574451
Epoch: 9175, Batch Gradient Norm after: 11.193312579574451
Epoch 9176/10000, Prediction Accuracy = 63.33%, Loss = 0.3750168263912201
Epoch: 9176, Batch Gradient Norm: 8.653649850795645
Epoch: 9176, Batch Gradient Norm after: 8.653649850795645
Epoch 9177/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.356860363483429
Epoch: 9177, Batch Gradient Norm: 7.7229239596299895
Epoch: 9177, Batch Gradient Norm after: 7.7229239596299895
Epoch 9178/10000, Prediction Accuracy = 63.42%, Loss = 0.3508907794952393
Epoch: 9178, Batch Gradient Norm: 8.612715621944822
Epoch: 9178, Batch Gradient Norm after: 8.612715621944822
Epoch 9179/10000, Prediction Accuracy = 63.45%, Loss = 0.35518681406974795
Epoch: 9179, Batch Gradient Norm: 9.617581556510752
Epoch: 9179, Batch Gradient Norm after: 9.617581556510752
Epoch 9180/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3655867576599121
Epoch: 9180, Batch Gradient Norm: 10.52990668625772
Epoch: 9180, Batch Gradient Norm after: 10.52990668625772
Epoch 9181/10000, Prediction Accuracy = 63.512%, Loss = 0.3705734431743622
Epoch: 9181, Batch Gradient Norm: 11.413983945072731
Epoch: 9181, Batch Gradient Norm after: 11.413983945072731
Epoch 9182/10000, Prediction Accuracy = 63.388%, Loss = 0.3768350899219513
Epoch: 9182, Batch Gradient Norm: 10.909219316347475
Epoch: 9182, Batch Gradient Norm after: 10.909219316347475
Epoch 9183/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.36916533708572385
Epoch: 9183, Batch Gradient Norm: 10.851408742440634
Epoch: 9183, Batch Gradient Norm after: 10.851408742440634
Epoch 9184/10000, Prediction Accuracy = 63.69%, Loss = 0.37150445580482483
Epoch: 9184, Batch Gradient Norm: 9.511620271130292
Epoch: 9184, Batch Gradient Norm after: 9.511620271130292
Epoch 9185/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.364301335811615
Epoch: 9185, Batch Gradient Norm: 8.78421259490099
Epoch: 9185, Batch Gradient Norm after: 8.78421259490099
Epoch 9186/10000, Prediction Accuracy = 63.563999999999986%, Loss = 0.35600839257240297
Epoch: 9186, Batch Gradient Norm: 12.057680066636628
Epoch: 9186, Batch Gradient Norm after: 12.057680066636628
Epoch 9187/10000, Prediction Accuracy = 63.3%, Loss = 0.380496209859848
Epoch: 9187, Batch Gradient Norm: 9.690180322047857
Epoch: 9187, Batch Gradient Norm after: 9.690180322047857
Epoch 9188/10000, Prediction Accuracy = 63.5%, Loss = 0.36243507266044617
Epoch: 9188, Batch Gradient Norm: 11.109062079366147
Epoch: 9188, Batch Gradient Norm after: 11.109062079366147
Epoch 9189/10000, Prediction Accuracy = 63.516000000000005%, Loss = 0.3785237967967987
Epoch: 9189, Batch Gradient Norm: 9.875694444900434
Epoch: 9189, Batch Gradient Norm after: 9.875694444900434
Epoch 9190/10000, Prediction Accuracy = 63.604%, Loss = 0.36722943782806394
Epoch: 9190, Batch Gradient Norm: 9.356778769926132
Epoch: 9190, Batch Gradient Norm after: 9.356778769926132
Epoch 9191/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3620659351348877
Epoch: 9191, Batch Gradient Norm: 12.712209580001971
Epoch: 9191, Batch Gradient Norm after: 12.712209580001971
Epoch 9192/10000, Prediction Accuracy = 63.44%, Loss = 0.3867699086666107
Epoch: 9192, Batch Gradient Norm: 11.308439261035899
Epoch: 9192, Batch Gradient Norm after: 11.308439261035899
Epoch 9193/10000, Prediction Accuracy = 63.666%, Loss = 0.3751476764678955
Epoch: 9193, Batch Gradient Norm: 7.3954042894943015
Epoch: 9193, Batch Gradient Norm after: 7.3954042894943015
Epoch 9194/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3499851584434509
Epoch: 9194, Batch Gradient Norm: 6.004294749962778
Epoch: 9194, Batch Gradient Norm after: 6.004294749962778
Epoch 9195/10000, Prediction Accuracy = 63.73%, Loss = 0.3411864936351776
Epoch: 9195, Batch Gradient Norm: 6.869940822785348
Epoch: 9195, Batch Gradient Norm after: 6.869940822785348
Epoch 9196/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.34675285816192625
Epoch: 9196, Batch Gradient Norm: 9.797036605965035
Epoch: 9196, Batch Gradient Norm after: 9.797036605965035
Epoch 9197/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3634050488471985
Epoch: 9197, Batch Gradient Norm: 11.878372456851038
Epoch: 9197, Batch Gradient Norm after: 11.878372456851038
Epoch 9198/10000, Prediction Accuracy = 63.20400000000001%, Loss = 0.3823432087898254
Epoch: 9198, Batch Gradient Norm: 9.259550257937345
Epoch: 9198, Batch Gradient Norm after: 9.259550257937345
Epoch 9199/10000, Prediction Accuracy = 63.496%, Loss = 0.35974312424659727
Epoch: 9199, Batch Gradient Norm: 11.16598034531046
Epoch: 9199, Batch Gradient Norm after: 11.16598034531046
Epoch 9200/10000, Prediction Accuracy = 63.44599999999999%, Loss = 0.3708928108215332
Epoch: 9200, Batch Gradient Norm: 13.522087448284273
Epoch: 9200, Batch Gradient Norm after: 13.522087448284273
Epoch 9201/10000, Prediction Accuracy = 63.504%, Loss = 0.39084518551826475
Epoch: 9201, Batch Gradient Norm: 7.781886555139591
Epoch: 9201, Batch Gradient Norm after: 7.781886555139591
Epoch 9202/10000, Prediction Accuracy = 63.512%, Loss = 0.3505569398403168
Epoch: 9202, Batch Gradient Norm: 8.520149346533769
Epoch: 9202, Batch Gradient Norm after: 8.520149346533769
Epoch 9203/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.35538161396980283
Epoch: 9203, Batch Gradient Norm: 7.446594778196217
Epoch: 9203, Batch Gradient Norm after: 7.446594778196217
Epoch 9204/10000, Prediction Accuracy = 63.584%, Loss = 0.34958661794662477
Epoch: 9204, Batch Gradient Norm: 7.68143418107425
Epoch: 9204, Batch Gradient Norm after: 7.68143418107425
Epoch 9205/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.3525935173034668
Epoch: 9205, Batch Gradient Norm: 7.119287500144367
Epoch: 9205, Batch Gradient Norm after: 7.119287500144367
Epoch 9206/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.34879311323165896
Epoch: 9206, Batch Gradient Norm: 10.17045959549529
Epoch: 9206, Batch Gradient Norm after: 10.17045959549529
Epoch 9207/10000, Prediction Accuracy = 63.572%, Loss = 0.36770117878913877
Epoch: 9207, Batch Gradient Norm: 10.98485110907023
Epoch: 9207, Batch Gradient Norm after: 10.98485110907023
Epoch 9208/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.3736808180809021
Epoch: 9208, Batch Gradient Norm: 12.42659460190534
Epoch: 9208, Batch Gradient Norm after: 12.42659460190534
Epoch 9209/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.38507654070854186
Epoch: 9209, Batch Gradient Norm: 11.681148228527997
Epoch: 9209, Batch Gradient Norm after: 11.681148228527997
Epoch 9210/10000, Prediction Accuracy = 63.56600000000001%, Loss = 0.38312296867370604
Epoch: 9210, Batch Gradient Norm: 8.871712287199257
Epoch: 9210, Batch Gradient Norm after: 8.871712287199257
Epoch 9211/10000, Prediction Accuracy = 63.452%, Loss = 0.36087656021118164
Epoch: 9211, Batch Gradient Norm: 8.63106724812027
Epoch: 9211, Batch Gradient Norm after: 8.63106724812027
Epoch 9212/10000, Prediction Accuracy = 63.58800000000001%, Loss = 0.3588343024253845
Epoch: 9212, Batch Gradient Norm: 9.18191491205764
Epoch: 9212, Batch Gradient Norm after: 9.18191491205764
Epoch 9213/10000, Prediction Accuracy = 63.626%, Loss = 0.36024298667907717
Epoch: 9213, Batch Gradient Norm: 8.88496661301884
Epoch: 9213, Batch Gradient Norm after: 8.88496661301884
Epoch 9214/10000, Prediction Accuracy = 63.60999999999999%, Loss = 0.3574449300765991
Epoch: 9214, Batch Gradient Norm: 10.65105773191385
Epoch: 9214, Batch Gradient Norm after: 10.65105773191385
Epoch 9215/10000, Prediction Accuracy = 63.528%, Loss = 0.37248981595039365
Epoch: 9215, Batch Gradient Norm: 9.9169756143446
Epoch: 9215, Batch Gradient Norm after: 9.9169756143446
Epoch 9216/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.3651185095310211
Epoch: 9216, Batch Gradient Norm: 10.654611956967324
Epoch: 9216, Batch Gradient Norm after: 10.654611956967324
Epoch 9217/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.36730233430862425
Epoch: 9217, Batch Gradient Norm: 12.244205283153
Epoch: 9217, Batch Gradient Norm after: 12.244205283153
Epoch 9218/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.37905313372612
Epoch: 9218, Batch Gradient Norm: 8.72668957519274
Epoch: 9218, Batch Gradient Norm after: 8.72668957519274
Epoch 9219/10000, Prediction Accuracy = 63.52%, Loss = 0.35763691663742064
Epoch: 9219, Batch Gradient Norm: 8.55654681752228
Epoch: 9219, Batch Gradient Norm after: 8.55654681752228
Epoch 9220/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3571018218994141
Epoch: 9220, Batch Gradient Norm: 9.681545715649088
Epoch: 9220, Batch Gradient Norm after: 9.681545715649088
Epoch 9221/10000, Prediction Accuracy = 63.488%, Loss = 0.36398226022720337
Epoch: 9221, Batch Gradient Norm: 8.535055462305786
Epoch: 9221, Batch Gradient Norm after: 8.535055462305786
Epoch 9222/10000, Prediction Accuracy = 63.646%, Loss = 0.35642672181129453
Epoch: 9222, Batch Gradient Norm: 8.038917739708047
Epoch: 9222, Batch Gradient Norm after: 8.038917739708047
Epoch 9223/10000, Prediction Accuracy = 63.233999999999995%, Loss = 0.3522555291652679
Epoch: 9223, Batch Gradient Norm: 10.889731670888876
Epoch: 9223, Batch Gradient Norm after: 10.889731670888876
Epoch 9224/10000, Prediction Accuracy = 63.348%, Loss = 0.37105796337127683
Epoch: 9224, Batch Gradient Norm: 11.557615301625178
Epoch: 9224, Batch Gradient Norm after: 11.557615301625178
Epoch 9225/10000, Prediction Accuracy = 63.38800000000001%, Loss = 0.37697936296463014
Epoch: 9225, Batch Gradient Norm: 8.481127291083828
Epoch: 9225, Batch Gradient Norm after: 8.481127291083828
Epoch 9226/10000, Prediction Accuracy = 63.53399999999999%, Loss = 0.352234935760498
Epoch: 9226, Batch Gradient Norm: 9.6085850695947
Epoch: 9226, Batch Gradient Norm after: 9.6085850695947
Epoch 9227/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.3636098146438599
Epoch: 9227, Batch Gradient Norm: 10.706871794773084
Epoch: 9227, Batch Gradient Norm after: 10.706871794773084
Epoch 9228/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.37035558819770814
Epoch: 9228, Batch Gradient Norm: 10.400383679480864
Epoch: 9228, Batch Gradient Norm after: 10.400383679480864
Epoch 9229/10000, Prediction Accuracy = 63.44%, Loss = 0.3728710889816284
Epoch: 9229, Batch Gradient Norm: 9.719919888872568
Epoch: 9229, Batch Gradient Norm after: 9.719919888872568
Epoch 9230/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.36405043601989745
Epoch: 9230, Batch Gradient Norm: 9.73295411229254
Epoch: 9230, Batch Gradient Norm after: 9.73295411229254
Epoch 9231/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3614348828792572
Epoch: 9231, Batch Gradient Norm: 9.706224863970066
Epoch: 9231, Batch Gradient Norm after: 9.706224863970066
Epoch 9232/10000, Prediction Accuracy = 63.492%, Loss = 0.3625579416751862
Epoch: 9232, Batch Gradient Norm: 9.55611269419002
Epoch: 9232, Batch Gradient Norm after: 9.55611269419002
Epoch 9233/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.36026771664619445
Epoch: 9233, Batch Gradient Norm: 11.69008441485081
Epoch: 9233, Batch Gradient Norm after: 11.69008441485081
Epoch 9234/10000, Prediction Accuracy = 63.209999999999994%, Loss = 0.3824489414691925
Epoch: 9234, Batch Gradient Norm: 9.539052620879847
Epoch: 9234, Batch Gradient Norm after: 9.539052620879847
Epoch 9235/10000, Prediction Accuracy = 63.568000000000005%, Loss = 0.36194756627082825
Epoch: 9235, Batch Gradient Norm: 10.530481000948594
Epoch: 9235, Batch Gradient Norm after: 10.530481000948594
Epoch 9236/10000, Prediction Accuracy = 63.664%, Loss = 0.36854140758514403
Epoch: 9236, Batch Gradient Norm: 11.41680499514718
Epoch: 9236, Batch Gradient Norm after: 11.41680499514718
Epoch 9237/10000, Prediction Accuracy = 63.636%, Loss = 0.3768341958522797
Epoch: 9237, Batch Gradient Norm: 7.800230759008667
Epoch: 9237, Batch Gradient Norm after: 7.800230759008667
Epoch 9238/10000, Prediction Accuracy = 63.604%, Loss = 0.3511541306972504
Epoch: 9238, Batch Gradient Norm: 7.274160251457736
Epoch: 9238, Batch Gradient Norm after: 7.274160251457736
Epoch 9239/10000, Prediction Accuracy = 63.512%, Loss = 0.3512498736381531
Epoch: 9239, Batch Gradient Norm: 9.185772814130035
Epoch: 9239, Batch Gradient Norm after: 9.185772814130035
Epoch 9240/10000, Prediction Accuracy = 63.45400000000001%, Loss = 0.36051817536354064
Epoch: 9240, Batch Gradient Norm: 11.547170047828835
Epoch: 9240, Batch Gradient Norm after: 11.547170047828835
Epoch 9241/10000, Prediction Accuracy = 63.516%, Loss = 0.3765619397163391
Epoch: 9241, Batch Gradient Norm: 13.298516423024628
Epoch: 9241, Batch Gradient Norm after: 13.298516423024628
Epoch 9242/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.38972747325897217
Epoch: 9242, Batch Gradient Norm: 12.154537198164075
Epoch: 9242, Batch Gradient Norm after: 12.154537198164075
Epoch 9243/10000, Prediction Accuracy = 63.624%, Loss = 0.3857382357120514
Epoch: 9243, Batch Gradient Norm: 7.598388958163843
Epoch: 9243, Batch Gradient Norm after: 7.598388958163843
Epoch 9244/10000, Prediction Accuracy = 63.372%, Loss = 0.3492569625377655
Epoch: 9244, Batch Gradient Norm: 11.110188205241416
Epoch: 9244, Batch Gradient Norm after: 11.110188205241416
Epoch 9245/10000, Prediction Accuracy = 63.443999999999996%, Loss = 0.3693642973899841
Epoch: 9245, Batch Gradient Norm: 11.419909443509564
Epoch: 9245, Batch Gradient Norm after: 11.419909443509564
Epoch 9246/10000, Prediction Accuracy = 63.488%, Loss = 0.3763611733913422
Epoch: 9246, Batch Gradient Norm: 7.746836259720937
Epoch: 9246, Batch Gradient Norm after: 7.746836259720937
Epoch 9247/10000, Prediction Accuracy = 63.624%, Loss = 0.3526712477207184
Epoch: 9247, Batch Gradient Norm: 8.422848915939399
Epoch: 9247, Batch Gradient Norm after: 8.422848915939399
Epoch 9248/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.35501381754875183
Epoch: 9248, Batch Gradient Norm: 8.623229297485421
Epoch: 9248, Batch Gradient Norm after: 8.623229297485421
Epoch 9249/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.3564984619617462
Epoch: 9249, Batch Gradient Norm: 7.583694700918979
Epoch: 9249, Batch Gradient Norm after: 7.583694700918979
Epoch 9250/10000, Prediction Accuracy = 63.598%, Loss = 0.3493143141269684
Epoch: 9250, Batch Gradient Norm: 8.051777942560202
Epoch: 9250, Batch Gradient Norm after: 8.051777942560202
Epoch 9251/10000, Prediction Accuracy = 63.664%, Loss = 0.35108526349067687
Epoch: 9251, Batch Gradient Norm: 9.439805839658902
Epoch: 9251, Batch Gradient Norm after: 9.439805839658902
Epoch 9252/10000, Prediction Accuracy = 63.648%, Loss = 0.35996339917182923
Epoch: 9252, Batch Gradient Norm: 10.846157925905135
Epoch: 9252, Batch Gradient Norm after: 10.846157925905135
Epoch 9253/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.3681750655174255
Epoch: 9253, Batch Gradient Norm: 13.266301890317939
Epoch: 9253, Batch Gradient Norm after: 13.266301890317939
Epoch 9254/10000, Prediction Accuracy = 63.398%, Loss = 0.39645764231681824
Epoch: 9254, Batch Gradient Norm: 12.379665986539827
Epoch: 9254, Batch Gradient Norm after: 12.379665986539827
Epoch 9255/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.39147092700004577
Epoch: 9255, Batch Gradient Norm: 6.829077010412841
Epoch: 9255, Batch Gradient Norm after: 6.829077010412841
Epoch 9256/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.3496526837348938
Epoch: 9256, Batch Gradient Norm: 8.806892441787095
Epoch: 9256, Batch Gradient Norm after: 8.806892441787095
Epoch 9257/10000, Prediction Accuracy = 63.54%, Loss = 0.35951327085494994
Epoch: 9257, Batch Gradient Norm: 7.611646556110518
Epoch: 9257, Batch Gradient Norm after: 7.611646556110518
Epoch 9258/10000, Prediction Accuracy = 63.40999999999999%, Loss = 0.35131911635398866
Epoch: 9258, Batch Gradient Norm: 8.18047582372766
Epoch: 9258, Batch Gradient Norm after: 8.18047582372766
Epoch 9259/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.35569818019866944
Epoch: 9259, Batch Gradient Norm: 10.903716771310105
Epoch: 9259, Batch Gradient Norm after: 10.903716771310105
Epoch 9260/10000, Prediction Accuracy = 63.438%, Loss = 0.3674116849899292
Epoch: 9260, Batch Gradient Norm: 12.822079338530965
Epoch: 9260, Batch Gradient Norm after: 12.822079338530965
Epoch 9261/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3842617332935333
Epoch: 9261, Batch Gradient Norm: 11.790554980579072
Epoch: 9261, Batch Gradient Norm after: 11.790554980579072
Epoch 9262/10000, Prediction Accuracy = 63.378%, Loss = 0.3805069148540497
Epoch: 9262, Batch Gradient Norm: 11.474388805672858
Epoch: 9262, Batch Gradient Norm after: 11.474388805672858
Epoch 9263/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.37402309775352477
Epoch: 9263, Batch Gradient Norm: 8.581294683201577
Epoch: 9263, Batch Gradient Norm after: 8.581294683201577
Epoch 9264/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.3545840263366699
Epoch: 9264, Batch Gradient Norm: 7.953024363597289
Epoch: 9264, Batch Gradient Norm after: 7.953024363597289
Epoch 9265/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.35022290945053103
Epoch: 9265, Batch Gradient Norm: 11.070902211154625
Epoch: 9265, Batch Gradient Norm after: 11.070902211154625
Epoch 9266/10000, Prediction Accuracy = 63.5%, Loss = 0.3703927457332611
Epoch: 9266, Batch Gradient Norm: 10.979049946105121
Epoch: 9266, Batch Gradient Norm after: 10.979049946105121
Epoch 9267/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3695860803127289
Epoch: 9267, Batch Gradient Norm: 9.127844989035516
Epoch: 9267, Batch Gradient Norm after: 9.127844989035516
Epoch 9268/10000, Prediction Accuracy = 63.40400000000001%, Loss = 0.3579441666603088
Epoch: 9268, Batch Gradient Norm: 9.278480181242694
Epoch: 9268, Batch Gradient Norm after: 9.278480181242694
Epoch 9269/10000, Prediction Accuracy = 63.54%, Loss = 0.35919434428215025
Epoch: 9269, Batch Gradient Norm: 8.170152683739953
Epoch: 9269, Batch Gradient Norm after: 8.170152683739953
Epoch 9270/10000, Prediction Accuracy = 63.63000000000001%, Loss = 0.35273662209510803
Epoch: 9270, Batch Gradient Norm: 9.424314895142793
Epoch: 9270, Batch Gradient Norm after: 9.424314895142793
Epoch 9271/10000, Prediction Accuracy = 63.648%, Loss = 0.359307324886322
Epoch: 9271, Batch Gradient Norm: 10.694536494629094
Epoch: 9271, Batch Gradient Norm after: 10.694536494629094
Epoch 9272/10000, Prediction Accuracy = 63.388%, Loss = 0.36953728795051577
Epoch: 9272, Batch Gradient Norm: 10.641226134252607
Epoch: 9272, Batch Gradient Norm after: 10.641226134252607
Epoch 9273/10000, Prediction Accuracy = 63.492%, Loss = 0.3721909999847412
Epoch: 9273, Batch Gradient Norm: 7.583867477260978
Epoch: 9273, Batch Gradient Norm after: 7.583867477260978
Epoch 9274/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.3491827309131622
Epoch: 9274, Batch Gradient Norm: 7.521763906011857
Epoch: 9274, Batch Gradient Norm after: 7.521763906011857
Epoch 9275/10000, Prediction Accuracy = 63.71%, Loss = 0.34986377954483033
Epoch: 9275, Batch Gradient Norm: 9.683832534849097
Epoch: 9275, Batch Gradient Norm after: 9.683832534849097
Epoch 9276/10000, Prediction Accuracy = 63.5%, Loss = 0.3628114700317383
Epoch: 9276, Batch Gradient Norm: 12.267005658484598
Epoch: 9276, Batch Gradient Norm after: 12.267005658484598
Epoch 9277/10000, Prediction Accuracy = 63.41600000000001%, Loss = 0.3835580587387085
Epoch: 9277, Batch Gradient Norm: 9.877646060005446
Epoch: 9277, Batch Gradient Norm after: 9.877646060005446
Epoch 9278/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.36752169132232665
Epoch: 9278, Batch Gradient Norm: 8.151461950100297
Epoch: 9278, Batch Gradient Norm after: 8.151461950100297
Epoch 9279/10000, Prediction Accuracy = 63.52199999999999%, Loss = 0.35313796401023867
Epoch: 9279, Batch Gradient Norm: 8.087017500353054
Epoch: 9279, Batch Gradient Norm after: 8.087017500353054
Epoch 9280/10000, Prediction Accuracy = 63.733999999999995%, Loss = 0.3506906807422638
Epoch: 9280, Batch Gradient Norm: 12.500168552705121
Epoch: 9280, Batch Gradient Norm after: 12.500168552705121
Epoch 9281/10000, Prediction Accuracy = 63.448%, Loss = 0.3827329814434052
Epoch: 9281, Batch Gradient Norm: 11.298082073683746
Epoch: 9281, Batch Gradient Norm after: 11.298082073683746
Epoch 9282/10000, Prediction Accuracy = 63.378%, Loss = 0.37524709701538084
Epoch: 9282, Batch Gradient Norm: 10.772191088301634
Epoch: 9282, Batch Gradient Norm after: 10.772191088301634
Epoch 9283/10000, Prediction Accuracy = 63.54%, Loss = 0.3729007840156555
Epoch: 9283, Batch Gradient Norm: 9.623576654615187
Epoch: 9283, Batch Gradient Norm after: 9.623576654615187
Epoch 9284/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.36205055713653567
Epoch: 9284, Batch Gradient Norm: 7.268028492418927
Epoch: 9284, Batch Gradient Norm after: 7.268028492418927
Epoch 9285/10000, Prediction Accuracy = 63.58%, Loss = 0.34688747525215147
Epoch: 9285, Batch Gradient Norm: 11.772618795416923
Epoch: 9285, Batch Gradient Norm after: 11.772618795416923
Epoch 9286/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.37488665580749514
Epoch: 9286, Batch Gradient Norm: 11.216556444017005
Epoch: 9286, Batch Gradient Norm after: 11.216556444017005
Epoch 9287/10000, Prediction Accuracy = 63.194%, Loss = 0.37271246314048767
Epoch: 9287, Batch Gradient Norm: 11.412453512965884
Epoch: 9287, Batch Gradient Norm after: 11.412453512965884
Epoch 9288/10000, Prediction Accuracy = 63.338%, Loss = 0.37737682461738586
Epoch: 9288, Batch Gradient Norm: 8.965878330106307
Epoch: 9288, Batch Gradient Norm after: 8.965878330106307
Epoch 9289/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.35997275114059446
Epoch: 9289, Batch Gradient Norm: 10.132127679423622
Epoch: 9289, Batch Gradient Norm after: 10.132127679423622
Epoch 9290/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.36534796953201293
Epoch: 9290, Batch Gradient Norm: 10.316313032968518
Epoch: 9290, Batch Gradient Norm after: 10.316313032968518
Epoch 9291/10000, Prediction Accuracy = 63.61%, Loss = 0.3684043824672699
Epoch: 9291, Batch Gradient Norm: 10.213489558516912
Epoch: 9291, Batch Gradient Norm after: 10.213489558516912
Epoch 9292/10000, Prediction Accuracy = 63.53599999999999%, Loss = 0.3668993294239044
Epoch: 9292, Batch Gradient Norm: 8.344997508796704
Epoch: 9292, Batch Gradient Norm after: 8.344997508796704
Epoch 9293/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.3534617781639099
Epoch: 9293, Batch Gradient Norm: 8.67844142481927
Epoch: 9293, Batch Gradient Norm after: 8.67844142481927
Epoch 9294/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.35421053767204286
Epoch: 9294, Batch Gradient Norm: 9.607396807257675
Epoch: 9294, Batch Gradient Norm after: 9.607396807257675
Epoch 9295/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.3601698219776154
Epoch: 9295, Batch Gradient Norm: 9.518181367833826
Epoch: 9295, Batch Gradient Norm after: 9.518181367833826
Epoch 9296/10000, Prediction Accuracy = 63.6%, Loss = 0.36052908897399905
Epoch: 9296, Batch Gradient Norm: 10.169163833207847
Epoch: 9296, Batch Gradient Norm after: 10.169163833207847
Epoch 9297/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.36511183381080625
Epoch: 9297, Batch Gradient Norm: 11.117819159960062
Epoch: 9297, Batch Gradient Norm after: 11.117819159960062
Epoch 9298/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3703209638595581
Epoch: 9298, Batch Gradient Norm: 10.291306289913141
Epoch: 9298, Batch Gradient Norm after: 10.291306289913141
Epoch 9299/10000, Prediction Accuracy = 63.444%, Loss = 0.3641738295555115
Epoch: 9299, Batch Gradient Norm: 9.95168572894005
Epoch: 9299, Batch Gradient Norm after: 9.95168572894005
Epoch 9300/10000, Prediction Accuracy = 63.424%, Loss = 0.36309942603111267
Epoch: 9300, Batch Gradient Norm: 9.120780597757
Epoch: 9300, Batch Gradient Norm after: 9.120780597757
Epoch 9301/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3562964558601379
Epoch: 9301, Batch Gradient Norm: 9.509832783470975
Epoch: 9301, Batch Gradient Norm after: 9.509832783470975
Epoch 9302/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.36015690565109254
Epoch: 9302, Batch Gradient Norm: 10.677474979100412
Epoch: 9302, Batch Gradient Norm after: 10.677474979100412
Epoch 9303/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.3651517450809479
Epoch: 9303, Batch Gradient Norm: 9.380825358366218
Epoch: 9303, Batch Gradient Norm after: 9.380825358366218
Epoch 9304/10000, Prediction Accuracy = 63.62199999999999%, Loss = 0.3599674642086029
Epoch: 9304, Batch Gradient Norm: 9.664727007989699
Epoch: 9304, Batch Gradient Norm after: 9.664727007989699
Epoch 9305/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3618861258029938
Epoch: 9305, Batch Gradient Norm: 8.866591329837958
Epoch: 9305, Batch Gradient Norm after: 8.866591329837958
Epoch 9306/10000, Prediction Accuracy = 63.472%, Loss = 0.356306666135788
Epoch: 9306, Batch Gradient Norm: 9.50152925612077
Epoch: 9306, Batch Gradient Norm after: 9.50152925612077
Epoch 9307/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.360071462392807
Epoch: 9307, Batch Gradient Norm: 10.495233824448675
Epoch: 9307, Batch Gradient Norm after: 10.495233824448675
Epoch 9308/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.3686063766479492
Epoch: 9308, Batch Gradient Norm: 11.040705616519393
Epoch: 9308, Batch Gradient Norm after: 11.040705616519393
Epoch 9309/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3742847144603729
Epoch: 9309, Batch Gradient Norm: 7.572782263203475
Epoch: 9309, Batch Gradient Norm after: 7.572782263203475
Epoch 9310/10000, Prediction Accuracy = 63.722%, Loss = 0.34892473816871644
Epoch: 9310, Batch Gradient Norm: 8.738328691553258
Epoch: 9310, Batch Gradient Norm after: 8.738328691553258
Epoch 9311/10000, Prediction Accuracy = 63.552%, Loss = 0.3546499788761139
Epoch: 9311, Batch Gradient Norm: 10.367398343674209
Epoch: 9311, Batch Gradient Norm after: 10.367398343674209
Epoch 9312/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3648596823215485
Epoch: 9312, Batch Gradient Norm: 10.834195349982718
Epoch: 9312, Batch Gradient Norm after: 10.834195349982718
Epoch 9313/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.36871266961097715
Epoch: 9313, Batch Gradient Norm: 11.39992847549302
Epoch: 9313, Batch Gradient Norm after: 11.39992847549302
Epoch 9314/10000, Prediction Accuracy = 63.35%, Loss = 0.3716914176940918
Epoch: 9314, Batch Gradient Norm: 12.140403386110632
Epoch: 9314, Batch Gradient Norm after: 12.140403386110632
Epoch 9315/10000, Prediction Accuracy = 63.45400000000001%, Loss = 0.37727504372596743
Epoch: 9315, Batch Gradient Norm: 10.093061091076713
Epoch: 9315, Batch Gradient Norm after: 10.093061091076713
Epoch 9316/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.3669634938240051
Epoch: 9316, Batch Gradient Norm: 14.759196041777638
Epoch: 9316, Batch Gradient Norm after: 14.759196041777638
Epoch 9317/10000, Prediction Accuracy = 63.576%, Loss = 0.41861780285835265
Epoch: 9317, Batch Gradient Norm: 8.158213938822271
Epoch: 9317, Batch Gradient Norm after: 8.158213938822271
Epoch 9318/10000, Prediction Accuracy = 63.464%, Loss = 0.3578480124473572
Epoch: 9318, Batch Gradient Norm: 7.125797558159753
Epoch: 9318, Batch Gradient Norm after: 7.125797558159753
Epoch 9319/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3472152233123779
Epoch: 9319, Batch Gradient Norm: 7.206999322725337
Epoch: 9319, Batch Gradient Norm after: 7.206999322725337
Epoch 9320/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.3481204330921173
Epoch: 9320, Batch Gradient Norm: 10.77276389149157
Epoch: 9320, Batch Gradient Norm after: 10.77276389149157
Epoch 9321/10000, Prediction Accuracy = 63.443999999999996%, Loss = 0.3718813478946686
Epoch: 9321, Batch Gradient Norm: 12.846102689248955
Epoch: 9321, Batch Gradient Norm after: 12.846102689248955
Epoch 9322/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.39068124890327455
Epoch: 9322, Batch Gradient Norm: 10.803441455047658
Epoch: 9322, Batch Gradient Norm after: 10.803441455047658
Epoch 9323/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.36908861994743347
Epoch: 9323, Batch Gradient Norm: 9.753550552611348
Epoch: 9323, Batch Gradient Norm after: 9.753550552611348
Epoch 9324/10000, Prediction Accuracy = 63.519999999999996%, Loss = 0.36058574318885805
Epoch: 9324, Batch Gradient Norm: 7.435371251959087
Epoch: 9324, Batch Gradient Norm after: 7.435371251959087
Epoch 9325/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.3481714129447937
Epoch: 9325, Batch Gradient Norm: 9.146798886998704
Epoch: 9325, Batch Gradient Norm after: 9.146798886998704
Epoch 9326/10000, Prediction Accuracy = 63.652%, Loss = 0.35710001587867735
Epoch: 9326, Batch Gradient Norm: 11.237877910774559
Epoch: 9326, Batch Gradient Norm after: 11.237877910774559
Epoch 9327/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.37009766697883606
Epoch: 9327, Batch Gradient Norm: 8.490740217419539
Epoch: 9327, Batch Gradient Norm after: 8.490740217419539
Epoch 9328/10000, Prediction Accuracy = 63.706%, Loss = 0.35228644609451293
Epoch: 9328, Batch Gradient Norm: 8.097173246036272
Epoch: 9328, Batch Gradient Norm after: 8.097173246036272
Epoch 9329/10000, Prediction Accuracy = 63.698%, Loss = 0.3525915026664734
Epoch: 9329, Batch Gradient Norm: 8.007780748213495
Epoch: 9329, Batch Gradient Norm after: 8.007780748213495
Epoch 9330/10000, Prediction Accuracy = 63.462%, Loss = 0.3491119146347046
Epoch: 9330, Batch Gradient Norm: 9.492329401385728
Epoch: 9330, Batch Gradient Norm after: 9.492329401385728
Epoch 9331/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.3582570731639862
Epoch: 9331, Batch Gradient Norm: 8.319346884969407
Epoch: 9331, Batch Gradient Norm after: 8.319346884969407
Epoch 9332/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.3529787242412567
Epoch: 9332, Batch Gradient Norm: 9.752813470638918
Epoch: 9332, Batch Gradient Norm after: 9.752813470638918
Epoch 9333/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.36197301745414734
Epoch: 9333, Batch Gradient Norm: 11.813077336840639
Epoch: 9333, Batch Gradient Norm after: 11.813077336840639
Epoch 9334/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.37813984751701357
Epoch: 9334, Batch Gradient Norm: 11.424799111116759
Epoch: 9334, Batch Gradient Norm after: 11.424799111116759
Epoch 9335/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3769322633743286
Epoch: 9335, Batch Gradient Norm: 9.32916423920129
Epoch: 9335, Batch Gradient Norm after: 9.32916423920129
Epoch 9336/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.36084259748458863
Epoch: 9336, Batch Gradient Norm: 7.792146788904843
Epoch: 9336, Batch Gradient Norm after: 7.792146788904843
Epoch 9337/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.35010979175567625
Epoch: 9337, Batch Gradient Norm: 10.502728381402582
Epoch: 9337, Batch Gradient Norm after: 10.502728381402582
Epoch 9338/10000, Prediction Accuracy = 63.612%, Loss = 0.3696712672710419
Epoch: 9338, Batch Gradient Norm: 8.765476451657465
Epoch: 9338, Batch Gradient Norm after: 8.765476451657465
Epoch 9339/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3591249525547028
Epoch: 9339, Batch Gradient Norm: 8.667913751810511
Epoch: 9339, Batch Gradient Norm after: 8.667913751810511
Epoch 9340/10000, Prediction Accuracy = 63.496%, Loss = 0.35497077107429503
Epoch: 9340, Batch Gradient Norm: 13.493811374418975
Epoch: 9340, Batch Gradient Norm after: 13.493811374418975
Epoch 9341/10000, Prediction Accuracy = 63.528%, Loss = 0.39116020798683165
Epoch: 9341, Batch Gradient Norm: 10.816651345557387
Epoch: 9341, Batch Gradient Norm after: 10.816651345557387
Epoch 9342/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.36874141097068786
Epoch: 9342, Batch Gradient Norm: 8.607587661859077
Epoch: 9342, Batch Gradient Norm after: 8.607587661859077
Epoch 9343/10000, Prediction Accuracy = 63.568%, Loss = 0.35527762174606325
Epoch: 9343, Batch Gradient Norm: 9.375079767357724
Epoch: 9343, Batch Gradient Norm after: 9.375079767357724
Epoch 9344/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.3622086763381958
Epoch: 9344, Batch Gradient Norm: 9.435876407086345
Epoch: 9344, Batch Gradient Norm after: 9.435876407086345
Epoch 9345/10000, Prediction Accuracy = 63.738%, Loss = 0.3617684543132782
Epoch: 9345, Batch Gradient Norm: 10.703449119323858
Epoch: 9345, Batch Gradient Norm after: 10.703449119323858
Epoch 9346/10000, Prediction Accuracy = 63.592%, Loss = 0.36573336124420164
Epoch: 9346, Batch Gradient Norm: 9.677387053623898
Epoch: 9346, Batch Gradient Norm after: 9.677387053623898
Epoch 9347/10000, Prediction Accuracy = 63.61%, Loss = 0.36121485829353334
Epoch: 9347, Batch Gradient Norm: 8.892034640070696
Epoch: 9347, Batch Gradient Norm after: 8.892034640070696
Epoch 9348/10000, Prediction Accuracy = 63.688%, Loss = 0.35685524344444275
Epoch: 9348, Batch Gradient Norm: 13.80184532530734
Epoch: 9348, Batch Gradient Norm after: 13.80184532530734
Epoch 9349/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3965752124786377
Epoch: 9349, Batch Gradient Norm: 10.200641057241665
Epoch: 9349, Batch Gradient Norm after: 10.200641057241665
Epoch 9350/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.366606342792511
Epoch: 9350, Batch Gradient Norm: 10.13636965421714
Epoch: 9350, Batch Gradient Norm after: 10.13636965421714
Epoch 9351/10000, Prediction Accuracy = 63.572%, Loss = 0.36588732004165647
Epoch: 9351, Batch Gradient Norm: 9.656282645364321
Epoch: 9351, Batch Gradient Norm after: 9.656282645364321
Epoch 9352/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3650906026363373
Epoch: 9352, Batch Gradient Norm: 11.004259991911972
Epoch: 9352, Batch Gradient Norm after: 11.004259991911972
Epoch 9353/10000, Prediction Accuracy = 63.592%, Loss = 0.3702743172645569
Epoch: 9353, Batch Gradient Norm: 12.138726128528731
Epoch: 9353, Batch Gradient Norm after: 12.138726128528731
Epoch 9354/10000, Prediction Accuracy = 63.406000000000006%, Loss = 0.3782499849796295
Epoch: 9354, Batch Gradient Norm: 8.067287172754602
Epoch: 9354, Batch Gradient Norm after: 8.067287172754602
Epoch 9355/10000, Prediction Accuracy = 63.666%, Loss = 0.3518014430999756
Epoch: 9355, Batch Gradient Norm: 8.465284523195825
Epoch: 9355, Batch Gradient Norm after: 8.465284523195825
Epoch 9356/10000, Prediction Accuracy = 63.786%, Loss = 0.3554128766059875
Epoch: 9356, Batch Gradient Norm: 7.476979652443572
Epoch: 9356, Batch Gradient Norm after: 7.476979652443572
Epoch 9357/10000, Prediction Accuracy = 63.712%, Loss = 0.3472469449043274
Epoch: 9357, Batch Gradient Norm: 9.924711306517263
Epoch: 9357, Batch Gradient Norm after: 9.924711306517263
Epoch 9358/10000, Prediction Accuracy = 63.584%, Loss = 0.35994378924369813
Epoch: 9358, Batch Gradient Norm: 12.527069296758375
Epoch: 9358, Batch Gradient Norm after: 12.527069296758375
Epoch 9359/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.3824932396411896
Epoch: 9359, Batch Gradient Norm: 13.357088726608497
Epoch: 9359, Batch Gradient Norm after: 13.357088726608497
Epoch 9360/10000, Prediction Accuracy = 63.56%, Loss = 0.3908109188079834
Epoch: 9360, Batch Gradient Norm: 8.167906562891439
Epoch: 9360, Batch Gradient Norm after: 8.167906562891439
Epoch 9361/10000, Prediction Accuracy = 63.734%, Loss = 0.3549982726573944
Epoch: 9361, Batch Gradient Norm: 7.545134853527246
Epoch: 9361, Batch Gradient Norm after: 7.545134853527246
Epoch 9362/10000, Prediction Accuracy = 63.45%, Loss = 0.3506723642349243
Epoch: 9362, Batch Gradient Norm: 7.727954131042757
Epoch: 9362, Batch Gradient Norm after: 7.727954131042757
Epoch 9363/10000, Prediction Accuracy = 63.638%, Loss = 0.3511967957019806
Epoch: 9363, Batch Gradient Norm: 8.16700064833604
Epoch: 9363, Batch Gradient Norm after: 8.16700064833604
Epoch 9364/10000, Prediction Accuracy = 63.664%, Loss = 0.3504518926143646
Epoch: 9364, Batch Gradient Norm: 9.316528662430873
Epoch: 9364, Batch Gradient Norm after: 9.316528662430873
Epoch 9365/10000, Prediction Accuracy = 63.56%, Loss = 0.35805649757385255
Epoch: 9365, Batch Gradient Norm: 9.300667250164151
Epoch: 9365, Batch Gradient Norm after: 9.300667250164151
Epoch 9366/10000, Prediction Accuracy = 63.444%, Loss = 0.35556933283805847
Epoch: 9366, Batch Gradient Norm: 11.087900928122187
Epoch: 9366, Batch Gradient Norm after: 11.087900928122187
Epoch 9367/10000, Prediction Accuracy = 63.508%, Loss = 0.3692641019821167
Epoch: 9367, Batch Gradient Norm: 10.278795097770638
Epoch: 9367, Batch Gradient Norm after: 10.278795097770638
Epoch 9368/10000, Prediction Accuracy = 63.556%, Loss = 0.3652860462665558
Epoch: 9368, Batch Gradient Norm: 9.424292930041323
Epoch: 9368, Batch Gradient Norm after: 9.424292930041323
Epoch 9369/10000, Prediction Accuracy = 63.314%, Loss = 0.36135756969451904
Epoch: 9369, Batch Gradient Norm: 8.692321794317873
Epoch: 9369, Batch Gradient Norm after: 8.692321794317873
Epoch 9370/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.35577951073646547
Epoch: 9370, Batch Gradient Norm: 9.55102841345078
Epoch: 9370, Batch Gradient Norm after: 9.55102841345078
Epoch 9371/10000, Prediction Accuracy = 63.568%, Loss = 0.36316826939582825
Epoch: 9371, Batch Gradient Norm: 12.008314414945637
Epoch: 9371, Batch Gradient Norm after: 12.008314414945637
Epoch 9372/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.3776956081390381
Epoch: 9372, Batch Gradient Norm: 11.339033829282155
Epoch: 9372, Batch Gradient Norm after: 11.339033829282155
Epoch 9373/10000, Prediction Accuracy = 63.672000000000004%, Loss = 0.3727643370628357
Epoch: 9373, Batch Gradient Norm: 9.70898529066043
Epoch: 9373, Batch Gradient Norm after: 9.70898529066043
Epoch 9374/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.3576362907886505
Epoch: 9374, Batch Gradient Norm: 10.279683090675821
Epoch: 9374, Batch Gradient Norm after: 10.279683090675821
Epoch 9375/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.36657471060752866
Epoch: 9375, Batch Gradient Norm: 9.456480079389431
Epoch: 9375, Batch Gradient Norm after: 9.456480079389431
Epoch 9376/10000, Prediction Accuracy = 63.63000000000001%, Loss = 0.3573215663433075
Epoch: 9376, Batch Gradient Norm: 11.56125922441339
Epoch: 9376, Batch Gradient Norm after: 11.56125922441339
Epoch 9377/10000, Prediction Accuracy = 63.746%, Loss = 0.3717909693717957
Epoch: 9377, Batch Gradient Norm: 10.645584177336595
Epoch: 9377, Batch Gradient Norm after: 10.645584177336595
Epoch 9378/10000, Prediction Accuracy = 63.54%, Loss = 0.3686722218990326
Epoch: 9378, Batch Gradient Norm: 9.768023220929708
Epoch: 9378, Batch Gradient Norm after: 9.768023220929708
Epoch 9379/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.36281291842460633
Epoch: 9379, Batch Gradient Norm: 10.027384881609562
Epoch: 9379, Batch Gradient Norm after: 10.027384881609562
Epoch 9380/10000, Prediction Accuracy = 63.431999999999995%, Loss = 0.3641503632068634
Epoch: 9380, Batch Gradient Norm: 10.492677995493267
Epoch: 9380, Batch Gradient Norm after: 10.492677995493267
Epoch 9381/10000, Prediction Accuracy = 63.438%, Loss = 0.3681079626083374
Epoch: 9381, Batch Gradient Norm: 8.946000786111995
Epoch: 9381, Batch Gradient Norm after: 8.946000786111995
Epoch 9382/10000, Prediction Accuracy = 63.69200000000001%, Loss = 0.3564557909965515
Epoch: 9382, Batch Gradient Norm: 8.950973577944179
Epoch: 9382, Batch Gradient Norm after: 8.950973577944179
Epoch 9383/10000, Prediction Accuracy = 63.712%, Loss = 0.35381171107292175
Epoch: 9383, Batch Gradient Norm: 10.072470311655543
Epoch: 9383, Batch Gradient Norm after: 10.072470311655543
Epoch 9384/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.36563615798950194
Epoch: 9384, Batch Gradient Norm: 9.294876565063046
Epoch: 9384, Batch Gradient Norm after: 9.294876565063046
Epoch 9385/10000, Prediction Accuracy = 63.648%, Loss = 0.35857740640640257
Epoch: 9385, Batch Gradient Norm: 9.69347554956102
Epoch: 9385, Batch Gradient Norm after: 9.69347554956102
Epoch 9386/10000, Prediction Accuracy = 63.714%, Loss = 0.3594108700752258
Epoch: 9386, Batch Gradient Norm: 9.201689071985195
Epoch: 9386, Batch Gradient Norm after: 9.201689071985195
Epoch 9387/10000, Prediction Accuracy = 63.568000000000005%, Loss = 0.35876771807670593
Epoch: 9387, Batch Gradient Norm: 7.908434615890608
Epoch: 9387, Batch Gradient Norm after: 7.908434615890608
Epoch 9388/10000, Prediction Accuracy = 63.775999999999996%, Loss = 0.3530487596988678
Epoch: 9388, Batch Gradient Norm: 11.176655967999613
Epoch: 9388, Batch Gradient Norm after: 11.176655967999613
Epoch 9389/10000, Prediction Accuracy = 63.488%, Loss = 0.37142372131347656
Epoch: 9389, Batch Gradient Norm: 10.250551461366518
Epoch: 9389, Batch Gradient Norm after: 10.250551461366518
Epoch 9390/10000, Prediction Accuracy = 63.64200000000001%, Loss = 0.3637278139591217
Epoch: 9390, Batch Gradient Norm: 10.214910907024658
Epoch: 9390, Batch Gradient Norm after: 10.214910907024658
Epoch 9391/10000, Prediction Accuracy = 63.63599999999999%, Loss = 0.3668395638465881
Epoch: 9391, Batch Gradient Norm: 10.338293201394675
Epoch: 9391, Batch Gradient Norm after: 10.338293201394675
Epoch 9392/10000, Prediction Accuracy = 63.556%, Loss = 0.3683837831020355
Epoch: 9392, Batch Gradient Norm: 10.832811921570007
Epoch: 9392, Batch Gradient Norm after: 10.832811921570007
Epoch 9393/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.37009757161140444
Epoch: 9393, Batch Gradient Norm: 9.834650717929156
Epoch: 9393, Batch Gradient Norm after: 9.834650717929156
Epoch 9394/10000, Prediction Accuracy = 63.634%, Loss = 0.3617862522602081
Epoch: 9394, Batch Gradient Norm: 8.556240276715487
Epoch: 9394, Batch Gradient Norm after: 8.556240276715487
Epoch 9395/10000, Prediction Accuracy = 63.77%, Loss = 0.3513366162776947
Epoch: 9395, Batch Gradient Norm: 10.135845142983046
Epoch: 9395, Batch Gradient Norm after: 10.135845142983046
Epoch 9396/10000, Prediction Accuracy = 63.624%, Loss = 0.36491583585739135
Epoch: 9396, Batch Gradient Norm: 8.221467584203696
Epoch: 9396, Batch Gradient Norm after: 8.221467584203696
Epoch 9397/10000, Prediction Accuracy = 63.426%, Loss = 0.35267744660377504
Epoch: 9397, Batch Gradient Norm: 9.719074856433478
Epoch: 9397, Batch Gradient Norm after: 9.719074856433478
Epoch 9398/10000, Prediction Accuracy = 63.71600000000001%, Loss = 0.3592032790184021
Epoch: 9398, Batch Gradient Norm: 11.821643339564499
Epoch: 9398, Batch Gradient Norm after: 11.821643339564499
Epoch 9399/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.3787041246891022
Epoch: 9399, Batch Gradient Norm: 12.317105744108334
Epoch: 9399, Batch Gradient Norm after: 12.317105744108334
Epoch 9400/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.38473016023635864
Epoch: 9400, Batch Gradient Norm: 8.270053152415391
Epoch: 9400, Batch Gradient Norm after: 8.270053152415391
Epoch 9401/10000, Prediction Accuracy = 63.54%, Loss = 0.35404711961746216
Epoch: 9401, Batch Gradient Norm: 12.523716381855714
Epoch: 9401, Batch Gradient Norm after: 12.523716381855714
Epoch 9402/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.3820961117744446
Epoch: 9402, Batch Gradient Norm: 12.422257410587168
Epoch: 9402, Batch Gradient Norm after: 12.422257410587168
Epoch 9403/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3808786332607269
Epoch: 9403, Batch Gradient Norm: 7.973285636568996
Epoch: 9403, Batch Gradient Norm after: 7.973285636568996
Epoch 9404/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.34890618920326233
Epoch: 9404, Batch Gradient Norm: 7.23240325135292
Epoch: 9404, Batch Gradient Norm after: 7.23240325135292
Epoch 9405/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.3459711134433746
Epoch: 9405, Batch Gradient Norm: 8.279815169674936
Epoch: 9405, Batch Gradient Norm after: 8.279815169674936
Epoch 9406/10000, Prediction Accuracy = 63.624%, Loss = 0.35530844926834104
Epoch: 9406, Batch Gradient Norm: 12.369266536460318
Epoch: 9406, Batch Gradient Norm after: 12.369266536460318
Epoch 9407/10000, Prediction Accuracy = 63.63399999999999%, Loss = 0.38024646043777466
Epoch: 9407, Batch Gradient Norm: 12.26473632573787
Epoch: 9407, Batch Gradient Norm after: 12.26473632573787
Epoch 9408/10000, Prediction Accuracy = 63.648%, Loss = 0.3825449526309967
Epoch: 9408, Batch Gradient Norm: 9.094406752802758
Epoch: 9408, Batch Gradient Norm after: 9.094406752802758
Epoch 9409/10000, Prediction Accuracy = 63.544%, Loss = 0.35780370235443115
Epoch: 9409, Batch Gradient Norm: 6.710977356691678
Epoch: 9409, Batch Gradient Norm after: 6.710977356691678
Epoch 9410/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.3441697537899017
Epoch: 9410, Batch Gradient Norm: 8.04037261428894
Epoch: 9410, Batch Gradient Norm after: 8.04037261428894
Epoch 9411/10000, Prediction Accuracy = 63.536%, Loss = 0.3494177103042603
Epoch: 9411, Batch Gradient Norm: 11.725281479217326
Epoch: 9411, Batch Gradient Norm after: 11.725281479217326
Epoch 9412/10000, Prediction Accuracy = 63.42%, Loss = 0.37267683148384095
Epoch: 9412, Batch Gradient Norm: 10.56310770229165
Epoch: 9412, Batch Gradient Norm after: 10.56310770229165
Epoch 9413/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.36744955778121946
Epoch: 9413, Batch Gradient Norm: 9.319125094327733
Epoch: 9413, Batch Gradient Norm after: 9.319125094327733
Epoch 9414/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.36022173166275023
Epoch: 9414, Batch Gradient Norm: 8.467350430884819
Epoch: 9414, Batch Gradient Norm after: 8.467350430884819
Epoch 9415/10000, Prediction Accuracy = 63.718%, Loss = 0.35244138836860656
Epoch: 9415, Batch Gradient Norm: 11.041172601549704
Epoch: 9415, Batch Gradient Norm after: 11.041172601549704
Epoch 9416/10000, Prediction Accuracy = 63.61200000000001%, Loss = 0.3661098301410675
Epoch: 9416, Batch Gradient Norm: 10.762371984583105
Epoch: 9416, Batch Gradient Norm after: 10.762371984583105
Epoch 9417/10000, Prediction Accuracy = 63.636%, Loss = 0.369150710105896
Epoch: 9417, Batch Gradient Norm: 10.129159250348998
Epoch: 9417, Batch Gradient Norm after: 10.129159250348998
Epoch 9418/10000, Prediction Accuracy = 63.598%, Loss = 0.3638619124889374
Epoch: 9418, Batch Gradient Norm: 8.310696905306127
Epoch: 9418, Batch Gradient Norm after: 8.310696905306127
Epoch 9419/10000, Prediction Accuracy = 63.586%, Loss = 0.35282598733901976
Epoch: 9419, Batch Gradient Norm: 8.966707948773907
Epoch: 9419, Batch Gradient Norm after: 8.966707948773907
Epoch 9420/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.3580477237701416
Epoch: 9420, Batch Gradient Norm: 9.353745224708778
Epoch: 9420, Batch Gradient Norm after: 9.353745224708778
Epoch 9421/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.35913140177726743
Epoch: 9421, Batch Gradient Norm: 9.66814747474665
Epoch: 9421, Batch Gradient Norm after: 9.66814747474665
Epoch 9422/10000, Prediction Accuracy = 63.56%, Loss = 0.36324127912521365
Epoch: 9422, Batch Gradient Norm: 10.877208431657566
Epoch: 9422, Batch Gradient Norm after: 10.877208431657566
Epoch 9423/10000, Prediction Accuracy = 63.402%, Loss = 0.37070552706718446
Epoch: 9423, Batch Gradient Norm: 9.721005064587493
Epoch: 9423, Batch Gradient Norm after: 9.721005064587493
Epoch 9424/10000, Prediction Accuracy = 63.616%, Loss = 0.36111598610877993
Epoch: 9424, Batch Gradient Norm: 9.245848059464425
Epoch: 9424, Batch Gradient Norm after: 9.245848059464425
Epoch 9425/10000, Prediction Accuracy = 63.55%, Loss = 0.35767617225646975
Epoch: 9425, Batch Gradient Norm: 9.817519060958988
Epoch: 9425, Batch Gradient Norm after: 9.817519060958988
Epoch 9426/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.36364460587501524
Epoch: 9426, Batch Gradient Norm: 9.129647007424742
Epoch: 9426, Batch Gradient Norm after: 9.129647007424742
Epoch 9427/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.35661420226097107
Epoch: 9427, Batch Gradient Norm: 10.069777085091973
Epoch: 9427, Batch Gradient Norm after: 10.069777085091973
Epoch 9428/10000, Prediction Accuracy = 63.678%, Loss = 0.36126229763031004
Epoch: 9428, Batch Gradient Norm: 12.15775337706451
Epoch: 9428, Batch Gradient Norm after: 12.15775337706451
Epoch 9429/10000, Prediction Accuracy = 63.634%, Loss = 0.377448707818985
Epoch: 9429, Batch Gradient Norm: 9.820375555867699
Epoch: 9429, Batch Gradient Norm after: 9.820375555867699
Epoch 9430/10000, Prediction Accuracy = 63.648%, Loss = 0.35881938338279723
Epoch: 9430, Batch Gradient Norm: 9.02644463177792
Epoch: 9430, Batch Gradient Norm after: 9.02644463177792
Epoch 9431/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3590387463569641
Epoch: 9431, Batch Gradient Norm: 8.816018564765866
Epoch: 9431, Batch Gradient Norm after: 8.816018564765866
Epoch 9432/10000, Prediction Accuracy = 63.48%, Loss = 0.3534555733203888
Epoch: 9432, Batch Gradient Norm: 11.819494355696957
Epoch: 9432, Batch Gradient Norm after: 11.819494355696957
Epoch 9433/10000, Prediction Accuracy = 63.374%, Loss = 0.373350989818573
Epoch: 9433, Batch Gradient Norm: 12.230377185805233
Epoch: 9433, Batch Gradient Norm after: 12.230377185805233
Epoch 9434/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3819900631904602
Epoch: 9434, Batch Gradient Norm: 9.903768858053278
Epoch: 9434, Batch Gradient Norm after: 9.903768858053278
Epoch 9435/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.3619194507598877
Epoch: 9435, Batch Gradient Norm: 8.04330872979512
Epoch: 9435, Batch Gradient Norm after: 8.04330872979512
Epoch 9436/10000, Prediction Accuracy = 63.714%, Loss = 0.3522976815700531
Epoch: 9436, Batch Gradient Norm: 8.851948768389846
Epoch: 9436, Batch Gradient Norm after: 8.851948768389846
Epoch 9437/10000, Prediction Accuracy = 63.60600000000001%, Loss = 0.3564608097076416
Epoch: 9437, Batch Gradient Norm: 7.232716848210991
Epoch: 9437, Batch Gradient Norm after: 7.232716848210991
Epoch 9438/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3452422559261322
Epoch: 9438, Batch Gradient Norm: 9.777659774373017
Epoch: 9438, Batch Gradient Norm after: 9.777659774373017
Epoch 9439/10000, Prediction Accuracy = 63.588%, Loss = 0.3601628541946411
Epoch: 9439, Batch Gradient Norm: 9.93190355808556
Epoch: 9439, Batch Gradient Norm after: 9.93190355808556
Epoch 9440/10000, Prediction Accuracy = 63.824%, Loss = 0.3585988938808441
Epoch: 9440, Batch Gradient Norm: 11.638468536280566
Epoch: 9440, Batch Gradient Norm after: 11.638468536280566
Epoch 9441/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.37765671610832213
Epoch: 9441, Batch Gradient Norm: 9.725233613689989
Epoch: 9441, Batch Gradient Norm after: 9.725233613689989
Epoch 9442/10000, Prediction Accuracy = 63.598%, Loss = 0.36392276287078856
Epoch: 9442, Batch Gradient Norm: 8.126112724762084
Epoch: 9442, Batch Gradient Norm after: 8.126112724762084
Epoch 9443/10000, Prediction Accuracy = 63.742%, Loss = 0.3516044557094574
Epoch: 9443, Batch Gradient Norm: 9.08460882334673
Epoch: 9443, Batch Gradient Norm after: 9.08460882334673
Epoch 9444/10000, Prediction Accuracy = 63.64%, Loss = 0.3534531116485596
Epoch: 9444, Batch Gradient Norm: 8.98367043152557
Epoch: 9444, Batch Gradient Norm after: 8.98367043152557
Epoch 9445/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3550284385681152
Epoch: 9445, Batch Gradient Norm: 11.745290534173133
Epoch: 9445, Batch Gradient Norm after: 11.745290534173133
Epoch 9446/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.37546645998954775
Epoch: 9446, Batch Gradient Norm: 10.375448838904072
Epoch: 9446, Batch Gradient Norm after: 10.375448838904072
Epoch 9447/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.36498948335647585
Epoch: 9447, Batch Gradient Norm: 10.309443501451074
Epoch: 9447, Batch Gradient Norm after: 10.309443501451074
Epoch 9448/10000, Prediction Accuracy = 63.74400000000001%, Loss = 0.3628958463668823
Epoch: 9448, Batch Gradient Norm: 10.33130448732287
Epoch: 9448, Batch Gradient Norm after: 10.33130448732287
Epoch 9449/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.3633379340171814
Epoch: 9449, Batch Gradient Norm: 10.802143625328556
Epoch: 9449, Batch Gradient Norm after: 10.802143625328556
Epoch 9450/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.37336641550064087
Epoch: 9450, Batch Gradient Norm: 12.57960243745049
Epoch: 9450, Batch Gradient Norm after: 12.57960243745049
Epoch 9451/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.3868479549884796
Epoch: 9451, Batch Gradient Norm: 9.498767071390551
Epoch: 9451, Batch Gradient Norm after: 9.498767071390551
Epoch 9452/10000, Prediction Accuracy = 63.81%, Loss = 0.3588567733764648
Epoch: 9452, Batch Gradient Norm: 9.142084297510344
Epoch: 9452, Batch Gradient Norm after: 9.142084297510344
Epoch 9453/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.35247781276702883
Epoch: 9453, Batch Gradient Norm: 8.848314155971634
Epoch: 9453, Batch Gradient Norm after: 8.848314155971634
Epoch 9454/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.35457143783569334
Epoch: 9454, Batch Gradient Norm: 9.359043459617045
Epoch: 9454, Batch Gradient Norm after: 9.359043459617045
Epoch 9455/10000, Prediction Accuracy = 63.65%, Loss = 0.35992578864097596
Epoch: 9455, Batch Gradient Norm: 7.986779088188403
Epoch: 9455, Batch Gradient Norm after: 7.986779088188403
Epoch 9456/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.3482813835144043
Epoch: 9456, Batch Gradient Norm: 9.806803669269074
Epoch: 9456, Batch Gradient Norm after: 9.806803669269074
Epoch 9457/10000, Prediction Accuracy = 63.626%, Loss = 0.3627977132797241
Epoch: 9457, Batch Gradient Norm: 9.47596907626294
Epoch: 9457, Batch Gradient Norm after: 9.47596907626294
Epoch 9458/10000, Prediction Accuracy = 63.82800000000001%, Loss = 0.359295928478241
Epoch: 9458, Batch Gradient Norm: 8.95261301502516
Epoch: 9458, Batch Gradient Norm after: 8.95261301502516
Epoch 9459/10000, Prediction Accuracy = 63.516%, Loss = 0.35816805958747866
Epoch: 9459, Batch Gradient Norm: 10.21150765568432
Epoch: 9459, Batch Gradient Norm after: 10.21150765568432
Epoch 9460/10000, Prediction Accuracy = 63.416%, Loss = 0.36318680047988894
Epoch: 9460, Batch Gradient Norm: 12.517780097090132
Epoch: 9460, Batch Gradient Norm after: 12.517780097090132
Epoch 9461/10000, Prediction Accuracy = 63.507999999999996%, Loss = 0.3814324736595154
Epoch: 9461, Batch Gradient Norm: 12.44531359144148
Epoch: 9461, Batch Gradient Norm after: 12.44531359144148
Epoch 9462/10000, Prediction Accuracy = 63.492%, Loss = 0.3781864821910858
Epoch: 9462, Batch Gradient Norm: 9.094200751219244
Epoch: 9462, Batch Gradient Norm after: 9.094200751219244
Epoch 9463/10000, Prediction Accuracy = 63.686%, Loss = 0.3530629754066467
Epoch: 9463, Batch Gradient Norm: 8.4870850048403
Epoch: 9463, Batch Gradient Norm after: 8.4870850048403
Epoch 9464/10000, Prediction Accuracy = 63.678%, Loss = 0.35136566758155824
Epoch: 9464, Batch Gradient Norm: 10.545894382971833
Epoch: 9464, Batch Gradient Norm after: 10.545894382971833
Epoch 9465/10000, Prediction Accuracy = 63.681999999999995%, Loss = 0.3646997272968292
Epoch: 9465, Batch Gradient Norm: 10.541978915533354
Epoch: 9465, Batch Gradient Norm after: 10.541978915533354
Epoch 9466/10000, Prediction Accuracy = 63.412%, Loss = 0.3641064167022705
Epoch: 9466, Batch Gradient Norm: 10.887835569617332
Epoch: 9466, Batch Gradient Norm after: 10.887835569617332
Epoch 9467/10000, Prediction Accuracy = 63.726%, Loss = 0.3672304689884186
Epoch: 9467, Batch Gradient Norm: 11.615880322509852
Epoch: 9467, Batch Gradient Norm after: 11.615880322509852
Epoch 9468/10000, Prediction Accuracy = 63.806000000000004%, Loss = 0.37461860179901124
Epoch: 9468, Batch Gradient Norm: 8.389924825913413
Epoch: 9468, Batch Gradient Norm after: 8.389924825913413
Epoch 9469/10000, Prediction Accuracy = 63.726%, Loss = 0.3540332973003387
Epoch: 9469, Batch Gradient Norm: 10.06664826255682
Epoch: 9469, Batch Gradient Norm after: 10.06664826255682
Epoch 9470/10000, Prediction Accuracy = 63.67800000000001%, Loss = 0.3718410849571228
Epoch: 9470, Batch Gradient Norm: 7.527563187017434
Epoch: 9470, Batch Gradient Norm after: 7.527563187017434
Epoch 9471/10000, Prediction Accuracy = 63.63399999999999%, Loss = 0.34712973833084104
Epoch: 9471, Batch Gradient Norm: 10.644824023558014
Epoch: 9471, Batch Gradient Norm after: 10.644824023558014
Epoch 9472/10000, Prediction Accuracy = 63.626%, Loss = 0.36486564874649047
Epoch: 9472, Batch Gradient Norm: 9.945314404479676
Epoch: 9472, Batch Gradient Norm after: 9.945314404479676
Epoch 9473/10000, Prediction Accuracy = 63.593999999999994%, Loss = 0.36283243298530576
Epoch: 9473, Batch Gradient Norm: 8.672705994608645
Epoch: 9473, Batch Gradient Norm after: 8.672705994608645
Epoch 9474/10000, Prediction Accuracy = 63.802%, Loss = 0.3537402391433716
Epoch: 9474, Batch Gradient Norm: 8.988752530852057
Epoch: 9474, Batch Gradient Norm after: 8.988752530852057
Epoch 9475/10000, Prediction Accuracy = 63.524%, Loss = 0.3535871684551239
Epoch: 9475, Batch Gradient Norm: 10.417787865652164
Epoch: 9475, Batch Gradient Norm after: 10.417787865652164
Epoch 9476/10000, Prediction Accuracy = 63.528%, Loss = 0.3654907524585724
Epoch: 9476, Batch Gradient Norm: 8.824242788637255
Epoch: 9476, Batch Gradient Norm after: 8.824242788637255
Epoch 9477/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.35720043182373046
Epoch: 9477, Batch Gradient Norm: 7.71867654237578
Epoch: 9477, Batch Gradient Norm after: 7.71867654237578
Epoch 9478/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.3463632881641388
Epoch: 9478, Batch Gradient Norm: 12.315688711246947
Epoch: 9478, Batch Gradient Norm after: 12.315688711246947
Epoch 9479/10000, Prediction Accuracy = 63.438%, Loss = 0.37981664538383486
Epoch: 9479, Batch Gradient Norm: 10.190069305953557
Epoch: 9479, Batch Gradient Norm after: 10.190069305953557
Epoch 9480/10000, Prediction Accuracy = 63.624%, Loss = 0.3646388649940491
Epoch: 9480, Batch Gradient Norm: 10.631004870706912
Epoch: 9480, Batch Gradient Norm after: 10.631004870706912
Epoch 9481/10000, Prediction Accuracy = 63.734%, Loss = 0.3653765022754669
Epoch: 9481, Batch Gradient Norm: 8.861572517348856
Epoch: 9481, Batch Gradient Norm after: 8.861572517348856
Epoch 9482/10000, Prediction Accuracy = 63.75600000000001%, Loss = 0.35406015515327455
Epoch: 9482, Batch Gradient Norm: 8.211232177947112
Epoch: 9482, Batch Gradient Norm after: 8.211232177947112
Epoch 9483/10000, Prediction Accuracy = 63.678%, Loss = 0.34944490194320676
Epoch: 9483, Batch Gradient Norm: 11.628562137065892
Epoch: 9483, Batch Gradient Norm after: 11.628562137065892
Epoch 9484/10000, Prediction Accuracy = 63.656000000000006%, Loss = 0.3720636308193207
Epoch: 9484, Batch Gradient Norm: 11.035366952173396
Epoch: 9484, Batch Gradient Norm after: 11.035366952173396
Epoch 9485/10000, Prediction Accuracy = 63.65%, Loss = 0.3698392748832703
Epoch: 9485, Batch Gradient Norm: 10.25284507020677
Epoch: 9485, Batch Gradient Norm after: 10.25284507020677
Epoch 9486/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.36249061226844786
Epoch: 9486, Batch Gradient Norm: 10.600419265474258
Epoch: 9486, Batch Gradient Norm after: 10.600419265474258
Epoch 9487/10000, Prediction Accuracy = 63.646%, Loss = 0.3670696258544922
Epoch: 9487, Batch Gradient Norm: 8.709745999569327
Epoch: 9487, Batch Gradient Norm after: 8.709745999569327
Epoch 9488/10000, Prediction Accuracy = 63.782000000000004%, Loss = 0.35186508297920227
Epoch: 9488, Batch Gradient Norm: 10.219878179611381
Epoch: 9488, Batch Gradient Norm after: 10.219878179611381
Epoch 9489/10000, Prediction Accuracy = 63.634%, Loss = 0.36148618459701537
Epoch: 9489, Batch Gradient Norm: 10.543619633988254
Epoch: 9489, Batch Gradient Norm after: 10.543619633988254
Epoch 9490/10000, Prediction Accuracy = 63.58%, Loss = 0.36608638167381286
Epoch: 9490, Batch Gradient Norm: 10.322501896031305
Epoch: 9490, Batch Gradient Norm after: 10.322501896031305
Epoch 9491/10000, Prediction Accuracy = 63.652%, Loss = 0.3658895015716553
Epoch: 9491, Batch Gradient Norm: 7.789054746875534
Epoch: 9491, Batch Gradient Norm after: 7.789054746875534
Epoch 9492/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.3456539213657379
Epoch: 9492, Batch Gradient Norm: 9.548963733086916
Epoch: 9492, Batch Gradient Norm after: 9.548963733086916
Epoch 9493/10000, Prediction Accuracy = 63.855999999999995%, Loss = 0.35574175119400026
Epoch: 9493, Batch Gradient Norm: 9.001670239453931
Epoch: 9493, Batch Gradient Norm after: 9.001670239453931
Epoch 9494/10000, Prediction Accuracy = 63.572%, Loss = 0.3556215286254883
Epoch: 9494, Batch Gradient Norm: 8.936024226648332
Epoch: 9494, Batch Gradient Norm after: 8.936024226648332
Epoch 9495/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.3542191445827484
Epoch: 9495, Batch Gradient Norm: 9.194234218512925
Epoch: 9495, Batch Gradient Norm after: 9.194234218512925
Epoch 9496/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35730292797088625
Epoch: 9496, Batch Gradient Norm: 9.804671432653496
Epoch: 9496, Batch Gradient Norm after: 9.804671432653496
Epoch 9497/10000, Prediction Accuracy = 63.65%, Loss = 0.3607639014720917
Epoch: 9497, Batch Gradient Norm: 11.07351874677785
Epoch: 9497, Batch Gradient Norm after: 11.07351874677785
Epoch 9498/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.37352223992347716
Epoch: 9498, Batch Gradient Norm: 12.620124079771637
Epoch: 9498, Batch Gradient Norm after: 12.620124079771637
Epoch 9499/10000, Prediction Accuracy = 63.652%, Loss = 0.3828582942485809
Epoch: 9499, Batch Gradient Norm: 11.749193115241278
Epoch: 9499, Batch Gradient Norm after: 11.749193115241278
Epoch 9500/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.3763992369174957
Epoch: 9500, Batch Gradient Norm: 8.199851784459387
Epoch: 9500, Batch Gradient Norm after: 8.199851784459387
Epoch 9501/10000, Prediction Accuracy = 63.664%, Loss = 0.34945220947265626
Epoch: 9501, Batch Gradient Norm: 9.501037711414742
Epoch: 9501, Batch Gradient Norm after: 9.501037711414742
Epoch 9502/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.3553380250930786
Epoch: 9502, Batch Gradient Norm: 10.672501608666838
Epoch: 9502, Batch Gradient Norm after: 10.672501608666838
Epoch 9503/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.3686173379421234
Epoch: 9503, Batch Gradient Norm: 11.068081490840644
Epoch: 9503, Batch Gradient Norm after: 11.068081490840644
Epoch 9504/10000, Prediction Accuracy = 63.39%, Loss = 0.37155171632766726
Epoch: 9504, Batch Gradient Norm: 9.076568054058336
Epoch: 9504, Batch Gradient Norm after: 9.076568054058336
Epoch 9505/10000, Prediction Accuracy = 63.864%, Loss = 0.35362529158592226
Epoch: 9505, Batch Gradient Norm: 8.453095687169647
Epoch: 9505, Batch Gradient Norm after: 8.453095687169647
Epoch 9506/10000, Prediction Accuracy = 63.762000000000015%, Loss = 0.3515113711357117
Epoch: 9506, Batch Gradient Norm: 9.184254268141187
Epoch: 9506, Batch Gradient Norm after: 9.184254268141187
Epoch 9507/10000, Prediction Accuracy = 63.898%, Loss = 0.3545611143112183
Epoch: 9507, Batch Gradient Norm: 9.2989998443887
Epoch: 9507, Batch Gradient Norm after: 9.2989998443887
Epoch 9508/10000, Prediction Accuracy = 63.822%, Loss = 0.3532635152339935
Epoch: 9508, Batch Gradient Norm: 9.167248228754127
Epoch: 9508, Batch Gradient Norm after: 9.167248228754127
Epoch 9509/10000, Prediction Accuracy = 63.767999999999994%, Loss = 0.35804352164268494
Epoch: 9509, Batch Gradient Norm: 8.628548371537784
Epoch: 9509, Batch Gradient Norm after: 8.628548371537784
Epoch 9510/10000, Prediction Accuracy = 63.688%, Loss = 0.3545904040336609
Epoch: 9510, Batch Gradient Norm: 11.012525445149086
Epoch: 9510, Batch Gradient Norm after: 11.012525445149086
Epoch 9511/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.36700910329818726
Epoch: 9511, Batch Gradient Norm: 10.415691649057287
Epoch: 9511, Batch Gradient Norm after: 10.415691649057287
Epoch 9512/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.36452781558036806
Epoch: 9512, Batch Gradient Norm: 7.935886552017441
Epoch: 9512, Batch Gradient Norm after: 7.935886552017441
Epoch 9513/10000, Prediction Accuracy = 63.818%, Loss = 0.34738094806671144
Epoch: 9513, Batch Gradient Norm: 10.372184975914003
Epoch: 9513, Batch Gradient Norm after: 10.372184975914003
Epoch 9514/10000, Prediction Accuracy = 63.75600000000001%, Loss = 0.365725576877594
Epoch: 9514, Batch Gradient Norm: 12.381315194909272
Epoch: 9514, Batch Gradient Norm after: 12.381315194909272
Epoch 9515/10000, Prediction Accuracy = 63.622%, Loss = 0.377592945098877
Epoch: 9515, Batch Gradient Norm: 11.796509576768765
Epoch: 9515, Batch Gradient Norm after: 11.796509576768765
Epoch 9516/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.37196710109710696
Epoch: 9516, Batch Gradient Norm: 8.377365457051626
Epoch: 9516, Batch Gradient Norm after: 8.377365457051626
Epoch 9517/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.3486889719963074
Epoch: 9517, Batch Gradient Norm: 7.56996703004441
Epoch: 9517, Batch Gradient Norm after: 7.56996703004441
Epoch 9518/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.3460796236991882
Epoch: 9518, Batch Gradient Norm: 9.990316918243161
Epoch: 9518, Batch Gradient Norm after: 9.990316918243161
Epoch 9519/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.36251147389411925
Epoch: 9519, Batch Gradient Norm: 7.478693174054814
Epoch: 9519, Batch Gradient Norm after: 7.478693174054814
Epoch 9520/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35086650848388673
Epoch: 9520, Batch Gradient Norm: 13.509983439182944
Epoch: 9520, Batch Gradient Norm after: 13.509983439182944
Epoch 9521/10000, Prediction Accuracy = 63.496%, Loss = 0.3884024202823639
Epoch: 9521, Batch Gradient Norm: 11.59758288257903
Epoch: 9521, Batch Gradient Norm after: 11.59758288257903
Epoch 9522/10000, Prediction Accuracy = 63.66799999999999%, Loss = 0.37261711359024047
Epoch: 9522, Batch Gradient Norm: 8.803034139107652
Epoch: 9522, Batch Gradient Norm after: 8.803034139107652
Epoch 9523/10000, Prediction Accuracy = 63.812%, Loss = 0.35091206431388855
Epoch: 9523, Batch Gradient Norm: 10.513855907017748
Epoch: 9523, Batch Gradient Norm after: 10.513855907017748
Epoch 9524/10000, Prediction Accuracy = 63.59599999999999%, Loss = 0.36421146988868713
Epoch: 9524, Batch Gradient Norm: 9.874827261082096
Epoch: 9524, Batch Gradient Norm after: 9.874827261082096
Epoch 9525/10000, Prediction Accuracy = 63.686%, Loss = 0.3618101716041565
Epoch: 9525, Batch Gradient Norm: 9.618317663650837
Epoch: 9525, Batch Gradient Norm after: 9.618317663650837
Epoch 9526/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.3591269075870514
Epoch: 9526, Batch Gradient Norm: 8.224640813552426
Epoch: 9526, Batch Gradient Norm after: 8.224640813552426
Epoch 9527/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.34973855018615724
Epoch: 9527, Batch Gradient Norm: 9.26022816852428
Epoch: 9527, Batch Gradient Norm after: 9.26022816852428
Epoch 9528/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.35820955634117124
Epoch: 9528, Batch Gradient Norm: 10.727561666402405
Epoch: 9528, Batch Gradient Norm after: 10.727561666402405
Epoch 9529/10000, Prediction Accuracy = 63.646%, Loss = 0.37055711150169374
Epoch: 9529, Batch Gradient Norm: 9.001053228973188
Epoch: 9529, Batch Gradient Norm after: 9.001053228973188
Epoch 9530/10000, Prediction Accuracy = 63.35%, Loss = 0.35458402037620546
Epoch: 9530, Batch Gradient Norm: 10.178671862957186
Epoch: 9530, Batch Gradient Norm after: 10.178671862957186
Epoch 9531/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.3621411621570587
Epoch: 9531, Batch Gradient Norm: 13.242387700566203
Epoch: 9531, Batch Gradient Norm after: 13.242387700566203
Epoch 9532/10000, Prediction Accuracy = 63.588%, Loss = 0.39153255224227906
Epoch: 9532, Batch Gradient Norm: 9.387560623062834
Epoch: 9532, Batch Gradient Norm after: 9.387560623062834
Epoch 9533/10000, Prediction Accuracy = 63.760000000000005%, Loss = 0.36173760890960693
Epoch: 9533, Batch Gradient Norm: 7.461827267543157
Epoch: 9533, Batch Gradient Norm after: 7.461827267543157
Epoch 9534/10000, Prediction Accuracy = 63.746%, Loss = 0.34549718499183657
Epoch: 9534, Batch Gradient Norm: 8.895997111191003
Epoch: 9534, Batch Gradient Norm after: 8.895997111191003
Epoch 9535/10000, Prediction Accuracy = 63.754000000000005%, Loss = 0.3534837901592255
Epoch: 9535, Batch Gradient Norm: 8.858328533804245
Epoch: 9535, Batch Gradient Norm after: 8.858328533804245
Epoch 9536/10000, Prediction Accuracy = 63.818000000000005%, Loss = 0.35409685373306277
Epoch: 9536, Batch Gradient Norm: 10.493727863199716
Epoch: 9536, Batch Gradient Norm after: 10.493727863199716
Epoch 9537/10000, Prediction Accuracy = 63.70399999999999%, Loss = 0.36539784669876096
Epoch: 9537, Batch Gradient Norm: 9.67764782724693
Epoch: 9537, Batch Gradient Norm after: 9.67764782724693
Epoch 9538/10000, Prediction Accuracy = 63.662%, Loss = 0.35700615048408507
Epoch: 9538, Batch Gradient Norm: 10.077780631034555
Epoch: 9538, Batch Gradient Norm after: 10.077780631034555
Epoch 9539/10000, Prediction Accuracy = 63.83%, Loss = 0.3614646255970001
Epoch: 9539, Batch Gradient Norm: 13.047478536550178
Epoch: 9539, Batch Gradient Norm after: 13.047478536550178
Epoch 9540/10000, Prediction Accuracy = 63.528000000000006%, Loss = 0.3848985433578491
Epoch: 9540, Batch Gradient Norm: 10.164891975967477
Epoch: 9540, Batch Gradient Norm after: 10.164891975967477
Epoch 9541/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.36331288814544677
Epoch: 9541, Batch Gradient Norm: 8.473241148114154
Epoch: 9541, Batch Gradient Norm after: 8.473241148114154
Epoch 9542/10000, Prediction Accuracy = 63.71%, Loss = 0.3529193639755249
Epoch: 9542, Batch Gradient Norm: 9.612026449458012
Epoch: 9542, Batch Gradient Norm after: 9.612026449458012
Epoch 9543/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.35859241485595705
Epoch: 9543, Batch Gradient Norm: 10.050324582945724
Epoch: 9543, Batch Gradient Norm after: 10.050324582945724
Epoch 9544/10000, Prediction Accuracy = 63.592%, Loss = 0.35990157127380373
Epoch: 9544, Batch Gradient Norm: 8.851073338479605
Epoch: 9544, Batch Gradient Norm after: 8.851073338479605
Epoch 9545/10000, Prediction Accuracy = 63.472%, Loss = 0.3502352237701416
Epoch: 9545, Batch Gradient Norm: 10.002756533482412
Epoch: 9545, Batch Gradient Norm after: 10.002756533482412
Epoch 9546/10000, Prediction Accuracy = 63.484%, Loss = 0.35901198983192445
Epoch: 9546, Batch Gradient Norm: 11.916521934769426
Epoch: 9546, Batch Gradient Norm after: 11.916521934769426
Epoch 9547/10000, Prediction Accuracy = 63.552%, Loss = 0.3761676847934723
Epoch: 9547, Batch Gradient Norm: 7.839885880665636
Epoch: 9547, Batch Gradient Norm after: 7.839885880665636
Epoch 9548/10000, Prediction Accuracy = 63.802%, Loss = 0.3493937194347382
Epoch: 9548, Batch Gradient Norm: 10.184684436327814
Epoch: 9548, Batch Gradient Norm after: 10.184684436327814
Epoch 9549/10000, Prediction Accuracy = 63.898%, Loss = 0.3633477330207825
Epoch: 9549, Batch Gradient Norm: 9.789856397937287
Epoch: 9549, Batch Gradient Norm after: 9.789856397937287
Epoch 9550/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3594201624393463
Epoch: 9550, Batch Gradient Norm: 11.325000335923091
Epoch: 9550, Batch Gradient Norm after: 11.325000335923091
Epoch 9551/10000, Prediction Accuracy = 63.70000000000001%, Loss = 0.3686849296092987
Epoch: 9551, Batch Gradient Norm: 14.096088719286756
Epoch: 9551, Batch Gradient Norm after: 14.096088719286756
Epoch 9552/10000, Prediction Accuracy = 63.548%, Loss = 0.39351807832717894
Epoch: 9552, Batch Gradient Norm: 9.421681048780425
Epoch: 9552, Batch Gradient Norm after: 9.421681048780425
Epoch 9553/10000, Prediction Accuracy = 63.734%, Loss = 0.357758629322052
Epoch: 9553, Batch Gradient Norm: 7.605894403035443
Epoch: 9553, Batch Gradient Norm after: 7.605894403035443
Epoch 9554/10000, Prediction Accuracy = 63.788%, Loss = 0.3453715145587921
Epoch: 9554, Batch Gradient Norm: 10.293557131241297
Epoch: 9554, Batch Gradient Norm after: 10.293557131241297
Epoch 9555/10000, Prediction Accuracy = 63.584%, Loss = 0.3616325855255127
Epoch: 9555, Batch Gradient Norm: 13.098961083121248
Epoch: 9555, Batch Gradient Norm after: 13.098961083121248
Epoch 9556/10000, Prediction Accuracy = 63.70399999999999%, Loss = 0.3869811475276947
Epoch: 9556, Batch Gradient Norm: 7.942971669570766
Epoch: 9556, Batch Gradient Norm after: 7.942971669570766
Epoch 9557/10000, Prediction Accuracy = 63.65200000000001%, Loss = 0.3460495889186859
Epoch: 9557, Batch Gradient Norm: 5.687169913601089
Epoch: 9557, Batch Gradient Norm after: 5.687169913601089
Epoch 9558/10000, Prediction Accuracy = 63.712%, Loss = 0.3359401047229767
Epoch: 9558, Batch Gradient Norm: 9.67449173671947
Epoch: 9558, Batch Gradient Norm after: 9.67449173671947
Epoch 9559/10000, Prediction Accuracy = 63.598%, Loss = 0.3591290771961212
Epoch: 9559, Batch Gradient Norm: 10.473700708284378
Epoch: 9559, Batch Gradient Norm after: 10.473700708284378
Epoch 9560/10000, Prediction Accuracy = 63.436%, Loss = 0.3654364287853241
Epoch: 9560, Batch Gradient Norm: 10.351192892701567
Epoch: 9560, Batch Gradient Norm after: 10.351192892701567
Epoch 9561/10000, Prediction Accuracy = 63.745999999999995%, Loss = 0.36246705055236816
Epoch: 9561, Batch Gradient Norm: 7.335516557570268
Epoch: 9561, Batch Gradient Norm after: 7.335516557570268
Epoch 9562/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3439626097679138
Epoch: 9562, Batch Gradient Norm: 9.424795911528983
Epoch: 9562, Batch Gradient Norm after: 9.424795911528983
Epoch 9563/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35767953395843505
Epoch: 9563, Batch Gradient Norm: 10.429539366224413
Epoch: 9563, Batch Gradient Norm after: 10.429539366224413
Epoch 9564/10000, Prediction Accuracy = 63.726%, Loss = 0.36522355675697327
Epoch: 9564, Batch Gradient Norm: 12.60100475719777
Epoch: 9564, Batch Gradient Norm after: 12.60100475719777
Epoch 9565/10000, Prediction Accuracy = 63.468%, Loss = 0.37870949506759644
Epoch: 9565, Batch Gradient Norm: 9.54489813901622
Epoch: 9565, Batch Gradient Norm after: 9.54489813901622
Epoch 9566/10000, Prediction Accuracy = 63.806%, Loss = 0.3568966925144196
Epoch: 9566, Batch Gradient Norm: 9.66329729011035
Epoch: 9566, Batch Gradient Norm after: 9.66329729011035
Epoch 9567/10000, Prediction Accuracy = 63.73%, Loss = 0.3578136801719666
Epoch: 9567, Batch Gradient Norm: 9.866236258154917
Epoch: 9567, Batch Gradient Norm after: 9.866236258154917
Epoch 9568/10000, Prediction Accuracy = 63.638%, Loss = 0.3581546604633331
Epoch: 9568, Batch Gradient Norm: 11.619138780841244
Epoch: 9568, Batch Gradient Norm after: 11.619138780841244
Epoch 9569/10000, Prediction Accuracy = 63.662%, Loss = 0.3682148575782776
Epoch: 9569, Batch Gradient Norm: 10.514447033989695
Epoch: 9569, Batch Gradient Norm after: 10.514447033989695
Epoch 9570/10000, Prediction Accuracy = 63.55%, Loss = 0.36415316462516784
Epoch: 9570, Batch Gradient Norm: 8.94995949896205
Epoch: 9570, Batch Gradient Norm after: 8.94995949896205
Epoch 9571/10000, Prediction Accuracy = 63.662%, Loss = 0.35514709949493406
Epoch: 9571, Batch Gradient Norm: 8.006702235320974
Epoch: 9571, Batch Gradient Norm after: 8.006702235320974
Epoch 9572/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.3488197386264801
Epoch: 9572, Batch Gradient Norm: 8.807622288020418
Epoch: 9572, Batch Gradient Norm after: 8.807622288020418
Epoch 9573/10000, Prediction Accuracy = 63.586%, Loss = 0.35279920101165774
Epoch: 9573, Batch Gradient Norm: 9.658600838924146
Epoch: 9573, Batch Gradient Norm after: 9.658600838924146
Epoch 9574/10000, Prediction Accuracy = 63.507999999999996%, Loss = 0.3564759373664856
Epoch: 9574, Batch Gradient Norm: 11.055897162507554
Epoch: 9574, Batch Gradient Norm after: 11.055897162507554
Epoch 9575/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.367577052116394
Epoch: 9575, Batch Gradient Norm: 10.252060985221144
Epoch: 9575, Batch Gradient Norm after: 10.252060985221144
Epoch 9576/10000, Prediction Accuracy = 63.576%, Loss = 0.36635987758636473
Epoch: 9576, Batch Gradient Norm: 7.033880927499167
Epoch: 9576, Batch Gradient Norm after: 7.033880927499167
Epoch 9577/10000, Prediction Accuracy = 63.82199999999999%, Loss = 0.34313852787017823
Epoch: 9577, Batch Gradient Norm: 7.516941431737472
Epoch: 9577, Batch Gradient Norm after: 7.516941431737472
Epoch 9578/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.34656843543052673
Epoch: 9578, Batch Gradient Norm: 11.5435115538373
Epoch: 9578, Batch Gradient Norm after: 11.5435115538373
Epoch 9579/10000, Prediction Accuracy = 63.774%, Loss = 0.3701833844184875
Epoch: 9579, Batch Gradient Norm: 15.140811818003836
Epoch: 9579, Batch Gradient Norm after: 15.140811818003836
Epoch 9580/10000, Prediction Accuracy = 63.762%, Loss = 0.4079525589942932
Epoch: 9580, Batch Gradient Norm: 12.067361458694291
Epoch: 9580, Batch Gradient Norm after: 12.067361458694291
Epoch 9581/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.377646666765213
Epoch: 9581, Batch Gradient Norm: 7.901760595292245
Epoch: 9581, Batch Gradient Norm after: 7.901760595292245
Epoch 9582/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.34854339957237246
Epoch: 9582, Batch Gradient Norm: 7.189558291413482
Epoch: 9582, Batch Gradient Norm after: 7.189558291413482
Epoch 9583/10000, Prediction Accuracy = 63.714%, Loss = 0.34251766204833983
Epoch: 9583, Batch Gradient Norm: 8.815852243521721
Epoch: 9583, Batch Gradient Norm after: 8.815852243521721
Epoch 9584/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3545807361602783
Epoch: 9584, Batch Gradient Norm: 12.26754496988046
Epoch: 9584, Batch Gradient Norm after: 12.26754496988046
Epoch 9585/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.38008891940116885
Epoch: 9585, Batch Gradient Norm: 9.486329809406019
Epoch: 9585, Batch Gradient Norm after: 9.486329809406019
Epoch 9586/10000, Prediction Accuracy = 63.57199999999999%, Loss = 0.35832178592681885
Epoch: 9586, Batch Gradient Norm: 8.044827551451467
Epoch: 9586, Batch Gradient Norm after: 8.044827551451467
Epoch 9587/10000, Prediction Accuracy = 63.568000000000005%, Loss = 0.3503066420555115
Epoch: 9587, Batch Gradient Norm: 8.130426057363284
Epoch: 9587, Batch Gradient Norm after: 8.130426057363284
Epoch 9588/10000, Prediction Accuracy = 63.798%, Loss = 0.34969091415405273
Epoch: 9588, Batch Gradient Norm: 8.655288335364094
Epoch: 9588, Batch Gradient Norm after: 8.655288335364094
Epoch 9589/10000, Prediction Accuracy = 63.666%, Loss = 0.35149083733558656
Epoch: 9589, Batch Gradient Norm: 8.910008251024044
Epoch: 9589, Batch Gradient Norm after: 8.910008251024044
Epoch 9590/10000, Prediction Accuracy = 63.766%, Loss = 0.35115442276000974
Epoch: 9590, Batch Gradient Norm: 10.80063081244972
Epoch: 9590, Batch Gradient Norm after: 10.80063081244972
Epoch 9591/10000, Prediction Accuracy = 63.448%, Loss = 0.3652687072753906
Epoch: 9591, Batch Gradient Norm: 10.724653167185203
Epoch: 9591, Batch Gradient Norm after: 10.724653167185203
Epoch 9592/10000, Prediction Accuracy = 63.694%, Loss = 0.36442750692367554
Epoch: 9592, Batch Gradient Norm: 10.270083767639685
Epoch: 9592, Batch Gradient Norm after: 10.270083767639685
Epoch 9593/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.36158275604248047
Epoch: 9593, Batch Gradient Norm: 8.515811923350407
Epoch: 9593, Batch Gradient Norm after: 8.515811923350407
Epoch 9594/10000, Prediction Accuracy = 63.834%, Loss = 0.3525598168373108
Epoch: 9594, Batch Gradient Norm: 10.081541546473861
Epoch: 9594, Batch Gradient Norm after: 10.081541546473861
Epoch 9595/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.36035445928573606
Epoch: 9595, Batch Gradient Norm: 10.147512394171468
Epoch: 9595, Batch Gradient Norm after: 10.147512394171468
Epoch 9596/10000, Prediction Accuracy = 63.586%, Loss = 0.3628301203250885
Epoch: 9596, Batch Gradient Norm: 9.465594057906522
Epoch: 9596, Batch Gradient Norm after: 9.465594057906522
Epoch 9597/10000, Prediction Accuracy = 63.754%, Loss = 0.35346842408180235
Epoch: 9597, Batch Gradient Norm: 11.34184988903253
Epoch: 9597, Batch Gradient Norm after: 11.34184988903253
Epoch 9598/10000, Prediction Accuracy = 63.6%, Loss = 0.3678953409194946
Epoch: 9598, Batch Gradient Norm: 12.16126239875476
Epoch: 9598, Batch Gradient Norm after: 12.16126239875476
Epoch 9599/10000, Prediction Accuracy = 63.75599999999999%, Loss = 0.37424967885017396
Epoch: 9599, Batch Gradient Norm: 10.120920161756553
Epoch: 9599, Batch Gradient Norm after: 10.120920161756553
Epoch 9600/10000, Prediction Accuracy = 63.862%, Loss = 0.36301121711730955
Epoch: 9600, Batch Gradient Norm: 11.247435855722136
Epoch: 9600, Batch Gradient Norm after: 11.247435855722136
Epoch 9601/10000, Prediction Accuracy = 63.80800000000001%, Loss = 0.36948375701904296
Epoch: 9601, Batch Gradient Norm: 13.543412489809473
Epoch: 9601, Batch Gradient Norm after: 13.543412489809473
Epoch 9602/10000, Prediction Accuracy = 63.576%, Loss = 0.3969021141529083
Epoch: 9602, Batch Gradient Norm: 9.117373239295002
Epoch: 9602, Batch Gradient Norm after: 9.117373239295002
Epoch 9603/10000, Prediction Accuracy = 63.7%, Loss = 0.35576430559158323
Epoch: 9603, Batch Gradient Norm: 10.090814097802607
Epoch: 9603, Batch Gradient Norm after: 10.090814097802607
Epoch 9604/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.35999011993408203
Epoch: 9604, Batch Gradient Norm: 9.784452366280625
Epoch: 9604, Batch Gradient Norm after: 9.784452366280625
Epoch 9605/10000, Prediction Accuracy = 63.65%, Loss = 0.3563283383846283
Epoch: 9605, Batch Gradient Norm: 8.938747103923388
Epoch: 9605, Batch Gradient Norm after: 8.938747103923388
Epoch 9606/10000, Prediction Accuracy = 63.556%, Loss = 0.35597949624061587
Epoch: 9606, Batch Gradient Norm: 6.382238110015136
Epoch: 9606, Batch Gradient Norm after: 6.382238110015136
Epoch 9607/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.33928601145744325
Epoch: 9607, Batch Gradient Norm: 9.29676964724743
Epoch: 9607, Batch Gradient Norm after: 9.29676964724743
Epoch 9608/10000, Prediction Accuracy = 63.666%, Loss = 0.35576058030128477
Epoch: 9608, Batch Gradient Norm: 10.411022702496632
Epoch: 9608, Batch Gradient Norm after: 10.411022702496632
Epoch 9609/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.365145868062973
Epoch: 9609, Batch Gradient Norm: 10.01522111628414
Epoch: 9609, Batch Gradient Norm after: 10.01522111628414
Epoch 9610/10000, Prediction Accuracy = 63.586%, Loss = 0.35776447057724
Epoch: 9610, Batch Gradient Norm: 8.465102345413323
Epoch: 9610, Batch Gradient Norm after: 8.465102345413323
Epoch 9611/10000, Prediction Accuracy = 63.754%, Loss = 0.34747040271759033
Epoch: 9611, Batch Gradient Norm: 10.24656367792456
Epoch: 9611, Batch Gradient Norm after: 10.24656367792456
Epoch 9612/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3613811552524567
Epoch: 9612, Batch Gradient Norm: 9.494422889795487
Epoch: 9612, Batch Gradient Norm after: 9.494422889795487
Epoch 9613/10000, Prediction Accuracy = 63.69599999999999%, Loss = 0.35878958106040953
Epoch: 9613, Batch Gradient Norm: 9.698810247817452
Epoch: 9613, Batch Gradient Norm after: 9.698810247817452
Epoch 9614/10000, Prediction Accuracy = 63.806%, Loss = 0.360327011346817
Epoch: 9614, Batch Gradient Norm: 10.683790304034412
Epoch: 9614, Batch Gradient Norm after: 10.683790304034412
Epoch 9615/10000, Prediction Accuracy = 63.6%, Loss = 0.3686931490898132
Epoch: 9615, Batch Gradient Norm: 11.45047165077971
Epoch: 9615, Batch Gradient Norm after: 11.45047165077971
Epoch 9616/10000, Prediction Accuracy = 63.69%, Loss = 0.3711052894592285
Epoch: 9616, Batch Gradient Norm: 10.534824278021865
Epoch: 9616, Batch Gradient Norm after: 10.534824278021865
Epoch 9617/10000, Prediction Accuracy = 63.59599999999999%, Loss = 0.3615639805793762
Epoch: 9617, Batch Gradient Norm: 9.04837234238194
Epoch: 9617, Batch Gradient Norm after: 9.04837234238194
Epoch 9618/10000, Prediction Accuracy = 63.751999999999995%, Loss = 0.35502532720565794
Epoch: 9618, Batch Gradient Norm: 10.644857066195348
Epoch: 9618, Batch Gradient Norm after: 10.644857066195348
Epoch 9619/10000, Prediction Accuracy = 63.774%, Loss = 0.36093624830245974
Epoch: 9619, Batch Gradient Norm: 11.54243838961189
Epoch: 9619, Batch Gradient Norm after: 11.54243838961189
Epoch 9620/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.37357895970344546
Epoch: 9620, Batch Gradient Norm: 8.627288393082845
Epoch: 9620, Batch Gradient Norm after: 8.627288393082845
Epoch 9621/10000, Prediction Accuracy = 63.794000000000004%, Loss = 0.34759525060653684
Epoch: 9621, Batch Gradient Norm: 7.6952959070216975
Epoch: 9621, Batch Gradient Norm after: 7.6952959070216975
Epoch 9622/10000, Prediction Accuracy = 63.910000000000004%, Loss = 0.3430462419986725
Epoch: 9622, Batch Gradient Norm: 9.66039001370979
Epoch: 9622, Batch Gradient Norm after: 9.66039001370979
Epoch 9623/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.3570437550544739
Epoch: 9623, Batch Gradient Norm: 10.563318806004464
Epoch: 9623, Batch Gradient Norm after: 10.563318806004464
Epoch 9624/10000, Prediction Accuracy = 63.626%, Loss = 0.3639651656150818
Epoch: 9624, Batch Gradient Norm: 12.138595684682029
Epoch: 9624, Batch Gradient Norm after: 12.138595684682029
Epoch 9625/10000, Prediction Accuracy = 63.688%, Loss = 0.3755333125591278
Epoch: 9625, Batch Gradient Norm: 10.320810393766148
Epoch: 9625, Batch Gradient Norm after: 10.320810393766148
Epoch 9626/10000, Prediction Accuracy = 63.59599999999999%, Loss = 0.36260839700698855
Epoch: 9626, Batch Gradient Norm: 9.218834964683047
Epoch: 9626, Batch Gradient Norm after: 9.218834964683047
Epoch 9627/10000, Prediction Accuracy = 63.86%, Loss = 0.3544949948787689
Epoch: 9627, Batch Gradient Norm: 10.925888570397294
Epoch: 9627, Batch Gradient Norm after: 10.925888570397294
Epoch 9628/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.36439375281333924
Epoch: 9628, Batch Gradient Norm: 9.535180472459823
Epoch: 9628, Batch Gradient Norm after: 9.535180472459823
Epoch 9629/10000, Prediction Accuracy = 63.79200000000001%, Loss = 0.3560410439968109
Epoch: 9629, Batch Gradient Norm: 9.826066664772004
Epoch: 9629, Batch Gradient Norm after: 9.826066664772004
Epoch 9630/10000, Prediction Accuracy = 63.74400000000001%, Loss = 0.35670057535171507
Epoch: 9630, Batch Gradient Norm: 11.943264995330603
Epoch: 9630, Batch Gradient Norm after: 11.943264995330603
Epoch 9631/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3716855049133301
Epoch: 9631, Batch Gradient Norm: 9.416226584432593
Epoch: 9631, Batch Gradient Norm after: 9.416226584432593
Epoch 9632/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.35471461415290834
Epoch: 9632, Batch Gradient Norm: 7.950736821612085
Epoch: 9632, Batch Gradient Norm after: 7.950736821612085
Epoch 9633/10000, Prediction Accuracy = 63.65%, Loss = 0.35003605484962463
Epoch: 9633, Batch Gradient Norm: 7.75409588361631
Epoch: 9633, Batch Gradient Norm after: 7.75409588361631
Epoch 9634/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.3470113158226013
Epoch: 9634, Batch Gradient Norm: 10.027245855112971
Epoch: 9634, Batch Gradient Norm after: 10.027245855112971
Epoch 9635/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.3602128684520721
Epoch: 9635, Batch Gradient Norm: 12.006037836879795
Epoch: 9635, Batch Gradient Norm after: 12.006037836879795
Epoch 9636/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.37503278255462646
Epoch: 9636, Batch Gradient Norm: 8.839594633032283
Epoch: 9636, Batch Gradient Norm after: 8.839594633032283
Epoch 9637/10000, Prediction Accuracy = 63.760000000000005%, Loss = 0.3510307013988495
Epoch: 9637, Batch Gradient Norm: 6.613037660651657
Epoch: 9637, Batch Gradient Norm after: 6.613037660651657
Epoch 9638/10000, Prediction Accuracy = 63.926%, Loss = 0.33798412084579466
Epoch: 9638, Batch Gradient Norm: 9.98472203896477
Epoch: 9638, Batch Gradient Norm after: 9.98472203896477
Epoch 9639/10000, Prediction Accuracy = 63.838%, Loss = 0.3591223418712616
Epoch: 9639, Batch Gradient Norm: 10.772744650613182
Epoch: 9639, Batch Gradient Norm after: 10.772744650613182
Epoch 9640/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.36379393339157107
Epoch: 9640, Batch Gradient Norm: 11.240062718711949
Epoch: 9640, Batch Gradient Norm after: 11.240062718711949
Epoch 9641/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.37397940158843995
Epoch: 9641, Batch Gradient Norm: 9.422978454187215
Epoch: 9641, Batch Gradient Norm after: 9.422978454187215
Epoch 9642/10000, Prediction Accuracy = 63.69200000000001%, Loss = 0.35445364117622374
Epoch: 9642, Batch Gradient Norm: 9.952187667002795
Epoch: 9642, Batch Gradient Norm after: 9.952187667002795
Epoch 9643/10000, Prediction Accuracy = 63.58599999999999%, Loss = 0.36145437359809873
Epoch: 9643, Batch Gradient Norm: 8.683709908344373
Epoch: 9643, Batch Gradient Norm after: 8.683709908344373
Epoch 9644/10000, Prediction Accuracy = 63.86400000000001%, Loss = 0.3505153238773346
Epoch: 9644, Batch Gradient Norm: 10.553071568313735
Epoch: 9644, Batch Gradient Norm after: 10.553071568313735
Epoch 9645/10000, Prediction Accuracy = 63.83%, Loss = 0.36229126453399657
Epoch: 9645, Batch Gradient Norm: 11.671444949621328
Epoch: 9645, Batch Gradient Norm after: 11.671444949621328
Epoch 9646/10000, Prediction Accuracy = 63.572%, Loss = 0.37131158709526063
Epoch: 9646, Batch Gradient Norm: 9.089246474636594
Epoch: 9646, Batch Gradient Norm after: 9.089246474636594
Epoch 9647/10000, Prediction Accuracy = 63.467999999999996%, Loss = 0.35508291721343993
Epoch: 9647, Batch Gradient Norm: 9.34021311503731
Epoch: 9647, Batch Gradient Norm after: 9.34021311503731
Epoch 9648/10000, Prediction Accuracy = 63.614%, Loss = 0.3545205116271973
Epoch: 9648, Batch Gradient Norm: 10.389083867541625
Epoch: 9648, Batch Gradient Norm after: 10.389083867541625
Epoch 9649/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3602851152420044
Epoch: 9649, Batch Gradient Norm: 10.232371991677656
Epoch: 9649, Batch Gradient Norm after: 10.232371991677656
Epoch 9650/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.3589914798736572
Epoch: 9650, Batch Gradient Norm: 9.921108642161741
Epoch: 9650, Batch Gradient Norm after: 9.921108642161741
Epoch 9651/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.3594677567481995
Epoch: 9651, Batch Gradient Norm: 10.337539484274172
Epoch: 9651, Batch Gradient Norm after: 10.337539484274172
Epoch 9652/10000, Prediction Accuracy = 63.878%, Loss = 0.361585396528244
Epoch: 9652, Batch Gradient Norm: 9.629353305739993
Epoch: 9652, Batch Gradient Norm after: 9.629353305739993
Epoch 9653/10000, Prediction Accuracy = 63.698%, Loss = 0.35489575266838075
Epoch: 9653, Batch Gradient Norm: 12.626834487974255
Epoch: 9653, Batch Gradient Norm after: 12.626834487974255
Epoch 9654/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3796763360500336
Epoch: 9654, Batch Gradient Norm: 12.040231387772412
Epoch: 9654, Batch Gradient Norm after: 12.040231387772412
Epoch 9655/10000, Prediction Accuracy = 63.5%, Loss = 0.37412468194961546
Epoch: 9655, Batch Gradient Norm: 9.181649427161334
Epoch: 9655, Batch Gradient Norm after: 9.181649427161334
Epoch 9656/10000, Prediction Accuracy = 63.564%, Loss = 0.35185897946357725
Epoch: 9656, Batch Gradient Norm: 9.047975763326761
Epoch: 9656, Batch Gradient Norm after: 9.047975763326761
Epoch 9657/10000, Prediction Accuracy = 63.712%, Loss = 0.3559820055961609
Epoch: 9657, Batch Gradient Norm: 10.583720337249696
Epoch: 9657, Batch Gradient Norm after: 10.583720337249696
Epoch 9658/10000, Prediction Accuracy = 63.826%, Loss = 0.3607640743255615
Epoch: 9658, Batch Gradient Norm: 11.46474349234762
Epoch: 9658, Batch Gradient Norm after: 11.46474349234762
Epoch 9659/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.3703762114048004
Epoch: 9659, Batch Gradient Norm: 9.477741192262476
Epoch: 9659, Batch Gradient Norm after: 9.477741192262476
Epoch 9660/10000, Prediction Accuracy = 63.688%, Loss = 0.3535014271736145
Epoch: 9660, Batch Gradient Norm: 8.505222660813647
Epoch: 9660, Batch Gradient Norm after: 8.505222660813647
Epoch 9661/10000, Prediction Accuracy = 63.78399999999999%, Loss = 0.3483966886997223
Epoch: 9661, Batch Gradient Norm: 9.603913603743347
Epoch: 9661, Batch Gradient Norm after: 9.603913603743347
Epoch 9662/10000, Prediction Accuracy = 63.68399999999999%, Loss = 0.3568149387836456
Epoch: 9662, Batch Gradient Norm: 12.018252878810735
Epoch: 9662, Batch Gradient Norm after: 12.018252878810735
Epoch 9663/10000, Prediction Accuracy = 63.504%, Loss = 0.381428462266922
Epoch: 9663, Batch Gradient Norm: 9.373829968015663
Epoch: 9663, Batch Gradient Norm after: 9.373829968015663
Epoch 9664/10000, Prediction Accuracy = 63.678%, Loss = 0.3532021641731262
Epoch: 9664, Batch Gradient Norm: 8.206230135648601
Epoch: 9664, Batch Gradient Norm after: 8.206230135648601
Epoch 9665/10000, Prediction Accuracy = 63.718%, Loss = 0.3474448800086975
Epoch: 9665, Batch Gradient Norm: 7.114884032779958
Epoch: 9665, Batch Gradient Norm after: 7.114884032779958
Epoch 9666/10000, Prediction Accuracy = 63.824%, Loss = 0.3436831831932068
Epoch: 9666, Batch Gradient Norm: 9.205465286272338
Epoch: 9666, Batch Gradient Norm after: 9.205465286272338
Epoch 9667/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.3574908792972565
Epoch: 9667, Batch Gradient Norm: 10.68527901632728
Epoch: 9667, Batch Gradient Norm after: 10.68527901632728
Epoch 9668/10000, Prediction Accuracy = 63.742%, Loss = 0.3626591622829437
Epoch: 9668, Batch Gradient Norm: 11.780147806910067
Epoch: 9668, Batch Gradient Norm after: 11.780147806910067
Epoch 9669/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.37249411940574645
Epoch: 9669, Batch Gradient Norm: 9.219909083596853
Epoch: 9669, Batch Gradient Norm after: 9.219909083596853
Epoch 9670/10000, Prediction Accuracy = 63.824%, Loss = 0.3525866687297821
Epoch: 9670, Batch Gradient Norm: 9.057434750856151
Epoch: 9670, Batch Gradient Norm after: 9.057434750856151
Epoch 9671/10000, Prediction Accuracy = 63.73599999999999%, Loss = 0.351353520154953
Epoch: 9671, Batch Gradient Norm: 8.823339837240068
Epoch: 9671, Batch Gradient Norm after: 8.823339837240068
Epoch 9672/10000, Prediction Accuracy = 63.71600000000001%, Loss = 0.34973458051681516
Epoch: 9672, Batch Gradient Norm: 9.536090873760813
Epoch: 9672, Batch Gradient Norm after: 9.536090873760813
Epoch 9673/10000, Prediction Accuracy = 63.774%, Loss = 0.35395452976226804
Epoch: 9673, Batch Gradient Norm: 10.111927021111457
Epoch: 9673, Batch Gradient Norm after: 10.111927021111457
Epoch 9674/10000, Prediction Accuracy = 63.76400000000001%, Loss = 0.3589995980262756
Epoch: 9674, Batch Gradient Norm: 10.591322756353863
Epoch: 9674, Batch Gradient Norm after: 10.591322756353863
Epoch 9675/10000, Prediction Accuracy = 63.686%, Loss = 0.36285600662231443
Epoch: 9675, Batch Gradient Norm: 9.60784698056769
Epoch: 9675, Batch Gradient Norm after: 9.60784698056769
Epoch 9676/10000, Prediction Accuracy = 63.65%, Loss = 0.3588206052780151
Epoch: 9676, Batch Gradient Norm: 9.17851823762862
Epoch: 9676, Batch Gradient Norm after: 9.17851823762862
Epoch 9677/10000, Prediction Accuracy = 63.738%, Loss = 0.3523650586605072
Epoch: 9677, Batch Gradient Norm: 11.6935329445081
Epoch: 9677, Batch Gradient Norm after: 11.6935329445081
Epoch 9678/10000, Prediction Accuracy = 63.452%, Loss = 0.37336429953575134
Epoch: 9678, Batch Gradient Norm: 9.303210230721763
Epoch: 9678, Batch Gradient Norm after: 9.303210230721763
Epoch 9679/10000, Prediction Accuracy = 63.778%, Loss = 0.355319082736969
Epoch: 9679, Batch Gradient Norm: 10.846726072893768
Epoch: 9679, Batch Gradient Norm after: 10.846726072893768
Epoch 9680/10000, Prediction Accuracy = 63.626%, Loss = 0.3636794090270996
Epoch: 9680, Batch Gradient Norm: 11.567943960882916
Epoch: 9680, Batch Gradient Norm after: 11.567943960882916
Epoch 9681/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.371013468503952
Epoch: 9681, Batch Gradient Norm: 10.71991802955851
Epoch: 9681, Batch Gradient Norm after: 10.71991802955851
Epoch 9682/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.3652308344841003
Epoch: 9682, Batch Gradient Norm: 10.993900436928776
Epoch: 9682, Batch Gradient Norm after: 10.993900436928776
Epoch 9683/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.36599103808403016
Epoch: 9683, Batch Gradient Norm: 8.87395376732265
Epoch: 9683, Batch Gradient Norm after: 8.87395376732265
Epoch 9684/10000, Prediction Accuracy = 63.766%, Loss = 0.35412721037864686
Epoch: 9684, Batch Gradient Norm: 7.304737737734097
Epoch: 9684, Batch Gradient Norm after: 7.304737737734097
Epoch 9685/10000, Prediction Accuracy = 63.738%, Loss = 0.34345346689224243
Epoch: 9685, Batch Gradient Norm: 11.255711198658155
Epoch: 9685, Batch Gradient Norm after: 11.255711198658155
Epoch 9686/10000, Prediction Accuracy = 63.6%, Loss = 0.3706053912639618
Epoch: 9686, Batch Gradient Norm: 9.13846751415908
Epoch: 9686, Batch Gradient Norm after: 9.13846751415908
Epoch 9687/10000, Prediction Accuracy = 63.634%, Loss = 0.355640172958374
Epoch: 9687, Batch Gradient Norm: 10.913288385964576
Epoch: 9687, Batch Gradient Norm after: 10.913288385964576
Epoch 9688/10000, Prediction Accuracy = 63.662%, Loss = 0.36677151918411255
Epoch: 9688, Batch Gradient Norm: 10.27385801816277
Epoch: 9688, Batch Gradient Norm after: 10.27385801816277
Epoch 9689/10000, Prediction Accuracy = 63.86800000000001%, Loss = 0.35584147572517394
Epoch: 9689, Batch Gradient Norm: 9.53808547604775
Epoch: 9689, Batch Gradient Norm after: 9.53808547604775
Epoch 9690/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.3596970856189728
Epoch: 9690, Batch Gradient Norm: 10.424971199744583
Epoch: 9690, Batch Gradient Norm after: 10.424971199744583
Epoch 9691/10000, Prediction Accuracy = 63.802%, Loss = 0.36357499957084655
Epoch: 9691, Batch Gradient Norm: 11.135157991324382
Epoch: 9691, Batch Gradient Norm after: 11.135157991324382
Epoch 9692/10000, Prediction Accuracy = 63.59000000000001%, Loss = 0.37440221905708315
Epoch: 9692, Batch Gradient Norm: 8.80392097831187
Epoch: 9692, Batch Gradient Norm after: 8.80392097831187
Epoch 9693/10000, Prediction Accuracy = 63.79200000000001%, Loss = 0.3504584193229675
Epoch: 9693, Batch Gradient Norm: 8.049906729657447
Epoch: 9693, Batch Gradient Norm after: 8.049906729657447
Epoch 9694/10000, Prediction Accuracy = 63.802%, Loss = 0.3467835247516632
Epoch: 9694, Batch Gradient Norm: 8.668723638575658
Epoch: 9694, Batch Gradient Norm after: 8.668723638575658
Epoch 9695/10000, Prediction Accuracy = 63.79200000000001%, Loss = 0.3507911145687103
Epoch: 9695, Batch Gradient Norm: 11.434694306342205
Epoch: 9695, Batch Gradient Norm after: 11.434694306342205
Epoch 9696/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.37087546586990355
Epoch: 9696, Batch Gradient Norm: 10.086874759209984
Epoch: 9696, Batch Gradient Norm after: 10.086874759209984
Epoch 9697/10000, Prediction Accuracy = 63.638%, Loss = 0.3603751420974731
Epoch: 9697, Batch Gradient Norm: 9.851789737326031
Epoch: 9697, Batch Gradient Norm after: 9.851789737326031
Epoch 9698/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.35362383127212527
Epoch: 9698, Batch Gradient Norm: 13.12831134046618
Epoch: 9698, Batch Gradient Norm after: 13.12831134046618
Epoch 9699/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.38243608474731444
Epoch: 9699, Batch Gradient Norm: 10.646748065593806
Epoch: 9699, Batch Gradient Norm after: 10.646748065593806
Epoch 9700/10000, Prediction Accuracy = 63.714%, Loss = 0.36165496706962585
Epoch: 9700, Batch Gradient Norm: 8.525813014867092
Epoch: 9700, Batch Gradient Norm after: 8.525813014867092
Epoch 9701/10000, Prediction Accuracy = 63.826%, Loss = 0.34831867814064027
Epoch: 9701, Batch Gradient Norm: 8.80378464527015
Epoch: 9701, Batch Gradient Norm after: 8.80378464527015
Epoch 9702/10000, Prediction Accuracy = 63.674%, Loss = 0.35061107873916625
Epoch: 9702, Batch Gradient Norm: 8.507209876711425
Epoch: 9702, Batch Gradient Norm after: 8.507209876711425
Epoch 9703/10000, Prediction Accuracy = 63.8%, Loss = 0.35155028104782104
Epoch: 9703, Batch Gradient Norm: 10.307436475499722
Epoch: 9703, Batch Gradient Norm after: 10.307436475499722
Epoch 9704/10000, Prediction Accuracy = 63.778%, Loss = 0.36047630906105044
Epoch: 9704, Batch Gradient Norm: 9.362583049945542
Epoch: 9704, Batch Gradient Norm after: 9.362583049945542
Epoch 9705/10000, Prediction Accuracy = 63.604%, Loss = 0.35408217310905454
Epoch: 9705, Batch Gradient Norm: 9.54571878975463
Epoch: 9705, Batch Gradient Norm after: 9.54571878975463
Epoch 9706/10000, Prediction Accuracy = 63.738000000000014%, Loss = 0.3537747263908386
Epoch: 9706, Batch Gradient Norm: 9.337614998280538
Epoch: 9706, Batch Gradient Norm after: 9.337614998280538
Epoch 9707/10000, Prediction Accuracy = 63.907999999999994%, Loss = 0.35529935359954834
Epoch: 9707, Batch Gradient Norm: 8.06034732428897
Epoch: 9707, Batch Gradient Norm after: 8.06034732428897
Epoch 9708/10000, Prediction Accuracy = 63.824%, Loss = 0.34896594285964966
Epoch: 9708, Batch Gradient Norm: 8.502314687939403
Epoch: 9708, Batch Gradient Norm after: 8.502314687939403
Epoch 9709/10000, Prediction Accuracy = 63.734%, Loss = 0.3505997657775879
Epoch: 9709, Batch Gradient Norm: 12.209303033611175
Epoch: 9709, Batch Gradient Norm after: 12.209303033611175
Epoch 9710/10000, Prediction Accuracy = 63.848%, Loss = 0.37523735165596006
Epoch: 9710, Batch Gradient Norm: 10.98275408865788
Epoch: 9710, Batch Gradient Norm after: 10.98275408865788
Epoch 9711/10000, Prediction Accuracy = 63.709999999999994%, Loss = 0.36912707686424256
Epoch: 9711, Batch Gradient Norm: 9.686614914420277
Epoch: 9711, Batch Gradient Norm after: 9.686614914420277
Epoch 9712/10000, Prediction Accuracy = 63.79%, Loss = 0.35621886849403384
Epoch: 9712, Batch Gradient Norm: 9.745278556354222
Epoch: 9712, Batch Gradient Norm after: 9.745278556354222
Epoch 9713/10000, Prediction Accuracy = 63.791999999999994%, Loss = 0.35385951995849607
Epoch: 9713, Batch Gradient Norm: 10.005748408225468
Epoch: 9713, Batch Gradient Norm after: 10.005748408225468
Epoch 9714/10000, Prediction Accuracy = 63.67%, Loss = 0.35791788101196287
Epoch: 9714, Batch Gradient Norm: 10.884238408571749
Epoch: 9714, Batch Gradient Norm after: 10.884238408571749
Epoch 9715/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.36377384662628176
Epoch: 9715, Batch Gradient Norm: 10.67348535676827
Epoch: 9715, Batch Gradient Norm after: 10.67348535676827
Epoch 9716/10000, Prediction Accuracy = 63.614%, Loss = 0.3611030340194702
Epoch: 9716, Batch Gradient Norm: 10.067736385853053
Epoch: 9716, Batch Gradient Norm after: 10.067736385853053
Epoch 9717/10000, Prediction Accuracy = 63.754%, Loss = 0.3590307056903839
Epoch: 9717, Batch Gradient Norm: 7.924916255436766
Epoch: 9717, Batch Gradient Norm after: 7.924916255436766
Epoch 9718/10000, Prediction Accuracy = 63.992%, Loss = 0.34514583349227906
Epoch: 9718, Batch Gradient Norm: 9.425609057300262
Epoch: 9718, Batch Gradient Norm after: 9.425609057300262
Epoch 9719/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.3571035981178284
Epoch: 9719, Batch Gradient Norm: 11.633491416400696
Epoch: 9719, Batch Gradient Norm after: 11.633491416400696
Epoch 9720/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.373121577501297
Epoch: 9720, Batch Gradient Norm: 8.226044836502444
Epoch: 9720, Batch Gradient Norm after: 8.226044836502444
Epoch 9721/10000, Prediction Accuracy = 63.879999999999995%, Loss = 0.347161602973938
Epoch: 9721, Batch Gradient Norm: 11.727738231175328
Epoch: 9721, Batch Gradient Norm after: 11.727738231175328
Epoch 9722/10000, Prediction Accuracy = 63.788%, Loss = 0.3689382791519165
Epoch: 9722, Batch Gradient Norm: 11.921909051914025
Epoch: 9722, Batch Gradient Norm after: 11.921909051914025
Epoch 9723/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.36944607496261594
Epoch: 9723, Batch Gradient Norm: 9.892574375310101
Epoch: 9723, Batch Gradient Norm after: 9.892574375310101
Epoch 9724/10000, Prediction Accuracy = 63.722%, Loss = 0.3564832150936127
Epoch: 9724, Batch Gradient Norm: 6.377888124746935
Epoch: 9724, Batch Gradient Norm after: 6.377888124746935
Epoch 9725/10000, Prediction Accuracy = 63.84400000000001%, Loss = 0.33885385394096373
Epoch: 9725, Batch Gradient Norm: 11.290835771157035
Epoch: 9725, Batch Gradient Norm after: 11.290835771157035
Epoch 9726/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3689128041267395
Epoch: 9726, Batch Gradient Norm: 10.317872936757361
Epoch: 9726, Batch Gradient Norm after: 10.317872936757361
Epoch 9727/10000, Prediction Accuracy = 63.666%, Loss = 0.35881117582321165
Epoch: 9727, Batch Gradient Norm: 8.554309467748038
Epoch: 9727, Batch Gradient Norm after: 8.554309467748038
Epoch 9728/10000, Prediction Accuracy = 63.965999999999994%, Loss = 0.3501546919345856
Epoch: 9728, Batch Gradient Norm: 12.183382788319182
Epoch: 9728, Batch Gradient Norm after: 12.183382788319182
Epoch 9729/10000, Prediction Accuracy = 63.648%, Loss = 0.3722306787967682
Epoch: 9729, Batch Gradient Norm: 10.600228845306683
Epoch: 9729, Batch Gradient Norm after: 10.600228845306683
Epoch 9730/10000, Prediction Accuracy = 63.562%, Loss = 0.3633734405040741
Epoch: 9730, Batch Gradient Norm: 9.549053503670452
Epoch: 9730, Batch Gradient Norm after: 9.549053503670452
Epoch 9731/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.354606556892395
Epoch: 9731, Batch Gradient Norm: 9.38672018738509
Epoch: 9731, Batch Gradient Norm after: 9.38672018738509
Epoch 9732/10000, Prediction Accuracy = 63.65%, Loss = 0.35765507221221926
Epoch: 9732, Batch Gradient Norm: 7.922835018778012
Epoch: 9732, Batch Gradient Norm after: 7.922835018778012
Epoch 9733/10000, Prediction Accuracy = 63.720000000000006%, Loss = 0.34849867820739744
Epoch: 9733, Batch Gradient Norm: 7.320305827235403
Epoch: 9733, Batch Gradient Norm after: 7.320305827235403
Epoch 9734/10000, Prediction Accuracy = 63.854%, Loss = 0.34160942435264585
Epoch: 9734, Batch Gradient Norm: 11.66983736305759
Epoch: 9734, Batch Gradient Norm after: 11.66983736305759
Epoch 9735/10000, Prediction Accuracy = 63.75599999999999%, Loss = 0.36746172308921815
Epoch: 9735, Batch Gradient Norm: 11.014931913512227
Epoch: 9735, Batch Gradient Norm after: 11.014931913512227
Epoch 9736/10000, Prediction Accuracy = 63.69000000000001%, Loss = 0.36590256094932555
Epoch: 9736, Batch Gradient Norm: 10.061878001827026
Epoch: 9736, Batch Gradient Norm after: 10.061878001827026
Epoch 9737/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3647843420505524
Epoch: 9737, Batch Gradient Norm: 9.220842116338469
Epoch: 9737, Batch Gradient Norm after: 9.220842116338469
Epoch 9738/10000, Prediction Accuracy = 63.576%, Loss = 0.3537634193897247
Epoch: 9738, Batch Gradient Norm: 10.69653289441862
Epoch: 9738, Batch Gradient Norm after: 10.69653289441862
Epoch 9739/10000, Prediction Accuracy = 63.548%, Loss = 0.36223493218421937
Epoch: 9739, Batch Gradient Norm: 10.415457970028578
Epoch: 9739, Batch Gradient Norm after: 10.415457970028578
Epoch 9740/10000, Prediction Accuracy = 63.89200000000001%, Loss = 0.36225456595420835
Epoch: 9740, Batch Gradient Norm: 12.187431391791497
Epoch: 9740, Batch Gradient Norm after: 12.187431391791497
Epoch 9741/10000, Prediction Accuracy = 63.730000000000004%, Loss = 0.3779872238636017
Epoch: 9741, Batch Gradient Norm: 11.180474226776932
Epoch: 9741, Batch Gradient Norm after: 11.180474226776932
Epoch 9742/10000, Prediction Accuracy = 63.874%, Loss = 0.3667709469795227
Epoch: 9742, Batch Gradient Norm: 9.0545389682414
Epoch: 9742, Batch Gradient Norm after: 9.0545389682414
Epoch 9743/10000, Prediction Accuracy = 64.008%, Loss = 0.3520100831985474
Epoch: 9743, Batch Gradient Norm: 10.916248843770209
Epoch: 9743, Batch Gradient Norm after: 10.916248843770209
Epoch 9744/10000, Prediction Accuracy = 63.56400000000001%, Loss = 0.3629392087459564
Epoch: 9744, Batch Gradient Norm: 10.157560461204655
Epoch: 9744, Batch Gradient Norm after: 10.157560461204655
Epoch 9745/10000, Prediction Accuracy = 63.69000000000001%, Loss = 0.358228474855423
Epoch: 9745, Batch Gradient Norm: 9.409297171486577
Epoch: 9745, Batch Gradient Norm after: 9.409297171486577
Epoch 9746/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.35407772064208987
Epoch: 9746, Batch Gradient Norm: 9.981287301186363
Epoch: 9746, Batch Gradient Norm after: 9.981287301186363
Epoch 9747/10000, Prediction Accuracy = 63.739999999999995%, Loss = 0.3566731154918671
Epoch: 9747, Batch Gradient Norm: 9.870791980646052
Epoch: 9747, Batch Gradient Norm after: 9.870791980646052
Epoch 9748/10000, Prediction Accuracy = 63.718%, Loss = 0.35820590853691103
Epoch: 9748, Batch Gradient Norm: 11.348205494821748
Epoch: 9748, Batch Gradient Norm after: 11.348205494821748
Epoch 9749/10000, Prediction Accuracy = 63.9%, Loss = 0.3662989914417267
Epoch: 9749, Batch Gradient Norm: 10.196692734735986
Epoch: 9749, Batch Gradient Norm after: 10.196692734735986
Epoch 9750/10000, Prediction Accuracy = 63.734%, Loss = 0.35934625267982484
Epoch: 9750, Batch Gradient Norm: 10.382727718833918
Epoch: 9750, Batch Gradient Norm after: 10.382727718833918
Epoch 9751/10000, Prediction Accuracy = 63.824%, Loss = 0.3623736321926117
Epoch: 9751, Batch Gradient Norm: 8.171690441932817
Epoch: 9751, Batch Gradient Norm after: 8.171690441932817
Epoch 9752/10000, Prediction Accuracy = 63.79599999999999%, Loss = 0.3468144655227661
Epoch: 9752, Batch Gradient Norm: 9.312450436566007
Epoch: 9752, Batch Gradient Norm after: 9.312450436566007
Epoch 9753/10000, Prediction Accuracy = 63.80999999999999%, Loss = 0.3539409518241882
Epoch: 9753, Batch Gradient Norm: 10.874148917414784
Epoch: 9753, Batch Gradient Norm after: 10.874148917414784
Epoch 9754/10000, Prediction Accuracy = 63.626%, Loss = 0.3656515538692474
Epoch: 9754, Batch Gradient Norm: 8.396363578430838
Epoch: 9754, Batch Gradient Norm after: 8.396363578430838
Epoch 9755/10000, Prediction Accuracy = 63.907999999999994%, Loss = 0.3478387176990509
Epoch: 9755, Batch Gradient Norm: 8.677612697690822
Epoch: 9755, Batch Gradient Norm after: 8.677612697690822
Epoch 9756/10000, Prediction Accuracy = 63.834%, Loss = 0.34785373210906984
Epoch: 9756, Batch Gradient Norm: 9.914350511104429
Epoch: 9756, Batch Gradient Norm after: 9.914350511104429
Epoch 9757/10000, Prediction Accuracy = 63.74400000000001%, Loss = 0.3545468389987946
Epoch: 9757, Batch Gradient Norm: 10.164425992762874
Epoch: 9757, Batch Gradient Norm after: 10.164425992762874
Epoch 9758/10000, Prediction Accuracy = 63.706%, Loss = 0.36014068126678467
Epoch: 9758, Batch Gradient Norm: 8.922145311389238
Epoch: 9758, Batch Gradient Norm after: 8.922145311389238
Epoch 9759/10000, Prediction Accuracy = 63.86600000000001%, Loss = 0.3502105712890625
Epoch: 9759, Batch Gradient Norm: 9.059723864013321
Epoch: 9759, Batch Gradient Norm after: 9.059723864013321
Epoch 9760/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3522561967372894
Epoch: 9760, Batch Gradient Norm: 8.762912761041319
Epoch: 9760, Batch Gradient Norm after: 8.762912761041319
Epoch 9761/10000, Prediction Accuracy = 63.802%, Loss = 0.350979220867157
Epoch: 9761, Batch Gradient Norm: 9.485382674597227
Epoch: 9761, Batch Gradient Norm after: 9.485382674597227
Epoch 9762/10000, Prediction Accuracy = 63.746%, Loss = 0.3518377423286438
Epoch: 9762, Batch Gradient Norm: 11.630493157681322
Epoch: 9762, Batch Gradient Norm after: 11.630493157681322
Epoch 9763/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3676207959651947
Epoch: 9763, Batch Gradient Norm: 12.695128644997252
Epoch: 9763, Batch Gradient Norm after: 12.695128644997252
Epoch 9764/10000, Prediction Accuracy = 63.760000000000005%, Loss = 0.38127251863479616
Epoch: 9764, Batch Gradient Norm: 13.720192162612896
Epoch: 9764, Batch Gradient Norm after: 13.720192162612896
Epoch 9765/10000, Prediction Accuracy = 63.648%, Loss = 0.38527131676673887
Epoch: 9765, Batch Gradient Norm: 9.779173033300792
Epoch: 9765, Batch Gradient Norm after: 9.779173033300792
Epoch 9766/10000, Prediction Accuracy = 63.86399999999999%, Loss = 0.3580688536167145
Epoch: 9766, Batch Gradient Norm: 8.416863433747752
Epoch: 9766, Batch Gradient Norm after: 8.416863433747752
Epoch 9767/10000, Prediction Accuracy = 63.88599999999999%, Loss = 0.34871745109558105
Epoch: 9767, Batch Gradient Norm: 8.597652070397762
Epoch: 9767, Batch Gradient Norm after: 8.597652070397762
Epoch 9768/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.34893816113471987
Epoch: 9768, Batch Gradient Norm: 10.865550181388953
Epoch: 9768, Batch Gradient Norm after: 10.865550181388953
Epoch 9769/10000, Prediction Accuracy = 63.742%, Loss = 0.3657423257827759
Epoch: 9769, Batch Gradient Norm: 7.923975289254874
Epoch: 9769, Batch Gradient Norm after: 7.923975289254874
Epoch 9770/10000, Prediction Accuracy = 63.864%, Loss = 0.34708787202835084
Epoch: 9770, Batch Gradient Norm: 8.358261629517937
Epoch: 9770, Batch Gradient Norm after: 8.358261629517937
Epoch 9771/10000, Prediction Accuracy = 63.908%, Loss = 0.3458283841609955
Epoch: 9771, Batch Gradient Norm: 9.138099928223877
Epoch: 9771, Batch Gradient Norm after: 9.138099928223877
Epoch 9772/10000, Prediction Accuracy = 63.92%, Loss = 0.3529280245304108
Epoch: 9772, Batch Gradient Norm: 8.63572774459725
Epoch: 9772, Batch Gradient Norm after: 8.63572774459725
Epoch 9773/10000, Prediction Accuracy = 63.73%, Loss = 0.34879312515258787
Epoch: 9773, Batch Gradient Norm: 9.281045868223124
Epoch: 9773, Batch Gradient Norm after: 9.281045868223124
Epoch 9774/10000, Prediction Accuracy = 63.572%, Loss = 0.3492931127548218
Epoch: 9774, Batch Gradient Norm: 10.409458609508519
Epoch: 9774, Batch Gradient Norm after: 10.409458609508519
Epoch 9775/10000, Prediction Accuracy = 63.629999999999995%, Loss = 0.35819230079650877
Epoch: 9775, Batch Gradient Norm: 9.268409341151802
Epoch: 9775, Batch Gradient Norm after: 9.268409341151802
Epoch 9776/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.35301187038421633
Epoch: 9776, Batch Gradient Norm: 10.537035463986827
Epoch: 9776, Batch Gradient Norm after: 10.537035463986827
Epoch 9777/10000, Prediction Accuracy = 63.720000000000006%, Loss = 0.3567658305168152
Epoch: 9777, Batch Gradient Norm: 11.12360593772689
Epoch: 9777, Batch Gradient Norm after: 11.12360593772689
Epoch 9778/10000, Prediction Accuracy = 63.734%, Loss = 0.36268713474273684
Epoch: 9778, Batch Gradient Norm: 13.04364730283772
Epoch: 9778, Batch Gradient Norm after: 13.04364730283772
Epoch 9779/10000, Prediction Accuracy = 63.858000000000004%, Loss = 0.3835075080394745
Epoch: 9779, Batch Gradient Norm: 10.522855688293461
Epoch: 9779, Batch Gradient Norm after: 10.522855688293461
Epoch 9780/10000, Prediction Accuracy = 63.720000000000006%, Loss = 0.364085978269577
Epoch: 9780, Batch Gradient Norm: 9.242981392287259
Epoch: 9780, Batch Gradient Norm after: 9.242981392287259
Epoch 9781/10000, Prediction Accuracy = 63.734%, Loss = 0.3542700171470642
Epoch: 9781, Batch Gradient Norm: 10.551211495722583
Epoch: 9781, Batch Gradient Norm after: 10.551211495722583
Epoch 9782/10000, Prediction Accuracy = 63.80800000000001%, Loss = 0.3636091947555542
Epoch: 9782, Batch Gradient Norm: 8.790594537769831
Epoch: 9782, Batch Gradient Norm after: 8.790594537769831
Epoch 9783/10000, Prediction Accuracy = 63.694%, Loss = 0.3521860420703888
Epoch: 9783, Batch Gradient Norm: 9.666797489244473
Epoch: 9783, Batch Gradient Norm after: 9.666797489244473
Epoch 9784/10000, Prediction Accuracy = 63.886%, Loss = 0.35465705394744873
Epoch: 9784, Batch Gradient Norm: 9.831857750403122
Epoch: 9784, Batch Gradient Norm after: 9.831857750403122
Epoch 9785/10000, Prediction Accuracy = 63.81199999999999%, Loss = 0.35307403206825255
Epoch: 9785, Batch Gradient Norm: 10.078730152734419
Epoch: 9785, Batch Gradient Norm after: 10.078730152734419
Epoch 9786/10000, Prediction Accuracy = 63.779999999999994%, Loss = 0.3581220805644989
Epoch: 9786, Batch Gradient Norm: 9.2491105312261
Epoch: 9786, Batch Gradient Norm after: 9.2491105312261
Epoch 9787/10000, Prediction Accuracy = 63.68399999999999%, Loss = 0.3498219668865204
Epoch: 9787, Batch Gradient Norm: 11.976651677532857
Epoch: 9787, Batch Gradient Norm after: 11.976651677532857
Epoch 9788/10000, Prediction Accuracy = 63.681999999999995%, Loss = 0.3693310797214508
Epoch: 9788, Batch Gradient Norm: 10.833738515938272
Epoch: 9788, Batch Gradient Norm after: 10.833738515938272
Epoch 9789/10000, Prediction Accuracy = 63.838%, Loss = 0.3647548079490662
Epoch: 9789, Batch Gradient Norm: 9.207285038597124
Epoch: 9789, Batch Gradient Norm after: 9.207285038597124
Epoch 9790/10000, Prediction Accuracy = 63.962%, Loss = 0.35487101078033445
Epoch: 9790, Batch Gradient Norm: 6.883997626369777
Epoch: 9790, Batch Gradient Norm after: 6.883997626369777
Epoch 9791/10000, Prediction Accuracy = 63.980000000000004%, Loss = 0.3405343949794769
Epoch: 9791, Batch Gradient Norm: 10.56062624780297
Epoch: 9791, Batch Gradient Norm after: 10.56062624780297
Epoch 9792/10000, Prediction Accuracy = 63.733999999999995%, Loss = 0.3637539505958557
Epoch: 9792, Batch Gradient Norm: 9.936438804488251
Epoch: 9792, Batch Gradient Norm after: 9.936438804488251
Epoch 9793/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.35845752954483034
Epoch: 9793, Batch Gradient Norm: 9.7926491259977
Epoch: 9793, Batch Gradient Norm after: 9.7926491259977
Epoch 9794/10000, Prediction Accuracy = 63.646%, Loss = 0.3561924695968628
Epoch: 9794, Batch Gradient Norm: 9.097083227545415
Epoch: 9794, Batch Gradient Norm after: 9.097083227545415
Epoch 9795/10000, Prediction Accuracy = 63.988%, Loss = 0.3541170597076416
Epoch: 9795, Batch Gradient Norm: 8.779743293376427
Epoch: 9795, Batch Gradient Norm after: 8.779743293376427
Epoch 9796/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.34842408895492555
Epoch: 9796, Batch Gradient Norm: 10.167682359854586
Epoch: 9796, Batch Gradient Norm after: 10.167682359854586
Epoch 9797/10000, Prediction Accuracy = 63.791999999999994%, Loss = 0.35641645789146426
Epoch: 9797, Batch Gradient Norm: 8.635375285089204
Epoch: 9797, Batch Gradient Norm after: 8.635375285089204
Epoch 9798/10000, Prediction Accuracy = 63.788%, Loss = 0.3482240676879883
Epoch: 9798, Batch Gradient Norm: 12.174059138966548
Epoch: 9798, Batch Gradient Norm after: 12.174059138966548
Epoch 9799/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.371690970659256
Epoch: 9799, Batch Gradient Norm: 11.299027328450771
Epoch: 9799, Batch Gradient Norm after: 11.299027328450771
Epoch 9800/10000, Prediction Accuracy = 63.602%, Loss = 0.36820335388183595
Epoch: 9800, Batch Gradient Norm: 11.518664776883638
Epoch: 9800, Batch Gradient Norm after: 11.518664776883638
Epoch 9801/10000, Prediction Accuracy = 63.9%, Loss = 0.36894614100456236
Epoch: 9801, Batch Gradient Norm: 12.579406241106817
Epoch: 9801, Batch Gradient Norm after: 12.579406241106817
Epoch 9802/10000, Prediction Accuracy = 63.8%, Loss = 0.3796644449234009
Epoch: 9802, Batch Gradient Norm: 9.683642766530598
Epoch: 9802, Batch Gradient Norm after: 9.683642766530598
Epoch 9803/10000, Prediction Accuracy = 63.866%, Loss = 0.35371350646018984
Epoch: 9803, Batch Gradient Norm: 8.209434135264084
Epoch: 9803, Batch Gradient Norm after: 8.209434135264084
Epoch 9804/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.3453970491886139
Epoch: 9804, Batch Gradient Norm: 9.080442987131036
Epoch: 9804, Batch Gradient Norm after: 9.080442987131036
Epoch 9805/10000, Prediction Accuracy = 63.622%, Loss = 0.3503421127796173
Epoch: 9805, Batch Gradient Norm: 8.68297162421464
Epoch: 9805, Batch Gradient Norm after: 8.68297162421464
Epoch 9806/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.3509203791618347
Epoch: 9806, Batch Gradient Norm: 9.195796620023879
Epoch: 9806, Batch Gradient Norm after: 9.195796620023879
Epoch 9807/10000, Prediction Accuracy = 63.676%, Loss = 0.3525240957736969
Epoch: 9807, Batch Gradient Norm: 8.227468064762732
Epoch: 9807, Batch Gradient Norm after: 8.227468064762732
Epoch 9808/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.34342159032821656
Epoch: 9808, Batch Gradient Norm: 9.235935157163006
Epoch: 9808, Batch Gradient Norm after: 9.235935157163006
Epoch 9809/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.34951279163360593
Epoch: 9809, Batch Gradient Norm: 9.355861709743223
Epoch: 9809, Batch Gradient Norm after: 9.355861709743223
Epoch 9810/10000, Prediction Accuracy = 63.85600000000001%, Loss = 0.3515163898468018
Epoch: 9810, Batch Gradient Norm: 10.464171687824182
Epoch: 9810, Batch Gradient Norm after: 10.464171687824182
Epoch 9811/10000, Prediction Accuracy = 63.742%, Loss = 0.363603812456131
Epoch: 9811, Batch Gradient Norm: 9.121648348944277
Epoch: 9811, Batch Gradient Norm after: 9.121648348944277
Epoch 9812/10000, Prediction Accuracy = 63.826%, Loss = 0.3539172947406769
Epoch: 9812, Batch Gradient Norm: 10.738965128920826
Epoch: 9812, Batch Gradient Norm after: 10.738965128920826
Epoch 9813/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.3617818713188171
Epoch: 9813, Batch Gradient Norm: 12.521038990606945
Epoch: 9813, Batch Gradient Norm after: 12.521038990606945
Epoch 9814/10000, Prediction Accuracy = 63.584%, Loss = 0.3735314726829529
Epoch: 9814, Batch Gradient Norm: 9.554720720045397
Epoch: 9814, Batch Gradient Norm after: 9.554720720045397
Epoch 9815/10000, Prediction Accuracy = 63.908%, Loss = 0.3528742492198944
Epoch: 9815, Batch Gradient Norm: 8.662647711799792
Epoch: 9815, Batch Gradient Norm after: 8.662647711799792
Epoch 9816/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.3463421642780304
Epoch: 9816, Batch Gradient Norm: 13.477697130541308
Epoch: 9816, Batch Gradient Norm after: 13.477697130541308
Epoch 9817/10000, Prediction Accuracy = 63.648%, Loss = 0.3854042708873749
Epoch: 9817, Batch Gradient Norm: 11.641920044106623
Epoch: 9817, Batch Gradient Norm after: 11.641920044106623
Epoch 9818/10000, Prediction Accuracy = 63.636%, Loss = 0.3654007017612457
Epoch: 9818, Batch Gradient Norm: 9.188823689770254
Epoch: 9818, Batch Gradient Norm after: 9.188823689770254
Epoch 9819/10000, Prediction Accuracy = 63.70799999999999%, Loss = 0.3515471637248993
Epoch: 9819, Batch Gradient Norm: 8.017933794287575
Epoch: 9819, Batch Gradient Norm after: 8.017933794287575
Epoch 9820/10000, Prediction Accuracy = 63.69%, Loss = 0.3445824146270752
Epoch: 9820, Batch Gradient Norm: 10.668947180536236
Epoch: 9820, Batch Gradient Norm after: 10.668947180536236
Epoch 9821/10000, Prediction Accuracy = 63.722%, Loss = 0.35970929861068723
Epoch: 9821, Batch Gradient Norm: 9.810956710602833
Epoch: 9821, Batch Gradient Norm after: 9.810956710602833
Epoch 9822/10000, Prediction Accuracy = 63.674%, Loss = 0.3569047927856445
Epoch: 9822, Batch Gradient Norm: 8.14522825563004
Epoch: 9822, Batch Gradient Norm after: 8.14522825563004
Epoch 9823/10000, Prediction Accuracy = 63.64%, Loss = 0.3457472324371338
Epoch: 9823, Batch Gradient Norm: 10.236639827848693
Epoch: 9823, Batch Gradient Norm after: 10.236639827848693
Epoch 9824/10000, Prediction Accuracy = 63.74000000000001%, Loss = 0.3612590551376343
Epoch: 9824, Batch Gradient Norm: 9.236482285493127
Epoch: 9824, Batch Gradient Norm after: 9.236482285493127
Epoch 9825/10000, Prediction Accuracy = 63.761999999999986%, Loss = 0.3519951343536377
Epoch: 9825, Batch Gradient Norm: 11.209652987806074
Epoch: 9825, Batch Gradient Norm after: 11.209652987806074
Epoch 9826/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.3644572854042053
Epoch: 9826, Batch Gradient Norm: 7.90712795015134
Epoch: 9826, Batch Gradient Norm after: 7.90712795015134
Epoch 9827/10000, Prediction Accuracy = 63.971999999999994%, Loss = 0.34123902916908266
Epoch: 9827, Batch Gradient Norm: 9.189442405079845
Epoch: 9827, Batch Gradient Norm after: 9.189442405079845
Epoch 9828/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.34881815910339353
Epoch: 9828, Batch Gradient Norm: 12.907955226090706
Epoch: 9828, Batch Gradient Norm after: 12.907955226090706
Epoch 9829/10000, Prediction Accuracy = 63.604%, Loss = 0.38429032564163207
Epoch: 9829, Batch Gradient Norm: 9.506610742273473
Epoch: 9829, Batch Gradient Norm after: 9.506610742273473
Epoch 9830/10000, Prediction Accuracy = 63.754%, Loss = 0.35439610481262207
Epoch: 9830, Batch Gradient Norm: 9.545307984143811
Epoch: 9830, Batch Gradient Norm after: 9.545307984143811
Epoch 9831/10000, Prediction Accuracy = 63.926%, Loss = 0.35317516326904297
Epoch: 9831, Batch Gradient Norm: 9.656248697430309
Epoch: 9831, Batch Gradient Norm after: 9.656248697430309
Epoch 9832/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.35561650395393374
Epoch: 9832, Batch Gradient Norm: 10.731390245587574
Epoch: 9832, Batch Gradient Norm after: 10.731390245587574
Epoch 9833/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.3643862783908844
Epoch: 9833, Batch Gradient Norm: 11.727785192376482
Epoch: 9833, Batch Gradient Norm after: 11.727785192376482
Epoch 9834/10000, Prediction Accuracy = 63.666%, Loss = 0.375747936964035
Epoch: 9834, Batch Gradient Norm: 11.276524162240829
Epoch: 9834, Batch Gradient Norm after: 11.276524162240829
Epoch 9835/10000, Prediction Accuracy = 63.448%, Loss = 0.36662370562553404
Epoch: 9835, Batch Gradient Norm: 7.881679461641532
Epoch: 9835, Batch Gradient Norm after: 7.881679461641532
Epoch 9836/10000, Prediction Accuracy = 63.886%, Loss = 0.34309213161468505
Epoch: 9836, Batch Gradient Norm: 7.317008319809849
Epoch: 9836, Batch Gradient Norm after: 7.317008319809849
Epoch 9837/10000, Prediction Accuracy = 63.772000000000006%, Loss = 0.34177224040031434
Epoch: 9837, Batch Gradient Norm: 9.415311075172117
Epoch: 9837, Batch Gradient Norm after: 9.415311075172117
Epoch 9838/10000, Prediction Accuracy = 63.81600000000001%, Loss = 0.3522972226142883
Epoch: 9838, Batch Gradient Norm: 9.31044969015925
Epoch: 9838, Batch Gradient Norm after: 9.31044969015925
Epoch 9839/10000, Prediction Accuracy = 63.658%, Loss = 0.35273940563201905
Epoch: 9839, Batch Gradient Norm: 9.082865801022196
Epoch: 9839, Batch Gradient Norm after: 9.082865801022196
Epoch 9840/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3511867344379425
Epoch: 9840, Batch Gradient Norm: 11.86558233898164
Epoch: 9840, Batch Gradient Norm after: 11.86558233898164
Epoch 9841/10000, Prediction Accuracy = 63.64%, Loss = 0.3743240237236023
Epoch: 9841, Batch Gradient Norm: 12.272817313857681
Epoch: 9841, Batch Gradient Norm after: 12.272817313857681
Epoch 9842/10000, Prediction Accuracy = 63.568%, Loss = 0.37219858169555664
Epoch: 9842, Batch Gradient Norm: 6.831360107002162
Epoch: 9842, Batch Gradient Norm after: 6.831360107002162
Epoch 9843/10000, Prediction Accuracy = 63.94%, Loss = 0.3362128913402557
Epoch: 9843, Batch Gradient Norm: 6.944239397915423
Epoch: 9843, Batch Gradient Norm after: 6.944239397915423
Epoch 9844/10000, Prediction Accuracy = 63.769999999999996%, Loss = 0.3397855877876282
Epoch: 9844, Batch Gradient Norm: 9.529517851309263
Epoch: 9844, Batch Gradient Norm after: 9.529517851309263
Epoch 9845/10000, Prediction Accuracy = 63.902%, Loss = 0.3543359041213989
Epoch: 9845, Batch Gradient Norm: 8.437160476742875
Epoch: 9845, Batch Gradient Norm after: 8.437160476742875
Epoch 9846/10000, Prediction Accuracy = 63.624%, Loss = 0.3496230185031891
Epoch: 9846, Batch Gradient Norm: 9.699919059514919
Epoch: 9846, Batch Gradient Norm after: 9.699919059514919
Epoch 9847/10000, Prediction Accuracy = 63.762%, Loss = 0.3577310562133789
Epoch: 9847, Batch Gradient Norm: 10.02491569807374
Epoch: 9847, Batch Gradient Norm after: 10.02491569807374
Epoch 9848/10000, Prediction Accuracy = 63.67199999999999%, Loss = 0.3566754937171936
Epoch: 9848, Batch Gradient Norm: 13.805938408826886
Epoch: 9848, Batch Gradient Norm after: 13.805938408826886
Epoch 9849/10000, Prediction Accuracy = 63.58%, Loss = 0.39339942336082456
Epoch: 9849, Batch Gradient Norm: 11.055282927217544
Epoch: 9849, Batch Gradient Norm after: 11.055282927217544
Epoch 9850/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.36481423377990724
Epoch: 9850, Batch Gradient Norm: 9.869063018597778
Epoch: 9850, Batch Gradient Norm after: 9.869063018597778
Epoch 9851/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.35328211188316344
Epoch: 9851, Batch Gradient Norm: 9.508895945160456
Epoch: 9851, Batch Gradient Norm after: 9.508895945160456
Epoch 9852/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.35364163517951963
Epoch: 9852, Batch Gradient Norm: 11.153697224593234
Epoch: 9852, Batch Gradient Norm after: 11.153697224593234
Epoch 9853/10000, Prediction Accuracy = 63.822%, Loss = 0.3661733210086823
Epoch: 9853, Batch Gradient Norm: 10.63060596446234
Epoch: 9853, Batch Gradient Norm after: 10.63060596446234
Epoch 9854/10000, Prediction Accuracy = 63.974000000000004%, Loss = 0.3591332077980042
Epoch: 9854, Batch Gradient Norm: 9.569497280553842
Epoch: 9854, Batch Gradient Norm after: 9.569497280553842
Epoch 9855/10000, Prediction Accuracy = 63.733999999999995%, Loss = 0.35146464705467223
Epoch: 9855, Batch Gradient Norm: 11.384242913819337
Epoch: 9855, Batch Gradient Norm after: 11.384242913819337
Epoch 9856/10000, Prediction Accuracy = 63.786%, Loss = 0.36500113010406493
Epoch: 9856, Batch Gradient Norm: 12.307136451267679
Epoch: 9856, Batch Gradient Norm after: 12.307136451267679
Epoch 9857/10000, Prediction Accuracy = 63.749999999999986%, Loss = 0.37618203163146974
Epoch: 9857, Batch Gradient Norm: 10.648176395717787
Epoch: 9857, Batch Gradient Norm after: 10.648176395717787
Epoch 9858/10000, Prediction Accuracy = 63.834%, Loss = 0.36097789406776426
Epoch: 9858, Batch Gradient Norm: 9.83817352679056
Epoch: 9858, Batch Gradient Norm after: 9.83817352679056
Epoch 9859/10000, Prediction Accuracy = 63.798%, Loss = 0.35267671942710876
Epoch: 9859, Batch Gradient Norm: 8.468440036664749
Epoch: 9859, Batch Gradient Norm after: 8.468440036664749
Epoch 9860/10000, Prediction Accuracy = 63.634%, Loss = 0.34550036787986754
Epoch: 9860, Batch Gradient Norm: 9.456262015137236
Epoch: 9860, Batch Gradient Norm after: 9.456262015137236
Epoch 9861/10000, Prediction Accuracy = 63.834%, Loss = 0.3539606213569641
Epoch: 9861, Batch Gradient Norm: 9.614165789419888
Epoch: 9861, Batch Gradient Norm after: 9.614165789419888
Epoch 9862/10000, Prediction Accuracy = 63.928%, Loss = 0.3563005864620209
Epoch: 9862, Batch Gradient Norm: 10.3929301191326
Epoch: 9862, Batch Gradient Norm after: 10.3929301191326
Epoch 9863/10000, Prediction Accuracy = 63.71200000000001%, Loss = 0.35820519328117373
Epoch: 9863, Batch Gradient Norm: 11.092208228787738
Epoch: 9863, Batch Gradient Norm after: 11.092208228787738
Epoch 9864/10000, Prediction Accuracy = 63.69000000000001%, Loss = 0.3612923562526703
Epoch: 9864, Batch Gradient Norm: 10.777429753297474
Epoch: 9864, Batch Gradient Norm after: 10.777429753297474
Epoch 9865/10000, Prediction Accuracy = 63.80799999999999%, Loss = 0.3652944803237915
Epoch: 9865, Batch Gradient Norm: 8.560983757234993
Epoch: 9865, Batch Gradient Norm after: 8.560983757234993
Epoch 9866/10000, Prediction Accuracy = 64.026%, Loss = 0.34966684579849244
Epoch: 9866, Batch Gradient Norm: 9.237953825374003
Epoch: 9866, Batch Gradient Norm after: 9.237953825374003
Epoch 9867/10000, Prediction Accuracy = 63.886%, Loss = 0.35419767498970034
Epoch: 9867, Batch Gradient Norm: 8.18580965070801
Epoch: 9867, Batch Gradient Norm after: 8.18580965070801
Epoch 9868/10000, Prediction Accuracy = 63.798%, Loss = 0.34728965163230896
Epoch: 9868, Batch Gradient Norm: 6.9862131815079245
Epoch: 9868, Batch Gradient Norm after: 6.9862131815079245
Epoch 9869/10000, Prediction Accuracy = 64.00200000000001%, Loss = 0.3374089777469635
Epoch: 9869, Batch Gradient Norm: 9.954588189208033
Epoch: 9869, Batch Gradient Norm after: 9.954588189208033
Epoch 9870/10000, Prediction Accuracy = 63.908%, Loss = 0.35481430888175963
Epoch: 9870, Batch Gradient Norm: 10.976354231409216
Epoch: 9870, Batch Gradient Norm after: 10.976354231409216
Epoch 9871/10000, Prediction Accuracy = 63.788%, Loss = 0.36237794160842896
Epoch: 9871, Batch Gradient Norm: 9.725261942499213
Epoch: 9871, Batch Gradient Norm after: 9.725261942499213
Epoch 9872/10000, Prediction Accuracy = 63.624%, Loss = 0.3559340059757233
Epoch: 9872, Batch Gradient Norm: 8.197183637456066
Epoch: 9872, Batch Gradient Norm after: 8.197183637456066
Epoch 9873/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.3488840937614441
Epoch: 9873, Batch Gradient Norm: 9.852849925317749
Epoch: 9873, Batch Gradient Norm after: 9.852849925317749
Epoch 9874/10000, Prediction Accuracy = 63.91799999999999%, Loss = 0.35890236496925354
Epoch: 9874, Batch Gradient Norm: 11.405635867099432
Epoch: 9874, Batch Gradient Norm after: 11.405635867099432
Epoch 9875/10000, Prediction Accuracy = 63.85%, Loss = 0.3687426269054413
Epoch: 9875, Batch Gradient Norm: 9.58211153226385
Epoch: 9875, Batch Gradient Norm after: 9.58211153226385
Epoch 9876/10000, Prediction Accuracy = 63.831999999999994%, Loss = 0.35443550944328306
Epoch: 9876, Batch Gradient Norm: 9.258759120688008
Epoch: 9876, Batch Gradient Norm after: 9.258759120688008
Epoch 9877/10000, Prediction Accuracy = 63.815999999999995%, Loss = 0.3544473469257355
Epoch: 9877, Batch Gradient Norm: 10.736490997989053
Epoch: 9877, Batch Gradient Norm after: 10.736490997989053
Epoch 9878/10000, Prediction Accuracy = 63.98599999999999%, Loss = 0.3612878918647766
Epoch: 9878, Batch Gradient Norm: 11.464746924601888
Epoch: 9878, Batch Gradient Norm after: 11.464746924601888
Epoch 9879/10000, Prediction Accuracy = 63.70399999999999%, Loss = 0.3619436383247375
Epoch: 9879, Batch Gradient Norm: 10.162454968974165
Epoch: 9879, Batch Gradient Norm after: 10.162454968974165
Epoch 9880/10000, Prediction Accuracy = 63.874%, Loss = 0.354467099905014
Epoch: 9880, Batch Gradient Norm: 9.88813169951606
Epoch: 9880, Batch Gradient Norm after: 9.88813169951606
Epoch 9881/10000, Prediction Accuracy = 63.824%, Loss = 0.35744487047195433
Epoch: 9881, Batch Gradient Norm: 7.754543814650167
Epoch: 9881, Batch Gradient Norm after: 7.754543814650167
Epoch 9882/10000, Prediction Accuracy = 63.720000000000006%, Loss = 0.34288503527641295
Epoch: 9882, Batch Gradient Norm: 11.868048512068121
Epoch: 9882, Batch Gradient Norm after: 11.868048512068121
Epoch 9883/10000, Prediction Accuracy = 63.775999999999996%, Loss = 0.36757566332817077
Epoch: 9883, Batch Gradient Norm: 11.381487313469085
Epoch: 9883, Batch Gradient Norm after: 11.381487313469085
Epoch 9884/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3660104513168335
Epoch: 9884, Batch Gradient Norm: 10.31040340959677
Epoch: 9884, Batch Gradient Norm after: 10.31040340959677
Epoch 9885/10000, Prediction Accuracy = 63.612%, Loss = 0.35518866777420044
Epoch: 9885, Batch Gradient Norm: 9.959242537571294
Epoch: 9885, Batch Gradient Norm after: 9.959242537571294
Epoch 9886/10000, Prediction Accuracy = 63.824%, Loss = 0.3540129840373993
Epoch: 9886, Batch Gradient Norm: 9.370690230212524
Epoch: 9886, Batch Gradient Norm after: 9.370690230212524
Epoch 9887/10000, Prediction Accuracy = 63.751999999999995%, Loss = 0.349822074174881
Epoch: 9887, Batch Gradient Norm: 11.953684871971983
Epoch: 9887, Batch Gradient Norm after: 11.953684871971983
Epoch 9888/10000, Prediction Accuracy = 63.706%, Loss = 0.3713728904724121
Epoch: 9888, Batch Gradient Norm: 8.70573948453815
Epoch: 9888, Batch Gradient Norm after: 8.70573948453815
Epoch 9889/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.3473339319229126
Epoch: 9889, Batch Gradient Norm: 9.798411421274997
Epoch: 9889, Batch Gradient Norm after: 9.798411421274997
Epoch 9890/10000, Prediction Accuracy = 63.754%, Loss = 0.35658897161483766
Epoch: 9890, Batch Gradient Norm: 9.304575296501563
Epoch: 9890, Batch Gradient Norm after: 9.304575296501563
Epoch 9891/10000, Prediction Accuracy = 63.884%, Loss = 0.35182005167007446
Epoch: 9891, Batch Gradient Norm: 7.687205419497194
Epoch: 9891, Batch Gradient Norm after: 7.687205419497194
Epoch 9892/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.34326085448265076
Epoch: 9892, Batch Gradient Norm: 10.03901554024344
Epoch: 9892, Batch Gradient Norm after: 10.03901554024344
Epoch 9893/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.35564101934432985
Epoch: 9893, Batch Gradient Norm: 10.373984584475194
Epoch: 9893, Batch Gradient Norm after: 10.373984584475194
Epoch 9894/10000, Prediction Accuracy = 63.745999999999995%, Loss = 0.35971187949180605
Epoch: 9894, Batch Gradient Norm: 11.451118950024934
Epoch: 9894, Batch Gradient Norm after: 11.451118950024934
Epoch 9895/10000, Prediction Accuracy = 63.617999999999995%, Loss = 0.36813663840293886
Epoch: 9895, Batch Gradient Norm: 10.921217659875824
Epoch: 9895, Batch Gradient Norm after: 10.921217659875824
Epoch 9896/10000, Prediction Accuracy = 63.80799999999999%, Loss = 0.3657014548778534
Epoch: 9896, Batch Gradient Norm: 7.802342360817401
Epoch: 9896, Batch Gradient Norm after: 7.802342360817401
Epoch 9897/10000, Prediction Accuracy = 64.08399999999999%, Loss = 0.3395447611808777
Epoch: 9897, Batch Gradient Norm: 9.464068566410782
Epoch: 9897, Batch Gradient Norm after: 9.464068566410782
Epoch 9898/10000, Prediction Accuracy = 63.726%, Loss = 0.3515655755996704
Epoch: 9898, Batch Gradient Norm: 10.817753051039297
Epoch: 9898, Batch Gradient Norm after: 10.817753051039297
Epoch 9899/10000, Prediction Accuracy = 63.678%, Loss = 0.36241663694381715
Epoch: 9899, Batch Gradient Norm: 10.899066844928928
Epoch: 9899, Batch Gradient Norm after: 10.899066844928928
Epoch 9900/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.3627449572086334
Epoch: 9900, Batch Gradient Norm: 10.203131764277224
Epoch: 9900, Batch Gradient Norm after: 10.203131764277224
Epoch 9901/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.35675097703933717
Epoch: 9901, Batch Gradient Norm: 10.088428628181937
Epoch: 9901, Batch Gradient Norm after: 10.088428628181937
Epoch 9902/10000, Prediction Accuracy = 63.98%, Loss = 0.3550046026706696
Epoch: 9902, Batch Gradient Norm: 9.867215826831456
Epoch: 9902, Batch Gradient Norm after: 9.867215826831456
Epoch 9903/10000, Prediction Accuracy = 63.794000000000004%, Loss = 0.3575351059436798
Epoch: 9903, Batch Gradient Norm: 7.8213070066522326
Epoch: 9903, Batch Gradient Norm after: 7.8213070066522326
Epoch 9904/10000, Prediction Accuracy = 63.852%, Loss = 0.3402350842952728
Epoch: 9904, Batch Gradient Norm: 8.468936301338749
Epoch: 9904, Batch Gradient Norm after: 8.468936301338749
Epoch 9905/10000, Prediction Accuracy = 63.88199999999999%, Loss = 0.34503353834152223
Epoch: 9905, Batch Gradient Norm: 9.546955706005711
Epoch: 9905, Batch Gradient Norm after: 9.546955706005711
Epoch 9906/10000, Prediction Accuracy = 63.757999999999996%, Loss = 0.35264060497283933
Epoch: 9906, Batch Gradient Norm: 9.787283772418018
Epoch: 9906, Batch Gradient Norm after: 9.787283772418018
Epoch 9907/10000, Prediction Accuracy = 63.686%, Loss = 0.35329893231391907
Epoch: 9907, Batch Gradient Norm: 9.304796645451272
Epoch: 9907, Batch Gradient Norm after: 9.304796645451272
Epoch 9908/10000, Prediction Accuracy = 63.82199999999999%, Loss = 0.35199509263038636
Epoch: 9908, Batch Gradient Norm: 11.649552452360462
Epoch: 9908, Batch Gradient Norm after: 11.649552452360462
Epoch 9909/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3726079881191254
Epoch: 9909, Batch Gradient Norm: 10.326682873137143
Epoch: 9909, Batch Gradient Norm after: 10.326682873137143
Epoch 9910/10000, Prediction Accuracy = 63.83599999999999%, Loss = 0.36182321310043336
Epoch: 9910, Batch Gradient Norm: 12.097535458706401
Epoch: 9910, Batch Gradient Norm after: 12.097535458706401
Epoch 9911/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.37314934730529786
Epoch: 9911, Batch Gradient Norm: 9.244533136100017
Epoch: 9911, Batch Gradient Norm after: 9.244533136100017
Epoch 9912/10000, Prediction Accuracy = 63.854%, Loss = 0.35108366012573244
Epoch: 9912, Batch Gradient Norm: 8.122076697282322
Epoch: 9912, Batch Gradient Norm after: 8.122076697282322
Epoch 9913/10000, Prediction Accuracy = 63.882000000000005%, Loss = 0.34323516488075256
Epoch: 9913, Batch Gradient Norm: 9.962570731011152
Epoch: 9913, Batch Gradient Norm after: 9.962570731011152
Epoch 9914/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3551711142063141
Epoch: 9914, Batch Gradient Norm: 10.535315307812844
Epoch: 9914, Batch Gradient Norm after: 10.535315307812844
Epoch 9915/10000, Prediction Accuracy = 63.664%, Loss = 0.361413961648941
Epoch: 9915, Batch Gradient Norm: 8.484680157030615
Epoch: 9915, Batch Gradient Norm after: 8.484680157030615
Epoch 9916/10000, Prediction Accuracy = 63.73%, Loss = 0.3487178087234497
Epoch: 9916, Batch Gradient Norm: 9.503769605313835
Epoch: 9916, Batch Gradient Norm after: 9.503769605313835
Epoch 9917/10000, Prediction Accuracy = 63.896%, Loss = 0.3504184424877167
Epoch: 9917, Batch Gradient Norm: 13.393652909573778
Epoch: 9917, Batch Gradient Norm after: 13.393652909573778
Epoch 9918/10000, Prediction Accuracy = 63.718%, Loss = 0.3806087851524353
Epoch: 9918, Batch Gradient Norm: 11.80488760160432
Epoch: 9918, Batch Gradient Norm after: 11.80488760160432
Epoch 9919/10000, Prediction Accuracy = 63.824%, Loss = 0.3694271147251129
Epoch: 9919, Batch Gradient Norm: 10.618176884187411
Epoch: 9919, Batch Gradient Norm after: 10.618176884187411
Epoch 9920/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.36048129200935364
Epoch: 9920, Batch Gradient Norm: 8.141603676841761
Epoch: 9920, Batch Gradient Norm after: 8.141603676841761
Epoch 9921/10000, Prediction Accuracy = 63.838%, Loss = 0.34409765601158143
Epoch: 9921, Batch Gradient Norm: 8.47607202276232
Epoch: 9921, Batch Gradient Norm after: 8.47607202276232
Epoch 9922/10000, Prediction Accuracy = 63.862%, Loss = 0.3436870574951172
Epoch: 9922, Batch Gradient Norm: 10.050169020649928
Epoch: 9922, Batch Gradient Norm after: 10.050169020649928
Epoch 9923/10000, Prediction Accuracy = 63.646%, Loss = 0.35257304906845094
Epoch: 9923, Batch Gradient Norm: 10.36023217902017
Epoch: 9923, Batch Gradient Norm after: 10.36023217902017
Epoch 9924/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.3562695860862732
Epoch: 9924, Batch Gradient Norm: 10.50114487720587
Epoch: 9924, Batch Gradient Norm after: 10.50114487720587
Epoch 9925/10000, Prediction Accuracy = 63.8%, Loss = 0.35958938002586366
Epoch: 9925, Batch Gradient Norm: 10.125895467733827
Epoch: 9925, Batch Gradient Norm after: 10.125895467733827
Epoch 9926/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.35683984160423277
Epoch: 9926, Batch Gradient Norm: 10.30267634945641
Epoch: 9926, Batch Gradient Norm after: 10.30267634945641
Epoch 9927/10000, Prediction Accuracy = 63.872%, Loss = 0.3569618403911591
Epoch: 9927, Batch Gradient Norm: 8.997277314626167
Epoch: 9927, Batch Gradient Norm after: 8.997277314626167
Epoch 9928/10000, Prediction Accuracy = 63.815999999999995%, Loss = 0.3502475380897522
Epoch: 9928, Batch Gradient Norm: 9.699749595194655
Epoch: 9928, Batch Gradient Norm after: 9.699749595194655
Epoch 9929/10000, Prediction Accuracy = 63.73599999999999%, Loss = 0.3549211859703064
Epoch: 9929, Batch Gradient Norm: 8.675426062754092
Epoch: 9929, Batch Gradient Norm after: 8.675426062754092
Epoch 9930/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.3492585062980652
Epoch: 9930, Batch Gradient Norm: 7.738376250414589
Epoch: 9930, Batch Gradient Norm after: 7.738376250414589
Epoch 9931/10000, Prediction Accuracy = 63.96600000000001%, Loss = 0.34267243146896365
Epoch: 9931, Batch Gradient Norm: 10.85437716018743
Epoch: 9931, Batch Gradient Norm after: 10.85437716018743
Epoch 9932/10000, Prediction Accuracy = 63.87199999999999%, Loss = 0.36154723167419434
Epoch: 9932, Batch Gradient Norm: 11.324896206940211
Epoch: 9932, Batch Gradient Norm after: 11.324896206940211
Epoch 9933/10000, Prediction Accuracy = 63.938%, Loss = 0.36268559098243713
Epoch: 9933, Batch Gradient Norm: 10.59984586655526
Epoch: 9933, Batch Gradient Norm after: 10.59984586655526
Epoch 9934/10000, Prediction Accuracy = 63.970000000000006%, Loss = 0.3620177567005157
Epoch: 9934, Batch Gradient Norm: 9.390988690373051
Epoch: 9934, Batch Gradient Norm after: 9.390988690373051
Epoch 9935/10000, Prediction Accuracy = 63.89399999999999%, Loss = 0.35032469034194946
Epoch: 9935, Batch Gradient Norm: 8.976074403671776
Epoch: 9935, Batch Gradient Norm after: 8.976074403671776
Epoch 9936/10000, Prediction Accuracy = 63.924%, Loss = 0.34930369853973386
Epoch: 9936, Batch Gradient Norm: 11.426004114509508
Epoch: 9936, Batch Gradient Norm after: 11.426004114509508
Epoch 9937/10000, Prediction Accuracy = 63.95799999999999%, Loss = 0.36392902135849
Epoch: 9937, Batch Gradient Norm: 10.702514145289577
Epoch: 9937, Batch Gradient Norm after: 10.702514145289577
Epoch 9938/10000, Prediction Accuracy = 63.722%, Loss = 0.3597863256931305
Epoch: 9938, Batch Gradient Norm: 8.560103206876027
Epoch: 9938, Batch Gradient Norm after: 8.560103206876027
Epoch 9939/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.3469935476779938
Epoch: 9939, Batch Gradient Norm: 8.711514374020311
Epoch: 9939, Batch Gradient Norm after: 8.711514374020311
Epoch 9940/10000, Prediction Accuracy = 63.562%, Loss = 0.3444701671600342
Epoch: 9940, Batch Gradient Norm: 11.541856604166535
Epoch: 9940, Batch Gradient Norm after: 11.541856604166535
Epoch 9941/10000, Prediction Accuracy = 63.92999999999999%, Loss = 0.364557558298111
Epoch: 9941, Batch Gradient Norm: 10.688080196190691
Epoch: 9941, Batch Gradient Norm after: 10.688080196190691
Epoch 9942/10000, Prediction Accuracy = 63.89%, Loss = 0.3605735063552856
Epoch: 9942, Batch Gradient Norm: 9.36063903758812
Epoch: 9942, Batch Gradient Norm after: 9.36063903758812
Epoch 9943/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.35360822081565857
Epoch: 9943, Batch Gradient Norm: 7.554061899524174
Epoch: 9943, Batch Gradient Norm after: 7.554061899524174
Epoch 9944/10000, Prediction Accuracy = 63.89399999999999%, Loss = 0.34126251339912417
Epoch: 9944, Batch Gradient Norm: 10.021579749621027
Epoch: 9944, Batch Gradient Norm after: 10.021579749621027
Epoch 9945/10000, Prediction Accuracy = 63.827999999999996%, Loss = 0.3514352023601532
Epoch: 9945, Batch Gradient Norm: 11.271362262015485
Epoch: 9945, Batch Gradient Norm after: 11.271362262015485
Epoch 9946/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.36473206281661985
Epoch: 9946, Batch Gradient Norm: 10.38047567662447
Epoch: 9946, Batch Gradient Norm after: 10.38047567662447
Epoch 9947/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.36158721446990966
Epoch: 9947, Batch Gradient Norm: 11.514128622883906
Epoch: 9947, Batch Gradient Norm after: 11.514128622883906
Epoch 9948/10000, Prediction Accuracy = 63.83599999999999%, Loss = 0.37171236276626585
Epoch: 9948, Batch Gradient Norm: 7.979264850386158
Epoch: 9948, Batch Gradient Norm after: 7.979264850386158
Epoch 9949/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.3431182324886322
Epoch: 9949, Batch Gradient Norm: 9.70454071541601
Epoch: 9949, Batch Gradient Norm after: 9.70454071541601
Epoch 9950/10000, Prediction Accuracy = 63.812%, Loss = 0.35264002680778506
Epoch: 9950, Batch Gradient Norm: 10.638902732551072
Epoch: 9950, Batch Gradient Norm after: 10.638902732551072
Epoch 9951/10000, Prediction Accuracy = 63.8%, Loss = 0.3613332390785217
Epoch: 9951, Batch Gradient Norm: 9.643812244228133
Epoch: 9951, Batch Gradient Norm after: 9.643812244228133
Epoch 9952/10000, Prediction Accuracy = 63.79%, Loss = 0.3494013428688049
Epoch: 9952, Batch Gradient Norm: 10.067815245246717
Epoch: 9952, Batch Gradient Norm after: 10.067815245246717
Epoch 9953/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3567468702793121
Epoch: 9953, Batch Gradient Norm: 8.621125188174823
Epoch: 9953, Batch Gradient Norm after: 8.621125188174823
Epoch 9954/10000, Prediction Accuracy = 63.786000000000016%, Loss = 0.34629989266395567
Epoch: 9954, Batch Gradient Norm: 11.961783491095412
Epoch: 9954, Batch Gradient Norm after: 11.961783491095412
Epoch 9955/10000, Prediction Accuracy = 63.444%, Loss = 0.3725325524806976
Epoch: 9955, Batch Gradient Norm: 10.024716692620444
Epoch: 9955, Batch Gradient Norm after: 10.024716692620444
Epoch 9956/10000, Prediction Accuracy = 64.054%, Loss = 0.36091344356536864
Epoch: 9956, Batch Gradient Norm: 7.75143584080257
Epoch: 9956, Batch Gradient Norm after: 7.75143584080257
Epoch 9957/10000, Prediction Accuracy = 63.977999999999994%, Loss = 0.34041643142700195
Epoch: 9957, Batch Gradient Norm: 10.487926019266054
Epoch: 9957, Batch Gradient Norm after: 10.487926019266054
Epoch 9958/10000, Prediction Accuracy = 63.693999999999996%, Loss = 0.35941060185432433
Epoch: 9958, Batch Gradient Norm: 10.868790120372124
Epoch: 9958, Batch Gradient Norm after: 10.868790120372124
Epoch 9959/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.36687433123588564
Epoch: 9959, Batch Gradient Norm: 7.891164080288508
Epoch: 9959, Batch Gradient Norm after: 7.891164080288508
Epoch 9960/10000, Prediction Accuracy = 63.824%, Loss = 0.3411890208721161
Epoch: 9960, Batch Gradient Norm: 8.12928658945521
Epoch: 9960, Batch Gradient Norm after: 8.12928658945521
Epoch 9961/10000, Prediction Accuracy = 63.769999999999996%, Loss = 0.34159154891967775
Epoch: 9961, Batch Gradient Norm: 8.65791242296504
Epoch: 9961, Batch Gradient Norm after: 8.65791242296504
Epoch 9962/10000, Prediction Accuracy = 63.92999999999999%, Loss = 0.34443589448928835
Epoch: 9962, Batch Gradient Norm: 10.995104429228538
Epoch: 9962, Batch Gradient Norm after: 10.995104429228538
Epoch 9963/10000, Prediction Accuracy = 64.074%, Loss = 0.361254358291626
Epoch: 9963, Batch Gradient Norm: 11.85310318225427
Epoch: 9963, Batch Gradient Norm after: 11.85310318225427
Epoch 9964/10000, Prediction Accuracy = 63.896%, Loss = 0.36652329564094543
Epoch: 9964, Batch Gradient Norm: 12.339420208862824
Epoch: 9964, Batch Gradient Norm after: 12.339420208862824
Epoch 9965/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.36987152099609377
Epoch: 9965, Batch Gradient Norm: 9.428568532250512
Epoch: 9965, Batch Gradient Norm after: 9.428568532250512
Epoch 9966/10000, Prediction Accuracy = 63.81999999999999%, Loss = 0.3488324999809265
Epoch: 9966, Batch Gradient Norm: 9.897377802265027
Epoch: 9966, Batch Gradient Norm after: 9.897377802265027
Epoch 9967/10000, Prediction Accuracy = 63.888%, Loss = 0.35374476313591
Epoch: 9967, Batch Gradient Norm: 12.048813481151107
Epoch: 9967, Batch Gradient Norm after: 12.048813481151107
Epoch 9968/10000, Prediction Accuracy = 63.718%, Loss = 0.37098535895347595
Epoch: 9968, Batch Gradient Norm: 9.034473468149784
Epoch: 9968, Batch Gradient Norm after: 9.034473468149784
Epoch 9969/10000, Prediction Accuracy = 63.646%, Loss = 0.34654776453971864
Epoch: 9969, Batch Gradient Norm: 7.304361380520104
Epoch: 9969, Batch Gradient Norm after: 7.304361380520104
Epoch 9970/10000, Prediction Accuracy = 63.98%, Loss = 0.339873206615448
Epoch: 9970, Batch Gradient Norm: 9.652221797472029
Epoch: 9970, Batch Gradient Norm after: 9.652221797472029
Epoch 9971/10000, Prediction Accuracy = 63.943999999999996%, Loss = 0.35559889674186707
Epoch: 9971, Batch Gradient Norm: 12.726725097143962
Epoch: 9971, Batch Gradient Norm after: 12.726725097143962
Epoch 9972/10000, Prediction Accuracy = 63.79799999999999%, Loss = 0.3787315309047699
Epoch: 9972, Batch Gradient Norm: 11.521670947765196
Epoch: 9972, Batch Gradient Norm after: 11.521670947765196
Epoch 9973/10000, Prediction Accuracy = 63.767999999999994%, Loss = 0.37196162939071653
Epoch: 9973, Batch Gradient Norm: 9.928712007194537
Epoch: 9973, Batch Gradient Norm after: 9.928712007194537
Epoch 9974/10000, Prediction Accuracy = 63.761999999999986%, Loss = 0.3569150149822235
Epoch: 9974, Batch Gradient Norm: 8.870105463311763
Epoch: 9974, Batch Gradient Norm after: 8.870105463311763
Epoch 9975/10000, Prediction Accuracy = 63.955999999999996%, Loss = 0.3505567371845245
Epoch: 9975, Batch Gradient Norm: 8.676820818697683
Epoch: 9975, Batch Gradient Norm after: 8.676820818697683
Epoch 9976/10000, Prediction Accuracy = 64.04400000000001%, Loss = 0.3476918816566467
Epoch: 9976, Batch Gradient Norm: 11.943574295653272
Epoch: 9976, Batch Gradient Norm after: 11.943574295653272
Epoch 9977/10000, Prediction Accuracy = 63.803999999999995%, Loss = 0.37228315472602846
Epoch: 9977, Batch Gradient Norm: 7.899529002963893
Epoch: 9977, Batch Gradient Norm after: 7.899529002963893
Epoch 9978/10000, Prediction Accuracy = 63.988%, Loss = 0.3413098633289337
Epoch: 9978, Batch Gradient Norm: 8.741537232087094
Epoch: 9978, Batch Gradient Norm after: 8.741537232087094
Epoch 9979/10000, Prediction Accuracy = 64.02199999999999%, Loss = 0.3443333923816681
Epoch: 9979, Batch Gradient Norm: 8.992691336089846
Epoch: 9979, Batch Gradient Norm after: 8.992691336089846
Epoch 9980/10000, Prediction Accuracy = 63.89%, Loss = 0.3438195824623108
Epoch: 9980, Batch Gradient Norm: 10.277926040112728
Epoch: 9980, Batch Gradient Norm after: 10.277926040112728
Epoch 9981/10000, Prediction Accuracy = 64.00999999999999%, Loss = 0.3519506573677063
Epoch: 9981, Batch Gradient Norm: 12.024577147827154
Epoch: 9981, Batch Gradient Norm after: 12.024577147827154
Epoch 9982/10000, Prediction Accuracy = 63.822%, Loss = 0.37049868106842043
Epoch: 9982, Batch Gradient Norm: 8.930335973158883
Epoch: 9982, Batch Gradient Norm after: 8.930335973158883
Epoch 9983/10000, Prediction Accuracy = 63.84599999999999%, Loss = 0.3512876570224762
Epoch: 9983, Batch Gradient Norm: 10.713198848412448
Epoch: 9983, Batch Gradient Norm after: 10.713198848412448
Epoch 9984/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3605343997478485
Epoch: 9984, Batch Gradient Norm: 9.617794981674683
Epoch: 9984, Batch Gradient Norm after: 9.617794981674683
Epoch 9985/10000, Prediction Accuracy = 63.79%, Loss = 0.35099361538887025
Epoch: 9985, Batch Gradient Norm: 9.432491608993766
Epoch: 9985, Batch Gradient Norm after: 9.432491608993766
Epoch 9986/10000, Prediction Accuracy = 63.91400000000001%, Loss = 0.350677615404129
Epoch: 9986, Batch Gradient Norm: 9.948861211950879
Epoch: 9986, Batch Gradient Norm after: 9.948861211950879
Epoch 9987/10000, Prediction Accuracy = 63.61199999999999%, Loss = 0.35398114919662477
Epoch: 9987, Batch Gradient Norm: 12.103663559717212
Epoch: 9987, Batch Gradient Norm after: 12.103663559717212
Epoch 9988/10000, Prediction Accuracy = 63.674%, Loss = 0.36967629194259644
Epoch: 9988, Batch Gradient Norm: 12.111357501286642
Epoch: 9988, Batch Gradient Norm after: 12.111357501286642
Epoch 9989/10000, Prediction Accuracy = 63.874%, Loss = 0.3785079002380371
Epoch: 9989, Batch Gradient Norm: 8.830130831946288
Epoch: 9989, Batch Gradient Norm after: 8.830130831946288
Epoch 9990/10000, Prediction Accuracy = 63.988%, Loss = 0.3507062613964081
Epoch: 9990, Batch Gradient Norm: 8.233411154247786
Epoch: 9990, Batch Gradient Norm after: 8.233411154247786
Epoch 9991/10000, Prediction Accuracy = 63.958000000000006%, Loss = 0.3446581423282623
Epoch: 9991, Batch Gradient Norm: 8.385428036455922
Epoch: 9991, Batch Gradient Norm after: 8.385428036455922
Epoch 9992/10000, Prediction Accuracy = 63.931999999999995%, Loss = 0.34367882609367373
Epoch: 9992, Batch Gradient Norm: 9.839564008397675
Epoch: 9992, Batch Gradient Norm after: 9.839564008397675
Epoch 9993/10000, Prediction Accuracy = 63.734%, Loss = 0.35366804599761964
Epoch: 9993, Batch Gradient Norm: 10.04876595373728
Epoch: 9993, Batch Gradient Norm after: 10.04876595373728
Epoch 9994/10000, Prediction Accuracy = 63.95399999999999%, Loss = 0.35314759612083435
Epoch: 9994, Batch Gradient Norm: 11.0369673518085
Epoch: 9994, Batch Gradient Norm after: 11.0369673518085
Epoch 9995/10000, Prediction Accuracy = 63.836%, Loss = 0.3624232053756714
Epoch: 9995, Batch Gradient Norm: 10.803713780711782
Epoch: 9995, Batch Gradient Norm after: 10.803713780711782
Epoch 9996/10000, Prediction Accuracy = 63.854000000000006%, Loss = 0.36207462549209596
Epoch: 9996, Batch Gradient Norm: 8.49899651567088
Epoch: 9996, Batch Gradient Norm after: 8.49899651567088
Epoch 9997/10000, Prediction Accuracy = 63.908%, Loss = 0.34439060688018797
Epoch: 9997, Batch Gradient Norm: 9.85037056412756
Epoch: 9997, Batch Gradient Norm after: 9.85037056412756
Epoch 9998/10000, Prediction Accuracy = 63.815999999999995%, Loss = 0.35688366293907164
Epoch: 9998, Batch Gradient Norm: 11.354591849855307
Epoch: 9998, Batch Gradient Norm after: 11.354591849855307
Epoch 9999/10000, Prediction Accuracy = 63.786%, Loss = 0.35961424708366396
Epoch: 9999, Batch Gradient Norm: 11.145970300398313
Epoch: 9999, Batch Gradient Norm after: 11.145970300398313
Epoch 10000/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.360314804315567
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode20-dqn/classify_rsmProp2.py", line 226, in <module>
    plt.show()  # Show the final plot
    ^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/pyplot.py", line 607, in show
    return _get_backend_mod().show(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 3567, in show
    cls.mainloop()
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backends/backend_macosx.py", line 178, in start_main_loop
    with _allow_interrupt_macos():
  File "/Users/athmajanvivekananthan/miniconda3/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 1686, in _allow_interrupt
    old_sigint_handler(*handler_args)
KeyboardInterrupt