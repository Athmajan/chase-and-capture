Epoch: 0, Batch Gradient Norm: 82.00640429082709
Epoch: 0, Batch Gradient Norm after: 22.360680440172015
Epoch 1/10000, Prediction Accuracy = 22.374000000000002%, Loss = 58.55632781982422
Epoch: 1, Batch Gradient Norm: 82.49432628962471
Epoch: 1, Batch Gradient Norm after: 22.36067991425004
Epoch 2/10000, Prediction Accuracy = 24.174%, Loss = 52.77161331176758
Epoch: 2, Batch Gradient Norm: 83.42665009750901
Epoch: 2, Batch Gradient Norm after: 22.360679520159316
Epoch 3/10000, Prediction Accuracy = 25.221999999999998%, Loss = 49.70743255615234
Epoch: 3, Batch Gradient Norm: 83.98292458967826
Epoch: 3, Batch Gradient Norm after: 22.3606793446048
Epoch 4/10000, Prediction Accuracy = 25.68%, Loss = 47.40668869018555
Epoch: 4, Batch Gradient Norm: 84.10538428967553
Epoch: 4, Batch Gradient Norm after: 22.36068144363868
Epoch 5/10000, Prediction Accuracy = 26.012%, Loss = 45.53244400024414
Epoch: 5, Batch Gradient Norm: 83.97673307644814
Epoch: 5, Batch Gradient Norm after: 22.360679432239245
Epoch 6/10000, Prediction Accuracy = 26.546%, Loss = 43.925103759765626
Epoch: 6, Batch Gradient Norm: 83.6559085310687
Epoch: 6, Batch Gradient Norm after: 22.360679668254996
Epoch 7/10000, Prediction Accuracy = 26.776%, Loss = 42.4985466003418
Epoch: 7, Batch Gradient Norm: 83.16805871128871
Epoch: 7, Batch Gradient Norm after: 22.360680634484748
Epoch 8/10000, Prediction Accuracy = 26.53%, Loss = 41.20493621826172
Epoch: 8, Batch Gradient Norm: 82.58337359689992
Epoch: 8, Batch Gradient Norm after: 22.36067845808633
Epoch 9/10000, Prediction Accuracy = 25.986%, Loss = 40.01471481323242
Epoch: 9, Batch Gradient Norm: 81.93976650103393
Epoch: 9, Batch Gradient Norm after: 22.36068030491564
Epoch 10/10000, Prediction Accuracy = 25.434000000000005%, Loss = 38.90831527709961
Epoch: 10, Batch Gradient Norm: 81.26531033707916
Epoch: 10, Batch Gradient Norm after: 22.360679546556614
Epoch 11/10000, Prediction Accuracy = 24.958%, Loss = 37.86967468261719
Epoch: 11, Batch Gradient Norm: 80.57473170101204
Epoch: 11, Batch Gradient Norm after: 22.36067916434627
Epoch 12/10000, Prediction Accuracy = 24.810000000000002%, Loss = 36.887114715576175
Epoch: 12, Batch Gradient Norm: 79.87738281994159
Epoch: 12, Batch Gradient Norm after: 22.360679136076723
Epoch 13/10000, Prediction Accuracy = 24.992%, Loss = 35.95029754638672
Epoch: 13, Batch Gradient Norm: 79.18467141166083
Epoch: 13, Batch Gradient Norm after: 22.36068025038582
Epoch 14/10000, Prediction Accuracy = 25.54%, Loss = 35.05192642211914
Epoch: 14, Batch Gradient Norm: 78.48655108328741
Epoch: 14, Batch Gradient Norm after: 22.360679495180914
Epoch 15/10000, Prediction Accuracy = 26.03%, Loss = 34.1862548828125
Epoch: 15, Batch Gradient Norm: 77.77562145283395
Epoch: 15, Batch Gradient Norm after: 22.36067941719373
Epoch 16/10000, Prediction Accuracy = 26.509999999999998%, Loss = 33.35084075927735
Epoch: 16, Batch Gradient Norm: 77.03691813078804
Epoch: 16, Batch Gradient Norm after: 22.360680299320936
Epoch 17/10000, Prediction Accuracy = 26.602000000000004%, Loss = 32.54278678894043
Epoch: 17, Batch Gradient Norm: 76.27995838989953
Epoch: 17, Batch Gradient Norm after: 22.360679775618934
Epoch 18/10000, Prediction Accuracy = 26.656%, Loss = 31.76131820678711
Epoch: 18, Batch Gradient Norm: 75.50950448296186
Epoch: 18, Batch Gradient Norm after: 22.360679422908195
Epoch 19/10000, Prediction Accuracy = 26.645999999999997%, Loss = 31.00253257751465
Epoch: 19, Batch Gradient Norm: 74.7446692132312
Epoch: 19, Batch Gradient Norm after: 22.360680117167263
Epoch 20/10000, Prediction Accuracy = 26.698%, Loss = 30.26316909790039
Epoch: 20, Batch Gradient Norm: 73.9725031867654
Epoch: 20, Batch Gradient Norm after: 22.36068105181873
Epoch 21/10000, Prediction Accuracy = 26.784000000000002%, Loss = 29.541448974609374
Epoch: 21, Batch Gradient Norm: 73.19377601309452
Epoch: 21, Batch Gradient Norm after: 22.360679501178787
Epoch 22/10000, Prediction Accuracy = 26.868000000000002%, Loss = 28.835869979858398
Epoch: 22, Batch Gradient Norm: 72.41374218759007
Epoch: 22, Batch Gradient Norm after: 22.36068015729498
Epoch 23/10000, Prediction Accuracy = 26.869999999999997%, Loss = 28.14385986328125
Epoch: 23, Batch Gradient Norm: 71.63350138361031
Epoch: 23, Batch Gradient Norm after: 22.36067946706098
Epoch 24/10000, Prediction Accuracy = 26.886000000000003%, Loss = 27.464729690551756
Epoch: 24, Batch Gradient Norm: 70.83611415606346
Epoch: 24, Batch Gradient Norm after: 22.360677905967624
Epoch 25/10000, Prediction Accuracy = 26.752%, Loss = 26.79863586425781
Epoch: 25, Batch Gradient Norm: 70.0098234111025
Epoch: 25, Batch Gradient Norm after: 22.360678680776257
Epoch 26/10000, Prediction Accuracy = 26.534%, Loss = 26.147388076782228
Epoch: 26, Batch Gradient Norm: 69.15487918072824
Epoch: 26, Batch Gradient Norm after: 22.360677163090585
Epoch 27/10000, Prediction Accuracy = 26.196000000000005%, Loss = 25.510232162475585
Epoch: 27, Batch Gradient Norm: 68.29610821462737
Epoch: 27, Batch Gradient Norm after: 22.360679564431983
Epoch 28/10000, Prediction Accuracy = 25.796%, Loss = 24.886411666870117
Epoch: 28, Batch Gradient Norm: 67.4435098322754
Epoch: 28, Batch Gradient Norm after: 22.360678077554798
Epoch 29/10000, Prediction Accuracy = 25.497999999999998%, Loss = 24.274095916748045
Epoch: 29, Batch Gradient Norm: 66.59196064991816
Epoch: 29, Batch Gradient Norm after: 22.360677956142613
Epoch 30/10000, Prediction Accuracy = 25.142%, Loss = 23.672797012329102
Epoch: 30, Batch Gradient Norm: 65.71971628342077
Epoch: 30, Batch Gradient Norm after: 22.36067854252932
Epoch 31/10000, Prediction Accuracy = 24.862000000000002%, Loss = 23.083130645751954
Epoch: 31, Batch Gradient Norm: 64.82450136206256
Epoch: 31, Batch Gradient Norm after: 22.360677225426674
Epoch 32/10000, Prediction Accuracy = 24.721999999999998%, Loss = 22.50567283630371
Epoch: 32, Batch Gradient Norm: 63.92377205984505
Epoch: 32, Batch Gradient Norm after: 22.360678412872375
Epoch 33/10000, Prediction Accuracy = 24.594%, Loss = 21.939665222167967
Epoch: 33, Batch Gradient Norm: 63.027905400423975
Epoch: 33, Batch Gradient Norm after: 22.360678807739568
Epoch 34/10000, Prediction Accuracy = 24.564%, Loss = 21.384148025512694
Epoch: 34, Batch Gradient Norm: 62.15475037326335
Epoch: 34, Batch Gradient Norm after: 22.360677467705308
Epoch 35/10000, Prediction Accuracy = 24.548%, Loss = 20.84466896057129
Epoch: 35, Batch Gradient Norm: 61.27862857719352
Epoch: 35, Batch Gradient Norm after: 22.36067869769484
Epoch 36/10000, Prediction Accuracy = 24.494%, Loss = 20.3140625
Epoch: 36, Batch Gradient Norm: 60.3657342484777
Epoch: 36, Batch Gradient Norm after: 22.3606790628148
Epoch 37/10000, Prediction Accuracy = 24.500000000000004%, Loss = 19.780973434448242
Epoch: 37, Batch Gradient Norm: 59.45163918126709
Epoch: 37, Batch Gradient Norm after: 22.360677666197226
Epoch 38/10000, Prediction Accuracy = 24.612%, Loss = 19.257801818847657
Epoch: 38, Batch Gradient Norm: 58.55835266771435
Epoch: 38, Batch Gradient Norm after: 22.36067954070948
Epoch 39/10000, Prediction Accuracy = 24.626%, Loss = 18.7482608795166
Epoch: 39, Batch Gradient Norm: 57.643364334026856
Epoch: 39, Batch Gradient Norm after: 22.360678518498315
Epoch 40/10000, Prediction Accuracy = 24.7%, Loss = 18.246663665771486
Epoch: 40, Batch Gradient Norm: 56.74814567643897
Epoch: 40, Batch Gradient Norm after: 22.36067779034421
Epoch 41/10000, Prediction Accuracy = 24.840000000000003%, Loss = 17.75930290222168
Epoch: 41, Batch Gradient Norm: 55.839100535911136
Epoch: 41, Batch Gradient Norm after: 22.360678186879255
Epoch 42/10000, Prediction Accuracy = 24.999999999999996%, Loss = 17.281586837768554
Epoch: 42, Batch Gradient Norm: 54.9628979984451
Epoch: 42, Batch Gradient Norm after: 22.360679073887358
Epoch 43/10000, Prediction Accuracy = 25.232%, Loss = 16.81393699645996
Epoch: 43, Batch Gradient Norm: 53.99793910276789
Epoch: 43, Batch Gradient Norm after: 22.360678042860382
Epoch 44/10000, Prediction Accuracy = 25.616000000000003%, Loss = 16.343074798583984
Epoch: 44, Batch Gradient Norm: 53.12000606730952
Epoch: 44, Batch Gradient Norm after: 22.360678747144508
Epoch 45/10000, Prediction Accuracy = 25.898000000000003%, Loss = 15.891475486755372
Epoch: 45, Batch Gradient Norm: 52.184187613636816
Epoch: 45, Batch Gradient Norm after: 22.360677845503105
Epoch 46/10000, Prediction Accuracy = 26.226%, Loss = 15.442880439758301
Epoch: 46, Batch Gradient Norm: 51.230136657376576
Epoch: 46, Batch Gradient Norm after: 22.360677976450006
Epoch 47/10000, Prediction Accuracy = 26.444%, Loss = 15.000645065307618
Epoch: 47, Batch Gradient Norm: 50.3040420733691
Epoch: 47, Batch Gradient Norm after: 22.360678585877118
Epoch 48/10000, Prediction Accuracy = 26.631999999999998%, Loss = 14.572355461120605
Epoch: 48, Batch Gradient Norm: 49.376764668276444
Epoch: 48, Batch Gradient Norm after: 22.36067700316087
Epoch 49/10000, Prediction Accuracy = 26.79%, Loss = 14.153534507751464
Epoch: 49, Batch Gradient Norm: 48.562017438186395
Epoch: 49, Batch Gradient Norm after: 22.360677068750658
Epoch 50/10000, Prediction Accuracy = 26.982%, Loss = 13.75427417755127
Epoch: 50, Batch Gradient Norm: 47.73425534790376
Epoch: 50, Batch Gradient Norm after: 22.360678994930083
Epoch 51/10000, Prediction Accuracy = 27.140000000000004%, Loss = 13.361623573303223
Epoch: 51, Batch Gradient Norm: 46.801818134237315
Epoch: 51, Batch Gradient Norm after: 22.360677906096257
Epoch 52/10000, Prediction Accuracy = 27.35%, Loss = 12.96599006652832
Epoch: 52, Batch Gradient Norm: 45.73934848992251
Epoch: 52, Batch Gradient Norm after: 22.36067774979598
Epoch 53/10000, Prediction Accuracy = 27.619999999999997%, Loss = 12.571370506286621
Epoch: 53, Batch Gradient Norm: 44.86022857526834
Epoch: 53, Batch Gradient Norm after: 22.360677712470444
Epoch 54/10000, Prediction Accuracy = 27.838000000000005%, Loss = 12.19922046661377
Epoch: 54, Batch Gradient Norm: 43.948148303210814
Epoch: 54, Batch Gradient Norm after: 22.360677856483335
Epoch 55/10000, Prediction Accuracy = 28.038%, Loss = 11.83473072052002
Epoch: 55, Batch Gradient Norm: 43.15028265792659
Epoch: 55, Batch Gradient Norm after: 22.36067728162664
Epoch 56/10000, Prediction Accuracy = 28.226000000000006%, Loss = 11.485407829284668
Epoch: 56, Batch Gradient Norm: 42.12254018116587
Epoch: 56, Batch Gradient Norm after: 22.360679618401306
Epoch 57/10000, Prediction Accuracy = 28.274%, Loss = 11.126683235168457
Epoch: 57, Batch Gradient Norm: 41.26627532209867
Epoch: 57, Batch Gradient Norm after: 22.360677187951676
Epoch 58/10000, Prediction Accuracy = 28.330000000000002%, Loss = 10.785231971740723
Epoch: 58, Batch Gradient Norm: 40.349980492503015
Epoch: 58, Batch Gradient Norm after: 22.360676491571805
Epoch 59/10000, Prediction Accuracy = 28.448%, Loss = 10.445260429382325
Epoch: 59, Batch Gradient Norm: 39.560640724753036
Epoch: 59, Batch Gradient Norm after: 22.360679435103734
Epoch 60/10000, Prediction Accuracy = 28.477999999999998%, Loss = 10.119844055175781
Epoch: 60, Batch Gradient Norm: 38.70708473293102
Epoch: 60, Batch Gradient Norm after: 22.360677058761283
Epoch 61/10000, Prediction Accuracy = 28.55%, Loss = 9.798437881469727
Epoch: 61, Batch Gradient Norm: 37.948071626872114
Epoch: 61, Batch Gradient Norm after: 22.36067639153884
Epoch 62/10000, Prediction Accuracy = 28.683999999999997%, Loss = 9.493415641784669
Epoch: 62, Batch Gradient Norm: 37.36868534038356
Epoch: 62, Batch Gradient Norm after: 22.360676455055984
Epoch 63/10000, Prediction Accuracy = 28.758%, Loss = 9.203271865844727
Epoch: 63, Batch Gradient Norm: 36.24123078273772
Epoch: 63, Batch Gradient Norm after: 22.360677278272856
Epoch 64/10000, Prediction Accuracy = 28.942%, Loss = 8.897404289245605
Epoch: 64, Batch Gradient Norm: 35.33670199133371
Epoch: 64, Batch Gradient Norm after: 22.360677330268764
Epoch 65/10000, Prediction Accuracy = 29.082%, Loss = 8.605675888061523
Epoch: 65, Batch Gradient Norm: 34.37064288048582
Epoch: 65, Batch Gradient Norm after: 22.36067705233927
Epoch 66/10000, Prediction Accuracy = 29.363999999999997%, Loss = 8.321936321258544
Epoch: 66, Batch Gradient Norm: 33.63552649151774
Epoch: 66, Batch Gradient Norm after: 22.360678617491605
Epoch 67/10000, Prediction Accuracy = 29.556%, Loss = 8.053347110748291
Epoch: 67, Batch Gradient Norm: 32.843619012873
Epoch: 67, Batch Gradient Norm after: 22.360677843097186
Epoch 68/10000, Prediction Accuracy = 29.742%, Loss = 7.7910713195800785
Epoch: 68, Batch Gradient Norm: 32.0060421970081
Epoch: 68, Batch Gradient Norm after: 22.360678091196693
Epoch 69/10000, Prediction Accuracy = 29.916000000000004%, Loss = 7.528760242462158
Epoch: 69, Batch Gradient Norm: 31.391490616158663
Epoch: 69, Batch Gradient Norm after: 22.36067843012172
Epoch 70/10000, Prediction Accuracy = 30.104000000000003%, Loss = 7.276755046844483
Epoch: 70, Batch Gradient Norm: 30.86803461913306
Epoch: 70, Batch Gradient Norm after: 22.360678984212484
Epoch 71/10000, Prediction Accuracy = 30.188%, Loss = 7.037616157531739
Epoch: 71, Batch Gradient Norm: 30.196880768995374
Epoch: 71, Batch Gradient Norm after: 22.360676783282052
Epoch 72/10000, Prediction Accuracy = 30.369999999999994%, Loss = 6.797069358825683
Epoch: 72, Batch Gradient Norm: 29.200918305911497
Epoch: 72, Batch Gradient Norm after: 22.360678686522423
Epoch 73/10000, Prediction Accuracy = 30.766000000000002%, Loss = 6.551308441162109
Epoch: 73, Batch Gradient Norm: 28.551882489735167
Epoch: 73, Batch Gradient Norm after: 22.36067731995631
Epoch 74/10000, Prediction Accuracy = 31.106000000000005%, Loss = 6.309135627746582
Epoch: 74, Batch Gradient Norm: 27.971202766821694
Epoch: 74, Batch Gradient Norm after: 22.360677874684384
Epoch 75/10000, Prediction Accuracy = 31.532%, Loss = 6.102646064758301
Epoch: 75, Batch Gradient Norm: 27.050503685380903
Epoch: 75, Batch Gradient Norm after: 22.360679111859426
Epoch 76/10000, Prediction Accuracy = 31.85%, Loss = 5.884222221374512
Epoch: 76, Batch Gradient Norm: 26.284649370053184
Epoch: 76, Batch Gradient Norm after: 22.360679168910075
Epoch 77/10000, Prediction Accuracy = 32.169999999999995%, Loss = 5.684363842010498
Epoch: 77, Batch Gradient Norm: 25.524413712797795
Epoch: 77, Batch Gradient Norm after: 22.360680350213688
Epoch 78/10000, Prediction Accuracy = 32.489999999999995%, Loss = 5.48992223739624
Epoch: 78, Batch Gradient Norm: 25.241529168053347
Epoch: 78, Batch Gradient Norm after: 22.360677982680922
Epoch 79/10000, Prediction Accuracy = 32.788%, Loss = 5.323559379577636
Epoch: 79, Batch Gradient Norm: 24.477369340812203
Epoch: 79, Batch Gradient Norm after: 22.360677337558965
Epoch 80/10000, Prediction Accuracy = 33.132%, Loss = 5.144761085510254
Epoch: 80, Batch Gradient Norm: 24.173064242284443
Epoch: 80, Batch Gradient Norm after: 22.36067661196079
Epoch 81/10000, Prediction Accuracy = 33.489999999999995%, Loss = 4.9956714630126955
Epoch: 81, Batch Gradient Norm: 23.01863349073163
Epoch: 81, Batch Gradient Norm after: 22.296522210825312
Epoch 82/10000, Prediction Accuracy = 33.775999999999996%, Loss = 4.815048408508301
Epoch: 82, Batch Gradient Norm: 22.939561521937026
Epoch: 82, Batch Gradient Norm after: 22.307050327227316
Epoch 83/10000, Prediction Accuracy = 34.025999999999996%, Loss = 4.678086853027343
Epoch: 83, Batch Gradient Norm: 22.13742392958923
Epoch: 83, Batch Gradient Norm after: 22.06028591270793
Epoch 84/10000, Prediction Accuracy = 34.304%, Loss = 4.528794670104981
Epoch: 84, Batch Gradient Norm: 21.491195997581915
Epoch: 84, Batch Gradient Norm after: 21.491195997581915
Epoch 85/10000, Prediction Accuracy = 34.55%, Loss = 4.390243816375732
Epoch: 85, Batch Gradient Norm: 21.090106709145516
Epoch: 85, Batch Gradient Norm after: 21.090106709145516
Epoch 86/10000, Prediction Accuracy = 34.821999999999996%, Loss = 4.272040748596192
Epoch: 86, Batch Gradient Norm: 20.099839694127994
Epoch: 86, Batch Gradient Norm after: 20.099839694127994
Epoch 87/10000, Prediction Accuracy = 35.102%, Loss = 4.135416841506958
Epoch: 87, Batch Gradient Norm: 19.845565362222494
Epoch: 87, Batch Gradient Norm after: 19.845565362222494
Epoch 88/10000, Prediction Accuracy = 35.326%, Loss = 4.029142999649048
Epoch: 88, Batch Gradient Norm: 19.164244688466873
Epoch: 88, Batch Gradient Norm after: 19.164244688466873
Epoch 89/10000, Prediction Accuracy = 35.542%, Loss = 3.8989978313446043
Epoch: 89, Batch Gradient Norm: 19.028187098262027
Epoch: 89, Batch Gradient Norm after: 19.028187098262027
Epoch 90/10000, Prediction Accuracy = 35.784000000000006%, Loss = 3.803053617477417
Epoch: 90, Batch Gradient Norm: 18.669977906090384
Epoch: 90, Batch Gradient Norm after: 18.669977906090384
Epoch 91/10000, Prediction Accuracy = 36.002%, Loss = 3.706057596206665
Epoch: 91, Batch Gradient Norm: 18.966700832177946
Epoch: 91, Batch Gradient Norm after: 18.966700832177946
Epoch 92/10000, Prediction Accuracy = 36.212%, Loss = 3.6332232475280763
Epoch: 92, Batch Gradient Norm: 18.226074900834284
Epoch: 92, Batch Gradient Norm after: 18.226074900834284
Epoch 93/10000, Prediction Accuracy = 36.48%, Loss = 3.536574125289917
Epoch: 93, Batch Gradient Norm: 17.948835577176187
Epoch: 93, Batch Gradient Norm after: 17.948835577176187
Epoch 94/10000, Prediction Accuracy = 36.666000000000004%, Loss = 3.457827425003052
Epoch: 94, Batch Gradient Norm: 17.213811204041154
Epoch: 94, Batch Gradient Norm after: 17.213811204041154
Epoch 95/10000, Prediction Accuracy = 36.894000000000005%, Loss = 3.370311164855957
Epoch: 95, Batch Gradient Norm: 17.27296465253872
Epoch: 95, Batch Gradient Norm after: 17.27296465253872
Epoch 96/10000, Prediction Accuracy = 37.062%, Loss = 3.303644371032715
Epoch: 96, Batch Gradient Norm: 16.62555452225137
Epoch: 96, Batch Gradient Norm after: 16.62555452225137
Epoch 97/10000, Prediction Accuracy = 37.232000000000006%, Loss = 3.2248552799224854
Epoch: 97, Batch Gradient Norm: 16.82303234234122
Epoch: 97, Batch Gradient Norm after: 16.82303234234122
Epoch 98/10000, Prediction Accuracy = 37.434%, Loss = 3.1655226707458497
Epoch: 98, Batch Gradient Norm: 15.99648275957715
Epoch: 98, Batch Gradient Norm after: 15.99648275957715
Epoch 99/10000, Prediction Accuracy = 37.57%, Loss = 3.0888705253601074
Epoch: 99, Batch Gradient Norm: 16.56643362369905
Epoch: 99, Batch Gradient Norm after: 16.56643362369905
Epoch 100/10000, Prediction Accuracy = 37.736000000000004%, Loss = 3.0401015281677246
Epoch: 100, Batch Gradient Norm: 15.984309441800868
Epoch: 100, Batch Gradient Norm after: 15.984309441800868
Epoch 101/10000, Prediction Accuracy = 37.873999999999995%, Loss = 2.9715926170349123
Epoch: 101, Batch Gradient Norm: 16.878569317886125
Epoch: 101, Batch Gradient Norm after: 16.878569317886125
Epoch 102/10000, Prediction Accuracy = 37.982%, Loss = 2.931895303726196
Epoch: 102, Batch Gradient Norm: 15.450041304144793
Epoch: 102, Batch Gradient Norm after: 15.450041304144793
Epoch 103/10000, Prediction Accuracy = 38.116%, Loss = 2.8538878917694093
Epoch: 103, Batch Gradient Norm: 15.513847775351083
Epoch: 103, Batch Gradient Norm after: 15.513847775351083
Epoch 104/10000, Prediction Accuracy = 38.19199999999999%, Loss = 2.803425979614258
Epoch: 104, Batch Gradient Norm: 14.547535066732799
Epoch: 104, Batch Gradient Norm after: 14.547535066732799
Epoch 105/10000, Prediction Accuracy = 38.306%, Loss = 2.737785816192627
Epoch: 105, Batch Gradient Norm: 15.083603399375118
Epoch: 105, Batch Gradient Norm after: 15.083603399375118
Epoch 106/10000, Prediction Accuracy = 38.436%, Loss = 2.6971380710601807
Epoch: 106, Batch Gradient Norm: 14.217332630303753
Epoch: 106, Batch Gradient Norm after: 14.217332630303753
Epoch 107/10000, Prediction Accuracy = 38.544%, Loss = 2.6365894317626952
Epoch: 107, Batch Gradient Norm: 15.595685986474988
Epoch: 107, Batch Gradient Norm after: 15.595685986474988
Epoch 108/10000, Prediction Accuracy = 38.616%, Loss = 2.6115137577056884
Epoch: 108, Batch Gradient Norm: 14.625191353829047
Epoch: 108, Batch Gradient Norm after: 14.625191353829047
Epoch 109/10000, Prediction Accuracy = 38.686%, Loss = 2.5515400886535646
Epoch: 109, Batch Gradient Norm: 15.252480075288826
Epoch: 109, Batch Gradient Norm after: 15.252480075288826
Epoch 110/10000, Prediction Accuracy = 38.75%, Loss = 2.516304588317871
Epoch: 110, Batch Gradient Norm: 14.06356345130243
Epoch: 110, Batch Gradient Norm after: 14.06356345130243
Epoch 111/10000, Prediction Accuracy = 38.86%, Loss = 2.4559879302978516
Epoch: 111, Batch Gradient Norm: 14.129976815540928
Epoch: 111, Batch Gradient Norm after: 14.129976815540928
Epoch 112/10000, Prediction Accuracy = 38.9%, Loss = 2.413997840881348
Epoch: 112, Batch Gradient Norm: 13.390019390562648
Epoch: 112, Batch Gradient Norm after: 13.390019390562648
Epoch 113/10000, Prediction Accuracy = 39.0%, Loss = 2.363683271408081
Epoch: 113, Batch Gradient Norm: 14.716026739102844
Epoch: 113, Batch Gradient Norm after: 14.716026739102844
Epoch 114/10000, Prediction Accuracy = 38.972%, Loss = 2.3056214809417725
Epoch: 114, Batch Gradient Norm: 13.202447849583564
Epoch: 114, Batch Gradient Norm after: 13.202447849583564
Epoch 115/10000, Prediction Accuracy = 39.138%, Loss = 2.2116322040557863
Epoch: 115, Batch Gradient Norm: 14.081866063282607
Epoch: 115, Batch Gradient Norm after: 14.081866063282607
Epoch 116/10000, Prediction Accuracy = 39.31%, Loss = 2.14951491355896
Epoch: 116, Batch Gradient Norm: 12.366660949490687
Epoch: 116, Batch Gradient Norm after: 12.366660949490687
Epoch 117/10000, Prediction Accuracy = 39.362%, Loss = 2.0765082120895384
Epoch: 117, Batch Gradient Norm: 13.010781991714797
Epoch: 117, Batch Gradient Norm after: 13.010781991714797
Epoch 118/10000, Prediction Accuracy = 39.368%, Loss = 2.039671754837036
Epoch: 118, Batch Gradient Norm: 11.827322169995652
Epoch: 118, Batch Gradient Norm after: 11.827322169995652
Epoch 119/10000, Prediction Accuracy = 39.426%, Loss = 1.9795245409011841
Epoch: 119, Batch Gradient Norm: 12.726523110447214
Epoch: 119, Batch Gradient Norm after: 12.726523110447214
Epoch 120/10000, Prediction Accuracy = 39.454%, Loss = 1.9467453241348267
Epoch: 120, Batch Gradient Norm: 11.319582117027112
Epoch: 120, Batch Gradient Norm after: 11.319582117027112
Epoch 121/10000, Prediction Accuracy = 39.510000000000005%, Loss = 1.8841081619262696
Epoch: 121, Batch Gradient Norm: 13.435061538218152
Epoch: 121, Batch Gradient Norm after: 13.435061538218152
Epoch 122/10000, Prediction Accuracy = 39.504%, Loss = 1.853345561027527
Epoch: 122, Batch Gradient Norm: 10.826678775026775
Epoch: 122, Batch Gradient Norm after: 10.826678775026775
Epoch 123/10000, Prediction Accuracy = 39.562%, Loss = 1.7953969717025757
Epoch: 123, Batch Gradient Norm: 13.74388713666834
Epoch: 123, Batch Gradient Norm after: 13.74388713666834
Epoch 124/10000, Prediction Accuracy = 39.63%, Loss = 1.7891913890838622
Epoch: 124, Batch Gradient Norm: 12.663117528027861
Epoch: 124, Batch Gradient Norm after: 12.663117528027861
Epoch 125/10000, Prediction Accuracy = 39.69799999999999%, Loss = 1.7458264827728271
Epoch: 125, Batch Gradient Norm: 13.27810936953691
Epoch: 125, Batch Gradient Norm after: 13.27810936953691
Epoch 126/10000, Prediction Accuracy = 39.696%, Loss = 1.7086978912353517
Epoch: 126, Batch Gradient Norm: 10.237140636940458
Epoch: 126, Batch Gradient Norm after: 10.237140636940458
Epoch 127/10000, Prediction Accuracy = 39.608000000000004%, Loss = 1.6349878549575805
Epoch: 127, Batch Gradient Norm: 11.087210929183637
Epoch: 127, Batch Gradient Norm after: 11.087210929183637
Epoch 128/10000, Prediction Accuracy = 39.676%, Loss = 1.6025469303131104
Epoch: 128, Batch Gradient Norm: 9.267431753412765
Epoch: 128, Batch Gradient Norm after: 9.267431753412765
Epoch 129/10000, Prediction Accuracy = 39.71%, Loss = 1.5473432779312133
Epoch: 129, Batch Gradient Norm: 12.131489546931718
Epoch: 129, Batch Gradient Norm after: 12.131489546931718
Epoch 130/10000, Prediction Accuracy = 39.75%, Loss = 1.5404480695724487
Epoch: 130, Batch Gradient Norm: 9.572941638020604
Epoch: 130, Batch Gradient Norm after: 9.572941638020604
Epoch 131/10000, Prediction Accuracy = 39.75599999999999%, Loss = 1.4983537197113037
Epoch: 131, Batch Gradient Norm: 13.35402179963496
Epoch: 131, Batch Gradient Norm after: 13.35402179963496
Epoch 132/10000, Prediction Accuracy = 39.834%, Loss = 1.4886189460754395
Epoch: 132, Batch Gradient Norm: 9.01257132384249
Epoch: 132, Batch Gradient Norm after: 9.01257132384249
Epoch 133/10000, Prediction Accuracy = 39.757999999999996%, Loss = 1.413365364074707
Epoch: 133, Batch Gradient Norm: 12.284529257808629
Epoch: 133, Batch Gradient Norm after: 12.284529257808629
Epoch 134/10000, Prediction Accuracy = 39.8%, Loss = 1.4061060905456544
Epoch: 134, Batch Gradient Norm: 8.51607288595983
Epoch: 134, Batch Gradient Norm after: 8.51607288595983
Epoch 135/10000, Prediction Accuracy = 39.83200000000001%, Loss = 1.3602747678756715
Epoch: 135, Batch Gradient Norm: 11.405457026569715
Epoch: 135, Batch Gradient Norm after: 11.405457026569715
Epoch 136/10000, Prediction Accuracy = 39.866%, Loss = 1.3597080707550049
Epoch: 136, Batch Gradient Norm: 8.218701105236557
Epoch: 136, Batch Gradient Norm after: 8.218701105236557
Epoch 137/10000, Prediction Accuracy = 39.924%, Loss = 1.312242817878723
Epoch: 137, Batch Gradient Norm: 11.060686165529152
Epoch: 137, Batch Gradient Norm after: 11.060686165529152
Epoch 138/10000, Prediction Accuracy = 39.916%, Loss = 1.302257204055786
Epoch: 138, Batch Gradient Norm: 9.378257695660473
Epoch: 138, Batch Gradient Norm after: 9.378257695660473
Epoch 139/10000, Prediction Accuracy = 39.962%, Loss = 1.275394892692566
Epoch: 139, Batch Gradient Norm: 12.326994453611052
Epoch: 139, Batch Gradient Norm after: 12.326994453611052
Epoch 140/10000, Prediction Accuracy = 39.99399999999999%, Loss = 1.2750657796859741
Epoch: 140, Batch Gradient Norm: 8.197546237103197
Epoch: 140, Batch Gradient Norm after: 8.197546237103197
Epoch 141/10000, Prediction Accuracy = 40.036%, Loss = 1.233419370651245
Epoch: 141, Batch Gradient Norm: 13.753477404255548
Epoch: 141, Batch Gradient Norm after: 13.753477404255548
Epoch 142/10000, Prediction Accuracy = 39.992%, Loss = 1.2471782207489013
Epoch: 142, Batch Gradient Norm: 7.578501801557843
Epoch: 142, Batch Gradient Norm after: 7.578501801557843
Epoch 143/10000, Prediction Accuracy = 40.12%, Loss = 1.1793684244155884
Epoch: 143, Batch Gradient Norm: 12.258861216708727
Epoch: 143, Batch Gradient Norm after: 12.258861216708727
Epoch 144/10000, Prediction Accuracy = 40.324%, Loss = 1.1847595930099488
Epoch: 144, Batch Gradient Norm: 6.2482341560644254
Epoch: 144, Batch Gradient Norm after: 6.2482341560644254
Epoch 145/10000, Prediction Accuracy = 40.46%, Loss = 1.1411920547485352
Epoch: 145, Batch Gradient Norm: 9.948609324824904
Epoch: 145, Batch Gradient Norm after: 9.948609324824904
Epoch 146/10000, Prediction Accuracy = 40.76%, Loss = 1.1453193426132202
Epoch: 146, Batch Gradient Norm: 6.481585084244952
Epoch: 146, Batch Gradient Norm after: 6.481585084244952
Epoch 147/10000, Prediction Accuracy = 40.944%, Loss = 1.1213847637176513
Epoch: 147, Batch Gradient Norm: 11.207581953649386
Epoch: 147, Batch Gradient Norm after: 11.207581953649386
Epoch 148/10000, Prediction Accuracy = 41.246%, Loss = 1.1371131658554077
Epoch: 148, Batch Gradient Norm: 8.078660712099019
Epoch: 148, Batch Gradient Norm after: 8.078660712099019
Epoch 149/10000, Prediction Accuracy = 41.528%, Loss = 1.114366102218628
Epoch: 149, Batch Gradient Norm: 13.147255013762537
Epoch: 149, Batch Gradient Norm after: 13.147255013762537
Epoch 150/10000, Prediction Accuracy = 41.812%, Loss = 1.1336514234542847
Epoch: 150, Batch Gradient Norm: 7.628449659905363
Epoch: 150, Batch Gradient Norm after: 7.628449659905363
Epoch 151/10000, Prediction Accuracy = 42.034%, Loss = 1.0938470840454102
Epoch: 151, Batch Gradient Norm: 16.679046357130247
Epoch: 151, Batch Gradient Norm after: 16.679046357130247
Epoch 152/10000, Prediction Accuracy = 42.336%, Loss = 1.145337986946106
Epoch: 152, Batch Gradient Norm: 8.611755854084796
Epoch: 152, Batch Gradient Norm after: 8.611755854084796
Epoch 153/10000, Prediction Accuracy = 42.624%, Loss = 1.0867656946182251
Epoch: 153, Batch Gradient Norm: 12.476447251647535
Epoch: 153, Batch Gradient Norm after: 12.476447251647535
Epoch 154/10000, Prediction Accuracy = 43.254%, Loss = 1.0953070640563964
Epoch: 154, Batch Gradient Norm: 6.365708118039883
Epoch: 154, Batch Gradient Norm after: 6.365708118039883
Epoch 155/10000, Prediction Accuracy = 43.826%, Loss = 1.0591697692871094
Epoch: 155, Batch Gradient Norm: 11.526467807403757
Epoch: 155, Batch Gradient Norm after: 11.526467807403757
Epoch 156/10000, Prediction Accuracy = 44.926%, Loss = 1.0729267120361328
Epoch: 156, Batch Gradient Norm: 6.262119575552703
Epoch: 156, Batch Gradient Norm after: 6.262119575552703
Epoch 157/10000, Prediction Accuracy = 45.624%, Loss = 1.0471306920051575
Epoch: 157, Batch Gradient Norm: 13.649033243376918
Epoch: 157, Batch Gradient Norm after: 13.649033243376918
Epoch 158/10000, Prediction Accuracy = 46.278%, Loss = 1.0765366315841676
Epoch: 158, Batch Gradient Norm: 6.9727176906972845
Epoch: 158, Batch Gradient Norm after: 6.9727176906972845
Epoch 159/10000, Prediction Accuracy = 46.756%, Loss = 1.0423424005508424
Epoch: 159, Batch Gradient Norm: 12.520984639271363
Epoch: 159, Batch Gradient Norm after: 12.520984639271363
Epoch 160/10000, Prediction Accuracy = 47.209999999999994%, Loss = 1.0607057332992553
Epoch: 160, Batch Gradient Norm: 6.81109923503008
Epoch: 160, Batch Gradient Norm after: 6.81109923503008
Epoch 161/10000, Prediction Accuracy = 47.69000000000001%, Loss = 1.0323202013969421
Epoch: 161, Batch Gradient Norm: 12.229829425943546
Epoch: 161, Batch Gradient Norm after: 12.229829425943546
Epoch 162/10000, Prediction Accuracy = 48.132000000000005%, Loss = 1.0521204471588135
Epoch: 162, Batch Gradient Norm: 9.478910225629967
Epoch: 162, Batch Gradient Norm after: 9.478910225629967
Epoch 163/10000, Prediction Accuracy = 48.408%, Loss = 1.04070383310318
Epoch: 163, Batch Gradient Norm: 13.554716587140385
Epoch: 163, Batch Gradient Norm after: 13.554716587140385
Epoch 164/10000, Prediction Accuracy = 48.84%, Loss = 1.056480884552002
Epoch: 164, Batch Gradient Norm: 7.1899891548291395
Epoch: 164, Batch Gradient Norm after: 7.1899891548291395
Epoch 165/10000, Prediction Accuracy = 49.022000000000006%, Loss = 1.0204604268074036
Epoch: 165, Batch Gradient Norm: 12.144328229807652
Epoch: 165, Batch Gradient Norm after: 12.144328229807652
Epoch 166/10000, Prediction Accuracy = 49.316%, Loss = 1.0365320444107056
Epoch: 166, Batch Gradient Norm: 6.64139671892024
Epoch: 166, Batch Gradient Norm after: 6.64139671892024
Epoch 167/10000, Prediction Accuracy = 49.568%, Loss = 1.0125298976898194
Epoch: 167, Batch Gradient Norm: 13.243308081149726
Epoch: 167, Batch Gradient Norm after: 13.243308081149726
Epoch 168/10000, Prediction Accuracy = 49.709999999999994%, Loss = 1.0409080028533935
Epoch: 168, Batch Gradient Norm: 7.788312252234803
Epoch: 168, Batch Gradient Norm after: 7.788312252234803
Epoch 169/10000, Prediction Accuracy = 49.846%, Loss = 1.0138710021972657
Epoch: 169, Batch Gradient Norm: 12.054729616594319
Epoch: 169, Batch Gradient Norm after: 12.054729616594319
Epoch 170/10000, Prediction Accuracy = 50.114%, Loss = 1.0264045953750611
Epoch: 170, Batch Gradient Norm: 6.153755386588347
Epoch: 170, Batch Gradient Norm after: 6.153755386588347
Epoch 171/10000, Prediction Accuracy = 50.178%, Loss = 0.9981849670410157
Epoch: 171, Batch Gradient Norm: 11.431406237773043
Epoch: 171, Batch Gradient Norm after: 11.431406237773043
Epoch 172/10000, Prediction Accuracy = 50.418%, Loss = 1.0146979212760925
Epoch: 172, Batch Gradient Norm: 6.4131352684775695
Epoch: 172, Batch Gradient Norm after: 6.4131352684775695
Epoch 173/10000, Prediction Accuracy = 50.524%, Loss = 0.9944153666496277
Epoch: 173, Batch Gradient Norm: 13.448856970914466
Epoch: 173, Batch Gradient Norm after: 13.448856970914466
Epoch 174/10000, Prediction Accuracy = 50.629999999999995%, Loss = 1.0233994364738463
Epoch: 174, Batch Gradient Norm: 7.8766767112336975
Epoch: 174, Batch Gradient Norm after: 7.8766767112336975
Epoch 175/10000, Prediction Accuracy = 50.75999999999999%, Loss = 0.9980041027069092
Epoch: 175, Batch Gradient Norm: 13.686497430318116
Epoch: 175, Batch Gradient Norm after: 13.686497430318116
Epoch 176/10000, Prediction Accuracy = 50.874%, Loss = 1.0198387145996093
Epoch: 176, Batch Gradient Norm: 6.566342319875844
Epoch: 176, Batch Gradient Norm after: 6.566342319875844
Epoch 177/10000, Prediction Accuracy = 50.966%, Loss = 0.9851423501968384
Epoch: 177, Batch Gradient Norm: 13.202115659953241
Epoch: 177, Batch Gradient Norm after: 13.202115659953241
Epoch 178/10000, Prediction Accuracy = 51.092%, Loss = 1.0096047639846801
Epoch: 178, Batch Gradient Norm: 6.775226843427325
Epoch: 178, Batch Gradient Norm after: 6.775226843427325
Epoch 179/10000, Prediction Accuracy = 51.11%, Loss = 0.9810674309730529
Epoch: 179, Batch Gradient Norm: 14.387805158738937
Epoch: 179, Batch Gradient Norm after: 14.387805158738937
Epoch 180/10000, Prediction Accuracy = 51.278%, Loss = 1.0149913787841798
Epoch: 180, Batch Gradient Norm: 8.067154655420545
Epoch: 180, Batch Gradient Norm after: 8.067154655420545
Epoch 181/10000, Prediction Accuracy = 51.239999999999995%, Loss = 0.9833709836006165
Epoch: 181, Batch Gradient Norm: 14.266223990664916
Epoch: 181, Batch Gradient Norm after: 14.266223990664916
Epoch 182/10000, Prediction Accuracy = 51.374%, Loss = 1.0116089224815368
Epoch: 182, Batch Gradient Norm: 12.074100985884556
Epoch: 182, Batch Gradient Norm after: 12.074100985884556
Epoch 183/10000, Prediction Accuracy = 51.314%, Loss = 1.0071071267127991
Epoch: 183, Batch Gradient Norm: 12.687833602740351
Epoch: 183, Batch Gradient Norm after: 12.687833602740351
Epoch 184/10000, Prediction Accuracy = 51.402%, Loss = 0.9991535425186158
Epoch: 184, Batch Gradient Norm: 7.220710046519261
Epoch: 184, Batch Gradient Norm after: 7.220710046519261
Epoch 185/10000, Prediction Accuracy = 51.464%, Loss = 0.9697927713394165
Epoch: 185, Batch Gradient Norm: 10.093980150392978
Epoch: 185, Batch Gradient Norm after: 10.093980150392978
Epoch 186/10000, Prediction Accuracy = 51.504%, Loss = 0.9742954134941101
Epoch: 186, Batch Gradient Norm: 5.564443086362598
Epoch: 186, Batch Gradient Norm after: 5.564443086362598
Epoch 187/10000, Prediction Accuracy = 51.534000000000006%, Loss = 0.958220899105072
Epoch: 187, Batch Gradient Norm: 12.930163508016376
Epoch: 187, Batch Gradient Norm after: 12.930163508016376
Epoch 188/10000, Prediction Accuracy = 51.641999999999996%, Loss = 0.9830012321472168
Epoch: 188, Batch Gradient Norm: 6.764856671546765
Epoch: 188, Batch Gradient Norm after: 6.764856671546765
Epoch 189/10000, Prediction Accuracy = 51.67%, Loss = 0.9581991553306579
Epoch: 189, Batch Gradient Norm: 16.25274877016972
Epoch: 189, Batch Gradient Norm after: 16.25274877016972
Epoch 190/10000, Prediction Accuracy = 51.693999999999996%, Loss = 1.002640163898468
Epoch: 190, Batch Gradient Norm: 7.9912623029225305
Epoch: 190, Batch Gradient Norm after: 7.9912623029225305
Epoch 191/10000, Prediction Accuracy = 51.73%, Loss = 0.9638299226760865
Epoch: 191, Batch Gradient Norm: 14.422758401627375
Epoch: 191, Batch Gradient Norm after: 14.422758401627375
Epoch 192/10000, Prediction Accuracy = 51.746%, Loss = 0.9955030083656311
Epoch: 192, Batch Gradient Norm: 7.59164411911094
Epoch: 192, Batch Gradient Norm after: 7.59164411911094
Epoch 193/10000, Prediction Accuracy = 51.772000000000006%, Loss = 0.9572142601013184
Epoch: 193, Batch Gradient Norm: 11.268890728547397
Epoch: 193, Batch Gradient Norm after: 11.268890728547397
Epoch 194/10000, Prediction Accuracy = 51.83%, Loss = 0.9672162532806396
Epoch: 194, Batch Gradient Norm: 6.227051520958016
Epoch: 194, Batch Gradient Norm after: 6.227051520958016
Epoch 195/10000, Prediction Accuracy = 51.884%, Loss = 0.945352840423584
Epoch: 195, Batch Gradient Norm: 12.782938862684178
Epoch: 195, Batch Gradient Norm after: 12.782938862684178
Epoch 196/10000, Prediction Accuracy = 51.95399999999999%, Loss = 0.9714233994483947
Epoch: 196, Batch Gradient Norm: 9.081843674851722
Epoch: 196, Batch Gradient Norm after: 9.081843674851722
Epoch 197/10000, Prediction Accuracy = 51.94199999999999%, Loss = 0.955928635597229
Epoch: 197, Batch Gradient Norm: 15.707244780300863
Epoch: 197, Batch Gradient Norm after: 15.707244780300863
Epoch 198/10000, Prediction Accuracy = 52.022000000000006%, Loss = 0.9897529125213623
Epoch: 198, Batch Gradient Norm: 10.692240772065158
Epoch: 198, Batch Gradient Norm after: 10.692240772065158
Epoch 199/10000, Prediction Accuracy = 52.068%, Loss = 0.9625010132789612
Epoch: 199, Batch Gradient Norm: 12.73626061143956
Epoch: 199, Batch Gradient Norm after: 12.73626061143956
Epoch 200/10000, Prediction Accuracy = 52.074%, Loss = 0.9643696665763855
Epoch: 200, Batch Gradient Norm: 7.379018255622048
Epoch: 200, Batch Gradient Norm after: 7.379018255622048
Epoch 201/10000, Prediction Accuracy = 52.193999999999996%, Loss = 0.9399078607559204
Epoch: 201, Batch Gradient Norm: 12.265998286298233
Epoch: 201, Batch Gradient Norm after: 12.265998286298233
Epoch 202/10000, Prediction Accuracy = 52.16799999999999%, Loss = 0.9541903972625733
Epoch: 202, Batch Gradient Norm: 6.7400617361073
Epoch: 202, Batch Gradient Norm after: 6.7400617361073
Epoch 203/10000, Prediction Accuracy = 52.174%, Loss = 0.9331117987632751
Epoch: 203, Batch Gradient Norm: 15.429597538890194
Epoch: 203, Batch Gradient Norm after: 15.429597538890194
Epoch 204/10000, Prediction Accuracy = 52.21199999999999%, Loss = 0.9685859084129333
Epoch: 204, Batch Gradient Norm: 7.037576457134899
Epoch: 204, Batch Gradient Norm after: 7.037576457134899
Epoch 205/10000, Prediction Accuracy = 52.258%, Loss = 0.9316492199897766
Epoch: 205, Batch Gradient Norm: 15.016171254788173
Epoch: 205, Batch Gradient Norm after: 15.016171254788173
Epoch 206/10000, Prediction Accuracy = 52.239999999999995%, Loss = 0.9645860075950623
Epoch: 206, Batch Gradient Norm: 8.48584215089951
Epoch: 206, Batch Gradient Norm after: 8.48584215089951
Epoch 207/10000, Prediction Accuracy = 52.29200000000001%, Loss = 0.9367873191833496
Epoch: 207, Batch Gradient Norm: 15.505359806300953
Epoch: 207, Batch Gradient Norm after: 15.505359806300953
Epoch 208/10000, Prediction Accuracy = 52.31%, Loss = 0.9706534028053284
Epoch: 208, Batch Gradient Norm: 7.855251873148492
Epoch: 208, Batch Gradient Norm after: 7.855251873148492
Epoch 209/10000, Prediction Accuracy = 52.384%, Loss = 0.9297126173973084
Epoch: 209, Batch Gradient Norm: 13.161622528802333
Epoch: 209, Batch Gradient Norm after: 13.161622528802333
Epoch 210/10000, Prediction Accuracy = 52.412%, Loss = 0.9490182280540467
Epoch: 210, Batch Gradient Norm: 7.172045402367176
Epoch: 210, Batch Gradient Norm after: 7.172045402367176
Epoch 211/10000, Prediction Accuracy = 52.42%, Loss = 0.9226770043373108
Epoch: 211, Batch Gradient Norm: 13.438792257094262
Epoch: 211, Batch Gradient Norm after: 13.438792257094262
Epoch 212/10000, Prediction Accuracy = 52.467999999999996%, Loss = 0.9480515718460083
Epoch: 212, Batch Gradient Norm: 9.249707555897778
Epoch: 212, Batch Gradient Norm after: 9.249707555897778
Epoch 213/10000, Prediction Accuracy = 52.501999999999995%, Loss = 0.9300819993019104
Epoch: 213, Batch Gradient Norm: 13.920018447307836
Epoch: 213, Batch Gradient Norm after: 13.920018447307836
Epoch 214/10000, Prediction Accuracy = 52.538%, Loss = 0.9506594896316528
Epoch: 214, Batch Gradient Norm: 9.937143220218113
Epoch: 214, Batch Gradient Norm after: 9.937143220218113
Epoch 215/10000, Prediction Accuracy = 52.55%, Loss = 0.9316735744476319
Epoch: 215, Batch Gradient Norm: 11.866137626717094
Epoch: 215, Batch Gradient Norm after: 11.866137626717094
Epoch 216/10000, Prediction Accuracy = 52.61800000000001%, Loss = 0.9322357654571534
Epoch: 216, Batch Gradient Norm: 7.158147576167429
Epoch: 216, Batch Gradient Norm after: 7.158147576167429
Epoch 217/10000, Prediction Accuracy = 52.61%, Loss = 0.9129107832908631
Epoch: 217, Batch Gradient Norm: 14.107991810667889
Epoch: 217, Batch Gradient Norm after: 14.107991810667889
Epoch 218/10000, Prediction Accuracy = 52.722%, Loss = 0.9374048113822937
Epoch: 218, Batch Gradient Norm: 7.974524903338175
Epoch: 218, Batch Gradient Norm after: 7.974524903338175
Epoch 219/10000, Prediction Accuracy = 52.656000000000006%, Loss = 0.912069571018219
Epoch: 219, Batch Gradient Norm: 16.72052843225859
Epoch: 219, Batch Gradient Norm after: 16.72052843225859
Epoch 220/10000, Prediction Accuracy = 52.766%, Loss = 0.953388798236847
Epoch: 220, Batch Gradient Norm: 7.705631672590705
Epoch: 220, Batch Gradient Norm after: 7.705631672590705
Epoch 221/10000, Prediction Accuracy = 52.736000000000004%, Loss = 0.911172878742218
Epoch: 221, Batch Gradient Norm: 13.762838853290171
Epoch: 221, Batch Gradient Norm after: 13.762838853290171
Epoch 222/10000, Prediction Accuracy = 52.83200000000001%, Loss = 0.9363333582878113
Epoch: 222, Batch Gradient Norm: 8.16916730882255
Epoch: 222, Batch Gradient Norm after: 8.16916730882255
Epoch 223/10000, Prediction Accuracy = 52.758%, Loss = 0.9109670996665955
Epoch: 223, Batch Gradient Norm: 12.696171093805587
Epoch: 223, Batch Gradient Norm after: 12.696171093805587
Epoch 224/10000, Prediction Accuracy = 52.872%, Loss = 0.9287049174308777
Epoch: 224, Batch Gradient Norm: 9.225226551930636
Epoch: 224, Batch Gradient Norm after: 9.225226551930636
Epoch 225/10000, Prediction Accuracy = 52.826%, Loss = 0.9125357627868652
Epoch: 225, Batch Gradient Norm: 13.122930196376226
Epoch: 225, Batch Gradient Norm after: 13.122930196376226
Epoch 226/10000, Prediction Accuracy = 52.962%, Loss = 0.9283579230308533
Epoch: 226, Batch Gradient Norm: 9.76889969140696
Epoch: 226, Batch Gradient Norm after: 9.76889969140696
Epoch 227/10000, Prediction Accuracy = 52.924%, Loss = 0.9116021752357483
Epoch: 227, Batch Gradient Norm: 12.871326527079896
Epoch: 227, Batch Gradient Norm after: 12.871326527079896
Epoch 228/10000, Prediction Accuracy = 52.996%, Loss = 0.9200851559638977
Epoch: 228, Batch Gradient Norm: 8.468662387258382
Epoch: 228, Batch Gradient Norm after: 8.468662387258382
Epoch 229/10000, Prediction Accuracy = 52.99400000000001%, Loss = 0.9011372089385986
Epoch: 229, Batch Gradient Norm: 15.801704689703001
Epoch: 229, Batch Gradient Norm after: 15.801704689703001
Epoch 230/10000, Prediction Accuracy = 53.044%, Loss = 0.9316665410995484
Epoch: 230, Batch Gradient Norm: 8.111893464985416
Epoch: 230, Batch Gradient Norm after: 8.111893464985416
Epoch 231/10000, Prediction Accuracy = 53.08200000000001%, Loss = 0.8967288851737976
Epoch: 231, Batch Gradient Norm: 14.910491805553194
Epoch: 231, Batch Gradient Norm after: 14.910491805553194
Epoch 232/10000, Prediction Accuracy = 53.096000000000004%, Loss = 0.923287296295166
Epoch: 232, Batch Gradient Norm: 6.591710659782245
Epoch: 232, Batch Gradient Norm after: 6.591710659782245
Epoch 233/10000, Prediction Accuracy = 53.122%, Loss = 0.8892435312271119
Epoch: 233, Batch Gradient Norm: 13.530451274341655
Epoch: 233, Batch Gradient Norm after: 13.530451274341655
Epoch 234/10000, Prediction Accuracy = 53.15599999999999%, Loss = 0.9138732433319092
Epoch: 234, Batch Gradient Norm: 8.370700080890536
Epoch: 234, Batch Gradient Norm after: 8.370700080890536
Epoch 235/10000, Prediction Accuracy = 53.262%, Loss = 0.8953826189041137
Epoch: 235, Batch Gradient Norm: 17.49623481448835
Epoch: 235, Batch Gradient Norm after: 17.49623481448835
Epoch 236/10000, Prediction Accuracy = 53.24399999999999%, Loss = 0.9452292442321777
Epoch: 236, Batch Gradient Norm: 10.846949471638906
Epoch: 236, Batch Gradient Norm after: 10.846949471638906
Epoch 237/10000, Prediction Accuracy = 53.3%, Loss = 0.9071499943733216
Epoch: 237, Batch Gradient Norm: 16.570534424734394
Epoch: 237, Batch Gradient Norm after: 16.570534424734394
Epoch 238/10000, Prediction Accuracy = 53.30799999999999%, Loss = 0.9363054275512696
Epoch: 238, Batch Gradient Norm: 7.5927386612733
Epoch: 238, Batch Gradient Norm after: 7.5927386612733
Epoch 239/10000, Prediction Accuracy = 53.352%, Loss = 0.8867623329162597
Epoch: 239, Batch Gradient Norm: 13.375192947202978
Epoch: 239, Batch Gradient Norm after: 13.375192947202978
Epoch 240/10000, Prediction Accuracy = 53.403999999999996%, Loss = 0.9071951627731323
Epoch: 240, Batch Gradient Norm: 6.776887970572508
Epoch: 240, Batch Gradient Norm after: 6.776887970572508
Epoch 241/10000, Prediction Accuracy = 53.403999999999996%, Loss = 0.8796100497245789
Epoch: 241, Batch Gradient Norm: 13.763456668216767
Epoch: 241, Batch Gradient Norm after: 13.763456668216767
Epoch 242/10000, Prediction Accuracy = 53.49400000000001%, Loss = 0.9069185256958008
Epoch: 242, Batch Gradient Norm: 9.266165972382554
Epoch: 242, Batch Gradient Norm after: 9.266165972382554
Epoch 243/10000, Prediction Accuracy = 53.45399999999999%, Loss = 0.8901684880256653
Epoch: 243, Batch Gradient Norm: 14.131278315076983
Epoch: 243, Batch Gradient Norm after: 14.131278315076983
Epoch 244/10000, Prediction Accuracy = 53.54%, Loss = 0.9088847875595093
Epoch: 244, Batch Gradient Norm: 7.892263471315631
Epoch: 244, Batch Gradient Norm after: 7.892263471315631
Epoch 245/10000, Prediction Accuracy = 53.532%, Loss = 0.8801875710487366
Epoch: 245, Batch Gradient Norm: 12.819424154565853
Epoch: 245, Batch Gradient Norm after: 12.819424154565853
Epoch 246/10000, Prediction Accuracy = 53.60600000000001%, Loss = 0.8942359805107116
Epoch: 246, Batch Gradient Norm: 6.9703056144654365
Epoch: 246, Batch Gradient Norm after: 6.9703056144654365
Epoch 247/10000, Prediction Accuracy = 53.565999999999995%, Loss = 0.8720491409301758
Epoch: 247, Batch Gradient Norm: 15.344998042629413
Epoch: 247, Batch Gradient Norm after: 15.344998042629413
Epoch 248/10000, Prediction Accuracy = 53.636%, Loss = 0.9053571820259094
Epoch: 248, Batch Gradient Norm: 10.0078776720301
Epoch: 248, Batch Gradient Norm after: 10.0078776720301
Epoch 249/10000, Prediction Accuracy = 53.617999999999995%, Loss = 0.8848608613014222
Epoch: 249, Batch Gradient Norm: 16.45018186128954
Epoch: 249, Batch Gradient Norm after: 16.45018186128954
Epoch 250/10000, Prediction Accuracy = 53.684000000000005%, Loss = 0.9197110295295715
Epoch: 250, Batch Gradient Norm: 10.419162790817525
Epoch: 250, Batch Gradient Norm after: 10.419162790817525
Epoch 251/10000, Prediction Accuracy = 53.67999999999999%, Loss = 0.8867778778076172
Epoch: 251, Batch Gradient Norm: 11.799689867997243
Epoch: 251, Batch Gradient Norm after: 11.799689867997243
Epoch 252/10000, Prediction Accuracy = 53.748000000000005%, Loss = 0.8863477230072021
Epoch: 252, Batch Gradient Norm: 7.640707607049376
Epoch: 252, Batch Gradient Norm after: 7.640707607049376
Epoch 253/10000, Prediction Accuracy = 53.81%, Loss = 0.869569730758667
Epoch: 253, Batch Gradient Norm: 14.049973585404292
Epoch: 253, Batch Gradient Norm after: 14.049973585404292
Epoch 254/10000, Prediction Accuracy = 53.870000000000005%, Loss = 0.8986053586006164
Epoch: 254, Batch Gradient Norm: 8.672206537725538
Epoch: 254, Batch Gradient Norm after: 8.672206537725538
Epoch 255/10000, Prediction Accuracy = 53.826%, Loss = 0.8722816705703735
Epoch: 255, Batch Gradient Norm: 16.772811564979985
Epoch: 255, Batch Gradient Norm after: 16.772811564979985
Epoch 256/10000, Prediction Accuracy = 53.85799999999999%, Loss = 0.908870792388916
Epoch: 256, Batch Gradient Norm: 7.864887695008465
Epoch: 256, Batch Gradient Norm after: 7.864887695008465
Epoch 257/10000, Prediction Accuracy = 53.839999999999996%, Loss = 0.8648325681686402
Epoch: 257, Batch Gradient Norm: 17.304419061631602
Epoch: 257, Batch Gradient Norm after: 17.304419061631602
Epoch 258/10000, Prediction Accuracy = 53.918000000000006%, Loss = 0.9141491293907166
Epoch: 258, Batch Gradient Norm: 10.489006697428369
Epoch: 258, Batch Gradient Norm after: 10.489006697428369
Epoch 259/10000, Prediction Accuracy = 53.944%, Loss = 0.8799627780914306
Epoch: 259, Batch Gradient Norm: 14.268218015982553
Epoch: 259, Batch Gradient Norm after: 14.268218015982553
Epoch 260/10000, Prediction Accuracy = 53.998000000000005%, Loss = 0.8956876158714294
Epoch: 260, Batch Gradient Norm: 7.098079848902207
Epoch: 260, Batch Gradient Norm after: 7.098079848902207
Epoch 261/10000, Prediction Accuracy = 54.06199999999999%, Loss = 0.8586951851844787
Epoch: 261, Batch Gradient Norm: 10.191388271209568
Epoch: 261, Batch Gradient Norm after: 10.191388271209568
Epoch 262/10000, Prediction Accuracy = 54.102%, Loss = 0.8665291428565979
Epoch: 262, Batch Gradient Norm: 5.757118842624365
Epoch: 262, Batch Gradient Norm after: 5.757118842624365
Epoch 263/10000, Prediction Accuracy = 54.1%, Loss = 0.8508099317550659
Epoch: 263, Batch Gradient Norm: 12.092263926107517
Epoch: 263, Batch Gradient Norm after: 12.092263926107517
Epoch 264/10000, Prediction Accuracy = 54.122%, Loss = 0.8715587019920349
Epoch: 264, Batch Gradient Norm: 9.213321612788315
Epoch: 264, Batch Gradient Norm after: 9.213321612788315
Epoch 265/10000, Prediction Accuracy = 54.1%, Loss = 0.8619593262672425
Epoch: 265, Batch Gradient Norm: 19.143415105629412
Epoch: 265, Batch Gradient Norm after: 19.143415105629412
Epoch 266/10000, Prediction Accuracy = 54.072%, Loss = 0.9171404242515564
Epoch: 266, Batch Gradient Norm: 10.823296577760603
Epoch: 266, Batch Gradient Norm after: 10.823296577760603
Epoch 267/10000, Prediction Accuracy = 54.096000000000004%, Loss = 0.8719901084899903
Epoch: 267, Batch Gradient Norm: 13.621534155004536
Epoch: 267, Batch Gradient Norm after: 13.621534155004536
Epoch 268/10000, Prediction Accuracy = 54.122%, Loss = 0.8800291895866394
Epoch: 268, Batch Gradient Norm: 10.095792148960792
Epoch: 268, Batch Gradient Norm after: 10.095792148960792
Epoch 269/10000, Prediction Accuracy = 54.220000000000006%, Loss = 0.8653529524803162
Epoch: 269, Batch Gradient Norm: 12.728291547965231
Epoch: 269, Batch Gradient Norm after: 12.728291547965231
Epoch 270/10000, Prediction Accuracy = 54.227999999999994%, Loss = 0.8712603926658631
Epoch: 270, Batch Gradient Norm: 9.059356613261455
Epoch: 270, Batch Gradient Norm after: 9.059356613261455
Epoch 271/10000, Prediction Accuracy = 54.251999999999995%, Loss = 0.8575211048126221
Epoch: 271, Batch Gradient Norm: 16.942234113327316
Epoch: 271, Batch Gradient Norm after: 16.942234113327316
Epoch 272/10000, Prediction Accuracy = 54.25600000000001%, Loss = 0.8946870565414429
Epoch: 272, Batch Gradient Norm: 9.576844641234086
Epoch: 272, Batch Gradient Norm after: 9.576844641234086
Epoch 273/10000, Prediction Accuracy = 54.303999999999995%, Loss = 0.8588255047798157
Epoch: 273, Batch Gradient Norm: 16.23449486433334
Epoch: 273, Batch Gradient Norm after: 16.23449486433334
Epoch 274/10000, Prediction Accuracy = 54.278%, Loss = 0.8891903400421143
Epoch: 274, Batch Gradient Norm: 8.22514406524129
Epoch: 274, Batch Gradient Norm after: 8.22514406524129
Epoch 275/10000, Prediction Accuracy = 54.394000000000005%, Loss = 0.8505488395690918
Epoch: 275, Batch Gradient Norm: 14.449693177319869
Epoch: 275, Batch Gradient Norm after: 14.449693177319869
Epoch 276/10000, Prediction Accuracy = 54.386%, Loss = 0.8784111976623535
Epoch: 276, Batch Gradient Norm: 9.596366629214385
Epoch: 276, Batch Gradient Norm after: 9.596366629214385
Epoch 277/10000, Prediction Accuracy = 54.406000000000006%, Loss = 0.8569195747375489
Epoch: 277, Batch Gradient Norm: 11.913434863556992
Epoch: 277, Batch Gradient Norm after: 11.913434863556992
Epoch 278/10000, Prediction Accuracy = 54.46999999999999%, Loss = 0.862501609325409
Epoch: 278, Batch Gradient Norm: 7.206496699593675
Epoch: 278, Batch Gradient Norm after: 7.206496699593675
Epoch 279/10000, Prediction Accuracy = 54.464%, Loss = 0.8415920495986938
Epoch: 279, Batch Gradient Norm: 9.888148934884937
Epoch: 279, Batch Gradient Norm after: 9.888148934884937
Epoch 280/10000, Prediction Accuracy = 54.548%, Loss = 0.846651840209961
Epoch: 280, Batch Gradient Norm: 6.017590029257832
Epoch: 280, Batch Gradient Norm after: 6.017590029257832
Epoch 281/10000, Prediction Accuracy = 54.536%, Loss = 0.8342410802841187
Epoch: 281, Batch Gradient Norm: 16.681226139719936
Epoch: 281, Batch Gradient Norm after: 16.681226139719936
Epoch 282/10000, Prediction Accuracy = 54.482000000000006%, Loss = 0.8777764081954956
Epoch: 282, Batch Gradient Norm: 9.885446364402705
Epoch: 282, Batch Gradient Norm after: 9.885446364402705
Epoch 283/10000, Prediction Accuracy = 54.536%, Loss = 0.8458290457725525
Epoch: 283, Batch Gradient Norm: 16.743669852443738
Epoch: 283, Batch Gradient Norm after: 16.743669852443738
Epoch 284/10000, Prediction Accuracy = 54.501999999999995%, Loss = 0.8793984413146972
Epoch: 284, Batch Gradient Norm: 10.397918093592912
Epoch: 284, Batch Gradient Norm after: 10.397918093592912
Epoch 285/10000, Prediction Accuracy = 54.572%, Loss = 0.8507828712463379
Epoch: 285, Batch Gradient Norm: 15.181246478325281
Epoch: 285, Batch Gradient Norm after: 15.181246478325281
Epoch 286/10000, Prediction Accuracy = 54.636%, Loss = 0.8758036255836487
Epoch: 286, Batch Gradient Norm: 13.035686969482388
Epoch: 286, Batch Gradient Norm after: 13.035686969482388
Epoch 287/10000, Prediction Accuracy = 54.653999999999996%, Loss = 0.8664537787437439
Epoch: 287, Batch Gradient Norm: 11.053833316043633
Epoch: 287, Batch Gradient Norm after: 11.053833316043633
Epoch 288/10000, Prediction Accuracy = 54.63599999999999%, Loss = 0.8525032043457031
Epoch: 288, Batch Gradient Norm: 10.965475997179563
Epoch: 288, Batch Gradient Norm after: 10.965475997179563
Epoch 289/10000, Prediction Accuracy = 54.77%, Loss = 0.8494848728179931
Epoch: 289, Batch Gradient Norm: 7.236778521462603
Epoch: 289, Batch Gradient Norm after: 7.236778521462603
Epoch 290/10000, Prediction Accuracy = 54.748000000000005%, Loss = 0.8323797941207886
Epoch: 290, Batch Gradient Norm: 15.801887399848797
Epoch: 290, Batch Gradient Norm after: 15.801887399848797
Epoch 291/10000, Prediction Accuracy = 54.76800000000001%, Loss = 0.8691718578338623
Epoch: 291, Batch Gradient Norm: 10.06543711347984
Epoch: 291, Batch Gradient Norm after: 10.06543711347984
Epoch 292/10000, Prediction Accuracy = 54.784000000000006%, Loss = 0.8389087200164795
Epoch: 292, Batch Gradient Norm: 18.923633520163293
Epoch: 292, Batch Gradient Norm after: 18.923633520163293
Epoch 293/10000, Prediction Accuracy = 54.75%, Loss = 0.8926337361335754
Epoch: 293, Batch Gradient Norm: 9.693647939303622
Epoch: 293, Batch Gradient Norm after: 9.693647939303622
Epoch 294/10000, Prediction Accuracy = 54.836%, Loss = 0.8416645765304566
Epoch: 294, Batch Gradient Norm: 14.56559578564357
Epoch: 294, Batch Gradient Norm after: 14.56559578564357
Epoch 295/10000, Prediction Accuracy = 54.790000000000006%, Loss = 0.8658882856369019
Epoch: 295, Batch Gradient Norm: 8.020084426603448
Epoch: 295, Batch Gradient Norm after: 8.020084426603448
Epoch 296/10000, Prediction Accuracy = 54.902%, Loss = 0.8308478355407715
Epoch: 296, Batch Gradient Norm: 10.423794809576169
Epoch: 296, Batch Gradient Norm after: 10.423794809576169
Epoch 297/10000, Prediction Accuracy = 54.879999999999995%, Loss = 0.8347231388092041
Epoch: 297, Batch Gradient Norm: 5.994912295623342
Epoch: 297, Batch Gradient Norm after: 5.994912295623342
Epoch 298/10000, Prediction Accuracy = 54.888%, Loss = 0.820296561717987
Epoch: 298, Batch Gradient Norm: 13.954214325986698
Epoch: 298, Batch Gradient Norm after: 13.954214325986698
Epoch 299/10000, Prediction Accuracy = 54.86800000000001%, Loss = 0.8479905962944031
Epoch: 299, Batch Gradient Norm: 9.658728582057414
Epoch: 299, Batch Gradient Norm after: 9.658728582057414
Epoch 300/10000, Prediction Accuracy = 54.838%, Loss = 0.832960057258606
Epoch: 300, Batch Gradient Norm: 17.750206247890333
Epoch: 300, Batch Gradient Norm after: 17.750206247890333
Epoch 301/10000, Prediction Accuracy = 54.924%, Loss = 0.8752007007598877
Epoch: 301, Batch Gradient Norm: 8.39252262213568
Epoch: 301, Batch Gradient Norm after: 8.39252262213568
Epoch 302/10000, Prediction Accuracy = 54.924%, Loss = 0.8280599474906921
Epoch: 302, Batch Gradient Norm: 12.73208874727432
Epoch: 302, Batch Gradient Norm after: 12.73208874727432
Epoch 303/10000, Prediction Accuracy = 54.96%, Loss = 0.8446801424026489
Epoch: 303, Batch Gradient Norm: 10.190833588744468
Epoch: 303, Batch Gradient Norm after: 10.190833588744468
Epoch 304/10000, Prediction Accuracy = 54.944%, Loss = 0.8324457287788392
Epoch: 304, Batch Gradient Norm: 13.132924553465484
Epoch: 304, Batch Gradient Norm after: 13.132924553465484
Epoch 305/10000, Prediction Accuracy = 55.00599999999999%, Loss = 0.8505441546440125
Epoch: 305, Batch Gradient Norm: 13.572001803466724
Epoch: 305, Batch Gradient Norm after: 13.572001803466724
Epoch 306/10000, Prediction Accuracy = 55.034000000000006%, Loss = 0.846876347064972
Epoch: 306, Batch Gradient Norm: 11.026461012901018
Epoch: 306, Batch Gradient Norm after: 11.026461012901018
Epoch 307/10000, Prediction Accuracy = 54.988%, Loss = 0.8342345356941223
Epoch: 307, Batch Gradient Norm: 17.902034841113093
Epoch: 307, Batch Gradient Norm after: 17.902034841113093
Epoch 308/10000, Prediction Accuracy = 55.052%, Loss = 0.8678899526596069
Epoch: 308, Batch Gradient Norm: 8.448359052258626
Epoch: 308, Batch Gradient Norm after: 8.448359052258626
Epoch 309/10000, Prediction Accuracy = 55.05800000000001%, Loss = 0.8199402332305908
Epoch: 309, Batch Gradient Norm: 14.173136953287877
Epoch: 309, Batch Gradient Norm after: 14.173136953287877
Epoch 310/10000, Prediction Accuracy = 55.141999999999996%, Loss = 0.8444052815437317
Epoch: 310, Batch Gradient Norm: 9.427858056436522
Epoch: 310, Batch Gradient Norm after: 9.427858056436522
Epoch 311/10000, Prediction Accuracy = 55.074%, Loss = 0.8252625107765198
Epoch: 311, Batch Gradient Norm: 13.963369671170245
Epoch: 311, Batch Gradient Norm after: 13.963369671170245
Epoch 312/10000, Prediction Accuracy = 55.168000000000006%, Loss = 0.8457218170166015
Epoch: 312, Batch Gradient Norm: 8.20185373983627
Epoch: 312, Batch Gradient Norm after: 8.20185373983627
Epoch 313/10000, Prediction Accuracy = 55.14%, Loss = 0.8174486517906189
Epoch: 313, Batch Gradient Norm: 13.036465784696288
Epoch: 313, Batch Gradient Norm after: 13.036465784696288
Epoch 314/10000, Prediction Accuracy = 55.152%, Loss = 0.8338059544563293
Epoch: 314, Batch Gradient Norm: 7.783101251851973
Epoch: 314, Batch Gradient Norm after: 7.783101251851973
Epoch 315/10000, Prediction Accuracy = 55.2%, Loss = 0.8122622728347778
Epoch: 315, Batch Gradient Norm: 18.608276869046563
Epoch: 315, Batch Gradient Norm after: 18.608276869046563
Epoch 316/10000, Prediction Accuracy = 55.194%, Loss = 0.871458375453949
Epoch: 316, Batch Gradient Norm: 11.159879602390358
Epoch: 316, Batch Gradient Norm after: 11.159879602390358
Epoch 317/10000, Prediction Accuracy = 55.262%, Loss = 0.8310206413269043
Epoch: 317, Batch Gradient Norm: 15.662395938513605
Epoch: 317, Batch Gradient Norm after: 15.662395938513605
Epoch 318/10000, Prediction Accuracy = 55.25600000000001%, Loss = 0.8507585406303406
Epoch: 318, Batch Gradient Norm: 6.190162192813674
Epoch: 318, Batch Gradient Norm after: 6.190162192813674
Epoch 319/10000, Prediction Accuracy = 55.312%, Loss = 0.8041415333747863
Epoch: 319, Batch Gradient Norm: 12.361791985822416
Epoch: 319, Batch Gradient Norm after: 12.361791985822416
Epoch 320/10000, Prediction Accuracy = 55.306%, Loss = 0.8242749094963073
Epoch: 320, Batch Gradient Norm: 6.238587193901055
Epoch: 320, Batch Gradient Norm after: 6.238587193901055
Epoch 321/10000, Prediction Accuracy = 55.282000000000004%, Loss = 0.8020081400871277
Epoch: 321, Batch Gradient Norm: 15.398553980917029
Epoch: 321, Batch Gradient Norm after: 15.398553980917029
Epoch 322/10000, Prediction Accuracy = 55.266%, Loss = 0.8403333902359009
Epoch: 322, Batch Gradient Norm: 10.076529859149812
Epoch: 322, Batch Gradient Norm after: 10.076529859149812
Epoch 323/10000, Prediction Accuracy = 55.31400000000001%, Loss = 0.8200656175613403
Epoch: 323, Batch Gradient Norm: 15.176784484327595
Epoch: 323, Batch Gradient Norm after: 15.176784484327595
Epoch 324/10000, Prediction Accuracy = 55.263999999999996%, Loss = 0.8420028686523438
Epoch: 324, Batch Gradient Norm: 11.220289000659516
Epoch: 324, Batch Gradient Norm after: 11.220289000659516
Epoch 325/10000, Prediction Accuracy = 55.303999999999995%, Loss = 0.8242687463760376
Epoch: 325, Batch Gradient Norm: 16.017064566019723
Epoch: 325, Batch Gradient Norm after: 16.017064566019723
Epoch 326/10000, Prediction Accuracy = 55.338%, Loss = 0.8520946502685547
Epoch: 326, Batch Gradient Norm: 14.85549873851866
Epoch: 326, Batch Gradient Norm after: 14.85549873851866
Epoch 327/10000, Prediction Accuracy = 55.378%, Loss = 0.847211217880249
Epoch: 327, Batch Gradient Norm: 9.775607509793428
Epoch: 327, Batch Gradient Norm after: 9.775607509793428
Epoch 328/10000, Prediction Accuracy = 55.426%, Loss = 0.8113123536109924
Epoch: 328, Batch Gradient Norm: 6.541711390350324
Epoch: 328, Batch Gradient Norm after: 6.541711390350324
Epoch 329/10000, Prediction Accuracy = 55.513999999999996%, Loss = 0.7964725732803345
Epoch: 329, Batch Gradient Norm: 9.670764754204136
Epoch: 329, Batch Gradient Norm after: 9.670764754204136
Epoch 330/10000, Prediction Accuracy = 55.51800000000001%, Loss = 0.8043129563331604
Epoch: 330, Batch Gradient Norm: 9.008634562198134
Epoch: 330, Batch Gradient Norm after: 9.008634562198134
Epoch 331/10000, Prediction Accuracy = 55.462%, Loss = 0.8013043999671936
Epoch: 331, Batch Gradient Norm: 20.17390044813504
Epoch: 331, Batch Gradient Norm after: 19.535300488324182
Epoch 332/10000, Prediction Accuracy = 55.452%, Loss = 0.8675751447677612
Epoch: 332, Batch Gradient Norm: 7.223846240087169
Epoch: 332, Batch Gradient Norm after: 7.223846240087169
Epoch 333/10000, Prediction Accuracy = 55.488%, Loss = 0.7985343217849732
Epoch: 333, Batch Gradient Norm: 14.891319773589169
Epoch: 333, Batch Gradient Norm after: 14.891319773589169
Epoch 334/10000, Prediction Accuracy = 55.528%, Loss = 0.8322293400764466
Epoch: 334, Batch Gradient Norm: 9.269079380396219
Epoch: 334, Batch Gradient Norm after: 9.269079380396219
Epoch 335/10000, Prediction Accuracy = 55.525999999999996%, Loss = 0.8070631742477417
Epoch: 335, Batch Gradient Norm: 16.399244404370616
Epoch: 335, Batch Gradient Norm after: 16.399244404370616
Epoch 336/10000, Prediction Accuracy = 55.536%, Loss = 0.8467522263526917
Epoch: 336, Batch Gradient Norm: 10.52041684895818
Epoch: 336, Batch Gradient Norm after: 10.52041684895818
Epoch 337/10000, Prediction Accuracy = 55.646%, Loss = 0.8122826814651489
Epoch: 337, Batch Gradient Norm: 13.286134040660317
Epoch: 337, Batch Gradient Norm after: 13.286134040660317
Epoch 338/10000, Prediction Accuracy = 55.61600000000001%, Loss = 0.8221085786819458
Epoch: 338, Batch Gradient Norm: 7.753765080950092
Epoch: 338, Batch Gradient Norm after: 7.753765080950092
Epoch 339/10000, Prediction Accuracy = 55.7%, Loss = 0.7959283351898193
Epoch: 339, Batch Gradient Norm: 15.955434330489185
Epoch: 339, Batch Gradient Norm after: 15.955434330489185
Epoch 340/10000, Prediction Accuracy = 55.69199999999999%, Loss = 0.8314662337303161
Epoch: 340, Batch Gradient Norm: 10.576212236946947
Epoch: 340, Batch Gradient Norm after: 10.576212236946947
Epoch 341/10000, Prediction Accuracy = 55.678%, Loss = 0.8062721252441406
Epoch: 341, Batch Gradient Norm: 16.304636030028348
Epoch: 341, Batch Gradient Norm after: 16.304636030028348
Epoch 342/10000, Prediction Accuracy = 55.66600000000001%, Loss = 0.8331112265586853
Epoch: 342, Batch Gradient Norm: 9.487906834890293
Epoch: 342, Batch Gradient Norm after: 9.487906834890293
Epoch 343/10000, Prediction Accuracy = 55.720000000000006%, Loss = 0.8003795862197876
Epoch: 343, Batch Gradient Norm: 14.029274410721406
Epoch: 343, Batch Gradient Norm after: 14.029274410721406
Epoch 344/10000, Prediction Accuracy = 55.69%, Loss = 0.8225021362304688
Epoch: 344, Batch Gradient Norm: 14.484454118891255
Epoch: 344, Batch Gradient Norm after: 14.484454118891255
Epoch 345/10000, Prediction Accuracy = 55.718%, Loss = 0.8305511355400086
Epoch: 345, Batch Gradient Norm: 10.646854669357342
Epoch: 345, Batch Gradient Norm after: 10.646854669357342
Epoch 346/10000, Prediction Accuracy = 55.769999999999996%, Loss = 0.806409728527069
Epoch: 346, Batch Gradient Norm: 10.62596497651482
Epoch: 346, Batch Gradient Norm after: 10.62596497651482
Epoch 347/10000, Prediction Accuracy = 55.802%, Loss = 0.7989924430847168
Epoch: 347, Batch Gradient Norm: 8.16203892190533
Epoch: 347, Batch Gradient Norm after: 8.16203892190533
Epoch 348/10000, Prediction Accuracy = 55.812%, Loss = 0.7900097489356994
Epoch: 348, Batch Gradient Norm: 19.558778168359215
Epoch: 348, Batch Gradient Norm after: 19.459690446273452
Epoch 349/10000, Prediction Accuracy = 55.778%, Loss = 0.8517392754554749
Epoch: 349, Batch Gradient Norm: 8.091028410102446
Epoch: 349, Batch Gradient Norm after: 8.091028410102446
Epoch 350/10000, Prediction Accuracy = 55.802%, Loss = 0.7908766865730286
Epoch: 350, Batch Gradient Norm: 14.132770299268143
Epoch: 350, Batch Gradient Norm after: 14.132770299268143
Epoch 351/10000, Prediction Accuracy = 55.88000000000001%, Loss = 0.8184655666351318
Epoch: 351, Batch Gradient Norm: 9.953805274446605
Epoch: 351, Batch Gradient Norm after: 9.953805274446605
Epoch 352/10000, Prediction Accuracy = 55.85%, Loss = 0.8003864645957947
Epoch: 352, Batch Gradient Norm: 12.855010968103043
Epoch: 352, Batch Gradient Norm after: 12.855010968103043
Epoch 353/10000, Prediction Accuracy = 55.936%, Loss = 0.8123448967933655
Epoch: 353, Batch Gradient Norm: 7.453580727786632
Epoch: 353, Batch Gradient Norm after: 7.453580727786632
Epoch 354/10000, Prediction Accuracy = 55.898%, Loss = 0.7850331425666809
Epoch: 354, Batch Gradient Norm: 12.221079659910663
Epoch: 354, Batch Gradient Norm after: 12.221079659910663
Epoch 355/10000, Prediction Accuracy = 55.970000000000006%, Loss = 0.8009831666946411
Epoch: 355, Batch Gradient Norm: 9.376303502734071
Epoch: 355, Batch Gradient Norm after: 9.376303502734071
Epoch 356/10000, Prediction Accuracy = 55.93399999999999%, Loss = 0.789948308467865
Epoch: 356, Batch Gradient Norm: 18.86033128375796
Epoch: 356, Batch Gradient Norm after: 18.86033128375796
Epoch 357/10000, Prediction Accuracy = 55.938%, Loss = 0.8426589965820312
Epoch: 357, Batch Gradient Norm: 9.57256613649286
Epoch: 357, Batch Gradient Norm after: 9.57256613649286
Epoch 358/10000, Prediction Accuracy = 56.011999999999986%, Loss = 0.7933649301528931
Epoch: 358, Batch Gradient Norm: 16.37162463845791
Epoch: 358, Batch Gradient Norm after: 16.37162463845791
Epoch 359/10000, Prediction Accuracy = 55.938%, Loss = 0.8290001273155212
Epoch: 359, Batch Gradient Norm: 11.37498791485029
Epoch: 359, Batch Gradient Norm after: 11.37498791485029
Epoch 360/10000, Prediction Accuracy = 55.976%, Loss = 0.8010500073432922
Epoch: 360, Batch Gradient Norm: 16.029758244592045
Epoch: 360, Batch Gradient Norm after: 16.029758244592045
Epoch 361/10000, Prediction Accuracy = 56.038%, Loss = 0.8272932887077331
Epoch: 361, Batch Gradient Norm: 9.343852092784736
Epoch: 361, Batch Gradient Norm after: 9.343852092784736
Epoch 362/10000, Prediction Accuracy = 56.02199999999999%, Loss = 0.7881775021553039
Epoch: 362, Batch Gradient Norm: 12.522925451284076
Epoch: 362, Batch Gradient Norm after: 12.522925451284076
Epoch 363/10000, Prediction Accuracy = 56.088%, Loss = 0.8030714750289917
Epoch: 363, Batch Gradient Norm: 8.678230008365668
Epoch: 363, Batch Gradient Norm after: 8.678230008365668
Epoch 364/10000, Prediction Accuracy = 56.065999999999995%, Loss = 0.784416663646698
Epoch: 364, Batch Gradient Norm: 9.615123873553541
Epoch: 364, Batch Gradient Norm after: 9.615123873553541
Epoch 365/10000, Prediction Accuracy = 56.146%, Loss = 0.7859366655349731
Epoch: 365, Batch Gradient Norm: 9.320956043256608
Epoch: 365, Batch Gradient Norm after: 9.320956043256608
Epoch 366/10000, Prediction Accuracy = 56.10600000000001%, Loss = 0.782663369178772
Epoch: 366, Batch Gradient Norm: 12.252902608466119
Epoch: 366, Batch Gradient Norm after: 12.252902608466119
Epoch 367/10000, Prediction Accuracy = 56.138%, Loss = 0.7953822016716003
Epoch: 367, Batch Gradient Norm: 19.972967786893346
Epoch: 367, Batch Gradient Norm after: 18.24268777788467
Epoch 368/10000, Prediction Accuracy = 56.034000000000006%, Loss = 0.8440006613731384
Epoch: 368, Batch Gradient Norm: 8.239025034494148
Epoch: 368, Batch Gradient Norm after: 8.239025034494148
Epoch 369/10000, Prediction Accuracy = 56.160000000000004%, Loss = 0.7792673945426941
Epoch: 369, Batch Gradient Norm: 15.377824489403467
Epoch: 369, Batch Gradient Norm after: 15.377824489403467
Epoch 370/10000, Prediction Accuracy = 56.138%, Loss = 0.8110292792320252
Epoch: 370, Batch Gradient Norm: 10.981535578531608
Epoch: 370, Batch Gradient Norm after: 10.981535578531608
Epoch 371/10000, Prediction Accuracy = 56.184000000000005%, Loss = 0.7958714246749878
Epoch: 371, Batch Gradient Norm: 14.492170043498637
Epoch: 371, Batch Gradient Norm after: 14.492170043498637
Epoch 372/10000, Prediction Accuracy = 56.238%, Loss = 0.8086840748786926
Epoch: 372, Batch Gradient Norm: 8.950070035122813
Epoch: 372, Batch Gradient Norm after: 8.950070035122813
Epoch 373/10000, Prediction Accuracy = 56.24400000000001%, Loss = 0.7813423871994019
Epoch: 373, Batch Gradient Norm: 14.079228200254061
Epoch: 373, Batch Gradient Norm after: 14.079228200254061
Epoch 374/10000, Prediction Accuracy = 56.222%, Loss = 0.8028757095336914
Epoch: 374, Batch Gradient Norm: 9.49328509095479
Epoch: 374, Batch Gradient Norm after: 9.49328509095479
Epoch 375/10000, Prediction Accuracy = 56.27%, Loss = 0.7799880981445313
Epoch: 375, Batch Gradient Norm: 17.24039646452942
Epoch: 375, Batch Gradient Norm after: 17.24039646452942
Epoch 376/10000, Prediction Accuracy = 56.208000000000006%, Loss = 0.8219000458717346
Epoch: 376, Batch Gradient Norm: 10.595443971830404
Epoch: 376, Batch Gradient Norm after: 10.595443971830404
Epoch 377/10000, Prediction Accuracy = 56.290000000000006%, Loss = 0.7836272954940796
Epoch: 377, Batch Gradient Norm: 18.37772178088681
Epoch: 377, Batch Gradient Norm after: 18.37772178088681
Epoch 378/10000, Prediction Accuracy = 56.2%, Loss = 0.8296346426010132
Epoch: 378, Batch Gradient Norm: 8.858451109633199
Epoch: 378, Batch Gradient Norm after: 8.858451109633199
Epoch 379/10000, Prediction Accuracy = 56.29200000000001%, Loss = 0.7768800497055054
Epoch: 379, Batch Gradient Norm: 13.53169619838894
Epoch: 379, Batch Gradient Norm after: 13.53169619838894
Epoch 380/10000, Prediction Accuracy = 56.284000000000006%, Loss = 0.795491099357605
Epoch: 380, Batch Gradient Norm: 10.888931152282444
Epoch: 380, Batch Gradient Norm after: 10.888931152282444
Epoch 381/10000, Prediction Accuracy = 56.394000000000005%, Loss = 0.7855952501296997
Epoch: 381, Batch Gradient Norm: 13.867486321324526
Epoch: 381, Batch Gradient Norm after: 13.867486321324526
Epoch 382/10000, Prediction Accuracy = 56.31999999999999%, Loss = 0.7991860866546631
Epoch: 382, Batch Gradient Norm: 10.728043726249668
Epoch: 382, Batch Gradient Norm after: 10.728043726249668
Epoch 383/10000, Prediction Accuracy = 56.396%, Loss = 0.7836912274360657
Epoch: 383, Batch Gradient Norm: 12.459719175538373
Epoch: 383, Batch Gradient Norm after: 12.459719175538373
Epoch 384/10000, Prediction Accuracy = 56.342000000000006%, Loss = 0.7861400008201599
Epoch: 384, Batch Gradient Norm: 11.033903694031158
Epoch: 384, Batch Gradient Norm after: 11.033903694031158
Epoch 385/10000, Prediction Accuracy = 56.396%, Loss = 0.781684398651123
Epoch: 385, Batch Gradient Norm: 19.121280716555013
Epoch: 385, Batch Gradient Norm after: 19.121280716555013
Epoch 386/10000, Prediction Accuracy = 56.44199999999999%, Loss = 0.8338443160057067
Epoch: 386, Batch Gradient Norm: 11.11334399850894
Epoch: 386, Batch Gradient Norm after: 11.11334399850894
Epoch 387/10000, Prediction Accuracy = 56.424%, Loss = 0.7857967734336853
Epoch: 387, Batch Gradient Norm: 12.66018908915336
Epoch: 387, Batch Gradient Norm after: 12.66018908915336
Epoch 388/10000, Prediction Accuracy = 56.492000000000004%, Loss = 0.7873930335044861
Epoch: 388, Batch Gradient Norm: 6.889695186914582
Epoch: 388, Batch Gradient Norm after: 6.889695186914582
Epoch 389/10000, Prediction Accuracy = 56.48199999999999%, Loss = 0.7626567363739014
Epoch: 389, Batch Gradient Norm: 12.558413853862048
Epoch: 389, Batch Gradient Norm after: 12.558413853862048
Epoch 390/10000, Prediction Accuracy = 56.492%, Loss = 0.7832234740257263
Epoch: 390, Batch Gradient Norm: 8.904535505747845
Epoch: 390, Batch Gradient Norm after: 8.904535505747845
Epoch 391/10000, Prediction Accuracy = 56.470000000000006%, Loss = 0.7696298956871033
Epoch: 391, Batch Gradient Norm: 17.182583169996256
Epoch: 391, Batch Gradient Norm after: 17.182583169996256
Epoch 392/10000, Prediction Accuracy = 56.50599999999999%, Loss = 0.8134072780609131
Epoch: 392, Batch Gradient Norm: 9.013455224565392
Epoch: 392, Batch Gradient Norm after: 9.013455224565392
Epoch 393/10000, Prediction Accuracy = 56.55799999999999%, Loss = 0.769804310798645
Epoch: 393, Batch Gradient Norm: 16.09405454536668
Epoch: 393, Batch Gradient Norm after: 16.09405454536668
Epoch 394/10000, Prediction Accuracy = 56.51800000000001%, Loss = 0.8048897624015808
Epoch: 394, Batch Gradient Norm: 10.162959309196275
Epoch: 394, Batch Gradient Norm after: 10.162959309196275
Epoch 395/10000, Prediction Accuracy = 56.60799999999999%, Loss = 0.7730615854263305
Epoch: 395, Batch Gradient Norm: 16.245257356967635
Epoch: 395, Batch Gradient Norm after: 16.245257356967635
Epoch 396/10000, Prediction Accuracy = 56.552%, Loss = 0.8063103556632996
Epoch: 396, Batch Gradient Norm: 8.76966425642947
Epoch: 396, Batch Gradient Norm after: 8.76966425642947
Epoch 397/10000, Prediction Accuracy = 56.58%, Loss = 0.7655846834182739
Epoch: 397, Batch Gradient Norm: 15.051329574596089
Epoch: 397, Batch Gradient Norm after: 15.051329574596089
Epoch 398/10000, Prediction Accuracy = 56.616%, Loss = 0.7976766586303711
Epoch: 398, Batch Gradient Norm: 10.949907102169213
Epoch: 398, Batch Gradient Norm after: 10.949907102169213
Epoch 399/10000, Prediction Accuracy = 56.544%, Loss = 0.7798465728759766
Epoch: 399, Batch Gradient Norm: 13.218311063342309
Epoch: 399, Batch Gradient Norm after: 13.218311063342309
Epoch 400/10000, Prediction Accuracy = 56.626%, Loss = 0.7848930597305298
Epoch: 400, Batch Gradient Norm: 7.313830565114005
Epoch: 400, Batch Gradient Norm after: 7.313830565114005
Epoch 401/10000, Prediction Accuracy = 56.58200000000001%, Loss = 0.7586150884628295
Epoch: 401, Batch Gradient Norm: 14.51987993157778
Epoch: 401, Batch Gradient Norm after: 14.51987993157778
Epoch 402/10000, Prediction Accuracy = 56.616%, Loss = 0.7851173639297485
Epoch: 402, Batch Gradient Norm: 12.610722278583452
Epoch: 402, Batch Gradient Norm after: 12.610722278583452
Epoch 403/10000, Prediction Accuracy = 56.553999999999995%, Loss = 0.7818825125694275
Epoch: 403, Batch Gradient Norm: 18.839788650979056
Epoch: 403, Batch Gradient Norm after: 18.839788650979056
Epoch 404/10000, Prediction Accuracy = 56.666%, Loss = 0.8254279732704163
Epoch: 404, Batch Gradient Norm: 11.270023461025358
Epoch: 404, Batch Gradient Norm after: 11.270023461025358
Epoch 405/10000, Prediction Accuracy = 56.66799999999999%, Loss = 0.7776838898658752
Epoch: 405, Batch Gradient Norm: 9.740404433553465
Epoch: 405, Batch Gradient Norm after: 9.740404433553465
Epoch 406/10000, Prediction Accuracy = 56.672000000000004%, Loss = 0.7627711296081543
Epoch: 406, Batch Gradient Norm: 6.6794156005207
Epoch: 406, Batch Gradient Norm after: 6.6794156005207
Epoch 407/10000, Prediction Accuracy = 56.684000000000005%, Loss = 0.7518206596374511
Epoch: 407, Batch Gradient Norm: 10.574372242160239
Epoch: 407, Batch Gradient Norm after: 10.574372242160239
Epoch 408/10000, Prediction Accuracy = 56.682%, Loss = 0.7630867958068848
Epoch: 408, Batch Gradient Norm: 10.536662812341866
Epoch: 408, Batch Gradient Norm after: 10.536662812341866
Epoch 409/10000, Prediction Accuracy = 56.676%, Loss = 0.7656504034996032
Epoch: 409, Batch Gradient Norm: 19.917971642705922
Epoch: 409, Batch Gradient Norm after: 19.917971642705922
Epoch 410/10000, Prediction Accuracy = 56.726%, Loss = 0.8302326321601867
Epoch: 410, Batch Gradient Norm: 13.192390389205535
Epoch: 410, Batch Gradient Norm after: 13.192390389205535
Epoch 411/10000, Prediction Accuracy = 56.754%, Loss = 0.7859594702720643
Epoch: 411, Batch Gradient Norm: 11.794425981480746
Epoch: 411, Batch Gradient Norm after: 11.794425981480746
Epoch 412/10000, Prediction Accuracy = 56.8%, Loss = 0.7724056720733643
Epoch: 412, Batch Gradient Norm: 9.080181094024436
Epoch: 412, Batch Gradient Norm after: 9.080181094024436
Epoch 413/10000, Prediction Accuracy = 56.80800000000001%, Loss = 0.7586243271827697
Epoch: 413, Batch Gradient Norm: 8.078449379406146
Epoch: 413, Batch Gradient Norm after: 8.078449379406146
Epoch 414/10000, Prediction Accuracy = 56.83%, Loss = 0.7531774759292602
Epoch: 414, Batch Gradient Norm: 12.71551273610482
Epoch: 414, Batch Gradient Norm after: 12.71551273610482
Epoch 415/10000, Prediction Accuracy = 56.75600000000001%, Loss = 0.7766383528709412
Epoch: 415, Batch Gradient Norm: 13.372963854731085
Epoch: 415, Batch Gradient Norm after: 13.372963854731085
Epoch 416/10000, Prediction Accuracy = 56.836%, Loss = 0.7839985251426697
Epoch: 416, Batch Gradient Norm: 20.84794806973555
Epoch: 416, Batch Gradient Norm after: 20.54919783293382
Epoch 417/10000, Prediction Accuracy = 56.870000000000005%, Loss = 0.8317490100860596
Epoch: 417, Batch Gradient Norm: 8.304104781010185
Epoch: 417, Batch Gradient Norm after: 8.304104781010185
Epoch 418/10000, Prediction Accuracy = 56.848%, Loss = 0.7536530733108521
Epoch: 418, Batch Gradient Norm: 14.279424829867292
Epoch: 418, Batch Gradient Norm after: 14.279424829867292
Epoch 419/10000, Prediction Accuracy = 56.874%, Loss = 0.7788512349128723
Epoch: 419, Batch Gradient Norm: 7.681289424314078
Epoch: 419, Batch Gradient Norm after: 7.681289424314078
Epoch 420/10000, Prediction Accuracy = 56.91799999999999%, Loss = 0.7499813079833985
Epoch: 420, Batch Gradient Norm: 14.718878360962917
Epoch: 420, Batch Gradient Norm after: 14.718878360962917
Epoch 421/10000, Prediction Accuracy = 56.93399999999999%, Loss = 0.783764123916626
Epoch: 421, Batch Gradient Norm: 9.791383367547798
Epoch: 421, Batch Gradient Norm after: 9.791383367547798
Epoch 422/10000, Prediction Accuracy = 56.926%, Loss = 0.758506178855896
Epoch: 422, Batch Gradient Norm: 12.855271947374328
Epoch: 422, Batch Gradient Norm after: 12.855271947374328
Epoch 423/10000, Prediction Accuracy = 56.958000000000006%, Loss = 0.7730244398117065
Epoch: 423, Batch Gradient Norm: 8.694542683969436
Epoch: 423, Batch Gradient Norm after: 8.694542683969436
Epoch 424/10000, Prediction Accuracy = 56.94200000000001%, Loss = 0.7510903120040894
Epoch: 424, Batch Gradient Norm: 11.175557625322183
Epoch: 424, Batch Gradient Norm after: 11.175557625322183
Epoch 425/10000, Prediction Accuracy = 56.928%, Loss = 0.7642860770225525
Epoch: 425, Batch Gradient Norm: 19.61483756154127
Epoch: 425, Batch Gradient Norm after: 19.61483756154127
Epoch 426/10000, Prediction Accuracy = 56.976%, Loss = 0.8151594400405884
Epoch: 426, Batch Gradient Norm: 11.665354147149376
Epoch: 426, Batch Gradient Norm after: 11.665354147149376
Epoch 427/10000, Prediction Accuracy = 56.86999999999999%, Loss = 0.7669460773468018
Epoch: 427, Batch Gradient Norm: 13.453995298516084
Epoch: 427, Batch Gradient Norm after: 13.453995298516084
Epoch 428/10000, Prediction Accuracy = 56.968%, Loss = 0.7700177907943726
Epoch: 428, Batch Gradient Norm: 6.002941521315561
Epoch: 428, Batch Gradient Norm after: 6.002941521315561
Epoch 429/10000, Prediction Accuracy = 56.903999999999996%, Loss = 0.7402678966522217
Epoch: 429, Batch Gradient Norm: 12.845762780668094
Epoch: 429, Batch Gradient Norm after: 12.845762780668094
Epoch 430/10000, Prediction Accuracy = 56.952%, Loss = 0.766146433353424
Epoch: 430, Batch Gradient Norm: 12.916525597425961
Epoch: 430, Batch Gradient Norm after: 12.916525597425961
Epoch 431/10000, Prediction Accuracy = 57.010000000000005%, Loss = 0.7743222832679748
Epoch: 431, Batch Gradient Norm: 17.24887610179367
Epoch: 431, Batch Gradient Norm after: 17.24887610179367
Epoch 432/10000, Prediction Accuracy = 57.034000000000006%, Loss = 0.8023689746856689
Epoch: 432, Batch Gradient Norm: 7.583882985724983
Epoch: 432, Batch Gradient Norm after: 7.583882985724983
Epoch 433/10000, Prediction Accuracy = 57.029999999999994%, Loss = 0.7447715640068054
Epoch: 433, Batch Gradient Norm: 13.705373787052931
Epoch: 433, Batch Gradient Norm after: 13.705373787052931
Epoch 434/10000, Prediction Accuracy = 57.02600000000001%, Loss = 0.7680375456809998
Epoch: 434, Batch Gradient Norm: 10.996280217374466
Epoch: 434, Batch Gradient Norm after: 10.996280217374466
Epoch 435/10000, Prediction Accuracy = 57.00600000000001%, Loss = 0.7565903782844543
Epoch: 435, Batch Gradient Norm: 18.870367450081336
Epoch: 435, Batch Gradient Norm after: 18.870367450081336
Epoch 436/10000, Prediction Accuracy = 57.028%, Loss = 0.8081711530685425
Epoch: 436, Batch Gradient Norm: 10.312514891947727
Epoch: 436, Batch Gradient Norm after: 10.312514891947727
Epoch 437/10000, Prediction Accuracy = 57.02%, Loss = 0.7574728727340698
Epoch: 437, Batch Gradient Norm: 13.3108776206348
Epoch: 437, Batch Gradient Norm after: 13.3108776206348
Epoch 438/10000, Prediction Accuracy = 57.06%, Loss = 0.7718542575836181
Epoch: 438, Batch Gradient Norm: 8.955448365361814
Epoch: 438, Batch Gradient Norm after: 8.955448365361814
Epoch 439/10000, Prediction Accuracy = 57.022000000000006%, Loss = 0.7483122110366821
Epoch: 439, Batch Gradient Norm: 10.89568196817569
Epoch: 439, Batch Gradient Norm after: 10.89568196817569
Epoch 440/10000, Prediction Accuracy = 57.089999999999996%, Loss = 0.7540821552276611
Epoch: 440, Batch Gradient Norm: 10.080779010949717
Epoch: 440, Batch Gradient Norm after: 10.080779010949717
Epoch 441/10000, Prediction Accuracy = 57.053999999999995%, Loss = 0.750145161151886
Epoch: 441, Batch Gradient Norm: 16.442416483563317
Epoch: 441, Batch Gradient Norm after: 16.442416483563317
Epoch 442/10000, Prediction Accuracy = 57.081999999999994%, Loss = 0.785066294670105
Epoch: 442, Batch Gradient Norm: 14.435691030442344
Epoch: 442, Batch Gradient Norm after: 14.435691030442344
Epoch 443/10000, Prediction Accuracy = 57.028%, Loss = 0.7770875930786133
Epoch: 443, Batch Gradient Norm: 14.61143023594372
Epoch: 443, Batch Gradient Norm after: 14.61143023594372
Epoch 444/10000, Prediction Accuracy = 57.141999999999996%, Loss = 0.7706794142723083
Epoch: 444, Batch Gradient Norm: 9.631997343686294
Epoch: 444, Batch Gradient Norm after: 9.631997343686294
Epoch 445/10000, Prediction Accuracy = 57.062%, Loss = 0.7476494431495666
Epoch: 445, Batch Gradient Norm: 13.415433087553936
Epoch: 445, Batch Gradient Norm after: 13.415433087553936
Epoch 446/10000, Prediction Accuracy = 57.114%, Loss = 0.7618433475494385
Epoch: 446, Batch Gradient Norm: 9.993597629272191
Epoch: 446, Batch Gradient Norm after: 9.993597629272191
Epoch 447/10000, Prediction Accuracy = 57.08%, Loss = 0.7490972757339478
Epoch: 447, Batch Gradient Norm: 15.586551459209831
Epoch: 447, Batch Gradient Norm after: 15.586551459209831
Epoch 448/10000, Prediction Accuracy = 57.188%, Loss = 0.778576397895813
Epoch: 448, Batch Gradient Norm: 8.833158478581677
Epoch: 448, Batch Gradient Norm after: 8.833158478581677
Epoch 449/10000, Prediction Accuracy = 57.124%, Loss = 0.7430259943008423
Epoch: 449, Batch Gradient Norm: 13.987869784290353
Epoch: 449, Batch Gradient Norm after: 13.987869784290353
Epoch 450/10000, Prediction Accuracy = 57.214%, Loss = 0.766071903705597
Epoch: 450, Batch Gradient Norm: 8.613328925917275
Epoch: 450, Batch Gradient Norm after: 8.613328925917275
Epoch 451/10000, Prediction Accuracy = 57.20799999999999%, Loss = 0.7385295391082763
Epoch: 451, Batch Gradient Norm: 18.056575714060628
Epoch: 451, Batch Gradient Norm after: 18.056575714060628
Epoch 452/10000, Prediction Accuracy = 57.2%, Loss = 0.7937798023223877
Epoch: 452, Batch Gradient Norm: 11.354524219491218
Epoch: 452, Batch Gradient Norm after: 11.354524219491218
Epoch 453/10000, Prediction Accuracy = 57.20799999999999%, Loss = 0.7533432245254517
Epoch: 453, Batch Gradient Norm: 15.29450070531253
Epoch: 453, Batch Gradient Norm after: 15.29450070531253
Epoch 454/10000, Prediction Accuracy = 57.194%, Loss = 0.7734295129776001
Epoch: 454, Batch Gradient Norm: 7.84258418476289
Epoch: 454, Batch Gradient Norm after: 7.84258418476289
Epoch 455/10000, Prediction Accuracy = 57.168000000000006%, Loss = 0.7365335106849671
Epoch: 455, Batch Gradient Norm: 14.007978778306956
Epoch: 455, Batch Gradient Norm after: 14.007978778306956
Epoch 456/10000, Prediction Accuracy = 57.17999999999999%, Loss = 0.762402331829071
Epoch: 456, Batch Gradient Norm: 11.183202549513705
Epoch: 456, Batch Gradient Norm after: 11.183202549513705
Epoch 457/10000, Prediction Accuracy = 57.238%, Loss = 0.7510623693466186
Epoch: 457, Batch Gradient Norm: 16.429131844257203
Epoch: 457, Batch Gradient Norm after: 16.429131844257203
Epoch 458/10000, Prediction Accuracy = 57.214%, Loss = 0.7820150971412658
Epoch: 458, Batch Gradient Norm: 11.042603262778044
Epoch: 458, Batch Gradient Norm after: 11.042603262778044
Epoch 459/10000, Prediction Accuracy = 57.263999999999996%, Loss = 0.7504035472869873
Epoch: 459, Batch Gradient Norm: 12.511285912661846
Epoch: 459, Batch Gradient Norm after: 12.511285912661846
Epoch 460/10000, Prediction Accuracy = 57.232000000000006%, Loss = 0.7541547536849975
Epoch: 460, Batch Gradient Norm: 9.89127848545453
Epoch: 460, Batch Gradient Norm after: 9.89127848545453
Epoch 461/10000, Prediction Accuracy = 57.248000000000005%, Loss = 0.7417081713676452
Epoch: 461, Batch Gradient Norm: 16.150652455345497
Epoch: 461, Batch Gradient Norm after: 16.150652455345497
Epoch 462/10000, Prediction Accuracy = 57.260000000000005%, Loss = 0.7769172072410584
Epoch: 462, Batch Gradient Norm: 12.918137489260015
Epoch: 462, Batch Gradient Norm after: 12.918137489260015
Epoch 463/10000, Prediction Accuracy = 57.258%, Loss = 0.7613873958587647
Epoch: 463, Batch Gradient Norm: 13.258513187538906
Epoch: 463, Batch Gradient Norm after: 13.258513187538906
Epoch 464/10000, Prediction Accuracy = 57.28799999999999%, Loss = 0.7561868309974671
Epoch: 464, Batch Gradient Norm: 7.927788040674584
Epoch: 464, Batch Gradient Norm after: 7.927788040674584
Epoch 465/10000, Prediction Accuracy = 57.262%, Loss = 0.7327305316925049
Epoch: 465, Batch Gradient Norm: 12.957980206712307
Epoch: 465, Batch Gradient Norm after: 12.957980206712307
Epoch 466/10000, Prediction Accuracy = 57.324%, Loss = 0.7508568882942199
Epoch: 466, Batch Gradient Norm: 9.84119408515942
Epoch: 466, Batch Gradient Norm after: 9.84119408515942
Epoch 467/10000, Prediction Accuracy = 57.3%, Loss = 0.7391909480094909
Epoch: 467, Batch Gradient Norm: 18.073008749094985
Epoch: 467, Batch Gradient Norm after: 18.073008749094985
Epoch 468/10000, Prediction Accuracy = 57.35799999999999%, Loss = 0.7883272886276245
Epoch: 468, Batch Gradient Norm: 10.548248022850279
Epoch: 468, Batch Gradient Norm after: 10.548248022850279
Epoch 469/10000, Prediction Accuracy = 57.34000000000001%, Loss = 0.7419400215148926
Epoch: 469, Batch Gradient Norm: 13.726818352408051
Epoch: 469, Batch Gradient Norm after: 13.726818352408051
Epoch 470/10000, Prediction Accuracy = 57.339999999999996%, Loss = 0.757184636592865
Epoch: 470, Batch Gradient Norm: 6.383337866046341
Epoch: 470, Batch Gradient Norm after: 6.383337866046341
Epoch 471/10000, Prediction Accuracy = 57.32000000000001%, Loss = 0.7229887127876282
Epoch: 471, Batch Gradient Norm: 14.635857334724239
Epoch: 471, Batch Gradient Norm after: 14.635857334724239
Epoch 472/10000, Prediction Accuracy = 57.35600000000001%, Loss = 0.7607123613357544
Epoch: 472, Batch Gradient Norm: 11.76861675181223
Epoch: 472, Batch Gradient Norm after: 11.76861675181223
Epoch 473/10000, Prediction Accuracy = 57.352%, Loss = 0.7472995042800903
Epoch: 473, Batch Gradient Norm: 17.72034944684459
Epoch: 473, Batch Gradient Norm after: 17.72034944684459
Epoch 474/10000, Prediction Accuracy = 57.410000000000004%, Loss = 0.787031638622284
Epoch: 474, Batch Gradient Norm: 9.671647865744411
Epoch: 474, Batch Gradient Norm after: 9.671647865744411
Epoch 475/10000, Prediction Accuracy = 57.398%, Loss = 0.7383519291877747
Epoch: 475, Batch Gradient Norm: 11.304688268619765
Epoch: 475, Batch Gradient Norm after: 11.304688268619765
Epoch 476/10000, Prediction Accuracy = 57.444%, Loss = 0.7428947687149048
Epoch: 476, Batch Gradient Norm: 11.284926655342515
Epoch: 476, Batch Gradient Norm after: 11.284926655342515
Epoch 477/10000, Prediction Accuracy = 57.4%, Loss = 0.7455683708190918
Epoch: 477, Batch Gradient Norm: 12.511712825363595
Epoch: 477, Batch Gradient Norm after: 12.511712825363595
Epoch 478/10000, Prediction Accuracy = 57.431999999999995%, Loss = 0.7512051820755005
Epoch: 478, Batch Gradient Norm: 12.6141555289177
Epoch: 478, Batch Gradient Norm after: 12.6141555289177
Epoch 479/10000, Prediction Accuracy = 57.407999999999994%, Loss = 0.7478267550468445
Epoch: 479, Batch Gradient Norm: 13.395837180527902
Epoch: 479, Batch Gradient Norm after: 13.395837180527902
Epoch 480/10000, Prediction Accuracy = 57.394000000000005%, Loss = 0.7510123252868652
Epoch: 480, Batch Gradient Norm: 18.90170328818109
Epoch: 480, Batch Gradient Norm after: 18.821148048735985
Epoch 481/10000, Prediction Accuracy = 57.35%, Loss = 0.7874602556228638
Epoch: 481, Batch Gradient Norm: 10.783446705222973
Epoch: 481, Batch Gradient Norm after: 10.783446705222973
Epoch 482/10000, Prediction Accuracy = 57.418000000000006%, Loss = 0.7389594316482544
Epoch: 482, Batch Gradient Norm: 11.411184017524501
Epoch: 482, Batch Gradient Norm after: 11.411184017524501
Epoch 483/10000, Prediction Accuracy = 57.464%, Loss = 0.739092481136322
Epoch: 483, Batch Gradient Norm: 10.67607747915598
Epoch: 483, Batch Gradient Norm after: 10.67607747915598
Epoch 484/10000, Prediction Accuracy = 57.455999999999996%, Loss = 0.7376654148101807
Epoch: 484, Batch Gradient Norm: 10.36197576608643
Epoch: 484, Batch Gradient Norm after: 10.36197576608643
Epoch 485/10000, Prediction Accuracy = 57.48%, Loss = 0.7349431395530701
Epoch: 485, Batch Gradient Norm: 10.49681060221463
Epoch: 485, Batch Gradient Norm after: 10.49681060221463
Epoch 486/10000, Prediction Accuracy = 57.456%, Loss = 0.7353091478347779
Epoch: 486, Batch Gradient Norm: 8.691840027348123
Epoch: 486, Batch Gradient Norm after: 8.691840027348123
Epoch 487/10000, Prediction Accuracy = 57.484%, Loss = 0.7271476984024048
Epoch: 487, Batch Gradient Norm: 13.798583130093615
Epoch: 487, Batch Gradient Norm after: 13.798583130093615
Epoch 488/10000, Prediction Accuracy = 57.498000000000005%, Loss = 0.7503972291946411
Epoch: 488, Batch Gradient Norm: 16.41413912224772
Epoch: 488, Batch Gradient Norm after: 16.41413912224772
Epoch 489/10000, Prediction Accuracy = 57.422000000000004%, Loss = 0.7693017363548279
Epoch: 489, Batch Gradient Norm: 18.517716162074212
Epoch: 489, Batch Gradient Norm after: 18.191612417813428
Epoch 490/10000, Prediction Accuracy = 57.443999999999996%, Loss = 0.7855954051017762
Epoch: 490, Batch Gradient Norm: 5.996791524016826
Epoch: 490, Batch Gradient Norm after: 5.996791524016826
Epoch 491/10000, Prediction Accuracy = 57.519999999999996%, Loss = 0.7162896156311035
Epoch: 491, Batch Gradient Norm: 9.345977548597062
Epoch: 491, Batch Gradient Norm after: 9.345977548597062
Epoch 492/10000, Prediction Accuracy = 57.522000000000006%, Loss = 0.7258502840995789
Epoch: 492, Batch Gradient Norm: 8.420254969616009
Epoch: 492, Batch Gradient Norm after: 8.420254969616009
Epoch 493/10000, Prediction Accuracy = 57.528000000000006%, Loss = 0.7239207744598388
Epoch: 493, Batch Gradient Norm: 11.468841076822352
Epoch: 493, Batch Gradient Norm after: 11.468841076822352
Epoch 494/10000, Prediction Accuracy = 57.510000000000005%, Loss = 0.738674259185791
Epoch: 494, Batch Gradient Norm: 11.980612179630668
Epoch: 494, Batch Gradient Norm after: 11.980612179630668
Epoch 495/10000, Prediction Accuracy = 57.564%, Loss = 0.7434577465057373
Epoch: 495, Batch Gradient Norm: 10.032154458546339
Epoch: 495, Batch Gradient Norm after: 10.032154458546339
Epoch 496/10000, Prediction Accuracy = 57.510000000000005%, Loss = 0.730875837802887
Epoch: 496, Batch Gradient Norm: 13.455429887654446
Epoch: 496, Batch Gradient Norm after: 13.455429887654446
Epoch 497/10000, Prediction Accuracy = 57.556000000000004%, Loss = 0.7474005818367004
Epoch: 497, Batch Gradient Norm: 13.89199125706355
Epoch: 497, Batch Gradient Norm after: 13.89199125706355
Epoch 498/10000, Prediction Accuracy = 57.464%, Loss = 0.7491737484931946
Epoch: 498, Batch Gradient Norm: 19.775744726896853
Epoch: 498, Batch Gradient Norm after: 19.113705346993587
Epoch 499/10000, Prediction Accuracy = 57.492000000000004%, Loss = 0.7907157421112061
Epoch: 499, Batch Gradient Norm: 8.495941731104905
Epoch: 499, Batch Gradient Norm after: 8.495941731104905
Epoch 500/10000, Prediction Accuracy = 57.556%, Loss = 0.7199224352836608
Epoch: 500, Batch Gradient Norm: 15.400130489452625
Epoch: 500, Batch Gradient Norm after: 15.400130489452625
Epoch 501/10000, Prediction Accuracy = 57.487999999999985%, Loss = 0.7549900412559509
Epoch: 501, Batch Gradient Norm: 12.46485605224185
Epoch: 501, Batch Gradient Norm after: 12.46485605224185
Epoch 502/10000, Prediction Accuracy = 57.580000000000005%, Loss = 0.743634021282196
Epoch: 502, Batch Gradient Norm: 17.43622667063113
Epoch: 502, Batch Gradient Norm after: 17.43622667063113
Epoch 503/10000, Prediction Accuracy = 57.517999999999994%, Loss = 0.7774638056755065
Epoch: 503, Batch Gradient Norm: 8.43569884894594
Epoch: 503, Batch Gradient Norm after: 8.43569884894594
Epoch 504/10000, Prediction Accuracy = 57.61800000000001%, Loss = 0.7199751138687134
Epoch: 504, Batch Gradient Norm: 10.899096573081852
Epoch: 504, Batch Gradient Norm after: 10.899096573081852
Epoch 505/10000, Prediction Accuracy = 57.574%, Loss = 0.7271901607513428
Epoch: 505, Batch Gradient Norm: 9.536817377650332
Epoch: 505, Batch Gradient Norm after: 9.536817377650332
Epoch 506/10000, Prediction Accuracy = 57.62800000000001%, Loss = 0.7215340614318848
Epoch: 506, Batch Gradient Norm: 16.62416335297642
Epoch: 506, Batch Gradient Norm after: 16.62416335297642
Epoch 507/10000, Prediction Accuracy = 57.54200000000001%, Loss = 0.7624398708343506
Epoch: 507, Batch Gradient Norm: 15.682775770539095
Epoch: 507, Batch Gradient Norm after: 15.682775770539095
Epoch 508/10000, Prediction Accuracy = 57.622%, Loss = 0.7615655422210693
Epoch: 508, Batch Gradient Norm: 11.98459976706211
Epoch: 508, Batch Gradient Norm after: 11.98459976706211
Epoch 509/10000, Prediction Accuracy = 57.604%, Loss = 0.7330461859703064
Epoch: 509, Batch Gradient Norm: 9.911695926506185
Epoch: 509, Batch Gradient Norm after: 9.911695926506185
Epoch 510/10000, Prediction Accuracy = 57.6%, Loss = 0.72473646402359
Epoch: 510, Batch Gradient Norm: 11.052655132603668
Epoch: 510, Batch Gradient Norm after: 11.052655132603668
Epoch 511/10000, Prediction Accuracy = 57.653999999999996%, Loss = 0.7285897612571717
Epoch: 511, Batch Gradient Norm: 9.726145983916265
Epoch: 511, Batch Gradient Norm after: 9.726145983916265
Epoch 512/10000, Prediction Accuracy = 57.632000000000005%, Loss = 0.723090398311615
Epoch: 512, Batch Gradient Norm: 13.519176122971663
Epoch: 512, Batch Gradient Norm after: 13.519176122971663
Epoch 513/10000, Prediction Accuracy = 57.632000000000005%, Loss = 0.7383784055709839
Epoch: 513, Batch Gradient Norm: 11.033562414004251
Epoch: 513, Batch Gradient Norm after: 11.033562414004251
Epoch 514/10000, Prediction Accuracy = 57.634%, Loss = 0.7256045341491699
Epoch: 514, Batch Gradient Norm: 17.274103845293663
Epoch: 514, Batch Gradient Norm after: 17.274103845293663
Epoch 515/10000, Prediction Accuracy = 57.612%, Loss = 0.7646456956863403
Epoch: 515, Batch Gradient Norm: 11.286865105163928
Epoch: 515, Batch Gradient Norm after: 11.286865105163928
Epoch 516/10000, Prediction Accuracy = 57.67999999999999%, Loss = 0.7332264184951782
Epoch: 516, Batch Gradient Norm: 13.95468215773896
Epoch: 516, Batch Gradient Norm after: 13.95468215773896
Epoch 517/10000, Prediction Accuracy = 57.688%, Loss = 0.745569384098053
Epoch: 517, Batch Gradient Norm: 7.719501032431768
Epoch: 517, Batch Gradient Norm after: 7.719501032431768
Epoch 518/10000, Prediction Accuracy = 57.73599999999999%, Loss = 0.712888765335083
Epoch: 518, Batch Gradient Norm: 10.606892664121899
Epoch: 518, Batch Gradient Norm after: 10.606892664121899
Epoch 519/10000, Prediction Accuracy = 57.678%, Loss = 0.7214582324028015
Epoch: 519, Batch Gradient Norm: 7.16201431711695
Epoch: 519, Batch Gradient Norm after: 7.16201431711695
Epoch 520/10000, Prediction Accuracy = 57.727999999999994%, Loss = 0.7072721958160401
Epoch: 520, Batch Gradient Norm: 17.550239109477214
Epoch: 520, Batch Gradient Norm after: 17.550239109477214
Epoch 521/10000, Prediction Accuracy = 57.64200000000001%, Loss = 0.764786159992218
Epoch: 521, Batch Gradient Norm: 13.305556962982592
Epoch: 521, Batch Gradient Norm after: 13.305556962982592
Epoch 522/10000, Prediction Accuracy = 57.712%, Loss = 0.7426782965660095
Epoch: 522, Batch Gradient Norm: 16.429978791897383
Epoch: 522, Batch Gradient Norm after: 16.429978791897383
Epoch 523/10000, Prediction Accuracy = 57.696000000000005%, Loss = 0.7623232483863831
Epoch: 523, Batch Gradient Norm: 9.633319253003767
Epoch: 523, Batch Gradient Norm after: 9.633319253003767
Epoch 524/10000, Prediction Accuracy = 57.734%, Loss = 0.7193131804466247
Epoch: 524, Batch Gradient Norm: 17.03199406793757
Epoch: 524, Batch Gradient Norm after: 17.03199406793757
Epoch 525/10000, Prediction Accuracy = 57.742%, Loss = 0.7652936100959777
Epoch: 525, Batch Gradient Norm: 13.239512168369401
Epoch: 525, Batch Gradient Norm after: 13.239512168369401
Epoch 526/10000, Prediction Accuracy = 57.73%, Loss = 0.7391819477081298
Epoch: 526, Batch Gradient Norm: 11.79369479500978
Epoch: 526, Batch Gradient Norm after: 11.79369479500978
Epoch 527/10000, Prediction Accuracy = 57.76800000000001%, Loss = 0.7273327708244324
Epoch: 527, Batch Gradient Norm: 7.230773592984159
Epoch: 527, Batch Gradient Norm after: 7.230773592984159
Epoch 528/10000, Prediction Accuracy = 57.79600000000001%, Loss = 0.7053202986717224
Epoch: 528, Batch Gradient Norm: 8.68410998809009
Epoch: 528, Batch Gradient Norm after: 8.68410998809009
Epoch 529/10000, Prediction Accuracy = 57.79%, Loss = 0.7101826071739197
Epoch: 529, Batch Gradient Norm: 14.690359837223884
Epoch: 529, Batch Gradient Norm after: 14.690359837223884
Epoch 530/10000, Prediction Accuracy = 57.706%, Loss = 0.7376868963241577
Epoch: 530, Batch Gradient Norm: 16.121710485815136
Epoch: 530, Batch Gradient Norm after: 16.121710485815136
Epoch 531/10000, Prediction Accuracy = 57.696000000000005%, Loss = 0.7535971283912659
Epoch: 531, Batch Gradient Norm: 14.765273827740762
Epoch: 531, Batch Gradient Norm after: 14.765273827740762
Epoch 532/10000, Prediction Accuracy = 57.751999999999995%, Loss = 0.741978108882904
Epoch: 532, Batch Gradient Norm: 7.856973605551198
Epoch: 532, Batch Gradient Norm after: 7.856973605551198
Epoch 533/10000, Prediction Accuracy = 57.775999999999996%, Loss = 0.7078201174736023
Epoch: 533, Batch Gradient Norm: 8.905708483217632
Epoch: 533, Batch Gradient Norm after: 8.905708483217632
Epoch 534/10000, Prediction Accuracy = 57.85%, Loss = 0.7093319535255432
Epoch: 534, Batch Gradient Norm: 11.255556116750325
Epoch: 534, Batch Gradient Norm after: 11.255556116750325
Epoch 535/10000, Prediction Accuracy = 57.80799999999999%, Loss = 0.7243382334709167
Epoch: 535, Batch Gradient Norm: 13.010672895450936
Epoch: 535, Batch Gradient Norm after: 13.010672895450936
Epoch 536/10000, Prediction Accuracy = 57.788%, Loss = 0.7339313507080079
Epoch: 536, Batch Gradient Norm: 10.744213525538157
Epoch: 536, Batch Gradient Norm after: 10.744213525538157
Epoch 537/10000, Prediction Accuracy = 57.834%, Loss = 0.7212836265563964
Epoch: 537, Batch Gradient Norm: 15.118274232612158
Epoch: 537, Batch Gradient Norm after: 15.118274232612158
Epoch 538/10000, Prediction Accuracy = 57.778%, Loss = 0.7418718934059143
Epoch: 538, Batch Gradient Norm: 14.080648349029113
Epoch: 538, Batch Gradient Norm after: 14.080648349029113
Epoch 539/10000, Prediction Accuracy = 57.717999999999996%, Loss = 0.7381254076957703
Epoch: 539, Batch Gradient Norm: 18.267322685822243
Epoch: 539, Batch Gradient Norm after: 18.267322685822243
Epoch 540/10000, Prediction Accuracy = 57.82000000000001%, Loss = 0.769026780128479
Epoch: 540, Batch Gradient Norm: 7.97903731759414
Epoch: 540, Batch Gradient Norm after: 7.97903731759414
Epoch 541/10000, Prediction Accuracy = 57.824%, Loss = 0.7070302486419677
Epoch: 541, Batch Gradient Norm: 11.001731531824
Epoch: 541, Batch Gradient Norm after: 11.001731531824
Epoch 542/10000, Prediction Accuracy = 57.80000000000001%, Loss = 0.7175187468528748
Epoch: 542, Batch Gradient Norm: 8.697364882400025
Epoch: 542, Batch Gradient Norm after: 8.697364882400025
Epoch 543/10000, Prediction Accuracy = 57.855999999999995%, Loss = 0.7096752882003784
Epoch: 543, Batch Gradient Norm: 15.473965158337611
Epoch: 543, Batch Gradient Norm after: 15.473965158337611
Epoch 544/10000, Prediction Accuracy = 57.838%, Loss = 0.7476024866104126
Epoch: 544, Batch Gradient Norm: 12.828506459121028
Epoch: 544, Batch Gradient Norm after: 12.828506459121028
Epoch 545/10000, Prediction Accuracy = 57.872%, Loss = 0.7306971549987793
Epoch: 545, Batch Gradient Norm: 14.594385196099594
Epoch: 545, Batch Gradient Norm after: 14.594385196099594
Epoch 546/10000, Prediction Accuracy = 57.855999999999995%, Loss = 0.7379305362701416
Epoch: 546, Batch Gradient Norm: 9.180936114061769
Epoch: 546, Batch Gradient Norm after: 9.180936114061769
Epoch 547/10000, Prediction Accuracy = 57.866%, Loss = 0.7084089875221252
Epoch: 547, Batch Gradient Norm: 15.748639798353535
Epoch: 547, Batch Gradient Norm after: 15.748639798353535
Epoch 548/10000, Prediction Accuracy = 57.89200000000001%, Loss = 0.7443483710289002
Epoch: 548, Batch Gradient Norm: 11.466414182494521
Epoch: 548, Batch Gradient Norm after: 11.466414182494521
Epoch 549/10000, Prediction Accuracy = 57.83399999999999%, Loss = 0.7190183997154236
Epoch: 549, Batch Gradient Norm: 15.858288980985174
Epoch: 549, Batch Gradient Norm after: 15.858288980985174
Epoch 550/10000, Prediction Accuracy = 57.876%, Loss = 0.7450761914253234
Epoch: 550, Batch Gradient Norm: 9.170512262008272
Epoch: 550, Batch Gradient Norm after: 9.170512262008272
Epoch 551/10000, Prediction Accuracy = 57.867999999999995%, Loss = 0.7069480180740356
Epoch: 551, Batch Gradient Norm: 12.48413150768319
Epoch: 551, Batch Gradient Norm after: 12.48413150768319
Epoch 552/10000, Prediction Accuracy = 57.886%, Loss = 0.7206616282463074
Epoch: 552, Batch Gradient Norm: 12.2143477680005
Epoch: 552, Batch Gradient Norm after: 12.2143477680005
Epoch 553/10000, Prediction Accuracy = 57.91799999999999%, Loss = 0.7203668594360352
Epoch: 553, Batch Gradient Norm: 14.40155364528901
Epoch: 553, Batch Gradient Norm after: 14.40155364528901
Epoch 554/10000, Prediction Accuracy = 57.884%, Loss = 0.7318006873130798
Epoch: 554, Batch Gradient Norm: 13.649482997869288
Epoch: 554, Batch Gradient Norm after: 13.649482997869288
Epoch 555/10000, Prediction Accuracy = 57.907999999999994%, Loss = 0.7312579393386841
Epoch: 555, Batch Gradient Norm: 12.198379459918488
Epoch: 555, Batch Gradient Norm after: 12.198379459918488
Epoch 556/10000, Prediction Accuracy = 57.902%, Loss = 0.7201386213302612
Epoch: 556, Batch Gradient Norm: 10.376087017254966
Epoch: 556, Batch Gradient Norm after: 10.376087017254966
Epoch 557/10000, Prediction Accuracy = 57.946000000000005%, Loss = 0.7120841026306153
Epoch: 557, Batch Gradient Norm: 10.463361595113978
Epoch: 557, Batch Gradient Norm after: 10.463361595113978
Epoch 558/10000, Prediction Accuracy = 57.912%, Loss = 0.7093401432037354
Epoch: 558, Batch Gradient Norm: 8.847439505710835
Epoch: 558, Batch Gradient Norm after: 8.847439505710835
Epoch 559/10000, Prediction Accuracy = 57.910000000000004%, Loss = 0.7026433110237121
Epoch: 559, Batch Gradient Norm: 14.543231129192824
Epoch: 559, Batch Gradient Norm after: 14.543231129192824
Epoch 560/10000, Prediction Accuracy = 57.898%, Loss = 0.7293811321258545
Epoch: 560, Batch Gradient Norm: 10.231159938746247
Epoch: 560, Batch Gradient Norm after: 10.231159938746247
Epoch 561/10000, Prediction Accuracy = 57.86800000000001%, Loss = 0.7075060367584228
Epoch: 561, Batch Gradient Norm: 15.804207010118919
Epoch: 561, Batch Gradient Norm after: 15.804207010118919
Epoch 562/10000, Prediction Accuracy = 57.884%, Loss = 0.739563250541687
Epoch: 562, Batch Gradient Norm: 10.990828457125772
Epoch: 562, Batch Gradient Norm after: 10.990828457125772
Epoch 563/10000, Prediction Accuracy = 57.931999999999995%, Loss = 0.7125000834465027
Epoch: 563, Batch Gradient Norm: 18.406010983815865
Epoch: 563, Batch Gradient Norm after: 18.406010983815865
Epoch 564/10000, Prediction Accuracy = 57.878%, Loss = 0.7653133153915406
Epoch: 564, Batch Gradient Norm: 13.146814058401445
Epoch: 564, Batch Gradient Norm after: 13.146814058401445
Epoch 565/10000, Prediction Accuracy = 57.955999999999996%, Loss = 0.7283711433410645
Epoch: 565, Batch Gradient Norm: 13.81670011100247
Epoch: 565, Batch Gradient Norm after: 13.81670011100247
Epoch 566/10000, Prediction Accuracy = 57.874%, Loss = 0.7286253690719604
Epoch: 566, Batch Gradient Norm: 9.547907579791683
Epoch: 566, Batch Gradient Norm after: 9.547907579791683
Epoch 567/10000, Prediction Accuracy = 58.053999999999995%, Loss = 0.7047780394554138
Epoch: 567, Batch Gradient Norm: 11.443667146118525
Epoch: 567, Batch Gradient Norm after: 11.443667146118525
Epoch 568/10000, Prediction Accuracy = 57.976%, Loss = 0.7101433873176575
Epoch: 568, Batch Gradient Norm: 10.56366413667148
Epoch: 568, Batch Gradient Norm after: 10.56366413667148
Epoch 569/10000, Prediction Accuracy = 58.00599999999999%, Loss = 0.7073599576950074
Epoch: 569, Batch Gradient Norm: 14.475957087631802
Epoch: 569, Batch Gradient Norm after: 14.475957087631802
Epoch 570/10000, Prediction Accuracy = 57.977999999999994%, Loss = 0.7275485634803772
Epoch: 570, Batch Gradient Norm: 14.07772509239664
Epoch: 570, Batch Gradient Norm after: 14.07772509239664
Epoch 571/10000, Prediction Accuracy = 58.076%, Loss = 0.731261157989502
Epoch: 571, Batch Gradient Norm: 14.625729173852358
Epoch: 571, Batch Gradient Norm after: 14.625729173852358
Epoch 572/10000, Prediction Accuracy = 58.008%, Loss = 0.7350452303886413
Epoch: 572, Batch Gradient Norm: 9.740493367247574
Epoch: 572, Batch Gradient Norm after: 9.740493367247574
Epoch 573/10000, Prediction Accuracy = 58.04%, Loss = 0.7045529961585999
Epoch: 573, Batch Gradient Norm: 12.105261189583294
Epoch: 573, Batch Gradient Norm after: 12.105261189583294
Epoch 574/10000, Prediction Accuracy = 57.944%, Loss = 0.7140201091766357
Epoch: 574, Batch Gradient Norm: 11.738210224239307
Epoch: 574, Batch Gradient Norm after: 11.738210224239307
Epoch 575/10000, Prediction Accuracy = 57.976%, Loss = 0.7138187050819397
Epoch: 575, Batch Gradient Norm: 17.8880855836239
Epoch: 575, Batch Gradient Norm after: 17.8880855836239
Epoch 576/10000, Prediction Accuracy = 57.976%, Loss = 0.756329345703125
Epoch: 576, Batch Gradient Norm: 8.532426293018199
Epoch: 576, Batch Gradient Norm after: 8.532426293018199
Epoch 577/10000, Prediction Accuracy = 58.017999999999994%, Loss = 0.6988968133926392
Epoch: 577, Batch Gradient Norm: 11.312134166465126
Epoch: 577, Batch Gradient Norm after: 11.312134166465126
Epoch 578/10000, Prediction Accuracy = 58.001999999999995%, Loss = 0.7078253865242005
Epoch: 578, Batch Gradient Norm: 7.719427217242853
Epoch: 578, Batch Gradient Norm after: 7.719427217242853
Epoch 579/10000, Prediction Accuracy = 58.00599999999999%, Loss = 0.692183518409729
Epoch: 579, Batch Gradient Norm: 13.821525360115196
Epoch: 579, Batch Gradient Norm after: 13.821525360115196
Epoch 580/10000, Prediction Accuracy = 57.94200000000001%, Loss = 0.7206114053726196
Epoch: 580, Batch Gradient Norm: 11.063684054610967
Epoch: 580, Batch Gradient Norm after: 11.063684054610967
Epoch 581/10000, Prediction Accuracy = 58.012%, Loss = 0.7061758279800415
Epoch: 581, Batch Gradient Norm: 15.763091311590781
Epoch: 581, Batch Gradient Norm after: 15.763091311590781
Epoch 582/10000, Prediction Accuracy = 57.914%, Loss = 0.7339217066764832
Epoch: 582, Batch Gradient Norm: 10.469529807719233
Epoch: 582, Batch Gradient Norm after: 10.469529807719233
Epoch 583/10000, Prediction Accuracy = 58.056%, Loss = 0.703213346004486
Epoch: 583, Batch Gradient Norm: 15.748270291466833
Epoch: 583, Batch Gradient Norm after: 15.748270291466833
Epoch 584/10000, Prediction Accuracy = 57.996%, Loss = 0.7355098724365234
Epoch: 584, Batch Gradient Norm: 11.88753048763758
Epoch: 584, Batch Gradient Norm after: 11.88753048763758
Epoch 585/10000, Prediction Accuracy = 58.102%, Loss = 0.7134517431259155
Epoch: 585, Batch Gradient Norm: 12.989165351957203
Epoch: 585, Batch Gradient Norm after: 12.989165351957203
Epoch 586/10000, Prediction Accuracy = 58.013999999999996%, Loss = 0.7169820070266724
Epoch: 586, Batch Gradient Norm: 10.227146433240824
Epoch: 586, Batch Gradient Norm after: 10.227146433240824
Epoch 587/10000, Prediction Accuracy = 58.122%, Loss = 0.7029305458068847
Epoch: 587, Batch Gradient Norm: 11.831386395065572
Epoch: 587, Batch Gradient Norm after: 11.831386395065572
Epoch 588/10000, Prediction Accuracy = 58.032%, Loss = 0.7079638361930847
Epoch: 588, Batch Gradient Norm: 10.118084124760271
Epoch: 588, Batch Gradient Norm after: 10.118084124760271
Epoch 589/10000, Prediction Accuracy = 58.141999999999996%, Loss = 0.7004797697067261
Epoch: 589, Batch Gradient Norm: 15.929854461735992
Epoch: 589, Batch Gradient Norm after: 15.929854461735992
Epoch 590/10000, Prediction Accuracy = 57.996%, Loss = 0.734071958065033
Epoch: 590, Batch Gradient Norm: 11.120141434651536
Epoch: 590, Batch Gradient Norm after: 11.120141434651536
Epoch 591/10000, Prediction Accuracy = 58.166%, Loss = 0.707327914237976
Epoch: 591, Batch Gradient Norm: 14.89339836697215
Epoch: 591, Batch Gradient Norm after: 14.89339836697215
Epoch 592/10000, Prediction Accuracy = 58.03800000000001%, Loss = 0.7294304490089416
Epoch: 592, Batch Gradient Norm: 7.962584867551498
Epoch: 592, Batch Gradient Norm after: 7.962584867551498
Epoch 593/10000, Prediction Accuracy = 58.13199999999999%, Loss = 0.6903443098068237
Epoch: 593, Batch Gradient Norm: 13.955904515113613
Epoch: 593, Batch Gradient Norm after: 13.955904515113613
Epoch 594/10000, Prediction Accuracy = 58.052%, Loss = 0.718868088722229
Epoch: 594, Batch Gradient Norm: 13.27278772311173
Epoch: 594, Batch Gradient Norm after: 13.27278772311173
Epoch 595/10000, Prediction Accuracy = 58.136%, Loss = 0.7160184741020202
Epoch: 595, Batch Gradient Norm: 17.186293574307285
Epoch: 595, Batch Gradient Norm after: 17.186293574307285
Epoch 596/10000, Prediction Accuracy = 58.089999999999996%, Loss = 0.7490313410758972
Epoch: 596, Batch Gradient Norm: 12.415282154885704
Epoch: 596, Batch Gradient Norm after: 12.415282154885704
Epoch 597/10000, Prediction Accuracy = 58.15%, Loss = 0.7107563614845276
Epoch: 597, Batch Gradient Norm: 13.664567669561519
Epoch: 597, Batch Gradient Norm after: 13.664567669561519
Epoch 598/10000, Prediction Accuracy = 58.10600000000001%, Loss = 0.7166935563087463
Epoch: 598, Batch Gradient Norm: 16.042043271280953
Epoch: 598, Batch Gradient Norm after: 16.042043271280953
Epoch 599/10000, Prediction Accuracy = 58.062%, Loss = 0.7327277302742005
Epoch: 599, Batch Gradient Norm: 7.5999194916378245
Epoch: 599, Batch Gradient Norm after: 7.5999194916378245
Epoch 600/10000, Prediction Accuracy = 58.13399999999999%, Loss = 0.6875183463096619
Epoch: 600, Batch Gradient Norm: 9.369261414838661
Epoch: 600, Batch Gradient Norm after: 9.369261414838661
Epoch 601/10000, Prediction Accuracy = 58.176%, Loss = 0.6921600937843323
Epoch: 601, Batch Gradient Norm: 9.039473339565728
Epoch: 601, Batch Gradient Norm after: 9.039473339565728
Epoch 602/10000, Prediction Accuracy = 58.116%, Loss = 0.6931432485580444
Epoch: 602, Batch Gradient Norm: 12.9637202528923
Epoch: 602, Batch Gradient Norm after: 12.9637202528923
Epoch 603/10000, Prediction Accuracy = 58.14%, Loss = 0.7125709414482116
Epoch: 603, Batch Gradient Norm: 11.460463355002611
Epoch: 603, Batch Gradient Norm after: 11.460463355002611
Epoch 604/10000, Prediction Accuracy = 58.086%, Loss = 0.7060868501663208
Epoch: 604, Batch Gradient Norm: 13.234872916556085
Epoch: 604, Batch Gradient Norm after: 13.234872916556085
Epoch 605/10000, Prediction Accuracy = 58.168000000000006%, Loss = 0.7139114618301392
Epoch: 605, Batch Gradient Norm: 9.945195823584818
Epoch: 605, Batch Gradient Norm after: 9.945195823584818
Epoch 606/10000, Prediction Accuracy = 58.148%, Loss = 0.6964520812034607
Epoch: 606, Batch Gradient Norm: 12.20322467569748
Epoch: 606, Batch Gradient Norm after: 12.20322467569748
Epoch 607/10000, Prediction Accuracy = 58.162%, Loss = 0.7074617266654968
Epoch: 607, Batch Gradient Norm: 9.22119359605248
Epoch: 607, Batch Gradient Norm after: 9.22119359605248
Epoch 608/10000, Prediction Accuracy = 58.2%, Loss = 0.6916976809501648
Epoch: 608, Batch Gradient Norm: 17.651774527668632
Epoch: 608, Batch Gradient Norm after: 17.651774527668632
Epoch 609/10000, Prediction Accuracy = 58.146%, Loss = 0.7415099024772644
Epoch: 609, Batch Gradient Norm: 13.393993877194315
Epoch: 609, Batch Gradient Norm after: 13.393993877194315
Epoch 610/10000, Prediction Accuracy = 58.172000000000004%, Loss = 0.7138106226921082
Epoch: 610, Batch Gradient Norm: 14.677504268387791
Epoch: 610, Batch Gradient Norm after: 14.677504268387791
Epoch 611/10000, Prediction Accuracy = 58.157999999999994%, Loss = 0.7194882154464721
Epoch: 611, Batch Gradient Norm: 13.71662349554597
Epoch: 611, Batch Gradient Norm after: 13.71662349554597
Epoch 612/10000, Prediction Accuracy = 58.248000000000005%, Loss = 0.7141730070114136
Epoch: 612, Batch Gradient Norm: 12.731795982829752
Epoch: 612, Batch Gradient Norm after: 12.731795982829752
Epoch 613/10000, Prediction Accuracy = 58.25600000000001%, Loss = 0.7072794198989868
Epoch: 613, Batch Gradient Norm: 13.17406658791114
Epoch: 613, Batch Gradient Norm after: 13.17406658791114
Epoch 614/10000, Prediction Accuracy = 58.193999999999996%, Loss = 0.711254334449768
Epoch: 614, Batch Gradient Norm: 9.040651478454535
Epoch: 614, Batch Gradient Norm after: 9.040651478454535
Epoch 615/10000, Prediction Accuracy = 58.3%, Loss = 0.6900523900985718
Epoch: 615, Batch Gradient Norm: 12.596685814248254
Epoch: 615, Batch Gradient Norm after: 12.596685814248254
Epoch 616/10000, Prediction Accuracy = 58.14399999999999%, Loss = 0.7065715432167053
Epoch: 616, Batch Gradient Norm: 11.831823467848013
Epoch: 616, Batch Gradient Norm after: 11.831823467848013
Epoch 617/10000, Prediction Accuracy = 58.260000000000005%, Loss = 0.7011338472366333
Epoch: 617, Batch Gradient Norm: 17.590504307988134
Epoch: 617, Batch Gradient Norm after: 17.590504307988134
Epoch 618/10000, Prediction Accuracy = 58.162%, Loss = 0.7397311449050903
Epoch: 618, Batch Gradient Norm: 9.525957064722325
Epoch: 618, Batch Gradient Norm after: 9.525957064722325
Epoch 619/10000, Prediction Accuracy = 58.29600000000001%, Loss = 0.691503381729126
Epoch: 619, Batch Gradient Norm: 12.824347320733738
Epoch: 619, Batch Gradient Norm after: 12.824347320733738
Epoch 620/10000, Prediction Accuracy = 58.236000000000004%, Loss = 0.7080723524093628
Epoch: 620, Batch Gradient Norm: 11.791855287063214
Epoch: 620, Batch Gradient Norm after: 11.791855287063214
Epoch 621/10000, Prediction Accuracy = 58.24400000000001%, Loss = 0.7045059680938721
Epoch: 621, Batch Gradient Norm: 10.96961758815157
Epoch: 621, Batch Gradient Norm after: 10.96961758815157
Epoch 622/10000, Prediction Accuracy = 58.282%, Loss = 0.6960487127304077
Epoch: 622, Batch Gradient Norm: 8.406643331165126
Epoch: 622, Batch Gradient Norm after: 8.406643331165126
Epoch 623/10000, Prediction Accuracy = 58.30800000000001%, Loss = 0.6839382648468018
Epoch: 623, Batch Gradient Norm: 11.386120177291527
Epoch: 623, Batch Gradient Norm after: 11.386120177291527
Epoch 624/10000, Prediction Accuracy = 58.261999999999986%, Loss = 0.6942646861076355
Epoch: 624, Batch Gradient Norm: 10.694662137419838
Epoch: 624, Batch Gradient Norm after: 10.694662137419838
Epoch 625/10000, Prediction Accuracy = 58.238%, Loss = 0.6922214746475219
Epoch: 625, Batch Gradient Norm: 15.88897896068291
Epoch: 625, Batch Gradient Norm after: 15.88897896068291
Epoch 626/10000, Prediction Accuracy = 58.266000000000005%, Loss = 0.725282609462738
Epoch: 626, Batch Gradient Norm: 12.828962421787585
Epoch: 626, Batch Gradient Norm after: 12.828962421787585
Epoch 627/10000, Prediction Accuracy = 58.346000000000004%, Loss = 0.7098962306976319
Epoch: 627, Batch Gradient Norm: 13.554750540748781
Epoch: 627, Batch Gradient Norm after: 13.554750540748781
Epoch 628/10000, Prediction Accuracy = 58.370000000000005%, Loss = 0.7123566031455993
Epoch: 628, Batch Gradient Norm: 12.355190279804553
Epoch: 628, Batch Gradient Norm after: 12.355190279804553
Epoch 629/10000, Prediction Accuracy = 58.378%, Loss = 0.7011142134666443
Epoch: 629, Batch Gradient Norm: 12.846765446329782
Epoch: 629, Batch Gradient Norm after: 12.846765446329782
Epoch 630/10000, Prediction Accuracy = 58.33%, Loss = 0.7046968221664429
Epoch: 630, Batch Gradient Norm: 17.09185165648409
Epoch: 630, Batch Gradient Norm after: 17.09185165648409
Epoch 631/10000, Prediction Accuracy = 58.217999999999996%, Loss = 0.732638156414032
Epoch: 631, Batch Gradient Norm: 11.222822063207815
Epoch: 631, Batch Gradient Norm after: 11.222822063207815
Epoch 632/10000, Prediction Accuracy = 58.326%, Loss = 0.6936591744422913
Epoch: 632, Batch Gradient Norm: 15.087082059997028
Epoch: 632, Batch Gradient Norm after: 15.087082059997028
Epoch 633/10000, Prediction Accuracy = 58.27399999999999%, Loss = 0.7167783856391907
Epoch: 633, Batch Gradient Norm: 8.586346228429647
Epoch: 633, Batch Gradient Norm after: 8.586346228429647
Epoch 634/10000, Prediction Accuracy = 58.42%, Loss = 0.6817063689231873
Epoch: 634, Batch Gradient Norm: 14.588986709755872
Epoch: 634, Batch Gradient Norm after: 14.588986709755872
Epoch 635/10000, Prediction Accuracy = 58.298%, Loss = 0.7155405759811402
Epoch: 635, Batch Gradient Norm: 10.762591051590224
Epoch: 635, Batch Gradient Norm after: 10.762591051590224
Epoch 636/10000, Prediction Accuracy = 58.422000000000004%, Loss = 0.6934304475784302
Epoch: 636, Batch Gradient Norm: 11.494493182382394
Epoch: 636, Batch Gradient Norm after: 11.494493182382394
Epoch 637/10000, Prediction Accuracy = 58.39%, Loss = 0.6954645156860352
Epoch: 637, Batch Gradient Norm: 7.891050440527352
Epoch: 637, Batch Gradient Norm after: 7.891050440527352
Epoch 638/10000, Prediction Accuracy = 58.438%, Loss = 0.6776440024375916
Epoch: 638, Batch Gradient Norm: 14.317707993336946
Epoch: 638, Batch Gradient Norm after: 14.317707993336946
Epoch 639/10000, Prediction Accuracy = 58.343999999999994%, Loss = 0.7095584630966186
Epoch: 639, Batch Gradient Norm: 16.348118336154993
Epoch: 639, Batch Gradient Norm after: 16.348118336154993
Epoch 640/10000, Prediction Accuracy = 58.30800000000001%, Loss = 0.7279110074043273
Epoch: 640, Batch Gradient Norm: 14.593190497418941
Epoch: 640, Batch Gradient Norm after: 14.593190497418941
Epoch 641/10000, Prediction Accuracy = 58.339999999999996%, Loss = 0.7137595057487488
Epoch: 641, Batch Gradient Norm: 9.55109739356758
Epoch: 641, Batch Gradient Norm after: 9.55109739356758
Epoch 642/10000, Prediction Accuracy = 58.382000000000005%, Loss = 0.684630835056305
Epoch: 642, Batch Gradient Norm: 9.538369064943373
Epoch: 642, Batch Gradient Norm after: 9.538369064943373
Epoch 643/10000, Prediction Accuracy = 58.464%, Loss = 0.6822094559669495
Epoch: 643, Batch Gradient Norm: 11.597188353939664
Epoch: 643, Batch Gradient Norm after: 11.597188353939664
Epoch 644/10000, Prediction Accuracy = 58.35999999999999%, Loss = 0.6943859696388245
Epoch: 644, Batch Gradient Norm: 12.258908573769945
Epoch: 644, Batch Gradient Norm after: 12.258908573769945
Epoch 645/10000, Prediction Accuracy = 58.452%, Loss = 0.6951362133026123
Epoch: 645, Batch Gradient Norm: 12.165605205223608
Epoch: 645, Batch Gradient Norm after: 12.165605205223608
Epoch 646/10000, Prediction Accuracy = 58.386%, Loss = 0.6963397264480591
Epoch: 646, Batch Gradient Norm: 15.080791376647616
Epoch: 646, Batch Gradient Norm after: 15.080791376647616
Epoch 647/10000, Prediction Accuracy = 58.41600000000001%, Loss = 0.7131831884384155
Epoch: 647, Batch Gradient Norm: 11.15394258648072
Epoch: 647, Batch Gradient Norm after: 11.15394258648072
Epoch 648/10000, Prediction Accuracy = 58.438%, Loss = 0.6922155022621155
Epoch: 648, Batch Gradient Norm: 14.119528834433375
Epoch: 648, Batch Gradient Norm after: 14.119528834433375
Epoch 649/10000, Prediction Accuracy = 58.41799999999999%, Loss = 0.7096072316169739
Epoch: 649, Batch Gradient Norm: 9.517430379609607
Epoch: 649, Batch Gradient Norm after: 9.517430379609607
Epoch 650/10000, Prediction Accuracy = 58.446000000000005%, Loss = 0.684502112865448
Epoch: 650, Batch Gradient Norm: 12.686092492916119
Epoch: 650, Batch Gradient Norm after: 12.686092492916119
Epoch 651/10000, Prediction Accuracy = 58.434000000000005%, Loss = 0.6988928318023682
Epoch: 651, Batch Gradient Norm: 10.063590038968226
Epoch: 651, Batch Gradient Norm after: 10.063590038968226
Epoch 652/10000, Prediction Accuracy = 58.43000000000001%, Loss = 0.6845050096511841
Epoch: 652, Batch Gradient Norm: 14.788981691289116
Epoch: 652, Batch Gradient Norm after: 14.788981691289116
Epoch 653/10000, Prediction Accuracy = 58.36800000000001%, Loss = 0.7103752613067627
Epoch: 653, Batch Gradient Norm: 12.072117347814697
Epoch: 653, Batch Gradient Norm after: 12.072117347814697
Epoch 654/10000, Prediction Accuracy = 58.376%, Loss = 0.6949233293533326
Epoch: 654, Batch Gradient Norm: 16.46361603282824
Epoch: 654, Batch Gradient Norm after: 16.46361603282824
Epoch 655/10000, Prediction Accuracy = 58.35799999999999%, Loss = 0.7255334377288818
Epoch: 655, Batch Gradient Norm: 14.22649815267995
Epoch: 655, Batch Gradient Norm after: 14.22649815267995
Epoch 656/10000, Prediction Accuracy = 58.455999999999996%, Loss = 0.7111375570297241
Epoch: 656, Batch Gradient Norm: 14.840139328185623
Epoch: 656, Batch Gradient Norm after: 14.840139328185623
Epoch 657/10000, Prediction Accuracy = 58.4%, Loss = 0.7146111369132996
Epoch: 657, Batch Gradient Norm: 9.121178485402373
Epoch: 657, Batch Gradient Norm after: 9.121178485402373
Epoch 658/10000, Prediction Accuracy = 58.528%, Loss = 0.6788893103599548
Epoch: 658, Batch Gradient Norm: 9.940069135336953
Epoch: 658, Batch Gradient Norm after: 9.940069135336953
Epoch 659/10000, Prediction Accuracy = 58.43399999999999%, Loss = 0.6812016487121582
Epoch: 659, Batch Gradient Norm: 9.527990588367029
Epoch: 659, Batch Gradient Norm after: 9.527990588367029
Epoch 660/10000, Prediction Accuracy = 58.532%, Loss = 0.6785306811332703
Epoch: 660, Batch Gradient Norm: 11.888901705641832
Epoch: 660, Batch Gradient Norm after: 11.888901705641832
Epoch 661/10000, Prediction Accuracy = 58.41600000000001%, Loss = 0.6904487013816833
Epoch: 661, Batch Gradient Norm: 16.287177643607027
Epoch: 661, Batch Gradient Norm after: 16.287177643607027
Epoch 662/10000, Prediction Accuracy = 58.443999999999996%, Loss = 0.7172722816467285
Epoch: 662, Batch Gradient Norm: 10.427004113358317
Epoch: 662, Batch Gradient Norm after: 10.427004113358317
Epoch 663/10000, Prediction Accuracy = 58.468%, Loss = 0.6827844381332397
Epoch: 663, Batch Gradient Norm: 12.379611105174462
Epoch: 663, Batch Gradient Norm after: 12.379611105174462
Epoch 664/10000, Prediction Accuracy = 58.448%, Loss = 0.6909855127334594
Epoch: 664, Batch Gradient Norm: 11.257128363318728
Epoch: 664, Batch Gradient Norm after: 11.257128363318728
Epoch 665/10000, Prediction Accuracy = 58.484%, Loss = 0.6870466947555542
Epoch: 665, Batch Gradient Norm: 17.219468435527407
Epoch: 665, Batch Gradient Norm after: 17.219468435527407
Epoch 666/10000, Prediction Accuracy = 58.403999999999996%, Loss = 0.7296599626541138
Epoch: 666, Batch Gradient Norm: 11.741464956738795
Epoch: 666, Batch Gradient Norm after: 11.741464956738795
Epoch 667/10000, Prediction Accuracy = 58.53000000000001%, Loss = 0.6924144625663757
Epoch: 667, Batch Gradient Norm: 11.919747962099606
Epoch: 667, Batch Gradient Norm after: 11.919747962099606
Epoch 668/10000, Prediction Accuracy = 58.448%, Loss = 0.690872311592102
Epoch: 668, Batch Gradient Norm: 10.512835746993998
Epoch: 668, Batch Gradient Norm after: 10.512835746993998
Epoch 669/10000, Prediction Accuracy = 58.57000000000001%, Loss = 0.6844782829284668
Epoch: 669, Batch Gradient Norm: 12.382006049351075
Epoch: 669, Batch Gradient Norm after: 12.382006049351075
Epoch 670/10000, Prediction Accuracy = 58.486000000000004%, Loss = 0.6930232167243957
Epoch: 670, Batch Gradient Norm: 10.36824452521701
Epoch: 670, Batch Gradient Norm after: 10.36824452521701
Epoch 671/10000, Prediction Accuracy = 58.55%, Loss = 0.6823714375495911
Epoch: 671, Batch Gradient Norm: 13.227198003121858
Epoch: 671, Batch Gradient Norm after: 13.227198003121858
Epoch 672/10000, Prediction Accuracy = 58.44%, Loss = 0.6946218132972717
Epoch: 672, Batch Gradient Norm: 11.65975967012592
Epoch: 672, Batch Gradient Norm after: 11.65975967012592
Epoch 673/10000, Prediction Accuracy = 58.50600000000001%, Loss = 0.6863262057304382
Epoch: 673, Batch Gradient Norm: 18.05142184565613
Epoch: 673, Batch Gradient Norm after: 18.05142184565613
Epoch 674/10000, Prediction Accuracy = 58.428%, Loss = 0.7326531052589417
Epoch: 674, Batch Gradient Norm: 11.7271755567512
Epoch: 674, Batch Gradient Norm after: 11.7271755567512
Epoch 675/10000, Prediction Accuracy = 58.612%, Loss = 0.6882744312286377
Epoch: 675, Batch Gradient Norm: 13.011801547505454
Epoch: 675, Batch Gradient Norm after: 13.011801547505454
Epoch 676/10000, Prediction Accuracy = 58.489999999999995%, Loss = 0.6953891158103943
Epoch: 676, Batch Gradient Norm: 7.347825349415407
Epoch: 676, Batch Gradient Norm after: 7.347825349415407
Epoch 677/10000, Prediction Accuracy = 58.608000000000004%, Loss = 0.6660562038421631
Epoch: 677, Batch Gradient Norm: 10.039475206428987
Epoch: 677, Batch Gradient Norm after: 10.039475206428987
Epoch 678/10000, Prediction Accuracy = 58.548%, Loss = 0.6769683718681335
Epoch: 678, Batch Gradient Norm: 11.13764690813989
Epoch: 678, Batch Gradient Norm after: 11.13764690813989
Epoch 679/10000, Prediction Accuracy = 58.574%, Loss = 0.6812831640243531
Epoch: 679, Batch Gradient Norm: 15.261322972053934
Epoch: 679, Batch Gradient Norm after: 15.261322972053934
Epoch 680/10000, Prediction Accuracy = 58.431999999999995%, Loss = 0.7063935995101929
Epoch: 680, Batch Gradient Norm: 17.051760016704307
Epoch: 680, Batch Gradient Norm after: 17.051760016704307
Epoch 681/10000, Prediction Accuracy = 58.48199999999999%, Loss = 0.7221998810768128
Epoch: 681, Batch Gradient Norm: 11.423369621844747
Epoch: 681, Batch Gradient Norm after: 11.423369621844747
Epoch 682/10000, Prediction Accuracy = 58.50600000000001%, Loss = 0.6866870760917664
Epoch: 682, Batch Gradient Norm: 13.657463518515264
Epoch: 682, Batch Gradient Norm after: 13.657463518515264
Epoch 683/10000, Prediction Accuracy = 58.49400000000001%, Loss = 0.701327109336853
Epoch: 683, Batch Gradient Norm: 10.04417289329738
Epoch: 683, Batch Gradient Norm after: 10.04417289329738
Epoch 684/10000, Prediction Accuracy = 58.54200000000001%, Loss = 0.6795350670814514
Epoch: 684, Batch Gradient Norm: 10.467891884507353
Epoch: 684, Batch Gradient Norm after: 10.467891884507353
Epoch 685/10000, Prediction Accuracy = 58.553999999999995%, Loss = 0.6778497934341431
Epoch: 685, Batch Gradient Norm: 8.004948812227232
Epoch: 685, Batch Gradient Norm after: 8.004948812227232
Epoch 686/10000, Prediction Accuracy = 58.508%, Loss = 0.6658508896827697
Epoch: 686, Batch Gradient Norm: 15.018591919802752
Epoch: 686, Batch Gradient Norm after: 15.018591919802752
Epoch 687/10000, Prediction Accuracy = 58.52%, Loss = 0.7023286104202271
Epoch: 687, Batch Gradient Norm: 11.82718552843319
Epoch: 687, Batch Gradient Norm after: 11.82718552843319
Epoch 688/10000, Prediction Accuracy = 58.522000000000006%, Loss = 0.6841079473495484
Epoch: 688, Batch Gradient Norm: 15.393911778061225
Epoch: 688, Batch Gradient Norm after: 15.393911778061225
Epoch 689/10000, Prediction Accuracy = 58.54600000000001%, Loss = 0.7074925899505615
Epoch: 689, Batch Gradient Norm: 9.501809927730834
Epoch: 689, Batch Gradient Norm after: 9.501809927730834
Epoch 690/10000, Prediction Accuracy = 58.577999999999996%, Loss = 0.6726858019828796
Epoch: 690, Batch Gradient Norm: 11.91609335617283
Epoch: 690, Batch Gradient Norm after: 11.91609335617283
Epoch 691/10000, Prediction Accuracy = 58.598%, Loss = 0.686402690410614
Epoch: 691, Batch Gradient Norm: 13.61822129991258
Epoch: 691, Batch Gradient Norm after: 13.61822129991258
Epoch 692/10000, Prediction Accuracy = 58.61999999999999%, Loss = 0.699237871170044
Epoch: 692, Batch Gradient Norm: 14.038476371953484
Epoch: 692, Batch Gradient Norm after: 14.038476371953484
Epoch 693/10000, Prediction Accuracy = 58.562%, Loss = 0.7007163524627685
Epoch: 693, Batch Gradient Norm: 11.99089278902191
Epoch: 693, Batch Gradient Norm after: 11.99089278902191
Epoch 694/10000, Prediction Accuracy = 58.65%, Loss = 0.6850552678108215
Epoch: 694, Batch Gradient Norm: 13.861298006957558
Epoch: 694, Batch Gradient Norm after: 13.861298006957558
Epoch 695/10000, Prediction Accuracy = 58.622%, Loss = 0.6926305770874024
Epoch: 695, Batch Gradient Norm: 14.619376156573262
Epoch: 695, Batch Gradient Norm after: 14.619376156573262
Epoch 696/10000, Prediction Accuracy = 58.553999999999995%, Loss = 0.7011287808418274
Epoch: 696, Batch Gradient Norm: 14.897347957366629
Epoch: 696, Batch Gradient Norm after: 14.897347957366629
Epoch 697/10000, Prediction Accuracy = 58.524%, Loss = 0.7043616771697998
Epoch: 697, Batch Gradient Norm: 7.86401607535472
Epoch: 697, Batch Gradient Norm after: 7.86401607535472
Epoch 698/10000, Prediction Accuracy = 58.664%, Loss = 0.6658668279647827
Epoch: 698, Batch Gradient Norm: 12.74853528634781
Epoch: 698, Batch Gradient Norm after: 12.74853528634781
Epoch 699/10000, Prediction Accuracy = 58.55%, Loss = 0.6895781517028808
Epoch: 699, Batch Gradient Norm: 12.00663701851424
Epoch: 699, Batch Gradient Norm after: 12.00663701851424
Epoch 700/10000, Prediction Accuracy = 58.742%, Loss = 0.6853885889053345
Epoch: 700, Batch Gradient Norm: 14.315186341136787
Epoch: 700, Batch Gradient Norm after: 14.315186341136787
Epoch 701/10000, Prediction Accuracy = 58.55799999999999%, Loss = 0.6992176413536072
Epoch: 701, Batch Gradient Norm: 10.492106017624803
Epoch: 701, Batch Gradient Norm after: 10.492106017624803
Epoch 702/10000, Prediction Accuracy = 58.696000000000005%, Loss = 0.6732365131378174
Epoch: 702, Batch Gradient Norm: 11.921825259638819
Epoch: 702, Batch Gradient Norm after: 11.921825259638819
Epoch 703/10000, Prediction Accuracy = 58.54599999999999%, Loss = 0.6804121494293213
Epoch: 703, Batch Gradient Norm: 15.118692050763164
Epoch: 703, Batch Gradient Norm after: 15.118692050763164
Epoch 704/10000, Prediction Accuracy = 58.616%, Loss = 0.7008729457855225
Epoch: 704, Batch Gradient Norm: 9.50009689260506
Epoch: 704, Batch Gradient Norm after: 9.50009689260506
Epoch 705/10000, Prediction Accuracy = 58.586%, Loss = 0.6704428791999817
Epoch: 705, Batch Gradient Norm: 12.173917225373021
Epoch: 705, Batch Gradient Norm after: 12.173917225373021
Epoch 706/10000, Prediction Accuracy = 58.674%, Loss = 0.683734142780304
Epoch: 706, Batch Gradient Norm: 13.203252065709485
Epoch: 706, Batch Gradient Norm after: 13.203252065709485
Epoch 707/10000, Prediction Accuracy = 58.604%, Loss = 0.693185031414032
Epoch: 707, Batch Gradient Norm: 11.835056008066136
Epoch: 707, Batch Gradient Norm after: 11.835056008066136
Epoch 708/10000, Prediction Accuracy = 58.65599999999999%, Loss = 0.6815447092056275
Epoch: 708, Batch Gradient Norm: 10.09774737109582
Epoch: 708, Batch Gradient Norm after: 10.09774737109582
Epoch 709/10000, Prediction Accuracy = 58.686%, Loss = 0.6706590056419373
Epoch: 709, Batch Gradient Norm: 11.15765489703921
Epoch: 709, Batch Gradient Norm after: 11.15765489703921
Epoch 710/10000, Prediction Accuracy = 58.604000000000006%, Loss = 0.6749491930007935
Epoch: 710, Batch Gradient Norm: 15.634456906292296
Epoch: 710, Batch Gradient Norm after: 15.634456906292296
Epoch 711/10000, Prediction Accuracy = 58.676%, Loss = 0.7018413424491883
Epoch: 711, Batch Gradient Norm: 10.934391187228343
Epoch: 711, Batch Gradient Norm after: 10.934391187228343
Epoch 712/10000, Prediction Accuracy = 58.574%, Loss = 0.6756585955619812
Epoch: 712, Batch Gradient Norm: 11.738159924738143
Epoch: 712, Batch Gradient Norm after: 11.738159924738143
Epoch 713/10000, Prediction Accuracy = 58.63199999999999%, Loss = 0.67790447473526
Epoch: 713, Batch Gradient Norm: 10.341761089658013
Epoch: 713, Batch Gradient Norm after: 10.341761089658013
Epoch 714/10000, Prediction Accuracy = 58.620000000000005%, Loss = 0.6727865695953369
Epoch: 714, Batch Gradient Norm: 17.765421729458964
Epoch: 714, Batch Gradient Norm after: 17.765421729458964
Epoch 715/10000, Prediction Accuracy = 58.565999999999995%, Loss = 0.7222519278526306
Epoch: 715, Batch Gradient Norm: 13.360602428761958
Epoch: 715, Batch Gradient Norm after: 13.360602428761958
Epoch 716/10000, Prediction Accuracy = 58.70799999999999%, Loss = 0.6898428320884704
Epoch: 716, Batch Gradient Norm: 14.458052152015128
Epoch: 716, Batch Gradient Norm after: 14.458052152015128
Epoch 717/10000, Prediction Accuracy = 58.653999999999996%, Loss = 0.6978240013122559
Epoch: 717, Batch Gradient Norm: 9.994983650901752
Epoch: 717, Batch Gradient Norm after: 9.994983650901752
Epoch 718/10000, Prediction Accuracy = 58.693999999999996%, Loss = 0.6706154942512512
Epoch: 718, Batch Gradient Norm: 11.870617046077264
Epoch: 718, Batch Gradient Norm after: 11.870617046077264
Epoch 719/10000, Prediction Accuracy = 58.69799999999999%, Loss = 0.680402648448944
Epoch: 719, Batch Gradient Norm: 10.933875740960653
Epoch: 719, Batch Gradient Norm after: 10.933875740960653
Epoch 720/10000, Prediction Accuracy = 58.658%, Loss = 0.6746074080467224
Epoch: 720, Batch Gradient Norm: 12.011275209742191
Epoch: 720, Batch Gradient Norm after: 12.011275209742191
Epoch 721/10000, Prediction Accuracy = 58.714%, Loss = 0.6781598329544067
Epoch: 721, Batch Gradient Norm: 12.580067751545402
Epoch: 721, Batch Gradient Norm after: 12.580067751545402
Epoch 722/10000, Prediction Accuracy = 58.63399999999999%, Loss = 0.6805002212524414
Epoch: 722, Batch Gradient Norm: 14.9112788006958
Epoch: 722, Batch Gradient Norm after: 14.9112788006958
Epoch 723/10000, Prediction Accuracy = 58.730000000000004%, Loss = 0.6962035417556762
Epoch: 723, Batch Gradient Norm: 9.793672832207882
Epoch: 723, Batch Gradient Norm after: 9.793672832207882
Epoch 724/10000, Prediction Accuracy = 58.638%, Loss = 0.6667507529258728
Epoch: 724, Batch Gradient Norm: 10.991494494591546
Epoch: 724, Batch Gradient Norm after: 10.991494494591546
Epoch 725/10000, Prediction Accuracy = 58.712%, Loss = 0.6722232580184937
Epoch: 725, Batch Gradient Norm: 10.050038508936074
Epoch: 725, Batch Gradient Norm after: 10.050038508936074
Epoch 726/10000, Prediction Accuracy = 58.662%, Loss = 0.6675664901733398
Epoch: 726, Batch Gradient Norm: 11.23861086204192
Epoch: 726, Batch Gradient Norm after: 11.23861086204192
Epoch 727/10000, Prediction Accuracy = 58.732000000000006%, Loss = 0.6727525115013122
Epoch: 727, Batch Gradient Norm: 12.218407528256757
Epoch: 727, Batch Gradient Norm after: 12.218407528256757
Epoch 728/10000, Prediction Accuracy = 58.70399999999999%, Loss = 0.6777792692184448
Epoch: 728, Batch Gradient Norm: 17.036572004284388
Epoch 729/10000, Prediction Accuracy = 58.624%, Loss = 0.7136019349098206
Epoch: 729, Batch Gradient Norm: 13.476938715234512
Epoch: 729, Batch Gradient Norm after: 13.476938715234512
Epoch 730/10000, Prediction Accuracy = 58.717999999999996%, Loss = 0.688750970363617
Epoch: 730, Batch Gradient Norm: 10.810750605819207
Epoch: 730, Batch Gradient Norm after: 10.810750605819207
Epoch 731/10000, Prediction Accuracy = 58.678%, Loss = 0.6699499011039733
Epoch: 731, Batch Gradient Norm: 11.720332962645102
Epoch: 731, Batch Gradient Norm after: 11.720332962645102
Epoch 732/10000, Prediction Accuracy = 58.652%, Loss = 0.6752395033836365
Epoch: 732, Batch Gradient Norm: 17.958290093847616
Epoch: 732, Batch Gradient Norm after: 17.958290093847616
Epoch 733/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.7230055809020997
Epoch: 733, Batch Gradient Norm: 12.663529510712227
Epoch: 733, Batch Gradient Norm after: 12.663529510712227
Epoch 734/10000, Prediction Accuracy = 58.722%, Loss = 0.684159517288208
Epoch: 734, Batch Gradient Norm: 14.294044519754157
Epoch: 734, Batch Gradient Norm after: 14.294044519754157
Epoch 735/10000, Prediction Accuracy = 58.668000000000006%, Loss = 0.6922505021095275
Epoch: 735, Batch Gradient Norm: 8.030289871418944
Epoch: 735, Batch Gradient Norm after: 8.030289871418944
Epoch 736/10000, Prediction Accuracy = 58.774%, Loss = 0.6561336755752564
Epoch: 736, Batch Gradient Norm: 11.131711085904564
Epoch: 736, Batch Gradient Norm after: 11.131711085904564
Epoch 737/10000, Prediction Accuracy = 58.686%, Loss = 0.6699870705604554
Epoch: 737, Batch Gradient Norm: 8.742116797865746
Epoch: 737, Batch Gradient Norm after: 8.742116797865746
Epoch 738/10000, Prediction Accuracy = 58.727999999999994%, Loss = 0.658102560043335
Epoch: 738, Batch Gradient Norm: 15.816484536850941
Epoch: 738, Batch Gradient Norm after: 15.816484536850941
Epoch 739/10000, Prediction Accuracy = 58.71%, Loss = 0.7010934472084045
Epoch: 739, Batch Gradient Norm: 12.08447924422264
Epoch: 739, Batch Gradient Norm after: 12.08447924422264
Epoch 740/10000, Prediction Accuracy = 58.7%, Loss = 0.6764142513275146
Epoch: 740, Batch Gradient Norm: 14.079700359404292
Epoch: 740, Batch Gradient Norm after: 14.079700359404292
Epoch 741/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.6877612709999085
Epoch: 741, Batch Gradient Norm: 8.634291408116328
Epoch: 741, Batch Gradient Norm after: 8.634291408116328
Epoch 742/10000, Prediction Accuracy = 58.693999999999996%, Loss = 0.6579012513160706
Epoch: 742, Batch Gradient Norm: 11.491693801672833
Epoch: 742, Batch Gradient Norm after: 11.491693801672833
Epoch 743/10000, Prediction Accuracy = 58.782%, Loss = 0.6703393578529357
Epoch: 743, Batch Gradient Norm: 12.024796559095462
Epoch: 743, Batch Gradient Norm after: 12.024796559095462
Epoch 744/10000, Prediction Accuracy = 58.658%, Loss = 0.6749665379524231
Epoch: 744, Batch Gradient Norm: 14.150493515836475
Epoch: 744, Batch Gradient Norm after: 14.150493515836475
Epoch 745/10000, Prediction Accuracy = 58.82000000000001%, Loss = 0.6888544797897339
Epoch: 745, Batch Gradient Norm: 13.344368847197723
Epoch: 745, Batch Gradient Norm after: 13.344368847197723
Epoch 746/10000, Prediction Accuracy = 58.657999999999994%, Loss = 0.6836912631988525
Epoch: 746, Batch Gradient Norm: 11.974527944299334
Epoch: 746, Batch Gradient Norm after: 11.974527944299334
Epoch 747/10000, Prediction Accuracy = 58.75%, Loss = 0.673140037059784
Epoch: 747, Batch Gradient Norm: 13.363815614641522
Epoch: 747, Batch Gradient Norm after: 13.363815614641522
Epoch 748/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.681267774105072
Epoch: 748, Batch Gradient Norm: 13.925016093164226
Epoch: 748, Batch Gradient Norm after: 13.925016093164226
Epoch 749/10000, Prediction Accuracy = 58.779999999999994%, Loss = 0.6849872708320618
Epoch: 749, Batch Gradient Norm: 11.204556362801327
Epoch: 749, Batch Gradient Norm after: 11.204556362801327
Epoch 750/10000, Prediction Accuracy = 58.75%, Loss = 0.6684345483779908
Epoch: 750, Batch Gradient Norm: 11.16416996866118
Epoch: 750, Batch Gradient Norm after: 11.16416996866118
Epoch 751/10000, Prediction Accuracy = 58.775999999999996%, Loss = 0.6661696791648865
Epoch: 751, Batch Gradient Norm: 10.337347116812444
Epoch: 751, Batch Gradient Norm after: 10.337347116812444
Epoch 752/10000, Prediction Accuracy = 58.74399999999999%, Loss = 0.661829161643982
Epoch: 752, Batch Gradient Norm: 15.178519500159869
Epoch: 752, Batch Gradient Norm after: 15.178519500159869
Epoch 753/10000, Prediction Accuracy = 58.788%, Loss = 0.6918713331222535
Epoch: 753, Batch Gradient Norm: 12.084979304640429
Epoch: 753, Batch Gradient Norm after: 12.084979304640429
Epoch 754/10000, Prediction Accuracy = 58.754%, Loss = 0.6750343918800354
Epoch: 754, Batch Gradient Norm: 16.987983980474723
Epoch: 754, Batch Gradient Norm after: 16.987983980474723
Epoch 755/10000, Prediction Accuracy = 58.766000000000005%, Loss = 0.7105237245559692
Epoch: 755, Batch Gradient Norm: 11.414445367420017
Epoch: 755, Batch Gradient Norm after: 11.414445367420017
Epoch 756/10000, Prediction Accuracy = 58.774%, Loss = 0.6713464140892029
Epoch: 756, Batch Gradient Norm: 10.726190356077312
Epoch: 756, Batch Gradient Norm after: 10.726190356077312
Epoch 757/10000, Prediction Accuracy = 58.782%, Loss = 0.6667803525924683
Epoch: 757, Batch Gradient Norm: 9.29545216261302
Epoch: 757, Batch Gradient Norm after: 9.29545216261302
Epoch 758/10000, Prediction Accuracy = 58.736000000000004%, Loss = 0.6588022708892822
Epoch: 758, Batch Gradient Norm: 11.38407605386112
Epoch: 758, Batch Gradient Norm after: 11.38407605386112
Epoch 759/10000, Prediction Accuracy = 58.84000000000001%, Loss = 0.6688274025917054
Epoch: 759, Batch Gradient Norm: 12.761208310051092
Epoch: 759, Batch Gradient Norm after: 12.761208310051092
Epoch 760/10000, Prediction Accuracy = 58.694%, Loss = 0.6761490106582642
Epoch: 760, Batch Gradient Norm: 14.85365599358116
Epoch: 760, Batch Gradient Norm after: 14.85365599358116
Epoch 761/10000, Prediction Accuracy = 58.842%, Loss = 0.6906915664672851
Epoch: 761, Batch Gradient Norm: 10.464426966057092
Epoch: 761, Batch Gradient Norm after: 10.464426966057092
Epoch 762/10000, Prediction Accuracy = 58.742000000000004%, Loss = 0.6647660613059998
Epoch: 762, Batch Gradient Norm: 9.70905266785767
Epoch: 762, Batch Gradient Norm after: 9.70905266785767
Epoch 763/10000, Prediction Accuracy = 58.827999999999996%, Loss = 0.6589621305465698
Epoch: 763, Batch Gradient Norm: 9.370948581858284
Epoch: 763, Batch Gradient Norm after: 9.370948581858284
Epoch 764/10000, Prediction Accuracy = 58.822%, Loss = 0.6581040382385254
Epoch: 764, Batch Gradient Norm: 12.238589692174976
Epoch: 764, Batch Gradient Norm after: 12.238589692174976
Epoch 765/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.6714224100112915
Epoch: 765, Batch Gradient Norm: 14.253987875485512
Epoch: 765, Batch Gradient Norm after: 14.253987875485512
Epoch 766/10000, Prediction Accuracy = 58.848%, Loss = 0.6844115018844604
Epoch: 766, Batch Gradient Norm: 16.697492039290726
Epoch: 766, Batch Gradient Norm after: 16.697492039290726
Epoch 767/10000, Prediction Accuracy = 58.81999999999999%, Loss = 0.7027405500411987
Epoch: 767, Batch Gradient Norm: 10.619217455877006
Epoch: 767, Batch Gradient Norm after: 10.619217455877006
Epoch 768/10000, Prediction Accuracy = 58.852%, Loss = 0.6609666824340821
Epoch: 768, Batch Gradient Norm: 16.606885394448764
Epoch: 768, Batch Gradient Norm after: 16.606885394448764
Epoch 769/10000, Prediction Accuracy = 58.84400000000001%, Loss = 0.7006072402000427
Epoch: 769, Batch Gradient Norm: 12.277258124077163
Epoch: 769, Batch Gradient Norm after: 12.277258124077163
Epoch 770/10000, Prediction Accuracy = 58.748000000000005%, Loss = 0.672239089012146
Epoch: 770, Batch Gradient Norm: 13.336316916181376
Epoch: 770, Batch Gradient Norm after: 13.336316916181376
Epoch 771/10000, Prediction Accuracy = 58.8%, Loss = 0.6778353929519654
Epoch: 771, Batch Gradient Norm: 7.599249429796582
Epoch: 771, Batch Gradient Norm after: 7.599249429796582
Epoch 772/10000, Prediction Accuracy = 58.794000000000004%, Loss = 0.6478291749954224
Epoch: 772, Batch Gradient Norm: 10.202675643328968
Epoch: 772, Batch Gradient Norm after: 10.202675643328968
Epoch 773/10000, Prediction Accuracy = 58.824%, Loss = 0.658012330532074
Epoch: 773, Batch Gradient Norm: 11.31876740698189
Epoch: 773, Batch Gradient Norm after: 11.31876740698189
Epoch 774/10000, Prediction Accuracy = 58.791999999999994%, Loss = 0.6655686616897583
Epoch: 774, Batch Gradient Norm: 16.05133873553727
Epoch: 774, Batch Gradient Norm after: 16.05133873553727
Epoch 775/10000, Prediction Accuracy = 58.864%, Loss = 0.6999917268753052
Epoch: 775, Batch Gradient Norm: 14.805126799691552
Epoch: 775, Batch Gradient Norm after: 14.805126799691552
Epoch 776/10000, Prediction Accuracy = 58.772000000000006%, Loss = 0.691619598865509
Epoch: 776, Batch Gradient Norm: 11.719553501020245
Epoch: 776, Batch Gradient Norm after: 11.719553501020245
Epoch 777/10000, Prediction Accuracy = 58.916%, Loss = 0.6679156422615051
Epoch: 777, Batch Gradient Norm: 10.63802699245581
Epoch: 777, Batch Gradient Norm after: 10.63802699245581
Epoch 778/10000, Prediction Accuracy = 58.818%, Loss = 0.6604675650596619
Epoch: 778, Batch Gradient Norm: 10.515785413661996
Epoch: 778, Batch Gradient Norm after: 10.515785413661996
Epoch 779/10000, Prediction Accuracy = 58.88199999999999%, Loss = 0.6590774297714234
Epoch: 779, Batch Gradient Norm: 11.478292727092823
Epoch: 779, Batch Gradient Norm after: 11.478292727092823
Epoch 780/10000, Prediction Accuracy = 58.774%, Loss = 0.6642539024353027
Epoch: 780, Batch Gradient Norm: 12.948868825450742
Epoch: 780, Batch Gradient Norm after: 12.948868825450742
Epoch 781/10000, Prediction Accuracy = 58.862%, Loss = 0.6727982878684997
Epoch: 781, Batch Gradient Norm: 12.410864338071011
Epoch: 781, Batch Gradient Norm after: 12.410864338071011
Epoch 782/10000, Prediction Accuracy = 58.818%, Loss = 0.6713648080825806
Epoch: 782, Batch Gradient Norm: 13.92164019256547
Epoch: 782, Batch Gradient Norm after: 13.92164019256547
Epoch 783/10000, Prediction Accuracy = 58.874%, Loss = 0.6794322013854981
Epoch: 783, Batch Gradient Norm: 11.941545878989679
Epoch: 783, Batch Gradient Norm after: 11.941545878989679
Epoch 784/10000, Prediction Accuracy = 58.852%, Loss = 0.6672522068023682
Epoch: 784, Batch Gradient Norm: 15.226854093901126
Epoch: 784, Batch Gradient Norm after: 15.226854093901126
Epoch 785/10000, Prediction Accuracy = 58.86%, Loss = 0.6868490934371948
Epoch: 785, Batch Gradient Norm: 11.263766666369149
Epoch: 785, Batch Gradient Norm after: 11.263766666369149
Epoch 786/10000, Prediction Accuracy = 58.842000000000006%, Loss = 0.6613203406333923
Epoch: 786, Batch Gradient Norm: 13.582741124395083
Epoch: 786, Batch Gradient Norm after: 13.582741124395083
Epoch 787/10000, Prediction Accuracy = 58.91600000000001%, Loss = 0.6749942064285278
Epoch: 787, Batch Gradient Norm: 10.514517988867361
Epoch: 787, Batch Gradient Norm after: 10.514517988867361
Epoch 788/10000, Prediction Accuracy = 58.93399999999999%, Loss = 0.6574942827224731
Epoch: 788, Batch Gradient Norm: 12.174070587698981
Epoch: 788, Batch Gradient Norm after: 12.174070587698981
Epoch 789/10000, Prediction Accuracy = 58.95400000000001%, Loss = 0.6666729569435119
Epoch: 789, Batch Gradient Norm: 9.953174618666147
Epoch: 789, Batch Gradient Norm after: 9.953174618666147
Epoch 790/10000, Prediction Accuracy = 58.886%, Loss = 0.655321192741394
Epoch: 790, Batch Gradient Norm: 10.651010344949858
Epoch: 790, Batch Gradient Norm after: 10.651010344949858
Epoch 791/10000, Prediction Accuracy = 58.90400000000001%, Loss = 0.6582850575447082
Epoch: 791, Batch Gradient Norm: 10.070897308279376
Epoch: 791, Batch Gradient Norm after: 10.070897308279376
Epoch 792/10000, Prediction Accuracy = 58.89%, Loss = 0.6568099975585937
Epoch: 792, Batch Gradient Norm: 14.387506634921534
Epoch: 792, Batch Gradient Norm after: 14.387506634921534
Epoch 793/10000, Prediction Accuracy = 58.928%, Loss = 0.6820385336875916
Epoch: 793, Batch Gradient Norm: 14.652131221764161
Epoch: 793, Batch Gradient Norm after: 14.652131221764161
Epoch 794/10000, Prediction Accuracy = 58.85%, Loss = 0.6854891538619995
Epoch: 794, Batch Gradient Norm: 15.114100425685091
Epoch: 794, Batch Gradient Norm after: 15.114100425685091
Epoch 795/10000, Prediction Accuracy = 58.92999999999999%, Loss = 0.6875082015991211
Epoch: 795, Batch Gradient Norm: 9.99275951488485
Epoch: 795, Batch Gradient Norm after: 9.99275951488485
Epoch 796/10000, Prediction Accuracy = 58.864%, Loss = 0.6549444198608398
Epoch: 796, Batch Gradient Norm: 12.14822828859924
Epoch: 796, Batch Gradient Norm after: 12.14822828859924
Epoch 797/10000, Prediction Accuracy = 58.938%, Loss = 0.6674281120300293
Epoch: 797, Batch Gradient Norm: 13.790249384473286
Epoch: 797, Batch Gradient Norm after: 13.790249384473286
Epoch 798/10000, Prediction Accuracy = 58.902%, Loss = 0.6787010908126831
Epoch: 798, Batch Gradient Norm: 13.144341887027458
Epoch: 798, Batch Gradient Norm after: 13.144341887027458
Epoch 799/10000, Prediction Accuracy = 58.976%, Loss = 0.6713476300239563
Epoch: 799, Batch Gradient Norm: 14.416732049132232
Epoch: 799, Batch Gradient Norm after: 14.416732049132232
Epoch 800/10000, Prediction Accuracy = 58.818000000000005%, Loss = 0.6796428084373474
Epoch: 800, Batch Gradient Norm: 12.463024632915522
Epoch: 800, Batch Gradient Norm after: 12.463024632915522
Epoch 801/10000, Prediction Accuracy = 58.92%, Loss = 0.6661411762237549
Epoch: 801, Batch Gradient Norm: 11.352177034282885
Epoch: 801, Batch Gradient Norm after: 11.352177034282885
Epoch 802/10000, Prediction Accuracy = 58.876%, Loss = 0.6599457740783692
Epoch: 802, Batch Gradient Norm: 11.526424984272431
Epoch: 802, Batch Gradient Norm after: 11.526424984272431
Epoch 803/10000, Prediction Accuracy = 59.028%, Loss = 0.6595678687095642
Epoch: 803, Batch Gradient Norm: 10.458969508199566
Epoch: 803, Batch Gradient Norm after: 10.458969508199566
Epoch 804/10000, Prediction Accuracy = 58.908%, Loss = 0.6539371132850647
Epoch: 804, Batch Gradient Norm: 14.531135111828572
Epoch: 804, Batch Gradient Norm after: 14.531135111828572
Epoch 805/10000, Prediction Accuracy = 59.012%, Loss = 0.6787929773330689
Epoch: 805, Batch Gradient Norm: 11.816760941016911
Epoch: 805, Batch Gradient Norm after: 11.816760941016911
Epoch 806/10000, Prediction Accuracy = 58.94199999999999%, Loss = 0.6628127932548523
Epoch: 806, Batch Gradient Norm: 13.713232100011986
Epoch: 806, Batch Gradient Norm after: 13.713232100011986
Epoch 807/10000, Prediction Accuracy = 58.912%, Loss = 0.6739735841751099
Epoch: 807, Batch Gradient Norm: 8.416427674811588
Epoch: 807, Batch Gradient Norm after: 8.416427674811588
Epoch 808/10000, Prediction Accuracy = 58.96%, Loss = 0.6445723295211792
Epoch: 808, Batch Gradient Norm: 10.955268860492827
Epoch: 808, Batch Gradient Norm after: 10.955268860492827
Epoch 809/10000, Prediction Accuracy = 59.017999999999994%, Loss = 0.6552407741546631
Epoch: 809, Batch Gradient Norm: 10.56080432588966
Epoch: 809, Batch Gradient Norm after: 10.56080432588966
Epoch 810/10000, Prediction Accuracy = 58.912%, Loss = 0.6537430047988891
Epoch: 810, Batch Gradient Norm: 16.29812326955166
Epoch: 810, Batch Gradient Norm after: 16.29812326955166
Epoch 811/10000, Prediction Accuracy = 58.95%, Loss = 0.6936544060707093
Epoch: 811, Batch Gradient Norm: 12.132580871579767
Epoch: 811, Batch Gradient Norm after: 12.132580871579767
Epoch 812/10000, Prediction Accuracy = 58.955999999999996%, Loss = 0.6652836322784423
Epoch: 812, Batch Gradient Norm: 13.479495309196906
Epoch: 812, Batch Gradient Norm after: 13.479495309196906
Epoch 813/10000, Prediction Accuracy = 59.004000000000005%, Loss = 0.6718155860900878
Epoch: 813, Batch Gradient Norm: 11.912018020955543
Epoch: 813, Batch Gradient Norm after: 11.912018020955543
Epoch 814/10000, Prediction Accuracy = 59.001999999999995%, Loss = 0.6604166507720948
Epoch: 814, Batch Gradient Norm: 14.319032567649081
Epoch: 814, Batch Gradient Norm after: 14.319032567649081
Epoch 815/10000, Prediction Accuracy = 58.996%, Loss = 0.6758222699165344
Epoch: 815, Batch Gradient Norm: 11.483570737139962
Epoch: 815, Batch Gradient Norm after: 11.483570737139962
Epoch 816/10000, Prediction Accuracy = 59.072%, Loss = 0.6573860883712769
Epoch: 816, Batch Gradient Norm: 11.069243626306656
Epoch: 816, Batch Gradient Norm after: 11.069243626306656
Epoch 817/10000, Prediction Accuracy = 58.964%, Loss = 0.6539293169975281
Epoch: 817, Batch Gradient Norm: 14.06929669553006
Epoch: 817, Batch Gradient Norm after: 14.06929669553006
Epoch 818/10000, Prediction Accuracy = 59.040000000000006%, Loss = 0.6719607591629029
Epoch: 818, Batch Gradient Norm: 12.312526868469394
Epoch: 818, Batch Gradient Norm after: 12.312526868469394
Epoch 819/10000, Prediction Accuracy = 58.972%, Loss = 0.6628906488418579
Epoch: 819, Batch Gradient Norm: 12.704742183769188
Epoch: 819, Batch Gradient Norm after: 12.704742183769188
Epoch 820/10000, Prediction Accuracy = 59.064%, Loss = 0.6666061043739319
Epoch: 820, Batch Gradient Norm: 13.284334221844576
Epoch: 820, Batch Gradient Norm after: 13.284334221844576
Epoch 821/10000, Prediction Accuracy = 58.93000000000001%, Loss = 0.6724841237068176
Epoch: 821, Batch Gradient Norm: 10.14898602836332
Epoch: 821, Batch Gradient Norm after: 10.14898602836332
Epoch 822/10000, Prediction Accuracy = 59.032000000000004%, Loss = 0.6522141575813294
Epoch: 822, Batch Gradient Norm: 8.958347411365386
Epoch: 822, Batch Gradient Norm after: 8.958347411365386
Epoch 823/10000, Prediction Accuracy = 59.038%, Loss = 0.6455495953559875
Epoch: 823, Batch Gradient Norm: 6.978877071701431
Epoch: 823, Batch Gradient Norm after: 6.978877071701431
Epoch 824/10000, Prediction Accuracy = 59.034000000000006%, Loss = 0.6361944079399109
Epoch: 824, Batch Gradient Norm: 12.763764885482074
Epoch: 824, Batch Gradient Norm after: 12.763764885482074
Epoch 825/10000, Prediction Accuracy = 59.077999999999996%, Loss = 0.6653897762298584
Epoch: 825, Batch Gradient Norm: 16.75814871310801
Epoch: 825, Batch Gradient Norm after: 16.75814871310801
Epoch 826/10000, Prediction Accuracy = 59.008%, Loss = 0.6956455826759338
Epoch: 826, Batch Gradient Norm: 15.987417921450833
Epoch: 826, Batch Gradient Norm after: 15.987417921450833
Epoch 827/10000, Prediction Accuracy = 59.05800000000001%, Loss = 0.6904485106468201
Epoch: 827, Batch Gradient Norm: 9.630295015868983
Epoch: 827, Batch Gradient Norm after: 9.630295015868983
Epoch 828/10000, Prediction Accuracy = 59.036%, Loss = 0.6471042990684509
Epoch: 828, Batch Gradient Norm: 12.302640715907733
Epoch: 828, Batch Gradient Norm after: 12.302640715907733
Epoch 829/10000, Prediction Accuracy = 58.96999999999999%, Loss = 0.6603337526321411
Epoch: 829, Batch Gradient Norm: 9.886191709396575
Epoch: 829, Batch Gradient Norm after: 9.886191709396575
Epoch 830/10000, Prediction Accuracy = 59.056%, Loss = 0.6474258422851562
Epoch: 830, Batch Gradient Norm: 11.849846089265933
Epoch: 830, Batch Gradient Norm after: 11.849846089265933
Epoch 831/10000, Prediction Accuracy = 58.98599999999999%, Loss = 0.6586493968963623
Epoch: 831, Batch Gradient Norm: 9.576972396624377
Epoch: 831, Batch Gradient Norm after: 9.576972396624377
Epoch 832/10000, Prediction Accuracy = 59.108000000000004%, Loss = 0.6453866004943848
Epoch: 832, Batch Gradient Norm: 13.038498209344976
Epoch: 832, Batch Gradient Norm after: 13.038498209344976
Epoch 833/10000, Prediction Accuracy = 59.08%, Loss = 0.6640231013298035
Epoch: 833, Batch Gradient Norm: 15.939963973023852
Epoch: 833, Batch Gradient Norm after: 15.939963973023852
Epoch 834/10000, Prediction Accuracy = 59.08%, Loss = 0.6841299653053283
Epoch: 834, Batch Gradient Norm: 13.295528252312456
Epoch: 834, Batch Gradient Norm after: 13.295528252312456
Epoch 835/10000, Prediction Accuracy = 59.010000000000005%, Loss = 0.6673292994499207
Epoch: 835, Batch Gradient Norm: 14.314759286896017
Epoch: 835, Batch Gradient Norm after: 14.314759286896017
Epoch 836/10000, Prediction Accuracy = 59.010000000000005%, Loss = 0.6740041255950928
Epoch: 836, Batch Gradient Norm: 12.24640105928861
Epoch: 836, Batch Gradient Norm after: 12.24640105928861
Epoch 837/10000, Prediction Accuracy = 59.074%, Loss = 0.6599799275398255
Epoch: 837, Batch Gradient Norm: 9.057625196469662
Epoch: 837, Batch Gradient Norm after: 9.057625196469662
Epoch 838/10000, Prediction Accuracy = 59.024%, Loss = 0.6420485258102417
Epoch: 838, Batch Gradient Norm: 8.343082878011296
Epoch: 838, Batch Gradient Norm after: 8.343082878011296
Epoch 839/10000, Prediction Accuracy = 59.096000000000004%, Loss = 0.6375579118728638
Epoch: 839, Batch Gradient Norm: 8.917894881610057
Epoch: 839, Batch Gradient Norm after: 8.917894881610057
Epoch 840/10000, Prediction Accuracy = 59.11%, Loss = 0.6393033981323242
Epoch: 840, Batch Gradient Norm: 17.47457691078876
Epoch: 840, Batch Gradient Norm after: 17.47457691078876
Epoch 841/10000, Prediction Accuracy = 59.08599999999999%, Loss = 0.6947003841400147
Epoch: 841, Batch Gradient Norm: 13.638410432700802
Epoch: 841, Batch Gradient Norm after: 13.638410432700802
Epoch 842/10000, Prediction Accuracy = 59.05999999999999%, Loss = 0.6710267424583435
Epoch: 842, Batch Gradient Norm: 16.387838880206257
Epoch: 842, Batch Gradient Norm after: 16.387838880206257
Epoch 843/10000, Prediction Accuracy = 59.028%, Loss = 0.6934572339057923
Epoch: 843, Batch Gradient Norm: 11.015636278971053
Epoch: 843, Batch Gradient Norm after: 11.015636278971053
Epoch 844/10000, Prediction Accuracy = 59.09400000000001%, Loss = 0.6550451517105103
Epoch: 844, Batch Gradient Norm: 8.390929325421613
Epoch: 844, Batch Gradient Norm after: 8.390929325421613
Epoch 845/10000, Prediction Accuracy = 59.098%, Loss = 0.638727343082428
Epoch: 845, Batch Gradient Norm: 8.386470321055157
Epoch: 845, Batch Gradient Norm after: 8.386470321055157
Epoch 846/10000, Prediction Accuracy = 59.081999999999994%, Loss = 0.6375617384910583
Epoch: 846, Batch Gradient Norm: 12.328992685897465
Epoch: 846, Batch Gradient Norm after: 12.328992685897465
Epoch 847/10000, Prediction Accuracy = 59.126%, Loss = 0.6586448192596436
Epoch: 847, Batch Gradient Norm: 15.238545114668204
Epoch: 847, Batch Gradient Norm after: 15.238545114668204
Epoch 848/10000, Prediction Accuracy = 59.053999999999995%, Loss = 0.6793675899505616
Epoch: 848, Batch Gradient Norm: 13.758947858937628
Epoch: 848, Batch Gradient Norm after: 13.758947858937628
Epoch 849/10000, Prediction Accuracy = 59.088%, Loss = 0.6673497200012207
Epoch: 849, Batch Gradient Norm: 11.219080807839035
Epoch: 849, Batch Gradient Norm after: 11.219080807839035
Epoch 850/10000, Prediction Accuracy = 59.089999999999996%, Loss = 0.6514601469039917
Epoch: 850, Batch Gradient Norm: 12.120023153226926
Epoch: 850, Batch Gradient Norm after: 12.120023153226926
Epoch 851/10000, Prediction Accuracy = 59.11%, Loss = 0.6559523463249206
Epoch: 851, Batch Gradient Norm: 11.300823255743829
Epoch: 851, Batch Gradient Norm after: 11.300823255743829
Epoch 852/10000, Prediction Accuracy = 59.10999999999999%, Loss = 0.6522735238075257
Epoch: 852, Batch Gradient Norm: 11.830787539735685
Epoch: 852, Batch Gradient Norm after: 11.830787539735685
Epoch 853/10000, Prediction Accuracy = 59.13199999999999%, Loss = 0.6535011410713196
Epoch: 853, Batch Gradient Norm: 11.27232121165243
Epoch: 853, Batch Gradient Norm after: 11.27232121165243
Epoch 854/10000, Prediction Accuracy = 59.14%, Loss = 0.6503120422363281
Epoch: 854, Batch Gradient Norm: 14.456194969343958
Epoch: 854, Batch Gradient Norm after: 14.456194969343958
Epoch 855/10000, Prediction Accuracy = 59.138%, Loss = 0.6712410807609558
Epoch: 855, Batch Gradient Norm: 10.922545271602297
Epoch: 855, Batch Gradient Norm after: 10.922545271602297
Epoch 856/10000, Prediction Accuracy = 59.141999999999996%, Loss = 0.64965318441391
Epoch: 856, Batch Gradient Norm: 12.759852232282388
Epoch: 856, Batch Gradient Norm after: 12.759852232282388
Epoch 857/10000, Prediction Accuracy = 59.1%, Loss = 0.6601971387863159
Epoch: 857, Batch Gradient Norm: 10.541143603004052
Epoch: 857, Batch Gradient Norm after: 10.541143603004052
Epoch 858/10000, Prediction Accuracy = 59.144000000000005%, Loss = 0.6460257172584534
Epoch: 858, Batch Gradient Norm: 16.72740976997589
Epoch: 858, Batch Gradient Norm after: 16.72740976997589
Epoch 859/10000, Prediction Accuracy = 59.098%, Loss = 0.6870556116104126
Epoch: 859, Batch Gradient Norm: 13.584277199714386
Epoch: 859, Batch Gradient Norm after: 13.584277199714386
Epoch 860/10000, Prediction Accuracy = 59.176%, Loss = 0.6630675911903381
Epoch: 860, Batch Gradient Norm: 13.155787229377204
Epoch: 860, Batch Gradient Norm after: 13.155787229377204
Epoch 861/10000, Prediction Accuracy = 59.15%, Loss = 0.6610746383666992
Epoch: 861, Batch Gradient Norm: 8.203034403336433
Epoch: 861, Batch Gradient Norm after: 8.203034403336433
Epoch 862/10000, Prediction Accuracy = 59.186%, Loss = 0.6352065801620483
Epoch: 862, Batch Gradient Norm: 10.895775806872596
Epoch: 862, Batch Gradient Norm after: 10.895775806872596
Epoch 863/10000, Prediction Accuracy = 59.138%, Loss = 0.6491979241371155
Epoch: 863, Batch Gradient Norm: 10.286606211516228
Epoch: 863, Batch Gradient Norm after: 10.286606211516228
Epoch 864/10000, Prediction Accuracy = 59.227999999999994%, Loss = 0.6462335705757141
Epoch: 864, Batch Gradient Norm: 12.371972643741662
Epoch: 864, Batch Gradient Norm after: 12.371972643741662
Epoch 865/10000, Prediction Accuracy = 59.126%, Loss = 0.6577605962753296
Epoch: 865, Batch Gradient Norm: 11.019390381118662
Epoch: 865, Batch Gradient Norm after: 11.019390381118662
Epoch 866/10000, Prediction Accuracy = 59.27%, Loss = 0.6498829007148743
Epoch: 866, Batch Gradient Norm: 11.270847347648278
Epoch: 866, Batch Gradient Norm after: 11.270847347648278
Epoch 867/10000, Prediction Accuracy = 59.19200000000001%, Loss = 0.6492161750793457
Epoch: 867, Batch Gradient Norm: 15.314147498802342
Epoch: 867, Batch Gradient Norm after: 15.314147498802342
Epoch 868/10000, Prediction Accuracy = 59.120000000000005%, Loss = 0.6749606609344483
Epoch: 868, Batch Gradient Norm: 15.00127967399008
Epoch: 868, Batch Gradient Norm after: 15.00127967399008
Epoch 869/10000, Prediction Accuracy = 59.176%, Loss = 0.6773587703704834
Epoch: 869, Batch Gradient Norm: 10.077182634753699
Epoch: 869, Batch Gradient Norm after: 10.077182634753699
Epoch 870/10000, Prediction Accuracy = 59.14200000000001%, Loss = 0.6426790833473206
Epoch: 870, Batch Gradient Norm: 10.583635856918088
Epoch: 870, Batch Gradient Norm after: 10.583635856918088
Epoch 871/10000, Prediction Accuracy = 59.205999999999996%, Loss = 0.6443342447280884
Epoch: 871, Batch Gradient Norm: 13.826428946259503
Epoch: 871, Batch Gradient Norm after: 13.826428946259503
Epoch 872/10000, Prediction Accuracy = 59.146%, Loss = 0.663650119304657
Epoch: 872, Batch Gradient Norm: 11.955679879846684
Epoch: 872, Batch Gradient Norm after: 11.955679879846684
Epoch 873/10000, Prediction Accuracy = 59.214%, Loss = 0.6514928340911865
Epoch: 873, Batch Gradient Norm: 13.53336230240258
Epoch: 873, Batch Gradient Norm after: 13.53336230240258
Epoch 874/10000, Prediction Accuracy = 59.208000000000006%, Loss = 0.6608938574790955
Epoch: 874, Batch Gradient Norm: 14.868230281002658
Epoch: 874, Batch Gradient Norm after: 14.868230281002658
Epoch 875/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.6701924085617066
Epoch: 875, Batch Gradient Norm: 12.87019933880028
Epoch: 875, Batch Gradient Norm after: 12.87019933880028
Epoch 876/10000, Prediction Accuracy = 59.186%, Loss = 0.6571532845497131
Epoch: 876, Batch Gradient Norm: 12.945459988589162
Epoch: 876, Batch Gradient Norm after: 12.945459988589162
Epoch 877/10000, Prediction Accuracy = 59.181999999999995%, Loss = 0.6566827416419982
Epoch: 877, Batch Gradient Norm: 9.078008903581976
Epoch: 877, Batch Gradient Norm after: 9.078008903581976
Epoch 878/10000, Prediction Accuracy = 59.2%, Loss = 0.6356322050094605
Epoch: 878, Batch Gradient Norm: 11.790587226517744
Epoch: 878, Batch Gradient Norm after: 11.790587226517744
Epoch 879/10000, Prediction Accuracy = 59.182%, Loss = 0.6489736318588257
Epoch: 879, Batch Gradient Norm: 10.531069492526832
Epoch: 879, Batch Gradient Norm after: 10.531069492526832
Epoch 880/10000, Prediction Accuracy = 59.24000000000001%, Loss = 0.642432713508606
Epoch: 880, Batch Gradient Norm: 13.769845454454927
Epoch: 880, Batch Gradient Norm after: 13.769845454454927
Epoch 881/10000, Prediction Accuracy = 59.23199999999999%, Loss = 0.6623768329620361
Epoch: 881, Batch Gradient Norm: 12.979989520749857
Epoch: 881, Batch Gradient Norm after: 12.979989520749857
Epoch 882/10000, Prediction Accuracy = 59.215999999999994%, Loss = 0.658497440814972
Epoch: 882, Batch Gradient Norm: 11.92412598810529
Epoch: 882, Batch Gradient Norm after: 11.92412598810529
Epoch 883/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.6516104698181152
Epoch: 883, Batch Gradient Norm: 10.470487026862095
Epoch: 883, Batch Gradient Norm after: 10.470487026862095
Epoch 884/10000, Prediction Accuracy = 59.2%, Loss = 0.6431722402572632
Epoch: 884, Batch Gradient Norm: 9.320273123150065
Epoch: 884, Batch Gradient Norm after: 9.320273123150065
Epoch 885/10000, Prediction Accuracy = 59.274%, Loss = 0.6361138939857482
Epoch: 885, Batch Gradient Norm: 13.233195132778791
Epoch: 885, Batch Gradient Norm after: 13.233195132778791
Epoch 886/10000, Prediction Accuracy = 59.2%, Loss = 0.6588692665100098
Epoch: 886, Batch Gradient Norm: 14.118917577292832
Epoch: 886, Batch Gradient Norm after: 14.118917577292832
Epoch 887/10000, Prediction Accuracy = 59.224000000000004%, Loss = 0.6627147912979126
Epoch: 887, Batch Gradient Norm: 12.907565549648984
Epoch: 887, Batch Gradient Norm after: 12.907565549648984
Epoch 888/10000, Prediction Accuracy = 59.214%, Loss = 0.655265212059021
Epoch: 888, Batch Gradient Norm: 12.280836444798725
Epoch: 888, Batch Gradient Norm after: 12.280836444798725
Epoch 889/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.6510362982749939
Epoch: 889, Batch Gradient Norm: 11.770521838858171
Epoch: 889, Batch Gradient Norm after: 11.770521838858171
Epoch 890/10000, Prediction Accuracy = 59.24400000000001%, Loss = 0.6497928500175476
Epoch: 890, Batch Gradient Norm: 14.781697878385314
Epoch: 890, Batch Gradient Norm after: 14.781697878385314
Epoch 891/10000, Prediction Accuracy = 59.21%, Loss = 0.6703809022903442
Epoch: 891, Batch Gradient Norm: 13.069538248286129
Epoch: 891, Batch Gradient Norm after: 13.069538248286129
Epoch 892/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.657613468170166
Epoch: 892, Batch Gradient Norm: 13.533943525993516
Epoch: 892, Batch Gradient Norm after: 13.533943525993516
Epoch 893/10000, Prediction Accuracy = 59.242%, Loss = 0.6582372546195984
Epoch: 893, Batch Gradient Norm: 10.802606697651889
Epoch: 893, Batch Gradient Norm after: 10.802606697651889
Epoch 894/10000, Prediction Accuracy = 59.3%, Loss = 0.6417932033538818
Epoch: 894, Batch Gradient Norm: 13.426017679396015
Epoch: 894, Batch Gradient Norm after: 13.426017679396015
Epoch 895/10000, Prediction Accuracy = 59.196000000000005%, Loss = 0.6567985057830811
Epoch: 895, Batch Gradient Norm: 12.673740436080623
Epoch: 895, Batch Gradient Norm after: 12.673740436080623
Epoch 896/10000, Prediction Accuracy = 59.31%, Loss = 0.6517189025878907
Epoch: 896, Batch Gradient Norm: 15.08311691625331
Epoch: 896, Batch Gradient Norm after: 15.08311691625331
Epoch 897/10000, Prediction Accuracy = 59.166%, Loss = 0.6700739145278931
Epoch: 897, Batch Gradient Norm: 8.657566736498332
Epoch: 897, Batch Gradient Norm after: 8.657566736498332
Epoch 898/10000, Prediction Accuracy = 59.234%, Loss = 0.6311483025550843
Epoch: 898, Batch Gradient Norm: 9.4947355002117
Epoch: 898, Batch Gradient Norm after: 9.4947355002117
Epoch 899/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.6340657711029053
Epoch: 899, Batch Gradient Norm: 8.869852271225849
Epoch: 899, Batch Gradient Norm after: 8.869852271225849
Epoch 900/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.6306588053703308
Epoch: 900, Batch Gradient Norm: 12.833216474069138
Epoch: 900, Batch Gradient Norm after: 12.833216474069138
Epoch 901/10000, Prediction Accuracy = 59.279999999999994%, Loss = 0.6553598880767822
Epoch: 901, Batch Gradient Norm: 13.539394423667877
Epoch: 901, Batch Gradient Norm after: 13.539394423667877
Epoch 902/10000, Prediction Accuracy = 59.226%, Loss = 0.6623505473136901
Epoch: 902, Batch Gradient Norm: 10.712842213102379
Epoch: 902, Batch Gradient Norm after: 10.712842213102379
Epoch 903/10000, Prediction Accuracy = 59.334%, Loss = 0.6425710916519165
Epoch: 903, Batch Gradient Norm: 10.68521960887524
Epoch: 903, Batch Gradient Norm after: 10.68521960887524
Epoch 904/10000, Prediction Accuracy = 59.254%, Loss = 0.639609980583191
Epoch: 904, Batch Gradient Norm: 11.916704078308793
Epoch: 904, Batch Gradient Norm after: 11.916704078308793
Epoch 905/10000, Prediction Accuracy = 59.260000000000005%, Loss = 0.6447737216949463
Epoch: 905, Batch Gradient Norm: 16.26405513357498
Epoch: 905, Batch Gradient Norm after: 16.26405513357498
Epoch 906/10000, Prediction Accuracy = 59.174%, Loss = 0.6771196722984314
Epoch: 906, Batch Gradient Norm: 13.96064544784341
Epoch: 906, Batch Gradient Norm after: 13.96064544784341
Epoch 907/10000, Prediction Accuracy = 59.274%, Loss = 0.6613433122634887
Epoch: 907, Batch Gradient Norm: 8.760361336508629
Epoch: 907, Batch Gradient Norm after: 8.760361336508629
Epoch 908/10000, Prediction Accuracy = 59.275999999999996%, Loss = 0.6307934165000916
Epoch: 908, Batch Gradient Norm: 9.879660445891606
Epoch: 908, Batch Gradient Norm after: 9.879660445891606
Epoch 909/10000, Prediction Accuracy = 59.36%, Loss = 0.6354166269302368
Epoch: 909, Batch Gradient Norm: 12.586040164732724
Epoch: 909, Batch Gradient Norm after: 12.586040164732724
Epoch 910/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.6517581224441529
Epoch: 910, Batch Gradient Norm: 16.35167215755656
Epoch: 910, Batch Gradient Norm after: 16.35167215755656
Epoch 911/10000, Prediction Accuracy = 59.318000000000005%, Loss = 0.6821295261383057
Epoch: 911, Batch Gradient Norm: 12.16150859893704
Epoch: 911, Batch Gradient Norm after: 12.16150859893704
Epoch 912/10000, Prediction Accuracy = 59.236000000000004%, Loss = 0.6492436528205872
Epoch: 912, Batch Gradient Norm: 9.28198041253913
Epoch: 912, Batch Gradient Norm after: 9.28198041253913
Epoch 913/10000, Prediction Accuracy = 59.346000000000004%, Loss = 0.6309366822242737
Epoch: 913, Batch Gradient Norm: 8.975869788859939
Epoch: 913, Batch Gradient Norm after: 8.975869788859939
Epoch 914/10000, Prediction Accuracy = 59.266%, Loss = 0.6291789174079895
Epoch: 914, Batch Gradient Norm: 11.666376470405863
Epoch: 914, Batch Gradient Norm after: 11.666376470405863
Epoch 915/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.6419539451599121
Epoch: 915, Batch Gradient Norm: 13.894195697207389
Epoch: 915, Batch Gradient Norm after: 13.894195697207389
Epoch 916/10000, Prediction Accuracy = 59.29200000000001%, Loss = 0.6579180717468261
Epoch: 916, Batch Gradient Norm: 14.583649481980306
Epoch: 916, Batch Gradient Norm after: 14.583649481980306
Epoch 917/10000, Prediction Accuracy = 59.262%, Loss = 0.6636523842811585
Epoch: 917, Batch Gradient Norm: 14.097086479515486
Epoch: 917, Batch Gradient Norm after: 14.097086479515486
Epoch 918/10000, Prediction Accuracy = 59.274%, Loss = 0.6580963134765625
Epoch: 918, Batch Gradient Norm: 16.026503712993375
Epoch: 918, Batch Gradient Norm after: 16.026503712993375
Epoch 919/10000, Prediction Accuracy = 59.220000000000006%, Loss = 0.6765470862388611
Epoch: 919, Batch Gradient Norm: 9.170747151514767
Epoch: 919, Batch Gradient Norm after: 9.170747151514767
Epoch 920/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.630502462387085
Epoch: 920, Batch Gradient Norm: 14.003402840270626
Epoch: 920, Batch Gradient Norm after: 14.003402840270626
Epoch 921/10000, Prediction Accuracy = 59.25%, Loss = 0.6582802653312683
Epoch: 921, Batch Gradient Norm: 11.435565056372038
Epoch: 921, Batch Gradient Norm after: 11.435565056372038
Epoch 922/10000, Prediction Accuracy = 59.354%, Loss = 0.642457926273346
Epoch: 922, Batch Gradient Norm: 10.469123168951947
Epoch: 922, Batch Gradient Norm after: 10.469123168951947
Epoch 923/10000, Prediction Accuracy = 59.315999999999995%, Loss = 0.6356207370758057
Epoch: 923, Batch Gradient Norm: 7.241355939366069
Epoch: 923, Batch Gradient Norm after: 7.241355939366069
Epoch 924/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.6207043886184692
Epoch: 924, Batch Gradient Norm: 11.0820565999385
Epoch: 924, Batch Gradient Norm after: 11.0820565999385
Epoch 925/10000, Prediction Accuracy = 59.286%, Loss = 0.6372354030609131
Epoch: 925, Batch Gradient Norm: 12.531463052222659
Epoch: 925, Batch Gradient Norm after: 12.531463052222659
Epoch 926/10000, Prediction Accuracy = 59.248000000000005%, Loss = 0.647347915172577
Epoch: 926, Batch Gradient Norm: 13.782903301722012
Epoch: 926, Batch Gradient Norm after: 13.782903301722012
Epoch 927/10000, Prediction Accuracy = 59.348%, Loss = 0.6567921757698059
Epoch: 927, Batch Gradient Norm: 15.26992197403243
Epoch: 927, Batch Gradient Norm after: 15.26992197403243
Epoch 928/10000, Prediction Accuracy = 59.29200000000001%, Loss = 0.669429361820221
Epoch: 928, Batch Gradient Norm: 11.861107355778513
Epoch: 928, Batch Gradient Norm after: 11.861107355778513
Epoch 929/10000, Prediction Accuracy = 59.374%, Loss = 0.6452640891075134
Epoch: 929, Batch Gradient Norm: 10.894040762034463
Epoch: 929, Batch Gradient Norm after: 10.894040762034463
Epoch 930/10000, Prediction Accuracy = 59.354%, Loss = 0.6383743047714233
Epoch: 930, Batch Gradient Norm: 9.459416429891686
Epoch: 930, Batch Gradient Norm after: 9.459416429891686
Epoch 931/10000, Prediction Accuracy = 59.33399999999999%, Loss = 0.6305440306663513
Epoch: 931, Batch Gradient Norm: 10.795368682349265
Epoch: 931, Batch Gradient Norm after: 10.795368682349265
Epoch 932/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.6359092950820923
Epoch: 932, Batch Gradient Norm: 15.289869765110087
Epoch: 932, Batch Gradient Norm after: 15.289869765110087
Epoch 933/10000, Prediction Accuracy = 59.238%, Loss = 0.6663388133049011
Epoch: 933, Batch Gradient Norm: 12.918311457941222
Epoch: 933, Batch Gradient Norm after: 12.918311457941222
Epoch 934/10000, Prediction Accuracy = 59.372%, Loss = 0.651350736618042
Epoch: 934, Batch Gradient Norm: 11.672827980535544
Epoch: 934, Batch Gradient Norm after: 11.672827980535544
Epoch 935/10000, Prediction Accuracy = 59.294000000000004%, Loss = 0.6417981863021851
Epoch: 935, Batch Gradient Norm: 9.347316256017095
Epoch: 935, Batch Gradient Norm after: 9.347316256017095
Epoch 936/10000, Prediction Accuracy = 59.366%, Loss = 0.6289786696434021
Epoch: 936, Batch Gradient Norm: 12.919110968284388
Epoch: 936, Batch Gradient Norm after: 12.919110968284388
Epoch 937/10000, Prediction Accuracy = 59.33599999999999%, Loss = 0.6479473948478699
Epoch: 937, Batch Gradient Norm: 13.727160135674746
Epoch: 937, Batch Gradient Norm after: 13.727160135674746
Epoch 938/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.6542641997337342
Epoch: 938, Batch Gradient Norm: 13.340814576319739
Epoch: 938, Batch Gradient Norm after: 13.340814576319739
Epoch 939/10000, Prediction Accuracy = 59.354%, Loss = 0.6506121754646301
Epoch: 939, Batch Gradient Norm: 10.43642052297644
Epoch: 939, Batch Gradient Norm after: 10.43642052297644
Epoch 940/10000, Prediction Accuracy = 59.382000000000005%, Loss = 0.6327671051025391
Epoch: 940, Batch Gradient Norm: 14.471839428946248
Epoch: 940, Batch Gradient Norm after: 14.471839428946248
Epoch 941/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.6583146572113037
Epoch: 941, Batch Gradient Norm: 13.751112757784995
Epoch: 941, Batch Gradient Norm after: 13.751112757784995
Epoch 942/10000, Prediction Accuracy = 59.326%, Loss = 0.6532040476799011
Epoch: 942, Batch Gradient Norm: 13.810187678873364
Epoch: 942, Batch Gradient Norm after: 13.810187678873364
Epoch 943/10000, Prediction Accuracy = 59.29600000000001%, Loss = 0.6548207879066468
Epoch: 943, Batch Gradient Norm: 9.05252648771514
Epoch: 943, Batch Gradient Norm after: 9.05252648771514
Epoch 944/10000, Prediction Accuracy = 59.32000000000001%, Loss = 0.6261576533317565
Epoch: 944, Batch Gradient Norm: 10.251157279058301
Epoch: 944, Batch Gradient Norm after: 10.251157279058301
Epoch 945/10000, Prediction Accuracy = 59.384%, Loss = 0.6318225741386414
Epoch: 945, Batch Gradient Norm: 11.623622793321472
Epoch: 945, Batch Gradient Norm after: 11.623622793321472
Epoch 946/10000, Prediction Accuracy = 59.334%, Loss = 0.6392547726631165
Epoch: 946, Batch Gradient Norm: 13.669121897190216
Epoch: 946, Batch Gradient Norm after: 13.669121897190216
Epoch 947/10000, Prediction Accuracy = 59.379999999999995%, Loss = 0.6536679744720459
Epoch: 947, Batch Gradient Norm: 14.193202601291278
Epoch: 947, Batch Gradient Norm after: 14.193202601291278
Epoch 948/10000, Prediction Accuracy = 59.31600000000001%, Loss = 0.6570270657539368
Epoch: 948, Batch Gradient Norm: 11.543738399370639
Epoch: 948, Batch Gradient Norm after: 11.543738399370639
Epoch 949/10000, Prediction Accuracy = 59.372%, Loss = 0.6394109606742859
Epoch: 949, Batch Gradient Norm: 12.813520906131766
Epoch: 949, Batch Gradient Norm after: 12.813520906131766
Epoch 950/10000, Prediction Accuracy = 59.348%, Loss = 0.6473195314407348
Epoch: 950, Batch Gradient Norm: 11.24274563914406
Epoch: 950, Batch Gradient Norm after: 11.24274563914406
Epoch 951/10000, Prediction Accuracy = 59.364%, Loss = 0.6377435326576233
Epoch: 951, Batch Gradient Norm: 13.02898682001831
Epoch: 951, Batch Gradient Norm after: 13.02898682001831
Epoch 952/10000, Prediction Accuracy = 59.36999999999999%, Loss = 0.6486328482627869
Epoch: 952, Batch Gradient Norm: 10.861999996969397
Epoch: 952, Batch Gradient Norm after: 10.861999996969397
Epoch 953/10000, Prediction Accuracy = 59.30400000000001%, Loss = 0.6333691716194153
Epoch: 953, Batch Gradient Norm: 12.253088031281772
Epoch: 953, Batch Gradient Norm after: 12.253088031281772
Epoch 954/10000, Prediction Accuracy = 59.418000000000006%, Loss = 0.6400230407714844
Epoch: 954, Batch Gradient Norm: 15.680528868193859
Epoch: 954, Batch Gradient Norm after: 15.680528868193859
Epoch 955/10000, Prediction Accuracy = 59.28399999999999%, Loss = 0.6654645681381226
Epoch: 955, Batch Gradient Norm: 10.165596920806625
Epoch: 955, Batch Gradient Norm after: 10.165596920806625
Epoch 956/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.631245493888855
Epoch: 956, Batch Gradient Norm: 9.833313695210801
Epoch: 956, Batch Gradient Norm after: 9.833313695210801
Epoch 957/10000, Prediction Accuracy = 59.364%, Loss = 0.6281737923622132
Epoch: 957, Batch Gradient Norm: 9.175812665990353
Epoch: 957, Batch Gradient Norm after: 9.175812665990353
Epoch 958/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.6251403093338013
Epoch: 958, Batch Gradient Norm: 12.525187761259788
Epoch: 958, Batch Gradient Norm after: 12.525187761259788
Epoch 959/10000, Prediction Accuracy = 59.318000000000005%, Loss = 0.6435311555862426
Epoch: 959, Batch Gradient Norm: 12.626894254198765
Epoch: 959, Batch Gradient Norm after: 12.626894254198765
Epoch 960/10000, Prediction Accuracy = 59.45399999999999%, Loss = 0.6450743913650513
Epoch: 960, Batch Gradient Norm: 11.938437843732192
Epoch: 960, Batch Gradient Norm after: 11.938437843732192
Epoch 961/10000, Prediction Accuracy = 59.376%, Loss = 0.6393621802330017
Epoch: 961, Batch Gradient Norm: 9.887990775425958
Epoch: 961, Batch Gradient Norm after: 9.887990775425958
Epoch 962/10000, Prediction Accuracy = 59.434000000000005%, Loss = 0.628081202507019
Epoch: 962, Batch Gradient Norm: 12.101109735590894
Epoch: 962, Batch Gradient Norm after: 12.101109735590894
Epoch 963/10000, Prediction Accuracy = 59.384%, Loss = 0.6428368926048279
Epoch: 963, Batch Gradient Norm: 11.356033755131147
Epoch: 963, Batch Gradient Norm after: 11.356033755131147
Epoch 964/10000, Prediction Accuracy = 59.398%, Loss = 0.6362096667289734
Epoch: 964, Batch Gradient Norm: 13.992064348281522
Epoch: 964, Batch Gradient Norm after: 13.992064348281522
Epoch 965/10000, Prediction Accuracy = 59.326%, Loss = 0.6531755328178406
Epoch: 965, Batch Gradient Norm: 10.274106473965412
Epoch: 965, Batch Gradient Norm after: 10.274106473965412
Epoch 966/10000, Prediction Accuracy = 59.39%, Loss = 0.6289270043373107
Epoch: 966, Batch Gradient Norm: 14.192299894380044
Epoch: 966, Batch Gradient Norm after: 14.192299894380044
Epoch 967/10000, Prediction Accuracy = 59.35600000000001%, Loss = 0.6527645468711853
Epoch: 967, Batch Gradient Norm: 15.973917692268806
Epoch: 967, Batch Gradient Norm after: 15.973917692268806
Epoch 968/10000, Prediction Accuracy = 59.396%, Loss = 0.6676329731941223
Epoch: 968, Batch Gradient Norm: 13.930370547652986
Epoch: 968, Batch Gradient Norm after: 13.930370547652986
Epoch 969/10000, Prediction Accuracy = 59.352%, Loss = 0.6508281707763672
Epoch: 969, Batch Gradient Norm: 9.627610636862327
Epoch: 969, Batch Gradient Norm after: 9.627610636862327
Epoch 970/10000, Prediction Accuracy = 59.362%, Loss = 0.6247483253479004
Epoch: 970, Batch Gradient Norm: 12.093759177625769
Epoch: 970, Batch Gradient Norm after: 12.093759177625769
Epoch 971/10000, Prediction Accuracy = 59.464%, Loss = 0.6394908547401428
Epoch: 971, Batch Gradient Norm: 11.749564864793355
Epoch: 971, Batch Gradient Norm after: 11.749564864793355
Epoch 972/10000, Prediction Accuracy = 59.35%, Loss = 0.6379452109336853
Epoch: 972, Batch Gradient Norm: 10.707158579985547
Epoch: 972, Batch Gradient Norm after: 10.707158579985547
Epoch 973/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.6313444256782532
Epoch: 973, Batch Gradient Norm: 12.304535760988312
Epoch: 973, Batch Gradient Norm after: 12.304535760988312
Epoch 974/10000, Prediction Accuracy = 59.298%, Loss = 0.639311945438385
Epoch: 974, Batch Gradient Norm: 14.167529823092432
Epoch: 974, Batch Gradient Norm after: 14.167529823092432
Epoch 975/10000, Prediction Accuracy = 59.407999999999994%, Loss = 0.6524794697761536
Epoch: 975, Batch Gradient Norm: 13.949509011695605
Epoch: 975, Batch Gradient Norm after: 13.949509011695605
Epoch 976/10000, Prediction Accuracy = 59.370000000000005%, Loss = 0.6534802556037903
Epoch: 976, Batch Gradient Norm: 8.34738378598184
Epoch: 976, Batch Gradient Norm after: 8.34738378598184
Epoch 977/10000, Prediction Accuracy = 59.424%, Loss = 0.6198687076568603
Epoch: 977, Batch Gradient Norm: 11.694487321523322
Epoch: 977, Batch Gradient Norm after: 11.694487321523322
Epoch 978/10000, Prediction Accuracy = 59.410000000000004%, Loss = 0.6378857016563415
Epoch: 978, Batch Gradient Norm: 10.716224164851507
Epoch: 978, Batch Gradient Norm after: 10.716224164851507
Epoch 979/10000, Prediction Accuracy = 59.431999999999995%, Loss = 0.6310223698616028
Epoch: 979, Batch Gradient Norm: 16.181797349095838
Epoch: 979, Batch Gradient Norm after: 16.181797349095838
Epoch 980/10000, Prediction Accuracy = 59.36%, Loss = 0.6663224577903748
Epoch: 980, Batch Gradient Norm: 15.198837644545895
Epoch: 980, Batch Gradient Norm after: 15.198837644545895
Epoch 981/10000, Prediction Accuracy = 59.40599999999999%, Loss = 0.6598262667655945
Epoch: 981, Batch Gradient Norm: 11.514449385107794
Epoch: 981, Batch Gradient Norm after: 11.514449385107794
Epoch 982/10000, Prediction Accuracy = 59.386%, Loss = 0.6336926817893982
Epoch: 982, Batch Gradient Norm: 9.144222965606168
Epoch: 982, Batch Gradient Norm after: 9.144222965606168
Epoch 983/10000, Prediction Accuracy = 59.448%, Loss = 0.6208324909210206
Epoch: 983, Batch Gradient Norm: 9.685468299656431
Epoch: 983, Batch Gradient Norm after: 9.685468299656431
Epoch 984/10000, Prediction Accuracy = 59.396%, Loss = 0.6222984910011291
Epoch: 984, Batch Gradient Norm: 10.564665595217907
Epoch: 984, Batch Gradient Norm after: 10.564665595217907
Epoch 985/10000, Prediction Accuracy = 59.396%, Loss = 0.6273104548454285
Epoch: 985, Batch Gradient Norm: 11.549124380810921
Epoch: 985, Batch Gradient Norm after: 11.549124380810921
Epoch 986/10000, Prediction Accuracy = 59.39399999999999%, Loss = 0.6325048565864563
Epoch: 986, Batch Gradient Norm: 11.735666000097883
Epoch: 986, Batch Gradient Norm after: 11.735666000097883
Epoch 987/10000, Prediction Accuracy = 59.35%, Loss = 0.6344651341438293
Epoch: 987, Batch Gradient Norm: 10.01076706205167
Epoch: 987, Batch Gradient Norm after: 10.01076706205167
Epoch 988/10000, Prediction Accuracy = 59.444%, Loss = 0.6243706464767456
Epoch: 988, Batch Gradient Norm: 12.059739462685299
Epoch: 988, Batch Gradient Norm after: 12.059739462685299
Epoch 989/10000, Prediction Accuracy = 59.41600000000001%, Loss = 0.6344361424446106
Epoch: 989, Batch Gradient Norm: 17.908650425317564
Epoch: 989, Batch Gradient Norm after: 17.908650425317564
Epoch 990/10000, Prediction Accuracy = 59.42%, Loss = 0.6803071618080139
Epoch: 990, Batch Gradient Norm: 14.16986338141359
Epoch: 990, Batch Gradient Norm after: 14.16986338141359
Epoch 991/10000, Prediction Accuracy = 59.391999999999996%, Loss = 0.6539562940597534
Epoch: 991, Batch Gradient Norm: 7.516438334221022
Epoch: 991, Batch Gradient Norm after: 7.516438334221022
Epoch 992/10000, Prediction Accuracy = 59.44000000000001%, Loss = 0.6130607724189758
Epoch: 992, Batch Gradient Norm: 10.064967739541002
Epoch: 992, Batch Gradient Norm after: 10.064967739541002
Epoch 993/10000, Prediction Accuracy = 59.44000000000001%, Loss = 0.6234210729598999
Epoch: 993, Batch Gradient Norm: 13.50387109852892
Epoch: 993, Batch Gradient Norm after: 13.50387109852892
Epoch 994/10000, Prediction Accuracy = 59.376%, Loss = 0.6465125560760498
Epoch: 994, Batch Gradient Norm: 14.362260242430665
Epoch: 994, Batch Gradient Norm after: 14.362260242430665
Epoch 995/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.6538688898086548
Epoch: 995, Batch Gradient Norm: 10.943854464009323
Epoch: 995, Batch Gradient Norm after: 10.943854464009323
Epoch 996/10000, Prediction Accuracy = 59.355999999999995%, Loss = 0.6290816068649292
Epoch: 996, Batch Gradient Norm: 11.56095451043404
Epoch: 996, Batch Gradient Norm after: 11.56095451043404
Epoch 997/10000, Prediction Accuracy = 59.484%, Loss = 0.6321694493293762
Epoch: 997, Batch Gradient Norm: 13.666307917438449
Epoch: 997, Batch Gradient Norm after: 13.666307917438449
Epoch 998/10000, Prediction Accuracy = 59.4%, Loss = 0.6476695656776428
Epoch: 998, Batch Gradient Norm: 11.16810649156278
Epoch: 998, Batch Gradient Norm after: 11.16810649156278
Epoch 999/10000, Prediction Accuracy = 59.48199999999999%, Loss = 0.6332136511802673
Epoch: 999, Batch Gradient Norm: 11.097069868152868
Epoch: 999, Batch Gradient Norm after: 11.097069868152868
Epoch 1000/10000, Prediction Accuracy = 59.46%, Loss = 0.6305798172950745
Epoch: 1000, Batch Gradient Norm: 11.093448373790677
Epoch: 1000, Batch Gradient Norm after: 11.093448373790677
Epoch 1001/10000, Prediction Accuracy = 59.446000000000005%, Loss = 0.6292973160743713
Epoch: 1001, Batch Gradient Norm: 15.36877111864347
Epoch: 1001, Batch Gradient Norm after: 15.36877111864347
Epoch 1002/10000, Prediction Accuracy = 59.42%, Loss = 0.6576902866363525
Epoch: 1002, Batch Gradient Norm: 13.346762355857143
Epoch: 1002, Batch Gradient Norm after: 13.346762355857143
Epoch 1003/10000, Prediction Accuracy = 59.436%, Loss = 0.6430221319198608
Epoch: 1003, Batch Gradient Norm: 11.147945791518078
Epoch: 1003, Batch Gradient Norm after: 11.147945791518078
Epoch 1004/10000, Prediction Accuracy = 59.45%, Loss = 0.6284799695014953
Epoch: 1004, Batch Gradient Norm: 8.731289352809432
Epoch: 1004, Batch Gradient Norm after: 8.731289352809432
Epoch 1005/10000, Prediction Accuracy = 59.472%, Loss = 0.6155360817909241
Epoch: 1005, Batch Gradient Norm: 10.065594802772019
Epoch: 1005, Batch Gradient Norm after: 10.065594802772019
Epoch 1006/10000, Prediction Accuracy = 59.462%, Loss = 0.6214413404464721
Epoch: 1006, Batch Gradient Norm: 11.802189262239029
Epoch: 1006, Batch Gradient Norm after: 11.802189262239029
Epoch 1007/10000, Prediction Accuracy = 59.414%, Loss = 0.6330862402915954
Epoch: 1007, Batch Gradient Norm: 11.67716809546766
Epoch: 1007, Batch Gradient Norm after: 11.67716809546766
Epoch 1008/10000, Prediction Accuracy = 59.44200000000001%, Loss = 0.632524597644806
Epoch: 1008, Batch Gradient Norm: 13.909663092624863
Epoch: 1008, Batch Gradient Norm after: 13.909663092624863
Epoch 1009/10000, Prediction Accuracy = 59.472%, Loss = 0.6475684762001037
Epoch: 1009, Batch Gradient Norm: 12.322088490377249
Epoch: 1009, Batch Gradient Norm after: 12.322088490377249
Epoch 1010/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.6352031707763672
Epoch: 1010, Batch Gradient Norm: 14.770869056131898
Epoch: 1010, Batch Gradient Norm after: 14.770869056131898
Epoch 1011/10000, Prediction Accuracy = 59.43000000000001%, Loss = 0.6523959875106812
Epoch: 1011, Batch Gradient Norm: 10.115283357127094
Epoch: 1011, Batch Gradient Norm after: 10.115283357127094
Epoch 1012/10000, Prediction Accuracy = 59.49000000000001%, Loss = 0.6223438143730163
Epoch: 1012, Batch Gradient Norm: 10.968734436941284
Epoch: 1012, Batch Gradient Norm after: 10.968734436941284
Epoch 1013/10000, Prediction Accuracy = 59.455999999999996%, Loss = 0.6267497301101684
Epoch: 1013, Batch Gradient Norm: 12.575698161250163
Epoch: 1013, Batch Gradient Norm after: 12.575698161250163
Epoch 1014/10000, Prediction Accuracy = 59.438%, Loss = 0.6362266659736633
Epoch: 1014, Batch Gradient Norm: 15.57052984075852
Epoch: 1014, Batch Gradient Norm after: 15.57052984075852
Epoch 1015/10000, Prediction Accuracy = 59.4%, Loss = 0.6571403503417969
Epoch: 1015, Batch Gradient Norm: 12.989169995017312
Epoch: 1015, Batch Gradient Norm after: 12.989169995017312
Epoch 1016/10000, Prediction Accuracy = 59.468%, Loss = 0.6375227689743042
Epoch: 1016, Batch Gradient Norm: 8.775527855542078
Epoch: 1016, Batch Gradient Norm after: 8.775527855542078
Epoch 1017/10000, Prediction Accuracy = 59.476%, Loss = 0.6139833807945252
Epoch: 1017, Batch Gradient Norm: 9.551953060222347
Epoch: 1017, Batch Gradient Norm after: 9.551953060222347
Epoch 1018/10000, Prediction Accuracy = 59.511999999999986%, Loss = 0.6174232244491578
Epoch: 1018, Batch Gradient Norm: 11.091315950643297
Epoch: 1018, Batch Gradient Norm after: 11.091315950643297
Epoch 1019/10000, Prediction Accuracy = 59.470000000000006%, Loss = 0.626704216003418
Epoch: 1019, Batch Gradient Norm: 14.915686897267998
Epoch: 1019, Batch Gradient Norm after: 14.915686897267998
Epoch 1020/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.65419602394104
Epoch: 1020, Batch Gradient Norm: 9.396422938233025
Epoch: 1020, Batch Gradient Norm after: 9.396422938233025
Epoch 1021/10000, Prediction Accuracy = 59.476%, Loss = 0.6192809820175171
Epoch: 1021, Batch Gradient Norm: 9.095360167138468
Epoch: 1021, Batch Gradient Norm after: 9.095360167138468
Epoch 1022/10000, Prediction Accuracy = 59.492%, Loss = 0.6159509062767029
Epoch: 1022, Batch Gradient Norm: 13.621336887834293
Epoch: 1022, Batch Gradient Norm after: 13.621336887834293
Epoch 1023/10000, Prediction Accuracy = 59.488%, Loss = 0.6455092310905457
Epoch: 1023, Batch Gradient Norm: 15.464788302141807
Epoch: 1023, Batch Gradient Norm after: 15.464788302141807
Epoch 1024/10000, Prediction Accuracy = 59.38199999999999%, Loss = 0.66204434633255
Epoch: 1024, Batch Gradient Norm: 11.457627564840717
Epoch: 1024, Batch Gradient Norm after: 11.457627564840717
Epoch 1025/10000, Prediction Accuracy = 59.541999999999994%, Loss = 0.6309318900108337
Epoch: 1025, Batch Gradient Norm: 13.08998773458141
Epoch: 1025, Batch Gradient Norm after: 13.08998773458141
Epoch 1026/10000, Prediction Accuracy = 59.419999999999995%, Loss = 0.6396159410476685
Epoch: 1026, Batch Gradient Norm: 12.822280112397443
Epoch: 1026, Batch Gradient Norm after: 12.822280112397443
Epoch 1027/10000, Prediction Accuracy = 59.586%, Loss = 0.6382715225219726
Epoch: 1027, Batch Gradient Norm: 13.358809545452122
Epoch: 1027, Batch Gradient Norm after: 13.358809545452122
Epoch 1028/10000, Prediction Accuracy = 59.55%, Loss = 0.6404829382896423
Epoch: 1028, Batch Gradient Norm: 11.95078307950285
Epoch: 1028, Batch Gradient Norm after: 11.95078307950285
Epoch 1029/10000, Prediction Accuracy = 59.44000000000001%, Loss = 0.6300543308258056
Epoch: 1029, Batch Gradient Norm: 12.709370740411956
Epoch: 1029, Batch Gradient Norm after: 12.709370740411956
Epoch 1030/10000, Prediction Accuracy = 59.534000000000006%, Loss = 0.6335834264755249
Epoch: 1030, Batch Gradient Norm: 12.038350263933099
Epoch: 1030, Batch Gradient Norm after: 12.038350263933099
Epoch 1031/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.6294342279434204
Epoch: 1031, Batch Gradient Norm: 11.739755478307327
Epoch: 1031, Batch Gradient Norm after: 11.739755478307327
Epoch 1032/10000, Prediction Accuracy = 59.510000000000005%, Loss = 0.6268859386444092
Epoch: 1032, Batch Gradient Norm: 10.074854820490394
Epoch: 1032, Batch Gradient Norm after: 10.074854820490394
Epoch 1033/10000, Prediction Accuracy = 59.474000000000004%, Loss = 0.617961871623993
Epoch: 1033, Batch Gradient Norm: 10.566596045525115
Epoch: 1033, Batch Gradient Norm after: 10.566596045525115
Epoch 1034/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.6204413652420044
Epoch: 1034, Batch Gradient Norm: 11.076432324321969
Epoch: 1034, Batch Gradient Norm after: 11.076432324321969
Epoch 1035/10000, Prediction Accuracy = 59.52%, Loss = 0.6228113532066345
Epoch: 1035, Batch Gradient Norm: 16.192924788328604
Epoch: 1035, Batch Gradient Norm after: 16.192924788328604
Epoch 1036/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.660278582572937
Epoch: 1036, Batch Gradient Norm: 10.960804468729227
Epoch: 1036, Batch Gradient Norm after: 10.960804468729227
Epoch 1037/10000, Prediction Accuracy = 59.472%, Loss = 0.6247076392173767
Epoch: 1037, Batch Gradient Norm: 11.18920692867267
Epoch: 1037, Batch Gradient Norm after: 11.18920692867267
Epoch 1038/10000, Prediction Accuracy = 59.477999999999994%, Loss = 0.6250072121620178
Epoch: 1038, Batch Gradient Norm: 12.519340726166895
Epoch: 1038, Batch Gradient Norm after: 12.519340726166895
Epoch 1039/10000, Prediction Accuracy = 59.538%, Loss = 0.6332443594932556
Epoch: 1039, Batch Gradient Norm: 13.31278132312623
Epoch: 1039, Batch Gradient Norm after: 13.31278132312623
Epoch 1040/10000, Prediction Accuracy = 59.44%, Loss = 0.6386448383331299
Epoch: 1040, Batch Gradient Norm: 12.017487721823356
Epoch: 1040, Batch Gradient Norm after: 12.017487721823356
Epoch 1041/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.6296781659126282
Epoch: 1041, Batch Gradient Norm: 10.817487114247452
Epoch: 1041, Batch Gradient Norm after: 10.817487114247452
Epoch 1042/10000, Prediction Accuracy = 59.452%, Loss = 0.6208921909332276
Epoch: 1042, Batch Gradient Norm: 14.217389883363923
Epoch: 1042, Batch Gradient Norm after: 14.217389883363923
Epoch 1043/10000, Prediction Accuracy = 59.492000000000004%, Loss = 0.6442947506904602
Epoch: 1043, Batch Gradient Norm: 10.390471835694774
Epoch: 1043, Batch Gradient Norm after: 10.390471835694774
Epoch 1044/10000, Prediction Accuracy = 59.484%, Loss = 0.6203655123710632
Epoch: 1044, Batch Gradient Norm: 12.897793881384333
Epoch: 1044, Batch Gradient Norm after: 12.897793881384333
Epoch 1045/10000, Prediction Accuracy = 59.49400000000001%, Loss = 0.6354217767715454
Epoch: 1045, Batch Gradient Norm: 11.517097752823373
Epoch: 1045, Batch Gradient Norm after: 11.517097752823373
Epoch 1046/10000, Prediction Accuracy = 59.53399999999999%, Loss = 0.6268091559410095
Epoch: 1046, Batch Gradient Norm: 13.113504999155934
Epoch: 1046, Batch Gradient Norm after: 13.113504999155934
Epoch 1047/10000, Prediction Accuracy = 59.565999999999995%, Loss = 0.6354941129684448
Epoch: 1047, Batch Gradient Norm: 12.284197304649977
Epoch: 1047, Batch Gradient Norm after: 12.284197304649977
Epoch 1048/10000, Prediction Accuracy = 59.467999999999996%, Loss = 0.6295768857002259
Epoch: 1048, Batch Gradient Norm: 13.089901732745712
Epoch: 1048, Batch Gradient Norm after: 13.089901732745712
Epoch 1049/10000, Prediction Accuracy = 59.56600000000001%, Loss = 0.6352569341659546
Epoch: 1049, Batch Gradient Norm: 12.886924970996645
Epoch: 1049, Batch Gradient Norm after: 12.886924970996645
Epoch 1050/10000, Prediction Accuracy = 59.498000000000005%, Loss = 0.635098147392273
Epoch: 1050, Batch Gradient Norm: 12.608582358542405
Epoch: 1050, Batch Gradient Norm after: 12.608582358542405
Epoch 1051/10000, Prediction Accuracy = 59.602%, Loss = 0.6351289868354797
Epoch: 1051, Batch Gradient Norm: 10.786304458530838
Epoch: 1051, Batch Gradient Norm after: 10.786304458530838
Epoch 1052/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.6230037212371826
Epoch: 1052, Batch Gradient Norm: 8.334199266446761
Epoch: 1052, Batch Gradient Norm after: 8.334199266446761
Epoch 1053/10000, Prediction Accuracy = 59.580000000000005%, Loss = 0.6094194054603577
Epoch: 1053, Batch Gradient Norm: 8.867489854087463
Epoch: 1053, Batch Gradient Norm after: 8.867489854087463
Epoch 1054/10000, Prediction Accuracy = 59.54%, Loss = 0.6109200716018677
Epoch: 1054, Batch Gradient Norm: 8.50486973221378
Epoch: 1054, Batch Gradient Norm after: 8.50486973221378
Epoch 1055/10000, Prediction Accuracy = 59.629999999999995%, Loss = 0.6089584589004516
Epoch: 1055, Batch Gradient Norm: 12.070549665235344
Epoch: 1055, Batch Gradient Norm after: 12.070549665235344
Epoch 1056/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.6283337235450744
Epoch: 1056, Batch Gradient Norm: 12.908761618421016
Epoch: 1056, Batch Gradient Norm after: 12.908761618421016
Epoch 1057/10000, Prediction Accuracy = 59.58%, Loss = 0.6336757659912109
Epoch: 1057, Batch Gradient Norm: 16.222517360325103
Epoch: 1057, Batch Gradient Norm after: 16.222517360325103
Epoch 1058/10000, Prediction Accuracy = 59.501999999999995%, Loss = 0.6620158672332763
Epoch: 1058, Batch Gradient Norm: 11.351958631059668
Epoch: 1058, Batch Gradient Norm after: 11.351958631059668
Epoch 1059/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.6261455535888671
Epoch: 1059, Batch Gradient Norm: 10.622780288080751
Epoch: 1059, Batch Gradient Norm after: 10.622780288080751
Epoch 1060/10000, Prediction Accuracy = 59.52%, Loss = 0.619670295715332
Epoch: 1060, Batch Gradient Norm: 13.65149362883176
Epoch: 1060, Batch Gradient Norm after: 13.65149362883176
Epoch 1061/10000, Prediction Accuracy = 59.614%, Loss = 0.6366556882858276
Epoch: 1061, Batch Gradient Norm: 12.795633237844346
Epoch: 1061, Batch Gradient Norm after: 12.795633237844346
Epoch 1062/10000, Prediction Accuracy = 59.55800000000001%, Loss = 0.6309764981269836
Epoch: 1062, Batch Gradient Norm: 11.949302597442953
Epoch: 1062, Batch Gradient Norm after: 11.949302597442953
Epoch 1063/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.6247377634048462
Epoch: 1063, Batch Gradient Norm: 12.933721395480717
Epoch: 1063, Batch Gradient Norm after: 12.933721395480717
Epoch 1064/10000, Prediction Accuracy = 59.548%, Loss = 0.6308667421340942
Epoch: 1064, Batch Gradient Norm: 15.46202217932157
Epoch: 1064, Batch Gradient Norm after: 15.46202217932157
Epoch 1065/10000, Prediction Accuracy = 59.57000000000001%, Loss = 0.6507522821426391
Epoch: 1065, Batch Gradient Norm: 14.126677219390139
Epoch: 1065, Batch Gradient Norm after: 14.126677219390139
Epoch 1066/10000, Prediction Accuracy = 59.536%, Loss = 0.6429425239562988
Epoch: 1066, Batch Gradient Norm: 10.313414905000398
Epoch: 1066, Batch Gradient Norm after: 10.313414905000398
Epoch 1067/10000, Prediction Accuracy = 59.612%, Loss = 0.6175680875778198
Epoch: 1067, Batch Gradient Norm: 8.946346541875625
Epoch: 1067, Batch Gradient Norm after: 8.946346541875625
Epoch 1068/10000, Prediction Accuracy = 59.536%, Loss = 0.6103194832801819
Epoch: 1068, Batch Gradient Norm: 9.766559040124918
Epoch: 1068, Batch Gradient Norm after: 9.766559040124918
Epoch 1069/10000, Prediction Accuracy = 59.574%, Loss = 0.6133734703063964
Epoch: 1069, Batch Gradient Norm: 10.322854996177398
Epoch: 1069, Batch Gradient Norm after: 10.322854996177398
Epoch 1070/10000, Prediction Accuracy = 59.556%, Loss = 0.6169286727905273
Epoch: 1070, Batch Gradient Norm: 13.05935919839287
Epoch: 1070, Batch Gradient Norm after: 13.05935919839287
Epoch 1071/10000, Prediction Accuracy = 59.525999999999996%, Loss = 0.6333896040916442
Epoch: 1071, Batch Gradient Norm: 12.739359065733652
Epoch: 1071, Batch Gradient Norm after: 12.739359065733652
Epoch 1072/10000, Prediction Accuracy = 59.529999999999994%, Loss = 0.6318040132522583
Epoch: 1072, Batch Gradient Norm: 13.396991643554522
Epoch: 1072, Batch Gradient Norm after: 13.396991643554522
Epoch 1073/10000, Prediction Accuracy = 59.576%, Loss = 0.6362723708152771
Epoch: 1073, Batch Gradient Norm: 13.375454712925013
Epoch: 1073, Batch Gradient Norm after: 13.375454712925013
Epoch 1074/10000, Prediction Accuracy = 59.458000000000006%, Loss = 0.6348027467727662
Epoch: 1074, Batch Gradient Norm: 12.418724107968472
Epoch: 1074, Batch Gradient Norm after: 12.418724107968472
Epoch 1075/10000, Prediction Accuracy = 59.614%, Loss = 0.6295011281967163
Epoch: 1075, Batch Gradient Norm: 12.41473510849266
Epoch: 1075, Batch Gradient Norm after: 12.41473510849266
Epoch 1076/10000, Prediction Accuracy = 59.508%, Loss = 0.628189992904663
Epoch: 1076, Batch Gradient Norm: 10.947599313347675
Epoch: 1076, Batch Gradient Norm after: 10.947599313347675
Epoch 1077/10000, Prediction Accuracy = 59.616%, Loss = 0.6180944323539734
Epoch: 1077, Batch Gradient Norm: 14.456159398679606
Epoch: 1077, Batch Gradient Norm after: 14.456159398679606
Epoch 1078/10000, Prediction Accuracy = 59.538%, Loss = 0.6412790179252624
Epoch: 1078, Batch Gradient Norm: 11.881646401384142
Epoch: 1078, Batch Gradient Norm after: 11.881646401384142
Epoch 1079/10000, Prediction Accuracy = 59.536%, Loss = 0.6235761165618896
Epoch: 1079, Batch Gradient Norm: 12.450519254541542
Epoch: 1079, Batch Gradient Norm after: 12.450519254541542
Epoch 1080/10000, Prediction Accuracy = 59.628%, Loss = 0.6285546064376831
Epoch: 1080, Batch Gradient Norm: 9.53750600527145
Epoch: 1080, Batch Gradient Norm after: 9.53750600527145
Epoch 1081/10000, Prediction Accuracy = 59.544000000000004%, Loss = 0.6112833261489868
Epoch: 1081, Batch Gradient Norm: 10.547101862160337
Epoch: 1081, Batch Gradient Norm after: 10.547101862160337
Epoch 1082/10000, Prediction Accuracy = 59.686%, Loss = 0.6172089099884033
Epoch: 1082, Batch Gradient Norm: 10.352478821939748
Epoch: 1082, Batch Gradient Norm after: 10.352478821939748
Epoch 1083/10000, Prediction Accuracy = 59.605999999999995%, Loss = 0.6160011529922486
Epoch: 1083, Batch Gradient Norm: 11.795397681986808
Epoch: 1083, Batch Gradient Norm after: 11.795397681986808
Epoch 1084/10000, Prediction Accuracy = 59.628%, Loss = 0.6247651934623718
Epoch: 1084, Batch Gradient Norm: 12.899364810031505
Epoch: 1084, Batch Gradient Norm after: 12.899364810031505
Epoch 1085/10000, Prediction Accuracy = 59.576%, Loss = 0.630773413181305
Epoch: 1085, Batch Gradient Norm: 14.282770039350579
Epoch: 1085, Batch Gradient Norm after: 14.282770039350579
Epoch 1086/10000, Prediction Accuracy = 59.53799999999999%, Loss = 0.640526807308197
Epoch: 1086, Batch Gradient Norm: 11.754586392064729
Epoch: 1086, Batch Gradient Norm after: 11.754586392064729
Epoch 1087/10000, Prediction Accuracy = 59.672000000000004%, Loss = 0.622156023979187
Epoch: 1087, Batch Gradient Norm: 10.037140690644847
Epoch: 1087, Batch Gradient Norm after: 10.037140690644847
Epoch 1088/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.6125726938247681
Epoch: 1088, Batch Gradient Norm: 9.201982321476082
Epoch: 1088, Batch Gradient Norm after: 9.201982321476082
Epoch 1089/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.6071002721786499
Epoch: 1089, Batch Gradient Norm: 11.716154985854466
Epoch: 1089, Batch Gradient Norm after: 11.716154985854466
Epoch 1090/10000, Prediction Accuracy = 59.59599999999999%, Loss = 0.6216955304145813
Epoch: 1090, Batch Gradient Norm: 14.1629161659871
Epoch: 1090, Batch Gradient Norm after: 14.1629161659871
Epoch 1091/10000, Prediction Accuracy = 59.646%, Loss = 0.6371738910675049
Epoch: 1091, Batch Gradient Norm: 15.051607127429262
Epoch: 1091, Batch Gradient Norm after: 15.051607127429262
Epoch 1092/10000, Prediction Accuracy = 59.51800000000001%, Loss = 0.6458018660545349
Epoch: 1092, Batch Gradient Norm: 12.127398993224505
Epoch: 1092, Batch Gradient Norm after: 12.127398993224505
Epoch 1093/10000, Prediction Accuracy = 59.59400000000001%, Loss = 0.6258161902427674
Epoch: 1093, Batch Gradient Norm: 9.641398650080138
Epoch: 1093, Batch Gradient Norm after: 9.641398650080138
Epoch 1094/10000, Prediction Accuracy = 59.65400000000001%, Loss = 0.6106806039810181
Epoch: 1094, Batch Gradient Norm: 10.25531753143533
Epoch: 1094, Batch Gradient Norm after: 10.25531753143533
Epoch 1095/10000, Prediction Accuracy = 59.678%, Loss = 0.613364326953888
Epoch: 1095, Batch Gradient Norm: 15.779230666485924
Epoch: 1095, Batch Gradient Norm after: 15.779230666485924
Epoch 1096/10000, Prediction Accuracy = 59.61800000000001%, Loss = 0.6498173594474792
Epoch: 1096, Batch Gradient Norm: 13.827037489606543
Epoch: 1096, Batch Gradient Norm after: 13.827037489606543
Epoch 1097/10000, Prediction Accuracy = 59.646%, Loss = 0.636007833480835
Epoch: 1097, Batch Gradient Norm: 12.172200590628323
Epoch: 1097, Batch Gradient Norm after: 12.172200590628323
Epoch 1098/10000, Prediction Accuracy = 59.616%, Loss = 0.6243161201477051
Epoch: 1098, Batch Gradient Norm: 7.786252590042537
Epoch: 1098, Batch Gradient Norm after: 7.786252590042537
Epoch 1099/10000, Prediction Accuracy = 59.634%, Loss = 0.6002597451210022
Epoch: 1099, Batch Gradient Norm: 9.839270752375821
Epoch: 1099, Batch Gradient Norm after: 9.839270752375821
Epoch 1100/10000, Prediction Accuracy = 59.58%, Loss = 0.6099477887153626
Epoch: 1100, Batch Gradient Norm: 10.52588337735661
Epoch: 1100, Batch Gradient Norm after: 10.52588337735661
Epoch 1101/10000, Prediction Accuracy = 59.626%, Loss = 0.6134183287620545
Epoch: 1101, Batch Gradient Norm: 13.977470561978175
Epoch: 1101, Batch Gradient Norm after: 13.977470561978175
Epoch 1102/10000, Prediction Accuracy = 59.63800000000001%, Loss = 0.6382384657859802
Epoch: 1102, Batch Gradient Norm: 14.190931478421923
Epoch: 1102, Batch Gradient Norm after: 14.190931478421923
Epoch 1103/10000, Prediction Accuracy = 59.598%, Loss = 0.6403743028640747
Epoch: 1103, Batch Gradient Norm: 11.621317550259088
Epoch: 1103, Batch Gradient Norm after: 11.621317550259088
Epoch 1104/10000, Prediction Accuracy = 59.622%, Loss = 0.6219751954078674
Epoch: 1104, Batch Gradient Norm: 13.334541193148258
Epoch: 1104, Batch Gradient Norm after: 13.334541193148258
Epoch 1105/10000, Prediction Accuracy = 59.608000000000004%, Loss = 0.6330134868621826
Epoch: 1105, Batch Gradient Norm: 12.765567526097247
Epoch: 1105, Batch Gradient Norm after: 12.765567526097247
Epoch 1106/10000, Prediction Accuracy = 59.63199999999999%, Loss = 0.6268675327301025
Epoch: 1106, Batch Gradient Norm: 13.184994400884559
Epoch: 1106, Batch Gradient Norm after: 13.184994400884559
Epoch 1107/10000, Prediction Accuracy = 59.69%, Loss = 0.6291149854660034
Epoch: 1107, Batch Gradient Norm: 11.065886794032489
Epoch: 1107, Batch Gradient Norm after: 11.065886794032489
Epoch 1108/10000, Prediction Accuracy = 59.58%, Loss = 0.6149003982543946
Epoch: 1108, Batch Gradient Norm: 9.843627591891028
Epoch: 1108, Batch Gradient Norm after: 9.843627591891028
Epoch 1109/10000, Prediction Accuracy = 59.69%, Loss = 0.6077502250671387
Epoch: 1109, Batch Gradient Norm: 9.890886750625533
Epoch: 1109, Batch Gradient Norm after: 9.890886750625533
Epoch 1110/10000, Prediction Accuracy = 59.593999999999994%, Loss = 0.6082582592964172
Epoch: 1110, Batch Gradient Norm: 11.333345664423462
Epoch: 1110, Batch Gradient Norm after: 11.333345664423462
Epoch 1111/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.6162092804908752
Epoch: 1111, Batch Gradient Norm: 14.202404247377945
Epoch: 1111, Batch Gradient Norm after: 14.202404247377945
Epoch 1112/10000, Prediction Accuracy = 59.669999999999995%, Loss = 0.6392977714538575
Epoch: 1112, Batch Gradient Norm: 13.441220005560023
Epoch: 1112, Batch Gradient Norm after: 13.441220005560023
Epoch 1113/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.6342989444732666
Epoch: 1113, Batch Gradient Norm: 10.128988590025028
Epoch: 1113, Batch Gradient Norm after: 10.128988590025028
Epoch 1114/10000, Prediction Accuracy = 59.669999999999995%, Loss = 0.6119766354560852
Epoch: 1114, Batch Gradient Norm: 8.801741715467461
Epoch: 1114, Batch Gradient Norm after: 8.801741715467461
Epoch 1115/10000, Prediction Accuracy = 59.638%, Loss = 0.6033915758132935
Epoch: 1115, Batch Gradient Norm: 11.654928831253752
Epoch: 1115, Batch Gradient Norm after: 11.654928831253752
Epoch 1116/10000, Prediction Accuracy = 59.664%, Loss = 0.6166612982749939
Epoch: 1116, Batch Gradient Norm: 17.050632782392476
Epoch: 1116, Batch Gradient Norm after: 17.050632782392476
Epoch 1117/10000, Prediction Accuracy = 59.64399999999999%, Loss = 0.6568608164787293
Epoch: 1117, Batch Gradient Norm: 15.46797072622953
Epoch: 1117, Batch Gradient Norm after: 15.46797072622953
Epoch 1118/10000, Prediction Accuracy = 59.641999999999996%, Loss = 0.6466956257820129
Epoch: 1118, Batch Gradient Norm: 9.108617521973693
Epoch: 1118, Batch Gradient Norm after: 9.108617521973693
Epoch 1119/10000, Prediction Accuracy = 59.71%, Loss = 0.6050015807151794
Epoch: 1119, Batch Gradient Norm: 11.184063547775924
Epoch: 1119, Batch Gradient Norm after: 11.184063547775924
Epoch 1120/10000, Prediction Accuracy = 59.64%, Loss = 0.6157926559448242
Epoch: 1120, Batch Gradient Norm: 11.591671654968168
Epoch: 1120, Batch Gradient Norm after: 11.591671654968168
Epoch 1121/10000, Prediction Accuracy = 59.69%, Loss = 0.6189321279525757
Epoch: 1121, Batch Gradient Norm: 12.600199095810142
Epoch: 1121, Batch Gradient Norm after: 12.600199095810142
Epoch 1122/10000, Prediction Accuracy = 59.668000000000006%, Loss = 0.6257205843925476
Epoch: 1122, Batch Gradient Norm: 11.123588035491183
Epoch: 1122, Batch Gradient Norm after: 11.123588035491183
Epoch 1123/10000, Prediction Accuracy = 59.669999999999995%, Loss = 0.6158214211463928
Epoch: 1123, Batch Gradient Norm: 10.618838079493633
Epoch: 1123, Batch Gradient Norm after: 10.618838079493633
Epoch 1124/10000, Prediction Accuracy = 59.69199999999999%, Loss = 0.6121950626373291
Epoch: 1124, Batch Gradient Norm: 9.756797460793925
Epoch: 1124, Batch Gradient Norm after: 9.756797460793925
Epoch 1125/10000, Prediction Accuracy = 59.678%, Loss = 0.6066097378730774
Epoch: 1125, Batch Gradient Norm: 10.492442877005576
Epoch: 1125, Batch Gradient Norm after: 10.492442877005576
Epoch 1126/10000, Prediction Accuracy = 59.734%, Loss = 0.6095948815345764
Epoch: 1126, Batch Gradient Norm: 13.393569448299779
Epoch: 1126, Batch Gradient Norm after: 13.393569448299779
Epoch 1127/10000, Prediction Accuracy = 59.622%, Loss = 0.6265451312065125
Epoch: 1127, Batch Gradient Norm: 16.060214045712947
Epoch: 1127, Batch Gradient Norm after: 16.060214045712947
Epoch 1128/10000, Prediction Accuracy = 59.682%, Loss = 0.649413001537323
Epoch: 1128, Batch Gradient Norm: 12.877760361369843
Epoch: 1128, Batch Gradient Norm after: 12.877760361369843
Epoch 1129/10000, Prediction Accuracy = 59.638%, Loss = 0.6265133500099183
Epoch: 1129, Batch Gradient Norm: 11.377209095446801
Epoch: 1129, Batch Gradient Norm after: 11.377209095446801
Epoch 1130/10000, Prediction Accuracy = 59.676%, Loss = 0.6169169187545777
Epoch: 1130, Batch Gradient Norm: 9.400732849138343
Epoch: 1130, Batch Gradient Norm after: 9.400732849138343
Epoch 1131/10000, Prediction Accuracy = 59.660000000000004%, Loss = 0.6061698913574218
Epoch: 1131, Batch Gradient Norm: 10.092186334766401
Epoch: 1131, Batch Gradient Norm after: 10.092186334766401
Epoch 1132/10000, Prediction Accuracy = 59.693999999999996%, Loss = 0.6091881632804871
Epoch: 1132, Batch Gradient Norm: 8.739780691092705
Epoch: 1132, Batch Gradient Norm after: 8.739780691092705
Epoch 1133/10000, Prediction Accuracy = 59.715999999999994%, Loss = 0.6024786233901978
Epoch: 1133, Batch Gradient Norm: 9.544003419076033
Epoch: 1133, Batch Gradient Norm after: 9.544003419076033
Epoch 1134/10000, Prediction Accuracy = 59.67%, Loss = 0.605438506603241
Epoch: 1134, Batch Gradient Norm: 11.519111834516401
Epoch: 1134, Batch Gradient Norm after: 11.519111834516401
Epoch 1135/10000, Prediction Accuracy = 59.688%, Loss = 0.6170059561729431
Epoch: 1135, Batch Gradient Norm: 14.041071053713909
Epoch: 1135, Batch Gradient Norm after: 14.041071053713909
Epoch 1136/10000, Prediction Accuracy = 59.688%, Loss = 0.6338379621505738
Epoch: 1136, Batch Gradient Norm: 15.74475500887465
Epoch: 1136, Batch Gradient Norm after: 15.74475500887465
Epoch 1137/10000, Prediction Accuracy = 59.64799999999999%, Loss = 0.647830069065094
Epoch: 1137, Batch Gradient Norm: 12.92784993191052
Epoch: 1137, Batch Gradient Norm after: 12.92784993191052
Epoch 1138/10000, Prediction Accuracy = 59.676%, Loss = 0.6238058567047119
Epoch: 1138, Batch Gradient Norm: 14.094363739459448
Epoch: 1138, Batch Gradient Norm after: 14.094363739459448
Epoch 1139/10000, Prediction Accuracy = 59.678%, Loss = 0.6324058175086975
Epoch: 1139, Batch Gradient Norm: 11.769305202412578
Epoch: 1139, Batch Gradient Norm after: 11.769305202412578
Epoch 1140/10000, Prediction Accuracy = 59.657999999999994%, Loss = 0.6180927753448486
Epoch: 1140, Batch Gradient Norm: 12.007183691950011
Epoch: 1140, Batch Gradient Norm after: 12.007183691950011
Epoch 1141/10000, Prediction Accuracy = 59.682%, Loss = 0.6194118380546569
Epoch: 1141, Batch Gradient Norm: 11.04840040724752
Epoch: 1141, Batch Gradient Norm after: 11.04840040724752
Epoch 1142/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.612469220161438
Epoch: 1142, Batch Gradient Norm: 12.140045304688751
Epoch: 1142, Batch Gradient Norm after: 12.140045304688751
Epoch 1143/10000, Prediction Accuracy = 59.726%, Loss = 0.6182378768920899
Epoch: 1143, Batch Gradient Norm: 11.609038149431132
Epoch: 1143, Batch Gradient Norm after: 11.609038149431132
Epoch 1144/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.6138033986091613
Epoch: 1144, Batch Gradient Norm: 11.992927311346886
Epoch: 1144, Batch Gradient Norm after: 11.992927311346886
Epoch 1145/10000, Prediction Accuracy = 59.666%, Loss = 0.6160901069641114
Epoch: 1145, Batch Gradient Norm: 11.904231110119836
Epoch: 1145, Batch Gradient Norm after: 11.904231110119836
Epoch 1146/10000, Prediction Accuracy = 59.778%, Loss = 0.6152001142501831
Epoch: 1146, Batch Gradient Norm: 12.221747908069736
Epoch: 1146, Batch Gradient Norm after: 12.221747908069736
Epoch 1147/10000, Prediction Accuracy = 59.66799999999999%, Loss = 0.6186861634254456
Epoch: 1147, Batch Gradient Norm: 12.042697095524344
Epoch: 1147, Batch Gradient Norm after: 12.042697095524344
Epoch 1148/10000, Prediction Accuracy = 59.734%, Loss = 0.6180043816566467
Epoch: 1148, Batch Gradient Norm: 10.700403669107766
Epoch: 1148, Batch Gradient Norm after: 10.700403669107766
Epoch 1149/10000, Prediction Accuracy = 59.602%, Loss = 0.6100427865982055
Epoch: 1149, Batch Gradient Norm: 11.687608933836003
Epoch: 1149, Batch Gradient Norm after: 11.687608933836003
Epoch 1150/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.6162054657936096
Epoch: 1150, Batch Gradient Norm: 13.407346639459655
Epoch: 1150, Batch Gradient Norm after: 13.407346639459655
Epoch 1151/10000, Prediction Accuracy = 59.762%, Loss = 0.6266534566879273
Epoch: 1151, Batch Gradient Norm: 14.700100036307276
Epoch: 1151, Batch Gradient Norm after: 14.700100036307276
Epoch 1152/10000, Prediction Accuracy = 59.666%, Loss = 0.6399614214897156
Epoch: 1152, Batch Gradient Norm: 10.279766886536958
Epoch: 1152, Batch Gradient Norm after: 10.279766886536958
Epoch 1153/10000, Prediction Accuracy = 59.736000000000004%, Loss = 0.6091145634651184
Epoch: 1153, Batch Gradient Norm: 9.145451554251737
Epoch: 1153, Batch Gradient Norm after: 9.145451554251737
Epoch 1154/10000, Prediction Accuracy = 59.702%, Loss = 0.6021005511283875
Epoch: 1154, Batch Gradient Norm: 12.638397600542405
Epoch: 1154, Batch Gradient Norm after: 12.638397600542405
Epoch 1155/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.6225705981254578
Epoch: 1155, Batch Gradient Norm: 12.967685645206704
Epoch: 1155, Batch Gradient Norm after: 12.967685645206704
Epoch 1156/10000, Prediction Accuracy = 59.734%, Loss = 0.6254578948020935
Epoch: 1156, Batch Gradient Norm: 13.031771864737179
Epoch: 1156, Batch Gradient Norm after: 13.031771864737179
Epoch 1157/10000, Prediction Accuracy = 59.70399999999999%, Loss = 0.6240476369857788
Epoch: 1157, Batch Gradient Norm: 9.781262293265977
Epoch: 1157, Batch Gradient Norm after: 9.781262293265977
Epoch 1158/10000, Prediction Accuracy = 59.662%, Loss = 0.6031175374984741
Epoch: 1158, Batch Gradient Norm: 11.891830811706582
Epoch: 1158, Batch Gradient Norm after: 11.891830811706582
Epoch 1159/10000, Prediction Accuracy = 59.705999999999996%, Loss = 0.6142442703247071
Epoch: 1159, Batch Gradient Norm: 12.380659894536949
Epoch: 1159, Batch Gradient Norm after: 12.380659894536949
Epoch 1160/10000, Prediction Accuracy = 59.678%, Loss = 0.6186920642852783
Epoch: 1160, Batch Gradient Norm: 10.518536158626688
Epoch: 1160, Batch Gradient Norm after: 10.518536158626688
Epoch 1161/10000, Prediction Accuracy = 59.70799999999999%, Loss = 0.607338547706604
Epoch: 1161, Batch Gradient Norm: 10.467112940988722
Epoch: 1161, Batch Gradient Norm after: 10.467112940988722
Epoch 1162/10000, Prediction Accuracy = 59.779999999999994%, Loss = 0.6069968819618226
Epoch: 1162, Batch Gradient Norm: 12.046675009590714
Epoch: 1162, Batch Gradient Norm after: 12.046675009590714
Epoch 1163/10000, Prediction Accuracy = 59.766000000000005%, Loss = 0.6157458662986756
Epoch: 1163, Batch Gradient Norm: 12.059239504772368
Epoch: 1163, Batch Gradient Norm after: 12.059239504772368
Epoch 1164/10000, Prediction Accuracy = 59.71%, Loss = 0.6166952133178711
Epoch: 1164, Batch Gradient Norm: 10.29049486609585
Epoch: 1164, Batch Gradient Norm after: 10.29049486609585
Epoch 1165/10000, Prediction Accuracy = 59.778%, Loss = 0.6048766255378724
Epoch: 1165, Batch Gradient Norm: 11.389408717256732
Epoch: 1165, Batch Gradient Norm after: 11.389408717256732
Epoch 1166/10000, Prediction Accuracy = 59.67%, Loss = 0.6107887744903564
Epoch: 1166, Batch Gradient Norm: 14.54297207351923
Epoch: 1166, Batch Gradient Norm after: 14.54297207351923
Epoch 1167/10000, Prediction Accuracy = 59.766%, Loss = 0.6334102392196655
Epoch: 1167, Batch Gradient Norm: 15.042094465701574
Epoch: 1167, Batch Gradient Norm after: 15.042094465701574
Epoch 1168/10000, Prediction Accuracy = 59.646%, Loss = 0.637913954257965
Epoch: 1168, Batch Gradient Norm: 14.136944906667212
Epoch: 1168, Batch Gradient Norm after: 14.136944906667212
Epoch 1169/10000, Prediction Accuracy = 59.798%, Loss = 0.6320831418037415
Epoch: 1169, Batch Gradient Norm: 12.591806477033808
Epoch: 1169, Batch Gradient Norm after: 12.591806477033808
Epoch 1170/10000, Prediction Accuracy = 59.77%, Loss = 0.6212388157844544
Epoch: 1170, Batch Gradient Norm: 10.027631149090178
Epoch: 1170, Batch Gradient Norm after: 10.027631149090178
Epoch 1171/10000, Prediction Accuracy = 59.746%, Loss = 0.6036019444465637
Epoch: 1171, Batch Gradient Norm: 13.408416228960894
Epoch: 1171, Batch Gradient Norm after: 13.408416228960894
Epoch 1172/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.6237608313560485
Epoch: 1172, Batch Gradient Norm: 11.894708707047029
Epoch: 1172, Batch Gradient Norm after: 11.894708707047029
Epoch 1173/10000, Prediction Accuracy = 59.712%, Loss = 0.612955915927887
Epoch: 1173, Batch Gradient Norm: 11.637616982725746
Epoch: 1173, Batch Gradient Norm after: 11.637616982725746
Epoch 1174/10000, Prediction Accuracy = 59.73%, Loss = 0.6118783950805664
Epoch: 1174, Batch Gradient Norm: 9.913538846934818
Epoch: 1174, Batch Gradient Norm after: 9.913538846934818
Epoch 1175/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.6019729614257813
Epoch: 1175, Batch Gradient Norm: 11.329220165789348
Epoch: 1175, Batch Gradient Norm after: 11.329220165789348
Epoch 1176/10000, Prediction Accuracy = 59.763999999999996%, Loss = 0.6109687566757203
Epoch: 1176, Batch Gradient Norm: 11.5687277814269
Epoch: 1176, Batch Gradient Norm after: 11.5687277814269
Epoch 1177/10000, Prediction Accuracy = 59.734%, Loss = 0.6123546004295349
Epoch: 1177, Batch Gradient Norm: 11.403803253952924
Epoch: 1177, Batch Gradient Norm after: 11.403803253952924
Epoch 1178/10000, Prediction Accuracy = 59.778%, Loss = 0.6114885568618774
Epoch: 1178, Batch Gradient Norm: 13.395944340137534
Epoch: 1178, Batch Gradient Norm after: 13.395944340137534
Epoch 1179/10000, Prediction Accuracy = 59.782%, Loss = 0.6248244285583496
Epoch: 1179, Batch Gradient Norm: 12.02462579488126
Epoch: 1179, Batch Gradient Norm after: 12.02462579488126
Epoch 1180/10000, Prediction Accuracy = 59.726%, Loss = 0.615773606300354
Epoch: 1180, Batch Gradient Norm: 13.60347557253835
Epoch: 1180, Batch Gradient Norm after: 13.60347557253835
Epoch 1181/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.627272081375122
Epoch: 1181, Batch Gradient Norm: 11.238224460791916
Epoch: 1181, Batch Gradient Norm after: 11.238224460791916
Epoch 1182/10000, Prediction Accuracy = 59.702%, Loss = 0.6112181425094605
Epoch: 1182, Batch Gradient Norm: 10.18327798698474
Epoch: 1182, Batch Gradient Norm after: 10.18327798698474
Epoch 1183/10000, Prediction Accuracy = 59.798%, Loss = 0.6033182144165039
Epoch: 1183, Batch Gradient Norm: 10.894864622146452
Epoch: 1183, Batch Gradient Norm after: 10.894864622146452
Epoch 1184/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.6068679571151734
Epoch: 1184, Batch Gradient Norm: 11.260284715179083
Epoch: 1184, Batch Gradient Norm after: 11.260284715179083
Epoch 1185/10000, Prediction Accuracy = 59.81%, Loss = 0.6087300181388855
Epoch: 1185, Batch Gradient Norm: 12.412393141290936
Epoch: 1185, Batch Gradient Norm after: 12.412393141290936
Epoch 1186/10000, Prediction Accuracy = 59.786%, Loss = 0.6167494177818298
Epoch: 1186, Batch Gradient Norm: 10.700060229044377
Epoch: 1186, Batch Gradient Norm after: 10.700060229044377
Epoch 1187/10000, Prediction Accuracy = 59.826%, Loss = 0.6063459753990174
Epoch: 1187, Batch Gradient Norm: 10.35099247734365
Epoch: 1187, Batch Gradient Norm after: 10.35099247734365
Epoch 1188/10000, Prediction Accuracy = 59.814%, Loss = 0.6042267203330993
Epoch: 1188, Batch Gradient Norm: 10.302463640463156
Epoch: 1188, Batch Gradient Norm after: 10.302463640463156
Epoch 1189/10000, Prediction Accuracy = 59.8%, Loss = 0.6037110805511474
Epoch: 1189, Batch Gradient Norm: 12.42597506189181
Epoch: 1189, Batch Gradient Norm after: 12.42597506189181
Epoch 1190/10000, Prediction Accuracy = 59.76400000000001%, Loss = 0.6164265036582947
Epoch: 1190, Batch Gradient Norm: 13.436711818196983
Epoch: 1190, Batch Gradient Norm after: 13.436711818196983
Epoch 1191/10000, Prediction Accuracy = 59.81%, Loss = 0.6244732022285462
Epoch: 1191, Batch Gradient Norm: 11.308115414160618
Epoch: 1191, Batch Gradient Norm after: 11.308115414160618
Epoch 1192/10000, Prediction Accuracy = 59.767999999999994%, Loss = 0.609190833568573
Epoch: 1192, Batch Gradient Norm: 14.279061370839852
Epoch: 1192, Batch Gradient Norm after: 14.279061370839852
Epoch 1193/10000, Prediction Accuracy = 59.794000000000004%, Loss = 0.6290235996246338
Epoch: 1193, Batch Gradient Norm: 14.775532794052067
Epoch: 1193, Batch Gradient Norm after: 14.775532794052067
Epoch 1194/10000, Prediction Accuracy = 59.846000000000004%, Loss = 0.6336894750595092
Epoch: 1194, Batch Gradient Norm: 13.99464089723257
Epoch: 1194, Batch Gradient Norm after: 13.99464089723257
Epoch 1195/10000, Prediction Accuracy = 59.739999999999995%, Loss = 0.6262660503387452
Epoch: 1195, Batch Gradient Norm: 12.381857846255551
Epoch: 1195, Batch Gradient Norm after: 12.381857846255551
Epoch 1196/10000, Prediction Accuracy = 59.75599999999999%, Loss = 0.6154161810874939
Epoch: 1196, Batch Gradient Norm: 11.760127932697744
Epoch: 1196, Batch Gradient Norm after: 11.760127932697744
Epoch 1197/10000, Prediction Accuracy = 59.751999999999995%, Loss = 0.6111852645874023
Epoch: 1197, Batch Gradient Norm: 10.62772018327961
Epoch: 1197, Batch Gradient Norm after: 10.62772018327961
Epoch 1198/10000, Prediction Accuracy = 59.79%, Loss = 0.604244875907898
Epoch: 1198, Batch Gradient Norm: 11.986013732075714
Epoch: 1198, Batch Gradient Norm after: 11.986013732075714
Epoch 1199/10000, Prediction Accuracy = 59.774%, Loss = 0.6119884014129638
Epoch: 1199, Batch Gradient Norm: 11.106880562837175
Epoch: 1199, Batch Gradient Norm after: 11.106880562837175
Epoch 1200/10000, Prediction Accuracy = 59.854%, Loss = 0.6075491189956665
Epoch: 1200, Batch Gradient Norm: 9.65709143502296
Epoch: 1200, Batch Gradient Norm after: 9.65709143502296
Epoch 1201/10000, Prediction Accuracy = 59.831999999999994%, Loss = 0.5986769080162049
Epoch: 1201, Batch Gradient Norm: 11.115436559176677
Epoch: 1201, Batch Gradient Norm after: 11.115436559176677
Epoch 1202/10000, Prediction Accuracy = 59.791999999999994%, Loss = 0.6063576340675354
Epoch: 1202, Batch Gradient Norm: 12.618664985827273
Epoch: 1202, Batch Gradient Norm after: 12.618664985827273
Epoch 1203/10000, Prediction Accuracy = 59.815999999999995%, Loss = 0.6150461196899414
Epoch: 1203, Batch Gradient Norm: 15.246839211044998
Epoch: 1203, Batch Gradient Norm after: 15.246839211044998
Epoch 1204/10000, Prediction Accuracy = 59.77%, Loss = 0.6366597056388855
Epoch: 1204, Batch Gradient Norm: 10.855071776637311
Epoch: 1204, Batch Gradient Norm after: 10.855071776637311
Epoch 1205/10000, Prediction Accuracy = 59.798%, Loss = 0.6063031554222107
Epoch: 1205, Batch Gradient Norm: 11.386256290061578
Epoch: 1205, Batch Gradient Norm after: 11.386256290061578
Epoch 1206/10000, Prediction Accuracy = 59.802%, Loss = 0.6083073854446411
Epoch: 1206, Batch Gradient Norm: 12.248195420856614
Epoch: 1206, Batch Gradient Norm after: 12.248195420856614
Epoch 1207/10000, Prediction Accuracy = 59.762%, Loss = 0.6135784149169922
Epoch: 1207, Batch Gradient Norm: 12.209319705848797
Epoch: 1207, Batch Gradient Norm after: 12.209319705848797
Epoch 1208/10000, Prediction Accuracy = 59.854%, Loss = 0.6134892702102661
Epoch: 1208, Batch Gradient Norm: 12.16693747993089
Epoch: 1208, Batch Gradient Norm after: 12.16693747993089
Epoch 1209/10000, Prediction Accuracy = 59.81%, Loss = 0.6129051566123962
Epoch: 1209, Batch Gradient Norm: 10.199564616960933
Epoch: 1209, Batch Gradient Norm after: 10.199564616960933
Epoch 1210/10000, Prediction Accuracy = 59.864%, Loss = 0.6008188962936402
Epoch: 1210, Batch Gradient Norm: 10.75373975608267
Epoch: 1210, Batch Gradient Norm after: 10.75373975608267
Epoch 1211/10000, Prediction Accuracy = 59.778%, Loss = 0.6040171623229981
Epoch: 1211, Batch Gradient Norm: 9.211388565374646
Epoch: 1211, Batch Gradient Norm after: 9.211388565374646
Epoch 1212/10000, Prediction Accuracy = 59.886%, Loss = 0.5955663800239563
Epoch: 1212, Batch Gradient Norm: 11.326590558914427
Epoch: 1212, Batch Gradient Norm after: 11.326590558914427
Epoch 1213/10000, Prediction Accuracy = 59.774%, Loss = 0.6071528792381287
Epoch: 1213, Batch Gradient Norm: 13.154122071413864
Epoch: 1213, Batch Gradient Norm after: 13.154122071413864
Epoch 1214/10000, Prediction Accuracy = 59.818%, Loss = 0.6184973001480103
Epoch: 1214, Batch Gradient Norm: 14.491518019302852
Epoch: 1214, Batch Gradient Norm after: 14.491518019302852
Epoch 1215/10000, Prediction Accuracy = 59.842%, Loss = 0.6276555061340332
Epoch: 1215, Batch Gradient Norm: 13.162200549524599
Epoch: 1215, Batch Gradient Norm after: 13.162200549524599
Epoch 1216/10000, Prediction Accuracy = 59.78000000000001%, Loss = 0.6176817893981934
Epoch: 1216, Batch Gradient Norm: 12.410910036790032
Epoch: 1216, Batch Gradient Norm after: 12.410910036790032
Epoch 1217/10000, Prediction Accuracy = 59.748000000000005%, Loss = 0.6140745401382446
Epoch: 1217, Batch Gradient Norm: 10.12900040946864
Epoch: 1217, Batch Gradient Norm after: 10.12900040946864
Epoch 1218/10000, Prediction Accuracy = 59.81600000000001%, Loss = 0.5999037623405457
Epoch: 1218, Batch Gradient Norm: 10.470140631182506
Epoch: 1218, Batch Gradient Norm after: 10.470140631182506
Epoch 1219/10000, Prediction Accuracy = 59.757999999999996%, Loss = 0.6012514472007752
Epoch: 1219, Batch Gradient Norm: 10.47379514168158
Epoch: 1219, Batch Gradient Norm after: 10.47379514168158
Epoch 1220/10000, Prediction Accuracy = 59.798%, Loss = 0.6011961460113525
Epoch: 1220, Batch Gradient Norm: 11.68314167639874
Epoch: 1220, Batch Gradient Norm after: 11.68314167639874
Epoch 1221/10000, Prediction Accuracy = 59.814%, Loss = 0.6081884145736695
Epoch: 1221, Batch Gradient Norm: 10.803427599236292
Epoch: 1221, Batch Gradient Norm after: 10.803427599236292
Epoch 1222/10000, Prediction Accuracy = 59.806%, Loss = 0.602644395828247
Epoch: 1222, Batch Gradient Norm: 14.852905781844898
Epoch: 1222, Batch Gradient Norm after: 14.852905781844898
Epoch 1223/10000, Prediction Accuracy = 59.812%, Loss = 0.6287894606590271
Epoch: 1223, Batch Gradient Norm: 14.315397024596301
Epoch: 1223, Batch Gradient Norm after: 14.315397024596301
Epoch 1224/10000, Prediction Accuracy = 59.76800000000001%, Loss = 0.628105616569519
Epoch: 1224, Batch Gradient Norm: 11.170218306633885
Epoch: 1224, Batch Gradient Norm after: 11.170218306633885
Epoch 1225/10000, Prediction Accuracy = 59.874%, Loss = 0.6043526291847229
Epoch: 1225, Batch Gradient Norm: 11.451681407186829
Epoch: 1225, Batch Gradient Norm after: 11.451681407186829
Epoch 1226/10000, Prediction Accuracy = 59.803999999999995%, Loss = 0.6050086140632629
Epoch: 1226, Batch Gradient Norm: 14.0759037076386
Epoch: 1226, Batch Gradient Norm after: 14.0759037076386
Epoch 1227/10000, Prediction Accuracy = 59.872%, Loss = 0.6241928577423096
Epoch: 1227, Batch Gradient Norm: 11.248139165265561
Epoch: 1227, Batch Gradient Norm after: 11.248139165265561
Epoch 1228/10000, Prediction Accuracy = 59.802%, Loss = 0.6056952357292176
Epoch: 1228, Batch Gradient Norm: 10.797455414940115
Epoch: 1228, Batch Gradient Norm after: 10.797455414940115
Epoch 1229/10000, Prediction Accuracy = 59.862%, Loss = 0.603185510635376
Epoch: 1229, Batch Gradient Norm: 11.317365526422716
Epoch: 1229, Batch Gradient Norm after: 11.317365526422716
Epoch 1230/10000, Prediction Accuracy = 59.782000000000004%, Loss = 0.6051077485084534
Epoch: 1230, Batch Gradient Norm: 15.904870156261603
Epoch: 1230, Batch Gradient Norm after: 15.904870156261603
Epoch 1231/10000, Prediction Accuracy = 59.760000000000005%, Loss = 0.640311849117279
Epoch: 1231, Batch Gradient Norm: 12.326532680379406
Epoch: 1231, Batch Gradient Norm after: 12.326532680379406
Epoch 1232/10000, Prediction Accuracy = 59.83399999999999%, Loss = 0.6153017401695251
Epoch: 1232, Batch Gradient Norm: 8.551900907142965
Epoch: 1232, Batch Gradient Norm after: 8.551900907142965
Epoch 1233/10000, Prediction Accuracy = 59.824%, Loss = 0.591776430606842
Epoch: 1233, Batch Gradient Norm: 7.988042414099709
Epoch: 1233, Batch Gradient Norm after: 7.988042414099709
Epoch 1234/10000, Prediction Accuracy = 59.878%, Loss = 0.5884045362472534
Epoch: 1234, Batch Gradient Norm: 8.493172482518887
Epoch: 1234, Batch Gradient Norm after: 8.493172482518887
Epoch 1235/10000, Prediction Accuracy = 59.81%, Loss = 0.5900045394897461
Epoch: 1235, Batch Gradient Norm: 11.086095386615735
Epoch: 1235, Batch Gradient Norm after: 11.086095386615735
Epoch 1236/10000, Prediction Accuracy = 59.848%, Loss = 0.6039402723312378
Epoch: 1236, Batch Gradient Norm: 13.459826556472002
Epoch: 1236, Batch Gradient Norm after: 13.459826556472002
Epoch 1237/10000, Prediction Accuracy = 59.848%, Loss = 0.6187362909317017
Epoch: 1237, Batch Gradient Norm: 13.962350652674493
Epoch: 1237, Batch Gradient Norm after: 13.962350652674493
Epoch 1238/10000, Prediction Accuracy = 59.843999999999994%, Loss = 0.6220410823822021
Epoch: 1238, Batch Gradient Norm: 12.051178811491457
Epoch: 1238, Batch Gradient Norm after: 12.051178811491457
Epoch 1239/10000, Prediction Accuracy = 59.838%, Loss = 0.6081390142440796
Epoch: 1239, Batch Gradient Norm: 9.388457033775857
Epoch: 1239, Batch Gradient Norm after: 9.388457033775857
Epoch 1240/10000, Prediction Accuracy = 59.86%, Loss = 0.5920428037643433
Epoch: 1240, Batch Gradient Norm: 11.726116689763405
Epoch: 1240, Batch Gradient Norm after: 11.726116689763405
Epoch 1241/10000, Prediction Accuracy = 59.788%, Loss = 0.6057898044586182
Epoch: 1241, Batch Gradient Norm: 12.945458668160366
Epoch: 1241, Batch Gradient Norm after: 12.945458668160366
Epoch 1242/10000, Prediction Accuracy = 59.788%, Loss = 0.6139383912086487
Epoch: 1242, Batch Gradient Norm: 13.757602640054003
Epoch: 1242, Batch Gradient Norm after: 13.757602640054003
Epoch 1243/10000, Prediction Accuracy = 59.878%, Loss = 0.6220405220985412
Epoch: 1243, Batch Gradient Norm: 10.00768482180693
Epoch: 1243, Batch Gradient Norm after: 10.00768482180693
Epoch 1244/10000, Prediction Accuracy = 59.938%, Loss = 0.5966180801391602
Epoch: 1244, Batch Gradient Norm: 11.49349815082817
Epoch: 1244, Batch Gradient Norm after: 11.49349815082817
Epoch 1245/10000, Prediction Accuracy = 59.866%, Loss = 0.6044422507286071
Epoch: 1245, Batch Gradient Norm: 12.97657146200183
Epoch: 1245, Batch Gradient Norm after: 12.97657146200183
Epoch 1246/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.6141469001770019
Epoch: 1246, Batch Gradient Norm: 13.347777738402671
Epoch: 1246, Batch Gradient Norm after: 13.347777738402671
Epoch 1247/10000, Prediction Accuracy = 59.862%, Loss = 0.6176428914070129
Epoch: 1247, Batch Gradient Norm: 11.471742391356267
Epoch: 1247, Batch Gradient Norm after: 11.471742391356267
Epoch 1248/10000, Prediction Accuracy = 59.874%, Loss = 0.6050950527191162
Epoch: 1248, Batch Gradient Norm: 11.85043126249805
Epoch: 1248, Batch Gradient Norm after: 11.85043126249805
Epoch 1249/10000, Prediction Accuracy = 59.89%, Loss = 0.6070998430252075
Epoch: 1249, Batch Gradient Norm: 12.963754809210515
Epoch: 1249, Batch Gradient Norm after: 12.963754809210515
Epoch 1250/10000, Prediction Accuracy = 59.82000000000001%, Loss = 0.6148912072181701
Epoch: 1250, Batch Gradient Norm: 14.415540406599854
Epoch: 1250, Batch Gradient Norm after: 14.415540406599854
Epoch 1251/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.6266183257102966
Epoch: 1251, Batch Gradient Norm: 11.98621048686279
Epoch: 1251, Batch Gradient Norm after: 11.98621048686279
Epoch 1252/10000, Prediction Accuracy = 59.822%, Loss = 0.6087262749671936
Epoch: 1252, Batch Gradient Norm: 12.2107789927432
Epoch: 1252, Batch Gradient Norm after: 12.2107789927432
Epoch 1253/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.6092634439468384
Epoch: 1253, Batch Gradient Norm: 9.783699137547853
Epoch: 1253, Batch Gradient Norm after: 9.783699137547853
Epoch 1254/10000, Prediction Accuracy = 59.830000000000005%, Loss = 0.5946603655815125
Epoch: 1254, Batch Gradient Norm: 10.007149246555082
Epoch: 1254, Batch Gradient Norm after: 10.007149246555082
Epoch 1255/10000, Prediction Accuracy = 59.867999999999995%, Loss = 0.5944751739501953
Epoch: 1255, Batch Gradient Norm: 10.792428937503361
Epoch: 1255, Batch Gradient Norm after: 10.792428937503361
Epoch 1256/10000, Prediction Accuracy = 59.85%, Loss = 0.5991200804710388
Epoch: 1256, Batch Gradient Norm: 12.673168307281507
Epoch: 1256, Batch Gradient Norm after: 12.673168307281507
Epoch 1257/10000, Prediction Accuracy = 59.842000000000006%, Loss = 0.6103557705879211
Epoch: 1257, Batch Gradient Norm: 12.372226467696109
Epoch: 1257, Batch Gradient Norm after: 12.372226467696109
Epoch 1258/10000, Prediction Accuracy = 59.806%, Loss = 0.6093939900398254
Epoch: 1258, Batch Gradient Norm: 10.330734521704624
Epoch: 1258, Batch Gradient Norm after: 10.330734521704624
Epoch 1259/10000, Prediction Accuracy = 59.826%, Loss = 0.5960873246192933
Epoch: 1259, Batch Gradient Norm: 12.262464800641515
Epoch: 1259, Batch Gradient Norm after: 12.262464800641515
Epoch 1260/10000, Prediction Accuracy = 59.838%, Loss = 0.6062921762466431
Epoch: 1260, Batch Gradient Norm: 15.301581365344486
Epoch: 1260, Batch Gradient Norm after: 15.301581365344486
Epoch 1261/10000, Prediction Accuracy = 59.822%, Loss = 0.6348337173461914
Epoch: 1261, Batch Gradient Norm: 9.353225954907817
Epoch: 1261, Batch Gradient Norm after: 9.353225954907817
Epoch 1262/10000, Prediction Accuracy = 59.919999999999995%, Loss = 0.5933185935020446
Epoch: 1262, Batch Gradient Norm: 10.651987661369153
Epoch: 1262, Batch Gradient Norm after: 10.651987661369153
Epoch 1263/10000, Prediction Accuracy = 59.895999999999994%, Loss = 0.5998417258262634
Epoch: 1263, Batch Gradient Norm: 10.249691670965598
Epoch: 1263, Batch Gradient Norm after: 10.249691670965598
Epoch 1264/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.5969267845153808
Epoch: 1264, Batch Gradient Norm: 11.101703935639263
Epoch: 1264, Batch Gradient Norm after: 11.101703935639263
Epoch 1265/10000, Prediction Accuracy = 59.896%, Loss = 0.6010890245437622
Epoch: 1265, Batch Gradient Norm: 10.404011757348917
Epoch: 1265, Batch Gradient Norm after: 10.404011757348917
Epoch 1266/10000, Prediction Accuracy = 59.86%, Loss = 0.595752215385437
Epoch: 1266, Batch Gradient Norm: 13.518108540387573
Epoch: 1266, Batch Gradient Norm after: 13.518108540387573
Epoch 1267/10000, Prediction Accuracy = 59.912%, Loss = 0.6142483115196228
Epoch: 1267, Batch Gradient Norm: 14.833529848174466
Epoch: 1267, Batch Gradient Norm after: 14.833529848174466
Epoch 1268/10000, Prediction Accuracy = 59.92%, Loss = 0.6258418917655945
Epoch: 1268, Batch Gradient Norm: 12.515639888463095
Epoch: 1268, Batch Gradient Norm after: 12.515639888463095
Epoch 1269/10000, Prediction Accuracy = 59.895999999999994%, Loss = 0.6107080936431885
Epoch: 1269, Batch Gradient Norm: 10.979470529798046
Epoch: 1269, Batch Gradient Norm after: 10.979470529798046
Epoch 1270/10000, Prediction Accuracy = 59.855999999999995%, Loss = 0.6005619764328003
Epoch: 1270, Batch Gradient Norm: 9.884815169534113
Epoch: 1270, Batch Gradient Norm after: 9.884815169534113
Epoch 1271/10000, Prediction Accuracy = 59.874%, Loss = 0.5940250873565673
Epoch: 1271, Batch Gradient Norm: 10.117158057584646
Epoch: 1271, Batch Gradient Norm after: 10.117158057584646
Epoch 1272/10000, Prediction Accuracy = 59.89%, Loss = 0.5941953420639038
Epoch: 1272, Batch Gradient Norm: 11.272565751727978
Epoch: 1272, Batch Gradient Norm after: 11.272565751727978
Epoch 1273/10000, Prediction Accuracy = 59.86999999999999%, Loss = 0.5999633312225342
Epoch: 1273, Batch Gradient Norm: 15.57261014839686
Epoch: 1273, Batch Gradient Norm after: 15.57261014839686
Epoch 1274/10000, Prediction Accuracy = 59.836%, Loss = 0.6315858840942383
Epoch: 1274, Batch Gradient Norm: 11.052064038407316
Epoch: 1274, Batch Gradient Norm after: 11.052064038407316
Epoch 1275/10000, Prediction Accuracy = 59.836%, Loss = 0.599342405796051
Epoch: 1275, Batch Gradient Norm: 9.88851667531572
Epoch: 1275, Batch Gradient Norm after: 9.88851667531572
Epoch 1276/10000, Prediction Accuracy = 59.8%, Loss = 0.5918737292289734
Epoch: 1276, Batch Gradient Norm: 12.429905677104546
Epoch: 1276, Batch Gradient Norm after: 12.429905677104546
Epoch 1277/10000, Prediction Accuracy = 59.89200000000001%, Loss = 0.6083719491958618
Epoch: 1277, Batch Gradient Norm: 13.014112103995949
Epoch: 1277, Batch Gradient Norm after: 13.014112103995949
Epoch 1278/10000, Prediction Accuracy = 59.843999999999994%, Loss = 0.6152314901351928
Epoch: 1278, Batch Gradient Norm: 10.970280730279317
Epoch: 1278, Batch Gradient Norm after: 10.970280730279317
Epoch 1279/10000, Prediction Accuracy = 59.86%, Loss = 0.6004251837730408
Epoch: 1279, Batch Gradient Norm: 14.066023227476459
Epoch: 1279, Batch Gradient Norm after: 14.066023227476459
Epoch 1280/10000, Prediction Accuracy = 59.89399999999999%, Loss = 0.6204348683357239
Epoch: 1280, Batch Gradient Norm: 13.17545126410712
Epoch: 1280, Batch Gradient Norm after: 13.17545126410712
Epoch 1281/10000, Prediction Accuracy = 59.88000000000001%, Loss = 0.6144842982292176
Epoch: 1281, Batch Gradient Norm: 10.416469586821863
Epoch: 1281, Batch Gradient Norm after: 10.416469586821863
Epoch 1282/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.5963942289352417
Epoch: 1282, Batch Gradient Norm: 10.758147604152452
Epoch: 1282, Batch Gradient Norm after: 10.758147604152452
Epoch 1283/10000, Prediction Accuracy = 59.936%, Loss = 0.5974410057067872
Epoch: 1283, Batch Gradient Norm: 11.593550832044922
Epoch: 1283, Batch Gradient Norm after: 11.593550832044922
Epoch 1284/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.6026858568191529
Epoch: 1284, Batch Gradient Norm: 11.923389058815674
Epoch: 1284, Batch Gradient Norm after: 11.923389058815674
Epoch 1285/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.60500168800354
Epoch: 1285, Batch Gradient Norm: 10.990903195189809
Epoch: 1285, Batch Gradient Norm after: 10.990903195189809
Epoch 1286/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.5984193563461304
Epoch: 1286, Batch Gradient Norm: 11.171449149920077
Epoch: 1286, Batch Gradient Norm after: 11.171449149920077
Epoch 1287/10000, Prediction Accuracy = 59.912%, Loss = 0.5989821910858154
Epoch: 1287, Batch Gradient Norm: 12.716323564687551
Epoch: 1287, Batch Gradient Norm after: 12.716323564687551
Epoch 1288/10000, Prediction Accuracy = 59.914%, Loss = 0.6079931139945984
Epoch: 1288, Batch Gradient Norm: 14.09639536341538
Epoch: 1288, Batch Gradient Norm after: 14.09639536341538
Epoch 1289/10000, Prediction Accuracy = 59.902%, Loss = 0.6181042551994324
Epoch: 1289, Batch Gradient Norm: 13.701936489766288
Epoch: 1289, Batch Gradient Norm after: 13.701936489766288
Epoch 1290/10000, Prediction Accuracy = 59.948%, Loss = 0.6153537631034851
Epoch: 1290, Batch Gradient Norm: 11.141935586390368
Epoch: 1290, Batch Gradient Norm after: 11.141935586390368
Epoch 1291/10000, Prediction Accuracy = 59.896%, Loss = 0.5973639488220215
Epoch: 1291, Batch Gradient Norm: 11.83647046142912
Epoch: 1291, Batch Gradient Norm after: 11.83647046142912
Epoch 1292/10000, Prediction Accuracy = 59.96999999999999%, Loss = 0.6022103309631348
Epoch: 1292, Batch Gradient Norm: 12.503445123509382
Epoch: 1292, Batch Gradient Norm after: 12.503445123509382
Epoch 1293/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.607786500453949
Epoch: 1293, Batch Gradient Norm: 10.281732178774096
Epoch: 1293, Batch Gradient Norm after: 10.281732178774096
Epoch 1294/10000, Prediction Accuracy = 59.976%, Loss = 0.5942001581192017
Epoch: 1294, Batch Gradient Norm: 10.840458774560146
Epoch: 1294, Batch Gradient Norm after: 10.840458774560146
Epoch 1295/10000, Prediction Accuracy = 59.896%, Loss = 0.5977963089942933
Epoch: 1295, Batch Gradient Norm: 9.058058287868068
Epoch: 1295, Batch Gradient Norm after: 9.058058287868068
Epoch 1296/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.5869994163513184
Epoch: 1296, Batch Gradient Norm: 11.100171580491656
Epoch: 1296, Batch Gradient Norm after: 11.100171580491656
Epoch 1297/10000, Prediction Accuracy = 59.86800000000001%, Loss = 0.5990925550460815
Epoch: 1297, Batch Gradient Norm: 12.044529724308449
Epoch: 1297, Batch Gradient Norm after: 12.044529724308449
Epoch 1298/10000, Prediction Accuracy = 59.96%, Loss = 0.6044608592987061
Epoch: 1298, Batch Gradient Norm: 12.44922992399611
Epoch: 1298, Batch Gradient Norm after: 12.44922992399611
Epoch 1299/10000, Prediction Accuracy = 59.874%, Loss = 0.6081910848617553
Epoch: 1299, Batch Gradient Norm: 11.763316592855018
Epoch: 1299, Batch Gradient Norm after: 11.763316592855018
Epoch 1300/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.602026617527008
Epoch: 1300, Batch Gradient Norm: 11.095185100816986
Epoch: 1300, Batch Gradient Norm after: 11.095185100816986
Epoch 1301/10000, Prediction Accuracy = 59.903999999999996%, Loss = 0.5973299026489258
Epoch: 1301, Batch Gradient Norm: 14.61664820657392
Epoch: 1301, Batch Gradient Norm after: 14.61664820657392
Epoch 1302/10000, Prediction Accuracy = 59.838%, Loss = 0.6212320446968078
Epoch: 1302, Batch Gradient Norm: 12.425441671036108
Epoch: 1302, Batch Gradient Norm after: 12.425441671036108
Epoch 1303/10000, Prediction Accuracy = 59.839999999999996%, Loss = 0.6040164709091187
Epoch: 1303, Batch Gradient Norm: 11.602958596946992
Epoch: 1303, Batch Gradient Norm after: 11.602958596946992
Epoch 1304/10000, Prediction Accuracy = 59.81%, Loss = 0.5980075359344482
Epoch: 1304, Batch Gradient Norm: 12.107687426567503
Epoch: 1304, Batch Gradient Norm after: 12.107687426567503
Epoch 1305/10000, Prediction Accuracy = 59.928%, Loss = 0.6007760286331176
Epoch: 1305, Batch Gradient Norm: 14.542041736709818
Epoch: 1305, Batch Gradient Norm after: 14.542041736709818
Epoch 1306/10000, Prediction Accuracy = 59.836%, Loss = 0.6224564790725708
Epoch: 1306, Batch Gradient Norm: 9.911220415436217
Epoch: 1306, Batch Gradient Norm after: 9.911220415436217
Epoch 1307/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.591494870185852
Epoch: 1307, Batch Gradient Norm: 9.857680982587082
Epoch: 1307, Batch Gradient Norm after: 9.857680982587082
Epoch 1308/10000, Prediction Accuracy = 59.936%, Loss = 0.5900027036666871
Epoch: 1308, Batch Gradient Norm: 11.925581281892063
Epoch: 1308, Batch Gradient Norm after: 11.925581281892063
Epoch 1309/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.6017253279685975
Epoch: 1309, Batch Gradient Norm: 12.562480116447889
Epoch: 1309, Batch Gradient Norm after: 12.562480116447889
Epoch 1310/10000, Prediction Accuracy = 59.90599999999999%, Loss = 0.6064173460006714
Epoch: 1310, Batch Gradient Norm: 10.85418900387988
Epoch: 1310, Batch Gradient Norm after: 10.85418900387988
Epoch 1311/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.5950731754302978
Epoch: 1311, Batch Gradient Norm: 12.806395367710024
Epoch: 1311, Batch Gradient Norm after: 12.806395367710024
Epoch 1312/10000, Prediction Accuracy = 60.007999999999996%, Loss = 0.6080404639244079
Epoch: 1312, Batch Gradient Norm: 12.239194546180547
Epoch: 1312, Batch Gradient Norm after: 12.239194546180547
Epoch 1313/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6055845141410827
Epoch: 1313, Batch Gradient Norm: 10.860159795645234
Epoch: 1313, Batch Gradient Norm after: 10.860159795645234
Epoch 1314/10000, Prediction Accuracy = 60.017999999999994%, Loss = 0.5954412937164306
Epoch: 1314, Batch Gradient Norm: 12.226394897647213
Epoch: 1314, Batch Gradient Norm after: 12.226394897647213
Epoch 1315/10000, Prediction Accuracy = 59.938%, Loss = 0.6024192571640015
Epoch: 1315, Batch Gradient Norm: 14.652388953804309
Epoch: 1315, Batch Gradient Norm after: 14.652388953804309
Epoch 1316/10000, Prediction Accuracy = 59.948%, Loss = 0.6211100935935974
Epoch: 1316, Batch Gradient Norm: 12.61766031001332
Epoch: 1316, Batch Gradient Norm after: 12.61766031001332
Epoch 1317/10000, Prediction Accuracy = 59.876%, Loss = 0.6079571843147278
Epoch: 1317, Batch Gradient Norm: 10.430479518062004
Epoch: 1317, Batch Gradient Norm after: 10.430479518062004
Epoch 1318/10000, Prediction Accuracy = 59.972%, Loss = 0.5927907228469849
Epoch: 1318, Batch Gradient Norm: 8.93938604293405
Epoch: 1318, Batch Gradient Norm after: 8.93938604293405
Epoch 1319/10000, Prediction Accuracy = 59.907999999999994%, Loss = 0.5841799139976501
Epoch: 1319, Batch Gradient Norm: 11.713063196882803
Epoch: 1319, Batch Gradient Norm after: 11.713063196882803
Epoch 1320/10000, Prediction Accuracy = 59.956%, Loss = 0.5990251064300537
Epoch: 1320, Batch Gradient Norm: 11.437609166035012
Epoch: 1320, Batch Gradient Norm after: 11.437609166035012
Epoch 1321/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.5983102083206177
Epoch: 1321, Batch Gradient Norm: 11.61810115441485
Epoch: 1321, Batch Gradient Norm after: 11.61810115441485
Epoch 1322/10000, Prediction Accuracy = 59.936%, Loss = 0.599474835395813
Epoch: 1322, Batch Gradient Norm: 8.863177297988369
Epoch: 1322, Batch Gradient Norm after: 8.863177297988369
Epoch 1323/10000, Prediction Accuracy = 59.931999999999995%, Loss = 0.5837469458580017
Epoch: 1323, Batch Gradient Norm: 10.152809655734572
Epoch: 1323, Batch Gradient Norm after: 10.152809655734572
Epoch 1324/10000, Prediction Accuracy = 59.92999999999999%, Loss = 0.5898206114768982
Epoch: 1324, Batch Gradient Norm: 11.473406081965019
Epoch: 1324, Batch Gradient Norm after: 11.473406081965019
Epoch 1325/10000, Prediction Accuracy = 59.916%, Loss = 0.5963950753211975
Epoch: 1325, Batch Gradient Norm: 14.934726287957321
Epoch: 1325, Batch Gradient Norm after: 14.934726287957321
Epoch 1326/10000, Prediction Accuracy = 59.879999999999995%, Loss = 0.6199982166290283
Epoch: 1326, Batch Gradient Norm: 12.802993452578253
Epoch: 1326, Batch Gradient Norm after: 12.802993452578253
Epoch 1327/10000, Prediction Accuracy = 59.89%, Loss = 0.6050411581993103
Epoch: 1327, Batch Gradient Norm: 11.262439714199084
Epoch: 1327, Batch Gradient Norm after: 11.262439714199084
Epoch 1328/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5944596767425537
Epoch: 1328, Batch Gradient Norm: 14.177866104696362
Epoch: 1328, Batch Gradient Norm after: 14.177866104696362
Epoch 1329/10000, Prediction Accuracy = 59.89%, Loss = 0.6167321324348449
Epoch: 1329, Batch Gradient Norm: 12.36002459959874
Epoch: 1329, Batch Gradient Norm after: 12.36002459959874
Epoch 1330/10000, Prediction Accuracy = 60.019999999999996%, Loss = 0.6051607489585876
Epoch: 1330, Batch Gradient Norm: 11.113381588198624
Epoch: 1330, Batch Gradient Norm after: 11.113381588198624
Epoch 1331/10000, Prediction Accuracy = 59.924%, Loss = 0.5962047457695008
Epoch: 1331, Batch Gradient Norm: 9.98712737468546
Epoch: 1331, Batch Gradient Norm after: 9.98712737468546
Epoch 1332/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.5887052893638611
Epoch: 1332, Batch Gradient Norm: 10.751872988436054
Epoch: 1332, Batch Gradient Norm after: 10.751872988436054
Epoch 1333/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.5927729368209839
Epoch: 1333, Batch Gradient Norm: 10.789228500904928
Epoch: 1333, Batch Gradient Norm after: 10.789228500904928
Epoch 1334/10000, Prediction Accuracy = 59.948%, Loss = 0.5931203842163086
Epoch: 1334, Batch Gradient Norm: 12.836644177331168
Epoch: 1334, Batch Gradient Norm after: 12.836644177331168
Epoch 1335/10000, Prediction Accuracy = 59.96600000000001%, Loss = 0.605163323879242
Epoch: 1335, Batch Gradient Norm: 13.790038947692612
Epoch: 1335, Batch Gradient Norm after: 13.790038947692612
Epoch 1336/10000, Prediction Accuracy = 59.922000000000004%, Loss = 0.6126798033714295
Epoch: 1336, Batch Gradient Norm: 13.570965078252831
Epoch: 1336, Batch Gradient Norm after: 13.570965078252831
Epoch 1337/10000, Prediction Accuracy = 59.908%, Loss = 0.6121309041976929
Epoch: 1337, Batch Gradient Norm: 9.659346611799693
Epoch: 1337, Batch Gradient Norm after: 9.659346611799693
Epoch 1338/10000, Prediction Accuracy = 59.984%, Loss = 0.5864213466644287
Epoch: 1338, Batch Gradient Norm: 11.321341383954655
Epoch: 1338, Batch Gradient Norm after: 11.321341383954655
Epoch 1339/10000, Prediction Accuracy = 59.936%, Loss = 0.5959283590316773
Epoch: 1339, Batch Gradient Norm: 15.51964940943369
Epoch: 1339, Batch Gradient Norm after: 15.51964940943369
Epoch 1340/10000, Prediction Accuracy = 60.012%, Loss = 0.6290050864219665
Epoch: 1340, Batch Gradient Norm: 12.276213337056797
Epoch: 1340, Batch Gradient Norm after: 12.276213337056797
Epoch 1341/10000, Prediction Accuracy = 59.992%, Loss = 0.6049219250679017
Epoch: 1341, Batch Gradient Norm: 9.138243703716181
Epoch: 1341, Batch Gradient Norm after: 9.138243703716181
Epoch 1342/10000, Prediction Accuracy = 60.048%, Loss = 0.5846527338027954
Epoch: 1342, Batch Gradient Norm: 7.554758316684094
Epoch: 1342, Batch Gradient Norm after: 7.554758316684094
Epoch 1343/10000, Prediction Accuracy = 59.976%, Loss = 0.5762312412261963
Epoch: 1343, Batch Gradient Norm: 9.157333081986167
Epoch: 1343, Batch Gradient Norm after: 9.157333081986167
Epoch 1344/10000, Prediction Accuracy = 59.962%, Loss = 0.5837186455726624
Epoch: 1344, Batch Gradient Norm: 10.919324540889448
Epoch: 1344, Batch Gradient Norm after: 10.919324540889448
Epoch 1345/10000, Prediction Accuracy = 59.938%, Loss = 0.5923316597938537
Epoch: 1345, Batch Gradient Norm: 14.512182031058206
Epoch: 1345, Batch Gradient Norm after: 14.512182031058206
Epoch 1346/10000, Prediction Accuracy = 59.948%, Loss = 0.6195182204246521
Epoch: 1346, Batch Gradient Norm: 13.0714240491861
Epoch: 1346, Batch Gradient Norm after: 13.0714240491861
Epoch 1347/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6080662131309509
Epoch: 1347, Batch Gradient Norm: 10.394283669529514
Epoch: 1347, Batch Gradient Norm after: 10.394283669529514
Epoch 1348/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5890694379806518
Epoch: 1348, Batch Gradient Norm: 12.696331077667093
Epoch: 1348, Batch Gradient Norm after: 12.696331077667093
Epoch 1349/10000, Prediction Accuracy = 59.926%, Loss = 0.6012996196746826
Epoch: 1349, Batch Gradient Norm: 13.611187674394849
Epoch: 1349, Batch Gradient Norm after: 13.611187674394849
Epoch 1350/10000, Prediction Accuracy = 59.964%, Loss = 0.6072634935379029
Epoch: 1350, Batch Gradient Norm: 11.036650885178085
Epoch: 1350, Batch Gradient Norm after: 11.036650885178085
Epoch 1351/10000, Prediction Accuracy = 59.92%, Loss = 0.5911095619201661
Epoch: 1351, Batch Gradient Norm: 11.865510544578958
Epoch: 1351, Batch Gradient Norm after: 11.865510544578958
Epoch 1352/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.5975523233413697
Epoch: 1352, Batch Gradient Norm: 12.425197039943809
Epoch: 1352, Batch Gradient Norm after: 12.425197039943809
Epoch 1353/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.6004489064216614
Epoch: 1353, Batch Gradient Norm: 14.64322671489793
Epoch: 1353, Batch Gradient Norm after: 14.64322671489793
Epoch 1354/10000, Prediction Accuracy = 59.970000000000006%, Loss = 0.6202335000038147
Epoch: 1354, Batch Gradient Norm: 9.082209173589508
Epoch: 1354, Batch Gradient Norm after: 9.082209173589508
Epoch 1355/10000, Prediction Accuracy = 60.102%, Loss = 0.5825055480003357
Epoch: 1355, Batch Gradient Norm: 9.201517314429372
Epoch: 1355, Batch Gradient Norm after: 9.201517314429372
Epoch 1356/10000, Prediction Accuracy = 59.988%, Loss = 0.5814869165420532
Epoch: 1356, Batch Gradient Norm: 13.07841307876947
Epoch: 1356, Batch Gradient Norm after: 13.07841307876947
Epoch 1357/10000, Prediction Accuracy = 60.02%, Loss = 0.6040688991546631
Epoch: 1357, Batch Gradient Norm: 13.388661260946153
Epoch: 1357, Batch Gradient Norm after: 13.388661260946153
Epoch 1358/10000, Prediction Accuracy = 59.944%, Loss = 0.607529366016388
Epoch: 1358, Batch Gradient Norm: 11.338554016713012
Epoch: 1358, Batch Gradient Norm after: 11.338554016713012
Epoch 1359/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.5939927816390991
Epoch: 1359, Batch Gradient Norm: 8.222388545066833
Epoch: 1359, Batch Gradient Norm after: 8.222388545066833
Epoch 1360/10000, Prediction Accuracy = 59.924%, Loss = 0.5771216869354248
Epoch: 1360, Batch Gradient Norm: 8.94398503813216
Epoch: 1360, Batch Gradient Norm after: 8.94398503813216
Epoch 1361/10000, Prediction Accuracy = 59.96%, Loss = 0.5805376648902894
Epoch: 1361, Batch Gradient Norm: 11.333645879672039
Epoch: 1361, Batch Gradient Norm after: 11.333645879672039
Epoch 1362/10000, Prediction Accuracy = 59.965999999999994%, Loss = 0.5938403725624084
Epoch: 1362, Batch Gradient Norm: 14.294883492780325
Epoch: 1362, Batch Gradient Norm after: 14.294883492780325
Epoch 1363/10000, Prediction Accuracy = 60.048%, Loss = 0.6163114309310913
Epoch: 1363, Batch Gradient Norm: 12.954340425981169
Epoch: 1363, Batch Gradient Norm after: 12.954340425981169
Epoch 1364/10000, Prediction Accuracy = 60.076%, Loss = 0.6074283361434937
Epoch: 1364, Batch Gradient Norm: 9.068901123431623
Epoch: 1364, Batch Gradient Norm after: 9.068901123431623
Epoch 1365/10000, Prediction Accuracy = 60.04200000000001%, Loss = 0.5817882299423218
Epoch: 1365, Batch Gradient Norm: 11.185592936974468
Epoch: 1365, Batch Gradient Norm after: 11.185592936974468
Epoch 1366/10000, Prediction Accuracy = 59.958000000000006%, Loss = 0.5922778487205506
Epoch: 1366, Batch Gradient Norm: 14.62189087419397
Epoch: 1366, Batch Gradient Norm after: 14.62189087419397
Epoch 1367/10000, Prediction Accuracy = 59.952%, Loss = 0.6193276047706604
Epoch: 1367, Batch Gradient Norm: 13.876421165315424
Epoch: 1367, Batch Gradient Norm after: 13.876421165315424
Epoch 1368/10000, Prediction Accuracy = 59.980000000000004%, Loss = 0.6141491532325745
Epoch: 1368, Batch Gradient Norm: 9.468471000643339
Epoch: 1368, Batch Gradient Norm after: 9.468471000643339
Epoch 1369/10000, Prediction Accuracy = 59.984%, Loss = 0.5838301062583924
Epoch: 1369, Batch Gradient Norm: 8.226617103958453
Epoch: 1369, Batch Gradient Norm after: 8.226617103958453
Epoch 1370/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5765323996543884
Epoch: 1370, Batch Gradient Norm: 10.214766211109195
Epoch: 1370, Batch Gradient Norm after: 10.214766211109195
Epoch 1371/10000, Prediction Accuracy = 60.029999999999994%, Loss = 0.5857900500297546
Epoch: 1371, Batch Gradient Norm: 12.376239461555711
Epoch: 1371, Batch Gradient Norm after: 12.376239461555711
Epoch 1372/10000, Prediction Accuracy = 59.952%, Loss = 0.6007720708847046
Epoch: 1372, Batch Gradient Norm: 12.266662446156042
Epoch: 1372, Batch Gradient Norm after: 12.266662446156042
Epoch 1373/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.5994180202484131
Epoch: 1373, Batch Gradient Norm: 12.940351895572322
Epoch: 1373, Batch Gradient Norm after: 12.940351895572322
Epoch 1374/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.6034037113189697
Epoch: 1374, Batch Gradient Norm: 12.734507721032227
Epoch: 1374, Batch Gradient Norm after: 12.734507721032227
Epoch 1375/10000, Prediction Accuracy = 59.95%, Loss = 0.6014051675796509
Epoch: 1375, Batch Gradient Norm: 10.028024458444309
Epoch: 1375, Batch Gradient Norm after: 10.028024458444309
Epoch 1376/10000, Prediction Accuracy = 59.95399999999999%, Loss = 0.5838352799415588
Epoch: 1376, Batch Gradient Norm: 10.676917819869203
Epoch: 1376, Batch Gradient Norm after: 10.676917819869203
Epoch 1377/10000, Prediction Accuracy = 59.98199999999999%, Loss = 0.5866678595542908
Epoch: 1377, Batch Gradient Norm: 14.59581160792012
Epoch: 1377, Batch Gradient Norm after: 14.59581160792012
Epoch 1378/10000, Prediction Accuracy = 59.99000000000001%, Loss = 0.6135576486587524
Epoch: 1378, Batch Gradient Norm: 15.19686309876092
Epoch: 1378, Batch Gradient Norm after: 15.19686309876092
Epoch 1379/10000, Prediction Accuracy = 60.07000000000001%, Loss = 0.619865620136261
Epoch: 1379, Batch Gradient Norm: 12.987174060122276
Epoch: 1379, Batch Gradient Norm after: 12.987174060122276
Epoch 1380/10000, Prediction Accuracy = 60.022000000000006%, Loss = 0.6050208687782288
Epoch: 1380, Batch Gradient Norm: 8.668684043277699
Epoch: 1380, Batch Gradient Norm after: 8.668684043277699
Epoch 1381/10000, Prediction Accuracy = 60.13000000000001%, Loss = 0.5781575679779053
Epoch: 1381, Batch Gradient Norm: 10.278374031373248
Epoch: 1381, Batch Gradient Norm after: 10.278374031373248
Epoch 1382/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.5856885075569153
Epoch: 1382, Batch Gradient Norm: 11.899250840214693
Epoch: 1382, Batch Gradient Norm after: 11.899250840214693
Epoch 1383/10000, Prediction Accuracy = 60.114%, Loss = 0.595242726802826
Epoch: 1383, Batch Gradient Norm: 12.92113612504174
Epoch: 1383, Batch Gradient Norm after: 12.92113612504174
Epoch 1384/10000, Prediction Accuracy = 59.918000000000006%, Loss = 0.6037694334983825
Epoch: 1384, Batch Gradient Norm: 10.825113450521558
Epoch: 1384, Batch Gradient Norm after: 10.825113450521558
Epoch 1385/10000, Prediction Accuracy = 60.05999999999999%, Loss = 0.5894895672798157
Epoch: 1385, Batch Gradient Norm: 11.129808527321682
Epoch: 1385, Batch Gradient Norm after: 11.129808527321682
Epoch 1386/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5912177205085755
Epoch: 1386, Batch Gradient Norm: 14.154753976874481
Epoch: 1386, Batch Gradient Norm after: 14.154753976874481
Epoch 1387/10000, Prediction Accuracy = 60.05800000000001%, Loss = 0.6127245783805847
Epoch: 1387, Batch Gradient Norm: 11.669570660641297
Epoch: 1387, Batch Gradient Norm after: 11.669570660641297
Epoch 1388/10000, Prediction Accuracy = 60.00600000000001%, Loss = 0.5934581637382508
Epoch: 1388, Batch Gradient Norm: 12.236640304117785
Epoch: 1388, Batch Gradient Norm after: 12.236640304117785
Epoch 1389/10000, Prediction Accuracy = 60.044%, Loss = 0.5971174359321594
Epoch: 1389, Batch Gradient Norm: 10.937983962088992
Epoch: 1389, Batch Gradient Norm after: 10.937983962088992
Epoch 1390/10000, Prediction Accuracy = 60.004%, Loss = 0.5890255451202393
Epoch: 1390, Batch Gradient Norm: 10.857725928362806
Epoch: 1390, Batch Gradient Norm after: 10.857725928362806
Epoch 1391/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.5886216044425965
Epoch: 1391, Batch Gradient Norm: 9.347886689949158
Epoch: 1391, Batch Gradient Norm after: 9.347886689949158
Epoch 1392/10000, Prediction Accuracy = 59.976%, Loss = 0.5804276823997497
Epoch: 1392, Batch Gradient Norm: 9.691045604865197
Epoch: 1392, Batch Gradient Norm after: 9.691045604865197
Epoch 1393/10000, Prediction Accuracy = 60.04%, Loss = 0.5818445801734924
Epoch: 1393, Batch Gradient Norm: 9.979641515750137
Epoch: 1393, Batch Gradient Norm after: 9.979641515750137
Epoch 1394/10000, Prediction Accuracy = 59.976%, Loss = 0.5836361885070801
Epoch: 1394, Batch Gradient Norm: 13.127559952539281
Epoch: 1394, Batch Gradient Norm after: 13.127559952539281
Epoch 1395/10000, Prediction Accuracy = 60.016%, Loss = 0.6027570486068725
Epoch: 1395, Batch Gradient Norm: 13.117794541960592
Epoch: 1395, Batch Gradient Norm after: 13.117794541960592
Epoch 1396/10000, Prediction Accuracy = 60.01800000000001%, Loss = 0.6026465773582459
Epoch: 1396, Batch Gradient Norm: 11.116483370661758
Epoch: 1396, Batch Gradient Norm after: 11.116483370661758
Epoch 1397/10000, Prediction Accuracy = 60.013999999999996%, Loss = 0.5887297630310059
Epoch: 1397, Batch Gradient Norm: 10.027630184404558
Epoch: 1397, Batch Gradient Norm after: 10.027630184404558
Epoch 1398/10000, Prediction Accuracy = 60.025999999999996%, Loss = 0.5817270517349243
Epoch: 1398, Batch Gradient Norm: 11.097170901745322
Epoch: 1398, Batch Gradient Norm after: 11.097170901745322
Epoch 1399/10000, Prediction Accuracy = 60.016000000000005%, Loss = 0.5882389068603515
Epoch: 1399, Batch Gradient Norm: 11.870674378056757
Epoch: 1399, Batch Gradient Norm after: 11.870674378056757
Epoch 1400/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.5934133768081665
Epoch: 1400, Batch Gradient Norm: 10.78386595723469
Epoch: 1400, Batch Gradient Norm after: 10.78386595723469
Epoch 1401/10000, Prediction Accuracy = 59.977999999999994%, Loss = 0.5867255091667175
Epoch: 1401, Batch Gradient Norm: 13.360144568092872
Epoch: 1401, Batch Gradient Norm after: 13.360144568092872
Epoch 1402/10000, Prediction Accuracy = 60.016%, Loss = 0.6067394971847534
Epoch: 1402, Batch Gradient Norm: 14.288931861281641
Epoch: 1402, Batch Gradient Norm after: 14.288931861281641
Epoch 1403/10000, Prediction Accuracy = 59.910000000000004%, Loss = 0.6160116910934448
Epoch: 1403, Batch Gradient Norm: 13.319961474780902
Epoch: 1403, Batch Gradient Norm after: 13.319961474780902
Epoch 1404/10000, Prediction Accuracy = 60.176%, Loss = 0.606657886505127
Epoch: 1404, Batch Gradient Norm: 13.195213548139526
Epoch: 1404, Batch Gradient Norm after: 13.195213548139526
Epoch 1405/10000, Prediction Accuracy = 59.97800000000001%, Loss = 0.6035118937492371
Epoch: 1405, Batch Gradient Norm: 10.837668250420307
Epoch: 1405, Batch Gradient Norm after: 10.837668250420307
Epoch 1406/10000, Prediction Accuracy = 60.076%, Loss = 0.5865241050720215
Epoch: 1406, Batch Gradient Norm: 11.25652943692835
Epoch: 1406, Batch Gradient Norm after: 11.25652943692835
Epoch 1407/10000, Prediction Accuracy = 60.02%, Loss = 0.5889381527900696
Epoch: 1407, Batch Gradient Norm: 11.80408448655145
Epoch: 1407, Batch Gradient Norm after: 11.80408448655145
Epoch 1408/10000, Prediction Accuracy = 59.984%, Loss = 0.5916267037391663
Epoch: 1408, Batch Gradient Norm: 13.710604822642175
Epoch: 1408, Batch Gradient Norm after: 13.710604822642175
Epoch 1409/10000, Prediction Accuracy = 60.034000000000006%, Loss = 0.6042636394500732
Epoch: 1409, Batch Gradient Norm: 13.253848863312845
Epoch: 1409, Batch Gradient Norm after: 13.253848863312845
Epoch 1410/10000, Prediction Accuracy = 59.996%, Loss = 0.603614866733551
Epoch: 1410, Batch Gradient Norm: 10.192049776807947
Epoch: 1410, Batch Gradient Norm after: 10.192049776807947
Epoch 1411/10000, Prediction Accuracy = 60.10999999999999%, Loss = 0.5836715698242188
Epoch: 1411, Batch Gradient Norm: 10.02044211872909
Epoch: 1411, Batch Gradient Norm after: 10.02044211872909
Epoch 1412/10000, Prediction Accuracy = 59.996%, Loss = 0.5824453711509705
Epoch: 1412, Batch Gradient Norm: 12.480595529383981
Epoch: 1412, Batch Gradient Norm after: 12.480595529383981
Epoch 1413/10000, Prediction Accuracy = 60.08%, Loss = 0.5984261870384217
Epoch: 1413, Batch Gradient Norm: 11.84709194044227
Epoch: 1413, Batch Gradient Norm after: 11.84709194044227
Epoch 1414/10000, Prediction Accuracy = 60.010000000000005%, Loss = 0.5946834921836853
Epoch: 1414, Batch Gradient Norm: 10.234270551266865
Epoch: 1414, Batch Gradient Norm after: 10.234270551266865
Epoch 1415/10000, Prediction Accuracy = 60.06%, Loss = 0.5842433452606202
Epoch: 1415, Batch Gradient Norm: 7.354961373069473
Epoch: 1415, Batch Gradient Norm after: 7.354961373069473
Epoch 1416/10000, Prediction Accuracy = 60.05%, Loss = 0.5691272974014282
Epoch: 1416, Batch Gradient Norm: 10.003687052797943
Epoch: 1416, Batch Gradient Norm after: 10.003687052797943
Epoch 1417/10000, Prediction Accuracy = 60.036%, Loss = 0.581317687034607
Epoch: 1417, Batch Gradient Norm: 11.976087136897533
Epoch: 1417, Batch Gradient Norm after: 11.976087136897533
Epoch 1418/10000, Prediction Accuracy = 60.02%, Loss = 0.5940022349357605
Epoch: 1418, Batch Gradient Norm: 14.073549954820633
Epoch: 1418, Batch Gradient Norm after: 14.073549954820633
Epoch 1419/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.6096824884414673
Epoch: 1419, Batch Gradient Norm: 11.181435674642454
Epoch: 1419, Batch Gradient Norm after: 11.181435674642454
Epoch 1420/10000, Prediction Accuracy = 60.062%, Loss = 0.5891523957252502
Epoch: 1420, Batch Gradient Norm: 11.361850569310008
Epoch: 1420, Batch Gradient Norm after: 11.361850569310008
Epoch 1421/10000, Prediction Accuracy = 60.02199999999999%, Loss = 0.5893214702606201
Epoch: 1421, Batch Gradient Norm: 11.440337287452069
Epoch: 1421, Batch Gradient Norm after: 11.440337287452069
Epoch 1422/10000, Prediction Accuracy = 60.084%, Loss = 0.5889197587966919
Epoch: 1422, Batch Gradient Norm: 13.941294235490403
Epoch: 1422, Batch Gradient Norm after: 13.941294235490403
Epoch 1423/10000, Prediction Accuracy = 60.012%, Loss = 0.6052954196929932
Epoch: 1423, Batch Gradient Norm: 13.625968401272162
Epoch: 1423, Batch Gradient Norm after: 13.625968401272162
Epoch 1424/10000, Prediction Accuracy = 60.056%, Loss = 0.6034555315971375
Epoch: 1424, Batch Gradient Norm: 12.516647408804332
Epoch: 1424, Batch Gradient Norm after: 12.516647408804332
Epoch 1425/10000, Prediction Accuracy = 59.99400000000001%, Loss = 0.5963569164276123
Epoch: 1425, Batch Gradient Norm: 10.964929580106089
Epoch: 1425, Batch Gradient Norm after: 10.964929580106089
Epoch 1426/10000, Prediction Accuracy = 60.088%, Loss = 0.5855426788330078
Epoch: 1426, Batch Gradient Norm: 11.63135799703642
Epoch: 1426, Batch Gradient Norm after: 11.63135799703642
Epoch 1427/10000, Prediction Accuracy = 60.036%, Loss = 0.5896175146102905
Epoch: 1427, Batch Gradient Norm: 13.17820635194403
Epoch: 1427, Batch Gradient Norm after: 13.17820635194403
Epoch 1428/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.6021239161491394
Epoch: 1428, Batch Gradient Norm: 11.165222692161075
Epoch: 1428, Batch Gradient Norm after: 11.165222692161075
Epoch 1429/10000, Prediction Accuracy = 60.08%, Loss = 0.5892266035079956
Epoch: 1429, Batch Gradient Norm: 9.634832974834572
Epoch: 1429, Batch Gradient Norm after: 9.634832974834572
Epoch 1430/10000, Prediction Accuracy = 60.032000000000004%, Loss = 0.5793389678001404
Epoch: 1430, Batch Gradient Norm: 10.069785386253676
Epoch: 1430, Batch Gradient Norm after: 10.069785386253676
Epoch 1431/10000, Prediction Accuracy = 60.074%, Loss = 0.5810107350349426
Epoch: 1431, Batch Gradient Norm: 11.579869818953327
Epoch: 1431, Batch Gradient Norm after: 11.579869818953327
Epoch 1432/10000, Prediction Accuracy = 60.052%, Loss = 0.5896644353866577
Epoch: 1432, Batch Gradient Norm: 14.186318063793031
Epoch: 1432, Batch Gradient Norm after: 14.186318063793031
Epoch 1433/10000, Prediction Accuracy = 60.08%, Loss = 0.6076008200645446
Epoch: 1433, Batch Gradient Norm: 12.953074808839416
Epoch: 1433, Batch Gradient Norm after: 12.953074808839416
Epoch 1434/10000, Prediction Accuracy = 60.001999999999995%, Loss = 0.598934555053711
Epoch: 1434, Batch Gradient Norm: 9.509868790429353
Epoch: 1434, Batch Gradient Norm after: 9.509868790429353
Epoch 1435/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.5768878579139709
Epoch: 1435, Batch Gradient Norm: 8.579467159661833
Epoch: 1435, Batch Gradient Norm after: 8.579467159661833
Epoch 1436/10000, Prediction Accuracy = 60.0%, Loss = 0.5720524907112121
Epoch: 1436, Batch Gradient Norm: 10.364174697780673
Epoch: 1436, Batch Gradient Norm after: 10.364174697780673
Epoch 1437/10000, Prediction Accuracy = 60.032%, Loss = 0.5801948428153991
Epoch: 1437, Batch Gradient Norm: 14.043631163149064
Epoch: 1437, Batch Gradient Norm after: 14.043631163149064
Epoch 1438/10000, Prediction Accuracy = 60.00599999999999%, Loss = 0.6064501285552979
Epoch: 1438, Batch Gradient Norm: 13.310556741448863
Epoch: 1438, Batch Gradient Norm after: 13.310556741448863
Epoch 1439/10000, Prediction Accuracy = 60.114%, Loss = 0.6017171382904053
Epoch: 1439, Batch Gradient Norm: 12.129537465526566
Epoch: 1439, Batch Gradient Norm after: 12.129537465526566
Epoch 1440/10000, Prediction Accuracy = 60.012%, Loss = 0.59539954662323
Epoch: 1440, Batch Gradient Norm: 11.080532293773
Epoch: 1440, Batch Gradient Norm after: 11.080532293773
Epoch 1441/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.5890074729919433
Epoch: 1441, Batch Gradient Norm: 9.946377371355474
Epoch: 1441, Batch Gradient Norm after: 9.946377371355474
Epoch 1442/10000, Prediction Accuracy = 60.016%, Loss = 0.5816152453422546
Epoch: 1442, Batch Gradient Norm: 10.239006694099514
Epoch: 1442, Batch Gradient Norm after: 10.239006694099514
Epoch 1443/10000, Prediction Accuracy = 60.138%, Loss = 0.5824939370155334
Epoch: 1443, Batch Gradient Norm: 9.959049208121924
Epoch: 1443, Batch Gradient Norm after: 9.959049208121924
Epoch 1444/10000, Prediction Accuracy = 60.052%, Loss = 0.5792308807373047
Epoch: 1444, Batch Gradient Norm: 11.369494937182976
Epoch: 1444, Batch Gradient Norm after: 11.369494937182976
Epoch 1445/10000, Prediction Accuracy = 60.12800000000001%, Loss = 0.5866827011108399
Epoch: 1445, Batch Gradient Norm: 12.720363857514887
Epoch: 1445, Batch Gradient Norm after: 12.720363857514887
Epoch 1446/10000, Prediction Accuracy = 60.044000000000004%, Loss = 0.5946403026580811
Epoch: 1446, Batch Gradient Norm: 11.728955009255351
Epoch: 1446, Batch Gradient Norm after: 11.728955009255351
Epoch 1447/10000, Prediction Accuracy = 60.088%, Loss = 0.588163948059082
Epoch: 1447, Batch Gradient Norm: 14.072414496761514
Epoch: 1447, Batch Gradient Norm after: 14.072414496761514
Epoch 1448/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.6046483159065247
Epoch: 1448, Batch Gradient Norm: 14.459552700967748
Epoch: 1448, Batch Gradient Norm after: 14.459552700967748
Epoch 1449/10000, Prediction Accuracy = 60.0%, Loss = 0.6106367588043213
Epoch: 1449, Batch Gradient Norm: 12.489231034132718
Epoch: 1449, Batch Gradient Norm after: 12.489231034132718
Epoch 1450/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.5947878122329712
Epoch: 1450, Batch Gradient Norm: 14.03779889290473
Epoch: 1450, Batch Gradient Norm after: 14.03779889290473
Epoch 1451/10000, Prediction Accuracy = 60.074%, Loss = 0.6066135287284851
Epoch: 1451, Batch Gradient Norm: 11.566121156010292
Epoch: 1451, Batch Gradient Norm after: 11.566121156010292
Epoch 1452/10000, Prediction Accuracy = 60.116%, Loss = 0.5888174295425415
Epoch: 1452, Batch Gradient Norm: 11.765866287728524
Epoch: 1452, Batch Gradient Norm after: 11.765866287728524
Epoch 1453/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.589654290676117
Epoch: 1453, Batch Gradient Norm: 9.985519325823843
Epoch: 1453, Batch Gradient Norm after: 9.985519325823843
Epoch 1454/10000, Prediction Accuracy = 60.116%, Loss = 0.5786334633827209
Epoch: 1454, Batch Gradient Norm: 9.97361363500399
Epoch: 1454, Batch Gradient Norm after: 9.97361363500399
Epoch 1455/10000, Prediction Accuracy = 60.132000000000005%, Loss = 0.5784878969192505
Epoch: 1455, Batch Gradient Norm: 9.536893362565305
Epoch: 1455, Batch Gradient Norm after: 9.536893362565305
Epoch 1456/10000, Prediction Accuracy = 60.086%, Loss = 0.5762308239936829
Epoch: 1456, Batch Gradient Norm: 11.141700595225128
Epoch: 1456, Batch Gradient Norm after: 11.141700595225128
Epoch 1457/10000, Prediction Accuracy = 60.092000000000006%, Loss = 0.5856489658355712
Epoch: 1457, Batch Gradient Norm: 13.713793226459368
Epoch: 1457, Batch Gradient Norm after: 13.713793226459368
Epoch 1458/10000, Prediction Accuracy = 60.10799999999999%, Loss = 0.6037100672721862
Epoch: 1458, Batch Gradient Norm: 13.176015483503686
Epoch: 1458, Batch Gradient Norm after: 13.176015483503686
Epoch 1459/10000, Prediction Accuracy = 60.089999999999996%, Loss = 0.5989780068397522
Epoch: 1459, Batch Gradient Norm: 10.337944844542022
Epoch: 1459, Batch Gradient Norm after: 10.337944844542022
Epoch 1460/10000, Prediction Accuracy = 60.068%, Loss = 0.5791955590248108
Epoch: 1460, Batch Gradient Norm: 9.311106753377569
Epoch: 1460, Batch Gradient Norm after: 9.311106753377569
Epoch 1461/10000, Prediction Accuracy = 60.102%, Loss = 0.5728732943534851
Epoch: 1461, Batch Gradient Norm: 9.96177761684931
Epoch: 1461, Batch Gradient Norm after: 9.96177761684931
Epoch 1462/10000, Prediction Accuracy = 60.053999999999995%, Loss = 0.5751584291458129
Epoch: 1462, Batch Gradient Norm: 11.632673479238916
Epoch: 1462, Batch Gradient Norm after: 11.632673479238916
Epoch 1463/10000, Prediction Accuracy = 60.098%, Loss = 0.5852534890174865
Epoch: 1463, Batch Gradient Norm: 11.614466235940892
Epoch: 1463, Batch Gradient Norm after: 11.614466235940892
Epoch 1464/10000, Prediction Accuracy = 60.072%, Loss = 0.5855578422546387
Epoch: 1464, Batch Gradient Norm: 12.297045755681316
Epoch: 1464, Batch Gradient Norm after: 12.297045755681316
Epoch 1465/10000, Prediction Accuracy = 60.04600000000001%, Loss = 0.5916645884513855
Epoch: 1465, Batch Gradient Norm: 13.631343309257439
Epoch: 1465, Batch Gradient Norm after: 13.631343309257439
Epoch 1466/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.6043544411659241
Epoch: 1466, Batch Gradient Norm: 11.317606489519239
Epoch: 1466, Batch Gradient Norm after: 11.317606489519239
Epoch 1467/10000, Prediction Accuracy = 60.077999999999996%, Loss = 0.5890642166137695
Epoch: 1467, Batch Gradient Norm: 10.220538067557108
Epoch: 1467, Batch Gradient Norm after: 10.220538067557108
Epoch 1468/10000, Prediction Accuracy = 60.194%, Loss = 0.5809998273849487
Epoch: 1468, Batch Gradient Norm: 11.286907883355967
Epoch: 1468, Batch Gradient Norm after: 11.286907883355967
Epoch 1469/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.5867279529571533
Epoch: 1469, Batch Gradient Norm: 12.334658778342325
Epoch: 1469, Batch Gradient Norm after: 12.334658778342325
Epoch 1470/10000, Prediction Accuracy = 60.176%, Loss = 0.5923285484313965
Epoch: 1470, Batch Gradient Norm: 14.525923452561239
Epoch: 1470, Batch Gradient Norm after: 14.525923452561239
Epoch 1471/10000, Prediction Accuracy = 60.076%, Loss = 0.608433973789215
Epoch: 1471, Batch Gradient Norm: 11.240516489853855
Epoch: 1471, Batch Gradient Norm after: 11.240516489853855
Epoch 1472/10000, Prediction Accuracy = 60.19199999999999%, Loss = 0.5851788401603699
Epoch: 1472, Batch Gradient Norm: 9.772810990575787
Epoch: 1472, Batch Gradient Norm after: 9.772810990575787
Epoch 1473/10000, Prediction Accuracy = 60.104%, Loss = 0.5763077735900879
Epoch: 1473, Batch Gradient Norm: 8.310919529525975
Epoch: 1473, Batch Gradient Norm after: 8.310919529525975
Epoch 1474/10000, Prediction Accuracy = 60.1%, Loss = 0.5683231949806213
Epoch: 1474, Batch Gradient Norm: 9.558850193299119
Epoch: 1474, Batch Gradient Norm after: 9.558850193299119
Epoch 1475/10000, Prediction Accuracy = 60.072%, Loss = 0.5749547004699707
Epoch: 1475, Batch Gradient Norm: 11.16989183506754
Epoch: 1475, Batch Gradient Norm after: 11.16989183506754
Epoch 1476/10000, Prediction Accuracy = 60.104%, Loss = 0.5848227858543396
Epoch: 1476, Batch Gradient Norm: 12.752069350063723
Epoch: 1476, Batch Gradient Norm after: 12.752069350063723
Epoch 1477/10000, Prediction Accuracy = 60.11800000000001%, Loss = 0.5960006237030029
Epoch: 1477, Batch Gradient Norm: 14.69010816890925
Epoch: 1477, Batch Gradient Norm after: 14.69010816890925
Epoch 1478/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.6109308719635009
Epoch: 1478, Batch Gradient Norm: 12.546585093092894
Epoch: 1478, Batch Gradient Norm after: 12.546585093092894
Epoch 1479/10000, Prediction Accuracy = 60.093999999999994%, Loss = 0.590548038482666
Epoch: 1479, Batch Gradient Norm: 14.879428004574253
Epoch: 1479, Batch Gradient Norm after: 14.879428004574253
Epoch 1480/10000, Prediction Accuracy = 60.088%, Loss = 0.6083914637565613
Epoch: 1480, Batch Gradient Norm: 11.738365571069282
Epoch: 1480, Batch Gradient Norm after: 11.738365571069282
Epoch 1481/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.58834810256958
Epoch: 1481, Batch Gradient Norm: 9.720601121518243
Epoch: 1481, Batch Gradient Norm after: 9.720601121518243
Epoch 1482/10000, Prediction Accuracy = 60.124%, Loss = 0.5756783604621887
Epoch: 1482, Batch Gradient Norm: 10.222272848378124
Epoch: 1482, Batch Gradient Norm after: 10.222272848378124
Epoch 1483/10000, Prediction Accuracy = 60.05%, Loss = 0.578170907497406
Epoch: 1483, Batch Gradient Norm: 14.54853762855289
Epoch: 1483, Batch Gradient Norm after: 14.54853762855289
Epoch 1484/10000, Prediction Accuracy = 60.112%, Loss = 0.6066989302635193
Epoch: 1484, Batch Gradient Norm: 12.875888583920293
Epoch: 1484, Batch Gradient Norm after: 12.875888583920293
Epoch 1485/10000, Prediction Accuracy = 60.080000000000005%, Loss = 0.5941430807113648
Epoch: 1485, Batch Gradient Norm: 9.972309953950926
Epoch: 1485, Batch Gradient Norm after: 9.972309953950926
Epoch 1486/10000, Prediction Accuracy = 60.104000000000006%, Loss = 0.5753812789916992
Epoch: 1486, Batch Gradient Norm: 8.184606599594778
Epoch: 1486, Batch Gradient Norm after: 8.184606599594778
Epoch 1487/10000, Prediction Accuracy = 60.10600000000001%, Loss = 0.5661639213562012
Epoch: 1487, Batch Gradient Norm: 9.448462412532505
Epoch: 1487, Batch Gradient Norm after: 9.448462412532505
Epoch 1488/10000, Prediction Accuracy = 60.184000000000005%, Loss = 0.5720908164978027
Epoch: 1488, Batch Gradient Norm: 12.527311075742269
Epoch: 1488, Batch Gradient Norm after: 12.527311075742269
Epoch 1489/10000, Prediction Accuracy = 60.064%, Loss = 0.5915411591529847
Epoch: 1489, Batch Gradient Norm: 12.66484863994499
Epoch: 1489, Batch Gradient Norm after: 12.66484863994499
Epoch 1490/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.5940666913986206
Epoch: 1490, Batch Gradient Norm: 10.968447256695052
Epoch: 1490, Batch Gradient Norm after: 10.968447256695052
Epoch 1491/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.5826166033744812
Epoch: 1491, Batch Gradient Norm: 10.175447513004979
Epoch: 1491, Batch Gradient Norm after: 10.175447513004979
Epoch 1492/10000, Prediction Accuracy = 60.168000000000006%, Loss = 0.5764160513877868
Epoch: 1492, Batch Gradient Norm: 12.278572192556377
Epoch: 1492, Batch Gradient Norm after: 12.278572192556377
Epoch 1493/10000, Prediction Accuracy = 60.076%, Loss = 0.5906144380569458
Epoch: 1493, Batch Gradient Norm: 11.037949414426144
Epoch: 1493, Batch Gradient Norm after: 11.037949414426144
Epoch 1494/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.5827791094779968
Epoch: 1494, Batch Gradient Norm: 9.887243704245556
Epoch: 1494, Batch Gradient Norm after: 9.887243704245556
Epoch 1495/10000, Prediction Accuracy = 60.074%, Loss = 0.5756968855857849
Epoch: 1495, Batch Gradient Norm: 10.397486900834599
Epoch: 1495, Batch Gradient Norm after: 10.397486900834599
Epoch 1496/10000, Prediction Accuracy = 60.084%, Loss = 0.577873170375824
Epoch: 1496, Batch Gradient Norm: 13.759454684884254
Epoch: 1496, Batch Gradient Norm after: 13.759454684884254
Epoch 1497/10000, Prediction Accuracy = 60.134%, Loss = 0.6017516493797302
Epoch: 1497, Batch Gradient Norm: 13.789524826637347
Epoch: 1497, Batch Gradient Norm after: 13.789524826637347
Epoch 1498/10000, Prediction Accuracy = 60.11%, Loss = 0.6012102842330933
Epoch: 1498, Batch Gradient Norm: 12.845437717466636
Epoch: 1498, Batch Gradient Norm after: 12.845437717466636
Epoch 1499/10000, Prediction Accuracy = 60.2%, Loss = 0.5926418542861939
Epoch: 1499, Batch Gradient Norm: 13.13186959976747
Epoch: 1499, Batch Gradient Norm after: 13.13186959976747
Epoch 1500/10000, Prediction Accuracy = 60.122%, Loss = 0.5951992273330688
Epoch: 1500, Batch Gradient Norm: 12.780000250367827
Epoch: 1500, Batch Gradient Norm after: 12.780000250367827
Epoch 1501/10000, Prediction Accuracy = 60.098%, Loss = 0.5927187204360962
Epoch: 1501, Batch Gradient Norm: 10.623238966636539
Epoch: 1501, Batch Gradient Norm after: 10.623238966636539
Epoch 1502/10000, Prediction Accuracy = 60.128%, Loss = 0.5781771898269653
Epoch: 1502, Batch Gradient Norm: 11.392931405040779
Epoch: 1502, Batch Gradient Norm after: 11.392931405040779
Epoch 1503/10000, Prediction Accuracy = 60.146%, Loss = 0.5827715039253235
Epoch: 1503, Batch Gradient Norm: 11.30451739224621
Epoch: 1503, Batch Gradient Norm after: 11.30451739224621
Epoch 1504/10000, Prediction Accuracy = 60.193999999999996%, Loss = 0.5834800124168396
Epoch: 1504, Batch Gradient Norm: 11.438152025141028
Epoch: 1504, Batch Gradient Norm after: 11.438152025141028
Epoch 1505/10000, Prediction Accuracy = 60.186%, Loss = 0.5853041648864746
Epoch: 1505, Batch Gradient Norm: 10.040440572235486
Epoch: 1505, Batch Gradient Norm after: 10.040440572235486
Epoch 1506/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.5760348200798034
Epoch: 1506, Batch Gradient Norm: 12.089724553049964
Epoch: 1506, Batch Gradient Norm after: 12.089724553049964
Epoch 1507/10000, Prediction Accuracy = 60.12800000000001%, Loss = 0.5882909655570984
Epoch: 1507, Batch Gradient Norm: 9.88849924400635
Epoch: 1507, Batch Gradient Norm after: 9.88849924400635
Epoch 1508/10000, Prediction Accuracy = 60.188%, Loss = 0.574175488948822
Epoch: 1508, Batch Gradient Norm: 9.889072590513946
Epoch: 1508, Batch Gradient Norm after: 9.889072590513946
Epoch 1509/10000, Prediction Accuracy = 60.120000000000005%, Loss = 0.5736182689666748
Epoch: 1509, Batch Gradient Norm: 10.446568076196035
Epoch: 1509, Batch Gradient Norm after: 10.446568076196035
Epoch 1510/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.5771107077598572
Epoch: 1510, Batch Gradient Norm: 11.144404105979739
Epoch: 1510, Batch Gradient Norm after: 11.144404105979739
Epoch 1511/10000, Prediction Accuracy = 60.08200000000001%, Loss = 0.5816988229751587
Epoch: 1511, Batch Gradient Norm: 11.142819185572279
Epoch: 1511, Batch Gradient Norm after: 11.142819185572279
Epoch 1512/10000, Prediction Accuracy = 60.158%, Loss = 0.5818580508232116
Epoch: 1512, Batch Gradient Norm: 10.504678438577187
Epoch: 1512, Batch Gradient Norm after: 10.504678438577187
Epoch 1513/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.5768331289291382
Epoch: 1513, Batch Gradient Norm: 13.196850717625983
Epoch: 1513, Batch Gradient Norm after: 13.196850717625983
Epoch 1514/10000, Prediction Accuracy = 60.17%, Loss = 0.5950968503952027
Epoch: 1514, Batch Gradient Norm: 13.600584378077558
Epoch: 1514, Batch Gradient Norm after: 13.600584378077558
Epoch 1515/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.5998858213424683
Epoch: 1515, Batch Gradient Norm: 13.43673117745016
Epoch: 1515, Batch Gradient Norm after: 13.43673117745016
Epoch 1516/10000, Prediction Accuracy = 60.20399999999999%, Loss = 0.5950221061706543
Epoch: 1516, Batch Gradient Norm: 14.11636471834433
Epoch: 1516, Batch Gradient Norm after: 14.11636471834433
Epoch 1517/10000, Prediction Accuracy = 60.14%, Loss = 0.5995616436004638
Epoch: 1517, Batch Gradient Norm: 11.170798408261335
Epoch: 1517, Batch Gradient Norm after: 11.170798408261335
Epoch 1518/10000, Prediction Accuracy = 60.2%, Loss = 0.5794102787971497
Epoch: 1518, Batch Gradient Norm: 8.754396721569165
Epoch: 1518, Batch Gradient Norm after: 8.754396721569165
Epoch 1519/10000, Prediction Accuracy = 60.158%, Loss = 0.5665750622749328
Epoch: 1519, Batch Gradient Norm: 10.048581925636668
Epoch: 1519, Batch Gradient Norm after: 10.048581925636668
Epoch 1520/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.5734849810600281
Epoch: 1520, Batch Gradient Norm: 10.942832003627572
Epoch: 1520, Batch Gradient Norm after: 10.942832003627572
Epoch 1521/10000, Prediction Accuracy = 60.174%, Loss = 0.5787285089492797
Epoch: 1521, Batch Gradient Norm: 15.61829558288012
Epoch: 1521, Batch Gradient Norm after: 15.61829558288012
Epoch 1522/10000, Prediction Accuracy = 60.186%, Loss = 0.6147219657897949
Epoch: 1522, Batch Gradient Norm: 11.817739040850618
Epoch: 1522, Batch Gradient Norm after: 11.817739040850618
Epoch 1523/10000, Prediction Accuracy = 60.129999999999995%, Loss = 0.585682499408722
Epoch: 1523, Batch Gradient Norm: 9.539654125369477
Epoch: 1523, Batch Gradient Norm after: 9.539654125369477
Epoch 1524/10000, Prediction Accuracy = 60.152%, Loss = 0.5705118179321289
Epoch: 1524, Batch Gradient Norm: 9.467937585206032
Epoch: 1524, Batch Gradient Norm after: 9.467937585206032
Epoch 1525/10000, Prediction Accuracy = 60.141999999999996%, Loss = 0.5696638345718383
Epoch: 1525, Batch Gradient Norm: 9.850353220297745
Epoch: 1525, Batch Gradient Norm after: 9.850353220297745
Epoch 1526/10000, Prediction Accuracy = 60.169999999999995%, Loss = 0.5710765719413757
Epoch: 1526, Batch Gradient Norm: 12.620694162612569
Epoch: 1526, Batch Gradient Norm after: 12.620694162612569
Epoch 1527/10000, Prediction Accuracy = 60.102%, Loss = 0.5900280833244324
Epoch: 1527, Batch Gradient Norm: 15.249120764827495
Epoch: 1527, Batch Gradient Norm after: 15.249120764827495
Epoch 1528/10000, Prediction Accuracy = 60.226%, Loss = 0.6146237015724182
Epoch: 1528, Batch Gradient Norm: 13.309828304239174
Epoch: 1528, Batch Gradient Norm after: 13.309828304239174
Epoch 1529/10000, Prediction Accuracy = 60.08%, Loss = 0.5993564248085022
Epoch: 1529, Batch Gradient Norm: 10.087600287357665
Epoch: 1529, Batch Gradient Norm after: 10.087600287357665
Epoch 1530/10000, Prediction Accuracy = 60.238%, Loss = 0.5741036891937256
Epoch: 1530, Batch Gradient Norm: 10.817186203213868
Epoch: 1530, Batch Gradient Norm after: 10.817186203213868
Epoch 1531/10000, Prediction Accuracy = 60.186%, Loss = 0.5789582252502441
Epoch: 1531, Batch Gradient Norm: 9.489201680380196
Epoch: 1531, Batch Gradient Norm after: 9.489201680380196
Epoch 1532/10000, Prediction Accuracy = 60.176%, Loss = 0.5708000540733338
Epoch: 1532, Batch Gradient Norm: 9.69626163883334
Epoch: 1532, Batch Gradient Norm after: 9.69626163883334
Epoch 1533/10000, Prediction Accuracy = 60.16799999999999%, Loss = 0.5711508750915527
Epoch: 1533, Batch Gradient Norm: 10.512420029197159
Epoch: 1533, Batch Gradient Norm after: 10.512420029197159
Epoch 1534/10000, Prediction Accuracy = 60.162%, Loss = 0.5754151225090027
Epoch: 1534, Batch Gradient Norm: 11.138069603429136
Epoch: 1534, Batch Gradient Norm after: 11.138069603429136
Epoch 1535/10000, Prediction Accuracy = 60.181999999999995%, Loss = 0.5799652457237243
Epoch: 1535, Batch Gradient Norm: 9.336669586196138
Epoch: 1535, Batch Gradient Norm after: 9.336669586196138
Epoch 1536/10000, Prediction Accuracy = 60.145999999999994%, Loss = 0.5690525650978089
Epoch: 1536, Batch Gradient Norm: 9.752957281304752
Epoch: 1536, Batch Gradient Norm after: 9.752957281304752
Epoch 1537/10000, Prediction Accuracy = 60.166%, Loss = 0.5712489724159241
Epoch: 1537, Batch Gradient Norm: 10.585984217979679
Epoch: 1537, Batch Gradient Norm after: 10.585984217979679
Epoch 1538/10000, Prediction Accuracy = 60.126%, Loss = 0.5752382040023803
Epoch: 1538, Batch Gradient Norm: 14.358333145697417
Epoch: 1538, Batch Gradient Norm after: 14.358333145697417
Epoch 1539/10000, Prediction Accuracy = 60.172000000000004%, Loss = 0.6021641016006469
Epoch: 1539, Batch Gradient Norm: 14.690624290135531
Epoch: 1539, Batch Gradient Norm after: 14.690624290135531
Epoch 1540/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.6064218640327453
Epoch: 1540, Batch Gradient Norm: 12.469389394599032
Epoch: 1540, Batch Gradient Norm after: 12.469389394599032
Epoch 1541/10000, Prediction Accuracy = 60.2%, Loss = 0.588286530971527
Epoch: 1541, Batch Gradient Norm: 13.793307391394068
Epoch: 1541, Batch Gradient Norm after: 13.793307391394068
Epoch 1542/10000, Prediction Accuracy = 60.148%, Loss = 0.597362732887268
Epoch: 1542, Batch Gradient Norm: 13.818866875916225
Epoch: 1542, Batch Gradient Norm after: 13.818866875916225
Epoch 1543/10000, Prediction Accuracy = 60.105999999999995%, Loss = 0.5968980550765991
Epoch: 1543, Batch Gradient Norm: 10.77814193845627
Epoch: 1543, Batch Gradient Norm after: 10.77814193845627
Epoch 1544/10000, Prediction Accuracy = 60.178%, Loss = 0.5763538479804993
Epoch: 1544, Batch Gradient Norm: 11.389052694170095
Epoch: 1544, Batch Gradient Norm after: 11.389052694170095
Epoch 1545/10000, Prediction Accuracy = 60.182%, Loss = 0.5804261684417724
Epoch: 1545, Batch Gradient Norm: 10.381372540927888
Epoch: 1545, Batch Gradient Norm after: 10.381372540927888
Epoch 1546/10000, Prediction Accuracy = 60.21%, Loss = 0.5747153043746949
Epoch: 1546, Batch Gradient Norm: 12.006029780128655
Epoch: 1546, Batch Gradient Norm after: 12.006029780128655
Epoch 1547/10000, Prediction Accuracy = 60.194%, Loss = 0.5852687239646912
Epoch: 1547, Batch Gradient Norm: 11.791935187441455
Epoch: 1547, Batch Gradient Norm after: 11.791935187441455
Epoch 1548/10000, Prediction Accuracy = 60.238000000000014%, Loss = 0.5831287503242493
Epoch: 1548, Batch Gradient Norm: 11.132305929804494
Epoch: 1548, Batch Gradient Norm after: 11.132305929804494
Epoch 1549/10000, Prediction Accuracy = 60.17%, Loss = 0.5784391760826111
Epoch: 1549, Batch Gradient Norm: 11.450967296865937
Epoch: 1549, Batch Gradient Norm after: 11.450967296865937
Epoch 1550/10000, Prediction Accuracy = 60.26400000000001%, Loss = 0.5795565247535706
Epoch: 1550, Batch Gradient Norm: 12.220284803110937
Epoch: 1550, Batch Gradient Norm after: 12.220284803110937
Epoch 1551/10000, Prediction Accuracy = 60.146%, Loss = 0.5855852961540222
Epoch: 1551, Batch Gradient Norm: 11.680526919943516
Epoch: 1551, Batch Gradient Norm after: 11.680526919943516
Epoch 1552/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.5826826214790344
Epoch: 1552, Batch Gradient Norm: 9.255527586122708
Epoch: 1552, Batch Gradient Norm after: 9.255527586122708
Epoch 1553/10000, Prediction Accuracy = 60.160000000000004%, Loss = 0.5676243543624878
Epoch: 1553, Batch Gradient Norm: 9.782942500241605
Epoch: 1553, Batch Gradient Norm after: 9.782942500241605
Epoch 1554/10000, Prediction Accuracy = 60.182%, Loss = 0.5691011548042297
Epoch: 1554, Batch Gradient Norm: 12.202017990355335
Epoch: 1554, Batch Gradient Norm after: 12.202017990355335
Epoch 1555/10000, Prediction Accuracy = 60.176%, Loss = 0.5834805488586425
Epoch: 1555, Batch Gradient Norm: 12.223208762706138
Epoch: 1555, Batch Gradient Norm after: 12.223208762706138
Epoch 1556/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.5835764050483704
Epoch: 1556, Batch Gradient Norm: 11.932897657554944
Epoch: 1556, Batch Gradient Norm after: 11.932897657554944
Epoch 1557/10000, Prediction Accuracy = 60.176%, Loss = 0.5816789031028747
Epoch: 1557, Batch Gradient Norm: 10.533628181008572
Epoch: 1557, Batch Gradient Norm after: 10.533628181008572
Epoch 1558/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.5735747575759887
Epoch: 1558, Batch Gradient Norm: 12.455070645674919
Epoch: 1558, Batch Gradient Norm after: 12.455070645674919
Epoch 1559/10000, Prediction Accuracy = 60.222%, Loss = 0.5859362483024597
Epoch: 1559, Batch Gradient Norm: 15.483358570335197
Epoch: 1559, Batch Gradient Norm after: 15.483358570335197
Epoch 1560/10000, Prediction Accuracy = 60.146%, Loss = 0.6111782670021058
Epoch: 1560, Batch Gradient Norm: 11.920880219764614
Epoch: 1560, Batch Gradient Norm after: 11.920880219764614
Epoch 1561/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5843215942382812
Epoch: 1561, Batch Gradient Norm: 8.739670763188762
Epoch: 1561, Batch Gradient Norm after: 8.739670763188762
Epoch 1562/10000, Prediction Accuracy = 60.152%, Loss = 0.5649528980255127
Epoch: 1562, Batch Gradient Norm: 7.3407236362312
Epoch: 1562, Batch Gradient Norm after: 7.3407236362312
Epoch 1563/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.5579396605491638
Epoch: 1563, Batch Gradient Norm: 10.3441332932198
Epoch: 1563, Batch Gradient Norm after: 10.3441332932198
Epoch 1564/10000, Prediction Accuracy = 60.17999999999999%, Loss = 0.5720754981040954
Epoch: 1564, Batch Gradient Norm: 12.283610528776329
Epoch: 1564, Batch Gradient Norm after: 12.283610528776329
Epoch 1565/10000, Prediction Accuracy = 60.234%, Loss = 0.5871649384498596
Epoch: 1565, Batch Gradient Norm: 10.792454167843012
Epoch: 1565, Batch Gradient Norm after: 10.792454167843012
Epoch 1566/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.5774159908294678
Epoch: 1566, Batch Gradient Norm: 9.501022246624858
Epoch: 1566, Batch Gradient Norm after: 9.501022246624858
Epoch 1567/10000, Prediction Accuracy = 60.14200000000001%, Loss = 0.5689931035041809
Epoch: 1567, Batch Gradient Norm: 11.18944400381686
Epoch: 1567, Batch Gradient Norm after: 11.18944400381686
Epoch 1568/10000, Prediction Accuracy = 60.188%, Loss = 0.5777300834655762
Epoch: 1568, Batch Gradient Norm: 13.217410276469394
Epoch: 1568, Batch Gradient Norm after: 13.217410276469394
Epoch 1569/10000, Prediction Accuracy = 60.188%, Loss = 0.5912428259849548
Epoch: 1569, Batch Gradient Norm: 15.277003659920291
Epoch: 1569, Batch Gradient Norm after: 15.277003659920291
Epoch 1570/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.606731402873993
Epoch: 1570, Batch Gradient Norm: 13.649327318210245
Epoch: 1570, Batch Gradient Norm after: 13.649327318210245
Epoch 1571/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.596315336227417
Epoch: 1571, Batch Gradient Norm: 9.564576440067869
Epoch: 1571, Batch Gradient Norm after: 9.564576440067869
Epoch 1572/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5688295125961303
Epoch: 1572, Batch Gradient Norm: 6.826909318327192
Epoch: 1572, Batch Gradient Norm after: 6.826909318327192
Epoch 1573/10000, Prediction Accuracy = 60.27%, Loss = 0.555118203163147
Epoch: 1573, Batch Gradient Norm: 7.488762023813749
Epoch: 1573, Batch Gradient Norm after: 7.488762023813749
Epoch 1574/10000, Prediction Accuracy = 60.224000000000004%, Loss = 0.5577058434486389
Epoch: 1574, Batch Gradient Norm: 10.29324494504569
Epoch: 1574, Batch Gradient Norm after: 10.29324494504569
Epoch 1575/10000, Prediction Accuracy = 60.245999999999995%, Loss = 0.5712979674339295
Epoch: 1575, Batch Gradient Norm: 15.872025461750072
Epoch: 1575, Batch Gradient Norm after: 15.872025461750072
Epoch 1576/10000, Prediction Accuracy = 60.188%, Loss = 0.6140324592590332
Epoch: 1576, Batch Gradient Norm: 13.975509485662
Epoch: 1576, Batch Gradient Norm after: 13.975509485662
Epoch 1577/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5979359745979309
Epoch: 1577, Batch Gradient Norm: 11.527845856841958
Epoch: 1577, Batch Gradient Norm after: 11.527845856841958
Epoch 1578/10000, Prediction Accuracy = 60.153999999999996%, Loss = 0.5787012815475464
Epoch: 1578, Batch Gradient Norm: 13.056152338539098
Epoch: 1578, Batch Gradient Norm after: 13.056152338539098
Epoch 1579/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.5884990692138672
Epoch: 1579, Batch Gradient Norm: 13.135398406138773
Epoch: 1579, Batch Gradient Norm after: 13.135398406138773
Epoch 1580/10000, Prediction Accuracy = 60.226%, Loss = 0.5896828293800354
Epoch: 1580, Batch Gradient Norm: 9.791825872343649
Epoch: 1580, Batch Gradient Norm after: 9.791825872343649
Epoch 1581/10000, Prediction Accuracy = 60.246%, Loss = 0.5677411556243896
Epoch: 1581, Batch Gradient Norm: 9.85049108321117
Epoch: 1581, Batch Gradient Norm after: 9.85049108321117
Epoch 1582/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.5678906083106995
Epoch: 1582, Batch Gradient Norm: 10.349520617713425
Epoch: 1582, Batch Gradient Norm after: 10.349520617713425
Epoch 1583/10000, Prediction Accuracy = 60.24400000000001%, Loss = 0.5719622373580933
Epoch: 1583, Batch Gradient Norm: 12.498495139417397
Epoch: 1583, Batch Gradient Norm after: 12.498495139417397
Epoch 1584/10000, Prediction Accuracy = 60.278%, Loss = 0.5876901149749756
Epoch: 1584, Batch Gradient Norm: 10.344927574796394
Epoch: 1584, Batch Gradient Norm after: 10.344927574796394
Epoch 1585/10000, Prediction Accuracy = 60.202%, Loss = 0.5737191677093506
Epoch: 1585, Batch Gradient Norm: 10.096292889315285
Epoch: 1585, Batch Gradient Norm after: 10.096292889315285
Epoch 1586/10000, Prediction Accuracy = 60.258%, Loss = 0.5706583499908447
Epoch: 1586, Batch Gradient Norm: 9.57673507866256
Epoch: 1586, Batch Gradient Norm after: 9.57673507866256
Epoch 1587/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.5659175992012024
Epoch: 1587, Batch Gradient Norm: 13.658074540628782
Epoch: 1587, Batch Gradient Norm after: 13.658074540628782
Epoch 1588/10000, Prediction Accuracy = 60.220000000000006%, Loss = 0.5900781989097595
Epoch: 1588, Batch Gradient Norm: 14.34352344000874
Epoch: 1588, Batch Gradient Norm after: 14.34352344000874
Epoch 1589/10000, Prediction Accuracy = 60.21600000000001%, Loss = 0.5965149164199829
Epoch: 1589, Batch Gradient Norm: 11.806319456283061
Epoch: 1589, Batch Gradient Norm after: 11.806319456283061
Epoch 1590/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.580289363861084
Epoch: 1590, Batch Gradient Norm: 8.255917099477065
Epoch: 1590, Batch Gradient Norm after: 8.255917099477065
Epoch 1591/10000, Prediction Accuracy = 60.267999999999994%, Loss = 0.5604177355766297
Epoch: 1591, Batch Gradient Norm: 11.071992836356111
Epoch: 1591, Batch Gradient Norm after: 11.071992836356111
Epoch 1592/10000, Prediction Accuracy = 60.236000000000004%, Loss = 0.5750491976737976
Epoch: 1592, Batch Gradient Norm: 13.541565335534624
Epoch: 1592, Batch Gradient Norm after: 13.541565335534624
Epoch 1593/10000, Prediction Accuracy = 60.266%, Loss = 0.5933683633804321
Epoch: 1593, Batch Gradient Norm: 12.673605169136781
Epoch: 1593, Batch Gradient Norm after: 12.673605169136781
Epoch 1594/10000, Prediction Accuracy = 60.260000000000005%, Loss = 0.5863672018051147
Epoch: 1594, Batch Gradient Norm: 9.64540993393621
Epoch: 1594, Batch Gradient Norm after: 9.64540993393621
Epoch 1595/10000, Prediction Accuracy = 60.208000000000006%, Loss = 0.5669079661369324
Epoch: 1595, Batch Gradient Norm: 10.563695820680337
Epoch: 1595, Batch Gradient Norm after: 10.563695820680337
Epoch 1596/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.5723102688789368
Epoch: 1596, Batch Gradient Norm: 10.746939720595792
Epoch: 1596, Batch Gradient Norm after: 10.746939720595792
Epoch 1597/10000, Prediction Accuracy = 60.254%, Loss = 0.5737817287445068
Epoch: 1597, Batch Gradient Norm: 11.975097237032697
Epoch: 1597, Batch Gradient Norm after: 11.975097237032697
Epoch 1598/10000, Prediction Accuracy = 60.288%, Loss = 0.5814896345138549
Epoch: 1598, Batch Gradient Norm: 12.989115409193111
Epoch: 1598, Batch Gradient Norm after: 12.989115409193111
Epoch 1599/10000, Prediction Accuracy = 60.302%, Loss = 0.5885043501853943
Epoch: 1599, Batch Gradient Norm: 11.062978204764145
Epoch: 1599, Batch Gradient Norm after: 11.062978204764145
Epoch 1600/10000, Prediction Accuracy = 60.215999999999994%, Loss = 0.5752009749412537
Epoch: 1600, Batch Gradient Norm: 11.004720860661799
Epoch: 1600, Batch Gradient Norm after: 11.004720860661799
Epoch 1601/10000, Prediction Accuracy = 60.288%, Loss = 0.574075448513031
Epoch: 1601, Batch Gradient Norm: 12.922801833284186
Epoch: 1601, Batch Gradient Norm after: 12.922801833284186
Epoch 1602/10000, Prediction Accuracy = 60.294%, Loss = 0.5859450101852417
Epoch: 1602, Batch Gradient Norm: 13.35357939209809
Epoch: 1602, Batch Gradient Norm after: 13.35357939209809
Epoch 1603/10000, Prediction Accuracy = 60.25999999999999%, Loss = 0.589579451084137
Epoch: 1603, Batch Gradient Norm: 10.207203294078774
Epoch: 1603, Batch Gradient Norm after: 10.207203294078774
Epoch 1604/10000, Prediction Accuracy = 60.31%, Loss = 0.5692411899566651
Epoch: 1604, Batch Gradient Norm: 9.55525778016424
Epoch: 1604, Batch Gradient Norm after: 9.55525778016424
Epoch 1605/10000, Prediction Accuracy = 60.239999999999995%, Loss = 0.5657587051391602
Epoch: 1605, Batch Gradient Norm: 8.859858212255416
Epoch: 1605, Batch Gradient Norm after: 8.859858212255416
Epoch 1606/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5620773196220398
Epoch: 1606, Batch Gradient Norm: 11.015259470069477
Epoch: 1606, Batch Gradient Norm after: 11.015259470069477
Epoch 1607/10000, Prediction Accuracy = 60.294%, Loss = 0.5736851453781128
Epoch: 1607, Batch Gradient Norm: 16.082276315916275
Epoch: 1607, Batch Gradient Norm after: 16.082276315916275
Epoch 1608/10000, Prediction Accuracy = 60.196000000000005%, Loss = 0.6106377720832825
Epoch: 1608, Batch Gradient Norm: 14.620745472513251
Epoch: 1608, Batch Gradient Norm after: 14.620745472513251
Epoch 1609/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.5997821927070618
Epoch: 1609, Batch Gradient Norm: 9.970352179224712
Epoch: 1609, Batch Gradient Norm after: 9.970352179224712
Epoch 1610/10000, Prediction Accuracy = 60.258%, Loss = 0.568415641784668
Epoch: 1610, Batch Gradient Norm: 7.879012679616359
Epoch: 1610, Batch Gradient Norm after: 7.879012679616359
Epoch 1611/10000, Prediction Accuracy = 60.34599999999999%, Loss = 0.5574390649795532
Epoch: 1611, Batch Gradient Norm: 9.998101589044625
Epoch: 1611, Batch Gradient Norm after: 9.998101589044625
Epoch 1612/10000, Prediction Accuracy = 60.251999999999995%, Loss = 0.5680316329002381
Epoch: 1612, Batch Gradient Norm: 11.662349798413132
Epoch: 1612, Batch Gradient Norm after: 11.662349798413132
Epoch 1613/10000, Prediction Accuracy = 60.278%, Loss = 0.5779711008071899
Epoch: 1613, Batch Gradient Norm: 11.547363836727014
Epoch: 1613, Batch Gradient Norm after: 11.547363836727014
Epoch 1614/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.5769375205039978
Epoch: 1614, Batch Gradient Norm: 10.530826052749255
Epoch: 1614, Batch Gradient Norm after: 10.530826052749255
Epoch 1615/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.5688329935073853
Epoch: 1615, Batch Gradient Norm: 11.505441776643087
Epoch: 1615, Batch Gradient Norm after: 11.505441776643087
Epoch 1616/10000, Prediction Accuracy = 60.29600000000001%, Loss = 0.5737252473831177
Epoch: 1616, Batch Gradient Norm: 12.849049942014014
Epoch: 1616, Batch Gradient Norm after: 12.849049942014014
Epoch 1617/10000, Prediction Accuracy = 60.257999999999996%, Loss = 0.5827138900756836
Epoch: 1617, Batch Gradient Norm: 12.475561992212473
Epoch: 1617, Batch Gradient Norm after: 12.475561992212473
Epoch 1618/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5816660284996032
Epoch: 1618, Batch Gradient Norm: 13.139650195753015
Epoch: 1618, Batch Gradient Norm after: 13.139650195753015
Epoch 1619/10000, Prediction Accuracy = 60.23199999999999%, Loss = 0.5874268293380738
Epoch: 1619, Batch Gradient Norm: 11.743478666095605
Epoch: 1619, Batch Gradient Norm after: 11.743478666095605
Epoch 1620/10000, Prediction Accuracy = 60.275999999999996%, Loss = 0.5785203218460083
Epoch: 1620, Batch Gradient Norm: 10.106940983604497
Epoch: 1620, Batch Gradient Norm after: 10.106940983604497
Epoch 1621/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5678640007972717
Epoch: 1621, Batch Gradient Norm: 10.612263392387794
Epoch: 1621, Batch Gradient Norm after: 10.612263392387794
Epoch 1622/10000, Prediction Accuracy = 60.258%, Loss = 0.5705073952674866
Epoch: 1622, Batch Gradient Norm: 12.597898309157584
Epoch: 1622, Batch Gradient Norm after: 12.597898309157584
Epoch 1623/10000, Prediction Accuracy = 60.298%, Loss = 0.5838090658187867
Epoch: 1623, Batch Gradient Norm: 11.295425436052462
Epoch: 1623, Batch Gradient Norm after: 11.295425436052462
Epoch 1624/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.575032901763916
Epoch: 1624, Batch Gradient Norm: 10.155546010579789
Epoch: 1624, Batch Gradient Norm after: 10.155546010579789
Epoch 1625/10000, Prediction Accuracy = 60.291999999999994%, Loss = 0.5674390912055969
Epoch: 1625, Batch Gradient Norm: 10.76732684098528
Epoch: 1625, Batch Gradient Norm after: 10.76732684098528
Epoch 1626/10000, Prediction Accuracy = 60.272000000000006%, Loss = 0.5708923697471618
Epoch: 1626, Batch Gradient Norm: 14.578964795119461
Epoch: 1626, Batch Gradient Norm after: 14.578964795119461
Epoch 1627/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.6010418891906738
Epoch: 1627, Batch Gradient Norm: 9.669266676250944
Epoch: 1627, Batch Gradient Norm after: 9.669266676250944
Epoch 1628/10000, Prediction Accuracy = 60.238%, Loss = 0.566472589969635
Epoch: 1628, Batch Gradient Norm: 9.765230794835363
Epoch: 1628, Batch Gradient Norm after: 9.765230794835363
Epoch 1629/10000, Prediction Accuracy = 60.326%, Loss = 0.5654895424842834
Epoch: 1629, Batch Gradient Norm: 8.835054896895814
Epoch: 1629, Batch Gradient Norm after: 8.835054896895814
Epoch 1630/10000, Prediction Accuracy = 60.343999999999994%, Loss = 0.5600410342216492
Epoch: 1630, Batch Gradient Norm: 11.149569355850703
Epoch: 1630, Batch Gradient Norm after: 11.149569355850703
Epoch 1631/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.5725320100784301
Epoch: 1631, Batch Gradient Norm: 11.869092073300605
Epoch: 1631, Batch Gradient Norm after: 11.869092073300605
Epoch 1632/10000, Prediction Accuracy = 60.248000000000005%, Loss = 0.5771609425544739
Epoch: 1632, Batch Gradient Norm: 12.95890637511987
Epoch: 1632, Batch Gradient Norm after: 12.95890637511987
Epoch 1633/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.5865827798843384
Epoch: 1633, Batch Gradient Norm: 11.473453239678559
Epoch: 1633, Batch Gradient Norm after: 11.473453239678559
Epoch 1634/10000, Prediction Accuracy = 60.222%, Loss = 0.576670479774475
Epoch: 1634, Batch Gradient Norm: 12.380357680183726
Epoch: 1634, Batch Gradient Norm after: 12.380357680183726
Epoch 1635/10000, Prediction Accuracy = 60.402%, Loss = 0.5807961344718933
Epoch: 1635, Batch Gradient Norm: 15.466274207548413
Epoch: 1635, Batch Gradient Norm after: 15.466274207548413
Epoch 1636/10000, Prediction Accuracy = 60.238%, Loss = 0.6038276195526123
Epoch: 1636, Batch Gradient Norm: 13.370892540560408
Epoch: 1636, Batch Gradient Norm after: 13.370892540560408
Epoch 1637/10000, Prediction Accuracy = 60.348%, Loss = 0.5867358684539795
Epoch: 1637, Batch Gradient Norm: 10.762403049347375
Epoch: 1637, Batch Gradient Norm after: 10.762403049347375
Epoch 1638/10000, Prediction Accuracy = 60.282000000000004%, Loss = 0.5697579145431518
Epoch: 1638, Batch Gradient Norm: 10.993378311663955
Epoch: 1638, Batch Gradient Norm after: 10.993378311663955
Epoch 1639/10000, Prediction Accuracy = 60.32199999999999%, Loss = 0.571359395980835
Epoch: 1639, Batch Gradient Norm: 11.389030864533645
Epoch: 1639, Batch Gradient Norm after: 11.389030864533645
Epoch 1640/10000, Prediction Accuracy = 60.330000000000005%, Loss = 0.5742650389671325
Epoch: 1640, Batch Gradient Norm: 10.865160117979496
Epoch: 1640, Batch Gradient Norm after: 10.865160117979496
Epoch 1641/10000, Prediction Accuracy = 60.314%, Loss = 0.5713663101196289
Epoch: 1641, Batch Gradient Norm: 10.781047441843066
Epoch: 1641, Batch Gradient Norm after: 10.781047441843066
Epoch 1642/10000, Prediction Accuracy = 60.314%, Loss = 0.5712783098220825
Epoch: 1642, Batch Gradient Norm: 9.957910704678085
Epoch: 1642, Batch Gradient Norm after: 9.957910704678085
Epoch 1643/10000, Prediction Accuracy = 60.288%, Loss = 0.5659266471862793
Epoch: 1643, Batch Gradient Norm: 10.970087623298836
Epoch: 1643, Batch Gradient Norm after: 10.970087623298836
Epoch 1644/10000, Prediction Accuracy = 60.31600000000001%, Loss = 0.5714288592338562
Epoch: 1644, Batch Gradient Norm: 12.077528078669756
Epoch: 1644, Batch Gradient Norm after: 12.077528078669756
Epoch 1645/10000, Prediction Accuracy = 60.294%, Loss = 0.5781925797462464
Epoch: 1645, Batch Gradient Norm: 12.066881538965676
Epoch: 1645, Batch Gradient Norm after: 12.066881538965676
Epoch 1646/10000, Prediction Accuracy = 60.314%, Loss = 0.5770754814147949
Epoch: 1646, Batch Gradient Norm: 11.097975234958701
Epoch: 1646, Batch Gradient Norm after: 11.097975234958701
Epoch 1647/10000, Prediction Accuracy = 60.31999999999999%, Loss = 0.5714442610740662
Epoch: 1647, Batch Gradient Norm: 9.278670598626029
Epoch: 1647, Batch Gradient Norm after: 9.278670598626029
Epoch 1648/10000, Prediction Accuracy = 60.298%, Loss = 0.5607331752777099
Epoch: 1648, Batch Gradient Norm: 9.093693337764602
Epoch: 1648, Batch Gradient Norm after: 9.093693337764602
Epoch 1649/10000, Prediction Accuracy = 60.286%, Loss = 0.5603493332862854
Epoch: 1649, Batch Gradient Norm: 11.336336222695508
Epoch: 1649, Batch Gradient Norm after: 11.336336222695508
Epoch 1650/10000, Prediction Accuracy = 60.372%, Loss = 0.5738222122192382
Epoch: 1650, Batch Gradient Norm: 11.749982120362466
Epoch: 1650, Batch Gradient Norm after: 11.749982120362466
Epoch 1651/10000, Prediction Accuracy = 60.23%, Loss = 0.5778070688247681
Epoch: 1651, Batch Gradient Norm: 12.62704325020744
Epoch: 1651, Batch Gradient Norm after: 12.62704325020744
Epoch 1652/10000, Prediction Accuracy = 60.254000000000005%, Loss = 0.5823815703392029
Epoch: 1652, Batch Gradient Norm: 13.029618636609813
Epoch: 1652, Batch Gradient Norm after: 13.029618636609813
Epoch 1653/10000, Prediction Accuracy = 60.274%, Loss = 0.5828790903091431
Epoch: 1653, Batch Gradient Norm: 13.155094901616936
Epoch: 1653, Batch Gradient Norm after: 13.155094901616936
Epoch 1654/10000, Prediction Accuracy = 60.374%, Loss = 0.5837233304977417
Epoch: 1654, Batch Gradient Norm: 11.512933748826976
Epoch: 1654, Batch Gradient Norm after: 11.512933748826976
Epoch 1655/10000, Prediction Accuracy = 60.364%, Loss = 0.5735228657722473
Epoch: 1655, Batch Gradient Norm: 8.989387020024246
Epoch: 1655, Batch Gradient Norm after: 8.989387020024246
Epoch 1656/10000, Prediction Accuracy = 60.352%, Loss = 0.5588844180107116
Epoch: 1656, Batch Gradient Norm: 10.876112231707399
Epoch: 1656, Batch Gradient Norm after: 10.876112231707399
Epoch 1657/10000, Prediction Accuracy = 60.374%, Loss = 0.5697000980377197
Epoch: 1657, Batch Gradient Norm: 11.778844855002818
Epoch: 1657, Batch Gradient Norm after: 11.778844855002818
Epoch 1658/10000, Prediction Accuracy = 60.318%, Loss = 0.5761295437812806
Epoch: 1658, Batch Gradient Norm: 14.42330450617598
Epoch: 1658, Batch Gradient Norm after: 14.42330450617598
Epoch 1659/10000, Prediction Accuracy = 60.263999999999996%, Loss = 0.5972016453742981
Epoch: 1659, Batch Gradient Norm: 10.720103491265123
Epoch: 1659, Batch Gradient Norm after: 10.720103491265123
Epoch 1660/10000, Prediction Accuracy = 60.35799999999999%, Loss = 0.5694450974464417
Epoch: 1660, Batch Gradient Norm: 10.604762519527009
Epoch: 1660, Batch Gradient Norm after: 10.604762519527009
Epoch 1661/10000, Prediction Accuracy = 60.3%, Loss = 0.5670880794525146
Epoch: 1661, Batch Gradient Norm: 13.713730698618923
Epoch: 1661, Batch Gradient Norm after: 13.713730698618923
Epoch 1662/10000, Prediction Accuracy = 60.367999999999995%, Loss = 0.587829852104187
Epoch: 1662, Batch Gradient Norm: 13.155144217372793
Epoch: 1662, Batch Gradient Norm after: 13.155144217372793
Epoch 1663/10000, Prediction Accuracy = 60.342%, Loss = 0.5862265825271606
Epoch: 1663, Batch Gradient Norm: 9.062852306440542
Epoch: 1663, Batch Gradient Norm after: 9.062852306440542
Epoch 1664/10000, Prediction Accuracy = 60.326%, Loss = 0.5595459342002869
Epoch: 1664, Batch Gradient Norm: 8.40826320984734
Epoch: 1664, Batch Gradient Norm after: 8.40826320984734
Epoch 1665/10000, Prediction Accuracy = 60.426%, Loss = 0.5557038903236389
Epoch: 1665, Batch Gradient Norm: 9.476143047365058
Epoch: 1665, Batch Gradient Norm after: 9.476143047365058
Epoch 1666/10000, Prediction Accuracy = 60.26800000000001%, Loss = 0.56039799451828
Epoch: 1666, Batch Gradient Norm: 13.73309620760578
Epoch: 1666, Batch Gradient Norm after: 13.73309620760578
Epoch 1667/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.5881899356842041
Epoch: 1667, Batch Gradient Norm: 13.270141574580054
Epoch: 1667, Batch Gradient Norm after: 13.270141574580054
Epoch 1668/10000, Prediction Accuracy = 60.274%, Loss = 0.5841959834098815
Epoch: 1668, Batch Gradient Norm: 12.430683483603058
Epoch: 1668, Batch Gradient Norm after: 12.430683483603058
Epoch 1669/10000, Prediction Accuracy = 60.336%, Loss = 0.5777745723724366
Epoch: 1669, Batch Gradient Norm: 12.192031016023595
Epoch: 1669, Batch Gradient Norm after: 12.192031016023595
Epoch 1670/10000, Prediction Accuracy = 60.38000000000001%, Loss = 0.5793304204940796
Epoch: 1670, Batch Gradient Norm: 11.20915114614906
Epoch: 1670, Batch Gradient Norm after: 11.20915114614906
Epoch 1671/10000, Prediction Accuracy = 60.36%, Loss = 0.5726366519927979
Epoch: 1671, Batch Gradient Norm: 9.4443749903278
Epoch: 1671, Batch Gradient Norm after: 9.4443749903278
Epoch 1672/10000, Prediction Accuracy = 60.342%, Loss = 0.561972713470459
Epoch: 1672, Batch Gradient Norm: 8.969804927696448
Epoch: 1672, Batch Gradient Norm after: 8.969804927696448
Epoch 1673/10000, Prediction Accuracy = 60.376%, Loss = 0.5582852602005005
Epoch: 1673, Batch Gradient Norm: 10.214336259935337
Epoch: 1673, Batch Gradient Norm after: 10.214336259935337
Epoch 1674/10000, Prediction Accuracy = 60.366%, Loss = 0.5653416037559509
Epoch: 1674, Batch Gradient Norm: 10.69230983148371
Epoch: 1674, Batch Gradient Norm after: 10.69230983148371
Epoch 1675/10000, Prediction Accuracy = 60.33599999999999%, Loss = 0.5679292201995849
Epoch: 1675, Batch Gradient Norm: 12.635660347332376
Epoch: 1675, Batch Gradient Norm after: 12.635660347332376
Epoch 1676/10000, Prediction Accuracy = 60.384%, Loss = 0.5806640744209289
Epoch: 1676, Batch Gradient Norm: 13.234351698153146
Epoch: 1676, Batch Gradient Norm after: 13.234351698153146
Epoch 1677/10000, Prediction Accuracy = 60.388%, Loss = 0.5840005040168762
Epoch: 1677, Batch Gradient Norm: 13.040908696866625
Epoch: 1677, Batch Gradient Norm after: 13.040908696866625
Epoch 1678/10000, Prediction Accuracy = 60.384%, Loss = 0.5814321398735046
Epoch: 1678, Batch Gradient Norm: 12.469471199854178
Epoch: 1678, Batch Gradient Norm after: 12.469471199854178
Epoch 1679/10000, Prediction Accuracy = 60.386%, Loss = 0.5772507309913635
Epoch: 1679, Batch Gradient Norm: 10.912678230125143
Epoch: 1679, Batch Gradient Norm after: 10.912678230125143
Epoch 1680/10000, Prediction Accuracy = 60.352%, Loss = 0.5675304293632507
Epoch: 1680, Batch Gradient Norm: 10.901938351779409
Epoch: 1680, Batch Gradient Norm after: 10.901938351779409
Epoch 1681/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5673375487327575
Epoch: 1681, Batch Gradient Norm: 10.883106550840463
Epoch: 1681, Batch Gradient Norm after: 10.883106550840463
Epoch 1682/10000, Prediction Accuracy = 60.41199999999999%, Loss = 0.5673423528671264
Epoch: 1682, Batch Gradient Norm: 11.497104029853732
Epoch: 1682, Batch Gradient Norm after: 11.497104029853732
Epoch 1683/10000, Prediction Accuracy = 60.298%, Loss = 0.570965850353241
Epoch: 1683, Batch Gradient Norm: 11.71323175183797
Epoch: 1683, Batch Gradient Norm after: 11.71323175183797
Epoch 1684/10000, Prediction Accuracy = 60.352%, Loss = 0.5717413663864136
Epoch: 1684, Batch Gradient Norm: 13.350653625408235
Epoch: 1684, Batch Gradient Norm after: 13.350653625408235
Epoch 1685/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.586266040802002
Epoch: 1685, Batch Gradient Norm: 11.029034273206081
Epoch: 1685, Batch Gradient Norm after: 11.029034273206081
Epoch 1686/10000, Prediction Accuracy = 60.34400000000001%, Loss = 0.571056866645813
Epoch: 1686, Batch Gradient Norm: 8.939428005294701
Epoch: 1686, Batch Gradient Norm after: 8.939428005294701
Epoch 1687/10000, Prediction Accuracy = 60.352%, Loss = 0.5578177213668823
Epoch: 1687, Batch Gradient Norm: 10.343798126028377
Epoch: 1687, Batch Gradient Norm after: 10.343798126028377
Epoch 1688/10000, Prediction Accuracy = 60.324%, Loss = 0.5646762013435364
Epoch: 1688, Batch Gradient Norm: 10.467215655222242
Epoch: 1688, Batch Gradient Norm after: 10.467215655222242
Epoch 1689/10000, Prediction Accuracy = 60.4%, Loss = 0.5655091047286988
Epoch: 1689, Batch Gradient Norm: 10.689650768815243
Epoch: 1689, Batch Gradient Norm after: 10.689650768815243
Epoch 1690/10000, Prediction Accuracy = 60.326%, Loss = 0.5665910243988037
Epoch: 1690, Batch Gradient Norm: 10.628379295064454
Epoch: 1690, Batch Gradient Norm after: 10.628379295064454
Epoch 1691/10000, Prediction Accuracy = 60.384%, Loss = 0.565374779701233
Epoch: 1691, Batch Gradient Norm: 12.173697309815632
Epoch: 1691, Batch Gradient Norm after: 12.173697309815632
Epoch 1692/10000, Prediction Accuracy = 60.35%, Loss = 0.5755336880683899
Epoch: 1692, Batch Gradient Norm: 13.529224296739194
Epoch: 1692, Batch Gradient Norm after: 13.529224296739194
Epoch 1693/10000, Prediction Accuracy = 60.346000000000004%, Loss = 0.5866281032562256
Epoch: 1693, Batch Gradient Norm: 11.369410298857261
Epoch: 1693, Batch Gradient Norm after: 11.369410298857261
Epoch 1694/10000, Prediction Accuracy = 60.342%, Loss = 0.5730179786682129
Epoch: 1694, Batch Gradient Norm: 9.015748701435292
Epoch: 1694, Batch Gradient Norm after: 9.015748701435292
Epoch 1695/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.5579843521118164
Epoch: 1695, Batch Gradient Norm: 8.891673613660691
Epoch: 1695, Batch Gradient Norm after: 8.891673613660691
Epoch 1696/10000, Prediction Accuracy = 60.3%, Loss = 0.5566047310829163
Epoch: 1696, Batch Gradient Norm: 13.413763536743446
Epoch: 1696, Batch Gradient Norm after: 13.413763536743446
Epoch 1697/10000, Prediction Accuracy = 60.35999999999999%, Loss = 0.5843693256378174
Epoch: 1697, Batch Gradient Norm: 14.33358983707766
Epoch: 1697, Batch Gradient Norm after: 14.33358983707766
Epoch 1698/10000, Prediction Accuracy = 60.286%, Loss = 0.5917298316955566
Epoch: 1698, Batch Gradient Norm: 12.432687217075465
Epoch: 1698, Batch Gradient Norm after: 12.432687217075465
Epoch 1699/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.5747557282447815
Epoch: 1699, Batch Gradient Norm: 12.40296915912559
Epoch: 1699, Batch Gradient Norm after: 12.40296915912559
Epoch 1700/10000, Prediction Accuracy = 60.36800000000001%, Loss = 0.5749437808990479
Epoch: 1700, Batch Gradient Norm: 11.399342318747134
Epoch: 1700, Batch Gradient Norm after: 11.399342318747134
Epoch 1701/10000, Prediction Accuracy = 60.396%, Loss = 0.569171667098999
Epoch: 1701, Batch Gradient Norm: 9.473448881259227
Epoch: 1701, Batch Gradient Norm after: 9.473448881259227
Epoch 1702/10000, Prediction Accuracy = 60.388%, Loss = 0.5582370758056641
Epoch: 1702, Batch Gradient Norm: 10.027698749056178
Epoch: 1702, Batch Gradient Norm after: 10.027698749056178
Epoch 1703/10000, Prediction Accuracy = 60.36600000000001%, Loss = 0.5618173360824585
Epoch: 1703, Batch Gradient Norm: 11.098086240098928
Epoch: 1703, Batch Gradient Norm after: 11.098086240098928
Epoch 1704/10000, Prediction Accuracy = 60.448%, Loss = 0.5688556790351867
Epoch: 1704, Batch Gradient Norm: 12.430758709800104
Epoch: 1704, Batch Gradient Norm after: 12.430758709800104
Epoch 1705/10000, Prediction Accuracy = 60.32000000000001%, Loss = 0.578773844242096
Epoch: 1705, Batch Gradient Norm: 11.813682478757944
Epoch: 1705, Batch Gradient Norm after: 11.813682478757944
Epoch 1706/10000, Prediction Accuracy = 60.44%, Loss = 0.5738853693008423
Epoch: 1706, Batch Gradient Norm: 10.736471574687286
Epoch: 1706, Batch Gradient Norm after: 10.736471574687286
Epoch 1707/10000, Prediction Accuracy = 60.35%, Loss = 0.5658702731132508
Epoch: 1707, Batch Gradient Norm: 11.524066773126814
Epoch: 1707, Batch Gradient Norm after: 11.524066773126814
Epoch 1708/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.5698373556137085
Epoch: 1708, Batch Gradient Norm: 12.734023352032334
Epoch: 1708, Batch Gradient Norm after: 12.734023352032334
Epoch 1709/10000, Prediction Accuracy = 60.391999999999996%, Loss = 0.5784536719322204
Epoch: 1709, Batch Gradient Norm: 13.65337516375031
Epoch: 1709, Batch Gradient Norm after: 13.65337516375031
Epoch 1710/10000, Prediction Accuracy = 60.376%, Loss = 0.5857776045799256
Epoch: 1710, Batch Gradient Norm: 13.786167903576747
Epoch: 1710, Batch Gradient Norm after: 13.786167903576747
Epoch 1711/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5869162321090698
Epoch: 1711, Batch Gradient Norm: 11.77978229185091
Epoch: 1711, Batch Gradient Norm after: 11.77978229185091
Epoch 1712/10000, Prediction Accuracy = 60.386%, Loss = 0.5735660314559936
Epoch: 1712, Batch Gradient Norm: 11.07221959298586
Epoch: 1712, Batch Gradient Norm after: 11.07221959298586
Epoch 1713/10000, Prediction Accuracy = 60.42999999999999%, Loss = 0.5690739035606385
Epoch: 1713, Batch Gradient Norm: 8.298579990389259
Epoch: 1713, Batch Gradient Norm after: 8.298579990389259
Epoch 1714/10000, Prediction Accuracy = 60.374%, Loss = 0.5527330994606018
Epoch: 1714, Batch Gradient Norm: 9.95776521012075
Epoch: 1714, Batch Gradient Norm after: 9.95776521012075
Epoch 1715/10000, Prediction Accuracy = 60.398%, Loss = 0.560895037651062
Epoch: 1715, Batch Gradient Norm: 9.715995602005119
Epoch: 1715, Batch Gradient Norm after: 9.715995602005119
Epoch 1716/10000, Prediction Accuracy = 60.376%, Loss = 0.5590686678886414
Epoch: 1716, Batch Gradient Norm: 11.995959344166234
Epoch: 1716, Batch Gradient Norm after: 11.995959344166234
Epoch 1717/10000, Prediction Accuracy = 60.314%, Loss = 0.5729546546936035
Epoch: 1717, Batch Gradient Norm: 11.43604774270358
Epoch: 1717, Batch Gradient Norm after: 11.43604774270358
Epoch 1718/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5682036995887756
Epoch: 1718, Batch Gradient Norm: 11.364394673003284
Epoch: 1718, Batch Gradient Norm after: 11.364394673003284
Epoch 1719/10000, Prediction Accuracy = 60.334%, Loss = 0.5670272946357727
Epoch: 1719, Batch Gradient Norm: 12.649356615758371
Epoch: 1719, Batch Gradient Norm after: 12.649356615758371
Epoch 1720/10000, Prediction Accuracy = 60.355999999999995%, Loss = 0.5747417688369751
Epoch: 1720, Batch Gradient Norm: 12.356662457064266
Epoch: 1720, Batch Gradient Norm after: 12.356662457064266
Epoch 1721/10000, Prediction Accuracy = 60.378%, Loss = 0.574565303325653
Epoch: 1721, Batch Gradient Norm: 10.776488874568019
Epoch: 1721, Batch Gradient Norm after: 10.776488874568019
Epoch 1722/10000, Prediction Accuracy = 60.36%, Loss = 0.5650826930999756
Epoch: 1722, Batch Gradient Norm: 8.653577071494114
Epoch: 1722, Batch Gradient Norm after: 8.653577071494114
Epoch 1723/10000, Prediction Accuracy = 60.426%, Loss = 0.5532169461250305
Epoch: 1723, Batch Gradient Norm: 9.382224784096442
Epoch: 1723, Batch Gradient Norm after: 9.382224784096442
Epoch 1724/10000, Prediction Accuracy = 60.40200000000001%, Loss = 0.5566789865493774
Epoch: 1724, Batch Gradient Norm: 12.462720519791867
Epoch: 1724, Batch Gradient Norm after: 12.462720519791867
Epoch 1725/10000, Prediction Accuracy = 60.327999999999996%, Loss = 0.5761291861534119
Epoch: 1725, Batch Gradient Norm: 12.589594344340352
Epoch: 1725, Batch Gradient Norm after: 12.589594344340352
Epoch 1726/10000, Prediction Accuracy = 60.426%, Loss = 0.5799201726913452
Epoch: 1726, Batch Gradient Norm: 10.35674522381132
Epoch: 1726, Batch Gradient Norm after: 10.35674522381132
Epoch 1727/10000, Prediction Accuracy = 60.484%, Loss = 0.5633459210395813
Epoch: 1727, Batch Gradient Norm: 10.173865067431853
Epoch: 1727, Batch Gradient Norm after: 10.173865067431853
Epoch 1728/10000, Prediction Accuracy = 60.38599999999999%, Loss = 0.5611482858657837
Epoch: 1728, Batch Gradient Norm: 10.840686070218105
Epoch: 1728, Batch Gradient Norm after: 10.840686070218105
Epoch 1729/10000, Prediction Accuracy = 60.396%, Loss = 0.5649296760559082
Epoch: 1729, Batch Gradient Norm: 12.35059464625224
Epoch: 1729, Batch Gradient Norm after: 12.35059464625224
Epoch 1730/10000, Prediction Accuracy = 60.384%, Loss = 0.5739137172698975
Epoch: 1730, Batch Gradient Norm: 12.272249425038178
Epoch: 1730, Batch Gradient Norm after: 12.272249425038178
Epoch 1731/10000, Prediction Accuracy = 60.33%, Loss = 0.5740157723426819
Epoch: 1731, Batch Gradient Norm: 12.252256324033077
Epoch: 1731, Batch Gradient Norm after: 12.252256324033077
Epoch 1732/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.5747702717781067
Epoch: 1732, Batch Gradient Norm: 12.238869762345422
Epoch: 1732, Batch Gradient Norm after: 12.238869762345422
Epoch 1733/10000, Prediction Accuracy = 60.39200000000001%, Loss = 0.5737231731414795
Epoch: 1733, Batch Gradient Norm: 12.974442194432726
Epoch: 1733, Batch Gradient Norm after: 12.974442194432726
Epoch 1734/10000, Prediction Accuracy = 60.376%, Loss = 0.5778726935386658
Epoch: 1734, Batch Gradient Norm: 13.254683330903207
Epoch: 1734, Batch Gradient Norm after: 13.254683330903207
Epoch 1735/10000, Prediction Accuracy = 60.352%, Loss = 0.5793273687362671
Epoch: 1735, Batch Gradient Norm: 12.923581911947508
Epoch: 1735, Batch Gradient Norm after: 12.923581911947508
Epoch 1736/10000, Prediction Accuracy = 60.428%, Loss = 0.5777937531471252
Epoch: 1736, Batch Gradient Norm: 12.017620641529147
Epoch: 1736, Batch Gradient Norm after: 12.017620641529147
Epoch 1737/10000, Prediction Accuracy = 60.334%, Loss = 0.5729742765426635
Epoch: 1737, Batch Gradient Norm: 10.755509990037893
Epoch: 1737, Batch Gradient Norm after: 10.755509990037893
Epoch 1738/10000, Prediction Accuracy = 60.44199999999999%, Loss = 0.565628957748413
Epoch: 1738, Batch Gradient Norm: 8.640280483995795
Epoch: 1738, Batch Gradient Norm after: 8.640280483995795
Epoch 1739/10000, Prediction Accuracy = 60.38199999999999%, Loss = 0.5532196283340454
Epoch: 1739, Batch Gradient Norm: 10.75092214464133
Epoch: 1739, Batch Gradient Norm after: 10.75092214464133
Epoch 1740/10000, Prediction Accuracy = 60.452%, Loss = 0.5647151827812195
Epoch: 1740, Batch Gradient Norm: 9.633435598121906
Epoch: 1740, Batch Gradient Norm after: 9.633435598121906
Epoch 1741/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.557888126373291
Epoch: 1741, Batch Gradient Norm: 10.603976780822181
Epoch: 1741, Batch Gradient Norm after: 10.603976780822181
Epoch 1742/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.563020646572113
Epoch: 1742, Batch Gradient Norm: 8.967170778005174
Epoch: 1742, Batch Gradient Norm after: 8.967170778005174
Epoch 1743/10000, Prediction Accuracy = 60.424%, Loss = 0.5529810190200806
Epoch: 1743, Batch Gradient Norm: 11.452034696823603
Epoch: 1743, Batch Gradient Norm after: 11.452034696823603
Epoch 1744/10000, Prediction Accuracy = 60.386%, Loss = 0.5668540716171264
Epoch: 1744, Batch Gradient Norm: 14.05708019702921
Epoch: 1744, Batch Gradient Norm after: 14.05708019702921
Epoch 1745/10000, Prediction Accuracy = 60.472%, Loss = 0.5849058628082275
Epoch: 1745, Batch Gradient Norm: 12.803326732637391
Epoch: 1745, Batch Gradient Norm after: 12.803326732637391
Epoch 1746/10000, Prediction Accuracy = 60.403999999999996%, Loss = 0.5764871954917907
Epoch: 1746, Batch Gradient Norm: 11.341684840977152
Epoch: 1746, Batch Gradient Norm after: 11.341684840977152
Epoch 1747/10000, Prediction Accuracy = 60.39399999999999%, Loss = 0.5660920023918152
Epoch: 1747, Batch Gradient Norm: 11.93575340803273
Epoch: 1747, Batch Gradient Norm after: 11.93575340803273
Epoch 1748/10000, Prediction Accuracy = 60.44%, Loss = 0.5730268836021424
Epoch: 1748, Batch Gradient Norm: 10.146147461957415
Epoch: 1748, Batch Gradient Norm after: 10.146147461957415
Epoch 1749/10000, Prediction Accuracy = 60.444%, Loss = 0.561357045173645
Epoch: 1749, Batch Gradient Norm: 8.668221331239057
Epoch: 1749, Batch Gradient Norm after: 8.668221331239057
Epoch 1750/10000, Prediction Accuracy = 60.41799999999999%, Loss = 0.5526655435562133
Epoch: 1750, Batch Gradient Norm: 9.111755859738839
Epoch: 1750, Batch Gradient Norm after: 9.111755859738839
Epoch 1751/10000, Prediction Accuracy = 60.428%, Loss = 0.5541698813438416
Epoch: 1751, Batch Gradient Norm: 12.072561047343113
Epoch: 1751, Batch Gradient Norm after: 12.072561047343113
Epoch 1752/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5715859055519104
Epoch: 1752, Batch Gradient Norm: 15.194788542666302
Epoch: 1752, Batch Gradient Norm after: 15.194788542666302
Epoch 1753/10000, Prediction Accuracy = 60.424%, Loss = 0.5958742260932922
Epoch: 1753, Batch Gradient Norm: 12.603295552992122
Epoch: 1753, Batch Gradient Norm after: 12.603295552992122
Epoch 1754/10000, Prediction Accuracy = 60.42%, Loss = 0.5759496808052063
Epoch: 1754, Batch Gradient Norm: 9.200968546029662
Epoch: 1754, Batch Gradient Norm after: 9.200968546029662
Epoch 1755/10000, Prediction Accuracy = 60.45400000000001%, Loss = 0.5540089726448059
Epoch: 1755, Batch Gradient Norm: 9.231342587660642
Epoch: 1755, Batch Gradient Norm after: 9.231342587660642
Epoch 1756/10000, Prediction Accuracy = 60.424%, Loss = 0.5529000520706177
Epoch: 1756, Batch Gradient Norm: 12.431833448168499
Epoch: 1756, Batch Gradient Norm after: 12.431833448168499
Epoch 1757/10000, Prediction Accuracy = 60.432%, Loss = 0.5725173115730285
Epoch: 1757, Batch Gradient Norm: 13.55825910404127
Epoch: 1757, Batch Gradient Norm after: 13.55825910404127
Epoch 1758/10000, Prediction Accuracy = 60.45799999999999%, Loss = 0.5828022837638855
Epoch: 1758, Batch Gradient Norm: 12.187969060362336
Epoch: 1758, Batch Gradient Norm after: 12.187969060362336
Epoch 1759/10000, Prediction Accuracy = 60.336%, Loss = 0.5731570363044739
Epoch: 1759, Batch Gradient Norm: 11.748658601792942
Epoch: 1759, Batch Gradient Norm after: 11.748658601792942
Epoch 1760/10000, Prediction Accuracy = 60.462%, Loss = 0.5682295322418213
Epoch: 1760, Batch Gradient Norm: 11.97480564410715
Epoch: 1760, Batch Gradient Norm after: 11.97480564410715
Epoch 1761/10000, Prediction Accuracy = 60.36999999999999%, Loss = 0.5683266639709472
Epoch: 1761, Batch Gradient Norm: 11.042714059462558
Epoch: 1761, Batch Gradient Norm after: 11.042714059462558
Epoch 1762/10000, Prediction Accuracy = 60.426%, Loss = 0.5619625449180603
Epoch: 1762, Batch Gradient Norm: 9.612017305087843
Epoch: 1762, Batch Gradient Norm after: 9.612017305087843
Epoch 1763/10000, Prediction Accuracy = 60.41600000000001%, Loss = 0.5541616082191467
Epoch: 1763, Batch Gradient Norm: 11.904483328511956
Epoch: 1763, Batch Gradient Norm after: 11.904483328511956
Epoch 1764/10000, Prediction Accuracy = 60.414%, Loss = 0.5690183997154236
Epoch: 1764, Batch Gradient Norm: 12.391944283516484
Epoch: 1764, Batch Gradient Norm after: 12.391944283516484
Epoch 1765/10000, Prediction Accuracy = 60.422000000000004%, Loss = 0.5756929278373718
Epoch: 1765, Batch Gradient Norm: 11.124225176060389
Epoch: 1765, Batch Gradient Norm after: 11.124225176060389
Epoch 1766/10000, Prediction Accuracy = 60.426%, Loss = 0.5662830352783204
Epoch: 1766, Batch Gradient Norm: 8.626338177130112
Epoch: 1766, Batch Gradient Norm after: 8.626338177130112
Epoch 1767/10000, Prediction Accuracy = 60.428%, Loss = 0.5504038333892822
Epoch: 1767, Batch Gradient Norm: 11.049651097613316
Epoch: 1767, Batch Gradient Norm after: 11.049651097613316
Epoch 1768/10000, Prediction Accuracy = 60.45399999999999%, Loss = 0.5631438493728638
Epoch: 1768, Batch Gradient Norm: 11.770669736018803
Epoch: 1768, Batch Gradient Norm after: 11.770669736018803
Epoch 1769/10000, Prediction Accuracy = 60.434000000000005%, Loss = 0.567723560333252
Epoch: 1769, Batch Gradient Norm: 12.771880419992424
Epoch: 1769, Batch Gradient Norm after: 12.771880419992424
Epoch 1770/10000, Prediction Accuracy = 60.484%, Loss = 0.5762646079063416
Epoch: 1770, Batch Gradient Norm: 10.80902180103968
Epoch: 1770, Batch Gradient Norm after: 10.80902180103968
Epoch 1771/10000, Prediction Accuracy = 60.464%, Loss = 0.5627640962600708
Epoch: 1771, Batch Gradient Norm: 12.035715236236245
Epoch: 1771, Batch Gradient Norm after: 12.035715236236245
Epoch 1772/10000, Prediction Accuracy = 60.418000000000006%, Loss = 0.5706743359565735
Epoch: 1772, Batch Gradient Norm: 11.852790368569943
Epoch: 1772, Batch Gradient Norm after: 11.852790368569943
Epoch 1773/10000, Prediction Accuracy = 60.42%, Loss = 0.5694685816764832
Epoch: 1773, Batch Gradient Norm: 12.09340842325269
Epoch: 1773, Batch Gradient Norm after: 12.09340842325269
Epoch 1774/10000, Prediction Accuracy = 60.46%, Loss = 0.5703085541725159
Epoch: 1774, Batch Gradient Norm: 10.185801575004545
Epoch: 1774, Batch Gradient Norm after: 10.185801575004545
Epoch 1775/10000, Prediction Accuracy = 60.5%, Loss = 0.5586418032646179
Epoch: 1775, Batch Gradient Norm: 8.98412604669866
Epoch: 1775, Batch Gradient Norm after: 8.98412604669866
Epoch 1776/10000, Prediction Accuracy = 60.476%, Loss = 0.5513751029968261
Epoch: 1776, Batch Gradient Norm: 9.064955470137564
Epoch: 1776, Batch Gradient Norm after: 9.064955470137564
Epoch 1777/10000, Prediction Accuracy = 60.484%, Loss = 0.5514861106872558
Epoch: 1777, Batch Gradient Norm: 11.927461174344334
Epoch: 1777, Batch Gradient Norm after: 11.927461174344334
Epoch 1778/10000, Prediction Accuracy = 60.472%, Loss = 0.5700820207595825
Epoch: 1778, Batch Gradient Norm: 11.793288429708717
Epoch: 1778, Batch Gradient Norm after: 11.793288429708717
Epoch 1779/10000, Prediction Accuracy = 60.517999999999994%, Loss = 0.5702199578285218
Epoch: 1779, Batch Gradient Norm: 11.074301624602915
Epoch: 1779, Batch Gradient Norm after: 11.074301624602915
Epoch 1780/10000, Prediction Accuracy = 60.495999999999995%, Loss = 0.5643979668617248
Epoch: 1780, Batch Gradient Norm: 9.855757047787534
Epoch: 1780, Batch Gradient Norm after: 9.855757047787534
Epoch 1781/10000, Prediction Accuracy = 60.52%, Loss = 0.5564812421798706
Epoch: 1781, Batch Gradient Norm: 9.557593966455576
Epoch: 1781, Batch Gradient Norm after: 9.557593966455576
Epoch 1782/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.5536282896995545
Epoch: 1782, Batch Gradient Norm: 13.74593191580376
Epoch: 1782, Batch Gradient Norm after: 13.74593191580376
Epoch 1783/10000, Prediction Accuracy = 60.484%, Loss = 0.580098819732666
Epoch: 1783, Batch Gradient Norm: 14.81527879436299
Epoch: 1783, Batch Gradient Norm after: 14.81527879436299
Epoch 1784/10000, Prediction Accuracy = 60.438%, Loss = 0.5890191435813904
Epoch: 1784, Batch Gradient Norm: 11.667004964663734
Epoch: 1784, Batch Gradient Norm after: 11.667004964663734
Epoch 1785/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.5654262423515319
Epoch: 1785, Batch Gradient Norm: 9.702288691190132
Epoch: 1785, Batch Gradient Norm after: 9.702288691190132
Epoch 1786/10000, Prediction Accuracy = 60.516%, Loss = 0.5535872459411622
Epoch: 1786, Batch Gradient Norm: 10.484547254143274
Epoch: 1786, Batch Gradient Norm after: 10.484547254143274
Epoch 1787/10000, Prediction Accuracy = 60.45%, Loss = 0.5588748455047607
Epoch: 1787, Batch Gradient Norm: 12.94717843467283
Epoch: 1787, Batch Gradient Norm after: 12.94717843467283
Epoch 1788/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.5778116226196289
Epoch: 1788, Batch Gradient Norm: 11.05669763924187
Epoch: 1788, Batch Gradient Norm after: 11.05669763924187
Epoch 1789/10000, Prediction Accuracy = 60.462%, Loss = 0.5641320705413818
Epoch: 1789, Batch Gradient Norm: 10.25020396769226
Epoch: 1789, Batch Gradient Norm after: 10.25020396769226
Epoch 1790/10000, Prediction Accuracy = 60.496%, Loss = 0.5579917550086975
Epoch: 1790, Batch Gradient Norm: 11.60528485070913
Epoch: 1790, Batch Gradient Norm after: 11.60528485070913
Epoch 1791/10000, Prediction Accuracy = 60.426%, Loss = 0.5657687902450561
Epoch: 1791, Batch Gradient Norm: 15.597235850007296
Epoch: 1791, Batch Gradient Norm after: 15.597235850007296
Epoch 1792/10000, Prediction Accuracy = 60.51800000000001%, Loss = 0.599254596233368
Epoch: 1792, Batch Gradient Norm: 12.30442845060205
Epoch: 1792, Batch Gradient Norm after: 12.30442845060205
Epoch 1793/10000, Prediction Accuracy = 60.45%, Loss = 0.573371160030365
Epoch: 1793, Batch Gradient Norm: 6.443615335926259
Epoch: 1793, Batch Gradient Norm after: 6.443615335926259
Epoch 1794/10000, Prediction Accuracy = 60.54600000000001%, Loss = 0.539664912223816
Epoch: 1794, Batch Gradient Norm: 6.943214577338203
Epoch: 1794, Batch Gradient Norm after: 6.943214577338203
Epoch 1795/10000, Prediction Accuracy = 60.472%, Loss = 0.5411418080329895
Epoch: 1795, Batch Gradient Norm: 9.151227597095588
Epoch: 1795, Batch Gradient Norm after: 9.151227597095588
Epoch 1796/10000, Prediction Accuracy = 60.474000000000004%, Loss = 0.5521134495735168
Epoch: 1796, Batch Gradient Norm: 10.846246796538018
Epoch: 1796, Batch Gradient Norm after: 10.846246796538018
Epoch 1797/10000, Prediction Accuracy = 60.448%, Loss = 0.5621725201606751
Epoch: 1797, Batch Gradient Norm: 14.171916812255123
Epoch: 1797, Batch Gradient Norm after: 14.171916812255123
Epoch 1798/10000, Prediction Accuracy = 60.482000000000006%, Loss = 0.5839089035987854
Epoch: 1798, Batch Gradient Norm: 13.951498473106104
Epoch: 1798, Batch Gradient Norm after: 13.951498473106104
Epoch 1799/10000, Prediction Accuracy = 60.412%, Loss = 0.5808319211006164
Epoch: 1799, Batch Gradient Norm: 10.909446550177055
Epoch: 1799, Batch Gradient Norm after: 10.909446550177055
Epoch 1800/10000, Prediction Accuracy = 60.49399999999999%, Loss = 0.5590612411499023
Epoch: 1800, Batch Gradient Norm: 9.34651018543626
Epoch: 1800, Batch Gradient Norm after: 9.34651018543626
Epoch 1801/10000, Prediction Accuracy = 60.446000000000005%, Loss = 0.5507301211357116
Epoch: 1801, Batch Gradient Norm: 9.19502725006102
Epoch: 1801, Batch Gradient Norm after: 9.19502725006102
Epoch 1802/10000, Prediction Accuracy = 60.488%, Loss = 0.5510229706764221
Epoch: 1802, Batch Gradient Norm: 9.322665596410666
Epoch: 1802, Batch Gradient Norm after: 9.322665596410666
Epoch 1803/10000, Prediction Accuracy = 60.470000000000006%, Loss = 0.5516615033149719
Epoch: 1803, Batch Gradient Norm: 12.156840403338212
Epoch: 1803, Batch Gradient Norm after: 12.156840403338212
Epoch 1804/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.5683953762054443
Epoch: 1804, Batch Gradient Norm: 16.73429043274987
Epoch: 1804, Batch Gradient Norm after: 16.73429043274987
Epoch 1805/10000, Prediction Accuracy = 60.45%, Loss = 0.6068328380584717
Epoch: 1805, Batch Gradient Norm: 11.043525495641008
Epoch: 1805, Batch Gradient Norm after: 11.043525495641008
Epoch 1806/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.5627063512802124
Epoch: 1806, Batch Gradient Norm: 8.577065433735152
Epoch: 1806, Batch Gradient Norm after: 8.577065433735152
Epoch 1807/10000, Prediction Accuracy = 60.507999999999996%, Loss = 0.5479240894317627
Epoch: 1807, Batch Gradient Norm: 8.750423291656181
Epoch: 1807, Batch Gradient Norm after: 8.750423291656181
Epoch 1808/10000, Prediction Accuracy = 60.51400000000001%, Loss = 0.5483636260032654
Epoch: 1808, Batch Gradient Norm: 9.640401731853846
Epoch: 1808, Batch Gradient Norm after: 9.640401731853846
Epoch 1809/10000, Prediction Accuracy = 60.477999999999994%, Loss = 0.5528169989585876
Epoch: 1809, Batch Gradient Norm: 14.197392811691627
Epoch: 1809, Batch Gradient Norm after: 14.197392811691627
Epoch 1810/10000, Prediction Accuracy = 60.538%, Loss = 0.584314501285553
Epoch: 1810, Batch Gradient Norm: 12.611401110089997
Epoch: 1810, Batch Gradient Norm after: 12.611401110089997
Epoch 1811/10000, Prediction Accuracy = 60.486000000000004%, Loss = 0.5753132820129394
Epoch: 1811, Batch Gradient Norm: 11.365912996784514
Epoch: 1811, Batch Gradient Norm after: 11.365912996784514
Epoch 1812/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5639363884925842
Epoch: 1812, Batch Gradient Norm: 11.481760210586051
Epoch: 1812, Batch Gradient Norm after: 11.481760210586051
Epoch 1813/10000, Prediction Accuracy = 60.467999999999996%, Loss = 0.5633877158164978
Epoch: 1813, Batch Gradient Norm: 11.39017382597058
Epoch: 1813, Batch Gradient Norm after: 11.39017382597058
Epoch 1814/10000, Prediction Accuracy = 60.564%, Loss = 0.5631640434265137
Epoch: 1814, Batch Gradient Norm: 10.883512747218308
Epoch: 1814, Batch Gradient Norm after: 10.883512747218308
Epoch 1815/10000, Prediction Accuracy = 60.562%, Loss = 0.5596117734909057
Epoch: 1815, Batch Gradient Norm: 12.958132958517767
Epoch: 1815, Batch Gradient Norm after: 12.958132958517767
Epoch 1816/10000, Prediction Accuracy = 60.568000000000005%, Loss = 0.5741022944450378
Epoch: 1816, Batch Gradient Norm: 12.608850092711167
Epoch: 1816, Batch Gradient Norm after: 12.608850092711167
Epoch 1817/10000, Prediction Accuracy = 60.592000000000006%, Loss = 0.5709733009338379
Epoch: 1817, Batch Gradient Norm: 11.897046921252569
Epoch: 1817, Batch Gradient Norm after: 11.897046921252569
Epoch 1818/10000, Prediction Accuracy = 60.510000000000005%, Loss = 0.5660355925559998
Epoch: 1818, Batch Gradient Norm: 11.218937204150734
Epoch: 1818, Batch Gradient Norm after: 11.218937204150734
Epoch 1819/10000, Prediction Accuracy = 60.6%, Loss = 0.5617253303527832
Epoch: 1819, Batch Gradient Norm: 10.635257257414061
Epoch: 1819, Batch Gradient Norm after: 10.635257257414061
Epoch 1820/10000, Prediction Accuracy = 60.538%, Loss = 0.5585950613021851
Epoch: 1820, Batch Gradient Norm: 9.483521552022765
Epoch: 1820, Batch Gradient Norm after: 9.483521552022765
Epoch 1821/10000, Prediction Accuracy = 60.528%, Loss = 0.5517663478851318
Epoch: 1821, Batch Gradient Norm: 11.99636385800065
Epoch: 1821, Batch Gradient Norm after: 11.99636385800065
Epoch 1822/10000, Prediction Accuracy = 60.48199999999999%, Loss = 0.5671281337738037
Epoch: 1822, Batch Gradient Norm: 11.434412230733487
Epoch: 1822, Batch Gradient Norm after: 11.434412230733487
Epoch 1823/10000, Prediction Accuracy = 60.55999999999999%, Loss = 0.5663094282150268
Epoch: 1823, Batch Gradient Norm: 8.274360962206519
Epoch: 1823, Batch Gradient Norm after: 8.274360962206519
Epoch 1824/10000, Prediction Accuracy = 60.524%, Loss = 0.5461429595947266
Epoch: 1824, Batch Gradient Norm: 7.697568842924985
Epoch: 1824, Batch Gradient Norm after: 7.697568842924985
Epoch 1825/10000, Prediction Accuracy = 60.552%, Loss = 0.5422107338905334
Epoch: 1825, Batch Gradient Norm: 9.947396246939082
Epoch: 1825, Batch Gradient Norm after: 9.947396246939082
Epoch 1826/10000, Prediction Accuracy = 60.516000000000005%, Loss = 0.5524864315986633
Epoch: 1826, Batch Gradient Norm: 12.61631662916011
Epoch: 1826, Batch Gradient Norm after: 12.61631662916011
Epoch 1827/10000, Prediction Accuracy = 60.508%, Loss = 0.5691033363342285
Epoch: 1827, Batch Gradient Norm: 13.825063409826035
Epoch: 1827, Batch Gradient Norm after: 13.825063409826035
Epoch 1828/10000, Prediction Accuracy = 60.552%, Loss = 0.5777422666549683
Epoch: 1828, Batch Gradient Norm: 13.381767841383295
Epoch: 1828, Batch Gradient Norm after: 13.381767841383295
Epoch 1829/10000, Prediction Accuracy = 60.528%, Loss = 0.5766696095466614
Epoch: 1829, Batch Gradient Norm: 10.817961140272203
Epoch: 1829, Batch Gradient Norm after: 10.817961140272203
Epoch 1830/10000, Prediction Accuracy = 60.498000000000005%, Loss = 0.5600488185882568
Epoch: 1830, Batch Gradient Norm: 9.780430974602329
Epoch: 1830, Batch Gradient Norm after: 9.780430974602329
Epoch 1831/10000, Prediction Accuracy = 60.572%, Loss = 0.5529297947883606
Epoch: 1831, Batch Gradient Norm: 11.974988807857065
Epoch: 1831, Batch Gradient Norm after: 11.974988807857065
Epoch 1832/10000, Prediction Accuracy = 60.517999999999994%, Loss = 0.5661773324012757
Epoch: 1832, Batch Gradient Norm: 11.484968023861391
Epoch: 1832, Batch Gradient Norm after: 11.484968023861391
Epoch 1833/10000, Prediction Accuracy = 60.524%, Loss = 0.5631468296051025
Epoch: 1833, Batch Gradient Norm: 11.317323086312893
Epoch: 1833, Batch Gradient Norm after: 11.317323086312893
Epoch 1834/10000, Prediction Accuracy = 60.522000000000006%, Loss = 0.5619759321212768
Epoch: 1834, Batch Gradient Norm: 11.195229324444288
Epoch: 1834, Batch Gradient Norm after: 11.195229324444288
Epoch 1835/10000, Prediction Accuracy = 60.592000000000006%, Loss = 0.5607610464096069
Epoch: 1835, Batch Gradient Norm: 12.456826310924612
Epoch: 1835, Batch Gradient Norm after: 12.456826310924612
Epoch 1836/10000, Prediction Accuracy = 60.50600000000001%, Loss = 0.5695609092712403
Epoch: 1836, Batch Gradient Norm: 13.01657342612603
Epoch: 1836, Batch Gradient Norm after: 13.01657342612603
Epoch 1837/10000, Prediction Accuracy = 60.476%, Loss = 0.5734457969665527
Epoch: 1837, Batch Gradient Norm: 11.492057743178904
Epoch: 1837, Batch Gradient Norm after: 11.492057743178904
Epoch 1838/10000, Prediction Accuracy = 60.532000000000004%, Loss = 0.5623015761375427
Epoch: 1838, Batch Gradient Norm: 11.040278069316228
Epoch: 1838, Batch Gradient Norm after: 11.040278069316228
Epoch 1839/10000, Prediction Accuracy = 60.484%, Loss = 0.5593653678894043
Epoch: 1839, Batch Gradient Norm: 9.694697907946491
Epoch: 1839, Batch Gradient Norm after: 9.694697907946491
Epoch 1840/10000, Prediction Accuracy = 60.57800000000001%, Loss = 0.552094328403473
Epoch: 1840, Batch Gradient Norm: 9.606822684153894
Epoch: 1840, Batch Gradient Norm after: 9.606822684153894
Epoch 1841/10000, Prediction Accuracy = 60.525999999999996%, Loss = 0.5505496263504028
Epoch: 1841, Batch Gradient Norm: 13.412109453295264
Epoch: 1841, Batch Gradient Norm after: 13.412109453295264
Epoch 1842/10000, Prediction Accuracy = 60.596000000000004%, Loss = 0.5759524226188659
Epoch: 1842, Batch Gradient Norm: 10.751809238984197
Epoch: 1842, Batch Gradient Norm after: 10.751809238984197
Epoch 1843/10000, Prediction Accuracy = 60.525999999999996%, Loss = 0.5595099449157714
Epoch: 1843, Batch Gradient Norm: 9.30603197630091
Epoch: 1843, Batch Gradient Norm after: 9.30603197630091
Epoch 1844/10000, Prediction Accuracy = 60.614%, Loss = 0.5499327898025512
Epoch: 1844, Batch Gradient Norm: 9.18658917238308
Epoch: 1844, Batch Gradient Norm after: 9.18658917238308
Epoch 1845/10000, Prediction Accuracy = 60.54599999999999%, Loss = 0.5483267426490783
Epoch: 1845, Batch Gradient Norm: 14.730799838341738
Epoch: 1845, Batch Gradient Norm after: 14.730799838341738
Epoch 1846/10000, Prediction Accuracy = 60.564%, Loss = 0.5862298607826233
Epoch: 1846, Batch Gradient Norm: 11.591570941106434
Epoch: 1846, Batch Gradient Norm after: 11.591570941106434
Epoch 1847/10000, Prediction Accuracy = 60.516%, Loss = 0.5629093408584595
Epoch: 1847, Batch Gradient Norm: 10.433724563391172
Epoch: 1847, Batch Gradient Norm after: 10.433724563391172
Epoch 1848/10000, Prediction Accuracy = 60.58200000000001%, Loss = 0.5542988300323486
Epoch: 1848, Batch Gradient Norm: 12.902898885724007
Epoch: 1848, Batch Gradient Norm after: 12.902898885724007
Epoch 1849/10000, Prediction Accuracy = 60.50600000000001%, Loss = 0.5712361454963684
Epoch: 1849, Batch Gradient Norm: 13.614309381681084
Epoch: 1849, Batch Gradient Norm after: 13.614309381681084
Epoch 1850/10000, Prediction Accuracy = 60.556%, Loss = 0.5785562038421631
Epoch: 1850, Batch Gradient Norm: 10.004939196815839
Epoch: 1850, Batch Gradient Norm after: 10.004939196815839
Epoch 1851/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.5537071704864502
Epoch: 1851, Batch Gradient Norm: 8.671694176142235
Epoch: 1851, Batch Gradient Norm after: 8.671694176142235
Epoch 1852/10000, Prediction Accuracy = 60.614%, Loss = 0.5456874251365662
Epoch: 1852, Batch Gradient Norm: 10.647156117830429
Epoch: 1852, Batch Gradient Norm after: 10.647156117830429
Epoch 1853/10000, Prediction Accuracy = 60.541999999999994%, Loss = 0.5557902455329895
Epoch: 1853, Batch Gradient Norm: 11.79124149692653
Epoch: 1853, Batch Gradient Norm after: 11.79124149692653
Epoch 1854/10000, Prediction Accuracy = 60.572%, Loss = 0.5647520065307617
Epoch: 1854, Batch Gradient Norm: 11.467700491823612
Epoch: 1854, Batch Gradient Norm after: 11.467700491823612
Epoch 1855/10000, Prediction Accuracy = 60.552%, Loss = 0.5624270558357238
Epoch: 1855, Batch Gradient Norm: 11.365178466517596
Epoch: 1855, Batch Gradient Norm after: 11.365178466517596
Epoch 1856/10000, Prediction Accuracy = 60.61800000000001%, Loss = 0.5607496857643127
Epoch: 1856, Batch Gradient Norm: 11.829509918855946
Epoch: 1856, Batch Gradient Norm after: 11.829509918855946
Epoch 1857/10000, Prediction Accuracy = 60.556%, Loss = 0.5631476998329162
Epoch: 1857, Batch Gradient Norm: 11.255331035649965
Epoch: 1857, Batch Gradient Norm after: 11.255331035649965
Epoch 1858/10000, Prediction Accuracy = 60.598%, Loss = 0.5592566728591919
Epoch: 1858, Batch Gradient Norm: 9.606024366833694
Epoch: 1858, Batch Gradient Norm after: 9.606024366833694
Epoch 1859/10000, Prediction Accuracy = 60.59599999999999%, Loss = 0.5492475748062133
Epoch: 1859, Batch Gradient Norm: 9.87637892195215
Epoch: 1859, Batch Gradient Norm after: 9.87637892195215
Epoch 1860/10000, Prediction Accuracy = 60.556000000000004%, Loss = 0.5510857105255127
Epoch: 1860, Batch Gradient Norm: 11.818717099455213
Epoch: 1860, Batch Gradient Norm after: 11.818717099455213
Epoch 1861/10000, Prediction Accuracy = 60.476%, Loss = 0.564132285118103
Epoch: 1861, Batch Gradient Norm: 12.917918599458545
Epoch: 1861, Batch Gradient Norm after: 12.917918599458545
Epoch 1862/10000, Prediction Accuracy = 60.540000000000006%, Loss = 0.5729905486106872
Epoch: 1862, Batch Gradient Norm: 12.98897008056309
Epoch: 1862, Batch Gradient Norm after: 12.98897008056309
Epoch 1863/10000, Prediction Accuracy = 60.58%, Loss = 0.5743474006652832
Epoch: 1863, Batch Gradient Norm: 9.164021300791234
Epoch: 1863, Batch Gradient Norm after: 9.164021300791234
Epoch 1864/10000, Prediction Accuracy = 60.563999999999986%, Loss = 0.5487149953842163
Epoch: 1864, Batch Gradient Norm: 9.010756511224018
Epoch: 1864, Batch Gradient Norm after: 9.010756511224018
Epoch 1865/10000, Prediction Accuracy = 60.50599999999999%, Loss = 0.5467123508453369
Epoch: 1865, Batch Gradient Norm: 11.276066160533135
Epoch: 1865, Batch Gradient Norm after: 11.276066160533135
Epoch 1866/10000, Prediction Accuracy = 60.586%, Loss = 0.5601719498634339
Epoch: 1866, Batch Gradient Norm: 11.990167343287924
Epoch: 1866, Batch Gradient Norm after: 11.990167343287924
Epoch 1867/10000, Prediction Accuracy = 60.58799999999999%, Loss = 0.5632266640663147
Epoch: 1867, Batch Gradient Norm: 13.8737630787916
Epoch: 1867, Batch Gradient Norm after: 13.8737630787916
Epoch 1868/10000, Prediction Accuracy = 60.53800000000001%, Loss = 0.5767515897750854
Epoch: 1868, Batch Gradient Norm: 13.498657461212716
Epoch: 1868, Batch Gradient Norm after: 13.498657461212716
Epoch 1869/10000, Prediction Accuracy = 60.636%, Loss = 0.5759879112243652
Epoch: 1869, Batch Gradient Norm: 10.362078273137419
Epoch: 1869, Batch Gradient Norm after: 10.362078273137419
Epoch 1870/10000, Prediction Accuracy = 60.56%, Loss = 0.5540742874145508
Epoch: 1870, Batch Gradient Norm: 9.30231370412157
Epoch: 1870, Batch Gradient Norm after: 9.30231370412157
Epoch 1871/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5472272872924805
Epoch: 1871, Batch Gradient Norm: 10.298730157901817
Epoch: 1871, Batch Gradient Norm after: 10.298730157901817
Epoch 1872/10000, Prediction Accuracy = 60.553999999999995%, Loss = 0.5529370546340943
Epoch: 1872, Batch Gradient Norm: 11.563813955784653
Epoch: 1872, Batch Gradient Norm after: 11.563813955784653
Epoch 1873/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.5613845229148865
Epoch: 1873, Batch Gradient Norm: 11.525030180151246
Epoch: 1873, Batch Gradient Norm after: 11.525030180151246
Epoch 1874/10000, Prediction Accuracy = 60.544000000000004%, Loss = 0.5608539938926697
Epoch: 1874, Batch Gradient Norm: 10.87573704428227
Epoch: 1874, Batch Gradient Norm after: 10.87573704428227
Epoch 1875/10000, Prediction Accuracy = 60.568000000000005%, Loss = 0.5565706133842468
Epoch: 1875, Batch Gradient Norm: 9.54732785286542
Epoch: 1875, Batch Gradient Norm after: 9.54732785286542
Epoch 1876/10000, Prediction Accuracy = 60.657999999999994%, Loss = 0.5481898427009583
Epoch: 1876, Batch Gradient Norm: 9.985533163704861
Epoch: 1876, Batch Gradient Norm after: 9.985533163704861
Epoch 1877/10000, Prediction Accuracy = 60.616%, Loss = 0.5494356393814087
Epoch: 1877, Batch Gradient Norm: 12.866468640134075
Epoch: 1877, Batch Gradient Norm after: 12.866468640134075
Epoch 1878/10000, Prediction Accuracy = 60.55%, Loss = 0.5690302610397339
Epoch: 1878, Batch Gradient Norm: 11.924494580644478
Epoch: 1878, Batch Gradient Norm after: 11.924494580644478
Epoch 1879/10000, Prediction Accuracy = 60.624%, Loss = 0.5634128212928772
Epoch: 1879, Batch Gradient Norm: 11.213659982027828
Epoch: 1879, Batch Gradient Norm after: 11.213659982027828
Epoch 1880/10000, Prediction Accuracy = 60.482000000000006%, Loss = 0.5596190094947815
Epoch: 1880, Batch Gradient Norm: 13.02601730740481
Epoch: 1880, Batch Gradient Norm after: 13.02601730740481
Epoch 1881/10000, Prediction Accuracy = 60.54%, Loss = 0.5744722604751586
Epoch: 1881, Batch Gradient Norm: 11.396987020237386
Epoch: 1881, Batch Gradient Norm after: 11.396987020237386
Epoch 1882/10000, Prediction Accuracy = 60.63199999999999%, Loss = 0.5615835309028625
Epoch: 1882, Batch Gradient Norm: 13.087333374874271
Epoch: 1882, Batch Gradient Norm after: 13.087333374874271
Epoch 1883/10000, Prediction Accuracy = 60.588%, Loss = 0.5710610985755921
Epoch: 1883, Batch Gradient Norm: 11.548366609790081
Epoch: 1883, Batch Gradient Norm after: 11.548366609790081
Epoch 1884/10000, Prediction Accuracy = 60.612%, Loss = 0.5598309278488159
Epoch: 1884, Batch Gradient Norm: 10.818429908825342
Epoch: 1884, Batch Gradient Norm after: 10.818429908825342
Epoch 1885/10000, Prediction Accuracy = 60.616%, Loss = 0.554596209526062
Epoch: 1885, Batch Gradient Norm: 11.034169330258326
Epoch: 1885, Batch Gradient Norm after: 11.034169330258326
Epoch 1886/10000, Prediction Accuracy = 60.562%, Loss = 0.5557871460914612
Epoch: 1886, Batch Gradient Norm: 11.21387583396829
Epoch: 1886, Batch Gradient Norm after: 11.21387583396829
Epoch 1887/10000, Prediction Accuracy = 60.626%, Loss = 0.5576875329017639
Epoch: 1887, Batch Gradient Norm: 10.80130480368118
Epoch: 1887, Batch Gradient Norm after: 10.80130480368118
Epoch 1888/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.5557755947113037
Epoch: 1888, Batch Gradient Norm: 9.395337659009824
Epoch: 1888, Batch Gradient Norm after: 9.395337659009824
Epoch 1889/10000, Prediction Accuracy = 60.634%, Loss = 0.5470350980758667
Epoch: 1889, Batch Gradient Norm: 11.556489589583393
Epoch: 1889, Batch Gradient Norm after: 11.556489589583393
Epoch 1890/10000, Prediction Accuracy = 60.6%, Loss = 0.5600838184356689
Epoch: 1890, Batch Gradient Norm: 11.281627653925568
Epoch: 1890, Batch Gradient Norm after: 11.281627653925568
Epoch 1891/10000, Prediction Accuracy = 60.628%, Loss = 0.5579877018928527
Epoch: 1891, Batch Gradient Norm: 11.740040743954044
Epoch: 1891, Batch Gradient Norm after: 11.740040743954044
Epoch 1892/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.5611336350440979
Epoch: 1892, Batch Gradient Norm: 10.200351848363956
Epoch: 1892, Batch Gradient Norm after: 10.200351848363956
Epoch 1893/10000, Prediction Accuracy = 60.632000000000005%, Loss = 0.5511468887329102
Epoch: 1893, Batch Gradient Norm: 9.87331497894366
Epoch: 1893, Batch Gradient Norm after: 9.87331497894366
Epoch 1894/10000, Prediction Accuracy = 60.61%, Loss = 0.5491387844085693
Epoch: 1894, Batch Gradient Norm: 10.766240588044731
Epoch: 1894, Batch Gradient Norm after: 10.766240588044731
Epoch 1895/10000, Prediction Accuracy = 60.63199999999999%, Loss = 0.5543023943901062
Epoch: 1895, Batch Gradient Norm: 10.882616748232035
Epoch: 1895, Batch Gradient Norm after: 10.882616748232035
Epoch 1896/10000, Prediction Accuracy = 60.584%, Loss = 0.5562212705612183
Epoch: 1896, Batch Gradient Norm: 11.617997145729404
Epoch: 1896, Batch Gradient Norm after: 11.617997145729404
Epoch 1897/10000, Prediction Accuracy = 60.61600000000001%, Loss = 0.562584114074707
Epoch: 1897, Batch Gradient Norm: 12.097193250882585
Epoch: 1897, Batch Gradient Norm after: 12.097193250882585
Epoch 1898/10000, Prediction Accuracy = 60.548%, Loss = 0.5631220221519471
Epoch: 1898, Batch Gradient Norm: 16.464698098237793
Epoch: 1898, Batch Gradient Norm after: 16.464698098237793
Epoch 1899/10000, Prediction Accuracy = 60.565999999999995%, Loss = 0.5981773138046265
Epoch: 1899, Batch Gradient Norm: 12.279346194443324
Epoch: 1899, Batch Gradient Norm after: 12.279346194443324
Epoch 1900/10000, Prediction Accuracy = 60.562%, Loss = 0.5649182081222535
Epoch: 1900, Batch Gradient Norm: 8.252268954528208
Epoch: 1900, Batch Gradient Norm after: 8.252268954528208
Epoch 1901/10000, Prediction Accuracy = 60.617999999999995%, Loss = 0.5405750274658203
Epoch: 1901, Batch Gradient Norm: 8.671255142053163
Epoch: 1901, Batch Gradient Norm after: 8.671255142053163
Epoch 1902/10000, Prediction Accuracy = 60.58399999999999%, Loss = 0.5417649388313294
Epoch: 1902, Batch Gradient Norm: 11.135967566425965
Epoch: 1902, Batch Gradient Norm after: 11.135967566425965
Epoch 1903/10000, Prediction Accuracy = 60.624%, Loss = 0.5565775156021118
Epoch: 1903, Batch Gradient Norm: 10.706432315147909
Epoch: 1903, Batch Gradient Norm after: 10.706432315147909
Epoch 1904/10000, Prediction Accuracy = 60.612%, Loss = 0.5540767312049866
Epoch: 1904, Batch Gradient Norm: 10.078179297660718
Epoch: 1904, Batch Gradient Norm after: 10.078179297660718
Epoch 1905/10000, Prediction Accuracy = 60.61800000000001%, Loss = 0.5503359794616699
Epoch: 1905, Batch Gradient Norm: 10.629033143322538
Epoch: 1905, Batch Gradient Norm after: 10.629033143322538
Epoch 1906/10000, Prediction Accuracy = 60.614%, Loss = 0.5536769509315491
Epoch: 1906, Batch Gradient Norm: 12.114302011798042
Epoch: 1906, Batch Gradient Norm after: 12.114302011798042
Epoch 1907/10000, Prediction Accuracy = 60.57000000000001%, Loss = 0.5632232904434205
Epoch: 1907, Batch Gradient Norm: 10.640707808823027
Epoch: 1907, Batch Gradient Norm after: 10.640707808823027
Epoch 1908/10000, Prediction Accuracy = 60.61999999999999%, Loss = 0.5536230206489563
Epoch: 1908, Batch Gradient Norm: 10.234389012428558
Epoch: 1908, Batch Gradient Norm after: 10.234389012428558
Epoch 1909/10000, Prediction Accuracy = 60.592%, Loss = 0.551209568977356
Epoch: 1909, Batch Gradient Norm: 9.075527023930718
Epoch: 1909, Batch Gradient Norm after: 9.075527023930718
Epoch 1910/10000, Prediction Accuracy = 60.636%, Loss = 0.5446620583534241
Epoch: 1910, Batch Gradient Norm: 11.861195066749534
Epoch: 1910, Batch Gradient Norm after: 11.861195066749534
Epoch 1911/10000, Prediction Accuracy = 60.54600000000001%, Loss = 0.5623050212860108
Epoch: 1911, Batch Gradient Norm: 15.366368136788086
Epoch: 1911, Batch Gradient Norm after: 15.366368136788086
Epoch 1912/10000, Prediction Accuracy = 60.59400000000001%, Loss = 0.5894225835800171
Epoch: 1912, Batch Gradient Norm: 13.150410915513605
Epoch: 1912, Batch Gradient Norm after: 13.150410915513605
Epoch 1913/10000, Prediction Accuracy = 60.532%, Loss = 0.5700561761856079
Epoch: 1913, Batch Gradient Norm: 12.101667669240554
Epoch: 1913, Batch Gradient Norm after: 12.101667669240554
Epoch 1914/10000, Prediction Accuracy = 60.61%, Loss = 0.5608352541923523
Epoch: 1914, Batch Gradient Norm: 12.123354404269252
Epoch: 1914, Batch Gradient Norm after: 12.123354404269252
Epoch 1915/10000, Prediction Accuracy = 60.589999999999996%, Loss = 0.5619457125663757
Epoch: 1915, Batch Gradient Norm: 10.816265187807529
Epoch: 1915, Batch Gradient Norm after: 10.816265187807529
Epoch 1916/10000, Prediction Accuracy = 60.638%, Loss = 0.5546088576316833
Epoch: 1916, Batch Gradient Norm: 10.933074667266128
Epoch: 1916, Batch Gradient Norm after: 10.933074667266128
Epoch 1917/10000, Prediction Accuracy = 60.628%, Loss = 0.5564618110656738
Epoch: 1917, Batch Gradient Norm: 8.479899226977428
Epoch: 1917, Batch Gradient Norm after: 8.479899226977428
Epoch 1918/10000, Prediction Accuracy = 60.605999999999995%, Loss = 0.5413105249404907
Epoch: 1918, Batch Gradient Norm: 10.305751645279232
Epoch: 1918, Batch Gradient Norm after: 10.305751645279232
Epoch 1919/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.5506410360336303
Epoch: 1919, Batch Gradient Norm: 11.702476712586575
Epoch: 1919, Batch Gradient Norm after: 11.702476712586575
Epoch 1920/10000, Prediction Accuracy = 60.68399999999999%, Loss = 0.5589002847671509
Epoch: 1920, Batch Gradient Norm: 12.068409147129527
Epoch: 1920, Batch Gradient Norm after: 12.068409147129527
Epoch 1921/10000, Prediction Accuracy = 60.64399999999999%, Loss = 0.5613975286483764
Epoch: 1921, Batch Gradient Norm: 11.014231038546047
Epoch: 1921, Batch Gradient Norm after: 11.014231038546047
Epoch 1922/10000, Prediction Accuracy = 60.686%, Loss = 0.5537479281425476
Epoch: 1922, Batch Gradient Norm: 11.071847084725755
Epoch: 1922, Batch Gradient Norm after: 11.071847084725755
Epoch 1923/10000, Prediction Accuracy = 60.61%, Loss = 0.5541171669960022
Epoch: 1923, Batch Gradient Norm: 12.520805824913422
Epoch: 1923, Batch Gradient Norm after: 12.520805824913422
Epoch 1924/10000, Prediction Accuracy = 60.646%, Loss = 0.5641916394233704
Epoch: 1924, Batch Gradient Norm: 12.025036449151855
Epoch: 1924, Batch Gradient Norm after: 12.025036449151855
Epoch 1925/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.5612474679946899
Epoch: 1925, Batch Gradient Norm: 10.660250066314715
Epoch: 1925, Batch Gradient Norm after: 10.660250066314715
Epoch 1926/10000, Prediction Accuracy = 60.678%, Loss = 0.5524802327156066
Epoch: 1926, Batch Gradient Norm: 10.10524751332529
Epoch: 1926, Batch Gradient Norm after: 10.10524751332529
Epoch 1927/10000, Prediction Accuracy = 60.6%, Loss = 0.5488223791122436
Epoch: 1927, Batch Gradient Norm: 11.159078220265048
Epoch: 1927, Batch Gradient Norm after: 11.159078220265048
Epoch 1928/10000, Prediction Accuracy = 60.69%, Loss = 0.5553799986839294
Epoch: 1928, Batch Gradient Norm: 11.650446763910463
Epoch: 1928, Batch Gradient Norm after: 11.650446763910463
Epoch 1929/10000, Prediction Accuracy = 60.616%, Loss = 0.55941401720047
Epoch: 1929, Batch Gradient Norm: 11.35787610917281
Epoch: 1929, Batch Gradient Norm after: 11.35787610917281
Epoch 1930/10000, Prediction Accuracy = 60.674%, Loss = 0.5563530802726746
Epoch: 1930, Batch Gradient Norm: 10.849909780542191
Epoch: 1930, Batch Gradient Norm after: 10.849909780542191
Epoch 1931/10000, Prediction Accuracy = 60.666%, Loss = 0.5521940469741822
Epoch: 1931, Batch Gradient Norm: 11.260897822390124
Epoch: 1931, Batch Gradient Norm after: 11.260897822390124
Epoch 1932/10000, Prediction Accuracy = 60.61%, Loss = 0.5534645080566406
Epoch: 1932, Batch Gradient Norm: 10.023086087782007
Epoch: 1932, Batch Gradient Norm after: 10.023086087782007
Epoch 1933/10000, Prediction Accuracy = 60.616%, Loss = 0.5460000157356262
Epoch: 1933, Batch Gradient Norm: 11.732437588298543
Epoch: 1933, Batch Gradient Norm after: 11.732437588298543
Epoch 1934/10000, Prediction Accuracy = 60.648%, Loss = 0.5574335336685181
Epoch: 1934, Batch Gradient Norm: 11.486801647686743
Epoch: 1934, Batch Gradient Norm after: 11.486801647686743
Epoch 1935/10000, Prediction Accuracy = 60.646%, Loss = 0.5588404178619385
Epoch: 1935, Batch Gradient Norm: 11.55090391128015
Epoch: 1935, Batch Gradient Norm after: 11.55090391128015
Epoch 1936/10000, Prediction Accuracy = 60.645999999999994%, Loss = 0.5605431318283081
Epoch: 1936, Batch Gradient Norm: 13.744429772627006
Epoch: 1936, Batch Gradient Norm after: 13.744429772627006
Epoch 1937/10000, Prediction Accuracy = 60.6%, Loss = 0.5754859685897827
Epoch: 1937, Batch Gradient Norm: 11.268331772423915
Epoch: 1937, Batch Gradient Norm after: 11.268331772423915
Epoch 1938/10000, Prediction Accuracy = 60.628%, Loss = 0.5566494464874268
Epoch: 1938, Batch Gradient Norm: 10.960293958082039
Epoch: 1938, Batch Gradient Norm after: 10.960293958082039
Epoch 1939/10000, Prediction Accuracy = 60.696000000000005%, Loss = 0.5540761828422547
Epoch: 1939, Batch Gradient Norm: 9.120816270919164
Epoch: 1939, Batch Gradient Norm after: 9.120816270919164
Epoch 1940/10000, Prediction Accuracy = 60.684000000000005%, Loss = 0.5430614590644837
Epoch: 1940, Batch Gradient Norm: 10.472802605014126
Epoch: 1940, Batch Gradient Norm after: 10.472802605014126
Epoch 1941/10000, Prediction Accuracy = 60.632000000000005%, Loss = 0.5506746768951416
Epoch: 1941, Batch Gradient Norm: 11.373043429605179
Epoch: 1941, Batch Gradient Norm after: 11.373043429605179
Epoch 1942/10000, Prediction Accuracy = 60.628%, Loss = 0.5552714347839356
Epoch: 1942, Batch Gradient Norm: 12.443808918963917
Epoch: 1942, Batch Gradient Norm after: 12.443808918963917
Epoch 1943/10000, Prediction Accuracy = 60.577999999999996%, Loss = 0.5625370860099792
Epoch: 1943, Batch Gradient Norm: 11.42542390190013
Epoch: 1943, Batch Gradient Norm after: 11.42542390190013
Epoch 1944/10000, Prediction Accuracy = 60.64200000000001%, Loss = 0.5559630513191223
Epoch: 1944, Batch Gradient Norm: 10.470469739737842
Epoch: 1944, Batch Gradient Norm after: 10.470469739737842
Epoch 1945/10000, Prediction Accuracy = 60.605999999999995%, Loss = 0.5498792290687561
Epoch: 1945, Batch Gradient Norm: 11.15126245912019
Epoch: 1945, Batch Gradient Norm after: 11.15126245912019
Epoch 1946/10000, Prediction Accuracy = 60.73199999999999%, Loss = 0.5540818452835083
Epoch: 1946, Batch Gradient Norm: 11.976252787610607
Epoch: 1946, Batch Gradient Norm after: 11.976252787610607
Epoch 1947/10000, Prediction Accuracy = 60.674%, Loss = 0.5602846622467041
Epoch: 1947, Batch Gradient Norm: 10.410365630507336
Epoch: 1947, Batch Gradient Norm after: 10.410365630507336
Epoch 1948/10000, Prediction Accuracy = 60.694%, Loss = 0.5509056687355042
Epoch: 1948, Batch Gradient Norm: 10.017771012428373
Epoch: 1948, Batch Gradient Norm after: 10.017771012428373
Epoch 1949/10000, Prediction Accuracy = 60.616%, Loss = 0.5476549506187439
Epoch: 1949, Batch Gradient Norm: 11.95441767318325
Epoch: 1949, Batch Gradient Norm after: 11.95441767318325
Epoch 1950/10000, Prediction Accuracy = 60.612%, Loss = 0.5593854188919067
Epoch: 1950, Batch Gradient Norm: 13.084936811952376
Epoch: 1950, Batch Gradient Norm after: 13.084936811952376
Epoch 1951/10000, Prediction Accuracy = 60.622%, Loss = 0.5677576899528504
Epoch: 1951, Batch Gradient Norm: 11.896779998022204
Epoch: 1951, Batch Gradient Norm after: 11.896779998022204
Epoch 1952/10000, Prediction Accuracy = 60.629999999999995%, Loss = 0.5583583354949951
Epoch: 1952, Batch Gradient Norm: 11.336812397157733
Epoch: 1952, Batch Gradient Norm after: 11.336812397157733
Epoch 1953/10000, Prediction Accuracy = 60.65%, Loss = 0.5541814804077149
Epoch: 1953, Batch Gradient Norm: 11.193568538762706
Epoch: 1953, Batch Gradient Norm after: 11.193568538762706
Epoch 1954/10000, Prediction Accuracy = 60.682%, Loss = 0.5539564251899719
Epoch: 1954, Batch Gradient Norm: 8.93184538402894
Epoch: 1954, Batch Gradient Norm after: 8.93184538402894
Epoch 1955/10000, Prediction Accuracy = 60.662%, Loss = 0.541129755973816
Epoch: 1955, Batch Gradient Norm: 8.754110185904404
Epoch: 1955, Batch Gradient Norm after: 8.754110185904404
Epoch 1956/10000, Prediction Accuracy = 60.714%, Loss = 0.5420786380767822
Epoch: 1956, Batch Gradient Norm: 7.875624503766946
Epoch: 1956, Batch Gradient Norm after: 7.875624503766946
Epoch 1957/10000, Prediction Accuracy = 60.682%, Loss = 0.5369105815887452
Epoch: 1957, Batch Gradient Norm: 10.447021752316195
Epoch: 1957, Batch Gradient Norm after: 10.447021752316195
Epoch 1958/10000, Prediction Accuracy = 60.622%, Loss = 0.5495205044746398
Epoch: 1958, Batch Gradient Norm: 15.457140766477918
Epoch: 1958, Batch Gradient Norm after: 15.457140766477918
Epoch 1959/10000, Prediction Accuracy = 60.628%, Loss = 0.5869166851043701
Epoch: 1959, Batch Gradient Norm: 14.678280986385937
Epoch: 1959, Batch Gradient Norm after: 14.678280986385937
Epoch 1960/10000, Prediction Accuracy = 60.624%, Loss = 0.577176570892334
Epoch: 1960, Batch Gradient Norm: 13.421440883754016
Epoch: 1960, Batch Gradient Norm after: 13.421440883754016
Epoch 1961/10000, Prediction Accuracy = 60.66600000000001%, Loss = 0.5683039784431457
Epoch: 1961, Batch Gradient Norm: 12.053639405812435
Epoch: 1961, Batch Gradient Norm after: 12.053639405812435
Epoch 1962/10000, Prediction Accuracy = 60.572%, Loss = 0.5606233477592468
Epoch: 1962, Batch Gradient Norm: 9.055474503165355
Epoch: 1962, Batch Gradient Norm after: 9.055474503165355
Epoch 1963/10000, Prediction Accuracy = 60.69%, Loss = 0.5419382810592651
Epoch: 1963, Batch Gradient Norm: 9.247161809443744
Epoch: 1963, Batch Gradient Norm after: 9.247161809443744
Epoch 1964/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.5424978375434876
Epoch: 1964, Batch Gradient Norm: 9.369886733223222
Epoch: 1964, Batch Gradient Norm after: 9.369886733223222
Epoch 1965/10000, Prediction Accuracy = 60.66799999999999%, Loss = 0.5434263348579407
Epoch: 1965, Batch Gradient Norm: 12.472672216810992
Epoch: 1965, Batch Gradient Norm after: 12.472672216810992
Epoch 1966/10000, Prediction Accuracy = 60.71600000000001%, Loss = 0.5632829070091248
Epoch: 1966, Batch Gradient Norm: 10.65559461593667
Epoch: 1966, Batch Gradient Norm after: 10.65559461593667
Epoch 1967/10000, Prediction Accuracy = 60.63199999999999%, Loss = 0.5509922504425049
Epoch: 1967, Batch Gradient Norm: 10.452975447609898
Epoch: 1967, Batch Gradient Norm after: 10.452975447609898
Epoch 1968/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.5495878696441651
Epoch: 1968, Batch Gradient Norm: 9.525386514542886
Epoch: 1968, Batch Gradient Norm after: 9.525386514542886
Epoch 1969/10000, Prediction Accuracy = 60.705999999999996%, Loss = 0.543534004688263
Epoch: 1969, Batch Gradient Norm: 10.491921194079655
Epoch: 1969, Batch Gradient Norm after: 10.491921194079655
Epoch 1970/10000, Prediction Accuracy = 60.65599999999999%, Loss = 0.548670768737793
Epoch: 1970, Batch Gradient Norm: 12.068281871632868
Epoch: 1970, Batch Gradient Norm after: 12.068281871632868
Epoch 1971/10000, Prediction Accuracy = 60.760000000000005%, Loss = 0.5582827687263489
Epoch: 1971, Batch Gradient Norm: 11.514266458422002
Epoch: 1971, Batch Gradient Norm after: 11.514266458422002
Epoch 1972/10000, Prediction Accuracy = 60.682%, Loss = 0.5543810963630676
Epoch: 1972, Batch Gradient Norm: 12.780441560282892
Epoch: 1972, Batch Gradient Norm after: 12.780441560282892
Epoch 1973/10000, Prediction Accuracy = 60.720000000000006%, Loss = 0.5639202237129212
Epoch: 1973, Batch Gradient Norm: 14.041797385031005
Epoch: 1973, Batch Gradient Norm after: 14.041797385031005
Epoch 1974/10000, Prediction Accuracy = 60.641999999999996%, Loss = 0.5745000958442688
Epoch: 1974, Batch Gradient Norm: 10.85575121216199
Epoch: 1974, Batch Gradient Norm after: 10.85575121216199
Epoch 1975/10000, Prediction Accuracy = 60.708000000000006%, Loss = 0.5518790006637573
Epoch: 1975, Batch Gradient Norm: 9.628673340337086
Epoch: 1975, Batch Gradient Norm after: 9.628673340337086
Epoch 1976/10000, Prediction Accuracy = 60.7%, Loss = 0.5430641412734986
Epoch: 1976, Batch Gradient Norm: 10.495726993631152
Epoch: 1976, Batch Gradient Norm after: 10.495726993631152
Epoch 1977/10000, Prediction Accuracy = 60.672000000000004%, Loss = 0.5476746082305908
Epoch: 1977, Batch Gradient Norm: 12.014398954980349
Epoch: 1977, Batch Gradient Norm after: 12.014398954980349
Epoch 1978/10000, Prediction Accuracy = 60.727999999999994%, Loss = 0.5570586800575257
Epoch: 1978, Batch Gradient Norm: 12.240584769140492
Epoch: 1978, Batch Gradient Norm after: 12.240584769140492
Epoch 1979/10000, Prediction Accuracy = 60.70799999999999%, Loss = 0.5591765403747558
Epoch: 1979, Batch Gradient Norm: 11.139619371652305
Epoch: 1979, Batch Gradient Norm after: 11.139619371652305
Epoch 1980/10000, Prediction Accuracy = 60.730000000000004%, Loss = 0.5518351674079895
Epoch: 1980, Batch Gradient Norm: 11.766684287343049
Epoch: 1980, Batch Gradient Norm after: 11.766684287343049
Epoch 1981/10000, Prediction Accuracy = 60.712%, Loss = 0.5578862190246582
Epoch: 1981, Batch Gradient Norm: 9.823617157352578
Epoch: 1981, Batch Gradient Norm after: 9.823617157352578
Epoch 1982/10000, Prediction Accuracy = 60.669999999999995%, Loss = 0.5454242706298829
Epoch: 1982, Batch Gradient Norm: 9.637753878437682
Epoch: 1982, Batch Gradient Norm after: 9.637753878437682
Epoch 1983/10000, Prediction Accuracy = 60.658%, Loss = 0.5428747057914733
Epoch: 1983, Batch Gradient Norm: 11.586797201435852
Epoch: 1983, Batch Gradient Norm after: 11.586797201435852
Epoch 1984/10000, Prediction Accuracy = 60.70399999999999%, Loss = 0.554015052318573
Epoch: 1984, Batch Gradient Norm: 11.8075201298625
Epoch: 1984, Batch Gradient Norm after: 11.8075201298625
Epoch 1985/10000, Prediction Accuracy = 60.67999999999999%, Loss = 0.5551809787750244
Epoch: 1985, Batch Gradient Norm: 12.743798203787172
Epoch: 1985, Batch Gradient Norm after: 12.743798203787172
Epoch 1986/10000, Prediction Accuracy = 60.646%, Loss = 0.5631009340286255
Epoch: 1986, Batch Gradient Norm: 12.40823577350879
Epoch: 1986, Batch Gradient Norm after: 12.40823577350879
Epoch 1987/10000, Prediction Accuracy = 60.602%, Loss = 0.5624358177185058
Epoch: 1987, Batch Gradient Norm: 10.508487890058488
Epoch: 1987, Batch Gradient Norm after: 10.508487890058488
Epoch 1988/10000, Prediction Accuracy = 60.69200000000001%, Loss = 0.5494513392448426
Epoch: 1988, Batch Gradient Norm: 9.631935687821102
Epoch: 1988, Batch Gradient Norm after: 9.631935687821102
Epoch 1989/10000, Prediction Accuracy = 60.727999999999994%, Loss = 0.5437070250511169
Epoch: 1989, Batch Gradient Norm: 8.219363581492777
Epoch: 1989, Batch Gradient Norm after: 8.219363581492777
Epoch 1990/10000, Prediction Accuracy = 60.686%, Loss = 0.5359649658203125
Epoch: 1990, Batch Gradient Norm: 10.348627730752339
Epoch: 1990, Batch Gradient Norm after: 10.348627730752339
Epoch 1991/10000, Prediction Accuracy = 60.686%, Loss = 0.5481237530708313
Epoch: 1991, Batch Gradient Norm: 11.274807710467934
Epoch: 1991, Batch Gradient Norm after: 11.274807710467934
Epoch 1992/10000, Prediction Accuracy = 60.698%, Loss = 0.5535732388496399
Epoch: 1992, Batch Gradient Norm: 13.259788748658046
Epoch: 1992, Batch Gradient Norm after: 13.259788748658046
Epoch 1993/10000, Prediction Accuracy = 60.757999999999996%, Loss = 0.5672924757003784
Epoch: 1993, Batch Gradient Norm: 9.678775341670963
Epoch: 1993, Batch Gradient Norm after: 9.678775341670963
Epoch 1994/10000, Prediction Accuracy = 60.664%, Loss = 0.5427184104919434
Epoch: 1994, Batch Gradient Norm: 11.440533572388151
Epoch: 1994, Batch Gradient Norm after: 11.440533572388151
Epoch 1995/10000, Prediction Accuracy = 60.702%, Loss = 0.5529634952545166
Epoch: 1995, Batch Gradient Norm: 13.06785356041488
Epoch: 1995, Batch Gradient Norm after: 13.06785356041488
Epoch 1996/10000, Prediction Accuracy = 60.714%, Loss = 0.5634469628334046
Epoch: 1996, Batch Gradient Norm: 12.109761098604341
Epoch: 1996, Batch Gradient Norm after: 12.109761098604341
Epoch 1997/10000, Prediction Accuracy = 60.664%, Loss = 0.5567477107048034
Epoch: 1997, Batch Gradient Norm: 11.456532060945216
Epoch: 1997, Batch Gradient Norm after: 11.456532060945216
Epoch 1998/10000, Prediction Accuracy = 60.80400000000001%, Loss = 0.552201223373413
Epoch: 1998, Batch Gradient Norm: 11.147430754417853
Epoch: 1998, Batch Gradient Norm after: 11.147430754417853
Epoch 1999/10000, Prediction Accuracy = 60.60999999999999%, Loss = 0.5508081316947937
Epoch: 1999, Batch Gradient Norm: 13.08087270031976
Epoch: 1999, Batch Gradient Norm after: 13.08087270031976
Epoch 2000/10000, Prediction Accuracy = 60.73%, Loss = 0.5659128069877625
Epoch: 2000, Batch Gradient Norm: 10.4484358232054
Epoch: 2000, Batch Gradient Norm after: 10.4484358232054
Epoch 2001/10000, Prediction Accuracy = 60.69%, Loss = 0.5487960934638977
Epoch: 2001, Batch Gradient Norm: 7.951518012448801
Epoch: 2001, Batch Gradient Norm after: 7.951518012448801
Epoch 2002/10000, Prediction Accuracy = 60.722%, Loss = 0.5340207695960999
Epoch: 2002, Batch Gradient Norm: 10.37317402870504
Epoch: 2002, Batch Gradient Norm after: 10.37317402870504
Epoch 2003/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.5458544015884399
Epoch: 2003, Batch Gradient Norm: 13.426122805038403
Epoch: 2003, Batch Gradient Norm after: 13.426122805038403
Epoch 2004/10000, Prediction Accuracy = 60.693999999999996%, Loss = 0.5673403024673462
Epoch: 2004, Batch Gradient Norm: 11.090027312208258
Epoch: 2004, Batch Gradient Norm after: 11.090027312208258
Epoch 2005/10000, Prediction Accuracy = 60.778%, Loss = 0.5509936213493347
Epoch: 2005, Batch Gradient Norm: 8.510273385486869
Epoch: 2005, Batch Gradient Norm after: 8.510273385486869
Epoch 2006/10000, Prediction Accuracy = 60.739999999999995%, Loss = 0.5358003854751587
Epoch: 2006, Batch Gradient Norm: 10.308107607717162
Epoch: 2006, Batch Gradient Norm after: 10.308107607717162
Epoch 2007/10000, Prediction Accuracy = 60.77%, Loss = 0.5450711011886596
Epoch: 2007, Batch Gradient Norm: 13.460970582138264
Epoch: 2007, Batch Gradient Norm after: 13.460970582138264
Epoch 2008/10000, Prediction Accuracy = 60.638%, Loss = 0.5678357243537903
Epoch: 2008, Batch Gradient Norm: 12.655183802796358
Epoch: 2008, Batch Gradient Norm after: 12.655183802796358
Epoch 2009/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.5628186941146851
Epoch: 2009, Batch Gradient Norm: 10.87204432684742
Epoch: 2009, Batch Gradient Norm after: 10.87204432684742
Epoch 2010/10000, Prediction Accuracy = 60.733999999999995%, Loss = 0.5501106381416321
Epoch: 2010, Batch Gradient Norm: 10.930381625169854
Epoch: 2010, Batch Gradient Norm after: 10.930381625169854
Epoch 2011/10000, Prediction Accuracy = 60.774%, Loss = 0.5502394914627076
Epoch: 2011, Batch Gradient Norm: 9.541123796363523
Epoch: 2011, Batch Gradient Norm after: 9.541123796363523
Epoch 2012/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5423157453536988
Epoch: 2012, Batch Gradient Norm: 9.311841745689518
Epoch: 2012, Batch Gradient Norm after: 9.311841745689518
Epoch 2013/10000, Prediction Accuracy = 60.74400000000001%, Loss = 0.5399398803710938
Epoch: 2013, Batch Gradient Norm: 9.939902756873646
Epoch: 2013, Batch Gradient Norm after: 9.939902756873646
Epoch 2014/10000, Prediction Accuracy = 60.69200000000001%, Loss = 0.5434696674346924
Epoch: 2014, Batch Gradient Norm: 12.327778077668327
Epoch: 2014, Batch Gradient Norm after: 12.327778077668327
Epoch 2015/10000, Prediction Accuracy = 60.73%, Loss = 0.5590109586715698
Epoch: 2015, Batch Gradient Norm: 12.635449956481384
Epoch: 2015, Batch Gradient Norm after: 12.635449956481384
Epoch 2016/10000, Prediction Accuracy = 60.705999999999996%, Loss = 0.5610250115394593
Epoch: 2016, Batch Gradient Norm: 11.884134989911514
Epoch: 2016, Batch Gradient Norm after: 11.884134989911514
Epoch 2017/10000, Prediction Accuracy = 60.754%, Loss = 0.5557538747787476
Epoch: 2017, Batch Gradient Norm: 10.384436492085573
Epoch: 2017, Batch Gradient Norm after: 10.384436492085573
Epoch 2018/10000, Prediction Accuracy = 60.70399999999999%, Loss = 0.5462420344352722
Epoch: 2018, Batch Gradient Norm: 10.292736815663423
Epoch: 2018, Batch Gradient Norm after: 10.292736815663423
Epoch 2019/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5459587216377259
Epoch: 2019, Batch Gradient Norm: 11.18445193707723
Epoch: 2019, Batch Gradient Norm after: 11.18445193707723
Epoch 2020/10000, Prediction Accuracy = 60.714%, Loss = 0.5514199733734131
Epoch: 2020, Batch Gradient Norm: 13.208415088354357
Epoch: 2020, Batch Gradient Norm after: 13.208415088354357
Epoch 2021/10000, Prediction Accuracy = 60.724000000000004%, Loss = 0.5647711396217346
Epoch: 2021, Batch Gradient Norm: 13.936803812017459
Epoch: 2021, Batch Gradient Norm after: 13.936803812017459
Epoch 2022/10000, Prediction Accuracy = 60.738%, Loss = 0.5705119490623474
Epoch: 2022, Batch Gradient Norm: 10.429625772728025
Epoch: 2022, Batch Gradient Norm after: 10.429625772728025
Epoch 2023/10000, Prediction Accuracy = 60.742%, Loss = 0.5452046751976013
Epoch: 2023, Batch Gradient Norm: 9.282753539806153
Epoch: 2023, Batch Gradient Norm after: 9.282753539806153
Epoch 2024/10000, Prediction Accuracy = 60.682%, Loss = 0.5384505987167358
Epoch: 2024, Batch Gradient Norm: 9.309662903137276
Epoch: 2024, Batch Gradient Norm after: 9.309662903137276
Epoch 2025/10000, Prediction Accuracy = 60.75%, Loss = 0.5393732905387878
Epoch: 2025, Batch Gradient Norm: 9.996115233551862
Epoch: 2025, Batch Gradient Norm after: 9.996115233551862
Epoch 2026/10000, Prediction Accuracy = 60.688%, Loss = 0.5441536068916321
Epoch: 2026, Batch Gradient Norm: 11.083636796756846
Epoch: 2026, Batch Gradient Norm after: 11.083636796756846
Epoch 2027/10000, Prediction Accuracy = 60.688%, Loss = 0.5510154008865357
Epoch: 2027, Batch Gradient Norm: 12.953773667264949
Epoch: 2027, Batch Gradient Norm after: 12.953773667264949
Epoch 2028/10000, Prediction Accuracy = 60.73%, Loss = 0.5648728728294372
Epoch: 2028, Batch Gradient Norm: 10.013465827614432
Epoch: 2028, Batch Gradient Norm after: 10.013465827614432
Epoch 2029/10000, Prediction Accuracy = 60.64%, Loss = 0.5442740440368652
Epoch: 2029, Batch Gradient Norm: 9.606060796365767
Epoch: 2029, Batch Gradient Norm after: 9.606060796365767
Epoch 2030/10000, Prediction Accuracy = 60.788%, Loss = 0.541601037979126
Epoch: 2030, Batch Gradient Norm: 7.538798497436195
Epoch: 2030, Batch Gradient Norm after: 7.538798497436195
Epoch 2031/10000, Prediction Accuracy = 60.748000000000005%, Loss = 0.5302721619606018
Epoch: 2031, Batch Gradient Norm: 9.86213799343762
Epoch: 2031, Batch Gradient Norm after: 9.86213799343762
Epoch 2032/10000, Prediction Accuracy = 60.78800000000001%, Loss = 0.5429930329322815
Epoch: 2032, Batch Gradient Norm: 9.217318946232982
Epoch: 2032, Batch Gradient Norm after: 9.217318946232982
Epoch 2033/10000, Prediction Accuracy = 60.779999999999994%, Loss = 0.5395670890808105
Epoch: 2033, Batch Gradient Norm: 12.112881378662108
Epoch: 2033, Batch Gradient Norm after: 12.112881378662108
Epoch 2034/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.5578353404998779
Epoch: 2034, Batch Gradient Norm: 15.30969937615617
Epoch: 2034, Batch Gradient Norm after: 15.30969937615617
Epoch 2035/10000, Prediction Accuracy = 60.73599999999999%, Loss = 0.5828514456748962
Epoch: 2035, Batch Gradient Norm: 13.101748344150499
Epoch: 2035, Batch Gradient Norm after: 13.101748344150499
Epoch 2036/10000, Prediction Accuracy = 60.74400000000001%, Loss = 0.5648226737976074
Epoch: 2036, Batch Gradient Norm: 9.198437230315513
Epoch: 2036, Batch Gradient Norm after: 9.198437230315513
Epoch 2037/10000, Prediction Accuracy = 60.70399999999999%, Loss = 0.537910521030426
Epoch: 2037, Batch Gradient Norm: 8.957867478499143
Epoch: 2037, Batch Gradient Norm after: 8.957867478499143
Epoch 2038/10000, Prediction Accuracy = 60.79200000000001%, Loss = 0.5367752075195312
Epoch: 2038, Batch Gradient Norm: 11.238117720683269
Epoch: 2038, Batch Gradient Norm after: 11.238117720683269
Epoch 2039/10000, Prediction Accuracy = 60.775999999999996%, Loss = 0.5495024204254151
Epoch: 2039, Batch Gradient Norm: 12.978620916236858
Epoch: 2039, Batch Gradient Norm after: 12.978620916236858
Epoch 2040/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5628559112548828
Epoch: 2040, Batch Gradient Norm: 13.430628549782574
Epoch: 2040, Batch Gradient Norm after: 13.430628549782574
Epoch 2041/10000, Prediction Accuracy = 60.714%, Loss = 0.5681154012680054
Epoch: 2041, Batch Gradient Norm: 9.440038793987005
Epoch: 2041, Batch Gradient Norm after: 9.440038793987005
Epoch 2042/10000, Prediction Accuracy = 60.733999999999995%, Loss = 0.5400463223457337
Epoch: 2042, Batch Gradient Norm: 10.056416150734849
Epoch: 2042, Batch Gradient Norm after: 10.056416150734849
Epoch 2043/10000, Prediction Accuracy = 60.73%, Loss = 0.5420489311218262
Epoch: 2043, Batch Gradient Norm: 13.626337614311186
Epoch: 2043, Batch Gradient Norm after: 13.626337614311186
Epoch 2044/10000, Prediction Accuracy = 60.73%, Loss = 0.5650262236595154
Epoch: 2044, Batch Gradient Norm: 14.73104240337333
Epoch: 2044, Batch Gradient Norm after: 14.73104240337333
Epoch 2045/10000, Prediction Accuracy = 60.782%, Loss = 0.573896074295044
Epoch: 2045, Batch Gradient Norm: 11.087806514814785
Epoch: 2045, Batch Gradient Norm after: 11.087806514814785
Epoch 2046/10000, Prediction Accuracy = 60.720000000000006%, Loss = 0.5473888158798218
Epoch: 2046, Batch Gradient Norm: 9.975554523503549
Epoch: 2046, Batch Gradient Norm after: 9.975554523503549
Epoch 2047/10000, Prediction Accuracy = 60.80800000000001%, Loss = 0.5403327703475952
Epoch: 2047, Batch Gradient Norm: 11.911618128648884
Epoch: 2047, Batch Gradient Norm after: 11.911618128648884
Epoch 2048/10000, Prediction Accuracy = 60.658%, Loss = 0.5557378768920899
Epoch: 2048, Batch Gradient Norm: 10.965865875526706
Epoch: 2048, Batch Gradient Norm after: 10.965865875526706
Epoch 2049/10000, Prediction Accuracy = 60.739999999999995%, Loss = 0.5496986627578735
Epoch: 2049, Batch Gradient Norm: 8.63317857161239
Epoch: 2049, Batch Gradient Norm after: 8.63317857161239
Epoch 2050/10000, Prediction Accuracy = 60.746%, Loss = 0.534660792350769
Epoch: 2050, Batch Gradient Norm: 10.146930943190732
Epoch: 2050, Batch Gradient Norm after: 10.146930943190732
Epoch 2051/10000, Prediction Accuracy = 60.738%, Loss = 0.5423966169357299
Epoch: 2051, Batch Gradient Norm: 10.486354059081043
Epoch: 2051, Batch Gradient Norm after: 10.486354059081043
Epoch 2052/10000, Prediction Accuracy = 60.715999999999994%, Loss = 0.5444514393806458
Epoch: 2052, Batch Gradient Norm: 10.101313935058156
Epoch: 2052, Batch Gradient Norm after: 10.101313935058156
Epoch 2053/10000, Prediction Accuracy = 60.754%, Loss = 0.5426470756530761
Epoch: 2053, Batch Gradient Norm: 9.657977331523528
Epoch: 2053, Batch Gradient Norm after: 9.657977331523528
Epoch 2054/10000, Prediction Accuracy = 60.774%, Loss = 0.5396878361701966
Epoch: 2054, Batch Gradient Norm: 10.591381994978102
Epoch: 2054, Batch Gradient Norm after: 10.591381994978102
Epoch 2055/10000, Prediction Accuracy = 60.746%, Loss = 0.5459436535835266
Epoch: 2055, Batch Gradient Norm: 11.793553537923827
Epoch: 2055, Batch Gradient Norm after: 11.793553537923827
Epoch 2056/10000, Prediction Accuracy = 60.842%, Loss = 0.5543797373771667
Epoch: 2056, Batch Gradient Norm: 10.054743449779984
Epoch: 2056, Batch Gradient Norm after: 10.054743449779984
Epoch 2057/10000, Prediction Accuracy = 60.69599999999999%, Loss = 0.5424804210662841
Epoch: 2057, Batch Gradient Norm: 11.165918043440582
Epoch: 2057, Batch Gradient Norm after: 11.165918043440582
Epoch 2058/10000, Prediction Accuracy = 60.806%, Loss = 0.5495537161827088
Epoch: 2058, Batch Gradient Norm: 9.955357723979077
Epoch: 2058, Batch Gradient Norm after: 9.955357723979077
Epoch 2059/10000, Prediction Accuracy = 60.742%, Loss = 0.541339910030365
Epoch: 2059, Batch Gradient Norm: 13.140321685475593
Epoch: 2059, Batch Gradient Norm after: 13.140321685475593
Epoch 2060/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5621613144874573
Epoch: 2060, Batch Gradient Norm: 12.28075570998957
Epoch: 2060, Batch Gradient Norm after: 12.28075570998957
Epoch 2061/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.5550175189971924
Epoch: 2061, Batch Gradient Norm: 11.952100229859791
Epoch: 2061, Batch Gradient Norm after: 11.952100229859791
Epoch 2062/10000, Prediction Accuracy = 60.73199999999999%, Loss = 0.5509357333183289
Epoch: 2062, Batch Gradient Norm: 12.840331248650239
Epoch: 2062, Batch Gradient Norm after: 12.840331248650239
Epoch 2063/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.55640709400177
Epoch: 2063, Batch Gradient Norm: 13.097962514816547
Epoch: 2063, Batch Gradient Norm after: 13.097962514816547
Epoch 2064/10000, Prediction Accuracy = 60.75599999999999%, Loss = 0.5595309734344482
Epoch: 2064, Batch Gradient Norm: 12.680918686286038
Epoch: 2064, Batch Gradient Norm after: 12.680918686286038
Epoch 2065/10000, Prediction Accuracy = 60.763999999999996%, Loss = 0.5584481954574585
Epoch: 2065, Batch Gradient Norm: 12.172857415947837
Epoch: 2065, Batch Gradient Norm after: 12.172857415947837
Epoch 2066/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.5575426697731019
Epoch: 2066, Batch Gradient Norm: 9.90211856973874
Epoch: 2066, Batch Gradient Norm after: 9.90211856973874
Epoch 2067/10000, Prediction Accuracy = 60.774%, Loss = 0.5419394969940186
Epoch: 2067, Batch Gradient Norm: 10.611955344355424
Epoch: 2067, Batch Gradient Norm after: 10.611955344355424
Epoch 2068/10000, Prediction Accuracy = 60.812%, Loss = 0.5451060056686401
Epoch: 2068, Batch Gradient Norm: 10.447202556362448
Epoch: 2068, Batch Gradient Norm after: 10.447202556362448
Epoch 2069/10000, Prediction Accuracy = 60.778%, Loss = 0.5436919689178467
Epoch: 2069, Batch Gradient Norm: 10.303491595114194
Epoch: 2069, Batch Gradient Norm after: 10.303491595114194
Epoch 2070/10000, Prediction Accuracy = 60.830000000000005%, Loss = 0.5424115777015686
Epoch: 2070, Batch Gradient Norm: 9.202181609406574
Epoch: 2070, Batch Gradient Norm after: 9.202181609406574
Epoch 2071/10000, Prediction Accuracy = 60.79200000000001%, Loss = 0.536185908317566
Epoch: 2071, Batch Gradient Norm: 8.94044878483771
Epoch: 2071, Batch Gradient Norm after: 8.94044878483771
Epoch 2072/10000, Prediction Accuracy = 60.778%, Loss = 0.5346246838569642
Epoch: 2072, Batch Gradient Norm: 10.815473286522693
Epoch: 2072, Batch Gradient Norm after: 10.815473286522693
Epoch 2073/10000, Prediction Accuracy = 60.848%, Loss = 0.5461403250694274
Epoch: 2073, Batch Gradient Norm: 13.017348090055092
Epoch: 2073, Batch Gradient Norm after: 13.017348090055092
Epoch 2074/10000, Prediction Accuracy = 60.726%, Loss = 0.5620689272880555
Epoch: 2074, Batch Gradient Norm: 13.196117201019689
Epoch: 2074, Batch Gradient Norm after: 13.196117201019689
Epoch 2075/10000, Prediction Accuracy = 60.746%, Loss = 0.5629180073738098
Epoch: 2075, Batch Gradient Norm: 10.322786248036499
Epoch: 2075, Batch Gradient Norm after: 10.322786248036499
Epoch 2076/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5424749255180359
Epoch: 2076, Batch Gradient Norm: 9.223131396491267
Epoch: 2076, Batch Gradient Norm after: 9.223131396491267
Epoch 2077/10000, Prediction Accuracy = 60.712%, Loss = 0.5355738401412964
Epoch: 2077, Batch Gradient Norm: 11.042167222647837
Epoch: 2077, Batch Gradient Norm after: 11.042167222647837
Epoch 2078/10000, Prediction Accuracy = 60.85%, Loss = 0.5452874302864075
Epoch: 2078, Batch Gradient Norm: 11.49331019856988
Epoch: 2078, Batch Gradient Norm after: 11.49331019856988
Epoch 2079/10000, Prediction Accuracy = 60.702%, Loss = 0.5474713206291199
Epoch: 2079, Batch Gradient Norm: 11.264260011352595
Epoch: 2079, Batch Gradient Norm after: 11.264260011352595
Epoch 2080/10000, Prediction Accuracy = 60.834%, Loss = 0.5454597353935242
Epoch: 2080, Batch Gradient Norm: 9.736943408254948
Epoch: 2080, Batch Gradient Norm after: 9.736943408254948
Epoch 2081/10000, Prediction Accuracy = 60.73199999999999%, Loss = 0.53593989610672
Epoch: 2081, Batch Gradient Norm: 11.651391336402305
Epoch: 2081, Batch Gradient Norm after: 11.651391336402305
Epoch 2082/10000, Prediction Accuracy = 60.75%, Loss = 0.5480233669281006
Epoch: 2082, Batch Gradient Norm: 10.56598115420733
Epoch: 2082, Batch Gradient Norm after: 10.56598115420733
Epoch 2083/10000, Prediction Accuracy = 60.75200000000001%, Loss = 0.545315957069397
Epoch: 2083, Batch Gradient Norm: 9.232964099204196
Epoch: 2083, Batch Gradient Norm after: 9.232964099204196
Epoch 2084/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5359039783477784
Epoch: 2084, Batch Gradient Norm: 10.949232157957393
Epoch: 2084, Batch Gradient Norm after: 10.949232157957393
Epoch 2085/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.5453264474868774
Epoch: 2085, Batch Gradient Norm: 13.826893041222307
Epoch: 2085, Batch Gradient Norm after: 13.826893041222307
Epoch 2086/10000, Prediction Accuracy = 60.734%, Loss = 0.5666884660720826
Epoch: 2086, Batch Gradient Norm: 13.495069607061824
Epoch: 2086, Batch Gradient Norm after: 13.495069607061824
Epoch 2087/10000, Prediction Accuracy = 60.774%, Loss = 0.5657434225082397
Epoch: 2087, Batch Gradient Norm: 9.904520635889304
Epoch: 2087, Batch Gradient Norm after: 9.904520635889304
Epoch 2088/10000, Prediction Accuracy = 60.824%, Loss = 0.5403318405151367
Epoch: 2088, Batch Gradient Norm: 9.372049863289146
Epoch: 2088, Batch Gradient Norm after: 9.372049863289146
Epoch 2089/10000, Prediction Accuracy = 60.794000000000004%, Loss = 0.5361144781112671
Epoch: 2089, Batch Gradient Norm: 10.405302077162979
Epoch: 2089, Batch Gradient Norm after: 10.405302077162979
Epoch 2090/10000, Prediction Accuracy = 60.802%, Loss = 0.5440557599067688
Epoch: 2090, Batch Gradient Norm: 13.740075926236617
Epoch: 2090, Batch Gradient Norm after: 13.740075926236617
Epoch 2091/10000, Prediction Accuracy = 60.874%, Loss = 0.5680601477622986
Epoch: 2091, Batch Gradient Norm: 13.551543905607554
Epoch: 2091, Batch Gradient Norm after: 13.551543905607554
Epoch 2092/10000, Prediction Accuracy = 60.738%, Loss = 0.5662745714187623
Epoch: 2092, Batch Gradient Norm: 11.592974282627823
Epoch: 2092, Batch Gradient Norm after: 11.592974282627823
Epoch 2093/10000, Prediction Accuracy = 60.788%, Loss = 0.5508715391159058
Epoch: 2093, Batch Gradient Norm: 9.154660591084088
Epoch: 2093, Batch Gradient Norm after: 9.154660591084088
Epoch 2094/10000, Prediction Accuracy = 60.818%, Loss = 0.5343911170959472
Epoch: 2094, Batch Gradient Norm: 10.603572466366431
Epoch: 2094, Batch Gradient Norm after: 10.603572466366431
Epoch 2095/10000, Prediction Accuracy = 60.784000000000006%, Loss = 0.5424965500831604
Epoch: 2095, Batch Gradient Norm: 11.768991545898286
Epoch: 2095, Batch Gradient Norm after: 11.768991545898286
Epoch 2096/10000, Prediction Accuracy = 60.81%, Loss = 0.5519134402275085
Epoch: 2096, Batch Gradient Norm: 7.7411747571123
Epoch: 2096, Batch Gradient Norm after: 7.7411747571123
Epoch 2097/10000, Prediction Accuracy = 60.81%, Loss = 0.5285756945610046
Epoch: 2097, Batch Gradient Norm: 7.8356822868305525
Epoch: 2097, Batch Gradient Norm after: 7.8356822868305525
Epoch 2098/10000, Prediction Accuracy = 60.786%, Loss = 0.5286477327346801
Epoch: 2098, Batch Gradient Norm: 7.746865204772941
Epoch: 2098, Batch Gradient Norm after: 7.746865204772941
Epoch 2099/10000, Prediction Accuracy = 60.822%, Loss = 0.5276468515396118
Epoch: 2099, Batch Gradient Norm: 14.410406621669576
Epoch: 2099, Batch Gradient Norm after: 14.410406621669576
Epoch 2100/10000, Prediction Accuracy = 60.812%, Loss = 0.5722249746322632
Epoch: 2100, Batch Gradient Norm: 11.929814786155537
Epoch: 2100, Batch Gradient Norm after: 11.929814786155537
Epoch 2101/10000, Prediction Accuracy = 60.80400000000001%, Loss = 0.554809284210205
Epoch: 2101, Batch Gradient Norm: 9.93915139202368
Epoch: 2101, Batch Gradient Norm after: 9.93915139202368
Epoch 2102/10000, Prediction Accuracy = 60.790000000000006%, Loss = 0.5371328353881836
Epoch: 2102, Batch Gradient Norm: 14.235768454854746
Epoch: 2102, Batch Gradient Norm after: 14.235768454854746
Epoch 2103/10000, Prediction Accuracy = 60.894000000000005%, Loss = 0.5642619848251342
Epoch: 2103, Batch Gradient Norm: 14.238020553638641
Epoch: 2103, Batch Gradient Norm after: 14.238020553638641
Epoch 2104/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5666498661041259
Epoch: 2104, Batch Gradient Norm: 11.021762234186562
Epoch: 2104, Batch Gradient Norm after: 11.021762234186562
Epoch 2105/10000, Prediction Accuracy = 60.812%, Loss = 0.545099675655365
Epoch: 2105, Batch Gradient Norm: 9.268813351967275
Epoch: 2105, Batch Gradient Norm after: 9.268813351967275
Epoch 2106/10000, Prediction Accuracy = 60.814%, Loss = 0.5350004076957703
Epoch: 2106, Batch Gradient Norm: 8.91311914331461
Epoch: 2106, Batch Gradient Norm after: 8.91311914331461
Epoch 2107/10000, Prediction Accuracy = 60.838%, Loss = 0.5329748272895813
Epoch: 2107, Batch Gradient Norm: 10.807253598678379
Epoch: 2107, Batch Gradient Norm after: 10.807253598678379
Epoch 2108/10000, Prediction Accuracy = 60.803999999999995%, Loss = 0.5440189361572265
Epoch: 2108, Batch Gradient Norm: 12.326308359496345
Epoch: 2108, Batch Gradient Norm after: 12.326308359496345
Epoch 2109/10000, Prediction Accuracy = 60.79600000000001%, Loss = 0.5553133487701416
Epoch: 2109, Batch Gradient Norm: 11.403282812620999
Epoch: 2109, Batch Gradient Norm after: 11.403282812620999
Epoch 2110/10000, Prediction Accuracy = 60.80000000000001%, Loss = 0.5487371563911438
Epoch: 2110, Batch Gradient Norm: 10.188751888438665
Epoch: 2110, Batch Gradient Norm after: 10.188751888438665
Epoch 2111/10000, Prediction Accuracy = 60.864%, Loss = 0.5395744204521179
Epoch: 2111, Batch Gradient Norm: 11.91000336431763
Epoch: 2111, Batch Gradient Norm after: 11.91000336431763
Epoch 2112/10000, Prediction Accuracy = 60.790000000000006%, Loss = 0.5509714722633362
Epoch: 2112, Batch Gradient Norm: 11.631388701647777
Epoch: 2112, Batch Gradient Norm after: 11.631388701647777
Epoch 2113/10000, Prediction Accuracy = 60.854%, Loss = 0.548709237575531
Epoch: 2113, Batch Gradient Norm: 10.306048482590414
Epoch: 2113, Batch Gradient Norm after: 10.306048482590414
Epoch 2114/10000, Prediction Accuracy = 60.822%, Loss = 0.5402773141860961
Epoch: 2114, Batch Gradient Norm: 11.077085949202756
Epoch: 2114, Batch Gradient Norm after: 11.077085949202756
Epoch 2115/10000, Prediction Accuracy = 60.84400000000001%, Loss = 0.5451454281806946
Epoch: 2115, Batch Gradient Norm: 10.956255464819902
Epoch: 2115, Batch Gradient Norm after: 10.956255464819902
Epoch 2116/10000, Prediction Accuracy = 60.83800000000001%, Loss = 0.5443609833717347
Epoch: 2116, Batch Gradient Norm: 10.313132281166242
Epoch: 2116, Batch Gradient Norm after: 10.313132281166242
Epoch 2117/10000, Prediction Accuracy = 60.85%, Loss = 0.5404440641403199
Epoch: 2117, Batch Gradient Norm: 12.125760804856785
Epoch: 2117, Batch Gradient Norm after: 12.125760804856785
Epoch 2118/10000, Prediction Accuracy = 60.88599999999999%, Loss = 0.5524568438529969
Epoch: 2118, Batch Gradient Norm: 11.678117177634903
Epoch: 2118, Batch Gradient Norm after: 11.678117177634903
Epoch 2119/10000, Prediction Accuracy = 60.81600000000001%, Loss = 0.5514326214790344
Epoch: 2119, Batch Gradient Norm: 10.570389158273727
Epoch: 2119, Batch Gradient Norm after: 10.570389158273727
Epoch 2120/10000, Prediction Accuracy = 60.83799999999999%, Loss = 0.5434043884277344
Epoch: 2120, Batch Gradient Norm: 8.69636828654999
Epoch: 2120, Batch Gradient Norm after: 8.69636828654999
Epoch 2121/10000, Prediction Accuracy = 60.826%, Loss = 0.5317569851875306
Epoch: 2121, Batch Gradient Norm: 10.87743843363928
Epoch: 2121, Batch Gradient Norm after: 10.87743843363928
Epoch 2122/10000, Prediction Accuracy = 60.831999999999994%, Loss = 0.5441998720169068
Epoch: 2122, Batch Gradient Norm: 10.908361507586086
Epoch: 2122, Batch Gradient Norm after: 10.908361507586086
Epoch 2123/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5461631417274475
Epoch: 2123, Batch Gradient Norm: 9.478544654689687
Epoch: 2123, Batch Gradient Norm after: 9.478544654689687
Epoch 2124/10000, Prediction Accuracy = 60.834%, Loss = 0.538011634349823
Epoch: 2124, Batch Gradient Norm: 9.580510050222632
Epoch: 2124, Batch Gradient Norm after: 9.580510050222632
Epoch 2125/10000, Prediction Accuracy = 60.838%, Loss = 0.5381960153579712
Epoch: 2125, Batch Gradient Norm: 9.79223565241974
Epoch: 2125, Batch Gradient Norm after: 9.79223565241974
Epoch 2126/10000, Prediction Accuracy = 60.82000000000001%, Loss = 0.5368285417556763
Epoch: 2126, Batch Gradient Norm: 14.72274209436566
Epoch: 2126, Batch Gradient Norm after: 14.72274209436566
Epoch 2127/10000, Prediction Accuracy = 60.896%, Loss = 0.5686928391456604
Epoch: 2127, Batch Gradient Norm: 14.147500435655363
Epoch: 2127, Batch Gradient Norm after: 14.147500435655363
Epoch 2128/10000, Prediction Accuracy = 60.794%, Loss = 0.5639363050460815
Epoch: 2128, Batch Gradient Norm: 10.488194343429281
Epoch: 2128, Batch Gradient Norm after: 10.488194343429281
Epoch 2129/10000, Prediction Accuracy = 60.884%, Loss = 0.5386271119117737
Epoch: 2129, Batch Gradient Norm: 9.71944780100462
Epoch: 2129, Batch Gradient Norm after: 9.71944780100462
Epoch 2130/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5345546126365661
Epoch: 2130, Batch Gradient Norm: 9.86458681052887
Epoch: 2130, Batch Gradient Norm after: 9.86458681052887
Epoch 2131/10000, Prediction Accuracy = 60.9%, Loss = 0.5364970564842224
Epoch: 2131, Batch Gradient Norm: 9.783807353480043
Epoch: 2131, Batch Gradient Norm after: 9.783807353480043
Epoch 2132/10000, Prediction Accuracy = 60.85%, Loss = 0.5362555861473084
Epoch: 2132, Batch Gradient Norm: 11.831507795725255
Epoch: 2132, Batch Gradient Norm after: 11.831507795725255
Epoch 2133/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5508127570152282
Epoch: 2133, Batch Gradient Norm: 12.613842823755276
Epoch: 2133, Batch Gradient Norm after: 12.613842823755276
Epoch 2134/10000, Prediction Accuracy = 60.838%, Loss = 0.5590482592582703
Epoch: 2134, Batch Gradient Norm: 8.824193427181731
Epoch: 2134, Batch Gradient Norm after: 8.824193427181731
Epoch 2135/10000, Prediction Accuracy = 60.870000000000005%, Loss = 0.5318673133850098
Epoch: 2135, Batch Gradient Norm: 8.520557973426136
Epoch: 2135, Batch Gradient Norm after: 8.520557973426136
Epoch 2136/10000, Prediction Accuracy = 60.85%, Loss = 0.5287829279899597
Epoch: 2136, Batch Gradient Norm: 11.11950218444277
Epoch: 2136, Batch Gradient Norm after: 11.11950218444277
Epoch 2137/10000, Prediction Accuracy = 60.767999999999994%, Loss = 0.5442859292030334
Epoch: 2137, Batch Gradient Norm: 15.270745359664177
Epoch: 2137, Batch Gradient Norm after: 15.270745359664177
Epoch 2138/10000, Prediction Accuracy = 60.772000000000006%, Loss = 0.5780522584915161
Epoch: 2138, Batch Gradient Norm: 13.723195385345948
Epoch: 2138, Batch Gradient Norm after: 13.723195385345948
Epoch 2139/10000, Prediction Accuracy = 60.774%, Loss = 0.5643068075180053
Epoch: 2139, Batch Gradient Norm: 10.301589538246661
Epoch: 2139, Batch Gradient Norm after: 10.301589538246661
Epoch 2140/10000, Prediction Accuracy = 60.86%, Loss = 0.5394075512886047
Epoch: 2140, Batch Gradient Norm: 10.318711895676373
Epoch: 2140, Batch Gradient Norm after: 10.318711895676373
Epoch 2141/10000, Prediction Accuracy = 60.884%, Loss = 0.5391765236854553
Epoch: 2141, Batch Gradient Norm: 9.024215793534669
Epoch: 2141, Batch Gradient Norm after: 9.024215793534669
Epoch 2142/10000, Prediction Accuracy = 60.82399999999999%, Loss = 0.5316160559654236
Epoch: 2142, Batch Gradient Norm: 10.415830152046214
Epoch: 2142, Batch Gradient Norm after: 10.415830152046214
Epoch 2143/10000, Prediction Accuracy = 60.89%, Loss = 0.5395341753959656
Epoch: 2143, Batch Gradient Norm: 12.050573670310092
Epoch: 2143, Batch Gradient Norm after: 12.050573670310092
Epoch 2144/10000, Prediction Accuracy = 60.786%, Loss = 0.5502508878707886
Epoch: 2144, Batch Gradient Norm: 11.0848618705044
Epoch: 2144, Batch Gradient Norm after: 11.0848618705044
Epoch 2145/10000, Prediction Accuracy = 60.836%, Loss = 0.5439200162887573
Epoch: 2145, Batch Gradient Norm: 10.235562542813181
Epoch: 2145, Batch Gradient Norm after: 10.235562542813181
Epoch 2146/10000, Prediction Accuracy = 60.8%, Loss = 0.5394704103469848
Epoch: 2146, Batch Gradient Norm: 10.141931994889491
Epoch: 2146, Batch Gradient Norm after: 10.141931994889491
Epoch 2147/10000, Prediction Accuracy = 60.838%, Loss = 0.5401619076728821
Epoch: 2147, Batch Gradient Norm: 11.275184334215309
Epoch: 2147, Batch Gradient Norm after: 11.275184334215309
Epoch 2148/10000, Prediction Accuracy = 60.788%, Loss = 0.5465476989746094
Epoch: 2148, Batch Gradient Norm: 13.73193159452033
Epoch: 2148, Batch Gradient Norm after: 13.73193159452033
Epoch 2149/10000, Prediction Accuracy = 60.81%, Loss = 0.5655102372169495
Epoch: 2149, Batch Gradient Norm: 9.344106133171433
Epoch: 2149, Batch Gradient Norm after: 9.344106133171433
Epoch 2150/10000, Prediction Accuracy = 60.836%, Loss = 0.5339975118637085
Epoch: 2150, Batch Gradient Norm: 8.484054768205803
Epoch: 2150, Batch Gradient Norm after: 8.484054768205803
Epoch 2151/10000, Prediction Accuracy = 60.898%, Loss = 0.5283973336219787
Epoch: 2151, Batch Gradient Norm: 7.987134026203141
Epoch: 2151, Batch Gradient Norm after: 7.987134026203141
Epoch 2152/10000, Prediction Accuracy = 60.751999999999995%, Loss = 0.5260087966918945
Epoch: 2152, Batch Gradient Norm: 10.087689808972685
Epoch: 2152, Batch Gradient Norm after: 10.087689808972685
Epoch 2153/10000, Prediction Accuracy = 60.903999999999996%, Loss = 0.5363790273666382
Epoch: 2153, Batch Gradient Norm: 13.979483226781037
Epoch: 2153, Batch Gradient Norm after: 13.979483226781037
Epoch 2154/10000, Prediction Accuracy = 60.758%, Loss = 0.5598232865333557
Epoch: 2154, Batch Gradient Norm: 16.673096130102927
Epoch: 2154, Batch Gradient Norm after: 16.673096130102927
Epoch 2155/10000, Prediction Accuracy = 60.806000000000004%, Loss = 0.5856688857078552
Epoch: 2155, Batch Gradient Norm: 9.767551356769044
Epoch: 2155, Batch Gradient Norm after: 9.767551356769044
Epoch 2156/10000, Prediction Accuracy = 60.91600000000001%, Loss = 0.5355304956436158
Epoch: 2156, Batch Gradient Norm: 9.177813710874132
Epoch: 2156, Batch Gradient Norm after: 9.177813710874132
Epoch 2157/10000, Prediction Accuracy = 60.879999999999995%, Loss = 0.5313179969787598
Epoch: 2157, Batch Gradient Norm: 13.088797889844948
Epoch: 2157, Batch Gradient Norm after: 13.088797889844948
Epoch 2158/10000, Prediction Accuracy = 60.867999999999995%, Loss = 0.5593537569046021
Epoch: 2158, Batch Gradient Norm: 11.2983403364897
Epoch: 2158, Batch Gradient Norm after: 11.2983403364897
Epoch 2159/10000, Prediction Accuracy = 60.943999999999996%, Loss = 0.5454091787338257
Epoch: 2159, Batch Gradient Norm: 10.611580194743565
Epoch: 2159, Batch Gradient Norm after: 10.611580194743565
Epoch 2160/10000, Prediction Accuracy = 60.912%, Loss = 0.539898669719696
Epoch: 2160, Batch Gradient Norm: 10.545580942518354
Epoch: 2160, Batch Gradient Norm after: 10.545580942518354
Epoch 2161/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.539684247970581
Epoch: 2161, Batch Gradient Norm: 10.29646532996668
Epoch: 2161, Batch Gradient Norm after: 10.29646532996668
Epoch 2162/10000, Prediction Accuracy = 60.80799999999999%, Loss = 0.5379517793655395
Epoch: 2162, Batch Gradient Norm: 10.244412285449947
Epoch: 2162, Batch Gradient Norm after: 10.244412285449947
Epoch 2163/10000, Prediction Accuracy = 60.89399999999999%, Loss = 0.5379581570625305
Epoch: 2163, Batch Gradient Norm: 10.724743459068035
Epoch: 2163, Batch Gradient Norm after: 10.724743459068035
Epoch 2164/10000, Prediction Accuracy = 60.790000000000006%, Loss = 0.5398109436035157
Epoch: 2164, Batch Gradient Norm: 12.659434513390135
Epoch: 2164, Batch Gradient Norm after: 12.659434513390135
Epoch 2165/10000, Prediction Accuracy = 60.90999999999999%, Loss = 0.5528337478637695
Epoch: 2165, Batch Gradient Norm: 12.483574263338912
Epoch: 2165, Batch Gradient Norm after: 12.483574263338912
Epoch 2166/10000, Prediction Accuracy = 60.815999999999995%, Loss = 0.5525197744369507
Epoch: 2166, Batch Gradient Norm: 10.37955875986122
Epoch: 2166, Batch Gradient Norm after: 10.37955875986122
Epoch 2167/10000, Prediction Accuracy = 60.952%, Loss = 0.5386518597602844
Epoch: 2167, Batch Gradient Norm: 9.776660236644696
Epoch: 2167, Batch Gradient Norm after: 9.776660236644696
Epoch 2168/10000, Prediction Accuracy = 60.827999999999996%, Loss = 0.5347394585609436
Epoch: 2168, Batch Gradient Norm: 10.416564281537159
Epoch: 2168, Batch Gradient Norm after: 10.416564281537159
Epoch 2169/10000, Prediction Accuracy = 60.95%, Loss = 0.5385114550590515
Epoch: 2169, Batch Gradient Norm: 10.287261137921966
Epoch: 2169, Batch Gradient Norm after: 10.287261137921966
Epoch 2170/10000, Prediction Accuracy = 60.788%, Loss = 0.537997043132782
Epoch: 2170, Batch Gradient Norm: 9.331310333242968
Epoch: 2170, Batch Gradient Norm after: 9.331310333242968
Epoch 2171/10000, Prediction Accuracy = 60.822%, Loss = 0.532400929927826
Epoch: 2171, Batch Gradient Norm: 9.20037263187975
Epoch: 2171, Batch Gradient Norm after: 9.20037263187975
Epoch 2172/10000, Prediction Accuracy = 60.798%, Loss = 0.5317484021186829
Epoch: 2172, Batch Gradient Norm: 9.564356167733925
Epoch: 2172, Batch Gradient Norm after: 9.564356167733925
Epoch 2173/10000, Prediction Accuracy = 60.876%, Loss = 0.5336853265762329
Epoch: 2173, Batch Gradient Norm: 12.46484765401141
Epoch: 2173, Batch Gradient Norm after: 12.46484765401141
Epoch 2174/10000, Prediction Accuracy = 60.842000000000006%, Loss = 0.5515843987464905
Epoch: 2174, Batch Gradient Norm: 13.683639230349373
Epoch: 2174, Batch Gradient Norm after: 13.683639230349373
Epoch 2175/10000, Prediction Accuracy = 60.862%, Loss = 0.55890394449234
Epoch: 2175, Batch Gradient Norm: 13.49053818231301
Epoch: 2175, Batch Gradient Norm after: 13.49053818231301
Epoch 2176/10000, Prediction Accuracy = 60.92%, Loss = 0.5579289317131042
Epoch: 2176, Batch Gradient Norm: 12.308824297475246
Epoch: 2176, Batch Gradient Norm after: 12.308824297475246
Epoch 2177/10000, Prediction Accuracy = 60.878%, Loss = 0.5522953987121582
Epoch: 2177, Batch Gradient Norm: 10.162744319850326
Epoch: 2177, Batch Gradient Norm after: 10.162744319850326
Epoch 2178/10000, Prediction Accuracy = 60.838%, Loss = 0.5383732795715332
Epoch: 2178, Batch Gradient Norm: 10.60170395775812
Epoch: 2178, Batch Gradient Norm after: 10.60170395775812
Epoch 2179/10000, Prediction Accuracy = 60.93399999999999%, Loss = 0.540329885482788
Epoch: 2179, Batch Gradient Norm: 10.158706737961475
Epoch: 2179, Batch Gradient Norm after: 10.158706737961475
Epoch 2180/10000, Prediction Accuracy = 60.83200000000001%, Loss = 0.5360903978347779
Epoch: 2180, Batch Gradient Norm: 11.601566443629793
Epoch: 2180, Batch Gradient Norm after: 11.601566443629793
Epoch 2181/10000, Prediction Accuracy = 60.94200000000001%, Loss = 0.544462263584137
Epoch: 2181, Batch Gradient Norm: 10.00279612659083
Epoch: 2181, Batch Gradient Norm after: 10.00279612659083
Epoch 2182/10000, Prediction Accuracy = 60.79599999999999%, Loss = 0.5346425890922546
Epoch: 2182, Batch Gradient Norm: 11.263664346791325
Epoch: 2182, Batch Gradient Norm after: 11.263664346791325
Epoch 2183/10000, Prediction Accuracy = 60.952%, Loss = 0.5443361520767211
Epoch: 2183, Batch Gradient Norm: 8.261412549721967
Epoch: 2183, Batch Gradient Norm after: 8.261412549721967
Epoch 2184/10000, Prediction Accuracy = 60.826%, Loss = 0.5271345496177673
Epoch: 2184, Batch Gradient Norm: 9.373054825937139
Epoch: 2184, Batch Gradient Norm after: 9.373054825937139
Epoch 2185/10000, Prediction Accuracy = 60.858000000000004%, Loss = 0.5330206036567688
Epoch: 2185, Batch Gradient Norm: 9.437587419963759
Epoch: 2185, Batch Gradient Norm after: 9.437587419963759
Epoch 2186/10000, Prediction Accuracy = 60.916%, Loss = 0.5316213369369507
Epoch: 2186, Batch Gradient Norm: 14.59107828135994
Epoch: 2186, Batch Gradient Norm after: 14.59107828135994
Epoch 2187/10000, Prediction Accuracy = 60.854%, Loss = 0.5690816044807434
Epoch: 2187, Batch Gradient Norm: 13.382623071178056
Epoch: 2187, Batch Gradient Norm after: 13.382623071178056
Epoch 2188/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5592333078384399
Epoch: 2188, Batch Gradient Norm: 12.01300399128488
Epoch: 2188, Batch Gradient Norm after: 12.01300399128488
Epoch 2189/10000, Prediction Accuracy = 60.83%, Loss = 0.5477757453918457
Epoch: 2189, Batch Gradient Norm: 11.111026502776742
Epoch: 2189, Batch Gradient Norm after: 11.111026502776742
Epoch 2190/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.5416188359260559
Epoch: 2190, Batch Gradient Norm: 11.13567383136034
Epoch: 2190, Batch Gradient Norm after: 11.13567383136034
Epoch 2191/10000, Prediction Accuracy = 60.864%, Loss = 0.5410664081573486
Epoch: 2191, Batch Gradient Norm: 11.160673489964001
Epoch: 2191, Batch Gradient Norm after: 11.160673489964001
Epoch 2192/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.5410684466361999
Epoch: 2192, Batch Gradient Norm: 11.086040198103966
Epoch: 2192, Batch Gradient Norm after: 11.086040198103966
Epoch 2193/10000, Prediction Accuracy = 60.874%, Loss = 0.5410373330116272
Epoch: 2193, Batch Gradient Norm: 11.02157707654925
Epoch: 2193, Batch Gradient Norm after: 11.02157707654925
Epoch 2194/10000, Prediction Accuracy = 60.916%, Loss = 0.5407930612564087
Epoch: 2194, Batch Gradient Norm: 11.002963339865337
Epoch: 2194, Batch Gradient Norm after: 11.002963339865337
Epoch 2195/10000, Prediction Accuracy = 60.862%, Loss = 0.5417952299118042
Epoch: 2195, Batch Gradient Norm: 10.814364338133696
Epoch: 2195, Batch Gradient Norm after: 10.814364338133696
Epoch 2196/10000, Prediction Accuracy = 60.896%, Loss = 0.5407652497291565
Epoch: 2196, Batch Gradient Norm: 11.049197580662396
Epoch: 2196, Batch Gradient Norm after: 11.049197580662396
Epoch 2197/10000, Prediction Accuracy = 60.874%, Loss = 0.541032075881958
Epoch: 2197, Batch Gradient Norm: 12.234428798321227
Epoch: 2197, Batch Gradient Norm after: 12.234428798321227
Epoch 2198/10000, Prediction Accuracy = 60.90400000000001%, Loss = 0.547737717628479
Epoch: 2198, Batch Gradient Norm: 12.027142385540296
Epoch: 2198, Batch Gradient Norm after: 12.027142385540296
Epoch 2199/10000, Prediction Accuracy = 60.907999999999994%, Loss = 0.546068549156189
Epoch: 2199, Batch Gradient Norm: 10.433881391323514
Epoch: 2199, Batch Gradient Norm after: 10.433881391323514
Epoch 2200/10000, Prediction Accuracy = 60.872%, Loss = 0.5369049191474915
Epoch: 2200, Batch Gradient Norm: 8.73801296953906
Epoch: 2200, Batch Gradient Norm after: 8.73801296953906
Epoch 2201/10000, Prediction Accuracy = 60.93000000000001%, Loss = 0.5275702953338623
Epoch: 2201, Batch Gradient Norm: 9.473294144363324
Epoch: 2201, Batch Gradient Norm after: 9.473294144363324
Epoch 2202/10000, Prediction Accuracy = 60.86800000000001%, Loss = 0.5307971477508545
Epoch: 2202, Batch Gradient Norm: 10.34451876922383
Epoch: 2202, Batch Gradient Norm after: 10.34451876922383
Epoch 2203/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.5362613558769226
Epoch: 2203, Batch Gradient Norm: 10.699542253512787
Epoch: 2203, Batch Gradient Norm after: 10.699542253512787
Epoch 2204/10000, Prediction Accuracy = 60.888%, Loss = 0.5383942008018494
Epoch: 2204, Batch Gradient Norm: 10.899417367534587
Epoch: 2204, Batch Gradient Norm after: 10.899417367534587
Epoch 2205/10000, Prediction Accuracy = 60.932%, Loss = 0.540684175491333
Epoch: 2205, Batch Gradient Norm: 9.503399417081786
Epoch: 2205, Batch Gradient Norm after: 9.503399417081786
Epoch 2206/10000, Prediction Accuracy = 60.85%, Loss = 0.5319059729576111
Epoch: 2206, Batch Gradient Norm: 10.442978047755991
Epoch: 2206, Batch Gradient Norm after: 10.442978047755991
Epoch 2207/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.5373263359069824
Epoch: 2207, Batch Gradient Norm: 12.209139404438403
Epoch: 2207, Batch Gradient Norm after: 12.209139404438403
Epoch 2208/10000, Prediction Accuracy = 60.864%, Loss = 0.5477365612983703
Epoch: 2208, Batch Gradient Norm: 12.789542028492457
Epoch: 2208, Batch Gradient Norm after: 12.789542028492457
Epoch 2209/10000, Prediction Accuracy = 60.924%, Loss = 0.5523289561271667
Epoch: 2209, Batch Gradient Norm: 11.371387622793415
Epoch: 2209, Batch Gradient Norm after: 11.371387622793415
Epoch 2210/10000, Prediction Accuracy = 60.855999999999995%, Loss = 0.542625367641449
Epoch: 2210, Batch Gradient Norm: 9.840347151093443
Epoch: 2210, Batch Gradient Norm after: 9.840347151093443
Epoch 2211/10000, Prediction Accuracy = 60.9%, Loss = 0.5328052878379822
Epoch: 2211, Batch Gradient Norm: 10.111904787506838
Epoch: 2211, Batch Gradient Norm after: 10.111904787506838
Epoch 2212/10000, Prediction Accuracy = 60.862%, Loss = 0.5337794184684753
Epoch: 2212, Batch Gradient Norm: 12.218093578045272
Epoch: 2212, Batch Gradient Norm after: 12.218093578045272
Epoch 2213/10000, Prediction Accuracy = 60.874%, Loss = 0.5481528043746948
Epoch: 2213, Batch Gradient Norm: 13.088830040087336
Epoch: 2213, Batch Gradient Norm after: 13.088830040087336
Epoch 2214/10000, Prediction Accuracy = 60.94%, Loss = 0.5562170386314392
Epoch: 2214, Batch Gradient Norm: 10.767739499293464
Epoch: 2214, Batch Gradient Norm after: 10.767739499293464
Epoch 2215/10000, Prediction Accuracy = 60.912%, Loss = 0.5391840457916259
Epoch: 2215, Batch Gradient Norm: 9.720171812310523
Epoch: 2215, Batch Gradient Norm after: 9.720171812310523
Epoch 2216/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.5319236993789673
Epoch: 2216, Batch Gradient Norm: 10.507248424070555
Epoch: 2216, Batch Gradient Norm after: 10.507248424070555
Epoch 2217/10000, Prediction Accuracy = 60.914%, Loss = 0.5359690904617309
Epoch: 2217, Batch Gradient Norm: 12.941625427170568
Epoch: 2217, Batch Gradient Norm after: 12.941625427170568
Epoch 2218/10000, Prediction Accuracy = 60.898%, Loss = 0.5526707649230957
Epoch: 2218, Batch Gradient Norm: 12.237575029106242
Epoch: 2218, Batch Gradient Norm after: 12.237575029106242
Epoch 2219/10000, Prediction Accuracy = 60.952%, Loss = 0.5485012888908386
Epoch: 2219, Batch Gradient Norm: 8.495727481559275
Epoch: 2219, Batch Gradient Norm after: 8.495727481559275
Epoch 2220/10000, Prediction Accuracy = 60.86%, Loss = 0.5249592661857605
Epoch: 2220, Batch Gradient Norm: 9.840881870530184
Epoch: 2220, Batch Gradient Norm after: 9.840881870530184
Epoch 2221/10000, Prediction Accuracy = 60.888%, Loss = 0.5324073076248169
Epoch: 2221, Batch Gradient Norm: 12.34936349823661
Epoch: 2221, Batch Gradient Norm after: 12.34936349823661
Epoch 2222/10000, Prediction Accuracy = 60.824%, Loss = 0.5511512279510498
Epoch: 2222, Batch Gradient Norm: 11.387981598510654
Epoch: 2222, Batch Gradient Norm after: 11.387981598510654
Epoch 2223/10000, Prediction Accuracy = 60.92%, Loss = 0.5417075872421264
Epoch: 2223, Batch Gradient Norm: 11.232903206720227
Epoch: 2223, Batch Gradient Norm after: 11.232903206720227
Epoch 2224/10000, Prediction Accuracy = 60.96%, Loss = 0.5396587491035462
Epoch: 2224, Batch Gradient Norm: 10.631424637809111
Epoch: 2224, Batch Gradient Norm after: 10.631424637809111
Epoch 2225/10000, Prediction Accuracy = 60.78000000000001%, Loss = 0.5372345805168152
Epoch: 2225, Batch Gradient Norm: 11.139579793765225
Epoch: 2225, Batch Gradient Norm after: 11.139579793765225
Epoch 2226/10000, Prediction Accuracy = 60.92%, Loss = 0.5414179563522339
Epoch: 2226, Batch Gradient Norm: 11.919771847375072
Epoch: 2226, Batch Gradient Norm after: 11.919771847375072
Epoch 2227/10000, Prediction Accuracy = 60.812%, Loss = 0.5474001884460449
Epoch: 2227, Batch Gradient Norm: 11.868091233588997
Epoch: 2227, Batch Gradient Norm after: 11.868091233588997
Epoch 2228/10000, Prediction Accuracy = 60.936%, Loss = 0.5487637519836426
Epoch: 2228, Batch Gradient Norm: 8.832210551367805
Epoch: 2228, Batch Gradient Norm after: 8.832210551367805
Epoch 2229/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5267523288726806
Epoch: 2229, Batch Gradient Norm: 12.91067146074259
Epoch: 2229, Batch Gradient Norm after: 12.91067146074259
Epoch 2230/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5519256711006164
Epoch: 2230, Batch Gradient Norm: 12.034239783271518
Epoch: 2230, Batch Gradient Norm after: 12.034239783271518
Epoch 2231/10000, Prediction Accuracy = 60.948%, Loss = 0.5465476989746094
Epoch: 2231, Batch Gradient Norm: 9.488540165938625
Epoch: 2231, Batch Gradient Norm after: 9.488540165938625
Epoch 2232/10000, Prediction Accuracy = 60.83800000000001%, Loss = 0.529841959476471
Epoch: 2232, Batch Gradient Norm: 8.537843919077627
Epoch: 2232, Batch Gradient Norm after: 8.537843919077627
Epoch 2233/10000, Prediction Accuracy = 60.968%, Loss = 0.5239806294441223
Epoch: 2233, Batch Gradient Norm: 10.458549671905155
Epoch: 2233, Batch Gradient Norm after: 10.458549671905155
Epoch 2234/10000, Prediction Accuracy = 60.81999999999999%, Loss = 0.5338422298431397
Epoch: 2234, Batch Gradient Norm: 13.333504346978662
Epoch: 2234, Batch Gradient Norm after: 13.333504346978662
Epoch 2235/10000, Prediction Accuracy = 60.916%, Loss = 0.5546117663383484
Epoch: 2235, Batch Gradient Norm: 10.243983105013044
Epoch: 2235, Batch Gradient Norm after: 10.243983105013044
Epoch 2236/10000, Prediction Accuracy = 60.9%, Loss = 0.5339390397071838
Epoch: 2236, Batch Gradient Norm: 9.708527938341488
Epoch: 2236, Batch Gradient Norm after: 9.708527938341488
Epoch 2237/10000, Prediction Accuracy = 60.78399999999999%, Loss = 0.5312446713447571
Epoch: 2237, Batch Gradient Norm: 10.501802441967966
Epoch: 2237, Batch Gradient Norm after: 10.501802441967966
Epoch 2238/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.5357329845428467
Epoch: 2238, Batch Gradient Norm: 12.718919303403199
Epoch: 2238, Batch Gradient Norm after: 12.718919303403199
Epoch 2239/10000, Prediction Accuracy = 60.834%, Loss = 0.5520576953887939
Epoch: 2239, Batch Gradient Norm: 12.942297775430125
Epoch: 2239, Batch Gradient Norm after: 12.942297775430125
Epoch 2240/10000, Prediction Accuracy = 60.924%, Loss = 0.5556222915649414
Epoch: 2240, Batch Gradient Norm: 11.219987901223947
Epoch: 2240, Batch Gradient Norm after: 11.219987901223947
Epoch 2241/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.5409072875976563
Epoch: 2241, Batch Gradient Norm: 10.168524493404833
Epoch: 2241, Batch Gradient Norm after: 10.168524493404833
Epoch 2242/10000, Prediction Accuracy = 60.94200000000001%, Loss = 0.5332945108413696
Epoch: 2242, Batch Gradient Norm: 9.689732607268645
Epoch: 2242, Batch Gradient Norm after: 9.689732607268645
Epoch 2243/10000, Prediction Accuracy = 60.938%, Loss = 0.5300097942352295
Epoch: 2243, Batch Gradient Norm: 10.084242483529904
Epoch: 2243, Batch Gradient Norm after: 10.084242483529904
Epoch 2244/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.531343424320221
Epoch: 2244, Batch Gradient Norm: 12.097554917435334
Epoch: 2244, Batch Gradient Norm after: 12.097554917435334
Epoch 2245/10000, Prediction Accuracy = 60.924%, Loss = 0.5444546222686768
Epoch: 2245, Batch Gradient Norm: 11.613422929512737
Epoch: 2245, Batch Gradient Norm after: 11.613422929512737
Epoch 2246/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.5418269753456115
Epoch: 2246, Batch Gradient Norm: 8.95210990248618
Epoch: 2246, Batch Gradient Norm after: 8.95210990248618
Epoch 2247/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5256020903587342
Epoch: 2247, Batch Gradient Norm: 9.334786226854337
Epoch: 2247, Batch Gradient Norm after: 9.334786226854337
Epoch 2248/10000, Prediction Accuracy = 60.938%, Loss = 0.5271746039390564
Epoch: 2248, Batch Gradient Norm: 11.599389746180815
Epoch: 2248, Batch Gradient Norm after: 11.599389746180815
Epoch 2249/10000, Prediction Accuracy = 60.928%, Loss = 0.5416704058647156
Epoch: 2249, Batch Gradient Norm: 12.10864275877004
Epoch: 2249, Batch Gradient Norm after: 12.10864275877004
Epoch 2250/10000, Prediction Accuracy = 60.902%, Loss = 0.5476557016372681
Epoch: 2250, Batch Gradient Norm: 11.608724039565328
Epoch: 2250, Batch Gradient Norm after: 11.608724039565328
Epoch 2251/10000, Prediction Accuracy = 60.99400000000001%, Loss = 0.5467334270477295
Epoch: 2251, Batch Gradient Norm: 8.519390144030002
Epoch: 2251, Batch Gradient Norm after: 8.519390144030002
Epoch 2252/10000, Prediction Accuracy = 60.882000000000005%, Loss = 0.5247744798660279
Epoch: 2252, Batch Gradient Norm: 12.905539361075318
Epoch: 2252, Batch Gradient Norm after: 12.905539361075318
Epoch 2253/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5514561414718628
Epoch: 2253, Batch Gradient Norm: 11.944734969034352
Epoch: 2253, Batch Gradient Norm after: 11.944734969034352
Epoch 2254/10000, Prediction Accuracy = 60.81%, Loss = 0.5440492987632751
Epoch: 2254, Batch Gradient Norm: 10.181869244708045
Epoch: 2254, Batch Gradient Norm after: 10.181869244708045
Epoch 2255/10000, Prediction Accuracy = 61.008%, Loss = 0.5318771839141846
Epoch: 2255, Batch Gradient Norm: 8.810535795346269
Epoch: 2255, Batch Gradient Norm after: 8.810535795346269
Epoch 2256/10000, Prediction Accuracy = 60.9%, Loss = 0.5232551574707032
Epoch: 2256, Batch Gradient Norm: 12.338967078381657
Epoch: 2256, Batch Gradient Norm after: 12.338967078381657
Epoch 2257/10000, Prediction Accuracy = 60.948%, Loss = 0.5450789213180542
Epoch: 2257, Batch Gradient Norm: 12.264653230120855
Epoch: 2257, Batch Gradient Norm after: 12.264653230120855
Epoch 2258/10000, Prediction Accuracy = 60.914%, Loss = 0.5445412516593933
Epoch: 2258, Batch Gradient Norm: 13.835202526180286
Epoch: 2258, Batch Gradient Norm after: 13.835202526180286
Epoch 2259/10000, Prediction Accuracy = 60.914%, Loss = 0.5591249108314514
Epoch: 2259, Batch Gradient Norm: 12.023357090408963
Epoch: 2259, Batch Gradient Norm after: 12.023357090408963
Epoch 2260/10000, Prediction Accuracy = 60.812%, Loss = 0.5473072528839111
Epoch: 2260, Batch Gradient Norm: 10.13964369618452
Epoch: 2260, Batch Gradient Norm after: 10.13964369618452
Epoch 2261/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5327025413513183
Epoch: 2261, Batch Gradient Norm: 9.286816251614631
Epoch: 2261, Batch Gradient Norm after: 9.286816251614631
Epoch 2262/10000, Prediction Accuracy = 60.874%, Loss = 0.5271661162376404
Epoch: 2262, Batch Gradient Norm: 10.129001789175948
Epoch: 2262, Batch Gradient Norm after: 10.129001789175948
Epoch 2263/10000, Prediction Accuracy = 61.02%, Loss = 0.5324678301811219
Epoch: 2263, Batch Gradient Norm: 8.696181282246812
Epoch: 2263, Batch Gradient Norm after: 8.696181282246812
Epoch 2264/10000, Prediction Accuracy = 60.912%, Loss = 0.5246720910072327
Epoch: 2264, Batch Gradient Norm: 8.728753242304
Epoch: 2264, Batch Gradient Norm after: 8.728753242304
Epoch 2265/10000, Prediction Accuracy = 60.95%, Loss = 0.5256970882415771
Epoch: 2265, Batch Gradient Norm: 8.86368638949745
Epoch: 2265, Batch Gradient Norm after: 8.86368638949745
Epoch 2266/10000, Prediction Accuracy = 60.984%, Loss = 0.5259054660797119
Epoch: 2266, Batch Gradient Norm: 11.259528070048288
Epoch: 2266, Batch Gradient Norm after: 11.259528070048288
Epoch 2267/10000, Prediction Accuracy = 60.94199999999999%, Loss = 0.5404343485832215
Epoch: 2267, Batch Gradient Norm: 13.967365337118958
Epoch: 2267, Batch Gradient Norm after: 13.967365337118958
Epoch 2268/10000, Prediction Accuracy = 60.879999999999995%, Loss = 0.5612098932266235
Epoch: 2268, Batch Gradient Norm: 10.94709602137448
Epoch: 2268, Batch Gradient Norm after: 10.94709602137448
Epoch 2269/10000, Prediction Accuracy = 60.86200000000001%, Loss = 0.5373435258865357
Epoch: 2269, Batch Gradient Norm: 11.484167175103105
Epoch: 2269, Batch Gradient Norm after: 11.484167175103105
Epoch 2270/10000, Prediction Accuracy = 60.891999999999996%, Loss = 0.5416003584861755
Epoch: 2270, Batch Gradient Norm: 11.298121093538423
Epoch: 2270, Batch Gradient Norm after: 11.298121093538423
Epoch 2271/10000, Prediction Accuracy = 60.958000000000006%, Loss = 0.5423235774040223
Epoch: 2271, Batch Gradient Norm: 9.361470226059653
Epoch: 2271, Batch Gradient Norm after: 9.361470226059653
Epoch 2272/10000, Prediction Accuracy = 60.92%, Loss = 0.5284143209457397
Epoch: 2272, Batch Gradient Norm: 12.413688643811327
Epoch: 2272, Batch Gradient Norm after: 12.413688643811327
Epoch 2273/10000, Prediction Accuracy = 61.007999999999996%, Loss = 0.5451078414916992
Epoch: 2273, Batch Gradient Norm: 15.457489721598135
Epoch: 2273, Batch Gradient Norm after: 15.457489721598135
Epoch 2274/10000, Prediction Accuracy = 60.80400000000001%, Loss = 0.5686615943908692
Epoch: 2274, Batch Gradient Norm: 12.248716456367783
Epoch: 2274, Batch Gradient Norm after: 12.248716456367783
Epoch 2275/10000, Prediction Accuracy = 60.914%, Loss = 0.5435383915901184
Epoch: 2275, Batch Gradient Norm: 8.710779132887408
Epoch: 2275, Batch Gradient Norm after: 8.710779132887408
Epoch 2276/10000, Prediction Accuracy = 61.0%, Loss = 0.523426866531372
Epoch: 2276, Batch Gradient Norm: 8.107157024035455
Epoch: 2276, Batch Gradient Norm after: 8.107157024035455
Epoch 2277/10000, Prediction Accuracy = 60.912%, Loss = 0.5199115157127381
Epoch: 2277, Batch Gradient Norm: 9.382169125582637
Epoch: 2277, Batch Gradient Norm after: 9.382169125582637
Epoch 2278/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.526404058933258
Epoch: 2278, Batch Gradient Norm: 11.224161611074921
Epoch: 2278, Batch Gradient Norm after: 11.224161611074921
Epoch 2279/10000, Prediction Accuracy = 60.914%, Loss = 0.5388705015182496
Epoch: 2279, Batch Gradient Norm: 9.407854430927866
Epoch: 2279, Batch Gradient Norm after: 9.407854430927866
Epoch 2280/10000, Prediction Accuracy = 60.924%, Loss = 0.5268926858901978
Epoch: 2280, Batch Gradient Norm: 12.046012199392987
Epoch: 2280, Batch Gradient Norm after: 12.046012199392987
Epoch 2281/10000, Prediction Accuracy = 60.996%, Loss = 0.5426304817199707
Epoch: 2281, Batch Gradient Norm: 12.381751936990703
Epoch: 2281, Batch Gradient Norm after: 12.381751936990703
Epoch 2282/10000, Prediction Accuracy = 60.89%, Loss = 0.5489455699920655
Epoch: 2282, Batch Gradient Norm: 10.800231238447298
Epoch: 2282, Batch Gradient Norm after: 10.800231238447298
Epoch 2283/10000, Prediction Accuracy = 60.95%, Loss = 0.5354514122009277
Epoch: 2283, Batch Gradient Norm: 13.115140005937645
Epoch: 2283, Batch Gradient Norm after: 13.115140005937645
Epoch 2284/10000, Prediction Accuracy = 60.89%, Loss = 0.5512149453163147
Epoch: 2284, Batch Gradient Norm: 12.405485708531225
Epoch: 2284, Batch Gradient Norm after: 12.405485708531225
Epoch 2285/10000, Prediction Accuracy = 60.962%, Loss = 0.5464056611061097
Epoch: 2285, Batch Gradient Norm: 10.714251605550302
Epoch: 2285, Batch Gradient Norm after: 10.714251605550302
Epoch 2286/10000, Prediction Accuracy = 60.976%, Loss = 0.5350605010986328
Epoch: 2286, Batch Gradient Norm: 8.308834230078581
Epoch: 2286, Batch Gradient Norm after: 8.308834230078581
Epoch 2287/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5215748310089111
Epoch: 2287, Batch Gradient Norm: 9.437131972755953
Epoch: 2287, Batch Gradient Norm after: 9.437131972755953
Epoch 2288/10000, Prediction Accuracy = 61.036%, Loss = 0.5274803876876831
Epoch: 2288, Batch Gradient Norm: 9.520187101911532
Epoch: 2288, Batch Gradient Norm after: 9.520187101911532
Epoch 2289/10000, Prediction Accuracy = 60.918000000000006%, Loss = 0.5275422930717468
Epoch: 2289, Batch Gradient Norm: 13.214184413217462
Epoch: 2289, Batch Gradient Norm after: 13.214184413217462
Epoch 2290/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5524223566055297
Epoch: 2290, Batch Gradient Norm: 11.106577431991651
Epoch: 2290, Batch Gradient Norm after: 11.106577431991651
Epoch 2291/10000, Prediction Accuracy = 60.843999999999994%, Loss = 0.5367948412895203
Epoch: 2291, Batch Gradient Norm: 10.24773237895184
Epoch: 2291, Batch Gradient Norm after: 10.24773237895184
Epoch 2292/10000, Prediction Accuracy = 61.038%, Loss = 0.5309820294380188
Epoch: 2292, Batch Gradient Norm: 9.279611069152217
Epoch: 2292, Batch Gradient Norm after: 9.279611069152217
Epoch 2293/10000, Prediction Accuracy = 60.936%, Loss = 0.5249219179153443
Epoch: 2293, Batch Gradient Norm: 11.170804053236504
Epoch: 2293, Batch Gradient Norm after: 11.170804053236504
Epoch 2294/10000, Prediction Accuracy = 60.962%, Loss = 0.5357560753822327
Epoch: 2294, Batch Gradient Norm: 13.390303050821466
Epoch: 2294, Batch Gradient Norm after: 13.390303050821466
Epoch 2295/10000, Prediction Accuracy = 60.924%, Loss = 0.5516493201255799
Epoch: 2295, Batch Gradient Norm: 12.221930538956459
Epoch: 2295, Batch Gradient Norm after: 12.221930538956459
Epoch 2296/10000, Prediction Accuracy = 60.982000000000006%, Loss = 0.54342622756958
Epoch: 2296, Batch Gradient Norm: 9.325031308012319
Epoch: 2296, Batch Gradient Norm after: 9.325031308012319
Epoch 2297/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.5254743576049805
Epoch: 2297, Batch Gradient Norm: 9.280862381577696
Epoch: 2297, Batch Gradient Norm after: 9.280862381577696
Epoch 2298/10000, Prediction Accuracy = 61.016%, Loss = 0.5257616519927979
Epoch: 2298, Batch Gradient Norm: 9.922640044316315
Epoch: 2298, Batch Gradient Norm after: 9.922640044316315
Epoch 2299/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.5329689979553223
Epoch: 2299, Batch Gradient Norm: 8.725108463725011
Epoch: 2299, Batch Gradient Norm after: 8.725108463725011
Epoch 2300/10000, Prediction Accuracy = 61.016%, Loss = 0.5244146227836609
Epoch: 2300, Batch Gradient Norm: 9.104586809429358
Epoch: 2300, Batch Gradient Norm after: 9.104586809429358
Epoch 2301/10000, Prediction Accuracy = 60.96600000000001%, Loss = 0.5256659626960755
Epoch: 2301, Batch Gradient Norm: 10.252262951874286
Epoch: 2301, Batch Gradient Norm after: 10.252262951874286
Epoch 2302/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.5327617168426514
Epoch: 2302, Batch Gradient Norm: 9.43167588921296
Epoch: 2302, Batch Gradient Norm after: 9.43167588921296
Epoch 2303/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5260404706001282
Epoch: 2303, Batch Gradient Norm: 13.216076423149028
Epoch: 2303, Batch Gradient Norm after: 13.216076423149028
Epoch 2304/10000, Prediction Accuracy = 60.916%, Loss = 0.5519941687583924
Epoch: 2304, Batch Gradient Norm: 13.58578982044875
Epoch: 2304, Batch Gradient Norm after: 13.58578982044875
Epoch 2305/10000, Prediction Accuracy = 60.902%, Loss = 0.5557980537414551
Epoch: 2305, Batch Gradient Norm: 11.613778506143653
Epoch: 2305, Batch Gradient Norm after: 11.613778506143653
Epoch 2306/10000, Prediction Accuracy = 60.896%, Loss = 0.5401195645332336
Epoch: 2306, Batch Gradient Norm: 12.181514129569372
Epoch: 2306, Batch Gradient Norm after: 12.181514129569372
Epoch 2307/10000, Prediction Accuracy = 60.94000000000001%, Loss = 0.5467729806900025
Epoch: 2307, Batch Gradient Norm: 10.661360124834154
Epoch: 2307, Batch Gradient Norm after: 10.661360124834154
Epoch 2308/10000, Prediction Accuracy = 60.912%, Loss = 0.5342526078224182
Epoch: 2308, Batch Gradient Norm: 10.571411403002056
Epoch: 2308, Batch Gradient Norm after: 10.571411403002056
Epoch 2309/10000, Prediction Accuracy = 60.964%, Loss = 0.5325787663459778
Epoch: 2309, Batch Gradient Norm: 11.421072276733106
Epoch: 2309, Batch Gradient Norm after: 11.421072276733106
Epoch 2310/10000, Prediction Accuracy = 60.902%, Loss = 0.5382912158966064
Epoch: 2310, Batch Gradient Norm: 12.610695943327224
Epoch: 2310, Batch Gradient Norm after: 12.610695943327224
Epoch 2311/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.5439970135688782
Epoch: 2311, Batch Gradient Norm: 15.228058782585597
Epoch: 2311, Batch Gradient Norm after: 15.228058782585597
Epoch 2312/10000, Prediction Accuracy = 60.962%, Loss = 0.566134238243103
Epoch: 2312, Batch Gradient Norm: 10.469674273576278
Epoch: 2312, Batch Gradient Norm after: 10.469674273576278
Epoch 2313/10000, Prediction Accuracy = 60.934000000000005%, Loss = 0.5315643787384033
Epoch: 2313, Batch Gradient Norm: 6.256684624664437
Epoch: 2313, Batch Gradient Norm after: 6.256684624664437
Epoch 2314/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.510483694076538
Epoch: 2314, Batch Gradient Norm: 7.4687836730171675
Epoch: 2314, Batch Gradient Norm after: 7.4687836730171675
Epoch 2315/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.515632039308548
Epoch: 2315, Batch Gradient Norm: 9.372943236821808
Epoch: 2315, Batch Gradient Norm after: 9.372943236821808
Epoch 2316/10000, Prediction Accuracy = 60.944%, Loss = 0.5269692659378051
Epoch: 2316, Batch Gradient Norm: 12.136028900385885
Epoch: 2316, Batch Gradient Norm after: 12.136028900385885
Epoch 2317/10000, Prediction Accuracy = 60.895999999999994%, Loss = 0.5469152927398682
Epoch: 2317, Batch Gradient Norm: 12.70778547037242
Epoch: 2317, Batch Gradient Norm after: 12.70778547037242
Epoch 2318/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5486364841461182
Epoch: 2318, Batch Gradient Norm: 10.11486689345883
Epoch: 2318, Batch Gradient Norm after: 10.11486689345883
Epoch 2319/10000, Prediction Accuracy = 60.88599999999999%, Loss = 0.5297505140304566
Epoch: 2319, Batch Gradient Norm: 11.341019068309244
Epoch: 2319, Batch Gradient Norm after: 11.341019068309244
Epoch 2320/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5376327633857727
Epoch: 2320, Batch Gradient Norm: 11.69095396902647
Epoch: 2320, Batch Gradient Norm after: 11.69095396902647
Epoch 2321/10000, Prediction Accuracy = 60.92999999999999%, Loss = 0.542296040058136
Epoch: 2321, Batch Gradient Norm: 10.421306065458015
Epoch: 2321, Batch Gradient Norm after: 10.421306065458015
Epoch 2322/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.5336016774177551
Epoch: 2322, Batch Gradient Norm: 8.979191713296066
Epoch: 2322, Batch Gradient Norm after: 8.979191713296066
Epoch 2323/10000, Prediction Accuracy = 60.943999999999996%, Loss = 0.5222525715827941
Epoch: 2323, Batch Gradient Norm: 11.758964302417159
Epoch: 2323, Batch Gradient Norm after: 11.758964302417159
Epoch 2324/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.5376337885856628
Epoch: 2324, Batch Gradient Norm: 13.174409254679404
Epoch: 2324, Batch Gradient Norm after: 13.174409254679404
Epoch 2325/10000, Prediction Accuracy = 60.87199999999999%, Loss = 0.5473400592803955
Epoch: 2325, Batch Gradient Norm: 11.933885255842952
Epoch: 2325, Batch Gradient Norm after: 11.933885255842952
Epoch 2326/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.539752459526062
Epoch: 2326, Batch Gradient Norm: 9.29573092546381
Epoch: 2326, Batch Gradient Norm after: 9.29573092546381
Epoch 2327/10000, Prediction Accuracy = 60.99000000000001%, Loss = 0.5236873388290405
Epoch: 2327, Batch Gradient Norm: 8.807893756368353
Epoch: 2327, Batch Gradient Norm after: 8.807893756368353
Epoch 2328/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.5216691732406616
Epoch: 2328, Batch Gradient Norm: 11.329814204762434
Epoch: 2328, Batch Gradient Norm after: 11.329814204762434
Epoch 2329/10000, Prediction Accuracy = 61.05800000000001%, Loss = 0.5373862981796265
Epoch: 2329, Batch Gradient Norm: 12.14484407079635
Epoch: 2329, Batch Gradient Norm after: 12.14484407079635
Epoch 2330/10000, Prediction Accuracy = 60.98%, Loss = 0.542593264579773
Epoch: 2330, Batch Gradient Norm: 12.162644760586575
Epoch: 2330, Batch Gradient Norm after: 12.162644760586575
Epoch 2331/10000, Prediction Accuracy = 60.972%, Loss = 0.5424002408981323
Epoch: 2331, Batch Gradient Norm: 11.101619840186313
Epoch: 2331, Batch Gradient Norm after: 11.101619840186313
Epoch 2332/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5342350721359252
Epoch: 2332, Batch Gradient Norm: 10.550221651485167
Epoch: 2332, Batch Gradient Norm after: 10.550221651485167
Epoch 2333/10000, Prediction Accuracy = 60.944%, Loss = 0.5307030200958252
Epoch: 2333, Batch Gradient Norm: 10.664215035740842
Epoch: 2333, Batch Gradient Norm after: 10.664215035740842
Epoch 2334/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.5312647700309754
Epoch: 2334, Batch Gradient Norm: 10.299316602835834
Epoch: 2334, Batch Gradient Norm after: 10.299316602835834
Epoch 2335/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5287439823150635
Epoch: 2335, Batch Gradient Norm: 11.137678456937222
Epoch: 2335, Batch Gradient Norm after: 11.137678456937222
Epoch 2336/10000, Prediction Accuracy = 60.964%, Loss = 0.5336119294166565
Epoch: 2336, Batch Gradient Norm: 11.986113833107717
Epoch: 2336, Batch Gradient Norm after: 11.986113833107717
Epoch 2337/10000, Prediction Accuracy = 60.976%, Loss = 0.5394618391990662
Epoch: 2337, Batch Gradient Norm: 11.77006486831836
Epoch: 2337, Batch Gradient Norm after: 11.77006486831836
Epoch 2338/10000, Prediction Accuracy = 60.96%, Loss = 0.5387355804443359
Epoch: 2338, Batch Gradient Norm: 11.064762042738844
Epoch: 2338, Batch Gradient Norm after: 11.064762042738844
Epoch 2339/10000, Prediction Accuracy = 61.00999999999999%, Loss = 0.535306453704834
Epoch: 2339, Batch Gradient Norm: 11.089426748919111
Epoch: 2339, Batch Gradient Norm after: 11.089426748919111
Epoch 2340/10000, Prediction Accuracy = 60.96%, Loss = 0.5365590691566468
Epoch: 2340, Batch Gradient Norm: 10.970590766245902
Epoch: 2340, Batch Gradient Norm after: 10.970590766245902
Epoch 2341/10000, Prediction Accuracy = 61.016%, Loss = 0.5367138862609864
Epoch: 2341, Batch Gradient Norm: 11.345809140912426
Epoch: 2341, Batch Gradient Norm after: 11.345809140912426
Epoch 2342/10000, Prediction Accuracy = 60.898%, Loss = 0.536836040019989
Epoch: 2342, Batch Gradient Norm: 10.82648945692626
Epoch: 2342, Batch Gradient Norm after: 10.82648945692626
Epoch 2343/10000, Prediction Accuracy = 60.989999999999995%, Loss = 0.5329866170883178
Epoch: 2343, Batch Gradient Norm: 9.671927254294696
Epoch: 2343, Batch Gradient Norm after: 9.671927254294696
Epoch 2344/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5259572505950928
Epoch: 2344, Batch Gradient Norm: 8.744946497856196
Epoch: 2344, Batch Gradient Norm after: 8.744946497856196
Epoch 2345/10000, Prediction Accuracy = 60.914%, Loss = 0.5215358734130859
Epoch: 2345, Batch Gradient Norm: 11.017632528582771
Epoch: 2345, Batch Gradient Norm after: 11.017632528582771
Epoch 2346/10000, Prediction Accuracy = 60.886%, Loss = 0.5381586790084839
Epoch: 2346, Batch Gradient Norm: 8.911277692982273
Epoch: 2346, Batch Gradient Norm after: 8.911277692982273
Epoch 2347/10000, Prediction Accuracy = 61.044000000000004%, Loss = 0.5231980800628662
Epoch: 2347, Batch Gradient Norm: 9.663184877187122
Epoch: 2347, Batch Gradient Norm after: 9.663184877187122
Epoch 2348/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5269070506095886
Epoch: 2348, Batch Gradient Norm: 11.853665613397983
Epoch: 2348, Batch Gradient Norm after: 11.853665613397983
Epoch 2349/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.5422345757484436
Epoch: 2349, Batch Gradient Norm: 11.275670346298853
Epoch: 2349, Batch Gradient Norm after: 11.275670346298853
Epoch 2350/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5369685769081116
Epoch: 2350, Batch Gradient Norm: 13.448566383695484
Epoch: 2350, Batch Gradient Norm after: 13.448566383695484
Epoch 2351/10000, Prediction Accuracy = 61.056%, Loss = 0.5521188497543335
Epoch: 2351, Batch Gradient Norm: 10.76425345624532
Epoch: 2351, Batch Gradient Norm after: 10.76425345624532
Epoch 2352/10000, Prediction Accuracy = 60.898%, Loss = 0.5318579077720642
Epoch: 2352, Batch Gradient Norm: 11.551614849728281
Epoch: 2352, Batch Gradient Norm after: 11.551614849728281
Epoch 2353/10000, Prediction Accuracy = 61.098%, Loss = 0.5353077530860901
Epoch: 2353, Batch Gradient Norm: 13.78931087452622
Epoch: 2353, Batch Gradient Norm after: 13.78931087452622
Epoch 2354/10000, Prediction Accuracy = 60.85600000000001%, Loss = 0.5514241218566894
Epoch: 2354, Batch Gradient Norm: 11.947106881178858
Epoch: 2354, Batch Gradient Norm after: 11.947106881178858
Epoch 2355/10000, Prediction Accuracy = 61.028%, Loss = 0.5383358597755432
Epoch: 2355, Batch Gradient Norm: 9.251554323720534
Epoch: 2355, Batch Gradient Norm after: 9.251554323720534
Epoch 2356/10000, Prediction Accuracy = 60.92%, Loss = 0.5215329527854919
Epoch: 2356, Batch Gradient Norm: 9.958743228397072
Epoch: 2356, Batch Gradient Norm after: 9.958743228397072
Epoch 2357/10000, Prediction Accuracy = 61.041999999999994%, Loss = 0.5251990795135498
Epoch: 2357, Batch Gradient Norm: 12.370371630370988
Epoch: 2357, Batch Gradient Norm after: 12.370371630370988
Epoch 2358/10000, Prediction Accuracy = 60.964%, Loss = 0.5438671469688415
Epoch: 2358, Batch Gradient Norm: 11.168036649045103
Epoch: 2358, Batch Gradient Norm after: 11.168036649045103
Epoch 2359/10000, Prediction Accuracy = 61.056%, Loss = 0.5360160827636719
Epoch: 2359, Batch Gradient Norm: 8.092719674808631
Epoch: 2359, Batch Gradient Norm after: 8.092719674808631
Epoch 2360/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5170267343521118
Epoch: 2360, Batch Gradient Norm: 8.797530022849436
Epoch: 2360, Batch Gradient Norm after: 8.797530022849436
Epoch 2361/10000, Prediction Accuracy = 61.016000000000005%, Loss = 0.51961430311203
Epoch: 2361, Batch Gradient Norm: 11.204648742498968
Epoch: 2361, Batch Gradient Norm after: 11.204648742498968
Epoch 2362/10000, Prediction Accuracy = 61.06%, Loss = 0.5326531291007995
Epoch: 2362, Batch Gradient Norm: 14.322723471063988
Epoch: 2362, Batch Gradient Norm after: 14.322723471063988
Epoch 2363/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.556851613521576
Epoch: 2363, Batch Gradient Norm: 12.22518090656685
Epoch: 2363, Batch Gradient Norm after: 12.22518090656685
Epoch 2364/10000, Prediction Accuracy = 60.992%, Loss = 0.5405118107795716
Epoch: 2364, Batch Gradient Norm: 10.274635914757322
Epoch: 2364, Batch Gradient Norm after: 10.274635914757322
Epoch 2365/10000, Prediction Accuracy = 60.98%, Loss = 0.5281539916992187
Epoch: 2365, Batch Gradient Norm: 9.398652243282955
Epoch: 2365, Batch Gradient Norm after: 9.398652243282955
Epoch 2366/10000, Prediction Accuracy = 61.007999999999996%, Loss = 0.5233106970787048
Epoch: 2366, Batch Gradient Norm: 9.77973251292327
Epoch: 2366, Batch Gradient Norm after: 9.77973251292327
Epoch 2367/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5279939413070679
Epoch: 2367, Batch Gradient Norm: 9.082812992826739
Epoch: 2367, Batch Gradient Norm after: 9.082812992826739
Epoch 2368/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5224480092525482
Epoch: 2368, Batch Gradient Norm: 11.113905183453026
Epoch: 2368, Batch Gradient Norm after: 11.113905183453026
Epoch 2369/10000, Prediction Accuracy = 60.936%, Loss = 0.5334506750106811
Epoch: 2369, Batch Gradient Norm: 13.336259896399659
Epoch: 2369, Batch Gradient Norm after: 13.336259896399659
Epoch 2370/10000, Prediction Accuracy = 61.008%, Loss = 0.5515790462493897
Epoch: 2370, Batch Gradient Norm: 9.34504464978329
Epoch: 2370, Batch Gradient Norm after: 9.34504464978329
Epoch 2371/10000, Prediction Accuracy = 60.988%, Loss = 0.5222105324268341
Epoch: 2371, Batch Gradient Norm: 9.463058492991571
Epoch: 2371, Batch Gradient Norm after: 9.463058492991571
Epoch 2372/10000, Prediction Accuracy = 60.995999999999995%, Loss = 0.5219457268714904
Epoch: 2372, Batch Gradient Norm: 12.008810313688983
Epoch: 2372, Batch Gradient Norm after: 12.008810313688983
Epoch 2373/10000, Prediction Accuracy = 60.92%, Loss = 0.5387110471725464
Epoch: 2373, Batch Gradient Norm: 10.946786685238934
Epoch: 2373, Batch Gradient Norm after: 10.946786685238934
Epoch 2374/10000, Prediction Accuracy = 60.982000000000006%, Loss = 0.5321263194084167
Epoch: 2374, Batch Gradient Norm: 10.617984329074016
Epoch: 2374, Batch Gradient Norm after: 10.617984329074016
Epoch 2375/10000, Prediction Accuracy = 61.02%, Loss = 0.5298454642295838
Epoch: 2375, Batch Gradient Norm: 10.506168235285266
Epoch: 2375, Batch Gradient Norm after: 10.506168235285266
Epoch 2376/10000, Prediction Accuracy = 60.952%, Loss = 0.5287041664123535
Epoch: 2376, Batch Gradient Norm: 12.129576571980571
Epoch: 2376, Batch Gradient Norm after: 12.129576571980571
Epoch 2377/10000, Prediction Accuracy = 61.04%, Loss = 0.5410302162170411
Epoch: 2377, Batch Gradient Norm: 11.211213498162643
Epoch: 2377, Batch Gradient Norm after: 11.211213498162643
Epoch 2378/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5364560961723328
Epoch: 2378, Batch Gradient Norm: 8.556647963623332
Epoch: 2378, Batch Gradient Norm after: 8.556647963623332
Epoch 2379/10000, Prediction Accuracy = 60.99399999999999%, Loss = 0.5186920404434204
Epoch: 2379, Batch Gradient Norm: 11.296345722898339
Epoch: 2379, Batch Gradient Norm after: 11.296345722898339
Epoch 2380/10000, Prediction Accuracy = 61.024%, Loss = 0.5342887997627258
Epoch: 2380, Batch Gradient Norm: 13.479345912987226
Epoch: 2380, Batch Gradient Norm after: 13.479345912987226
Epoch 2381/10000, Prediction Accuracy = 60.886%, Loss = 0.5502969741821289
Epoch: 2381, Batch Gradient Norm: 12.136507091003827
Epoch: 2381, Batch Gradient Norm after: 12.136507091003827
Epoch 2382/10000, Prediction Accuracy = 61.024%, Loss = 0.5401079893112183
Epoch: 2382, Batch Gradient Norm: 11.74864654583295
Epoch: 2382, Batch Gradient Norm after: 11.74864654583295
Epoch 2383/10000, Prediction Accuracy = 60.91799999999999%, Loss = 0.5359736323356629
Epoch: 2383, Batch Gradient Norm: 12.74140091069596
Epoch: 2383, Batch Gradient Norm after: 12.74140091069596
Epoch 2384/10000, Prediction Accuracy = 61.028%, Loss = 0.5441479086875916
Epoch: 2384, Batch Gradient Norm: 10.828219190198219
Epoch: 2384, Batch Gradient Norm after: 10.828219190198219
Epoch 2385/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5335730195045472
Epoch: 2385, Batch Gradient Norm: 8.571305038357877
Epoch: 2385, Batch Gradient Norm after: 8.571305038357877
Epoch 2386/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5190161764621735
Epoch: 2386, Batch Gradient Norm: 7.3269639198078735
Epoch: 2386, Batch Gradient Norm after: 7.3269639198078735
Epoch 2387/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.511927455663681
Epoch: 2387, Batch Gradient Norm: 11.133737257111283
Epoch: 2387, Batch Gradient Norm after: 11.133737257111283
Epoch 2388/10000, Prediction Accuracy = 61.08%, Loss = 0.5324447393417359
Epoch: 2388, Batch Gradient Norm: 13.016619505422554
Epoch: 2388, Batch Gradient Norm after: 13.016619505422554
Epoch 2389/10000, Prediction Accuracy = 60.936%, Loss = 0.5463602304458618
Epoch: 2389, Batch Gradient Norm: 11.289041836468508
Epoch: 2389, Batch Gradient Norm after: 11.289041836468508
Epoch 2390/10000, Prediction Accuracy = 60.96%, Loss = 0.5343093276023865
Epoch: 2390, Batch Gradient Norm: 9.51544380514554
Epoch: 2390, Batch Gradient Norm after: 9.51544380514554
Epoch 2391/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.5224892735481262
Epoch: 2391, Batch Gradient Norm: 9.598279053663214
Epoch: 2391, Batch Gradient Norm after: 9.598279053663214
Epoch 2392/10000, Prediction Accuracy = 60.962%, Loss = 0.5221895813941956
Epoch: 2392, Batch Gradient Norm: 11.844719873519638
Epoch: 2392, Batch Gradient Norm after: 11.844719873519638
Epoch 2393/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.53568434715271
Epoch: 2393, Batch Gradient Norm: 11.649156047536128
Epoch: 2393, Batch Gradient Norm after: 11.649156047536128
Epoch 2394/10000, Prediction Accuracy = 60.996%, Loss = 0.53607679605484
Epoch: 2394, Batch Gradient Norm: 10.233902084787196
Epoch: 2394, Batch Gradient Norm after: 10.233902084787196
Epoch 2395/10000, Prediction Accuracy = 61.08200000000001%, Loss = 0.5282235860824585
Epoch: 2395, Batch Gradient Norm: 8.843355476384108
Epoch: 2395, Batch Gradient Norm after: 8.843355476384108
Epoch 2396/10000, Prediction Accuracy = 61.040000000000006%, Loss = 0.5194689273834229
Epoch: 2396, Batch Gradient Norm: 11.787162765372393
Epoch: 2396, Batch Gradient Norm after: 11.787162765372393
Epoch 2397/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5368839263916015
Epoch: 2397, Batch Gradient Norm: 13.089049864552292
Epoch: 2397, Batch Gradient Norm after: 13.089049864552292
Epoch 2398/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.545513391494751
Epoch: 2398, Batch Gradient Norm: 12.622211207726277
Epoch: 2398, Batch Gradient Norm after: 12.622211207726277
Epoch 2399/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5426836013793945
Epoch: 2399, Batch Gradient Norm: 9.153747140157472
Epoch: 2399, Batch Gradient Norm after: 9.153747140157472
Epoch 2400/10000, Prediction Accuracy = 60.94%, Loss = 0.5204051494598388
Epoch: 2400, Batch Gradient Norm: 8.680683549048466
Epoch: 2400, Batch Gradient Norm after: 8.680683549048466
Epoch 2401/10000, Prediction Accuracy = 61.02%, Loss = 0.5182387292385101
Epoch: 2401, Batch Gradient Norm: 10.78246219744763
Epoch: 2401, Batch Gradient Norm after: 10.78246219744763
Epoch 2402/10000, Prediction Accuracy = 60.882000000000005%, Loss = 0.5336334466934204
Epoch: 2402, Batch Gradient Norm: 8.215110968782795
Epoch: 2402, Batch Gradient Norm after: 8.215110968782795
Epoch 2403/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.5182680666446686
Epoch: 2403, Batch Gradient Norm: 9.236960853591444
Epoch: 2403, Batch Gradient Norm after: 9.236960853591444
Epoch 2404/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.5223028898239136
Epoch: 2404, Batch Gradient Norm: 12.21860526588243
Epoch: 2404, Batch Gradient Norm after: 12.21860526588243
Epoch 2405/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.5385087490081787
Epoch: 2405, Batch Gradient Norm: 13.332486178209882
Epoch: 2405, Batch Gradient Norm after: 13.332486178209882
Epoch 2406/10000, Prediction Accuracy = 60.931999999999995%, Loss = 0.5450446724891662
Epoch: 2406, Batch Gradient Norm: 12.73725119459861
Epoch: 2406, Batch Gradient Norm after: 12.73725119459861
Epoch 2407/10000, Prediction Accuracy = 61.053999999999995%, Loss = 0.5415539383888245
Epoch: 2407, Batch Gradient Norm: 10.815080382779477
Epoch: 2407, Batch Gradient Norm after: 10.815080382779477
Epoch 2408/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.5287163257598877
Epoch: 2408, Batch Gradient Norm: 10.92366340457742
Epoch: 2408, Batch Gradient Norm after: 10.92366340457742
Epoch 2409/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.529999041557312
Epoch: 2409, Batch Gradient Norm: 11.922112870779197
Epoch: 2409, Batch Gradient Norm after: 11.922112870779197
Epoch 2410/10000, Prediction Accuracy = 61.024%, Loss = 0.53951655626297
Epoch: 2410, Batch Gradient Norm: 9.43861441651256
Epoch: 2410, Batch Gradient Norm after: 9.43861441651256
Epoch 2411/10000, Prediction Accuracy = 60.996%, Loss = 0.5238266825675965
Epoch: 2411, Batch Gradient Norm: 9.92100746821776
Epoch: 2411, Batch Gradient Norm after: 9.92100746821776
Epoch 2412/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.5251129627227783
Epoch: 2412, Batch Gradient Norm: 11.204219019591289
Epoch: 2412, Batch Gradient Norm after: 11.204219019591289
Epoch 2413/10000, Prediction Accuracy = 60.943999999999996%, Loss = 0.5320145726203919
Epoch: 2413, Batch Gradient Norm: 11.595839282478769
Epoch: 2413, Batch Gradient Norm after: 11.595839282478769
Epoch 2414/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.5348083972930908
Epoch: 2414, Batch Gradient Norm: 9.710966737001618
Epoch: 2414, Batch Gradient Norm after: 9.710966737001618
Epoch 2415/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.5221192002296448
Epoch: 2415, Batch Gradient Norm: 8.705194313083668
Epoch: 2415, Batch Gradient Norm after: 8.705194313083668
Epoch 2416/10000, Prediction Accuracy = 61.008%, Loss = 0.5164664626121521
Epoch: 2416, Batch Gradient Norm: 10.001001566622659
Epoch: 2416, Batch Gradient Norm after: 10.001001566622659
Epoch 2417/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5235640645027161
Epoch: 2417, Batch Gradient Norm: 8.949483327711809
Epoch: 2417, Batch Gradient Norm after: 8.949483327711809
Epoch 2418/10000, Prediction Accuracy = 60.96999999999999%, Loss = 0.5179819822311401
Epoch: 2418, Batch Gradient Norm: 10.668882182216526
Epoch: 2418, Batch Gradient Norm after: 10.668882182216526
Epoch 2419/10000, Prediction Accuracy = 61.068000000000005%, Loss = 0.5284637212753296
Epoch: 2419, Batch Gradient Norm: 10.702494366387276
Epoch: 2419, Batch Gradient Norm after: 10.702494366387276
Epoch 2420/10000, Prediction Accuracy = 60.992000000000004%, Loss = 0.5284038424491883
Epoch: 2420, Batch Gradient Norm: 13.045878033965094
Epoch: 2420, Batch Gradient Norm after: 13.045878033965094
Epoch 2421/10000, Prediction Accuracy = 61.038%, Loss = 0.5482728362083436
Epoch: 2421, Batch Gradient Norm: 11.641974512080186
Epoch: 2421, Batch Gradient Norm after: 11.641974512080186
Epoch 2422/10000, Prediction Accuracy = 60.938%, Loss = 0.5344576716423035
Epoch: 2422, Batch Gradient Norm: 12.747349476112769
Epoch: 2422, Batch Gradient Norm after: 12.747349476112769
Epoch 2423/10000, Prediction Accuracy = 61.05799999999999%, Loss = 0.5412294268608093
Epoch: 2423, Batch Gradient Norm: 12.768301441270086
Epoch: 2423, Batch Gradient Norm after: 12.768301441270086
Epoch 2424/10000, Prediction Accuracy = 60.95%, Loss = 0.5412380218505859
Epoch: 2424, Batch Gradient Norm: 13.834440550617781
Epoch: 2424, Batch Gradient Norm after: 13.834440550617781
Epoch 2425/10000, Prediction Accuracy = 60.992%, Loss = 0.5499947309494019
Epoch: 2425, Batch Gradient Norm: 12.63649878643698
Epoch: 2425, Batch Gradient Norm after: 12.63649878643698
Epoch 2426/10000, Prediction Accuracy = 61.044%, Loss = 0.542592990398407
Epoch: 2426, Batch Gradient Norm: 8.735234409328577
Epoch: 2426, Batch Gradient Norm after: 8.735234409328577
Epoch 2427/10000, Prediction Accuracy = 61.008%, Loss = 0.5170414686203003
Epoch: 2427, Batch Gradient Norm: 7.030512976689762
Epoch: 2427, Batch Gradient Norm after: 7.030512976689762
Epoch 2428/10000, Prediction Accuracy = 61.04%, Loss = 0.5084036767482758
Epoch: 2428, Batch Gradient Norm: 7.882800689778834
Epoch: 2428, Batch Gradient Norm after: 7.882800689778834
Epoch 2429/10000, Prediction Accuracy = 61.074%, Loss = 0.5118177473545075
Epoch: 2429, Batch Gradient Norm: 11.066699256535491
Epoch: 2429, Batch Gradient Norm after: 11.066699256535491
Epoch 2430/10000, Prediction Accuracy = 60.976%, Loss = 0.5339992046356201
Epoch: 2430, Batch Gradient Norm: 9.034841861288365
Epoch: 2430, Batch Gradient Norm after: 9.034841861288365
Epoch 2431/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.521270215511322
Epoch: 2431, Batch Gradient Norm: 11.729061385303023
Epoch: 2431, Batch Gradient Norm after: 11.729061385303023
Epoch 2432/10000, Prediction Accuracy = 60.934000000000005%, Loss = 0.5360546112060547
Epoch: 2432, Batch Gradient Norm: 13.723998216426477
Epoch: 2432, Batch Gradient Norm after: 13.723998216426477
Epoch 2433/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5543423891067505
Epoch: 2433, Batch Gradient Norm: 7.69512725530381
Epoch: 2433, Batch Gradient Norm after: 7.69512725530381
Epoch 2434/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.512594872713089
Epoch: 2434, Batch Gradient Norm: 7.962188156344283
Epoch: 2434, Batch Gradient Norm after: 7.962188156344283
Epoch 2435/10000, Prediction Accuracy = 61.038%, Loss = 0.5124967813491821
Epoch: 2435, Batch Gradient Norm: 13.496452893841578
Epoch: 2435, Batch Gradient Norm after: 13.496452893841578
Epoch 2436/10000, Prediction Accuracy = 61.03800000000001%, Loss = 0.5496074318885803
Epoch: 2436, Batch Gradient Norm: 12.78003096867155
Epoch: 2436, Batch Gradient Norm after: 12.78003096867155
Epoch 2437/10000, Prediction Accuracy = 61.016%, Loss = 0.5439160466194153
Epoch: 2437, Batch Gradient Norm: 9.579596546297847
Epoch: 2437, Batch Gradient Norm after: 9.579596546297847
Epoch 2438/10000, Prediction Accuracy = 60.952%, Loss = 0.5207296013832092
Epoch: 2438, Batch Gradient Norm: 9.418141430532781
Epoch: 2438, Batch Gradient Norm after: 9.418141430532781
Epoch 2439/10000, Prediction Accuracy = 61.05400000000001%, Loss = 0.51911461353302
Epoch: 2439, Batch Gradient Norm: 9.674023952615523
Epoch: 2439, Batch Gradient Norm after: 9.674023952615523
Epoch 2440/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.520111870765686
Epoch: 2440, Batch Gradient Norm: 12.315795887308886
Epoch: 2440, Batch Gradient Norm after: 12.315795887308886
Epoch 2441/10000, Prediction Accuracy = 60.94799999999999%, Loss = 0.5393667101860047
Epoch: 2441, Batch Gradient Norm: 12.04189688879748
Epoch: 2441, Batch Gradient Norm after: 12.04189688879748
Epoch 2442/10000, Prediction Accuracy = 60.972%, Loss = 0.5354358673095703
Epoch: 2442, Batch Gradient Norm: 13.742294284120549
Epoch: 2442, Batch Gradient Norm after: 13.742294284120549
Epoch 2443/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.548409104347229
Epoch: 2443, Batch Gradient Norm: 11.966618424267644
Epoch: 2443, Batch Gradient Norm after: 11.966618424267644
Epoch 2444/10000, Prediction Accuracy = 60.914%, Loss = 0.5366165161132812
Epoch: 2444, Batch Gradient Norm: 8.976122786438399
Epoch: 2444, Batch Gradient Norm after: 8.976122786438399
Epoch 2445/10000, Prediction Accuracy = 61.06%, Loss = 0.5182304441928863
Epoch: 2445, Batch Gradient Norm: 10.069449790379558
Epoch: 2445, Batch Gradient Norm after: 10.069449790379558
Epoch 2446/10000, Prediction Accuracy = 61.1%, Loss = 0.5248472929000855
Epoch: 2446, Batch Gradient Norm: 10.277686853301566
Epoch: 2446, Batch Gradient Norm after: 10.277686853301566
Epoch 2447/10000, Prediction Accuracy = 61.007999999999996%, Loss = 0.5252207756042481
Epoch: 2447, Batch Gradient Norm: 11.923371451737946
Epoch: 2447, Batch Gradient Norm after: 11.923371451737946
Epoch 2448/10000, Prediction Accuracy = 61.074%, Loss = 0.5344220519065856
Epoch: 2448, Batch Gradient Norm: 12.114807090828215
Epoch: 2448, Batch Gradient Norm after: 12.114807090828215
Epoch 2449/10000, Prediction Accuracy = 60.948%, Loss = 0.5358580946922302
Epoch: 2449, Batch Gradient Norm: 11.180163825509412
Epoch: 2449, Batch Gradient Norm after: 11.180163825509412
Epoch 2450/10000, Prediction Accuracy = 60.989999999999995%, Loss = 0.5292353391647339
Epoch: 2450, Batch Gradient Norm: 11.137090302045321
Epoch: 2450, Batch Gradient Norm after: 11.137090302045321
Epoch 2451/10000, Prediction Accuracy = 61.08200000000001%, Loss = 0.5308034062385559
Epoch: 2451, Batch Gradient Norm: 9.443977519221233
Epoch: 2451, Batch Gradient Norm after: 9.443977519221233
Epoch 2452/10000, Prediction Accuracy = 60.946000000000005%, Loss = 0.5202308535575867
Epoch: 2452, Batch Gradient Norm: 10.170564664081416
Epoch: 2452, Batch Gradient Norm after: 10.170564664081416
Epoch 2453/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5249604701995849
Epoch: 2453, Batch Gradient Norm: 8.812687560571751
Epoch: 2453, Batch Gradient Norm after: 8.812687560571751
Epoch 2454/10000, Prediction Accuracy = 60.910000000000004%, Loss = 0.5167298913002014
Epoch: 2454, Batch Gradient Norm: 10.49345124600972
Epoch: 2454, Batch Gradient Norm after: 10.49345124600972
Epoch 2455/10000, Prediction Accuracy = 61.06600000000001%, Loss = 0.5258837223052979
Epoch: 2455, Batch Gradient Norm: 10.910825683493284
Epoch: 2455, Batch Gradient Norm after: 10.910825683493284
Epoch 2456/10000, Prediction Accuracy = 60.902%, Loss = 0.5271615624427796
Epoch: 2456, Batch Gradient Norm: 12.400871090832213
Epoch: 2456, Batch Gradient Norm after: 12.400871090832213
Epoch 2457/10000, Prediction Accuracy = 60.992000000000004%, Loss = 0.5373386859893798
Epoch: 2457, Batch Gradient Norm: 12.132768442360298
Epoch: 2457, Batch Gradient Norm after: 12.132768442360298
Epoch 2458/10000, Prediction Accuracy = 60.914%, Loss = 0.5366900444030762
Epoch: 2458, Batch Gradient Norm: 11.092637030395046
Epoch: 2458, Batch Gradient Norm after: 11.092637030395046
Epoch 2459/10000, Prediction Accuracy = 60.955999999999996%, Loss = 0.5312257051467896
Epoch: 2459, Batch Gradient Norm: 10.24093540571033
Epoch: 2459, Batch Gradient Norm after: 10.24093540571033
Epoch 2460/10000, Prediction Accuracy = 60.95%, Loss = 0.5273947715759277
Epoch: 2460, Batch Gradient Norm: 9.09213869325163
Epoch: 2460, Batch Gradient Norm after: 9.09213869325163
Epoch 2461/10000, Prediction Accuracy = 61.062%, Loss = 0.5186275541782379
Epoch: 2461, Batch Gradient Norm: 8.897838309557718
Epoch: 2461, Batch Gradient Norm after: 8.897838309557718
Epoch 2462/10000, Prediction Accuracy = 61.044%, Loss = 0.5173202574253082
Epoch: 2462, Batch Gradient Norm: 10.294236889475668
Epoch: 2462, Batch Gradient Norm after: 10.294236889475668
Epoch 2463/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.5257100582122802
Epoch: 2463, Batch Gradient Norm: 10.511104408575209
Epoch: 2463, Batch Gradient Norm after: 10.511104408575209
Epoch 2464/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5244466066360474
Epoch: 2464, Batch Gradient Norm: 13.630561043359917
Epoch: 2464, Batch Gradient Norm after: 13.630561043359917
Epoch 2465/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5452439188957214
Epoch: 2465, Batch Gradient Norm: 12.007829468313227
Epoch: 2465, Batch Gradient Norm after: 12.007829468313227
Epoch 2466/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.533857548236847
Epoch: 2466, Batch Gradient Norm: 10.005909195092128
Epoch: 2466, Batch Gradient Norm after: 10.005909195092128
Epoch 2467/10000, Prediction Accuracy = 61.056%, Loss = 0.5211945652961731
Epoch: 2467, Batch Gradient Norm: 8.50843638081815
Epoch: 2467, Batch Gradient Norm after: 8.50843638081815
Epoch 2468/10000, Prediction Accuracy = 61.010000000000005%, Loss = 0.5134932577610016
Epoch: 2468, Batch Gradient Norm: 9.949889109557336
Epoch: 2468, Batch Gradient Norm after: 9.949889109557336
Epoch 2469/10000, Prediction Accuracy = 61.068000000000005%, Loss = 0.5222312211990356
Epoch: 2469, Batch Gradient Norm: 12.952161349220024
Epoch: 2469, Batch Gradient Norm after: 12.952161349220024
Epoch 2470/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.5434214115142822
Epoch: 2470, Batch Gradient Norm: 15.051616814351808
Epoch: 2470, Batch Gradient Norm after: 15.051616814351808
Epoch 2471/10000, Prediction Accuracy = 60.976%, Loss = 0.5600717067718506
Epoch: 2471, Batch Gradient Norm: 10.74134589324352
Epoch: 2471, Batch Gradient Norm after: 10.74134589324352
Epoch 2472/10000, Prediction Accuracy = 61.024%, Loss = 0.5266363143920898
Epoch: 2472, Batch Gradient Norm: 10.484003124159525
Epoch: 2472, Batch Gradient Norm after: 10.484003124159525
Epoch 2473/10000, Prediction Accuracy = 60.95%, Loss = 0.5255086302757264
Epoch: 2473, Batch Gradient Norm: 11.135290945029386
Epoch: 2473, Batch Gradient Norm after: 11.135290945029386
Epoch 2474/10000, Prediction Accuracy = 61.010000000000005%, Loss = 0.5307156920433045
Epoch: 2474, Batch Gradient Norm: 10.382910311674777
Epoch: 2474, Batch Gradient Norm after: 10.382910311674777
Epoch 2475/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5250943183898926
Epoch: 2475, Batch Gradient Norm: 9.935261264131775
Epoch: 2475, Batch Gradient Norm after: 9.935261264131775
Epoch 2476/10000, Prediction Accuracy = 61.028%, Loss = 0.522271478176117
Epoch: 2476, Batch Gradient Norm: 8.851808417521804
Epoch: 2476, Batch Gradient Norm after: 8.851808417521804
Epoch 2477/10000, Prediction Accuracy = 60.965999999999994%, Loss = 0.515911728143692
Epoch: 2477, Batch Gradient Norm: 11.38910309299265
Epoch: 2477, Batch Gradient Norm after: 11.38910309299265
Epoch 2478/10000, Prediction Accuracy = 60.95799999999999%, Loss = 0.5329278826713562
Epoch: 2478, Batch Gradient Norm: 9.79948231444642
Epoch: 2478, Batch Gradient Norm after: 9.79948231444642
Epoch 2479/10000, Prediction Accuracy = 61.04%, Loss = 0.5227123022079467
Epoch: 2479, Batch Gradient Norm: 10.455258277781155
Epoch: 2479, Batch Gradient Norm after: 10.455258277781155
Epoch 2480/10000, Prediction Accuracy = 61.04%, Loss = 0.5234780669212341
Epoch: 2480, Batch Gradient Norm: 13.233010752771708
Epoch: 2480, Batch Gradient Norm after: 13.233010752771708
Epoch 2481/10000, Prediction Accuracy = 60.968%, Loss = 0.5431779503822327
Epoch: 2481, Batch Gradient Norm: 10.217026082536373
Epoch: 2481, Batch Gradient Norm after: 10.217026082536373
Epoch 2482/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5218970775604248
Epoch: 2482, Batch Gradient Norm: 8.386165976909943
Epoch: 2482, Batch Gradient Norm after: 8.386165976909943
Epoch 2483/10000, Prediction Accuracy = 60.99399999999999%, Loss = 0.5112062096595764
Epoch: 2483, Batch Gradient Norm: 9.926301247111258
Epoch: 2483, Batch Gradient Norm after: 9.926301247111258
Epoch 2484/10000, Prediction Accuracy = 61.138%, Loss = 0.5194100975990296
Epoch: 2484, Batch Gradient Norm: 11.524323987530302
Epoch: 2484, Batch Gradient Norm after: 11.524323987530302
Epoch 2485/10000, Prediction Accuracy = 60.94200000000001%, Loss = 0.5306321263313294
Epoch: 2485, Batch Gradient Norm: 11.809889345064493
Epoch: 2485, Batch Gradient Norm after: 11.809889345064493
Epoch 2486/10000, Prediction Accuracy = 61.092%, Loss = 0.5349386215209961
Epoch: 2486, Batch Gradient Norm: 9.268759783691195
Epoch: 2486, Batch Gradient Norm after: 9.268759783691195
Epoch 2487/10000, Prediction Accuracy = 61.012%, Loss = 0.5168127298355103
Epoch: 2487, Batch Gradient Norm: 11.365370398415545
Epoch: 2487, Batch Gradient Norm after: 11.365370398415545
Epoch 2488/10000, Prediction Accuracy = 61.024%, Loss = 0.529467761516571
Epoch: 2488, Batch Gradient Norm: 12.209612610447188
Epoch: 2488, Batch Gradient Norm after: 12.209612610447188
Epoch 2489/10000, Prediction Accuracy = 60.96%, Loss = 0.5380204796791077
Epoch: 2489, Batch Gradient Norm: 11.511154844856836
Epoch: 2489, Batch Gradient Norm after: 11.511154844856836
Epoch 2490/10000, Prediction Accuracy = 61.093999999999994%, Loss = 0.5331862092018127
Epoch: 2490, Batch Gradient Norm: 9.393265107290576
Epoch: 2490, Batch Gradient Norm after: 9.393265107290576
Epoch 2491/10000, Prediction Accuracy = 61.048%, Loss = 0.5183729887008667
Epoch: 2491, Batch Gradient Norm: 9.756464452564126
Epoch: 2491, Batch Gradient Norm after: 9.756464452564126
Epoch 2492/10000, Prediction Accuracy = 61.07800000000001%, Loss = 0.5193141222000122
Epoch: 2492, Batch Gradient Norm: 10.109139066640136
Epoch: 2492, Batch Gradient Norm after: 10.109139066640136
Epoch 2493/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5212360382080078
Epoch: 2493, Batch Gradient Norm: 12.100966167408776
Epoch: 2493, Batch Gradient Norm after: 12.100966167408776
Epoch 2494/10000, Prediction Accuracy = 61.008%, Loss = 0.533852505683899
Epoch: 2494, Batch Gradient Norm: 12.593524974667803
Epoch: 2494, Batch Gradient Norm after: 12.593524974667803
Epoch 2495/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5381868720054627
Epoch: 2495, Batch Gradient Norm: 12.300830008954305
Epoch: 2495, Batch Gradient Norm after: 12.300830008954305
Epoch 2496/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.5383164286613464
Epoch: 2496, Batch Gradient Norm: 9.814672998257555
Epoch: 2496, Batch Gradient Norm after: 9.814672998257555
Epoch 2497/10000, Prediction Accuracy = 61.068000000000005%, Loss = 0.5218331634998321
Epoch: 2497, Batch Gradient Norm: 9.55811263682409
Epoch: 2497, Batch Gradient Norm after: 9.55811263682409
Epoch 2498/10000, Prediction Accuracy = 61.004%, Loss = 0.5191973328590394
Epoch: 2498, Batch Gradient Norm: 11.711938711678176
Epoch: 2498, Batch Gradient Norm after: 11.711938711678176
Epoch 2499/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.5328996181488037
Epoch: 2499, Batch Gradient Norm: 10.552516308365593
Epoch: 2499, Batch Gradient Norm after: 10.552516308365593
Epoch 2500/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.5251457333564759
Epoch: 2500, Batch Gradient Norm: 10.055527125685781
Epoch: 2500, Batch Gradient Norm after: 10.055527125685781
Epoch 2501/10000, Prediction Accuracy = 61.0%, Loss = 0.5218180298805237
Epoch: 2501, Batch Gradient Norm: 12.084392347070818
Epoch: 2501, Batch Gradient Norm after: 12.084392347070818
Epoch 2502/10000, Prediction Accuracy = 60.980000000000004%, Loss = 0.5347326874732972
Epoch: 2502, Batch Gradient Norm: 13.913741378863211
Epoch: 2502, Batch Gradient Norm after: 13.913741378863211
Epoch 2503/10000, Prediction Accuracy = 61.048%, Loss = 0.548204493522644
Epoch: 2503, Batch Gradient Norm: 10.993772412252225
Epoch: 2503, Batch Gradient Norm after: 10.993772412252225
Epoch 2504/10000, Prediction Accuracy = 61.05800000000001%, Loss = 0.5256624698638916
Epoch: 2504, Batch Gradient Norm: 9.941611582738695
Epoch: 2504, Batch Gradient Norm after: 9.941611582738695
Epoch 2505/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5208523631095886
Epoch: 2505, Batch Gradient Norm: 7.107443254851175
Epoch: 2505, Batch Gradient Norm after: 7.107443254851175
Epoch 2506/10000, Prediction Accuracy = 61.010000000000005%, Loss = 0.5062690496444702
Epoch: 2506, Batch Gradient Norm: 8.454655824295303
Epoch: 2506, Batch Gradient Norm after: 8.454655824295303
Epoch 2507/10000, Prediction Accuracy = 61.03000000000001%, Loss = 0.5128541946411133
Epoch: 2507, Batch Gradient Norm: 9.873436639059534
Epoch: 2507, Batch Gradient Norm after: 9.873436639059534
Epoch 2508/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5181716561317444
Epoch: 2508, Batch Gradient Norm: 15.083927928773278
Epoch: 2508, Batch Gradient Norm after: 15.083927928773278
Epoch 2509/10000, Prediction Accuracy = 60.962%, Loss = 0.5574157953262329
Epoch: 2509, Batch Gradient Norm: 11.010398894322824
Epoch: 2509, Batch Gradient Norm after: 11.010398894322824
Epoch 2510/10000, Prediction Accuracy = 61.134%, Loss = 0.5278890252113342
Epoch: 2510, Batch Gradient Norm: 7.005958823691456
Epoch: 2510, Batch Gradient Norm after: 7.005958823691456
Epoch 2511/10000, Prediction Accuracy = 61.02%, Loss = 0.5049496412277221
Epoch: 2511, Batch Gradient Norm: 9.800405440752835
Epoch: 2511, Batch Gradient Norm after: 9.800405440752835
Epoch 2512/10000, Prediction Accuracy = 61.092%, Loss = 0.5188701152801514
Epoch: 2512, Batch Gradient Norm: 11.722256177867544
Epoch: 2512, Batch Gradient Norm after: 11.722256177867544
Epoch 2513/10000, Prediction Accuracy = 60.980000000000004%, Loss = 0.531577217578888
Epoch: 2513, Batch Gradient Norm: 12.404185832177763
Epoch: 2513, Batch Gradient Norm after: 12.404185832177763
Epoch 2514/10000, Prediction Accuracy = 61.06000000000002%, Loss = 0.5365530371665954
Epoch: 2514, Batch Gradient Norm: 12.345917552965938
Epoch: 2514, Batch Gradient Norm after: 12.345917552965938
Epoch 2515/10000, Prediction Accuracy = 60.964%, Loss = 0.5380099296569825
Epoch: 2515, Batch Gradient Norm: 9.525039339722827
Epoch: 2515, Batch Gradient Norm after: 9.525039339722827
Epoch 2516/10000, Prediction Accuracy = 61.102%, Loss = 0.5197066903114319
Epoch: 2516, Batch Gradient Norm: 8.358041304217165
Epoch: 2516, Batch Gradient Norm after: 8.358041304217165
Epoch 2517/10000, Prediction Accuracy = 61.04200000000001%, Loss = 0.5117327332496643
Epoch: 2517, Batch Gradient Norm: 9.586017520836673
Epoch: 2517, Batch Gradient Norm after: 9.586017520836673
Epoch 2518/10000, Prediction Accuracy = 61.11%, Loss = 0.518898344039917
Epoch: 2518, Batch Gradient Norm: 9.558632073289653
Epoch: 2518, Batch Gradient Norm after: 9.558632073289653
Epoch 2519/10000, Prediction Accuracy = 61.01800000000001%, Loss = 0.5175999701023102
Epoch: 2519, Batch Gradient Norm: 10.513711625274944
Epoch: 2519, Batch Gradient Norm after: 10.513711625274944
Epoch 2520/10000, Prediction Accuracy = 61.092%, Loss = 0.5225034832954407
Epoch: 2520, Batch Gradient Norm: 11.170340969970136
Epoch: 2520, Batch Gradient Norm after: 11.170340969970136
Epoch 2521/10000, Prediction Accuracy = 60.98199999999999%, Loss = 0.5272881150245666
Epoch: 2521, Batch Gradient Norm: 10.154982683290612
Epoch: 2521, Batch Gradient Norm after: 10.154982683290612
Epoch 2522/10000, Prediction Accuracy = 61.07199999999999%, Loss = 0.5204749166965484
Epoch: 2522, Batch Gradient Norm: 10.381990532760279
Epoch: 2522, Batch Gradient Norm after: 10.381990532760279
Epoch 2523/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5226340889930725
Epoch: 2523, Batch Gradient Norm: 12.615862275468952
Epoch: 2523, Batch Gradient Norm after: 12.615862275468952
Epoch 2524/10000, Prediction Accuracy = 61.05%, Loss = 0.5372815608978272
Epoch: 2524, Batch Gradient Norm: 14.789040800327017
Epoch: 2524, Batch Gradient Norm after: 14.789040800327017
Epoch 2525/10000, Prediction Accuracy = 60.964%, Loss = 0.557091474533081
Epoch: 2525, Batch Gradient Norm: 12.370515078055208
Epoch: 2525, Batch Gradient Norm after: 12.370515078055208
Epoch 2526/10000, Prediction Accuracy = 61.064%, Loss = 0.539534842967987
Epoch: 2526, Batch Gradient Norm: 8.654127977076262
Epoch: 2526, Batch Gradient Norm after: 8.654127977076262
Epoch 2527/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5143942952156066
Epoch: 2527, Batch Gradient Norm: 8.76223112696393
Epoch: 2527, Batch Gradient Norm after: 8.76223112696393
Epoch 2528/10000, Prediction Accuracy = 61.025999999999996%, Loss = 0.5141464710235596
Epoch: 2528, Batch Gradient Norm: 10.845333434857498
Epoch: 2528, Batch Gradient Norm after: 10.845333434857498
Epoch 2529/10000, Prediction Accuracy = 61.0%, Loss = 0.5251670360565186
Epoch: 2529, Batch Gradient Norm: 11.960144868020233
Epoch: 2529, Batch Gradient Norm after: 11.960144868020233
Epoch 2530/10000, Prediction Accuracy = 60.970000000000006%, Loss = 0.5323781132698059
Epoch: 2530, Batch Gradient Norm: 9.296923246596524
Epoch: 2530, Batch Gradient Norm after: 9.296923246596524
Epoch 2531/10000, Prediction Accuracy = 61.012%, Loss = 0.5149477660655976
Epoch: 2531, Batch Gradient Norm: 8.808233180893492
Epoch: 2531, Batch Gradient Norm after: 8.808233180893492
Epoch 2532/10000, Prediction Accuracy = 61.03399999999999%, Loss = 0.5114444315433502
Epoch: 2532, Batch Gradient Norm: 11.960228511579604
Epoch: 2532, Batch Gradient Norm after: 11.960228511579604
Epoch 2533/10000, Prediction Accuracy = 61.092%, Loss = 0.5299223184585571
Epoch: 2533, Batch Gradient Norm: 13.00364356099574
Epoch: 2533, Batch Gradient Norm after: 13.00364356099574
Epoch 2534/10000, Prediction Accuracy = 60.996%, Loss = 0.5382363796234131
Epoch: 2534, Batch Gradient Norm: 12.048467430793588
Epoch: 2534, Batch Gradient Norm after: 12.048467430793588
Epoch 2535/10000, Prediction Accuracy = 61.08599999999999%, Loss = 0.5332399368286133
Epoch: 2535, Batch Gradient Norm: 9.258006314152016
Epoch: 2535, Batch Gradient Norm after: 9.258006314152016
Epoch 2536/10000, Prediction Accuracy = 61.122%, Loss = 0.5159400522708892
Epoch: 2536, Batch Gradient Norm: 8.708553412197043
Epoch: 2536, Batch Gradient Norm after: 8.708553412197043
Epoch 2537/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5136764645576477
Epoch: 2537, Batch Gradient Norm: 9.29993131532064
Epoch: 2537, Batch Gradient Norm after: 9.29993131532064
Epoch 2538/10000, Prediction Accuracy = 61.104%, Loss = 0.5167348027229309
Epoch: 2538, Batch Gradient Norm: 10.336009507880107
Epoch: 2538, Batch Gradient Norm after: 10.336009507880107
Epoch 2539/10000, Prediction Accuracy = 61.00600000000001%, Loss = 0.5228808760643006
Epoch: 2539, Batch Gradient Norm: 11.670846494664552
Epoch: 2539, Batch Gradient Norm after: 11.670846494664552
Epoch 2540/10000, Prediction Accuracy = 61.08399999999999%, Loss = 0.5299007296562195
Epoch: 2540, Batch Gradient Norm: 14.390907128367902
Epoch: 2540, Batch Gradient Norm after: 14.390907128367902
Epoch 2541/10000, Prediction Accuracy = 60.944%, Loss = 0.5494320273399353
Epoch: 2541, Batch Gradient Norm: 12.795675446428245
Epoch: 2541, Batch Gradient Norm after: 12.795675446428245
Epoch 2542/10000, Prediction Accuracy = 61.08%, Loss = 0.5383757710456848
Epoch: 2542, Batch Gradient Norm: 9.456322190448006
Epoch: 2542, Batch Gradient Norm after: 9.456322190448006
Epoch 2543/10000, Prediction Accuracy = 61.072%, Loss = 0.5171389758586884
Epoch: 2543, Batch Gradient Norm: 8.016317067378784
Epoch: 2543, Batch Gradient Norm after: 8.016317067378784
Epoch 2544/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.508726280927658
Epoch: 2544, Batch Gradient Norm: 9.286742520465939
Epoch: 2544, Batch Gradient Norm after: 9.286742520465939
Epoch 2545/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5153236806392669
Epoch: 2545, Batch Gradient Norm: 9.84031353732612
Epoch: 2545, Batch Gradient Norm after: 9.84031353732612
Epoch 2546/10000, Prediction Accuracy = 60.974000000000004%, Loss = 0.5182617485523224
Epoch: 2546, Batch Gradient Norm: 9.486629858748758
Epoch: 2546, Batch Gradient Norm after: 9.486629858748758
Epoch 2547/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5157064616680145
Epoch: 2547, Batch Gradient Norm: 11.338623145264007
Epoch: 2547, Batch Gradient Norm after: 11.338623145264007
Epoch 2548/10000, Prediction Accuracy = 61.068%, Loss = 0.5263779640197754
Epoch: 2548, Batch Gradient Norm: 14.600498793859249
Epoch: 2548, Batch Gradient Norm after: 14.600498793859249
Epoch 2549/10000, Prediction Accuracy = 60.878%, Loss = 0.5528678655624389
Epoch: 2549, Batch Gradient Norm: 11.841204502042254
Epoch: 2549, Batch Gradient Norm after: 11.841204502042254
Epoch 2550/10000, Prediction Accuracy = 60.968%, Loss = 0.5332220077514649
Epoch: 2550, Batch Gradient Norm: 9.166513695478967
Epoch: 2550, Batch Gradient Norm after: 9.166513695478967
Epoch 2551/10000, Prediction Accuracy = 61.052%, Loss = 0.5149865686893463
Epoch: 2551, Batch Gradient Norm: 8.806745991622291
Epoch: 2551, Batch Gradient Norm after: 8.806745991622291
Epoch 2552/10000, Prediction Accuracy = 61.02%, Loss = 0.5119682550430298
Epoch: 2552, Batch Gradient Norm: 10.761557550408039
Epoch: 2552, Batch Gradient Norm after: 10.761557550408039
Epoch 2553/10000, Prediction Accuracy = 61.080000000000005%, Loss = 0.522941243648529
Epoch: 2553, Batch Gradient Norm: 9.291178311021303
Epoch: 2553, Batch Gradient Norm after: 9.291178311021303
Epoch 2554/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.5133144319057464
Epoch: 2554, Batch Gradient Norm: 9.294554723941841
Epoch: 2554, Batch Gradient Norm after: 9.294554723941841
Epoch 2555/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.5129192054271698
Epoch: 2555, Batch Gradient Norm: 11.785690652206235
Epoch: 2555, Batch Gradient Norm after: 11.785690652206235
Epoch 2556/10000, Prediction Accuracy = 61.02%, Loss = 0.5289260745048523
Epoch: 2556, Batch Gradient Norm: 13.794491531929912
Epoch: 2556, Batch Gradient Norm after: 13.794491531929912
Epoch 2557/10000, Prediction Accuracy = 61.0%, Loss = 0.5470034956932068
Epoch: 2557, Batch Gradient Norm: 13.418650092531337
Epoch: 2557, Batch Gradient Norm after: 13.418650092531337
Epoch 2558/10000, Prediction Accuracy = 61.06%, Loss = 0.5448890566825867
Epoch: 2558, Batch Gradient Norm: 8.47368608452959
Epoch: 2558, Batch Gradient Norm after: 8.47368608452959
Epoch 2559/10000, Prediction Accuracy = 61.05400000000001%, Loss = 0.5103683650493622
Epoch: 2559, Batch Gradient Norm: 8.05793223608145
Epoch: 2559, Batch Gradient Norm after: 8.05793223608145
Epoch 2560/10000, Prediction Accuracy = 61.07000000000001%, Loss = 0.5075315177440644
Epoch: 2560, Batch Gradient Norm: 8.162707236503017
Epoch: 2560, Batch Gradient Norm after: 8.162707236503017
Epoch 2561/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5080587029457092
Epoch: 2561, Batch Gradient Norm: 11.109320488763988
Epoch: 2561, Batch Gradient Norm after: 11.109320488763988
Epoch 2562/10000, Prediction Accuracy = 61.08200000000001%, Loss = 0.5246419906616211
Epoch: 2562, Batch Gradient Norm: 14.984245712405743
Epoch: 2562, Batch Gradient Norm after: 14.984245712405743
Epoch 2563/10000, Prediction Accuracy = 60.922000000000004%, Loss = 0.553922975063324
Epoch: 2563, Batch Gradient Norm: 12.453961806398304
Epoch: 2563, Batch Gradient Norm after: 12.453961806398304
Epoch 2564/10000, Prediction Accuracy = 61.102%, Loss = 0.5352465748786926
Epoch: 2564, Batch Gradient Norm: 7.54876352973356
Epoch: 2564, Batch Gradient Norm after: 7.54876352973356
Epoch 2565/10000, Prediction Accuracy = 61.024%, Loss = 0.5057148933410645
Epoch: 2565, Batch Gradient Norm: 8.284092220018612
Epoch: 2565, Batch Gradient Norm after: 8.284092220018612
Epoch 2566/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.5095315396785736
Epoch: 2566, Batch Gradient Norm: 11.040447454109623
Epoch: 2566, Batch Gradient Norm after: 11.040447454109623
Epoch 2567/10000, Prediction Accuracy = 60.992000000000004%, Loss = 0.5247026324272156
Epoch: 2567, Batch Gradient Norm: 15.112175658101908
Epoch: 2567, Batch Gradient Norm after: 15.112175658101908
Epoch 2568/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5594127774238586
Epoch: 2568, Batch Gradient Norm: 10.576537959505925
Epoch: 2568, Batch Gradient Norm after: 10.576537959505925
Epoch 2569/10000, Prediction Accuracy = 61.048%, Loss = 0.5227242469787597
Epoch: 2569, Batch Gradient Norm: 9.167604505979153
Epoch: 2569, Batch Gradient Norm after: 9.167604505979153
Epoch 2570/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.5129113078117371
Epoch: 2570, Batch Gradient Norm: 11.341873674366123
Epoch: 2570, Batch Gradient Norm after: 11.341873674366123
Epoch 2571/10000, Prediction Accuracy = 60.99400000000001%, Loss = 0.5261788606643677
Epoch: 2571, Batch Gradient Norm: 10.848452364559693
Epoch: 2571, Batch Gradient Norm after: 10.848452364559693
Epoch 2572/10000, Prediction Accuracy = 61.004%, Loss = 0.5232717871665955
Epoch: 2572, Batch Gradient Norm: 8.775619217545879
Epoch: 2572, Batch Gradient Norm after: 8.775619217545879
Epoch 2573/10000, Prediction Accuracy = 61.044000000000004%, Loss = 0.510912424325943
Epoch: 2573, Batch Gradient Norm: 7.753680331985956
Epoch: 2573, Batch Gradient Norm after: 7.753680331985956
Epoch 2574/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5056834936141967
Epoch: 2574, Batch Gradient Norm: 10.099624978499017
Epoch: 2574, Batch Gradient Norm after: 10.099624978499017
Epoch 2575/10000, Prediction Accuracy = 61.022000000000006%, Loss = 0.5197480201721192
Epoch: 2575, Batch Gradient Norm: 10.383136641616609
Epoch: 2575, Batch Gradient Norm after: 10.383136641616609
Epoch 2576/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5211331069469451
Epoch: 2576, Batch Gradient Norm: 12.070425670491323
Epoch: 2576, Batch Gradient Norm after: 12.070425670491323
Epoch 2577/10000, Prediction Accuracy = 60.988%, Loss = 0.531519865989685
Epoch: 2577, Batch Gradient Norm: 13.016106365101319
Epoch: 2577, Batch Gradient Norm after: 13.016106365101319
Epoch 2578/10000, Prediction Accuracy = 61.104%, Loss = 0.5415770173072815
Epoch: 2578, Batch Gradient Norm: 8.881782138249726
Epoch: 2578, Batch Gradient Norm after: 8.881782138249726
Epoch 2579/10000, Prediction Accuracy = 61.072%, Loss = 0.5128067195415497
Epoch: 2579, Batch Gradient Norm: 9.743315922865724
Epoch: 2579, Batch Gradient Norm after: 9.743315922865724
Epoch 2580/10000, Prediction Accuracy = 61.1%, Loss = 0.5152117371559143
Epoch: 2580, Batch Gradient Norm: 11.370372368106334
Epoch: 2580, Batch Gradient Norm after: 11.370372368106334
Epoch 2581/10000, Prediction Accuracy = 61.008%, Loss = 0.5228181719779968
Epoch: 2581, Batch Gradient Norm: 13.05863484921879
Epoch: 2581, Batch Gradient Norm after: 13.05863484921879
Epoch 2582/10000, Prediction Accuracy = 61.146%, Loss = 0.5349790453910828
Epoch: 2582, Batch Gradient Norm: 14.034346946203849
Epoch: 2582, Batch Gradient Norm after: 14.034346946203849
Epoch 2583/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.5481268525123596
Epoch: 2583, Batch Gradient Norm: 10.670761734257256
Epoch: 2583, Batch Gradient Norm after: 10.670761734257256
Epoch 2584/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.5237167119979859
Epoch: 2584, Batch Gradient Norm: 10.568896074028963
Epoch: 2584, Batch Gradient Norm after: 10.568896074028963
Epoch 2585/10000, Prediction Accuracy = 61.093999999999994%, Loss = 0.5214353680610657
Epoch: 2585, Batch Gradient Norm: 8.977469207176908
Epoch: 2585, Batch Gradient Norm after: 8.977469207176908
Epoch 2586/10000, Prediction Accuracy = 61.028%, Loss = 0.5113084614276886
Epoch: 2586, Batch Gradient Norm: 9.472599190043608
Epoch: 2586, Batch Gradient Norm after: 9.472599190043608
Epoch 2587/10000, Prediction Accuracy = 61.102%, Loss = 0.5134307980537415
Epoch: 2587, Batch Gradient Norm: 9.569343709654193
Epoch: 2587, Batch Gradient Norm after: 9.569343709654193
Epoch 2588/10000, Prediction Accuracy = 61.02%, Loss = 0.5140207350254059
Epoch: 2588, Batch Gradient Norm: 12.31405100969823
Epoch: 2588, Batch Gradient Norm after: 12.31405100969823
Epoch 2589/10000, Prediction Accuracy = 61.086%, Loss = 0.5327999114990234
Epoch: 2589, Batch Gradient Norm: 12.411220987686823
Epoch: 2589, Batch Gradient Norm after: 12.411220987686823
Epoch 2590/10000, Prediction Accuracy = 61.062%, Loss = 0.5339446306228638
Epoch: 2590, Batch Gradient Norm: 10.892199788949434
Epoch: 2590, Batch Gradient Norm after: 10.892199788949434
Epoch 2591/10000, Prediction Accuracy = 61.074%, Loss = 0.5231592774391174
Epoch: 2591, Batch Gradient Norm: 9.651745809518008
Epoch: 2591, Batch Gradient Norm after: 9.651745809518008
Epoch 2592/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.515142971277237
Epoch: 2592, Batch Gradient Norm: 10.049406673442537
Epoch: 2592, Batch Gradient Norm after: 10.049406673442537
Epoch 2593/10000, Prediction Accuracy = 61.032000000000004%, Loss = 0.5174656629562377
Epoch: 2593, Batch Gradient Norm: 11.02203649893049
Epoch: 2593, Batch Gradient Norm after: 11.02203649893049
Epoch 2594/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.523167896270752
Epoch: 2594, Batch Gradient Norm: 11.14641862525101
Epoch: 2594, Batch Gradient Norm after: 11.14641862525101
Epoch 2595/10000, Prediction Accuracy = 61.044%, Loss = 0.5241780877113342
Epoch: 2595, Batch Gradient Norm: 10.611059791435073
Epoch: 2595, Batch Gradient Norm after: 10.611059791435073
Epoch 2596/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.5201228380203247
Epoch: 2596, Batch Gradient Norm: 11.38297419671972
Epoch: 2596, Batch Gradient Norm after: 11.38297419671972
Epoch 2597/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.525606107711792
Epoch: 2597, Batch Gradient Norm: 11.28096737288849
Epoch: 2597, Batch Gradient Norm after: 11.28096737288849
Epoch 2598/10000, Prediction Accuracy = 61.04%, Loss = 0.5251528501510621
Epoch: 2598, Batch Gradient Norm: 11.692587191510608
Epoch: 2598, Batch Gradient Norm after: 11.692587191510608
Epoch 2599/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.528054416179657
Epoch: 2599, Batch Gradient Norm: 9.970183209786939
Epoch: 2599, Batch Gradient Norm after: 9.970183209786939
Epoch 2600/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5169569432735444
Epoch: 2600, Batch Gradient Norm: 10.671787382624585
Epoch: 2600, Batch Gradient Norm after: 10.671787382624585
Epoch 2601/10000, Prediction Accuracy = 61.004%, Loss = 0.5209830284118653
Epoch: 2601, Batch Gradient Norm: 9.543884440470968
Epoch: 2601, Batch Gradient Norm after: 9.543884440470968
Epoch 2602/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5154299437999725
Epoch: 2602, Batch Gradient Norm: 8.969433585974112
Epoch: 2602, Batch Gradient Norm after: 8.969433585974112
Epoch 2603/10000, Prediction Accuracy = 61.06%, Loss = 0.5116162002086639
Epoch: 2603, Batch Gradient Norm: 9.643940822929775
Epoch: 2603, Batch Gradient Norm after: 9.643940822929775
Epoch 2604/10000, Prediction Accuracy = 61.08%, Loss = 0.513961011171341
Epoch: 2604, Batch Gradient Norm: 11.63360218316396
Epoch: 2604, Batch Gradient Norm after: 11.63360218316396
Epoch 2605/10000, Prediction Accuracy = 61.076%, Loss = 0.5264951348304748
Epoch: 2605, Batch Gradient Norm: 10.862829070148287
Epoch: 2605, Batch Gradient Norm after: 10.862829070148287
Epoch 2606/10000, Prediction Accuracy = 60.988%, Loss = 0.5220707178115844
Epoch: 2606, Batch Gradient Norm: 9.65978196222227
Epoch: 2606, Batch Gradient Norm after: 9.65978196222227
Epoch 2607/10000, Prediction Accuracy = 61.112%, Loss = 0.5133109211921691
Epoch: 2607, Batch Gradient Norm: 12.508435235835403
Epoch: 2607, Batch Gradient Norm after: 12.508435235835403
Epoch 2608/10000, Prediction Accuracy = 61.017999999999994%, Loss = 0.5326596975326539
Epoch: 2608, Batch Gradient Norm: 12.403515211620201
Epoch: 2608, Batch Gradient Norm after: 12.403515211620201
Epoch 2609/10000, Prediction Accuracy = 61.081999999999994%, Loss = 0.5339051008224487
Epoch: 2609, Batch Gradient Norm: 9.56991300811703
Epoch: 2609, Batch Gradient Norm after: 9.56991300811703
Epoch 2610/10000, Prediction Accuracy = 61.00599999999999%, Loss = 0.5144594192504883
Epoch: 2610, Batch Gradient Norm: 10.323791252287899
Epoch: 2610, Batch Gradient Norm after: 10.323791252287899
Epoch 2611/10000, Prediction Accuracy = 61.05400000000001%, Loss = 0.5184970617294311
Epoch: 2611, Batch Gradient Norm: 12.46448218037824
Epoch: 2611, Batch Gradient Norm after: 12.46448218037824
Epoch 2612/10000, Prediction Accuracy = 60.996%, Loss = 0.5326113104820251
Epoch: 2612, Batch Gradient Norm: 10.72891559613633
Epoch: 2612, Batch Gradient Norm after: 10.72891559613633
Epoch 2613/10000, Prediction Accuracy = 61.028%, Loss = 0.5206475138664246
Epoch: 2613, Batch Gradient Norm: 10.7032050358696
Epoch: 2613, Batch Gradient Norm after: 10.7032050358696
Epoch 2614/10000, Prediction Accuracy = 61.052%, Loss = 0.521428394317627
Epoch: 2614, Batch Gradient Norm: 10.045361776782688
Epoch: 2614, Batch Gradient Norm after: 10.045361776782688
Epoch 2615/10000, Prediction Accuracy = 61.11800000000001%, Loss = 0.5174841523170471
Epoch: 2615, Batch Gradient Norm: 11.75527734645051
Epoch: 2615, Batch Gradient Norm after: 11.75527734645051
Epoch 2616/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5271175980567933
Epoch: 2616, Batch Gradient Norm: 13.413740170834277
Epoch: 2616, Batch Gradient Norm after: 13.413740170834277
Epoch 2617/10000, Prediction Accuracy = 61.132000000000005%, Loss = 0.541819953918457
Epoch: 2617, Batch Gradient Norm: 9.092602249655512
Epoch: 2617, Batch Gradient Norm after: 9.092602249655512
Epoch 2618/10000, Prediction Accuracy = 61.056%, Loss = 0.5111259698867798
Epoch: 2618, Batch Gradient Norm: 9.657770175505382
Epoch: 2618, Batch Gradient Norm after: 9.657770175505382
Epoch 2619/10000, Prediction Accuracy = 61.036%, Loss = 0.5143209338188172
Epoch: 2619, Batch Gradient Norm: 9.750827472591036
Epoch: 2619, Batch Gradient Norm after: 9.750827472591036
Epoch 2620/10000, Prediction Accuracy = 61.05400000000001%, Loss = 0.5163348078727722
Epoch: 2620, Batch Gradient Norm: 9.719823852120363
Epoch: 2620, Batch Gradient Norm after: 9.719823852120363
Epoch 2621/10000, Prediction Accuracy = 61.104%, Loss = 0.5155620098114013
Epoch: 2621, Batch Gradient Norm: 12.295331982068562
Epoch: 2621, Batch Gradient Norm after: 12.295331982068562
Epoch 2622/10000, Prediction Accuracy = 61.11600000000001%, Loss = 0.5336499691009522
Epoch: 2622, Batch Gradient Norm: 10.243322469719491
Epoch: 2622, Batch Gradient Norm after: 10.243322469719491
Epoch 2623/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5184061229228973
Epoch: 2623, Batch Gradient Norm: 11.308045942479946
Epoch: 2623, Batch Gradient Norm after: 11.308045942479946
Epoch 2624/10000, Prediction Accuracy = 61.141999999999996%, Loss = 0.5250399470329284
Epoch: 2624, Batch Gradient Norm: 10.320068878294975
Epoch: 2624, Batch Gradient Norm after: 10.320068878294975
Epoch 2625/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5182022154331207
Epoch: 2625, Batch Gradient Norm: 10.866738407827064
Epoch: 2625, Batch Gradient Norm after: 10.866738407827064
Epoch 2626/10000, Prediction Accuracy = 61.048%, Loss = 0.5205505132675171
Epoch: 2626, Batch Gradient Norm: 12.620471993602946
Epoch: 2626, Batch Gradient Norm after: 12.620471993602946
Epoch 2627/10000, Prediction Accuracy = 61.10600000000001%, Loss = 0.5312638282775879
Epoch: 2627, Batch Gradient Norm: 12.167299596866187
Epoch: 2627, Batch Gradient Norm after: 12.167299596866187
Epoch 2628/10000, Prediction Accuracy = 60.977999999999994%, Loss = 0.5281958460807801
Epoch: 2628, Batch Gradient Norm: 10.699982525330523
Epoch: 2628, Batch Gradient Norm after: 10.699982525330523
Epoch 2629/10000, Prediction Accuracy = 61.10799999999999%, Loss = 0.5184725403785706
Epoch: 2629, Batch Gradient Norm: 10.38580424401052
Epoch: 2629, Batch Gradient Norm after: 10.38580424401052
Epoch 2630/10000, Prediction Accuracy = 61.048%, Loss = 0.517181271314621
Epoch: 2630, Batch Gradient Norm: 9.38280802371682
Epoch: 2630, Batch Gradient Norm after: 9.38280802371682
Epoch 2631/10000, Prediction Accuracy = 61.028%, Loss = 0.5119835197925567
Epoch: 2631, Batch Gradient Norm: 9.88142077017478
Epoch: 2631, Batch Gradient Norm after: 9.88142077017478
Epoch 2632/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5147036552429199
Epoch: 2632, Batch Gradient Norm: 10.138974706668407
Epoch: 2632, Batch Gradient Norm after: 10.138974706668407
Epoch 2633/10000, Prediction Accuracy = 60.986000000000004%, Loss = 0.5164595067501068
Epoch: 2633, Batch Gradient Norm: 10.88692065423358
Epoch: 2633, Batch Gradient Norm after: 10.88692065423358
Epoch 2634/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5209416151046753
Epoch: 2634, Batch Gradient Norm: 11.049955478359179
Epoch: 2634, Batch Gradient Norm after: 11.049955478359179
Epoch 2635/10000, Prediction Accuracy = 60.96999999999999%, Loss = 0.5213409185409545
Epoch: 2635, Batch Gradient Norm: 11.867474254887403
Epoch: 2635, Batch Gradient Norm after: 11.867474254887403
Epoch 2636/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5274338841438293
Epoch: 2636, Batch Gradient Norm: 9.014140123361427
Epoch: 2636, Batch Gradient Norm after: 9.014140123361427
Epoch 2637/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5094762146472931
Epoch: 2637, Batch Gradient Norm: 9.82085998187879
Epoch: 2637, Batch Gradient Norm after: 9.82085998187879
Epoch 2638/10000, Prediction Accuracy = 61.05799999999999%, Loss = 0.5139734447002411
Epoch: 2638, Batch Gradient Norm: 10.995794039011097
Epoch: 2638, Batch Gradient Norm after: 10.995794039011097
Epoch 2639/10000, Prediction Accuracy = 61.184000000000005%, Loss = 0.5224976897239685
Epoch: 2639, Batch Gradient Norm: 11.2573690005311
Epoch: 2639, Batch Gradient Norm after: 11.2573690005311
Epoch 2640/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.5237238526344299
Epoch: 2640, Batch Gradient Norm: 11.785276518162044
Epoch: 2640, Batch Gradient Norm after: 11.785276518162044
Epoch 2641/10000, Prediction Accuracy = 61.06600000000001%, Loss = 0.5258756875991821
Epoch: 2641, Batch Gradient Norm: 13.302150389405579
Epoch: 2641, Batch Gradient Norm after: 13.302150389405579
Epoch 2642/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.5396716356277466
Epoch: 2642, Batch Gradient Norm: 9.923098088502321
Epoch: 2642, Batch Gradient Norm after: 9.923098088502321
Epoch 2643/10000, Prediction Accuracy = 61.029999999999994%, Loss = 0.5159642040729523
Epoch: 2643, Batch Gradient Norm: 8.950003621572886
Epoch: 2643, Batch Gradient Norm after: 8.950003621572886
Epoch 2644/10000, Prediction Accuracy = 61.086%, Loss = 0.5097521603107452
Epoch: 2644, Batch Gradient Norm: 9.960283353025257
Epoch: 2644, Batch Gradient Norm after: 9.960283353025257
Epoch 2645/10000, Prediction Accuracy = 61.056000000000004%, Loss = 0.5149323463439941
Epoch: 2645, Batch Gradient Norm: 12.587189741192228
Epoch: 2645, Batch Gradient Norm after: 12.587189741192228
Epoch 2646/10000, Prediction Accuracy = 61.077999999999996%, Loss = 0.5326171517372131
Epoch: 2646, Batch Gradient Norm: 12.336622567170279
Epoch: 2646, Batch Gradient Norm after: 12.336622567170279
Epoch 2647/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.5318756222724914
Epoch: 2647, Batch Gradient Norm: 9.224380299195335
Epoch: 2647, Batch Gradient Norm after: 9.224380299195335
Epoch 2648/10000, Prediction Accuracy = 61.01800000000001%, Loss = 0.5094295561313629
Epoch: 2648, Batch Gradient Norm: 9.982725621421292
Epoch: 2648, Batch Gradient Norm after: 9.982725621421292
Epoch 2649/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5122123837471009
Epoch: 2649, Batch Gradient Norm: 12.278397532009485
Epoch: 2649, Batch Gradient Norm after: 12.278397532009485
Epoch 2650/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.5278587698936462
Epoch: 2650, Batch Gradient Norm: 11.391980840596919
Epoch: 2650, Batch Gradient Norm after: 11.391980840596919
Epoch 2651/10000, Prediction Accuracy = 61.104%, Loss = 0.5226377844810486
Epoch: 2651, Batch Gradient Norm: 9.54523996443202
Epoch: 2651, Batch Gradient Norm after: 9.54523996443202
Epoch 2652/10000, Prediction Accuracy = 61.1%, Loss = 0.5121494591236114
Epoch: 2652, Batch Gradient Norm: 7.858200262176608
Epoch: 2652, Batch Gradient Norm after: 7.858200262176608
Epoch 2653/10000, Prediction Accuracy = 61.138%, Loss = 0.5026757419109344
Epoch: 2653, Batch Gradient Norm: 9.976336138435222
Epoch: 2653, Batch Gradient Norm after: 9.976336138435222
Epoch 2654/10000, Prediction Accuracy = 61.09000000000001%, Loss = 0.5145588278770447
Epoch: 2654, Batch Gradient Norm: 13.094633313232523
Epoch: 2654, Batch Gradient Norm after: 13.094633313232523
Epoch 2655/10000, Prediction Accuracy = 61.1%, Loss = 0.5388105273246765
Epoch: 2655, Batch Gradient Norm: 9.723572435074125
Epoch: 2655, Batch Gradient Norm after: 9.723572435074125
Epoch 2656/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5147822201251984
Epoch: 2656, Batch Gradient Norm: 10.51607474636247
Epoch: 2656, Batch Gradient Norm after: 10.51607474636247
Epoch 2657/10000, Prediction Accuracy = 61.092000000000006%, Loss = 0.518817412853241
Epoch: 2657, Batch Gradient Norm: 10.784086640202139
Epoch: 2657, Batch Gradient Norm after: 10.784086640202139
Epoch 2658/10000, Prediction Accuracy = 61.112%, Loss = 0.5198265075683594
Epoch: 2658, Batch Gradient Norm: 11.687020847945425
Epoch: 2658, Batch Gradient Norm after: 11.687020847945425
Epoch 2659/10000, Prediction Accuracy = 61.024%, Loss = 0.5270763397216797
Epoch: 2659, Batch Gradient Norm: 10.17309773446185
Epoch: 2659, Batch Gradient Norm after: 10.17309773446185
Epoch 2660/10000, Prediction Accuracy = 61.116%, Loss = 0.5183042883872986
Epoch: 2660, Batch Gradient Norm: 7.809523737187158
Epoch: 2660, Batch Gradient Norm after: 7.809523737187158
Epoch 2661/10000, Prediction Accuracy = 61.136%, Loss = 0.5033608913421631
Epoch: 2661, Batch Gradient Norm: 9.71070179301842
Epoch: 2661, Batch Gradient Norm after: 9.71070179301842
Epoch 2662/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.5132663011550903
Epoch: 2662, Batch Gradient Norm: 12.464850606977022
Epoch: 2662, Batch Gradient Norm after: 12.464850606977022
Epoch 2663/10000, Prediction Accuracy = 60.95399999999999%, Loss = 0.5321319699287415
Epoch: 2663, Batch Gradient Norm: 10.20124245848094
Epoch: 2663, Batch Gradient Norm after: 10.20124245848094
Epoch 2664/10000, Prediction Accuracy = 61.114%, Loss = 0.5155119478702546
Epoch: 2664, Batch Gradient Norm: 9.254380207275354
Epoch: 2664, Batch Gradient Norm after: 9.254380207275354
Epoch 2665/10000, Prediction Accuracy = 61.104000000000006%, Loss = 0.5088372886180877
Epoch: 2665, Batch Gradient Norm: 12.030994350907743
Epoch: 2665, Batch Gradient Norm after: 12.030994350907743
Epoch 2666/10000, Prediction Accuracy = 61.141999999999996%, Loss = 0.5275357723236084
Epoch: 2666, Batch Gradient Norm: 12.532832281329949
Epoch: 2666, Batch Gradient Norm after: 12.532832281329949
Epoch 2667/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.5327922463417053
Epoch: 2667, Batch Gradient Norm: 10.39380416219767
Epoch: 2667, Batch Gradient Norm after: 10.39380416219767
Epoch 2668/10000, Prediction Accuracy = 61.024%, Loss = 0.5163645505905151
Epoch: 2668, Batch Gradient Norm: 12.726864410349602
Epoch: 2668, Batch Gradient Norm after: 12.726864410349602
Epoch 2669/10000, Prediction Accuracy = 61.157999999999994%, Loss = 0.5313399314880372
Epoch: 2669, Batch Gradient Norm: 12.26624753030507
Epoch: 2669, Batch Gradient Norm after: 12.26624753030507
Epoch 2670/10000, Prediction Accuracy = 60.967999999999996%, Loss = 0.5268872737884521
Epoch: 2670, Batch Gradient Norm: 10.33272626331518
Epoch: 2670, Batch Gradient Norm after: 10.33272626331518
Epoch 2671/10000, Prediction Accuracy = 61.14%, Loss = 0.5142582476139068
Epoch: 2671, Batch Gradient Norm: 9.575941433158917
Epoch: 2671, Batch Gradient Norm after: 9.575941433158917
Epoch 2672/10000, Prediction Accuracy = 61.08399999999999%, Loss = 0.5102490365505219
Epoch: 2672, Batch Gradient Norm: 10.40438490647688
Epoch: 2672, Batch Gradient Norm after: 10.40438490647688
Epoch 2673/10000, Prediction Accuracy = 61.05800000000001%, Loss = 0.5165370225906372
Epoch: 2673, Batch Gradient Norm: 11.351768728615856
Epoch: 2673, Batch Gradient Norm after: 11.351768728615856
Epoch 2674/10000, Prediction Accuracy = 61.068%, Loss = 0.5246191501617432
Epoch: 2674, Batch Gradient Norm: 9.954620898360497
Epoch: 2674, Batch Gradient Norm after: 9.954620898360497
Epoch 2675/10000, Prediction Accuracy = 61.088%, Loss = 0.5142826437950134
Epoch: 2675, Batch Gradient Norm: 9.33343539045107
Epoch: 2675, Batch Gradient Norm after: 9.33343539045107
Epoch 2676/10000, Prediction Accuracy = 61.105999999999995%, Loss = 0.509754192829132
Epoch: 2676, Batch Gradient Norm: 9.173898838428135
Epoch: 2676, Batch Gradient Norm after: 9.173898838428135
Epoch 2677/10000, Prediction Accuracy = 61.138%, Loss = 0.508087021112442
Epoch: 2677, Batch Gradient Norm: 11.461667213147665
Epoch: 2677, Batch Gradient Norm after: 11.461667213147665
Epoch 2678/10000, Prediction Accuracy = 61.128%, Loss = 0.5215365588665009
Epoch: 2678, Batch Gradient Norm: 13.65923550406273
Epoch: 2678, Batch Gradient Norm after: 13.65923550406273
Epoch 2679/10000, Prediction Accuracy = 60.998000000000005%, Loss = 0.5386117815971374
Epoch: 2679, Batch Gradient Norm: 14.249685177390596
Epoch: 2679, Batch Gradient Norm after: 14.249685177390596
Epoch 2680/10000, Prediction Accuracy = 61.144000000000005%, Loss = 0.5457513928413391
Epoch: 2680, Batch Gradient Norm: 9.295065661499427
Epoch: 2680, Batch Gradient Norm after: 9.295065661499427
Epoch 2681/10000, Prediction Accuracy = 61.128%, Loss = 0.5104450047016144
Epoch: 2681, Batch Gradient Norm: 9.313164018830982
Epoch: 2681, Batch Gradient Norm after: 9.313164018830982
Epoch 2682/10000, Prediction Accuracy = 61.068000000000005%, Loss = 0.5100594341754914
Epoch: 2682, Batch Gradient Norm: 9.785255506938727
Epoch: 2682, Batch Gradient Norm after: 9.785255506938727
Epoch 2683/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.512154471874237
Epoch: 2683, Batch Gradient Norm: 11.372782286616902
Epoch: 2683, Batch Gradient Norm after: 11.372782286616902
Epoch 2684/10000, Prediction Accuracy = 61.162%, Loss = 0.5239832997322083
Epoch: 2684, Batch Gradient Norm: 8.855700969062081
Epoch: 2684, Batch Gradient Norm after: 8.855700969062081
Epoch 2685/10000, Prediction Accuracy = 61.15599999999999%, Loss = 0.5072636187076569
Epoch: 2685, Batch Gradient Norm: 8.19270428533719
Epoch: 2685, Batch Gradient Norm after: 8.19270428533719
Epoch 2686/10000, Prediction Accuracy = 61.11800000000001%, Loss = 0.5028684318065644
Epoch: 2686, Batch Gradient Norm: 10.182689390454016
Epoch: 2686, Batch Gradient Norm after: 10.182689390454016
Epoch 2687/10000, Prediction Accuracy = 61.15400000000001%, Loss = 0.5132392227649689
Epoch: 2687, Batch Gradient Norm: 11.468378402615523
Epoch: 2687, Batch Gradient Norm after: 11.468378402615523
Epoch 2688/10000, Prediction Accuracy = 61.126%, Loss = 0.5217185616493225
Epoch: 2688, Batch Gradient Norm: 11.289880145484002
Epoch: 2688, Batch Gradient Norm after: 11.289880145484002
Epoch 2689/10000, Prediction Accuracy = 61.05%, Loss = 0.521316409111023
Epoch: 2689, Batch Gradient Norm: 10.448471982051403
Epoch: 2689, Batch Gradient Norm after: 10.448471982051403
Epoch 2690/10000, Prediction Accuracy = 61.086%, Loss = 0.5190938591957093
Epoch: 2690, Batch Gradient Norm: 8.230398766779356
Epoch: 2690, Batch Gradient Norm after: 8.230398766779356
Epoch 2691/10000, Prediction Accuracy = 61.056%, Loss = 0.504254949092865
Epoch: 2691, Batch Gradient Norm: 11.541591988969165
Epoch: 2691, Batch Gradient Norm after: 11.541591988969165
Epoch 2692/10000, Prediction Accuracy = 61.089999999999996%, Loss = 0.5267495751380921
Epoch: 2692, Batch Gradient Norm: 9.400197412188389
Epoch: 2692, Batch Gradient Norm after: 9.400197412188389
Epoch 2693/10000, Prediction Accuracy = 61.080000000000005%, Loss = 0.5105587184429169
Epoch: 2693, Batch Gradient Norm: 12.120297290542513
Epoch: 2693, Batch Gradient Norm after: 12.120297290542513
Epoch 2694/10000, Prediction Accuracy = 61.178%, Loss = 0.5248153686523438
Epoch: 2694, Batch Gradient Norm: 14.296050210703473
Epoch: 2694, Batch Gradient Norm after: 14.296050210703473
Epoch 2695/10000, Prediction Accuracy = 61.08800000000001%, Loss = 0.542770266532898
Epoch: 2695, Batch Gradient Norm: 10.61921165726222
Epoch: 2695, Batch Gradient Norm after: 10.61921165726222
Epoch 2696/10000, Prediction Accuracy = 61.148%, Loss = 0.5155664384365082
Epoch: 2696, Batch Gradient Norm: 9.949835235138643
Epoch: 2696, Batch Gradient Norm after: 9.949835235138643
Epoch 2697/10000, Prediction Accuracy = 61.001999999999995%, Loss = 0.5116611003875733
Epoch: 2697, Batch Gradient Norm: 12.362177131337585
Epoch: 2697, Batch Gradient Norm after: 12.362177131337585
Epoch 2698/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.5294359207153321
Epoch: 2698, Batch Gradient Norm: 11.063962542200846
Epoch: 2698, Batch Gradient Norm after: 11.063962542200846
Epoch 2699/10000, Prediction Accuracy = 61.088%, Loss = 0.5195218324661255
Epoch: 2699, Batch Gradient Norm: 11.2546498090185
Epoch: 2699, Batch Gradient Norm after: 11.2546498090185
Epoch 2700/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.5233378052711487
Epoch: 2700, Batch Gradient Norm: 8.876665542069873
Epoch: 2700, Batch Gradient Norm after: 8.876665542069873
Epoch 2701/10000, Prediction Accuracy = 61.065999999999995%, Loss = 0.5084808111190796
Epoch: 2701, Batch Gradient Norm: 10.934109788047579
Epoch: 2701, Batch Gradient Norm after: 10.934109788047579
Epoch 2702/10000, Prediction Accuracy = 61.11999999999999%, Loss = 0.5195258378982544
Epoch: 2702, Batch Gradient Norm: 11.494999674415814
Epoch: 2702, Batch Gradient Norm after: 11.494999674415814
Epoch 2703/10000, Prediction Accuracy = 61.032%, Loss = 0.5228955388069153
Epoch: 2703, Batch Gradient Norm: 8.699567580148184
Epoch: 2703, Batch Gradient Norm after: 8.699567580148184
Epoch 2704/10000, Prediction Accuracy = 61.14%, Loss = 0.5051332354545593
Epoch: 2704, Batch Gradient Norm: 6.916390686034646
Epoch: 2704, Batch Gradient Norm after: 6.916390686034646
Epoch 2705/10000, Prediction Accuracy = 61.120000000000005%, Loss = 0.49594826698303224
Epoch: 2705, Batch Gradient Norm: 8.374494494345836
Epoch: 2705, Batch Gradient Norm after: 8.374494494345836
Epoch 2706/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.5019657194614411
Epoch: 2706, Batch Gradient Norm: 14.275774944280018
Epoch: 2706, Batch Gradient Norm after: 14.275774944280018
Epoch 2707/10000, Prediction Accuracy = 61.088%, Loss = 0.5445871829986573
Epoch: 2707, Batch Gradient Norm: 10.955912918275814
Epoch: 2707, Batch Gradient Norm after: 10.955912918275814
Epoch 2708/10000, Prediction Accuracy = 61.164%, Loss = 0.5204270362854004
Epoch: 2708, Batch Gradient Norm: 9.802799387386939
Epoch: 2708, Batch Gradient Norm after: 9.802799387386939
Epoch 2709/10000, Prediction Accuracy = 61.21%, Loss = 0.5110370576381683
Epoch: 2709, Batch Gradient Norm: 11.026468462806015
Epoch: 2709, Batch Gradient Norm after: 11.026468462806015
Epoch 2710/10000, Prediction Accuracy = 61.041999999999994%, Loss = 0.5165751516819
Epoch: 2710, Batch Gradient Norm: 13.621468829215083
Epoch: 2710, Batch Gradient Norm after: 13.621468829215083
Epoch 2711/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5355164647102356
Epoch: 2711, Batch Gradient Norm: 10.614814515924392
Epoch: 2711, Batch Gradient Norm after: 10.614814515924392
Epoch 2712/10000, Prediction Accuracy = 61.086%, Loss = 0.5143842279911042
Epoch: 2712, Batch Gradient Norm: 8.912069339135941
Epoch: 2712, Batch Gradient Norm after: 8.912069339135941
Epoch 2713/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.5045442819595337
Epoch: 2713, Batch Gradient Norm: 12.01622987858171
Epoch: 2713, Batch Gradient Norm after: 12.01622987858171
Epoch 2714/10000, Prediction Accuracy = 61.176%, Loss = 0.5250844836235047
Epoch: 2714, Batch Gradient Norm: 12.172221331751178
Epoch: 2714, Batch Gradient Norm after: 12.172221331751178
Epoch 2715/10000, Prediction Accuracy = 61.074%, Loss = 0.5297114372253418
Epoch: 2715, Batch Gradient Norm: 10.179597111845071
Epoch: 2715, Batch Gradient Norm after: 10.179597111845071
Epoch 2716/10000, Prediction Accuracy = 61.080000000000005%, Loss = 0.5141925811767578
Epoch: 2716, Batch Gradient Norm: 8.433406110155818
Epoch: 2716, Batch Gradient Norm after: 8.433406110155818
Epoch 2717/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.503210860490799
Epoch: 2717, Batch Gradient Norm: 10.17396223236632
Epoch: 2717, Batch Gradient Norm after: 10.17396223236632
Epoch 2718/10000, Prediction Accuracy = 61.14%, Loss = 0.5127849221229553
Epoch: 2718, Batch Gradient Norm: 13.158160435753311
Epoch: 2718, Batch Gradient Norm after: 13.158160435753311
Epoch 2719/10000, Prediction Accuracy = 61.06600000000001%, Loss = 0.5385263681411743
Epoch: 2719, Batch Gradient Norm: 7.843697695336912
Epoch: 2719, Batch Gradient Norm after: 7.843697695336912
Epoch 2720/10000, Prediction Accuracy = 61.102%, Loss = 0.5027727127075196
Epoch: 2720, Batch Gradient Norm: 7.911600754153387
Epoch: 2720, Batch Gradient Norm after: 7.911600754153387
Epoch 2721/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5016510665416718
Epoch: 2721, Batch Gradient Norm: 10.659372339495851
Epoch: 2721, Batch Gradient Norm after: 10.659372339495851
Epoch 2722/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.5165303945541382
Epoch: 2722, Batch Gradient Norm: 13.962344093299329
Epoch: 2722, Batch Gradient Norm after: 13.962344093299329
Epoch 2723/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.5409414887428283
Epoch: 2723, Batch Gradient Norm: 12.71303894983889
Epoch: 2723, Batch Gradient Norm after: 12.71303894983889
Epoch 2724/10000, Prediction Accuracy = 61.15400000000001%, Loss = 0.5307558536529541
Epoch: 2724, Batch Gradient Norm: 9.465382376125874
Epoch: 2724, Batch Gradient Norm after: 9.465382376125874
Epoch 2725/10000, Prediction Accuracy = 61.134%, Loss = 0.5073977470397949
Epoch: 2725, Batch Gradient Norm: 10.391916668321693
Epoch: 2725, Batch Gradient Norm after: 10.391916668321693
Epoch 2726/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.5135062515735627
Epoch: 2726, Batch Gradient Norm: 11.723712293607988
Epoch: 2726, Batch Gradient Norm after: 11.723712293607988
Epoch 2727/10000, Prediction Accuracy = 61.136%, Loss = 0.5246705651283264
Epoch: 2727, Batch Gradient Norm: 8.326278450413392
Epoch: 2727, Batch Gradient Norm after: 8.326278450413392
Epoch 2728/10000, Prediction Accuracy = 61.166%, Loss = 0.5023830950260162
Epoch: 2728, Batch Gradient Norm: 9.303348163477171
Epoch: 2728, Batch Gradient Norm after: 9.303348163477171
Epoch 2729/10000, Prediction Accuracy = 61.11600000000001%, Loss = 0.506687581539154
Epoch: 2729, Batch Gradient Norm: 9.717620377999655
Epoch: 2729, Batch Gradient Norm after: 9.717620377999655
Epoch 2730/10000, Prediction Accuracy = 61.074%, Loss = 0.5093132078647613
Epoch: 2730, Batch Gradient Norm: 10.264737624260313
Epoch: 2730, Batch Gradient Norm after: 10.264737624260313
Epoch 2731/10000, Prediction Accuracy = 61.126%, Loss = 0.5128747642040252
Epoch: 2731, Batch Gradient Norm: 9.736365283577095
Epoch: 2731, Batch Gradient Norm after: 9.736365283577095
Epoch 2732/10000, Prediction Accuracy = 61.08%, Loss = 0.510278582572937
Epoch: 2732, Batch Gradient Norm: 9.841205405758979
Epoch: 2732, Batch Gradient Norm after: 9.841205405758979
Epoch 2733/10000, Prediction Accuracy = 61.102%, Loss = 0.510284560918808
Epoch: 2733, Batch Gradient Norm: 13.346899547569034
Epoch: 2733, Batch Gradient Norm after: 13.346899547569034
Epoch 2734/10000, Prediction Accuracy = 61.102%, Loss = 0.5338402032852173
Epoch: 2734, Batch Gradient Norm: 13.298975765727588
Epoch: 2734, Batch Gradient Norm after: 13.298975765727588
Epoch 2735/10000, Prediction Accuracy = 61.198%, Loss = 0.5350546598434448
Epoch: 2735, Batch Gradient Norm: 10.11509665746577
Epoch: 2735, Batch Gradient Norm after: 10.11509665746577
Epoch 2736/10000, Prediction Accuracy = 61.162%, Loss = 0.5130334496498108
Epoch: 2736, Batch Gradient Norm: 8.546815168459052
Epoch: 2736, Batch Gradient Norm after: 8.546815168459052
Epoch 2737/10000, Prediction Accuracy = 61.14200000000001%, Loss = 0.5034995317459107
Epoch: 2737, Batch Gradient Norm: 11.186606865513248
Epoch: 2737, Batch Gradient Norm after: 11.186606865513248
Epoch 2738/10000, Prediction Accuracy = 61.146%, Loss = 0.5196720600128174
Epoch: 2738, Batch Gradient Norm: 12.579891673286216
Epoch: 2738, Batch Gradient Norm after: 12.579891673286216
Epoch 2739/10000, Prediction Accuracy = 61.068%, Loss = 0.5308353066444397
Epoch: 2739, Batch Gradient Norm: 11.108900357964064
Epoch: 2739, Batch Gradient Norm after: 11.108900357964064
Epoch 2740/10000, Prediction Accuracy = 61.136%, Loss = 0.5195742249488831
Epoch: 2740, Batch Gradient Norm: 10.26644999167491
Epoch: 2740, Batch Gradient Norm after: 10.26644999167491
Epoch 2741/10000, Prediction Accuracy = 61.11%, Loss = 0.512907749414444
Epoch: 2741, Batch Gradient Norm: 9.997377858266018
Epoch: 2741, Batch Gradient Norm after: 9.997377858266018
Epoch 2742/10000, Prediction Accuracy = 61.152%, Loss = 0.5112432658672332
Epoch: 2742, Batch Gradient Norm: 9.616796421567381
Epoch: 2742, Batch Gradient Norm after: 9.616796421567381
Epoch 2743/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.5097681879997253
Epoch: 2743, Batch Gradient Norm: 10.249864301503553
Epoch: 2743, Batch Gradient Norm after: 10.249864301503553
Epoch 2744/10000, Prediction Accuracy = 61.226%, Loss = 0.5128165900707244
Epoch: 2744, Batch Gradient Norm: 12.762132745594421
Epoch: 2744, Batch Gradient Norm after: 12.762132745594421
Epoch 2745/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.5293288588523865
Epoch: 2745, Batch Gradient Norm: 12.176038654852876
Epoch: 2745, Batch Gradient Norm after: 12.176038654852876
Epoch 2746/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5256893515586853
Epoch: 2746, Batch Gradient Norm: 8.577796300322461
Epoch: 2746, Batch Gradient Norm after: 8.577796300322461
Epoch 2747/10000, Prediction Accuracy = 61.14000000000001%, Loss = 0.5029223978519439
Epoch: 2747, Batch Gradient Norm: 9.91192475340613
Epoch: 2747, Batch Gradient Norm after: 9.91192475340613
Epoch 2748/10000, Prediction Accuracy = 61.112%, Loss = 0.5109008133411408
Epoch: 2748, Batch Gradient Norm: 10.131906444044922
Epoch: 2748, Batch Gradient Norm after: 10.131906444044922
Epoch 2749/10000, Prediction Accuracy = 61.114%, Loss = 0.512193214893341
Epoch: 2749, Batch Gradient Norm: 11.261155441885908
Epoch: 2749, Batch Gradient Norm after: 11.261155441885908
Epoch 2750/10000, Prediction Accuracy = 61.146%, Loss = 0.5191681265830994
Epoch: 2750, Batch Gradient Norm: 13.038058885016637
Epoch: 2750, Batch Gradient Norm after: 13.038058885016637
Epoch 2751/10000, Prediction Accuracy = 61.136%, Loss = 0.5307422876358032
Epoch: 2751, Batch Gradient Norm: 12.365781020486468
Epoch: 2751, Batch Gradient Norm after: 12.365781020486468
Epoch 2752/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5262044906616211
Epoch: 2752, Batch Gradient Norm: 10.260014560138027
Epoch: 2752, Batch Gradient Norm after: 10.260014560138027
Epoch 2753/10000, Prediction Accuracy = 61.20200000000001%, Loss = 0.5125479280948639
Epoch: 2753, Batch Gradient Norm: 9.549459597537256
Epoch: 2753, Batch Gradient Norm after: 9.549459597537256
Epoch 2754/10000, Prediction Accuracy = 61.128%, Loss = 0.5080356657505035
Epoch: 2754, Batch Gradient Norm: 10.873069645145192
Epoch: 2754, Batch Gradient Norm after: 10.873069645145192
Epoch 2755/10000, Prediction Accuracy = 61.11%, Loss = 0.5164960086345672
Epoch: 2755, Batch Gradient Norm: 12.024010647974176
Epoch: 2755, Batch Gradient Norm after: 12.024010647974176
Epoch 2756/10000, Prediction Accuracy = 61.162%, Loss = 0.524991762638092
Epoch: 2756, Batch Gradient Norm: 9.38382495445145
Epoch: 2756, Batch Gradient Norm after: 9.38382495445145
Epoch 2757/10000, Prediction Accuracy = 61.152%, Loss = 0.5066244423389434
Epoch: 2757, Batch Gradient Norm: 9.147794512694654
Epoch: 2757, Batch Gradient Norm after: 9.147794512694654
Epoch 2758/10000, Prediction Accuracy = 61.19%, Loss = 0.5049345016479492
Epoch: 2758, Batch Gradient Norm: 8.189371490344987
Epoch: 2758, Batch Gradient Norm after: 8.189371490344987
Epoch 2759/10000, Prediction Accuracy = 61.09000000000001%, Loss = 0.49979021549224856
Epoch: 2759, Batch Gradient Norm: 11.103037589938191
Epoch: 2759, Batch Gradient Norm after: 11.103037589938191
Epoch 2760/10000, Prediction Accuracy = 61.2%, Loss = 0.5172595143318176
Epoch: 2760, Batch Gradient Norm: 11.273072763646683
Epoch: 2760, Batch Gradient Norm after: 11.273072763646683
Epoch 2761/10000, Prediction Accuracy = 61.102%, Loss = 0.5198760628700256
Epoch: 2761, Batch Gradient Norm: 9.45446269851289
Epoch: 2761, Batch Gradient Norm after: 9.45446269851289
Epoch 2762/10000, Prediction Accuracy = 61.146%, Loss = 0.5073024570941925
Epoch: 2762, Batch Gradient Norm: 9.115166218778905
Epoch: 2762, Batch Gradient Norm after: 9.115166218778905
Epoch 2763/10000, Prediction Accuracy = 61.2%, Loss = 0.504717218875885
Epoch: 2763, Batch Gradient Norm: 9.35124169786705
Epoch: 2763, Batch Gradient Norm after: 9.35124169786705
Epoch 2764/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.5057012677192688
Epoch: 2764, Batch Gradient Norm: 12.444596392986414
Epoch: 2764, Batch Gradient Norm after: 12.444596392986414
Epoch 2765/10000, Prediction Accuracy = 61.17%, Loss = 0.5266200065612793
Epoch: 2765, Batch Gradient Norm: 11.79436107002945
Epoch: 2765, Batch Gradient Norm after: 11.79436107002945
Epoch 2766/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.5244481444358826
Epoch: 2766, Batch Gradient Norm: 13.232139197104644
Epoch: 2766, Batch Gradient Norm after: 13.232139197104644
Epoch 2767/10000, Prediction Accuracy = 61.06600000000001%, Loss = 0.5359905123710632
Epoch: 2767, Batch Gradient Norm: 11.00550212078293
Epoch: 2767, Batch Gradient Norm after: 11.00550212078293
Epoch 2768/10000, Prediction Accuracy = 61.124%, Loss = 0.5178541243076324
Epoch: 2768, Batch Gradient Norm: 10.857635176752021
Epoch: 2768, Batch Gradient Norm after: 10.857635176752021
Epoch 2769/10000, Prediction Accuracy = 61.181999999999995%, Loss = 0.5150260329246521
Epoch: 2769, Batch Gradient Norm: 10.265277902710451
Epoch: 2769, Batch Gradient Norm after: 10.265277902710451
Epoch 2770/10000, Prediction Accuracy = 61.08%, Loss = 0.5112806975841522
Epoch: 2770, Batch Gradient Norm: 9.826929890827435
Epoch: 2770, Batch Gradient Norm after: 9.826929890827435
Epoch 2771/10000, Prediction Accuracy = 61.152%, Loss = 0.5078597009181977
Epoch: 2771, Batch Gradient Norm: 10.9532775878333
Epoch: 2771, Batch Gradient Norm after: 10.9532775878333
Epoch 2772/10000, Prediction Accuracy = 61.013999999999996%, Loss = 0.5140209257602691
Epoch: 2772, Batch Gradient Norm: 12.719099533971582
Epoch: 2772, Batch Gradient Norm after: 12.719099533971582
Epoch 2773/10000, Prediction Accuracy = 61.17%, Loss = 0.5254310488700866
Epoch: 2773, Batch Gradient Norm: 11.716814833570947
Epoch: 2773, Batch Gradient Norm after: 11.716814833570947
Epoch 2774/10000, Prediction Accuracy = 61.036%, Loss = 0.5189121246337891
Epoch: 2774, Batch Gradient Norm: 10.710535052491698
Epoch: 2774, Batch Gradient Norm after: 10.710535052491698
Epoch 2775/10000, Prediction Accuracy = 61.212%, Loss = 0.5151907503604889
Epoch: 2775, Batch Gradient Norm: 8.726721854272219
Epoch: 2775, Batch Gradient Norm after: 8.726721854272219
Epoch 2776/10000, Prediction Accuracy = 61.164%, Loss = 0.5033598124980927
Epoch: 2776, Batch Gradient Norm: 9.662837118196505
Epoch: 2776, Batch Gradient Norm after: 9.662837118196505
Epoch 2777/10000, Prediction Accuracy = 61.164%, Loss = 0.5076915442943573
Epoch: 2777, Batch Gradient Norm: 11.100383617774984
Epoch: 2777, Batch Gradient Norm after: 11.100383617774984
Epoch 2778/10000, Prediction Accuracy = 61.169999999999995%, Loss = 0.5160103619098664
Epoch: 2778, Batch Gradient Norm: 10.815451039429922
Epoch: 2778, Batch Gradient Norm after: 10.815451039429922
Epoch 2779/10000, Prediction Accuracy = 61.086%, Loss = 0.514979898929596
Epoch: 2779, Batch Gradient Norm: 10.654793063383819
Epoch: 2779, Batch Gradient Norm after: 10.654793063383819
Epoch 2780/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.5160927891731262
Epoch: 2780, Batch Gradient Norm: 7.904939910564753
Epoch: 2780, Batch Gradient Norm after: 7.904939910564753
Epoch 2781/10000, Prediction Accuracy = 61.136%, Loss = 0.4989770114421844
Epoch: 2781, Batch Gradient Norm: 9.638340442793222
Epoch: 2781, Batch Gradient Norm after: 9.638340442793222
Epoch 2782/10000, Prediction Accuracy = 61.10999999999999%, Loss = 0.510034692287445
Epoch: 2782, Batch Gradient Norm: 10.225714312017974
Epoch: 2782, Batch Gradient Norm after: 10.225714312017974
Epoch 2783/10000, Prediction Accuracy = 61.138%, Loss = 0.5119354724884033
Epoch: 2783, Batch Gradient Norm: 12.904808093432539
Epoch: 2783, Batch Gradient Norm after: 12.904808093432539
Epoch 2784/10000, Prediction Accuracy = 61.13599999999999%, Loss = 0.5313732743263244
Epoch: 2784, Batch Gradient Norm: 11.667517587236755
Epoch: 2784, Batch Gradient Norm after: 11.667517587236755
Epoch 2785/10000, Prediction Accuracy = 61.218%, Loss = 0.5227206945419312
Epoch: 2785, Batch Gradient Norm: 6.700949809785663
Epoch: 2785, Batch Gradient Norm after: 6.700949809785663
Epoch 2786/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.49279470443725587
Epoch: 2786, Batch Gradient Norm: 7.407110873941412
Epoch: 2786, Batch Gradient Norm after: 7.407110873941412
Epoch 2787/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.49489150047302244
Epoch: 2787, Batch Gradient Norm: 9.854119531577417
Epoch: 2787, Batch Gradient Norm after: 9.854119531577417
Epoch 2788/10000, Prediction Accuracy = 61.092%, Loss = 0.5074053883552552
Epoch: 2788, Batch Gradient Norm: 13.210754998936194
Epoch: 2788, Batch Gradient Norm after: 13.210754998936194
Epoch 2789/10000, Prediction Accuracy = 61.160000000000004%, Loss = 0.5310404658317566
Epoch: 2789, Batch Gradient Norm: 14.045559538001513
Epoch: 2789, Batch Gradient Norm after: 14.045559538001513
Epoch 2790/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5403489232063293
Epoch: 2790, Batch Gradient Norm: 11.567912272044525
Epoch: 2790, Batch Gradient Norm after: 11.567912272044525
Epoch 2791/10000, Prediction Accuracy = 61.242%, Loss = 0.5223555445671082
Epoch: 2791, Batch Gradient Norm: 9.381531229164883
Epoch: 2791, Batch Gradient Norm after: 9.381531229164883
Epoch 2792/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5061699211597442
Epoch: 2792, Batch Gradient Norm: 10.054259624521192
Epoch: 2792, Batch Gradient Norm after: 10.054259624521192
Epoch 2793/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.5082690298557282
Epoch: 2793, Batch Gradient Norm: 13.430331554617082
Epoch: 2793, Batch Gradient Norm after: 13.430331554617082
Epoch 2794/10000, Prediction Accuracy = 61.19200000000001%, Loss = 0.5324905395507813
Epoch: 2794, Batch Gradient Norm: 11.884491279253695
Epoch: 2794, Batch Gradient Norm after: 11.884491279253695
Epoch 2795/10000, Prediction Accuracy = 61.11%, Loss = 0.5210823297500611
Epoch: 2795, Batch Gradient Norm: 9.262569293016579
Epoch: 2795, Batch Gradient Norm after: 9.262569293016579
Epoch 2796/10000, Prediction Accuracy = 61.134%, Loss = 0.5045410990715027
Epoch: 2796, Batch Gradient Norm: 8.855727774855284
Epoch: 2796, Batch Gradient Norm after: 8.855727774855284
Epoch 2797/10000, Prediction Accuracy = 61.226%, Loss = 0.5023327171802521
Epoch: 2797, Batch Gradient Norm: 10.130417176296717
Epoch: 2797, Batch Gradient Norm after: 10.130417176296717
Epoch 2798/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.509886771440506
Epoch: 2798, Batch Gradient Norm: 13.077522788129599
Epoch: 2798, Batch Gradient Norm after: 13.077522788129599
Epoch 2799/10000, Prediction Accuracy = 61.04600000000001%, Loss = 0.5311217308044434
Epoch: 2799, Batch Gradient Norm: 11.20891450204458
Epoch: 2799, Batch Gradient Norm after: 11.20891450204458
Epoch 2800/10000, Prediction Accuracy = 61.11600000000001%, Loss = 0.5173324823379517
Epoch: 2800, Batch Gradient Norm: 9.324015317902806
Epoch: 2800, Batch Gradient Norm after: 9.324015317902806
Epoch 2801/10000, Prediction Accuracy = 61.076%, Loss = 0.505567866563797
Epoch: 2801, Batch Gradient Norm: 9.437942454127347
Epoch: 2801, Batch Gradient Norm after: 9.437942454127347
Epoch 2802/10000, Prediction Accuracy = 61.218%, Loss = 0.5063765168190002
Epoch: 2802, Batch Gradient Norm: 11.562434111851386
Epoch: 2802, Batch Gradient Norm after: 11.562434111851386
Epoch 2803/10000, Prediction Accuracy = 61.10600000000001%, Loss = 0.5169497609138489
Epoch: 2803, Batch Gradient Norm: 14.4369839731911
Epoch: 2803, Batch Gradient Norm after: 14.4369839731911
Epoch 2804/10000, Prediction Accuracy = 61.096000000000004%, Loss = 0.5399331450462341
Epoch: 2804, Batch Gradient Norm: 10.778135664952408
Epoch: 2804, Batch Gradient Norm after: 10.778135664952408
Epoch 2805/10000, Prediction Accuracy = 61.166%, Loss = 0.5141770422458649
Epoch: 2805, Batch Gradient Norm: 7.311191029904961
Epoch: 2805, Batch Gradient Norm after: 7.311191029904961
Epoch 2806/10000, Prediction Accuracy = 61.186%, Loss = 0.49431219696998596
Epoch: 2806, Batch Gradient Norm: 8.69348167341585
Epoch: 2806, Batch Gradient Norm after: 8.69348167341585
Epoch 2807/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.5008152723312378
Epoch: 2807, Batch Gradient Norm: 10.748611617862172
Epoch: 2807, Batch Gradient Norm after: 10.748611617862172
Epoch 2808/10000, Prediction Accuracy = 61.145999999999994%, Loss = 0.5132584810256958
Epoch: 2808, Batch Gradient Norm: 11.21209277006232
Epoch: 2808, Batch Gradient Norm after: 11.21209277006232
Epoch 2809/10000, Prediction Accuracy = 61.188%, Loss = 0.5171266317367553
Epoch: 2809, Batch Gradient Norm: 8.46325079070351
Epoch: 2809, Batch Gradient Norm after: 8.46325079070351
Epoch 2810/10000, Prediction Accuracy = 61.112%, Loss = 0.4994722783565521
Epoch: 2810, Batch Gradient Norm: 8.479004058568432
Epoch: 2810, Batch Gradient Norm after: 8.479004058568432
Epoch 2811/10000, Prediction Accuracy = 61.198%, Loss = 0.49891923666000365
Epoch: 2811, Batch Gradient Norm: 9.458475258212953
Epoch: 2811, Batch Gradient Norm after: 9.458475258212953
Epoch 2812/10000, Prediction Accuracy = 61.04799999999999%, Loss = 0.5039876997470856
Epoch: 2812, Batch Gradient Norm: 13.14026230777402
Epoch: 2812, Batch Gradient Norm after: 13.14026230777402
Epoch 2813/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.5301645278930665
Epoch: 2813, Batch Gradient Norm: 11.974712587577374
Epoch: 2813, Batch Gradient Norm after: 11.974712587577374
Epoch 2814/10000, Prediction Accuracy = 61.132000000000005%, Loss = 0.5211846113204956
Epoch: 2814, Batch Gradient Norm: 11.037094029825543
Epoch: 2814, Batch Gradient Norm after: 11.037094029825543
Epoch 2815/10000, Prediction Accuracy = 61.122%, Loss = 0.5135701060295105
Epoch: 2815, Batch Gradient Norm: 10.688179927372042
Epoch: 2815, Batch Gradient Norm after: 10.688179927372042
Epoch 2816/10000, Prediction Accuracy = 61.25%, Loss = 0.5101933240890503
Epoch: 2816, Batch Gradient Norm: 10.581757675432726
Epoch: 2816, Batch Gradient Norm after: 10.581757675432726
Epoch 2817/10000, Prediction Accuracy = 61.09400000000001%, Loss = 0.5111367881298066
Epoch: 2817, Batch Gradient Norm: 10.754956201958658
Epoch: 2817, Batch Gradient Norm after: 10.754956201958658
Epoch 2818/10000, Prediction Accuracy = 61.178%, Loss = 0.516470468044281
Epoch: 2818, Batch Gradient Norm: 7.535470506035774
Epoch: 2818, Batch Gradient Norm after: 7.535470506035774
Epoch 2819/10000, Prediction Accuracy = 61.138%, Loss = 0.49698627591133115
Epoch: 2819, Batch Gradient Norm: 9.95488608210953
Epoch: 2819, Batch Gradient Norm after: 9.95488608210953
Epoch 2820/10000, Prediction Accuracy = 61.076%, Loss = 0.5099125444889069
Epoch: 2820, Batch Gradient Norm: 11.44462391193658
Epoch: 2820, Batch Gradient Norm after: 11.44462391193658
Epoch 2821/10000, Prediction Accuracy = 61.164%, Loss = 0.5184574961662293
Epoch: 2821, Batch Gradient Norm: 13.275701498108937
Epoch: 2821, Batch Gradient Norm after: 13.275701498108937
Epoch 2822/10000, Prediction Accuracy = 61.112%, Loss = 0.5323013544082642
Epoch: 2822, Batch Gradient Norm: 11.590256306969902
Epoch: 2822, Batch Gradient Norm after: 11.590256306969902
Epoch 2823/10000, Prediction Accuracy = 61.246%, Loss = 0.519897872209549
Epoch: 2823, Batch Gradient Norm: 11.370701791527077
Epoch: 2823, Batch Gradient Norm after: 11.370701791527077
Epoch 2824/10000, Prediction Accuracy = 61.302%, Loss = 0.517830491065979
Epoch: 2824, Batch Gradient Norm: 12.650065750779634
Epoch: 2824, Batch Gradient Norm after: 12.650065750779634
Epoch 2825/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.5282965183258057
Epoch: 2825, Batch Gradient Norm: 10.351338129062459
Epoch: 2825, Batch Gradient Norm after: 10.351338129062459
Epoch 2826/10000, Prediction Accuracy = 61.246%, Loss = 0.5106599986553192
Epoch: 2826, Batch Gradient Norm: 9.366350579419336
Epoch: 2826, Batch Gradient Norm after: 9.366350579419336
Epoch 2827/10000, Prediction Accuracy = 61.238%, Loss = 0.5039512395858765
Epoch: 2827, Batch Gradient Norm: 8.37850862303284
Epoch: 2827, Batch Gradient Norm after: 8.37850862303284
Epoch 2828/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.4982734084129333
Epoch: 2828, Batch Gradient Norm: 10.016638537772659
Epoch: 2828, Batch Gradient Norm after: 10.016638537772659
Epoch 2829/10000, Prediction Accuracy = 61.21%, Loss = 0.5072994649410247
Epoch: 2829, Batch Gradient Norm: 10.651939975612404
Epoch: 2829, Batch Gradient Norm after: 10.651939975612404
Epoch 2830/10000, Prediction Accuracy = 61.214%, Loss = 0.5120903372764587
Epoch: 2830, Batch Gradient Norm: 10.698451507937214
Epoch: 2830, Batch Gradient Norm after: 10.698451507937214
Epoch 2831/10000, Prediction Accuracy = 61.162%, Loss = 0.5135575056076049
Epoch: 2831, Batch Gradient Norm: 10.31311598358476
Epoch: 2831, Batch Gradient Norm after: 10.31311598358476
Epoch 2832/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.509780865907669
Epoch: 2832, Batch Gradient Norm: 11.313502377412831
Epoch: 2832, Batch Gradient Norm after: 11.313502377412831
Epoch 2833/10000, Prediction Accuracy = 61.178%, Loss = 0.5147931337356567
Epoch: 2833, Batch Gradient Norm: 13.090813701320528
Epoch: 2833, Batch Gradient Norm after: 13.090813701320528
Epoch 2834/10000, Prediction Accuracy = 61.138%, Loss = 0.5260979533195496
Epoch: 2834, Batch Gradient Norm: 11.637749991200455
Epoch: 2834, Batch Gradient Norm after: 11.637749991200455
Epoch 2835/10000, Prediction Accuracy = 61.17%, Loss = 0.5152123093605041
Epoch: 2835, Batch Gradient Norm: 10.08195910533767
Epoch: 2835, Batch Gradient Norm after: 10.08195910533767
Epoch 2836/10000, Prediction Accuracy = 61.074%, Loss = 0.5062234580516816
Epoch: 2836, Batch Gradient Norm: 10.258283651179696
Epoch: 2836, Batch Gradient Norm after: 10.258283651179696
Epoch 2837/10000, Prediction Accuracy = 61.348%, Loss = 0.5089041531085968
Epoch: 2837, Batch Gradient Norm: 9.054842533340484
Epoch: 2837, Batch Gradient Norm after: 9.054842533340484
Epoch 2838/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5030556082725525
Epoch: 2838, Batch Gradient Norm: 9.181287921482804
Epoch: 2838, Batch Gradient Norm after: 9.181287921482804
Epoch 2839/10000, Prediction Accuracy = 61.254%, Loss = 0.5032694339752197
Epoch: 2839, Batch Gradient Norm: 10.002719455780849
Epoch: 2839, Batch Gradient Norm after: 10.002719455780849
Epoch 2840/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.5071368098258973
Epoch: 2840, Batch Gradient Norm: 12.135184732962747
Epoch: 2840, Batch Gradient Norm after: 12.135184732962747
Epoch 2841/10000, Prediction Accuracy = 61.11%, Loss = 0.5217069327831269
Epoch: 2841, Batch Gradient Norm: 12.217506961376822
Epoch: 2841, Batch Gradient Norm after: 12.217506961376822
Epoch 2842/10000, Prediction Accuracy = 61.162%, Loss = 0.5272179007530212
Epoch: 2842, Batch Gradient Norm: 7.948490795364875
Epoch: 2842, Batch Gradient Norm after: 7.948490795364875
Epoch 2843/10000, Prediction Accuracy = 61.138%, Loss = 0.4968432903289795
Epoch: 2843, Batch Gradient Norm: 10.357580281855729
Epoch: 2843, Batch Gradient Norm after: 10.357580281855729
Epoch 2844/10000, Prediction Accuracy = 61.222%, Loss = 0.5091181457042694
Epoch: 2844, Batch Gradient Norm: 11.596111110680795
Epoch: 2844, Batch Gradient Norm after: 11.596111110680795
Epoch 2845/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.5175797045230865
Epoch: 2845, Batch Gradient Norm: 10.990657589666434
Epoch: 2845, Batch Gradient Norm after: 10.990657589666434
Epoch 2846/10000, Prediction Accuracy = 61.242%, Loss = 0.514925503730774
Epoch: 2846, Batch Gradient Norm: 9.67134259783308
Epoch: 2846, Batch Gradient Norm after: 9.67134259783308
Epoch 2847/10000, Prediction Accuracy = 61.24399999999999%, Loss = 0.5087653040885926
Epoch: 2847, Batch Gradient Norm: 7.058443780988541
Epoch: 2847, Batch Gradient Norm after: 7.058443780988541
Epoch 2848/10000, Prediction Accuracy = 61.214%, Loss = 0.49244277477264403
Epoch: 2848, Batch Gradient Norm: 12.08966186647672
Epoch: 2848, Batch Gradient Norm after: 12.08966186647672
Epoch 2849/10000, Prediction Accuracy = 61.19999999999999%, Loss = 0.5208724975585938
Epoch: 2849, Batch Gradient Norm: 14.050229191000739
Epoch: 2849, Batch Gradient Norm after: 14.050229191000739
Epoch 2850/10000, Prediction Accuracy = 61.05%, Loss = 0.5367937684059143
Epoch: 2850, Batch Gradient Norm: 10.415496620947572
Epoch: 2850, Batch Gradient Norm after: 10.415496620947572
Epoch 2851/10000, Prediction Accuracy = 61.186%, Loss = 0.5090704023838043
Epoch: 2851, Batch Gradient Norm: 7.802692081444407
Epoch: 2851, Batch Gradient Norm after: 7.802692081444407
Epoch 2852/10000, Prediction Accuracy = 61.098%, Loss = 0.4933731913566589
Epoch: 2852, Batch Gradient Norm: 11.247407332335026
Epoch: 2852, Batch Gradient Norm after: 11.247407332335026
Epoch 2853/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.5115569829940796
Epoch: 2853, Batch Gradient Norm: 13.20995172693192
Epoch: 2853, Batch Gradient Norm after: 13.20995172693192
Epoch 2854/10000, Prediction Accuracy = 61.034000000000006%, Loss = 0.5258452415466308
Epoch: 2854, Batch Gradient Norm: 10.599511487919903
Epoch: 2854, Batch Gradient Norm after: 10.599511487919903
Epoch 2855/10000, Prediction Accuracy = 61.226%, Loss = 0.5082555413246155
Epoch: 2855, Batch Gradient Norm: 9.58002292492396
Epoch: 2855, Batch Gradient Norm after: 9.58002292492396
Epoch 2856/10000, Prediction Accuracy = 61.153999999999996%, Loss = 0.5031552314758301
Epoch: 2856, Batch Gradient Norm: 9.890379209648426
Epoch: 2856, Batch Gradient Norm after: 9.890379209648426
Epoch 2857/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5052427887916565
Epoch: 2857, Batch Gradient Norm: 12.144289929113018
Epoch: 2857, Batch Gradient Norm after: 12.144289929113018
Epoch 2858/10000, Prediction Accuracy = 61.134%, Loss = 0.5221097946166993
Epoch: 2858, Batch Gradient Norm: 11.486237318645992
Epoch: 2858, Batch Gradient Norm after: 11.486237318645992
Epoch 2859/10000, Prediction Accuracy = 61.129999999999995%, Loss = 0.5183230340480804
Epoch: 2859, Batch Gradient Norm: 9.228549027775358
Epoch: 2859, Batch Gradient Norm after: 9.228549027775358
Epoch 2860/10000, Prediction Accuracy = 61.196000000000005%, Loss = 0.5033461391925812
Epoch: 2860, Batch Gradient Norm: 10.414566440463116
Epoch: 2860, Batch Gradient Norm after: 10.414566440463116
Epoch 2861/10000, Prediction Accuracy = 61.254%, Loss = 0.511055338382721
Epoch: 2861, Batch Gradient Norm: 8.053399400578922
Epoch: 2861, Batch Gradient Norm after: 8.053399400578922
Epoch 2862/10000, Prediction Accuracy = 61.186%, Loss = 0.4969476103782654
Epoch: 2862, Batch Gradient Norm: 8.397966780079651
Epoch: 2862, Batch Gradient Norm after: 8.397966780079651
Epoch 2863/10000, Prediction Accuracy = 61.33399999999999%, Loss = 0.4984677195549011
Epoch: 2863, Batch Gradient Norm: 10.451520223261072
Epoch: 2863, Batch Gradient Norm after: 10.451520223261072
Epoch 2864/10000, Prediction Accuracy = 61.23%, Loss = 0.509104996919632
Epoch: 2864, Batch Gradient Norm: 14.706185782317103
Epoch: 2864, Batch Gradient Norm after: 14.706185782317103
Epoch 2865/10000, Prediction Accuracy = 61.11%, Loss = 0.5445403695106507
Epoch: 2865, Batch Gradient Norm: 12.192680410499989
Epoch: 2865, Batch Gradient Norm after: 12.192680410499989
Epoch 2866/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.5228324294090271
Epoch: 2866, Batch Gradient Norm: 9.190289997444141
Epoch: 2866, Batch Gradient Norm after: 9.190289997444141
Epoch 2867/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.5008616268634796
Epoch: 2867, Batch Gradient Norm: 9.32441998038326
Epoch: 2867, Batch Gradient Norm after: 9.32441998038326
Epoch 2868/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.501253855228424
Epoch: 2868, Batch Gradient Norm: 9.333879113617481
Epoch: 2868, Batch Gradient Norm after: 9.333879113617481
Epoch 2869/10000, Prediction Accuracy = 61.202%, Loss = 0.5022708833217621
Epoch: 2869, Batch Gradient Norm: 8.64563407045941
Epoch: 2869, Batch Gradient Norm after: 8.64563407045941
Epoch 2870/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.49895864725112915
Epoch: 2870, Batch Gradient Norm: 11.30366241246237
Epoch: 2870, Batch Gradient Norm after: 11.30366241246237
Epoch 2871/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5156893074512482
Epoch: 2871, Batch Gradient Norm: 12.89056000614649
Epoch: 2871, Batch Gradient Norm after: 12.89056000614649
Epoch 2872/10000, Prediction Accuracy = 61.202%, Loss = 0.527719509601593
Epoch: 2872, Batch Gradient Norm: 12.703458689294768
Epoch: 2872, Batch Gradient Norm after: 12.703458689294768
Epoch 2873/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5265560150146484
Epoch: 2873, Batch Gradient Norm: 8.424483435904289
Epoch: 2873, Batch Gradient Norm after: 8.424483435904289
Epoch 2874/10000, Prediction Accuracy = 61.302%, Loss = 0.49734253287315366
Epoch: 2874, Batch Gradient Norm: 8.469669797174229
Epoch: 2874, Batch Gradient Norm after: 8.469669797174229
Epoch 2875/10000, Prediction Accuracy = 61.14399999999999%, Loss = 0.4969592154026031
Epoch: 2875, Batch Gradient Norm: 11.789747439721069
Epoch: 2875, Batch Gradient Norm after: 11.789747439721069
Epoch 2876/10000, Prediction Accuracy = 61.157999999999994%, Loss = 0.5166162014007568
Epoch: 2876, Batch Gradient Norm: 13.337484430494163
Epoch: 2876, Batch Gradient Norm after: 13.337484430494163
Epoch 2877/10000, Prediction Accuracy = 61.126%, Loss = 0.5285879969596863
Epoch: 2877, Batch Gradient Norm: 11.554074722756782
Epoch: 2877, Batch Gradient Norm after: 11.554074722756782
Epoch 2878/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.5146766602993011
Epoch: 2878, Batch Gradient Norm: 10.800654624327978
Epoch: 2878, Batch Gradient Norm after: 10.800654624327978
Epoch 2879/10000, Prediction Accuracy = 61.274%, Loss = 0.5094054818153382
Epoch: 2879, Batch Gradient Norm: 11.365312881301636
Epoch: 2879, Batch Gradient Norm after: 11.365312881301636
Epoch 2880/10000, Prediction Accuracy = 61.23%, Loss = 0.5136463403701782
Epoch: 2880, Batch Gradient Norm: 11.072498409423726
Epoch: 2880, Batch Gradient Norm after: 11.072498409423726
Epoch 2881/10000, Prediction Accuracy = 61.172000000000004%, Loss = 0.5121992826461792
Epoch: 2881, Batch Gradient Norm: 10.188319342393378
Epoch: 2881, Batch Gradient Norm after: 10.188319342393378
Epoch 2882/10000, Prediction Accuracy = 61.25600000000001%, Loss = 0.5084150314331055
Epoch: 2882, Batch Gradient Norm: 8.017724902315145
Epoch: 2882, Batch Gradient Norm after: 8.017724902315145
Epoch 2883/10000, Prediction Accuracy = 61.13399999999999%, Loss = 0.49618250131607056
Epoch: 2883, Batch Gradient Norm: 9.566645781242268
Epoch: 2883, Batch Gradient Norm after: 9.566645781242268
Epoch 2884/10000, Prediction Accuracy = 61.238%, Loss = 0.5056656062602997
Epoch: 2884, Batch Gradient Norm: 7.772213159130554
Epoch: 2884, Batch Gradient Norm after: 7.772213159130554
Epoch 2885/10000, Prediction Accuracy = 61.16799999999999%, Loss = 0.494774729013443
Epoch: 2885, Batch Gradient Norm: 9.384568847391984
Epoch: 2885, Batch Gradient Norm after: 9.384568847391984
Epoch 2886/10000, Prediction Accuracy = 61.326%, Loss = 0.5025103330612183
Epoch: 2886, Batch Gradient Norm: 11.678476262615877
Epoch: 2886, Batch Gradient Norm after: 11.678476262615877
Epoch 2887/10000, Prediction Accuracy = 61.144000000000005%, Loss = 0.5170297980308532
Epoch: 2887, Batch Gradient Norm: 11.058539849996567
Epoch: 2887, Batch Gradient Norm after: 11.058539849996567
Epoch 2888/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.5116823315620422
Epoch: 2888, Batch Gradient Norm: 10.565880686847262
Epoch: 2888, Batch Gradient Norm after: 10.565880686847262
Epoch 2889/10000, Prediction Accuracy = 61.2%, Loss = 0.5074006199836731
Epoch: 2889, Batch Gradient Norm: 10.903402694238205
Epoch: 2889, Batch Gradient Norm after: 10.903402694238205
Epoch 2890/10000, Prediction Accuracy = 61.288%, Loss = 0.5090233504772186
Epoch: 2890, Batch Gradient Norm: 12.995286092238633
Epoch: 2890, Batch Gradient Norm after: 12.995286092238633
Epoch 2891/10000, Prediction Accuracy = 61.222%, Loss = 0.5262630105018615
Epoch: 2891, Batch Gradient Norm: 12.20081429575497
Epoch: 2891, Batch Gradient Norm after: 12.20081429575497
Epoch 2892/10000, Prediction Accuracy = 61.186%, Loss = 0.5221746325492859
Epoch: 2892, Batch Gradient Norm: 10.612386283995976
Epoch: 2892, Batch Gradient Norm after: 10.612386283995976
Epoch 2893/10000, Prediction Accuracy = 61.27199999999999%, Loss = 0.5094488859176636
Epoch: 2893, Batch Gradient Norm: 11.10523619405698
Epoch: 2893, Batch Gradient Norm after: 11.10523619405698
Epoch 2894/10000, Prediction Accuracy = 61.157999999999994%, Loss = 0.5121820151805878
Epoch: 2894, Batch Gradient Norm: 10.641866666995627
Epoch: 2894, Batch Gradient Norm after: 10.641866666995627
Epoch 2895/10000, Prediction Accuracy = 61.238%, Loss = 0.5096657633781433
Epoch: 2895, Batch Gradient Norm: 10.89748311012872
Epoch: 2895, Batch Gradient Norm after: 10.89748311012872
Epoch 2896/10000, Prediction Accuracy = 61.138%, Loss = 0.5114574611186982
Epoch: 2896, Batch Gradient Norm: 11.54325731999204
Epoch: 2896, Batch Gradient Norm after: 11.54325731999204
Epoch 2897/10000, Prediction Accuracy = 61.178%, Loss = 0.5168608665466309
Epoch: 2897, Batch Gradient Norm: 9.729540975307334
Epoch: 2897, Batch Gradient Norm after: 9.729540975307334
Epoch 2898/10000, Prediction Accuracy = 61.196000000000005%, Loss = 0.5042934775352478
Epoch: 2898, Batch Gradient Norm: 9.08531052602385
Epoch: 2898, Batch Gradient Norm after: 9.08531052602385
Epoch 2899/10000, Prediction Accuracy = 61.29%, Loss = 0.49894940853118896
Epoch: 2899, Batch Gradient Norm: 10.831662526140162
Epoch: 2899, Batch Gradient Norm after: 10.831662526140162
Epoch 2900/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.5087296366691589
Epoch: 2900, Batch Gradient Norm: 10.75696298149585
Epoch: 2900, Batch Gradient Norm after: 10.75696298149585
Epoch 2901/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.5088267683982849
Epoch: 2901, Batch Gradient Norm: 10.017810402574735
Epoch: 2901, Batch Gradient Norm after: 10.017810402574735
Epoch 2902/10000, Prediction Accuracy = 61.238%, Loss = 0.5046355307102204
Epoch: 2902, Batch Gradient Norm: 9.451632275544213
Epoch: 2902, Batch Gradient Norm after: 9.451632275544213
Epoch 2903/10000, Prediction Accuracy = 61.18800000000001%, Loss = 0.501128739118576
Epoch: 2903, Batch Gradient Norm: 10.599554139605315
Epoch: 2903, Batch Gradient Norm after: 10.599554139605315
Epoch 2904/10000, Prediction Accuracy = 61.193999999999996%, Loss = 0.5083249926567077
Epoch: 2904, Batch Gradient Norm: 10.128182260403674
Epoch: 2904, Batch Gradient Norm after: 10.128182260403674
Epoch 2905/10000, Prediction Accuracy = 61.269999999999996%, Loss = 0.5052818953990936
Epoch: 2905, Batch Gradient Norm: 10.343912076358032
Epoch: 2905, Batch Gradient Norm after: 10.343912076358032
Epoch 2906/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.5075873672962189
Epoch: 2906, Batch Gradient Norm: 11.80449661293787
Epoch: 2906, Batch Gradient Norm after: 11.80449661293787
Epoch 2907/10000, Prediction Accuracy = 61.182%, Loss = 0.522362494468689
Epoch: 2907, Batch Gradient Norm: 9.886186179537269
Epoch: 2907, Batch Gradient Norm after: 9.886186179537269
Epoch 2908/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.5039686858654022
Epoch: 2908, Batch Gradient Norm: 13.54894337363012
Epoch: 2908, Batch Gradient Norm after: 13.54894337363012
Epoch 2909/10000, Prediction Accuracy = 61.205999999999996%, Loss = 0.5289290189743042
Epoch: 2909, Batch Gradient Norm: 11.567431637955417
Epoch: 2909, Batch Gradient Norm after: 11.567431637955417
Epoch 2910/10000, Prediction Accuracy = 61.084%, Loss = 0.5145213961601257
Epoch: 2910, Batch Gradient Norm: 10.04699675847446
Epoch: 2910, Batch Gradient Norm after: 10.04699675847446
Epoch 2911/10000, Prediction Accuracy = 61.21400000000001%, Loss = 0.5046343147754669
Epoch: 2911, Batch Gradient Norm: 8.67192107862495
Epoch: 2911, Batch Gradient Norm after: 8.67192107862495
Epoch 2912/10000, Prediction Accuracy = 61.176%, Loss = 0.4967713713645935
Epoch: 2912, Batch Gradient Norm: 11.081635004858224
Epoch: 2912, Batch Gradient Norm after: 11.081635004858224
Epoch 2913/10000, Prediction Accuracy = 61.29%, Loss = 0.5134206295013428
Epoch: 2913, Batch Gradient Norm: 8.000790477186637
Epoch: 2913, Batch Gradient Norm after: 8.000790477186637
Epoch 2914/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.49499266147613524
Epoch: 2914, Batch Gradient Norm: 9.06810473975879
Epoch: 2914, Batch Gradient Norm after: 9.06810473975879
Epoch 2915/10000, Prediction Accuracy = 61.176%, Loss = 0.5003951072692872
Epoch: 2915, Batch Gradient Norm: 10.186797688405283
Epoch: 2915, Batch Gradient Norm after: 10.186797688405283
Epoch 2916/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.5064653337001801
Epoch: 2916, Batch Gradient Norm: 12.115903781519714
Epoch: 2916, Batch Gradient Norm after: 12.115903781519714
Epoch 2917/10000, Prediction Accuracy = 61.218%, Loss = 0.5206652164459229
Epoch: 2917, Batch Gradient Norm: 10.678675975158791
Epoch: 2917, Batch Gradient Norm after: 10.678675975158791
Epoch 2918/10000, Prediction Accuracy = 61.288%, Loss = 0.5097246587276458
Epoch: 2918, Batch Gradient Norm: 10.2567788843371
Epoch: 2918, Batch Gradient Norm after: 10.2567788843371
Epoch 2919/10000, Prediction Accuracy = 61.257999999999996%, Loss = 0.505862045288086
Epoch: 2919, Batch Gradient Norm: 12.268584762213948
Epoch: 2919, Batch Gradient Norm after: 12.268584762213948
Epoch 2920/10000, Prediction Accuracy = 61.152%, Loss = 0.5182372272014618
Epoch: 2920, Batch Gradient Norm: 11.9992443985784
Epoch: 2920, Batch Gradient Norm after: 11.9992443985784
Epoch 2921/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.516681569814682
Epoch: 2921, Batch Gradient Norm: 10.383154463814934
Epoch: 2921, Batch Gradient Norm after: 10.383154463814934
Epoch 2922/10000, Prediction Accuracy = 61.314%, Loss = 0.505598533153534
Epoch: 2922, Batch Gradient Norm: 11.992047671207219
Epoch: 2922, Batch Gradient Norm after: 11.992047671207219
Epoch 2923/10000, Prediction Accuracy = 61.214%, Loss = 0.5198392391204834
Epoch: 2923, Batch Gradient Norm: 10.262506098930494
Epoch: 2923, Batch Gradient Norm after: 10.262506098930494
Epoch 2924/10000, Prediction Accuracy = 61.366%, Loss = 0.507350218296051
Epoch: 2924, Batch Gradient Norm: 10.239777771234365
Epoch: 2924, Batch Gradient Norm after: 10.239777771234365
Epoch 2925/10000, Prediction Accuracy = 61.2%, Loss = 0.5052847445011139
Epoch: 2925, Batch Gradient Norm: 10.429805554629601
Epoch: 2925, Batch Gradient Norm after: 10.429805554629601
Epoch 2926/10000, Prediction Accuracy = 61.239999999999995%, Loss = 0.5045779585838318
Epoch: 2926, Batch Gradient Norm: 11.49453097985155
Epoch: 2926, Batch Gradient Norm after: 11.49453097985155
Epoch 2927/10000, Prediction Accuracy = 61.112%, Loss = 0.5120405554771423
Epoch: 2927, Batch Gradient Norm: 11.819013877627713
Epoch: 2927, Batch Gradient Norm after: 11.819013877627713
Epoch 2928/10000, Prediction Accuracy = 61.206%, Loss = 0.51725994348526
Epoch: 2928, Batch Gradient Norm: 10.779332073208082
Epoch: 2928, Batch Gradient Norm after: 10.779332073208082
Epoch 2929/10000, Prediction Accuracy = 61.181999999999995%, Loss = 0.5088189661502838
Epoch: 2929, Batch Gradient Norm: 11.154458503417478
Epoch: 2929, Batch Gradient Norm after: 11.154458503417478
Epoch 2930/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.5114632308483124
Epoch: 2930, Batch Gradient Norm: 9.391731816108525
Epoch: 2930, Batch Gradient Norm after: 9.391731816108525
Epoch 2931/10000, Prediction Accuracy = 61.282%, Loss = 0.5004211962223053
Epoch: 2931, Batch Gradient Norm: 8.66072125857661
Epoch: 2931, Batch Gradient Norm after: 8.66072125857661
Epoch 2932/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.49567118287086487
Epoch: 2932, Batch Gradient Norm: 10.880605717511338
Epoch: 2932, Batch Gradient Norm after: 10.880605717511338
Epoch 2933/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.5088114678859711
Epoch: 2933, Batch Gradient Norm: 13.152788119498501
Epoch: 2933, Batch Gradient Norm after: 13.152788119498501
Epoch 2934/10000, Prediction Accuracy = 61.29%, Loss = 0.5267803907394409
Epoch: 2934, Batch Gradient Norm: 10.25838677554095
Epoch: 2934, Batch Gradient Norm after: 10.25838677554095
Epoch 2935/10000, Prediction Accuracy = 61.186%, Loss = 0.5055256128311157
Epoch: 2935, Batch Gradient Norm: 9.153252602166122
Epoch: 2935, Batch Gradient Norm after: 9.153252602166122
Epoch 2936/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.49801293611526487
Epoch: 2936, Batch Gradient Norm: 10.9775758334542
Epoch: 2936, Batch Gradient Norm after: 10.9775758334542
Epoch 2937/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5091955959796906
Epoch: 2937, Batch Gradient Norm: 11.808936565351447
Epoch: 2937, Batch Gradient Norm after: 11.808936565351447
Epoch 2938/10000, Prediction Accuracy = 61.215999999999994%, Loss = 0.5167104244232178
Epoch: 2938, Batch Gradient Norm: 8.94310821931048
Epoch: 2938, Batch Gradient Norm after: 8.94310821931048
Epoch 2939/10000, Prediction Accuracy = 61.269999999999996%, Loss = 0.4985243439674377
Epoch: 2939, Batch Gradient Norm: 7.140065434901382
Epoch: 2939, Batch Gradient Norm after: 7.140065434901382
Epoch 2940/10000, Prediction Accuracy = 61.206%, Loss = 0.4891244828701019
Epoch: 2940, Batch Gradient Norm: 9.784947654317447
Epoch: 2940, Batch Gradient Norm after: 9.784947654317447
Epoch 2941/10000, Prediction Accuracy = 61.218%, Loss = 0.5044285476207733
Epoch: 2941, Batch Gradient Norm: 7.958951447843059
Epoch: 2941, Batch Gradient Norm after: 7.958951447843059
Epoch 2942/10000, Prediction Accuracy = 61.17999999999999%, Loss = 0.49282057881355285
Epoch: 2942, Batch Gradient Norm: 10.156197591869988
Epoch: 2942, Batch Gradient Norm after: 10.156197591869988
Epoch 2943/10000, Prediction Accuracy = 61.206%, Loss = 0.5037438452243805
Epoch: 2943, Batch Gradient Norm: 10.529361184750325
Epoch: 2943, Batch Gradient Norm after: 10.529361184750325
Epoch 2944/10000, Prediction Accuracy = 61.202%, Loss = 0.5045702755451202
Epoch: 2944, Batch Gradient Norm: 11.797353949819117
Epoch: 2944, Batch Gradient Norm after: 11.797353949819117
Epoch 2945/10000, Prediction Accuracy = 61.21%, Loss = 0.5130024373531341
Epoch: 2945, Batch Gradient Norm: 12.896863995503766
Epoch: 2945, Batch Gradient Norm after: 12.896863995503766
Epoch 2946/10000, Prediction Accuracy = 61.290000000000006%, Loss = 0.523841142654419
Epoch: 2946, Batch Gradient Norm: 13.053884954438074
Epoch: 2946, Batch Gradient Norm after: 13.053884954438074
Epoch 2947/10000, Prediction Accuracy = 61.168000000000006%, Loss = 0.5271644711494445
Epoch: 2947, Batch Gradient Norm: 11.645463921220792
Epoch: 2947, Batch Gradient Norm after: 11.645463921220792
Epoch 2948/10000, Prediction Accuracy = 61.13000000000001%, Loss = 0.5155749797821045
Epoch: 2948, Batch Gradient Norm: 11.371658396133634
Epoch: 2948, Batch Gradient Norm after: 11.371658396133634
Epoch 2949/10000, Prediction Accuracy = 61.262%, Loss = 0.5124354302883148
Epoch: 2949, Batch Gradient Norm: 10.85401634320254
Epoch: 2949, Batch Gradient Norm after: 10.85401634320254
Epoch 2950/10000, Prediction Accuracy = 61.181999999999995%, Loss = 0.5087132573127746
Epoch: 2950, Batch Gradient Norm: 10.327328206497207
Epoch: 2950, Batch Gradient Norm after: 10.327328206497207
Epoch 2951/10000, Prediction Accuracy = 61.302%, Loss = 0.5053549647331238
Epoch: 2951, Batch Gradient Norm: 10.302191641574352
Epoch: 2951, Batch Gradient Norm after: 10.302191641574352
Epoch 2952/10000, Prediction Accuracy = 61.258%, Loss = 0.5062942147254944
Epoch: 2952, Batch Gradient Norm: 9.500890321617645
Epoch: 2952, Batch Gradient Norm after: 9.500890321617645
Epoch 2953/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5012737810611725
Epoch: 2953, Batch Gradient Norm: 10.040879849767867
Epoch: 2953, Batch Gradient Norm after: 10.040879849767867
Epoch 2954/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.5037935972213745
Epoch: 2954, Batch Gradient Norm: 8.924759448251892
Epoch: 2954, Batch Gradient Norm after: 8.924759448251892
Epoch 2955/10000, Prediction Accuracy = 61.186%, Loss = 0.4971623957157135
Epoch: 2955, Batch Gradient Norm: 8.712693529936118
Epoch: 2955, Batch Gradient Norm after: 8.712693529936118
Epoch 2956/10000, Prediction Accuracy = 61.338%, Loss = 0.494860577583313
Epoch: 2956, Batch Gradient Norm: 9.539402324844694
Epoch: 2956, Batch Gradient Norm after: 9.539402324844694
Epoch 2957/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.49844850301742555
Epoch: 2957, Batch Gradient Norm: 14.473779197549081
Epoch: 2957, Batch Gradient Norm after: 14.473779197549081
Epoch 2958/10000, Prediction Accuracy = 61.222%, Loss = 0.5336649179458618
Epoch: 2958, Batch Gradient Norm: 13.810214906447937
Epoch: 2958, Batch Gradient Norm after: 13.810214906447937
Epoch 2959/10000, Prediction Accuracy = 61.29600000000001%, Loss = 0.5317829012870788
Epoch: 2959, Batch Gradient Norm: 9.560402472031091
Epoch: 2959, Batch Gradient Norm after: 9.560402472031091
Epoch 2960/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.500938493013382
Epoch: 2960, Batch Gradient Norm: 10.786764946902661
Epoch: 2960, Batch Gradient Norm after: 10.786764946902661
Epoch 2961/10000, Prediction Accuracy = 61.188%, Loss = 0.5084277391433716
Epoch: 2961, Batch Gradient Norm: 10.157940161585868
Epoch: 2961, Batch Gradient Norm after: 10.157940161585868
Epoch 2962/10000, Prediction Accuracy = 61.286%, Loss = 0.5035703778266907
Epoch: 2962, Batch Gradient Norm: 11.26171484004376
Epoch: 2962, Batch Gradient Norm after: 11.26171484004376
Epoch 2963/10000, Prediction Accuracy = 61.198%, Loss = 0.5112094640731811
Epoch: 2963, Batch Gradient Norm: 11.198970762376083
Epoch: 2963, Batch Gradient Norm after: 11.198970762376083
Epoch 2964/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.5098606824874878
Epoch: 2964, Batch Gradient Norm: 10.176465901485024
Epoch: 2964, Batch Gradient Norm after: 10.176465901485024
Epoch 2965/10000, Prediction Accuracy = 61.218%, Loss = 0.5029379844665527
Epoch: 2965, Batch Gradient Norm: 9.985331443369983
Epoch: 2965, Batch Gradient Norm after: 9.985331443369983
Epoch 2966/10000, Prediction Accuracy = 61.238%, Loss = 0.5024725377559662
Epoch: 2966, Batch Gradient Norm: 9.605514920312254
Epoch: 2966, Batch Gradient Norm after: 9.605514920312254
Epoch 2967/10000, Prediction Accuracy = 61.218%, Loss = 0.49996851086616517
Epoch: 2967, Batch Gradient Norm: 11.739507475064698
Epoch: 2967, Batch Gradient Norm after: 11.739507475064698
Epoch 2968/10000, Prediction Accuracy = 61.354%, Loss = 0.5156785845756531
Epoch: 2968, Batch Gradient Norm: 8.950550884811285
Epoch: 2968, Batch Gradient Norm after: 8.950550884811285
Epoch 2969/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.4965803325176239
Epoch: 2969, Batch Gradient Norm: 7.906509504508136
Epoch: 2969, Batch Gradient Norm after: 7.906509504508136
Epoch 2970/10000, Prediction Accuracy = 61.274%, Loss = 0.4903859615325928
Epoch: 2970, Batch Gradient Norm: 9.048318812479442
Epoch: 2970, Batch Gradient Norm after: 9.048318812479442
Epoch 2971/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.4951820433139801
Epoch: 2971, Batch Gradient Norm: 11.356945871143857
Epoch: 2971, Batch Gradient Norm after: 11.356945871143857
Epoch 2972/10000, Prediction Accuracy = 61.158%, Loss = 0.5103247225284576
Epoch: 2972, Batch Gradient Norm: 11.467854518597669
Epoch: 2972, Batch Gradient Norm after: 11.467854518597669
Epoch 2973/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.5148991703987121
Epoch: 2973, Batch Gradient Norm: 9.028759919992908
Epoch: 2973, Batch Gradient Norm after: 9.028759919992908
Epoch 2974/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.499314147233963
Epoch: 2974, Batch Gradient Norm: 10.741057069295966
Epoch: 2974, Batch Gradient Norm after: 10.741057069295966
Epoch 2975/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.5106809675693512
Epoch: 2975, Batch Gradient Norm: 11.272962175188024
Epoch: 2975, Batch Gradient Norm after: 11.272962175188024
Epoch 2976/10000, Prediction Accuracy = 61.233999999999995%, Loss = 0.5111521899700164
Epoch: 2976, Batch Gradient Norm: 10.075681938104331
Epoch: 2976, Batch Gradient Norm after: 10.075681938104331
Epoch 2977/10000, Prediction Accuracy = 61.326%, Loss = 0.501571524143219
Epoch: 2977, Batch Gradient Norm: 9.990568277875386
Epoch: 2977, Batch Gradient Norm after: 9.990568277875386
Epoch 2978/10000, Prediction Accuracy = 61.267999999999994%, Loss = 0.5000643968582154
Epoch: 2978, Batch Gradient Norm: 12.850895349140282
Epoch: 2978, Batch Gradient Norm after: 12.850895349140282
Epoch 2979/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.5206223368644715
Epoch: 2979, Batch Gradient Norm: 13.768934097338548
Epoch: 2979, Batch Gradient Norm after: 13.768934097338548
Epoch 2980/10000, Prediction Accuracy = 61.227999999999994%, Loss = 0.5301711559295654
Epoch: 2980, Batch Gradient Norm: 11.410667727056104
Epoch: 2980, Batch Gradient Norm after: 11.410667727056104
Epoch 2981/10000, Prediction Accuracy = 61.098%, Loss = 0.5121545314788818
Epoch: 2981, Batch Gradient Norm: 9.72941962311298
Epoch: 2981, Batch Gradient Norm after: 9.72941962311298
Epoch 2982/10000, Prediction Accuracy = 61.278%, Loss = 0.500291359424591
Epoch: 2982, Batch Gradient Norm: 8.934550965588208
Epoch: 2982, Batch Gradient Norm after: 8.934550965588208
Epoch 2983/10000, Prediction Accuracy = 61.176%, Loss = 0.49491646885871887
Epoch: 2983, Batch Gradient Norm: 9.645036191316615
Epoch: 2983, Batch Gradient Norm after: 9.645036191316615
Epoch 2984/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.4985389053821564
Epoch: 2984, Batch Gradient Norm: 8.797364384788212
Epoch: 2984, Batch Gradient Norm after: 8.797364384788212
Epoch 2985/10000, Prediction Accuracy = 61.2%, Loss = 0.4937080919742584
Epoch: 2985, Batch Gradient Norm: 10.157824628540862
Epoch: 2985, Batch Gradient Norm after: 10.157824628540862
Epoch 2986/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.5017183363437653
Epoch: 2986, Batch Gradient Norm: 12.238150002294589
Epoch: 2986, Batch Gradient Norm after: 12.238150002294589
Epoch 2987/10000, Prediction Accuracy = 61.288%, Loss = 0.5216312408447266
Epoch: 2987, Batch Gradient Norm: 11.939932546290388
Epoch: 2987, Batch Gradient Norm after: 11.939932546290388
Epoch 2988/10000, Prediction Accuracy = 61.212%, Loss = 0.5186695456504822
Epoch: 2988, Batch Gradient Norm: 9.270244049305077
Epoch: 2988, Batch Gradient Norm after: 9.270244049305077
Epoch 2989/10000, Prediction Accuracy = 61.238%, Loss = 0.49959715008735656
Epoch: 2989, Batch Gradient Norm: 9.255823422962736
Epoch: 2989, Batch Gradient Norm after: 9.255823422962736
Epoch 2990/10000, Prediction Accuracy = 61.36%, Loss = 0.49784326553344727
Epoch: 2990, Batch Gradient Norm: 8.647780748991803
Epoch: 2990, Batch Gradient Norm after: 8.647780748991803
Epoch 2991/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.4946469247341156
Epoch: 2991, Batch Gradient Norm: 10.562973583626377
Epoch: 2991, Batch Gradient Norm after: 10.562973583626377
Epoch 2992/10000, Prediction Accuracy = 61.298%, Loss = 0.5059645056724549
Epoch: 2992, Batch Gradient Norm: 13.102979163086518
Epoch: 2992, Batch Gradient Norm after: 13.102979163086518
Epoch 2993/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.5273032069206238
Epoch: 2993, Batch Gradient Norm: 10.658967569660295
Epoch: 2993, Batch Gradient Norm after: 10.658967569660295
Epoch 2994/10000, Prediction Accuracy = 61.26800000000001%, Loss = 0.505368584394455
Epoch: 2994, Batch Gradient Norm: 11.841519971286196
Epoch: 2994, Batch Gradient Norm after: 11.841519971286196
Epoch 2995/10000, Prediction Accuracy = 61.33599999999999%, Loss = 0.5118535041809082
Epoch: 2995, Batch Gradient Norm: 11.760249517773536
Epoch: 2995, Batch Gradient Norm after: 11.760249517773536
Epoch 2996/10000, Prediction Accuracy = 61.302%, Loss = 0.5117251217365265
Epoch: 2996, Batch Gradient Norm: 9.746703872404197
Epoch: 2996, Batch Gradient Norm after: 9.746703872404197
Epoch 2997/10000, Prediction Accuracy = 61.298%, Loss = 0.4992745339870453
Epoch: 2997, Batch Gradient Norm: 9.293830637591515
Epoch: 2997, Batch Gradient Norm after: 9.293830637591515
Epoch 2998/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.49695269465446473
Epoch: 2998, Batch Gradient Norm: 8.821857916505802
Epoch: 2998, Batch Gradient Norm after: 8.821857916505802
Epoch 2999/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.4937967896461487
Epoch: 2999, Batch Gradient Norm: 12.354521970336029
Epoch: 2999, Batch Gradient Norm after: 12.354521970336029
Epoch 3000/10000, Prediction Accuracy = 61.217999999999996%, Loss = 0.5173679709434509
Epoch: 3000, Batch Gradient Norm: 12.372055121568946
Epoch: 3000, Batch Gradient Norm after: 12.372055121568946
Epoch 3001/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.5174464464187623
Epoch: 3001, Batch Gradient Norm: 10.488467984577714
Epoch: 3001, Batch Gradient Norm after: 10.488467984577714
Epoch 3002/10000, Prediction Accuracy = 61.208000000000006%, Loss = 0.5040982961654663
Epoch: 3002, Batch Gradient Norm: 10.533926451627048
Epoch: 3002, Batch Gradient Norm after: 10.533926451627048
Epoch 3003/10000, Prediction Accuracy = 61.30999999999999%, Loss = 0.5046911060810089
Epoch: 3003, Batch Gradient Norm: 8.854476387599924
Epoch: 3003, Batch Gradient Norm after: 8.854476387599924
Epoch 3004/10000, Prediction Accuracy = 61.25%, Loss = 0.494689416885376
Epoch: 3004, Batch Gradient Norm: 10.514982041529356
Epoch: 3004, Batch Gradient Norm after: 10.514982041529356
Epoch 3005/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.5058375120162963
Epoch: 3005, Batch Gradient Norm: 11.162645405633782
Epoch: 3005, Batch Gradient Norm after: 11.162645405633782
Epoch 3006/10000, Prediction Accuracy = 61.428%, Loss = 0.5104894518852234
Epoch: 3006, Batch Gradient Norm: 9.7431488787631
Epoch: 3006, Batch Gradient Norm after: 9.7431488787631
Epoch 3007/10000, Prediction Accuracy = 61.218%, Loss = 0.5020745515823364
Epoch: 3007, Batch Gradient Norm: 8.031926368091973
Epoch: 3007, Batch Gradient Norm after: 8.031926368091973
Epoch 3008/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.49005170464515685
Epoch: 3008, Batch Gradient Norm: 10.350885118008781
Epoch: 3008, Batch Gradient Norm after: 10.350885118008781
Epoch 3009/10000, Prediction Accuracy = 61.21%, Loss = 0.5025479614734649
Epoch: 3009, Batch Gradient Norm: 12.346730318476475
Epoch: 3009, Batch Gradient Norm after: 12.346730318476475
Epoch 3010/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.517701244354248
Epoch: 3010, Batch Gradient Norm: 11.731131538014436
Epoch: 3010, Batch Gradient Norm after: 11.731131538014436
Epoch 3011/10000, Prediction Accuracy = 61.218%, Loss = 0.5130728363990784
Epoch: 3011, Batch Gradient Norm: 10.44407174968734
Epoch: 3011, Batch Gradient Norm after: 10.44407174968734
Epoch 3012/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.503977644443512
Epoch: 3012, Batch Gradient Norm: 10.8305823199384
Epoch: 3012, Batch Gradient Norm after: 10.8305823199384
Epoch 3013/10000, Prediction Accuracy = 61.266000000000005%, Loss = 0.506301736831665
Epoch: 3013, Batch Gradient Norm: 11.67837535303573
Epoch: 3013, Batch Gradient Norm after: 11.67837535303573
Epoch 3014/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5130364120006561
Epoch: 3014, Batch Gradient Norm: 9.301581757042078
Epoch: 3014, Batch Gradient Norm after: 9.301581757042078
Epoch 3015/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.4976495921611786
Epoch: 3015, Batch Gradient Norm: 9.231828757568213
Epoch: 3015, Batch Gradient Norm after: 9.231828757568213
Epoch 3016/10000, Prediction Accuracy = 61.266000000000005%, Loss = 0.4969885230064392
Epoch: 3016, Batch Gradient Norm: 10.513078831939058
Epoch: 3016, Batch Gradient Norm after: 10.513078831939058
Epoch 3017/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.5033759534358978
Epoch: 3017, Batch Gradient Norm: 11.944178576496004
Epoch: 3017, Batch Gradient Norm after: 11.944178576496004
Epoch 3018/10000, Prediction Accuracy = 61.202%, Loss = 0.5131362676620483
Epoch: 3018, Batch Gradient Norm: 11.936896891609484
Epoch: 3018, Batch Gradient Norm after: 11.936896891609484
Epoch 3019/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.512269139289856
Epoch: 3019, Batch Gradient Norm: 10.886110653593354
Epoch: 3019, Batch Gradient Norm after: 10.886110653593354
Epoch 3020/10000, Prediction Accuracy = 61.25%, Loss = 0.5045861840248108
Epoch: 3020, Batch Gradient Norm: 11.156429293392405
Epoch: 3020, Batch Gradient Norm after: 11.156429293392405
Epoch 3021/10000, Prediction Accuracy = 61.221999999999994%, Loss = 0.5069324970245361
Epoch: 3021, Batch Gradient Norm: 8.79465934060142
Epoch: 3021, Batch Gradient Norm after: 8.79465934060142
Epoch 3022/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.4929149866104126
Epoch: 3022, Batch Gradient Norm: 9.825244507667088
Epoch: 3022, Batch Gradient Norm after: 9.825244507667088
Epoch 3023/10000, Prediction Accuracy = 61.274%, Loss = 0.4995238363742828
Epoch: 3023, Batch Gradient Norm: 10.541085131984447
Epoch: 3023, Batch Gradient Norm after: 10.541085131984447
Epoch 3024/10000, Prediction Accuracy = 61.15%, Loss = 0.5061393737792969
Epoch: 3024, Batch Gradient Norm: 10.782307624915909
Epoch: 3024, Batch Gradient Norm after: 10.782307624915909
Epoch 3025/10000, Prediction Accuracy = 61.298%, Loss = 0.507802015542984
Epoch: 3025, Batch Gradient Norm: 10.21936549626234
Epoch: 3025, Batch Gradient Norm after: 10.21936549626234
Epoch 3026/10000, Prediction Accuracy = 61.290000000000006%, Loss = 0.5028263866901398
Epoch: 3026, Batch Gradient Norm: 10.023316442694957
Epoch: 3026, Batch Gradient Norm after: 10.023316442694957
Epoch 3027/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.5011091828346252
Epoch: 3027, Batch Gradient Norm: 10.858001585118542
Epoch: 3027, Batch Gradient Norm after: 10.858001585118542
Epoch 3028/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5069487631320954
Epoch: 3028, Batch Gradient Norm: 11.067600213565823
Epoch: 3028, Batch Gradient Norm after: 11.067600213565823
Epoch 3029/10000, Prediction Accuracy = 61.269999999999996%, Loss = 0.5069120228290558
Epoch: 3029, Batch Gradient Norm: 12.055481282823251
Epoch: 3029, Batch Gradient Norm after: 12.055481282823251
Epoch 3030/10000, Prediction Accuracy = 61.262%, Loss = 0.5144478559494019
Epoch: 3030, Batch Gradient Norm: 12.718200822839796
Epoch: 3030, Batch Gradient Norm after: 12.718200822839796
Epoch 3031/10000, Prediction Accuracy = 61.257999999999996%, Loss = 0.521302330493927
Epoch: 3031, Batch Gradient Norm: 8.782624073665367
Epoch: 3031, Batch Gradient Norm after: 8.782624073665367
Epoch 3032/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.4936915338039398
Epoch: 3032, Batch Gradient Norm: 8.784223970326321
Epoch: 3032, Batch Gradient Norm after: 8.784223970326321
Epoch 3033/10000, Prediction Accuracy = 61.336%, Loss = 0.4927737653255463
Epoch: 3033, Batch Gradient Norm: 9.211141028782121
Epoch: 3033, Batch Gradient Norm after: 9.211141028782121
Epoch 3034/10000, Prediction Accuracy = 61.23199999999999%, Loss = 0.49437330961227416
Epoch: 3034, Batch Gradient Norm: 10.713463218253322
Epoch: 3034, Batch Gradient Norm after: 10.713463218253322
Epoch 3035/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.5035482823848725
Epoch: 3035, Batch Gradient Norm: 10.293391902446828
Epoch: 3035, Batch Gradient Norm after: 10.293391902446828
Epoch 3036/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5003113269805908
Epoch: 3036, Batch Gradient Norm: 11.912946503457581
Epoch: 3036, Batch Gradient Norm after: 11.912946503457581
Epoch 3037/10000, Prediction Accuracy = 61.25200000000001%, Loss = 0.5118970394134521
Epoch: 3037, Batch Gradient Norm: 10.756881067154442
Epoch: 3037, Batch Gradient Norm after: 10.756881067154442
Epoch 3038/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.5052038609981537
Epoch: 3038, Batch Gradient Norm: 9.382542126630112
Epoch: 3038, Batch Gradient Norm after: 9.382542126630112
Epoch 3039/10000, Prediction Accuracy = 61.186%, Loss = 0.49699116349220274
Epoch: 3039, Batch Gradient Norm: 9.47765053137441
Epoch: 3039, Batch Gradient Norm after: 9.47765053137441
Epoch 3040/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.4978814601898193
Epoch: 3040, Batch Gradient Norm: 12.954417181582098
Epoch: 3040, Batch Gradient Norm after: 12.954417181582098
Epoch 3041/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5220249772071839
Epoch: 3041, Batch Gradient Norm: 12.72597513821828
Epoch: 3041, Batch Gradient Norm after: 12.72597513821828
Epoch 3042/10000, Prediction Accuracy = 61.262%, Loss = 0.5190122306346894
Epoch: 3042, Batch Gradient Norm: 9.587590955582545
Epoch: 3042, Batch Gradient Norm after: 9.587590955582545
Epoch 3043/10000, Prediction Accuracy = 61.30799999999999%, Loss = 0.4971619784832001
Epoch: 3043, Batch Gradient Norm: 8.13775235221775
Epoch: 3043, Batch Gradient Norm after: 8.13775235221775
Epoch 3044/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.4888882517814636
Epoch: 3044, Batch Gradient Norm: 10.785407842085904
Epoch: 3044, Batch Gradient Norm after: 10.785407842085904
Epoch 3045/10000, Prediction Accuracy = 61.2%, Loss = 0.505605411529541
Epoch: 3045, Batch Gradient Norm: 11.441100356844697
Epoch: 3045, Batch Gradient Norm after: 11.441100356844697
Epoch 3046/10000, Prediction Accuracy = 61.25600000000001%, Loss = 0.5120772361755371
Epoch: 3046, Batch Gradient Norm: 7.815831044815823
Epoch: 3046, Batch Gradient Norm after: 7.815831044815823
Epoch 3047/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.48765690326690675
Epoch: 3047, Batch Gradient Norm: 9.756454700624737
Epoch: 3047, Batch Gradient Norm after: 9.756454700624737
Epoch 3048/10000, Prediction Accuracy = 61.302%, Loss = 0.4956494867801666
Epoch: 3048, Batch Gradient Norm: 14.549755169952412
Epoch: 3048, Batch Gradient Norm after: 14.549755169952412
Epoch 3049/10000, Prediction Accuracy = 61.312%, Loss = 0.5319645643234253
Epoch: 3049, Batch Gradient Norm: 10.240844882952617
Epoch: 3049, Batch Gradient Norm after: 10.240844882952617
Epoch 3050/10000, Prediction Accuracy = 61.242000000000004%, Loss = 0.5017400026321411
Epoch: 3050, Batch Gradient Norm: 8.361376651579018
Epoch: 3050, Batch Gradient Norm after: 8.361376651579018
Epoch 3051/10000, Prediction Accuracy = 61.322%, Loss = 0.4901745319366455
Epoch: 3051, Batch Gradient Norm: 7.919129080707918
Epoch: 3051, Batch Gradient Norm after: 7.919129080707918
Epoch 3052/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.4869585156440735
Epoch: 3052, Batch Gradient Norm: 12.374305857134617
Epoch: 3052, Batch Gradient Norm after: 12.374305857134617
Epoch 3053/10000, Prediction Accuracy = 61.343999999999994%, Loss = 0.5144967555999755
Epoch: 3053, Batch Gradient Norm: 12.15778577990761
Epoch: 3053, Batch Gradient Norm after: 12.15778577990761
Epoch 3054/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.5164580702781677
Epoch: 3054, Batch Gradient Norm: 10.533269968951341
Epoch: 3054, Batch Gradient Norm after: 10.533269968951341
Epoch 3055/10000, Prediction Accuracy = 61.324%, Loss = 0.5028863251209259
Epoch: 3055, Batch Gradient Norm: 11.362806209764466
Epoch: 3055, Batch Gradient Norm after: 11.362806209764466
Epoch 3056/10000, Prediction Accuracy = 61.21%, Loss = 0.5092977643013
Epoch: 3056, Batch Gradient Norm: 11.547275634056957
Epoch: 3056, Batch Gradient Norm after: 11.547275634056957
Epoch 3057/10000, Prediction Accuracy = 61.28399999999999%, Loss = 0.5123383522033691
Epoch: 3057, Batch Gradient Norm: 9.276875741764657
Epoch: 3057, Batch Gradient Norm after: 9.276875741764657
Epoch 3058/10000, Prediction Accuracy = 61.232000000000006%, Loss = 0.4956895887851715
Epoch: 3058, Batch Gradient Norm: 10.155125487904057
Epoch: 3058, Batch Gradient Norm after: 10.155125487904057
Epoch 3059/10000, Prediction Accuracy = 61.36600000000001%, Loss = 0.5007946133613587
Epoch: 3059, Batch Gradient Norm: 9.40418130189696
Epoch: 3059, Batch Gradient Norm after: 9.40418130189696
Epoch 3060/10000, Prediction Accuracy = 61.233999999999995%, Loss = 0.4955792248249054
Epoch: 3060, Batch Gradient Norm: 11.123814420030339
Epoch: 3060, Batch Gradient Norm after: 11.123814420030339
Epoch 3061/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.5067030906677246
Epoch: 3061, Batch Gradient Norm: 11.138188499361684
Epoch: 3061, Batch Gradient Norm after: 11.138188499361684
Epoch 3062/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.508523803949356
Epoch: 3062, Batch Gradient Norm: 9.632750419289817
Epoch: 3062, Batch Gradient Norm after: 9.632750419289817
Epoch 3063/10000, Prediction Accuracy = 61.282000000000004%, Loss = 0.49769097566604614
Epoch: 3063, Batch Gradient Norm: 9.83381775968051
Epoch: 3063, Batch Gradient Norm after: 9.83381775968051
Epoch 3064/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.4967418611049652
Epoch: 3064, Batch Gradient Norm: 12.323687623663984
Epoch: 3064, Batch Gradient Norm after: 12.323687623663984
Epoch 3065/10000, Prediction Accuracy = 61.336%, Loss = 0.5138352334499359
Epoch: 3065, Batch Gradient Norm: 11.249831664112657
Epoch: 3065, Batch Gradient Norm after: 11.249831664112657
Epoch 3066/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.506185132265091
Epoch: 3066, Batch Gradient Norm: 10.770374703561732
Epoch: 3066, Batch Gradient Norm after: 10.770374703561732
Epoch 3067/10000, Prediction Accuracy = 61.312%, Loss = 0.5039365649223327
Epoch: 3067, Batch Gradient Norm: 11.217210690668054
Epoch: 3067, Batch Gradient Norm after: 11.217210690668054
Epoch 3068/10000, Prediction Accuracy = 61.29%, Loss = 0.5093546211719513
Epoch: 3068, Batch Gradient Norm: 9.54704468511607
Epoch: 3068, Batch Gradient Norm after: 9.54704468511607
Epoch 3069/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.4963372528553009
Epoch: 3069, Batch Gradient Norm: 10.441747054623782
Epoch: 3069, Batch Gradient Norm after: 10.441747054623782
Epoch 3070/10000, Prediction Accuracy = 61.23%, Loss = 0.5012546241283417
Epoch: 3070, Batch Gradient Norm: 11.800323513760109
Epoch: 3070, Batch Gradient Norm after: 11.800323513760109
Epoch 3071/10000, Prediction Accuracy = 61.224000000000004%, Loss = 0.5113788783550263
Epoch: 3071, Batch Gradient Norm: 8.91160875955083
Epoch: 3071, Batch Gradient Norm after: 8.91160875955083
Epoch 3072/10000, Prediction Accuracy = 61.266000000000005%, Loss = 0.49319716095924376
Epoch: 3072, Batch Gradient Norm: 8.042126632944209
Epoch: 3072, Batch Gradient Norm after: 8.042126632944209
Epoch 3073/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.48814367651939394
Epoch: 3073, Batch Gradient Norm: 8.03259772103292
Epoch: 3073, Batch Gradient Norm after: 8.03259772103292
Epoch 3074/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.487525999546051
Epoch: 3074, Batch Gradient Norm: 11.331729752290888
Epoch: 3074, Batch Gradient Norm after: 11.331729752290888
Epoch 3075/10000, Prediction Accuracy = 61.39200000000001%, Loss = 0.5089632511138916
Epoch: 3075, Batch Gradient Norm: 11.36182569708152
Epoch: 3075, Batch Gradient Norm after: 11.36182569708152
Epoch 3076/10000, Prediction Accuracy = 61.20399999999999%, Loss = 0.509333211183548
Epoch: 3076, Batch Gradient Norm: 10.727309856463723
Epoch: 3076, Batch Gradient Norm after: 10.727309856463723
Epoch 3077/10000, Prediction Accuracy = 61.386%, Loss = 0.5044871389865875
Epoch: 3077, Batch Gradient Norm: 11.87290013576597
Epoch: 3077, Batch Gradient Norm after: 11.87290013576597
Epoch 3078/10000, Prediction Accuracy = 61.294000000000004%, Loss = 0.5132363379001618
Epoch: 3078, Batch Gradient Norm: 9.065461686389645
Epoch: 3078, Batch Gradient Norm after: 9.065461686389645
Epoch 3079/10000, Prediction Accuracy = 61.326%, Loss = 0.49466381072998045
Epoch: 3079, Batch Gradient Norm: 8.352052055394816
Epoch: 3079, Batch Gradient Norm after: 8.352052055394816
Epoch 3080/10000, Prediction Accuracy = 61.29%, Loss = 0.4901763737201691
Epoch: 3080, Batch Gradient Norm: 8.8830125295706
Epoch: 3080, Batch Gradient Norm after: 8.8830125295706
Epoch 3081/10000, Prediction Accuracy = 61.324%, Loss = 0.4918314337730408
Epoch: 3081, Batch Gradient Norm: 11.723240083141677
Epoch: 3081, Batch Gradient Norm after: 11.723240083141677
Epoch 3082/10000, Prediction Accuracy = 61.19199999999999%, Loss = 0.509022307395935
Epoch: 3082, Batch Gradient Norm: 12.994309235592706
Epoch: 3082, Batch Gradient Norm after: 12.994309235592706
Epoch 3083/10000, Prediction Accuracy = 61.272000000000006%, Loss = 0.5195202112197876
Epoch: 3083, Batch Gradient Norm: 11.044183458035857
Epoch: 3083, Batch Gradient Norm after: 11.044183458035857
Epoch 3084/10000, Prediction Accuracy = 61.209999999999994%, Loss = 0.5025888919830322
Epoch: 3084, Batch Gradient Norm: 14.334525405773618
Epoch: 3084, Batch Gradient Norm after: 14.334525405773618
Epoch 3085/10000, Prediction Accuracy = 61.3%, Loss = 0.5273144483566284
Epoch: 3085, Batch Gradient Norm: 11.955460805899303
Epoch: 3085, Batch Gradient Norm after: 11.955460805899303
Epoch 3086/10000, Prediction Accuracy = 61.312%, Loss = 0.5100290715694428
Epoch: 3086, Batch Gradient Norm: 8.322368510618283
Epoch: 3086, Batch Gradient Norm after: 8.322368510618283
Epoch 3087/10000, Prediction Accuracy = 61.31400000000001%, Loss = 0.4881473481655121
Epoch: 3087, Batch Gradient Norm: 7.686693276333345
Epoch: 3087, Batch Gradient Norm after: 7.686693276333345
Epoch 3088/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.48497734069824217
Epoch: 3088, Batch Gradient Norm: 9.647316080286656
Epoch: 3088, Batch Gradient Norm after: 9.647316080286656
Epoch 3089/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4980309188365936
Epoch: 3089, Batch Gradient Norm: 11.77306623057941
Epoch: 3089, Batch Gradient Norm after: 11.77306623057941
Epoch 3090/10000, Prediction Accuracy = 61.3%, Loss = 0.5120288312435151
Epoch: 3090, Batch Gradient Norm: 12.197616470386762
Epoch: 3090, Batch Gradient Norm after: 12.197616470386762
Epoch 3091/10000, Prediction Accuracy = 61.266000000000005%, Loss = 0.5149802684783935
Epoch: 3091, Batch Gradient Norm: 10.394087891260865
Epoch: 3091, Batch Gradient Norm after: 10.394087891260865
Epoch 3092/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5010491847991944
Epoch: 3092, Batch Gradient Norm: 8.593495375030209
Epoch: 3092, Batch Gradient Norm after: 8.593495375030209
Epoch 3093/10000, Prediction Accuracy = 61.372%, Loss = 0.4895860552787781
Epoch: 3093, Batch Gradient Norm: 9.559854118274323
Epoch: 3093, Batch Gradient Norm after: 9.559854118274323
Epoch 3094/10000, Prediction Accuracy = 61.306%, Loss = 0.49466070532798767
Epoch: 3094, Batch Gradient Norm: 10.997683595978513
Epoch: 3094, Batch Gradient Norm after: 10.997683595978513
Epoch 3095/10000, Prediction Accuracy = 61.426%, Loss = 0.5034839749336243
Epoch: 3095, Batch Gradient Norm: 12.156216425901084
Epoch: 3095, Batch Gradient Norm after: 12.156216425901084
Epoch 3096/10000, Prediction Accuracy = 61.32000000000001%, Loss = 0.5111840963363647
Epoch: 3096, Batch Gradient Norm: 11.49500743287896
Epoch: 3096, Batch Gradient Norm after: 11.49500743287896
Epoch 3097/10000, Prediction Accuracy = 61.226%, Loss = 0.5066163837909698
Epoch: 3097, Batch Gradient Norm: 12.054691833717968
Epoch: 3097, Batch Gradient Norm after: 12.054691833717968
Epoch 3098/10000, Prediction Accuracy = 61.336%, Loss = 0.5133662402629853
Epoch: 3098, Batch Gradient Norm: 8.786646446605374
Epoch: 3098, Batch Gradient Norm after: 8.786646446605374
Epoch 3099/10000, Prediction Accuracy = 61.2%, Loss = 0.4922175109386444
Epoch: 3099, Batch Gradient Norm: 8.242815255618252
Epoch: 3099, Batch Gradient Norm after: 8.242815255618252
Epoch 3100/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.4886156678199768
Epoch: 3100, Batch Gradient Norm: 8.552614608510343
Epoch: 3100, Batch Gradient Norm after: 8.552614608510343
Epoch 3101/10000, Prediction Accuracy = 61.29%, Loss = 0.48889670372009275
Epoch: 3101, Batch Gradient Norm: 10.592015139717237
Epoch: 3101, Batch Gradient Norm after: 10.592015139717237
Epoch 3102/10000, Prediction Accuracy = 61.23%, Loss = 0.5015585958957672
Epoch: 3102, Batch Gradient Norm: 12.007301635892624
Epoch: 3102, Batch Gradient Norm after: 12.007301635892624
Epoch 3103/10000, Prediction Accuracy = 61.288%, Loss = 0.5155643939971923
Epoch: 3103, Batch Gradient Norm: 6.9947521056720685
Epoch: 3103, Batch Gradient Norm after: 6.9947521056720685
Epoch 3104/10000, Prediction Accuracy = 61.242%, Loss = 0.4833909273147583
Epoch: 3104, Batch Gradient Norm: 9.057254694223289
Epoch: 3104, Batch Gradient Norm after: 9.057254694223289
Epoch 3105/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.49253299832344055
Epoch: 3105, Batch Gradient Norm: 10.285101919629753
Epoch: 3105, Batch Gradient Norm after: 10.285101919629753
Epoch 3106/10000, Prediction Accuracy = 61.314%, Loss = 0.49808918833732607
Epoch: 3106, Batch Gradient Norm: 12.381811012792948
Epoch: 3106, Batch Gradient Norm after: 12.381811012792948
Epoch 3107/10000, Prediction Accuracy = 61.248000000000005%, Loss = 0.5122554183006287
Epoch: 3107, Batch Gradient Norm: 13.652359172391696
Epoch: 3107, Batch Gradient Norm after: 13.652359172391696
Epoch 3108/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.5232731223106384
Epoch: 3108, Batch Gradient Norm: 12.876678997924119
Epoch: 3108, Batch Gradient Norm after: 12.876678997924119
Epoch 3109/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.5201048135757447
Epoch: 3109, Batch Gradient Norm: 8.969024525866917
Epoch: 3109, Batch Gradient Norm after: 8.969024525866917
Epoch 3110/10000, Prediction Accuracy = 61.251999999999995%, Loss = 0.49280655980110166
Epoch: 3110, Batch Gradient Norm: 9.535575839964308
Epoch: 3110, Batch Gradient Norm after: 9.535575839964308
Epoch 3111/10000, Prediction Accuracy = 61.37199999999999%, Loss = 0.49539039134979246
Epoch: 3111, Batch Gradient Norm: 10.10216026661258
Epoch: 3111, Batch Gradient Norm after: 10.10216026661258
Epoch 3112/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.4979116201400757
Epoch: 3112, Batch Gradient Norm: 11.1105775291169
Epoch: 3112, Batch Gradient Norm after: 11.1105775291169
Epoch 3113/10000, Prediction Accuracy = 61.220000000000006%, Loss = 0.5048417985439301
Epoch: 3113, Batch Gradient Norm: 9.962265178601637
Epoch: 3113, Batch Gradient Norm after: 9.962265178601637
Epoch 3114/10000, Prediction Accuracy = 61.38599999999999%, Loss = 0.49705013632774353
Epoch: 3114, Batch Gradient Norm: 9.413760001269836
Epoch: 3114, Batch Gradient Norm after: 9.413760001269836
Epoch 3115/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.49422268867492675
Epoch: 3115, Batch Gradient Norm: 11.133295586616763
Epoch: 3115, Batch Gradient Norm after: 11.133295586616763
Epoch 3116/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.5051687479019165
Epoch: 3116, Batch Gradient Norm: 11.796229805755308
Epoch: 3116, Batch Gradient Norm after: 11.796229805755308
Epoch 3117/10000, Prediction Accuracy = 61.324%, Loss = 0.5086794853210449
Epoch: 3117, Batch Gradient Norm: 11.597595901608257
Epoch: 3117, Batch Gradient Norm after: 11.597595901608257
Epoch 3118/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.5071725189685822
Epoch: 3118, Batch Gradient Norm: 10.931562057091991
Epoch: 3118, Batch Gradient Norm after: 10.931562057091991
Epoch 3119/10000, Prediction Accuracy = 61.33399999999999%, Loss = 0.5025750696659088
Epoch: 3119, Batch Gradient Norm: 9.631364685132997
Epoch: 3119, Batch Gradient Norm after: 9.631364685132997
Epoch 3120/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.49464868307113646
Epoch: 3120, Batch Gradient Norm: 8.8347512212204
Epoch: 3120, Batch Gradient Norm after: 8.8347512212204
Epoch 3121/10000, Prediction Accuracy = 61.274%, Loss = 0.48973312973976135
Epoch: 3121, Batch Gradient Norm: 8.904943048258733
Epoch: 3121, Batch Gradient Norm after: 8.904943048258733
Epoch 3122/10000, Prediction Accuracy = 61.436%, Loss = 0.4900484323501587
Epoch: 3122, Batch Gradient Norm: 8.561861273216172
Epoch: 3122, Batch Gradient Norm after: 8.561861273216172
Epoch 3123/10000, Prediction Accuracy = 61.20799999999999%, Loss = 0.488084089756012
Epoch: 3123, Batch Gradient Norm: 11.373755078886225
Epoch: 3123, Batch Gradient Norm after: 11.373755078886225
Epoch 3124/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.5075882136821747
Epoch: 3124, Batch Gradient Norm: 8.993039300383607
Epoch: 3124, Batch Gradient Norm after: 8.993039300383607
Epoch 3125/10000, Prediction Accuracy = 61.269999999999996%, Loss = 0.49250114560127256
Epoch: 3125, Batch Gradient Norm: 10.52319626869921
Epoch: 3125, Batch Gradient Norm after: 10.52319626869921
Epoch 3126/10000, Prediction Accuracy = 61.396%, Loss = 0.501615846157074
Epoch: 3126, Batch Gradient Norm: 13.384360929540291
Epoch: 3126, Batch Gradient Norm after: 13.384360929540291
Epoch 3127/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.5290043234825135
Epoch: 3127, Batch Gradient Norm: 8.286891709505861
Epoch: 3127, Batch Gradient Norm after: 8.286891709505861
Epoch 3128/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.4888733685016632
Epoch: 3128, Batch Gradient Norm: 10.758808379952207
Epoch: 3128, Batch Gradient Norm after: 10.758808379952207
Epoch 3129/10000, Prediction Accuracy = 61.386%, Loss = 0.49960840940475465
Epoch: 3129, Batch Gradient Norm: 16.118896715294802
Epoch: 3129, Batch Gradient Norm after: 16.118896715294802
Epoch 3130/10000, Prediction Accuracy = 61.114%, Loss = 0.546204435825348
Epoch: 3130, Batch Gradient Norm: 11.077113596228477
Epoch: 3130, Batch Gradient Norm after: 11.077113596228477
Epoch 3131/10000, Prediction Accuracy = 61.318%, Loss = 0.5045010685920716
Epoch: 3131, Batch Gradient Norm: 8.43568041510631
Epoch: 3131, Batch Gradient Norm after: 8.43568041510631
Epoch 3132/10000, Prediction Accuracy = 61.336%, Loss = 0.4873826622962952
Epoch: 3132, Batch Gradient Norm: 9.427514310494367
Epoch: 3132, Batch Gradient Norm after: 9.427514310494367
Epoch 3133/10000, Prediction Accuracy = 61.348%, Loss = 0.4924638032913208
Epoch: 3133, Batch Gradient Norm: 9.252864525764528
Epoch: 3133, Batch Gradient Norm after: 9.252864525764528
Epoch 3134/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.4911907196044922
Epoch: 3134, Batch Gradient Norm: 10.793845145935096
Epoch: 3134, Batch Gradient Norm after: 10.793845145935096
Epoch 3135/10000, Prediction Accuracy = 61.364%, Loss = 0.5008181810379029
Epoch: 3135, Batch Gradient Norm: 10.802464998994047
Epoch: 3135, Batch Gradient Norm after: 10.802464998994047
Epoch 3136/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.5001246094703674
Epoch: 3136, Batch Gradient Norm: 12.056291031537656
Epoch: 3136, Batch Gradient Norm after: 12.056291031537656
Epoch 3137/10000, Prediction Accuracy = 61.260000000000005%, Loss = 0.5093328595161438
Epoch: 3137, Batch Gradient Norm: 10.68482917903828
Epoch: 3137, Batch Gradient Norm after: 10.68482917903828
Epoch 3138/10000, Prediction Accuracy = 61.38199999999999%, Loss = 0.5002236425876617
Epoch: 3138, Batch Gradient Norm: 9.832782744815665
Epoch: 3138, Batch Gradient Norm after: 9.832782744815665
Epoch 3139/10000, Prediction Accuracy = 61.352%, Loss = 0.49587944746017454
Epoch: 3139, Batch Gradient Norm: 10.277076999673579
Epoch: 3139, Batch Gradient Norm after: 10.277076999673579
Epoch 3140/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.4999316930770874
Epoch: 3140, Batch Gradient Norm: 12.475154439713513
Epoch: 3140, Batch Gradient Norm after: 12.475154439713513
Epoch 3141/10000, Prediction Accuracy = 61.36600000000001%, Loss = 0.5146441698074341
Epoch: 3141, Batch Gradient Norm: 9.693890326408336
Epoch: 3141, Batch Gradient Norm after: 9.693890326408336
Epoch 3142/10000, Prediction Accuracy = 61.291999999999994%, Loss = 0.4941697120666504
Epoch: 3142, Batch Gradient Norm: 8.805994238504123
Epoch: 3142, Batch Gradient Norm after: 8.805994238504123
Epoch 3143/10000, Prediction Accuracy = 61.39399999999999%, Loss = 0.48852155208587644
Epoch: 3143, Batch Gradient Norm: 8.229468330698015
Epoch: 3143, Batch Gradient Norm after: 8.229468330698015
Epoch 3144/10000, Prediction Accuracy = 61.362%, Loss = 0.4853805422782898
Epoch: 3144, Batch Gradient Norm: 10.699785009434612
Epoch: 3144, Batch Gradient Norm after: 10.699785009434612
Epoch 3145/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.5006429553031921
Epoch: 3145, Batch Gradient Norm: 11.370371678459168
Epoch: 3145, Batch Gradient Norm after: 11.370371678459168
Epoch 3146/10000, Prediction Accuracy = 61.395999999999994%, Loss = 0.5066788196563721
Epoch: 3146, Batch Gradient Norm: 9.71761861327897
Epoch: 3146, Batch Gradient Norm after: 9.71761861327897
Epoch 3147/10000, Prediction Accuracy = 61.303999999999995%, Loss = 0.4938844382762909
Epoch: 3147, Batch Gradient Norm: 11.913174282184139
Epoch: 3147, Batch Gradient Norm after: 11.913174282184139
Epoch 3148/10000, Prediction Accuracy = 61.302%, Loss = 0.5099229335784912
Epoch: 3148, Batch Gradient Norm: 10.35836550029572
Epoch: 3148, Batch Gradient Norm after: 10.35836550029572
Epoch 3149/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.5006677627563476
Epoch: 3149, Batch Gradient Norm: 7.9842742584463355
Epoch: 3149, Batch Gradient Norm after: 7.9842742584463355
Epoch 3150/10000, Prediction Accuracy = 61.30400000000001%, Loss = 0.4852947473526001
Epoch: 3150, Batch Gradient Norm: 7.810060362535411
Epoch: 3150, Batch Gradient Norm after: 7.810060362535411
Epoch 3151/10000, Prediction Accuracy = 61.402%, Loss = 0.4835473716259003
Epoch: 3151, Batch Gradient Norm: 9.23479596286111
Epoch: 3151, Batch Gradient Norm after: 9.23479596286111
Epoch 3152/10000, Prediction Accuracy = 61.27%, Loss = 0.48996280431747435
Epoch: 3152, Batch Gradient Norm: 12.848327315734439
Epoch: 3152, Batch Gradient Norm after: 12.848327315734439
Epoch 3153/10000, Prediction Accuracy = 61.38000000000001%, Loss = 0.5126800298690796
Epoch: 3153, Batch Gradient Norm: 14.377540448450734
Epoch: 3153, Batch Gradient Norm after: 14.377540448450734
Epoch 3154/10000, Prediction Accuracy = 61.236000000000004%, Loss = 0.5273275732994079
Epoch: 3154, Batch Gradient Norm: 10.242656719213805
Epoch: 3154, Batch Gradient Norm after: 10.242656719213805
Epoch 3155/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.49679386615753174
Epoch: 3155, Batch Gradient Norm: 8.87154394453416
Epoch: 3155, Batch Gradient Norm after: 8.87154394453416
Epoch 3156/10000, Prediction Accuracy = 61.39000000000001%, Loss = 0.4885620176792145
Epoch: 3156, Batch Gradient Norm: 12.312210080963737
Epoch: 3156, Batch Gradient Norm after: 12.312210080963737
Epoch 3157/10000, Prediction Accuracy = 61.279999999999994%, Loss = 0.5160248279571533
Epoch: 3157, Batch Gradient Norm: 9.314407052988612
Epoch: 3157, Batch Gradient Norm after: 9.314407052988612
Epoch 3158/10000, Prediction Accuracy = 61.364%, Loss = 0.4949373364448547
Epoch: 3158, Batch Gradient Norm: 7.680497618692854
Epoch: 3158, Batch Gradient Norm after: 7.680497618692854
Epoch 3159/10000, Prediction Accuracy = 61.29%, Loss = 0.48338822722435
Epoch: 3159, Batch Gradient Norm: 11.147125973333369
Epoch: 3159, Batch Gradient Norm after: 11.147125973333369
Epoch 3160/10000, Prediction Accuracy = 61.35%, Loss = 0.5021756947040558
Epoch: 3160, Batch Gradient Norm: 11.62621481450127
Epoch: 3160, Batch Gradient Norm after: 11.62621481450127
Epoch 3161/10000, Prediction Accuracy = 61.26800000000001%, Loss = 0.5063183009624481
Epoch: 3161, Batch Gradient Norm: 11.806910888742559
Epoch: 3161, Batch Gradient Norm after: 11.806910888742559
Epoch 3162/10000, Prediction Accuracy = 61.318000000000005%, Loss = 0.507289457321167
Epoch: 3162, Batch Gradient Norm: 13.542478748641475
Epoch: 3162, Batch Gradient Norm after: 13.542478748641475
Epoch 3163/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.5214412808418274
Epoch: 3163, Batch Gradient Norm: 10.90365104128431
Epoch: 3163, Batch Gradient Norm after: 10.90365104128431
Epoch 3164/10000, Prediction Accuracy = 61.314%, Loss = 0.5021308302879334
Epoch: 3164, Batch Gradient Norm: 9.412202304438017
Epoch: 3164, Batch Gradient Norm after: 9.412202304438017
Epoch 3165/10000, Prediction Accuracy = 61.33%, Loss = 0.4919160783290863
Epoch: 3165, Batch Gradient Norm: 9.497315283940432
Epoch: 3165, Batch Gradient Norm after: 9.497315283940432
Epoch 3166/10000, Prediction Accuracy = 61.336%, Loss = 0.4927453756332397
Epoch: 3166, Batch Gradient Norm: 9.809392763936398
Epoch: 3166, Batch Gradient Norm after: 9.809392763936398
Epoch 3167/10000, Prediction Accuracy = 61.314%, Loss = 0.4947072625160217
Epoch: 3167, Batch Gradient Norm: 9.834220663379897
Epoch: 3167, Batch Gradient Norm after: 9.834220663379897
Epoch 3168/10000, Prediction Accuracy = 61.436%, Loss = 0.4957104742527008
Epoch: 3168, Batch Gradient Norm: 9.081348874371226
Epoch: 3168, Batch Gradient Norm after: 9.081348874371226
Epoch 3169/10000, Prediction Accuracy = 61.302%, Loss = 0.49061896800994875
Epoch: 3169, Batch Gradient Norm: 9.884222606733625
Epoch: 3169, Batch Gradient Norm after: 9.884222606733625
Epoch 3170/10000, Prediction Accuracy = 61.46400000000001%, Loss = 0.4972431719303131
Epoch: 3170, Batch Gradient Norm: 8.79437505907416
Epoch: 3170, Batch Gradient Norm after: 8.79437505907416
Epoch 3171/10000, Prediction Accuracy = 61.338%, Loss = 0.49009106755256654
Epoch: 3171, Batch Gradient Norm: 10.96788435793902
Epoch: 3171, Batch Gradient Norm after: 10.96788435793902
Epoch 3172/10000, Prediction Accuracy = 61.42%, Loss = 0.5026331424713135
Epoch: 3172, Batch Gradient Norm: 12.54178113847973
Epoch: 3172, Batch Gradient Norm after: 12.54178113847973
Epoch 3173/10000, Prediction Accuracy = 61.388%, Loss = 0.5129628777503967
Epoch: 3173, Batch Gradient Norm: 11.491260341748655
Epoch: 3173, Batch Gradient Norm after: 11.491260341748655
Epoch 3174/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.5053674459457398
Epoch: 3174, Batch Gradient Norm: 10.298799334640274
Epoch: 3174, Batch Gradient Norm after: 10.298799334640274
Epoch 3175/10000, Prediction Accuracy = 61.42%, Loss = 0.49765058755874636
Epoch: 3175, Batch Gradient Norm: 8.278628631980341
Epoch: 3175, Batch Gradient Norm after: 8.278628631980341
Epoch 3176/10000, Prediction Accuracy = 61.35%, Loss = 0.4855035364627838
Epoch: 3176, Batch Gradient Norm: 8.947734902938624
Epoch: 3176, Batch Gradient Norm after: 8.947734902938624
Epoch 3177/10000, Prediction Accuracy = 61.294%, Loss = 0.48902947902679444
Epoch: 3177, Batch Gradient Norm: 9.15103393483594
Epoch: 3177, Batch Gradient Norm after: 9.15103393483594
Epoch 3178/10000, Prediction Accuracy = 61.39%, Loss = 0.4900436639785767
Epoch: 3178, Batch Gradient Norm: 11.951450566944656
Epoch: 3178, Batch Gradient Norm after: 11.951450566944656
Epoch 3179/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.5078333914279938
Epoch: 3179, Batch Gradient Norm: 12.399630197810607
Epoch: 3179, Batch Gradient Norm after: 12.399630197810607
Epoch 3180/10000, Prediction Accuracy = 61.326%, Loss = 0.510614413022995
Epoch: 3180, Batch Gradient Norm: 11.297155024630513
Epoch: 3180, Batch Gradient Norm after: 11.297155024630513
Epoch 3181/10000, Prediction Accuracy = 61.3%, Loss = 0.5022374927997589
Epoch: 3181, Batch Gradient Norm: 10.35753037523612
Epoch: 3181, Batch Gradient Norm after: 10.35753037523612
Epoch 3182/10000, Prediction Accuracy = 61.238%, Loss = 0.49580371379852295
Epoch: 3182, Batch Gradient Norm: 11.449502940510206
Epoch: 3182, Batch Gradient Norm after: 11.449502940510206
Epoch 3183/10000, Prediction Accuracy = 61.364%, Loss = 0.5048278152942658
Epoch: 3183, Batch Gradient Norm: 9.64886291721754
Epoch: 3183, Batch Gradient Norm after: 9.64886291721754
Epoch 3184/10000, Prediction Accuracy = 61.266000000000005%, Loss = 0.4926161527633667
Epoch: 3184, Batch Gradient Norm: 9.505702646977424
Epoch: 3184, Batch Gradient Norm after: 9.505702646977424
Epoch 3185/10000, Prediction Accuracy = 61.38000000000001%, Loss = 0.49117566347122193
Epoch: 3185, Batch Gradient Norm: 10.941706690500162
Epoch: 3185, Batch Gradient Norm after: 10.941706690500162
Epoch 3186/10000, Prediction Accuracy = 61.39399999999999%, Loss = 0.5021538734436035
Epoch: 3186, Batch Gradient Norm: 10.149016061372718
Epoch: 3186, Batch Gradient Norm after: 10.149016061372718
Epoch 3187/10000, Prediction Accuracy = 61.275999999999996%, Loss = 0.4985509097576141
Epoch: 3187, Batch Gradient Norm: 10.727962299866201
Epoch: 3187, Batch Gradient Norm after: 10.727962299866201
Epoch 3188/10000, Prediction Accuracy = 61.336%, Loss = 0.4988475561141968
Epoch: 3188, Batch Gradient Norm: 15.270449735474722
Epoch: 3188, Batch Gradient Norm after: 15.270449735474722
Epoch 3189/10000, Prediction Accuracy = 61.334%, Loss = 0.5348236083984375
Epoch: 3189, Batch Gradient Norm: 10.00312319766291
Epoch: 3189, Batch Gradient Norm after: 10.00312319766291
Epoch 3190/10000, Prediction Accuracy = 61.28399999999999%, Loss = 0.49473847150802613
Epoch: 3190, Batch Gradient Norm: 7.8791573600934965
Epoch: 3190, Batch Gradient Norm after: 7.8791573600934965
Epoch 3191/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.482713919878006
Epoch: 3191, Batch Gradient Norm: 8.246451584213222
Epoch: 3191, Batch Gradient Norm after: 8.246451584213222
Epoch 3192/10000, Prediction Accuracy = 61.33%, Loss = 0.48489991426467893
Epoch: 3192, Batch Gradient Norm: 10.723991118977388
Epoch: 3192, Batch Gradient Norm after: 10.723991118977388
Epoch 3193/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.4997261106967926
Epoch: 3193, Batch Gradient Norm: 10.209050733112099
Epoch: 3193, Batch Gradient Norm after: 10.209050733112099
Epoch 3194/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.4957166612148285
Epoch: 3194, Batch Gradient Norm: 10.449849575555653
Epoch: 3194, Batch Gradient Norm after: 10.449849575555653
Epoch 3195/10000, Prediction Accuracy = 61.302%, Loss = 0.4965801417827606
Epoch: 3195, Batch Gradient Norm: 10.798604893020755
Epoch: 3195, Batch Gradient Norm after: 10.798604893020755
Epoch 3196/10000, Prediction Accuracy = 61.414%, Loss = 0.5007589399814606
Epoch: 3196, Batch Gradient Norm: 9.927298532192744
Epoch: 3196, Batch Gradient Norm after: 9.927298532192744
Epoch 3197/10000, Prediction Accuracy = 61.27%, Loss = 0.49638631939888
Epoch: 3197, Batch Gradient Norm: 8.25958269991458
Epoch: 3197, Batch Gradient Norm after: 8.25958269991458
Epoch 3198/10000, Prediction Accuracy = 61.33%, Loss = 0.4850085139274597
Epoch: 3198, Batch Gradient Norm: 11.897273348543415
Epoch: 3198, Batch Gradient Norm after: 11.897273348543415
Epoch 3199/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.5083452522754669
Epoch: 3199, Batch Gradient Norm: 11.454732814894827
Epoch: 3199, Batch Gradient Norm after: 11.454732814894827
Epoch 3200/10000, Prediction Accuracy = 61.384%, Loss = 0.5045641839504242
Epoch: 3200, Batch Gradient Norm: 10.680909009805477
Epoch: 3200, Batch Gradient Norm after: 10.680909009805477
Epoch 3201/10000, Prediction Accuracy = 61.412%, Loss = 0.49860355257987976
Epoch: 3201, Batch Gradient Norm: 11.369841125672101
Epoch: 3201, Batch Gradient Norm after: 11.369841125672101
Epoch 3202/10000, Prediction Accuracy = 61.322%, Loss = 0.5017522633075714
Epoch: 3202, Batch Gradient Norm: 11.457697362026192
Epoch: 3202, Batch Gradient Norm after: 11.457697362026192
Epoch 3203/10000, Prediction Accuracy = 61.386%, Loss = 0.5020144402980804
Epoch: 3203, Batch Gradient Norm: 10.494709963869488
Epoch: 3203, Batch Gradient Norm after: 10.494709963869488
Epoch 3204/10000, Prediction Accuracy = 61.36%, Loss = 0.49687079191207884
Epoch: 3204, Batch Gradient Norm: 8.444846557134962
Epoch: 3204, Batch Gradient Norm after: 8.444846557134962
Epoch 3205/10000, Prediction Accuracy = 61.388%, Loss = 0.48448081612586974
Epoch: 3205, Batch Gradient Norm: 9.8760619749142
Epoch: 3205, Batch Gradient Norm after: 9.8760619749142
Epoch 3206/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.4932775139808655
Epoch: 3206, Batch Gradient Norm: 10.606476285217006
Epoch: 3206, Batch Gradient Norm after: 10.606476285217006
Epoch 3207/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.49944949746131895
Epoch: 3207, Batch Gradient Norm: 10.835326926993709
Epoch: 3207, Batch Gradient Norm after: 10.835326926993709
Epoch 3208/10000, Prediction Accuracy = 61.432%, Loss = 0.500495308637619
Epoch: 3208, Batch Gradient Norm: 11.598336221081542
Epoch: 3208, Batch Gradient Norm after: 11.598336221081542
Epoch 3209/10000, Prediction Accuracy = 61.39%, Loss = 0.5042683243751526
Epoch: 3209, Batch Gradient Norm: 11.295093364347753
Epoch: 3209, Batch Gradient Norm after: 11.295093364347753
Epoch 3210/10000, Prediction Accuracy = 61.37800000000001%, Loss = 0.5013609886169433
Epoch: 3210, Batch Gradient Norm: 9.861671030722295
Epoch: 3210, Batch Gradient Norm after: 9.861671030722295
Epoch 3211/10000, Prediction Accuracy = 61.37399999999999%, Loss = 0.4923220455646515
Epoch: 3211, Batch Gradient Norm: 11.260816382694268
Epoch: 3211, Batch Gradient Norm after: 11.260816382694268
Epoch 3212/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.5048176407814026
Epoch: 3212, Batch Gradient Norm: 10.57981082967723
Epoch: 3212, Batch Gradient Norm after: 10.57981082967723
Epoch 3213/10000, Prediction Accuracy = 61.306%, Loss = 0.4992836058139801
Epoch: 3213, Batch Gradient Norm: 11.456578119337186
Epoch: 3213, Batch Gradient Norm after: 11.456578119337186
Epoch 3214/10000, Prediction Accuracy = 61.462%, Loss = 0.5040905892848968
Epoch: 3214, Batch Gradient Norm: 11.812148376871187
Epoch: 3214, Batch Gradient Norm after: 11.812148376871187
Epoch 3215/10000, Prediction Accuracy = 61.372%, Loss = 0.5061176538467407
Epoch: 3215, Batch Gradient Norm: 9.506086841439737
Epoch: 3215, Batch Gradient Norm after: 9.506086841439737
Epoch 3216/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.4914584577083588
Epoch: 3216, Batch Gradient Norm: 9.173959229618115
Epoch: 3216, Batch Gradient Norm after: 9.173959229618115
Epoch 3217/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.48869011402130125
Epoch: 3217, Batch Gradient Norm: 9.73484747068654
Epoch: 3217, Batch Gradient Norm after: 9.73484747068654
Epoch 3218/10000, Prediction Accuracy = 61.302%, Loss = 0.4911896765232086
Epoch: 3218, Batch Gradient Norm: 12.29561312239856
Epoch: 3218, Batch Gradient Norm after: 12.29561312239856
Epoch 3219/10000, Prediction Accuracy = 61.462%, Loss = 0.5102538347244263
Epoch: 3219, Batch Gradient Norm: 8.853807151168965
Epoch: 3219, Batch Gradient Norm after: 8.853807151168965
Epoch 3220/10000, Prediction Accuracy = 61.254%, Loss = 0.48884740471839905
Epoch: 3220, Batch Gradient Norm: 8.609655044580983
Epoch: 3220, Batch Gradient Norm after: 8.609655044580983
Epoch 3221/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4865225672721863
Epoch: 3221, Batch Gradient Norm: 9.19286185639819
Epoch: 3221, Batch Gradient Norm after: 9.19286185639819
Epoch 3222/10000, Prediction Accuracy = 61.364%, Loss = 0.48864774107933046
Epoch: 3222, Batch Gradient Norm: 11.955970988888307
Epoch: 3222, Batch Gradient Norm after: 11.955970988888307
Epoch 3223/10000, Prediction Accuracy = 61.338%, Loss = 0.5065628945827484
Epoch: 3223, Batch Gradient Norm: 10.054736602460844
Epoch: 3223, Batch Gradient Norm after: 10.054736602460844
Epoch 3224/10000, Prediction Accuracy = 61.416%, Loss = 0.49257235527038573
Epoch: 3224, Batch Gradient Norm: 10.006405800537385
Epoch: 3224, Batch Gradient Norm after: 10.006405800537385
Epoch 3225/10000, Prediction Accuracy = 61.372%, Loss = 0.49088231921195985
Epoch: 3225, Batch Gradient Norm: 11.720138837121336
Epoch: 3225, Batch Gradient Norm after: 11.720138837121336
Epoch 3226/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.5012314021587372
Epoch: 3226, Batch Gradient Norm: 12.566336511076752
Epoch: 3226, Batch Gradient Norm after: 12.566336511076752
Epoch 3227/10000, Prediction Accuracy = 61.372%, Loss = 0.5093609988689423
Epoch: 3227, Batch Gradient Norm: 11.400471584224315
Epoch: 3227, Batch Gradient Norm after: 11.400471584224315
Epoch 3228/10000, Prediction Accuracy = 61.284000000000006%, Loss = 0.5038897573947907
Epoch: 3228, Batch Gradient Norm: 8.969711178897583
Epoch: 3228, Batch Gradient Norm after: 8.969711178897583
Epoch 3229/10000, Prediction Accuracy = 61.388%, Loss = 0.4892635405063629
Epoch: 3229, Batch Gradient Norm: 9.766835620628806
Epoch: 3229, Batch Gradient Norm after: 9.766835620628806
Epoch 3230/10000, Prediction Accuracy = 61.376%, Loss = 0.4925800085067749
Epoch: 3230, Batch Gradient Norm: 12.758441778621043
Epoch: 3230, Batch Gradient Norm after: 12.758441778621043
Epoch 3231/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.5113404929637909
Epoch: 3231, Batch Gradient Norm: 12.162892326038207
Epoch: 3231, Batch Gradient Norm after: 12.162892326038207
Epoch 3232/10000, Prediction Accuracy = 61.362%, Loss = 0.5074386537075043
Epoch: 3232, Batch Gradient Norm: 8.11802490434006
Epoch: 3232, Batch Gradient Norm after: 8.11802490434006
Epoch 3233/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.4820687770843506
Epoch: 3233, Batch Gradient Norm: 8.19041598896659
Epoch: 3233, Batch Gradient Norm after: 8.19041598896659
Epoch 3234/10000, Prediction Accuracy = 61.452%, Loss = 0.48310239911079406
Epoch: 3234, Batch Gradient Norm: 8.449981614184642
Epoch: 3234, Batch Gradient Norm after: 8.449981614184642
Epoch 3235/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.48668213486671447
Epoch: 3235, Batch Gradient Norm: 9.842334229135412
Epoch: 3235, Batch Gradient Norm after: 9.842334229135412
Epoch 3236/10000, Prediction Accuracy = 61.44199999999999%, Loss = 0.4942838191986084
Epoch: 3236, Batch Gradient Norm: 9.41458365967828
Epoch: 3236, Batch Gradient Norm after: 9.41458365967828
Epoch 3237/10000, Prediction Accuracy = 61.354%, Loss = 0.49059096574783323
Epoch: 3237, Batch Gradient Norm: 9.779169944121795
Epoch: 3237, Batch Gradient Norm after: 9.779169944121795
Epoch 3238/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.49247167110443113
Epoch: 3238, Batch Gradient Norm: 11.295333579600756
Epoch: 3238, Batch Gradient Norm after: 11.295333579600756
Epoch 3239/10000, Prediction Accuracy = 61.376%, Loss = 0.5009339869022369
Epoch: 3239, Batch Gradient Norm: 14.772207764460486
Epoch: 3239, Batch Gradient Norm after: 14.772207764460486
Epoch 3240/10000, Prediction Accuracy = 61.288%, Loss = 0.531512176990509
Epoch: 3240, Batch Gradient Norm: 11.697087797145576
Epoch: 3240, Batch Gradient Norm after: 11.697087797145576
Epoch 3241/10000, Prediction Accuracy = 61.25599999999999%, Loss = 0.5058350026607513
Epoch: 3241, Batch Gradient Norm: 9.718225571776756
Epoch: 3241, Batch Gradient Norm after: 9.718225571776756
Epoch 3242/10000, Prediction Accuracy = 61.483999999999995%, Loss = 0.4916830062866211
Epoch: 3242, Batch Gradient Norm: 9.774215338276122
Epoch: 3242, Batch Gradient Norm after: 9.774215338276122
Epoch 3243/10000, Prediction Accuracy = 61.346000000000004%, Loss = 0.49185497760772706
Epoch: 3243, Batch Gradient Norm: 10.140383390243432
Epoch: 3243, Batch Gradient Norm after: 10.140383390243432
Epoch 3244/10000, Prediction Accuracy = 61.424%, Loss = 0.4947836875915527
Epoch: 3244, Batch Gradient Norm: 9.98753327817735
Epoch: 3244, Batch Gradient Norm after: 9.98753327817735
Epoch 3245/10000, Prediction Accuracy = 61.342%, Loss = 0.49310255646705625
Epoch: 3245, Batch Gradient Norm: 9.379473470804683
Epoch: 3245, Batch Gradient Norm after: 9.379473470804683
Epoch 3246/10000, Prediction Accuracy = 61.274%, Loss = 0.48933923840522764
Epoch: 3246, Batch Gradient Norm: 10.860077869159223
Epoch: 3246, Batch Gradient Norm after: 10.860077869159223
Epoch 3247/10000, Prediction Accuracy = 61.426%, Loss = 0.49878840446472167
Epoch: 3247, Batch Gradient Norm: 9.926964051845404
Epoch: 3247, Batch Gradient Norm after: 9.926964051845404
Epoch 3248/10000, Prediction Accuracy = 61.254%, Loss = 0.49172887206077576
Epoch: 3248, Batch Gradient Norm: 10.619728243110869
Epoch: 3248, Batch Gradient Norm after: 10.619728243110869
Epoch 3249/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.49591341614723206
Epoch: 3249, Batch Gradient Norm: 9.580298552852618
Epoch: 3249, Batch Gradient Norm after: 9.580298552852618
Epoch 3250/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.4890517294406891
Epoch: 3250, Batch Gradient Norm: 10.784830767358565
Epoch: 3250, Batch Gradient Norm after: 10.784830767358565
Epoch 3251/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.49694878458976743
Epoch: 3251, Batch Gradient Norm: 12.931450666817994
Epoch: 3251, Batch Gradient Norm after: 12.931450666817994
Epoch 3252/10000, Prediction Accuracy = 61.29200000000001%, Loss = 0.5143315315246582
Epoch: 3252, Batch Gradient Norm: 11.580656052759497
Epoch: 3252, Batch Gradient Norm after: 11.580656052759497
Epoch 3253/10000, Prediction Accuracy = 61.366%, Loss = 0.501356428861618
Epoch: 3253, Batch Gradient Norm: 10.872334936145888
Epoch: 3253, Batch Gradient Norm after: 10.872334936145888
Epoch 3254/10000, Prediction Accuracy = 61.334%, Loss = 0.49640082120895385
Epoch: 3254, Batch Gradient Norm: 10.332020861488939
Epoch: 3254, Batch Gradient Norm after: 10.332020861488939
Epoch 3255/10000, Prediction Accuracy = 61.38399999999999%, Loss = 0.4936582326889038
Epoch: 3255, Batch Gradient Norm: 9.834189329815354
Epoch: 3255, Batch Gradient Norm after: 9.834189329815354
Epoch 3256/10000, Prediction Accuracy = 61.330000000000005%, Loss = 0.4911160469055176
Epoch: 3256, Batch Gradient Norm: 10.22964943823246
Epoch: 3256, Batch Gradient Norm after: 10.22964943823246
Epoch 3257/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.49401724338531494
Epoch: 3257, Batch Gradient Norm: 9.747898557987037
Epoch: 3257, Batch Gradient Norm after: 9.747898557987037
Epoch 3258/10000, Prediction Accuracy = 61.35%, Loss = 0.4902359962463379
Epoch: 3258, Batch Gradient Norm: 11.34490465145636
Epoch: 3258, Batch Gradient Norm after: 11.34490465145636
Epoch 3259/10000, Prediction Accuracy = 61.354000000000006%, Loss = 0.5019904792308807
Epoch: 3259, Batch Gradient Norm: 11.440516523953402
Epoch: 3259, Batch Gradient Norm after: 11.440516523953402
Epoch 3260/10000, Prediction Accuracy = 61.30800000000001%, Loss = 0.5042949438095092
Epoch: 3260, Batch Gradient Norm: 9.943448190078772
Epoch: 3260, Batch Gradient Norm after: 9.943448190078772
Epoch 3261/10000, Prediction Accuracy = 61.39%, Loss = 0.4921084761619568
Epoch: 3261, Batch Gradient Norm: 12.090486623142779
Epoch: 3261, Batch Gradient Norm after: 12.090486623142779
Epoch 3262/10000, Prediction Accuracy = 61.352%, Loss = 0.506418776512146
Epoch: 3262, Batch Gradient Norm: 10.8002174030281
Epoch: 3262, Batch Gradient Norm after: 10.8002174030281
Epoch 3263/10000, Prediction Accuracy = 61.483999999999995%, Loss = 0.5002638876438141
Epoch: 3263, Batch Gradient Norm: 8.592534159305925
Epoch: 3263, Batch Gradient Norm after: 8.592534159305925
Epoch 3264/10000, Prediction Accuracy = 61.374%, Loss = 0.4846661388874054
Epoch: 3264, Batch Gradient Norm: 8.535491501856468
Epoch: 3264, Batch Gradient Norm after: 8.535491501856468
Epoch 3265/10000, Prediction Accuracy = 61.39000000000001%, Loss = 0.48280577063560487
Epoch: 3265, Batch Gradient Norm: 11.450077488063851
Epoch: 3265, Batch Gradient Norm after: 11.450077488063851
Epoch 3266/10000, Prediction Accuracy = 61.312%, Loss = 0.501063984632492
Epoch: 3266, Batch Gradient Norm: 10.698818362469876
Epoch: 3266, Batch Gradient Norm after: 10.698818362469876
Epoch 3267/10000, Prediction Accuracy = 61.374%, Loss = 0.4957668900489807
Epoch: 3267, Batch Gradient Norm: 10.707793028719188
Epoch: 3267, Batch Gradient Norm after: 10.707793028719188
Epoch 3268/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.49640476107597353
Epoch: 3268, Batch Gradient Norm: 10.55642746772713
Epoch: 3268, Batch Gradient Norm after: 10.55642746772713
Epoch 3269/10000, Prediction Accuracy = 61.438%, Loss = 0.49557405710220337
Epoch: 3269, Batch Gradient Norm: 8.85180493868809
Epoch: 3269, Batch Gradient Norm after: 8.85180493868809
Epoch 3270/10000, Prediction Accuracy = 61.376%, Loss = 0.48534247279167175
Epoch: 3270, Batch Gradient Norm: 8.347821601028965
Epoch: 3270, Batch Gradient Norm after: 8.347821601028965
Epoch 3271/10000, Prediction Accuracy = 61.384%, Loss = 0.48194909691810606
Epoch: 3271, Batch Gradient Norm: 7.705870716208781
Epoch: 3271, Batch Gradient Norm after: 7.705870716208781
Epoch 3272/10000, Prediction Accuracy = 61.266%, Loss = 0.4784803926944733
Epoch: 3272, Batch Gradient Norm: 9.993335360309972
Epoch: 3272, Batch Gradient Norm after: 9.993335360309972
Epoch 3273/10000, Prediction Accuracy = 61.49399999999999%, Loss = 0.49232707023620603
Epoch: 3273, Batch Gradient Norm: 9.565004521919406
Epoch: 3273, Batch Gradient Norm after: 9.565004521919406
Epoch 3274/10000, Prediction Accuracy = 61.336%, Loss = 0.48956422209739686
Epoch: 3274, Batch Gradient Norm: 12.559210762910421
Epoch: 3274, Batch Gradient Norm after: 12.559210762910421
Epoch 3275/10000, Prediction Accuracy = 61.43399999999999%, Loss = 0.5102316617965699
Epoch: 3275, Batch Gradient Norm: 13.679601598865306
Epoch: 3275, Batch Gradient Norm after: 13.679601598865306
Epoch 3276/10000, Prediction Accuracy = 61.44999999999999%, Loss = 0.5178842663764953
Epoch: 3276, Batch Gradient Norm: 14.12495733413653
Epoch: 3276, Batch Gradient Norm after: 14.12495733413653
Epoch 3277/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.52279691696167
Epoch: 3277, Batch Gradient Norm: 10.235085676515261
Epoch: 3277, Batch Gradient Norm after: 10.235085676515261
Epoch 3278/10000, Prediction Accuracy = 61.414%, Loss = 0.49446786642074586
Epoch: 3278, Batch Gradient Norm: 9.270907448729943
Epoch: 3278, Batch Gradient Norm after: 9.270907448729943
Epoch 3279/10000, Prediction Accuracy = 61.36%, Loss = 0.4882508397102356
Epoch: 3279, Batch Gradient Norm: 8.946855687253837
Epoch: 3279, Batch Gradient Norm after: 8.946855687253837
Epoch 3280/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.48555421829223633
Epoch: 3280, Batch Gradient Norm: 11.493110607252886
Epoch: 3280, Batch Gradient Norm after: 11.493110607252886
Epoch 3281/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.5020579099655151
Epoch: 3281, Batch Gradient Norm: 10.709536286348753
Epoch: 3281, Batch Gradient Norm after: 10.709536286348753
Epoch 3282/10000, Prediction Accuracy = 61.36999999999999%, Loss = 0.4959901750087738
Epoch: 3282, Batch Gradient Norm: 9.920571687546197
Epoch: 3282, Batch Gradient Norm after: 9.920571687546197
Epoch 3283/10000, Prediction Accuracy = 61.410000000000004%, Loss = 0.49066937565803526
Epoch: 3283, Batch Gradient Norm: 8.568289285227555
Epoch: 3283, Batch Gradient Norm after: 8.568289285227555
Epoch 3284/10000, Prediction Accuracy = 61.46999999999999%, Loss = 0.48279399871826173
Epoch: 3284, Batch Gradient Norm: 8.586610507577722
Epoch: 3284, Batch Gradient Norm after: 8.586610507577722
Epoch 3285/10000, Prediction Accuracy = 61.282%, Loss = 0.4828094482421875
Epoch: 3285, Batch Gradient Norm: 10.235824689033228
Epoch: 3285, Batch Gradient Norm after: 10.235824689033228
Epoch 3286/10000, Prediction Accuracy = 61.410000000000004%, Loss = 0.492796790599823
Epoch: 3286, Batch Gradient Norm: 12.610334617799131
Epoch: 3286, Batch Gradient Norm after: 12.610334617799131
Epoch 3287/10000, Prediction Accuracy = 61.29799999999999%, Loss = 0.5111068964004517
Epoch: 3287, Batch Gradient Norm: 10.96879320254323
Epoch: 3287, Batch Gradient Norm after: 10.96879320254323
Epoch 3288/10000, Prediction Accuracy = 61.41600000000001%, Loss = 0.4982257425785065
Epoch: 3288, Batch Gradient Norm: 11.442237124037561
Epoch: 3288, Batch Gradient Norm after: 11.442237124037561
Epoch 3289/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.49927363991737367
Epoch: 3289, Batch Gradient Norm: 11.164373454752814
Epoch: 3289, Batch Gradient Norm after: 11.164373454752814
Epoch 3290/10000, Prediction Accuracy = 61.331999999999994%, Loss = 0.4972313404083252
Epoch: 3290, Batch Gradient Norm: 9.757497513692204
Epoch: 3290, Batch Gradient Norm after: 9.757497513692204
Epoch 3291/10000, Prediction Accuracy = 61.386%, Loss = 0.4883877396583557
Epoch: 3291, Batch Gradient Norm: 9.690283365555844
Epoch: 3291, Batch Gradient Norm after: 9.690283365555844
Epoch 3292/10000, Prediction Accuracy = 61.31600000000001%, Loss = 0.4885095477104187
Epoch: 3292, Batch Gradient Norm: 10.025610727422002
Epoch: 3292, Batch Gradient Norm after: 10.025610727422002
Epoch 3293/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.4923785924911499
Epoch: 3293, Batch Gradient Norm: 10.58369479756717
Epoch: 3293, Batch Gradient Norm after: 10.58369479756717
Epoch 3294/10000, Prediction Accuracy = 61.464%, Loss = 0.4956759333610535
Epoch: 3294, Batch Gradient Norm: 9.569244607023489
Epoch: 3294, Batch Gradient Norm after: 9.569244607023489
Epoch 3295/10000, Prediction Accuracy = 61.364%, Loss = 0.4887982428073883
Epoch: 3295, Batch Gradient Norm: 10.308138494179143
Epoch: 3295, Batch Gradient Norm after: 10.308138494179143
Epoch 3296/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.4933628022670746
Epoch: 3296, Batch Gradient Norm: 8.835478333781124
Epoch: 3296, Batch Gradient Norm after: 8.835478333781124
Epoch 3297/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.4842491626739502
Epoch: 3297, Batch Gradient Norm: 9.467646142387247
Epoch: 3297, Batch Gradient Norm after: 9.467646142387247
Epoch 3298/10000, Prediction Accuracy = 61.38000000000001%, Loss = 0.4885511815547943
Epoch: 3298, Batch Gradient Norm: 11.186049894532978
Epoch: 3298, Batch Gradient Norm after: 11.186049894532978
Epoch 3299/10000, Prediction Accuracy = 61.374%, Loss = 0.4994281351566315
Epoch: 3299, Batch Gradient Norm: 12.104075782880615
Epoch: 3299, Batch Gradient Norm after: 12.104075782880615
Epoch 3300/10000, Prediction Accuracy = 61.388%, Loss = 0.5073482096195221
Epoch: 3300, Batch Gradient Norm: 11.19759006744035
Epoch: 3300, Batch Gradient Norm after: 11.19759006744035
Epoch 3301/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.4999356269836426
Epoch: 3301, Batch Gradient Norm: 11.224848701900257
Epoch: 3301, Batch Gradient Norm after: 11.224848701900257
Epoch 3302/10000, Prediction Accuracy = 61.315999999999995%, Loss = 0.4991919457912445
Epoch: 3302, Batch Gradient Norm: 11.855131708524302
Epoch: 3302, Batch Gradient Norm after: 11.855131708524302
Epoch 3303/10000, Prediction Accuracy = 61.370000000000005%, Loss = 0.5054319739341736
Epoch: 3303, Batch Gradient Norm: 11.354642314589826
Epoch: 3303, Batch Gradient Norm after: 11.354642314589826
Epoch 3304/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.501399689912796
Epoch: 3304, Batch Gradient Norm: 10.662835189039143
Epoch: 3304, Batch Gradient Norm after: 10.662835189039143
Epoch 3305/10000, Prediction Accuracy = 61.428%, Loss = 0.4946289360523224
Epoch: 3305, Batch Gradient Norm: 10.205083020191132
Epoch: 3305, Batch Gradient Norm after: 10.205083020191132
Epoch 3306/10000, Prediction Accuracy = 61.45799999999999%, Loss = 0.49148584008216856
Epoch: 3306, Batch Gradient Norm: 9.839678224420839
Epoch: 3306, Batch Gradient Norm after: 9.839678224420839
Epoch 3307/10000, Prediction Accuracy = 61.407999999999994%, Loss = 0.4884501755237579
Epoch: 3307, Batch Gradient Norm: 10.108024605954714
Epoch: 3307, Batch Gradient Norm after: 10.108024605954714
Epoch 3308/10000, Prediction Accuracy = 61.384%, Loss = 0.4902420282363892
Epoch: 3308, Batch Gradient Norm: 10.953597517413206
Epoch: 3308, Batch Gradient Norm after: 10.953597517413206
Epoch 3309/10000, Prediction Accuracy = 61.36800000000001%, Loss = 0.4971445858478546
Epoch: 3309, Batch Gradient Norm: 9.693587371292033
Epoch: 3309, Batch Gradient Norm after: 9.693587371292033
Epoch 3310/10000, Prediction Accuracy = 61.34000000000001%, Loss = 0.48798367381095886
Epoch: 3310, Batch Gradient Norm: 10.45921166703091
Epoch: 3310, Batch Gradient Norm after: 10.45921166703091
Epoch 3311/10000, Prediction Accuracy = 61.398%, Loss = 0.49267667531967163
Epoch: 3311, Batch Gradient Norm: 10.113994190460378
Epoch: 3311, Batch Gradient Norm after: 10.113994190460378
Epoch 3312/10000, Prediction Accuracy = 61.338%, Loss = 0.49012442827224734
Epoch: 3312, Batch Gradient Norm: 8.95253122867453
Epoch: 3312, Batch Gradient Norm after: 8.95253122867453
Epoch 3313/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.4837510704994202
Epoch: 3313, Batch Gradient Norm: 9.291498813834961
Epoch: 3313, Batch Gradient Norm after: 9.291498813834961
Epoch 3314/10000, Prediction Accuracy = 61.36999999999999%, Loss = 0.48697776794433595
Epoch: 3314, Batch Gradient Norm: 8.355649636754348
Epoch: 3314, Batch Gradient Norm after: 8.355649636754348
Epoch 3315/10000, Prediction Accuracy = 61.34000000000001%, Loss = 0.48210873603820803
Epoch: 3315, Batch Gradient Norm: 10.842362651333627
Epoch: 3315, Batch Gradient Norm after: 10.842362651333627
Epoch 3316/10000, Prediction Accuracy = 61.45400000000001%, Loss = 0.4988373875617981
Epoch: 3316, Batch Gradient Norm: 9.614902253076863
Epoch: 3316, Batch Gradient Norm after: 9.614902253076863
Epoch 3317/10000, Prediction Accuracy = 61.426%, Loss = 0.4886410176753998
Epoch: 3317, Batch Gradient Norm: 10.639679216557791
Epoch: 3317, Batch Gradient Norm after: 10.639679216557791
Epoch 3318/10000, Prediction Accuracy = 61.431999999999995%, Loss = 0.4943107545375824
Epoch: 3318, Batch Gradient Norm: 11.942257003110369
Epoch: 3318, Batch Gradient Norm after: 11.942257003110369
Epoch 3319/10000, Prediction Accuracy = 61.45%, Loss = 0.5040545642375946
Epoch: 3319, Batch Gradient Norm: 10.064959318349844
Epoch: 3319, Batch Gradient Norm after: 10.064959318349844
Epoch 3320/10000, Prediction Accuracy = 61.414%, Loss = 0.4910928666591644
Epoch: 3320, Batch Gradient Norm: 9.792114399467273
Epoch: 3320, Batch Gradient Norm after: 9.792114399467273
Epoch 3321/10000, Prediction Accuracy = 61.480000000000004%, Loss = 0.48876299858093264
Epoch: 3321, Batch Gradient Norm: 11.872645782748707
Epoch: 3321, Batch Gradient Norm after: 11.872645782748707
Epoch 3322/10000, Prediction Accuracy = 61.41199999999999%, Loss = 0.49994040131568906
Epoch: 3322, Batch Gradient Norm: 15.468362445657595
Epoch: 3322, Batch Gradient Norm after: 15.468362445657595
Epoch 3323/10000, Prediction Accuracy = 61.312%, Loss = 0.5341580748558045
Epoch: 3323, Batch Gradient Norm: 10.268316000425227
Epoch: 3323, Batch Gradient Norm after: 10.268316000425227
Epoch 3324/10000, Prediction Accuracy = 61.44199999999999%, Loss = 0.494884592294693
Epoch: 3324, Batch Gradient Norm: 7.873788420451953
Epoch: 3324, Batch Gradient Norm after: 7.873788420451953
Epoch 3325/10000, Prediction Accuracy = 61.4%, Loss = 0.478565913438797
Epoch: 3325, Batch Gradient Norm: 9.460980324702312
Epoch: 3325, Batch Gradient Norm after: 9.460980324702312
Epoch 3326/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.4856240272521973
Epoch: 3326, Batch Gradient Norm: 13.36933798515756
Epoch: 3326, Batch Gradient Norm after: 13.36933798515756
Epoch 3327/10000, Prediction Accuracy = 61.414%, Loss = 0.5127464532852173
Epoch: 3327, Batch Gradient Norm: 10.99389287781177
Epoch: 3327, Batch Gradient Norm after: 10.99389287781177
Epoch 3328/10000, Prediction Accuracy = 61.465999999999994%, Loss = 0.49536381363868714
Epoch: 3328, Batch Gradient Norm: 10.04591538432146
Epoch: 3328, Batch Gradient Norm after: 10.04591538432146
Epoch 3329/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4905154526233673
Epoch: 3329, Batch Gradient Norm: 9.973244496411402
Epoch: 3329, Batch Gradient Norm after: 9.973244496411402
Epoch 3330/10000, Prediction Accuracy = 61.46999999999999%, Loss = 0.49114538431167604
Epoch: 3330, Batch Gradient Norm: 8.097267514549824
Epoch: 3330, Batch Gradient Norm after: 8.097267514549824
Epoch 3331/10000, Prediction Accuracy = 61.306%, Loss = 0.4802989184856415
Epoch: 3331, Batch Gradient Norm: 8.540238163748112
Epoch: 3331, Batch Gradient Norm after: 8.540238163748112
Epoch 3332/10000, Prediction Accuracy = 61.45%, Loss = 0.48176623582839967
Epoch: 3332, Batch Gradient Norm: 9.700957499075045
Epoch: 3332, Batch Gradient Norm after: 9.700957499075045
Epoch 3333/10000, Prediction Accuracy = 61.364%, Loss = 0.487356573343277
Epoch: 3333, Batch Gradient Norm: 11.358848854551022
Epoch: 3333, Batch Gradient Norm after: 11.358848854551022
Epoch 3334/10000, Prediction Accuracy = 61.42%, Loss = 0.49826339483261106
Epoch: 3334, Batch Gradient Norm: 11.260698533846067
Epoch: 3334, Batch Gradient Norm after: 11.260698533846067
Epoch 3335/10000, Prediction Accuracy = 61.343999999999994%, Loss = 0.499360716342926
Epoch: 3335, Batch Gradient Norm: 8.74566561037517
Epoch: 3335, Batch Gradient Norm after: 8.74566561037517
Epoch 3336/10000, Prediction Accuracy = 61.426%, Loss = 0.48393797874450684
Epoch: 3336, Batch Gradient Norm: 8.545390011816687
Epoch: 3336, Batch Gradient Norm after: 8.545390011816687
Epoch 3337/10000, Prediction Accuracy = 61.396%, Loss = 0.48255317807197573
Epoch: 3337, Batch Gradient Norm: 11.248198298886866
Epoch: 3337, Batch Gradient Norm after: 11.248198298886866
Epoch 3338/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4976777195930481
Epoch: 3338, Batch Gradient Norm: 13.599853183884633
Epoch: 3338, Batch Gradient Norm after: 13.599853183884633
Epoch 3339/10000, Prediction Accuracy = 61.348%, Loss = 0.5159635543823242
Epoch: 3339, Batch Gradient Norm: 11.067560930491045
Epoch: 3339, Batch Gradient Norm after: 11.067560930491045
Epoch 3340/10000, Prediction Accuracy = 61.362%, Loss = 0.4969712674617767
Epoch: 3340, Batch Gradient Norm: 8.969691941593789
Epoch: 3340, Batch Gradient Norm after: 8.969691941593789
Epoch 3341/10000, Prediction Accuracy = 61.306%, Loss = 0.48350955843925475
Epoch: 3341, Batch Gradient Norm: 9.385259709684473
Epoch: 3341, Batch Gradient Norm after: 9.385259709684473
Epoch 3342/10000, Prediction Accuracy = 61.408%, Loss = 0.48662503361701964
Epoch: 3342, Batch Gradient Norm: 10.853397453793775
Epoch: 3342, Batch Gradient Norm after: 10.853397453793775
Epoch 3343/10000, Prediction Accuracy = 61.4%, Loss = 0.49673880338668824
Epoch: 3343, Batch Gradient Norm: 9.3261210268133
Epoch: 3343, Batch Gradient Norm after: 9.3261210268133
Epoch 3344/10000, Prediction Accuracy = 61.42600000000001%, Loss = 0.4860979437828064
Epoch: 3344, Batch Gradient Norm: 9.443076081353533
Epoch: 3344, Batch Gradient Norm after: 9.443076081353533
Epoch 3345/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4859648048877716
Epoch: 3345, Batch Gradient Norm: 10.595031265685739
Epoch: 3345, Batch Gradient Norm after: 10.595031265685739
Epoch 3346/10000, Prediction Accuracy = 61.476%, Loss = 0.4947716474533081
Epoch: 3346, Batch Gradient Norm: 9.4376876289657
Epoch: 3346, Batch Gradient Norm after: 9.4376876289657
Epoch 3347/10000, Prediction Accuracy = 61.338%, Loss = 0.4870969891548157
Epoch: 3347, Batch Gradient Norm: 9.40749483534113
Epoch: 3347, Batch Gradient Norm after: 9.40749483534113
Epoch 3348/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.4859713613986969
Epoch: 3348, Batch Gradient Norm: 11.921613993106803
Epoch: 3348, Batch Gradient Norm after: 11.921613993106803
Epoch 3349/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.49907138347625735
Epoch: 3349, Batch Gradient Norm: 15.154897972428422
Epoch: 3349, Batch Gradient Norm after: 15.154897972428422
Epoch 3350/10000, Prediction Accuracy = 61.428%, Loss = 0.5264709949493408
Epoch: 3350, Batch Gradient Norm: 10.538541499409469
Epoch: 3350, Batch Gradient Norm after: 10.538541499409469
Epoch 3351/10000, Prediction Accuracy = 61.384%, Loss = 0.4918450891971588
Epoch: 3351, Batch Gradient Norm: 8.039839772172117
Epoch: 3351, Batch Gradient Norm after: 8.039839772172117
Epoch 3352/10000, Prediction Accuracy = 61.476%, Loss = 0.4778735041618347
Epoch: 3352, Batch Gradient Norm: 10.828414499054315
Epoch: 3352, Batch Gradient Norm after: 10.828414499054315
Epoch 3353/10000, Prediction Accuracy = 61.4%, Loss = 0.49792354702949526
Epoch: 3353, Batch Gradient Norm: 8.19420484329328
Epoch: 3353, Batch Gradient Norm after: 8.19420484329328
Epoch 3354/10000, Prediction Accuracy = 61.410000000000004%, Loss = 0.4805610775947571
Epoch: 3354, Batch Gradient Norm: 8.114078380278679
Epoch: 3354, Batch Gradient Norm after: 8.114078380278679
Epoch 3355/10000, Prediction Accuracy = 61.412%, Loss = 0.4783086359500885
Epoch: 3355, Batch Gradient Norm: 8.899962426092799
Epoch: 3355, Batch Gradient Norm after: 8.899962426092799
Epoch 3356/10000, Prediction Accuracy = 61.324%, Loss = 0.4816021978855133
Epoch: 3356, Batch Gradient Norm: 10.709687210362222
Epoch: 3356, Batch Gradient Norm after: 10.709687210362222
Epoch 3357/10000, Prediction Accuracy = 61.426%, Loss = 0.49279308319091797
Epoch: 3357, Batch Gradient Norm: 11.155557335980387
Epoch: 3357, Batch Gradient Norm after: 11.155557335980387
Epoch 3358/10000, Prediction Accuracy = 61.267999999999994%, Loss = 0.4963072955608368
Epoch: 3358, Batch Gradient Norm: 10.383906250752927
Epoch: 3358, Batch Gradient Norm after: 10.383906250752927
Epoch 3359/10000, Prediction Accuracy = 61.432%, Loss = 0.4924567937850952
Epoch: 3359, Batch Gradient Norm: 10.855934466290034
Epoch: 3359, Batch Gradient Norm after: 10.855934466290034
Epoch 3360/10000, Prediction Accuracy = 61.35799999999999%, Loss = 0.4948202192783356
Epoch: 3360, Batch Gradient Norm: 12.421225333514151
Epoch: 3360, Batch Gradient Norm after: 12.421225333514151
Epoch 3361/10000, Prediction Accuracy = 61.338%, Loss = 0.5047820210456848
Epoch: 3361, Batch Gradient Norm: 13.930177120899005
Epoch: 3361, Batch Gradient Norm after: 13.930177120899005
Epoch 3362/10000, Prediction Accuracy = 61.426%, Loss = 0.5178237438201905
Epoch: 3362, Batch Gradient Norm: 10.914057462738374
Epoch: 3362, Batch Gradient Norm after: 10.914057462738374
Epoch 3363/10000, Prediction Accuracy = 61.278%, Loss = 0.49466161131858827
Epoch: 3363, Batch Gradient Norm: 10.405055246194816
Epoch: 3363, Batch Gradient Norm after: 10.405055246194816
Epoch 3364/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.49182276129722596
Epoch: 3364, Batch Gradient Norm: 8.454716221609807
Epoch: 3364, Batch Gradient Norm after: 8.454716221609807
Epoch 3365/10000, Prediction Accuracy = 61.42199999999999%, Loss = 0.48057706356048585
Epoch: 3365, Batch Gradient Norm: 8.98946541405127
Epoch: 3365, Batch Gradient Norm after: 8.98946541405127
Epoch 3366/10000, Prediction Accuracy = 61.338%, Loss = 0.4833305418491364
Epoch: 3366, Batch Gradient Norm: 9.725261990002407
Epoch: 3366, Batch Gradient Norm after: 9.725261990002407
Epoch 3367/10000, Prediction Accuracy = 61.386%, Loss = 0.4867498934268951
Epoch: 3367, Batch Gradient Norm: 12.46858556324184
Epoch: 3367, Batch Gradient Norm after: 12.46858556324184
Epoch 3368/10000, Prediction Accuracy = 61.327999999999996%, Loss = 0.5060196876525879
Epoch: 3368, Batch Gradient Norm: 12.312735094938931
Epoch: 3368, Batch Gradient Norm after: 12.312735094938931
Epoch 3369/10000, Prediction Accuracy = 61.355999999999995%, Loss = 0.5080822348594666
Epoch: 3369, Batch Gradient Norm: 8.259201848876303
Epoch: 3369, Batch Gradient Norm after: 8.259201848876303
Epoch 3370/10000, Prediction Accuracy = 61.39%, Loss = 0.47981731295585633
Epoch: 3370, Batch Gradient Norm: 8.231941881884966
Epoch: 3370, Batch Gradient Norm after: 8.231941881884966
Epoch 3371/10000, Prediction Accuracy = 61.396%, Loss = 0.4787705183029175
Epoch: 3371, Batch Gradient Norm: 8.172246313824548
Epoch: 3371, Batch Gradient Norm after: 8.172246313824548
Epoch 3372/10000, Prediction Accuracy = 61.436%, Loss = 0.4780451118946075
Epoch: 3372, Batch Gradient Norm: 9.441268757178108
Epoch: 3372, Batch Gradient Norm after: 9.441268757178108
Epoch 3373/10000, Prediction Accuracy = 61.45400000000001%, Loss = 0.48430747985839845
Epoch: 3373, Batch Gradient Norm: 11.299972100689061
Epoch: 3373, Batch Gradient Norm after: 11.299972100689061
Epoch 3374/10000, Prediction Accuracy = 61.4%, Loss = 0.4958666145801544
Epoch: 3374, Batch Gradient Norm: 12.17347378538303
Epoch: 3374, Batch Gradient Norm after: 12.17347378538303
Epoch 3375/10000, Prediction Accuracy = 61.336%, Loss = 0.5016708433628082
Epoch: 3375, Batch Gradient Norm: 11.45620833548683
Epoch: 3375, Batch Gradient Norm after: 11.45620833548683
Epoch 3376/10000, Prediction Accuracy = 61.4%, Loss = 0.4987965226173401
Epoch: 3376, Batch Gradient Norm: 9.422600454468581
Epoch: 3376, Batch Gradient Norm after: 9.422600454468581
Epoch 3377/10000, Prediction Accuracy = 61.424%, Loss = 0.4852371633052826
Epoch: 3377, Batch Gradient Norm: 10.293779579774963
Epoch: 3377, Batch Gradient Norm after: 10.293779579774963
Epoch 3378/10000, Prediction Accuracy = 61.419999999999995%, Loss = 0.490871661901474
Epoch: 3378, Batch Gradient Norm: 10.707175386087414
Epoch: 3378, Batch Gradient Norm after: 10.707175386087414
Epoch 3379/10000, Prediction Accuracy = 61.414%, Loss = 0.49457578659057616
Epoch: 3379, Batch Gradient Norm: 10.087669032670131
Epoch: 3379, Batch Gradient Norm after: 10.087669032670131
Epoch 3380/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.48824345469474795
Epoch: 3380, Batch Gradient Norm: 11.053012657459456
Epoch: 3380, Batch Gradient Norm after: 11.053012657459456
Epoch 3381/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.49452301263809206
Epoch: 3381, Batch Gradient Norm: 11.871390454306034
Epoch: 3381, Batch Gradient Norm after: 11.871390454306034
Epoch 3382/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.50145725607872
Epoch: 3382, Batch Gradient Norm: 10.06681455292434
Epoch: 3382, Batch Gradient Norm after: 10.06681455292434
Epoch 3383/10000, Prediction Accuracy = 61.53599999999999%, Loss = 0.49005942344665526
Epoch: 3383, Batch Gradient Norm: 9.161926810053611
Epoch: 3383, Batch Gradient Norm after: 9.161926810053611
Epoch 3384/10000, Prediction Accuracy = 61.364%, Loss = 0.4837166011333466
Epoch: 3384, Batch Gradient Norm: 9.166729250707494
Epoch: 3384, Batch Gradient Norm after: 9.166729250707494
Epoch 3385/10000, Prediction Accuracy = 61.464%, Loss = 0.4820968508720398
Epoch: 3385, Batch Gradient Norm: 11.02343298421596
Epoch: 3385, Batch Gradient Norm after: 11.02343298421596
Epoch 3386/10000, Prediction Accuracy = 61.398%, Loss = 0.49316757917404175
Epoch: 3386, Batch Gradient Norm: 11.784365297472311
Epoch: 3386, Batch Gradient Norm after: 11.784365297472311
Epoch 3387/10000, Prediction Accuracy = 61.414%, Loss = 0.498851478099823
Epoch: 3387, Batch Gradient Norm: 10.795054042675504
Epoch: 3387, Batch Gradient Norm after: 10.795054042675504
Epoch 3388/10000, Prediction Accuracy = 61.472%, Loss = 0.4927644908428192
Epoch: 3388, Batch Gradient Norm: 10.78907136464971
Epoch: 3388, Batch Gradient Norm after: 10.78907136464971
Epoch 3389/10000, Prediction Accuracy = 61.366%, Loss = 0.493102365732193
Epoch: 3389, Batch Gradient Norm: 10.309735211920808
Epoch: 3389, Batch Gradient Norm after: 10.309735211920808
Epoch 3390/10000, Prediction Accuracy = 61.492000000000004%, Loss = 0.49060683846473696
Epoch: 3390, Batch Gradient Norm: 10.474258405741926
Epoch: 3390, Batch Gradient Norm after: 10.474258405741926
Epoch 3391/10000, Prediction Accuracy = 61.34400000000001%, Loss = 0.4910251021385193
Epoch: 3391, Batch Gradient Norm: 11.476396650430171
Epoch: 3391, Batch Gradient Norm after: 11.476396650430171
Epoch 3392/10000, Prediction Accuracy = 61.426%, Loss = 0.49761914014816283
Epoch: 3392, Batch Gradient Norm: 11.981369654802615
Epoch: 3392, Batch Gradient Norm after: 11.981369654802615
Epoch 3393/10000, Prediction Accuracy = 61.378%, Loss = 0.5024460434913636
Epoch: 3393, Batch Gradient Norm: 10.568106889848037
Epoch: 3393, Batch Gradient Norm after: 10.568106889848037
Epoch 3394/10000, Prediction Accuracy = 61.339999999999996%, Loss = 0.49226805567741394
Epoch: 3394, Batch Gradient Norm: 10.061252753971258
Epoch: 3394, Batch Gradient Norm after: 10.061252753971258
Epoch 3395/10000, Prediction Accuracy = 61.431999999999995%, Loss = 0.4884361565113068
Epoch: 3395, Batch Gradient Norm: 9.053583797796295
Epoch: 3395, Batch Gradient Norm after: 9.053583797796295
Epoch 3396/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.4806198179721832
Epoch: 3396, Batch Gradient Norm: 10.067017845880304
Epoch: 3396, Batch Gradient Norm after: 10.067017845880304
Epoch 3397/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.48608651757240295
Epoch: 3397, Batch Gradient Norm: 11.86708904443615
Epoch: 3397, Batch Gradient Norm after: 11.86708904443615
Epoch 3398/10000, Prediction Accuracy = 61.35200000000001%, Loss = 0.5023218333721161
Epoch: 3398, Batch Gradient Norm: 8.07878752325288
Epoch: 3398, Batch Gradient Norm after: 8.07878752325288
Epoch 3399/10000, Prediction Accuracy = 61.496%, Loss = 0.4790565729141235
Epoch: 3399, Batch Gradient Norm: 7.401605324933706
Epoch: 3399, Batch Gradient Norm after: 7.401605324933706
Epoch 3400/10000, Prediction Accuracy = 61.40999999999999%, Loss = 0.4742610454559326
Epoch: 3400, Batch Gradient Norm: 9.414309128284511
Epoch: 3400, Batch Gradient Norm after: 9.414309128284511
Epoch 3401/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.48544102907180786
Epoch: 3401, Batch Gradient Norm: 11.06168567080812
Epoch: 3401, Batch Gradient Norm after: 11.06168567080812
Epoch 3402/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.49508016705513
Epoch: 3402, Batch Gradient Norm: 12.41353483037477
Epoch: 3402, Batch Gradient Norm after: 12.41353483037477
Epoch 3403/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.5049383580684662
Epoch: 3403, Batch Gradient Norm: 9.564075033958552
Epoch: 3403, Batch Gradient Norm after: 9.564075033958552
Epoch 3404/10000, Prediction Accuracy = 61.432%, Loss = 0.48509406447410586
Epoch: 3404, Batch Gradient Norm: 9.494428886367928
Epoch: 3404, Batch Gradient Norm after: 9.494428886367928
Epoch 3405/10000, Prediction Accuracy = 61.43000000000001%, Loss = 0.483978796005249
Epoch: 3405, Batch Gradient Norm: 11.860274009223806
Epoch: 3405, Batch Gradient Norm after: 11.860274009223806
Epoch 3406/10000, Prediction Accuracy = 61.416%, Loss = 0.500728291273117
Epoch: 3406, Batch Gradient Norm: 10.163792189520297
Epoch: 3406, Batch Gradient Norm after: 10.163792189520297
Epoch 3407/10000, Prediction Accuracy = 61.33200000000001%, Loss = 0.4881274998188019
Epoch: 3407, Batch Gradient Norm: 9.450383716501982
Epoch: 3407, Batch Gradient Norm after: 9.450383716501982
Epoch 3408/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.4829331815242767
Epoch: 3408, Batch Gradient Norm: 10.184287344772772
Epoch: 3408, Batch Gradient Norm after: 10.184287344772772
Epoch 3409/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.4867296814918518
Epoch: 3409, Batch Gradient Norm: 11.483689638627169
Epoch: 3409, Batch Gradient Norm after: 11.483689638627169
Epoch 3410/10000, Prediction Accuracy = 61.331999999999994%, Loss = 0.49774524569511414
Epoch: 3410, Batch Gradient Norm: 9.282080245230949
Epoch: 3410, Batch Gradient Norm after: 9.282080245230949
Epoch 3411/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4848270297050476
Epoch: 3411, Batch Gradient Norm: 9.321562561649078
Epoch: 3411, Batch Gradient Norm after: 9.321562561649078
Epoch 3412/10000, Prediction Accuracy = 61.342000000000006%, Loss = 0.4839381158351898
Epoch: 3412, Batch Gradient Norm: 11.342877590852861
Epoch: 3412, Batch Gradient Norm after: 11.342877590852861
Epoch 3413/10000, Prediction Accuracy = 61.468%, Loss = 0.4983503818511963
Epoch: 3413, Batch Gradient Norm: 11.313480845872425
Epoch: 3413, Batch Gradient Norm after: 11.313480845872425
Epoch 3414/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4974707305431366
Epoch: 3414, Batch Gradient Norm: 9.735721941142037
Epoch: 3414, Batch Gradient Norm after: 9.735721941142037
Epoch 3415/10000, Prediction Accuracy = 61.424%, Loss = 0.48430028557777405
Epoch: 3415, Batch Gradient Norm: 11.346056734410825
Epoch: 3415, Batch Gradient Norm after: 11.346056734410825
Epoch 3416/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.49428746700286863
Epoch: 3416, Batch Gradient Norm: 11.84980525450269
Epoch: 3416, Batch Gradient Norm after: 11.84980525450269
Epoch 3417/10000, Prediction Accuracy = 61.306000000000004%, Loss = 0.49696500301361085
Epoch: 3417, Batch Gradient Norm: 12.251195208648987
Epoch: 3417, Batch Gradient Norm after: 12.251195208648987
Epoch 3418/10000, Prediction Accuracy = 61.45%, Loss = 0.5008444428443909
Epoch: 3418, Batch Gradient Norm: 10.553744540263759
Epoch: 3418, Batch Gradient Norm after: 10.553744540263759
Epoch 3419/10000, Prediction Accuracy = 61.35600000000001%, Loss = 0.4900394558906555
Epoch: 3419, Batch Gradient Norm: 9.234398584256814
Epoch: 3419, Batch Gradient Norm after: 9.234398584256814
Epoch 3420/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.4817188024520874
Epoch: 3420, Batch Gradient Norm: 12.058634923915578
Epoch: 3420, Batch Gradient Norm after: 12.058634923915578
Epoch 3421/10000, Prediction Accuracy = 61.468%, Loss = 0.5005366086959839
Epoch: 3421, Batch Gradient Norm: 13.271148198696345
Epoch: 3421, Batch Gradient Norm after: 13.271148198696345
Epoch 3422/10000, Prediction Accuracy = 61.45799999999999%, Loss = 0.5106301307678223
Epoch: 3422, Batch Gradient Norm: 8.937462234705562
Epoch: 3422, Batch Gradient Norm after: 8.937462234705562
Epoch 3423/10000, Prediction Accuracy = 61.379999999999995%, Loss = 0.4817025661468506
Epoch: 3423, Batch Gradient Norm: 8.2031113843932
Epoch: 3423, Batch Gradient Norm after: 8.2031113843932
Epoch 3424/10000, Prediction Accuracy = 61.48%, Loss = 0.4771897792816162
Epoch: 3424, Batch Gradient Norm: 8.040048782328517
Epoch: 3424, Batch Gradient Norm after: 8.040048782328517
Epoch 3425/10000, Prediction Accuracy = 61.41600000000001%, Loss = 0.4770278811454773
Epoch: 3425, Batch Gradient Norm: 8.998207137822414
Epoch: 3425, Batch Gradient Norm after: 8.998207137822414
Epoch 3426/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.4826690495014191
Epoch: 3426, Batch Gradient Norm: 8.628703391841357
Epoch: 3426, Batch Gradient Norm after: 8.628703391841357
Epoch 3427/10000, Prediction Accuracy = 61.48%, Loss = 0.47857052087783813
Epoch: 3427, Batch Gradient Norm: 11.853607037838472
Epoch: 3427, Batch Gradient Norm after: 11.853607037838472
Epoch 3428/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.500056904554367
Epoch: 3428, Batch Gradient Norm: 10.941966633943846
Epoch: 3428, Batch Gradient Norm after: 10.941966633943846
Epoch 3429/10000, Prediction Accuracy = 61.398%, Loss = 0.49553097486495973
Epoch: 3429, Batch Gradient Norm: 8.75412490875993
Epoch: 3429, Batch Gradient Norm after: 8.75412490875993
Epoch 3430/10000, Prediction Accuracy = 61.414%, Loss = 0.4782159626483917
Epoch: 3430, Batch Gradient Norm: 11.208094335784153
Epoch: 3430, Batch Gradient Norm after: 11.208094335784153
Epoch 3431/10000, Prediction Accuracy = 61.422000000000004%, Loss = 0.4916473090648651
Epoch: 3431, Batch Gradient Norm: 12.978581834203084
Epoch: 3431, Batch Gradient Norm after: 12.978581834203084
Epoch 3432/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.5066439747810364
Epoch: 3432, Batch Gradient Norm: 9.94133032180143
Epoch: 3432, Batch Gradient Norm after: 9.94133032180143
Epoch 3433/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.4867748022079468
Epoch: 3433, Batch Gradient Norm: 8.828054013168465
Epoch: 3433, Batch Gradient Norm after: 8.828054013168465
Epoch 3434/10000, Prediction Accuracy = 61.38199999999999%, Loss = 0.4803132593631744
Epoch: 3434, Batch Gradient Norm: 9.482215109392452
Epoch: 3434, Batch Gradient Norm after: 9.482215109392452
Epoch 3435/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4822129189968109
Epoch: 3435, Batch Gradient Norm: 13.578062505302404
Epoch: 3435, Batch Gradient Norm after: 13.578062505302404
Epoch 3436/10000, Prediction Accuracy = 61.391999999999996%, Loss = 0.5116392731666565
Epoch: 3436, Batch Gradient Norm: 11.488774267343752
Epoch: 3436, Batch Gradient Norm after: 11.488774267343752
Epoch 3437/10000, Prediction Accuracy = 61.462%, Loss = 0.49648083448410035
Epoch: 3437, Batch Gradient Norm: 9.088368096241744
Epoch: 3437, Batch Gradient Norm after: 9.088368096241744
Epoch 3438/10000, Prediction Accuracy = 61.444%, Loss = 0.4804384708404541
Epoch: 3438, Batch Gradient Norm: 11.488200647877363
Epoch: 3438, Batch Gradient Norm after: 11.488200647877363
Epoch 3439/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.49577524662017824
Epoch: 3439, Batch Gradient Norm: 9.670664786737458
Epoch: 3439, Batch Gradient Norm after: 9.670664786737458
Epoch 3440/10000, Prediction Accuracy = 61.35999999999999%, Loss = 0.4837247967720032
Epoch: 3440, Batch Gradient Norm: 8.568309526719357
Epoch: 3440, Batch Gradient Norm after: 8.568309526719357
Epoch 3441/10000, Prediction Accuracy = 61.459999999999994%, Loss = 0.4773614525794983
Epoch: 3441, Batch Gradient Norm: 7.345106535813882
Epoch: 3441, Batch Gradient Norm after: 7.345106535813882
Epoch 3442/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.47063751220703126
Epoch: 3442, Batch Gradient Norm: 10.673150862631376
Epoch: 3442, Batch Gradient Norm after: 10.673150862631376
Epoch 3443/10000, Prediction Accuracy = 61.508%, Loss = 0.4905740559101105
Epoch: 3443, Batch Gradient Norm: 12.656969667967214
Epoch: 3443, Batch Gradient Norm after: 12.656969667967214
Epoch 3444/10000, Prediction Accuracy = 61.322%, Loss = 0.5060880124568939
Epoch: 3444, Batch Gradient Norm: 11.56345548794707
Epoch: 3444, Batch Gradient Norm after: 11.56345548794707
Epoch 3445/10000, Prediction Accuracy = 61.402%, Loss = 0.4974867880344391
Epoch: 3445, Batch Gradient Norm: 10.235725126534192
Epoch: 3445, Batch Gradient Norm after: 10.235725126534192
Epoch 3446/10000, Prediction Accuracy = 61.33599999999999%, Loss = 0.489443039894104
Epoch: 3446, Batch Gradient Norm: 9.644346765250805
Epoch: 3446, Batch Gradient Norm after: 9.644346765250805
Epoch 3447/10000, Prediction Accuracy = 61.44%, Loss = 0.48574707508087156
Epoch: 3447, Batch Gradient Norm: 10.043496466972861
Epoch: 3447, Batch Gradient Norm after: 10.043496466972861
Epoch 3448/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.48775020241737366
Epoch: 3448, Batch Gradient Norm: 11.658080499357414
Epoch: 3448, Batch Gradient Norm after: 11.658080499357414
Epoch 3449/10000, Prediction Accuracy = 61.39%, Loss = 0.498273503780365
Epoch: 3449, Batch Gradient Norm: 12.38600513803071
Epoch: 3449, Batch Gradient Norm after: 12.38600513803071
Epoch 3450/10000, Prediction Accuracy = 61.426%, Loss = 0.5023451805114746
Epoch: 3450, Batch Gradient Norm: 10.647705561012641
Epoch: 3450, Batch Gradient Norm after: 10.647705561012641
Epoch 3451/10000, Prediction Accuracy = 61.412%, Loss = 0.48985821604728697
Epoch: 3451, Batch Gradient Norm: 8.343483630746839
Epoch: 3451, Batch Gradient Norm after: 8.343483630746839
Epoch 3452/10000, Prediction Accuracy = 61.388%, Loss = 0.47571608424186707
Epoch: 3452, Batch Gradient Norm: 8.379152037352613
Epoch: 3452, Batch Gradient Norm after: 8.379152037352613
Epoch 3453/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.47606738209724425
Epoch: 3453, Batch Gradient Norm: 9.719486062478376
Epoch: 3453, Batch Gradient Norm after: 9.719486062478376
Epoch 3454/10000, Prediction Accuracy = 61.42%, Loss = 0.48512194156646726
Epoch: 3454, Batch Gradient Norm: 10.520725802854994
Epoch: 3454, Batch Gradient Norm after: 10.520725802854994
Epoch 3455/10000, Prediction Accuracy = 61.468%, Loss = 0.4921411097049713
Epoch: 3455, Batch Gradient Norm: 8.924598532544575
Epoch: 3455, Batch Gradient Norm after: 8.924598532544575
Epoch 3456/10000, Prediction Accuracy = 61.416%, Loss = 0.4800315260887146
Epoch: 3456, Batch Gradient Norm: 11.807816867421915
Epoch: 3456, Batch Gradient Norm after: 11.807816867421915
Epoch 3457/10000, Prediction Accuracy = 61.44%, Loss = 0.49718270301818845
Epoch: 3457, Batch Gradient Norm: 13.24509057592408
Epoch: 3457, Batch Gradient Norm after: 13.24509057592408
Epoch 3458/10000, Prediction Accuracy = 61.496%, Loss = 0.5091012477874756
Epoch: 3458, Batch Gradient Norm: 9.177595536925176
Epoch: 3458, Batch Gradient Norm after: 9.177595536925176
Epoch 3459/10000, Prediction Accuracy = 61.419999999999995%, Loss = 0.4803533971309662
Epoch: 3459, Batch Gradient Norm: 7.134687120180196
Epoch: 3459, Batch Gradient Norm after: 7.134687120180196
Epoch 3460/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.4694491446018219
Epoch: 3460, Batch Gradient Norm: 9.951297044591339
Epoch: 3460, Batch Gradient Norm after: 9.951297044591339
Epoch 3461/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.48426162600517275
Epoch: 3461, Batch Gradient Norm: 13.900709832045475
Epoch: 3461, Batch Gradient Norm after: 13.900709832045475
Epoch 3462/10000, Prediction Accuracy = 61.489999999999995%, Loss = 0.5148738384246826
Epoch: 3462, Batch Gradient Norm: 12.510115868030509
Epoch: 3462, Batch Gradient Norm after: 12.510115868030509
Epoch 3463/10000, Prediction Accuracy = 61.39%, Loss = 0.503328675031662
Epoch: 3463, Batch Gradient Norm: 9.63694460083264
Epoch: 3463, Batch Gradient Norm after: 9.63694460083264
Epoch 3464/10000, Prediction Accuracy = 61.552%, Loss = 0.4812085509300232
Epoch: 3464, Batch Gradient Norm: 10.315947846162953
Epoch: 3464, Batch Gradient Norm after: 10.315947846162953
Epoch 3465/10000, Prediction Accuracy = 61.40599999999999%, Loss = 0.485016930103302
Epoch: 3465, Batch Gradient Norm: 10.501765930264314
Epoch: 3465, Batch Gradient Norm after: 10.501765930264314
Epoch 3466/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.48715381026268006
Epoch: 3466, Batch Gradient Norm: 8.3741873684385
Epoch: 3466, Batch Gradient Norm after: 8.3741873684385
Epoch 3467/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.4750380277633667
Epoch: 3467, Batch Gradient Norm: 9.141609538760493
Epoch: 3467, Batch Gradient Norm after: 9.141609538760493
Epoch 3468/10000, Prediction Accuracy = 61.459999999999994%, Loss = 0.4801049709320068
Epoch: 3468, Batch Gradient Norm: 9.358877262694373
Epoch: 3468, Batch Gradient Norm after: 9.358877262694373
Epoch 3469/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.4822133660316467
Epoch: 3469, Batch Gradient Norm: 10.954399721491988
Epoch: 3469, Batch Gradient Norm after: 10.954399721491988
Epoch 3470/10000, Prediction Accuracy = 61.395999999999994%, Loss = 0.49311675429344176
Epoch: 3470, Batch Gradient Norm: 10.439304801398274
Epoch: 3470, Batch Gradient Norm after: 10.439304801398274
Epoch 3471/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.48852538466453554
Epoch: 3471, Batch Gradient Norm: 11.128426117032184
Epoch: 3471, Batch Gradient Norm after: 11.128426117032184
Epoch 3472/10000, Prediction Accuracy = 61.492000000000004%, Loss = 0.4934874176979065
Epoch: 3472, Batch Gradient Norm: 10.158813873190772
Epoch: 3472, Batch Gradient Norm after: 10.158813873190772
Epoch 3473/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.48907709717750547
Epoch: 3473, Batch Gradient Norm: 8.439915380622931
Epoch: 3473, Batch Gradient Norm after: 8.439915380622931
Epoch 3474/10000, Prediction Accuracy = 61.472%, Loss = 0.4766764223575592
Epoch: 3474, Batch Gradient Norm: 11.287937876686854
Epoch: 3474, Batch Gradient Norm after: 11.287937876686854
Epoch 3475/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.49453125
Epoch: 3475, Batch Gradient Norm: 11.826116474682813
Epoch: 3475, Batch Gradient Norm after: 11.826116474682813
Epoch 3476/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.49811443090438845
Epoch: 3476, Batch Gradient Norm: 9.578077020953424
Epoch: 3476, Batch Gradient Norm after: 9.578077020953424
Epoch 3477/10000, Prediction Accuracy = 61.46999999999999%, Loss = 0.4823730766773224
Epoch: 3477, Batch Gradient Norm: 7.590983883802085
Epoch: 3477, Batch Gradient Norm after: 7.590983883802085
Epoch 3478/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.4712455451488495
Epoch: 3478, Batch Gradient Norm: 9.080983856233592
Epoch: 3478, Batch Gradient Norm after: 9.080983856233592
Epoch 3479/10000, Prediction Accuracy = 61.512%, Loss = 0.47909375429153445
Epoch: 3479, Batch Gradient Norm: 11.163515602534506
Epoch: 3479, Batch Gradient Norm after: 11.163515602534506
Epoch 3480/10000, Prediction Accuracy = 61.418000000000006%, Loss = 0.4944957077503204
Epoch: 3480, Batch Gradient Norm: 11.44046682152718
Epoch: 3480, Batch Gradient Norm after: 11.44046682152718
Epoch 3481/10000, Prediction Accuracy = 61.471999999999994%, Loss = 0.4943533957004547
Epoch: 3481, Batch Gradient Norm: 13.20587812310098
Epoch: 3481, Batch Gradient Norm after: 13.20587812310098
Epoch 3482/10000, Prediction Accuracy = 61.524%, Loss = 0.5054680526256561
Epoch: 3482, Batch Gradient Norm: 12.077062932609667
Epoch: 3482, Batch Gradient Norm after: 12.077062932609667
Epoch 3483/10000, Prediction Accuracy = 61.33599999999999%, Loss = 0.4974509060382843
Epoch: 3483, Batch Gradient Norm: 9.778268354431898
Epoch: 3483, Batch Gradient Norm after: 9.778268354431898
Epoch 3484/10000, Prediction Accuracy = 61.395999999999994%, Loss = 0.48320324420928956
Epoch: 3484, Batch Gradient Norm: 9.68446509937507
Epoch: 3484, Batch Gradient Norm after: 9.68446509937507
Epoch 3485/10000, Prediction Accuracy = 61.406000000000006%, Loss = 0.4834611177444458
Epoch: 3485, Batch Gradient Norm: 12.808890786356583
Epoch: 3485, Batch Gradient Norm after: 12.808890786356583
Epoch 3486/10000, Prediction Accuracy = 61.358000000000004%, Loss = 0.5053858816623688
Epoch: 3486, Batch Gradient Norm: 12.562400548870182
Epoch: 3486, Batch Gradient Norm after: 12.562400548870182
Epoch 3487/10000, Prediction Accuracy = 61.508%, Loss = 0.503284364938736
Epoch: 3487, Batch Gradient Norm: 8.832592862518002
Epoch: 3487, Batch Gradient Norm after: 8.832592862518002
Epoch 3488/10000, Prediction Accuracy = 61.396%, Loss = 0.4780319154262543
Epoch: 3488, Batch Gradient Norm: 7.969569734410356
Epoch: 3488, Batch Gradient Norm after: 7.969569734410356
Epoch 3489/10000, Prediction Accuracy = 61.574%, Loss = 0.47357988357543945
Epoch: 3489, Batch Gradient Norm: 8.059487246138866
Epoch: 3489, Batch Gradient Norm after: 8.059487246138866
Epoch 3490/10000, Prediction Accuracy = 61.410000000000004%, Loss = 0.4743445813655853
Epoch: 3490, Batch Gradient Norm: 9.292862379693874
Epoch: 3490, Batch Gradient Norm after: 9.292862379693874
Epoch 3491/10000, Prediction Accuracy = 61.443999999999996%, Loss = 0.48183342814445496
Epoch: 3491, Batch Gradient Norm: 8.265751318121913
Epoch: 3491, Batch Gradient Norm after: 8.265751318121913
Epoch 3492/10000, Prediction Accuracy = 61.528000000000006%, Loss = 0.4745506763458252
Epoch: 3492, Batch Gradient Norm: 9.97322967784643
Epoch: 3492, Batch Gradient Norm after: 9.97322967784643
Epoch 3493/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.4829752087593079
Epoch: 3493, Batch Gradient Norm: 13.578877262588017
Epoch: 3493, Batch Gradient Norm after: 13.578877262588017
Epoch 3494/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.5122783422470093
Epoch: 3494, Batch Gradient Norm: 11.311033875038616
Epoch: 3494, Batch Gradient Norm after: 11.311033875038616
Epoch 3495/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.49558562636375425
Epoch: 3495, Batch Gradient Norm: 11.233173413795635
Epoch: 3495, Batch Gradient Norm after: 11.233173413795635
Epoch 3496/10000, Prediction Accuracy = 61.4%, Loss = 0.49355286955833433
Epoch: 3496, Batch Gradient Norm: 11.106702419209022
Epoch: 3496, Batch Gradient Norm after: 11.106702419209022
Epoch 3497/10000, Prediction Accuracy = 61.472%, Loss = 0.49194672107696535
Epoch: 3497, Batch Gradient Norm: 9.694857547625928
Epoch: 3497, Batch Gradient Norm after: 9.694857547625928
Epoch 3498/10000, Prediction Accuracy = 61.492000000000004%, Loss = 0.4821541428565979
Epoch: 3498, Batch Gradient Norm: 8.74812824995329
Epoch: 3498, Batch Gradient Norm after: 8.74812824995329
Epoch 3499/10000, Prediction Accuracy = 61.484%, Loss = 0.4765825033187866
Epoch: 3499, Batch Gradient Norm: 9.110465203551257
Epoch: 3499, Batch Gradient Norm after: 9.110465203551257
Epoch 3500/10000, Prediction Accuracy = 61.45399999999999%, Loss = 0.47868428826332093
Epoch: 3500, Batch Gradient Norm: 11.040951330164837
Epoch: 3500, Batch Gradient Norm after: 11.040951330164837
Epoch 3501/10000, Prediction Accuracy = 61.496%, Loss = 0.4919364809989929
Epoch: 3501, Batch Gradient Norm: 10.363651993344638
Epoch: 3501, Batch Gradient Norm after: 10.363651993344638
Epoch 3502/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.4849278092384338
Epoch: 3502, Batch Gradient Norm: 12.431990734975885
Epoch: 3502, Batch Gradient Norm after: 12.431990734975885
Epoch 3503/10000, Prediction Accuracy = 61.408%, Loss = 0.49905439615249636
Epoch: 3503, Batch Gradient Norm: 11.39148801637149
Epoch: 3503, Batch Gradient Norm after: 11.39148801637149
Epoch 3504/10000, Prediction Accuracy = 61.55%, Loss = 0.49316219091415403
Epoch: 3504, Batch Gradient Norm: 9.068271279495672
Epoch: 3504, Batch Gradient Norm after: 9.068271279495672
Epoch 3505/10000, Prediction Accuracy = 61.476%, Loss = 0.47791813015937806
Epoch: 3505, Batch Gradient Norm: 8.150804292192996
Epoch: 3505, Batch Gradient Norm after: 8.150804292192996
Epoch 3506/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4725719690322876
Epoch: 3506, Batch Gradient Norm: 8.611174898543469
Epoch: 3506, Batch Gradient Norm after: 8.611174898543469
Epoch 3507/10000, Prediction Accuracy = 61.492%, Loss = 0.47494269013404844
Epoch: 3507, Batch Gradient Norm: 11.456936973262662
Epoch: 3507, Batch Gradient Norm after: 11.456936973262662
Epoch 3508/10000, Prediction Accuracy = 61.354000000000006%, Loss = 0.49368345737457275
Epoch: 3508, Batch Gradient Norm: 11.549859677175656
Epoch: 3508, Batch Gradient Norm after: 11.549859677175656
Epoch 3509/10000, Prediction Accuracy = 61.426%, Loss = 0.4984687328338623
Epoch: 3509, Batch Gradient Norm: 8.918920230495765
Epoch: 3509, Batch Gradient Norm after: 8.918920230495765
Epoch 3510/10000, Prediction Accuracy = 61.541999999999994%, Loss = 0.4780758321285248
Epoch: 3510, Batch Gradient Norm: 11.754985566591142
Epoch: 3510, Batch Gradient Norm after: 11.754985566591142
Epoch 3511/10000, Prediction Accuracy = 61.39200000000001%, Loss = 0.4959412455558777
Epoch: 3511, Batch Gradient Norm: 12.622023959962965
Epoch: 3511, Batch Gradient Norm after: 12.622023959962965
Epoch 3512/10000, Prediction Accuracy = 61.476%, Loss = 0.504944908618927
Epoch: 3512, Batch Gradient Norm: 8.803869968894436
Epoch: 3512, Batch Gradient Norm after: 8.803869968894436
Epoch 3513/10000, Prediction Accuracy = 61.426%, Loss = 0.4771582782268524
Epoch: 3513, Batch Gradient Norm: 9.677386206675745
Epoch: 3513, Batch Gradient Norm after: 9.677386206675745
Epoch 3514/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.4813480615615845
Epoch: 3514, Batch Gradient Norm: 9.873306129350208
Epoch: 3514, Batch Gradient Norm after: 9.873306129350208
Epoch 3515/10000, Prediction Accuracy = 61.416%, Loss = 0.4826549530029297
Epoch: 3515, Batch Gradient Norm: 9.810851877914091
Epoch: 3515, Batch Gradient Norm after: 9.810851877914091
Epoch 3516/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.48329455256462095
Epoch: 3516, Batch Gradient Norm: 8.361268759278714
Epoch: 3516, Batch Gradient Norm after: 8.361268759278714
Epoch 3517/10000, Prediction Accuracy = 61.496%, Loss = 0.4753279507160187
Epoch: 3517, Batch Gradient Norm: 10.164404327057774
Epoch: 3517, Batch Gradient Norm after: 10.164404327057774
Epoch 3518/10000, Prediction Accuracy = 61.386%, Loss = 0.4862002909183502
Epoch: 3518, Batch Gradient Norm: 13.064470190834557
Epoch: 3518, Batch Gradient Norm after: 13.064470190834557
Epoch 3519/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.5050963640213013
Epoch: 3519, Batch Gradient Norm: 12.755059183566388
Epoch: 3519, Batch Gradient Norm after: 12.755059183566388
Epoch 3520/10000, Prediction Accuracy = 61.432%, Loss = 0.5020776450634002
Epoch: 3520, Batch Gradient Norm: 10.398263726560092
Epoch: 3520, Batch Gradient Norm after: 10.398263726560092
Epoch 3521/10000, Prediction Accuracy = 61.44%, Loss = 0.4850513219833374
Epoch: 3521, Batch Gradient Norm: 9.402463612143448
Epoch: 3521, Batch Gradient Norm after: 9.402463612143448
Epoch 3522/10000, Prediction Accuracy = 61.448%, Loss = 0.47930687069892886
Epoch: 3522, Batch Gradient Norm: 9.704830714851457
Epoch: 3522, Batch Gradient Norm after: 9.704830714851457
Epoch 3523/10000, Prediction Accuracy = 61.408%, Loss = 0.4818443298339844
Epoch: 3523, Batch Gradient Norm: 8.73614237218309
Epoch: 3523, Batch Gradient Norm after: 8.73614237218309
Epoch 3524/10000, Prediction Accuracy = 61.416%, Loss = 0.4766310930252075
Epoch: 3524, Batch Gradient Norm: 10.368137032065247
Epoch: 3524, Batch Gradient Norm after: 10.368137032065247
Epoch 3525/10000, Prediction Accuracy = 61.512%, Loss = 0.48780425190925597
Epoch: 3525, Batch Gradient Norm: 8.947458526198721
Epoch: 3525, Batch Gradient Norm after: 8.947458526198721
Epoch 3526/10000, Prediction Accuracy = 61.426%, Loss = 0.47776848673820493
Epoch: 3526, Batch Gradient Norm: 11.295690371228106
Epoch: 3526, Batch Gradient Norm after: 11.295690371228106
Epoch 3527/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.49036031365394595
Epoch: 3527, Batch Gradient Norm: 13.084359906051748
Epoch: 3527, Batch Gradient Norm after: 13.084359906051748
Epoch 3528/10000, Prediction Accuracy = 61.46999999999999%, Loss = 0.5034752249717712
Epoch: 3528, Batch Gradient Norm: 10.713630840856668
Epoch: 3528, Batch Gradient Norm after: 10.713630840856668
Epoch 3529/10000, Prediction Accuracy = 61.536%, Loss = 0.48769020438194277
Epoch: 3529, Batch Gradient Norm: 9.284495232087188
Epoch: 3529, Batch Gradient Norm after: 9.284495232087188
Epoch 3530/10000, Prediction Accuracy = 61.488%, Loss = 0.4800530970096588
Epoch: 3530, Batch Gradient Norm: 9.768667567504984
Epoch: 3530, Batch Gradient Norm after: 9.768667567504984
Epoch 3531/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.48296839594841
Epoch: 3531, Batch Gradient Norm: 11.890865749985815
Epoch: 3531, Batch Gradient Norm after: 11.890865749985815
Epoch 3532/10000, Prediction Accuracy = 61.412%, Loss = 0.49725592136383057
Epoch: 3532, Batch Gradient Norm: 11.558996142242087
Epoch: 3532, Batch Gradient Norm after: 11.558996142242087
Epoch 3533/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4937829375267029
Epoch: 3533, Batch Gradient Norm: 11.72945232871175
Epoch: 3533, Batch Gradient Norm after: 11.72945232871175
Epoch 3534/10000, Prediction Accuracy = 61.394000000000005%, Loss = 0.4939260184764862
Epoch: 3534, Batch Gradient Norm: 10.092472094030342
Epoch: 3534, Batch Gradient Norm after: 10.092472094030342
Epoch 3535/10000, Prediction Accuracy = 61.438%, Loss = 0.4829435110092163
Epoch: 3535, Batch Gradient Norm: 9.784786203926984
Epoch: 3535, Batch Gradient Norm after: 9.784786203926984
Epoch 3536/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.48177210092544553
Epoch: 3536, Batch Gradient Norm: 8.812062644264728
Epoch: 3536, Batch Gradient Norm after: 8.812062644264728
Epoch 3537/10000, Prediction Accuracy = 61.476%, Loss = 0.4766442716121674
Epoch: 3537, Batch Gradient Norm: 8.879309593232191
Epoch: 3537, Batch Gradient Norm after: 8.879309593232191
Epoch 3538/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.4770197331905365
Epoch: 3538, Batch Gradient Norm: 9.231673875952138
Epoch: 3538, Batch Gradient Norm after: 9.231673875952138
Epoch 3539/10000, Prediction Accuracy = 61.434000000000005%, Loss = 0.4781797707080841
Epoch: 3539, Batch Gradient Norm: 10.927856382822126
Epoch: 3539, Batch Gradient Norm after: 10.927856382822126
Epoch 3540/10000, Prediction Accuracy = 61.384%, Loss = 0.48859766125679016
Epoch: 3540, Batch Gradient Norm: 11.387843636037692
Epoch: 3540, Batch Gradient Norm after: 11.387843636037692
Epoch 3541/10000, Prediction Accuracy = 61.504%, Loss = 0.4927660644054413
Epoch: 3541, Batch Gradient Norm: 9.036597731668822
Epoch: 3541, Batch Gradient Norm after: 9.036597731668822
Epoch 3542/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4781372606754303
Epoch: 3542, Batch Gradient Norm: 9.833825589242508
Epoch: 3542, Batch Gradient Norm after: 9.833825589242508
Epoch 3543/10000, Prediction Accuracy = 61.42800000000001%, Loss = 0.48313587307929995
Epoch: 3543, Batch Gradient Norm: 11.126985470036468
Epoch: 3543, Batch Gradient Norm after: 11.126985470036468
Epoch 3544/10000, Prediction Accuracy = 61.46999999999999%, Loss = 0.4905551791191101
Epoch: 3544, Batch Gradient Norm: 11.666518075966895
Epoch: 3544, Batch Gradient Norm after: 11.666518075966895
Epoch 3545/10000, Prediction Accuracy = 61.456%, Loss = 0.4945691466331482
Epoch: 3545, Batch Gradient Norm: 9.538072402787337
Epoch: 3545, Batch Gradient Norm after: 9.538072402787337
Epoch 3546/10000, Prediction Accuracy = 61.376%, Loss = 0.47971317172050476
Epoch: 3546, Batch Gradient Norm: 11.198056752035054
Epoch: 3546, Batch Gradient Norm after: 11.198056752035054
Epoch 3547/10000, Prediction Accuracy = 61.52%, Loss = 0.49107101559638977
Epoch: 3547, Batch Gradient Norm: 10.73614400696694
Epoch: 3547, Batch Gradient Norm after: 10.73614400696694
Epoch 3548/10000, Prediction Accuracy = 61.416%, Loss = 0.4882756471633911
Epoch: 3548, Batch Gradient Norm: 10.293549317843857
Epoch: 3548, Batch Gradient Norm after: 10.293549317843857
Epoch 3549/10000, Prediction Accuracy = 61.5%, Loss = 0.48409377336502074
Epoch: 3549, Batch Gradient Norm: 10.554591105805143
Epoch: 3549, Batch Gradient Norm after: 10.554591105805143
Epoch 3550/10000, Prediction Accuracy = 61.45%, Loss = 0.484686017036438
Epoch: 3550, Batch Gradient Norm: 10.676578768327525
Epoch: 3550, Batch Gradient Norm after: 10.676578768327525
Epoch 3551/10000, Prediction Accuracy = 61.446000000000005%, Loss = 0.48475311398506166
Epoch: 3551, Batch Gradient Norm: 10.633683716690218
Epoch: 3551, Batch Gradient Norm after: 10.633683716690218
Epoch 3552/10000, Prediction Accuracy = 61.46400000000001%, Loss = 0.4851283133029938
Epoch: 3552, Batch Gradient Norm: 9.517373860830228
Epoch: 3552, Batch Gradient Norm after: 9.517373860830228
Epoch 3553/10000, Prediction Accuracy = 61.406000000000006%, Loss = 0.4791319966316223
Epoch: 3553, Batch Gradient Norm: 10.281217434838238
Epoch: 3553, Batch Gradient Norm after: 10.281217434838238
Epoch 3554/10000, Prediction Accuracy = 61.48199999999999%, Loss = 0.4851125478744507
Epoch: 3554, Batch Gradient Norm: 9.748775003912845
Epoch: 3554, Batch Gradient Norm after: 9.748775003912845
Epoch 3555/10000, Prediction Accuracy = 61.426%, Loss = 0.48208948969841003
Epoch: 3555, Batch Gradient Norm: 10.56833675882786
Epoch: 3555, Batch Gradient Norm after: 10.56833675882786
Epoch 3556/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.48673409819602964
Epoch: 3556, Batch Gradient Norm: 9.850274627580045
Epoch: 3556, Batch Gradient Norm after: 9.850274627580045
Epoch 3557/10000, Prediction Accuracy = 61.51800000000001%, Loss = 0.48156928420066836
Epoch: 3557, Batch Gradient Norm: 10.143614180204118
Epoch: 3557, Batch Gradient Norm after: 10.143614180204118
Epoch 3558/10000, Prediction Accuracy = 61.462%, Loss = 0.48349976539611816
Epoch: 3558, Batch Gradient Norm: 11.206780102897646
Epoch: 3558, Batch Gradient Norm after: 11.206780102897646
Epoch 3559/10000, Prediction Accuracy = 61.448%, Loss = 0.4932478368282318
Epoch: 3559, Batch Gradient Norm: 12.007604782263263
Epoch: 3559, Batch Gradient Norm after: 12.007604782263263
Epoch 3560/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4976553559303284
Epoch: 3560, Batch Gradient Norm: 10.636893741841169
Epoch: 3560, Batch Gradient Norm after: 10.636893741841169
Epoch 3561/10000, Prediction Accuracy = 61.414%, Loss = 0.48834116458892823
Epoch: 3561, Batch Gradient Norm: 8.158779498710198
Epoch: 3561, Batch Gradient Norm after: 8.158779498710198
Epoch 3562/10000, Prediction Accuracy = 61.58%, Loss = 0.4719869911670685
Epoch: 3562, Batch Gradient Norm: 8.04869933433276
Epoch: 3562, Batch Gradient Norm after: 8.04869933433276
Epoch 3563/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.47015105485916137
Epoch: 3563, Batch Gradient Norm: 9.00897303284521
Epoch: 3563, Batch Gradient Norm after: 9.00897303284521
Epoch 3564/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.4745829164981842
Epoch: 3564, Batch Gradient Norm: 10.412685135774115
Epoch: 3564, Batch Gradient Norm after: 10.412685135774115
Epoch 3565/10000, Prediction Accuracy = 61.532000000000004%, Loss = 0.4824755072593689
Epoch: 3565, Batch Gradient Norm: 12.47691471882007
Epoch: 3565, Batch Gradient Norm after: 12.47691471882007
Epoch 3566/10000, Prediction Accuracy = 61.426%, Loss = 0.49826620221138
Epoch: 3566, Batch Gradient Norm: 12.712112589518158
Epoch: 3566, Batch Gradient Norm after: 12.712112589518158
Epoch 3567/10000, Prediction Accuracy = 61.495999999999995%, Loss = 0.5028586447238922
Epoch: 3567, Batch Gradient Norm: 11.61569175165995
Epoch: 3567, Batch Gradient Norm after: 11.61569175165995
Epoch 3568/10000, Prediction Accuracy = 61.54600000000001%, Loss = 0.4953331708908081
Epoch: 3568, Batch Gradient Norm: 9.867297041079674
Epoch: 3568, Batch Gradient Norm after: 9.867297041079674
Epoch 3569/10000, Prediction Accuracy = 61.488%, Loss = 0.4826083600521088
Epoch: 3569, Batch Gradient Norm: 8.045364544705947
Epoch: 3569, Batch Gradient Norm after: 8.045364544705947
Epoch 3570/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.47172234058380125
Epoch: 3570, Batch Gradient Norm: 8.538280992423433
Epoch: 3570, Batch Gradient Norm after: 8.538280992423433
Epoch 3571/10000, Prediction Accuracy = 61.53399999999999%, Loss = 0.4745232820510864
Epoch: 3571, Batch Gradient Norm: 9.953346302352797
Epoch: 3571, Batch Gradient Norm after: 9.953346302352797
Epoch 3572/10000, Prediction Accuracy = 61.444%, Loss = 0.4816656351089478
Epoch: 3572, Batch Gradient Norm: 12.304380134749088
Epoch: 3572, Batch Gradient Norm after: 12.304380134749088
Epoch 3573/10000, Prediction Accuracy = 61.57199999999999%, Loss = 0.49761065244674685
Epoch: 3573, Batch Gradient Norm: 12.548670305084276
Epoch: 3573, Batch Gradient Norm after: 12.548670305084276
Epoch 3574/10000, Prediction Accuracy = 61.403999999999996%, Loss = 0.49906927943229673
Epoch: 3574, Batch Gradient Norm: 10.886011203977363
Epoch: 3574, Batch Gradient Norm after: 10.886011203977363
Epoch 3575/10000, Prediction Accuracy = 61.492000000000004%, Loss = 0.4876939058303833
Epoch: 3575, Batch Gradient Norm: 10.92284802346615
Epoch: 3575, Batch Gradient Norm after: 10.92284802346615
Epoch 3576/10000, Prediction Accuracy = 61.44%, Loss = 0.4891869068145752
Epoch: 3576, Batch Gradient Norm: 9.158777583505223
Epoch: 3576, Batch Gradient Norm after: 9.158777583505223
Epoch 3577/10000, Prediction Accuracy = 61.49399999999999%, Loss = 0.4767977178096771
Epoch: 3577, Batch Gradient Norm: 8.801936599106384
Epoch: 3577, Batch Gradient Norm after: 8.801936599106384
Epoch 3578/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.47442479729652404
Epoch: 3578, Batch Gradient Norm: 8.284806812503106
Epoch: 3578, Batch Gradient Norm after: 8.284806812503106
Epoch 3579/10000, Prediction Accuracy = 61.452%, Loss = 0.47104390859603884
Epoch: 3579, Batch Gradient Norm: 8.783852290221468
Epoch: 3579, Batch Gradient Norm after: 8.783852290221468
Epoch 3580/10000, Prediction Accuracy = 61.504%, Loss = 0.4732478320598602
Epoch: 3580, Batch Gradient Norm: 11.29004359254438
Epoch: 3580, Batch Gradient Norm after: 11.29004359254438
Epoch 3581/10000, Prediction Accuracy = 61.528%, Loss = 0.4901872217655182
Epoch: 3581, Batch Gradient Norm: 11.77723007961757
Epoch: 3581, Batch Gradient Norm after: 11.77723007961757
Epoch 3582/10000, Prediction Accuracy = 61.49399999999999%, Loss = 0.49434356689453124
Epoch: 3582, Batch Gradient Norm: 11.882883807436743
Epoch: 3582, Batch Gradient Norm after: 11.882883807436743
Epoch 3583/10000, Prediction Accuracy = 61.488%, Loss = 0.49489387273788454
Epoch: 3583, Batch Gradient Norm: 10.997036558662291
Epoch: 3583, Batch Gradient Norm after: 10.997036558662291
Epoch 3584/10000, Prediction Accuracy = 61.465999999999994%, Loss = 0.49000409841537473
Epoch: 3584, Batch Gradient Norm: 8.342125379630668
Epoch: 3584, Batch Gradient Norm after: 8.342125379630668
Epoch 3585/10000, Prediction Accuracy = 61.436%, Loss = 0.47239285707473755
Epoch: 3585, Batch Gradient Norm: 9.411821911934252
Epoch: 3585, Batch Gradient Norm after: 9.411821911934252
Epoch 3586/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.47885852456092837
Epoch: 3586, Batch Gradient Norm: 8.618549580976843
Epoch: 3586, Batch Gradient Norm after: 8.618549580976843
Epoch 3587/10000, Prediction Accuracy = 61.462%, Loss = 0.4730229377746582
Epoch: 3587, Batch Gradient Norm: 10.090121228224293
Epoch: 3587, Batch Gradient Norm after: 10.090121228224293
Epoch 3588/10000, Prediction Accuracy = 61.55999999999999%, Loss = 0.4807020008563995
Epoch: 3588, Batch Gradient Norm: 11.988693204255911
Epoch: 3588, Batch Gradient Norm after: 11.988693204255911
Epoch 3589/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.49238035678863523
Epoch: 3589, Batch Gradient Norm: 13.044234383163655
Epoch: 3589, Batch Gradient Norm after: 13.044234383163655
Epoch 3590/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.5011169075965881
Epoch: 3590, Batch Gradient Norm: 13.375637921747922
Epoch: 3590, Batch Gradient Norm after: 13.375637921747922
Epoch 3591/10000, Prediction Accuracy = 61.388%, Loss = 0.5068207800388336
Epoch: 3591, Batch Gradient Norm: 11.23045860993091
Epoch: 3591, Batch Gradient Norm after: 11.23045860993091
Epoch 3592/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.49158689975738523
Epoch: 3592, Batch Gradient Norm: 8.639703921758425
Epoch: 3592, Batch Gradient Norm after: 8.639703921758425
Epoch 3593/10000, Prediction Accuracy = 61.488%, Loss = 0.47344969511032103
Epoch: 3593, Batch Gradient Norm: 8.737835778558729
Epoch: 3593, Batch Gradient Norm after: 8.737835778558729
Epoch 3594/10000, Prediction Accuracy = 61.52%, Loss = 0.4729186236858368
Epoch: 3594, Batch Gradient Norm: 9.963037630398395
Epoch: 3594, Batch Gradient Norm after: 9.963037630398395
Epoch 3595/10000, Prediction Accuracy = 61.476%, Loss = 0.4800424873828888
Epoch: 3595, Batch Gradient Norm: 10.902581226519287
Epoch: 3595, Batch Gradient Norm after: 10.902581226519287
Epoch 3596/10000, Prediction Accuracy = 61.477999999999994%, Loss = 0.4864037573337555
Epoch: 3596, Batch Gradient Norm: 9.464749482216648
Epoch: 3596, Batch Gradient Norm after: 9.464749482216648
Epoch 3597/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.4780698657035828
Epoch: 3597, Batch Gradient Norm: 9.090565316402184
Epoch: 3597, Batch Gradient Norm after: 9.090565316402184
Epoch 3598/10000, Prediction Accuracy = 61.41199999999999%, Loss = 0.4753066062927246
Epoch: 3598, Batch Gradient Norm: 10.314912838547809
Epoch: 3598, Batch Gradient Norm after: 10.314912838547809
Epoch 3599/10000, Prediction Accuracy = 61.56999999999999%, Loss = 0.48420992493629456
Epoch: 3599, Batch Gradient Norm: 8.674441151942084
Epoch: 3599, Batch Gradient Norm after: 8.674441151942084
Epoch 3600/10000, Prediction Accuracy = 61.43800000000001%, Loss = 0.47447388172149657
Epoch: 3600, Batch Gradient Norm: 8.689429628719855
Epoch: 3600, Batch Gradient Norm after: 8.689429628719855
Epoch 3601/10000, Prediction Accuracy = 61.474000000000004%, Loss = 0.4740997314453125
Epoch: 3601, Batch Gradient Norm: 8.608025464769607
Epoch: 3601, Batch Gradient Norm after: 8.608025464769607
Epoch 3602/10000, Prediction Accuracy = 61.46%, Loss = 0.47303398251533507
Epoch: 3602, Batch Gradient Norm: 11.280442305165826
Epoch: 3602, Batch Gradient Norm after: 11.280442305165826
Epoch 3603/10000, Prediction Accuracy = 61.426%, Loss = 0.48978039622306824
Epoch: 3603, Batch Gradient Norm: 13.70511094518613
Epoch: 3603, Batch Gradient Norm after: 13.70511094518613
Epoch 3604/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.5103970885276794
Epoch: 3604, Batch Gradient Norm: 10.571685215791929
Epoch: 3604, Batch Gradient Norm after: 10.571685215791929
Epoch 3605/10000, Prediction Accuracy = 61.512%, Loss = 0.48733530640602113
Epoch: 3605, Batch Gradient Norm: 9.923255778338772
Epoch: 3605, Batch Gradient Norm after: 9.923255778338772
Epoch 3606/10000, Prediction Accuracy = 61.536%, Loss = 0.48202582001686095
Epoch: 3606, Batch Gradient Norm: 9.917871514555689
Epoch: 3606, Batch Gradient Norm after: 9.917871514555689
Epoch 3607/10000, Prediction Accuracy = 61.552%, Loss = 0.4810931205749512
Epoch: 3607, Batch Gradient Norm: 11.165412404990656
Epoch: 3607, Batch Gradient Norm after: 11.165412404990656
Epoch 3608/10000, Prediction Accuracy = 61.464%, Loss = 0.4881138801574707
Epoch: 3608, Batch Gradient Norm: 11.506217871346767
Epoch: 3608, Batch Gradient Norm after: 11.506217871346767
Epoch 3609/10000, Prediction Accuracy = 61.488%, Loss = 0.4887452006340027
Epoch: 3609, Batch Gradient Norm: 11.134481734346172
Epoch: 3609, Batch Gradient Norm after: 11.134481734346172
Epoch 3610/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.48628122806549073
Epoch: 3610, Batch Gradient Norm: 9.62130290235975
Epoch: 3610, Batch Gradient Norm after: 9.62130290235975
Epoch 3611/10000, Prediction Accuracy = 61.501999999999995%, Loss = 0.47748228907585144
Epoch: 3611, Batch Gradient Norm: 8.057620883544384
Epoch: 3611, Batch Gradient Norm after: 8.057620883544384
Epoch 3612/10000, Prediction Accuracy = 61.55%, Loss = 0.46887763142585753
Epoch: 3612, Batch Gradient Norm: 8.879841220169919
Epoch: 3612, Batch Gradient Norm after: 8.879841220169919
Epoch 3613/10000, Prediction Accuracy = 61.536%, Loss = 0.4737914323806763
Epoch: 3613, Batch Gradient Norm: 10.152524597743428
Epoch: 3613, Batch Gradient Norm after: 10.152524597743428
Epoch 3614/10000, Prediction Accuracy = 61.41799999999999%, Loss = 0.48161853551864625
Epoch: 3614, Batch Gradient Norm: 12.105894134635404
Epoch: 3614, Batch Gradient Norm after: 12.105894134635404
Epoch 3615/10000, Prediction Accuracy = 61.507999999999996%, Loss = 0.4981741666793823
Epoch: 3615, Batch Gradient Norm: 9.511217247489101
Epoch: 3615, Batch Gradient Norm after: 9.511217247489101
Epoch 3616/10000, Prediction Accuracy = 61.55%, Loss = 0.4776281535625458
Epoch: 3616, Batch Gradient Norm: 10.694025345512435
Epoch: 3616, Batch Gradient Norm after: 10.694025345512435
Epoch 3617/10000, Prediction Accuracy = 61.580000000000005%, Loss = 0.48410064578056333
Epoch: 3617, Batch Gradient Norm: 12.477222677349596
Epoch: 3617, Batch Gradient Norm after: 12.477222677349596
Epoch 3618/10000, Prediction Accuracy = 61.41600000000001%, Loss = 0.49910020232200625
Epoch: 3618, Batch Gradient Norm: 11.2713761462521
Epoch: 3618, Batch Gradient Norm after: 11.2713761462521
Epoch 3619/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.48851572871208193
Epoch: 3619, Batch Gradient Norm: 10.472797781744367
Epoch: 3619, Batch Gradient Norm after: 10.472797781744367
Epoch 3620/10000, Prediction Accuracy = 61.46%, Loss = 0.48254777789115905
Epoch: 3620, Batch Gradient Norm: 10.201977164586932
Epoch: 3620, Batch Gradient Norm after: 10.201977164586932
Epoch 3621/10000, Prediction Accuracy = 61.512%, Loss = 0.4807534754276276
Epoch: 3621, Batch Gradient Norm: 10.915195958467912
Epoch: 3621, Batch Gradient Norm after: 10.915195958467912
Epoch 3622/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.48524185419082644
Epoch: 3622, Batch Gradient Norm: 12.753533504966004
Epoch: 3622, Batch Gradient Norm after: 12.753533504966004
Epoch 3623/10000, Prediction Accuracy = 61.465999999999994%, Loss = 0.49903552532196044
Epoch: 3623, Batch Gradient Norm: 10.76786043004087
Epoch: 3623, Batch Gradient Norm after: 10.76786043004087
Epoch 3624/10000, Prediction Accuracy = 61.581999999999994%, Loss = 0.48642816543579104
Epoch: 3624, Batch Gradient Norm: 7.426185965872996
Epoch: 3624, Batch Gradient Norm after: 7.426185965872996
Epoch 3625/10000, Prediction Accuracy = 61.492000000000004%, Loss = 0.4668381571769714
Epoch: 3625, Batch Gradient Norm: 8.298270417617717
Epoch: 3625, Batch Gradient Norm after: 8.298270417617717
Epoch 3626/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4718137443065643
Epoch: 3626, Batch Gradient Norm: 8.009719443172115
Epoch: 3626, Batch Gradient Norm after: 8.009719443172115
Epoch 3627/10000, Prediction Accuracy = 61.532%, Loss = 0.4711323261260986
Epoch: 3627, Batch Gradient Norm: 9.181631222413195
Epoch: 3627, Batch Gradient Norm after: 9.181631222413195
Epoch 3628/10000, Prediction Accuracy = 61.458000000000006%, Loss = 0.47602981328964233
Epoch: 3628, Batch Gradient Norm: 9.945458195354908
Epoch: 3628, Batch Gradient Norm after: 9.945458195354908
Epoch 3629/10000, Prediction Accuracy = 61.516%, Loss = 0.47921496629714966
Epoch: 3629, Batch Gradient Norm: 11.869892438089298
Epoch: 3629, Batch Gradient Norm after: 11.869892438089298
Epoch 3630/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.49278104305267334
Epoch: 3630, Batch Gradient Norm: 11.952213216972245
Epoch: 3630, Batch Gradient Norm after: 11.952213216972245
Epoch 3631/10000, Prediction Accuracy = 61.524%, Loss = 0.4956596553325653
Epoch: 3631, Batch Gradient Norm: 8.831791860032704
Epoch: 3631, Batch Gradient Norm after: 8.831791860032704
Epoch 3632/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4746842265129089
Epoch: 3632, Batch Gradient Norm: 9.844231066458327
Epoch: 3632, Batch Gradient Norm after: 9.844231066458327
Epoch 3633/10000, Prediction Accuracy = 61.5%, Loss = 0.4793492555618286
Epoch: 3633, Batch Gradient Norm: 11.5460560625976
Epoch: 3633, Batch Gradient Norm after: 11.5460560625976
Epoch 3634/10000, Prediction Accuracy = 61.55799999999999%, Loss = 0.48985114097595217
Epoch: 3634, Batch Gradient Norm: 11.738955905453992
Epoch: 3634, Batch Gradient Norm after: 11.738955905453992
Epoch 3635/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.4917768180370331
Epoch: 3635, Batch Gradient Norm: 8.805097761422758
Epoch: 3635, Batch Gradient Norm after: 8.805097761422758
Epoch 3636/10000, Prediction Accuracy = 61.580000000000005%, Loss = 0.4722011208534241
Epoch: 3636, Batch Gradient Norm: 10.895544920566934
Epoch: 3636, Batch Gradient Norm after: 10.895544920566934
Epoch 3637/10000, Prediction Accuracy = 61.54200000000001%, Loss = 0.4835879862308502
Epoch: 3637, Batch Gradient Norm: 13.12924140357806
Epoch: 3637, Batch Gradient Norm after: 13.12924140357806
Epoch 3638/10000, Prediction Accuracy = 61.456%, Loss = 0.49990384578704833
Epoch: 3638, Batch Gradient Norm: 11.157822377581239
Epoch: 3638, Batch Gradient Norm after: 11.157822377581239
Epoch 3639/10000, Prediction Accuracy = 61.448%, Loss = 0.4870043694972992
Epoch: 3639, Batch Gradient Norm: 10.335809904718262
Epoch: 3639, Batch Gradient Norm after: 10.335809904718262
Epoch 3640/10000, Prediction Accuracy = 61.648%, Loss = 0.48176448941230776
Epoch: 3640, Batch Gradient Norm: 9.473015563833593
Epoch: 3640, Batch Gradient Norm after: 9.473015563833593
Epoch 3641/10000, Prediction Accuracy = 61.464%, Loss = 0.47786269187927244
Epoch: 3641, Batch Gradient Norm: 8.49803692479132
Epoch: 3641, Batch Gradient Norm after: 8.49803692479132
Epoch 3642/10000, Prediction Accuracy = 61.574%, Loss = 0.47103497982025144
Epoch: 3642, Batch Gradient Norm: 9.429413505646375
Epoch: 3642, Batch Gradient Norm after: 9.429413505646375
Epoch 3643/10000, Prediction Accuracy = 61.46%, Loss = 0.4758395075798035
Epoch: 3643, Batch Gradient Norm: 10.273742933092821
Epoch: 3643, Batch Gradient Norm after: 10.273742933092821
Epoch 3644/10000, Prediction Accuracy = 61.484%, Loss = 0.4814080476760864
Epoch: 3644, Batch Gradient Norm: 10.676645168930705
Epoch: 3644, Batch Gradient Norm after: 10.676645168930705
Epoch 3645/10000, Prediction Accuracy = 61.592%, Loss = 0.4843107759952545
Epoch: 3645, Batch Gradient Norm: 9.670536866830695
Epoch: 3645, Batch Gradient Norm after: 9.670536866830695
Epoch 3646/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.4774389684200287
Epoch: 3646, Batch Gradient Norm: 9.541448718797517
Epoch: 3646, Batch Gradient Norm after: 9.541448718797517
Epoch 3647/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4769745349884033
Epoch: 3647, Batch Gradient Norm: 10.21589093110439
Epoch: 3647, Batch Gradient Norm after: 10.21589093110439
Epoch 3648/10000, Prediction Accuracy = 61.498000000000005%, Loss = 0.4799638867378235
Epoch: 3648, Batch Gradient Norm: 11.98309822468921
Epoch: 3648, Batch Gradient Norm after: 11.98309822468921
Epoch 3649/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.49178563356399535
Epoch: 3649, Batch Gradient Norm: 12.836036770072264
Epoch: 3649, Batch Gradient Norm after: 12.836036770072264
Epoch 3650/10000, Prediction Accuracy = 61.42999999999999%, Loss = 0.4994176149368286
Epoch: 3650, Batch Gradient Norm: 10.953931242572857
Epoch: 3650, Batch Gradient Norm after: 10.953931242572857
Epoch 3651/10000, Prediction Accuracy = 61.504%, Loss = 0.48573129177093505
Epoch: 3651, Batch Gradient Norm: 10.024385304433563
Epoch: 3651, Batch Gradient Norm after: 10.024385304433563
Epoch 3652/10000, Prediction Accuracy = 61.382000000000005%, Loss = 0.4791626811027527
Epoch: 3652, Batch Gradient Norm: 9.680483470447625
Epoch: 3652, Batch Gradient Norm after: 9.680483470447625
Epoch 3653/10000, Prediction Accuracy = 61.488%, Loss = 0.4779572248458862
Epoch: 3653, Batch Gradient Norm: 9.149667407318521
Epoch: 3653, Batch Gradient Norm after: 9.149667407318521
Epoch 3654/10000, Prediction Accuracy = 61.432%, Loss = 0.4754196584224701
Epoch: 3654, Batch Gradient Norm: 9.166118592586406
Epoch: 3654, Batch Gradient Norm after: 9.166118592586406
Epoch 3655/10000, Prediction Accuracy = 61.476%, Loss = 0.4774812340736389
Epoch: 3655, Batch Gradient Norm: 6.485149329599971
Epoch: 3655, Batch Gradient Norm after: 6.485149329599971
Epoch 3656/10000, Prediction Accuracy = 61.54%, Loss = 0.4626207411289215
Epoch: 3656, Batch Gradient Norm: 7.375256573943954
Epoch: 3656, Batch Gradient Norm after: 7.375256573943954
Epoch 3657/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.4656034529209137
Epoch: 3657, Batch Gradient Norm: 10.17578075584064
Epoch: 3657, Batch Gradient Norm after: 10.17578075584064
Epoch 3658/10000, Prediction Accuracy = 61.532%, Loss = 0.4814846277236938
Epoch: 3658, Batch Gradient Norm: 12.635101770740834
Epoch: 3658, Batch Gradient Norm after: 12.635101770740834
Epoch 3659/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.4985868275165558
Epoch: 3659, Batch Gradient Norm: 10.412883607645954
Epoch: 3659, Batch Gradient Norm after: 10.412883607645954
Epoch 3660/10000, Prediction Accuracy = 61.588%, Loss = 0.48170108795166017
Epoch: 3660, Batch Gradient Norm: 8.960171266983137
Epoch: 3660, Batch Gradient Norm after: 8.960171266983137
Epoch 3661/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.47219008207321167
Epoch: 3661, Batch Gradient Norm: 10.169640827474781
Epoch: 3661, Batch Gradient Norm after: 10.169640827474781
Epoch 3662/10000, Prediction Accuracy = 61.54%, Loss = 0.4802023947238922
Epoch: 3662, Batch Gradient Norm: 12.957250544283662
Epoch: 3662, Batch Gradient Norm after: 12.957250544283662
Epoch 3663/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.5027731001377106
Epoch: 3663, Batch Gradient Norm: 10.755633369379417
Epoch: 3663, Batch Gradient Norm after: 10.755633369379417
Epoch 3664/10000, Prediction Accuracy = 61.54%, Loss = 0.48478257060050967
Epoch: 3664, Batch Gradient Norm: 9.891685942664639
Epoch: 3664, Batch Gradient Norm after: 9.891685942664639
Epoch 3665/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.47761449217796326
Epoch: 3665, Batch Gradient Norm: 13.063461458782069
Epoch: 3665, Batch Gradient Norm after: 13.063461458782069
Epoch 3666/10000, Prediction Accuracy = 61.48%, Loss = 0.4993932664394379
Epoch: 3666, Batch Gradient Norm: 13.404690930255061
Epoch: 3666, Batch Gradient Norm after: 13.404690930255061
Epoch 3667/10000, Prediction Accuracy = 61.508%, Loss = 0.5034384965896607
Epoch: 3667, Batch Gradient Norm: 9.968165159828342
Epoch: 3667, Batch Gradient Norm after: 9.968165159828342
Epoch 3668/10000, Prediction Accuracy = 61.496%, Loss = 0.4776934623718262
Epoch: 3668, Batch Gradient Norm: 10.222030003740675
Epoch: 3668, Batch Gradient Norm after: 10.222030003740675
Epoch 3669/10000, Prediction Accuracy = 61.56600000000001%, Loss = 0.47821595072746276
Epoch: 3669, Batch Gradient Norm: 11.562803550208171
Epoch: 3669, Batch Gradient Norm after: 11.562803550208171
Epoch 3670/10000, Prediction Accuracy = 61.592%, Loss = 0.48863822817802427
Epoch: 3670, Batch Gradient Norm: 9.013570912536625
Epoch: 3670, Batch Gradient Norm after: 9.013570912536625
Epoch 3671/10000, Prediction Accuracy = 61.492%, Loss = 0.4731841742992401
Epoch: 3671, Batch Gradient Norm: 7.9129623423491084
Epoch: 3671, Batch Gradient Norm after: 7.9129623423491084
Epoch 3672/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46689278483390806
Epoch: 3672, Batch Gradient Norm: 9.80108682847199
Epoch: 3672, Batch Gradient Norm after: 9.80108682847199
Epoch 3673/10000, Prediction Accuracy = 61.472%, Loss = 0.4783195972442627
Epoch: 3673, Batch Gradient Norm: 9.652795738879956
Epoch: 3673, Batch Gradient Norm after: 9.652795738879956
Epoch 3674/10000, Prediction Accuracy = 61.572%, Loss = 0.477112478017807
Epoch: 3674, Batch Gradient Norm: 10.646236657712066
Epoch: 3674, Batch Gradient Norm after: 10.646236657712066
Epoch 3675/10000, Prediction Accuracy = 61.524%, Loss = 0.4822156071662903
Epoch: 3675, Batch Gradient Norm: 9.885688250965455
Epoch: 3675, Batch Gradient Norm after: 9.885688250965455
Epoch 3676/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.4770357012748718
Epoch: 3676, Batch Gradient Norm: 9.521713825105234
Epoch: 3676, Batch Gradient Norm after: 9.521713825105234
Epoch 3677/10000, Prediction Accuracy = 61.516000000000005%, Loss = 0.474586546421051
Epoch: 3677, Batch Gradient Norm: 11.172193376878052
Epoch: 3677, Batch Gradient Norm after: 11.172193376878052
Epoch 3678/10000, Prediction Accuracy = 61.64%, Loss = 0.4883663415908813
Epoch: 3678, Batch Gradient Norm: 8.979555398812685
Epoch: 3678, Batch Gradient Norm after: 8.979555398812685
Epoch 3679/10000, Prediction Accuracy = 61.562%, Loss = 0.475913268327713
Epoch: 3679, Batch Gradient Norm: 8.224260999173318
Epoch: 3679, Batch Gradient Norm after: 8.224260999173318
Epoch 3680/10000, Prediction Accuracy = 61.58%, Loss = 0.47065619826316835
Epoch: 3680, Batch Gradient Norm: 7.899587938270583
Epoch: 3680, Batch Gradient Norm after: 7.899587938270583
Epoch 3681/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4680692791938782
Epoch: 3681, Batch Gradient Norm: 9.889529354048236
Epoch: 3681, Batch Gradient Norm after: 9.889529354048236
Epoch 3682/10000, Prediction Accuracy = 61.49400000000001%, Loss = 0.47801731824874877
Epoch: 3682, Batch Gradient Norm: 12.801341900968872
Epoch: 3682, Batch Gradient Norm after: 12.801341900968872
Epoch 3683/10000, Prediction Accuracy = 61.58%, Loss = 0.49874926209449766
Epoch: 3683, Batch Gradient Norm: 11.375991004495692
Epoch: 3683, Batch Gradient Norm after: 11.375991004495692
Epoch 3684/10000, Prediction Accuracy = 61.51800000000001%, Loss = 0.48602567315101625
Epoch: 3684, Batch Gradient Norm: 10.917739440557774
Epoch: 3684, Batch Gradient Norm after: 10.917739440557774
Epoch 3685/10000, Prediction Accuracy = 61.55799999999999%, Loss = 0.4837270438671112
Epoch: 3685, Batch Gradient Norm: 12.55212736579307
Epoch: 3685, Batch Gradient Norm after: 12.55212736579307
Epoch 3686/10000, Prediction Accuracy = 61.455999999999996%, Loss = 0.49419421553611753
Epoch: 3686, Batch Gradient Norm: 14.065967281297807
Epoch: 3686, Batch Gradient Norm after: 14.065967281297807
Epoch 3687/10000, Prediction Accuracy = 61.548%, Loss = 0.5086818099021911
Epoch: 3687, Batch Gradient Norm: 10.999883822855386
Epoch: 3687, Batch Gradient Norm after: 10.999883822855386
Epoch 3688/10000, Prediction Accuracy = 61.507999999999996%, Loss = 0.48558096289634706
Epoch: 3688, Batch Gradient Norm: 8.42083007819674
Epoch: 3688, Batch Gradient Norm after: 8.42083007819674
Epoch 3689/10000, Prediction Accuracy = 61.54599999999999%, Loss = 0.469626784324646
Epoch: 3689, Batch Gradient Norm: 8.010821776326834
Epoch: 3689, Batch Gradient Norm after: 8.010821776326834
Epoch 3690/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4671443700790405
Epoch: 3690, Batch Gradient Norm: 8.249825610991293
Epoch: 3690, Batch Gradient Norm after: 8.249825610991293
Epoch 3691/10000, Prediction Accuracy = 61.538%, Loss = 0.4687244415283203
Epoch: 3691, Batch Gradient Norm: 8.584544590963894
Epoch: 3691, Batch Gradient Norm after: 8.584544590963894
Epoch 3692/10000, Prediction Accuracy = 61.581999999999994%, Loss = 0.4694905936717987
Epoch: 3692, Batch Gradient Norm: 10.666338869537054
Epoch: 3692, Batch Gradient Norm after: 10.666338869537054
Epoch 3693/10000, Prediction Accuracy = 61.464%, Loss = 0.4813883125782013
Epoch: 3693, Batch Gradient Norm: 12.275718270900951
Epoch: 3693, Batch Gradient Norm after: 12.275718270900951
Epoch 3694/10000, Prediction Accuracy = 61.476%, Loss = 0.4928300142288208
Epoch: 3694, Batch Gradient Norm: 12.619081364311873
Epoch: 3694, Batch Gradient Norm after: 12.619081364311873
Epoch 3695/10000, Prediction Accuracy = 61.504%, Loss = 0.49647354483604433
Epoch: 3695, Batch Gradient Norm: 10.55593780485746
Epoch: 3695, Batch Gradient Norm after: 10.55593780485746
Epoch 3696/10000, Prediction Accuracy = 61.55%, Loss = 0.48158583641052244
Epoch: 3696, Batch Gradient Norm: 8.451265810952476
Epoch: 3696, Batch Gradient Norm after: 8.451265810952476
Epoch 3697/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.468302446603775
Epoch: 3697, Batch Gradient Norm: 8.087096863711926
Epoch: 3697, Batch Gradient Norm after: 8.087096863711926
Epoch 3698/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.46626601815223695
Epoch: 3698, Batch Gradient Norm: 8.229066965458012
Epoch: 3698, Batch Gradient Norm after: 8.229066965458012
Epoch 3699/10000, Prediction Accuracy = 61.508%, Loss = 0.4668764710426331
Epoch: 3699, Batch Gradient Norm: 11.580032635434959
Epoch: 3699, Batch Gradient Norm after: 11.580032635434959
Epoch 3700/10000, Prediction Accuracy = 61.562%, Loss = 0.4914266049861908
Epoch: 3700, Batch Gradient Norm: 7.449055700038171
Epoch: 3700, Batch Gradient Norm after: 7.449055700038171
Epoch 3701/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.4661548018455505
Epoch: 3701, Batch Gradient Norm: 7.380528212226388
Epoch: 3701, Batch Gradient Norm after: 7.380528212226388
Epoch 3702/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.46480422019958495
Epoch: 3702, Batch Gradient Norm: 10.173788026914783
Epoch: 3702, Batch Gradient Norm after: 10.173788026914783
Epoch 3703/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.48138025403022766
Epoch: 3703, Batch Gradient Norm: 14.424186956643457
Epoch: 3703, Batch Gradient Norm after: 14.424186956643457
Epoch 3704/10000, Prediction Accuracy = 61.510000000000005%, Loss = 0.5143348813056946
Epoch: 3704, Batch Gradient Norm: 12.39557278122058
Epoch: 3704, Batch Gradient Norm after: 12.39557278122058
Epoch 3705/10000, Prediction Accuracy = 61.529999999999994%, Loss = 0.4961896359920502
Epoch: 3705, Batch Gradient Norm: 8.372183954375055
Epoch: 3705, Batch Gradient Norm after: 8.372183954375055
Epoch 3706/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.4676132142543793
Epoch: 3706, Batch Gradient Norm: 9.76633224815814
Epoch: 3706, Batch Gradient Norm after: 9.76633224815814
Epoch 3707/10000, Prediction Accuracy = 61.576%, Loss = 0.4753193616867065
Epoch: 3707, Batch Gradient Norm: 10.80951887281737
Epoch: 3707, Batch Gradient Norm after: 10.80951887281737
Epoch 3708/10000, Prediction Accuracy = 61.638%, Loss = 0.4819916009902954
Epoch: 3708, Batch Gradient Norm: 10.208144601477635
Epoch: 3708, Batch Gradient Norm after: 10.208144601477635
Epoch 3709/10000, Prediction Accuracy = 61.536%, Loss = 0.47876508831977843
Epoch: 3709, Batch Gradient Norm: 9.136829154421369
Epoch: 3709, Batch Gradient Norm after: 9.136829154421369
Epoch 3710/10000, Prediction Accuracy = 61.69%, Loss = 0.47120234966278074
Epoch: 3710, Batch Gradient Norm: 10.813528939296681
Epoch: 3710, Batch Gradient Norm after: 10.813528939296681
Epoch 3711/10000, Prediction Accuracy = 61.459999999999994%, Loss = 0.4807908356189728
Epoch: 3711, Batch Gradient Norm: 12.163702895830822
Epoch: 3711, Batch Gradient Norm after: 12.163702895830822
Epoch 3712/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.4934541165828705
Epoch: 3712, Batch Gradient Norm: 10.870848767552298
Epoch: 3712, Batch Gradient Norm after: 10.870848767552298
Epoch 3713/10000, Prediction Accuracy = 61.513999999999996%, Loss = 0.48294498920440676
Epoch: 3713, Batch Gradient Norm: 9.018384574354485
Epoch: 3713, Batch Gradient Norm after: 9.018384574354485
Epoch 3714/10000, Prediction Accuracy = 61.57199999999999%, Loss = 0.471980881690979
Epoch: 3714, Batch Gradient Norm: 8.785436793870678
Epoch: 3714, Batch Gradient Norm after: 8.785436793870678
Epoch 3715/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.4698737978935242
Epoch: 3715, Batch Gradient Norm: 11.519738266987863
Epoch: 3715, Batch Gradient Norm after: 11.519738266987863
Epoch 3716/10000, Prediction Accuracy = 61.50599999999999%, Loss = 0.4867489397525787
Epoch: 3716, Batch Gradient Norm: 12.11727488300915
Epoch: 3716, Batch Gradient Norm after: 12.11727488300915
Epoch 3717/10000, Prediction Accuracy = 61.492%, Loss = 0.49257524609565734
Epoch: 3717, Batch Gradient Norm: 11.182971199083239
Epoch: 3717, Batch Gradient Norm after: 11.182971199083239
Epoch 3718/10000, Prediction Accuracy = 61.504%, Loss = 0.485880845785141
Epoch: 3718, Batch Gradient Norm: 12.110891470472056
Epoch: 3718, Batch Gradient Norm after: 12.110891470472056
Epoch 3719/10000, Prediction Accuracy = 61.564%, Loss = 0.49136780500411986
Epoch: 3719, Batch Gradient Norm: 13.411882699429833
Epoch: 3719, Batch Gradient Norm after: 13.411882699429833
Epoch 3720/10000, Prediction Accuracy = 61.598%, Loss = 0.501350712776184
Epoch: 3720, Batch Gradient Norm: 10.571142715225845
Epoch: 3720, Batch Gradient Norm after: 10.571142715225845
Epoch 3721/10000, Prediction Accuracy = 61.562%, Loss = 0.48156278729438784
Epoch: 3721, Batch Gradient Norm: 7.204869200192801
Epoch: 3721, Batch Gradient Norm after: 7.204869200192801
Epoch 3722/10000, Prediction Accuracy = 61.548%, Loss = 0.4627464234828949
Epoch: 3722, Batch Gradient Norm: 6.977367934872135
Epoch: 3722, Batch Gradient Norm after: 6.977367934872135
Epoch 3723/10000, Prediction Accuracy = 61.624%, Loss = 0.46142449975013733
Epoch: 3723, Batch Gradient Norm: 6.976321541513118
Epoch: 3723, Batch Gradient Norm after: 6.976321541513118
Epoch 3724/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.46149930357933044
Epoch: 3724, Batch Gradient Norm: 9.22636687249373
Epoch: 3724, Batch Gradient Norm after: 9.22636687249373
Epoch 3725/10000, Prediction Accuracy = 61.584%, Loss = 0.4733723521232605
Epoch: 3725, Batch Gradient Norm: 13.158137693102951
Epoch: 3725, Batch Gradient Norm after: 13.158137693102951
Epoch 3726/10000, Prediction Accuracy = 61.572%, Loss = 0.5026557385921478
Epoch: 3726, Batch Gradient Norm: 10.71780022350842
Epoch: 3726, Batch Gradient Norm after: 10.71780022350842
Epoch 3727/10000, Prediction Accuracy = 61.516%, Loss = 0.4833489000797272
Epoch: 3727, Batch Gradient Norm: 7.963470894231075
Epoch: 3727, Batch Gradient Norm after: 7.963470894231075
Epoch 3728/10000, Prediction Accuracy = 61.652%, Loss = 0.4650283992290497
Epoch: 3728, Batch Gradient Norm: 7.731005868919555
Epoch: 3728, Batch Gradient Norm after: 7.731005868919555
Epoch 3729/10000, Prediction Accuracy = 61.574%, Loss = 0.46293082237243655
Epoch: 3729, Batch Gradient Norm: 11.227299703856996
Epoch: 3729, Batch Gradient Norm after: 11.227299703856996
Epoch 3730/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.4839220643043518
Epoch: 3730, Batch Gradient Norm: 13.1936331442324
Epoch: 3730, Batch Gradient Norm after: 13.1936331442324
Epoch 3731/10000, Prediction Accuracy = 61.470000000000006%, Loss = 0.5003227055072784
Epoch: 3731, Batch Gradient Norm: 9.349945826015661
Epoch: 3731, Batch Gradient Norm after: 9.349945826015661
Epoch 3732/10000, Prediction Accuracy = 61.56999999999999%, Loss = 0.4729817986488342
Epoch: 3732, Batch Gradient Norm: 9.09689190885207
Epoch: 3732, Batch Gradient Norm after: 9.09689190885207
Epoch 3733/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.4713867366313934
Epoch: 3733, Batch Gradient Norm: 9.887701323861755
Epoch: 3733, Batch Gradient Norm after: 9.887701323861755
Epoch 3734/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.47622509598731994
Epoch: 3734, Batch Gradient Norm: 10.459677835737889
Epoch: 3734, Batch Gradient Norm after: 10.459677835737889
Epoch 3735/10000, Prediction Accuracy = 61.538%, Loss = 0.47982690334320066
Epoch: 3735, Batch Gradient Norm: 10.215173512863851
Epoch: 3735, Batch Gradient Norm after: 10.215173512863851
Epoch 3736/10000, Prediction Accuracy = 61.564%, Loss = 0.47872164845466614
Epoch: 3736, Batch Gradient Norm: 8.726241490946341
Epoch: 3736, Batch Gradient Norm after: 8.726241490946341
Epoch 3737/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.4705592095851898
Epoch: 3737, Batch Gradient Norm: 10.376072557589385
Epoch: 3737, Batch Gradient Norm after: 10.376072557589385
Epoch 3738/10000, Prediction Accuracy = 61.482000000000006%, Loss = 0.4786027729511261
Epoch: 3738, Batch Gradient Norm: 14.019404450368889
Epoch: 3738, Batch Gradient Norm after: 14.019404450368889
Epoch 3739/10000, Prediction Accuracy = 61.534000000000006%, Loss = 0.5058457314968109
Epoch: 3739, Batch Gradient Norm: 11.198240119588311
Epoch: 3739, Batch Gradient Norm after: 11.198240119588311
Epoch 3740/10000, Prediction Accuracy = 61.536%, Loss = 0.48422048687934877
Epoch: 3740, Batch Gradient Norm: 8.105551400682879
Epoch: 3740, Batch Gradient Norm after: 8.105551400682879
Epoch 3741/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4657646894454956
Epoch: 3741, Batch Gradient Norm: 9.95653799096335
Epoch: 3741, Batch Gradient Norm after: 9.95653799096335
Epoch 3742/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.4764963388442993
Epoch: 3742, Batch Gradient Norm: 12.96413173105955
Epoch: 3742, Batch Gradient Norm after: 12.96413173105955
Epoch 3743/10000, Prediction Accuracy = 61.55800000000001%, Loss = 0.498140424489975
Epoch: 3743, Batch Gradient Norm: 12.012764523716017
Epoch: 3743, Batch Gradient Norm after: 12.012764523716017
Epoch 3744/10000, Prediction Accuracy = 61.522000000000006%, Loss = 0.4905939638614655
Epoch: 3744, Batch Gradient Norm: 8.11739745277238
Epoch: 3744, Batch Gradient Norm after: 8.11739745277238
Epoch 3745/10000, Prediction Accuracy = 61.544000000000004%, Loss = 0.4653769075870514
Epoch: 3745, Batch Gradient Norm: 8.37175955574504
Epoch: 3745, Batch Gradient Norm after: 8.37175955574504
Epoch 3746/10000, Prediction Accuracy = 61.564%, Loss = 0.46726232171058657
Epoch: 3746, Batch Gradient Norm: 7.099079073021019
Epoch: 3746, Batch Gradient Norm after: 7.099079073021019
Epoch 3747/10000, Prediction Accuracy = 61.586%, Loss = 0.46131351590156555
Epoch: 3747, Batch Gradient Norm: 8.363261509030828
Epoch: 3747, Batch Gradient Norm after: 8.363261509030828
Epoch 3748/10000, Prediction Accuracy = 61.532000000000004%, Loss = 0.46696342825889586
Epoch: 3748, Batch Gradient Norm: 9.877006983288346
Epoch: 3748, Batch Gradient Norm after: 9.877006983288346
Epoch 3749/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.47464174032211304
Epoch: 3749, Batch Gradient Norm: 14.247540604364392
Epoch: 3749, Batch Gradient Norm after: 14.247540604364392
Epoch 3750/10000, Prediction Accuracy = 61.486000000000004%, Loss = 0.5082495212554932
Epoch: 3750, Batch Gradient Norm: 13.00546190919065
Epoch: 3750, Batch Gradient Norm after: 13.00546190919065
Epoch 3751/10000, Prediction Accuracy = 61.65%, Loss = 0.5003202259540558
Epoch: 3751, Batch Gradient Norm: 8.070613277291232
Epoch: 3751, Batch Gradient Norm after: 8.070613277291232
Epoch 3752/10000, Prediction Accuracy = 61.56%, Loss = 0.4665585160255432
Epoch: 3752, Batch Gradient Norm: 7.493431044212654
Epoch: 3752, Batch Gradient Norm after: 7.493431044212654
Epoch 3753/10000, Prediction Accuracy = 61.676%, Loss = 0.4621982932090759
Epoch: 3753, Batch Gradient Norm: 11.535708801830783
Epoch: 3753, Batch Gradient Norm after: 11.535708801830783
Epoch 3754/10000, Prediction Accuracy = 61.58200000000001%, Loss = 0.48639941215515137
Epoch: 3754, Batch Gradient Norm: 11.516092320017425
Epoch: 3754, Batch Gradient Norm after: 11.516092320017425
Epoch 3755/10000, Prediction Accuracy = 61.56%, Loss = 0.48576171398162843
Epoch: 3755, Batch Gradient Norm: 9.453940400776768
Epoch: 3755, Batch Gradient Norm after: 9.453940400776768
Epoch 3756/10000, Prediction Accuracy = 61.58399999999999%, Loss = 0.4713714063167572
Epoch: 3756, Batch Gradient Norm: 11.068264746527626
Epoch: 3756, Batch Gradient Norm after: 11.068264746527626
Epoch 3757/10000, Prediction Accuracy = 61.60600000000001%, Loss = 0.4828791081905365
Epoch: 3757, Batch Gradient Norm: 11.17168124020042
Epoch: 3757, Batch Gradient Norm after: 11.17168124020042
Epoch 3758/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.48543991446495055
Epoch: 3758, Batch Gradient Norm: 9.299237148425759
Epoch: 3758, Batch Gradient Norm after: 9.299237148425759
Epoch 3759/10000, Prediction Accuracy = 61.664%, Loss = 0.4728385806083679
Epoch: 3759, Batch Gradient Norm: 9.953006417293462
Epoch: 3759, Batch Gradient Norm after: 9.953006417293462
Epoch 3760/10000, Prediction Accuracy = 61.604%, Loss = 0.47522589564323425
Epoch: 3760, Batch Gradient Norm: 11.369083114730577
Epoch: 3760, Batch Gradient Norm after: 11.369083114730577
Epoch 3761/10000, Prediction Accuracy = 61.577999999999996%, Loss = 0.4850301444530487
Epoch: 3761, Batch Gradient Norm: 10.580696638145326
Epoch: 3761, Batch Gradient Norm after: 10.580696638145326
Epoch 3762/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.47963190674781797
Epoch: 3762, Batch Gradient Norm: 9.64544841071734
Epoch: 3762, Batch Gradient Norm after: 9.64544841071734
Epoch 3763/10000, Prediction Accuracy = 61.593999999999994%, Loss = 0.4747127711772919
Epoch: 3763, Batch Gradient Norm: 10.406241269542502
Epoch: 3763, Batch Gradient Norm after: 10.406241269542502
Epoch 3764/10000, Prediction Accuracy = 61.652%, Loss = 0.4796594142913818
Epoch: 3764, Batch Gradient Norm: 10.611731519770844
Epoch: 3764, Batch Gradient Norm after: 10.611731519770844
Epoch 3765/10000, Prediction Accuracy = 61.544%, Loss = 0.4797947347164154
Epoch: 3765, Batch Gradient Norm: 9.661644416741789
Epoch: 3765, Batch Gradient Norm after: 9.661644416741789
Epoch 3766/10000, Prediction Accuracy = 61.634%, Loss = 0.47281656265258787
Epoch: 3766, Batch Gradient Norm: 10.16579290946724
Epoch: 3766, Batch Gradient Norm after: 10.16579290946724
Epoch 3767/10000, Prediction Accuracy = 61.524%, Loss = 0.476282799243927
Epoch: 3767, Batch Gradient Norm: 10.451395891146415
Epoch: 3767, Batch Gradient Norm after: 10.451395891146415
Epoch 3768/10000, Prediction Accuracy = 61.576%, Loss = 0.4786419928073883
Epoch: 3768, Batch Gradient Norm: 12.17880775384865
Epoch: 3768, Batch Gradient Norm after: 12.17880775384865
Epoch 3769/10000, Prediction Accuracy = 61.512%, Loss = 0.49140015840530393
Epoch: 3769, Batch Gradient Norm: 10.651953047074446
Epoch: 3769, Batch Gradient Norm after: 10.651953047074446
Epoch 3770/10000, Prediction Accuracy = 61.614%, Loss = 0.48030529618263246
Epoch: 3770, Batch Gradient Norm: 8.436005469189388
Epoch: 3770, Batch Gradient Norm after: 8.436005469189388
Epoch 3771/10000, Prediction Accuracy = 61.548%, Loss = 0.466493821144104
Epoch: 3771, Batch Gradient Norm: 9.386662971103519
Epoch: 3771, Batch Gradient Norm after: 9.386662971103519
Epoch 3772/10000, Prediction Accuracy = 61.586%, Loss = 0.47234420776367186
Epoch: 3772, Batch Gradient Norm: 9.316266541819306
Epoch: 3772, Batch Gradient Norm after: 9.316266541819306
Epoch 3773/10000, Prediction Accuracy = 61.592%, Loss = 0.4734114229679108
Epoch: 3773, Batch Gradient Norm: 9.62497730448066
Epoch: 3773, Batch Gradient Norm after: 9.62497730448066
Epoch 3774/10000, Prediction Accuracy = 61.55%, Loss = 0.4745944499969482
Epoch: 3774, Batch Gradient Norm: 12.240643365339665
Epoch: 3774, Batch Gradient Norm after: 12.240643365339665
Epoch 3775/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.4923243820667267
Epoch: 3775, Batch Gradient Norm: 11.9832501530365
Epoch: 3775, Batch Gradient Norm after: 11.9832501530365
Epoch 3776/10000, Prediction Accuracy = 61.586%, Loss = 0.49037136435508727
Epoch: 3776, Batch Gradient Norm: 10.46007749145554
Epoch: 3776, Batch Gradient Norm after: 10.46007749145554
Epoch 3777/10000, Prediction Accuracy = 61.65%, Loss = 0.4783829629421234
Epoch: 3777, Batch Gradient Norm: 9.040714934656792
Epoch: 3777, Batch Gradient Norm after: 9.040714934656792
Epoch 3778/10000, Prediction Accuracy = 61.57800000000001%, Loss = 0.4689840853214264
Epoch: 3778, Batch Gradient Norm: 9.16934336703476
Epoch: 3778, Batch Gradient Norm after: 9.16934336703476
Epoch 3779/10000, Prediction Accuracy = 61.7%, Loss = 0.4702978074550629
Epoch: 3779, Batch Gradient Norm: 8.098631713616454
Epoch: 3779, Batch Gradient Norm after: 8.098631713616454
Epoch 3780/10000, Prediction Accuracy = 61.548%, Loss = 0.4643558979034424
Epoch: 3780, Batch Gradient Norm: 8.649556753150684
Epoch: 3780, Batch Gradient Norm after: 8.649556753150684
Epoch 3781/10000, Prediction Accuracy = 61.712%, Loss = 0.46726723313331603
Epoch: 3781, Batch Gradient Norm: 12.306279530815837
Epoch: 3781, Batch Gradient Norm after: 12.306279530815837
Epoch 3782/10000, Prediction Accuracy = 61.538%, Loss = 0.49131239056587217
Epoch: 3782, Batch Gradient Norm: 13.289362207100881
Epoch: 3782, Batch Gradient Norm after: 13.289362207100881
Epoch 3783/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.4985420823097229
Epoch: 3783, Batch Gradient Norm: 11.277571035255068
Epoch: 3783, Batch Gradient Norm after: 11.277571035255068
Epoch 3784/10000, Prediction Accuracy = 61.562%, Loss = 0.4831069827079773
Epoch: 3784, Batch Gradient Norm: 8.856001295767955
Epoch: 3784, Batch Gradient Norm after: 8.856001295767955
Epoch 3785/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4681865513324738
Epoch: 3785, Batch Gradient Norm: 7.918173778316251
Epoch: 3785, Batch Gradient Norm after: 7.918173778316251
Epoch 3786/10000, Prediction Accuracy = 61.622%, Loss = 0.4635183274745941
Epoch: 3786, Batch Gradient Norm: 8.789946897292982
Epoch: 3786, Batch Gradient Norm after: 8.789946897292982
Epoch 3787/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.4683964490890503
Epoch: 3787, Batch Gradient Norm: 9.014921188214135
Epoch: 3787, Batch Gradient Norm after: 9.014921188214135
Epoch 3788/10000, Prediction Accuracy = 61.556%, Loss = 0.4695561289787292
Epoch: 3788, Batch Gradient Norm: 10.639638270755261
Epoch: 3788, Batch Gradient Norm after: 10.639638270755261
Epoch 3789/10000, Prediction Accuracy = 61.608000000000004%, Loss = 0.48041362762451173
Epoch: 3789, Batch Gradient Norm: 10.246696311688579
Epoch: 3789, Batch Gradient Norm after: 10.246696311688579
Epoch 3790/10000, Prediction Accuracy = 61.589999999999996%, Loss = 0.47611142992973327
Epoch: 3790, Batch Gradient Norm: 11.599542976794975
Epoch: 3790, Batch Gradient Norm after: 11.599542976794975
Epoch 3791/10000, Prediction Accuracy = 61.602%, Loss = 0.48425660133361814
Epoch: 3791, Batch Gradient Norm: 13.088930028116556
Epoch: 3791, Batch Gradient Norm after: 13.088930028116556
Epoch 3792/10000, Prediction Accuracy = 61.552%, Loss = 0.4977295458316803
Epoch: 3792, Batch Gradient Norm: 10.115915448791817
Epoch: 3792, Batch Gradient Norm after: 10.115915448791817
Epoch 3793/10000, Prediction Accuracy = 61.61%, Loss = 0.4758028745651245
Epoch: 3793, Batch Gradient Norm: 9.58152128096822
Epoch: 3793, Batch Gradient Norm after: 9.58152128096822
Epoch 3794/10000, Prediction Accuracy = 61.584%, Loss = 0.47239052653312685
Epoch: 3794, Batch Gradient Norm: 10.858103976924792
Epoch: 3794, Batch Gradient Norm after: 10.858103976924792
Epoch 3795/10000, Prediction Accuracy = 61.626%, Loss = 0.48075859546661376
Epoch: 3795, Batch Gradient Norm: 10.883839241173991
Epoch: 3795, Batch Gradient Norm after: 10.883839241173991
Epoch 3796/10000, Prediction Accuracy = 61.634%, Loss = 0.48237237334251404
Epoch: 3796, Batch Gradient Norm: 9.86870804803573
Epoch: 3796, Batch Gradient Norm after: 9.86870804803573
Epoch 3797/10000, Prediction Accuracy = 61.568000000000005%, Loss = 0.47770833373069765
Epoch: 3797, Batch Gradient Norm: 8.663695943672085
Epoch: 3797, Batch Gradient Norm after: 8.663695943672085
Epoch 3798/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46819633841514585
Epoch: 3798, Batch Gradient Norm: 11.271191031342562
Epoch: 3798, Batch Gradient Norm after: 11.271191031342562
Epoch 3799/10000, Prediction Accuracy = 61.565999999999995%, Loss = 0.48264901638031005
Epoch: 3799, Batch Gradient Norm: 12.366102751169164
Epoch: 3799, Batch Gradient Norm after: 12.366102751169164
Epoch 3800/10000, Prediction Accuracy = 61.58399999999999%, Loss = 0.49108492136001586
Epoch: 3800, Batch Gradient Norm: 8.56075701016536
Epoch: 3800, Batch Gradient Norm after: 8.56075701016536
Epoch 3801/10000, Prediction Accuracy = 61.576%, Loss = 0.46584831476211547
Epoch: 3801, Batch Gradient Norm: 7.053717394537359
Epoch: 3801, Batch Gradient Norm after: 7.053717394537359
Epoch 3802/10000, Prediction Accuracy = 61.638%, Loss = 0.45796531438827515
Epoch: 3802, Batch Gradient Norm: 9.25090146663399
Epoch: 3802, Batch Gradient Norm after: 9.25090146663399
Epoch 3803/10000, Prediction Accuracy = 61.572%, Loss = 0.4697971999645233
Epoch: 3803, Batch Gradient Norm: 10.654715095017675
Epoch: 3803, Batch Gradient Norm after: 10.654715095017675
Epoch 3804/10000, Prediction Accuracy = 61.602%, Loss = 0.47782102823257444
Epoch: 3804, Batch Gradient Norm: 12.163716252142088
Epoch: 3804, Batch Gradient Norm after: 12.163716252142088
Epoch 3805/10000, Prediction Accuracy = 61.59400000000001%, Loss = 0.4885232925415039
Epoch: 3805, Batch Gradient Norm: 11.294420321002734
Epoch: 3805, Batch Gradient Norm after: 11.294420321002734
Epoch 3806/10000, Prediction Accuracy = 61.574%, Loss = 0.48152273893356323
Epoch: 3806, Batch Gradient Norm: 12.599900330460109
Epoch: 3806, Batch Gradient Norm after: 12.599900330460109
Epoch 3807/10000, Prediction Accuracy = 61.662%, Loss = 0.4919403553009033
Epoch: 3807, Batch Gradient Norm: 12.46869326355593
Epoch: 3807, Batch Gradient Norm after: 12.46869326355593
Epoch 3808/10000, Prediction Accuracy = 61.448%, Loss = 0.49244253635406493
Epoch: 3808, Batch Gradient Norm: 9.664880823544932
Epoch: 3808, Batch Gradient Norm after: 9.664880823544932
Epoch 3809/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.4727401316165924
Epoch: 3809, Batch Gradient Norm: 8.628993856258612
Epoch: 3809, Batch Gradient Norm after: 8.628993856258612
Epoch 3810/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.4666300296783447
Epoch: 3810, Batch Gradient Norm: 9.387631826493893
Epoch: 3810, Batch Gradient Norm after: 9.387631826493893
Epoch 3811/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4723945319652557
Epoch: 3811, Batch Gradient Norm: 8.584543020565679
Epoch: 3811, Batch Gradient Norm after: 8.584543020565679
Epoch 3812/10000, Prediction Accuracy = 61.58%, Loss = 0.46775474548339846
Epoch: 3812, Batch Gradient Norm: 9.700573077203371
Epoch: 3812, Batch Gradient Norm after: 9.700573077203371
Epoch 3813/10000, Prediction Accuracy = 61.593999999999994%, Loss = 0.4745339691638947
Epoch: 3813, Batch Gradient Norm: 10.801795003305243
Epoch: 3813, Batch Gradient Norm after: 10.801795003305243
Epoch 3814/10000, Prediction Accuracy = 61.626%, Loss = 0.48218184113502505
Epoch: 3814, Batch Gradient Norm: 10.14858586945276
Epoch: 3814, Batch Gradient Norm after: 10.14858586945276
Epoch 3815/10000, Prediction Accuracy = 61.614%, Loss = 0.47754557728767394
Epoch: 3815, Batch Gradient Norm: 8.302180747437905
Epoch: 3815, Batch Gradient Norm after: 8.302180747437905
Epoch 3816/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46558732986450196
Epoch: 3816, Batch Gradient Norm: 6.8576128088108685
Epoch: 3816, Batch Gradient Norm after: 6.8576128088108685
Epoch 3817/10000, Prediction Accuracy = 61.612%, Loss = 0.45710188150405884
Epoch: 3817, Batch Gradient Norm: 9.841823657409503
Epoch: 3817, Batch Gradient Norm after: 9.841823657409503
Epoch 3818/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.471372926235199
Epoch: 3818, Batch Gradient Norm: 14.125296304862163
Epoch: 3818, Batch Gradient Norm after: 14.125296304862163
Epoch 3819/10000, Prediction Accuracy = 61.574%, Loss = 0.5015781998634339
Epoch: 3819, Batch Gradient Norm: 13.194963826068385
Epoch: 3819, Batch Gradient Norm after: 13.194963826068385
Epoch 3820/10000, Prediction Accuracy = 61.58%, Loss = 0.49663745760917666
Epoch: 3820, Batch Gradient Norm: 9.461228544980024
Epoch: 3820, Batch Gradient Norm after: 9.461228544980024
Epoch 3821/10000, Prediction Accuracy = 61.57000000000001%, Loss = 0.47131255865097044
Epoch: 3821, Batch Gradient Norm: 9.537656836276314
Epoch: 3821, Batch Gradient Norm after: 9.537656836276314
Epoch 3822/10000, Prediction Accuracy = 61.64%, Loss = 0.47085519433021544
Epoch: 3822, Batch Gradient Norm: 10.722446096676952
Epoch: 3822, Batch Gradient Norm after: 10.722446096676952
Epoch 3823/10000, Prediction Accuracy = 61.572%, Loss = 0.4778908610343933
Epoch: 3823, Batch Gradient Norm: 10.072871512740583
Epoch: 3823, Batch Gradient Norm after: 10.072871512740583
Epoch 3824/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.4733194589614868
Epoch: 3824, Batch Gradient Norm: 8.440424244976686
Epoch: 3824, Batch Gradient Norm after: 8.440424244976686
Epoch 3825/10000, Prediction Accuracy = 61.588%, Loss = 0.46396284103393554
Epoch: 3825, Batch Gradient Norm: 9.859045674515198
Epoch: 3825, Batch Gradient Norm after: 9.859045674515198
Epoch 3826/10000, Prediction Accuracy = 61.67%, Loss = 0.47320294976234434
Epoch: 3826, Batch Gradient Norm: 9.560062899343896
Epoch: 3826, Batch Gradient Norm after: 9.560062899343896
Epoch 3827/10000, Prediction Accuracy = 61.548%, Loss = 0.4739012360572815
Epoch: 3827, Batch Gradient Norm: 9.141555390924758
Epoch: 3827, Batch Gradient Norm after: 9.141555390924758
Epoch 3828/10000, Prediction Accuracy = 61.682%, Loss = 0.47020267844200136
Epoch: 3828, Batch Gradient Norm: 12.060927635406516
Epoch: 3828, Batch Gradient Norm after: 12.060927635406516
Epoch 3829/10000, Prediction Accuracy = 61.622%, Loss = 0.4874399363994598
Epoch: 3829, Batch Gradient Norm: 14.63843710297594
Epoch: 3829, Batch Gradient Norm after: 14.63843710297594
Epoch 3830/10000, Prediction Accuracy = 61.593999999999994%, Loss = 0.5111344337463379
Epoch: 3830, Batch Gradient Norm: 10.377169245330396
Epoch: 3830, Batch Gradient Norm after: 10.377169245330396
Epoch 3831/10000, Prediction Accuracy = 61.61%, Loss = 0.47773817777633665
Epoch: 3831, Batch Gradient Norm: 7.905820663574056
Epoch: 3831, Batch Gradient Norm after: 7.905820663574056
Epoch 3832/10000, Prediction Accuracy = 61.653999999999996%, Loss = 0.46236419677734375
Epoch: 3832, Batch Gradient Norm: 8.174011546497267
Epoch: 3832, Batch Gradient Norm after: 8.174011546497267
Epoch 3833/10000, Prediction Accuracy = 61.628%, Loss = 0.4630324304103851
Epoch: 3833, Batch Gradient Norm: 10.724087116067802
Epoch: 3833, Batch Gradient Norm after: 10.724087116067802
Epoch 3834/10000, Prediction Accuracy = 61.60799999999999%, Loss = 0.4793142855167389
Epoch: 3834, Batch Gradient Norm: 10.405933180608343
Epoch: 3834, Batch Gradient Norm after: 10.405933180608343
Epoch 3835/10000, Prediction Accuracy = 61.666%, Loss = 0.47614248394966124
Epoch: 3835, Batch Gradient Norm: 11.466548129895713
Epoch: 3835, Batch Gradient Norm after: 11.466548129895713
Epoch 3836/10000, Prediction Accuracy = 61.596000000000004%, Loss = 0.4836790025234222
Epoch: 3836, Batch Gradient Norm: 10.674469199535343
Epoch: 3836, Batch Gradient Norm after: 10.674469199535343
Epoch 3837/10000, Prediction Accuracy = 61.634%, Loss = 0.47831777334213255
Epoch: 3837, Batch Gradient Norm: 10.773349544946441
Epoch: 3837, Batch Gradient Norm after: 10.773349544946441
Epoch 3838/10000, Prediction Accuracy = 61.538%, Loss = 0.4789712905883789
Epoch: 3838, Batch Gradient Norm: 10.24887180321172
Epoch: 3838, Batch Gradient Norm after: 10.24887180321172
Epoch 3839/10000, Prediction Accuracy = 61.652%, Loss = 0.4749884128570557
Epoch: 3839, Batch Gradient Norm: 9.602876805103595
Epoch: 3839, Batch Gradient Norm after: 9.602876805103595
Epoch 3840/10000, Prediction Accuracy = 61.562%, Loss = 0.4717443108558655
Epoch: 3840, Batch Gradient Norm: 8.404482216502522
Epoch: 3840, Batch Gradient Norm after: 8.404482216502522
Epoch 3841/10000, Prediction Accuracy = 61.688%, Loss = 0.46528969407081605
Epoch: 3841, Batch Gradient Norm: 8.603963672234343
Epoch: 3841, Batch Gradient Norm after: 8.603963672234343
Epoch 3842/10000, Prediction Accuracy = 61.61%, Loss = 0.4658446431159973
Epoch: 3842, Batch Gradient Norm: 11.42236707662337
Epoch: 3842, Batch Gradient Norm after: 11.42236707662337
Epoch 3843/10000, Prediction Accuracy = 61.54200000000001%, Loss = 0.4830212712287903
Epoch: 3843, Batch Gradient Norm: 11.611697994490694
Epoch: 3843, Batch Gradient Norm after: 11.611697994490694
Epoch 3844/10000, Prediction Accuracy = 61.63800000000001%, Loss = 0.48412185311317446
Epoch: 3844, Batch Gradient Norm: 11.034373198383834
Epoch: 3844, Batch Gradient Norm after: 11.034373198383834
Epoch 3845/10000, Prediction Accuracy = 61.61600000000001%, Loss = 0.47861579060554504
Epoch: 3845, Batch Gradient Norm: 12.044353084267373
Epoch: 3845, Batch Gradient Norm after: 12.044353084267373
Epoch 3846/10000, Prediction Accuracy = 61.67600000000001%, Loss = 0.4851667046546936
Epoch: 3846, Batch Gradient Norm: 11.085867065292382
Epoch: 3846, Batch Gradient Norm after: 11.085867065292382
Epoch 3847/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4793354690074921
Epoch: 3847, Batch Gradient Norm: 10.729253517257154
Epoch: 3847, Batch Gradient Norm after: 10.729253517257154
Epoch 3848/10000, Prediction Accuracy = 61.634%, Loss = 0.47908705472946167
Epoch: 3848, Batch Gradient Norm: 9.67951388108073
Epoch: 3848, Batch Gradient Norm after: 9.67951388108073
Epoch 3849/10000, Prediction Accuracy = 61.55799999999999%, Loss = 0.4744101583957672
Epoch: 3849, Batch Gradient Norm: 7.406879663590958
Epoch: 3849, Batch Gradient Norm after: 7.406879663590958
Epoch 3850/10000, Prediction Accuracy = 61.762%, Loss = 0.4596650004386902
Epoch: 3850, Batch Gradient Norm: 7.991501135469053
Epoch: 3850, Batch Gradient Norm after: 7.991501135469053
Epoch 3851/10000, Prediction Accuracy = 61.626%, Loss = 0.4615631401538849
Epoch: 3851, Batch Gradient Norm: 8.740196571829259
Epoch: 3851, Batch Gradient Norm after: 8.740196571829259
Epoch 3852/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.46520538330078126
Epoch: 3852, Batch Gradient Norm: 9.781183916337314
Epoch: 3852, Batch Gradient Norm after: 9.781183916337314
Epoch 3853/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.47179356813430784
Epoch: 3853, Batch Gradient Norm: 9.43448138288977
Epoch: 3853, Batch Gradient Norm after: 9.43448138288977
Epoch 3854/10000, Prediction Accuracy = 61.626%, Loss = 0.47090967297554015
Epoch: 3854, Batch Gradient Norm: 10.422700744727667
Epoch: 3854, Batch Gradient Norm after: 10.422700744727667
Epoch 3855/10000, Prediction Accuracy = 61.653999999999996%, Loss = 0.4762532711029053
Epoch: 3855, Batch Gradient Norm: 12.80931665282924
Epoch: 3855, Batch Gradient Norm after: 12.80931665282924
Epoch 3856/10000, Prediction Accuracy = 61.605999999999995%, Loss = 0.49310592412948606
Epoch: 3856, Batch Gradient Norm: 10.909421039902767
Epoch: 3856, Batch Gradient Norm after: 10.909421039902767
Epoch 3857/10000, Prediction Accuracy = 61.64%, Loss = 0.4788049101829529
Epoch: 3857, Batch Gradient Norm: 9.535291495563666
Epoch: 3857, Batch Gradient Norm after: 9.535291495563666
Epoch 3858/10000, Prediction Accuracy = 61.6%, Loss = 0.4705220937728882
Epoch: 3858, Batch Gradient Norm: 9.047636320891645
Epoch: 3858, Batch Gradient Norm after: 9.047636320891645
Epoch 3859/10000, Prediction Accuracy = 61.593999999999994%, Loss = 0.469253146648407
Epoch: 3859, Batch Gradient Norm: 9.987167529117103
Epoch: 3859, Batch Gradient Norm after: 9.987167529117103
Epoch 3860/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.47639251351356504
Epoch: 3860, Batch Gradient Norm: 10.174704277796017
Epoch: 3860, Batch Gradient Norm after: 10.174704277796017
Epoch 3861/10000, Prediction Accuracy = 61.584%, Loss = 0.475079333782196
Epoch: 3861, Batch Gradient Norm: 12.337730322555503
Epoch: 3861, Batch Gradient Norm after: 12.337730322555503
Epoch 3862/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.48930535912513734
Epoch: 3862, Batch Gradient Norm: 11.666391488594725
Epoch: 3862, Batch Gradient Norm after: 11.666391488594725
Epoch 3863/10000, Prediction Accuracy = 61.65%, Loss = 0.482930052280426
Epoch: 3863, Batch Gradient Norm: 9.74187988595298
Epoch: 3863, Batch Gradient Norm after: 9.74187988595298
Epoch 3864/10000, Prediction Accuracy = 61.626%, Loss = 0.4697424352169037
Epoch: 3864, Batch Gradient Norm: 12.41593878601407
Epoch: 3864, Batch Gradient Norm after: 12.41593878601407
Epoch 3865/10000, Prediction Accuracy = 61.56%, Loss = 0.4896867752075195
Epoch: 3865, Batch Gradient Norm: 11.181348220977933
Epoch: 3865, Batch Gradient Norm after: 11.181348220977933
Epoch 3866/10000, Prediction Accuracy = 61.68800000000001%, Loss = 0.4830795884132385
Epoch: 3866, Batch Gradient Norm: 8.442017889677663
Epoch: 3866, Batch Gradient Norm after: 8.442017889677663
Epoch 3867/10000, Prediction Accuracy = 61.56%, Loss = 0.4629998981952667
Epoch: 3867, Batch Gradient Norm: 10.51340251664305
Epoch: 3867, Batch Gradient Norm after: 10.51340251664305
Epoch 3868/10000, Prediction Accuracy = 61.722%, Loss = 0.4752682983875275
Epoch: 3868, Batch Gradient Norm: 10.70223525044079
Epoch: 3868, Batch Gradient Norm after: 10.70223525044079
Epoch 3869/10000, Prediction Accuracy = 61.61600000000001%, Loss = 0.47844564318656924
Epoch: 3869, Batch Gradient Norm: 9.276555732047669
Epoch: 3869, Batch Gradient Norm after: 9.276555732047669
Epoch 3870/10000, Prediction Accuracy = 61.622%, Loss = 0.4694179892539978
Epoch: 3870, Batch Gradient Norm: 7.2002557115615105
Epoch: 3870, Batch Gradient Norm after: 7.2002557115615105
Epoch 3871/10000, Prediction Accuracy = 61.67%, Loss = 0.4575780689716339
Epoch: 3871, Batch Gradient Norm: 8.95819878140899
Epoch: 3871, Batch Gradient Norm after: 8.95819878140899
Epoch 3872/10000, Prediction Accuracy = 61.581999999999994%, Loss = 0.4664875030517578
Epoch: 3872, Batch Gradient Norm: 11.372170628072082
Epoch: 3872, Batch Gradient Norm after: 11.372170628072082
Epoch 3873/10000, Prediction Accuracy = 61.59000000000001%, Loss = 0.4839073598384857
Epoch: 3873, Batch Gradient Norm: 12.485507170868111
Epoch: 3873, Batch Gradient Norm after: 12.485507170868111
Epoch 3874/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.48882734775543213
Epoch: 3874, Batch Gradient Norm: 13.041194562084975
Epoch: 3874, Batch Gradient Norm after: 13.041194562084975
Epoch 3875/10000, Prediction Accuracy = 61.592%, Loss = 0.4932017982006073
Epoch: 3875, Batch Gradient Norm: 10.258748049805842
Epoch: 3875, Batch Gradient Norm after: 10.258748049805842
Epoch 3876/10000, Prediction Accuracy = 61.65%, Loss = 0.47365192174911497
Epoch: 3876, Batch Gradient Norm: 8.57589167896878
Epoch: 3876, Batch Gradient Norm after: 8.57589167896878
Epoch 3877/10000, Prediction Accuracy = 61.629999999999995%, Loss = 0.463286954164505
Epoch: 3877, Batch Gradient Norm: 8.522357066263721
Epoch: 3877, Batch Gradient Norm after: 8.522357066263721
Epoch 3878/10000, Prediction Accuracy = 61.626%, Loss = 0.462846040725708
Epoch: 3878, Batch Gradient Norm: 9.381103670769658
Epoch: 3878, Batch Gradient Norm after: 9.381103670769658
Epoch 3879/10000, Prediction Accuracy = 61.54%, Loss = 0.46761143803596494
Epoch: 3879, Batch Gradient Norm: 9.43135315352666
Epoch: 3879, Batch Gradient Norm after: 9.43135315352666
Epoch 3880/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.46810182929039
Epoch: 3880, Batch Gradient Norm: 10.282129666505506
Epoch: 3880, Batch Gradient Norm after: 10.282129666505506
Epoch 3881/10000, Prediction Accuracy = 61.604000000000006%, Loss = 0.47432993054389955
Epoch: 3881, Batch Gradient Norm: 11.843282128742105
Epoch: 3881, Batch Gradient Norm after: 11.843282128742105
Epoch 3882/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.4855880618095398
Epoch: 3882, Batch Gradient Norm: 10.900544987263972
Epoch: 3882, Batch Gradient Norm after: 10.900544987263972
Epoch 3883/10000, Prediction Accuracy = 61.58%, Loss = 0.4800366163253784
Epoch: 3883, Batch Gradient Norm: 10.22181470777011
Epoch: 3883, Batch Gradient Norm after: 10.22181470777011
Epoch 3884/10000, Prediction Accuracy = 61.588%, Loss = 0.47339914441108705
Epoch: 3884, Batch Gradient Norm: 8.935486852845278
Epoch: 3884, Batch Gradient Norm after: 8.935486852845278
Epoch 3885/10000, Prediction Accuracy = 61.646%, Loss = 0.4665844917297363
Epoch: 3885, Batch Gradient Norm: 8.752667363543456
Epoch: 3885, Batch Gradient Norm after: 8.752667363543456
Epoch 3886/10000, Prediction Accuracy = 61.64%, Loss = 0.4670660436153412
Epoch: 3886, Batch Gradient Norm: 7.127868545902245
Epoch: 3886, Batch Gradient Norm after: 7.127868545902245
Epoch 3887/10000, Prediction Accuracy = 61.694%, Loss = 0.4577349007129669
Epoch: 3887, Batch Gradient Norm: 9.60476945939792
Epoch: 3887, Batch Gradient Norm after: 9.60476945939792
Epoch 3888/10000, Prediction Accuracy = 61.73%, Loss = 0.4709103345870972
Epoch: 3888, Batch Gradient Norm: 13.179292986650305
Epoch: 3888, Batch Gradient Norm after: 13.179292986650305
Epoch 3889/10000, Prediction Accuracy = 61.63000000000001%, Loss = 0.4960214376449585
Epoch: 3889, Batch Gradient Norm: 12.463384408269572
Epoch: 3889, Batch Gradient Norm after: 12.463384408269572
Epoch 3890/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.49088183641433714
Epoch: 3890, Batch Gradient Norm: 9.059585275671177
Epoch: 3890, Batch Gradient Norm after: 9.059585275671177
Epoch 3891/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4670563340187073
Epoch: 3891, Batch Gradient Norm: 8.89288600247291
Epoch: 3891, Batch Gradient Norm after: 8.89288600247291
Epoch 3892/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.46574735045433047
Epoch: 3892, Batch Gradient Norm: 12.628098459496707
Epoch: 3892, Batch Gradient Norm after: 12.628098459496707
Epoch 3893/10000, Prediction Accuracy = 61.55999999999999%, Loss = 0.4935059010982513
Epoch: 3893, Batch Gradient Norm: 10.56123591372501
Epoch: 3893, Batch Gradient Norm after: 10.56123591372501
Epoch 3894/10000, Prediction Accuracy = 61.604000000000006%, Loss = 0.4772421658039093
Epoch: 3894, Batch Gradient Norm: 9.305546042546307
Epoch: 3894, Batch Gradient Norm after: 9.305546042546307
Epoch 3895/10000, Prediction Accuracy = 61.688%, Loss = 0.4677135288715363
Epoch: 3895, Batch Gradient Norm: 10.427451104943172
Epoch: 3895, Batch Gradient Norm after: 10.427451104943172
Epoch 3896/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.47304571866989137
Epoch: 3896, Batch Gradient Norm: 12.839642080030742
Epoch: 3896, Batch Gradient Norm after: 12.839642080030742
Epoch 3897/10000, Prediction Accuracy = 61.584%, Loss = 0.4893537700176239
Epoch: 3897, Batch Gradient Norm: 10.685572344036165
Epoch: 3897, Batch Gradient Norm after: 10.685572344036165
Epoch 3898/10000, Prediction Accuracy = 61.676%, Loss = 0.4742393851280212
Epoch: 3898, Batch Gradient Norm: 8.312020886141447
Epoch: 3898, Batch Gradient Norm after: 8.312020886141447
Epoch 3899/10000, Prediction Accuracy = 61.616%, Loss = 0.46022722125053406
Epoch: 3899, Batch Gradient Norm: 9.505522541852807
Epoch: 3899, Batch Gradient Norm after: 9.505522541852807
Epoch 3900/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.4679121971130371
Epoch: 3900, Batch Gradient Norm: 11.55211523958908
Epoch: 3900, Batch Gradient Norm after: 11.55211523958908
Epoch 3901/10000, Prediction Accuracy = 61.532000000000004%, Loss = 0.48255500197410583
Epoch: 3901, Batch Gradient Norm: 12.143702822935541
Epoch: 3901, Batch Gradient Norm after: 12.143702822935541
Epoch 3902/10000, Prediction Accuracy = 61.75%, Loss = 0.4878749310970306
Epoch: 3902, Batch Gradient Norm: 10.336526667471047
Epoch: 3902, Batch Gradient Norm after: 10.336526667471047
Epoch 3903/10000, Prediction Accuracy = 61.624%, Loss = 0.4737724244594574
Epoch: 3903, Batch Gradient Norm: 10.27011617393722
Epoch: 3903, Batch Gradient Norm after: 10.27011617393722
Epoch 3904/10000, Prediction Accuracy = 61.626%, Loss = 0.47436131834983825
Epoch: 3904, Batch Gradient Norm: 9.171170378986153
Epoch: 3904, Batch Gradient Norm after: 9.171170378986153
Epoch 3905/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.468035489320755
Epoch: 3905, Batch Gradient Norm: 8.436358024946397
Epoch: 3905, Batch Gradient Norm after: 8.436358024946397
Epoch 3906/10000, Prediction Accuracy = 61.69000000000001%, Loss = 0.4621925413608551
Epoch: 3906, Batch Gradient Norm: 10.90330484515261
Epoch: 3906, Batch Gradient Norm after: 10.90330484515261
Epoch 3907/10000, Prediction Accuracy = 61.648%, Loss = 0.47718275189399717
Epoch: 3907, Batch Gradient Norm: 10.955236322510943
Epoch: 3907, Batch Gradient Norm after: 10.955236322510943
Epoch 3908/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.4792549252510071
Epoch: 3908, Batch Gradient Norm: 8.947807258683554
Epoch: 3908, Batch Gradient Norm after: 8.947807258683554
Epoch 3909/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.46651792526245117
Epoch: 3909, Batch Gradient Norm: 7.073541547672623
Epoch: 3909, Batch Gradient Norm after: 7.073541547672623
Epoch 3910/10000, Prediction Accuracy = 61.676%, Loss = 0.45641043186187746
Epoch: 3910, Batch Gradient Norm: 8.20808877598651
Epoch: 3910, Batch Gradient Norm after: 8.20808877598651
Epoch 3911/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.4605471849441528
Epoch: 3911, Batch Gradient Norm: 11.571860470006188
Epoch: 3911, Batch Gradient Norm after: 11.571860470006188
Epoch 3912/10000, Prediction Accuracy = 61.65%, Loss = 0.4816490948200226
Epoch: 3912, Batch Gradient Norm: 12.058023735476015
Epoch: 3912, Batch Gradient Norm after: 12.058023735476015
Epoch 3913/10000, Prediction Accuracy = 61.564%, Loss = 0.4856039881706238
Epoch: 3913, Batch Gradient Norm: 10.205660941619081
Epoch: 3913, Batch Gradient Norm after: 10.205660941619081
Epoch 3914/10000, Prediction Accuracy = 61.71%, Loss = 0.4715310335159302
Epoch: 3914, Batch Gradient Norm: 11.285641763566568
Epoch: 3914, Batch Gradient Norm after: 11.285641763566568
Epoch 3915/10000, Prediction Accuracy = 61.61999999999999%, Loss = 0.47859416604042054
Epoch: 3915, Batch Gradient Norm: 10.706654661508926
Epoch: 3915, Batch Gradient Norm after: 10.706654661508926
Epoch 3916/10000, Prediction Accuracy = 61.774%, Loss = 0.4755084693431854
Epoch: 3916, Batch Gradient Norm: 10.70082919854567
Epoch: 3916, Batch Gradient Norm after: 10.70082919854567
Epoch 3917/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.47711660265922545
Epoch: 3917, Batch Gradient Norm: 9.925122477311664
Epoch: 3917, Batch Gradient Norm after: 9.925122477311664
Epoch 3918/10000, Prediction Accuracy = 61.626%, Loss = 0.4725533723831177
Epoch: 3918, Batch Gradient Norm: 8.148117049549453
Epoch: 3918, Batch Gradient Norm after: 8.148117049549453
Epoch 3919/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.46189401149749754
Epoch: 3919, Batch Gradient Norm: 8.81282375559958
Epoch: 3919, Batch Gradient Norm after: 8.81282375559958
Epoch 3920/10000, Prediction Accuracy = 61.648%, Loss = 0.46605515480041504
Epoch: 3920, Batch Gradient Norm: 8.347364880531748
Epoch: 3920, Batch Gradient Norm after: 8.347364880531748
Epoch 3921/10000, Prediction Accuracy = 61.634%, Loss = 0.46329379081726074
Epoch: 3921, Batch Gradient Norm: 10.476750911873642
Epoch: 3921, Batch Gradient Norm after: 10.476750911873642
Epoch 3922/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.4747447967529297
Epoch: 3922, Batch Gradient Norm: 12.159124989897016
Epoch: 3922, Batch Gradient Norm after: 12.159124989897016
Epoch 3923/10000, Prediction Accuracy = 61.674%, Loss = 0.48362127542495725
Epoch: 3923, Batch Gradient Norm: 11.82831249527116
Epoch: 3923, Batch Gradient Norm after: 11.82831249527116
Epoch 3924/10000, Prediction Accuracy = 61.72600000000001%, Loss = 0.4820214927196503
Epoch: 3924, Batch Gradient Norm: 10.944501944994217
Epoch: 3924, Batch Gradient Norm after: 10.944501944994217
Epoch 3925/10000, Prediction Accuracy = 61.694%, Loss = 0.47759462594985963
Epoch: 3925, Batch Gradient Norm: 10.57539530166272
Epoch: 3925, Batch Gradient Norm after: 10.57539530166272
Epoch 3926/10000, Prediction Accuracy = 61.712%, Loss = 0.47580968737602236
Epoch: 3926, Batch Gradient Norm: 10.436205081917361
Epoch: 3926, Batch Gradient Norm after: 10.436205081917361
Epoch 3927/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.4745363354682922
Epoch: 3927, Batch Gradient Norm: 10.626544800982247
Epoch: 3927, Batch Gradient Norm after: 10.626544800982247
Epoch 3928/10000, Prediction Accuracy = 61.686%, Loss = 0.4739637732505798
Epoch: 3928, Batch Gradient Norm: 12.609481116698664
Epoch: 3928, Batch Gradient Norm after: 12.609481116698664
Epoch 3929/10000, Prediction Accuracy = 61.754%, Loss = 0.4890092432498932
Epoch: 3929, Batch Gradient Norm: 11.376697262649197
Epoch: 3929, Batch Gradient Norm after: 11.376697262649197
Epoch 3930/10000, Prediction Accuracy = 61.628%, Loss = 0.4802325189113617
Epoch: 3930, Batch Gradient Norm: 9.697712922374448
Epoch: 3930, Batch Gradient Norm after: 9.697712922374448
Epoch 3931/10000, Prediction Accuracy = 61.73%, Loss = 0.46815714836120603
Epoch: 3931, Batch Gradient Norm: 8.452809769536122
Epoch: 3931, Batch Gradient Norm after: 8.452809769536122
Epoch 3932/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4604999840259552
Epoch: 3932, Batch Gradient Norm: 9.169903161475217
Epoch: 3932, Batch Gradient Norm after: 9.169903161475217
Epoch 3933/10000, Prediction Accuracy = 61.662%, Loss = 0.46484065651893614
Epoch: 3933, Batch Gradient Norm: 9.475331228856161
Epoch: 3933, Batch Gradient Norm after: 9.475331228856161
Epoch 3934/10000, Prediction Accuracy = 61.67%, Loss = 0.46832320690155027
Epoch: 3934, Batch Gradient Norm: 10.170317904982166
Epoch: 3934, Batch Gradient Norm after: 10.170317904982166
Epoch 3935/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.47325878143310546
Epoch: 3935, Batch Gradient Norm: 11.583543095759591
Epoch: 3935, Batch Gradient Norm after: 11.583543095759591
Epoch 3936/10000, Prediction Accuracy = 61.628%, Loss = 0.4831497371196747
Epoch: 3936, Batch Gradient Norm: 11.62304352331671
Epoch: 3936, Batch Gradient Norm after: 11.62304352331671
Epoch 3937/10000, Prediction Accuracy = 61.682%, Loss = 0.4820354163646698
Epoch: 3937, Batch Gradient Norm: 11.061252079434803
Epoch: 3937, Batch Gradient Norm after: 11.061252079434803
Epoch 3938/10000, Prediction Accuracy = 61.63600000000001%, Loss = 0.47732980251312257
Epoch: 3938, Batch Gradient Norm: 9.895259839023138
Epoch: 3938, Batch Gradient Norm after: 9.895259839023138
Epoch 3939/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.47068514227867125
Epoch: 3939, Batch Gradient Norm: 8.064969652028575
Epoch: 3939, Batch Gradient Norm after: 8.064969652028575
Epoch 3940/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.4600090742111206
Epoch: 3940, Batch Gradient Norm: 8.73104338414434
Epoch: 3940, Batch Gradient Norm after: 8.73104338414434
Epoch 3941/10000, Prediction Accuracy = 61.69199999999999%, Loss = 0.4638999581336975
Epoch: 3941, Batch Gradient Norm: 8.238434459275656
Epoch: 3941, Batch Gradient Norm after: 8.238434459275656
Epoch 3942/10000, Prediction Accuracy = 61.620000000000005%, Loss = 0.4608281970024109
Epoch: 3942, Batch Gradient Norm: 9.079305578161433
Epoch: 3942, Batch Gradient Norm after: 9.079305578161433
Epoch 3943/10000, Prediction Accuracy = 61.64200000000001%, Loss = 0.46496957540512085
Epoch: 3943, Batch Gradient Norm: 10.233797121768186
Epoch: 3943, Batch Gradient Norm after: 10.233797121768186
Epoch 3944/10000, Prediction Accuracy = 61.634%, Loss = 0.4706851959228516
Epoch: 3944, Batch Gradient Norm: 12.961481273738809
Epoch: 3944, Batch Gradient Norm after: 12.961481273738809
Epoch 3945/10000, Prediction Accuracy = 61.708000000000006%, Loss = 0.4908925950527191
Epoch: 3945, Batch Gradient Norm: 11.764306998401649
Epoch: 3945, Batch Gradient Norm after: 11.764306998401649
Epoch 3946/10000, Prediction Accuracy = 61.648%, Loss = 0.4840791285037994
Epoch: 3946, Batch Gradient Norm: 10.879529429032406
Epoch: 3946, Batch Gradient Norm after: 10.879529429032406
Epoch 3947/10000, Prediction Accuracy = 61.658%, Loss = 0.47733150124549867
Epoch: 3947, Batch Gradient Norm: 11.540593811122788
Epoch: 3947, Batch Gradient Norm after: 11.540593811122788
Epoch 3948/10000, Prediction Accuracy = 61.70400000000001%, Loss = 0.48069087862968446
Epoch: 3948, Batch Gradient Norm: 12.237810342009233
Epoch: 3948, Batch Gradient Norm after: 12.237810342009233
Epoch 3949/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.48447408676147463
Epoch: 3949, Batch Gradient Norm: 10.334049436603632
Epoch: 3949, Batch Gradient Norm after: 10.334049436603632
Epoch 3950/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.47027273178100587
Epoch: 3950, Batch Gradient Norm: 9.341013211826677
Epoch: 3950, Batch Gradient Norm after: 9.341013211826677
Epoch 3951/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4649847447872162
Epoch: 3951, Batch Gradient Norm: 9.387213253035231
Epoch: 3951, Batch Gradient Norm after: 9.387213253035231
Epoch 3952/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.4668324589729309
Epoch: 3952, Batch Gradient Norm: 8.665781449748106
Epoch: 3952, Batch Gradient Norm after: 8.665781449748106
Epoch 3953/10000, Prediction Accuracy = 61.652%, Loss = 0.4632704138755798
Epoch: 3953, Batch Gradient Norm: 10.081861665524507
Epoch: 3953, Batch Gradient Norm after: 10.081861665524507
Epoch 3954/10000, Prediction Accuracy = 61.746%, Loss = 0.47157204151153564
Epoch: 3954, Batch Gradient Norm: 9.707399295040647
Epoch: 3954, Batch Gradient Norm after: 9.707399295040647
Epoch 3955/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.46822518706321714
Epoch: 3955, Batch Gradient Norm: 9.440943054720094
Epoch: 3955, Batch Gradient Norm after: 9.440943054720094
Epoch 3956/10000, Prediction Accuracy = 61.726%, Loss = 0.4666794419288635
Epoch: 3956, Batch Gradient Norm: 8.530308570971606
Epoch: 3956, Batch Gradient Norm after: 8.530308570971606
Epoch 3957/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.46136430501937864
Epoch: 3957, Batch Gradient Norm: 9.348309158301314
Epoch: 3957, Batch Gradient Norm after: 9.348309158301314
Epoch 3958/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4676846921443939
Epoch: 3958, Batch Gradient Norm: 9.351858106890036
Epoch: 3958, Batch Gradient Norm after: 9.351858106890036
Epoch 3959/10000, Prediction Accuracy = 61.71999999999999%, Loss = 0.46885998249053956
Epoch: 3959, Batch Gradient Norm: 11.080018961999222
Epoch: 3959, Batch Gradient Norm after: 11.080018961999222
Epoch 3960/10000, Prediction Accuracy = 61.622%, Loss = 0.4796173870563507
Epoch: 3960, Batch Gradient Norm: 11.725363824849106
Epoch: 3960, Batch Gradient Norm after: 11.725363824849106
Epoch 3961/10000, Prediction Accuracy = 61.742%, Loss = 0.48243710994720457
Epoch: 3961, Batch Gradient Norm: 10.27702454888801
Epoch: 3961, Batch Gradient Norm after: 10.27702454888801
Epoch 3962/10000, Prediction Accuracy = 61.67%, Loss = 0.47174239754676817
Epoch: 3962, Batch Gradient Norm: 9.393367680066136
Epoch: 3962, Batch Gradient Norm after: 9.393367680066136
Epoch 3963/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.46695694923400877
Epoch: 3963, Batch Gradient Norm: 9.333238629682395
Epoch: 3963, Batch Gradient Norm after: 9.333238629682395
Epoch 3964/10000, Prediction Accuracy = 61.718%, Loss = 0.466317617893219
Epoch: 3964, Batch Gradient Norm: 11.885183585896852
Epoch: 3964, Batch Gradient Norm after: 11.885183585896852
Epoch 3965/10000, Prediction Accuracy = 61.638%, Loss = 0.48327760100364686
Epoch: 3965, Batch Gradient Norm: 12.456927495126465
Epoch: 3965, Batch Gradient Norm after: 12.456927495126465
Epoch 3966/10000, Prediction Accuracy = 61.688%, Loss = 0.4871668815612793
Epoch: 3966, Batch Gradient Norm: 10.34019388673964
Epoch: 3966, Batch Gradient Norm after: 10.34019388673964
Epoch 3967/10000, Prediction Accuracy = 61.617999999999995%, Loss = 0.47113041281700135
Epoch: 3967, Batch Gradient Norm: 9.806069904440074
Epoch: 3967, Batch Gradient Norm after: 9.806069904440074
Epoch 3968/10000, Prediction Accuracy = 61.646%, Loss = 0.46748852729797363
Epoch: 3968, Batch Gradient Norm: 9.57532058768811
Epoch: 3968, Batch Gradient Norm after: 9.57532058768811
Epoch 3969/10000, Prediction Accuracy = 61.63399999999999%, Loss = 0.46558688282966615
Epoch: 3969, Batch Gradient Norm: 10.608011191723316
Epoch: 3969, Batch Gradient Norm after: 10.608011191723316
Epoch 3970/10000, Prediction Accuracy = 61.734%, Loss = 0.47198558449745176
Epoch: 3970, Batch Gradient Norm: 13.080620924163968
Epoch: 3970, Batch Gradient Norm after: 13.080620924163968
Epoch 3971/10000, Prediction Accuracy = 61.581999999999994%, Loss = 0.49035159349441526
Epoch: 3971, Batch Gradient Norm: 11.474253134711281
Epoch: 3971, Batch Gradient Norm after: 11.474253134711281
Epoch 3972/10000, Prediction Accuracy = 61.693999999999996%, Loss = 0.4807711899280548
Epoch: 3972, Batch Gradient Norm: 8.40014941399344
Epoch: 3972, Batch Gradient Norm after: 8.40014941399344
Epoch 3973/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4612551212310791
Epoch: 3973, Batch Gradient Norm: 6.652770765776707
Epoch: 3973, Batch Gradient Norm after: 6.652770765776707
Epoch 3974/10000, Prediction Accuracy = 61.718%, Loss = 0.45246200561523436
Epoch: 3974, Batch Gradient Norm: 9.52387724550815
Epoch: 3974, Batch Gradient Norm after: 9.52387724550815
Epoch 3975/10000, Prediction Accuracy = 61.664%, Loss = 0.46708683371543885
Epoch: 3975, Batch Gradient Norm: 11.526955231371794
Epoch: 3975, Batch Gradient Norm after: 11.526955231371794
Epoch 3976/10000, Prediction Accuracy = 61.584%, Loss = 0.47957785725593566
Epoch: 3976, Batch Gradient Norm: 12.735992836936399
Epoch: 3976, Batch Gradient Norm after: 12.735992836936399
Epoch 3977/10000, Prediction Accuracy = 61.604%, Loss = 0.4893289268016815
Epoch: 3977, Batch Gradient Norm: 11.948197404810593
Epoch: 3977, Batch Gradient Norm after: 11.948197404810593
Epoch 3978/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.48365800976753237
Epoch: 3978, Batch Gradient Norm: 9.605155696134236
Epoch: 3978, Batch Gradient Norm after: 9.605155696134236
Epoch 3979/10000, Prediction Accuracy = 61.653999999999996%, Loss = 0.46742563843727114
Epoch: 3979, Batch Gradient Norm: 8.777578771261105
Epoch: 3979, Batch Gradient Norm after: 8.777578771261105
Epoch 3980/10000, Prediction Accuracy = 61.76400000000001%, Loss = 0.46169876456260683
Epoch: 3980, Batch Gradient Norm: 9.438055253630218
Epoch: 3980, Batch Gradient Norm after: 9.438055253630218
Epoch 3981/10000, Prediction Accuracy = 61.688%, Loss = 0.4654355883598328
Epoch: 3981, Batch Gradient Norm: 9.005266328754248
Epoch: 3981, Batch Gradient Norm after: 9.005266328754248
Epoch 3982/10000, Prediction Accuracy = 61.754%, Loss = 0.4626044988632202
Epoch: 3982, Batch Gradient Norm: 9.323774951521052
Epoch: 3982, Batch Gradient Norm after: 9.323774951521052
Epoch 3983/10000, Prediction Accuracy = 61.702%, Loss = 0.4639822781085968
Epoch: 3983, Batch Gradient Norm: 9.321412032567812
Epoch: 3983, Batch Gradient Norm after: 9.321412032567812
Epoch 3984/10000, Prediction Accuracy = 61.742000000000004%, Loss = 0.46351887583732604
Epoch: 3984, Batch Gradient Norm: 11.001056653510636
Epoch: 3984, Batch Gradient Norm after: 11.001056653510636
Epoch 3985/10000, Prediction Accuracy = 61.638%, Loss = 0.47525835037231445
Epoch: 3985, Batch Gradient Norm: 11.86180803951774
Epoch: 3985, Batch Gradient Norm after: 11.86180803951774
Epoch 3986/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.484371954202652
Epoch: 3986, Batch Gradient Norm: 11.239300292911098
Epoch: 3986, Batch Gradient Norm after: 11.239300292911098
Epoch 3987/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4785576820373535
Epoch: 3987, Batch Gradient Norm: 10.14382148810494
Epoch: 3987, Batch Gradient Norm after: 10.14382148810494
Epoch 3988/10000, Prediction Accuracy = 61.662%, Loss = 0.4704592049121857
Epoch: 3988, Batch Gradient Norm: 9.52030918905162
Epoch: 3988, Batch Gradient Norm after: 9.52030918905162
Epoch 3989/10000, Prediction Accuracy = 61.65%, Loss = 0.46655823588371276
Epoch: 3989, Batch Gradient Norm: 7.904802417540019
Epoch: 3989, Batch Gradient Norm after: 7.904802417540019
Epoch 3990/10000, Prediction Accuracy = 61.712%, Loss = 0.4579960823059082
Epoch: 3990, Batch Gradient Norm: 8.51180356232237
Epoch: 3990, Batch Gradient Norm after: 8.51180356232237
Epoch 3991/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4624726712703705
Epoch: 3991, Batch Gradient Norm: 7.968156198543306
Epoch: 3991, Batch Gradient Norm after: 7.968156198543306
Epoch 3992/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.45858060121536254
Epoch: 3992, Batch Gradient Norm: 11.658605418292574
Epoch: 3992, Batch Gradient Norm after: 11.658605418292574
Epoch 3993/10000, Prediction Accuracy = 61.774%, Loss = 0.48086196184158325
Epoch: 3993, Batch Gradient Norm: 13.821432091620508
Epoch: 3993, Batch Gradient Norm after: 13.821432091620508
Epoch 3994/10000, Prediction Accuracy = 61.698%, Loss = 0.497284597158432
Epoch: 3994, Batch Gradient Norm: 11.917953911227979
Epoch: 3994, Batch Gradient Norm after: 11.917953911227979
Epoch 3995/10000, Prediction Accuracy = 61.712%, Loss = 0.4839291274547577
Epoch: 3995, Batch Gradient Norm: 9.622716899678556
Epoch: 3995, Batch Gradient Norm after: 9.622716899678556
Epoch 3996/10000, Prediction Accuracy = 61.738%, Loss = 0.46738618016242983
Epoch: 3996, Batch Gradient Norm: 9.607211975466367
Epoch: 3996, Batch Gradient Norm after: 9.607211975466367
Epoch 3997/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4667062222957611
Epoch: 3997, Batch Gradient Norm: 8.964270644277734
Epoch: 3997, Batch Gradient Norm after: 8.964270644277734
Epoch 3998/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4627789735794067
Epoch: 3998, Batch Gradient Norm: 10.179909453631986
Epoch: 3998, Batch Gradient Norm after: 10.179909453631986
Epoch 3999/10000, Prediction Accuracy = 61.67999999999999%, Loss = 0.47038872838020324
Epoch: 3999, Batch Gradient Norm: 11.951972623701401
Epoch: 3999, Batch Gradient Norm after: 11.951972623701401
Epoch 4000/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.4812312126159668
Epoch: 4000, Batch Gradient Norm: 12.946562034551983
Epoch: 4000, Batch Gradient Norm after: 12.946562034551983
Epoch 4001/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.48753634095191956
Epoch: 4001, Batch Gradient Norm: 10.0629071718464
Epoch: 4001, Batch Gradient Norm after: 10.0629071718464
Epoch 4002/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.46752871870994567
Epoch: 4002, Batch Gradient Norm: 9.031538238338957
Epoch: 4002, Batch Gradient Norm after: 9.031538238338957
Epoch 4003/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.4619055509567261
Epoch: 4003, Batch Gradient Norm: 9.669384406753043
Epoch: 4003, Batch Gradient Norm after: 9.669384406753043
Epoch 4004/10000, Prediction Accuracy = 61.718%, Loss = 0.46746874451637266
Epoch: 4004, Batch Gradient Norm: 9.80454780010604
Epoch: 4004, Batch Gradient Norm after: 9.80454780010604
Epoch 4005/10000, Prediction Accuracy = 61.794%, Loss = 0.466925835609436
Epoch: 4005, Batch Gradient Norm: 11.566490115146022
Epoch: 4005, Batch Gradient Norm after: 11.566490115146022
Epoch 4006/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4771375358104706
Epoch: 4006, Batch Gradient Norm: 11.049969342396095
Epoch: 4006, Batch Gradient Norm after: 11.049969342396095
Epoch 4007/10000, Prediction Accuracy = 61.662%, Loss = 0.47349215149879453
Epoch: 4007, Batch Gradient Norm: 9.510901212523882
Epoch: 4007, Batch Gradient Norm after: 9.510901212523882
Epoch 4008/10000, Prediction Accuracy = 61.698%, Loss = 0.46419082283973695
Epoch: 4008, Batch Gradient Norm: 9.725852128917053
Epoch: 4008, Batch Gradient Norm after: 9.725852128917053
Epoch 4009/10000, Prediction Accuracy = 61.66799999999999%, Loss = 0.4673651397228241
Epoch: 4009, Batch Gradient Norm: 9.400590387300884
Epoch: 4009, Batch Gradient Norm after: 9.400590387300884
Epoch 4010/10000, Prediction Accuracy = 61.774%, Loss = 0.4656905770301819
Epoch: 4010, Batch Gradient Norm: 9.33676803852423
Epoch: 4010, Batch Gradient Norm after: 9.33676803852423
Epoch 4011/10000, Prediction Accuracy = 61.652%, Loss = 0.46379958391189574
Epoch: 4011, Batch Gradient Norm: 10.320001093027543
Epoch: 4011, Batch Gradient Norm after: 10.320001093027543
Epoch 4012/10000, Prediction Accuracy = 61.8%, Loss = 0.4700222373008728
Epoch: 4012, Batch Gradient Norm: 10.408510048679723
Epoch: 4012, Batch Gradient Norm after: 10.408510048679723
Epoch 4013/10000, Prediction Accuracy = 61.62800000000001%, Loss = 0.46984313130378724
Epoch: 4013, Batch Gradient Norm: 10.455557107205784
Epoch: 4013, Batch Gradient Norm after: 10.455557107205784
Epoch 4014/10000, Prediction Accuracy = 61.79599999999999%, Loss = 0.4708090782165527
Epoch: 4014, Batch Gradient Norm: 11.760766643945542
Epoch: 4014, Batch Gradient Norm after: 11.760766643945542
Epoch 4015/10000, Prediction Accuracy = 61.641999999999996%, Loss = 0.4822452664375305
Epoch: 4015, Batch Gradient Norm: 9.357019512694817
Epoch: 4015, Batch Gradient Norm after: 9.357019512694817
Epoch 4016/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4662939846515656
Epoch: 4016, Batch Gradient Norm: 9.01445521465452
Epoch: 4016, Batch Gradient Norm after: 9.01445521465452
Epoch 4017/10000, Prediction Accuracy = 61.738%, Loss = 0.4633434355258942
Epoch: 4017, Batch Gradient Norm: 7.997400058969249
Epoch: 4017, Batch Gradient Norm after: 7.997400058969249
Epoch 4018/10000, Prediction Accuracy = 61.678%, Loss = 0.4562428712844849
Epoch: 4018, Batch Gradient Norm: 9.390460286049581
Epoch: 4018, Batch Gradient Norm after: 9.390460286049581
Epoch 4019/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.46352132558822634
Epoch: 4019, Batch Gradient Norm: 10.286428597922757
Epoch: 4019, Batch Gradient Norm after: 10.286428597922757
Epoch 4020/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.4687329649925232
Epoch: 4020, Batch Gradient Norm: 11.599229184828399
Epoch: 4020, Batch Gradient Norm after: 11.599229184828399
Epoch 4021/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4790721356868744
Epoch: 4021, Batch Gradient Norm: 12.434452249108372
Epoch: 4021, Batch Gradient Norm after: 12.434452249108372
Epoch 4022/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.48888561725616453
Epoch: 4022, Batch Gradient Norm: 11.091351003253532
Epoch: 4022, Batch Gradient Norm after: 11.091351003253532
Epoch 4023/10000, Prediction Accuracy = 61.822%, Loss = 0.47932009100914
Epoch: 4023, Batch Gradient Norm: 10.436399008258686
Epoch: 4023, Batch Gradient Norm after: 10.436399008258686
Epoch 4024/10000, Prediction Accuracy = 61.712%, Loss = 0.47159201502799986
Epoch: 4024, Batch Gradient Norm: 10.969919568754996
Epoch: 4024, Batch Gradient Norm after: 10.969919568754996
Epoch 4025/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4758702039718628
Epoch: 4025, Batch Gradient Norm: 9.332887680602754
Epoch: 4025, Batch Gradient Norm after: 9.332887680602754
Epoch 4026/10000, Prediction Accuracy = 61.698%, Loss = 0.46528613567352295
Epoch: 4026, Batch Gradient Norm: 9.549034379325422
Epoch: 4026, Batch Gradient Norm after: 9.549034379325422
Epoch 4027/10000, Prediction Accuracy = 61.79%, Loss = 0.4656721830368042
Epoch: 4027, Batch Gradient Norm: 11.073114538821978
Epoch: 4027, Batch Gradient Norm after: 11.073114538821978
Epoch 4028/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.4740432620048523
Epoch: 4028, Batch Gradient Norm: 11.1462085212978
Epoch: 4028, Batch Gradient Norm after: 11.1462085212978
Epoch 4029/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4743518531322479
Epoch: 4029, Batch Gradient Norm: 10.737346766446372
Epoch: 4029, Batch Gradient Norm after: 10.737346766446372
Epoch 4030/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.47204039096832273
Epoch: 4030, Batch Gradient Norm: 10.248553760513431
Epoch: 4030, Batch Gradient Norm after: 10.248553760513431
Epoch 4031/10000, Prediction Accuracy = 61.74399999999999%, Loss = 0.46951852440834047
Epoch: 4031, Batch Gradient Norm: 10.876333272999332
Epoch: 4031, Batch Gradient Norm after: 10.876333272999332
Epoch 4032/10000, Prediction Accuracy = 61.612%, Loss = 0.4742518484592438
Epoch: 4032, Batch Gradient Norm: 10.683922811857396
Epoch: 4032, Batch Gradient Norm after: 10.683922811857396
Epoch 4033/10000, Prediction Accuracy = 61.8%, Loss = 0.4715914845466614
Epoch: 4033, Batch Gradient Norm: 9.944463054375543
Epoch: 4033, Batch Gradient Norm after: 9.944463054375543
Epoch 4034/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.4664336621761322
Epoch: 4034, Batch Gradient Norm: 9.32937040652203
Epoch: 4034, Batch Gradient Norm after: 9.32937040652203
Epoch 4035/10000, Prediction Accuracy = 61.726%, Loss = 0.4628507435321808
Epoch: 4035, Batch Gradient Norm: 9.957711465037695
Epoch: 4035, Batch Gradient Norm after: 9.957711465037695
Epoch 4036/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.46710450053215025
Epoch: 4036, Batch Gradient Norm: 9.567132489563939
Epoch: 4036, Batch Gradient Norm after: 9.567132489563939
Epoch 4037/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.46446200013160704
Epoch: 4037, Batch Gradient Norm: 9.500509497377871
Epoch: 4037, Batch Gradient Norm after: 9.500509497377871
Epoch 4038/10000, Prediction Accuracy = 61.76400000000001%, Loss = 0.46415974497795104
Epoch: 4038, Batch Gradient Norm: 9.187827087207678
Epoch: 4038, Batch Gradient Norm after: 9.187827087207678
Epoch 4039/10000, Prediction Accuracy = 61.79799999999999%, Loss = 0.4628675699234009
Epoch: 4039, Batch Gradient Norm: 10.268370800586025
Epoch: 4039, Batch Gradient Norm after: 10.268370800586025
Epoch 4040/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.47031269073486326
Epoch: 4040, Batch Gradient Norm: 10.19353468486746
Epoch: 4040, Batch Gradient Norm after: 10.19353468486746
Epoch 4041/10000, Prediction Accuracy = 61.688%, Loss = 0.4691589832305908
Epoch: 4041, Batch Gradient Norm: 10.896417669904896
Epoch: 4041, Batch Gradient Norm after: 10.896417669904896
Epoch 4042/10000, Prediction Accuracy = 61.61800000000001%, Loss = 0.47147101163864136
Epoch: 4042, Batch Gradient Norm: 11.61123065519509
Epoch: 4042, Batch Gradient Norm after: 11.61123065519509
Epoch 4043/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.47861127853393554
Epoch: 4043, Batch Gradient Norm: 8.580933918193548
Epoch: 4043, Batch Gradient Norm after: 8.580933918193548
Epoch 4044/10000, Prediction Accuracy = 61.73%, Loss = 0.46046115159988404
Epoch: 4044, Batch Gradient Norm: 8.135410869654146
Epoch: 4044, Batch Gradient Norm after: 8.135410869654146
Epoch 4045/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.4576167702674866
Epoch: 4045, Batch Gradient Norm: 8.270931190751279
Epoch: 4045, Batch Gradient Norm after: 8.270931190751279
Epoch 4046/10000, Prediction Accuracy = 61.736000000000004%, Loss = 0.4573711097240448
Epoch: 4046, Batch Gradient Norm: 11.326031241135585
Epoch: 4046, Batch Gradient Norm after: 11.326031241135585
Epoch 4047/10000, Prediction Accuracy = 61.64399999999999%, Loss = 0.4749281942844391
Epoch: 4047, Batch Gradient Norm: 14.843631503006142
Epoch: 4047, Batch Gradient Norm after: 14.843631503006142
Epoch 4048/10000, Prediction Accuracy = 61.75%, Loss = 0.5038207173347473
Epoch: 4048, Batch Gradient Norm: 11.448382281885012
Epoch: 4048, Batch Gradient Norm after: 11.448382281885012
Epoch 4049/10000, Prediction Accuracy = 61.678%, Loss = 0.4788621723651886
Epoch: 4049, Batch Gradient Norm: 8.779092338493102
Epoch: 4049, Batch Gradient Norm after: 8.779092338493102
Epoch 4050/10000, Prediction Accuracy = 61.766%, Loss = 0.4612461805343628
Epoch: 4050, Batch Gradient Norm: 8.96192977198013
Epoch: 4050, Batch Gradient Norm after: 8.96192977198013
Epoch 4051/10000, Prediction Accuracy = 61.674%, Loss = 0.4628447949886322
Epoch: 4051, Batch Gradient Norm: 9.59000166036406
Epoch: 4051, Batch Gradient Norm after: 9.59000166036406
Epoch 4052/10000, Prediction Accuracy = 61.826%, Loss = 0.46661139130592344
Epoch: 4052, Batch Gradient Norm: 10.10776586532534
Epoch: 4052, Batch Gradient Norm after: 10.10776586532534
Epoch 4053/10000, Prediction Accuracy = 61.648%, Loss = 0.4689279019832611
Epoch: 4053, Batch Gradient Norm: 9.444570477237571
Epoch: 4053, Batch Gradient Norm after: 9.444570477237571
Epoch 4054/10000, Prediction Accuracy = 61.738%, Loss = 0.4637834906578064
Epoch: 4054, Batch Gradient Norm: 9.33732876253908
Epoch: 4054, Batch Gradient Norm after: 9.33732876253908
Epoch 4055/10000, Prediction Accuracy = 61.71%, Loss = 0.46259068250656127
Epoch: 4055, Batch Gradient Norm: 9.948604565422675
Epoch: 4055, Batch Gradient Norm after: 9.948604565422675
Epoch 4056/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.466588693857193
Epoch: 4056, Batch Gradient Norm: 9.753345061963467
Epoch: 4056, Batch Gradient Norm after: 9.753345061963467
Epoch 4057/10000, Prediction Accuracy = 61.67%, Loss = 0.46621265411376955
Epoch: 4057, Batch Gradient Norm: 9.353483754665474
Epoch: 4057, Batch Gradient Norm after: 9.353483754665474
Epoch 4058/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4625608026981354
Epoch: 4058, Batch Gradient Norm: 12.550541628009267
Epoch: 4058, Batch Gradient Norm after: 12.550541628009267
Epoch 4059/10000, Prediction Accuracy = 61.63599999999999%, Loss = 0.48514556884765625
Epoch: 4059, Batch Gradient Norm: 11.705992833877827
Epoch: 4059, Batch Gradient Norm after: 11.705992833877827
Epoch 4060/10000, Prediction Accuracy = 61.786%, Loss = 0.4801983952522278
Epoch: 4060, Batch Gradient Norm: 9.213056733533502
Epoch: 4060, Batch Gradient Norm after: 9.213056733533502
Epoch 4061/10000, Prediction Accuracy = 61.762%, Loss = 0.46265602111816406
Epoch: 4061, Batch Gradient Norm: 10.24080964510803
Epoch: 4061, Batch Gradient Norm after: 10.24080964510803
Epoch 4062/10000, Prediction Accuracy = 61.862%, Loss = 0.4676178991794586
Epoch: 4062, Batch Gradient Norm: 12.275436131933812
Epoch: 4062, Batch Gradient Norm after: 12.275436131933812
Epoch 4063/10000, Prediction Accuracy = 61.726%, Loss = 0.4834583103656769
Epoch: 4063, Batch Gradient Norm: 11.168818666567361
Epoch: 4063, Batch Gradient Norm after: 11.168818666567361
Epoch 4064/10000, Prediction Accuracy = 61.722%, Loss = 0.47496354579925537
Epoch: 4064, Batch Gradient Norm: 10.187392760021666
Epoch: 4064, Batch Gradient Norm after: 10.187392760021666
Epoch 4065/10000, Prediction Accuracy = 61.818%, Loss = 0.46662322878837587
Epoch: 4065, Batch Gradient Norm: 11.907552811849214
Epoch: 4065, Batch Gradient Norm after: 11.907552811849214
Epoch 4066/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.47880387902259824
Epoch: 4066, Batch Gradient Norm: 11.183450766230889
Epoch: 4066, Batch Gradient Norm after: 11.183450766230889
Epoch 4067/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4753371119499207
Epoch: 4067, Batch Gradient Norm: 9.1585770562486
Epoch: 4067, Batch Gradient Norm after: 9.1585770562486
Epoch 4068/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.46235484480857847
Epoch: 4068, Batch Gradient Norm: 8.364711681653445
Epoch: 4068, Batch Gradient Norm after: 8.364711681653445
Epoch 4069/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4570991337299347
Epoch: 4069, Batch Gradient Norm: 8.542662365411074
Epoch: 4069, Batch Gradient Norm after: 8.542662365411074
Epoch 4070/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.45750924944877625
Epoch: 4070, Batch Gradient Norm: 9.30216918705408
Epoch: 4070, Batch Gradient Norm after: 9.30216918705408
Epoch 4071/10000, Prediction Accuracy = 61.662%, Loss = 0.4610384225845337
Epoch: 4071, Batch Gradient Norm: 10.569092119589408
Epoch: 4071, Batch Gradient Norm after: 10.569092119589408
Epoch 4072/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.47004985213279726
Epoch: 4072, Batch Gradient Norm: 9.773442429874077
Epoch: 4072, Batch Gradient Norm after: 9.773442429874077
Epoch 4073/10000, Prediction Accuracy = 61.634%, Loss = 0.4681159913539886
Epoch: 4073, Batch Gradient Norm: 9.028030400969044
Epoch: 4073, Batch Gradient Norm after: 9.028030400969044
Epoch 4074/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.46317845582962036
Epoch: 4074, Batch Gradient Norm: 7.8977706255801605
Epoch: 4074, Batch Gradient Norm after: 7.8977706255801605
Epoch 4075/10000, Prediction Accuracy = 61.778000000000006%, Loss = 0.4553041458129883
Epoch: 4075, Batch Gradient Norm: 11.146930343803925
Epoch: 4075, Batch Gradient Norm after: 11.146930343803925
Epoch 4076/10000, Prediction Accuracy = 61.684000000000005%, Loss = 0.47484784126281737
Epoch: 4076, Batch Gradient Norm: 13.559503562676488
Epoch: 4076, Batch Gradient Norm after: 13.559503562676488
Epoch 4077/10000, Prediction Accuracy = 61.7%, Loss = 0.4934782922267914
Epoch: 4077, Batch Gradient Norm: 11.652179113744417
Epoch: 4077, Batch Gradient Norm after: 11.652179113744417
Epoch 4078/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.4794288456439972
Epoch: 4078, Batch Gradient Norm: 9.014237021863146
Epoch: 4078, Batch Gradient Norm after: 9.014237021863146
Epoch 4079/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.46013215780258176
Epoch: 4079, Batch Gradient Norm: 9.166517853865738
Epoch: 4079, Batch Gradient Norm after: 9.166517853865738
Epoch 4080/10000, Prediction Accuracy = 61.746%, Loss = 0.45969983339309695
Epoch: 4080, Batch Gradient Norm: 10.701835291442704
Epoch: 4080, Batch Gradient Norm after: 10.701835291442704
Epoch 4081/10000, Prediction Accuracy = 61.7%, Loss = 0.46856595277786256
Epoch: 4081, Batch Gradient Norm: 10.924968770702312
Epoch: 4081, Batch Gradient Norm after: 10.924968770702312
Epoch 4082/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4710217654705048
Epoch: 4082, Batch Gradient Norm: 10.690508520130166
Epoch: 4082, Batch Gradient Norm after: 10.690508520130166
Epoch 4083/10000, Prediction Accuracy = 61.738%, Loss = 0.4709426462650299
Epoch: 4083, Batch Gradient Norm: 11.304531336342068
Epoch: 4083, Batch Gradient Norm after: 11.304531336342068
Epoch 4084/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.478226912021637
Epoch: 4084, Batch Gradient Norm: 9.44274292842619
Epoch: 4084, Batch Gradient Norm after: 9.44274292842619
Epoch 4085/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.46493396162986755
Epoch: 4085, Batch Gradient Norm: 8.04880821875799
Epoch: 4085, Batch Gradient Norm after: 8.04880821875799
Epoch 4086/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.45474777817726136
Epoch: 4086, Batch Gradient Norm: 9.930382575753596
Epoch: 4086, Batch Gradient Norm after: 9.930382575753596
Epoch 4087/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.4659464776515961
Epoch: 4087, Batch Gradient Norm: 9.61489620370914
Epoch: 4087, Batch Gradient Norm after: 9.61489620370914
Epoch 4088/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.46368310451507566
Epoch: 4088, Batch Gradient Norm: 10.544890798628069
Epoch: 4088, Batch Gradient Norm after: 10.544890798628069
Epoch 4089/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4690967559814453
Epoch: 4089, Batch Gradient Norm: 12.566610240075603
Epoch: 4089, Batch Gradient Norm after: 12.566610240075603
Epoch 4090/10000, Prediction Accuracy = 61.742%, Loss = 0.48544015288352965
Epoch: 4090, Batch Gradient Norm: 9.145897694731548
Epoch: 4090, Batch Gradient Norm after: 9.145897694731548
Epoch 4091/10000, Prediction Accuracy = 61.681999999999995%, Loss = 0.4619670331478119
Epoch: 4091, Batch Gradient Norm: 7.369381623131823
Epoch: 4091, Batch Gradient Norm after: 7.369381623131823
Epoch 4092/10000, Prediction Accuracy = 61.757999999999996%, Loss = 0.45169671773910525
Epoch: 4092, Batch Gradient Norm: 8.5892509842091
Epoch: 4092, Batch Gradient Norm after: 8.5892509842091
Epoch 4093/10000, Prediction Accuracy = 61.794000000000004%, Loss = 0.4571834444999695
Epoch: 4093, Batch Gradient Norm: 10.418135592775576
Epoch: 4093, Batch Gradient Norm after: 10.418135592775576
Epoch 4094/10000, Prediction Accuracy = 61.660000000000004%, Loss = 0.468266624212265
Epoch: 4094, Batch Gradient Norm: 11.552627504134
Epoch: 4094, Batch Gradient Norm after: 11.552627504134
Epoch 4095/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.4757974326610565
Epoch: 4095, Batch Gradient Norm: 10.646996469846833
Epoch: 4095, Batch Gradient Norm after: 10.646996469846833
Epoch 4096/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.4708707332611084
Epoch: 4096, Batch Gradient Norm: 9.759517831966729
Epoch: 4096, Batch Gradient Norm after: 9.759517831966729
Epoch 4097/10000, Prediction Accuracy = 61.754%, Loss = 0.46547834277153016
Epoch: 4097, Batch Gradient Norm: 8.860783151192916
Epoch: 4097, Batch Gradient Norm after: 8.860783151192916
Epoch 4098/10000, Prediction Accuracy = 61.812%, Loss = 0.458619749546051
Epoch: 4098, Batch Gradient Norm: 12.453191752331083
Epoch: 4098, Batch Gradient Norm after: 12.453191752331083
Epoch 4099/10000, Prediction Accuracy = 61.705999999999996%, Loss = 0.4838755548000336
Epoch: 4099, Batch Gradient Norm: 11.570779396553734
Epoch: 4099, Batch Gradient Norm after: 11.570779396553734
Epoch 4100/10000, Prediction Accuracy = 61.81400000000001%, Loss = 0.4786012709140778
Epoch: 4100, Batch Gradient Norm: 10.368126540042686
Epoch: 4100, Batch Gradient Norm after: 10.368126540042686
Epoch 4101/10000, Prediction Accuracy = 61.754000000000005%, Loss = 0.46792633533477784
Epoch: 4101, Batch Gradient Norm: 12.226385926968224
Epoch: 4101, Batch Gradient Norm after: 12.226385926968224
Epoch 4102/10000, Prediction Accuracy = 61.598%, Loss = 0.4805694341659546
Epoch: 4102, Batch Gradient Norm: 12.434209184503679
Epoch: 4102, Batch Gradient Norm after: 12.434209184503679
Epoch 4103/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4830738067626953
Epoch: 4103, Batch Gradient Norm: 11.003868497236443
Epoch: 4103, Batch Gradient Norm after: 11.003868497236443
Epoch 4104/10000, Prediction Accuracy = 61.672000000000004%, Loss = 0.47410789132118225
Epoch: 4104, Batch Gradient Norm: 10.115327136543891
Epoch: 4104, Batch Gradient Norm after: 10.115327136543891
Epoch 4105/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.46631640791893003
Epoch: 4105, Batch Gradient Norm: 10.295366509588854
Epoch: 4105, Batch Gradient Norm after: 10.295366509588854
Epoch 4106/10000, Prediction Accuracy = 61.676%, Loss = 0.4666692316532135
Epoch: 4106, Batch Gradient Norm: 9.571344229628625
Epoch: 4106, Batch Gradient Norm after: 9.571344229628625
Epoch 4107/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4620685696601868
Epoch: 4107, Batch Gradient Norm: 8.560629288771242
Epoch: 4107, Batch Gradient Norm after: 8.560629288771242
Epoch 4108/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.45628172159194946
Epoch: 4108, Batch Gradient Norm: 9.077395183505585
Epoch: 4108, Batch Gradient Norm after: 9.077395183505585
Epoch 4109/10000, Prediction Accuracy = 61.802%, Loss = 0.46016523241996765
Epoch: 4109, Batch Gradient Norm: 9.69027686718291
Epoch: 4109, Batch Gradient Norm after: 9.69027686718291
Epoch 4110/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.46315916776657107
Epoch: 4110, Batch Gradient Norm: 10.379774586641215
Epoch: 4110, Batch Gradient Norm after: 10.379774586641215
Epoch 4111/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.4676643371582031
Epoch: 4111, Batch Gradient Norm: 10.367883681263372
Epoch: 4111, Batch Gradient Norm after: 10.367883681263372
Epoch 4112/10000, Prediction Accuracy = 61.614%, Loss = 0.4680900514125824
Epoch: 4112, Batch Gradient Norm: 8.0982367149477
Epoch: 4112, Batch Gradient Norm after: 8.0982367149477
Epoch 4113/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.4542961120605469
Epoch: 4113, Batch Gradient Norm: 7.650166340656134
Epoch: 4113, Batch Gradient Norm after: 7.650166340656134
Epoch 4114/10000, Prediction Accuracy = 61.714%, Loss = 0.4512780368328094
Epoch: 4114, Batch Gradient Norm: 9.755409906994466
Epoch: 4114, Batch Gradient Norm after: 9.755409906994466
Epoch 4115/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.46260674595832824
Epoch: 4115, Batch Gradient Norm: 13.162432059168683
Epoch: 4115, Batch Gradient Norm after: 13.162432059168683
Epoch 4116/10000, Prediction Accuracy = 61.662%, Loss = 0.49090092778205874
Epoch: 4116, Batch Gradient Norm: 10.08654449497059
Epoch: 4116, Batch Gradient Norm after: 10.08654449497059
Epoch 4117/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4686684012413025
Epoch: 4117, Batch Gradient Norm: 6.8956828895875155
Epoch: 4117, Batch Gradient Norm after: 6.8956828895875155
Epoch 4118/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.4486422657966614
Epoch: 4118, Batch Gradient Norm: 10.101673052649286
Epoch: 4118, Batch Gradient Norm after: 10.101673052649286
Epoch 4119/10000, Prediction Accuracy = 61.712%, Loss = 0.4669762909412384
Epoch: 4119, Batch Gradient Norm: 12.690837722798374
Epoch: 4119, Batch Gradient Norm after: 12.690837722798374
Epoch 4120/10000, Prediction Accuracy = 61.734%, Loss = 0.4871550977230072
Epoch: 4120, Batch Gradient Norm: 10.487324931707354
Epoch: 4120, Batch Gradient Norm after: 10.487324931707354
Epoch 4121/10000, Prediction Accuracy = 61.65599999999999%, Loss = 0.4701204240322113
Epoch: 4121, Batch Gradient Norm: 8.590040612922175
Epoch: 4121, Batch Gradient Norm after: 8.590040612922175
Epoch 4122/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.4570798218250275
Epoch: 4122, Batch Gradient Norm: 8.87670945606935
Epoch: 4122, Batch Gradient Norm after: 8.87670945606935
Epoch 4123/10000, Prediction Accuracy = 61.712%, Loss = 0.45896885395050047
Epoch: 4123, Batch Gradient Norm: 10.175560362305148
Epoch: 4123, Batch Gradient Norm after: 10.175560362305148
Epoch 4124/10000, Prediction Accuracy = 61.75600000000001%, Loss = 0.4679820418357849
Epoch: 4124, Batch Gradient Norm: 9.932567565257331
Epoch: 4124, Batch Gradient Norm after: 9.932567565257331
Epoch 4125/10000, Prediction Accuracy = 61.694%, Loss = 0.4647620439529419
Epoch: 4125, Batch Gradient Norm: 10.99852331123701
Epoch: 4125, Batch Gradient Norm after: 10.99852331123701
Epoch 4126/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4708764851093292
Epoch: 4126, Batch Gradient Norm: 12.016132093133729
Epoch: 4126, Batch Gradient Norm after: 12.016132093133729
Epoch 4127/10000, Prediction Accuracy = 61.738%, Loss = 0.47754493951797483
Epoch: 4127, Batch Gradient Norm: 13.251341737644255
Epoch: 4127, Batch Gradient Norm after: 13.251341737644255
Epoch 4128/10000, Prediction Accuracy = 61.70399999999999%, Loss = 0.48863892555236815
Epoch: 4128, Batch Gradient Norm: 11.251417212077314
Epoch: 4128, Batch Gradient Norm after: 11.251417212077314
Epoch 4129/10000, Prediction Accuracy = 61.726%, Loss = 0.4747326672077179
Epoch: 4129, Batch Gradient Norm: 8.840942832252557
Epoch: 4129, Batch Gradient Norm after: 8.840942832252557
Epoch 4130/10000, Prediction Accuracy = 61.738%, Loss = 0.45846293568611146
Epoch: 4130, Batch Gradient Norm: 8.051495875216705
Epoch: 4130, Batch Gradient Norm after: 8.051495875216705
Epoch 4131/10000, Prediction Accuracy = 61.77%, Loss = 0.4541060864925385
Epoch: 4131, Batch Gradient Norm: 9.183036393319238
Epoch: 4131, Batch Gradient Norm after: 9.183036393319238
Epoch 4132/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.45943615436553953
Epoch: 4132, Batch Gradient Norm: 10.939090388736615
Epoch: 4132, Batch Gradient Norm after: 10.939090388736615
Epoch 4133/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.469200986623764
Epoch: 4133, Batch Gradient Norm: 13.230838394228044
Epoch: 4133, Batch Gradient Norm after: 13.230838394228044
Epoch 4134/10000, Prediction Accuracy = 61.632000000000005%, Loss = 0.4850732982158661
Epoch: 4134, Batch Gradient Norm: 12.097133384248997
Epoch: 4134, Batch Gradient Norm after: 12.097133384248997
Epoch 4135/10000, Prediction Accuracy = 61.762%, Loss = 0.47946333289146426
Epoch: 4135, Batch Gradient Norm: 8.82861208856873
Epoch: 4135, Batch Gradient Norm after: 8.82861208856873
Epoch 4136/10000, Prediction Accuracy = 61.734%, Loss = 0.45801459550857543
Epoch: 4136, Batch Gradient Norm: 8.061342028131088
Epoch: 4136, Batch Gradient Norm after: 8.061342028131088
Epoch 4137/10000, Prediction Accuracy = 61.788%, Loss = 0.4532862722873688
Epoch: 4137, Batch Gradient Norm: 8.232316281515011
Epoch: 4137, Batch Gradient Norm after: 8.232316281515011
Epoch 4138/10000, Prediction Accuracy = 61.73%, Loss = 0.4542897164821625
Epoch: 4138, Batch Gradient Norm: 8.882122306678944
Epoch: 4138, Batch Gradient Norm after: 8.882122306678944
Epoch 4139/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.45766052007675173
Epoch: 4139, Batch Gradient Norm: 12.166670472070253
Epoch: 4139, Batch Gradient Norm after: 12.166670472070253
Epoch 4140/10000, Prediction Accuracy = 61.658%, Loss = 0.47989514470100403
Epoch: 4140, Batch Gradient Norm: 12.357133390969466
Epoch: 4140, Batch Gradient Norm after: 12.357133390969466
Epoch 4141/10000, Prediction Accuracy = 61.786%, Loss = 0.48348047733306887
Epoch: 4141, Batch Gradient Norm: 8.905245396116607
Epoch: 4141, Batch Gradient Norm after: 8.905245396116607
Epoch 4142/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.45850124955177307
Epoch: 4142, Batch Gradient Norm: 7.9723142462172385
Epoch: 4142, Batch Gradient Norm after: 7.9723142462172385
Epoch 4143/10000, Prediction Accuracy = 61.678%, Loss = 0.4522155225276947
Epoch: 4143, Batch Gradient Norm: 10.86747482794553
Epoch: 4143, Batch Gradient Norm after: 10.86747482794553
Epoch 4144/10000, Prediction Accuracy = 61.772000000000006%, Loss = 0.4702451407909393
Epoch: 4144, Batch Gradient Norm: 10.983738089320548
Epoch: 4144, Batch Gradient Norm after: 10.983738089320548
Epoch 4145/10000, Prediction Accuracy = 61.73%, Loss = 0.4720696032047272
Epoch: 4145, Batch Gradient Norm: 9.711049912542235
Epoch: 4145, Batch Gradient Norm after: 9.711049912542235
Epoch 4146/10000, Prediction Accuracy = 61.802%, Loss = 0.4630319058895111
Epoch: 4146, Batch Gradient Norm: 10.032475360396493
Epoch: 4146, Batch Gradient Norm after: 10.032475360396493
Epoch 4147/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.4649172842502594
Epoch: 4147, Batch Gradient Norm: 10.758770580763484
Epoch: 4147, Batch Gradient Norm after: 10.758770580763484
Epoch 4148/10000, Prediction Accuracy = 61.854%, Loss = 0.47090513706207277
Epoch: 4148, Batch Gradient Norm: 9.956803566224394
Epoch: 4148, Batch Gradient Norm after: 9.956803566224394
Epoch 4149/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.466046941280365
Epoch: 4149, Batch Gradient Norm: 8.087533062331222
Epoch: 4149, Batch Gradient Norm after: 8.087533062331222
Epoch 4150/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4533550500869751
Epoch: 4150, Batch Gradient Norm: 9.96684369689303
Epoch: 4150, Batch Gradient Norm after: 9.96684369689303
Epoch 4151/10000, Prediction Accuracy = 61.80200000000001%, Loss = 0.46372694969177247
Epoch: 4151, Batch Gradient Norm: 11.464517232796382
Epoch: 4151, Batch Gradient Norm after: 11.464517232796382
Epoch 4152/10000, Prediction Accuracy = 61.742000000000004%, Loss = 0.47436060905456545
Epoch: 4152, Batch Gradient Norm: 11.856291610097141
Epoch: 4152, Batch Gradient Norm after: 11.856291610097141
Epoch 4153/10000, Prediction Accuracy = 61.758%, Loss = 0.47862533330917356
Epoch: 4153, Batch Gradient Norm: 8.969327919176004
Epoch: 4153, Batch Gradient Norm after: 8.969327919176004
Epoch 4154/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.45861695408821107
Epoch: 4154, Batch Gradient Norm: 8.485018882171259
Epoch: 4154, Batch Gradient Norm after: 8.485018882171259
Epoch 4155/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4552392303943634
Epoch: 4155, Batch Gradient Norm: 9.284282161668099
Epoch: 4155, Batch Gradient Norm after: 9.284282161668099
Epoch 4156/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.45899155735969543
Epoch: 4156, Batch Gradient Norm: 10.872715189387508
Epoch: 4156, Batch Gradient Norm after: 10.872715189387508
Epoch 4157/10000, Prediction Accuracy = 61.836%, Loss = 0.4689013481140137
Epoch: 4157, Batch Gradient Norm: 11.6574155134311
Epoch: 4157, Batch Gradient Norm after: 11.6574155134311
Epoch 4158/10000, Prediction Accuracy = 61.668000000000006%, Loss = 0.4749875724315643
Epoch: 4158, Batch Gradient Norm: 10.797609937144651
Epoch: 4158, Batch Gradient Norm after: 10.797609937144651
Epoch 4159/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.46889403462409973
Epoch: 4159, Batch Gradient Norm: 11.102478332095671
Epoch: 4159, Batch Gradient Norm after: 11.102478332095671
Epoch 4160/10000, Prediction Accuracy = 61.7%, Loss = 0.4709009289741516
Epoch: 4160, Batch Gradient Norm: 11.915912215447541
Epoch: 4160, Batch Gradient Norm after: 11.915912215447541
Epoch 4161/10000, Prediction Accuracy = 61.814%, Loss = 0.47946099042892454
Epoch: 4161, Batch Gradient Norm: 10.009063573380391
Epoch: 4161, Batch Gradient Norm after: 10.009063573380391
Epoch 4162/10000, Prediction Accuracy = 61.732000000000006%, Loss = 0.46505969762802124
Epoch: 4162, Batch Gradient Norm: 9.57889451858016
Epoch: 4162, Batch Gradient Norm after: 9.57889451858016
Epoch 4163/10000, Prediction Accuracy = 61.86800000000001%, Loss = 0.46085121631622317
Epoch: 4163, Batch Gradient Norm: 10.134828092033338
Epoch: 4163, Batch Gradient Norm after: 10.134828092033338
Epoch 4164/10000, Prediction Accuracy = 61.778%, Loss = 0.4648549497127533
Epoch: 4164, Batch Gradient Norm: 10.01416153594162
Epoch: 4164, Batch Gradient Norm after: 10.01416153594162
Epoch 4165/10000, Prediction Accuracy = 61.738%, Loss = 0.4656756341457367
Epoch: 4165, Batch Gradient Norm: 8.300207821888733
Epoch: 4165, Batch Gradient Norm after: 8.300207821888733
Epoch 4166/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.45457305312156676
Epoch: 4166, Batch Gradient Norm: 10.186175091685673
Epoch: 4166, Batch Gradient Norm after: 10.186175091685673
Epoch 4167/10000, Prediction Accuracy = 61.730000000000004%, Loss = 0.4648496747016907
Epoch: 4167, Batch Gradient Norm: 12.060334486442931
Epoch: 4167, Batch Gradient Norm after: 12.060334486442931
Epoch 4168/10000, Prediction Accuracy = 61.77%, Loss = 0.4783360600471497
Epoch: 4168, Batch Gradient Norm: 11.698073202958534
Epoch: 4168, Batch Gradient Norm after: 11.698073202958534
Epoch 4169/10000, Prediction Accuracy = 61.738%, Loss = 0.4770305633544922
Epoch: 4169, Batch Gradient Norm: 9.321931519999207
Epoch: 4169, Batch Gradient Norm after: 9.321931519999207
Epoch 4170/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.46017042398452757
Epoch: 4170, Batch Gradient Norm: 7.8034001239702695
Epoch: 4170, Batch Gradient Norm after: 7.8034001239702695
Epoch 4171/10000, Prediction Accuracy = 61.754%, Loss = 0.45118074417114257
Epoch: 4171, Batch Gradient Norm: 10.541999522582373
Epoch: 4171, Batch Gradient Norm after: 10.541999522582373
Epoch 4172/10000, Prediction Accuracy = 61.754%, Loss = 0.466831761598587
Epoch: 4172, Batch Gradient Norm: 11.149173043702728
Epoch: 4172, Batch Gradient Norm after: 11.149173043702728
Epoch 4173/10000, Prediction Accuracy = 61.722%, Loss = 0.4716756701469421
Epoch: 4173, Batch Gradient Norm: 10.65339341939714
Epoch: 4173, Batch Gradient Norm after: 10.65339341939714
Epoch 4174/10000, Prediction Accuracy = 61.720000000000006%, Loss = 0.46805397272109983
Epoch: 4174, Batch Gradient Norm: 9.405361500448434
Epoch: 4174, Batch Gradient Norm after: 9.405361500448434
Epoch 4175/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4611403465270996
Epoch: 4175, Batch Gradient Norm: 8.110253234288223
Epoch: 4175, Batch Gradient Norm after: 8.110253234288223
Epoch 4176/10000, Prediction Accuracy = 61.73199999999999%, Loss = 0.4538343608379364
Epoch: 4176, Batch Gradient Norm: 9.51405729158771
Epoch: 4176, Batch Gradient Norm after: 9.51405729158771
Epoch 4177/10000, Prediction Accuracy = 61.72800000000001%, Loss = 0.460729056596756
Epoch: 4177, Batch Gradient Norm: 11.66341169877577
Epoch: 4177, Batch Gradient Norm after: 11.66341169877577
Epoch 4178/10000, Prediction Accuracy = 61.751999999999995%, Loss = 0.47489548921585084
Epoch: 4178, Batch Gradient Norm: 11.2931440791434
Epoch: 4178, Batch Gradient Norm after: 11.2931440791434
Epoch 4179/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.4719225287437439
Epoch: 4179, Batch Gradient Norm: 10.388220000703878
Epoch: 4179, Batch Gradient Norm after: 10.388220000703878
Epoch 4180/10000, Prediction Accuracy = 61.864%, Loss = 0.46479775905609133
Epoch: 4180, Batch Gradient Norm: 11.10712558702916
Epoch: 4180, Batch Gradient Norm after: 11.10712558702916
Epoch 4181/10000, Prediction Accuracy = 61.717999999999996%, Loss = 0.4704282104969025
Epoch: 4181, Batch Gradient Norm: 10.78273471204249
Epoch: 4181, Batch Gradient Norm after: 10.78273471204249
Epoch 4182/10000, Prediction Accuracy = 61.694%, Loss = 0.47114495038986204
Epoch: 4182, Batch Gradient Norm: 9.54628122172725
Epoch: 4182, Batch Gradient Norm after: 9.54628122172725
Epoch 4183/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.46118497252464297
Epoch: 4183, Batch Gradient Norm: 10.840230223367168
Epoch: 4183, Batch Gradient Norm after: 10.840230223367168
Epoch 4184/10000, Prediction Accuracy = 61.715999999999994%, Loss = 0.4689881920814514
Epoch: 4184, Batch Gradient Norm: 10.645641043368242
Epoch: 4184, Batch Gradient Norm after: 10.645641043368242
Epoch 4185/10000, Prediction Accuracy = 61.862%, Loss = 0.46658830642700194
Epoch: 4185, Batch Gradient Norm: 10.01486510474435
Epoch: 4185, Batch Gradient Norm after: 10.01486510474435
Epoch 4186/10000, Prediction Accuracy = 61.754%, Loss = 0.46183820962905886
Epoch: 4186, Batch Gradient Norm: 10.00824259106732
Epoch: 4186, Batch Gradient Norm after: 10.00824259106732
Epoch 4187/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.46253750324249265
Epoch: 4187, Batch Gradient Norm: 10.160571735630267
Epoch: 4187, Batch Gradient Norm after: 10.160571735630267
Epoch 4188/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.46379966139793394
Epoch: 4188, Batch Gradient Norm: 10.072180634001958
Epoch: 4188, Batch Gradient Norm after: 10.072180634001958
Epoch 4189/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.46351745128631594
Epoch: 4189, Batch Gradient Norm: 10.686288024331972
Epoch: 4189, Batch Gradient Norm after: 10.686288024331972
Epoch 4190/10000, Prediction Accuracy = 61.774%, Loss = 0.4683776438236237
Epoch: 4190, Batch Gradient Norm: 9.76488660669501
Epoch: 4190, Batch Gradient Norm after: 9.76488660669501
Epoch 4191/10000, Prediction Accuracy = 61.779999999999994%, Loss = 0.46212329864501955
Epoch: 4191, Batch Gradient Norm: 9.865614385901102
Epoch: 4191, Batch Gradient Norm after: 9.865614385901102
Epoch 4192/10000, Prediction Accuracy = 61.826%, Loss = 0.46323023438453675
Epoch: 4192, Batch Gradient Norm: 8.238537920391256
Epoch: 4192, Batch Gradient Norm after: 8.238537920391256
Epoch 4193/10000, Prediction Accuracy = 61.812%, Loss = 0.4531292080879211
Epoch: 4193, Batch Gradient Norm: 10.194561199525216
Epoch: 4193, Batch Gradient Norm after: 10.194561199525216
Epoch 4194/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.4644371151924133
Epoch: 4194, Batch Gradient Norm: 12.735302329744554
Epoch: 4194, Batch Gradient Norm after: 12.735302329744554
Epoch 4195/10000, Prediction Accuracy = 61.69%, Loss = 0.48400349020957945
Epoch: 4195, Batch Gradient Norm: 10.61244804686764
Epoch: 4195, Batch Gradient Norm after: 10.61244804686764
Epoch 4196/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.467500776052475
Epoch: 4196, Batch Gradient Norm: 9.280058441426606
Epoch: 4196, Batch Gradient Norm after: 9.280058441426606
Epoch 4197/10000, Prediction Accuracy = 61.826%, Loss = 0.45796151757240294
Epoch: 4197, Batch Gradient Norm: 9.263360718281739
Epoch: 4197, Batch Gradient Norm after: 9.263360718281739
Epoch 4198/10000, Prediction Accuracy = 61.75599999999999%, Loss = 0.45798306465148925
Epoch: 4198, Batch Gradient Norm: 10.066516167959367
Epoch: 4198, Batch Gradient Norm after: 10.066516167959367
Epoch 4199/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.4645485758781433
Epoch: 4199, Batch Gradient Norm: 10.165305121886732
Epoch: 4199, Batch Gradient Norm after: 10.165305121886732
Epoch 4200/10000, Prediction Accuracy = 61.694%, Loss = 0.46642720103263857
Epoch: 4200, Batch Gradient Norm: 10.294185369626598
Epoch: 4200, Batch Gradient Norm after: 10.294185369626598
Epoch 4201/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.46636263728141786
Epoch: 4201, Batch Gradient Norm: 11.152944750713358
Epoch: 4201, Batch Gradient Norm after: 11.152944750713358
Epoch 4202/10000, Prediction Accuracy = 61.78000000000001%, Loss = 0.47144014239311216
Epoch: 4202, Batch Gradient Norm: 11.705442572195764
Epoch: 4202, Batch Gradient Norm after: 11.705442572195764
Epoch 4203/10000, Prediction Accuracy = 61.75999999999999%, Loss = 0.4742205560207367
Epoch: 4203, Batch Gradient Norm: 10.238108037801652
Epoch: 4203, Batch Gradient Norm after: 10.238108037801652
Epoch 4204/10000, Prediction Accuracy = 61.78000000000001%, Loss = 0.4642595112323761
Epoch: 4204, Batch Gradient Norm: 8.65479805304972
Epoch: 4204, Batch Gradient Norm after: 8.65479805304972
Epoch 4205/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.45379593372344973
Epoch: 4205, Batch Gradient Norm: 9.020321269106459
Epoch: 4205, Batch Gradient Norm after: 9.020321269106459
Epoch 4206/10000, Prediction Accuracy = 61.85%, Loss = 0.4553041160106659
Epoch: 4206, Batch Gradient Norm: 10.844446366246235
Epoch: 4206, Batch Gradient Norm after: 10.844446366246235
Epoch 4207/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4666021645069122
Epoch: 4207, Batch Gradient Norm: 10.752237455472088
Epoch: 4207, Batch Gradient Norm after: 10.752237455472088
Epoch 4208/10000, Prediction Accuracy = 61.848%, Loss = 0.46733332276344297
Epoch: 4208, Batch Gradient Norm: 10.186382173920098
Epoch: 4208, Batch Gradient Norm after: 10.186382173920098
Epoch 4209/10000, Prediction Accuracy = 61.766%, Loss = 0.46461035013198854
Epoch: 4209, Batch Gradient Norm: 9.09153930602543
Epoch: 4209, Batch Gradient Norm after: 9.09153930602543
Epoch 4210/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.458154159784317
Epoch: 4210, Batch Gradient Norm: 10.595314147663835
Epoch: 4210, Batch Gradient Norm after: 10.595314147663835
Epoch 4211/10000, Prediction Accuracy = 61.746%, Loss = 0.4671326816082001
Epoch: 4211, Batch Gradient Norm: 13.201923903827955
Epoch: 4211, Batch Gradient Norm after: 13.201923903827955
Epoch 4212/10000, Prediction Accuracy = 61.724000000000004%, Loss = 0.48493512272834777
Epoch: 4212, Batch Gradient Norm: 11.31025904922323
Epoch: 4212, Batch Gradient Norm after: 11.31025904922323
Epoch 4213/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.47191026210784914
Epoch: 4213, Batch Gradient Norm: 8.425046333553702
Epoch: 4213, Batch Gradient Norm after: 8.425046333553702
Epoch 4214/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4541179001331329
Epoch: 4214, Batch Gradient Norm: 8.15395635515548
Epoch: 4214, Batch Gradient Norm after: 8.15395635515548
Epoch 4215/10000, Prediction Accuracy = 61.791999999999994%, Loss = 0.4533457219600677
Epoch: 4215, Batch Gradient Norm: 8.752772224240674
Epoch: 4215, Batch Gradient Norm after: 8.752772224240674
Epoch 4216/10000, Prediction Accuracy = 61.806%, Loss = 0.45697968602180483
Epoch: 4216, Batch Gradient Norm: 8.17448763089456
Epoch: 4216, Batch Gradient Norm after: 8.17448763089456
Epoch 4217/10000, Prediction Accuracy = 61.85%, Loss = 0.4516942620277405
Epoch: 4217, Batch Gradient Norm: 11.902999040451794
Epoch: 4217, Batch Gradient Norm after: 11.902999040451794
Epoch 4218/10000, Prediction Accuracy = 61.766000000000005%, Loss = 0.4751674234867096
Epoch: 4218, Batch Gradient Norm: 13.31541692827512
Epoch: 4218, Batch Gradient Norm after: 13.31541692827512
Epoch 4219/10000, Prediction Accuracy = 61.769999999999996%, Loss = 0.48978330492973327
Epoch: 4219, Batch Gradient Norm: 9.441294119764814
Epoch: 4219, Batch Gradient Norm after: 9.441294119764814
Epoch 4220/10000, Prediction Accuracy = 61.758%, Loss = 0.4604243874549866
Epoch: 4220, Batch Gradient Norm: 9.20967575240383
Epoch: 4220, Batch Gradient Norm after: 9.20967575240383
Epoch 4221/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.45743510127067566
Epoch: 4221, Batch Gradient Norm: 10.572397560401093
Epoch: 4221, Batch Gradient Norm after: 10.572397560401093
Epoch 4222/10000, Prediction Accuracy = 61.872%, Loss = 0.46504116654396055
Epoch: 4222, Batch Gradient Norm: 10.610182939411303
Epoch: 4222, Batch Gradient Norm after: 10.610182939411303
Epoch 4223/10000, Prediction Accuracy = 61.763999999999996%, Loss = 0.46670771241188047
Epoch: 4223, Batch Gradient Norm: 8.520587859019697
Epoch: 4223, Batch Gradient Norm after: 8.520587859019697
Epoch 4224/10000, Prediction Accuracy = 61.94599999999999%, Loss = 0.45336562395095825
Epoch: 4224, Batch Gradient Norm: 8.890625484862449
Epoch: 4224, Batch Gradient Norm after: 8.890625484862449
Epoch 4225/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.45498072504997256
Epoch: 4225, Batch Gradient Norm: 11.131152012248288
Epoch: 4225, Batch Gradient Norm after: 11.131152012248288
Epoch 4226/10000, Prediction Accuracy = 61.878%, Loss = 0.4685685753822327
Epoch: 4226, Batch Gradient Norm: 13.102838413394228
Epoch: 4226, Batch Gradient Norm after: 13.102838413394228
Epoch 4227/10000, Prediction Accuracy = 61.746%, Loss = 0.4835934638977051
Epoch: 4227, Batch Gradient Norm: 13.4070659093695
Epoch: 4227, Batch Gradient Norm after: 13.4070659093695
Epoch 4228/10000, Prediction Accuracy = 61.762%, Loss = 0.4855155825614929
Epoch: 4228, Batch Gradient Norm: 10.593428092834595
Epoch: 4228, Batch Gradient Norm after: 10.593428092834595
Epoch 4229/10000, Prediction Accuracy = 61.836%, Loss = 0.46506924629211427
Epoch: 4229, Batch Gradient Norm: 7.870204203493955
Epoch: 4229, Batch Gradient Norm after: 7.870204203493955
Epoch 4230/10000, Prediction Accuracy = 61.69200000000001%, Loss = 0.4487061083316803
Epoch: 4230, Batch Gradient Norm: 9.219864772706552
Epoch: 4230, Batch Gradient Norm after: 9.219864772706552
Epoch 4231/10000, Prediction Accuracy = 61.806%, Loss = 0.45649526715278627
Epoch: 4231, Batch Gradient Norm: 9.50824603981333
Epoch: 4231, Batch Gradient Norm after: 9.50824603981333
Epoch 4232/10000, Prediction Accuracy = 61.714%, Loss = 0.46100870370864866
Epoch: 4232, Batch Gradient Norm: 8.190056350729176
Epoch: 4232, Batch Gradient Norm after: 8.190056350729176
Epoch 4233/10000, Prediction Accuracy = 61.79200000000001%, Loss = 0.4532327353954315
Epoch: 4233, Batch Gradient Norm: 6.618125169281551
Epoch: 4233, Batch Gradient Norm after: 6.618125169281551
Epoch 4234/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.44425696730613706
Epoch: 4234, Batch Gradient Norm: 8.431444092125863
Epoch: 4234, Batch Gradient Norm after: 8.431444092125863
Epoch 4235/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.4531234562397003
Epoch: 4235, Batch Gradient Norm: 10.275347081859758
Epoch: 4235, Batch Gradient Norm after: 10.275347081859758
Epoch 4236/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.46422386169433594
Epoch: 4236, Batch Gradient Norm: 12.68454324554222
Epoch: 4236, Batch Gradient Norm after: 12.68454324554222
Epoch 4237/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.48243163228034974
Epoch: 4237, Batch Gradient Norm: 12.23952226134206
Epoch: 4237, Batch Gradient Norm after: 12.23952226134206
Epoch 4238/10000, Prediction Accuracy = 61.644000000000005%, Loss = 0.47950775623321534
Epoch: 4238, Batch Gradient Norm: 9.209835568979432
Epoch: 4238, Batch Gradient Norm after: 9.209835568979432
Epoch 4239/10000, Prediction Accuracy = 61.906000000000006%, Loss = 0.45765286684036255
Epoch: 4239, Batch Gradient Norm: 9.131487006881082
Epoch: 4239, Batch Gradient Norm after: 9.131487006881082
Epoch 4240/10000, Prediction Accuracy = 61.786%, Loss = 0.45612136721611024
Epoch: 4240, Batch Gradient Norm: 10.600132856694387
Epoch: 4240, Batch Gradient Norm after: 10.600132856694387
Epoch 4241/10000, Prediction Accuracy = 61.8%, Loss = 0.4670967996120453
Epoch: 4241, Batch Gradient Norm: 10.442938503133256
Epoch: 4241, Batch Gradient Norm after: 10.442938503133256
Epoch 4242/10000, Prediction Accuracy = 61.738%, Loss = 0.4661152958869934
Epoch: 4242, Batch Gradient Norm: 10.871751288041288
Epoch: 4242, Batch Gradient Norm after: 10.871751288041288
Epoch 4243/10000, Prediction Accuracy = 61.84400000000001%, Loss = 0.46836763620376587
Epoch: 4243, Batch Gradient Norm: 9.9103119273831
Epoch: 4243, Batch Gradient Norm after: 9.9103119273831
Epoch 4244/10000, Prediction Accuracy = 61.836%, Loss = 0.46097872257232664
Epoch: 4244, Batch Gradient Norm: 9.579258800241796
Epoch: 4244, Batch Gradient Norm after: 9.579258800241796
Epoch 4245/10000, Prediction Accuracy = 61.812%, Loss = 0.45812601447105405
Epoch: 4245, Batch Gradient Norm: 9.795550712744072
Epoch: 4245, Batch Gradient Norm after: 9.795550712744072
Epoch 4246/10000, Prediction Accuracy = 61.831999999999994%, Loss = 0.45859084129333494
Epoch: 4246, Batch Gradient Norm: 11.926235780130629
Epoch: 4246, Batch Gradient Norm after: 11.926235780130629
Epoch 4247/10000, Prediction Accuracy = 61.696000000000005%, Loss = 0.4753021001815796
Epoch: 4247, Batch Gradient Norm: 11.869915960701624
Epoch: 4247, Batch Gradient Norm after: 11.869915960701624
Epoch 4248/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.4760719180107117
Epoch: 4248, Batch Gradient Norm: 11.034186391583097
Epoch: 4248, Batch Gradient Norm after: 11.034186391583097
Epoch 4249/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.46834716796875
Epoch: 4249, Batch Gradient Norm: 9.883909857288007
Epoch: 4249, Batch Gradient Norm after: 9.883909857288007
Epoch 4250/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4597280383110046
Epoch: 4250, Batch Gradient Norm: 10.576143075838658
Epoch: 4250, Batch Gradient Norm after: 10.576143075838658
Epoch 4251/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.4656529605388641
Epoch: 4251, Batch Gradient Norm: 8.21873826051022
Epoch: 4251, Batch Gradient Norm after: 8.21873826051022
Epoch 4252/10000, Prediction Accuracy = 61.754%, Loss = 0.45191613435745237
Epoch: 4252, Batch Gradient Norm: 7.240359121089173
Epoch: 4252, Batch Gradient Norm after: 7.240359121089173
Epoch 4253/10000, Prediction Accuracy = 61.838%, Loss = 0.4466216504573822
Epoch: 4253, Batch Gradient Norm: 6.6252625166752255
Epoch: 4253, Batch Gradient Norm after: 6.6252625166752255
Epoch 4254/10000, Prediction Accuracy = 61.824%, Loss = 0.44351781606674195
Epoch: 4254, Batch Gradient Norm: 9.125438229949747
Epoch: 4254, Batch Gradient Norm after: 9.125438229949747
Epoch 4255/10000, Prediction Accuracy = 61.822%, Loss = 0.4569207489490509
Epoch: 4255, Batch Gradient Norm: 11.48614014972628
Epoch: 4255, Batch Gradient Norm after: 11.48614014972628
Epoch 4256/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4719782412052155
Epoch: 4256, Batch Gradient Norm: 13.724943639754649
Epoch: 4256, Batch Gradient Norm after: 13.724943639754649
Epoch 4257/10000, Prediction Accuracy = 61.75%, Loss = 0.48835265040397646
Epoch: 4257, Batch Gradient Norm: 12.31695074814471
Epoch: 4257, Batch Gradient Norm after: 12.31695074814471
Epoch 4258/10000, Prediction Accuracy = 61.786%, Loss = 0.47722305059432985
Epoch: 4258, Batch Gradient Norm: 11.437873352618922
Epoch: 4258, Batch Gradient Norm after: 11.437873352618922
Epoch 4259/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.47215389013290404
Epoch: 4259, Batch Gradient Norm: 10.432897852656357
Epoch: 4259, Batch Gradient Norm after: 10.432897852656357
Epoch 4260/10000, Prediction Accuracy = 61.89%, Loss = 0.46624701023101806
Epoch: 4260, Batch Gradient Norm: 8.416398134604643
Epoch: 4260, Batch Gradient Norm after: 8.416398134604643
Epoch 4261/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4532189130783081
Epoch: 4261, Batch Gradient Norm: 6.632096361282756
Epoch: 4261, Batch Gradient Norm after: 6.632096361282756
Epoch 4262/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4433931171894073
Epoch: 4262, Batch Gradient Norm: 8.172320231552028
Epoch: 4262, Batch Gradient Norm after: 8.172320231552028
Epoch 4263/10000, Prediction Accuracy = 61.706%, Loss = 0.45058125257492065
Epoch: 4263, Batch Gradient Norm: 13.032619804946314
Epoch: 4263, Batch Gradient Norm after: 13.032619804946314
Epoch 4264/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.48206992745399474
Epoch: 4264, Batch Gradient Norm: 14.319421340156994
Epoch: 4264, Batch Gradient Norm after: 14.319421340156994
Epoch 4265/10000, Prediction Accuracy = 61.68399999999999%, Loss = 0.4946074247360229
Epoch: 4265, Batch Gradient Norm: 9.543660624584776
Epoch: 4265, Batch Gradient Norm after: 9.543660624584776
Epoch 4266/10000, Prediction Accuracy = 61.8%, Loss = 0.45842849612236025
Epoch: 4266, Batch Gradient Norm: 7.935830377139616
Epoch: 4266, Batch Gradient Norm after: 7.935830377139616
Epoch 4267/10000, Prediction Accuracy = 61.782%, Loss = 0.4492370128631592
Epoch: 4267, Batch Gradient Norm: 9.009911109331282
Epoch: 4267, Batch Gradient Norm after: 9.009911109331282
Epoch 4268/10000, Prediction Accuracy = 61.822%, Loss = 0.45458985567092897
Epoch: 4268, Batch Gradient Norm: 10.690850979334325
Epoch: 4268, Batch Gradient Norm after: 10.690850979334325
Epoch 4269/10000, Prediction Accuracy = 61.824%, Loss = 0.46679115295410156
Epoch: 4269, Batch Gradient Norm: 9.893423167290718
Epoch: 4269, Batch Gradient Norm after: 9.893423167290718
Epoch 4270/10000, Prediction Accuracy = 61.77%, Loss = 0.45960027575492857
Epoch: 4270, Batch Gradient Norm: 10.590599292812843
Epoch: 4270, Batch Gradient Norm after: 10.590599292812843
Epoch 4271/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.46305023431777953
Epoch: 4271, Batch Gradient Norm: 11.258476639423167
Epoch: 4271, Batch Gradient Norm after: 11.258476639423167
Epoch 4272/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.46816972494125364
Epoch: 4272, Batch Gradient Norm: 9.3384242631021
Epoch: 4272, Batch Gradient Norm after: 9.3384242631021
Epoch 4273/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.45708422660827636
Epoch: 4273, Batch Gradient Norm: 9.491249388636648
Epoch: 4273, Batch Gradient Norm after: 9.491249388636648
Epoch 4274/10000, Prediction Accuracy = 61.74400000000001%, Loss = 0.4590034782886505
Epoch: 4274, Batch Gradient Norm: 9.074318075852323
Epoch: 4274, Batch Gradient Norm after: 9.074318075852323
Epoch 4275/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.45697330236434935
Epoch: 4275, Batch Gradient Norm: 10.734926291175427
Epoch: 4275, Batch Gradient Norm after: 10.734926291175427
Epoch 4276/10000, Prediction Accuracy = 61.86400000000001%, Loss = 0.46629456281661985
Epoch: 4276, Batch Gradient Norm: 11.862450212539288
Epoch: 4276, Batch Gradient Norm after: 11.862450212539288
Epoch 4277/10000, Prediction Accuracy = 61.762%, Loss = 0.47536516189575195
Epoch: 4277, Batch Gradient Norm: 9.74658310336011
Epoch: 4277, Batch Gradient Norm after: 9.74658310336011
Epoch 4278/10000, Prediction Accuracy = 61.932%, Loss = 0.45970035791397096
Epoch: 4278, Batch Gradient Norm: 9.151385162573114
Epoch: 4278, Batch Gradient Norm after: 9.151385162573114
Epoch 4279/10000, Prediction Accuracy = 61.81600000000001%, Loss = 0.4549821257591248
Epoch: 4279, Batch Gradient Norm: 10.513381312582247
Epoch: 4279, Batch Gradient Norm after: 10.513381312582247
Epoch 4280/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.46343199610710145
Epoch: 4280, Batch Gradient Norm: 10.331584695584613
Epoch: 4280, Batch Gradient Norm after: 10.331584695584613
Epoch 4281/10000, Prediction Accuracy = 61.812%, Loss = 0.46380854249000547
Epoch: 4281, Batch Gradient Norm: 8.591366565656452
Epoch: 4281, Batch Gradient Norm after: 8.591366565656452
Epoch 4282/10000, Prediction Accuracy = 61.876%, Loss = 0.4532144606113434
Epoch: 4282, Batch Gradient Norm: 8.543979968145972
Epoch: 4282, Batch Gradient Norm after: 8.543979968145972
Epoch 4283/10000, Prediction Accuracy = 61.826%, Loss = 0.4521112620830536
Epoch: 4283, Batch Gradient Norm: 10.452893576084259
Epoch: 4283, Batch Gradient Norm after: 10.452893576084259
Epoch 4284/10000, Prediction Accuracy = 61.948%, Loss = 0.4638950347900391
Epoch: 4284, Batch Gradient Norm: 10.888302764853108
Epoch: 4284, Batch Gradient Norm after: 10.888302764853108
Epoch 4285/10000, Prediction Accuracy = 61.67%, Loss = 0.46825736165046694
Epoch: 4285, Batch Gradient Norm: 8.800894047813578
Epoch: 4285, Batch Gradient Norm after: 8.800894047813578
Epoch 4286/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4530016601085663
Epoch: 4286, Batch Gradient Norm: 10.245867337875115
Epoch: 4286, Batch Gradient Norm after: 10.245867337875115
Epoch 4287/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4604484856128693
Epoch: 4287, Batch Gradient Norm: 11.544667885353107
Epoch: 4287, Batch Gradient Norm after: 11.544667885353107
Epoch 4288/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.4690294563770294
Epoch: 4288, Batch Gradient Norm: 11.875968980897063
Epoch: 4288, Batch Gradient Norm after: 11.875968980897063
Epoch 4289/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.4727644145488739
Epoch: 4289, Batch Gradient Norm: 11.073831128993254
Epoch: 4289, Batch Gradient Norm after: 11.073831128993254
Epoch 4290/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.46834865808486936
Epoch: 4290, Batch Gradient Norm: 11.114825123749764
Epoch: 4290, Batch Gradient Norm after: 11.114825123749764
Epoch 4291/10000, Prediction Accuracy = 61.872%, Loss = 0.4673240721225739
Epoch: 4291, Batch Gradient Norm: 12.271578519550518
Epoch: 4291, Batch Gradient Norm after: 12.271578519550518
Epoch 4292/10000, Prediction Accuracy = 61.802%, Loss = 0.47714311480522154
Epoch: 4292, Batch Gradient Norm: 10.048907825248778
Epoch: 4292, Batch Gradient Norm after: 10.048907825248778
Epoch 4293/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4624096691608429
Epoch: 4293, Batch Gradient Norm: 8.243292542698365
Epoch: 4293, Batch Gradient Norm after: 8.243292542698365
Epoch 4294/10000, Prediction Accuracy = 61.852%, Loss = 0.45125887989997865
Epoch: 4294, Batch Gradient Norm: 8.494175565866223
Epoch: 4294, Batch Gradient Norm after: 8.494175565866223
Epoch 4295/10000, Prediction Accuracy = 61.714%, Loss = 0.4519651532173157
Epoch: 4295, Batch Gradient Norm: 10.496617822494764
Epoch: 4295, Batch Gradient Norm after: 10.496617822494764
Epoch 4296/10000, Prediction Accuracy = 61.83%, Loss = 0.4640371918678284
Epoch: 4296, Batch Gradient Norm: 10.84670725949405
Epoch: 4296, Batch Gradient Norm after: 10.84670725949405
Epoch 4297/10000, Prediction Accuracy = 61.688%, Loss = 0.4660377502441406
Epoch: 4297, Batch Gradient Norm: 9.71633987121746
Epoch: 4297, Batch Gradient Norm after: 9.71633987121746
Epoch 4298/10000, Prediction Accuracy = 61.907999999999994%, Loss = 0.45844933986663816
Epoch: 4298, Batch Gradient Norm: 9.200524077162074
Epoch: 4298, Batch Gradient Norm after: 9.200524077162074
Epoch 4299/10000, Prediction Accuracy = 61.745999999999995%, Loss = 0.45463950634002687
Epoch: 4299, Batch Gradient Norm: 9.957635625605166
Epoch: 4299, Batch Gradient Norm after: 9.957635625605166
Epoch 4300/10000, Prediction Accuracy = 61.916%, Loss = 0.4592248260974884
Epoch: 4300, Batch Gradient Norm: 11.54906610540595
Epoch: 4300, Batch Gradient Norm after: 11.54906610540595
Epoch 4301/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.47008211016654966
Epoch: 4301, Batch Gradient Norm: 11.781144998294288
Epoch: 4301, Batch Gradient Norm after: 11.781144998294288
Epoch 4302/10000, Prediction Accuracy = 61.834%, Loss = 0.47153195142745974
Epoch: 4302, Batch Gradient Norm: 11.879564305993263
Epoch: 4302, Batch Gradient Norm after: 11.879564305993263
Epoch 4303/10000, Prediction Accuracy = 61.806000000000004%, Loss = 0.47394335865974424
Epoch: 4303, Batch Gradient Norm: 10.071575251008683
Epoch: 4303, Batch Gradient Norm after: 10.071575251008683
Epoch 4304/10000, Prediction Accuracy = 61.83%, Loss = 0.4625731885433197
Epoch: 4304, Batch Gradient Norm: 8.088172365578298
Epoch: 4304, Batch Gradient Norm after: 8.088172365578298
Epoch 4305/10000, Prediction Accuracy = 61.8%, Loss = 0.4500609040260315
Epoch: 4305, Batch Gradient Norm: 8.61309877906201
Epoch: 4305, Batch Gradient Norm after: 8.61309877906201
Epoch 4306/10000, Prediction Accuracy = 61.866%, Loss = 0.4525467395782471
Epoch: 4306, Batch Gradient Norm: 7.690888372907563
Epoch: 4306, Batch Gradient Norm after: 7.690888372907563
Epoch 4307/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.447783625125885
Epoch: 4307, Batch Gradient Norm: 8.614658438252889
Epoch: 4307, Batch Gradient Norm after: 8.614658438252889
Epoch 4308/10000, Prediction Accuracy = 61.79599999999999%, Loss = 0.45221502184867857
Epoch: 4308, Batch Gradient Norm: 10.243034060770587
Epoch: 4308, Batch Gradient Norm after: 10.243034060770587
Epoch 4309/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4601831376552582
Epoch: 4309, Batch Gradient Norm: 13.34617675487774
Epoch: 4309, Batch Gradient Norm after: 13.34617675487774
Epoch 4310/10000, Prediction Accuracy = 61.762%, Loss = 0.48190183043479917
Epoch: 4310, Batch Gradient Norm: 11.477949908919191
Epoch: 4310, Batch Gradient Norm after: 11.477949908919191
Epoch 4311/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.46870931386947634
Epoch: 4311, Batch Gradient Norm: 8.21204926369859
Epoch: 4311, Batch Gradient Norm after: 8.21204926369859
Epoch 4312/10000, Prediction Accuracy = 61.786%, Loss = 0.44938247203826903
Epoch: 4312, Batch Gradient Norm: 7.8243694472152185
Epoch: 4312, Batch Gradient Norm after: 7.8243694472152185
Epoch 4313/10000, Prediction Accuracy = 61.916%, Loss = 0.44706740975379944
Epoch: 4313, Batch Gradient Norm: 10.632017072996376
Epoch: 4313, Batch Gradient Norm after: 10.632017072996376
Epoch 4314/10000, Prediction Accuracy = 61.896%, Loss = 0.46465983986854553
Epoch: 4314, Batch Gradient Norm: 12.726151468132697
Epoch: 4314, Batch Gradient Norm after: 12.726151468132697
Epoch 4315/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.48013978600502016
Epoch: 4315, Batch Gradient Norm: 10.930698510753873
Epoch: 4315, Batch Gradient Norm after: 10.930698510753873
Epoch 4316/10000, Prediction Accuracy = 61.86%, Loss = 0.46741933822631837
Epoch: 4316, Batch Gradient Norm: 9.048600998227682
Epoch: 4316, Batch Gradient Norm after: 9.048600998227682
Epoch 4317/10000, Prediction Accuracy = 61.9%, Loss = 0.45372903943061826
Epoch: 4317, Batch Gradient Norm: 10.37017229800656
Epoch: 4317, Batch Gradient Norm after: 10.37017229800656
Epoch 4318/10000, Prediction Accuracy = 61.89399999999999%, Loss = 0.4617877542972565
Epoch: 4318, Batch Gradient Norm: 11.869211734861706
Epoch: 4318, Batch Gradient Norm after: 11.869211734861706
Epoch 4319/10000, Prediction Accuracy = 61.77%, Loss = 0.47422794699668885
Epoch: 4319, Batch Gradient Norm: 10.49891486413474
Epoch: 4319, Batch Gradient Norm after: 10.49891486413474
Epoch 4320/10000, Prediction Accuracy = 61.794%, Loss = 0.4648761749267578
Epoch: 4320, Batch Gradient Norm: 9.043537217071838
Epoch: 4320, Batch Gradient Norm after: 9.043537217071838
Epoch 4321/10000, Prediction Accuracy = 61.786%, Loss = 0.454611998796463
Epoch: 4321, Batch Gradient Norm: 8.796699898723917
Epoch: 4321, Batch Gradient Norm after: 8.796699898723917
Epoch 4322/10000, Prediction Accuracy = 61.872%, Loss = 0.4522758960723877
Epoch: 4322, Batch Gradient Norm: 9.310545107055962
Epoch: 4322, Batch Gradient Norm after: 9.310545107055962
Epoch 4323/10000, Prediction Accuracy = 61.826%, Loss = 0.4551670074462891
Epoch: 4323, Batch Gradient Norm: 10.599130188390573
Epoch: 4323, Batch Gradient Norm after: 10.599130188390573
Epoch 4324/10000, Prediction Accuracy = 61.79%, Loss = 0.46327308416366575
Epoch: 4324, Batch Gradient Norm: 10.847267717841214
Epoch: 4324, Batch Gradient Norm after: 10.847267717841214
Epoch 4325/10000, Prediction Accuracy = 61.826%, Loss = 0.4645659863948822
Epoch: 4325, Batch Gradient Norm: 11.131888917466808
Epoch: 4325, Batch Gradient Norm after: 11.131888917466808
Epoch 4326/10000, Prediction Accuracy = 61.916%, Loss = 0.4665810465812683
Epoch: 4326, Batch Gradient Norm: 9.51772695577151
Epoch: 4326, Batch Gradient Norm after: 9.51772695577151
Epoch 4327/10000, Prediction Accuracy = 61.884%, Loss = 0.45585893392562865
Epoch: 4327, Batch Gradient Norm: 9.55430971179459
Epoch: 4327, Batch Gradient Norm after: 9.55430971179459
Epoch 4328/10000, Prediction Accuracy = 61.814%, Loss = 0.45758005380630495
Epoch: 4328, Batch Gradient Norm: 8.791268499745941
Epoch: 4328, Batch Gradient Norm after: 8.791268499745941
Epoch 4329/10000, Prediction Accuracy = 61.914%, Loss = 0.45373321771621705
Epoch: 4329, Batch Gradient Norm: 8.554292340085853
Epoch: 4329, Batch Gradient Norm after: 8.554292340085853
Epoch 4330/10000, Prediction Accuracy = 61.75%, Loss = 0.45175233483314514
Epoch: 4330, Batch Gradient Norm: 10.476238258721668
Epoch: 4330, Batch Gradient Norm after: 10.476238258721668
Epoch 4331/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.46404898166656494
Epoch: 4331, Batch Gradient Norm: 10.994216348444988
Epoch: 4331, Batch Gradient Norm after: 10.994216348444988
Epoch 4332/10000, Prediction Accuracy = 61.824%, Loss = 0.46659661531448365
Epoch: 4332, Batch Gradient Norm: 12.094727235757048
Epoch: 4332, Batch Gradient Norm after: 12.094727235757048
Epoch 4333/10000, Prediction Accuracy = 61.842%, Loss = 0.47430831789970396
Epoch: 4333, Batch Gradient Norm: 11.180070983102658
Epoch: 4333, Batch Gradient Norm after: 11.180070983102658
Epoch 4334/10000, Prediction Accuracy = 61.70799999999999%, Loss = 0.4672031939029694
Epoch: 4334, Batch Gradient Norm: 11.028139650846612
Epoch: 4334, Batch Gradient Norm after: 11.028139650846612
Epoch 4335/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.4647876381874084
Epoch: 4335, Batch Gradient Norm: 12.32903869751102
Epoch: 4335, Batch Gradient Norm after: 12.32903869751102
Epoch 4336/10000, Prediction Accuracy = 61.742000000000004%, Loss = 0.47436879873275756
Epoch: 4336, Batch Gradient Norm: 9.319211045614553
Epoch: 4336, Batch Gradient Norm after: 9.319211045614553
Epoch 4337/10000, Prediction Accuracy = 61.862%, Loss = 0.45558375120162964
Epoch: 4337, Batch Gradient Norm: 7.136538010008275
Epoch: 4337, Batch Gradient Norm after: 7.136538010008275
Epoch 4338/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4429110586643219
Epoch: 4338, Batch Gradient Norm: 9.921800425616976
Epoch: 4338, Batch Gradient Norm after: 9.921800425616976
Epoch 4339/10000, Prediction Accuracy = 61.806%, Loss = 0.45746910572052
Epoch: 4339, Batch Gradient Norm: 13.2102270384094
Epoch: 4339, Batch Gradient Norm after: 13.2102270384094
Epoch 4340/10000, Prediction Accuracy = 61.876%, Loss = 0.4826457917690277
Epoch: 4340, Batch Gradient Norm: 11.07834494282952
Epoch: 4340, Batch Gradient Norm after: 11.07834494282952
Epoch 4341/10000, Prediction Accuracy = 61.81%, Loss = 0.46837420463562013
Epoch: 4341, Batch Gradient Norm: 7.768756668027595
Epoch: 4341, Batch Gradient Norm after: 7.768756668027595
Epoch 4342/10000, Prediction Accuracy = 62.007999999999996%, Loss = 0.44644423127174376
Epoch: 4342, Batch Gradient Norm: 7.436415143133432
Epoch: 4342, Batch Gradient Norm after: 7.436415143133432
Epoch 4343/10000, Prediction Accuracy = 61.775999999999996%, Loss = 0.4441400647163391
Epoch: 4343, Batch Gradient Norm: 9.657032917866948
Epoch: 4343, Batch Gradient Norm after: 9.657032917866948
Epoch 4344/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.45601433515548706
Epoch: 4344, Batch Gradient Norm: 11.034365721150323
Epoch: 4344, Batch Gradient Norm after: 11.034365721150323
Epoch 4345/10000, Prediction Accuracy = 61.80800000000001%, Loss = 0.4660644710063934
Epoch: 4345, Batch Gradient Norm: 10.91403426373121
Epoch: 4345, Batch Gradient Norm after: 10.91403426373121
Epoch 4346/10000, Prediction Accuracy = 61.872%, Loss = 0.46519407629966736
Epoch: 4346, Batch Gradient Norm: 9.72226197776767
Epoch: 4346, Batch Gradient Norm after: 9.72226197776767
Epoch 4347/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.4561160087585449
Epoch: 4347, Batch Gradient Norm: 10.452883414842493
Epoch: 4347, Batch Gradient Norm after: 10.452883414842493
Epoch 4348/10000, Prediction Accuracy = 61.798%, Loss = 0.4602594912052155
Epoch: 4348, Batch Gradient Norm: 9.941376331858411
Epoch: 4348, Batch Gradient Norm after: 9.941376331858411
Epoch 4349/10000, Prediction Accuracy = 61.9%, Loss = 0.4569395363330841
Epoch: 4349, Batch Gradient Norm: 9.007130204547304
Epoch: 4349, Batch Gradient Norm after: 9.007130204547304
Epoch 4350/10000, Prediction Accuracy = 61.838%, Loss = 0.45187504291534425
Epoch: 4350, Batch Gradient Norm: 9.071837768926713
Epoch: 4350, Batch Gradient Norm after: 9.071837768926713
Epoch 4351/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.45283514857292173
Epoch: 4351, Batch Gradient Norm: 11.555012026399478
Epoch: 4351, Batch Gradient Norm after: 11.555012026399478
Epoch 4352/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4711281418800354
Epoch: 4352, Batch Gradient Norm: 11.8802466144423
Epoch: 4352, Batch Gradient Norm after: 11.8802466144423
Epoch 4353/10000, Prediction Accuracy = 61.852%, Loss = 0.4759700357913971
Epoch: 4353, Batch Gradient Norm: 9.340836936492083
Epoch: 4353, Batch Gradient Norm after: 9.340836936492083
Epoch 4354/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4558770298957825
Epoch: 4354, Batch Gradient Norm: 9.05246694499711
Epoch: 4354, Batch Gradient Norm after: 9.05246694499711
Epoch 4355/10000, Prediction Accuracy = 61.916%, Loss = 0.4526887834072113
Epoch: 4355, Batch Gradient Norm: 9.81246344497985
Epoch: 4355, Batch Gradient Norm after: 9.81246344497985
Epoch 4356/10000, Prediction Accuracy = 61.858000000000004%, Loss = 0.4578656733036041
Epoch: 4356, Batch Gradient Norm: 9.154761960115769
Epoch: 4356, Batch Gradient Norm after: 9.154761960115769
Epoch 4357/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.45294237732887266
Epoch: 4357, Batch Gradient Norm: 9.899906044104739
Epoch: 4357, Batch Gradient Norm after: 9.899906044104739
Epoch 4358/10000, Prediction Accuracy = 61.778%, Loss = 0.45769243240356444
Epoch: 4358, Batch Gradient Norm: 11.006735044738338
Epoch: 4358, Batch Gradient Norm after: 11.006735044738338
Epoch 4359/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.4682395815849304
Epoch: 4359, Batch Gradient Norm: 9.346691641924744
Epoch: 4359, Batch Gradient Norm after: 9.346691641924744
Epoch 4360/10000, Prediction Accuracy = 61.83%, Loss = 0.4559764266014099
Epoch: 4360, Batch Gradient Norm: 12.23956455130928
Epoch: 4360, Batch Gradient Norm after: 12.23956455130928
Epoch 4361/10000, Prediction Accuracy = 61.784000000000006%, Loss = 0.47437787652015684
Epoch: 4361, Batch Gradient Norm: 13.077544390539268
Epoch: 4361, Batch Gradient Norm after: 13.077544390539268
Epoch 4362/10000, Prediction Accuracy = 61.86%, Loss = 0.48224450945854186
Epoch: 4362, Batch Gradient Norm: 11.206176447449506
Epoch: 4362, Batch Gradient Norm after: 11.206176447449506
Epoch 4363/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.4680888712406158
Epoch: 4363, Batch Gradient Norm: 9.466421798761676
Epoch: 4363, Batch Gradient Norm after: 9.466421798761676
Epoch 4364/10000, Prediction Accuracy = 61.826%, Loss = 0.4555434763431549
Epoch: 4364, Batch Gradient Norm: 8.725396145301826
Epoch: 4364, Batch Gradient Norm after: 8.725396145301826
Epoch 4365/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.44945854544639585
Epoch: 4365, Batch Gradient Norm: 9.086839670315923
Epoch: 4365, Batch Gradient Norm after: 9.086839670315923
Epoch 4366/10000, Prediction Accuracy = 61.8%, Loss = 0.45081332325935364
Epoch: 4366, Batch Gradient Norm: 9.837461272107713
Epoch: 4366, Batch Gradient Norm after: 9.837461272107713
Epoch 4367/10000, Prediction Accuracy = 61.98%, Loss = 0.45576181411743166
Epoch: 4367, Batch Gradient Norm: 8.959173333767994
Epoch: 4367, Batch Gradient Norm after: 8.959173333767994
Epoch 4368/10000, Prediction Accuracy = 61.762%, Loss = 0.451511538028717
Epoch: 4368, Batch Gradient Norm: 9.850309482311006
Epoch: 4368, Batch Gradient Norm after: 9.850309482311006
Epoch 4369/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.45760637521743774
Epoch: 4369, Batch Gradient Norm: 12.926947785016843
Epoch: 4369, Batch Gradient Norm after: 12.926947785016843
Epoch 4370/10000, Prediction Accuracy = 61.838%, Loss = 0.4808899760246277
Epoch: 4370, Batch Gradient Norm: 11.62064421127438
Epoch: 4370, Batch Gradient Norm after: 11.62064421127438
Epoch 4371/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.471109938621521
Epoch: 4371, Batch Gradient Norm: 9.561622074274812
Epoch: 4371, Batch Gradient Norm after: 9.561622074274812
Epoch 4372/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.4544372260570526
Epoch: 4372, Batch Gradient Norm: 11.670653005923022
Epoch: 4372, Batch Gradient Norm after: 11.670653005923022
Epoch 4373/10000, Prediction Accuracy = 61.826%, Loss = 0.4671895444393158
Epoch: 4373, Batch Gradient Norm: 11.106052657637377
Epoch: 4373, Batch Gradient Norm after: 11.106052657637377
Epoch 4374/10000, Prediction Accuracy = 61.956%, Loss = 0.4642216324806213
Epoch: 4374, Batch Gradient Norm: 8.989211103025818
Epoch: 4374, Batch Gradient Norm after: 8.989211103025818
Epoch 4375/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.4514120101928711
Epoch: 4375, Batch Gradient Norm: 7.994500470270335
Epoch: 4375, Batch Gradient Norm after: 7.994500470270335
Epoch 4376/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.44692412614822385
Epoch: 4376, Batch Gradient Norm: 9.342462378550561
Epoch: 4376, Batch Gradient Norm after: 9.342462378550561
Epoch 4377/10000, Prediction Accuracy = 61.872%, Loss = 0.455730265378952
Epoch: 4377, Batch Gradient Norm: 8.620816862325354
Epoch: 4377, Batch Gradient Norm after: 8.620816862325354
Epoch 4378/10000, Prediction Accuracy = 61.83%, Loss = 0.45090346336364745
Epoch: 4378, Batch Gradient Norm: 9.370703285778085
Epoch: 4378, Batch Gradient Norm after: 9.370703285778085
Epoch 4379/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.45372960567474363
Epoch: 4379, Batch Gradient Norm: 9.562568212438988
Epoch: 4379, Batch Gradient Norm after: 9.562568212438988
Epoch 4380/10000, Prediction Accuracy = 61.842000000000006%, Loss = 0.45412129163742065
Epoch: 4380, Batch Gradient Norm: 9.607668635139534
Epoch: 4380, Batch Gradient Norm after: 9.607668635139534
Epoch 4381/10000, Prediction Accuracy = 61.864%, Loss = 0.45413753390312195
Epoch: 4381, Batch Gradient Norm: 11.04248577632989
Epoch: 4381, Batch Gradient Norm after: 11.04248577632989
Epoch 4382/10000, Prediction Accuracy = 61.896%, Loss = 0.4647479116916656
Epoch: 4382, Batch Gradient Norm: 11.011481042130614
Epoch: 4382, Batch Gradient Norm after: 11.011481042130614
Epoch 4383/10000, Prediction Accuracy = 61.806%, Loss = 0.46785309314727785
Epoch: 4383, Batch Gradient Norm: 9.789948166926608
Epoch: 4383, Batch Gradient Norm after: 9.789948166926608
Epoch 4384/10000, Prediction Accuracy = 61.824%, Loss = 0.45723903775215147
Epoch: 4384, Batch Gradient Norm: 11.491879326472773
Epoch: 4384, Batch Gradient Norm after: 11.491879326472773
Epoch 4385/10000, Prediction Accuracy = 61.908%, Loss = 0.46803371906280516
Epoch: 4385, Batch Gradient Norm: 10.459639773580848
Epoch: 4385, Batch Gradient Norm after: 10.459639773580848
Epoch 4386/10000, Prediction Accuracy = 61.76800000000001%, Loss = 0.46041340827941896
Epoch: 4386, Batch Gradient Norm: 10.284801756287344
Epoch: 4386, Batch Gradient Norm after: 10.284801756287344
Epoch 4387/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.45874568819999695
Epoch: 4387, Batch Gradient Norm: 10.958905515359168
Epoch: 4387, Batch Gradient Norm after: 10.958905515359168
Epoch 4388/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.46307114958763124
Epoch: 4388, Batch Gradient Norm: 11.00940522816147
Epoch: 4388, Batch Gradient Norm after: 11.00940522816147
Epoch 4389/10000, Prediction Accuracy = 61.81999999999999%, Loss = 0.464118218421936
Epoch: 4389, Batch Gradient Norm: 11.156417027962089
Epoch: 4389, Batch Gradient Norm after: 11.156417027962089
Epoch 4390/10000, Prediction Accuracy = 61.83200000000001%, Loss = 0.4663377821445465
Epoch: 4390, Batch Gradient Norm: 8.906348987238813
Epoch: 4390, Batch Gradient Norm after: 8.906348987238813
Epoch 4391/10000, Prediction Accuracy = 61.818000000000005%, Loss = 0.4515078902244568
Epoch: 4391, Batch Gradient Norm: 8.522008886913568
Epoch: 4391, Batch Gradient Norm after: 8.522008886913568
Epoch 4392/10000, Prediction Accuracy = 61.866%, Loss = 0.45009632110595704
Epoch: 4392, Batch Gradient Norm: 8.274050804667803
Epoch: 4392, Batch Gradient Norm after: 8.274050804667803
Epoch 4393/10000, Prediction Accuracy = 61.886%, Loss = 0.4481538414955139
Epoch: 4393, Batch Gradient Norm: 11.140714362354982
Epoch: 4393, Batch Gradient Norm after: 11.140714362354982
Epoch 4394/10000, Prediction Accuracy = 61.824%, Loss = 0.46721103191375735
Epoch: 4394, Batch Gradient Norm: 12.801116831456747
Epoch: 4394, Batch Gradient Norm after: 12.801116831456747
Epoch 4395/10000, Prediction Accuracy = 61.79%, Loss = 0.4792149603366852
Epoch: 4395, Batch Gradient Norm: 10.954109180997152
Epoch: 4395, Batch Gradient Norm after: 10.954109180997152
Epoch 4396/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4642477810382843
Epoch: 4396, Batch Gradient Norm: 9.845550591887662
Epoch: 4396, Batch Gradient Norm after: 9.845550591887662
Epoch 4397/10000, Prediction Accuracy = 61.803999999999995%, Loss = 0.456910514831543
Epoch: 4397, Batch Gradient Norm: 9.414893643307865
Epoch: 4397, Batch Gradient Norm after: 9.414893643307865
Epoch 4398/10000, Prediction Accuracy = 62.02%, Loss = 0.4551431119441986
Epoch: 4398, Batch Gradient Norm: 7.861921018297815
Epoch: 4398, Batch Gradient Norm after: 7.861921018297815
Epoch 4399/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.44538065791130066
Epoch: 4399, Batch Gradient Norm: 9.790946709392756
Epoch: 4399, Batch Gradient Norm after: 9.790946709392756
Epoch 4400/10000, Prediction Accuracy = 61.956%, Loss = 0.4542010486125946
Epoch: 4400, Batch Gradient Norm: 15.081542750023333
Epoch: 4400, Batch Gradient Norm after: 15.081542750023333
Epoch 4401/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.49546640515327456
Epoch: 4401, Batch Gradient Norm: 12.35439795848215
Epoch: 4401, Batch Gradient Norm after: 12.35439795848215
Epoch 4402/10000, Prediction Accuracy = 61.82399999999999%, Loss = 0.4741848766803741
Epoch: 4402, Batch Gradient Norm: 8.119504893043853
Epoch: 4402, Batch Gradient Norm after: 8.119504893043853
Epoch 4403/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.44670437574386596
Epoch: 4403, Batch Gradient Norm: 6.062316509166032
Epoch: 4403, Batch Gradient Norm after: 6.062316509166032
Epoch 4404/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4371468544006348
Epoch: 4404, Batch Gradient Norm: 7.703699330241771
Epoch: 4404, Batch Gradient Norm after: 7.703699330241771
Epoch 4405/10000, Prediction Accuracy = 61.924%, Loss = 0.444462788105011
Epoch: 4405, Batch Gradient Norm: 8.406791752912456
Epoch: 4405, Batch Gradient Norm after: 8.406791752912456
Epoch 4406/10000, Prediction Accuracy = 61.812%, Loss = 0.44925892949104307
Epoch: 4406, Batch Gradient Norm: 9.189605613092912
Epoch: 4406, Batch Gradient Norm after: 9.189605613092912
Epoch 4407/10000, Prediction Accuracy = 61.898%, Loss = 0.45382593274116517
Epoch: 4407, Batch Gradient Norm: 11.008284487308169
Epoch: 4407, Batch Gradient Norm after: 11.008284487308169
Epoch 4408/10000, Prediction Accuracy = 61.968%, Loss = 0.46353790163993835
Epoch: 4408, Batch Gradient Norm: 12.674489695322318
Epoch: 4408, Batch Gradient Norm after: 12.674489695322318
Epoch 4409/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.4764217734336853
Epoch: 4409, Batch Gradient Norm: 9.74742081627599
Epoch: 4409, Batch Gradient Norm after: 9.74742081627599
Epoch 4410/10000, Prediction Accuracy = 61.898%, Loss = 0.45546312928199767
Epoch: 4410, Batch Gradient Norm: 9.228863139676148
Epoch: 4410, Batch Gradient Norm after: 9.228863139676148
Epoch 4411/10000, Prediction Accuracy = 61.754000000000005%, Loss = 0.45081874132156374
Epoch: 4411, Batch Gradient Norm: 11.31313729409138
Epoch: 4411, Batch Gradient Norm after: 11.31313729409138
Epoch 4412/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4637801945209503
Epoch: 4412, Batch Gradient Norm: 12.009110078329899
Epoch: 4412, Batch Gradient Norm after: 12.009110078329899
Epoch 4413/10000, Prediction Accuracy = 61.739999999999995%, Loss = 0.4712249875068665
Epoch: 4413, Batch Gradient Norm: 10.231774801708937
Epoch: 4413, Batch Gradient Norm after: 10.231774801708937
Epoch 4414/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.45897778272628786
Epoch: 4414, Batch Gradient Norm: 9.589480349685617
Epoch: 4414, Batch Gradient Norm after: 9.589480349685617
Epoch 4415/10000, Prediction Accuracy = 61.834%, Loss = 0.45465717315673826
Epoch: 4415, Batch Gradient Norm: 9.215369193059878
Epoch: 4415, Batch Gradient Norm after: 9.215369193059878
Epoch 4416/10000, Prediction Accuracy = 61.874%, Loss = 0.4515670418739319
Epoch: 4416, Batch Gradient Norm: 9.039686356647497
Epoch: 4416, Batch Gradient Norm after: 9.039686356647497
Epoch 4417/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4501779794692993
Epoch: 4417, Batch Gradient Norm: 9.656314255376575
Epoch: 4417, Batch Gradient Norm after: 9.656314255376575
Epoch 4418/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.45304563641548157
Epoch: 4418, Batch Gradient Norm: 11.272259036828357
Epoch: 4418, Batch Gradient Norm after: 11.272259036828357
Epoch 4419/10000, Prediction Accuracy = 61.798%, Loss = 0.4639748275279999
Epoch: 4419, Batch Gradient Norm: 12.273251655660788
Epoch: 4419, Batch Gradient Norm after: 12.273251655660788
Epoch 4420/10000, Prediction Accuracy = 61.908%, Loss = 0.47260614633560183
Epoch: 4420, Batch Gradient Norm: 11.44116925511413
Epoch: 4420, Batch Gradient Norm after: 11.44116925511413
Epoch 4421/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.4668527185916901
Epoch: 4421, Batch Gradient Norm: 11.788407885296277
Epoch: 4421, Batch Gradient Norm after: 11.788407885296277
Epoch 4422/10000, Prediction Accuracy = 61.86%, Loss = 0.4725687146186829
Epoch: 4422, Batch Gradient Norm: 8.896861985041701
Epoch: 4422, Batch Gradient Norm after: 8.896861985041701
Epoch 4423/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.45187082290649416
Epoch: 4423, Batch Gradient Norm: 8.479299184469857
Epoch: 4423, Batch Gradient Norm after: 8.479299184469857
Epoch 4424/10000, Prediction Accuracy = 61.872%, Loss = 0.4468664526939392
Epoch: 4424, Batch Gradient Norm: 9.914721471016978
Epoch: 4424, Batch Gradient Norm after: 9.914721471016978
Epoch 4425/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4553772985935211
Epoch: 4425, Batch Gradient Norm: 9.947395244230265
Epoch: 4425, Batch Gradient Norm after: 9.947395244230265
Epoch 4426/10000, Prediction Accuracy = 61.86800000000001%, Loss = 0.4563372075557709
Epoch: 4426, Batch Gradient Norm: 9.145091643217574
Epoch: 4426, Batch Gradient Norm after: 9.145091643217574
Epoch 4427/10000, Prediction Accuracy = 61.778%, Loss = 0.45155231952667235
Epoch: 4427, Batch Gradient Norm: 11.630989072364677
Epoch: 4427, Batch Gradient Norm after: 11.630989072364677
Epoch 4428/10000, Prediction Accuracy = 61.85%, Loss = 0.47104601860046386
Epoch: 4428, Batch Gradient Norm: 9.138266072063924
Epoch: 4428, Batch Gradient Norm after: 9.138266072063924
Epoch 4429/10000, Prediction Accuracy = 61.83399999999999%, Loss = 0.4530505836009979
Epoch: 4429, Batch Gradient Norm: 9.42150990440326
Epoch: 4429, Batch Gradient Norm after: 9.42150990440326
Epoch 4430/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.4529917359352112
Epoch: 4430, Batch Gradient Norm: 12.057719084284296
Epoch: 4430, Batch Gradient Norm after: 12.057719084284296
Epoch 4431/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.47078744769096376
Epoch: 4431, Batch Gradient Norm: 11.667171944230951
Epoch: 4431, Batch Gradient Norm after: 11.667171944230951
Epoch 4432/10000, Prediction Accuracy = 61.822%, Loss = 0.47075341939926146
Epoch: 4432, Batch Gradient Norm: 7.752614945419952
Epoch: 4432, Batch Gradient Norm after: 7.752614945419952
Epoch 4433/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.44400368332862855
Epoch: 4433, Batch Gradient Norm: 7.004918477263046
Epoch: 4433, Batch Gradient Norm after: 7.004918477263046
Epoch 4434/10000, Prediction Accuracy = 61.848%, Loss = 0.43956091403961184
Epoch: 4434, Batch Gradient Norm: 8.207242185750449
Epoch: 4434, Batch Gradient Norm after: 8.207242185750449
Epoch 4435/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4447904944419861
Epoch: 4435, Batch Gradient Norm: 10.507339855703982
Epoch: 4435, Batch Gradient Norm after: 10.507339855703982
Epoch 4436/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.4583771347999573
Epoch: 4436, Batch Gradient Norm: 11.327894247545796
Epoch: 4436, Batch Gradient Norm after: 11.327894247545796
Epoch 4437/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.46472089290618895
Epoch: 4437, Batch Gradient Norm: 10.90005151075977
Epoch: 4437, Batch Gradient Norm after: 10.90005151075977
Epoch 4438/10000, Prediction Accuracy = 61.82000000000001%, Loss = 0.4632045328617096
Epoch: 4438, Batch Gradient Norm: 8.705157060753749
Epoch: 4438, Batch Gradient Norm after: 8.705157060753749
Epoch 4439/10000, Prediction Accuracy = 61.872%, Loss = 0.4495736300945282
Epoch: 4439, Batch Gradient Norm: 9.377530252869096
Epoch: 4439, Batch Gradient Norm after: 9.377530252869096
Epoch 4440/10000, Prediction Accuracy = 61.790000000000006%, Loss = 0.45338037610054016
Epoch: 4440, Batch Gradient Norm: 11.111365410759865
Epoch: 4440, Batch Gradient Norm after: 11.111365410759865
Epoch 4441/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.4652693808078766
Epoch: 4441, Batch Gradient Norm: 13.195558348260954
Epoch: 4441, Batch Gradient Norm after: 13.195558348260954
Epoch 4442/10000, Prediction Accuracy = 61.727999999999994%, Loss = 0.48082364797592164
Epoch: 4442, Batch Gradient Norm: 11.848861623007808
Epoch: 4442, Batch Gradient Norm after: 11.848861623007808
Epoch 4443/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.46917749643325807
Epoch: 4443, Batch Gradient Norm: 9.226072045293398
Epoch: 4443, Batch Gradient Norm after: 9.226072045293398
Epoch 4444/10000, Prediction Accuracy = 61.94200000000001%, Loss = 0.4504931092262268
Epoch: 4444, Batch Gradient Norm: 10.423257378376366
Epoch: 4444, Batch Gradient Norm after: 10.423257378376366
Epoch 4445/10000, Prediction Accuracy = 61.786%, Loss = 0.4574968457221985
Epoch: 4445, Batch Gradient Norm: 10.822235830247415
Epoch: 4445, Batch Gradient Norm after: 10.822235830247415
Epoch 4446/10000, Prediction Accuracy = 61.976%, Loss = 0.46134200096130373
Epoch: 4446, Batch Gradient Norm: 8.582630304091195
Epoch: 4446, Batch Gradient Norm after: 8.582630304091195
Epoch 4447/10000, Prediction Accuracy = 61.80799999999999%, Loss = 0.44742779731750487
Epoch: 4447, Batch Gradient Norm: 8.730110295098228
Epoch: 4447, Batch Gradient Norm after: 8.730110295098228
Epoch 4448/10000, Prediction Accuracy = 61.96%, Loss = 0.4489596664905548
Epoch: 4448, Batch Gradient Norm: 9.533158656044524
Epoch: 4448, Batch Gradient Norm after: 9.533158656044524
Epoch 4449/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.45380319356918336
Epoch: 4449, Batch Gradient Norm: 10.991393211150513
Epoch: 4449, Batch Gradient Norm after: 10.991393211150513
Epoch 4450/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4644827902317047
Epoch: 4450, Batch Gradient Norm: 11.136554663069974
Epoch: 4450, Batch Gradient Norm after: 11.136554663069974
Epoch 4451/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.46484464406967163
Epoch: 4451, Batch Gradient Norm: 9.750604653398717
Epoch: 4451, Batch Gradient Norm after: 9.750604653398717
Epoch 4452/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.45504417419433596
Epoch: 4452, Batch Gradient Norm: 9.633878004499318
Epoch: 4452, Batch Gradient Norm after: 9.633878004499318
Epoch 4453/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.45408430099487307
Epoch: 4453, Batch Gradient Norm: 10.819353450712068
Epoch: 4453, Batch Gradient Norm after: 10.819353450712068
Epoch 4454/10000, Prediction Accuracy = 61.976%, Loss = 0.4608392298221588
Epoch: 4454, Batch Gradient Norm: 11.942714246786645
Epoch: 4454, Batch Gradient Norm after: 11.942714246786645
Epoch 4455/10000, Prediction Accuracy = 61.815999999999995%, Loss = 0.4680195927619934
Epoch: 4455, Batch Gradient Norm: 10.550708766703414
Epoch: 4455, Batch Gradient Norm after: 10.550708766703414
Epoch 4456/10000, Prediction Accuracy = 62.028%, Loss = 0.4585390567779541
Epoch: 4456, Batch Gradient Norm: 10.388185106579686
Epoch: 4456, Batch Gradient Norm after: 10.388185106579686
Epoch 4457/10000, Prediction Accuracy = 61.884%, Loss = 0.45877367854118345
Epoch: 4457, Batch Gradient Norm: 9.63569818669912
Epoch: 4457, Batch Gradient Norm after: 9.63569818669912
Epoch 4458/10000, Prediction Accuracy = 61.888%, Loss = 0.45623143315315245
Epoch: 4458, Batch Gradient Norm: 8.180638740845026
Epoch: 4458, Batch Gradient Norm after: 8.180638740845026
Epoch 4459/10000, Prediction Accuracy = 61.874%, Loss = 0.4465128660202026
Epoch: 4459, Batch Gradient Norm: 7.811974958076358
Epoch: 4459, Batch Gradient Norm after: 7.811974958076358
Epoch 4460/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.4425572633743286
Epoch: 4460, Batch Gradient Norm: 11.066667052218605
Epoch: 4460, Batch Gradient Norm after: 11.066667052218605
Epoch 4461/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4609024167060852
Epoch: 4461, Batch Gradient Norm: 12.022750177030149
Epoch: 4461, Batch Gradient Norm after: 12.022750177030149
Epoch 4462/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4693951189517975
Epoch: 4462, Batch Gradient Norm: 9.922686127481914
Epoch: 4462, Batch Gradient Norm after: 9.922686127481914
Epoch 4463/10000, Prediction Accuracy = 61.970000000000006%, Loss = 0.45640540719032285
Epoch: 4463, Batch Gradient Norm: 8.321474496056405
Epoch: 4463, Batch Gradient Norm after: 8.321474496056405
Epoch 4464/10000, Prediction Accuracy = 61.896%, Loss = 0.4463371455669403
Epoch: 4464, Batch Gradient Norm: 11.530996551723696
Epoch: 4464, Batch Gradient Norm after: 11.530996551723696
Epoch 4465/10000, Prediction Accuracy = 61.86999999999999%, Loss = 0.465471088886261
Epoch: 4465, Batch Gradient Norm: 12.93195184083867
Epoch: 4465, Batch Gradient Norm after: 12.93195184083867
Epoch 4466/10000, Prediction Accuracy = 61.95%, Loss = 0.4761332094669342
Epoch: 4466, Batch Gradient Norm: 10.305953263979127
Epoch: 4466, Batch Gradient Norm after: 10.305953263979127
Epoch 4467/10000, Prediction Accuracy = 61.855999999999995%, Loss = 0.4563972890377045
Epoch: 4467, Batch Gradient Norm: 9.401826865283866
Epoch: 4467, Batch Gradient Norm after: 9.401826865283866
Epoch 4468/10000, Prediction Accuracy = 61.872%, Loss = 0.4511125206947327
Epoch: 4468, Batch Gradient Norm: 10.30513100626157
Epoch: 4468, Batch Gradient Norm after: 10.30513100626157
Epoch 4469/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4581171214580536
Epoch: 4469, Batch Gradient Norm: 10.151497213326259
Epoch: 4469, Batch Gradient Norm after: 10.151497213326259
Epoch 4470/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.4575955092906952
Epoch: 4470, Batch Gradient Norm: 9.05790650999645
Epoch: 4470, Batch Gradient Norm after: 9.05790650999645
Epoch 4471/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.4499518036842346
Epoch: 4471, Batch Gradient Norm: 8.893418376262947
Epoch: 4471, Batch Gradient Norm after: 8.893418376262947
Epoch 4472/10000, Prediction Accuracy = 61.872%, Loss = 0.44880095720291135
Epoch: 4472, Batch Gradient Norm: 9.94245882358466
Epoch: 4472, Batch Gradient Norm after: 9.94245882358466
Epoch 4473/10000, Prediction Accuracy = 61.867999999999995%, Loss = 0.4567662954330444
Epoch: 4473, Batch Gradient Norm: 10.037629244100556
Epoch: 4473, Batch Gradient Norm after: 10.037629244100556
Epoch 4474/10000, Prediction Accuracy = 61.902%, Loss = 0.4575133204460144
Epoch: 4474, Batch Gradient Norm: 10.301073300074881
Epoch: 4474, Batch Gradient Norm after: 10.301073300074881
Epoch 4475/10000, Prediction Accuracy = 61.77%, Loss = 0.4587910771369934
Epoch: 4475, Batch Gradient Norm: 9.685934746583385
Epoch: 4475, Batch Gradient Norm after: 9.685934746583385
Epoch 4476/10000, Prediction Accuracy = 61.94%, Loss = 0.45372927784919737
Epoch: 4476, Batch Gradient Norm: 9.706591976529237
Epoch: 4476, Batch Gradient Norm after: 9.706591976529237
Epoch 4477/10000, Prediction Accuracy = 61.86%, Loss = 0.45358641147613527
Epoch: 4477, Batch Gradient Norm: 9.604709293462669
Epoch: 4477, Batch Gradient Norm after: 9.604709293462669
Epoch 4478/10000, Prediction Accuracy = 61.888%, Loss = 0.4528126299381256
Epoch: 4478, Batch Gradient Norm: 9.70598731834044
Epoch: 4478, Batch Gradient Norm after: 9.70598731834044
Epoch 4479/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4528256952762604
Epoch: 4479, Batch Gradient Norm: 10.911754089666214
Epoch: 4479, Batch Gradient Norm after: 10.911754089666214
Epoch 4480/10000, Prediction Accuracy = 61.866%, Loss = 0.46100001931190493
Epoch: 4480, Batch Gradient Norm: 10.47879758602387
Epoch: 4480, Batch Gradient Norm after: 10.47879758602387
Epoch 4481/10000, Prediction Accuracy = 61.836%, Loss = 0.4579400420188904
Epoch: 4481, Batch Gradient Norm: 10.234148147996489
Epoch: 4481, Batch Gradient Norm after: 10.234148147996489
Epoch 4482/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.4558406412601471
Epoch: 4482, Batch Gradient Norm: 11.55013008056647
Epoch: 4482, Batch Gradient Norm after: 11.55013008056647
Epoch 4483/10000, Prediction Accuracy = 61.760000000000005%, Loss = 0.4638278603553772
Epoch: 4483, Batch Gradient Norm: 11.530836303025579
Epoch: 4483, Batch Gradient Norm after: 11.530836303025579
Epoch 4484/10000, Prediction Accuracy = 61.95%, Loss = 0.4646428644657135
Epoch: 4484, Batch Gradient Norm: 11.26661950235906
Epoch: 4484, Batch Gradient Norm after: 11.26661950235906
Epoch 4485/10000, Prediction Accuracy = 61.83%, Loss = 0.4640301585197449
Epoch: 4485, Batch Gradient Norm: 10.104200113193517
Epoch: 4485, Batch Gradient Norm after: 10.104200113193517
Epoch 4486/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.45989397168159485
Epoch: 4486, Batch Gradient Norm: 6.861003449564384
Epoch: 4486, Batch Gradient Norm after: 6.861003449564384
Epoch 4487/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.4397857189178467
Epoch: 4487, Batch Gradient Norm: 6.149392133196478
Epoch: 4487, Batch Gradient Norm after: 6.149392133196478
Epoch 4488/10000, Prediction Accuracy = 61.928%, Loss = 0.43516746163368225
Epoch: 4488, Batch Gradient Norm: 9.496195785250691
Epoch: 4488, Batch Gradient Norm after: 9.496195785250691
Epoch 4489/10000, Prediction Accuracy = 61.782%, Loss = 0.4518181920051575
Epoch: 4489, Batch Gradient Norm: 13.834967104710415
Epoch: 4489, Batch Gradient Norm after: 13.834967104710415
Epoch 4490/10000, Prediction Accuracy = 61.766000000000005%, Loss = 0.4850978910923004
Epoch: 4490, Batch Gradient Norm: 11.657396332382277
Epoch: 4490, Batch Gradient Norm after: 11.657396332382277
Epoch 4491/10000, Prediction Accuracy = 61.96%, Loss = 0.4663821876049042
Epoch: 4491, Batch Gradient Norm: 10.765059327760888
Epoch: 4491, Batch Gradient Norm after: 10.765059327760888
Epoch 4492/10000, Prediction Accuracy = 61.914%, Loss = 0.4587335646152496
Epoch: 4492, Batch Gradient Norm: 11.761175908923754
Epoch: 4492, Batch Gradient Norm after: 11.761175908923754
Epoch 4493/10000, Prediction Accuracy = 61.916%, Loss = 0.4671430289745331
Epoch: 4493, Batch Gradient Norm: 9.138012011626794
Epoch: 4493, Batch Gradient Norm after: 9.138012011626794
Epoch 4494/10000, Prediction Accuracy = 61.926%, Loss = 0.45008463263511655
Epoch: 4494, Batch Gradient Norm: 8.586304184588668
Epoch: 4494, Batch Gradient Norm after: 8.586304184588668
Epoch 4495/10000, Prediction Accuracy = 61.888%, Loss = 0.4467354118824005
Epoch: 4495, Batch Gradient Norm: 8.325595418947742
Epoch: 4495, Batch Gradient Norm after: 8.325595418947742
Epoch 4496/10000, Prediction Accuracy = 61.926%, Loss = 0.44513611793518065
Epoch: 4496, Batch Gradient Norm: 9.94014198177381
Epoch: 4496, Batch Gradient Norm after: 9.94014198177381
Epoch 4497/10000, Prediction Accuracy = 61.89%, Loss = 0.45444445610046386
Epoch: 4497, Batch Gradient Norm: 10.380973193801166
Epoch: 4497, Batch Gradient Norm after: 10.380973193801166
Epoch 4498/10000, Prediction Accuracy = 61.822%, Loss = 0.45633692741394044
Epoch: 4498, Batch Gradient Norm: 11.313397951088916
Epoch: 4498, Batch Gradient Norm after: 11.313397951088916
Epoch 4499/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.4633240342140198
Epoch: 4499, Batch Gradient Norm: 10.818900375085077
Epoch: 4499, Batch Gradient Norm after: 10.818900375085077
Epoch 4500/10000, Prediction Accuracy = 61.876%, Loss = 0.4602983832359314
Epoch: 4500, Batch Gradient Norm: 9.529863762141428
Epoch: 4500, Batch Gradient Norm after: 9.529863762141428
Epoch 4501/10000, Prediction Accuracy = 61.906000000000006%, Loss = 0.453822535276413
Epoch: 4501, Batch Gradient Norm: 8.015480135752208
Epoch: 4501, Batch Gradient Norm after: 8.015480135752208
Epoch 4502/10000, Prediction Accuracy = 61.958000000000006%, Loss = 0.4443709313869476
Epoch: 4502, Batch Gradient Norm: 9.995040591742118
Epoch: 4502, Batch Gradient Norm after: 9.995040591742118
Epoch 4503/10000, Prediction Accuracy = 61.8%, Loss = 0.4553120195865631
Epoch: 4503, Batch Gradient Norm: 11.820188069182594
Epoch: 4503, Batch Gradient Norm after: 11.820188069182594
Epoch 4504/10000, Prediction Accuracy = 61.92600000000001%, Loss = 0.46698787808418274
Epoch: 4504, Batch Gradient Norm: 10.973691150583996
Epoch: 4504, Batch Gradient Norm after: 10.973691150583996
Epoch 4505/10000, Prediction Accuracy = 61.902%, Loss = 0.4605619370937347
Epoch: 4505, Batch Gradient Norm: 8.994638067673108
Epoch: 4505, Batch Gradient Norm after: 8.994638067673108
Epoch 4506/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.44797223806381226
Epoch: 4506, Batch Gradient Norm: 8.996060561323187
Epoch: 4506, Batch Gradient Norm after: 8.996060561323187
Epoch 4507/10000, Prediction Accuracy = 61.896%, Loss = 0.4483437180519104
Epoch: 4507, Batch Gradient Norm: 9.47230722869235
Epoch: 4507, Batch Gradient Norm after: 9.47230722869235
Epoch 4508/10000, Prediction Accuracy = 61.943999999999996%, Loss = 0.4514146387577057
Epoch: 4508, Batch Gradient Norm: 8.78604270663602
Epoch: 4508, Batch Gradient Norm after: 8.78604270663602
Epoch 4509/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.44616228342056274
Epoch: 4509, Batch Gradient Norm: 11.143989999056558
Epoch: 4509, Batch Gradient Norm after: 11.143989999056558
Epoch 4510/10000, Prediction Accuracy = 61.838%, Loss = 0.4601372957229614
Epoch: 4510, Batch Gradient Norm: 13.537453344650478
Epoch: 4510, Batch Gradient Norm after: 13.537453344650478
Epoch 4511/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.48298231363296507
Epoch: 4511, Batch Gradient Norm: 9.587172484673271
Epoch: 4511, Batch Gradient Norm after: 9.587172484673271
Epoch 4512/10000, Prediction Accuracy = 61.826%, Loss = 0.4535512149333954
Epoch: 4512, Batch Gradient Norm: 7.70665184947176
Epoch: 4512, Batch Gradient Norm after: 7.70665184947176
Epoch 4513/10000, Prediction Accuracy = 61.988%, Loss = 0.44134289026260376
Epoch: 4513, Batch Gradient Norm: 10.061680647017356
Epoch: 4513, Batch Gradient Norm after: 10.061680647017356
Epoch 4514/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.45299801230430603
Epoch: 4514, Batch Gradient Norm: 12.203391369198473
Epoch: 4514, Batch Gradient Norm after: 12.203391369198473
Epoch 4515/10000, Prediction Accuracy = 61.906000000000006%, Loss = 0.46874900460243224
Epoch: 4515, Batch Gradient Norm: 12.254941400927521
Epoch: 4515, Batch Gradient Norm after: 12.254941400927521
Epoch 4516/10000, Prediction Accuracy = 61.748000000000005%, Loss = 0.47180838584899903
Epoch: 4516, Batch Gradient Norm: 10.351767031556962
Epoch: 4516, Batch Gradient Norm after: 10.351767031556962
Epoch 4517/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.45877020955085757
Epoch: 4517, Batch Gradient Norm: 9.879632850126098
Epoch: 4517, Batch Gradient Norm after: 9.879632850126098
Epoch 4518/10000, Prediction Accuracy = 61.822%, Loss = 0.4556781411170959
Epoch: 4518, Batch Gradient Norm: 9.446064709135523
Epoch: 4518, Batch Gradient Norm after: 9.446064709135523
Epoch 4519/10000, Prediction Accuracy = 61.914%, Loss = 0.4523459792137146
Epoch: 4519, Batch Gradient Norm: 8.10134972446647
Epoch: 4519, Batch Gradient Norm after: 8.10134972446647
Epoch 4520/10000, Prediction Accuracy = 61.97800000000001%, Loss = 0.44375272989273074
Epoch: 4520, Batch Gradient Norm: 8.71656153387652
Epoch: 4520, Batch Gradient Norm after: 8.71656153387652
Epoch 4521/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4469048500061035
Epoch: 4521, Batch Gradient Norm: 9.441427661554854
Epoch: 4521, Batch Gradient Norm after: 9.441427661554854
Epoch 4522/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4514537453651428
Epoch: 4522, Batch Gradient Norm: 9.43146372229716
Epoch: 4522, Batch Gradient Norm after: 9.43146372229716
Epoch 4523/10000, Prediction Accuracy = 61.85600000000001%, Loss = 0.45180757641792296
Epoch: 4523, Batch Gradient Norm: 8.986217670157625
Epoch: 4523, Batch Gradient Norm after: 8.986217670157625
Epoch 4524/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.44851940870285034
Epoch: 4524, Batch Gradient Norm: 9.655391669750223
Epoch: 4524, Batch Gradient Norm after: 9.655391669750223
Epoch 4525/10000, Prediction Accuracy = 61.838%, Loss = 0.45295587182044983
Epoch: 4525, Batch Gradient Norm: 9.95822748591929
Epoch: 4525, Batch Gradient Norm after: 9.95822748591929
Epoch 4526/10000, Prediction Accuracy = 61.9%, Loss = 0.45502820014953616
Epoch: 4526, Batch Gradient Norm: 10.759641152233138
Epoch: 4526, Batch Gradient Norm after: 10.759641152233138
Epoch 4527/10000, Prediction Accuracy = 61.926%, Loss = 0.4578034996986389
Epoch: 4527, Batch Gradient Norm: 12.701316050540605
Epoch: 4527, Batch Gradient Norm after: 12.701316050540605
Epoch 4528/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.47265790700912474
Epoch: 4528, Batch Gradient Norm: 9.716748047291915
Epoch: 4528, Batch Gradient Norm after: 9.716748047291915
Epoch 4529/10000, Prediction Accuracy = 61.888%, Loss = 0.451738566160202
Epoch: 4529, Batch Gradient Norm: 10.762030571261832
Epoch: 4529, Batch Gradient Norm after: 10.762030571261832
Epoch 4530/10000, Prediction Accuracy = 61.86%, Loss = 0.4582082509994507
Epoch: 4530, Batch Gradient Norm: 12.280558197112654
Epoch: 4530, Batch Gradient Norm after: 12.280558197112654
Epoch 4531/10000, Prediction Accuracy = 61.84000000000001%, Loss = 0.4701679885387421
Epoch: 4531, Batch Gradient Norm: 11.684382829935194
Epoch: 4531, Batch Gradient Norm after: 11.684382829935194
Epoch 4532/10000, Prediction Accuracy = 62.068%, Loss = 0.46437424421310425
Epoch: 4532, Batch Gradient Norm: 10.929757356532926
Epoch: 4532, Batch Gradient Norm after: 10.929757356532926
Epoch 4533/10000, Prediction Accuracy = 61.798%, Loss = 0.4589790105819702
Epoch: 4533, Batch Gradient Norm: 9.01905851089339
Epoch: 4533, Batch Gradient Norm after: 9.01905851089339
Epoch 4534/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4470724999904633
Epoch: 4534, Batch Gradient Norm: 8.973548424126735
Epoch: 4534, Batch Gradient Norm after: 8.973548424126735
Epoch 4535/10000, Prediction Accuracy = 61.962%, Loss = 0.44714881777763366
Epoch: 4535, Batch Gradient Norm: 10.622406746372732
Epoch: 4535, Batch Gradient Norm after: 10.622406746372732
Epoch 4536/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.45983654260635376
Epoch: 4536, Batch Gradient Norm: 9.6881264495123
Epoch: 4536, Batch Gradient Norm after: 9.6881264495123
Epoch 4537/10000, Prediction Accuracy = 61.968%, Loss = 0.45258486866950987
Epoch: 4537, Batch Gradient Norm: 10.520506912797456
Epoch: 4537, Batch Gradient Norm after: 10.520506912797456
Epoch 4538/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.45564189553260803
Epoch: 4538, Batch Gradient Norm: 12.064900516412767
Epoch: 4538, Batch Gradient Norm after: 12.064900516412767
Epoch 4539/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.46615601181983946
Epoch: 4539, Batch Gradient Norm: 10.247930679547041
Epoch: 4539, Batch Gradient Norm after: 10.247930679547041
Epoch 4540/10000, Prediction Accuracy = 61.924%, Loss = 0.45379791855812074
Epoch: 4540, Batch Gradient Norm: 8.897131309204125
Epoch: 4540, Batch Gradient Norm after: 8.897131309204125
Epoch 4541/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.44636120796203616
Epoch: 4541, Batch Gradient Norm: 8.99563884936927
Epoch: 4541, Batch Gradient Norm after: 8.99563884936927
Epoch 4542/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.44830679297447207
Epoch: 4542, Batch Gradient Norm: 9.060921198122623
Epoch: 4542, Batch Gradient Norm after: 9.060921198122623
Epoch 4543/10000, Prediction Accuracy = 61.914%, Loss = 0.4504681289196014
Epoch: 4543, Batch Gradient Norm: 8.3754392344181
Epoch: 4543, Batch Gradient Norm after: 8.3754392344181
Epoch 4544/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.44511032700538633
Epoch: 4544, Batch Gradient Norm: 9.86235442364005
Epoch: 4544, Batch Gradient Norm after: 9.86235442364005
Epoch 4545/10000, Prediction Accuracy = 61.926%, Loss = 0.454170024394989
Epoch: 4545, Batch Gradient Norm: 11.073783116000763
Epoch: 4545, Batch Gradient Norm after: 11.073783116000763
Epoch 4546/10000, Prediction Accuracy = 61.908%, Loss = 0.46309186816215514
Epoch: 4546, Batch Gradient Norm: 10.891612967142956
Epoch: 4546, Batch Gradient Norm after: 10.891612967142956
Epoch 4547/10000, Prediction Accuracy = 61.924%, Loss = 0.46169930696487427
Epoch: 4547, Batch Gradient Norm: 11.227433434399652
Epoch: 4547, Batch Gradient Norm after: 11.227433434399652
Epoch 4548/10000, Prediction Accuracy = 61.952%, Loss = 0.46259115934371947
Epoch: 4548, Batch Gradient Norm: 11.745834602766378
Epoch: 4548, Batch Gradient Norm after: 11.745834602766378
Epoch 4549/10000, Prediction Accuracy = 61.988%, Loss = 0.4647541880607605
Epoch: 4549, Batch Gradient Norm: 9.993739446198509
Epoch: 4549, Batch Gradient Norm after: 9.993739446198509
Epoch 4550/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.45256144404411314
Epoch: 4550, Batch Gradient Norm: 8.549561832559126
Epoch: 4550, Batch Gradient Norm after: 8.549561832559126
Epoch 4551/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.4439111232757568
Epoch: 4551, Batch Gradient Norm: 8.299488852307567
Epoch: 4551, Batch Gradient Norm after: 8.299488852307567
Epoch 4552/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.44187854528427123
Epoch: 4552, Batch Gradient Norm: 11.011111973624594
Epoch: 4552, Batch Gradient Norm after: 11.011111973624594
Epoch 4553/10000, Prediction Accuracy = 61.90999999999999%, Loss = 0.4588246881961823
Epoch: 4553, Batch Gradient Norm: 11.471954711230252
Epoch: 4553, Batch Gradient Norm after: 11.471954711230252
Epoch 4554/10000, Prediction Accuracy = 61.93000000000001%, Loss = 0.4647526144981384
Epoch: 4554, Batch Gradient Norm: 9.515377133710812
Epoch: 4554, Batch Gradient Norm after: 9.515377133710812
Epoch 4555/10000, Prediction Accuracy = 61.89%, Loss = 0.4511711061000824
Epoch: 4555, Batch Gradient Norm: 9.477939336706426
Epoch: 4555, Batch Gradient Norm after: 9.477939336706426
Epoch 4556/10000, Prediction Accuracy = 61.83%, Loss = 0.44951747059822084
Epoch: 4556, Batch Gradient Norm: 11.945305798558703
Epoch: 4556, Batch Gradient Norm after: 11.945305798558703
Epoch 4557/10000, Prediction Accuracy = 61.926%, Loss = 0.46542697548866274
Epoch: 4557, Batch Gradient Norm: 11.446797024696211
Epoch: 4557, Batch Gradient Norm after: 11.446797024696211
Epoch 4558/10000, Prediction Accuracy = 61.862%, Loss = 0.462275904417038
Epoch: 4558, Batch Gradient Norm: 10.336872950714177
Epoch: 4558, Batch Gradient Norm after: 10.336872950714177
Epoch 4559/10000, Prediction Accuracy = 61.854%, Loss = 0.4552704691886902
Epoch: 4559, Batch Gradient Norm: 9.888095403433274
Epoch: 4559, Batch Gradient Norm after: 9.888095403433274
Epoch 4560/10000, Prediction Accuracy = 61.827999999999996%, Loss = 0.45236226320266726
Epoch: 4560, Batch Gradient Norm: 9.547929664500556
Epoch: 4560, Batch Gradient Norm after: 9.547929664500556
Epoch 4561/10000, Prediction Accuracy = 61.924%, Loss = 0.451760596036911
Epoch: 4561, Batch Gradient Norm: 8.838963447340785
Epoch: 4561, Batch Gradient Norm after: 8.838963447340785
Epoch 4562/10000, Prediction Accuracy = 61.86600000000001%, Loss = 0.4463585793972015
Epoch: 4562, Batch Gradient Norm: 11.162187276590068
Epoch: 4562, Batch Gradient Norm after: 11.162187276590068
Epoch 4563/10000, Prediction Accuracy = 61.846000000000004%, Loss = 0.46116029024124144
Epoch: 4563, Batch Gradient Norm: 11.398904166226242
Epoch: 4563, Batch Gradient Norm after: 11.398904166226242
Epoch 4564/10000, Prediction Accuracy = 61.919999999999995%, Loss = 0.46357581615447996
Epoch: 4564, Batch Gradient Norm: 8.630342046670899
Epoch: 4564, Batch Gradient Norm after: 8.630342046670899
Epoch 4565/10000, Prediction Accuracy = 62.004%, Loss = 0.4453001916408539
Epoch: 4565, Batch Gradient Norm: 8.122948373957092
Epoch: 4565, Batch Gradient Norm after: 8.122948373957092
Epoch 4566/10000, Prediction Accuracy = 62.028%, Loss = 0.4431489765644073
Epoch: 4566, Batch Gradient Norm: 8.723415172634773
Epoch: 4566, Batch Gradient Norm after: 8.723415172634773
Epoch 4567/10000, Prediction Accuracy = 61.902%, Loss = 0.44827733635902406
Epoch: 4567, Batch Gradient Norm: 8.653848124126123
Epoch: 4567, Batch Gradient Norm after: 8.653848124126123
Epoch 4568/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.444746720790863
Epoch: 4568, Batch Gradient Norm: 13.280964388168467
Epoch: 4568, Batch Gradient Norm after: 13.280964388168467
Epoch 4569/10000, Prediction Accuracy = 61.767999999999994%, Loss = 0.47417727708816526
Epoch: 4569, Batch Gradient Norm: 13.066375421987756
Epoch: 4569, Batch Gradient Norm after: 13.066375421987756
Epoch 4570/10000, Prediction Accuracy = 62.06%, Loss = 0.4734259307384491
Epoch: 4570, Batch Gradient Norm: 10.090836987668615
Epoch: 4570, Batch Gradient Norm after: 10.090836987668615
Epoch 4571/10000, Prediction Accuracy = 61.79600000000001%, Loss = 0.4521760046482086
Epoch: 4571, Batch Gradient Norm: 10.414446869560596
Epoch: 4571, Batch Gradient Norm after: 10.414446869560596
Epoch 4572/10000, Prediction Accuracy = 61.96%, Loss = 0.45522021055221557
Epoch: 4572, Batch Gradient Norm: 9.69850023946207
Epoch: 4572, Batch Gradient Norm after: 9.69850023946207
Epoch 4573/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.45062079429626467
Epoch: 4573, Batch Gradient Norm: 8.97387069919522
Epoch: 4573, Batch Gradient Norm after: 8.97387069919522
Epoch 4574/10000, Prediction Accuracy = 61.962%, Loss = 0.4461353898048401
Epoch: 4574, Batch Gradient Norm: 7.935656865648791
Epoch: 4574, Batch Gradient Norm after: 7.935656865648791
Epoch 4575/10000, Prediction Accuracy = 61.96%, Loss = 0.4398660957813263
Epoch: 4575, Batch Gradient Norm: 9.48561477745633
Epoch: 4575, Batch Gradient Norm after: 9.48561477745633
Epoch 4576/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.4490285158157349
Epoch: 4576, Batch Gradient Norm: 10.68113730858262
Epoch: 4576, Batch Gradient Norm after: 10.68113730858262
Epoch 4577/10000, Prediction Accuracy = 61.912%, Loss = 0.45817520618438723
Epoch: 4577, Batch Gradient Norm: 10.045056050507954
Epoch: 4577, Batch Gradient Norm after: 10.045056050507954
Epoch 4578/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.4552868902683258
Epoch: 4578, Batch Gradient Norm: 8.197288084012104
Epoch: 4578, Batch Gradient Norm after: 8.197288084012104
Epoch 4579/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4433227777481079
Epoch: 4579, Batch Gradient Norm: 9.504570987079465
Epoch: 4579, Batch Gradient Norm after: 9.504570987079465
Epoch 4580/10000, Prediction Accuracy = 61.96%, Loss = 0.4488831639289856
Epoch: 4580, Batch Gradient Norm: 12.660176433444578
Epoch: 4580, Batch Gradient Norm after: 12.660176433444578
Epoch 4581/10000, Prediction Accuracy = 61.902%, Loss = 0.47309947609901426
Epoch: 4581, Batch Gradient Norm: 11.690166320785902
Epoch: 4581, Batch Gradient Norm after: 11.690166320785902
Epoch 4582/10000, Prediction Accuracy = 61.95%, Loss = 0.46544761061668394
Epoch: 4582, Batch Gradient Norm: 10.044354270245442
Epoch: 4582, Batch Gradient Norm after: 10.044354270245442
Epoch 4583/10000, Prediction Accuracy = 61.895999999999994%, Loss = 0.4532197296619415
Epoch: 4583, Batch Gradient Norm: 9.992843848931429
Epoch: 4583, Batch Gradient Norm after: 9.992843848931429
Epoch 4584/10000, Prediction Accuracy = 61.944%, Loss = 0.4527813494205475
Epoch: 4584, Batch Gradient Norm: 9.36413411622686
Epoch: 4584, Batch Gradient Norm after: 9.36413411622686
Epoch 4585/10000, Prediction Accuracy = 62.024%, Loss = 0.4474974274635315
Epoch: 4585, Batch Gradient Norm: 9.350018173688786
Epoch: 4585, Batch Gradient Norm after: 9.350018173688786
Epoch 4586/10000, Prediction Accuracy = 61.968%, Loss = 0.4471264183521271
Epoch: 4586, Batch Gradient Norm: 9.078612620274882
Epoch: 4586, Batch Gradient Norm after: 9.078612620274882
Epoch 4587/10000, Prediction Accuracy = 61.86%, Loss = 0.44567970633506776
Epoch: 4587, Batch Gradient Norm: 9.590198205771664
Epoch: 4587, Batch Gradient Norm after: 9.590198205771664
Epoch 4588/10000, Prediction Accuracy = 61.92%, Loss = 0.4495108604431152
Epoch: 4588, Batch Gradient Norm: 11.473243556495605
Epoch: 4588, Batch Gradient Norm after: 11.473243556495605
Epoch 4589/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.46289331316947935
Epoch: 4589, Batch Gradient Norm: 12.63082597584423
Epoch: 4589, Batch Gradient Norm after: 12.63082597584423
Epoch 4590/10000, Prediction Accuracy = 61.836%, Loss = 0.47330284118652344
Epoch: 4590, Batch Gradient Norm: 10.849184202472673
Epoch: 4590, Batch Gradient Norm after: 10.849184202472673
Epoch 4591/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.45751103162765505
Epoch: 4591, Batch Gradient Norm: 10.630040303861177
Epoch: 4591, Batch Gradient Norm after: 10.630040303861177
Epoch 4592/10000, Prediction Accuracy = 61.902%, Loss = 0.4552716135978699
Epoch: 4592, Batch Gradient Norm: 10.464099090761431
Epoch: 4592, Batch Gradient Norm after: 10.464099090761431
Epoch 4593/10000, Prediction Accuracy = 62.068%, Loss = 0.4549358904361725
Epoch: 4593, Batch Gradient Norm: 10.852551253081522
Epoch: 4593, Batch Gradient Norm after: 10.852551253081522
Epoch 4594/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.4595176339149475
Epoch: 4594, Batch Gradient Norm: 10.006020363336003
Epoch: 4594, Batch Gradient Norm after: 10.006020363336003
Epoch 4595/10000, Prediction Accuracy = 61.9%, Loss = 0.4552731871604919
Epoch: 4595, Batch Gradient Norm: 9.315579795433008
Epoch: 4595, Batch Gradient Norm after: 9.315579795433008
Epoch 4596/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.4480262339115143
Epoch: 4596, Batch Gradient Norm: 9.61495558871354
Epoch: 4596, Batch Gradient Norm after: 9.61495558871354
Epoch 4597/10000, Prediction Accuracy = 61.9%, Loss = 0.44833977818489074
Epoch: 4597, Batch Gradient Norm: 9.787109201995602
Epoch: 4597, Batch Gradient Norm after: 9.787109201995602
Epoch 4598/10000, Prediction Accuracy = 62.06%, Loss = 0.44870525002479555
Epoch: 4598, Batch Gradient Norm: 10.900783147760453
Epoch: 4598, Batch Gradient Norm after: 10.900783147760453
Epoch 4599/10000, Prediction Accuracy = 61.88199999999999%, Loss = 0.45783778429031374
Epoch: 4599, Batch Gradient Norm: 9.68623143831303
Epoch: 4599, Batch Gradient Norm after: 9.68623143831303
Epoch 4600/10000, Prediction Accuracy = 61.992%, Loss = 0.45074427127838135
Epoch: 4600, Batch Gradient Norm: 9.28976131597156
Epoch: 4600, Batch Gradient Norm after: 9.28976131597156
Epoch 4601/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.4488759279251099
Epoch: 4601, Batch Gradient Norm: 8.043755566173454
Epoch: 4601, Batch Gradient Norm after: 8.043755566173454
Epoch 4602/10000, Prediction Accuracy = 61.955999999999996%, Loss = 0.4424031674861908
Epoch: 4602, Batch Gradient Norm: 9.276082898751786
Epoch: 4602, Batch Gradient Norm after: 9.276082898751786
Epoch 4603/10000, Prediction Accuracy = 61.964%, Loss = 0.4493583023548126
Epoch: 4603, Batch Gradient Norm: 11.008971732038274
Epoch: 4603, Batch Gradient Norm after: 11.008971732038274
Epoch 4604/10000, Prediction Accuracy = 61.838%, Loss = 0.461175537109375
Epoch: 4604, Batch Gradient Norm: 10.481487539803092
Epoch: 4604, Batch Gradient Norm after: 10.481487539803092
Epoch 4605/10000, Prediction Accuracy = 61.952%, Loss = 0.45498414635658263
Epoch: 4605, Batch Gradient Norm: 10.166003167591876
Epoch: 4605, Batch Gradient Norm after: 10.166003167591876
Epoch 4606/10000, Prediction Accuracy = 61.92199999999999%, Loss = 0.45115774869918823
Epoch: 4606, Batch Gradient Norm: 11.516835777826074
Epoch: 4606, Batch Gradient Norm after: 11.516835777826074
Epoch 4607/10000, Prediction Accuracy = 61.944%, Loss = 0.4606484830379486
Epoch: 4607, Batch Gradient Norm: 11.293663784296736
Epoch: 4607, Batch Gradient Norm after: 11.293663784296736
Epoch 4608/10000, Prediction Accuracy = 61.870000000000005%, Loss = 0.4606130301952362
Epoch: 4608, Batch Gradient Norm: 9.467133108830112
Epoch: 4608, Batch Gradient Norm after: 9.467133108830112
Epoch 4609/10000, Prediction Accuracy = 61.972%, Loss = 0.44906527996063234
Epoch: 4609, Batch Gradient Norm: 8.589069463403746
Epoch: 4609, Batch Gradient Norm after: 8.589069463403746
Epoch 4610/10000, Prediction Accuracy = 61.90599999999999%, Loss = 0.4431691229343414
Epoch: 4610, Batch Gradient Norm: 10.466398502795649
Epoch: 4610, Batch Gradient Norm after: 10.466398502795649
Epoch 4611/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.45405533313751223
Epoch: 4611, Batch Gradient Norm: 11.120161857433523
Epoch: 4611, Batch Gradient Norm after: 11.120161857433523
Epoch 4612/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.4588357448577881
Epoch: 4612, Batch Gradient Norm: 10.880125721490591
Epoch: 4612, Batch Gradient Norm after: 10.880125721490591
Epoch 4613/10000, Prediction Accuracy = 61.896%, Loss = 0.45878621339797976
Epoch: 4613, Batch Gradient Norm: 8.897408526654523
Epoch: 4613, Batch Gradient Norm after: 8.897408526654523
Epoch 4614/10000, Prediction Accuracy = 61.98%, Loss = 0.4458636701107025
Epoch: 4614, Batch Gradient Norm: 9.068246926920521
Epoch: 4614, Batch Gradient Norm after: 9.068246926920521
Epoch 4615/10000, Prediction Accuracy = 61.956%, Loss = 0.4455326497554779
Epoch: 4615, Batch Gradient Norm: 10.789686669812232
Epoch: 4615, Batch Gradient Norm after: 10.789686669812232
Epoch 4616/10000, Prediction Accuracy = 61.918000000000006%, Loss = 0.4577335834503174
Epoch: 4616, Batch Gradient Norm: 9.496913387979204
Epoch: 4616, Batch Gradient Norm after: 9.496913387979204
Epoch 4617/10000, Prediction Accuracy = 62.104%, Loss = 0.4502169847488403
Epoch: 4617, Batch Gradient Norm: 9.4663628024676
Epoch: 4617, Batch Gradient Norm after: 9.4663628024676
Epoch 4618/10000, Prediction Accuracy = 61.891999999999996%, Loss = 0.44740604758262636
Epoch: 4618, Batch Gradient Norm: 11.863391611993222
Epoch: 4618, Batch Gradient Norm after: 11.863391611993222
Epoch 4619/10000, Prediction Accuracy = 61.956%, Loss = 0.4638141930103302
Epoch: 4619, Batch Gradient Norm: 10.535856828028312
Epoch: 4619, Batch Gradient Norm after: 10.535856828028312
Epoch 4620/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.45540556907653806
Epoch: 4620, Batch Gradient Norm: 8.298405585526902
Epoch: 4620, Batch Gradient Norm after: 8.298405585526902
Epoch 4621/10000, Prediction Accuracy = 61.989999999999995%, Loss = 0.4415949463844299
Epoch: 4621, Batch Gradient Norm: 9.691868793260369
Epoch: 4621, Batch Gradient Norm after: 9.691868793260369
Epoch 4622/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.45187196135520935
Epoch: 4622, Batch Gradient Norm: 10.540722000027836
Epoch: 4622, Batch Gradient Norm after: 10.540722000027836
Epoch 4623/10000, Prediction Accuracy = 61.944%, Loss = 0.45762051939964293
Epoch: 4623, Batch Gradient Norm: 10.191960480824164
Epoch: 4623, Batch Gradient Norm after: 10.191960480824164
Epoch 4624/10000, Prediction Accuracy = 62.0%, Loss = 0.45401389002799986
Epoch: 4624, Batch Gradient Norm: 9.4224993545748
Epoch: 4624, Batch Gradient Norm after: 9.4224993545748
Epoch 4625/10000, Prediction Accuracy = 61.972%, Loss = 0.44818041324615476
Epoch: 4625, Batch Gradient Norm: 8.568981156902359
Epoch: 4625, Batch Gradient Norm after: 8.568981156902359
Epoch 4626/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.4425446093082428
Epoch: 4626, Batch Gradient Norm: 13.272946516604359
Epoch: 4626, Batch Gradient Norm after: 13.272946516604359
Epoch 4627/10000, Prediction Accuracy = 61.948%, Loss = 0.4747406244277954
Epoch: 4627, Batch Gradient Norm: 15.450684246862696
Epoch: 4627, Batch Gradient Norm after: 15.450684246862696
Epoch 4628/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4947132349014282
Epoch: 4628, Batch Gradient Norm: 9.355611602127164
Epoch: 4628, Batch Gradient Norm after: 9.355611602127164
Epoch 4629/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4476434588432312
Epoch: 4629, Batch Gradient Norm: 7.621459215727474
Epoch: 4629, Batch Gradient Norm after: 7.621459215727474
Epoch 4630/10000, Prediction Accuracy = 62.044%, Loss = 0.43858563899993896
Epoch: 4630, Batch Gradient Norm: 7.422860084915565
Epoch: 4630, Batch Gradient Norm after: 7.422860084915565
Epoch 4631/10000, Prediction Accuracy = 61.896%, Loss = 0.43875702619552615
Epoch: 4631, Batch Gradient Norm: 8.038255046097039
Epoch: 4631, Batch Gradient Norm after: 8.038255046097039
Epoch 4632/10000, Prediction Accuracy = 62.016%, Loss = 0.44067848920822145
Epoch: 4632, Batch Gradient Norm: 10.291298815120728
Epoch: 4632, Batch Gradient Norm after: 10.291298815120728
Epoch 4633/10000, Prediction Accuracy = 61.826%, Loss = 0.4540302097797394
Epoch: 4633, Batch Gradient Norm: 10.145395559134196
Epoch: 4633, Batch Gradient Norm after: 10.145395559134196
Epoch 4634/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.45202766060829164
Epoch: 4634, Batch Gradient Norm: 8.995534591467635
Epoch: 4634, Batch Gradient Norm after: 8.995534591467635
Epoch 4635/10000, Prediction Accuracy = 61.878%, Loss = 0.44386361837387084
Epoch: 4635, Batch Gradient Norm: 9.460540738315167
Epoch: 4635, Batch Gradient Norm after: 9.460540738315167
Epoch 4636/10000, Prediction Accuracy = 62.104%, Loss = 0.4466648161411285
Epoch: 4636, Batch Gradient Norm: 11.665909148783932
Epoch: 4636, Batch Gradient Norm after: 11.665909148783932
Epoch 4637/10000, Prediction Accuracy = 61.842%, Loss = 0.4623312890529633
Epoch: 4637, Batch Gradient Norm: 12.224981476285967
Epoch: 4637, Batch Gradient Norm after: 12.224981476285967
Epoch 4638/10000, Prediction Accuracy = 61.976%, Loss = 0.46737672090530397
Epoch: 4638, Batch Gradient Norm: 9.870552983384544
Epoch: 4638, Batch Gradient Norm after: 9.870552983384544
Epoch 4639/10000, Prediction Accuracy = 61.88399999999999%, Loss = 0.4498215615749359
Epoch: 4639, Batch Gradient Norm: 9.56644462556496
Epoch: 4639, Batch Gradient Norm after: 9.56644462556496
Epoch 4640/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.44694923758506777
Epoch: 4640, Batch Gradient Norm: 11.780616726056826
Epoch: 4640, Batch Gradient Norm after: 11.780616726056826
Epoch 4641/10000, Prediction Accuracy = 61.944%, Loss = 0.4630550742149353
Epoch: 4641, Batch Gradient Norm: 10.743632525685694
Epoch: 4641, Batch Gradient Norm after: 10.743632525685694
Epoch 4642/10000, Prediction Accuracy = 61.90400000000001%, Loss = 0.4556141495704651
Epoch: 4642, Batch Gradient Norm: 8.820728257260326
Epoch: 4642, Batch Gradient Norm after: 8.820728257260326
Epoch 4643/10000, Prediction Accuracy = 62.00999999999999%, Loss = 0.44354066252708435
Epoch: 4643, Batch Gradient Norm: 8.914296367290152
Epoch: 4643, Batch Gradient Norm after: 8.914296367290152
Epoch 4644/10000, Prediction Accuracy = 61.948%, Loss = 0.44468535780906676
Epoch: 4644, Batch Gradient Norm: 9.675917154527385
Epoch: 4644, Batch Gradient Norm after: 9.675917154527385
Epoch 4645/10000, Prediction Accuracy = 62.068%, Loss = 0.4503211617469788
Epoch: 4645, Batch Gradient Norm: 8.178783919748327
Epoch: 4645, Batch Gradient Norm after: 8.178783919748327
Epoch 4646/10000, Prediction Accuracy = 61.996%, Loss = 0.44058387279510497
Epoch: 4646, Batch Gradient Norm: 8.514925546967897
Epoch: 4646, Batch Gradient Norm after: 8.514925546967897
Epoch 4647/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.44189817309379575
Epoch: 4647, Batch Gradient Norm: 8.637256708594618
Epoch: 4647, Batch Gradient Norm after: 8.637256708594618
Epoch 4648/10000, Prediction Accuracy = 61.95799999999999%, Loss = 0.44190009236335753
Epoch: 4648, Batch Gradient Norm: 12.261507735656236
Epoch: 4648, Batch Gradient Norm after: 12.261507735656236
Epoch 4649/10000, Prediction Accuracy = 61.894000000000005%, Loss = 0.46761186718940734
Epoch: 4649, Batch Gradient Norm: 14.8820002493945
Epoch: 4649, Batch Gradient Norm after: 14.8820002493945
Epoch 4650/10000, Prediction Accuracy = 61.94%, Loss = 0.48999505639076235
Epoch: 4650, Batch Gradient Norm: 11.596461615467922
Epoch: 4650, Batch Gradient Norm after: 11.596461615467922
Epoch 4651/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.4628073811531067
Epoch: 4651, Batch Gradient Norm: 8.233305505574853
Epoch: 4651, Batch Gradient Norm after: 8.233305505574853
Epoch 4652/10000, Prediction Accuracy = 62.112%, Loss = 0.4401685416698456
Epoch: 4652, Batch Gradient Norm: 9.61018667353845
Epoch: 4652, Batch Gradient Norm after: 9.61018667353845
Epoch 4653/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.4485512554645538
Epoch: 4653, Batch Gradient Norm: 10.209330737663638
Epoch: 4653, Batch Gradient Norm after: 10.209330737663638
Epoch 4654/10000, Prediction Accuracy = 61.95399999999999%, Loss = 0.4522585928440094
Epoch: 4654, Batch Gradient Norm: 9.12503129955281
Epoch: 4654, Batch Gradient Norm after: 9.12503129955281
Epoch 4655/10000, Prediction Accuracy = 61.958000000000006%, Loss = 0.4451937675476074
Epoch: 4655, Batch Gradient Norm: 8.798035132786293
Epoch: 4655, Batch Gradient Norm after: 8.798035132786293
Epoch 4656/10000, Prediction Accuracy = 61.964%, Loss = 0.4427528381347656
Epoch: 4656, Batch Gradient Norm: 10.993565704134506
Epoch: 4656, Batch Gradient Norm after: 10.993565704134506
Epoch 4657/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.45681633353233336
Epoch: 4657, Batch Gradient Norm: 12.040104044852516
Epoch: 4657, Batch Gradient Norm after: 12.040104044852516
Epoch 4658/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.46559590101242065
Epoch: 4658, Batch Gradient Norm: 10.437349613417195
Epoch: 4658, Batch Gradient Norm after: 10.437349613417195
Epoch 4659/10000, Prediction Accuracy = 61.879999999999995%, Loss = 0.4555388569831848
Epoch: 4659, Batch Gradient Norm: 7.517600890972108
Epoch: 4659, Batch Gradient Norm after: 7.517600890972108
Epoch 4660/10000, Prediction Accuracy = 62.044%, Loss = 0.437721848487854
Epoch: 4660, Batch Gradient Norm: 7.368492974313832
Epoch: 4660, Batch Gradient Norm after: 7.368492974313832
Epoch 4661/10000, Prediction Accuracy = 61.96%, Loss = 0.43651242852211
Epoch: 4661, Batch Gradient Norm: 9.786815252533907
Epoch: 4661, Batch Gradient Norm after: 9.786815252533907
Epoch 4662/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.44931156635284425
Epoch: 4662, Batch Gradient Norm: 10.756589151863826
Epoch: 4662, Batch Gradient Norm after: 10.756589151863826
Epoch 4663/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.45628525614738463
Epoch: 4663, Batch Gradient Norm: 10.769107885678629
Epoch: 4663, Batch Gradient Norm after: 10.769107885678629
Epoch 4664/10000, Prediction Accuracy = 61.910000000000004%, Loss = 0.45498262643814086
Epoch: 4664, Batch Gradient Norm: 10.267455949800363
Epoch: 4664, Batch Gradient Norm after: 10.267455949800363
Epoch 4665/10000, Prediction Accuracy = 62.052%, Loss = 0.4508554458618164
Epoch: 4665, Batch Gradient Norm: 12.264944651679437
Epoch: 4665, Batch Gradient Norm after: 12.264944651679437
Epoch 4666/10000, Prediction Accuracy = 61.84599999999999%, Loss = 0.4646996557712555
Epoch: 4666, Batch Gradient Norm: 11.501184341374408
Epoch: 4666, Batch Gradient Norm after: 11.501184341374408
Epoch 4667/10000, Prediction Accuracy = 62.1%, Loss = 0.4605747818946838
Epoch: 4667, Batch Gradient Norm: 8.259170263092948
Epoch: 4667, Batch Gradient Norm after: 8.259170263092948
Epoch 4668/10000, Prediction Accuracy = 61.92%, Loss = 0.4402496039867401
Epoch: 4668, Batch Gradient Norm: 8.42744730769108
Epoch: 4668, Batch Gradient Norm after: 8.42744730769108
Epoch 4669/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4416533589363098
Epoch: 4669, Batch Gradient Norm: 8.305381931876873
Epoch: 4669, Batch Gradient Norm after: 8.305381931876873
Epoch 4670/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4411916434764862
Epoch: 4670, Batch Gradient Norm: 8.826780658000535
Epoch: 4670, Batch Gradient Norm after: 8.826780658000535
Epoch 4671/10000, Prediction Accuracy = 61.926%, Loss = 0.44395336508750916
Epoch: 4671, Batch Gradient Norm: 9.790163917322127
Epoch: 4671, Batch Gradient Norm after: 9.790163917322127
Epoch 4672/10000, Prediction Accuracy = 62.0%, Loss = 0.4482515573501587
Epoch: 4672, Batch Gradient Norm: 13.993859892786627
Epoch: 4672, Batch Gradient Norm after: 13.993859892786627
Epoch 4673/10000, Prediction Accuracy = 61.931999999999995%, Loss = 0.48175392150878904
Epoch: 4673, Batch Gradient Norm: 13.05144460659656
Epoch: 4673, Batch Gradient Norm after: 13.05144460659656
Epoch 4674/10000, Prediction Accuracy = 61.98%, Loss = 0.473124235868454
Epoch: 4674, Batch Gradient Norm: 11.256678690927682
Epoch: 4674, Batch Gradient Norm after: 11.256678690927682
Epoch 4675/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4576395571231842
Epoch: 4675, Batch Gradient Norm: 10.023436118279157
Epoch: 4675, Batch Gradient Norm after: 10.023436118279157
Epoch 4676/10000, Prediction Accuracy = 61.948%, Loss = 0.4504964351654053
Epoch: 4676, Batch Gradient Norm: 8.678294634535034
Epoch: 4676, Batch Gradient Norm after: 8.678294634535034
Epoch 4677/10000, Prediction Accuracy = 61.95%, Loss = 0.4420405745506287
Epoch: 4677, Batch Gradient Norm: 8.747288890431333
Epoch: 4677, Batch Gradient Norm after: 8.747288890431333
Epoch 4678/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4423103153705597
Epoch: 4678, Batch Gradient Norm: 9.46539630942211
Epoch: 4678, Batch Gradient Norm after: 9.46539630942211
Epoch 4679/10000, Prediction Accuracy = 62.11800000000001%, Loss = 0.44637241363525393
Epoch: 4679, Batch Gradient Norm: 9.87614526860192
Epoch: 4679, Batch Gradient Norm after: 9.87614526860192
Epoch 4680/10000, Prediction Accuracy = 62.008%, Loss = 0.4490268886089325
Epoch: 4680, Batch Gradient Norm: 10.699061357594275
Epoch: 4680, Batch Gradient Norm after: 10.699061357594275
Epoch 4681/10000, Prediction Accuracy = 62.04%, Loss = 0.4543595314025879
Epoch: 4681, Batch Gradient Norm: 11.892916564077167
Epoch: 4681, Batch Gradient Norm after: 11.892916564077167
Epoch 4682/10000, Prediction Accuracy = 61.839999999999996%, Loss = 0.46531251072883606
Epoch: 4682, Batch Gradient Norm: 8.569442459977632
Epoch: 4682, Batch Gradient Norm after: 8.569442459977632
Epoch 4683/10000, Prediction Accuracy = 62.064%, Loss = 0.44165324568748476
Epoch: 4683, Batch Gradient Norm: 8.312809317277997
Epoch: 4683, Batch Gradient Norm after: 8.312809317277997
Epoch 4684/10000, Prediction Accuracy = 61.944%, Loss = 0.43875992894172666
Epoch: 4684, Batch Gradient Norm: 10.284025681829995
Epoch: 4684, Batch Gradient Norm after: 10.284025681829995
Epoch 4685/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.44963284730911257
Epoch: 4685, Batch Gradient Norm: 10.919484755277367
Epoch: 4685, Batch Gradient Norm after: 10.919484755277367
Epoch 4686/10000, Prediction Accuracy = 61.876%, Loss = 0.45550972819328306
Epoch: 4686, Batch Gradient Norm: 10.101241268216882
Epoch: 4686, Batch Gradient Norm after: 10.101241268216882
Epoch 4687/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.45100926160812377
Epoch: 4687, Batch Gradient Norm: 8.708108109593981
Epoch: 4687, Batch Gradient Norm after: 8.708108109593981
Epoch 4688/10000, Prediction Accuracy = 62.074%, Loss = 0.44236899018287656
Epoch: 4688, Batch Gradient Norm: 11.048415874906715
Epoch: 4688, Batch Gradient Norm after: 11.048415874906715
Epoch 4689/10000, Prediction Accuracy = 61.98%, Loss = 0.45804309844970703
Epoch: 4689, Batch Gradient Norm: 10.773066373518782
Epoch: 4689, Batch Gradient Norm after: 10.773066373518782
Epoch 4690/10000, Prediction Accuracy = 62.032%, Loss = 0.45744755268096926
Epoch: 4690, Batch Gradient Norm: 8.982884498612393
Epoch: 4690, Batch Gradient Norm after: 8.982884498612393
Epoch 4691/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.44380618929862975
Epoch: 4691, Batch Gradient Norm: 10.372865133292903
Epoch: 4691, Batch Gradient Norm after: 10.372865133292903
Epoch 4692/10000, Prediction Accuracy = 61.964%, Loss = 0.45152665972709655
Epoch: 4692, Batch Gradient Norm: 11.117108144603336
Epoch: 4692, Batch Gradient Norm after: 11.117108144603336
Epoch 4693/10000, Prediction Accuracy = 62.10999999999999%, Loss = 0.45551531910896303
Epoch: 4693, Batch Gradient Norm: 11.353902489318491
Epoch: 4693, Batch Gradient Norm after: 11.353902489318491
Epoch 4694/10000, Prediction Accuracy = 61.94%, Loss = 0.4570178985595703
Epoch: 4694, Batch Gradient Norm: 10.833697581517113
Epoch: 4694, Batch Gradient Norm after: 10.833697581517113
Epoch 4695/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.4556917369365692
Epoch: 4695, Batch Gradient Norm: 8.99759197355878
Epoch: 4695, Batch Gradient Norm after: 8.99759197355878
Epoch 4696/10000, Prediction Accuracy = 61.968%, Loss = 0.44411820769309995
Epoch: 4696, Batch Gradient Norm: 9.543052357504585
Epoch: 4696, Batch Gradient Norm after: 9.543052357504585
Epoch 4697/10000, Prediction Accuracy = 62.048%, Loss = 0.44750276803970335
Epoch: 4697, Batch Gradient Norm: 12.498783394438897
Epoch: 4697, Batch Gradient Norm after: 12.498783394438897
Epoch 4698/10000, Prediction Accuracy = 61.878%, Loss = 0.46820544004440307
Epoch: 4698, Batch Gradient Norm: 11.70067994741732
Epoch: 4698, Batch Gradient Norm after: 11.70067994741732
Epoch 4699/10000, Prediction Accuracy = 61.96999999999999%, Loss = 0.46265249848365786
Epoch: 4699, Batch Gradient Norm: 9.07512899325166
Epoch: 4699, Batch Gradient Norm after: 9.07512899325166
Epoch 4700/10000, Prediction Accuracy = 61.94199999999999%, Loss = 0.4435224115848541
Epoch: 4700, Batch Gradient Norm: 8.982560896162749
Epoch: 4700, Batch Gradient Norm after: 8.982560896162749
Epoch 4701/10000, Prediction Accuracy = 62.038%, Loss = 0.44272044897079466
Epoch: 4701, Batch Gradient Norm: 9.673022147601296
Epoch: 4701, Batch Gradient Norm after: 9.673022147601296
Epoch 4702/10000, Prediction Accuracy = 61.995999999999995%, Loss = 0.44821526408195494
Epoch: 4702, Batch Gradient Norm: 9.331639350057808
Epoch: 4702, Batch Gradient Norm after: 9.331639350057808
Epoch 4703/10000, Prediction Accuracy = 62.02199999999999%, Loss = 0.4456673800945282
Epoch: 4703, Batch Gradient Norm: 8.781432594992665
Epoch: 4703, Batch Gradient Norm after: 8.781432594992665
Epoch 4704/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.44201971888542174
Epoch: 4704, Batch Gradient Norm: 9.593490768061677
Epoch: 4704, Batch Gradient Norm after: 9.593490768061677
Epoch 4705/10000, Prediction Accuracy = 61.96%, Loss = 0.44785823225975036
Epoch: 4705, Batch Gradient Norm: 8.15967911744698
Epoch: 4705, Batch Gradient Norm after: 8.15967911744698
Epoch 4706/10000, Prediction Accuracy = 62.00999999999999%, Loss = 0.4395030915737152
Epoch: 4706, Batch Gradient Norm: 9.888723068846456
Epoch: 4706, Batch Gradient Norm after: 9.888723068846456
Epoch 4707/10000, Prediction Accuracy = 61.926%, Loss = 0.44941741824150083
Epoch: 4707, Batch Gradient Norm: 13.06026299787576
Epoch: 4707, Batch Gradient Norm after: 13.06026299787576
Epoch 4708/10000, Prediction Accuracy = 61.988%, Loss = 0.47228232622146604
Epoch: 4708, Batch Gradient Norm: 11.228602143505658
Epoch: 4708, Batch Gradient Norm after: 11.228602143505658
Epoch 4709/10000, Prediction Accuracy = 61.88000000000001%, Loss = 0.45876513719558715
Epoch: 4709, Batch Gradient Norm: 9.334437091075987
Epoch: 4709, Batch Gradient Norm after: 9.334437091075987
Epoch 4710/10000, Prediction Accuracy = 62.052%, Loss = 0.44436055421829224
Epoch: 4710, Batch Gradient Norm: 10.512343567881382
Epoch: 4710, Batch Gradient Norm after: 10.512343567881382
Epoch 4711/10000, Prediction Accuracy = 62.084%, Loss = 0.45070359110832214
Epoch: 4711, Batch Gradient Norm: 10.767208520278011
Epoch: 4711, Batch Gradient Norm after: 10.767208520278011
Epoch 4712/10000, Prediction Accuracy = 61.91799999999999%, Loss = 0.45244324803352354
Epoch: 4712, Batch Gradient Norm: 8.96746697501302
Epoch: 4712, Batch Gradient Norm after: 8.96746697501302
Epoch 4713/10000, Prediction Accuracy = 62.14%, Loss = 0.44175947904586793
Epoch: 4713, Batch Gradient Norm: 9.713393345033746
Epoch: 4713, Batch Gradient Norm after: 9.713393345033746
Epoch 4714/10000, Prediction Accuracy = 61.86199999999999%, Loss = 0.4467006862163544
Epoch: 4714, Batch Gradient Norm: 11.458691866740228
Epoch: 4714, Batch Gradient Norm after: 11.458691866740228
Epoch 4715/10000, Prediction Accuracy = 62.06%, Loss = 0.45948047041893003
Epoch: 4715, Batch Gradient Norm: 11.555005721777013
Epoch: 4715, Batch Gradient Norm after: 11.555005721777013
Epoch 4716/10000, Prediction Accuracy = 61.89200000000001%, Loss = 0.4599341094493866
Epoch: 4716, Batch Gradient Norm: 9.746113429274013
Epoch: 4716, Batch Gradient Norm after: 9.746113429274013
Epoch 4717/10000, Prediction Accuracy = 62.08399999999999%, Loss = 0.4476041436195374
Epoch: 4717, Batch Gradient Norm: 9.318596294412986
Epoch: 4717, Batch Gradient Norm after: 9.318596294412986
Epoch 4718/10000, Prediction Accuracy = 61.99799999999999%, Loss = 0.44469902515411375
Epoch: 4718, Batch Gradient Norm: 9.80140067621516
Epoch: 4718, Batch Gradient Norm after: 9.80140067621516
Epoch 4719/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4471049547195435
Epoch: 4719, Batch Gradient Norm: 10.963876644946739
Epoch: 4719, Batch Gradient Norm after: 10.963876644946739
Epoch 4720/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.45485600233078005
Epoch: 4720, Batch Gradient Norm: 12.36850343729192
Epoch: 4720, Batch Gradient Norm after: 12.36850343729192
Epoch 4721/10000, Prediction Accuracy = 61.968%, Loss = 0.46715729832649233
Epoch: 4721, Batch Gradient Norm: 10.708796991966292
Epoch: 4721, Batch Gradient Norm after: 10.708796991966292
Epoch 4722/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.455012184381485
Epoch: 4722, Batch Gradient Norm: 8.888584220916078
Epoch: 4722, Batch Gradient Norm after: 8.888584220916078
Epoch 4723/10000, Prediction Accuracy = 61.96%, Loss = 0.44213815331459044
Epoch: 4723, Batch Gradient Norm: 8.239093619007267
Epoch: 4723, Batch Gradient Norm after: 8.239093619007267
Epoch 4724/10000, Prediction Accuracy = 62.04599999999999%, Loss = 0.4377159118652344
Epoch: 4724, Batch Gradient Norm: 9.293801830978088
Epoch: 4724, Batch Gradient Norm after: 9.293801830978088
Epoch 4725/10000, Prediction Accuracy = 61.962%, Loss = 0.4431962311267853
Epoch: 4725, Batch Gradient Norm: 11.131093014177374
Epoch: 4725, Batch Gradient Norm after: 11.131093014177374
Epoch 4726/10000, Prediction Accuracy = 62.086%, Loss = 0.4565917134284973
Epoch: 4726, Batch Gradient Norm: 9.605608043174824
Epoch: 4726, Batch Gradient Norm after: 9.605608043174824
Epoch 4727/10000, Prediction Accuracy = 62.116%, Loss = 0.4470243215560913
Epoch: 4727, Batch Gradient Norm: 8.5325395903402
Epoch: 4727, Batch Gradient Norm after: 8.5325395903402
Epoch 4728/10000, Prediction Accuracy = 62.044%, Loss = 0.4402739405632019
Epoch: 4728, Batch Gradient Norm: 7.134150139513058
Epoch: 4728, Batch Gradient Norm after: 7.134150139513058
Epoch 4729/10000, Prediction Accuracy = 62.088%, Loss = 0.432531464099884
Epoch: 4729, Batch Gradient Norm: 7.8648492857933086
Epoch: 4729, Batch Gradient Norm after: 7.8648492857933086
Epoch 4730/10000, Prediction Accuracy = 61.96600000000001%, Loss = 0.4363493502140045
Epoch: 4730, Batch Gradient Norm: 8.580330594389721
Epoch: 4730, Batch Gradient Norm after: 8.580330594389721
Epoch 4731/10000, Prediction Accuracy = 62.077999999999996%, Loss = 0.4394399285316467
Epoch: 4731, Batch Gradient Norm: 13.722670570816447
Epoch: 4731, Batch Gradient Norm after: 13.722670570816447
Epoch 4732/10000, Prediction Accuracy = 61.903999999999996%, Loss = 0.47676426768302915
Epoch: 4732, Batch Gradient Norm: 13.34093967174434
Epoch: 4732, Batch Gradient Norm after: 13.34093967174434
Epoch 4733/10000, Prediction Accuracy = 62.062%, Loss = 0.4751477360725403
Epoch: 4733, Batch Gradient Norm: 9.444938316744581
Epoch: 4733, Batch Gradient Norm after: 9.444938316744581
Epoch 4734/10000, Prediction Accuracy = 61.988%, Loss = 0.4458631694316864
Epoch: 4734, Batch Gradient Norm: 10.845120135678087
Epoch: 4734, Batch Gradient Norm after: 10.845120135678087
Epoch 4735/10000, Prediction Accuracy = 61.964%, Loss = 0.4607183992862701
Epoch: 4735, Batch Gradient Norm: 7.752250869252972
Epoch: 4735, Batch Gradient Norm after: 7.752250869252972
Epoch 4736/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.43641615509986875
Epoch: 4736, Batch Gradient Norm: 10.134159867851858
Epoch: 4736, Batch Gradient Norm after: 10.134159867851858
Epoch 4737/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.44803733229637144
Epoch: 4737, Batch Gradient Norm: 12.83603801248969
Epoch: 4737, Batch Gradient Norm after: 12.83603801248969
Epoch 4738/10000, Prediction Accuracy = 61.886%, Loss = 0.46850895285606386
Epoch: 4738, Batch Gradient Norm: 10.439363809874346
Epoch: 4738, Batch Gradient Norm after: 10.439363809874346
Epoch 4739/10000, Prediction Accuracy = 62.10999999999999%, Loss = 0.45105117559432983
Epoch: 4739, Batch Gradient Norm: 9.429722807245469
Epoch: 4739, Batch Gradient Norm after: 9.429722807245469
Epoch 4740/10000, Prediction Accuracy = 61.914%, Loss = 0.44404537677764894
Epoch: 4740, Batch Gradient Norm: 9.99428425018268
Epoch: 4740, Batch Gradient Norm after: 9.99428425018268
Epoch 4741/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.4484516501426697
Epoch: 4741, Batch Gradient Norm: 9.279808197464162
Epoch: 4741, Batch Gradient Norm after: 9.279808197464162
Epoch 4742/10000, Prediction Accuracy = 61.934000000000005%, Loss = 0.4440873324871063
Epoch: 4742, Batch Gradient Norm: 9.904771202416976
Epoch: 4742, Batch Gradient Norm after: 9.904771202416976
Epoch 4743/10000, Prediction Accuracy = 61.90999999999999%, Loss = 0.4481410324573517
Epoch: 4743, Batch Gradient Norm: 12.036783489471825
Epoch: 4743, Batch Gradient Norm after: 12.036783489471825
Epoch 4744/10000, Prediction Accuracy = 62.148%, Loss = 0.46299664974212645
Epoch: 4744, Batch Gradient Norm: 11.210380831969625
Epoch: 4744, Batch Gradient Norm after: 11.210380831969625
Epoch 4745/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.45816431045532224
Epoch: 4745, Batch Gradient Norm: 8.390419941797127
Epoch: 4745, Batch Gradient Norm after: 8.390419941797127
Epoch 4746/10000, Prediction Accuracy = 62.08399999999999%, Loss = 0.4388028383255005
Epoch: 4746, Batch Gradient Norm: 9.124947351896603
Epoch: 4746, Batch Gradient Norm after: 9.124947351896603
Epoch 4747/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.44188215136528014
Epoch: 4747, Batch Gradient Norm: 11.836330702121728
Epoch: 4747, Batch Gradient Norm after: 11.836330702121728
Epoch 4748/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.45976381897926333
Epoch: 4748, Batch Gradient Norm: 10.564554800700465
Epoch: 4748, Batch Gradient Norm after: 10.564554800700465
Epoch 4749/10000, Prediction Accuracy = 61.996%, Loss = 0.45052743554115293
Epoch: 4749, Batch Gradient Norm: 8.903356424447288
Epoch: 4749, Batch Gradient Norm after: 8.903356424447288
Epoch 4750/10000, Prediction Accuracy = 62.02%, Loss = 0.4400611758232117
Epoch: 4750, Batch Gradient Norm: 10.812735855080263
Epoch: 4750, Batch Gradient Norm after: 10.812735855080263
Epoch 4751/10000, Prediction Accuracy = 61.944%, Loss = 0.4552685976028442
Epoch: 4751, Batch Gradient Norm: 7.685169297488238
Epoch: 4751, Batch Gradient Norm after: 7.685169297488238
Epoch 4752/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.43646215796470644
Epoch: 4752, Batch Gradient Norm: 6.982442718977192
Epoch: 4752, Batch Gradient Norm after: 6.982442718977192
Epoch 4753/10000, Prediction Accuracy = 62.0%, Loss = 0.4321022927761078
Epoch: 4753, Batch Gradient Norm: 9.1008640886659
Epoch: 4753, Batch Gradient Norm after: 9.1008640886659
Epoch 4754/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.443252956867218
Epoch: 4754, Batch Gradient Norm: 12.807899537651856
Epoch: 4754, Batch Gradient Norm after: 12.807899537651856
Epoch 4755/10000, Prediction Accuracy = 61.878%, Loss = 0.47085772156715394
Epoch: 4755, Batch Gradient Norm: 12.068539382866557
Epoch: 4755, Batch Gradient Norm after: 12.068539382866557
Epoch 4756/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.46358792781829833
Epoch: 4756, Batch Gradient Norm: 9.185546875159385
Epoch: 4756, Batch Gradient Norm after: 9.185546875159385
Epoch 4757/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.44254066944122317
Epoch: 4757, Batch Gradient Norm: 8.17978761999613
Epoch: 4757, Batch Gradient Norm after: 8.17978761999613
Epoch 4758/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.43699792623519895
Epoch: 4758, Batch Gradient Norm: 9.229349672347476
Epoch: 4758, Batch Gradient Norm after: 9.229349672347476
Epoch 4759/10000, Prediction Accuracy = 61.988%, Loss = 0.4436162769794464
Epoch: 4759, Batch Gradient Norm: 10.214121343221652
Epoch: 4759, Batch Gradient Norm after: 10.214121343221652
Epoch 4760/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.44867480993270875
Epoch: 4760, Batch Gradient Norm: 12.062578850889803
Epoch: 4760, Batch Gradient Norm after: 12.062578850889803
Epoch 4761/10000, Prediction Accuracy = 61.9%, Loss = 0.4618676841259003
Epoch: 4761, Batch Gradient Norm: 11.02449544052953
Epoch: 4761, Batch Gradient Norm after: 11.02449544052953
Epoch 4762/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.45506985783576964
Epoch: 4762, Batch Gradient Norm: 10.693519689186298
Epoch: 4762, Batch Gradient Norm after: 10.693519689186298
Epoch 4763/10000, Prediction Accuracy = 61.922000000000004%, Loss = 0.45254506468772887
Epoch: 4763, Batch Gradient Norm: 12.005610835503406
Epoch: 4763, Batch Gradient Norm after: 12.005610835503406
Epoch 4764/10000, Prediction Accuracy = 61.98199999999999%, Loss = 0.462367981672287
Epoch: 4764, Batch Gradient Norm: 10.805096579349637
Epoch: 4764, Batch Gradient Norm after: 10.805096579349637
Epoch 4765/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.4534516274929047
Epoch: 4765, Batch Gradient Norm: 9.361897011473763
Epoch: 4765, Batch Gradient Norm after: 9.361897011473763
Epoch 4766/10000, Prediction Accuracy = 61.98599999999999%, Loss = 0.44368632435798644
Epoch: 4766, Batch Gradient Norm: 8.891016282420393
Epoch: 4766, Batch Gradient Norm after: 8.891016282420393
Epoch 4767/10000, Prediction Accuracy = 62.08399999999999%, Loss = 0.44097036123275757
Epoch: 4767, Batch Gradient Norm: 9.751546578574654
Epoch: 4767, Batch Gradient Norm after: 9.751546578574654
Epoch 4768/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.4471244394779205
Epoch: 4768, Batch Gradient Norm: 9.638631124898646
Epoch: 4768, Batch Gradient Norm after: 9.638631124898646
Epoch 4769/10000, Prediction Accuracy = 62.036%, Loss = 0.4455286622047424
Epoch: 4769, Batch Gradient Norm: 12.273407374863218
Epoch: 4769, Batch Gradient Norm after: 12.273407374863218
Epoch 4770/10000, Prediction Accuracy = 62.040000000000006%, Loss = 0.4625395774841309
Epoch: 4770, Batch Gradient Norm: 12.178301473197173
Epoch: 4770, Batch Gradient Norm after: 12.178301473197173
Epoch 4771/10000, Prediction Accuracy = 62.03000000000001%, Loss = 0.463037383556366
Epoch: 4771, Batch Gradient Norm: 8.888193918237024
Epoch: 4771, Batch Gradient Norm after: 8.888193918237024
Epoch 4772/10000, Prediction Accuracy = 62.004%, Loss = 0.44097661375999453
Epoch: 4772, Batch Gradient Norm: 7.618180084957869
Epoch: 4772, Batch Gradient Norm after: 7.618180084957869
Epoch 4773/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.4344613075256348
Epoch: 4773, Batch Gradient Norm: 7.438174251833945
Epoch: 4773, Batch Gradient Norm after: 7.438174251833945
Epoch 4774/10000, Prediction Accuracy = 61.977999999999994%, Loss = 0.43268986940383913
Epoch: 4774, Batch Gradient Norm: 8.87466587300257
Epoch: 4774, Batch Gradient Norm after: 8.87466587300257
Epoch 4775/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.44057453274726865
Epoch: 4775, Batch Gradient Norm: 10.028282045456146
Epoch: 4775, Batch Gradient Norm after: 10.028282045456146
Epoch 4776/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4474273145198822
Epoch: 4776, Batch Gradient Norm: 10.399247870730958
Epoch: 4776, Batch Gradient Norm after: 10.399247870730958
Epoch 4777/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4504960536956787
Epoch: 4777, Batch Gradient Norm: 9.872023211581588
Epoch: 4777, Batch Gradient Norm after: 9.872023211581588
Epoch 4778/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4479990601539612
Epoch: 4778, Batch Gradient Norm: 11.246205609961685
Epoch: 4778, Batch Gradient Norm after: 11.246205609961685
Epoch 4779/10000, Prediction Accuracy = 62.076%, Loss = 0.45822311043739317
Epoch: 4779, Batch Gradient Norm: 12.625110252720653
Epoch: 4779, Batch Gradient Norm after: 12.625110252720653
Epoch 4780/10000, Prediction Accuracy = 62.036%, Loss = 0.4672370314598083
Epoch: 4780, Batch Gradient Norm: 12.175281223462566
Epoch: 4780, Batch Gradient Norm after: 12.175281223462566
Epoch 4781/10000, Prediction Accuracy = 62.06999999999999%, Loss = 0.462172794342041
Epoch: 4781, Batch Gradient Norm: 9.234986831913004
Epoch: 4781, Batch Gradient Norm after: 9.234986831913004
Epoch 4782/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.44190824031829834
Epoch: 4782, Batch Gradient Norm: 8.647452565076263
Epoch: 4782, Batch Gradient Norm after: 8.647452565076263
Epoch 4783/10000, Prediction Accuracy = 62.11%, Loss = 0.4384742498397827
Epoch: 4783, Batch Gradient Norm: 10.449054491972694
Epoch: 4783, Batch Gradient Norm after: 10.449054491972694
Epoch 4784/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.450130033493042
Epoch: 4784, Batch Gradient Norm: 10.206654987098814
Epoch: 4784, Batch Gradient Norm after: 10.206654987098814
Epoch 4785/10000, Prediction Accuracy = 62.062%, Loss = 0.4493465840816498
Epoch: 4785, Batch Gradient Norm: 8.877742830581719
Epoch: 4785, Batch Gradient Norm after: 8.877742830581719
Epoch 4786/10000, Prediction Accuracy = 62.08%, Loss = 0.4410680830478668
Epoch: 4786, Batch Gradient Norm: 8.554297816026054
Epoch: 4786, Batch Gradient Norm after: 8.554297816026054
Epoch 4787/10000, Prediction Accuracy = 62.056%, Loss = 0.43906405568122864
Epoch: 4787, Batch Gradient Norm: 10.087479778559285
Epoch: 4787, Batch Gradient Norm after: 10.087479778559285
Epoch 4788/10000, Prediction Accuracy = 61.992000000000004%, Loss = 0.4490944504737854
Epoch: 4788, Batch Gradient Norm: 10.607560348005217
Epoch: 4788, Batch Gradient Norm after: 10.607560348005217
Epoch 4789/10000, Prediction Accuracy = 62.013999999999996%, Loss = 0.4531330049037933
Epoch: 4789, Batch Gradient Norm: 9.634193404657397
Epoch: 4789, Batch Gradient Norm after: 9.634193404657397
Epoch 4790/10000, Prediction Accuracy = 61.938%, Loss = 0.4461790382862091
Epoch: 4790, Batch Gradient Norm: 8.943715545594078
Epoch: 4790, Batch Gradient Norm after: 8.943715545594078
Epoch 4791/10000, Prediction Accuracy = 62.146%, Loss = 0.4412021696567535
Epoch: 4791, Batch Gradient Norm: 9.8504437547185
Epoch: 4791, Batch Gradient Norm after: 9.8504437547185
Epoch 4792/10000, Prediction Accuracy = 61.92999999999999%, Loss = 0.4451531648635864
Epoch: 4792, Batch Gradient Norm: 11.870141800348192
Epoch: 4792, Batch Gradient Norm after: 11.870141800348192
Epoch 4793/10000, Prediction Accuracy = 62.08%, Loss = 0.4599312961101532
Epoch: 4793, Batch Gradient Norm: 11.5375168690134
Epoch: 4793, Batch Gradient Norm after: 11.5375168690134
Epoch 4794/10000, Prediction Accuracy = 61.882000000000005%, Loss = 0.45905535817146303
Epoch: 4794, Batch Gradient Norm: 9.48639101751376
Epoch: 4794, Batch Gradient Norm after: 9.48639101751376
Epoch 4795/10000, Prediction Accuracy = 62.068%, Loss = 0.4430993437767029
Epoch: 4795, Batch Gradient Norm: 11.793352132155025
Epoch: 4795, Batch Gradient Norm after: 11.793352132155025
Epoch 4796/10000, Prediction Accuracy = 62.04%, Loss = 0.45743276476860045
Epoch: 4796, Batch Gradient Norm: 11.90664504211721
Epoch: 4796, Batch Gradient Norm after: 11.90664504211721
Epoch 4797/10000, Prediction Accuracy = 62.05799999999999%, Loss = 0.4595377743244171
Epoch: 4797, Batch Gradient Norm: 10.157722562529193
Epoch: 4797, Batch Gradient Norm after: 10.157722562529193
Epoch 4798/10000, Prediction Accuracy = 61.964%, Loss = 0.4484084725379944
Epoch: 4798, Batch Gradient Norm: 8.850833304853504
Epoch: 4798, Batch Gradient Norm after: 8.850833304853504
Epoch 4799/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.4400202512741089
Epoch: 4799, Batch Gradient Norm: 10.360551864905249
Epoch: 4799, Batch Gradient Norm after: 10.360551864905249
Epoch 4800/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.4477038562297821
Epoch: 4800, Batch Gradient Norm: 11.476178418488049
Epoch: 4800, Batch Gradient Norm after: 11.476178418488049
Epoch 4801/10000, Prediction Accuracy = 62.09400000000001%, Loss = 0.45625290274620056
Epoch: 4801, Batch Gradient Norm: 9.794229963701033
Epoch: 4801, Batch Gradient Norm after: 9.794229963701033
Epoch 4802/10000, Prediction Accuracy = 62.084%, Loss = 0.44540520310401915
Epoch: 4802, Batch Gradient Norm: 8.260399513283978
Epoch: 4802, Batch Gradient Norm after: 8.260399513283978
Epoch 4803/10000, Prediction Accuracy = 62.088%, Loss = 0.4360386371612549
Epoch: 4803, Batch Gradient Norm: 9.506744784010328
Epoch: 4803, Batch Gradient Norm after: 9.506744784010328
Epoch 4804/10000, Prediction Accuracy = 62.116%, Loss = 0.4424004852771759
Epoch: 4804, Batch Gradient Norm: 11.268939105327892
Epoch: 4804, Batch Gradient Norm after: 11.268939105327892
Epoch 4805/10000, Prediction Accuracy = 61.924%, Loss = 0.4543079316616058
Epoch: 4805, Batch Gradient Norm: 10.649687544189579
Epoch: 4805, Batch Gradient Norm after: 10.649687544189579
Epoch 4806/10000, Prediction Accuracy = 62.05%, Loss = 0.451457279920578
Epoch: 4806, Batch Gradient Norm: 9.599400367912786
Epoch: 4806, Batch Gradient Norm after: 9.599400367912786
Epoch 4807/10000, Prediction Accuracy = 61.94%, Loss = 0.44665912389755247
Epoch: 4807, Batch Gradient Norm: 7.534771349019484
Epoch: 4807, Batch Gradient Norm after: 7.534771349019484
Epoch 4808/10000, Prediction Accuracy = 62.09000000000001%, Loss = 0.43467808365821836
Epoch: 4808, Batch Gradient Norm: 7.560910668868115
Epoch: 4808, Batch Gradient Norm after: 7.560910668868115
Epoch 4809/10000, Prediction Accuracy = 62.068%, Loss = 0.43382824659347535
Epoch: 4809, Batch Gradient Norm: 7.841692143094927
Epoch: 4809, Batch Gradient Norm after: 7.841692143094927
Epoch 4810/10000, Prediction Accuracy = 62.04%, Loss = 0.43400086760520934
Epoch: 4810, Batch Gradient Norm: 10.30181141776412
Epoch: 4810, Batch Gradient Norm after: 10.30181141776412
Epoch 4811/10000, Prediction Accuracy = 62.036%, Loss = 0.44858528971672057
Epoch: 4811, Batch Gradient Norm: 12.28207482940585
Epoch: 4811, Batch Gradient Norm after: 12.28207482940585
Epoch 4812/10000, Prediction Accuracy = 61.956%, Loss = 0.4630796253681183
Epoch: 4812, Batch Gradient Norm: 12.059258612958404
Epoch: 4812, Batch Gradient Norm after: 12.059258612958404
Epoch 4813/10000, Prediction Accuracy = 61.878%, Loss = 0.46343255043029785
Epoch: 4813, Batch Gradient Norm: 10.389251725367023
Epoch: 4813, Batch Gradient Norm after: 10.389251725367023
Epoch 4814/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4503529191017151
Epoch: 4814, Batch Gradient Norm: 12.106762756684757
Epoch: 4814, Batch Gradient Norm after: 12.106762756684757
Epoch 4815/10000, Prediction Accuracy = 62.08200000000001%, Loss = 0.461993932723999
Epoch: 4815, Batch Gradient Norm: 11.559615706490794
Epoch: 4815, Batch Gradient Norm after: 11.559615706490794
Epoch 4816/10000, Prediction Accuracy = 62.03800000000001%, Loss = 0.4579213559627533
Epoch: 4816, Batch Gradient Norm: 8.723675285113048
Epoch: 4816, Batch Gradient Norm after: 8.723675285113048
Epoch 4817/10000, Prediction Accuracy = 62.076%, Loss = 0.4388920485973358
Epoch: 4817, Batch Gradient Norm: 7.556903894706424
Epoch: 4817, Batch Gradient Norm after: 7.556903894706424
Epoch 4818/10000, Prediction Accuracy = 62.04600000000001%, Loss = 0.43173460364341737
Epoch: 4818, Batch Gradient Norm: 8.257987242426255
Epoch: 4818, Batch Gradient Norm after: 8.257987242426255
Epoch 4819/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4350153565406799
Epoch: 4819, Batch Gradient Norm: 10.372285979409384
Epoch: 4819, Batch Gradient Norm after: 10.372285979409384
Epoch 4820/10000, Prediction Accuracy = 61.94%, Loss = 0.4464050352573395
Epoch: 4820, Batch Gradient Norm: 12.763588867297328
Epoch: 4820, Batch Gradient Norm after: 12.763588867297328
Epoch 4821/10000, Prediction Accuracy = 62.1%, Loss = 0.46541765332221985
Epoch: 4821, Batch Gradient Norm: 11.628275682184691
Epoch: 4821, Batch Gradient Norm after: 11.628275682184691
Epoch 4822/10000, Prediction Accuracy = 61.896%, Loss = 0.4601648569107056
Epoch: 4822, Batch Gradient Norm: 9.04842888115513
Epoch: 4822, Batch Gradient Norm after: 9.04842888115513
Epoch 4823/10000, Prediction Accuracy = 62.102%, Loss = 0.4404582381248474
Epoch: 4823, Batch Gradient Norm: 10.523310193998967
Epoch: 4823, Batch Gradient Norm after: 10.523310193998967
Epoch 4824/10000, Prediction Accuracy = 61.996%, Loss = 0.44857951402664187
Epoch: 4824, Batch Gradient Norm: 12.041254474969605
Epoch: 4824, Batch Gradient Norm after: 12.041254474969605
Epoch 4825/10000, Prediction Accuracy = 62.052%, Loss = 0.46044139862060546
Epoch: 4825, Batch Gradient Norm: 9.734279949883934
Epoch: 4825, Batch Gradient Norm after: 9.734279949883934
Epoch 4826/10000, Prediction Accuracy = 62.076%, Loss = 0.4443045198917389
Epoch: 4826, Batch Gradient Norm: 8.474898006539496
Epoch: 4826, Batch Gradient Norm after: 8.474898006539496
Epoch 4827/10000, Prediction Accuracy = 62.068%, Loss = 0.4366308867931366
Epoch: 4827, Batch Gradient Norm: 9.259823695396648
Epoch: 4827, Batch Gradient Norm after: 9.259823695396648
Epoch 4828/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.4405936777591705
Epoch: 4828, Batch Gradient Norm: 10.610938375440938
Epoch: 4828, Batch Gradient Norm after: 10.610938375440938
Epoch 4829/10000, Prediction Accuracy = 61.962%, Loss = 0.4526988387107849
Epoch: 4829, Batch Gradient Norm: 8.274315377109337
Epoch: 4829, Batch Gradient Norm after: 8.274315377109337
Epoch 4830/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.437929230928421
Epoch: 4830, Batch Gradient Norm: 8.969911212594274
Epoch: 4830, Batch Gradient Norm after: 8.969911212594274
Epoch 4831/10000, Prediction Accuracy = 62.112%, Loss = 0.43930500745773315
Epoch: 4831, Batch Gradient Norm: 12.74432247924294
Epoch: 4831, Batch Gradient Norm after: 12.74432247924294
Epoch 4832/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4654339551925659
Epoch: 4832, Batch Gradient Norm: 11.80695431200301
Epoch: 4832, Batch Gradient Norm after: 11.80695431200301
Epoch 4833/10000, Prediction Accuracy = 62.074%, Loss = 0.4590493977069855
Epoch: 4833, Batch Gradient Norm: 8.803385687414625
Epoch: 4833, Batch Gradient Norm after: 8.803385687414625
Epoch 4834/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4393373966217041
Epoch: 4834, Batch Gradient Norm: 8.577082283017623
Epoch: 4834, Batch Gradient Norm after: 8.577082283017623
Epoch 4835/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.43737645745277404
Epoch: 4835, Batch Gradient Norm: 9.632723219715634
Epoch: 4835, Batch Gradient Norm after: 9.632723219715634
Epoch 4836/10000, Prediction Accuracy = 62.092%, Loss = 0.44369747638702395
Epoch: 4836, Batch Gradient Norm: 10.121630792478882
Epoch: 4836, Batch Gradient Norm after: 10.121630792478882
Epoch 4837/10000, Prediction Accuracy = 62.074%, Loss = 0.44610831141471863
Epoch: 4837, Batch Gradient Norm: 9.947411499465971
Epoch: 4837, Batch Gradient Norm after: 9.947411499465971
Epoch 4838/10000, Prediction Accuracy = 62.05%, Loss = 0.4455291569232941
Epoch: 4838, Batch Gradient Norm: 9.645190561203949
Epoch: 4838, Batch Gradient Norm after: 9.645190561203949
Epoch 4839/10000, Prediction Accuracy = 62.07000000000001%, Loss = 0.44330537915229795
Epoch: 4839, Batch Gradient Norm: 10.454055326207563
Epoch: 4839, Batch Gradient Norm after: 10.454055326207563
Epoch 4840/10000, Prediction Accuracy = 61.976%, Loss = 0.4477431416511536
Epoch: 4840, Batch Gradient Norm: 12.071432253916626
Epoch: 4840, Batch Gradient Norm after: 12.071432253916626
Epoch 4841/10000, Prediction Accuracy = 62.152%, Loss = 0.4589314222335815
Epoch: 4841, Batch Gradient Norm: 10.562571721016893
Epoch: 4841, Batch Gradient Norm after: 10.562571721016893
Epoch 4842/10000, Prediction Accuracy = 61.974000000000004%, Loss = 0.448975020647049
Epoch: 4842, Batch Gradient Norm: 8.97768077154298
Epoch: 4842, Batch Gradient Norm after: 8.97768077154298
Epoch 4843/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.43892579078674315
Epoch: 4843, Batch Gradient Norm: 9.876297415429617
Epoch: 4843, Batch Gradient Norm after: 9.876297415429617
Epoch 4844/10000, Prediction Accuracy = 61.936%, Loss = 0.4454323351383209
Epoch: 4844, Batch Gradient Norm: 11.106756134496944
Epoch: 4844, Batch Gradient Norm after: 11.106756134496944
Epoch 4845/10000, Prediction Accuracy = 61.928%, Loss = 0.45594980716705324
Epoch: 4845, Batch Gradient Norm: 10.994724438791742
Epoch: 4845, Batch Gradient Norm after: 10.994724438791742
Epoch 4846/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.45238569378852844
Epoch: 4846, Batch Gradient Norm: 11.754049161084826
Epoch: 4846, Batch Gradient Norm after: 11.754049161084826
Epoch 4847/10000, Prediction Accuracy = 62.074%, Loss = 0.4579950630664825
Epoch: 4847, Batch Gradient Norm: 10.448515558917924
Epoch: 4847, Batch Gradient Norm after: 10.448515558917924
Epoch 4848/10000, Prediction Accuracy = 62.024%, Loss = 0.4500155508518219
Epoch: 4848, Batch Gradient Norm: 7.429701141495337
Epoch: 4848, Batch Gradient Norm after: 7.429701141495337
Epoch 4849/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.43165661096572877
Epoch: 4849, Batch Gradient Norm: 8.424774972553834
Epoch: 4849, Batch Gradient Norm after: 8.424774972553834
Epoch 4850/10000, Prediction Accuracy = 61.952%, Loss = 0.43660842180252074
Epoch: 4850, Batch Gradient Norm: 10.440033084463922
Epoch: 4850, Batch Gradient Norm after: 10.440033084463922
Epoch 4851/10000, Prediction Accuracy = 62.102%, Loss = 0.4496812462806702
Epoch: 4851, Batch Gradient Norm: 12.030115692738441
Epoch: 4851, Batch Gradient Norm after: 12.030115692738441
Epoch 4852/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.46304582953453066
Epoch: 4852, Batch Gradient Norm: 9.068356204235679
Epoch: 4852, Batch Gradient Norm after: 9.068356204235679
Epoch 4853/10000, Prediction Accuracy = 62.084%, Loss = 0.44203304052352904
Epoch: 4853, Batch Gradient Norm: 7.303153674901685
Epoch: 4853, Batch Gradient Norm after: 7.303153674901685
Epoch 4854/10000, Prediction Accuracy = 62.04599999999999%, Loss = 0.4310668110847473
Epoch: 4854, Batch Gradient Norm: 8.455853449486412
Epoch: 4854, Batch Gradient Norm after: 8.455853449486412
Epoch 4855/10000, Prediction Accuracy = 62.008%, Loss = 0.4360749423503876
Epoch: 4855, Batch Gradient Norm: 10.41686719748976
Epoch: 4855, Batch Gradient Norm after: 10.41686719748976
Epoch 4856/10000, Prediction Accuracy = 62.10799999999999%, Loss = 0.4475144147872925
Epoch: 4856, Batch Gradient Norm: 11.850604364558015
Epoch: 4856, Batch Gradient Norm after: 11.850604364558015
Epoch 4857/10000, Prediction Accuracy = 61.908%, Loss = 0.45805891752243044
Epoch: 4857, Batch Gradient Norm: 10.95069578150901
Epoch: 4857, Batch Gradient Norm after: 10.95069578150901
Epoch 4858/10000, Prediction Accuracy = 62.148%, Loss = 0.4531040549278259
Epoch: 4858, Batch Gradient Norm: 10.48912585201542
Epoch: 4858, Batch Gradient Norm after: 10.48912585201542
Epoch 4859/10000, Prediction Accuracy = 62.056%, Loss = 0.44889922738075255
Epoch: 4859, Batch Gradient Norm: 10.505803467948446
Epoch: 4859, Batch Gradient Norm after: 10.505803467948446
Epoch 4860/10000, Prediction Accuracy = 61.972%, Loss = 0.447139972448349
Epoch: 4860, Batch Gradient Norm: 10.036727622896278
Epoch: 4860, Batch Gradient Norm after: 10.036727622896278
Epoch 4861/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4440445005893707
Epoch: 4861, Batch Gradient Norm: 9.367074343696665
Epoch: 4861, Batch Gradient Norm after: 9.367074343696665
Epoch 4862/10000, Prediction Accuracy = 61.992%, Loss = 0.4403139650821686
Epoch: 4862, Batch Gradient Norm: 10.041346182480885
Epoch: 4862, Batch Gradient Norm after: 10.041346182480885
Epoch 4863/10000, Prediction Accuracy = 62.178%, Loss = 0.445350307226181
Epoch: 4863, Batch Gradient Norm: 11.546840978717354
Epoch: 4863, Batch Gradient Norm after: 11.546840978717354
Epoch 4864/10000, Prediction Accuracy = 62.036%, Loss = 0.45623701214790346
Epoch: 4864, Batch Gradient Norm: 11.779228219231259
Epoch: 4864, Batch Gradient Norm after: 11.779228219231259
Epoch 4865/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4576931297779083
Epoch: 4865, Batch Gradient Norm: 9.754055616415739
Epoch: 4865, Batch Gradient Norm after: 9.754055616415739
Epoch 4866/10000, Prediction Accuracy = 62.074%, Loss = 0.444859117269516
Epoch: 4866, Batch Gradient Norm: 7.124265350884749
Epoch: 4866, Batch Gradient Norm after: 7.124265350884749
Epoch 4867/10000, Prediction Accuracy = 62.06600000000001%, Loss = 0.4295688450336456
Epoch: 4867, Batch Gradient Norm: 7.594954497772867
Epoch: 4867, Batch Gradient Norm after: 7.594954497772867
Epoch 4868/10000, Prediction Accuracy = 62.068%, Loss = 0.43140937089920045
Epoch: 4868, Batch Gradient Norm: 9.628625846974382
Epoch: 4868, Batch Gradient Norm after: 9.628625846974382
Epoch 4869/10000, Prediction Accuracy = 62.03399999999999%, Loss = 0.4427539348602295
Epoch: 4869, Batch Gradient Norm: 13.091021581129958
Epoch: 4869, Batch Gradient Norm after: 13.091021581129958
Epoch 4870/10000, Prediction Accuracy = 61.98%, Loss = 0.4675787925720215
Epoch: 4870, Batch Gradient Norm: 13.06357340379027
Epoch: 4870, Batch Gradient Norm after: 13.06357340379027
Epoch 4871/10000, Prediction Accuracy = 62.02%, Loss = 0.4675122082233429
Epoch: 4871, Batch Gradient Norm: 9.129820822597347
Epoch: 4871, Batch Gradient Norm after: 9.129820822597347
Epoch 4872/10000, Prediction Accuracy = 62.1%, Loss = 0.44014160633087157
Epoch: 4872, Batch Gradient Norm: 7.960037058831119
Epoch: 4872, Batch Gradient Norm after: 7.960037058831119
Epoch 4873/10000, Prediction Accuracy = 62.048%, Loss = 0.43401460647583007
Epoch: 4873, Batch Gradient Norm: 8.51055214132699
Epoch: 4873, Batch Gradient Norm after: 8.51055214132699
Epoch 4874/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.43688278198242186
Epoch: 4874, Batch Gradient Norm: 10.112702490030278
Epoch: 4874, Batch Gradient Norm after: 10.112702490030278
Epoch 4875/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.4474434733390808
Epoch: 4875, Batch Gradient Norm: 10.762542495889672
Epoch: 4875, Batch Gradient Norm after: 10.762542495889672
Epoch 4876/10000, Prediction Accuracy = 62.098%, Loss = 0.44963171482086184
Epoch: 4876, Batch Gradient Norm: 11.90849326534342
Epoch: 4876, Batch Gradient Norm after: 11.90849326534342
Epoch 4877/10000, Prediction Accuracy = 62.02199999999999%, Loss = 0.4590960741043091
Epoch: 4877, Batch Gradient Norm: 12.053438100764017
Epoch: 4877, Batch Gradient Norm after: 12.053438100764017
Epoch 4878/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.4601834714412689
Epoch: 4878, Batch Gradient Norm: 10.685783854157828
Epoch: 4878, Batch Gradient Norm after: 10.685783854157828
Epoch 4879/10000, Prediction Accuracy = 62.138%, Loss = 0.45021170377731323
Epoch: 4879, Batch Gradient Norm: 9.04176495461785
Epoch: 4879, Batch Gradient Norm after: 9.04176495461785
Epoch 4880/10000, Prediction Accuracy = 62.088%, Loss = 0.43934227228164674
Epoch: 4880, Batch Gradient Norm: 8.108651221634751
Epoch: 4880, Batch Gradient Norm after: 8.108651221634751
Epoch 4881/10000, Prediction Accuracy = 62.074%, Loss = 0.43384106159210206
Epoch: 4881, Batch Gradient Norm: 8.487432847559532
Epoch: 4881, Batch Gradient Norm after: 8.487432847559532
Epoch 4882/10000, Prediction Accuracy = 61.972%, Loss = 0.43653241395950315
Epoch: 4882, Batch Gradient Norm: 9.579181524881944
Epoch: 4882, Batch Gradient Norm after: 9.579181524881944
Epoch 4883/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4422154605388641
Epoch: 4883, Batch Gradient Norm: 10.910328526677052
Epoch: 4883, Batch Gradient Norm after: 10.910328526677052
Epoch 4884/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.4505328953266144
Epoch: 4884, Batch Gradient Norm: 9.546315835604632
Epoch: 4884, Batch Gradient Norm after: 9.546315835604632
Epoch 4885/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.441168874502182
Epoch: 4885, Batch Gradient Norm: 9.3287612141068
Epoch: 4885, Batch Gradient Norm after: 9.3287612141068
Epoch 4886/10000, Prediction Accuracy = 62.05%, Loss = 0.43935532569885255
Epoch: 4886, Batch Gradient Norm: 11.13315985518507
Epoch: 4886, Batch Gradient Norm after: 11.13315985518507
Epoch 4887/10000, Prediction Accuracy = 62.03599999999999%, Loss = 0.4525922119617462
Epoch: 4887, Batch Gradient Norm: 10.529472004163845
Epoch: 4887, Batch Gradient Norm after: 10.529472004163845
Epoch 4888/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4493311822414398
Epoch: 4888, Batch Gradient Norm: 9.622416107942172
Epoch: 4888, Batch Gradient Norm after: 9.622416107942172
Epoch 4889/10000, Prediction Accuracy = 62.065999999999995%, Loss = 0.44310325384140015
Epoch: 4889, Batch Gradient Norm: 9.710578968418922
Epoch: 4889, Batch Gradient Norm after: 9.710578968418922
Epoch 4890/10000, Prediction Accuracy = 62.08%, Loss = 0.4429932773113251
Epoch: 4890, Batch Gradient Norm: 9.848333290838383
Epoch: 4890, Batch Gradient Norm after: 9.848333290838383
Epoch 4891/10000, Prediction Accuracy = 62.00599999999999%, Loss = 0.44509552121162416
Epoch: 4891, Batch Gradient Norm: 8.260444840875907
Epoch: 4891, Batch Gradient Norm after: 8.260444840875907
Epoch 4892/10000, Prediction Accuracy = 62.086%, Loss = 0.4348836302757263
Epoch: 4892, Batch Gradient Norm: 10.8326039038731
Epoch: 4892, Batch Gradient Norm after: 10.8326039038731
Epoch 4893/10000, Prediction Accuracy = 61.99399999999999%, Loss = 0.44947865009307864
Epoch: 4893, Batch Gradient Norm: 14.356260581838248
Epoch: 4893, Batch Gradient Norm after: 14.356260581838248
Epoch 4894/10000, Prediction Accuracy = 62.010000000000005%, Loss = 0.4782860934734344
Epoch: 4894, Batch Gradient Norm: 11.30442160003759
Epoch: 4894, Batch Gradient Norm after: 11.30442160003759
Epoch 4895/10000, Prediction Accuracy = 61.996%, Loss = 0.45303993225097655
Epoch: 4895, Batch Gradient Norm: 9.609588253329154
Epoch: 4895, Batch Gradient Norm after: 9.609588253329154
Epoch 4896/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.4413847684860229
Epoch: 4896, Batch Gradient Norm: 10.829840235790563
Epoch: 4896, Batch Gradient Norm after: 10.829840235790563
Epoch 4897/10000, Prediction Accuracy = 62.05800000000001%, Loss = 0.4487330675125122
Epoch: 4897, Batch Gradient Norm: 11.514646384083623
Epoch: 4897, Batch Gradient Norm after: 11.514646384083623
Epoch 4898/10000, Prediction Accuracy = 62.01800000000001%, Loss = 0.4539851903915405
Epoch: 4898, Batch Gradient Norm: 9.975916761342722
Epoch: 4898, Batch Gradient Norm after: 9.975916761342722
Epoch 4899/10000, Prediction Accuracy = 62.084%, Loss = 0.44360668063163755
Epoch: 4899, Batch Gradient Norm: 8.809063200934075
Epoch: 4899, Batch Gradient Norm after: 8.809063200934075
Epoch 4900/10000, Prediction Accuracy = 62.024%, Loss = 0.43689284324645994
Epoch: 4900, Batch Gradient Norm: 9.150457606227691
Epoch: 4900, Batch Gradient Norm after: 9.150457606227691
Epoch 4901/10000, Prediction Accuracy = 62.124%, Loss = 0.4395518183708191
Epoch: 4901, Batch Gradient Norm: 9.27245277853413
Epoch: 4901, Batch Gradient Norm after: 9.27245277853413
Epoch 4902/10000, Prediction Accuracy = 62.04200000000001%, Loss = 0.44073122143745425
Epoch: 4902, Batch Gradient Norm: 9.143869708794215
Epoch: 4902, Batch Gradient Norm after: 9.143869708794215
Epoch 4903/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.43866777420043945
Epoch: 4903, Batch Gradient Norm: 10.826470937081385
Epoch: 4903, Batch Gradient Norm after: 10.826470937081385
Epoch 4904/10000, Prediction Accuracy = 62.098%, Loss = 0.45015994906425477
Epoch: 4904, Batch Gradient Norm: 9.623904383022191
Epoch: 4904, Batch Gradient Norm after: 9.623904383022191
Epoch 4905/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.44423248171806334
Epoch: 4905, Batch Gradient Norm: 7.883078622375385
Epoch: 4905, Batch Gradient Norm after: 7.883078622375385
Epoch 4906/10000, Prediction Accuracy = 62.148%, Loss = 0.4322619140148163
Epoch: 4906, Batch Gradient Norm: 8.346668144391453
Epoch: 4906, Batch Gradient Norm after: 8.346668144391453
Epoch 4907/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.43376835584640505
Epoch: 4907, Batch Gradient Norm: 10.904503447126086
Epoch: 4907, Batch Gradient Norm after: 10.904503447126086
Epoch 4908/10000, Prediction Accuracy = 62.152%, Loss = 0.44962744116783143
Epoch: 4908, Batch Gradient Norm: 12.659561424294276
Epoch: 4908, Batch Gradient Norm after: 12.659561424294276
Epoch 4909/10000, Prediction Accuracy = 62.06999999999999%, Loss = 0.46505010724067686
Epoch: 4909, Batch Gradient Norm: 11.002309210829429
Epoch: 4909, Batch Gradient Norm after: 11.002309210829429
Epoch 4910/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4516183495521545
Epoch: 4910, Batch Gradient Norm: 8.953906955302353
Epoch: 4910, Batch Gradient Norm after: 8.953906955302353
Epoch 4911/10000, Prediction Accuracy = 62.08599999999999%, Loss = 0.4372089087963104
Epoch: 4911, Batch Gradient Norm: 11.001003978613602
Epoch: 4911, Batch Gradient Norm after: 11.001003978613602
Epoch 4912/10000, Prediction Accuracy = 62.068000000000005%, Loss = 0.45083285570144654
Epoch: 4912, Batch Gradient Norm: 10.941388787423417
Epoch: 4912, Batch Gradient Norm after: 10.941388787423417
Epoch 4913/10000, Prediction Accuracy = 62.13599999999999%, Loss = 0.45081034302711487
Epoch: 4913, Batch Gradient Norm: 10.06739888638761
Epoch: 4913, Batch Gradient Norm after: 10.06739888638761
Epoch 4914/10000, Prediction Accuracy = 61.988%, Loss = 0.4442968785762787
Epoch: 4914, Batch Gradient Norm: 9.12384700794968
Epoch: 4914, Batch Gradient Norm after: 9.12384700794968
Epoch 4915/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4385020315647125
Epoch: 4915, Batch Gradient Norm: 9.031115764884458
Epoch: 4915, Batch Gradient Norm after: 9.031115764884458
Epoch 4916/10000, Prediction Accuracy = 61.946000000000005%, Loss = 0.43804452419281004
Epoch: 4916, Batch Gradient Norm: 9.766698099676374
Epoch: 4916, Batch Gradient Norm after: 9.766698099676374
Epoch 4917/10000, Prediction Accuracy = 62.025999999999996%, Loss = 0.44258151650428773
Epoch: 4917, Batch Gradient Norm: 9.725167087808375
Epoch: 4917, Batch Gradient Norm after: 9.725167087808375
Epoch 4918/10000, Prediction Accuracy = 62.05999999999999%, Loss = 0.44195727109909055
Epoch: 4918, Batch Gradient Norm: 10.084333051907134
Epoch: 4918, Batch Gradient Norm after: 10.084333051907134
Epoch 4919/10000, Prediction Accuracy = 62.024%, Loss = 0.4441299080848694
Epoch: 4919, Batch Gradient Norm: 9.185778692429318
Epoch: 4919, Batch Gradient Norm after: 9.185778692429318
Epoch 4920/10000, Prediction Accuracy = 62.062%, Loss = 0.4394995033740997
Epoch: 4920, Batch Gradient Norm: 9.426757712058173
Epoch: 4920, Batch Gradient Norm after: 9.426757712058173
Epoch 4921/10000, Prediction Accuracy = 62.028%, Loss = 0.4414010226726532
Epoch: 4921, Batch Gradient Norm: 10.312791490417393
Epoch: 4921, Batch Gradient Norm after: 10.312791490417393
Epoch 4922/10000, Prediction Accuracy = 62.056%, Loss = 0.44594561457633974
Epoch: 4922, Batch Gradient Norm: 13.001831153361428
Epoch: 4922, Batch Gradient Norm after: 13.001831153361428
Epoch 4923/10000, Prediction Accuracy = 62.086%, Loss = 0.46563910841941836
Epoch: 4923, Batch Gradient Norm: 12.702440902001701
Epoch: 4923, Batch Gradient Norm after: 12.702440902001701
Epoch 4924/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.46447243094444274
Epoch: 4924, Batch Gradient Norm: 8.940867193472469
Epoch: 4924, Batch Gradient Norm after: 8.940867193472469
Epoch 4925/10000, Prediction Accuracy = 62.098%, Loss = 0.4383219420909882
Epoch: 4925, Batch Gradient Norm: 7.347656692852953
Epoch: 4925, Batch Gradient Norm after: 7.347656692852953
Epoch 4926/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.4281680703163147
Epoch: 4926, Batch Gradient Norm: 8.194910015533953
Epoch: 4926, Batch Gradient Norm after: 8.194910015533953
Epoch 4927/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.43160369992256165
Epoch: 4927, Batch Gradient Norm: 10.251687024191899
Epoch: 4927, Batch Gradient Norm after: 10.251687024191899
Epoch 4928/10000, Prediction Accuracy = 61.986000000000004%, Loss = 0.4431294858455658
Epoch: 4928, Batch Gradient Norm: 11.693814811339452
Epoch: 4928, Batch Gradient Norm after: 11.693814811339452
Epoch 4929/10000, Prediction Accuracy = 62.126%, Loss = 0.45413156151771544
Epoch: 4929, Batch Gradient Norm: 12.21454341717083
Epoch: 4929, Batch Gradient Norm after: 12.21454341717083
Epoch 4930/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.4612078249454498
Epoch: 4930, Batch Gradient Norm: 9.708404664257326
Epoch: 4930, Batch Gradient Norm after: 9.708404664257326
Epoch 4931/10000, Prediction Accuracy = 62.076%, Loss = 0.44406627416610717
Epoch: 4931, Batch Gradient Norm: 7.909530875046065
Epoch: 4931, Batch Gradient Norm after: 7.909530875046065
Epoch 4932/10000, Prediction Accuracy = 62.124%, Loss = 0.4323213636875153
Epoch: 4932, Batch Gradient Norm: 9.556799443935503
Epoch: 4932, Batch Gradient Norm after: 9.556799443935503
Epoch 4933/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4423598349094391
Epoch: 4933, Batch Gradient Norm: 10.667468161476206
Epoch: 4933, Batch Gradient Norm after: 10.667468161476206
Epoch 4934/10000, Prediction Accuracy = 62.02%, Loss = 0.44945340156555175
Epoch: 4934, Batch Gradient Norm: 11.40936925106948
Epoch: 4934, Batch Gradient Norm after: 11.40936925106948
Epoch 4935/10000, Prediction Accuracy = 62.04599999999999%, Loss = 0.45411282777786255
Epoch: 4935, Batch Gradient Norm: 11.414631339342415
Epoch: 4935, Batch Gradient Norm after: 11.414631339342415
Epoch 4936/10000, Prediction Accuracy = 62.029999999999994%, Loss = 0.4524101197719574
Epoch: 4936, Batch Gradient Norm: 10.629956029897583
Epoch: 4936, Batch Gradient Norm after: 10.629956029897583
Epoch 4937/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4467278242111206
Epoch: 4937, Batch Gradient Norm: 9.9164940436805
Epoch: 4937, Batch Gradient Norm after: 9.9164940436805
Epoch 4938/10000, Prediction Accuracy = 62.064%, Loss = 0.4429800808429718
Epoch: 4938, Batch Gradient Norm: 8.30669511621315
Epoch: 4938, Batch Gradient Norm after: 8.30669511621315
Epoch 4939/10000, Prediction Accuracy = 62.117999999999995%, Loss = 0.4329860031604767
Epoch: 4939, Batch Gradient Norm: 10.209978724332819
Epoch: 4939, Batch Gradient Norm after: 10.209978724332819
Epoch 4940/10000, Prediction Accuracy = 62.00600000000001%, Loss = 0.44473757147789
Epoch: 4940, Batch Gradient Norm: 12.04893659690107
Epoch: 4940, Batch Gradient Norm after: 12.04893659690107
Epoch 4941/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.4580835819244385
Epoch: 4941, Batch Gradient Norm: 10.186448650298107
Epoch: 4941, Batch Gradient Norm after: 10.186448650298107
Epoch 4942/10000, Prediction Accuracy = 61.972%, Loss = 0.4454840123653412
Epoch: 4942, Batch Gradient Norm: 8.39727072839063
Epoch: 4942, Batch Gradient Norm after: 8.39727072839063
Epoch 4943/10000, Prediction Accuracy = 62.128%, Loss = 0.43394066095352174
Epoch: 4943, Batch Gradient Norm: 9.098686313297796
Epoch: 4943, Batch Gradient Norm after: 9.098686313297796
Epoch 4944/10000, Prediction Accuracy = 62.124%, Loss = 0.4369239151477814
Epoch: 4944, Batch Gradient Norm: 10.384896872455139
Epoch: 4944, Batch Gradient Norm after: 10.384896872455139
Epoch 4945/10000, Prediction Accuracy = 62.036%, Loss = 0.445018070936203
Epoch: 4945, Batch Gradient Norm: 10.451436203690875
Epoch: 4945, Batch Gradient Norm after: 10.451436203690875
Epoch 4946/10000, Prediction Accuracy = 61.944%, Loss = 0.4454736351966858
Epoch: 4946, Batch Gradient Norm: 9.503443872666129
Epoch: 4946, Batch Gradient Norm after: 9.503443872666129
Epoch 4947/10000, Prediction Accuracy = 62.15400000000001%, Loss = 0.4397709727287292
Epoch: 4947, Batch Gradient Norm: 9.746546257791472
Epoch: 4947, Batch Gradient Norm after: 9.746546257791472
Epoch 4948/10000, Prediction Accuracy = 61.967999999999996%, Loss = 0.44320993423461913
Epoch: 4948, Batch Gradient Norm: 8.307682159876045
Epoch: 4948, Batch Gradient Norm after: 8.307682159876045
Epoch 4949/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.433852756023407
Epoch: 4949, Batch Gradient Norm: 9.323909930602131
Epoch: 4949, Batch Gradient Norm after: 9.323909930602131
Epoch 4950/10000, Prediction Accuracy = 62.17%, Loss = 0.43863622546195985
Epoch: 4950, Batch Gradient Norm: 11.58122381539379
Epoch: 4950, Batch Gradient Norm after: 11.58122381539379
Epoch 4951/10000, Prediction Accuracy = 61.992%, Loss = 0.45450331568717955
Epoch: 4951, Batch Gradient Norm: 11.483982004633468
Epoch: 4951, Batch Gradient Norm after: 11.483982004633468
Epoch 4952/10000, Prediction Accuracy = 62.174%, Loss = 0.45501450896263124
Epoch: 4952, Batch Gradient Norm: 10.133426161587384
Epoch: 4952, Batch Gradient Norm after: 10.133426161587384
Epoch 4953/10000, Prediction Accuracy = 62.024%, Loss = 0.4446308195590973
Epoch: 4953, Batch Gradient Norm: 11.73713104426142
Epoch: 4953, Batch Gradient Norm after: 11.73713104426142
Epoch 4954/10000, Prediction Accuracy = 62.174%, Loss = 0.45437774658203123
Epoch: 4954, Batch Gradient Norm: 11.597235758438226
Epoch: 4954, Batch Gradient Norm after: 11.597235758438226
Epoch 4955/10000, Prediction Accuracy = 62.068%, Loss = 0.45374605655670164
Epoch: 4955, Batch Gradient Norm: 9.208025607908915
Epoch: 4955, Batch Gradient Norm after: 9.208025607908915
Epoch 4956/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4379047930240631
Epoch: 4956, Batch Gradient Norm: 7.826988269482762
Epoch: 4956, Batch Gradient Norm after: 7.826988269482762
Epoch 4957/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.43016418218612673
Epoch: 4957, Batch Gradient Norm: 9.296467531212805
Epoch: 4957, Batch Gradient Norm after: 9.296467531212805
Epoch 4958/10000, Prediction Accuracy = 62.15%, Loss = 0.4391172707080841
Epoch: 4958, Batch Gradient Norm: 10.874814928324886
Epoch: 4958, Batch Gradient Norm after: 10.874814928324886
Epoch 4959/10000, Prediction Accuracy = 62.053999999999995%, Loss = 0.45070184469223024
Epoch: 4959, Batch Gradient Norm: 9.412067257892737
Epoch: 4959, Batch Gradient Norm after: 9.412067257892737
Epoch 4960/10000, Prediction Accuracy = 62.098%, Loss = 0.4398437738418579
Epoch: 4960, Batch Gradient Norm: 8.584282970108045
Epoch: 4960, Batch Gradient Norm after: 8.584282970108045
Epoch 4961/10000, Prediction Accuracy = 61.992%, Loss = 0.4349940061569214
Epoch: 4961, Batch Gradient Norm: 8.85068146543388
Epoch: 4961, Batch Gradient Norm after: 8.85068146543388
Epoch 4962/10000, Prediction Accuracy = 62.124%, Loss = 0.4355547368526459
Epoch: 4962, Batch Gradient Norm: 11.225867820311045
Epoch: 4962, Batch Gradient Norm after: 11.225867820311045
Epoch 4963/10000, Prediction Accuracy = 61.996%, Loss = 0.4513692080974579
Epoch: 4963, Batch Gradient Norm: 11.455004307634276
Epoch: 4963, Batch Gradient Norm after: 11.455004307634276
Epoch 4964/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4542184412479401
Epoch: 4964, Batch Gradient Norm: 9.142374090487943
Epoch: 4964, Batch Gradient Norm after: 9.142374090487943
Epoch 4965/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4375829815864563
Epoch: 4965, Batch Gradient Norm: 10.351417925274404
Epoch: 4965, Batch Gradient Norm after: 10.351417925274404
Epoch 4966/10000, Prediction Accuracy = 62.088%, Loss = 0.4468082070350647
Epoch: 4966, Batch Gradient Norm: 9.906243842151722
Epoch: 4966, Batch Gradient Norm after: 9.906243842151722
Epoch 4967/10000, Prediction Accuracy = 62.036%, Loss = 0.4439315557479858
Epoch: 4967, Batch Gradient Norm: 13.008559640299369
Epoch: 4967, Batch Gradient Norm after: 13.008559640299369
Epoch 4968/10000, Prediction Accuracy = 62.134%, Loss = 0.46396649479866026
Epoch: 4968, Batch Gradient Norm: 13.08221480386796
Epoch: 4968, Batch Gradient Norm after: 13.08221480386796
Epoch 4969/10000, Prediction Accuracy = 62.024%, Loss = 0.46476895213127134
Epoch: 4969, Batch Gradient Norm: 8.23422093473518
Epoch: 4969, Batch Gradient Norm after: 8.23422093473518
Epoch 4970/10000, Prediction Accuracy = 62.198%, Loss = 0.4321347177028656
Epoch: 4970, Batch Gradient Norm: 6.588553562647822
Epoch: 4970, Batch Gradient Norm after: 6.588553562647822
Epoch 4971/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.42397941946983336
Epoch: 4971, Batch Gradient Norm: 7.083574639532276
Epoch: 4971, Batch Gradient Norm after: 7.083574639532276
Epoch 4972/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.42575869560241697
Epoch: 4972, Batch Gradient Norm: 10.477324023763082
Epoch: 4972, Batch Gradient Norm after: 10.477324023763082
Epoch 4973/10000, Prediction Accuracy = 62.126%, Loss = 0.4461676299571991
Epoch: 4973, Batch Gradient Norm: 12.253863713853292
Epoch: 4973, Batch Gradient Norm after: 12.253863713853292
Epoch 4974/10000, Prediction Accuracy = 62.086%, Loss = 0.45941746830940244
Epoch: 4974, Batch Gradient Norm: 11.604310660494626
Epoch: 4974, Batch Gradient Norm after: 11.604310660494626
Epoch 4975/10000, Prediction Accuracy = 62.126%, Loss = 0.45380815863609314
Epoch: 4975, Batch Gradient Norm: 9.239632537362503
Epoch: 4975, Batch Gradient Norm after: 9.239632537362503
Epoch 4976/10000, Prediction Accuracy = 62.06999999999999%, Loss = 0.4378455340862274
Epoch: 4976, Batch Gradient Norm: 8.595541745209765
Epoch: 4976, Batch Gradient Norm after: 8.595541745209765
Epoch 4977/10000, Prediction Accuracy = 62.152%, Loss = 0.4335803985595703
Epoch: 4977, Batch Gradient Norm: 10.490076947483725
Epoch: 4977, Batch Gradient Norm after: 10.490076947483725
Epoch 4978/10000, Prediction Accuracy = 61.998000000000005%, Loss = 0.4460960805416107
Epoch: 4978, Batch Gradient Norm: 10.956774253773402
Epoch: 4978, Batch Gradient Norm after: 10.956774253773402
Epoch 4979/10000, Prediction Accuracy = 62.068%, Loss = 0.4498192310333252
Epoch: 4979, Batch Gradient Norm: 9.859532410579765
Epoch: 4979, Batch Gradient Norm after: 9.859532410579765
Epoch 4980/10000, Prediction Accuracy = 62.104%, Loss = 0.44027845859527587
Epoch: 4980, Batch Gradient Norm: 10.115313599416519
Epoch: 4980, Batch Gradient Norm after: 10.115313599416519
Epoch 4981/10000, Prediction Accuracy = 62.202%, Loss = 0.44196982979774474
Epoch: 4981, Batch Gradient Norm: 10.190194219882551
Epoch: 4981, Batch Gradient Norm after: 10.190194219882551
Epoch 4982/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.44358119964599607
Epoch: 4982, Batch Gradient Norm: 9.825194379497278
Epoch: 4982, Batch Gradient Norm after: 9.825194379497278
Epoch 4983/10000, Prediction Accuracy = 62.162%, Loss = 0.4425550639629364
Epoch: 4983, Batch Gradient Norm: 9.361673185454293
Epoch: 4983, Batch Gradient Norm after: 9.361673185454293
Epoch 4984/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4392751812934875
Epoch: 4984, Batch Gradient Norm: 10.001596220293868
Epoch: 4984, Batch Gradient Norm after: 10.001596220293868
Epoch 4985/10000, Prediction Accuracy = 62.062%, Loss = 0.44074193835258485
Epoch: 4985, Batch Gradient Norm: 10.95072751441663
Epoch: 4985, Batch Gradient Norm after: 10.95072751441663
Epoch 4986/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.44832106232643126
Epoch: 4986, Batch Gradient Norm: 9.885972886727608
Epoch: 4986, Batch Gradient Norm after: 9.885972886727608
Epoch 4987/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.4408364355564117
Epoch: 4987, Batch Gradient Norm: 11.020412266142259
Epoch: 4987, Batch Gradient Norm after: 11.020412266142259
Epoch 4988/10000, Prediction Accuracy = 62.15400000000001%, Loss = 0.44867451786994933
Epoch: 4988, Batch Gradient Norm: 11.94972780564483
Epoch: 4988, Batch Gradient Norm after: 11.94972780564483
Epoch 4989/10000, Prediction Accuracy = 62.072%, Loss = 0.45581090450286865
Epoch: 4989, Batch Gradient Norm: 9.9613088104682
Epoch: 4989, Batch Gradient Norm after: 9.9613088104682
Epoch 4990/10000, Prediction Accuracy = 62.076%, Loss = 0.4436378538608551
Epoch: 4990, Batch Gradient Norm: 8.156852477174722
Epoch: 4990, Batch Gradient Norm after: 8.156852477174722
Epoch 4991/10000, Prediction Accuracy = 62.212%, Loss = 0.43156808614730835
Epoch: 4991, Batch Gradient Norm: 10.180382484160821
Epoch: 4991, Batch Gradient Norm after: 10.180382484160821
Epoch 4992/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.4424517273902893
Epoch: 4992, Batch Gradient Norm: 12.045918471924205
Epoch: 4992, Batch Gradient Norm after: 12.045918471924205
Epoch 4993/10000, Prediction Accuracy = 62.17%, Loss = 0.45564736127853395
Epoch: 4993, Batch Gradient Norm: 10.405718839666383
Epoch: 4993, Batch Gradient Norm after: 10.405718839666383
Epoch 4994/10000, Prediction Accuracy = 61.99400000000001%, Loss = 0.444439560174942
Epoch: 4994, Batch Gradient Norm: 10.117467032811014
Epoch: 4994, Batch Gradient Norm after: 10.117467032811014
Epoch 4995/10000, Prediction Accuracy = 62.11%, Loss = 0.4426743507385254
Epoch: 4995, Batch Gradient Norm: 11.06716276431272
Epoch: 4995, Batch Gradient Norm after: 11.06716276431272
Epoch 4996/10000, Prediction Accuracy = 61.996%, Loss = 0.449980366230011
Epoch: 4996, Batch Gradient Norm: 9.69205546811896
Epoch: 4996, Batch Gradient Norm after: 9.69205546811896
Epoch 4997/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.44032858610153197
Epoch: 4997, Batch Gradient Norm: 10.16011940530728
Epoch: 4997, Batch Gradient Norm after: 10.16011940530728
Epoch 4998/10000, Prediction Accuracy = 62.048%, Loss = 0.44375401735305786
Epoch: 4998, Batch Gradient Norm: 9.890418374173398
Epoch: 4998, Batch Gradient Norm after: 9.890418374173398
Epoch 4999/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.44184811115264894
Epoch: 4999, Batch Gradient Norm: 8.745198365310593
Epoch: 4999, Batch Gradient Norm after: 8.745198365310593
Epoch 5000/10000, Prediction Accuracy = 62.056%, Loss = 0.43476619124412536
Epoch: 5000, Batch Gradient Norm: 8.02499490762147
Epoch: 5000, Batch Gradient Norm after: 8.02499490762147
Epoch 5001/10000, Prediction Accuracy = 62.17%, Loss = 0.4295628786087036
Epoch: 5001, Batch Gradient Norm: 9.655005935155824
Epoch: 5001, Batch Gradient Norm after: 9.655005935155824
Epoch 5002/10000, Prediction Accuracy = 62.108000000000004%, Loss = 0.43895106911659243
Epoch: 5002, Batch Gradient Norm: 11.715560063756753
Epoch: 5002, Batch Gradient Norm after: 11.715560063756753
Epoch 5003/10000, Prediction Accuracy = 62.072%, Loss = 0.4562087655067444
Epoch: 5003, Batch Gradient Norm: 9.787854781873307
Epoch: 5003, Batch Gradient Norm after: 9.787854781873307
Epoch 5004/10000, Prediction Accuracy = 62.262%, Loss = 0.44181511998176576
Epoch: 5004, Batch Gradient Norm: 8.713953179935821
Epoch: 5004, Batch Gradient Norm after: 8.713953179935821
Epoch 5005/10000, Prediction Accuracy = 62.1%, Loss = 0.4339791238307953
Epoch: 5005, Batch Gradient Norm: 11.172053489722543
Epoch: 5005, Batch Gradient Norm after: 11.172053489722543
Epoch 5006/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.4483835756778717
Epoch: 5006, Batch Gradient Norm: 11.590173293959463
Epoch: 5006, Batch Gradient Norm after: 11.590173293959463
Epoch 5007/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.4518758416175842
Epoch: 5007, Batch Gradient Norm: 9.688442532710432
Epoch: 5007, Batch Gradient Norm after: 9.688442532710432
Epoch 5008/10000, Prediction Accuracy = 62.138%, Loss = 0.43888097405433657
Epoch: 5008, Batch Gradient Norm: 8.906459588585598
Epoch: 5008, Batch Gradient Norm after: 8.906459588585598
Epoch 5009/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4340933322906494
Epoch: 5009, Batch Gradient Norm: 8.82208994368607
Epoch: 5009, Batch Gradient Norm after: 8.82208994368607
Epoch 5010/10000, Prediction Accuracy = 62.158%, Loss = 0.43407312631607053
Epoch: 5010, Batch Gradient Norm: 10.604513866349109
Epoch: 5010, Batch Gradient Norm after: 10.604513866349109
Epoch 5011/10000, Prediction Accuracy = 62.174%, Loss = 0.4473883450031281
Epoch: 5011, Batch Gradient Norm: 10.225342735857936
Epoch: 5011, Batch Gradient Norm after: 10.225342735857936
Epoch 5012/10000, Prediction Accuracy = 62.162%, Loss = 0.4462529718875885
Epoch: 5012, Batch Gradient Norm: 8.91168073682224
Epoch: 5012, Batch Gradient Norm after: 8.91168073682224
Epoch 5013/10000, Prediction Accuracy = 62.068%, Loss = 0.43615904450416565
Epoch: 5013, Batch Gradient Norm: 9.673279403700535
Epoch: 5013, Batch Gradient Norm after: 9.673279403700535
Epoch 5014/10000, Prediction Accuracy = 62.17%, Loss = 0.4389537930488586
Epoch: 5014, Batch Gradient Norm: 11.159534346999319
Epoch: 5014, Batch Gradient Norm after: 11.159534346999319
Epoch 5015/10000, Prediction Accuracy = 62.089999999999996%, Loss = 0.4492115080356598
Epoch: 5015, Batch Gradient Norm: 10.991062002578323
Epoch: 5015, Batch Gradient Norm after: 10.991062002578323
Epoch 5016/10000, Prediction Accuracy = 62.001999999999995%, Loss = 0.45029814839363097
Epoch: 5016, Batch Gradient Norm: 9.625580742872707
Epoch: 5016, Batch Gradient Norm after: 9.625580742872707
Epoch 5017/10000, Prediction Accuracy = 62.044000000000004%, Loss = 0.43994398713111876
Epoch: 5017, Batch Gradient Norm: 10.309336745743787
Epoch: 5017, Batch Gradient Norm after: 10.309336745743787
Epoch 5018/10000, Prediction Accuracy = 62.038%, Loss = 0.4432352602481842
Epoch: 5018, Batch Gradient Norm: 11.223259243186897
Epoch: 5018, Batch Gradient Norm after: 11.223259243186897
Epoch 5019/10000, Prediction Accuracy = 62.186%, Loss = 0.44921897649765014
Epoch: 5019, Batch Gradient Norm: 10.143306215367199
Epoch: 5019, Batch Gradient Norm after: 10.143306215367199
Epoch 5020/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.4421635687351227
Epoch: 5020, Batch Gradient Norm: 8.662776684830682
Epoch: 5020, Batch Gradient Norm after: 8.662776684830682
Epoch 5021/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4348181426525116
Epoch: 5021, Batch Gradient Norm: 7.688503488528075
Epoch: 5021, Batch Gradient Norm after: 7.688503488528075
Epoch 5022/10000, Prediction Accuracy = 62.06%, Loss = 0.42912672758102416
Epoch: 5022, Batch Gradient Norm: 8.946984820671556
Epoch: 5022, Batch Gradient Norm after: 8.946984820671556
Epoch 5023/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.43638123869895934
Epoch: 5023, Batch Gradient Norm: 11.052850067326439
Epoch: 5023, Batch Gradient Norm after: 11.052850067326439
Epoch 5024/10000, Prediction Accuracy = 62.157999999999994%, Loss = 0.4481533467769623
Epoch: 5024, Batch Gradient Norm: 13.182596124178618
Epoch: 5024, Batch Gradient Norm after: 13.182596124178618
Epoch 5025/10000, Prediction Accuracy = 62.056%, Loss = 0.46360752582550047
Epoch: 5025, Batch Gradient Norm: 10.993958589620256
Epoch: 5025, Batch Gradient Norm after: 10.993958589620256
Epoch 5026/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4499685764312744
Epoch: 5026, Batch Gradient Norm: 9.293518330134
Epoch: 5026, Batch Gradient Norm after: 9.293518330134
Epoch 5027/10000, Prediction Accuracy = 62.14399999999999%, Loss = 0.43910512924194334
Epoch: 5027, Batch Gradient Norm: 10.223291562156604
Epoch: 5027, Batch Gradient Norm after: 10.223291562156604
Epoch 5028/10000, Prediction Accuracy = 62.11%, Loss = 0.44258623719215395
Epoch: 5028, Batch Gradient Norm: 12.06433618573437
Epoch: 5028, Batch Gradient Norm after: 12.06433618573437
Epoch 5029/10000, Prediction Accuracy = 62.186%, Loss = 0.4542842507362366
Epoch: 5029, Batch Gradient Norm: 11.90965082991649
Epoch: 5029, Batch Gradient Norm after: 11.90965082991649
Epoch 5030/10000, Prediction Accuracy = 62.080000000000005%, Loss = 0.4530586302280426
Epoch: 5030, Batch Gradient Norm: 8.934795012545953
Epoch: 5030, Batch Gradient Norm after: 8.934795012545953
Epoch 5031/10000, Prediction Accuracy = 62.112%, Loss = 0.43369768261909486
Epoch: 5031, Batch Gradient Norm: 7.9066561884136295
Epoch: 5031, Batch Gradient Norm after: 7.9066561884136295
Epoch 5032/10000, Prediction Accuracy = 62.178%, Loss = 0.4280888795852661
Epoch: 5032, Batch Gradient Norm: 9.039351219498021
Epoch: 5032, Batch Gradient Norm after: 9.039351219498021
Epoch 5033/10000, Prediction Accuracy = 62.152%, Loss = 0.4336871922016144
Epoch: 5033, Batch Gradient Norm: 11.192577023200355
Epoch: 5033, Batch Gradient Norm after: 11.192577023200355
Epoch 5034/10000, Prediction Accuracy = 62.162%, Loss = 0.4497023046016693
Epoch: 5034, Batch Gradient Norm: 9.435027523209813
Epoch: 5034, Batch Gradient Norm after: 9.435027523209813
Epoch 5035/10000, Prediction Accuracy = 62.012%, Loss = 0.43832011222839357
Epoch: 5035, Batch Gradient Norm: 10.45943744048976
Epoch: 5035, Batch Gradient Norm after: 10.45943744048976
Epoch 5036/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4432248532772064
Epoch: 5036, Batch Gradient Norm: 12.404369639723473
Epoch: 5036, Batch Gradient Norm after: 12.404369639723473
Epoch 5037/10000, Prediction Accuracy = 61.91400000000001%, Loss = 0.4583312749862671
Epoch: 5037, Batch Gradient Norm: 9.572846781024113
Epoch: 5037, Batch Gradient Norm after: 9.572846781024113
Epoch 5038/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4383806049823761
Epoch: 5038, Batch Gradient Norm: 7.997468482472979
Epoch: 5038, Batch Gradient Norm after: 7.997468482472979
Epoch 5039/10000, Prediction Accuracy = 62.041999999999994%, Loss = 0.4290665566921234
Epoch: 5039, Batch Gradient Norm: 8.595420755319005
Epoch: 5039, Batch Gradient Norm after: 8.595420755319005
Epoch 5040/10000, Prediction Accuracy = 62.194%, Loss = 0.43333345651626587
Epoch: 5040, Batch Gradient Norm: 10.058701113976461
Epoch: 5040, Batch Gradient Norm after: 10.058701113976461
Epoch 5041/10000, Prediction Accuracy = 61.965999999999994%, Loss = 0.44305182099342344
Epoch: 5041, Batch Gradient Norm: 11.453825292984728
Epoch: 5041, Batch Gradient Norm after: 11.453825292984728
Epoch 5042/10000, Prediction Accuracy = 62.162%, Loss = 0.45381641387939453
Epoch: 5042, Batch Gradient Norm: 9.518071375603503
Epoch: 5042, Batch Gradient Norm after: 9.518071375603503
Epoch 5043/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.4395332455635071
Epoch: 5043, Batch Gradient Norm: 7.890696665575165
Epoch: 5043, Batch Gradient Norm after: 7.890696665575165
Epoch 5044/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.42885430455207824
Epoch: 5044, Batch Gradient Norm: 8.923103123332156
Epoch: 5044, Batch Gradient Norm after: 8.923103123332156
Epoch 5045/10000, Prediction Accuracy = 62.102%, Loss = 0.4335855722427368
Epoch: 5045, Batch Gradient Norm: 10.647523366740613
Epoch: 5045, Batch Gradient Norm after: 10.647523366740613
Epoch 5046/10000, Prediction Accuracy = 62.0%, Loss = 0.4438701272010803
Epoch: 5046, Batch Gradient Norm: 11.588911824184876
Epoch: 5046, Batch Gradient Norm after: 11.588911824184876
Epoch 5047/10000, Prediction Accuracy = 62.098%, Loss = 0.45073904991149905
Epoch: 5047, Batch Gradient Norm: 11.849134937086728
Epoch: 5047, Batch Gradient Norm after: 11.849134937086728
Epoch 5048/10000, Prediction Accuracy = 62.105999999999995%, Loss = 0.4561409592628479
Epoch: 5048, Batch Gradient Norm: 9.101335336039833
Epoch: 5048, Batch Gradient Norm after: 9.101335336039833
Epoch 5049/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.4371587038040161
Epoch: 5049, Batch Gradient Norm: 8.754953909726238
Epoch: 5049, Batch Gradient Norm after: 8.754953909726238
Epoch 5050/10000, Prediction Accuracy = 62.126%, Loss = 0.4333750128746033
Epoch: 5050, Batch Gradient Norm: 11.114411381293499
Epoch: 5050, Batch Gradient Norm after: 11.114411381293499
Epoch 5051/10000, Prediction Accuracy = 62.098%, Loss = 0.4477032482624054
Epoch: 5051, Batch Gradient Norm: 11.515834901088157
Epoch: 5051, Batch Gradient Norm after: 11.515834901088157
Epoch 5052/10000, Prediction Accuracy = 62.166%, Loss = 0.4502145528793335
Epoch: 5052, Batch Gradient Norm: 9.417973782500278
Epoch: 5052, Batch Gradient Norm after: 9.417973782500278
Epoch 5053/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.43555768132209777
Epoch: 5053, Batch Gradient Norm: 9.320455514481026
Epoch: 5053, Batch Gradient Norm after: 9.320455514481026
Epoch 5054/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4351455569267273
Epoch: 5054, Batch Gradient Norm: 9.37794811174768
Epoch: 5054, Batch Gradient Norm after: 9.37794811174768
Epoch 5055/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4360750138759613
Epoch: 5055, Batch Gradient Norm: 9.86786894474475
Epoch: 5055, Batch Gradient Norm after: 9.86786894474475
Epoch 5056/10000, Prediction Accuracy = 62.11600000000001%, Loss = 0.4396828353404999
Epoch: 5056, Batch Gradient Norm: 12.034169895933388
Epoch: 5056, Batch Gradient Norm after: 12.034169895933388
Epoch 5057/10000, Prediction Accuracy = 62.114%, Loss = 0.4565862476825714
Epoch: 5057, Batch Gradient Norm: 12.119734740530635
Epoch: 5057, Batch Gradient Norm after: 12.119734740530635
Epoch 5058/10000, Prediction Accuracy = 62.03000000000001%, Loss = 0.45754384994506836
Epoch: 5058, Batch Gradient Norm: 9.184818235653804
Epoch: 5058, Batch Gradient Norm after: 9.184818235653804
Epoch 5059/10000, Prediction Accuracy = 62.188%, Loss = 0.4358530879020691
Epoch: 5059, Batch Gradient Norm: 7.843463469428374
Epoch: 5059, Batch Gradient Norm after: 7.843463469428374
Epoch 5060/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4270179748535156
Epoch: 5060, Batch Gradient Norm: 10.820111467503555
Epoch: 5060, Batch Gradient Norm after: 10.820111467503555
Epoch 5061/10000, Prediction Accuracy = 62.05800000000001%, Loss = 0.44644300937652587
Epoch: 5061, Batch Gradient Norm: 10.598179780610426
Epoch: 5061, Batch Gradient Norm after: 10.598179780610426
Epoch 5062/10000, Prediction Accuracy = 62.05%, Loss = 0.44639517068862916
Epoch: 5062, Batch Gradient Norm: 9.709370718373474
Epoch: 5062, Batch Gradient Norm after: 9.709370718373474
Epoch 5063/10000, Prediction Accuracy = 62.212%, Loss = 0.4383844792842865
Epoch: 5063, Batch Gradient Norm: 11.134905096849552
Epoch: 5063, Batch Gradient Norm after: 11.134905096849552
Epoch 5064/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.44780671000480654
Epoch: 5064, Batch Gradient Norm: 11.119738216733607
Epoch: 5064, Batch Gradient Norm after: 11.119738216733607
Epoch 5065/10000, Prediction Accuracy = 62.21%, Loss = 0.44908222556114197
Epoch: 5065, Batch Gradient Norm: 9.9250269910144
Epoch: 5065, Batch Gradient Norm after: 9.9250269910144
Epoch 5066/10000, Prediction Accuracy = 62.074%, Loss = 0.4411328434944153
Epoch: 5066, Batch Gradient Norm: 9.220419594428703
Epoch: 5066, Batch Gradient Norm after: 9.220419594428703
Epoch 5067/10000, Prediction Accuracy = 62.226%, Loss = 0.43415250778198244
Epoch: 5067, Batch Gradient Norm: 11.35244052091516
Epoch: 5067, Batch Gradient Norm after: 11.35244052091516
Epoch 5068/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.44729982018470765
Epoch: 5068, Batch Gradient Norm: 10.769178565609513
Epoch: 5068, Batch Gradient Norm after: 10.769178565609513
Epoch 5069/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.4432170808315277
Epoch: 5069, Batch Gradient Norm: 10.271158495375305
Epoch: 5069, Batch Gradient Norm after: 10.271158495375305
Epoch 5070/10000, Prediction Accuracy = 62.10799999999999%, Loss = 0.439967679977417
Epoch: 5070, Batch Gradient Norm: 11.413321180744795
Epoch: 5070, Batch Gradient Norm after: 11.413321180744795
Epoch 5071/10000, Prediction Accuracy = 62.114%, Loss = 0.44974254369735717
Epoch: 5071, Batch Gradient Norm: 10.557415337714753
Epoch: 5071, Batch Gradient Norm after: 10.557415337714753
Epoch 5072/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4451893329620361
Epoch: 5072, Batch Gradient Norm: 8.216194806556556
Epoch: 5072, Batch Gradient Norm after: 8.216194806556556
Epoch 5073/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.42939000725746157
Epoch: 5073, Batch Gradient Norm: 7.872280509226711
Epoch: 5073, Batch Gradient Norm after: 7.872280509226711
Epoch 5074/10000, Prediction Accuracy = 62.14%, Loss = 0.4269994556903839
Epoch: 5074, Batch Gradient Norm: 9.67499780017586
Epoch: 5074, Batch Gradient Norm after: 9.67499780017586
Epoch 5075/10000, Prediction Accuracy = 62.062%, Loss = 0.43839900493621825
Epoch: 5075, Batch Gradient Norm: 9.133655086762051
Epoch: 5075, Batch Gradient Norm after: 9.133655086762051
Epoch 5076/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.43812826871871946
Epoch: 5076, Batch Gradient Norm: 6.418069973304933
Epoch: 5076, Batch Gradient Norm after: 6.418069973304933
Epoch 5077/10000, Prediction Accuracy = 62.158%, Loss = 0.4220064342021942
Epoch: 5077, Batch Gradient Norm: 7.776510897818528
Epoch: 5077, Batch Gradient Norm after: 7.776510897818528
Epoch 5078/10000, Prediction Accuracy = 62.19%, Loss = 0.4272158741950989
Epoch: 5078, Batch Gradient Norm: 9.948372259119418
Epoch: 5078, Batch Gradient Norm after: 9.948372259119418
Epoch 5079/10000, Prediction Accuracy = 62.194%, Loss = 0.43912304639816285
Epoch: 5079, Batch Gradient Norm: 12.260196331808396
Epoch: 5079, Batch Gradient Norm after: 12.260196331808396
Epoch 5080/10000, Prediction Accuracy = 62.222%, Loss = 0.4553373396396637
Epoch: 5080, Batch Gradient Norm: 11.416612772744182
Epoch: 5080, Batch Gradient Norm after: 11.416612772744182
Epoch 5081/10000, Prediction Accuracy = 62.092000000000006%, Loss = 0.44918540120124817
Epoch: 5081, Batch Gradient Norm: 9.53080933130599
Epoch: 5081, Batch Gradient Norm after: 9.53080933130599
Epoch 5082/10000, Prediction Accuracy = 62.15%, Loss = 0.4369854390621185
Epoch: 5082, Batch Gradient Norm: 9.166186092919071
Epoch: 5082, Batch Gradient Norm after: 9.166186092919071
Epoch 5083/10000, Prediction Accuracy = 62.06%, Loss = 0.43475219011306765
Epoch: 5083, Batch Gradient Norm: 10.59529018000253
Epoch: 5083, Batch Gradient Norm after: 10.59529018000253
Epoch 5084/10000, Prediction Accuracy = 62.164%, Loss = 0.44359267950057985
Epoch: 5084, Batch Gradient Norm: 10.873216294134062
Epoch: 5084, Batch Gradient Norm after: 10.873216294134062
Epoch 5085/10000, Prediction Accuracy = 62.19%, Loss = 0.44688963890075684
Epoch: 5085, Batch Gradient Norm: 10.34074586801365
Epoch: 5085, Batch Gradient Norm after: 10.34074586801365
Epoch 5086/10000, Prediction Accuracy = 62.084%, Loss = 0.4455583393573761
Epoch: 5086, Batch Gradient Norm: 8.809151274328311
Epoch: 5086, Batch Gradient Norm after: 8.809151274328311
Epoch 5087/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.43401226997375486
Epoch: 5087, Batch Gradient Norm: 11.529009439125435
Epoch: 5087, Batch Gradient Norm after: 11.529009439125435
Epoch 5088/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.45047112703323366
Epoch: 5088, Batch Gradient Norm: 12.812758268852185
Epoch: 5088, Batch Gradient Norm after: 12.812758268852185
Epoch 5089/10000, Prediction Accuracy = 62.04%, Loss = 0.46235358715057373
Epoch: 5089, Batch Gradient Norm: 10.498422821251161
Epoch: 5089, Batch Gradient Norm after: 10.498422821251161
Epoch 5090/10000, Prediction Accuracy = 62.062%, Loss = 0.44351705312728884
Epoch: 5090, Batch Gradient Norm: 10.542213346292382
Epoch: 5090, Batch Gradient Norm after: 10.542213346292382
Epoch 5091/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4420927345752716
Epoch: 5091, Batch Gradient Norm: 10.915645954082965
Epoch: 5091, Batch Gradient Norm after: 10.915645954082965
Epoch 5092/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.44437155723571775
Epoch: 5092, Batch Gradient Norm: 10.467482870986615
Epoch: 5092, Batch Gradient Norm after: 10.467482870986615
Epoch 5093/10000, Prediction Accuracy = 62.234%, Loss = 0.4418108403682709
Epoch: 5093, Batch Gradient Norm: 11.226557704602085
Epoch: 5093, Batch Gradient Norm after: 11.226557704602085
Epoch 5094/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.4478662431240082
Epoch: 5094, Batch Gradient Norm: 10.723849591052812
Epoch: 5094, Batch Gradient Norm after: 10.723849591052812
Epoch 5095/10000, Prediction Accuracy = 61.89000000000001%, Loss = 0.44365306496620177
Epoch: 5095, Batch Gradient Norm: 9.973426220149369
Epoch: 5095, Batch Gradient Norm after: 9.973426220149369
Epoch 5096/10000, Prediction Accuracy = 62.116%, Loss = 0.43805171251297
Epoch: 5096, Batch Gradient Norm: 9.153524910474058
Epoch: 5096, Batch Gradient Norm after: 9.153524910474058
Epoch 5097/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4330678880214691
Epoch: 5097, Batch Gradient Norm: 8.785150832832482
Epoch: 5097, Batch Gradient Norm after: 8.785150832832482
Epoch 5098/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.4319331109523773
Epoch: 5098, Batch Gradient Norm: 7.994851566352995
Epoch: 5098, Batch Gradient Norm after: 7.994851566352995
Epoch 5099/10000, Prediction Accuracy = 62.10799999999999%, Loss = 0.4286131262779236
Epoch: 5099, Batch Gradient Norm: 7.311765834615615
Epoch: 5099, Batch Gradient Norm after: 7.311765834615615
Epoch 5100/10000, Prediction Accuracy = 62.198%, Loss = 0.4242704749107361
Epoch: 5100, Batch Gradient Norm: 10.601828025325062
Epoch: 5100, Batch Gradient Norm after: 10.601828025325062
Epoch 5101/10000, Prediction Accuracy = 62.052%, Loss = 0.4447313129901886
Epoch: 5101, Batch Gradient Norm: 10.250278682200644
Epoch: 5101, Batch Gradient Norm after: 10.250278682200644
Epoch 5102/10000, Prediction Accuracy = 62.14200000000001%, Loss = 0.4458994925022125
Epoch: 5102, Batch Gradient Norm: 7.347026652679949
Epoch: 5102, Batch Gradient Norm after: 7.347026652679949
Epoch 5103/10000, Prediction Accuracy = 62.112%, Loss = 0.42514927983283995
Epoch: 5103, Batch Gradient Norm: 8.027432364326371
Epoch: 5103, Batch Gradient Norm after: 8.027432364326371
Epoch 5104/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4270846962928772
Epoch: 5104, Batch Gradient Norm: 10.674160480402524
Epoch: 5104, Batch Gradient Norm after: 10.674160480402524
Epoch 5105/10000, Prediction Accuracy = 62.138%, Loss = 0.44209936261177063
Epoch: 5105, Batch Gradient Norm: 10.782429553071438
Epoch: 5105, Batch Gradient Norm after: 10.782429553071438
Epoch 5106/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.4432805418968201
Epoch: 5106, Batch Gradient Norm: 10.703884903330477
Epoch: 5106, Batch Gradient Norm after: 10.703884903330477
Epoch 5107/10000, Prediction Accuracy = 62.14399999999999%, Loss = 0.4434453725814819
Epoch: 5107, Batch Gradient Norm: 11.40107290966828
Epoch: 5107, Batch Gradient Norm after: 11.40107290966828
Epoch 5108/10000, Prediction Accuracy = 62.024%, Loss = 0.44889082908630373
Epoch: 5108, Batch Gradient Norm: 10.868549042486633
Epoch: 5108, Batch Gradient Norm after: 10.868549042486633
Epoch 5109/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.4448201894760132
Epoch: 5109, Batch Gradient Norm: 10.32502766091306
Epoch: 5109, Batch Gradient Norm after: 10.32502766091306
Epoch 5110/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.4409493148326874
Epoch: 5110, Batch Gradient Norm: 13.867703775402607
Epoch: 5110, Batch Gradient Norm after: 13.867703775402607
Epoch 5111/10000, Prediction Accuracy = 62.076%, Loss = 0.46986583471298216
Epoch: 5111, Batch Gradient Norm: 13.55463316966255
Epoch: 5111, Batch Gradient Norm after: 13.55463316966255
Epoch 5112/10000, Prediction Accuracy = 62.10799999999999%, Loss = 0.46837018728256224
Epoch: 5112, Batch Gradient Norm: 8.628468374204907
Epoch: 5112, Batch Gradient Norm after: 8.628468374204907
Epoch 5113/10000, Prediction Accuracy = 62.12399999999999%, Loss = 0.4315228521823883
Epoch: 5113, Batch Gradient Norm: 7.159257113436386
Epoch: 5113, Batch Gradient Norm after: 7.159257113436386
Epoch 5114/10000, Prediction Accuracy = 62.114%, Loss = 0.4234335541725159
Epoch: 5114, Batch Gradient Norm: 7.591760815116347
Epoch: 5114, Batch Gradient Norm after: 7.591760815116347
Epoch 5115/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.42610889077186587
Epoch: 5115, Batch Gradient Norm: 8.511515536410865
Epoch: 5115, Batch Gradient Norm after: 8.511515536410865
Epoch 5116/10000, Prediction Accuracy = 62.148%, Loss = 0.43122074007987976
Epoch: 5116, Batch Gradient Norm: 8.308964431793905
Epoch: 5116, Batch Gradient Norm after: 8.308964431793905
Epoch 5117/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.4301959753036499
Epoch: 5117, Batch Gradient Norm: 9.567323780157857
Epoch: 5117, Batch Gradient Norm after: 9.567323780157857
Epoch 5118/10000, Prediction Accuracy = 62.13000000000001%, Loss = 0.43783908486366274
Epoch: 5118, Batch Gradient Norm: 12.15992659765226
Epoch: 5118, Batch Gradient Norm after: 12.15992659765226
Epoch 5119/10000, Prediction Accuracy = 62.21%, Loss = 0.455556845664978
Epoch: 5119, Batch Gradient Norm: 11.050476210453862
Epoch: 5119, Batch Gradient Norm after: 11.050476210453862
Epoch 5120/10000, Prediction Accuracy = 62.136%, Loss = 0.4474427402019501
Epoch: 5120, Batch Gradient Norm: 10.19772155190326
Epoch: 5120, Batch Gradient Norm after: 10.19772155190326
Epoch 5121/10000, Prediction Accuracy = 62.164%, Loss = 0.4398318469524384
Epoch: 5121, Batch Gradient Norm: 11.354612473601636
Epoch: 5121, Batch Gradient Norm after: 11.354612473601636
Epoch 5122/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.44771830439567567
Epoch: 5122, Batch Gradient Norm: 10.447115443582753
Epoch: 5122, Batch Gradient Norm after: 10.447115443582753
Epoch 5123/10000, Prediction Accuracy = 62.1%, Loss = 0.4406287431716919
Epoch: 5123, Batch Gradient Norm: 10.8250035923673
Epoch: 5123, Batch Gradient Norm after: 10.8250035923673
Epoch 5124/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4428566038608551
Epoch: 5124, Batch Gradient Norm: 11.225852426717587
Epoch: 5124, Batch Gradient Norm after: 11.225852426717587
Epoch 5125/10000, Prediction Accuracy = 62.088%, Loss = 0.4461646258831024
Epoch: 5125, Batch Gradient Norm: 10.030566824297745
Epoch: 5125, Batch Gradient Norm after: 10.030566824297745
Epoch 5126/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.4383770048618317
Epoch: 5126, Batch Gradient Norm: 8.020397795980411
Epoch: 5126, Batch Gradient Norm after: 8.020397795980411
Epoch 5127/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.42644243240356444
Epoch: 5127, Batch Gradient Norm: 7.891425228917353
Epoch: 5127, Batch Gradient Norm after: 7.891425228917353
Epoch 5128/10000, Prediction Accuracy = 62.166%, Loss = 0.42611235976219175
Epoch: 5128, Batch Gradient Norm: 9.925380528490237
Epoch: 5128, Batch Gradient Norm after: 9.925380528490237
Epoch 5129/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.44047553539276124
Epoch: 5129, Batch Gradient Norm: 9.062355026955014
Epoch: 5129, Batch Gradient Norm after: 9.062355026955014
Epoch 5130/10000, Prediction Accuracy = 62.112%, Loss = 0.43510926365852354
Epoch: 5130, Batch Gradient Norm: 11.558406524580143
Epoch: 5130, Batch Gradient Norm after: 11.558406524580143
Epoch 5131/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.4465071320533752
Epoch: 5131, Batch Gradient Norm: 13.835193478417159
Epoch: 5131, Batch Gradient Norm after: 13.835193478417159
Epoch 5132/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.4658249020576477
Epoch: 5132, Batch Gradient Norm: 9.962470963753228
Epoch: 5132, Batch Gradient Norm after: 9.962470963753228
Epoch 5133/10000, Prediction Accuracy = 62.164%, Loss = 0.43812023401260375
Epoch: 5133, Batch Gradient Norm: 7.117852686954105
Epoch: 5133, Batch Gradient Norm after: 7.117852686954105
Epoch 5134/10000, Prediction Accuracy = 62.169999999999995%, Loss = 0.4224955976009369
Epoch: 5134, Batch Gradient Norm: 9.808807168733086
Epoch: 5134, Batch Gradient Norm after: 9.808807168733086
Epoch 5135/10000, Prediction Accuracy = 62.076%, Loss = 0.4386357605457306
Epoch: 5135, Batch Gradient Norm: 10.218591865141644
Epoch: 5135, Batch Gradient Norm after: 10.218591865141644
Epoch 5136/10000, Prediction Accuracy = 62.06%, Loss = 0.4438181042671204
Epoch: 5136, Batch Gradient Norm: 7.063689022347278
Epoch: 5136, Batch Gradient Norm after: 7.063689022347278
Epoch 5137/10000, Prediction Accuracy = 62.146%, Loss = 0.42355590462684634
Epoch: 5137, Batch Gradient Norm: 8.043734377091107
Epoch: 5137, Batch Gradient Norm after: 8.043734377091107
Epoch 5138/10000, Prediction Accuracy = 62.2%, Loss = 0.42670431137084963
Epoch: 5138, Batch Gradient Norm: 12.024319478356158
Epoch: 5138, Batch Gradient Norm after: 12.024319478356158
Epoch 5139/10000, Prediction Accuracy = 62.024%, Loss = 0.45284044146537783
Epoch: 5139, Batch Gradient Norm: 12.190973066484409
Epoch: 5139, Batch Gradient Norm after: 12.190973066484409
Epoch 5140/10000, Prediction Accuracy = 62.158%, Loss = 0.4549354791641235
Epoch: 5140, Batch Gradient Norm: 9.176444803470567
Epoch: 5140, Batch Gradient Norm after: 9.176444803470567
Epoch 5141/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.4330632150173187
Epoch: 5141, Batch Gradient Norm: 10.04477766560515
Epoch: 5141, Batch Gradient Norm after: 10.04477766560515
Epoch 5142/10000, Prediction Accuracy = 62.286%, Loss = 0.4380902528762817
Epoch: 5142, Batch Gradient Norm: 12.05782860936694
Epoch: 5142, Batch Gradient Norm after: 12.05782860936694
Epoch 5143/10000, Prediction Accuracy = 62.104000000000006%, Loss = 0.4542669773101807
Epoch: 5143, Batch Gradient Norm: 9.635036844000805
Epoch: 5143, Batch Gradient Norm after: 9.635036844000805
Epoch 5144/10000, Prediction Accuracy = 62.176%, Loss = 0.4367971956729889
Epoch: 5144, Batch Gradient Norm: 7.769460365234991
Epoch: 5144, Batch Gradient Norm after: 7.769460365234991
Epoch 5145/10000, Prediction Accuracy = 62.138%, Loss = 0.42545201182365416
Epoch: 5145, Batch Gradient Norm: 8.696693433050703
Epoch: 5145, Batch Gradient Norm after: 8.696693433050703
Epoch 5146/10000, Prediction Accuracy = 62.124%, Loss = 0.43017820715904237
Epoch: 5146, Batch Gradient Norm: 10.183841048695601
Epoch: 5146, Batch Gradient Norm after: 10.183841048695601
Epoch 5147/10000, Prediction Accuracy = 62.298%, Loss = 0.4396332025527954
Epoch: 5147, Batch Gradient Norm: 10.992130575765906
Epoch: 5147, Batch Gradient Norm after: 10.992130575765906
Epoch 5148/10000, Prediction Accuracy = 62.032%, Loss = 0.44407368898391725
Epoch: 5148, Batch Gradient Norm: 11.446069850338251
Epoch: 5148, Batch Gradient Norm after: 11.446069850338251
Epoch 5149/10000, Prediction Accuracy = 62.212%, Loss = 0.44760462641716003
Epoch: 5149, Batch Gradient Norm: 11.350836829171099
Epoch: 5149, Batch Gradient Norm after: 11.350836829171099
Epoch 5150/10000, Prediction Accuracy = 62.126%, Loss = 0.44787949323654175
Epoch: 5150, Batch Gradient Norm: 10.497043328101721
Epoch: 5150, Batch Gradient Norm after: 10.497043328101721
Epoch 5151/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4413691520690918
Epoch: 5151, Batch Gradient Norm: 10.050122432097682
Epoch: 5151, Batch Gradient Norm after: 10.050122432097682
Epoch 5152/10000, Prediction Accuracy = 62.09599999999999%, Loss = 0.43844550251960757
Epoch: 5152, Batch Gradient Norm: 10.888522711149253
Epoch: 5152, Batch Gradient Norm after: 10.888522711149253
Epoch 5153/10000, Prediction Accuracy = 62.218%, Loss = 0.44515575766563414
Epoch: 5153, Batch Gradient Norm: 10.024569466079734
Epoch: 5153, Batch Gradient Norm after: 10.024569466079734
Epoch 5154/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4385158956050873
Epoch: 5154, Batch Gradient Norm: 9.814665425798783
Epoch: 5154, Batch Gradient Norm after: 9.814665425798783
Epoch 5155/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.43671226501464844
Epoch: 5155, Batch Gradient Norm: 9.757945239324286
Epoch: 5155, Batch Gradient Norm after: 9.757945239324286
Epoch 5156/10000, Prediction Accuracy = 62.162%, Loss = 0.43643843531608584
Epoch: 5156, Batch Gradient Norm: 9.289884537591554
Epoch: 5156, Batch Gradient Norm after: 9.289884537591554
Epoch 5157/10000, Prediction Accuracy = 62.174%, Loss = 0.4337662696838379
Epoch: 5157, Batch Gradient Norm: 9.833311915107089
Epoch: 5157, Batch Gradient Norm after: 9.833311915107089
Epoch 5158/10000, Prediction Accuracy = 62.188%, Loss = 0.4367427408695221
Epoch: 5158, Batch Gradient Norm: 10.19104349566361
Epoch: 5158, Batch Gradient Norm after: 10.19104349566361
Epoch 5159/10000, Prediction Accuracy = 62.298%, Loss = 0.43912596702575685
Epoch: 5159, Batch Gradient Norm: 11.802797404748626
Epoch: 5159, Batch Gradient Norm after: 11.802797404748626
Epoch 5160/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.44990450143814087
Epoch: 5160, Batch Gradient Norm: 12.986052963782686
Epoch: 5160, Batch Gradient Norm after: 12.986052963782686
Epoch 5161/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.45975071787834165
Epoch: 5161, Batch Gradient Norm: 10.601538428060845
Epoch: 5161, Batch Gradient Norm after: 10.601538428060845
Epoch 5162/10000, Prediction Accuracy = 62.034000000000006%, Loss = 0.442663711309433
Epoch: 5162, Batch Gradient Norm: 7.304885961780733
Epoch: 5162, Batch Gradient Norm after: 7.304885961780733
Epoch 5163/10000, Prediction Accuracy = 62.226%, Loss = 0.42322927713394165
Epoch: 5163, Batch Gradient Norm: 7.294572575752468
Epoch: 5163, Batch Gradient Norm after: 7.294572575752468
Epoch 5164/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.42331927418708803
Epoch: 5164, Batch Gradient Norm: 8.956969665561616
Epoch: 5164, Batch Gradient Norm after: 8.956969665561616
Epoch 5165/10000, Prediction Accuracy = 62.188%, Loss = 0.43240808248519896
Epoch: 5165, Batch Gradient Norm: 10.829235453324955
Epoch: 5165, Batch Gradient Norm after: 10.829235453324955
Epoch 5166/10000, Prediction Accuracy = 62.08200000000001%, Loss = 0.4435766935348511
Epoch: 5166, Batch Gradient Norm: 11.10233131685349
Epoch: 5166, Batch Gradient Norm after: 11.10233131685349
Epoch 5167/10000, Prediction Accuracy = 62.169999999999995%, Loss = 0.4437644898891449
Epoch: 5167, Batch Gradient Norm: 10.086830511478192
Epoch: 5167, Batch Gradient Norm after: 10.086830511478192
Epoch 5168/10000, Prediction Accuracy = 62.022000000000006%, Loss = 0.4370222747325897
Epoch: 5168, Batch Gradient Norm: 9.061969145237429
Epoch: 5168, Batch Gradient Norm after: 9.061969145237429
Epoch 5169/10000, Prediction Accuracy = 62.162%, Loss = 0.4315208375453949
Epoch: 5169, Batch Gradient Norm: 8.231186862261561
Epoch: 5169, Batch Gradient Norm after: 8.231186862261561
Epoch 5170/10000, Prediction Accuracy = 62.08799999999999%, Loss = 0.4269623637199402
Epoch: 5170, Batch Gradient Norm: 7.904248961134555
Epoch: 5170, Batch Gradient Norm after: 7.904248961134555
Epoch 5171/10000, Prediction Accuracy = 62.178%, Loss = 0.42566514015197754
Epoch: 5171, Batch Gradient Norm: 8.612086047963684
Epoch: 5171, Batch Gradient Norm after: 8.612086047963684
Epoch 5172/10000, Prediction Accuracy = 62.088%, Loss = 0.42945317029953
Epoch: 5172, Batch Gradient Norm: 10.911027342698688
Epoch: 5172, Batch Gradient Norm after: 10.911027342698688
Epoch 5173/10000, Prediction Accuracy = 62.12800000000001%, Loss = 0.44468658566474917
Epoch: 5173, Batch Gradient Norm: 12.879539410692422
Epoch: 5173, Batch Gradient Norm after: 12.879539410692422
Epoch 5174/10000, Prediction Accuracy = 62.068%, Loss = 0.46110881567001344
Epoch: 5174, Batch Gradient Norm: 9.793775309557299
Epoch: 5174, Batch Gradient Norm after: 9.793775309557299
Epoch 5175/10000, Prediction Accuracy = 62.222%, Loss = 0.43744110465049746
Epoch: 5175, Batch Gradient Norm: 10.015376943283577
Epoch: 5175, Batch Gradient Norm after: 10.015376943283577
Epoch 5176/10000, Prediction Accuracy = 62.074%, Loss = 0.437148904800415
Epoch: 5176, Batch Gradient Norm: 10.6732553379974
Epoch: 5176, Batch Gradient Norm after: 10.6732553379974
Epoch 5177/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.44296109676361084
Epoch: 5177, Batch Gradient Norm: 9.874876209713648
Epoch: 5177, Batch Gradient Norm after: 9.874876209713648
Epoch 5178/10000, Prediction Accuracy = 62.21600000000001%, Loss = 0.43934361934661864
Epoch: 5178, Batch Gradient Norm: 9.935138295726517
Epoch: 5178, Batch Gradient Norm after: 9.935138295726517
Epoch 5179/10000, Prediction Accuracy = 62.19599999999999%, Loss = 0.437846040725708
Epoch: 5179, Batch Gradient Norm: 11.423579668816792
Epoch: 5179, Batch Gradient Norm after: 11.423579668816792
Epoch 5180/10000, Prediction Accuracy = 62.124%, Loss = 0.4470705151557922
Epoch: 5180, Batch Gradient Norm: 11.999503767062649
Epoch: 5180, Batch Gradient Norm after: 11.999503767062649
Epoch 5181/10000, Prediction Accuracy = 62.234%, Loss = 0.45086330771446226
Epoch: 5181, Batch Gradient Norm: 10.40021675902537
Epoch: 5181, Batch Gradient Norm after: 10.40021675902537
Epoch 5182/10000, Prediction Accuracy = 62.2%, Loss = 0.4399307548999786
Epoch: 5182, Batch Gradient Norm: 9.750670220749862
Epoch: 5182, Batch Gradient Norm after: 9.750670220749862
Epoch 5183/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.4356997311115265
Epoch: 5183, Batch Gradient Norm: 9.430982327487714
Epoch: 5183, Batch Gradient Norm after: 9.430982327487714
Epoch 5184/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4327100098133087
Epoch: 5184, Batch Gradient Norm: 11.178347701212575
Epoch: 5184, Batch Gradient Norm after: 11.178347701212575
Epoch 5185/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.4440981328487396
Epoch: 5185, Batch Gradient Norm: 11.781881087684367
Epoch: 5185, Batch Gradient Norm after: 11.781881087684367
Epoch 5186/10000, Prediction Accuracy = 62.35%, Loss = 0.4537374973297119
Epoch: 5186, Batch Gradient Norm: 7.814181769299637
Epoch: 5186, Batch Gradient Norm after: 7.814181769299637
Epoch 5187/10000, Prediction Accuracy = 62.157999999999994%, Loss = 0.4258141040802002
Epoch: 5187, Batch Gradient Norm: 8.584834350392196
Epoch: 5187, Batch Gradient Norm after: 8.584834350392196
Epoch 5188/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.42818431854248046
Epoch: 5188, Batch Gradient Norm: 10.36340711428663
Epoch: 5188, Batch Gradient Norm after: 10.36340711428663
Epoch 5189/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.43963568806648257
Epoch: 5189, Batch Gradient Norm: 9.362168119199383
Epoch: 5189, Batch Gradient Norm after: 9.362168119199383
Epoch 5190/10000, Prediction Accuracy = 62.148%, Loss = 0.4331614375114441
Epoch: 5190, Batch Gradient Norm: 7.706753188702574
Epoch: 5190, Batch Gradient Norm after: 7.706753188702574
Epoch 5191/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.42410665154457095
Epoch: 5191, Batch Gradient Norm: 9.324236430866907
Epoch: 5191, Batch Gradient Norm after: 9.324236430866907
Epoch 5192/10000, Prediction Accuracy = 62.25%, Loss = 0.4330354452133179
Epoch: 5192, Batch Gradient Norm: 13.181042255378403
Epoch: 5192, Batch Gradient Norm after: 13.181042255378403
Epoch 5193/10000, Prediction Accuracy = 62.15%, Loss = 0.46043015122413633
Epoch: 5193, Batch Gradient Norm: 12.706284643412996
Epoch: 5193, Batch Gradient Norm after: 12.706284643412996
Epoch 5194/10000, Prediction Accuracy = 62.162%, Loss = 0.45673338770866395
Epoch: 5194, Batch Gradient Norm: 9.093365305573565
Epoch: 5194, Batch Gradient Norm after: 9.093365305573565
Epoch 5195/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.4310167908668518
Epoch: 5195, Batch Gradient Norm: 7.865262785871839
Epoch: 5195, Batch Gradient Norm after: 7.865262785871839
Epoch 5196/10000, Prediction Accuracy = 62.157999999999994%, Loss = 0.42469643950462344
Epoch: 5196, Batch Gradient Norm: 7.96043756776589
Epoch: 5196, Batch Gradient Norm after: 7.96043756776589
Epoch 5197/10000, Prediction Accuracy = 62.19%, Loss = 0.4264288067817688
Epoch: 5197, Batch Gradient Norm: 8.168603131151976
Epoch: 5197, Batch Gradient Norm after: 8.168603131151976
Epoch 5198/10000, Prediction Accuracy = 62.174%, Loss = 0.4275320589542389
Epoch: 5198, Batch Gradient Norm: 9.358710174200217
Epoch: 5198, Batch Gradient Norm after: 9.358710174200217
Epoch 5199/10000, Prediction Accuracy = 62.286%, Loss = 0.43266879916191103
Epoch: 5199, Batch Gradient Norm: 14.201431391979304
Epoch: 5199, Batch Gradient Norm after: 14.201431391979304
Epoch 5200/10000, Prediction Accuracy = 62.017999999999994%, Loss = 0.4687274992465973
Epoch: 5200, Batch Gradient Norm: 12.986772752951524
Epoch: 5200, Batch Gradient Norm after: 12.986772752951524
Epoch 5201/10000, Prediction Accuracy = 62.226%, Loss = 0.46130613088607786
Epoch: 5201, Batch Gradient Norm: 7.956232375147255
Epoch: 5201, Batch Gradient Norm after: 7.956232375147255
Epoch 5202/10000, Prediction Accuracy = 62.2%, Loss = 0.4256704211235046
Epoch: 5202, Batch Gradient Norm: 7.501360448778029
Epoch: 5202, Batch Gradient Norm after: 7.501360448778029
Epoch 5203/10000, Prediction Accuracy = 62.238%, Loss = 0.42266311049461364
Epoch: 5203, Batch Gradient Norm: 9.541240688846662
Epoch: 5203, Batch Gradient Norm after: 9.541240688846662
Epoch 5204/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.4329434037208557
Epoch: 5204, Batch Gradient Norm: 12.164176506812677
Epoch: 5204, Batch Gradient Norm after: 12.164176506812677
Epoch 5205/10000, Prediction Accuracy = 62.02%, Loss = 0.45168663263320924
Epoch: 5205, Batch Gradient Norm: 10.806470942505399
Epoch: 5205, Batch Gradient Norm after: 10.806470942505399
Epoch 5206/10000, Prediction Accuracy = 62.148%, Loss = 0.4415797650814056
Epoch: 5206, Batch Gradient Norm: 9.864019744731081
Epoch: 5206, Batch Gradient Norm after: 9.864019744731081
Epoch 5207/10000, Prediction Accuracy = 62.214%, Loss = 0.4347125232219696
Epoch: 5207, Batch Gradient Norm: 12.05712714517532
Epoch: 5207, Batch Gradient Norm after: 12.05712714517532
Epoch 5208/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.45125612616539
Epoch: 5208, Batch Gradient Norm: 10.114865052834164
Epoch: 5208, Batch Gradient Norm after: 10.114865052834164
Epoch 5209/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.4381431758403778
Epoch: 5209, Batch Gradient Norm: 7.852707458433614
Epoch: 5209, Batch Gradient Norm after: 7.852707458433614
Epoch 5210/10000, Prediction Accuracy = 62.148%, Loss = 0.4242754101753235
Epoch: 5210, Batch Gradient Norm: 7.383601288668268
Epoch: 5210, Batch Gradient Norm after: 7.383601288668268
Epoch 5211/10000, Prediction Accuracy = 62.13199999999999%, Loss = 0.42141693234443667
Epoch: 5211, Batch Gradient Norm: 9.24407702332745
Epoch: 5211, Batch Gradient Norm after: 9.24407702332745
Epoch 5212/10000, Prediction Accuracy = 62.176%, Loss = 0.4309283196926117
Epoch: 5212, Batch Gradient Norm: 11.34572556996925
Epoch: 5212, Batch Gradient Norm after: 11.34572556996925
Epoch 5213/10000, Prediction Accuracy = 62.11199999999999%, Loss = 0.4467264175415039
Epoch: 5213, Batch Gradient Norm: 10.096546662930912
Epoch: 5213, Batch Gradient Norm after: 10.096546662930912
Epoch 5214/10000, Prediction Accuracy = 62.136%, Loss = 0.4375022709369659
Epoch: 5214, Batch Gradient Norm: 10.544563316360309
Epoch: 5214, Batch Gradient Norm after: 10.544563316360309
Epoch 5215/10000, Prediction Accuracy = 62.158%, Loss = 0.44005459547042847
Epoch: 5215, Batch Gradient Norm: 11.594691826063023
Epoch: 5215, Batch Gradient Norm after: 11.594691826063023
Epoch 5216/10000, Prediction Accuracy = 62.188%, Loss = 0.4507665514945984
Epoch: 5216, Batch Gradient Norm: 8.30318700173504
Epoch: 5216, Batch Gradient Norm after: 8.30318700173504
Epoch 5217/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.42779547572135923
Epoch: 5217, Batch Gradient Norm: 7.430760311443221
Epoch: 5217, Batch Gradient Norm after: 7.430760311443221
Epoch 5218/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.42209202647209165
Epoch: 5218, Batch Gradient Norm: 8.167935743680982
Epoch: 5218, Batch Gradient Norm after: 8.167935743680982
Epoch 5219/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.42557414174079894
Epoch: 5219, Batch Gradient Norm: 10.35211757009897
Epoch: 5219, Batch Gradient Norm after: 10.35211757009897
Epoch 5220/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.439842689037323
Epoch: 5220, Batch Gradient Norm: 12.375277391444863
Epoch: 5220, Batch Gradient Norm after: 12.375277391444863
Epoch 5221/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.4553437173366547
Epoch: 5221, Batch Gradient Norm: 12.211413182977129
Epoch: 5221, Batch Gradient Norm after: 12.211413182977129
Epoch 5222/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.45299561619758605
Epoch: 5222, Batch Gradient Norm: 9.18943387459619
Epoch: 5222, Batch Gradient Norm after: 9.18943387459619
Epoch 5223/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.43194582462310793
Epoch: 5223, Batch Gradient Norm: 9.070164362776643
Epoch: 5223, Batch Gradient Norm after: 9.070164362776643
Epoch 5224/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.42978801131248473
Epoch: 5224, Batch Gradient Norm: 12.0239856191748
Epoch: 5224, Batch Gradient Norm after: 12.0239856191748
Epoch 5225/10000, Prediction Accuracy = 62.081999999999994%, Loss = 0.44923256039619447
Epoch: 5225, Batch Gradient Norm: 12.303097444100375
Epoch: 5225, Batch Gradient Norm after: 12.303097444100375
Epoch 5226/10000, Prediction Accuracy = 62.188%, Loss = 0.4522518336772919
Epoch: 5226, Batch Gradient Norm: 9.479914473750727
Epoch: 5226, Batch Gradient Norm after: 9.479914473750727
Epoch 5227/10000, Prediction Accuracy = 62.17%, Loss = 0.43234461545944214
Epoch: 5227, Batch Gradient Norm: 8.06864029726191
Epoch: 5227, Batch Gradient Norm after: 8.06864029726191
Epoch 5228/10000, Prediction Accuracy = 62.298%, Loss = 0.4237498939037323
Epoch: 5228, Batch Gradient Norm: 8.862973964243231
Epoch: 5228, Batch Gradient Norm after: 8.862973964243231
Epoch 5229/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.42761144042015076
Epoch: 5229, Batch Gradient Norm: 10.65020073708396
Epoch: 5229, Batch Gradient Norm after: 10.65020073708396
Epoch 5230/10000, Prediction Accuracy = 62.286%, Loss = 0.4393422305583954
Epoch: 5230, Batch Gradient Norm: 10.719974012167217
Epoch: 5230, Batch Gradient Norm after: 10.719974012167217
Epoch 5231/10000, Prediction Accuracy = 62.15200000000001%, Loss = 0.44223661422729493
Epoch: 5231, Batch Gradient Norm: 9.286551461285987
Epoch: 5231, Batch Gradient Norm after: 9.286551461285987
Epoch 5232/10000, Prediction Accuracy = 62.182%, Loss = 0.43345844745635986
Epoch: 5232, Batch Gradient Norm: 10.048519761966025
Epoch: 5232, Batch Gradient Norm after: 10.048519761966025
Epoch 5233/10000, Prediction Accuracy = 62.134%, Loss = 0.43718031644821165
Epoch: 5233, Batch Gradient Norm: 12.105107177765303
Epoch: 5233, Batch Gradient Norm after: 12.105107177765303
Epoch 5234/10000, Prediction Accuracy = 62.16600000000001%, Loss = 0.4496084153652191
Epoch: 5234, Batch Gradient Norm: 11.092384874633389
Epoch: 5234, Batch Gradient Norm after: 11.092384874633389
Epoch 5235/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4423160791397095
Epoch: 5235, Batch Gradient Norm: 9.492471749195454
Epoch: 5235, Batch Gradient Norm after: 9.492471749195454
Epoch 5236/10000, Prediction Accuracy = 62.218%, Loss = 0.4318763315677643
Epoch: 5236, Batch Gradient Norm: 10.461323047530737
Epoch: 5236, Batch Gradient Norm after: 10.461323047530737
Epoch 5237/10000, Prediction Accuracy = 62.129999999999995%, Loss = 0.44145422577857973
Epoch: 5237, Batch Gradient Norm: 9.820165374517417
Epoch: 5237, Batch Gradient Norm after: 9.820165374517417
Epoch 5238/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.43827615976333617
Epoch: 5238, Batch Gradient Norm: 8.974847238955808
Epoch: 5238, Batch Gradient Norm after: 8.974847238955808
Epoch 5239/10000, Prediction Accuracy = 62.126%, Loss = 0.43004062175750735
Epoch: 5239, Batch Gradient Norm: 10.556500583198572
Epoch: 5239, Batch Gradient Norm after: 10.556500583198572
Epoch 5240/10000, Prediction Accuracy = 62.3%, Loss = 0.43922329545021055
Epoch: 5240, Batch Gradient Norm: 11.776997175523407
Epoch: 5240, Batch Gradient Norm after: 11.776997175523407
Epoch 5241/10000, Prediction Accuracy = 62.222%, Loss = 0.4503780484199524
Epoch: 5241, Batch Gradient Norm: 9.078649362006136
Epoch: 5241, Batch Gradient Norm after: 9.078649362006136
Epoch 5242/10000, Prediction Accuracy = 62.278%, Loss = 0.431497848033905
Epoch: 5242, Batch Gradient Norm: 8.717107700113038
Epoch: 5242, Batch Gradient Norm after: 8.717107700113038
Epoch 5243/10000, Prediction Accuracy = 62.168000000000006%, Loss = 0.4287125408649445
Epoch: 5243, Batch Gradient Norm: 9.526154463024591
Epoch: 5243, Batch Gradient Norm after: 9.526154463024591
Epoch 5244/10000, Prediction Accuracy = 62.102%, Loss = 0.433324933052063
Epoch: 5244, Batch Gradient Norm: 9.626101321441725
Epoch: 5244, Batch Gradient Norm after: 9.626101321441725
Epoch 5245/10000, Prediction Accuracy = 62.132000000000005%, Loss = 0.43385027050971986
Epoch: 5245, Batch Gradient Norm: 8.864880893975425
Epoch: 5245, Batch Gradient Norm after: 8.864880893975425
Epoch 5246/10000, Prediction Accuracy = 62.098%, Loss = 0.42879496812820433
Epoch: 5246, Batch Gradient Norm: 9.326861814623726
Epoch: 5246, Batch Gradient Norm after: 9.326861814623726
Epoch 5247/10000, Prediction Accuracy = 62.338%, Loss = 0.4319857954978943
Epoch: 5247, Batch Gradient Norm: 10.565493629962164
Epoch: 5247, Batch Gradient Norm after: 10.565493629962164
Epoch 5248/10000, Prediction Accuracy = 62.246%, Loss = 0.4407114565372467
Epoch: 5248, Batch Gradient Norm: 11.259119498733064
Epoch: 5248, Batch Gradient Norm after: 11.259119498733064
Epoch 5249/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.44524229168891905
Epoch: 5249, Batch Gradient Norm: 11.150491353065195
Epoch: 5249, Batch Gradient Norm after: 11.150491353065195
Epoch 5250/10000, Prediction Accuracy = 62.288%, Loss = 0.44339907765388487
Epoch: 5250, Batch Gradient Norm: 10.477473438015265
Epoch: 5250, Batch Gradient Norm after: 10.477473438015265
Epoch 5251/10000, Prediction Accuracy = 62.198%, Loss = 0.4383411169052124
Epoch: 5251, Batch Gradient Norm: 9.818150299081688
Epoch: 5251, Batch Gradient Norm after: 9.818150299081688
Epoch 5252/10000, Prediction Accuracy = 62.146%, Loss = 0.43375730514526367
Epoch: 5252, Batch Gradient Norm: 8.651724916656148
Epoch: 5252, Batch Gradient Norm after: 8.651724916656148
Epoch 5253/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.4270317792892456
Epoch: 5253, Batch Gradient Norm: 8.01980175549446
Epoch: 5253, Batch Gradient Norm after: 8.01980175549446
Epoch 5254/10000, Prediction Accuracy = 62.145999999999994%, Loss = 0.42289533019065856
Epoch: 5254, Batch Gradient Norm: 10.122133512615026
Epoch: 5254, Batch Gradient Norm after: 10.122133512615026
Epoch 5255/10000, Prediction Accuracy = 62.193999999999996%, Loss = 0.4345681667327881
Epoch: 5255, Batch Gradient Norm: 13.793700126129202
Epoch: 5255, Batch Gradient Norm after: 13.793700126129202
Epoch 5256/10000, Prediction Accuracy = 62.2%, Loss = 0.4640523254871368
Epoch: 5256, Batch Gradient Norm: 11.660265409100488
Epoch: 5256, Batch Gradient Norm after: 11.660265409100488
Epoch 5257/10000, Prediction Accuracy = 62.182%, Loss = 0.448311448097229
Epoch: 5257, Batch Gradient Norm: 8.572790324475992
Epoch: 5257, Batch Gradient Norm after: 8.572790324475992
Epoch 5258/10000, Prediction Accuracy = 62.188%, Loss = 0.4270203709602356
Epoch: 5258, Batch Gradient Norm: 9.276329424131035
Epoch: 5258, Batch Gradient Norm after: 9.276329424131035
Epoch 5259/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4330871760845184
Epoch: 5259, Batch Gradient Norm: 10.211387722085668
Epoch: 5259, Batch Gradient Norm after: 10.211387722085668
Epoch 5260/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4387403428554535
Epoch: 5260, Batch Gradient Norm: 10.691189029348328
Epoch: 5260, Batch Gradient Norm after: 10.691189029348328
Epoch 5261/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.4395163893699646
Epoch: 5261, Batch Gradient Norm: 11.149537123159085
Epoch: 5261, Batch Gradient Norm after: 11.149537123159085
Epoch 5262/10000, Prediction Accuracy = 62.202%, Loss = 0.4416817545890808
Epoch: 5262, Batch Gradient Norm: 10.754124055771054
Epoch: 5262, Batch Gradient Norm after: 10.754124055771054
Epoch 5263/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.439959728717804
Epoch: 5263, Batch Gradient Norm: 8.866627706846357
Epoch: 5263, Batch Gradient Norm after: 8.866627706846357
Epoch 5264/10000, Prediction Accuracy = 62.17%, Loss = 0.42991361021995544
Epoch: 5264, Batch Gradient Norm: 7.501107591898077
Epoch: 5264, Batch Gradient Norm after: 7.501107591898077
Epoch 5265/10000, Prediction Accuracy = 62.266%, Loss = 0.421195924282074
Epoch: 5265, Batch Gradient Norm: 9.494822243672811
Epoch: 5265, Batch Gradient Norm after: 9.494822243672811
Epoch 5266/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.43242273330688474
Epoch: 5266, Batch Gradient Norm: 11.962169093535453
Epoch: 5266, Batch Gradient Norm after: 11.962169093535453
Epoch 5267/10000, Prediction Accuracy = 62.062%, Loss = 0.4501089334487915
Epoch: 5267, Batch Gradient Norm: 10.083279203821645
Epoch: 5267, Batch Gradient Norm after: 10.083279203821645
Epoch 5268/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.43687095642089846
Epoch: 5268, Batch Gradient Norm: 8.500078378456134
Epoch: 5268, Batch Gradient Norm after: 8.500078378456134
Epoch 5269/10000, Prediction Accuracy = 62.198%, Loss = 0.4256763935089111
Epoch: 5269, Batch Gradient Norm: 9.087308870215749
Epoch: 5269, Batch Gradient Norm after: 9.087308870215749
Epoch 5270/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.42946097254753113
Epoch: 5270, Batch Gradient Norm: 10.023418030485903
Epoch: 5270, Batch Gradient Norm after: 10.023418030485903
Epoch 5271/10000, Prediction Accuracy = 62.174%, Loss = 0.43799727559089663
Epoch: 5271, Batch Gradient Norm: 9.971914372579175
Epoch: 5271, Batch Gradient Norm after: 9.971914372579175
Epoch 5272/10000, Prediction Accuracy = 62.16799999999999%, Loss = 0.4351755201816559
Epoch: 5272, Batch Gradient Norm: 10.879264176287782
Epoch: 5272, Batch Gradient Norm after: 10.879264176287782
Epoch 5273/10000, Prediction Accuracy = 62.242%, Loss = 0.44186331033706666
Epoch: 5273, Batch Gradient Norm: 9.825820942642132
Epoch: 5273, Batch Gradient Norm after: 9.825820942642132
Epoch 5274/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.43436856269836427
Epoch: 5274, Batch Gradient Norm: 8.80896861573806
Epoch: 5274, Batch Gradient Norm after: 8.80896861573806
Epoch 5275/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.42755478620529175
Epoch: 5275, Batch Gradient Norm: 9.386087521747347
Epoch: 5275, Batch Gradient Norm after: 9.386087521747347
Epoch 5276/10000, Prediction Accuracy = 62.138%, Loss = 0.43110803365707395
Epoch: 5276, Batch Gradient Norm: 10.454399153707845
Epoch: 5276, Batch Gradient Norm after: 10.454399153707845
Epoch 5277/10000, Prediction Accuracy = 62.145999999999994%, Loss = 0.44017404317855835
Epoch: 5277, Batch Gradient Norm: 8.801524979202044
Epoch: 5277, Batch Gradient Norm after: 8.801524979202044
Epoch 5278/10000, Prediction Accuracy = 62.27%, Loss = 0.4304490625858307
Epoch: 5278, Batch Gradient Norm: 7.764929696265628
Epoch: 5278, Batch Gradient Norm after: 7.764929696265628
Epoch 5279/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4239239990711212
Epoch: 5279, Batch Gradient Norm: 11.698732338187689
Epoch: 5279, Batch Gradient Norm after: 11.698732338187689
Epoch 5280/10000, Prediction Accuracy = 62.336%, Loss = 0.447011661529541
Epoch: 5280, Batch Gradient Norm: 15.4504591562243
Epoch: 5280, Batch Gradient Norm after: 15.4504591562243
Epoch 5281/10000, Prediction Accuracy = 62.132000000000005%, Loss = 0.4815265119075775
Epoch: 5281, Batch Gradient Norm: 10.554785767656279
Epoch: 5281, Batch Gradient Norm after: 10.554785767656279
Epoch 5282/10000, Prediction Accuracy = 62.388%, Loss = 0.4380989134311676
Epoch: 5282, Batch Gradient Norm: 10.163494164487652
Epoch: 5282, Batch Gradient Norm after: 10.163494164487652
Epoch 5283/10000, Prediction Accuracy = 62.20400000000001%, Loss = 0.4358336329460144
Epoch: 5283, Batch Gradient Norm: 9.628094657643057
Epoch: 5283, Batch Gradient Norm after: 9.628094657643057
Epoch 5284/10000, Prediction Accuracy = 62.036%, Loss = 0.43387672305107117
Epoch: 5284, Batch Gradient Norm: 8.387780838184792
Epoch: 5284, Batch Gradient Norm after: 8.387780838184792
Epoch 5285/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.4256422817707062
Epoch: 5285, Batch Gradient Norm: 9.047430301456153
Epoch: 5285, Batch Gradient Norm after: 9.047430301456153
Epoch 5286/10000, Prediction Accuracy = 62.096000000000004%, Loss = 0.42860339879989623
Epoch: 5286, Batch Gradient Norm: 11.06956431262551
Epoch: 5286, Batch Gradient Norm after: 11.06956431262551
Epoch 5287/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.4419802129268646
Epoch: 5287, Batch Gradient Norm: 10.436074323144595
Epoch: 5287, Batch Gradient Norm after: 10.436074323144595
Epoch 5288/10000, Prediction Accuracy = 62.174%, Loss = 0.4374628007411957
Epoch: 5288, Batch Gradient Norm: 8.88401055808311
Epoch: 5288, Batch Gradient Norm after: 8.88401055808311
Epoch 5289/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.427343612909317
Epoch: 5289, Batch Gradient Norm: 7.9772971067710445
Epoch: 5289, Batch Gradient Norm after: 7.9772971067710445
Epoch 5290/10000, Prediction Accuracy = 62.174%, Loss = 0.42225809693336486
Epoch: 5290, Batch Gradient Norm: 9.109036844955423
Epoch: 5290, Batch Gradient Norm after: 9.109036844955423
Epoch 5291/10000, Prediction Accuracy = 62.2%, Loss = 0.42805294394493104
Epoch: 5291, Batch Gradient Norm: 9.785704004735504
Epoch: 5291, Batch Gradient Norm after: 9.785704004735504
Epoch 5292/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.4337545812129974
Epoch: 5292, Batch Gradient Norm: 9.754948496759841
Epoch: 5292, Batch Gradient Norm after: 9.754948496759841
Epoch 5293/10000, Prediction Accuracy = 62.372%, Loss = 0.4338830530643463
Epoch: 5293, Batch Gradient Norm: 13.037719813297588
Epoch: 5293, Batch Gradient Norm after: 13.037719813297588
Epoch 5294/10000, Prediction Accuracy = 62.092000000000006%, Loss = 0.45958091616630553
Epoch: 5294, Batch Gradient Norm: 11.650007379647374
Epoch: 5294, Batch Gradient Norm after: 11.650007379647374
Epoch 5295/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.44872506260871886
Epoch: 5295, Batch Gradient Norm: 9.06850484482108
Epoch: 5295, Batch Gradient Norm after: 9.06850484482108
Epoch 5296/10000, Prediction Accuracy = 62.19599999999999%, Loss = 0.42864336371421813
Epoch: 5296, Batch Gradient Norm: 9.76790981749775
Epoch: 5296, Batch Gradient Norm after: 9.76790981749775
Epoch 5297/10000, Prediction Accuracy = 62.254%, Loss = 0.4320246398448944
Epoch: 5297, Batch Gradient Norm: 10.249544782674365
Epoch: 5297, Batch Gradient Norm after: 10.249544782674365
Epoch 5298/10000, Prediction Accuracy = 62.214%, Loss = 0.4357644319534302
Epoch: 5298, Batch Gradient Norm: 10.713975326171328
Epoch: 5298, Batch Gradient Norm after: 10.713975326171328
Epoch 5299/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.4395106852054596
Epoch: 5299, Batch Gradient Norm: 10.069921872517039
Epoch: 5299, Batch Gradient Norm after: 10.069921872517039
Epoch 5300/10000, Prediction Accuracy = 62.288%, Loss = 0.4358290612697601
Epoch: 5300, Batch Gradient Norm: 9.309922005630881
Epoch: 5300, Batch Gradient Norm after: 9.309922005630881
Epoch 5301/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.43110122084617614
Epoch: 5301, Batch Gradient Norm: 8.56721259900791
Epoch: 5301, Batch Gradient Norm after: 8.56721259900791
Epoch 5302/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4254722833633423
Epoch: 5302, Batch Gradient Norm: 9.589427796501747
Epoch: 5302, Batch Gradient Norm after: 9.589427796501747
Epoch 5303/10000, Prediction Accuracy = 62.286%, Loss = 0.43177089691162107
Epoch: 5303, Batch Gradient Norm: 11.970720582135497
Epoch: 5303, Batch Gradient Norm after: 11.970720582135497
Epoch 5304/10000, Prediction Accuracy = 62.212%, Loss = 0.44999470710754397
Epoch: 5304, Batch Gradient Norm: 12.160521220016632
Epoch: 5304, Batch Gradient Norm after: 12.160521220016632
Epoch 5305/10000, Prediction Accuracy = 62.222%, Loss = 0.4499577164649963
Epoch: 5305, Batch Gradient Norm: 11.362927631221776
Epoch: 5305, Batch Gradient Norm after: 11.362927631221776
Epoch 5306/10000, Prediction Accuracy = 62.378%, Loss = 0.44307944774627683
Epoch: 5306, Batch Gradient Norm: 10.101605514890208
Epoch: 5306, Batch Gradient Norm after: 10.101605514890208
Epoch 5307/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.43393781781196594
Epoch: 5307, Batch Gradient Norm: 9.386170782056732
Epoch: 5307, Batch Gradient Norm after: 9.386170782056732
Epoch 5308/10000, Prediction Accuracy = 62.354%, Loss = 0.4286419928073883
Epoch: 5308, Batch Gradient Norm: 10.35429083411456
Epoch: 5308, Batch Gradient Norm after: 10.35429083411456
Epoch 5309/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.43535544276237487
Epoch: 5309, Batch Gradient Norm: 10.068834193376546
Epoch: 5309, Batch Gradient Norm after: 10.068834193376546
Epoch 5310/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.4347041666507721
Epoch: 5310, Batch Gradient Norm: 9.122020118773381
Epoch: 5310, Batch Gradient Norm after: 9.122020118773381
Epoch 5311/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.42993947863578796
Epoch: 5311, Batch Gradient Norm: 9.512672099776607
Epoch: 5311, Batch Gradient Norm after: 9.512672099776607
Epoch 5312/10000, Prediction Accuracy = 62.274%, Loss = 0.4318536281585693
Epoch: 5312, Batch Gradient Norm: 10.57327633094313
Epoch: 5312, Batch Gradient Norm after: 10.57327633094313
Epoch 5313/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.43822553753852844
Epoch: 5313, Batch Gradient Norm: 9.794989137098097
Epoch: 5313, Batch Gradient Norm after: 9.794989137098097
Epoch 5314/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.43244650959968567
Epoch: 5314, Batch Gradient Norm: 9.220961838165001
Epoch: 5314, Batch Gradient Norm after: 9.220961838165001
Epoch 5315/10000, Prediction Accuracy = 62.169999999999995%, Loss = 0.429169625043869
Epoch: 5315, Batch Gradient Norm: 9.68182101727224
Epoch: 5315, Batch Gradient Norm after: 9.68182101727224
Epoch 5316/10000, Prediction Accuracy = 62.16199999999999%, Loss = 0.4335183620452881
Epoch: 5316, Batch Gradient Norm: 8.736823061782214
Epoch: 5316, Batch Gradient Norm after: 8.736823061782214
Epoch 5317/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.42803511023521423
Epoch: 5317, Batch Gradient Norm: 9.014207528633548
Epoch: 5317, Batch Gradient Norm after: 9.014207528633548
Epoch 5318/10000, Prediction Accuracy = 62.25600000000001%, Loss = 0.42895348072052003
Epoch: 5318, Batch Gradient Norm: 9.69741950492201
Epoch: 5318, Batch Gradient Norm after: 9.69741950492201
Epoch 5319/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4322598159313202
Epoch: 5319, Batch Gradient Norm: 11.394726240190348
Epoch: 5319, Batch Gradient Norm after: 11.394726240190348
Epoch 5320/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.44458155035972596
Epoch: 5320, Batch Gradient Norm: 11.6638558330972
Epoch: 5320, Batch Gradient Norm after: 11.6638558330972
Epoch 5321/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.44926891326904295
Epoch: 5321, Batch Gradient Norm: 9.240647322525176
Epoch: 5321, Batch Gradient Norm after: 9.240647322525176
Epoch 5322/10000, Prediction Accuracy = 62.274%, Loss = 0.4289514601230621
Epoch: 5322, Batch Gradient Norm: 10.354576299398827
Epoch: 5322, Batch Gradient Norm after: 10.354576299398827
Epoch 5323/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4346831917762756
Epoch: 5323, Batch Gradient Norm: 10.632073366533442
Epoch: 5323, Batch Gradient Norm after: 10.632073366533442
Epoch 5324/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.438001412153244
Epoch: 5324, Batch Gradient Norm: 10.298375047524065
Epoch: 5324, Batch Gradient Norm after: 10.298375047524065
Epoch 5325/10000, Prediction Accuracy = 62.286%, Loss = 0.4361992120742798
Epoch: 5325, Batch Gradient Norm: 11.062655320979799
Epoch: 5325, Batch Gradient Norm after: 11.062655320979799
Epoch 5326/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.4417824804782867
Epoch: 5326, Batch Gradient Norm: 11.40501054844077
Epoch: 5326, Batch Gradient Norm after: 11.40501054844077
Epoch 5327/10000, Prediction Accuracy = 62.19200000000001%, Loss = 0.4447028398513794
Epoch: 5327, Batch Gradient Norm: 9.57902566906893
Epoch: 5327, Batch Gradient Norm after: 9.57902566906893
Epoch 5328/10000, Prediction Accuracy = 62.22800000000001%, Loss = 0.43170962333679197
Epoch: 5328, Batch Gradient Norm: 9.204947053095042
Epoch: 5328, Batch Gradient Norm after: 9.204947053095042
Epoch 5329/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.4286047399044037
Epoch: 5329, Batch Gradient Norm: 9.345996515396044
Epoch: 5329, Batch Gradient Norm after: 9.345996515396044
Epoch 5330/10000, Prediction Accuracy = 62.278%, Loss = 0.4292954921722412
Epoch: 5330, Batch Gradient Norm: 10.470327684937766
Epoch: 5330, Batch Gradient Norm after: 10.470327684937766
Epoch 5331/10000, Prediction Accuracy = 62.212%, Loss = 0.43653433918952944
Epoch: 5331, Batch Gradient Norm: 10.97420007316445
Epoch: 5331, Batch Gradient Norm after: 10.97420007316445
Epoch 5332/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.438903272151947
Epoch: 5332, Batch Gradient Norm: 11.680178498569111
Epoch: 5332, Batch Gradient Norm after: 11.680178498569111
Epoch 5333/10000, Prediction Accuracy = 62.174%, Loss = 0.4432448327541351
Epoch: 5333, Batch Gradient Norm: 10.408364432602477
Epoch: 5333, Batch Gradient Norm after: 10.408364432602477
Epoch 5334/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.4357711970806122
Epoch: 5334, Batch Gradient Norm: 8.91505450673311
Epoch: 5334, Batch Gradient Norm after: 8.91505450673311
Epoch 5335/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.42718284726142886
Epoch: 5335, Batch Gradient Norm: 8.240137675377822
Epoch: 5335, Batch Gradient Norm after: 8.240137675377822
Epoch 5336/10000, Prediction Accuracy = 62.193999999999996%, Loss = 0.424614804983139
Epoch: 5336, Batch Gradient Norm: 7.785721022061786
Epoch: 5336, Batch Gradient Norm after: 7.785721022061786
Epoch 5337/10000, Prediction Accuracy = 62.30400000000001%, Loss = 0.42096002101898194
Epoch: 5337, Batch Gradient Norm: 10.240938754529521
Epoch: 5337, Batch Gradient Norm after: 10.240938754529521
Epoch 5338/10000, Prediction Accuracy = 62.157999999999994%, Loss = 0.4346722364425659
Epoch: 5338, Batch Gradient Norm: 11.768334324486894
Epoch: 5338, Batch Gradient Norm after: 11.768334324486894
Epoch 5339/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.4462271749973297
Epoch: 5339, Batch Gradient Norm: 9.2841708813324
Epoch: 5339, Batch Gradient Norm after: 9.2841708813324
Epoch 5340/10000, Prediction Accuracy = 62.238%, Loss = 0.42844358086586
Epoch: 5340, Batch Gradient Norm: 9.09862460406233
Epoch: 5340, Batch Gradient Norm after: 9.09862460406233
Epoch 5341/10000, Prediction Accuracy = 62.384%, Loss = 0.4276032328605652
Epoch: 5341, Batch Gradient Norm: 10.497084385635446
Epoch: 5341, Batch Gradient Norm after: 10.497084385635446
Epoch 5342/10000, Prediction Accuracy = 62.306%, Loss = 0.43920485973358153
Epoch: 5342, Batch Gradient Norm: 11.043684051808075
Epoch: 5342, Batch Gradient Norm after: 11.043684051808075
Epoch 5343/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.4420479297637939
Epoch: 5343, Batch Gradient Norm: 12.528206278419816
Epoch: 5343, Batch Gradient Norm after: 12.528206278419816
Epoch 5344/10000, Prediction Accuracy = 62.2%, Loss = 0.45171424746513367
Epoch: 5344, Batch Gradient Norm: 11.765198952088701
Epoch: 5344, Batch Gradient Norm after: 11.765198952088701
Epoch 5345/10000, Prediction Accuracy = 62.14%, Loss = 0.44634305834770205
Epoch: 5345, Batch Gradient Norm: 8.610279061042979
Epoch: 5345, Batch Gradient Norm after: 8.610279061042979
Epoch 5346/10000, Prediction Accuracy = 62.25%, Loss = 0.42530211210250857
Epoch: 5346, Batch Gradient Norm: 8.317316560514662
Epoch: 5346, Batch Gradient Norm after: 8.317316560514662
Epoch 5347/10000, Prediction Accuracy = 62.302%, Loss = 0.4236104846000671
Epoch: 5347, Batch Gradient Norm: 9.306571122562826
Epoch: 5347, Batch Gradient Norm after: 9.306571122562826
Epoch 5348/10000, Prediction Accuracy = 62.14%, Loss = 0.4291920065879822
Epoch: 5348, Batch Gradient Norm: 10.267768577086542
Epoch: 5348, Batch Gradient Norm after: 10.267768577086542
Epoch 5349/10000, Prediction Accuracy = 62.302%, Loss = 0.4355480790138245
Epoch: 5349, Batch Gradient Norm: 10.496893601326333
Epoch: 5349, Batch Gradient Norm after: 10.496893601326333
Epoch 5350/10000, Prediction Accuracy = 62.132000000000005%, Loss = 0.4364328861236572
Epoch: 5350, Batch Gradient Norm: 10.90802610230259
Epoch: 5350, Batch Gradient Norm after: 10.90802610230259
Epoch 5351/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.438869833946228
Epoch: 5351, Batch Gradient Norm: 11.109873077912505
Epoch: 5351, Batch Gradient Norm after: 11.109873077912505
Epoch 5352/10000, Prediction Accuracy = 62.120000000000005%, Loss = 0.43977770805358884
Epoch: 5352, Batch Gradient Norm: 10.208107841294465
Epoch: 5352, Batch Gradient Norm after: 10.208107841294465
Epoch 5353/10000, Prediction Accuracy = 62.186%, Loss = 0.43394471406936647
Epoch: 5353, Batch Gradient Norm: 9.583047108243598
Epoch: 5353, Batch Gradient Norm after: 9.583047108243598
Epoch 5354/10000, Prediction Accuracy = 62.148%, Loss = 0.4313971698284149
Epoch: 5354, Batch Gradient Norm: 8.512941589413
Epoch: 5354, Batch Gradient Norm after: 8.512941589413
Epoch 5355/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.42478683590888977
Epoch: 5355, Batch Gradient Norm: 8.450513102371866
Epoch: 5355, Batch Gradient Norm after: 8.450513102371866
Epoch 5356/10000, Prediction Accuracy = 62.35%, Loss = 0.4244639933109283
Epoch: 5356, Batch Gradient Norm: 7.965412964401072
Epoch: 5356, Batch Gradient Norm after: 7.965412964401072
Epoch 5357/10000, Prediction Accuracy = 62.30999999999999%, Loss = 0.42171435952186587
Epoch: 5357, Batch Gradient Norm: 9.410941596990252
Epoch: 5357, Batch Gradient Norm after: 9.410941596990252
Epoch 5358/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4296908795833588
Epoch: 5358, Batch Gradient Norm: 12.920573076301281
Epoch: 5358, Batch Gradient Norm after: 12.920573076301281
Epoch 5359/10000, Prediction Accuracy = 62.152%, Loss = 0.4565518260002136
Epoch: 5359, Batch Gradient Norm: 11.71319399155886
Epoch: 5359, Batch Gradient Norm after: 11.71319399155886
Epoch 5360/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.4475680112838745
Epoch: 5360, Batch Gradient Norm: 9.554902948644179
Epoch: 5360, Batch Gradient Norm after: 9.554902948644179
Epoch 5361/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.4304152727127075
Epoch: 5361, Batch Gradient Norm: 10.26025036102481
Epoch: 5361, Batch Gradient Norm after: 10.26025036102481
Epoch 5362/10000, Prediction Accuracy = 62.306%, Loss = 0.43402870297431945
Epoch: 5362, Batch Gradient Norm: 10.961597180938147
Epoch: 5362, Batch Gradient Norm after: 10.961597180938147
Epoch 5363/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.438907265663147
Epoch: 5363, Batch Gradient Norm: 10.914561066418234
Epoch: 5363, Batch Gradient Norm after: 10.914561066418234
Epoch 5364/10000, Prediction Accuracy = 62.215999999999994%, Loss = 0.4392212092876434
Epoch: 5364, Batch Gradient Norm: 10.6964700196841
Epoch: 5364, Batch Gradient Norm after: 10.6964700196841
Epoch 5365/10000, Prediction Accuracy = 62.176%, Loss = 0.43766955137252805
Epoch: 5365, Batch Gradient Norm: 9.344828465175299
Epoch: 5365, Batch Gradient Norm after: 9.344828465175299
Epoch 5366/10000, Prediction Accuracy = 62.02%, Loss = 0.42898468375205995
Epoch: 5366, Batch Gradient Norm: 8.497497536528074
Epoch: 5366, Batch Gradient Norm after: 8.497497536528074
Epoch 5367/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.42397414445877074
Epoch: 5367, Batch Gradient Norm: 10.221829373572081
Epoch: 5367, Batch Gradient Norm after: 10.221829373572081
Epoch 5368/10000, Prediction Accuracy = 62.298%, Loss = 0.4351438105106354
Epoch: 5368, Batch Gradient Norm: 12.047783088776624
Epoch: 5368, Batch Gradient Norm after: 12.047783088776624
Epoch 5369/10000, Prediction Accuracy = 62.23199999999999%, Loss = 0.4494452953338623
Epoch: 5369, Batch Gradient Norm: 10.239805442305864
Epoch: 5369, Batch Gradient Norm after: 10.239805442305864
Epoch 5370/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4340542614459991
Epoch: 5370, Batch Gradient Norm: 9.379990192731468
Epoch: 5370, Batch Gradient Norm after: 9.379990192731468
Epoch 5371/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.42805989384651183
Epoch: 5371, Batch Gradient Norm: 8.756477789347812
Epoch: 5371, Batch Gradient Norm after: 8.756477789347812
Epoch 5372/10000, Prediction Accuracy = 62.176%, Loss = 0.42532262206077576
Epoch: 5372, Batch Gradient Norm: 9.227934505979803
Epoch: 5372, Batch Gradient Norm after: 9.227934505979803
Epoch 5373/10000, Prediction Accuracy = 62.152%, Loss = 0.4289692580699921
Epoch: 5373, Batch Gradient Norm: 10.194990689209767
Epoch: 5373, Batch Gradient Norm after: 10.194990689209767
Epoch 5374/10000, Prediction Accuracy = 62.21600000000001%, Loss = 0.43597981333732605
Epoch: 5374, Batch Gradient Norm: 10.516515066969143
Epoch: 5374, Batch Gradient Norm after: 10.516515066969143
Epoch 5375/10000, Prediction Accuracy = 62.21400000000001%, Loss = 0.43689708709716796
Epoch: 5375, Batch Gradient Norm: 10.980026557441294
Epoch: 5375, Batch Gradient Norm after: 10.980026557441294
Epoch 5376/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.4390277683734894
Epoch: 5376, Batch Gradient Norm: 10.741154074582546
Epoch: 5376, Batch Gradient Norm after: 10.741154074582546
Epoch 5377/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.4382602095603943
Epoch: 5377, Batch Gradient Norm: 9.323815797393117
Epoch: 5377, Batch Gradient Norm after: 9.323815797393117
Epoch 5378/10000, Prediction Accuracy = 62.245999999999995%, Loss = 0.4282465636730194
Epoch: 5378, Batch Gradient Norm: 9.29505891436253
Epoch: 5378, Batch Gradient Norm after: 9.29505891436253
Epoch 5379/10000, Prediction Accuracy = 62.274%, Loss = 0.42840301990509033
Epoch: 5379, Batch Gradient Norm: 9.671814168416358
Epoch: 5379, Batch Gradient Norm after: 9.671814168416358
Epoch 5380/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.43005561232566836
Epoch: 5380, Batch Gradient Norm: 12.194112599965937
Epoch: 5380, Batch Gradient Norm after: 12.194112599965937
Epoch 5381/10000, Prediction Accuracy = 62.288%, Loss = 0.44715484976768494
Epoch: 5381, Batch Gradient Norm: 11.851697268916727
Epoch: 5381, Batch Gradient Norm after: 11.851697268916727
Epoch 5382/10000, Prediction Accuracy = 62.14399999999999%, Loss = 0.44617596864700315
Epoch: 5382, Batch Gradient Norm: 8.516329325338784
Epoch: 5382, Batch Gradient Norm after: 8.516329325338784
Epoch 5383/10000, Prediction Accuracy = 62.31%, Loss = 0.42388967871665956
Epoch: 5383, Batch Gradient Norm: 8.044755696183758
Epoch: 5383, Batch Gradient Norm after: 8.044755696183758
Epoch 5384/10000, Prediction Accuracy = 62.158%, Loss = 0.42143644094467164
Epoch: 5384, Batch Gradient Norm: 8.11695029054401
Epoch: 5384, Batch Gradient Norm after: 8.11695029054401
Epoch 5385/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4224966764450073
Epoch: 5385, Batch Gradient Norm: 9.893959564023705
Epoch: 5385, Batch Gradient Norm after: 9.893959564023705
Epoch 5386/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.43074071407318115
Epoch: 5386, Batch Gradient Norm: 12.299586865939066
Epoch: 5386, Batch Gradient Norm after: 12.299586865939066
Epoch 5387/10000, Prediction Accuracy = 62.31400000000001%, Loss = 0.44772460460662844
Epoch: 5387, Batch Gradient Norm: 10.968945940644495
Epoch: 5387, Batch Gradient Norm after: 10.968945940644495
Epoch 5388/10000, Prediction Accuracy = 62.222%, Loss = 0.43811448812484743
Epoch: 5388, Batch Gradient Norm: 8.623125123003001
Epoch: 5388, Batch Gradient Norm after: 8.623125123003001
Epoch 5389/10000, Prediction Accuracy = 62.093999999999994%, Loss = 0.42334339022636414
Epoch: 5389, Batch Gradient Norm: 8.49935239042009
Epoch: 5389, Batch Gradient Norm after: 8.49935239042009
Epoch 5390/10000, Prediction Accuracy = 62.263999999999996%, Loss = 0.42252748012542723
Epoch: 5390, Batch Gradient Norm: 9.855126405525843
Epoch: 5390, Batch Gradient Norm after: 9.855126405525843
Epoch 5391/10000, Prediction Accuracy = 62.098%, Loss = 0.43055605292320254
Epoch: 5391, Batch Gradient Norm: 11.035961871775006
Epoch: 5391, Batch Gradient Norm after: 11.035961871775006
Epoch 5392/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.4394751310348511
Epoch: 5392, Batch Gradient Norm: 10.833684981465737
Epoch: 5392, Batch Gradient Norm after: 10.833684981465737
Epoch 5393/10000, Prediction Accuracy = 62.134%, Loss = 0.43969947695732114
Epoch: 5393, Batch Gradient Norm: 10.701540719375096
Epoch: 5393, Batch Gradient Norm after: 10.701540719375096
Epoch 5394/10000, Prediction Accuracy = 62.176%, Loss = 0.43909251093864443
Epoch: 5394, Batch Gradient Norm: 12.198931212584636
Epoch: 5394, Batch Gradient Norm after: 12.198931212584636
Epoch 5395/10000, Prediction Accuracy = 62.263999999999996%, Loss = 0.44978741407394407
Epoch: 5395, Batch Gradient Norm: 10.90168607784011
Epoch: 5395, Batch Gradient Norm after: 10.90168607784011
Epoch 5396/10000, Prediction Accuracy = 62.224000000000004%, Loss = 0.43929303884506227
Epoch: 5396, Batch Gradient Norm: 9.510336398346734
Epoch: 5396, Batch Gradient Norm after: 9.510336398346734
Epoch 5397/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.42906858325004577
Epoch: 5397, Batch Gradient Norm: 10.114786541718654
Epoch: 5397, Batch Gradient Norm after: 10.114786541718654
Epoch 5398/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4325835943222046
Epoch: 5398, Batch Gradient Norm: 10.515387908765598
Epoch: 5398, Batch Gradient Norm after: 10.515387908765598
Epoch 5399/10000, Prediction Accuracy = 62.25999999999999%, Loss = 0.43584909439086916
Epoch: 5399, Batch Gradient Norm: 9.791820251787462
Epoch: 5399, Batch Gradient Norm after: 9.791820251787462
Epoch 5400/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.43109518885612486
Epoch: 5400, Batch Gradient Norm: 9.612659309304782
Epoch: 5400, Batch Gradient Norm after: 9.612659309304782
Epoch 5401/10000, Prediction Accuracy = 62.238%, Loss = 0.4294868528842926
Epoch: 5401, Batch Gradient Norm: 9.720409344916185
Epoch: 5401, Batch Gradient Norm after: 9.720409344916185
Epoch 5402/10000, Prediction Accuracy = 62.330000000000005%, Loss = 0.4310707151889801
Epoch: 5402, Batch Gradient Norm: 8.854819214865428
Epoch: 5402, Batch Gradient Norm after: 8.854819214865428
Epoch 5403/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.42548700571060183
Epoch: 5403, Batch Gradient Norm: 9.902764914228582
Epoch: 5403, Batch Gradient Norm after: 9.902764914228582
Epoch 5404/10000, Prediction Accuracy = 62.209999999999994%, Loss = 0.43192810416221616
Epoch: 5404, Batch Gradient Norm: 10.156519262022165
Epoch: 5404, Batch Gradient Norm after: 10.156519262022165
Epoch 5405/10000, Prediction Accuracy = 62.10600000000001%, Loss = 0.43431262373924256
Epoch: 5405, Batch Gradient Norm: 8.525748162176722
Epoch: 5405, Batch Gradient Norm after: 8.525748162176722
Epoch 5406/10000, Prediction Accuracy = 62.164%, Loss = 0.4241263628005981
Epoch: 5406, Batch Gradient Norm: 8.858328381702588
Epoch: 5406, Batch Gradient Norm after: 8.858328381702588
Epoch 5407/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.42538681626319885
Epoch: 5407, Batch Gradient Norm: 11.458462701137378
Epoch: 5407, Batch Gradient Norm after: 11.458462701137378
Epoch 5408/10000, Prediction Accuracy = 62.186%, Loss = 0.4432179510593414
Epoch: 5408, Batch Gradient Norm: 11.10485340142896
Epoch: 5408, Batch Gradient Norm after: 11.10485340142896
Epoch 5409/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4400108098983765
Epoch: 5409, Batch Gradient Norm: 9.48217262103848
Epoch: 5409, Batch Gradient Norm after: 9.48217262103848
Epoch 5410/10000, Prediction Accuracy = 62.188%, Loss = 0.4288896143436432
Epoch: 5410, Batch Gradient Norm: 9.707604490671704
Epoch: 5410, Batch Gradient Norm after: 9.707604490671704
Epoch 5411/10000, Prediction Accuracy = 62.378%, Loss = 0.4317935883998871
Epoch: 5411, Batch Gradient Norm: 11.011207728133385
Epoch: 5411, Batch Gradient Norm after: 11.011207728133385
Epoch 5412/10000, Prediction Accuracy = 62.3%, Loss = 0.4395699143409729
Epoch: 5412, Batch Gradient Norm: 12.32228813842556
Epoch: 5412, Batch Gradient Norm after: 12.32228813842556
Epoch 5413/10000, Prediction Accuracy = 62.416%, Loss = 0.4475340247154236
Epoch: 5413, Batch Gradient Norm: 10.433584419074865
Epoch: 5413, Batch Gradient Norm after: 10.433584419074865
Epoch 5414/10000, Prediction Accuracy = 62.178%, Loss = 0.43353216648101806
Epoch: 5414, Batch Gradient Norm: 8.54296225774386
Epoch: 5414, Batch Gradient Norm after: 8.54296225774386
Epoch 5415/10000, Prediction Accuracy = 62.384%, Loss = 0.4228163778781891
Epoch: 5415, Batch Gradient Norm: 9.650805291148844
Epoch: 5415, Batch Gradient Norm after: 9.650805291148844
Epoch 5416/10000, Prediction Accuracy = 62.266%, Loss = 0.4288600981235504
Epoch: 5416, Batch Gradient Norm: 12.890282655310886
Epoch: 5416, Batch Gradient Norm after: 12.890282655310886
Epoch 5417/10000, Prediction Accuracy = 62.3%, Loss = 0.4525610744953156
Epoch: 5417, Batch Gradient Norm: 11.945841494491496
Epoch: 5417, Batch Gradient Norm after: 11.945841494491496
Epoch 5418/10000, Prediction Accuracy = 62.214%, Loss = 0.44468496441841127
Epoch: 5418, Batch Gradient Norm: 8.381750111154059
Epoch: 5418, Batch Gradient Norm after: 8.381750111154059
Epoch 5419/10000, Prediction Accuracy = 62.128%, Loss = 0.4213947832584381
Epoch: 5419, Batch Gradient Norm: 7.750716861122062
Epoch: 5419, Batch Gradient Norm after: 7.750716861122062
Epoch 5420/10000, Prediction Accuracy = 62.29%, Loss = 0.4181110918521881
Epoch: 5420, Batch Gradient Norm: 10.1532158920691
Epoch: 5420, Batch Gradient Norm after: 10.1532158920691
Epoch 5421/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.43366181254386904
Epoch: 5421, Batch Gradient Norm: 8.677446255011676
Epoch: 5421, Batch Gradient Norm after: 8.677446255011676
Epoch 5422/10000, Prediction Accuracy = 62.21999999999999%, Loss = 0.42605515718460085
Epoch: 5422, Batch Gradient Norm: 8.103458839700055
Epoch: 5422, Batch Gradient Norm after: 8.103458839700055
Epoch 5423/10000, Prediction Accuracy = 62.35%, Loss = 0.4207510113716125
Epoch: 5423, Batch Gradient Norm: 9.902725880427841
Epoch: 5423, Batch Gradient Norm after: 9.902725880427841
Epoch 5424/10000, Prediction Accuracy = 62.282%, Loss = 0.4298794329166412
Epoch: 5424, Batch Gradient Norm: 12.882346470815564
Epoch: 5424, Batch Gradient Norm after: 12.882346470815564
Epoch 5425/10000, Prediction Accuracy = 62.141999999999996%, Loss = 0.4525767743587494
Epoch: 5425, Batch Gradient Norm: 11.436159422345806
Epoch: 5425, Batch Gradient Norm after: 11.436159422345806
Epoch 5426/10000, Prediction Accuracy = 62.288%, Loss = 0.4425072133541107
Epoch: 5426, Batch Gradient Norm: 8.68568904870794
Epoch: 5426, Batch Gradient Norm after: 8.68568904870794
Epoch 5427/10000, Prediction Accuracy = 62.254%, Loss = 0.423673540353775
Epoch: 5427, Batch Gradient Norm: 9.179826472662343
Epoch: 5427, Batch Gradient Norm after: 9.179826472662343
Epoch 5428/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4254194736480713
Epoch: 5428, Batch Gradient Norm: 10.8122850144647
Epoch: 5428, Batch Gradient Norm after: 10.8122850144647
Epoch 5429/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.4362455427646637
Epoch: 5429, Batch Gradient Norm: 10.548254357654614
Epoch: 5429, Batch Gradient Norm after: 10.548254357654614
Epoch 5430/10000, Prediction Accuracy = 62.178%, Loss = 0.4344015777111053
Epoch: 5430, Batch Gradient Norm: 8.984630992729736
Epoch: 5430, Batch Gradient Norm after: 8.984630992729736
Epoch 5431/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.4244532763957977
Epoch: 5431, Batch Gradient Norm: 8.851397311017935
Epoch: 5431, Batch Gradient Norm after: 8.851397311017935
Epoch 5432/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4241418778896332
Epoch: 5432, Batch Gradient Norm: 9.163894816154611
Epoch: 5432, Batch Gradient Norm after: 9.163894816154611
Epoch 5433/10000, Prediction Accuracy = 62.17%, Loss = 0.42664802074432373
Epoch: 5433, Batch Gradient Norm: 8.528717141549093
Epoch: 5433, Batch Gradient Norm after: 8.528717141549093
Epoch 5434/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.42316576838493347
Epoch: 5434, Batch Gradient Norm: 9.391916285050813
Epoch: 5434, Batch Gradient Norm after: 9.391916285050813
Epoch 5435/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.42737913131713867
Epoch: 5435, Batch Gradient Norm: 11.753128866597221
Epoch: 5435, Batch Gradient Norm after: 11.753128866597221
Epoch 5436/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.44201828837394713
Epoch: 5436, Batch Gradient Norm: 12.553288127985128
Epoch: 5436, Batch Gradient Norm after: 12.553288127985128
Epoch 5437/10000, Prediction Accuracy = 62.206%, Loss = 0.4497973442077637
Epoch: 5437, Batch Gradient Norm: 9.45000869234913
Epoch: 5437, Batch Gradient Norm after: 9.45000869234913
Epoch 5438/10000, Prediction Accuracy = 62.388%, Loss = 0.4282903432846069
Epoch: 5438, Batch Gradient Norm: 10.121998478236
Epoch: 5438, Batch Gradient Norm after: 10.121998478236
Epoch 5439/10000, Prediction Accuracy = 62.198%, Loss = 0.4330489218235016
Epoch: 5439, Batch Gradient Norm: 12.880231042157119
Epoch: 5439, Batch Gradient Norm after: 12.880231042157119
Epoch 5440/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4534791588783264
Epoch: 5440, Batch Gradient Norm: 11.688382531044585
Epoch: 5440, Batch Gradient Norm after: 11.688382531044585
Epoch 5441/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.44406636953353884
Epoch: 5441, Batch Gradient Norm: 9.297178270348086
Epoch: 5441, Batch Gradient Norm after: 9.297178270348086
Epoch 5442/10000, Prediction Accuracy = 62.214%, Loss = 0.42660937309265134
Epoch: 5442, Batch Gradient Norm: 8.901702436539626
Epoch: 5442, Batch Gradient Norm after: 8.901702436539626
Epoch 5443/10000, Prediction Accuracy = 62.206%, Loss = 0.4247303009033203
Epoch: 5443, Batch Gradient Norm: 10.22572741145636
Epoch: 5443, Batch Gradient Norm after: 10.22572741145636
Epoch 5444/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.43331133723258974
Epoch: 5444, Batch Gradient Norm: 10.950094463822824
Epoch: 5444, Batch Gradient Norm after: 10.950094463822824
Epoch 5445/10000, Prediction Accuracy = 62.166%, Loss = 0.43889138102531433
Epoch: 5445, Batch Gradient Norm: 10.269867006643834
Epoch: 5445, Batch Gradient Norm after: 10.269867006643834
Epoch 5446/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.4355248749256134
Epoch: 5446, Batch Gradient Norm: 7.93809038704799
Epoch: 5446, Batch Gradient Norm after: 7.93809038704799
Epoch 5447/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.4203067421913147
Epoch: 5447, Batch Gradient Norm: 7.865863678520145
Epoch: 5447, Batch Gradient Norm after: 7.865863678520145
Epoch 5448/10000, Prediction Accuracy = 62.352%, Loss = 0.41955049633979796
Epoch: 5448, Batch Gradient Norm: 8.399463785543801
Epoch: 5448, Batch Gradient Norm after: 8.399463785543801
Epoch 5449/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.4215783715248108
Epoch: 5449, Batch Gradient Norm: 10.44558993289021
Epoch: 5449, Batch Gradient Norm after: 10.44558993289021
Epoch 5450/10000, Prediction Accuracy = 62.334%, Loss = 0.43385682702064515
Epoch: 5450, Batch Gradient Norm: 12.905967635669002
Epoch: 5450, Batch Gradient Norm after: 12.905967635669002
Epoch 5451/10000, Prediction Accuracy = 62.288%, Loss = 0.4516320824623108
Epoch: 5451, Batch Gradient Norm: 12.091511266512425
Epoch: 5451, Batch Gradient Norm after: 12.091511266512425
Epoch 5452/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4462445914745331
Epoch: 5452, Batch Gradient Norm: 9.035028508793255
Epoch: 5452, Batch Gradient Norm after: 9.035028508793255
Epoch 5453/10000, Prediction Accuracy = 62.306%, Loss = 0.42373932600021363
Epoch: 5453, Batch Gradient Norm: 8.75796171446991
Epoch: 5453, Batch Gradient Norm after: 8.75796171446991
Epoch 5454/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.42214497923851013
Epoch: 5454, Batch Gradient Norm: 9.856514631715816
Epoch: 5454, Batch Gradient Norm after: 9.856514631715816
Epoch 5455/10000, Prediction Accuracy = 62.246%, Loss = 0.4291879415512085
Epoch: 5455, Batch Gradient Norm: 11.475052587078432
Epoch: 5455, Batch Gradient Norm after: 11.475052587078432
Epoch 5456/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.44267808794975283
Epoch: 5456, Batch Gradient Norm: 9.30654588199942
Epoch: 5456, Batch Gradient Norm after: 9.30654588199942
Epoch 5457/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4277628481388092
Epoch: 5457, Batch Gradient Norm: 9.640718006885026
Epoch: 5457, Batch Gradient Norm after: 9.640718006885026
Epoch 5458/10000, Prediction Accuracy = 62.42%, Loss = 0.42792720794677735
Epoch: 5458, Batch Gradient Norm: 10.777513565880739
Epoch: 5458, Batch Gradient Norm after: 10.777513565880739
Epoch 5459/10000, Prediction Accuracy = 62.2%, Loss = 0.43539509177207947
Epoch: 5459, Batch Gradient Norm: 10.408078436012925
Epoch: 5459, Batch Gradient Norm after: 10.408078436012925
Epoch 5460/10000, Prediction Accuracy = 62.364%, Loss = 0.43396600484848025
Epoch: 5460, Batch Gradient Norm: 9.416126017216772
Epoch: 5460, Batch Gradient Norm after: 9.416126017216772
Epoch 5461/10000, Prediction Accuracy = 62.136%, Loss = 0.4284372866153717
Epoch: 5461, Batch Gradient Norm: 8.253824920660268
Epoch: 5461, Batch Gradient Norm after: 8.253824920660268
Epoch 5462/10000, Prediction Accuracy = 62.17%, Loss = 0.4214917480945587
Epoch: 5462, Batch Gradient Norm: 8.018591158235184
Epoch: 5462, Batch Gradient Norm after: 8.018591158235184
Epoch 5463/10000, Prediction Accuracy = 62.206%, Loss = 0.4202907383441925
Epoch: 5463, Batch Gradient Norm: 9.372077914486338
Epoch: 5463, Batch Gradient Norm after: 9.372077914486338
Epoch 5464/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.42801313400268554
Epoch: 5464, Batch Gradient Norm: 12.02264714317882
Epoch: 5464, Batch Gradient Norm after: 12.02264714317882
Epoch 5465/10000, Prediction Accuracy = 62.342000000000006%, Loss = 0.4459392368793488
Epoch: 5465, Batch Gradient Norm: 13.22963946924174
Epoch: 5465, Batch Gradient Norm after: 13.22963946924174
Epoch 5466/10000, Prediction Accuracy = 62.314%, Loss = 0.4544063687324524
Epoch: 5466, Batch Gradient Norm: 9.055134180917474
Epoch: 5466, Batch Gradient Norm after: 9.055134180917474
Epoch 5467/10000, Prediction Accuracy = 62.31%, Loss = 0.4245447337627411
Epoch: 5467, Batch Gradient Norm: 7.5551426965984145
Epoch: 5467, Batch Gradient Norm after: 7.5551426965984145
Epoch 5468/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.4162555754184723
Epoch: 5468, Batch Gradient Norm: 8.62812077065968
Epoch: 5468, Batch Gradient Norm after: 8.62812077065968
Epoch 5469/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4211640596389771
Epoch: 5469, Batch Gradient Norm: 11.535240660009125
Epoch: 5469, Batch Gradient Norm after: 11.535240660009125
Epoch 5470/10000, Prediction Accuracy = 62.374%, Loss = 0.4408312201499939
Epoch: 5470, Batch Gradient Norm: 10.93412015130679
Epoch: 5470, Batch Gradient Norm after: 10.93412015130679
Epoch 5471/10000, Prediction Accuracy = 62.19000000000001%, Loss = 0.4367158651351929
Epoch: 5471, Batch Gradient Norm: 10.800519432251157
Epoch: 5471, Batch Gradient Norm after: 10.800519432251157
Epoch 5472/10000, Prediction Accuracy = 62.27%, Loss = 0.4356652319431305
Epoch: 5472, Batch Gradient Norm: 11.175776549938805
Epoch: 5472, Batch Gradient Norm after: 11.175776549938805
Epoch 5473/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.4414253056049347
Epoch: 5473, Batch Gradient Norm: 8.421350260297675
Epoch: 5473, Batch Gradient Norm after: 8.421350260297675
Epoch 5474/10000, Prediction Accuracy = 62.4%, Loss = 0.42221846580505373
Epoch: 5474, Batch Gradient Norm: 7.750286563130045
Epoch: 5474, Batch Gradient Norm after: 7.750286563130045
Epoch 5475/10000, Prediction Accuracy = 62.209999999999994%, Loss = 0.41757769584655763
Epoch: 5475, Batch Gradient Norm: 9.251083227787998
Epoch: 5475, Batch Gradient Norm after: 9.251083227787998
Epoch 5476/10000, Prediction Accuracy = 62.302%, Loss = 0.42602954506874086
Epoch: 5476, Batch Gradient Norm: 11.104443369989902
Epoch: 5476, Batch Gradient Norm after: 11.104443369989902
Epoch 5477/10000, Prediction Accuracy = 62.18399999999999%, Loss = 0.43835204243659975
Epoch: 5477, Batch Gradient Norm: 10.12096340839573
Epoch: 5477, Batch Gradient Norm after: 10.12096340839573
Epoch 5478/10000, Prediction Accuracy = 62.242%, Loss = 0.431106424331665
Epoch: 5478, Batch Gradient Norm: 9.776402272091165
Epoch: 5478, Batch Gradient Norm after: 9.776402272091165
Epoch 5479/10000, Prediction Accuracy = 62.33%, Loss = 0.42820921540260315
Epoch: 5479, Batch Gradient Norm: 12.78330749497666
Epoch: 5479, Batch Gradient Norm after: 12.78330749497666
Epoch 5480/10000, Prediction Accuracy = 62.25%, Loss = 0.4503394186496735
Epoch: 5480, Batch Gradient Norm: 12.79673145894352
Epoch: 5480, Batch Gradient Norm after: 12.79673145894352
Epoch 5481/10000, Prediction Accuracy = 62.378%, Loss = 0.45199407935142516
Epoch: 5481, Batch Gradient Norm: 10.296276781939293
Epoch: 5481, Batch Gradient Norm after: 10.296276781939293
Epoch 5482/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4330796539783478
Epoch: 5482, Batch Gradient Norm: 9.287518581955895
Epoch: 5482, Batch Gradient Norm after: 9.287518581955895
Epoch 5483/10000, Prediction Accuracy = 62.33%, Loss = 0.4253287732601166
Epoch: 5483, Batch Gradient Norm: 9.138490365281482
Epoch: 5483, Batch Gradient Norm after: 9.138490365281482
Epoch 5484/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.425066339969635
Epoch: 5484, Batch Gradient Norm: 9.107082621286693
Epoch: 5484, Batch Gradient Norm after: 9.107082621286693
Epoch 5485/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4249533534049988
Epoch: 5485, Batch Gradient Norm: 9.252136229601106
Epoch: 5485, Batch Gradient Norm after: 9.252136229601106
Epoch 5486/10000, Prediction Accuracy = 62.348%, Loss = 0.42541581988334654
Epoch: 5486, Batch Gradient Norm: 9.591372114631028
Epoch: 5486, Batch Gradient Norm after: 9.591372114631028
Epoch 5487/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.4273191452026367
Epoch: 5487, Batch Gradient Norm: 10.622396997185902
Epoch: 5487, Batch Gradient Norm after: 10.622396997185902
Epoch 5488/10000, Prediction Accuracy = 62.25%, Loss = 0.4354984700679779
Epoch: 5488, Batch Gradient Norm: 10.256258136200392
Epoch: 5488, Batch Gradient Norm after: 10.256258136200392
Epoch 5489/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4332699656486511
Epoch: 5489, Batch Gradient Norm: 10.396127009236828
Epoch: 5489, Batch Gradient Norm after: 10.396127009236828
Epoch 5490/10000, Prediction Accuracy = 62.27%, Loss = 0.43276631236076357
Epoch: 5490, Batch Gradient Norm: 11.361231279732936
Epoch: 5490, Batch Gradient Norm after: 11.361231279732936
Epoch 5491/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.43902714252471925
Epoch: 5491, Batch Gradient Norm: 10.622736616384923
Epoch: 5491, Batch Gradient Norm after: 10.622736616384923
Epoch 5492/10000, Prediction Accuracy = 62.254%, Loss = 0.4335650086402893
Epoch: 5492, Batch Gradient Norm: 10.46177426058181
Epoch: 5492, Batch Gradient Norm after: 10.46177426058181
Epoch 5493/10000, Prediction Accuracy = 62.384%, Loss = 0.4318052351474762
Epoch: 5493, Batch Gradient Norm: 10.536871139413066
Epoch: 5493, Batch Gradient Norm after: 10.536871139413066
Epoch 5494/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.43226616382598876
Epoch: 5494, Batch Gradient Norm: 10.068920405014968
Epoch: 5494, Batch Gradient Norm after: 10.068920405014968
Epoch 5495/10000, Prediction Accuracy = 62.298%, Loss = 0.43023804426193235
Epoch: 5495, Batch Gradient Norm: 9.620643288126553
Epoch: 5495, Batch Gradient Norm after: 9.620643288126553
Epoch 5496/10000, Prediction Accuracy = 62.214%, Loss = 0.42947134375572205
Epoch: 5496, Batch Gradient Norm: 7.771673888144401
Epoch: 5496, Batch Gradient Norm after: 7.771673888144401
Epoch 5497/10000, Prediction Accuracy = 62.262%, Loss = 0.4188543438911438
Epoch: 5497, Batch Gradient Norm: 7.682687483171582
Epoch: 5497, Batch Gradient Norm after: 7.682687483171582
Epoch 5498/10000, Prediction Accuracy = 62.424%, Loss = 0.4170106053352356
Epoch: 5498, Batch Gradient Norm: 8.466267117057058
Epoch: 5498, Batch Gradient Norm after: 8.466267117057058
Epoch 5499/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.41994677782058715
Epoch: 5499, Batch Gradient Norm: 10.460619544741045
Epoch: 5499, Batch Gradient Norm after: 10.460619544741045
Epoch 5500/10000, Prediction Accuracy = 62.24000000000001%, Loss = 0.4325577735900879
Epoch: 5500, Batch Gradient Norm: 11.48541187034589
Epoch: 5500, Batch Gradient Norm after: 11.48541187034589
Epoch 5501/10000, Prediction Accuracy = 62.136%, Loss = 0.44093685746192934
Epoch: 5501, Batch Gradient Norm: 11.685215653947571
Epoch: 5501, Batch Gradient Norm after: 11.685215653947571
Epoch 5502/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.44365785717964173
Epoch: 5502, Batch Gradient Norm: 10.289226989695297
Epoch: 5502, Batch Gradient Norm after: 10.289226989695297
Epoch 5503/10000, Prediction Accuracy = 62.419999999999995%, Loss = 0.4333665490150452
Epoch: 5503, Batch Gradient Norm: 10.43885389219251
Epoch: 5503, Batch Gradient Norm after: 10.43885389219251
Epoch 5504/10000, Prediction Accuracy = 62.269999999999996%, Loss = 0.4322014331817627
Epoch: 5504, Batch Gradient Norm: 10.847651806416442
Epoch: 5504, Batch Gradient Norm after: 10.847651806416442
Epoch 5505/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.43370957374572755
Epoch: 5505, Batch Gradient Norm: 10.45562901155565
Epoch: 5505, Batch Gradient Norm after: 10.45562901155565
Epoch 5506/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4316781282424927
Epoch: 5506, Batch Gradient Norm: 8.79728373900686
Epoch: 5506, Batch Gradient Norm after: 8.79728373900686
Epoch 5507/10000, Prediction Accuracy = 62.286%, Loss = 0.42179293632507325
Epoch: 5507, Batch Gradient Norm: 9.429065596815912
Epoch: 5507, Batch Gradient Norm after: 9.429065596815912
Epoch 5508/10000, Prediction Accuracy = 62.152%, Loss = 0.42581971883773806
Epoch: 5508, Batch Gradient Norm: 12.080634749666885
Epoch: 5508, Batch Gradient Norm after: 12.080634749666885
Epoch 5509/10000, Prediction Accuracy = 62.176%, Loss = 0.4462646782398224
Epoch: 5509, Batch Gradient Norm: 10.884982307728382
Epoch: 5509, Batch Gradient Norm after: 10.884982307728382
Epoch 5510/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.43688818216323855
Epoch: 5510, Batch Gradient Norm: 9.900618317222674
Epoch: 5510, Batch Gradient Norm after: 9.900618317222674
Epoch 5511/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.42819650173187257
Epoch: 5511, Batch Gradient Norm: 10.299984921279465
Epoch: 5511, Batch Gradient Norm after: 10.299984921279465
Epoch 5512/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.42977840900421144
Epoch: 5512, Batch Gradient Norm: 9.870521782237526
Epoch: 5512, Batch Gradient Norm after: 9.870521782237526
Epoch 5513/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.4268050968647003
Epoch: 5513, Batch Gradient Norm: 10.063990747510168
Epoch: 5513, Batch Gradient Norm after: 10.063990747510168
Epoch 5514/10000, Prediction Accuracy = 62.378%, Loss = 0.42943708300590516
Epoch: 5514, Batch Gradient Norm: 9.2786540715542
Epoch: 5514, Batch Gradient Norm after: 9.2786540715542
Epoch 5515/10000, Prediction Accuracy = 62.326%, Loss = 0.42757627964019773
Epoch: 5515, Batch Gradient Norm: 8.393420293748669
Epoch: 5515, Batch Gradient Norm after: 8.393420293748669
Epoch 5516/10000, Prediction Accuracy = 62.312%, Loss = 0.421038556098938
Epoch: 5516, Batch Gradient Norm: 10.511283091756429
Epoch: 5516, Batch Gradient Norm after: 10.511283091756429
Epoch 5517/10000, Prediction Accuracy = 62.314%, Loss = 0.43269120454788207
Epoch: 5517, Batch Gradient Norm: 12.253062769052605
Epoch: 5517, Batch Gradient Norm after: 12.253062769052605
Epoch 5518/10000, Prediction Accuracy = 62.29%, Loss = 0.44644291400909425
Epoch: 5518, Batch Gradient Norm: 9.584817611954477
Epoch: 5518, Batch Gradient Norm after: 9.584817611954477
Epoch 5519/10000, Prediction Accuracy = 62.334%, Loss = 0.42807272672653196
Epoch: 5519, Batch Gradient Norm: 8.499900731934412
Epoch: 5519, Batch Gradient Norm after: 8.499900731934412
Epoch 5520/10000, Prediction Accuracy = 62.29%, Loss = 0.42102071046829226
Epoch: 5520, Batch Gradient Norm: 9.174910331942696
Epoch: 5520, Batch Gradient Norm after: 9.174910331942696
Epoch 5521/10000, Prediction Accuracy = 62.242%, Loss = 0.4252011477947235
Epoch: 5521, Batch Gradient Norm: 10.38029841568926
Epoch: 5521, Batch Gradient Norm after: 10.38029841568926
Epoch 5522/10000, Prediction Accuracy = 62.254000000000005%, Loss = 0.4319080114364624
Epoch: 5522, Batch Gradient Norm: 11.420857949072879
Epoch: 5522, Batch Gradient Norm after: 11.420857949072879
Epoch 5523/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.4396992862224579
Epoch: 5523, Batch Gradient Norm: 10.274910957002415
Epoch: 5523, Batch Gradient Norm after: 10.274910957002415
Epoch 5524/10000, Prediction Accuracy = 62.314%, Loss = 0.4319277167320251
Epoch: 5524, Batch Gradient Norm: 8.787281395504504
Epoch: 5524, Batch Gradient Norm after: 8.787281395504504
Epoch 5525/10000, Prediction Accuracy = 62.246%, Loss = 0.4216072976589203
Epoch: 5525, Batch Gradient Norm: 9.866865426591085
Epoch: 5525, Batch Gradient Norm after: 9.866865426591085
Epoch 5526/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.4270699083805084
Epoch: 5526, Batch Gradient Norm: 11.845604706306668
Epoch: 5526, Batch Gradient Norm after: 11.845604706306668
Epoch 5527/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4421414017677307
Epoch: 5527, Batch Gradient Norm: 10.130908824488097
Epoch: 5527, Batch Gradient Norm after: 10.130908824488097
Epoch 5528/10000, Prediction Accuracy = 62.39%, Loss = 0.4314404547214508
Epoch: 5528, Batch Gradient Norm: 8.520510980976539
Epoch: 5528, Batch Gradient Norm after: 8.520510980976539
Epoch 5529/10000, Prediction Accuracy = 62.266%, Loss = 0.42032278776168824
Epoch: 5529, Batch Gradient Norm: 9.088143052321126
Epoch: 5529, Batch Gradient Norm after: 9.088143052321126
Epoch 5530/10000, Prediction Accuracy = 62.37800000000001%, Loss = 0.4224209189414978
Epoch: 5530, Batch Gradient Norm: 10.603673993998292
Epoch: 5530, Batch Gradient Norm after: 10.603673993998292
Epoch 5531/10000, Prediction Accuracy = 62.21%, Loss = 0.43225120902061465
Epoch: 5531, Batch Gradient Norm: 9.352962055693304
Epoch: 5531, Batch Gradient Norm after: 9.352962055693304
Epoch 5532/10000, Prediction Accuracy = 62.352%, Loss = 0.42546849250793456
Epoch: 5532, Batch Gradient Norm: 8.698053763387632
Epoch: 5532, Batch Gradient Norm after: 8.698053763387632
Epoch 5533/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.4210114598274231
Epoch: 5533, Batch Gradient Norm: 11.115508040460448
Epoch: 5533, Batch Gradient Norm after: 11.115508040460448
Epoch 5534/10000, Prediction Accuracy = 62.246%, Loss = 0.4377905547618866
Epoch: 5534, Batch Gradient Norm: 11.487550206770841
Epoch: 5534, Batch Gradient Norm after: 11.487550206770841
Epoch 5535/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4410873353481293
Epoch: 5535, Batch Gradient Norm: 8.6873899408822
Epoch: 5535, Batch Gradient Norm after: 8.6873899408822
Epoch 5536/10000, Prediction Accuracy = 62.242000000000004%, Loss = 0.4212189197540283
Epoch: 5536, Batch Gradient Norm: 8.757604159695072
Epoch: 5536, Batch Gradient Norm after: 8.757604159695072
Epoch 5537/10000, Prediction Accuracy = 62.36199999999999%, Loss = 0.4202524244785309
Epoch: 5537, Batch Gradient Norm: 12.979952673624805
Epoch: 5537, Batch Gradient Norm after: 12.979952673624805
Epoch 5538/10000, Prediction Accuracy = 62.072%, Loss = 0.4497990071773529
Epoch: 5538, Batch Gradient Norm: 13.82748056561302
Epoch: 5538, Batch Gradient Norm after: 13.82748056561302
Epoch 5539/10000, Prediction Accuracy = 62.288%, Loss = 0.45984346866607667
Epoch: 5539, Batch Gradient Norm: 10.202227702481897
Epoch: 5539, Batch Gradient Norm after: 10.202227702481897
Epoch 5540/10000, Prediction Accuracy = 62.28000000000001%, Loss = 0.43037917613983157
Epoch: 5540, Batch Gradient Norm: 9.206684076961022
Epoch: 5540, Batch Gradient Norm after: 9.206684076961022
Epoch 5541/10000, Prediction Accuracy = 62.214%, Loss = 0.42460601925849917
Epoch: 5541, Batch Gradient Norm: 8.39904821784346
Epoch: 5541, Batch Gradient Norm after: 8.39904821784346
Epoch 5542/10000, Prediction Accuracy = 62.314%, Loss = 0.4203775584697723
Epoch: 5542, Batch Gradient Norm: 7.697850140824689
Epoch: 5542, Batch Gradient Norm after: 7.697850140824689
Epoch 5543/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41662502884864805
Epoch: 5543, Batch Gradient Norm: 9.630700642860049
Epoch: 5543, Batch Gradient Norm after: 9.630700642860049
Epoch 5544/10000, Prediction Accuracy = 62.29%, Loss = 0.42739440202713014
Epoch: 5544, Batch Gradient Norm: 12.562966559515207
Epoch: 5544, Batch Gradient Norm after: 12.562966559515207
Epoch 5545/10000, Prediction Accuracy = 62.31%, Loss = 0.44925338625907896
Epoch: 5545, Batch Gradient Norm: 10.934392805480147
Epoch: 5545, Batch Gradient Norm after: 10.934392805480147
Epoch 5546/10000, Prediction Accuracy = 62.148%, Loss = 0.4345278382301331
Epoch: 5546, Batch Gradient Norm: 8.17309695376875
Epoch: 5546, Batch Gradient Norm after: 8.17309695376875
Epoch 5547/10000, Prediction Accuracy = 62.396%, Loss = 0.4170360565185547
Epoch: 5547, Batch Gradient Norm: 7.404383343844741
Epoch: 5547, Batch Gradient Norm after: 7.404383343844741
Epoch 5548/10000, Prediction Accuracy = 62.31%, Loss = 0.4127806842327118
Epoch: 5548, Batch Gradient Norm: 8.712552418046256
Epoch: 5548, Batch Gradient Norm after: 8.712552418046256
Epoch 5549/10000, Prediction Accuracy = 62.31999999999999%, Loss = 0.4198610603809357
Epoch: 5549, Batch Gradient Norm: 11.26910282152644
Epoch: 5549, Batch Gradient Norm after: 11.26910282152644
Epoch 5550/10000, Prediction Accuracy = 62.282%, Loss = 0.4382000625133514
Epoch: 5550, Batch Gradient Norm: 10.961125205853248
Epoch: 5550, Batch Gradient Norm after: 10.961125205853248
Epoch 5551/10000, Prediction Accuracy = 62.37800000000001%, Loss = 0.4348350644111633
Epoch: 5551, Batch Gradient Norm: 11.752696861289492
Epoch: 5551, Batch Gradient Norm after: 11.752696861289492
Epoch 5552/10000, Prediction Accuracy = 62.254%, Loss = 0.4406810462474823
Epoch: 5552, Batch Gradient Norm: 11.617203320517959
Epoch: 5552, Batch Gradient Norm after: 11.617203320517959
Epoch 5553/10000, Prediction Accuracy = 62.26400000000001%, Loss = 0.44030059576034547
Epoch: 5553, Batch Gradient Norm: 10.835993234917447
Epoch: 5553, Batch Gradient Norm after: 10.835993234917447
Epoch 5554/10000, Prediction Accuracy = 62.2%, Loss = 0.43487994074821473
Epoch: 5554, Batch Gradient Norm: 10.253070105600349
Epoch: 5554, Batch Gradient Norm after: 10.253070105600349
Epoch 5555/10000, Prediction Accuracy = 62.21%, Loss = 0.43106337189674376
Epoch: 5555, Batch Gradient Norm: 8.97343343190073
Epoch: 5555, Batch Gradient Norm after: 8.97343343190073
Epoch 5556/10000, Prediction Accuracy = 62.302%, Loss = 0.4225491344928741
Epoch: 5556, Batch Gradient Norm: 7.953109714469741
Epoch: 5556, Batch Gradient Norm after: 7.953109714469741
Epoch 5557/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41653531789779663
Epoch: 5557, Batch Gradient Norm: 8.186583593883054
Epoch: 5557, Batch Gradient Norm after: 8.186583593883054
Epoch 5558/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.4176636874675751
Epoch: 5558, Batch Gradient Norm: 9.057664948861358
Epoch: 5558, Batch Gradient Norm after: 9.057664948861358
Epoch 5559/10000, Prediction Accuracy = 62.314%, Loss = 0.4230688393115997
Epoch: 5559, Batch Gradient Norm: 10.254385890672614
Epoch: 5559, Batch Gradient Norm after: 10.254385890672614
Epoch 5560/10000, Prediction Accuracy = 62.220000000000006%, Loss = 0.43059908747673037
Epoch: 5560, Batch Gradient Norm: 11.13769077089212
Epoch: 5560, Batch Gradient Norm after: 11.13769077089212
Epoch 5561/10000, Prediction Accuracy = 62.314%, Loss = 0.4376260221004486
Epoch: 5561, Batch Gradient Norm: 10.107559303228436
Epoch: 5561, Batch Gradient Norm after: 10.107559303228436
Epoch 5562/10000, Prediction Accuracy = 62.246%, Loss = 0.430600243806839
Epoch: 5562, Batch Gradient Norm: 9.47856094800121
Epoch: 5562, Batch Gradient Norm after: 9.47856094800121
Epoch 5563/10000, Prediction Accuracy = 62.238%, Loss = 0.42649797201156614
Epoch: 5563, Batch Gradient Norm: 10.679293864443641
Epoch: 5563, Batch Gradient Norm after: 10.679293864443641
Epoch 5564/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.43466476202011106
Epoch: 5564, Batch Gradient Norm: 11.81076099340266
Epoch: 5564, Batch Gradient Norm after: 11.81076099340266
Epoch 5565/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.4422666132450104
Epoch: 5565, Batch Gradient Norm: 11.224389003349998
Epoch: 5565, Batch Gradient Norm after: 11.224389003349998
Epoch 5566/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4368965089321136
Epoch: 5566, Batch Gradient Norm: 9.522677287632915
Epoch: 5566, Batch Gradient Norm after: 9.522677287632915
Epoch 5567/10000, Prediction Accuracy = 62.336%, Loss = 0.42552616000175475
Epoch: 5567, Batch Gradient Norm: 8.298920612897419
Epoch: 5567, Batch Gradient Norm after: 8.298920612897419
Epoch 5568/10000, Prediction Accuracy = 62.306000000000004%, Loss = 0.41841148138046264
Epoch: 5568, Batch Gradient Norm: 8.889586309906292
Epoch: 5568, Batch Gradient Norm after: 8.889586309906292
Epoch 5569/10000, Prediction Accuracy = 62.354%, Loss = 0.42104997038841246
Epoch: 5569, Batch Gradient Norm: 10.678927548333316
Epoch: 5569, Batch Gradient Norm after: 10.678927548333316
Epoch 5570/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.43266960978507996
Epoch: 5570, Batch Gradient Norm: 11.117415802305969
Epoch: 5570, Batch Gradient Norm after: 11.117415802305969
Epoch 5571/10000, Prediction Accuracy = 62.35%, Loss = 0.4364916205406189
Epoch: 5571, Batch Gradient Norm: 10.13124372699214
Epoch: 5571, Batch Gradient Norm after: 10.13124372699214
Epoch 5572/10000, Prediction Accuracy = 62.39000000000001%, Loss = 0.429719078540802
Epoch: 5572, Batch Gradient Norm: 10.5498394660315
Epoch: 5572, Batch Gradient Norm after: 10.5498394660315
Epoch 5573/10000, Prediction Accuracy = 62.15599999999999%, Loss = 0.4314440727233887
Epoch: 5573, Batch Gradient Norm: 11.375642134263837
Epoch: 5573, Batch Gradient Norm after: 11.375642134263837
Epoch 5574/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.43688505291938784
Epoch: 5574, Batch Gradient Norm: 11.167048630097089
Epoch: 5574, Batch Gradient Norm after: 11.167048630097089
Epoch 5575/10000, Prediction Accuracy = 62.27%, Loss = 0.43573963046073916
Epoch: 5575, Batch Gradient Norm: 10.868697765079675
Epoch: 5575, Batch Gradient Norm after: 10.868697765079675
Epoch 5576/10000, Prediction Accuracy = 62.322%, Loss = 0.4354482233524323
Epoch: 5576, Batch Gradient Norm: 9.551388666995674
Epoch: 5576, Batch Gradient Norm after: 9.551388666995674
Epoch 5577/10000, Prediction Accuracy = 62.364%, Loss = 0.42657933831214906
Epoch: 5577, Batch Gradient Norm: 9.272085829197415
Epoch: 5577, Batch Gradient Norm after: 9.272085829197415
Epoch 5578/10000, Prediction Accuracy = 62.153999999999996%, Loss = 0.42281520962715147
Epoch: 5578, Batch Gradient Norm: 10.519023600810113
Epoch: 5578, Batch Gradient Norm after: 10.519023600810113
Epoch 5579/10000, Prediction Accuracy = 62.322%, Loss = 0.43026520013809205
Epoch: 5579, Batch Gradient Norm: 11.020573671376189
Epoch: 5579, Batch Gradient Norm after: 11.020573671376189
Epoch 5580/10000, Prediction Accuracy = 62.181999999999995%, Loss = 0.4345480680465698
Epoch: 5580, Batch Gradient Norm: 9.124380993003257
Epoch: 5580, Batch Gradient Norm after: 9.124380993003257
Epoch 5581/10000, Prediction Accuracy = 62.39799999999999%, Loss = 0.4219523012638092
Epoch: 5581, Batch Gradient Norm: 8.978355114014116
Epoch: 5581, Batch Gradient Norm after: 8.978355114014116
Epoch 5582/10000, Prediction Accuracy = 62.306%, Loss = 0.42058971524238586
Epoch: 5582, Batch Gradient Norm: 10.861569565220833
Epoch: 5582, Batch Gradient Norm after: 10.861569565220833
Epoch 5583/10000, Prediction Accuracy = 62.322%, Loss = 0.43210047483444214
Epoch: 5583, Batch Gradient Norm: 11.822009552507879
Epoch: 5583, Batch Gradient Norm after: 11.822009552507879
Epoch 5584/10000, Prediction Accuracy = 62.24399999999999%, Loss = 0.4399219751358032
Epoch: 5584, Batch Gradient Norm: 10.091713688378508
Epoch: 5584, Batch Gradient Norm after: 10.091713688378508
Epoch 5585/10000, Prediction Accuracy = 62.27%, Loss = 0.42850732803344727
Epoch: 5585, Batch Gradient Norm: 8.682093710604425
Epoch: 5585, Batch Gradient Norm after: 8.682093710604425
Epoch 5586/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.42072035670280455
Epoch: 5586, Batch Gradient Norm: 8.215167743522723
Epoch: 5586, Batch Gradient Norm after: 8.215167743522723
Epoch 5587/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.41957369446754456
Epoch: 5587, Batch Gradient Norm: 7.312783459148634
Epoch: 5587, Batch Gradient Norm after: 7.312783459148634
Epoch 5588/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.4156301259994507
Epoch: 5588, Batch Gradient Norm: 7.0364277022432455
Epoch: 5588, Batch Gradient Norm after: 7.0364277022432455
Epoch 5589/10000, Prediction Accuracy = 62.322%, Loss = 0.4130560576915741
Epoch: 5589, Batch Gradient Norm: 9.175355729644691
Epoch: 5589, Batch Gradient Norm after: 9.175355729644691
Epoch 5590/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4231168031692505
Epoch: 5590, Batch Gradient Norm: 12.704046252236536
Epoch: 5590, Batch Gradient Norm after: 12.704046252236536
Epoch 5591/10000, Prediction Accuracy = 62.31%, Loss = 0.44599042534828187
Epoch: 5591, Batch Gradient Norm: 12.980634318589358
Epoch: 5591, Batch Gradient Norm after: 12.980634318589358
Epoch 5592/10000, Prediction Accuracy = 62.21%, Loss = 0.4485583245754242
Epoch: 5592, Batch Gradient Norm: 10.477318943233724
Epoch: 5592, Batch Gradient Norm after: 10.477318943233724
Epoch 5593/10000, Prediction Accuracy = 62.205999999999996%, Loss = 0.4309088349342346
Epoch: 5593, Batch Gradient Norm: 9.923947498549298
Epoch: 5593, Batch Gradient Norm after: 9.923947498549298
Epoch 5594/10000, Prediction Accuracy = 62.234%, Loss = 0.42778990864753724
Epoch: 5594, Batch Gradient Norm: 9.58104860088699
Epoch: 5594, Batch Gradient Norm after: 9.58104860088699
Epoch 5595/10000, Prediction Accuracy = 62.176%, Loss = 0.42499237656593325
Epoch: 5595, Batch Gradient Norm: 9.228338826032136
Epoch: 5595, Batch Gradient Norm after: 9.228338826032136
Epoch 5596/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4224883198738098
Epoch: 5596, Batch Gradient Norm: 9.297157262531409
Epoch: 5596, Batch Gradient Norm after: 9.297157262531409
Epoch 5597/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.4232508897781372
Epoch: 5597, Batch Gradient Norm: 9.830707197266147
Epoch: 5597, Batch Gradient Norm after: 9.830707197266147
Epoch 5598/10000, Prediction Accuracy = 62.218%, Loss = 0.4273160398006439
Epoch: 5598, Batch Gradient Norm: 11.086862183529943
Epoch: 5598, Batch Gradient Norm after: 11.086862183529943
Epoch 5599/10000, Prediction Accuracy = 62.312%, Loss = 0.4380170464515686
Epoch: 5599, Batch Gradient Norm: 11.559044044563263
Epoch: 5599, Batch Gradient Norm after: 11.559044044563263
Epoch 5600/10000, Prediction Accuracy = 62.239999999999995%, Loss = 0.4414952456951141
Epoch: 5600, Batch Gradient Norm: 9.16654774867476
Epoch: 5600, Batch Gradient Norm after: 9.16654774867476
Epoch 5601/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.4232548356056213
Epoch: 5601, Batch Gradient Norm: 9.259644286423242
Epoch: 5601, Batch Gradient Norm after: 9.259644286423242
Epoch 5602/10000, Prediction Accuracy = 62.322%, Loss = 0.4232418179512024
Epoch: 5602, Batch Gradient Norm: 9.790087944771564
Epoch: 5602, Batch Gradient Norm after: 9.790087944771564
Epoch 5603/10000, Prediction Accuracy = 62.302%, Loss = 0.42620710134506223
Epoch: 5603, Batch Gradient Norm: 11.094599620875295
Epoch: 5603, Batch Gradient Norm after: 11.094599620875295
Epoch 5604/10000, Prediction Accuracy = 62.27%, Loss = 0.4347082912921906
Epoch: 5604, Batch Gradient Norm: 10.988905938917998
Epoch: 5604, Batch Gradient Norm after: 10.988905938917998
Epoch 5605/10000, Prediction Accuracy = 62.45%, Loss = 0.4346284449100494
Epoch: 5605, Batch Gradient Norm: 8.883830834384282
Epoch: 5605, Batch Gradient Norm after: 8.883830834384282
Epoch 5606/10000, Prediction Accuracy = 62.298%, Loss = 0.4210741877555847
Epoch: 5606, Batch Gradient Norm: 8.592132926879957
Epoch: 5606, Batch Gradient Norm after: 8.592132926879957
Epoch 5607/10000, Prediction Accuracy = 62.398%, Loss = 0.4196488559246063
Epoch: 5607, Batch Gradient Norm: 9.927396776098945
Epoch: 5607, Batch Gradient Norm after: 9.927396776098945
Epoch 5608/10000, Prediction Accuracy = 62.324%, Loss = 0.42683462500572206
Epoch: 5608, Batch Gradient Norm: 12.94006125308879
Epoch: 5608, Batch Gradient Norm after: 12.94006125308879
Epoch 5609/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.44900187849998474
Epoch: 5609, Batch Gradient Norm: 12.081872513451854
Epoch: 5609, Batch Gradient Norm after: 12.081872513451854
Epoch 5610/10000, Prediction Accuracy = 62.36%, Loss = 0.44171798825263975
Epoch: 5610, Batch Gradient Norm: 9.44230399159926
Epoch: 5610, Batch Gradient Norm after: 9.44230399159926
Epoch 5611/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.42357234954833983
Epoch: 5611, Batch Gradient Norm: 7.699221232428123
Epoch: 5611, Batch Gradient Norm after: 7.699221232428123
Epoch 5612/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4139184892177582
Epoch: 5612, Batch Gradient Norm: 8.247102057745316
Epoch: 5612, Batch Gradient Norm after: 8.247102057745316
Epoch 5613/10000, Prediction Accuracy = 62.254%, Loss = 0.41691486835479735
Epoch: 5613, Batch Gradient Norm: 10.075278570120911
Epoch: 5613, Batch Gradient Norm after: 10.075278570120911
Epoch 5614/10000, Prediction Accuracy = 62.266000000000005%, Loss = 0.42875418066978455
Epoch: 5614, Batch Gradient Norm: 12.500671997220808
Epoch: 5614, Batch Gradient Norm after: 12.500671997220808
Epoch 5615/10000, Prediction Accuracy = 62.471999999999994%, Loss = 0.4460959255695343
Epoch: 5615, Batch Gradient Norm: 11.424742538645287
Epoch: 5615, Batch Gradient Norm after: 11.424742538645287
Epoch 5616/10000, Prediction Accuracy = 62.160000000000004%, Loss = 0.43835155963897704
Epoch: 5616, Batch Gradient Norm: 9.372918227335624
Epoch: 5616, Batch Gradient Norm after: 9.372918227335624
Epoch 5617/10000, Prediction Accuracy = 62.366%, Loss = 0.4230305850505829
Epoch: 5617, Batch Gradient Norm: 9.278966961882142
Epoch: 5617, Batch Gradient Norm after: 9.278966961882142
Epoch 5618/10000, Prediction Accuracy = 62.158%, Loss = 0.42157294154167174
Epoch: 5618, Batch Gradient Norm: 10.086548763547762
Epoch: 5618, Batch Gradient Norm after: 10.086548763547762
Epoch 5619/10000, Prediction Accuracy = 62.302%, Loss = 0.4285160839557648
Epoch: 5619, Batch Gradient Norm: 9.078404725406415
Epoch: 5619, Batch Gradient Norm after: 9.078404725406415
Epoch 5620/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4226565778255463
Epoch: 5620, Batch Gradient Norm: 7.925191520549237
Epoch: 5620, Batch Gradient Norm after: 7.925191520549237
Epoch 5621/10000, Prediction Accuracy = 62.294%, Loss = 0.41561920046806333
Epoch: 5621, Batch Gradient Norm: 9.978218571535667
Epoch: 5621, Batch Gradient Norm after: 9.978218571535667
Epoch 5622/10000, Prediction Accuracy = 62.5%, Loss = 0.4261674642562866
Epoch: 5622, Batch Gradient Norm: 11.123267371313837
Epoch: 5622, Batch Gradient Norm after: 11.123267371313837
Epoch 5623/10000, Prediction Accuracy = 62.278%, Loss = 0.4343379080295563
Epoch: 5623, Batch Gradient Norm: 10.20600759451511
Epoch: 5623, Batch Gradient Norm after: 10.20600759451511
Epoch 5624/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.4290729820728302
Epoch: 5624, Batch Gradient Norm: 9.017946920880856
Epoch: 5624, Batch Gradient Norm after: 9.017946920880856
Epoch 5625/10000, Prediction Accuracy = 62.36400000000001%, Loss = 0.42180814743041994
Epoch: 5625, Batch Gradient Norm: 8.752127931154446
Epoch: 5625, Batch Gradient Norm after: 8.752127931154446
Epoch 5626/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.4202243149280548
Epoch: 5626, Batch Gradient Norm: 11.655239593298028
Epoch: 5626, Batch Gradient Norm after: 11.655239593298028
Epoch 5627/10000, Prediction Accuracy = 62.354%, Loss = 0.43772175908088684
Epoch: 5627, Batch Gradient Norm: 12.229125667549068
Epoch: 5627, Batch Gradient Norm after: 12.229125667549068
Epoch 5628/10000, Prediction Accuracy = 62.274%, Loss = 0.4445058345794678
Epoch: 5628, Batch Gradient Norm: 8.339204878403578
Epoch: 5628, Batch Gradient Norm after: 8.339204878403578
Epoch 5629/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.41753135323524476
Epoch: 5629, Batch Gradient Norm: 7.437997670947246
Epoch: 5629, Batch Gradient Norm after: 7.437997670947246
Epoch 5630/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.41173189878463745
Epoch: 5630, Batch Gradient Norm: 10.371586363861853
Epoch: 5630, Batch Gradient Norm after: 10.371586363861853
Epoch 5631/10000, Prediction Accuracy = 62.19199999999999%, Loss = 0.42940937876701357
Epoch: 5631, Batch Gradient Norm: 10.789672675877835
Epoch: 5631, Batch Gradient Norm after: 10.789672675877835
Epoch 5632/10000, Prediction Accuracy = 62.362%, Loss = 0.4343613386154175
Epoch: 5632, Batch Gradient Norm: 10.900598692516402
Epoch: 5632, Batch Gradient Norm after: 10.900598692516402
Epoch 5633/10000, Prediction Accuracy = 62.206%, Loss = 0.4324226379394531
Epoch: 5633, Batch Gradient Norm: 12.697406946520507
Epoch: 5633, Batch Gradient Norm after: 12.697406946520507
Epoch 5634/10000, Prediction Accuracy = 62.226%, Loss = 0.4459623575210571
Epoch: 5634, Batch Gradient Norm: 11.46531589209205
Epoch: 5634, Batch Gradient Norm after: 11.46531589209205
Epoch 5635/10000, Prediction Accuracy = 62.32199999999999%, Loss = 0.4367198169231415
Epoch: 5635, Batch Gradient Norm: 8.909908188918918
Epoch: 5635, Batch Gradient Norm after: 8.909908188918918
Epoch 5636/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.41940516233444214
Epoch: 5636, Batch Gradient Norm: 9.108376006469658
Epoch: 5636, Batch Gradient Norm after: 9.108376006469658
Epoch 5637/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.42064619064331055
Epoch: 5637, Batch Gradient Norm: 10.383152633732179
Epoch: 5637, Batch Gradient Norm after: 10.383152633732179
Epoch 5638/10000, Prediction Accuracy = 62.194%, Loss = 0.4291776180267334
Epoch: 5638, Batch Gradient Norm: 11.599405408277107
Epoch: 5638, Batch Gradient Norm after: 11.599405408277107
Epoch 5639/10000, Prediction Accuracy = 62.402%, Loss = 0.44075013399124147
Epoch: 5639, Batch Gradient Norm: 9.20792918522384
Epoch: 5639, Batch Gradient Norm after: 9.20792918522384
Epoch 5640/10000, Prediction Accuracy = 62.374%, Loss = 0.42306508421897887
Epoch: 5640, Batch Gradient Norm: 10.055252308489838
Epoch: 5640, Batch Gradient Norm after: 10.055252308489838
Epoch 5641/10000, Prediction Accuracy = 62.274%, Loss = 0.4269962012767792
Epoch: 5641, Batch Gradient Norm: 10.855831145279916
Epoch: 5641, Batch Gradient Norm after: 10.855831145279916
Epoch 5642/10000, Prediction Accuracy = 62.372%, Loss = 0.43425934910774233
Epoch: 5642, Batch Gradient Norm: 8.375114030976828
Epoch: 5642, Batch Gradient Norm after: 8.375114030976828
Epoch 5643/10000, Prediction Accuracy = 62.4%, Loss = 0.4179352581501007
Epoch: 5643, Batch Gradient Norm: 6.934705084888056
Epoch: 5643, Batch Gradient Norm after: 6.934705084888056
Epoch 5644/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.4099013388156891
Epoch: 5644, Batch Gradient Norm: 8.303039325516286
Epoch: 5644, Batch Gradient Norm after: 8.303039325516286
Epoch 5645/10000, Prediction Accuracy = 62.398%, Loss = 0.4162132263183594
Epoch: 5645, Batch Gradient Norm: 11.777848790210571
Epoch: 5645, Batch Gradient Norm after: 11.777848790210571
Epoch 5646/10000, Prediction Accuracy = 62.18799999999999%, Loss = 0.43868295550346376
Epoch: 5646, Batch Gradient Norm: 11.244596419065465
Epoch: 5646, Batch Gradient Norm after: 11.244596419065465
Epoch 5647/10000, Prediction Accuracy = 62.312%, Loss = 0.4366209328174591
Epoch: 5647, Batch Gradient Norm: 8.985134630849242
Epoch: 5647, Batch Gradient Norm after: 8.985134630849242
Epoch 5648/10000, Prediction Accuracy = 62.298%, Loss = 0.42118335962295533
Epoch: 5648, Batch Gradient Norm: 10.250774579660858
Epoch: 5648, Batch Gradient Norm after: 10.250774579660858
Epoch 5649/10000, Prediction Accuracy = 62.084%, Loss = 0.42811248302459715
Epoch: 5649, Batch Gradient Norm: 12.172421720355999
Epoch: 5649, Batch Gradient Norm after: 12.172421720355999
Epoch 5650/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.4414666354656219
Epoch: 5650, Batch Gradient Norm: 10.424718629486616
Epoch: 5650, Batch Gradient Norm after: 10.424718629486616
Epoch 5651/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.42816612124443054
Epoch: 5651, Batch Gradient Norm: 8.03127032265558
Epoch: 5651, Batch Gradient Norm after: 8.03127032265558
Epoch 5652/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.41347070336341857
Epoch: 5652, Batch Gradient Norm: 7.7849691219506365
Epoch: 5652, Batch Gradient Norm after: 7.7849691219506365
Epoch 5653/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.41203855276107787
Epoch: 5653, Batch Gradient Norm: 10.860400521280503
Epoch: 5653, Batch Gradient Norm after: 10.860400521280503
Epoch 5654/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.43100897669792176
Epoch: 5654, Batch Gradient Norm: 12.473774295899801
Epoch: 5654, Batch Gradient Norm after: 12.473774295899801
Epoch 5655/10000, Prediction Accuracy = 62.174%, Loss = 0.44528177976608274
Epoch: 5655, Batch Gradient Norm: 9.934717725056915
Epoch: 5655, Batch Gradient Norm after: 9.934717725056915
Epoch 5656/10000, Prediction Accuracy = 62.162%, Loss = 0.4257334232330322
Epoch: 5656, Batch Gradient Norm: 10.17329850490261
Epoch: 5656, Batch Gradient Norm after: 10.17329850490261
Epoch 5657/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.42755733132362367
Epoch: 5657, Batch Gradient Norm: 10.709852220487386
Epoch: 5657, Batch Gradient Norm after: 10.709852220487386
Epoch 5658/10000, Prediction Accuracy = 62.17999999999999%, Loss = 0.43440762162208557
Epoch: 5658, Batch Gradient Norm: 8.57970601220032
Epoch: 5658, Batch Gradient Norm after: 8.57970601220032
Epoch 5659/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4194962978363037
Epoch: 5659, Batch Gradient Norm: 8.674702902161595
Epoch: 5659, Batch Gradient Norm after: 8.674702902161595
Epoch 5660/10000, Prediction Accuracy = 62.32800000000001%, Loss = 0.4189753711223602
Epoch: 5660, Batch Gradient Norm: 11.243672979851633
Epoch: 5660, Batch Gradient Norm after: 11.243672979851633
Epoch 5661/10000, Prediction Accuracy = 62.338%, Loss = 0.4372978687286377
Epoch: 5661, Batch Gradient Norm: 10.262257508975733
Epoch: 5661, Batch Gradient Norm after: 10.262257508975733
Epoch 5662/10000, Prediction Accuracy = 62.326%, Loss = 0.42854406833648684
Epoch: 5662, Batch Gradient Norm: 11.131981433441101
Epoch: 5662, Batch Gradient Norm after: 11.131981433441101
Epoch 5663/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4350109934806824
Epoch: 5663, Batch Gradient Norm: 10.298656804678817
Epoch: 5663, Batch Gradient Norm after: 10.298656804678817
Epoch 5664/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.4293430507183075
Epoch: 5664, Batch Gradient Norm: 10.462577961947197
Epoch: 5664, Batch Gradient Norm after: 10.462577961947197
Epoch 5665/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.42833285331726073
Epoch: 5665, Batch Gradient Norm: 10.26657323890286
Epoch: 5665, Batch Gradient Norm after: 10.26657323890286
Epoch 5666/10000, Prediction Accuracy = 62.266%, Loss = 0.42641705870628355
Epoch: 5666, Batch Gradient Norm: 9.34340770895061
Epoch: 5666, Batch Gradient Norm after: 9.34340770895061
Epoch 5667/10000, Prediction Accuracy = 62.412%, Loss = 0.42119001150131224
Epoch: 5667, Batch Gradient Norm: 8.635691181691492
Epoch: 5667, Batch Gradient Norm after: 8.635691181691492
Epoch 5668/10000, Prediction Accuracy = 62.290000000000006%, Loss = 0.4174902379512787
Epoch: 5668, Batch Gradient Norm: 8.696292633090106
Epoch: 5668, Batch Gradient Norm after: 8.696292633090106
Epoch 5669/10000, Prediction Accuracy = 62.338%, Loss = 0.4183929145336151
Epoch: 5669, Batch Gradient Norm: 10.115031438310579
Epoch: 5669, Batch Gradient Norm after: 10.115031438310579
Epoch 5670/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.42754759192466735
Epoch: 5670, Batch Gradient Norm: 10.830881294097072
Epoch: 5670, Batch Gradient Norm after: 10.830881294097072
Epoch 5671/10000, Prediction Accuracy = 62.212%, Loss = 0.431544291973114
Epoch: 5671, Batch Gradient Norm: 10.674129366976459
Epoch: 5671, Batch Gradient Norm after: 10.674129366976459
Epoch 5672/10000, Prediction Accuracy = 62.312%, Loss = 0.43080863952636717
Epoch: 5672, Batch Gradient Norm: 9.527994819652827
Epoch: 5672, Batch Gradient Norm after: 9.527994819652827
Epoch 5673/10000, Prediction Accuracy = 62.24000000000001%, Loss = 0.42395107746124266
Epoch: 5673, Batch Gradient Norm: 8.47230982189173
Epoch: 5673, Batch Gradient Norm after: 8.47230982189173
Epoch 5674/10000, Prediction Accuracy = 62.194%, Loss = 0.4174355387687683
Epoch: 5674, Batch Gradient Norm: 10.403981332871037
Epoch: 5674, Batch Gradient Norm after: 10.403981332871037
Epoch 5675/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.427424418926239
Epoch: 5675, Batch Gradient Norm: 12.340469474874116
Epoch: 5675, Batch Gradient Norm after: 12.340469474874116
Epoch 5676/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.44088390469551086
Epoch: 5676, Batch Gradient Norm: 11.30895147605158
Epoch: 5676, Batch Gradient Norm after: 11.30895147605158
Epoch 5677/10000, Prediction Accuracy = 62.278000000000006%, Loss = 0.4334326386451721
Epoch: 5677, Batch Gradient Norm: 10.863557691257668
Epoch: 5677, Batch Gradient Norm after: 10.863557691257668
Epoch 5678/10000, Prediction Accuracy = 62.196000000000005%, Loss = 0.4321993410587311
Epoch: 5678, Batch Gradient Norm: 9.74804065596269
Epoch: 5678, Batch Gradient Norm after: 9.74804065596269
Epoch 5679/10000, Prediction Accuracy = 62.156000000000006%, Loss = 0.4258480966091156
Epoch: 5679, Batch Gradient Norm: 8.94182167596297
Epoch: 5679, Batch Gradient Norm after: 8.94182167596297
Epoch 5680/10000, Prediction Accuracy = 62.384%, Loss = 0.4205451846122742
Epoch: 5680, Batch Gradient Norm: 11.767697576106489
Epoch: 5680, Batch Gradient Norm after: 11.767697576106489
Epoch 5681/10000, Prediction Accuracy = 62.396%, Loss = 0.43893574476242064
Epoch: 5681, Batch Gradient Norm: 11.380848911656736
Epoch: 5681, Batch Gradient Norm after: 11.380848911656736
Epoch 5682/10000, Prediction Accuracy = 62.352%, Loss = 0.43685947060585023
Epoch: 5682, Batch Gradient Norm: 9.673915480924236
Epoch: 5682, Batch Gradient Norm after: 9.673915480924236
Epoch 5683/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.42404918670654296
Epoch: 5683, Batch Gradient Norm: 9.289561310120495
Epoch: 5683, Batch Gradient Norm after: 9.289561310120495
Epoch 5684/10000, Prediction Accuracy = 62.36999999999999%, Loss = 0.4203249216079712
Epoch: 5684, Batch Gradient Norm: 9.977988736521937
Epoch: 5684, Batch Gradient Norm after: 9.977988736521937
Epoch 5685/10000, Prediction Accuracy = 62.193999999999996%, Loss = 0.42486181259155276
Epoch: 5685, Batch Gradient Norm: 9.302617151568223
Epoch: 5685, Batch Gradient Norm after: 9.302617151568223
Epoch 5686/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4213309407234192
Epoch: 5686, Batch Gradient Norm: 8.077778778146183
Epoch: 5686, Batch Gradient Norm after: 8.077778778146183
Epoch 5687/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4142004907131195
Epoch: 5687, Batch Gradient Norm: 8.829699158285923
Epoch: 5687, Batch Gradient Norm after: 8.829699158285923
Epoch 5688/10000, Prediction Accuracy = 62.4%, Loss = 0.4192216217517853
Epoch: 5688, Batch Gradient Norm: 9.022012869342491
Epoch: 5688, Batch Gradient Norm after: 9.022012869342491
Epoch 5689/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.41963707804679873
Epoch: 5689, Batch Gradient Norm: 10.968024905464187
Epoch: 5689, Batch Gradient Norm after: 10.968024905464187
Epoch 5690/10000, Prediction Accuracy = 62.34000000000001%, Loss = 0.43315383791923523
Epoch: 5690, Batch Gradient Norm: 12.19098881649538
Epoch: 5690, Batch Gradient Norm after: 12.19098881649538
Epoch 5691/10000, Prediction Accuracy = 62.306%, Loss = 0.4430152177810669
Epoch: 5691, Batch Gradient Norm: 10.303638895587646
Epoch: 5691, Batch Gradient Norm after: 10.303638895587646
Epoch 5692/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.43082594871520996
Epoch: 5692, Batch Gradient Norm: 8.515941916929485
Epoch: 5692, Batch Gradient Norm after: 8.515941916929485
Epoch 5693/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4166994750499725
Epoch: 5693, Batch Gradient Norm: 9.98614362700873
Epoch: 5693, Batch Gradient Norm after: 9.98614362700873
Epoch 5694/10000, Prediction Accuracy = 62.25999999999999%, Loss = 0.4247617840766907
Epoch: 5694, Batch Gradient Norm: 10.396929407246661
Epoch: 5694, Batch Gradient Norm after: 10.396929407246661
Epoch 5695/10000, Prediction Accuracy = 62.117999999999995%, Loss = 0.4270905375480652
Epoch: 5695, Batch Gradient Norm: 9.855339258127113
Epoch: 5695, Batch Gradient Norm after: 9.855339258127113
Epoch 5696/10000, Prediction Accuracy = 62.318%, Loss = 0.42343221306800843
Epoch: 5696, Batch Gradient Norm: 9.741978935125722
Epoch: 5696, Batch Gradient Norm after: 9.741978935125722
Epoch 5697/10000, Prediction Accuracy = 62.278%, Loss = 0.42232308387756345
Epoch: 5697, Batch Gradient Norm: 10.786238824787617
Epoch: 5697, Batch Gradient Norm after: 10.786238824787617
Epoch 5698/10000, Prediction Accuracy = 62.31999999999999%, Loss = 0.4291226387023926
Epoch: 5698, Batch Gradient Norm: 10.786175721729574
Epoch: 5698, Batch Gradient Norm after: 10.786175721729574
Epoch 5699/10000, Prediction Accuracy = 62.202%, Loss = 0.4293026804924011
Epoch: 5699, Batch Gradient Norm: 10.960326532606237
Epoch: 5699, Batch Gradient Norm after: 10.960326532606237
Epoch 5700/10000, Prediction Accuracy = 62.302%, Loss = 0.4328033983707428
Epoch: 5700, Batch Gradient Norm: 10.715565905456144
Epoch: 5700, Batch Gradient Norm after: 10.715565905456144
Epoch 5701/10000, Prediction Accuracy = 62.386%, Loss = 0.43313307166099546
Epoch: 5701, Batch Gradient Norm: 9.483192477748386
Epoch: 5701, Batch Gradient Norm after: 9.483192477748386
Epoch 5702/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.4229531764984131
Epoch: 5702, Batch Gradient Norm: 9.770228109199941
Epoch: 5702, Batch Gradient Norm after: 9.770228109199941
Epoch 5703/10000, Prediction Accuracy = 62.164%, Loss = 0.4242084324359894
Epoch: 5703, Batch Gradient Norm: 9.814418230907933
Epoch: 5703, Batch Gradient Norm after: 9.814418230907933
Epoch 5704/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4242196261882782
Epoch: 5704, Batch Gradient Norm: 10.896511526672862
Epoch: 5704, Batch Gradient Norm after: 10.896511526672862
Epoch 5705/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.43185728788375854
Epoch: 5705, Batch Gradient Norm: 11.594086852902857
Epoch: 5705, Batch Gradient Norm after: 11.594086852902857
Epoch 5706/10000, Prediction Accuracy = 62.44%, Loss = 0.4362505257129669
Epoch: 5706, Batch Gradient Norm: 10.299014474294436
Epoch: 5706, Batch Gradient Norm after: 10.299014474294436
Epoch 5707/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.4258182466030121
Epoch: 5707, Batch Gradient Norm: 9.622718475092357
Epoch: 5707, Batch Gradient Norm after: 9.622718475092357
Epoch 5708/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.4209938943386078
Epoch: 5708, Batch Gradient Norm: 11.929747957773676
Epoch: 5708, Batch Gradient Norm after: 11.929747957773676
Epoch 5709/10000, Prediction Accuracy = 62.176%, Loss = 0.4377714693546295
Epoch: 5709, Batch Gradient Norm: 11.555608187808273
Epoch: 5709, Batch Gradient Norm after: 11.555608187808273
Epoch 5710/10000, Prediction Accuracy = 62.408%, Loss = 0.4384122848510742
Epoch: 5710, Batch Gradient Norm: 7.384885766083183
Epoch: 5710, Batch Gradient Norm after: 7.384885766083183
Epoch 5711/10000, Prediction Accuracy = 62.398%, Loss = 0.4113899230957031
Epoch: 5711, Batch Gradient Norm: 6.570976147209893
Epoch: 5711, Batch Gradient Norm after: 6.570976147209893
Epoch 5712/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.4069261133670807
Epoch: 5712, Batch Gradient Norm: 8.462988069415944
Epoch: 5712, Batch Gradient Norm after: 8.462988069415944
Epoch 5713/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.41670364141464233
Epoch: 5713, Batch Gradient Norm: 9.120580775974679
Epoch: 5713, Batch Gradient Norm after: 9.120580775974679
Epoch 5714/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.42114476561546327
Epoch: 5714, Batch Gradient Norm: 10.542787600332522
Epoch: 5714, Batch Gradient Norm after: 10.542787600332522
Epoch 5715/10000, Prediction Accuracy = 62.29599999999999%, Loss = 0.42988970279693606
Epoch: 5715, Batch Gradient Norm: 10.646844863219767
Epoch: 5715, Batch Gradient Norm after: 10.646844863219767
Epoch 5716/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.4307760953903198
Epoch: 5716, Batch Gradient Norm: 10.68655912211304
Epoch: 5716, Batch Gradient Norm after: 10.68655912211304
Epoch 5717/10000, Prediction Accuracy = 62.302%, Loss = 0.4320263981819153
Epoch: 5717, Batch Gradient Norm: 10.582466958772173
Epoch: 5717, Batch Gradient Norm after: 10.582466958772173
Epoch 5718/10000, Prediction Accuracy = 62.412%, Loss = 0.4284895837306976
Epoch: 5718, Batch Gradient Norm: 12.46801428045275
Epoch: 5718, Batch Gradient Norm after: 12.46801428045275
Epoch 5719/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.4418376684188843
Epoch: 5719, Batch Gradient Norm: 9.784630499448095
Epoch: 5719, Batch Gradient Norm after: 9.784630499448095
Epoch 5720/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4236915707588196
Epoch: 5720, Batch Gradient Norm: 7.850205605102139
Epoch: 5720, Batch Gradient Norm after: 7.850205605102139
Epoch 5721/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.41234986782073973
Epoch: 5721, Batch Gradient Norm: 8.372414480685034
Epoch: 5721, Batch Gradient Norm after: 8.372414480685034
Epoch 5722/10000, Prediction Accuracy = 62.3%, Loss = 0.41531450152397154
Epoch: 5722, Batch Gradient Norm: 10.348666750291745
Epoch: 5722, Batch Gradient Norm after: 10.348666750291745
Epoch 5723/10000, Prediction Accuracy = 62.362%, Loss = 0.42614837884902956
Epoch: 5723, Batch Gradient Norm: 11.541368023841084
Epoch: 5723, Batch Gradient Norm after: 11.541368023841084
Epoch 5724/10000, Prediction Accuracy = 62.288%, Loss = 0.4343040704727173
Epoch: 5724, Batch Gradient Norm: 10.097072808412005
Epoch: 5724, Batch Gradient Norm after: 10.097072808412005
Epoch 5725/10000, Prediction Accuracy = 62.278%, Loss = 0.425745952129364
Epoch: 5725, Batch Gradient Norm: 9.024367352385365
Epoch: 5725, Batch Gradient Norm after: 9.024367352385365
Epoch 5726/10000, Prediction Accuracy = 62.238%, Loss = 0.4200470507144928
Epoch: 5726, Batch Gradient Norm: 8.454455424045982
Epoch: 5726, Batch Gradient Norm after: 8.454455424045982
Epoch 5727/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4174279928207397
Epoch: 5727, Batch Gradient Norm: 7.858655415898817
Epoch: 5727, Batch Gradient Norm after: 7.858655415898817
Epoch 5728/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.41302253007888795
Epoch: 5728, Batch Gradient Norm: 8.810634911765506
Epoch: 5728, Batch Gradient Norm after: 8.810634911765506
Epoch 5729/10000, Prediction Accuracy = 62.402%, Loss = 0.41812766194343565
Epoch: 5729, Batch Gradient Norm: 10.666542084950198
Epoch: 5729, Batch Gradient Norm after: 10.666542084950198
Epoch 5730/10000, Prediction Accuracy = 62.352%, Loss = 0.4293844699859619
Epoch: 5730, Batch Gradient Norm: 13.397595525364112
Epoch: 5730, Batch Gradient Norm after: 13.397595525364112
Epoch 5731/10000, Prediction Accuracy = 62.374%, Loss = 0.4492455780506134
Epoch: 5731, Batch Gradient Norm: 12.581023505921824
Epoch: 5731, Batch Gradient Norm after: 12.581023505921824
Epoch 5732/10000, Prediction Accuracy = 62.23%, Loss = 0.4430780291557312
Epoch: 5732, Batch Gradient Norm: 10.367615794976276
Epoch: 5732, Batch Gradient Norm after: 10.367615794976276
Epoch 5733/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.4270041465759277
Epoch: 5733, Batch Gradient Norm: 10.924584433625181
Epoch: 5733, Batch Gradient Norm after: 10.924584433625181
Epoch 5734/10000, Prediction Accuracy = 62.15%, Loss = 0.43143497705459594
Epoch: 5734, Batch Gradient Norm: 11.614071781532017
Epoch: 5734, Batch Gradient Norm after: 11.614071781532017
Epoch 5735/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4373436808586121
Epoch: 5735, Batch Gradient Norm: 9.728619415821392
Epoch: 5735, Batch Gradient Norm after: 9.728619415821392
Epoch 5736/10000, Prediction Accuracy = 62.288%, Loss = 0.423611181974411
Epoch: 5736, Batch Gradient Norm: 9.210343025137012
Epoch: 5736, Batch Gradient Norm after: 9.210343025137012
Epoch 5737/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.41893036365509034
Epoch: 5737, Batch Gradient Norm: 10.828142660249082
Epoch: 5737, Batch Gradient Norm after: 10.828142660249082
Epoch 5738/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.4298380196094513
Epoch: 5738, Batch Gradient Norm: 10.378210194545174
Epoch: 5738, Batch Gradient Norm after: 10.378210194545174
Epoch 5739/10000, Prediction Accuracy = 62.20399999999999%, Loss = 0.42779070138931274
Epoch: 5739, Batch Gradient Norm: 7.772303848462011
Epoch: 5739, Batch Gradient Norm after: 7.772303848462011
Epoch 5740/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.41253607869148257
Epoch: 5740, Batch Gradient Norm: 7.049934579621818
Epoch: 5740, Batch Gradient Norm after: 7.049934579621818
Epoch 5741/10000, Prediction Accuracy = 62.312%, Loss = 0.40799123644828794
Epoch: 5741, Batch Gradient Norm: 9.34785463206421
Epoch: 5741, Batch Gradient Norm after: 9.34785463206421
Epoch 5742/10000, Prediction Accuracy = 62.452%, Loss = 0.4204386055469513
Epoch: 5742, Batch Gradient Norm: 11.150361703768104
Epoch: 5742, Batch Gradient Norm after: 11.150361703768104
Epoch 5743/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.4342203915119171
Epoch: 5743, Batch Gradient Norm: 10.101867643489008
Epoch: 5743, Batch Gradient Norm after: 10.101867643489008
Epoch 5744/10000, Prediction Accuracy = 62.306%, Loss = 0.427215576171875
Epoch: 5744, Batch Gradient Norm: 9.23235265062893
Epoch: 5744, Batch Gradient Norm after: 9.23235265062893
Epoch 5745/10000, Prediction Accuracy = 62.386%, Loss = 0.42068904638290405
Epoch: 5745, Batch Gradient Norm: 8.462809185719044
Epoch: 5745, Batch Gradient Norm after: 8.462809185719044
Epoch 5746/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41363494396209716
Epoch: 5746, Batch Gradient Norm: 10.665410701106905
Epoch: 5746, Batch Gradient Norm after: 10.665410701106905
Epoch 5747/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.42818790674209595
Epoch: 5747, Batch Gradient Norm: 10.038156994046036
Epoch: 5747, Batch Gradient Norm after: 10.038156994046036
Epoch 5748/10000, Prediction Accuracy = 62.33599999999999%, Loss = 0.42499061226844786
Epoch: 5748, Batch Gradient Norm: 10.268395515907077
Epoch: 5748, Batch Gradient Norm after: 10.268395515907077
Epoch 5749/10000, Prediction Accuracy = 62.260000000000005%, Loss = 0.4250250518321991
Epoch: 5749, Batch Gradient Norm: 12.484534832370038
Epoch: 5749, Batch Gradient Norm after: 12.484534832370038
Epoch 5750/10000, Prediction Accuracy = 62.303999999999995%, Loss = 0.44084292054176333
Epoch: 5750, Batch Gradient Norm: 12.012399856434675
Epoch: 5750, Batch Gradient Norm after: 12.012399856434675
Epoch 5751/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4388533294200897
Epoch: 5751, Batch Gradient Norm: 10.203325181249154
Epoch: 5751, Batch Gradient Norm after: 10.203325181249154
Epoch 5752/10000, Prediction Accuracy = 62.258%, Loss = 0.42585939168930054
Epoch: 5752, Batch Gradient Norm: 10.383485618721274
Epoch: 5752, Batch Gradient Norm after: 10.383485618721274
Epoch 5753/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.42622451186180116
Epoch: 5753, Batch Gradient Norm: 10.586641955697102
Epoch: 5753, Batch Gradient Norm after: 10.586641955697102
Epoch 5754/10000, Prediction Accuracy = 62.31200000000001%, Loss = 0.4278095602989197
Epoch: 5754, Batch Gradient Norm: 8.968642618333853
Epoch: 5754, Batch Gradient Norm after: 8.968642618333853
Epoch 5755/10000, Prediction Accuracy = 62.338%, Loss = 0.41789361238479616
Epoch: 5755, Batch Gradient Norm: 8.273883141722997
Epoch: 5755, Batch Gradient Norm after: 8.273883141722997
Epoch 5756/10000, Prediction Accuracy = 62.28399999999999%, Loss = 0.41346471905708315
Epoch: 5756, Batch Gradient Norm: 8.623387234596136
Epoch: 5756, Batch Gradient Norm after: 8.623387234596136
Epoch 5757/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.4155979037284851
Epoch: 5757, Batch Gradient Norm: 9.581586373052161
Epoch: 5757, Batch Gradient Norm after: 9.581586373052161
Epoch 5758/10000, Prediction Accuracy = 62.314%, Loss = 0.4213826239109039
Epoch: 5758, Batch Gradient Norm: 10.712953492327747
Epoch: 5758, Batch Gradient Norm after: 10.712953492327747
Epoch 5759/10000, Prediction Accuracy = 62.326%, Loss = 0.42913751006126405
Epoch: 5759, Batch Gradient Norm: 11.736206345118134
Epoch: 5759, Batch Gradient Norm after: 11.736206345118134
Epoch 5760/10000, Prediction Accuracy = 62.136%, Loss = 0.43742917776107787
Epoch: 5760, Batch Gradient Norm: 10.96498721245731
Epoch: 5760, Batch Gradient Norm after: 10.96498721245731
Epoch 5761/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4319200277328491
Epoch: 5761, Batch Gradient Norm: 8.640300606826466
Epoch: 5761, Batch Gradient Norm after: 8.640300606826466
Epoch 5762/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.4156708776950836
Epoch: 5762, Batch Gradient Norm: 9.631121718976255
Epoch: 5762, Batch Gradient Norm after: 9.631121718976255
Epoch 5763/10000, Prediction Accuracy = 62.324%, Loss = 0.4214751303195953
Epoch: 5763, Batch Gradient Norm: 11.022119078622854
Epoch: 5763, Batch Gradient Norm after: 11.022119078622854
Epoch 5764/10000, Prediction Accuracy = 62.272000000000006%, Loss = 0.4322585344314575
Epoch: 5764, Batch Gradient Norm: 9.168233668530096
Epoch: 5764, Batch Gradient Norm after: 9.168233668530096
Epoch 5765/10000, Prediction Accuracy = 62.322%, Loss = 0.4197352588176727
Epoch: 5765, Batch Gradient Norm: 8.889684126718265
Epoch: 5765, Batch Gradient Norm after: 8.889684126718265
Epoch 5766/10000, Prediction Accuracy = 62.39399999999999%, Loss = 0.41687437891960144
Epoch: 5766, Batch Gradient Norm: 10.667962038649106
Epoch: 5766, Batch Gradient Norm after: 10.667962038649106
Epoch 5767/10000, Prediction Accuracy = 62.364%, Loss = 0.42786208987236024
Epoch: 5767, Batch Gradient Norm: 12.313724785447327
Epoch: 5767, Batch Gradient Norm after: 12.313724785447327
Epoch 5768/10000, Prediction Accuracy = 62.25600000000001%, Loss = 0.4436272919178009
Epoch: 5768, Batch Gradient Norm: 10.127827101511341
Epoch: 5768, Batch Gradient Norm after: 10.127827101511341
Epoch 5769/10000, Prediction Accuracy = 62.306%, Loss = 0.4260584831237793
Epoch: 5769, Batch Gradient Norm: 9.01736608598254
Epoch: 5769, Batch Gradient Norm after: 9.01736608598254
Epoch 5770/10000, Prediction Accuracy = 62.208000000000006%, Loss = 0.4186787843704224
Epoch: 5770, Batch Gradient Norm: 9.41989761074542
Epoch: 5770, Batch Gradient Norm after: 9.41989761074542
Epoch 5771/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.4204969346523285
Epoch: 5771, Batch Gradient Norm: 9.707442499313213
Epoch: 5771, Batch Gradient Norm after: 9.707442499313213
Epoch 5772/10000, Prediction Accuracy = 62.354000000000006%, Loss = 0.4214466631412506
Epoch: 5772, Batch Gradient Norm: 9.659697726833743
Epoch: 5772, Batch Gradient Norm after: 9.659697726833743
Epoch 5773/10000, Prediction Accuracy = 62.27%, Loss = 0.4201217532157898
Epoch: 5773, Batch Gradient Norm: 10.109621945869058
Epoch: 5773, Batch Gradient Norm after: 10.109621945869058
Epoch 5774/10000, Prediction Accuracy = 62.424%, Loss = 0.42413694858551027
Epoch: 5774, Batch Gradient Norm: 8.880320834633078
Epoch: 5774, Batch Gradient Norm after: 8.880320834633078
Epoch 5775/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.4175636887550354
Epoch: 5775, Batch Gradient Norm: 9.408022889109905
Epoch: 5775, Batch Gradient Norm after: 9.408022889109905
Epoch 5776/10000, Prediction Accuracy = 62.398%, Loss = 0.4208838641643524
Epoch: 5776, Batch Gradient Norm: 12.674499330343785
Epoch: 5776, Batch Gradient Norm after: 12.674499330343785
Epoch 5777/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.4427726626396179
Epoch: 5777, Batch Gradient Norm: 12.904899273461284
Epoch: 5777, Batch Gradient Norm after: 12.904899273461284
Epoch 5778/10000, Prediction Accuracy = 62.25599999999999%, Loss = 0.44553133845329285
Epoch: 5778, Batch Gradient Norm: 9.820833906524461
Epoch: 5778, Batch Gradient Norm after: 9.820833906524461
Epoch 5779/10000, Prediction Accuracy = 62.314%, Loss = 0.4249971091747284
Epoch: 5779, Batch Gradient Norm: 7.976274752115222
Epoch: 5779, Batch Gradient Norm after: 7.976274752115222
Epoch 5780/10000, Prediction Accuracy = 62.306%, Loss = 0.4136239945888519
Epoch: 5780, Batch Gradient Norm: 7.664790151656676
Epoch: 5780, Batch Gradient Norm after: 7.664790151656676
Epoch 5781/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.41111752986907957
Epoch: 5781, Batch Gradient Norm: 9.320326644954841
Epoch: 5781, Batch Gradient Norm after: 9.320326644954841
Epoch 5782/10000, Prediction Accuracy = 62.370000000000005%, Loss = 0.4198740780353546
Epoch: 5782, Batch Gradient Norm: 10.317911964452962
Epoch: 5782, Batch Gradient Norm after: 10.317911964452962
Epoch 5783/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4263519704341888
Epoch: 5783, Batch Gradient Norm: 10.719945743154252
Epoch: 5783, Batch Gradient Norm after: 10.719945743154252
Epoch 5784/10000, Prediction Accuracy = 62.214%, Loss = 0.42851165533065794
Epoch: 5784, Batch Gradient Norm: 11.488833864938345
Epoch: 5784, Batch Gradient Norm after: 11.488833864938345
Epoch 5785/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.43221297264099123
Epoch: 5785, Batch Gradient Norm: 11.690375354352348
Epoch: 5785, Batch Gradient Norm after: 11.690375354352348
Epoch 5786/10000, Prediction Accuracy = 62.34599999999999%, Loss = 0.4336673319339752
Epoch: 5786, Batch Gradient Norm: 10.023252001199857
Epoch: 5786, Batch Gradient Norm after: 10.023252001199857
Epoch 5787/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.4223280608654022
Epoch: 5787, Batch Gradient Norm: 10.076011860593452
Epoch: 5787, Batch Gradient Norm after: 10.076011860593452
Epoch 5788/10000, Prediction Accuracy = 62.322%, Loss = 0.4238064825534821
Epoch: 5788, Batch Gradient Norm: 9.93771867890247
Epoch: 5788, Batch Gradient Norm after: 9.93771867890247
Epoch 5789/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.42315065264701845
Epoch: 5789, Batch Gradient Norm: 10.30753249786689
Epoch: 5789, Batch Gradient Norm after: 10.30753249786689
Epoch 5790/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4260189414024353
Epoch: 5790, Batch Gradient Norm: 9.640563531371253
Epoch: 5790, Batch Gradient Norm after: 9.640563531371253
Epoch 5791/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.4214845776557922
Epoch: 5791, Batch Gradient Norm: 9.248185676179526
Epoch: 5791, Batch Gradient Norm after: 9.248185676179526
Epoch 5792/10000, Prediction Accuracy = 62.330000000000005%, Loss = 0.4193520188331604
Epoch: 5792, Batch Gradient Norm: 9.283685411180093
Epoch: 5792, Batch Gradient Norm after: 9.283685411180093
Epoch 5793/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.41942251920700074
Epoch: 5793, Batch Gradient Norm: 10.294692615114958
Epoch: 5793, Batch Gradient Norm after: 10.294692615114958
Epoch 5794/10000, Prediction Accuracy = 62.334%, Loss = 0.4267013370990753
Epoch: 5794, Batch Gradient Norm: 9.484479831545068
Epoch: 5794, Batch Gradient Norm after: 9.484479831545068
Epoch 5795/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4207812488079071
Epoch: 5795, Batch Gradient Norm: 10.228660998792652
Epoch: 5795, Batch Gradient Norm after: 10.228660998792652
Epoch 5796/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.4244269967079163
Epoch: 5796, Batch Gradient Norm: 10.804523013860807
Epoch: 5796, Batch Gradient Norm after: 10.804523013860807
Epoch 5797/10000, Prediction Accuracy = 62.438%, Loss = 0.4274897396564484
Epoch: 5797, Batch Gradient Norm: 10.637404779968124
Epoch: 5797, Batch Gradient Norm after: 10.637404779968124
Epoch 5798/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4261451482772827
Epoch: 5798, Batch Gradient Norm: 9.497601681273164
Epoch: 5798, Batch Gradient Norm after: 9.497601681273164
Epoch 5799/10000, Prediction Accuracy = 62.28000000000001%, Loss = 0.41852818727493285
Epoch: 5799, Batch Gradient Norm: 9.239914780966943
Epoch: 5799, Batch Gradient Norm after: 9.239914780966943
Epoch 5800/10000, Prediction Accuracy = 62.348%, Loss = 0.4177100360393524
Epoch: 5800, Batch Gradient Norm: 10.371490290697144
Epoch: 5800, Batch Gradient Norm after: 10.371490290697144
Epoch 5801/10000, Prediction Accuracy = 62.144000000000005%, Loss = 0.42660937309265134
Epoch: 5801, Batch Gradient Norm: 10.393357267359162
Epoch: 5801, Batch Gradient Norm after: 10.393357267359162
Epoch 5802/10000, Prediction Accuracy = 62.184000000000005%, Loss = 0.4282027244567871
Epoch: 5802, Batch Gradient Norm: 10.318776516726015
Epoch: 5802, Batch Gradient Norm after: 10.318776516726015
Epoch 5803/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4248164415359497
Epoch: 5803, Batch Gradient Norm: 10.290365899376203
Epoch: 5803, Batch Gradient Norm after: 10.290365899376203
Epoch 5804/10000, Prediction Accuracy = 62.25%, Loss = 0.42417178153991697
Epoch: 5804, Batch Gradient Norm: 9.39960873384569
Epoch: 5804, Batch Gradient Norm after: 9.39960873384569
Epoch 5805/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.4203832924365997
Epoch: 5805, Batch Gradient Norm: 8.424925273680024
Epoch: 5805, Batch Gradient Norm after: 8.424925273680024
Epoch 5806/10000, Prediction Accuracy = 62.217999999999996%, Loss = 0.4144317388534546
Epoch: 5806, Batch Gradient Norm: 10.956846254799856
Epoch: 5806, Batch Gradient Norm after: 10.956846254799856
Epoch 5807/10000, Prediction Accuracy = 62.152%, Loss = 0.4303131639957428
Epoch: 5807, Batch Gradient Norm: 13.78720661143227
Epoch: 5807, Batch Gradient Norm after: 13.78720661143227
Epoch 5808/10000, Prediction Accuracy = 62.288%, Loss = 0.4529659807682037
Epoch: 5808, Batch Gradient Norm: 10.191925819462355
Epoch: 5808, Batch Gradient Norm after: 10.191925819462355
Epoch 5809/10000, Prediction Accuracy = 62.31%, Loss = 0.4249834716320038
Epoch: 5809, Batch Gradient Norm: 7.459900995934741
Epoch: 5809, Batch Gradient Norm after: 7.459900995934741
Epoch 5810/10000, Prediction Accuracy = 62.352%, Loss = 0.4081857919692993
Epoch: 5810, Batch Gradient Norm: 8.371165148778754
Epoch: 5810, Batch Gradient Norm after: 8.371165148778754
Epoch 5811/10000, Prediction Accuracy = 62.33%, Loss = 0.41274940967559814
Epoch: 5811, Batch Gradient Norm: 11.018585338092157
Epoch: 5811, Batch Gradient Norm after: 11.018585338092157
Epoch 5812/10000, Prediction Accuracy = 62.480000000000004%, Loss = 0.43042322993278503
Epoch: 5812, Batch Gradient Norm: 10.799650755804398
Epoch: 5812, Batch Gradient Norm after: 10.799650755804398
Epoch 5813/10000, Prediction Accuracy = 62.278%, Loss = 0.42941439151763916
Epoch: 5813, Batch Gradient Norm: 9.167149904642548
Epoch: 5813, Batch Gradient Norm after: 9.167149904642548
Epoch 5814/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4176963806152344
Epoch: 5814, Batch Gradient Norm: 9.650733290442117
Epoch: 5814, Batch Gradient Norm after: 9.650733290442117
Epoch 5815/10000, Prediction Accuracy = 62.376%, Loss = 0.420731121301651
Epoch: 5815, Batch Gradient Norm: 10.973766808494613
Epoch: 5815, Batch Gradient Norm after: 10.973766808494613
Epoch 5816/10000, Prediction Accuracy = 62.27%, Loss = 0.4300557017326355
Epoch: 5816, Batch Gradient Norm: 9.887983867817358
Epoch: 5816, Batch Gradient Norm after: 9.887983867817358
Epoch 5817/10000, Prediction Accuracy = 62.4%, Loss = 0.42189755439758303
Epoch: 5817, Batch Gradient Norm: 9.389009695935659
Epoch: 5817, Batch Gradient Norm after: 9.389009695935659
Epoch 5818/10000, Prediction Accuracy = 62.336%, Loss = 0.41890743374824524
Epoch: 5818, Batch Gradient Norm: 9.573149410070771
Epoch: 5818, Batch Gradient Norm after: 9.573149410070771
Epoch 5819/10000, Prediction Accuracy = 62.374%, Loss = 0.42050492763519287
Epoch: 5819, Batch Gradient Norm: 10.89631108374692
Epoch: 5819, Batch Gradient Norm after: 10.89631108374692
Epoch 5820/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4294625759124756
Epoch: 5820, Batch Gradient Norm: 11.874469909820338
Epoch: 5820, Batch Gradient Norm after: 11.874469909820338
Epoch 5821/10000, Prediction Accuracy = 62.324%, Loss = 0.43638230562210084
Epoch: 5821, Batch Gradient Norm: 10.442025152329695
Epoch: 5821, Batch Gradient Norm after: 10.442025152329695
Epoch 5822/10000, Prediction Accuracy = 62.336%, Loss = 0.42576483488082884
Epoch: 5822, Batch Gradient Norm: 9.152760597636977
Epoch: 5822, Batch Gradient Norm after: 9.152760597636977
Epoch 5823/10000, Prediction Accuracy = 62.267999999999994%, Loss = 0.41759220957756044
Epoch: 5823, Batch Gradient Norm: 9.119598050054767
Epoch: 5823, Batch Gradient Norm after: 9.119598050054767
Epoch 5824/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.41794792413711546
Epoch: 5824, Batch Gradient Norm: 9.460877285899702
Epoch: 5824, Batch Gradient Norm after: 9.460877285899702
Epoch 5825/10000, Prediction Accuracy = 62.266%, Loss = 0.419609534740448
Epoch: 5825, Batch Gradient Norm: 9.707184181184198
Epoch: 5825, Batch Gradient Norm after: 9.707184181184198
Epoch 5826/10000, Prediction Accuracy = 62.33%, Loss = 0.4205471038818359
Epoch: 5826, Batch Gradient Norm: 10.55724664506098
Epoch: 5826, Batch Gradient Norm after: 10.55724664506098
Epoch 5827/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.42477325797080995
Epoch: 5827, Batch Gradient Norm: 11.19859132865149
Epoch: 5827, Batch Gradient Norm after: 11.19859132865149
Epoch 5828/10000, Prediction Accuracy = 62.25%, Loss = 0.4296573519706726
Epoch: 5828, Batch Gradient Norm: 10.690635062329205
Epoch: 5828, Batch Gradient Norm after: 10.690635062329205
Epoch 5829/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.4279300570487976
Epoch: 5829, Batch Gradient Norm: 9.248017153375104
Epoch: 5829, Batch Gradient Norm after: 9.248017153375104
Epoch 5830/10000, Prediction Accuracy = 62.352%, Loss = 0.41842866539955137
Epoch: 5830, Batch Gradient Norm: 10.33948147164752
Epoch: 5830, Batch Gradient Norm after: 10.33948147164752
Epoch 5831/10000, Prediction Accuracy = 62.472%, Loss = 0.4254967749118805
Epoch: 5831, Batch Gradient Norm: 10.883267111639384
Epoch: 5831, Batch Gradient Norm after: 10.883267111639384
Epoch 5832/10000, Prediction Accuracy = 62.302%, Loss = 0.4310278594493866
Epoch: 5832, Batch Gradient Norm: 9.30910785232089
Epoch: 5832, Batch Gradient Norm after: 9.30910785232089
Epoch 5833/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4187577784061432
Epoch: 5833, Batch Gradient Norm: 9.074456255313779
Epoch: 5833, Batch Gradient Norm after: 9.074456255313779
Epoch 5834/10000, Prediction Accuracy = 62.352%, Loss = 0.41637402176856997
Epoch: 5834, Batch Gradient Norm: 9.543056121285574
Epoch: 5834, Batch Gradient Norm after: 9.543056121285574
Epoch 5835/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.4186019778251648
Epoch: 5835, Batch Gradient Norm: 10.537291170168036
Epoch: 5835, Batch Gradient Norm after: 10.537291170168036
Epoch 5836/10000, Prediction Accuracy = 62.348%, Loss = 0.4245430827140808
Epoch: 5836, Batch Gradient Norm: 11.07657111128942
Epoch: 5836, Batch Gradient Norm after: 11.07657111128942
Epoch 5837/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4285259783267975
Epoch: 5837, Batch Gradient Norm: 10.263601453580163
Epoch: 5837, Batch Gradient Norm after: 10.263601453580163
Epoch 5838/10000, Prediction Accuracy = 62.326%, Loss = 0.42313092947006226
Epoch: 5838, Batch Gradient Norm: 9.280874856706344
Epoch: 5838, Batch Gradient Norm after: 9.280874856706344
Epoch 5839/10000, Prediction Accuracy = 62.15%, Loss = 0.4170822262763977
Epoch: 5839, Batch Gradient Norm: 10.425969955708497
Epoch: 5839, Batch Gradient Norm after: 10.425969955708497
Epoch 5840/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4245501935482025
Epoch: 5840, Batch Gradient Norm: 11.379062043993978
Epoch: 5840, Batch Gradient Norm after: 11.379062043993978
Epoch 5841/10000, Prediction Accuracy = 62.164%, Loss = 0.4327967047691345
Epoch: 5841, Batch Gradient Norm: 9.538855134237208
Epoch: 5841, Batch Gradient Norm after: 9.538855134237208
Epoch 5842/10000, Prediction Accuracy = 62.396%, Loss = 0.4204625725746155
Epoch: 5842, Batch Gradient Norm: 8.932160291778704
Epoch: 5842, Batch Gradient Norm after: 8.932160291778704
Epoch 5843/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4167976379394531
Epoch: 5843, Batch Gradient Norm: 10.851901911037583
Epoch: 5843, Batch Gradient Norm after: 10.851901911037583
Epoch 5844/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.42926586866378785
Epoch: 5844, Batch Gradient Norm: 11.313757149988621
Epoch: 5844, Batch Gradient Norm after: 11.313757149988621
Epoch 5845/10000, Prediction Accuracy = 62.278000000000006%, Loss = 0.4319776475429535
Epoch: 5845, Batch Gradient Norm: 9.47240862697615
Epoch: 5845, Batch Gradient Norm after: 9.47240862697615
Epoch 5846/10000, Prediction Accuracy = 62.29799999999999%, Loss = 0.41875632405281066
Epoch: 5846, Batch Gradient Norm: 7.8168646795326095
Epoch: 5846, Batch Gradient Norm after: 7.8168646795326095
Epoch 5847/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4087637305259705
Epoch: 5847, Batch Gradient Norm: 9.726120245511897
Epoch: 5847, Batch Gradient Norm after: 9.726120245511897
Epoch 5848/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.420427018404007
Epoch: 5848, Batch Gradient Norm: 10.10808970433218
Epoch: 5848, Batch Gradient Norm after: 10.10808970433218
Epoch 5849/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.42585437893867495
Epoch: 5849, Batch Gradient Norm: 9.343868515880065
Epoch: 5849, Batch Gradient Norm after: 9.343868515880065
Epoch 5850/10000, Prediction Accuracy = 62.334%, Loss = 0.4184264659881592
Epoch: 5850, Batch Gradient Norm: 11.33851293419871
Epoch: 5850, Batch Gradient Norm after: 11.33851293419871
Epoch 5851/10000, Prediction Accuracy = 62.314%, Loss = 0.43154566287994384
Epoch: 5851, Batch Gradient Norm: 12.443460235116795
Epoch: 5851, Batch Gradient Norm after: 12.443460235116795
Epoch 5852/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.44182599782943727
Epoch: 5852, Batch Gradient Norm: 11.062595161786462
Epoch: 5852, Batch Gradient Norm after: 11.062595161786462
Epoch 5853/10000, Prediction Accuracy = 62.172000000000004%, Loss = 0.43006578683853147
Epoch: 5853, Batch Gradient Norm: 9.745732399722893
Epoch: 5853, Batch Gradient Norm after: 9.745732399722893
Epoch 5854/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.41955910325050355
Epoch: 5854, Batch Gradient Norm: 10.494075384481146
Epoch: 5854, Batch Gradient Norm after: 10.494075384481146
Epoch 5855/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4258003652095795
Epoch: 5855, Batch Gradient Norm: 9.162332986477644
Epoch: 5855, Batch Gradient Norm after: 9.162332986477644
Epoch 5856/10000, Prediction Accuracy = 62.4%, Loss = 0.41863524317741396
Epoch: 5856, Batch Gradient Norm: 6.819156507679127
Epoch: 5856, Batch Gradient Norm after: 6.819156507679127
Epoch 5857/10000, Prediction Accuracy = 62.312%, Loss = 0.40532379150390624
Epoch: 5857, Batch Gradient Norm: 7.023770993760135
Epoch: 5857, Batch Gradient Norm after: 7.023770993760135
Epoch 5858/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4052173435688019
Epoch: 5858, Batch Gradient Norm: 9.036466870522997
Epoch: 5858, Batch Gradient Norm after: 9.036466870522997
Epoch 5859/10000, Prediction Accuracy = 62.326%, Loss = 0.4171039521694183
Epoch: 5859, Batch Gradient Norm: 10.092016037163809
Epoch: 5859, Batch Gradient Norm after: 10.092016037163809
Epoch 5860/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4246667265892029
Epoch: 5860, Batch Gradient Norm: 11.097343737305708
Epoch: 5860, Batch Gradient Norm after: 11.097343737305708
Epoch 5861/10000, Prediction Accuracy = 62.302%, Loss = 0.42956589460372924
Epoch: 5861, Batch Gradient Norm: 12.82145565707099
Epoch: 5861, Batch Gradient Norm after: 12.82145565707099
Epoch 5862/10000, Prediction Accuracy = 62.261999999999986%, Loss = 0.4417909324169159
Epoch: 5862, Batch Gradient Norm: 10.70240298334702
Epoch: 5862, Batch Gradient Norm after: 10.70240298334702
Epoch 5863/10000, Prediction Accuracy = 62.407999999999994%, Loss = 0.4272087275981903
Epoch: 5863, Batch Gradient Norm: 9.103953354243648
Epoch: 5863, Batch Gradient Norm after: 9.103953354243648
Epoch 5864/10000, Prediction Accuracy = 62.334%, Loss = 0.4151274263858795
Epoch: 5864, Batch Gradient Norm: 10.565449354714678
Epoch: 5864, Batch Gradient Norm after: 10.565449354714678
Epoch 5865/10000, Prediction Accuracy = 62.39%, Loss = 0.4245113134384155
Epoch: 5865, Batch Gradient Norm: 11.975374005075652
Epoch: 5865, Batch Gradient Norm after: 11.975374005075652
Epoch 5866/10000, Prediction Accuracy = 62.274%, Loss = 0.43567933440208434
Epoch: 5866, Batch Gradient Norm: 11.142175744341683
Epoch: 5866, Batch Gradient Norm after: 11.142175744341683
Epoch 5867/10000, Prediction Accuracy = 62.354%, Loss = 0.42930634021759034
Epoch: 5867, Batch Gradient Norm: 9.346553937182673
Epoch: 5867, Batch Gradient Norm after: 9.346553937182673
Epoch 5868/10000, Prediction Accuracy = 62.30800000000001%, Loss = 0.4171448290348053
Epoch: 5868, Batch Gradient Norm: 8.39914296561244
Epoch: 5868, Batch Gradient Norm after: 8.39914296561244
Epoch 5869/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4112893998622894
Epoch: 5869, Batch Gradient Norm: 8.849032203739252
Epoch: 5869, Batch Gradient Norm after: 8.849032203739252
Epoch 5870/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.41445730328559877
Epoch: 5870, Batch Gradient Norm: 10.101361635235454
Epoch: 5870, Batch Gradient Norm after: 10.101361635235454
Epoch 5871/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.4240932881832123
Epoch: 5871, Batch Gradient Norm: 9.965917227198474
Epoch: 5871, Batch Gradient Norm after: 9.965917227198474
Epoch 5872/10000, Prediction Accuracy = 62.312%, Loss = 0.42118426561355593
Epoch: 5872, Batch Gradient Norm: 12.46706310368431
Epoch: 5872, Batch Gradient Norm after: 12.46706310368431
Epoch 5873/10000, Prediction Accuracy = 62.438%, Loss = 0.4389124572277069
Epoch: 5873, Batch Gradient Norm: 10.52386709584346
Epoch: 5873, Batch Gradient Norm after: 10.52386709584346
Epoch 5874/10000, Prediction Accuracy = 62.376%, Loss = 0.4268385350704193
Epoch: 5874, Batch Gradient Norm: 7.9824048440143995
Epoch: 5874, Batch Gradient Norm after: 7.9824048440143995
Epoch 5875/10000, Prediction Accuracy = 62.33%, Loss = 0.4104997456073761
Epoch: 5875, Batch Gradient Norm: 8.283739009822437
Epoch: 5875, Batch Gradient Norm after: 8.283739009822437
Epoch 5876/10000, Prediction Accuracy = 62.38000000000001%, Loss = 0.4115546226501465
Epoch: 5876, Batch Gradient Norm: 11.079369927729982
Epoch: 5876, Batch Gradient Norm after: 11.079369927729982
Epoch 5877/10000, Prediction Accuracy = 62.238%, Loss = 0.42894012331962583
Epoch: 5877, Batch Gradient Norm: 11.408454695581453
Epoch: 5877, Batch Gradient Norm after: 11.408454695581453
Epoch 5878/10000, Prediction Accuracy = 62.39%, Loss = 0.4303155541419983
Epoch: 5878, Batch Gradient Norm: 10.047808328243585
Epoch: 5878, Batch Gradient Norm after: 10.047808328243585
Epoch 5879/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.42045578360557556
Epoch: 5879, Batch Gradient Norm: 9.539674369628344
Epoch: 5879, Batch Gradient Norm after: 9.539674369628344
Epoch 5880/10000, Prediction Accuracy = 62.327999999999996%, Loss = 0.4183457612991333
Epoch: 5880, Batch Gradient Norm: 9.886072550891654
Epoch: 5880, Batch Gradient Norm after: 9.886072550891654
Epoch 5881/10000, Prediction Accuracy = 62.236000000000004%, Loss = 0.42270866632461546
Epoch: 5881, Batch Gradient Norm: 8.852704976449461
Epoch: 5881, Batch Gradient Norm after: 8.852704976449461
Epoch 5882/10000, Prediction Accuracy = 62.27%, Loss = 0.4154551923274994
Epoch: 5882, Batch Gradient Norm: 9.933201190668756
Epoch: 5882, Batch Gradient Norm after: 9.933201190668756
Epoch 5883/10000, Prediction Accuracy = 62.448%, Loss = 0.42135632038116455
Epoch: 5883, Batch Gradient Norm: 12.396593286771187
Epoch: 5883, Batch Gradient Norm after: 12.396593286771187
Epoch 5884/10000, Prediction Accuracy = 62.376%, Loss = 0.43997657895088194
Epoch: 5884, Batch Gradient Norm: 10.810973724328397
Epoch: 5884, Batch Gradient Norm after: 10.810973724328397
Epoch 5885/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.42834014296531675
Epoch: 5885, Batch Gradient Norm: 7.849930281569915
Epoch: 5885, Batch Gradient Norm after: 7.849930281569915
Epoch 5886/10000, Prediction Accuracy = 62.412%, Loss = 0.4086995303630829
Epoch: 5886, Batch Gradient Norm: 8.581038354139853
Epoch: 5886, Batch Gradient Norm after: 8.581038354139853
Epoch 5887/10000, Prediction Accuracy = 62.314%, Loss = 0.4123174250125885
Epoch: 5887, Batch Gradient Norm: 10.61891741910872
Epoch: 5887, Batch Gradient Norm after: 10.61891741910872
Epoch 5888/10000, Prediction Accuracy = 62.209999999999994%, Loss = 0.4278135120868683
Epoch: 5888, Batch Gradient Norm: 10.998009666688546
Epoch: 5888, Batch Gradient Norm after: 10.998009666688546
Epoch 5889/10000, Prediction Accuracy = 62.31400000000001%, Loss = 0.4269935071468353
Epoch: 5889, Batch Gradient Norm: 11.789980830433242
Epoch: 5889, Batch Gradient Norm after: 11.789980830433242
Epoch 5890/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.4322672426700592
Epoch: 5890, Batch Gradient Norm: 10.336472479243456
Epoch: 5890, Batch Gradient Norm after: 10.336472479243456
Epoch 5891/10000, Prediction Accuracy = 62.4%, Loss = 0.4223572015762329
Epoch: 5891, Batch Gradient Norm: 8.880506302475485
Epoch: 5891, Batch Gradient Norm after: 8.880506302475485
Epoch 5892/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.41313270330429075
Epoch: 5892, Batch Gradient Norm: 9.92658927217541
Epoch: 5892, Batch Gradient Norm after: 9.92658927217541
Epoch 5893/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.4203926384449005
Epoch: 5893, Batch Gradient Norm: 10.064147686578464
Epoch: 5893, Batch Gradient Norm after: 10.064147686578464
Epoch 5894/10000, Prediction Accuracy = 62.278000000000006%, Loss = 0.421648895740509
Epoch: 5894, Batch Gradient Norm: 9.723893664900084
Epoch: 5894, Batch Gradient Norm after: 9.723893664900084
Epoch 5895/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.41975892782211305
Epoch: 5895, Batch Gradient Norm: 10.19678444694226
Epoch: 5895, Batch Gradient Norm after: 10.19678444694226
Epoch 5896/10000, Prediction Accuracy = 62.33%, Loss = 0.4236015021800995
Epoch: 5896, Batch Gradient Norm: 11.368827637507687
Epoch: 5896, Batch Gradient Norm after: 11.368827637507687
Epoch 5897/10000, Prediction Accuracy = 62.298%, Loss = 0.43191192150115965
Epoch: 5897, Batch Gradient Norm: 11.68894397244954
Epoch: 5897, Batch Gradient Norm after: 11.68894397244954
Epoch 5898/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.4326295435428619
Epoch: 5898, Batch Gradient Norm: 10.341738404234029
Epoch: 5898, Batch Gradient Norm after: 10.341738404234029
Epoch 5899/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.42217170000076293
Epoch: 5899, Batch Gradient Norm: 9.58035798098712
Epoch: 5899, Batch Gradient Norm after: 9.58035798098712
Epoch 5900/10000, Prediction Accuracy = 62.32000000000001%, Loss = 0.4180651783943176
Epoch: 5900, Batch Gradient Norm: 8.205588334781723
Epoch: 5900, Batch Gradient Norm after: 8.205588334781723
Epoch 5901/10000, Prediction Accuracy = 62.372%, Loss = 0.4105193614959717
Epoch: 5901, Batch Gradient Norm: 7.49541347565954
Epoch: 5901, Batch Gradient Norm after: 7.49541347565954
Epoch 5902/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4067133545875549
Epoch: 5902, Batch Gradient Norm: 7.125902085006038
Epoch: 5902, Batch Gradient Norm after: 7.125902085006038
Epoch 5903/10000, Prediction Accuracy = 62.336%, Loss = 0.40505866408348085
Epoch: 5903, Batch Gradient Norm: 8.539728205456102
Epoch: 5903, Batch Gradient Norm after: 8.539728205456102
Epoch 5904/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.41253127455711364
Epoch: 5904, Batch Gradient Norm: 10.533710132573233
Epoch: 5904, Batch Gradient Norm after: 10.533710132573233
Epoch 5905/10000, Prediction Accuracy = 62.27%, Loss = 0.4256540358066559
Epoch: 5905, Batch Gradient Norm: 11.671517879458252
Epoch: 5905, Batch Gradient Norm after: 11.671517879458252
Epoch 5906/10000, Prediction Accuracy = 62.398%, Loss = 0.43431465029716493
Epoch: 5906, Batch Gradient Norm: 11.03416978464081
Epoch: 5906, Batch Gradient Norm after: 11.03416978464081
Epoch 5907/10000, Prediction Accuracy = 62.278%, Loss = 0.429592365026474
Epoch: 5907, Batch Gradient Norm: 9.795881327601304
Epoch: 5907, Batch Gradient Norm after: 9.795881327601304
Epoch 5908/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.42052077054977416
Epoch: 5908, Batch Gradient Norm: 10.994390826399233
Epoch: 5908, Batch Gradient Norm after: 10.994390826399233
Epoch 5909/10000, Prediction Accuracy = 62.4%, Loss = 0.42776877880096437
Epoch: 5909, Batch Gradient Norm: 11.942994961758854
Epoch: 5909, Batch Gradient Norm after: 11.942994961758854
Epoch 5910/10000, Prediction Accuracy = 62.278%, Loss = 0.4333257615566254
Epoch: 5910, Batch Gradient Norm: 10.127298263675026
Epoch: 5910, Batch Gradient Norm after: 10.127298263675026
Epoch 5911/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4210102677345276
Epoch: 5911, Batch Gradient Norm: 10.01071427383539
Epoch: 5911, Batch Gradient Norm after: 10.01071427383539
Epoch 5912/10000, Prediction Accuracy = 62.245999999999995%, Loss = 0.4208163142204285
Epoch: 5912, Batch Gradient Norm: 10.753137853531607
Epoch: 5912, Batch Gradient Norm after: 10.753137853531607
Epoch 5913/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.42697301506996155
Epoch: 5913, Batch Gradient Norm: 9.459100249407337
Epoch: 5913, Batch Gradient Norm after: 9.459100249407337
Epoch 5914/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.417445570230484
Epoch: 5914, Batch Gradient Norm: 8.822978230570447
Epoch: 5914, Batch Gradient Norm after: 8.822978230570447
Epoch 5915/10000, Prediction Accuracy = 62.448%, Loss = 0.41332429051399233
Epoch: 5915, Batch Gradient Norm: 10.480936332856299
Epoch: 5915, Batch Gradient Norm after: 10.480936332856299
Epoch 5916/10000, Prediction Accuracy = 62.288%, Loss = 0.4240172624588013
Epoch: 5916, Batch Gradient Norm: 11.843611257147924
Epoch: 5916, Batch Gradient Norm after: 11.843611257147924
Epoch 5917/10000, Prediction Accuracy = 62.388%, Loss = 0.43474695086479187
Epoch: 5917, Batch Gradient Norm: 12.208195151050763
Epoch: 5917, Batch Gradient Norm after: 12.208195151050763
Epoch 5918/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4361302495002747
Epoch: 5918, Batch Gradient Norm: 10.445359117712204
Epoch: 5918, Batch Gradient Norm after: 10.445359117712204
Epoch 5919/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.42303707003593444
Epoch: 5919, Batch Gradient Norm: 9.666259806406583
Epoch: 5919, Batch Gradient Norm after: 9.666259806406583
Epoch 5920/10000, Prediction Accuracy = 62.412%, Loss = 0.4192097544670105
Epoch: 5920, Batch Gradient Norm: 8.368697615344379
Epoch: 5920, Batch Gradient Norm after: 8.368697615344379
Epoch 5921/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.41206076741218567
Epoch: 5921, Batch Gradient Norm: 7.669961970311533
Epoch: 5921, Batch Gradient Norm after: 7.669961970311533
Epoch 5922/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.40672969818115234
Epoch: 5922, Batch Gradient Norm: 10.184424527595231
Epoch: 5922, Batch Gradient Norm after: 10.184424527595231
Epoch 5923/10000, Prediction Accuracy = 62.29%, Loss = 0.421662038564682
Epoch: 5923, Batch Gradient Norm: 11.60137020032539
Epoch: 5923, Batch Gradient Norm after: 11.60137020032539
Epoch 5924/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.43615636229515076
Epoch: 5924, Batch Gradient Norm: 9.601929695639685
Epoch: 5924, Batch Gradient Norm after: 9.601929695639685
Epoch 5925/10000, Prediction Accuracy = 62.30799999999999%, Loss = 0.4188090920448303
Epoch: 5925, Batch Gradient Norm: 11.501246293769222
Epoch: 5925, Batch Gradient Norm after: 11.501246293769222
Epoch 5926/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.42901108264923093
Epoch: 5926, Batch Gradient Norm: 11.65751259894603
Epoch: 5926, Batch Gradient Norm after: 11.65751259894603
Epoch 5927/10000, Prediction Accuracy = 62.248000000000005%, Loss = 0.4309424042701721
Epoch: 5927, Batch Gradient Norm: 8.311986814096993
Epoch: 5927, Batch Gradient Norm after: 8.311986814096993
Epoch 5928/10000, Prediction Accuracy = 62.322%, Loss = 0.4103338301181793
Epoch: 5928, Batch Gradient Norm: 6.789581567192459
Epoch: 5928, Batch Gradient Norm after: 6.789581567192459
Epoch 5929/10000, Prediction Accuracy = 62.342%, Loss = 0.4028186023235321
Epoch: 5929, Batch Gradient Norm: 7.7462788570244765
Epoch: 5929, Batch Gradient Norm after: 7.7462788570244765
Epoch 5930/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4073005199432373
Epoch: 5930, Batch Gradient Norm: 10.849467520828226
Epoch: 5930, Batch Gradient Norm after: 10.849467520828226
Epoch 5931/10000, Prediction Accuracy = 62.324%, Loss = 0.4257991373538971
Epoch: 5931, Batch Gradient Norm: 12.410844233427373
Epoch: 5931, Batch Gradient Norm after: 12.410844233427373
Epoch 5932/10000, Prediction Accuracy = 62.412%, Loss = 0.43599963188171387
Epoch: 5932, Batch Gradient Norm: 9.856991285654205
Epoch: 5932, Batch Gradient Norm after: 9.856991285654205
Epoch 5933/10000, Prediction Accuracy = 62.348%, Loss = 0.4179180145263672
Epoch: 5933, Batch Gradient Norm: 8.93970094365793
Epoch: 5933, Batch Gradient Norm after: 8.93970094365793
Epoch 5934/10000, Prediction Accuracy = 62.40999999999999%, Loss = 0.4129303276538849
Epoch: 5934, Batch Gradient Norm: 10.24115276505967
Epoch: 5934, Batch Gradient Norm after: 10.24115276505967
Epoch 5935/10000, Prediction Accuracy = 62.266%, Loss = 0.4227832853794098
Epoch: 5935, Batch Gradient Norm: 9.229069015709374
Epoch: 5935, Batch Gradient Norm after: 9.229069015709374
Epoch 5936/10000, Prediction Accuracy = 62.334%, Loss = 0.4167685329914093
Epoch: 5936, Batch Gradient Norm: 9.162281988205267
Epoch: 5936, Batch Gradient Norm after: 9.162281988205267
Epoch 5937/10000, Prediction Accuracy = 62.234%, Loss = 0.4157048285007477
Epoch: 5937, Batch Gradient Norm: 10.370421300385097
Epoch: 5937, Batch Gradient Norm after: 10.370421300385097
Epoch 5938/10000, Prediction Accuracy = 62.378%, Loss = 0.4245284914970398
Epoch: 5938, Batch Gradient Norm: 9.161576489556149
Epoch: 5938, Batch Gradient Norm after: 9.161576489556149
Epoch 5939/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.4161269187927246
Epoch: 5939, Batch Gradient Norm: 8.743413820736242
Epoch: 5939, Batch Gradient Norm after: 8.743413820736242
Epoch 5940/10000, Prediction Accuracy = 62.364%, Loss = 0.4128881454467773
Epoch: 5940, Batch Gradient Norm: 11.052396890886191
Epoch: 5940, Batch Gradient Norm after: 11.052396890886191
Epoch 5941/10000, Prediction Accuracy = 62.416%, Loss = 0.426920884847641
Epoch: 5941, Batch Gradient Norm: 13.999794485846435
Epoch: 5941, Batch Gradient Norm after: 13.999794485846435
Epoch 5942/10000, Prediction Accuracy = 62.222%, Loss = 0.4516595244407654
Epoch: 5942, Batch Gradient Norm: 10.753474372173912
Epoch: 5942, Batch Gradient Norm after: 10.753474372173912
Epoch 5943/10000, Prediction Accuracy = 62.336%, Loss = 0.4274237334728241
Epoch: 5943, Batch Gradient Norm: 8.228448948028516
Epoch: 5943, Batch Gradient Norm after: 8.228448948028516
Epoch 5944/10000, Prediction Accuracy = 62.3%, Loss = 0.40942280888557436
Epoch: 5944, Batch Gradient Norm: 8.761199950621823
Epoch: 5944, Batch Gradient Norm after: 8.761199950621823
Epoch 5945/10000, Prediction Accuracy = 62.426%, Loss = 0.4114977180957794
Epoch: 5945, Batch Gradient Norm: 10.83993287204344
Epoch: 5945, Batch Gradient Norm after: 10.83993287204344
Epoch 5946/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.4245489537715912
Epoch: 5946, Batch Gradient Norm: 10.706737056991711
Epoch: 5946, Batch Gradient Norm after: 10.706737056991711
Epoch 5947/10000, Prediction Accuracy = 62.234%, Loss = 0.42451809644699096
Epoch: 5947, Batch Gradient Norm: 10.610927580736568
Epoch: 5947, Batch Gradient Norm after: 10.610927580736568
Epoch 5948/10000, Prediction Accuracy = 62.364%, Loss = 0.4245162129402161
Epoch: 5948, Batch Gradient Norm: 9.679596174777128
Epoch: 5948, Batch Gradient Norm after: 9.679596174777128
Epoch 5949/10000, Prediction Accuracy = 62.302%, Loss = 0.41872782707214357
Epoch: 5949, Batch Gradient Norm: 9.511612445075865
Epoch: 5949, Batch Gradient Norm after: 9.511612445075865
Epoch 5950/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.41944443583488467
Epoch: 5950, Batch Gradient Norm: 8.864736100526791
Epoch: 5950, Batch Gradient Norm after: 8.864736100526791
Epoch 5951/10000, Prediction Accuracy = 62.44%, Loss = 0.4148847579956055
Epoch: 5951, Batch Gradient Norm: 10.399395924700011
Epoch: 5951, Batch Gradient Norm after: 10.399395924700011
Epoch 5952/10000, Prediction Accuracy = 62.306%, Loss = 0.42177600264549253
Epoch: 5952, Batch Gradient Norm: 11.96917326252844
Epoch: 5952, Batch Gradient Norm after: 11.96917326252844
Epoch 5953/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.4322641909122467
Epoch: 5953, Batch Gradient Norm: 10.071716625578128
Epoch: 5953, Batch Gradient Norm after: 10.071716625578128
Epoch 5954/10000, Prediction Accuracy = 62.374%, Loss = 0.4201716721057892
Epoch: 5954, Batch Gradient Norm: 8.394763833498374
Epoch: 5954, Batch Gradient Norm after: 8.394763833498374
Epoch 5955/10000, Prediction Accuracy = 62.452%, Loss = 0.41021188497543337
Epoch: 5955, Batch Gradient Norm: 9.767646999921334
Epoch: 5955, Batch Gradient Norm after: 9.767646999921334
Epoch 5956/10000, Prediction Accuracy = 62.42199999999999%, Loss = 0.4182340383529663
Epoch: 5956, Batch Gradient Norm: 11.897969009272503
Epoch: 5956, Batch Gradient Norm after: 11.897969009272503
Epoch 5957/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4314597427845001
Epoch: 5957, Batch Gradient Norm: 11.924398945416469
Epoch: 5957, Batch Gradient Norm after: 11.924398945416469
Epoch 5958/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.43229677677154543
Epoch: 5958, Batch Gradient Norm: 8.67547639215919
Epoch: 5958, Batch Gradient Norm after: 8.67547639215919
Epoch 5959/10000, Prediction Accuracy = 62.33%, Loss = 0.41133670806884765
Epoch: 5959, Batch Gradient Norm: 7.700382849920377
Epoch: 5959, Batch Gradient Norm after: 7.700382849920377
Epoch 5960/10000, Prediction Accuracy = 62.512%, Loss = 0.40613983273506166
Epoch: 5960, Batch Gradient Norm: 7.588740821623603
Epoch: 5960, Batch Gradient Norm after: 7.588740821623603
Epoch 5961/10000, Prediction Accuracy = 62.403999999999996%, Loss = 0.4061577022075653
Epoch: 5961, Batch Gradient Norm: 9.380720588606682
Epoch: 5961, Batch Gradient Norm after: 9.380720588606682
Epoch 5962/10000, Prediction Accuracy = 62.262%, Loss = 0.41714502573013307
Epoch: 5962, Batch Gradient Norm: 10.588119904952903
Epoch: 5962, Batch Gradient Norm after: 10.588119904952903
Epoch 5963/10000, Prediction Accuracy = 62.396%, Loss = 0.42572133541107177
Epoch: 5963, Batch Gradient Norm: 12.716546572054412
Epoch: 5963, Batch Gradient Norm after: 12.716546572054412
Epoch 5964/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4412554144859314
Epoch: 5964, Batch Gradient Norm: 10.595689943640377
Epoch: 5964, Batch Gradient Norm after: 10.595689943640377
Epoch 5965/10000, Prediction Accuracy = 62.444%, Loss = 0.4244113564491272
Epoch: 5965, Batch Gradient Norm: 7.624320786666514
Epoch: 5965, Batch Gradient Norm after: 7.624320786666514
Epoch 5966/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4055378377437592
Epoch: 5966, Batch Gradient Norm: 8.638358506621806
Epoch: 5966, Batch Gradient Norm after: 8.638358506621806
Epoch 5967/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.4110510408878326
Epoch: 5967, Batch Gradient Norm: 10.821497731395274
Epoch: 5967, Batch Gradient Norm after: 10.821497731395274
Epoch 5968/10000, Prediction Accuracy = 62.138%, Loss = 0.4259398281574249
Epoch: 5968, Batch Gradient Norm: 11.349091274871453
Epoch: 5968, Batch Gradient Norm after: 11.349091274871453
Epoch 5969/10000, Prediction Accuracy = 62.257999999999996%, Loss = 0.4291346549987793
Epoch: 5969, Batch Gradient Norm: 12.483256132886465
Epoch: 5969, Batch Gradient Norm after: 12.483256132886465
Epoch 5970/10000, Prediction Accuracy = 62.386%, Loss = 0.43733983039855956
Epoch: 5970, Batch Gradient Norm: 11.517412254852658
Epoch: 5970, Batch Gradient Norm after: 11.517412254852658
Epoch 5971/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.43016660809516905
Epoch: 5971, Batch Gradient Norm: 10.119326315884647
Epoch: 5971, Batch Gradient Norm after: 10.119326315884647
Epoch 5972/10000, Prediction Accuracy = 62.424%, Loss = 0.4200869560241699
Epoch: 5972, Batch Gradient Norm: 9.526065591956549
Epoch: 5972, Batch Gradient Norm after: 9.526065591956549
Epoch 5973/10000, Prediction Accuracy = 62.352%, Loss = 0.4165017306804657
Epoch: 5973, Batch Gradient Norm: 9.359605775955504
Epoch: 5973, Batch Gradient Norm after: 9.359605775955504
Epoch 5974/10000, Prediction Accuracy = 62.438%, Loss = 0.41511994004249575
Epoch: 5974, Batch Gradient Norm: 8.79009475468845
Epoch: 5974, Batch Gradient Norm after: 8.79009475468845
Epoch 5975/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.4118897020816803
Epoch: 5975, Batch Gradient Norm: 10.126245652379907
Epoch: 5975, Batch Gradient Norm after: 10.126245652379907
Epoch 5976/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.42076635360717773
Epoch: 5976, Batch Gradient Norm: 9.591041866740852
Epoch: 5976, Batch Gradient Norm after: 9.591041866740852
Epoch 5977/10000, Prediction Accuracy = 62.45%, Loss = 0.4195405900478363
Epoch: 5977, Batch Gradient Norm: 9.632750318906076
Epoch: 5977, Batch Gradient Norm after: 9.632750318906076
Epoch 5978/10000, Prediction Accuracy = 62.384%, Loss = 0.41849754452705384
Epoch: 5978, Batch Gradient Norm: 10.886350550475672
Epoch: 5978, Batch Gradient Norm after: 10.886350550475672
Epoch 5979/10000, Prediction Accuracy = 62.436%, Loss = 0.42588248252868655
Epoch: 5979, Batch Gradient Norm: 11.183718698779884
Epoch: 5979, Batch Gradient Norm after: 11.183718698779884
Epoch 5980/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.4265546977519989
Epoch: 5980, Batch Gradient Norm: 10.485025909671888
Epoch: 5980, Batch Gradient Norm after: 10.485025909671888
Epoch 5981/10000, Prediction Accuracy = 62.38199999999999%, Loss = 0.4213664948940277
Epoch: 5981, Batch Gradient Norm: 8.99117145117282
Epoch: 5981, Batch Gradient Norm after: 8.99117145117282
Epoch 5982/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.4128014981746674
Epoch: 5982, Batch Gradient Norm: 8.96810413427418
Epoch: 5982, Batch Gradient Norm after: 8.96810413427418
Epoch 5983/10000, Prediction Accuracy = 62.355999999999995%, Loss = 0.4134110927581787
Epoch: 5983, Batch Gradient Norm: 10.021845877690394
Epoch: 5983, Batch Gradient Norm after: 10.021845877690394
Epoch 5984/10000, Prediction Accuracy = 62.24400000000001%, Loss = 0.41940242052078247
Epoch: 5984, Batch Gradient Norm: 11.03389533906301
Epoch: 5984, Batch Gradient Norm after: 11.03389533906301
Epoch 5985/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4265506207942963
Epoch: 5985, Batch Gradient Norm: 12.251905701216922
Epoch: 5985, Batch Gradient Norm after: 12.251905701216922
Epoch 5986/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.43567365407943726
Epoch: 5986, Batch Gradient Norm: 9.980165592284223
Epoch: 5986, Batch Gradient Norm after: 9.980165592284223
Epoch 5987/10000, Prediction Accuracy = 62.428%, Loss = 0.41999067068099977
Epoch: 5987, Batch Gradient Norm: 8.979867332534948
Epoch: 5987, Batch Gradient Norm after: 8.979867332534948
Epoch 5988/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4122846364974976
Epoch: 5988, Batch Gradient Norm: 9.5337287241132
Epoch: 5988, Batch Gradient Norm after: 9.5337287241132
Epoch 5989/10000, Prediction Accuracy = 62.374%, Loss = 0.4154942572116852
Epoch: 5989, Batch Gradient Norm: 10.79319418017308
Epoch: 5989, Batch Gradient Norm after: 10.79319418017308
Epoch 5990/10000, Prediction Accuracy = 62.27%, Loss = 0.42578635811805726
Epoch: 5990, Batch Gradient Norm: 9.360879182979835
Epoch: 5990, Batch Gradient Norm after: 9.360879182979835
Epoch 5991/10000, Prediction Accuracy = 62.36600000000001%, Loss = 0.4158193111419678
Epoch: 5991, Batch Gradient Norm: 9.019627917430729
Epoch: 5991, Batch Gradient Norm after: 9.019627917430729
Epoch 5992/10000, Prediction Accuracy = 62.42%, Loss = 0.4125775098800659
Epoch: 5992, Batch Gradient Norm: 10.054059361910376
Epoch: 5992, Batch Gradient Norm after: 10.054059361910376
Epoch 5993/10000, Prediction Accuracy = 62.21%, Loss = 0.4177568793296814
Epoch: 5993, Batch Gradient Norm: 10.950695917575285
Epoch: 5993, Batch Gradient Norm after: 10.950695917575285
Epoch 5994/10000, Prediction Accuracy = 62.396%, Loss = 0.4243538439273834
Epoch: 5994, Batch Gradient Norm: 9.802128955154563
Epoch: 5994, Batch Gradient Norm after: 9.802128955154563
Epoch 5995/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.41694520115852357
Epoch: 5995, Batch Gradient Norm: 9.167667382597214
Epoch: 5995, Batch Gradient Norm after: 9.167667382597214
Epoch 5996/10000, Prediction Accuracy = 62.562%, Loss = 0.4126837134361267
Epoch: 5996, Batch Gradient Norm: 9.423541839841208
Epoch: 5996, Batch Gradient Norm after: 9.423541839841208
Epoch 5997/10000, Prediction Accuracy = 62.314%, Loss = 0.4150861084461212
Epoch: 5997, Batch Gradient Norm: 9.37101781497565
Epoch: 5997, Batch Gradient Norm after: 9.37101781497565
Epoch 5998/10000, Prediction Accuracy = 62.388%, Loss = 0.41494420170783997
Epoch: 5998, Batch Gradient Norm: 9.272722288813874
Epoch: 5998, Batch Gradient Norm after: 9.272722288813874
Epoch 5999/10000, Prediction Accuracy = 62.35%, Loss = 0.4141804099082947
Epoch: 5999, Batch Gradient Norm: 10.383765240951071
Epoch: 5999, Batch Gradient Norm after: 10.383765240951071
Epoch 6000/10000, Prediction Accuracy = 62.482000000000006%, Loss = 0.4213810920715332
Epoch: 6000, Batch Gradient Norm: 11.837462241011746
Epoch: 6000, Batch Gradient Norm after: 11.837462241011746
Epoch 6001/10000, Prediction Accuracy = 62.322%, Loss = 0.43476092219352724
Epoch: 6001, Batch Gradient Norm: 9.746729613322437
Epoch: 6001, Batch Gradient Norm after: 9.746729613322437
Epoch 6002/10000, Prediction Accuracy = 62.36%, Loss = 0.41998867988586425
Epoch: 6002, Batch Gradient Norm: 8.323535825226124
Epoch: 6002, Batch Gradient Norm after: 8.323535825226124
Epoch 6003/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4089614450931549
Epoch: 6003, Batch Gradient Norm: 9.622903532084568
Epoch: 6003, Batch Gradient Norm after: 9.622903532084568
Epoch 6004/10000, Prediction Accuracy = 62.20799999999999%, Loss = 0.41576218605041504
Epoch: 6004, Batch Gradient Norm: 11.89605582153153
Epoch: 6004, Batch Gradient Norm after: 11.89605582153153
Epoch 6005/10000, Prediction Accuracy = 62.382000000000005%, Loss = 0.4312674403190613
Epoch: 6005, Batch Gradient Norm: 12.058593428593912
Epoch: 6005, Batch Gradient Norm after: 12.058593428593912
Epoch 6006/10000, Prediction Accuracy = 62.314%, Loss = 0.4331037998199463
Epoch: 6006, Batch Gradient Norm: 11.348035903912857
Epoch: 6006, Batch Gradient Norm after: 11.348035903912857
Epoch 6007/10000, Prediction Accuracy = 62.346000000000004%, Loss = 0.4273863732814789
Epoch: 6007, Batch Gradient Norm: 11.781010340615973
Epoch: 6007, Batch Gradient Norm after: 11.781010340615973
Epoch 6008/10000, Prediction Accuracy = 62.286000000000016%, Loss = 0.431272029876709
Epoch: 6008, Batch Gradient Norm: 10.502329475491061
Epoch: 6008, Batch Gradient Norm after: 10.502329475491061
Epoch 6009/10000, Prediction Accuracy = 62.322%, Loss = 0.42155337929725645
Epoch: 6009, Batch Gradient Norm: 8.604196286929644
Epoch: 6009, Batch Gradient Norm after: 8.604196286929644
Epoch 6010/10000, Prediction Accuracy = 62.366%, Loss = 0.40908360481262207
Epoch: 6010, Batch Gradient Norm: 8.978808467966845
Epoch: 6010, Batch Gradient Norm after: 8.978808467966845
Epoch 6011/10000, Prediction Accuracy = 62.324%, Loss = 0.4115048170089722
Epoch: 6011, Batch Gradient Norm: 10.583361405840767
Epoch: 6011, Batch Gradient Norm after: 10.583361405840767
Epoch 6012/10000, Prediction Accuracy = 62.282000000000004%, Loss = 0.42250463366508484
Epoch: 6012, Batch Gradient Norm: 9.935865797986208
Epoch: 6012, Batch Gradient Norm after: 9.935865797986208
Epoch 6013/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.41989009380340575
Epoch: 6013, Batch Gradient Norm: 9.860165781467387
Epoch: 6013, Batch Gradient Norm after: 9.860165781467387
Epoch 6014/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4210250496864319
Epoch: 6014, Batch Gradient Norm: 8.584613430943097
Epoch: 6014, Batch Gradient Norm after: 8.584613430943097
Epoch 6015/10000, Prediction Accuracy = 62.35%, Loss = 0.4127622783184052
Epoch: 6015, Batch Gradient Norm: 8.760036497142439
Epoch: 6015, Batch Gradient Norm after: 8.760036497142439
Epoch 6016/10000, Prediction Accuracy = 62.436%, Loss = 0.41164523363113403
Epoch: 6016, Batch Gradient Norm: 10.966249150117259
Epoch: 6016, Batch Gradient Norm after: 10.966249150117259
Epoch 6017/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.4253807783126831
Epoch: 6017, Batch Gradient Norm: 11.142598068715223
Epoch: 6017, Batch Gradient Norm after: 11.142598068715223
Epoch 6018/10000, Prediction Accuracy = 62.302%, Loss = 0.4269925057888031
Epoch: 6018, Batch Gradient Norm: 9.956866396272117
Epoch: 6018, Batch Gradient Norm after: 9.956866396272117
Epoch 6019/10000, Prediction Accuracy = 62.483999999999995%, Loss = 0.4174596190452576
Epoch: 6019, Batch Gradient Norm: 10.108399429974154
Epoch: 6019, Batch Gradient Norm after: 10.108399429974154
Epoch 6020/10000, Prediction Accuracy = 62.374%, Loss = 0.4182309627532959
Epoch: 6020, Batch Gradient Norm: 9.302455765153187
Epoch: 6020, Batch Gradient Norm after: 9.302455765153187
Epoch 6021/10000, Prediction Accuracy = 62.38599999999999%, Loss = 0.41389940977096557
Epoch: 6021, Batch Gradient Norm: 8.710096261330815
Epoch: 6021, Batch Gradient Norm after: 8.710096261330815
Epoch 6022/10000, Prediction Accuracy = 62.398%, Loss = 0.4105743050575256
Epoch: 6022, Batch Gradient Norm: 9.510214175816941
Epoch: 6022, Batch Gradient Norm after: 9.510214175816941
Epoch 6023/10000, Prediction Accuracy = 62.402%, Loss = 0.4160657227039337
Epoch: 6023, Batch Gradient Norm: 10.729001832161186
Epoch: 6023, Batch Gradient Norm after: 10.729001832161186
Epoch 6024/10000, Prediction Accuracy = 62.444%, Loss = 0.42466217279434204
Epoch: 6024, Batch Gradient Norm: 10.391871687944485
Epoch: 6024, Batch Gradient Norm after: 10.391871687944485
Epoch 6025/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.4210235118865967
Epoch: 6025, Batch Gradient Norm: 10.558488600487827
Epoch: 6025, Batch Gradient Norm after: 10.558488600487827
Epoch 6026/10000, Prediction Accuracy = 62.45799999999999%, Loss = 0.42117634415626526
Epoch: 6026, Batch Gradient Norm: 11.05535127564625
Epoch: 6026, Batch Gradient Norm after: 11.05535127564625
Epoch 6027/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4253215968608856
Epoch: 6027, Batch Gradient Norm: 9.894614786119815
Epoch: 6027, Batch Gradient Norm after: 9.894614786119815
Epoch 6028/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.4180741965770721
Epoch: 6028, Batch Gradient Norm: 9.037069089141898
Epoch: 6028, Batch Gradient Norm after: 9.037069089141898
Epoch 6029/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.4124362289905548
Epoch: 6029, Batch Gradient Norm: 10.450975342050315
Epoch: 6029, Batch Gradient Norm after: 10.450975342050315
Epoch 6030/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.421490353345871
Epoch: 6030, Batch Gradient Norm: 11.667729800406496
Epoch: 6030, Batch Gradient Norm after: 11.667729800406496
Epoch 6031/10000, Prediction Accuracy = 62.294%, Loss = 0.4292046368122101
Epoch: 6031, Batch Gradient Norm: 11.166317529389778
Epoch: 6031, Batch Gradient Norm after: 11.166317529389778
Epoch 6032/10000, Prediction Accuracy = 62.484%, Loss = 0.42574758529663087
Epoch: 6032, Batch Gradient Norm: 10.764707448250762
Epoch: 6032, Batch Gradient Norm after: 10.764707448250762
Epoch 6033/10000, Prediction Accuracy = 62.254%, Loss = 0.4222444653511047
Epoch: 6033, Batch Gradient Norm: 9.641280919728942
Epoch: 6033, Batch Gradient Norm after: 9.641280919728942
Epoch 6034/10000, Prediction Accuracy = 62.352%, Loss = 0.4149405717849731
Epoch: 6034, Batch Gradient Norm: 9.76310687045652
Epoch: 6034, Batch Gradient Norm after: 9.76310687045652
Epoch 6035/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.4156175434589386
Epoch: 6035, Batch Gradient Norm: 9.90680270931919
Epoch: 6035, Batch Gradient Norm after: 9.90680270931919
Epoch 6036/10000, Prediction Accuracy = 62.388%, Loss = 0.4172190070152283
Epoch: 6036, Batch Gradient Norm: 9.548238752509988
Epoch: 6036, Batch Gradient Norm after: 9.548238752509988
Epoch 6037/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.41585549116134646
Epoch: 6037, Batch Gradient Norm: 8.407125288617323
Epoch: 6037, Batch Gradient Norm after: 8.407125288617323
Epoch 6038/10000, Prediction Accuracy = 62.36199999999999%, Loss = 0.41005701422691343
Epoch: 6038, Batch Gradient Norm: 8.64322167853038
Epoch: 6038, Batch Gradient Norm after: 8.64322167853038
Epoch 6039/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.41026684641838074
Epoch: 6039, Batch Gradient Norm: 12.396798488365654
Epoch: 6039, Batch Gradient Norm after: 12.396798488365654
Epoch 6040/10000, Prediction Accuracy = 62.306000000000004%, Loss = 0.436250901222229
Epoch: 6040, Batch Gradient Norm: 11.51320969044216
Epoch: 6040, Batch Gradient Norm after: 11.51320969044216
Epoch 6041/10000, Prediction Accuracy = 62.233999999999995%, Loss = 0.4311307191848755
Epoch: 6041, Batch Gradient Norm: 8.25434940884702
Epoch: 6041, Batch Gradient Norm after: 8.25434940884702
Epoch 6042/10000, Prediction Accuracy = 62.372%, Loss = 0.4078239381313324
Epoch: 6042, Batch Gradient Norm: 8.254537448861305
Epoch: 6042, Batch Gradient Norm after: 8.254537448861305
Epoch 6043/10000, Prediction Accuracy = 62.44%, Loss = 0.40732824206352236
Epoch: 6043, Batch Gradient Norm: 10.104333733430789
Epoch: 6043, Batch Gradient Norm after: 10.104333733430789
Epoch 6044/10000, Prediction Accuracy = 62.391999999999996%, Loss = 0.4188119530677795
Epoch: 6044, Batch Gradient Norm: 11.301377728787653
Epoch: 6044, Batch Gradient Norm after: 11.301377728787653
Epoch 6045/10000, Prediction Accuracy = 62.408%, Loss = 0.42826741337776186
Epoch: 6045, Batch Gradient Norm: 9.497436096629972
Epoch: 6045, Batch Gradient Norm after: 9.497436096629972
Epoch 6046/10000, Prediction Accuracy = 62.428%, Loss = 0.41553460359573363
Epoch: 6046, Batch Gradient Norm: 9.263109373777095
Epoch: 6046, Batch Gradient Norm after: 9.263109373777095
Epoch 6047/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.41387759447097777
Epoch: 6047, Batch Gradient Norm: 9.414121860632582
Epoch: 6047, Batch Gradient Norm after: 9.414121860632582
Epoch 6048/10000, Prediction Accuracy = 62.338%, Loss = 0.4154080390930176
Epoch: 6048, Batch Gradient Norm: 10.172868054297046
Epoch: 6048, Batch Gradient Norm after: 10.172868054297046
Epoch 6049/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4187279999256134
Epoch: 6049, Batch Gradient Norm: 11.505695626080266
Epoch: 6049, Batch Gradient Norm after: 11.505695626080266
Epoch 6050/10000, Prediction Accuracy = 62.294000000000004%, Loss = 0.42730404138565065
Epoch: 6050, Batch Gradient Norm: 11.33106441825134
Epoch: 6050, Batch Gradient Norm after: 11.33106441825134
Epoch 6051/10000, Prediction Accuracy = 62.472%, Loss = 0.4261877119541168
Epoch: 6051, Batch Gradient Norm: 9.822161173509446
Epoch: 6051, Batch Gradient Norm after: 9.822161173509446
Epoch 6052/10000, Prediction Accuracy = 62.36%, Loss = 0.41613274812698364
Epoch: 6052, Batch Gradient Norm: 9.318534308261867
Epoch: 6052, Batch Gradient Norm after: 9.318534308261867
Epoch 6053/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.4125468015670776
Epoch: 6053, Batch Gradient Norm: 10.26593762391725
Epoch: 6053, Batch Gradient Norm after: 10.26593762391725
Epoch 6054/10000, Prediction Accuracy = 62.366%, Loss = 0.41896024346351624
Epoch: 6054, Batch Gradient Norm: 11.316719276810534
Epoch: 6054, Batch Gradient Norm after: 11.316719276810534
Epoch 6055/10000, Prediction Accuracy = 62.476%, Loss = 0.42575312256813047
Epoch: 6055, Batch Gradient Norm: 11.840392889386326
Epoch: 6055, Batch Gradient Norm after: 11.840392889386326
Epoch 6056/10000, Prediction Accuracy = 62.29600000000001%, Loss = 0.43096485137939455
Epoch: 6056, Batch Gradient Norm: 9.735496687284902
Epoch: 6056, Batch Gradient Norm after: 9.735496687284902
Epoch 6057/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.41668214201927184
Epoch: 6057, Batch Gradient Norm: 9.51778711896342
Epoch: 6057, Batch Gradient Norm after: 9.51778711896342
Epoch 6058/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4156168222427368
Epoch: 6058, Batch Gradient Norm: 10.036154965982297
Epoch: 6058, Batch Gradient Norm after: 10.036154965982297
Epoch 6059/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.4182815611362457
Epoch: 6059, Batch Gradient Norm: 10.87825636712
Epoch: 6059, Batch Gradient Norm after: 10.87825636712
Epoch 6060/10000, Prediction Accuracy = 62.354000000000006%, Loss = 0.42304844260215757
Epoch: 6060, Batch Gradient Norm: 10.278709473244362
Epoch: 6060, Batch Gradient Norm after: 10.278709473244362
Epoch 6061/10000, Prediction Accuracy = 62.275999999999996%, Loss = 0.4198815405368805
Epoch: 6061, Batch Gradient Norm: 8.732969534546672
Epoch: 6061, Batch Gradient Norm after: 8.732969534546672
Epoch 6062/10000, Prediction Accuracy = 62.464%, Loss = 0.41054393649101256
Epoch: 6062, Batch Gradient Norm: 7.907056313761138
Epoch: 6062, Batch Gradient Norm after: 7.907056313761138
Epoch 6063/10000, Prediction Accuracy = 62.41400000000001%, Loss = 0.40543659329414367
Epoch: 6063, Batch Gradient Norm: 9.623117933398703
Epoch: 6063, Batch Gradient Norm after: 9.623117933398703
Epoch 6064/10000, Prediction Accuracy = 62.366%, Loss = 0.414275199174881
Epoch: 6064, Batch Gradient Norm: 12.094624976023907
Epoch: 6064, Batch Gradient Norm after: 12.094624976023907
Epoch 6065/10000, Prediction Accuracy = 62.31600000000001%, Loss = 0.4316493272781372
Epoch: 6065, Batch Gradient Norm: 11.169178970733858
Epoch: 6065, Batch Gradient Norm after: 11.169178970733858
Epoch 6066/10000, Prediction Accuracy = 62.342%, Loss = 0.4258675515651703
Epoch: 6066, Batch Gradient Norm: 9.136440092962818
Epoch: 6066, Batch Gradient Norm after: 9.136440092962818
Epoch 6067/10000, Prediction Accuracy = 62.5%, Loss = 0.4133586585521698
Epoch: 6067, Batch Gradient Norm: 8.283168119035572
Epoch: 6067, Batch Gradient Norm after: 8.283168119035572
Epoch 6068/10000, Prediction Accuracy = 62.416%, Loss = 0.4071885824203491
Epoch: 6068, Batch Gradient Norm: 9.694422269876524
Epoch: 6068, Batch Gradient Norm after: 9.694422269876524
Epoch 6069/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4149432837963104
Epoch: 6069, Batch Gradient Norm: 10.760367218211844
Epoch: 6069, Batch Gradient Norm after: 10.760367218211844
Epoch 6070/10000, Prediction Accuracy = 62.36800000000001%, Loss = 0.4215447545051575
Epoch: 6070, Batch Gradient Norm: 11.148935271857304
Epoch: 6070, Batch Gradient Norm after: 11.148935271857304
Epoch 6071/10000, Prediction Accuracy = 62.436%, Loss = 0.42497164011001587
Epoch: 6071, Batch Gradient Norm: 10.56784209828366
Epoch: 6071, Batch Gradient Norm after: 10.56784209828366
Epoch 6072/10000, Prediction Accuracy = 62.31400000000001%, Loss = 0.4218191683292389
Epoch: 6072, Batch Gradient Norm: 9.28502536501649
Epoch: 6072, Batch Gradient Norm after: 9.28502536501649
Epoch 6073/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.41403527855873107
Epoch: 6073, Batch Gradient Norm: 9.307003798448068
Epoch: 6073, Batch Gradient Norm after: 9.307003798448068
Epoch 6074/10000, Prediction Accuracy = 62.45%, Loss = 0.41400753855705263
Epoch: 6074, Batch Gradient Norm: 10.617436076618592
Epoch: 6074, Batch Gradient Norm after: 10.617436076618592
Epoch 6075/10000, Prediction Accuracy = 62.27%, Loss = 0.4223602294921875
Epoch: 6075, Batch Gradient Norm: 10.184340765587867
Epoch: 6075, Batch Gradient Norm after: 10.184340765587867
Epoch 6076/10000, Prediction Accuracy = 62.4%, Loss = 0.41936107277870177
Epoch: 6076, Batch Gradient Norm: 8.600313959629343
Epoch: 6076, Batch Gradient Norm after: 8.600313959629343
Epoch 6077/10000, Prediction Accuracy = 62.32800000000001%, Loss = 0.4089988350868225
Epoch: 6077, Batch Gradient Norm: 8.12730066060627
Epoch: 6077, Batch Gradient Norm after: 8.12730066060627
Epoch 6078/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.4059429168701172
Epoch: 6078, Batch Gradient Norm: 9.979075012950553
Epoch: 6078, Batch Gradient Norm after: 9.979075012950553
Epoch 6079/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.41677502989768983
Epoch: 6079, Batch Gradient Norm: 12.826525167037548
Epoch: 6079, Batch Gradient Norm after: 12.826525167037548
Epoch 6080/10000, Prediction Accuracy = 62.4%, Loss = 0.4384030044078827
Epoch: 6080, Batch Gradient Norm: 11.492217218906509
Epoch: 6080, Batch Gradient Norm after: 11.492217218906509
Epoch 6081/10000, Prediction Accuracy = 62.354%, Loss = 0.42722306251525877
Epoch: 6081, Batch Gradient Norm: 11.948811702985795
Epoch: 6081, Batch Gradient Norm after: 11.948811702985795
Epoch 6082/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.4296052098274231
Epoch: 6082, Batch Gradient Norm: 11.877681129274803
Epoch: 6082, Batch Gradient Norm after: 11.877681129274803
Epoch 6083/10000, Prediction Accuracy = 62.366%, Loss = 0.42984762191772463
Epoch: 6083, Batch Gradient Norm: 9.918526710326645
Epoch: 6083, Batch Gradient Norm after: 9.918526710326645
Epoch 6084/10000, Prediction Accuracy = 62.36600000000001%, Loss = 0.41798800230026245
Epoch: 6084, Batch Gradient Norm: 7.682888103499506
Epoch: 6084, Batch Gradient Norm after: 7.682888103499506
Epoch 6085/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.4041046380996704
Epoch: 6085, Batch Gradient Norm: 8.059252848616419
Epoch: 6085, Batch Gradient Norm after: 8.059252848616419
Epoch 6086/10000, Prediction Accuracy = 62.416%, Loss = 0.4052050530910492
Epoch: 6086, Batch Gradient Norm: 10.431068133675705
Epoch: 6086, Batch Gradient Norm after: 10.431068133675705
Epoch 6087/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.41969638466835024
Epoch: 6087, Batch Gradient Norm: 11.050486294910444
Epoch: 6087, Batch Gradient Norm after: 11.050486294910444
Epoch 6088/10000, Prediction Accuracy = 62.26800000000001%, Loss = 0.42536318898200987
Epoch: 6088, Batch Gradient Norm: 9.382025215502985
Epoch: 6088, Batch Gradient Norm after: 9.382025215502985
Epoch 6089/10000, Prediction Accuracy = 62.444%, Loss = 0.41397337317466737
Epoch: 6089, Batch Gradient Norm: 9.555916281578018
Epoch: 6089, Batch Gradient Norm after: 9.555916281578018
Epoch 6090/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.413909375667572
Epoch: 6090, Batch Gradient Norm: 10.317485198925123
Epoch: 6090, Batch Gradient Norm after: 10.317485198925123
Epoch 6091/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4188291609287262
Epoch: 6091, Batch Gradient Norm: 9.66633962637741
Epoch: 6091, Batch Gradient Norm after: 9.66633962637741
Epoch 6092/10000, Prediction Accuracy = 62.452%, Loss = 0.41508194208145144
Epoch: 6092, Batch Gradient Norm: 8.439637774741188
Epoch: 6092, Batch Gradient Norm after: 8.439637774741188
Epoch 6093/10000, Prediction Accuracy = 62.246%, Loss = 0.4084084153175354
Epoch: 6093, Batch Gradient Norm: 8.087221054124749
Epoch: 6093, Batch Gradient Norm after: 8.087221054124749
Epoch 6094/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.4066624641418457
Epoch: 6094, Batch Gradient Norm: 10.023993490721628
Epoch: 6094, Batch Gradient Norm after: 10.023993490721628
Epoch 6095/10000, Prediction Accuracy = 62.25%, Loss = 0.4174097001552582
Epoch: 6095, Batch Gradient Norm: 11.818084110863264
Epoch: 6095, Batch Gradient Norm after: 11.818084110863264
Epoch 6096/10000, Prediction Accuracy = 62.476%, Loss = 0.43021432757377626
Epoch: 6096, Batch Gradient Norm: 10.58829311205162
Epoch: 6096, Batch Gradient Norm after: 10.58829311205162
Epoch 6097/10000, Prediction Accuracy = 62.402%, Loss = 0.4204963147640228
Epoch: 6097, Batch Gradient Norm: 9.953240705290325
Epoch: 6097, Batch Gradient Norm after: 9.953240705290325
Epoch 6098/10000, Prediction Accuracy = 62.334%, Loss = 0.4170172572135925
Epoch: 6098, Batch Gradient Norm: 9.721937885959779
Epoch: 6098, Batch Gradient Norm after: 9.721937885959779
Epoch 6099/10000, Prediction Accuracy = 62.42%, Loss = 0.4174234211444855
Epoch: 6099, Batch Gradient Norm: 8.876405058616704
Epoch: 6099, Batch Gradient Norm after: 8.876405058616704
Epoch 6100/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.40996999144554136
Epoch: 6100, Batch Gradient Norm: 10.305272360272129
Epoch: 6100, Batch Gradient Norm after: 10.305272360272129
Epoch 6101/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.4179244816303253
Epoch: 6101, Batch Gradient Norm: 11.000360255629095
Epoch: 6101, Batch Gradient Norm after: 11.000360255629095
Epoch 6102/10000, Prediction Accuracy = 62.284000000000006%, Loss = 0.42307600378990173
Epoch: 6102, Batch Gradient Norm: 10.133001785857813
Epoch: 6102, Batch Gradient Norm after: 10.133001785857813
Epoch 6103/10000, Prediction Accuracy = 62.202%, Loss = 0.4174159049987793
Epoch: 6103, Batch Gradient Norm: 9.872464414916507
Epoch: 6103, Batch Gradient Norm after: 9.872464414916507
Epoch 6104/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4160042107105255
Epoch: 6104, Batch Gradient Norm: 10.050871864178614
Epoch: 6104, Batch Gradient Norm after: 10.050871864178614
Epoch 6105/10000, Prediction Accuracy = 62.278%, Loss = 0.4165725588798523
Epoch: 6105, Batch Gradient Norm: 10.330215521533995
Epoch: 6105, Batch Gradient Norm after: 10.330215521533995
Epoch 6106/10000, Prediction Accuracy = 62.158%, Loss = 0.41921788454055786
Epoch: 6106, Batch Gradient Norm: 10.803317171354399
Epoch: 6106, Batch Gradient Norm after: 10.803317171354399
Epoch 6107/10000, Prediction Accuracy = 62.42%, Loss = 0.42334131002426145
Epoch: 6107, Batch Gradient Norm: 11.13721488726672
Epoch: 6107, Batch Gradient Norm after: 11.13721488726672
Epoch 6108/10000, Prediction Accuracy = 62.37800000000001%, Loss = 0.4260144650936127
Epoch: 6108, Batch Gradient Norm: 11.136162636576834
Epoch: 6108, Batch Gradient Norm after: 11.136162636576834
Epoch 6109/10000, Prediction Accuracy = 62.366%, Loss = 0.4243092179298401
Epoch: 6109, Batch Gradient Norm: 10.885171022825883
Epoch: 6109, Batch Gradient Norm after: 10.885171022825883
Epoch 6110/10000, Prediction Accuracy = 62.338%, Loss = 0.42116971015930177
Epoch: 6110, Batch Gradient Norm: 9.7735216862809
Epoch: 6110, Batch Gradient Norm after: 9.7735216862809
Epoch 6111/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.41363282799720763
Epoch: 6111, Batch Gradient Norm: 10.546162731749545
Epoch: 6111, Batch Gradient Norm after: 10.546162731749545
Epoch 6112/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.41980961561203
Epoch: 6112, Batch Gradient Norm: 10.268803922982148
Epoch: 6112, Batch Gradient Norm after: 10.268803922982148
Epoch 6113/10000, Prediction Accuracy = 62.388%, Loss = 0.41849313378334047
Epoch: 6113, Batch Gradient Norm: 9.937403527267572
Epoch: 6113, Batch Gradient Norm after: 9.937403527267572
Epoch 6114/10000, Prediction Accuracy = 62.342000000000006%, Loss = 0.41649835705757143
Epoch: 6114, Batch Gradient Norm: 9.263701249705546
Epoch: 6114, Batch Gradient Norm after: 9.263701249705546
Epoch 6115/10000, Prediction Accuracy = 62.4%, Loss = 0.41194230914115904
Epoch: 6115, Batch Gradient Norm: 9.40135030976968
Epoch: 6115, Batch Gradient Norm after: 9.40135030976968
Epoch 6116/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.41404626965522767
Epoch: 6116, Batch Gradient Norm: 8.51894229007329
Epoch: 6116, Batch Gradient Norm after: 8.51894229007329
Epoch 6117/10000, Prediction Accuracy = 62.472%, Loss = 0.4085018873214722
Epoch: 6117, Batch Gradient Norm: 9.3698922490316
Epoch: 6117, Batch Gradient Norm after: 9.3698922490316
Epoch 6118/10000, Prediction Accuracy = 62.492%, Loss = 0.41221743226051333
Epoch: 6118, Batch Gradient Norm: 10.300283989308504
Epoch: 6118, Batch Gradient Norm after: 10.300283989308504
Epoch 6119/10000, Prediction Accuracy = 62.45%, Loss = 0.4175727367401123
Epoch: 6119, Batch Gradient Norm: 11.149298025018988
Epoch: 6119, Batch Gradient Norm after: 11.149298025018988
Epoch 6120/10000, Prediction Accuracy = 62.472%, Loss = 0.42418503761291504
Epoch: 6120, Batch Gradient Norm: 10.97677687863163
Epoch: 6120, Batch Gradient Norm after: 10.97677687863163
Epoch 6121/10000, Prediction Accuracy = 62.291999999999994%, Loss = 0.4239774763584137
Epoch: 6121, Batch Gradient Norm: 10.234097491513335
Epoch: 6121, Batch Gradient Norm after: 10.234097491513335
Epoch 6122/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.4173819363117218
Epoch: 6122, Batch Gradient Norm: 10.497974548889452
Epoch: 6122, Batch Gradient Norm after: 10.497974548889452
Epoch 6123/10000, Prediction Accuracy = 62.412%, Loss = 0.418321019411087
Epoch: 6123, Batch Gradient Norm: 10.596594312359104
Epoch: 6123, Batch Gradient Norm after: 10.596594312359104
Epoch 6124/10000, Prediction Accuracy = 62.488%, Loss = 0.41946394443511964
Epoch: 6124, Batch Gradient Norm: 9.683705777718627
Epoch: 6124, Batch Gradient Norm after: 9.683705777718627
Epoch 6125/10000, Prediction Accuracy = 62.261999999999986%, Loss = 0.41440730094909667
Epoch: 6125, Batch Gradient Norm: 9.13560181534553
Epoch: 6125, Batch Gradient Norm after: 9.13560181534553
Epoch 6126/10000, Prediction Accuracy = 62.455999999999996%, Loss = 0.41136065125465393
Epoch: 6126, Batch Gradient Norm: 9.295937360356609
Epoch: 6126, Batch Gradient Norm after: 9.295937360356609
Epoch 6127/10000, Prediction Accuracy = 62.50600000000001%, Loss = 0.4127234399318695
Epoch: 6127, Batch Gradient Norm: 9.394279416182801
Epoch: 6127, Batch Gradient Norm after: 9.394279416182801
Epoch 6128/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4137836813926697
Epoch: 6128, Batch Gradient Norm: 10.15289227143221
Epoch: 6128, Batch Gradient Norm after: 10.15289227143221
Epoch 6129/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4186930537223816
Epoch: 6129, Batch Gradient Norm: 9.655660674805555
Epoch: 6129, Batch Gradient Norm after: 9.655660674805555
Epoch 6130/10000, Prediction Accuracy = 62.354%, Loss = 0.4142920792102814
Epoch: 6130, Batch Gradient Norm: 10.229191032419369
Epoch: 6130, Batch Gradient Norm after: 10.229191032419369
Epoch 6131/10000, Prediction Accuracy = 62.4%, Loss = 0.4178489625453949
Epoch: 6131, Batch Gradient Norm: 9.204081248191002
Epoch: 6131, Batch Gradient Norm after: 9.204081248191002
Epoch 6132/10000, Prediction Accuracy = 62.35600000000001%, Loss = 0.4134353816509247
Epoch: 6132, Batch Gradient Norm: 8.075212637346214
Epoch: 6132, Batch Gradient Norm after: 8.075212637346214
Epoch 6133/10000, Prediction Accuracy = 62.438%, Loss = 0.4061120569705963
Epoch: 6133, Batch Gradient Norm: 11.170869320822934
Epoch: 6133, Batch Gradient Norm after: 11.170869320822934
Epoch 6134/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.42557676434516906
Epoch: 6134, Batch Gradient Norm: 11.975120565593336
Epoch: 6134, Batch Gradient Norm after: 11.975120565593336
Epoch 6135/10000, Prediction Accuracy = 62.414%, Loss = 0.4319905638694763
Epoch: 6135, Batch Gradient Norm: 8.664724820657304
Epoch: 6135, Batch Gradient Norm after: 8.664724820657304
Epoch 6136/10000, Prediction Accuracy = 62.324%, Loss = 0.4082587242126465
Epoch: 6136, Batch Gradient Norm: 8.141792980359288
Epoch: 6136, Batch Gradient Norm after: 8.141792980359288
Epoch 6137/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.40353708267211913
Epoch: 6137, Batch Gradient Norm: 11.489056088423196
Epoch: 6137, Batch Gradient Norm after: 11.489056088423196
Epoch 6138/10000, Prediction Accuracy = 62.376%, Loss = 0.42370717525482177
Epoch: 6138, Batch Gradient Norm: 14.308859586593414
Epoch: 6138, Batch Gradient Norm after: 14.308859586593414
Epoch 6139/10000, Prediction Accuracy = 62.472%, Loss = 0.44804041385650634
Epoch: 6139, Batch Gradient Norm: 11.227711179878522
Epoch: 6139, Batch Gradient Norm after: 11.227711179878522
Epoch 6140/10000, Prediction Accuracy = 62.402%, Loss = 0.4242520809173584
Epoch: 6140, Batch Gradient Norm: 9.690238380573229
Epoch: 6140, Batch Gradient Norm after: 9.690238380573229
Epoch 6141/10000, Prediction Accuracy = 62.462%, Loss = 0.41349726915359497
Epoch: 6141, Batch Gradient Norm: 10.282780219345863
Epoch: 6141, Batch Gradient Norm after: 10.282780219345863
Epoch 6142/10000, Prediction Accuracy = 62.416%, Loss = 0.41767418384552
Epoch: 6142, Batch Gradient Norm: 10.969585660242249
Epoch: 6142, Batch Gradient Norm after: 10.969585660242249
Epoch 6143/10000, Prediction Accuracy = 62.38800000000001%, Loss = 0.42239893078804014
Epoch: 6143, Batch Gradient Norm: 9.425074629051124
Epoch: 6143, Batch Gradient Norm after: 9.425074629051124
Epoch 6144/10000, Prediction Accuracy = 62.462%, Loss = 0.4122705340385437
Epoch: 6144, Batch Gradient Norm: 7.751490379901373
Epoch: 6144, Batch Gradient Norm after: 7.751490379901373
Epoch 6145/10000, Prediction Accuracy = 62.376%, Loss = 0.402599573135376
Epoch: 6145, Batch Gradient Norm: 7.83696439197914
Epoch: 6145, Batch Gradient Norm after: 7.83696439197914
Epoch 6146/10000, Prediction Accuracy = 62.36999999999999%, Loss = 0.4039065182209015
Epoch: 6146, Batch Gradient Norm: 7.061404672040867
Epoch: 6146, Batch Gradient Norm after: 7.061404672040867
Epoch 6147/10000, Prediction Accuracy = 62.432%, Loss = 0.40090020298957824
Epoch: 6147, Batch Gradient Norm: 8.234480524130461
Epoch: 6147, Batch Gradient Norm after: 8.234480524130461
Epoch 6148/10000, Prediction Accuracy = 62.458000000000006%, Loss = 0.4055743873119354
Epoch: 6148, Batch Gradient Norm: 11.026234868663861
Epoch: 6148, Batch Gradient Norm after: 11.026234868663861
Epoch 6149/10000, Prediction Accuracy = 62.306%, Loss = 0.4267170488834381
Epoch: 6149, Batch Gradient Norm: 9.595139791286698
Epoch: 6149, Batch Gradient Norm after: 9.595139791286698
Epoch 6150/10000, Prediction Accuracy = 62.44599999999999%, Loss = 0.41740297675132754
Epoch: 6150, Batch Gradient Norm: 10.22007605077503
Epoch: 6150, Batch Gradient Norm after: 10.22007605077503
Epoch 6151/10000, Prediction Accuracy = 62.414%, Loss = 0.41635125279426577
Epoch: 6151, Batch Gradient Norm: 12.705788228975932
Epoch: 6151, Batch Gradient Norm after: 12.705788228975932
Epoch 6152/10000, Prediction Accuracy = 62.443999999999996%, Loss = 0.4330581724643707
Epoch: 6152, Batch Gradient Norm: 12.091707400233242
Epoch: 6152, Batch Gradient Norm after: 12.091707400233242
Epoch 6153/10000, Prediction Accuracy = 62.29200000000001%, Loss = 0.4285559892654419
Epoch: 6153, Batch Gradient Norm: 10.17773664750012
Epoch: 6153, Batch Gradient Norm after: 10.17773664750012
Epoch 6154/10000, Prediction Accuracy = 62.408%, Loss = 0.4160975217819214
Epoch: 6154, Batch Gradient Norm: 9.663159686381638
Epoch: 6154, Batch Gradient Norm after: 9.663159686381638
Epoch 6155/10000, Prediction Accuracy = 62.352%, Loss = 0.41332492232322693
Epoch: 6155, Batch Gradient Norm: 9.226336395547102
Epoch: 6155, Batch Gradient Norm after: 9.226336395547102
Epoch 6156/10000, Prediction Accuracy = 62.376%, Loss = 0.4107338547706604
Epoch: 6156, Batch Gradient Norm: 8.69173177543337
Epoch: 6156, Batch Gradient Norm after: 8.69173177543337
Epoch 6157/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.40740675926208497
Epoch: 6157, Batch Gradient Norm: 9.072788209158634
Epoch: 6157, Batch Gradient Norm after: 9.072788209158634
Epoch 6158/10000, Prediction Accuracy = 62.374%, Loss = 0.40930001735687255
Epoch: 6158, Batch Gradient Norm: 10.21672264433021
Epoch: 6158, Batch Gradient Norm after: 10.21672264433021
Epoch 6159/10000, Prediction Accuracy = 62.412%, Loss = 0.41784133911132815
Epoch: 6159, Batch Gradient Norm: 10.901609634599364
Epoch: 6159, Batch Gradient Norm after: 10.901609634599364
Epoch 6160/10000, Prediction Accuracy = 62.258%, Loss = 0.42310875058174136
Epoch: 6160, Batch Gradient Norm: 12.024644003835649
Epoch: 6160, Batch Gradient Norm after: 12.024644003835649
Epoch 6161/10000, Prediction Accuracy = 62.492%, Loss = 0.4313066303730011
Epoch: 6161, Batch Gradient Norm: 10.553699693961915
Epoch: 6161, Batch Gradient Norm after: 10.553699693961915
Epoch 6162/10000, Prediction Accuracy = 62.318%, Loss = 0.4207391917705536
Epoch: 6162, Batch Gradient Norm: 7.966488256164564
Epoch: 6162, Batch Gradient Norm after: 7.966488256164564
Epoch 6163/10000, Prediction Accuracy = 62.459999999999994%, Loss = 0.40378810167312623
Epoch: 6163, Batch Gradient Norm: 7.842239128311305
Epoch: 6163, Batch Gradient Norm after: 7.842239128311305
Epoch 6164/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.40235901474952696
Epoch: 6164, Batch Gradient Norm: 10.312919349603096
Epoch: 6164, Batch Gradient Norm after: 10.312919349603096
Epoch 6165/10000, Prediction Accuracy = 62.412%, Loss = 0.4174094557762146
Epoch: 6165, Batch Gradient Norm: 12.666786477130099
Epoch: 6165, Batch Gradient Norm after: 12.666786477130099
Epoch 6166/10000, Prediction Accuracy = 62.376%, Loss = 0.43384018540382385
Epoch: 6166, Batch Gradient Norm: 13.347596855899113
Epoch: 6166, Batch Gradient Norm after: 13.347596855899113
Epoch 6167/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.44001697897911074
Epoch: 6167, Batch Gradient Norm: 10.946664546113439
Epoch: 6167, Batch Gradient Norm after: 10.946664546113439
Epoch 6168/10000, Prediction Accuracy = 62.438%, Loss = 0.42184893488883973
Epoch: 6168, Batch Gradient Norm: 8.672540036049842
Epoch: 6168, Batch Gradient Norm after: 8.672540036049842
Epoch 6169/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.4073418080806732
Epoch: 6169, Batch Gradient Norm: 9.34862761539327
Epoch: 6169, Batch Gradient Norm after: 9.34862761539327
Epoch 6170/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.4118762195110321
Epoch: 6170, Batch Gradient Norm: 9.198940790501041
Epoch: 6170, Batch Gradient Norm after: 9.198940790501041
Epoch 6171/10000, Prediction Accuracy = 62.462%, Loss = 0.4113039493560791
Epoch: 6171, Batch Gradient Norm: 9.08810110350838
Epoch: 6171, Batch Gradient Norm after: 9.08810110350838
Epoch 6172/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.4109914779663086
Epoch: 6172, Batch Gradient Norm: 9.509384917575945
Epoch: 6172, Batch Gradient Norm after: 9.509384917575945
Epoch 6173/10000, Prediction Accuracy = 62.314%, Loss = 0.41256201863288877
Epoch: 6173, Batch Gradient Norm: 11.243580824062942
Epoch: 6173, Batch Gradient Norm after: 11.243580824062942
Epoch 6174/10000, Prediction Accuracy = 62.432%, Loss = 0.4240261733531952
Epoch: 6174, Batch Gradient Norm: 11.666621994578769
Epoch: 6174, Batch Gradient Norm after: 11.666621994578769
Epoch 6175/10000, Prediction Accuracy = 62.324%, Loss = 0.42708141803741456
Epoch: 6175, Batch Gradient Norm: 9.92100924063376
Epoch: 6175, Batch Gradient Norm after: 9.92100924063376
Epoch 6176/10000, Prediction Accuracy = 62.412%, Loss = 0.41517338156700134
Epoch: 6176, Batch Gradient Norm: 8.875816547906025
Epoch: 6176, Batch Gradient Norm after: 8.875816547906025
Epoch 6177/10000, Prediction Accuracy = 62.378%, Loss = 0.40780951380729674
Epoch: 6177, Batch Gradient Norm: 9.61773063516478
Epoch: 6177, Batch Gradient Norm after: 9.61773063516478
Epoch 6178/10000, Prediction Accuracy = 62.35%, Loss = 0.4115434169769287
Epoch: 6178, Batch Gradient Norm: 10.908144016299934
Epoch: 6178, Batch Gradient Norm after: 10.908144016299934
Epoch 6179/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.4210025131702423
Epoch: 6179, Batch Gradient Norm: 9.35102560378167
Epoch: 6179, Batch Gradient Norm after: 9.35102560378167
Epoch 6180/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.411793053150177
Epoch: 6180, Batch Gradient Norm: 8.211493774645675
Epoch: 6180, Batch Gradient Norm after: 8.211493774645675
Epoch 6181/10000, Prediction Accuracy = 62.492%, Loss = 0.4052578151226044
Epoch: 6181, Batch Gradient Norm: 8.316717578731389
Epoch: 6181, Batch Gradient Norm after: 8.316717578731389
Epoch 6182/10000, Prediction Accuracy = 62.5%, Loss = 0.4053062617778778
Epoch: 6182, Batch Gradient Norm: 9.641788249087575
Epoch: 6182, Batch Gradient Norm after: 9.641788249087575
Epoch 6183/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.41323078870773317
Epoch: 6183, Batch Gradient Norm: 11.468479903512597
Epoch: 6183, Batch Gradient Norm after: 11.468479903512597
Epoch 6184/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4261547863483429
Epoch: 6184, Batch Gradient Norm: 10.884302975341106
Epoch: 6184, Batch Gradient Norm after: 10.884302975341106
Epoch 6185/10000, Prediction Accuracy = 62.339999999999996%, Loss = 0.42285587787628176
Epoch: 6185, Batch Gradient Norm: 10.15988690487536
Epoch: 6185, Batch Gradient Norm after: 10.15988690487536
Epoch 6186/10000, Prediction Accuracy = 62.428%, Loss = 0.417292982339859
Epoch: 6186, Batch Gradient Norm: 10.30826967816125
Epoch: 6186, Batch Gradient Norm after: 10.30826967816125
Epoch 6187/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.4165728747844696
Epoch: 6187, Batch Gradient Norm: 11.243402833829329
Epoch: 6187, Batch Gradient Norm after: 11.243402833829329
Epoch 6188/10000, Prediction Accuracy = 62.396%, Loss = 0.42247930765151975
Epoch: 6188, Batch Gradient Norm: 11.070464213360356
Epoch: 6188, Batch Gradient Norm after: 11.070464213360356
Epoch 6189/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.4220218062400818
Epoch: 6189, Batch Gradient Norm: 10.469153770355579
Epoch: 6189, Batch Gradient Norm after: 10.469153770355579
Epoch 6190/10000, Prediction Accuracy = 62.336%, Loss = 0.4170046031475067
Epoch: 6190, Batch Gradient Norm: 10.368179933590833
Epoch: 6190, Batch Gradient Norm after: 10.368179933590833
Epoch 6191/10000, Prediction Accuracy = 62.324%, Loss = 0.4162285506725311
Epoch: 6191, Batch Gradient Norm: 9.81578373390624
Epoch: 6191, Batch Gradient Norm after: 9.81578373390624
Epoch 6192/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.41240776777267457
Epoch: 6192, Batch Gradient Norm: 8.489989586043228
Epoch: 6192, Batch Gradient Norm after: 8.489989586043228
Epoch 6193/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.40453570485115053
Epoch: 6193, Batch Gradient Norm: 8.654335628743828
Epoch: 6193, Batch Gradient Norm after: 8.654335628743828
Epoch 6194/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4058689475059509
Epoch: 6194, Batch Gradient Norm: 9.521802868402446
Epoch: 6194, Batch Gradient Norm after: 9.521802868402446
Epoch 6195/10000, Prediction Accuracy = 62.426%, Loss = 0.41229780912399294
Epoch: 6195, Batch Gradient Norm: 10.056342003042579
Epoch: 6195, Batch Gradient Norm after: 10.056342003042579
Epoch 6196/10000, Prediction Accuracy = 62.37600000000001%, Loss = 0.41719081401824953
Epoch: 6196, Batch Gradient Norm: 9.997598862446061
Epoch: 6196, Batch Gradient Norm after: 9.997598862446061
Epoch 6197/10000, Prediction Accuracy = 62.45%, Loss = 0.4149529278278351
Epoch: 6197, Batch Gradient Norm: 11.592388621558658
Epoch: 6197, Batch Gradient Norm after: 11.592388621558658
Epoch 6198/10000, Prediction Accuracy = 62.46%, Loss = 0.4255856335163116
Epoch: 6198, Batch Gradient Norm: 12.30726205551023
Epoch: 6198, Batch Gradient Norm after: 12.30726205551023
Epoch 6199/10000, Prediction Accuracy = 62.372%, Loss = 0.43315274119377134
Epoch: 6199, Batch Gradient Norm: 9.26238333020754
Epoch: 6199, Batch Gradient Norm after: 9.26238333020754
Epoch 6200/10000, Prediction Accuracy = 62.58%, Loss = 0.4105846703052521
Epoch: 6200, Batch Gradient Norm: 8.312948451447724
Epoch: 6200, Batch Gradient Norm after: 8.312948451447724
Epoch 6201/10000, Prediction Accuracy = 62.43399999999999%, Loss = 0.4037569582462311
Epoch: 6201, Batch Gradient Norm: 9.309158021141672
Epoch: 6201, Batch Gradient Norm after: 9.309158021141672
Epoch 6202/10000, Prediction Accuracy = 62.528%, Loss = 0.4101377546787262
Epoch: 6202, Batch Gradient Norm: 9.967225911392958
Epoch: 6202, Batch Gradient Norm after: 9.967225911392958
Epoch 6203/10000, Prediction Accuracy = 62.338%, Loss = 0.4153454601764679
Epoch: 6203, Batch Gradient Norm: 9.976493585932822
Epoch: 6203, Batch Gradient Norm after: 9.976493585932822
Epoch 6204/10000, Prediction Accuracy = 62.406000000000006%, Loss = 0.4156570076942444
Epoch: 6204, Batch Gradient Norm: 9.614142418342801
Epoch: 6204, Batch Gradient Norm after: 9.614142418342801
Epoch 6205/10000, Prediction Accuracy = 62.44200000000001%, Loss = 0.41304967999458314
Epoch: 6205, Batch Gradient Norm: 9.582292445758135
Epoch: 6205, Batch Gradient Norm after: 9.582292445758135
Epoch 6206/10000, Prediction Accuracy = 62.44%, Loss = 0.41229185461997986
Epoch: 6206, Batch Gradient Norm: 10.025575144038847
Epoch: 6206, Batch Gradient Norm after: 10.025575144038847
Epoch 6207/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.41432693004608157
Epoch: 6207, Batch Gradient Norm: 12.59951678611001
Epoch: 6207, Batch Gradient Norm after: 12.59951678611001
Epoch 6208/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.4342692196369171
Epoch: 6208, Batch Gradient Norm: 10.901919766887588
Epoch: 6208, Batch Gradient Norm after: 10.901919766887588
Epoch 6209/10000, Prediction Accuracy = 62.432%, Loss = 0.42150619626045227
Epoch: 6209, Batch Gradient Norm: 9.737233613526296
Epoch: 6209, Batch Gradient Norm after: 9.737233613526296
Epoch 6210/10000, Prediction Accuracy = 62.354000000000006%, Loss = 0.4116366744041443
Epoch: 6210, Batch Gradient Norm: 11.015097961481171
Epoch: 6210, Batch Gradient Norm after: 11.015097961481171
Epoch 6211/10000, Prediction Accuracy = 62.318000000000005%, Loss = 0.4198923885822296
Epoch: 6211, Batch Gradient Norm: 11.245326551902533
Epoch: 6211, Batch Gradient Norm after: 11.245326551902533
Epoch 6212/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.42308205366134644
Epoch: 6212, Batch Gradient Norm: 9.056891344758055
Epoch: 6212, Batch Gradient Norm after: 9.056891344758055
Epoch 6213/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.40918333530426027
Epoch: 6213, Batch Gradient Norm: 8.714463678719532
Epoch: 6213, Batch Gradient Norm after: 8.714463678719532
Epoch 6214/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.4060246109962463
Epoch: 6214, Batch Gradient Norm: 9.720009104861141
Epoch: 6214, Batch Gradient Norm after: 9.720009104861141
Epoch 6215/10000, Prediction Accuracy = 62.343999999999994%, Loss = 0.41251141428947447
Epoch: 6215, Batch Gradient Norm: 10.34682809773954
Epoch: 6215, Batch Gradient Norm after: 10.34682809773954
Epoch 6216/10000, Prediction Accuracy = 62.524%, Loss = 0.4194006145000458
Epoch: 6216, Batch Gradient Norm: 8.97913395549925
Epoch: 6216, Batch Gradient Norm after: 8.97913395549925
Epoch 6217/10000, Prediction Accuracy = 62.31%, Loss = 0.409343010187149
Epoch: 6217, Batch Gradient Norm: 8.235087182972032
Epoch: 6217, Batch Gradient Norm after: 8.235087182972032
Epoch 6218/10000, Prediction Accuracy = 62.462%, Loss = 0.40353923439979555
Epoch: 6218, Batch Gradient Norm: 9.896047629879165
Epoch: 6218, Batch Gradient Norm after: 9.896047629879165
Epoch 6219/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.412536096572876
Epoch: 6219, Batch Gradient Norm: 10.647423727403933
Epoch: 6219, Batch Gradient Norm after: 10.647423727403933
Epoch 6220/10000, Prediction Accuracy = 62.342000000000006%, Loss = 0.4186339020729065
Epoch: 6220, Batch Gradient Norm: 9.876310372319463
Epoch: 6220, Batch Gradient Norm after: 9.876310372319463
Epoch 6221/10000, Prediction Accuracy = 62.544%, Loss = 0.4148173749446869
Epoch: 6221, Batch Gradient Norm: 9.400391915341379
Epoch: 6221, Batch Gradient Norm after: 9.400391915341379
Epoch 6222/10000, Prediction Accuracy = 62.378%, Loss = 0.4106564700603485
Epoch: 6222, Batch Gradient Norm: 11.950439880202335
Epoch: 6222, Batch Gradient Norm after: 11.950439880202335
Epoch 6223/10000, Prediction Accuracy = 62.512%, Loss = 0.4280812978744507
Epoch: 6223, Batch Gradient Norm: 12.074519844960854
Epoch: 6223, Batch Gradient Norm after: 12.074519844960854
Epoch 6224/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.4293358027935028
Epoch: 6224, Batch Gradient Norm: 9.488360572697415
Epoch: 6224, Batch Gradient Norm after: 9.488360572697415
Epoch 6225/10000, Prediction Accuracy = 62.42%, Loss = 0.41130253076553347
Epoch: 6225, Batch Gradient Norm: 7.9390709988717525
Epoch: 6225, Batch Gradient Norm after: 7.9390709988717525
Epoch 6226/10000, Prediction Accuracy = 62.348%, Loss = 0.4021095335483551
Epoch: 6226, Batch Gradient Norm: 8.77482097778146
Epoch: 6226, Batch Gradient Norm after: 8.77482097778146
Epoch 6227/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4061168134212494
Epoch: 6227, Batch Gradient Norm: 10.494096920650088
Epoch: 6227, Batch Gradient Norm after: 10.494096920650088
Epoch 6228/10000, Prediction Accuracy = 62.338%, Loss = 0.4173277020454407
Epoch: 6228, Batch Gradient Norm: 11.133933978209614
Epoch: 6228, Batch Gradient Norm after: 11.133933978209614
Epoch 6229/10000, Prediction Accuracy = 62.422000000000004%, Loss = 0.4219400882720947
Epoch: 6229, Batch Gradient Norm: 10.959094635063092
Epoch: 6229, Batch Gradient Norm after: 10.959094635063092
Epoch 6230/10000, Prediction Accuracy = 62.40599999999999%, Loss = 0.4212114453315735
Epoch: 6230, Batch Gradient Norm: 11.043541381713757
Epoch: 6230, Batch Gradient Norm after: 11.043541381713757
Epoch 6231/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.42201152443885803
Epoch: 6231, Batch Gradient Norm: 10.976255857596811
Epoch: 6231, Batch Gradient Norm after: 10.976255857596811
Epoch 6232/10000, Prediction Accuracy = 62.398%, Loss = 0.42118957042694094
Epoch: 6232, Batch Gradient Norm: 11.452035642583803
Epoch: 6232, Batch Gradient Norm after: 11.452035642583803
Epoch 6233/10000, Prediction Accuracy = 62.362%, Loss = 0.42376614809036256
Epoch: 6233, Batch Gradient Norm: 10.628676810214984
Epoch: 6233, Batch Gradient Norm after: 10.628676810214984
Epoch 6234/10000, Prediction Accuracy = 62.572%, Loss = 0.41813873052597045
Epoch: 6234, Batch Gradient Norm: 8.6994145635283
Epoch: 6234, Batch Gradient Norm after: 8.6994145635283
Epoch 6235/10000, Prediction Accuracy = 62.5%, Loss = 0.40557647347450254
Epoch: 6235, Batch Gradient Norm: 7.955634702546707
Epoch: 6235, Batch Gradient Norm after: 7.955634702546707
Epoch 6236/10000, Prediction Accuracy = 62.548%, Loss = 0.40118713974952697
Epoch: 6236, Batch Gradient Norm: 10.43322917576214
Epoch: 6236, Batch Gradient Norm after: 10.43322917576214
Epoch 6237/10000, Prediction Accuracy = 62.367999999999995%, Loss = 0.4180866777896881
Epoch: 6237, Batch Gradient Norm: 10.598942767082775
Epoch: 6237, Batch Gradient Norm after: 10.598942767082775
Epoch 6238/10000, Prediction Accuracy = 62.31999999999999%, Loss = 0.42095243334770205
Epoch: 6238, Batch Gradient Norm: 9.177285343761584
Epoch: 6238, Batch Gradient Norm after: 9.177285343761584
Epoch 6239/10000, Prediction Accuracy = 62.428%, Loss = 0.4086998403072357
Epoch: 6239, Batch Gradient Norm: 9.764446543163093
Epoch: 6239, Batch Gradient Norm after: 9.764446543163093
Epoch 6240/10000, Prediction Accuracy = 62.33599999999999%, Loss = 0.4108941316604614
Epoch: 6240, Batch Gradient Norm: 10.443044582738418
Epoch: 6240, Batch Gradient Norm after: 10.443044582738418
Epoch 6241/10000, Prediction Accuracy = 62.45799999999999%, Loss = 0.41559481620788574
Epoch: 6241, Batch Gradient Norm: 9.729789983158748
Epoch: 6241, Batch Gradient Norm after: 9.729789983158748
Epoch 6242/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4112558841705322
Epoch: 6242, Batch Gradient Norm: 9.999897487391017
Epoch: 6242, Batch Gradient Norm after: 9.999897487391017
Epoch 6243/10000, Prediction Accuracy = 62.592000000000006%, Loss = 0.41338921785354615
Epoch: 6243, Batch Gradient Norm: 11.51997858094704
Epoch: 6243, Batch Gradient Norm after: 11.51997858094704
Epoch 6244/10000, Prediction Accuracy = 62.398%, Loss = 0.42556664943695066
Epoch: 6244, Batch Gradient Norm: 11.874053738729586
Epoch: 6244, Batch Gradient Norm after: 11.874053738729586
Epoch 6245/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.42819203734397887
Epoch: 6245, Batch Gradient Norm: 10.38959568921106
Epoch: 6245, Batch Gradient Norm after: 10.38959568921106
Epoch 6246/10000, Prediction Accuracy = 62.33%, Loss = 0.41617814898490907
Epoch: 6246, Batch Gradient Norm: 8.965905595238786
Epoch: 6246, Batch Gradient Norm after: 8.965905595238786
Epoch 6247/10000, Prediction Accuracy = 62.46%, Loss = 0.4068894922733307
Epoch: 6247, Batch Gradient Norm: 8.783964918053417
Epoch: 6247, Batch Gradient Norm after: 8.783964918053417
Epoch 6248/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4061668455600739
Epoch: 6248, Batch Gradient Norm: 8.65657252018941
Epoch: 6248, Batch Gradient Norm after: 8.65657252018941
Epoch 6249/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4060640692710876
Epoch: 6249, Batch Gradient Norm: 9.173821439518631
Epoch: 6249, Batch Gradient Norm after: 9.173821439518631
Epoch 6250/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.409539395570755
Epoch: 6250, Batch Gradient Norm: 9.88473602760659
Epoch: 6250, Batch Gradient Norm after: 9.88473602760659
Epoch 6251/10000, Prediction Accuracy = 62.396%, Loss = 0.41355451941490173
Epoch: 6251, Batch Gradient Norm: 10.815213701607231
Epoch: 6251, Batch Gradient Norm after: 10.815213701607231
Epoch 6252/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.4201932907104492
Epoch: 6252, Batch Gradient Norm: 9.74659735569805
Epoch: 6252, Batch Gradient Norm after: 9.74659735569805
Epoch 6253/10000, Prediction Accuracy = 62.364%, Loss = 0.4127459585666656
Epoch: 6253, Batch Gradient Norm: 8.545913816777837
Epoch: 6253, Batch Gradient Norm after: 8.545913816777837
Epoch 6254/10000, Prediction Accuracy = 62.428%, Loss = 0.4057084023952484
Epoch: 6254, Batch Gradient Norm: 9.050934517851534
Epoch: 6254, Batch Gradient Norm after: 9.050934517851534
Epoch 6255/10000, Prediction Accuracy = 62.227999999999994%, Loss = 0.40750682950019834
Epoch: 6255, Batch Gradient Norm: 10.954132484921109
Epoch: 6255, Batch Gradient Norm after: 10.954132484921109
Epoch 6256/10000, Prediction Accuracy = 62.29%, Loss = 0.4202495038509369
Epoch: 6256, Batch Gradient Norm: 11.763015089633502
Epoch: 6256, Batch Gradient Norm after: 11.763015089633502
Epoch 6257/10000, Prediction Accuracy = 62.424%, Loss = 0.4252799928188324
Epoch: 6257, Batch Gradient Norm: 11.366224770748902
Epoch: 6257, Batch Gradient Norm after: 11.366224770748902
Epoch 6258/10000, Prediction Accuracy = 62.414%, Loss = 0.42238431572914126
Epoch: 6258, Batch Gradient Norm: 9.550509292698116
Epoch: 6258, Batch Gradient Norm after: 9.550509292698116
Epoch 6259/10000, Prediction Accuracy = 62.456%, Loss = 0.41127054691314696
Epoch: 6259, Batch Gradient Norm: 8.7860206738226
Epoch: 6259, Batch Gradient Norm after: 8.7860206738226
Epoch 6260/10000, Prediction Accuracy = 62.55%, Loss = 0.40625213980674746
Epoch: 6260, Batch Gradient Norm: 9.491008964823486
Epoch: 6260, Batch Gradient Norm after: 9.491008964823486
Epoch 6261/10000, Prediction Accuracy = 62.49000000000001%, Loss = 0.4095996916294098
Epoch: 6261, Batch Gradient Norm: 11.095150087023177
Epoch: 6261, Batch Gradient Norm after: 11.095150087023177
Epoch 6262/10000, Prediction Accuracy = 62.6%, Loss = 0.42008501291275024
Epoch: 6262, Batch Gradient Norm: 11.336797268712763
Epoch: 6262, Batch Gradient Norm after: 11.336797268712763
Epoch 6263/10000, Prediction Accuracy = 62.354%, Loss = 0.4219512760639191
Epoch: 6263, Batch Gradient Norm: 11.608708446715287
Epoch: 6263, Batch Gradient Norm after: 11.608708446715287
Epoch 6264/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.425988233089447
Epoch: 6264, Batch Gradient Norm: 10.984922191362488
Epoch: 6264, Batch Gradient Norm after: 10.984922191362488
Epoch 6265/10000, Prediction Accuracy = 62.42%, Loss = 0.4202691435813904
Epoch: 6265, Batch Gradient Norm: 9.165925181773684
Epoch: 6265, Batch Gradient Norm after: 9.165925181773684
Epoch 6266/10000, Prediction Accuracy = 62.438%, Loss = 0.407803475856781
Epoch: 6266, Batch Gradient Norm: 7.695610128294274
Epoch: 6266, Batch Gradient Norm after: 7.695610128294274
Epoch 6267/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.39951780438423157
Epoch: 6267, Batch Gradient Norm: 7.62961088796013
Epoch: 6267, Batch Gradient Norm after: 7.62961088796013
Epoch 6268/10000, Prediction Accuracy = 62.352%, Loss = 0.3991205632686615
Epoch: 6268, Batch Gradient Norm: 10.046194290202248
Epoch: 6268, Batch Gradient Norm after: 10.046194290202248
Epoch 6269/10000, Prediction Accuracy = 62.43000000000001%, Loss = 0.41468964219093324
Epoch: 6269, Batch Gradient Norm: 10.280515596825497
Epoch: 6269, Batch Gradient Norm after: 10.280515596825497
Epoch 6270/10000, Prediction Accuracy = 62.464%, Loss = 0.417271488904953
Epoch: 6270, Batch Gradient Norm: 10.15348992897617
Epoch: 6270, Batch Gradient Norm after: 10.15348992897617
Epoch 6271/10000, Prediction Accuracy = 62.46%, Loss = 0.41426470279693606
Epoch: 6271, Batch Gradient Norm: 11.046378689064435
Epoch: 6271, Batch Gradient Norm after: 11.046378689064435
Epoch 6272/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.419382119178772
Epoch: 6272, Batch Gradient Norm: 11.968342971544375
Epoch: 6272, Batch Gradient Norm after: 11.968342971544375
Epoch 6273/10000, Prediction Accuracy = 62.431999999999995%, Loss = 0.4286708116531372
Epoch: 6273, Batch Gradient Norm: 11.4529224715306
Epoch: 6273, Batch Gradient Norm after: 11.4529224715306
Epoch 6274/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.42419023513793946
Epoch: 6274, Batch Gradient Norm: 11.113499256566174
Epoch: 6274, Batch Gradient Norm after: 11.113499256566174
Epoch 6275/10000, Prediction Accuracy = 62.388%, Loss = 0.419819575548172
Epoch: 6275, Batch Gradient Norm: 10.96497049613543
Epoch: 6275, Batch Gradient Norm after: 10.96497049613543
Epoch 6276/10000, Prediction Accuracy = 62.524%, Loss = 0.4184807300567627
Epoch: 6276, Batch Gradient Norm: 9.975157564470383
Epoch: 6276, Batch Gradient Norm after: 9.975157564470383
Epoch 6277/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.41277827620506286
Epoch: 6277, Batch Gradient Norm: 9.221300814685222
Epoch: 6277, Batch Gradient Norm after: 9.221300814685222
Epoch 6278/10000, Prediction Accuracy = 62.572%, Loss = 0.4095006823539734
Epoch: 6278, Batch Gradient Norm: 8.025816270285198
Epoch: 6278, Batch Gradient Norm after: 8.025816270285198
Epoch 6279/10000, Prediction Accuracy = 62.532%, Loss = 0.4017286837100983
Epoch: 6279, Batch Gradient Norm: 9.303648213550355
Epoch: 6279, Batch Gradient Norm after: 9.303648213550355
Epoch 6280/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.4088951647281647
Epoch: 6280, Batch Gradient Norm: 9.328436704307833
Epoch: 6280, Batch Gradient Norm after: 9.328436704307833
Epoch 6281/10000, Prediction Accuracy = 62.412%, Loss = 0.4092594921588898
Epoch: 6281, Batch Gradient Norm: 8.63133656095414
Epoch: 6281, Batch Gradient Norm after: 8.63133656095414
Epoch 6282/10000, Prediction Accuracy = 62.49000000000001%, Loss = 0.40460320115089415
Epoch: 6282, Batch Gradient Norm: 8.887883097855187
Epoch: 6282, Batch Gradient Norm after: 8.887883097855187
Epoch 6283/10000, Prediction Accuracy = 62.45400000000001%, Loss = 0.40578795671463014
Epoch: 6283, Batch Gradient Norm: 10.796426004247735
Epoch: 6283, Batch Gradient Norm after: 10.796426004247735
Epoch 6284/10000, Prediction Accuracy = 62.251999999999995%, Loss = 0.41735607385635376
Epoch: 6284, Batch Gradient Norm: 12.552090020508931
Epoch: 6284, Batch Gradient Norm after: 12.552090020508931
Epoch 6285/10000, Prediction Accuracy = 62.402%, Loss = 0.4314976632595062
Epoch: 6285, Batch Gradient Norm: 10.669794274535654
Epoch: 6285, Batch Gradient Norm after: 10.669794274535654
Epoch 6286/10000, Prediction Accuracy = 62.410000000000004%, Loss = 0.4179094970226288
Epoch: 6286, Batch Gradient Norm: 9.754875593588347
Epoch: 6286, Batch Gradient Norm after: 9.754875593588347
Epoch 6287/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.4124045014381409
Epoch: 6287, Batch Gradient Norm: 9.705617736195737
Epoch: 6287, Batch Gradient Norm after: 9.705617736195737
Epoch 6288/10000, Prediction Accuracy = 62.334%, Loss = 0.4129077136516571
Epoch: 6288, Batch Gradient Norm: 9.591369614375212
Epoch: 6288, Batch Gradient Norm after: 9.591369614375212
Epoch 6289/10000, Prediction Accuracy = 62.468%, Loss = 0.4119464695453644
Epoch: 6289, Batch Gradient Norm: 10.360336252222307
Epoch: 6289, Batch Gradient Norm after: 10.360336252222307
Epoch 6290/10000, Prediction Accuracy = 62.416%, Loss = 0.415883332490921
Epoch: 6290, Batch Gradient Norm: 10.347706371250426
Epoch: 6290, Batch Gradient Norm after: 10.347706371250426
Epoch 6291/10000, Prediction Accuracy = 62.504%, Loss = 0.4157284200191498
Epoch: 6291, Batch Gradient Norm: 9.956761939314854
Epoch: 6291, Batch Gradient Norm after: 9.956761939314854
Epoch 6292/10000, Prediction Accuracy = 62.416%, Loss = 0.4132184863090515
Epoch: 6292, Batch Gradient Norm: 9.643652469906167
Epoch: 6292, Batch Gradient Norm after: 9.643652469906167
Epoch 6293/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.41074044704437257
Epoch: 6293, Batch Gradient Norm: 10.582024374014075
Epoch: 6293, Batch Gradient Norm after: 10.582024374014075
Epoch 6294/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.4158496856689453
Epoch: 6294, Batch Gradient Norm: 11.420542714039966
Epoch: 6294, Batch Gradient Norm after: 11.420542714039966
Epoch 6295/10000, Prediction Accuracy = 62.528%, Loss = 0.42144941687583926
Epoch: 6295, Batch Gradient Norm: 10.395987935766524
Epoch: 6295, Batch Gradient Norm after: 10.395987935766524
Epoch 6296/10000, Prediction Accuracy = 62.279999999999994%, Loss = 0.4153281509876251
Epoch: 6296, Batch Gradient Norm: 8.39916503870384
Epoch: 6296, Batch Gradient Norm after: 8.39916503870384
Epoch 6297/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.40270957350730896
Epoch: 6297, Batch Gradient Norm: 8.842434087692217
Epoch: 6297, Batch Gradient Norm after: 8.842434087692217
Epoch 6298/10000, Prediction Accuracy = 62.488%, Loss = 0.40502346158027647
Epoch: 6298, Batch Gradient Norm: 10.143020373958745
Epoch: 6298, Batch Gradient Norm after: 10.143020373958745
Epoch 6299/10000, Prediction Accuracy = 62.45%, Loss = 0.41339418292045593
Epoch: 6299, Batch Gradient Norm: 9.420394123402774
Epoch: 6299, Batch Gradient Norm after: 9.420394123402774
Epoch 6300/10000, Prediction Accuracy = 62.576%, Loss = 0.4086185932159424
Epoch: 6300, Batch Gradient Norm: 10.401693699240301
Epoch: 6300, Batch Gradient Norm after: 10.401693699240301
Epoch 6301/10000, Prediction Accuracy = 62.315999999999995%, Loss = 0.4149430453777313
Epoch: 6301, Batch Gradient Norm: 11.604117535235375
Epoch: 6301, Batch Gradient Norm after: 11.604117535235375
Epoch 6302/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.42686988711357116
Epoch: 6302, Batch Gradient Norm: 9.979784613907066
Epoch: 6302, Batch Gradient Norm after: 9.979784613907066
Epoch 6303/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.41403321623802186
Epoch: 6303, Batch Gradient Norm: 10.54031908443859
Epoch: 6303, Batch Gradient Norm after: 10.54031908443859
Epoch 6304/10000, Prediction Accuracy = 62.492%, Loss = 0.41477057337760925
Epoch: 6304, Batch Gradient Norm: 11.620753231342542
Epoch: 6304, Batch Gradient Norm after: 11.620753231342542
Epoch 6305/10000, Prediction Accuracy = 62.396%, Loss = 0.4236957609653473
Epoch: 6305, Batch Gradient Norm: 9.478904676085246
Epoch: 6305, Batch Gradient Norm after: 9.478904676085246
Epoch 6306/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.4087811768054962
Epoch: 6306, Batch Gradient Norm: 9.224469088495482
Epoch: 6306, Batch Gradient Norm after: 9.224469088495482
Epoch 6307/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.4061966359615326
Epoch: 6307, Batch Gradient Norm: 10.633955171305798
Epoch: 6307, Batch Gradient Norm after: 10.633955171305798
Epoch 6308/10000, Prediction Accuracy = 62.38000000000001%, Loss = 0.41637004017829893
Epoch: 6308, Batch Gradient Norm: 10.388360602271893
Epoch: 6308, Batch Gradient Norm after: 10.388360602271893
Epoch 6309/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.4169862627983093
Epoch: 6309, Batch Gradient Norm: 8.07600050149603
Epoch: 6309, Batch Gradient Norm after: 8.07600050149603
Epoch 6310/10000, Prediction Accuracy = 62.436%, Loss = 0.40227463841438293
Epoch: 6310, Batch Gradient Norm: 8.201348281785572
Epoch: 6310, Batch Gradient Norm after: 8.201348281785572
Epoch 6311/10000, Prediction Accuracy = 62.59000000000001%, Loss = 0.40229682326316835
Epoch: 6311, Batch Gradient Norm: 9.31612063445698
Epoch: 6311, Batch Gradient Norm after: 9.31612063445698
Epoch 6312/10000, Prediction Accuracy = 62.452%, Loss = 0.40880389213562013
Epoch: 6312, Batch Gradient Norm: 10.833429762021927
Epoch: 6312, Batch Gradient Norm after: 10.833429762021927
Epoch 6313/10000, Prediction Accuracy = 62.412%, Loss = 0.418631386756897
Epoch: 6313, Batch Gradient Norm: 11.85379363960563
Epoch: 6313, Batch Gradient Norm after: 11.85379363960563
Epoch 6314/10000, Prediction Accuracy = 62.358000000000004%, Loss = 0.42602484822273257
Epoch: 6314, Batch Gradient Norm: 12.09692377308021
Epoch: 6314, Batch Gradient Norm after: 12.09692377308021
Epoch 6315/10000, Prediction Accuracy = 62.395999999999994%, Loss = 0.4277742028236389
Epoch: 6315, Batch Gradient Norm: 10.668877847472107
Epoch: 6315, Batch Gradient Norm after: 10.668877847472107
Epoch 6316/10000, Prediction Accuracy = 62.374%, Loss = 0.41760783791542055
Epoch: 6316, Batch Gradient Norm: 8.388422302297164
Epoch: 6316, Batch Gradient Norm after: 8.388422302297164
Epoch 6317/10000, Prediction Accuracy = 62.468%, Loss = 0.4027810335159302
Epoch: 6317, Batch Gradient Norm: 8.388896428446756
Epoch: 6317, Batch Gradient Norm after: 8.388896428446756
Epoch 6318/10000, Prediction Accuracy = 62.448%, Loss = 0.40194726586341856
Epoch: 6318, Batch Gradient Norm: 10.488441247023118
Epoch: 6318, Batch Gradient Norm after: 10.488441247023118
Epoch 6319/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.41458899974823
Epoch: 6319, Batch Gradient Norm: 12.18795284769198
Epoch: 6319, Batch Gradient Norm after: 12.18795284769198
Epoch 6320/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4269787549972534
Epoch: 6320, Batch Gradient Norm: 10.643873461373106
Epoch: 6320, Batch Gradient Norm after: 10.643873461373106
Epoch 6321/10000, Prediction Accuracy = 62.462%, Loss = 0.4159445106983185
Epoch: 6321, Batch Gradient Norm: 8.754491585760128
Epoch: 6321, Batch Gradient Norm after: 8.754491585760128
Epoch 6322/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.40406265258789065
Epoch: 6322, Batch Gradient Norm: 8.684854958011911
Epoch: 6322, Batch Gradient Norm after: 8.684854958011911
Epoch 6323/10000, Prediction Accuracy = 62.52%, Loss = 0.40280386805534363
Epoch: 6323, Batch Gradient Norm: 11.502936968565399
Epoch: 6323, Batch Gradient Norm after: 11.502936968565399
Epoch 6324/10000, Prediction Accuracy = 62.476%, Loss = 0.42239006161689757
Epoch: 6324, Batch Gradient Norm: 10.478273272475295
Epoch: 6324, Batch Gradient Norm after: 10.478273272475295
Epoch 6325/10000, Prediction Accuracy = 62.467999999999996%, Loss = 0.4164455771446228
Epoch: 6325, Batch Gradient Norm: 7.921441378081965
Epoch: 6325, Batch Gradient Norm after: 7.921441378081965
Epoch 6326/10000, Prediction Accuracy = 62.524%, Loss = 0.4002914011478424
Epoch: 6326, Batch Gradient Norm: 9.231962241470328
Epoch: 6326, Batch Gradient Norm after: 9.231962241470328
Epoch 6327/10000, Prediction Accuracy = 62.61%, Loss = 0.4076110601425171
Epoch: 6327, Batch Gradient Norm: 10.44079813375673
Epoch: 6327, Batch Gradient Norm after: 10.44079813375673
Epoch 6328/10000, Prediction Accuracy = 62.364%, Loss = 0.41792553663253784
Epoch: 6328, Batch Gradient Norm: 9.066996860167087
Epoch: 6328, Batch Gradient Norm after: 9.066996860167087
Epoch 6329/10000, Prediction Accuracy = 62.576%, Loss = 0.4085672676563263
Epoch: 6329, Batch Gradient Norm: 11.359522044017014
Epoch: 6329, Batch Gradient Norm after: 11.359522044017014
Epoch 6330/10000, Prediction Accuracy = 62.459999999999994%, Loss = 0.42204853892326355
Epoch: 6330, Batch Gradient Norm: 11.476900749836954
Epoch: 6330, Batch Gradient Norm after: 11.476900749836954
Epoch 6331/10000, Prediction Accuracy = 62.524%, Loss = 0.423846697807312
Epoch: 6331, Batch Gradient Norm: 8.570042723146974
Epoch: 6331, Batch Gradient Norm after: 8.570042723146974
Epoch 6332/10000, Prediction Accuracy = 62.438%, Loss = 0.4026833951473236
Epoch: 6332, Batch Gradient Norm: 9.249699967022703
Epoch: 6332, Batch Gradient Norm after: 9.249699967022703
Epoch 6333/10000, Prediction Accuracy = 62.412%, Loss = 0.406478613615036
Epoch: 6333, Batch Gradient Norm: 10.514428594955566
Epoch: 6333, Batch Gradient Norm after: 10.514428594955566
Epoch 6334/10000, Prediction Accuracy = 62.424%, Loss = 0.41469310522079467
Epoch: 6334, Batch Gradient Norm: 11.285487141570721
Epoch: 6334, Batch Gradient Norm after: 11.285487141570721
Epoch 6335/10000, Prediction Accuracy = 62.512000000000015%, Loss = 0.41947548389434813
Epoch: 6335, Batch Gradient Norm: 12.191523268634294
Epoch: 6335, Batch Gradient Norm after: 12.191523268634294
Epoch 6336/10000, Prediction Accuracy = 62.416%, Loss = 0.4260390877723694
Epoch: 6336, Batch Gradient Norm: 11.342388185948435
Epoch: 6336, Batch Gradient Norm after: 11.342388185948435
Epoch 6337/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.4208590567111969
Epoch: 6337, Batch Gradient Norm: 9.352914709518632
Epoch: 6337, Batch Gradient Norm after: 9.352914709518632
Epoch 6338/10000, Prediction Accuracy = 62.42%, Loss = 0.4079221487045288
Epoch: 6338, Batch Gradient Norm: 8.439101823132136
Epoch: 6338, Batch Gradient Norm after: 8.439101823132136
Epoch 6339/10000, Prediction Accuracy = 62.33399999999999%, Loss = 0.4032923638820648
Epoch: 6339, Batch Gradient Norm: 7.895726051643199
Epoch: 6339, Batch Gradient Norm after: 7.895726051643199
Epoch 6340/10000, Prediction Accuracy = 62.462%, Loss = 0.40075728893280027
Epoch: 6340, Batch Gradient Norm: 8.504855670676346
Epoch: 6340, Batch Gradient Norm after: 8.504855670676346
Epoch 6341/10000, Prediction Accuracy = 62.562%, Loss = 0.40279130935668944
Epoch: 6341, Batch Gradient Norm: 11.292256600149573
Epoch: 6341, Batch Gradient Norm after: 11.292256600149573
Epoch 6342/10000, Prediction Accuracy = 62.374%, Loss = 0.4205062687397003
Epoch: 6342, Batch Gradient Norm: 11.839618429739428
Epoch: 6342, Batch Gradient Norm after: 11.839618429739428
Epoch 6343/10000, Prediction Accuracy = 62.702%, Loss = 0.4235015630722046
Epoch: 6343, Batch Gradient Norm: 11.470498664896308
Epoch: 6343, Batch Gradient Norm after: 11.470498664896308
Epoch 6344/10000, Prediction Accuracy = 62.34400000000001%, Loss = 0.42059603333473206
Epoch: 6344, Batch Gradient Norm: 9.315212923261083
Epoch: 6344, Batch Gradient Norm after: 9.315212923261083
Epoch 6345/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.40792703032493594
Epoch: 6345, Batch Gradient Norm: 7.581150740688601
Epoch: 6345, Batch Gradient Norm after: 7.581150740688601
Epoch 6346/10000, Prediction Accuracy = 62.484%, Loss = 0.3985332250595093
Epoch: 6346, Batch Gradient Norm: 8.66780195578122
Epoch: 6346, Batch Gradient Norm after: 8.66780195578122
Epoch 6347/10000, Prediction Accuracy = 62.516%, Loss = 0.4039677262306213
Epoch: 6347, Batch Gradient Norm: 10.274281781114702
Epoch: 6347, Batch Gradient Norm after: 10.274281781114702
Epoch 6348/10000, Prediction Accuracy = 62.524%, Loss = 0.4137912213802338
Epoch: 6348, Batch Gradient Norm: 13.588414436278578
Epoch: 6348, Batch Gradient Norm after: 13.588414436278578
Epoch 6349/10000, Prediction Accuracy = 62.36600000000001%, Loss = 0.43960840106010435
Epoch: 6349, Batch Gradient Norm: 11.782999069487994
Epoch: 6349, Batch Gradient Norm after: 11.782999069487994
Epoch 6350/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.42599703669548034
Epoch: 6350, Batch Gradient Norm: 8.739446561813766
Epoch: 6350, Batch Gradient Norm after: 8.739446561813766
Epoch 6351/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.4043704330921173
Epoch: 6351, Batch Gradient Norm: 8.64579166985603
Epoch: 6351, Batch Gradient Norm after: 8.64579166985603
Epoch 6352/10000, Prediction Accuracy = 62.488%, Loss = 0.4048313081264496
Epoch: 6352, Batch Gradient Norm: 8.255421755915664
Epoch: 6352, Batch Gradient Norm after: 8.255421755915664
Epoch 6353/10000, Prediction Accuracy = 62.44199999999999%, Loss = 0.4022553026676178
Epoch: 6353, Batch Gradient Norm: 8.057354700356505
Epoch: 6353, Batch Gradient Norm after: 8.057354700356505
Epoch 6354/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4003149688243866
Epoch: 6354, Batch Gradient Norm: 8.571106199578619
Epoch: 6354, Batch Gradient Norm after: 8.571106199578619
Epoch 6355/10000, Prediction Accuracy = 62.452%, Loss = 0.4015936553478241
Epoch: 6355, Batch Gradient Norm: 9.914958081931218
Epoch: 6355, Batch Gradient Norm after: 9.914958081931218
Epoch 6356/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.4091831386089325
Epoch: 6356, Batch Gradient Norm: 11.600023283568412
Epoch: 6356, Batch Gradient Norm after: 11.600023283568412
Epoch 6357/10000, Prediction Accuracy = 62.508%, Loss = 0.4206761121749878
Epoch: 6357, Batch Gradient Norm: 11.103614302035892
Epoch: 6357, Batch Gradient Norm after: 11.103614302035892
Epoch 6358/10000, Prediction Accuracy = 62.528%, Loss = 0.41748127341270447
Epoch: 6358, Batch Gradient Norm: 8.967694776849164
Epoch: 6358, Batch Gradient Norm after: 8.967694776849164
Epoch 6359/10000, Prediction Accuracy = 62.556000000000004%, Loss = 0.40400588512420654
Epoch: 6359, Batch Gradient Norm: 10.372063442274634
Epoch: 6359, Batch Gradient Norm after: 10.372063442274634
Epoch 6360/10000, Prediction Accuracy = 62.538%, Loss = 0.41319029927253725
Epoch: 6360, Batch Gradient Norm: 13.45071303663572
Epoch: 6360, Batch Gradient Norm after: 13.45071303663572
Epoch 6361/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.43983911275863646
Epoch: 6361, Batch Gradient Norm: 11.827554709606128
Epoch: 6361, Batch Gradient Norm after: 11.827554709606128
Epoch 6362/10000, Prediction Accuracy = 62.538%, Loss = 0.4265625774860382
Epoch: 6362, Batch Gradient Norm: 9.96463130115378
Epoch: 6362, Batch Gradient Norm after: 9.96463130115378
Epoch 6363/10000, Prediction Accuracy = 62.556000000000004%, Loss = 0.41307719945907595
Epoch: 6363, Batch Gradient Norm: 8.578299682678015
Epoch: 6363, Batch Gradient Norm after: 8.578299682678015
Epoch 6364/10000, Prediction Accuracy = 62.428%, Loss = 0.4027883291244507
Epoch: 6364, Batch Gradient Norm: 9.505831487251886
Epoch: 6364, Batch Gradient Norm after: 9.505831487251886
Epoch 6365/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.40776904225349425
Epoch: 6365, Batch Gradient Norm: 10.056767779369999
Epoch: 6365, Batch Gradient Norm after: 10.056767779369999
Epoch 6366/10000, Prediction Accuracy = 62.495999999999995%, Loss = 0.4110065817832947
Epoch: 6366, Batch Gradient Norm: 10.397388198137977
Epoch: 6366, Batch Gradient Norm after: 10.397388198137977
Epoch 6367/10000, Prediction Accuracy = 62.45%, Loss = 0.41318029165267944
Epoch: 6367, Batch Gradient Norm: 10.111400713166457
Epoch: 6367, Batch Gradient Norm after: 10.111400713166457
Epoch 6368/10000, Prediction Accuracy = 62.432%, Loss = 0.41175689697265627
Epoch: 6368, Batch Gradient Norm: 9.654137121516413
Epoch: 6368, Batch Gradient Norm after: 9.654137121516413
Epoch 6369/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4091244041919708
Epoch: 6369, Batch Gradient Norm: 10.04359508751208
Epoch: 6369, Batch Gradient Norm after: 10.04359508751208
Epoch 6370/10000, Prediction Accuracy = 62.501999999999995%, Loss = 0.4119315207004547
Epoch: 6370, Batch Gradient Norm: 9.400331908782324
Epoch: 6370, Batch Gradient Norm after: 9.400331908782324
Epoch 6371/10000, Prediction Accuracy = 62.598%, Loss = 0.4086061716079712
Epoch: 6371, Batch Gradient Norm: 9.659430395188412
Epoch: 6371, Batch Gradient Norm after: 9.659430395188412
Epoch 6372/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.40938361883163454
Epoch: 6372, Batch Gradient Norm: 11.448962380649949
Epoch: 6372, Batch Gradient Norm after: 11.448962380649949
Epoch 6373/10000, Prediction Accuracy = 62.576%, Loss = 0.42271820902824403
Epoch: 6373, Batch Gradient Norm: 10.036220576890438
Epoch: 6373, Batch Gradient Norm after: 10.036220576890438
Epoch 6374/10000, Prediction Accuracy = 62.33200000000001%, Loss = 0.4122235536575317
Epoch: 6374, Batch Gradient Norm: 8.486721807583848
Epoch: 6374, Batch Gradient Norm after: 8.486721807583848
Epoch 6375/10000, Prediction Accuracy = 62.674%, Loss = 0.40265711545944216
Epoch: 6375, Batch Gradient Norm: 9.189917759366
Epoch: 6375, Batch Gradient Norm after: 9.189917759366
Epoch 6376/10000, Prediction Accuracy = 62.484%, Loss = 0.4079279720783234
Epoch: 6376, Batch Gradient Norm: 9.558500225945787
Epoch: 6376, Batch Gradient Norm after: 9.558500225945787
Epoch 6377/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.40878915786743164
Epoch: 6377, Batch Gradient Norm: 11.549843510844719
Epoch: 6377, Batch Gradient Norm after: 11.549843510844719
Epoch 6378/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4204264223575592
Epoch: 6378, Batch Gradient Norm: 12.293109529783868
Epoch: 6378, Batch Gradient Norm after: 12.293109529783868
Epoch 6379/10000, Prediction Accuracy = 62.622%, Loss = 0.4262064516544342
Epoch: 6379, Batch Gradient Norm: 10.041592967867777
Epoch: 6379, Batch Gradient Norm after: 10.041592967867777
Epoch 6380/10000, Prediction Accuracy = 62.443999999999996%, Loss = 0.41046631932258604
Epoch: 6380, Batch Gradient Norm: 8.97001673839293
Epoch: 6380, Batch Gradient Norm after: 8.97001673839293
Epoch 6381/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.4040193736553192
Epoch: 6381, Batch Gradient Norm: 9.460183101445617
Epoch: 6381, Batch Gradient Norm after: 9.460183101445617
Epoch 6382/10000, Prediction Accuracy = 62.462%, Loss = 0.4082421541213989
Epoch: 6382, Batch Gradient Norm: 9.944203827458805
Epoch: 6382, Batch Gradient Norm after: 9.944203827458805
Epoch 6383/10000, Prediction Accuracy = 62.512%, Loss = 0.4121778249740601
Epoch: 6383, Batch Gradient Norm: 8.885627137496497
Epoch: 6383, Batch Gradient Norm after: 8.885627137496497
Epoch 6384/10000, Prediction Accuracy = 62.584%, Loss = 0.4039527416229248
Epoch: 6384, Batch Gradient Norm: 10.313574080166777
Epoch: 6384, Batch Gradient Norm after: 10.313574080166777
Epoch 6385/10000, Prediction Accuracy = 62.39200000000001%, Loss = 0.41269652247428895
Epoch: 6385, Batch Gradient Norm: 11.14907014433318
Epoch: 6385, Batch Gradient Norm after: 11.14907014433318
Epoch 6386/10000, Prediction Accuracy = 62.468%, Loss = 0.4197697103023529
Epoch: 6386, Batch Gradient Norm: 10.180450490912923
Epoch: 6386, Batch Gradient Norm after: 10.180450490912923
Epoch 6387/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.4141538619995117
Epoch: 6387, Batch Gradient Norm: 9.403599158106651
Epoch: 6387, Batch Gradient Norm after: 9.403599158106651
Epoch 6388/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.40718351006507875
Epoch: 6388, Batch Gradient Norm: 11.837982467948274
Epoch: 6388, Batch Gradient Norm after: 11.837982467948274
Epoch 6389/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.42253466248512267
Epoch: 6389, Batch Gradient Norm: 12.126162184442293
Epoch: 6389, Batch Gradient Norm after: 12.126162184442293
Epoch 6390/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.42524718642234804
Epoch: 6390, Batch Gradient Norm: 9.608583358718915
Epoch: 6390, Batch Gradient Norm after: 9.608583358718915
Epoch 6391/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.40754063725471495
Epoch: 6391, Batch Gradient Norm: 9.495340095261348
Epoch: 6391, Batch Gradient Norm after: 9.495340095261348
Epoch 6392/10000, Prediction Accuracy = 62.462%, Loss = 0.4069424390792847
Epoch: 6392, Batch Gradient Norm: 9.683330412113499
Epoch: 6392, Batch Gradient Norm after: 9.683330412113499
Epoch 6393/10000, Prediction Accuracy = 62.41600000000001%, Loss = 0.40781173706054685
Epoch: 6393, Batch Gradient Norm: 10.020858305624953
Epoch: 6393, Batch Gradient Norm after: 10.020858305624953
Epoch 6394/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.410439133644104
Epoch: 6394, Batch Gradient Norm: 9.757639730925582
Epoch: 6394, Batch Gradient Norm after: 9.757639730925582
Epoch 6395/10000, Prediction Accuracy = 62.54200000000001%, Loss = 0.4093266069889069
Epoch: 6395, Batch Gradient Norm: 9.997649269250607
Epoch: 6395, Batch Gradient Norm after: 9.997649269250607
Epoch 6396/10000, Prediction Accuracy = 62.604%, Loss = 0.41272525787353515
Epoch: 6396, Batch Gradient Norm: 9.662260691577755
Epoch: 6396, Batch Gradient Norm after: 9.662260691577755
Epoch 6397/10000, Prediction Accuracy = 62.5%, Loss = 0.4105976581573486
Epoch: 6397, Batch Gradient Norm: 9.689197454889227
Epoch: 6397, Batch Gradient Norm after: 9.689197454889227
Epoch 6398/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.41014873385429385
Epoch: 6398, Batch Gradient Norm: 9.863594069583645
Epoch: 6398, Batch Gradient Norm after: 9.863594069583645
Epoch 6399/10000, Prediction Accuracy = 62.476%, Loss = 0.4096710205078125
Epoch: 6399, Batch Gradient Norm: 9.815162951960929
Epoch: 6399, Batch Gradient Norm after: 9.815162951960929
Epoch 6400/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.40898030400276186
Epoch: 6400, Batch Gradient Norm: 8.969354973837698
Epoch: 6400, Batch Gradient Norm after: 8.969354973837698
Epoch 6401/10000, Prediction Accuracy = 62.38000000000001%, Loss = 0.40417435169219973
Epoch: 6401, Batch Gradient Norm: 8.120174721994479
Epoch: 6401, Batch Gradient Norm after: 8.120174721994479
Epoch 6402/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.3989437401294708
Epoch: 6402, Batch Gradient Norm: 10.037447013593349
Epoch: 6402, Batch Gradient Norm after: 10.037447013593349
Epoch 6403/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.410638165473938
Epoch: 6403, Batch Gradient Norm: 10.52348468338149
Epoch: 6403, Batch Gradient Norm after: 10.52348468338149
Epoch 6404/10000, Prediction Accuracy = 62.556%, Loss = 0.4153399407863617
Epoch: 6404, Batch Gradient Norm: 9.053260184338113
Epoch: 6404, Batch Gradient Norm after: 9.053260184338113
Epoch 6405/10000, Prediction Accuracy = 62.379999999999995%, Loss = 0.4060603082180023
Epoch: 6405, Batch Gradient Norm: 7.914716867962173
Epoch: 6405, Batch Gradient Norm after: 7.914716867962173
Epoch 6406/10000, Prediction Accuracy = 62.476%, Loss = 0.3994336247444153
Epoch: 6406, Batch Gradient Norm: 9.328731863796454
Epoch: 6406, Batch Gradient Norm after: 9.328731863796454
Epoch 6407/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.4073571920394897
Epoch: 6407, Batch Gradient Norm: 11.847923207183156
Epoch: 6407, Batch Gradient Norm after: 11.847923207183156
Epoch 6408/10000, Prediction Accuracy = 62.418000000000006%, Loss = 0.42464812397956847
Epoch: 6408, Batch Gradient Norm: 12.21951608888324
Epoch: 6408, Batch Gradient Norm after: 12.21951608888324
Epoch 6409/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4256059527397156
Epoch: 6409, Batch Gradient Norm: 12.019491455256565
Epoch: 6409, Batch Gradient Norm after: 12.019491455256565
Epoch 6410/10000, Prediction Accuracy = 62.42999999999999%, Loss = 0.4234947025775909
Epoch: 6410, Batch Gradient Norm: 12.234933090748047
Epoch: 6410, Batch Gradient Norm after: 12.234933090748047
Epoch 6411/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.4285305380821228
Epoch: 6411, Batch Gradient Norm: 8.941218838791695
Epoch: 6411, Batch Gradient Norm after: 8.941218838791695
Epoch 6412/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.4052827477455139
Epoch: 6412, Batch Gradient Norm: 10.179632444708718
Epoch: 6412, Batch Gradient Norm after: 10.179632444708718
Epoch 6413/10000, Prediction Accuracy = 62.294%, Loss = 0.41033963561058046
Epoch: 6413, Batch Gradient Norm: 10.832765238059814
Epoch: 6413, Batch Gradient Norm after: 10.832765238059814
Epoch 6414/10000, Prediction Accuracy = 62.69%, Loss = 0.41576220393180846
Epoch: 6414, Batch Gradient Norm: 9.076240473174913
Epoch: 6414, Batch Gradient Norm after: 9.076240473174913
Epoch 6415/10000, Prediction Accuracy = 62.532%, Loss = 0.4041605532169342
Epoch: 6415, Batch Gradient Norm: 9.428336497733689
Epoch: 6415, Batch Gradient Norm after: 9.428336497733689
Epoch 6416/10000, Prediction Accuracy = 62.519999999999996%, Loss = 0.4073784828186035
Epoch: 6416, Batch Gradient Norm: 9.586221744976024
Epoch: 6416, Batch Gradient Norm after: 9.586221744976024
Epoch 6417/10000, Prediction Accuracy = 62.476%, Loss = 0.4094914376735687
Epoch: 6417, Batch Gradient Norm: 8.934594774213364
Epoch: 6417, Batch Gradient Norm after: 8.934594774213364
Epoch 6418/10000, Prediction Accuracy = 62.412%, Loss = 0.4040407598018646
Epoch: 6418, Batch Gradient Norm: 9.508113063177728
Epoch: 6418, Batch Gradient Norm after: 9.508113063177728
Epoch 6419/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4081989943981171
Epoch: 6419, Batch Gradient Norm: 10.497893164867778
Epoch: 6419, Batch Gradient Norm after: 10.497893164867778
Epoch 6420/10000, Prediction Accuracy = 62.394000000000005%, Loss = 0.41498903632164
Epoch: 6420, Batch Gradient Norm: 11.787814928983234
Epoch: 6420, Batch Gradient Norm after: 11.787814928983234
Epoch 6421/10000, Prediction Accuracy = 62.376%, Loss = 0.42271392941474917
Epoch: 6421, Batch Gradient Norm: 11.989244447914908
Epoch: 6421, Batch Gradient Norm after: 11.989244447914908
Epoch 6422/10000, Prediction Accuracy = 62.476%, Loss = 0.42329648733139036
Epoch: 6422, Batch Gradient Norm: 10.76280199217152
Epoch: 6422, Batch Gradient Norm after: 10.76280199217152
Epoch 6423/10000, Prediction Accuracy = 62.45%, Loss = 0.4142965257167816
Epoch: 6423, Batch Gradient Norm: 10.49287139523425
Epoch: 6423, Batch Gradient Norm after: 10.49287139523425
Epoch 6424/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.41301671266555784
Epoch: 6424, Batch Gradient Norm: 10.114702068346062
Epoch: 6424, Batch Gradient Norm after: 10.114702068346062
Epoch 6425/10000, Prediction Accuracy = 62.49000000000001%, Loss = 0.4112155377864838
Epoch: 6425, Batch Gradient Norm: 8.90895056225416
Epoch: 6425, Batch Gradient Norm after: 8.90895056225416
Epoch 6426/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.40337855219841
Epoch: 6426, Batch Gradient Norm: 8.976787245048238
Epoch: 6426, Batch Gradient Norm after: 8.976787245048238
Epoch 6427/10000, Prediction Accuracy = 62.496%, Loss = 0.4036892831325531
Epoch: 6427, Batch Gradient Norm: 9.534143759737969
Epoch: 6427, Batch Gradient Norm after: 9.534143759737969
Epoch 6428/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.4069544017314911
Epoch: 6428, Batch Gradient Norm: 10.188367713363744
Epoch: 6428, Batch Gradient Norm after: 10.188367713363744
Epoch 6429/10000, Prediction Accuracy = 62.492%, Loss = 0.41199183464050293
Epoch: 6429, Batch Gradient Norm: 9.116105733422069
Epoch: 6429, Batch Gradient Norm after: 9.116105733422069
Epoch 6430/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.40451377630233765
Epoch: 6430, Batch Gradient Norm: 9.088101535261082
Epoch: 6430, Batch Gradient Norm after: 9.088101535261082
Epoch 6431/10000, Prediction Accuracy = 62.544000000000004%, Loss = 0.40402390956878664
Epoch: 6431, Batch Gradient Norm: 9.36402893779045
Epoch: 6431, Batch Gradient Norm after: 9.36402893779045
Epoch 6432/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.4057052195072174
Epoch: 6432, Batch Gradient Norm: 10.95214541115054
Epoch: 6432, Batch Gradient Norm after: 10.95214541115054
Epoch 6433/10000, Prediction Accuracy = 62.48199999999999%, Loss = 0.4161581516265869
Epoch: 6433, Batch Gradient Norm: 11.343332949359626
Epoch: 6433, Batch Gradient Norm after: 11.343332949359626
Epoch 6434/10000, Prediction Accuracy = 62.432%, Loss = 0.41947892904281614
Epoch: 6434, Batch Gradient Norm: 10.540841980856943
Epoch: 6434, Batch Gradient Norm after: 10.540841980856943
Epoch 6435/10000, Prediction Accuracy = 62.476%, Loss = 0.41369758248329164
Epoch: 6435, Batch Gradient Norm: 8.969460801762068
Epoch: 6435, Batch Gradient Norm after: 8.969460801762068
Epoch 6436/10000, Prediction Accuracy = 62.50600000000001%, Loss = 0.402883505821228
Epoch: 6436, Batch Gradient Norm: 9.017918078212487
Epoch: 6436, Batch Gradient Norm after: 9.017918078212487
Epoch 6437/10000, Prediction Accuracy = 62.512%, Loss = 0.40283498764038084
Epoch: 6437, Batch Gradient Norm: 10.50787860931543
Epoch: 6437, Batch Gradient Norm after: 10.50787860931543
Epoch 6438/10000, Prediction Accuracy = 62.39%, Loss = 0.4124825239181519
Epoch: 6438, Batch Gradient Norm: 12.677136037711431
Epoch: 6438, Batch Gradient Norm after: 12.677136037711431
Epoch 6439/10000, Prediction Accuracy = 62.35%, Loss = 0.43037971258163454
Epoch: 6439, Batch Gradient Norm: 11.130591293166324
Epoch: 6439, Batch Gradient Norm after: 11.130591293166324
Epoch 6440/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.4188714802265167
Epoch: 6440, Batch Gradient Norm: 8.19289084856289
Epoch: 6440, Batch Gradient Norm after: 8.19289084856289
Epoch 6441/10000, Prediction Accuracy = 62.576%, Loss = 0.3984354853630066
Epoch: 6441, Batch Gradient Norm: 8.703005125470844
Epoch: 6441, Batch Gradient Norm after: 8.703005125470844
Epoch 6442/10000, Prediction Accuracy = 62.574%, Loss = 0.40066962838172915
Epoch: 6442, Batch Gradient Norm: 10.552488291280524
Epoch: 6442, Batch Gradient Norm after: 10.552488291280524
Epoch 6443/10000, Prediction Accuracy = 62.41799999999999%, Loss = 0.41337518095970155
Epoch: 6443, Batch Gradient Norm: 10.167368137851259
Epoch: 6443, Batch Gradient Norm after: 10.167368137851259
Epoch 6444/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.41158133149147036
Epoch: 6444, Batch Gradient Norm: 9.608655442863919
Epoch: 6444, Batch Gradient Norm after: 9.608655442863919
Epoch 6445/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.40647987723350526
Epoch: 6445, Batch Gradient Norm: 11.341993089704474
Epoch: 6445, Batch Gradient Norm after: 11.341993089704474
Epoch 6446/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.4172755002975464
Epoch: 6446, Batch Gradient Norm: 11.51469808008931
Epoch: 6446, Batch Gradient Norm after: 11.51469808008931
Epoch 6447/10000, Prediction Accuracy = 62.424%, Loss = 0.42047693729400637
Epoch: 6447, Batch Gradient Norm: 9.449955785651385
Epoch: 6447, Batch Gradient Norm after: 9.449955785651385
Epoch 6448/10000, Prediction Accuracy = 62.58%, Loss = 0.40829128623008726
Epoch: 6448, Batch Gradient Norm: 7.6917516535684145
Epoch: 6448, Batch Gradient Norm after: 7.6917516535684145
Epoch 6449/10000, Prediction Accuracy = 62.592%, Loss = 0.3972538113594055
Epoch: 6449, Batch Gradient Norm: 9.578254653046205
Epoch: 6449, Batch Gradient Norm after: 9.578254653046205
Epoch 6450/10000, Prediction Accuracy = 62.495999999999995%, Loss = 0.40769012570381163
Epoch: 6450, Batch Gradient Norm: 11.207930595777954
Epoch: 6450, Batch Gradient Norm after: 11.207930595777954
Epoch 6451/10000, Prediction Accuracy = 62.556%, Loss = 0.4189884603023529
Epoch: 6451, Batch Gradient Norm: 10.778529044515032
Epoch: 6451, Batch Gradient Norm after: 10.778529044515032
Epoch 6452/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.41591955423355104
Epoch: 6452, Batch Gradient Norm: 9.490235348027104
Epoch: 6452, Batch Gradient Norm after: 9.490235348027104
Epoch 6453/10000, Prediction Accuracy = 62.564%, Loss = 0.40752007961273196
Epoch: 6453, Batch Gradient Norm: 8.990805217970006
Epoch: 6453, Batch Gradient Norm after: 8.990805217970006
Epoch 6454/10000, Prediction Accuracy = 62.588%, Loss = 0.40317410230636597
Epoch: 6454, Batch Gradient Norm: 11.166635771174796
Epoch: 6454, Batch Gradient Norm after: 11.166635771174796
Epoch 6455/10000, Prediction Accuracy = 62.658%, Loss = 0.41772048473358153
Epoch: 6455, Batch Gradient Norm: 11.520033176304162
Epoch: 6455, Batch Gradient Norm after: 11.520033176304162
Epoch 6456/10000, Prediction Accuracy = 62.508%, Loss = 0.4213106632232666
Epoch: 6456, Batch Gradient Norm: 9.191804733400398
Epoch: 6456, Batch Gradient Norm after: 9.191804733400398
Epoch 6457/10000, Prediction Accuracy = 62.562%, Loss = 0.4056727349758148
Epoch: 6457, Batch Gradient Norm: 8.265593674769844
Epoch: 6457, Batch Gradient Norm after: 8.265593674769844
Epoch 6458/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.40005154609680177
Epoch: 6458, Batch Gradient Norm: 8.650238518292305
Epoch: 6458, Batch Gradient Norm after: 8.650238518292305
Epoch 6459/10000, Prediction Accuracy = 62.572%, Loss = 0.4019068360328674
Epoch: 6459, Batch Gradient Norm: 9.597884478535127
Epoch: 6459, Batch Gradient Norm after: 9.597884478535127
Epoch 6460/10000, Prediction Accuracy = 62.55400000000001%, Loss = 0.40684473514556885
Epoch: 6460, Batch Gradient Norm: 9.78472313984329
Epoch: 6460, Batch Gradient Norm after: 9.78472313984329
Epoch 6461/10000, Prediction Accuracy = 62.322%, Loss = 0.4078818619251251
Epoch: 6461, Batch Gradient Norm: 10.006439424389882
Epoch: 6461, Batch Gradient Norm after: 10.006439424389882
Epoch 6462/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.40913488864898684
Epoch: 6462, Batch Gradient Norm: 12.72150367185736
Epoch: 6462, Batch Gradient Norm after: 12.72150367185736
Epoch 6463/10000, Prediction Accuracy = 62.38399999999999%, Loss = 0.4284634470939636
Epoch: 6463, Batch Gradient Norm: 13.120481914239468
Epoch: 6463, Batch Gradient Norm after: 13.120481914239468
Epoch 6464/10000, Prediction Accuracy = 62.612%, Loss = 0.43238967657089233
Epoch: 6464, Batch Gradient Norm: 9.96425519200269
Epoch: 6464, Batch Gradient Norm after: 9.96425519200269
Epoch 6465/10000, Prediction Accuracy = 62.496%, Loss = 0.40933257937431333
Epoch: 6465, Batch Gradient Norm: 8.648315904137656
Epoch: 6465, Batch Gradient Norm after: 8.648315904137656
Epoch 6466/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.4010692656040192
Epoch: 6466, Batch Gradient Norm: 8.92787512617775
Epoch: 6466, Batch Gradient Norm after: 8.92787512617775
Epoch 6467/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.40215268135070803
Epoch: 6467, Batch Gradient Norm: 10.63020080806462
Epoch: 6467, Batch Gradient Norm after: 10.63020080806462
Epoch 6468/10000, Prediction Accuracy = 62.504%, Loss = 0.41273272037506104
Epoch: 6468, Batch Gradient Norm: 12.03397582570993
Epoch: 6468, Batch Gradient Norm after: 12.03397582570993
Epoch 6469/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.42420837879180906
Epoch: 6469, Batch Gradient Norm: 10.101631821021753
Epoch: 6469, Batch Gradient Norm after: 10.101631821021753
Epoch 6470/10000, Prediction Accuracy = 62.43399999999999%, Loss = 0.4107067704200745
Epoch: 6470, Batch Gradient Norm: 8.504267384905347
Epoch: 6470, Batch Gradient Norm after: 8.504267384905347
Epoch 6471/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.4001713037490845
Epoch: 6471, Batch Gradient Norm: 8.20300417577924
Epoch: 6471, Batch Gradient Norm after: 8.20300417577924
Epoch 6472/10000, Prediction Accuracy = 62.61%, Loss = 0.3984915196895599
Epoch: 6472, Batch Gradient Norm: 8.779882073363636
Epoch: 6472, Batch Gradient Norm after: 8.779882073363636
Epoch 6473/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.40195035338401797
Epoch: 6473, Batch Gradient Norm: 11.206552453674954
Epoch: 6473, Batch Gradient Norm after: 11.206552453674954
Epoch 6474/10000, Prediction Accuracy = 62.516%, Loss = 0.41667535305023196
Epoch: 6474, Batch Gradient Norm: 11.830715844772492
Epoch: 6474, Batch Gradient Norm after: 11.830715844772492
Epoch 6475/10000, Prediction Accuracy = 62.5%, Loss = 0.42269524931907654
Epoch: 6475, Batch Gradient Norm: 8.948376843271667
Epoch: 6475, Batch Gradient Norm after: 8.948376843271667
Epoch 6476/10000, Prediction Accuracy = 62.64%, Loss = 0.40387375354766847
Epoch: 6476, Batch Gradient Norm: 7.493417288398159
Epoch: 6476, Batch Gradient Norm after: 7.493417288398159
Epoch 6477/10000, Prediction Accuracy = 62.574%, Loss = 0.3952990710735321
Epoch: 6477, Batch Gradient Norm: 8.599742905417509
Epoch: 6477, Batch Gradient Norm after: 8.599742905417509
Epoch 6478/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.4019114255905151
Epoch: 6478, Batch Gradient Norm: 10.445051120219688
Epoch: 6478, Batch Gradient Norm after: 10.445051120219688
Epoch 6479/10000, Prediction Accuracy = 62.232000000000006%, Loss = 0.41296780705451963
Epoch: 6479, Batch Gradient Norm: 11.657458755861457
Epoch: 6479, Batch Gradient Norm after: 11.657458755861457
Epoch 6480/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.42035371661186216
Epoch: 6480, Batch Gradient Norm: 12.152500887896755
Epoch: 6480, Batch Gradient Norm after: 12.152500887896755
Epoch 6481/10000, Prediction Accuracy = 62.564%, Loss = 0.423871648311615
Epoch: 6481, Batch Gradient Norm: 8.775432056859842
Epoch: 6481, Batch Gradient Norm after: 8.775432056859842
Epoch 6482/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.40151435136795044
Epoch: 6482, Batch Gradient Norm: 7.00149134306283
Epoch: 6482, Batch Gradient Norm after: 7.00149134306283
Epoch 6483/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3918111801147461
Epoch: 6483, Batch Gradient Norm: 7.103912538552728
Epoch: 6483, Batch Gradient Norm after: 7.103912538552728
Epoch 6484/10000, Prediction Accuracy = 62.574%, Loss = 0.3921019434928894
Epoch: 6484, Batch Gradient Norm: 9.632467627095883
Epoch: 6484, Batch Gradient Norm after: 9.632467627095883
Epoch 6485/10000, Prediction Accuracy = 62.524%, Loss = 0.40585575699806214
Epoch: 6485, Batch Gradient Norm: 13.381342840939412
Epoch: 6485, Batch Gradient Norm after: 13.381342840939412
Epoch 6486/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.43570915460586546
Epoch: 6486, Batch Gradient Norm: 10.769500839294576
Epoch: 6486, Batch Gradient Norm after: 10.769500839294576
Epoch 6487/10000, Prediction Accuracy = 62.507999999999996%, Loss = 0.41618911623954774
Epoch: 6487, Batch Gradient Norm: 7.986410964971112
Epoch: 6487, Batch Gradient Norm after: 7.986410964971112
Epoch 6488/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.3969444394111633
Epoch: 6488, Batch Gradient Norm: 11.080253728551947
Epoch: 6488, Batch Gradient Norm after: 11.080253728551947
Epoch 6489/10000, Prediction Accuracy = 62.562%, Loss = 0.41782172918319704
Epoch: 6489, Batch Gradient Norm: 10.962051630187574
Epoch: 6489, Batch Gradient Norm after: 10.962051630187574
Epoch 6490/10000, Prediction Accuracy = 62.448%, Loss = 0.419094181060791
Epoch: 6490, Batch Gradient Norm: 9.176772847324983
Epoch: 6490, Batch Gradient Norm after: 9.176772847324983
Epoch 6491/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.40412128567695615
Epoch: 6491, Batch Gradient Norm: 10.780994863317169
Epoch: 6491, Batch Gradient Norm after: 10.780994863317169
Epoch 6492/10000, Prediction Accuracy = 62.634%, Loss = 0.4138419806957245
Epoch: 6492, Batch Gradient Norm: 11.64976488686979
Epoch: 6492, Batch Gradient Norm after: 11.64976488686979
Epoch 6493/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.4198678433895111
Epoch: 6493, Batch Gradient Norm: 10.176833265766415
Epoch: 6493, Batch Gradient Norm after: 10.176833265766415
Epoch 6494/10000, Prediction Accuracy = 62.586%, Loss = 0.4087775945663452
Epoch: 6494, Batch Gradient Norm: 11.21035011960478
Epoch: 6494, Batch Gradient Norm after: 11.21035011960478
Epoch 6495/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.4149888396263123
Epoch: 6495, Batch Gradient Norm: 13.466684178140136
Epoch: 6495, Batch Gradient Norm after: 13.466684178140136
Epoch 6496/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.43561992049217224
Epoch: 6496, Batch Gradient Norm: 10.237507420609814
Epoch: 6496, Batch Gradient Norm after: 10.237507420609814
Epoch 6497/10000, Prediction Accuracy = 62.43599999999999%, Loss = 0.41094207763671875
Epoch: 6497, Batch Gradient Norm: 8.376427248507564
Epoch: 6497, Batch Gradient Norm after: 8.376427248507564
Epoch 6498/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.3993872404098511
Epoch: 6498, Batch Gradient Norm: 8.458204302220473
Epoch: 6498, Batch Gradient Norm after: 8.458204302220473
Epoch 6499/10000, Prediction Accuracy = 62.562%, Loss = 0.40056183338165285
Epoch: 6499, Batch Gradient Norm: 8.167391244094139
Epoch: 6499, Batch Gradient Norm after: 8.167391244094139
Epoch 6500/10000, Prediction Accuracy = 62.55%, Loss = 0.39923474192619324
Epoch: 6500, Batch Gradient Norm: 8.06839358900983
Epoch: 6500, Batch Gradient Norm after: 8.06839358900983
Epoch 6501/10000, Prediction Accuracy = 62.488%, Loss = 0.39844039678573606
Epoch: 6501, Batch Gradient Norm: 8.600622339446806
Epoch: 6501, Batch Gradient Norm after: 8.600622339446806
Epoch 6502/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.40104624032974245
Epoch: 6502, Batch Gradient Norm: 9.866266459978036
Epoch: 6502, Batch Gradient Norm after: 9.866266459978036
Epoch 6503/10000, Prediction Accuracy = 62.488%, Loss = 0.40908366441726685
Epoch: 6503, Batch Gradient Norm: 10.801496801419301
Epoch: 6503, Batch Gradient Norm after: 10.801496801419301
Epoch 6504/10000, Prediction Accuracy = 62.528%, Loss = 0.4147121489048004
Epoch: 6504, Batch Gradient Norm: 11.731636192206714
Epoch: 6504, Batch Gradient Norm after: 11.731636192206714
Epoch 6505/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.42052285075187684
Epoch: 6505, Batch Gradient Norm: 11.439459307665324
Epoch: 6505, Batch Gradient Norm after: 11.439459307665324
Epoch 6506/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.4190261960029602
Epoch: 6506, Batch Gradient Norm: 9.034698209687175
Epoch: 6506, Batch Gradient Norm after: 9.034698209687175
Epoch 6507/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.4041966199874878
Epoch: 6507, Batch Gradient Norm: 7.712425827518437
Epoch: 6507, Batch Gradient Norm after: 7.712425827518437
Epoch 6508/10000, Prediction Accuracy = 62.604%, Loss = 0.3957091152667999
Epoch: 6508, Batch Gradient Norm: 9.815910869570905
Epoch: 6508, Batch Gradient Norm after: 9.815910869570905
Epoch 6509/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.40690040588378906
Epoch: 6509, Batch Gradient Norm: 11.372687449316915
Epoch: 6509, Batch Gradient Norm after: 11.372687449316915
Epoch 6510/10000, Prediction Accuracy = 62.59799999999999%, Loss = 0.4163207232952118
Epoch: 6510, Batch Gradient Norm: 10.8209146667417
Epoch: 6510, Batch Gradient Norm after: 10.8209146667417
Epoch 6511/10000, Prediction Accuracy = 62.408%, Loss = 0.412283593416214
Epoch: 6511, Batch Gradient Norm: 8.511996353322157
Epoch: 6511, Batch Gradient Norm after: 8.511996353322157
Epoch 6512/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.3985978960990906
Epoch: 6512, Batch Gradient Norm: 8.18145793388562
Epoch: 6512, Batch Gradient Norm after: 8.18145793388562
Epoch 6513/10000, Prediction Accuracy = 62.52%, Loss = 0.3972511112689972
Epoch: 6513, Batch Gradient Norm: 9.631297534731397
Epoch: 6513, Batch Gradient Norm after: 9.631297534731397
Epoch 6514/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.4064602792263031
Epoch: 6514, Batch Gradient Norm: 11.285586881907193
Epoch: 6514, Batch Gradient Norm after: 11.285586881907193
Epoch 6515/10000, Prediction Accuracy = 62.55%, Loss = 0.41977051496505735
Epoch: 6515, Batch Gradient Norm: 10.379460888219118
Epoch: 6515, Batch Gradient Norm after: 10.379460888219118
Epoch 6516/10000, Prediction Accuracy = 62.374%, Loss = 0.4115283489227295
Epoch: 6516, Batch Gradient Norm: 11.049539777035552
Epoch: 6516, Batch Gradient Norm after: 11.049539777035552
Epoch 6517/10000, Prediction Accuracy = 62.572%, Loss = 0.4155850291252136
Epoch: 6517, Batch Gradient Norm: 11.549493747038108
Epoch: 6517, Batch Gradient Norm after: 11.549493747038108
Epoch 6518/10000, Prediction Accuracy = 62.532%, Loss = 0.41842896342277525
Epoch: 6518, Batch Gradient Norm: 10.958929765385609
Epoch: 6518, Batch Gradient Norm after: 10.958929765385609
Epoch 6519/10000, Prediction Accuracy = 62.516000000000005%, Loss = 0.41425846219062806
Epoch: 6519, Batch Gradient Norm: 10.877508888641973
Epoch: 6519, Batch Gradient Norm after: 10.877508888641973
Epoch 6520/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.4142365276813507
Epoch: 6520, Batch Gradient Norm: 12.401241404607887
Epoch: 6520, Batch Gradient Norm after: 12.401241404607887
Epoch 6521/10000, Prediction Accuracy = 62.676%, Loss = 0.4258541166782379
Epoch: 6521, Batch Gradient Norm: 11.081309759470722
Epoch: 6521, Batch Gradient Norm after: 11.081309759470722
Epoch 6522/10000, Prediction Accuracy = 62.592%, Loss = 0.41647308468818667
Epoch: 6522, Batch Gradient Norm: 8.662820300605825
Epoch: 6522, Batch Gradient Norm after: 8.662820300605825
Epoch 6523/10000, Prediction Accuracy = 62.658%, Loss = 0.3993224561214447
Epoch: 6523, Batch Gradient Norm: 8.147944721971259
Epoch: 6523, Batch Gradient Norm after: 8.147944721971259
Epoch 6524/10000, Prediction Accuracy = 62.498000000000005%, Loss = 0.3971046984195709
Epoch: 6524, Batch Gradient Norm: 7.774650475982509
Epoch: 6524, Batch Gradient Norm after: 7.774650475982509
Epoch 6525/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3962698519229889
Epoch: 6525, Batch Gradient Norm: 7.419375082630672
Epoch: 6525, Batch Gradient Norm after: 7.419375082630672
Epoch 6526/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.3939470648765564
Epoch: 6526, Batch Gradient Norm: 8.309720654762225
Epoch: 6526, Batch Gradient Norm after: 8.309720654762225
Epoch 6527/10000, Prediction Accuracy = 62.508%, Loss = 0.3984650015830994
Epoch: 6527, Batch Gradient Norm: 9.2456966185128
Epoch: 6527, Batch Gradient Norm after: 9.2456966185128
Epoch 6528/10000, Prediction Accuracy = 62.474000000000004%, Loss = 0.40336631536483764
Epoch: 6528, Batch Gradient Norm: 11.113104369242931
Epoch: 6528, Batch Gradient Norm after: 11.113104369242931
Epoch 6529/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4157435536384583
Epoch: 6529, Batch Gradient Norm: 10.622411428797456
Epoch: 6529, Batch Gradient Norm after: 10.622411428797456
Epoch 6530/10000, Prediction Accuracy = 62.586%, Loss = 0.41386382579803466
Epoch: 6530, Batch Gradient Norm: 9.234244245840424
Epoch: 6530, Batch Gradient Norm after: 9.234244245840424
Epoch 6531/10000, Prediction Accuracy = 62.516%, Loss = 0.4038650691509247
Epoch: 6531, Batch Gradient Norm: 11.825827808774612
Epoch: 6531, Batch Gradient Norm after: 11.825827808774612
Epoch 6532/10000, Prediction Accuracy = 62.504%, Loss = 0.42173768281936647
Epoch: 6532, Batch Gradient Norm: 12.591262534799629
Epoch: 6532, Batch Gradient Norm after: 12.591262534799629
Epoch 6533/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.42845268845558165
Epoch: 6533, Batch Gradient Norm: 10.05852973667858
Epoch: 6533, Batch Gradient Norm after: 10.05852973667858
Epoch 6534/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.40813104510307313
Epoch: 6534, Batch Gradient Norm: 9.24711969937601
Epoch: 6534, Batch Gradient Norm after: 9.24711969937601
Epoch 6535/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.40244359374046323
Epoch: 6535, Batch Gradient Norm: 11.170492979662427
Epoch: 6535, Batch Gradient Norm after: 11.170492979662427
Epoch 6536/10000, Prediction Accuracy = 62.592%, Loss = 0.41463300585746765
Epoch: 6536, Batch Gradient Norm: 11.9336027798401
Epoch: 6536, Batch Gradient Norm after: 11.9336027798401
Epoch 6537/10000, Prediction Accuracy = 62.588%, Loss = 0.4204192399978638
Epoch: 6537, Batch Gradient Norm: 9.680563977175238
Epoch: 6537, Batch Gradient Norm after: 9.680563977175238
Epoch 6538/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.405186128616333
Epoch: 6538, Batch Gradient Norm: 9.085449588950715
Epoch: 6538, Batch Gradient Norm after: 9.085449588950715
Epoch 6539/10000, Prediction Accuracy = 62.412%, Loss = 0.4022927522659302
Epoch: 6539, Batch Gradient Norm: 9.342137094906485
Epoch: 6539, Batch Gradient Norm after: 9.342137094906485
Epoch 6540/10000, Prediction Accuracy = 62.59000000000001%, Loss = 0.4039751887321472
Epoch: 6540, Batch Gradient Norm: 10.174825135654771
Epoch: 6540, Batch Gradient Norm after: 10.174825135654771
Epoch 6541/10000, Prediction Accuracy = 62.434000000000005%, Loss = 0.4096870422363281
Epoch: 6541, Batch Gradient Norm: 10.325156539674271
Epoch: 6541, Batch Gradient Norm after: 10.325156539674271
Epoch 6542/10000, Prediction Accuracy = 62.564%, Loss = 0.41139574646949767
Epoch: 6542, Batch Gradient Norm: 8.768219525323547
Epoch: 6542, Batch Gradient Norm after: 8.768219525323547
Epoch 6543/10000, Prediction Accuracy = 62.46%, Loss = 0.40220688581466674
Epoch: 6543, Batch Gradient Norm: 7.541123499063473
Epoch: 6543, Batch Gradient Norm after: 7.541123499063473
Epoch 6544/10000, Prediction Accuracy = 62.586%, Loss = 0.3947236657142639
Epoch: 6544, Batch Gradient Norm: 9.283804660173082
Epoch: 6544, Batch Gradient Norm after: 9.283804660173082
Epoch 6545/10000, Prediction Accuracy = 62.576%, Loss = 0.40278400182724
Epoch: 6545, Batch Gradient Norm: 12.686130049186993
Epoch: 6545, Batch Gradient Norm after: 12.686130049186993
Epoch 6546/10000, Prediction Accuracy = 62.556000000000004%, Loss = 0.42668423652648924
Epoch: 6546, Batch Gradient Norm: 12.272436846076284
Epoch: 6546, Batch Gradient Norm after: 12.272436846076284
Epoch 6547/10000, Prediction Accuracy = 62.54%, Loss = 0.4230864763259888
Epoch: 6547, Batch Gradient Norm: 9.809852440971214
Epoch: 6547, Batch Gradient Norm after: 9.809852440971214
Epoch 6548/10000, Prediction Accuracy = 62.722%, Loss = 0.40640464425086975
Epoch: 6548, Batch Gradient Norm: 8.406336798089077
Epoch: 6548, Batch Gradient Norm after: 8.406336798089077
Epoch 6549/10000, Prediction Accuracy = 62.492%, Loss = 0.3986215889453888
Epoch: 6549, Batch Gradient Norm: 8.483437773182812
Epoch: 6549, Batch Gradient Norm after: 8.483437773182812
Epoch 6550/10000, Prediction Accuracy = 62.674%, Loss = 0.4007255077362061
Epoch: 6550, Batch Gradient Norm: 8.299848681328077
Epoch: 6550, Batch Gradient Norm after: 8.299848681328077
Epoch 6551/10000, Prediction Accuracy = 62.548%, Loss = 0.39918083548545835
Epoch: 6551, Batch Gradient Norm: 9.16190180055997
Epoch: 6551, Batch Gradient Norm after: 9.16190180055997
Epoch 6552/10000, Prediction Accuracy = 62.574%, Loss = 0.4040186405181885
Epoch: 6552, Batch Gradient Norm: 10.799148528768486
Epoch: 6552, Batch Gradient Norm after: 10.799148528768486
Epoch 6553/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.414029586315155
Epoch: 6553, Batch Gradient Norm: 10.571949686245397
Epoch: 6553, Batch Gradient Norm after: 10.571949686245397
Epoch 6554/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.4111307621002197
Epoch: 6554, Batch Gradient Norm: 11.005920020494123
Epoch: 6554, Batch Gradient Norm after: 11.005920020494123
Epoch 6555/10000, Prediction Accuracy = 62.46999999999999%, Loss = 0.41397210359573366
Epoch: 6555, Batch Gradient Norm: 11.664689265818376
Epoch: 6555, Batch Gradient Norm after: 11.664689265818376
Epoch 6556/10000, Prediction Accuracy = 62.512%, Loss = 0.41951319575309753
Epoch: 6556, Batch Gradient Norm: 11.918047507428643
Epoch: 6556, Batch Gradient Norm after: 11.918047507428643
Epoch 6557/10000, Prediction Accuracy = 62.459999999999994%, Loss = 0.42163466811180117
Epoch: 6557, Batch Gradient Norm: 12.02545739665938
Epoch: 6557, Batch Gradient Norm after: 12.02545739665938
Epoch 6558/10000, Prediction Accuracy = 62.61%, Loss = 0.42159463167190553
Epoch: 6558, Batch Gradient Norm: 10.77772430279618
Epoch: 6558, Batch Gradient Norm after: 10.77772430279618
Epoch 6559/10000, Prediction Accuracy = 62.584%, Loss = 0.41178905963897705
Epoch: 6559, Batch Gradient Norm: 9.230526673617522
Epoch: 6559, Batch Gradient Norm after: 9.230526673617522
Epoch 6560/10000, Prediction Accuracy = 62.624%, Loss = 0.40146605372428895
Epoch: 6560, Batch Gradient Norm: 9.275844222367732
Epoch: 6560, Batch Gradient Norm after: 9.275844222367732
Epoch 6561/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.4027331233024597
Epoch: 6561, Batch Gradient Norm: 8.839148233597996
Epoch: 6561, Batch Gradient Norm after: 8.839148233597996
Epoch 6562/10000, Prediction Accuracy = 62.52%, Loss = 0.4010513961315155
Epoch: 6562, Batch Gradient Norm: 8.510134312795673
Epoch: 6562, Batch Gradient Norm after: 8.510134312795673
Epoch 6563/10000, Prediction Accuracy = 62.576%, Loss = 0.3983941078186035
Epoch: 6563, Batch Gradient Norm: 8.776119109770574
Epoch: 6563, Batch Gradient Norm after: 8.776119109770574
Epoch 6564/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.39901396036148074
Epoch: 6564, Batch Gradient Norm: 10.151710990987032
Epoch: 6564, Batch Gradient Norm after: 10.151710990987032
Epoch 6565/10000, Prediction Accuracy = 62.562%, Loss = 0.40700533986091614
Epoch: 6565, Batch Gradient Norm: 11.503621964602086
Epoch: 6565, Batch Gradient Norm after: 11.503621964602086
Epoch 6566/10000, Prediction Accuracy = 62.512%, Loss = 0.4170648157596588
Epoch: 6566, Batch Gradient Norm: 11.799186265917113
Epoch: 6566, Batch Gradient Norm after: 11.799186265917113
Epoch 6567/10000, Prediction Accuracy = 62.538%, Loss = 0.4212166011333466
Epoch: 6567, Batch Gradient Norm: 10.197719019951219
Epoch: 6567, Batch Gradient Norm after: 10.197719019951219
Epoch 6568/10000, Prediction Accuracy = 62.64200000000001%, Loss = 0.4091863870620728
Epoch: 6568, Batch Gradient Norm: 9.32313864750714
Epoch: 6568, Batch Gradient Norm after: 9.32313864750714
Epoch 6569/10000, Prediction Accuracy = 62.516%, Loss = 0.40271891951560973
Epoch: 6569, Batch Gradient Norm: 8.989428498191536
Epoch: 6569, Batch Gradient Norm after: 8.989428498191536
Epoch 6570/10000, Prediction Accuracy = 62.564%, Loss = 0.4000454664230347
Epoch: 6570, Batch Gradient Norm: 9.190694219748378
Epoch: 6570, Batch Gradient Norm after: 9.190694219748378
Epoch 6571/10000, Prediction Accuracy = 62.588%, Loss = 0.40066750049591066
Epoch: 6571, Batch Gradient Norm: 9.934439157976133
Epoch: 6571, Batch Gradient Norm after: 9.934439157976133
Epoch 6572/10000, Prediction Accuracy = 62.614%, Loss = 0.4057512104511261
Epoch: 6572, Batch Gradient Norm: 10.851616577925713
Epoch: 6572, Batch Gradient Norm after: 10.851616577925713
Epoch 6573/10000, Prediction Accuracy = 62.598%, Loss = 0.4153196394443512
Epoch: 6573, Batch Gradient Norm: 8.841560242461352
Epoch: 6573, Batch Gradient Norm after: 8.841560242461352
Epoch 6574/10000, Prediction Accuracy = 62.528%, Loss = 0.402394962310791
Epoch: 6574, Batch Gradient Norm: 10.01853207357515
Epoch: 6574, Batch Gradient Norm after: 10.01853207357515
Epoch 6575/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.40902509689331057
Epoch: 6575, Batch Gradient Norm: 10.75698753432326
Epoch: 6575, Batch Gradient Norm after: 10.75698753432326
Epoch 6576/10000, Prediction Accuracy = 62.58799999999999%, Loss = 0.4153135001659393
Epoch: 6576, Batch Gradient Norm: 8.435660370018438
Epoch: 6576, Batch Gradient Norm after: 8.435660370018438
Epoch 6577/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.39806810617446897
Epoch: 6577, Batch Gradient Norm: 10.63504386162737
Epoch: 6577, Batch Gradient Norm after: 10.63504386162737
Epoch 6578/10000, Prediction Accuracy = 62.470000000000006%, Loss = 0.41074257493019106
Epoch: 6578, Batch Gradient Norm: 11.782946072239119
Epoch: 6578, Batch Gradient Norm after: 11.782946072239119
Epoch 6579/10000, Prediction Accuracy = 62.414%, Loss = 0.41970119476318357
Epoch: 6579, Batch Gradient Norm: 10.168408772201959
Epoch: 6579, Batch Gradient Norm after: 10.168408772201959
Epoch 6580/10000, Prediction Accuracy = 62.432%, Loss = 0.40767624974250793
Epoch: 6580, Batch Gradient Norm: 9.258163686261007
Epoch: 6580, Batch Gradient Norm after: 9.258163686261007
Epoch 6581/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.4023037850856781
Epoch: 6581, Batch Gradient Norm: 9.942981239216376
Epoch: 6581, Batch Gradient Norm after: 9.942981239216376
Epoch 6582/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.4073019683361053
Epoch: 6582, Batch Gradient Norm: 10.940988764790985
Epoch: 6582, Batch Gradient Norm after: 10.940988764790985
Epoch 6583/10000, Prediction Accuracy = 62.42%, Loss = 0.41355648040771487
Epoch: 6583, Batch Gradient Norm: 12.049452008652917
Epoch: 6583, Batch Gradient Norm after: 12.049452008652917
Epoch 6584/10000, Prediction Accuracy = 62.477999999999994%, Loss = 0.42177984714508054
Epoch: 6584, Batch Gradient Norm: 10.0598550996438
Epoch: 6584, Batch Gradient Norm after: 10.0598550996438
Epoch 6585/10000, Prediction Accuracy = 62.55999999999999%, Loss = 0.4074782311916351
Epoch: 6585, Batch Gradient Norm: 10.16319341235602
Epoch: 6585, Batch Gradient Norm after: 10.16319341235602
Epoch 6586/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.40928401350975036
Epoch: 6586, Batch Gradient Norm: 10.907658680654993
Epoch: 6586, Batch Gradient Norm after: 10.907658680654993
Epoch 6587/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.41428942084312437
Epoch: 6587, Batch Gradient Norm: 10.201054526889461
Epoch: 6587, Batch Gradient Norm after: 10.201054526889461
Epoch 6588/10000, Prediction Accuracy = 62.476%, Loss = 0.40805416107177733
Epoch: 6588, Batch Gradient Norm: 9.233536221893617
Epoch: 6588, Batch Gradient Norm after: 9.233536221893617
Epoch 6589/10000, Prediction Accuracy = 62.686%, Loss = 0.4013064205646515
Epoch: 6589, Batch Gradient Norm: 8.444502922692434
Epoch: 6589, Batch Gradient Norm after: 8.444502922692434
Epoch 6590/10000, Prediction Accuracy = 62.489999999999995%, Loss = 0.3964828550815582
Epoch: 6590, Batch Gradient Norm: 9.342162454185477
Epoch: 6590, Batch Gradient Norm after: 9.342162454185477
Epoch 6591/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.40178664326667785
Epoch: 6591, Batch Gradient Norm: 9.771261077863807
Epoch: 6591, Batch Gradient Norm after: 9.771261077863807
Epoch 6592/10000, Prediction Accuracy = 62.634%, Loss = 0.40439683198928833
Epoch: 6592, Batch Gradient Norm: 9.611036575903542
Epoch: 6592, Batch Gradient Norm after: 9.611036575903542
Epoch 6593/10000, Prediction Accuracy = 62.694%, Loss = 0.4038099408149719
Epoch: 6593, Batch Gradient Norm: 10.095007835971785
Epoch: 6593, Batch Gradient Norm after: 10.095007835971785
Epoch 6594/10000, Prediction Accuracy = 62.564%, Loss = 0.40672669410705564
Epoch: 6594, Batch Gradient Norm: 11.110474678020932
Epoch: 6594, Batch Gradient Norm after: 11.110474678020932
Epoch 6595/10000, Prediction Accuracy = 62.522000000000006%, Loss = 0.4163967192173004
Epoch: 6595, Batch Gradient Norm: 9.644384098030955
Epoch: 6595, Batch Gradient Norm after: 9.644384098030955
Epoch 6596/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4062071621417999
Epoch: 6596, Batch Gradient Norm: 8.206290848083393
Epoch: 6596, Batch Gradient Norm after: 8.206290848083393
Epoch 6597/10000, Prediction Accuracy = 62.64%, Loss = 0.3967310845851898
Epoch: 6597, Batch Gradient Norm: 9.409463031433436
Epoch: 6597, Batch Gradient Norm after: 9.409463031433436
Epoch 6598/10000, Prediction Accuracy = 62.588%, Loss = 0.4041219413280487
Epoch: 6598, Batch Gradient Norm: 12.311080521313302
Epoch: 6598, Batch Gradient Norm after: 12.311080521313302
Epoch 6599/10000, Prediction Accuracy = 62.516%, Loss = 0.42377246022224424
Epoch: 6599, Batch Gradient Norm: 12.740386256682775
Epoch: 6599, Batch Gradient Norm after: 12.740386256682775
Epoch 6600/10000, Prediction Accuracy = 62.519999999999996%, Loss = 0.42706592082977296
Epoch: 6600, Batch Gradient Norm: 10.041959557749903
Epoch: 6600, Batch Gradient Norm after: 10.041959557749903
Epoch 6601/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.4064434111118317
Epoch: 6601, Batch Gradient Norm: 9.436560587655533
Epoch: 6601, Batch Gradient Norm after: 9.436560587655533
Epoch 6602/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.40272061824798583
Epoch: 6602, Batch Gradient Norm: 10.282023421698643
Epoch: 6602, Batch Gradient Norm after: 10.282023421698643
Epoch 6603/10000, Prediction Accuracy = 62.517999999999994%, Loss = 0.40905836820602415
Epoch: 6603, Batch Gradient Norm: 9.671871542982453
Epoch: 6603, Batch Gradient Norm after: 9.671871542982453
Epoch 6604/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.4058772921562195
Epoch: 6604, Batch Gradient Norm: 8.953584932807983
Epoch: 6604, Batch Gradient Norm after: 8.953584932807983
Epoch 6605/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.40127016305923463
Epoch: 6605, Batch Gradient Norm: 9.214327280170263
Epoch: 6605, Batch Gradient Norm after: 9.214327280170263
Epoch 6606/10000, Prediction Accuracy = 62.64%, Loss = 0.4022262215614319
Epoch: 6606, Batch Gradient Norm: 10.97642744444001
Epoch: 6606, Batch Gradient Norm after: 10.97642744444001
Epoch 6607/10000, Prediction Accuracy = 62.584%, Loss = 0.41253610849380495
Epoch: 6607, Batch Gradient Norm: 10.62112991108863
Epoch: 6607, Batch Gradient Norm after: 10.62112991108863
Epoch 6608/10000, Prediction Accuracy = 62.652%, Loss = 0.4093786120414734
Epoch: 6608, Batch Gradient Norm: 9.422723216494102
Epoch: 6608, Batch Gradient Norm after: 9.422723216494102
Epoch 6609/10000, Prediction Accuracy = 62.556%, Loss = 0.40199804306030273
Epoch: 6609, Batch Gradient Norm: 9.541638647393961
Epoch: 6609, Batch Gradient Norm after: 9.541638647393961
Epoch 6610/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.4034889698028564
Epoch: 6610, Batch Gradient Norm: 11.524976702297545
Epoch: 6610, Batch Gradient Norm after: 11.524976702297545
Epoch 6611/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4178595721721649
Epoch: 6611, Batch Gradient Norm: 11.16919739681735
Epoch: 6611, Batch Gradient Norm after: 11.16919739681735
Epoch 6612/10000, Prediction Accuracy = 62.626%, Loss = 0.4157639920711517
Epoch: 6612, Batch Gradient Norm: 9.584014794783245
Epoch: 6612, Batch Gradient Norm after: 9.584014794783245
Epoch 6613/10000, Prediction Accuracy = 62.512%, Loss = 0.40427389144897463
Epoch: 6613, Batch Gradient Norm: 10.059048032434028
Epoch: 6613, Batch Gradient Norm after: 10.059048032434028
Epoch 6614/10000, Prediction Accuracy = 62.75%, Loss = 0.4078413486480713
Epoch: 6614, Batch Gradient Norm: 11.156606979244694
Epoch: 6614, Batch Gradient Norm after: 11.156606979244694
Epoch 6615/10000, Prediction Accuracy = 62.612%, Loss = 0.415076756477356
Epoch: 6615, Batch Gradient Norm: 10.294950561592213
Epoch: 6615, Batch Gradient Norm after: 10.294950561592213
Epoch 6616/10000, Prediction Accuracy = 62.538%, Loss = 0.40824065208435056
Epoch: 6616, Batch Gradient Norm: 9.11298530375954
Epoch: 6616, Batch Gradient Norm after: 9.11298530375954
Epoch 6617/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.401183557510376
Epoch: 6617, Batch Gradient Norm: 8.104933931951614
Epoch: 6617, Batch Gradient Norm after: 8.104933931951614
Epoch 6618/10000, Prediction Accuracy = 62.556%, Loss = 0.3948671340942383
Epoch: 6618, Batch Gradient Norm: 8.94713715636031
Epoch: 6618, Batch Gradient Norm after: 8.94713715636031
Epoch 6619/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.40003945827484133
Epoch: 6619, Batch Gradient Norm: 9.665902852205877
Epoch: 6619, Batch Gradient Norm after: 9.665902852205877
Epoch 6620/10000, Prediction Accuracy = 62.538%, Loss = 0.4039751648902893
Epoch: 6620, Batch Gradient Norm: 9.238195902065314
Epoch: 6620, Batch Gradient Norm after: 9.238195902065314
Epoch 6621/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.40102153420448305
Epoch: 6621, Batch Gradient Norm: 9.021021759934431
Epoch: 6621, Batch Gradient Norm after: 9.021021759934431
Epoch 6622/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.3995456099510193
Epoch: 6622, Batch Gradient Norm: 10.441999065391734
Epoch: 6622, Batch Gradient Norm after: 10.441999065391734
Epoch 6623/10000, Prediction Accuracy = 62.488%, Loss = 0.4083190977573395
Epoch: 6623, Batch Gradient Norm: 13.068041913104882
Epoch: 6623, Batch Gradient Norm after: 13.068041913104882
Epoch 6624/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.42823352217674254
Epoch: 6624, Batch Gradient Norm: 11.009045066734233
Epoch: 6624, Batch Gradient Norm after: 11.009045066734233
Epoch 6625/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.41190074682235717
Epoch: 6625, Batch Gradient Norm: 10.302904269056253
Epoch: 6625, Batch Gradient Norm after: 10.302904269056253
Epoch 6626/10000, Prediction Accuracy = 62.398%, Loss = 0.409344482421875
Epoch: 6626, Batch Gradient Norm: 9.778653730387285
Epoch: 6626, Batch Gradient Norm after: 9.778653730387285
Epoch 6627/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.4082236707210541
Epoch: 6627, Batch Gradient Norm: 8.364457202981887
Epoch: 6627, Batch Gradient Norm after: 8.364457202981887
Epoch 6628/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.3965899646282196
Epoch: 6628, Batch Gradient Norm: 9.718767659521381
Epoch: 6628, Batch Gradient Norm after: 9.718767659521381
Epoch 6629/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.4038156747817993
Epoch: 6629, Batch Gradient Norm: 10.452225343291728
Epoch: 6629, Batch Gradient Norm after: 10.452225343291728
Epoch 6630/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.409026038646698
Epoch: 6630, Batch Gradient Norm: 9.391211862681965
Epoch: 6630, Batch Gradient Norm after: 9.391211862681965
Epoch 6631/10000, Prediction Accuracy = 62.686%, Loss = 0.40115870237350465
Epoch: 6631, Batch Gradient Norm: 11.824951244513699
Epoch: 6631, Batch Gradient Norm after: 11.824951244513699
Epoch 6632/10000, Prediction Accuracy = 62.598%, Loss = 0.4168383002281189
Epoch: 6632, Batch Gradient Norm: 13.453715502123991
Epoch: 6632, Batch Gradient Norm after: 13.453715502123991
Epoch 6633/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.43250333070755004
Epoch: 6633, Batch Gradient Norm: 10.529942146578138
Epoch: 6633, Batch Gradient Norm after: 10.529942146578138
Epoch 6634/10000, Prediction Accuracy = 62.55400000000001%, Loss = 0.4109161734580994
Epoch: 6634, Batch Gradient Norm: 9.310078301121392
Epoch: 6634, Batch Gradient Norm after: 9.310078301121392
Epoch 6635/10000, Prediction Accuracy = 62.540000000000006%, Loss = 0.40173292756080625
Epoch: 6635, Batch Gradient Norm: 9.81510613767895
Epoch: 6635, Batch Gradient Norm after: 9.81510613767895
Epoch 6636/10000, Prediction Accuracy = 62.628%, Loss = 0.40433120131492617
Epoch: 6636, Batch Gradient Norm: 9.71190064858034
Epoch: 6636, Batch Gradient Norm after: 9.71190064858034
Epoch 6637/10000, Prediction Accuracy = 62.45399999999999%, Loss = 0.4044548511505127
Epoch: 6637, Batch Gradient Norm: 8.685501002967824
Epoch: 6637, Batch Gradient Norm after: 8.685501002967824
Epoch 6638/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.3985494911670685
Epoch: 6638, Batch Gradient Norm: 9.039005439648427
Epoch: 6638, Batch Gradient Norm after: 9.039005439648427
Epoch 6639/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.4013912916183472
Epoch: 6639, Batch Gradient Norm: 10.225321310288102
Epoch: 6639, Batch Gradient Norm after: 10.225321310288102
Epoch 6640/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.4083414912223816
Epoch: 6640, Batch Gradient Norm: 10.551568799973843
Epoch: 6640, Batch Gradient Norm after: 10.551568799973843
Epoch 6641/10000, Prediction Accuracy = 62.634%, Loss = 0.4101333200931549
Epoch: 6641, Batch Gradient Norm: 10.866293537528477
Epoch: 6641, Batch Gradient Norm after: 10.866293537528477
Epoch 6642/10000, Prediction Accuracy = 62.446000000000005%, Loss = 0.4116548717021942
Epoch: 6642, Batch Gradient Norm: 9.153606448586928
Epoch: 6642, Batch Gradient Norm after: 9.153606448586928
Epoch 6643/10000, Prediction Accuracy = 62.64799999999999%, Loss = 0.40087359547615053
Epoch: 6643, Batch Gradient Norm: 8.64509497580125
Epoch: 6643, Batch Gradient Norm after: 8.64509497580125
Epoch 6644/10000, Prediction Accuracy = 62.548%, Loss = 0.3969837486743927
Epoch: 6644, Batch Gradient Norm: 10.167697541395636
Epoch: 6644, Batch Gradient Norm after: 10.167697541395636
Epoch 6645/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.4078647196292877
Epoch: 6645, Batch Gradient Norm: 10.353345949507474
Epoch: 6645, Batch Gradient Norm after: 10.353345949507474
Epoch 6646/10000, Prediction Accuracy = 62.534000000000006%, Loss = 0.4090177118778229
Epoch: 6646, Batch Gradient Norm: 10.008700565458794
Epoch: 6646, Batch Gradient Norm after: 10.008700565458794
Epoch 6647/10000, Prediction Accuracy = 62.562%, Loss = 0.40577374696731566
Epoch: 6647, Batch Gradient Norm: 10.185763871767445
Epoch: 6647, Batch Gradient Norm after: 10.185763871767445
Epoch 6648/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.40647223591804504
Epoch: 6648, Batch Gradient Norm: 9.907114610950455
Epoch: 6648, Batch Gradient Norm after: 9.907114610950455
Epoch 6649/10000, Prediction Accuracy = 62.476%, Loss = 0.4051978409290314
Epoch: 6649, Batch Gradient Norm: 9.649828534003737
Epoch: 6649, Batch Gradient Norm after: 9.649828534003737
Epoch 6650/10000, Prediction Accuracy = 62.602%, Loss = 0.40446923971176146
Epoch: 6650, Batch Gradient Norm: 10.724552219499643
Epoch: 6650, Batch Gradient Norm after: 10.724552219499643
Epoch 6651/10000, Prediction Accuracy = 62.622%, Loss = 0.40939012765884397
Epoch: 6651, Batch Gradient Norm: 12.45880314098532
Epoch: 6651, Batch Gradient Norm after: 12.45880314098532
Epoch 6652/10000, Prediction Accuracy = 62.604%, Loss = 0.421632194519043
Epoch: 6652, Batch Gradient Norm: 9.599933315192807
Epoch: 6652, Batch Gradient Norm after: 9.599933315192807
Epoch 6653/10000, Prediction Accuracy = 62.712%, Loss = 0.40169647336006165
Epoch: 6653, Batch Gradient Norm: 8.366408540627717
Epoch: 6653, Batch Gradient Norm after: 8.366408540627717
Epoch 6654/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.39465205669403075
Epoch: 6654, Batch Gradient Norm: 9.884379232526294
Epoch: 6654, Batch Gradient Norm after: 9.884379232526294
Epoch 6655/10000, Prediction Accuracy = 62.314%, Loss = 0.4053198635578156
Epoch: 6655, Batch Gradient Norm: 9.379211773431482
Epoch: 6655, Batch Gradient Norm after: 9.379211773431482
Epoch 6656/10000, Prediction Accuracy = 62.5%, Loss = 0.40303834676742556
Epoch: 6656, Batch Gradient Norm: 8.668161382862623
Epoch: 6656, Batch Gradient Norm after: 8.668161382862623
Epoch 6657/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.39782966375350953
Epoch: 6657, Batch Gradient Norm: 10.496060859180174
Epoch: 6657, Batch Gradient Norm after: 10.496060859180174
Epoch 6658/10000, Prediction Accuracy = 62.662%, Loss = 0.4088981509208679
Epoch: 6658, Batch Gradient Norm: 12.059047144451348
Epoch: 6658, Batch Gradient Norm after: 12.059047144451348
Epoch 6659/10000, Prediction Accuracy = 62.42%, Loss = 0.42144824266433717
Epoch: 6659, Batch Gradient Norm: 10.873929213059581
Epoch: 6659, Batch Gradient Norm after: 10.873929213059581
Epoch 6660/10000, Prediction Accuracy = 62.658%, Loss = 0.4123885095119476
Epoch: 6660, Batch Gradient Norm: 10.255464313032583
Epoch: 6660, Batch Gradient Norm after: 10.255464313032583
Epoch 6661/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.4082894682884216
Epoch: 6661, Batch Gradient Norm: 10.725050317314464
Epoch: 6661, Batch Gradient Norm after: 10.725050317314464
Epoch 6662/10000, Prediction Accuracy = 62.43399999999999%, Loss = 0.4098370552062988
Epoch: 6662, Batch Gradient Norm: 11.060909130850261
Epoch: 6662, Batch Gradient Norm after: 11.060909130850261
Epoch 6663/10000, Prediction Accuracy = 62.698%, Loss = 0.4132958292961121
Epoch: 6663, Batch Gradient Norm: 9.732561000029756
Epoch: 6663, Batch Gradient Norm after: 9.732561000029756
Epoch 6664/10000, Prediction Accuracy = 62.726%, Loss = 0.4053064227104187
Epoch: 6664, Batch Gradient Norm: 8.991762543441695
Epoch: 6664, Batch Gradient Norm after: 8.991762543441695
Epoch 6665/10000, Prediction Accuracy = 62.602%, Loss = 0.40103440880775454
Epoch: 6665, Batch Gradient Norm: 9.38921139969441
Epoch: 6665, Batch Gradient Norm after: 9.38921139969441
Epoch 6666/10000, Prediction Accuracy = 62.552%, Loss = 0.40239604115486144
Epoch: 6666, Batch Gradient Norm: 10.4057344023453
Epoch: 6666, Batch Gradient Norm after: 10.4057344023453
Epoch 6667/10000, Prediction Accuracy = 62.59599999999999%, Loss = 0.4069302022457123
Epoch: 6667, Batch Gradient Norm: 10.303798295977565
Epoch: 6667, Batch Gradient Norm after: 10.303798295977565
Epoch 6668/10000, Prediction Accuracy = 62.616%, Loss = 0.40621756911277773
Epoch: 6668, Batch Gradient Norm: 8.620955682793241
Epoch: 6668, Batch Gradient Norm after: 8.620955682793241
Epoch 6669/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.39623863697052003
Epoch: 6669, Batch Gradient Norm: 9.32614099858776
Epoch: 6669, Batch Gradient Norm after: 9.32614099858776
Epoch 6670/10000, Prediction Accuracy = 62.544%, Loss = 0.40071406960487366
Epoch: 6670, Batch Gradient Norm: 11.725522156627735
Epoch: 6670, Batch Gradient Norm after: 11.725522156627735
Epoch 6671/10000, Prediction Accuracy = 62.624%, Loss = 0.41864187717437745
Epoch: 6671, Batch Gradient Norm: 10.645605834264765
Epoch: 6671, Batch Gradient Norm after: 10.645605834264765
Epoch 6672/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.4107821941375732
Epoch: 6672, Batch Gradient Norm: 8.739586870143073
Epoch: 6672, Batch Gradient Norm after: 8.739586870143073
Epoch 6673/10000, Prediction Accuracy = 62.598%, Loss = 0.39715256690979006
Epoch: 6673, Batch Gradient Norm: 10.239592115836004
Epoch: 6673, Batch Gradient Norm after: 10.239592115836004
Epoch 6674/10000, Prediction Accuracy = 62.843999999999994%, Loss = 0.40632917881011965
Epoch: 6674, Batch Gradient Norm: 10.821668520313466
Epoch: 6674, Batch Gradient Norm after: 10.821668520313466
Epoch 6675/10000, Prediction Accuracy = 62.572%, Loss = 0.41275490522384645
Epoch: 6675, Batch Gradient Norm: 9.129602801502033
Epoch: 6675, Batch Gradient Norm after: 9.129602801502033
Epoch 6676/10000, Prediction Accuracy = 62.694%, Loss = 0.39973985552787783
Epoch: 6676, Batch Gradient Norm: 9.505454356159321
Epoch: 6676, Batch Gradient Norm after: 9.505454356159321
Epoch 6677/10000, Prediction Accuracy = 62.616%, Loss = 0.400797700881958
Epoch: 6677, Batch Gradient Norm: 11.479414366977839
Epoch: 6677, Batch Gradient Norm after: 11.479414366977839
Epoch 6678/10000, Prediction Accuracy = 62.436%, Loss = 0.4159295380115509
Epoch: 6678, Batch Gradient Norm: 11.139440449351289
Epoch: 6678, Batch Gradient Norm after: 11.139440449351289
Epoch 6679/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.41498649716377256
Epoch: 6679, Batch Gradient Norm: 9.630766822974305
Epoch: 6679, Batch Gradient Norm after: 9.630766822974305
Epoch 6680/10000, Prediction Accuracy = 62.564%, Loss = 0.4037782192230225
Epoch: 6680, Batch Gradient Norm: 8.697663542510131
Epoch: 6680, Batch Gradient Norm after: 8.697663542510131
Epoch 6681/10000, Prediction Accuracy = 62.45%, Loss = 0.39771621823310854
Epoch: 6681, Batch Gradient Norm: 7.7589223764216895
Epoch: 6681, Batch Gradient Norm after: 7.7589223764216895
Epoch 6682/10000, Prediction Accuracy = 62.61%, Loss = 0.3918271720409393
Epoch: 6682, Batch Gradient Norm: 10.098943439961953
Epoch: 6682, Batch Gradient Norm after: 10.098943439961953
Epoch 6683/10000, Prediction Accuracy = 62.614%, Loss = 0.40583279728889465
Epoch: 6683, Batch Gradient Norm: 12.621798098237516
Epoch: 6683, Batch Gradient Norm after: 12.621798098237516
Epoch 6684/10000, Prediction Accuracy = 62.552%, Loss = 0.4268085896968842
Epoch: 6684, Batch Gradient Norm: 11.083584690060237
Epoch: 6684, Batch Gradient Norm after: 11.083584690060237
Epoch 6685/10000, Prediction Accuracy = 62.612%, Loss = 0.41395663619041445
Epoch: 6685, Batch Gradient Norm: 10.849996401329932
Epoch: 6685, Batch Gradient Norm after: 10.849996401329932
Epoch 6686/10000, Prediction Accuracy = 62.656000000000006%, Loss = 0.4098616480827332
Epoch: 6686, Batch Gradient Norm: 11.515435030429513
Epoch: 6686, Batch Gradient Norm after: 11.515435030429513
Epoch 6687/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.41410101652145387
Epoch: 6687, Batch Gradient Norm: 9.62273456804935
Epoch: 6687, Batch Gradient Norm after: 9.62273456804935
Epoch 6688/10000, Prediction Accuracy = 62.682%, Loss = 0.4011122703552246
Epoch: 6688, Batch Gradient Norm: 8.750545247039717
Epoch: 6688, Batch Gradient Norm after: 8.750545247039717
Epoch 6689/10000, Prediction Accuracy = 62.602%, Loss = 0.3957717537879944
Epoch: 6689, Batch Gradient Norm: 10.131373252111992
Epoch: 6689, Batch Gradient Norm after: 10.131373252111992
Epoch 6690/10000, Prediction Accuracy = 62.682%, Loss = 0.40585861206054685
Epoch: 6690, Batch Gradient Norm: 10.316390754424534
Epoch: 6690, Batch Gradient Norm after: 10.316390754424534
Epoch 6691/10000, Prediction Accuracy = 62.604%, Loss = 0.40804498791694643
Epoch: 6691, Batch Gradient Norm: 8.472545514365844
Epoch: 6691, Batch Gradient Norm after: 8.472545514365844
Epoch 6692/10000, Prediction Accuracy = 62.73%, Loss = 0.39545832872390746
Epoch: 6692, Batch Gradient Norm: 8.13243827443924
Epoch: 6692, Batch Gradient Norm after: 8.13243827443924
Epoch 6693/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.39211505651474
Epoch: 6693, Batch Gradient Norm: 10.553194631910696
Epoch: 6693, Batch Gradient Norm after: 10.553194631910696
Epoch 6694/10000, Prediction Accuracy = 62.674%, Loss = 0.40528941750526426
Epoch: 6694, Batch Gradient Norm: 11.92494972490542
Epoch: 6694, Batch Gradient Norm after: 11.92494972490542
Epoch 6695/10000, Prediction Accuracy = 62.472%, Loss = 0.4187305986881256
Epoch: 6695, Batch Gradient Norm: 8.687928508189604
Epoch: 6695, Batch Gradient Norm after: 8.687928508189604
Epoch 6696/10000, Prediction Accuracy = 62.598%, Loss = 0.3974521577358246
Epoch: 6696, Batch Gradient Norm: 6.804155406554311
Epoch: 6696, Batch Gradient Norm after: 6.804155406554311
Epoch 6697/10000, Prediction Accuracy = 62.64%, Loss = 0.38722420334815977
Epoch: 6697, Batch Gradient Norm: 9.580213960112856
Epoch: 6697, Batch Gradient Norm after: 9.580213960112856
Epoch 6698/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.4008981823921204
Epoch: 6698, Batch Gradient Norm: 13.916572954648219
Epoch: 6698, Batch Gradient Norm after: 13.916572954648219
Epoch 6699/10000, Prediction Accuracy = 62.396%, Loss = 0.43682425618171694
Epoch: 6699, Batch Gradient Norm: 11.774922068374472
Epoch: 6699, Batch Gradient Norm after: 11.774922068374472
Epoch 6700/10000, Prediction Accuracy = 62.50599999999999%, Loss = 0.4192658722400665
Epoch: 6700, Batch Gradient Norm: 9.464118053465649
Epoch: 6700, Batch Gradient Norm after: 9.464118053465649
Epoch 6701/10000, Prediction Accuracy = 62.664%, Loss = 0.40261900424957275
Epoch: 6701, Batch Gradient Norm: 10.088564122869261
Epoch: 6701, Batch Gradient Norm after: 10.088564122869261
Epoch 6702/10000, Prediction Accuracy = 62.686%, Loss = 0.40633490681648254
Epoch: 6702, Batch Gradient Norm: 10.071653056713807
Epoch: 6702, Batch Gradient Norm after: 10.071653056713807
Epoch 6703/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.4055274069309235
Epoch: 6703, Batch Gradient Norm: 9.688816258405325
Epoch: 6703, Batch Gradient Norm after: 9.688816258405325
Epoch 6704/10000, Prediction Accuracy = 62.64%, Loss = 0.402596253156662
Epoch: 6704, Batch Gradient Norm: 10.193717098268634
Epoch: 6704, Batch Gradient Norm after: 10.193717098268634
Epoch 6705/10000, Prediction Accuracy = 62.638%, Loss = 0.40682058334350585
Epoch: 6705, Batch Gradient Norm: 10.13479194037918
Epoch: 6705, Batch Gradient Norm after: 10.13479194037918
Epoch 6706/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.4058950483798981
Epoch: 6706, Batch Gradient Norm: 10.0390875718834
Epoch: 6706, Batch Gradient Norm after: 10.0390875718834
Epoch 6707/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.40427581071853635
Epoch: 6707, Batch Gradient Norm: 9.777308020379913
Epoch: 6707, Batch Gradient Norm after: 9.777308020379913
Epoch 6708/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.40300174355506896
Epoch: 6708, Batch Gradient Norm: 8.849102735328474
Epoch: 6708, Batch Gradient Norm after: 8.849102735328474
Epoch 6709/10000, Prediction Accuracy = 62.54%, Loss = 0.3982278823852539
Epoch: 6709, Batch Gradient Norm: 7.662789487298528
Epoch: 6709, Batch Gradient Norm after: 7.662789487298528
Epoch 6710/10000, Prediction Accuracy = 62.67800000000001%, Loss = 0.3919845759868622
Epoch: 6710, Batch Gradient Norm: 8.947924391258516
Epoch: 6710, Batch Gradient Norm after: 8.947924391258516
Epoch 6711/10000, Prediction Accuracy = 62.664%, Loss = 0.399418705701828
Epoch: 6711, Batch Gradient Norm: 10.246979177992927
Epoch: 6711, Batch Gradient Norm after: 10.246979177992927
Epoch 6712/10000, Prediction Accuracy = 62.55%, Loss = 0.4083125352859497
Epoch: 6712, Batch Gradient Norm: 11.084856155109376
Epoch: 6712, Batch Gradient Norm after: 11.084856155109376
Epoch 6713/10000, Prediction Accuracy = 62.580000000000005%, Loss = 0.41410091519355774
Epoch: 6713, Batch Gradient Norm: 11.84271039423072
Epoch: 6713, Batch Gradient Norm after: 11.84271039423072
Epoch 6714/10000, Prediction Accuracy = 62.512%, Loss = 0.4169494450092316
Epoch: 6714, Batch Gradient Norm: 11.830858877998308
Epoch: 6714, Batch Gradient Norm after: 11.830858877998308
Epoch 6715/10000, Prediction Accuracy = 62.486000000000004%, Loss = 0.416912305355072
Epoch: 6715, Batch Gradient Norm: 9.54678925654263
Epoch: 6715, Batch Gradient Norm after: 9.54678925654263
Epoch 6716/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.4010456442832947
Epoch: 6716, Batch Gradient Norm: 9.54865037560313
Epoch: 6716, Batch Gradient Norm after: 9.54865037560313
Epoch 6717/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.4015432596206665
Epoch: 6717, Batch Gradient Norm: 10.683908523713747
Epoch: 6717, Batch Gradient Norm after: 10.683908523713747
Epoch 6718/10000, Prediction Accuracy = 62.536%, Loss = 0.410451340675354
Epoch: 6718, Batch Gradient Norm: 10.194887273026737
Epoch: 6718, Batch Gradient Norm after: 10.194887273026737
Epoch 6719/10000, Prediction Accuracy = 62.7%, Loss = 0.40565834641456605
Epoch: 6719, Batch Gradient Norm: 10.603113542837832
Epoch: 6719, Batch Gradient Norm after: 10.603113542837832
Epoch 6720/10000, Prediction Accuracy = 62.49399999999999%, Loss = 0.4080928146839142
Epoch: 6720, Batch Gradient Norm: 9.825429908188756
Epoch: 6720, Batch Gradient Norm after: 9.825429908188756
Epoch 6721/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.40319752097129824
Epoch: 6721, Batch Gradient Norm: 10.337409202946695
Epoch: 6721, Batch Gradient Norm after: 10.337409202946695
Epoch 6722/10000, Prediction Accuracy = 62.604%, Loss = 0.4063475489616394
Epoch: 6722, Batch Gradient Norm: 12.021452586071318
Epoch: 6722, Batch Gradient Norm after: 12.021452586071318
Epoch 6723/10000, Prediction Accuracy = 62.682%, Loss = 0.4177880585193634
Epoch: 6723, Batch Gradient Norm: 11.415075206442992
Epoch: 6723, Batch Gradient Norm after: 11.415075206442992
Epoch 6724/10000, Prediction Accuracy = 62.698%, Loss = 0.4136047899723053
Epoch: 6724, Batch Gradient Norm: 8.586660286499592
Epoch: 6724, Batch Gradient Norm after: 8.586660286499592
Epoch 6725/10000, Prediction Accuracy = 62.612%, Loss = 0.3948256134986877
Epoch: 6725, Batch Gradient Norm: 7.591650535838649
Epoch: 6725, Batch Gradient Norm after: 7.591650535838649
Epoch 6726/10000, Prediction Accuracy = 62.75%, Loss = 0.38965936899185183
Epoch: 6726, Batch Gradient Norm: 8.24130738256402
Epoch: 6726, Batch Gradient Norm after: 8.24130738256402
Epoch 6727/10000, Prediction Accuracy = 62.56%, Loss = 0.3928521633148193
Epoch: 6727, Batch Gradient Norm: 9.441553237430295
Epoch: 6727, Batch Gradient Norm after: 9.441553237430295
Epoch 6728/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.4011552810668945
Epoch: 6728, Batch Gradient Norm: 9.81473141927481
Epoch: 6728, Batch Gradient Norm after: 9.81473141927481
Epoch 6729/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.40385217070579527
Epoch: 6729, Batch Gradient Norm: 9.87366777847001
Epoch: 6729, Batch Gradient Norm after: 9.87366777847001
Epoch 6730/10000, Prediction Accuracy = 62.416%, Loss = 0.40461097955703734
Epoch: 6730, Batch Gradient Norm: 10.660421691485537
Epoch: 6730, Batch Gradient Norm after: 10.660421691485537
Epoch 6731/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.4097572386264801
Epoch: 6731, Batch Gradient Norm: 11.372081361038394
Epoch: 6731, Batch Gradient Norm after: 11.372081361038394
Epoch 6732/10000, Prediction Accuracy = 62.5%, Loss = 0.4142707884311676
Epoch: 6732, Batch Gradient Norm: 10.200700644399099
Epoch: 6732, Batch Gradient Norm after: 10.200700644399099
Epoch 6733/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.40661635994911194
Epoch: 6733, Batch Gradient Norm: 11.0217605490264
Epoch: 6733, Batch Gradient Norm after: 11.0217605490264
Epoch 6734/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.4121597170829773
Epoch: 6734, Batch Gradient Norm: 12.296760072467599
Epoch: 6734, Batch Gradient Norm after: 12.296760072467599
Epoch 6735/10000, Prediction Accuracy = 62.61%, Loss = 0.422359836101532
Epoch: 6735, Batch Gradient Norm: 10.163568412405702
Epoch: 6735, Batch Gradient Norm after: 10.163568412405702
Epoch 6736/10000, Prediction Accuracy = 62.519999999999996%, Loss = 0.4052913308143616
Epoch: 6736, Batch Gradient Norm: 8.418582128950206
Epoch: 6736, Batch Gradient Norm after: 8.418582128950206
Epoch 6737/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.3938897609710693
Epoch: 6737, Batch Gradient Norm: 7.637734680558099
Epoch: 6737, Batch Gradient Norm after: 7.637734680558099
Epoch 6738/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.38962390422821047
Epoch: 6738, Batch Gradient Norm: 8.972646344558447
Epoch: 6738, Batch Gradient Norm after: 8.972646344558447
Epoch 6739/10000, Prediction Accuracy = 62.63800000000001%, Loss = 0.39694944024086
Epoch: 6739, Batch Gradient Norm: 11.063957795847246
Epoch: 6739, Batch Gradient Norm after: 11.063957795847246
Epoch 6740/10000, Prediction Accuracy = 62.588%, Loss = 0.4109382688999176
Epoch: 6740, Batch Gradient Norm: 11.319391416699824
Epoch: 6740, Batch Gradient Norm after: 11.319391416699824
Epoch 6741/10000, Prediction Accuracy = 62.513999999999996%, Loss = 0.4121315240859985
Epoch: 6741, Batch Gradient Norm: 10.339870709225542
Epoch: 6741, Batch Gradient Norm after: 10.339870709225542
Epoch 6742/10000, Prediction Accuracy = 62.57000000000001%, Loss = 0.4059339702129364
Epoch: 6742, Batch Gradient Norm: 9.396387005576981
Epoch: 6742, Batch Gradient Norm after: 9.396387005576981
Epoch 6743/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.39933453798294066
Epoch: 6743, Batch Gradient Norm: 10.478801625004463
Epoch: 6743, Batch Gradient Norm after: 10.478801625004463
Epoch 6744/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.40623396039009096
Epoch: 6744, Batch Gradient Norm: 10.632973167720836
Epoch: 6744, Batch Gradient Norm after: 10.632973167720836
Epoch 6745/10000, Prediction Accuracy = 62.721999999999994%, Loss = 0.40812593698501587
Epoch: 6745, Batch Gradient Norm: 10.61640546163364
Epoch: 6745, Batch Gradient Norm after: 10.61640546163364
Epoch 6746/10000, Prediction Accuracy = 62.645999999999994%, Loss = 0.40792426466941833
Epoch: 6746, Batch Gradient Norm: 10.742269739721031
Epoch: 6746, Batch Gradient Norm after: 10.742269739721031
Epoch 6747/10000, Prediction Accuracy = 62.572%, Loss = 0.40810348391532897
Epoch: 6747, Batch Gradient Norm: 10.748376881331632
Epoch: 6747, Batch Gradient Norm after: 10.748376881331632
Epoch 6748/10000, Prediction Accuracy = 62.69200000000001%, Loss = 0.40770804286003115
Epoch: 6748, Batch Gradient Norm: 10.557276474275255
Epoch: 6748, Batch Gradient Norm after: 10.557276474275255
Epoch 6749/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.40763217210769653
Epoch: 6749, Batch Gradient Norm: 9.422222112470545
Epoch: 6749, Batch Gradient Norm after: 9.422222112470545
Epoch 6750/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.40096426010131836
Epoch: 6750, Batch Gradient Norm: 8.477830209337204
Epoch: 6750, Batch Gradient Norm after: 8.477830209337204
Epoch 6751/10000, Prediction Accuracy = 62.500000000000014%, Loss = 0.3945896565914154
Epoch: 6751, Batch Gradient Norm: 9.575008176646538
Epoch: 6751, Batch Gradient Norm after: 9.575008176646538
Epoch 6752/10000, Prediction Accuracy = 62.626%, Loss = 0.40163275599479675
Epoch: 6752, Batch Gradient Norm: 9.297188191802988
Epoch: 6752, Batch Gradient Norm after: 9.297188191802988
Epoch 6753/10000, Prediction Accuracy = 62.544000000000004%, Loss = 0.4010246813297272
Epoch: 6753, Batch Gradient Norm: 10.01375918058559
Epoch: 6753, Batch Gradient Norm after: 10.01375918058559
Epoch 6754/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.4038629710674286
Epoch: 6754, Batch Gradient Norm: 11.999595973571482
Epoch: 6754, Batch Gradient Norm after: 11.999595973571482
Epoch 6755/10000, Prediction Accuracy = 62.54%, Loss = 0.4183175444602966
Epoch: 6755, Batch Gradient Norm: 11.22049695379223
Epoch: 6755, Batch Gradient Norm after: 11.22049695379223
Epoch 6756/10000, Prediction Accuracy = 62.516000000000005%, Loss = 0.41335068345069886
Epoch: 6756, Batch Gradient Norm: 9.622507287536669
Epoch: 6756, Batch Gradient Norm after: 9.622507287536669
Epoch 6757/10000, Prediction Accuracy = 62.634%, Loss = 0.4024646520614624
Epoch: 6757, Batch Gradient Norm: 8.24158505582818
Epoch: 6757, Batch Gradient Norm after: 8.24158505582818
Epoch 6758/10000, Prediction Accuracy = 62.702%, Loss = 0.3940966069698334
Epoch: 6758, Batch Gradient Norm: 7.620513626849385
Epoch: 6758, Batch Gradient Norm after: 7.620513626849385
Epoch 6759/10000, Prediction Accuracy = 62.638%, Loss = 0.3900338411331177
Epoch: 6759, Batch Gradient Norm: 8.010379043098462
Epoch: 6759, Batch Gradient Norm after: 8.010379043098462
Epoch 6760/10000, Prediction Accuracy = 62.628%, Loss = 0.3917921006679535
Epoch: 6760, Batch Gradient Norm: 8.32335146797113
Epoch: 6760, Batch Gradient Norm after: 8.32335146797113
Epoch 6761/10000, Prediction Accuracy = 62.646%, Loss = 0.39313170313835144
Epoch: 6761, Batch Gradient Norm: 11.975985791946094
Epoch: 6761, Batch Gradient Norm after: 11.975985791946094
Epoch 6762/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.41887396574020386
Epoch: 6762, Batch Gradient Norm: 12.927634328114083
Epoch: 6762, Batch Gradient Norm after: 12.927634328114083
Epoch 6763/10000, Prediction Accuracy = 62.624%, Loss = 0.42758243083953856
Epoch: 6763, Batch Gradient Norm: 11.069823683739855
Epoch: 6763, Batch Gradient Norm after: 11.069823683739855
Epoch 6764/10000, Prediction Accuracy = 62.528%, Loss = 0.4096443772315979
Epoch: 6764, Batch Gradient Norm: 10.58276670298385
Epoch: 6764, Batch Gradient Norm after: 10.58276670298385
Epoch 6765/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.40705623626708987
Epoch: 6765, Batch Gradient Norm: 8.912763695313199
Epoch: 6765, Batch Gradient Norm after: 8.912763695313199
Epoch 6766/10000, Prediction Accuracy = 62.686000000000014%, Loss = 0.39617114663124087
Epoch: 6766, Batch Gradient Norm: 8.758275235341092
Epoch: 6766, Batch Gradient Norm after: 8.758275235341092
Epoch 6767/10000, Prediction Accuracy = 62.80799999999999%, Loss = 0.3956881105899811
Epoch: 6767, Batch Gradient Norm: 8.934318804778393
Epoch: 6767, Batch Gradient Norm after: 8.934318804778393
Epoch 6768/10000, Prediction Accuracy = 62.702%, Loss = 0.3964486598968506
Epoch: 6768, Batch Gradient Norm: 10.046359911598564
Epoch: 6768, Batch Gradient Norm after: 10.046359911598564
Epoch 6769/10000, Prediction Accuracy = 62.52%, Loss = 0.4043114960193634
Epoch: 6769, Batch Gradient Norm: 9.264447089875135
Epoch: 6769, Batch Gradient Norm after: 9.264447089875135
Epoch 6770/10000, Prediction Accuracy = 62.69%, Loss = 0.39963883757591245
Epoch: 6770, Batch Gradient Norm: 10.286748980982297
Epoch: 6770, Batch Gradient Norm after: 10.286748980982297
Epoch 6771/10000, Prediction Accuracy = 62.548%, Loss = 0.4077727735042572
Epoch: 6771, Batch Gradient Norm: 11.015601204540406
Epoch: 6771, Batch Gradient Norm after: 11.015601204540406
Epoch 6772/10000, Prediction Accuracy = 62.556%, Loss = 0.41250426769256593
Epoch: 6772, Batch Gradient Norm: 12.68153874410334
Epoch: 6772, Batch Gradient Norm after: 12.68153874410334
Epoch 6773/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.4226448893547058
Epoch: 6773, Batch Gradient Norm: 12.034912251895033
Epoch: 6773, Batch Gradient Norm after: 12.034912251895033
Epoch 6774/10000, Prediction Accuracy = 62.55400000000001%, Loss = 0.41763784289360045
Epoch: 6774, Batch Gradient Norm: 9.191823903919829
Epoch: 6774, Batch Gradient Norm after: 9.191823903919829
Epoch 6775/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3986422657966614
Epoch: 6775, Batch Gradient Norm: 8.309921465970245
Epoch: 6775, Batch Gradient Norm after: 8.309921465970245
Epoch 6776/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3926068127155304
Epoch: 6776, Batch Gradient Norm: 9.616481138691581
Epoch: 6776, Batch Gradient Norm after: 9.616481138691581
Epoch 6777/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.3996365964412689
Epoch: 6777, Batch Gradient Norm: 9.980081363096916
Epoch: 6777, Batch Gradient Norm after: 9.980081363096916
Epoch 6778/10000, Prediction Accuracy = 62.622%, Loss = 0.402331680059433
Epoch: 6778, Batch Gradient Norm: 9.740475775418538
Epoch: 6778, Batch Gradient Norm after: 9.740475775418538
Epoch 6779/10000, Prediction Accuracy = 62.58200000000001%, Loss = 0.40151074528694153
Epoch: 6779, Batch Gradient Norm: 10.559696972582808
Epoch: 6779, Batch Gradient Norm after: 10.559696972582808
Epoch 6780/10000, Prediction Accuracy = 62.538%, Loss = 0.4094666659832001
Epoch: 6780, Batch Gradient Norm: 9.344866733289926
Epoch: 6780, Batch Gradient Norm after: 9.344866733289926
Epoch 6781/10000, Prediction Accuracy = 62.548%, Loss = 0.40156391859054563
Epoch: 6781, Batch Gradient Norm: 8.364484405835043
Epoch: 6781, Batch Gradient Norm after: 8.364484405835043
Epoch 6782/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.39339019656181334
Epoch: 6782, Batch Gradient Norm: 9.094844803714006
Epoch: 6782, Batch Gradient Norm after: 9.094844803714006
Epoch 6783/10000, Prediction Accuracy = 62.614%, Loss = 0.3974982976913452
Epoch: 6783, Batch Gradient Norm: 9.606192140271503
Epoch: 6783, Batch Gradient Norm after: 9.606192140271503
Epoch 6784/10000, Prediction Accuracy = 62.64%, Loss = 0.4014484047889709
Epoch: 6784, Batch Gradient Norm: 11.825535600019048
Epoch: 6784, Batch Gradient Norm after: 11.825535600019048
Epoch 6785/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.41739606857299805
Epoch: 6785, Batch Gradient Norm: 14.262278515187925
Epoch: 6785, Batch Gradient Norm after: 14.262278515187925
Epoch 6786/10000, Prediction Accuracy = 62.448%, Loss = 0.43763235211372375
Epoch: 6786, Batch Gradient Norm: 11.455395175869112
Epoch: 6786, Batch Gradient Norm after: 11.455395175869112
Epoch 6787/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.4143100678920746
Epoch: 6787, Batch Gradient Norm: 8.751330391688716
Epoch: 6787, Batch Gradient Norm after: 8.751330391688716
Epoch 6788/10000, Prediction Accuracy = 62.652%, Loss = 0.3953231513500214
Epoch: 6788, Batch Gradient Norm: 7.865466159701107
Epoch: 6788, Batch Gradient Norm after: 7.865466159701107
Epoch 6789/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.3902758240699768
Epoch: 6789, Batch Gradient Norm: 8.06154223120691
Epoch: 6789, Batch Gradient Norm after: 8.06154223120691
Epoch 6790/10000, Prediction Accuracy = 62.562%, Loss = 0.39082934856414797
Epoch: 6790, Batch Gradient Norm: 9.123658684330039
Epoch: 6790, Batch Gradient Norm after: 9.123658684330039
Epoch 6791/10000, Prediction Accuracy = 62.524%, Loss = 0.3959734320640564
Epoch: 6791, Batch Gradient Norm: 10.657111608496741
Epoch: 6791, Batch Gradient Norm after: 10.657111608496741
Epoch 6792/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.4055473983287811
Epoch: 6792, Batch Gradient Norm: 12.16227279550022
Epoch: 6792, Batch Gradient Norm after: 12.16227279550022
Epoch 6793/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.41759857535362244
Epoch: 6793, Batch Gradient Norm: 10.164633197660997
Epoch: 6793, Batch Gradient Norm after: 10.164633197660997
Epoch 6794/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.4045882046222687
Epoch: 6794, Batch Gradient Norm: 9.270950196623685
Epoch: 6794, Batch Gradient Norm after: 9.270950196623685
Epoch 6795/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.39947490096092225
Epoch: 6795, Batch Gradient Norm: 10.30551465050942
Epoch: 6795, Batch Gradient Norm after: 10.30551465050942
Epoch 6796/10000, Prediction Accuracy = 62.71%, Loss = 0.4052074432373047
Epoch: 6796, Batch Gradient Norm: 11.595854201101881
Epoch: 6796, Batch Gradient Norm after: 11.595854201101881
Epoch 6797/10000, Prediction Accuracy = 62.61%, Loss = 0.4152549088001251
Epoch: 6797, Batch Gradient Norm: 9.351010604048597
Epoch: 6797, Batch Gradient Norm after: 9.351010604048597
Epoch 6798/10000, Prediction Accuracy = 62.726%, Loss = 0.3990833342075348
Epoch: 6798, Batch Gradient Norm: 8.034528925340775
Epoch: 6798, Batch Gradient Norm after: 8.034528925340775
Epoch 6799/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.39188143610954285
Epoch: 6799, Batch Gradient Norm: 8.679346347824575
Epoch: 6799, Batch Gradient Norm after: 8.679346347824575
Epoch 6800/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.39595823287963866
Epoch: 6800, Batch Gradient Norm: 9.772421872367982
Epoch: 6800, Batch Gradient Norm after: 9.772421872367982
Epoch 6801/10000, Prediction Accuracy = 62.676%, Loss = 0.4031999588012695
Epoch: 6801, Batch Gradient Norm: 9.98467163804598
Epoch: 6801, Batch Gradient Norm after: 9.98467163804598
Epoch 6802/10000, Prediction Accuracy = 62.65%, Loss = 0.40365787148475646
Epoch: 6802, Batch Gradient Norm: 8.730160924218263
Epoch: 6802, Batch Gradient Norm after: 8.730160924218263
Epoch 6803/10000, Prediction Accuracy = 62.592000000000006%, Loss = 0.39468276500701904
Epoch: 6803, Batch Gradient Norm: 10.002874502400784
Epoch: 6803, Batch Gradient Norm after: 10.002874502400784
Epoch 6804/10000, Prediction Accuracy = 62.70200000000001%, Loss = 0.4021909713745117
Epoch: 6804, Batch Gradient Norm: 10.503787017589216
Epoch: 6804, Batch Gradient Norm after: 10.503787017589216
Epoch 6805/10000, Prediction Accuracy = 62.638%, Loss = 0.40727949142456055
Epoch: 6805, Batch Gradient Norm: 9.013056554724315
Epoch: 6805, Batch Gradient Norm after: 9.013056554724315
Epoch 6806/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.39803227186203005
Epoch: 6806, Batch Gradient Norm: 9.834648300674592
Epoch: 6806, Batch Gradient Norm after: 9.834648300674592
Epoch 6807/10000, Prediction Accuracy = 62.598%, Loss = 0.40138739347457886
Epoch: 6807, Batch Gradient Norm: 12.226827901343144
Epoch: 6807, Batch Gradient Norm after: 12.226827901343144
Epoch 6808/10000, Prediction Accuracy = 62.338%, Loss = 0.4186189293861389
Epoch: 6808, Batch Gradient Norm: 11.679536017355064
Epoch: 6808, Batch Gradient Norm after: 11.679536017355064
Epoch 6809/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.41469995975494384
Epoch: 6809, Batch Gradient Norm: 10.058941383171671
Epoch: 6809, Batch Gradient Norm after: 10.058941383171671
Epoch 6810/10000, Prediction Accuracy = 62.57800000000001%, Loss = 0.40288220047950746
Epoch: 6810, Batch Gradient Norm: 10.623914521591002
Epoch: 6810, Batch Gradient Norm after: 10.623914521591002
Epoch 6811/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.40910778641700746
Epoch: 6811, Batch Gradient Norm: 11.13872614571425
Epoch: 6811, Batch Gradient Norm after: 11.13872614571425
Epoch 6812/10000, Prediction Accuracy = 62.54600000000001%, Loss = 0.4116710960865021
Epoch: 6812, Batch Gradient Norm: 10.94998458993677
Epoch: 6812, Batch Gradient Norm after: 10.94998458993677
Epoch 6813/10000, Prediction Accuracy = 62.44%, Loss = 0.4098750054836273
Epoch: 6813, Batch Gradient Norm: 10.009447408837232
Epoch: 6813, Batch Gradient Norm after: 10.009447408837232
Epoch 6814/10000, Prediction Accuracy = 62.465999999999994%, Loss = 0.4030560553073883
Epoch: 6814, Batch Gradient Norm: 10.787047340391982
Epoch: 6814, Batch Gradient Norm after: 10.787047340391982
Epoch 6815/10000, Prediction Accuracy = 62.55799999999999%, Loss = 0.40834025740623475
Epoch: 6815, Batch Gradient Norm: 10.593205254080294
Epoch: 6815, Batch Gradient Norm after: 10.593205254080294
Epoch 6816/10000, Prediction Accuracy = 62.662%, Loss = 0.40655245184898375
Epoch: 6816, Batch Gradient Norm: 9.570601807767115
Epoch: 6816, Batch Gradient Norm after: 9.570601807767115
Epoch 6817/10000, Prediction Accuracy = 62.686%, Loss = 0.398474782705307
Epoch: 6817, Batch Gradient Norm: 9.24656852475604
Epoch: 6817, Batch Gradient Norm after: 9.24656852475604
Epoch 6818/10000, Prediction Accuracy = 62.57600000000001%, Loss = 0.39637952446937563
Epoch: 6818, Batch Gradient Norm: 9.922632086416002
Epoch: 6818, Batch Gradient Norm after: 9.922632086416002
Epoch 6819/10000, Prediction Accuracy = 62.68399999999999%, Loss = 0.4014327347278595
Epoch: 6819, Batch Gradient Norm: 9.660092783392502
Epoch: 6819, Batch Gradient Norm after: 9.660092783392502
Epoch 6820/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.4005741477012634
Epoch: 6820, Batch Gradient Norm: 9.640857188491903
Epoch: 6820, Batch Gradient Norm after: 9.640857188491903
Epoch 6821/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.4000963866710663
Epoch: 6821, Batch Gradient Norm: 10.863445824399093
Epoch: 6821, Batch Gradient Norm after: 10.863445824399093
Epoch 6822/10000, Prediction Accuracy = 62.682%, Loss = 0.40750807523727417
Epoch: 6822, Batch Gradient Norm: 11.068686210941095
Epoch: 6822, Batch Gradient Norm after: 11.068686210941095
Epoch 6823/10000, Prediction Accuracy = 62.588%, Loss = 0.40955824255943296
Epoch: 6823, Batch Gradient Norm: 9.332244198351255
Epoch: 6823, Batch Gradient Norm after: 9.332244198351255
Epoch 6824/10000, Prediction Accuracy = 62.702%, Loss = 0.3992809057235718
Epoch: 6824, Batch Gradient Norm: 8.01809735311411
Epoch: 6824, Batch Gradient Norm after: 8.01809735311411
Epoch 6825/10000, Prediction Accuracy = 62.712%, Loss = 0.3916176676750183
Epoch: 6825, Batch Gradient Norm: 8.309503739227154
Epoch: 6825, Batch Gradient Norm after: 8.309503739227154
Epoch 6826/10000, Prediction Accuracy = 62.57800000000001%, Loss = 0.39212610125541686
Epoch: 6826, Batch Gradient Norm: 10.00291376075087
Epoch: 6826, Batch Gradient Norm after: 10.00291376075087
Epoch 6827/10000, Prediction Accuracy = 62.69000000000001%, Loss = 0.4007857084274292
Epoch: 6827, Batch Gradient Norm: 11.347161928812277
Epoch: 6827, Batch Gradient Norm after: 11.347161928812277
Epoch 6828/10000, Prediction Accuracy = 62.41400000000001%, Loss = 0.411143559217453
Epoch: 6828, Batch Gradient Norm: 9.729080047368495
Epoch: 6828, Batch Gradient Norm after: 9.729080047368495
Epoch 6829/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.40161730647087096
Epoch: 6829, Batch Gradient Norm: 9.95643880167609
Epoch: 6829, Batch Gradient Norm after: 9.95643880167609
Epoch 6830/10000, Prediction Accuracy = 62.61600000000001%, Loss = 0.40321995615959166
Epoch: 6830, Batch Gradient Norm: 11.15452576736472
Epoch: 6830, Batch Gradient Norm after: 11.15452576736472
Epoch 6831/10000, Prediction Accuracy = 62.602%, Loss = 0.4128598034381866
Epoch: 6831, Batch Gradient Norm: 9.688356731414407
Epoch: 6831, Batch Gradient Norm after: 9.688356731414407
Epoch 6832/10000, Prediction Accuracy = 62.688%, Loss = 0.4016815185546875
Epoch: 6832, Batch Gradient Norm: 9.310031875916343
Epoch: 6832, Batch Gradient Norm after: 9.310031875916343
Epoch 6833/10000, Prediction Accuracy = 62.584%, Loss = 0.39881196022033694
Epoch: 6833, Batch Gradient Norm: 10.40447738318387
Epoch: 6833, Batch Gradient Norm after: 10.40447738318387
Epoch 6834/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.4047958791255951
Epoch: 6834, Batch Gradient Norm: 12.98456446856549
Epoch: 6834, Batch Gradient Norm after: 12.98456446856549
Epoch 6835/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.4246958792209625
Epoch: 6835, Batch Gradient Norm: 10.654081916168023
Epoch: 6835, Batch Gradient Norm after: 10.654081916168023
Epoch 6836/10000, Prediction Accuracy = 62.614%, Loss = 0.4060900866985321
Epoch: 6836, Batch Gradient Norm: 8.742296912422093
Epoch: 6836, Batch Gradient Norm after: 8.742296912422093
Epoch 6837/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.393347841501236
Epoch: 6837, Batch Gradient Norm: 8.826524917007784
Epoch: 6837, Batch Gradient Norm after: 8.826524917007784
Epoch 6838/10000, Prediction Accuracy = 62.592000000000006%, Loss = 0.39406713247299197
Epoch: 6838, Batch Gradient Norm: 10.524870182394096
Epoch: 6838, Batch Gradient Norm after: 10.524870182394096
Epoch 6839/10000, Prediction Accuracy = 62.676%, Loss = 0.40732375383377073
Epoch: 6839, Batch Gradient Norm: 10.519405686715654
Epoch: 6839, Batch Gradient Norm after: 10.519405686715654
Epoch 6840/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.4069545030593872
Epoch: 6840, Batch Gradient Norm: 10.95488708315411
Epoch: 6840, Batch Gradient Norm after: 10.95488708315411
Epoch 6841/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.4086742579936981
Epoch: 6841, Batch Gradient Norm: 10.494733064285596
Epoch: 6841, Batch Gradient Norm after: 10.494733064285596
Epoch 6842/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.4057159245014191
Epoch: 6842, Batch Gradient Norm: 10.0997439939272
Epoch: 6842, Batch Gradient Norm after: 10.0997439939272
Epoch 6843/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.40371946096420286
Epoch: 6843, Batch Gradient Norm: 10.284258820456266
Epoch: 6843, Batch Gradient Norm after: 10.284258820456266
Epoch 6844/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.40362986326217654
Epoch: 6844, Batch Gradient Norm: 10.659001995862146
Epoch: 6844, Batch Gradient Norm after: 10.659001995862146
Epoch 6845/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.4050478935241699
Epoch: 6845, Batch Gradient Norm: 10.625554010632985
Epoch: 6845, Batch Gradient Norm after: 10.625554010632985
Epoch 6846/10000, Prediction Accuracy = 62.688%, Loss = 0.4053521752357483
Epoch: 6846, Batch Gradient Norm: 9.26156897467383
Epoch: 6846, Batch Gradient Norm after: 9.26156897467383
Epoch 6847/10000, Prediction Accuracy = 62.702%, Loss = 0.3972472012042999
Epoch: 6847, Batch Gradient Norm: 8.142393776861585
Epoch: 6847, Batch Gradient Norm after: 8.142393776861585
Epoch 6848/10000, Prediction Accuracy = 62.664%, Loss = 0.39057347774505613
Epoch: 6848, Batch Gradient Norm: 8.633411257752835
Epoch: 6848, Batch Gradient Norm after: 8.633411257752835
Epoch 6849/10000, Prediction Accuracy = 62.7%, Loss = 0.3932820498943329
Epoch: 6849, Batch Gradient Norm: 8.983804265374104
Epoch: 6849, Batch Gradient Norm after: 8.983804265374104
Epoch 6850/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3957907557487488
Epoch: 6850, Batch Gradient Norm: 9.16643615896795
Epoch: 6850, Batch Gradient Norm after: 9.16643615896795
Epoch 6851/10000, Prediction Accuracy = 62.746%, Loss = 0.3978449165821075
Epoch: 6851, Batch Gradient Norm: 9.897142024470437
Epoch: 6851, Batch Gradient Norm after: 9.897142024470437
Epoch 6852/10000, Prediction Accuracy = 62.614%, Loss = 0.4014004826545715
Epoch: 6852, Batch Gradient Norm: 10.747124914250955
Epoch: 6852, Batch Gradient Norm after: 10.747124914250955
Epoch 6853/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4061024248600006
Epoch: 6853, Batch Gradient Norm: 11.749067721428627
Epoch: 6853, Batch Gradient Norm after: 11.749067721428627
Epoch 6854/10000, Prediction Accuracy = 62.648%, Loss = 0.41266060471534727
Epoch: 6854, Batch Gradient Norm: 12.060044760102906
Epoch: 6854, Batch Gradient Norm after: 12.060044760102906
Epoch 6855/10000, Prediction Accuracy = 62.598%, Loss = 0.418296879529953
Epoch: 6855, Batch Gradient Norm: 10.54849184736905
Epoch: 6855, Batch Gradient Norm after: 10.54849184736905
Epoch 6856/10000, Prediction Accuracy = 62.662%, Loss = 0.4067257225513458
Epoch: 6856, Batch Gradient Norm: 10.21193396952616
Epoch: 6856, Batch Gradient Norm after: 10.21193396952616
Epoch 6857/10000, Prediction Accuracy = 62.55999999999999%, Loss = 0.4025186598300934
Epoch: 6857, Batch Gradient Norm: 10.646497411552286
Epoch: 6857, Batch Gradient Norm after: 10.646497411552286
Epoch 6858/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.4049108445644379
Epoch: 6858, Batch Gradient Norm: 9.760407700904576
Epoch: 6858, Batch Gradient Norm after: 9.760407700904576
Epoch 6859/10000, Prediction Accuracy = 62.536%, Loss = 0.3992246985435486
Epoch: 6859, Batch Gradient Norm: 9.82800714550067
Epoch: 6859, Batch Gradient Norm after: 9.82800714550067
Epoch 6860/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.4004197418689728
Epoch: 6860, Batch Gradient Norm: 10.66996435666848
Epoch: 6860, Batch Gradient Norm after: 10.66996435666848
Epoch 6861/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.40897789001464846
Epoch: 6861, Batch Gradient Norm: 9.222873032827314
Epoch: 6861, Batch Gradient Norm after: 9.222873032827314
Epoch 6862/10000, Prediction Accuracy = 62.638%, Loss = 0.39883412718772887
Epoch: 6862, Batch Gradient Norm: 9.763206436421855
Epoch: 6862, Batch Gradient Norm after: 9.763206436421855
Epoch 6863/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.4003965318202972
Epoch: 6863, Batch Gradient Norm: 9.420286726665678
Epoch: 6863, Batch Gradient Norm after: 9.420286726665678
Epoch 6864/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.39768378138542176
Epoch: 6864, Batch Gradient Norm: 8.357998007498296
Epoch: 6864, Batch Gradient Norm after: 8.357998007498296
Epoch 6865/10000, Prediction Accuracy = 62.612%, Loss = 0.39167059063911436
Epoch: 6865, Batch Gradient Norm: 7.876143884799866
Epoch: 6865, Batch Gradient Norm after: 7.876143884799866
Epoch 6866/10000, Prediction Accuracy = 62.75%, Loss = 0.3894307017326355
Epoch: 6866, Batch Gradient Norm: 8.877019317910854
Epoch: 6866, Batch Gradient Norm after: 8.877019317910854
Epoch 6867/10000, Prediction Accuracy = 62.61199999999999%, Loss = 0.39580742716789247
Epoch: 6867, Batch Gradient Norm: 10.773038037606234
Epoch: 6867, Batch Gradient Norm after: 10.773038037606234
Epoch 6868/10000, Prediction Accuracy = 62.7%, Loss = 0.40845186114311216
Epoch: 6868, Batch Gradient Norm: 9.420587984329666
Epoch: 6868, Batch Gradient Norm after: 9.420587984329666
Epoch 6869/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.39924595952034
Epoch: 6869, Batch Gradient Norm: 9.7618439116404
Epoch: 6869, Batch Gradient Norm after: 9.7618439116404
Epoch 6870/10000, Prediction Accuracy = 62.75599999999999%, Loss = 0.4006330192089081
Epoch: 6870, Batch Gradient Norm: 12.456036966646037
Epoch: 6870, Batch Gradient Norm after: 12.456036966646037
Epoch 6871/10000, Prediction Accuracy = 62.612%, Loss = 0.41858711242675783
Epoch: 6871, Batch Gradient Norm: 11.78191728941726
Epoch: 6871, Batch Gradient Norm after: 11.78191728941726
Epoch 6872/10000, Prediction Accuracy = 62.55%, Loss = 0.41370757222175597
Epoch: 6872, Batch Gradient Norm: 9.167941386829039
Epoch: 6872, Batch Gradient Norm after: 9.167941386829039
Epoch 6873/10000, Prediction Accuracy = 62.652%, Loss = 0.396712464094162
Epoch: 6873, Batch Gradient Norm: 9.520511209793634
Epoch: 6873, Batch Gradient Norm after: 9.520511209793634
Epoch 6874/10000, Prediction Accuracy = 62.548%, Loss = 0.3991153299808502
Epoch: 6874, Batch Gradient Norm: 13.273575826242329
Epoch: 6874, Batch Gradient Norm after: 13.273575826242329
Epoch 6875/10000, Prediction Accuracy = 62.65%, Loss = 0.4266140580177307
Epoch: 6875, Batch Gradient Norm: 12.224346663507177
Epoch: 6875, Batch Gradient Norm after: 12.224346663507177
Epoch 6876/10000, Prediction Accuracy = 62.77%, Loss = 0.4178847908973694
Epoch: 6876, Batch Gradient Norm: 9.001706661660956
Epoch: 6876, Batch Gradient Norm after: 9.001706661660956
Epoch 6877/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.39516185522079467
Epoch: 6877, Batch Gradient Norm: 10.018389916565194
Epoch: 6877, Batch Gradient Norm after: 10.018389916565194
Epoch 6878/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.402332067489624
Epoch: 6878, Batch Gradient Norm: 10.05546091125933
Epoch: 6878, Batch Gradient Norm after: 10.05546091125933
Epoch 6879/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.40347002148628236
Epoch: 6879, Batch Gradient Norm: 8.119251116401974
Epoch: 6879, Batch Gradient Norm after: 8.119251116401974
Epoch 6880/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3904090642929077
Epoch: 6880, Batch Gradient Norm: 9.301117279685009
Epoch: 6880, Batch Gradient Norm after: 9.301117279685009
Epoch 6881/10000, Prediction Accuracy = 62.702%, Loss = 0.39608694314956666
Epoch: 6881, Batch Gradient Norm: 12.069189401123474
Epoch: 6881, Batch Gradient Norm after: 12.069189401123474
Epoch 6882/10000, Prediction Accuracy = 62.524%, Loss = 0.41627241373062135
Epoch: 6882, Batch Gradient Norm: 11.19992583688523
Epoch: 6882, Batch Gradient Norm after: 11.19992583688523
Epoch 6883/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.41129823327064513
Epoch: 6883, Batch Gradient Norm: 8.757981915715769
Epoch: 6883, Batch Gradient Norm after: 8.757981915715769
Epoch 6884/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.39319908618927
Epoch: 6884, Batch Gradient Norm: 7.697832724814304
Epoch: 6884, Batch Gradient Norm after: 7.697832724814304
Epoch 6885/10000, Prediction Accuracy = 62.574%, Loss = 0.3867507755756378
Epoch: 6885, Batch Gradient Norm: 8.136775711555826
Epoch: 6885, Batch Gradient Norm after: 8.136775711555826
Epoch 6886/10000, Prediction Accuracy = 62.866%, Loss = 0.3897303342819214
Epoch: 6886, Batch Gradient Norm: 9.872076776767356
Epoch: 6886, Batch Gradient Norm after: 9.872076776767356
Epoch 6887/10000, Prediction Accuracy = 62.63399999999999%, Loss = 0.4006804943084717
Epoch: 6887, Batch Gradient Norm: 11.211590494827703
Epoch: 6887, Batch Gradient Norm after: 11.211590494827703
Epoch 6888/10000, Prediction Accuracy = 62.648%, Loss = 0.40991904139518737
Epoch: 6888, Batch Gradient Norm: 10.982635115173377
Epoch: 6888, Batch Gradient Norm after: 10.982635115173377
Epoch 6889/10000, Prediction Accuracy = 62.598%, Loss = 0.40718351006507875
Epoch: 6889, Batch Gradient Norm: 9.275708868578857
Epoch: 6889, Batch Gradient Norm after: 9.275708868578857
Epoch 6890/10000, Prediction Accuracy = 62.562%, Loss = 0.39593226313591
Epoch: 6890, Batch Gradient Norm: 9.31052791739469
Epoch: 6890, Batch Gradient Norm after: 9.31052791739469
Epoch 6891/10000, Prediction Accuracy = 62.758%, Loss = 0.3977652251720428
Epoch: 6891, Batch Gradient Norm: 9.045270481247767
Epoch: 6891, Batch Gradient Norm after: 9.045270481247767
Epoch 6892/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.39633195996284487
Epoch: 6892, Batch Gradient Norm: 9.981819336031046
Epoch: 6892, Batch Gradient Norm after: 9.981819336031046
Epoch 6893/10000, Prediction Accuracy = 62.682%, Loss = 0.40116071701049805
Epoch: 6893, Batch Gradient Norm: 11.701763742652577
Epoch: 6893, Batch Gradient Norm after: 11.701763742652577
Epoch 6894/10000, Prediction Accuracy = 62.525999999999996%, Loss = 0.4128209114074707
Epoch: 6894, Batch Gradient Norm: 11.633282679888683
Epoch: 6894, Batch Gradient Norm after: 11.633282679888683
Epoch 6895/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.41286085844039916
Epoch: 6895, Batch Gradient Norm: 11.402429753851715
Epoch: 6895, Batch Gradient Norm after: 11.402429753851715
Epoch 6896/10000, Prediction Accuracy = 62.577999999999996%, Loss = 0.41111708283424375
Epoch: 6896, Batch Gradient Norm: 10.690710705270162
Epoch: 6896, Batch Gradient Norm after: 10.690710705270162
Epoch 6897/10000, Prediction Accuracy = 62.564%, Loss = 0.40731383562088014
Epoch: 6897, Batch Gradient Norm: 9.647965116040453
Epoch: 6897, Batch Gradient Norm after: 9.647965116040453
Epoch 6898/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.3994905412197113
Epoch: 6898, Batch Gradient Norm: 10.664994953294308
Epoch: 6898, Batch Gradient Norm after: 10.664994953294308
Epoch 6899/10000, Prediction Accuracy = 62.56600000000001%, Loss = 0.40537899136543276
Epoch: 6899, Batch Gradient Norm: 10.874475582411819
Epoch: 6899, Batch Gradient Norm after: 10.874475582411819
Epoch 6900/10000, Prediction Accuracy = 62.69%, Loss = 0.40658435225486755
Epoch: 6900, Batch Gradient Norm: 9.79614469219397
Epoch: 6900, Batch Gradient Norm after: 9.79614469219397
Epoch 6901/10000, Prediction Accuracy = 62.626%, Loss = 0.39886087775230405
Epoch: 6901, Batch Gradient Norm: 8.540719876105685
Epoch: 6901, Batch Gradient Norm after: 8.540719876105685
Epoch 6902/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.3912820637226105
Epoch: 6902, Batch Gradient Norm: 9.426066645387689
Epoch: 6902, Batch Gradient Norm after: 9.426066645387689
Epoch 6903/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.39671375751495364
Epoch: 6903, Batch Gradient Norm: 10.168151011811217
Epoch: 6903, Batch Gradient Norm after: 10.168151011811217
Epoch 6904/10000, Prediction Accuracy = 62.712%, Loss = 0.40291309356689453
Epoch: 6904, Batch Gradient Norm: 10.314663027083283
Epoch: 6904, Batch Gradient Norm after: 10.314663027083283
Epoch 6905/10000, Prediction Accuracy = 62.702%, Loss = 0.4045002222061157
Epoch: 6905, Batch Gradient Norm: 9.809210107571408
Epoch: 6905, Batch Gradient Norm after: 9.809210107571408
Epoch 6906/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.4012886106967926
Epoch: 6906, Batch Gradient Norm: 9.669274124772256
Epoch: 6906, Batch Gradient Norm after: 9.669274124772256
Epoch 6907/10000, Prediction Accuracy = 62.519999999999996%, Loss = 0.3993042528629303
Epoch: 6907, Batch Gradient Norm: 10.658627933464897
Epoch: 6907, Batch Gradient Norm after: 10.658627933464897
Epoch 6908/10000, Prediction Accuracy = 62.702%, Loss = 0.4048015832901001
Epoch: 6908, Batch Gradient Norm: 11.389273195230365
Epoch: 6908, Batch Gradient Norm after: 11.389273195230365
Epoch 6909/10000, Prediction Accuracy = 62.63199999999999%, Loss = 0.4100942492485046
Epoch: 6909, Batch Gradient Norm: 9.975217617966294
Epoch: 6909, Batch Gradient Norm after: 9.975217617966294
Epoch 6910/10000, Prediction Accuracy = 62.693999999999996%, Loss = 0.39940899014472964
Epoch: 6910, Batch Gradient Norm: 9.770287739535725
Epoch: 6910, Batch Gradient Norm after: 9.770287739535725
Epoch 6911/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.3984401524066925
Epoch: 6911, Batch Gradient Norm: 9.575206919937392
Epoch: 6911, Batch Gradient Norm after: 9.575206919937392
Epoch 6912/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3970013499259949
Epoch: 6912, Batch Gradient Norm: 9.533484233125476
Epoch: 6912, Batch Gradient Norm after: 9.533484233125476
Epoch 6913/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3975763499736786
Epoch: 6913, Batch Gradient Norm: 8.714187541710817
Epoch: 6913, Batch Gradient Norm after: 8.714187541710817
Epoch 6914/10000, Prediction Accuracy = 62.674%, Loss = 0.3924848437309265
Epoch: 6914, Batch Gradient Norm: 9.54136592213197
Epoch: 6914, Batch Gradient Norm after: 9.54136592213197
Epoch 6915/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.39938957095146177
Epoch: 6915, Batch Gradient Norm: 8.986220039896434
Epoch: 6915, Batch Gradient Norm after: 8.986220039896434
Epoch 6916/10000, Prediction Accuracy = 62.580000000000005%, Loss = 0.3978642523288727
Epoch: 6916, Batch Gradient Norm: 10.217951992721435
Epoch: 6916, Batch Gradient Norm after: 10.217951992721435
Epoch 6917/10000, Prediction Accuracy = 62.593999999999994%, Loss = 0.4029904127120972
Epoch: 6917, Batch Gradient Norm: 11.457346221091854
Epoch: 6917, Batch Gradient Norm after: 11.457346221091854
Epoch 6918/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.4130260109901428
Epoch: 6918, Batch Gradient Norm: 8.931232610847738
Epoch: 6918, Batch Gradient Norm after: 8.931232610847738
Epoch 6919/10000, Prediction Accuracy = 62.73%, Loss = 0.3949879825115204
Epoch: 6919, Batch Gradient Norm: 9.193443011811803
Epoch: 6919, Batch Gradient Norm after: 9.193443011811803
Epoch 6920/10000, Prediction Accuracy = 62.68800000000001%, Loss = 0.39441444277763366
Epoch: 6920, Batch Gradient Norm: 13.036911595465142
Epoch: 6920, Batch Gradient Norm after: 13.036911595465142
Epoch 6921/10000, Prediction Accuracy = 62.548%, Loss = 0.42229854464530947
Epoch: 6921, Batch Gradient Norm: 12.57857422439454
Epoch: 6921, Batch Gradient Norm after: 12.57857422439454
Epoch 6922/10000, Prediction Accuracy = 62.60799999999999%, Loss = 0.4195972740650177
Epoch: 6922, Batch Gradient Norm: 10.156484089738091
Epoch: 6922, Batch Gradient Norm after: 10.156484089738091
Epoch 6923/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.40191684365272523
Epoch: 6923, Batch Gradient Norm: 10.12492153728399
Epoch: 6923, Batch Gradient Norm after: 10.12492153728399
Epoch 6924/10000, Prediction Accuracy = 62.64000000000001%, Loss = 0.4008701264858246
Epoch: 6924, Batch Gradient Norm: 11.653655511143185
Epoch: 6924, Batch Gradient Norm after: 11.653655511143185
Epoch 6925/10000, Prediction Accuracy = 62.604%, Loss = 0.41151761412620547
Epoch: 6925, Batch Gradient Norm: 9.698867796076778
Epoch: 6925, Batch Gradient Norm after: 9.698867796076778
Epoch 6926/10000, Prediction Accuracy = 62.75%, Loss = 0.3978602349758148
Epoch: 6926, Batch Gradient Norm: 8.499758193106091
Epoch: 6926, Batch Gradient Norm after: 8.499758193106091
Epoch 6927/10000, Prediction Accuracy = 62.766%, Loss = 0.391251540184021
Epoch: 6927, Batch Gradient Norm: 8.808147706481511
Epoch: 6927, Batch Gradient Norm after: 8.808147706481511
Epoch 6928/10000, Prediction Accuracy = 62.598%, Loss = 0.39348328709602354
Epoch: 6928, Batch Gradient Norm: 10.116208331771727
Epoch: 6928, Batch Gradient Norm after: 10.116208331771727
Epoch 6929/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.40221101641654966
Epoch: 6929, Batch Gradient Norm: 10.530783995108893
Epoch: 6929, Batch Gradient Norm after: 10.530783995108893
Epoch 6930/10000, Prediction Accuracy = 62.59000000000001%, Loss = 0.4039269208908081
Epoch: 6930, Batch Gradient Norm: 9.4150721755542
Epoch: 6930, Batch Gradient Norm after: 9.4150721755542
Epoch 6931/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.3964002251625061
Epoch: 6931, Batch Gradient Norm: 9.763602314072694
Epoch: 6931, Batch Gradient Norm after: 9.763602314072694
Epoch 6932/10000, Prediction Accuracy = 62.636%, Loss = 0.3992084801197052
Epoch: 6932, Batch Gradient Norm: 10.406898040778252
Epoch: 6932, Batch Gradient Norm after: 10.406898040778252
Epoch 6933/10000, Prediction Accuracy = 62.552%, Loss = 0.4028002142906189
Epoch: 6933, Batch Gradient Norm: 11.91393609823678
Epoch: 6933, Batch Gradient Norm after: 11.91393609823678
Epoch 6934/10000, Prediction Accuracy = 62.56999999999999%, Loss = 0.4130902051925659
Epoch: 6934, Batch Gradient Norm: 10.960090140977995
Epoch: 6934, Batch Gradient Norm after: 10.960090140977995
Epoch 6935/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.4061241030693054
Epoch: 6935, Batch Gradient Norm: 9.16434405425396
Epoch: 6935, Batch Gradient Norm after: 9.16434405425396
Epoch 6936/10000, Prediction Accuracy = 62.75600000000001%, Loss = 0.39573985934257505
Epoch: 6936, Batch Gradient Norm: 7.77349192818105
Epoch: 6936, Batch Gradient Norm after: 7.77349192818105
Epoch 6937/10000, Prediction Accuracy = 62.6%, Loss = 0.3882486402988434
Epoch: 6937, Batch Gradient Norm: 7.863100958213762
Epoch: 6937, Batch Gradient Norm after: 7.863100958213762
Epoch 6938/10000, Prediction Accuracy = 62.624%, Loss = 0.38850862979888917
Epoch: 6938, Batch Gradient Norm: 9.274368563692695
Epoch: 6938, Batch Gradient Norm after: 9.274368563692695
Epoch 6939/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.39584978818893435
Epoch: 6939, Batch Gradient Norm: 10.73219130984336
Epoch: 6939, Batch Gradient Norm after: 10.73219130984336
Epoch 6940/10000, Prediction Accuracy = 62.852%, Loss = 0.4048682451248169
Epoch: 6940, Batch Gradient Norm: 11.06733158667993
Epoch: 6940, Batch Gradient Norm after: 11.06733158667993
Epoch 6941/10000, Prediction Accuracy = 62.556%, Loss = 0.4062021911144257
Epoch: 6941, Batch Gradient Norm: 11.989582028381434
Epoch: 6941, Batch Gradient Norm after: 11.989582028381434
Epoch 6942/10000, Prediction Accuracy = 62.71%, Loss = 0.41331467628479
Epoch: 6942, Batch Gradient Norm: 11.542715362314391
Epoch: 6942, Batch Gradient Norm after: 11.542715362314391
Epoch 6943/10000, Prediction Accuracy = 62.624%, Loss = 0.41110467314720156
Epoch: 6943, Batch Gradient Norm: 8.885454516764904
Epoch: 6943, Batch Gradient Norm after: 8.885454516764904
Epoch 6944/10000, Prediction Accuracy = 62.686%, Loss = 0.39334667921066285
Epoch: 6944, Batch Gradient Norm: 7.559840126157041
Epoch: 6944, Batch Gradient Norm after: 7.559840126157041
Epoch 6945/10000, Prediction Accuracy = 62.658%, Loss = 0.38578673005104064
Epoch: 6945, Batch Gradient Norm: 8.635191589312344
Epoch: 6945, Batch Gradient Norm after: 8.635191589312344
Epoch 6946/10000, Prediction Accuracy = 62.496%, Loss = 0.3921463906764984
Epoch: 6946, Batch Gradient Norm: 11.137038213817492
Epoch: 6946, Batch Gradient Norm after: 11.137038213817492
Epoch 6947/10000, Prediction Accuracy = 62.53800000000001%, Loss = 0.4106461524963379
Epoch: 6947, Batch Gradient Norm: 12.189652870299888
Epoch: 6947, Batch Gradient Norm after: 12.189652870299888
Epoch 6948/10000, Prediction Accuracy = 62.662%, Loss = 0.41861109137535096
Epoch: 6948, Batch Gradient Norm: 10.65550314574903
Epoch: 6948, Batch Gradient Norm after: 10.65550314574903
Epoch 6949/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.404351270198822
Epoch: 6949, Batch Gradient Norm: 10.277988591801364
Epoch: 6949, Batch Gradient Norm after: 10.277988591801364
Epoch 6950/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4006605088710785
Epoch: 6950, Batch Gradient Norm: 9.68109241062686
Epoch: 6950, Batch Gradient Norm after: 9.68109241062686
Epoch 6951/10000, Prediction Accuracy = 62.852%, Loss = 0.3972624957561493
Epoch: 6951, Batch Gradient Norm: 9.463464300740203
Epoch: 6951, Batch Gradient Norm after: 9.463464300740203
Epoch 6952/10000, Prediction Accuracy = 62.548%, Loss = 0.3967805147171021
Epoch: 6952, Batch Gradient Norm: 9.576783762004148
Epoch: 6952, Batch Gradient Norm after: 9.576783762004148
Epoch 6953/10000, Prediction Accuracy = 62.79200000000001%, Loss = 0.3973038375377655
Epoch: 6953, Batch Gradient Norm: 10.202191660818066
Epoch: 6953, Batch Gradient Norm after: 10.202191660818066
Epoch 6954/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.4014765977859497
Epoch: 6954, Batch Gradient Norm: 10.643311276513224
Epoch: 6954, Batch Gradient Norm after: 10.643311276513224
Epoch 6955/10000, Prediction Accuracy = 62.592000000000006%, Loss = 0.40506009459495546
Epoch: 6955, Batch Gradient Norm: 11.269629484886924
Epoch: 6955, Batch Gradient Norm after: 11.269629484886924
Epoch 6956/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.409953373670578
Epoch: 6956, Batch Gradient Norm: 10.432610637988885
Epoch: 6956, Batch Gradient Norm after: 10.432610637988885
Epoch 6957/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.4046937167644501
Epoch: 6957, Batch Gradient Norm: 8.268952168355193
Epoch: 6957, Batch Gradient Norm after: 8.268952168355193
Epoch 6958/10000, Prediction Accuracy = 62.754%, Loss = 0.39064526557922363
Epoch: 6958, Batch Gradient Norm: 9.518408855479
Epoch: 6958, Batch Gradient Norm after: 9.518408855479
Epoch 6959/10000, Prediction Accuracy = 62.714%, Loss = 0.39718326926231384
Epoch: 6959, Batch Gradient Norm: 11.478571705151637
Epoch: 6959, Batch Gradient Norm after: 11.478571705151637
Epoch 6960/10000, Prediction Accuracy = 62.7%, Loss = 0.4110247313976288
Epoch: 6960, Batch Gradient Norm: 10.686997861268724
Epoch: 6960, Batch Gradient Norm after: 10.686997861268724
Epoch 6961/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.4053670108318329
Epoch: 6961, Batch Gradient Norm: 8.8607742275023
Epoch: 6961, Batch Gradient Norm after: 8.8607742275023
Epoch 6962/10000, Prediction Accuracy = 62.598%, Loss = 0.3925725042819977
Epoch: 6962, Batch Gradient Norm: 8.741731658341594
Epoch: 6962, Batch Gradient Norm after: 8.741731658341594
Epoch 6963/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3909016132354736
Epoch: 6963, Batch Gradient Norm: 10.514740161883894
Epoch: 6963, Batch Gradient Norm after: 10.514740161883894
Epoch 6964/10000, Prediction Accuracy = 62.510000000000005%, Loss = 0.4009630739688873
Epoch: 6964, Batch Gradient Norm: 10.69012713742284
Epoch: 6964, Batch Gradient Norm after: 10.69012713742284
Epoch 6965/10000, Prediction Accuracy = 62.834%, Loss = 0.4025118947029114
Epoch: 6965, Batch Gradient Norm: 9.885420218499672
Epoch: 6965, Batch Gradient Norm after: 9.885420218499672
Epoch 6966/10000, Prediction Accuracy = 62.529999999999994%, Loss = 0.3970854878425598
Epoch: 6966, Batch Gradient Norm: 11.331874097286981
Epoch: 6966, Batch Gradient Norm after: 11.331874097286981
Epoch 6967/10000, Prediction Accuracy = 62.574%, Loss = 0.40843902826309203
Epoch: 6967, Batch Gradient Norm: 10.53695413243557
Epoch: 6967, Batch Gradient Norm after: 10.53695413243557
Epoch 6968/10000, Prediction Accuracy = 62.622%, Loss = 0.40587165355682375
Epoch: 6968, Batch Gradient Norm: 8.318319203518667
Epoch: 6968, Batch Gradient Norm after: 8.318319203518667
Epoch 6969/10000, Prediction Accuracy = 62.738%, Loss = 0.39102513194084165
Epoch: 6969, Batch Gradient Norm: 9.387053906285018
Epoch: 6969, Batch Gradient Norm after: 9.387053906285018
Epoch 6970/10000, Prediction Accuracy = 62.722%, Loss = 0.39691433906555174
Epoch: 6970, Batch Gradient Norm: 11.280550105613882
Epoch: 6970, Batch Gradient Norm after: 11.280550105613882
Epoch 6971/10000, Prediction Accuracy = 62.581999999999994%, Loss = 0.4097424328327179
Epoch: 6971, Batch Gradient Norm: 11.08080611610706
Epoch: 6971, Batch Gradient Norm after: 11.08080611610706
Epoch 6972/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.40752547383308413
Epoch: 6972, Batch Gradient Norm: 9.578297723149513
Epoch: 6972, Batch Gradient Norm after: 9.578297723149513
Epoch 6973/10000, Prediction Accuracy = 62.65%, Loss = 0.3968620955944061
Epoch: 6973, Batch Gradient Norm: 9.292336271914074
Epoch: 6973, Batch Gradient Norm after: 9.292336271914074
Epoch 6974/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.3948067188262939
Epoch: 6974, Batch Gradient Norm: 10.171726874307607
Epoch: 6974, Batch Gradient Norm after: 10.171726874307607
Epoch 6975/10000, Prediction Accuracy = 62.674%, Loss = 0.3999315142631531
Epoch: 6975, Batch Gradient Norm: 11.53940276799918
Epoch: 6975, Batch Gradient Norm after: 11.53940276799918
Epoch 6976/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.4090393841266632
Epoch: 6976, Batch Gradient Norm: 12.662656599453296
Epoch: 6976, Batch Gradient Norm after: 12.662656599453296
Epoch 6977/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.4194229066371918
Epoch: 6977, Batch Gradient Norm: 10.651173185120921
Epoch: 6977, Batch Gradient Norm after: 10.651173185120921
Epoch 6978/10000, Prediction Accuracy = 62.67%, Loss = 0.40659126043319704
Epoch: 6978, Batch Gradient Norm: 8.49279051264303
Epoch: 6978, Batch Gradient Norm after: 8.49279051264303
Epoch 6979/10000, Prediction Accuracy = 62.758%, Loss = 0.39104517698287966
Epoch: 6979, Batch Gradient Norm: 8.605873877245829
Epoch: 6979, Batch Gradient Norm after: 8.605873877245829
Epoch 6980/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.39060123562812804
Epoch: 6980, Batch Gradient Norm: 9.834846124949715
Epoch: 6980, Batch Gradient Norm after: 9.834846124949715
Epoch 6981/10000, Prediction Accuracy = 62.65%, Loss = 0.3980689764022827
Epoch: 6981, Batch Gradient Norm: 9.729956261293765
Epoch: 6981, Batch Gradient Norm after: 9.729956261293765
Epoch 6982/10000, Prediction Accuracy = 62.604%, Loss = 0.39771167039871214
Epoch: 6982, Batch Gradient Norm: 9.34257079205859
Epoch: 6982, Batch Gradient Norm after: 9.34257079205859
Epoch 6983/10000, Prediction Accuracy = 62.62600000000001%, Loss = 0.3956404685974121
Epoch: 6983, Batch Gradient Norm: 9.278013604272298
Epoch: 6983, Batch Gradient Norm after: 9.278013604272298
Epoch 6984/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.39491255283355714
Epoch: 6984, Batch Gradient Norm: 9.819415032358654
Epoch: 6984, Batch Gradient Norm after: 9.819415032358654
Epoch 6985/10000, Prediction Accuracy = 62.694%, Loss = 0.3986577570438385
Epoch: 6985, Batch Gradient Norm: 9.698886391417599
Epoch: 6985, Batch Gradient Norm after: 9.698886391417599
Epoch 6986/10000, Prediction Accuracy = 62.726%, Loss = 0.3986099183559418
Epoch: 6986, Batch Gradient Norm: 10.0922901310712
Epoch: 6986, Batch Gradient Norm after: 10.0922901310712
Epoch 6987/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.40116643309593203
Epoch: 6987, Batch Gradient Norm: 11.104785724511958
Epoch: 6987, Batch Gradient Norm after: 11.104785724511958
Epoch 6988/10000, Prediction Accuracy = 62.59400000000001%, Loss = 0.4077801525592804
Epoch: 6988, Batch Gradient Norm: 11.321472456929714
Epoch: 6988, Batch Gradient Norm after: 11.321472456929714
Epoch 6989/10000, Prediction Accuracy = 62.688%, Loss = 0.40844627022743224
Epoch: 6989, Batch Gradient Norm: 10.999241870045088
Epoch: 6989, Batch Gradient Norm after: 10.999241870045088
Epoch 6990/10000, Prediction Accuracy = 62.676%, Loss = 0.40640830993652344
Epoch: 6990, Batch Gradient Norm: 10.311982238869874
Epoch: 6990, Batch Gradient Norm after: 10.311982238869874
Epoch 6991/10000, Prediction Accuracy = 62.6%, Loss = 0.4015691041946411
Epoch: 6991, Batch Gradient Norm: 10.101559653201521
Epoch: 6991, Batch Gradient Norm after: 10.101559653201521
Epoch 6992/10000, Prediction Accuracy = 62.686%, Loss = 0.39948542714118956
Epoch: 6992, Batch Gradient Norm: 10.493249930777331
Epoch: 6992, Batch Gradient Norm after: 10.493249930777331
Epoch 6993/10000, Prediction Accuracy = 62.794%, Loss = 0.4012731611728668
Epoch: 6993, Batch Gradient Norm: 10.940529799496481
Epoch: 6993, Batch Gradient Norm after: 10.940529799496481
Epoch 6994/10000, Prediction Accuracy = 62.541999999999994%, Loss = 0.40484031438827517
Epoch: 6994, Batch Gradient Norm: 9.662844601044032
Epoch: 6994, Batch Gradient Norm after: 9.662844601044032
Epoch 6995/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.39732049107551576
Epoch: 6995, Batch Gradient Norm: 8.122945269187234
Epoch: 6995, Batch Gradient Norm after: 8.122945269187234
Epoch 6996/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.3881183385848999
Epoch: 6996, Batch Gradient Norm: 9.632178792345494
Epoch: 6996, Batch Gradient Norm after: 9.632178792345494
Epoch 6997/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.39646520018577575
Epoch: 6997, Batch Gradient Norm: 11.758330752975338
Epoch: 6997, Batch Gradient Norm after: 11.758330752975338
Epoch 6998/10000, Prediction Accuracy = 62.61%, Loss = 0.4112654387950897
Epoch: 6998, Batch Gradient Norm: 11.085222937514029
Epoch: 6998, Batch Gradient Norm after: 11.085222937514029
Epoch 6999/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.40622739791870116
Epoch: 6999, Batch Gradient Norm: 9.607580762319479
Epoch: 6999, Batch Gradient Norm after: 9.607580762319479
Epoch 7000/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.39562529921531675
Epoch: 7000, Batch Gradient Norm: 9.911713487962208
Epoch: 7000, Batch Gradient Norm after: 9.911713487962208
Epoch 7001/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3974922835826874
Epoch: 7001, Batch Gradient Norm: 10.232472562886972
Epoch: 7001, Batch Gradient Norm after: 10.232472562886972
Epoch 7002/10000, Prediction Accuracy = 62.79%, Loss = 0.4010276794433594
Epoch: 7002, Batch Gradient Norm: 9.780073138025811
Epoch: 7002, Batch Gradient Norm after: 9.780073138025811
Epoch 7003/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.3981211483478546
Epoch: 7003, Batch Gradient Norm: 9.21834511486415
Epoch: 7003, Batch Gradient Norm after: 9.21834511486415
Epoch 7004/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.39458332657814027
Epoch: 7004, Batch Gradient Norm: 9.658778862794842
Epoch: 7004, Batch Gradient Norm after: 9.658778862794842
Epoch 7005/10000, Prediction Accuracy = 62.658%, Loss = 0.3963250696659088
Epoch: 7005, Batch Gradient Norm: 10.817949732248362
Epoch: 7005, Batch Gradient Norm after: 10.817949732248362
Epoch 7006/10000, Prediction Accuracy = 62.553999999999995%, Loss = 0.40456036329269407
Epoch: 7006, Batch Gradient Norm: 10.480469583371866
Epoch: 7006, Batch Gradient Norm after: 10.480469583371866
Epoch 7007/10000, Prediction Accuracy = 62.652%, Loss = 0.4020031690597534
Epoch: 7007, Batch Gradient Norm: 10.096496652395139
Epoch: 7007, Batch Gradient Norm after: 10.096496652395139
Epoch 7008/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.3998936593532562
Epoch: 7008, Batch Gradient Norm: 10.429930621075274
Epoch: 7008, Batch Gradient Norm after: 10.429930621075274
Epoch 7009/10000, Prediction Accuracy = 62.6%, Loss = 0.40265402793884275
Epoch: 7009, Batch Gradient Norm: 9.76559217129873
Epoch: 7009, Batch Gradient Norm after: 9.76559217129873
Epoch 7010/10000, Prediction Accuracy = 62.818%, Loss = 0.40065653920173644
Epoch: 7010, Batch Gradient Norm: 8.073738066129401
Epoch: 7010, Batch Gradient Norm after: 8.073738066129401
Epoch 7011/10000, Prediction Accuracy = 62.648%, Loss = 0.3891644775867462
Epoch: 7011, Batch Gradient Norm: 9.22677820802668
Epoch: 7011, Batch Gradient Norm after: 9.22677820802668
Epoch 7012/10000, Prediction Accuracy = 62.762%, Loss = 0.3953565776348114
Epoch: 7012, Batch Gradient Norm: 11.487854637964434
Epoch: 7012, Batch Gradient Norm after: 11.487854637964434
Epoch 7013/10000, Prediction Accuracy = 62.678%, Loss = 0.4099946916103363
Epoch: 7013, Batch Gradient Norm: 11.378093746750096
Epoch: 7013, Batch Gradient Norm after: 11.378093746750096
Epoch 7014/10000, Prediction Accuracy = 62.605999999999995%, Loss = 0.40814759731292727
Epoch: 7014, Batch Gradient Norm: 9.39300116867226
Epoch: 7014, Batch Gradient Norm after: 9.39300116867226
Epoch 7015/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.394597190618515
Epoch: 7015, Batch Gradient Norm: 9.597600138263976
Epoch: 7015, Batch Gradient Norm after: 9.597600138263976
Epoch 7016/10000, Prediction Accuracy = 62.56400000000001%, Loss = 0.3958843171596527
Epoch: 7016, Batch Gradient Norm: 11.577339535027
Epoch: 7016, Batch Gradient Norm after: 11.577339535027
Epoch 7017/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.4103462815284729
Epoch: 7017, Batch Gradient Norm: 11.407183093031033
Epoch: 7017, Batch Gradient Norm after: 11.407183093031033
Epoch 7018/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.40991496443748476
Epoch: 7018, Batch Gradient Norm: 9.116954932616082
Epoch: 7018, Batch Gradient Norm after: 9.116954932616082
Epoch 7019/10000, Prediction Accuracy = 62.718%, Loss = 0.3928466379642487
Epoch: 7019, Batch Gradient Norm: 10.288215062024161
Epoch: 7019, Batch Gradient Norm after: 10.288215062024161
Epoch 7020/10000, Prediction Accuracy = 62.67%, Loss = 0.3993252098560333
Epoch: 7020, Batch Gradient Norm: 12.444371117031581
Epoch: 7020, Batch Gradient Norm after: 12.444371117031581
Epoch 7021/10000, Prediction Accuracy = 62.628%, Loss = 0.41557472944259644
Epoch: 7021, Batch Gradient Norm: 10.769621619472206
Epoch: 7021, Batch Gradient Norm after: 10.769621619472206
Epoch 7022/10000, Prediction Accuracy = 62.73%, Loss = 0.40380691885948183
Epoch: 7022, Batch Gradient Norm: 8.265333215478453
Epoch: 7022, Batch Gradient Norm after: 8.265333215478453
Epoch 7023/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.3882396280765533
Epoch: 7023, Batch Gradient Norm: 7.838954412703632
Epoch: 7023, Batch Gradient Norm after: 7.838954412703632
Epoch 7024/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.38566042184829713
Epoch: 7024, Batch Gradient Norm: 10.676787153077239
Epoch: 7024, Batch Gradient Norm after: 10.676787153077239
Epoch 7025/10000, Prediction Accuracy = 62.628%, Loss = 0.4046233057975769
Epoch: 7025, Batch Gradient Norm: 10.583781222796697
Epoch: 7025, Batch Gradient Norm after: 10.583781222796697
Epoch 7026/10000, Prediction Accuracy = 62.69000000000001%, Loss = 0.40377179384231565
Epoch: 7026, Batch Gradient Norm: 10.387332552054756
Epoch: 7026, Batch Gradient Norm after: 10.387332552054756
Epoch 7027/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.39990770220756533
Epoch: 7027, Batch Gradient Norm: 10.348795911825725
Epoch: 7027, Batch Gradient Norm after: 10.348795911825725
Epoch 7028/10000, Prediction Accuracy = 62.69%, Loss = 0.3991314351558685
Epoch: 7028, Batch Gradient Norm: 10.41515487847939
Epoch: 7028, Batch Gradient Norm after: 10.41515487847939
Epoch 7029/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.400909823179245
Epoch: 7029, Batch Gradient Norm: 9.663202814911596
Epoch: 7029, Batch Gradient Norm after: 9.663202814911596
Epoch 7030/10000, Prediction Accuracy = 62.754%, Loss = 0.39744141697883606
Epoch: 7030, Batch Gradient Norm: 8.08732121565578
Epoch: 7030, Batch Gradient Norm after: 8.08732121565578
Epoch 7031/10000, Prediction Accuracy = 62.67%, Loss = 0.38750830888748167
Epoch: 7031, Batch Gradient Norm: 8.976403886055435
Epoch: 7031, Batch Gradient Norm after: 8.976403886055435
Epoch 7032/10000, Prediction Accuracy = 62.81%, Loss = 0.39127073884010316
Epoch: 7032, Batch Gradient Norm: 12.440637865213747
Epoch: 7032, Batch Gradient Norm after: 12.440637865213747
Epoch 7033/10000, Prediction Accuracy = 62.496%, Loss = 0.41711071133613586
Epoch: 7033, Batch Gradient Norm: 9.933030868401433
Epoch: 7033, Batch Gradient Norm after: 9.933030868401433
Epoch 7034/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.39948067665100095
Epoch: 7034, Batch Gradient Norm: 7.779442871704766
Epoch: 7034, Batch Gradient Norm after: 7.779442871704766
Epoch 7035/10000, Prediction Accuracy = 62.678%, Loss = 0.384868985414505
Epoch: 7035, Batch Gradient Norm: 8.541021128612694
Epoch: 7035, Batch Gradient Norm after: 8.541021128612694
Epoch 7036/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.3884547114372253
Epoch: 7036, Batch Gradient Norm: 10.96662947913398
Epoch: 7036, Batch Gradient Norm after: 10.96662947913398
Epoch 7037/10000, Prediction Accuracy = 62.57199999999999%, Loss = 0.4047615349292755
Epoch: 7037, Batch Gradient Norm: 10.73593910148875
Epoch: 7037, Batch Gradient Norm after: 10.73593910148875
Epoch 7038/10000, Prediction Accuracy = 62.79200000000001%, Loss = 0.404954206943512
Epoch: 7038, Batch Gradient Norm: 9.71419389362481
Epoch: 7038, Batch Gradient Norm after: 9.71419389362481
Epoch 7039/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.3968842804431915
Epoch: 7039, Batch Gradient Norm: 9.989117909899193
Epoch: 7039, Batch Gradient Norm after: 9.989117909899193
Epoch 7040/10000, Prediction Accuracy = 62.636%, Loss = 0.3977172374725342
Epoch: 7040, Batch Gradient Norm: 10.310367049551756
Epoch: 7040, Batch Gradient Norm after: 10.310367049551756
Epoch 7041/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.39939671754837036
Epoch: 7041, Batch Gradient Norm: 11.358681157028395
Epoch: 7041, Batch Gradient Norm after: 11.358681157028395
Epoch 7042/10000, Prediction Accuracy = 62.60600000000001%, Loss = 0.40772305727005004
Epoch: 7042, Batch Gradient Norm: 12.090575100837858
Epoch: 7042, Batch Gradient Norm after: 12.090575100837858
Epoch 7043/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.41474405527114866
Epoch: 7043, Batch Gradient Norm: 10.04907434347661
Epoch: 7043, Batch Gradient Norm after: 10.04907434347661
Epoch 7044/10000, Prediction Accuracy = 62.767999999999994%, Loss = 0.40078859925270083
Epoch: 7044, Batch Gradient Norm: 8.51401289243483
Epoch: 7044, Batch Gradient Norm after: 8.51401289243483
Epoch 7045/10000, Prediction Accuracy = 62.702%, Loss = 0.39071149230003355
Epoch: 7045, Batch Gradient Norm: 9.504696477936974
Epoch: 7045, Batch Gradient Norm after: 9.504696477936974
Epoch 7046/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.39689897894859316
Epoch: 7046, Batch Gradient Norm: 10.245210122089741
Epoch: 7046, Batch Gradient Norm after: 10.245210122089741
Epoch 7047/10000, Prediction Accuracy = 62.484%, Loss = 0.4013033390045166
Epoch: 7047, Batch Gradient Norm: 10.376639066278777
Epoch: 7047, Batch Gradient Norm after: 10.376639066278777
Epoch 7048/10000, Prediction Accuracy = 62.8%, Loss = 0.40194413661956785
Epoch: 7048, Batch Gradient Norm: 11.078590624622617
Epoch: 7048, Batch Gradient Norm after: 11.078590624622617
Epoch 7049/10000, Prediction Accuracy = 62.658%, Loss = 0.406901478767395
Epoch: 7049, Batch Gradient Norm: 9.334825761616763
Epoch: 7049, Batch Gradient Norm after: 9.334825761616763
Epoch 7050/10000, Prediction Accuracy = 62.648%, Loss = 0.39539152979850767
Epoch: 7050, Batch Gradient Norm: 8.691041713592577
Epoch: 7050, Batch Gradient Norm after: 8.691041713592577
Epoch 7051/10000, Prediction Accuracy = 62.754%, Loss = 0.39129671454429626
Epoch: 7051, Batch Gradient Norm: 8.510590832452273
Epoch: 7051, Batch Gradient Norm after: 8.510590832452273
Epoch 7052/10000, Prediction Accuracy = 62.746%, Loss = 0.3904857516288757
Epoch: 7052, Batch Gradient Norm: 9.033272253510416
Epoch: 7052, Batch Gradient Norm after: 9.033272253510416
Epoch 7053/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.3928657591342926
Epoch: 7053, Batch Gradient Norm: 10.514102595972318
Epoch: 7053, Batch Gradient Norm after: 10.514102595972318
Epoch 7054/10000, Prediction Accuracy = 62.638%, Loss = 0.4018900394439697
Epoch: 7054, Batch Gradient Norm: 12.364044153296541
Epoch: 7054, Batch Gradient Norm after: 12.364044153296541
Epoch 7055/10000, Prediction Accuracy = 62.678%, Loss = 0.41707803606987
Epoch: 7055, Batch Gradient Norm: 10.59620456276393
Epoch: 7055, Batch Gradient Norm after: 10.59620456276393
Epoch 7056/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.4027889370918274
Epoch: 7056, Batch Gradient Norm: 9.376889437815649
Epoch: 7056, Batch Gradient Norm after: 9.376889437815649
Epoch 7057/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.3930309355258942
Epoch: 7057, Batch Gradient Norm: 10.299444355432735
Epoch: 7057, Batch Gradient Norm after: 10.299444355432735
Epoch 7058/10000, Prediction Accuracy = 62.81400000000001%, Loss = 0.39743695855140687
Epoch: 7058, Batch Gradient Norm: 10.979032204246092
Epoch: 7058, Batch Gradient Norm after: 10.979032204246092
Epoch 7059/10000, Prediction Accuracy = 62.65%, Loss = 0.4020317018032074
Epoch: 7059, Batch Gradient Norm: 9.332406171796809
Epoch: 7059, Batch Gradient Norm after: 9.332406171796809
Epoch 7060/10000, Prediction Accuracy = 62.540000000000006%, Loss = 0.39244189858436584
Epoch: 7060, Batch Gradient Norm: 8.47677753588213
Epoch: 7060, Batch Gradient Norm after: 8.47677753588213
Epoch 7061/10000, Prediction Accuracy = 62.712%, Loss = 0.38762523531913756
Epoch: 7061, Batch Gradient Norm: 11.582486380724461
Epoch: 7061, Batch Gradient Norm after: 11.582486380724461
Epoch 7062/10000, Prediction Accuracy = 62.626%, Loss = 0.407599413394928
Epoch: 7062, Batch Gradient Norm: 14.075829977196344
Epoch: 7062, Batch Gradient Norm after: 14.075829977196344
Epoch 7063/10000, Prediction Accuracy = 62.638%, Loss = 0.42911175489425657
Epoch: 7063, Batch Gradient Norm: 10.522278567888309
Epoch: 7063, Batch Gradient Norm after: 10.522278567888309
Epoch 7064/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.40155518651008604
Epoch: 7064, Batch Gradient Norm: 9.310936637078726
Epoch: 7064, Batch Gradient Norm after: 9.310936637078726
Epoch 7065/10000, Prediction Accuracy = 62.874%, Loss = 0.3957800030708313
Epoch: 7065, Batch Gradient Norm: 8.766808383049746
Epoch: 7065, Batch Gradient Norm after: 8.766808383049746
Epoch 7066/10000, Prediction Accuracy = 62.638%, Loss = 0.3902674674987793
Epoch: 7066, Batch Gradient Norm: 11.177318560567345
Epoch: 7066, Batch Gradient Norm after: 11.177318560567345
Epoch 7067/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.4058486640453339
Epoch: 7067, Batch Gradient Norm: 10.134160332398032
Epoch: 7067, Batch Gradient Norm after: 10.134160332398032
Epoch 7068/10000, Prediction Accuracy = 62.620000000000005%, Loss = 0.3995588064193726
Epoch: 7068, Batch Gradient Norm: 8.01240280419406
Epoch: 7068, Batch Gradient Norm after: 8.01240280419406
Epoch 7069/10000, Prediction Accuracy = 62.742%, Loss = 0.38604270815849306
Epoch: 7069, Batch Gradient Norm: 8.637490147741486
Epoch: 7069, Batch Gradient Norm after: 8.637490147741486
Epoch 7070/10000, Prediction Accuracy = 62.702%, Loss = 0.3890842080116272
Epoch: 7070, Batch Gradient Norm: 9.686816942560597
Epoch: 7070, Batch Gradient Norm after: 9.686816942560597
Epoch 7071/10000, Prediction Accuracy = 62.598%, Loss = 0.3959254503250122
Epoch: 7071, Batch Gradient Norm: 10.675682372254924
Epoch: 7071, Batch Gradient Norm after: 10.675682372254924
Epoch 7072/10000, Prediction Accuracy = 62.576%, Loss = 0.40258108377456664
Epoch: 7072, Batch Gradient Norm: 11.247800329676748
Epoch: 7072, Batch Gradient Norm after: 11.247800329676748
Epoch 7073/10000, Prediction Accuracy = 62.726%, Loss = 0.4072025716304779
Epoch: 7073, Batch Gradient Norm: 9.997299257312315
Epoch: 7073, Batch Gradient Norm after: 9.997299257312315
Epoch 7074/10000, Prediction Accuracy = 62.524%, Loss = 0.39898443818092344
Epoch: 7074, Batch Gradient Norm: 8.652021568413277
Epoch: 7074, Batch Gradient Norm after: 8.652021568413277
Epoch 7075/10000, Prediction Accuracy = 62.662%, Loss = 0.3901792407035828
Epoch: 7075, Batch Gradient Norm: 8.696776082521659
Epoch: 7075, Batch Gradient Norm after: 8.696776082521659
Epoch 7076/10000, Prediction Accuracy = 62.742%, Loss = 0.3897063434123993
Epoch: 7076, Batch Gradient Norm: 9.72084694576629
Epoch: 7076, Batch Gradient Norm after: 9.72084694576629
Epoch 7077/10000, Prediction Accuracy = 62.686%, Loss = 0.3965058147907257
Epoch: 7077, Batch Gradient Norm: 10.329078082286602
Epoch: 7077, Batch Gradient Norm after: 10.329078082286602
Epoch 7078/10000, Prediction Accuracy = 62.676%, Loss = 0.4016976773738861
Epoch: 7078, Batch Gradient Norm: 10.35881945421506
Epoch: 7078, Batch Gradient Norm after: 10.35881945421506
Epoch 7079/10000, Prediction Accuracy = 62.588%, Loss = 0.4017392694950104
Epoch: 7079, Batch Gradient Norm: 10.043319040259325
Epoch: 7079, Batch Gradient Norm after: 10.043319040259325
Epoch 7080/10000, Prediction Accuracy = 62.73%, Loss = 0.3988369941711426
Epoch: 7080, Batch Gradient Norm: 9.833655876277696
Epoch: 7080, Batch Gradient Norm after: 9.833655876277696
Epoch 7081/10000, Prediction Accuracy = 62.662%, Loss = 0.3964316248893738
Epoch: 7081, Batch Gradient Norm: 10.020794627036457
Epoch: 7081, Batch Gradient Norm after: 10.020794627036457
Epoch 7082/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.3965180218219757
Epoch: 7082, Batch Gradient Norm: 11.999328489747873
Epoch: 7082, Batch Gradient Norm after: 11.999328489747873
Epoch 7083/10000, Prediction Accuracy = 62.666%, Loss = 0.4106426537036896
Epoch: 7083, Batch Gradient Norm: 12.660635011755465
Epoch: 7083, Batch Gradient Norm after: 12.660635011755465
Epoch 7084/10000, Prediction Accuracy = 62.552%, Loss = 0.41856794953346255
Epoch: 7084, Batch Gradient Norm: 10.008931044366458
Epoch: 7084, Batch Gradient Norm after: 10.008931044366458
Epoch 7085/10000, Prediction Accuracy = 62.738%, Loss = 0.40026543736457826
Epoch: 7085, Batch Gradient Norm: 8.68168596866247
Epoch: 7085, Batch Gradient Norm after: 8.68168596866247
Epoch 7086/10000, Prediction Accuracy = 62.588%, Loss = 0.3900925159454346
Epoch: 7086, Batch Gradient Norm: 9.389398428963853
Epoch: 7086, Batch Gradient Norm after: 9.389398428963853
Epoch 7087/10000, Prediction Accuracy = 62.674%, Loss = 0.39393935799598695
Epoch: 7087, Batch Gradient Norm: 10.809172503238248
Epoch: 7087, Batch Gradient Norm after: 10.809172503238248
Epoch 7088/10000, Prediction Accuracy = 62.778%, Loss = 0.40455104112625123
Epoch: 7088, Batch Gradient Norm: 9.367557513382861
Epoch: 7088, Batch Gradient Norm after: 9.367557513382861
Epoch 7089/10000, Prediction Accuracy = 62.646%, Loss = 0.39386053681373595
Epoch: 7089, Batch Gradient Norm: 10.10004107453189
Epoch: 7089, Batch Gradient Norm after: 10.10004107453189
Epoch 7090/10000, Prediction Accuracy = 62.86600000000001%, Loss = 0.39748868346214294
Epoch: 7090, Batch Gradient Norm: 11.90549924864499
Epoch: 7090, Batch Gradient Norm after: 11.90549924864499
Epoch 7091/10000, Prediction Accuracy = 62.73%, Loss = 0.4097536563873291
Epoch: 7091, Batch Gradient Norm: 10.890193853054338
Epoch: 7091, Batch Gradient Norm after: 10.890193853054338
Epoch 7092/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.403489476442337
Epoch: 7092, Batch Gradient Norm: 8.409366145536284
Epoch: 7092, Batch Gradient Norm after: 8.409366145536284
Epoch 7093/10000, Prediction Accuracy = 62.75%, Loss = 0.3878853738307953
Epoch: 7093, Batch Gradient Norm: 8.08184381728304
Epoch: 7093, Batch Gradient Norm after: 8.08184381728304
Epoch 7094/10000, Prediction Accuracy = 62.726%, Loss = 0.38646976351737977
Epoch: 7094, Batch Gradient Norm: 9.699059664143391
Epoch: 7094, Batch Gradient Norm after: 9.699059664143391
Epoch 7095/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3957540690898895
Epoch: 7095, Batch Gradient Norm: 10.469674549090785
Epoch: 7095, Batch Gradient Norm after: 10.469674549090785
Epoch 7096/10000, Prediction Accuracy = 62.712%, Loss = 0.40006080865859983
Epoch: 7096, Batch Gradient Norm: 10.567310150949691
Epoch: 7096, Batch Gradient Norm after: 10.567310150949691
Epoch 7097/10000, Prediction Accuracy = 62.666%, Loss = 0.39903879165649414
Epoch: 7097, Batch Gradient Norm: 10.54091298412376
Epoch: 7097, Batch Gradient Norm after: 10.54091298412376
Epoch 7098/10000, Prediction Accuracy = 62.70400000000001%, Loss = 0.39880201816558836
Epoch: 7098, Batch Gradient Norm: 9.894377024041658
Epoch: 7098, Batch Gradient Norm after: 9.894377024041658
Epoch 7099/10000, Prediction Accuracy = 62.641999999999996%, Loss = 0.39547728896141054
Epoch: 7099, Batch Gradient Norm: 10.081588666537062
Epoch: 7099, Batch Gradient Norm after: 10.081588666537062
Epoch 7100/10000, Prediction Accuracy = 62.75999999999999%, Loss = 0.39835797548294066
Epoch: 7100, Batch Gradient Norm: 9.934752161155982
Epoch: 7100, Batch Gradient Norm after: 9.934752161155982
Epoch 7101/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.39845117926597595
Epoch: 7101, Batch Gradient Norm: 9.966645435430172
Epoch: 7101, Batch Gradient Norm after: 9.966645435430172
Epoch 7102/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3987138271331787
Epoch: 7102, Batch Gradient Norm: 10.112708136979464
Epoch: 7102, Batch Gradient Norm after: 10.112708136979464
Epoch 7103/10000, Prediction Accuracy = 62.746%, Loss = 0.39890944957733154
Epoch: 7103, Batch Gradient Norm: 10.94579848752012
Epoch: 7103, Batch Gradient Norm after: 10.94579848752012
Epoch 7104/10000, Prediction Accuracy = 62.602%, Loss = 0.40393596291542055
Epoch: 7104, Batch Gradient Norm: 11.453776307685999
Epoch: 7104, Batch Gradient Norm after: 11.453776307685999
Epoch 7105/10000, Prediction Accuracy = 62.634%, Loss = 0.40700618028640745
Epoch: 7105, Batch Gradient Norm: 11.664212455770969
Epoch: 7105, Batch Gradient Norm after: 11.664212455770969
Epoch 7106/10000, Prediction Accuracy = 62.694%, Loss = 0.4090051054954529
Epoch: 7106, Batch Gradient Norm: 10.275096253717756
Epoch: 7106, Batch Gradient Norm after: 10.275096253717756
Epoch 7107/10000, Prediction Accuracy = 62.653999999999996%, Loss = 0.3999787926673889
Epoch: 7107, Batch Gradient Norm: 8.286392278984547
Epoch: 7107, Batch Gradient Norm after: 8.286392278984547
Epoch 7108/10000, Prediction Accuracy = 62.702%, Loss = 0.38751964569091796
Epoch: 7108, Batch Gradient Norm: 7.6761697485949965
Epoch: 7108, Batch Gradient Norm after: 7.6761697485949965
Epoch 7109/10000, Prediction Accuracy = 62.702%, Loss = 0.3841339111328125
Epoch: 7109, Batch Gradient Norm: 8.288593126367575
Epoch: 7109, Batch Gradient Norm after: 8.288593126367575
Epoch 7110/10000, Prediction Accuracy = 62.66400000000001%, Loss = 0.3878047585487366
Epoch: 7110, Batch Gradient Norm: 10.08463618078772
Epoch: 7110, Batch Gradient Norm after: 10.08463618078772
Epoch 7111/10000, Prediction Accuracy = 62.742%, Loss = 0.39801597595214844
Epoch: 7111, Batch Gradient Norm: 11.905539016977926
Epoch: 7111, Batch Gradient Norm after: 11.905539016977926
Epoch 7112/10000, Prediction Accuracy = 62.55800000000001%, Loss = 0.41257794499397277
Epoch: 7112, Batch Gradient Norm: 9.072232966465688
Epoch: 7112, Batch Gradient Norm after: 9.072232966465688
Epoch 7113/10000, Prediction Accuracy = 62.85600000000001%, Loss = 0.39369890093803406
Epoch: 7113, Batch Gradient Norm: 8.44922642222202
Epoch: 7113, Batch Gradient Norm after: 8.44922642222202
Epoch 7114/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.38840324282646177
Epoch: 7114, Batch Gradient Norm: 9.415539318728186
Epoch: 7114, Batch Gradient Norm after: 9.415539318728186
Epoch 7115/10000, Prediction Accuracy = 62.848%, Loss = 0.3945807576179504
Epoch: 7115, Batch Gradient Norm: 9.597425293511584
Epoch: 7115, Batch Gradient Norm after: 9.597425293511584
Epoch 7116/10000, Prediction Accuracy = 62.65599999999999%, Loss = 0.39488378167152405
Epoch: 7116, Batch Gradient Norm: 9.531118621743802
Epoch: 7116, Batch Gradient Norm after: 9.531118621743802
Epoch 7117/10000, Prediction Accuracy = 62.598%, Loss = 0.39421333074569703
Epoch: 7117, Batch Gradient Norm: 9.67836135558645
Epoch: 7117, Batch Gradient Norm after: 9.67836135558645
Epoch 7118/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.39588539600372313
Epoch: 7118, Batch Gradient Norm: 11.804493955427112
Epoch: 7118, Batch Gradient Norm after: 11.804493955427112
Epoch 7119/10000, Prediction Accuracy = 62.49400000000001%, Loss = 0.4113565504550934
Epoch: 7119, Batch Gradient Norm: 11.937464007392927
Epoch: 7119, Batch Gradient Norm after: 11.937464007392927
Epoch 7120/10000, Prediction Accuracy = 62.698%, Loss = 0.41372241973876955
Epoch: 7120, Batch Gradient Norm: 8.76815275765455
Epoch: 7120, Batch Gradient Norm after: 8.76815275765455
Epoch 7121/10000, Prediction Accuracy = 62.73%, Loss = 0.390152645111084
Epoch: 7121, Batch Gradient Norm: 7.7778914661809875
Epoch: 7121, Batch Gradient Norm after: 7.7778914661809875
Epoch 7122/10000, Prediction Accuracy = 62.75200000000001%, Loss = 0.38305288553237915
Epoch: 7122, Batch Gradient Norm: 11.577848574945701
Epoch: 7122, Batch Gradient Norm after: 11.577848574945701
Epoch 7123/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.40658565759658816
Epoch: 7123, Batch Gradient Norm: 13.7106967480885
Epoch: 7123, Batch Gradient Norm after: 13.7106967480885
Epoch 7124/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.4250182926654816
Epoch: 7124, Batch Gradient Norm: 11.999319065108267
Epoch: 7124, Batch Gradient Norm after: 11.999319065108267
Epoch 7125/10000, Prediction Accuracy = 62.702%, Loss = 0.4099130630493164
Epoch: 7125, Batch Gradient Norm: 12.32349276662007
Epoch: 7125, Batch Gradient Norm after: 12.32349276662007
Epoch 7126/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.41303521394729614
Epoch: 7126, Batch Gradient Norm: 10.80213057211273
Epoch: 7126, Batch Gradient Norm after: 10.80213057211273
Epoch 7127/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.40222165584564207
Epoch: 7127, Batch Gradient Norm: 7.977208048177415
Epoch: 7127, Batch Gradient Norm after: 7.977208048177415
Epoch 7128/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.38398075103759766
Epoch: 7128, Batch Gradient Norm: 7.4701414661265435
Epoch: 7128, Batch Gradient Norm after: 7.4701414661265435
Epoch 7129/10000, Prediction Accuracy = 62.79%, Loss = 0.38071964383125306
Epoch: 7129, Batch Gradient Norm: 10.15024354679772
Epoch: 7129, Batch Gradient Norm after: 10.15024354679772
Epoch 7130/10000, Prediction Accuracy = 62.662%, Loss = 0.3966293394565582
Epoch: 7130, Batch Gradient Norm: 10.415512091400542
Epoch: 7130, Batch Gradient Norm after: 10.415512091400542
Epoch 7131/10000, Prediction Accuracy = 62.774%, Loss = 0.40085062980651853
Epoch: 7131, Batch Gradient Norm: 8.9776358918823
Epoch: 7131, Batch Gradient Norm after: 8.9776358918823
Epoch 7132/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.3912434339523315
Epoch: 7132, Batch Gradient Norm: 8.862465227516784
Epoch: 7132, Batch Gradient Norm after: 8.862465227516784
Epoch 7133/10000, Prediction Accuracy = 62.788%, Loss = 0.3905962646007538
Epoch: 7133, Batch Gradient Norm: 10.06456344158529
Epoch: 7133, Batch Gradient Norm after: 10.06456344158529
Epoch 7134/10000, Prediction Accuracy = 62.778%, Loss = 0.3974857211112976
Epoch: 7134, Batch Gradient Norm: 11.73202372611841
Epoch: 7134, Batch Gradient Norm after: 11.73202372611841
Epoch 7135/10000, Prediction Accuracy = 62.55%, Loss = 0.4094195306301117
Epoch: 7135, Batch Gradient Norm: 9.9892853209742
Epoch: 7135, Batch Gradient Norm after: 9.9892853209742
Epoch 7136/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.3963989019393921
Epoch: 7136, Batch Gradient Norm: 8.990570193985233
Epoch: 7136, Batch Gradient Norm after: 8.990570193985233
Epoch 7137/10000, Prediction Accuracy = 62.762%, Loss = 0.3892780184745789
Epoch: 7137, Batch Gradient Norm: 9.14745195682148
Epoch: 7137, Batch Gradient Norm after: 9.14745195682148
Epoch 7138/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.38955981731414796
Epoch: 7138, Batch Gradient Norm: 10.824453963879026
Epoch: 7138, Batch Gradient Norm after: 10.824453963879026
Epoch 7139/10000, Prediction Accuracy = 62.608000000000004%, Loss = 0.40053367614746094
Epoch: 7139, Batch Gradient Norm: 10.579698839681623
Epoch: 7139, Batch Gradient Norm after: 10.579698839681623
Epoch 7140/10000, Prediction Accuracy = 62.688%, Loss = 0.40052778720855714
Epoch: 7140, Batch Gradient Norm: 9.651366130150656
Epoch: 7140, Batch Gradient Norm after: 9.651366130150656
Epoch 7141/10000, Prediction Accuracy = 62.624%, Loss = 0.3960117340087891
Epoch: 7141, Batch Gradient Norm: 8.254389174903398
Epoch: 7141, Batch Gradient Norm after: 8.254389174903398
Epoch 7142/10000, Prediction Accuracy = 62.77%, Loss = 0.3874497294425964
Epoch: 7142, Batch Gradient Norm: 8.855644561558105
Epoch: 7142, Batch Gradient Norm after: 8.855644561558105
Epoch 7143/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.3903773307800293
Epoch: 7143, Batch Gradient Norm: 11.173867459523152
Epoch: 7143, Batch Gradient Norm after: 11.173867459523152
Epoch 7144/10000, Prediction Accuracy = 62.592%, Loss = 0.4047511458396912
Epoch: 7144, Batch Gradient Norm: 11.66561465085033
Epoch: 7144, Batch Gradient Norm after: 11.66561465085033
Epoch 7145/10000, Prediction Accuracy = 62.802%, Loss = 0.41061341762542725
Epoch: 7145, Batch Gradient Norm: 10.428885888472822
Epoch: 7145, Batch Gradient Norm after: 10.428885888472822
Epoch 7146/10000, Prediction Accuracy = 62.722%, Loss = 0.3999123156070709
Epoch: 7146, Batch Gradient Norm: 10.375484605677647
Epoch: 7146, Batch Gradient Norm after: 10.375484605677647
Epoch 7147/10000, Prediction Accuracy = 62.65999999999999%, Loss = 0.39929924011230467
Epoch: 7147, Batch Gradient Norm: 10.459505438409789
Epoch: 7147, Batch Gradient Norm after: 10.459505438409789
Epoch 7148/10000, Prediction Accuracy = 62.788%, Loss = 0.4002951383590698
Epoch: 7148, Batch Gradient Norm: 10.202271308016197
Epoch: 7148, Batch Gradient Norm after: 10.202271308016197
Epoch 7149/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3985909283161163
Epoch: 7149, Batch Gradient Norm: 9.9558924666141
Epoch: 7149, Batch Gradient Norm after: 9.9558924666141
Epoch 7150/10000, Prediction Accuracy = 62.812%, Loss = 0.3970170021057129
Epoch: 7150, Batch Gradient Norm: 9.826509699729069
Epoch: 7150, Batch Gradient Norm after: 9.826509699729069
Epoch 7151/10000, Prediction Accuracy = 62.75%, Loss = 0.3960195600986481
Epoch: 7151, Batch Gradient Norm: 10.308724832618017
Epoch: 7151, Batch Gradient Norm after: 10.308724832618017
Epoch 7152/10000, Prediction Accuracy = 62.734%, Loss = 0.3993625819683075
Epoch: 7152, Batch Gradient Norm: 10.723274919102762
Epoch: 7152, Batch Gradient Norm after: 10.723274919102762
Epoch 7153/10000, Prediction Accuracy = 62.758%, Loss = 0.40145419239997865
Epoch: 7153, Batch Gradient Norm: 10.792316252610723
Epoch: 7153, Batch Gradient Norm after: 10.792316252610723
Epoch 7154/10000, Prediction Accuracy = 62.626%, Loss = 0.40006054639816285
Epoch: 7154, Batch Gradient Norm: 10.925768124136845
Epoch: 7154, Batch Gradient Norm after: 10.925768124136845
Epoch 7155/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.400343656539917
Epoch: 7155, Batch Gradient Norm: 11.298725857135363
Epoch: 7155, Batch Gradient Norm after: 11.298725857135363
Epoch 7156/10000, Prediction Accuracy = 62.516%, Loss = 0.4046328365802765
Epoch: 7156, Batch Gradient Norm: 9.905334752745608
Epoch: 7156, Batch Gradient Norm after: 9.905334752745608
Epoch 7157/10000, Prediction Accuracy = 62.568000000000005%, Loss = 0.3958852827548981
Epoch: 7157, Batch Gradient Norm: 8.637067342199062
Epoch: 7157, Batch Gradient Norm after: 8.637067342199062
Epoch 7158/10000, Prediction Accuracy = 62.774%, Loss = 0.38834195137023925
Epoch: 7158, Batch Gradient Norm: 8.938740548956645
Epoch: 7158, Batch Gradient Norm after: 8.938740548956645
Epoch 7159/10000, Prediction Accuracy = 62.678%, Loss = 0.38902510404586793
Epoch: 7159, Batch Gradient Norm: 10.946806757581623
Epoch: 7159, Batch Gradient Norm after: 10.946806757581623
Epoch 7160/10000, Prediction Accuracy = 62.73%, Loss = 0.40450671315193176
Epoch: 7160, Batch Gradient Norm: 10.302490299916425
Epoch: 7160, Batch Gradient Norm after: 10.302490299916425
Epoch 7161/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.40037190318107607
Epoch: 7161, Batch Gradient Norm: 10.139844000669857
Epoch: 7161, Batch Gradient Norm after: 10.139844000669857
Epoch 7162/10000, Prediction Accuracy = 62.71%, Loss = 0.39669160842895507
Epoch: 7162, Batch Gradient Norm: 10.680333916890783
Epoch: 7162, Batch Gradient Norm after: 10.680333916890783
Epoch 7163/10000, Prediction Accuracy = 62.69199999999999%, Loss = 0.3986110985279083
Epoch: 7163, Batch Gradient Norm: 9.798343120125669
Epoch: 7163, Batch Gradient Norm after: 9.798343120125669
Epoch 7164/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.39329713582992554
Epoch: 7164, Batch Gradient Norm: 8.617508604361
Epoch: 7164, Batch Gradient Norm after: 8.617508604361
Epoch 7165/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.3866325497627258
Epoch: 7165, Batch Gradient Norm: 9.185905729071468
Epoch: 7165, Batch Gradient Norm after: 9.185905729071468
Epoch 7166/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.39007214903831483
Epoch: 7166, Batch Gradient Norm: 11.13505968180801
Epoch: 7166, Batch Gradient Norm after: 11.13505968180801
Epoch 7167/10000, Prediction Accuracy = 62.657999999999994%, Loss = 0.4021018326282501
Epoch: 7167, Batch Gradient Norm: 11.43372217177953
Epoch: 7167, Batch Gradient Norm after: 11.43372217177953
Epoch 7168/10000, Prediction Accuracy = 62.676%, Loss = 0.4054664671421051
Epoch: 7168, Batch Gradient Norm: 11.167446848521243
Epoch: 7168, Batch Gradient Norm after: 11.167446848521243
Epoch 7169/10000, Prediction Accuracy = 62.64000000000001%, Loss = 0.40610116720199585
Epoch: 7169, Batch Gradient Norm: 8.33724133395163
Epoch: 7169, Batch Gradient Norm after: 8.33724133395163
Epoch 7170/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.387474399805069
Epoch: 7170, Batch Gradient Norm: 8.372816815414964
Epoch: 7170, Batch Gradient Norm after: 8.372816815414964
Epoch 7171/10000, Prediction Accuracy = 62.85600000000001%, Loss = 0.38587306141853334
Epoch: 7171, Batch Gradient Norm: 11.491292848602424
Epoch: 7171, Batch Gradient Norm after: 11.491292848602424
Epoch 7172/10000, Prediction Accuracy = 62.666%, Loss = 0.40500408411026
Epoch: 7172, Batch Gradient Norm: 11.646279857315825
Epoch: 7172, Batch Gradient Norm after: 11.646279857315825
Epoch 7173/10000, Prediction Accuracy = 62.73%, Loss = 0.4064925014972687
Epoch: 7173, Batch Gradient Norm: 9.593783021948667
Epoch: 7173, Batch Gradient Norm after: 9.593783021948667
Epoch 7174/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.3932140529155731
Epoch: 7174, Batch Gradient Norm: 8.299039202540632
Epoch: 7174, Batch Gradient Norm after: 8.299039202540632
Epoch 7175/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3851686716079712
Epoch: 7175, Batch Gradient Norm: 10.677129083291753
Epoch: 7175, Batch Gradient Norm after: 10.677129083291753
Epoch 7176/10000, Prediction Accuracy = 62.70799999999999%, Loss = 0.40015444755554197
Epoch: 7176, Batch Gradient Norm: 11.57610844446004
Epoch: 7176, Batch Gradient Norm after: 11.57610844446004
Epoch 7177/10000, Prediction Accuracy = 62.672000000000004%, Loss = 0.4091200351715088
Epoch: 7177, Batch Gradient Norm: 8.690613146567015
Epoch: 7177, Batch Gradient Norm after: 8.690613146567015
Epoch 7178/10000, Prediction Accuracy = 62.762%, Loss = 0.38931295871734617
Epoch: 7178, Batch Gradient Norm: 7.253319307853121
Epoch: 7178, Batch Gradient Norm after: 7.253319307853121
Epoch 7179/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38130866289138793
Epoch: 7179, Batch Gradient Norm: 8.709343479190665
Epoch: 7179, Batch Gradient Norm after: 8.709343479190665
Epoch 7180/10000, Prediction Accuracy = 62.722%, Loss = 0.38878564834594725
Epoch: 7180, Batch Gradient Norm: 10.010971823416574
Epoch: 7180, Batch Gradient Norm after: 10.010971823416574
Epoch 7181/10000, Prediction Accuracy = 62.658%, Loss = 0.3992722988128662
Epoch: 7181, Batch Gradient Norm: 9.1036263401859
Epoch: 7181, Batch Gradient Norm after: 9.1036263401859
Epoch 7182/10000, Prediction Accuracy = 62.718%, Loss = 0.3930130660533905
Epoch: 7182, Batch Gradient Norm: 10.925522050826544
Epoch: 7182, Batch Gradient Norm after: 10.925522050826544
Epoch 7183/10000, Prediction Accuracy = 62.662%, Loss = 0.4019658982753754
Epoch: 7183, Batch Gradient Norm: 11.917853868729875
Epoch: 7183, Batch Gradient Norm after: 11.917853868729875
Epoch 7184/10000, Prediction Accuracy = 62.674%, Loss = 0.4086664319038391
Epoch: 7184, Batch Gradient Norm: 10.380117533437637
Epoch: 7184, Batch Gradient Norm after: 10.380117533437637
Epoch 7185/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.3972143232822418
Epoch: 7185, Batch Gradient Norm: 10.833615622109342
Epoch: 7185, Batch Gradient Norm after: 10.833615622109342
Epoch 7186/10000, Prediction Accuracy = 62.532000000000004%, Loss = 0.4022770583629608
Epoch: 7186, Batch Gradient Norm: 11.551843296202343
Epoch: 7186, Batch Gradient Norm after: 11.551843296202343
Epoch 7187/10000, Prediction Accuracy = 62.698%, Loss = 0.41008002161979673
Epoch: 7187, Batch Gradient Norm: 10.172580876418609
Epoch: 7187, Batch Gradient Norm after: 10.172580876418609
Epoch 7188/10000, Prediction Accuracy = 62.61800000000001%, Loss = 0.39792981147766116
Epoch: 7188, Batch Gradient Norm: 9.038132578571794
Epoch: 7188, Batch Gradient Norm after: 9.038132578571794
Epoch 7189/10000, Prediction Accuracy = 62.668000000000006%, Loss = 0.3895732402801514
Epoch: 7189, Batch Gradient Norm: 8.79486358237969
Epoch: 7189, Batch Gradient Norm after: 8.79486358237969
Epoch 7190/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.3874211847782135
Epoch: 7190, Batch Gradient Norm: 9.724099547757826
Epoch: 7190, Batch Gradient Norm after: 9.724099547757826
Epoch 7191/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3933356285095215
Epoch: 7191, Batch Gradient Norm: 11.277857241064659
Epoch: 7191, Batch Gradient Norm after: 11.277857241064659
Epoch 7192/10000, Prediction Accuracy = 62.71%, Loss = 0.4061733365058899
Epoch: 7192, Batch Gradient Norm: 11.014811012906405
Epoch: 7192, Batch Gradient Norm after: 11.014811012906405
Epoch 7193/10000, Prediction Accuracy = 62.712%, Loss = 0.40263891220092773
Epoch: 7193, Batch Gradient Norm: 12.4548369780646
Epoch: 7193, Batch Gradient Norm after: 12.4548369780646
Epoch 7194/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.4131017982959747
Epoch: 7194, Batch Gradient Norm: 11.073860373401676
Epoch: 7194, Batch Gradient Norm after: 11.073860373401676
Epoch 7195/10000, Prediction Accuracy = 62.67999999999999%, Loss = 0.40366545915603635
Epoch: 7195, Batch Gradient Norm: 8.515085352673422
Epoch: 7195, Batch Gradient Norm after: 8.515085352673422
Epoch 7196/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.38636804223060606
Epoch: 7196, Batch Gradient Norm: 9.71101550120845
Epoch: 7196, Batch Gradient Norm after: 9.71101550120845
Epoch 7197/10000, Prediction Accuracy = 62.574%, Loss = 0.39239230155944826
Epoch: 7197, Batch Gradient Norm: 10.87706160568963
Epoch: 7197, Batch Gradient Norm after: 10.87706160568963
Epoch 7198/10000, Prediction Accuracy = 62.596000000000004%, Loss = 0.40168086886405946
Epoch: 7198, Batch Gradient Norm: 11.026283290201444
Epoch: 7198, Batch Gradient Norm after: 11.026283290201444
Epoch 7199/10000, Prediction Accuracy = 62.698%, Loss = 0.4048126876354218
Epoch: 7199, Batch Gradient Norm: 8.780696312344784
Epoch: 7199, Batch Gradient Norm after: 8.780696312344784
Epoch 7200/10000, Prediction Accuracy = 62.812%, Loss = 0.38923360109329225
Epoch: 7200, Batch Gradient Norm: 7.766577057272185
Epoch: 7200, Batch Gradient Norm after: 7.766577057272185
Epoch 7201/10000, Prediction Accuracy = 62.822%, Loss = 0.3820077359676361
Epoch: 7201, Batch Gradient Norm: 9.492206351146727
Epoch: 7201, Batch Gradient Norm after: 9.492206351146727
Epoch 7202/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.390758091211319
Epoch: 7202, Batch Gradient Norm: 10.652824569937112
Epoch: 7202, Batch Gradient Norm after: 10.652824569937112
Epoch 7203/10000, Prediction Accuracy = 62.576%, Loss = 0.3988718748092651
Epoch: 7203, Batch Gradient Norm: 9.847666383069754
Epoch: 7203, Batch Gradient Norm after: 9.847666383069754
Epoch 7204/10000, Prediction Accuracy = 62.864%, Loss = 0.3951355218887329
Epoch: 7204, Batch Gradient Norm: 8.791503321210685
Epoch: 7204, Batch Gradient Norm after: 8.791503321210685
Epoch 7205/10000, Prediction Accuracy = 62.65400000000001%, Loss = 0.38885737657547
Epoch: 7205, Batch Gradient Norm: 8.388796416839412
Epoch: 7205, Batch Gradient Norm after: 8.388796416839412
Epoch 7206/10000, Prediction Accuracy = 62.814%, Loss = 0.3865340828895569
Epoch: 7206, Batch Gradient Norm: 9.088711223346333
Epoch: 7206, Batch Gradient Norm after: 9.088711223346333
Epoch 7207/10000, Prediction Accuracy = 62.678%, Loss = 0.3894535779953003
Epoch: 7207, Batch Gradient Norm: 11.23842019119114
Epoch: 7207, Batch Gradient Norm after: 11.23842019119114
Epoch 7208/10000, Prediction Accuracy = 62.708000000000006%, Loss = 0.4043001472949982
Epoch: 7208, Batch Gradient Norm: 10.940148959369568
Epoch: 7208, Batch Gradient Norm after: 10.940148959369568
Epoch 7209/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.4029081404209137
Epoch: 7209, Batch Gradient Norm: 9.713359471216014
Epoch: 7209, Batch Gradient Norm after: 9.713359471216014
Epoch 7210/10000, Prediction Accuracy = 62.75600000000001%, Loss = 0.3928906500339508
Epoch: 7210, Batch Gradient Norm: 10.791590474978648
Epoch: 7210, Batch Gradient Norm after: 10.791590474978648
Epoch 7211/10000, Prediction Accuracy = 62.806%, Loss = 0.3997676968574524
Epoch: 7211, Batch Gradient Norm: 12.112493348647336
Epoch: 7211, Batch Gradient Norm after: 12.112493348647336
Epoch 7212/10000, Prediction Accuracy = 62.634%, Loss = 0.41025254130363464
Epoch: 7212, Batch Gradient Norm: 10.777390554288015
Epoch: 7212, Batch Gradient Norm after: 10.777390554288015
Epoch 7213/10000, Prediction Accuracy = 62.746%, Loss = 0.4005249202251434
Epoch: 7213, Batch Gradient Norm: 9.664979128559647
Epoch: 7213, Batch Gradient Norm after: 9.664979128559647
Epoch 7214/10000, Prediction Accuracy = 62.712%, Loss = 0.3922857463359833
Epoch: 7214, Batch Gradient Norm: 9.792730242136058
Epoch: 7214, Batch Gradient Norm after: 9.792730242136058
Epoch 7215/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.39362406730651855
Epoch: 7215, Batch Gradient Norm: 9.39271142956336
Epoch: 7215, Batch Gradient Norm after: 9.39271142956336
Epoch 7216/10000, Prediction Accuracy = 62.67%, Loss = 0.3919462740421295
Epoch: 7216, Batch Gradient Norm: 10.303981168732488
Epoch: 7216, Batch Gradient Norm after: 10.303981168732488
Epoch 7217/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.3984706044197083
Epoch: 7217, Batch Gradient Norm: 12.723796800215299
Epoch: 7217, Batch Gradient Norm after: 12.723796800215299
Epoch 7218/10000, Prediction Accuracy = 62.628%, Loss = 0.41589624285697935
Epoch: 7218, Batch Gradient Norm: 10.989777060521918
Epoch: 7218, Batch Gradient Norm after: 10.989777060521918
Epoch 7219/10000, Prediction Accuracy = 62.769999999999996%, Loss = 0.4026613414287567
Epoch: 7219, Batch Gradient Norm: 9.013557644478798
Epoch: 7219, Batch Gradient Norm after: 9.013557644478798
Epoch 7220/10000, Prediction Accuracy = 62.602%, Loss = 0.3891274809837341
Epoch: 7220, Batch Gradient Norm: 7.534843106059109
Epoch: 7220, Batch Gradient Norm after: 7.534843106059109
Epoch 7221/10000, Prediction Accuracy = 62.698%, Loss = 0.38097777366638186
Epoch: 7221, Batch Gradient Norm: 8.38132341766082
Epoch: 7221, Batch Gradient Norm after: 8.38132341766082
Epoch 7222/10000, Prediction Accuracy = 62.75%, Loss = 0.3860046625137329
Epoch: 7222, Batch Gradient Norm: 10.420684897201014
Epoch: 7222, Batch Gradient Norm after: 10.420684897201014
Epoch 7223/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.4002863228321075
Epoch: 7223, Batch Gradient Norm: 10.287001528659435
Epoch: 7223, Batch Gradient Norm after: 10.287001528659435
Epoch 7224/10000, Prediction Accuracy = 62.818000000000005%, Loss = 0.39932199120521544
Epoch: 7224, Batch Gradient Norm: 10.189713398509719
Epoch: 7224, Batch Gradient Norm after: 10.189713398509719
Epoch 7225/10000, Prediction Accuracy = 62.706%, Loss = 0.3956477463245392
Epoch: 7225, Batch Gradient Norm: 12.054776451808117
Epoch: 7225, Batch Gradient Norm after: 12.054776451808117
Epoch 7226/10000, Prediction Accuracy = 62.69%, Loss = 0.4093382179737091
Epoch: 7226, Batch Gradient Norm: 10.747515976325131
Epoch: 7226, Batch Gradient Norm after: 10.747515976325131
Epoch 7227/10000, Prediction Accuracy = 62.658%, Loss = 0.40200822353363036
Epoch: 7227, Batch Gradient Norm: 8.081716417300191
Epoch: 7227, Batch Gradient Norm after: 8.081716417300191
Epoch 7228/10000, Prediction Accuracy = 62.705999999999996%, Loss = 0.3845669388771057
Epoch: 7228, Batch Gradient Norm: 8.918682798963827
Epoch: 7228, Batch Gradient Norm after: 8.918682798963827
Epoch 7229/10000, Prediction Accuracy = 62.75999999999999%, Loss = 0.3877246081829071
Epoch: 7229, Batch Gradient Norm: 10.83899406335396
Epoch: 7229, Batch Gradient Norm after: 10.83899406335396
Epoch 7230/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.40062338709831236
Epoch: 7230, Batch Gradient Norm: 10.303837076955242
Epoch: 7230, Batch Gradient Norm after: 10.303837076955242
Epoch 7231/10000, Prediction Accuracy = 62.782%, Loss = 0.39645408987998965
Epoch: 7231, Batch Gradient Norm: 10.194713064639608
Epoch: 7231, Batch Gradient Norm after: 10.194713064639608
Epoch 7232/10000, Prediction Accuracy = 62.824%, Loss = 0.3954936683177948
Epoch: 7232, Batch Gradient Norm: 9.942183166620906
Epoch: 7232, Batch Gradient Norm after: 9.942183166620906
Epoch 7233/10000, Prediction Accuracy = 62.632000000000005%, Loss = 0.39465020298957826
Epoch: 7233, Batch Gradient Norm: 9.668248838887259
Epoch: 7233, Batch Gradient Norm after: 9.668248838887259
Epoch 7234/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.39256287813186647
Epoch: 7234, Batch Gradient Norm: 11.939487884721526
Epoch: 7234, Batch Gradient Norm after: 11.939487884721526
Epoch 7235/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.4087209165096283
Epoch: 7235, Batch Gradient Norm: 10.339507977346987
Epoch: 7235, Batch Gradient Norm after: 10.339507977346987
Epoch 7236/10000, Prediction Accuracy = 62.8%, Loss = 0.39774792194366454
Epoch: 7236, Batch Gradient Norm: 9.147698894315191
Epoch: 7236, Batch Gradient Norm after: 9.147698894315191
Epoch 7237/10000, Prediction Accuracy = 62.858000000000004%, Loss = 0.3896572828292847
Epoch: 7237, Batch Gradient Norm: 9.673175754900532
Epoch: 7237, Batch Gradient Norm after: 9.673175754900532
Epoch 7238/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.39305058121681213
Epoch: 7238, Batch Gradient Norm: 10.659909044712773
Epoch: 7238, Batch Gradient Norm after: 10.659909044712773
Epoch 7239/10000, Prediction Accuracy = 62.684000000000005%, Loss = 0.4007046639919281
Epoch: 7239, Batch Gradient Norm: 10.632646227814913
Epoch: 7239, Batch Gradient Norm after: 10.632646227814913
Epoch 7240/10000, Prediction Accuracy = 62.712%, Loss = 0.4000693917274475
Epoch: 7240, Batch Gradient Norm: 9.547156995680552
Epoch: 7240, Batch Gradient Norm after: 9.547156995680552
Epoch 7241/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.39206054210662844
Epoch: 7241, Batch Gradient Norm: 9.520073391305479
Epoch: 7241, Batch Gradient Norm after: 9.520073391305479
Epoch 7242/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.3914927303791046
Epoch: 7242, Batch Gradient Norm: 9.86372654317585
Epoch: 7242, Batch Gradient Norm after: 9.86372654317585
Epoch 7243/10000, Prediction Accuracy = 62.602%, Loss = 0.39363621473312377
Epoch: 7243, Batch Gradient Norm: 10.307060833325394
Epoch: 7243, Batch Gradient Norm after: 10.307060833325394
Epoch 7244/10000, Prediction Accuracy = 62.71999999999999%, Loss = 0.396535187959671
Epoch: 7244, Batch Gradient Norm: 10.878559687444456
Epoch: 7244, Batch Gradient Norm after: 10.878559687444456
Epoch 7245/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.40067472457885744
Epoch: 7245, Batch Gradient Norm: 10.325814703616622
Epoch: 7245, Batch Gradient Norm after: 10.325814703616622
Epoch 7246/10000, Prediction Accuracy = 62.722%, Loss = 0.39655917286872866
Epoch: 7246, Batch Gradient Norm: 10.973929047883836
Epoch: 7246, Batch Gradient Norm after: 10.973929047883836
Epoch 7247/10000, Prediction Accuracy = 62.634%, Loss = 0.4016578733921051
Epoch: 7247, Batch Gradient Norm: 10.693264989671999
Epoch: 7247, Batch Gradient Norm after: 10.693264989671999
Epoch 7248/10000, Prediction Accuracy = 62.660000000000004%, Loss = 0.4007221102714539
Epoch: 7248, Batch Gradient Norm: 8.7534374826499
Epoch: 7248, Batch Gradient Norm after: 8.7534374826499
Epoch 7249/10000, Prediction Accuracy = 62.646%, Loss = 0.38766165971755984
Epoch: 7249, Batch Gradient Norm: 8.738640680694491
Epoch: 7249, Batch Gradient Norm after: 8.738640680694491
Epoch 7250/10000, Prediction Accuracy = 62.75999999999999%, Loss = 0.3864972829818726
Epoch: 7250, Batch Gradient Norm: 10.45844823158029
Epoch: 7250, Batch Gradient Norm after: 10.45844823158029
Epoch 7251/10000, Prediction Accuracy = 62.674%, Loss = 0.39753648042678835
Epoch: 7251, Batch Gradient Norm: 11.34840147788181
Epoch: 7251, Batch Gradient Norm after: 11.34840147788181
Epoch 7252/10000, Prediction Accuracy = 62.78399999999999%, Loss = 0.40404488444328307
Epoch: 7252, Batch Gradient Norm: 11.450496003998218
Epoch: 7252, Batch Gradient Norm after: 11.450496003998218
Epoch 7253/10000, Prediction Accuracy = 62.814%, Loss = 0.40305296778678895
Epoch: 7253, Batch Gradient Norm: 10.14091596235459
Epoch: 7253, Batch Gradient Norm after: 10.14091596235459
Epoch 7254/10000, Prediction Accuracy = 62.662%, Loss = 0.39354525208473207
Epoch: 7254, Batch Gradient Norm: 8.809737070461997
Epoch: 7254, Batch Gradient Norm after: 8.809737070461997
Epoch 7255/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.38588696122169497
Epoch: 7255, Batch Gradient Norm: 9.683775238583106
Epoch: 7255, Batch Gradient Norm after: 9.683775238583106
Epoch 7256/10000, Prediction Accuracy = 62.788%, Loss = 0.39220731258392333
Epoch: 7256, Batch Gradient Norm: 11.181457008721226
Epoch: 7256, Batch Gradient Norm after: 11.181457008721226
Epoch 7257/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.405175918340683
Epoch: 7257, Batch Gradient Norm: 10.57653952634243
Epoch: 7257, Batch Gradient Norm after: 10.57653952634243
Epoch 7258/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.40112019181251524
Epoch: 7258, Batch Gradient Norm: 9.928874139810066
Epoch: 7258, Batch Gradient Norm after: 9.928874139810066
Epoch 7259/10000, Prediction Accuracy = 62.624%, Loss = 0.39393640160560606
Epoch: 7259, Batch Gradient Norm: 9.2915017786574
Epoch: 7259, Batch Gradient Norm after: 9.2915017786574
Epoch 7260/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.3891825437545776
Epoch: 7260, Batch Gradient Norm: 9.904603241359453
Epoch: 7260, Batch Gradient Norm after: 9.904603241359453
Epoch 7261/10000, Prediction Accuracy = 62.788%, Loss = 0.3931025505065918
Epoch: 7261, Batch Gradient Norm: 10.159137162040217
Epoch: 7261, Batch Gradient Norm after: 10.159137162040217
Epoch 7262/10000, Prediction Accuracy = 62.818%, Loss = 0.39746511578559873
Epoch: 7262, Batch Gradient Norm: 8.361053917711775
Epoch: 7262, Batch Gradient Norm after: 8.361053917711775
Epoch 7263/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.3860423624515533
Epoch: 7263, Batch Gradient Norm: 9.400046994531548
Epoch: 7263, Batch Gradient Norm after: 9.400046994531548
Epoch 7264/10000, Prediction Accuracy = 62.762%, Loss = 0.3905168056488037
Epoch: 7264, Batch Gradient Norm: 13.656369782961871
Epoch: 7264, Batch Gradient Norm after: 13.656369782961871
Epoch 7265/10000, Prediction Accuracy = 62.626%, Loss = 0.4239003658294678
Epoch: 7265, Batch Gradient Norm: 11.922458145136419
Epoch: 7265, Batch Gradient Norm after: 11.922458145136419
Epoch 7266/10000, Prediction Accuracy = 62.73%, Loss = 0.41013752222061156
Epoch: 7266, Batch Gradient Norm: 8.606107253837683
Epoch: 7266, Batch Gradient Norm after: 8.606107253837683
Epoch 7267/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3863255262374878
Epoch: 7267, Batch Gradient Norm: 7.914767828476372
Epoch: 7267, Batch Gradient Norm after: 7.914767828476372
Epoch 7268/10000, Prediction Accuracy = 62.802%, Loss = 0.3822583079338074
Epoch: 7268, Batch Gradient Norm: 8.434133862533377
Epoch: 7268, Batch Gradient Norm after: 8.434133862533377
Epoch 7269/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.38408873677253724
Epoch: 7269, Batch Gradient Norm: 11.294753560225956
Epoch: 7269, Batch Gradient Norm after: 11.294753560225956
Epoch 7270/10000, Prediction Accuracy = 62.714%, Loss = 0.4033345222473145
Epoch: 7270, Batch Gradient Norm: 10.880779685085363
Epoch: 7270, Batch Gradient Norm after: 10.880779685085363
Epoch 7271/10000, Prediction Accuracy = 62.674%, Loss = 0.400843071937561
Epoch: 7271, Batch Gradient Norm: 10.504599925903745
Epoch: 7271, Batch Gradient Norm after: 10.504599925903745
Epoch 7272/10000, Prediction Accuracy = 62.652%, Loss = 0.39700828194618226
Epoch: 7272, Batch Gradient Norm: 10.969853639881004
Epoch: 7272, Batch Gradient Norm after: 10.969853639881004
Epoch 7273/10000, Prediction Accuracy = 62.604%, Loss = 0.3991647958755493
Epoch: 7273, Batch Gradient Norm: 10.118219799544127
Epoch: 7273, Batch Gradient Norm after: 10.118219799544127
Epoch 7274/10000, Prediction Accuracy = 62.712%, Loss = 0.39405243992805483
Epoch: 7274, Batch Gradient Norm: 9.411596928162535
Epoch: 7274, Batch Gradient Norm after: 9.411596928162535
Epoch 7275/10000, Prediction Accuracy = 62.698%, Loss = 0.39053875207901
Epoch: 7275, Batch Gradient Norm: 9.21510294801419
Epoch: 7275, Batch Gradient Norm after: 9.21510294801419
Epoch 7276/10000, Prediction Accuracy = 62.91600000000001%, Loss = 0.38961299061775206
Epoch: 7276, Batch Gradient Norm: 10.854130123246632
Epoch: 7276, Batch Gradient Norm after: 10.854130123246632
Epoch 7277/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.4001909554004669
Epoch: 7277, Batch Gradient Norm: 11.667735508538497
Epoch: 7277, Batch Gradient Norm after: 11.667735508538497
Epoch 7278/10000, Prediction Accuracy = 62.806%, Loss = 0.4064383924007416
Epoch: 7278, Batch Gradient Norm: 9.062809563664743
Epoch: 7278, Batch Gradient Norm after: 9.062809563664743
Epoch 7279/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.3886573851108551
Epoch: 7279, Batch Gradient Norm: 8.322099804963981
Epoch: 7279, Batch Gradient Norm after: 8.322099804963981
Epoch 7280/10000, Prediction Accuracy = 62.742%, Loss = 0.38318572044372556
Epoch: 7280, Batch Gradient Norm: 10.487730187686667
Epoch: 7280, Batch Gradient Norm after: 10.487730187686667
Epoch 7281/10000, Prediction Accuracy = 62.702%, Loss = 0.3970856785774231
Epoch: 7281, Batch Gradient Norm: 10.841034996916608
Epoch: 7281, Batch Gradient Norm after: 10.841034996916608
Epoch 7282/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.4004136800765991
Epoch: 7282, Batch Gradient Norm: 9.147231708641252
Epoch: 7282, Batch Gradient Norm after: 9.147231708641252
Epoch 7283/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.39030652642250063
Epoch: 7283, Batch Gradient Norm: 8.740647850012094
Epoch: 7283, Batch Gradient Norm after: 8.740647850012094
Epoch 7284/10000, Prediction Accuracy = 62.734%, Loss = 0.3864184141159058
Epoch: 7284, Batch Gradient Norm: 9.774806992238421
Epoch: 7284, Batch Gradient Norm after: 9.774806992238421
Epoch 7285/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.39236280918121336
Epoch: 7285, Batch Gradient Norm: 10.179464532592068
Epoch: 7285, Batch Gradient Norm after: 10.179464532592068
Epoch 7286/10000, Prediction Accuracy = 62.786%, Loss = 0.3942802965641022
Epoch: 7286, Batch Gradient Norm: 11.375655857268212
Epoch: 7286, Batch Gradient Norm after: 11.375655857268212
Epoch 7287/10000, Prediction Accuracy = 62.589999999999996%, Loss = 0.4021631062030792
Epoch: 7287, Batch Gradient Norm: 11.568599954696083
Epoch: 7287, Batch Gradient Norm after: 11.568599954696083
Epoch 7288/10000, Prediction Accuracy = 62.71400000000001%, Loss = 0.40650765895843505
Epoch: 7288, Batch Gradient Norm: 9.724204531700302
Epoch: 7288, Batch Gradient Norm after: 9.724204531700302
Epoch 7289/10000, Prediction Accuracy = 62.676%, Loss = 0.393965345621109
Epoch: 7289, Batch Gradient Norm: 9.019030959095652
Epoch: 7289, Batch Gradient Norm after: 9.019030959095652
Epoch 7290/10000, Prediction Accuracy = 62.802%, Loss = 0.38839406371116636
Epoch: 7290, Batch Gradient Norm: 9.57110526679998
Epoch: 7290, Batch Gradient Norm after: 9.57110526679998
Epoch 7291/10000, Prediction Accuracy = 62.7%, Loss = 0.39137897491455076
Epoch: 7291, Batch Gradient Norm: 9.73021596479941
Epoch: 7291, Batch Gradient Norm after: 9.73021596479941
Epoch 7292/10000, Prediction Accuracy = 62.617999999999995%, Loss = 0.3917154252529144
Epoch: 7292, Batch Gradient Norm: 9.262256121551202
Epoch: 7292, Batch Gradient Norm after: 9.262256121551202
Epoch 7293/10000, Prediction Accuracy = 62.778%, Loss = 0.3893379271030426
Epoch: 7293, Batch Gradient Norm: 10.160344616271537
Epoch: 7293, Batch Gradient Norm after: 10.160344616271537
Epoch 7294/10000, Prediction Accuracy = 62.75%, Loss = 0.39618816375732424
Epoch: 7294, Batch Gradient Norm: 11.10357869181111
Epoch: 7294, Batch Gradient Norm after: 11.10357869181111
Epoch 7295/10000, Prediction Accuracy = 62.69%, Loss = 0.4023437023162842
Epoch: 7295, Batch Gradient Norm: 10.77648426414797
Epoch: 7295, Batch Gradient Norm after: 10.77648426414797
Epoch 7296/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.39960435032844543
Epoch: 7296, Batch Gradient Norm: 9.200512703427378
Epoch: 7296, Batch Gradient Norm after: 9.200512703427378
Epoch 7297/10000, Prediction Accuracy = 62.726%, Loss = 0.389012998342514
Epoch: 7297, Batch Gradient Norm: 8.85888127833463
Epoch: 7297, Batch Gradient Norm after: 8.85888127833463
Epoch 7298/10000, Prediction Accuracy = 62.843999999999994%, Loss = 0.38805427551269533
Epoch: 7298, Batch Gradient Norm: 8.718051483110374
Epoch: 7298, Batch Gradient Norm after: 8.718051483110374
Epoch 7299/10000, Prediction Accuracy = 62.838%, Loss = 0.3877403974533081
Epoch: 7299, Batch Gradient Norm: 12.714682399607526
Epoch: 7299, Batch Gradient Norm after: 12.714682399607526
Epoch 7300/10000, Prediction Accuracy = 62.65%, Loss = 0.4143333971500397
Epoch: 7300, Batch Gradient Norm: 13.51209225912138
Epoch: 7300, Batch Gradient Norm after: 13.51209225912138
Epoch 7301/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.42087750434875487
Epoch: 7301, Batch Gradient Norm: 9.726305874168453
Epoch: 7301, Batch Gradient Norm after: 9.726305874168453
Epoch 7302/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.39182978868484497
Epoch: 7302, Batch Gradient Norm: 7.737646561931831
Epoch: 7302, Batch Gradient Norm after: 7.737646561931831
Epoch 7303/10000, Prediction Accuracy = 62.612%, Loss = 0.38090542554855344
Epoch: 7303, Batch Gradient Norm: 8.145593584954076
Epoch: 7303, Batch Gradient Norm after: 8.145593584954076
Epoch 7304/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.38228626251220704
Epoch: 7304, Batch Gradient Norm: 11.126869367924746
Epoch: 7304, Batch Gradient Norm after: 11.126869367924746
Epoch 7305/10000, Prediction Accuracy = 62.712%, Loss = 0.40059013962745665
Epoch: 7305, Batch Gradient Norm: 11.867922807537298
Epoch: 7305, Batch Gradient Norm after: 11.867922807537298
Epoch 7306/10000, Prediction Accuracy = 62.842%, Loss = 0.4067244648933411
Epoch: 7306, Batch Gradient Norm: 10.1360084700722
Epoch: 7306, Batch Gradient Norm after: 10.1360084700722
Epoch 7307/10000, Prediction Accuracy = 62.664%, Loss = 0.39504040479660035
Epoch: 7307, Batch Gradient Norm: 10.240082187461836
Epoch: 7307, Batch Gradient Norm after: 10.240082187461836
Epoch 7308/10000, Prediction Accuracy = 62.826%, Loss = 0.3948709726333618
Epoch: 7308, Batch Gradient Norm: 12.205794008056557
Epoch: 7308, Batch Gradient Norm after: 12.205794008056557
Epoch 7309/10000, Prediction Accuracy = 62.676%, Loss = 0.40795848369598386
Epoch: 7309, Batch Gradient Norm: 11.130082822839166
Epoch: 7309, Batch Gradient Norm after: 11.130082822839166
Epoch 7310/10000, Prediction Accuracy = 62.612%, Loss = 0.4008102059364319
Epoch: 7310, Batch Gradient Norm: 8.60452538495519
Epoch: 7310, Batch Gradient Norm after: 8.60452538495519
Epoch 7311/10000, Prediction Accuracy = 62.822%, Loss = 0.3847826302051544
Epoch: 7311, Batch Gradient Norm: 7.884656685700577
Epoch: 7311, Batch Gradient Norm after: 7.884656685700577
Epoch 7312/10000, Prediction Accuracy = 62.76400000000001%, Loss = 0.3816980361938477
Epoch: 7312, Batch Gradient Norm: 8.93508942393052
Epoch: 7312, Batch Gradient Norm after: 8.93508942393052
Epoch 7313/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.38807247281074525
Epoch: 7313, Batch Gradient Norm: 10.47161127763892
Epoch: 7313, Batch Gradient Norm after: 10.47161127763892
Epoch 7314/10000, Prediction Accuracy = 62.81400000000001%, Loss = 0.398404723405838
Epoch: 7314, Batch Gradient Norm: 10.713642390469955
Epoch: 7314, Batch Gradient Norm after: 10.713642390469955
Epoch 7315/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3998951852321625
Epoch: 7315, Batch Gradient Norm: 9.0733065185143
Epoch: 7315, Batch Gradient Norm after: 9.0733065185143
Epoch 7316/10000, Prediction Accuracy = 62.698%, Loss = 0.3882766604423523
Epoch: 7316, Batch Gradient Norm: 8.421697006479295
Epoch: 7316, Batch Gradient Norm after: 8.421697006479295
Epoch 7317/10000, Prediction Accuracy = 62.774%, Loss = 0.3846998631954193
Epoch: 7317, Batch Gradient Norm: 9.138256410658776
Epoch: 7317, Batch Gradient Norm after: 9.138256410658776
Epoch 7318/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.38796855211257936
Epoch: 7318, Batch Gradient Norm: 10.237545172649096
Epoch: 7318, Batch Gradient Norm after: 10.237545172649096
Epoch 7319/10000, Prediction Accuracy = 62.762%, Loss = 0.39404410123825073
Epoch: 7319, Batch Gradient Norm: 10.486178726284852
Epoch: 7319, Batch Gradient Norm after: 10.486178726284852
Epoch 7320/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.39563579559326173
Epoch: 7320, Batch Gradient Norm: 9.403549241958942
Epoch: 7320, Batch Gradient Norm after: 9.403549241958942
Epoch 7321/10000, Prediction Accuracy = 62.676%, Loss = 0.3883234143257141
Epoch: 7321, Batch Gradient Norm: 10.445650138351958
Epoch: 7321, Batch Gradient Norm after: 10.445650138351958
Epoch 7322/10000, Prediction Accuracy = 62.7%, Loss = 0.3955740869045258
Epoch: 7322, Batch Gradient Norm: 13.22177723849796
Epoch: 7322, Batch Gradient Norm after: 13.22177723849796
Epoch 7323/10000, Prediction Accuracy = 62.662%, Loss = 0.4181448519229889
Epoch: 7323, Batch Gradient Norm: 12.545907593842907
Epoch: 7323, Batch Gradient Norm after: 12.545907593842907
Epoch 7324/10000, Prediction Accuracy = 62.734%, Loss = 0.4136215388774872
Epoch: 7324, Batch Gradient Norm: 9.800946626317597
Epoch: 7324, Batch Gradient Norm after: 9.800946626317597
Epoch 7325/10000, Prediction Accuracy = 62.71%, Loss = 0.3908339738845825
Epoch: 7325, Batch Gradient Norm: 10.762070982326593
Epoch: 7325, Batch Gradient Norm after: 10.762070982326593
Epoch 7326/10000, Prediction Accuracy = 62.720000000000006%, Loss = 0.39645844101905825
Epoch: 7326, Batch Gradient Norm: 10.433528317695583
Epoch: 7326, Batch Gradient Norm after: 10.433528317695583
Epoch 7327/10000, Prediction Accuracy = 62.762%, Loss = 0.3966635882854462
Epoch: 7327, Batch Gradient Norm: 7.98317882573104
Epoch: 7327, Batch Gradient Norm after: 7.98317882573104
Epoch 7328/10000, Prediction Accuracy = 62.83200000000001%, Loss = 0.3816478192806244
Epoch: 7328, Batch Gradient Norm: 7.614788284473334
Epoch: 7328, Batch Gradient Norm after: 7.614788284473334
Epoch 7329/10000, Prediction Accuracy = 62.86999999999999%, Loss = 0.3798466920852661
Epoch: 7329, Batch Gradient Norm: 8.81472512770286
Epoch: 7329, Batch Gradient Norm after: 8.81472512770286
Epoch 7330/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38586402535438535
Epoch: 7330, Batch Gradient Norm: 9.983302038199744
Epoch: 7330, Batch Gradient Norm after: 9.983302038199744
Epoch 7331/10000, Prediction Accuracy = 62.69599999999999%, Loss = 0.3924962282180786
Epoch: 7331, Batch Gradient Norm: 12.050688739219767
Epoch: 7331, Batch Gradient Norm after: 12.050688739219767
Epoch 7332/10000, Prediction Accuracy = 62.733999999999995%, Loss = 0.40696852207183837
Epoch: 7332, Batch Gradient Norm: 11.798561454894212
Epoch: 7332, Batch Gradient Norm after: 11.798561454894212
Epoch 7333/10000, Prediction Accuracy = 62.646%, Loss = 0.407014000415802
Epoch: 7333, Batch Gradient Norm: 9.064123324801107
Epoch: 7333, Batch Gradient Norm after: 9.064123324801107
Epoch 7334/10000, Prediction Accuracy = 62.824%, Loss = 0.38824678063392637
Epoch: 7334, Batch Gradient Norm: 8.343120176468847
Epoch: 7334, Batch Gradient Norm after: 8.343120176468847
Epoch 7335/10000, Prediction Accuracy = 62.766%, Loss = 0.38338489532470704
Epoch: 7335, Batch Gradient Norm: 10.320411847196885
Epoch: 7335, Batch Gradient Norm after: 10.320411847196885
Epoch 7336/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3957287073135376
Epoch: 7336, Batch Gradient Norm: 11.817309545891641
Epoch: 7336, Batch Gradient Norm after: 11.817309545891641
Epoch 7337/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4067730069160461
Epoch: 7337, Batch Gradient Norm: 10.962074101808316
Epoch: 7337, Batch Gradient Norm after: 10.962074101808316
Epoch 7338/10000, Prediction Accuracy = 62.69200000000001%, Loss = 0.40029757022857665
Epoch: 7338, Batch Gradient Norm: 9.338983597290543
Epoch: 7338, Batch Gradient Norm after: 9.338983597290543
Epoch 7339/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.38947011828422545
Epoch: 7339, Batch Gradient Norm: 8.933914599049972
Epoch: 7339, Batch Gradient Norm after: 8.933914599049972
Epoch 7340/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.3873750567436218
Epoch: 7340, Batch Gradient Norm: 8.997439199376261
Epoch: 7340, Batch Gradient Norm after: 8.997439199376261
Epoch 7341/10000, Prediction Accuracy = 62.80400000000001%, Loss = 0.38791726231575013
Epoch: 7341, Batch Gradient Norm: 9.656731735916233
Epoch: 7341, Batch Gradient Norm after: 9.656731735916233
Epoch 7342/10000, Prediction Accuracy = 62.843999999999994%, Loss = 0.3922497868537903
Epoch: 7342, Batch Gradient Norm: 11.158790094811517
Epoch: 7342, Batch Gradient Norm after: 11.158790094811517
Epoch 7343/10000, Prediction Accuracy = 62.738%, Loss = 0.40212816596031187
Epoch: 7343, Batch Gradient Norm: 11.114762852789655
Epoch: 7343, Batch Gradient Norm after: 11.114762852789655
Epoch 7344/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.40235319137573244
Epoch: 7344, Batch Gradient Norm: 10.594344568586493
Epoch: 7344, Batch Gradient Norm after: 10.594344568586493
Epoch 7345/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.3975804030895233
Epoch: 7345, Batch Gradient Norm: 8.295142831402861
Epoch: 7345, Batch Gradient Norm after: 8.295142831402861
Epoch 7346/10000, Prediction Accuracy = 62.822%, Loss = 0.38321534395217893
Epoch: 7346, Batch Gradient Norm: 7.710160169559159
Epoch: 7346, Batch Gradient Norm after: 7.710160169559159
Epoch 7347/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3800238311290741
Epoch: 7347, Batch Gradient Norm: 8.100583666395734
Epoch: 7347, Batch Gradient Norm after: 8.100583666395734
Epoch 7348/10000, Prediction Accuracy = 62.83%, Loss = 0.3818518102169037
Epoch: 7348, Batch Gradient Norm: 8.951237759290967
Epoch: 7348, Batch Gradient Norm after: 8.951237759290967
Epoch 7349/10000, Prediction Accuracy = 62.727999999999994%, Loss = 0.38634021282196046
Epoch: 7349, Batch Gradient Norm: 9.71255092260819
Epoch: 7349, Batch Gradient Norm after: 9.71255092260819
Epoch 7350/10000, Prediction Accuracy = 62.862%, Loss = 0.3897113800048828
Epoch: 7350, Batch Gradient Norm: 11.683018227509232
Epoch: 7350, Batch Gradient Norm after: 11.683018227509232
Epoch 7351/10000, Prediction Accuracy = 62.58800000000001%, Loss = 0.40281547904014586
Epoch: 7351, Batch Gradient Norm: 12.76436785671469
Epoch: 7351, Batch Gradient Norm after: 12.76436785671469
Epoch 7352/10000, Prediction Accuracy = 62.824%, Loss = 0.41327733397483823
Epoch: 7352, Batch Gradient Norm: 12.066664136252545
Epoch: 7352, Batch Gradient Norm after: 12.066664136252545
Epoch 7353/10000, Prediction Accuracy = 62.734%, Loss = 0.4084161281585693
Epoch: 7353, Batch Gradient Norm: 11.498601289991619
Epoch: 7353, Batch Gradient Norm after: 11.498601289991619
Epoch 7354/10000, Prediction Accuracy = 62.681999999999995%, Loss = 0.40450518727302553
Epoch: 7354, Batch Gradient Norm: 9.663156830754252
Epoch: 7354, Batch Gradient Norm after: 9.663156830754252
Epoch 7355/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.39107093811035154
Epoch: 7355, Batch Gradient Norm: 8.736445184855683
Epoch: 7355, Batch Gradient Norm after: 8.736445184855683
Epoch 7356/10000, Prediction Accuracy = 62.826%, Loss = 0.38442826867103574
Epoch: 7356, Batch Gradient Norm: 10.261062264784623
Epoch: 7356, Batch Gradient Norm after: 10.261062264784623
Epoch 7357/10000, Prediction Accuracy = 62.67%, Loss = 0.3932908892631531
Epoch: 7357, Batch Gradient Norm: 11.524591968291007
Epoch: 7357, Batch Gradient Norm after: 11.524591968291007
Epoch 7358/10000, Prediction Accuracy = 62.754%, Loss = 0.4044878363609314
Epoch: 7358, Batch Gradient Norm: 10.125199689984344
Epoch: 7358, Batch Gradient Norm after: 10.125199689984344
Epoch 7359/10000, Prediction Accuracy = 62.762%, Loss = 0.39492802023887635
Epoch: 7359, Batch Gradient Norm: 9.061280074423872
Epoch: 7359, Batch Gradient Norm after: 9.061280074423872
Epoch 7360/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38583163022994993
Epoch: 7360, Batch Gradient Norm: 10.171327358734002
Epoch: 7360, Batch Gradient Norm after: 10.171327358734002
Epoch 7361/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.39389083385467527
Epoch: 7361, Batch Gradient Norm: 10.254643172917632
Epoch: 7361, Batch Gradient Norm after: 10.254643172917632
Epoch 7362/10000, Prediction Accuracy = 62.76800000000001%, Loss = 0.39529192447662354
Epoch: 7362, Batch Gradient Norm: 11.023198002934675
Epoch: 7362, Batch Gradient Norm after: 11.023198002934675
Epoch 7363/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.4002061903476715
Epoch: 7363, Batch Gradient Norm: 12.03831508310418
Epoch: 7363, Batch Gradient Norm after: 12.03831508310418
Epoch 7364/10000, Prediction Accuracy = 62.62399999999999%, Loss = 0.40682415962219237
Epoch: 7364, Batch Gradient Norm: 9.988451287827141
Epoch: 7364, Batch Gradient Norm after: 9.988451287827141
Epoch 7365/10000, Prediction Accuracy = 62.852%, Loss = 0.39220179319381715
Epoch: 7365, Batch Gradient Norm: 7.811206388003783
Epoch: 7365, Batch Gradient Norm after: 7.811206388003783
Epoch 7366/10000, Prediction Accuracy = 62.7%, Loss = 0.37906454205513
Epoch: 7366, Batch Gradient Norm: 7.376648866273708
Epoch: 7366, Batch Gradient Norm after: 7.376648866273708
Epoch 7367/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3768314182758331
Epoch: 7367, Batch Gradient Norm: 9.807718209675723
Epoch: 7367, Batch Gradient Norm after: 9.807718209675723
Epoch 7368/10000, Prediction Accuracy = 62.717999999999996%, Loss = 0.3926717936992645
Epoch: 7368, Batch Gradient Norm: 10.933358177596759
Epoch: 7368, Batch Gradient Norm after: 10.933358177596759
Epoch 7369/10000, Prediction Accuracy = 62.644000000000005%, Loss = 0.40107277035713196
Epoch: 7369, Batch Gradient Norm: 9.549734958078504
Epoch: 7369, Batch Gradient Norm after: 9.549734958078504
Epoch 7370/10000, Prediction Accuracy = 62.806%, Loss = 0.3912391662597656
Epoch: 7370, Batch Gradient Norm: 8.301662958038802
Epoch: 7370, Batch Gradient Norm after: 8.301662958038802
Epoch 7371/10000, Prediction Accuracy = 62.798%, Loss = 0.3832056403160095
Epoch: 7371, Batch Gradient Norm: 7.991196027788994
Epoch: 7371, Batch Gradient Norm after: 7.991196027788994
Epoch 7372/10000, Prediction Accuracy = 62.688%, Loss = 0.3812109470367432
Epoch: 7372, Batch Gradient Norm: 9.355660158015244
Epoch: 7372, Batch Gradient Norm after: 9.355660158015244
Epoch 7373/10000, Prediction Accuracy = 62.812%, Loss = 0.3891509473323822
Epoch: 7373, Batch Gradient Norm: 11.48346887475698
Epoch: 7373, Batch Gradient Norm after: 11.48346887475698
Epoch 7374/10000, Prediction Accuracy = 62.696000000000005%, Loss = 0.4021769523620605
Epoch: 7374, Batch Gradient Norm: 12.433890832962135
Epoch: 7374, Batch Gradient Norm after: 12.433890832962135
Epoch 7375/10000, Prediction Accuracy = 62.624%, Loss = 0.4100374042987823
Epoch: 7375, Batch Gradient Norm: 11.097119612670948
Epoch: 7375, Batch Gradient Norm after: 11.097119612670948
Epoch 7376/10000, Prediction Accuracy = 62.812%, Loss = 0.4015067994594574
Epoch: 7376, Batch Gradient Norm: 9.088519250090217
Epoch: 7376, Batch Gradient Norm after: 9.088519250090217
Epoch 7377/10000, Prediction Accuracy = 62.838%, Loss = 0.38829050660133363
Epoch: 7377, Batch Gradient Norm: 9.379103953030008
Epoch: 7377, Batch Gradient Norm after: 9.379103953030008
Epoch 7378/10000, Prediction Accuracy = 62.788%, Loss = 0.3886909544467926
Epoch: 7378, Batch Gradient Norm: 11.31024379065396
Epoch: 7378, Batch Gradient Norm after: 11.31024379065396
Epoch 7379/10000, Prediction Accuracy = 62.814%, Loss = 0.4012545108795166
Epoch: 7379, Batch Gradient Norm: 12.400652252158999
Epoch: 7379, Batch Gradient Norm after: 12.400652252158999
Epoch 7380/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.4089290976524353
Epoch: 7380, Batch Gradient Norm: 11.765895467603888
Epoch: 7380, Batch Gradient Norm after: 11.765895467603888
Epoch 7381/10000, Prediction Accuracy = 62.80999999999999%, Loss = 0.40362743735313417
Epoch: 7381, Batch Gradient Norm: 9.977181505653167
Epoch: 7381, Batch Gradient Norm after: 9.977181505653167
Epoch 7382/10000, Prediction Accuracy = 62.86%, Loss = 0.3906882703304291
Epoch: 7382, Batch Gradient Norm: 8.202667235838835
Epoch: 7382, Batch Gradient Norm after: 8.202667235838835
Epoch 7383/10000, Prediction Accuracy = 62.83200000000001%, Loss = 0.38034936785697937
Epoch: 7383, Batch Gradient Norm: 8.51638684632617
Epoch: 7383, Batch Gradient Norm after: 8.51638684632617
Epoch 7384/10000, Prediction Accuracy = 62.79%, Loss = 0.38254080414772035
Epoch: 7384, Batch Gradient Norm: 10.361069184303656
Epoch: 7384, Batch Gradient Norm after: 10.361069184303656
Epoch 7385/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.3964220345020294
Epoch: 7385, Batch Gradient Norm: 9.173442604722872
Epoch: 7385, Batch Gradient Norm after: 9.173442604722872
Epoch 7386/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.38823667764663694
Epoch: 7386, Batch Gradient Norm: 9.65713332215063
Epoch: 7386, Batch Gradient Norm after: 9.65713332215063
Epoch 7387/10000, Prediction Accuracy = 62.798%, Loss = 0.39122002720832827
Epoch: 7387, Batch Gradient Norm: 10.636017025459832
Epoch: 7387, Batch Gradient Norm after: 10.636017025459832
Epoch 7388/10000, Prediction Accuracy = 62.788%, Loss = 0.3983246922492981
Epoch: 7388, Batch Gradient Norm: 10.996840834920636
Epoch: 7388, Batch Gradient Norm after: 10.996840834920636
Epoch 7389/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.4021138370037079
Epoch: 7389, Batch Gradient Norm: 9.26611899234445
Epoch: 7389, Batch Gradient Norm after: 9.26611899234445
Epoch 7390/10000, Prediction Accuracy = 62.7%, Loss = 0.38906837701797486
Epoch: 7390, Batch Gradient Norm: 8.977343324697546
Epoch: 7390, Batch Gradient Norm after: 8.977343324697546
Epoch 7391/10000, Prediction Accuracy = 62.688%, Loss = 0.3853448212146759
Epoch: 7391, Batch Gradient Norm: 10.907857456927589
Epoch: 7391, Batch Gradient Norm after: 10.907857456927589
Epoch 7392/10000, Prediction Accuracy = 62.778%, Loss = 0.3963416039943695
Epoch: 7392, Batch Gradient Norm: 12.495992217603385
Epoch: 7392, Batch Gradient Norm after: 12.495992217603385
Epoch 7393/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.4091778755187988
Epoch: 7393, Batch Gradient Norm: 10.148739653061318
Epoch: 7393, Batch Gradient Norm after: 10.148739653061318
Epoch 7394/10000, Prediction Accuracy = 62.822%, Loss = 0.39374935030937197
Epoch: 7394, Batch Gradient Norm: 8.145808130271437
Epoch: 7394, Batch Gradient Norm after: 8.145808130271437
Epoch 7395/10000, Prediction Accuracy = 62.854%, Loss = 0.38219144344329836
Epoch: 7395, Batch Gradient Norm: 8.050031358526878
Epoch: 7395, Batch Gradient Norm after: 8.050031358526878
Epoch 7396/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.3820536136627197
Epoch: 7396, Batch Gradient Norm: 7.444186914387223
Epoch: 7396, Batch Gradient Norm after: 7.444186914387223
Epoch 7397/10000, Prediction Accuracy = 62.866%, Loss = 0.3783866107463837
Epoch: 7397, Batch Gradient Norm: 9.64135796616382
Epoch: 7397, Batch Gradient Norm after: 9.64135796616382
Epoch 7398/10000, Prediction Accuracy = 62.754%, Loss = 0.39001439809799193
Epoch: 7398, Batch Gradient Norm: 13.477732017100472
Epoch: 7398, Batch Gradient Norm after: 13.477732017100472
Epoch 7399/10000, Prediction Accuracy = 62.624%, Loss = 0.42028011083602906
Epoch: 7399, Batch Gradient Norm: 12.133055084884473
Epoch: 7399, Batch Gradient Norm after: 12.133055084884473
Epoch 7400/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.40794166922569275
Epoch: 7400, Batch Gradient Norm: 9.298825497250352
Epoch: 7400, Batch Gradient Norm after: 9.298825497250352
Epoch 7401/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.38645647168159486
Epoch: 7401, Batch Gradient Norm: 9.442461391599775
Epoch: 7401, Batch Gradient Norm after: 9.442461391599775
Epoch 7402/10000, Prediction Accuracy = 62.742000000000004%, Loss = 0.3859435498714447
Epoch: 7402, Batch Gradient Norm: 11.861317687640494
Epoch: 7402, Batch Gradient Norm after: 11.861317687640494
Epoch 7403/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.40309628248214724
Epoch: 7403, Batch Gradient Norm: 11.441942494761438
Epoch: 7403, Batch Gradient Norm after: 11.441942494761438
Epoch 7404/10000, Prediction Accuracy = 62.79599999999999%, Loss = 0.4025944471359253
Epoch: 7404, Batch Gradient Norm: 8.993434163716326
Epoch: 7404, Batch Gradient Norm after: 8.993434163716326
Epoch 7405/10000, Prediction Accuracy = 62.872%, Loss = 0.38733723759651184
Epoch: 7405, Batch Gradient Norm: 8.238506352049189
Epoch: 7405, Batch Gradient Norm after: 8.238506352049189
Epoch 7406/10000, Prediction Accuracy = 62.862%, Loss = 0.3821394920349121
Epoch: 7406, Batch Gradient Norm: 9.793290136540959
Epoch: 7406, Batch Gradient Norm after: 9.793290136540959
Epoch 7407/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.39180693626403806
Epoch: 7407, Batch Gradient Norm: 9.855265589342354
Epoch: 7407, Batch Gradient Norm after: 9.855265589342354
Epoch 7408/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.3919172763824463
Epoch: 7408, Batch Gradient Norm: 8.88284419678977
Epoch: 7408, Batch Gradient Norm after: 8.88284419678977
Epoch 7409/10000, Prediction Accuracy = 62.75%, Loss = 0.3853225469589233
Epoch: 7409, Batch Gradient Norm: 10.819607755151472
Epoch: 7409, Batch Gradient Norm after: 10.819607755151472
Epoch 7410/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.3967242419719696
Epoch: 7410, Batch Gradient Norm: 12.91419673446835
Epoch: 7410, Batch Gradient Norm after: 12.91419673446835
Epoch 7411/10000, Prediction Accuracy = 62.762%, Loss = 0.4122455954551697
Epoch: 7411, Batch Gradient Norm: 10.466764075718109
Epoch: 7411, Batch Gradient Norm after: 10.466764075718109
Epoch 7412/10000, Prediction Accuracy = 62.742%, Loss = 0.3938800096511841
Epoch: 7412, Batch Gradient Norm: 9.120265487762088
Epoch: 7412, Batch Gradient Norm after: 9.120265487762088
Epoch 7413/10000, Prediction Accuracy = 62.85%, Loss = 0.3848229289054871
Epoch: 7413, Batch Gradient Norm: 11.028562025618461
Epoch: 7413, Batch Gradient Norm after: 11.028562025618461
Epoch 7414/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.4007571280002594
Epoch: 7414, Batch Gradient Norm: 9.03148558759022
Epoch: 7414, Batch Gradient Norm after: 9.03148558759022
Epoch 7415/10000, Prediction Accuracy = 62.66799999999999%, Loss = 0.389001852273941
Epoch: 7415, Batch Gradient Norm: 8.146821077378437
Epoch: 7415, Batch Gradient Norm after: 8.146821077378437
Epoch 7416/10000, Prediction Accuracy = 62.79%, Loss = 0.38239923119544983
Epoch: 7416, Batch Gradient Norm: 8.450252432069588
Epoch: 7416, Batch Gradient Norm after: 8.450252432069588
Epoch 7417/10000, Prediction Accuracy = 62.702%, Loss = 0.38285791873931885
Epoch: 7417, Batch Gradient Norm: 10.650125531777087
Epoch: 7417, Batch Gradient Norm after: 10.650125531777087
Epoch 7418/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.39626453518867494
Epoch: 7418, Batch Gradient Norm: 10.89145560168654
Epoch: 7418, Batch Gradient Norm after: 10.89145560168654
Epoch 7419/10000, Prediction Accuracy = 62.85999999999999%, Loss = 0.3982280969619751
Epoch: 7419, Batch Gradient Norm: 9.059751136733404
Epoch: 7419, Batch Gradient Norm after: 9.059751136733404
Epoch 7420/10000, Prediction Accuracy = 62.824%, Loss = 0.38560558557510377
Epoch: 7420, Batch Gradient Norm: 10.265952545002273
Epoch: 7420, Batch Gradient Norm after: 10.265952545002273
Epoch 7421/10000, Prediction Accuracy = 62.786%, Loss = 0.39324827790260314
Epoch: 7421, Batch Gradient Norm: 12.118327102208786
Epoch: 7421, Batch Gradient Norm after: 12.118327102208786
Epoch 7422/10000, Prediction Accuracy = 62.666%, Loss = 0.4065726637840271
Epoch: 7422, Batch Gradient Norm: 11.298610091890074
Epoch: 7422, Batch Gradient Norm after: 11.298610091890074
Epoch 7423/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.400225430727005
Epoch: 7423, Batch Gradient Norm: 10.270241587196022
Epoch: 7423, Batch Gradient Norm after: 10.270241587196022
Epoch 7424/10000, Prediction Accuracy = 62.914%, Loss = 0.39228484630584715
Epoch: 7424, Batch Gradient Norm: 9.618344218099368
Epoch: 7424, Batch Gradient Norm after: 9.618344218099368
Epoch 7425/10000, Prediction Accuracy = 62.9%, Loss = 0.38841370344161985
Epoch: 7425, Batch Gradient Norm: 10.090127275268845
Epoch: 7425, Batch Gradient Norm after: 10.090127275268845
Epoch 7426/10000, Prediction Accuracy = 62.760000000000005%, Loss = 0.3917450666427612
Epoch: 7426, Batch Gradient Norm: 9.64174964117198
Epoch: 7426, Batch Gradient Norm after: 9.64174964117198
Epoch 7427/10000, Prediction Accuracy = 62.762%, Loss = 0.38854758739471434
Epoch: 7427, Batch Gradient Norm: 10.744321562090839
Epoch: 7427, Batch Gradient Norm after: 10.744321562090839
Epoch 7428/10000, Prediction Accuracy = 62.71%, Loss = 0.39804214239120483
Epoch: 7428, Batch Gradient Norm: 10.438700063667138
Epoch: 7428, Batch Gradient Norm after: 10.438700063667138
Epoch 7429/10000, Prediction Accuracy = 62.83%, Loss = 0.3942006230354309
Epoch: 7429, Batch Gradient Norm: 10.94679712102819
Epoch: 7429, Batch Gradient Norm after: 10.94679712102819
Epoch 7430/10000, Prediction Accuracy = 62.666%, Loss = 0.3969698131084442
Epoch: 7430, Batch Gradient Norm: 10.336990902085358
Epoch: 7430, Batch Gradient Norm after: 10.336990902085358
Epoch 7431/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3929844319820404
Epoch: 7431, Batch Gradient Norm: 8.737401255452593
Epoch: 7431, Batch Gradient Norm after: 8.737401255452593
Epoch 7432/10000, Prediction Accuracy = 62.85%, Loss = 0.3834894299507141
Epoch: 7432, Batch Gradient Norm: 8.671681087216827
Epoch: 7432, Batch Gradient Norm after: 8.671681087216827
Epoch 7433/10000, Prediction Accuracy = 62.74399999999999%, Loss = 0.38384053111076355
Epoch: 7433, Batch Gradient Norm: 8.933026858644789
Epoch: 7433, Batch Gradient Norm after: 8.933026858644789
Epoch 7434/10000, Prediction Accuracy = 62.786%, Loss = 0.3856268346309662
Epoch: 7434, Batch Gradient Norm: 9.96210748296841
Epoch: 7434, Batch Gradient Norm after: 9.96210748296841
Epoch 7435/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.391601026058197
Epoch: 7435, Batch Gradient Norm: 11.455679931899382
Epoch: 7435, Batch Gradient Norm after: 11.455679931899382
Epoch 7436/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4020447552204132
Epoch: 7436, Batch Gradient Norm: 12.408129088570838
Epoch: 7436, Batch Gradient Norm after: 12.408129088570838
Epoch 7437/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.4088249921798706
Epoch: 7437, Batch Gradient Norm: 11.460161833065325
Epoch: 7437, Batch Gradient Norm after: 11.460161833065325
Epoch 7438/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.40216472148895266
Epoch: 7438, Batch Gradient Norm: 9.914663182796081
Epoch: 7438, Batch Gradient Norm after: 9.914663182796081
Epoch 7439/10000, Prediction Accuracy = 62.754%, Loss = 0.391618025302887
Epoch: 7439, Batch Gradient Norm: 8.564021626500507
Epoch: 7439, Batch Gradient Norm after: 8.564021626500507
Epoch 7440/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.38317306637763976
Epoch: 7440, Batch Gradient Norm: 8.46441628836593
Epoch: 7440, Batch Gradient Norm after: 8.46441628836593
Epoch 7441/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.3821119129657745
Epoch: 7441, Batch Gradient Norm: 9.395742331503792
Epoch: 7441, Batch Gradient Norm after: 9.395742331503792
Epoch 7442/10000, Prediction Accuracy = 62.89%, Loss = 0.38862574100494385
Epoch: 7442, Batch Gradient Norm: 9.427539708874017
Epoch: 7442, Batch Gradient Norm after: 9.427539708874017
Epoch 7443/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.38991403579711914
Epoch: 7443, Batch Gradient Norm: 8.33788099978464
Epoch: 7443, Batch Gradient Norm after: 8.33788099978464
Epoch 7444/10000, Prediction Accuracy = 62.928%, Loss = 0.3829828560352325
Epoch: 7444, Batch Gradient Norm: 8.96717595816121
Epoch: 7444, Batch Gradient Norm after: 8.96717595816121
Epoch 7445/10000, Prediction Accuracy = 62.71600000000001%, Loss = 0.38484469056129456
Epoch: 7445, Batch Gradient Norm: 12.400058277617747
Epoch: 7445, Batch Gradient Norm after: 12.400058277617747
Epoch 7446/10000, Prediction Accuracy = 62.718%, Loss = 0.40795848369598386
Epoch: 7446, Batch Gradient Norm: 13.223819226831353
Epoch: 7446, Batch Gradient Norm after: 13.223819226831353
Epoch 7447/10000, Prediction Accuracy = 62.786%, Loss = 0.41581360101699827
Epoch: 7447, Batch Gradient Norm: 9.863064048195694
Epoch: 7447, Batch Gradient Norm after: 9.863064048195694
Epoch 7448/10000, Prediction Accuracy = 62.766%, Loss = 0.3900110125541687
Epoch: 7448, Batch Gradient Norm: 9.341754064267993
Epoch: 7448, Batch Gradient Norm after: 9.341754064267993
Epoch 7449/10000, Prediction Accuracy = 62.757999999999996%, Loss = 0.38564852476119993
Epoch: 7449, Batch Gradient Norm: 10.498087809063396
Epoch: 7449, Batch Gradient Norm after: 10.498087809063396
Epoch 7450/10000, Prediction Accuracy = 62.718%, Loss = 0.3941586911678314
Epoch: 7450, Batch Gradient Norm: 11.035149749736801
Epoch: 7450, Batch Gradient Norm after: 11.035149749736801
Epoch 7451/10000, Prediction Accuracy = 62.738%, Loss = 0.3991737365722656
Epoch: 7451, Batch Gradient Norm: 10.054347863769335
Epoch: 7451, Batch Gradient Norm after: 10.054347863769335
Epoch 7452/10000, Prediction Accuracy = 62.878%, Loss = 0.3908422887325287
Epoch: 7452, Batch Gradient Norm: 9.00953618901837
Epoch: 7452, Batch Gradient Norm after: 9.00953618901837
Epoch 7453/10000, Prediction Accuracy = 62.888%, Loss = 0.3843480825424194
Epoch: 7453, Batch Gradient Norm: 9.158988776013263
Epoch: 7453, Batch Gradient Norm after: 9.158988776013263
Epoch 7454/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.3857671797275543
Epoch: 7454, Batch Gradient Norm: 9.207137433250082
Epoch: 7454, Batch Gradient Norm after: 9.207137433250082
Epoch 7455/10000, Prediction Accuracy = 62.896%, Loss = 0.3858495354652405
Epoch: 7455, Batch Gradient Norm: 10.107175187905959
Epoch: 7455, Batch Gradient Norm after: 10.107175187905959
Epoch 7456/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.3908515989780426
Epoch: 7456, Batch Gradient Norm: 11.143344612449324
Epoch: 7456, Batch Gradient Norm after: 11.143344612449324
Epoch 7457/10000, Prediction Accuracy = 62.822%, Loss = 0.397829270362854
Epoch: 7457, Batch Gradient Norm: 10.649830083096042
Epoch: 7457, Batch Gradient Norm after: 10.649830083096042
Epoch 7458/10000, Prediction Accuracy = 62.766%, Loss = 0.3964690327644348
Epoch: 7458, Batch Gradient Norm: 9.398492810274703
Epoch: 7458, Batch Gradient Norm after: 9.398492810274703
Epoch 7459/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.38930525779724123
Epoch: 7459, Batch Gradient Norm: 9.74571341195745
Epoch: 7459, Batch Gradient Norm after: 9.74571341195745
Epoch 7460/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3908499300479889
Epoch: 7460, Batch Gradient Norm: 9.895746073248569
Epoch: 7460, Batch Gradient Norm after: 9.895746073248569
Epoch 7461/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.3914041519165039
Epoch: 7461, Batch Gradient Norm: 9.01224850961221
Epoch: 7461, Batch Gradient Norm after: 9.01224850961221
Epoch 7462/10000, Prediction Accuracy = 62.86%, Loss = 0.38490246534347533
Epoch: 7462, Batch Gradient Norm: 11.124807596721089
Epoch: 7462, Batch Gradient Norm after: 11.124807596721089
Epoch 7463/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.39866485595703127
Epoch: 7463, Batch Gradient Norm: 11.476630139948723
Epoch: 7463, Batch Gradient Norm after: 11.476630139948723
Epoch 7464/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.4021438121795654
Epoch: 7464, Batch Gradient Norm: 9.7918689524356
Epoch: 7464, Batch Gradient Norm after: 9.7918689524356
Epoch 7465/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.39106777906417844
Epoch: 7465, Batch Gradient Norm: 10.504490068480605
Epoch: 7465, Batch Gradient Norm after: 10.504490068480605
Epoch 7466/10000, Prediction Accuracy = 62.586%, Loss = 0.39401410818099974
Epoch: 7466, Batch Gradient Norm: 11.570378513456987
Epoch: 7466, Batch Gradient Norm after: 11.570378513456987
Epoch 7467/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.4020013153553009
Epoch: 7467, Batch Gradient Norm: 9.753852741330505
Epoch: 7467, Batch Gradient Norm after: 9.753852741330505
Epoch 7468/10000, Prediction Accuracy = 62.902%, Loss = 0.3894588351249695
Epoch: 7468, Batch Gradient Norm: 8.091197301125558
Epoch: 7468, Batch Gradient Norm after: 8.091197301125558
Epoch 7469/10000, Prediction Accuracy = 62.834%, Loss = 0.379004442691803
Epoch: 7469, Batch Gradient Norm: 9.16176803702731
Epoch: 7469, Batch Gradient Norm after: 9.16176803702731
Epoch 7470/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.38518892526626586
Epoch: 7470, Batch Gradient Norm: 10.382514942818181
Epoch: 7470, Batch Gradient Norm after: 10.382514942818181
Epoch 7471/10000, Prediction Accuracy = 62.824%, Loss = 0.39387070536613467
Epoch: 7471, Batch Gradient Norm: 10.691335504927022
Epoch: 7471, Batch Gradient Norm after: 10.691335504927022
Epoch 7472/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.39677483439445493
Epoch: 7472, Batch Gradient Norm: 9.14123475328438
Epoch: 7472, Batch Gradient Norm after: 9.14123475328438
Epoch 7473/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3866547286510468
Epoch: 7473, Batch Gradient Norm: 8.677710702715242
Epoch: 7473, Batch Gradient Norm after: 8.677710702715242
Epoch 7474/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.38254597783088684
Epoch: 7474, Batch Gradient Norm: 10.618012623944445
Epoch: 7474, Batch Gradient Norm after: 10.618012623944445
Epoch 7475/10000, Prediction Accuracy = 62.818%, Loss = 0.3948971152305603
Epoch: 7475, Batch Gradient Norm: 11.321967903075615
Epoch: 7475, Batch Gradient Norm after: 11.321967903075615
Epoch 7476/10000, Prediction Accuracy = 62.688%, Loss = 0.4002001345157623
Epoch: 7476, Batch Gradient Norm: 10.59635932687988
Epoch: 7476, Batch Gradient Norm after: 10.59635932687988
Epoch 7477/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.3950358808040619
Epoch: 7477, Batch Gradient Norm: 11.58638081505054
Epoch: 7477, Batch Gradient Norm after: 11.58638081505054
Epoch 7478/10000, Prediction Accuracy = 62.79599999999999%, Loss = 0.40140154361724856
Epoch: 7478, Batch Gradient Norm: 11.791529865883637
Epoch: 7478, Batch Gradient Norm after: 11.791529865883637
Epoch 7479/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.402671754360199
Epoch: 7479, Batch Gradient Norm: 9.864420046892908
Epoch: 7479, Batch Gradient Norm after: 9.864420046892908
Epoch 7480/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.3897331476211548
Epoch: 7480, Batch Gradient Norm: 8.795586819248987
Epoch: 7480, Batch Gradient Norm after: 8.795586819248987
Epoch 7481/10000, Prediction Accuracy = 62.842%, Loss = 0.3824147880077362
Epoch: 7481, Batch Gradient Norm: 11.216831573053291
Epoch: 7481, Batch Gradient Norm after: 11.216831573053291
Epoch 7482/10000, Prediction Accuracy = 62.862%, Loss = 0.399135547876358
Epoch: 7482, Batch Gradient Norm: 11.453773633236008
Epoch: 7482, Batch Gradient Norm after: 11.453773633236008
Epoch 7483/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.40190773606300356
Epoch: 7483, Batch Gradient Norm: 9.09985343967589
Epoch: 7483, Batch Gradient Norm after: 9.09985343967589
Epoch 7484/10000, Prediction Accuracy = 62.896%, Loss = 0.38502911329269407
Epoch: 7484, Batch Gradient Norm: 8.882869715566287
Epoch: 7484, Batch Gradient Norm after: 8.882869715566287
Epoch 7485/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3832654595375061
Epoch: 7485, Batch Gradient Norm: 9.747037097193182
Epoch: 7485, Batch Gradient Norm after: 9.747037097193182
Epoch 7486/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.38940282464027404
Epoch: 7486, Batch Gradient Norm: 9.417465199158164
Epoch: 7486, Batch Gradient Norm after: 9.417465199158164
Epoch 7487/10000, Prediction Accuracy = 62.91199999999999%, Loss = 0.3881241142749786
Epoch: 7487, Batch Gradient Norm: 9.68370313535801
Epoch: 7487, Batch Gradient Norm after: 9.68370313535801
Epoch 7488/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3890934348106384
Epoch: 7488, Batch Gradient Norm: 10.204493235668258
Epoch: 7488, Batch Gradient Norm after: 10.204493235668258
Epoch 7489/10000, Prediction Accuracy = 62.63000000000001%, Loss = 0.39224759936332704
Epoch: 7489, Batch Gradient Norm: 10.25669096680494
Epoch: 7489, Batch Gradient Norm after: 10.25669096680494
Epoch 7490/10000, Prediction Accuracy = 62.842%, Loss = 0.3916793763637543
Epoch: 7490, Batch Gradient Norm: 10.557252307963447
Epoch: 7490, Batch Gradient Norm after: 10.557252307963447
Epoch 7491/10000, Prediction Accuracy = 62.666%, Loss = 0.3942831218242645
Epoch: 7491, Batch Gradient Norm: 9.652154753761975
Epoch: 7491, Batch Gradient Norm after: 9.652154753761975
Epoch 7492/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.38907344937324523
Epoch: 7492, Batch Gradient Norm: 9.451188223086929
Epoch: 7492, Batch Gradient Norm after: 9.451188223086929
Epoch 7493/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.3871514916419983
Epoch: 7493, Batch Gradient Norm: 9.600756526942492
Epoch: 7493, Batch Gradient Norm after: 9.600756526942492
Epoch 7494/10000, Prediction Accuracy = 62.763999999999996%, Loss = 0.3875668942928314
Epoch: 7494, Batch Gradient Norm: 10.672734711084733
Epoch: 7494, Batch Gradient Norm after: 10.672734711084733
Epoch 7495/10000, Prediction Accuracy = 62.746%, Loss = 0.3952408194541931
Epoch: 7495, Batch Gradient Norm: 10.67890065590445
Epoch: 7495, Batch Gradient Norm after: 10.67890065590445
Epoch 7496/10000, Prediction Accuracy = 62.854%, Loss = 0.3952440142631531
Epoch: 7496, Batch Gradient Norm: 10.347938967776757
Epoch: 7496, Batch Gradient Norm after: 10.347938967776757
Epoch 7497/10000, Prediction Accuracy = 62.75999999999999%, Loss = 0.3920476198196411
Epoch: 7497, Batch Gradient Norm: 10.877796056406165
Epoch: 7497, Batch Gradient Norm after: 10.877796056406165
Epoch 7498/10000, Prediction Accuracy = 62.854000000000006%, Loss = 0.3945254564285278
Epoch: 7498, Batch Gradient Norm: 12.743544560380304
Epoch: 7498, Batch Gradient Norm after: 12.743544560380304
Epoch 7499/10000, Prediction Accuracy = 62.56999999999999%, Loss = 0.4093458831310272
Epoch: 7499, Batch Gradient Norm: 11.218935008634018
Epoch: 7499, Batch Gradient Norm after: 11.218935008634018
Epoch 7500/10000, Prediction Accuracy = 62.794%, Loss = 0.39954659938812254
Epoch: 7500, Batch Gradient Norm: 8.972351058895303
Epoch: 7500, Batch Gradient Norm after: 8.972351058895303
Epoch 7501/10000, Prediction Accuracy = 62.762%, Loss = 0.385186630487442
Epoch: 7501, Batch Gradient Norm: 8.099327559971735
Epoch: 7501, Batch Gradient Norm after: 8.099327559971735
Epoch 7502/10000, Prediction Accuracy = 62.766%, Loss = 0.3796955108642578
Epoch: 7502, Batch Gradient Norm: 8.273845243330646
Epoch: 7502, Batch Gradient Norm after: 8.273845243330646
Epoch 7503/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3801142156124115
Epoch: 7503, Batch Gradient Norm: 10.414205474729512
Epoch: 7503, Batch Gradient Norm after: 10.414205474729512
Epoch 7504/10000, Prediction Accuracy = 62.722%, Loss = 0.39085663557052613
Epoch: 7504, Batch Gradient Norm: 12.481922010038835
Epoch: 7504, Batch Gradient Norm after: 12.481922010038835
Epoch 7505/10000, Prediction Accuracy = 62.798%, Loss = 0.4071797490119934
Epoch: 7505, Batch Gradient Norm: 10.246382352636465
Epoch: 7505, Batch Gradient Norm after: 10.246382352636465
Epoch 7506/10000, Prediction Accuracy = 62.826%, Loss = 0.39096460938453675
Epoch: 7506, Batch Gradient Norm: 9.41554348159023
Epoch: 7506, Batch Gradient Norm after: 9.41554348159023
Epoch 7507/10000, Prediction Accuracy = 62.78399999999999%, Loss = 0.38591846227645876
Epoch: 7507, Batch Gradient Norm: 9.852059540010464
Epoch: 7507, Batch Gradient Norm after: 9.852059540010464
Epoch 7508/10000, Prediction Accuracy = 62.83%, Loss = 0.38910905718803407
Epoch: 7508, Batch Gradient Norm: 9.433859486745598
Epoch: 7508, Batch Gradient Norm after: 9.433859486745598
Epoch 7509/10000, Prediction Accuracy = 62.822%, Loss = 0.3864059031009674
Epoch: 7509, Batch Gradient Norm: 9.44494334854769
Epoch: 7509, Batch Gradient Norm after: 9.44494334854769
Epoch 7510/10000, Prediction Accuracy = 62.8%, Loss = 0.38617177605628966
Epoch: 7510, Batch Gradient Norm: 9.277505147632422
Epoch: 7510, Batch Gradient Norm after: 9.277505147632422
Epoch 7511/10000, Prediction Accuracy = 62.886%, Loss = 0.38559611439704894
Epoch: 7511, Batch Gradient Norm: 10.00845676941135
Epoch: 7511, Batch Gradient Norm after: 10.00845676941135
Epoch 7512/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.39102386236190795
Epoch: 7512, Batch Gradient Norm: 9.511221297637396
Epoch: 7512, Batch Gradient Norm after: 9.511221297637396
Epoch 7513/10000, Prediction Accuracy = 62.788%, Loss = 0.3886012673377991
Epoch: 7513, Batch Gradient Norm: 9.931136495999286
Epoch: 7513, Batch Gradient Norm after: 9.931136495999286
Epoch 7514/10000, Prediction Accuracy = 62.85%, Loss = 0.39184215664863586
Epoch: 7514, Batch Gradient Norm: 11.014875337057227
Epoch: 7514, Batch Gradient Norm after: 11.014875337057227
Epoch 7515/10000, Prediction Accuracy = 62.65999999999999%, Loss = 0.39977595806121824
Epoch: 7515, Batch Gradient Norm: 10.357193484867407
Epoch: 7515, Batch Gradient Norm after: 10.357193484867407
Epoch 7516/10000, Prediction Accuracy = 62.706%, Loss = 0.3939259946346283
Epoch: 7516, Batch Gradient Norm: 10.11493510019881
Epoch: 7516, Batch Gradient Norm after: 10.11493510019881
Epoch 7517/10000, Prediction Accuracy = 62.908%, Loss = 0.3915218889713287
Epoch: 7517, Batch Gradient Norm: 10.478217782534834
Epoch: 7517, Batch Gradient Norm after: 10.478217782534834
Epoch 7518/10000, Prediction Accuracy = 62.778%, Loss = 0.39504913687705995
Epoch: 7518, Batch Gradient Norm: 11.854293097806694
Epoch: 7518, Batch Gradient Norm after: 11.854293097806694
Epoch 7519/10000, Prediction Accuracy = 62.738%, Loss = 0.40351014137268065
Epoch: 7519, Batch Gradient Norm: 12.332921060157078
Epoch: 7519, Batch Gradient Norm after: 12.332921060157078
Epoch 7520/10000, Prediction Accuracy = 62.74400000000001%, Loss = 0.40573243498802186
Epoch: 7520, Batch Gradient Norm: 9.99174114048618
Epoch: 7520, Batch Gradient Norm after: 9.99174114048618
Epoch 7521/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.38885162472724916
Epoch: 7521, Batch Gradient Norm: 8.553595973593113
Epoch: 7521, Batch Gradient Norm after: 8.553595973593113
Epoch 7522/10000, Prediction Accuracy = 62.76400000000001%, Loss = 0.3798413693904877
Epoch: 7522, Batch Gradient Norm: 9.131189868718716
Epoch: 7522, Batch Gradient Norm after: 9.131189868718716
Epoch 7523/10000, Prediction Accuracy = 62.86200000000001%, Loss = 0.3824602723121643
Epoch: 7523, Batch Gradient Norm: 11.207179255268258
Epoch: 7523, Batch Gradient Norm after: 11.207179255268258
Epoch 7524/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.3961949050426483
Epoch: 7524, Batch Gradient Norm: 10.518760975536207
Epoch: 7524, Batch Gradient Norm after: 10.518760975536207
Epoch 7525/10000, Prediction Accuracy = 62.82399999999999%, Loss = 0.39265182614326477
Epoch: 7525, Batch Gradient Norm: 9.215555293727416
Epoch: 7525, Batch Gradient Norm after: 9.215555293727416
Epoch 7526/10000, Prediction Accuracy = 62.786%, Loss = 0.3851607024669647
Epoch: 7526, Batch Gradient Norm: 8.30889849126666
Epoch: 7526, Batch Gradient Norm after: 8.30889849126666
Epoch 7527/10000, Prediction Accuracy = 62.83399999999999%, Loss = 0.3800928831100464
Epoch: 7527, Batch Gradient Norm: 8.482922458685035
Epoch: 7527, Batch Gradient Norm after: 8.482922458685035
Epoch 7528/10000, Prediction Accuracy = 62.862%, Loss = 0.3806567370891571
Epoch: 7528, Batch Gradient Norm: 11.442470005042205
Epoch: 7528, Batch Gradient Norm after: 11.442470005042205
Epoch 7529/10000, Prediction Accuracy = 62.68599999999999%, Loss = 0.3989252865314484
Epoch: 7529, Batch Gradient Norm: 11.860455316550535
Epoch: 7529, Batch Gradient Norm after: 11.860455316550535
Epoch 7530/10000, Prediction Accuracy = 62.772000000000006%, Loss = 0.4034899652004242
Epoch: 7530, Batch Gradient Norm: 9.329760584092783
Epoch: 7530, Batch Gradient Norm after: 9.329760584092783
Epoch 7531/10000, Prediction Accuracy = 62.802%, Loss = 0.3852507948875427
Epoch: 7531, Batch Gradient Norm: 8.574674285983239
Epoch: 7531, Batch Gradient Norm after: 8.574674285983239
Epoch 7532/10000, Prediction Accuracy = 62.842%, Loss = 0.38036524057388305
Epoch: 7532, Batch Gradient Norm: 10.459197058064388
Epoch: 7532, Batch Gradient Norm after: 10.459197058064388
Epoch 7533/10000, Prediction Accuracy = 62.784000000000006%, Loss = 0.3934429883956909
Epoch: 7533, Batch Gradient Norm: 10.07695012721461
Epoch: 7533, Batch Gradient Norm after: 10.07695012721461
Epoch 7534/10000, Prediction Accuracy = 62.914%, Loss = 0.3948709011077881
Epoch: 7534, Batch Gradient Norm: 8.721519859120455
Epoch: 7534, Batch Gradient Norm after: 8.721519859120455
Epoch 7535/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.3844631016254425
Epoch: 7535, Batch Gradient Norm: 9.472243756061195
Epoch: 7535, Batch Gradient Norm after: 9.472243756061195
Epoch 7536/10000, Prediction Accuracy = 62.85799999999999%, Loss = 0.3868234038352966
Epoch: 7536, Batch Gradient Norm: 12.032026965364995
Epoch: 7536, Batch Gradient Norm after: 12.032026965364995
Epoch 7537/10000, Prediction Accuracy = 62.678%, Loss = 0.40525064468383787
Epoch: 7537, Batch Gradient Norm: 10.963529868783414
Epoch: 7537, Batch Gradient Norm after: 10.963529868783414
Epoch 7538/10000, Prediction Accuracy = 62.896%, Loss = 0.39756777286529543
Epoch: 7538, Batch Gradient Norm: 10.67018295373595
Epoch: 7538, Batch Gradient Norm after: 10.67018295373595
Epoch 7539/10000, Prediction Accuracy = 62.838%, Loss = 0.3938028156757355
Epoch: 7539, Batch Gradient Norm: 11.331214493153702
Epoch: 7539, Batch Gradient Norm after: 11.331214493153702
Epoch 7540/10000, Prediction Accuracy = 62.84000000000001%, Loss = 0.3992133319377899
Epoch: 7540, Batch Gradient Norm: 9.564214681578273
Epoch: 7540, Batch Gradient Norm after: 9.564214681578273
Epoch 7541/10000, Prediction Accuracy = 62.888%, Loss = 0.38775007128715516
Epoch: 7541, Batch Gradient Norm: 8.209574251629695
Epoch: 7541, Batch Gradient Norm after: 8.209574251629695
Epoch 7542/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.37906978726387025
Epoch: 7542, Batch Gradient Norm: 9.33102142454669
Epoch: 7542, Batch Gradient Norm after: 9.33102142454669
Epoch 7543/10000, Prediction Accuracy = 62.9%, Loss = 0.3852623164653778
Epoch: 7543, Batch Gradient Norm: 11.106788420431256
Epoch: 7543, Batch Gradient Norm after: 11.106788420431256
Epoch 7544/10000, Prediction Accuracy = 62.774%, Loss = 0.3956060230731964
Epoch: 7544, Batch Gradient Norm: 11.029884112671741
Epoch: 7544, Batch Gradient Norm after: 11.029884112671741
Epoch 7545/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.39458186030387876
Epoch: 7545, Batch Gradient Norm: 12.329285825973129
Epoch: 7545, Batch Gradient Norm after: 12.329285825973129
Epoch 7546/10000, Prediction Accuracy = 62.732000000000006%, Loss = 0.4046375036239624
Epoch: 7546, Batch Gradient Norm: 11.191844846519539
Epoch: 7546, Batch Gradient Norm after: 11.191844846519539
Epoch 7547/10000, Prediction Accuracy = 62.778%, Loss = 0.4002604842185974
Epoch: 7547, Batch Gradient Norm: 6.909371127167015
Epoch: 7547, Batch Gradient Norm after: 6.909371127167015
Epoch 7548/10000, Prediction Accuracy = 62.864%, Loss = 0.3729814410209656
Epoch: 7548, Batch Gradient Norm: 6.351487434104537
Epoch: 7548, Batch Gradient Norm after: 6.351487434104537
Epoch 7549/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3700099170207977
Epoch: 7549, Batch Gradient Norm: 8.67343933171735
Epoch: 7549, Batch Gradient Norm after: 8.67343933171735
Epoch 7550/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.38253811597824094
Epoch: 7550, Batch Gradient Norm: 11.000556813132322
Epoch: 7550, Batch Gradient Norm after: 11.000556813132322
Epoch 7551/10000, Prediction Accuracy = 62.812%, Loss = 0.39929522275924684
Epoch: 7551, Batch Gradient Norm: 12.26152833420109
Epoch: 7551, Batch Gradient Norm after: 12.26152833420109
Epoch 7552/10000, Prediction Accuracy = 62.714%, Loss = 0.4072908699512482
Epoch: 7552, Batch Gradient Norm: 10.602161361560096
Epoch: 7552, Batch Gradient Norm after: 10.602161361560096
Epoch 7553/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.3936569571495056
Epoch: 7553, Batch Gradient Norm: 10.310803596759712
Epoch: 7553, Batch Gradient Norm after: 10.310803596759712
Epoch 7554/10000, Prediction Accuracy = 62.824%, Loss = 0.39060362577438357
Epoch: 7554, Batch Gradient Norm: 10.784607848991463
Epoch: 7554, Batch Gradient Norm after: 10.784607848991463
Epoch 7555/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.3938079595565796
Epoch: 7555, Batch Gradient Norm: 9.934170679924174
Epoch: 7555, Batch Gradient Norm after: 9.934170679924174
Epoch 7556/10000, Prediction Accuracy = 62.836%, Loss = 0.3893450856208801
Epoch: 7556, Batch Gradient Norm: 8.064697579435524
Epoch: 7556, Batch Gradient Norm after: 8.064697579435524
Epoch 7557/10000, Prediction Accuracy = 62.791999999999994%, Loss = 0.3773837149143219
Epoch: 7557, Batch Gradient Norm: 7.981648791772295
Epoch: 7557, Batch Gradient Norm after: 7.981648791772295
Epoch 7558/10000, Prediction Accuracy = 62.864%, Loss = 0.3771955192089081
Epoch: 7558, Batch Gradient Norm: 9.019546395600218
Epoch: 7558, Batch Gradient Norm after: 9.019546395600218
Epoch 7559/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.38347407579422
Epoch: 7559, Batch Gradient Norm: 10.100392625554871
Epoch: 7559, Batch Gradient Norm after: 10.100392625554871
Epoch 7560/10000, Prediction Accuracy = 62.722%, Loss = 0.39121081829071047
Epoch: 7560, Batch Gradient Norm: 10.837240577130867
Epoch: 7560, Batch Gradient Norm after: 10.837240577130867
Epoch 7561/10000, Prediction Accuracy = 62.782%, Loss = 0.3953494966030121
Epoch: 7561, Batch Gradient Norm: 10.395416364486668
Epoch: 7561, Batch Gradient Norm after: 10.395416364486668
Epoch 7562/10000, Prediction Accuracy = 62.872%, Loss = 0.3912287950515747
Epoch: 7562, Batch Gradient Norm: 10.407781577773447
Epoch: 7562, Batch Gradient Norm after: 10.407781577773447
Epoch 7563/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.3911813199520111
Epoch: 7563, Batch Gradient Norm: 10.743738652275393
Epoch: 7563, Batch Gradient Norm after: 10.743738652275393
Epoch 7564/10000, Prediction Accuracy = 62.85999999999999%, Loss = 0.3947857260704041
Epoch: 7564, Batch Gradient Norm: 10.734168535583384
Epoch: 7564, Batch Gradient Norm after: 10.734168535583384
Epoch 7565/10000, Prediction Accuracy = 62.782000000000004%, Loss = 0.3955961287021637
Epoch: 7565, Batch Gradient Norm: 10.786209642109617
Epoch: 7565, Batch Gradient Norm after: 10.786209642109617
Epoch 7566/10000, Prediction Accuracy = 62.762%, Loss = 0.39560959935188295
Epoch: 7566, Batch Gradient Norm: 10.921182451042178
Epoch: 7566, Batch Gradient Norm after: 10.921182451042178
Epoch 7567/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3951558947563171
Epoch: 7567, Batch Gradient Norm: 10.78896560686567
Epoch: 7567, Batch Gradient Norm after: 10.78896560686567
Epoch 7568/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.3930631935596466
Epoch: 7568, Batch Gradient Norm: 10.69515791740247
Epoch: 7568, Batch Gradient Norm after: 10.69515791740247
Epoch 7569/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.39282066822052003
Epoch: 7569, Batch Gradient Norm: 10.968681363258847
Epoch: 7569, Batch Gradient Norm after: 10.968681363258847
Epoch 7570/10000, Prediction Accuracy = 62.584%, Loss = 0.3964779794216156
Epoch: 7570, Batch Gradient Norm: 8.975082427168283
Epoch: 7570, Batch Gradient Norm after: 8.975082427168283
Epoch 7571/10000, Prediction Accuracy = 62.822%, Loss = 0.38372364044189455
Epoch: 7571, Batch Gradient Norm: 7.917638387868131
Epoch: 7571, Batch Gradient Norm after: 7.917638387868131
Epoch 7572/10000, Prediction Accuracy = 62.91799999999999%, Loss = 0.37665286660194397
Epoch: 7572, Batch Gradient Norm: 10.481602061856059
Epoch: 7572, Batch Gradient Norm after: 10.481602061856059
Epoch 7573/10000, Prediction Accuracy = 62.812%, Loss = 0.39198881983757017
Epoch: 7573, Batch Gradient Norm: 13.570863542234129
Epoch: 7573, Batch Gradient Norm after: 13.570863542234129
Epoch 7574/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.4179149866104126
Epoch: 7574, Batch Gradient Norm: 11.246567937242446
Epoch: 7574, Batch Gradient Norm after: 11.246567937242446
Epoch 7575/10000, Prediction Accuracy = 62.86%, Loss = 0.39867334365844725
Epoch: 7575, Batch Gradient Norm: 7.794258039834718
Epoch: 7575, Batch Gradient Norm after: 7.794258039834718
Epoch 7576/10000, Prediction Accuracy = 62.948%, Loss = 0.3759432673454285
Epoch: 7576, Batch Gradient Norm: 8.55765811536407
Epoch: 7576, Batch Gradient Norm after: 8.55765811536407
Epoch 7577/10000, Prediction Accuracy = 62.779999999999994%, Loss = 0.38082022666931153
Epoch: 7577, Batch Gradient Norm: 10.323073784803718
Epoch: 7577, Batch Gradient Norm after: 10.323073784803718
Epoch 7578/10000, Prediction Accuracy = 62.878%, Loss = 0.3936133921146393
Epoch: 7578, Batch Gradient Norm: 9.736141848325897
Epoch: 7578, Batch Gradient Norm after: 9.736141848325897
Epoch 7579/10000, Prediction Accuracy = 62.786%, Loss = 0.3891690194606781
Epoch: 7579, Batch Gradient Norm: 8.824369173324811
Epoch: 7579, Batch Gradient Norm after: 8.824369173324811
Epoch 7580/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.38139681816101073
Epoch: 7580, Batch Gradient Norm: 9.504062565091818
Epoch: 7580, Batch Gradient Norm after: 9.504062565091818
Epoch 7581/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.38497782945632936
Epoch: 7581, Batch Gradient Norm: 10.224271771061385
Epoch: 7581, Batch Gradient Norm after: 10.224271771061385
Epoch 7582/10000, Prediction Accuracy = 62.78599999999999%, Loss = 0.38963802456855773
Epoch: 7582, Batch Gradient Norm: 12.167990440072247
Epoch: 7582, Batch Gradient Norm after: 12.167990440072247
Epoch 7583/10000, Prediction Accuracy = 62.598%, Loss = 0.4037794053554535
Epoch: 7583, Batch Gradient Norm: 10.855377902313865
Epoch: 7583, Batch Gradient Norm after: 10.855377902313865
Epoch 7584/10000, Prediction Accuracy = 62.826%, Loss = 0.39516415596008303
Epoch: 7584, Batch Gradient Norm: 9.196458861837604
Epoch: 7584, Batch Gradient Norm after: 9.196458861837604
Epoch 7585/10000, Prediction Accuracy = 62.836%, Loss = 0.3826438128948212
Epoch: 7585, Batch Gradient Norm: 9.803981086158933
Epoch: 7585, Batch Gradient Norm after: 9.803981086158933
Epoch 7586/10000, Prediction Accuracy = 62.896%, Loss = 0.3867308974266052
Epoch: 7586, Batch Gradient Norm: 10.736310121234762
Epoch: 7586, Batch Gradient Norm after: 10.736310121234762
Epoch 7587/10000, Prediction Accuracy = 62.824%, Loss = 0.39320441484451296
Epoch: 7587, Batch Gradient Norm: 10.75980475820201
Epoch: 7587, Batch Gradient Norm after: 10.75980475820201
Epoch 7588/10000, Prediction Accuracy = 62.884%, Loss = 0.3931557655334473
Epoch: 7588, Batch Gradient Norm: 11.027504354221554
Epoch: 7588, Batch Gradient Norm after: 11.027504354221554
Epoch 7589/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.3958220064640045
Epoch: 7589, Batch Gradient Norm: 10.047116546192573
Epoch: 7589, Batch Gradient Norm after: 10.047116546192573
Epoch 7590/10000, Prediction Accuracy = 62.85999999999999%, Loss = 0.38940757513046265
Epoch: 7590, Batch Gradient Norm: 9.090823935265556
Epoch: 7590, Batch Gradient Norm after: 9.090823935265556
Epoch 7591/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.38236257433891296
Epoch: 7591, Batch Gradient Norm: 10.567029511696113
Epoch: 7591, Batch Gradient Norm after: 10.567029511696113
Epoch 7592/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.39073541164398196
Epoch: 7592, Batch Gradient Norm: 11.522693347365097
Epoch: 7592, Batch Gradient Norm after: 11.522693347365097
Epoch 7593/10000, Prediction Accuracy = 62.588%, Loss = 0.3975775897502899
Epoch: 7593, Batch Gradient Norm: 10.631783005164307
Epoch: 7593, Batch Gradient Norm after: 10.631783005164307
Epoch 7594/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.39387932419776917
Epoch: 7594, Batch Gradient Norm: 9.354413103819013
Epoch: 7594, Batch Gradient Norm after: 9.354413103819013
Epoch 7595/10000, Prediction Accuracy = 62.722%, Loss = 0.3853741824626923
Epoch: 7595, Batch Gradient Norm: 9.498736739621805
Epoch: 7595, Batch Gradient Norm after: 9.498736739621805
Epoch 7596/10000, Prediction Accuracy = 62.85799999999999%, Loss = 0.3858668148517609
Epoch: 7596, Batch Gradient Norm: 10.567124311394778
Epoch: 7596, Batch Gradient Norm after: 10.567124311394778
Epoch 7597/10000, Prediction Accuracy = 62.70399999999999%, Loss = 0.39175221920013426
Epoch: 7597, Batch Gradient Norm: 10.862357389213813
Epoch: 7597, Batch Gradient Norm after: 10.862357389213813
Epoch 7598/10000, Prediction Accuracy = 62.826%, Loss = 0.3944954812526703
Epoch: 7598, Batch Gradient Norm: 9.802872435872565
Epoch: 7598, Batch Gradient Norm after: 9.802872435872565
Epoch 7599/10000, Prediction Accuracy = 62.80400000000001%, Loss = 0.3870857894420624
Epoch: 7599, Batch Gradient Norm: 9.334570931953543
Epoch: 7599, Batch Gradient Norm after: 9.334570931953543
Epoch 7600/10000, Prediction Accuracy = 62.89200000000001%, Loss = 0.38406063318252565
Epoch: 7600, Batch Gradient Norm: 9.128799459535818
Epoch: 7600, Batch Gradient Norm after: 9.128799459535818
Epoch 7601/10000, Prediction Accuracy = 62.854%, Loss = 0.38299967646598815
Epoch: 7601, Batch Gradient Norm: 9.008705026364941
Epoch: 7601, Batch Gradient Norm after: 9.008705026364941
Epoch 7602/10000, Prediction Accuracy = 62.84000000000001%, Loss = 0.3823547542095184
Epoch: 7602, Batch Gradient Norm: 10.158017161569637
Epoch: 7602, Batch Gradient Norm after: 10.158017161569637
Epoch 7603/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.38813588619232176
Epoch: 7603, Batch Gradient Norm: 10.716277888332147
Epoch: 7603, Batch Gradient Norm after: 10.716277888332147
Epoch 7604/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.3933974802494049
Epoch: 7604, Batch Gradient Norm: 10.136405343003432
Epoch: 7604, Batch Gradient Norm after: 10.136405343003432
Epoch 7605/10000, Prediction Accuracy = 62.836%, Loss = 0.3913558483123779
Epoch: 7605, Batch Gradient Norm: 9.729192896055983
Epoch: 7605, Batch Gradient Norm after: 9.729192896055983
Epoch 7606/10000, Prediction Accuracy = 62.75%, Loss = 0.38969225287437437
Epoch: 7606, Batch Gradient Norm: 9.700351295395581
Epoch: 7606, Batch Gradient Norm after: 9.700351295395581
Epoch 7607/10000, Prediction Accuracy = 62.914%, Loss = 0.3876625120639801
Epoch: 7607, Batch Gradient Norm: 11.45450305760232
Epoch: 7607, Batch Gradient Norm after: 11.45450305760232
Epoch 7608/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3982287526130676
Epoch: 7608, Batch Gradient Norm: 10.962051804860385
Epoch: 7608, Batch Gradient Norm after: 10.962051804860385
Epoch 7609/10000, Prediction Accuracy = 62.916%, Loss = 0.39513479471206664
Epoch: 7609, Batch Gradient Norm: 9.14680457711863
Epoch: 7609, Batch Gradient Norm after: 9.14680457711863
Epoch 7610/10000, Prediction Accuracy = 62.858000000000004%, Loss = 0.38238813281059264
Epoch: 7610, Batch Gradient Norm: 10.18853019499245
Epoch: 7610, Batch Gradient Norm after: 10.18853019499245
Epoch 7611/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.38816207051277163
Epoch: 7611, Batch Gradient Norm: 10.948781339849367
Epoch: 7611, Batch Gradient Norm after: 10.948781339849367
Epoch 7612/10000, Prediction Accuracy = 62.843999999999994%, Loss = 0.3926530122756958
Epoch: 7612, Batch Gradient Norm: 11.349401563087419
Epoch: 7612, Batch Gradient Norm after: 11.349401563087419
Epoch 7613/10000, Prediction Accuracy = 62.85%, Loss = 0.39659627079963683
Epoch: 7613, Batch Gradient Norm: 11.24040625412607
Epoch: 7613, Batch Gradient Norm after: 11.24040625412607
Epoch 7614/10000, Prediction Accuracy = 62.96%, Loss = 0.3981819570064545
Epoch: 7614, Batch Gradient Norm: 9.139711316267018
Epoch: 7614, Batch Gradient Norm after: 9.139711316267018
Epoch 7615/10000, Prediction Accuracy = 62.818000000000005%, Loss = 0.3847171425819397
Epoch: 7615, Batch Gradient Norm: 8.617596617324455
Epoch: 7615, Batch Gradient Norm after: 8.617596617324455
Epoch 7616/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.3802961766719818
Epoch: 7616, Batch Gradient Norm: 9.545645611452361
Epoch: 7616, Batch Gradient Norm after: 9.545645611452361
Epoch 7617/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3846769392490387
Epoch: 7617, Batch Gradient Norm: 10.672690323556894
Epoch: 7617, Batch Gradient Norm after: 10.672690323556894
Epoch 7618/10000, Prediction Accuracy = 62.774%, Loss = 0.39218454360961913
Epoch: 7618, Batch Gradient Norm: 11.217747936099926
Epoch: 7618, Batch Gradient Norm after: 11.217747936099926
Epoch 7619/10000, Prediction Accuracy = 62.674%, Loss = 0.39762511253356936
Epoch: 7619, Batch Gradient Norm: 11.567964252691397
Epoch: 7619, Batch Gradient Norm after: 11.567964252691397
Epoch 7620/10000, Prediction Accuracy = 62.79%, Loss = 0.4014125883579254
Epoch: 7620, Batch Gradient Norm: 10.574751263618879
Epoch: 7620, Batch Gradient Norm after: 10.574751263618879
Epoch 7621/10000, Prediction Accuracy = 62.694%, Loss = 0.3930182933807373
Epoch: 7621, Batch Gradient Norm: 9.436027135949269
Epoch: 7621, Batch Gradient Norm after: 9.436027135949269
Epoch 7622/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.38401793837547304
Epoch: 7622, Batch Gradient Norm: 9.237034754160511
Epoch: 7622, Batch Gradient Norm after: 9.237034754160511
Epoch 7623/10000, Prediction Accuracy = 62.80800000000001%, Loss = 0.38251752853393556
Epoch: 7623, Batch Gradient Norm: 10.040646050130777
Epoch: 7623, Batch Gradient Norm after: 10.040646050130777
Epoch 7624/10000, Prediction Accuracy = 62.882000000000005%, Loss = 0.38735944628715513
Epoch: 7624, Batch Gradient Norm: 10.1531307782017
Epoch: 7624, Batch Gradient Norm after: 10.1531307782017
Epoch 7625/10000, Prediction Accuracy = 62.95%, Loss = 0.3888238787651062
Epoch: 7625, Batch Gradient Norm: 9.889351858687036
Epoch: 7625, Batch Gradient Norm after: 9.889351858687036
Epoch 7626/10000, Prediction Accuracy = 62.855999999999995%, Loss = 0.387360554933548
Epoch: 7626, Batch Gradient Norm: 10.093708944587064
Epoch: 7626, Batch Gradient Norm after: 10.093708944587064
Epoch 7627/10000, Prediction Accuracy = 63.032%, Loss = 0.3892978072166443
Epoch: 7627, Batch Gradient Norm: 11.203747225387358
Epoch: 7627, Batch Gradient Norm after: 11.203747225387358
Epoch 7628/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.39846975803375245
Epoch: 7628, Batch Gradient Norm: 9.64404882939767
Epoch: 7628, Batch Gradient Norm after: 9.64404882939767
Epoch 7629/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.38767183423042295
Epoch: 7629, Batch Gradient Norm: 8.256062315177205
Epoch: 7629, Batch Gradient Norm after: 8.256062315177205
Epoch 7630/10000, Prediction Accuracy = 62.914%, Loss = 0.3786592960357666
Epoch: 7630, Batch Gradient Norm: 9.669555919340949
Epoch: 7630, Batch Gradient Norm after: 9.669555919340949
Epoch 7631/10000, Prediction Accuracy = 62.565999999999995%, Loss = 0.3880201756954193
Epoch: 7631, Batch Gradient Norm: 10.519938312887321
Epoch: 7631, Batch Gradient Norm after: 10.519938312887321
Epoch 7632/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3935156226158142
Epoch: 7632, Batch Gradient Norm: 10.924476462382161
Epoch: 7632, Batch Gradient Norm after: 10.924476462382161
Epoch 7633/10000, Prediction Accuracy = 62.834%, Loss = 0.3941451370716095
Epoch: 7633, Batch Gradient Norm: 10.575270705911603
Epoch: 7633, Batch Gradient Norm after: 10.575270705911603
Epoch 7634/10000, Prediction Accuracy = 62.872%, Loss = 0.39047578573226926
Epoch: 7634, Batch Gradient Norm: 10.475569903319109
Epoch: 7634, Batch Gradient Norm after: 10.475569903319109
Epoch 7635/10000, Prediction Accuracy = 62.912%, Loss = 0.3897326588630676
Epoch: 7635, Batch Gradient Norm: 9.80913942437907
Epoch: 7635, Batch Gradient Norm after: 9.80913942437907
Epoch 7636/10000, Prediction Accuracy = 62.814%, Loss = 0.38616558313369753
Epoch: 7636, Batch Gradient Norm: 9.010979125066113
Epoch: 7636, Batch Gradient Norm after: 9.010979125066113
Epoch 7637/10000, Prediction Accuracy = 62.848%, Loss = 0.381862872838974
Epoch: 7637, Batch Gradient Norm: 9.837223814407983
Epoch: 7637, Batch Gradient Norm after: 9.837223814407983
Epoch 7638/10000, Prediction Accuracy = 62.8%, Loss = 0.38614011406898496
Epoch: 7638, Batch Gradient Norm: 11.841972653444287
Epoch: 7638, Batch Gradient Norm after: 11.841972653444287
Epoch 7639/10000, Prediction Accuracy = 62.78000000000001%, Loss = 0.4039806365966797
Epoch: 7639, Batch Gradient Norm: 9.387980952572464
Epoch: 7639, Batch Gradient Norm after: 9.387980952572464
Epoch 7640/10000, Prediction Accuracy = 62.842%, Loss = 0.38660928010940554
Epoch: 7640, Batch Gradient Norm: 7.750896932683006
Epoch: 7640, Batch Gradient Norm after: 7.750896932683006
Epoch 7641/10000, Prediction Accuracy = 62.888%, Loss = 0.37466445565223694
Epoch: 7641, Batch Gradient Norm: 10.00866633488848
Epoch: 7641, Batch Gradient Norm after: 10.00866633488848
Epoch 7642/10000, Prediction Accuracy = 62.83399999999999%, Loss = 0.38587504625320435
Epoch: 7642, Batch Gradient Norm: 13.635726079220676
Epoch: 7642, Batch Gradient Norm after: 13.635726079220676
Epoch 7643/10000, Prediction Accuracy = 62.71400000000001%, Loss = 0.41409740447998045
Epoch: 7643, Batch Gradient Norm: 11.179901576427142
Epoch: 7643, Batch Gradient Norm after: 11.179901576427142
Epoch 7644/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.3963170349597931
Epoch: 7644, Batch Gradient Norm: 8.324017588557433
Epoch: 7644, Batch Gradient Norm after: 8.324017588557433
Epoch 7645/10000, Prediction Accuracy = 62.826%, Loss = 0.3782749354839325
Epoch: 7645, Batch Gradient Norm: 9.056981348294654
Epoch: 7645, Batch Gradient Norm after: 9.056981348294654
Epoch 7646/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.38386386036872866
Epoch: 7646, Batch Gradient Norm: 9.300793331374429
Epoch: 7646, Batch Gradient Norm after: 9.300793331374429
Epoch 7647/10000, Prediction Accuracy = 62.864%, Loss = 0.38490080237388613
Epoch: 7647, Batch Gradient Norm: 9.829201351329601
Epoch: 7647, Batch Gradient Norm after: 9.829201351329601
Epoch 7648/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3874007105827332
Epoch: 7648, Batch Gradient Norm: 10.47194110277634
Epoch: 7648, Batch Gradient Norm after: 10.47194110277634
Epoch 7649/10000, Prediction Accuracy = 62.906000000000006%, Loss = 0.3905077278614044
Epoch: 7649, Batch Gradient Norm: 11.1575370236053
Epoch: 7649, Batch Gradient Norm after: 11.1575370236053
Epoch 7650/10000, Prediction Accuracy = 62.96%, Loss = 0.39568156003952026
Epoch: 7650, Batch Gradient Norm: 10.063127900414711
Epoch: 7650, Batch Gradient Norm after: 10.063127900414711
Epoch 7651/10000, Prediction Accuracy = 62.928%, Loss = 0.3878821849822998
Epoch: 7651, Batch Gradient Norm: 9.672191498148795
Epoch: 7651, Batch Gradient Norm after: 9.672191498148795
Epoch 7652/10000, Prediction Accuracy = 62.812%, Loss = 0.38465629816055297
Epoch: 7652, Batch Gradient Norm: 10.276217876505047
Epoch: 7652, Batch Gradient Norm after: 10.276217876505047
Epoch 7653/10000, Prediction Accuracy = 62.862%, Loss = 0.3890630006790161
Epoch: 7653, Batch Gradient Norm: 10.231288446838583
Epoch: 7653, Batch Gradient Norm after: 10.231288446838583
Epoch 7654/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.38950722217559813
Epoch: 7654, Batch Gradient Norm: 9.188668108206185
Epoch: 7654, Batch Gradient Norm after: 9.188668108206185
Epoch 7655/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3831398248672485
Epoch: 7655, Batch Gradient Norm: 9.82306548755592
Epoch: 7655, Batch Gradient Norm after: 9.82306548755592
Epoch 7656/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.38586186170578
Epoch: 7656, Batch Gradient Norm: 12.358953445117823
Epoch: 7656, Batch Gradient Norm after: 12.358953445117823
Epoch 7657/10000, Prediction Accuracy = 62.629999999999995%, Loss = 0.40408015847206114
Epoch: 7657, Batch Gradient Norm: 10.34376736035953
Epoch: 7657, Batch Gradient Norm after: 10.34376736035953
Epoch 7658/10000, Prediction Accuracy = 62.766000000000005%, Loss = 0.3908924162387848
Epoch: 7658, Batch Gradient Norm: 9.178782866313274
Epoch: 7658, Batch Gradient Norm after: 9.178782866313274
Epoch 7659/10000, Prediction Accuracy = 62.898%, Loss = 0.38185922503471376
Epoch: 7659, Batch Gradient Norm: 11.89500408606304
Epoch: 7659, Batch Gradient Norm after: 11.89500408606304
Epoch 7660/10000, Prediction Accuracy = 62.822%, Loss = 0.40053669810295106
Epoch: 7660, Batch Gradient Norm: 12.838046661992866
Epoch: 7660, Batch Gradient Norm after: 12.838046661992866
Epoch 7661/10000, Prediction Accuracy = 62.775999999999996%, Loss = 0.4081795632839203
Epoch: 7661, Batch Gradient Norm: 10.259762914977
Epoch: 7661, Batch Gradient Norm after: 10.259762914977
Epoch 7662/10000, Prediction Accuracy = 62.812%, Loss = 0.3887811303138733
Epoch: 7662, Batch Gradient Norm: 8.64293832095618
Epoch: 7662, Batch Gradient Norm after: 8.64293832095618
Epoch 7663/10000, Prediction Accuracy = 62.92600000000001%, Loss = 0.3780799627304077
Epoch: 7663, Batch Gradient Norm: 8.573291725996821
Epoch: 7663, Batch Gradient Norm after: 8.573291725996821
Epoch 7664/10000, Prediction Accuracy = 62.84000000000001%, Loss = 0.37811927795410155
Epoch: 7664, Batch Gradient Norm: 9.887546935653972
Epoch: 7664, Batch Gradient Norm after: 9.887546935653972
Epoch 7665/10000, Prediction Accuracy = 62.94599999999999%, Loss = 0.3872823596000671
Epoch: 7665, Batch Gradient Norm: 10.194963364745309
Epoch: 7665, Batch Gradient Norm after: 10.194963364745309
Epoch 7666/10000, Prediction Accuracy = 62.834%, Loss = 0.3897700786590576
Epoch: 7666, Batch Gradient Norm: 10.062405110473593
Epoch: 7666, Batch Gradient Norm after: 10.062405110473593
Epoch 7667/10000, Prediction Accuracy = 62.73199999999999%, Loss = 0.38940964341163636
Epoch: 7667, Batch Gradient Norm: 9.84427339386876
Epoch: 7667, Batch Gradient Norm after: 9.84427339386876
Epoch 7668/10000, Prediction Accuracy = 62.88599999999999%, Loss = 0.3895545661449432
Epoch: 7668, Batch Gradient Norm: 8.027911593498606
Epoch: 7668, Batch Gradient Norm after: 8.027911593498606
Epoch 7669/10000, Prediction Accuracy = 62.838%, Loss = 0.37699705362319946
Epoch: 7669, Batch Gradient Norm: 9.093406399176725
Epoch: 7669, Batch Gradient Norm after: 9.093406399176725
Epoch 7670/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3811264932155609
Epoch: 7670, Batch Gradient Norm: 11.252581467177151
Epoch: 7670, Batch Gradient Norm after: 11.252581467177151
Epoch 7671/10000, Prediction Accuracy = 62.898%, Loss = 0.39618988037109376
Epoch: 7671, Batch Gradient Norm: 9.831123976221614
Epoch: 7671, Batch Gradient Norm after: 9.831123976221614
Epoch 7672/10000, Prediction Accuracy = 62.898%, Loss = 0.38637635111808777
Epoch: 7672, Batch Gradient Norm: 9.990945444591171
Epoch: 7672, Batch Gradient Norm after: 9.990945444591171
Epoch 7673/10000, Prediction Accuracy = 62.9%, Loss = 0.3871105372905731
Epoch: 7673, Batch Gradient Norm: 11.065454075019277
Epoch: 7673, Batch Gradient Norm after: 11.065454075019277
Epoch 7674/10000, Prediction Accuracy = 62.85%, Loss = 0.39514663219451907
Epoch: 7674, Batch Gradient Norm: 10.933996289595532
Epoch: 7674, Batch Gradient Norm after: 10.933996289595532
Epoch 7675/10000, Prediction Accuracy = 62.874%, Loss = 0.39424513578414916
Epoch: 7675, Batch Gradient Norm: 10.039695578860483
Epoch: 7675, Batch Gradient Norm after: 10.039695578860483
Epoch 7676/10000, Prediction Accuracy = 62.866%, Loss = 0.3883009374141693
Epoch: 7676, Batch Gradient Norm: 10.211625415330216
Epoch: 7676, Batch Gradient Norm after: 10.211625415330216
Epoch 7677/10000, Prediction Accuracy = 62.79600000000001%, Loss = 0.38967963457107546
Epoch: 7677, Batch Gradient Norm: 10.248628059363272
Epoch: 7677, Batch Gradient Norm after: 10.248628059363272
Epoch 7678/10000, Prediction Accuracy = 62.852%, Loss = 0.3890946269035339
Epoch: 7678, Batch Gradient Norm: 10.929182865865638
Epoch: 7678, Batch Gradient Norm after: 10.929182865865638
Epoch 7679/10000, Prediction Accuracy = 62.798%, Loss = 0.39373748898506167
Epoch: 7679, Batch Gradient Norm: 10.66574018266526
Epoch: 7679, Batch Gradient Norm after: 10.66574018266526
Epoch 7680/10000, Prediction Accuracy = 62.916%, Loss = 0.3926224887371063
Epoch: 7680, Batch Gradient Norm: 9.707830088438964
Epoch: 7680, Batch Gradient Norm after: 9.707830088438964
Epoch 7681/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.3864946663379669
Epoch: 7681, Batch Gradient Norm: 9.493405535557876
Epoch: 7681, Batch Gradient Norm after: 9.493405535557876
Epoch 7682/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3836092591285706
Epoch: 7682, Batch Gradient Norm: 10.32171432357295
Epoch: 7682, Batch Gradient Norm after: 10.32171432357295
Epoch 7683/10000, Prediction Accuracy = 62.854%, Loss = 0.38864090442657473
Epoch: 7683, Batch Gradient Norm: 10.231245185013137
Epoch: 7683, Batch Gradient Norm after: 10.231245185013137
Epoch 7684/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.38807791471481323
Epoch: 7684, Batch Gradient Norm: 9.524567491438267
Epoch: 7684, Batch Gradient Norm after: 9.524567491438267
Epoch 7685/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.383108389377594
Epoch: 7685, Batch Gradient Norm: 10.193163089758283
Epoch: 7685, Batch Gradient Norm after: 10.193163089758283
Epoch 7686/10000, Prediction Accuracy = 62.99399999999999%, Loss = 0.38824146389961245
Epoch: 7686, Batch Gradient Norm: 10.529667948311872
Epoch: 7686, Batch Gradient Norm after: 10.529667948311872
Epoch 7687/10000, Prediction Accuracy = 62.715999999999994%, Loss = 0.39135944843292236
Epoch: 7687, Batch Gradient Norm: 9.315538217192321
Epoch: 7687, Batch Gradient Norm after: 9.315538217192321
Epoch 7688/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.3832399010658264
Epoch: 7688, Batch Gradient Norm: 9.281111851834131
Epoch: 7688, Batch Gradient Norm after: 9.281111851834131
Epoch 7689/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.38199726939201356
Epoch: 7689, Batch Gradient Norm: 10.705648761147595
Epoch: 7689, Batch Gradient Norm after: 10.705648761147595
Epoch 7690/10000, Prediction Accuracy = 62.81999999999999%, Loss = 0.39028518199920653
Epoch: 7690, Batch Gradient Norm: 12.961868374655811
Epoch: 7690, Batch Gradient Norm after: 12.961868374655811
Epoch 7691/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.4080341815948486
Epoch: 7691, Batch Gradient Norm: 11.608436771288773
Epoch: 7691, Batch Gradient Norm after: 11.608436771288773
Epoch 7692/10000, Prediction Accuracy = 62.666%, Loss = 0.3981528580188751
Epoch: 7692, Batch Gradient Norm: 9.718111343715076
Epoch: 7692, Batch Gradient Norm after: 9.718111343715076
Epoch 7693/10000, Prediction Accuracy = 62.906000000000006%, Loss = 0.38477304577827454
Epoch: 7693, Batch Gradient Norm: 9.679896002183183
Epoch: 7693, Batch Gradient Norm after: 9.679896002183183
Epoch 7694/10000, Prediction Accuracy = 62.962%, Loss = 0.385140997171402
Epoch: 7694, Batch Gradient Norm: 10.090305635553198
Epoch: 7694, Batch Gradient Norm after: 10.090305635553198
Epoch 7695/10000, Prediction Accuracy = 62.91600000000001%, Loss = 0.3869579792022705
Epoch: 7695, Batch Gradient Norm: 10.880226343669346
Epoch: 7695, Batch Gradient Norm after: 10.880226343669346
Epoch 7696/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.3936218559741974
Epoch: 7696, Batch Gradient Norm: 9.828804184338754
Epoch: 7696, Batch Gradient Norm after: 9.828804184338754
Epoch 7697/10000, Prediction Accuracy = 62.976%, Loss = 0.38694489002227783
Epoch: 7697, Batch Gradient Norm: 9.115307743401416
Epoch: 7697, Batch Gradient Norm after: 9.115307743401416
Epoch 7698/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.38109307289123534
Epoch: 7698, Batch Gradient Norm: 9.953790799301634
Epoch: 7698, Batch Gradient Norm after: 9.953790799301634
Epoch 7699/10000, Prediction Accuracy = 62.886%, Loss = 0.38555324673652647
Epoch: 7699, Batch Gradient Norm: 9.82411310392266
Epoch: 7699, Batch Gradient Norm after: 9.82411310392266
Epoch 7700/10000, Prediction Accuracy = 62.938%, Loss = 0.3849182963371277
Epoch: 7700, Batch Gradient Norm: 9.94747882127375
Epoch: 7700, Batch Gradient Norm after: 9.94747882127375
Epoch 7701/10000, Prediction Accuracy = 62.762%, Loss = 0.38581429719924926
Epoch: 7701, Batch Gradient Norm: 9.257283786101388
Epoch: 7701, Batch Gradient Norm after: 9.257283786101388
Epoch 7702/10000, Prediction Accuracy = 62.815999999999995%, Loss = 0.3824451446533203
Epoch: 7702, Batch Gradient Norm: 9.359577558356282
Epoch: 7702, Batch Gradient Norm after: 9.359577558356282
Epoch 7703/10000, Prediction Accuracy = 62.81999999999999%, Loss = 0.3828722178936005
Epoch: 7703, Batch Gradient Norm: 11.198619852680183
Epoch: 7703, Batch Gradient Norm after: 11.198619852680183
Epoch 7704/10000, Prediction Accuracy = 62.83399999999999%, Loss = 0.3979516625404358
Epoch: 7704, Batch Gradient Norm: 11.01843196216261
Epoch: 7704, Batch Gradient Norm after: 11.01843196216261
Epoch 7705/10000, Prediction Accuracy = 62.839999999999996%, Loss = 0.39668341279029845
Epoch: 7705, Batch Gradient Norm: 9.351535518411858
Epoch: 7705, Batch Gradient Norm after: 9.351535518411858
Epoch 7706/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.38411139249801635
Epoch: 7706, Batch Gradient Norm: 8.529351383590226
Epoch: 7706, Batch Gradient Norm after: 8.529351383590226
Epoch 7707/10000, Prediction Accuracy = 62.94%, Loss = 0.37841999530792236
Epoch: 7707, Batch Gradient Norm: 8.807256357343022
Epoch: 7707, Batch Gradient Norm after: 8.807256357343022
Epoch 7708/10000, Prediction Accuracy = 63.062%, Loss = 0.3788678884506226
Epoch: 7708, Batch Gradient Norm: 10.928206209314078
Epoch: 7708, Batch Gradient Norm after: 10.928206209314078
Epoch 7709/10000, Prediction Accuracy = 62.992%, Loss = 0.3918253660202026
Epoch: 7709, Batch Gradient Norm: 11.376983701326417
Epoch: 7709, Batch Gradient Norm after: 11.376983701326417
Epoch 7710/10000, Prediction Accuracy = 62.872%, Loss = 0.39743268489837646
Epoch: 7710, Batch Gradient Norm: 8.714010837485747
Epoch: 7710, Batch Gradient Norm after: 8.714010837485747
Epoch 7711/10000, Prediction Accuracy = 63.0%, Loss = 0.3811692714691162
Epoch: 7711, Batch Gradient Norm: 7.528391138215009
Epoch: 7711, Batch Gradient Norm after: 7.528391138215009
Epoch 7712/10000, Prediction Accuracy = 62.83%, Loss = 0.37387393712997435
Epoch: 7712, Batch Gradient Norm: 10.224263180849308
Epoch: 7712, Batch Gradient Norm after: 10.224263180849308
Epoch 7713/10000, Prediction Accuracy = 62.908%, Loss = 0.38814335465431216
Epoch: 7713, Batch Gradient Norm: 13.584699456019239
Epoch: 7713, Batch Gradient Norm after: 13.584699456019239
Epoch 7714/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.4135396838188171
Epoch: 7714, Batch Gradient Norm: 12.026766793002826
Epoch: 7714, Batch Gradient Norm after: 12.026766793002826
Epoch 7715/10000, Prediction Accuracy = 62.738%, Loss = 0.4002719223499298
Epoch: 7715, Batch Gradient Norm: 9.511322000714394
Epoch: 7715, Batch Gradient Norm after: 9.511322000714394
Epoch 7716/10000, Prediction Accuracy = 62.83599999999999%, Loss = 0.3834183931350708
Epoch: 7716, Batch Gradient Norm: 8.215992547949547
Epoch: 7716, Batch Gradient Norm after: 8.215992547949547
Epoch 7717/10000, Prediction Accuracy = 62.862%, Loss = 0.3764859139919281
Epoch: 7717, Batch Gradient Norm: 9.06450232106534
Epoch: 7717, Batch Gradient Norm after: 9.06450232106534
Epoch 7718/10000, Prediction Accuracy = 62.876%, Loss = 0.3825025498867035
Epoch: 7718, Batch Gradient Norm: 10.204306541170565
Epoch: 7718, Batch Gradient Norm after: 10.204306541170565
Epoch 7719/10000, Prediction Accuracy = 62.874%, Loss = 0.389151269197464
Epoch: 7719, Batch Gradient Norm: 10.940638821824049
Epoch: 7719, Batch Gradient Norm after: 10.940638821824049
Epoch 7720/10000, Prediction Accuracy = 62.774%, Loss = 0.3940137982368469
Epoch: 7720, Batch Gradient Norm: 11.472070469173396
Epoch: 7720, Batch Gradient Norm after: 11.472070469173396
Epoch 7721/10000, Prediction Accuracy = 62.898%, Loss = 0.39621121287345884
Epoch: 7721, Batch Gradient Norm: 12.4891501415416
Epoch: 7721, Batch Gradient Norm after: 12.4891501415416
Epoch 7722/10000, Prediction Accuracy = 62.739999999999995%, Loss = 0.40241274833679197
Epoch: 7722, Batch Gradient Norm: 11.838596196755113
Epoch: 7722, Batch Gradient Norm after: 11.838596196755113
Epoch 7723/10000, Prediction Accuracy = 62.812%, Loss = 0.3984417378902435
Epoch: 7723, Batch Gradient Norm: 8.851696208185633
Epoch: 7723, Batch Gradient Norm after: 8.851696208185633
Epoch 7724/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.37855892181396483
Epoch: 7724, Batch Gradient Norm: 8.752845186786843
Epoch: 7724, Batch Gradient Norm after: 8.752845186786843
Epoch 7725/10000, Prediction Accuracy = 62.94200000000001%, Loss = 0.379395592212677
Epoch: 7725, Batch Gradient Norm: 8.766648450606551
Epoch: 7725, Batch Gradient Norm after: 8.766648450606551
Epoch 7726/10000, Prediction Accuracy = 63.02%, Loss = 0.3809241890907288
Epoch: 7726, Batch Gradient Norm: 8.119865197116328
Epoch: 7726, Batch Gradient Norm after: 8.119865197116328
Epoch 7727/10000, Prediction Accuracy = 62.83200000000001%, Loss = 0.3751465201377869
Epoch: 7727, Batch Gradient Norm: 8.559992489250378
Epoch: 7727, Batch Gradient Norm after: 8.559992489250378
Epoch 7728/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3773286879062653
Epoch: 7728, Batch Gradient Norm: 10.039894868587368
Epoch: 7728, Batch Gradient Norm after: 10.039894868587368
Epoch 7729/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.38732152581214907
Epoch: 7729, Batch Gradient Norm: 10.394935498158393
Epoch: 7729, Batch Gradient Norm after: 10.394935498158393
Epoch 7730/10000, Prediction Accuracy = 62.914%, Loss = 0.3899826884269714
Epoch: 7730, Batch Gradient Norm: 10.91052590084494
Epoch: 7730, Batch Gradient Norm after: 10.91052590084494
Epoch 7731/10000, Prediction Accuracy = 62.83399999999999%, Loss = 0.3927400171756744
Epoch: 7731, Batch Gradient Norm: 10.98033134692514
Epoch: 7731, Batch Gradient Norm after: 10.98033134692514
Epoch 7732/10000, Prediction Accuracy = 62.888%, Loss = 0.39399179220199587
Epoch: 7732, Batch Gradient Norm: 9.647283495845972
Epoch: 7732, Batch Gradient Norm after: 9.647283495845972
Epoch 7733/10000, Prediction Accuracy = 62.876%, Loss = 0.38491250276565553
Epoch: 7733, Batch Gradient Norm: 9.71663598138614
Epoch: 7733, Batch Gradient Norm after: 9.71663598138614
Epoch 7734/10000, Prediction Accuracy = 62.886%, Loss = 0.3855532228946686
Epoch: 7734, Batch Gradient Norm: 9.715004990675498
Epoch: 7734, Batch Gradient Norm after: 9.715004990675498
Epoch 7735/10000, Prediction Accuracy = 62.928%, Loss = 0.38568286299705506
Epoch: 7735, Batch Gradient Norm: 10.726742256116292
Epoch: 7735, Batch Gradient Norm after: 10.726742256116292
Epoch 7736/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.390986031293869
Epoch: 7736, Batch Gradient Norm: 12.570939949995077
Epoch: 7736, Batch Gradient Norm after: 12.570939949995077
Epoch 7737/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.4038627684116364
Epoch: 7737, Batch Gradient Norm: 11.329404940349667
Epoch: 7737, Batch Gradient Norm after: 11.329404940349667
Epoch 7738/10000, Prediction Accuracy = 62.852%, Loss = 0.3967190146446228
Epoch: 7738, Batch Gradient Norm: 9.108333054513043
Epoch: 7738, Batch Gradient Norm after: 9.108333054513043
Epoch 7739/10000, Prediction Accuracy = 62.952%, Loss = 0.3810088574886322
Epoch: 7739, Batch Gradient Norm: 11.154326037047635
Epoch: 7739, Batch Gradient Norm after: 11.154326037047635
Epoch 7740/10000, Prediction Accuracy = 62.85%, Loss = 0.3937957644462585
Epoch: 7740, Batch Gradient Norm: 11.742787419237045
Epoch: 7740, Batch Gradient Norm after: 11.742787419237045
Epoch 7741/10000, Prediction Accuracy = 62.82000000000001%, Loss = 0.3993316888809204
Epoch: 7741, Batch Gradient Norm: 8.60644306634115
Epoch: 7741, Batch Gradient Norm after: 8.60644306634115
Epoch 7742/10000, Prediction Accuracy = 62.907999999999994%, Loss = 0.3781874418258667
Epoch: 7742, Batch Gradient Norm: 7.14772182606062
Epoch: 7742, Batch Gradient Norm after: 7.14772182606062
Epoch 7743/10000, Prediction Accuracy = 62.944%, Loss = 0.37048840522766113
Epoch: 7743, Batch Gradient Norm: 8.69886049504431
Epoch: 7743, Batch Gradient Norm after: 8.69886049504431
Epoch 7744/10000, Prediction Accuracy = 62.662%, Loss = 0.3783582627773285
Epoch: 7744, Batch Gradient Norm: 9.565560021853383
Epoch: 7744, Batch Gradient Norm after: 9.565560021853383
Epoch 7745/10000, Prediction Accuracy = 62.67%, Loss = 0.38389238715171814
Epoch: 7745, Batch Gradient Norm: 10.865897996024541
Epoch: 7745, Batch Gradient Norm after: 10.865897996024541
Epoch 7746/10000, Prediction Accuracy = 62.788%, Loss = 0.3904903531074524
Epoch: 7746, Batch Gradient Norm: 12.533051848647338
Epoch: 7746, Batch Gradient Norm after: 12.533051848647338
Epoch 7747/10000, Prediction Accuracy = 62.924%, Loss = 0.4040062129497528
Epoch: 7747, Batch Gradient Norm: 10.741059930637734
Epoch: 7747, Batch Gradient Norm after: 10.741059930637734
Epoch 7748/10000, Prediction Accuracy = 62.92%, Loss = 0.39125379323959353
Epoch: 7748, Batch Gradient Norm: 8.580538210947
Epoch: 7748, Batch Gradient Norm after: 8.580538210947
Epoch 7749/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3777521729469299
Epoch: 7749, Batch Gradient Norm: 8.611412157350172
Epoch: 7749, Batch Gradient Norm after: 8.611412157350172
Epoch 7750/10000, Prediction Accuracy = 62.888%, Loss = 0.37807953357696533
Epoch: 7750, Batch Gradient Norm: 9.62829110112471
Epoch: 7750, Batch Gradient Norm after: 9.62829110112471
Epoch 7751/10000, Prediction Accuracy = 62.938%, Loss = 0.3833585798740387
Epoch: 7751, Batch Gradient Norm: 10.369538689564907
Epoch: 7751, Batch Gradient Norm after: 10.369538689564907
Epoch 7752/10000, Prediction Accuracy = 62.914%, Loss = 0.38840038180351255
Epoch: 7752, Batch Gradient Norm: 10.82294903932643
Epoch: 7752, Batch Gradient Norm after: 10.82294903932643
Epoch 7753/10000, Prediction Accuracy = 62.948%, Loss = 0.39446271657943727
Epoch: 7753, Batch Gradient Norm: 10.221777469297855
Epoch: 7753, Batch Gradient Norm after: 10.221777469297855
Epoch 7754/10000, Prediction Accuracy = 62.745999999999995%, Loss = 0.3882719397544861
Epoch: 7754, Batch Gradient Norm: 11.99751532230252
Epoch: 7754, Batch Gradient Norm after: 11.99751532230252
Epoch 7755/10000, Prediction Accuracy = 62.788%, Loss = 0.3993236064910889
Epoch: 7755, Batch Gradient Norm: 12.027404768822077
Epoch: 7755, Batch Gradient Norm after: 12.027404768822077
Epoch 7756/10000, Prediction Accuracy = 62.802%, Loss = 0.4005265474319458
Epoch: 7756, Batch Gradient Norm: 10.002018594529604
Epoch: 7756, Batch Gradient Norm after: 10.002018594529604
Epoch 7757/10000, Prediction Accuracy = 62.924%, Loss = 0.38536455035209655
Epoch: 7757, Batch Gradient Norm: 9.115638497046152
Epoch: 7757, Batch Gradient Norm after: 9.115638497046152
Epoch 7758/10000, Prediction Accuracy = 62.924%, Loss = 0.37993977069854734
Epoch: 7758, Batch Gradient Norm: 9.097490892872392
Epoch: 7758, Batch Gradient Norm after: 9.097490892872392
Epoch 7759/10000, Prediction Accuracy = 62.9%, Loss = 0.3795416593551636
Epoch: 7759, Batch Gradient Norm: 9.251363247939974
Epoch: 7759, Batch Gradient Norm after: 9.251363247939974
Epoch 7760/10000, Prediction Accuracy = 62.972%, Loss = 0.3812870800495148
Epoch: 7760, Batch Gradient Norm: 9.00202827121479
Epoch: 7760, Batch Gradient Norm after: 9.00202827121479
Epoch 7761/10000, Prediction Accuracy = 62.867999999999995%, Loss = 0.38008117079734804
Epoch: 7761, Batch Gradient Norm: 8.136352435413562
Epoch: 7761, Batch Gradient Norm after: 8.136352435413562
Epoch 7762/10000, Prediction Accuracy = 63.04%, Loss = 0.374954092502594
Epoch: 7762, Batch Gradient Norm: 8.996169399909302
Epoch: 7762, Batch Gradient Norm after: 8.996169399909302
Epoch 7763/10000, Prediction Accuracy = 62.92800000000001%, Loss = 0.3796024858951569
Epoch: 7763, Batch Gradient Norm: 10.283912407506904
Epoch: 7763, Batch Gradient Norm after: 10.283912407506904
Epoch 7764/10000, Prediction Accuracy = 62.84400000000001%, Loss = 0.3890734076499939
Epoch: 7764, Batch Gradient Norm: 10.533848921552407
Epoch: 7764, Batch Gradient Norm after: 10.533848921552407
Epoch 7765/10000, Prediction Accuracy = 62.912%, Loss = 0.39345152378082277
Epoch: 7765, Batch Gradient Norm: 9.76753335975241
Epoch: 7765, Batch Gradient Norm after: 9.76753335975241
Epoch 7766/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.38536404371261596
Epoch: 7766, Batch Gradient Norm: 11.412736870967647
Epoch: 7766, Batch Gradient Norm after: 11.412736870967647
Epoch 7767/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.3974113821983337
Epoch: 7767, Batch Gradient Norm: 9.805974802548565
Epoch: 7767, Batch Gradient Norm after: 9.805974802548565
Epoch 7768/10000, Prediction Accuracy = 62.96%, Loss = 0.3873783528804779
Epoch: 7768, Batch Gradient Norm: 9.754504191367467
Epoch: 7768, Batch Gradient Norm after: 9.754504191367467
Epoch 7769/10000, Prediction Accuracy = 62.866%, Loss = 0.3839893937110901
Epoch: 7769, Batch Gradient Norm: 10.170205105005428
Epoch: 7769, Batch Gradient Norm after: 10.170205105005428
Epoch 7770/10000, Prediction Accuracy = 62.736000000000004%, Loss = 0.3861942648887634
Epoch: 7770, Batch Gradient Norm: 11.208907812615369
Epoch: 7770, Batch Gradient Norm after: 11.208907812615369
Epoch 7771/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.39316417574882506
Epoch: 7771, Batch Gradient Norm: 10.86834535282751
Epoch: 7771, Batch Gradient Norm after: 10.86834535282751
Epoch 7772/10000, Prediction Accuracy = 62.9%, Loss = 0.3913387477397919
Epoch: 7772, Batch Gradient Norm: 10.20167158279316
Epoch: 7772, Batch Gradient Norm after: 10.20167158279316
Epoch 7773/10000, Prediction Accuracy = 62.86600000000001%, Loss = 0.3868072211742401
Epoch: 7773, Batch Gradient Norm: 10.940700698074489
Epoch: 7773, Batch Gradient Norm after: 10.940700698074489
Epoch 7774/10000, Prediction Accuracy = 62.92800000000001%, Loss = 0.39139183759689333
Epoch: 7774, Batch Gradient Norm: 11.730145790565816
Epoch: 7774, Batch Gradient Norm after: 11.730145790565816
Epoch 7775/10000, Prediction Accuracy = 62.874%, Loss = 0.39704873561859133
Epoch: 7775, Batch Gradient Norm: 10.685496155311787
Epoch: 7775, Batch Gradient Norm after: 10.685496155311787
Epoch 7776/10000, Prediction Accuracy = 62.818%, Loss = 0.38829982876777647
Epoch: 7776, Batch Gradient Norm: 10.471534700351802
Epoch: 7776, Batch Gradient Norm after: 10.471534700351802
Epoch 7777/10000, Prediction Accuracy = 62.848%, Loss = 0.38772071003913877
Epoch: 7777, Batch Gradient Norm: 10.20492841394178
Epoch: 7777, Batch Gradient Norm after: 10.20492841394178
Epoch 7778/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3875146806240082
Epoch: 7778, Batch Gradient Norm: 10.981523476963257
Epoch: 7778, Batch Gradient Norm after: 10.981523476963257
Epoch 7779/10000, Prediction Accuracy = 62.802%, Loss = 0.39276047945022585
Epoch: 7779, Batch Gradient Norm: 11.091515917010968
Epoch: 7779, Batch Gradient Norm after: 11.091515917010968
Epoch 7780/10000, Prediction Accuracy = 62.988%, Loss = 0.39304459691047666
Epoch: 7780, Batch Gradient Norm: 11.152521257349726
Epoch: 7780, Batch Gradient Norm after: 11.152521257349726
Epoch 7781/10000, Prediction Accuracy = 62.98%, Loss = 0.39352233409881593
Epoch: 7781, Batch Gradient Norm: 11.108848050323584
Epoch: 7781, Batch Gradient Norm after: 11.108848050323584
Epoch 7782/10000, Prediction Accuracy = 62.916%, Loss = 0.3932494163513184
Epoch: 7782, Batch Gradient Norm: 9.82048966710723
Epoch: 7782, Batch Gradient Norm after: 9.82048966710723
Epoch 7783/10000, Prediction Accuracy = 62.99399999999999%, Loss = 0.38438219428062437
Epoch: 7783, Batch Gradient Norm: 8.95039144690455
Epoch: 7783, Batch Gradient Norm after: 8.95039144690455
Epoch 7784/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.37923020124435425
Epoch: 7784, Batch Gradient Norm: 8.558060023978193
Epoch: 7784, Batch Gradient Norm after: 8.558060023978193
Epoch 7785/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.377371609210968
Epoch: 7785, Batch Gradient Norm: 8.260771561602526
Epoch: 7785, Batch Gradient Norm after: 8.260771561602526
Epoch 7786/10000, Prediction Accuracy = 63.04200000000001%, Loss = 0.3760879337787628
Epoch: 7786, Batch Gradient Norm: 8.599180167815568
Epoch: 7786, Batch Gradient Norm after: 8.599180167815568
Epoch 7787/10000, Prediction Accuracy = 62.867999999999995%, Loss = 0.37760432362556456
Epoch: 7787, Batch Gradient Norm: 9.861385047371991
Epoch: 7787, Batch Gradient Norm after: 9.861385047371991
Epoch 7788/10000, Prediction Accuracy = 62.896%, Loss = 0.3845292150974274
Epoch: 7788, Batch Gradient Norm: 10.289407423446946
Epoch: 7788, Batch Gradient Norm after: 10.289407423446946
Epoch 7789/10000, Prediction Accuracy = 62.79%, Loss = 0.3870239436626434
Epoch: 7789, Batch Gradient Norm: 9.268429330763587
Epoch: 7789, Batch Gradient Norm after: 9.268429330763587
Epoch 7790/10000, Prediction Accuracy = 62.86%, Loss = 0.38012691736221316
Epoch: 7790, Batch Gradient Norm: 9.144022339463866
Epoch: 7790, Batch Gradient Norm after: 9.144022339463866
Epoch 7791/10000, Prediction Accuracy = 62.867999999999995%, Loss = 0.38111615777015684
Epoch: 7791, Batch Gradient Norm: 9.709307361033245
Epoch: 7791, Batch Gradient Norm after: 9.709307361033245
Epoch 7792/10000, Prediction Accuracy = 62.948%, Loss = 0.3859091639518738
Epoch: 7792, Batch Gradient Norm: 10.638306136701395
Epoch: 7792, Batch Gradient Norm after: 10.638306136701395
Epoch 7793/10000, Prediction Accuracy = 62.826%, Loss = 0.39130495190620423
Epoch: 7793, Batch Gradient Norm: 10.486505666363625
Epoch: 7793, Batch Gradient Norm after: 10.486505666363625
Epoch 7794/10000, Prediction Accuracy = 62.852%, Loss = 0.38824387192726134
Epoch: 7794, Batch Gradient Norm: 10.3599425498204
Epoch: 7794, Batch Gradient Norm after: 10.3599425498204
Epoch 7795/10000, Prediction Accuracy = 62.874%, Loss = 0.38523625731468203
Epoch: 7795, Batch Gradient Norm: 12.189657694924545
Epoch: 7795, Batch Gradient Norm after: 12.189657694924545
Epoch 7796/10000, Prediction Accuracy = 62.884%, Loss = 0.3981760203838348
Epoch: 7796, Batch Gradient Norm: 12.310276052508788
Epoch: 7796, Batch Gradient Norm after: 12.310276052508788
Epoch 7797/10000, Prediction Accuracy = 62.936%, Loss = 0.4001913845539093
Epoch: 7797, Batch Gradient Norm: 10.669046983828816
Epoch: 7797, Batch Gradient Norm after: 10.669046983828816
Epoch 7798/10000, Prediction Accuracy = 62.803999999999995%, Loss = 0.38981435298919676
Epoch: 7798, Batch Gradient Norm: 9.533657189643904
Epoch: 7798, Batch Gradient Norm after: 9.533657189643904
Epoch 7799/10000, Prediction Accuracy = 62.92399999999999%, Loss = 0.3846175968647003
Epoch: 7799, Batch Gradient Norm: 8.378229689668675
Epoch: 7799, Batch Gradient Norm after: 8.378229689668675
Epoch 7800/10000, Prediction Accuracy = 62.794000000000004%, Loss = 0.3777506768703461
Epoch: 7800, Batch Gradient Norm: 8.408032189658401
Epoch: 7800, Batch Gradient Norm after: 8.408032189658401
Epoch 7801/10000, Prediction Accuracy = 62.96%, Loss = 0.3768587648868561
Epoch: 7801, Batch Gradient Norm: 11.308429916875799
Epoch: 7801, Batch Gradient Norm after: 11.308429916875799
Epoch 7802/10000, Prediction Accuracy = 62.822%, Loss = 0.39371155500411986
Epoch: 7802, Batch Gradient Norm: 12.993128600242075
Epoch: 7802, Batch Gradient Norm after: 12.993128600242075
Epoch 7803/10000, Prediction Accuracy = 62.926%, Loss = 0.4069559395313263
Epoch: 7803, Batch Gradient Norm: 10.196378499325771
Epoch: 7803, Batch Gradient Norm after: 10.196378499325771
Epoch 7804/10000, Prediction Accuracy = 62.827999999999996%, Loss = 0.38618708252906797
Epoch: 7804, Batch Gradient Norm: 9.279841918775121
Epoch: 7804, Batch Gradient Norm after: 9.279841918775121
Epoch 7805/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3806613922119141
Epoch: 7805, Batch Gradient Norm: 9.943793567974106
Epoch: 7805, Batch Gradient Norm after: 9.943793567974106
Epoch 7806/10000, Prediction Accuracy = 62.864%, Loss = 0.38670977354049685
Epoch: 7806, Batch Gradient Norm: 9.030715341299784
Epoch: 7806, Batch Gradient Norm after: 9.030715341299784
Epoch 7807/10000, Prediction Accuracy = 62.972%, Loss = 0.38019809126853943
Epoch: 7807, Batch Gradient Norm: 8.944317320859
Epoch: 7807, Batch Gradient Norm after: 8.944317320859
Epoch 7808/10000, Prediction Accuracy = 62.912%, Loss = 0.37870862483978274
Epoch: 7808, Batch Gradient Norm: 9.001005082700761
Epoch: 7808, Batch Gradient Norm after: 9.001005082700761
Epoch 7809/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3788818120956421
Epoch: 7809, Batch Gradient Norm: 8.623605314671028
Epoch: 7809, Batch Gradient Norm after: 8.623605314671028
Epoch 7810/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.3763633191585541
Epoch: 7810, Batch Gradient Norm: 10.206088798679188
Epoch: 7810, Batch Gradient Norm after: 10.206088798679188
Epoch 7811/10000, Prediction Accuracy = 62.948%, Loss = 0.385604590177536
Epoch: 7811, Batch Gradient Norm: 12.112836887764159
Epoch: 7811, Batch Gradient Norm after: 12.112836887764159
Epoch 7812/10000, Prediction Accuracy = 62.724000000000004%, Loss = 0.40067631006240845
Epoch: 7812, Batch Gradient Norm: 9.64429821993673
Epoch: 7812, Batch Gradient Norm after: 9.64429821993673
Epoch 7813/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3834276258945465
Epoch: 7813, Batch Gradient Norm: 8.492714969620318
Epoch: 7813, Batch Gradient Norm after: 8.492714969620318
Epoch 7814/10000, Prediction Accuracy = 62.928%, Loss = 0.37656084895133973
Epoch: 7814, Batch Gradient Norm: 9.281995441967458
Epoch: 7814, Batch Gradient Norm after: 9.281995441967458
Epoch 7815/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.38104034066200254
Epoch: 7815, Batch Gradient Norm: 11.902581145837882
Epoch: 7815, Batch Gradient Norm after: 11.902581145837882
Epoch 7816/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.400640869140625
Epoch: 7816, Batch Gradient Norm: 12.918182206915368
Epoch: 7816, Batch Gradient Norm after: 12.918182206915368
Epoch 7817/10000, Prediction Accuracy = 62.806000000000004%, Loss = 0.4096365988254547
Epoch: 7817, Batch Gradient Norm: 10.155445636150297
Epoch: 7817, Batch Gradient Norm after: 10.155445636150297
Epoch 7818/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.3867196202278137
Epoch: 7818, Batch Gradient Norm: 9.121825290945495
Epoch: 7818, Batch Gradient Norm after: 9.121825290945495
Epoch 7819/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.3783731937408447
Epoch: 7819, Batch Gradient Norm: 10.71038813834398
Epoch: 7819, Batch Gradient Norm after: 10.71038813834398
Epoch 7820/10000, Prediction Accuracy = 62.952%, Loss = 0.38740199208259585
Epoch: 7820, Batch Gradient Norm: 12.37136321501745
Epoch: 7820, Batch Gradient Norm after: 12.37136321501745
Epoch 7821/10000, Prediction Accuracy = 62.88399999999999%, Loss = 0.4011361598968506
Epoch: 7821, Batch Gradient Norm: 11.087915240044529
Epoch: 7821, Batch Gradient Norm after: 11.087915240044529
Epoch 7822/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.39344226717948916
Epoch: 7822, Batch Gradient Norm: 9.305948898143797
Epoch: 7822, Batch Gradient Norm after: 9.305948898143797
Epoch 7823/10000, Prediction Accuracy = 62.976%, Loss = 0.3816944360733032
Epoch: 7823, Batch Gradient Norm: 8.630572489379546
Epoch: 7823, Batch Gradient Norm after: 8.630572489379546
Epoch 7824/10000, Prediction Accuracy = 63.044%, Loss = 0.3765577137470245
Epoch: 7824, Batch Gradient Norm: 9.64110453836475
Epoch: 7824, Batch Gradient Norm after: 9.64110453836475
Epoch 7825/10000, Prediction Accuracy = 62.924%, Loss = 0.38147943615913393
Epoch: 7825, Batch Gradient Norm: 11.036441638648824
Epoch: 7825, Batch Gradient Norm after: 11.036441638648824
Epoch 7826/10000, Prediction Accuracy = 62.95%, Loss = 0.39025980830192564
Epoch: 7826, Batch Gradient Norm: 10.68367103973936
Epoch: 7826, Batch Gradient Norm after: 10.68367103973936
Epoch 7827/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3896081507205963
Epoch: 7827, Batch Gradient Norm: 10.611012570340483
Epoch: 7827, Batch Gradient Norm after: 10.611012570340483
Epoch 7828/10000, Prediction Accuracy = 62.89%, Loss = 0.38959383964538574
Epoch: 7828, Batch Gradient Norm: 10.804345495304705
Epoch: 7828, Batch Gradient Norm after: 10.804345495304705
Epoch 7829/10000, Prediction Accuracy = 62.75%, Loss = 0.39113523364067077
Epoch: 7829, Batch Gradient Norm: 8.984588504815264
Epoch: 7829, Batch Gradient Norm after: 8.984588504815264
Epoch 7830/10000, Prediction Accuracy = 62.972%, Loss = 0.3795062005519867
Epoch: 7830, Batch Gradient Norm: 9.148962379458256
Epoch: 7830, Batch Gradient Norm after: 9.148962379458256
Epoch 7831/10000, Prediction Accuracy = 62.92%, Loss = 0.3792546629905701
Epoch: 7831, Batch Gradient Norm: 11.046631202825532
Epoch: 7831, Batch Gradient Norm after: 11.046631202825532
Epoch 7832/10000, Prediction Accuracy = 62.86%, Loss = 0.39143343567848204
Epoch: 7832, Batch Gradient Norm: 11.816911512948044
Epoch: 7832, Batch Gradient Norm after: 11.816911512948044
Epoch 7833/10000, Prediction Accuracy = 62.938%, Loss = 0.39740642309188845
Epoch: 7833, Batch Gradient Norm: 9.531104953004773
Epoch: 7833, Batch Gradient Norm after: 9.531104953004773
Epoch 7834/10000, Prediction Accuracy = 62.876%, Loss = 0.3806265890598297
Epoch: 7834, Batch Gradient Norm: 8.789356365432402
Epoch: 7834, Batch Gradient Norm after: 8.789356365432402
Epoch 7835/10000, Prediction Accuracy = 63.016%, Loss = 0.3766448199748993
Epoch: 7835, Batch Gradient Norm: 9.074020434888011
Epoch: 7835, Batch Gradient Norm after: 9.074020434888011
Epoch 7836/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.3793594717979431
Epoch: 7836, Batch Gradient Norm: 9.109031766817548
Epoch: 7836, Batch Gradient Norm after: 9.109031766817548
Epoch 7837/10000, Prediction Accuracy = 62.876%, Loss = 0.38090031743049624
Epoch: 7837, Batch Gradient Norm: 8.223817720124034
Epoch: 7837, Batch Gradient Norm after: 8.223817720124034
Epoch 7838/10000, Prediction Accuracy = 62.97600000000001%, Loss = 0.37491092681884763
Epoch: 7838, Batch Gradient Norm: 7.858968502087941
Epoch: 7838, Batch Gradient Norm after: 7.858968502087941
Epoch 7839/10000, Prediction Accuracy = 62.932%, Loss = 0.3711289882659912
Epoch: 7839, Batch Gradient Norm: 9.37200418677402
Epoch: 7839, Batch Gradient Norm after: 9.37200418677402
Epoch 7840/10000, Prediction Accuracy = 62.908%, Loss = 0.3795537889003754
Epoch: 7840, Batch Gradient Norm: 10.7420516557993
Epoch: 7840, Batch Gradient Norm after: 10.7420516557993
Epoch 7841/10000, Prediction Accuracy = 62.896%, Loss = 0.3891254961490631
Epoch: 7841, Batch Gradient Norm: 12.40441793516184
Epoch: 7841, Batch Gradient Norm after: 12.40441793516184
Epoch 7842/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.4009009122848511
Epoch: 7842, Batch Gradient Norm: 12.36163191489653
Epoch: 7842, Batch Gradient Norm after: 12.36163191489653
Epoch 7843/10000, Prediction Accuracy = 62.81600000000001%, Loss = 0.4015077412128448
Epoch: 7843, Batch Gradient Norm: 10.371565483702623
Epoch: 7843, Batch Gradient Norm after: 10.371565483702623
Epoch 7844/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.3870096981525421
Epoch: 7844, Batch Gradient Norm: 9.49432217030787
Epoch: 7844, Batch Gradient Norm after: 9.49432217030787
Epoch 7845/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.38181577920913695
Epoch: 7845, Batch Gradient Norm: 10.228068674468506
Epoch: 7845, Batch Gradient Norm after: 10.228068674468506
Epoch 7846/10000, Prediction Accuracy = 62.924%, Loss = 0.3863678157329559
Epoch: 7846, Batch Gradient Norm: 10.983082663196605
Epoch: 7846, Batch Gradient Norm after: 10.983082663196605
Epoch 7847/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.3901756465435028
Epoch: 7847, Batch Gradient Norm: 10.861139431248017
Epoch: 7847, Batch Gradient Norm after: 10.861139431248017
Epoch 7848/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.389119553565979
Epoch: 7848, Batch Gradient Norm: 9.75405451203851
Epoch: 7848, Batch Gradient Norm after: 9.75405451203851
Epoch 7849/10000, Prediction Accuracy = 62.864%, Loss = 0.38259715437889097
Epoch: 7849, Batch Gradient Norm: 10.441449981750367
Epoch: 7849, Batch Gradient Norm after: 10.441449981750367
Epoch 7850/10000, Prediction Accuracy = 62.938%, Loss = 0.38835789561271666
Epoch: 7850, Batch Gradient Norm: 10.50944893080079
Epoch: 7850, Batch Gradient Norm after: 10.50944893080079
Epoch 7851/10000, Prediction Accuracy = 62.89399999999999%, Loss = 0.38979145884513855
Epoch: 7851, Batch Gradient Norm: 9.123549218576565
Epoch: 7851, Batch Gradient Norm after: 9.123549218576565
Epoch 7852/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.37982370853424074
Epoch: 7852, Batch Gradient Norm: 8.914121809804433
Epoch: 7852, Batch Gradient Norm after: 8.914121809804433
Epoch 7853/10000, Prediction Accuracy = 63.032000000000004%, Loss = 0.3775356411933899
Epoch: 7853, Batch Gradient Norm: 10.468070100663349
Epoch: 7853, Batch Gradient Norm after: 10.468070100663349
Epoch 7854/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3891868948936462
Epoch: 7854, Batch Gradient Norm: 10.84325683185298
Epoch: 7854, Batch Gradient Norm after: 10.84325683185298
Epoch 7855/10000, Prediction Accuracy = 62.89%, Loss = 0.39274111986160276
Epoch: 7855, Batch Gradient Norm: 10.88910103170086
Epoch: 7855, Batch Gradient Norm after: 10.88910103170086
Epoch 7856/10000, Prediction Accuracy = 62.778%, Loss = 0.3908628225326538
Epoch: 7856, Batch Gradient Norm: 11.009319168808412
Epoch: 7856, Batch Gradient Norm after: 11.009319168808412
Epoch 7857/10000, Prediction Accuracy = 62.976%, Loss = 0.3917993068695068
Epoch: 7857, Batch Gradient Norm: 9.058531130519087
Epoch: 7857, Batch Gradient Norm after: 9.058531130519087
Epoch 7858/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3784904420375824
Epoch: 7858, Batch Gradient Norm: 8.379281703161407
Epoch: 7858, Batch Gradient Norm after: 8.379281703161407
Epoch 7859/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.37396200299263
Epoch: 7859, Batch Gradient Norm: 9.658228437759456
Epoch: 7859, Batch Gradient Norm after: 9.658228437759456
Epoch 7860/10000, Prediction Accuracy = 63.036%, Loss = 0.381183385848999
Epoch: 7860, Batch Gradient Norm: 11.317011444414186
Epoch: 7860, Batch Gradient Norm after: 11.317011444414186
Epoch 7861/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3941101014614105
Epoch: 7861, Batch Gradient Norm: 10.970645295705697
Epoch: 7861, Batch Gradient Norm after: 10.970645295705697
Epoch 7862/10000, Prediction Accuracy = 62.94%, Loss = 0.391739422082901
Epoch: 7862, Batch Gradient Norm: 9.587567456349108
Epoch: 7862, Batch Gradient Norm after: 9.587567456349108
Epoch 7863/10000, Prediction Accuracy = 62.92%, Loss = 0.38134371042251586
Epoch: 7863, Batch Gradient Norm: 9.454594510757419
Epoch: 7863, Batch Gradient Norm after: 9.454594510757419
Epoch 7864/10000, Prediction Accuracy = 62.774%, Loss = 0.3800536870956421
Epoch: 7864, Batch Gradient Norm: 9.662212009207417
Epoch: 7864, Batch Gradient Norm after: 9.662212009207417
Epoch 7865/10000, Prediction Accuracy = 62.93399999999999%, Loss = 0.38208316564559935
Epoch: 7865, Batch Gradient Norm: 11.499909916945356
Epoch: 7865, Batch Gradient Norm after: 11.499909916945356
Epoch 7866/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.396554571390152
Epoch: 7866, Batch Gradient Norm: 11.54413602187008
Epoch: 7866, Batch Gradient Norm after: 11.54413602187008
Epoch 7867/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.39785219430923463
Epoch: 7867, Batch Gradient Norm: 10.74785097229185
Epoch: 7867, Batch Gradient Norm after: 10.74785097229185
Epoch 7868/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.38924176096916197
Epoch: 7868, Batch Gradient Norm: 10.510899090836446
Epoch: 7868, Batch Gradient Norm after: 10.510899090836446
Epoch 7869/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.38630794882774355
Epoch: 7869, Batch Gradient Norm: 9.833354575803893
Epoch: 7869, Batch Gradient Norm after: 9.833354575803893
Epoch 7870/10000, Prediction Accuracy = 62.932%, Loss = 0.38205028176307676
Epoch: 7870, Batch Gradient Norm: 8.915081128266586
Epoch: 7870, Batch Gradient Norm after: 8.915081128266586
Epoch 7871/10000, Prediction Accuracy = 62.94%, Loss = 0.3764835834503174
Epoch: 7871, Batch Gradient Norm: 9.667891888695754
Epoch: 7871, Batch Gradient Norm after: 9.667891888695754
Epoch 7872/10000, Prediction Accuracy = 62.812%, Loss = 0.38146300315856935
Epoch: 7872, Batch Gradient Norm: 10.581264137720925
Epoch: 7872, Batch Gradient Norm after: 10.581264137720925
Epoch 7873/10000, Prediction Accuracy = 62.83%, Loss = 0.38847432732582093
Epoch: 7873, Batch Gradient Norm: 10.21800886864241
Epoch: 7873, Batch Gradient Norm after: 10.21800886864241
Epoch 7874/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.3852733731269836
Epoch: 7874, Batch Gradient Norm: 10.434244768542035
Epoch: 7874, Batch Gradient Norm after: 10.434244768542035
Epoch 7875/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.38779234886169434
Epoch: 7875, Batch Gradient Norm: 10.008065600246729
Epoch: 7875, Batch Gradient Norm after: 10.008065600246729
Epoch 7876/10000, Prediction Accuracy = 62.9%, Loss = 0.38626486659049986
Epoch: 7876, Batch Gradient Norm: 9.000817293716764
Epoch: 7876, Batch Gradient Norm after: 9.000817293716764
Epoch 7877/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.3791922450065613
Epoch: 7877, Batch Gradient Norm: 8.969901504817798
Epoch: 7877, Batch Gradient Norm after: 8.969901504817798
Epoch 7878/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.37666539549827577
Epoch: 7878, Batch Gradient Norm: 10.215505563077782
Epoch: 7878, Batch Gradient Norm after: 10.215505563077782
Epoch 7879/10000, Prediction Accuracy = 62.848%, Loss = 0.38426032066345217
Epoch: 7879, Batch Gradient Norm: 11.126327761228294
Epoch: 7879, Batch Gradient Norm after: 11.126327761228294
Epoch 7880/10000, Prediction Accuracy = 62.9%, Loss = 0.39189003109931947
Epoch: 7880, Batch Gradient Norm: 11.41565713152919
Epoch: 7880, Batch Gradient Norm after: 11.41565713152919
Epoch 7881/10000, Prediction Accuracy = 62.89%, Loss = 0.39463611245155333
Epoch: 7881, Batch Gradient Norm: 10.622288647901396
Epoch: 7881, Batch Gradient Norm after: 10.622288647901396
Epoch 7882/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.38781610131263733
Epoch: 7882, Batch Gradient Norm: 9.57039457907046
Epoch: 7882, Batch Gradient Norm after: 9.57039457907046
Epoch 7883/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3797763526439667
Epoch: 7883, Batch Gradient Norm: 9.148696017875475
Epoch: 7883, Batch Gradient Norm after: 9.148696017875475
Epoch 7884/10000, Prediction Accuracy = 62.79%, Loss = 0.3765650689601898
Epoch: 7884, Batch Gradient Norm: 10.682396375471948
Epoch: 7884, Batch Gradient Norm after: 10.682396375471948
Epoch 7885/10000, Prediction Accuracy = 62.996%, Loss = 0.38802794814109803
Epoch: 7885, Batch Gradient Norm: 10.967787381956258
Epoch: 7885, Batch Gradient Norm after: 10.967787381956258
Epoch 7886/10000, Prediction Accuracy = 63.056000000000004%, Loss = 0.393315851688385
Epoch: 7886, Batch Gradient Norm: 8.643015121749064
Epoch: 7886, Batch Gradient Norm after: 8.643015121749064
Epoch 7887/10000, Prediction Accuracy = 63.05%, Loss = 0.3761301517486572
Epoch: 7887, Batch Gradient Norm: 9.026448061377211
Epoch: 7887, Batch Gradient Norm after: 9.026448061377211
Epoch 7888/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.3772119700908661
Epoch: 7888, Batch Gradient Norm: 10.461940356424511
Epoch: 7888, Batch Gradient Norm after: 10.461940356424511
Epoch 7889/10000, Prediction Accuracy = 62.878%, Loss = 0.3858359336853027
Epoch: 7889, Batch Gradient Norm: 11.444649235086583
Epoch: 7889, Batch Gradient Norm after: 11.444649235086583
Epoch 7890/10000, Prediction Accuracy = 62.812%, Loss = 0.39492249488830566
Epoch: 7890, Batch Gradient Norm: 11.706762909345148
Epoch: 7890, Batch Gradient Norm after: 11.706762909345148
Epoch 7891/10000, Prediction Accuracy = 63.024%, Loss = 0.39729819297790525
Epoch: 7891, Batch Gradient Norm: 10.237868953664954
Epoch: 7891, Batch Gradient Norm after: 10.237868953664954
Epoch 7892/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3860191524028778
Epoch: 7892, Batch Gradient Norm: 9.424183891563844
Epoch: 7892, Batch Gradient Norm after: 9.424183891563844
Epoch 7893/10000, Prediction Accuracy = 63.112%, Loss = 0.38032364249229433
Epoch: 7893, Batch Gradient Norm: 9.971636193668106
Epoch: 7893, Batch Gradient Norm after: 9.971636193668106
Epoch 7894/10000, Prediction Accuracy = 62.706%, Loss = 0.38413039445877073
Epoch: 7894, Batch Gradient Norm: 10.354698900107403
Epoch: 7894, Batch Gradient Norm after: 10.354698900107403
Epoch 7895/10000, Prediction Accuracy = 62.854%, Loss = 0.3869365990161896
Epoch: 7895, Batch Gradient Norm: 11.289514300907095
Epoch: 7895, Batch Gradient Norm after: 11.289514300907095
Epoch 7896/10000, Prediction Accuracy = 63.004%, Loss = 0.3932736337184906
Epoch: 7896, Batch Gradient Norm: 10.77636638146823
Epoch: 7896, Batch Gradient Norm after: 10.77636638146823
Epoch 7897/10000, Prediction Accuracy = 62.964%, Loss = 0.3901753306388855
Epoch: 7897, Batch Gradient Norm: 9.87905051390589
Epoch: 7897, Batch Gradient Norm after: 9.87905051390589
Epoch 7898/10000, Prediction Accuracy = 62.914%, Loss = 0.3837199568748474
Epoch: 7898, Batch Gradient Norm: 9.646752731944765
Epoch: 7898, Batch Gradient Norm after: 9.646752731944765
Epoch 7899/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.38059787154197694
Epoch: 7899, Batch Gradient Norm: 11.066140495367913
Epoch: 7899, Batch Gradient Norm after: 11.066140495367913
Epoch 7900/10000, Prediction Accuracy = 62.908%, Loss = 0.38926883935928347
Epoch: 7900, Batch Gradient Norm: 11.76056335932897
Epoch: 7900, Batch Gradient Norm after: 11.76056335932897
Epoch 7901/10000, Prediction Accuracy = 62.79%, Loss = 0.3953950822353363
Epoch: 7901, Batch Gradient Norm: 9.656970486548785
Epoch: 7901, Batch Gradient Norm after: 9.656970486548785
Epoch 7902/10000, Prediction Accuracy = 62.798%, Loss = 0.38199501037597655
Epoch: 7902, Batch Gradient Norm: 7.973846507871004
Epoch: 7902, Batch Gradient Norm after: 7.973846507871004
Epoch 7903/10000, Prediction Accuracy = 62.986000000000004%, Loss = 0.3712320923805237
Epoch: 7903, Batch Gradient Norm: 8.317760220994055
Epoch: 7903, Batch Gradient Norm after: 8.317760220994055
Epoch 7904/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.37253187894821166
Epoch: 7904, Batch Gradient Norm: 10.149810430049314
Epoch: 7904, Batch Gradient Norm after: 10.149810430049314
Epoch 7905/10000, Prediction Accuracy = 62.85799999999999%, Loss = 0.3847085416316986
Epoch: 7905, Batch Gradient Norm: 10.103602917994962
Epoch: 7905, Batch Gradient Norm after: 10.103602917994962
Epoch 7906/10000, Prediction Accuracy = 62.85%, Loss = 0.3861474096775055
Epoch: 7906, Batch Gradient Norm: 9.424019246123061
Epoch: 7906, Batch Gradient Norm after: 9.424019246123061
Epoch 7907/10000, Prediction Accuracy = 63.08%, Loss = 0.3808353006839752
Epoch: 7907, Batch Gradient Norm: 10.435928873181567
Epoch: 7907, Batch Gradient Norm after: 10.435928873181567
Epoch 7908/10000, Prediction Accuracy = 63.05%, Loss = 0.3857195436954498
Epoch: 7908, Batch Gradient Norm: 11.013757801711865
Epoch: 7908, Batch Gradient Norm after: 11.013757801711865
Epoch 7909/10000, Prediction Accuracy = 63.084%, Loss = 0.3919616281986237
Epoch: 7909, Batch Gradient Norm: 8.629855528582986
Epoch: 7909, Batch Gradient Norm after: 8.629855528582986
Epoch 7910/10000, Prediction Accuracy = 63.06%, Loss = 0.377070415019989
Epoch: 7910, Batch Gradient Norm: 8.919887499057342
Epoch: 7910, Batch Gradient Norm after: 8.919887499057342
Epoch 7911/10000, Prediction Accuracy = 62.962%, Loss = 0.3783699333667755
Epoch: 7911, Batch Gradient Norm: 10.837302202012102
Epoch: 7911, Batch Gradient Norm after: 10.837302202012102
Epoch 7912/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3900424957275391
Epoch: 7912, Batch Gradient Norm: 12.313013392245438
Epoch: 7912, Batch Gradient Norm after: 12.313013392245438
Epoch 7913/10000, Prediction Accuracy = 62.848%, Loss = 0.40101616978645327
Epoch: 7913, Batch Gradient Norm: 10.774487494825728
Epoch: 7913, Batch Gradient Norm after: 10.774487494825728
Epoch 7914/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.38843569755554197
Epoch: 7914, Batch Gradient Norm: 10.40829629543903
Epoch: 7914, Batch Gradient Norm after: 10.40829629543903
Epoch 7915/10000, Prediction Accuracy = 63.007999999999996%, Loss = 0.3845620572566986
Epoch: 7915, Batch Gradient Norm: 10.58533960503526
Epoch: 7915, Batch Gradient Norm after: 10.58533960503526
Epoch 7916/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3862158417701721
Epoch: 7916, Batch Gradient Norm: 8.714555699633808
Epoch: 7916, Batch Gradient Norm after: 8.714555699633808
Epoch 7917/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.37462479472160337
Epoch: 7917, Batch Gradient Norm: 7.582665697109302
Epoch: 7917, Batch Gradient Norm after: 7.582665697109302
Epoch 7918/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.3686791121959686
Epoch: 7918, Batch Gradient Norm: 9.00860063484223
Epoch: 7918, Batch Gradient Norm after: 9.00860063484223
Epoch 7919/10000, Prediction Accuracy = 62.9%, Loss = 0.3765842914581299
Epoch: 7919, Batch Gradient Norm: 11.276848118562912
Epoch: 7919, Batch Gradient Norm after: 11.276848118562912
Epoch 7920/10000, Prediction Accuracy = 62.61%, Loss = 0.39281301498413085
Epoch: 7920, Batch Gradient Norm: 11.33806307383705
Epoch: 7920, Batch Gradient Norm after: 11.33806307383705
Epoch 7921/10000, Prediction Accuracy = 62.854%, Loss = 0.39396076202392577
Epoch: 7921, Batch Gradient Norm: 9.873577956287582
Epoch: 7921, Batch Gradient Norm after: 9.873577956287582
Epoch 7922/10000, Prediction Accuracy = 62.88199999999999%, Loss = 0.38265090584754946
Epoch: 7922, Batch Gradient Norm: 9.721067627554612
Epoch: 7922, Batch Gradient Norm after: 9.721067627554612
Epoch 7923/10000, Prediction Accuracy = 62.862%, Loss = 0.3814020872116089
Epoch: 7923, Batch Gradient Norm: 10.118572485048807
Epoch: 7923, Batch Gradient Norm after: 10.118572485048807
Epoch 7924/10000, Prediction Accuracy = 62.91400000000001%, Loss = 0.38496307730674745
Epoch: 7924, Batch Gradient Norm: 10.39191854082793
Epoch: 7924, Batch Gradient Norm after: 10.39191854082793
Epoch 7925/10000, Prediction Accuracy = 62.96%, Loss = 0.38716630935668944
Epoch: 7925, Batch Gradient Norm: 11.026653015156837
Epoch: 7925, Batch Gradient Norm after: 11.026653015156837
Epoch 7926/10000, Prediction Accuracy = 62.972%, Loss = 0.39227100014686583
Epoch: 7926, Batch Gradient Norm: 9.515715526678132
Epoch: 7926, Batch Gradient Norm after: 9.515715526678132
Epoch 7927/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.38348308801651
Epoch: 7927, Batch Gradient Norm: 9.064195483860134
Epoch: 7927, Batch Gradient Norm after: 9.064195483860134
Epoch 7928/10000, Prediction Accuracy = 62.926%, Loss = 0.3793344974517822
Epoch: 7928, Batch Gradient Norm: 11.032999690837165
Epoch: 7928, Batch Gradient Norm after: 11.032999690837165
Epoch 7929/10000, Prediction Accuracy = 62.916%, Loss = 0.3913584291934967
Epoch: 7929, Batch Gradient Norm: 12.078089241132005
Epoch: 7929, Batch Gradient Norm after: 12.078089241132005
Epoch 7930/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.3973369896411896
Epoch: 7930, Batch Gradient Norm: 11.647076381667286
Epoch: 7930, Batch Gradient Norm after: 11.647076381667286
Epoch 7931/10000, Prediction Accuracy = 62.931999999999995%, Loss = 0.39273343682289125
Epoch: 7931, Batch Gradient Norm: 10.572681493385522
Epoch: 7931, Batch Gradient Norm after: 10.572681493385522
Epoch 7932/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3853616535663605
Epoch: 7932, Batch Gradient Norm: 9.08014833923964
Epoch: 7932, Batch Gradient Norm after: 9.08014833923964
Epoch 7933/10000, Prediction Accuracy = 62.919999999999995%, Loss = 0.37625840306282043
Epoch: 7933, Batch Gradient Norm: 8.915706679797806
Epoch: 7933, Batch Gradient Norm after: 8.915706679797806
Epoch 7934/10000, Prediction Accuracy = 62.912%, Loss = 0.37504178285598755
Epoch: 7934, Batch Gradient Norm: 9.477508081289676
Epoch: 7934, Batch Gradient Norm after: 9.477508081289676
Epoch 7935/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.37828634977340697
Epoch: 7935, Batch Gradient Norm: 10.803615462334097
Epoch: 7935, Batch Gradient Norm after: 10.803615462334097
Epoch 7936/10000, Prediction Accuracy = 62.867999999999995%, Loss = 0.38666353821754457
Epoch: 7936, Batch Gradient Norm: 11.2604592595301
Epoch: 7936, Batch Gradient Norm after: 11.2604592595301
Epoch 7937/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3915654420852661
Epoch: 7937, Batch Gradient Norm: 10.049632519506138
Epoch: 7937, Batch Gradient Norm after: 10.049632519506138
Epoch 7938/10000, Prediction Accuracy = 62.888%, Loss = 0.3838421702384949
Epoch: 7938, Batch Gradient Norm: 10.685117036850784
Epoch: 7938, Batch Gradient Norm after: 10.685117036850784
Epoch 7939/10000, Prediction Accuracy = 62.886%, Loss = 0.38799409866333007
Epoch: 7939, Batch Gradient Norm: 11.892656327040948
Epoch: 7939, Batch Gradient Norm after: 11.892656327040948
Epoch 7940/10000, Prediction Accuracy = 62.888%, Loss = 0.3977705180644989
Epoch: 7940, Batch Gradient Norm: 11.15982447431336
Epoch: 7940, Batch Gradient Norm after: 11.15982447431336
Epoch 7941/10000, Prediction Accuracy = 63.17%, Loss = 0.3919964969158173
Epoch: 7941, Batch Gradient Norm: 9.679323255430162
Epoch: 7941, Batch Gradient Norm after: 9.679323255430162
Epoch 7942/10000, Prediction Accuracy = 62.982000000000006%, Loss = 0.3811531662940979
Epoch: 7942, Batch Gradient Norm: 8.565503796922812
Epoch: 7942, Batch Gradient Norm after: 8.565503796922812
Epoch 7943/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3744153380393982
Epoch: 7943, Batch Gradient Norm: 7.938842597487832
Epoch: 7943, Batch Gradient Norm after: 7.938842597487832
Epoch 7944/10000, Prediction Accuracy = 63.062%, Loss = 0.37180857062339784
Epoch: 7944, Batch Gradient Norm: 8.311689696402995
Epoch: 7944, Batch Gradient Norm after: 8.311689696402995
Epoch 7945/10000, Prediction Accuracy = 63.004%, Loss = 0.3731545627117157
Epoch: 7945, Batch Gradient Norm: 10.177149538682212
Epoch: 7945, Batch Gradient Norm after: 10.177149538682212
Epoch 7946/10000, Prediction Accuracy = 62.88800000000001%, Loss = 0.38425002098083494
Epoch: 7946, Batch Gradient Norm: 10.826846972873966
Epoch: 7946, Batch Gradient Norm after: 10.826846972873966
Epoch 7947/10000, Prediction Accuracy = 62.85600000000001%, Loss = 0.38835486173629763
Epoch: 7947, Batch Gradient Norm: 10.639652835545906
Epoch: 7947, Batch Gradient Norm after: 10.639652835545906
Epoch 7948/10000, Prediction Accuracy = 62.96%, Loss = 0.38703798651695254
Epoch: 7948, Batch Gradient Norm: 9.273445876512733
Epoch: 7948, Batch Gradient Norm after: 9.273445876512733
Epoch 7949/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.37909168004989624
Epoch: 7949, Batch Gradient Norm: 10.031464922335893
Epoch: 7949, Batch Gradient Norm after: 10.031464922335893
Epoch 7950/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.38426888585090635
Epoch: 7950, Batch Gradient Norm: 11.60390687344021
Epoch: 7950, Batch Gradient Norm after: 11.60390687344021
Epoch 7951/10000, Prediction Accuracy = 62.866%, Loss = 0.395080691576004
Epoch: 7951, Batch Gradient Norm: 10.539514004328977
Epoch: 7951, Batch Gradient Norm after: 10.539514004328977
Epoch 7952/10000, Prediction Accuracy = 62.996%, Loss = 0.3864264369010925
Epoch: 7952, Batch Gradient Norm: 9.638121800717412
Epoch: 7952, Batch Gradient Norm after: 9.638121800717412
Epoch 7953/10000, Prediction Accuracy = 62.948%, Loss = 0.37980580925941465
Epoch: 7953, Batch Gradient Norm: 9.854568780623927
Epoch: 7953, Batch Gradient Norm after: 9.854568780623927
Epoch 7954/10000, Prediction Accuracy = 62.838%, Loss = 0.3827764093875885
Epoch: 7954, Batch Gradient Norm: 8.81923019355101
Epoch: 7954, Batch Gradient Norm after: 8.81923019355101
Epoch 7955/10000, Prediction Accuracy = 62.988%, Loss = 0.3756845772266388
Epoch: 7955, Batch Gradient Norm: 9.760884241742202
Epoch: 7955, Batch Gradient Norm after: 9.760884241742202
Epoch 7956/10000, Prediction Accuracy = 63.02399999999999%, Loss = 0.3810009598731995
Epoch: 7956, Batch Gradient Norm: 11.945659842940433
Epoch: 7956, Batch Gradient Norm after: 11.945659842940433
Epoch 7957/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3962876498699188
Epoch: 7957, Batch Gradient Norm: 11.942478412934788
Epoch: 7957, Batch Gradient Norm after: 11.942478412934788
Epoch 7958/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.39631741046905516
Epoch: 7958, Batch Gradient Norm: 10.466932662019603
Epoch: 7958, Batch Gradient Norm after: 10.466932662019603
Epoch 7959/10000, Prediction Accuracy = 63.074%, Loss = 0.38616169691085817
Epoch: 7959, Batch Gradient Norm: 9.47797094714388
Epoch: 7959, Batch Gradient Norm after: 9.47797094714388
Epoch 7960/10000, Prediction Accuracy = 63.004%, Loss = 0.3797935307025909
Epoch: 7960, Batch Gradient Norm: 9.119147085574816
Epoch: 7960, Batch Gradient Norm after: 9.119147085574816
Epoch 7961/10000, Prediction Accuracy = 62.924%, Loss = 0.37686766386032106
Epoch: 7961, Batch Gradient Norm: 10.407146085688694
Epoch: 7961, Batch Gradient Norm after: 10.407146085688694
Epoch 7962/10000, Prediction Accuracy = 62.864%, Loss = 0.38475103974342345
Epoch: 7962, Batch Gradient Norm: 11.423526163484802
Epoch: 7962, Batch Gradient Norm after: 11.423526163484802
Epoch 7963/10000, Prediction Accuracy = 62.852%, Loss = 0.3916217863559723
Epoch: 7963, Batch Gradient Norm: 11.068146767370438
Epoch: 7963, Batch Gradient Norm after: 11.068146767370438
Epoch 7964/10000, Prediction Accuracy = 62.9%, Loss = 0.3894063591957092
Epoch: 7964, Batch Gradient Norm: 9.1838567785355
Epoch: 7964, Batch Gradient Norm after: 9.1838567785355
Epoch 7965/10000, Prediction Accuracy = 62.976%, Loss = 0.3780555725097656
Epoch: 7965, Batch Gradient Norm: 8.505142397176975
Epoch: 7965, Batch Gradient Norm after: 8.505142397176975
Epoch 7966/10000, Prediction Accuracy = 63.028%, Loss = 0.37436257004737855
Epoch: 7966, Batch Gradient Norm: 8.657842775184804
Epoch: 7966, Batch Gradient Norm after: 8.657842775184804
Epoch 7967/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3757504642009735
Epoch: 7967, Batch Gradient Norm: 8.657152205662596
Epoch: 7967, Batch Gradient Norm after: 8.657152205662596
Epoch 7968/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.37553954124450684
Epoch: 7968, Batch Gradient Norm: 9.651693462289694
Epoch: 7968, Batch Gradient Norm after: 9.651693462289694
Epoch 7969/10000, Prediction Accuracy = 63.016%, Loss = 0.37943395376205447
Epoch: 7969, Batch Gradient Norm: 11.910296707172568
Epoch: 7969, Batch Gradient Norm after: 11.910296707172568
Epoch 7970/10000, Prediction Accuracy = 62.908%, Loss = 0.394194221496582
Epoch: 7970, Batch Gradient Norm: 12.028883087801159
Epoch: 7970, Batch Gradient Norm after: 12.028883087801159
Epoch 7971/10000, Prediction Accuracy = 62.92999999999999%, Loss = 0.39573042988777163
Epoch: 7971, Batch Gradient Norm: 10.979034022450254
Epoch: 7971, Batch Gradient Norm after: 10.979034022450254
Epoch 7972/10000, Prediction Accuracy = 62.977999999999994%, Loss = 0.3895712673664093
Epoch: 7972, Batch Gradient Norm: 9.906609669017097
Epoch: 7972, Batch Gradient Norm after: 9.906609669017097
Epoch 7973/10000, Prediction Accuracy = 63.084%, Loss = 0.3835453987121582
Epoch: 7973, Batch Gradient Norm: 8.920346145048038
Epoch: 7973, Batch Gradient Norm after: 8.920346145048038
Epoch 7974/10000, Prediction Accuracy = 62.8%, Loss = 0.37778879404067994
Epoch: 7974, Batch Gradient Norm: 9.63854524185583
Epoch: 7974, Batch Gradient Norm after: 9.63854524185583
Epoch 7975/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.38162135481834414
Epoch: 7975, Batch Gradient Norm: 11.05212940025198
Epoch: 7975, Batch Gradient Norm after: 11.05212940025198
Epoch 7976/10000, Prediction Accuracy = 62.751999999999995%, Loss = 0.39002291560173036
Epoch: 7976, Batch Gradient Norm: 10.979073512802186
Epoch: 7976, Batch Gradient Norm after: 10.979073512802186
Epoch 7977/10000, Prediction Accuracy = 62.984%, Loss = 0.3898202359676361
Epoch: 7977, Batch Gradient Norm: 10.078395212115106
Epoch: 7977, Batch Gradient Norm after: 10.078395212115106
Epoch 7978/10000, Prediction Accuracy = 62.972%, Loss = 0.3822087287902832
Epoch: 7978, Batch Gradient Norm: 10.743387631317388
Epoch: 7978, Batch Gradient Norm after: 10.743387631317388
Epoch 7979/10000, Prediction Accuracy = 63.012%, Loss = 0.38726412653923037
Epoch: 7979, Batch Gradient Norm: 10.261591197804167
Epoch: 7979, Batch Gradient Norm after: 10.261591197804167
Epoch 7980/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.38450254797935485
Epoch: 7980, Batch Gradient Norm: 9.252306122805841
Epoch: 7980, Batch Gradient Norm after: 9.252306122805841
Epoch 7981/10000, Prediction Accuracy = 62.87199999999999%, Loss = 0.37778563499450685
Epoch: 7981, Batch Gradient Norm: 8.695243389216964
Epoch: 7981, Batch Gradient Norm after: 8.695243389216964
Epoch 7982/10000, Prediction Accuracy = 63.04200000000001%, Loss = 0.3754737973213196
Epoch: 7982, Batch Gradient Norm: 9.92644368826427
Epoch: 7982, Batch Gradient Norm after: 9.92644368826427
Epoch 7983/10000, Prediction Accuracy = 63.088%, Loss = 0.38334856629371644
Epoch: 7983, Batch Gradient Norm: 11.728999985317476
Epoch: 7983, Batch Gradient Norm after: 11.728999985317476
Epoch 7984/10000, Prediction Accuracy = 62.952%, Loss = 0.3945783734321594
Epoch: 7984, Batch Gradient Norm: 10.627065779595585
Epoch: 7984, Batch Gradient Norm after: 10.627065779595585
Epoch 7985/10000, Prediction Accuracy = 62.996%, Loss = 0.3879841804504395
Epoch: 7985, Batch Gradient Norm: 9.95600262973109
Epoch: 7985, Batch Gradient Norm after: 9.95600262973109
Epoch 7986/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.3829394519329071
Epoch: 7986, Batch Gradient Norm: 11.73992902071289
Epoch: 7986, Batch Gradient Norm after: 11.73992902071289
Epoch 7987/10000, Prediction Accuracy = 62.914%, Loss = 0.3934848129749298
Epoch: 7987, Batch Gradient Norm: 11.400052810077108
Epoch: 7987, Batch Gradient Norm after: 11.400052810077108
Epoch 7988/10000, Prediction Accuracy = 63.076%, Loss = 0.39079258441925047
Epoch: 7988, Batch Gradient Norm: 8.499911381715481
Epoch: 7988, Batch Gradient Norm after: 8.499911381715481
Epoch 7989/10000, Prediction Accuracy = 63.024%, Loss = 0.3722178816795349
Epoch: 7989, Batch Gradient Norm: 7.794733567444431
Epoch: 7989, Batch Gradient Norm after: 7.794733567444431
Epoch 7990/10000, Prediction Accuracy = 63.008%, Loss = 0.36873856782913206
Epoch: 7990, Batch Gradient Norm: 8.874044705767716
Epoch: 7990, Batch Gradient Norm after: 8.874044705767716
Epoch 7991/10000, Prediction Accuracy = 62.898%, Loss = 0.3752856731414795
Epoch: 7991, Batch Gradient Norm: 10.43581823324988
Epoch: 7991, Batch Gradient Norm after: 10.43581823324988
Epoch 7992/10000, Prediction Accuracy = 62.958000000000006%, Loss = 0.38564656376838685
Epoch: 7992, Batch Gradient Norm: 12.796163495573765
Epoch: 7992, Batch Gradient Norm after: 12.796163495573765
Epoch 7993/10000, Prediction Accuracy = 62.907999999999994%, Loss = 0.40353643894195557
Epoch: 7993, Batch Gradient Norm: 11.039417337392306
Epoch: 7993, Batch Gradient Norm after: 11.039417337392306
Epoch 7994/10000, Prediction Accuracy = 62.910000000000004%, Loss = 0.3890487790107727
Epoch: 7994, Batch Gradient Norm: 8.868262414557822
Epoch: 7994, Batch Gradient Norm after: 8.868262414557822
Epoch 7995/10000, Prediction Accuracy = 62.86600000000001%, Loss = 0.37412331700325013
Epoch: 7995, Batch Gradient Norm: 9.40804061375898
Epoch: 7995, Batch Gradient Norm after: 9.40804061375898
Epoch 7996/10000, Prediction Accuracy = 62.852%, Loss = 0.3774082660675049
Epoch: 7996, Batch Gradient Norm: 11.181834597062156
Epoch: 7996, Batch Gradient Norm after: 11.181834597062156
Epoch 7997/10000, Prediction Accuracy = 63.02%, Loss = 0.38942180275917054
Epoch: 7997, Batch Gradient Norm: 10.902829993120239
Epoch: 7997, Batch Gradient Norm after: 10.902829993120239
Epoch 7998/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.38843194842338563
Epoch: 7998, Batch Gradient Norm: 9.673511794808205
Epoch: 7998, Batch Gradient Norm after: 9.673511794808205
Epoch 7999/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3810795068740845
Epoch: 7999, Batch Gradient Norm: 9.263372112599884
Epoch: 7999, Batch Gradient Norm after: 9.263372112599884
Epoch 8000/10000, Prediction Accuracy = 62.864%, Loss = 0.3789886236190796
Epoch: 8000, Batch Gradient Norm: 8.255177904032562
Epoch: 8000, Batch Gradient Norm after: 8.255177904032562
Epoch 8001/10000, Prediction Accuracy = 63.064%, Loss = 0.37319459319114684
Epoch: 8001, Batch Gradient Norm: 7.438781436806506
Epoch: 8001, Batch Gradient Norm after: 7.438781436806506
Epoch 8002/10000, Prediction Accuracy = 63.044000000000004%, Loss = 0.36705743670463564
Epoch: 8002, Batch Gradient Norm: 9.500439974313732
Epoch: 8002, Batch Gradient Norm after: 9.500439974313732
Epoch 8003/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.37809590697288514
Epoch: 8003, Batch Gradient Norm: 12.892027928129604
Epoch: 8003, Batch Gradient Norm after: 12.892027928129604
Epoch 8004/10000, Prediction Accuracy = 62.842000000000006%, Loss = 0.4040761232376099
Epoch: 8004, Batch Gradient Norm: 12.342273407074574
Epoch: 8004, Batch Gradient Norm after: 12.342273407074574
Epoch 8005/10000, Prediction Accuracy = 62.924%, Loss = 0.40148885250091554
Epoch: 8005, Batch Gradient Norm: 10.0292587436309
Epoch: 8005, Batch Gradient Norm after: 10.0292587436309
Epoch 8006/10000, Prediction Accuracy = 62.922000000000004%, Loss = 0.3827285528182983
Epoch: 8006, Batch Gradient Norm: 10.407072006286283
Epoch: 8006, Batch Gradient Norm after: 10.407072006286283
Epoch 8007/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.3841792345046997
Epoch: 8007, Batch Gradient Norm: 10.667171892888438
Epoch: 8007, Batch Gradient Norm after: 10.667171892888438
Epoch 8008/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3850986063480377
Epoch: 8008, Batch Gradient Norm: 10.61828599487393
Epoch: 8008, Batch Gradient Norm after: 10.61828599487393
Epoch 8009/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.3847184479236603
Epoch: 8009, Batch Gradient Norm: 9.663080810724717
Epoch: 8009, Batch Gradient Norm after: 9.663080810724717
Epoch 8010/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.37917510271072385
Epoch: 8010, Batch Gradient Norm: 8.512662470799725
Epoch: 8010, Batch Gradient Norm after: 8.512662470799725
Epoch 8011/10000, Prediction Accuracy = 63.08800000000001%, Loss = 0.3731006860733032
Epoch: 8011, Batch Gradient Norm: 8.976688344268455
Epoch: 8011, Batch Gradient Norm after: 8.976688344268455
Epoch 8012/10000, Prediction Accuracy = 63.012%, Loss = 0.3754806637763977
Epoch: 8012, Batch Gradient Norm: 10.768840401961857
Epoch: 8012, Batch Gradient Norm after: 10.768840401961857
Epoch 8013/10000, Prediction Accuracy = 62.962%, Loss = 0.38599308729171755
Epoch: 8013, Batch Gradient Norm: 11.213031988069977
Epoch: 8013, Batch Gradient Norm after: 11.213031988069977
Epoch 8014/10000, Prediction Accuracy = 62.92999999999999%, Loss = 0.38886908292770384
Epoch: 8014, Batch Gradient Norm: 10.678424612789932
Epoch: 8014, Batch Gradient Norm after: 10.678424612789932
Epoch 8015/10000, Prediction Accuracy = 62.932%, Loss = 0.3855279326438904
Epoch: 8015, Batch Gradient Norm: 10.044941100637805
Epoch: 8015, Batch Gradient Norm after: 10.044941100637805
Epoch 8016/10000, Prediction Accuracy = 63.016%, Loss = 0.38227337002754214
Epoch: 8016, Batch Gradient Norm: 9.60159132665722
Epoch: 8016, Batch Gradient Norm after: 9.60159132665722
Epoch 8017/10000, Prediction Accuracy = 62.952%, Loss = 0.3791672885417938
Epoch: 8017, Batch Gradient Norm: 10.879780828596154
Epoch: 8017, Batch Gradient Norm after: 10.879780828596154
Epoch 8018/10000, Prediction Accuracy = 62.91799999999999%, Loss = 0.387437242269516
Epoch: 8018, Batch Gradient Norm: 11.527316733266403
Epoch: 8018, Batch Gradient Norm after: 11.527316733266403
Epoch 8019/10000, Prediction Accuracy = 62.848%, Loss = 0.3956829905509949
Epoch: 8019, Batch Gradient Norm: 9.030452800807643
Epoch: 8019, Batch Gradient Norm after: 9.030452800807643
Epoch 8020/10000, Prediction Accuracy = 62.996%, Loss = 0.37816586494445803
Epoch: 8020, Batch Gradient Norm: 8.170456033089952
Epoch: 8020, Batch Gradient Norm after: 8.170456033089952
Epoch 8021/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3725146412849426
Epoch: 8021, Batch Gradient Norm: 10.11705770662384
Epoch: 8021, Batch Gradient Norm after: 10.11705770662384
Epoch 8022/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.38426080346107483
Epoch: 8022, Batch Gradient Norm: 11.037308099426866
Epoch: 8022, Batch Gradient Norm after: 11.037308099426866
Epoch 8023/10000, Prediction Accuracy = 62.92%, Loss = 0.3910303473472595
Epoch: 8023, Batch Gradient Norm: 9.040618138346465
Epoch: 8023, Batch Gradient Norm after: 9.040618138346465
Epoch 8024/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.37595123052597046
Epoch: 8024, Batch Gradient Norm: 9.395793920761733
Epoch: 8024, Batch Gradient Norm after: 9.395793920761733
Epoch 8025/10000, Prediction Accuracy = 62.936%, Loss = 0.37858360409736636
Epoch: 8025, Batch Gradient Norm: 9.069042879517175
Epoch: 8025, Batch Gradient Norm after: 9.069042879517175
Epoch 8026/10000, Prediction Accuracy = 63.056%, Loss = 0.377619868516922
Epoch: 8026, Batch Gradient Norm: 8.825572410388789
Epoch: 8026, Batch Gradient Norm after: 8.825572410388789
Epoch 8027/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.3760022759437561
Epoch: 8027, Batch Gradient Norm: 10.192526358973033
Epoch: 8027, Batch Gradient Norm after: 10.192526358973033
Epoch 8028/10000, Prediction Accuracy = 63.038%, Loss = 0.38199684023857117
Epoch: 8028, Batch Gradient Norm: 13.794702397634822
Epoch: 8028, Batch Gradient Norm after: 13.794702397634822
Epoch 8029/10000, Prediction Accuracy = 62.831999999999994%, Loss = 0.4077354907989502
Epoch: 8029, Batch Gradient Norm: 11.955583611933154
Epoch: 8029, Batch Gradient Norm after: 11.955583611933154
Epoch 8030/10000, Prediction Accuracy = 62.98599999999999%, Loss = 0.3935851573944092
Epoch: 8030, Batch Gradient Norm: 9.519751991706919
Epoch: 8030, Batch Gradient Norm after: 9.519751991706919
Epoch 8031/10000, Prediction Accuracy = 63.04%, Loss = 0.3775543212890625
Epoch: 8031, Batch Gradient Norm: 10.347677795450812
Epoch: 8031, Batch Gradient Norm after: 10.347677795450812
Epoch 8032/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.38345045447349546
Epoch: 8032, Batch Gradient Norm: 11.730576196443439
Epoch: 8032, Batch Gradient Norm after: 11.730576196443439
Epoch 8033/10000, Prediction Accuracy = 62.932%, Loss = 0.39294445514678955
Epoch: 8033, Batch Gradient Norm: 11.435596182874027
Epoch: 8033, Batch Gradient Norm after: 11.435596182874027
Epoch 8034/10000, Prediction Accuracy = 62.852%, Loss = 0.39224115014076233
Epoch: 8034, Batch Gradient Norm: 8.157328324916431
Epoch: 8034, Batch Gradient Norm after: 8.157328324916431
Epoch 8035/10000, Prediction Accuracy = 63.032%, Loss = 0.37172067165374756
Epoch: 8035, Batch Gradient Norm: 7.0845992171169785
Epoch: 8035, Batch Gradient Norm after: 7.0845992171169785
Epoch 8036/10000, Prediction Accuracy = 63.056%, Loss = 0.3659858047962189
Epoch: 8036, Batch Gradient Norm: 8.639378906869965
Epoch: 8036, Batch Gradient Norm after: 8.639378906869965
Epoch 8037/10000, Prediction Accuracy = 63.112%, Loss = 0.3738817870616913
Epoch: 8037, Batch Gradient Norm: 9.969498337560415
Epoch: 8037, Batch Gradient Norm after: 9.969498337560415
Epoch 8038/10000, Prediction Accuracy = 63.0%, Loss = 0.3816933810710907
Epoch: 8038, Batch Gradient Norm: 10.590844910306583
Epoch: 8038, Batch Gradient Norm after: 10.590844910306583
Epoch 8039/10000, Prediction Accuracy = 62.908%, Loss = 0.3836139976978302
Epoch: 8039, Batch Gradient Norm: 11.610863004374421
Epoch: 8039, Batch Gradient Norm after: 11.610863004374421
Epoch 8040/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.39054142832756045
Epoch: 8040, Batch Gradient Norm: 11.274107376897792
Epoch: 8040, Batch Gradient Norm after: 11.274107376897792
Epoch 8041/10000, Prediction Accuracy = 63.016000000000005%, Loss = 0.39024920463562013
Epoch: 8041, Batch Gradient Norm: 9.737971021575952
Epoch: 8041, Batch Gradient Norm after: 9.737971021575952
Epoch 8042/10000, Prediction Accuracy = 63.044%, Loss = 0.3798306524753571
Epoch: 8042, Batch Gradient Norm: 9.628308224395576
Epoch: 8042, Batch Gradient Norm after: 9.628308224395576
Epoch 8043/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.37841117978096006
Epoch: 8043, Batch Gradient Norm: 10.886478138104975
Epoch: 8043, Batch Gradient Norm after: 10.886478138104975
Epoch 8044/10000, Prediction Accuracy = 63.03800000000001%, Loss = 0.38730508685112
Epoch: 8044, Batch Gradient Norm: 11.058808365175809
Epoch: 8044, Batch Gradient Norm after: 11.058808365175809
Epoch 8045/10000, Prediction Accuracy = 63.041999999999994%, Loss = 0.3890697419643402
Epoch: 8045, Batch Gradient Norm: 9.790005454388243
Epoch: 8045, Batch Gradient Norm after: 9.790005454388243
Epoch 8046/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3818156898021698
Epoch: 8046, Batch Gradient Norm: 8.480134626382478
Epoch: 8046, Batch Gradient Norm after: 8.480134626382478
Epoch 8047/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.37352120876312256
Epoch: 8047, Batch Gradient Norm: 9.506437661944027
Epoch: 8047, Batch Gradient Norm after: 9.506437661944027
Epoch 8048/10000, Prediction Accuracy = 62.98%, Loss = 0.3790464043617249
Epoch: 8048, Batch Gradient Norm: 10.762593115426018
Epoch: 8048, Batch Gradient Norm after: 10.762593115426018
Epoch 8049/10000, Prediction Accuracy = 62.94200000000001%, Loss = 0.3877979040145874
Epoch: 8049, Batch Gradient Norm: 11.548243643959548
Epoch: 8049, Batch Gradient Norm after: 11.548243643959548
Epoch 8050/10000, Prediction Accuracy = 62.822%, Loss = 0.39227302074432374
Epoch: 8050, Batch Gradient Norm: 10.450162109736448
Epoch: 8050, Batch Gradient Norm after: 10.450162109736448
Epoch 8051/10000, Prediction Accuracy = 63.04%, Loss = 0.3848807871341705
Epoch: 8051, Batch Gradient Norm: 9.023961392971495
Epoch: 8051, Batch Gradient Norm after: 9.023961392971495
Epoch 8052/10000, Prediction Accuracy = 63.008%, Loss = 0.37600221633911135
Epoch: 8052, Batch Gradient Norm: 9.52875886681216
Epoch: 8052, Batch Gradient Norm after: 9.52875886681216
Epoch 8053/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.3790558338165283
Epoch: 8053, Batch Gradient Norm: 11.01064511221135
Epoch: 8053, Batch Gradient Norm after: 11.01064511221135
Epoch 8054/10000, Prediction Accuracy = 63.06999999999999%, Loss = 0.3894825577735901
Epoch: 8054, Batch Gradient Norm: 9.930575620650131
Epoch: 8054, Batch Gradient Norm after: 9.930575620650131
Epoch 8055/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.38113353252410886
Epoch: 8055, Batch Gradient Norm: 9.20337566038698
Epoch: 8055, Batch Gradient Norm after: 9.20337566038698
Epoch 8056/10000, Prediction Accuracy = 63.092000000000006%, Loss = 0.3749582588672638
Epoch: 8056, Batch Gradient Norm: 11.840025444743773
Epoch: 8056, Batch Gradient Norm after: 11.840025444743773
Epoch 8057/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.3934709906578064
Epoch: 8057, Batch Gradient Norm: 12.05671636791093
Epoch: 8057, Batch Gradient Norm after: 12.05671636791093
Epoch 8058/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3962870419025421
Epoch: 8058, Batch Gradient Norm: 9.399505181839187
Epoch: 8058, Batch Gradient Norm after: 9.399505181839187
Epoch 8059/10000, Prediction Accuracy = 63.092000000000006%, Loss = 0.3771884202957153
Epoch: 8059, Batch Gradient Norm: 7.775865312483798
Epoch: 8059, Batch Gradient Norm after: 7.775865312483798
Epoch 8060/10000, Prediction Accuracy = 62.972%, Loss = 0.36725465655326844
Epoch: 8060, Batch Gradient Norm: 9.67001029580878
Epoch: 8060, Batch Gradient Norm after: 9.67001029580878
Epoch 8061/10000, Prediction Accuracy = 63.09599999999999%, Loss = 0.37752899527549744
Epoch: 8061, Batch Gradient Norm: 12.294265850995703
Epoch: 8061, Batch Gradient Norm after: 12.294265850995703
Epoch 8062/10000, Prediction Accuracy = 62.984%, Loss = 0.3988648414611816
Epoch: 8062, Batch Gradient Norm: 10.64460811738443
Epoch: 8062, Batch Gradient Norm after: 10.64460811738443
Epoch 8063/10000, Prediction Accuracy = 62.976%, Loss = 0.38613436222076414
Epoch: 8063, Batch Gradient Norm: 10.3415143190704
Epoch: 8063, Batch Gradient Norm after: 10.3415143190704
Epoch 8064/10000, Prediction Accuracy = 62.959999999999994%, Loss = 0.384690648317337
Epoch: 8064, Batch Gradient Norm: 9.952270362587004
Epoch: 8064, Batch Gradient Norm after: 9.952270362587004
Epoch 8065/10000, Prediction Accuracy = 63.052%, Loss = 0.38214391469955444
Epoch: 8065, Batch Gradient Norm: 9.486301976369502
Epoch: 8065, Batch Gradient Norm after: 9.486301976369502
Epoch 8066/10000, Prediction Accuracy = 63.134%, Loss = 0.3781136035919189
Epoch: 8066, Batch Gradient Norm: 10.161379525805804
Epoch: 8066, Batch Gradient Norm after: 10.161379525805804
Epoch 8067/10000, Prediction Accuracy = 62.748000000000005%, Loss = 0.38064411878585813
Epoch: 8067, Batch Gradient Norm: 9.653725323819371
Epoch: 8067, Batch Gradient Norm after: 9.653725323819371
Epoch 8068/10000, Prediction Accuracy = 62.92%, Loss = 0.377143931388855
Epoch: 8068, Batch Gradient Norm: 10.36934287465161
Epoch: 8068, Batch Gradient Norm after: 10.36934287465161
Epoch 8069/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.38232566714286803
Epoch: 8069, Batch Gradient Norm: 10.888693994468477
Epoch: 8069, Batch Gradient Norm after: 10.888693994468477
Epoch 8070/10000, Prediction Accuracy = 63.004%, Loss = 0.3875282406806946
Epoch: 8070, Batch Gradient Norm: 9.895127141656358
Epoch: 8070, Batch Gradient Norm after: 9.895127141656358
Epoch 8071/10000, Prediction Accuracy = 63.08%, Loss = 0.38084561824798585
Epoch: 8071, Batch Gradient Norm: 9.603197942206428
Epoch: 8071, Batch Gradient Norm after: 9.603197942206428
Epoch 8072/10000, Prediction Accuracy = 62.984%, Loss = 0.3792791962623596
Epoch: 8072, Batch Gradient Norm: 9.23683640727686
Epoch: 8072, Batch Gradient Norm after: 9.23683640727686
Epoch 8073/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.3771044135093689
Epoch: 8073, Batch Gradient Norm: 9.555855060900818
Epoch: 8073, Batch Gradient Norm after: 9.555855060900818
Epoch 8074/10000, Prediction Accuracy = 63.05%, Loss = 0.3778706073760986
Epoch: 8074, Batch Gradient Norm: 11.126730515296002
Epoch: 8074, Batch Gradient Norm after: 11.126730515296002
Epoch 8075/10000, Prediction Accuracy = 62.944%, Loss = 0.38654672503471377
Epoch: 8075, Batch Gradient Norm: 11.529650290730908
Epoch: 8075, Batch Gradient Norm after: 11.529650290730908
Epoch 8076/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.38992146253585813
Epoch: 8076, Batch Gradient Norm: 10.017778953748945
Epoch: 8076, Batch Gradient Norm after: 10.017778953748945
Epoch 8077/10000, Prediction Accuracy = 62.834%, Loss = 0.38034326434135435
Epoch: 8077, Batch Gradient Norm: 9.018314583896945
Epoch: 8077, Batch Gradient Norm after: 9.018314583896945
Epoch 8078/10000, Prediction Accuracy = 63.134%, Loss = 0.37455959916114806
Epoch: 8078, Batch Gradient Norm: 9.613031397079789
Epoch: 8078, Batch Gradient Norm after: 9.613031397079789
Epoch 8079/10000, Prediction Accuracy = 62.952%, Loss = 0.37875630855560305
Epoch: 8079, Batch Gradient Norm: 9.520670974151477
Epoch: 8079, Batch Gradient Norm after: 9.520670974151477
Epoch 8080/10000, Prediction Accuracy = 63.044%, Loss = 0.3796306312084198
Epoch: 8080, Batch Gradient Norm: 9.896527673934063
Epoch: 8080, Batch Gradient Norm after: 9.896527673934063
Epoch 8081/10000, Prediction Accuracy = 62.870000000000005%, Loss = 0.3794820666313171
Epoch: 8081, Batch Gradient Norm: 11.692096325970583
Epoch: 8081, Batch Gradient Norm after: 11.692096325970583
Epoch 8082/10000, Prediction Accuracy = 62.85799999999999%, Loss = 0.39145237803459165
Epoch: 8082, Batch Gradient Norm: 10.551644307340167
Epoch: 8082, Batch Gradient Norm after: 10.551644307340167
Epoch 8083/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.3843878388404846
Epoch: 8083, Batch Gradient Norm: 8.495965167744934
Epoch: 8083, Batch Gradient Norm after: 8.495965167744934
Epoch 8084/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3723557353019714
Epoch: 8084, Batch Gradient Norm: 10.050042410357232
Epoch: 8084, Batch Gradient Norm after: 10.050042410357232
Epoch 8085/10000, Prediction Accuracy = 62.92999999999999%, Loss = 0.38407987356185913
Epoch: 8085, Batch Gradient Norm: 11.15155775432563
Epoch: 8085, Batch Gradient Norm after: 11.15155775432563
Epoch 8086/10000, Prediction Accuracy = 63.086%, Loss = 0.39214356541633605
Epoch: 8086, Batch Gradient Norm: 11.963337318437238
Epoch: 8086, Batch Gradient Norm after: 11.963337318437238
Epoch 8087/10000, Prediction Accuracy = 62.891999999999996%, Loss = 0.3968175172805786
Epoch: 8087, Batch Gradient Norm: 10.567728374208247
Epoch: 8087, Batch Gradient Norm after: 10.567728374208247
Epoch 8088/10000, Prediction Accuracy = 63.03000000000001%, Loss = 0.3854612588882446
Epoch: 8088, Batch Gradient Norm: 10.450204461123048
Epoch: 8088, Batch Gradient Norm after: 10.450204461123048
Epoch 8089/10000, Prediction Accuracy = 63.104%, Loss = 0.3843817710876465
Epoch: 8089, Batch Gradient Norm: 9.81826756869546
Epoch: 8089, Batch Gradient Norm after: 9.81826756869546
Epoch 8090/10000, Prediction Accuracy = 62.946000000000005%, Loss = 0.3795113265514374
Epoch: 8090, Batch Gradient Norm: 9.19866726106917
Epoch: 8090, Batch Gradient Norm after: 9.19866726106917
Epoch 8091/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.37561812400817873
Epoch: 8091, Batch Gradient Norm: 8.496845186839796
Epoch: 8091, Batch Gradient Norm after: 8.496845186839796
Epoch 8092/10000, Prediction Accuracy = 62.976%, Loss = 0.37123762965202334
Epoch: 8092, Batch Gradient Norm: 9.64323385462268
Epoch: 8092, Batch Gradient Norm after: 9.64323385462268
Epoch 8093/10000, Prediction Accuracy = 63.072%, Loss = 0.37812241315841677
Epoch: 8093, Batch Gradient Norm: 10.636727752664182
Epoch: 8093, Batch Gradient Norm after: 10.636727752664182
Epoch 8094/10000, Prediction Accuracy = 62.972%, Loss = 0.3848247706890106
Epoch: 8094, Batch Gradient Norm: 10.115941745995562
Epoch: 8094, Batch Gradient Norm after: 10.115941745995562
Epoch 8095/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.38011514544487
Epoch: 8095, Batch Gradient Norm: 10.44435438189983
Epoch: 8095, Batch Gradient Norm after: 10.44435438189983
Epoch 8096/10000, Prediction Accuracy = 62.836%, Loss = 0.3812114953994751
Epoch: 8096, Batch Gradient Norm: 11.418224902190712
Epoch: 8096, Batch Gradient Norm after: 11.418224902190712
Epoch 8097/10000, Prediction Accuracy = 62.96%, Loss = 0.3897793352603912
Epoch: 8097, Batch Gradient Norm: 10.026471636388834
Epoch: 8097, Batch Gradient Norm after: 10.026471636388834
Epoch 8098/10000, Prediction Accuracy = 63.052%, Loss = 0.3826373338699341
Epoch: 8098, Batch Gradient Norm: 8.551267300849279
Epoch: 8098, Batch Gradient Norm after: 8.551267300849279
Epoch 8099/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.37213234305381776
Epoch: 8099, Batch Gradient Norm: 10.475748915882493
Epoch: 8099, Batch Gradient Norm after: 10.475748915882493
Epoch 8100/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3839051783084869
Epoch: 8100, Batch Gradient Norm: 10.960292490936883
Epoch: 8100, Batch Gradient Norm after: 10.960292490936883
Epoch 8101/10000, Prediction Accuracy = 63.041999999999994%, Loss = 0.3888199210166931
Epoch: 8101, Batch Gradient Norm: 9.905931651306787
Epoch: 8101, Batch Gradient Norm after: 9.905931651306787
Epoch 8102/10000, Prediction Accuracy = 63.008%, Loss = 0.3816183924674988
Epoch: 8102, Batch Gradient Norm: 11.84806791819814
Epoch: 8102, Batch Gradient Norm after: 11.84806791819814
Epoch 8103/10000, Prediction Accuracy = 63.048%, Loss = 0.3935512125492096
Epoch: 8103, Batch Gradient Norm: 12.169162667406802
Epoch: 8103, Batch Gradient Norm after: 12.169162667406802
Epoch 8104/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.3939761400222778
Epoch: 8104, Batch Gradient Norm: 9.47880362726296
Epoch: 8104, Batch Gradient Norm after: 9.47880362726296
Epoch 8105/10000, Prediction Accuracy = 62.9%, Loss = 0.37533857822418215
Epoch: 8105, Batch Gradient Norm: 8.30391370406531
Epoch: 8105, Batch Gradient Norm after: 8.30391370406531
Epoch 8106/10000, Prediction Accuracy = 63.084%, Loss = 0.36860504150390627
Epoch: 8106, Batch Gradient Norm: 9.299422380761303
Epoch: 8106, Batch Gradient Norm after: 9.299422380761303
Epoch 8107/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.37411001324653625
Epoch: 8107, Batch Gradient Norm: 11.300888066300615
Epoch: 8107, Batch Gradient Norm after: 11.300888066300615
Epoch 8108/10000, Prediction Accuracy = 63.068%, Loss = 0.3895272433757782
Epoch: 8108, Batch Gradient Norm: 10.660571359550463
Epoch: 8108, Batch Gradient Norm after: 10.660571359550463
Epoch 8109/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.38516536355018616
Epoch: 8109, Batch Gradient Norm: 9.650690743611484
Epoch: 8109, Batch Gradient Norm after: 9.650690743611484
Epoch 8110/10000, Prediction Accuracy = 62.965999999999994%, Loss = 0.37862820029258726
Epoch: 8110, Batch Gradient Norm: 8.909990648014912
Epoch: 8110, Batch Gradient Norm after: 8.909990648014912
Epoch 8111/10000, Prediction Accuracy = 62.94%, Loss = 0.37441882491111755
Epoch: 8111, Batch Gradient Norm: 10.631688575200922
Epoch: 8111, Batch Gradient Norm after: 10.631688575200922
Epoch 8112/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.3865805506706238
Epoch: 8112, Batch Gradient Norm: 11.102930536763946
Epoch: 8112, Batch Gradient Norm after: 11.102930536763946
Epoch 8113/10000, Prediction Accuracy = 63.092%, Loss = 0.38965668678283694
Epoch: 8113, Batch Gradient Norm: 9.17965240969571
Epoch: 8113, Batch Gradient Norm after: 9.17965240969571
Epoch 8114/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.37492557168006896
Epoch: 8114, Batch Gradient Norm: 8.789748409814244
Epoch: 8114, Batch Gradient Norm after: 8.789748409814244
Epoch 8115/10000, Prediction Accuracy = 63.112%, Loss = 0.3707736313343048
Epoch: 8115, Batch Gradient Norm: 9.98321046744099
Epoch: 8115, Batch Gradient Norm after: 9.98321046744099
Epoch 8116/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.37780038118362425
Epoch: 8116, Batch Gradient Norm: 11.576635176753141
Epoch: 8116, Batch Gradient Norm after: 11.576635176753141
Epoch 8117/10000, Prediction Accuracy = 63.0%, Loss = 0.39102311730384826
Epoch: 8117, Batch Gradient Norm: 10.725386449812335
Epoch: 8117, Batch Gradient Norm after: 10.725386449812335
Epoch 8118/10000, Prediction Accuracy = 63.064%, Loss = 0.3887496292591095
Epoch: 8118, Batch Gradient Norm: 8.788069563716352
Epoch: 8118, Batch Gradient Norm after: 8.788069563716352
Epoch 8119/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3756072759628296
Epoch: 8119, Batch Gradient Norm: 8.754658411939495
Epoch: 8119, Batch Gradient Norm after: 8.754658411939495
Epoch 8120/10000, Prediction Accuracy = 63.062%, Loss = 0.37366063594818116
Epoch: 8120, Batch Gradient Norm: 10.369761338711706
Epoch: 8120, Batch Gradient Norm after: 10.369761338711706
Epoch 8121/10000, Prediction Accuracy = 63.116%, Loss = 0.38383389115333555
Epoch: 8121, Batch Gradient Norm: 10.043180019735969
Epoch: 8121, Batch Gradient Norm after: 10.043180019735969
Epoch 8122/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.38133288025856016
Epoch: 8122, Batch Gradient Norm: 9.855956076723482
Epoch: 8122, Batch Gradient Norm after: 9.855956076723482
Epoch 8123/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.37908969521522523
Epoch: 8123, Batch Gradient Norm: 11.774882693395833
Epoch: 8123, Batch Gradient Norm after: 11.774882693395833
Epoch 8124/10000, Prediction Accuracy = 62.996%, Loss = 0.3927920460700989
Epoch: 8124, Batch Gradient Norm: 11.62207677869314
Epoch: 8124, Batch Gradient Norm after: 11.62207677869314
Epoch 8125/10000, Prediction Accuracy = 62.99000000000001%, Loss = 0.3919604480266571
Epoch: 8125, Batch Gradient Norm: 10.27949965303674
Epoch: 8125, Batch Gradient Norm after: 10.27949965303674
Epoch 8126/10000, Prediction Accuracy = 63.062%, Loss = 0.3825159430503845
Epoch: 8126, Batch Gradient Norm: 9.222622808209652
Epoch: 8126, Batch Gradient Norm after: 9.222622808209652
Epoch 8127/10000, Prediction Accuracy = 62.983999999999995%, Loss = 0.37626160979270934
Epoch: 8127, Batch Gradient Norm: 8.344295210392271
Epoch: 8127, Batch Gradient Norm after: 8.344295210392271
Epoch 8128/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.37029539942741396
Epoch: 8128, Batch Gradient Norm: 9.259150116451613
Epoch: 8128, Batch Gradient Norm after: 9.259150116451613
Epoch 8129/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.37426418662071226
Epoch: 8129, Batch Gradient Norm: 12.91294912822749
Epoch: 8129, Batch Gradient Norm after: 12.91294912822749
Epoch 8130/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3996328592300415
Epoch: 8130, Batch Gradient Norm: 13.424389462072025
Epoch: 8130, Batch Gradient Norm after: 13.424389462072025
Epoch 8131/10000, Prediction Accuracy = 62.98199999999999%, Loss = 0.4055093228816986
Epoch: 8131, Batch Gradient Norm: 10.070094693665522
Epoch: 8131, Batch Gradient Norm after: 10.070094693665522
Epoch 8132/10000, Prediction Accuracy = 62.86800000000001%, Loss = 0.38172723054885865
Epoch: 8132, Batch Gradient Norm: 7.3246986257973505
Epoch: 8132, Batch Gradient Norm after: 7.3246986257973505
Epoch 8133/10000, Prediction Accuracy = 63.112%, Loss = 0.36586185693740847
Epoch: 8133, Batch Gradient Norm: 7.038500804329651
Epoch: 8133, Batch Gradient Norm after: 7.038500804329651
Epoch 8134/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.36353098750114443
Epoch: 8134, Batch Gradient Norm: 8.688851835678815
Epoch: 8134, Batch Gradient Norm after: 8.688851835678815
Epoch 8135/10000, Prediction Accuracy = 62.998000000000005%, Loss = 0.37140374183654784
Epoch: 8135, Batch Gradient Norm: 11.805240315771758
Epoch: 8135, Batch Gradient Norm after: 11.805240315771758
Epoch 8136/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3916490077972412
Epoch: 8136, Batch Gradient Norm: 13.214129227373663
Epoch: 8136, Batch Gradient Norm after: 13.214129227373663
Epoch 8137/10000, Prediction Accuracy = 62.90599999999999%, Loss = 0.40592882633209226
Epoch: 8137, Batch Gradient Norm: 10.53971277576996
Epoch: 8137, Batch Gradient Norm after: 10.53971277576996
Epoch 8138/10000, Prediction Accuracy = 62.842%, Loss = 0.38537817597389223
Epoch: 8138, Batch Gradient Norm: 7.788895323855286
Epoch: 8138, Batch Gradient Norm after: 7.788895323855286
Epoch 8139/10000, Prediction Accuracy = 63.05%, Loss = 0.3681498408317566
Epoch: 8139, Batch Gradient Norm: 6.953457099131753
Epoch: 8139, Batch Gradient Norm after: 6.953457099131753
Epoch 8140/10000, Prediction Accuracy = 63.15%, Loss = 0.3628310263156891
Epoch: 8140, Batch Gradient Norm: 9.469106254644013
Epoch: 8140, Batch Gradient Norm after: 9.469106254644013
Epoch 8141/10000, Prediction Accuracy = 62.972%, Loss = 0.3766492187976837
Epoch: 8141, Batch Gradient Norm: 11.817849869399305
Epoch: 8141, Batch Gradient Norm after: 11.817849869399305
Epoch 8142/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.39335557222366335
Epoch: 8142, Batch Gradient Norm: 11.251847923377799
Epoch: 8142, Batch Gradient Norm after: 11.251847923377799
Epoch 8143/10000, Prediction Accuracy = 63.028%, Loss = 0.3886751174926758
Epoch: 8143, Batch Gradient Norm: 9.179083587019221
Epoch: 8143, Batch Gradient Norm after: 9.179083587019221
Epoch 8144/10000, Prediction Accuracy = 62.980000000000004%, Loss = 0.3729259669780731
Epoch: 8144, Batch Gradient Norm: 8.46550633508967
Epoch: 8144, Batch Gradient Norm after: 8.46550633508967
Epoch 8145/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.3690226078033447
Epoch: 8145, Batch Gradient Norm: 8.432572749543546
Epoch: 8145, Batch Gradient Norm after: 8.432572749543546
Epoch 8146/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.3695826709270477
Epoch: 8146, Batch Gradient Norm: 10.131944321986287
Epoch: 8146, Batch Gradient Norm after: 10.131944321986287
Epoch 8147/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3823121964931488
Epoch: 8147, Batch Gradient Norm: 11.36959168552872
Epoch: 8147, Batch Gradient Norm after: 11.36959168552872
Epoch 8148/10000, Prediction Accuracy = 62.94199999999999%, Loss = 0.39237394332885744
Epoch: 8148, Batch Gradient Norm: 11.119959910006198
Epoch: 8148, Batch Gradient Norm after: 11.119959910006198
Epoch 8149/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3894012331962585
Epoch: 8149, Batch Gradient Norm: 10.232946875253255
Epoch: 8149, Batch Gradient Norm after: 10.232946875253255
Epoch 8150/10000, Prediction Accuracy = 63.148%, Loss = 0.3815470516681671
Epoch: 8150, Batch Gradient Norm: 10.588244102319724
Epoch: 8150, Batch Gradient Norm after: 10.588244102319724
Epoch 8151/10000, Prediction Accuracy = 62.916%, Loss = 0.38389797806739806
Epoch: 8151, Batch Gradient Norm: 11.172579046434793
Epoch: 8151, Batch Gradient Norm after: 11.172579046434793
Epoch 8152/10000, Prediction Accuracy = 63.104%, Loss = 0.3892196476459503
Epoch: 8152, Batch Gradient Norm: 10.157548804835274
Epoch: 8152, Batch Gradient Norm after: 10.157548804835274
Epoch 8153/10000, Prediction Accuracy = 62.902%, Loss = 0.3810844600200653
Epoch: 8153, Batch Gradient Norm: 11.443394022971004
Epoch: 8153, Batch Gradient Norm after: 11.443394022971004
Epoch 8154/10000, Prediction Accuracy = 62.95399999999999%, Loss = 0.3881612479686737
Epoch: 8154, Batch Gradient Norm: 12.540195836027566
Epoch: 8154, Batch Gradient Norm after: 12.540195836027566
Epoch 8155/10000, Prediction Accuracy = 62.9%, Loss = 0.39672943353652956
Epoch: 8155, Batch Gradient Norm: 10.411860477399216
Epoch: 8155, Batch Gradient Norm after: 10.411860477399216
Epoch 8156/10000, Prediction Accuracy = 63.114%, Loss = 0.3827854752540588
Epoch: 8156, Batch Gradient Norm: 9.339902978442803
Epoch: 8156, Batch Gradient Norm after: 9.339902978442803
Epoch 8157/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.3767297029495239
Epoch: 8157, Batch Gradient Norm: 9.281563870267268
Epoch: 8157, Batch Gradient Norm after: 9.281563870267268
Epoch 8158/10000, Prediction Accuracy = 62.918000000000006%, Loss = 0.37519108653068545
Epoch: 8158, Batch Gradient Norm: 10.870020616878717
Epoch: 8158, Batch Gradient Norm after: 10.870020616878717
Epoch 8159/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.38605305552482605
Epoch: 8159, Batch Gradient Norm: 10.557957502319665
Epoch: 8159, Batch Gradient Norm after: 10.557957502319665
Epoch 8160/10000, Prediction Accuracy = 63.122%, Loss = 0.3836905539035797
Epoch: 8160, Batch Gradient Norm: 10.133096981615004
Epoch: 8160, Batch Gradient Norm after: 10.133096981615004
Epoch 8161/10000, Prediction Accuracy = 63.128%, Loss = 0.3793930530548096
Epoch: 8161, Batch Gradient Norm: 10.060557953214113
Epoch: 8161, Batch Gradient Norm after: 10.060557953214113
Epoch 8162/10000, Prediction Accuracy = 63.146%, Loss = 0.3785530149936676
Epoch: 8162, Batch Gradient Norm: 9.210002036284038
Epoch: 8162, Batch Gradient Norm after: 9.210002036284038
Epoch 8163/10000, Prediction Accuracy = 63.10999999999999%, Loss = 0.37358266711235044
Epoch: 8163, Batch Gradient Norm: 8.422657186997183
Epoch: 8163, Batch Gradient Norm after: 8.422657186997183
Epoch 8164/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.3694886386394501
Epoch: 8164, Batch Gradient Norm: 8.881579107815686
Epoch: 8164, Batch Gradient Norm after: 8.881579107815686
Epoch 8165/10000, Prediction Accuracy = 63.126%, Loss = 0.3729957342147827
Epoch: 8165, Batch Gradient Norm: 10.18638539160736
Epoch: 8165, Batch Gradient Norm after: 10.18638539160736
Epoch 8166/10000, Prediction Accuracy = 63.152%, Loss = 0.381085604429245
Epoch: 8166, Batch Gradient Norm: 11.00416729907604
Epoch: 8166, Batch Gradient Norm after: 11.00416729907604
Epoch 8167/10000, Prediction Accuracy = 63.074%, Loss = 0.3858153998851776
Epoch: 8167, Batch Gradient Norm: 10.720296237494038
Epoch: 8167, Batch Gradient Norm after: 10.720296237494038
Epoch 8168/10000, Prediction Accuracy = 62.903999999999996%, Loss = 0.38285830020904543
Epoch: 8168, Batch Gradient Norm: 10.284264281262647
Epoch: 8168, Batch Gradient Norm after: 10.284264281262647
Epoch 8169/10000, Prediction Accuracy = 62.898%, Loss = 0.3818857192993164
Epoch: 8169, Batch Gradient Norm: 8.803870310429229
Epoch: 8169, Batch Gradient Norm after: 8.803870310429229
Epoch 8170/10000, Prediction Accuracy = 63.122%, Loss = 0.37416401505470276
Epoch: 8170, Batch Gradient Norm: 8.651582423592165
Epoch: 8170, Batch Gradient Norm after: 8.651582423592165
Epoch 8171/10000, Prediction Accuracy = 63.08%, Loss = 0.373427814245224
Epoch: 8171, Batch Gradient Norm: 10.111995958115237
Epoch: 8171, Batch Gradient Norm after: 10.111995958115237
Epoch 8172/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3817972242832184
Epoch: 8172, Batch Gradient Norm: 11.44220391518472
Epoch: 8172, Batch Gradient Norm after: 11.44220391518472
Epoch 8173/10000, Prediction Accuracy = 63.122%, Loss = 0.3911083936691284
Epoch: 8173, Batch Gradient Norm: 10.83599648340703
Epoch: 8173, Batch Gradient Norm after: 10.83599648340703
Epoch 8174/10000, Prediction Accuracy = 62.992%, Loss = 0.3851348876953125
Epoch: 8174, Batch Gradient Norm: 10.748528546512494
Epoch: 8174, Batch Gradient Norm after: 10.748528546512494
Epoch 8175/10000, Prediction Accuracy = 63.14%, Loss = 0.38333595395088194
Epoch: 8175, Batch Gradient Norm: 10.82781482364486
Epoch: 8175, Batch Gradient Norm after: 10.82781482364486
Epoch 8176/10000, Prediction Accuracy = 62.984%, Loss = 0.3839049756526947
Epoch: 8176, Batch Gradient Norm: 10.542517632318901
Epoch: 8176, Batch Gradient Norm after: 10.542517632318901
Epoch 8177/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.38210976123809814
Epoch: 8177, Batch Gradient Norm: 10.246122850532029
Epoch: 8177, Batch Gradient Norm after: 10.246122850532029
Epoch 8178/10000, Prediction Accuracy = 63.048%, Loss = 0.3795216977596283
Epoch: 8178, Batch Gradient Norm: 10.200812749005188
Epoch: 8178, Batch Gradient Norm after: 10.200812749005188
Epoch 8179/10000, Prediction Accuracy = 63.019999999999996%, Loss = 0.3789604663848877
Epoch: 8179, Batch Gradient Norm: 10.191551030633178
Epoch: 8179, Batch Gradient Norm after: 10.191551030633178
Epoch 8180/10000, Prediction Accuracy = 63.028%, Loss = 0.37876508235931394
Epoch: 8180, Batch Gradient Norm: 10.828010975657204
Epoch: 8180, Batch Gradient Norm after: 10.828010975657204
Epoch 8181/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.3847422242164612
Epoch: 8181, Batch Gradient Norm: 10.289821159162111
Epoch: 8181, Batch Gradient Norm after: 10.289821159162111
Epoch 8182/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.38263897895812987
Epoch: 8182, Batch Gradient Norm: 9.253702072541733
Epoch: 8182, Batch Gradient Norm after: 9.253702072541733
Epoch 8183/10000, Prediction Accuracy = 63.092%, Loss = 0.37663914561271666
Epoch: 8183, Batch Gradient Norm: 8.46357245713108
Epoch: 8183, Batch Gradient Norm after: 8.46357245713108
Epoch 8184/10000, Prediction Accuracy = 63.072%, Loss = 0.37150794863700864
Epoch: 8184, Batch Gradient Norm: 9.45141390585782
Epoch: 8184, Batch Gradient Norm after: 9.45141390585782
Epoch 8185/10000, Prediction Accuracy = 63.062%, Loss = 0.37733718752861023
Epoch: 8185, Batch Gradient Norm: 11.609791667485016
Epoch: 8185, Batch Gradient Norm after: 11.609791667485016
Epoch 8186/10000, Prediction Accuracy = 63.08%, Loss = 0.3935296833515167
Epoch: 8186, Batch Gradient Norm: 10.80332666262592
Epoch: 8186, Batch Gradient Norm after: 10.80332666262592
Epoch 8187/10000, Prediction Accuracy = 63.062%, Loss = 0.3875931441783905
Epoch: 8187, Batch Gradient Norm: 9.60670326612628
Epoch: 8187, Batch Gradient Norm after: 9.60670326612628
Epoch 8188/10000, Prediction Accuracy = 63.124%, Loss = 0.376915180683136
Epoch: 8188, Batch Gradient Norm: 9.741100093130573
Epoch: 8188, Batch Gradient Norm after: 9.741100093130573
Epoch 8189/10000, Prediction Accuracy = 62.846000000000004%, Loss = 0.37693899869918823
Epoch: 8189, Batch Gradient Norm: 9.373671361444865
Epoch: 8189, Batch Gradient Norm after: 9.373671361444865
Epoch 8190/10000, Prediction Accuracy = 62.96999999999999%, Loss = 0.37530089616775514
Epoch: 8190, Batch Gradient Norm: 9.2749229130218
Epoch: 8190, Batch Gradient Norm after: 9.2749229130218
Epoch 8191/10000, Prediction Accuracy = 62.95799999999999%, Loss = 0.3748069405555725
Epoch: 8191, Batch Gradient Norm: 11.011584966553379
Epoch: 8191, Batch Gradient Norm after: 11.011584966553379
Epoch 8192/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.38590956330299375
Epoch: 8192, Batch Gradient Norm: 12.195794727471588
Epoch: 8192, Batch Gradient Norm after: 12.195794727471588
Epoch 8193/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.394371098279953
Epoch: 8193, Batch Gradient Norm: 10.90344228656762
Epoch: 8193, Batch Gradient Norm after: 10.90344228656762
Epoch 8194/10000, Prediction Accuracy = 63.102%, Loss = 0.3850847542285919
Epoch: 8194, Batch Gradient Norm: 8.965952392670761
Epoch: 8194, Batch Gradient Norm after: 8.965952392670761
Epoch 8195/10000, Prediction Accuracy = 63.004000000000005%, Loss = 0.37293016314506533
Epoch: 8195, Batch Gradient Norm: 8.452475954807559
Epoch: 8195, Batch Gradient Norm after: 8.452475954807559
Epoch 8196/10000, Prediction Accuracy = 62.971999999999994%, Loss = 0.3695279836654663
Epoch: 8196, Batch Gradient Norm: 9.206954463625024
Epoch: 8196, Batch Gradient Norm after: 9.206954463625024
Epoch 8197/10000, Prediction Accuracy = 63.012%, Loss = 0.3739148020744324
Epoch: 8197, Batch Gradient Norm: 9.121713289486882
Epoch: 8197, Batch Gradient Norm after: 9.121713289486882
Epoch 8198/10000, Prediction Accuracy = 63.098%, Loss = 0.37322173118591306
Epoch: 8198, Batch Gradient Norm: 9.901698901329347
Epoch: 8198, Batch Gradient Norm after: 9.901698901329347
Epoch 8199/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.3772185564041138
Epoch: 8199, Batch Gradient Norm: 10.912847170729117
Epoch: 8199, Batch Gradient Norm after: 10.912847170729117
Epoch 8200/10000, Prediction Accuracy = 63.016000000000005%, Loss = 0.3841866314411163
Epoch: 8200, Batch Gradient Norm: 11.794548230178664
Epoch: 8200, Batch Gradient Norm after: 11.794548230178664
Epoch 8201/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.39276996850967405
Epoch: 8201, Batch Gradient Norm: 10.308111527160248
Epoch: 8201, Batch Gradient Norm after: 10.308111527160248
Epoch 8202/10000, Prediction Accuracy = 63.056%, Loss = 0.3832849860191345
Epoch: 8202, Batch Gradient Norm: 9.536942941427705
Epoch: 8202, Batch Gradient Norm after: 9.536942941427705
Epoch 8203/10000, Prediction Accuracy = 62.967999999999996%, Loss = 0.37654985785484313
Epoch: 8203, Batch Gradient Norm: 11.797442871059195
Epoch: 8203, Batch Gradient Norm after: 11.797442871059195
Epoch 8204/10000, Prediction Accuracy = 62.94%, Loss = 0.39044328927993777
Epoch: 8204, Batch Gradient Norm: 12.63580767681585
Epoch: 8204, Batch Gradient Norm after: 12.63580767681585
Epoch 8205/10000, Prediction Accuracy = 62.974000000000004%, Loss = 0.39630928039550783
Epoch: 8205, Batch Gradient Norm: 11.455554571765447
Epoch: 8205, Batch Gradient Norm after: 11.455554571765447
Epoch 8206/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3887250363826752
Epoch: 8206, Batch Gradient Norm: 10.031594517232326
Epoch: 8206, Batch Gradient Norm after: 10.031594517232326
Epoch 8207/10000, Prediction Accuracy = 63.15%, Loss = 0.38023780584335326
Epoch: 8207, Batch Gradient Norm: 8.242229180874588
Epoch: 8207, Batch Gradient Norm after: 8.242229180874588
Epoch 8208/10000, Prediction Accuracy = 63.056%, Loss = 0.368549782037735
Epoch: 8208, Batch Gradient Norm: 8.562379814356138
Epoch: 8208, Batch Gradient Norm after: 8.562379814356138
Epoch 8209/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.36962666511535647
Epoch: 8209, Batch Gradient Norm: 10.213730051803832
Epoch: 8209, Batch Gradient Norm after: 10.213730051803832
Epoch 8210/10000, Prediction Accuracy = 63.128%, Loss = 0.3797191560268402
Epoch: 8210, Batch Gradient Norm: 10.460331036419193
Epoch: 8210, Batch Gradient Norm after: 10.460331036419193
Epoch 8211/10000, Prediction Accuracy = 62.848%, Loss = 0.3825382173061371
Epoch: 8211, Batch Gradient Norm: 8.465474815274852
Epoch: 8211, Batch Gradient Norm after: 8.465474815274852
Epoch 8212/10000, Prediction Accuracy = 62.91199999999999%, Loss = 0.3697237968444824
Epoch: 8212, Batch Gradient Norm: 9.13410662514851
Epoch: 8212, Batch Gradient Norm after: 9.13410662514851
Epoch 8213/10000, Prediction Accuracy = 63.1%, Loss = 0.3720775246620178
Epoch: 8213, Batch Gradient Norm: 10.56733197008757
Epoch: 8213, Batch Gradient Norm after: 10.56733197008757
Epoch 8214/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.3835750758647919
Epoch: 8214, Batch Gradient Norm: 8.966021768757749
Epoch: 8214, Batch Gradient Norm after: 8.966021768757749
Epoch 8215/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.37355388402938844
Epoch: 8215, Batch Gradient Norm: 8.20042089622013
Epoch: 8215, Batch Gradient Norm after: 8.20042089622013
Epoch 8216/10000, Prediction Accuracy = 63.10200000000001%, Loss = 0.3684132516384125
Epoch: 8216, Batch Gradient Norm: 9.808712324284668
Epoch: 8216, Batch Gradient Norm after: 9.808712324284668
Epoch 8217/10000, Prediction Accuracy = 63.032%, Loss = 0.37829841375350953
Epoch: 8217, Batch Gradient Norm: 12.55610847771482
Epoch: 8217, Batch Gradient Norm after: 12.55610847771482
Epoch 8218/10000, Prediction Accuracy = 63.088%, Loss = 0.3993587136268616
Epoch: 8218, Batch Gradient Norm: 11.008208120952377
Epoch: 8218, Batch Gradient Norm after: 11.008208120952377
Epoch 8219/10000, Prediction Accuracy = 63.062%, Loss = 0.3873614609241486
Epoch: 8219, Batch Gradient Norm: 9.03394009443954
Epoch: 8219, Batch Gradient Norm after: 9.03394009443954
Epoch 8220/10000, Prediction Accuracy = 63.186%, Loss = 0.3717978775501251
Epoch: 8220, Batch Gradient Norm: 10.421959844463899
Epoch: 8220, Batch Gradient Norm after: 10.421959844463899
Epoch 8221/10000, Prediction Accuracy = 62.962%, Loss = 0.37951422333717344
Epoch: 8221, Batch Gradient Norm: 10.968988126102536
Epoch: 8221, Batch Gradient Norm after: 10.968988126102536
Epoch 8222/10000, Prediction Accuracy = 62.879999999999995%, Loss = 0.3851624608039856
Epoch: 8222, Batch Gradient Norm: 10.48643690675162
Epoch: 8222, Batch Gradient Norm after: 10.48643690675162
Epoch 8223/10000, Prediction Accuracy = 62.934000000000005%, Loss = 0.3819994330406189
Epoch: 8223, Batch Gradient Norm: 10.76282155497619
Epoch: 8223, Batch Gradient Norm after: 10.76282155497619
Epoch 8224/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.38388078212738036
Epoch: 8224, Batch Gradient Norm: 11.604480099618247
Epoch: 8224, Batch Gradient Norm after: 11.604480099618247
Epoch 8225/10000, Prediction Accuracy = 62.878%, Loss = 0.3905389130115509
Epoch: 8225, Batch Gradient Norm: 11.983186827257644
Epoch: 8225, Batch Gradient Norm after: 11.983186827257644
Epoch 8226/10000, Prediction Accuracy = 62.944%, Loss = 0.39333197474479675
Epoch: 8226, Batch Gradient Norm: 10.520096036401036
Epoch: 8226, Batch Gradient Norm after: 10.520096036401036
Epoch 8227/10000, Prediction Accuracy = 63.166%, Loss = 0.3822334289550781
Epoch: 8227, Batch Gradient Norm: 9.17529525410068
Epoch: 8227, Batch Gradient Norm after: 9.17529525410068
Epoch 8228/10000, Prediction Accuracy = 63.10600000000001%, Loss = 0.3730496883392334
Epoch: 8228, Batch Gradient Norm: 10.09771621340337
Epoch: 8228, Batch Gradient Norm after: 10.09771621340337
Epoch 8229/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.3799218416213989
Epoch: 8229, Batch Gradient Norm: 10.45800281112883
Epoch: 8229, Batch Gradient Norm after: 10.45800281112883
Epoch 8230/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3818130731582642
Epoch: 8230, Batch Gradient Norm: 10.367495339128787
Epoch: 8230, Batch Gradient Norm after: 10.367495339128787
Epoch 8231/10000, Prediction Accuracy = 63.152%, Loss = 0.3804416358470917
Epoch: 8231, Batch Gradient Norm: 9.87855016208758
Epoch: 8231, Batch Gradient Norm after: 9.87855016208758
Epoch 8232/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.3770825624465942
Epoch: 8232, Batch Gradient Norm: 9.683239314931399
Epoch: 8232, Batch Gradient Norm after: 9.683239314931399
Epoch 8233/10000, Prediction Accuracy = 63.148%, Loss = 0.37654441595077515
Epoch: 8233, Batch Gradient Norm: 10.004589941698237
Epoch: 8233, Batch Gradient Norm after: 10.004589941698237
Epoch 8234/10000, Prediction Accuracy = 63.124%, Loss = 0.37936044931411744
Epoch: 8234, Batch Gradient Norm: 9.587039481201474
Epoch: 8234, Batch Gradient Norm after: 9.587039481201474
Epoch 8235/10000, Prediction Accuracy = 63.0%, Loss = 0.374864262342453
Epoch: 8235, Batch Gradient Norm: 8.78022927770049
Epoch: 8235, Batch Gradient Norm after: 8.78022927770049
Epoch 8236/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.36907808780670165
Epoch: 8236, Batch Gradient Norm: 9.7253240521709
Epoch: 8236, Batch Gradient Norm after: 9.7253240521709
Epoch 8237/10000, Prediction Accuracy = 63.068%, Loss = 0.37400583624839784
Epoch: 8237, Batch Gradient Norm: 12.474644345383572
Epoch: 8237, Batch Gradient Norm after: 12.474644345383572
Epoch 8238/10000, Prediction Accuracy = 63.052%, Loss = 0.3944903314113617
Epoch: 8238, Batch Gradient Norm: 12.563114400359982
Epoch: 8238, Batch Gradient Norm after: 12.563114400359982
Epoch 8239/10000, Prediction Accuracy = 63.114%, Loss = 0.3982682228088379
Epoch: 8239, Batch Gradient Norm: 9.713503206303429
Epoch: 8239, Batch Gradient Norm after: 9.713503206303429
Epoch 8240/10000, Prediction Accuracy = 63.153999999999996%, Loss = 0.37856030464172363
Epoch: 8240, Batch Gradient Norm: 8.392100622756246
Epoch: 8240, Batch Gradient Norm after: 8.392100622756246
Epoch 8241/10000, Prediction Accuracy = 63.098%, Loss = 0.36954517364501954
Epoch: 8241, Batch Gradient Norm: 9.057717157744051
Epoch: 8241, Batch Gradient Norm after: 9.057717157744051
Epoch 8242/10000, Prediction Accuracy = 63.04599999999999%, Loss = 0.37336245775222776
Epoch: 8242, Batch Gradient Norm: 9.373243139481144
Epoch: 8242, Batch Gradient Norm after: 9.373243139481144
Epoch 8243/10000, Prediction Accuracy = 63.098%, Loss = 0.37516475319862364
Epoch: 8243, Batch Gradient Norm: 9.746436429184632
Epoch: 8243, Batch Gradient Norm after: 9.746436429184632
Epoch 8244/10000, Prediction Accuracy = 63.122%, Loss = 0.3767582595348358
Epoch: 8244, Batch Gradient Norm: 10.224561184319157
Epoch: 8244, Batch Gradient Norm after: 10.224561184319157
Epoch 8245/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.37999373078346255
Epoch: 8245, Batch Gradient Norm: 10.241300320683212
Epoch: 8245, Batch Gradient Norm after: 10.241300320683212
Epoch 8246/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.380250608921051
Epoch: 8246, Batch Gradient Norm: 10.743122406353448
Epoch: 8246, Batch Gradient Norm after: 10.743122406353448
Epoch 8247/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.3840814232826233
Epoch: 8247, Batch Gradient Norm: 11.398628963125926
Epoch: 8247, Batch Gradient Norm after: 11.398628963125926
Epoch 8248/10000, Prediction Accuracy = 62.916%, Loss = 0.38885318040847777
Epoch: 8248, Batch Gradient Norm: 10.192239135166512
Epoch: 8248, Batch Gradient Norm after: 10.192239135166512
Epoch 8249/10000, Prediction Accuracy = 63.022000000000006%, Loss = 0.37934974431991575
Epoch: 8249, Batch Gradient Norm: 9.135495559860113
Epoch: 8249, Batch Gradient Norm after: 9.135495559860113
Epoch 8250/10000, Prediction Accuracy = 63.068%, Loss = 0.37334722876548765
Epoch: 8250, Batch Gradient Norm: 8.744688028063363
Epoch: 8250, Batch Gradient Norm after: 8.744688028063363
Epoch 8251/10000, Prediction Accuracy = 63.116%, Loss = 0.3721937894821167
Epoch: 8251, Batch Gradient Norm: 9.676897575732866
Epoch: 8251, Batch Gradient Norm after: 9.676897575732866
Epoch 8252/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.3775705575942993
Epoch: 8252, Batch Gradient Norm: 12.25815999396877
Epoch: 8252, Batch Gradient Norm after: 12.25815999396877
Epoch 8253/10000, Prediction Accuracy = 63.138%, Loss = 0.3947016656398773
Epoch: 8253, Batch Gradient Norm: 12.075580870907508
Epoch: 8253, Batch Gradient Norm after: 12.075580870907508
Epoch 8254/10000, Prediction Accuracy = 63.088%, Loss = 0.3923361778259277
Epoch: 8254, Batch Gradient Norm: 9.521652371625626
Epoch: 8254, Batch Gradient Norm after: 9.521652371625626
Epoch 8255/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.37418380975723264
Epoch: 8255, Batch Gradient Norm: 9.084817910128308
Epoch: 8255, Batch Gradient Norm after: 9.084817910128308
Epoch 8256/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.371962296962738
Epoch: 8256, Batch Gradient Norm: 10.575127430517089
Epoch: 8256, Batch Gradient Norm after: 10.575127430517089
Epoch 8257/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3819404900074005
Epoch: 8257, Batch Gradient Norm: 10.304653616781039
Epoch: 8257, Batch Gradient Norm after: 10.304653616781039
Epoch 8258/10000, Prediction Accuracy = 63.029999999999994%, Loss = 0.3808658838272095
Epoch: 8258, Batch Gradient Norm: 9.81091478379787
Epoch: 8258, Batch Gradient Norm after: 9.81091478379787
Epoch 8259/10000, Prediction Accuracy = 62.894000000000005%, Loss = 0.37763614058494566
Epoch: 8259, Batch Gradient Norm: 8.72372686136478
Epoch: 8259, Batch Gradient Norm after: 8.72372686136478
Epoch 8260/10000, Prediction Accuracy = 63.108000000000004%, Loss = 0.37079536318778994
Epoch: 8260, Batch Gradient Norm: 10.363435230532884
Epoch: 8260, Batch Gradient Norm after: 10.363435230532884
Epoch 8261/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.38024760484695436
Epoch: 8261, Batch Gradient Norm: 11.843558399827465
Epoch: 8261, Batch Gradient Norm after: 11.843558399827465
Epoch 8262/10000, Prediction Accuracy = 63.17199999999999%, Loss = 0.3906176269054413
Epoch: 8262, Batch Gradient Norm: 10.659451426112291
Epoch: 8262, Batch Gradient Norm after: 10.659451426112291
Epoch 8263/10000, Prediction Accuracy = 63.16600000000001%, Loss = 0.38073107600212097
Epoch: 8263, Batch Gradient Norm: 9.371422791225953
Epoch: 8263, Batch Gradient Norm after: 9.371422791225953
Epoch 8264/10000, Prediction Accuracy = 63.148%, Loss = 0.37237985134124757
Epoch: 8264, Batch Gradient Norm: 10.072614268265895
Epoch: 8264, Batch Gradient Norm after: 10.072614268265895
Epoch 8265/10000, Prediction Accuracy = 63.052%, Loss = 0.3778929352760315
Epoch: 8265, Batch Gradient Norm: 11.003283904129672
Epoch: 8265, Batch Gradient Norm after: 11.003283904129672
Epoch 8266/10000, Prediction Accuracy = 62.992000000000004%, Loss = 0.3841450810432434
Epoch: 8266, Batch Gradient Norm: 9.56064155490584
Epoch: 8266, Batch Gradient Norm after: 9.56064155490584
Epoch 8267/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3744245648384094
Epoch: 8267, Batch Gradient Norm: 8.043333972385431
Epoch: 8267, Batch Gradient Norm after: 8.043333972385431
Epoch 8268/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.36473560333251953
Epoch: 8268, Batch Gradient Norm: 9.290304396514456
Epoch: 8268, Batch Gradient Norm after: 9.290304396514456
Epoch 8269/10000, Prediction Accuracy = 63.064%, Loss = 0.3720012724399567
Epoch: 8269, Batch Gradient Norm: 11.668920340804391
Epoch: 8269, Batch Gradient Norm after: 11.668920340804391
Epoch 8270/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3910240113735199
Epoch: 8270, Batch Gradient Norm: 11.086144691414253
Epoch: 8270, Batch Gradient Norm after: 11.086144691414253
Epoch 8271/10000, Prediction Accuracy = 62.988%, Loss = 0.3882195770740509
Epoch: 8271, Batch Gradient Norm: 9.99327076569876
Epoch: 8271, Batch Gradient Norm after: 9.99327076569876
Epoch 8272/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.3799233973026276
Epoch: 8272, Batch Gradient Norm: 9.64227612036175
Epoch: 8272, Batch Gradient Norm after: 9.64227612036175
Epoch 8273/10000, Prediction Accuracy = 63.128%, Loss = 0.3749440789222717
Epoch: 8273, Batch Gradient Norm: 10.960825984785602
Epoch: 8273, Batch Gradient Norm after: 10.960825984785602
Epoch 8274/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.38282052874565126
Epoch: 8274, Batch Gradient Norm: 10.097060396961433
Epoch: 8274, Batch Gradient Norm after: 10.097060396961433
Epoch 8275/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.37859697341918946
Epoch: 8275, Batch Gradient Norm: 9.284896264456934
Epoch: 8275, Batch Gradient Norm after: 9.284896264456934
Epoch 8276/10000, Prediction Accuracy = 63.007999999999996%, Loss = 0.37305837869644165
Epoch: 8276, Batch Gradient Norm: 11.116503369965741
Epoch: 8276, Batch Gradient Norm after: 11.116503369965741
Epoch 8277/10000, Prediction Accuracy = 63.0%, Loss = 0.3862984240055084
Epoch: 8277, Batch Gradient Norm: 11.231310896468452
Epoch: 8277, Batch Gradient Norm after: 11.231310896468452
Epoch 8278/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.38670364022254944
Epoch: 8278, Batch Gradient Norm: 9.417040810859028
Epoch: 8278, Batch Gradient Norm after: 9.417040810859028
Epoch 8279/10000, Prediction Accuracy = 63.14%, Loss = 0.37336901426315305
Epoch: 8279, Batch Gradient Norm: 8.500877453819617
Epoch: 8279, Batch Gradient Norm after: 8.500877453819617
Epoch 8280/10000, Prediction Accuracy = 63.116%, Loss = 0.36749905347824097
Epoch: 8280, Batch Gradient Norm: 9.621846893516858
Epoch: 8280, Batch Gradient Norm after: 9.621846893516858
Epoch 8281/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3744250893592834
Epoch: 8281, Batch Gradient Norm: 11.342390384066919
Epoch: 8281, Batch Gradient Norm after: 11.342390384066919
Epoch 8282/10000, Prediction Accuracy = 63.07399999999999%, Loss = 0.388371342420578
Epoch: 8282, Batch Gradient Norm: 10.793114680715915
Epoch: 8282, Batch Gradient Norm after: 10.793114680715915
Epoch 8283/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.38513603806495667
Epoch: 8283, Batch Gradient Norm: 9.82206576032281
Epoch: 8283, Batch Gradient Norm after: 9.82206576032281
Epoch 8284/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.37835243344306946
Epoch: 8284, Batch Gradient Norm: 10.89065515923107
Epoch: 8284, Batch Gradient Norm after: 10.89065515923107
Epoch 8285/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.386590850353241
Epoch: 8285, Batch Gradient Norm: 10.295595632352072
Epoch: 8285, Batch Gradient Norm after: 10.295595632352072
Epoch 8286/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3825619161128998
Epoch: 8286, Batch Gradient Norm: 9.182490469608648
Epoch: 8286, Batch Gradient Norm after: 9.182490469608648
Epoch 8287/10000, Prediction Accuracy = 63.008%, Loss = 0.3734521448612213
Epoch: 8287, Batch Gradient Norm: 8.671858727280272
Epoch: 8287, Batch Gradient Norm after: 8.671858727280272
Epoch 8288/10000, Prediction Accuracy = 63.098%, Loss = 0.36952292919158936
Epoch: 8288, Batch Gradient Norm: 9.505261682793119
Epoch: 8288, Batch Gradient Norm after: 9.505261682793119
Epoch 8289/10000, Prediction Accuracy = 63.138%, Loss = 0.37322498559951783
Epoch: 8289, Batch Gradient Norm: 12.362436938875026
Epoch: 8289, Batch Gradient Norm after: 12.362436938875026
Epoch 8290/10000, Prediction Accuracy = 63.104%, Loss = 0.3924665987491608
Epoch: 8290, Batch Gradient Norm: 13.106475913585045
Epoch: 8290, Batch Gradient Norm after: 13.106475913585045
Epoch 8291/10000, Prediction Accuracy = 63.04200000000001%, Loss = 0.39992702603340147
Epoch: 8291, Batch Gradient Norm: 11.579389130726693
Epoch: 8291, Batch Gradient Norm after: 11.579389130726693
Epoch 8292/10000, Prediction Accuracy = 63.072%, Loss = 0.3912048518657684
Epoch: 8292, Batch Gradient Norm: 9.302054780604843
Epoch: 8292, Batch Gradient Norm after: 9.302054780604843
Epoch 8293/10000, Prediction Accuracy = 63.02%, Loss = 0.3732788324356079
Epoch: 8293, Batch Gradient Norm: 9.813565592459755
Epoch: 8293, Batch Gradient Norm after: 9.813565592459755
Epoch 8294/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.37532433271408083
Epoch: 8294, Batch Gradient Norm: 10.015802613815769
Epoch: 8294, Batch Gradient Norm after: 10.015802613815769
Epoch 8295/10000, Prediction Accuracy = 63.1%, Loss = 0.3771925687789917
Epoch: 8295, Batch Gradient Norm: 9.160741416636359
Epoch: 8295, Batch Gradient Norm after: 9.160741416636359
Epoch 8296/10000, Prediction Accuracy = 63.188%, Loss = 0.3729656934738159
Epoch: 8296, Batch Gradient Norm: 8.443431002085402
Epoch: 8296, Batch Gradient Norm after: 8.443431002085402
Epoch 8297/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3682763874530792
Epoch: 8297, Batch Gradient Norm: 8.811548065677911
Epoch: 8297, Batch Gradient Norm after: 8.811548065677911
Epoch 8298/10000, Prediction Accuracy = 63.278%, Loss = 0.3713503360748291
Epoch: 8298, Batch Gradient Norm: 9.039805742447577
Epoch: 8298, Batch Gradient Norm after: 9.039805742447577
Epoch 8299/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.3725133419036865
Epoch: 8299, Batch Gradient Norm: 10.266255267643748
Epoch: 8299, Batch Gradient Norm after: 10.266255267643748
Epoch 8300/10000, Prediction Accuracy = 63.072%, Loss = 0.38100560307502745
Epoch: 8300, Batch Gradient Norm: 9.30309315645499
Epoch: 8300, Batch Gradient Norm after: 9.30309315645499
Epoch 8301/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.37512648701667783
Epoch: 8301, Batch Gradient Norm: 9.367607402287044
Epoch: 8301, Batch Gradient Norm after: 9.367607402287044
Epoch 8302/10000, Prediction Accuracy = 63.21%, Loss = 0.37456220388412476
Epoch: 8302, Batch Gradient Norm: 11.23212985293501
Epoch: 8302, Batch Gradient Norm after: 11.23212985293501
Epoch 8303/10000, Prediction Accuracy = 63.14%, Loss = 0.38719363808631896
Epoch: 8303, Batch Gradient Norm: 11.262466457326324
Epoch: 8303, Batch Gradient Norm after: 11.262466457326324
Epoch 8304/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3858575224876404
Epoch: 8304, Batch Gradient Norm: 10.74559360387556
Epoch: 8304, Batch Gradient Norm after: 10.74559360387556
Epoch 8305/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3805225670337677
Epoch: 8305, Batch Gradient Norm: 11.044200173313975
Epoch: 8305, Batch Gradient Norm after: 11.044200173313975
Epoch 8306/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.38227535486221315
Epoch: 8306, Batch Gradient Norm: 10.379035476005583
Epoch: 8306, Batch Gradient Norm after: 10.379035476005583
Epoch 8307/10000, Prediction Accuracy = 63.098%, Loss = 0.3792582094669342
Epoch: 8307, Batch Gradient Norm: 8.952933921458474
Epoch: 8307, Batch Gradient Norm after: 8.952933921458474
Epoch 8308/10000, Prediction Accuracy = 63.19%, Loss = 0.37045601606369016
Epoch: 8308, Batch Gradient Norm: 9.441953922421925
Epoch: 8308, Batch Gradient Norm after: 9.441953922421925
Epoch 8309/10000, Prediction Accuracy = 63.076%, Loss = 0.3725618779659271
Epoch: 8309, Batch Gradient Norm: 10.857396381638369
Epoch: 8309, Batch Gradient Norm after: 10.857396381638369
Epoch 8310/10000, Prediction Accuracy = 63.062%, Loss = 0.38104196786880495
Epoch: 8310, Batch Gradient Norm: 11.600023129229388
Epoch: 8310, Batch Gradient Norm after: 11.600023129229388
Epoch 8311/10000, Prediction Accuracy = 62.83%, Loss = 0.3873040795326233
Epoch: 8311, Batch Gradient Norm: 10.794011368565464
Epoch: 8311, Batch Gradient Norm after: 10.794011368565464
Epoch 8312/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3821418762207031
Epoch: 8312, Batch Gradient Norm: 11.429661552973164
Epoch: 8312, Batch Gradient Norm after: 11.429661552973164
Epoch 8313/10000, Prediction Accuracy = 63.06999999999999%, Loss = 0.3878504991531372
Epoch: 8313, Batch Gradient Norm: 10.725789596534396
Epoch: 8313, Batch Gradient Norm after: 10.725789596534396
Epoch 8314/10000, Prediction Accuracy = 62.992%, Loss = 0.3833566546440125
Epoch: 8314, Batch Gradient Norm: 9.854805094579898
Epoch: 8314, Batch Gradient Norm after: 9.854805094579898
Epoch 8315/10000, Prediction Accuracy = 63.09400000000001%, Loss = 0.3762079536914825
Epoch: 8315, Batch Gradient Norm: 11.32225631914116
Epoch: 8315, Batch Gradient Norm after: 11.32225631914116
Epoch 8316/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3866327524185181
Epoch: 8316, Batch Gradient Norm: 11.165444675001009
Epoch: 8316, Batch Gradient Norm after: 11.165444675001009
Epoch 8317/10000, Prediction Accuracy = 63.117999999999995%, Loss = 0.386639404296875
Epoch: 8317, Batch Gradient Norm: 8.972862861758793
Epoch: 8317, Batch Gradient Norm after: 8.972862861758793
Epoch 8318/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.37231634855270385
Epoch: 8318, Batch Gradient Norm: 8.691976673558264
Epoch: 8318, Batch Gradient Norm after: 8.691976673558264
Epoch 8319/10000, Prediction Accuracy = 63.11999999999999%, Loss = 0.37034578919410704
Epoch: 8319, Batch Gradient Norm: 9.391949040907111
Epoch: 8319, Batch Gradient Norm after: 9.391949040907111
Epoch 8320/10000, Prediction Accuracy = 63.13199999999999%, Loss = 0.3721857309341431
Epoch: 8320, Batch Gradient Norm: 10.972676394596053
Epoch: 8320, Batch Gradient Norm after: 10.972676394596053
Epoch 8321/10000, Prediction Accuracy = 63.162%, Loss = 0.38386865258216857
Epoch: 8321, Batch Gradient Norm: 9.450104048023695
Epoch: 8321, Batch Gradient Norm after: 9.450104048023695
Epoch 8322/10000, Prediction Accuracy = 63.064%, Loss = 0.37468714714050294
Epoch: 8322, Batch Gradient Norm: 8.400864776391264
Epoch: 8322, Batch Gradient Norm after: 8.400864776391264
Epoch 8323/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3675516664981842
Epoch: 8323, Batch Gradient Norm: 8.95411743891751
Epoch: 8323, Batch Gradient Norm after: 8.95411743891751
Epoch 8324/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3696083962917328
Epoch: 8324, Batch Gradient Norm: 10.180725930163952
Epoch: 8324, Batch Gradient Norm after: 10.180725930163952
Epoch 8325/10000, Prediction Accuracy = 63.14%, Loss = 0.37611828446388246
Epoch: 8325, Batch Gradient Norm: 9.589358198812924
Epoch: 8325, Batch Gradient Norm after: 9.589358198812924
Epoch 8326/10000, Prediction Accuracy = 63.102%, Loss = 0.37226536870002747
Epoch: 8326, Batch Gradient Norm: 10.636818442068899
Epoch: 8326, Batch Gradient Norm after: 10.636818442068899
Epoch 8327/10000, Prediction Accuracy = 63.08%, Loss = 0.379450124502182
Epoch: 8327, Batch Gradient Norm: 13.316290798780045
Epoch: 8327, Batch Gradient Norm after: 13.316290798780045
Epoch 8328/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.4025640428066254
Epoch: 8328, Batch Gradient Norm: 11.117765242914764
Epoch: 8328, Batch Gradient Norm after: 11.117765242914764
Epoch 8329/10000, Prediction Accuracy = 63.05%, Loss = 0.38592702746391294
Epoch: 8329, Batch Gradient Norm: 8.73889589054342
Epoch: 8329, Batch Gradient Norm after: 8.73889589054342
Epoch 8330/10000, Prediction Accuracy = 63.102%, Loss = 0.3691813588142395
Epoch: 8330, Batch Gradient Norm: 8.40300208339794
Epoch: 8330, Batch Gradient Norm after: 8.40300208339794
Epoch 8331/10000, Prediction Accuracy = 63.126%, Loss = 0.36768526434898374
Epoch: 8331, Batch Gradient Norm: 9.451968959265136
Epoch: 8331, Batch Gradient Norm after: 9.451968959265136
Epoch 8332/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3743336260318756
Epoch: 8332, Batch Gradient Norm: 10.98509289985225
Epoch: 8332, Batch Gradient Norm after: 10.98509289985225
Epoch 8333/10000, Prediction Accuracy = 63.114%, Loss = 0.38471524715423583
Epoch: 8333, Batch Gradient Norm: 11.682145573957815
Epoch: 8333, Batch Gradient Norm after: 11.682145573957815
Epoch 8334/10000, Prediction Accuracy = 63.162%, Loss = 0.3889303863048553
Epoch: 8334, Batch Gradient Norm: 10.987529177989188
Epoch: 8334, Batch Gradient Norm after: 10.987529177989188
Epoch 8335/10000, Prediction Accuracy = 63.136%, Loss = 0.3845294892787933
Epoch: 8335, Batch Gradient Norm: 9.444150668386756
Epoch: 8335, Batch Gradient Norm after: 9.444150668386756
Epoch 8336/10000, Prediction Accuracy = 63.15%, Loss = 0.37352695465087893
Epoch: 8336, Batch Gradient Norm: 9.078018933998246
Epoch: 8336, Batch Gradient Norm after: 9.078018933998246
Epoch 8337/10000, Prediction Accuracy = 63.162%, Loss = 0.37145071029663085
Epoch: 8337, Batch Gradient Norm: 9.696409529227402
Epoch: 8337, Batch Gradient Norm after: 9.696409529227402
Epoch 8338/10000, Prediction Accuracy = 63.25%, Loss = 0.3757079243659973
Epoch: 8338, Batch Gradient Norm: 10.007607181298924
Epoch: 8338, Batch Gradient Norm after: 10.007607181298924
Epoch 8339/10000, Prediction Accuracy = 63.1%, Loss = 0.37812563180923464
Epoch: 8339, Batch Gradient Norm: 10.290721433543078
Epoch: 8339, Batch Gradient Norm after: 10.290721433543078
Epoch 8340/10000, Prediction Accuracy = 63.16799999999999%, Loss = 0.37959304451942444
Epoch: 8340, Batch Gradient Norm: 10.500680272658526
Epoch: 8340, Batch Gradient Norm after: 10.500680272658526
Epoch 8341/10000, Prediction Accuracy = 63.03000000000001%, Loss = 0.38125490546226504
Epoch: 8341, Batch Gradient Norm: 10.273297081412181
Epoch: 8341, Batch Gradient Norm after: 10.273297081412181
Epoch 8342/10000, Prediction Accuracy = 63.162%, Loss = 0.38134212493896485
Epoch: 8342, Batch Gradient Norm: 10.103367378647157
Epoch: 8342, Batch Gradient Norm after: 10.103367378647157
Epoch 8343/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3798833966255188
Epoch: 8343, Batch Gradient Norm: 10.298336790217443
Epoch: 8343, Batch Gradient Norm after: 10.298336790217443
Epoch 8344/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.38049726486206054
Epoch: 8344, Batch Gradient Norm: 10.2203323385834
Epoch: 8344, Batch Gradient Norm after: 10.2203323385834
Epoch 8345/10000, Prediction Accuracy = 63.169999999999995%, Loss = 0.377693110704422
Epoch: 8345, Batch Gradient Norm: 10.524735189100243
Epoch: 8345, Batch Gradient Norm after: 10.524735189100243
Epoch 8346/10000, Prediction Accuracy = 63.152%, Loss = 0.37879419326782227
Epoch: 8346, Batch Gradient Norm: 10.559345739349336
Epoch: 8346, Batch Gradient Norm after: 10.559345739349336
Epoch 8347/10000, Prediction Accuracy = 63.122%, Loss = 0.379236513376236
Epoch: 8347, Batch Gradient Norm: 9.495038873354028
Epoch: 8347, Batch Gradient Norm after: 9.495038873354028
Epoch 8348/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3732422649860382
Epoch: 8348, Batch Gradient Norm: 9.498661454444125
Epoch: 8348, Batch Gradient Norm after: 9.498661454444125
Epoch 8349/10000, Prediction Accuracy = 63.126%, Loss = 0.3743587613105774
Epoch: 8349, Batch Gradient Norm: 9.899539269549031
Epoch: 8349, Batch Gradient Norm after: 9.899539269549031
Epoch 8350/10000, Prediction Accuracy = 63.1%, Loss = 0.37657020092010496
Epoch: 8350, Batch Gradient Norm: 11.88134819949317
Epoch: 8350, Batch Gradient Norm after: 11.88134819949317
Epoch 8351/10000, Prediction Accuracy = 63.148%, Loss = 0.38918877243995664
Epoch: 8351, Batch Gradient Norm: 12.704348914725363
Epoch: 8351, Batch Gradient Norm after: 12.704348914725363
Epoch 8352/10000, Prediction Accuracy = 62.924%, Loss = 0.39503909945487975
Epoch: 8352, Batch Gradient Norm: 10.284129486491578
Epoch: 8352, Batch Gradient Norm after: 10.284129486491578
Epoch 8353/10000, Prediction Accuracy = 63.124%, Loss = 0.3778660655021667
Epoch: 8353, Batch Gradient Norm: 10.047065028352938
Epoch: 8353, Batch Gradient Norm after: 10.047065028352938
Epoch 8354/10000, Prediction Accuracy = 63.128%, Loss = 0.37630059123039244
Epoch: 8354, Batch Gradient Norm: 10.509477906266902
Epoch: 8354, Batch Gradient Norm after: 10.509477906266902
Epoch 8355/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.38039158582687377
Epoch: 8355, Batch Gradient Norm: 10.203034973067636
Epoch: 8355, Batch Gradient Norm after: 10.203034973067636
Epoch 8356/10000, Prediction Accuracy = 63.23599999999999%, Loss = 0.37914809584617615
Epoch: 8356, Batch Gradient Norm: 9.765931463074327
Epoch: 8356, Batch Gradient Norm after: 9.765931463074327
Epoch 8357/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.37549019455909727
Epoch: 8357, Batch Gradient Norm: 9.511861195599584
Epoch: 8357, Batch Gradient Norm after: 9.511861195599584
Epoch 8358/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.37392584085464475
Epoch: 8358, Batch Gradient Norm: 8.36785010195747
Epoch: 8358, Batch Gradient Norm after: 8.36785010195747
Epoch 8359/10000, Prediction Accuracy = 63.138%, Loss = 0.36758276224136355
Epoch: 8359, Batch Gradient Norm: 8.393281592183936
Epoch: 8359, Batch Gradient Norm after: 8.393281592183936
Epoch 8360/10000, Prediction Accuracy = 63.14%, Loss = 0.36731876730918883
Epoch: 8360, Batch Gradient Norm: 8.662066843946928
Epoch: 8360, Batch Gradient Norm after: 8.662066843946928
Epoch 8361/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.36873275637626646
Epoch: 8361, Batch Gradient Norm: 10.032413118259479
Epoch: 8361, Batch Gradient Norm after: 10.032413118259479
Epoch 8362/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.3765556991100311
Epoch: 8362, Batch Gradient Norm: 12.289604314671111
Epoch: 8362, Batch Gradient Norm after: 12.289604314671111
Epoch 8363/10000, Prediction Accuracy = 63.098%, Loss = 0.39341830015182494
Epoch: 8363, Batch Gradient Norm: 10.802037820801708
Epoch: 8363, Batch Gradient Norm after: 10.802037820801708
Epoch 8364/10000, Prediction Accuracy = 63.088%, Loss = 0.38343043327331544
Epoch: 8364, Batch Gradient Norm: 9.636422170708506
Epoch: 8364, Batch Gradient Norm after: 9.636422170708506
Epoch 8365/10000, Prediction Accuracy = 63.0%, Loss = 0.37328659296035765
Epoch: 8365, Batch Gradient Norm: 11.070880852852557
Epoch: 8365, Batch Gradient Norm after: 11.070880852852557
Epoch 8366/10000, Prediction Accuracy = 62.970000000000006%, Loss = 0.38319706320762636
Epoch: 8366, Batch Gradient Norm: 11.499111230640937
Epoch: 8366, Batch Gradient Norm after: 11.499111230640937
Epoch 8367/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.3874348163604736
Epoch: 8367, Batch Gradient Norm: 10.209690106734103
Epoch: 8367, Batch Gradient Norm after: 10.209690106734103
Epoch 8368/10000, Prediction Accuracy = 63.138%, Loss = 0.3794789433479309
Epoch: 8368, Batch Gradient Norm: 8.663065167922198
Epoch: 8368, Batch Gradient Norm after: 8.663065167922198
Epoch 8369/10000, Prediction Accuracy = 63.17800000000001%, Loss = 0.368394935131073
Epoch: 8369, Batch Gradient Norm: 9.903770484127739
Epoch: 8369, Batch Gradient Norm after: 9.903770484127739
Epoch 8370/10000, Prediction Accuracy = 63.036%, Loss = 0.37571393251419066
Epoch: 8370, Batch Gradient Norm: 10.914581441062161
Epoch: 8370, Batch Gradient Norm after: 10.914581441062161
Epoch 8371/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.38265671730041506
Epoch: 8371, Batch Gradient Norm: 10.109008409202993
Epoch: 8371, Batch Gradient Norm after: 10.109008409202993
Epoch 8372/10000, Prediction Accuracy = 63.074%, Loss = 0.37739070057868956
Epoch: 8372, Batch Gradient Norm: 9.855731728712197
Epoch: 8372, Batch Gradient Norm after: 9.855731728712197
Epoch 8373/10000, Prediction Accuracy = 63.088%, Loss = 0.3758353888988495
Epoch: 8373, Batch Gradient Norm: 9.894152570749101
Epoch: 8373, Batch Gradient Norm after: 9.894152570749101
Epoch 8374/10000, Prediction Accuracy = 63.102%, Loss = 0.37550076842308044
Epoch: 8374, Batch Gradient Norm: 10.503799692572938
Epoch: 8374, Batch Gradient Norm after: 10.503799692572938
Epoch 8375/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.37823508977890014
Epoch: 8375, Batch Gradient Norm: 11.794646162790883
Epoch: 8375, Batch Gradient Norm after: 11.794646162790883
Epoch 8376/10000, Prediction Accuracy = 62.936%, Loss = 0.3860063016414642
Epoch: 8376, Batch Gradient Norm: 11.718145154904237
Epoch: 8376, Batch Gradient Norm after: 11.718145154904237
Epoch 8377/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.38729940056800843
Epoch: 8377, Batch Gradient Norm: 10.963216611246995
Epoch: 8377, Batch Gradient Norm after: 10.963216611246995
Epoch 8378/10000, Prediction Accuracy = 63.138%, Loss = 0.383819717168808
Epoch: 8378, Batch Gradient Norm: 10.247213031073986
Epoch: 8378, Batch Gradient Norm after: 10.247213031073986
Epoch 8379/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.37900864481925967
Epoch: 8379, Batch Gradient Norm: 10.60302379336684
Epoch: 8379, Batch Gradient Norm after: 10.60302379336684
Epoch 8380/10000, Prediction Accuracy = 63.18799999999999%, Loss = 0.3805863618850708
Epoch: 8380, Batch Gradient Norm: 9.874224617803597
Epoch: 8380, Batch Gradient Norm after: 9.874224617803597
Epoch 8381/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.37527236342430115
Epoch: 8381, Batch Gradient Norm: 8.821796894779887
Epoch: 8381, Batch Gradient Norm after: 8.821796894779887
Epoch 8382/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.3688284456729889
Epoch: 8382, Batch Gradient Norm: 8.942202682429938
Epoch: 8382, Batch Gradient Norm after: 8.942202682429938
Epoch 8383/10000, Prediction Accuracy = 63.31600000000001%, Loss = 0.3708288252353668
Epoch: 8383, Batch Gradient Norm: 8.721719479750627
Epoch: 8383, Batch Gradient Norm after: 8.721719479750627
Epoch 8384/10000, Prediction Accuracy = 63.134%, Loss = 0.370636111497879
Epoch: 8384, Batch Gradient Norm: 8.549029744234321
Epoch: 8384, Batch Gradient Norm after: 8.549029744234321
Epoch 8385/10000, Prediction Accuracy = 63.202%, Loss = 0.3686410546302795
Epoch: 8385, Batch Gradient Norm: 10.024474042773058
Epoch: 8385, Batch Gradient Norm after: 10.024474042773058
Epoch 8386/10000, Prediction Accuracy = 63.076%, Loss = 0.37636796832084657
Epoch: 8386, Batch Gradient Norm: 11.905894518285821
Epoch: 8386, Batch Gradient Norm after: 11.905894518285821
Epoch 8387/10000, Prediction Accuracy = 63.076%, Loss = 0.3894795894622803
Epoch: 8387, Batch Gradient Norm: 10.887889170165632
Epoch: 8387, Batch Gradient Norm after: 10.887889170165632
Epoch 8388/10000, Prediction Accuracy = 63.128%, Loss = 0.3818376958370209
Epoch: 8388, Batch Gradient Norm: 9.399074413636303
Epoch: 8388, Batch Gradient Norm after: 9.399074413636303
Epoch 8389/10000, Prediction Accuracy = 63.212%, Loss = 0.37173818349838256
Epoch: 8389, Batch Gradient Norm: 8.305088744481015
Epoch: 8389, Batch Gradient Norm after: 8.305088744481015
Epoch 8390/10000, Prediction Accuracy = 63.048%, Loss = 0.36543521881103513
Epoch: 8390, Batch Gradient Norm: 9.30458885614444
Epoch: 8390, Batch Gradient Norm after: 9.30458885614444
Epoch 8391/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.37108691930770876
Epoch: 8391, Batch Gradient Norm: 10.948742974443466
Epoch: 8391, Batch Gradient Norm after: 10.948742974443466
Epoch 8392/10000, Prediction Accuracy = 63.024%, Loss = 0.3826881587505341
Epoch: 8392, Batch Gradient Norm: 10.723607933446361
Epoch: 8392, Batch Gradient Norm after: 10.723607933446361
Epoch 8393/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.3824844121932983
Epoch: 8393, Batch Gradient Norm: 10.369485910525135
Epoch: 8393, Batch Gradient Norm after: 10.369485910525135
Epoch 8394/10000, Prediction Accuracy = 63.246%, Loss = 0.3789607584476471
Epoch: 8394, Batch Gradient Norm: 11.210546012601524
Epoch: 8394, Batch Gradient Norm after: 11.210546012601524
Epoch 8395/10000, Prediction Accuracy = 63.10799999999999%, Loss = 0.3843399226665497
Epoch: 8395, Batch Gradient Norm: 11.41001963639426
Epoch: 8395, Batch Gradient Norm after: 11.41001963639426
Epoch 8396/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.38589202165603637
Epoch: 8396, Batch Gradient Norm: 10.168744740275724
Epoch: 8396, Batch Gradient Norm after: 10.168744740275724
Epoch 8397/10000, Prediction Accuracy = 63.136%, Loss = 0.3767505943775177
Epoch: 8397, Batch Gradient Norm: 9.209254599255122
Epoch: 8397, Batch Gradient Norm after: 9.209254599255122
Epoch 8398/10000, Prediction Accuracy = 63.112%, Loss = 0.37129237651824953
Epoch: 8398, Batch Gradient Norm: 9.421259867821258
Epoch: 8398, Batch Gradient Norm after: 9.421259867821258
Epoch 8399/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.37296187281608584
Epoch: 8399, Batch Gradient Norm: 11.038989875989282
Epoch: 8399, Batch Gradient Norm after: 11.038989875989282
Epoch 8400/10000, Prediction Accuracy = 63.105999999999995%, Loss = 0.3843114674091339
Epoch: 8400, Batch Gradient Norm: 11.509138542276757
Epoch: 8400, Batch Gradient Norm after: 11.509138542276757
Epoch 8401/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3888014078140259
Epoch: 8401, Batch Gradient Norm: 9.323004362522923
Epoch: 8401, Batch Gradient Norm after: 9.323004362522923
Epoch 8402/10000, Prediction Accuracy = 62.989999999999995%, Loss = 0.37241562604904177
Epoch: 8402, Batch Gradient Norm: 9.343827636144688
Epoch: 8402, Batch Gradient Norm after: 9.343827636144688
Epoch 8403/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.3719026267528534
Epoch: 8403, Batch Gradient Norm: 11.184116106206059
Epoch: 8403, Batch Gradient Norm after: 11.184116106206059
Epoch 8404/10000, Prediction Accuracy = 63.017999999999994%, Loss = 0.38478280901908873
Epoch: 8404, Batch Gradient Norm: 11.40199217309974
Epoch: 8404, Batch Gradient Norm after: 11.40199217309974
Epoch 8405/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3872849643230438
Epoch: 8405, Batch Gradient Norm: 9.90885276438004
Epoch: 8405, Batch Gradient Norm after: 9.90885276438004
Epoch 8406/10000, Prediction Accuracy = 63.162%, Loss = 0.37498214840888977
Epoch: 8406, Batch Gradient Norm: 10.61696821405141
Epoch: 8406, Batch Gradient Norm after: 10.61696821405141
Epoch 8407/10000, Prediction Accuracy = 63.074%, Loss = 0.3787438809871674
Epoch: 8407, Batch Gradient Norm: 10.883094247405053
Epoch: 8407, Batch Gradient Norm after: 10.883094247405053
Epoch 8408/10000, Prediction Accuracy = 63.19999999999999%, Loss = 0.38075957298278806
Epoch: 8408, Batch Gradient Norm: 10.449792824239744
Epoch: 8408, Batch Gradient Norm after: 10.449792824239744
Epoch 8409/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.37898274660110476
Epoch: 8409, Batch Gradient Norm: 10.796601415648164
Epoch: 8409, Batch Gradient Norm after: 10.796601415648164
Epoch 8410/10000, Prediction Accuracy = 63.102%, Loss = 0.38015058636665344
Epoch: 8410, Batch Gradient Norm: 11.013538849011534
Epoch: 8410, Batch Gradient Norm after: 11.013538849011534
Epoch 8411/10000, Prediction Accuracy = 63.238%, Loss = 0.3815199375152588
Epoch: 8411, Batch Gradient Norm: 9.270779969034365
Epoch: 8411, Batch Gradient Norm after: 9.270779969034365
Epoch 8412/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.36988710761070254
Epoch: 8412, Batch Gradient Norm: 8.347277323958332
Epoch: 8412, Batch Gradient Norm after: 8.347277323958332
Epoch 8413/10000, Prediction Accuracy = 63.30800000000001%, Loss = 0.3646825134754181
Epoch: 8413, Batch Gradient Norm: 9.305727683690176
Epoch: 8413, Batch Gradient Norm after: 9.305727683690176
Epoch 8414/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.37086597084999084
Epoch: 8414, Batch Gradient Norm: 10.19895972528779
Epoch: 8414, Batch Gradient Norm after: 10.19895972528779
Epoch 8415/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.378684937953949
Epoch: 8415, Batch Gradient Norm: 10.029441379768894
Epoch: 8415, Batch Gradient Norm after: 10.029441379768894
Epoch 8416/10000, Prediction Accuracy = 63.126%, Loss = 0.37672379016876223
Epoch: 8416, Batch Gradient Norm: 11.19761744822414
Epoch: 8416, Batch Gradient Norm after: 11.19761744822414
Epoch 8417/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.3845446527004242
Epoch: 8417, Batch Gradient Norm: 9.990953156345514
Epoch: 8417, Batch Gradient Norm after: 9.990953156345514
Epoch 8418/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.3767220675945282
Epoch: 8418, Batch Gradient Norm: 8.760095856873747
Epoch: 8418, Batch Gradient Norm after: 8.760095856873747
Epoch 8419/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.36855103373527526
Epoch: 8419, Batch Gradient Norm: 9.91967732288504
Epoch: 8419, Batch Gradient Norm after: 9.91967732288504
Epoch 8420/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.37502970099449157
Epoch: 8420, Batch Gradient Norm: 10.654760628890426
Epoch: 8420, Batch Gradient Norm after: 10.654760628890426
Epoch 8421/10000, Prediction Accuracy = 62.916%, Loss = 0.3812881648540497
Epoch: 8421, Batch Gradient Norm: 9.333200626258195
Epoch: 8421, Batch Gradient Norm after: 9.333200626258195
Epoch 8422/10000, Prediction Accuracy = 63.21199999999999%, Loss = 0.3725088119506836
Epoch: 8422, Batch Gradient Norm: 8.69818372141558
Epoch: 8422, Batch Gradient Norm after: 8.69818372141558
Epoch 8423/10000, Prediction Accuracy = 62.971999999999994%, Loss = 0.36843385696411135
Epoch: 8423, Batch Gradient Norm: 8.729350733907108
Epoch: 8423, Batch Gradient Norm after: 8.729350733907108
Epoch 8424/10000, Prediction Accuracy = 63.146%, Loss = 0.36820173263549805
Epoch: 8424, Batch Gradient Norm: 10.158514222182935
Epoch: 8424, Batch Gradient Norm after: 10.158514222182935
Epoch 8425/10000, Prediction Accuracy = 63.076%, Loss = 0.3773951828479767
Epoch: 8425, Batch Gradient Norm: 10.646929013697322
Epoch: 8425, Batch Gradient Norm after: 10.646929013697322
Epoch 8426/10000, Prediction Accuracy = 63.17%, Loss = 0.3800279498100281
Epoch: 8426, Batch Gradient Norm: 11.370780082377433
Epoch: 8426, Batch Gradient Norm after: 11.370780082377433
Epoch 8427/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.38391571640968325
Epoch: 8427, Batch Gradient Norm: 11.881188820228735
Epoch: 8427, Batch Gradient Norm after: 11.881188820228735
Epoch 8428/10000, Prediction Accuracy = 62.955999999999996%, Loss = 0.3870646595954895
Epoch: 8428, Batch Gradient Norm: 12.075348977788945
Epoch: 8428, Batch Gradient Norm after: 12.075348977788945
Epoch 8429/10000, Prediction Accuracy = 63.124%, Loss = 0.3911016881465912
Epoch: 8429, Batch Gradient Norm: 10.660092270614294
Epoch: 8429, Batch Gradient Norm after: 10.660092270614294
Epoch 8430/10000, Prediction Accuracy = 63.138%, Loss = 0.3803116977214813
Epoch: 8430, Batch Gradient Norm: 10.404792559830094
Epoch: 8430, Batch Gradient Norm after: 10.404792559830094
Epoch 8431/10000, Prediction Accuracy = 63.26400000000001%, Loss = 0.3762821674346924
Epoch: 8431, Batch Gradient Norm: 11.283614249494917
Epoch: 8431, Batch Gradient Norm after: 11.283614249494917
Epoch 8432/10000, Prediction Accuracy = 63.076%, Loss = 0.38268086314201355
Epoch: 8432, Batch Gradient Norm: 10.509701080246577
Epoch: 8432, Batch Gradient Norm after: 10.509701080246577
Epoch 8433/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.37925033569335936
Epoch: 8433, Batch Gradient Norm: 9.903334186985347
Epoch: 8433, Batch Gradient Norm after: 9.903334186985347
Epoch 8434/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.3751687467098236
Epoch: 8434, Batch Gradient Norm: 9.612893617897097
Epoch: 8434, Batch Gradient Norm after: 9.612893617897097
Epoch 8435/10000, Prediction Accuracy = 63.025999999999996%, Loss = 0.3734573543071747
Epoch: 8435, Batch Gradient Norm: 9.272613564929763
Epoch: 8435, Batch Gradient Norm after: 9.272613564929763
Epoch 8436/10000, Prediction Accuracy = 63.176%, Loss = 0.3714530527591705
Epoch: 8436, Batch Gradient Norm: 9.371944402328301
Epoch: 8436, Batch Gradient Norm after: 9.371944402328301
Epoch 8437/10000, Prediction Accuracy = 63.186%, Loss = 0.37211068868637087
Epoch: 8437, Batch Gradient Norm: 9.948854893963466
Epoch: 8437, Batch Gradient Norm after: 9.948854893963466
Epoch 8438/10000, Prediction Accuracy = 63.258%, Loss = 0.3770930767059326
Epoch: 8438, Batch Gradient Norm: 10.048562016989278
Epoch: 8438, Batch Gradient Norm after: 10.048562016989278
Epoch 8439/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.37794135212898256
Epoch: 8439, Batch Gradient Norm: 11.209257482362316
Epoch: 8439, Batch Gradient Norm after: 11.209257482362316
Epoch 8440/10000, Prediction Accuracy = 63.036%, Loss = 0.3840759754180908
Epoch: 8440, Batch Gradient Norm: 10.217774508572083
Epoch: 8440, Batch Gradient Norm after: 10.217774508572083
Epoch 8441/10000, Prediction Accuracy = 63.056%, Loss = 0.3769462823867798
Epoch: 8441, Batch Gradient Norm: 8.085810565888822
Epoch: 8441, Batch Gradient Norm after: 8.085810565888822
Epoch 8442/10000, Prediction Accuracy = 63.162%, Loss = 0.36350126266479493
Epoch: 8442, Batch Gradient Norm: 7.937796019326671
Epoch: 8442, Batch Gradient Norm after: 7.937796019326671
Epoch 8443/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.36234056353569033
Epoch: 8443, Batch Gradient Norm: 10.828307612048198
Epoch: 8443, Batch Gradient Norm after: 10.828307612048198
Epoch 8444/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.38036043643951417
Epoch: 8444, Batch Gradient Norm: 11.798281195217266
Epoch: 8444, Batch Gradient Norm after: 11.798281195217266
Epoch 8445/10000, Prediction Accuracy = 63.062%, Loss = 0.3873299241065979
Epoch: 8445, Batch Gradient Norm: 10.345876048101239
Epoch: 8445, Batch Gradient Norm after: 10.345876048101239
Epoch 8446/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3767887532711029
Epoch: 8446, Batch Gradient Norm: 9.45441499216274
Epoch: 8446, Batch Gradient Norm after: 9.45441499216274
Epoch 8447/10000, Prediction Accuracy = 63.20400000000001%, Loss = 0.3715808570384979
Epoch: 8447, Batch Gradient Norm: 9.3580267971672
Epoch: 8447, Batch Gradient Norm after: 9.3580267971672
Epoch 8448/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.37117724418640136
Epoch: 8448, Batch Gradient Norm: 9.51634788018783
Epoch: 8448, Batch Gradient Norm after: 9.51634788018783
Epoch 8449/10000, Prediction Accuracy = 63.25%, Loss = 0.37258288264274597
Epoch: 8449, Batch Gradient Norm: 10.695084210580175
Epoch: 8449, Batch Gradient Norm after: 10.695084210580175
Epoch 8450/10000, Prediction Accuracy = 63.13000000000001%, Loss = 0.3807135343551636
Epoch: 8450, Batch Gradient Norm: 10.967443956320093
Epoch: 8450, Batch Gradient Norm after: 10.967443956320093
Epoch 8451/10000, Prediction Accuracy = 63.012%, Loss = 0.3832134485244751
Epoch: 8451, Batch Gradient Norm: 10.227242118835486
Epoch: 8451, Batch Gradient Norm after: 10.227242118835486
Epoch 8452/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3775135040283203
Epoch: 8452, Batch Gradient Norm: 12.329191582403734
Epoch: 8452, Batch Gradient Norm after: 12.329191582403734
Epoch 8453/10000, Prediction Accuracy = 63.053999999999995%, Loss = 0.3919896960258484
Epoch: 8453, Batch Gradient Norm: 12.184675267001966
Epoch: 8453, Batch Gradient Norm after: 12.184675267001966
Epoch 8454/10000, Prediction Accuracy = 63.186%, Loss = 0.3913628876209259
Epoch: 8454, Batch Gradient Norm: 9.772177139122144
Epoch: 8454, Batch Gradient Norm after: 9.772177139122144
Epoch 8455/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3724504828453064
Epoch: 8455, Batch Gradient Norm: 9.2067798700748
Epoch: 8455, Batch Gradient Norm after: 9.2067798700748
Epoch 8456/10000, Prediction Accuracy = 63.29%, Loss = 0.36948750019073484
Epoch: 8456, Batch Gradient Norm: 9.836504096632618
Epoch: 8456, Batch Gradient Norm after: 9.836504096632618
Epoch 8457/10000, Prediction Accuracy = 63.19200000000001%, Loss = 0.373984295129776
Epoch: 8457, Batch Gradient Norm: 10.503559669854154
Epoch: 8457, Batch Gradient Norm after: 10.503559669854154
Epoch 8458/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.37973180413246155
Epoch: 8458, Batch Gradient Norm: 10.282633317432438
Epoch: 8458, Batch Gradient Norm after: 10.282633317432438
Epoch 8459/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.37906530499458313
Epoch: 8459, Batch Gradient Norm: 9.46655737590119
Epoch: 8459, Batch Gradient Norm after: 9.46655737590119
Epoch 8460/10000, Prediction Accuracy = 63.162%, Loss = 0.37171584367752075
Epoch: 8460, Batch Gradient Norm: 10.485899598288231
Epoch: 8460, Batch Gradient Norm after: 10.485899598288231
Epoch 8461/10000, Prediction Accuracy = 63.266%, Loss = 0.3769790768623352
Epoch: 8461, Batch Gradient Norm: 10.244336291647121
Epoch: 8461, Batch Gradient Norm after: 10.244336291647121
Epoch 8462/10000, Prediction Accuracy = 63.198%, Loss = 0.3756064772605896
Epoch: 8462, Batch Gradient Norm: 9.413100021652538
Epoch: 8462, Batch Gradient Norm after: 9.413100021652538
Epoch 8463/10000, Prediction Accuracy = 63.102%, Loss = 0.37050401568412783
Epoch: 8463, Batch Gradient Norm: 9.021230234641479
Epoch: 8463, Batch Gradient Norm after: 9.021230234641479
Epoch 8464/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.3688942015171051
Epoch: 8464, Batch Gradient Norm: 9.977053221063933
Epoch: 8464, Batch Gradient Norm after: 9.977053221063933
Epoch 8465/10000, Prediction Accuracy = 63.112%, Loss = 0.3765369772911072
Epoch: 8465, Batch Gradient Norm: 10.057173644588353
Epoch: 8465, Batch Gradient Norm after: 10.057173644588353
Epoch 8466/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.37604494094848634
Epoch: 8466, Batch Gradient Norm: 11.277832509350723
Epoch: 8466, Batch Gradient Norm after: 11.277832509350723
Epoch 8467/10000, Prediction Accuracy = 63.156000000000006%, Loss = 0.38341761827468873
Epoch: 8467, Batch Gradient Norm: 12.913958949761343
Epoch: 8467, Batch Gradient Norm after: 12.913958949761343
Epoch 8468/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.3964564323425293
Epoch: 8468, Batch Gradient Norm: 10.066907250369601
Epoch: 8468, Batch Gradient Norm after: 10.066907250369601
Epoch 8469/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.376372891664505
Epoch: 8469, Batch Gradient Norm: 8.306027224314079
Epoch: 8469, Batch Gradient Norm after: 8.306027224314079
Epoch 8470/10000, Prediction Accuracy = 63.198%, Loss = 0.36435227394104003
Epoch: 8470, Batch Gradient Norm: 10.121097608658614
Epoch: 8470, Batch Gradient Norm after: 10.121097608658614
Epoch 8471/10000, Prediction Accuracy = 63.124%, Loss = 0.37571311593055723
Epoch: 8471, Batch Gradient Norm: 11.400449828158639
Epoch: 8471, Batch Gradient Norm after: 11.400449828158639
Epoch 8472/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.38496374487876894
Epoch: 8472, Batch Gradient Norm: 10.321452154051482
Epoch: 8472, Batch Gradient Norm after: 10.321452154051482
Epoch 8473/10000, Prediction Accuracy = 63.14000000000001%, Loss = 0.37715584635734556
Epoch: 8473, Batch Gradient Norm: 9.752739901846292
Epoch: 8473, Batch Gradient Norm after: 9.752739901846292
Epoch 8474/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.37189760208129885
Epoch: 8474, Batch Gradient Norm: 10.569523175631028
Epoch: 8474, Batch Gradient Norm after: 10.569523175631028
Epoch 8475/10000, Prediction Accuracy = 63.07000000000001%, Loss = 0.3780626952648163
Epoch: 8475, Batch Gradient Norm: 10.963013458340331
Epoch: 8475, Batch Gradient Norm after: 10.963013458340331
Epoch 8476/10000, Prediction Accuracy = 63.010000000000005%, Loss = 0.3810485780239105
Epoch: 8476, Batch Gradient Norm: 11.377553201860277
Epoch: 8476, Batch Gradient Norm after: 11.377553201860277
Epoch 8477/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3865840256214142
Epoch: 8477, Batch Gradient Norm: 9.113931501623092
Epoch: 8477, Batch Gradient Norm after: 9.113931501623092
Epoch 8478/10000, Prediction Accuracy = 63.209999999999994%, Loss = 0.3707811176776886
Epoch: 8478, Batch Gradient Norm: 8.481728780594594
Epoch: 8478, Batch Gradient Norm after: 8.481728780594594
Epoch 8479/10000, Prediction Accuracy = 63.25%, Loss = 0.36577210426330564
Epoch: 8479, Batch Gradient Norm: 8.491235970845791
Epoch: 8479, Batch Gradient Norm after: 8.491235970845791
Epoch 8480/10000, Prediction Accuracy = 63.186%, Loss = 0.3654941856861115
Epoch: 8480, Batch Gradient Norm: 7.835079783508449
Epoch: 8480, Batch Gradient Norm after: 7.835079783508449
Epoch 8481/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.3614246666431427
Epoch: 8481, Batch Gradient Norm: 8.001819948200687
Epoch: 8481, Batch Gradient Norm after: 8.001819948200687
Epoch 8482/10000, Prediction Accuracy = 63.15%, Loss = 0.3622662961483002
Epoch: 8482, Batch Gradient Norm: 10.669022812680348
Epoch: 8482, Batch Gradient Norm after: 10.669022812680348
Epoch 8483/10000, Prediction Accuracy = 63.104%, Loss = 0.3796380698680878
Epoch: 8483, Batch Gradient Norm: 11.92894994941554
Epoch: 8483, Batch Gradient Norm after: 11.92894994941554
Epoch 8484/10000, Prediction Accuracy = 63.008%, Loss = 0.389781254529953
Epoch: 8484, Batch Gradient Norm: 11.376515193604689
Epoch: 8484, Batch Gradient Norm after: 11.376515193604689
Epoch 8485/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.3850864768028259
Epoch: 8485, Batch Gradient Norm: 11.34856934801247
Epoch: 8485, Batch Gradient Norm after: 11.34856934801247
Epoch 8486/10000, Prediction Accuracy = 63.028%, Loss = 0.38445130586624143
Epoch: 8486, Batch Gradient Norm: 10.761059660754311
Epoch: 8486, Batch Gradient Norm after: 10.761059660754311
Epoch 8487/10000, Prediction Accuracy = 63.21%, Loss = 0.3809739530086517
Epoch: 8487, Batch Gradient Norm: 9.958005745562739
Epoch: 8487, Batch Gradient Norm after: 9.958005745562739
Epoch 8488/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.3744486153125763
Epoch: 8488, Batch Gradient Norm: 9.167652885562745
Epoch: 8488, Batch Gradient Norm after: 9.167652885562745
Epoch 8489/10000, Prediction Accuracy = 63.162%, Loss = 0.36939295530319216
Epoch: 8489, Batch Gradient Norm: 9.001019671154479
Epoch: 8489, Batch Gradient Norm after: 9.001019671154479
Epoch 8490/10000, Prediction Accuracy = 63.138%, Loss = 0.3684162199497223
Epoch: 8490, Batch Gradient Norm: 9.355137772518264
Epoch: 8490, Batch Gradient Norm after: 9.355137772518264
Epoch 8491/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.37212502360343935
Epoch: 8491, Batch Gradient Norm: 10.918813683249754
Epoch: 8491, Batch Gradient Norm after: 10.918813683249754
Epoch 8492/10000, Prediction Accuracy = 63.048%, Loss = 0.3815111994743347
Epoch: 8492, Batch Gradient Norm: 13.375420451994875
Epoch: 8492, Batch Gradient Norm after: 13.375420451994875
Epoch 8493/10000, Prediction Accuracy = 63.152%, Loss = 0.40076738595962524
Epoch: 8493, Batch Gradient Norm: 10.080373455090683
Epoch: 8493, Batch Gradient Norm after: 10.080373455090683
Epoch 8494/10000, Prediction Accuracy = 63.112%, Loss = 0.3758900582790375
Epoch: 8494, Batch Gradient Norm: 8.635532520770168
Epoch: 8494, Batch Gradient Norm after: 8.635532520770168
Epoch 8495/10000, Prediction Accuracy = 63.282%, Loss = 0.3664117157459259
Epoch: 8495, Batch Gradient Norm: 8.921985951007473
Epoch: 8495, Batch Gradient Norm after: 8.921985951007473
Epoch 8496/10000, Prediction Accuracy = 63.00600000000001%, Loss = 0.36819019317626955
Epoch: 8496, Batch Gradient Norm: 9.376321707634835
Epoch: 8496, Batch Gradient Norm after: 9.376321707634835
Epoch 8497/10000, Prediction Accuracy = 63.19000000000001%, Loss = 0.3709109127521515
Epoch: 8497, Batch Gradient Norm: 9.52866525447539
Epoch: 8497, Batch Gradient Norm after: 9.52866525447539
Epoch 8498/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.37128694653511046
Epoch: 8498, Batch Gradient Norm: 10.155283301953874
Epoch: 8498, Batch Gradient Norm after: 10.155283301953874
Epoch 8499/10000, Prediction Accuracy = 63.314%, Loss = 0.37591736316680907
Epoch: 8499, Batch Gradient Norm: 9.560090649532036
Epoch: 8499, Batch Gradient Norm after: 9.560090649532036
Epoch 8500/10000, Prediction Accuracy = 63.28399999999999%, Loss = 0.3726913869380951
Epoch: 8500, Batch Gradient Norm: 8.90185607923514
Epoch: 8500, Batch Gradient Norm after: 8.90185607923514
Epoch 8501/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.3685762107372284
Epoch: 8501, Batch Gradient Norm: 9.758511273107516
Epoch: 8501, Batch Gradient Norm after: 9.758511273107516
Epoch 8502/10000, Prediction Accuracy = 63.134%, Loss = 0.37379051446914674
Epoch: 8502, Batch Gradient Norm: 12.415207479380557
Epoch: 8502, Batch Gradient Norm after: 12.415207479380557
Epoch 8503/10000, Prediction Accuracy = 63.0%, Loss = 0.39356189370155337
Epoch: 8503, Batch Gradient Norm: 11.626559516057307
Epoch: 8503, Batch Gradient Norm after: 11.626559516057307
Epoch 8504/10000, Prediction Accuracy = 63.286%, Loss = 0.3870419919490814
Epoch: 8504, Batch Gradient Norm: 9.437276743987567
Epoch: 8504, Batch Gradient Norm after: 9.437276743987567
Epoch 8505/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.37035863995552065
Epoch: 8505, Batch Gradient Norm: 9.939570036814917
Epoch: 8505, Batch Gradient Norm after: 9.939570036814917
Epoch 8506/10000, Prediction Accuracy = 63.001999999999995%, Loss = 0.37366772890090943
Epoch: 8506, Batch Gradient Norm: 9.252092709436463
Epoch: 8506, Batch Gradient Norm after: 9.252092709436463
Epoch 8507/10000, Prediction Accuracy = 63.104%, Loss = 0.36841978430747985
Epoch: 8507, Batch Gradient Norm: 9.729082075937054
Epoch: 8507, Batch Gradient Norm after: 9.729082075937054
Epoch 8508/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.3710151731967926
Epoch: 8508, Batch Gradient Norm: 11.44515853010239
Epoch: 8508, Batch Gradient Norm after: 11.44515853010239
Epoch 8509/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3847951889038086
Epoch: 8509, Batch Gradient Norm: 12.154820000212291
Epoch: 8509, Batch Gradient Norm after: 12.154820000212291
Epoch 8510/10000, Prediction Accuracy = 63.077999999999996%, Loss = 0.39166718125343325
Epoch: 8510, Batch Gradient Norm: 10.542277765395044
Epoch: 8510, Batch Gradient Norm after: 10.542277765395044
Epoch 8511/10000, Prediction Accuracy = 63.21%, Loss = 0.3782199561595917
Epoch: 8511, Batch Gradient Norm: 10.35518122684917
Epoch: 8511, Batch Gradient Norm after: 10.35518122684917
Epoch 8512/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3763201475143433
Epoch: 8512, Batch Gradient Norm: 9.810141396969948
Epoch: 8512, Batch Gradient Norm after: 9.810141396969948
Epoch 8513/10000, Prediction Accuracy = 63.25%, Loss = 0.3739587664604187
Epoch: 8513, Batch Gradient Norm: 9.272750797256238
Epoch: 8513, Batch Gradient Norm after: 9.272750797256238
Epoch 8514/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.37153013348579406
Epoch: 8514, Batch Gradient Norm: 11.12935754370642
Epoch: 8514, Batch Gradient Norm after: 11.12935754370642
Epoch 8515/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.38217380046844485
Epoch: 8515, Batch Gradient Norm: 12.174964730430345
Epoch: 8515, Batch Gradient Norm after: 12.174964730430345
Epoch 8516/10000, Prediction Accuracy = 63.0%, Loss = 0.38827606439590456
Epoch: 8516, Batch Gradient Norm: 9.981848748010211
Epoch: 8516, Batch Gradient Norm after: 9.981848748010211
Epoch 8517/10000, Prediction Accuracy = 63.15%, Loss = 0.37126200199127196
Epoch: 8517, Batch Gradient Norm: 8.58008519087949
Epoch: 8517, Batch Gradient Norm after: 8.58008519087949
Epoch 8518/10000, Prediction Accuracy = 63.186%, Loss = 0.36319775581359864
Epoch: 8518, Batch Gradient Norm: 9.659554211836404
Epoch: 8518, Batch Gradient Norm after: 9.659554211836404
Epoch 8519/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.37123403549194334
Epoch: 8519, Batch Gradient Norm: 10.515225696393232
Epoch: 8519, Batch Gradient Norm after: 10.515225696393232
Epoch 8520/10000, Prediction Accuracy = 63.186%, Loss = 0.3793145418167114
Epoch: 8520, Batch Gradient Norm: 8.92804887216233
Epoch: 8520, Batch Gradient Norm after: 8.92804887216233
Epoch 8521/10000, Prediction Accuracy = 63.217999999999996%, Loss = 0.36909147500991824
Epoch: 8521, Batch Gradient Norm: 9.258439560154713
Epoch: 8521, Batch Gradient Norm after: 9.258439560154713
Epoch 8522/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.3701819062232971
Epoch: 8522, Batch Gradient Norm: 11.182121225103455
Epoch: 8522, Batch Gradient Norm after: 11.182121225103455
Epoch 8523/10000, Prediction Accuracy = 63.11%, Loss = 0.38570222854614256
Epoch: 8523, Batch Gradient Norm: 9.740889181607768
Epoch: 8523, Batch Gradient Norm after: 9.740889181607768
Epoch 8524/10000, Prediction Accuracy = 63.166%, Loss = 0.3744100987911224
Epoch: 8524, Batch Gradient Norm: 9.804322004376724
Epoch: 8524, Batch Gradient Norm after: 9.804322004376724
Epoch 8525/10000, Prediction Accuracy = 63.138%, Loss = 0.3721147119998932
Epoch: 8525, Batch Gradient Norm: 10.054057109862656
Epoch: 8525, Batch Gradient Norm after: 10.054057109862656
Epoch 8526/10000, Prediction Accuracy = 63.152%, Loss = 0.3725222826004028
Epoch: 8526, Batch Gradient Norm: 10.632478648820518
Epoch: 8526, Batch Gradient Norm after: 10.632478648820518
Epoch 8527/10000, Prediction Accuracy = 63.093999999999994%, Loss = 0.376785808801651
Epoch: 8527, Batch Gradient Norm: 11.054145616224085
Epoch: 8527, Batch Gradient Norm after: 11.054145616224085
Epoch 8528/10000, Prediction Accuracy = 63.2%, Loss = 0.38245896697044374
Epoch: 8528, Batch Gradient Norm: 9.821334880474584
Epoch: 8528, Batch Gradient Norm after: 9.821334880474584
Epoch 8529/10000, Prediction Accuracy = 63.134%, Loss = 0.37509870529174805
Epoch: 8529, Batch Gradient Norm: 8.566395243882482
Epoch: 8529, Batch Gradient Norm after: 8.566395243882482
Epoch 8530/10000, Prediction Accuracy = 63.288%, Loss = 0.36645079851150514
Epoch: 8530, Batch Gradient Norm: 9.8885438904618
Epoch: 8530, Batch Gradient Norm after: 9.8885438904618
Epoch 8531/10000, Prediction Accuracy = 63.186%, Loss = 0.3725733578205109
Epoch: 8531, Batch Gradient Norm: 11.945107275776328
Epoch: 8531, Batch Gradient Norm after: 11.945107275776328
Epoch 8532/10000, Prediction Accuracy = 63.016%, Loss = 0.3863768219947815
Epoch: 8532, Batch Gradient Norm: 11.126709779293549
Epoch: 8532, Batch Gradient Norm after: 11.126709779293549
Epoch 8533/10000, Prediction Accuracy = 63.065999999999995%, Loss = 0.3833431601524353
Epoch: 8533, Batch Gradient Norm: 10.353358418868956
Epoch: 8533, Batch Gradient Norm after: 10.353358418868956
Epoch 8534/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3783804953098297
Epoch: 8534, Batch Gradient Norm: 10.484992013075518
Epoch: 8534, Batch Gradient Norm after: 10.484992013075518
Epoch 8535/10000, Prediction Accuracy = 63.2%, Loss = 0.378812301158905
Epoch: 8535, Batch Gradient Norm: 9.69058092652325
Epoch: 8535, Batch Gradient Norm after: 9.69058092652325
Epoch 8536/10000, Prediction Accuracy = 63.164%, Loss = 0.3719356179237366
Epoch: 8536, Batch Gradient Norm: 9.91385145070168
Epoch: 8536, Batch Gradient Norm after: 9.91385145070168
Epoch 8537/10000, Prediction Accuracy = 63.15599999999999%, Loss = 0.3706117391586304
Epoch: 8537, Batch Gradient Norm: 11.328114575087875
Epoch: 8537, Batch Gradient Norm after: 11.328114575087875
Epoch 8538/10000, Prediction Accuracy = 63.01800000000001%, Loss = 0.3804731547832489
Epoch: 8538, Batch Gradient Norm: 11.04999922983116
Epoch: 8538, Batch Gradient Norm after: 11.04999922983116
Epoch 8539/10000, Prediction Accuracy = 63.14%, Loss = 0.3800275564193726
Epoch: 8539, Batch Gradient Norm: 9.443423536275272
Epoch: 8539, Batch Gradient Norm after: 9.443423536275272
Epoch 8540/10000, Prediction Accuracy = 63.148%, Loss = 0.3705669462680817
Epoch: 8540, Batch Gradient Norm: 8.833234595860016
Epoch: 8540, Batch Gradient Norm after: 8.833234595860016
Epoch 8541/10000, Prediction Accuracy = 63.262%, Loss = 0.3678578555583954
Epoch: 8541, Batch Gradient Norm: 8.849161457425522
Epoch: 8541, Batch Gradient Norm after: 8.849161457425522
Epoch 8542/10000, Prediction Accuracy = 63.198%, Loss = 0.36792791485786436
Epoch: 8542, Batch Gradient Norm: 11.229589565394607
Epoch: 8542, Batch Gradient Norm after: 11.229589565394607
Epoch 8543/10000, Prediction Accuracy = 63.048%, Loss = 0.38409623503685
Epoch: 8543, Batch Gradient Norm: 12.521701315621396
Epoch: 8543, Batch Gradient Norm after: 12.521701315621396
Epoch 8544/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.39254515171051024
Epoch: 8544, Batch Gradient Norm: 11.389508785384674
Epoch: 8544, Batch Gradient Norm after: 11.389508785384674
Epoch 8545/10000, Prediction Accuracy = 63.08%, Loss = 0.3822139024734497
Epoch: 8545, Batch Gradient Norm: 8.8298490480215
Epoch: 8545, Batch Gradient Norm after: 8.8298490480215
Epoch 8546/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.36555402278900145
Epoch: 8546, Batch Gradient Norm: 7.770261865015982
Epoch: 8546, Batch Gradient Norm after: 7.770261865015982
Epoch 8547/10000, Prediction Accuracy = 63.068000000000005%, Loss = 0.35980415940284727
Epoch: 8547, Batch Gradient Norm: 9.001198279750328
Epoch: 8547, Batch Gradient Norm after: 9.001198279750328
Epoch 8548/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.36767364740371705
Epoch: 8548, Batch Gradient Norm: 10.217757598998844
Epoch: 8548, Batch Gradient Norm after: 10.217757598998844
Epoch 8549/10000, Prediction Accuracy = 63.124%, Loss = 0.376444548368454
Epoch: 8549, Batch Gradient Norm: 10.308701863508734
Epoch: 8549, Batch Gradient Norm after: 10.308701863508734
Epoch 8550/10000, Prediction Accuracy = 63.306%, Loss = 0.37739076018333434
Epoch: 8550, Batch Gradient Norm: 10.098150584114299
Epoch: 8550, Batch Gradient Norm after: 10.098150584114299
Epoch 8551/10000, Prediction Accuracy = 63.176%, Loss = 0.3752549171447754
Epoch: 8551, Batch Gradient Norm: 10.365563416704376
Epoch: 8551, Batch Gradient Norm after: 10.365563416704376
Epoch 8552/10000, Prediction Accuracy = 63.116%, Loss = 0.3767191469669342
Epoch: 8552, Batch Gradient Norm: 10.687359549448912
Epoch: 8552, Batch Gradient Norm after: 10.687359549448912
Epoch 8553/10000, Prediction Accuracy = 63.21%, Loss = 0.37993939518928527
Epoch: 8553, Batch Gradient Norm: 9.870552383986693
Epoch: 8553, Batch Gradient Norm after: 9.870552383986693
Epoch 8554/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.37423580288887026
Epoch: 8554, Batch Gradient Norm: 9.34467643598675
Epoch: 8554, Batch Gradient Norm after: 9.34467643598675
Epoch 8555/10000, Prediction Accuracy = 63.218%, Loss = 0.36931670308113096
Epoch: 8555, Batch Gradient Norm: 10.527207778737225
Epoch: 8555, Batch Gradient Norm after: 10.527207778737225
Epoch 8556/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.3754929006099701
Epoch: 8556, Batch Gradient Norm: 11.73447905511131
Epoch: 8556, Batch Gradient Norm after: 11.73447905511131
Epoch 8557/10000, Prediction Accuracy = 63.15%, Loss = 0.3863935172557831
Epoch: 8557, Batch Gradient Norm: 10.27908514150231
Epoch: 8557, Batch Gradient Norm after: 10.27908514150231
Epoch 8558/10000, Prediction Accuracy = 63.374%, Loss = 0.37583876252174375
Epoch: 8558, Batch Gradient Norm: 10.297784929943672
Epoch: 8558, Batch Gradient Norm after: 10.297784929943672
Epoch 8559/10000, Prediction Accuracy = 63.128%, Loss = 0.37625588178634645
Epoch: 8559, Batch Gradient Norm: 10.799789869068144
Epoch: 8559, Batch Gradient Norm after: 10.799789869068144
Epoch 8560/10000, Prediction Accuracy = 63.206%, Loss = 0.3801365077495575
Epoch: 8560, Batch Gradient Norm: 10.16929985542166
Epoch: 8560, Batch Gradient Norm after: 10.16929985542166
Epoch 8561/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.37544346451759336
Epoch: 8561, Batch Gradient Norm: 9.61042875263827
Epoch: 8561, Batch Gradient Norm after: 9.61042875263827
Epoch 8562/10000, Prediction Accuracy = 63.104000000000006%, Loss = 0.37174938917160033
Epoch: 8562, Batch Gradient Norm: 10.015439102936853
Epoch: 8562, Batch Gradient Norm after: 10.015439102936853
Epoch 8563/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.37311617136001585
Epoch: 8563, Batch Gradient Norm: 11.539751165619338
Epoch: 8563, Batch Gradient Norm after: 11.539751165619338
Epoch 8564/10000, Prediction Accuracy = 63.104%, Loss = 0.3824724853038788
Epoch: 8564, Batch Gradient Norm: 11.649850348879298
Epoch: 8564, Batch Gradient Norm after: 11.649850348879298
Epoch 8565/10000, Prediction Accuracy = 63.144000000000005%, Loss = 0.3826990842819214
Epoch: 8565, Batch Gradient Norm: 10.042968350504168
Epoch: 8565, Batch Gradient Norm after: 10.042968350504168
Epoch 8566/10000, Prediction Accuracy = 63.29%, Loss = 0.3729028820991516
Epoch: 8566, Batch Gradient Norm: 9.395634819920943
Epoch: 8566, Batch Gradient Norm after: 9.395634819920943
Epoch 8567/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.37068198919296264
Epoch: 8567, Batch Gradient Norm: 9.181015379297333
Epoch: 8567, Batch Gradient Norm after: 9.181015379297333
Epoch 8568/10000, Prediction Accuracy = 63.322%, Loss = 0.3703972101211548
Epoch: 8568, Batch Gradient Norm: 9.24539739199594
Epoch: 8568, Batch Gradient Norm after: 9.24539739199594
Epoch 8569/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3698994815349579
Epoch: 8569, Batch Gradient Norm: 9.774450767583549
Epoch: 8569, Batch Gradient Norm after: 9.774450767583549
Epoch 8570/10000, Prediction Accuracy = 63.086%, Loss = 0.3731097996234894
Epoch: 8570, Batch Gradient Norm: 9.903885050463318
Epoch: 8570, Batch Gradient Norm after: 9.903885050463318
Epoch 8571/10000, Prediction Accuracy = 63.382000000000005%, Loss = 0.3737523019313812
Epoch: 8571, Batch Gradient Norm: 9.263096525196573
Epoch: 8571, Batch Gradient Norm after: 9.263096525196573
Epoch 8572/10000, Prediction Accuracy = 63.33200000000001%, Loss = 0.36928303837776183
Epoch: 8572, Batch Gradient Norm: 9.659366934863433
Epoch: 8572, Batch Gradient Norm after: 9.659366934863433
Epoch 8573/10000, Prediction Accuracy = 63.25200000000001%, Loss = 0.3711899220943451
Epoch: 8573, Batch Gradient Norm: 9.913539492895774
Epoch: 8573, Batch Gradient Norm after: 9.913539492895774
Epoch 8574/10000, Prediction Accuracy = 63.212%, Loss = 0.37334243059158323
Epoch: 8574, Batch Gradient Norm: 9.971024497437496
Epoch: 8574, Batch Gradient Norm after: 9.971024497437496
Epoch 8575/10000, Prediction Accuracy = 63.048%, Loss = 0.37457568645477296
Epoch: 8575, Batch Gradient Norm: 9.849904150127795
Epoch: 8575, Batch Gradient Norm after: 9.849904150127795
Epoch 8576/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.37380754947662354
Epoch: 8576, Batch Gradient Norm: 10.19127070788267
Epoch: 8576, Batch Gradient Norm after: 10.19127070788267
Epoch 8577/10000, Prediction Accuracy = 63.088%, Loss = 0.3750818371772766
Epoch: 8577, Batch Gradient Norm: 11.297938419936226
Epoch: 8577, Batch Gradient Norm after: 11.297938419936226
Epoch 8578/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3823175966739655
Epoch: 8578, Batch Gradient Norm: 11.659998595869775
Epoch: 8578, Batch Gradient Norm after: 11.659998595869775
Epoch 8579/10000, Prediction Accuracy = 63.1%, Loss = 0.38404237627983095
Epoch: 8579, Batch Gradient Norm: 11.718003025799359
Epoch: 8579, Batch Gradient Norm after: 11.718003025799359
Epoch 8580/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.38369653820991517
Epoch: 8580, Batch Gradient Norm: 12.455692815907819
Epoch: 8580, Batch Gradient Norm after: 12.455692815907819
Epoch 8581/10000, Prediction Accuracy = 63.214%, Loss = 0.39045109748840334
Epoch: 8581, Batch Gradient Norm: 9.913702047231565
Epoch: 8581, Batch Gradient Norm after: 9.913702047231565
Epoch 8582/10000, Prediction Accuracy = 63.326%, Loss = 0.37360079288482667
Epoch: 8582, Batch Gradient Norm: 7.2866065182360185
Epoch: 8582, Batch Gradient Norm after: 7.2866065182360185
Epoch 8583/10000, Prediction Accuracy = 63.343999999999994%, Loss = 0.3577458500862122
Epoch: 8583, Batch Gradient Norm: 7.142445027764922
Epoch: 8583, Batch Gradient Norm after: 7.142445027764922
Epoch 8584/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.35589157938957217
Epoch: 8584, Batch Gradient Norm: 8.634426007568702
Epoch: 8584, Batch Gradient Norm after: 8.634426007568702
Epoch 8585/10000, Prediction Accuracy = 63.174%, Loss = 0.36364429593086245
Epoch: 8585, Batch Gradient Norm: 10.696326225582528
Epoch: 8585, Batch Gradient Norm after: 10.696326225582528
Epoch 8586/10000, Prediction Accuracy = 62.984%, Loss = 0.380530458688736
Epoch: 8586, Batch Gradient Norm: 12.73729443457409
Epoch: 8586, Batch Gradient Norm after: 12.73729443457409
Epoch 8587/10000, Prediction Accuracy = 63.16199999999999%, Loss = 0.3944626867771149
Epoch: 8587, Batch Gradient Norm: 11.910752311605957
Epoch: 8587, Batch Gradient Norm after: 11.910752311605957
Epoch 8588/10000, Prediction Accuracy = 63.198%, Loss = 0.3889052331447601
Epoch: 8588, Batch Gradient Norm: 9.788374872466033
Epoch: 8588, Batch Gradient Norm after: 9.788374872466033
Epoch 8589/10000, Prediction Accuracy = 63.33800000000001%, Loss = 0.37238689661026003
Epoch: 8589, Batch Gradient Norm: 8.595466255310605
Epoch: 8589, Batch Gradient Norm after: 8.595466255310605
Epoch 8590/10000, Prediction Accuracy = 63.25%, Loss = 0.3642756164073944
Epoch: 8590, Batch Gradient Norm: 9.753695670198258
Epoch: 8590, Batch Gradient Norm after: 9.753695670198258
Epoch 8591/10000, Prediction Accuracy = 63.212%, Loss = 0.37058178186416624
Epoch: 8591, Batch Gradient Norm: 10.828920845778043
Epoch: 8591, Batch Gradient Norm after: 10.828920845778043
Epoch 8592/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.37733919620513917
Epoch: 8592, Batch Gradient Norm: 10.480325450439778
Epoch: 8592, Batch Gradient Norm after: 10.480325450439778
Epoch 8593/10000, Prediction Accuracy = 63.1%, Loss = 0.3771297514438629
Epoch: 8593, Batch Gradient Norm: 9.94906598438498
Epoch: 8593, Batch Gradient Norm after: 9.94906598438498
Epoch 8594/10000, Prediction Accuracy = 63.221999999999994%, Loss = 0.3732059240341187
Epoch: 8594, Batch Gradient Norm: 11.354410739224443
Epoch: 8594, Batch Gradient Norm after: 11.354410739224443
Epoch 8595/10000, Prediction Accuracy = 63.122%, Loss = 0.38097232580184937
Epoch: 8595, Batch Gradient Norm: 11.62697159235615
Epoch: 8595, Batch Gradient Norm after: 11.62697159235615
Epoch 8596/10000, Prediction Accuracy = 63.089999999999996%, Loss = 0.3819419860839844
Epoch: 8596, Batch Gradient Norm: 9.931635957991036
Epoch: 8596, Batch Gradient Norm after: 9.931635957991036
Epoch 8597/10000, Prediction Accuracy = 63.182%, Loss = 0.37175693511962893
Epoch: 8597, Batch Gradient Norm: 9.05513868607388
Epoch: 8597, Batch Gradient Norm after: 9.05513868607388
Epoch 8598/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.368144291639328
Epoch: 8598, Batch Gradient Norm: 8.492564731664979
Epoch: 8598, Batch Gradient Norm after: 8.492564731664979
Epoch 8599/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3641489207744598
Epoch: 8599, Batch Gradient Norm: 10.309612800833744
Epoch: 8599, Batch Gradient Norm after: 10.309612800833744
Epoch 8600/10000, Prediction Accuracy = 63.182%, Loss = 0.3754293203353882
Epoch: 8600, Batch Gradient Norm: 10.894097079123206
Epoch: 8600, Batch Gradient Norm after: 10.894097079123206
Epoch 8601/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.38181118965148925
Epoch: 8601, Batch Gradient Norm: 9.288660698317681
Epoch: 8601, Batch Gradient Norm after: 9.288660698317681
Epoch 8602/10000, Prediction Accuracy = 63.186%, Loss = 0.37019535303115847
Epoch: 8602, Batch Gradient Norm: 10.32629112985426
Epoch: 8602, Batch Gradient Norm after: 10.32629112985426
Epoch 8603/10000, Prediction Accuracy = 63.18399999999999%, Loss = 0.37620729804039
Epoch: 8603, Batch Gradient Norm: 10.952259271091522
Epoch: 8603, Batch Gradient Norm after: 10.952259271091522
Epoch 8604/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.38045490980148317
Epoch: 8604, Batch Gradient Norm: 9.809483530469135
Epoch: 8604, Batch Gradient Norm after: 9.809483530469135
Epoch 8605/10000, Prediction Accuracy = 63.33599999999999%, Loss = 0.3722057640552521
Epoch: 8605, Batch Gradient Norm: 8.722033155513659
Epoch: 8605, Batch Gradient Norm after: 8.722033155513659
Epoch 8606/10000, Prediction Accuracy = 63.36%, Loss = 0.36407517194747924
Epoch: 8606, Batch Gradient Norm: 9.71759564317441
Epoch: 8606, Batch Gradient Norm after: 9.71759564317441
Epoch 8607/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3699356853961945
Epoch: 8607, Batch Gradient Norm: 12.122695905145326
Epoch: 8607, Batch Gradient Norm after: 12.122695905145326
Epoch 8608/10000, Prediction Accuracy = 63.013999999999996%, Loss = 0.3885875403881073
Epoch: 8608, Batch Gradient Norm: 11.268429769071972
Epoch: 8608, Batch Gradient Norm after: 11.268429769071972
Epoch 8609/10000, Prediction Accuracy = 63.11199999999999%, Loss = 0.3825073778629303
Epoch: 8609, Batch Gradient Norm: 10.351478616410875
Epoch: 8609, Batch Gradient Norm after: 10.351478616410875
Epoch 8610/10000, Prediction Accuracy = 63.254000000000005%, Loss = 0.3735992908477783
Epoch: 8610, Batch Gradient Norm: 10.804910514647359
Epoch: 8610, Batch Gradient Norm after: 10.804910514647359
Epoch 8611/10000, Prediction Accuracy = 63.034000000000006%, Loss = 0.37593945264816286
Epoch: 8611, Batch Gradient Norm: 9.747558834432205
Epoch: 8611, Batch Gradient Norm after: 9.747558834432205
Epoch 8612/10000, Prediction Accuracy = 63.186%, Loss = 0.3703957796096802
Epoch: 8612, Batch Gradient Norm: 9.19500921654629
Epoch: 8612, Batch Gradient Norm after: 9.19500921654629
Epoch 8613/10000, Prediction Accuracy = 63.024%, Loss = 0.36764933466911315
Epoch: 8613, Batch Gradient Norm: 9.63219468053346
Epoch: 8613, Batch Gradient Norm after: 9.63219468053346
Epoch 8614/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3717537999153137
Epoch: 8614, Batch Gradient Norm: 9.581251581964008
Epoch: 8614, Batch Gradient Norm after: 9.581251581964008
Epoch 8615/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.37280755639076235
Epoch: 8615, Batch Gradient Norm: 8.89466392446098
Epoch: 8615, Batch Gradient Norm after: 8.89466392446098
Epoch 8616/10000, Prediction Accuracy = 63.178%, Loss = 0.36855170130729675
Epoch: 8616, Batch Gradient Norm: 9.217103690254893
Epoch: 8616, Batch Gradient Norm after: 9.217103690254893
Epoch 8617/10000, Prediction Accuracy = 63.28800000000001%, Loss = 0.3679647624492645
Epoch: 8617, Batch Gradient Norm: 9.959153158863765
Epoch: 8617, Batch Gradient Norm after: 9.959153158863765
Epoch 8618/10000, Prediction Accuracy = 63.068%, Loss = 0.37175710797309874
Epoch: 8618, Batch Gradient Norm: 11.181693705757025
Epoch: 8618, Batch Gradient Norm after: 11.181693705757025
Epoch 8619/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.3796395599842072
Epoch: 8619, Batch Gradient Norm: 13.840705286323944
Epoch: 8619, Batch Gradient Norm after: 13.840705286323944
Epoch 8620/10000, Prediction Accuracy = 63.3%, Loss = 0.4050986051559448
Epoch: 8620, Batch Gradient Norm: 9.67845585410998
Epoch: 8620, Batch Gradient Norm after: 9.67845585410998
Epoch 8621/10000, Prediction Accuracy = 63.21%, Loss = 0.3717564225196838
Epoch: 8621, Batch Gradient Norm: 9.226850425011364
Epoch: 8621, Batch Gradient Norm after: 9.226850425011364
Epoch 8622/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.36705962419509885
Epoch: 8622, Batch Gradient Norm: 10.876753381581716
Epoch: 8622, Batch Gradient Norm after: 10.876753381581716
Epoch 8623/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3788685381412506
Epoch: 8623, Batch Gradient Norm: 10.285807339016525
Epoch: 8623, Batch Gradient Norm after: 10.285807339016525
Epoch 8624/10000, Prediction Accuracy = 63.10799999999999%, Loss = 0.37646483182907103
Epoch: 8624, Batch Gradient Norm: 7.878194802769897
Epoch: 8624, Batch Gradient Norm after: 7.878194802769897
Epoch 8625/10000, Prediction Accuracy = 63.298%, Loss = 0.3613808572292328
Epoch: 8625, Batch Gradient Norm: 7.269111430584613
Epoch: 8625, Batch Gradient Norm after: 7.269111430584613
Epoch 8626/10000, Prediction Accuracy = 63.209999999999994%, Loss = 0.3568621873855591
Epoch: 8626, Batch Gradient Norm: 9.660761557361884
Epoch: 8626, Batch Gradient Norm after: 9.660761557361884
Epoch 8627/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.3703655660152435
Epoch: 8627, Batch Gradient Norm: 11.7108151157641
Epoch: 8627, Batch Gradient Norm after: 11.7108151157641
Epoch 8628/10000, Prediction Accuracy = 63.254%, Loss = 0.38425031304359436
Epoch: 8628, Batch Gradient Norm: 11.99453810751648
Epoch: 8628, Batch Gradient Norm after: 11.99453810751648
Epoch 8629/10000, Prediction Accuracy = 63.262%, Loss = 0.3873726189136505
Epoch: 8629, Batch Gradient Norm: 10.94189917123021
Epoch: 8629, Batch Gradient Norm after: 10.94189917123021
Epoch 8630/10000, Prediction Accuracy = 63.152%, Loss = 0.38068886995315554
Epoch: 8630, Batch Gradient Norm: 9.299352895056444
Epoch: 8630, Batch Gradient Norm after: 9.299352895056444
Epoch 8631/10000, Prediction Accuracy = 63.116%, Loss = 0.3692414164543152
Epoch: 8631, Batch Gradient Norm: 9.10456990078854
Epoch: 8631, Batch Gradient Norm after: 9.10456990078854
Epoch 8632/10000, Prediction Accuracy = 63.188%, Loss = 0.3669900894165039
Epoch: 8632, Batch Gradient Norm: 10.659098430722683
Epoch: 8632, Batch Gradient Norm after: 10.659098430722683
Epoch 8633/10000, Prediction Accuracy = 63.14399999999999%, Loss = 0.3767074942588806
Epoch: 8633, Batch Gradient Norm: 11.13755208159096
Epoch: 8633, Batch Gradient Norm after: 11.13755208159096
Epoch 8634/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.37947367429733275
Epoch: 8634, Batch Gradient Norm: 9.802711064251294
Epoch: 8634, Batch Gradient Norm after: 9.802711064251294
Epoch 8635/10000, Prediction Accuracy = 63.148%, Loss = 0.3706246316432953
Epoch: 8635, Batch Gradient Norm: 9.010843732064185
Epoch: 8635, Batch Gradient Norm after: 9.010843732064185
Epoch 8636/10000, Prediction Accuracy = 63.128%, Loss = 0.3649042248725891
Epoch: 8636, Batch Gradient Norm: 11.170952501527866
Epoch: 8636, Batch Gradient Norm after: 11.170952501527866
Epoch 8637/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.37970797419548036
Epoch: 8637, Batch Gradient Norm: 12.620362413803498
Epoch: 8637, Batch Gradient Norm after: 12.620362413803498
Epoch 8638/10000, Prediction Accuracy = 63.098%, Loss = 0.3932413399219513
Epoch: 8638, Batch Gradient Norm: 10.894017863758938
Epoch: 8638, Batch Gradient Norm after: 10.894017863758938
Epoch 8639/10000, Prediction Accuracy = 63.122%, Loss = 0.38202162384986876
Epoch: 8639, Batch Gradient Norm: 7.822860967343743
Epoch: 8639, Batch Gradient Norm after: 7.822860967343743
Epoch 8640/10000, Prediction Accuracy = 63.35%, Loss = 0.3605149507522583
Epoch: 8640, Batch Gradient Norm: 7.932493151929879
Epoch: 8640, Batch Gradient Norm after: 7.932493151929879
Epoch 8641/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.3599553644657135
Epoch: 8641, Batch Gradient Norm: 10.775677629661901
Epoch: 8641, Batch Gradient Norm after: 10.775677629661901
Epoch 8642/10000, Prediction Accuracy = 63.386%, Loss = 0.37703080773353576
Epoch: 8642, Batch Gradient Norm: 12.108701510397834
Epoch: 8642, Batch Gradient Norm after: 12.108701510397834
Epoch 8643/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.38759873509407045
Epoch: 8643, Batch Gradient Norm: 10.239786162036387
Epoch: 8643, Batch Gradient Norm after: 10.239786162036387
Epoch 8644/10000, Prediction Accuracy = 63.286%, Loss = 0.3739527702331543
Epoch: 8644, Batch Gradient Norm: 8.897668899179404
Epoch: 8644, Batch Gradient Norm after: 8.897668899179404
Epoch 8645/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.36496068835258483
Epoch: 8645, Batch Gradient Norm: 10.348419222865836
Epoch: 8645, Batch Gradient Norm after: 10.348419222865836
Epoch 8646/10000, Prediction Accuracy = 63.146%, Loss = 0.3755811393260956
Epoch: 8646, Batch Gradient Norm: 10.939461319725243
Epoch: 8646, Batch Gradient Norm after: 10.939461319725243
Epoch 8647/10000, Prediction Accuracy = 63.064%, Loss = 0.37919684052467345
Epoch: 8647, Batch Gradient Norm: 10.775038783173395
Epoch: 8647, Batch Gradient Norm after: 10.775038783173395
Epoch 8648/10000, Prediction Accuracy = 63.214%, Loss = 0.3783157169818878
Epoch: 8648, Batch Gradient Norm: 10.126190700885106
Epoch: 8648, Batch Gradient Norm after: 10.126190700885106
Epoch 8649/10000, Prediction Accuracy = 63.2%, Loss = 0.3743520319461823
Epoch: 8649, Batch Gradient Norm: 9.089911582821932
Epoch: 8649, Batch Gradient Norm after: 9.089911582821932
Epoch 8650/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3659307062625885
Epoch: 8650, Batch Gradient Norm: 9.243551811205853
Epoch: 8650, Batch Gradient Norm after: 9.243551811205853
Epoch 8651/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3662338674068451
Epoch: 8651, Batch Gradient Norm: 7.867468549956057
Epoch: 8651, Batch Gradient Norm after: 7.867468549956057
Epoch 8652/10000, Prediction Accuracy = 63.324%, Loss = 0.35973845720291137
Epoch: 8652, Batch Gradient Norm: 8.237683993086174
Epoch: 8652, Batch Gradient Norm after: 8.237683993086174
Epoch 8653/10000, Prediction Accuracy = 63.21400000000001%, Loss = 0.36214483380317686
Epoch: 8653, Batch Gradient Norm: 10.768778798814411
Epoch: 8653, Batch Gradient Norm after: 10.768778798814411
Epoch 8654/10000, Prediction Accuracy = 63.15%, Loss = 0.3791332721710205
Epoch: 8654, Batch Gradient Norm: 13.600061647019025
Epoch: 8654, Batch Gradient Norm after: 13.600061647019025
Epoch 8655/10000, Prediction Accuracy = 62.99400000000001%, Loss = 0.39914538860321047
Epoch: 8655, Batch Gradient Norm: 12.127726910356408
Epoch: 8655, Batch Gradient Norm after: 12.127726910356408
Epoch 8656/10000, Prediction Accuracy = 63.124%, Loss = 0.386928915977478
Epoch: 8656, Batch Gradient Norm: 10.66199568692683
Epoch: 8656, Batch Gradient Norm after: 10.66199568692683
Epoch 8657/10000, Prediction Accuracy = 63.13000000000001%, Loss = 0.3763752281665802
Epoch: 8657, Batch Gradient Norm: 11.709244134787571
Epoch: 8657, Batch Gradient Norm after: 11.709244134787571
Epoch 8658/10000, Prediction Accuracy = 63.29%, Loss = 0.38483336567878723
Epoch: 8658, Batch Gradient Norm: 10.339671697639739
Epoch: 8658, Batch Gradient Norm after: 10.339671697639739
Epoch 8659/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.37495537996292116
Epoch: 8659, Batch Gradient Norm: 8.835885848202402
Epoch: 8659, Batch Gradient Norm after: 8.835885848202402
Epoch 8660/10000, Prediction Accuracy = 63.21199999999999%, Loss = 0.36407727003097534
Epoch: 8660, Batch Gradient Norm: 9.402010141218021
Epoch: 8660, Batch Gradient Norm after: 9.402010141218021
Epoch 8661/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.3674101769924164
Epoch: 8661, Batch Gradient Norm: 10.53465186803871
Epoch: 8661, Batch Gradient Norm after: 10.53465186803871
Epoch 8662/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.3757659614086151
Epoch: 8662, Batch Gradient Norm: 9.666563633850167
Epoch: 8662, Batch Gradient Norm after: 9.666563633850167
Epoch 8663/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.37057084441184995
Epoch: 8663, Batch Gradient Norm: 8.90326771124435
Epoch: 8663, Batch Gradient Norm after: 8.90326771124435
Epoch 8664/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3651890218257904
Epoch: 8664, Batch Gradient Norm: 9.658258486741495
Epoch: 8664, Batch Gradient Norm after: 9.658258486741495
Epoch 8665/10000, Prediction Accuracy = 63.158%, Loss = 0.36974183320999143
Epoch: 8665, Batch Gradient Norm: 10.718927977287555
Epoch: 8665, Batch Gradient Norm after: 10.718927977287555
Epoch 8666/10000, Prediction Accuracy = 63.286%, Loss = 0.3776081442832947
Epoch: 8666, Batch Gradient Norm: 10.718965270199655
Epoch: 8666, Batch Gradient Norm after: 10.718965270199655
Epoch 8667/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3772018730640411
Epoch: 8667, Batch Gradient Norm: 10.389509027157073
Epoch: 8667, Batch Gradient Norm after: 10.389509027157073
Epoch 8668/10000, Prediction Accuracy = 63.25%, Loss = 0.3740088939666748
Epoch: 8668, Batch Gradient Norm: 9.150610293986798
Epoch: 8668, Batch Gradient Norm after: 9.150610293986798
Epoch 8669/10000, Prediction Accuracy = 63.33599999999999%, Loss = 0.3659601092338562
Epoch: 8669, Batch Gradient Norm: 9.712282934203767
Epoch: 8669, Batch Gradient Norm after: 9.712282934203767
Epoch 8670/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.3698228061199188
Epoch: 8670, Batch Gradient Norm: 10.558895151766084
Epoch: 8670, Batch Gradient Norm after: 10.558895151766084
Epoch 8671/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3760952949523926
Epoch: 8671, Batch Gradient Norm: 11.6638772159109
Epoch: 8671, Batch Gradient Norm after: 11.6638772159109
Epoch 8672/10000, Prediction Accuracy = 63.348%, Loss = 0.3845054566860199
Epoch: 8672, Batch Gradient Norm: 10.86084754317427
Epoch: 8672, Batch Gradient Norm after: 10.86084754317427
Epoch 8673/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.37980496883392334
Epoch: 8673, Batch Gradient Norm: 9.537718908117972
Epoch: 8673, Batch Gradient Norm after: 9.537718908117972
Epoch 8674/10000, Prediction Accuracy = 63.362%, Loss = 0.37112361192703247
Epoch: 8674, Batch Gradient Norm: 9.007343773508053
Epoch: 8674, Batch Gradient Norm after: 9.007343773508053
Epoch 8675/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.3665432870388031
Epoch: 8675, Batch Gradient Norm: 9.350757116887854
Epoch: 8675, Batch Gradient Norm after: 9.350757116887854
Epoch 8676/10000, Prediction Accuracy = 63.18399999999999%, Loss = 0.3687714636325836
Epoch: 8676, Batch Gradient Norm: 10.826183326972417
Epoch: 8676, Batch Gradient Norm after: 10.826183326972417
Epoch 8677/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.378179258108139
Epoch: 8677, Batch Gradient Norm: 11.101696950932567
Epoch: 8677, Batch Gradient Norm after: 11.101696950932567
Epoch 8678/10000, Prediction Accuracy = 63.15%, Loss = 0.3801334977149963
Epoch: 8678, Batch Gradient Norm: 9.616308199450348
Epoch: 8678, Batch Gradient Norm after: 9.616308199450348
Epoch 8679/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.37051820755004883
Epoch: 8679, Batch Gradient Norm: 9.086784413736025
Epoch: 8679, Batch Gradient Norm after: 9.086784413736025
Epoch 8680/10000, Prediction Accuracy = 63.05%, Loss = 0.36736798882484434
Epoch: 8680, Batch Gradient Norm: 9.393767639329639
Epoch: 8680, Batch Gradient Norm after: 9.393767639329639
Epoch 8681/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.3690992593765259
Epoch: 8681, Batch Gradient Norm: 10.304878327156446
Epoch: 8681, Batch Gradient Norm after: 10.304878327156446
Epoch 8682/10000, Prediction Accuracy = 63.262%, Loss = 0.3744490027427673
Epoch: 8682, Batch Gradient Norm: 10.124798434035364
Epoch: 8682, Batch Gradient Norm after: 10.124798434035364
Epoch 8683/10000, Prediction Accuracy = 63.188%, Loss = 0.3716058492660522
Epoch: 8683, Batch Gradient Norm: 8.754115672730968
Epoch: 8683, Batch Gradient Norm after: 8.754115672730968
Epoch 8684/10000, Prediction Accuracy = 63.11%, Loss = 0.3631501615047455
Epoch: 8684, Batch Gradient Norm: 8.211693849202335
Epoch: 8684, Batch Gradient Norm after: 8.211693849202335
Epoch 8685/10000, Prediction Accuracy = 63.129999999999995%, Loss = 0.36016464829444883
Epoch: 8685, Batch Gradient Norm: 9.658712217167215
Epoch: 8685, Batch Gradient Norm after: 9.658712217167215
Epoch 8686/10000, Prediction Accuracy = 63.21%, Loss = 0.36800968647003174
Epoch: 8686, Batch Gradient Norm: 12.79268453141054
Epoch: 8686, Batch Gradient Norm after: 12.79268453141054
Epoch 8687/10000, Prediction Accuracy = 63.230000000000004%, Loss = 0.39175283908843994
Epoch: 8687, Batch Gradient Norm: 12.284742436998894
Epoch: 8687, Batch Gradient Norm after: 12.284742436998894
Epoch 8688/10000, Prediction Accuracy = 63.2%, Loss = 0.3885184168815613
Epoch: 8688, Batch Gradient Norm: 9.80689218483798
Epoch: 8688, Batch Gradient Norm after: 9.80689218483798
Epoch 8689/10000, Prediction Accuracy = 63.314%, Loss = 0.3712697684764862
Epoch: 8689, Batch Gradient Norm: 10.424167636827915
Epoch: 8689, Batch Gradient Norm after: 10.424167636827915
Epoch 8690/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.3753006637096405
Epoch: 8690, Batch Gradient Norm: 13.2455288480605
Epoch: 8690, Batch Gradient Norm after: 13.2455288480605
Epoch 8691/10000, Prediction Accuracy = 63.188%, Loss = 0.3943821549415588
Epoch: 8691, Batch Gradient Norm: 12.869285627614405
Epoch: 8691, Batch Gradient Norm after: 12.869285627614405
Epoch 8692/10000, Prediction Accuracy = 63.06999999999999%, Loss = 0.39194687604904177
Epoch: 8692, Batch Gradient Norm: 9.265759655843645
Epoch: 8692, Batch Gradient Norm after: 9.265759655843645
Epoch 8693/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.3674782574176788
Epoch: 8693, Batch Gradient Norm: 9.157329067661859
Epoch: 8693, Batch Gradient Norm after: 9.157329067661859
Epoch 8694/10000, Prediction Accuracy = 63.05800000000001%, Loss = 0.36750097274780275
Epoch: 8694, Batch Gradient Norm: 8.827040735718882
Epoch: 8694, Batch Gradient Norm after: 8.827040735718882
Epoch 8695/10000, Prediction Accuracy = 63.330000000000005%, Loss = 0.36659298539161683
Epoch: 8695, Batch Gradient Norm: 7.993850517490292
Epoch: 8695, Batch Gradient Norm after: 7.993850517490292
Epoch 8696/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3613604843616486
Epoch: 8696, Batch Gradient Norm: 8.214741920889285
Epoch: 8696, Batch Gradient Norm after: 8.214741920889285
Epoch 8697/10000, Prediction Accuracy = 63.34000000000001%, Loss = 0.36201488971710205
Epoch: 8697, Batch Gradient Norm: 9.981288024332407
Epoch: 8697, Batch Gradient Norm after: 9.981288024332407
Epoch 8698/10000, Prediction Accuracy = 63.348%, Loss = 0.37166513204574586
Epoch: 8698, Batch Gradient Norm: 12.484820122304393
Epoch: 8698, Batch Gradient Norm after: 12.484820122304393
Epoch 8699/10000, Prediction Accuracy = 63.25%, Loss = 0.3899251639842987
Epoch: 8699, Batch Gradient Norm: 11.202262928319817
Epoch: 8699, Batch Gradient Norm after: 11.202262928319817
Epoch 8700/10000, Prediction Accuracy = 63.262%, Loss = 0.3806108355522156
Epoch: 8700, Batch Gradient Norm: 9.574122134131063
Epoch: 8700, Batch Gradient Norm after: 9.574122134131063
Epoch 8701/10000, Prediction Accuracy = 63.188%, Loss = 0.36867682337760926
Epoch: 8701, Batch Gradient Norm: 10.01379690612617
Epoch: 8701, Batch Gradient Norm after: 10.01379690612617
Epoch 8702/10000, Prediction Accuracy = 63.17%, Loss = 0.37060742974281313
Epoch: 8702, Batch Gradient Norm: 11.046033929523716
Epoch: 8702, Batch Gradient Norm after: 11.046033929523716
Epoch 8703/10000, Prediction Accuracy = 63.052%, Loss = 0.3780703067779541
Epoch: 8703, Batch Gradient Norm: 10.600508536692935
Epoch: 8703, Batch Gradient Norm after: 10.600508536692935
Epoch 8704/10000, Prediction Accuracy = 63.26800000000001%, Loss = 0.3743340313434601
Epoch: 8704, Batch Gradient Norm: 10.630726103348675
Epoch: 8704, Batch Gradient Norm after: 10.630726103348675
Epoch 8705/10000, Prediction Accuracy = 63.388%, Loss = 0.3749273896217346
Epoch: 8705, Batch Gradient Norm: 9.551363806189132
Epoch: 8705, Batch Gradient Norm after: 9.551363806189132
Epoch 8706/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.3688700020313263
Epoch: 8706, Batch Gradient Norm: 8.050171787491365
Epoch: 8706, Batch Gradient Norm after: 8.050171787491365
Epoch 8707/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.3592048525810242
Epoch: 8707, Batch Gradient Norm: 8.306458744077183
Epoch: 8707, Batch Gradient Norm after: 8.306458744077183
Epoch 8708/10000, Prediction Accuracy = 63.286%, Loss = 0.36075120568275454
Epoch: 8708, Batch Gradient Norm: 9.46486653812836
Epoch: 8708, Batch Gradient Norm after: 9.46486653812836
Epoch 8709/10000, Prediction Accuracy = 63.266%, Loss = 0.3661969184875488
Epoch: 8709, Batch Gradient Norm: 13.285909265432922
Epoch: 8709, Batch Gradient Norm after: 13.285909265432922
Epoch 8710/10000, Prediction Accuracy = 63.213999999999984%, Loss = 0.3943894267082214
Epoch: 8710, Batch Gradient Norm: 11.355012253170395
Epoch: 8710, Batch Gradient Norm after: 11.355012253170395
Epoch 8711/10000, Prediction Accuracy = 63.1%, Loss = 0.38143579959869384
Epoch: 8711, Batch Gradient Norm: 8.397288419891511
Epoch: 8711, Batch Gradient Norm after: 8.397288419891511
Epoch 8712/10000, Prediction Accuracy = 63.36400000000001%, Loss = 0.36211836338043213
Epoch: 8712, Batch Gradient Norm: 8.887947940446686
Epoch: 8712, Batch Gradient Norm after: 8.887947940446686
Epoch 8713/10000, Prediction Accuracy = 63.205999999999996%, Loss = 0.3648726582527161
Epoch: 8713, Batch Gradient Norm: 10.599477724403249
Epoch: 8713, Batch Gradient Norm after: 10.599477724403249
Epoch 8714/10000, Prediction Accuracy = 63.23%, Loss = 0.3768294155597687
Epoch: 8714, Batch Gradient Norm: 10.584257056192746
Epoch: 8714, Batch Gradient Norm after: 10.584257056192746
Epoch 8715/10000, Prediction Accuracy = 63.246%, Loss = 0.37693445682525634
Epoch: 8715, Batch Gradient Norm: 9.247560914044373
Epoch: 8715, Batch Gradient Norm after: 9.247560914044373
Epoch 8716/10000, Prediction Accuracy = 63.324%, Loss = 0.3677479028701782
Epoch: 8716, Batch Gradient Norm: 9.697133749417906
Epoch: 8716, Batch Gradient Norm after: 9.697133749417906
Epoch 8717/10000, Prediction Accuracy = 63.176%, Loss = 0.370605057477951
Epoch: 8717, Batch Gradient Norm: 11.138289425695394
Epoch: 8717, Batch Gradient Norm after: 11.138289425695394
Epoch 8718/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3801775097846985
Epoch: 8718, Batch Gradient Norm: 12.099604908626647
Epoch: 8718, Batch Gradient Norm after: 12.099604908626647
Epoch 8719/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.38767180442810056
Epoch: 8719, Batch Gradient Norm: 10.286131023297761
Epoch: 8719, Batch Gradient Norm after: 10.286131023297761
Epoch 8720/10000, Prediction Accuracy = 63.2%, Loss = 0.3738685607910156
Epoch: 8720, Batch Gradient Norm: 9.661421623026676
Epoch: 8720, Batch Gradient Norm after: 9.661421623026676
Epoch 8721/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.37031651139259336
Epoch: 8721, Batch Gradient Norm: 9.96760625044265
Epoch: 8721, Batch Gradient Norm after: 9.96760625044265
Epoch 8722/10000, Prediction Accuracy = 63.11%, Loss = 0.3721193432807922
Epoch: 8722, Batch Gradient Norm: 11.179329179294808
Epoch: 8722, Batch Gradient Norm after: 11.179329179294808
Epoch 8723/10000, Prediction Accuracy = 63.122%, Loss = 0.38041260838508606
Epoch: 8723, Batch Gradient Norm: 11.066316637892216
Epoch: 8723, Batch Gradient Norm after: 11.066316637892216
Epoch 8724/10000, Prediction Accuracy = 63.25%, Loss = 0.37997070550918577
Epoch: 8724, Batch Gradient Norm: 9.83421962021155
Epoch: 8724, Batch Gradient Norm after: 9.83421962021155
Epoch 8725/10000, Prediction Accuracy = 63.21%, Loss = 0.37082860469818113
Epoch: 8725, Batch Gradient Norm: 9.901547984922761
Epoch: 8725, Batch Gradient Norm after: 9.901547984922761
Epoch 8726/10000, Prediction Accuracy = 63.234%, Loss = 0.3706601619720459
Epoch: 8726, Batch Gradient Norm: 10.203193581964163
Epoch: 8726, Batch Gradient Norm after: 10.203193581964163
Epoch 8727/10000, Prediction Accuracy = 63.30800000000001%, Loss = 0.3719591677188873
Epoch: 8727, Batch Gradient Norm: 9.234189895246391
Epoch: 8727, Batch Gradient Norm after: 9.234189895246391
Epoch 8728/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.36537145376205443
Epoch: 8728, Batch Gradient Norm: 8.848673965031312
Epoch: 8728, Batch Gradient Norm after: 8.848673965031312
Epoch 8729/10000, Prediction Accuracy = 63.348%, Loss = 0.36253499388694765
Epoch: 8729, Batch Gradient Norm: 9.078006325017007
Epoch: 8729, Batch Gradient Norm after: 9.078006325017007
Epoch 8730/10000, Prediction Accuracy = 63.116%, Loss = 0.36356801390647886
Epoch: 8730, Batch Gradient Norm: 11.070244858590845
Epoch: 8730, Batch Gradient Norm after: 11.070244858590845
Epoch 8731/10000, Prediction Accuracy = 63.13399999999999%, Loss = 0.37864680886268615
Epoch: 8731, Batch Gradient Norm: 10.989594549487801
Epoch: 8731, Batch Gradient Norm after: 10.989594549487801
Epoch 8732/10000, Prediction Accuracy = 63.206%, Loss = 0.38079758286476134
Epoch: 8732, Batch Gradient Norm: 9.499016420595296
Epoch: 8732, Batch Gradient Norm after: 9.499016420595296
Epoch 8733/10000, Prediction Accuracy = 63.318%, Loss = 0.3696869254112244
Epoch: 8733, Batch Gradient Norm: 9.757488701625098
Epoch: 8733, Batch Gradient Norm after: 9.757488701625098
Epoch 8734/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3688212811946869
Epoch: 8734, Batch Gradient Norm: 10.99898548137188
Epoch: 8734, Batch Gradient Norm after: 10.99898548137188
Epoch 8735/10000, Prediction Accuracy = 63.20399999999999%, Loss = 0.3754584789276123
Epoch: 8735, Batch Gradient Norm: 12.04341967922047
Epoch: 8735, Batch Gradient Norm after: 12.04341967922047
Epoch 8736/10000, Prediction Accuracy = 63.041999999999994%, Loss = 0.383696448802948
Epoch: 8736, Batch Gradient Norm: 11.320400283606206
Epoch: 8736, Batch Gradient Norm after: 11.320400283606206
Epoch 8737/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3798069417476654
Epoch: 8737, Batch Gradient Norm: 9.712931309824969
Epoch: 8737, Batch Gradient Norm after: 9.712931309824969
Epoch 8738/10000, Prediction Accuracy = 63.18800000000001%, Loss = 0.36967198848724364
Epoch: 8738, Batch Gradient Norm: 8.601728312130941
Epoch: 8738, Batch Gradient Norm after: 8.601728312130941
Epoch 8739/10000, Prediction Accuracy = 63.326%, Loss = 0.3629019379615784
Epoch: 8739, Batch Gradient Norm: 9.02067748266055
Epoch: 8739, Batch Gradient Norm after: 9.02067748266055
Epoch 8740/10000, Prediction Accuracy = 63.164%, Loss = 0.36529929041862486
Epoch: 8740, Batch Gradient Norm: 10.2647605023219
Epoch: 8740, Batch Gradient Norm after: 10.2647605023219
Epoch 8741/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.37294461131095885
Epoch: 8741, Batch Gradient Norm: 10.910113181719051
Epoch: 8741, Batch Gradient Norm after: 10.910113181719051
Epoch 8742/10000, Prediction Accuracy = 63.16400000000001%, Loss = 0.3787642538547516
Epoch: 8742, Batch Gradient Norm: 9.435305636762077
Epoch: 8742, Batch Gradient Norm after: 9.435305636762077
Epoch 8743/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.3677509009838104
Epoch: 8743, Batch Gradient Norm: 10.312346054128344
Epoch: 8743, Batch Gradient Norm after: 10.312346054128344
Epoch 8744/10000, Prediction Accuracy = 63.336%, Loss = 0.37285915613174436
Epoch: 8744, Batch Gradient Norm: 11.478347638705026
Epoch: 8744, Batch Gradient Norm after: 11.478347638705026
Epoch 8745/10000, Prediction Accuracy = 63.3%, Loss = 0.3822776138782501
Epoch: 8745, Batch Gradient Norm: 10.141710406471786
Epoch: 8745, Batch Gradient Norm after: 10.141710406471786
Epoch 8746/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.3726461172103882
Epoch: 8746, Batch Gradient Norm: 8.69104868887115
Epoch: 8746, Batch Gradient Norm after: 8.69104868887115
Epoch 8747/10000, Prediction Accuracy = 63.396%, Loss = 0.3632164537906647
Epoch: 8747, Batch Gradient Norm: 8.67546976773852
Epoch: 8747, Batch Gradient Norm after: 8.67546976773852
Epoch 8748/10000, Prediction Accuracy = 63.30400000000001%, Loss = 0.3619983732700348
Epoch: 8748, Batch Gradient Norm: 11.745484280403911
Epoch: 8748, Batch Gradient Norm after: 11.745484280403911
Epoch 8749/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.38331310749053954
Epoch: 8749, Batch Gradient Norm: 11.079993409827205
Epoch: 8749, Batch Gradient Norm after: 11.079993409827205
Epoch 8750/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3799209654331207
Epoch: 8750, Batch Gradient Norm: 9.562659896017047
Epoch: 8750, Batch Gradient Norm after: 9.562659896017047
Epoch 8751/10000, Prediction Accuracy = 63.306000000000004%, Loss = 0.36897061467170716
Epoch: 8751, Batch Gradient Norm: 12.011620437355912
Epoch: 8751, Batch Gradient Norm after: 12.011620437355912
Epoch 8752/10000, Prediction Accuracy = 63.312%, Loss = 0.38582695722579957
Epoch: 8752, Batch Gradient Norm: 11.491687068001461
Epoch: 8752, Batch Gradient Norm after: 11.491687068001461
Epoch 8753/10000, Prediction Accuracy = 63.226%, Loss = 0.3802708864212036
Epoch: 8753, Batch Gradient Norm: 9.215966750355934
Epoch: 8753, Batch Gradient Norm after: 9.215966750355934
Epoch 8754/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.364132821559906
Epoch: 8754, Batch Gradient Norm: 8.62251221120337
Epoch: 8754, Batch Gradient Norm after: 8.62251221120337
Epoch 8755/10000, Prediction Accuracy = 63.206%, Loss = 0.360798305273056
Epoch: 8755, Batch Gradient Norm: 10.024036638438597
Epoch: 8755, Batch Gradient Norm after: 10.024036638438597
Epoch 8756/10000, Prediction Accuracy = 62.996%, Loss = 0.3707843363285065
Epoch: 8756, Batch Gradient Norm: 10.98411833610455
Epoch: 8756, Batch Gradient Norm after: 10.98411833610455
Epoch 8757/10000, Prediction Accuracy = 63.396%, Loss = 0.3812226176261902
Epoch: 8757, Batch Gradient Norm: 8.772451018632502
Epoch: 8757, Batch Gradient Norm after: 8.772451018632502
Epoch 8758/10000, Prediction Accuracy = 63.232000000000006%, Loss = 0.36441461443901063
Epoch: 8758, Batch Gradient Norm: 8.656606458232533
Epoch: 8758, Batch Gradient Norm after: 8.656606458232533
Epoch 8759/10000, Prediction Accuracy = 63.222%, Loss = 0.36161051988601683
Epoch: 8759, Batch Gradient Norm: 10.61254077685882
Epoch: 8759, Batch Gradient Norm after: 10.61254077685882
Epoch 8760/10000, Prediction Accuracy = 63.266%, Loss = 0.3733613431453705
Epoch: 8760, Batch Gradient Norm: 12.96144123351379
Epoch: 8760, Batch Gradient Norm after: 12.96144123351379
Epoch 8761/10000, Prediction Accuracy = 63.21%, Loss = 0.39185372591018675
Epoch: 8761, Batch Gradient Norm: 11.855512639701702
Epoch: 8761, Batch Gradient Norm after: 11.855512639701702
Epoch 8762/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.38384065628051756
Epoch: 8762, Batch Gradient Norm: 9.741984165141758
Epoch: 8762, Batch Gradient Norm after: 9.741984165141758
Epoch 8763/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.369249564409256
Epoch: 8763, Batch Gradient Norm: 8.074059585179091
Epoch: 8763, Batch Gradient Norm after: 8.074059585179091
Epoch 8764/10000, Prediction Accuracy = 63.367999999999995%, Loss = 0.35899836421012876
Epoch: 8764, Batch Gradient Norm: 8.483102253428804
Epoch: 8764, Batch Gradient Norm after: 8.483102253428804
Epoch 8765/10000, Prediction Accuracy = 63.338%, Loss = 0.36081809401512144
Epoch: 8765, Batch Gradient Norm: 10.520840007535996
Epoch: 8765, Batch Gradient Norm after: 10.520840007535996
Epoch 8766/10000, Prediction Accuracy = 63.17%, Loss = 0.37417635321617126
Epoch: 8766, Batch Gradient Norm: 10.94905501588691
Epoch: 8766, Batch Gradient Norm after: 10.94905501588691
Epoch 8767/10000, Prediction Accuracy = 63.26800000000001%, Loss = 0.3794568955898285
Epoch: 8767, Batch Gradient Norm: 8.759884538404295
Epoch: 8767, Batch Gradient Norm after: 8.759884538404295
Epoch 8768/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.36407155394554136
Epoch: 8768, Batch Gradient Norm: 9.767344595270913
Epoch: 8768, Batch Gradient Norm after: 9.767344595270913
Epoch 8769/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3678468346595764
Epoch: 8769, Batch Gradient Norm: 11.687536704599818
Epoch: 8769, Batch Gradient Norm after: 11.687536704599818
Epoch 8770/10000, Prediction Accuracy = 63.238%, Loss = 0.38123766183853147
Epoch: 8770, Batch Gradient Norm: 10.569986228947396
Epoch: 8770, Batch Gradient Norm after: 10.569986228947396
Epoch 8771/10000, Prediction Accuracy = 63.17%, Loss = 0.37397359013557435
Epoch: 8771, Batch Gradient Norm: 8.097007761551188
Epoch: 8771, Batch Gradient Norm after: 8.097007761551188
Epoch 8772/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.3591994822025299
Epoch: 8772, Batch Gradient Norm: 8.512382784782128
Epoch: 8772, Batch Gradient Norm after: 8.512382784782128
Epoch 8773/10000, Prediction Accuracy = 63.19199999999999%, Loss = 0.36189340949058535
Epoch: 8773, Batch Gradient Norm: 10.507525382018528
Epoch: 8773, Batch Gradient Norm after: 10.507525382018528
Epoch 8774/10000, Prediction Accuracy = 63.31%, Loss = 0.37720767855644227
Epoch: 8774, Batch Gradient Norm: 12.30703484902589
Epoch: 8774, Batch Gradient Norm after: 12.30703484902589
Epoch 8775/10000, Prediction Accuracy = 63.188%, Loss = 0.3875187873840332
Epoch: 8775, Batch Gradient Norm: 13.43650131785775
Epoch: 8775, Batch Gradient Norm after: 13.43650131785775
Epoch 8776/10000, Prediction Accuracy = 63.1%, Loss = 0.39559919834136964
Epoch: 8776, Batch Gradient Norm: 10.115419821776014
Epoch: 8776, Batch Gradient Norm after: 10.115419821776014
Epoch 8777/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.3710725247859955
Epoch: 8777, Batch Gradient Norm: 8.561409953078003
Epoch: 8777, Batch Gradient Norm after: 8.561409953078003
Epoch 8778/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.3609810471534729
Epoch: 8778, Batch Gradient Norm: 10.326480151474753
Epoch: 8778, Batch Gradient Norm after: 10.326480151474753
Epoch 8779/10000, Prediction Accuracy = 63.28000000000001%, Loss = 0.3728897511959076
Epoch: 8779, Batch Gradient Norm: 10.809172594006746
Epoch: 8779, Batch Gradient Norm after: 10.809172594006746
Epoch 8780/10000, Prediction Accuracy = 63.25%, Loss = 0.37699136734008787
Epoch: 8780, Batch Gradient Norm: 8.700213876548554
Epoch: 8780, Batch Gradient Norm after: 8.700213876548554
Epoch 8781/10000, Prediction Accuracy = 63.35%, Loss = 0.36260656714439393
Epoch: 8781, Batch Gradient Norm: 8.053904373921114
Epoch: 8781, Batch Gradient Norm after: 8.053904373921114
Epoch 8782/10000, Prediction Accuracy = 63.242%, Loss = 0.358816534280777
Epoch: 8782, Batch Gradient Norm: 9.798671515937322
Epoch: 8782, Batch Gradient Norm after: 9.798671515937322
Epoch 8783/10000, Prediction Accuracy = 63.222%, Loss = 0.3698339700698853
Epoch: 8783, Batch Gradient Norm: 11.985764622926608
Epoch: 8783, Batch Gradient Norm after: 11.985764622926608
Epoch 8784/10000, Prediction Accuracy = 63.00599999999999%, Loss = 0.38831949830055235
Epoch: 8784, Batch Gradient Norm: 10.157182485305448
Epoch: 8784, Batch Gradient Norm after: 10.157182485305448
Epoch 8785/10000, Prediction Accuracy = 63.318%, Loss = 0.3731046676635742
Epoch: 8785, Batch Gradient Norm: 9.521552919822431
Epoch: 8785, Batch Gradient Norm after: 9.521552919822431
Epoch 8786/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.36663087606430056
Epoch: 8786, Batch Gradient Norm: 11.135325149811656
Epoch: 8786, Batch Gradient Norm after: 11.135325149811656
Epoch 8787/10000, Prediction Accuracy = 63.17999999999999%, Loss = 0.37562994956970214
Epoch: 8787, Batch Gradient Norm: 10.382529159302793
Epoch: 8787, Batch Gradient Norm after: 10.382529159302793
Epoch 8788/10000, Prediction Accuracy = 63.14000000000001%, Loss = 0.37166436314582824
Epoch: 8788, Batch Gradient Norm: 9.081574758382962
Epoch: 8788, Batch Gradient Norm after: 9.081574758382962
Epoch 8789/10000, Prediction Accuracy = 63.372%, Loss = 0.3635894119739532
Epoch: 8789, Batch Gradient Norm: 10.030991270995974
Epoch: 8789, Batch Gradient Norm after: 10.030991270995974
Epoch 8790/10000, Prediction Accuracy = 63.267999999999994%, Loss = 0.36947662830352784
Epoch: 8790, Batch Gradient Norm: 11.937672760674024
Epoch: 8790, Batch Gradient Norm after: 11.937672760674024
Epoch 8791/10000, Prediction Accuracy = 63.186%, Loss = 0.3854186475276947
Epoch: 8791, Batch Gradient Norm: 10.352495125022434
Epoch: 8791, Batch Gradient Norm after: 10.352495125022434
Epoch 8792/10000, Prediction Accuracy = 63.288%, Loss = 0.37603725790977477
Epoch: 8792, Batch Gradient Norm: 8.538572162781083
Epoch: 8792, Batch Gradient Norm after: 8.538572162781083
Epoch 8793/10000, Prediction Accuracy = 63.282%, Loss = 0.3623736321926117
Epoch: 8793, Batch Gradient Norm: 9.139301143070494
Epoch: 8793, Batch Gradient Norm after: 9.139301143070494
Epoch 8794/10000, Prediction Accuracy = 63.47399999999999%, Loss = 0.36521366238594055
Epoch: 8794, Batch Gradient Norm: 11.071037792974556
Epoch: 8794, Batch Gradient Norm after: 11.071037792974556
Epoch 8795/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3782477080821991
Epoch: 8795, Batch Gradient Norm: 11.430869839102442
Epoch: 8795, Batch Gradient Norm after: 11.430869839102442
Epoch 8796/10000, Prediction Accuracy = 63.24799999999999%, Loss = 0.3808803617954254
Epoch: 8796, Batch Gradient Norm: 10.592996410084544
Epoch: 8796, Batch Gradient Norm after: 10.592996410084544
Epoch 8797/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.37447808384895326
Epoch: 8797, Batch Gradient Norm: 10.302355227872019
Epoch: 8797, Batch Gradient Norm after: 10.302355227872019
Epoch 8798/10000, Prediction Accuracy = 63.30400000000001%, Loss = 0.3720092475414276
Epoch: 8798, Batch Gradient Norm: 10.231151492292094
Epoch: 8798, Batch Gradient Norm after: 10.231151492292094
Epoch 8799/10000, Prediction Accuracy = 63.40400000000001%, Loss = 0.37099130153656007
Epoch: 8799, Batch Gradient Norm: 10.911064213761707
Epoch: 8799, Batch Gradient Norm after: 10.911064213761707
Epoch 8800/10000, Prediction Accuracy = 63.278%, Loss = 0.3753529727458954
Epoch: 8800, Batch Gradient Norm: 9.890866747197846
Epoch: 8800, Batch Gradient Norm after: 9.890866747197846
Epoch 8801/10000, Prediction Accuracy = 63.25%, Loss = 0.37008715867996217
Epoch: 8801, Batch Gradient Norm: 8.999325316027633
Epoch: 8801, Batch Gradient Norm after: 8.999325316027633
Epoch 8802/10000, Prediction Accuracy = 63.262%, Loss = 0.36440175771713257
Epoch: 8802, Batch Gradient Norm: 8.75614585470373
Epoch: 8802, Batch Gradient Norm after: 8.75614585470373
Epoch 8803/10000, Prediction Accuracy = 63.31600000000001%, Loss = 0.36227467060089114
Epoch: 8803, Batch Gradient Norm: 9.982838176350228
Epoch: 8803, Batch Gradient Norm after: 9.982838176350228
Epoch 8804/10000, Prediction Accuracy = 63.35799999999999%, Loss = 0.37156686186790466
Epoch: 8804, Batch Gradient Norm: 10.5278237714742
Epoch: 8804, Batch Gradient Norm after: 10.5278237714742
Epoch 8805/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.37587637901306153
Epoch: 8805, Batch Gradient Norm: 11.034695307911324
Epoch: 8805, Batch Gradient Norm after: 11.034695307911324
Epoch 8806/10000, Prediction Accuracy = 63.18000000000001%, Loss = 0.37770071625709534
Epoch: 8806, Batch Gradient Norm: 9.843784597169602
Epoch: 8806, Batch Gradient Norm after: 9.843784597169602
Epoch 8807/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.3696792244911194
Epoch: 8807, Batch Gradient Norm: 8.759383561093125
Epoch: 8807, Batch Gradient Norm after: 8.759383561093125
Epoch 8808/10000, Prediction Accuracy = 63.208000000000006%, Loss = 0.36140066385269165
Epoch: 8808, Batch Gradient Norm: 10.545896436802474
Epoch: 8808, Batch Gradient Norm after: 10.545896436802474
Epoch 8809/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3723200798034668
Epoch: 8809, Batch Gradient Norm: 13.442391454755628
Epoch: 8809, Batch Gradient Norm after: 13.442391454755628
Epoch 8810/10000, Prediction Accuracy = 63.26800000000001%, Loss = 0.3954626441001892
Epoch: 8810, Batch Gradient Norm: 10.811385725645652
Epoch: 8810, Batch Gradient Norm after: 10.811385725645652
Epoch 8811/10000, Prediction Accuracy = 63.331999999999994%, Loss = 0.3765493810176849
Epoch: 8811, Batch Gradient Norm: 8.1192139259839
Epoch: 8811, Batch Gradient Norm after: 8.1192139259839
Epoch 8812/10000, Prediction Accuracy = 63.298%, Loss = 0.3591065526008606
Epoch: 8812, Batch Gradient Norm: 8.423333729777955
Epoch: 8812, Batch Gradient Norm after: 8.423333729777955
Epoch 8813/10000, Prediction Accuracy = 63.362%, Loss = 0.36000497341156007
Epoch: 8813, Batch Gradient Norm: 11.054380741269943
Epoch: 8813, Batch Gradient Norm after: 11.054380741269943
Epoch 8814/10000, Prediction Accuracy = 63.25999999999999%, Loss = 0.3768570303916931
Epoch: 8814, Batch Gradient Norm: 12.281074512228855
Epoch: 8814, Batch Gradient Norm after: 12.281074512228855
Epoch 8815/10000, Prediction Accuracy = 63.206%, Loss = 0.387007349729538
Epoch: 8815, Batch Gradient Norm: 10.656748993942145
Epoch: 8815, Batch Gradient Norm after: 10.656748993942145
Epoch 8816/10000, Prediction Accuracy = 63.26800000000001%, Loss = 0.37474305033683775
Epoch: 8816, Batch Gradient Norm: 9.797877579718827
Epoch: 8816, Batch Gradient Norm after: 9.797877579718827
Epoch 8817/10000, Prediction Accuracy = 63.274%, Loss = 0.3692639470100403
Epoch: 8817, Batch Gradient Norm: 9.811275616261613
Epoch: 8817, Batch Gradient Norm after: 9.811275616261613
Epoch 8818/10000, Prediction Accuracy = 63.28399999999999%, Loss = 0.36869136691093446
Epoch: 8818, Batch Gradient Norm: 10.160363071358978
Epoch: 8818, Batch Gradient Norm after: 10.160363071358978
Epoch 8819/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.3712315857410431
Epoch: 8819, Batch Gradient Norm: 9.196391681979176
Epoch: 8819, Batch Gradient Norm after: 9.196391681979176
Epoch 8820/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.3650260329246521
Epoch: 8820, Batch Gradient Norm: 8.766743107647313
Epoch: 8820, Batch Gradient Norm after: 8.766743107647313
Epoch 8821/10000, Prediction Accuracy = 63.238%, Loss = 0.36258895993232726
Epoch: 8821, Batch Gradient Norm: 9.41889250480769
Epoch: 8821, Batch Gradient Norm after: 9.41889250480769
Epoch 8822/10000, Prediction Accuracy = 63.278%, Loss = 0.36493964195251466
Epoch: 8822, Batch Gradient Norm: 11.164362888354125
Epoch: 8822, Batch Gradient Norm after: 11.164362888354125
Epoch 8823/10000, Prediction Accuracy = 63.21%, Loss = 0.37549155950546265
Epoch: 8823, Batch Gradient Norm: 12.57749096856208
Epoch: 8823, Batch Gradient Norm after: 12.57749096856208
Epoch 8824/10000, Prediction Accuracy = 63.25%, Loss = 0.3879020929336548
Epoch: 8824, Batch Gradient Norm: 10.64884461089736
Epoch: 8824, Batch Gradient Norm after: 10.64884461089736
Epoch 8825/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.37464186549186707
Epoch: 8825, Batch Gradient Norm: 8.932701997069518
Epoch: 8825, Batch Gradient Norm after: 8.932701997069518
Epoch 8826/10000, Prediction Accuracy = 63.33399999999999%, Loss = 0.36326263546943666
Epoch: 8826, Batch Gradient Norm: 9.56503266601255
Epoch: 8826, Batch Gradient Norm after: 9.56503266601255
Epoch 8827/10000, Prediction Accuracy = 63.29%, Loss = 0.36814942955970764
Epoch: 8827, Batch Gradient Norm: 10.008283082049415
Epoch: 8827, Batch Gradient Norm after: 10.008283082049415
Epoch 8828/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.3723220705986023
Epoch: 8828, Batch Gradient Norm: 10.418920821800878
Epoch: 8828, Batch Gradient Norm after: 10.418920821800878
Epoch 8829/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.3751331388950348
Epoch: 8829, Batch Gradient Norm: 11.22284423477161
Epoch: 8829, Batch Gradient Norm after: 11.22284423477161
Epoch 8830/10000, Prediction Accuracy = 63.181999999999995%, Loss = 0.38093348741531374
Epoch: 8830, Batch Gradient Norm: 10.199359189144321
Epoch: 8830, Batch Gradient Norm after: 10.199359189144321
Epoch 8831/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.3720705986022949
Epoch: 8831, Batch Gradient Norm: 8.554253053772305
Epoch: 8831, Batch Gradient Norm after: 8.554253053772305
Epoch 8832/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.36095054149627687
Epoch: 8832, Batch Gradient Norm: 7.779279584534209
Epoch: 8832, Batch Gradient Norm after: 7.779279584534209
Epoch 8833/10000, Prediction Accuracy = 63.19%, Loss = 0.3568856120109558
Epoch: 8833, Batch Gradient Norm: 7.25511603334398
Epoch: 8833, Batch Gradient Norm after: 7.25511603334398
Epoch 8834/10000, Prediction Accuracy = 63.314%, Loss = 0.35414649844169616
Epoch: 8834, Batch Gradient Norm: 9.827319390825231
Epoch: 8834, Batch Gradient Norm after: 9.827319390825231
Epoch 8835/10000, Prediction Accuracy = 63.120000000000005%, Loss = 0.368609744310379
Epoch: 8835, Batch Gradient Norm: 13.053954558809867
Epoch: 8835, Batch Gradient Norm after: 13.053954558809867
Epoch 8836/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3936085760593414
Epoch: 8836, Batch Gradient Norm: 11.174740193312985
Epoch: 8836, Batch Gradient Norm after: 11.174740193312985
Epoch 8837/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.37853400111198426
Epoch: 8837, Batch Gradient Norm: 10.020644381323542
Epoch: 8837, Batch Gradient Norm after: 10.020644381323542
Epoch 8838/10000, Prediction Accuracy = 63.23%, Loss = 0.36980233788490297
Epoch: 8838, Batch Gradient Norm: 9.587939911767332
Epoch: 8838, Batch Gradient Norm after: 9.587939911767332
Epoch 8839/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.3673078536987305
Epoch: 8839, Batch Gradient Norm: 8.35479612988989
Epoch: 8839, Batch Gradient Norm after: 8.35479612988989
Epoch 8840/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3596514344215393
Epoch: 8840, Batch Gradient Norm: 9.204596318437503
Epoch: 8840, Batch Gradient Norm after: 9.204596318437503
Epoch 8841/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.36304324865341187
Epoch: 8841, Batch Gradient Norm: 12.292501328216872
Epoch: 8841, Batch Gradient Norm after: 12.292501328216872
Epoch 8842/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.38441234827041626
Epoch: 8842, Batch Gradient Norm: 11.64431196415611
Epoch: 8842, Batch Gradient Norm after: 11.64431196415611
Epoch 8843/10000, Prediction Accuracy = 63.242000000000004%, Loss = 0.38103561401367186
Epoch: 8843, Batch Gradient Norm: 11.80808491617678
Epoch: 8843, Batch Gradient Norm after: 11.80808491617678
Epoch 8844/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3837918221950531
Epoch: 8844, Batch Gradient Norm: 11.652609346219148
Epoch: 8844, Batch Gradient Norm after: 11.652609346219148
Epoch 8845/10000, Prediction Accuracy = 63.298%, Loss = 0.3823542773723602
Epoch: 8845, Batch Gradient Norm: 10.821541574686721
Epoch: 8845, Batch Gradient Norm after: 10.821541574686721
Epoch 8846/10000, Prediction Accuracy = 63.302%, Loss = 0.3748361587524414
Epoch: 8846, Batch Gradient Norm: 9.478015777991635
Epoch: 8846, Batch Gradient Norm after: 9.478015777991635
Epoch 8847/10000, Prediction Accuracy = 63.396%, Loss = 0.3662001252174377
Epoch: 8847, Batch Gradient Norm: 8.839382458467915
Epoch: 8847, Batch Gradient Norm after: 8.839382458467915
Epoch 8848/10000, Prediction Accuracy = 63.25%, Loss = 0.36285862922668455
Epoch: 8848, Batch Gradient Norm: 9.361289845996591
Epoch: 8848, Batch Gradient Norm after: 9.361289845996591
Epoch 8849/10000, Prediction Accuracy = 63.326%, Loss = 0.365721732378006
Epoch: 8849, Batch Gradient Norm: 11.739632320633268
Epoch: 8849, Batch Gradient Norm after: 11.739632320633268
Epoch 8850/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.3810254275798798
Epoch: 8850, Batch Gradient Norm: 11.540838745673746
Epoch: 8850, Batch Gradient Norm after: 11.540838745673746
Epoch 8851/10000, Prediction Accuracy = 63.081999999999994%, Loss = 0.3814261734485626
Epoch: 8851, Batch Gradient Norm: 8.877402417423479
Epoch: 8851, Batch Gradient Norm after: 8.877402417423479
Epoch 8852/10000, Prediction Accuracy = 63.352%, Loss = 0.36248167753219607
Epoch: 8852, Batch Gradient Norm: 8.264824325002115
Epoch: 8852, Batch Gradient Norm after: 8.264824325002115
Epoch 8853/10000, Prediction Accuracy = 63.338%, Loss = 0.3576592206954956
Epoch: 8853, Batch Gradient Norm: 9.401009708001029
Epoch: 8853, Batch Gradient Norm after: 9.401009708001029
Epoch 8854/10000, Prediction Accuracy = 63.202%, Loss = 0.36424207091331484
Epoch: 8854, Batch Gradient Norm: 10.163977104337938
Epoch: 8854, Batch Gradient Norm after: 10.163977104337938
Epoch 8855/10000, Prediction Accuracy = 63.318%, Loss = 0.36885020732879636
Epoch: 8855, Batch Gradient Norm: 10.536165356523629
Epoch: 8855, Batch Gradient Norm after: 10.536165356523629
Epoch 8856/10000, Prediction Accuracy = 63.402%, Loss = 0.37203567028045653
Epoch: 8856, Batch Gradient Norm: 10.51186843186467
Epoch: 8856, Batch Gradient Norm after: 10.51186843186467
Epoch 8857/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.37304648756980896
Epoch: 8857, Batch Gradient Norm: 10.326720794075083
Epoch: 8857, Batch Gradient Norm after: 10.326720794075083
Epoch 8858/10000, Prediction Accuracy = 63.298%, Loss = 0.37250187397003176
Epoch: 8858, Batch Gradient Norm: 10.63821930850431
Epoch: 8858, Batch Gradient Norm after: 10.63821930850431
Epoch 8859/10000, Prediction Accuracy = 63.282%, Loss = 0.3747744023799896
Epoch: 8859, Batch Gradient Norm: 11.107748067172269
Epoch: 8859, Batch Gradient Norm after: 11.107748067172269
Epoch 8860/10000, Prediction Accuracy = 63.194%, Loss = 0.37757434844970705
Epoch: 8860, Batch Gradient Norm: 9.674929827482355
Epoch: 8860, Batch Gradient Norm after: 9.674929827482355
Epoch 8861/10000, Prediction Accuracy = 63.27%, Loss = 0.36721118092536925
Epoch: 8861, Batch Gradient Norm: 9.263928514632276
Epoch: 8861, Batch Gradient Norm after: 9.263928514632276
Epoch 8862/10000, Prediction Accuracy = 63.16799999999999%, Loss = 0.36474775671958926
Epoch: 8862, Batch Gradient Norm: 9.207696297146539
Epoch: 8862, Batch Gradient Norm after: 9.207696297146539
Epoch 8863/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3639674484729767
Epoch: 8863, Batch Gradient Norm: 12.172328143126784
Epoch: 8863, Batch Gradient Norm after: 12.172328143126784
Epoch 8864/10000, Prediction Accuracy = 63.162%, Loss = 0.3861743986606598
Epoch: 8864, Batch Gradient Norm: 11.343954047304337
Epoch: 8864, Batch Gradient Norm after: 11.343954047304337
Epoch 8865/10000, Prediction Accuracy = 63.116%, Loss = 0.3809646308422089
Epoch: 8865, Batch Gradient Norm: 9.329541709842172
Epoch: 8865, Batch Gradient Norm after: 9.329541709842172
Epoch 8866/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.3657748460769653
Epoch: 8866, Batch Gradient Norm: 10.339554988089995
Epoch: 8866, Batch Gradient Norm after: 10.339554988089995
Epoch 8867/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.3710717141628265
Epoch: 8867, Batch Gradient Norm: 11.621706778723132
Epoch: 8867, Batch Gradient Norm after: 11.621706778723132
Epoch 8868/10000, Prediction Accuracy = 63.212%, Loss = 0.3805123805999756
Epoch: 8868, Batch Gradient Norm: 10.560357989509402
Epoch: 8868, Batch Gradient Norm after: 10.560357989509402
Epoch 8869/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.3735120177268982
Epoch: 8869, Batch Gradient Norm: 8.722625076301604
Epoch: 8869, Batch Gradient Norm after: 8.722625076301604
Epoch 8870/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3622319221496582
Epoch: 8870, Batch Gradient Norm: 7.701026845381258
Epoch: 8870, Batch Gradient Norm after: 7.701026845381258
Epoch 8871/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3558369934558868
Epoch: 8871, Batch Gradient Norm: 8.97039106091534
Epoch: 8871, Batch Gradient Norm after: 8.97039106091534
Epoch 8872/10000, Prediction Accuracy = 63.266%, Loss = 0.3630281388759613
Epoch: 8872, Batch Gradient Norm: 10.850476686076487
Epoch: 8872, Batch Gradient Norm after: 10.850476686076487
Epoch 8873/10000, Prediction Accuracy = 63.233999999999995%, Loss = 0.3770397901535034
Epoch: 8873, Batch Gradient Norm: 11.456409010298792
Epoch: 8873, Batch Gradient Norm after: 11.456409010298792
Epoch 8874/10000, Prediction Accuracy = 63.398%, Loss = 0.381217885017395
Epoch: 8874, Batch Gradient Norm: 11.426762085426198
Epoch: 8874, Batch Gradient Norm after: 11.426762085426198
Epoch 8875/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.37804142832756044
Epoch: 8875, Batch Gradient Norm: 11.812440577271138
Epoch: 8875, Batch Gradient Norm after: 11.812440577271138
Epoch 8876/10000, Prediction Accuracy = 63.158%, Loss = 0.3785641372203827
Epoch: 8876, Batch Gradient Norm: 10.812658635836145
Epoch: 8876, Batch Gradient Norm after: 10.812658635836145
Epoch 8877/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3735403478145599
Epoch: 8877, Batch Gradient Norm: 8.838954664561527
Epoch: 8877, Batch Gradient Norm after: 8.838954664561527
Epoch 8878/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3625457167625427
Epoch: 8878, Batch Gradient Norm: 8.162624008221437
Epoch: 8878, Batch Gradient Norm after: 8.162624008221437
Epoch 8879/10000, Prediction Accuracy = 63.362%, Loss = 0.3596130132675171
Epoch: 8879, Batch Gradient Norm: 9.016202107621195
Epoch: 8879, Batch Gradient Norm after: 9.016202107621195
Epoch 8880/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.36435261368751526
Epoch: 8880, Batch Gradient Norm: 10.624316296826052
Epoch: 8880, Batch Gradient Norm after: 10.624316296826052
Epoch 8881/10000, Prediction Accuracy = 63.386%, Loss = 0.3738544762134552
Epoch: 8881, Batch Gradient Norm: 11.270038167383415
Epoch: 8881, Batch Gradient Norm after: 11.270038167383415
Epoch 8882/10000, Prediction Accuracy = 63.124%, Loss = 0.3779188096523285
Epoch: 8882, Batch Gradient Norm: 9.572481849339889
Epoch: 8882, Batch Gradient Norm after: 9.572481849339889
Epoch 8883/10000, Prediction Accuracy = 63.338%, Loss = 0.3661873161792755
Epoch: 8883, Batch Gradient Norm: 9.300077633522257
Epoch: 8883, Batch Gradient Norm after: 9.300077633522257
Epoch 8884/10000, Prediction Accuracy = 63.25%, Loss = 0.3641523480415344
Epoch: 8884, Batch Gradient Norm: 9.766494028416348
Epoch: 8884, Batch Gradient Norm after: 9.766494028416348
Epoch 8885/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.3684357464313507
Epoch: 8885, Batch Gradient Norm: 10.89879255373339
Epoch: 8885, Batch Gradient Norm after: 10.89879255373339
Epoch 8886/10000, Prediction Accuracy = 63.372%, Loss = 0.37808430194854736
Epoch: 8886, Batch Gradient Norm: 9.5681063380313
Epoch: 8886, Batch Gradient Norm after: 9.5681063380313
Epoch 8887/10000, Prediction Accuracy = 63.45%, Loss = 0.36787827014923097
Epoch: 8887, Batch Gradient Norm: 10.194540225984486
Epoch: 8887, Batch Gradient Norm after: 10.194540225984486
Epoch 8888/10000, Prediction Accuracy = 63.06%, Loss = 0.37020280957221985
Epoch: 8888, Batch Gradient Norm: 10.64889912381088
Epoch: 8888, Batch Gradient Norm after: 10.64889912381088
Epoch 8889/10000, Prediction Accuracy = 63.21600000000001%, Loss = 0.3713689148426056
Epoch: 8889, Batch Gradient Norm: 12.008653135957818
Epoch: 8889, Batch Gradient Norm after: 12.008653135957818
Epoch 8890/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.38104374408721925
Epoch: 8890, Batch Gradient Norm: 11.955895390079068
Epoch: 8890, Batch Gradient Norm after: 11.955895390079068
Epoch 8891/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.38148351907730105
Epoch: 8891, Batch Gradient Norm: 10.09926254393144
Epoch: 8891, Batch Gradient Norm after: 10.09926254393144
Epoch 8892/10000, Prediction Accuracy = 63.266%, Loss = 0.36807642579078675
Epoch: 8892, Batch Gradient Norm: 9.157331735931157
Epoch: 8892, Batch Gradient Norm after: 9.157331735931157
Epoch 8893/10000, Prediction Accuracy = 63.266%, Loss = 0.3618670880794525
Epoch: 8893, Batch Gradient Norm: 9.52955042944995
Epoch: 8893, Batch Gradient Norm after: 9.52955042944995
Epoch 8894/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.363767945766449
Epoch: 8894, Batch Gradient Norm: 9.991262609708523
Epoch: 8894, Batch Gradient Norm after: 9.991262609708523
Epoch 8895/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.36773099899291994
Epoch: 8895, Batch Gradient Norm: 10.011304464305708
Epoch: 8895, Batch Gradient Norm after: 10.011304464305708
Epoch 8896/10000, Prediction Accuracy = 63.234%, Loss = 0.3689257025718689
Epoch: 8896, Batch Gradient Norm: 10.537068268373593
Epoch: 8896, Batch Gradient Norm after: 10.537068268373593
Epoch 8897/10000, Prediction Accuracy = 63.402%, Loss = 0.37386597990989684
Epoch: 8897, Batch Gradient Norm: 11.788317810992393
Epoch: 8897, Batch Gradient Norm after: 11.788317810992393
Epoch 8898/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.38274850249290465
Epoch: 8898, Batch Gradient Norm: 11.289302336269875
Epoch: 8898, Batch Gradient Norm after: 11.289302336269875
Epoch 8899/10000, Prediction Accuracy = 63.334%, Loss = 0.3786557376384735
Epoch: 8899, Batch Gradient Norm: 9.713805935941197
Epoch: 8899, Batch Gradient Norm after: 9.713805935941197
Epoch 8900/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.36822600960731505
Epoch: 8900, Batch Gradient Norm: 8.278469707190666
Epoch: 8900, Batch Gradient Norm after: 8.278469707190666
Epoch 8901/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3591704726219177
Epoch: 8901, Batch Gradient Norm: 7.97012203035015
Epoch: 8901, Batch Gradient Norm after: 7.97012203035015
Epoch 8902/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3569439709186554
Epoch: 8902, Batch Gradient Norm: 9.743419690106794
Epoch: 8902, Batch Gradient Norm after: 9.743419690106794
Epoch 8903/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.36774405241012575
Epoch: 8903, Batch Gradient Norm: 11.200111724586673
Epoch: 8903, Batch Gradient Norm after: 11.200111724586673
Epoch 8904/10000, Prediction Accuracy = 63.39%, Loss = 0.3807034373283386
Epoch: 8904, Batch Gradient Norm: 8.545409720911207
Epoch: 8904, Batch Gradient Norm after: 8.545409720911207
Epoch 8905/10000, Prediction Accuracy = 63.44000000000001%, Loss = 0.36216092109680176
Epoch: 8905, Batch Gradient Norm: 8.242054511124538
Epoch: 8905, Batch Gradient Norm after: 8.242054511124538
Epoch 8906/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.3577767789363861
Epoch: 8906, Batch Gradient Norm: 10.71505145171149
Epoch: 8906, Batch Gradient Norm after: 10.71505145171149
Epoch 8907/10000, Prediction Accuracy = 63.254%, Loss = 0.374006575345993
Epoch: 8907, Batch Gradient Norm: 11.114351501724286
Epoch: 8907, Batch Gradient Norm after: 11.114351501724286
Epoch 8908/10000, Prediction Accuracy = 63.262%, Loss = 0.37851907014846803
Epoch: 8908, Batch Gradient Norm: 11.084578850903721
Epoch: 8908, Batch Gradient Norm after: 11.084578850903721
Epoch 8909/10000, Prediction Accuracy = 63.2%, Loss = 0.37503921389579775
Epoch: 8909, Batch Gradient Norm: 11.238742291256182
Epoch: 8909, Batch Gradient Norm after: 11.238742291256182
Epoch 8910/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.376194167137146
Epoch: 8910, Batch Gradient Norm: 10.665977965194857
Epoch: 8910, Batch Gradient Norm after: 10.665977965194857
Epoch 8911/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.37272494435310366
Epoch: 8911, Batch Gradient Norm: 9.779838018204774
Epoch: 8911, Batch Gradient Norm after: 9.779838018204774
Epoch 8912/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3661637544631958
Epoch: 8912, Batch Gradient Norm: 10.644468389650097
Epoch: 8912, Batch Gradient Norm after: 10.644468389650097
Epoch 8913/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.37151776552200316
Epoch: 8913, Batch Gradient Norm: 11.887777506731991
Epoch: 8913, Batch Gradient Norm after: 11.887777506731991
Epoch 8914/10000, Prediction Accuracy = 63.303999999999995%, Loss = 0.3822973847389221
Epoch: 8914, Batch Gradient Norm: 9.727204763241888
Epoch: 8914, Batch Gradient Norm after: 9.727204763241888
Epoch 8915/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.36801562905311586
Epoch: 8915, Batch Gradient Norm: 8.983221819296693
Epoch: 8915, Batch Gradient Norm after: 8.983221819296693
Epoch 8916/10000, Prediction Accuracy = 63.282%, Loss = 0.3625766456127167
Epoch: 8916, Batch Gradient Norm: 11.68474801201185
Epoch: 8916, Batch Gradient Norm after: 11.68474801201185
Epoch 8917/10000, Prediction Accuracy = 63.352%, Loss = 0.38163046836853026
Epoch: 8917, Batch Gradient Norm: 10.295846169570039
Epoch: 8917, Batch Gradient Norm after: 10.295846169570039
Epoch 8918/10000, Prediction Accuracy = 63.31%, Loss = 0.37311949133872985
Epoch: 8918, Batch Gradient Norm: 8.85775992502004
Epoch: 8918, Batch Gradient Norm after: 8.85775992502004
Epoch 8919/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.36323343515396117
Epoch: 8919, Batch Gradient Norm: 9.548026802558077
Epoch: 8919, Batch Gradient Norm after: 9.548026802558077
Epoch 8920/10000, Prediction Accuracy = 63.45799999999999%, Loss = 0.3669815182685852
Epoch: 8920, Batch Gradient Norm: 10.69305133862297
Epoch: 8920, Batch Gradient Norm after: 10.69305133862297
Epoch 8921/10000, Prediction Accuracy = 63.278%, Loss = 0.3733338057994843
Epoch: 8921, Batch Gradient Norm: 10.223954629120554
Epoch: 8921, Batch Gradient Norm after: 10.223954629120554
Epoch 8922/10000, Prediction Accuracy = 63.43599999999999%, Loss = 0.36963284611701963
Epoch: 8922, Batch Gradient Norm: 9.176676932051693
Epoch: 8922, Batch Gradient Norm after: 9.176676932051693
Epoch 8923/10000, Prediction Accuracy = 63.24000000000001%, Loss = 0.3632729768753052
Epoch: 8923, Batch Gradient Norm: 7.766277318937831
Epoch: 8923, Batch Gradient Norm after: 7.766277318937831
Epoch 8924/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.3552196443080902
Epoch: 8924, Batch Gradient Norm: 8.952278958172666
Epoch: 8924, Batch Gradient Norm after: 8.952278958172666
Epoch 8925/10000, Prediction Accuracy = 63.314%, Loss = 0.3613993525505066
Epoch: 8925, Batch Gradient Norm: 11.764529963036933
Epoch: 8925, Batch Gradient Norm after: 11.764529963036933
Epoch 8926/10000, Prediction Accuracy = 63.21%, Loss = 0.3802445948123932
Epoch: 8926, Batch Gradient Norm: 11.79803910428675
Epoch: 8926, Batch Gradient Norm after: 11.79803910428675
Epoch 8927/10000, Prediction Accuracy = 63.27%, Loss = 0.3807722866535187
Epoch: 8927, Batch Gradient Norm: 11.725011463319763
Epoch: 8927, Batch Gradient Norm after: 11.725011463319763
Epoch 8928/10000, Prediction Accuracy = 63.23599999999999%, Loss = 0.38168455958366393
Epoch: 8928, Batch Gradient Norm: 11.900524602253697
Epoch: 8928, Batch Gradient Norm after: 11.900524602253697
Epoch 8929/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.3810923874378204
Epoch: 8929, Batch Gradient Norm: 11.24220467539713
Epoch: 8929, Batch Gradient Norm after: 11.24220467539713
Epoch 8930/10000, Prediction Accuracy = 63.038%, Loss = 0.3758455991744995
Epoch: 8930, Batch Gradient Norm: 9.419293202264777
Epoch: 8930, Batch Gradient Norm after: 9.419293202264777
Epoch 8931/10000, Prediction Accuracy = 63.306%, Loss = 0.3632641613483429
Epoch: 8931, Batch Gradient Norm: 8.32429267255794
Epoch: 8931, Batch Gradient Norm after: 8.32429267255794
Epoch 8932/10000, Prediction Accuracy = 63.282%, Loss = 0.35660143494606017
Epoch: 8932, Batch Gradient Norm: 8.92931822417243
Epoch: 8932, Batch Gradient Norm after: 8.92931822417243
Epoch 8933/10000, Prediction Accuracy = 63.35%, Loss = 0.3610675811767578
Epoch: 8933, Batch Gradient Norm: 10.632675833494137
Epoch: 8933, Batch Gradient Norm after: 10.632675833494137
Epoch 8934/10000, Prediction Accuracy = 63.298%, Loss = 0.3750536382198334
Epoch: 8934, Batch Gradient Norm: 8.845669981131959
Epoch: 8934, Batch Gradient Norm after: 8.845669981131959
Epoch 8935/10000, Prediction Accuracy = 63.398%, Loss = 0.3639926195144653
Epoch: 8935, Batch Gradient Norm: 8.907784525579597
Epoch: 8935, Batch Gradient Norm after: 8.907784525579597
Epoch 8936/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.36251360177993774
Epoch: 8936, Batch Gradient Norm: 10.31493050494397
Epoch: 8936, Batch Gradient Norm after: 10.31493050494397
Epoch 8937/10000, Prediction Accuracy = 63.262%, Loss = 0.37115386724472044
Epoch: 8937, Batch Gradient Norm: 11.745506741222078
Epoch: 8937, Batch Gradient Norm after: 11.745506741222078
Epoch 8938/10000, Prediction Accuracy = 63.338%, Loss = 0.3814002096652985
Epoch: 8938, Batch Gradient Norm: 10.742859411660405
Epoch: 8938, Batch Gradient Norm after: 10.742859411660405
Epoch 8939/10000, Prediction Accuracy = 63.358000000000004%, Loss = 0.3736890733242035
Epoch: 8939, Batch Gradient Norm: 9.90924756594222
Epoch: 8939, Batch Gradient Norm after: 9.90924756594222
Epoch 8940/10000, Prediction Accuracy = 63.314%, Loss = 0.3683726191520691
Epoch: 8940, Batch Gradient Norm: 10.19293718373555
Epoch: 8940, Batch Gradient Norm after: 10.19293718373555
Epoch 8941/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.3707322120666504
Epoch: 8941, Batch Gradient Norm: 9.540326918962345
Epoch: 8941, Batch Gradient Norm after: 9.540326918962345
Epoch 8942/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.36540243625640867
Epoch: 8942, Batch Gradient Norm: 9.78375145125822
Epoch: 8942, Batch Gradient Norm after: 9.78375145125822
Epoch 8943/10000, Prediction Accuracy = 63.342%, Loss = 0.36709575057029725
Epoch: 8943, Batch Gradient Norm: 10.068442191622763
Epoch: 8943, Batch Gradient Norm after: 10.068442191622763
Epoch 8944/10000, Prediction Accuracy = 63.362%, Loss = 0.3701929450035095
Epoch: 8944, Batch Gradient Norm: 10.592450888242237
Epoch: 8944, Batch Gradient Norm after: 10.592450888242237
Epoch 8945/10000, Prediction Accuracy = 63.376%, Loss = 0.372799152135849
Epoch: 8945, Batch Gradient Norm: 10.911363437656888
Epoch: 8945, Batch Gradient Norm after: 10.911363437656888
Epoch 8946/10000, Prediction Accuracy = 63.388%, Loss = 0.37453685998916625
Epoch: 8946, Batch Gradient Norm: 11.14926990952822
Epoch: 8946, Batch Gradient Norm after: 11.14926990952822
Epoch 8947/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.37558203339576723
Epoch: 8947, Batch Gradient Norm: 11.501827378937131
Epoch: 8947, Batch Gradient Norm after: 11.501827378937131
Epoch 8948/10000, Prediction Accuracy = 63.31%, Loss = 0.3763363301753998
Epoch: 8948, Batch Gradient Norm: 10.437374031911245
Epoch: 8948, Batch Gradient Norm after: 10.437374031911245
Epoch 8949/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.36857813596725464
Epoch: 8949, Batch Gradient Norm: 11.445596530739031
Epoch: 8949, Batch Gradient Norm after: 11.445596530739031
Epoch 8950/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.3760353744029999
Epoch: 8950, Batch Gradient Norm: 11.716975094646777
Epoch: 8950, Batch Gradient Norm after: 11.716975094646777
Epoch 8951/10000, Prediction Accuracy = 63.230000000000004%, Loss = 0.3797054648399353
Epoch: 8951, Batch Gradient Norm: 9.706743450019882
Epoch: 8951, Batch Gradient Norm after: 9.706743450019882
Epoch 8952/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.36632766127586364
Epoch: 8952, Batch Gradient Norm: 8.670447466377919
Epoch: 8952, Batch Gradient Norm after: 8.670447466377919
Epoch 8953/10000, Prediction Accuracy = 63.376%, Loss = 0.35969002842903136
Epoch: 8953, Batch Gradient Norm: 9.536767138787502
Epoch: 8953, Batch Gradient Norm after: 9.536767138787502
Epoch 8954/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.364749950170517
Epoch: 8954, Batch Gradient Norm: 9.867626097259047
Epoch: 8954, Batch Gradient Norm after: 9.867626097259047
Epoch 8955/10000, Prediction Accuracy = 63.388%, Loss = 0.3674838364124298
Epoch: 8955, Batch Gradient Norm: 8.985674869850106
Epoch: 8955, Batch Gradient Norm after: 8.985674869850106
Epoch 8956/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.36166685819625854
Epoch: 8956, Batch Gradient Norm: 8.647301413479084
Epoch: 8956, Batch Gradient Norm after: 8.647301413479084
Epoch 8957/10000, Prediction Accuracy = 63.248000000000005%, Loss = 0.3602136790752411
Epoch: 8957, Batch Gradient Norm: 9.734134378135796
Epoch: 8957, Batch Gradient Norm after: 9.734134378135796
Epoch 8958/10000, Prediction Accuracy = 63.226%, Loss = 0.3679501414299011
Epoch: 8958, Batch Gradient Norm: 10.674405546524492
Epoch: 8958, Batch Gradient Norm after: 10.674405546524492
Epoch 8959/10000, Prediction Accuracy = 63.274%, Loss = 0.37560647130012514
Epoch: 8959, Batch Gradient Norm: 10.408219529409315
Epoch: 8959, Batch Gradient Norm after: 10.408219529409315
Epoch 8960/10000, Prediction Accuracy = 63.388%, Loss = 0.3736226916313171
Epoch: 8960, Batch Gradient Norm: 10.895935338584682
Epoch: 8960, Batch Gradient Norm after: 10.895935338584682
Epoch 8961/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3758342683315277
Epoch: 8961, Batch Gradient Norm: 10.124181254322558
Epoch: 8961, Batch Gradient Norm after: 10.124181254322558
Epoch 8962/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.36938204169273375
Epoch: 8962, Batch Gradient Norm: 9.931455791544789
Epoch: 8962, Batch Gradient Norm after: 9.931455791544789
Epoch 8963/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3669666647911072
Epoch: 8963, Batch Gradient Norm: 10.240558517017163
Epoch: 8963, Batch Gradient Norm after: 10.240558517017163
Epoch 8964/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.36943258047103883
Epoch: 8964, Batch Gradient Norm: 10.451011884585421
Epoch: 8964, Batch Gradient Norm after: 10.451011884585421
Epoch 8965/10000, Prediction Accuracy = 63.376%, Loss = 0.3707824409008026
Epoch: 8965, Batch Gradient Norm: 10.641502552799313
Epoch: 8965, Batch Gradient Norm after: 10.641502552799313
Epoch 8966/10000, Prediction Accuracy = 63.160000000000004%, Loss = 0.36998425126075746
Epoch: 8966, Batch Gradient Norm: 10.806198630490638
Epoch: 8966, Batch Gradient Norm after: 10.806198630490638
Epoch 8967/10000, Prediction Accuracy = 63.27%, Loss = 0.3715275764465332
Epoch: 8967, Batch Gradient Norm: 10.414224409230005
Epoch: 8967, Batch Gradient Norm after: 10.414224409230005
Epoch 8968/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.3705456376075745
Epoch: 8968, Batch Gradient Norm: 8.936388356596638
Epoch: 8968, Batch Gradient Norm after: 8.936388356596638
Epoch 8969/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.3608258545398712
Epoch: 8969, Batch Gradient Norm: 9.117876935026576
Epoch: 8969, Batch Gradient Norm after: 9.117876935026576
Epoch 8970/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.36107178330421447
Epoch: 8970, Batch Gradient Norm: 10.214753493031106
Epoch: 8970, Batch Gradient Norm after: 10.214753493031106
Epoch 8971/10000, Prediction Accuracy = 63.25599999999999%, Loss = 0.3675912082195282
Epoch: 8971, Batch Gradient Norm: 12.289069191316546
Epoch: 8971, Batch Gradient Norm after: 12.289069191316546
Epoch 8972/10000, Prediction Accuracy = 63.04600000000001%, Loss = 0.38283516764640807
Epoch: 8972, Batch Gradient Norm: 13.643479013674982
Epoch: 8972, Batch Gradient Norm after: 13.643479013674982
Epoch 8973/10000, Prediction Accuracy = 63.188%, Loss = 0.3950703203678131
Epoch: 8973, Batch Gradient Norm: 11.264237690272425
Epoch: 8973, Batch Gradient Norm after: 11.264237690272425
Epoch 8974/10000, Prediction Accuracy = 63.224000000000004%, Loss = 0.37784326672554014
Epoch: 8974, Batch Gradient Norm: 8.702791040602966
Epoch: 8974, Batch Gradient Norm after: 8.702791040602966
Epoch 8975/10000, Prediction Accuracy = 63.424%, Loss = 0.3614551365375519
Epoch: 8975, Batch Gradient Norm: 7.947355247525266
Epoch: 8975, Batch Gradient Norm after: 7.947355247525266
Epoch 8976/10000, Prediction Accuracy = 63.44000000000001%, Loss = 0.3570790827274323
Epoch: 8976, Batch Gradient Norm: 7.714557385725511
Epoch: 8976, Batch Gradient Norm after: 7.714557385725511
Epoch 8977/10000, Prediction Accuracy = 63.372%, Loss = 0.3549371063709259
Epoch: 8977, Batch Gradient Norm: 8.640276826034697
Epoch: 8977, Batch Gradient Norm after: 8.640276826034697
Epoch 8978/10000, Prediction Accuracy = 63.33%, Loss = 0.35787631273269654
Epoch: 8978, Batch Gradient Norm: 11.394292598064151
Epoch: 8978, Batch Gradient Norm after: 11.394292598064151
Epoch 8979/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3762996196746826
Epoch: 8979, Batch Gradient Norm: 14.090741137086189
Epoch: 8979, Batch Gradient Norm after: 14.090741137086189
Epoch 8980/10000, Prediction Accuracy = 63.376%, Loss = 0.4005967557430267
Epoch: 8980, Batch Gradient Norm: 12.15934967996559
Epoch: 8980, Batch Gradient Norm after: 12.15934967996559
Epoch 8981/10000, Prediction Accuracy = 63.348%, Loss = 0.3856929659843445
Epoch: 8981, Batch Gradient Norm: 8.671599528033116
Epoch: 8981, Batch Gradient Norm after: 8.671599528033116
Epoch 8982/10000, Prediction Accuracy = 63.35%, Loss = 0.3601714730262756
Epoch: 8982, Batch Gradient Norm: 8.246630530111158
Epoch: 8982, Batch Gradient Norm after: 8.246630530111158
Epoch 8983/10000, Prediction Accuracy = 63.412%, Loss = 0.35695279836654664
Epoch: 8983, Batch Gradient Norm: 9.518209219958653
Epoch: 8983, Batch Gradient Norm after: 9.518209219958653
Epoch 8984/10000, Prediction Accuracy = 63.186%, Loss = 0.3660237193107605
Epoch: 8984, Batch Gradient Norm: 8.402100048074551
Epoch: 8984, Batch Gradient Norm after: 8.402100048074551
Epoch 8985/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.35930153727531433
Epoch: 8985, Batch Gradient Norm: 8.591572752912201
Epoch: 8985, Batch Gradient Norm after: 8.591572752912201
Epoch 8986/10000, Prediction Accuracy = 63.29%, Loss = 0.35935195088386535
Epoch: 8986, Batch Gradient Norm: 10.021271734316649
Epoch: 8986, Batch Gradient Norm after: 10.021271734316649
Epoch 8987/10000, Prediction Accuracy = 63.330000000000005%, Loss = 0.36717161536216736
Epoch: 8987, Batch Gradient Norm: 11.409904069227288
Epoch: 8987, Batch Gradient Norm after: 11.409904069227288
Epoch 8988/10000, Prediction Accuracy = 63.28399999999999%, Loss = 0.3766829609870911
Epoch: 8988, Batch Gradient Norm: 11.175965761542171
Epoch: 8988, Batch Gradient Norm after: 11.175965761542171
Epoch 8989/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.3746228814125061
Epoch: 8989, Batch Gradient Norm: 10.64972369834744
Epoch: 8989, Batch Gradient Norm after: 10.64972369834744
Epoch 8990/10000, Prediction Accuracy = 63.374%, Loss = 0.3721721112728119
Epoch: 8990, Batch Gradient Norm: 9.834407340655606
Epoch: 8990, Batch Gradient Norm after: 9.834407340655606
Epoch 8991/10000, Prediction Accuracy = 63.39200000000001%, Loss = 0.36741023063659667
Epoch: 8991, Batch Gradient Norm: 9.452406305458767
Epoch: 8991, Batch Gradient Norm after: 9.452406305458767
Epoch 8992/10000, Prediction Accuracy = 63.384%, Loss = 0.36406412720680237
Epoch: 8992, Batch Gradient Norm: 10.051155734801739
Epoch: 8992, Batch Gradient Norm after: 10.051155734801739
Epoch 8993/10000, Prediction Accuracy = 63.215999999999994%, Loss = 0.3677228450775146
Epoch: 8993, Batch Gradient Norm: 10.465928390713911
Epoch: 8993, Batch Gradient Norm after: 10.465928390713911
Epoch 8994/10000, Prediction Accuracy = 63.468%, Loss = 0.37068902850151064
Epoch: 8994, Batch Gradient Norm: 10.325806328605688
Epoch: 8994, Batch Gradient Norm after: 10.325806328605688
Epoch 8995/10000, Prediction Accuracy = 63.36%, Loss = 0.37095803022384644
Epoch: 8995, Batch Gradient Norm: 9.934600552982268
Epoch: 8995, Batch Gradient Norm after: 9.934600552982268
Epoch 8996/10000, Prediction Accuracy = 63.45%, Loss = 0.36741229295730593
Epoch: 8996, Batch Gradient Norm: 10.39685819322848
Epoch: 8996, Batch Gradient Norm after: 10.39685819322848
Epoch 8997/10000, Prediction Accuracy = 63.286%, Loss = 0.3698783338069916
Epoch: 8997, Batch Gradient Norm: 10.107177346801848
Epoch: 8997, Batch Gradient Norm after: 10.107177346801848
Epoch 8998/10000, Prediction Accuracy = 63.266%, Loss = 0.36755220890045165
Epoch: 8998, Batch Gradient Norm: 9.998406843508464
Epoch: 8998, Batch Gradient Norm after: 9.998406843508464
Epoch 8999/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3661509931087494
Epoch: 8999, Batch Gradient Norm: 11.52016270586824
Epoch: 8999, Batch Gradient Norm after: 11.52016270586824
Epoch 9000/10000, Prediction Accuracy = 63.23199999999999%, Loss = 0.3764457106590271
Epoch: 9000, Batch Gradient Norm: 11.894966380269537
Epoch: 9000, Batch Gradient Norm after: 11.894966380269537
Epoch 9001/10000, Prediction Accuracy = 63.362%, Loss = 0.3819503903388977
Epoch: 9001, Batch Gradient Norm: 10.336696419247183
Epoch: 9001, Batch Gradient Norm after: 10.336696419247183
Epoch 9002/10000, Prediction Accuracy = 63.444%, Loss = 0.37216113805770873
Epoch: 9002, Batch Gradient Norm: 8.739447379001108
Epoch: 9002, Batch Gradient Norm after: 8.739447379001108
Epoch 9003/10000, Prediction Accuracy = 63.38399999999999%, Loss = 0.36097498536109923
Epoch: 9003, Batch Gradient Norm: 10.23317511181555
Epoch: 9003, Batch Gradient Norm after: 10.23317511181555
Epoch 9004/10000, Prediction Accuracy = 63.33%, Loss = 0.368412709236145
Epoch: 9004, Batch Gradient Norm: 10.972706486442044
Epoch: 9004, Batch Gradient Norm after: 10.972706486442044
Epoch 9005/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.3729635179042816
Epoch: 9005, Batch Gradient Norm: 11.270661326871332
Epoch: 9005, Batch Gradient Norm after: 11.270661326871332
Epoch 9006/10000, Prediction Accuracy = 63.227999999999994%, Loss = 0.37360886931419374
Epoch: 9006, Batch Gradient Norm: 10.057169283849337
Epoch: 9006, Batch Gradient Norm after: 10.057169283849337
Epoch 9007/10000, Prediction Accuracy = 63.220000000000006%, Loss = 0.3672332286834717
Epoch: 9007, Batch Gradient Norm: 8.88227831212148
Epoch: 9007, Batch Gradient Norm after: 8.88227831212148
Epoch 9008/10000, Prediction Accuracy = 63.31600000000001%, Loss = 0.3603801250457764
Epoch: 9008, Batch Gradient Norm: 8.192712633543369
Epoch: 9008, Batch Gradient Norm after: 8.192712633543369
Epoch 9009/10000, Prediction Accuracy = 63.472%, Loss = 0.35765631794929503
Epoch: 9009, Batch Gradient Norm: 7.988624885373161
Epoch: 9009, Batch Gradient Norm after: 7.988624885373161
Epoch 9010/10000, Prediction Accuracy = 63.318%, Loss = 0.35716854929924013
Epoch: 9010, Batch Gradient Norm: 8.159955376338425
Epoch: 9010, Batch Gradient Norm after: 8.159955376338425
Epoch 9011/10000, Prediction Accuracy = 63.414%, Loss = 0.3580071210861206
Epoch: 9011, Batch Gradient Norm: 10.233914592136994
Epoch: 9011, Batch Gradient Norm after: 10.233914592136994
Epoch 9012/10000, Prediction Accuracy = 63.16600000000001%, Loss = 0.36922276616096494
Epoch: 9012, Batch Gradient Norm: 13.941090181872072
Epoch: 9012, Batch Gradient Norm after: 13.941090181872072
Epoch 9013/10000, Prediction Accuracy = 63.20799999999999%, Loss = 0.39591063261032106
Epoch: 9013, Batch Gradient Norm: 12.454666204688047
Epoch: 9013, Batch Gradient Norm after: 12.454666204688047
Epoch 9014/10000, Prediction Accuracy = 63.407999999999994%, Loss = 0.3854902982711792
Epoch: 9014, Batch Gradient Norm: 8.982991702897918
Epoch: 9014, Batch Gradient Norm after: 8.982991702897918
Epoch 9015/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.36118640303611754
Epoch: 9015, Batch Gradient Norm: 8.550725121134276
Epoch: 9015, Batch Gradient Norm after: 8.550725121134276
Epoch 9016/10000, Prediction Accuracy = 63.08200000000001%, Loss = 0.3591127276420593
Epoch: 9016, Batch Gradient Norm: 8.829306884181566
Epoch: 9016, Batch Gradient Norm after: 8.829306884181566
Epoch 9017/10000, Prediction Accuracy = 63.396%, Loss = 0.360562390089035
Epoch: 9017, Batch Gradient Norm: 10.34974189062283
Epoch: 9017, Batch Gradient Norm after: 10.34974189062283
Epoch 9018/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.3706647872924805
Epoch: 9018, Batch Gradient Norm: 10.312849336527323
Epoch: 9018, Batch Gradient Norm after: 10.312849336527323
Epoch 9019/10000, Prediction Accuracy = 63.42%, Loss = 0.3708875298500061
Epoch: 9019, Batch Gradient Norm: 10.499811160945956
Epoch: 9019, Batch Gradient Norm after: 10.499811160945956
Epoch 9020/10000, Prediction Accuracy = 63.315999999999995%, Loss = 0.37153385281562806
Epoch: 9020, Batch Gradient Norm: 10.380615032903897
Epoch: 9020, Batch Gradient Norm after: 10.380615032903897
Epoch 9021/10000, Prediction Accuracy = 63.339999999999996%, Loss = 0.3714817762374878
Epoch: 9021, Batch Gradient Norm: 9.997491844555913
Epoch: 9021, Batch Gradient Norm after: 9.997491844555913
Epoch 9022/10000, Prediction Accuracy = 63.274%, Loss = 0.36690281629562377
Epoch: 9022, Batch Gradient Norm: 11.196206548272569
Epoch: 9022, Batch Gradient Norm after: 11.196206548272569
Epoch 9023/10000, Prediction Accuracy = 63.3%, Loss = 0.3748045861721039
Epoch: 9023, Batch Gradient Norm: 11.609729936878157
Epoch: 9023, Batch Gradient Norm after: 11.609729936878157
Epoch 9024/10000, Prediction Accuracy = 63.364%, Loss = 0.3767702043056488
Epoch: 9024, Batch Gradient Norm: 11.620363015822573
Epoch: 9024, Batch Gradient Norm after: 11.620363015822573
Epoch 9025/10000, Prediction Accuracy = 63.378%, Loss = 0.37823533415794375
Epoch: 9025, Batch Gradient Norm: 11.710187275022953
Epoch: 9025, Batch Gradient Norm after: 11.710187275022953
Epoch 9026/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.37958924770355223
Epoch: 9026, Batch Gradient Norm: 10.247657786072894
Epoch: 9026, Batch Gradient Norm after: 10.247657786072894
Epoch 9027/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.3702163636684418
Epoch: 9027, Batch Gradient Norm: 9.286570766037306
Epoch: 9027, Batch Gradient Norm after: 9.286570766037306
Epoch 9028/10000, Prediction Accuracy = 63.342%, Loss = 0.3626693844795227
Epoch: 9028, Batch Gradient Norm: 8.78839601726143
Epoch: 9028, Batch Gradient Norm after: 8.78839601726143
Epoch 9029/10000, Prediction Accuracy = 63.314%, Loss = 0.3594829082489014
Epoch: 9029, Batch Gradient Norm: 7.795306451409135
Epoch: 9029, Batch Gradient Norm after: 7.795306451409135
Epoch 9030/10000, Prediction Accuracy = 63.374%, Loss = 0.35399208664894105
Epoch: 9030, Batch Gradient Norm: 8.405699451290815
Epoch: 9030, Batch Gradient Norm after: 8.405699451290815
Epoch 9031/10000, Prediction Accuracy = 63.438%, Loss = 0.3558190107345581
Epoch: 9031, Batch Gradient Norm: 11.320915557713201
Epoch: 9031, Batch Gradient Norm after: 11.320915557713201
Epoch 9032/10000, Prediction Accuracy = 63.376%, Loss = 0.37451753616333006
Epoch: 9032, Batch Gradient Norm: 11.844083874337672
Epoch: 9032, Batch Gradient Norm after: 11.844083874337672
Epoch 9033/10000, Prediction Accuracy = 63.45799999999999%, Loss = 0.38012155294418337
Epoch: 9033, Batch Gradient Norm: 10.058538861230318
Epoch: 9033, Batch Gradient Norm after: 10.058538861230318
Epoch 9034/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.36727083921432496
Epoch: 9034, Batch Gradient Norm: 10.620792072078896
Epoch: 9034, Batch Gradient Norm after: 10.620792072078896
Epoch 9035/10000, Prediction Accuracy = 63.384%, Loss = 0.3709504783153534
Epoch: 9035, Batch Gradient Norm: 11.539164114490736
Epoch: 9035, Batch Gradient Norm after: 11.539164114490736
Epoch 9036/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.3776401221752167
Epoch: 9036, Batch Gradient Norm: 9.940017773187023
Epoch: 9036, Batch Gradient Norm after: 9.940017773187023
Epoch 9037/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.36646019816398623
Epoch: 9037, Batch Gradient Norm: 8.66470214444556
Epoch: 9037, Batch Gradient Norm after: 8.66470214444556
Epoch 9038/10000, Prediction Accuracy = 63.31600000000001%, Loss = 0.3584034562110901
Epoch: 9038, Batch Gradient Norm: 9.85949367444188
Epoch: 9038, Batch Gradient Norm after: 9.85949367444188
Epoch 9039/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.36639629006385804
Epoch: 9039, Batch Gradient Norm: 10.749271334926407
Epoch: 9039, Batch Gradient Norm after: 10.749271334926407
Epoch 9040/10000, Prediction Accuracy = 63.056%, Loss = 0.37399824857711794
Epoch: 9040, Batch Gradient Norm: 10.022784856165108
Epoch: 9040, Batch Gradient Norm after: 10.022784856165108
Epoch 9041/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.36836339831352233
Epoch: 9041, Batch Gradient Norm: 10.768521923555351
Epoch: 9041, Batch Gradient Norm after: 10.768521923555351
Epoch 9042/10000, Prediction Accuracy = 63.386%, Loss = 0.37252950072288515
Epoch: 9042, Batch Gradient Norm: 11.299270947605397
Epoch: 9042, Batch Gradient Norm after: 11.299270947605397
Epoch 9043/10000, Prediction Accuracy = 63.372%, Loss = 0.37643728256225584
Epoch: 9043, Batch Gradient Norm: 9.596844780864378
Epoch: 9043, Batch Gradient Norm after: 9.596844780864378
Epoch 9044/10000, Prediction Accuracy = 63.412%, Loss = 0.36488415002822877
Epoch: 9044, Batch Gradient Norm: 8.909185546918433
Epoch: 9044, Batch Gradient Norm after: 8.909185546918433
Epoch 9045/10000, Prediction Accuracy = 63.416%, Loss = 0.35975517630577086
Epoch: 9045, Batch Gradient Norm: 9.173763031678178
Epoch: 9045, Batch Gradient Norm after: 9.173763031678178
Epoch 9046/10000, Prediction Accuracy = 63.262%, Loss = 0.3620780766010284
Epoch: 9046, Batch Gradient Norm: 10.439042481583847
Epoch: 9046, Batch Gradient Norm after: 10.439042481583847
Epoch 9047/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.37027543783187866
Epoch: 9047, Batch Gradient Norm: 11.72266050912702
Epoch: 9047, Batch Gradient Norm after: 11.72266050912702
Epoch 9048/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.38012792468070983
Epoch: 9048, Batch Gradient Norm: 10.310438740864193
Epoch: 9048, Batch Gradient Norm after: 10.310438740864193
Epoch 9049/10000, Prediction Accuracy = 63.372%, Loss = 0.3686559200286865
Epoch: 9049, Batch Gradient Norm: 9.491474684616522
Epoch: 9049, Batch Gradient Norm after: 9.491474684616522
Epoch 9050/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.36139451861381533
Epoch: 9050, Batch Gradient Norm: 11.34514637862115
Epoch: 9050, Batch Gradient Norm after: 11.34514637862115
Epoch 9051/10000, Prediction Accuracy = 63.074%, Loss = 0.37402369976043703
Epoch: 9051, Batch Gradient Norm: 10.833503682038582
Epoch: 9051, Batch Gradient Norm after: 10.833503682038582
Epoch 9052/10000, Prediction Accuracy = 63.366%, Loss = 0.37150918841362
Epoch: 9052, Batch Gradient Norm: 11.076029750633454
Epoch: 9052, Batch Gradient Norm after: 11.076029750633454
Epoch 9053/10000, Prediction Accuracy = 63.436%, Loss = 0.37408186197280885
Epoch: 9053, Batch Gradient Norm: 10.027283885577813
Epoch: 9053, Batch Gradient Norm after: 10.027283885577813
Epoch 9054/10000, Prediction Accuracy = 63.412%, Loss = 0.368345832824707
Epoch: 9054, Batch Gradient Norm: 8.505661558703762
Epoch: 9054, Batch Gradient Norm after: 8.505661558703762
Epoch 9055/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.35765010118484497
Epoch: 9055, Batch Gradient Norm: 9.417761021469946
Epoch: 9055, Batch Gradient Norm after: 9.417761021469946
Epoch 9056/10000, Prediction Accuracy = 63.386%, Loss = 0.363223934173584
Epoch: 9056, Batch Gradient Norm: 9.858185684530348
Epoch: 9056, Batch Gradient Norm after: 9.858185684530348
Epoch 9057/10000, Prediction Accuracy = 63.35%, Loss = 0.3652364373207092
Epoch: 9057, Batch Gradient Norm: 10.167703651168617
Epoch: 9057, Batch Gradient Norm after: 10.167703651168617
Epoch 9058/10000, Prediction Accuracy = 63.348%, Loss = 0.36723793745040895
Epoch: 9058, Batch Gradient Norm: 10.30034390457191
Epoch: 9058, Batch Gradient Norm after: 10.30034390457191
Epoch 9059/10000, Prediction Accuracy = 63.278%, Loss = 0.36799450516700744
Epoch: 9059, Batch Gradient Norm: 9.933712158197144
Epoch: 9059, Batch Gradient Norm after: 9.933712158197144
Epoch 9060/10000, Prediction Accuracy = 63.338%, Loss = 0.3676560580730438
Epoch: 9060, Batch Gradient Norm: 9.45780310601394
Epoch: 9060, Batch Gradient Norm after: 9.45780310601394
Epoch 9061/10000, Prediction Accuracy = 63.424%, Loss = 0.36339210867881777
Epoch: 9061, Batch Gradient Norm: 11.571568788224175
Epoch: 9061, Batch Gradient Norm after: 11.571568788224175
Epoch 9062/10000, Prediction Accuracy = 63.382000000000005%, Loss = 0.3781366586685181
Epoch: 9062, Batch Gradient Norm: 11.94677071409244
Epoch: 9062, Batch Gradient Norm after: 11.94677071409244
Epoch 9063/10000, Prediction Accuracy = 63.504%, Loss = 0.3811724841594696
Epoch: 9063, Batch Gradient Norm: 10.46366679991373
Epoch: 9063, Batch Gradient Norm after: 10.46366679991373
Epoch 9064/10000, Prediction Accuracy = 63.348%, Loss = 0.3691816568374634
Epoch: 9064, Batch Gradient Norm: 9.963456078395568
Epoch: 9064, Batch Gradient Norm after: 9.963456078395568
Epoch 9065/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3668113172054291
Epoch: 9065, Batch Gradient Norm: 9.411074908731566
Epoch: 9065, Batch Gradient Norm after: 9.411074908731566
Epoch 9066/10000, Prediction Accuracy = 63.169999999999995%, Loss = 0.36371114253997805
Epoch: 9066, Batch Gradient Norm: 9.38854269967406
Epoch: 9066, Batch Gradient Norm after: 9.38854269967406
Epoch 9067/10000, Prediction Accuracy = 63.336%, Loss = 0.36457605957984923
Epoch: 9067, Batch Gradient Norm: 10.093522768194806
Epoch: 9067, Batch Gradient Norm after: 10.093522768194806
Epoch 9068/10000, Prediction Accuracy = 63.284000000000006%, Loss = 0.36812966465950014
Epoch: 9068, Batch Gradient Norm: 10.66848479958519
Epoch: 9068, Batch Gradient Norm after: 10.66848479958519
Epoch 9069/10000, Prediction Accuracy = 63.364%, Loss = 0.37139272689819336
Epoch: 9069, Batch Gradient Norm: 10.158828453015364
Epoch: 9069, Batch Gradient Norm after: 10.158828453015364
Epoch 9070/10000, Prediction Accuracy = 63.462%, Loss = 0.366343492269516
Epoch: 9070, Batch Gradient Norm: 10.96057652386772
Epoch: 9070, Batch Gradient Norm after: 10.96057652386772
Epoch 9071/10000, Prediction Accuracy = 63.476%, Loss = 0.3705008864402771
Epoch: 9071, Batch Gradient Norm: 12.407152881540188
Epoch: 9071, Batch Gradient Norm after: 12.407152881540188
Epoch 9072/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.38396084904670713
Epoch: 9072, Batch Gradient Norm: 9.939085163625881
Epoch: 9072, Batch Gradient Norm after: 9.939085163625881
Epoch 9073/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.36666411757469175
Epoch: 9073, Batch Gradient Norm: 8.31855900119808
Epoch: 9073, Batch Gradient Norm after: 8.31855900119808
Epoch 9074/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.3566700279712677
Epoch: 9074, Batch Gradient Norm: 8.083030546043709
Epoch: 9074, Batch Gradient Norm after: 8.083030546043709
Epoch 9075/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.35537135004997256
Epoch: 9075, Batch Gradient Norm: 9.688496603598303
Epoch: 9075, Batch Gradient Norm after: 9.688496603598303
Epoch 9076/10000, Prediction Accuracy = 63.44599999999999%, Loss = 0.36514251828193667
Epoch: 9076, Batch Gradient Norm: 10.876319255002565
Epoch: 9076, Batch Gradient Norm after: 10.876319255002565
Epoch 9077/10000, Prediction Accuracy = 63.263999999999996%, Loss = 0.3737429141998291
Epoch: 9077, Batch Gradient Norm: 9.797702661482036
Epoch: 9077, Batch Gradient Norm after: 9.797702661482036
Epoch 9078/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.36510748863220216
Epoch: 9078, Batch Gradient Norm: 10.937691791421642
Epoch: 9078, Batch Gradient Norm after: 10.937691791421642
Epoch 9079/10000, Prediction Accuracy = 63.378%, Loss = 0.3709591507911682
Epoch: 9079, Batch Gradient Norm: 11.987194091657459
Epoch: 9079, Batch Gradient Norm after: 11.987194091657459
Epoch 9080/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.37984803318977356
Epoch: 9080, Batch Gradient Norm: 11.15079621219847
Epoch: 9080, Batch Gradient Norm after: 11.15079621219847
Epoch 9081/10000, Prediction Accuracy = 63.24399999999999%, Loss = 0.374606853723526
Epoch: 9081, Batch Gradient Norm: 8.861576305482163
Epoch: 9081, Batch Gradient Norm after: 8.861576305482163
Epoch 9082/10000, Prediction Accuracy = 63.366%, Loss = 0.3595398187637329
Epoch: 9082, Batch Gradient Norm: 8.460611275505666
Epoch: 9082, Batch Gradient Norm after: 8.460611275505666
Epoch 9083/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.35663444399833677
Epoch: 9083, Batch Gradient Norm: 9.474305309672923
Epoch: 9083, Batch Gradient Norm after: 9.474305309672923
Epoch 9084/10000, Prediction Accuracy = 63.326%, Loss = 0.3625290811061859
Epoch: 9084, Batch Gradient Norm: 11.514929587374485
Epoch: 9084, Batch Gradient Norm after: 11.514929587374485
Epoch 9085/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.3775525689125061
Epoch: 9085, Batch Gradient Norm: 10.803983278433224
Epoch: 9085, Batch Gradient Norm after: 10.803983278433224
Epoch 9086/10000, Prediction Accuracy = 63.39000000000001%, Loss = 0.3731204986572266
Epoch: 9086, Batch Gradient Norm: 9.461044229925548
Epoch: 9086, Batch Gradient Norm after: 9.461044229925548
Epoch 9087/10000, Prediction Accuracy = 63.343999999999994%, Loss = 0.36482787132263184
Epoch: 9087, Batch Gradient Norm: 8.785174941682907
Epoch: 9087, Batch Gradient Norm after: 8.785174941682907
Epoch 9088/10000, Prediction Accuracy = 63.324%, Loss = 0.3596197783946991
Epoch: 9088, Batch Gradient Norm: 12.143687846921518
Epoch: 9088, Batch Gradient Norm after: 12.143687846921518
Epoch 9089/10000, Prediction Accuracy = 63.172000000000004%, Loss = 0.3820169985294342
Epoch: 9089, Batch Gradient Norm: 12.928990108054478
Epoch: 9089, Batch Gradient Norm after: 12.928990108054478
Epoch 9090/10000, Prediction Accuracy = 63.414%, Loss = 0.38876771926879883
Epoch: 9090, Batch Gradient Norm: 10.27580360636688
Epoch: 9090, Batch Gradient Norm after: 10.27580360636688
Epoch 9091/10000, Prediction Accuracy = 63.416%, Loss = 0.3685522794723511
Epoch: 9091, Batch Gradient Norm: 9.167183191063682
Epoch: 9091, Batch Gradient Norm after: 9.167183191063682
Epoch 9092/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.36081395149230955
Epoch: 9092, Batch Gradient Norm: 10.498690505353242
Epoch: 9092, Batch Gradient Norm after: 10.498690505353242
Epoch 9093/10000, Prediction Accuracy = 63.35%, Loss = 0.368188750743866
Epoch: 9093, Batch Gradient Norm: 11.189724186517614
Epoch: 9093, Batch Gradient Norm after: 11.189724186517614
Epoch 9094/10000, Prediction Accuracy = 63.29200000000001%, Loss = 0.3729264736175537
Epoch: 9094, Batch Gradient Norm: 10.260659524439136
Epoch: 9094, Batch Gradient Norm after: 10.260659524439136
Epoch 9095/10000, Prediction Accuracy = 63.30799999999999%, Loss = 0.3663427114486694
Epoch: 9095, Batch Gradient Norm: 9.977694346932806
Epoch: 9095, Batch Gradient Norm after: 9.977694346932806
Epoch 9096/10000, Prediction Accuracy = 63.364%, Loss = 0.36466761231422423
Epoch: 9096, Batch Gradient Norm: 9.770743630892035
Epoch: 9096, Batch Gradient Norm after: 9.770743630892035
Epoch 9097/10000, Prediction Accuracy = 63.352%, Loss = 0.3637260913848877
Epoch: 9097, Batch Gradient Norm: 9.907582313584026
Epoch: 9097, Batch Gradient Norm after: 9.907582313584026
Epoch 9098/10000, Prediction Accuracy = 63.436%, Loss = 0.36381412148475645
Epoch: 9098, Batch Gradient Norm: 9.885546596669316
Epoch: 9098, Batch Gradient Norm after: 9.885546596669316
Epoch 9099/10000, Prediction Accuracy = 63.35%, Loss = 0.36361828446388245
Epoch: 9099, Batch Gradient Norm: 10.721807127187025
Epoch: 9099, Batch Gradient Norm after: 10.721807127187025
Epoch 9100/10000, Prediction Accuracy = 63.279999999999994%, Loss = 0.37092947363853457
Epoch: 9100, Batch Gradient Norm: 9.948106036853
Epoch: 9100, Batch Gradient Norm after: 9.948106036853
Epoch 9101/10000, Prediction Accuracy = 63.406000000000006%, Loss = 0.36647945642471313
Epoch: 9101, Batch Gradient Norm: 8.97366405631713
Epoch: 9101, Batch Gradient Norm after: 8.97366405631713
Epoch 9102/10000, Prediction Accuracy = 63.39%, Loss = 0.3601185381412506
Epoch: 9102, Batch Gradient Norm: 9.12962055719368
Epoch: 9102, Batch Gradient Norm after: 9.12962055719368
Epoch 9103/10000, Prediction Accuracy = 63.424%, Loss = 0.3615524709224701
Epoch: 9103, Batch Gradient Norm: 9.517882955832894
Epoch: 9103, Batch Gradient Norm after: 9.517882955832894
Epoch 9104/10000, Prediction Accuracy = 63.34000000000001%, Loss = 0.3651330590248108
Epoch: 9104, Batch Gradient Norm: 9.3886030637393
Epoch: 9104, Batch Gradient Norm after: 9.3886030637393
Epoch 9105/10000, Prediction Accuracy = 63.31%, Loss = 0.36377890706062316
Epoch: 9105, Batch Gradient Norm: 11.044873415390136
Epoch: 9105, Batch Gradient Norm after: 11.044873415390136
Epoch 9106/10000, Prediction Accuracy = 63.412%, Loss = 0.37373979687690734
Epoch: 9106, Batch Gradient Norm: 11.894104706766417
Epoch: 9106, Batch Gradient Norm after: 11.894104706766417
Epoch 9107/10000, Prediction Accuracy = 63.384%, Loss = 0.37932937741279604
Epoch: 9107, Batch Gradient Norm: 11.602825196217978
Epoch: 9107, Batch Gradient Norm after: 11.602825196217978
Epoch 9108/10000, Prediction Accuracy = 63.4%, Loss = 0.3769837856292725
Epoch: 9108, Batch Gradient Norm: 11.163937687365431
Epoch: 9108, Batch Gradient Norm after: 11.163937687365431
Epoch 9109/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.3740644514560699
Epoch: 9109, Batch Gradient Norm: 9.533261287788557
Epoch: 9109, Batch Gradient Norm after: 9.533261287788557
Epoch 9110/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3636331558227539
Epoch: 9110, Batch Gradient Norm: 8.752104768079295
Epoch: 9110, Batch Gradient Norm after: 8.752104768079295
Epoch 9111/10000, Prediction Accuracy = 63.376%, Loss = 0.3583555042743683
Epoch: 9111, Batch Gradient Norm: 10.070287352594779
Epoch: 9111, Batch Gradient Norm after: 10.070287352594779
Epoch 9112/10000, Prediction Accuracy = 63.257999999999996%, Loss = 0.3647387742996216
Epoch: 9112, Batch Gradient Norm: 10.584656709505051
Epoch: 9112, Batch Gradient Norm after: 10.584656709505051
Epoch 9113/10000, Prediction Accuracy = 63.374%, Loss = 0.3688542664051056
Epoch: 9113, Batch Gradient Norm: 11.148806675464938
Epoch: 9113, Batch Gradient Norm after: 11.148806675464938
Epoch 9114/10000, Prediction Accuracy = 63.46%, Loss = 0.3727326214313507
Epoch: 9114, Batch Gradient Norm: 10.713143220787078
Epoch: 9114, Batch Gradient Norm after: 10.713143220787078
Epoch 9115/10000, Prediction Accuracy = 63.432%, Loss = 0.37043456435203553
Epoch: 9115, Batch Gradient Norm: 8.609366604852863
Epoch: 9115, Batch Gradient Norm after: 8.609366604852863
Epoch 9116/10000, Prediction Accuracy = 63.452%, Loss = 0.35750455856323243
Epoch: 9116, Batch Gradient Norm: 7.909352760000273
Epoch: 9116, Batch Gradient Norm after: 7.909352760000273
Epoch 9117/10000, Prediction Accuracy = 63.386%, Loss = 0.3542297661304474
Epoch: 9117, Batch Gradient Norm: 8.547907702578616
Epoch: 9117, Batch Gradient Norm after: 8.547907702578616
Epoch 9118/10000, Prediction Accuracy = 63.318000000000005%, Loss = 0.3575342059135437
Epoch: 9118, Batch Gradient Norm: 10.74076551536876
Epoch: 9118, Batch Gradient Norm after: 10.74076551536876
Epoch 9119/10000, Prediction Accuracy = 63.25%, Loss = 0.3713317096233368
Epoch: 9119, Batch Gradient Norm: 10.969992528216492
Epoch: 9119, Batch Gradient Norm after: 10.969992528216492
Epoch 9120/10000, Prediction Accuracy = 63.306%, Loss = 0.37450157999992373
Epoch: 9120, Batch Gradient Norm: 8.428512858547247
Epoch: 9120, Batch Gradient Norm after: 8.428512858547247
Epoch 9121/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.35696589946746826
Epoch: 9121, Batch Gradient Norm: 10.59914792052106
Epoch: 9121, Batch Gradient Norm after: 10.59914792052106
Epoch 9122/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.3695231080055237
Epoch: 9122, Batch Gradient Norm: 13.194948062639007
Epoch: 9122, Batch Gradient Norm after: 13.194948062639007
Epoch 9123/10000, Prediction Accuracy = 63.4%, Loss = 0.39348127841949465
Epoch: 9123, Batch Gradient Norm: 9.883442437600474
Epoch: 9123, Batch Gradient Norm after: 9.883442437600474
Epoch 9124/10000, Prediction Accuracy = 63.354%, Loss = 0.36628893613815305
Epoch: 9124, Batch Gradient Norm: 10.79039398059603
Epoch: 9124, Batch Gradient Norm after: 10.79039398059603
Epoch 9125/10000, Prediction Accuracy = 63.42%, Loss = 0.3701532244682312
Epoch: 9125, Batch Gradient Norm: 12.659881696185547
Epoch: 9125, Batch Gradient Norm after: 12.659881696185547
Epoch 9126/10000, Prediction Accuracy = 63.262%, Loss = 0.38509504199028016
Epoch: 9126, Batch Gradient Norm: 10.504625577654636
Epoch: 9126, Batch Gradient Norm after: 10.504625577654636
Epoch 9127/10000, Prediction Accuracy = 63.46%, Loss = 0.3690778136253357
Epoch: 9127, Batch Gradient Norm: 8.794655859747206
Epoch: 9127, Batch Gradient Norm after: 8.794655859747206
Epoch 9128/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.356608510017395
Epoch: 9128, Batch Gradient Norm: 9.270530303554317
Epoch: 9128, Batch Gradient Norm after: 9.270530303554317
Epoch 9129/10000, Prediction Accuracy = 63.234%, Loss = 0.3588090479373932
Epoch: 9129, Batch Gradient Norm: 9.287313040709384
Epoch: 9129, Batch Gradient Norm after: 9.287313040709384
Epoch 9130/10000, Prediction Accuracy = 63.386%, Loss = 0.35949569940567017
Epoch: 9130, Batch Gradient Norm: 10.083813044762882
Epoch: 9130, Batch Gradient Norm after: 10.083813044762882
Epoch 9131/10000, Prediction Accuracy = 63.36%, Loss = 0.36603490114212034
Epoch: 9131, Batch Gradient Norm: 10.593607996522959
Epoch: 9131, Batch Gradient Norm after: 10.593607996522959
Epoch 9132/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.37107927799224855
Epoch: 9132, Batch Gradient Norm: 10.254561980018936
Epoch: 9132, Batch Gradient Norm after: 10.254561980018936
Epoch 9133/10000, Prediction Accuracy = 63.452%, Loss = 0.3686803936958313
Epoch: 9133, Batch Gradient Norm: 10.625697741262243
Epoch: 9133, Batch Gradient Norm after: 10.625697741262243
Epoch 9134/10000, Prediction Accuracy = 63.275999999999996%, Loss = 0.37087432742118837
Epoch: 9134, Batch Gradient Norm: 10.408392378780743
Epoch: 9134, Batch Gradient Norm after: 10.408392378780743
Epoch 9135/10000, Prediction Accuracy = 63.488%, Loss = 0.36953164339065553
Epoch: 9135, Batch Gradient Norm: 10.352327111130881
Epoch: 9135, Batch Gradient Norm after: 10.352327111130881
Epoch 9136/10000, Prediction Accuracy = 63.40999999999999%, Loss = 0.36902283430099486
Epoch: 9136, Batch Gradient Norm: 9.972969909997433
Epoch: 9136, Batch Gradient Norm after: 9.972969909997433
Epoch 9137/10000, Prediction Accuracy = 63.44%, Loss = 0.3650519073009491
Epoch: 9137, Batch Gradient Norm: 10.874244004610725
Epoch: 9137, Batch Gradient Norm after: 10.874244004610725
Epoch 9138/10000, Prediction Accuracy = 63.42%, Loss = 0.37194313406944274
Epoch: 9138, Batch Gradient Norm: 10.092739982864897
Epoch: 9138, Batch Gradient Norm after: 10.092739982864897
Epoch 9139/10000, Prediction Accuracy = 63.398%, Loss = 0.3674401819705963
Epoch: 9139, Batch Gradient Norm: 9.104363025919994
Epoch: 9139, Batch Gradient Norm after: 9.104363025919994
Epoch 9140/10000, Prediction Accuracy = 63.42%, Loss = 0.3608461618423462
Epoch: 9140, Batch Gradient Norm: 8.713742877630748
Epoch: 9140, Batch Gradient Norm after: 8.713742877630748
Epoch 9141/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.35864495038986205
Epoch: 9141, Batch Gradient Norm: 10.712859792272603
Epoch: 9141, Batch Gradient Norm after: 10.712859792272603
Epoch 9142/10000, Prediction Accuracy = 63.27%, Loss = 0.36938398480415346
Epoch: 9142, Batch Gradient Norm: 11.885017037839793
Epoch: 9142, Batch Gradient Norm after: 11.885017037839793
Epoch 9143/10000, Prediction Accuracy = 63.246%, Loss = 0.3797905147075653
Epoch: 9143, Batch Gradient Norm: 9.328183996355715
Epoch: 9143, Batch Gradient Norm after: 9.328183996355715
Epoch 9144/10000, Prediction Accuracy = 63.528%, Loss = 0.3611135363578796
Epoch: 9144, Batch Gradient Norm: 9.020353626975208
Epoch: 9144, Batch Gradient Norm after: 9.020353626975208
Epoch 9145/10000, Prediction Accuracy = 63.39%, Loss = 0.35787642002105713
Epoch: 9145, Batch Gradient Norm: 11.875245471544593
Epoch: 9145, Batch Gradient Norm after: 11.875245471544593
Epoch 9146/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.3758043467998505
Epoch: 9146, Batch Gradient Norm: 10.980294953644856
Epoch: 9146, Batch Gradient Norm after: 10.980294953644856
Epoch 9147/10000, Prediction Accuracy = 63.334%, Loss = 0.37038655281066896
Epoch: 9147, Batch Gradient Norm: 9.406386384473597
Epoch: 9147, Batch Gradient Norm after: 9.406386384473597
Epoch 9148/10000, Prediction Accuracy = 63.336%, Loss = 0.3610230803489685
Epoch: 9148, Batch Gradient Norm: 10.260931154842725
Epoch: 9148, Batch Gradient Norm after: 10.260931154842725
Epoch 9149/10000, Prediction Accuracy = 63.536%, Loss = 0.3689200639724731
Epoch: 9149, Batch Gradient Norm: 9.618806777562396
Epoch: 9149, Batch Gradient Norm after: 9.618806777562396
Epoch 9150/10000, Prediction Accuracy = 63.456%, Loss = 0.3648801505565643
Epoch: 9150, Batch Gradient Norm: 9.745247187802432
Epoch: 9150, Batch Gradient Norm after: 9.745247187802432
Epoch 9151/10000, Prediction Accuracy = 63.45%, Loss = 0.3644742786884308
Epoch: 9151, Batch Gradient Norm: 11.088743745229083
Epoch: 9151, Batch Gradient Norm after: 11.088743745229083
Epoch 9152/10000, Prediction Accuracy = 63.44000000000001%, Loss = 0.37389642000198364
Epoch: 9152, Batch Gradient Norm: 9.972678696314189
Epoch: 9152, Batch Gradient Norm after: 9.972678696314189
Epoch 9153/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3650643050670624
Epoch: 9153, Batch Gradient Norm: 10.240791111772602
Epoch: 9153, Batch Gradient Norm after: 10.240791111772602
Epoch 9154/10000, Prediction Accuracy = 63.141999999999996%, Loss = 0.36517189145088197
Epoch: 9154, Batch Gradient Norm: 8.571657135746069
Epoch: 9154, Batch Gradient Norm after: 8.571657135746069
Epoch 9155/10000, Prediction Accuracy = 63.251999999999995%, Loss = 0.35598246455192567
Epoch: 9155, Batch Gradient Norm: 8.829045105398649
Epoch: 9155, Batch Gradient Norm after: 8.829045105398649
Epoch 9156/10000, Prediction Accuracy = 63.39200000000001%, Loss = 0.3582971513271332
Epoch: 9156, Batch Gradient Norm: 11.210417915904454
Epoch: 9156, Batch Gradient Norm after: 11.210417915904454
Epoch 9157/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.3753743290901184
Epoch: 9157, Batch Gradient Norm: 10.490033025052657
Epoch: 9157, Batch Gradient Norm after: 10.490033025052657
Epoch 9158/10000, Prediction Accuracy = 63.524%, Loss = 0.37035327553749087
Epoch: 9158, Batch Gradient Norm: 10.767397845090377
Epoch: 9158, Batch Gradient Norm after: 10.767397845090377
Epoch 9159/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.37069106101989746
Epoch: 9159, Batch Gradient Norm: 11.994864857428032
Epoch: 9159, Batch Gradient Norm after: 11.994864857428032
Epoch 9160/10000, Prediction Accuracy = 63.254%, Loss = 0.38079615831375124
Epoch: 9160, Batch Gradient Norm: 11.179536171175009
Epoch: 9160, Batch Gradient Norm after: 11.179536171175009
Epoch 9161/10000, Prediction Accuracy = 63.376%, Loss = 0.37499200701713564
Epoch: 9161, Batch Gradient Norm: 9.843474499231416
Epoch: 9161, Batch Gradient Norm after: 9.843474499231416
Epoch 9162/10000, Prediction Accuracy = 63.378%, Loss = 0.3641498863697052
Epoch: 9162, Batch Gradient Norm: 9.96825340234284
Epoch: 9162, Batch Gradient Norm after: 9.96825340234284
Epoch 9163/10000, Prediction Accuracy = 63.374%, Loss = 0.3637896955013275
Epoch: 9163, Batch Gradient Norm: 11.023842673342376
Epoch: 9163, Batch Gradient Norm after: 11.023842673342376
Epoch 9164/10000, Prediction Accuracy = 63.33%, Loss = 0.36980931758880614
Epoch: 9164, Batch Gradient Norm: 11.764787902210179
Epoch: 9164, Batch Gradient Norm after: 11.764787902210179
Epoch 9165/10000, Prediction Accuracy = 63.36999999999999%, Loss = 0.37524781227111814
Epoch: 9165, Batch Gradient Norm: 10.793337134883435
Epoch: 9165, Batch Gradient Norm after: 10.793337134883435
Epoch 9166/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.3698998153209686
Epoch: 9166, Batch Gradient Norm: 9.790856982938145
Epoch: 9166, Batch Gradient Norm after: 9.790856982938145
Epoch 9167/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.36535831689834597
Epoch: 9167, Batch Gradient Norm: 9.843888718260578
Epoch: 9167, Batch Gradient Norm after: 9.843888718260578
Epoch 9168/10000, Prediction Accuracy = 63.36600000000001%, Loss = 0.3646011233329773
Epoch: 9168, Batch Gradient Norm: 11.021110459181928
Epoch: 9168, Batch Gradient Norm after: 11.021110459181928
Epoch 9169/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.37163782119750977
Epoch: 9169, Batch Gradient Norm: 9.997231816976358
Epoch: 9169, Batch Gradient Norm after: 9.997231816976358
Epoch 9170/10000, Prediction Accuracy = 63.202%, Loss = 0.3640061914920807
Epoch: 9170, Batch Gradient Norm: 8.266020892132213
Epoch: 9170, Batch Gradient Norm after: 8.266020892132213
Epoch 9171/10000, Prediction Accuracy = 63.42%, Loss = 0.3541114866733551
Epoch: 9171, Batch Gradient Norm: 8.263216166191208
Epoch: 9171, Batch Gradient Norm after: 8.263216166191208
Epoch 9172/10000, Prediction Accuracy = 63.396%, Loss = 0.3543414533138275
Epoch: 9172, Batch Gradient Norm: 9.635757380194383
Epoch: 9172, Batch Gradient Norm after: 9.635757380194383
Epoch 9173/10000, Prediction Accuracy = 63.47800000000001%, Loss = 0.364566433429718
Epoch: 9173, Batch Gradient Norm: 8.829727551975838
Epoch: 9173, Batch Gradient Norm after: 8.829727551975838
Epoch 9174/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3589976906776428
Epoch: 9174, Batch Gradient Norm: 9.821267554002013
Epoch: 9174, Batch Gradient Norm after: 9.821267554002013
Epoch 9175/10000, Prediction Accuracy = 63.386%, Loss = 0.36283456683158877
Epoch: 9175, Batch Gradient Norm: 12.362826956300282
Epoch: 9175, Batch Gradient Norm after: 12.362826956300282
Epoch 9176/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.3810878336429596
Epoch: 9176, Batch Gradient Norm: 10.92734570364424
Epoch: 9176, Batch Gradient Norm after: 10.92734570364424
Epoch 9177/10000, Prediction Accuracy = 63.480000000000004%, Loss = 0.3719259977340698
Epoch: 9177, Batch Gradient Norm: 9.353442574680725
Epoch: 9177, Batch Gradient Norm after: 9.353442574680725
Epoch 9178/10000, Prediction Accuracy = 63.496%, Loss = 0.3612270772457123
Epoch: 9178, Batch Gradient Norm: 10.427834967027389
Epoch: 9178, Batch Gradient Norm after: 10.427834967027389
Epoch 9179/10000, Prediction Accuracy = 63.35%, Loss = 0.3686084091663361
Epoch: 9179, Batch Gradient Norm: 10.856443753292314
Epoch: 9179, Batch Gradient Norm after: 10.856443753292314
Epoch 9180/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3718961775302887
Epoch: 9180, Batch Gradient Norm: 9.296694932083735
Epoch: 9180, Batch Gradient Norm after: 9.296694932083735
Epoch 9181/10000, Prediction Accuracy = 63.426%, Loss = 0.3599145174026489
Epoch: 9181, Batch Gradient Norm: 8.87691801948142
Epoch: 9181, Batch Gradient Norm after: 8.87691801948142
Epoch 9182/10000, Prediction Accuracy = 63.302%, Loss = 0.35673401951789857
Epoch: 9182, Batch Gradient Norm: 11.235272028551796
Epoch: 9182, Batch Gradient Norm after: 11.235272028551796
Epoch 9183/10000, Prediction Accuracy = 63.294000000000004%, Loss = 0.37432392239570617
Epoch: 9183, Batch Gradient Norm: 11.973506218000525
Epoch: 9183, Batch Gradient Norm after: 11.973506218000525
Epoch 9184/10000, Prediction Accuracy = 63.338%, Loss = 0.38120232820510863
Epoch: 9184, Batch Gradient Norm: 10.675629688612766
Epoch: 9184, Batch Gradient Norm after: 10.675629688612766
Epoch 9185/10000, Prediction Accuracy = 63.548%, Loss = 0.36982918381690977
Epoch: 9185, Batch Gradient Norm: 9.912214160334289
Epoch: 9185, Batch Gradient Norm after: 9.912214160334289
Epoch 9186/10000, Prediction Accuracy = 63.448%, Loss = 0.3640809178352356
Epoch: 9186, Batch Gradient Norm: 10.33267960175944
Epoch: 9186, Batch Gradient Norm after: 10.33267960175944
Epoch 9187/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3679847538471222
Epoch: 9187, Batch Gradient Norm: 10.506517502974317
Epoch: 9187, Batch Gradient Norm after: 10.506517502974317
Epoch 9188/10000, Prediction Accuracy = 63.262%, Loss = 0.3692300021648407
Epoch: 9188, Batch Gradient Norm: 9.892935536531592
Epoch: 9188, Batch Gradient Norm after: 9.892935536531592
Epoch 9189/10000, Prediction Accuracy = 63.376%, Loss = 0.36462244391441345
Epoch: 9189, Batch Gradient Norm: 9.640417298307993
Epoch: 9189, Batch Gradient Norm after: 9.640417298307993
Epoch 9190/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3621026337146759
Epoch: 9190, Batch Gradient Norm: 9.986715753187568
Epoch: 9190, Batch Gradient Norm after: 9.986715753187568
Epoch 9191/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.36481913924217224
Epoch: 9191, Batch Gradient Norm: 10.14337485053716
Epoch: 9191, Batch Gradient Norm after: 10.14337485053716
Epoch 9192/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.36561129093170164
Epoch: 9192, Batch Gradient Norm: 10.91381679132474
Epoch: 9192, Batch Gradient Norm after: 10.91381679132474
Epoch 9193/10000, Prediction Accuracy = 63.33%, Loss = 0.370465612411499
Epoch: 9193, Batch Gradient Norm: 11.066916440969779
Epoch: 9193, Batch Gradient Norm after: 11.066916440969779
Epoch 9194/10000, Prediction Accuracy = 63.184000000000005%, Loss = 0.3721832275390625
Epoch: 9194, Batch Gradient Norm: 10.137541717741216
Epoch: 9194, Batch Gradient Norm after: 10.137541717741216
Epoch 9195/10000, Prediction Accuracy = 63.362%, Loss = 0.3644417285919189
Epoch: 9195, Batch Gradient Norm: 9.860145196646016
Epoch: 9195, Batch Gradient Norm after: 9.860145196646016
Epoch 9196/10000, Prediction Accuracy = 63.376%, Loss = 0.3627285838127136
Epoch: 9196, Batch Gradient Norm: 8.899193715848927
Epoch: 9196, Batch Gradient Norm after: 8.899193715848927
Epoch 9197/10000, Prediction Accuracy = 63.39200000000001%, Loss = 0.35753175616264343
Epoch: 9197, Batch Gradient Norm: 9.641883715548726
Epoch: 9197, Batch Gradient Norm after: 9.641883715548726
Epoch 9198/10000, Prediction Accuracy = 63.432%, Loss = 0.36164951920509336
Epoch: 9198, Batch Gradient Norm: 12.02475372119948
Epoch: 9198, Batch Gradient Norm after: 12.02475372119948
Epoch 9199/10000, Prediction Accuracy = 63.25%, Loss = 0.37861499190330505
Epoch: 9199, Batch Gradient Norm: 13.400759334808425
Epoch: 9199, Batch Gradient Norm after: 13.400759334808425
Epoch 9200/10000, Prediction Accuracy = 63.334%, Loss = 0.39306690692901614
Epoch: 9200, Batch Gradient Norm: 9.306145491291334
Epoch: 9200, Batch Gradient Norm after: 9.306145491291334
Epoch 9201/10000, Prediction Accuracy = 63.516%, Loss = 0.3626392066478729
Epoch: 9201, Batch Gradient Norm: 7.453656963528692
Epoch: 9201, Batch Gradient Norm after: 7.453656963528692
Epoch 9202/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3504362940788269
Epoch: 9202, Batch Gradient Norm: 8.051491707730232
Epoch: 9202, Batch Gradient Norm after: 8.051491707730232
Epoch 9203/10000, Prediction Accuracy = 63.468%, Loss = 0.3530001938343048
Epoch: 9203, Batch Gradient Norm: 9.4693229284379
Epoch: 9203, Batch Gradient Norm after: 9.4693229284379
Epoch 9204/10000, Prediction Accuracy = 63.34599999999999%, Loss = 0.3618402600288391
Epoch: 9204, Batch Gradient Norm: 10.911354944495649
Epoch: 9204, Batch Gradient Norm after: 10.911354944495649
Epoch 9205/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.37209717035293577
Epoch: 9205, Batch Gradient Norm: 10.233394650772725
Epoch: 9205, Batch Gradient Norm after: 10.233394650772725
Epoch 9206/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3679095506668091
Epoch: 9206, Batch Gradient Norm: 10.288965106450293
Epoch: 9206, Batch Gradient Norm after: 10.288965106450293
Epoch 9207/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3663940727710724
Epoch: 9207, Batch Gradient Norm: 11.306255555392061
Epoch: 9207, Batch Gradient Norm after: 11.306255555392061
Epoch 9208/10000, Prediction Accuracy = 63.11800000000001%, Loss = 0.3711991190910339
Epoch: 9208, Batch Gradient Norm: 11.064277368228847
Epoch: 9208, Batch Gradient Norm after: 11.064277368228847
Epoch 9209/10000, Prediction Accuracy = 63.318%, Loss = 0.3698300838470459
Epoch: 9209, Batch Gradient Norm: 11.106526132797036
Epoch: 9209, Batch Gradient Norm after: 11.106526132797036
Epoch 9210/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3719152629375458
Epoch: 9210, Batch Gradient Norm: 10.327886729422387
Epoch: 9210, Batch Gradient Norm after: 10.327886729422387
Epoch 9211/10000, Prediction Accuracy = 63.496%, Loss = 0.3682672202587128
Epoch: 9211, Batch Gradient Norm: 9.188462922506279
Epoch: 9211, Batch Gradient Norm after: 9.188462922506279
Epoch 9212/10000, Prediction Accuracy = 63.46400000000001%, Loss = 0.3615266680717468
Epoch: 9212, Batch Gradient Norm: 9.186452591757325
Epoch: 9212, Batch Gradient Norm after: 9.186452591757325
Epoch 9213/10000, Prediction Accuracy = 63.46%, Loss = 0.36078662276268003
Epoch: 9213, Batch Gradient Norm: 9.279272937007324
Epoch: 9213, Batch Gradient Norm after: 9.279272937007324
Epoch 9214/10000, Prediction Accuracy = 63.242%, Loss = 0.3582324802875519
Epoch: 9214, Batch Gradient Norm: 10.703385455692478
Epoch: 9214, Batch Gradient Norm after: 10.703385455692478
Epoch 9215/10000, Prediction Accuracy = 63.422000000000004%, Loss = 0.36634632349014284
Epoch: 9215, Batch Gradient Norm: 13.838434835936843
Epoch: 9215, Batch Gradient Norm after: 13.838434835936843
Epoch 9216/10000, Prediction Accuracy = 63.364%, Loss = 0.39146904945373534
Epoch: 9216, Batch Gradient Norm: 10.853245053988275
Epoch: 9216, Batch Gradient Norm after: 10.853245053988275
Epoch 9217/10000, Prediction Accuracy = 63.5%, Loss = 0.37050636410713195
Epoch: 9217, Batch Gradient Norm: 8.034029253436529
Epoch: 9217, Batch Gradient Norm after: 8.034029253436529
Epoch 9218/10000, Prediction Accuracy = 63.604%, Loss = 0.35298146605491637
Epoch: 9218, Batch Gradient Norm: 9.299149116663926
Epoch: 9218, Batch Gradient Norm after: 9.299149116663926
Epoch 9219/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.3621839165687561
Epoch: 9219, Batch Gradient Norm: 9.17063447899681
Epoch: 9219, Batch Gradient Norm after: 9.17063447899681
Epoch 9220/10000, Prediction Accuracy = 63.628%, Loss = 0.3613894283771515
Epoch: 9220, Batch Gradient Norm: 8.529170735070284
Epoch: 9220, Batch Gradient Norm after: 8.529170735070284
Epoch 9221/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.35635423064231875
Epoch: 9221, Batch Gradient Norm: 10.16708075521024
Epoch: 9221, Batch Gradient Norm after: 10.16708075521024
Epoch 9222/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3660337388515472
Epoch: 9222, Batch Gradient Norm: 12.179455975402805
Epoch: 9222, Batch Gradient Norm after: 12.179455975402805
Epoch 9223/10000, Prediction Accuracy = 63.076%, Loss = 0.3809415876865387
Epoch: 9223, Batch Gradient Norm: 13.240879159843198
Epoch: 9223, Batch Gradient Norm after: 13.240879159843198
Epoch 9224/10000, Prediction Accuracy = 63.26400000000001%, Loss = 0.3890374839305878
Epoch: 9224, Batch Gradient Norm: 11.024487577900668
Epoch: 9224, Batch Gradient Norm after: 11.024487577900668
Epoch 9225/10000, Prediction Accuracy = 63.53000000000001%, Loss = 0.3709379851818085
Epoch: 9225, Batch Gradient Norm: 9.4816798613492
Epoch: 9225, Batch Gradient Norm after: 9.4816798613492
Epoch 9226/10000, Prediction Accuracy = 63.55%, Loss = 0.3606040894985199
Epoch: 9226, Batch Gradient Norm: 9.343503874960678
Epoch: 9226, Batch Gradient Norm after: 9.343503874960678
Epoch 9227/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3600857198238373
Epoch: 9227, Batch Gradient Norm: 9.669091481624873
Epoch: 9227, Batch Gradient Norm after: 9.669091481624873
Epoch 9228/10000, Prediction Accuracy = 63.352%, Loss = 0.36161585450172423
Epoch: 9228, Batch Gradient Norm: 9.553941108348608
Epoch: 9228, Batch Gradient Norm after: 9.553941108348608
Epoch 9229/10000, Prediction Accuracy = 63.352%, Loss = 0.36108863949775694
Epoch: 9229, Batch Gradient Norm: 10.112105550708005
Epoch: 9229, Batch Gradient Norm after: 10.112105550708005
Epoch 9230/10000, Prediction Accuracy = 63.412%, Loss = 0.36400154829025266
Epoch: 9230, Batch Gradient Norm: 10.167175717584707
Epoch: 9230, Batch Gradient Norm after: 10.167175717584707
Epoch 9231/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.3646623373031616
Epoch: 9231, Batch Gradient Norm: 9.9432426745781
Epoch: 9231, Batch Gradient Norm after: 9.9432426745781
Epoch 9232/10000, Prediction Accuracy = 63.372%, Loss = 0.3632577061653137
Epoch: 9232, Batch Gradient Norm: 8.99580881738304
Epoch: 9232, Batch Gradient Norm after: 8.99580881738304
Epoch 9233/10000, Prediction Accuracy = 63.438%, Loss = 0.35801677107810975
Epoch: 9233, Batch Gradient Norm: 9.451100405990003
Epoch: 9233, Batch Gradient Norm after: 9.451100405990003
Epoch 9234/10000, Prediction Accuracy = 63.35%, Loss = 0.36142433881759645
Epoch: 9234, Batch Gradient Norm: 9.855469561996845
Epoch: 9234, Batch Gradient Norm after: 9.855469561996845
Epoch 9235/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.3642552554607391
Epoch: 9235, Batch Gradient Norm: 9.743456204745646
Epoch: 9235, Batch Gradient Norm after: 9.743456204745646
Epoch 9236/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3633289039134979
Epoch: 9236, Batch Gradient Norm: 9.922227806339581
Epoch: 9236, Batch Gradient Norm after: 9.922227806339581
Epoch 9237/10000, Prediction Accuracy = 63.480000000000004%, Loss = 0.3642653703689575
Epoch: 9237, Batch Gradient Norm: 9.514421034890235
Epoch: 9237, Batch Gradient Norm after: 9.514421034890235
Epoch 9238/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3620203971862793
Epoch: 9238, Batch Gradient Norm: 10.155871569050763
Epoch: 9238, Batch Gradient Norm after: 10.155871569050763
Epoch 9239/10000, Prediction Accuracy = 63.272000000000006%, Loss = 0.3670833706855774
Epoch: 9239, Batch Gradient Norm: 12.102268277243622
Epoch: 9239, Batch Gradient Norm after: 12.102268277243622
Epoch 9240/10000, Prediction Accuracy = 63.524%, Loss = 0.38081454038619994
Epoch: 9240, Batch Gradient Norm: 11.360989451431758
Epoch: 9240, Batch Gradient Norm after: 11.360989451431758
Epoch 9241/10000, Prediction Accuracy = 63.260000000000005%, Loss = 0.3753054618835449
Epoch: 9241, Batch Gradient Norm: 8.896459593416594
Epoch: 9241, Batch Gradient Norm after: 8.896459593416594
Epoch 9242/10000, Prediction Accuracy = 63.424%, Loss = 0.358920282125473
Epoch: 9242, Batch Gradient Norm: 9.15529211379544
Epoch: 9242, Batch Gradient Norm after: 9.15529211379544
Epoch 9243/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3595897674560547
Epoch: 9243, Batch Gradient Norm: 11.3018881389341
Epoch: 9243, Batch Gradient Norm after: 11.3018881389341
Epoch 9244/10000, Prediction Accuracy = 63.376%, Loss = 0.37435362339019773
Epoch: 9244, Batch Gradient Norm: 11.551392767758212
Epoch: 9244, Batch Gradient Norm after: 11.551392767758212
Epoch 9245/10000, Prediction Accuracy = 63.196000000000005%, Loss = 0.3750866711139679
Epoch: 9245, Batch Gradient Norm: 11.184356339346463
Epoch: 9245, Batch Gradient Norm after: 11.184356339346463
Epoch 9246/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.3730132818222046
Epoch: 9246, Batch Gradient Norm: 11.390566735902608
Epoch: 9246, Batch Gradient Norm after: 11.390566735902608
Epoch 9247/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3745603621006012
Epoch: 9247, Batch Gradient Norm: 10.56901831767459
Epoch: 9247, Batch Gradient Norm after: 10.56901831767459
Epoch 9248/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.36682721972465515
Epoch: 9248, Batch Gradient Norm: 10.214629966575885
Epoch: 9248, Batch Gradient Norm after: 10.214629966575885
Epoch 9249/10000, Prediction Accuracy = 63.46999999999999%, Loss = 0.36353227496147156
Epoch: 9249, Batch Gradient Norm: 10.49308271828646
Epoch: 9249, Batch Gradient Norm after: 10.49308271828646
Epoch 9250/10000, Prediction Accuracy = 63.482000000000006%, Loss = 0.36602977514266966
Epoch: 9250, Batch Gradient Norm: 9.579923025164867
Epoch: 9250, Batch Gradient Norm after: 9.579923025164867
Epoch 9251/10000, Prediction Accuracy = 63.48%, Loss = 0.3605575680732727
Epoch: 9251, Batch Gradient Norm: 8.71318543838432
Epoch: 9251, Batch Gradient Norm after: 8.71318543838432
Epoch 9252/10000, Prediction Accuracy = 63.576%, Loss = 0.35552127957344054
Epoch: 9252, Batch Gradient Norm: 9.420226743222075
Epoch: 9252, Batch Gradient Norm after: 9.420226743222075
Epoch 9253/10000, Prediction Accuracy = 63.291999999999994%, Loss = 0.3593805432319641
Epoch: 9253, Batch Gradient Norm: 10.284482051042765
Epoch: 9253, Batch Gradient Norm after: 10.284482051042765
Epoch 9254/10000, Prediction Accuracy = 63.438%, Loss = 0.3647615373134613
Epoch: 9254, Batch Gradient Norm: 11.07899187287699
Epoch: 9254, Batch Gradient Norm after: 11.07899187287699
Epoch 9255/10000, Prediction Accuracy = 63.56%, Loss = 0.3712144434452057
Epoch: 9255, Batch Gradient Norm: 9.659061747913734
Epoch: 9255, Batch Gradient Norm after: 9.659061747913734
Epoch 9256/10000, Prediction Accuracy = 63.388%, Loss = 0.3635996639728546
Epoch: 9256, Batch Gradient Norm: 8.686674111551383
Epoch: 9256, Batch Gradient Norm after: 8.686674111551383
Epoch 9257/10000, Prediction Accuracy = 63.428%, Loss = 0.3561138272285461
Epoch: 9257, Batch Gradient Norm: 9.62362760026713
Epoch: 9257, Batch Gradient Norm after: 9.62362760026713
Epoch 9258/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.3615878999233246
Epoch: 9258, Batch Gradient Norm: 11.377143099475099
Epoch: 9258, Batch Gradient Norm after: 11.377143099475099
Epoch 9259/10000, Prediction Accuracy = 63.35600000000001%, Loss = 0.374209326505661
Epoch: 9259, Batch Gradient Norm: 10.745590457907964
Epoch: 9259, Batch Gradient Norm after: 10.745590457907964
Epoch 9260/10000, Prediction Accuracy = 63.330000000000005%, Loss = 0.37080820798873904
Epoch: 9260, Batch Gradient Norm: 8.976348629480466
Epoch: 9260, Batch Gradient Norm after: 8.976348629480466
Epoch 9261/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.3593815088272095
Epoch: 9261, Batch Gradient Norm: 9.658138421020524
Epoch: 9261, Batch Gradient Norm after: 9.658138421020524
Epoch 9262/10000, Prediction Accuracy = 63.58%, Loss = 0.3626022279262543
Epoch: 9262, Batch Gradient Norm: 11.281152620520272
Epoch: 9262, Batch Gradient Norm after: 11.281152620520272
Epoch 9263/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.37375891804695127
Epoch: 9263, Batch Gradient Norm: 10.245684833803914
Epoch: 9263, Batch Gradient Norm after: 10.245684833803914
Epoch 9264/10000, Prediction Accuracy = 63.488%, Loss = 0.36534573435783385
Epoch: 9264, Batch Gradient Norm: 9.62785449682396
Epoch: 9264, Batch Gradient Norm after: 9.62785449682396
Epoch 9265/10000, Prediction Accuracy = 63.436%, Loss = 0.3603210747241974
Epoch: 9265, Batch Gradient Norm: 9.81039616454266
Epoch: 9265, Batch Gradient Norm after: 9.81039616454266
Epoch 9266/10000, Prediction Accuracy = 63.352%, Loss = 0.3603819012641907
Epoch: 9266, Batch Gradient Norm: 9.626782849469596
Epoch: 9266, Batch Gradient Norm after: 9.626782849469596
Epoch 9267/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3608324706554413
Epoch: 9267, Batch Gradient Norm: 11.42981964599642
Epoch: 9267, Batch Gradient Norm after: 11.42981964599642
Epoch 9268/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.3728678643703461
Epoch: 9268, Batch Gradient Norm: 11.478489842890433
Epoch: 9268, Batch Gradient Norm after: 11.478489842890433
Epoch 9269/10000, Prediction Accuracy = 63.536%, Loss = 0.37356874346733093
Epoch: 9269, Batch Gradient Norm: 12.587086921843682
Epoch: 9269, Batch Gradient Norm after: 12.587086921843682
Epoch 9270/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.38358855843544004
Epoch: 9270, Batch Gradient Norm: 11.737356552593651
Epoch: 9270, Batch Gradient Norm after: 11.737356552593651
Epoch 9271/10000, Prediction Accuracy = 63.298%, Loss = 0.3776509761810303
Epoch: 9271, Batch Gradient Norm: 9.115818430287053
Epoch: 9271, Batch Gradient Norm after: 9.115818430287053
Epoch 9272/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.3572114944458008
Epoch: 9272, Batch Gradient Norm: 8.600575675164265
Epoch: 9272, Batch Gradient Norm after: 8.600575675164265
Epoch 9273/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3532004117965698
Epoch: 9273, Batch Gradient Norm: 9.75263831030876
Epoch: 9273, Batch Gradient Norm after: 9.75263831030876
Epoch 9274/10000, Prediction Accuracy = 63.239999999999995%, Loss = 0.36034932136535647
Epoch: 9274, Batch Gradient Norm: 10.84343614564302
Epoch: 9274, Batch Gradient Norm after: 10.84343614564302
Epoch 9275/10000, Prediction Accuracy = 63.35799999999999%, Loss = 0.3702537834644318
Epoch: 9275, Batch Gradient Norm: 11.07373873304078
Epoch: 9275, Batch Gradient Norm after: 11.07373873304078
Epoch 9276/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3729120194911957
Epoch: 9276, Batch Gradient Norm: 10.154096424775762
Epoch: 9276, Batch Gradient Norm after: 10.154096424775762
Epoch 9277/10000, Prediction Accuracy = 63.362%, Loss = 0.3655740559101105
Epoch: 9277, Batch Gradient Norm: 9.888366654168824
Epoch: 9277, Batch Gradient Norm after: 9.888366654168824
Epoch 9278/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.36258000135421753
Epoch: 9278, Batch Gradient Norm: 10.350695281920157
Epoch: 9278, Batch Gradient Norm after: 10.350695281920157
Epoch 9279/10000, Prediction Accuracy = 63.428%, Loss = 0.36577760577201845
Epoch: 9279, Batch Gradient Norm: 10.381079997027479
Epoch: 9279, Batch Gradient Norm after: 10.381079997027479
Epoch 9280/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3666126787662506
Epoch: 9280, Batch Gradient Norm: 10.3234706128619
Epoch: 9280, Batch Gradient Norm after: 10.3234706128619
Epoch 9281/10000, Prediction Accuracy = 63.434000000000005%, Loss = 0.36579561829566953
Epoch: 9281, Batch Gradient Norm: 10.154230179343926
Epoch: 9281, Batch Gradient Norm after: 10.154230179343926
Epoch 9282/10000, Prediction Accuracy = 63.306%, Loss = 0.3653042852878571
Epoch: 9282, Batch Gradient Norm: 9.77546213757406
Epoch: 9282, Batch Gradient Norm after: 9.77546213757406
Epoch 9283/10000, Prediction Accuracy = 63.46%, Loss = 0.362979656457901
Epoch: 9283, Batch Gradient Norm: 9.764374596831166
Epoch: 9283, Batch Gradient Norm after: 9.764374596831166
Epoch 9284/10000, Prediction Accuracy = 63.64000000000001%, Loss = 0.36310391426086425
Epoch: 9284, Batch Gradient Norm: 9.327500004158054
Epoch: 9284, Batch Gradient Norm after: 9.327500004158054
Epoch 9285/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3612028121948242
Epoch: 9285, Batch Gradient Norm: 9.205004366526744
Epoch: 9285, Batch Gradient Norm after: 9.205004366526744
Epoch 9286/10000, Prediction Accuracy = 63.586%, Loss = 0.360091894865036
Epoch: 9286, Batch Gradient Norm: 9.392150200291505
Epoch: 9286, Batch Gradient Norm after: 9.392150200291505
Epoch 9287/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.36106913089752196
Epoch: 9287, Batch Gradient Norm: 9.568782234897759
Epoch: 9287, Batch Gradient Norm after: 9.568782234897759
Epoch 9288/10000, Prediction Accuracy = 63.238%, Loss = 0.36190580725669863
Epoch: 9288, Batch Gradient Norm: 9.328897139670705
Epoch: 9288, Batch Gradient Norm after: 9.328897139670705
Epoch 9289/10000, Prediction Accuracy = 63.444%, Loss = 0.35959646105766296
Epoch: 9289, Batch Gradient Norm: 9.726500493926757
Epoch: 9289, Batch Gradient Norm after: 9.726500493926757
Epoch 9290/10000, Prediction Accuracy = 63.472%, Loss = 0.3609320461750031
Epoch: 9290, Batch Gradient Norm: 10.746997749387017
Epoch: 9290, Batch Gradient Norm after: 10.746997749387017
Epoch 9291/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.3685361802577972
Epoch: 9291, Batch Gradient Norm: 9.726239085842478
Epoch: 9291, Batch Gradient Norm after: 9.726239085842478
Epoch 9292/10000, Prediction Accuracy = 63.428%, Loss = 0.3612284481525421
Epoch: 9292, Batch Gradient Norm: 8.910071083380476
Epoch: 9292, Batch Gradient Norm after: 8.910071083380476
Epoch 9293/10000, Prediction Accuracy = 63.402%, Loss = 0.35640066862106323
Epoch: 9293, Batch Gradient Norm: 11.611215077364688
Epoch: 9293, Batch Gradient Norm after: 11.611215077364688
Epoch 9294/10000, Prediction Accuracy = 63.334%, Loss = 0.375616717338562
Epoch: 9294, Batch Gradient Norm: 12.449985603709985
Epoch: 9294, Batch Gradient Norm after: 12.449985603709985
Epoch 9295/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.3839549541473389
Epoch: 9295, Batch Gradient Norm: 10.768496503999065
Epoch: 9295, Batch Gradient Norm after: 10.768496503999065
Epoch 9296/10000, Prediction Accuracy = 63.336%, Loss = 0.37007793188095095
Epoch: 9296, Batch Gradient Norm: 10.719040892553235
Epoch: 9296, Batch Gradient Norm after: 10.719040892553235
Epoch 9297/10000, Prediction Accuracy = 63.348%, Loss = 0.3671144664287567
Epoch: 9297, Batch Gradient Norm: 10.942975659591733
Epoch: 9297, Batch Gradient Norm after: 10.942975659591733
Epoch 9298/10000, Prediction Accuracy = 63.302%, Loss = 0.36828752160072326
Epoch: 9298, Batch Gradient Norm: 11.714809840668426
Epoch: 9298, Batch Gradient Norm after: 11.714809840668426
Epoch 9299/10000, Prediction Accuracy = 63.366%, Loss = 0.3746021568775177
Epoch: 9299, Batch Gradient Norm: 11.502477192308413
Epoch: 9299, Batch Gradient Norm after: 11.502477192308413
Epoch 9300/10000, Prediction Accuracy = 63.44%, Loss = 0.3725366652011871
Epoch: 9300, Batch Gradient Norm: 11.713871413247157
Epoch: 9300, Batch Gradient Norm after: 11.713871413247157
Epoch 9301/10000, Prediction Accuracy = 63.524%, Loss = 0.3737769365310669
Epoch: 9301, Batch Gradient Norm: 10.581977551663963
Epoch: 9301, Batch Gradient Norm after: 10.581977551663963
Epoch 9302/10000, Prediction Accuracy = 63.436%, Loss = 0.3677511870861053
Epoch: 9302, Batch Gradient Norm: 9.002278614150537
Epoch: 9302, Batch Gradient Norm after: 9.002278614150537
Epoch 9303/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3582387149333954
Epoch: 9303, Batch Gradient Norm: 8.445970460550141
Epoch: 9303, Batch Gradient Norm after: 8.445970460550141
Epoch 9304/10000, Prediction Accuracy = 63.426%, Loss = 0.3548966467380524
Epoch: 9304, Batch Gradient Norm: 9.430968269574953
Epoch: 9304, Batch Gradient Norm after: 9.430968269574953
Epoch 9305/10000, Prediction Accuracy = 63.464%, Loss = 0.3592776954174042
Epoch: 9305, Batch Gradient Norm: 10.609324029302993
Epoch: 9305, Batch Gradient Norm after: 10.609324029302993
Epoch 9306/10000, Prediction Accuracy = 63.2%, Loss = 0.36680169105529786
Epoch: 9306, Batch Gradient Norm: 10.369919295981363
Epoch: 9306, Batch Gradient Norm after: 10.369919295981363
Epoch 9307/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.36543278098106385
Epoch: 9307, Batch Gradient Norm: 9.926172134714161
Epoch: 9307, Batch Gradient Norm after: 9.926172134714161
Epoch 9308/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.3625852406024933
Epoch: 9308, Batch Gradient Norm: 9.658533374609773
Epoch: 9308, Batch Gradient Norm after: 9.658533374609773
Epoch 9309/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.36141607761383054
Epoch: 9309, Batch Gradient Norm: 9.67114490817741
Epoch: 9309, Batch Gradient Norm after: 9.67114490817741
Epoch 9310/10000, Prediction Accuracy = 63.59400000000001%, Loss = 0.3611882090568542
Epoch: 9310, Batch Gradient Norm: 9.480004133893205
Epoch: 9310, Batch Gradient Norm after: 9.480004133893205
Epoch 9311/10000, Prediction Accuracy = 63.464%, Loss = 0.36032402515411377
Epoch: 9311, Batch Gradient Norm: 10.612586397612457
Epoch: 9311, Batch Gradient Norm after: 10.612586397612457
Epoch 9312/10000, Prediction Accuracy = 63.552%, Loss = 0.3681315541267395
Epoch: 9312, Batch Gradient Norm: 11.59689094445139
Epoch: 9312, Batch Gradient Norm after: 11.59689094445139
Epoch 9313/10000, Prediction Accuracy = 63.245999999999995%, Loss = 0.3746229290962219
Epoch: 9313, Batch Gradient Norm: 10.802909526586527
Epoch: 9313, Batch Gradient Norm after: 10.802909526586527
Epoch 9314/10000, Prediction Accuracy = 63.416%, Loss = 0.3683120906352997
Epoch: 9314, Batch Gradient Norm: 9.036008246721746
Epoch: 9314, Batch Gradient Norm after: 9.036008246721746
Epoch 9315/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.35604492425918577
Epoch: 9315, Batch Gradient Norm: 9.308483092512231
Epoch: 9315, Batch Gradient Norm after: 9.308483092512231
Epoch 9316/10000, Prediction Accuracy = 63.40599999999999%, Loss = 0.3576232552528381
Epoch: 9316, Batch Gradient Norm: 10.69904734798156
Epoch: 9316, Batch Gradient Norm after: 10.69904734798156
Epoch 9317/10000, Prediction Accuracy = 63.31%, Loss = 0.3671483635902405
Epoch: 9317, Batch Gradient Norm: 10.470475532399053
Epoch: 9317, Batch Gradient Norm after: 10.470475532399053
Epoch 9318/10000, Prediction Accuracy = 63.324%, Loss = 0.3662496984004974
Epoch: 9318, Batch Gradient Norm: 9.822944236716232
Epoch: 9318, Batch Gradient Norm after: 9.822944236716232
Epoch 9319/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.3624509036540985
Epoch: 9319, Batch Gradient Norm: 8.83099978998137
Epoch: 9319, Batch Gradient Norm after: 8.83099978998137
Epoch 9320/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.3572657287120819
Epoch: 9320, Batch Gradient Norm: 8.832595138754444
Epoch: 9320, Batch Gradient Norm after: 8.832595138754444
Epoch 9321/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.35680427551269533
Epoch: 9321, Batch Gradient Norm: 10.96987251909075
Epoch: 9321, Batch Gradient Norm after: 10.96987251909075
Epoch 9322/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.3706441640853882
Epoch: 9322, Batch Gradient Norm: 11.49539706667982
Epoch: 9322, Batch Gradient Norm after: 11.49539706667982
Epoch 9323/10000, Prediction Accuracy = 63.26400000000001%, Loss = 0.3734668791294098
Epoch: 9323, Batch Gradient Norm: 10.58581937960081
Epoch: 9323, Batch Gradient Norm after: 10.58581937960081
Epoch 9324/10000, Prediction Accuracy = 63.354%, Loss = 0.3660766541957855
Epoch: 9324, Batch Gradient Norm: 11.204661723931062
Epoch: 9324, Batch Gradient Norm after: 11.204661723931062
Epoch 9325/10000, Prediction Accuracy = 63.36800000000001%, Loss = 0.3698765695095062
Epoch: 9325, Batch Gradient Norm: 12.110637215524115
Epoch: 9325, Batch Gradient Norm after: 12.110637215524115
Epoch 9326/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3774024248123169
Epoch: 9326, Batch Gradient Norm: 11.08306146652191
Epoch: 9326, Batch Gradient Norm after: 11.08306146652191
Epoch 9327/10000, Prediction Accuracy = 63.372%, Loss = 0.37133894562721254
Epoch: 9327, Batch Gradient Norm: 9.035849141202862
Epoch: 9327, Batch Gradient Norm after: 9.035849141202862
Epoch 9328/10000, Prediction Accuracy = 63.584%, Loss = 0.35744407773017883
Epoch: 9328, Batch Gradient Norm: 9.026770719972669
Epoch: 9328, Batch Gradient Norm after: 9.026770719972669
Epoch 9329/10000, Prediction Accuracy = 63.572%, Loss = 0.35655434131622316
Epoch: 9329, Batch Gradient Norm: 9.63426569997143
Epoch: 9329, Batch Gradient Norm after: 9.63426569997143
Epoch 9330/10000, Prediction Accuracy = 63.686%, Loss = 0.3603090882301331
Epoch: 9330, Batch Gradient Norm: 9.553303935814288
Epoch: 9330, Batch Gradient Norm after: 9.553303935814288
Epoch 9331/10000, Prediction Accuracy = 63.57199999999999%, Loss = 0.35932724475860595
Epoch: 9331, Batch Gradient Norm: 10.889598183043674
Epoch: 9331, Batch Gradient Norm after: 10.889598183043674
Epoch 9332/10000, Prediction Accuracy = 63.5%, Loss = 0.3689719378948212
Epoch: 9332, Batch Gradient Norm: 11.396569009743446
Epoch: 9332, Batch Gradient Norm after: 11.396569009743446
Epoch 9333/10000, Prediction Accuracy = 63.376%, Loss = 0.3735744833946228
Epoch: 9333, Batch Gradient Norm: 10.550634839489028
Epoch: 9333, Batch Gradient Norm after: 10.550634839489028
Epoch 9334/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.3685369431972504
Epoch: 9334, Batch Gradient Norm: 10.417973151880224
Epoch: 9334, Batch Gradient Norm after: 10.417973151880224
Epoch 9335/10000, Prediction Accuracy = 63.314%, Loss = 0.36651121377944945
Epoch: 9335, Batch Gradient Norm: 10.550020911235196
Epoch: 9335, Batch Gradient Norm after: 10.550020911235196
Epoch 9336/10000, Prediction Accuracy = 63.452%, Loss = 0.367182070016861
Epoch: 9336, Batch Gradient Norm: 10.67580204387633
Epoch: 9336, Batch Gradient Norm after: 10.67580204387633
Epoch 9337/10000, Prediction Accuracy = 63.36%, Loss = 0.3668613016605377
Epoch: 9337, Batch Gradient Norm: 10.290574717018936
Epoch: 9337, Batch Gradient Norm after: 10.290574717018936
Epoch 9338/10000, Prediction Accuracy = 63.355999999999995%, Loss = 0.3634669244289398
Epoch: 9338, Batch Gradient Norm: 10.109700983173513
Epoch: 9338, Batch Gradient Norm after: 10.109700983173513
Epoch 9339/10000, Prediction Accuracy = 63.34000000000001%, Loss = 0.36294432282447814
Epoch: 9339, Batch Gradient Norm: 9.475699292150031
Epoch: 9339, Batch Gradient Norm after: 9.475699292150031
Epoch 9340/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.36057172417640687
Epoch: 9340, Batch Gradient Norm: 9.488125498969955
Epoch: 9340, Batch Gradient Norm after: 9.488125498969955
Epoch 9341/10000, Prediction Accuracy = 63.626%, Loss = 0.3600758969783783
Epoch: 9341, Batch Gradient Norm: 10.78111110459498
Epoch: 9341, Batch Gradient Norm after: 10.78111110459498
Epoch 9342/10000, Prediction Accuracy = 63.436%, Loss = 0.3694888412952423
Epoch: 9342, Batch Gradient Norm: 10.487304348254554
Epoch: 9342, Batch Gradient Norm after: 10.487304348254554
Epoch 9343/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.36664888858795164
Epoch: 9343, Batch Gradient Norm: 10.025199021914682
Epoch: 9343, Batch Gradient Norm after: 10.025199021914682
Epoch 9344/10000, Prediction Accuracy = 63.556%, Loss = 0.3627286732196808
Epoch: 9344, Batch Gradient Norm: 9.785863917137345
Epoch: 9344, Batch Gradient Norm after: 9.785863917137345
Epoch 9345/10000, Prediction Accuracy = 63.388%, Loss = 0.3610217094421387
Epoch: 9345, Batch Gradient Norm: 9.405225234208254
Epoch: 9345, Batch Gradient Norm after: 9.405225234208254
Epoch 9346/10000, Prediction Accuracy = 63.572%, Loss = 0.3581839859485626
Epoch: 9346, Batch Gradient Norm: 10.464084607078291
Epoch: 9346, Batch Gradient Norm after: 10.464084607078291
Epoch 9347/10000, Prediction Accuracy = 63.412%, Loss = 0.364558732509613
Epoch: 9347, Batch Gradient Norm: 9.85757888365241
Epoch: 9347, Batch Gradient Norm after: 9.85757888365241
Epoch 9348/10000, Prediction Accuracy = 63.48%, Loss = 0.3621805191040039
Epoch: 9348, Batch Gradient Norm: 8.023721218049564
Epoch: 9348, Batch Gradient Norm after: 8.023721218049564
Epoch 9349/10000, Prediction Accuracy = 63.55%, Loss = 0.3510036051273346
Epoch: 9349, Batch Gradient Norm: 9.599568590725607
Epoch: 9349, Batch Gradient Norm after: 9.599568590725607
Epoch 9350/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3619222044944763
Epoch: 9350, Batch Gradient Norm: 10.75556530722059
Epoch: 9350, Batch Gradient Norm after: 10.75556530722059
Epoch 9351/10000, Prediction Accuracy = 63.168000000000006%, Loss = 0.37024774551391604
Epoch: 9351, Batch Gradient Norm: 11.759032349685748
Epoch: 9351, Batch Gradient Norm after: 11.759032349685748
Epoch 9352/10000, Prediction Accuracy = 63.52%, Loss = 0.37543013095855715
Epoch: 9352, Batch Gradient Norm: 11.869534706786718
Epoch: 9352, Batch Gradient Norm after: 11.869534706786718
Epoch 9353/10000, Prediction Accuracy = 63.443999999999996%, Loss = 0.37794487476348876
Epoch: 9353, Batch Gradient Norm: 9.490133725036815
Epoch: 9353, Batch Gradient Norm after: 9.490133725036815
Epoch 9354/10000, Prediction Accuracy = 63.536%, Loss = 0.36045871376991273
Epoch: 9354, Batch Gradient Norm: 8.91455489762955
Epoch: 9354, Batch Gradient Norm after: 8.91455489762955
Epoch 9355/10000, Prediction Accuracy = 63.472%, Loss = 0.3563365161418915
Epoch: 9355, Batch Gradient Norm: 10.261209884794882
Epoch: 9355, Batch Gradient Norm after: 10.261209884794882
Epoch 9356/10000, Prediction Accuracy = 63.536%, Loss = 0.36401923298835753
Epoch: 9356, Batch Gradient Norm: 12.035120267624949
Epoch: 9356, Batch Gradient Norm after: 12.035120267624949
Epoch 9357/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3775401473045349
Epoch: 9357, Batch Gradient Norm: 10.885104977940724
Epoch: 9357, Batch Gradient Norm after: 10.885104977940724
Epoch 9358/10000, Prediction Accuracy = 63.398%, Loss = 0.36809136271476744
Epoch: 9358, Batch Gradient Norm: 10.688104996469162
Epoch: 9358, Batch Gradient Norm after: 10.688104996469162
Epoch 9359/10000, Prediction Accuracy = 63.46%, Loss = 0.3672780990600586
Epoch: 9359, Batch Gradient Norm: 10.5754190459924
Epoch: 9359, Batch Gradient Norm after: 10.5754190459924
Epoch 9360/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.3669200956821442
Epoch: 9360, Batch Gradient Norm: 10.476278528372218
Epoch: 9360, Batch Gradient Norm after: 10.476278528372218
Epoch 9361/10000, Prediction Accuracy = 63.45%, Loss = 0.3653023958206177
Epoch: 9361, Batch Gradient Norm: 9.914537827245242
Epoch: 9361, Batch Gradient Norm after: 9.914537827245242
Epoch 9362/10000, Prediction Accuracy = 63.322%, Loss = 0.3610136330127716
Epoch: 9362, Batch Gradient Norm: 8.85316109932977
Epoch: 9362, Batch Gradient Norm after: 8.85316109932977
Epoch 9363/10000, Prediction Accuracy = 63.42%, Loss = 0.3547511756420135
Epoch: 9363, Batch Gradient Norm: 9.600495882265747
Epoch: 9363, Batch Gradient Norm after: 9.600495882265747
Epoch 9364/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.36042683124542235
Epoch: 9364, Batch Gradient Norm: 10.591751252190148
Epoch: 9364, Batch Gradient Norm after: 10.591751252190148
Epoch 9365/10000, Prediction Accuracy = 63.44599999999999%, Loss = 0.36554883122444154
Epoch: 9365, Batch Gradient Norm: 12.21315265125124
Epoch: 9365, Batch Gradient Norm after: 12.21315265125124
Epoch 9366/10000, Prediction Accuracy = 63.564%, Loss = 0.3780375480651855
Epoch: 9366, Batch Gradient Norm: 9.657425916221296
Epoch: 9366, Batch Gradient Norm after: 9.657425916221296
Epoch 9367/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3601832091808319
Epoch: 9367, Batch Gradient Norm: 8.804222968081339
Epoch: 9367, Batch Gradient Norm after: 8.804222968081339
Epoch 9368/10000, Prediction Accuracy = 63.484%, Loss = 0.35426493883132937
Epoch: 9368, Batch Gradient Norm: 9.481961761533388
Epoch: 9368, Batch Gradient Norm after: 9.481961761533388
Epoch 9369/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.3586837887763977
Epoch: 9369, Batch Gradient Norm: 9.739524891484681
Epoch: 9369, Batch Gradient Norm after: 9.739524891484681
Epoch 9370/10000, Prediction Accuracy = 63.424%, Loss = 0.35869758725166323
Epoch: 9370, Batch Gradient Norm: 9.977998109003046
Epoch: 9370, Batch Gradient Norm after: 9.977998109003046
Epoch 9371/10000, Prediction Accuracy = 63.5%, Loss = 0.36129267811775206
Epoch: 9371, Batch Gradient Norm: 10.668783211364715
Epoch: 9371, Batch Gradient Norm after: 10.668783211364715
Epoch 9372/10000, Prediction Accuracy = 63.29%, Loss = 0.36602710485458373
Epoch: 9372, Batch Gradient Norm: 12.28892119214816
Epoch: 9372, Batch Gradient Norm after: 12.28892119214816
Epoch 9373/10000, Prediction Accuracy = 63.4%, Loss = 0.37987655997276304
Epoch: 9373, Batch Gradient Norm: 10.887879576788563
Epoch: 9373, Batch Gradient Norm after: 10.887879576788563
Epoch 9374/10000, Prediction Accuracy = 63.572%, Loss = 0.37078378200531004
Epoch: 9374, Batch Gradient Norm: 9.584866187370897
Epoch: 9374, Batch Gradient Norm after: 9.584866187370897
Epoch 9375/10000, Prediction Accuracy = 63.39%, Loss = 0.3611722767353058
Epoch: 9375, Batch Gradient Norm: 10.55050744160703
Epoch: 9375, Batch Gradient Norm after: 10.55050744160703
Epoch 9376/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.36715382933616636
Epoch: 9376, Batch Gradient Norm: 10.765293552686476
Epoch: 9376, Batch Gradient Norm after: 10.765293552686476
Epoch 9377/10000, Prediction Accuracy = 63.455999999999996%, Loss = 0.3677791774272919
Epoch: 9377, Batch Gradient Norm: 11.130439085401074
Epoch: 9377, Batch Gradient Norm after: 11.130439085401074
Epoch 9378/10000, Prediction Accuracy = 63.44200000000001%, Loss = 0.36919554471969607
Epoch: 9378, Batch Gradient Norm: 10.148441257834966
Epoch: 9378, Batch Gradient Norm after: 10.148441257834966
Epoch 9379/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3633212625980377
Epoch: 9379, Batch Gradient Norm: 9.018264372372549
Epoch: 9379, Batch Gradient Norm after: 9.018264372372549
Epoch 9380/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.3556896150112152
Epoch: 9380, Batch Gradient Norm: 9.874217337179038
Epoch: 9380, Batch Gradient Norm after: 9.874217337179038
Epoch 9381/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.36124104261398315
Epoch: 9381, Batch Gradient Norm: 10.723156837500362
Epoch: 9381, Batch Gradient Norm after: 10.723156837500362
Epoch 9382/10000, Prediction Accuracy = 63.426%, Loss = 0.36836093068122866
Epoch: 9382, Batch Gradient Norm: 9.940985159318625
Epoch: 9382, Batch Gradient Norm after: 9.940985159318625
Epoch 9383/10000, Prediction Accuracy = 63.471999999999994%, Loss = 0.3633708477020264
Epoch: 9383, Batch Gradient Norm: 9.286199602756223
Epoch: 9383, Batch Gradient Norm after: 9.286199602756223
Epoch 9384/10000, Prediction Accuracy = 63.586%, Loss = 0.35880154371261597
Epoch: 9384, Batch Gradient Norm: 9.906846070472714
Epoch: 9384, Batch Gradient Norm after: 9.906846070472714
Epoch 9385/10000, Prediction Accuracy = 63.374%, Loss = 0.36320950388908385
Epoch: 9385, Batch Gradient Norm: 8.774205224329364
Epoch: 9385, Batch Gradient Norm after: 8.774205224329364
Epoch 9386/10000, Prediction Accuracy = 63.56%, Loss = 0.3562048614025116
Epoch: 9386, Batch Gradient Norm: 9.046574713352495
Epoch: 9386, Batch Gradient Norm after: 9.046574713352495
Epoch 9387/10000, Prediction Accuracy = 63.528%, Loss = 0.3570326268672943
Epoch: 9387, Batch Gradient Norm: 10.686512032765352
Epoch: 9387, Batch Gradient Norm after: 10.686512032765352
Epoch 9388/10000, Prediction Accuracy = 63.5%, Loss = 0.36582828760147096
Epoch: 9388, Batch Gradient Norm: 12.002816457747848
Epoch: 9388, Batch Gradient Norm after: 12.002816457747848
Epoch 9389/10000, Prediction Accuracy = 63.29600000000001%, Loss = 0.3755149781703949
Epoch: 9389, Batch Gradient Norm: 10.944536117365255
Epoch: 9389, Batch Gradient Norm after: 10.944536117365255
Epoch 9390/10000, Prediction Accuracy = 63.4%, Loss = 0.3679753839969635
Epoch: 9390, Batch Gradient Norm: 10.748888832231895
Epoch: 9390, Batch Gradient Norm after: 10.748888832231895
Epoch 9391/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.36809368133544923
Epoch: 9391, Batch Gradient Norm: 10.586265803041407
Epoch: 9391, Batch Gradient Norm after: 10.586265803041407
Epoch 9392/10000, Prediction Accuracy = 63.395999999999994%, Loss = 0.36699752807617186
Epoch: 9392, Batch Gradient Norm: 10.298616436519376
Epoch: 9392, Batch Gradient Norm after: 10.298616436519376
Epoch 9393/10000, Prediction Accuracy = 63.472%, Loss = 0.36456713676452634
Epoch: 9393, Batch Gradient Norm: 9.907126242869236
Epoch: 9393, Batch Gradient Norm after: 9.907126242869236
Epoch 9394/10000, Prediction Accuracy = 63.476%, Loss = 0.36158368587493894
Epoch: 9394, Batch Gradient Norm: 9.69481143281306
Epoch: 9394, Batch Gradient Norm after: 9.69481143281306
Epoch 9395/10000, Prediction Accuracy = 63.378%, Loss = 0.36042309999465943
Epoch: 9395, Batch Gradient Norm: 9.64304487377265
Epoch: 9395, Batch Gradient Norm after: 9.64304487377265
Epoch 9396/10000, Prediction Accuracy = 63.562%, Loss = 0.36031396985054015
Epoch: 9396, Batch Gradient Norm: 10.620439303458545
Epoch: 9396, Batch Gradient Norm after: 10.620439303458545
Epoch 9397/10000, Prediction Accuracy = 63.28599999999999%, Loss = 0.3653566539287567
Epoch: 9397, Batch Gradient Norm: 11.448206961124516
Epoch: 9397, Batch Gradient Norm after: 11.448206961124516
Epoch 9398/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.37075825929641726
Epoch: 9398, Batch Gradient Norm: 10.658966979986763
Epoch: 9398, Batch Gradient Norm after: 10.658966979986763
Epoch 9399/10000, Prediction Accuracy = 63.472%, Loss = 0.36514055728912354
Epoch: 9399, Batch Gradient Norm: 9.898176740846566
Epoch: 9399, Batch Gradient Norm after: 9.898176740846566
Epoch 9400/10000, Prediction Accuracy = 63.598%, Loss = 0.3608918786048889
Epoch: 9400, Batch Gradient Norm: 9.79731526917091
Epoch: 9400, Batch Gradient Norm after: 9.79731526917091
Epoch 9401/10000, Prediction Accuracy = 63.568000000000005%, Loss = 0.361323881149292
Epoch: 9401, Batch Gradient Norm: 10.08989442181906
Epoch: 9401, Batch Gradient Norm after: 10.08989442181906
Epoch 9402/10000, Prediction Accuracy = 63.538%, Loss = 0.3636611759662628
Epoch: 9402, Batch Gradient Norm: 10.77473687999895
Epoch: 9402, Batch Gradient Norm after: 10.77473687999895
Epoch 9403/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.367498379945755
Epoch: 9403, Batch Gradient Norm: 10.389358584214714
Epoch: 9403, Batch Gradient Norm after: 10.389358584214714
Epoch 9404/10000, Prediction Accuracy = 63.472%, Loss = 0.3647304534912109
Epoch: 9404, Batch Gradient Norm: 8.890006788325637
Epoch: 9404, Batch Gradient Norm after: 8.890006788325637
Epoch 9405/10000, Prediction Accuracy = 63.418000000000006%, Loss = 0.35558651089668275
Epoch: 9405, Batch Gradient Norm: 9.025179156650054
Epoch: 9405, Batch Gradient Norm after: 9.025179156650054
Epoch 9406/10000, Prediction Accuracy = 63.588%, Loss = 0.3558211147785187
Epoch: 9406, Batch Gradient Norm: 11.867616191393635
Epoch: 9406, Batch Gradient Norm after: 11.867616191393635
Epoch 9407/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.37372910380363467
Epoch: 9407, Batch Gradient Norm: 12.056441512056141
Epoch: 9407, Batch Gradient Norm after: 12.056441512056141
Epoch 9408/10000, Prediction Accuracy = 63.54%, Loss = 0.3754839479923248
Epoch: 9408, Batch Gradient Norm: 9.71064977722998
Epoch: 9408, Batch Gradient Norm after: 9.71064977722998
Epoch 9409/10000, Prediction Accuracy = 63.472%, Loss = 0.3592473268508911
Epoch: 9409, Batch Gradient Norm: 10.017651836014377
Epoch: 9409, Batch Gradient Norm after: 10.017651836014377
Epoch 9410/10000, Prediction Accuracy = 63.05%, Loss = 0.3623640418052673
Epoch: 9410, Batch Gradient Norm: 9.981771553580021
Epoch: 9410, Batch Gradient Norm after: 9.981771553580021
Epoch 9411/10000, Prediction Accuracy = 63.412%, Loss = 0.36336269974708557
Epoch: 9411, Batch Gradient Norm: 8.988075448591939
Epoch: 9411, Batch Gradient Norm after: 8.988075448591939
Epoch 9412/10000, Prediction Accuracy = 63.496%, Loss = 0.35647904872894287
Epoch: 9412, Batch Gradient Norm: 8.919861787484688
Epoch: 9412, Batch Gradient Norm after: 8.919861787484688
Epoch 9413/10000, Prediction Accuracy = 63.64%, Loss = 0.3555042862892151
Epoch: 9413, Batch Gradient Norm: 9.84222339195996
Epoch: 9413, Batch Gradient Norm after: 9.84222339195996
Epoch 9414/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.3584157407283783
Epoch: 9414, Batch Gradient Norm: 11.796319688927788
Epoch: 9414, Batch Gradient Norm after: 11.796319688927788
Epoch 9415/10000, Prediction Accuracy = 63.25%, Loss = 0.37090279459953307
Epoch: 9415, Batch Gradient Norm: 10.498224146338172
Epoch: 9415, Batch Gradient Norm after: 10.498224146338172
Epoch 9416/10000, Prediction Accuracy = 63.391999999999996%, Loss = 0.3639461100101471
Epoch: 9416, Batch Gradient Norm: 10.87710026213902
Epoch: 9416, Batch Gradient Norm after: 10.87710026213902
Epoch 9417/10000, Prediction Accuracy = 63.472%, Loss = 0.36880146265029906
Epoch: 9417, Batch Gradient Norm: 10.622881528180645
Epoch: 9417, Batch Gradient Norm after: 10.622881528180645
Epoch 9418/10000, Prediction Accuracy = 63.379999999999995%, Loss = 0.3688999652862549
Epoch: 9418, Batch Gradient Norm: 10.70219796625441
Epoch: 9418, Batch Gradient Norm after: 10.70219796625441
Epoch 9419/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.36690094470977785
Epoch: 9419, Batch Gradient Norm: 11.501422582117094
Epoch: 9419, Batch Gradient Norm after: 11.501422582117094
Epoch 9420/10000, Prediction Accuracy = 63.41799999999999%, Loss = 0.3720156252384186
Epoch: 9420, Batch Gradient Norm: 10.621000988425855
Epoch: 9420, Batch Gradient Norm after: 10.621000988425855
Epoch 9421/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.36603506803512575
Epoch: 9421, Batch Gradient Norm: 9.807908350407995
Epoch: 9421, Batch Gradient Norm after: 9.807908350407995
Epoch 9422/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.36240021586418153
Epoch: 9422, Batch Gradient Norm: 10.427785816625573
Epoch: 9422, Batch Gradient Norm after: 10.427785816625573
Epoch 9423/10000, Prediction Accuracy = 63.58%, Loss = 0.3675399124622345
Epoch: 9423, Batch Gradient Norm: 10.046490616860568
Epoch: 9423, Batch Gradient Norm after: 10.046490616860568
Epoch 9424/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.36475679874420164
Epoch: 9424, Batch Gradient Norm: 8.97675264844669
Epoch: 9424, Batch Gradient Norm after: 8.97675264844669
Epoch 9425/10000, Prediction Accuracy = 63.50200000000001%, Loss = 0.3564377546310425
Epoch: 9425, Batch Gradient Norm: 8.061048077167227
Epoch: 9425, Batch Gradient Norm after: 8.061048077167227
Epoch 9426/10000, Prediction Accuracy = 63.43399999999999%, Loss = 0.3504155993461609
Epoch: 9426, Batch Gradient Norm: 8.757493294762858
Epoch: 9426, Batch Gradient Norm after: 8.757493294762858
Epoch 9427/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.3523668944835663
Epoch: 9427, Batch Gradient Norm: 12.220832028180292
Epoch: 9427, Batch Gradient Norm after: 12.220832028180292
Epoch 9428/10000, Prediction Accuracy = 63.39%, Loss = 0.37648339867591857
Epoch: 9428, Batch Gradient Norm: 11.429455024094676
Epoch: 9428, Batch Gradient Norm after: 11.429455024094676
Epoch 9429/10000, Prediction Accuracy = 63.426%, Loss = 0.3727956175804138
Epoch: 9429, Batch Gradient Norm: 9.023483483508018
Epoch: 9429, Batch Gradient Norm after: 9.023483483508018
Epoch 9430/10000, Prediction Accuracy = 63.6%, Loss = 0.3556811213493347
Epoch: 9430, Batch Gradient Norm: 8.671824660997311
Epoch: 9430, Batch Gradient Norm after: 8.671824660997311
Epoch 9431/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.35330201387405397
Epoch: 9431, Batch Gradient Norm: 10.237790784310445
Epoch: 9431, Batch Gradient Norm after: 10.237790784310445
Epoch 9432/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.3639869153499603
Epoch: 9432, Batch Gradient Norm: 11.1799055677456
Epoch: 9432, Batch Gradient Norm after: 11.1799055677456
Epoch 9433/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.372159069776535
Epoch: 9433, Batch Gradient Norm: 11.28600443447072
Epoch: 9433, Batch Gradient Norm after: 11.28600443447072
Epoch 9434/10000, Prediction Accuracy = 63.436%, Loss = 0.37055624127388
Epoch: 9434, Batch Gradient Norm: 11.842641363215398
Epoch: 9434, Batch Gradient Norm after: 11.842641363215398
Epoch 9435/10000, Prediction Accuracy = 63.343999999999994%, Loss = 0.37362456917762754
Epoch: 9435, Batch Gradient Norm: 9.874725652274538
Epoch: 9435, Batch Gradient Norm after: 9.874725652274538
Epoch 9436/10000, Prediction Accuracy = 63.45%, Loss = 0.36020627617836
Epoch: 9436, Batch Gradient Norm: 9.308337791797978
Epoch: 9436, Batch Gradient Norm after: 9.308337791797978
Epoch 9437/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3570632815361023
Epoch: 9437, Batch Gradient Norm: 11.666984635005784
Epoch: 9437, Batch Gradient Norm after: 11.666984635005784
Epoch 9438/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.37259401082992555
Epoch: 9438, Batch Gradient Norm: 11.918101727745919
Epoch: 9438, Batch Gradient Norm after: 11.918101727745919
Epoch 9439/10000, Prediction Accuracy = 63.552%, Loss = 0.3756823241710663
Epoch: 9439, Batch Gradient Norm: 9.666765714900146
Epoch: 9439, Batch Gradient Norm after: 9.666765714900146
Epoch 9440/10000, Prediction Accuracy = 63.552%, Loss = 0.35843512415885925
Epoch: 9440, Batch Gradient Norm: 8.66898093065514
Epoch: 9440, Batch Gradient Norm after: 8.66898093065514
Epoch 9441/10000, Prediction Accuracy = 63.504%, Loss = 0.35236472487449644
Epoch: 9441, Batch Gradient Norm: 8.861009634509081
Epoch: 9441, Batch Gradient Norm after: 8.861009634509081
Epoch 9442/10000, Prediction Accuracy = 63.426%, Loss = 0.35448641777038575
Epoch: 9442, Batch Gradient Norm: 8.484429504290661
Epoch: 9442, Batch Gradient Norm after: 8.484429504290661
Epoch 9443/10000, Prediction Accuracy = 63.402%, Loss = 0.35233270525932314
Epoch: 9443, Batch Gradient Norm: 8.905488912936725
Epoch: 9443, Batch Gradient Norm after: 8.905488912936725
Epoch 9444/10000, Prediction Accuracy = 63.483999999999995%, Loss = 0.35516749024391175
Epoch: 9444, Batch Gradient Norm: 9.538150208369675
Epoch: 9444, Batch Gradient Norm after: 9.538150208369675
Epoch 9445/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3590324878692627
Epoch: 9445, Batch Gradient Norm: 10.022469574988595
Epoch: 9445, Batch Gradient Norm after: 10.022469574988595
Epoch 9446/10000, Prediction Accuracy = 63.436%, Loss = 0.3627711653709412
Epoch: 9446, Batch Gradient Norm: 10.255144389408699
Epoch: 9446, Batch Gradient Norm after: 10.255144389408699
Epoch 9447/10000, Prediction Accuracy = 63.302%, Loss = 0.3637255072593689
Epoch: 9447, Batch Gradient Norm: 9.645125745532946
Epoch: 9447, Batch Gradient Norm after: 9.645125745532946
Epoch 9448/10000, Prediction Accuracy = 63.412%, Loss = 0.3603568017482758
Epoch: 9448, Batch Gradient Norm: 10.231693979987794
Epoch: 9448, Batch Gradient Norm after: 10.231693979987794
Epoch 9449/10000, Prediction Accuracy = 63.452%, Loss = 0.36466113328933714
Epoch: 9449, Batch Gradient Norm: 10.807417672075282
Epoch: 9449, Batch Gradient Norm after: 10.807417672075282
Epoch 9450/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.36925885677337644
Epoch: 9450, Batch Gradient Norm: 10.252617411841964
Epoch: 9450, Batch Gradient Norm after: 10.252617411841964
Epoch 9451/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.36455740332603453
Epoch: 9451, Batch Gradient Norm: 10.986773509911055
Epoch: 9451, Batch Gradient Norm after: 10.986773509911055
Epoch 9452/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.36904003024101256
Epoch: 9452, Batch Gradient Norm: 12.280987720796807
Epoch: 9452, Batch Gradient Norm after: 12.280987720796807
Epoch 9453/10000, Prediction Accuracy = 63.524%, Loss = 0.37790011763572695
Epoch: 9453, Batch Gradient Norm: 11.269700875804743
Epoch: 9453, Batch Gradient Norm after: 11.269700875804743
Epoch 9454/10000, Prediction Accuracy = 63.326%, Loss = 0.3706528663635254
Epoch: 9454, Batch Gradient Norm: 10.434342307916001
Epoch: 9454, Batch Gradient Norm after: 10.434342307916001
Epoch 9455/10000, Prediction Accuracy = 63.326%, Loss = 0.36303109526634214
Epoch: 9455, Batch Gradient Norm: 10.570390385116504
Epoch: 9455, Batch Gradient Norm after: 10.570390385116504
Epoch 9456/10000, Prediction Accuracy = 63.52%, Loss = 0.3628397822380066
Epoch: 9456, Batch Gradient Norm: 10.850652760250393
Epoch: 9456, Batch Gradient Norm after: 10.850652760250393
Epoch 9457/10000, Prediction Accuracy = 63.42199999999999%, Loss = 0.36603639721870423
Epoch: 9457, Batch Gradient Norm: 11.236542292634807
Epoch: 9457, Batch Gradient Norm after: 11.236542292634807
Epoch 9458/10000, Prediction Accuracy = 63.42%, Loss = 0.3698341727256775
Epoch: 9458, Batch Gradient Norm: 9.546098118120286
Epoch: 9458, Batch Gradient Norm after: 9.546098118120286
Epoch 9459/10000, Prediction Accuracy = 63.622%, Loss = 0.3586418032646179
Epoch: 9459, Batch Gradient Norm: 9.188177792541131
Epoch: 9459, Batch Gradient Norm after: 9.188177792541131
Epoch 9460/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.35513949394226074
Epoch: 9460, Batch Gradient Norm: 9.62815537442595
Epoch: 9460, Batch Gradient Norm after: 9.62815537442595
Epoch 9461/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.35942238569259644
Epoch: 9461, Batch Gradient Norm: 10.330598666698739
Epoch: 9461, Batch Gradient Norm after: 10.330598666698739
Epoch 9462/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.3640764236450195
Epoch: 9462, Batch Gradient Norm: 11.16353374693461
Epoch: 9462, Batch Gradient Norm after: 11.16353374693461
Epoch 9463/10000, Prediction Accuracy = 63.53000000000001%, Loss = 0.36964295506477357
Epoch: 9463, Batch Gradient Norm: 10.262851405885364
Epoch: 9463, Batch Gradient Norm after: 10.262851405885364
Epoch 9464/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.3618780791759491
Epoch: 9464, Batch Gradient Norm: 9.661790589947872
Epoch: 9464, Batch Gradient Norm after: 9.661790589947872
Epoch 9465/10000, Prediction Accuracy = 63.465999999999994%, Loss = 0.3573644459247589
Epoch: 9465, Batch Gradient Norm: 10.689638874462633
Epoch: 9465, Batch Gradient Norm after: 10.689638874462633
Epoch 9466/10000, Prediction Accuracy = 63.236000000000004%, Loss = 0.3648018419742584
Epoch: 9466, Batch Gradient Norm: 11.604559423036761
Epoch: 9466, Batch Gradient Norm after: 11.604559423036761
Epoch 9467/10000, Prediction Accuracy = 63.496%, Loss = 0.3734222948551178
Epoch: 9467, Batch Gradient Norm: 10.981554850672019
Epoch: 9467, Batch Gradient Norm after: 10.981554850672019
Epoch 9468/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.36998714208602906
Epoch: 9468, Batch Gradient Norm: 9.506994842119205
Epoch: 9468, Batch Gradient Norm after: 9.506994842119205
Epoch 9469/10000, Prediction Accuracy = 63.536%, Loss = 0.3594214260578156
Epoch: 9469, Batch Gradient Norm: 9.292343898295966
Epoch: 9469, Batch Gradient Norm after: 9.292343898295966
Epoch 9470/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.3573027729988098
Epoch: 9470, Batch Gradient Norm: 10.870091024301415
Epoch: 9470, Batch Gradient Norm after: 10.870091024301415
Epoch 9471/10000, Prediction Accuracy = 63.61400000000001%, Loss = 0.3671783447265625
Epoch: 9471, Batch Gradient Norm: 10.508997707222788
Epoch: 9471, Batch Gradient Norm after: 10.508997707222788
Epoch 9472/10000, Prediction Accuracy = 63.536%, Loss = 0.3634355664253235
Epoch: 9472, Batch Gradient Norm: 10.366980112471518
Epoch: 9472, Batch Gradient Norm after: 10.366980112471518
Epoch 9473/10000, Prediction Accuracy = 63.388%, Loss = 0.3613843858242035
Epoch: 9473, Batch Gradient Norm: 9.138303241669266
Epoch: 9473, Batch Gradient Norm after: 9.138303241669266
Epoch 9474/10000, Prediction Accuracy = 63.41799999999999%, Loss = 0.3549946367740631
Epoch: 9474, Batch Gradient Norm: 8.60475345014902
Epoch: 9474, Batch Gradient Norm after: 8.60475345014902
Epoch 9475/10000, Prediction Accuracy = 63.556%, Loss = 0.35321619510650637
Epoch: 9475, Batch Gradient Norm: 9.35527361169233
Epoch: 9475, Batch Gradient Norm after: 9.35527361169233
Epoch 9476/10000, Prediction Accuracy = 63.61400000000001%, Loss = 0.3586788535118103
Epoch: 9476, Batch Gradient Norm: 9.898356267015751
Epoch: 9476, Batch Gradient Norm after: 9.898356267015751
Epoch 9477/10000, Prediction Accuracy = 63.584%, Loss = 0.3630879998207092
Epoch: 9477, Batch Gradient Norm: 10.80003248302775
Epoch: 9477, Batch Gradient Norm after: 10.80003248302775
Epoch 9478/10000, Prediction Accuracy = 63.312%, Loss = 0.3677274823188782
Epoch: 9478, Batch Gradient Norm: 10.556473379612678
Epoch: 9478, Batch Gradient Norm after: 10.556473379612678
Epoch 9479/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.3652641475200653
Epoch: 9479, Batch Gradient Norm: 10.754514605279683
Epoch: 9479, Batch Gradient Norm after: 10.754514605279683
Epoch 9480/10000, Prediction Accuracy = 63.45799999999999%, Loss = 0.36474506855010985
Epoch: 9480, Batch Gradient Norm: 11.327849181598783
Epoch: 9480, Batch Gradient Norm after: 11.327849181598783
Epoch 9481/10000, Prediction Accuracy = 63.593999999999994%, Loss = 0.3682910919189453
Epoch: 9481, Batch Gradient Norm: 11.118388018670185
Epoch: 9481, Batch Gradient Norm after: 11.118388018670185
Epoch 9482/10000, Prediction Accuracy = 63.468%, Loss = 0.367917138338089
Epoch: 9482, Batch Gradient Norm: 9.159508364598638
Epoch: 9482, Batch Gradient Norm after: 9.159508364598638
Epoch 9483/10000, Prediction Accuracy = 63.428%, Loss = 0.35666757822036743
Epoch: 9483, Batch Gradient Norm: 8.565304699349609
Epoch: 9483, Batch Gradient Norm after: 8.565304699349609
Epoch 9484/10000, Prediction Accuracy = 63.672000000000004%, Loss = 0.35260910987854005
Epoch: 9484, Batch Gradient Norm: 10.22528272214663
Epoch: 9484, Batch Gradient Norm after: 10.22528272214663
Epoch 9485/10000, Prediction Accuracy = 63.482000000000006%, Loss = 0.3622462749481201
Epoch: 9485, Batch Gradient Norm: 10.844468232604079
Epoch: 9485, Batch Gradient Norm after: 10.844468232604079
Epoch 9486/10000, Prediction Accuracy = 63.36%, Loss = 0.3659881830215454
Epoch: 9486, Batch Gradient Norm: 9.572965652023054
Epoch: 9486, Batch Gradient Norm after: 9.572965652023054
Epoch 9487/10000, Prediction Accuracy = 63.568%, Loss = 0.357877117395401
Epoch: 9487, Batch Gradient Norm: 10.289866677507618
Epoch: 9487, Batch Gradient Norm after: 10.289866677507618
Epoch 9488/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3625128984451294
Epoch: 9488, Batch Gradient Norm: 10.949917845075019
Epoch: 9488, Batch Gradient Norm after: 10.949917845075019
Epoch 9489/10000, Prediction Accuracy = 63.476%, Loss = 0.3686371326446533
Epoch: 9489, Batch Gradient Norm: 10.189442420990726
Epoch: 9489, Batch Gradient Norm after: 10.189442420990726
Epoch 9490/10000, Prediction Accuracy = 63.39399999999999%, Loss = 0.3635490894317627
Epoch: 9490, Batch Gradient Norm: 10.22985222866791
Epoch: 9490, Batch Gradient Norm after: 10.22985222866791
Epoch 9491/10000, Prediction Accuracy = 63.31%, Loss = 0.36430220007896424
Epoch: 9491, Batch Gradient Norm: 10.496769596820354
Epoch: 9491, Batch Gradient Norm after: 10.496769596820354
Epoch 9492/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.36412172913551333
Epoch: 9492, Batch Gradient Norm: 12.02385730133028
Epoch: 9492, Batch Gradient Norm after: 12.02385730133028
Epoch 9493/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.3737881124019623
Epoch: 9493, Batch Gradient Norm: 10.789287696055233
Epoch: 9493, Batch Gradient Norm after: 10.789287696055233
Epoch 9494/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.367328816652298
Epoch: 9494, Batch Gradient Norm: 8.76083148808655
Epoch: 9494, Batch Gradient Norm after: 8.76083148808655
Epoch 9495/10000, Prediction Accuracy = 63.614%, Loss = 0.3546076714992523
Epoch: 9495, Batch Gradient Norm: 8.5700485180438
Epoch: 9495, Batch Gradient Norm after: 8.5700485180438
Epoch 9496/10000, Prediction Accuracy = 63.444%, Loss = 0.3529051780700684
Epoch: 9496, Batch Gradient Norm: 8.981278867236535
Epoch: 9496, Batch Gradient Norm after: 8.981278867236535
Epoch 9497/10000, Prediction Accuracy = 63.34400000000001%, Loss = 0.35532386302948
Epoch: 9497, Batch Gradient Norm: 10.442146396599822
Epoch: 9497, Batch Gradient Norm after: 10.442146396599822
Epoch 9498/10000, Prediction Accuracy = 63.49399999999999%, Loss = 0.36516174077987673
Epoch: 9498, Batch Gradient Norm: 11.2090214571384
Epoch: 9498, Batch Gradient Norm after: 11.2090214571384
Epoch 9499/10000, Prediction Accuracy = 63.43399999999999%, Loss = 0.36968059539794923
Epoch: 9499, Batch Gradient Norm: 11.298474255609577
Epoch: 9499, Batch Gradient Norm after: 11.298474255609577
Epoch 9500/10000, Prediction Accuracy = 63.43000000000001%, Loss = 0.3703217923641205
Epoch: 9500, Batch Gradient Norm: 9.803654878801705
Epoch: 9500, Batch Gradient Norm after: 9.803654878801705
Epoch 9501/10000, Prediction Accuracy = 63.51400000000001%, Loss = 0.3601384162902832
Epoch: 9501, Batch Gradient Norm: 9.147736221794487
Epoch: 9501, Batch Gradient Norm after: 9.147736221794487
Epoch 9502/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.35569673776626587
Epoch: 9502, Batch Gradient Norm: 9.720803547957667
Epoch: 9502, Batch Gradient Norm after: 9.720803547957667
Epoch 9503/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.358954519033432
Epoch: 9503, Batch Gradient Norm: 10.057493258141978
Epoch: 9503, Batch Gradient Norm after: 10.057493258141978
Epoch 9504/10000, Prediction Accuracy = 63.524%, Loss = 0.360045850276947
Epoch: 9504, Batch Gradient Norm: 10.214688663009145
Epoch: 9504, Batch Gradient Norm after: 10.214688663009145
Epoch 9505/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3602607011795044
Epoch: 9505, Batch Gradient Norm: 9.950143944227774
Epoch: 9505, Batch Gradient Norm after: 9.950143944227774
Epoch 9506/10000, Prediction Accuracy = 63.604%, Loss = 0.3589214622974396
Epoch: 9506, Batch Gradient Norm: 9.904685515932
Epoch: 9506, Batch Gradient Norm after: 9.904685515932
Epoch 9507/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.36000707745552063
Epoch: 9507, Batch Gradient Norm: 9.84118608514702
Epoch: 9507, Batch Gradient Norm after: 9.84118608514702
Epoch 9508/10000, Prediction Accuracy = 63.52199999999999%, Loss = 0.3604931473731995
Epoch: 9508, Batch Gradient Norm: 10.757772848881231
Epoch: 9508, Batch Gradient Norm after: 10.757772848881231
Epoch 9509/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.36572203040122986
Epoch: 9509, Batch Gradient Norm: 11.09817917885862
Epoch: 9509, Batch Gradient Norm after: 11.09817917885862
Epoch 9510/10000, Prediction Accuracy = 63.384%, Loss = 0.3684364497661591
Epoch: 9510, Batch Gradient Norm: 10.410649698068912
Epoch: 9510, Batch Gradient Norm after: 10.410649698068912
Epoch 9511/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.36553180813789365
Epoch: 9511, Batch Gradient Norm: 10.117619389227393
Epoch: 9511, Batch Gradient Norm after: 10.117619389227393
Epoch 9512/10000, Prediction Accuracy = 63.38199999999999%, Loss = 0.36315863132476806
Epoch: 9512, Batch Gradient Norm: 13.172066631575717
Epoch: 9512, Batch Gradient Norm after: 13.172066631575717
Epoch 9513/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.38350178599357604
Epoch: 9513, Batch Gradient Norm: 11.90989646571822
Epoch: 9513, Batch Gradient Norm after: 11.90989646571822
Epoch 9514/10000, Prediction Accuracy = 63.524%, Loss = 0.37379068732261655
Epoch: 9514, Batch Gradient Norm: 8.934065178793158
Epoch: 9514, Batch Gradient Norm after: 8.934065178793158
Epoch 9515/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.35350596308708193
Epoch: 9515, Batch Gradient Norm: 9.370303033002418
Epoch: 9515, Batch Gradient Norm after: 9.370303033002418
Epoch 9516/10000, Prediction Accuracy = 63.468%, Loss = 0.3561721682548523
Epoch: 9516, Batch Gradient Norm: 11.511526792280808
Epoch: 9516, Batch Gradient Norm after: 11.511526792280808
Epoch 9517/10000, Prediction Accuracy = 63.464%, Loss = 0.3706930637359619
Epoch: 9517, Batch Gradient Norm: 10.943738521124907
Epoch: 9517, Batch Gradient Norm after: 10.943738521124907
Epoch 9518/10000, Prediction Accuracy = 63.39%, Loss = 0.3669907867908478
Epoch: 9518, Batch Gradient Norm: 8.798526343330446
Epoch: 9518, Batch Gradient Norm after: 8.798526343330446
Epoch 9519/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.35198203325271604
Epoch: 9519, Batch Gradient Norm: 8.070938101377694
Epoch: 9519, Batch Gradient Norm after: 8.070938101377694
Epoch 9520/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.3470679342746735
Epoch: 9520, Batch Gradient Norm: 9.39660727818278
Epoch: 9520, Batch Gradient Norm after: 9.39660727818278
Epoch 9521/10000, Prediction Accuracy = 63.327999999999996%, Loss = 0.35478733777999877
Epoch: 9521, Batch Gradient Norm: 9.874952211688006
Epoch: 9521, Batch Gradient Norm after: 9.874952211688006
Epoch 9522/10000, Prediction Accuracy = 63.662%, Loss = 0.36117084622383117
Epoch: 9522, Batch Gradient Norm: 9.116773118497141
Epoch: 9522, Batch Gradient Norm after: 9.116773118497141
Epoch 9523/10000, Prediction Accuracy = 63.42999999999999%, Loss = 0.35720397233963014
Epoch: 9523, Batch Gradient Norm: 9.87639068989149
Epoch: 9523, Batch Gradient Norm after: 9.87639068989149
Epoch 9524/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3605140745639801
Epoch: 9524, Batch Gradient Norm: 10.139674215860419
Epoch: 9524, Batch Gradient Norm after: 10.139674215860419
Epoch 9525/10000, Prediction Accuracy = 63.462%, Loss = 0.36267411708831787
Epoch: 9525, Batch Gradient Norm: 10.528396031142403
Epoch: 9525, Batch Gradient Norm after: 10.528396031142403
Epoch 9526/10000, Prediction Accuracy = 63.5%, Loss = 0.36547145843505857
Epoch: 9526, Batch Gradient Norm: 10.763233847911382
Epoch: 9526, Batch Gradient Norm after: 10.763233847911382
Epoch 9527/10000, Prediction Accuracy = 63.562%, Loss = 0.36884686946868894
Epoch: 9527, Batch Gradient Norm: 10.549874980657307
Epoch: 9527, Batch Gradient Norm after: 10.549874980657307
Epoch 9528/10000, Prediction Accuracy = 63.452%, Loss = 0.36416649222373965
Epoch: 9528, Batch Gradient Norm: 11.353682063019992
Epoch: 9528, Batch Gradient Norm after: 11.353682063019992
Epoch 9529/10000, Prediction Accuracy = 63.46999999999999%, Loss = 0.36921042799949644
Epoch: 9529, Batch Gradient Norm: 10.97341345971634
Epoch: 9529, Batch Gradient Norm after: 10.97341345971634
Epoch 9530/10000, Prediction Accuracy = 63.510000000000005%, Loss = 0.36760435104370115
Epoch: 9530, Batch Gradient Norm: 9.795537025569978
Epoch: 9530, Batch Gradient Norm after: 9.795537025569978
Epoch 9531/10000, Prediction Accuracy = 63.414%, Loss = 0.35813421607017515
Epoch: 9531, Batch Gradient Norm: 11.22214948072808
Epoch: 9531, Batch Gradient Norm after: 11.22214948072808
Epoch 9532/10000, Prediction Accuracy = 63.372%, Loss = 0.3664562046527863
Epoch: 9532, Batch Gradient Norm: 11.569043154628583
Epoch: 9532, Batch Gradient Norm after: 11.569043154628583
Epoch 9533/10000, Prediction Accuracy = 63.448%, Loss = 0.3692895591259003
Epoch: 9533, Batch Gradient Norm: 9.736617299045216
Epoch: 9533, Batch Gradient Norm after: 9.736617299045216
Epoch 9534/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.35731775760650636
Epoch: 9534, Batch Gradient Norm: 10.463266515390382
Epoch: 9534, Batch Gradient Norm after: 10.463266515390382
Epoch 9535/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3634727537631989
Epoch: 9535, Batch Gradient Norm: 10.427506035928046
Epoch: 9535, Batch Gradient Norm after: 10.427506035928046
Epoch 9536/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3659877240657806
Epoch: 9536, Batch Gradient Norm: 9.068777925633453
Epoch: 9536, Batch Gradient Norm after: 9.068777925633453
Epoch 9537/10000, Prediction Accuracy = 63.562%, Loss = 0.35587315559387206
Epoch: 9537, Batch Gradient Norm: 9.932824503745213
Epoch: 9537, Batch Gradient Norm after: 9.932824503745213
Epoch 9538/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.3601903259754181
Epoch: 9538, Batch Gradient Norm: 10.393656697922893
Epoch: 9538, Batch Gradient Norm after: 10.393656697922893
Epoch 9539/10000, Prediction Accuracy = 63.49400000000001%, Loss = 0.36249717473983767
Epoch: 9539, Batch Gradient Norm: 9.112258822080182
Epoch: 9539, Batch Gradient Norm after: 9.112258822080182
Epoch 9540/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.35330193042755126
Epoch: 9540, Batch Gradient Norm: 10.868509172534074
Epoch: 9540, Batch Gradient Norm after: 10.868509172534074
Epoch 9541/10000, Prediction Accuracy = 63.452%, Loss = 0.36451598405838015
Epoch: 9541, Batch Gradient Norm: 13.378731732123756
Epoch: 9541, Batch Gradient Norm after: 13.378731732123756
Epoch 9542/10000, Prediction Accuracy = 63.452%, Loss = 0.3876424729824066
Epoch: 9542, Batch Gradient Norm: 10.597269527108336
Epoch: 9542, Batch Gradient Norm after: 10.597269527108336
Epoch 9543/10000, Prediction Accuracy = 63.56%, Loss = 0.36512603163719176
Epoch: 9543, Batch Gradient Norm: 8.499838115223795
Epoch: 9543, Batch Gradient Norm after: 8.499838115223795
Epoch 9544/10000, Prediction Accuracy = 63.60799999999999%, Loss = 0.35093397498130796
Epoch: 9544, Batch Gradient Norm: 8.536818026852112
Epoch: 9544, Batch Gradient Norm after: 8.536818026852112
Epoch 9545/10000, Prediction Accuracy = 63.666%, Loss = 0.3504218578338623
Epoch: 9545, Batch Gradient Norm: 10.607522941871277
Epoch: 9545, Batch Gradient Norm after: 10.607522941871277
Epoch 9546/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.3645378649234772
Epoch: 9546, Batch Gradient Norm: 9.882232145898898
Epoch: 9546, Batch Gradient Norm after: 9.882232145898898
Epoch 9547/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3598239779472351
Epoch: 9547, Batch Gradient Norm: 8.477014297132426
Epoch: 9547, Batch Gradient Norm after: 8.477014297132426
Epoch 9548/10000, Prediction Accuracy = 63.488%, Loss = 0.35055710673332213
Epoch: 9548, Batch Gradient Norm: 8.727460336604754
Epoch: 9548, Batch Gradient Norm after: 8.727460336604754
Epoch 9549/10000, Prediction Accuracy = 63.468%, Loss = 0.35240715742111206
Epoch: 9549, Batch Gradient Norm: 10.092959635622607
Epoch: 9549, Batch Gradient Norm after: 10.092959635622607
Epoch 9550/10000, Prediction Accuracy = 63.498000000000005%, Loss = 0.3633234441280365
Epoch: 9550, Batch Gradient Norm: 9.982491174628683
Epoch: 9550, Batch Gradient Norm after: 9.982491174628683
Epoch 9551/10000, Prediction Accuracy = 63.396%, Loss = 0.36234370470046995
Epoch: 9551, Batch Gradient Norm: 11.52734744313816
Epoch: 9551, Batch Gradient Norm after: 11.52734744313816
Epoch 9552/10000, Prediction Accuracy = 63.32000000000001%, Loss = 0.3707914412021637
Epoch: 9552, Batch Gradient Norm: 11.395250049802245
Epoch: 9552, Batch Gradient Norm after: 11.395250049802245
Epoch 9553/10000, Prediction Accuracy = 63.444%, Loss = 0.3703488647937775
Epoch: 9553, Batch Gradient Norm: 8.659350814096111
Epoch: 9553, Batch Gradient Norm after: 8.659350814096111
Epoch 9554/10000, Prediction Accuracy = 63.443999999999996%, Loss = 0.3512685835361481
Epoch: 9554, Batch Gradient Norm: 10.310888943470596
Epoch: 9554, Batch Gradient Norm after: 10.310888943470596
Epoch 9555/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3607933759689331
Epoch: 9555, Batch Gradient Norm: 12.332842574115132
Epoch: 9555, Batch Gradient Norm after: 12.332842574115132
Epoch 9556/10000, Prediction Accuracy = 63.428%, Loss = 0.3791876435279846
Epoch: 9556, Batch Gradient Norm: 9.202973279970386
Epoch: 9556, Batch Gradient Norm after: 9.202973279970386
Epoch 9557/10000, Prediction Accuracy = 63.602%, Loss = 0.3562970101833344
Epoch: 9557, Batch Gradient Norm: 7.34591098124916
Epoch: 9557, Batch Gradient Norm after: 7.34591098124916
Epoch 9558/10000, Prediction Accuracy = 63.576%, Loss = 0.34466038942337035
Epoch: 9558, Batch Gradient Norm: 10.7227476970469
Epoch: 9558, Batch Gradient Norm after: 10.7227476970469
Epoch 9559/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.3643084466457367
Epoch: 9559, Batch Gradient Norm: 12.980853719116968
Epoch: 9559, Batch Gradient Norm after: 12.980853719116968
Epoch 9560/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.38340118527412415
Epoch: 9560, Batch Gradient Norm: 11.527456791745886
Epoch: 9560, Batch Gradient Norm after: 11.527456791745886
Epoch 9561/10000, Prediction Accuracy = 63.480000000000004%, Loss = 0.3709699332714081
Epoch: 9561, Batch Gradient Norm: 9.557923482706226
Epoch: 9561, Batch Gradient Norm after: 9.557923482706226
Epoch 9562/10000, Prediction Accuracy = 63.386%, Loss = 0.35656480193138124
Epoch: 9562, Batch Gradient Norm: 8.838308643384998
Epoch: 9562, Batch Gradient Norm after: 8.838308643384998
Epoch 9563/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.3525401711463928
Epoch: 9563, Batch Gradient Norm: 9.903131351570204
Epoch: 9563, Batch Gradient Norm after: 9.903131351570204
Epoch 9564/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.3592481791973114
Epoch: 9564, Batch Gradient Norm: 11.442188024172708
Epoch: 9564, Batch Gradient Norm after: 11.442188024172708
Epoch 9565/10000, Prediction Accuracy = 63.38399999999999%, Loss = 0.371589732170105
Epoch: 9565, Batch Gradient Norm: 11.529540706249898
Epoch: 9565, Batch Gradient Norm after: 11.529540706249898
Epoch 9566/10000, Prediction Accuracy = 63.65%, Loss = 0.3731037795543671
Epoch: 9566, Batch Gradient Norm: 10.282063775426401
Epoch: 9566, Batch Gradient Norm after: 10.282063775426401
Epoch 9567/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.362360018491745
Epoch: 9567, Batch Gradient Norm: 9.466541330994533
Epoch: 9567, Batch Gradient Norm after: 9.466541330994533
Epoch 9568/10000, Prediction Accuracy = 63.56%, Loss = 0.3559588074684143
Epoch: 9568, Batch Gradient Norm: 9.987518363704886
Epoch: 9568, Batch Gradient Norm after: 9.987518363704886
Epoch 9569/10000, Prediction Accuracy = 63.496%, Loss = 0.35917810201644895
Epoch: 9569, Batch Gradient Norm: 10.288990302087557
Epoch: 9569, Batch Gradient Norm after: 10.288990302087557
Epoch 9570/10000, Prediction Accuracy = 63.572%, Loss = 0.36177192330360414
Epoch: 9570, Batch Gradient Norm: 10.174356769543813
Epoch: 9570, Batch Gradient Norm after: 10.174356769543813
Epoch 9571/10000, Prediction Accuracy = 63.736000000000004%, Loss = 0.3605489432811737
Epoch: 9571, Batch Gradient Norm: 9.845931890118333
Epoch: 9571, Batch Gradient Norm after: 9.845931890118333
Epoch 9572/10000, Prediction Accuracy = 63.658%, Loss = 0.3583745718002319
Epoch: 9572, Batch Gradient Norm: 10.360188338506445
Epoch: 9572, Batch Gradient Norm after: 10.360188338506445
Epoch 9573/10000, Prediction Accuracy = 63.452%, Loss = 0.36117357611656187
Epoch: 9573, Batch Gradient Norm: 10.2575629029632
Epoch: 9573, Batch Gradient Norm after: 10.2575629029632
Epoch 9574/10000, Prediction Accuracy = 63.452%, Loss = 0.36067560911178587
Epoch: 9574, Batch Gradient Norm: 9.048281267287736
Epoch: 9574, Batch Gradient Norm after: 9.048281267287736
Epoch 9575/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.35325510501861573
Epoch: 9575, Batch Gradient Norm: 11.25295412740432
Epoch: 9575, Batch Gradient Norm after: 11.25295412740432
Epoch 9576/10000, Prediction Accuracy = 63.458000000000006%, Loss = 0.36794084310531616
Epoch: 9576, Batch Gradient Norm: 12.108538987875571
Epoch: 9576, Batch Gradient Norm after: 12.108538987875571
Epoch 9577/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.3758805274963379
Epoch: 9577, Batch Gradient Norm: 9.232432402426952
Epoch: 9577, Batch Gradient Norm after: 9.232432402426952
Epoch 9578/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.3549760401248932
Epoch: 9578, Batch Gradient Norm: 8.938897521360735
Epoch: 9578, Batch Gradient Norm after: 8.938897521360735
Epoch 9579/10000, Prediction Accuracy = 63.48%, Loss = 0.3532079577445984
Epoch: 9579, Batch Gradient Norm: 10.231852613845172
Epoch: 9579, Batch Gradient Norm after: 10.231852613845172
Epoch 9580/10000, Prediction Accuracy = 63.448%, Loss = 0.36197460889816285
Epoch: 9580, Batch Gradient Norm: 9.974125095372024
Epoch: 9580, Batch Gradient Norm after: 9.974125095372024
Epoch 9581/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.36143453121185304
Epoch: 9581, Batch Gradient Norm: 9.323429140059254
Epoch: 9581, Batch Gradient Norm after: 9.323429140059254
Epoch 9582/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.35616667866706847
Epoch: 9582, Batch Gradient Norm: 7.936473211157728
Epoch: 9582, Batch Gradient Norm after: 7.936473211157728
Epoch 9583/10000, Prediction Accuracy = 63.45%, Loss = 0.34807662963867186
Epoch: 9583, Batch Gradient Norm: 8.172617557441884
Epoch: 9583, Batch Gradient Norm after: 8.172617557441884
Epoch 9584/10000, Prediction Accuracy = 63.54599999999999%, Loss = 0.3480344295501709
Epoch: 9584, Batch Gradient Norm: 10.818531859873755
Epoch: 9584, Batch Gradient Norm after: 10.818531859873755
Epoch 9585/10000, Prediction Accuracy = 63.552%, Loss = 0.36613830327987673
Epoch: 9585, Batch Gradient Norm: 12.431086558986843
Epoch: 9585, Batch Gradient Norm after: 12.431086558986843
Epoch 9586/10000, Prediction Accuracy = 63.424%, Loss = 0.37991541624069214
Epoch: 9586, Batch Gradient Norm: 10.885861566362706
Epoch: 9586, Batch Gradient Norm after: 10.885861566362706
Epoch 9587/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3638304114341736
Epoch: 9587, Batch Gradient Norm: 10.505111671601908
Epoch: 9587, Batch Gradient Norm after: 10.505111671601908
Epoch 9588/10000, Prediction Accuracy = 63.508%, Loss = 0.3607087194919586
Epoch: 9588, Batch Gradient Norm: 11.985313328392284
Epoch: 9588, Batch Gradient Norm after: 11.985313328392284
Epoch 9589/10000, Prediction Accuracy = 63.452%, Loss = 0.3740368366241455
Epoch: 9589, Batch Gradient Norm: 11.921004596101302
Epoch: 9589, Batch Gradient Norm after: 11.921004596101302
Epoch 9590/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.37587944865226747
Epoch: 9590, Batch Gradient Norm: 9.742511879069246
Epoch: 9590, Batch Gradient Norm after: 9.742511879069246
Epoch 9591/10000, Prediction Accuracy = 63.548%, Loss = 0.35939385294914244
Epoch: 9591, Batch Gradient Norm: 9.094846852253841
Epoch: 9591, Batch Gradient Norm after: 9.094846852253841
Epoch 9592/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.35473025441169737
Epoch: 9592, Batch Gradient Norm: 10.957322433819076
Epoch: 9592, Batch Gradient Norm after: 10.957322433819076
Epoch 9593/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3654913902282715
Epoch: 9593, Batch Gradient Norm: 11.187728877654406
Epoch: 9593, Batch Gradient Norm after: 11.187728877654406
Epoch 9594/10000, Prediction Accuracy = 63.504%, Loss = 0.3681556940078735
Epoch: 9594, Batch Gradient Norm: 9.790372749498601
Epoch: 9594, Batch Gradient Norm after: 9.790372749498601
Epoch 9595/10000, Prediction Accuracy = 63.626%, Loss = 0.3587361812591553
Epoch: 9595, Batch Gradient Norm: 9.550014965935457
Epoch: 9595, Batch Gradient Norm after: 9.550014965935457
Epoch 9596/10000, Prediction Accuracy = 63.54200000000001%, Loss = 0.35756840109825133
Epoch: 9596, Batch Gradient Norm: 9.98079850868849
Epoch: 9596, Batch Gradient Norm after: 9.98079850868849
Epoch 9597/10000, Prediction Accuracy = 63.44199999999999%, Loss = 0.3608186483383179
Epoch: 9597, Batch Gradient Norm: 8.360908419470729
Epoch: 9597, Batch Gradient Norm after: 8.360908419470729
Epoch 9598/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.3494339227676392
Epoch: 9598, Batch Gradient Norm: 9.07129087993965
Epoch: 9598, Batch Gradient Norm after: 9.07129087993965
Epoch 9599/10000, Prediction Accuracy = 63.564%, Loss = 0.35269929766654967
Epoch: 9599, Batch Gradient Norm: 10.02960807897059
Epoch: 9599, Batch Gradient Norm after: 10.02960807897059
Epoch 9600/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.3588975191116333
Epoch: 9600, Batch Gradient Norm: 11.312136872610047
Epoch: 9600, Batch Gradient Norm after: 11.312136872610047
Epoch 9601/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3699501931667328
Epoch: 9601, Batch Gradient Norm: 11.368029215466088
Epoch: 9601, Batch Gradient Norm after: 11.368029215466088
Epoch 9602/10000, Prediction Accuracy = 63.30999999999999%, Loss = 0.3710263192653656
Epoch: 9602, Batch Gradient Norm: 10.107805215680198
Epoch: 9602, Batch Gradient Norm after: 10.107805215680198
Epoch 9603/10000, Prediction Accuracy = 63.513999999999996%, Loss = 0.360767787694931
Epoch: 9603, Batch Gradient Norm: 8.856608717795337
Epoch: 9603, Batch Gradient Norm after: 8.856608717795337
Epoch 9604/10000, Prediction Accuracy = 63.636%, Loss = 0.351437908411026
Epoch: 9604, Batch Gradient Norm: 10.447016174150175
Epoch: 9604, Batch Gradient Norm after: 10.447016174150175
Epoch 9605/10000, Prediction Accuracy = 63.402%, Loss = 0.3600176215171814
Epoch: 9605, Batch Gradient Norm: 11.667354279442154
Epoch: 9605, Batch Gradient Norm after: 11.667354279442154
Epoch 9606/10000, Prediction Accuracy = 63.24400000000001%, Loss = 0.3700395107269287
Epoch: 9606, Batch Gradient Norm: 10.395347909133301
Epoch: 9606, Batch Gradient Norm after: 10.395347909133301
Epoch 9607/10000, Prediction Accuracy = 63.574%, Loss = 0.3635413408279419
Epoch: 9607, Batch Gradient Norm: 9.714636827815648
Epoch: 9607, Batch Gradient Norm after: 9.714636827815648
Epoch 9608/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.358151251077652
Epoch: 9608, Batch Gradient Norm: 9.157461507640917
Epoch: 9608, Batch Gradient Norm after: 9.157461507640917
Epoch 9609/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.3539243519306183
Epoch: 9609, Batch Gradient Norm: 8.731954941923284
Epoch: 9609, Batch Gradient Norm after: 8.731954941923284
Epoch 9610/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.3503895103931427
Epoch: 9610, Batch Gradient Norm: 10.224981930249523
Epoch: 9610, Batch Gradient Norm after: 10.224981930249523
Epoch 9611/10000, Prediction Accuracy = 63.548%, Loss = 0.359371155500412
Epoch: 9611, Batch Gradient Norm: 11.924327388098677
Epoch: 9611, Batch Gradient Norm after: 11.924327388098677
Epoch 9612/10000, Prediction Accuracy = 63.517999999999994%, Loss = 0.37378202080726625
Epoch: 9612, Batch Gradient Norm: 10.404599929320375
Epoch: 9612, Batch Gradient Norm after: 10.404599929320375
Epoch 9613/10000, Prediction Accuracy = 63.516%, Loss = 0.3623543977737427
Epoch: 9613, Batch Gradient Norm: 9.973707425333739
Epoch: 9613, Batch Gradient Norm after: 9.973707425333739
Epoch 9614/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.35853220224380494
Epoch: 9614, Batch Gradient Norm: 11.647154907983724
Epoch: 9614, Batch Gradient Norm after: 11.647154907983724
Epoch 9615/10000, Prediction Accuracy = 63.438%, Loss = 0.37123964428901673
Epoch: 9615, Batch Gradient Norm: 10.83316193853078
Epoch: 9615, Batch Gradient Norm after: 10.83316193853078
Epoch 9616/10000, Prediction Accuracy = 63.472%, Loss = 0.3653299629688263
Epoch: 9616, Batch Gradient Norm: 9.635818922138634
Epoch: 9616, Batch Gradient Norm after: 9.635818922138634
Epoch 9617/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.3554478585720062
Epoch: 9617, Batch Gradient Norm: 11.508138782721717
Epoch: 9617, Batch Gradient Norm after: 11.508138782721717
Epoch 9618/10000, Prediction Accuracy = 63.488%, Loss = 0.36772351861000063
Epoch: 9618, Batch Gradient Norm: 12.745559044688603
Epoch: 9618, Batch Gradient Norm after: 12.745559044688603
Epoch 9619/10000, Prediction Accuracy = 63.5%, Loss = 0.3788318932056427
Epoch: 9619, Batch Gradient Norm: 10.106964254518116
Epoch: 9619, Batch Gradient Norm after: 10.106964254518116
Epoch 9620/10000, Prediction Accuracy = 63.5%, Loss = 0.36006578207015993
Epoch: 9620, Batch Gradient Norm: 8.80292500832833
Epoch: 9620, Batch Gradient Norm after: 8.80292500832833
Epoch 9621/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.3528127372264862
Epoch: 9621, Batch Gradient Norm: 8.903556051085927
Epoch: 9621, Batch Gradient Norm after: 8.903556051085927
Epoch 9622/10000, Prediction Accuracy = 63.614%, Loss = 0.35290364623069764
Epoch: 9622, Batch Gradient Norm: 10.04389612092793
Epoch: 9622, Batch Gradient Norm after: 10.04389612092793
Epoch 9623/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.35960724353790285
Epoch: 9623, Batch Gradient Norm: 9.827709674748437
Epoch: 9623, Batch Gradient Norm after: 9.827709674748437
Epoch 9624/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.3587236046791077
Epoch: 9624, Batch Gradient Norm: 9.279926920726039
Epoch: 9624, Batch Gradient Norm after: 9.279926920726039
Epoch 9625/10000, Prediction Accuracy = 63.48%, Loss = 0.3557321548461914
Epoch: 9625, Batch Gradient Norm: 10.567494068431161
Epoch: 9625, Batch Gradient Norm after: 10.567494068431161
Epoch 9626/10000, Prediction Accuracy = 63.56%, Loss = 0.36369884610176084
Epoch: 9626, Batch Gradient Norm: 10.532412720279932
Epoch: 9626, Batch Gradient Norm after: 10.532412720279932
Epoch 9627/10000, Prediction Accuracy = 63.54%, Loss = 0.3632036209106445
Epoch: 9627, Batch Gradient Norm: 8.908907613229896
Epoch: 9627, Batch Gradient Norm after: 8.908907613229896
Epoch 9628/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3521831095218658
Epoch: 9628, Batch Gradient Norm: 9.458915724346133
Epoch: 9628, Batch Gradient Norm after: 9.458915724346133
Epoch 9629/10000, Prediction Accuracy = 63.354%, Loss = 0.35583850741386414
Epoch: 9629, Batch Gradient Norm: 10.84288730330088
Epoch: 9629, Batch Gradient Norm after: 10.84288730330088
Epoch 9630/10000, Prediction Accuracy = 63.35%, Loss = 0.3666859447956085
Epoch: 9630, Batch Gradient Norm: 11.209520311788827
Epoch: 9630, Batch Gradient Norm after: 11.209520311788827
Epoch 9631/10000, Prediction Accuracy = 63.664%, Loss = 0.368496310710907
Epoch: 9631, Batch Gradient Norm: 11.645280219684459
Epoch: 9631, Batch Gradient Norm after: 11.645280219684459
Epoch 9632/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.3709199547767639
Epoch: 9632, Batch Gradient Norm: 11.746797895190685
Epoch: 9632, Batch Gradient Norm after: 11.746797895190685
Epoch 9633/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.37195812463760375
Epoch: 9633, Batch Gradient Norm: 9.12500186410238
Epoch: 9633, Batch Gradient Norm after: 9.12500186410238
Epoch 9634/10000, Prediction Accuracy = 63.754%, Loss = 0.3542142152786255
Epoch: 9634, Batch Gradient Norm: 8.877703495704246
Epoch: 9634, Batch Gradient Norm after: 8.877703495704246
Epoch 9635/10000, Prediction Accuracy = 63.622%, Loss = 0.3518201053142548
Epoch: 9635, Batch Gradient Norm: 9.09308876126019
Epoch: 9635, Batch Gradient Norm after: 9.09308876126019
Epoch 9636/10000, Prediction Accuracy = 63.467999999999996%, Loss = 0.3520350635051727
Epoch: 9636, Batch Gradient Norm: 8.459431354206036
Epoch: 9636, Batch Gradient Norm after: 8.459431354206036
Epoch 9637/10000, Prediction Accuracy = 63.507999999999996%, Loss = 0.3489721894264221
Epoch: 9637, Batch Gradient Norm: 8.75008127180762
Epoch: 9637, Batch Gradient Norm after: 8.75008127180762
Epoch 9638/10000, Prediction Accuracy = 63.572%, Loss = 0.3512356638908386
Epoch: 9638, Batch Gradient Norm: 11.167649463678151
Epoch: 9638, Batch Gradient Norm after: 11.167649463678151
Epoch 9639/10000, Prediction Accuracy = 63.638%, Loss = 0.3685783684253693
Epoch: 9639, Batch Gradient Norm: 13.078142408002444
Epoch: 9639, Batch Gradient Norm after: 13.078142408002444
Epoch 9640/10000, Prediction Accuracy = 63.395999999999994%, Loss = 0.38347762227058413
Epoch: 9640, Batch Gradient Norm: 11.021735879902819
Epoch: 9640, Batch Gradient Norm after: 11.021735879902819
Epoch 9641/10000, Prediction Accuracy = 63.410000000000004%, Loss = 0.3652453660964966
Epoch: 9641, Batch Gradient Norm: 9.07834138074148
Epoch: 9641, Batch Gradient Norm after: 9.07834138074148
Epoch 9642/10000, Prediction Accuracy = 63.592%, Loss = 0.3513394594192505
Epoch: 9642, Batch Gradient Norm: 10.660969211519053
Epoch: 9642, Batch Gradient Norm after: 10.660969211519053
Epoch 9643/10000, Prediction Accuracy = 63.46999999999999%, Loss = 0.3624863505363464
Epoch: 9643, Batch Gradient Norm: 10.514900708186781
Epoch: 9643, Batch Gradient Norm after: 10.514900708186781
Epoch 9644/10000, Prediction Accuracy = 63.564%, Loss = 0.36482598185539244
Epoch: 9644, Batch Gradient Norm: 8.872533166345791
Epoch: 9644, Batch Gradient Norm after: 8.872533166345791
Epoch 9645/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35345497727394104
Epoch: 9645, Batch Gradient Norm: 9.554445506329358
Epoch: 9645, Batch Gradient Norm after: 9.554445506329358
Epoch 9646/10000, Prediction Accuracy = 63.348%, Loss = 0.3562439978122711
Epoch: 9646, Batch Gradient Norm: 11.20605024313299
Epoch: 9646, Batch Gradient Norm after: 11.20605024313299
Epoch 9647/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.36763992309570315
Epoch: 9647, Batch Gradient Norm: 11.293113045513659
Epoch: 9647, Batch Gradient Norm after: 11.293113045513659
Epoch 9648/10000, Prediction Accuracy = 63.492000000000004%, Loss = 0.36928796768188477
Epoch: 9648, Batch Gradient Norm: 9.970611071819674
Epoch: 9648, Batch Gradient Norm after: 9.970611071819674
Epoch 9649/10000, Prediction Accuracy = 63.71%, Loss = 0.36049286723136903
Epoch: 9649, Batch Gradient Norm: 9.422542219490056
Epoch: 9649, Batch Gradient Norm after: 9.422542219490056
Epoch 9650/10000, Prediction Accuracy = 63.576%, Loss = 0.35653679370880126
Epoch: 9650, Batch Gradient Norm: 10.176271541839148
Epoch: 9650, Batch Gradient Norm after: 10.176271541839148
Epoch 9651/10000, Prediction Accuracy = 63.42%, Loss = 0.3613700270652771
Epoch: 9651, Batch Gradient Norm: 9.829543823579268
Epoch: 9651, Batch Gradient Norm after: 9.829543823579268
Epoch 9652/10000, Prediction Accuracy = 63.65000000000001%, Loss = 0.3584643125534058
Epoch: 9652, Batch Gradient Norm: 10.464924473598622
Epoch: 9652, Batch Gradient Norm after: 10.464924473598622
Epoch 9653/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3622497200965881
Epoch: 9653, Batch Gradient Norm: 10.346823880331247
Epoch: 9653, Batch Gradient Norm after: 10.346823880331247
Epoch 9654/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.36134899258613584
Epoch: 9654, Batch Gradient Norm: 10.847902388252878
Epoch: 9654, Batch Gradient Norm after: 10.847902388252878
Epoch 9655/10000, Prediction Accuracy = 63.45400000000001%, Loss = 0.36321414113044737
Epoch: 9655, Batch Gradient Norm: 10.689954854511987
Epoch: 9655, Batch Gradient Norm after: 10.689954854511987
Epoch 9656/10000, Prediction Accuracy = 63.394000000000005%, Loss = 0.3623009204864502
Epoch: 9656, Batch Gradient Norm: 10.432786053270956
Epoch: 9656, Batch Gradient Norm after: 10.432786053270956
Epoch 9657/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3594525933265686
Epoch: 9657, Batch Gradient Norm: 10.655639923590781
Epoch: 9657, Batch Gradient Norm after: 10.655639923590781
Epoch 9658/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.36215182542800906
Epoch: 9658, Batch Gradient Norm: 10.50013653938524
Epoch: 9658, Batch Gradient Norm after: 10.50013653938524
Epoch 9659/10000, Prediction Accuracy = 63.5%, Loss = 0.36200096011161803
Epoch: 9659, Batch Gradient Norm: 10.999718477332806
Epoch: 9659, Batch Gradient Norm after: 10.999718477332806
Epoch 9660/10000, Prediction Accuracy = 63.486000000000004%, Loss = 0.3664377748966217
Epoch: 9660, Batch Gradient Norm: 10.412793863410611
Epoch: 9660, Batch Gradient Norm after: 10.412793863410611
Epoch 9661/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3626926302909851
Epoch: 9661, Batch Gradient Norm: 9.056661622066898
Epoch: 9661, Batch Gradient Norm after: 9.056661622066898
Epoch 9662/10000, Prediction Accuracy = 63.612%, Loss = 0.35300583243370054
Epoch: 9662, Batch Gradient Norm: 8.454817580388024
Epoch: 9662, Batch Gradient Norm after: 8.454817580388024
Epoch 9663/10000, Prediction Accuracy = 63.626%, Loss = 0.34955726861953734
Epoch: 9663, Batch Gradient Norm: 10.053613658837076
Epoch: 9663, Batch Gradient Norm after: 10.053613658837076
Epoch 9664/10000, Prediction Accuracy = 63.48%, Loss = 0.3595836102962494
Epoch: 9664, Batch Gradient Norm: 10.818959349882187
Epoch: 9664, Batch Gradient Norm after: 10.818959349882187
Epoch 9665/10000, Prediction Accuracy = 63.64%, Loss = 0.36622422337532046
Epoch: 9665, Batch Gradient Norm: 8.728619986513648
Epoch: 9665, Batch Gradient Norm after: 8.728619986513648
Epoch 9666/10000, Prediction Accuracy = 63.574%, Loss = 0.3516179800033569
Epoch: 9666, Batch Gradient Norm: 8.50264782119126
Epoch: 9666, Batch Gradient Norm after: 8.50264782119126
Epoch 9667/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.3492036938667297
Epoch: 9667, Batch Gradient Norm: 10.562596013101826
Epoch: 9667, Batch Gradient Norm after: 10.562596013101826
Epoch 9668/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.36206725239753723
Epoch: 9668, Batch Gradient Norm: 11.892377076132586
Epoch: 9668, Batch Gradient Norm after: 11.892377076132586
Epoch 9669/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.37299199104309083
Epoch: 9669, Batch Gradient Norm: 11.939585838238362
Epoch: 9669, Batch Gradient Norm after: 11.939585838238362
Epoch 9670/10000, Prediction Accuracy = 63.55%, Loss = 0.37255299687385557
Epoch: 9670, Batch Gradient Norm: 11.672661186789986
Epoch: 9670, Batch Gradient Norm after: 11.672661186789986
Epoch 9671/10000, Prediction Accuracy = 63.419999999999995%, Loss = 0.3681153476238251
Epoch: 9671, Batch Gradient Norm: 11.030640751824441
Epoch: 9671, Batch Gradient Norm after: 11.030640751824441
Epoch 9672/10000, Prediction Accuracy = 63.378%, Loss = 0.36469108462333677
Epoch: 9672, Batch Gradient Norm: 9.730554819783615
Epoch: 9672, Batch Gradient Norm after: 9.730554819783615
Epoch 9673/10000, Prediction Accuracy = 63.614%, Loss = 0.3567128717899323
Epoch: 9673, Batch Gradient Norm: 9.265014146765932
Epoch: 9673, Batch Gradient Norm after: 9.265014146765932
Epoch 9674/10000, Prediction Accuracy = 63.60600000000001%, Loss = 0.3541867256164551
Epoch: 9674, Batch Gradient Norm: 9.143982376866534
Epoch: 9674, Batch Gradient Norm after: 9.143982376866534
Epoch 9675/10000, Prediction Accuracy = 63.678%, Loss = 0.3527517199516296
Epoch: 9675, Batch Gradient Norm: 9.436151709575022
Epoch: 9675, Batch Gradient Norm after: 9.436151709575022
Epoch 9676/10000, Prediction Accuracy = 63.59000000000001%, Loss = 0.3521330118179321
Epoch: 9676, Batch Gradient Norm: 10.47340691927057
Epoch: 9676, Batch Gradient Norm after: 10.47340691927057
Epoch 9677/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.3585620164871216
Epoch: 9677, Batch Gradient Norm: 11.163980493952982
Epoch: 9677, Batch Gradient Norm after: 11.163980493952982
Epoch 9678/10000, Prediction Accuracy = 63.53399999999999%, Loss = 0.3650308847427368
Epoch: 9678, Batch Gradient Norm: 11.562219468885571
Epoch: 9678, Batch Gradient Norm after: 11.562219468885571
Epoch 9679/10000, Prediction Accuracy = 63.666%, Loss = 0.3720738530158997
Epoch: 9679, Batch Gradient Norm: 9.336652131298306
Epoch: 9679, Batch Gradient Norm after: 9.336652131298306
Epoch 9680/10000, Prediction Accuracy = 63.464%, Loss = 0.35521175861358645
Epoch: 9680, Batch Gradient Norm: 8.832783448712776
Epoch: 9680, Batch Gradient Norm after: 8.832783448712776
Epoch 9681/10000, Prediction Accuracy = 63.45399999999999%, Loss = 0.3515334606170654
Epoch: 9681, Batch Gradient Norm: 10.760631741046309
Epoch: 9681, Batch Gradient Norm after: 10.760631741046309
Epoch 9682/10000, Prediction Accuracy = 63.58200000000001%, Loss = 0.36504505276679994
Epoch: 9682, Batch Gradient Norm: 11.498980627782023
Epoch: 9682, Batch Gradient Norm after: 11.498980627782023
Epoch 9683/10000, Prediction Accuracy = 63.636%, Loss = 0.37053072452545166
Epoch: 9683, Batch Gradient Norm: 10.708576141733007
Epoch: 9683, Batch Gradient Norm after: 10.708576141733007
Epoch 9684/10000, Prediction Accuracy = 63.568%, Loss = 0.3642346203327179
Epoch: 9684, Batch Gradient Norm: 9.605619501784231
Epoch: 9684, Batch Gradient Norm after: 9.605619501784231
Epoch 9685/10000, Prediction Accuracy = 63.396%, Loss = 0.3563234627246857
Epoch: 9685, Batch Gradient Norm: 10.346270199789203
Epoch: 9685, Batch Gradient Norm after: 10.346270199789203
Epoch 9686/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.36107903718948364
Epoch: 9686, Batch Gradient Norm: 9.78934556409558
Epoch: 9686, Batch Gradient Norm after: 9.78934556409558
Epoch 9687/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3590068697929382
Epoch: 9687, Batch Gradient Norm: 9.731199232075204
Epoch: 9687, Batch Gradient Norm after: 9.731199232075204
Epoch 9688/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.35621586441993713
Epoch: 9688, Batch Gradient Norm: 11.670789887231813
Epoch: 9688, Batch Gradient Norm after: 11.670789887231813
Epoch 9689/10000, Prediction Accuracy = 63.512%, Loss = 0.36871371865272523
Epoch: 9689, Batch Gradient Norm: 10.885882558366331
Epoch: 9689, Batch Gradient Norm after: 10.885882558366331
Epoch 9690/10000, Prediction Accuracy = 63.536%, Loss = 0.36380364298820494
Epoch: 9690, Batch Gradient Norm: 9.618978432916661
Epoch: 9690, Batch Gradient Norm after: 9.618978432916661
Epoch 9691/10000, Prediction Accuracy = 63.6%, Loss = 0.3558939337730408
Epoch: 9691, Batch Gradient Norm: 9.401338582198717
Epoch: 9691, Batch Gradient Norm after: 9.401338582198717
Epoch 9692/10000, Prediction Accuracy = 63.504%, Loss = 0.3557859122753143
Epoch: 9692, Batch Gradient Norm: 8.240185207447755
Epoch: 9692, Batch Gradient Norm after: 8.240185207447755
Epoch 9693/10000, Prediction Accuracy = 63.44%, Loss = 0.34911489486694336
Epoch: 9693, Batch Gradient Norm: 7.856773231815043
Epoch: 9693, Batch Gradient Norm after: 7.856773231815043
Epoch 9694/10000, Prediction Accuracy = 63.574%, Loss = 0.3467120110988617
Epoch: 9694, Batch Gradient Norm: 10.246174032716214
Epoch: 9694, Batch Gradient Norm after: 10.246174032716214
Epoch 9695/10000, Prediction Accuracy = 63.604%, Loss = 0.3612430810928345
Epoch: 9695, Batch Gradient Norm: 12.07985418618493
Epoch: 9695, Batch Gradient Norm after: 12.07985418618493
Epoch 9696/10000, Prediction Accuracy = 63.48%, Loss = 0.37610981464385984
Epoch: 9696, Batch Gradient Norm: 10.544032455696762
Epoch: 9696, Batch Gradient Norm after: 10.544032455696762
Epoch 9697/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.3629684805870056
Epoch: 9697, Batch Gradient Norm: 9.745084269617575
Epoch: 9697, Batch Gradient Norm after: 9.745084269617575
Epoch 9698/10000, Prediction Accuracy = 63.536%, Loss = 0.35580930709838865
Epoch: 9698, Batch Gradient Norm: 10.467001607812879
Epoch: 9698, Batch Gradient Norm after: 10.467001607812879
Epoch 9699/10000, Prediction Accuracy = 63.5%, Loss = 0.35998262763023375
Epoch: 9699, Batch Gradient Norm: 11.895641722756741
Epoch: 9699, Batch Gradient Norm after: 11.895641722756741
Epoch 9700/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.37081509828567505
Epoch: 9700, Batch Gradient Norm: 11.036084106085472
Epoch: 9700, Batch Gradient Norm after: 11.036084106085472
Epoch 9701/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.36479427814483645
Epoch: 9701, Batch Gradient Norm: 9.87353242576798
Epoch: 9701, Batch Gradient Norm after: 9.87353242576798
Epoch 9702/10000, Prediction Accuracy = 63.48%, Loss = 0.35764907002449037
Epoch: 9702, Batch Gradient Norm: 9.596190554627663
Epoch: 9702, Batch Gradient Norm after: 9.596190554627663
Epoch 9703/10000, Prediction Accuracy = 63.616%, Loss = 0.3553226113319397
Epoch: 9703, Batch Gradient Norm: 10.49126462821952
Epoch: 9703, Batch Gradient Norm after: 10.49126462821952
Epoch 9704/10000, Prediction Accuracy = 63.512%, Loss = 0.3612201452255249
Epoch: 9704, Batch Gradient Norm: 10.955105886785468
Epoch: 9704, Batch Gradient Norm after: 10.955105886785468
Epoch 9705/10000, Prediction Accuracy = 63.648%, Loss = 0.3652851104736328
Epoch: 9705, Batch Gradient Norm: 9.616230573790327
Epoch: 9705, Batch Gradient Norm after: 9.616230573790327
Epoch 9706/10000, Prediction Accuracy = 63.733999999999995%, Loss = 0.35536894798278806
Epoch: 9706, Batch Gradient Norm: 9.31621697209663
Epoch: 9706, Batch Gradient Norm after: 9.31621697209663
Epoch 9707/10000, Prediction Accuracy = 63.652%, Loss = 0.35235267877578735
Epoch: 9707, Batch Gradient Norm: 10.574835112278594
Epoch: 9707, Batch Gradient Norm after: 10.574835112278594
Epoch 9708/10000, Prediction Accuracy = 63.5%, Loss = 0.3601318717002869
Epoch: 9708, Batch Gradient Norm: 12.263206137456834
Epoch: 9708, Batch Gradient Norm after: 12.263206137456834
Epoch 9709/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3749187231063843
Epoch: 9709, Batch Gradient Norm: 10.979196690948447
Epoch: 9709, Batch Gradient Norm after: 10.979196690948447
Epoch 9710/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.367292058467865
Epoch: 9710, Batch Gradient Norm: 8.727155963341229
Epoch: 9710, Batch Gradient Norm after: 8.727155963341229
Epoch 9711/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.35248241424560545
Epoch: 9711, Batch Gradient Norm: 7.9782958689335945
Epoch: 9711, Batch Gradient Norm after: 7.9782958689335945
Epoch 9712/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.3470196664333344
Epoch: 9712, Batch Gradient Norm: 8.910343588310315
Epoch: 9712, Batch Gradient Norm after: 8.910343588310315
Epoch 9713/10000, Prediction Accuracy = 63.53000000000001%, Loss = 0.3515662491321564
Epoch: 9713, Batch Gradient Norm: 10.016415079312482
Epoch: 9713, Batch Gradient Norm after: 10.016415079312482
Epoch 9714/10000, Prediction Accuracy = 63.462%, Loss = 0.3584384322166443
Epoch: 9714, Batch Gradient Norm: 9.67952081087838
Epoch: 9714, Batch Gradient Norm after: 9.67952081087838
Epoch 9715/10000, Prediction Accuracy = 63.346000000000004%, Loss = 0.3555737495422363
Epoch: 9715, Batch Gradient Norm: 11.770914833350073
Epoch: 9715, Batch Gradient Norm after: 11.770914833350073
Epoch 9716/10000, Prediction Accuracy = 63.372%, Loss = 0.3686628818511963
Epoch: 9716, Batch Gradient Norm: 11.953316593095522
Epoch: 9716, Batch Gradient Norm after: 11.953316593095522
Epoch 9717/10000, Prediction Accuracy = 63.59000000000001%, Loss = 0.37223761081695556
Epoch: 9717, Batch Gradient Norm: 11.344074045240946
Epoch: 9717, Batch Gradient Norm after: 11.344074045240946
Epoch 9718/10000, Prediction Accuracy = 63.512%, Loss = 0.3665484130382538
Epoch: 9718, Batch Gradient Norm: 12.63542604680071
Epoch: 9718, Batch Gradient Norm after: 12.63542604680071
Epoch 9719/10000, Prediction Accuracy = 63.556000000000004%, Loss = 0.3775635302066803
Epoch: 9719, Batch Gradient Norm: 11.021818020208924
Epoch: 9719, Batch Gradient Norm after: 11.021818020208924
Epoch 9720/10000, Prediction Accuracy = 63.622%, Loss = 0.3666476309299469
Epoch: 9720, Batch Gradient Norm: 8.294720592290288
Epoch: 9720, Batch Gradient Norm after: 8.294720592290288
Epoch 9721/10000, Prediction Accuracy = 63.717999999999996%, Loss = 0.34782121777534486
Epoch: 9721, Batch Gradient Norm: 7.735647869296665
Epoch: 9721, Batch Gradient Norm after: 7.735647869296665
Epoch 9722/10000, Prediction Accuracy = 63.512%, Loss = 0.3437065124511719
Epoch: 9722, Batch Gradient Norm: 8.615299698107373
Epoch: 9722, Batch Gradient Norm after: 8.615299698107373
Epoch 9723/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.34857924580574035
Epoch: 9723, Batch Gradient Norm: 10.48169898544037
Epoch: 9723, Batch Gradient Norm after: 10.48169898544037
Epoch 9724/10000, Prediction Accuracy = 63.548%, Loss = 0.3608784437179565
Epoch: 9724, Batch Gradient Norm: 11.327128549692333
Epoch: 9724, Batch Gradient Norm after: 11.327128549692333
Epoch 9725/10000, Prediction Accuracy = 63.50600000000001%, Loss = 0.3675810158252716
Epoch: 9725, Batch Gradient Norm: 10.609647984234028
Epoch: 9725, Batch Gradient Norm after: 10.609647984234028
Epoch 9726/10000, Prediction Accuracy = 63.522000000000006%, Loss = 0.363593852519989
Epoch: 9726, Batch Gradient Norm: 9.046106326416322
Epoch: 9726, Batch Gradient Norm after: 9.046106326416322
Epoch 9727/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.3533097982406616
Epoch: 9727, Batch Gradient Norm: 9.674620265416584
Epoch: 9727, Batch Gradient Norm after: 9.674620265416584
Epoch 9728/10000, Prediction Accuracy = 63.70799999999999%, Loss = 0.3562750995159149
Epoch: 9728, Batch Gradient Norm: 10.03057052346513
Epoch: 9728, Batch Gradient Norm after: 10.03057052346513
Epoch 9729/10000, Prediction Accuracy = 63.552%, Loss = 0.35796022415161133
Epoch: 9729, Batch Gradient Norm: 10.309383720662913
Epoch: 9729, Batch Gradient Norm after: 10.309383720662913
Epoch 9730/10000, Prediction Accuracy = 63.622%, Loss = 0.36008901596069337
Epoch: 9730, Batch Gradient Norm: 10.491782029405432
Epoch: 9730, Batch Gradient Norm after: 10.491782029405432
Epoch 9731/10000, Prediction Accuracy = 63.678%, Loss = 0.3612981975078583
Epoch: 9731, Batch Gradient Norm: 10.918777406094943
Epoch: 9731, Batch Gradient Norm after: 10.918777406094943
Epoch 9732/10000, Prediction Accuracy = 63.75600000000001%, Loss = 0.36330329179763793
Epoch: 9732, Batch Gradient Norm: 10.63047351832541
Epoch: 9732, Batch Gradient Norm after: 10.63047351832541
Epoch 9733/10000, Prediction Accuracy = 63.528%, Loss = 0.36068645119667053
Epoch: 9733, Batch Gradient Norm: 9.034371069675716
Epoch: 9733, Batch Gradient Norm after: 9.034371069675716
Epoch 9734/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3508488655090332
Epoch: 9734, Batch Gradient Norm: 8.434325128945918
Epoch: 9734, Batch Gradient Norm after: 8.434325128945918
Epoch 9735/10000, Prediction Accuracy = 63.6%, Loss = 0.348463898897171
Epoch: 9735, Batch Gradient Norm: 10.104058630896072
Epoch: 9735, Batch Gradient Norm after: 10.104058630896072
Epoch 9736/10000, Prediction Accuracy = 63.658%, Loss = 0.3610642373561859
Epoch: 9736, Batch Gradient Norm: 11.164972894162553
Epoch: 9736, Batch Gradient Norm after: 11.164972894162553
Epoch 9737/10000, Prediction Accuracy = 63.577999999999996%, Loss = 0.3686015844345093
Epoch: 9737, Batch Gradient Norm: 10.882717020473027
Epoch: 9737, Batch Gradient Norm after: 10.882717020473027
Epoch 9738/10000, Prediction Accuracy = 63.48199999999999%, Loss = 0.3653978884220123
Epoch: 9738, Batch Gradient Norm: 9.947343826029204
Epoch: 9738, Batch Gradient Norm after: 9.947343826029204
Epoch 9739/10000, Prediction Accuracy = 63.4%, Loss = 0.35709444284439085
Epoch: 9739, Batch Gradient Norm: 11.45782826396113
Epoch: 9739, Batch Gradient Norm after: 11.45782826396113
Epoch 9740/10000, Prediction Accuracy = 63.414%, Loss = 0.367216032743454
Epoch: 9740, Batch Gradient Norm: 11.68636358413732
Epoch: 9740, Batch Gradient Norm after: 11.68636358413732
Epoch 9741/10000, Prediction Accuracy = 63.562%, Loss = 0.3701748430728912
Epoch: 9741, Batch Gradient Norm: 10.030657299789274
Epoch: 9741, Batch Gradient Norm after: 10.030657299789274
Epoch 9742/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.35806435346603394
Epoch: 9742, Batch Gradient Norm: 10.306644013133406
Epoch: 9742, Batch Gradient Norm after: 10.306644013133406
Epoch 9743/10000, Prediction Accuracy = 63.71%, Loss = 0.3594678223133087
Epoch: 9743, Batch Gradient Norm: 11.989220729365247
Epoch: 9743, Batch Gradient Norm after: 11.989220729365247
Epoch 9744/10000, Prediction Accuracy = 63.59000000000001%, Loss = 0.3724311292171478
Epoch: 9744, Batch Gradient Norm: 10.014372188845972
Epoch: 9744, Batch Gradient Norm after: 10.014372188845972
Epoch 9745/10000, Prediction Accuracy = 63.67800000000001%, Loss = 0.35841870307922363
Epoch: 9745, Batch Gradient Norm: 9.98663954853972
Epoch: 9745, Batch Gradient Norm after: 9.98663954853972
Epoch 9746/10000, Prediction Accuracy = 63.58399999999999%, Loss = 0.357170170545578
Epoch: 9746, Batch Gradient Norm: 10.567470732573836
Epoch: 9746, Batch Gradient Norm after: 10.567470732573836
Epoch 9747/10000, Prediction Accuracy = 63.4%, Loss = 0.36127310395240786
Epoch: 9747, Batch Gradient Norm: 10.169652218229846
Epoch: 9747, Batch Gradient Norm after: 10.169652218229846
Epoch 9748/10000, Prediction Accuracy = 63.584%, Loss = 0.3586698353290558
Epoch: 9748, Batch Gradient Norm: 9.476906110430184
Epoch: 9748, Batch Gradient Norm after: 9.476906110430184
Epoch 9749/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.35494478344917296
Epoch: 9749, Batch Gradient Norm: 9.517290649946107
Epoch: 9749, Batch Gradient Norm after: 9.517290649946107
Epoch 9750/10000, Prediction Accuracy = 63.688%, Loss = 0.357164466381073
Epoch: 9750, Batch Gradient Norm: 8.495575163173246
Epoch: 9750, Batch Gradient Norm after: 8.495575163173246
Epoch 9751/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.3505288243293762
Epoch: 9751, Batch Gradient Norm: 10.393958075118553
Epoch: 9751, Batch Gradient Norm after: 10.393958075118553
Epoch 9752/10000, Prediction Accuracy = 63.544000000000004%, Loss = 0.35923885703086855
Epoch: 9752, Batch Gradient Norm: 12.17712662890543
Epoch: 9752, Batch Gradient Norm after: 12.17712662890543
Epoch 9753/10000, Prediction Accuracy = 63.476%, Loss = 0.3712993681430817
Epoch: 9753, Batch Gradient Norm: 9.927458261281732
Epoch: 9753, Batch Gradient Norm after: 9.927458261281732
Epoch 9754/10000, Prediction Accuracy = 63.61600000000001%, Loss = 0.35539870858192446
Epoch: 9754, Batch Gradient Norm: 9.271906736188562
Epoch: 9754, Batch Gradient Norm after: 9.271906736188562
Epoch 9755/10000, Prediction Accuracy = 63.754%, Loss = 0.351336145401001
Epoch: 9755, Batch Gradient Norm: 10.086041332115515
Epoch: 9755, Batch Gradient Norm after: 10.086041332115515
Epoch 9756/10000, Prediction Accuracy = 63.70400000000001%, Loss = 0.3568238437175751
Epoch: 9756, Batch Gradient Norm: 12.01850223913087
Epoch: 9756, Batch Gradient Norm after: 12.01850223913087
Epoch 9757/10000, Prediction Accuracy = 63.464%, Loss = 0.3725231647491455
Epoch: 9757, Batch Gradient Norm: 10.664250114381586
Epoch: 9757, Batch Gradient Norm after: 10.664250114381586
Epoch 9758/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.36263259053230285
Epoch: 9758, Batch Gradient Norm: 9.782989611733282
Epoch: 9758, Batch Gradient Norm after: 9.782989611733282
Epoch 9759/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.35541131496429446
Epoch: 9759, Batch Gradient Norm: 8.963026547267038
Epoch: 9759, Batch Gradient Norm after: 8.963026547267038
Epoch 9760/10000, Prediction Accuracy = 63.698%, Loss = 0.35015103220939636
Epoch: 9760, Batch Gradient Norm: 9.492730113308081
Epoch: 9760, Batch Gradient Norm after: 9.492730113308081
Epoch 9761/10000, Prediction Accuracy = 63.470000000000006%, Loss = 0.35387812852859496
Epoch: 9761, Batch Gradient Norm: 10.309681886220261
Epoch: 9761, Batch Gradient Norm after: 10.309681886220261
Epoch 9762/10000, Prediction Accuracy = 63.59799999999999%, Loss = 0.3606098353862762
Epoch: 9762, Batch Gradient Norm: 10.74485264046405
Epoch: 9762, Batch Gradient Norm after: 10.74485264046405
Epoch 9763/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3645991563796997
Epoch: 9763, Batch Gradient Norm: 9.805168597713822
Epoch: 9763, Batch Gradient Norm after: 9.805168597713822
Epoch 9764/10000, Prediction Accuracy = 63.57000000000001%, Loss = 0.35736322999000547
Epoch: 9764, Batch Gradient Norm: 9.424836293577734
Epoch: 9764, Batch Gradient Norm after: 9.424836293577734
Epoch 9765/10000, Prediction Accuracy = 63.75%, Loss = 0.35457455515861513
Epoch: 9765, Batch Gradient Norm: 10.651524113857972
Epoch: 9765, Batch Gradient Norm after: 10.651524113857972
Epoch 9766/10000, Prediction Accuracy = 63.462%, Loss = 0.3617571771144867
Epoch: 9766, Batch Gradient Norm: 11.58258078298511
Epoch: 9766, Batch Gradient Norm after: 11.58258078298511
Epoch 9767/10000, Prediction Accuracy = 63.638%, Loss = 0.3671484410762787
Epoch: 9767, Batch Gradient Norm: 11.660237216856808
Epoch: 9767, Batch Gradient Norm after: 11.660237216856808
Epoch 9768/10000, Prediction Accuracy = 63.474000000000004%, Loss = 0.3677412927150726
Epoch: 9768, Batch Gradient Norm: 11.668647537819565
Epoch: 9768, Batch Gradient Norm after: 11.668647537819565
Epoch 9769/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.3696369886398315
Epoch: 9769, Batch Gradient Norm: 10.607223685551308
Epoch: 9769, Batch Gradient Norm after: 10.607223685551308
Epoch 9770/10000, Prediction Accuracy = 63.676%, Loss = 0.3631243407726288
Epoch: 9770, Batch Gradient Norm: 8.948004963886062
Epoch: 9770, Batch Gradient Norm after: 8.948004963886062
Epoch 9771/10000, Prediction Accuracy = 63.7%, Loss = 0.3513618052005768
Epoch: 9771, Batch Gradient Norm: 9.395677689482877
Epoch: 9771, Batch Gradient Norm after: 9.395677689482877
Epoch 9772/10000, Prediction Accuracy = 63.626%, Loss = 0.3536501586437225
Epoch: 9772, Batch Gradient Norm: 10.636419302778084
Epoch: 9772, Batch Gradient Norm after: 10.636419302778084
Epoch 9773/10000, Prediction Accuracy = 63.55%, Loss = 0.36236619353294375
Epoch: 9773, Batch Gradient Norm: 9.598544254069438
Epoch: 9773, Batch Gradient Norm after: 9.598544254069438
Epoch 9774/10000, Prediction Accuracy = 63.462%, Loss = 0.3553820848464966
Epoch: 9774, Batch Gradient Norm: 9.054793990568276
Epoch: 9774, Batch Gradient Norm after: 9.054793990568276
Epoch 9775/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.3510104715824127
Epoch: 9775, Batch Gradient Norm: 9.808572920646546
Epoch: 9775, Batch Gradient Norm after: 9.808572920646546
Epoch 9776/10000, Prediction Accuracy = 63.586%, Loss = 0.355072021484375
Epoch: 9776, Batch Gradient Norm: 10.488318766235382
Epoch: 9776, Batch Gradient Norm after: 10.488318766235382
Epoch 9777/10000, Prediction Accuracy = 63.584%, Loss = 0.36005179286003114
Epoch: 9777, Batch Gradient Norm: 9.88431108479674
Epoch: 9777, Batch Gradient Norm after: 9.88431108479674
Epoch 9778/10000, Prediction Accuracy = 63.70399999999999%, Loss = 0.3574230372905731
Epoch: 9778, Batch Gradient Norm: 9.010986526926605
Epoch: 9778, Batch Gradient Norm after: 9.010986526926605
Epoch 9779/10000, Prediction Accuracy = 63.67%, Loss = 0.3525354266166687
Epoch: 9779, Batch Gradient Norm: 9.297612823695165
Epoch: 9779, Batch Gradient Norm after: 9.297612823695165
Epoch 9780/10000, Prediction Accuracy = 63.762%, Loss = 0.35268144607543944
Epoch: 9780, Batch Gradient Norm: 11.23435080078921
Epoch: 9780, Batch Gradient Norm after: 11.23435080078921
Epoch 9781/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.3643409192562103
Epoch: 9781, Batch Gradient Norm: 12.107400919049525
Epoch: 9781, Batch Gradient Norm after: 12.107400919049525
Epoch 9782/10000, Prediction Accuracy = 63.362%, Loss = 0.3715114414691925
Epoch: 9782, Batch Gradient Norm: 10.513876126580676
Epoch: 9782, Batch Gradient Norm after: 10.513876126580676
Epoch 9783/10000, Prediction Accuracy = 63.617999999999995%, Loss = 0.36022062301635743
Epoch: 9783, Batch Gradient Norm: 9.661631016576104
Epoch: 9783, Batch Gradient Norm after: 9.661631016576104
Epoch 9784/10000, Prediction Accuracy = 63.648%, Loss = 0.35532393455505373
Epoch: 9784, Batch Gradient Norm: 9.499680233445082
Epoch: 9784, Batch Gradient Norm after: 9.499680233445082
Epoch 9785/10000, Prediction Accuracy = 63.67%, Loss = 0.35370848774909974
Epoch: 9785, Batch Gradient Norm: 9.983439146583741
Epoch: 9785, Batch Gradient Norm after: 9.983439146583741
Epoch 9786/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.3576142430305481
Epoch: 9786, Batch Gradient Norm: 10.070155681948364
Epoch: 9786, Batch Gradient Norm after: 10.070155681948364
Epoch 9787/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35953567624092103
Epoch: 9787, Batch Gradient Norm: 10.568097528739472
Epoch: 9787, Batch Gradient Norm after: 10.568097528739472
Epoch 9788/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.3636777400970459
Epoch: 9788, Batch Gradient Norm: 10.154326405858326
Epoch: 9788, Batch Gradient Norm after: 10.154326405858326
Epoch 9789/10000, Prediction Accuracy = 63.50999999999999%, Loss = 0.3611800014972687
Epoch: 9789, Batch Gradient Norm: 9.710855940446349
Epoch: 9789, Batch Gradient Norm after: 9.710855940446349
Epoch 9790/10000, Prediction Accuracy = 63.604%, Loss = 0.3548202633857727
Epoch: 9790, Batch Gradient Norm: 10.822993573884597
Epoch: 9790, Batch Gradient Norm after: 10.822993573884597
Epoch 9791/10000, Prediction Accuracy = 63.532%, Loss = 0.3618608951568604
Epoch: 9791, Batch Gradient Norm: 11.112118375763147
Epoch: 9791, Batch Gradient Norm after: 11.112118375763147
Epoch 9792/10000, Prediction Accuracy = 63.51800000000001%, Loss = 0.3644747376441956
Epoch: 9792, Batch Gradient Norm: 11.10184957502726
Epoch: 9792, Batch Gradient Norm after: 11.10184957502726
Epoch 9793/10000, Prediction Accuracy = 63.580000000000005%, Loss = 0.36471999287605283
Epoch: 9793, Batch Gradient Norm: 11.111973730596063
Epoch: 9793, Batch Gradient Norm after: 11.111973730596063
Epoch 9794/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3646710753440857
Epoch: 9794, Batch Gradient Norm: 10.725580207239865
Epoch: 9794, Batch Gradient Norm after: 10.725580207239865
Epoch 9795/10000, Prediction Accuracy = 63.67199999999999%, Loss = 0.36098835468292234
Epoch: 9795, Batch Gradient Norm: 10.900833531434342
Epoch: 9795, Batch Gradient Norm after: 10.900833531434342
Epoch 9796/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.3617109775543213
Epoch: 9796, Batch Gradient Norm: 10.51802457397938
Epoch: 9796, Batch Gradient Norm after: 10.51802457397938
Epoch 9797/10000, Prediction Accuracy = 63.574%, Loss = 0.3601690411567688
Epoch: 9797, Batch Gradient Norm: 9.31939853747091
Epoch: 9797, Batch Gradient Norm after: 9.31939853747091
Epoch 9798/10000, Prediction Accuracy = 63.788%, Loss = 0.35410434007644653
Epoch: 9798, Batch Gradient Norm: 8.207045472616901
Epoch: 9798, Batch Gradient Norm after: 8.207045472616901
Epoch 9799/10000, Prediction Accuracy = 63.676%, Loss = 0.34822055101394656
Epoch: 9799, Batch Gradient Norm: 8.475418285698701
Epoch: 9799, Batch Gradient Norm after: 8.475418285698701
Epoch 9800/10000, Prediction Accuracy = 63.714%, Loss = 0.3478971302509308
Epoch: 9800, Batch Gradient Norm: 9.650901378745228
Epoch: 9800, Batch Gradient Norm after: 9.650901378745228
Epoch 9801/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.35442203283309937
Epoch: 9801, Batch Gradient Norm: 10.796792661648729
Epoch: 9801, Batch Gradient Norm after: 10.796792661648729
Epoch 9802/10000, Prediction Accuracy = 63.536%, Loss = 0.3619342088699341
Epoch: 9802, Batch Gradient Norm: 11.064917433795944
Epoch: 9802, Batch Gradient Norm after: 11.064917433795944
Epoch 9803/10000, Prediction Accuracy = 63.664%, Loss = 0.36406704783439636
Epoch: 9803, Batch Gradient Norm: 12.292738462938896
Epoch: 9803, Batch Gradient Norm after: 12.292738462938896
Epoch 9804/10000, Prediction Accuracy = 63.598%, Loss = 0.3728744089603424
Epoch: 9804, Batch Gradient Norm: 12.096923191335566
Epoch: 9804, Batch Gradient Norm after: 12.096923191335566
Epoch 9805/10000, Prediction Accuracy = 63.65%, Loss = 0.37201217412948606
Epoch: 9805, Batch Gradient Norm: 10.403588437101277
Epoch: 9805, Batch Gradient Norm after: 10.403588437101277
Epoch 9806/10000, Prediction Accuracy = 63.70400000000001%, Loss = 0.3594433844089508
Epoch: 9806, Batch Gradient Norm: 10.377519502461212
Epoch: 9806, Batch Gradient Norm after: 10.377519502461212
Epoch 9807/10000, Prediction Accuracy = 63.464%, Loss = 0.3573656678199768
Epoch: 9807, Batch Gradient Norm: 12.22465760437385
Epoch: 9807, Batch Gradient Norm after: 12.22465760437385
Epoch 9808/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.3713203191757202
Epoch: 9808, Batch Gradient Norm: 10.71565648859934
Epoch: 9808, Batch Gradient Norm after: 10.71565648859934
Epoch 9809/10000, Prediction Accuracy = 63.446000000000005%, Loss = 0.36201674342155454
Epoch: 9809, Batch Gradient Norm: 7.98822352420213
Epoch: 9809, Batch Gradient Norm after: 7.98822352420213
Epoch 9810/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3450943768024445
Epoch: 9810, Batch Gradient Norm: 7.706561371903633
Epoch: 9810, Batch Gradient Norm after: 7.706561371903633
Epoch 9811/10000, Prediction Accuracy = 63.653999999999996%, Loss = 0.3426142275333405
Epoch: 9811, Batch Gradient Norm: 9.065767483942574
Epoch: 9811, Batch Gradient Norm after: 9.065767483942574
Epoch 9812/10000, Prediction Accuracy = 63.55800000000001%, Loss = 0.3494190812110901
Epoch: 9812, Batch Gradient Norm: 11.022522677356248
Epoch: 9812, Batch Gradient Norm after: 11.022522677356248
Epoch 9813/10000, Prediction Accuracy = 63.61800000000001%, Loss = 0.3632934629917145
Epoch: 9813, Batch Gradient Norm: 10.090264566571175
Epoch: 9813, Batch Gradient Norm after: 10.090264566571175
Epoch 9814/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.3585461676120758
Epoch: 9814, Batch Gradient Norm: 9.48305402493844
Epoch: 9814, Batch Gradient Norm after: 9.48305402493844
Epoch 9815/10000, Prediction Accuracy = 63.71600000000001%, Loss = 0.3551104664802551
Epoch: 9815, Batch Gradient Norm: 10.348143888326982
Epoch: 9815, Batch Gradient Norm after: 10.348143888326982
Epoch 9816/10000, Prediction Accuracy = 63.686%, Loss = 0.36006117463111875
Epoch: 9816, Batch Gradient Norm: 10.859337657542477
Epoch: 9816, Batch Gradient Norm after: 10.859337657542477
Epoch 9817/10000, Prediction Accuracy = 63.638%, Loss = 0.3635426044464111
Epoch: 9817, Batch Gradient Norm: 9.856569479471194
Epoch: 9817, Batch Gradient Norm after: 9.856569479471194
Epoch 9818/10000, Prediction Accuracy = 63.664%, Loss = 0.3553010165691376
Epoch: 9818, Batch Gradient Norm: 10.423330349058157
Epoch: 9818, Batch Gradient Norm after: 10.423330349058157
Epoch 9819/10000, Prediction Accuracy = 63.628%, Loss = 0.3593193292617798
Epoch: 9819, Batch Gradient Norm: 10.425639730576986
Epoch: 9819, Batch Gradient Norm after: 10.425639730576986
Epoch 9820/10000, Prediction Accuracy = 63.616%, Loss = 0.3608717262744904
Epoch: 9820, Batch Gradient Norm: 11.014823468730395
Epoch: 9820, Batch Gradient Norm after: 11.014823468730395
Epoch 9821/10000, Prediction Accuracy = 63.336%, Loss = 0.3649923801422119
Epoch: 9821, Batch Gradient Norm: 10.659609782731254
Epoch: 9821, Batch Gradient Norm after: 10.659609782731254
Epoch 9822/10000, Prediction Accuracy = 63.65%, Loss = 0.36157463788986205
Epoch: 9822, Batch Gradient Norm: 11.058607238950174
Epoch: 9822, Batch Gradient Norm after: 11.058607238950174
Epoch 9823/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.36351760625839236
Epoch: 9823, Batch Gradient Norm: 10.116525994581583
Epoch: 9823, Batch Gradient Norm after: 10.116525994581583
Epoch 9824/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.3579340517520905
Epoch: 9824, Batch Gradient Norm: 9.190898772305669
Epoch: 9824, Batch Gradient Norm after: 9.190898772305669
Epoch 9825/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.352761435508728
Epoch: 9825, Batch Gradient Norm: 8.813831373137978
Epoch: 9825, Batch Gradient Norm after: 8.813831373137978
Epoch 9826/10000, Prediction Accuracy = 63.71999999999999%, Loss = 0.35034130811691283
Epoch: 9826, Batch Gradient Norm: 9.32555623542705
Epoch: 9826, Batch Gradient Norm after: 9.32555623542705
Epoch 9827/10000, Prediction Accuracy = 63.758%, Loss = 0.35346822142601014
Epoch: 9827, Batch Gradient Norm: 9.081273512422218
Epoch: 9827, Batch Gradient Norm after: 9.081273512422218
Epoch 9828/10000, Prediction Accuracy = 63.468%, Loss = 0.3514025866985321
Epoch: 9828, Batch Gradient Norm: 9.005303763354869
Epoch: 9828, Batch Gradient Norm after: 9.005303763354869
Epoch 9829/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.3490492761135101
Epoch: 9829, Batch Gradient Norm: 13.081504447346557
Epoch: 9829, Batch Gradient Norm after: 13.081504447346557
Epoch 9830/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3773510217666626
Epoch: 9830, Batch Gradient Norm: 13.09171390209875
Epoch: 9830, Batch Gradient Norm after: 13.09171390209875
Epoch 9831/10000, Prediction Accuracy = 63.55%, Loss = 0.3803272247314453
Epoch: 9831, Batch Gradient Norm: 9.76257687589685
Epoch: 9831, Batch Gradient Norm after: 9.76257687589685
Epoch 9832/10000, Prediction Accuracy = 63.702%, Loss = 0.3545155942440033
Epoch: 9832, Batch Gradient Norm: 10.373475146639578
Epoch: 9832, Batch Gradient Norm after: 10.373475146639578
Epoch 9833/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.35836116075515745
Epoch: 9833, Batch Gradient Norm: 12.261086339537481
Epoch: 9833, Batch Gradient Norm after: 12.261086339537481
Epoch 9834/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.3741129994392395
Epoch: 9834, Batch Gradient Norm: 10.402432611232973
Epoch: 9834, Batch Gradient Norm after: 10.402432611232973
Epoch 9835/10000, Prediction Accuracy = 63.414%, Loss = 0.36044096350669863
Epoch: 9835, Batch Gradient Norm: 8.52947891187058
Epoch: 9835, Batch Gradient Norm after: 8.52947891187058
Epoch 9836/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3476392388343811
Epoch: 9836, Batch Gradient Norm: 9.704834119602769
Epoch: 9836, Batch Gradient Norm after: 9.704834119602769
Epoch 9837/10000, Prediction Accuracy = 63.61%, Loss = 0.3535906791687012
Epoch: 9837, Batch Gradient Norm: 11.042928044587455
Epoch: 9837, Batch Gradient Norm after: 11.042928044587455
Epoch 9838/10000, Prediction Accuracy = 63.712%, Loss = 0.3637553513050079
Epoch: 9838, Batch Gradient Norm: 10.589092135950724
Epoch: 9838, Batch Gradient Norm after: 10.589092135950724
Epoch 9839/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.36087862253189085
Epoch: 9839, Batch Gradient Norm: 9.77604163116131
Epoch: 9839, Batch Gradient Norm after: 9.77604163116131
Epoch 9840/10000, Prediction Accuracy = 63.624%, Loss = 0.35627531409263613
Epoch: 9840, Batch Gradient Norm: 8.388554867813024
Epoch: 9840, Batch Gradient Norm after: 8.388554867813024
Epoch 9841/10000, Prediction Accuracy = 63.69%, Loss = 0.34791733622550963
Epoch: 9841, Batch Gradient Norm: 8.978616534985854
Epoch: 9841, Batch Gradient Norm after: 8.978616534985854
Epoch 9842/10000, Prediction Accuracy = 63.66199999999999%, Loss = 0.34966194033622744
Epoch: 9842, Batch Gradient Norm: 9.993919737625166
Epoch: 9842, Batch Gradient Norm after: 9.993919737625166
Epoch 9843/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.3563764989376068
Epoch: 9843, Batch Gradient Norm: 8.875390733256236
Epoch: 9843, Batch Gradient Norm after: 8.875390733256236
Epoch 9844/10000, Prediction Accuracy = 63.708000000000006%, Loss = 0.34983193278312685
Epoch: 9844, Batch Gradient Norm: 8.400868997333388
Epoch: 9844, Batch Gradient Norm after: 8.400868997333388
Epoch 9845/10000, Prediction Accuracy = 63.705999999999996%, Loss = 0.3473526298999786
Epoch: 9845, Batch Gradient Norm: 10.559362635062046
Epoch: 9845, Batch Gradient Norm after: 10.559362635062046
Epoch 9846/10000, Prediction Accuracy = 63.553999999999995%, Loss = 0.3612843990325928
Epoch: 9846, Batch Gradient Norm: 12.525296978067626
Epoch: 9846, Batch Gradient Norm after: 12.525296978067626
Epoch 9847/10000, Prediction Accuracy = 63.648%, Loss = 0.37686885595321656
Epoch: 9847, Batch Gradient Norm: 11.251565729223081
Epoch: 9847, Batch Gradient Norm after: 11.251565729223081
Epoch 9848/10000, Prediction Accuracy = 63.484%, Loss = 0.365325391292572
Epoch: 9848, Batch Gradient Norm: 10.143063900252484
Epoch: 9848, Batch Gradient Norm after: 10.143063900252484
Epoch 9849/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3567193567752838
Epoch: 9849, Batch Gradient Norm: 9.373980170416361
Epoch: 9849, Batch Gradient Norm after: 9.373980170416361
Epoch 9850/10000, Prediction Accuracy = 63.636%, Loss = 0.3515391409397125
Epoch: 9850, Batch Gradient Norm: 10.621547979002967
Epoch: 9850, Batch Gradient Norm after: 10.621547979002967
Epoch 9851/10000, Prediction Accuracy = 63.61999999999999%, Loss = 0.3599531888961792
Epoch: 9851, Batch Gradient Norm: 10.552554895973126
Epoch: 9851, Batch Gradient Norm after: 10.552554895973126
Epoch 9852/10000, Prediction Accuracy = 63.58399999999999%, Loss = 0.36070990562438965
Epoch: 9852, Batch Gradient Norm: 10.398368628265214
Epoch: 9852, Batch Gradient Norm after: 10.398368628265214
Epoch 9853/10000, Prediction Accuracy = 63.55799999999999%, Loss = 0.3597073256969452
Epoch: 9853, Batch Gradient Norm: 10.845694136759112
Epoch: 9853, Batch Gradient Norm after: 10.845694136759112
Epoch 9854/10000, Prediction Accuracy = 63.742000000000004%, Loss = 0.3625149726867676
Epoch: 9854, Batch Gradient Norm: 10.651519293313978
Epoch: 9854, Batch Gradient Norm after: 10.651519293313978
Epoch 9855/10000, Prediction Accuracy = 63.674%, Loss = 0.36120258569717406
Epoch: 9855, Batch Gradient Norm: 11.125169302492775
Epoch: 9855, Batch Gradient Norm after: 11.125169302492775
Epoch 9856/10000, Prediction Accuracy = 63.6%, Loss = 0.36282610297203066
Epoch: 9856, Batch Gradient Norm: 10.618812243419141
Epoch: 9856, Batch Gradient Norm after: 10.618812243419141
Epoch 9857/10000, Prediction Accuracy = 63.501999999999995%, Loss = 0.3582127332687378
Epoch: 9857, Batch Gradient Norm: 10.325872117808512
Epoch: 9857, Batch Gradient Norm after: 10.325872117808512
Epoch 9858/10000, Prediction Accuracy = 63.532%, Loss = 0.35609065890312197
Epoch: 9858, Batch Gradient Norm: 10.366363319703968
Epoch: 9858, Batch Gradient Norm after: 10.366363319703968
Epoch 9859/10000, Prediction Accuracy = 63.54%, Loss = 0.35965808629989626
Epoch: 9859, Batch Gradient Norm: 8.886291486758413
Epoch: 9859, Batch Gradient Norm after: 8.886291486758413
Epoch 9860/10000, Prediction Accuracy = 63.626%, Loss = 0.35106141567230226
Epoch: 9860, Batch Gradient Norm: 8.606876183287582
Epoch: 9860, Batch Gradient Norm after: 8.606876183287582
Epoch 9861/10000, Prediction Accuracy = 63.74000000000001%, Loss = 0.34836583733558657
Epoch: 9861, Batch Gradient Norm: 11.64242257532679
Epoch: 9861, Batch Gradient Norm after: 11.64242257532679
Epoch 9862/10000, Prediction Accuracy = 63.714%, Loss = 0.368930584192276
Epoch: 9862, Batch Gradient Norm: 11.385754658607066
Epoch: 9862, Batch Gradient Norm after: 11.385754658607066
Epoch 9863/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.3683850169181824
Epoch: 9863, Batch Gradient Norm: 9.566771608525688
Epoch: 9863, Batch Gradient Norm after: 9.566771608525688
Epoch 9864/10000, Prediction Accuracy = 63.532000000000004%, Loss = 0.35542781949043273
Epoch: 9864, Batch Gradient Norm: 9.395480847369123
Epoch: 9864, Batch Gradient Norm after: 9.395480847369123
Epoch 9865/10000, Prediction Accuracy = 63.471999999999994%, Loss = 0.35312471985816957
Epoch: 9865, Batch Gradient Norm: 10.94124687722425
Epoch: 9865, Batch Gradient Norm after: 10.94124687722425
Epoch 9866/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.36173340678215027
Epoch: 9866, Batch Gradient Norm: 12.461053798959654
Epoch: 9866, Batch Gradient Norm after: 12.461053798959654
Epoch 9867/10000, Prediction Accuracy = 63.53599999999999%, Loss = 0.37282233834266665
Epoch: 9867, Batch Gradient Norm: 10.597962009779222
Epoch: 9867, Batch Gradient Norm after: 10.597962009779222
Epoch 9868/10000, Prediction Accuracy = 63.602%, Loss = 0.359766286611557
Epoch: 9868, Batch Gradient Norm: 8.642653286357042
Epoch: 9868, Batch Gradient Norm after: 8.642653286357042
Epoch 9869/10000, Prediction Accuracy = 63.652%, Loss = 0.34717935919761655
Epoch: 9869, Batch Gradient Norm: 9.653679710217949
Epoch: 9869, Batch Gradient Norm after: 9.653679710217949
Epoch 9870/10000, Prediction Accuracy = 63.53000000000001%, Loss = 0.3532406985759735
Epoch: 9870, Batch Gradient Norm: 9.53870434936424
Epoch: 9870, Batch Gradient Norm after: 9.53870434936424
Epoch 9871/10000, Prediction Accuracy = 63.56200000000001%, Loss = 0.35393372774124143
Epoch: 9871, Batch Gradient Norm: 8.387083874942473
Epoch: 9871, Batch Gradient Norm after: 8.387083874942473
Epoch 9872/10000, Prediction Accuracy = 63.622%, Loss = 0.3472563087940216
Epoch: 9872, Batch Gradient Norm: 9.076608997257013
Epoch: 9872, Batch Gradient Norm after: 9.076608997257013
Epoch 9873/10000, Prediction Accuracy = 63.754%, Loss = 0.350389701128006
Epoch: 9873, Batch Gradient Norm: 10.711596574717225
Epoch: 9873, Batch Gradient Norm after: 10.711596574717225
Epoch 9874/10000, Prediction Accuracy = 63.61%, Loss = 0.36131600141525266
Epoch: 9874, Batch Gradient Norm: 11.622709196292378
Epoch: 9874, Batch Gradient Norm after: 11.622709196292378
Epoch 9875/10000, Prediction Accuracy = 63.581999999999994%, Loss = 0.366499125957489
Epoch: 9875, Batch Gradient Norm: 10.169238177101953
Epoch: 9875, Batch Gradient Norm after: 10.169238177101953
Epoch 9876/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.35529600977897646
Epoch: 9876, Batch Gradient Norm: 9.64560579659053
Epoch: 9876, Batch Gradient Norm after: 9.64560579659053
Epoch 9877/10000, Prediction Accuracy = 63.50599999999999%, Loss = 0.3519171833992004
Epoch: 9877, Batch Gradient Norm: 10.7454026323988
Epoch: 9877, Batch Gradient Norm after: 10.7454026323988
Epoch 9878/10000, Prediction Accuracy = 63.644000000000005%, Loss = 0.35917469263076784
Epoch: 9878, Batch Gradient Norm: 12.238362066890316
Epoch: 9878, Batch Gradient Norm after: 12.238362066890316
Epoch 9879/10000, Prediction Accuracy = 63.65%, Loss = 0.3708375632762909
Epoch: 9879, Batch Gradient Norm: 11.134027129982204
Epoch: 9879, Batch Gradient Norm after: 11.134027129982204
Epoch 9880/10000, Prediction Accuracy = 63.61%, Loss = 0.36546712517738345
Epoch: 9880, Batch Gradient Norm: 9.147808389571015
Epoch: 9880, Batch Gradient Norm after: 9.147808389571015
Epoch 9881/10000, Prediction Accuracy = 63.769999999999996%, Loss = 0.35350077152252196
Epoch: 9881, Batch Gradient Norm: 7.935377279054027
Epoch: 9881, Batch Gradient Norm after: 7.935377279054027
Epoch 9882/10000, Prediction Accuracy = 63.620000000000005%, Loss = 0.34655546545982363
Epoch: 9882, Batch Gradient Norm: 8.256593232262173
Epoch: 9882, Batch Gradient Norm after: 8.256593232262173
Epoch 9883/10000, Prediction Accuracy = 63.572%, Loss = 0.347707062959671
Epoch: 9883, Batch Gradient Norm: 10.431135689272551
Epoch: 9883, Batch Gradient Norm after: 10.431135689272551
Epoch 9884/10000, Prediction Accuracy = 63.403999999999996%, Loss = 0.3591088056564331
Epoch: 9884, Batch Gradient Norm: 12.457084347476856
Epoch: 9884, Batch Gradient Norm after: 12.457084347476856
Epoch 9885/10000, Prediction Accuracy = 63.588%, Loss = 0.37402147650718687
Epoch: 9885, Batch Gradient Norm: 13.601545542169506
Epoch: 9885, Batch Gradient Norm after: 13.601545542169506
Epoch 9886/10000, Prediction Accuracy = 63.688%, Loss = 0.38541739583015444
Epoch: 9886, Batch Gradient Norm: 10.136933798005645
Epoch: 9886, Batch Gradient Norm after: 10.136933798005645
Epoch 9887/10000, Prediction Accuracy = 63.702%, Loss = 0.35693342685699464
Epoch: 9887, Batch Gradient Norm: 9.48644050949821
Epoch: 9887, Batch Gradient Norm after: 9.48644050949821
Epoch 9888/10000, Prediction Accuracy = 63.646%, Loss = 0.35196821093559266
Epoch: 9888, Batch Gradient Norm: 8.552707477290852
Epoch: 9888, Batch Gradient Norm after: 8.552707477290852
Epoch 9889/10000, Prediction Accuracy = 63.758%, Loss = 0.3462253987789154
Epoch: 9889, Batch Gradient Norm: 9.048922100236453
Epoch: 9889, Batch Gradient Norm after: 9.048922100236453
Epoch 9890/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.3494084060192108
Epoch: 9890, Batch Gradient Norm: 10.183763868919847
Epoch: 9890, Batch Gradient Norm after: 10.183763868919847
Epoch 9891/10000, Prediction Accuracy = 63.698%, Loss = 0.3569739282131195
Epoch: 9891, Batch Gradient Norm: 10.358496482649278
Epoch: 9891, Batch Gradient Norm after: 10.358496482649278
Epoch 9892/10000, Prediction Accuracy = 63.73199999999999%, Loss = 0.35967774987220763
Epoch: 9892, Batch Gradient Norm: 10.010194280686473
Epoch: 9892, Batch Gradient Norm after: 10.010194280686473
Epoch 9893/10000, Prediction Accuracy = 63.774%, Loss = 0.3574902653694153
Epoch: 9893, Batch Gradient Norm: 10.518103632379274
Epoch: 9893, Batch Gradient Norm after: 10.518103632379274
Epoch 9894/10000, Prediction Accuracy = 63.684000000000005%, Loss = 0.3610165297985077
Epoch: 9894, Batch Gradient Norm: 10.634177704256105
Epoch: 9894, Batch Gradient Norm after: 10.634177704256105
Epoch 9895/10000, Prediction Accuracy = 63.541999999999994%, Loss = 0.3604861617088318
Epoch: 9895, Batch Gradient Norm: 10.274953990186608
Epoch: 9895, Batch Gradient Norm after: 10.274953990186608
Epoch 9896/10000, Prediction Accuracy = 63.462%, Loss = 0.3553015947341919
Epoch: 9896, Batch Gradient Norm: 9.730062464859971
Epoch: 9896, Batch Gradient Norm after: 9.730062464859971
Epoch 9897/10000, Prediction Accuracy = 63.512%, Loss = 0.3512824535369873
Epoch: 9897, Batch Gradient Norm: 10.390476529671291
Epoch: 9897, Batch Gradient Norm after: 10.390476529671291
Epoch 9898/10000, Prediction Accuracy = 63.524%, Loss = 0.3557631254196167
Epoch: 9898, Batch Gradient Norm: 13.087399987900486
Epoch: 9898, Batch Gradient Norm after: 13.087399987900486
Epoch 9899/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.37934060096740724
Epoch: 9899, Batch Gradient Norm: 11.441358778143714
Epoch: 9899, Batch Gradient Norm after: 11.441358778143714
Epoch 9900/10000, Prediction Accuracy = 63.763999999999996%, Loss = 0.36892336010932925
Epoch: 9900, Batch Gradient Norm: 9.121569691220037
Epoch: 9900, Batch Gradient Norm after: 9.121569691220037
Epoch 9901/10000, Prediction Accuracy = 63.70399999999999%, Loss = 0.3510859966278076
Epoch: 9901, Batch Gradient Norm: 9.0024046515778
Epoch: 9901, Batch Gradient Norm after: 9.0024046515778
Epoch 9902/10000, Prediction Accuracy = 63.818%, Loss = 0.349470716714859
Epoch: 9902, Batch Gradient Norm: 9.041409773164998
Epoch: 9902, Batch Gradient Norm after: 9.041409773164998
Epoch 9903/10000, Prediction Accuracy = 63.67%, Loss = 0.34985566735267637
Epoch: 9903, Batch Gradient Norm: 8.629034316765333
Epoch: 9903, Batch Gradient Norm after: 8.629034316765333
Epoch 9904/10000, Prediction Accuracy = 63.616%, Loss = 0.34697035551071165
Epoch: 9904, Batch Gradient Norm: 9.645453348371417
Epoch: 9904, Batch Gradient Norm after: 9.645453348371417
Epoch 9905/10000, Prediction Accuracy = 63.528%, Loss = 0.3540366590023041
Epoch: 9905, Batch Gradient Norm: 10.189552211889167
Epoch: 9905, Batch Gradient Norm after: 10.189552211889167
Epoch 9906/10000, Prediction Accuracy = 63.41199999999999%, Loss = 0.3574384331703186
Epoch: 9906, Batch Gradient Norm: 10.866044841195702
Epoch: 9906, Batch Gradient Norm after: 10.866044841195702
Epoch 9907/10000, Prediction Accuracy = 63.662%, Loss = 0.3626074492931366
Epoch: 9907, Batch Gradient Norm: 9.450060175010577
Epoch: 9907, Batch Gradient Norm after: 9.450060175010577
Epoch 9908/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.3534424066543579
Epoch: 9908, Batch Gradient Norm: 9.169439176582596
Epoch: 9908, Batch Gradient Norm after: 9.169439176582596
Epoch 9909/10000, Prediction Accuracy = 63.664%, Loss = 0.3502593159675598
Epoch: 9909, Batch Gradient Norm: 11.988526747912442
Epoch: 9909, Batch Gradient Norm after: 11.988526747912442
Epoch 9910/10000, Prediction Accuracy = 63.774%, Loss = 0.3689460039138794
Epoch: 9910, Batch Gradient Norm: 13.317512751997995
Epoch: 9910, Batch Gradient Norm after: 13.317512751997995
Epoch 9911/10000, Prediction Accuracy = 63.54600000000001%, Loss = 0.377477365732193
Epoch: 9911, Batch Gradient Norm: 11.44539070304776
Epoch: 9911, Batch Gradient Norm after: 11.44539070304776
Epoch 9912/10000, Prediction Accuracy = 63.538%, Loss = 0.36415432691574096
Epoch: 9912, Batch Gradient Norm: 9.39917361158685
Epoch: 9912, Batch Gradient Norm after: 9.39917361158685
Epoch 9913/10000, Prediction Accuracy = 63.75600000000001%, Loss = 0.35098953247070314
Epoch: 9913, Batch Gradient Norm: 9.71937548943434
Epoch: 9913, Batch Gradient Norm after: 9.71937548943434
Epoch 9914/10000, Prediction Accuracy = 63.724000000000004%, Loss = 0.35437151193618777
Epoch: 9914, Batch Gradient Norm: 10.187097129734218
Epoch: 9914, Batch Gradient Norm after: 10.187097129734218
Epoch 9915/10000, Prediction Accuracy = 63.64%, Loss = 0.358175665140152
Epoch: 9915, Batch Gradient Norm: 9.316037438948815
Epoch: 9915, Batch Gradient Norm after: 9.316037438948815
Epoch 9916/10000, Prediction Accuracy = 63.73%, Loss = 0.3529185354709625
Epoch: 9916, Batch Gradient Norm: 8.2535859587059
Epoch: 9916, Batch Gradient Norm after: 8.2535859587059
Epoch 9917/10000, Prediction Accuracy = 63.616%, Loss = 0.345698744058609
Epoch: 9917, Batch Gradient Norm: 8.672314774336503
Epoch: 9917, Batch Gradient Norm after: 8.672314774336503
Epoch 9918/10000, Prediction Accuracy = 63.73%, Loss = 0.34771215319633486
Epoch: 9918, Batch Gradient Norm: 10.964249105375542
Epoch: 9918, Batch Gradient Norm after: 10.964249105375542
Epoch 9919/10000, Prediction Accuracy = 63.525999999999996%, Loss = 0.361906099319458
Epoch: 9919, Batch Gradient Norm: 12.440521761700934
Epoch: 9919, Batch Gradient Norm after: 12.440521761700934
Epoch 9920/10000, Prediction Accuracy = 63.60600000000001%, Loss = 0.37321901321411133
Epoch: 9920, Batch Gradient Norm: 10.477706359010943
Epoch: 9920, Batch Gradient Norm after: 10.477706359010943
Epoch 9921/10000, Prediction Accuracy = 63.674%, Loss = 0.3580839991569519
Epoch: 9921, Batch Gradient Norm: 9.929732120758171
Epoch: 9921, Batch Gradient Norm after: 9.929732120758171
Epoch 9922/10000, Prediction Accuracy = 63.676%, Loss = 0.3546311676502228
Epoch: 9922, Batch Gradient Norm: 10.345192060534862
Epoch: 9922, Batch Gradient Norm after: 10.345192060534862
Epoch 9923/10000, Prediction Accuracy = 63.593999999999994%, Loss = 0.36068511605262754
Epoch: 9923, Batch Gradient Norm: 10.5008771242353
Epoch: 9923, Batch Gradient Norm after: 10.5008771242353
Epoch 9924/10000, Prediction Accuracy = 63.536%, Loss = 0.3604798078536987
Epoch: 9924, Batch Gradient Norm: 11.349324866967416
Epoch: 9924, Batch Gradient Norm after: 11.349324866967416
Epoch 9925/10000, Prediction Accuracy = 63.604000000000006%, Loss = 0.36696726083755493
Epoch: 9925, Batch Gradient Norm: 10.532543912925606
Epoch: 9925, Batch Gradient Norm after: 10.532543912925606
Epoch 9926/10000, Prediction Accuracy = 63.686%, Loss = 0.3596651554107666
Epoch: 9926, Batch Gradient Norm: 10.22660258906489
Epoch: 9926, Batch Gradient Norm after: 10.22660258906489
Epoch 9927/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.35619994401931765
Epoch: 9927, Batch Gradient Norm: 10.29971715142548
Epoch: 9927, Batch Gradient Norm after: 10.29971715142548
Epoch 9928/10000, Prediction Accuracy = 63.712%, Loss = 0.3557628095149994
Epoch: 9928, Batch Gradient Norm: 10.983542496077886
Epoch: 9928, Batch Gradient Norm after: 10.983542496077886
Epoch 9929/10000, Prediction Accuracy = 63.660000000000004%, Loss = 0.3598743736743927
Epoch: 9929, Batch Gradient Norm: 10.962809387446717
Epoch: 9929, Batch Gradient Norm after: 10.962809387446717
Epoch 9930/10000, Prediction Accuracy = 63.662%, Loss = 0.36092899441719056
Epoch: 9930, Batch Gradient Norm: 9.198873980548342
Epoch: 9930, Batch Gradient Norm after: 9.198873980548342
Epoch 9931/10000, Prediction Accuracy = 63.726%, Loss = 0.3494158685207367
Epoch: 9931, Batch Gradient Norm: 9.121029595766451
Epoch: 9931, Batch Gradient Norm after: 9.121029595766451
Epoch 9932/10000, Prediction Accuracy = 63.757999999999996%, Loss = 0.34849355816841127
Epoch: 9932, Batch Gradient Norm: 10.179178445725226
Epoch: 9932, Batch Gradient Norm after: 10.179178445725226
Epoch 9933/10000, Prediction Accuracy = 63.757999999999996%, Loss = 0.3547604620456696
Epoch: 9933, Batch Gradient Norm: 11.80718267064125
Epoch: 9933, Batch Gradient Norm after: 11.80718267064125
Epoch 9934/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.36711148619651796
Epoch: 9934, Batch Gradient Norm: 10.9074434309333
Epoch: 9934, Batch Gradient Norm after: 10.9074434309333
Epoch 9935/10000, Prediction Accuracy = 63.732000000000006%, Loss = 0.3609659016132355
Epoch: 9935, Batch Gradient Norm: 10.386657093793723
Epoch: 9935, Batch Gradient Norm after: 10.386657093793723
Epoch 9936/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.3568511366844177
Epoch: 9936, Batch Gradient Norm: 9.788929874562921
Epoch: 9936, Batch Gradient Norm after: 9.788929874562921
Epoch 9937/10000, Prediction Accuracy = 63.61%, Loss = 0.35359603762626646
Epoch: 9937, Batch Gradient Norm: 9.844324978427844
Epoch: 9937, Batch Gradient Norm after: 9.844324978427844
Epoch 9938/10000, Prediction Accuracy = 63.668000000000006%, Loss = 0.35436828136444093
Epoch: 9938, Batch Gradient Norm: 9.825686407963213
Epoch: 9938, Batch Gradient Norm after: 9.825686407963213
Epoch 9939/10000, Prediction Accuracy = 63.82000000000001%, Loss = 0.35409648418426515
Epoch: 9939, Batch Gradient Norm: 10.029828693175588
Epoch: 9939, Batch Gradient Norm after: 10.029828693175588
Epoch 9940/10000, Prediction Accuracy = 63.472%, Loss = 0.3560096979141235
Epoch: 9940, Batch Gradient Norm: 10.159204392361117
Epoch: 9940, Batch Gradient Norm after: 10.159204392361117
Epoch 9941/10000, Prediction Accuracy = 63.696000000000005%, Loss = 0.3578184723854065
Epoch: 9941, Batch Gradient Norm: 10.18079545642226
Epoch: 9941, Batch Gradient Norm after: 10.18079545642226
Epoch 9942/10000, Prediction Accuracy = 63.754000000000005%, Loss = 0.3583239734172821
Epoch: 9942, Batch Gradient Norm: 10.92354304479431
Epoch: 9942, Batch Gradient Norm after: 10.92354304479431
Epoch 9943/10000, Prediction Accuracy = 63.778%, Loss = 0.3639292061328888
Epoch: 9943, Batch Gradient Norm: 10.833774426092253
Epoch: 9943, Batch Gradient Norm after: 10.833774426092253
Epoch 9944/10000, Prediction Accuracy = 63.748000000000005%, Loss = 0.3637004256248474
Epoch: 9944, Batch Gradient Norm: 9.540911048124904
Epoch: 9944, Batch Gradient Norm after: 9.540911048124904
Epoch 9945/10000, Prediction Accuracy = 63.63199999999999%, Loss = 0.35332830548286437
Epoch: 9945, Batch Gradient Norm: 9.665933126220407
Epoch: 9945, Batch Gradient Norm after: 9.665933126220407
Epoch 9946/10000, Prediction Accuracy = 63.589999999999996%, Loss = 0.3538148105144501
Epoch: 9946, Batch Gradient Norm: 9.33072541942396
Epoch: 9946, Batch Gradient Norm after: 9.33072541942396
Epoch 9947/10000, Prediction Accuracy = 63.686%, Loss = 0.3516171634197235
Epoch: 9947, Batch Gradient Norm: 9.040627105518526
Epoch: 9947, Batch Gradient Norm after: 9.040627105518526
Epoch 9948/10000, Prediction Accuracy = 63.602%, Loss = 0.3498335719108582
Epoch: 9948, Batch Gradient Norm: 9.546162231343967
Epoch: 9948, Batch Gradient Norm after: 9.546162231343967
Epoch 9949/10000, Prediction Accuracy = 63.843999999999994%, Loss = 0.35301942825317384
Epoch: 9949, Batch Gradient Norm: 10.202221153309312
Epoch: 9949, Batch Gradient Norm after: 10.202221153309312
Epoch 9950/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.3576351761817932
Epoch: 9950, Batch Gradient Norm: 10.503885669388454
Epoch: 9950, Batch Gradient Norm after: 10.503885669388454
Epoch 9951/10000, Prediction Accuracy = 63.68000000000001%, Loss = 0.3592177152633667
Epoch: 9951, Batch Gradient Norm: 10.135790716221932
Epoch: 9951, Batch Gradient Norm after: 10.135790716221932
Epoch 9952/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.35545212030410767
Epoch: 9952, Batch Gradient Norm: 10.528147263379351
Epoch: 9952, Batch Gradient Norm after: 10.528147263379351
Epoch 9953/10000, Prediction Accuracy = 63.64399999999999%, Loss = 0.3577644467353821
Epoch: 9953, Batch Gradient Norm: 11.48962087644901
Epoch: 9953, Batch Gradient Norm after: 11.48962087644901
Epoch 9954/10000, Prediction Accuracy = 63.688%, Loss = 0.3637510299682617
Epoch: 9954, Batch Gradient Norm: 12.021109531665397
Epoch: 9954, Batch Gradient Norm after: 12.021109531665397
Epoch 9955/10000, Prediction Accuracy = 63.489999999999995%, Loss = 0.3690945625305176
Epoch: 9955, Batch Gradient Norm: 9.986413494851645
Epoch: 9955, Batch Gradient Norm after: 9.986413494851645
Epoch 9956/10000, Prediction Accuracy = 63.565999999999995%, Loss = 0.3557681918144226
Epoch: 9956, Batch Gradient Norm: 10.305800424604119
Epoch: 9956, Batch Gradient Norm after: 10.305800424604119
Epoch 9957/10000, Prediction Accuracy = 63.6%, Loss = 0.35886772274971007
Epoch: 9957, Batch Gradient Norm: 9.398559172219905
Epoch: 9957, Batch Gradient Norm after: 9.398559172219905
Epoch 9958/10000, Prediction Accuracy = 63.67999999999999%, Loss = 0.35432628989219667
Epoch: 9958, Batch Gradient Norm: 8.23076584556023
Epoch: 9958, Batch Gradient Norm after: 8.23076584556023
Epoch 9959/10000, Prediction Accuracy = 63.727999999999994%, Loss = 0.34580718278884887
Epoch: 9959, Batch Gradient Norm: 8.782962980289843
Epoch: 9959, Batch Gradient Norm after: 8.782962980289843
Epoch 9960/10000, Prediction Accuracy = 63.726%, Loss = 0.3477340877056122
Epoch: 9960, Batch Gradient Norm: 11.600348115321605
Epoch: 9960, Batch Gradient Norm after: 11.600348115321605
Epoch 9961/10000, Prediction Accuracy = 63.652%, Loss = 0.3632894277572632
Epoch: 9961, Batch Gradient Norm: 12.561483144399164
Epoch: 9961, Batch Gradient Norm after: 12.561483144399164
Epoch 9962/10000, Prediction Accuracy = 63.370000000000005%, Loss = 0.3711566746234894
Epoch: 9962, Batch Gradient Norm: 10.226449841791998
Epoch: 9962, Batch Gradient Norm after: 10.226449841791998
Epoch 9963/10000, Prediction Accuracy = 63.76800000000001%, Loss = 0.3547690212726593
Epoch: 9963, Batch Gradient Norm: 10.661563318308596
Epoch: 9963, Batch Gradient Norm after: 10.661563318308596
Epoch 9964/10000, Prediction Accuracy = 63.638%, Loss = 0.3584143877029419
Epoch: 9964, Batch Gradient Norm: 10.994597131998033
Epoch: 9964, Batch Gradient Norm after: 10.994597131998033
Epoch 9965/10000, Prediction Accuracy = 63.758%, Loss = 0.361125522851944
Epoch: 9965, Batch Gradient Norm: 9.271008560712195
Epoch: 9965, Batch Gradient Norm after: 9.271008560712195
Epoch 9966/10000, Prediction Accuracy = 63.65599999999999%, Loss = 0.3498768091201782
Epoch: 9966, Batch Gradient Norm: 9.065299390538776
Epoch: 9966, Batch Gradient Norm after: 9.065299390538776
Epoch 9967/10000, Prediction Accuracy = 63.85%, Loss = 0.34857316613197326
Epoch: 9967, Batch Gradient Norm: 9.863556565508683
Epoch: 9967, Batch Gradient Norm after: 9.863556565508683
Epoch 9968/10000, Prediction Accuracy = 63.657999999999994%, Loss = 0.35433093309402464
Epoch: 9968, Batch Gradient Norm: 11.170800375346014
Epoch: 9968, Batch Gradient Norm after: 11.170800375346014
Epoch 9969/10000, Prediction Accuracy = 63.56%, Loss = 0.36469518542289736
Epoch: 9969, Batch Gradient Norm: 10.934683769605064
Epoch: 9969, Batch Gradient Norm after: 10.934683769605064
Epoch 9970/10000, Prediction Accuracy = 63.8%, Loss = 0.36369718313217164
Epoch: 9970, Batch Gradient Norm: 11.061948315795275
Epoch: 9970, Batch Gradient Norm after: 11.061948315795275
Epoch 9971/10000, Prediction Accuracy = 63.612%, Loss = 0.36276462078094485
Epoch: 9971, Batch Gradient Norm: 10.819807621165765
Epoch: 9971, Batch Gradient Norm after: 10.819807621165765
Epoch 9972/10000, Prediction Accuracy = 63.774%, Loss = 0.361503404378891
Epoch: 9972, Batch Gradient Norm: 8.85189473066938
Epoch: 9972, Batch Gradient Norm after: 8.85189473066938
Epoch 9973/10000, Prediction Accuracy = 63.608000000000004%, Loss = 0.3475225567817688
Epoch: 9973, Batch Gradient Norm: 8.763360568795937
Epoch: 9973, Batch Gradient Norm after: 8.763360568795937
Epoch 9974/10000, Prediction Accuracy = 63.86%, Loss = 0.34788021445274353
Epoch: 9974, Batch Gradient Norm: 9.879854717750032
Epoch: 9974, Batch Gradient Norm after: 9.879854717750032
Epoch 9975/10000, Prediction Accuracy = 63.529999999999994%, Loss = 0.35366231203079224
Epoch: 9975, Batch Gradient Norm: 12.228606388507929
Epoch: 9975, Batch Gradient Norm after: 12.228606388507929
Epoch 9976/10000, Prediction Accuracy = 63.477999999999994%, Loss = 0.3683101415634155
Epoch: 9976, Batch Gradient Norm: 11.185789400890359
Epoch: 9976, Batch Gradient Norm after: 11.185789400890359
Epoch 9977/10000, Prediction Accuracy = 63.7%, Loss = 0.3608620762825012
Epoch: 9977, Batch Gradient Norm: 8.80080795590885
Epoch: 9977, Batch Gradient Norm after: 8.80080795590885
Epoch 9978/10000, Prediction Accuracy = 63.814%, Loss = 0.34670729637145997
Epoch: 9978, Batch Gradient Norm: 8.644324818181254
Epoch: 9978, Batch Gradient Norm after: 8.644324818181254
Epoch 9979/10000, Prediction Accuracy = 63.751999999999995%, Loss = 0.3465484857559204
Epoch: 9979, Batch Gradient Norm: 10.506230246571075
Epoch: 9979, Batch Gradient Norm after: 10.506230246571075
Epoch 9980/10000, Prediction Accuracy = 63.715999999999994%, Loss = 0.3578202545642853
Epoch: 9980, Batch Gradient Norm: 12.691478638867343
Epoch: 9980, Batch Gradient Norm after: 12.691478638867343
Epoch 9981/10000, Prediction Accuracy = 63.50200000000001%, Loss = 0.3787375509738922
Epoch: 9981, Batch Gradient Norm: 12.716643069539554
Epoch: 9981, Batch Gradient Norm after: 12.716643069539554
Epoch 9982/10000, Prediction Accuracy = 63.03599999999999%, Loss = 0.37897081971168517
Epoch: 9982, Batch Gradient Norm: 6.470364594033462
Epoch: 9982, Batch Gradient Norm after: 6.470364594033462
Epoch 9983/10000, Prediction Accuracy = 63.846000000000004%, Loss = 0.33906310200691225
Epoch: 9983, Batch Gradient Norm: 9.374514237626514
Epoch: 9983, Batch Gradient Norm after: 9.374514237626514
Epoch 9984/10000, Prediction Accuracy = 63.839999999999996%, Loss = 0.3510061681270599
Epoch: 9984, Batch Gradient Norm: 12.047290732167976
Epoch: 9984, Batch Gradient Norm after: 12.047290732167976
Epoch 9985/10000, Prediction Accuracy = 63.641999999999996%, Loss = 0.370219749212265
Epoch: 9985, Batch Gradient Norm: 9.635943415055587
Epoch: 9985, Batch Gradient Norm after: 9.635943415055587
Epoch 9986/10000, Prediction Accuracy = 63.592000000000006%, Loss = 0.3519627571105957
Epoch: 9986, Batch Gradient Norm: 9.243035073469718
Epoch: 9986, Batch Gradient Norm after: 9.243035073469718
Epoch 9987/10000, Prediction Accuracy = 63.596000000000004%, Loss = 0.34805522561073304
Epoch: 9987, Batch Gradient Norm: 9.618750641857524
Epoch: 9987, Batch Gradient Norm after: 9.618750641857524
Epoch 9988/10000, Prediction Accuracy = 63.605999999999995%, Loss = 0.35126137137413027
Epoch: 9988, Batch Gradient Norm: 8.77334983982487
Epoch: 9988, Batch Gradient Norm after: 8.77334983982487
Epoch 9989/10000, Prediction Accuracy = 63.672000000000004%, Loss = 0.347478860616684
Epoch: 9989, Batch Gradient Norm: 8.911558953829147
Epoch: 9989, Batch Gradient Norm after: 8.911558953829147
Epoch 9990/10000, Prediction Accuracy = 63.85%, Loss = 0.34750126004219056
Epoch: 9990, Batch Gradient Norm: 10.995254958048568
Epoch: 9990, Batch Gradient Norm after: 10.995254958048568
Epoch 9991/10000, Prediction Accuracy = 63.86800000000001%, Loss = 0.36051955819129944
Epoch: 9991, Batch Gradient Norm: 12.218230983958605
Epoch: 9991, Batch Gradient Norm after: 12.218230983958605
Epoch 9992/10000, Prediction Accuracy = 63.666%, Loss = 0.3708240926265717
Epoch: 9992, Batch Gradient Norm: 10.469582772358109
Epoch: 9992, Batch Gradient Norm after: 10.469582772358109
Epoch 9993/10000, Prediction Accuracy = 63.628%, Loss = 0.3588195502758026
Epoch: 9993, Batch Gradient Norm: 10.075129129576599
Epoch: 9993, Batch Gradient Norm after: 10.075129129576599
Epoch 9994/10000, Prediction Accuracy = 63.602%, Loss = 0.35603557229042054
Epoch: 9994, Batch Gradient Norm: 10.72860590265848
Epoch: 9994, Batch Gradient Norm after: 10.72860590265848
Epoch 9995/10000, Prediction Accuracy = 63.534000000000006%, Loss = 0.35953077077865603
Epoch: 9995, Batch Gradient Norm: 11.433767170369244
Epoch: 9995, Batch Gradient Norm after: 11.433767170369244
Epoch 9996/10000, Prediction Accuracy = 63.698%, Loss = 0.36525545120239256
Epoch: 9996, Batch Gradient Norm: 10.294852576638249
Epoch: 9996, Batch Gradient Norm after: 10.294852576638249
Epoch 9997/10000, Prediction Accuracy = 63.552%, Loss = 0.3577259719371796
Epoch: 9997, Batch Gradient Norm: 8.538273823130144
Epoch: 9997, Batch Gradient Norm after: 8.538273823130144
Epoch 9998/10000, Prediction Accuracy = 63.734%, Loss = 0.34653759002685547
Epoch: 9998, Batch Gradient Norm: 9.240040646931723
Epoch: 9998, Batch Gradient Norm after: 9.240040646931723
Epoch 9999/10000, Prediction Accuracy = 63.694%, Loss = 0.35015698075294494
Epoch: 9999, Batch Gradient Norm: 11.434433382493635
Epoch: 9999, Batch Gradient Norm after: 11.434433382493635
Epoch 10000/10000, Prediction Accuracy = 63.632000000000005%, Loss = 0.36513946652412416
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/PPO/bert_marl/mode20-dqn/classify_rsmProp2.py", line 226, in <module>
    plt.show()  # Show the final plot
    ^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/pyplot.py", line 607, in show
    return _get_backend_mod().show(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 3567, in show
    cls.mainloop()
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backends/backend_macosx.py", line 178, in start_main_loop
    with _allow_interrupt_macos():
  File "/Users/athmajanvivekananthan/miniconda3/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/matplotlib/backend_bases.py", line 1686, in _allow_interrupt
    old_sigint_handler(*handler_args)
KeyboardInterrupt