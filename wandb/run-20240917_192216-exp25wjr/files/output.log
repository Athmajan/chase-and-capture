Started Training.
Found device: cpu.
60.31862367902483
Episode 0: Reward is -18.0, with steps 9 exeTime5.547817945480347
Episode 1: Reward is -14.0, with steps 7 exeTime5.510454893112183
Episode 2: Reward is -18.0, with steps 9 exeTime7.434659242630005
Episode 3: Reward is -18.0, with steps 9 exeTime6.182621002197266
Episode 4: Reward is -16.0, with steps 8 exeTime4.466475009918213
Episode 5: Reward is -14.0, with steps 7 exeTime5.287653923034668
Episode 6: Reward is -20.0, with steps 10 exeTime6.862122058868408
Episode 7: Reward is -16.0, with steps 8 exeTime6.339275121688843
Episode 8: Reward is -40.0, with steps 20 exeTime19.487218379974365
Episode 9: Reward is -42.0, with steps 21 exeTime18.66094398498535
Checpoint passed
Starting Evaluations with 0'th training epoch
Traceback (most recent call last):
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/learn_idqn_MSE.py", line 370, in <module>
    json.dump(trainingEvaluations, json_file, indent=4)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/learn_idqn_MSE.py", line 336, in train_qnetwork
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/learn_idqn_MSE.py", line 161, in runTest
    action_id, action_distances = agent.act(obs, prev_actions=prev_actions)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/src/agent_seq_rollout.py", line 69, in act
    best_action, action_q_values = self.act_with_info(obs, prev_actions)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/src/agent_seq_rollout.py", line 127, in act_with_info
    res = self._simulate_action_par(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/src/agent_seq_rollout.py", line 228, in _simulate_action_par
    sim_best_action = agent.act(sim_obs, prev_actions=sim_prev_actions)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/src/agent_rule_based.py", line 45, in act
    best_action, action_distances = self.act_with_info(obs)
                                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/iDQN/bert_5050_mode20/bertsekas-marl/src/agent_rule_based.py", line 58, in act_with_info
    min_indices = np.flatnonzero(arrSortedActDist == min_value)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/numpy/core/numeric.py", line 653, in flatnonzero
    return np.nonzero(np.ravel(a))[0]
                      ^^^^^^^^^^^
  File "/Users/athmajanvivekananthan/WCE/JEPA - MARL/multi-agent/bertsekas-marl/spider/lib/python3.11/site-packages/numpy/core/fromnumeric.py", line 1874, in ravel
    return asanyarray(a).ravel(order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt